<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 29 Sep 2025 15:40:58 +0800</lastBuildDate>
    <item>
      <title>Transfer Learning under Group-Label Shift: A Semiparametric Exponential Tilting Approach</title>
      <link>http://arxiv.org/abs/2509.22268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种适用于迁移学习场景的二分类新框架，能够处理源域和目标域间协变量分布和标签分布同时变化的情况。通过引入组标签偏移假设，该方法能够处理子群体不平衡问题并减少虚假相关性，从而提高对现实世界分布变化的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统迁移学习方法通常假设只有协变量分布或标签分布发生变化，而现实世界中这两种分布可能同时发生变化，导致现有方法效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的二分类框架，能够在协变量分布和标签分布同时变化的情况下有效进行迁移学习，提高目标域的分类准确性。&lt;h4&gt;方法&lt;/h4&gt;采用指数倾斜公式建模联合分布差异，通过工具变量策略建立识别条件；开发两步似然估计程序，结合源结果逻辑回归和协变量条件似然估计；推导估计量的一致性和渐近正态性，并将理论扩展到接收者操作特征曲线和曲线下面积等指标。&lt;h4&gt;主要发现&lt;/h4&gt;在子群体偏移场景下，该方法优于现有替代方法；使用waterbirds数据集的半合成应用证实了该方法能够有效传递信息并提高目标域分类准确性。&lt;h4&gt;结论&lt;/h4&gt;所提出的组标签偏移框架能够有效处理迁移学习中的分布变化问题，提高分类准确性，对现实世界应用具有更好的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种适用于迁移学习场景中二分类的新框架，其中源域和目标域的协变量分布和标签分布都可能发生变化。与传统协变量偏移或标签偏移假设不同，我们引入了组标签偏移假设，该假设能够处理子群体不平衡并减少虚假相关性，从而提高对现实世界分布变化的鲁棒性。为了建模联合分布差异，我们采用灵活的指数倾斜公式，并通过工具变量策略建立了温和且可验证的识别条件。我们开发了一种计算效率高的两步似然估计程序，将源结果模型逻辑回归与使用源域和目标域协变量的条件似然估计相结合。我们推导了所得估计量的一致性和渐近正态性，并将理论扩展到接收者操作特征曲线、曲线下面积和其他目标函数，解决了插入式分类器带来的非标准挑战。模拟研究表明，在子群体偏移场景下，我们的方法优于现有替代方法。使用waterbirds数据集的半合成应用进一步证实了所提出方法有效传递信息和提高目标域分类准确性的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new framework for binary classification in transfer learningsettings where both covariate and label distributions may shift between sourceand target domains. Unlike traditional covariate shift or label shiftassumptions, we introduce a group-label shift assumption that accommodatessubpopulation imbalance and mitigates spurious correlations, thereby improvingrobustness to real-world distributional changes. To model the jointdistribution difference, we adopt a flexible exponential tilting formulationand establish mild, verifiable identification conditions via an instrumentalvariable strategy. We develop a computationally efficient two-steplikelihood-based estimation procedure that combines logistic regression for thesource outcome model with conditional likelihood estimation using both sourceand target covariates. We derive consistency and asymptotic normality for theresulting estimators, and extend the theory to receiver operatingcharacteristic curves, the area under the curve, and other target functionals,addressing the nonstandard challenges posed by plug-in classifiers. Simulationstudies demonstrate that our method outperforms existing alternatives undersubpopulation shift scenarios. A semi-synthetic application using thewaterbirds dataset further confirms the proposed method's ability to transferinformation effectively and improve target-domain classification accuracy.</description>
      <author>example@mail.com (Manli Cheng, Subha Maity, Qinglong Tian, Pengfei Li)</author>
      <guid isPermaLink="false">2509.22268v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:57 +0800</pubDate>
    </item>
  <item>
      <title>Towards Understanding Feature Learning in Parameter Transfer</title>
      <link>http://arxiv.org/abs/2509.22056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了迁移学习中部分参数迁移的理论基础，分析了在ReLU卷积神经网络中部分参数重用的条件和影响因素，并通过实验验证了理论发现。&lt;h4&gt;背景&lt;/h4&gt;参数迁移是迁移学习的中心范式，通过在上游和下游模型间共享参数实现知识和任务复用。然而，当仅迁移上游模型的部分参数时，缺乏对这种部分参数重用有益条件的理论理解。&lt;h4&gt;目的&lt;/h4&gt;填补理论空白，理解部分参数重用的条件和影响因素，分析ReLU卷积神经网络设置下参数迁移的机制。&lt;h4&gt;方法&lt;/h4&gt;在理论框架下分析ReLU卷积神经网络的上游和下游模型，表征继承参数如何作为通用知识的载体，识别放大其对目标任务有益影响的关键因素。&lt;h4&gt;主要发现&lt;/h4&gt;分析了为什么在某些情况下，迁移参数会导致目标任务测试准确率低于从头开始训练新模型的原因，并识别了影响参数迁移效果的关键因素。&lt;h4&gt;结论&lt;/h4&gt;通过数值实验和真实世界数据实验验证了理论发现，为部分参数迁移提供了理论基础和实践指导。&lt;h4&gt;翻译&lt;/h4&gt;参数迁移是迁移学习中的中心范式，通过在上游和下游模型之间共享模型参数，实现跨任务和跨域的知识重用。然而，当只有上游模型的一部分参数被转移到下游模型时，我们仍然缺乏对这种部分参数重用有益条件的理论理解，以及影响其有效性的因素。为填补这一空白，我们分析了上游和下游模型都是ReLU卷积神经网络(CNN)的设置。在这一理论框架内，我们表征了继承的参数如何作为通用知识的载体，并确定了放大其对目标任务有益影响的关键因素。此外，我们的分析提供了为什么在某些情况下，转移参数会导致目标任务测试准确率低于从头开始训练新模型的理解。进行了数值实验和真实世界数据实验，以经验性地验证我们的理论发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter transfer is a central paradigm in transfer learning, enablingknowledge reuse across tasks and domains by sharing model parameters betweenupstream and downstream models. However, when only a subset of parameters fromthe upstream model is transferred to the downstream model, there remains a lackof theoretical understanding of the conditions under which such partialparameter reuse is beneficial and of the factors that govern its effectiveness.To address this gap, we analyze a setting in which both the upstream anddownstream models are ReLU convolutional neural networks (CNNs). Within thistheoretical framework, we characterize how the inherited parameters act ascarriers of universal knowledge and identify key factors that amplify theirbeneficial impact on the target task. Furthermore, our analysis providesinsight into why, in certain cases, transferring parameters can lead to lowertest accuracy on the target task than training a new model from scratch.Numerical experiments and real-world data experiments are conducted toempirically validate our theoretical findings.</description>
      <author>example@mail.com (Hua Yuan, Xuran Meng, Qiufeng Wang, Shiyu Xia, Ning Xu, Xu Yang, Jing Wang, Xin Geng, Yong Rui)</author>
      <guid isPermaLink="false">2509.22056v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Coreset selection based on Intra-class diversity</title>
      <link>http://arxiv.org/abs/2509.21380v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种智能的轻量级机制用于选择深度学习训练的代表性数据子集（corset），解决了深度学习模型训练中计算资源需求大的问题，通过提取类内多样性形成聚类进行采样，在保持模型性能的同时减少了计算负担。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型已改变医疗保健领域，特别是在生物医学图像分类方面。训练深度学习模型（从头开始或迁移学习）需要大量计算资源和时间，尤其是超参数设计空间探索需要多次训练。随着数据集增长，寻找解决方案已成为研究热点。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法选择数据集的代表性子集（corset）用于训练和超参数搜索，以减少计算需求同时保持模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种智能机制提取类内多样性，形成每个类的聚类，然后利用这些聚类进行最终采样，而非简单的随机采样。这种方法避免了随机采样对主导类的偏见和无法捕获类内多样性的问题。&lt;h4&gt;主要发现&lt;/h4&gt;在著名生物成像数据集上的分类实验表明，所提出的方案在多个性能指标上优于随机采样方法，能够有效减少计算需求同时保持模型性能。&lt;h4&gt;结论&lt;/h4&gt;通过智能选择代表性数据子集，可以在不牺牲模型性能的情况下显著减少深度学习训练所需的计算资源和时间，为处理大规模生物医学图像数据集提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型已经改变了各个领域，包括医疗保健部门，特别是在生物医学图像分类方面，通过学习复杂特征，能够对复杂疾病进行准确诊断。最近的研究采用两种不同的方法训练深度学习模型：从头开始训练和迁移学习。由于训练涉及大规模数据集，这两种方法都需要大量的计算时间和资源。由于选择最佳超参数所需的设计空间探索，这些计算需求进一步增加，这通常需要多次训练。随着数据集大小的增长，解决这一问题最近引起了研究界的关注。一个可行的解决方案是为训练和超参数搜索选择数据集的一个子集。这个子集称为corset，必须是原始数据集的代表性集合。选择corset的直接方法可能是采用随机采样，但代价是损害原始数据集的代表性。随机采样的一个关键局限性是对不平衡数据集中主导类的偏见。即使数据集具有类间平衡，这种随机采样也无法捕获类内多样性。本研究通过引入一种智能的轻量级机制来选择corset解决了这个问题。具体来说，它提出了一种提取类内多样性的方法，形成每个类的聚类用于最终采样。我们在著名的生物成像数据集上进行了广泛的分类实验，证明了所提出方法的有效性。结果表明，在相同条件下，所提出的方案在多个性能指标上优于随机采样方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Learning models have transformed various domains, including thehealthcare sector, particularly biomedical image classification by learningintricate features and enabling accurate diagnostics pertaining to complexdiseases. Recent studies have adopted two different approaches to train DLmodels: training from scratch and transfer learning. Both approaches demandsubstantial computational time and resources due to the involvement of massivedatasets in model training. These computational demands are further increaseddue to the design-space exploration required for selecting optimalhyperparameters, which typically necessitates several training rounds. With thegrowing sizes of datasets, exploring solutions to this problem has recentlygained the research community's attention. A plausible solution is to select asubset of the dataset for training and hyperparameter search. This subset,referred to as the corset, must be a representative set of the originaldataset. A straightforward approach to selecting the coreset could be employingrandom sampling, albeit at the cost of compromising the representativeness ofthe original dataset. A critical limitation of random sampling is the biastowards the dominant classes in an imbalanced dataset. Even if the dataset hasinter-class balance, this random sampling will not capture intra-classdiversity. This study addresses this issue by introducing an intelligent,lightweight mechanism for coreset selection. Specifically, it proposes a methodto extract intra-class diversity, forming per-class clusters that are utilizedfor the final sampling. We demonstrate the efficacy of the proposed methodologyby conducting extensive classification experiments on a well-known biomedicalimaging dataset. Results demonstrate that the proposed scheme outperforms therandom sampling approach on several performance metrics for uniform conditions.</description>
      <author>example@mail.com (Imran Ashraf, Mukhtar Ullah, Muhammad Faisal Nadeem, Muhammad Nouman Noor)</author>
      <guid isPermaLink="false">2509.21380v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Multilingual Vision-Language Models, A Survey</title>
      <link>http://arxiv.org/abs/2509.22123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一项关于多语言视觉-语言模型的综述研究，考察了能够处理多种语言文本和图像的模型，分析了不同架构模型的特点，并探讨了语言中立性与文化意识之间的张力。&lt;h4&gt;背景&lt;/h4&gt;多语言视觉-语言模型是一个正在发展的研究领域，涉及跨语言处理文本和图像的能力，目前存在语言中立性与文化意识之间的关键张力。&lt;h4&gt;目的&lt;/h4&gt;综述多语言视觉-语言模型，分析不同架构模型的特点，研究语言中立性与文化意识之间的张力，并评估当前训练方法与评估基准的匹配程度。&lt;h4&gt;方法&lt;/h4&gt;综述了31个模型和21个基准测试，涵盖了仅编码器和生成式架构，分析了训练方法和评估基准，识别了当前研究中的优势和不足。&lt;h4&gt;主要发现&lt;/h4&gt;当前训练方法通过对比学习倾向于语言中立性，而文化意识依赖于多样化数据；三分之二的评估基准使用基于翻译的方法优先考虑语义一致性，但最近工作开始融入基于文化的内容；跨语言能力存在差异，训练目标与评估目标之间存在差距。&lt;h4&gt;结论&lt;/h4&gt;多语言视觉-语言模型需要在保持语言中立性的同时增强文化意识，当前训练方法与评估基准之间存在不匹配，需要进一步研究以弥合这一差距。&lt;h4&gt;翻译&lt;/h4&gt;该研究强调了多语言视觉-语言模型在全球化应用中的重要性，指出模型需要在保持跨语言一致性的同时适应不同文化背景，这对开发真正具有包容性的AI系统具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This survey examines multilingual vision-language models that process textand images across languages. We review 31 models and 21 benchmarks, spanningencoder-only and generative architectures, and identify a key tension betweenlanguage neutrality (consistent cross-lingual representations) and culturalawareness (adaptation to cultural contexts). Current training methods favorneutrality through contrastive learning, while cultural awareness depends ondiverse data. Two-thirds of evaluation benchmarks use translation-basedapproaches prioritizing semantic consistency, though recent work incorporatesculturally grounded content. We find discrepancies in cross-lingualcapabilities and gaps between training objectives and evaluation goals.</description>
      <author>example@mail.com (Andrei-Alexandru Manea, Jindřich Libovický)</author>
      <guid isPermaLink="false">2509.22123v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Enriching Knowledge Distillation with Intra-Class Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.22053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进知识蒸馏的方法，通过在教师模型训练中引入类内对比损失来丰富软标签中的类内信息，并解决了训练不稳定和收敛慢的问题。&lt;h4&gt;背景&lt;/h4&gt;自知识蒸馏出现以来，研究重点在于如何有效利用教师模型生成的软标签。现有研究表明，软标签中的隐式知识源于数据中的多视图结构，同类样本内的特征变化有助于学生模型学习多样化表示并提高泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有蒸馏方法中教师模型主要遵循真实标签而忽视同一类内多样化表示的问题，通过丰富软标签中的类内信息来提升知识蒸馏效果。&lt;h4&gt;方法&lt;/h4&gt;在教师训练过程中引入类内对比损失，并将边界损失整合到类内对比学习中以提高训练稳定性和收敛速度。同时理论上分析了该损失对类内距离和类间距离的影响。&lt;h4&gt;主要发现&lt;/h4&gt;类内对比损失能够有效丰富类内多样性，实验结果证明了所提出方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过改进教师模型训练过程，引入类内对比损失和边界损失，可以显著提升知识蒸馏的效果，使学生模型能够更好地学习多样化表示。&lt;h4&gt;翻译&lt;/h4&gt;自从知识蒸馏出现以来，许多研究都集中在如何有效利用教师模型生成的软标签。现有研究表明，软标签中的隐式知识源于数据中存在的多视图结构。同类样本内的特征变化使学生模型能够通过学习多样化表示来更好地泛化。然而，在现有的蒸馏方法中，教师模型主要遵循真实标签作为目标，而没有考虑同一类内的多样化表示。因此，我们提出在教师训练过程中加入类内对比损失，以丰富软标签中包含的类内信息。在实践中，我们发现类内损失会导致训练不稳定并减慢收敛速度。为了缓解这些问题，将边界损失整合到类内对比学习中以提高训练稳定性和收敛速度。同时，我们理论上分析了这种损失对类内距离和类间距离的影响。已经证明类内对比损失可以丰富类内多样性。实验结果证明了所提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since the advent of knowledge distillation, much research has focused on howthe soft labels generated by the teacher model can be utilized effectively.Existing studies points out that the implicit knowledge within soft labelsoriginates from the multi-view structure present in the data. Featurevariations within samples of the same class allow the student model togeneralize better by learning diverse representations. However, in existingdistillation methods, teacher models predominantly adhere to ground-truthlabels as targets, without considering the diverse representations within thesame class. Therefore, we propose incorporating an intra-class contrastive lossduring teacher training to enrich the intra-class information contained in softlabels. In practice, we find that intra-class loss causes instability intraining and slows convergence. To mitigate these issues, margin loss isintegrated into intra-class contrastive learning to improve the trainingstability and convergence speed. Simultaneously, we theoretically analyze theimpact of this loss on the intra-class distances and inter-class distances. Ithas been proved that the intra-class contrastive loss can enrich theintra-class diversity. Experimental results demonstrate the effectiveness ofthe proposed method.</description>
      <author>example@mail.com (Hua Yuan, Ning Xu, Xin Geng, Yong Rui)</author>
      <guid isPermaLink="false">2509.22053v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>GRAM-TDI: adaptive multimodal representation learning for drug target interaction prediction</title>
      <link>http://arxiv.org/abs/2509.21971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GRAMDTI的药物靶点相互作用(DTI)预测预训练框架，通过整合多模态分子和蛋白质输入，实现了超越现有方法的性能表现。&lt;h4&gt;背景&lt;/h4&gt;药物靶点相互作用(DTI)预测是计算药物发现的核心，能够促进合理药物设计、药物重定位和机制洞察。然而，现有的深度学习方法主要依赖SMILES蛋白对，未能充分利用小分子和蛋白质的丰富多模态信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够整合多模态分子和蛋白质输入的预训练框架，提高DTI预测的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;GRAMDTI框架将基于体积的对比学习扩展到四种模态，捕获高阶语义对齐；提出自适应模态丢弃机制，动态调节各模态在预训练中的贡献；整合IC50活性测量作为弱监督，将表示锚定在生物学上有意义的相互作用强度上。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公开数据集上的实验表明，GRAMDTI框架在DTI预测任务中始终优于最先进的基线方法，证明了高阶多模态对齐、自适应模态利用和辅助监督的有效性。&lt;h4&gt;结论&lt;/h4&gt;高阶多模态对齐、自适应模态利用和辅助监督对于构建稳健和可推广的DTI预测模型至关重要，为药物发现提供了新的计算方法。&lt;h4&gt;翻译&lt;/h4&gt;药物靶点相互作用(DTI)预测是计算药物发现的基础，能够促进合理设计、药物重定位和机制洞察。虽然深度学习已推动DTI建模发展，但现有方法主要依赖SMILES蛋白对，未能充分利用小分子和蛋白质的丰富多模态信息。我们引入了GRAMDTI，一个将多模态分子和蛋白质输入整合为统一表示的预训练框架。GRAMDTI将基于体积的对比学习扩展到四种模态，捕获超越传统成对方法的高阶语义对齐。为处理模态信息量，我们提出自适应模态丢弃，在预训练过程中动态调节各模态的贡献。此外，当可用时，IC50活性测量被整合为弱监督，将表示锚定在生物学上有意义的相互作用强度上。在四个公开数据集上的实验表明，GRAMDTI始终优于最先进的基线方法。我们的结果突显了高阶多模态对齐、自适应模态利用和辅助监督对稳健和可推广的DTI预测的益处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug target interaction (DTI) prediction is a cornerstone of computationaldrug discovery, enabling rational design, repurposing, and mechanisticinsights. While deep learning has advanced DTI modeling, existing approachesprimarily rely on SMILES protein pairs and fail to exploit the rich multimodalinformation available for small molecules and proteins. We introduce GRAMDTI, apretraining framework that integrates multimodal molecular and protein inputsinto unified representations. GRAMDTI extends volume based contrastive learningto four modalities, capturing higher-order semantic alignment beyondconventional pairwise approaches. To handle modality informativeness, wepropose adaptive modality dropout, dynamically regulating each modality'scontribution during pre-training. Additionally, IC50 activity measurements,when available, are incorporated as weak supervision to ground representationsin biologically meaningful interaction strengths. Experiments on four publiclyavailable datasets demonstrate that GRAMDTI consistently outperforms state ofthe art baselines. Our results highlight the benefits of higher ordermultimodal alignment, adaptive modality utilization, and auxiliary supervisionfor robust and generalizable DTI prediction.</description>
      <author>example@mail.com (Feng Jiang, Amina Mollaysa, Hehuan Ma, Tommaso Mansi, Junzhou Huang, Mangal Prakash, Rui Liao)</author>
      <guid isPermaLink="false">2509.21971v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Vehicle Detection under Adverse Weather Conditions with Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.21916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种sideload-CL-adaptation框架，利用未标注数据改进北欧地区无人机图像中的车辆检测性能。&lt;h4&gt;背景&lt;/h4&gt;北欧地区从无人机图像检测车辆面临特殊挑战，包括强能见度问题和不同雪覆盖水平引起的域偏移。此外，遥感中的常见挑战如小目标、稀疏目标和计算成本限制也同时存在。标注数据获取成本高，而未标注数据通过无人机飞行可更经济地获取。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用未标注数据提高车辆检测性能的框架，同时使用轻量级模型以满足计算成本限制。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于CNN的表示提取器，通过在未标注数据上进行对比学习进行预训练，然后在微调阶段将预训练的表示提取器sideload到冻结的YOLO11n骨干网络中。通过广泛实验比较各种融合方法和粒度，以找到最优的sideload-CL-adaptation方案。&lt;h4&gt;主要发现&lt;/h4&gt;提出的sideload-CL-adaptation模型在NVD数据集上将检测性能提高了3.8%至9.5%（以mAP50衡量），证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该框架成功利用易获取的未标注数据显著提升了北欧地区复杂环境下无人机图像的车辆检测性能。&lt;h4&gt;翻译&lt;/h4&gt;除了遥感中的常见挑战，如小目标、稀疏目标和计算成本限制外，从北欧地区的无人机图像中检测车辆还面临强能见度挑战和由不同雪覆盖水平引起的域偏移。虽然标注数据昂贵，但未标注数据可以通过简单飞行无人机更便宜地获取。在这项工作中，我们提出了一个sideload-CL-adaptation框架，使未标注数据能够用于改进使用轻量级模型的车辆检测。具体而言，我们建议在预训练阶段通过未标注数据上的对比学习训练一个基于CNN的表示提取器，然后在微调阶段将其sideload到冻结的YOLO11n骨干网络中。为了找到稳健的sideload-CL-adaptation，我们进行了广泛实验以比较各种融合方法和粒度。我们提出的sideload-CL-adaptation模型在NVD数据集上将检测性能提高了3.8%至9.5%（以mAP50衡量）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aside from common challenges in remote sensing like small, sparse targets andcomputation cost limitations, detecting vehicles from UAV images in the Nordicregions faces strong visibility challenges and domain shifts caused by diverselevels of snow coverage. Although annotated data are expensive, unannotateddata is cheaper to obtain by simply flying the drones. In this work, weproposed a sideload-CL-adaptation framework that enables the use of unannotateddata to improve vehicle detection using lightweight models. Specifically, wepropose to train a CNN-based representation extractor through contrastivelearning on the unannotated data in the pretraining stage, and then sideload itto a frozen YOLO11n backbone in the fine-tuning stage. To find a robustsideload-CL-adaptation, we conducted extensive experiments to compare variousfusion methods and granularity. Our proposed sideload-CL-adaptation modelimproves the detection performance by 3.8% to 9.5% in terms of mAP50 on the NVDdataset.</description>
      <author>example@mail.com (Boying Li, Chang Liu, Petter Kyösti, Mattias Öhman, Devashish Singha Roy, Sofia Plazzi, Hamam Mokayed, Olle Hagner)</author>
      <guid isPermaLink="false">2509.21916v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>EMG-UP: Unsupervised Personalization in Cross-User EMG Gesture Recognition</title>
      <link>http://arxiv.org/abs/2509.21589v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为EMG-UP的新型无监督个性化框架，用于解决跨用户肌电手势识别中的泛化问题，通过两阶段自适应策略实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;跨用户肌电(EMG)手势识别是实现可扩展和个性化人机交互的基本挑战。现有方法由于EMG信号的内在生物变异性（源于解剖异质性和多样的任务执行风格）而难以有效泛化到不同用户。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效解决跨用户手势识别中泛化问题的无监督个性化框架。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段自适应策略：(1)序列交叉视角对比学习，通过捕捉对用户间变化不变的内在信号模式，解耦鲁棒和用户特定的特征表示；(2)伪标签引导微调，使模型能够针对单个用户进行优化，无需访问源域数据。&lt;h4&gt;主要发现&lt;/h4&gt;大量评估显示，EMG-UP达到了最先进的性能，在准确率上比先前方法至少高出2.0%。&lt;h4&gt;结论&lt;/h4&gt;EMG-UP框架有效地解决了跨用户EMG手势识别中的泛化问题，为可扩展和个性化人机交互提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;跨用户肌电(EMG)手势识别代表了在现实应用中实现可扩展和个性化人机交互的基本挑战。尽管付出了大量努力，但由于EMG信号的内在生物变异性（源于解剖异质性和多样的任务执行风格），现有方法难以有效跨用户泛化。为解决这一限制，我们引入了EMG-UP，一种用于跨用户手势识别中无监督个性化的新颖有效框架。所提出的框架利用两阶段自适应策略：(1)序列交叉视角对比学习，旨在通过捕捉对用户间变化不变的内在信号模式，解耦鲁健和用户特定的特征表示；(2)伪标签引导微调，使模型能够在无需访问源域数据的情况下针对单个用户进行优化。大量评估表明，EMG-UP实现了最先进的性能，在准确率上至少比先前方法高出2.0%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-user electromyography (EMG)-based gesture recognition represents afundamental challenge in achieving scalable and personalized human-machineinteraction within real-world applications. Despite extensive efforts, existingmethodologies struggle to generalize effectively across users due to theintrinsic biological variability of EMG signals, resulting from anatomicalheterogeneity and diverse task execution styles. To address this limitation, weintroduce EMG-UP, a novel and effective framework for UnsupervisedPersonalization in cross-user gesture recognition. The proposed frameworkleverages a two-stage adaptation strategy: (1) Sequence-Cross PerspectiveContrastive Learning, designed to disentangle robust and user-specific featurerepresentations by capturing intrinsic signal patterns invariant to inter-uservariability, and (2) Pseudo-Label-Guided Fine-Tuning, which enables modelrefinement for individual users without necessitating access to source domaindata. Extensive evaluations show that EMG-UP achieves state-of-the-artperformance, outperforming prior methods by at least 2.0% in accuracy.</description>
      <author>example@mail.com (Nana Wang, Gen Li, Zhaoxin Fan, Suli Wang)</author>
      <guid isPermaLink="false">2509.21589v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms</title>
      <link>http://arxiv.org/abs/2509.21573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新颖的空间正则化对比学习策略，通过结合半变异函数来解决基于图像的全球地理定位中的挑战，特别是假阴性和困难负样本问题。&lt;h4&gt;背景&lt;/h4&gt;基于图像的全球地理定位面临环境多样性、视觉模糊场景和许多地区缺乏明显地标的挑战，导致定位困难。&lt;h4&gt;目的&lt;/h4&gt;解决对比学习方法中忽略地理空间潜在空间依赖性的问题，以及由此产生的假阴性和困难负样本难以区分的问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种空间正则化对比学习策略，结合半变异函数建模空间相关性随距离的变化，通过将特征空间中图像距离与地理距离关联来拟合半变异函数，并基于此识别困难负样本和假阴性样本。&lt;h4&gt;主要发现&lt;/h4&gt;明确建模空间先验知识可以提高基于图像的地理定位性能，特别是在更细粒度的层面上。&lt;h4&gt;结论&lt;/h4&gt;将提出的空间正则化策略集成到GeoCLIP中并在OSV5M数据集上评估，证实了该策略的有效性。&lt;h4&gt;翻译&lt;/h4&gt;准确的全球规模图像地理定位具有挑战性，原因包括环境多样、视觉模糊场景以及许多地区缺乏明显地标。虽然对比学习方法通过将街景图像与相应位置的特征对齐显示出良好的性能，但它们忽略了地理空间中的潜在空间依赖性。因此，它们无法解决假阴性问题——即视觉和地理相似但被标记为负面的图像对，并且难以有效区分困难负样本——即视觉相似但地理距离远的样本。为解决这一问题，我们提出了一种新颖的空间正则化对比学习策略，结合了半变异函数，这是一种用于建模空间相关性如何随距离变化的地理统计工具。我们通过将特征空间中图像的距离与其地理距离相关联来拟合半变异函数，捕捉空间相关性中的预期视觉内容。利用拟合的半变异函数，我们将给定空间距离处的预期视觉差异定义为参考，以识别困难负样本和假阴性样本。我们将此策略集成到GeoCLIP中并在OSV5M数据集上评估，证明明确建模空间先验知识可以提高基于图像的地理定位性能，特别是在更细粒度的层面上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and robust image-based geo-localization at a global scale ischallenging due to diverse environments, visually ambiguous scenes, and thelack of distinctive landmarks in many regions. While contrastive learningmethods show promising performance by aligning features between street-viewimages and corresponding locations, they neglect the underlying spatialdependency in the geographic space. As a result, they fail to address the issueof false negatives -- image pairs that are both visually and geographicallysimilar but labeled as negatives, and struggle to effectively distinguish hardnegatives, which are visually similar but geographically distant. To addressthis issue, we propose a novel spatially regularized contrastive learningstrategy that integrates a semivariogram, which is a geostatistical tool formodeling how spatial correlation changes with distance. We fit thesemivariogram by relating the distance of images in feature space to theirgeographical distance, capturing the expected visual content in a spatialcorrelation. With the fitted semivariogram, we define the expected visualdissimilarity at a given spatial distance as reference to identify hardnegatives and false negatives. We integrate this strategy into GeoCLIP andevaluate it on the OSV5M dataset, demonstrating that explicitly modelingspatial priors improves image-based geo-localization performance, particularlyat finer granularity.</description>
      <author>example@mail.com (Boyi Chen, Zhangyu Wang, Fabian Deuser, Johann Maximilian Zollner, Martin Werner)</author>
      <guid isPermaLink="false">2509.21573v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations</title>
      <link>http://arxiv.org/abs/2509.21511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint. 9 pages main manuscript, 23 pages with appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了对比互信息机(cMIM)，一种结合了对比学习和互信息机(MIM)的表示学习框架，旨在同时提升判别式和生成式任务的表现。&lt;h4&gt;背景&lt;/h4&gt;表示学习中的核心挑战是学习能够良好迁移到多样化下游任务的表示。现有范式包括对比学习、自监督掩码和去噪自编码器，它们在应对这一挑战时存在不同权衡。互信息机(MIM)最大化输入和潜在表示之间的互信息并促进代码聚类，但在判别式任务上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时保留MIM的生成保真度并引入全局判别结构的框架，从而在判别式和生成式任务上都取得良好表现。&lt;h4&gt;方法&lt;/h4&gt;提出cMIM(对比互信息机)，这是MIM的对比扩展，结合了对比目标和MIM的互信息最大化方法。同时引入'信息丰富嵌入'技术，用于从编码器-解码器模型中提取增强特征，提高判别性能而无需额外训练。&lt;h4&gt;主要发现&lt;/h4&gt;1. cMIM不需要正面数据增强，并且对批次大小的敏感性显著低于InfoNCE；2. '信息丰富嵌入'技术可以提升判别性能，无需额外训练，并且可以广泛应用于MIM以外的模型；3. 在视觉和分子基准测试中，cMIM在分类和回归任务上均优于MIM和InfoNCE，同时保持了竞争性的重建质量。&lt;h4&gt;结论&lt;/h4&gt;cMIM被定位为一个统一的表示学习框架，推进了模型能够有效服务于判别式和生成式应用的目标。&lt;h4&gt;翻译&lt;/h4&gt;学习能够良好迁移到多样化下游任务的表示仍然是表示学习中的一个核心挑战。现有范式——对比学习、自监督掩码和去噪自编码器——通过不同的权衡来应对这一挑战。我们引入了对比互信息机(cMIM)，这是一个将互信息机(MIM)与对比目标相结合的概率框架。虽然MIM最大化输入和潜在表示之间的互信息并促进代码聚类，但在判别式任务上表现不佳。cMIM通过引入全局判别结构同时保留MIM的生成保真度来解决这一差距。我们的贡献有三方面。首先，我们提出了cMIM，这是MIM的对比扩展，它不需要正面数据增强，并且对批次大小的敏感性显著低于InfoNCE。其次，我们引入了'信息丰富嵌入'，这是一种从编码器-解码器模型中提取增强特征的通用技术，可以在无需额外训练的情况下提高判别性能，并且可以广泛应用于MIM以外的领域。第三，我们在视觉和分子基准测试中提供了经验证据，表明cMIM在分类和回归任务上均优于MIM和InfoNCE，同时保持了竞争性的重建质量。这些结果将cMIM定位为表示学习的统一框架，推进了模型能够有效服务于判别式和生成式应用的目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning representations that transfer well to diverse downstream tasksremains a central challenge in representation learning. Existing paradigms --contrastive learning, self-supervised masking, and denoising auto-encoders --balance this challenge with different trade-offs. We introduce the {contrastiveMutual Information Machine} (cMIM), a probabilistic framework that extends theMutual Information Machine (MIM) with a contrastive objective. While MIMmaximizes mutual information between inputs and latents and promotes clusteringof codes, it falls short on discriminative tasks. cMIM addresses this gap byimposing global discriminative structure while retaining MIM's generativefidelity. Our contributions are threefold. First, we propose cMIM, acontrastive extension of MIM that removes the need for positive dataaugmentation and is substantially less sensitive to batch size than InfoNCE.Second, we introduce {informative embeddings}, a general technique forextracting enriched features from encoder-decoder models that boostsdiscriminative performance without additional training and applies broadlybeyond MIM. Third, we provide empirical evidence across vision and molecularbenchmarks showing that cMIM outperforms MIM and InfoNCE on classification andregression tasks while preserving competitive reconstruction quality. Theseresults position cMIM as a unified framework for representation learning,advancing the goal of models that serve both discriminative and generativeapplications effectively.</description>
      <author>example@mail.com (Micha Livne)</author>
      <guid isPermaLink="false">2509.21511v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations</title>
      <link>http://arxiv.org/abs/2509.20048v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为DACL的新型混合框架，结合扩散模型和监督对比学习，用于生物信号表示学习，通过扩散过程生成噪声视图并学习鲁棒表示，在ECG数据集上取得了0.7815的竞争性AUROC值。&lt;h4&gt;背景&lt;/h4&gt;学习生物信号的鲁棒表示通常面临设计有效数据增强的挑战，传统方法无法捕捉生理数据中固有的复杂变化。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的混合框架，解决传统数据增强方法在生理数据上的局限性，学习对噪声具有鲁棒性的生物信号表示。&lt;h4&gt;方法&lt;/h4&gt;提出名为Diffusion-Augmented Contrastive Learning (DACL)的混合框架，融合扩散模型和监督对比学习概念；在基于Scattering Transformer特征训练的轻量级变分自编码器创建的潜在空间上运行；利用扩散前向过程作为数据增强技术生成噪声视图；使用监督对比目标训练U-Net风格编码器学习跨不同扩散时间步骤的鲁棒表示。&lt;h4&gt;主要发现&lt;/h4&gt;在PhysioNet 2017 ECG数据集上评估该方法，达到了0.7815的竞争性AUROC值。&lt;h4&gt;结论&lt;/h4&gt;通过使用扩散过程本身驱动对比目标，为表示学习建立了新范式，创建了对噪声不变的嵌入，显示出良好的类别可分离性基础。&lt;h4&gt;翻译&lt;/h4&gt;学习生物信号的鲁棒表示通常面临设计有效数据增强的挑战。传统方法可能无法捕捉生理数据中固有的复杂变化。在此背景下，我们提出了一种新的混合框架Diffusion-Augmented Contrastive Learning (DACL)，该框架融合了扩散模型和监督对比学习的概念。DACL框架在我们新颖的Scattering Transformer (ST)特征[12]训练的轻量级变分自编码器(VAE)创建的潜在空间上运行。它利用扩散前向过程作为数据增强技术，生成这些潜在嵌入的多个噪声视图。然后使用监督对比目标训练U-Net风格的编码器，学习一个在不同扩散时间步骤上对噪声具有鲁棒性的表示。我们在PhysioNet 2017 ECG数据集上评估了这个概念验证方法，取得了0.7815的竞争性AUROC值。这项工作通过使用扩散过程本身来驱动对比目标，为表示学习建立了新范式，创建了对噪声不变的嵌入，显示出良好的类别可分离性基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning robust representations for biosignals is often hampered by thechallenge of designing effective data augmentations.Traditional methods canfail to capture the complex variations inherent in physiological data. Withinthis context, we propose a novel hybrid framework, Diffusion-AugmentedContrastive Learning (DACL), that fuses concepts from diffusion models andsupervised contrastive learning. The DACL framework operates on a latent spacecreated by a lightweight Variational Autoencoder (VAE) trained on our novelScattering Transformer (ST) features [12]. It utilizes the diffusion forwardprocess as a principled data augmentation technique to generate multiple noisyviews of these latent embeddings. A U-Net style encoder is then trained with asupervised contrastive objective to learn a representation that balances classdiscrimination with robustness to noise across various diffusion time steps. Weevaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset,achieving a competitive AUROC of 0.7815. This work establishes a new paradigmfor representation learning by using the diffusion process itself to drive thecontrastive objective, creating noise-invariant embeddings that demonstrate astrong foundation for class separability.</description>
      <author>example@mail.com (Rami Zewail)</author>
      <guid isPermaLink="false">2509.20048v2</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Transport Based Mean Flows for Generative Modeling</title>
      <link>http://arxiv.org/abs/2509.22592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过整合基于最优传输的采样策略到Mean Flow框架中，解决了Mean Flows在连续数据生成中无法忠实近似原始多步flow-matching过程的问题，实现了在保持生成性能的同时显著提高推理速度的单步生成模型。&lt;h4&gt;背景&lt;/h4&gt;Flow-matching生成模型已成为连续数据生成的强大范式，在图像、3D形状和点云等领域取得了最先进的结果。然而，这些模型由于需要大量顺序采样步骤而面临推理速度慢的问题。最近的工作试图通过减少采样步骤数量来加速推理，其中Mean Flows提供了一种单步生成方法，在保持强大生成性能的同时带来了显著的速度提升。&lt;h4&gt;目的&lt;/h4&gt;解决Mean Flows在许多连续领域中无法忠实地近似原始多步flow-matching过程的行为，开发能够更好保留原始多步flow过程保真度和多样性的单步生成器。&lt;h4&gt;方法&lt;/h4&gt;将基于最优传输(optimal transport)的采样策略整合到Mean Flow框架中，创建能够更好保留原始多步flow过程保真度和多样性的单步生成器。&lt;h4&gt;主要发现&lt;/h4&gt;在受控的低维设置以及图像生成、图像到图像转换和点云生成等高维任务上的实验表明，该方法在单步生成建模中实现了卓越的推理准确性，优于现有的Mean Flows方法。&lt;h4&gt;结论&lt;/h4&gt;通过整合基于最优传输的采样策略，Mean Flow框架能够生成更忠实于原始多步过程的单步生成器，解决了flow-matching模型的主要瓶颈，在保持生成性能的同时显著提高了推理速度。&lt;h4&gt;翻译&lt;/h4&gt;Flow-matching生成模型已成为连续数据生成的强大范式，在图像、3D形状和点云等领域取得了最先进的结果。尽管取得了成功，但这些模型由于需要大量顺序采样步骤而面临推理速度慢的问题。最近的工作试图通过减少采样步骤数量来加速推理。特别是，Mean Flows提供了一种单步生成方法，在保持强大生成性能的同时带来了显著的速度提升。然而，在许多连续领域中，Mean Flows无法忠实地近似原始多步flow-matching过程的行为。在本工作中，我们通过将基于最优传输的采样策略整合到Mean Flow框架中，解决了这一局限性，使单步生成器能够更好地保留原始多步flow过程的保真度和多样性。在受控的低维设置以及图像生成、图像到图像转换和点云生成等高维任务上的实验表明，我们的方法在单步生成建模中实现了卓越的推理准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决Flow-matching生成模型推理速度慢的问题。传统Flow-matching模型需要大量顺序采样步骤，限制了实际应用。虽然MeanFlow方法提供了一步生成方案，但在许多连续领域中无法很好地近似原始多步过程。这个问题很重要，因为生成模型在图像、3D形状和点云等领域有广泛应用，提高推理速度同时保持生成质量是推动这些模型实际部署的关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到MeanFlow方法虽能实现快速推理，但在连续数据中无法很好地保留原始多步Flow-matching的行为。他们思考如何结合最优传输(OT)的轨迹拉直原则与MeanFlow的时间平均公式，以创建更高效的一步生成轨迹。该方法借鉴了多个现有工作：Flow Matching和Diffusion Models两种生成框架、最优传输理论(特别是Benamou-Brenier动态公式)、MeanFlow的平均速度场学习，以及Mini-batch OT Flow Matching的耦合策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将最优传输的轨迹拉直原则与MeanFlow的时间平均公式相结合，产生更直效的一步生成轨迹，通过OT基础的耦合与mean-flow监督相结合，实现几何感知和高效的一步生成。实现流程分为训练和推理两阶段：训练时，从源和目标分布采样小批次数据，计算OT计划，采样时间点，计算中间点和目标速度，计算目标平均速度场，通过最小化学习速度场与目标速度场的差异来更新参数；推理时，直接使用学习到的平均速度场一步生成目标数据，x1 ≈ x0 + u1,0(x0)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一框架，将条件flow matching、mini-batch OT flow matching和mean flow方法统一于OT-MF下；2) 改进的效率和准确性，在点云生成和图像转换中保留一步生成能力同时提高质量；3) 可扩展训练，集成线性OT和分层OT等加速方法。相比之前工作，OT-MF通过OT策略实现更直效轨迹提高生成质量，相比MeanFlow更好地保留原始多步过程的行为，相比传统Flow Matching大幅提升推理速度，同时引入近似OT变体在保持性能的同时提高计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出OT-MF方法，结合最优传输与平均流实现一步高质量生成，在保持推理速度的同时显著提升了生成模型的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flow-matching generative models have emerged as a powerful paradigm forcontinuous data generation, achieving state-of-the-art results across domainssuch as images, 3D shapes, and point clouds. Despite their success, thesemodels suffer from slow inference due to the requirement of numerous sequentialsampling steps. Recent work has sought to accelerate inference by reducing thenumber of sampling steps. In particular, Mean Flows offer a one-step generationapproach that delivers substantial speedups while retaining strong generativeperformance. Yet, in many continuous domains, Mean Flows fail to faithfullyapproximate the behavior of the original multi-step flow-matching process. Inthis work, we address this limitation by incorporating optimal transport-basedsampling strategies into the Mean Flow framework, enabling one-step generatorsthat better preserve the fidelity and diversity of the original multi-step flowprocess. Experiments on controlled low-dimensional settings and onhigh-dimensional tasks such as image generation, image-to-image translation,and point cloud generation demonstrate that our approach achieves superiorinference accuracy in one-step generative modeling.</description>
      <author>example@mail.com (Elaheh Akbari, Ping He, Ahmadreza Moradipari, Yikun Bai, Soheil Kolouri)</author>
      <guid isPermaLink="false">2509.22592v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>The Flood Complex: Large-Scale Persistent Homology on Millions of Points</title>
      <link>http://arxiv.org/abs/2509.22432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Flood复形的新方法，用于计算大规模欧几里得点云数据的持续同调(PH)，解决了传统方法在计算效率和可扩展性方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;计算大规模点云数据的持续同调面临严重计算限制，因为常用的Vietoris-Rips复形会呈指数增长。虽然Alpha复形和稀疏Rips近似等替代方法存在，但它们仍会产生大量单纯形，限制了在大规模点云上的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展的方法，能够高效计算大规模点云数据的持续同调，特别是在下游机器学习任务中的应用。&lt;h4&gt;方法&lt;/h4&gt;引入Flood复形，受Alpha复形和Witness复形的优势启发。在给定过滤值r时，Flood复形包含点云小子集的Delaunay三角剖分中被半径为r的球体完全覆盖的所有单纯形，这一过程称为'flooding'。&lt;h4&gt;主要发现&lt;/h4&gt;Flood复形允许高效的PH计算，具有理想的理论性质，适合GPU并行化。实验表明，该方法可在数百万个3D点上计算到2维的PH。在对象分类任务中，对于几何或拓扑复杂对象，该方法性能优于其他基于PH的方法和用于点云数据的神经网络。&lt;h4&gt;结论&lt;/h4&gt;Flood复形为大规模点云数据的持续同调计算提供了一种有效解决方案，其扩展能力对于处理复杂几何或拓扑对象至关重要，在机器学习应用中展现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;我们考虑为大规模欧几里得点云数据计算持续同调(PH)的问题，旨在用于下游机器学习任务，其中最广泛使用的Vietoris-Rips复形的指数增长带来了严重的计算限制。虽然存在更可扩展的替代方法，如Alpha复形或稀疏Rips近似，但它们通常仍会产生大量单纯形，这给复形的构建和随后的PH计算带来了挑战，限制了它们在大规模点云上的使用。为缓解这些问题，我们引入了Flood复形，受Alpha复形和Witness复形构建的优势启发。非正式地说，在给定的过滤值r≥0时，Flood复形包含点云X的一个小子集的Delaunay三角剖分中的所有单纯形，这些单纯形完全被从X发出的半径为r的球体覆盖，这个过程我们称为flooding。我们的构造允许高效的PH计算，具有几种理想的理论性质，并且适合GPU并行化。在3D点云数据上的扩展实验表明，我们可以在数百万个点上计算到2维的PH。重要的是，在真实世界和合成数据上评估对象分类性能时，我们提供了证据表明这种扩展能力是必要的，特别是当对象在几何或拓扑上复杂时，性能优于其他基于PH的方法和用于点云数据的神经网络。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决计算大规模欧几里得点云数据的持久同调(Persistent Homology, PH)时面临的计算挑战。这个问题很重要，因为持久同调是拓扑数据分析的核心工具，能揭示数据的拓扑和几何特性；随着数据规模增大，传统方法变得不可行；许多机器学习应用(如图分类、时间序列预测)依赖于PH提取特征；大规模点云数据在科学计算(如冷冻电镜图像)、3D扫描等领域很常见。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：Vietoris-Rips复形计算复杂度高；Alpha复形对大规模点云仍效率不足；子采样会丢失小拓扑特征；Witness复形构造过程脆弱。作者借鉴了Alpha复形(基于Delaunay三角剖分)和Witness复形(使用小地标集)的优点，设计出Flood复形：在给定过滤值r时，包含从点云小子集L的Delaunay三角剖分中所有被半径为r的球完全覆盖的单纯形，结合了两者的优势同时避免了它们的缺点。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：使用小地标点集构建Delaunay三角剖分，为每个单纯形计算被整个点云覆盖所需的最小半径作为过滤值。实现流程：1)选择地标点集(通常用最远点采样)；2)计算地标集的Delaunay三角剖分；3)对每个单纯形：计算包围球、创建掩码、在单纯形上离散采样点、计算过滤值；4)构建过滤的Flood复形；5)计算持久同调；6)将持久图转换为向量表示用于下游任务。作者通过GPU并行化和掩码技术提高效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)Flood复形构造结合了Alpha和Witness复形的优点；2)提供了近似质量的稳定性和理论保证；3)设计了高效的GPU并行算法；4)能够处理数百万点的点云。相比之前工作的不同：与子采样相比保留了小拓扑特征；与Witness复形相比更稳定且不需精细控制距离截止值；与Alpha复形相比使用更小的地标集效率更高；与稀疏Rips相比有更好的理论保证且更适合GPU并行化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Flood复形通过结合Alpha复形的拓扑特性和Witness复形的数据驱动方法，实现了对大规模点云数据的高效持久同调计算，在保持拓扑信息完整性的同时显著提升了计算效率，为拓扑数据分析在机器学习等领域的应用提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the problem of computing persistent homology (PH) for large-scaleEuclidean point cloud data, aimed at downstream machine learning tasks, wherethe exponential growth of the most widely-used Vietoris-Rips complex imposesserious computational limitations. Although more scalable alternatives such asthe Alpha complex or sparse Rips approximations exist, they often still resultin a prohibitively large number of simplices. This poses challenges in thecomplex construction and in the subsequent PH computation, prohibiting theiruse on large-scale point clouds. To mitigate these issues, we introduce theFlood complex, inspired by the advantages of the Alpha and Witness complexconstructions. Informally, at a given filtration value $r\geq 0$, the Floodcomplex contains all simplices from a Delaunay triangulation of a small subsetof the point cloud $X$ that are fully covered by balls of radius $r$ emanatingfrom $X$, a process we call flooding. Our construction allows for efficient PHcomputation, possesses several desirable theoretical properties, and isamenable to GPU parallelization. Scaling experiments on 3D point cloud datashow that we can compute PH of up to dimension 2 on several millions of points.Importantly, when evaluating object classification performance on real-worldand synthetic data, we provide evidence that this scaling capability is needed,especially if objects are geometrically or topologically complex, yieldingperformance superior to other PH-based methods and neural networks for pointcloud data.</description>
      <author>example@mail.com (Florian Graf, Paolo Pellizzoni, Martin Uray, Stefan Huber, Roland Kwitt)</author>
      <guid isPermaLink="false">2509.22432v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Wavelet-Induced Rotary Encodings: RoPE Meets Graphs</title>
      <link>http://arxiv.org/abs/2509.22259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为WIRE（小波诱导的旋转编码）的新方法，它扩展了在大型语言模型和视觉Transformer中流行的RoPE算法，使其能够应用于图结构数据。&lt;h4&gt;背景&lt;/h4&gt;RoPE（RotaryPosition Encodings）是一种在大型语言模型和视觉Transformer中广泛使用的算法，但主要针对序列数据设计，难以直接应用于图结构数据。&lt;h4&gt;目的&lt;/h4&gt;扩展RoPE算法，使其能够处理图结构数据，同时保持或提高其在各种任务中的性能，特别是在底层图结构重要的场景中。&lt;h4&gt;方法&lt;/h4&gt;作者提出了WIRE（Wavelet-Induced Rotary Encodings）方法，这是一种基于小波的旋转编码方法，专门设计用于图结构数据。WIRE是RoPE的泛化，在网格图的特例中可以恢复RoPE。&lt;h4&gt;主要发现&lt;/h4&gt;WIRE具有多种理想的理论特性，包括节点排序置换下的等变性、与线性注意力机制的兼容性，以及在特定假设下对图电阻距离的渐近依赖性。在多种合成和真实世界任务中，WIRE被证明是有效的，特别是在底层图结构重要的场景中。&lt;h4&gt;结论&lt;/h4&gt;WIRE是一种有效的图结构数据处理方法，它成功地将RoPE扩展到图数据领域，并在多种任务中展现出良好的性能，特别是在需要考虑图结构信息的应用场景中。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍WIRE：小波诱导的旋转编码。WIRE将RotaryPosition Encodings（RoPE）——一种在大型语言模型和视觉Transformer中流行的算法——扩展到图结构数据。我们证明WIRE比RoPE更通用，在网格图的特例中可以恢复后者。WIRE还具有许多理想的理论特性，包括在节点排序置换下的等变性、与线性注意力的兼容性，以及（在特定假设下）对图电阻距离的渐近依赖性。我们在一系列合成和真实世界任务上测试了WIRE，包括识别单色子图、点云语义分割以及更标准的图基准测试。我们发现，在底层图结构重要的场景中，WIRE是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce WIRE: Wavelet-Induced Rotary Encodings. WIRE extends RotaryPosition Encodings (RoPE), a popular algorithm in LLMs and ViTs, tograph-structured data. We demonstrate that WIRE is more general than RoPE,recovering the latter in the special case of grid graphs. WIRE also enjoys ahost of desirable theoretical properties, including equivariance under nodeordering permutation, compatibility with linear attention, and (under selectassumptions) asymptotic dependence on graph resistive distance. We test WIRE ona range of synthetic and real-world tasks, including identifying monochromaticsubgraphs, semantic segmentation of point clouds, and more standard graphbenchmarks. We find it to be effective in settings where the underlying graphstructure is important.</description>
      <author>example@mail.com (Isaac Reid, Arijit Sehanobish, Cedrik Höfs, Bruno Mlodozeniec, Leonhard Vulpius, Federico Barbero, Adrian Weller, Krzysztof Choromanski, Richard E. Turner, Petar Veličković)</author>
      <guid isPermaLink="false">2509.22259v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions</title>
      <link>http://arxiv.org/abs/2509.22150v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为联合图熵知识蒸馏(JGEKD)的分类策略，适用于非独立同分布的3D点云数据，通过基于联合图熵构建损失函数实现类别相关性的知识迁移。&lt;h4&gt;背景&lt;/h4&gt;3D点云分类任务通常假设类别事件是独立同分布的(IID)，但这种假设破坏了类别之间的相关性。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于非独立同分布的3D点云数据的分类策略，实现类别相关性的知识迁移。&lt;h4&gt;方法&lt;/h4&gt;提出联合图熵知识蒸馏(JGEKD)策略，通过基于联合图熵构建损失函数实现知识迁移；使用联合图捕获类别间的隐藏关系；构建孪生结构处理空间变换不变的3D点云；开发自知识蒸馏和教师知识蒸馏两种框架促进信息传递；利用框架在点云及其损坏形式间实现知识迁移提高鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanObject、ModelNet40、ScanntV2_cls和ModelNet-C数据集上的实验证明，JGEKD策略能够取得具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;JGEKD策略能有效处理非独立同分布的3D点云数据，通过联合图熵知识蒸馏实现类别相关性迁移，并在多个数据集上展现优异性能。&lt;h4&gt;翻译&lt;/h4&gt;分类任务在三维点云中通常假设类别事件遵循独立同分布，尽管这种假设破坏了类别之间的相关性。本研究提出了一种分类策略——联合图熵知识蒸馏(JGEKD)，适用于非独立同分布的三维点云数据，该策略通过基于联合图熵构建损失函数实现类别相关性的知识迁移。首先，我们使用联合图捕获类别之间的隐藏关系，通过计算图的熵实现知识蒸馏来训练我们的模型。随后，为处理空间变换不变的三维点云，我们构建孪生结构并开发了两种框架：自知识蒸馏和教师知识蒸馏，以促进相同数据不同变换形式之间的信息传递。此外，我们利用上述框架在点云及其损坏形式之间实现知识迁移，提高模型对损坏的鲁棒性。在ScanObject、ModelNet40、ScanntV2_cls和ModelNet-C上的大量实验证明，所提出的策略能够取得具有竞争力的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云分类任务中的三个关键问题：1) 传统方法假设类别事件独立同分布(IID)，忽略了类别间的相关性；2) 点云数据对空间变换敏感，不同变换下同一物体可能呈现相似表示；3) 点云数据易受噪声、遮挡等因素损坏，模型鲁棒性不足。这些问题在现实中很重要，因为点云数据规模小、标注成本高，忽略类别关系导致模型泛化能力差，而对变换和损坏的敏感性限制了模型在真实场景中的应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过分析指出IID假设在点云分类中的局限性，并通过类别相关性矩阵证明不同类别间存在关联。然后借鉴了知识蒸馏技术来建模类别间关系，使用图结构捕捉潜在联系。针对空间变换不变性，作者构建了孪生网络结构；为增强鲁棒性，开发了对抗训练策略。该方法综合借鉴了现有工作：包括基于特征层和logits的知识蒸馏方法、多视图和基于点的点云处理方法、多种数据增强策略以及对抗训练和输入随机化技术等，但创新性地将这些技术结合起来解决非IID点云分类问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合图建模类别间潜在关系，利用图熵量化关系信息，并通过知识蒸馏实现类别间知识传递，同时结合孪生网络处理点云的不同表示，最终增强模型对损坏的鲁棒性。整体流程包括：1) 构建联合图表示类别关系；2) 计算联合图熵作为损失函数；3) 设计JGEsKD和JGEtKD两种蒸馏框架；4) 应用对抗训练处理标准点云和损坏点云；5) 结合交叉熵和蒸馏损失进行模型训练。这种方法使模型能够学习类别间的隐含关系，适应不同变换，并对损坏点云保持鲁棒性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出联合图熵知识蒸馏(JGEKD)，首次使用联合图建模类别关系并通过图熵实现知识传递；2) 设计JGEsKD和JGEtKD两种框架，分别处理自蒸馏和教师蒸馏场景；3) 开发基于JGEKD的对抗训练策略增强模型鲁棒性。相比之前工作，不同之处在于：突破了传统IID假设限制，显式建模类别间关系；将知识蒸馏应用于类别间而非仅模型间知识传递；通过蒸馏而非简单数据增强增强鲁棒性；结合交叉熵和蒸馏损失全面学习特征表示。这些创新使模型在非IID点云数据上表现更稳定，对变换和损坏更具鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于联合图熵知识蒸馏的创新方法，有效解决了点云分类任务中类别关系建模、空间变换不变性和损坏鲁棒性三大挑战，显著提升了模型在非独立同分布点云数据上的分类性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classification tasks in 3D point clouds often assume that class events\replaced{are }{follow }independent and identically distributed (IID), althoughthis assumption destroys the correlation between classes. This \replaced{study}{paper }proposes a classification strategy, \textbf{J}oint \textbf{G}raph\textbf{E}ntropy \textbf{K}nowledge \textbf{D}istillation (JGEKD), suitable fornon-independent and identically distributed 3D point cloud data,\replaced{which }{the strategy } achieves knowledge transfer of classcorrelations through knowledge distillation by constructing a loss functionbased on joint graph entropy. First\deleted{ly}, we employ joint graphs tocapture add{the }hidden relationships between classes\replaced{ and}{,}implement knowledge distillation to train our model by calculating the entropyof add{add }graph.\replaced{ Subsequently}{ Then}, to handle 3D point clouds\deleted{that is }invariant to spatial transformations, we construct\replaced{S}{s}iamese structures and develop two frameworks, self-knowledgedistillation and teacher-knowledge distillation, to facilitate informationtransfer between different transformation forms of the same data. \replaced{Inaddition}{ Additionally}, we use the above framework to achieve knowledgetransfer between point clouds and their corrupted forms, and increase therobustness against corruption of model. Extensive experiments on ScanObject,ModelNet40, ScanntV2\_cls and ModelNet-C demonstrate that the proposed strategycan achieve competitive results.</description>
      <author>example@mail.com (Zhiqiang Tian, Weigang Li, Junwei Hu, Chunhua Deng)</author>
      <guid isPermaLink="false">2509.22150v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud</title>
      <link>http://arxiv.org/abs/2509.22132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的自监督点云补全方法，通过多视图增强和引入Mamba模型，解决了现有方法在真实数据集上泛化能力有限的问题，实现了在合成和真实数据集上的最先进结果。&lt;h4&gt;背景&lt;/h4&gt;点云补全旨在从部分观测中重建完整形状。当前方法存在局限性：监督方法依赖真实标签，受合成到真实域差距影响；无监督方法需要完整点云构建非配对数据；弱监督方法需要多视图观测；现有自监督方法因信号能力有限而效果不佳。&lt;h4&gt;目的&lt;/h4&gt;克服现有点云补全方法的局限性，提出一种新的自监督点云补全方法，提高模型在真实世界数据集上的性能。&lt;h4&gt;方法&lt;/h4&gt;1) 基于单个部分点云的多视图增强设计新自监督信号；2) 首次将Mamba模型引入自监督点云补全任务；3) 鼓励模型生成更高质量的点云。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实世界数据集上的实验表明，该方法实现了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的自监督点云补全方法通过多视图增强和Mamba模型的整合，有效克服了现有方法的局限性，在点云补全任务上取得了显著改进。&lt;h4&gt;翻译&lt;/h4&gt;点云补全旨在从部分观测中重建完整形状。尽管当前方法已取得了显著性能，但仍存在一些局限性：监督方法严重依赖真实标签，由于合成到真实的域差距，限制了它们在真实世界数据集上的泛化能力。无监督方法需要完整的点云来构建非配对训练数据，而弱监督方法需要物体的多视图观测。现有的自监督方法由于其自监督信号的有限能力，经常产生不令人满意的预测结果。为了克服这些挑战，我们提出了一种新颖的自监督点云补全方法。我们基于单个部分点云的多视图增强设计了一套新的自监督信号。此外，为了增强模型的学习能力，我们首次将Mamba引入自监督点云补全任务，鼓励模型生成更高质量的点云。在合成和真实世界数据集上的实验表明，我们的方法实现了最先进的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是点云补全问题，即从不完整的点云数据中重建完整的3D形状。这个问题在现实中很重要，因为现实世界中的点云数据常因传感器限制或遮挡而缺失部分区域，而完整的点云对机器人导航、自动驾驶、增强现实等领域至关重要。在研究中，解决这一问题可以克服现有方法对大量标注数据或完整参考数据的依赖，提高模型的泛化能力和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：监督方法依赖真实标签导致域差距问题，无监督方法需要完整点云，弱监督方法需要多视角观测，而现有自监督方法因自监督信号弱和模型架构不适合而效果不佳。基于这些问题，作者设计了两种主要创新：1)利用单部分点云的多视图增强创建新的自监督信号；2)首次将Mamba模型应用于此任务。作者借鉴了点云处理中的FPS采样、Hilbert曲线序列化、KNN局部划分等技术，以及Mamba的高效全局建模能力，但将这些技术以创新方式组合应用于自监督点云补全任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过单部分点云的多视图增强创建多样化的不完整性，提高模型的鲁棒性和泛化能力，同时利用Mamba模型增强特征提取。整体流程为：1)使用FPS选择关键点并用Hilbert曲线序列化；2)应用KNN划分为局部块并通过patch嵌入层生成tokens；3)使用8个Mamba块组成的编码器提取全局特征；4)通过多视图增强生成器从8个随机视角创建部分点云；5)使用包含三个全连接层的生成器生成完整点云；6)结合初始点云监督损失和多视图一致性损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于单部分点云多视图增强的新型自监督信号，引入不完整性多样性；2)首次将Mamba模型应用于自监督点云补全任务，设计专门的Mamba编码器；3)仅使用单部分点云无需任何先验信息的整体框架。相比之前的工作，本方法不依赖成对训练数据(监督方法)、不需要完整点云(无监督方法)、不需要多视角观测(弱监督方法)，且相比现有自监督方法使用了更强的自监督信号和更适合任务的Mamba架构，实验证明在合成和真实数据集上都取得了最先进结果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于单部分点云多视图增强的自监督点云补全方法，首次将Mamba模型应用于此任务，显著提高了点云补全的准确性和细节表现，在合成和真实世界数据集上都取得了最先进的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion aims to reconstruct complete shapes from partialobservations. Although current methods have achieved remarkable performance,they still have some limitations: Supervised methods heavily rely on groundtruth, which limits their generalization to real-world datasets due to thesynthetic-to-real domain gap. Unsupervised methods require complete pointclouds to compose unpaired training data, and weakly-supervised methods needmulti-view observations of the object. Existing self-supervised methodsfrequently produce unsatisfactory predictions due to the limited capabilitiesof their self-supervised signals. To overcome these challenges, we propose anovel self-supervised point cloud completion method. We design a set of novelself-supervised signals based on multi-view augmentations of the single partialpoint cloud. Additionally, to enhance the model's learning ability, we firstincorporate Mamba into self-supervised point cloud completion task, encouragingthe model to generate point clouds with better quality. Experiments onsynthetic and real-world datasets demonstrate that our method achievesstate-of-the-art results.</description>
      <author>example@mail.com (Jingjing Lu, Huilong Pi, Yunchuan Qin, Zhuo Tang, Ruihui Li)</author>
      <guid isPermaLink="false">2509.22132v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Large Material Gaussian Model for Relightable 3D Generation</title>
      <link>http://arxiv.org/abs/2509.22112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了大型材质高斯模型(MGM)，解决了现有3D重建模型无法生成材质属性的问题，实现了具有物理基础渲染(PBR)材质的高质量3D内容生成。&lt;h4&gt;背景&lt;/h4&gt;各行业对3D资产的需求不断增长，需要高效和自动化的3D内容创建方法。现有的3D高斯溅射技术虽然能实现高质量3D渲染，但无法生成资产的材质属性，这对不同光照环境中的真实感渲染至关重要。&lt;h4&gt;目的&lt;/h4&gt;引入大型材质高斯模型(MGM)框架，生成具有基于物理渲染(PBR)材质(反照率、粗糙度和金属属性)的高质量3D内容，而非仅生成具有不受控光照烘焙的RGB纹理。&lt;h4&gt;方法&lt;/h4&gt;首先微调基于输入深度和法线图条件化的多视图材质扩散模型；然后探索与2D高斯溅射对齐的材质表示方法，建模PBR材料的每个通道；最后利用重建的点云获取PBR属性，实现通过环境光照图的动态重新照明。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，MGM方法产生的材料不仅比基线方法具有更大的视觉吸引力，而且增强了材质建模，能够实现实用的下游渲染应用。&lt;h4&gt;结论&lt;/h4&gt;MGM成功解决了现有3D重建模型的材质生成缺陷，支持动态重新照明，为3D内容创建提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;各行业对3D资产不断增长的需求需要高效和自动化的3D内容创建方法。利用3D高斯溅射，最近的大型重建模型(LRMs)已经展示了通过集成多视图扩散进行生成和可扩展变换器进行重建，高效实现高质量3D渲染的能力。然而，现有模型无法生成资产的材质属性，这对于在不同光照环境中的真实感渲染至关重要。在本文中，我们引入了大型材质高斯模型(MGM)，这是一个新框架，旨在生成具有基于物理渲染(PBR)材质(即反照率、粗糙度和金属属性)的高质量3D内容，而不仅仅是生成具有不受控光照烘焙的RGB纹理。具体而言，我们首先微调了一个基于输入深度和法线图条件化的新多视图材质扩散模型。利用生成的多视图PBR图像，我们探索了一种与2D高斯溅射对齐的材质表示，同时建模PBR材料的每个通道。然后，重建的点云可以被渲染以获取PBR属性，通过应用各种环境光照图实现动态重新照明。大量实验证明，我们方法产生的材料不仅比基线方法具有更大的视觉吸引力，而且增强了材质建模，从而能够实现实用的下游渲染应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D生成方法无法产生物体材料属性的问题，包括反照率、粗糙度和金属度等PBR(基于物理渲染)材料特性。这个问题很重要，因为缺乏材料属性限制了3D资产在不同光照环境下的真实感渲染能力，使其无法适应动态光照变化，严重影响了3D内容在游戏、电影、虚拟现实等领域的应用效果和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D生成方法的局限性：优化方法如DreamFusion存在计算成本高和视图不一致问题；基于transformer的方法如LRM无法重建精细细节；而基于高斯溅射的方法虽然高效但无法控制光照和材料属性。作者借鉴了多项现有技术：MVDream等多视图扩散模型用于文本到多视图图像生成；2D高斯溅射作为3D表示；ControlNet控制深度和法线图提供几何先验；LaRa的体积解码器结构。主要创新在于首次提出生成具有PBR材料属性的高斯表示，并设计了专门的多视图PBR扩散模型和重建流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是生成具有物理渲染材料属性(反照率、粗糙度、金属度)的高斯表示，而不仅仅是RGB纹理，并支持动态重照明。整体流程包括：1)多视图PBR扩散：训练两个子模型分别生成反照率和粗糙度/金属度图像，使用深度和法线图作为条件；2)材料高斯重建：设计统一的高斯表示包含几何和材料属性，使用体积解码器预测3D高斯体积，通过重建损失、几何正则化和两阶段训练策略优化；3)重照明：使用Cook-Torrance微面模型将生成的PBR属性与环境光照图结合，实现不同光照条件下的真实感渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)材料高斯表示：首次提出包含PBR材料属性的高斯表示；2)多视图PBR扩散模型：专门设计生成无光照影响的材料图像；3)几何先验注入：在生成和重建阶段都使用深度和法线图确保一致性；4)两阶段训练策略：先训练反照率再训练所有材料组件。相比之前工作，不同之处在于：与优化方法相比更快速且避免了视图不一致；与LRM方法相比支持高分辨率渲染和材料生成；与其他高斯溅射方法相比能生成PBR材料属性；与其他PBR方法相比直接生成3D资产且速度更快。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提出了一种能够生成具有物理渲染材料属性的高斯表示方法，实现了从文本提示到可重照明的3D资产的高效生成，显著提升了3D内容创建的灵活性和真实感。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing demand for 3D assets across various industries necessitatesefficient and automated methods for 3D content creation. Leveraging 3D GaussianSplatting, recent large reconstruction models (LRMs) have demonstrated theability to efficiently achieve high-quality 3D rendering by integratingmultiview diffusion for generation and scalable transformers forreconstruction. However, existing models fail to produce the materialproperties of assets, which is crucial for realistic rendering in diverselighting environments. In this paper, we introduce the Large Material GaussianModel (MGM), a novel framework designed to generate high-quality 3D contentwith Physically Based Rendering (PBR) materials, ie, albedo, roughness, andmetallic properties, rather than merely producing RGB textures withuncontrolled light baking. Specifically, we first fine-tune a new multiviewmaterial diffusion model conditioned on input depth and normal maps. Utilizingthe generated multiview PBR images, we explore a Gaussian materialrepresentation that not only aligns with 2D Gaussian Splatting but also modelseach channel of the PBR materials. The reconstructed point clouds can then berendered to acquire PBR attributes, enabling dynamic relighting by applyingvarious ambient light maps. Extensive experiments demonstrate that thematerials produced by our method not only exhibit greater visual appealcompared to baseline methods but also enhance material modeling, therebyenabling practical downstream rendering applications.</description>
      <author>example@mail.com (Jingrui Ye, Lingting Zhu, Runze Zhang, Zeyu Hu, Yingda Yin, Lanjiong Li, Lequan Yu, Qingmin Liao)</author>
      <guid isPermaLink="false">2509.22112v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose</title>
      <link>http://arxiv.org/abs/2509.22058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于可靠初始位姿的自适应ICP LiDAR里程计方法，通过分布式粗配准获得可靠初始位姿，并结合动态阈值调整机制，显著提高了点云配准精度，在KITTI数据集上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;LiDAR里程计是移动机器人自主导航和定位的关键技术，在自动驾驶领域广泛应用。基于迭代最近点（ICP）的方法因能有效且准确地进行点云配准而成为LiDAR里程计的核心技术。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于ICP的方法不考虑初始位姿可靠性以及缺乏自适应机制处理复杂动态环境的问题，提高LiDAR里程计的精度。&lt;h4&gt;方法&lt;/h4&gt;1) 使用基于密度滤波的分布式粗配准获得初始位姿估计；2) 通过与运动预测位姿比较选择可靠初始位姿，减少源点云和目标点云间的初始误差；3) 结合当前和历史误差动态调整自适应阈值；4) 基于可靠初始位姿和自适应阈值执行点对平面自适应ICP配准。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在KITTI数据集上的实验表明，它优于现有方法，能够显著提高LiDAR里程计的精度，特别是在复杂动态环境中表现更佳。&lt;h4&gt;结论&lt;/h4&gt;通过可靠的初始位姿选择和自适应阈值调整机制，该方法有效解决了现有ICP方法在动态环境中的局限性，实现了高精度的点云配准。&lt;h4&gt;翻译&lt;/h4&gt;作为移动机器人自主导航和定位的关键技术，激光雷达里程计在自动驾驶应用中得到广泛应用。基于迭代最近点的方法因其在点云配准方面的高效性和准确性已成为LiDAR里程计的核心技术。然而，一些现有的基于ICP的方法没有考虑初始位姿的可靠性，可能导致方法收敛到局部最优。此外，缺乏自适应机制阻碍了对复杂动态环境的有效处理，导致配准精度显著下降。为解决这些问题，本文提出了一种基于可靠初始位姿的自适应ICP LiDAR里程计方法。首先，采用基于密度滤波的分布式粗配准来获得初始位姿估计。通过与运动预测位姿比较选择可靠的初始位姿，减少源点云和目标点云之间的初始误差。随后，通过结合当前和历史误差，动态调整自适应阈值以适应动态环境的实时变化。最后，基于可靠的初始位姿和自适应阈值，执行从当前帧到局部地图的点对平面自适应ICP配准，实现源点云和目标点云的高精度对齐。在公开KITTI数据集上的大量实验表明，所提出的方法优于现有方法，并显著提高了LiDAR里程计的精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有基于ICP的激光雷达里程计方法中初始位姿不可靠和缺乏自适应机制的问题。这个问题很重要，因为激光雷达里程计是自主导航和定位的关键技术，广泛应用于自动驾驶领域。初始位姿的可靠性直接影响配准精度，而动态环境下的精确定位对自动驾驶和机器人系统至关重要。解决这些问题可以提高定位精度和鲁棒性，特别是在复杂动态场景中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于ICP的激光雷达里程计方法的局限性，包括初始位姿不可靠和缺乏自适应机制。作者借鉴了现有的点云配准技术，如传统的ICP算法和GICP，并参考了特征点云处理方法，如密度过滤和协方差矩阵计算。作者引入了自适应机制来处理动态环境变化，并设计了一个四步流程：分布式粗配准、初始位姿确定、自适应阈值和自适应ICP配准。这些设计基于对现有方法的深入理解和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过密度过滤的分布式粗配准获取初始位姿估计，通过比较初始位姿与运动预测位姿选择可靠的初始位姿，结合当前和历史误差动态调整自适应阈值，基于可靠的初始位姿和自适应阈值执行点到平面自适应ICP配准。整体流程包括：1)分布式粗配准：计算点密度、过滤低密度点、计算协方差矩阵、查找最近点对并优化变换矩阵；2)初始位姿确定：生成预测位姿、比较与选择可靠位姿；3)自适应阈值：计算加速度变化率、模型偏差矩阵和加权误差；4)自适应ICP配准：应用位姿变换、查找最近邻、计算残差和权重、优化变换矩阵。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)可靠初始位姿获取方法：使用密度过滤的分布式粗配准和运动预测比较；2)自适应帧到局部地图ICP配准方法：结合当前和历史误差动态调整参数；3)自适应阈值机制：根据运动状态和点云环境变化动态调整阈值。相比之前的工作，不同之处在于传统ICP方法没有充分考虑初始位姿可靠性，容易陷入局部最优，且缺乏自适应机制难以处理动态环境。本方法通过可靠初始位姿和自适应机制显著提高了在复杂动态环境中的鲁棒性和精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于可靠初始位姿的自适应ICP激光雷达里程计方法，通过可靠的初始位姿估计和动态自适应机制，显著提高了在复杂动态环境中的定位精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TIM.2025.3571148&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a key technology for autonomous navigation and positioning in mobilerobots, light detection and ranging (LiDAR) odometry is widely used inautonomous driving applications. The Iterative Closest Point (ICP)-basedmethods have become the core technique in LiDAR odometry due to their efficientand accurate point cloud registration capability. However, some existingICP-based methods do not consider the reliability of the initial pose, whichmay cause the method to converge to a local optimum. Furthermore, the absenceof an adaptive mechanism hinders the effective handling of complex dynamicenvironments, resulting in a significant degradation of registration accuracy.To address these issues, this paper proposes an adaptive ICP-based LiDARodometry method that relies on a reliable initial pose. First, distributedcoarse registration based on density filtering is employed to obtain theinitial pose estimation. The reliable initial pose is then selected bycomparing it with the motion prediction pose, reducing the initial errorbetween the source and target point clouds. Subsequently, by combining thecurrent and historical errors, the adaptive threshold is dynamically adjustedto accommodate the real-time changes in the dynamic environment. Finally, basedon the reliable initial pose and the adaptive threshold, point-to-planeadaptive ICP registration is performed from the current frame to the local map,achieving high-precision alignment of the source and target point clouds.Extensive experiments on the public KITTI dataset demonstrate that the proposedmethod outperforms existing approaches and significantly enhances the accuracyof LiDAR odometry.</description>
      <author>example@mail.com (Qifeng Wang, Weigang Li, Lei Nie, Xin Xu, Wenping Liu, Zhe Xu)</author>
      <guid isPermaLink="false">2509.22058v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Convexity-Driven Projection for Point Cloud Dimensionality Reduction</title>
      <link>http://arxiv.org/abs/2509.22043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为凸性驱动投影(CDP)的新型点云降维方法，通过构建k-NN图并识别可接受点对来保持局部非凸性结构。&lt;h4&gt;背景&lt;/h4&gt;点云降维领域需要处理局部非凸性特征，传统方法可能无法有效保持这种特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保持点云中由绕行引起的局部非凸性的降维方法。&lt;h4&gt;方法&lt;/h4&gt;构建k-NN图，识别欧几里得到最短路径比率低于阈值的可接受点对，聚合这些点对的归一化方向形成半正定非凸性结构矩阵，使用该结构矩阵的前k个特征向量进行投影。&lt;h4&gt;主要发现&lt;/h4&gt;提供了两种可验证保证：点对的后验证书，用于限制每个可接受点对的投影后失真；以及平均情况谱边界，将捕获的方向能量的期望与结构矩阵的谱联系起来，得出典型失真的分位数陈述。&lt;h4&gt;结论&lt;/h4&gt;评估协议报告了固定点和重新选择点的绕行误差以及证书分位数，使实践者能够检查其数据上的保证。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种凸性驱动投影(CDP)，这是一种无边界的线性方法，用于点云的降维，旨在保持由绕行引起的局部非凸性。CDP构建一个k-NN图，识别欧几里得到最短路径比率低于阈值的可接受点对，并聚合它们的归一化方向以形成一个半正定非凸性结构矩阵。投影使用该结构矩阵的前k个特征向量。我们提供了两种可验证的保证：一种点对的后验证书，用于限制每个可接受点对的投影后失真；以及一种平均情况谱边界，它将捕获的方向能量的期望与结构矩阵的谱联系起来，从而得出典型失真的分位数陈述。我们的评估协议报告了固定点和重新选择点的绕行误差以及证书分位数，使实践者能够检查其数据上的保证。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云在高维空间中的降维问题，特别是如何保留由障碍物或曲率引起的'绕行(detour)'导致的局部非凸性结构。在3D建模、机器人和可视化等领域，点云数据中两点间的图最短路径可能远大于它们的欧几里得距离，这种绕行几何结构对于路径规划、形状分析等任务至关重要，而标准降维方法如PCA、t-SNE等无法有效保留这种结构。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有降维方法的局限性：PCA最大化解释方差但可能忽略测地线结构；非线性方法如t-SNE和UMAP缺乏对绕行几何的明确控制；基于图的线性投影方法不能突出显示非凸性方向。作者借鉴了基于图的降维方法思路，但针对绕行几何进行了专门设计。通过构建k-NN图，识别具有低欧几里得到最短路径比率的'可接受对'，并利用这些对的方向信息构建非凸性结构矩阵，从而专注于保留非凸结构而非保持邻近点接近。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过识别和保留那些显示显著绕行(即非凸性)的方向对来降维，同时最小化绕行结构的扭曲。整体流程：1)标准化坐标；2)构建互惠k-NN图；3)计算所有点对的最短路径距离；4)形成满足rij = 欧几里得距离/最短路径距离 ≤ 阈值τ的可接受对集合；5)构建非凸性结构矩阵，聚合这些对的归一化方向；6)计算该矩阵的前k个特征向量；7)将原始点投影到这些特征向量张成的子空间；8)为投影图分配边权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出边界无关的线性降维方法，专注于保留绕行引起的局部非凸性；2)构建正半定非凸性结构矩阵；3)提供成对后验证书和平均情况谱界限两种可验证保证；4)提出评估协议报告固定和重新选择的绕行错误。相比之前工作，CDP不最大化方差(PCA)，不优化概率邻域(t-SNE/UMAP)，也不最小化平滑性(LPP/NPP/OLPP)，而是突出显示非凸性方向，并提供理论保证和评估协议。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了凸性驱动投影(CDP)方法，一种线性降维技术，通过构建非凸性结构矩阵并利用其特征向量进行投影，有效保留了点云中的绕行诱导局部非凸结构，同时提供了可验证的理论保证和评估协议。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Convexity-Driven Projection (CDP), a boundary-free linear methodfor dimensionality reduction of point clouds that targets preservingdetour-induced local non-convexity. CDP builds a $k$-NN graph, identifiesadmissible pairs whose Euclidean-to-shortest-path ratios are below a threshold,and aggregates their normalized directions to form a positive semidefinitenon-convexity structure matrix. The projection uses the top-$k$ eigenvectors ofthe structure matrix. We give two verifiable guarantees. A pairwisea-posteriori certificate that bounds the post-projection distortion for eachadmissible pair, and an average-case spectral bound that links expectedcaptured direction energy to the spectrum of the structure matrix, yieldingquantile statements for typical distortion. Our evaluation protocol reportsfixed- and reselected-pairs detour errors and certificate quantiles, enablingpractitioners to check guarantees on their data.</description>
      <author>example@mail.com (Suman Sanyal)</author>
      <guid isPermaLink="false">2509.22043v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation</title>
      <link>http://arxiv.org/abs/2509.21905v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于扩散的统一框架，实现了文本和拖拽交互共同控制下的图像编辑，结合了文本驱动和拖拽驱动两种方法的优点。&lt;h4&gt;背景&lt;/h4&gt;文本驱动和拖拽驱动的图像编辑方法最近取得了显著进展，但存在互补限制：文本驱动方法擅长纹理处理但缺乏精确空间控制，拖拽驱动方法主要修改形状结构而缺乏细粒度纹理指导。&lt;h4&gt;目的&lt;/h4&gt;解决现有图像编辑方法的局限性，实现结合文本和拖拽优势的高保真联合图像编辑。&lt;h4&gt;方法&lt;/h4&gt;提出了两个关键创新：(1)点云确定性拖拽，通过3D特征映射增强潜在空间布局控制；(2)拖拽-文本引导去噪，动态平衡拖拽和文本条件的影响。支持仅文本、仅拖拽或组合条件的灵活编辑模式。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量定量和定性实验证明，该方法实现了高保真的联合编辑，性能匹配或超过了专门的仅文本或仅拖拽方法，建立了可控图像操作的通用可推广解决方案。&lt;h4&gt;结论&lt;/h4&gt;代码将公开可用，以重现本文呈现的所有结果。&lt;h4&gt;翻译&lt;/h4&gt;本文探索了在文本和拖拽交互共同控制下的图像编辑。虽然最近在文本驱动和拖拽驱动编辑方面取得了显著进展，但它们存在互补的限制：文本驱动方法擅长纹理处理但缺乏精确的空间控制，而拖拽驱动方法主要修改形状和结构，没有细粒度的纹理指导。为解决这些限制，我们提出了一个统一的基于扩散的框架，用于联合拖拽-文本图像编辑，整合了两种范式的优势。我们的框架引入了两个关键创新：(1)点云确定性拖拽，通过3D特征映射增强潜在空间布局控制；(2)拖拽-文本引导去噪，在去噪过程中动态平衡拖拽和文本条件的影响。值得注意的是，我们的模型支持灵活的编辑模式 - 可以在仅文本、仅拖拽或组合条件下运行 - 同时在每种设置中保持强性能。大量的定量和定性实验证明，我们的方法不仅实现了高保真的联合编辑，而且匹配或超过了专门的仅文本或仅拖拽方法的性能，为可控图像操作建立了一个多功能且可推广的解决方案。代码将公开可用，以重现本文呈现的所有结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores image editing under the joint control of text and draginteractions. While recent advances in text-driven and drag-driven editing haveachieved remarkable progress, they suffer from complementary limitations:text-driven methods excel in texture manipulation but lack precise spatialcontrol, whereas drag-driven approaches primarily modify shape and structurewithout fine-grained texture guidance. To address these limitations, we proposea unified diffusion-based framework for joint drag-text image editing,integrating the strengths of both paradigms. Our framework introduces two keyinnovations: (1) Point-Cloud Deterministic Drag, which enhances latent-spacelayout control through 3D feature mapping, and (2) Drag-Text Guided Denoising,dynamically balancing the influence of drag and text conditions duringdenoising. Notably, our model supports flexible editing modes - operating withtext-only, drag-only, or combined conditions - while maintaining strongperformance in each setting. Extensive quantitative and qualitative experimentsdemonstrate that our method not only achieves high-fidelity joint editing butalso matches or surpasses the performance of specialized text-only or drag-onlyapproaches, establishing a versatile and generalizable solution forcontrollable image manipulation. Code will be made publicly available toreproduce all results presented in this work.</description>
      <author>example@mail.com (Qihang Wang, Yaxiong Wang, Lechao Cheng, Zhun Zhong)</author>
      <guid isPermaLink="false">2509.21905v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Generating Stable Placements via Physics-guided Diffusion Models</title>
      <link>http://arxiv.org/abs/2509.21664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the IEEE International Conference on Robotics and  Automation 2026, Vienna, Austria, June 1-5, 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种将稳定性直接整合到扩散模型采样过程中的新方法，用于解决多物体场景中机器人放置物体的稳定性挑战。通过训练扩散模型生成稳定的放置，并结合几何感知先验和稳定性感知损失，实现了无需额外训练的高效稳定物体放置。&lt;h4&gt;背景&lt;/h4&gt;在多物体场景中稳定放置物体是机器人操作的基本挑战，放置必须满足无穿透、精确表面接触和力平衡等条件。现有方法依赖运行仿真引擎或基于启发式、外观的评估来评估稳定性，这些方法存在效率或准确性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接生成稳定放置的方法，无需依赖仿真引擎或启发式评估，提高放置稳定性的同时减少计算时间。&lt;h4&gt;方法&lt;/h4&gt;作者查询离线基于采样的规划器收集多模态放置标签，训练扩散模型生成稳定的放置。该模型基于场景和物体点云条件化，作为几何感知先验。利用基于分数的生成模型的组合特性，将学习到的先验与稳定性感知损失结合，增加从高稳定性区域采样的可能性。此方法无需额外的再训练或微调，可直接应用于现成模型。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准场景上的评估表明，物理引导的模型相比最先进的几何方法，放置对强力扰动的鲁棒性提高了56%，同时运行时间减少了47%。&lt;h4&gt;结论&lt;/h4&gt;将稳定性直接整合到扩散模型采样过程中的方法，能够有效提高物体放置的稳定性和效率，且无需额外的训练步骤，可直接应用于现有模型，为机器人操作中的物体放置问题提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在多物体场景中稳定放置物体是机器人操作中的一个基本挑战，因为放置必须无穿透、建立精确表面接触，并达到力平衡。为了评估稳定性，现有方法依赖运行仿真引擎或采用启发式、基于外观的评估。相比之下，我们的方法将稳定性直接整合到扩散模型的采样过程中。为此，我们查询离线基于采样的规划器以收集多模态放置标签，并训练扩散模型来生成稳定的放置。扩散模型基于场景和物体点云条件化，并作为几何感知先验。我们利用基于分数的生成模型的组合特性，将这种学习到的先验与稳定性感知损失相结合，从而增加从高稳定性区域采样的可能性。重要的是，这种策略不需要额外的再训练或微调，可以直接应用于现成模型。我们在四个稳定性可以准确计算的基准场景上评估了我们的方法。我们的物理引导模型相比最先进的几何方法，实现了对强力扰动56%更强的鲁棒性，同时将运行时间减少了47%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在多物体场景中稳定放置物体的问题，放置必须满足无穿透、建立精确表面接触和实现力平衡三个条件。这个问题很重要，因为它是机器人操作中的基本挑战，在建筑施工、场景重组和密集包装等高级任务中至关重要。现有方法要么依赖耗时的模拟引擎，要么依赖基于外观的启发式评估，效率低下且准确性有限。只有很少的工作空间姿态会导致有效放置，使得随机采样或搜索方法效率不高。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了稳定放置规划需要满足的严格几何和物理约束，将其分解为三个需求：几何感知的放置算法、场景平衡推理和稳定性验证器。他们评估了现有方法的局限性，然后设计了一种基于扩散模型的物理引导方法。作者借鉴了扩散模型在条件生成方面的能力，利用了装配鲁棒性概念，并采用采样规划器生成训练数据。他们使用U-Net架构进行点云编码，并通过组合学习到的几何先验与稳定性感知损失来实现物理引导。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将物理稳定性直接整合到扩散模型的采样过程中，利用扩散模型生成稳定的放置姿态，并通过鲁棒性引导指导采样过程朝向高稳定性区域。整体流程包括：1)使用采样规划器生成训练数据集；2)处理观测数据为点云表示；3)训练U-Net模型预测放置姿态；4)在推理过程中应用鲁棒性引导来生成稳定放置；5)评估放置的鲁棒性、非穿透性和稳定性。整个方法不需要额外训练或微调，可直接应用于现成模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将物理稳定性直接整合到扩散模型采样过程中；2)提出鲁棒性引导方案指导模型朝向高稳定性区域；3)利用基于分数的生成模型组合性质，结合几何先验与稳定性损失；4)引导方案独立于模型训练，可直接应用于现成模型；5)将几何感知和物理推理统一在一个框架中。相比之前工作，该方法不依赖模拟引擎（减少47%计算时间），不依赖外观评估或惯性参数假设，在未知场景上表现出更好的泛化能力，生成的放置具有更高的鲁棒性（平均高56%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种物理引导的扩散模型方法，通过将稳定性直接整合到采样过程中，实现了在未知场景中快速生成具有更高鲁棒性的物体放置方案，相比现有方法显著提高了稳定性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stably placing an object in a multi-object scene is a fundamental challengein robotic manipulation, as placements must be penetration-free, establishprecise surface contact, and result in a force equilibrium. To assessstability, existing methods rely on running a simulation engine or resort toheuristic, appearance-based assessments. In contrast, our approach integratesstability directly into the sampling process of a diffusion model. To this end,we query an offline sampling-based planner to gather multi-modal placementlabels and train a diffusion model to generate stable placements. The diffusionmodel is conditioned on scene and object point clouds, and serves as ageometry-aware prior. We leverage the compositional nature of score-basedgenerative models to combine this learned prior with a stability-aware loss,thereby increasing the likelihood of sampling from regions of high stability.Importantly, this strategy requires no additional re-training or fine-tuning,and can be directly applied to off-the-shelf models. We evaluate our method onfour benchmark scenes where stability can be accurately computed. Ourphysics-guided models achieve placements that are 56% more robust to forcefulperturbations while reducing runtime by 47% compared to a state-of-the-artgeometric method.</description>
      <author>example@mail.com (Philippe Nadeau, Miguel Rogel, Ivan Bilić, Ivan Petrović, Jonathan Kelly)</author>
      <guid isPermaLink="false">2509.21664v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>SeamCrafter: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.20725v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SeamCrafter的自回归GPT风格接缝生成器，用于解决3D表面UV参数化和纹理映射中的接缝放置问题。该方法通过双分支点云编码器和直接偏好优化技术，显著降低了接缝的UV变形和碎片化程度，同时保持拓扑一致性和视觉保真度。&lt;h4&gt;背景&lt;/h4&gt;网格接缝在3D表面的UV参数化和纹理映射中起着关键作用。不良的接缝放置会导致严重的UV变形或过度碎片化，从而阻碍纹理合成和艺术家工作流程。&lt;h4&gt;目的&lt;/h4&gt;解决现有接缝生成方法中高变形和碎片化之间的权衡问题，开发一种能够同时降低变形和碎片化的接缝生成方法。&lt;h4&gt;方法&lt;/h4&gt;SeamCrafter是一种基于点云输入的自回归GPT风格接缝生成器。它采用双分支点云编码器在预训练过程中解耦并捕获互补的拓扑和几何线索。此外，研究人员使用基于新型接缝评估框架的偏好数据集，通过直接偏好优化(DPO)微调模型，该框架主要通过UV变形和碎片化评估接缝并提供成对偏好标签。&lt;h4&gt;主要发现&lt;/h4&gt;SeamCrafter产生的接缝比先前方法的变形和碎片化程度低得多，同时保持拓扑一致性和视觉保真度。&lt;h4&gt;结论&lt;/h4&gt;SeamCrafter通过结合自回归GPT架构、双分支点云编码器和直接偏好优化技术，有效解决了3D表面UV参数化和纹理映射中的接缝放置问题，显著提高了接缝质量。&lt;h4&gt;翻译&lt;/h4&gt;网格接缝在3D表面的UV参数化和纹理映射的分区中起着关键作用。放置不当的接缝通常会导致严重的UV变形或过度碎片化，从而阻碍纹理合成并干扰艺术家工作流程。现有方法经常以一种失败模式换取另一种失败模式——要么产生高变形，要么产生许多分散的碎片。为此，我们引入了SeamCrafter，一个基于点云输入的自回归GPT风格接缝生成器。SeamCrafter采用双分支点云编码器，在预训练过程中解耦并捕获互补的拓扑和几何线索。为了进一步提高接缝质量，我们使用基于新型接缝评估框架的偏好数据集，通过直接偏好优化(DPO)对模型进行微调。该框架主要通过UV变形和碎片化评估接缝，并提供成对偏好标签来指导优化。大量实验表明，SeamCrafter产生的接缝比先前方法的变形和碎片化程度低得多，同时保持拓扑一致性和视觉保真度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D模型UV展开过程中的接缝生成问题。不当的接缝放置会导致严重的UV变形或过度碎片化，影响纹理映射质量并阻碍艺术家工作流程。这个问题在3D建模、游戏开发、电影特效等领域非常重要，因为高质量的UV展开能确保纹理正确映射到3D模型上，避免拉伸、压缩和可见不连续性，提高艺术家工作效率和最终作品质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统方法（如区域增长）产生碎片化UV图谱，对参数敏感；基于学习的方法（如SeamGPT）过度依赖拓扑线索而缺乏几何感知。作者借鉴了SeamGPT的自回归方法、VecSet点云编码器和直接偏好优化（DPO）等技术，但通过双分支编码器同时捕获拓扑和几何信息，并设计了专门的接缝评估框架来优化变形与碎片化之间的权衡。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是同时利用网格的拓扑和几何信息生成高质量接缝，并通过直接偏好优化将模型输出与人类偏好对齐。整体流程分为三阶段：1）预训练阶段，使用双分支点云编码器分别从顶点边和表面采样点捕获拓扑和几何信息，用沙漏transformer解码器生成接缝；2）后训练阶段，构建接缝评估系统生成偏好数据集，用DPO微调模型；3）推理阶段，预测接缝端点坐标，映射到网格表面并标记接缝路径，完成UV展开。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）双分支点云编码器，分别捕获拓扑结构和几何细节；2）接缝评估框架，提供基于UV变形和碎片化的偏好信号；3）直接偏好优化应用，将模型与人类判断对齐。相比之前工作，与传统方法相比避免了过度碎片化；与其他学习方法相比更好地保持了网格结构约束；与SeamGPT相比同时利用拓扑和几何信息，生成更连贯、结构合理的接缝布局。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SeamCrafter通过结合双分支几何-拓扑编码器和直接偏好优化技术，显著提高了3D模型UV展开的接缝生成质量，同时降低了UV变形和碎片化，为艺术家和3D内容创作者提供了更高效的工作流程。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mesh seams play a pivotal role in partitioning 3D surfaces for UVparametrization and texture mapping. Poorly placed seams often result in severeUV distortion or excessive fragmentation, thereby hindering texture synthesisand disrupting artist workflows. Existing methods frequently trade one failuremode for another-producing either high distortion or many scattered islands. Toaddress this, we introduce SeamCrafter, an autoregressive GPT-style seamgenerator conditioned on point cloud inputs. SeamCrafter employs a dual-branchpoint-cloud encoder that disentangles and captures complementary topologicaland geometric cues during pretraining. To further enhance seam quality, wefine-tune the model using Direct Preference Optimization (DPO) on a preferencedataset derived from a novel seam-evaluation framework. This framework assessesseams primarily by UV distortion and fragmentation, and provides pairwisepreference labels to guide optimization. Extensive experiments demonstrate thatSeamCrafter produces seams with substantially lower distortion andfragmentation than prior approaches, while preserving topological consistencyand visual fidelity.</description>
      <author>example@mail.com (Duoteng Xu, Yuguang Chen, Jing Li, Xinhai Liu, Xueqi Ma, Zhuo Chen, Dongyu Zhang, Chunchao Guo)</author>
      <guid isPermaLink="false">2509.20725v2</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>TUN3D: Towards Real-World Scene Understanding from Unposed Images</title>
      <link>http://arxiv.org/abs/2509.21388v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为TUN3D的新方法，首次实现了仅从多视图图像输入进行室内场景的联合布局估计和3D物体检测，无需真实相机位姿或深度监督。该方法在多个基准测试中取得了最先进的性能，显著提升了室内场景理解能力。&lt;h4&gt;背景&lt;/h4&gt;布局估计和3D物体检测是室内场景理解的两大基本任务。现有方法通常依赖于点云输入，但大多数消费者相机缺乏深度传感器，视觉数据更为常见，这限制了现有方法的应用范围。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够仅从多视图图像输入进行联合布局估计和3D物体检测的方法，不需要真实的相机位姿或深度监督，从而解决点云输入的限制问题。&lt;h4&gt;方法&lt;/h4&gt;TUN3D基于轻量级稀疏卷积骨干网络，采用两个专用头部分别处理3D物体检测和布局估计任务。布局估计部分利用了一种新颖且有效的参数化墙体表示方法。&lt;h4&gt;主要发现&lt;/h4&gt;TUN3D在三个具有挑战性的场景理解基准测试中取得了最先进的性能：(1)使用真实点云，(2)使用有位姿的图像，(3)使用无位姿的图像。与专门的3D物体检测方法性能相当的同时，显著提升了布局估计能力。&lt;h4&gt;结论&lt;/h4&gt;TUN3D为整体室内场景理解树立了新基准，证明了仅从视觉输入进行联合布局估计和3D物体检测的可行性，为缺乏深度传感器的消费级相机应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;布局估计和3D物体检测是室内场景理解的两大基本任务。当结合使用时，它们能够创建一个紧凑且语义丰富的场景空间表示。现有方法通常依赖于点云输入，这构成了一个主要限制，因为大多数消费者相机缺乏深度传感器，而纯视觉数据则更为常见。我们通过TUN3D解决了这一问题，这是首个在真实扫描中处理联合布局估计和3D物体检测的方法，以多视图图像为输入，且不需要真实的相机位姿或深度监督。我们的方法基于轻量级稀疏卷积骨干网络，并采用两个专用头部分别进行3D物体检测和布局估计，利用了一种新颖且有效的参数化墙体表示。大量实验表明，TUN3D在三个具有挑战性的场景理解基准测试中取得了最先进的性能：(i)使用真实点云，(ii)使用有位姿的图像，以及(iii)使用无位姿的图像。虽然性能与专门的3D物体检测方法相当，但TUN3D显著改进了布局估计，为整体室内场景理解树立了新基准。代码可在https://github.com/col14m/tun3d获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决室内场景理解中的联合布局估计和3D物体检测问题，特别是减少对点云数据的依赖。这个问题很重要，因为大多数消费级相机没有深度传感器，而现有方法通常需要点云输入，限制了在普通设备上的应用。室内场景理解在机器人、AR/VR、室内设计等领域有广泛应用，而紧凑的空间表示比密集3D重建更适合在设备上运行。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：点云输入要求高、联合模型要么速度慢要么精度低。他们决定构建在实时3D物体检测模型之上，并扩展输入模态从点云到带姿态图像再到无姿态图像。借鉴了TR3D的稀疏卷积骨干网络、PQ-Transformer的联合检测思路，以及DUSt3R的图像到点云转换方法。作者还提出了一种新的墙壁参数化方法，将3D问题简化为2D表示以提高效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用轻量级稀疏卷积网络处理多种输入模态（点云、带姿态图像、无姿态图像），通过新的墙壁参数化方法实现高效的联合布局估计和3D物体检测。整体流程：1)输入处理（点云直接体素化，图像通过DUSt3R转换为点云）；2)网络结构（稀疏卷积骨干+颈部+检测头+布局头）；3)墙壁参数化（2D偏移+高度）；4)训练（多组件损失函数）。这种方法无需深度或相机姿态监督，却能实现高性能场景理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个从多视角图像（带或不带姿态）进行联合布局估计和3D物体检测的方法；2)新的墙壁参数化方法（2D偏移+高度，仅需5个参数）；3)高效的稀疏卷积架构；4)灵活处理三种输入模态。相比之前工作，TUN3D不需要深度传感器或相机姿态，比现有方法快4-160倍，在布局估计上显著优于现有方法（比PQ-Transformer高23.6 F1分数），且能在真实场景上工作，而不仅仅是合成数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TUN3D首次实现了从多视角图像（带或不带相机姿态）进行无需深度监督的联合室内布局估计和3D物体检测，显著提高了场景理解的效率和准确性，为在普通设备上运行的空间理解应用开辟了新可能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Layout estimation and 3D object detection are two fundamental tasks in indoorscene understanding. When combined, they enable the creation of a compact yetsemantically rich spatial representation of a scene. Existing approachestypically rely on point cloud input, which poses a major limitation since mostconsumer cameras lack depth sensors and visual-only data remains far morecommon. We address this issue with TUN3D, the first method that tackles jointlayout estimation and 3D object detection in real scans, given multi-viewimages as input, and does not require ground-truth camera poses or depthsupervision. Our approach builds on a lightweight sparse-convolutional backboneand employs two dedicated heads: one for 3D object detection and one for layoutestimation, leveraging a novel and effective parametric wall representation.Extensive experiments show that TUN3D achieves state-of-the-art performanceacross three challenging scene understanding benchmarks: (i) using ground-truthpoint clouds, (ii) using posed images, and (iii) using unposed images. Whileperforming on par with specialized 3D object detection methods, TUN3Dsignificantly advances layout estimation, setting a new benchmark in holisticindoor scene understanding. Code is available athttps://github.com/col14m/tun3d .</description>
      <author>example@mail.com (Anton Konushin, Nikita Drozdov, Bulat Gabdullin, Alexey Zakharov, Anna Vorontsova, Danila Rukhovich, Maksim Kolodiazhnyi)</author>
      <guid isPermaLink="false">2509.21388v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Rate-Distortion Optimized Communication for Collaborative Perception</title>
      <link>http://arxiv.org/abs/2509.21994v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为RDcomm的通信高效协同感知框架，通过信息理论指导，优化多智能体间的信息共享，在保持高准确度的同时大幅减少通信量。&lt;h4&gt;背景&lt;/h4&gt;协同感知强调通过多智能体共享视觉信息来增强环境理解，但受限于带宽资源。先前工作探索了任务性能与通信量之间的权衡，但缺乏理论基础。&lt;h4&gt;目的&lt;/h4&gt;填补协同感知领域理论基础的空白，提出一种专门用于分析目标导向多智能体系统中性能-通信权衡的实用速率失真理论。&lt;h4&gt;方法&lt;/h4&gt;提出RDcomm框架，包含两个关键创新：任务熵离散编码，为特征分配任务相关码字长度以最大化实用信息效率；互信息驱动的消息选择，利用互信息神经估计实现无冗余传输。&lt;h4&gt;主要发现&lt;/h4&gt;在DAIR-V2X和OPV2V数据集上的3D目标检测和BEV分割实验表明，RDcomm实现了最先进的准确度，同时将通信量减少了高达108倍。&lt;h4&gt;结论&lt;/h4&gt;RDcomm框架通过理论指导的通信策略优化，有效解决了协同感知中的性能-通信权衡问题，为多智能体系统的高效协作提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;协同感知强调通过使多个智能体能够共享视觉信息来增强环境理解，同时受限于带宽资源。虽然之前的工作已经探索了任务性能和通信量之间的经验权衡，但理论基础仍存在显著差距。为了填补这一空白，我们借鉴信息理论，提出了一种面向多智能体协作的实用速率失真理论，专门用于分析目标导向多智能体系统中的性能-通信权衡。该理论具体化了两条设计最优通信策略的关键条件：提供实用相关信息和传输无冗余消息。基于这两个条件，我们提出了RDcomm，一种通信高效的协同感知框架，引入了两个关键创新：i) 任务熵离散编码，为具有任务相关码字长度的特征分配，以最大化提供实用信息的效率；ii) 互信息驱动的消息选择，利用互信息神经估计来接近最优无冗余条件。在DAIR-V2X和OPV2V上的3D目标检测和BEV分割实验表明，RDcomm实现了最先进的准确度，同时将通信量减少了高达108倍。代码将公开发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多智能体协作感知中的通信优化问题，即在有限带宽资源下如何平衡感知任务性能和通信量。这个问题在自动驾驶、机器人协作等领域至关重要，因为这些场景中多个智能体需要共享视觉信息来增强环境理解，但通信带宽有限。如果通信量过大会导致网络拥塞和延迟，过度压缩又可能丢失关键信息影响任务性能，因此研究如何在性能和通信效率间取得平衡具有重要理论和实践意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从信息论角度出发，借鉴经典率失真理论并针对多智能体协作特点进行扩展。他们首先分析现有工作局限性：大多数方法都是启发式的，缺乏理论基础。作者设计过程分三步：1) 提出多智能体协作的实用率失真理论，明确两个最优条件；2) 基于条件设计RDcomm框架；3) 通过实验验证有效性。作者借鉴了现有工作中的空间选择和特征压缩技术，但将其置于理论框架下，提供了更系统的方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过率失优化的方式，在保证感知任务性能前提下最小化通信量，即只传输对任务有用的信息且避免传输接收方已拥有的信息。整体流程：1) 感知管道将传感器输入转换为鸟瞰图特征；2) 任务熵离散编码：使用分层向量量化映射特征到码本，根据任务相关性分配不同长度编码；3) 互信息驱动消息选择：评估特征间冗余，选择互补性强的特征传输；4) 消息平滑和融合：对稀疏选择的消息平滑处理后与接收方本地特征融合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 提出多智能体协作的实用率失真理论，明确最优通信策略的两个条件；2) 设计任务熵离散编码模块，根据任务相关性分配不同长度编码；3) 提出互信息驱动的消息选择模块，减少智能体间冗余；4) 设计RDcomm框架实现性能与通信效率平衡。相比之前工作，不同在于：提供了理论基础而非仅启发式方法；同时优化消息选择和编码；采用任务相关编码策略而非通用压缩方法；实验显示在大幅减少通信量同时保持或提升任务性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出基于率失真理论的RDcomm框架，通过任务相关编码和互信息驱动的消息选择，在多智能体协作感知中实现通信量大幅降低（最高108倍）同时保持或提升感知任务性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative perception emphasizes enhancing environmental understanding byenabling multiple agents to share visual information with limited bandwidthresources. While prior work has explored the empirical trade-off between taskperformance and communication volume, a significant gap remains in thetheoretical foundation. To fill this gap, we draw on information theory andintroduce a pragmatic rate-distortion theory for multi-agent collaboration,specifically formulated to analyze performance-communication trade-off ingoal-oriented multi-agent systems. This theory concretizes two key conditionsfor designing optimal communication strategies: supplying pragmaticallyrelevant information and transmitting redundancy-less messages. Guided by thesetwo conditions, we propose RDcomm, a communication-efficient collaborativeperception framework that introduces two key innovations: i) task entropydiscrete coding, which assigns features with task-relevant codeword-lengths tomaximize the efficiency in supplying pragmatic information; ii)mutual-information-driven message selection, which utilizes mutual informationneural estimation to approach the optimal redundancy-less condition.Experiments on 3D object detection and BEV segmentation demonstrate that RDcommachieves state-of-the-art accuracy on DAIR-V2X and OPV2V, while reducingcommunication volume by up to 108 times. The code will be released.</description>
      <author>example@mail.com (Genjia Liu, Anning Hu, Yue Hu, Wenjun Zhang, Siheng Chen)</author>
      <guid isPermaLink="false">2509.21994v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation</title>
      <link>http://arxiv.org/abs/2509.22460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了GeoSketch，一个神经符号框架，用于解决几何问题。该框架将几何推理重新定义为交互式的感知-推理-行动循环，整合了感知模块、符号推理模块和草图行动模块。通过两阶段训练方法和GeoSketch基准测试，GeoSketch显著提高了多模态大语言模型在几何问题解决上的性能。&lt;h4&gt;背景&lt;/h4&gt;几何问题解决（GPS）对多模态大语言模型（MLLMs）构成独特挑战，不仅需要联合解释文本和图表，还需要迭代的空间推理。现有方法将图表处理为静态图像，缺乏动态操作能力，而这正是人类几何推理的核心方面，涉及辅助线构建和仿射变换。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理几何问题解决中动态操作需求的框架，特别是辅助线构建和仿射变换，以提升多模态大语言模型在几何问题上的推理能力。&lt;h4&gt;方法&lt;/h4&gt;GeoSketch是一个神经符号框架，包含三个主要模块：1) 感知模块：将图表抽象为结构化逻辑形式；2) 符号推理模块：应用几何定理决定下一个推理步骤；3) 草图行动模块：执行绘制辅助线或应用变换等操作，更新图表。训练采用两阶段流程：首先在2000个符号化整理的轨迹上进行监督微调，然后使用密集的符号奖励进行强化学习，以提高鲁棒性和策略探索能力。评估使用了GeoSketch基准测试，包含390个需要辅助构建或仿射变换的高质量几何问题。&lt;h4&gt;主要发现&lt;/h4&gt;在强大的MLLM基线模型上的实验表明，GeoSketch显著提高了逐步推理准确性和问题解决成功率，优于静态感知方法。&lt;h4&gt;结论&lt;/h4&gt;GeoSketch通过统一分层决策、可执行视觉动作和符号验证，将多模态推理从静态解释提升到动态、可验证的交互，为解决复杂空间视觉问题建立了新基础。&lt;h4&gt;翻译&lt;/h4&gt;几何问题解决（GPS）对多模态大语言模型（MLLMs）提出了独特挑战，不仅需要联合解释文本和图表，还需要迭代的空间推理。虽然现有方法将图表处理为静态图像，但它们缺乏动态操作能力——这是人类几何推理的一个核心方面，涉及辅助线构建和仿射变换。我们提出了GeoSketch，一个神经符号框架，将几何推理重新定义为交互式的感知-推理-行动循环。GeoSketch整合：（1）将图表抽象为结构化逻辑形式的感知模块，（2）应用几何定理决定下一个推理步骤的符号推理模块，以及（3）执行绘制辅助线或应用变换等操作的草图行动模块，从而在闭环中更新图表。为了训练这个智能体，我们开发了一个两阶段流程：首先在2000个符号化整理的轨迹上进行监督微调，然后使用密集的符号奖励进行强化学习，以提高鲁棒性和策略探索。为了评估这一范式，我们引入了GeoSketch基准测试，一个包含390个需要辅助构建或仿射变换的高质量几何问题集合。在强大的MLLM基线模型上的实验表明，GeoSketch显著提高了逐步推理准确性和问题解决成功率，优于静态感知方法。通过统一分层决策、可执行视觉动作和符号验证，GeoSketch将多模态推理从静态提升到动态、可验证的交互，为解决复杂空间视觉问题建立了新基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric Problem Solving (GPS) poses a unique challenge for Multimodal LargeLanguage Models (MLLMs), requiring not only the joint interpretation of textand diagrams but also iterative visuospatial reasoning. While existingapproaches process diagrams as static images, they lack the capacity fordynamic manipulation - a core aspect of human geometric reasoning involvingauxiliary line construction and affine transformations. We present GeoSketch, aneural-symbolic framework that recasts geometric reasoning as an interactiveperception-reasoning-action loop. GeoSketch integrates: (1) a Perception modulethat abstracts diagrams into structured logic forms, (2) a Symbolic Reasoningmodule that applies geometric theorems to decide the next deductive step, and(3) a Sketch Action module that executes operations such as drawing auxiliarylines or applying transformations, thereby updating the diagram in a closedloop. To train this agent, we develop a two-stage pipeline: supervisedfine-tuning on 2,000 symbolic-curated trajectories followed by reinforcementlearning with dense, symbolic rewards to enhance robustness and strategicexploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, ahigh-quality set of 390 geometry problems requiring auxiliary construction oraffine transformations. Experiments on strong MLLM baselines demonstrate thatGeoSketch significantly improves stepwise reasoning accuracy andproblem-solving success over static perception methods. By unifyinghierarchical decision-making, executable visual actions, and symbolicverification, GeoSketch advances multimodal reasoning from staticinterpretation to dynamic, verifiable interaction, establishing a newfoundation for solving complex visuospatial problems.</description>
      <author>example@mail.com (Shichao Weng, Zhiqiang Wang, Yuhua Zhou, Rui Lu, Ting Liu, Zhiyang Teng, Xiaozhang Liu, Hanmeng Liu)</author>
      <guid isPermaLink="false">2509.22460v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective</title>
      <link>http://arxiv.org/abs/2509.22228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanFeel是一个全面的基准测试，用于评估多模态大语言模型(MLLMs)在城市发展理解和主观环境感知方面的表现。通过评估20个最先进的模型，发现Gemini-2.5 Pro总体表现最佳，准确度接近人类专家水平，但在需要时间推理的城市发展任务中性能显著下降。&lt;h4&gt;背景&lt;/h4&gt;城市发展影响着全球一半以上的人口，对城市结构和感知变化的人本主义理解对可持续发展至关重要。尽管MLLMs在各个领域展现出显著能力，但现有探索MLLMs在城市环境中表现的基准测试有限，缺乏对城市环境时间演变和主观感知的系统探索。&lt;h4&gt;目的&lt;/h4&gt;提出UrbanFeel，一个全面的基准测试，用于评估MLLMs在城市发展理解和主观环境感知方面的表现，填补现有评估空白。&lt;h4&gt;方法&lt;/h4&gt;UrbanFeel包含14.3K个精心构建的视觉问题，跨越三个认知渐进维度：静态场景感知、时间变化理解和主观环境感知。研究从全球11个代表性城市收集多时序单视图和全景街景图像，并通过空间聚类、基于规则的生成、模型辅助提示和人工注释的混合流程生成高质量问答对。&lt;h4&gt;主要发现&lt;/h4&gt;Gemini-2.5 Pro总体表现最佳，准确度接近人类专家水平，平均差距仅1.5%。大多数模型在基于场景理解的任务上表现良好，一些模型甚至在像素级变化检测中超越了人类注释者。然而，在需要时间推理的城市发展任务中，性能显著下降。在主观感知维度，几个模型在美丽和安全等评估维度上达到或超过了人类水平的一致性。&lt;h4&gt;结论&lt;/h4&gt;UrbanFill基准测试揭示了当前MLLMs在城市环境理解中的优势与局限，特别是在时间推理方面的不足，为未来模型改进提供了明确方向。&lt;h4&gt;翻译&lt;/h4&gt;城市发展影响着全球一半以上的人口，使人本主义理解其结构和感知变化对可持续发展至关重要。虽然多模态大语言模型(MLLMs)已在各个领域展现出显著能力，但现有探索它们在城市环境中表现的基准测试仍然有限，缺乏对城市环境时间演变和主观感知的系统探索，这些探索应与人类感知相一致。为解决这些限制，我们提出了UrbanFeel，一个全面的基准测试，旨在评估MLLMs在城市发展理解和主观环境感知方面的表现。UrbanFeel包含14.3K个精心构建的视觉问题，跨越三个认知渐进维度：静态场景感知、时间变化理解和主观环境感知。我们从全球11个代表性城市收集多时序单视图和全景街景图像，并通过空间聚类、基于规则的生成、模型辅助提示和人工注释的混合流程生成高质量问答对。通过对20个最先进的MLLMs进行广泛评估，我们观察到Gemini-2.5 Pro取得了最佳总体表现，其准确度接近人类专家水平，将平均差距缩小至仅1.5%。大多数模型在基于场景理解的任务上表现良好。特别是，一些模型甚至在像素级变化检测中超越了人类注释者。然而，在需要时间推理的城市发展任务中，性能显著下降。此外，在主观感知维度，几个模型在美丽和安全等评估维度上达到或超过了人类水平的一致性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban development impacts over half of the global population, makinghuman-centered understanding of its structural and perceptual changes essentialfor sustainable development. While Multimodal Large Language Models (MLLMs)have shown remarkable capabilities across various domains, existing benchmarksthat explore their performance in urban environments remain limited, lackingsystematic exploration of temporal evolution and subjective perception of urbanenvironment that aligns with human perception. To address these limitations, wepropose UrbanFeel, a comprehensive benchmark designed to evaluate theperformance of MLLMs in urban development understanding and subjectiveenvironmental perception. UrbanFeel comprises 14.3K carefully constructedvisual questions spanning three cognitively progressive dimensions: StaticScene Perception, Temporal Change Understanding, and Subjective EnvironmentalPerception. We collect multi-temporal single-view and panoramic street-viewimages from 11 representative cities worldwide, and generate high-qualityquestion-answer pairs through a hybrid pipeline of spatial clustering,rule-based generation, model-assisted prompting, and manual annotation. Throughextensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5Pro achieves the best overall performance, with its accuracy approaching humanexpert levels and narrowing the average gap to just 1.5\%. Most models performwell on tasks grounded in scene understanding. In particular, some models evensurpass human annotators in pixel-level change detection. However, performancedrops notably in tasks requiring temporal reasoning over urban development.Additionally, in the subjective perception dimension, several models reachhuman-level or even higher consistency in evaluating dimension such asbeautiful and safety.</description>
      <author>example@mail.com (Jun He, Yi Lin, Zilong Huang, Jiacong Yin, Junyan Ye, Yuchuan Zhou, Weijia Li, Xiang Zhang)</author>
      <guid isPermaLink="false">2509.22228v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics</title>
      <link>http://arxiv.org/abs/2509.22014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种轻量级智能体多模态框架，用于医疗机器人的视频场景理解，结合Qwen2.5-VL-3B-Instruct模型与SmolAgent编排层，支持思维链推理、语音视觉融合和动态工具调用，在Video-MME基准测试和临床数据集上展现出竞争性准确性和改进的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;医疗机器人需要在动态临床环境中具有强大的多模态感知和推理能力以确保安全。当前的视觉-语言模型虽然展示了强大的通用能力，但在时间推理、不确定性估计和机器人规划所需的结构化输出方面仍然存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一个轻量级的智能体多模态框架，用于基于视频的场景理解，以解决当前VLMs在医疗机器人应用中的限制。&lt;h4&gt;方法&lt;/h4&gt;将Qwen2.5-VL-3B-Instruct模型与基于SmolAgent的编排层相结合，支持思维链推理、语音视觉融合和动态工具调用。该框架生成结构化场景图，并利用混合检索模块进行可解释和自适应的推理。&lt;h4&gt;主要发现&lt;/h4&gt;在Video-MME基准测试和自定义临床数据集上的评估表明，与最先进的VLMs相比，该框架具有竞争性的准确性和改进的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该框架展示了在机器人辅助手术、患者监测和决策支持中应用的潜力，为医疗机器人提供了更安全可靠的多模态感知和推理能力。&lt;h4&gt;翻译&lt;/h4&gt;医疗机器人需要在动态临床环境中具有强大的多模态感知和推理能力以确保安全。当前的视觉-语言模型展示了强大的通用能力，但在时间推理、不确定性估计和机器人规划所需的结构化输出方面仍然存在局限性。我们提出了一个用于视频场景理解的轻量级智能体多模态框架。将Qwen2.5-VL-3B-Instruct模型与基于SmolAgent的编排层相结合，它支持思维链推理、语音视觉融合和动态工具调用。该框架生成结构化场景图，并利用混合检索模块进行可解释和自适应的推理。在Video-MME基准测试和自定义临床数据集上的评估显示，与最先进的VLMs相比具有竞争性的准确性和改进的鲁棒性，展示了其在机器人辅助手术、患者监测和决策支持中的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Healthcare robotics requires robust multimodal perception and reasoning toensure safety in dynamic clinical environments. Current Vision-Language Models(VLMs) demonstrate strong general-purpose capabilities but remain limited intemporal reasoning, uncertainty estimation, and structured outputs needed forrobotic planning. We present a lightweight agentic multimodal framework forvideo-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct modelwith a SmolAgent-based orchestration layer, it supports chain-of-thoughtreasoning, speech-vision fusion, and dynamic tool invocation. The frameworkgenerates structured scene graphs and leverages a hybrid retrieval module forinterpretable and adaptive reasoning. Evaluations on the Video-MME benchmarkand a custom clinical dataset show competitive accuracy and improved robustnesscompared to state-of-the-art VLMs, demonstrating its potential for applicationsin robot-assisted surgery, patient monitoring, and decision support.</description>
      <author>example@mail.com (Saurav Jha, Stefan K. Ehrlich)</author>
      <guid isPermaLink="false">2509.22014v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding</title>
      <link>http://arxiv.org/abs/2509.21922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, NeurIPS Workshop SpaVLE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了基础模型对物体为中心的空间推理能力，发现现有模型在定位精度和空间理解之间存在权衡，真正的空间理解能力仍然不足。&lt;h4&gt;背景&lt;/h4&gt;空间理解是视觉基础模型的关键能力。最近大型视觉模型或视觉语言模型(VLMs)的发展扩展了识别能力，但大多数基准测试强调定位精度，而非模型是否捕捉到场景中物体的排列和关系。&lt;h4&gt;目的&lt;/h4&gt;提出一个针对基础模型中物体为中心的空间推理的系统性基准测试，评估模型是否真正理解物体在场景中的相对位置、分组和深度。&lt;h4&gt;方法&lt;/h4&gt;使用受控合成数据集，评估最先进的视觉模型(如GroundingDINO, Florence-2, OWLv2)和大型VLMs(如InternVL, LLaVA, GPT-4o)在三个任务上的表现：空间定位、空间推理和下游检索任务。&lt;h4&gt;主要发现&lt;/h4&gt;存在稳定的权衡关系：检测器如GroundingDINO和OWLv2提供精确的边界框但有限的推理能力，而VLMs如SmolVLM和GPT-4o提供粗略的布局线索和流畅的描述，但在精细的空间上下文中表现不佳。&lt;h4&gt;结论&lt;/h4&gt;研究突显了定位和真正空间理解之间的差距，指向社区需要开发具有空间感知能力的基础模型。&lt;h4&gt;翻译&lt;/h4&gt;空间理解是视觉基础模型的关键能力。虽然最近大型视觉模型或视觉语言模型(VLMs)的进展扩展了识别能力，但大多数基准测试强调定位精度，而非模型是否捕捉到场景中物体的排列和关系。这一差距很重要；有效的场景理解不仅需要识别物体，还需要推理它们的相对位置、分组和深度。在本文中，我们提出了一个针对基础模型中物体为中心的空间推理的系统性基准。使用受控合成数据集，我们评估了最先进的视觉模型(如GroundingDINO, Florence-2, OWLv2)和大型VLMs(如InternVL, LLaVA, GPT-4o)在三个任务上的表现：空间定位、空间推理和下游检索任务。我们发现存在稳定的权衡关系：检测器如GroundingDINO和OWLv2提供精确的边界框但有限的推理能力，而VLMs如SmolVLM和GPT-4o提供粗略的布局线索和流畅的描述，但在精细的空间上下文中表现不佳。我们的研究突显了定位和真正空间理解之间的差距，并指向社区需要开发具有空间感知能力的基础模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基础模型（特别是视觉模型和视觉-语言模型）中的空间推理能力评估问题，即模型是否能理解物体在场景中的排列方式和相互关系，而不仅仅是精确定位物体。这个问题很重要，因为有效的场景理解需要识别物体并理解其相对位置、分组和深度，这对电商推荐、人机交互、场景检索和具身AI等应用至关重要。例如，在购物场景中，沙发与咖啡桌的相对位置会影响推荐系统提供的相关商品建议。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有基准测试的局限性（主要强调定位准确性而非空间关系推理）来设计这个方法。他们创建了一个受控的合成数据集，包含9个家具类别的3D渲染图像，通过随机旋转、位移和缩放增加多样性，并与背景场景合成。他们借鉴了现有的开放词汇检测模型（如OWL-ViT、OWLv2）和视觉-语言模型（如LLaVA、InternVL）的评估方法，但创新性地设计了专门针对空间推理的评估任务和指标，包括空间定位、空间推理和下游检索任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是评估模型不仅能否精确定位物体，还能否理解物体之间的空间关系和上下文。整体流程包括：1) 数据生成：创建合成数据集，包含数据库图像（正面视图）和查询图像（角度视图）；2) 模型评估：评估14个模型（分为任务特定视觉模型和通用视觉-语言模型）在三种任务上的表现；3) 使用多种指标评估结果：空间定位使用准确率、macro-F1、MCC，空间推理使用准确率、精确率、召回率、F1，检索任务使用Precision@k和Hit@k。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出首个系统化的空间推理基准测试；2) 使用受控合成数据集精确控制变量；3) 同时评估多种类型的模型（视觉检测模型和视觉-语言模型）；4) 揭示精确定位与真实空间理解之间的差距；5) 提供标准化任务和指标。相比之前的工作，这篇论文不局限于定位准确性评估，而是专注于空间关系推理；不依赖真实世界数据集的不确定性，而是使用合成数据集进行受控评估；不仅评估单一模型类型，而是全面比较不同模型家族的优势和局限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出一个系统化的空间推理基准测试，揭示了当前基础模型在理解物体空间关系方面的局限性，为开发能够结合精确定位与空间上下文理解的新型基础模型奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial understanding is a critical capability for vision foundation models.While recent advances in large vision models or vision-language models (VLMs)have expanded recognition capabilities, most benchmarks emphasize localizationaccuracy rather than whether models capture how objects are arranged andrelated within a scene. This gap is consequential; effective sceneunderstanding requires not only identifying objects, but reasoning about theirrelative positions, groupings, and depth. In this paper, we present asystematic benchmark for object-centric spatial reasoning in foundation models.Using a controlled synthetic dataset, we evaluate state-of-the-art visionmodels (e.g., GroundingDINO, Florence-2, OWLv2) and large VLMs (e.g., InternVL,LLaVA, GPT-4o) across three tasks: spatial localization, spatial reasoning, anddownstream retrieval tasks. We find a stable trade-off: detectors such asGroundingDINO and OWLv2 deliver precise boxes with limited relationalreasoning, while VLMs like SmolVLM and GPT-4o provide coarse layout cues andfluent captions but struggle with fine-grained spatial context. Our studyhighlights the gap between localization and true spatial understanding, andpointing toward the need for spatially-aware foundation models in thecommunity.</description>
      <author>example@mail.com (Vahid Mirjalili, Ramin Giahi, Sriram Kollipara, Akshay Kekuda, Kehui Yao, Kai Zhao, Jianpeng Xu, Kaushiki Nag, Sinduja Subramaniam, Topojoy Biswas, Evren Korpeoglu, Kannan Achan)</author>
      <guid isPermaLink="false">2509.21922v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment</title>
      <link>http://arxiv.org/abs/2509.21919v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个根据文本提示可控生成移动声音的框架，通过构建合成数据集训练文本到轨迹预测模型，并结合预训练的文本到音频生成模型，实现了空间音频的生成。&lt;h4&gt;背景&lt;/h4&gt;人类听觉感知受到3D空间中移动声源的影响，但之前在生成式声音建模方面的工作主要局限于单声道信号或静态空间音频。&lt;h4&gt;目的&lt;/h4&gt;引入一个框架，可以根据文本提示以可控方式生成移动声音。&lt;h4&gt;方法&lt;/h4&gt;构建一个合成数据集，记录双耳格式的移动声音、它们的空间轨迹和文本描述；训练文本到轨迹预测模型；微调预训练的文本到音频生成模型以输出与轨迹时间对齐的单声道声音；使用预测的时间对齐轨迹模拟空间音频。&lt;h4&gt;主要发现&lt;/h4&gt;文本到轨迹模型展现出合理的空间理解能力。&lt;h4&gt;结论&lt;/h4&gt;该方法可以轻松集成到现有的文本到音频生成工作流程中，并可以扩展到其他空间音频格式中的移动声音生成。&lt;h4&gt;翻译&lt;/h4&gt;人类的听觉感知受到3D空间中移动声源的影响，然而在生成式声音建模方面之前的工作主要局限于单声道信号或静态空间音频。在这项工作中，我们引入了一个框架，可以根据文本提示以可控方式生成移动声音。为了使训练成为可能，我们构建了一个合成数据集，记录了双耳格式的移动声音、它们的空间轨迹以及关于声音事件和空间运动的文本描述。使用这个数据集，我们训练了一个文本到轨迹预测模型，该模型根据文本提示输出移动声源的三维轨迹。为了生成空间音频，我们首先微调了一个预训练的文本到音频生成模型，使其输出与轨迹时间对齐的单声道声音。然后使用预测的时间对齐轨迹来模拟空间音频。实验评估表明文本到轨迹模型具有合理的空间理解能力。这种方法可以轻松集成到现有的文本到音频生成工作流程中，并扩展到其他空间音频格式中的移动声音生成。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何根据文本描述生成具有空间移动特性的声音问题。这个问题很重要，因为人类听觉感知受到3D空间中移动声源的影响，而现有生成声音模型主要局限于单声道信号或静态空间音频，无法处理现实中许多不断移动的声源。空间音频在导航、媒体沉浸体验等日常应用中扮演着重要角色。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为两个组件：文本到轨迹预测模型和同步的文本到音频管道。他们借鉴了文本到空间声音建模、对象音频方法、轨迹预测和声音合成等领域的现有工作。特别是借鉴了机器人、自动驾驶等领域中从语言预测运动轨迹的方法，以及预训练的文本到音频生成模型。作者还构建了专门的合成数据集来支持训练和评估。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过文本预测声源在3D空间中的移动轨迹，生成与轨迹时间对齐的单声道声音，然后根据预测轨迹模拟空间音频。实现流程包括：1)文本到轨迹预测模型，包含文本语义编码器、时间编码器和轨迹解码器；2)微调预训练的文本到音频生成模型并添加时间调整机制；3)使用预测轨迹对生成的单声道声音进行空间化处理；4)构建包含双耳音频、空间轨迹和文本描述的合成数据集。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)第一个明确将文本、轨迹和音频统一在空间音频生成框架中的工作；2)将问题分解为轨迹预测和时间对齐两个组件，而非端到端生成；3)构建了专门的合成数据集，包含双耳音频、空间轨迹和文本描述；4)引入时间调整机制，使生成的音频与预测轨迹精确对齐；5)实现了对移动声音的精确控制和生成，而之前的工作主要关注静态空间音频。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Text2Move通过文本预测声源空间轨迹并生成与轨迹时间对齐的空间音频，实现了对移动声音的精确控制和生成。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human auditory perception is shaped by moving sound sources in 3D space, yetprior work in generative sound modelling has largely been restricted to monosignals or static spatial audio. In this work, we introduce a framework forgenerating moving sounds given text prompts in a controllable fashion. Toenable training, we construct a synthetic dataset that records moving sounds inbinaural format, their spatial trajectories, and text captions about the soundevent and spatial motion. Using this dataset, we train a text-to-trajectoryprediction model that outputs the three-dimensional trajectory of a movingsound source given text prompts. To generate spatial audio, we first fine-tunea pre-trained text-to-audio generative model to output temporally aligned monosound with the trajectory. The spatial audio is then simulated using thepredicted temporally-aligned trajectory. Experimental evaluation demonstratesreasonable spatial understanding of the text-to-trajectory model. This approachcould be easily integrated into existing text-to-audio generative workflow andextended to moving sound generation in other spatial audio formats.</description>
      <author>example@mail.com (Yunyi Liu, Shaofan Yang, Kai Li, Xu Li)</author>
      <guid isPermaLink="false">2509.21919v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Indoor Object SLAM with LLM-Enhanced Priors</title>
      <link>http://arxiv.org/abs/2509.21602v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用大型语言模型（LLMs）提供对象几何属性常识知识作为先验因素的方法，解决了对象级SLAM中因稀疏观测导致的优化约束不足问题，并在TUM RGB-D和3RScan数据集上将映射精度提高了36.8%。&lt;h4&gt;背景&lt;/h4&gt;对象级SLAM结合语义信息进行高级场景理解，但由于稀疏观测面临优化约束不足的挑战。之前的工作使用常识知识引入额外约束，但获取这些先验知识劳动密集且缺乏跨对象类别的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决获取常识先验知识劳动密集且泛化能力有限的问题，利用大型语言模型（LLMs）提供对象几何属性的常识知识作为先验因素，提高对象级SLAM的性能。&lt;h4&gt;方法&lt;/h4&gt;利用大型语言模型（LLMs）提供对象几何属性（大小和方向）的常识知识，在基于图的SLAM框架中将这些知识作为先验因素，实现完整流程整合这些先验知识，在稀疏对象级特征上实现鲁棒的数据关联，支持实时对象SLAM。&lt;h4&gt;主要发现&lt;/h4&gt;这些先验知识在对象观测有限的初始阶段特别有益；在TUM RGB-D和3RScan数据集上评估的系统比最新基线提高映射精度36.8%；实验展示了系统的实时性能。&lt;h4&gt;结论&lt;/h4&gt;使用大型语言模型提供常识先验知识可以显著提高对象级SLAM的性能；该方法解决了传统获取先验知识的劳动密集和泛化能力有限的问题。&lt;h4&gt;翻译&lt;/h4&gt;对象级同步定位与地图构建（SLAM）结合语义信息进行高级场景理解，但由于稀疏观测面临优化约束不足的挑战。先前的工作使用常识知识引入额外约束，但获取此类先验知识传统上劳动密集且缺乏跨不同对象类别的泛化能力。我们通过利用大型语言模型（LLMs）提供对象几何属性的常识知识，作为基于图的SLAM框架中的先验因素，来解决这一局限性。这些先验知识在对象观测有限的初始阶段特别有益。我们实现了一个整合这些先验因素的完整流程，实现了在稀疏对象级特征上的鲁棒数据关联，并支持实时对象SLAM。我们的系统在TUM RGB-D和3RScan数据集上评估，比最新基线提高映射精度36.8%。此外，我们在补充视频中展示了真实世界实验，证明了其实时性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决物体级SLAM中因观测稀疏导致的优化约束不足问题。这个问题很重要，因为在实际应用中，当相机帧率低或机器人移动快时，物体观测往往有限，这会影响初始建图质量，阻碍后续优化和数据关联，进而影响机器人导航、物体搜索等下游任务的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了物体级SLAM面临的核心问题是观测稀疏导致的约束不足，然后考虑引入常识知识作为额外约束。他们创新性地利用大型语言模型(LLM)自动提供这些先验知识，而不是依赖传统的人工标注方法。该方法借鉴了现有的QuadricSLAM和CubeSLAM框架、因子图优化技术、视觉里程计(ORB-SLAM3)和物体检测(YOLO)等现有工作，但将它们与LLM先验知识进行了创新性整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大型语言模型提供的常识知识(特别是物体的大小和方向)作为先验约束，整合到因子图SLAM框架中，解决稀疏观测下的优化问题。整体流程包括：1)RGB-D输入；2)ORB-SLAM3视觉里程计；3)YOLO物体检测；4)短期物体跟踪；5)长期物体关联；6)向LLM查询物体常识先验；7)构建包含里程计、观测和先验的因子图；8)使用iSAM2增量优化；9)输出物体级地图和相机轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将LLM提供的常识知识直接整合到SLAM系统；2)利用LLM生成物体几何先验作为因子图约束；3)设计了完整的从先验嵌入到优化的流程；4)实现了实时物体级SLAM系统。相比之前工作，不同之处在于：先验知识获取从人工标注变为LLM自动生成；先验应用从仅用于初始化扩展到整个优化过程；提供了完整的实时系统而非单一组件；在数据集上实现了36.8%的精度提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种利用大型语言模型提供的常识知识作为先验约束的实时物体级SLAM方法，显著提高了稀疏观测场景下的建图精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object-level Simultaneous Localization and Mapping (SLAM), which incorporatessemantic information for high-level scene understanding, faces challenges ofunder-constrained optimization due to sparse observations. Prior work hasintroduced additional constraints using commonsense knowledge, but obtainingsuch priors has traditionally been labor-intensive and lacks generalizabilityacross diverse object categories. We address this limitation by leveraginglarge language models (LLMs) to provide commonsense knowledge of objectgeometric attributes, specifically size and orientation, as prior factors in agraph-based SLAM framework. These priors are particularly beneficial during theinitial phase when object observations are limited. We implement a completepipeline integrating these priors, achieving robust data association on sparseobject-level features and enabling real-time object SLAM. Our system, evaluatedon the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\% overthe latest baseline. Additionally, we present real-world experiments in thesupplementary video, demonstrating its real-time performance.</description>
      <author>example@mail.com (Yang Jiao, Yiding Qiu, Henrik I. Christensen)</author>
      <guid isPermaLink="false">2509.21602v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Residual Vector Quantization For Communication-Efficient Multi-Agent Perception</title>
      <link>http://arxiv.org/abs/2509.21464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ReVQom是一种创新的多智能体协同感知特征压缩方法，通过学习到的特征编解码器在保持空间身份的同时大幅压缩特征数据，解决了通信带宽限制问题，实现了高效且准确的多智能体协同感知。&lt;h4&gt;背景&lt;/h4&gt;多智能体协同感知(CP)通过连接的智能体(如自动驾驶汽车、无人机和机器人)共享信息来提高场景理解能力，但通信带宽限制了其可扩展性。&lt;h4&gt;目的&lt;/h4&gt;提出ReVQom，一种学习到的特征编解码器，在压缩中间特征的同时保持空间身份，以解决通信带宽限制问题。&lt;h4&gt;方法&lt;/h4&gt;ReVQom是一种端到端方法，通过简单的瓶颈网络压缩特征维度，然后进行多阶段残差向量量化(RVQ)，只传输每个像素的代码索引来减少数据负载。&lt;h4&gt;主要发现&lt;/h4&gt;ReVQom将未压缩的32位浮点特征从每像素8192位压缩到每智能体6-30位，精度损失最小；在DAIR-V2X数据集上，30 bpp时实现273倍压缩，6 bpp时实现1365倍压缩；18 bpp时匹配或优于原始特征CP；6-12 bpp时实现超低带宽操作，性能优雅降级。&lt;h4&gt;结论&lt;/h4&gt;ReVQom实现了高效且准确的多智能体协同感知，为V2X(车对万物)的实际部署提供了可能性。&lt;h4&gt;翻译&lt;/h4&gt;多智能体协同感知(CP)通过连接的智能体(如自动驾驶汽车、无人机和机器人)共享信息来提高场景理解能力。然而，通信带宽限制了其可扩展性。我们提出了ReVQom，一种学习到的特征编解码器，在压缩中间特征的同时保持空间身份。ReVQom是一种端到端方法，通过简单的瓶颈网络压缩特征维度，然后进行多阶段残差向量量化(RVQ)。这使得只需传输每个像素的代码索引，将未压缩的32位浮点特征从每像素8192位减少到每智能体6-30位，精度损失最小。在DAIR-V2X真实世界CP数据集上，ReVQom在30 bpp时实现273倍压缩，在6 bpp时实现1365倍压缩。在18 bpp(455倍)时，ReVQom匹配或优于原始特征CP，在6-12 bpp时，它实现了超低带宽操作，性能优雅降级。ReVQom实现了高效且准确的多智能体协同感知，为V2X的实际部署迈出了一步。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多智能体协作感知中的通信带宽限制问题。在自动驾驶、无人机和机器人等场景中，多个智能体需要共享信息来提高场景理解能力，但原始特征数据量巨大（每像素8192位），严重限制了这种协作的扩展性和实际应用。这个问题在现实中非常重要，因为它阻碍了多智能体感知技术的规模化部署，特别是在带宽受限的V2X（车对万物）通信环境中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，发现它们主要关注'传输什么特征'而非'如何高效压缩特征'。然后提出使用残差向量量化（RVQ）来实现高效压缩。该方法借鉴了向量量化的基本思想，但创新性地将其扩展为多阶段残差量化；同时采用指数移动平均（EMA）更新码本技术，这与一些自学习方法类似。作者还保留了鸟瞰图（BEV）特征表示，与CoBEVT等多智能体融合框架一致，但将这些技术组合创新，形成了专门针对多智能体协作感知的高效压缩方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多阶段残差向量量化将高维特征压缩为少量码本索引，只传输这些索引而非完整特征，从而大幅减少通信量，同时保持空间身份信息确保重构特征的准确性。整体流程：发送方先通过1×1卷积减少通道数，然后进行多阶段量化，每阶段找到最接近残差的码本向量，记录索引并更新残差，最后传输所有索引；接收方根据索引查找码本向量，累积这些向量，应用后处理重构特征，用于多智能体融合和检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)空间身份保持的编解码器，通过1×1瓶颈网络和多阶段RVQ保持空间结构；2)仅传输索引的通信方案，将通信量从8192bpp降至6-30bpp；3)实用化的多智能体BEV融合集成，在极低带宽下仍能工作；4)系统化的码本自适应机制，使用EMA更新码本。相比之前的工作，ReVQom同时解决了高压缩率和保持空间信息两个关键问题，在18bpp时性能甚至优于原始特征传输的协作感知方法，而Where2comm和What2comm等方法主要关注选择传输哪些特征而非如何压缩。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ReVQom通过创新的残差向量量化方法，实现了多智能体协作感知中特征数据的高效压缩，在保持空间结构的同时将通信量减少到原来的1/273至1/1365，为实际V2X部署铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent collaborative perception (CP) improves scene understanding bysharing information across connected agents such as autonomous vehicles,unmanned aerial vehicles, and robots. Communication bandwidth, however,constrains scalability. We present ReVQom, a learned feature codec thatpreserves spatial identity while compressing intermediate features. ReVQom isan end-to-end method that compresses feature dimensions via a simple bottlenecknetwork followed by multi-stage residual vector quantization (RVQ). This allowsonly per-pixel code indices to be transmitted, reducing payloads from 8192 bitsper pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agentwith minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x),ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enablesultra-low-bandwidth operation with graceful degradation. ReVQom allowsefficient and accurate multi-agent collaborative perception with a step towardpractical V2X deployment.</description>
      <author>example@mail.com (Dereje Shenkut, B. V. K Vijaya Kumar)</author>
      <guid isPermaLink="false">2509.21464v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers</title>
      <link>http://arxiv.org/abs/2509.21893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://jibin86.github.io/syncphony_project_page&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Syncphony是一种音频到视频生成模型，能够生成与音频精确同步的高质量视频，解决了现有方法在时序控制方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;文本到视频和图像到视频生成在视觉质量方面取得了进展，但在控制运动时序方面仍然有限。音频与视频运动时间线索对齐，是时序控制视频生成的有前景的条件。然而，现有的音频到视频模型由于间接的条件机制或有限的时序建模能力，难以实现细粒度的同步。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成与音频精确同步的高质量视频的模型，提高音频与视频之间的同步精度。&lt;h4&gt;方法&lt;/h4&gt;Syncphony构建在预训练的视频骨干模型之上，并包含两个关键组件：1) 运动感知损失，强调在高运动区域的学习；2) 音频同步引导，使用没有音频层的视觉对齐不同步模型引导整个模型，在推理时更好地利用音频线索，同时保持视觉质量。&lt;h4&gt;主要发现&lt;/h4&gt;提出了CycleSync评估指标，这是一种基于视频到音频的度量，用于测量生成视频中运动线索的数量以重建原始音频。在AVSync15和The Greatest Hits数据集上的实验表明，Syncphony在同步准确性和视觉质量方面都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;Syncphony是一种有效的音频到视频生成方法，能够生成380x640分辨率、24fps的高质量视频，并与各种音频输入精确同步，在同步准确性和视觉质量方面表现优异。&lt;h4&gt;翻译&lt;/h4&gt;文本到视频和图像到视频生成在视觉质量方面取得了快速进展，但它们在控制运动精确时序方面仍然有限。相比之下，音频提供了与视频运动对齐的时间线索，使其成为时序控制视频生成的有前景的条件。然而，现有的音频到视频模型由于间接的条件机制或有限的时序建模能力，在细粒度同步方面存在困难。我们提出了Syncphony，它可以生成与各种音频输入同步的380x640分辨率、24fps视频。我们的方法基于预训练的视频骨干模型，并包含两个关键组件来提高同步性：1) 运动感知损失，强调在高运动区域的学习；2) 音频同步引导，使用没有音频层的视觉对齐不同步模型引导整个模型，在推理时更好地利用音频线索，同时保持视觉质量。为了评估同步性，我们提出了CycleSync，这是一种基于视频到音频的指标，用于测量生成视频中运动线索的数量以重建原始音频。在AVSync15和The Greatest Hits数据集上的实验表明，Syncphony在同步准确性和视觉质量方面都优于现有方法。项目页面可在：https://jibin86.github.io/syncphony_project_page 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-video and image-to-video generation have made rapid progress invisual quality, but they remain limited in controlling the precise timing ofmotion. In contrast, audio provides temporal cues aligned with video motion,making it a promising condition for temporally controlled video generation.However, existing audio-to-video (A2V) models struggle with fine-grainedsynchronization due to indirect conditioning mechanisms or limited temporalmodeling capacity. We present Syncphony, which generates 380x640 resolution,24fps videos synchronized with diverse audio inputs. Our approach builds upon apre-trained video backbone and incorporates two key components to improvesynchronization: (1) Motion-aware Loss, which emphasizes learning athigh-motion regions; (2) Audio Sync Guidance, which guides the full model usinga visually aligned off-sync model without audio layers to better exploit audiocues at inference while maintaining visual quality. To evaluatesynchronization, we propose CycleSync, a video-to-audio-based metric thatmeasures the amount of motion cues in the generated video to reconstruct theoriginal audio. Experiments on AVSync15 and The Greatest Hits datasetsdemonstrate that Syncphony outperforms existing methods in both synchronizationaccuracy and visual quality. Project page is available at:https://jibin86.github.io/syncphony_project_page</description>
      <author>example@mail.com (Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh)</author>
      <guid isPermaLink="false">2509.21893v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Evaluation of Transformer-Based Question Answering Models and RAG-Enhanced Design</title>
      <link>http://arxiv.org/abs/2509.21845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了多跳问答中的检索策略，提出了一种结合密集嵌入与词汇重叠的混合方法，并通过优化EfficientRAG管道提高了检索效率。实验证明，该方法在HotpotQA数据集上显著优于传统方法，为多跳问答提供了准确、高效且可解释的零样本解决方案。&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的模型推动了问答领域的发展，但多跳推理（需要结合多个段落证据来回答问题）仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;本文对检索增强生成框架下的多跳问答检索策略进行了全面评估。&lt;h4&gt;方法&lt;/h4&gt;比较了余弦相似度、最大边际相关性和一种结合密集嵌入与词汇重叠及重排序的混合方法。为了进一步提高检索效果，调整了EfficientRAG管道进行查询优化，引入了标记标记和迭代优化，同时保持效率。&lt;h4&gt;主要发现&lt;/h4&gt;在HotpotQA数据集上的实验显示，混合方法显著优于基线方法，与余弦相似度相比，精确匹配相对提高了50%，F1分数相对提高了47%。错误分析表明，混合检索提高了实体召回率和证据互补性，但在处理干扰项和时间推理方面仍有局限。&lt;h4&gt;结论&lt;/h4&gt;总体而言，结果表明混合检索增强生成为多跳问答提供了一个实用的零样本解决方案，平衡了准确性、效率和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;基于Transformer的模型推动了问答领域的发展，但多跳推理（需要结合多个段落证据来回答问题）仍然具有挑战性。本文对检索增强生成框架下的多跳问答检索策略进行了全面评估。我们比较了余弦相似度、最大边际相关性和一种结合密集嵌入与词汇重叠及重排序的混合方法。为了进一步提高检索效果，我们调整了EfficientRAG管道进行查询优化，引入了标记标记和迭代优化，同时保持效率。在HotpotQA数据集上的实验显示，混合方法显著优于基线方法，与余弦相似度相比，精确匹配相对提高了50%，F1分数相对提高了47%。错误分析表明，混合检索提高了实体召回率和证据互补性，但在处理干扰项和时间推理方面仍有局限。总体而言，结果表明混合检索增强生成为多跳问答提供了一个实用的零样本解决方案，平衡了准确性、效率和可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformer-based models have advanced the field of question answering, butmulti-hop reasoning, where answers require combining evidence across multiplepassages, remains difficult. This paper presents a comprehensive evaluation ofretrieval strategies for multi-hop question answering within aretrieval-augmented generation framework. We compare cosine similarity, maximalmarginal relevance, and a hybrid method that integrates dense embeddings withlexical overlap and re-ranking. To further improve retrieval, we adapt theEfficientRAG pipeline for query optimization, introducing token labeling anditerative refinement while maintaining efficiency. Experiments on the HotpotQAdataset show that the hybrid approach substantially outperforms baselinemethods, achieving a relative improvement of 50 percent in exact match and 47percent in F1 score compared to cosine similarity. Error analysis reveals thathybrid retrieval improves entity recall and evidence complementarity, whileremaining limited in handling distractors and temporal reasoning. Overall, theresults suggest that hybrid retrieval-augmented generation provides a practicalzero-shot solution for multi-hop question answering, balancing accuracy,efficiency, and interpretability.</description>
      <author>example@mail.com (Zichen Zhang, Kunlong Zhang, Hongwei Ruan, Yiming Luo)</author>
      <guid isPermaLink="false">2509.21845v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-guided Representation Disentanglement for Action Recognition</title>
      <link>http://arxiv.org/abs/2509.21783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ProDA的新框架，可以从多动作场景中分离指定动作，通过时空场景图和动态提示模块引导图解析神经网络生成动作特定表示，在视频动作识别实验中证明了其有效性。&lt;h4&gt;背景&lt;/h4&gt;动作识别是视频理解的基本任务，现有方法通常提取统一特征来处理一个视频中的所有动作，这使得在多动作场景中建模不同对象之间的交互变得具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;探索从复杂场景中分离指定动作作为有效解决方案，以解决多动作场景中不同对象间交互建模的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了提示引导的解耦表示用于动作识别（ProDA）框架，利用时空场景图（SSGs）和引入动态提示模块（DPM）来引导图解析神经网络（GPNN）生成动作特定的表示，并设计了视频适配的GPNN，使用动态权重聚合信息。&lt;h4&gt;主要发现&lt;/h4&gt;在视频动作识别实验中，与最先进的方法相比，ProDA方法证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;ProDA框架能够有效处理多动作场景中的动作识别任务，通过分离指定动作并生成动作特定表示，解决了现有方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;动作识别是视频理解的基本任务。现有方法通常提取统一特征来处理一个视频中的所有动作，这使得在多动作场景中建模不同对象之间的交互变得具有挑战性。为了缓解这一问题，我们探索从复杂场景中分离任何指定动作作为有效解决方案。在本文中，我们提出了提示引导的解耦表示用于动作识别（ProDA），这是一个新框架，可以从多动作场景中分离任何指定的动作。ProDA利用时空场景图（SSGs）并引入动态提示模块（DPM）来引导图解析神经网络（GPNN）生成动作特定的表示。此外，我们设计了一个视频适配的GPNN，使用动态权重聚合信息。在视频动作识别实验中，与最先进的方法相比，我们的方法证明了其有效性。我们的代码可以在https://github.com/iamsnaping/ProDA.git找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Action recognition is a fundamental task in video understanding. Existingmethods typically extract unified features to process all actions in one video,which makes it challenging to model the interactions between different objectsin multi-action scenarios. To alleviate this issue, we explore disentanglingany specified actions from complex scenes as an effective solution. In thispaper, we propose Prompt-guided Disentangled Representation for ActionRecognition (ProDA), a novel framework that disentangles any specified actionsfrom a multi-action scene. ProDA leverages Spatio-temporal Scene Graphs (SSGs)and introduces Dynamic Prompt Module (DPM) to guide a Graph Parsing NeuralNetwork (GPNN) in generating action-specific representations. Furthermore, wedesign a video-adapted GPNN that aggregates information using dynamic weights.Experiments in video action recognition demonstrate the effectiveness of ourapproach when compared with the state-of-the-art methods. Our code can be foundin https://github.com/iamsnaping/ProDA.git</description>
      <author>example@mail.com (Tianci Wu, Guangming Zhu, Jiang Lu, Siyuan Wang, Ning Wang, Nuoye Xiong, Zhang Liang)</author>
      <guid isPermaLink="false">2509.21783v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis</title>
      <link>http://arxiv.org/abs/2509.21595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了DINOv3和V-JEPA2两种视频动作识别的自监督学习架构，在UCF Sports数据集上评估了它们的特征质量。&lt;h4&gt;背景&lt;/h4&gt;研究关注两种用于视频动作识别的自监督学习架构：DINOv3（通过空间特征提取独立处理帧）和V-JEPA2（在视频序列中采用联合时间建模）。&lt;h4&gt;目的&lt;/h4&gt;评估这两种方法在UCF Sports数据集上的特征质量，包括分类准确率、聚类性能、类内一致性和类间判别能力。&lt;h4&gt;方法&lt;/h4&gt;在UCF Sports数据集上评估DINOv3和V-JEPA2两种架构，通过多个维度分析特征质量。&lt;h4&gt;主要发现&lt;/h4&gt;DINOv3在聚类性能（轮廓分数：0.31比0.21）和判别能力（6.16倍分离比）上表现更好，特别是对于姿态可识别的动作；V-JEPA2在所有动作类型上表现出一致可靠性，性能方差显著更低（0.094比0.288）；DINOv3在静态姿态识别上表现优异，但在依赖于动作的任务上表现下降；V-JEPA2的时间建模提供了跨不同动作类别的平衡表示质量。&lt;h4&gt;结论&lt;/h4&gt;这些发现有助于理解视频分析系统中的架构设计选择，并根据任务要求和可靠性约束提供选择适当特征提取方法的实证指导。&lt;h4&gt;翻译&lt;/h4&gt;本研究对两种用于视频动作识别的 prominent 自监督学习架构进行了全面的比较分析：DINOv3，它通过空间特征提取独立处理帧；以及 V-JEPA2，它在视频序列中采用联合时间建模。我们在 UCF Sports 数据集上评估了这两种方法，通过多个维度检查特征质量，包括分类准确率、聚类性能、类内一致性和类间判别能力。我们的分析揭示了基本的架构权衡：DINOv3 实现了更好的聚类性能（轮廓分数：0.31 比 0.21）并表现出卓越的判别能力（6.16 倍分离比），特别是对于姿态可识别的动作，而 V-JEPA2 在所有动作类型上表现出一致的可靠性，性能方差显著更低（0.094 比 0.288）。通过针对特定动作的评估，我们发现 DINOv3 的空间处理架构在静态姿态识别方面表现出色，但在依赖于动作的任务上表现出性能下降，而 V-JEPA2 的时间建模提供了跨不同动作类别的平衡表示质量。这些发现有助于理解视频分析系统中的架构设计选择，并根据任务要求和可靠性约束为选择适当的特征提取方法提供实证指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a comprehensive comparative analysis of two prominentself-supervised learning architectures for video action recognition: DINOv3,which processes frames independently through spatial feature extraction, andV-JEPA2, which employs joint temporal modeling across video sequences. Weevaluate both approaches on the UCF Sports dataset, examining feature qualitythrough multiple dimensions including classification accuracy, clusteringperformance, intra-class consistency, and inter-class discrimination. Ouranalysis reveals fundamental architectural trade-offs: DINOv3 achieves superiorclustering performance (Silhouette score: 0.31 vs 0.21) and demonstratesexceptional discrimination capability (6.16x separation ratio) particularly forpose-identifiable actions, while V-JEPA2 exhibits consistent reliability acrossall action types with significantly lower performance variance (0.094 vs0.288). Through action-specific evaluation, we identify that DINOv3's spatialprocessing architecture excels at static pose recognition but shows degradedperformance on motion-dependent actions, whereas V-JEPA2's temporal modelingprovides balanced representation quality across diverse action categories.These findings contribute to the understanding of architectural design choicesin video analysis systems and provide empirical guidance for selectingappropriate feature extraction methods based on task requirements andreliability constraints.</description>
      <author>example@mail.com (Sai Varun Kodathala, Rakesh Vunnam)</author>
      <guid isPermaLink="false">2509.21595v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding</title>
      <link>http://arxiv.org/abs/2509.21451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VideoJudge，一个专门用于评估视频理解模型输出的3B和7B大小的多模态大型语言模型评估器，在多个基准测试中表现优于更大的评估器基线。&lt;h4&gt;背景&lt;/h4&gt;精确评估视频理解模型具有挑战性，常用指标如BLEU、ROUGE和BERTScore无法捕捉人类判断的细微差别，而人工评估成本高昂。&lt;h4&gt;目的&lt;/h4&gt;探索使用大型语言模型或多模态大型语言模型作为视频理解模型评估工具的可能性。&lt;h4&gt;方法&lt;/h4&gt;提出VideoJudge评估器，通过生成器和评估器之间的交互进行训练：生成器根据目标评分提示生成响应，不符合评估器评分的响应被丢弃。&lt;h4&gt;主要发现&lt;/h4&gt;VideoJudge-7B在四个元评估基准中的三个上表现优于Qwen2.5-VL(32B和72B)等更大的MLLM评估器基线；LLM评估器表现不如MLLM评估器；长链式推理不会提高性能。&lt;h4&gt;结论&lt;/h4&gt;VideoJudge是一个有效的视频理解模型评估工具，比更大的模型表现更好，并且强调了视频输入在评估中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;精确评估视频理解模型仍然具有挑战性：常用的指标如BLEU、ROUGE和BERTScore无法捕捉人类判断的细微差别，而通过人工评估获得这种判断成本高昂。最近的工作已经探索使用大型语言模型(LLMs)或多模态大型语言模型(MLLMs)作为评估工具，但它们在视频理解领域的扩展仍然相对未被探索。在这项工作中，我们引入了VideoJudge，一个专门用于评估视频理解模型输出的3B和7B大小的MLLM评估器(即基于视频条件的文本响应)。为了训练VideoJudge，我们的方法建立在生成器和评估者之间的互动基础上：生成器被提示根据目标评分产生响应，不符合评估者评分的响应被丢弃。在四个元评估基准中的三个上，VideoJudge-7B优于更大的MLLM评估器基线，如Qwen2.5-VL(32B和72B)。值得注意的是，我们发现LLM评估器(Qwen3)模型表现不如MLLM评估器(Qwen2.5-VL)，且长链式推理不会提高性能，这表明提供视频输入对于评估视频理解任务至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Precisely evaluating video understanding models remains challenging: commonlyused metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness ofhuman judgment, while obtaining such judgments through manual evaluation iscostly. Recent work has explored using large language models (LLMs) ormultimodal LLMs (MLLMs) as evaluators, but their extension to videounderstanding remains relatively unexplored. In this work, we introduceVideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs fromvideo understanding models (\textit{i.e.}, text responses conditioned onvideos). To train VideoJudge, our recipe builds on the interplay between agenerator and an evaluator: the generator is prompted to produce responsesconditioned on a target rating, and responses not matching the evaluator'srating are discarded. Across three out of four meta-evaluation benchmarks,VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32Band 72B). Notably, we find that LLM judges (Qwen3) models perform worse thanMLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improveperformance, indicating that providing video inputs is crucial for evaluationof video understanding tasks.</description>
      <author>example@mail.com (Abdul Waheed, Zhen Wu, Dareen Alharthi, Seungone Kim, Bhiksha Raj)</author>
      <guid isPermaLink="false">2509.21451v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2509.21113v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出MOSS-ChatV框架，通过基于动态时间规整的过程奖励解决了多模态大语言模型在视频推理过程中的不一致性问题，显著提升了模型对时间动态的理解能力和推理轨迹的稳定性。&lt;h4&gt;背景&lt;/h4&gt;视频推理已成为多模态大语言模型的关键能力，但现有模型往往表现出过程不一致性，即使最终答案正确，中间推理也会偏离视频动态，损害了模型的可解释性和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态大语言模型在视频推理过程中的不一致性问题，提高模型对视频中时间动态的理解能力和推理轨迹的稳定性。&lt;h4&gt;方法&lt;/h4&gt;引入MOSS-ChatV强化学习框架，采用基于动态时间规整的过程奖励机制，无需辅助奖励模型即可实现高效的过程监督；同时构建MOSS-Video基准数据集，包含标注的推理轨迹，用于模型训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;MOSS-ChatV在MOSS-Video测试集上达到87.2%的性能，在MVBench和MMVU等通用视频基准上表现提升；该框架在不同架构(包括Qwen2.5-VL和Phi-2)上均能带来性能提升；评估显示MOSS-ChatV产生更一致和稳定的推理轨迹。&lt;h4&gt;结论&lt;/h4&gt;MOSS-ChatV框架有效解决了多模态大语言模型在视频推理过程中的不一致性问题，具有广泛的架构适用性，通过提高推理过程的一致性增强了模型的可解释性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;视频推理已成为多模态大语言模型的关键能力，要求模型超越静态感知，转向对复杂场景中时间动态的连贯理解。然而，现有的多模态大语言模型通常表现出过程不一致性，即使最终答案正确，中间推理也会偏离视频动态，损害了可解释性和鲁棒性。为解决这一问题，我们引入了MOSS-ChatV，这是一个基于强化学习的框架，采用基于动态时间规整的过程奖励。这种基于规则的奖励将推理轨迹与时间上锚定的参考对齐，无需辅助奖励模型即可实现高效的过程监督。我们进一步将动态状态预测确定为视频推理的关键度量，并构建了MOSS-Video基准，其中包含标注的推理轨迹，训练集用于微调MOSS-ChatV，保留集用于评估。MOSS-ChatV在MOSS-Video(测试)上达到87.2%，并提高了在MVBench和MMVU等通用视频基准上的性能。该框架在不同架构(包括Qwen2.5-VL和Phi-2)上 consistently 带来提升，证实了其广泛的适用性。使用GPT-4o-as-judge的进一步评估表明，MOSS-ChatV产生更一致和稳定的推理轨迹。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video reasoning has emerged as a critical capability for multimodal largelanguage models (MLLMs), requiring models to move beyond static perceptiontoward coherent understanding of temporal dynamics in complex scenes. Yetexisting MLLMs often exhibit process inconsistency, where intermediatereasoning drifts from video dynamics even when the final answer is correct,undermining interpretability and robustness. To address this issue, weintroduce MOSS-ChatV, a reinforcement learning framework with a Dynamic TimeWarping (DTW)-based process reward. This rule-based reward aligns reasoningtraces with temporally grounded references, enabling efficient processsupervision without auxiliary reward models. We further identify dynamic stateprediction as a key measure of video reasoning and construct MOSS-Video, abenchmark with annotated reasoning traces, where the training split is used tofine-tune MOSS-ChatV and the held-out split is reserved for evaluation.MOSS-ChatV achieves 87.2\% on MOSS-Video (test) and improves performance ongeneral video benchmarks such as MVBench and MMVU. The framework consistentlyyields gains across different architectures, including Qwen2.5-VL and Phi-2,confirming its broad applicability. Evaluations with GPT-4o-as-judge furthershow that MOSS-ChatV produces more consistent and stable reasoning traces.</description>
      <author>example@mail.com (Sicheng Tao, Jungang Li, Yibo Yan, Junyan Zhang, Yubo Gao, Hanqian Li, ShuHang Xun, Yuxuan Fan, Hong Chen, Jianxiang He, Xuming Hu)</author>
      <guid isPermaLink="false">2509.21113v2</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement</title>
      <link>http://arxiv.org/abs/2509.22553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型线性因果表征学习算法，在较弱的假设条件下仍能恢复潜在因果特征，并通过实验验证了其优越性和在大型语言模型可解释性分析中的应用潜力。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习(CRL)在因果推理和人工智能领域受到越来越多的关注，因为它能够利用现代数据集的异质性，将复杂数据生成机制解构为可因果解释的潜在特征。&lt;h4&gt;目的&lt;/h4&gt;为CRL文献做出进一步贡献，专注于潜在特征上的风格化线性结构因果模型，并假设线性混合函数将潜在特征映射到观测数据。&lt;h4&gt;方法&lt;/h4&gt;提出一种新型线性CRL算法，与现有方法不同，它在关于环境异质性和数据生成分布的较弱假设下运行，同时仍能恢复到等价类的潜在因果特征。&lt;h4&gt;主要发现&lt;/h4&gt;新算法在有限样本中优于竞争方法，且在大型语言模型的可解释性分析中展现出将因果性集成到AI中的潜力。&lt;h4&gt;结论&lt;/h4&gt;该算法克服了现有线性CRL方法对单节点干预数据或潜在特征和外生测量噪声分布的严格依赖要求，为因果表征学习提供了更实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;因果表征学习(CRL)因其能够利用现代数据集的异质性，将潜在复杂的数据生成机制解构为可因果解释的潜在特征，从而在因果推理和人工智能社区获得了越来越多的关注。在本文中，我们通过关注潜在特征上的风格化线性结构因果模型并假设将潜在特征映射到观测数据或测量值的线性混合函数，进一步为CRL文献做出贡献。现有的线性CRL方法通常依赖于严格的假设，例如访问单节点干预数据或对潜在特征和外生测量噪声的限制性分布约束。然而，这些前提在某些情况下可能难以满足。在本工作中，我们提出了一种新颖的线性CRL算法，与大多数现有的线性CRL方法不同，它在关于环境异质性和数据生成分布的较弱假设下运行，同时仍然能够恢复到等价类的潜在因果特征。我们通过合成实验和对大型语言模型(LLMs)的可解释性分析进一步验证了我们的新算法，展示了其在有限样本中优于竞争方法的潜力以及将因果性集成到AI中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal representation learning (CRL) has garnered increasing interests fromthe causal inference and artificial intelligence community, due to itscapability of disentangling potentially complex data-generating mechanism intocausally interpretable latent features, by leveraging the heterogeneity ofmodern datasets. In this paper, we further contribute to the CRL literature, byfocusing on the stylized linear structural causal model over the latentfeatures and assuming a linear mixing function that maps latent features to theobserved data or measurements. Existing linear CRL methods often rely onstringent assumptions, such as accessibility to single-node interventional dataor restrictive distributional constraints on latent features and exogenousmeasurement noise. However, these prerequisites can be challenging to satisfyin certain scenarios. In this work, we propose a novel linear CRL algorithmthat, unlike most existing linear CRL methods, operates under weakerassumptions about environment heterogeneity and data-generating distributionswhile still recovering latent causal features up to an equivalence class. Wefurther validate our new algorithm via synthetic experiments and aninterpretability analysis of large language models (LLMs), demonstrating bothits superiority over competing methods in finite samples and its potential inintegrating causality into AI.</description>
      <author>example@mail.com (Hao Chen, Lin Liu, Yu Guang Wang)</author>
      <guid isPermaLink="false">2509.22553v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Category Discovery: An Open-World Perspective</title>
      <link>http://arxiv.org/abs/2509.22542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了类别发现(Category Discovery, CD)这一新兴的开世界学习任务，提供了全面的文献回顾、详细的方法分析和深入的讨论。&lt;h4&gt;背景&lt;/h4&gt;类别发现是一个新兴的开世界学习任务，旨在给定一些已标记的已知类别数据的情况下，自动对包含未知类别实例的无标记数据进行分类。这个任务近年来受到了显著关注，并产生了丰富的文献。&lt;h4&gt;目的&lt;/h4&gt;提供对类别发现文献的全面回顾，对不同方法提供详细分析和深入讨论，并指出未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;1) 引入文献分类法，考虑新颖类别发现(NCD)和广义类别发现(GCD)两个基本设置，以及持续类别发现、倾斜数据分布、联邦类别发现等派生设置；2) 对每种设置的方法进行分析，包括表示学习、标签分配和类别数量估计三个组成部分；3) 对所有方法进行基准测试；4) 讨论关键见解并指出未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;大规模预训练骨干网络、层次化和辅助线索、课程式训练都有利于类别发现；但在标签分配设计、类别数量估计以及扩展到复杂多目标场景方面仍存在挑战。&lt;h4&gt;结论&lt;/h4&gt;讨论了文献中的关键见解，指出了有前途的未来研究方向，并提供了类别发现文献的动态调查页面(https://github.com/Visual-AI/Category-Discovery)。&lt;h4&gt;翻译&lt;/h4&gt;类别发现(CD)是一个新兴的开世界学习任务，旨在给定一些来自已知类别的标记数据的情况下，自动对包含未知类别实例的无标记数据进行分类。多年来，这个任务引起了广泛关注，并产生了大量从不同角度尝试解决该问题的文献。在本综述中，我们对文献进行了全面回顾，并对不同方法提供了详细分析和深入讨论。首先，我们通过考虑两个基本设置(即新颖类别发现(NCD)和广义类别发现(GCD))以及为应对不同实际应用场景中的额外挑战而设计的几个派生设置(包括持续类别发现、倾斜数据分布、联邦类别发现等)，为文献引入了一个分类法。其次，对每种设置，我们提供了包含三个基本组成部分(表示学习、标签分配和类别数量估计)的方法详细分析。第三，我们对所有方法进行了基准测试并提炼了关键见解，表明大规模预训练骨干网络、层次化和辅助线索以及课程式训练都有利于类别发现，但在标签分配设计、类别数量估计以及扩展到复杂多目标场景方面仍存在挑战。最后，我们讨论了迄今为止文献中的关键见解，并指出了有前途的未来研究方向。我们在https://github.com/Visual-AI/Category-Discovery上整理了类别发现文献的动态调查。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Category discovery (CD) is an emerging open-world learning task, which aimsat automatically categorizing unlabelled data containing instances from unseenclasses, given some labelled data from seen classes. This task has attractedsignificant attention over the years and leads to a rich body of literaturetrying to address the problem from different perspectives. In this survey, weprovide a comprehensive review of the literature, and offer detailed analysisand in-depth discussion on different methods. Firstly, we introduce a taxonomyfor the literature by considering two base settings, namely novel categorydiscovery (NCD) and generalized category discovery (GCD), and several derivedsettings that are designed to address the extra challenges in differentreal-world application scenarios, including continual category discovery,skewed data distribution, federated category discovery, etc. Secondly, for eachsetting, we offer a detailed analysis of the methods encompassing threefundamental components, representation learning, label assignment, andestimation of class number. Thirdly, we benchmark all the methods and distillkey insights showing that large-scale pretrained backbones, hierarchical andauxiliary cues, and curriculum-style training are all beneficial for categorydiscovery, while challenges remain in the design of label assignment, theestimation of class numbers, and scaling to complex multi-objectscenarios.Finally, we discuss the key insights from the literature so far andpoint out promising future research directions. We compile a living survey ofthe category discovery literature at\href{https://github.com/Visual-AI/Category-Discovery}{https://github.com/Visual-AI/Category-Discovery}.</description>
      <author>example@mail.com (Zhenqi He, Yuanpei Liu, Kai Han)</author>
      <guid isPermaLink="false">2509.22542v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong</title>
      <link>http://arxiv.org/abs/2509.22510v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了自适应多分支转向(AMBS)方法，解决了大型语言模型多目标对齐中的灾难性遗忘和推理碎片化问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在有用性、无害性和诚实性(HHH)目标上的对齐对安全可靠部署至关重要。先前的一对一转向向量方法会导致灾难性遗忘，而一对多方法虽然缓解了这一问题，但可能造成推理碎片化。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一且高效的多目标对齐框架，能够在保持跨目标一致性的同时实现目标特定控制。&lt;h4&gt;方法&lt;/h4&gt;AMBS是一个两阶段的一对多框架：第一阶段计算Transformer层的注意力后隐藏状态形成共享表示；第二阶段将该表示克隆到并行分支中，通过策略引用机制进行目标特定控制。&lt;h4&gt;主要发现&lt;/h4&gt;在Alpaca、BeaverTails和TruthfulQA上的评估显示，AMBS在多个7B LLM主干上显著改善了HHH对齐。在DeepSeek-7B上，与简单1-to-N基线相比，平均对齐分数提高32.4%，不安全输出减少11.0%，同时保持与最先进方法的竞争力。&lt;h4&gt;结论&lt;/h4&gt;AMBS有效解决了多目标对齐中的关键挑战，为大型语言模型的安全可靠部署提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在多个目标上的对齐-有用性、无害性和诚实性(HHH)-对于安全可靠的部署至关重要。先前工作使用转向向量-注入到隐藏状态中的小型控制信号-来引导LLM输出，通常通过一对一(1-to-1)Transformer解码器。在这种设置下，优化单一对齐目标可能会无意中覆盖为其他目标学习的表示，导致灾难性遗忘。更新的方法通过一对多(1-to-N)Transformer解码器扩展转向向量。虽然这缓解了灾难性遗忘，但简单的多分支设计独立优化每个目标，可能导致推理碎片化-跨HHH目标的输出可能变得不一致。我们提出了自适应多分支转向(AMBS)，这是一个两阶段的一对多框架，用于统一和高效的多目标对齐。在第一阶段，计算Transformer层的注意力后隐藏状态一次形成共享表示。在第二阶段，将该表示克隆到并行分支中，并通过策略引用机制进行转向，实现目标特定控制同时保持跨目标一致性。在Alpaca、BeaverTails和TruthfulQA上的经验评估表明，AMBS在多个7B LLM主干上持续改进HHH对齐。例如，在DeepSeek-7B上，与简单1-to-N基线相比，AMBS将平均对齐分数提高32.4%，将不安全输出减少11.0%，同时保持与最先进方法的竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alignment of Large Language Models (LLMs) along multipleobjectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safeand reliable deployment. Prior work has used steering vector-small controlsignals injected into hidden states-to guide LLM outputs, typically viaone-to-one (1-to-1) Transformer decoders. In this setting, optimizing a singlealignment objective can inadvertently overwrite representations learned forother objectives, leading to catastrophic forgetting. More recent approachesextend steering vectors via one-to-many (1-to-N) Transformer decoders. Whilethis alleviates catastrophic forgetting, naive multi-branch designs optimizeeach objective independently, which can cause inference fragmentation-outputsacross HHH objectives may become inconsistent. We propose Adaptive Multi-BranchSteering (AMBS), a two-stage 1-to-N framework for unified and efficientmulti-objective alignment. In Stage I, post-attention hidden states of theTransformer layer are computed once to form a shared representation. In StageII, this representation is cloned into parallel branches and steered via apolicy-reference mechanism, enabling objective-specific control whilemaintaining cross-objective consistency. Empirical evaluations on Alpaca,BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignmentacross multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improvesaverage alignment scores by +32.4% and reduces unsafe outputs by 11.0% comparedto a naive 1-to-N baseline, while remaining competitive with state-of-the-artmethods.</description>
      <author>example@mail.com (Gautam Siddharth Kashyap, Mark Dras, Usman Naseem)</author>
      <guid isPermaLink="false">2509.22510v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning</title>
      <link>http://arxiv.org/abs/2509.22481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种渐进式时空token选择(PSTTS)方法，用于解决事件数据中的时空冗余问题，无需额外参数即可显著提高计算效率&lt;h4&gt;背景&lt;/h4&gt;主流事件时空表征学习方法将事件流转换为事件帧序列处理，但忽略了事件帧序列中固有的高空间稀疏性和帧间运动冗余性，导致计算开销大&lt;h4&gt;目的&lt;/h4&gt;开发一种即插即用模块，有效识别并丢弃时空冗余token，在保持任务精度的同时提高计算效率&lt;h4&gt;方法&lt;/h4&gt;提出渐进式时空token选择(PSTTS)方法，包含空间token净化和时间token选择两个阶段，前者评估事件帧内事件的时空一致性以丢弃噪声，后者评估相邻帧运动模式相似性以移除冗余时间信息&lt;h4&gt;主要发现&lt;/h4&gt;PSTTS在保持任务准确性的同时，在DailyDVS-200数据集上减少了29-43.6%的计算量(FLOPs)，提高了21.6-41.3%的处理速度(FPS)&lt;h4&gt;结论&lt;/h4&gt;PSTTS是一种有效的即插即用模块，可应用于多种骨干网络，显著提升事件数据处理效率&lt;h4&gt;翻译&lt;/h4&gt;主流事件时空表征学习方法通常通过将事件流转换为事件帧序列来处理，取得显著性能。然而，它们忽略了事件帧序列中固有的高空间稀疏性和帧间运动冗余性，导致大量计算开销。现有的RGB视频token稀疏化方法依赖于不可靠的中间token表示，忽略了事件噪声的影响，因此不能直接应用于事件数据。本文提出渐进式时空token选择(PSTTS)，一种无需引入额外参数的事件数据即插即用模块。PSTTS利用原始事件数据中嵌入的时空分布特征，有效识别并丢弃时空冗余token，实现精度和效率之间的最佳平衡。具体而言，PSTTS包含两个阶段：空间token净化和时间token选择。空间token净化通过评估每个事件帧内事件的时空一致性，丢弃噪声和非事件区域，防止干扰后续的冗余性评估。时间token选择评估相邻事件帧之间的运动模式相似性，精确识别并移除冗余时间信息。我们将PSTTS应用于四个代表性骨干网络UniformerV2、VideoSwin、EVMamba和ExACT，在HARDVS、DailyDVS-200和SeACT数据集上进行测试。实验结果表明PSTTS实现了显著的效率提升。具体而言，在DailyDVS-200数据集上，PSTTS减少了29-43.6%的FLOPs，提高了21.6-41.3%的FPS，同时保持了任务准确性。我们的代码将会公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mainstream event-based spatio-temporal representation learning methodstypically process event streams by converting them into sequences of eventframes, achieving remarkable performance. However, they neglect the highspatial sparsity and inter-frame motion redundancy inherent in event framesequences, leading to significant computational overhead. Existing tokensparsification methods for RGB videos rely on unreliable intermediate tokenrepresentations and neglect the influence of event noise, making themineffective for direct application to event data. In this paper, we proposeProgressive Spatio-Temporal Token Selection (PSTTS), a Plug-and-Play module forevent data without introducing any additional parameters. PSTTS exploits thespatio-temporal distribution characteristics embedded in raw event data toeffectively identify and discard spatio-temporal redundant tokens, achieving anoptimal trade-off between accuracy and efficiency. Specifically, PSTTS consistsof two stages, Spatial Token Purification and Temporal Token Selection. SpatialToken Purification discards noise and non-event regions by assessing thespatio-temporal consistency of events within each event frame to preventinterference with subsequent temporal redundancy evaluation. Temporal TokenSelection evaluates the motion pattern similarity between adjacent eventframes, precisely identifying and removing redundant temporal information. Weapply PSTTS to four representative backbones UniformerV2, VideoSwin, EVMamba,and ExACT on the HARDVS, DailyDVS-200, and SeACT datasets. Experimental resultsdemonstrate that PSTTS achieves significant efficiency improvements.Specifically, PSTTS reduces FLOPs by 29-43.6% and increases FPS by 21.6-41.3%on the DailyDVS-200 dataset, while maintaining task accuracy. Our code will beavailable.</description>
      <author>example@mail.com (Xiangmo Zhao, Nan Yang, Yang Wang, Zhanwen Liu)</author>
      <guid isPermaLink="false">2509.22481v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining</title>
      <link>http://arxiv.org/abs/2509.22468v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;C-FREE是一种新的分子表示学习框架，通过整合2D图和3D构象信息，在MoleculeNet上取得了最先进的结果，证明3D信息对高质量分子表示至关重要。&lt;h4&gt;背景&lt;/h4&gt;高质量的分子表示对属性预测和分子设计至关重要，但大型标记数据集稀缺。现有自监督方法依赖手工增强或复杂生成目标，且通常只使用2D拓扑，忽视了3D结构信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种简单框架，能够同时利用2D拓扑和3D结构信息，提高分子表示学习质量，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出C-FREE（基于ego-net的无对比表示学习）框架，集成2D图和3D构象集合，通过预测子图嵌入学习分子表示，使用固定半径的ego-net作为建模单元，结合几何和拓扑信息，采用混合GNN-Transformer骨干网络，无需负样本、位置编码或昂贵的预处理。&lt;h4&gt;主要发现&lt;/h4&gt;在GEOM数据集上预训练后，C-FREE在MoleculeNet上超越对比学习、生成和其他多模态自监督方法，取得最先进结果。在不同大小和分子类型的数据集上微调，证明预训练能有效迁移到新的化学领域。&lt;h4&gt;结论&lt;/h4&gt;3D信息感知的分子表示对高质量分子表示学习至关重要，C-FREE框架有效解决了现有方法的局限性，为分子属性预测和设计提供了更好的表示。&lt;h4&gt;翻译&lt;/h4&gt;高质量的分子表示对于属性预测和分子设计至关重要，然而大型标记数据集仍然稀缺。虽然基于分子图的自监督预训练已显示出潜力，但许多现有方法要么依赖手工增强或复杂的生成目标，要么仅依赖2D拓扑，导致有价值的3D结构信息未被充分利用。为解决这一差距，我们引入了C-FREE（基于ego-net的无对比表示学习），这是一个整合2D图和3D构象集合的简单框架。C-FREE通过在潜在空间中预测子图嵌入来学习分子表示，使用不同构象中的固定半径ego-net作为建模单元。这种设计使我们能够在混合图神经网络(GNN)-Transformer骨干网络中整合几何和拓扑信息，无需负样本、位置编码或昂贵的预处理。在提供丰富3D构象多样性的GEOM数据集上进行预训练，C-FREE在MoleculeNet上取得了最先进的结果，超越了对比学习、生成和其他多模态自监督方法。在不同大小和分子类型的数据集上进行微调进一步证明，预训练能有效迁移到新的化学领域，突显了3D信息感知的分子表示的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality molecular representations are essential for property predictionand molecular design, yet large labeled datasets remain scarce. Whileself-supervised pretraining on molecular graphs has shown promise, manyexisting approaches either depend on hand-crafted augmentations or complexgenerative objectives, and often rely solely on 2D topology, leaving valuable3D structural information underutilized. To address this gap, we introduceC-FREE (Contrast-Free Representation learning on Ego-nets), a simple frameworkthat integrates 2D graphs with ensembles of 3D conformers. C-FREE learnsmolecular representations by predicting subgraph embeddings from theircomplementary neighborhoods in the latent space, using fixed-radius ego-nets asmodeling units across different conformers. This design allows us to integrateboth geometric and topological information within a hybrid Graph Neural Network(GNN)-Transformer backbone, without negatives, positional encodings, orexpensive pre-processing. Pretraining on the GEOM dataset, which provides rich3D conformational diversity, C-FREE achieves state-of-the-art results onMoleculeNet, surpassing contrastive, generative, and other multimodalself-supervised methods. Fine-tuning across datasets with diverse sizes andmolecule types further demonstrates that pretraining transfers effectively tonew chemical domains, highlighting the importance of 3D-informed molecularrepresentations.</description>
      <author>example@mail.com (Boshra Ariguib, Mathias Niepert, Andrei Manolache)</author>
      <guid isPermaLink="false">2509.22468v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing</title>
      <link>http://arxiv.org/abs/2509.22412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the IEEE/CVF Conference on Computer Vision and Pattern  Recognition (CVPR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FreqDebias的频率去偏差框架，用于解决Deepfake检测器在面对新型伪造类型时的泛化能力受限问题。&lt;h4&gt;背景&lt;/h4&gt;Deepfake检测器通常难以泛化到新型伪造类型，这是由于从有限训练数据中学习到的偏差导致的。&lt;h4&gt;目的&lt;/h4&gt;识别并解决频域中的一种新型模型偏差（频谱偏差），该偏差导致检测器过度依赖特定频带，限制了其对未见伪造类型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;FreqDebias框架采用两种互补策略：1）引入伪造混合增强（Fo-Mixup），动态增加训练样本的频率特性多样性；2）结合双一致性正则化（CR），使用类激活图强制局部一致性，并通过冯·米塞斯-费舍尔分布在超球嵌入空间上强制全局一致性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明FreqDebias显著增强了跨域泛化能力，并在跨域和域内设置中都优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;通过减轻频谱偏差，FreqDebias框架提高了Deepfake检测器对新型伪造类型的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要翻译：Deepfake检测器通常难以泛化到新型伪造类型，这是由于从有限训练数据中学习到的偏差导致的。在本文中，我们识别了频域中的一种新型模型偏差，称为频谱偏差，其中检测器过度依赖特定频带，限制了它们对未见伪造类型的泛化能力。为此，我们提出了FreqDebias，一个频率去偏差框架，通过两种互补策略减轻频谱偏差。首先，我们引入了一种新颖的伪造混合增强（Fo-Mixup），动态增加训练样本的频率特性多样性。其次，我们结合了双一致性正则化（CR），使用类激活图（CAMs）强制局部一致性，并通过超球嵌入空间上的冯·米塞斯-费舍尔（vMF）分布强制全局一致性。这种双CR通过在局部和全局监督下促进一致的表征学习，减轻了对特定频率成分的过度依赖。大量实验表明，FreqDebias显著增强了跨域泛化能力，并在跨域和域内设置中都优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake detectors often struggle to generalize to novel forgery types due tobiases learned from limited training data. In this paper, we identify a newtype of model bias in the frequency domain, termed spectral bias, wheredetectors overly rely on specific frequency bands, restricting their ability togeneralize across unseen forgeries. To address this, we propose FreqDebias, afrequency debiasing framework that mitigates spectral bias through twocomplementary strategies. First, we introduce a novel Forgery Mixup (Fo-Mixup)augmentation, which dynamically diversifies frequency characteristics oftraining samples. Second, we incorporate a dual consistency regularization(CR), which enforces both local consistency using class activation maps (CAMs)and global consistency through a von Mises-Fisher (vMF) distribution on ahyperspherical embedding space. This dual CR mitigates over-reliance on certainfrequency components by promoting consistent representation learning under bothlocal and global supervision. Extensive experiments show that FreqDebiassignificantly enhances cross-domain generalization and outperformsstate-of-the-art methods in both cross-domain and in-domain settings.</description>
      <author>example@mail.com (Hossein Kashiani, Niloufar Alipour Talemi, Fatemeh Afghah)</author>
      <guid isPermaLink="false">2509.22412v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization</title>
      <link>http://arxiv.org/abs/2509.22161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的平滑向量量化方法，通过简单直观的正则化技术同时满足量化器接近one-hot向量和充分利用所有码本条目的需求，在图像自编码和语音表示学习等任务中取得了优于先前方法的性能。&lt;h4&gt;背景&lt;/h4&gt;向量量化将连续向量空间离散化为有限个代表性向量(码本)，在现代机器学习中广泛应用，但其非可微的量化步骤阻碍了梯度反向传播。&lt;h4&gt;目的&lt;/h4&gt;解决向量量化中的非可微问题，同时确保平滑量化器接近one-hot向量并充分利用所有码本条目，防止码本坍塌。&lt;h4&gt;方法&lt;/h4&gt;引入一种简单直观的正则化方法，通过最小化每个单纯形顶点与其K个最近平滑量化器之间的距离，同时满足两个关键特性：量化器接近one-hot向量和所有码本条目被充分利用。&lt;h4&gt;主要发现&lt;/h4&gt;在离散图像自编码和对比语音表示学习等代表性基准测试中，所提出的方法实现了更可靠的码本利用并提高了性能。&lt;h4&gt;结论&lt;/h4&gt;该正则化方法能够同时解决向量量化中的两个关键挑战，为平滑向量量化提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;向量量化是将连续向量空间离散化为有限个代表性向量(码本)的方法，在现代机器学习中已被广泛采用。尽管其有效，但向量量化存在一个基本挑战：非可微的量化步骤阻碍了梯度反向传播。平滑向量量化通过将码本向量的硬分配放松为码本条目的加权和来解决这个问题，表示为单纯形向量与码本的矩阵乘积。有效平滑需要两个特性：(1)平滑量化器应接近one-hot向量，确保紧密近似；(2)应使用所有码本条目，防止码本坍塌。现有方法通常分别处理这些需求。相比之下，本研究引入了一种简单直观的正则化方法，通过最小化每个单纯形顶点与其K个最近平滑量化器之间的距离，同时促进这两个特性。在离散图像自编码和对比语音表示学习等代表性基准测试中的实验表明，与先前方法相比，所提出的方法实现了更可靠的码本利用并提高了性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vector quantization, which discretizes a continuous vector space into afinite set of representative vectors (a codebook), has been widely adopted inmodern machine learning. Despite its effectiveness, vector quantization poses afundamental challenge: the non-differentiable quantization step blocks gradientbackpropagation. Smoothed vector quantization addresses this issue by relaxingthe hard assignment of a codebook vector into a weighted combination ofcodebook entries, represented as the matrix product of a simplex vector and thecodebook. Effective smoothing requires two properties: (1) smoothed quantizersshould remain close to a onehot vector, ensuring tight approximation, and (2)all codebook entries should be utilized, preventing code collapse. Existingmethods typically address these desiderata separately. By contrast, the presentstudy introduces a simple and intuitive regularization that promotes bothsimultaneously by minimizing the distance between each simplex vertex and its$K$-nearest smoothed quantizers. Experiments on representative benchmarks,including discrete image autoencoding and contrastive speech representationlearning, demonstrate that the proposed method achieves more reliable codebookutilization and improves performance compared to prior approaches.</description>
      <author>example@mail.com (Takashi Morita)</author>
      <guid isPermaLink="false">2509.22161v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Mind the Missing: Variable-Aware Representation Learning for Irregular EHR Time Series using Large Language Models</title>
      <link>http://arxiv.org/abs/2509.22121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VITAL是一种基于大型语言模型的变量感知框架，专门用于从不规则采样的生理时间序列中学习。它通过区分生命体征和实验室检查的不同特性，并采用相应的处理方法，有效解决了不规则采样和高缺失率的挑战。在基准测试中，VITAL表现优于现有方法，并且在处理高缺失率数据时保持稳健。&lt;h4&gt;背景&lt;/h4&gt;从电子健康记录（EHRs）导出的时间序列数据面临不规则采样和高缺失率的挑战。临床变量根据工作流程和干预时间以不均匀的时间间隔进行测量，这使得传统的时间序列分析方法难以有效应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为VITAL的框架，用于从不规则采样的生理时间序列中有效学习，并解决临床数据中常见的高缺失率问题。&lt;h4&gt;方法&lt;/h4&gt;VITAL框架区分两种临床变量：1）生命体征，将其重新编程到语言空间，使LLM能够捕获时间上下文并推理缺失值；2）实验室检查，根据可用性使用代表性汇总值或可学习的'未测量'令牌进行嵌入。这种方法使模型能够处理不同特性的临床变量。&lt;h4&gt;主要发现&lt;/h4&gt;在PhysioNet的基准数据集上，VITAL优于为不规则时间序列设计的最先进方法。此外，在高缺失率情况下（现实临床场景中常见），VITAL保持稳健的性能，即使关键变量不可用。&lt;h4&gt;结论&lt;/h4&gt;VITAL框架能够有效处理来自电子健康记录的不规则时间序列数据，特别是在高缺失率情况下表现优异，为临床数据分析提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;不规则采样和高缺失率是从电子健康记录（EHRs）导出的时间序列建模中的内在挑战，其中临床变量根据工作流程和干预时间以不均匀间隔进行测量。为解决这一问题，我们提出了VITAL，这是一种变量感知的基于大型语言模型（LLM）的框架，专为从不规则采样的生理时间序列中学习而设计。VITAL区分两种不同类型的临床变量：生命体征，它们被频繁记录并表现出时间模式；以及实验室检查，它们被零星测量且缺乏时间结构。它将生命体征重新编程到语言空间，使LLM能够通过显式编码捕获时间上下文并对缺失值进行推理。相比之下，实验室变量根据其可用性，使用代表性汇总值或可学习的'未测量'令牌进行嵌入。在PhysioNet的基准数据集上进行的大量评估表明，VITAL优于为不规则时间序列设计的最先进方法。此外，在高缺失率情况下（现实临床场景中常见，关键变量通常不可用），它保持稳健的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Irregular sampling and high missingness are intrinsic challenges in modelingtime series derived from electronic health records (EHRs),where clinicalvariables are measured at uneven intervals depending on workflow andintervention timing. To address this, we propose VITAL, a variable-aware, largelanguage model (LLM) based framework tailored for learning from irregularlysampled physiological time series. VITAL differentiates between two distincttypes of clinical variables: vital signs, which are frequently recorded andexhibit temporal patterns, and laboratory tests, which are measuredsporadically and lack temporal structure. It reprograms vital signs into thelanguage space, enabling the LLM to capture temporal context and reason overmissing values through explicit encoding. In contrast, laboratory variables areembedded either using representative summary values or a learnable [Notmeasured] token, depending on their availability. Extensive evaluations on thebenchmark datasets from the PhysioNet demonstrate that VITAL outperforms stateof the art methods designed for irregular time series. Furthermore, itmaintains robust performance under high levels of missingness, which isprevalent in real world clinical scenarios where key variables are oftenunavailable.</description>
      <author>example@mail.com (Jeong Eul Kwon, Joo Heung Yoon, Hyo Kyung Lee)</author>
      <guid isPermaLink="false">2509.22121v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning</title>
      <link>http://arxiv.org/abs/2509.22050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BrainPro是一种创新的EEG基础模型，通过引入空间学习和状态解耦机制，解决了现有模型在捕捉通道间和区域间相互作用以及学习状态感知表示方面的挑战，在多个数据集上表现出色，具有广泛的适用性。&lt;h4&gt;背景&lt;/h4&gt;EEG是一种无创技术，用于记录大脑电活动，广泛应用于脑机接口和医疗保健领域。近期基于大规模数据集训练的EEG基础模型相比传统解码方法表现出更好的性能和泛化能力，但仍存在显著挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有EEG基础模型无法明确捕捉通道间和区域间相互作用、难以适应不同电极布局、以及很少学习状态感知表示的问题，开发一种能够灵活适应多样化任务和硬件设置的大型EEG模型。&lt;h4&gt;方法&lt;/h4&gt;提出BrainPro模型，引入基于检索的空间学习块以灵活捕捉不同电极布局下的通道和区域级相互作用，同时引入大脑状态解耦块，通过具有解耦和区域感知重建损失的并行编码器实现状态感知的表示学习，并在大规模EEG语料库上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;BrainPro在九个公共BCI数据集上实现了最先进的性能和强大的泛化能力，证明了该模型在处理多样化EEG数据时的有效性。&lt;h4&gt;结论&lt;/h4&gt;BrainPro通过创新的空间学习和状态解耦机制，成功解决了现有EEG基础模型的局限性，能够无缝适应多样化的任务和硬件设置，代码和预训练权重将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;脑电图是一种记录大脑电活动的无创技术，广泛应用于脑机接口和医疗保健领域。最近在大型数据集上训练的EEG基础模型相比传统解码方法显示出改进的性能和泛化能力，但仍然存在重大挑战。现有模型通常无法明确捕捉通道间和区域间的相互作用，这些是EEG信号中固有的关键信息源。由于不同数据集的通道配置不同，现有模型要么用自注意力近似空间结构，要么将训练限制在有限的通用通道集合，牺牲了灵活性和有效性。此外，尽管EEG数据集反映了情绪、运动等多种大脑状态，但当前模型在自监督预训练过程中很少学习状态感知的表示。为解决这些差距，我们提出BrainPro，一种大型EEG模型，引入了基于检索的空间学习块，以灵活捕捉不同电极布局下的通道和区域级相互作用，以及大脑状态解耦块，通过具有解耦和区域感知重建损失的并行编码器实现状态感知的表示学习。这种设计使BrainPro能够无缝适应多样化的任务和硬件设置。在大型EEG语料库上预训练后，BrainPro在九个公共BCI数据集上实现了最先进的性能和强大的泛化能力。我们的代码和预训练权重将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) is a non-invasive technique for recording brainelectrical activity, widely used in brain-computer interface (BCI) andhealthcare. Recent EEG foundation models trained on large-scale datasets haveshown improved performance and generalizability over traditional decodingmethods, yet significant challenges remain. Existing models often fail toexplicitly capture channel-to-channel and region-to-region interactions, whichare critical sources of information inherently encoded in EEG signals. Due tovarying channel configurations across datasets, they either approximate spatialstructure with self-attention or restrict training to a limited set of commonchannels, sacrificing flexibility and effectiveness. Moreover, although EEGdatasets reflect diverse brain states such as emotion, motor, and others,current models rarely learn state-aware representations during self-supervisedpre-training. To address these gaps, we propose BrainPro, a large EEG modelthat introduces a retrieval-based spatial learning block to flexibly capturechannel- and region-level interactions across varying electrode layouts, and abrain state-decoupling block that enables state-aware representation learningthrough parallel encoders with decoupling and region-aware reconstructionlosses. This design allows BrainPro to adapt seamlessly to diverse tasks andhardware settings. Pre-trained on an extensive EEG corpus, BrainPro achievesstate-of-the-art performance and robust generalization across nine public BCIdatasets. Our codes and the pre-trained weights will be released.</description>
      <author>example@mail.com (Yi Ding, Muyun Jiang, Weibang Jiang, Shuailei Zhang, Xinliang Zhou, Chenyu Liu, Shanglin Li, Yong Li, Cuntai Guan)</author>
      <guid isPermaLink="false">2509.22050v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>TRACE: Learning to Compute on Graphs</title>
      <link>http://arxiv.org/abs/2509.21886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TRACE是一种新的图计算学习范式，通过分层Transformer架构和函数位移学习目标，解决了主流消息传递神经网络在计算图建模中的架构不匹配问题&lt;h4&gt;背景&lt;/h4&gt;图表示学习中学习计算（建模计算图的功能行为）是一个基本挑战，但主流架构与这个任务不匹配&lt;h4&gt;目的&lt;/h4&gt;解决主流消息传递神经网络和基于Transformer的方法在捕捉计算位置感知和层次性质方面的局限性&lt;h4&gt;方法&lt;/h4&gt;TRACE采用分层Transformer镜像计算的逐步流动，取代排列不变聚合；引入函数位移学习，将学习问题解耦，只预测函数位移而非复杂全局函数&lt;h4&gt;主要发现&lt;/h4&gt;在电子电路这一复杂且经济重要的计算图类别上，TRACE在全面基准测试中明显优于所有先前架构&lt;h4&gt;结论&lt;/h4&gt;架构对齐骨干和解耦学习目标形成了一个更强大的范式，用于解决图上学习计算的基本挑战&lt;h4&gt;翻译&lt;/h4&gt;学习计算，即建模计算图功能行为的能力，是图表示学习的一个基本挑战。然而，主流范式在架构上与这个任务不匹配。这个有缺陷的假设是主流消息传递神经网络及其传统基于Transformer的对应方法的核心，阻碍了模型捕捉计算的位置感知和层次性质。为了解决这个问题，我们引入TRACE，这是一个基于架构合理的骨干和原则性学习目标的新范式。首先，TRACE采用分层Transformer，镜像计算的逐步流动，提供了一个忠实的架构骨干，取代了有缺陷的排列不变聚合。其次，我们引入函数位移学习，一个新颖的目标，将学习问题解耦。模型不是直接预测复杂全局函数，而是被训练为只预测函数位移，即真实全局函数与假设输入独立的简单局部近似之间的差异。我们在电子电路上验证了这个范式，电子电路是最复杂和经济上最重要的计算图类别之一。在全面的基准测试中，TRACE明显优于所有先前架构。这些结果表明，我们的架构对齐骨干和解耦学习目标形成了一个更强大的范式，用于解决图上学习计算的基本挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning to compute, the ability to model the functional behavior of acomputational graph, is a fundamental challenge for graph representationlearning. Yet, the dominant paradigm is architecturally mismatched for thistask. This flawed assumption, central to mainstream message passing neuralnetworks (MPNNs) and their conventional Transformer-based counterparts,prevents models from capturing the position-aware, hierarchical nature ofcomputation. To resolve this, we introduce \textbf{TRACE}, a new paradigm builton an architecturally sound backbone and a principled learning objective.First, TRACE employs a Hierarchical Transformer that mirrors the step-by-stepflow of computation, providing a faithful architectural backbone that replacesthe flawed permutation-invariant aggregation. Second, we introduce\textbf{function shift learning}, a novel objective that decouples the learningproblem. Instead of predicting the complex global function directly, our modelis trained to predict only the \textit{function shift}, the discrepancy betweenthe true global function and a simple local approximation that assumes inputindependence. We validate this paradigm on electronic circuits, one of the mostcomplex and economically critical classes of computational graphs. Across acomprehensive suite of benchmarks, TRACE substantially outperforms all priorarchitectures. These results demonstrate that our architecturally-alignedbackbone and decoupled learning objective form a more robust paradigm for thefundamental challenge of learning to compute on graphs.</description>
      <author>example@mail.com (Ziyang Zheng, Jiaying Zhu, Jingyi Zhou, Qiang Xu)</author>
      <guid isPermaLink="false">2509.21886v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>IndiSeek learns information-guided disentangled representations</title>
      <link>http://arxiv.org/abs/2509.21584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为IndiSeek的新解耦表示学习方法，用于多模态学习中的解耦表示学习问题。该方法通过结合独立性强制目标和计算高效的重建损失，有效平衡了特征独立性和完整性，能够在单细胞多组学等应用中提取模态特定特征。&lt;h4&gt;背景&lt;/h4&gt;解耦表示学习是多模态学习的基本任务。在现代应用如单细胞多组学中，共享特征和模态特定特征对表征细胞状态和支持下游分析都至关重要。然而，基于互信息的目标难以可靠估计，其变分替代在实践中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决多模态学习中解耦表示学习的挑战，特别是如何使模态特定特征独立于共享特征同时捕获每个模态内的互补信息，以及如何平衡特征独立性和完整性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了IndiSeek方法，它结合了独立性强制目标和计算高效的重建损失，该损失限制了条件互信息。这种公式明确平衡了独立性和完整性，实现了模态特定特征的原则性提取。&lt;h4&gt;主要发现&lt;/h4&gt;IndiSeek在合成模拟、CITE-seq数据集和多个真实世界多模态基准测试上表现出色，证明了其有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;IndiSeek为多模态学习中的解耦表示学习提供了新的解决方案，能够有效提取模态特定特征，支持下游分析任务。&lt;h4&gt;翻译&lt;/h4&gt;学习解耦表示是多模态学习中的一个基本任务。在现代应用中，如单细胞多组学，共享特征和模态特定特征对于表征细胞状态和支持下游分析都至关重要。理想情况下，模态特定特征应该独立于共享特征，同时捕获每个模态内的所有互补信息。这种权衡自然地通过信息论标准表达，但基于互信息的目标难以可靠估计，其变分替代在实践中通常表现不佳。在本文中，我们介绍了IndiSeek，这是一种新的解耦表示学习方法，它通过结合独立性强制目标和计算高效的重建损失来解决这一挑战，该损失限制了条件互信息。这种公式明确平衡了独立性和完整性，实现了模态特定特征的原则性提取。我们在合成模拟、CITE-seq数据集和多个真实世界多模态基准测试上证明了IndiSeek的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning disentangled representations is a fundamental task in multi-modallearning. In modern applications such as single-cell multi-omics, both sharedand modality-specific features are critical for characterizing cell states andsupporting downstream analyses. Ideally, modality-specific features should beindependent of shared ones while also capturing all complementary informationwithin each modality. This tradeoff is naturally expressed throughinformation-theoretic criteria, but mutual-information-based objectives aredifficult to estimate reliably, and their variational surrogates oftenunderperform in practice. In this paper, we introduce IndiSeek, a noveldisentangled representation learning approach that addresses this challenge bycombining an independence-enforcing objective with a computationally efficientreconstruction loss that bounds conditional mutual information. Thisformulation explicitly balances independence and completeness, enablingprincipled extraction of modality-specific features. We demonstrate theeffectiveness of IndiSeek on synthetic simulations, a CITE-seq dataset andmultiple real-world multi-modal benchmarks.</description>
      <author>example@mail.com (Yu Gui, Cong Ma, Zongming Ma)</author>
      <guid isPermaLink="false">2509.21584v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Physics-informed GNN for medium-high voltage AC power flow with edge-aware attention and line search correction operator</title>
      <link>http://arxiv.org/abs/2509.22458v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures. Submitted to ICASSP 2026. Code available at  https://github.com/Kimchangheon/PIGNN-Attn-LS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;物理信息图神经网络（PIGNNs）作为交流潮流计算求解器，通过结合边感知注意力机制和基于回溯线搜索的校正算子，显著提高了计算精度和速度，在保持高速的同时超越了传统牛顿-拉夫逊方法的性能。&lt;h4&gt;背景&lt;/h4&gt;物理信息图神经网络（PIGNNs）已发展为快速的交流潮流计算求解器，可替代经典牛顿-拉夫逊（NR）求解器，特别是在需要评估数千种场景的应用中。&lt;h4&gt;目的&lt;/h4&gt;解决当前PIGNNs在保持速度的同时需要提高准确性问题，特别是物理损失在推理阶段不起作用可能阻碍其实际应用的问题。&lt;h4&gt;方法&lt;/h4&gt;提出PIGNN-Attn-LS模型，结合边感知注意力机制通过每条边的偏置显式编码线路物理特性捕捉电网各向异性，以及基于回溯线搜索的全局化校正算子在推理阶段恢复有效下降准则。&lt;h4&gt;主要发现&lt;/h4&gt;在4-32总线电网的高压案例测试中，PIGNN-Attn-LS实现了电压均方根误差0.00033 p.u.和角度误差0.08°，分别比PIGNN-MLP基线提高99.5%和87.1%；在4-1024总线网格上，批处理推理速度比牛顿-拉夫逊方法快2-5倍。&lt;h4&gt;结论&lt;/h4&gt;PIGNN-Attn-LS通过结合物理信息与深度学习技术，显著提高了潮流计算的准确性和效率，为电网分析提供了一种快速且精确的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;物理信息图神经网络已成为快速的交流潮流计算求解器，可以替代经典的牛顿-拉夫逊求解器，特别是在需要评估数千种场景时。然而，当前的PIGNNs在保持速度的同时仍需提高准确性；特别是物理损失在推理阶段不起作用，这可能阻碍其实际应用。我们通过PIGNN-Attn-LS解决了这一问题，它结合了边感知注意力机制，通过每条边的偏置显式编码线路物理特性，捕捉电网的各向异性，以及基于回溯线搜索的全局化校正算子，在推理阶段恢复有效的下降准则。训练和测试使用真实的 高/中压场景生成器，仅使用NR构造参考状态。在包含4-32总线电网的保留高压案例中，PIGNN-Attn-LS实现了电压测试均方根误差0.00033 p.u.和角度误差0.08°，分别比PIGNN-MLP基线好99.5%和87.1%。使用流式微批处理，它在4-1024总线网格上的批处理推理速度比NR快2-5倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physics-informed graph neural networks (PIGNNs) have emerged as fast ACpower-flow solvers that can replace classic Newton--Raphson (NR) solvers,especially when thousands of scenarios must be evaluated. However, currentPIGNNs still need accuracy improvements at parity speed; in particular, thephysics loss is inoperative at inference, which can deter operational adoption.We address this with PIGNN-Attn-LS, combining an edge-aware attention mechanismthat explicitly encodes line physics via per-edge biases, capturing the grid'sanisotropy, with a backtracking line-search-based globalized correctionoperator that restores an operative decrease criterion at inference. Trainingand testing use a realistic High-/Medium-Voltage scenario generator, with NRused only to construct reference states. On held-out HV cases consisting of4--32-bus grids, PIGNN-Attn-LS achieves a test RMSE of 0.00033 p.u. in voltageand 0.08$^\circ$ in angle, outperforming the PIGNN-MLP baseline by 99.5\% and87.1\%, respectively. With streaming micro-batches, it delivers 2--5$\times$faster batched inference than NR on 4--1024-bus grids.</description>
      <author>example@mail.com (Changhun Kim, Timon Conrad, Redwanul Karim, Julian Oelhaf, David Riebesel, Tomás Arias-Vergara, Andreas Maier, Johann Jäger, Siming Bayer)</author>
      <guid isPermaLink="false">2509.22458v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>SHAKE-GNN: Scalable Hierarchical Kirchhoff-Forest Graph Neural Network</title>
      <link>http://arxiv.org/abs/2509.22100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了SHAKE-GNN，一种基于基尔霍夫森林层次结构的新型可扩展图级GNN框架，能够生成多尺度表示，在效率和性能之间提供灵活权衡，并在大规模图分类任务中取得具有竞争力的性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在各种学习任务中取得了显著成功，但将GNN扩展到大型图仍然是一个重大挑战，特别是对于图级任务。&lt;h4&gt;目的&lt;/h4&gt;解决GNNs在大规模图上扩展的挑战，特别是针对图级任务设计一种可扩展的框架。&lt;h4&gt;方法&lt;/h4&gt;引入SHAKE-GNN框架，基于基尔霍夫森林层次结构构建图的多分辨率随机分解；提出改进的、数据驱动的策略来选择权衡参数；分析SHAKE-GNN的时间复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;SHAKE-GNN能够生成多尺度表示，在多个大规模图分类基准测试中实现了具有竞争力的性能，同时提供了改进的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;SHAKE-GNN为解决GNN在大规模图上的扩展问题提供了一种有效方法，通过多尺度表示实现了效率和性能的良好平衡。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在各种学习任务中取得了显著成功。然而，将GNN扩展到大型图仍然是一个重大挑战，特别是对于图级任务。在这项工作中，我们介绍了SHAKE-GNN，一种基于基尔霍夫森林层次结构的新型可扩展图级GNN框架，基尔霍夫森林是一类用于构建图多分辨率随机分解的随机生成森林。SHAKE-GNN生成多尺度表示，能够灵活地在效率和性能之间进行权衡。我们引入了一种改进的、数据驱动的策略来选择权衡参数，并分析了SHAKE-GNN的时间复杂度。在多个大规模图分类基准测试上的实验结果表明，SHAKE-GNN实现了具有竞争力的性能，同时提供了改进的可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved remarkable success across a rangeof learning tasks. However, scaling GNNs to large graphs remains a significantchallenge, especially for graph-level tasks. In this work, we introduceSHAKE-GNN, a novel scalable graph-level GNN framework based on a hierarchy ofKirchhoff Forests, a class of random spanning forests used to constructstochastic multi-resolution decompositions of graphs. SHAKE-GNN producesmulti-scale representations, enabling flexible trade-offs between efficiencyand performance. We introduce an improved, data-driven strategy for selectingthe trade-off parameter and analyse the time-complexity of SHAKE-GNN.Experimental results on multiple large-scale graph classification benchmarksdemonstrate that SHAKE-GNN achieves competitive performance while offeringimproved scalability.</description>
      <author>example@mail.com (Zhipu Cui, Johannes Lutzeyer)</author>
      <guid isPermaLink="false">2509.22100v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Stable and Interpretable Jet Physics with IRC-Safe Equivariant Feature Extraction</title>
      <link>http://arxiv.org/abs/2509.22059v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 3 tables, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;深度学习在喷注分类中取得成功，但理解模型学习内容和特征与已知QCD可观测量之间的关系仍具挑战。本研究通过图神经网络和物理学动机的归纳偏置提高可解释性，展示了嵌入对称性和安全性约束如何提高模型鲁棒性并使网络表示与已知QCD结构对应。&lt;h4&gt;背景&lt;/h4&gt;深度学习在喷注分类任务中已取得显著成功，但理解这些模型学习的内容以及它们的特征如何与已知的量子色动力学(QCD)可观测量相关联仍然是一个关键挑战。提高可解释性对于在对撞机物理学中构建强大可靠的机器学习工具至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究图神经网络用于夸克-胶子鉴别，系统性地融入物理学动机的归纳偏置，以提高模型的可解释性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;设计强制执行红外和共线性(IRC)安全，以及在快度-方位角平面上的E(2)和O(2)等变性的消息传递架构。使用模拟的喷注数据集，将这些网络与无约束基线在分类性能、对软发射的鲁棒性和潜在表示结构方面进行比较。通过将能量流多项式回归到主要主成分上，建立学习表示与已建立的IRC安全喷注可观测量之间的对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;物理感知的网络在不同训练实例中更稳定，并将潜在方差分布在多个可解释的方向上。通过回归分析，建立了学习表示与已建立的IRC安全喷注可观测量之间的直接对应关系。&lt;h4&gt;结论&lt;/h4&gt;嵌入对称性和安全性约束不仅提高了模型的鲁棒性，还将网络表示建立在已知的QCD结构上，为对撞机物理学中的可解释深度学习提供了原则性方法。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在喷注分类任务中已取得显著成功，但一个关键挑战仍然存在：理解这些模型学习的内容以及它们的特征如何与已知的量子色动力学(QCD)可观测量相关。提高可解释性对于在对撞机物理学中构建强大可靠的机器学习工具至关重要。为了应对这一挑战，我们研究了用于夸克-胶子鉴别的图神经网络，系统性地融入了物理学动机的归纳偏置。特别是，我们设计了强制执行红外和共线性(IRC)安全，以及在快度-方位角平面上的E(2)和O(2)等变性的消息传递架构。使用模拟的喷注数据集，我们在分类性能、对软发射的鲁棒性和潜在表示结构方面将这些网络与无约束基线进行了比较。我们的分析表明，物理感知的网络在训练实例中更稳定，并将它们的潜在方差分布在多个可解释的方向上。通过将能量流多项式回归到主要主成分上，我们建立了学习表示与已建立的IRC安全喷注可观测量之间的直接对应关系。这些结果表明，嵌入对称性和安全性约束不仅提高了鲁棒性，还将网络表示建立在已知的QCD结构上，为对撞机物理学中的可解释深度学习提供了原则性方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has achieved remarkable success in jet classification tasks,yet a key challenge remains: understanding what these models learn and howtheir features relate to known QCD observables. Improving interpretability isessential for building robust and trustworthy machine learning tools incollider physics. To address this challenge, we investigate graph neuralnetworks for quark-gluon discrimination, systematically incorporatingphysics-motivated inductive biases. In particular, we design message-passingarchitectures that enforce infrared and collinear (IRC) safety, as well as E(2)and O(2) equivariance in the rapidity-azimuth plane. Using simulated jetdatasets, we compare these networks against unconstrained baselines in terms ofclassification performance, robustness to soft emissions, and latentrepresentation structures. Our analysis shows that physics-aware networks aremore stable across training instances and distribute their latent varianceacross multiple interpretable directions. By regressing Energy Flow Polynomialsonto the leading principal components, we establish a direct correspondencebetween learned representations and established IRC-safe jet observables. Theseresults demonstrate that embedding symmetry and safety constraints not onlyimproves robustness but also grounds network representations in known QCDstructures, providing a principled approach toward interpretable deep learningin collider physics.</description>
      <author>example@mail.com (Partha Konar, Vishal S. Ngairangbam, Michael Spannowsky, Deepanshu Srivastava)</author>
      <guid isPermaLink="false">2509.22059v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>MCGM: Multi-stage Clustered Global Modeling for Long-range Interactions in Molecules</title>
      <link>http://arxiv.org/abs/2509.22028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 1 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了多阶段聚类全局建模(MCGM)方法，解决了几何图神经网络在建模长距离相互作用时的局限性，实现了高效的全局信息捕获和特征增强。&lt;h4&gt;背景&lt;/h4&gt;几何图神经网络(GNNs)在捕捉分子几何方面表现出色，但其局部偏向的消息传递阻碍了长距离相互作用的建模。当前解决方案存在计算成本高、系统特定性强、参数调复杂等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级、即插即用的模块，使几何GNNs能够通过高效的聚类操作获得分层全局上下文，以更好地建模长距离相互作用。&lt;h4&gt;方法&lt;/h4&gt;MCGM方法构建原子簇的多分辨率层次结构，通过动态层次聚类提炼全局信息，通过学习转换传播此上下文，并通过残差连接最终强化原子特征。该方法可无缝集成到不同的骨干架构中。&lt;h4&gt;主要发现&lt;/h4&gt;MCGM将OE62能量预测误差平均降低26.2%；在AQM上实现了最先进的准确性(能量17.0 meV，力4.9 meV/Å)；同时使用比Neural P3M少20%的参数。&lt;h4&gt;结论&lt;/h4&gt;MCGM是一种有效的方法，能够增强几何GNNs建模长距离相互作用的能力，同时保持计算效率，具有广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;几何图神经网络(GNNs)在捕捉分子几何方面表现出色，但其局部偏向的消息传递阻碍了长距离相互作用的建模。当前解决方案存在根本性限制：扩展截止半径会导致计算成本随距离立方增长；物理启发的核(如库仑、色散)通常是系统特定的，缺乏通用性；傅里叶空间方法需要仔细调整多个参数，并增加计算开销。我们引入了多阶段聚类全局建模(MCGM)，这是一种轻量级、即插即用的模块，通过高效的聚类操作赋予几何GNNs分层全局上下文。MCGM构建原子簇的多分辨率层次结构，通过动态层次聚类提炼全局信息，并通过学习转换传播此上下文，最终通过残差连接强化原子特征。无缝集成到四种不同的骨干架构中，MCGM将OE62能量预测误差平均降低26.2%。在AQM上，MCGM实现了最先进的准确性(能量17.0 meV，力4.9 meV/Å)，同时使用的参数比Neural P3M少20%。代码将在接受后公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric graph neural networks (GNNs) excel at capturing molecular geometry,yet their locality-biased message passing hampers the modeling of long-rangeinteractions. Current solutions have fundamental limitations: extending cutoffradii causes computational costs to scale cubically with distance;physics-inspired kernels (e.g., Coulomb, dispersion) are often system-specificand lack generality; Fourier-space methods require careful tuning of multipleparameters (e.g., mesh size, k-space cutoff) with added computational overhead.We introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,plug-and-play module that endows geometric GNNs with hierarchical globalcontext through efficient clustering operations. MCGM builds a multi-resolutionhierarchy of atomic clusters, distills global information via dynamichierarchical clustering, and propagates this context back through learnedtransformations, ultimately reinforcing atomic features via residualconnections. Seamlessly integrated into four diverse backbone architectures,MCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGMachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\AA} forforces) while using 20% fewer parameters than Neural P3M. Code will be madeavailable upon acceptance.</description>
      <author>example@mail.com (Haodong Pan, Yusong Wang, Nanning Zheng, Caijui Jiang)</author>
      <guid isPermaLink="false">2509.22028v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks</title>
      <link>http://arxiv.org/abs/2509.21735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种可解释的时空图神经网络框架，利用双随机微分方程对不规则采样的纵向功能磁共振成像数据进行建模，以预测阿尔茨海默病进展。该框架在两个独立队列上得到验证，能够识别与疾病进展相关的大脑回路异常，并发现新的生物标志物。&lt;h4&gt;背景&lt;/h4&gt;识别能够预测阿尔茨海默病进展的神经影像生物标志物对于及时干预至关重要，但由于现有方法往往忽略了大脑网络时空特性的复杂功能障碍，这一任务仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个可解释的时空图神经网络框架来预测未来的阿尔茨海默病进展。&lt;h4&gt;方法&lt;/h4&gt;利用双随机微分方程对不规则采样的纵向功能磁共振成像数据进行建模，并在OASIS-3和ADNI两个独立队列上验证该方法。&lt;h4&gt;主要发现&lt;/h4&gt;框架能够学习稀疏的区域和连接重要性概率，识别出海马旁皮层、前额叶皮层和顶小叶为显著区域，在腹侧注意、背侧注意和默认模式网络中存在显著紊乱；这些异常与AD临床症状有强相关性；解释性策略揭示了已知的和新的神经系统水平以及性别特异性的生物标志物。&lt;h4&gt;结论&lt;/h4&gt;时空图学习方法在不规则采样的纵向成像数据背景下，对AD进展的早期、个体化预测具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;识别用于预测阿尔茨海默病进展的客观神经影像生物标志物对于及时干预至关重要。然而，由于潜在大脑网络时空特性的复杂功能障碍，这一任务仍然具有挑战性，现有方法往往忽视了这些特点。为解决这些局限性，我们开发了一个可解释的时空图神经网络框架来预测未来的AD进展，利用双随机微分方程对不规则采样的纵向功能磁共振成像数据进行建模。我们在两个独立队列上验证了我们的方法，包括开放获取影像研究系列(OASIS-3)和阿尔茨海默病神经影像倡议(ADNI)。我们的框架有效地学习稀疏的区域和连接重要性概率，能够识别与疾病进展相关的大脑回路异常。值得注意的是，我们检测到海马旁皮层、前额叶皮层和顶小叶为显著区域，在腹侧注意、背侧注意和默认模式网络中存在显著紊乱。这些异常与纵向AD相关临床症状有很强的相关性。此外，我们的解释性策略揭示了已知的和新的神经系统水平和性别特异性的生物标志物，为理解AD进展的神经生物学机制提供了新的见解。我们的研究结果强调了时空图学习方法在不规则采样纵向成像数据背景下对AD进展早期、个体化预测的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease(AD) progression is crucial for timely intervention. However, this task remainschallenging due to the complex dysfunctions in the spatio-temporalcharacteristics of underlying brain networks, which are often overlooked byexisting methods. To address these limitations, we develop an interpretablespatio-temporal graph neural network framework to predict future ADprogression, leveraging dual Stochastic Differential Equations (SDEs) to modelthe irregularly-sampled longitudinal functional magnetic resonance imaging(fMRI) data. We validate our approach on two independent cohorts, including theOpen Access Series of Imaging Studies (OASIS-3) and the Alzheimer's DiseaseNeuroimaging Initiative (ADNI). Our framework effectively learns sparseregional and connective importance probabilities, enabling the identificationof key brain circuit abnormalities associated with disease progression.Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietallobule as salient regions, with significant disruptions in the ventralattention, dorsal attention, and default mode networks. These abnormalitiescorrelate strongly with longitudinal AD-related clinical symptoms. Moreover,our interpretability strategy reveals both established and novel neuralsystems-level and sex-specific biomarkers, offering new insights into theneurobiological mechanisms underlying AD progression. Our findings highlightthe potential of spatio-temporal graph-based learning for early, individualizedprediction of AD progression, even in the context of irregularly-sampledlongitudinal imaging data.</description>
      <author>example@mail.com (Houliang Zhou, Rong Zhou, Yangying Liu, Kanhao Zhao, Li Shen, Brian Y. Chen, Yu Zhang, Lifang He, Alzheimer's Disease Neuroimaging Initiative)</author>
      <guid isPermaLink="false">2509.21735v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Exact Subgraph Isomorphism Network for Predictive Graph Mining</title>
      <link>http://arxiv.org/abs/2509.21699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了精确子图同构网络（EIN），结合子图枚举、神经网络和稀疏正则化，实现了高判别能力和可解释性的图级别预测。&lt;h4&gt;背景&lt;/h4&gt;在图级别预测任务中，输入图的子图信息起着关键作用，构建具有高判别能力和可解释性的图级别预测模型仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够实现高判别能力和可解释性的图级别预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出精确子图同构网络（EIN），结合精确子图枚举、神经网络和稀疏正则化，通过子图枚举和神经网络的组合提高判别能力，利用稀疏正则化实现有效剪枝和重要子图识别。&lt;h4&gt;主要发现&lt;/h4&gt;EIN相比标准图神经网络模型具有足够高的预测性能，并能通过所选子图进行有效的事后分析。&lt;h4&gt;结论&lt;/h4&gt;EIN通过结合子图枚举、神经网络和稀疏正则化，成功实现了高判别能力和可解释性的图级别预测。&lt;h4&gt;翻译&lt;/h4&gt;在图级别预测任务（为给定图预测标签）中，输入图的子图信息起着关键作用。在本文中，我们提出了精确子图同构网络（EIN），它结合了精确子图枚举、神经网络和稀疏正则化。通常，构建具有高判别能力和可解释性的图级别预测模型仍然是一个具有挑战性的问题。我们的子图枚举和神经网络的组合有助于提高对输入图子图结构的高判别能力。此外，EIN中的稀疏正则化使我们能够：1)推导出一种有效的剪枝策略，减轻枚举的计算难度，同时保持预测性能；2)识别对高可解释性有贡献的重要子图。我们经验性地证明，与标准图神经网络模型相比，EIN具有足够高的预测性能，同时，我们还展示了基于所选子图的事后分析示例。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the graph-level prediction task (predict a label for a given graph), theinformation contained in subgraphs of the input graph plays a key role. In thispaper, we propose Exact subgraph Isomorphism Network (EIN), which combines theexact subgraph enumeration, neural network, and a sparse regularization. Ingeneral, building a graph-level prediction model achieving high discriminativeability along with interpretability is still a challenging problem. Ourcombination of the subgraph enumeration and neural network contributes to highdiscriminative ability about the subgraph structure of the input graph.Further, the sparse regularization in EIN enables us 1) to derive an effectivepruning strategy that mitigates computational difficulty of the enumerationwhile maintaining the prediction performance, and 2) to identify importantsubgraphs that contributes to high interpretability. We empirically show thatEIN has sufficiently high prediction performance compared with standard graphneural network models, and also, we show examples of post-hoc analysis based onthe selected subgraphs.</description>
      <author>example@mail.com (Taiga Kojima, Masayuki Karasuyama)</author>
      <guid isPermaLink="false">2509.21699v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Shoot from the HIP: Hessian Interatomic Potentials without derivatives</title>
      <link>http://arxiv.org/abs/2509.21624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/BurgerAndreas/hip&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HIP的深度学习方法，可以直接预测分子Hessian矩阵，无需依赖自动微分或有限差分，在计算化学任务中表现出显著优势。&lt;h4&gt;背景&lt;/h4&gt;计算化学中的基本任务，如过渡态搜索和振动分析，依赖于分子Hessian矩阵（势能的二阶导数）。然而，传统方法计算成本高，且随着系统规模增大扩展性差，无论是量子力学方法还是神经网络方法都存在这一问题。&lt;h4&gt;目的&lt;/h4&gt;展示可以直接通过深度学习模型预测Hessian矩阵，提高计算效率并解决扩展性问题。&lt;h4&gt;方法&lt;/h4&gt;从图神经网络消息传递过程中计算出的不可约表示特征（最高到l=2度）构建SE(3)-等变、对称的Hessian矩阵。这种方法被称为HIP Hessians。&lt;h4&gt;主要发现&lt;/h4&gt;HIP Hessians比传统方法快一到两个数量级，更准确，内存效率更高，更容易训练，且随系统规模增大具有更好的扩展性。在过渡态搜索、加速几何优化、零点能校正和振动分析基准测试中均表现出一致优越的性能。&lt;h4&gt;结论&lt;/h4&gt;HIP方法为计算化学中的Hessian矩阵计算提供了高效解决方案，已开源代码库和模型权重以促进进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;计算化学中的基本任务，从过渡态搜索到振动分析，都依赖于分子Hessian矩阵，即势能的二阶导数。然而，Hessian矩阵的计算成本很高，并且随着系统规模增大而扩展性差，无论是量子力学方法还是神经网络方法都是如此。在这项工作中，我们展示Hessian矩阵可以直接从深度学习模型预测，而不依赖于自动微分或有限差分。我们观察到可以从图神经网络消息传递过程中计算出的不可约表示特征（最高到l=2度）构建SE(3)-等变、对称的Hessian矩阵。这使得HIP Hessians比传统方法快一到两个数量级，更准确，内存效率更高，更容易训练，并实现了随系统规模增大更有利的扩展性。我们在广泛的下游任务中验证了我们的预测，证明在过渡态搜索、加速几何优化、零点能校正和振动分析基准测试中均表现出一致优越的性能。我们在https://github.com/BurgerAndreas/hip开源了HIP代码库和模型权重，以促进Hessian直接预测的进一步发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fundamental tasks in computational chemistry, from transition state search tovibrational analysis, rely on molecular Hessians, which are the secondderivatives of the potential energy. Yet, Hessians are computationallyexpensive to calculate and scale poorly with system size, with both quantummechanical methods and neural networks. In this work, we demonstrate thatHessians can be predicted directly from a deep learning model, without relyingon automatic differentiation or finite differences. We observe that one canconstruct SE(3)-equivariant, symmetric Hessians from irreduciblerepresentations (irrep) features up to degree $l$=2 computed during messagepassing in graph neural networks. This makes HIP Hessians one to two orders ofmagnitude faster, more accurate, more memory efficient, easier to train, andenables more favorable scaling with system size. We validate our predictionsacross a wide range of downstream tasks, demonstrating consistently superiorperformance for transition state search, accelerated geometry optimization,zero-point energy corrections, and vibrational analysis benchmarks. Weopen-source the HIP codebase and model weights to enable further development ofthe direct prediction of Hessians at https://github.com/BurgerAndreas/hip</description>
      <author>example@mail.com (Andreas Burger, Luca Thiede, Nikolaj Rønne, Varinia Bernales, Nandita Vijaykumar, Tejs Vegge, Arghya Bhowmik, Alan Aspuru-Guzik)</author>
      <guid isPermaLink="false">2509.21624v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.21567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用脑电图(EEG)数据和机器学习模型预测消费者行为，比较了图神经网络(GNN)与经典机器学习模型的性能&lt;h4&gt;背景&lt;/h4&gt;消费者行为预测是营销、认知神经科学和人机交互领域的重要目的，EEG数据可通过提供大脑神经活动的详细信息来帮助分析决策过程&lt;h4&gt;目的&lt;/h4&gt;利用比较方法通过EEG数据预测消费者行为&lt;h4&gt;方法&lt;/h4&gt;提取并清理Neuma数据集的EEG特征，为GNN模型创建脑连接特征，实现不同架构的GNN模型，应用广泛的经典模型如集成模型进行对比&lt;h4&gt;主要发现&lt;/h4&gt;总体结果无显著差异，但GNN模型在某些基本标准上表现优于经典模型&lt;h4&gt;结论&lt;/h4&gt;结合EEG信号分析和机器学习模型可提供更深入理解消费者行为的方法，同时为EEG神经营销领域常用模型与新兴模型(如GNN)提供了全面比较&lt;h4&gt;翻译&lt;/h4&gt;消费者行为预测是营销、认知神经科学和人机交互领域的重要目的之一。脑电图(EEG)数据可以通过提供大脑神经活动的详细信息来帮助分析决策过程。在本研究中，采用比较方法通过EEG数据预测消费者行为。首先，提取并清理了来自Neuma数据集的EEG数据特征。对于图神经网络(GNN)模型，创建了脑连接特征。使用了不同的机器学习模型，如经典模型和图神经网络，并进行比较。实现了具有不同架构的GNN模型进行全面比较；此外，应用了广泛的经典模型，如集成模型，这些模型可以非常有帮助地展示每个模型在数据集上的差异和性能。尽管总体结果没有显示出显著差异，但GNN模型在某些基本标准上通常表现更好，而经典模型在这些标准上表现不令人满意。这项研究不仅表明结合EEG信号分析和机器学习模型可以提供一种更深入理解消费者行为的方法，还提供了先前在基于EEG的神经营销研究中广泛使用的机器学习模型(如支持向量机SVM)与在该领域未使用或很少使用的模型(如图神经网络)之间的全面比较。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prediction of consumer behavior is one of the important purposes inmarketing, cognitive neuroscience, and human-computer interaction. Theelectroencephalography (EEG) data can help analyze the decision process byproviding detailed information about the brain's neural activity. In thisresearch, a comparative approach is utilized for predicting consumer behaviorby EEG data. In the first step, the features of the EEG data from the NeuMadataset were extracted and cleaned. For the Graph Neural Network (GNN) models,the brain connectivity features were created. Different machine learningmodels, such as classical models and Graph Neural Networks, are used andcompared. The GNN models with different architectures are implemented to have acomprehensive comparison; furthermore, a wide range of classical models, suchas ensemble models, are applied, which can be very helpful to show thedifference and performance of each model on the dataset. Although the resultsdid not show a significant difference overall, the GNN models generallyperformed better in some basic criteria where classical models were notsatisfactory. This study not only shows that combining EEG signal analysis andmachine learning models can provide an approach to deeper understanding ofconsumer behavior, but also provides a comprehensive comparison between themachine learning models that have been widely used in previous studies in theEEG-based neuromarketing such as Support Vector Machine (SVM), and the modelswhich are not used or rarely used in the field, like Graph Neural Networks.</description>
      <author>example@mail.com (Mohammad Parsa Afshar, Aryan Azimi)</author>
      <guid isPermaLink="false">2509.21567v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>GraphPFN: A Prior-Data Fitted Graph Foundation Model</title>
      <link>http://arxiv.org/abs/2509.21489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GraphPFN，一种用于节点级预测的先验数据拟合网络，通过在合成图上预训练实现了图基础模型的性能突破。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大规模数据集上的预训练已改变自然语言处理和计算机视觉领域，但在图数据上的应用仍然有限。现有的图基础模型如G2T-FM虽优于之前尝试，但主要依赖手工特征，限制了学习复杂图模式的能力。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够捕捉图结构依赖关系的图基础模型，克服现有方法对手工特征的依赖。&lt;h4&gt;方法&lt;/h4&gt;1) 设计合成属性图的先验分布；2) 使用随机块模型和优先连接过程的组合生成图结构；3) 应用图感知结构因果模型生成节点属性和目标；4) 将表格基础模型LimiX与图邻域聚合层结合；5) 在合成图上训练模型以捕获图结构依赖。&lt;h4&gt;主要发现&lt;/h4&gt;GraphPFN在多达50,000个节点的多样化真实世界图数据集上展现出强大的上下文学习能力，微调后达到最先进结果，在大多数数据集上优于G2T-FM和任务特定图神经网络。&lt;h4&gt;结论&lt;/h4&gt;在精心设计的先验分布的合成图上进行预训练是构建图基础模型的有效策略。&lt;h4&gt;翻译&lt;/h4&gt;在大型数据集上预训练的基础模型已经改变了自然语言处理和计算机视觉等领域，但它们在图数据上的应用仍然有限。最近出现的图基础模型，如G2T-FM，利用表格基础模型处理图任务，并被证明显著优于之前创建图基础模型的尝试。然而，这些模型主要依赖手工制作的图特征，限制了它们学习复杂图特定模式的能力。在这项工作中，我们提出了GraphPFN：一种用于节点级预测的先验数据拟合网络。首先，我们设计了一种合成属性图的先验分布。对于图结构生成，我们使用多个随机块模型和优先连接过程的新组合。然后，我们应用图感知结构因果模型生成节点属性和目标。这个过程使我们能够高效生成各种真实的图数据集。接着，我们将表格基础模型LimiX与基于注意力的图邻域聚合层相结合，并在从我们的先验分布采样的合成图上训练它，使模型能够捕获表格数据中不存在的图结构依赖关系。在多达50,000个节点的多样化真实世界图数据集上，GraphPFN展现了强大的上下文学习能力，并且在微调后达到了最先进的结果，在大多数数据集上都优于G2T-FM和从零开始训练的任务特定图神经网络。更广泛地说，我们的工作证明了在精心设计的先验分布的合成图上进行预训练是构建图基础模型的有效策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models pretrained on large-scale datasets have transformed suchfields as natural language processing and computer vision, but theirapplication to graph data remains limited. Recently emerged graph foundationmodels, such as G2T-FM, utilize tabular foundation models for graph tasks andwere shown to significantly outperform prior attempts to create GFMs. However,these models primarily rely on hand-crafted graph features, limiting theirability to learn complex graph-specific patterns. In this work, we proposeGraphPFN: a prior-data fitted network for node-level prediction. First, wedesign a prior distribution of synthetic attributed graphs. For graph structuregeneration, we use a novel combination of multiple stochastic block models anda preferential attachment process. We then apply graph-aware structured causalmodels to generate node attributes and targets. This procedure allows us toefficiently generate a wide range of realistic graph datasets. Then, we augmentthe tabular foundation model LimiX with attention-based graph neighborhoodaggregation layers and train it on synthetic graphs sampled from our prior,allowing the model to capture graph structural dependencies not present intabular data. On diverse real-world graph datasets with up to 50,000 nodes,GraphPFN shows strong in-context learning performance and achievesstate-of-the-art results after finetuning, outperforming both G2T-FM andtask-specific GNNs trained from scratch on most datasets. More broadly, ourwork demonstrates that pretraining on synthetic graphs from a well-designedprior distribution is an effective strategy for building graph foundationmodels.</description>
      <author>example@mail.com (Dmitry Eremeev, Oleg Platonov, Gleb Bazhenov, Artem Babenko, Liudmila Prokhorenkova)</author>
      <guid isPermaLink="false">2509.21489v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision</title>
      <link>http://arxiv.org/abs/2509.22631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Labeling Copilot是一个用于计算机视觉的数据策展深度研究代理，通过大型多模态语言模型驱动的中央编排器代理，结合三个核心能力工具解决了高质量数据集构建的瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;高质量、特定领域的数据集是部署强大视觉系统的主要瓶颈，在研究大量未标记数据时，需要在数据质量、多样性和成本之间进行复杂的权衡。&lt;h4&gt;目的&lt;/h4&gt;介绍Labeling Copilot，这是第一个用于计算机视觉的数据策展深度研究代理，旨在解决数据集构建的瓶颈问题。&lt;h4&gt;方法&lt;/h4&gt;一个由大型多模态语言模型驱动的中央编排器代理，使用多步推理执行三个核心能力的专业工具：(1)校准发现：从大型存储库中获取相关、分布内的数据；(2)可控合成：通过强大的过滤为罕见场景生成新数据；(3)共识标注：通过新颖的共识机制编排多个基础模型产生准确的标注。&lt;h4&gt;主要发现&lt;/h4&gt;共识标注模块在COCO数据集上平均每张图像产生14.2个候选提案，几乎是真实物体数量的两倍，达到37.1%的标注mAP；在Open Images数据集上发现了903个新类别；校准发现工具比同类方法高效40倍。&lt;h4&gt;结论&lt;/h4&gt;具有优化、可扩展工具的代理工作流程为策展工业规模数据集提供了强大的基础。&lt;h4&gt;翻译&lt;/h4&gt;策划高质量、特定领域的数据集是部署强大视觉系统的主要瓶颈，在研究大量未标记数据时，需要在数据质量、多样性和成本之间进行复杂的权衡。我们介绍了Labeling Copilot，这是第一个用于计算机视觉的数据策展深度研究代理。一个由大型多模态语言模型驱动的中央编排器代理，使用多步推理执行三个核心能力的专业工具：(1)校准发现从大型存储库中获取相关、分布内的数据；(2)可控合成通过强大的过滤为罕见场景生成新数据；(3)共识标注通过新颖的共识机制编排多个基础模型产生准确的标注，该机制结合了非极大值抑制和投票。我们的大规模验证证明了Labeling Copilot组件的有效性。共识标注模块在物体发现方面表现出色：在密集的COCO数据集上，平均每张图像有14.2个候选提案，几乎是7.4个真实物体的两倍，最终达到37.1%的标注mAP。在网络规模的Open Images数据集上，它处理了极度的类别不平衡，发现了903个新的边界框类别。同时，校准发现工具在1000万样本规模测试中，采用主动学习策略，比具有相同样本效率的替代方法高效40倍。这些实验验证了具有优化、可扩展工具的代理工作流程为策展工业规模数据集提供了强大的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Curating high-quality, domain-specific datasets is a major bottleneck fordeploying robust vision systems, requiring complex trade-offs between dataquality, diversity, and cost when researching vast, unlabeled data lakes. Weintroduce Labeling Copilot, the first data curation deep research agent forcomputer vision. A central orchestrator agent, powered by a large multimodallanguage model, uses multi-step reasoning to execute specialized tools acrossthree core capabilities: (1) Calibrated Discovery sources relevant,in-distribution data from large repositories; (2) Controllable Synthesisgenerates novel data for rare scenarios with robust filtering; and (3)Consensus Annotation produces accurate labels by orchestrating multiplefoundation models via a novel consensus mechanism incorporating non-maximumsuppression and voting. Our large-scale validation proves the effectiveness ofLabeling Copilot's components. The Consensus Annotation module excels at objectdiscovery: on the dense COCO dataset, it averages 14.2 candidate proposals perimage-nearly double the 7.4 ground-truth objects-achieving a final annotationmAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme classimbalance to discover 903 new bounding box categories, expanding its capabilityto over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a10-million sample scale, features an active learning strategy that is up to 40xmore computationally efficient than alternatives with equivalent sampleefficiency. These experiments validate that an agentic workflow with optimized,scalable tools provides a robust foundation for curating industrial-scaledatasets.</description>
      <author>example@mail.com (Debargha Ganguly, Sumit Kumar, Ishwar Balappanawar, Weicong Chen, Shashank Kambhatla, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam Kumaraguru, Vipin Chaudhary)</author>
      <guid isPermaLink="false">2509.22631v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Guiding Evolution of Artificial Life Using Vision-Language Models</title>
      <link>http://arxiv.org/abs/2509.22447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures. Accepted for publication in the Proceedings of  the Artificial Life Conference 2025 (MIT Press)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出ASAL++方法，利用多模态基础模型引导的类开放式搜索在人工生命模拟中实现进化目标&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)为人工生命(ALife)领域提供了通过自动化搜索ALife模拟的新工具，先前工作使用视觉语言模型(VLMs)将ALife模拟与自然语言目标提示对齐&lt;h4&gt;目的&lt;/h4&gt;在ASAL方法基础上开发ASAL++，实现具有开放式特征的基础模型驱动的人工生命发现&lt;h4&gt;方法&lt;/h4&gt;使用第二个基础模型基于模拟的视觉历史提出新的进化目标，诱导出具有越来越复杂目标的进化轨迹；探索两种策略：(1)每次迭代进化模拟以匹配单个新提示(EST)；(2)进化模拟以匹配生成的提示序列(ETT)；在Lenia基质中使用Gemma-3测试&lt;h4&gt;主要发现&lt;/h4&gt;EST策略促进更大的视觉新颖性，ETT策略促进更连贯和可解释的进化序列&lt;h4&gt;结论&lt;/h4&gt;ASAL++指向具有开放式特征的基础模型驱动人工生命发现的新方向&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)最近通过提供强大的工具来自动化搜索人工生命(ALife)模拟，为人工生命领域开辟了新前沿。先前的工作使用视觉语言模型(VLMs)将人工生命模拟与自然语言目标提示对齐。我们在自动化搜索人工生命(ASAL)的基础上引入ASAL++，这是一种由多模态基础模型引导的类开放式搜索方法。我们使用第二个基础模型基于模拟的视觉历史提出新的进化目标，从而诱导出具有越来越复杂目标的进化轨迹。我们探索两种策略：(1)每次迭代进化模拟以匹配单个新提示(进化监督目标：EST)；(2)进化模拟以匹配生成的提示序列(进化时间目标：ETT)。我们在Lenia基质中使用Gemma-3提出进化目标，实证测试了我们的方法，结果表明EST促进更大的视觉新颖性，而ETT促进更连贯和可解释的进化序列。我们的研究结果表明，ASAL++指向具有开放式特征的基础模型驱动人工生命发现的新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) have recently opened up new frontiers in the field ofartificial life (ALife) by providing powerful tools to automate search throughALife simulations. Previous work aligns ALife simulations with natural languagetarget prompts using vision-language models (VLMs). We build on AutomatedSearch for Artificial Life (ASAL) by introducing ASAL++, a method foropen-ended-like search guided by multimodal FMs. We use a second FM to proposenew evolutionary targets based on a simulation's visual history. This inducesan evolutionary trajectory with increasingly complex targets.  We explore two strategies: (1) evolving a simulation to match a single newprompt at each iteration (Evolved Supervised Targets: EST) and (2) evolving asimulation to match the entire sequence of generated prompts (Evolved TemporalTargets: ETT). We test our method empirically in the Lenia substrate usingGemma-3 to propose evolutionary targets, and show that EST promotes greatervisual novelty, while ETT fosters more coherent and interpretable evolutionarysequences.  Our results suggest that ASAL++ points towards new directions for FM-drivenALife discovery with open-ended characteristics.</description>
      <author>example@mail.com (Nikhil Baid, Hannah Erlebach, Paul Hellegouarch, Frederico Wieser)</author>
      <guid isPermaLink="false">2509.22447v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation</title>
      <link>http://arxiv.org/abs/2509.22441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper introduces the first VLA framework for AUVs, featuring a  dual-brain architecture and zero-data MPC for real-world underwater  navigation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UnderwaterVLA，一种将多模态基础模型与具身智能系统集成的新型自主水下导航框架，通过三种创新解决了水下操作面临的挑战，并在实地测试中显示出优越性能。&lt;h4&gt;背景&lt;/h4&gt;水下操作面临流体动力学干扰、通信带宽有限以及浑浊水域中传感器性能下降等挑战，导致自主导航困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在复杂水下环境中实现高效导航的自主水下导航系统，减少对特定训练数据的依赖，提高环境适应性。&lt;h4&gt;方法&lt;/h4&gt;提出三种创新：1) 双脑架构解耦高级任务推理与低级反应控制；2) 首次将视觉-语言-行动模型应用于水下机器人，实现可解释决策；3) 流体动力学感知的模型预测控制方案实时补偿流体效应。&lt;h4&gt;主要发现&lt;/h4&gt;在实地测试中，UnderwaterVLA在视觉条件下降的情况下减少了导航误差，比基线方法提高任务完成率19%至27%。&lt;h4&gt;结论&lt;/h4&gt;UnderwaterVLA通过减少对水下特定训练数据的依赖并提高环境适应性，为下一代智能自主水下航行器(AUVs)提供了可扩展且经济有效的技术路径。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了UnderwaterVLA，一种新颖的自主水下导航框架，将多模态基础模型与具身智能系统集成。由于流体动力学干扰、通信带宽有限以及浑浊水域中传感器性能下降，水下操作仍然困难。为应对这些挑战，我们引入了三项创新。首先，双脑架构将高级任务推理与低级反应控制解耦，使系统在通信和计算约束下能够稳健运行。其次，我们首次将视觉-语言-行动模型应用于水下机器人，纳入结构化思维链推理以实现可解释的决策制定。第三，流体动力学感知的模型预测控制方案实时补偿流体效应，无需昂贵的任务特定训练。实地测试结果表明，UnderwaterVLA在视觉条件下降的情况下减少了导航误差，同时比基线方法提高任务完成率19%至27%。通过减少对水下特定训练数据的依赖并提高环境适应性，UnderwaterVLA为下一代智能AUVs提供了可扩展且经济有效的技术路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents UnderwaterVLA, a novel framework for autonomousunderwater navigation that integrates multimodal foundation models withembodied intelligence systems. Underwater operations remain difficult due tohydrodynamic disturbances, limited communication bandwidth, and degradedsensing in turbid waters. To address these challenges, we introduce threeinnovations. First, a dual-brain architecture decouples high-level missionreasoning from low-level reactive control, enabling robust operation undercommunication and computational constraints. Second, we applyVision-Language-Action(VLA) models to underwater robotics for the first time,incorporating structured chain-of-thought reasoning for interpretabledecision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)scheme compensates for fluid effects in real time without costly task-specifictraining. Experimental results in field tests show that UnderwaterVLA reducesnavigation errors in degraded visual conditions while maintaining higher taskcompletion by 19% to 27% over baseline. By minimizing reliance onunderwater-specific training data and improving adaptability acrossenvironments, UnderwaterVLA provides a scalable and cost-effective path towardthe next generation of intelligent AUVs.</description>
      <author>example@mail.com (Zhangyuan Wang, Yunpeng Zhu, Yuqi Yan, Xiaoyuan Tian, Xinhao Shao, Meixuan Li, Weikun Li, Guangsheng Su, Weicheng Cui, Dixia Fan)</author>
      <guid isPermaLink="false">2509.22441v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning</title>
      <link>http://arxiv.org/abs/2509.22403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MoveFM-R框架，通过结合移动基础模型(MFMs)和大型语言模型(LLMs)的优势，实现了更全面、可解释和强大的人类移动性建模。&lt;h4&gt;背景&lt;/h4&gt;移动基础模型(MFMs)在建模人类移动模式方面取得进展，但因数据规模和语义理解限制而面临瓶颈。大型语言模型(LLMs)虽具强大语义推理能力，但缺乏生成物理合理移动轨迹所需的时间和空间统计内在理解。&lt;h4&gt;目的&lt;/h4&gt;提出MoveFM-R框架，利用语言驱动的语义推理能力释放移动基础模型潜力，解决两个关键挑战：连续地理坐标与离散语言标记间的词汇不匹配，以及MFM潜在向量与LLM语义世界间的表示差距。&lt;h4&gt;方法&lt;/h4&gt;MoveFM-R建立在三个核心创新上：语义增强的位置编码以弥合地理-语言差距；渐进式课程使LLM推理与移动模式保持一致；交互式自反思机制用于条件轨迹生成。&lt;h4&gt;主要发现&lt;/h4&gt;广泛实验表明，MoveFM-R显著优于现有基于MFM和LLM的基线模型。在零样本设置中表现出强大泛化能力，并擅长根据自然语言指令生成真实轨迹。&lt;h4&gt;结论&lt;/h4&gt;通过结合MFM的统计能力和LLM的深度语义理解，MoveFM-R开创了人类移动性建模的新范式，实现更全面、可解释和强大的建模效果。&lt;h4&gt;翻译&lt;/h4&gt;移动基础模型(MFMs)已推进了人类移动模式的建模，但由于数据规模和语义理解的限制，它们遇到了瓶颈。虽然大型语言模型(LLMs)提供了强大的语义推理能力，但它们缺乏生成物理上合理的移动轨迹所需的时间和空间统计的内在理解。为了解决这些差距，我们提出了MoveFM-R，一个新颖的框架，通过利用语言驱动的语义推理能力释放了移动基础模型的全部潜力。它解决了两个关键挑战：连续地理坐标与离散语言标记之间的词汇不匹配，以及MFM的潜在向量与LLM的语义世界之间的表示差距。MoveFM-R建立在三个核心创新之上：语义增强的位置编码，以弥合地理-语言差距；渐进式课程，使LLM的推理与移动模式保持一致；以及交互式自反思机制，用于条件轨迹生成。广泛的实验表明，MoveFM-R显著优于现有的基于MFM和LLM的基线。它还在零样本设置中表现出强大的泛化能力，并擅长根据自然语言指令生成真实的轨迹。通过结合MFM的统计能力和LLM的深度语义理解，MoveFM-R开创了一种新范式，能够更全面、可解释和强大地建模人类移动性。MoveFM-R的实现可在网上https://anonymous.4open.science/r/MoveFM-R-CDE7/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobility Foundation Models (MFMs) have advanced the modeling of humanmovement patterns, yet they face a ceiling due to limitations in data scale andsemantic understanding. While Large Language Models (LLMs) offer powerfulsemantic reasoning, they lack the innate understanding of spatio-temporalstatistics required for generating physically plausible mobility trajectories.To address these gaps, we propose MoveFM-R, a novel framework that unlocks thefull potential of mobility foundation models by leveraging language-drivensemantic reasoning capabilities. It tackles two key challenges: the vocabularymismatch between continuous geographic coordinates and discrete languagetokens, and the representation gap between the latent vectors of MFMs and thesemantic world of LLMs. MoveFM-R is built on three core innovations: asemantically enhanced location encoding to bridge the geography-language gap, aprogressive curriculum to align the LLM's reasoning with mobility patterns, andan interactive self-reflection mechanism for conditional trajectory generation.Extensive experiments demonstrate that MoveFM-R significantly outperformsexisting MFM-based and LLM-based baselines. It also shows robust generalizationin zero-shot settings and excels at generating realistic trajectories fromnatural language instructions. By synthesizing the statistical power of MFMswith the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigmthat enables a more comprehensive, interpretable, and powerful modeling ofhuman mobility. The implementation of MoveFM-R is available online athttps://anonymous.4open.science/r/MoveFM-R-CDE7/.</description>
      <author>example@mail.com (Fanjin Meng, Yuan Yuan, Jingtao Ding, Jie Feng, Chonghua Han, Yong Li)</author>
      <guid isPermaLink="false">2509.22403v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models</title>
      <link>http://arxiv.org/abs/2509.22360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了CHRONOBERG，一个时间结构化的英语书籍语料库，跨越250年，来自古腾堡计划并添加了时间注释。研究通过这个语料库探讨了大型语言模型在捕捉语言随时间变化方面的能力，并展示了现代LLM工具在检测歧视性语言和跨时间段情境化情感方面的不足。&lt;h4&gt;背景&lt;/h4&gt;现有的大型语言模型能够利用社交媒体和网络抓取的各种数据在规模上运行。然而，虽然现有语料库多样化，但它们经常缺乏长期时间结构，这可能限制LLM将语言语义和规范演变置于语境中的能力，以及捕捉历时变化的能力。&lt;h4&gt;目的&lt;/h4&gt;为了支持对语言历时变化的分析和训练，作者引入了CHRONOBERG语料库，旨在帮助大型语言模型更好地捕捉语言的语义和规范演变，以及理解历时变化。&lt;h4&gt;方法&lt;/h4&gt;作者从古腾堡计划策划了一个跨越250年的英语书籍文本的时间结构化语料库，并添加了各种时间注释。他们利用书籍经过编辑的性质，通过时间敏感的效价-唤醒-优势分析来量化词汇语义变化随时间的变化，并构建了历史上校准的情感词汇表。&lt;h4&gt;主要发现&lt;/h4&gt;研究显示，现代基于LLM的工具需要更好地定位它们在不同时间段对歧视性语言的检测和情感的情境化。在CHRONOBERG上顺序训练的语言模型难以编码意义的历时转变，强调了时间感知训练和评估流程的必要性。&lt;h4&gt;结论&lt;/h4&gt;CHRONOBERG被定位为研究语言变化和时间泛化的可扩展资源，强调了时间感知训练和评估流程的重要性，以及需要改进大型语言模型捕捉语言随时间变化的能力。&lt;h4&gt;翻译&lt;/h4&gt;论文包括可能对读者有冒犯性的语言和样本展示。CHRONOBERG在HuggingFace上公开可用，代码可在GitHub获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) excel at operating at scale by leveraging socialmedia and various data crawled from the web. Whereas existing corpora arediverse, their frequent lack of long-term temporal structure may however limitan LLM's ability to contextualize semantic and normative evolution of languageand to capture diachronic variation. To support analysis and training for thelatter, we introduce CHRONOBERG, a temporally structured corpus of English booktexts spanning 250 years, curated from Project Gutenberg and enriched with avariety of temporal annotations. First, the edited nature of books enables usto quantify lexical semantic change through time-sensitiveValence-Arousal-Dominance (VAD) analysis and to construct historicallycalibrated affective lexicons to support temporally grounded interpretation.With the lexicons at hand, we demonstrate a need for modern LLM-based tools tobetter situate their detection of discriminatory language and contextualizationof sentiment across various time-periods. In fact, we show how language modelstrained sequentially on CHRONOBERG struggle to encode diachronic shifts inmeaning, emphasizing the need for temporally aware training and evaluationpipelines, and positioning CHRONOBERG as a scalable resource for the study oflinguistic change and temporal generalization. Disclaimer: This paper includeslanguage and display of samples that could be offensive to readers. OpenAccess: Chronoberg is available publicly on HuggingFace at (https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at(https://github.com/paulsubarna/Chronoberg).</description>
      <author>example@mail.com (Niharika Hegde, Subarnaduti Paul, Lars Joel-Frey, Manuel Brack, Kristian Kersting, Martin Mundt, Patrick Schramowski)</author>
      <guid isPermaLink="false">2509.22360v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Galaxy Zoo: Cosmic Dawn -- morphological classifications for over 41,000 galaxies in the Euclid Deep Field North from the Hawaii Two-0 Cosmic Dawn survey</title>
      <link>http://arxiv.org/abs/2509.22311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages; 21 figures; 3 tables; submitted to MNRAS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过Galaxy Zoo公民科学家和深度学习模型Zoobot对超过41,000个星系进行了形态学分类，覆盖了6平方度Euclid Deep Field North区域，红移范围至约2.5。研究公开发布了45,000多个物体的分类数据，并发现了51个新的引力透镜系统。&lt;h4&gt;背景&lt;/h4&gt;研究基于Hawaii Twenty Square Degree (H20)调查，这是Cosmic Dawn调查的一部分，使用Hyper Suprime-Cam (HSC)获取超深多波段图像，深度达到m_HSC-i = 21.5。Galaxy Zoo项目通过公民科学家参与大规模天文数据集的检查。&lt;h4&gt;目的&lt;/h4&gt;对EDFN区域中的星系进行大规模形态学分类，创建一个公开的数据集，为后续观测和深度学习模型训练提供基础。&lt;h4&gt;方法&lt;/h4&gt;结合Galaxy Zoo公民科学家和深度学习基础模型Zoobot对物体进行分类，采用Zoobot在主动学习循环中提高模型性能和志愿者体验，使用超深多波段HSC成像数据进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;在EDFN区域发现了51个新的引力透镜系统；公开发布了超过45,000个物体的分类数据，包括超过41,000个星系，这些星系的中位红移为0.42±0.23，并提供了相关的图像裁剪。&lt;h4&gt;结论&lt;/h4&gt;该数据集为EDFN中物体的后续成像提供了宝贵机会，同时可作为训练深度学习模型的真值集，应用于地面观测站如Vera C. Rubin天文台。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了对超过41,000个星系的形态学分类，这些星系位于六个平方度的Euclid Deep Field North(EDFN)区域，红移至z_phot~2.5，来自作为更广泛Cosmic Dawn调查一部分的Hawaii Twenty Square Degree (H20)调查。Galaxy Zoo公民科学家通过众包挖掘星系外成像数据，在检查大型天文数据集方面发挥着关键作用。这一轮Galaxy Zoo: Cosmic Dawn (GZCD)项目，数万名志愿者和深度学习基础模型Zoobot共同对超深多波段Hyper Suprime-Cam (HSC)成像中的物体进行了分类，深度达到m_HSC-i = 21.5。在此，我们展示了这一轮的细节和一般分析，包括在主动学习周期中使用Zoobot以提高模型性能和志愿者体验，以及在EDFN中发现51个新的引力透镜。我们还宣布公开发布超过45,000个物体的分类数据，包括超过41,000个星系（中位z_phot为0.42±0.23）及其相关的图像裁剪。该数据集为EDFN中物体的后续成像提供了宝贵机会，同时也作为训练深度学习模型的真值集，应用于新近投入运营的Vera C. Rubin天文台等地面调查。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present morphological classifications of over 41,000 galaxies out to$z_{\rm phot}\sim2.5$ across six square degrees of the Euclid Deep Field North(EDFN) from the Hawaii Twenty Square Degree (H20) survey, a part of the widerCosmic Dawn survey. Galaxy Zoo citizen scientists play a crucial role in theexamination of large astronomical data sets through crowdsourced data mining ofextragalactic imaging. This iteration, Galaxy Zoo: Cosmic Dawn (GZCD), saw tensof thousands of volunteers and the deep learning foundation model Zoobotcollectively classify objects in ultra-deep multiband Hyper Suprime-Cam (HSC)imaging down to a depth of $m_{HSC-i} = 21.5$. Here, we present the details andgeneral analysis of this iteration, including the use of Zoobot in an activelearning cycle to improve both model performance and volunteer experience, aswell as the discovery of 51 new gravitational lenses in the EDFN. We alsoannounce the public data release of the classifications for over 45,000subjects, including more than 41,000 galaxies (median $z_{\rm phot}$ of$0.42\pm0.23$), along with their associated image cutouts. This data setprovides a valuable opportunity for follow-up imaging of objects in the EDFN aswell as acting as a truth set for training deep learning models for applicationto ground-based surveys like that of the newly operational Vera C. RubinObservatory.</description>
      <author>example@mail.com (James Pearson, Hugh Dickinson, Stephen Serjeant, Mike Walmsley, Lucy Fortson, Sandor Kruk, Karen L. Masters, Brooke D. Simmons, R. J. Smethurst, Chris Lintott, Lukas Zalesky, Conor McPartland, John R. Weaver, Sune Toft, Dave Sanders, Nima Chartab, Henry Joy McCracken, Bahram Mobasher, Istvan Szapudi, Noah East, Wynne Turner, Matthew Malkan, William J. Pearson, Tomotsugu Goto, Nagisa Oi)</author>
      <guid isPermaLink="false">2509.22311v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Aurora: Towards Universal Generative Multimodal Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2509.22295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Aurora，一个多模态时间序列基础模型，支持多模态输入和零样本推理，具有强大的跨域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;跨域泛化在时间序列预测中非常重要，因为相似的历史信息可能因领域特定特性导致不同的未来趋势。现有单模态模型缺乏对文本等模态中领域知识的利用，而端到端多模态模型不支持跨域场景的零样本推理。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效利用多模态信息并支持跨域零样本推理的时间序列基础模型，以解决跨域泛化问题。&lt;h4&gt;方法&lt;/h4&gt;Aurora通过标记化、编码和蒸馏提取多模态领域知识，使用模态引导多头自注意力机制将这些知识注入时间表示建模，并在解码阶段提出原型引导流匹配方法进行生成概率预测。&lt;h4&gt;主要发现&lt;/h4&gt;在TimeMMD、TSFM-Bench和ProbTS等基准上的实验表明，Aurora在单模态和多模态场景下都取得了持续的最先进性能。&lt;h4&gt;结论&lt;/h4&gt;Aurora作为多模态时间序列基础模型，能够自适应提取和关注关键领域知识，有效解决了跨域泛化问题，并在各种场景下表现出色。&lt;h4&gt;翻译&lt;/h4&gt;跨域泛化在时间序列预测中非常重要，因为相似的历史信息可能因领域特定特性导致不同的未来趋势。最近的工作主要集中在构建单模态时间序列基础模型和端到端多模态监督模型。由于领域特定知识通常包含在文本等模态中，前者缺乏对它们的明确利用，从而限制了性能。后者针对端到端场景设计，不支持跨域场景的零样本推理。在这项工作中，我们引入了Aurora，一个多模态时间序列基础模型，它支持多模态输入和零样本推理。在跨域多模态时间序列语料库上预训练后，Aurora能够自适应提取并关注文本或图像模态中包含的关键领域知识，从而具有强大的跨域泛化能力。通过标记化、编码和蒸馏，Aurora可以将多模态领域知识提取为指导，然后使用模态引导多头自注意力机制将它们注入到时间表示的建模中。在解码阶段，多模态表示被用来生成未来标记的条件和原型，为生成概率预测做出贡献。在TimeMMD、TSFM-Bench和ProbTS等公认基准上的全面实验证明了Aurora在单模态和多模态场景下都取得了持续的最先进性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain generalization is very important in Time Series Forecastingbecause similar historical information may lead to distinct future trends dueto the domain-specific characteristics. Recent works focus on building unimodaltime series foundation models and end-to-end multimodal supervised models.Since domain-specific knowledge is often contained in modalities like texts,the former lacks the explicit utilization of them, thus hindering theperformance. The latter is tailored for end-to-end scenarios and does notsupport zero-shot inference for cross-domain scenarios. In this work, weintroduce Aurora, a Multimodal Time Series Foundation Model, which supportsmultimodal inputs and zero-shot inference. Pretrained on Corss-domainMultimodal Time Series Corpus, Aurora can adaptively extract and focus on keydomain knowledge contained in corrsponding text or image modalities, thuspossessing strong Cross-domain generalization capability. Through tokenization,encoding, and distillation, Aurora can extract multimodal domain knowledge asguidance and then utilizes a Modality-Guided Multi-head Self-Attention toinject them into the modeling of temporal representations. In the decodingphase, the multimodal representations are used to generate the conditions andprototypes of future tokens, contributing to a novel Prototype-Guided FlowMatching for generative probabilistic forecasting. Comprehensive experiments onwell-recognized benchmarks, including TimeMMD, TSFM-Bench and ProbTS,demonstrate the consistent state-of-the-art performance of Aurora on bothunimodal and multimodal scenarios.</description>
      <author>example@mail.com (Xingjian Wu, Jianxin Jin, Wanghui Qiu, Peng Chen, Yang Shu, Bin Yang, Chenjuan Guo)</author>
      <guid isPermaLink="false">2509.22295v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Speak Your Mind: The Speech Continuation Task as a Probe of Voice-Based Model Bias</title>
      <link>http://arxiv.org/abs/2509.22061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 1 figure, Submitted to IEEE ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对语音延续任务中的偏见进行了系统性评估，研究了性别和发声类型对语音生成行为的影响，发现模型在延续过程中存在性别和声音质量方面的系统性偏见。&lt;h4&gt;背景&lt;/h4&gt;语音延续(SC)是生成连贯语音扩展的任务，需要同时保持语义上下文和说话人身份。由于SC限制在单个音频流中，它比对话提供了更直接的环境来探测语音基础模型中的偏见。&lt;h4&gt;目的&lt;/h4&gt;首次对语音延续中的偏见进行系统性评估，研究性别和发声类型（气息声、沙哑声、末尾沙哑声）如何影响延续行为。&lt;h4&gt;方法&lt;/h4&gt;评估三种语音模型：SpiritLM（基础和表达版）、VAE-GSLM和SpeechGPT，从说话人相似性、声音质量保持和基于文本的偏见指标等方面进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;1) 说话人相似性和连贯性仍是挑战；2) 文本评估显示显著的模型和性别交互作用；3) 当连贯性足够高时，性别效应在文本指标上显现；4) 女性提示的延续比男性提示更强烈地转向常态发声，揭示系统性声音质量偏见。&lt;h4&gt;结论&lt;/h4&gt;语音延续可作为探测语音基础模型中社会相关表征偏见的受控探针，随着延续质量的提高，它将成为越来越有价值的诊断工具。&lt;h4&gt;翻译&lt;/h4&gt;语音延续(SC)是生成连贯语音扩展的任务，同时保持语义上下文和说话人身份。由于SC被限制在单个音频流中，它比对话提供了更直接的环境来探测语音基础模型中的偏见。在这项工作中，我们首次对SC中的偏见进行了系统性评估，研究了性别和发声类型（气息声、沙哑声、末尾沙哑声）如何影响延续行为。我们评估了三种最近的模型：SpiritLM（基础和表达版）、VAE-GSLM和SpeechGPT，从说话人相似性、声音质量保持和基于文本的偏见指标等方面进行评估。结果表明，虽然说话人相似性和连贯性仍然是一个挑战，但文本评估显示了显著的模型和性别交互作用：当连贯性足够高时（对于VAE-GSLM），性别效应会在文本指标上显现，如能动性和句子极性。此外，与男性提示相比，女性提示的延续更强烈地转向常态发声，揭示了系统性的声音质量偏见。这些发现突显了语音延续作为探测语音基础模型中社会相关表征偏见的受控探针的作用，并表明随着延续质量的提高，它将成为信息量越来越大的诊断工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech Continuation (SC) is the task of generating a coherent extension of aspoken prompt while preserving both semantic context and speaker identity.Because SC is constrained to a single audio stream, it offers a more directsetting for probing biases in speech foundation models than dialogue does. Inthis work we present the first systematic evaluation of bias in SC,investigating how gender and phonation type (breathy, creaky, end-creak) affectcontinuation behaviour. We evaluate three recent models: SpiritLM (base andexpressive), VAE-GSLM, and SpeechGPT across speaker similarity, voice qualitypreservation, and text-based bias metrics. Results show that while both speakersimilarity and coherence remain a challenge, textual evaluations revealsignificant model and gender interactions: once coherence is sufficiently high(for VAE-GSLM), gender effects emerge on text-metrics such as agency andsentence polarity. In addition, continuations revert toward modal phonationmore strongly for female prompts than for male ones, revealing a systematicvoice-quality bias. These findings highlight SC as a controlled probe ofsocially relevant representational biases in speech foundation models, andsuggest that it will become an increasingly informative diagnostic ascontinuation quality improves.</description>
      <author>example@mail.com (Shree Harsha Bokkahalli Satish, Harm Lameris, Olivier Perrotin, Gustav Eje Henter, Éva Székely)</author>
      <guid isPermaLink="false">2509.22061v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models</title>
      <link>http://arxiv.org/abs/2509.22020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了WeatherPEFT，一种专门针对天气基础模型(WFMs)的参数高效微调(PEFT)框架，解决了现有PEFT方法无法应对天气下游任务独特挑战的问题。&lt;h4&gt;背景&lt;/h4&gt;机器学习进步使天气基础模型具备跨多种下游任务的泛化能力，但模型规模扩大带来的计算需求增加阻碍了实际部署。现有针对视觉或语言任务的PEFT方法无法处理天气下游任务中的变量异质性、分辨率多样性和时空覆盖变化等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门针对WFMs的PEFT框架，以克服现有方法在天气下游任务上的局限性，实现与全参数微调相当的性能但使用更少的可训练参数。&lt;h4&gt;方法&lt;/h4&gt;WeatherPEFT框架包含两个创新：1) 任务自适应动态提示(TADP)，通过内部和外部模式提取动态将编码器嵌入权重注入预训练骨干网络输入令牌，实现特定下游任务的上下文感知特征重校准；2) 随机Fisher引导自适应选择(SFAS)，利用Fisher信息识别并更新最关键任务参数，保留预训练知识同时引入随机性稳定选择过程。&lt;h4&gt;主要发现&lt;/h4&gt;在三个下游任务上验证了WeatherPEFT的有效性和效率；现有PEFT方法与全参数微调相比存在显著差距；WeatherPEFT使用更少的可训练参数实现了与全参数微调相当的性能。&lt;h4&gt;结论&lt;/h4&gt;WeatherPEFT为WFMs提供了一种有效的参数高效微调方法，解决了现有方法在天气特定任务上的局限性，同时保持了计算效率。&lt;h4&gt;翻译&lt;/h4&gt;尽管机器学习的最新进展使天气基础模型(WFMs)具备了在多样化下游任务中 substantial泛化能力，但其规模扩大带来的日益增长的计算需求 increasingly阻碍了实际部署。当前为视觉或语言任务设计的参数高效微调(PEFT)方法无法应对天气下游任务的独特挑战，如变量异质性、分辨率多样性和时空覆盖变化，导致应用于WFMs时表现不佳。为弥合这一差距，我们引入了WeatherPEFT，这是一种针对WFMs的新型PEFT框架，包含两个协同创新。首先，在前向传播过程中，任务自适应动态提示(TADP)通过内部和外部模式提取，动态地将编码器中的嵌入权重注入到预训练骨干网络的输入令牌中，实现对特定下游任务的上下文感知特征重校准。此外，在反向传播过程中，随机Fisher引导自适应选择(SFAS)不仅利用Fisher信息识别和更新最关键的任务参数，从而保留不变的预训练知识，还引入随机性以稳定选择过程。我们在三个下游任务上证明了WeatherPEFT的有效性和效率，在这些任务上现有PEFT方法与全参数微调相比存在显著差距，而WeatherPEFT使用更少的可训练参数实现了与全参数微调相当的性能。本工作的代码将发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advances in machine learning have equipped Weather FoundationModels (WFMs) with substantial generalization capabilities across diversedownstream tasks, the escalating computational requirements associated withtheir expanding scale increasingly hinder practical deployment. CurrentParameter-Efficient Fine-Tuning (PEFT) methods, designed for vision or languagetasks, fail to address the unique challenges of weather downstream tasks, suchas variable heterogeneity, resolution diversity, and spatiotemporal coveragevariations, leading to suboptimal performance when applied to WFMs. To bridgethis gap, we introduce WeatherPEFT, a novel PEFT framework for WFMsincorporating two synergistic innovations. First, during the forward pass,Task-Adaptive Dynamic Prompting (TADP) dynamically injects the embeddingweights within the encoder to the input tokens of the pre-trained backbone viainternal and external pattern extraction, enabling context-aware featurerecalibration for specific downstream tasks. Furthermore, duringbackpropagation, Stochastic Fisher-Guided Adaptive Selection (SFAS) not onlyleverages Fisher information to identify and update the most task-criticalparameters, thereby preserving invariant pre-trained knowledge, but alsointroduces randomness to stabilize the selection. We demonstrate theeffectiveness and efficiency of WeatherPEFT on three downstream tasks, whereexisting PEFT methods show significant gaps versus Full-Tuning, and WeatherPEFTachieves performance parity with Full-Tuning using fewer trainable parameters.The code of this work will be released.</description>
      <author>example@mail.com (Shilei Cao, Hehai Lin, Jiashun Cheng, Yang Liu, Guowen Li, Xuehe Wang, Juepeng Zheng, Haoyuan Liang, Meng Jin, Chengwei Qin, Hong Cheng, Haohuan Fu)</author>
      <guid isPermaLink="false">2509.22020v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation</title>
      <link>http://arxiv.org/abs/2509.21930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a poster in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出DynaNav动态视觉导航框架，通过自适应特征和层选择解决了现有基础模型计算开销高和缺乏可解释性的问题，显著提高了视觉导航效率和性能。&lt;h4&gt;背景&lt;/h4&gt;视觉导航对机器人和具身AI至关重要，但现有基础模型（特别是具有transformer解码器的模型）存在高计算开销和缺乏可解释性的问题，限制了它们在资源受限场景中的部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可解释的视觉导航框架，减少计算成本，使其适合在资源受限场景中部署，同时提高导航性能。&lt;h4&gt;方法&lt;/h4&gt;提出DynaNav框架，根据场景复杂度自适应选择特征和层；使用可训练的硬特征选择器进行稀疏操作提高效率和可解释性；将特征选择集成到早期退出机制中，使用贝叶斯优化确定最优退出阈值以减少计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;在基于真实世界的数据集和模拟环境中进行实验表明，与ViNT相比，DynaNav实现了2.26倍的FLOPs减少，42.3%的推理时间降低，32.8%的内存使用减少，并在四个公共数据集上提高了导航性能。&lt;h4&gt;结论&lt;/h4&gt;DynaNav框架有效解决了现有视觉导航模型的计算效率问题，同时提高了导航性能和系统可解释性，适合在资源受限的场景中部署。&lt;h4&gt;翻译&lt;/h4&gt;视觉导航对机器人和具身AI至关重要。然而，现有的基础模型，特别是那些具有transformer解码器的模型，存在高计算开销和缺乏可解释性的问题，限制了它们在资源受限场景中的部署。为此，我们提出了DynaNav，一种基于场景复杂度自适应特征和层选择的动态视觉导航框架。它采用可训练的硬特征选择器进行稀疏操作，提高效率和可解释性。此外，我们将特征选择集成到早期退出机制中，使用贝叶斯优化确定最优退出阈值以减少计算成本。在基于真实世界的数据集和模拟环境中的广泛实验证明了DynaNav的有效性。与ViNT相比，DynaNav实现了2.26倍的FLOPs减少，42.3%的推理时间降低和32.8%的内存使用减少，同时在四个公共数据集上提高了导航性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual navigation is essential for robotics and embodied AI. However,existing foundation models, particularly those with transformer decoders,suffer from high computational overhead and lack interpretability, limitingtheir deployment in resource-tight scenarios. To address this, we proposeDynaNav, a Dynamic Visual Navigation framework that adapts feature and layerselection based on scene complexity. It employs a trainable hard featureselector for sparse operations, enhancing efficiency and interpretability.Additionally, we integrate feature selection into an early-exit mechanism, withBayesian Optimization determining optimal exit thresholds to reducecomputational cost. Extensive experiments in real-world-based datasets andsimulated environments demonstrate the effectiveness of DynaNav. Compared toViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time,and 32.8% lower memory usage, while improving navigation performance acrossfour public datasets.</description>
      <author>example@mail.com (Jiahui Wang, Changhao Chen)</author>
      <guid isPermaLink="false">2509.21930v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation</title>
      <link>http://arxiv.org/abs/2509.21894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  *Corresponding authors: Min Zhu (min.zhu@scu.edu.cn) and Junlong  Cheng (jlcheng@scu.edu.cn)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的语言引导变化检测模型(LG-CD)，通过整合文本和视觉信息提高遥感变化检测的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;遥感变化检测(RSCD)通常通过分析多时相图像识别土地覆盖变化，但现有深度学习方法主要关注单模态视觉信息，忽略了多模态数据(如文本)提供的丰富语义信息。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法忽视多模态数据语义信息的局限性，提出一种能够利用自然语言提示引导模型关注感兴趣区域的变化检测方法。&lt;h4&gt;方法&lt;/h4&gt;LG-CD模型使用视觉基础模型(SAM2)作为特征提取器捕获多尺度特征，采用多层适配器微调模型，设计文本融合注意力模块(TFAM)对齐视觉和文本信息，并实现视觉-语义融合解码器(V-SFD)深度整合两种信息。&lt;h4&gt;主要发现&lt;/h4&gt;在LEVIR-CD、WHU-CD和SYSU-CD三个数据集上的实验表明，LG-CD始终优于最先进的变化检测方法，多模态信息融合为通用变化检测提供了新思路。&lt;h4&gt;结论&lt;/h4&gt;语言引导的变化检测模型能够有效整合文本和视觉信息，显著提高变化检测性能，为遥感变化检测领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;遥感变化检测通常通过分析多时相图像来识别土地覆盖或地表条件的变化。目前，大多数基于深度学习的方法主要关注学习单模态视觉信息，而忽略了多模态数据(如文本)提供的丰富语义信息。为解决这一局限，我们提出了一种新颖的语言引导变化检测模型(LG-CD)。该模型利用自然语言提示引导网络关注感兴趣区域，显著提高了变化检测的准确性和鲁棒性。具体而言，LG-CD使用视觉基础模型(SAM2)作为特征提取器，从高分辨率到低分辨率捕获双时相遥感图像的多尺度金字塔特征。随后，采用多层适配器微调模型用于下游任务，确保其在遥感变化检测中的有效性。此外，我们设计了文本融合注意力模块(TFAM)对齐视觉和文本信息，使模型能够使用文本提示关注目标变化区域。最后，实现了视觉-语义融合解码器(V-SFD)，通过交叉注意力机制深度整合视觉和语义信息，生成高精度的变化检测掩码。在三个数据集(LEVIR-CD、WHU-CD和SYSU-CD)上的实验表明，LG-CD始终优于最先进的变化检测方法。此外，我们的方法通过利用多模态信息实现通用变化检测提供了新见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote Sensing Change Detection (RSCD) typically identifies changes in landcover or surface conditions by analyzing multi-temporal images. Currently, mostdeep learning-based methods primarily focus on learning unimodal visualinformation, while neglecting the rich semantic information provided bymultimodal data such as text. To address this limitation, we propose a novelLanguage-Guided Change Detection model (LG-CD). This model leverages naturallanguage prompts to direct the network's attention to regions of interest,significantly improving the accuracy and robustness of change detection.Specifically, LG-CD utilizes a visual foundational model (SAM2) as a featureextractor to capture multi-scale pyramid features from high-resolution tolow-resolution across bi-temporal remote sensing images. Subsequently,multi-layer adapters are employed to fine-tune the model for downstream tasks,ensuring its effectiveness in remote sensing change detection. Additionally, wedesign a Text Fusion Attention Module (TFAM) to align visual and textualinformation, enabling the model to focus on target change regions using textprompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented,which deeply integrates visual and semantic information through across-attention mechanism to produce highly accurate change detection masks.Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstratethat LG-CD consistently outperforms state-of-the-art change detection methods.Furthermore, our approach provides new insights into achieving generalizedchange detection by leveraging multimodal information.</description>
      <author>example@mail.com (Yixiao Liu, Yizhou Yang, Jinwen Li, Jun Tao, Ruoyu Li, Xiangkun Wang, Min Zhu, Junlong Cheng)</author>
      <guid isPermaLink="false">2509.21894v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>QoNext: Towards Next-generation QoE for Foundation Models</title>
      <link>http://arxiv.org/abs/2509.21889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了QoNext框架，这是首个将网络和多媒体领域的体验质量(QoE)原则应用于基础模型评估的框架，旨在解决现有评估方法无法全面捕捉用户体验的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的基础模型评估方法（包括最近以人为中心的方法）仅关注输出正确性，忽视了用户满意度来自于响应质量和交互之间的相互作用，因此无法全面评估用户体验。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉和评估用户体验的框架，为基础模型提供更全面、更实用的评估方法，并为产品化服务提供优化指导。&lt;h4&gt;方法&lt;/h4&gt;QoNext框架确定塑造用户体验的体验因素，将这些因素纳入受控实验中，收集不同配置下的人类评分，构建面向QoE的数据库，并训练预测模型从可测量的系统参数估计感知的用户体验。&lt;h4&gt;主要发现&lt;/h4&gt;QoNext框架能够进行主动和细粒度的评估，为优化基础模型的产品化服务提供可行的指导。&lt;h4&gt;结论&lt;/h4&gt;QoNext框架解决了现有基础模型评估方法无法全面捕捉用户体验的问题，通过将QoE原则应用于基础模型评估，提供了更全面、更实用的评估方法。&lt;h4&gt;翻译&lt;/h4&gt;现有的基础模型评估方法，包括最近以人为中心的方法，未能捕捉到真正重要的内容：用户在交互过程中的体验。当前方法将评估仅视为输出正确性的问题，忽视了用户满意度来自于响应质量和交互之间的相互作用，这限制了它们解释用户体验潜在机制的能力。为了解决这一差距，我们引入了QoNext，这是第一个将网络和多媒体领域的体验质量(QoE)原则应用于基础模型评估的框架。QoNext确定塑造用户体验的体验因素，并将它们纳入受控实验中，在这些实验中收集不同配置下的人类评分。从这些研究中，我们构建了一个面向QoE的数据库，并训练预测模型来从可测量的系统参数中估计感知的用户体验。我们的结果表明，QoNext不仅能够进行主动和细粒度的评估，还为优化基础模型的产品化服务提供了可行的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing evaluations of foundation models, including recent human-centricapproaches, fail to capture what truly matters: user's experience duringinteraction. Current methods treat evaluation as a matter of output correctnessalone, overlooking that user satisfaction emerges from the interplay betweenresponse quality and interaction, which limits their ability to account for themechanisms underlying user experience. To address this gap, we introduceQoNext, the first framework that adapts Quality of Experience (QoE) principlesfrom networking and multimedia to the assessment of foundation models. QoNextidentifies experiential factors that shape user experience and incorporatesthem into controlled experiments, where human ratings are collected undervaried configurations. From these studies we construct a QoE-oriented databaseand train predictive models that estimate perceived user experience frommeasurable system parameters. Our results demonstrate that QoNext not onlyenables proactive and fine-grained evaluation but also provides actionableguidance for productized services of optimizing foundation models in practice.</description>
      <author>example@mail.com (Yijin Guo, Ye Shen, Farong Wen, Junying Wang, Zicheng Zhang, Qi Jia, Guangtao Zhai)</author>
      <guid isPermaLink="false">2509.21889v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>MolSpectLLM: A Molecular Foundation Model Bridging Spectroscopy, Molecule Elucidation, and 3D Structure Generation</title>
      <link>http://arxiv.org/abs/2509.21861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了MolSpectLLM，一种统一实验光谱与分子3D结构的分子基础模型，在分子性质预测和分子设计方面表现出色，特别是在光谱相关任务上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;分子基础模型在分子性质预测和从头分子设计方面取得了显著进展，但大多数现有方法仅依赖于SMILES表示，忽略了实验光谱和3D结构信息这两个在真实场景中捕捉分子行为不可或缺的来源，限制了它们在立体化学、空间构象和实验验证等关键任务中的有效性。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法的局限性，研究者提出了一种统一实验光谱与分子3D结构的分子基础模型，以提高在需要考虑立体化学、空间构象和实验验证的任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;研究者提出了MolSpectLLM，一个基于Qwen2.5-7B预训练的分子基础模型，该模型明确建模分子光谱，将实验光谱与分子3D结构相结合。&lt;h4&gt;主要发现&lt;/h4&gt;MolSpectLLM在光谱相关任务上实现了最先进的性能，在NMR、IR和MS基准测试中平均准确率达到0.53。在光谱分析任务上，获得了15.5%的序列准确率和41.7%的token准确率，显著优于大型通用LLM。此外，还能直接从SMILES或光谱输入生成准确的3D分子结构。&lt;h4&gt;结论&lt;/h4&gt;MolSpectLLM通过结合实验光谱和分子3D结构信息，克服了现有分子基础模型的局限性，在光谱相关任务和分子设计任务上表现出色，为药物发现和其他需要考虑分子立体化学和空间构象的应用提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;分子基础模型的最新进展在分子性质预测和从头分子设计方面显示出卓越的性能，在药物发现和反应预测等领域有很好的应用前景。然而，大多数现有方法仅依赖于SMILES表示，忽略了实验光谱和3D结构信息这两个在真实场景中捕捉分子行为不可或缺的来源。这种局限性降低了它们在立体化学、空间构象和实验验证等关键任务中的有效性。为了克服这些挑战，我们提出了MolSpectLLM，这是一个基于Qwen2.5-7B预训练的分子基础模型，将实验光谱与分子3D结构统一起来。通过明确建模分子光谱，MolSpectLLM在光谱相关任务上实现了最先进的性能，在NMR、IR和MS基准测试中平均准确率为0.53。MolSpectLLM在光谱分析任务上也表现出色，在Spectra-to-SMILES上获得了15.5%的序列准确率和41.7%的token准确率，显著优于大型通用LLM。更重要的是，MolSpectLLM不仅在分子阐明任务上表现出色，还能直接从SMILES或光谱输入生成准确的3D分子结构，连接了光谱分析、分子阐明和分子设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in molecular foundation models have shown impressiveperformance in molecular property prediction and de novo molecular design, withpromising applications in areas such as drug discovery and reaction prediction.Nevertheless, most existing approaches rely exclusively on SMILESrepresentations and overlook both experimental spectra and 3D structuralinformation-two indispensable sources for capturing molecular behavior inreal-world scenarios. This limitation reduces their effectiveness in taskswhere stereochemistry, spatial conformation, and experimental validation arecritical. To overcome these challenges, we propose MolSpectLLM, a molecularfoundation model pretrained on Qwen2.5-7B that unifies experimentalspectroscopy with molecular 3D structure. By explicitly modeling molecularspectra, MolSpectLLM achieves state-of-the-art performance on spectrum-relatedtasks, with an average accuracy of 0.53 across NMR, IR, and MS benchmarks.MolSpectLLM also shows strong performance on the spectra analysis task,obtaining 15.5% sequence accuracy and 41.7% token accuracy onSpectra-to-SMILES, substantially outperforming large general-purpose LLMs. Moreimportantly, MolSpectLLM not only achieves strong performance on molecularelucidation tasks, but also generates accurate 3D molecular structures directlyfrom SMILES or spectral inputs, bridging spectral analysis, molecularelucidation, and molecular design.</description>
      <author>example@mail.com (Shuaike Shen, Jiaqing Xie, Zhuo Yang, Antong Zhang, Shuzhou Sun, Ben Gao, Tianfan Fu, Biqing Qi, Yuqiang Li)</author>
      <guid isPermaLink="false">2509.21861v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>ChaosNexus: A Foundation Model for Universal Chaotic System Forecasting with Multi-scale Representations</title>
      <link>http://arxiv.org/abs/2509.21802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ChaosNexus的基础模型，用于解决混沌系统预测中的泛化问题，通过在多样化混沌动力学数据上预训练，实现了强大的零样本或少样本预测能力。&lt;h4&gt;背景&lt;/h4&gt;准确预测混沌系统（如天气预测和流体动力学中普遍存在的系统）仍面临重大挑战，这些系统对初始条件敏感且观测数据稀缺，传统模型因针对特定系统训练而缺乏泛化能力。&lt;h4&gt;目的&lt;/h4&gt;克服传统模型在数据有限和面对新系统时的泛化障碍，开发能够进行零样本或少样本预测的基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出ChaosNexus基础模型，在多样化混沌动力学语料库上预训练，采用名为ScaleFormer的新型多尺度架构并加入专家混合层，以捕捉通用模式和系统特定行为。&lt;h4&gt;主要发现&lt;/h4&gt;ChaosNexus在合成和真实世界基准中展示最先进的零样本泛化能力，在9,000多个合成混沌系统测试中将长期吸引子统计保真度提高40%以上；在5天全球天气预报中实现低于1度的竞争性零样本平均误差；跨系统泛化源于训练系统多样性而非数据量。&lt;h4&gt;结论&lt;/h4&gt;ChaosNexus通过多样化混沌系统预训练克服了传统模型的泛化限制，为混沌系统预测提供了新的解决方案，并确立了科学基础模型的指导原则。&lt;h4&gt;翻译&lt;/h4&gt;准确预测混沌系统（在天气预测和流体动力学等领域普遍存在）仍然是一个重大的科学挑战。这些系统对初始条件固有的敏感性，加上观测数据的稀缺，严重限制了传统建模方法。由于这些模型通常针对特定系统进行训练，它们缺乏现实应用所需的泛化能力，这些应用需要在新的或数据有限的情况下进行强大的零样本或少样本预测。为了克服这一泛化障碍，我们提出了ChaosNexus，一种在多样化混沌动力学语料库上预训练的基础模型。ChaosNexus采用了一种名为ScaleFormer的新型多尺度架构，并加入了专家混合层，以捕捉通用模式和系统特定行为。该模型在合成和真实世界的基准测试中展示了最先进的零样本泛化能力。在包含9,000多个合成混沌系统的大规模测试平台上，与最先进的基线相比，它将长期吸引子统计的保真度提高了40%以上。这种强大的性能延伸到现实世界的应用中，具有出色的数据效率。例如，在5天全球天气预报中，ChaosNexus实现了低于1度的竞争性零样本平均误差，通过少样本微调进一步改善。此外，关于ChaosNexus扩展行为的实验为科学基础模型提供了指导原则：跨系统泛化源于训练系统的多样性，而非单纯的数据量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately forecasting chaotic systems, prevalent in domains such as weatherprediction and fluid dynamics, remains a significant scientific challenge. Theinherent sensitivity of these systems to initial conditions, coupled with ascarcity of observational data, severely constrains traditional modelingapproaches. Since these models are typically trained for a specific system,they lack the generalization capacity necessary for real-world applications,which demand robust zero-shot or few-shot forecasting on novel or data-limitedscenarios. To overcome this generalization barrier, we propose ChaosNexus, afoundation model pre-trained on a diverse corpus of chaotic dynamics.ChaosNexus employs a novel multi-scale architecture named ScaleFormer augmentedwith Mixture-of-Experts layers, to capture both universal patterns andsystem-specific behaviors. The model demonstrates state-of-the-art zero-shotgeneralization across both synthetic and real-world benchmarks. On alarge-scale testbed comprising over 9,000 synthetic chaotic systems, itimproves the fidelity of long-term attractor statistics by more than 40%compared to the leading baseline. This robust performance extends to real-worldapplications with exceptional data efficiency. For instance, in 5-day globalweather forecasting, ChaosNexus achieves a competitive zero-shot mean errorbelow 1 degree, a result that further improves with few-shot fine-tuning.Moreover, experiments on the scaling behavior of ChaosNexus provide a guidingprinciple for scientific foundation models: cross-system generalization stemsfrom the diversity of training systems, rather than sheer data volume.</description>
      <author>example@mail.com (Chang Liu, Bohao Zhao, Jingtao Ding, Yong Li)</author>
      <guid isPermaLink="false">2509.21802v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation</title>
      <link>http://arxiv.org/abs/2509.21777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Generative Recommender, Recommendation System, Information Retrieval&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SynerGen是一种新的生成式推荐模型，通过单一的生成式主干网络同时支持个性化搜索和推荐，解决了现有模型在统一这两方面功能时的性能权衡问题。&lt;h4&gt;背景&lt;/h4&gt;大规模推荐系统中的主流'检索后排序'管道存在架构分离导致的校准问题和工程开销大的问题。现有生成式序列模型通常只解决个性化搜索或无查询推荐中的一个方面，难以同时兼顾两者。&lt;h4&gt;目的&lt;/h4&gt;引入SynerGen模型，提供一个单一的生成式主干网络，同时支持个性化搜索和推荐，并在检索和排序任务上都能表现出色。&lt;h4&gt;方法&lt;/h4&gt;使用基于行为的序列进行训练，采用仅解码器的Transformer架构，通过InfoNCE进行联合优化用于检索，使用混合点对点损失函数用于排序，并提出了一种新型的时间感知旋转位置编码来整合时间信息到注意力机制中。&lt;h4&gt;主要发现&lt;/h4&gt;SynerGen在广泛采用的推荐和搜索基准上相比强大的生成式推荐基线和联合搜索与推荐基线取得了显著改进，搜索的语义信号可以改善推荐，反之亦然。&lt;h4&gt;结论&lt;/h4&gt;单一生成式基础模型在工业规模统一信息访问方面是可行的。&lt;h4&gt;翻译&lt;/h4&gt;大规模推荐系统中的主流'检索后排序'管道由于其架构分离和不同的优化目标而存在校准不准和工程开销大的问题。虽然最近的生成式序列模型通过自回归生成排序项目在统一检索和排序方面显示出潜力，但现有解决方案通常只处理个性化搜索或无查询推荐，在尝试统一两者时往往表现出性能权衡。我们引入了SynerGen，这是一种新颖的生成式推荐模型，通过为个性化搜索和推荐提供单一的生成式主干来弥合这一关键差距，同时在检索和排序任务上表现出色。我们的仅解码器Transformer基于行为序列进行训练，利用InfoNCE进行检索的联合优化，以及用于排序的混合点对点损失函数，允许搜索的语义信号改善推荐，反之亦然。我们还提出了一种新颖的时间感知旋转位置编码，将时间信息有效地整合到注意力机制中。与强大的生成式推荐基线和联合搜索与推荐基线相比，SynerGen在广泛采用的推荐和搜索基准上取得了显著改进。这项工作证明了单一生成式基础模型在工业规模统一信息访问方面的可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The dominant retrieve-then-rank pipeline in large-scale recommender systemssuffers from mis-calibration and engineering overhead due to its architecturalsplit and differing optimization objectives. While recent generative sequencemodels have shown promise in unifying retrieval and ranking byauto-regressively generating ranked items, existing solutions typically addresseither personalized search or query-free recommendation, often exhibitingperformance trade-offs when attempting to unify both. We introduce\textit{SynerGen}, a novel generative recommender model that bridges thiscritical gap by providing a single generative backbone for both personalizedsearch and recommendation, while simultaneously excelling at retrieval andranking tasks. Trained on behavioral sequences, our decoder-only Transformerleverages joint optimization with InfoNCE for retrieval and a hybridpointwise-pairwise loss for ranking, allowing semantic signals from search toimprove recommendation and vice versa. We also propose a novel time-awarerotary positional embedding to effectively incorporate time information intothe attention mechanism. \textit{SynerGen} achieves significant improvements onwidely adopted recommendation and search benchmarks compared to stronggenerative recommender and joint search and recommendation baselines. This workdemonstrates the viability of a single generative foundation model forindustrial-scale unified information access.</description>
      <author>example@mail.com (Vianne R. Gao, Chen Xue, Marc Versage, Xie Zhou, Zhongruo Wang, Chao Li, Yeon Seonwoo, Nan Chen, Zhen Ge, Gourab Kundu, Weiqi Zhang, Tian Wang, Qingjun Cui, Trishul Chilimbi)</author>
      <guid isPermaLink="false">2509.21777v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models</title>
      <link>http://arxiv.org/abs/2509.21760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了UniVid框架，通过微调视频扩散变换器处理多种视觉任务，无需任务特定修改。研究探索了预训练视频生成模型适应多样化视觉任务的能力，发现其具有良好的跨模态推理和跨源任务泛化能力，并能通过简单反转视觉句子顺序切换理解和生成任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型成功将多样化语言任务统一在单一生成框架内。受此启发，大型视觉模型（LVM）将此范式扩展到视觉领域，通过组织任务为顺序视觉句子，使用视觉提示作为上下文。然而，此类方法需要跨模态和来源的任务特定预训练，成本高昂且限制扩展性。&lt;h4&gt;目的&lt;/h4&gt;探索预训练视频生成模型能否适应多样化的图像和视频任务，提供一个更统一且可扩展的替代方案，解决现有方法需要高昂成本预训练的问题。&lt;h4&gt;方法&lt;/h4&gt;提出UniVid框架，微调视频扩散变换器处理各种视觉任务，无需任务特定修改。任务表示为视觉句子，上下文序列定义任务和期望输出模态。从两个角度评估泛化能力：(1) 使用图像和视频组成的上下文进行跨模态推理；(2) 从自然数据到注释数据的跨源任务，无需多源预训练。&lt;h4&gt;主要发现&lt;/h4&gt;尽管仅使用自然视频数据训练，UniVid在跨模态推理和跨源任务设置中均表现良好。通过简单反转视觉句子顺序，理解和生成任务可以轻松切换。&lt;h4&gt;结论&lt;/h4&gt;预训练视频生成模型可作为视觉建模的可扩展和统一基础，具有处理多样化视觉任务的潜力。研究代码将在https://github.com/CUC-MIPG/UniVid发布。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在大量语料库上训练成功，将多样化的语言任务统一在一个生成框架内。受此启发，近期工作如大型视觉模型（LVM）将这一范式扩展到视觉领域，通过将任务组织成顺序的视觉句子，其中视觉提示作为指导输出的上下文。然而，这种建模需要跨模态和来源的任务特定预训练，成本高昂且限制了扩展到未见任务的能力。鉴于预训练的视频生成模型固有地捕获时间序列依赖关系，我们探索了一个更统一且可扩展的替代方案：预训练的视频生成模型能否适应多样的图像和视频任务？为回答这个问题，我们提出了UniVid，这是一个微调视频扩散变换器以处理各种视觉任务的框架，无需针对特定任务进行修改。任务表示为视觉句子，其中上下文序列定义了任务和期望的输出模态。我们从两个角度评估UniVid的泛化能力：(1) 使用由图像和视频组成的上下文进行跨模态推理，扩展了LVM的单模态设置；(2) 从自然数据到注释数据的跨源任务，无需多源预训练。尽管仅使用自然视频数据训练，UniVid在这两种设置中都表现良好。值得注意的是，通过简单反转视觉句子顺序，理解和生成任务可以轻松切换。这些发现突显了预训练视频生成模型作为视觉建模的可扩展和统一基础的潜力。我们的代码将在https://github.com/CUC-MIPG/UniVid发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models, trained on extensive corpora, successfully unifydiverse linguistic tasks within a single generative framework. Inspired bythis, recent works like Large Vision Model (LVM) extend this paradigm to visionby organizing tasks into sequential visual sentences, where visual promptsserve as the context to guide outputs. However, such modeling requirestask-specific pre-training across modalities and sources, which is costly andlimits scalability to unseen tasks. Given that pre-trained video generationmodels inherently capture temporal sequence dependencies, we explore a moreunified and scalable alternative: can a pre-trained video generation modeladapt to diverse image and video tasks? To answer this, we propose UniVid, aframework that fine-tunes a video diffusion transformer to handle variousvision tasks without task-specific modifications. Tasks are represented asvisual sentences, where the context sequence defines both the task and theexpected output modality. We evaluate the generalization of UniVid from twoperspectives: (1) cross-modal inference with contexts composed of both imagesand videos, extending beyond LVM's uni-modal setting; (2) cross-source tasksfrom natural to annotated data, without multi-source pre-training. Despitebeing trained solely on natural video data, UniVid generalizes well in bothsettings. Notably, understanding and generation tasks can easily switch bysimply reversing the visual sentence order in this paradigm. These findingshighlight the potential of pre-trained video generation models to serve as ascalable and unified foundation for vision modeling. Our code will be releasedat https://github.com/CUC-MIPG/UniVid.</description>
      <author>example@mail.com (Lan Chen, Yuchao Gu, Qi Mao)</author>
      <guid isPermaLink="false">2509.21760v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum Transcription</title>
      <link>http://arxiv.org/abs/2509.21739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Noise-to-Notes (N2N)框架，将自动鼓转录重新定义为条件生成任务，利用扩散建模将音频条件的高斯噪声转换为带有速度的鼓事件，并整合音乐基础模型特征提高性能。&lt;h4&gt;背景&lt;/h4&gt;传统的自动鼓转录(ADT)被表述为判别性任务，用于从音频频谱图中预测鼓事件。&lt;h4&gt;目的&lt;/h4&gt;重新定义ADT为条件生成任务，开发一种新的框架来生成鼓事件及其相关速度。&lt;h4&gt;方法&lt;/h4&gt;利用扩散建模技术将基于音频条件的高斯噪声转换为鼓事件；引入退火伪Huber损失处理二元起始点和连续速度值的生成挑战；整合从音乐基础模型(MFMs)中提取的特征以增强低层频谱图特征和提高鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;生成式扩散方法提供了灵活的速度-准确性权衡和强大的修复能力；包含MFM特征显著提高了系统对域外鼓音频的鲁棒性；N2N在多个ADT基准测试上建立了新的最先进性能。&lt;h4&gt;结论&lt;/h4&gt;通过重新定义ADT为条件生成任务并利用扩散建模，结合音乐基础模型特征，N2N框架实现了更优的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;自动鼓转录(ADT)传统上被表述为从音频频谱图中预测鼓事件的判别性任务。在本工作中，我们将ADT重新定义为条件生成任务，并引入Noise-to-Notes (N2N)框架，利用扩散建模将基于音频条件的高斯噪声转换为带有相关速度的鼓事件。这种生成式扩散方法提供了明显的优势，包括灵活的速度-准确性权衡和强大的修复能力。然而，二元起始点和连续速度值的生成对扩散模型构成挑战，为此我们引入退火伪Huber损失以实现有效的联合优化。最后，为了增强低层频谱图特征，我们提议整合从音乐基础模型(MFMs)中提取的特征，这些特征捕获高层语义信息并增强对域外鼓音频的鲁棒性。实验结果表明，包含MFM特征显著提高了鲁棒性，N2N在多个ADT基准测试上建立了新的最先进性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic drum transcription (ADT) is traditionally formulated as adiscriminative task to predict drum events from audio spectrograms. In thiswork, we redefine ADT as a conditional generative task and introduceNoise-to-Notes (N2N), a framework leveraging diffusion modeling to transformaudio-conditioned Gaussian noise into drum events with associated velocities.This generative diffusion approach offers distinct advantages, including aflexible speed-accuracy trade-off and strong inpainting capabilities. However,the generation of binary onset and continuous velocity values presents achallenge for diffusion models, and to overcome this, we introduce an AnnealedPseudo-Huber loss to facilitate effective joint optimization. Finally, toaugment low-level spectrogram features, we propose incorporating featuresextracted from music foundation models (MFMs), which capture high-levelsemantic information and enhance robustness to out-of-domain drum audio.Experimental results demonstrate that including MFM features significantlyimproves robustness and N2N establishes a new state-of-the-art performanceacross multiple ADT benchmarks.</description>
      <author>example@mail.com (Michael Yeung, Keisuke Toyama, Toya Teramoto, Shusuke Takahashi, Tamaki Kojima)</author>
      <guid isPermaLink="false">2509.21739v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Frustratingly Easy Zero-Day Audio DeepFake Detection via Retrieval Augmentation and Profile Matching</title>
      <link>http://arxiv.org/abs/2509.21728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于知识表示、检索增强和语音档案匹配的零日音频深度伪造检测的无训练框架，实现了与微调模型相当的检测性能，无需额外训练。&lt;h4&gt;背景&lt;/h4&gt;现代音频深度伪造检测器使用基础模型和大型训练数据集已取得有前景的检测性能，但难以应对零日攻击（由新合成方法生成的音频样本）。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需微调的零日音频深度伪造检测方法，以应对需要快速响应的场景。&lt;h4&gt;方法&lt;/h4&gt;提出基于知识表示、检索增强和语音档案匹配的无训练框架，并设计了简单的知识检索和集成方法。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在DeepFake-Eval-2024上实现了与微调模型相当的性能，无需任何额外的模型级训练；检索池大小和语音档案属性与系统功效相关。&lt;h4&gt;结论&lt;/h4&gt;基于知识表示的无训练框架可以有效应对零日音频深度伪造攻击，避免了传统方法需要微调的局限性。&lt;h4&gt;翻译&lt;/h4&gt;使用基础模型和大型训练数据集的现代音频深度伪造检测器已取得有前景的检测性能。然而，它们难以应对零日攻击，即由新的合成方法生成的音频样本，这些方法在训练数据中并未出现过。针对此类攻击的传统方法需要对检测器进行微调，这在需要快速响应的情况下可能会成为问题。本研究提出了一种基于知识表示、检索增强和语音档案匹配的零日音频深度伪造检测的无训练框架。基于该框架，我们提出了简单而有效的知识检索和集成方法，这些方法在DeepFake-Eval-2024上实现了与微调模型相当的性能，而无需任何额外的模型级训练。我们还对检索池大小和语音档案属性进行了消融研究，验证了它们与系统功效的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern audio deepfake detectors using foundation models and large trainingdatasets have achieved promising detection performance. However, they strugglewith zero-day attacks, where the audio samples are generated by novel synthesismethods that models have not seen from reigning training data. Conventionalapproaches against such attacks require fine-tuning the detectors, which can beproblematic when prompt response is required. This study introduces atraining-free framework for zero-day audio deepfake detection based onknowledge representations, retrieval augmentation, and voice profile matching.Based on the framework, we propose simple yet effective knowledge retrieval andensemble methods that achieve performance comparable to fine-tuned models onDeepFake-Eval-2024, without any additional model-wise training. We also conductablation studies on retrieval pool size and voice profile attributes,validating their relevance to the system efficacy.</description>
      <author>example@mail.com (Xuechen Liu, Xin Wang, Junichi Yamagishi)</author>
      <guid isPermaLink="false">2509.21728v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>On the Status of Foundation Models for SAR Imagery</title>
      <link>http://arxiv.org/abs/2509.21722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基础AI/ML模型在合成孔径雷达（SAR）目标识别任务中的可行性，通过自监督学习技术对现有模型进行微调，成功提升了SAR目标识别性能。&lt;h4&gt;背景&lt;/h4&gt;自然图像领域AI/ML技术取得巨大进展，前沿实验室使用大规模数据集训练大型模型，这些自监督学习模型能够适应下游任务，对分布偏移具有鲁棒性，特征可迁移性强。&lt;h4&gt;目的&lt;/h4&gt;将自然图像领域的基础模型技术应用于SAR领域，提高SAR目标识别的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;测试当前最强大的视觉基础模型（DINOv2、DINOv3和PE-Core），观察其在SAR特征提取上的不足；使用SAR数据对公开SSL模型进行自监督微调，训练AFRL-DINOv2模型；分析不同骨干网络和下游任务适应方案的性能权衡；评估模型克服下游环境挑战的能力。&lt;h4&gt;主要发现&lt;/h4&gt;现有视觉基础模型在提取SAR语义目标特征方面存在局限；使用SAR数据对SSL模型进行自监督微调是可行路径；AFRL-DINOv2模型为SAR基础模型设定了新的最先进水平，显著优于当前最佳SAR领域模型SARATR-X。&lt;h4&gt;结论&lt;/h4&gt;尽管取得积极结果，SAR基础模型发展仍有很大空间；该研究将为未来SAR基础模型构建者提供参考和启发。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了基础AI/ML模型在合成孔径雷达（SAR）目标识别任务中的可行性。我们受到更广泛社区，特别是在自然图像领域取得的巨大进展的启发，前沿实验室使用前所未有的计算预算在网络规模数据集上训练大型模型。很明显，这些通常使用自监督学习（SSL）训练的模型将改变我们为目标识别任务开发AI/ML解决方案的方式 - 它们可以用非常有限的标记数据进行下游适应，对多种形式的分布偏移更具鲁棒性，并且它们的特征开箱即可高度迁移。由于这些原因以及更多，我们受到启发将这项技术应用到SAR领域。在我们的实验中，我们首先使用当今最强大的视觉基础模型进行测试，包括DINOv2、DINOv3和PE-Core，并观察到它们在即用状态下提取语义上有意义的区分性SAR目标特征方面的不足。然后，我们展示了使用SAR数据对公开可用的SSL模型进行自监督微调是一条可行的前进路径，通过训练几个AFRL-DINOv2模型，为SAR基础模型设定了新的最先进水平，显著优于当今最好的SAR领域模型SARATR-X。我们的实验进一步分析了使用不同骨干网络与不同下游任务适应方案的性能权衡，并监控每个模型克服下游环境挑战的能力（例如，扩展操作条件和少量标记数据）。我们希望这项工作将为未来的SAR基础模型构建者提供信息和启发，因为尽管我们的结果是积极的，但我们仍然有很长的路要走。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work we investigate the viability of foundational AI/ML models forSynthetic Aperture Radar (SAR) object recognition tasks. We are inspired by thetremendous progress being made in the wider community, particularly in thenatural image domain where frontier labs are training huge models on web-scaledatasets with unprecedented computing budgets. It has become clear that thesemodels, often trained with Self-Supervised Learning (SSL), will transform howwe develop AI/ML solutions for object recognition tasks - they can be adapteddownstream with very limited labeled data, they are more robust to many formsof distribution shift, and their features are highly transferableout-of-the-box. For these reasons and more, we are motivated to apply thistechnology to the SAR domain. In our experiments we first run tests withtoday's most powerful visual foundational models, including DINOv2, DINOv3 andPE-Core and observe their shortcomings at extracting semantically-interestingdiscriminative SAR target features when used off-the-shelf. We then show thatSelf-Supervised finetuning of publicly available SSL models with SAR data is aviable path forward by training several AFRL-DINOv2s and setting a newstate-of-the-art for SAR foundation models, significantly outperforming today'sbest SAR-domain model SARATR-X. Our experiments further analyze the performancetrade-off of using different backbones with different downstreamtask-adaptation recipes, and we monitor each model's ability to overcomechallenges within the downstream environments (e.g., extended operatingconditions and low amounts of labeled data). We hope this work will inform andinspire future SAR foundation model builders, because despite our positiveresults, we still have a long way to go.</description>
      <author>example@mail.com (Nathan Inkawhich)</author>
      <guid isPermaLink="false">2509.21722v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Wav2Arrest 2.0: Long-Horizon Cardiac Arrest Prediction with Time-to-Event Modeling, Identity-Invariance, and Pseudo-Lab Alignment</title>
      <link>http://arxiv.org/abs/2509.21695v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to BPSC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出三种改进方法，通过使用最少辅助信息增强基于PPG的心脏骤停预测系统，独立将24小时时间平均AUC从0.74提升至0.78-0.80范围，主要改善长时预测性能。&lt;h4&gt;背景&lt;/h4&gt;高频生理波形模态能提供对患者状态的深入、实时洞察。基于光电容积描记法(PPG)的生理基础模型(如PPG-GPT)已被证明可以预测包括心脏骤停在内的关键事件，但其强大表示能力在下游数据/标签稀缺时未得到充分利用。&lt;h4&gt;目的&lt;/h4&gt;通过使用最少的辅助信息来改进仅基于PPG的心脏骤停预测系统，提高预测性能，特别是在长时预测方面。&lt;h4&gt;方法&lt;/h4&gt;1) 事件时间建模，通过简单回归到事件发生时间或细粒度离散生存建模；2) 使模型学习CA聚焦特征，通过训练大规模匿名生物识别模型(p-vector)并对抗性使用来解耦可能导致过拟合的线索；3) 回归预训练辅助估计器网络生成的伪标签值，解决真实血液实验室测量数据稀少的问题。&lt;h4&gt;主要发现&lt;/h4&gt;提出的改进方法可独立将24小时时间平均AUC从0.74提升至0.78-0.80范围，主要改善长时预测性能，在事件附近最小程度降低性能，推动早期预警系统研究。&lt;h4&gt;结论&lt;/h4&gt;采用多任务公式诊断出竞争损失之间的高梯度冲突率，并通过PCGrad优化技术缓解了这一问题，有效提高了心脏骤停预测系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;高频生理波形模态能提供对患者状态的深入、实时洞察。最近，基于光电容积描记法(PPG)的生理基础模型(如PPG-GPT)已被证明可以预测包括心脏骤停在内的关键事件。然而，它们的强大表示能力仍未得到充分利用，特别是当下游数据/标签稀缺时。我们提出三种正交改进，通过使用最少的辅助信息来改进仅基于PPG的CA系统。首先，我们建议使用事件时间建模，通过简单回归到事件发生时间或追求细粒度离散生存建模。其次，我们鼓励模型学习CA聚焦特征，通过使它们对患者身份不变性。这是通过首先训练最大规模的匿名生物识别识别模型(称为p-vector)实现的，然后对抗性地使用它来解耦可能导致通过记忆过拟合的线索。第三，我们提出对预训练辅助估计器网络生成的伪标签值进行回归。这至关重要，因为真正的血液实验室测量(如乳酸、钠、肌钙蛋白和钾)收集稀少。通过零样本预测，辅助网络可以丰富心脏骤停波形标签并生成伪连续估计作为目标。我们的提案可以独立将24小时时间平均AUC从0.74提高到0.78-0.80范围。我们主要在更长的时间范围内改进，在事件附近最小程度降低性能，从而推动早期预警系统研究。最后，我们采用多任务公式并诊断出竞争损失之间的高梯度冲突率，我们通过PCGrad优化技术缓解了这一问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-frequency physiological waveform modality offers deep, real-timeinsights into patient status. Recently, physiological foundation models basedon Photoplethysmography (PPG), such as PPG-GPT, have been shown to predictcritical events, including Cardiac Arrest (CA). However, their powerfulrepresentation still needs to be leveraged suitably, especially when thedownstream data/label is scarce. We offer three orthogonal improvements toimprove PPG-only CA systems by using minimal auxiliary information. First, wepropose to use time-to-event modeling, either through simple regression to theevent onset time or by pursuing fine-grained discrete survival modeling.Second, we encourage the model to learn CA-focused features by making thempatient-identity invariant. This is achieved by first training thelargest-scale de-identified biometric identification model, referred to as thep-vector, and subsequently using it adversarially to deconfound cues, such asperson identity, that may cause overfitting through memorization. Third, wepropose regression on the pseudo-lab values generated by pre-trained auxiliaryestimator networks. This is crucial since true blood lab measurements, such aslactate, sodium, troponin, and potassium, are collected sparingly. Viazero-shot prediction, the auxiliary networks can enrich cardiac arrest waveformlabels and generate pseudo-continuous estimates as targets. Our proposals canindependently improve the 24-hour time-averaged AUC from the 0.74 to the0.78-0.80 range. We primarily improve over longer time horizons with minimaldegradation near the event, thus pushing the Early Warning System research.Finally, we pursue multi-task formulation and diagnose it with a high gradientconflict rate among competing losses, which we alleviate via the PCGradoptimization technique.</description>
      <author>example@mail.com (Saurabh Kataria, Davood Fattahi, Minxiao Wang, Ran Xiao, Matthew Clark, Timothy Ruchti, Mark Mai, Xiao Hu)</author>
      <guid isPermaLink="false">2509.21695v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Foundation Interatomic Potentials via Message-Passing Pruning and Graph Partitioning</title>
      <link>http://arxiv.org/abs/2509.21694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种加速和扩展原子基础模型(AFMs)的通用工作流程，通过移除低贡献的消息传递层进行模型剪枝，显著减少参数数量同时保持准确性和数据效率，并采用图分区GPU分布式策略实现大规模模拟。&lt;h4&gt;背景&lt;/h4&gt;原子基础模型(AFMs)作为准确的原子间势能很有前景，能够实现接近量子力学精度的数据高效分子动力学模拟。然而，AFMs在推理速度和内存占用方面明显劣于传统原子间势能，这是由于需要在大规模预训练数据集中捕获多种化学和结构模式，需要深度、参数丰富的模型架构。这些缺点限制了AFMs在扩展时间和空间尺度分子动力学模拟中的实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用工作流程，用于加速和扩展包含消息传递架构的原子基础模型(AFMs)，解决其在推理速度和内存占用方面的不足，使其更适合大规模分子动力学模拟。&lt;h4&gt;方法&lt;/h4&gt;从AFM主干中移除低贡献的消息传递层作为有效剪枝方法，显著减少参数数量同时保留准确性和数据效率。对剪枝后的模型采用图分区、GPU分布式策略，在AFM微调平台MatterTune中实现和演示，支持单GPU和多GPU上的百万原子级别模拟。&lt;h4&gt;主要发现&lt;/h4&gt;移除低贡献的消息传递层可显著减少AFMs的参数数量，同时保持其准确性和数据效率。该方法支持在单GPU和多GPU上进行百万原子级别的模拟，能够实现具有AFM级精度的纳秒时间尺度特定任务大规模模拟。&lt;h4&gt;结论&lt;/h4&gt;所提出的加速和扩展方法使AFMs更适合大规模分子动力学模拟，克服了AFMs在计算资源和内存使用方面的限制，使AFMs能够在扩展的时间和空间尺度上实现实际应用。&lt;h4&gt;翻译&lt;/h4&gt;原子基础模型(AFMs)作为准确的原子间势能具有巨大潜力，并已实现了接近量子力学精度的数据高效分子动力学模拟。然而，由于需要在预训练数据集中捕获广泛的化学和结构模式，这需要深度且参数丰富的模型架构，导致AFMs在推理速度上明显慢于传统原子间势能，且内存占用更大。这些缺点目前限制了AFMs在扩展时间和空间尺度的分子动力学(MD)模拟中的实际应用。为解决这一问题，我们提出了一种包含消息传递架构的AFMs的加速和扩展通用工作流程。我们发现，从AFM主干中移除低贡献的消息传递层是一种有效的剪枝方法，显著减少了参数数量，同时保留了AFMs的准确性和数据效率。剪枝后，这些模型通过图分区、GPU分布式策略更适合大规模模拟，我们在AFM微调平台MatterTune中实现了并演示了这一策略。我们表明该方法支持在单GPU和多GPU上进行百万原子级别的模拟，并能够在纳秒时间尺度上实现具有AFM级精度的特定任务大规模模拟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Atomistic foundation models (AFMs) have great promise as accurate interatomicpotentials, and have enabled data-efficient molecular dynamics simulations withnear quantum mechanical accuracy. However, AFMs remain markedly slower atinference and are far more memory-intensive than conventional interatomicpotentials, due to the need to capture a wide range of chemical and structuralmotifs in pre-training datasets requiring deep, parameter-rich modelarchitectures. These deficiencies currently limit the practical use of AFMs inmolecular dynamics (MD) simulations at extended temporal and spatial scales. Toaddress this problem, we propose a general workflow for accelerating andscaling AFMs containing message-passing architectures. We find that removinglow-contribution message-passing layers from AFM backbones serves as aneffective pruning method, significantly reducing the parameter count whilepreserving the accuracy and data-efficiency of AFMs. Once pruned, these modelsbecome more accessible for large scale simulations via a graph-partitioned,GPU-distributed strategy, which we implement and demonstrate within the AFMfine-tuning platform MatterTune. We show that this approach supportsmillion-atom simulations on both single and multiple GPUs, and enablestask-specific large-scale simulations at nanosecond timescales with AFM-levelaccuracy.</description>
      <author>example@mail.com (Lingyu Kong, Jaeheon Shim, Guoxiang Hu, Victor Fung)</author>
      <guid isPermaLink="false">2509.21694v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>SlotFM: A Motion Foundation Model with Slot Attention for Diverse Downstream Tasks</title>
      <link>http://arxiv.org/abs/2509.21673v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SlotFM是一种新型加速度计基础模型，通过时间-频率槽注意力机制处理原始信号，生成多个捕捉不同信号成分的小嵌入，在16个下游任务上展现出强大泛化能力，平均性能提升4.5%。&lt;h4&gt;背景&lt;/h4&gt;可穿戴加速度计被广泛应用于手势识别、步态分析和运动监测等领域。然而，现有基础模型主要专注于分类常见日常活动，限制了它们在依赖其他信号特征的任务中的适用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够泛化到各种下游任务的加速度计基础模型，扩展基础模型在更广泛应用中的适用性。&lt;h4&gt;方法&lt;/h4&gt;提出SlotFM，使用时间-频率槽注意力技术处理原始信号的时间频率表示。该方法生成多个小的嵌入（槽），每个槽捕捉不同信号成分，使任务特定头部能关注最相关数据部分。同时引入两个损失正则化器，捕捉局部结构和频率模式，改进细粒度细节重构并帮助嵌入保留任务相关信息。&lt;h4&gt;主要发现&lt;/h4&gt;在16个超越标准人类活动识别的分类和回归下游任务上评估，SlotFM在13个任务上优于现有自监督方法，在其余任务上实现与最佳方法相当的结果。平均性能提升4.5%，展示了传感基础模型的强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SlotFM通过创新的时间-频率槽注意力和损失正则化技术，成功实现了对多种下游任务的有效泛化，显著提升了传感基础模型的性能和应用范围。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴加速度计被广泛应用于各种应用，如手势识别、步态分析和运动监测。然而，大多数现有的基础模型主要专注于分类常见的日常活动，如移动和锻炼，这限制了它们在依赖其他信号特征的更广泛任务中的适用性。我们提出了SlotFM，这是一种能够泛化到各种下游任务的加速度计基础模型。SlotFM使用时间-频率槽注意力，这是槽注意力的扩展，能够处理原始信号的时间表示和频率表示。它生成多个小的嵌入（槽），每个槽捕捉不同的信号成分，使任务特定的头部能够关注数据中最相关的部分。我们还引入了两个损失正则化器，捕捉局部结构和频率模式，这些正则化器改进了细粒度细节的重构，并帮助嵌入保留任务相关信息。我们在16个超越标准人类活动识别的分类和回归下游任务上评估了SlotFM。在这些任务中，它在13个任务上优于现有的自监督方法，在其余任务上实现了与最佳方法相当的结果。平均而言，我们的方法实现了4.5%的性能提升，展示了传感基础模型的强大泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable accelerometers are used for a wide range of applications, such asgesture recognition, gait analysis, and sports monitoring. Yet most existingfoundation models focus primarily on classifying common daily activities suchas locomotion and exercise, limiting their applicability to the broader rangeof tasks that rely on other signal characteristics. We present SlotFM, anaccelerometer foundation model that generalizes across diverse downstreamtasks. SlotFM uses Time-Frequency Slot Attention, an extension of SlotAttention that processes both time and frequency representations of the rawsignals. It generates multiple small embeddings (slots), each capturingdifferent signal components, enabling task-specific heads to focus on the mostrelevant parts of the data. We also introduce two loss regularizers thatcapture local structure and frequency patterns, which improve reconstruction offine-grained details and helps the embeddings preserve task-relevantinformation. We evaluate SlotFM on 16 classification and regression downstreamtasks that extend beyond standard human activity recognition. It outperformsexisting self-supervised approaches on 13 of these tasks and achievescomparable results to the best performing approaches on the remaining tasks. Onaverage, our method yields a 4.5% performance gain, demonstrating stronggeneralization for sensing foundation models.</description>
      <author>example@mail.com (Junyong Park, Oron Levy, Rebecca Adaimi, Asaf Liberman, Gierad Laput, Abdelkareem Bedri)</author>
      <guid isPermaLink="false">2509.21673v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Neuroprobe: Evaluating Intracranial Brain Responses to Naturalistic Stimuli</title>
      <link>http://arxiv.org/abs/2509.21671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 7 main figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Neuroprobe是一个用于研究大脑多模态语言处理的解码任务套件，基于BrainTreebank数据集构建，包含10名受试者40小时的iEEG记录。它既可作为神经科学研究的资源，也可作为iEEG基础模型的评估框架。&lt;h4&gt;背景&lt;/h4&gt;高分辨率神经数据集为下一代脑机接口和神经治疗提供了基础模型，但社区需要严格的基准来区分竞争性的建模方法，而目前对于颅内脑电图(iEEG)记录还没有标准化的评估框架。&lt;h4&gt;目的&lt;/h4&gt;解决iEEG评估框架缺失的差距，提出Neuroprobe作为研究大脑多模态语言处理的解码任务套件，并为iEEG基础模型提供严格的评估框架。&lt;h4&gt;方法&lt;/h4&gt;Neuroprobe基于BrainTreebank数据集构建，该数据集包含10名人类受试者在观看自然电影任务中40小时的iEEG记录。与头皮脑电图不同，颅内脑电图需要侵入性手术来植入电极，直接从大脑记录神经活动，信号失真最小。Neuroprobe设计注重计算效率和易用性，代码公开可用并维护公开排行榜。&lt;h4&gt;主要发现&lt;/h4&gt;Neuroprobe有两个关键功能：1)作为神经科学研究资源，通过高时间和空间分辨率确定大脑中语言处理各方面计算的时间和位置；2)作为iEEG基础模型的评估框架。研究发现线性基线在许多任务上表现优于前沿基础模型。&lt;h4&gt;结论&lt;/h4&gt;Neuroprobe代码公开可用并维护公开排行榜，旨在促进iEEG基础模型领域的快速进展。&lt;h4&gt;翻译&lt;/h4&gt;高分辨率神经数据集为下一代脑机接口和神经治疗提供了基础模型。社区需要严格的基准来区分竞争性的建模方法，但目前对于颅内脑电图(iEEG)记录还没有标准化的评估框架。为解决这一差距，我们提出了Neuroprobe：一套用于研究大脑多模态语言处理的解码任务。与头皮脑电图不同，颅内脑电图需要侵入性手术来植入电极，直接从大脑记录神经活动，信号失真最小。Neuroprobe基于BrainTreebank数据集构建，该数据集包含10名人类受试者在观看自然电影任务中40小时的iEEG记录。Neuroprobe有两个关键功能。首先，它是一个可以挖掘神经科学见解的资源。其高时间和空间分辨率允许研究人员通过测量所有电极位置上每个特征的解码能力，系统性地确定大脑中每个语言处理方面计算的时间和位置。利用Neuroprobe，我们以纯数据驱动的方式可视化了信息如何从颞上流向前额叶皮层，以及从简单听觉特征到更复杂语言特征的进展。其次，随着领域向神经基础模型发展，Neuroprobe为竞争性架构和训练协议的比较提供了严格框架。我们发现线性基线出乎意料地强大，在许多任务上击败了前沿基础模型。Neuroprobe设计注重计算效率和易用性。我们公开Neuroprobe的代码并维护一个公开排行榜，旨在促进iEEG基础模型领域的快速进展，网址为https://neuroprobe.dev/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-resolution neural datasets enable foundation models for the nextgeneration of brain-computer interfaces and neurological treatments. Thecommunity requires rigorous benchmarks to discriminate between competingmodeling approaches, yet no standardized evaluation frameworks exist forintracranial EEG (iEEG) recordings. To address this gap, we present Neuroprobe:a suite of decoding tasks for studying multi-modal language processing in thebrain. Unlike scalp EEG, intracranial EEG requires invasive surgery to implantelectrodes that record neural activity directly from the brain with minimalsignal distortion. Neuroprobe is built on the BrainTreebank dataset, whichconsists of 40 hours of iEEG recordings from 10 human subjects performing anaturalistic movie viewing task. Neuroprobe serves two critical functions.First, it is a mine from which neuroscience insights can be drawn. Its hightemporal and spatial resolution allows researchers to systematically determinewhen and where computations for each aspect of language processing occur in thebrain by measuring the decodability of each feature across time and allelectrode locations. Using Neuroprobe, we visualize how information flows fromthe superior temporal gyrus to the prefrontal cortex, and the progression fromsimple auditory features to more complex language features in a purelydata-driven manner. Second, as the field moves toward neural foundation models,Neuroprobe provides a rigorous framework for comparing competing architecturesand training protocols. We found that the linear baseline is surprisinglystrong, beating frontier foundation models on many tasks. Neuroprobe isdesigned with computational efficiency and ease of use in mind. We make thecode for Neuroprobe openly available and maintain a public leaderboard, aimingto enable rapid progress in the field of iEEG foundation models, athttps://neuroprobe.dev/</description>
      <author>example@mail.com (Andrii Zahorodnii, Christopher Wang, Bennett Stankovits, Charikleia Moraitaki, Geeling Chau, Andrei Barbu, Boris Katz, Ila R Fiete)</author>
      <guid isPermaLink="false">2509.21671v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>MORPH: Shape-agnostic PDE Foundation Models</title>
      <link>http://arxiv.org/abs/2509.21670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为MORPH的偏微分方程基础模型，它基于卷积视觉变压器构建，能够处理异构时空数据，并在多种下游任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;科学机器学习需要处理异构、多模态的科学观测数据，而现有模型在处理不同维度、分辨率和场类型的数据时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种形状无关的自回归基础模型，能够无缝处理不同维度(1D-3D)、不同分辨率的异构时空数据集，以及包含标量和矢量分量的多场数据。&lt;h4&gt;方法&lt;/h4&gt;构建了结合三个关键组件的MORPH架构：(1)分量级卷积处理标量和矢量通道；(2)场间交叉注意力建模不同物理场间的信息传播；(3)轴向注意力减少计算负担同时保持表达能力。通过全模型微调和低秩适配器进行迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;MORPH在零样本和全样本泛化方面都优于从头开始训练的模型，在大量评估中匹配或超越了强大的基线和最新最先进模型。&lt;h4&gt;结论&lt;/h4&gt;MORPH提供了一个灵活且强大的主干，用于从科学观测的异构性和多模态性中学习，为可扩展和数据高效的科学机器学习铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了MORPH，这是一种与形状无关的自回归基础模型，用于偏微分方程(PDEs)。MORPH基于卷积视觉变压器主干构建，能够无缝处理不同维度(1D-3D)、不同分辨率的异构时空数据集，以及包含标量和矢量分量的多场数据。该架构结合了(1)分量级卷积，联合处理标量和矢量通道以捕获局部交互；(2)场间交叉注意力，建模和选择性传播不同物理场之间的信息；(3)轴向注意力，沿单个空间和时间轴分解完整的时空自注意力，以减少计算负担同时保持表达能力。我们在多样化的异构PDE数据集上预训练了多个模型变体，并评估了对一系列下游预测任务的迁移能力。通过全模型微调和参数高效的低秩适配器(LoRA)，MORPH在零样本和全样本泛化方面都优于从头开始训练的模型。在大量评估中，MORPH匹配或超越了强大的基线和最新的最先进模型。这些能力共同展示了一个灵活且强大的主干，用于从科学观测的异构性和多模态性中学习，为可扩展和数据高效的科学机器学习铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce MORPH, a shape-agnostic, autoregressive foundation model forpartial differential equations (PDEs). MORPH is built on a convolutional visiontransformer backbone that seamlessly handles heterogeneous spatiotemporaldatasets of varying data dimensionality (1D--3D) at different resolutions,multiple fields with mixed scalar and vector components. The architecturecombines (i) component-wise convolution, which jointly processes scalar andvector channels to capture local interactions, (ii) inter-fieldcross-attention, which models and selectively propagates information betweendifferent physical fields, (iii) axial attentions, which factorizes fullspatiotemporal self-attention along individual spatial and temporal axes toreduce computational burden while retaining expressivity. We pretrain multiplemodel variants on a diverse collection of heterogeneous PDE datasets andevaluate transfer to a range of downstream prediction tasks. Using bothfull-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPHoutperforms models trained from scratch in both zero-shot and full-shotgeneralization. Across extensive evaluations, MORPH matches or surpasses strongbaselines and recent state-of-the-art models. Collectively, these capabilitiespresent a flexible and powerful backbone for learning from heterogeneous andmultimodal nature of scientific observations, charting a path toward scalableand data-efficient scientific machine learning.</description>
      <author>example@mail.com (Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Ayan Biswas, Diane Oyen, Earl Lawrence)</author>
      <guid isPermaLink="false">2509.21670v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction</title>
      <link>http://arxiv.org/abs/2509.21657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为FantasyWorld的几何增强框架，通过为冻结的视频基础模型添加可训练的几何分支，实现了视频潜隐和隐式3D场的联合建模，有效解决了当前视频基础模型缺乏明确3D接地能力的问题，从而提高了空间一致性和下游3D推理任务的实用性。&lt;h4&gt;背景&lt;/h4&gt;高质量的3D世界模型对具身智能和通用人工智能至关重要，支持AR/VR内容创建和机器人导航等应用。然而，尽管现有的视频基础模型具有强大的想象先验，但它们缺乏明确的3D接地能力，限制了空间一致性和下游3D推理任务的效用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将视频想象与3D感知相结合的框架，增强视频基础模型的3D能力，使其能够生成空间一致且适用于下游3D任务的表示。&lt;h4&gt;方法&lt;/h4&gt;提出了FantasyWorld框架，这是一个几何增强系统，通过为冻结的视频基础模型添加可训练的几何分支，实现视频潜隐和隐式3D场的联合建模。引入跨分支监督机制，其中几何线索指导视频生成，视频先验正则化3D预测，从而产生一致且可泛化的3D感知视频表示。&lt;h4&gt;主要发现&lt;/h4&gt;FantasyWorld有效地桥接了视频想象和3D感知，在多视图一致性和风格一致性方面优于最近的几何一致性基线。几何分支产生的潜隐可作为下游3D任务（如新视图合成和导航）的通用表示，无需针对每个场景进行优化或微调。&lt;h4&gt;结论&lt;/h4&gt;FantasyWorld通过统一主干和跨分支信息交换，成功解决了视频基础模型的3D接地问题，为高质量3D世界模型的构建提供了新思路，有望推动具身智能和通用人工智能的发展。&lt;h4&gt;翻译&lt;/h4&gt;高质量3D世界模型对具身智能和通用人工智能(AGI)至关重要，支持AR/VR内容创建和机器人导航等应用。尽管已建立强大的想象先验，当前视频基础模型缺乏明确的3D接地能力，因此在空间一致性和下游3D推理任务的实用性方面受到限制。在这项工作中，我们提出了FantasyWorld，一个几何增强框架，通过为冻结的视频基础模型添加可训练的几何分支，能够在单次前向传递中联合建模视频潜隐和隐式3D场。我们的方法引入了跨分支监督，其中几何线索指导视频生成，视频先验正则化3D预测，从而产生一致且可泛化的3D感知视频表示。值得注意的是，几何分支产生的潜隐可作为下游3D任务（如新视图合成和导航）的通用表示，无需针对每个场景进行优化或微调。大量实验表明，FantasyWorld有效地桥接了视频想象和3D感知，在多视图一致性和风格一致性方面优于最近的几何一致性基线。消融研究进一步证实，这些改进来自于统一主干和跨分支信息交换。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前视频基础模型缺乏明确3D基础能力的问题，导致空间一致性和下游3D推理任务应用受限。这个问题很重要，因为高质量的3D世界模型对具身智能和人工智能至关重要，支撑着AR/VR内容创建、机器人导航等应用，能让AI系统更好地理解和生成与现实世界一致的环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有视频生成模型与3D感知之间的弱耦合问题，发现大多数方法仅在视频领域内操作，特征无法直接支持3D推理，且需要额外场景优化。他们设计了一个几何增强框架，在冻结视频模型上添加可训练几何分支。该方法借鉴了VGGT的3D特征提取架构、WanDiT的扩散模型架构和双向交叉注意力机制，实现了视频与几何的紧密集成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过统一框架同时进行视频生成和3D几何推理，使视频想象力和3D感知相互强化，无需场景特定优化。流程包括：1)接收图像、文本和相机轨迹输入；2)预处理块对视频潜在表示部分去噪；3)集成重建和生成块采用双分支设计，包含想象先验分支和几何一致分支；4)通过双向交叉注意力连接两个分支；5)输出几何一致视频帧和3D特征，可用于下游任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的视频-3D建模框架；2)2D/3D跨分支监督机制；3)可推广的3D特征潜力。相比之前工作，FantasyWorld实现了视频生成和3D感知的紧密耦合而非弱连接；无需场景特定优化；通过轻量级适配器实现高效集成；直接从视频潜在表示推断几何信息而非从RGB图像预测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FantasyWorld通过统一的视频-3D建模框架，实现了视频生成与几何推理的紧密耦合，在单一前向传递中产生既逼真又几何一致的3D世界表示，无需场景特定优化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality 3D world models are pivotal for embodied intelligence andArtificial General Intelligence (AGI), underpinning applications such as AR/VRcontent creation and robotic navigation. Despite the established strongimaginative priors, current video foundation models lack explicit 3D groundingcapabilities, thus being limited in both spatial consistency and their utilityfor downstream 3D reasoning tasks. In this work, we present FantasyWorld, ageometry-enhanced framework that augments frozen video foundation models with atrainable geometric branch, enabling joint modeling of video latents and animplicit 3D field in a single forward pass. Our approach introducescross-branch supervision, where geometry cues guide video generation and videopriors regularize 3D prediction, thus yielding consistent and generalizable3D-aware video representations. Notably, the resulting latents from thegeometric branch can potentially serve as versatile representations fordownstream 3D tasks such as novel view synthesis and navigation, withoutrequiring per-scene optimization or fine-tuning. Extensive experiments showthat FantasyWorld effectively bridges video imagination and 3D perception,outperforming recent geometry-consistent baselines in multi-view coherence andstyle consistency. Ablation studies further confirm that these gains stem fromthe unified backbone and cross-branch information exchange.</description>
      <author>example@mail.com (Yixiang Dai, Fan Jiang, Chiyu Wang, Mu Xu, Yonggang Qi)</author>
      <guid isPermaLink="false">2509.21657v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Can AI Perceive Physical Danger and Intervene?</title>
      <link>http://arxiv.org/abs/2509.21651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了具身AI系统与物理世界交互时的安全挑战，开发了一种可扩展的物理安全基准测试方法，分析了主要基础模型的安全理解能力，并提出了一种训练后范式来增强模型对具身安全约束的推理能力。&lt;h4&gt;背景&lt;/h4&gt;当AI与物理世界交互时（如机器人或辅助代理），出现了超越纯'数字AI'的新安全挑战。在物理交互中，造成实际伤害的可能性是直接且即时的。研究关注最先进的基础模型对物理安全常识（如物体重量、热物处理等）的理解程度。&lt;h4&gt;目的&lt;/h4&gt;评估和提升具身AI系统对物理安全的理解和推理能力，为安全关键代理应用的部署提供指导。&lt;h4&gt;方法&lt;/h4&gt;1) 开发基于真实世界伤害叙事和操作安全约束的可扩展物理安全基准测试方法，使用先进生成模型将叙事和约束转化为逼真的图像和视频；2) 全面分析主要基础模型的风险感知、安全推理和干预触发能力；3) 开发训练后范式，教模型通过系统指令明确推理具身特定的安全约束，使安全推理可解释和透明。&lt;h4&gt;主要发现&lt;/h4&gt;1) 具身AI系统在物理安全理解方面存在挑战；2) 主要基础模型在风险感知、安全推理和干预触发能力方面表现各异；3) 所提出的训练后范式能显著提升模型对具身安全约束的推理能力，使推理过程可解释和透明。&lt;h4&gt;结论&lt;/h4&gt;通过开发专门的基准测试方法和训练范式，可以提升具身AI系统对物理安全的理解和推理能力，为安全关键代理应用的部署提供支持。&lt;h4&gt;翻译&lt;/h4&gt;当AI与物理世界交互时——无论是作为机器人还是辅助代理——出现了超越纯'数字AI'的新安全挑战。在这种交互中，造成物理伤害的可能性是直接且即时的。最先进的基础模型对物理安全常识的理解程度如何？例如，一个盒子可能太重而无法举起，或者一杯热咖啡不应该递给儿童？在本文中，我们的贡献有三方面：首先，我们开发了一种高度可扩展的方法，用于基于真实世界伤害叙事和操作安全约束对具身AI系统进行持续的物理安全基准测试。为了探测多模态安全理解，我们使用先进的生成模型将这些叙事和约束转变为逼真的图像和视频，捕捉从安全到不安全状态的转变。其次，我们全面分析了主要基础模型感知风险、推理安全和触发干预的能力；这为它们在安全关键代理应用中的部署准备性提供了多方面的见解。最后，我们开发了一种训练后范式，教模型通过系统指令明确推理具身特定的安全约束。由此产生的模型生成思考轨迹，使安全推理可解释和透明，在约束满足评估中达到了最先进的性能。该基准测试将在https://asimov-benchmark.github.io/v2发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When AI interacts with the physical world -- as a robot or an assistive agent-- new safety challenges emerge beyond those of purely ``digital AI". In suchinteractions, the potential for physical harm is direct and immediate. How welldo state-of-the-art foundation models understand common-sense facts aboutphysical safety, e.g. that a box may be too heavy to lift, or that a hot cup ofcoffee should not be handed to a child? In this paper, our contributions arethree-fold: first, we develop a highly scalable approach to continuous physicalsafety benchmarking of Embodied AI systems, grounded in real-world injurynarratives and operational safety constraints. To probe multi-modal safetyunderstanding, we turn these narratives and constraints into photorealisticimages and videos capturing transitions from safe to unsafe states, usingadvanced generative models. Secondly, we comprehensively analyze the ability ofmajor foundation models to perceive risks, reason about safety, and triggerinterventions; this yields multi-faceted insights into their deploymentreadiness for safety-critical agentic applications. Finally, we develop apost-training paradigm to teach models to explicitly reason aboutembodiment-specific safety constraints provided through system instructions.The resulting models generate thinking traces that make safety reasoninginterpretable and transparent, achieving state of the art performance inconstraint satisfaction evaluations. The benchmark will be released athttps://asimov-benchmark.github.io/v2</description>
      <author>example@mail.com (Abhishek Jindal, Dmitry Kalashnikov, Oscar Chang, Divya Garikapati, Anirudha Majumdar, Pierre Sermanet, Vikas Sindhwani)</author>
      <guid isPermaLink="false">2509.21651v1</guid>
      <pubDate>Mon, 29 Sep 2025 15:40:58 +0800</pubDate>
    </item>
    <item>
      <title>Improved Therapeutic Antibody Reformatting through Multimodal Machine Learning</title>
      <link>http://arxiv.org/abs/2509.19604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 AI4Science Workshop and NeurIPS 2025 Multi-modal  Foundation Models and Large Language Models for Life Sciences Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文开发了一个机器学习框架用于预测抗体格式转换的成功性，帮助研究人员优先选择有前景的抗体格式转换方案，减少实验浪费。&lt;h4&gt;背景&lt;/h4&gt;现代治疗性抗体设计常涉及组合来自不同来源的多个功能域，这种复杂格式虽可扩大疾病适用性并提高安全性，但存在个体域在新格式中功能和稳定性无法保证、整个分子可能无法合成的工程挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个机器学习框架来预测'重新格式化成功'，即预测将抗体从一种格式转换为另一种格式是否会成功。&lt;h4&gt;方法&lt;/h4&gt;构建结合抗体序列和结构背景的机器学习框架，采用反映实际部署场景的评估协议，并在真实抗体重新格式化数据集上进行实验测试。&lt;h4&gt;主要发现&lt;/h4&gt;大型预训练蛋白质语言模型未能超越简单、针对特定领域的多模态表示；在最具挑战性的'新抗体，无数据'场景中，最佳多模态模型实现了高预测准确性。&lt;h4&gt;结论&lt;/h4&gt;所开发的多模态机器学习框架能有效预测抗体格式转换成功性，特别是在新型抗体且缺乏数据的情况下，可指导实验工作并减少资源浪费。&lt;h4&gt;翻译&lt;/h4&gt;现代治疗性抗体设计通常涉及组合来自不同来源或独立工程化的多个功能域的组装体。虽然这些复杂格式可以扩大疾病适用性并提高安全性，但它们带来了重大的工程挑战：个体域在新格式中的功能和稳定性无法保证，整个分子可能不再可合成。为解决这些挑战，我们开发了一个机器学习框架来预测'重新格式化成功'——即预测将抗体从一种格式转换为另一种格式是否会成功。我们的框架结合了抗体序列和结构背景，并采用反映实际部署场景的评估协议。在一个真实的抗体重新格式化数据集上的实验中，我们发现了一个令人惊讶的结果：大型预训练蛋白质语言模型未能超越简单、针对特定领域的多模态表示。在最具挑战性的评估设置中，特别是当我们测试模型对新型抗体的泛化能力时，这种差异尤为明显。在这个具有挑战性的'新抗体，无数据'场景中，我们的最佳多模态模型实现了高预测准确性，能够优先选择有前景的候选分子并减少浪费的实验工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern therapeutic antibody design often involves composing multi-partassemblages of individual functional domains, each of which may be derived froma different source or engineered independently. While these complex formats canexpand disease applicability and improve safety, they present a significantengineering challenge: the function and stability of individual domains are notguaranteed in the novel format, and the entire molecule may no longer besynthesizable. To address these challenges, we develop a machine learningframework to predict "reformatting success" -- whether converting an antibodyfrom one format to another will succeed or not. Our framework incorporates bothantibody sequence and structural context, incorporating an evaluation protocolthat reflects realistic deployment scenarios. In experiments on a real-worldantibody reformatting dataset, we find the surprising result that largepretrained protein language models (PLMs) fail to outperform simple,domain-tailored, multimodal representations. This is particularly evident inthe most difficult evaluation setting, where we test model generalization to anew starting antibody. In this challenging "new antibody, no data" scenario,our best multimodal model achieves high predictive accuracy, enablingprioritization of promising candidates and reducing wasted experimental effort.</description>
      <author>example@mail.com (Jiayi Xin, Aniruddh Raghu, Nick Bhattacharya, Adam Carr, Melanie Montgomery, Hunter Elliott)</author>
      <guid isPermaLink="false">2509.19604v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
  <item>
      <title>GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions</title>
      <link>http://arxiv.org/abs/2509.21050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了几何问题中的指代表达式理解(REC)任务，构建了GeoRef基准数据集，探索了监督微调(SFT)和组相对策略优化(GRPO)两种微调方法，并提出了验证和重新生成机制以提高准确性。研究结果表明，即使是先进的多模态大语言模型在几何基础方面也存在困难，但适当的训练可以改善下游几何推理任务的表现。&lt;h4&gt;背景&lt;/h4&gt;AI驱动的几何问题解决是一个复杂的视觉-语言任务，需要准确的图表解释、数学推理和强大的跨模态基础能力。根据自然语言查询识别和解释几何元素是一个基础但未被充分探索的能力。&lt;h4&gt;目的&lt;/h4&gt;引入几何问题中的指代表达式理解(REF)任务，评估模型是否能够根据文本提示在图表中定位点、形状和空间关系，并构建一个基准数据集(GeoRef)。&lt;h4&gt;方法&lt;/h4&gt;从现有几何问题语料库构建GeoRef基准数据集，使用结构化的几何形式语言生成大规模合成训练数据；探索监督微调(SFT)和组相对策略优化(GRPO)两种微调方法；提出验证和重新生成机制来检测不正确预测并使用上下文推理历史重新推断答案。&lt;h4&gt;主要发现&lt;/h4&gt;GRPO显著优于SFT，能更好地使模型行为与任务特定奖励保持一致；验证和重新生成机制进一步提高了准确性；即使是最先进的多模态大语言模型也难以完成此任务；在GeoRef上训练的模型在下游几何推理任务上显示出可衡量的改进。&lt;h4&gt;结论&lt;/h4&gt;明确评估和加强几何基础能力对于强大的几何问题解决是必要的；REC作为多模态数学理解的基础具有更广泛的价值。&lt;h4&gt;翻译&lt;/h4&gt;AI驱动的几何问题解决是一个复杂的视觉-语言任务，需要准确的图表解释、数学推理和强大的跨模态基础能力。这个任务的一个基础但未被充分探索的能力是根据自然语言查询识别和解释几何元素。为此，我们引入了几何问题中的指代表达式理解(REF)任务，评估模型是否能够根据文本提示在图表中定位点、形状和空间关系。我们提出了从现有几何问题语料库构建的GeoRef基准数据集，具有多样化、高质量的注释和查询。由于缺乏此任务的标注数据，我们使用结构化的几何形式语言生成了大规模合成训练数据，使几何概念覆盖广泛并促进模型适应。我们探索了两种微调方法：监督微调(SFT)和组相对策略优化(GRPO)。我们的结果表明，GRPO通过更好地使模型行为与任务特定奖励保持一致，显著优于SFT。此外，我们提出了一种验证和重新生成机制，可以检测不正确的预测并使用上下文推理历史重新推断答案，从而进一步提高准确性。值得注意的是，即使是最先进的多模态大语言模型也难以完成此任务，这凸显了明确评估和加强几何基础能力作为强大几何问题解决先决条件的必要性。此外，在GeoRef上训练的模型在下游几何推理任务上显示出可衡量的改进，突显了REC作为多模态数学理解基础的更广泛价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-driven geometric problem solving is a complex vision-language task thatrequires accurate diagram interpretation, mathematical reasoning, and robustcross-modal grounding. A foundational yet underexplored capability for thistask is the ability to identify and interpret geometric elements based onnatural language queries. To address this, we introduce the task of ReferringExpression Comprehension (REC) for geometric problems, which evaluates whethermodels can localize points, shapes, and spatial relations in diagrams inresponse to textual prompts. We present GeoRef, a benchmark dataset constructedfrom existing geometric problem corpora, featuring diverse, high-qualityannotations and queries. Due to the lack of annotated data for this task, wegenerate a large-scale synthetic training dataset using a structured geometricformal language, enabling broad coverage of geometric concepts and facilitatingmodel adaptation. We explore two fine-tuning approaches: Supervised Fine-Tuning(SFT) and Group Relative Policy Optimization (GRPO). Our results show that GRPOsignificantly outperforms SFT by better aligning model behavior withtask-specific rewards. Furthermore, we propose a verify-and-regeneratemechanism that detects incorrect predictions and re-infers answers usingcontextual reasoning history, further boosting accuracy. Notably, evenstate-of-the-art Multimodal Large Language Models (MLLMs) struggle with thistask, underscoring the necessity of explicitly evaluating and strengtheninggeometric grounding as a prerequisite for robust geometric problem solving.Moreover, models trained on GeoRef demonstrate measurable improvements ondownstream geometric reasoning tasks, highlighting the broader value of REC asa foundation for multimodal mathematical understanding.</description>
      <author>example@mail.com (Bing Liu, Wenqiang Yv, Xuzheng Yang, Shichang Wang, Junzhuo Liu, Peng Wang, Guoqing Wang, Yang Yang, Heng Tao Shen)</author>
      <guid isPermaLink="false">2509.21050v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection</title>
      <link>http://arxiv.org/abs/2509.20701v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的双路径边缘网络，用于解决红外小目标检测中的挑战，通过解耦边缘增强和语义建模，实现了精确的目标检测和定位。&lt;h4&gt;背景&lt;/h4&gt;红外小目标检测对于遥感应用如灾害预警和海事监视至关重要。然而，由于缺乏独特的纹理和形态特征，红外小目标很容易融入杂乱和嘈杂的背景中。现有方法通常依赖于固定的梯度算子或简单的注意力机制，在低对比度和高噪声条件下无法准确提取目标边缘。&lt;h4&gt;目的&lt;/h4&gt;解决红外小目标检测中捕获微小目标的高分辨率空间细节与提取较大目标的鲁棒语义上下文之间的内在冲突，提高检测准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出双路径边缘网络，包含两条互补处理路径：第一条路径采用双向交互模块，结合局部自注意力和全局自注意力捕获多尺度特征依赖性；第二条路径引入多边缘细化器，使用多级泰勒有限差分算子增强边缘细节，结合注意力驱动的门控机制实现精确定位和特征增强。&lt;h4&gt;主要发现&lt;/h4&gt;双路径边缘网络能够有效解决特征错位问题，实现不同大小目标的精确定位和特征增强，同时有效抑制噪声。&lt;h4&gt;结论&lt;/h4&gt;该方法为精确的红外小目标检测和定位提供了有希望的解决方案，在一个统一的框架中结合了结构语义和边缘细化。&lt;h4&gt;翻译&lt;/h4&gt;红外小目标检测对于遥感应用（如灾害预警和海事监视）至关重要。然而，由于缺乏独特的纹理和形态特征，红外小目标很容易融入杂乱和嘈杂的背景中。为此任务设计深度模型的一个基本挑战在于，捕获微小目标的高分辨率空间细节与提取较大目标的鲁棒语义上下文之间存在内在冲突，这通常会导致特征错位和次优性能。现有方法通常依赖于固定的梯度算子或简单的注意力机制，这些方法在低对比度和高噪声条件下无法准确提取目标边缘。在本文中，我们提出了一种新颖的双路径边缘网络，通过将边缘增强和语义建模解耦为两个互补的处理路径，明确地解决了这一挑战。第一条路径采用双向交互模块，该模块使用局部自注意力和全局自注意力来捕获多尺度的局部和全局特征依赖性。基于Transformer架构的全局注意力机制整合了长程语义关系和上下文信息，确保了鲁棒的场景理解。第二条路径引入了多边缘细化器，该细化器使用多级泰勒有限差分算子在多个尺度上增强细粒度的边缘细节。这种数学方法，结合注意力驱动的门控机制，能够实现不同大小目标的精确定位和特征增强，同时有效抑制噪声。我们的方法为精确的红外小目标检测和定位提供了有希望的解决方案，在一个统一的框架中结合了结构语义和边缘细化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infrared small target detection is crucial for remote sensing applicationslike disaster warning and maritime surveillance. However, due to the lack ofdistinctive texture and morphological features, infrared small targets arehighly susceptible to blending into cluttered and noisy backgrounds. Afundamental challenge in designing deep models for this task lies in theinherent conflict between capturing high-resolution spatial details for minutetargets and extracting robust semantic context for larger targets, oftenleading to feature misalignment and suboptimal performance. Existing methodsoften rely on fixed gradient operators or simplistic attention mechanisms,which are inadequate for accurately extracting target edges under low contrastand high noise. In this paper, we propose a novel Dual-Path Edge Network thatexplicitly addresses this challenge by decoupling edge enhancement and semanticmodeling into two complementary processing paths. The first path employs aBidirectional Interaction Module, which uses both Local Self-Attention andGlobal Self-Attention to capture multi-scale local and global featuredependencies. The global attention mechanism, based on a Transformerarchitecture, integrates long-range semantic relationships and contextualinformation, ensuring robust scene understanding. The second path introducesthe Multi-Edge Refiner, which enhances fine-grained edge details using cascadedTaylor finite difference operators at multiple scales. This mathematicalapproach, along with an attention-driven gating mechanism, enables precise edgelocalization and feature enhancement for targets of varying sizes, whileeffectively suppressing noise. Our method provides a promising solution forprecise infrared small target detection and localization, combining structuralsemantics and edge refinement in a unified framework.</description>
      <author>example@mail.com (Jiayi Zuo, Songwei Pei, Qian Li)</author>
      <guid isPermaLink="false">2509.20701v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.19973v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为OmniScene的类人框架，通过视觉语言模型和分层融合策略，使自动驾驶系统能够实现更接近人类的三维场景理解能力。&lt;h4&gt;背景&lt;/h4&gt;人类视觉能够将二维观察转化为以自我为中心的三维场景理解，支持复杂场景转换和适应性行为。然而，当前自动驾驶系统主要依赖基于深度的3D重建，缺乏真正的场景理解能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种类人框架OmniScene，弥补当前自动驾驶系统在场景理解方面的不足，实现更接近人类视觉系统的感知和理解能力。&lt;h4&gt;方法&lt;/h4&gt;1) 引入OmniScene视觉语言模型(OmniVLM)，整合多视图和时间感知实现4D场景理解；2) 采用教师-学生架构和知识蒸馏，将文本表示嵌入3D实例特征进行语义监督；3) 提出分层融合策略(HFS)，自适应校准几何和语义特征的相对重要性，解决模态贡献不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;OmniScene在nuScenes数据集上与十多种最先进模型进行对比测试，在感知、预测、规划和视觉问答等任务上均取得优越结果，建立了新的性能基准。&lt;h4&gt;结论&lt;/h4&gt;OmniScene框架通过模拟人类视觉系统的场景理解能力，结合有效的多模态融合策略，显著提升了自动驾驶系统的场景理解和适应能力，为未来自动驾驶技术的发展提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;人类视觉能够将二维观察转化为以自我为中心的三维场景理解，这支持了复杂场景的转换和适应性行为的能力。然而，当前自动驾驶系统仍缺乏这种能力，主流方法主要依赖基于深度的3D重建而非真正的场景理解。为解决这一局限，我们提出了一种名为OmniScene的新型类人框架。首先，我们引入了OmniScene视觉语言模型(OmniVLM)，这是一个整合多视图和时间感知以实现整体4D场景理解的视觉语言框架。然后，利用教师-学生OmniVLM架构和知识蒸馏，我们将文本表示嵌入3D实例特征中进行语义监督，丰富特征学习并明确捕获类人的注意力语义。这些特征表示进一步与人类驾驶行为对齐，形成更类人的感知-理解-行动架构。此外，我们提出了一种分层融合策略(HFS)来解决多模态融合过程中模态贡献不平衡的问题。我们的方法在多个抽象层次上自适应校准几何和语义特征的相对重要性，实现了来自视觉和文本模态互补线索的协同使用。这种可学习的动态融合能够更细致有效地利用异构信息。我们在nuScenes数据集上全面评估了OmniScene，在各种任务上与十多种最先进模型进行基准测试。我们的方法 consistently取得了优越的结果，在感知、预测、规划和视觉问答方面建立了新的基准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决当前自动驾驶系统缺乏人类视觉能力的问题，即无法像人类一样将二维观察转化为以自我为中心的三维场景理解。这个问题很重要，因为人类视觉能够处理复杂场景并做出适应性决策，而现有系统主要依赖基于深度的3D重建，缺乏真正的场景理解能力，导致在复杂和模糊场景下表现有限，无法有效整合交通动态和导航约束等关键上下文信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析人类视觉系统的能力来设计方法，借鉴了视觉-语言模型(VLM)、注意力机制、端到端自动驾驶和多模态融合等现有工作。作者设计了OmniScene框架，包括OmniVLM视觉-语言模型和分层融合策略(HFS)，并通过教师-学生架构提高计算效率。他们结合了现有VLM技术(如Qwen2.5VL)和知识蒸馏方法，同时参考了UniAD和SparseDrive等端到端自动驾驶系统，但针对自动驾驶场景进行了专门优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是模拟人类视觉系统，将二维观察转化为三维场景理解，并通过整合视觉、文本和3D几何特征实现全面的场景理解。整体流程包括：1)接收多视角图像流、操作命令和用户提示；2)学生OmniVLM生成场景文本注释；3)提取视觉特征和文本特征；4)通过分层融合策略(HFS)整合多模态信息，包括3D实例初始化、4D时空聚合、视觉变形聚合、文本条件聚合和深度细化；5)进行多模态预测和规划；6)输出感知结果、预测轨迹和规划决策。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)OmniScene框架实现人类般的场景理解；2)OmniVLM架构整合多视角和时间感知；3)教师-学生知识蒸馏将文本表示嵌入3D实例特征；4)分层融合策略(HFS)解决模态不平衡问题；5)全局多模态对齐策略。相比之前工作，OmniScene专注于真正的场景理解而非简单的3D重建，实现了视觉和文本模态的深度整合而非独立处理，具有明确的人类注意力建模能力，通过教师-学生架构提高效率，并在更全面的评估指标上表现优异。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OmniScene通过整合多模态信息、模拟人类注意力机制和采用分层融合策略，实现了更接近人类视觉能力的自动驾驶场景理解，在nuScenes数据集上取得了多项任务的最新性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human vision is capable of transforming two-dimensional observations into anegocentric three-dimensional scene understanding, which underpins the abilityto translate complex scenes and exhibit adaptive behaviors. This capability,however, remains lacking in current autonomous driving systems, wheremainstream approaches primarily rely on depth-based 3D reconstruction ratherthan true scene understanding. To address this limitation, we propose a novelhuman-like framework called OmniScene. First, we introduce the OmniSceneVision-Language Model (OmniVLM), a vision-language framework that integratesmulti-view and temporal perception for holistic 4D scene understanding. Then,harnessing a teacher-student OmniVLM architecture and knowledge distillation,we embed textual representations into 3D instance features for semanticsupervision, enriching feature learning, and explicitly capturing human-likeattentional semantics. These feature representations are further aligned withhuman driving behaviors, forming a more human-likeperception-understanding-action architecture. In addition, we propose aHierarchical Fusion Strategy (HFS) to address imbalances in modalitycontributions during multimodal integration. Our approach adaptively calibratesthe relative significance of geometric and semantic features at multipleabstraction levels, enabling the synergistic use of complementary cues fromvisual and textual modalities. This learnable dynamic fusion enables a morenuanced and effective exploitation of heterogeneous information. We evaluateOmniScene comprehensively on the nuScenes dataset, benchmarking it against overten state-of-the-art models across various tasks. Our approach consistentlyachieves superior results, establishing new benchmarks in perception,prediction, planning, and visual question answering.</description>
      <author>example@mail.com (Pei Liu, Hongliang Lu, Haichao Liu, Haipeng Liu, Xin Liu, Ruoyu Yao, Shengbo Eben Li, Jun Ma)</author>
      <guid isPermaLink="false">2509.19973v2</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment</title>
      <link>http://arxiv.org/abs/2509.20401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了SGAligner++，一种跨模态、语言辅助的3D场景图对齐框架，能有效处理不完整或有噪声的输入，在机器人导航和具身感知应用中实现准确的场景图对齐。&lt;h4&gt;背景&lt;/h4&gt;3D场景图对齐是机器人导航和具身感知应用中的关键初始步骤，但当前方法通常依赖单模态点云数据，在处理不完整或有噪声的输入时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理异构模态间部分重叠场景观测对齐挑战的方法，特别是在低重叠条件和传感器噪声环境下实现准确对齐。&lt;h4&gt;方法&lt;/h4&gt;SGAligner++通过学习统一的联合嵌入空间来处理跨异构模态的部分重叠场景观测对齐问题，采用轻量级单模态编码器和基于注意力的融合技术，增强视觉定位、3D重建和导航等任务的场景理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的大量评估表明，SGAligner++在有噪声的真实世界重建任务上比最先进的方法性能提高高达40%，同时实现了跨模态泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SGAligner++是一种有效的跨模态、语言辅助框架，能够处理不完整或有噪声的输入，在3D场景图对齐任务上取得了显著性能提升，同时确保了可扩展性和最小计算开销。&lt;h4&gt;翻译&lt;/h4&gt;对齐3D场景图是机器人导航和具身感知中多个应用的关键初始步骤。当前3D场景图对齐方法通常依赖单模态点云数据，难以处理不完整或有噪声的输入。我们提出了SGAligner++，一种用于3D场景图对齐的跨模态、语言辅助框架。我们的方法通过学习统一的联合嵌入空间，解决了跨异构模态对齐部分重叠场景观测的挑战，即使在低重叠条件和传感器噪声下也能实现准确对齐。通过采用轻量级单模态编码器和基于注意力的融合，SGAligner++增强了视觉定位、3D重建和导航等任务的场景理解能力，同时确保了可扩展性和最小计算开销。在真实世界数据集上的大量评估表明，SGAligner++在有噪声的真实世界重建任务上比最先进的方法性能提高高达40%，同时实现了跨模态泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景图在跨模态条件下的对齐问题，特别是当输入数据不完整或有噪声时的对齐挑战。这个问题在机器人导航和具身感知中至关重要，因为机器人需要在动态环境中将不同传感器获取的信息（如点云、CAD模型、文本描述等）整合成一致的空间理解，以便进行准确的定位、导航和交互。现有方法主要依赖单一模态数据，难以处理现实世界中常见的部分重叠场景和传感器噪声问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法依赖单一模态（如点云）和固定标签词汇表的局限性，难以处理不完整重建和跨模态场景。他们借鉴了SGAligner的单模态编码器设计、MCLEA的多模态表示学习以及EVA的跨模态对齐思想，但进行了创新改进。作者设计了一个融合结构、几何和语言信息的统一框架，使用轻量级单模态编码器和基于注意力的机制，使模型能够从语言中推理空间关系，并处理不同环境中的缺失数据。他们还采用图注意力网络捕获空间关系，并使用对比学习损失函数优化嵌入空间。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习一个统一的联合嵌入空间，使不同模态的数据能够在同一空间中表示和比较，从而实现跨模态对齐。整体流程包括：1) 使用单模态编码器分别处理点云、CAD网格、结构图、文本描述和空间参考；2) 将单模态特征投影到共享潜在空间并使用注意力权重融合；3) 通过对比学习损失函数优化嵌入空间；4) 在联合嵌入空间中使用余弦相似度匹配节点；5) 合并匹配节点的属性和融合多模态数据，构建统一的3D场景图。这种方法即使在部分重叠和噪声条件下也能保持鲁棒性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次实现跨模态3D场景图对齐，支持点云、CAD网格、文本描述等多种模态；2) 不仅对齐节点，还构建统一的场景图，合并重叠对象；3) 使用轻量级设计和基于注意力的融合确保计算效率；4) 在噪声和低重叠条件下保持准确对齐；5) 模块化设计支持新模态集成；6) 利用语言信息增强场景理解。相比之前工作，SGAligner++不再局限于单一模态或固定语义标签，而是支持开放词汇语义和跨模态泛化，同时保持低计算成本和高鲁棒性，在真实世界数据上表现显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SGAligner++通过融合几何、语义和空间关系信息，实现了一种轻量级、鲁棒的跨模态3D场景图对齐方法，即使在部分重叠和噪声条件下也能准确对齐场景并生成统一的场景表示。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aligning 3D scene graphs is a crucial initial step for several applicationsin robot navigation and embodied perception. Current methods in 3D scene graphalignment often rely on single-modality point cloud data and struggle withincomplete or noisy input. We introduce SGAligner++, a cross-modal,language-aided framework for 3D scene graph alignment. Our method addresses thechallenge of aligning partially overlapping scene observations acrossheterogeneous modalities by learning a unified joint embedding space, enablingaccurate alignment even under low-overlap conditions and sensor noise. Byemploying lightweight unimodal encoders and attention-based fusion, SGAligner++enhances scene understanding for tasks such as visual localization, 3Dreconstruction, and navigation, while ensuring scalability and minimalcomputational overhead. Extensive evaluations on real-world datasetsdemonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%on noisy real-world reconstructions, while enabling cross-modal generalization.</description>
      <author>example@mail.com (Binod Singh, Sayan Deb Sarkar, Iro Armeni)</author>
      <guid isPermaLink="false">2509.20401v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>SlideMamba: Entropy-Based Adaptive Fusion of GNN and Mamba for Enhanced Representation Learning in Digital Pathology</title>
      <link>http://arxiv.org/abs/2509.21239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为SlideMamba的深度学习框架，结合Mamba架构与图神经网络(GNNs)用于全切片图像(WSI)分析，通过自适应融合策略有效捕捉局部空间关系和长程上下文依赖关系。&lt;h4&gt;背景&lt;/h4&gt;计算病理学发展越来越依赖从全切片图像中提取有意义的表示来支持临床和生物学任务，但现有方法在捕捉多尺度信息方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;开发一个可推广的深度学习框架，整合Mamba架构与图神经网络，增强全切片图像分析能力，同时捕捉局部空间关系和长程上下文依赖关系。&lt;h4&gt;方法&lt;/h4&gt;设计结合Mamba模块(擅长捕捉长程全局依赖)与GNNs(强调细粒度短程空间交互)的框架，引入基于熵的置信度加权机制的自适应融合策略，动态平衡两个分支的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;在预测基因融合和突变状态的任务中，SlideMamba达到0.751±0.05的PRAUC，显著优于MIL、Trans-MIL、Mamba-only、GNN-only和GAT-Mamba等对比方法，在ROC AUC、敏感性和特异性方面也表现优异。&lt;h4&gt;结论&lt;/h4&gt;集成架构通过基于熵的自适应融合策略得到增强，在计算病理学中的空间分辨预测建模任务具有广阔的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;计算病理学的进步越来越依赖于从全切片图像中提取有意义的表示来支持各种临床和生物学任务。在本研究中，我们提出了一个可推广的深度学习框架，将Mamba架构与图神经网络(GNNs)相结合，以增强WSI分析。我们的方法旨在捕捉局部空间关系和长程上下文依赖关系，为数字病理分析提供灵活的架构。Mamba模块擅长捕捉长程全局依赖关系，而GNNs强调细粒度的短程空间交互。为了有效结合这些互补信号，我们引入了一种自适应融合策略，使用基于熵的置信度加权机制。这种方法通过根据上下文重要性为不同下游任务分配更高权重，动态平衡两个分支的贡献，将更高权重分配给具有更置信(低熵)预测的分支。我们在代表性任务上证明了我们方法的有效性：从WSIs预测基因融合和突变状态。我们的框架SlideMamba达到了0.751±0.05的精确召回曲线下面积(PRAUC)，优于MIL (0.491±0.042)、Trans-MIL (0.39±0.017)、仅使用Mamba (0.664±0.063)、仅使用GNN (0.748±0.091)以及先前类似工作GAT-Mamba (0.703±0.075)。SlideMamba在ROC AUC (0.738±0.055)、敏感性(0.662±0.083)和特异性(0.725±0.094)方面也取得了具有竞争力的结果。这些结果突显了集成架构的优势，通过提出的基于熵的自适应融合策略得到增强，并表明在计算病理学中空间分辨预测建模任务具有应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in computational pathology increasingly rely on extractingmeaningful representations from Whole Slide Images (WSIs) to support variousclinical and biological tasks. In this study, we propose a generalizable deeplearning framework that integrates the Mamba architecture with Graph NeuralNetworks (GNNs) for enhanced WSI analysis. Our method is designed to captureboth local spatial relationships and long-range contextual dependencies,offering a flexible architecture for digital pathology analysis. Mamba modulesexcels in capturing long-range global dependencies, while GNNs emphasizefine-grained short-range spatial interactions. To effectively combine thesecomplementary signals, we introduce an adaptive fusion strategy that uses anentropy-based confidence weighting mechanism. This approach dynamicallybalances contributions from both branches by assigning higher weight to thebranch with more confident (lower-entropy) predictions, depending on thecontextual importance of local versus global information for differentdownstream tasks. We demonstrate the utility of our approach on arepresentative task: predicting gene fusion and mutation status from WSIs. Ourframework, SlideMamba, achieves an area under the precision recall curve(PRAUC) of 0.751 \pm 0.05, outperforming MIL (0.491 \pm 0.042), Trans-MIL (0.39\pm 0.017), Mamba-only (0.664 \pm 0.063), GNN-only (0.748 \pm 0.091), and aprior similar work GAT-Mamba (0.703 \pm 0.075). SlideMamba also achievescompetitive results across ROC AUC (0.738 \pm 0.055), sensitivity (0.662 \pm0.083), and specificity (0.725 \pm 0.094). These results highlight the strengthof the integrated architecture, enhanced by the proposed entropy-based adaptivefusion strategy, and suggest promising potential for application ofspatially-resolved predictive modeling tasks in computational pathology.</description>
      <author>example@mail.com (Shakib Khan, Fariba Dambandkhameneh, Nazim Shaikh, Yao Nie, Raghavan Venugopal, Xiao Li)</author>
      <guid isPermaLink="false">2509.21239v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Representation Alignment with Mirror Neurons</title>
      <link>http://arxiv.org/abs/2509.21136v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过镜像神经元的机制，提出了一种统一的表示学习方法来建模动作理解和执行，实现了两种能力之间的协同效应，提高了表示质量和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;镜像神经元是一类在个体观察和执行同一动作时都会激活的神经元，揭示了动作理解和具身执行之间的基本联系。然而，现有机器学习方法大多忽视这种联系，将这两种能力视为独立任务。&lt;h4&gt;目的&lt;/h4&gt;通过表示学习的视角为动作理解和执行提供统一的建模方法，探索两种能力之间的协同效应。&lt;h4&gt;方法&lt;/h4&gt;首先观察到动作理解和执行的中间表示会自发对齐；受镜像神经元启发，引入方法明确对齐观察和执行动作的表示；使用两个线性层将表示映射到共享的潜在空间，通过对比学习强制相应表示的对齐，最大化它们之间的互信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种简单的方法促进了动作理解和执行两个任务之间的相互协同，有效提高了表示质量和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过模拟镜像神经元机制，证明了动作理解和执行可以通过统一的表示学习方法进行有效建模，且两种能力之间的协同效应可以提升整体性能。&lt;h4&gt;翻译&lt;/h4&gt;镜像神经元是一类在个体观察动作和执行同一动作时都会激活的神经元。这种机制揭示了动作理解和具身执行之间的基本互动，表明这两种能力本质上是相互连接的。尽管如此，现有的机器学习方法大多忽视了这种互动，将这些能力视为独立任务。在本研究中，我们通过表示学习的视角为它们建模提供了一个统一的视角。我们首先观察到它们的中间表示会自发对齐。受镜像神经元启发，我们进一步引入了一种方法，明确地对齐观察和执行动作的表示。具体来说，我们使用两个线性层将表示映射到共享的潜在空间，其中对比学习强制相应表示的对齐，有效最大化它们的互信息。实验证明，这种简单的方法促进了两个任务之间的相互协同，有效提高了表示质量和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mirror neurons are a class of neurons that activate both when an individualobserves an action and when they perform the same action. This mechanismreveals a fundamental interplay between action understanding and embodiedexecution, suggesting that these two abilities are inherently connected.Nonetheless, existing machine learning methods largely overlook this interplay,treating these abilities as separate tasks. In this study, we provide a unifiedperspective in modeling them through the lens of representation learning. Wefirst observe that their intermediate representations spontaneously align.Inspired by mirror neurons, we further introduce an approach that explicitlyaligns the representations of observed and executed actions. Specifically, weemploy two linear layers to map the representations to a shared latent space,where contrastive learning enforces the alignment of correspondingrepresentations, effectively maximizing their mutual information. Experimentsdemonstrate that this simple approach fosters mutual synergy between the twotasks, effectively improving representation quality and generalization.</description>
      <author>example@mail.com (Wentao Zhu, Zhining Zhang, Yuwei Ren, Yin Huang, Hao Xu, Yizhou Wang)</author>
      <guid isPermaLink="false">2509.21136v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</title>
      <link>http://arxiv.org/abs/2509.21086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeuriIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了UniTransfer架构，通过空间和时间步分解的渐进范式实现精确可控的视频概念迁移。该方法将视频解耦为前景主体、背景和运动流三个组件，采用双流到单流的DiT架构进行细粒度控制，并引入提示链机制和自监督预训练策略。同时创建了OpenAnimal数据集，实验证明该方法在视觉保真度和可编辑性方面优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;视频概念迁移是一个具有挑战性的任务，需要精确控制和高质量生成。现有方法在处理视频的空间和时间维度时缺乏足够的灵活性和控制能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现精确和可控视频概念迁移的新型架构，通过分解视频的不同组件并提供细粒度控制，提高生成视频的质量和可编辑性。&lt;h4&gt;方法&lt;/h4&gt;1. 提出UniTransfer架构，引入空间和时间步分解的渐进范式；2. 将视频解耦为前景主体、背景和运动流三个关键组件；3. 采用双流到单流的DiT架构支持对不同组件的细粒度控制；4. 提出基于随机掩码的自监督预训练策略增强分解表示学习；5. 引入提示链机制将去噪过程分解为三个不同粒度的阶段；6. 利用大型语言模型进行特定阶段的指导；7. 创建OpenAnimal视频数据集用于研究和基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;1. 空间和时间步分解能够有效提高视频概念迁移的精确性和可控性；2. 双流到单流的DiT架构能够实现对视频不同组件的细粒度控制；3. 提示链机制和大型语言模型的指导能够改善生成过程；4. 所提出的方法在视觉保真度和可编辑性方面优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;UniTransfer架构通过创新的空间和时间分解方法，实现了高质量和可控的视频概念迁移。该方法不仅提高了生成视频的质量，还提供了更好的控制能力，为视频概念迁移领域的研究开辟了新方向。OpenAnimal数据集的创建也为该领域的研究提供了宝贵的资源。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新型架构UniTransfer，它在渐进范式中引入了空间和扩散时间步分解，实现了精确可控的视频概念迁移。具体来说，在空间分解方面，我们将视频解耦为三个关键组件：前景主体、背景和运动流。基于这种分解公式，我们进一步引入了双流到单流的基于DiT的架构，以支持对视频中不同组件的细粒度控制。我们还引入了一种基于随机掩码的自监督预训练策略，以增强从大规模未标记视频数据中学习分解表示。受思维链推理范式的启发，我们重新审视了去噪扩散过程，并提出了提示链机制来实现时间步分解。我们将去噪过程分解为三个不同粒度的阶段，并利用大型语言模型进行特定阶段的指导，以逐步引导生成。我们还整理了一个以动物为中心的视频数据集，称为OpenAnimal，以促进和基准化视频概念转移研究。大量实验表明，我们的方法在不同参考图像和场景下实现了高质量和可控的视频概念转移，在视觉保真度和可编辑性方面都超越了现有基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel architecture UniTransfer, which introduces both spatialand diffusion timestep decomposition in a progressive paradigm, achievingprecise and controllable video concept transfer. Specifically, in terms ofspatial decomposition, we decouple videos into three key components: theforeground subject, the background, and the motion flow. Building upon thisdecomposed formulation, we further introduce a dual-to-single-stream DiT-basedarchitecture for supporting fine-grained control over different components inthe videos. We also introduce a self-supervised pretraining strategy based onrandom masking to enhance the decomposed representation learning fromlarge-scale unlabeled video data. Inspired by the Chain-of-Thought reasoningparadigm, we further revisit the denoising diffusion process and propose aChain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. Wedecompose the denoising process into three stages of different granularity andleverage large language models (LLMs) for stage-specific instructions to guidethe generation progressively. We also curate an animal-centric video datasetcalled OpenAnimal to facilitate the advancement and benchmarking of research invideo concept transfer. Extensive experiments demonstrate that our methodachieves high-quality and controllable video concept transfer across diversereference images and scenes, surpassing existing baselines in both visualfidelity and editability. Web Page:https://yu-shaonian.github.io/UniTransfer-Web/</description>
      <author>example@mail.com (Guojun Lei, Rong Zhang, Chi Wang, Tianhang Liu, Hong Li, Zhiyuan Ma, Weiwei Xu)</author>
      <guid isPermaLink="false">2509.21086v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning</title>
      <link>http://arxiv.org/abs/2509.20968v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为MixGate的框架，通过功能对齐解决了多视图学习中视图间结构异质性问题，使掩模建模技术能够有效工作并提高性能。&lt;h4&gt;背景&lt;/h4&gt;多视图学习在布尔电路上具有巨大潜力，不同的基于图的表示（如AIG与XMG）提供互补的结构和语义信息，但视图之间的巨大结构异质性是有效融合的关键障碍。&lt;h4&gt;目的&lt;/h4&gt;解决视图间结构异质性导致的多视图学习融合困难问题，特别是对于自监督技术如掩模建模。&lt;h4&gt;方法&lt;/h4&gt;提出MixGate框架，基于有原则的训练课程，首先通过等价对齐损失教导模型共享的、感知功能的表示空间，然后引入多视图掩模建模目标。&lt;h4&gt;主要发现&lt;/h4&gt;功能对齐是解锁多视图自监督能力的关键先决条件；对齐优先策略将掩模建模从无效技术转变为强大的性能驱动因素。&lt;h4&gt;结论&lt;/h4&gt;对齐优先策略有效解决了视图间结构异质性问题，使掩模建模能够有效工作并提高性能。&lt;h4&gt;翻译&lt;/h4&gt;布尔电路上的多视图学习具有巨大潜力，因为不同的基于图的表示提供了互补的结构和语义信息。然而，视图之间的巨大结构异质性，如与-反相图（AIG）与异或-多数图（XMG）之间的异质性，是有效融合的关键障碍，特别是对于掩模建模等自监督技术。简单地应用这些方法会失败，因为跨视图上下文被视为噪声。我们的关键见解是，功能对齐是解锁多视图自监督能力的关键先决条件。我们引入了MixGate，这是一个基于有原则的训练课程的框架，首先通过等价对齐损失教导模型一个共享的、感知功能的表示空间。然后我们才引入多视图掩模建模目标，现在可以利用对齐的视图作为丰富、互补的信号。包括关键消融研究在内的广泛实验证明，我们的对齐优先策略将掩模建模从无效技术转变为强大的性能驱动因素。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiview learning on Boolean circuits holds immense promise, as differentgraph-based representations offer complementary structural and semanticinformation. However, the vast structural heterogeneity between views, such asan And-Inverter Graph (AIG) versus an XOR-Majority Graph (XMG), poses acritical barrier to effective fusion, especially for self-supervised techniqueslike masked modeling. Naively applying such methods fails, as the cross-viewcontext is perceived as noise. Our key insight is that functional alignment isa necessary precondition to unlock the power of multiview self-supervision. Weintroduce MixGate, a framework built on a principled training curriculum thatfirst teaches the model a shared, function-aware representation space via anEquivalence Alignment Loss. Only then do we introduce a multiview maskedmodeling objective, which can now leverage the aligned views as a rich,complementary signal. Extensive experiments, including a crucial ablationstudy, demonstrate that our alignment-first strategy transforms masked modelingfrom an ineffective technique into a powerful performance driver.</description>
      <author>example@mail.com (Zhengyuan Shi, Jingxin Wang, Wentao Jiang, Chengyu Ma, Ziyang Zheng, Zhufei Chu, Weikang Qian, Qiang Xu)</author>
      <guid isPermaLink="false">2509.20968v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy</title>
      <link>http://arxiv.org/abs/2509.20952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Flow matching作为一种强大的生成模型框架，在低噪声情况下存在根本性不稳定问题。作者首次分析了这一现象（称为低噪声病理现象），并提出了Local Contrastive Flow (LCF)混合训练协议来解决此问题，提高了收敛速度并稳定了表示质量。&lt;h4&gt;背景&lt;/h4&gt;Flow matching最近已成为扩散模型的一个强大替代方案，为生成建模和表示学习提供了连续时间公式。&lt;h4&gt;目的&lt;/h4&gt;解决Flow matching在低噪声regime下的不稳定性问题，提高其收敛速度和表示质量，以充分发挥Flow matching在生成和表示学习方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了Local Contrastive Flow (LCF)，这是一种混合训练协议，在小噪声水平下用对比特征对齐替代直接速度回归，同时在中等和高噪声水平下保留标准Flow matching。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在低噪声情况下，输入的微小扰动会导致速度目标的大变化，导致学习问题的条件数发散。2. 这种不良条件不仅减缓了优化过程，还迫使编码器将其有限的Jacobian容量重新分配到噪声方向，从而降低了语义表示的质量。3. 作者首次对这种现象（称为低噪声病理现象）进行了理论分析，建立了它与Flow matching目标结构的内在联系。&lt;h4&gt;结论&lt;/h4&gt;解决低噪声病理现象对于充分发挥Flow matching在生成和表示学习方面的潜力至关重要。提出的LCF方法不仅提高了收敛速度，还稳定了表示质量。&lt;h4&gt;翻译&lt;/h4&gt;Flow matching最近已成为扩散模型的一个强大替代方案，为生成建模和表示学习提供了连续时间公式。然而，我们表明该框架在低噪声regime下存在根本不稳定性。当噪声水平接近零时，输入的任意小扰动会导致速度目标的大变化，导致学习问题的条件数发散。这种不良条件不仅减缓了优化过程，还迫使编码器将其有限的Jacobian容量重新分配到噪声方向，从而降低了语义表示的质量。我们首次对这种现象进行了理论分析，我们称之为低噪声病理现象，建立了它与Flow matching目标结构的内在联系。基于这些见解，我们提出了Local Contrastive Flow (LCF)，这是一种混合训练协议，在小噪声水平下用对比特征对齐替代直接速度回归，同时在中等和高噪声水平下保留标准Flow matching。从经验上看，LCF不仅提高了收敛速度，还稳定了表示质量。我们的发现强调了解决低噪声病理现象对于充分发挥Flow matching在生成和表示学习方面潜力的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flow matching has recently emerged as a powerful alternative to diffusionmodels, providing a continuous-time formulation for generative modeling andrepresentation learning. Yet, we show that this framework suffers from afundamental instability in the low-noise regime. As noise levels approach zero,arbitrarily small perturbations in the input can induce large variations in thevelocity target, causing the condition number of the learning problem todiverge. This ill-conditioning not only slows optimization but also forces theencoder to reallocate its limited Jacobian capacity toward noise directions,thereby degrading semantic representations. We provide the first theoreticalanalysis of this phenomenon, which we term the low-noise pathology,establishing its intrinsic link to the structure of the flow matchingobjective. Building on these insights, we propose Local Contrastive Flow (LCF),a hybrid training protocol that replaces direct velocity regression withcontrastive feature alignment at small noise levels, while retaining standardflow matching at moderate and high noise. Empirically, LCF not only improvesconvergence speed but also stabilizes representation quality. Our findingshighlight the critical importance of addressing low-noise pathologies to unlockthe full potential of flow matching for both generation and representationlearning.</description>
      <author>example@mail.com (Weili Zeng, Yichao Yan)</author>
      <guid isPermaLink="false">2509.20952v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Latent Twins</title>
      <link>http://arxiv.org/abs/2509.20615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 pages, 22 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Latent Twins，一个统一数学框架，为基础方程在潜在空间中创建隐藏的替代模型，将经典建模、反演、模型简化和算子近似统一为单一原则的特殊情况。&lt;h4&gt;背景&lt;/h4&gt;过去十年科学机器学习转变了分析和预测复杂系统的数学和计算框架，从反问题到数值PDEs、动力系统和模型简化，但这些进展往往并行发展，表示学习和算法求解方法作为独立管道演进。&lt;h4&gt;目的&lt;/h4&gt;创建一个统一数学框架，为基础方程在潜在空间中创建隐藏的替代模型，就像数字双胞胎镜像物理系统一样，Latent Twins镜像数学系统。&lt;h4&gt;方法&lt;/h4&gt;建立Latent Twins对ODEs和PDEs的基本近似特性，并在三个设置中展示：规范ODEs捕获多样化动力学状态；使用浅水方程的PDE基准测试；真实数据地势再分析数据集的重建和预测。&lt;h4&gt;主要发现&lt;/h4&gt;Latent Twins为解算子提供紧凑、可解释的替代模型，可在单次评估中跨越任意时间间隔，同时保持与科学管道的兼容性。&lt;h4&gt;结论&lt;/h4&gt;该框架提供可扩展、有理论基础的替代模型，跨越学科连接数据驱动的表示学习和经典科学建模。&lt;h4&gt;翻译&lt;/h4&gt;在过去的十年中，科学机器学习已经转变了分析和预测复杂系统的数学和计算框架的发展。从反问题到数值PDEs、动力系统和模型简化，这些进展已经推动了模拟能力的边界。然而，它们常常并行发展，表示学习和算法求解方法在很大程度上作为独立的管道演进。通过Latent Twins，我们提出了一个统一数学框架，为基础方程在潜在空间中创建隐藏的替代模型。而数字双胞胎在数字世界中镜像物理系统，Latent Twins在由算子控制的潜在空间中镜像数学系统。通过这种视角，经典建模、反演、模型简化和算子近似都作为单一原则的特殊情况出现。我们建立了Latent Twins对ODEs和PDEs的基本近似特性，并在三个代表性设置中展示了该框架：(i) 规范ODEs，捕获多样化的动力学状态；(ii) 使用浅水方程的PDE基准测试，将Latent Twin模拟与DeepONet对比，并将预测与4D-Var基线对比；(iii) 具有挑战性的真实数据地势再分析数据集，从稀疏、嘈杂的观测中重建和预测。Latent Twins为解算子提供了紧凑、可解释的替代模型，可以在单次评估中跨越任意时间间隔，同时保持与科学管道的兼容性，如数据同化、控制和不确定性量化。展望未来，该框架提供了可扩展的、有理论基础的替代模型，跨越学科连接数据驱动的表示学习和经典科学建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past decade, scientific machine learning has transformed thedevelopment of mathematical and computational frameworks for analyzing,modeling, and predicting complex systems. From inverse problems to numericalPDEs, dynamical systems, and model reduction, these advances have pushed theboundaries of what can be simulated. Yet they have often progressed inparallel, with representation learning and algorithmic solution methodsevolving largely as separate pipelines. With \emph{Latent Twins}, we propose aunifying mathematical framework that creates a hidden surrogate in latent spacefor the underlying equations. Whereas digital twins mirror physical systems inthe digital world, Latent Twins mirror mathematical systems in a learned latentspace governed by operators. Through this lens, classical modeling, inversion,model reduction, and operator approximation all emerge as special cases of asingle principle. We establish the fundamental approximation properties ofLatent Twins for both ODEs and PDEs and demonstrate the framework across threerepresentative settings: (i) canonical ODEs, capturing diverse dynamicalregimes; (ii) a PDE benchmark using the shallow-water equations, contrastingLatent Twin simulations with DeepONet and forecasts with a 4D-Var baseline; and(iii) a challenging real-data geopotential reanalysis dataset, reconstructingand forecasting from sparse, noisy observations. Latent Twins provide acompact, interpretable surrogate for solution operators that evaluate acrossarbitrary time gaps in a single-shot, while remaining compatible withscientific pipelines such as assimilation, control, and uncertaintyquantification. Looking forward, this framework offers scalable,theory-grounded surrogates that bridge data-driven representation learning andclassical scientific modeling across disciplines.</description>
      <author>example@mail.com (Matthias Chung, Deepanshu Verma, Max Collins, Amit N. Subrahmanya, Varuni Katti Sastry, Vishwas Rao)</author>
      <guid isPermaLink="false">2509.20615v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations</title>
      <link>http://arxiv.org/abs/2509.20567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to International Conference on Big Data 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SwasthLLM是一个统一的、零样本的、跨语言的和多任务学习的医疗诊断框架，能够在英语、印地语和孟加拉语等多种语言环境中有效工作，无需针对特定语言进行微调。&lt;h4&gt;背景&lt;/h4&gt;在多语言医疗环境中，由于低资源语言的标注医疗数据稀缺以及不同人群间的语言变异性，从临床文本中自动进行疾病诊断仍然是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在多种语言（英语、印地语和孟加拉语）上有效工作的医疗诊断框架，无需针对特定语言进行微调，特别是在低资源语言环境下也能表现良好。&lt;h4&gt;方法&lt;/h4&gt;SwasthLLM核心使用多语言XLM-RoBERTa编码器，并增强了一个语言感知注意力机制和疾病分类头；引入Siamese对比学习模块对齐不同语言的语义表示；使用翻译一致性模块和对比投影头强化语言不变表示学习；采用多任务学习策略联合优化多个目标；使用MAML使模型能够快速适应新语言或任务；采用分阶段训练流程强调稳健的表示对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在监督设置下，SwasthLLM实现了97.22%的测试准确率和97.17%的F1分数；在零样本场景下，在印地语医疗文本上达到92.78%的准确率，在孟加拉语医疗文本上达到73.33%的准确率；表明在低资源背景下具有强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SwasthLLM是一个有效的跨语言医疗诊断框架，能够在低资源语言环境下实现高性能诊断，无需针对特定语言进行微调。&lt;h4&gt;翻译&lt;/h4&gt;在多语言医疗环境中，由于低资源语言的标注医疗数据稀缺以及不同人群间的语言变异性，从临床文本中自动进行疾病诊断仍然是一个具有挑战性的任务。本文提出了SwasthLLM，一个统一的、零样本的、跨语言的和多任务学习的医疗诊断框架，能够在英语、印地语和孟加拉语上有效工作，无需针对特定语言进行微调。SwasthLLM核心使用多语言XLM-RoBERTa编码器，并增强了一个语言感知注意力机制和疾病分类头，使模型能够提取与医学相关的信息，无论语言结构如何。为了对齐不同语言的语义表示，引入了一个Siamese对比学习模块，确保不同语言中的等效医学文本产生相似的嵌入。此外，翻译一致性模块和对比投影头强化了语言不变的表示学习。SwasthLLM使用多任务学习策略进行训练，联合优化疾病分类、翻译对齐和对比学习目标。此外，我们采用模型无关元学习（MAML）使模型具备快速适应未见语言或任务的能力，只需最少的数据。我们的分阶段训练流程在任务特定微调前强调稳健的表示对齐。广泛的评估显示，SwasthLLM在监督设置下实现了高诊断性能，测试准确率为97.22%，F1分数为97.17%。关键的是，在零样本场景下，它在印地语医疗文本上达到92.78%的准确率，在孟加拉语医疗文本上达到73.33%的准确率，展示了在低资源环境下的强大泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In multilingual healthcare environments, automatic disease diagnosis fromclinical text remains a challenging task due to the scarcity of annotatedmedical data in low-resource languages and the linguistic variability acrosspopulations. This paper proposes SwasthLLM, a unified, zero-shot,cross-lingual, and multi-task learning framework for medical diagnosis thatoperates effectively across English, Hindi, and Bengali without requiringlanguage-specific fine-tuning. At its core, SwasthLLM leverages themultilingual XLM-RoBERTa encoder augmented with a language-aware attentionmechanism and a disease classification head, enabling the model to extractmedically relevant information regardless of the language structure. To alignsemantic representations across languages, a Siamese contrastive learningmodule is introduced, ensuring that equivalent medical texts in differentlanguages produce similar embeddings. Further, a translation consistency moduleand a contrastive projection head reinforce language-invariant representationlearning. SwasthLLM is trained using a multi-task learning strategy, jointlyoptimizing disease classification, translation alignment, and contrastivelearning objectives. Additionally, we employ Model-Agnostic Meta-Learning(MAML) to equip the model with rapid adaptation capabilities for unseenlanguages or tasks with minimal data. Our phased training pipeline emphasizesrobust representation alignment before task-specific fine-tuning. Extensiveevaluation shows that SwasthLLM achieves high diagnostic performance, with atest accuracy of 97.22% and an F1-score of 97.17% in supervised settings.Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and73.33% accuracy on Bengali medical text, demonstrating strong generalization inlow-resource contexts.</description>
      <author>example@mail.com (Ayan Sar, Pranav Singh Puri, Sumit Aich, Tanupriya Choudhury, Abhijit Kumar)</author>
      <guid isPermaLink="false">2509.20567v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules</title>
      <link>http://arxiv.org/abs/2509.20501v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为DARTVAE的规则引导多模态聚类框架，将领域特定约束直接整合到表示学习过程中，通过在损失函数中强制执行规则一致性和违规惩罚，实现了比传统聚类方法更有意义和可解释的聚类结果。&lt;h4&gt;背景&lt;/h4&gt;传统的聚类技术通常仅依赖于输入数据中的相似性，限制了它们捕捉许多领域中至关重要的结构或语义约束的能力。&lt;h4&gt;目的&lt;/h4&gt;引入DARTVAE框架，将领域特定约束直接整合到表示学习过程中，以实现更有意义和可解释的聚类结果。&lt;h4&gt;方法&lt;/h4&gt;DARTVAE扩展了VAE架构，将显式规则、语义表示和数据驱动特征嵌入到统一的潜在空间中，并通过损失函数中的规则一致性和违规惩罚来强制执行约束合规性。规则由LLMs生成，结构化为知识图谱，并通过结合重建、KL散度、一致性和违规惩罚的损失函数来执行。&lt;h4&gt;主要发现&lt;/h4&gt;在飞机和汽车数据集上的实验表明，规则引导的聚类产生了更具操作意义和可解释的聚类结果，例如隔离UAVs、统一隐形飞机或将SUV与轿车分开，同时改进了传统聚类指标。然而，该框架面临挑战：LLM生成的规则可能产生幻觉或冲突，过多的规则有过度拟合的风险，并且扩展到复杂领域会增加计算和一致性难度。&lt;h4&gt;结论&lt;/h4&gt;通过将规则编码与学习表示相结合，DARTVAE实现了比纯数据驱动模型更有意义和一致的聚类结果，突显了约束引导多模态聚类在复杂、知识密集型环境中的实用性。&lt;h4&gt;翻译&lt;/h4&gt;传统的聚类技术通常仅依赖于输入数据中的相似性，限制了它们捕捉许多领域中至关重要的结构或语义约束的能力。我们引入了领域感知规则触发变分自编码器（DARTVAE），这是一种规则引导的多模态聚类框架，将领域特定约束直接整合到表示学习过程中。DARTVAE通过将显式规则、语义表示和数据驱动特征嵌入到统一的潜在空间中来扩展VAE架构，同时通过损失函数中的规则一致性和违规惩罚来强制执行约束合规性。与仅依赖视觉相似性或将规则作为后处理过滤器应用的常规聚类方法不同，DARTVAE将规则视为第一类学习信号。规则由LLMs生成，结构化为知识图谱，并通过结合重建、KL散度、一致性和违规惩罚的损失函数来执行。在飞机和汽车数据集上的实验表明，规则引导的聚类产生了更具操作意义和可解释的聚类结果，例如隔离UAVs、统一隐形飞机或将SUV与轿车分开，同时改进了传统聚类指标。然而，该框架面临挑战：LLM生成的规则可能产生幻觉或冲突，过多的规则有过度拟合的风险，并且扩展到复杂领域会增加计算和一致性难度。通过将规则编码与学习表示相结合，DARTVAE实现了比纯数据驱动模型更有意义和一致的聚类结果，突显了约束引导多模态聚类在复杂、知识密集型环境中的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional clustering techniques often rely solely on similarity in theinput data, limiting their ability to capture structural or semanticconstraints that are critical in many domains. We introduce the Domain AwareRule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodalclustering framework that incorporates domain specific constraints directlyinto the representation learning process. DARTVAE extends the VAE architectureby embedding explicit rules, semantic representations, and data driven featuresinto a unified latent space, while enforcing constraint compliance through ruleconsistency and violation penalties in the loss function. Unlike conventionalclustering methods that rely only on visual similarity or apply rules as posthoc filters, DARTVAE treats rules as first class learning signals. The rulesare generated by LLMs, structured into knowledge graphs, and enforced through aloss function combining reconstruction, KL divergence, consistency, andviolation penalties. Experiments on aircraft and automotive datasetsdemonstrate that rule guided clustering produces more operationally meaningfuland interpretable clusters for example, isolating UAVs, unifying stealthaircraft, or separating SUVs from sedans while improving traditional clusteringmetrics. However, the framework faces challenges: LLM generated rules mayhallucinate or conflict, excessive rules risk overfitting, and scaling tocomplex domains increases computational and consistency difficulties. Bycombining rule encodings with learned representations, DARTVAE achieves moremeaningful and consistent clustering outcomes than purely data driven models,highlighting the utility of constraint guided multimodal clustering forcomplex, knowledge intensive settings.</description>
      <author>example@mail.com (Kishor Datta Gupta, Mohd Ariful Haque, Marufa Kamal, Ahmed Rafi Hasan, Md. Mahfuzur Rahman, Roy George)</author>
      <guid isPermaLink="false">2509.20501v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations</title>
      <link>http://arxiv.org/abs/2509.20478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一目标条件强化学习中对比表示和时间距离框架的新方法，通过拟度量表示空间结构和适当约束学习后续表示，实现最优目标达到。&lt;h4&gt;背景&lt;/h4&gt;目标条件强化学习中，两种有效的表示结构框架是对比表示(学习'后续特征')和时间距离(将表示空间距离与状态到目标的过渡时间关联)。&lt;h4&gt;目的&lt;/h4&gt;统一对比表示和时间距离两种框架，利用拟度量表示空间结构学习最优目标达到的后续表示，即使在次优数据和随机环境中也能工作。&lt;h4&gt;方法&lt;/h4&gt;使用拟度量表示空间(三角形不等式)结构，添加适当约束来学习后续表示，实现最优目标达到。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够利用拟度量距离参数化学习最优目标达到距离，即使在次优数据和随机环境中也能工作；保留了蒙特卡罗对比RL方法的稳定性和长期能力，同时获得了拟度量网络参数化的自由拼接能力。&lt;h4&gt;结论&lt;/h4&gt;在离线GCRL基准测试中，该方法在拼接任务和嘈杂高维环境中均优于现有方法，结合了两种框架的优势。&lt;h4&gt;翻译&lt;/h4&gt;目标条件强化学习(GCRL)的方法通常使用学习到的状态表示来提取目标达到策略。两种表示结构框架产生了特别有效的GCRL算法：(1)对比表示，其中方法使用对比目标学习'后续特征'，对未来结果进行推理；(2)时间距离，将表示空间中的(拟度量)距离与从状态到目标的过渡时间联系起来。我们提出了一种统一这两种框架的方法，使用拟度量表示空间的结构(三角形不等式)和适当的额外约束来学习后续表示，实现最优目标达到。与过去的工作不同，我们的方法能够利用拟度量距离参数化来学习最优目标达到距离，即使在次优数据和随机环境中也是如此。这使我们两全其美：我们保留了蒙特卡罗对比RL方法的稳定性和长期能力，同时获得了拟度量网络参数化的自由拼接能力。在现有的离线GCRL基准测试中，我们的表示学习目标提高了拼接任务的性能，而基于对比学习的方法在这些任务上表现不佳，并且在嘈杂、高维环境中也提高了性能，而基于拟度量网络的方法在这些环境中表现不佳。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Approaches for goal-conditioned reinforcement learning (GCRL) often uselearned state representations to extract goal-reaching policies. Two frameworksfor representation structure have yielded particularly effective GCRLalgorithms: (1) *contrastive representations*, in which methods learn"successor features" with a contrastive objective that performs inference overfuture outcomes, and (2) *temporal distances*, which link the (quasimetric)distance in representation space to the transit time from states to goals. Wepropose an approach that unifies these two frameworks, using the structure of aquasimetric representation space (triangle inequality) with the rightadditional constraints to learn successor representations that enable optimalgoal-reaching. Unlike past work, our approach is able to exploit a**quasimetric** distance parameterization to learn **optimal** goal-reachingdistances, even with **suboptimal** data and in **stochastic** environments.This gives us the best of both worlds: we retain the stability and long-horizoncapabilities of Monte Carlo contrastive RL methods, while getting the freestitching capabilities of quasimetric network parameterizations. On existingoffline GCRL benchmarks, our representation learning objective improvesperformance on stitching tasks where methods based on contrastive learningstruggle, and on noisy, high-dimensional environments where methods based onquasimetric networks struggle.</description>
      <author>example@mail.com (Vivek Myers, Bill Chunyuan Zheng, Benjamin Eysenbach, Sergey Levine)</author>
      <guid isPermaLink="false">2509.20478v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation</title>
      <link>http://arxiv.org/abs/2509.20269v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合反向传播和预测编码的混合训练方法，用于实现高效的设备域适应，使深度神经网络能够在动态环境中持续适应数据分布变化。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在动态、真实世界环境中部署时，依靠单一静态模型往往不够。传感器漂移或光照变化导致的输入数据分布变化需要模型持续适应。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合训练方法，通过结合反向传播和预测编码的优势，实现高效的设备域适应。&lt;h4&gt;方法&lt;/h4&gt;首先使用反向传播离线训练深度神经网络以获得高初始性能，然后使用预测编码进行在线适应，使模型能够恢复因输入数据分布变化而损失的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法利用反向传播在初始表示学习中的稳健性和预测编码在持续学习中的计算效率，特别适合资源受限的边缘设备或未来的神经形态加速器。在MNIST和CIFAR-10数据集上的实验结果表明，这种混合策略能够实现有效的适应，同时减少计算开销。&lt;h4&gt;结论&lt;/h4&gt;这种混合策略为在动态环境中保持模型性能提供了有希望的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着深度神经网络在动态、真实世界环境中的日益部署，依靠单一静态模型通常是不够的。传感器漂移或光照变化引起的输入数据分布变化需要模型持续适应。在本文中，我们提出了一种混合训练方法，通过结合反向传播和预测编码的优势，实现高效的设备域适应。该方法首先使用反向传播离线训练深度神经网络以获得高初始性能。随后，采用预测编码进行在线适应，使模型能够恢复因输入数据分布变化而损失的准确性。这种方法利用了反向传播在初始表示学习中的稳健性和预测编码在持续学习中的计算效率，使其特别适合资源受限的边缘设备或未来的神经形态加速器。在MNIST和CIFAR-10数据集上的实验结果表明，这种混合策略能够实现有效的适应，同时减少计算开销，为在动态环境中保持模型性能提供了有希望的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As deep neural networks are increasingly deployed in dynamic, real-worldenvironments, relying on a single static model is often insufficient. Changesin input data distributions caused by sensor drift or lighting variationsnecessitate continual model adaptation. In this paper, we propose a hybridtraining methodology that enables efficient on-device domain adaptation bycombining the strengths of Backpropagation and Predictive Coding. The methodbegins with a deep neural network trained offline using Backpropagation toachieve high initial performance. Subsequently, Predictive Coding is employedfor online adaptation, allowing the model to recover accuracy lost due toshifts in the input data distribution. This approach leverages the robustnessof Backpropagation for initial representation learning and the computationalefficiency of Predictive Coding for continual learning, making it particularlywell-suited for resource-constrained edge devices or future neuromorphicaccelerators. Experimental results on the MNIST and CIFAR-10 datasetsdemonstrate that this hybrid strategy enables effective adaptation with areduced computational overhead, offering a promising solution for maintainingmodel performance in dynamic environments.</description>
      <author>example@mail.com (Matteo Cardoni, Sam Leroux)</author>
      <guid isPermaLink="false">2509.20269v2</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>MolPILE - large-scale, diverse dataset for molecular representation learning</title>
      <link>http://arxiv.org/abs/2509.18353v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了MolPILE，一个包含2.22亿化合物的预训练数据集，旨在解决分子表示学习中的数据集限制问题，通过在MolPILE上重新训练模型可以提高泛化性能。&lt;h4&gt;背景&lt;/h4&gt;预训练数据集的大小、多样性和质量对基础模型的泛化能力至关重要。在化学信息学领域，现有小分子数据集的限制阻碍了分子表示学习的有效性。&lt;h4&gt;目的&lt;/h4&gt;解决现有分子数据集的局限性，创建一个类似于ImageNet规模的标准化学数据集，以提高分子表示学习的效果。&lt;h4&gt;方法&lt;/h4&gt;构建了MolPILE数据集，这是一个包含2.22亿化合物的集合，通过自动化筛选流程从6个大型数据库构建而成。对当前预训练数据集进行全面分析，并重新训练现有模型以评估性能提升。&lt;h4&gt;主要发现&lt;/h4&gt;当前预训练数据集在训练机器学习模型方面存在显著不足；在MolPILE上重新训练现有模型可以改善泛化性能。&lt;h4&gt;结论&lt;/h4&gt;MolPILE为模型训练提供了标准化资源，解决了分子化学领域对类ImageNet数据集的迫切需求。&lt;h4&gt;翻译&lt;/h4&gt;预训练数据集的大小、多样性和质量决定了基础模型的泛化能力。尽管它们在化学信息学中的重要性日益增加，但由于现有小分子数据集的限制，分子表示学习的有效性受到了阻碍。为了解决这一差距，我们提出了MolPILE，这是一个大规模、多样化且经过严格筛选的2.22亿化合物集合，使用自动化筛选流程从6个大型数据库构建而成。我们对当前的预训练数据集进行了全面分析，指出了它们在训练机器学习模型方面的显著不足，并展示了如何在MolPILE上重新训练现有模型以提高泛化性能。这项工作为模型训练提供了标准化资源，解决了分子化学领域对类ImageNet数据集的迫切需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The size, diversity, and quality of pretraining datasets critically determinethe generalization ability of foundation models. Despite their growingimportance in chemoinformatics, the effectiveness of molecular representationlearning has been hindered by limitations in existing small molecule datasets.To address this gap, we present MolPILE, large-scale, diverse, and rigorouslycurated collection of 222 million compounds, constructed from 6 large-scaledatabases using an automated curation pipeline. We present a comprehensiveanalysis of current pretraining datasets, highlighting considerableshortcomings for training ML models, and demonstrate how retraining existingmodels on MolPILE yields improvements in generalization performance. This workprovides a standardized resource for model training, addressing the pressingneed for an ImageNet-like dataset in molecular chemistry.</description>
      <author>example@mail.com (Jakub Adamczyk, Jakub Poziemski, Franciszek Job, Mateusz Król, Maciej Makowski)</author>
      <guid isPermaLink="false">2509.18353v2</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>LAVA: Explainability for Unsupervised Latent Embeddings</title>
      <link>http://arxiv.org/abs/2509.21149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, including references and appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为局部感知变量关联(LAVA)的新方法，用于解释无监督黑盒模型的潜在空间组织及其与输入特征的关系。&lt;h4&gt;背景&lt;/h4&gt;无监督黑盒模型可以推动科学发现但难以解释。现有监督学习的可解释性方法关注输入特征与预测目标的关系，而无监督对应方法应关注输入特征与学习到的潜在空间结构的联系。现有无监督学习解释方法要么过于细致，要么过于简化，难以提供有意义的信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自动根据潜在邻近性关联相似样本的策略，以解释局部嵌入组织及其与输入特征的关系，特别适用于不产生映射函数的流形学习方法。&lt;h4&gt;方法&lt;/h4&gt;引入LAVA(局部感知变量关联)，一种后期模型无关方法。该方法将潜在空间表示为一系列用原始特征间相关性描述的局部性(邻域)，然后揭示整个潜在空间中重复出现的相关性模式。&lt;h4&gt;主要发现&lt;/h4&gt;基于MNIST和单细胞肾脏数据集的UMAP嵌入测试表明，LVA能够捕获相关的特征关联，并在潜在空间的看似遥远区域之间共享具有视觉和生物学相关性的局部模式。&lt;h4&gt;结论&lt;/h4&gt;LVA提供了一种有效的方法来解释无监督黑盒模型的潜在空间结构，通过揭示特征关联和局部模式，帮助理解无监督学习的结果，促进科学发现。&lt;h4&gt;翻译&lt;/h4&gt;无监督黑盒模型可以成为科学发现的驱动力，但仍然难以解释。关键在于，发现依赖于理解模型输出，这通常是多维潜在嵌入，而非明确定义的目标。虽然监督学习的可解释性通常试图揭示输入特征如何用于预测目标，但其无监督对应方法应该将输入特征与学习到的潜在空间结构联系起来。为无监督学习调整的监督模型可解释性方法要么提供单个样本解释，要么提供整个数据集的摘要解释。然而，如果没有自动策略根据样本的潜在邻近性将相似样本相互关联，解释要么过于细致，要么过于简化而缺乏意义。这对于不产生映射函数的流形学习方法尤其相关，因为我们只有其嵌入的相对空间组织。我们引入了局部感知变量关联(LAVA)，一种后期模型无关方法，旨在通过其与输入特征的关系解释局部嵌入组织。为此，LAVA将潜在空间表示为一系列用原始特征间相关性描述的局部性(邻域)，然后揭示整个潜在空间中重复出现的相关性模式。基于MNIST和单细胞肾脏数据集的UMAP嵌入，我们表明LVA捕获了相关的特征关联，并在潜在空间的看似遥远区域之间共享具有视觉和生物学相关性的局部模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised black-box models can be drivers of scientific discovery, butremain difficult to interpret. Crucially, discovery hinges on understanding themodel output, which is often a multi-dimensional latent embedding rather than awell-defined target. While explainability for supervised learning usually seeksto uncover how input features are used to predict a target, its unsupervisedcounterpart should relate input features to the structure of the learned latentspace. Adaptations of supervised model explainability for unsupervised learningprovide either single-sample or dataset-wide summary explanations. However,without automated strategies of relating similar samples to one another guidedby their latent proximity, explanations remain either too fine-grained or tooreductive to be meaningful. This is especially relevant for manifold learningmethods that produce no mapping function, leaving us only with the relativespatial organization of their embeddings. We introduce Locality-Aware VariableAssociations (LAVA), a post-hoc model-agnostic method designed to explain localembedding organization through its relationship with the input features. Toachieve this, LAVA represents the latent space as a series of localities(neighborhoods) described in terms of correlations between the originalfeatures, and then reveals reoccurring patterns of correlations across theentire latent space. Based on UMAP embeddings of MNIST and a single-cell kidneydataset, we show that LAVA captures relevant feature associations, withvisually and biologically relevant local patterns shared among seeminglydistant regions of the latent spaces.</description>
      <author>example@mail.com (Ivan Stresec, Joana P. Gonçalves)</author>
      <guid isPermaLink="false">2509.21149v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI</title>
      <link>http://arxiv.org/abs/2509.21068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了量子软件工程(QSE)领域的挑战，通过机器学习方法对Stack Overflow上的量子相关讨论进行分类，实现了95%的高准确率，并使用SHAP增强了模型可解释性。&lt;h4&gt;背景&lt;/h4&gt;量子软件工程(QSE)是科技公司实践的研究领域。量子开发者在优化量子计算和QSE概念时面临挑战，他们使用Stack Overflow讨论问题并使用专门的量子标签标记帖子，但这些标签通常指向技术方面而非开发者帖子。&lt;h4&gt;目的&lt;/h4&gt;通过分类问题来识别量子软件工程中的常见挑战，并开发一种基于机器学习的分类方法来准确对这些讨论进行分类。&lt;h4&gt;方法&lt;/h4&gt;从Q&amp;A平台提取2829个量子相关问题，分析帖子识别六种主要挑战类型(工具、理论、学习、概念、错误和API使用)，使用内容分析和扎根理论构建数据集，通过ChatGPT验证人工注释，微调transformer算法(BERT、DistilBERT和RoBERTa)进行分类，并与D&amp;ML分类器比较，最后使用SHAP进行模型可解释性分析。&lt;h4&gt;主要发现&lt;/h4&gt;BERT DistilBERT实现了95%的平均准确率，比D&amp;ML分类器(FNN、CNN、LSTM)分别高出6%、9%和11%。Transformer方法在处理实际讨论(无需数据增强)时表现优异，SHAP分析揭示了语言特征如何驱动预测，提高了分类透明度。&lt;h4&gt;结论&lt;/h4&gt;这些发现可以帮助量子供应商和论坛更好地组织讨论，提高可访问性和可读性。然而，需要与实际开发者和供应商进行实证评估研究。&lt;h4&gt;翻译&lt;/h4&gt;量子软件工程(QSE)是科技公司实践的研究领域。量子开发者在优化量子计算和QSE概念时面临挑战。他们使用Stack Overflow讨论问题并使用专门的量子标签标记帖子，这些标签通常指向技术方面而非开发者帖子。基于量子概念对问题进行分类可以帮助识别频繁的QSE挑战。我们进行了研究将问题分类为各种挑战。我们使用量子相关标签从Q&amp;A平台提取了2829个问题。分析了帖子以识别常见挑战并开发了一种新的扎根理论。挑战包括工具、理论、学习、概念、错误和API使用。通过内容分析和扎根理论，使用常见挑战对讨论进行注释，构建了一个真实数据集。ChatGPT验证了人工注释并解决了分歧。微调的transformer算法，包括BERT、DistilBERT和RoBERTa，将讨论分类为常见挑战。我们使用BERT DistilBERT实现了95%的平均准确率，而微调的深度和机器学习分类器，包括前馈神经网络、卷积神经网络和长短期记忆网络，分别实现了89%、86%和84%的准确率。基于Transformer的方法比基于D&amp;ML的方法提高了6%的准确率，通过处理实际讨论，即无需数据增强。我们应用了SHAP进行模型可解释性分析，揭示了语言特征如何驱动预测，提高了分类的透明度。这些发现可以帮助量子供应商和论坛更好地组织讨论，提高可访问性和可读性。然而，需要与实际开发者和供应商进行实证评估研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum Software Engineering (QSE) is a research area practiced by techfirms. Quantum developers face challenges in optimizing quantum computing andQSE concepts. They use Stack Overflow (SO) to discuss challenges and labelposts with specialized quantum tags, which often refer to technical aspectsrather than developer posts. Categorizing questions based on quantum conceptscan help identify frequent QSE challenges. We conducted studies to classifyquestions into various challenges. We extracted 2829 questions from Q&amp;Aplatforms using quantum-related tags. Posts were analyzed to identify frequentchallenges and develop a novel grounded theory. Challenges include Tooling,Theoretical, Learning, Conceptual, Errors, and API Usage. Through contentanalysis and grounded theory, discussions were annotated with common challengesto develop a ground truth dataset. ChatGPT validated human annotations andresolved disagreements. Fine-tuned transformer algorithms, including BERT,DistilBERT, and RoBERTa, classified discussions into common challenges. Weachieved an average accuracy of 95% with BERT DistilBERT, compared tofine-tuned Deep and Machine Learning (D&amp;ML) classifiers, including FeedforwardNeural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-TermMemory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,respectively. The Transformer-based approach outperforms the D&amp;ML-basedapproach with a 6\% increase in accuracy by processing actual discussions,i.e., without data augmentation. We applied SHAP (SHapley AdditiveexPlanations) for model interpretability, revealing how linguistic featuresdrive predictions and enhancing transparency in classification. These findingscan help quantum vendors and forums better organize discussions for improvedaccess and readability. However,empirical evaluation studies with actualdevelopers and vendors are needed.</description>
      <author>example@mail.com (Nek Dil Khan, Javed Ali Khan, Mobashir Husain, Muhammad Sohail Khan, Arif Ali Khan, Muhammad Azeem Akbar, Shahid Hussain)</author>
      <guid isPermaLink="false">2509.21068v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2509.20946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于视觉的自动化系统，用于激光功率计传感器涂层的缺陷检测和分类，能够识别热损伤和划痕等影响激光能量测量精度的缺陷。&lt;h4&gt;背景&lt;/h4&gt;激光功率计传感器涂层缺陷（如热损伤和划痕）会影响医疗和工业应用中激光能量测量的准确性，需要有效的检测方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化的视觉系统，用于检测和分类激光功率计传感器涂层的缺陷，无需大量标记的缺陷数据集。&lt;h4&gt;方法&lt;/h4&gt;使用无监督异常检测框架，仅在良好传感器图像上训练；包含三个关键组件：(1)基于拉普拉斯边缘检测和K-means聚类的预处理管道；(2)通过StyleGAN2进行合成数据增强；(3)基于UFlow的神经网络架构进行多尺度特征提取和异常图生成。&lt;h4&gt;主要发现&lt;/h4&gt;在366个真实传感器图像上的实验评估显示，对缺陷样本的准确率为93.8%，对良好样本的准确率为89.3%，图像级AUROC为0.957，像素级AUROC为0.961。&lt;h4&gt;结论&lt;/h4&gt;该系统通过自动化质量控制提供潜在年度成本节约，在设备实现中每张图像处理时间为0.5秒。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于视觉的自动化系统，用于激光功率计传感器涂层的缺陷检测和分类。我们的方法解决了识别涂层缺陷的关键挑战，如热损伤和划痕，这些缺陷会影响医疗和工业应用中激光能量测量的准确性。该系统采用无监督异常检测框架，仅在良好传感器图像上训练，学习正常涂层分布模式，能够检测已知和新型的缺陷类型，而无需大量标记的缺陷数据集。我们的方法包含三个关键组件：(1)使用拉普拉斯边缘检测和K-means聚类的稳健预处理管道，分割感兴趣区域；(2)通过StyleGAN2进行合成数据增强；(3)基于UFlow的神经网络架构，用于多尺度特征提取和异常图生成。在366个真实传感器图像上的实验评估显示，对缺陷样本的准确率为93.8%，对良好样本的准确率为89.3%，图像级AUROC为0.957，像素级AUROC为0.961。该系统通过自动化质量控制提供潜在年度成本节约，在设备实现中每张图像处理时间为0.5秒。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an automated vision-based system for defect detection andclassification of laser power meter sensor coatings. Our approach addresses thecritical challenge of identifying coating defects such as thermal damage andscratches that can compromise laser energy measurement accuracy in medical andindustrial applications. The system employs an unsupervised anomaly detectionframework that trains exclusively on ``good'' sensor images to learn normalcoating distribution patterns, enabling detection of both known and noveldefect types without requiring extensive labeled defect datasets. Ourmethodology consists of three key components: (1) a robust preprocessingpipeline using Laplacian edge detection and K-means clustering to segment thearea of interest, (2) synthetic data augmentation via StyleGAN2, and (3) aUFlow-based neural network architecture for multi-scale feature extraction andanomaly map generation. Experimental evaluation on 366 real sensor imagesdemonstrates $93.8\%$ accuracy on defective samples and $89.3\%$ accuracy ongood samples, with image-level AUROC of 0.957 and pixel-level AUROC of 0.961.The system provides potential annual cost savings through automated qualitycontrol and processing times of 0.5 seconds per image in on-deviceimplementation.</description>
      <author>example@mail.com (Dongqi Zheng, Wenjin Fu, Guangzong Chen)</author>
      <guid isPermaLink="false">2509.20946v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>A Deep Transfer Learning-Based Low-overhead Beam Prediction in Vehicle Communications</title>
      <link>http://arxiv.org/abs/2509.20659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合微调和域适应的迁移学习方法，用于解决源域和目标域数据分布差异大时的波束预测问题。&lt;h4&gt;背景&lt;/h4&gt;现有的基于迁移学习的波束预测方法主要依赖于简单的微调，当目标域和源域之间的数据分布存在显著差异时，简单的微调限制了模型在目标域的性能。&lt;h4&gt;目的&lt;/h4&gt;解决源域和目标域数据分布差异大时模型性能受限的问题，提高模型在目标域的表现。&lt;h4&gt;方法&lt;/h4&gt;将域分类器集成到预训练模型的微调过程中，通过对抗训练使模型提取域不变特征，从而增强模型在目标域的性能。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果表明，所提出的迁移学习方法在目标域中比纯微调方法实现了更好的可达速率性能，并且接近在目标域上从头开始训练时的性能。&lt;h4&gt;结论&lt;/h4&gt;结合微调和域适应的迁移学习方法可以有效解决源域和目标域数据分布差异大的问题，显著提高模型在目标域的性能。&lt;h4&gt;翻译&lt;/h4&gt;现有的基于迁移学习的波束预测方法主要依赖于简单的微调。当目标域和源域之间的数据分布存在显著差异时，简单的微调限制了模型在目标域的性能。为了解决这个问题，我们提出了一种结合微调和域适应的基于迁移学习的波束预测方法。我们将域分类器集成到预训练模型的微调过程中，模型通过域分类器进行对抗训练，提取域不变特征，从而增强模型在目标域的性能。模拟结果表明，所提出的基于迁移学习的波束预测方法在目标域中比纯微调方法实现了更好的可达速率性能，并且接近在目标域上从头开始训练时的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing transfer learning-based beam prediction approaches primarily rely onsimple fine-tuning. When there is a significant difference in data distributionbetween the target domain and the source domain, simple fine-tuning limits themodel's performance in the target domain. To tackle this problem, we propose atransfer learning-based beam prediction method that combines fine-tuning withdomain adaptation. We integrate a domain classifier into fine-tuning thepre-trained model. The model extracts domain-invariant features in adversarialtraining with domain classifier, which can enhance model performance in thetarget domain. Simulation results demonstrate that the proposed transferlearning-based beam prediction method achieves better achievable rateperformance than the pure fine-tuning method in the target domain, and close tothose when the training is done from scratch on the target domain.</description>
      <author>example@mail.com (Zhiqiang Xiao, Yuwen Cao, Mondher Bouazizi, Tomoaki Ohtsuki, Shahid Mumtaz)</author>
      <guid isPermaLink="false">2509.20659v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework</title>
      <link>http://arxiv.org/abs/2509.20705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了BIM2RDT框架，这是一个代理人工智能系统，将静态建筑信息模型转变为动态、机器人就绪的数字孪生，优先考虑施工安全，通过整合多种数据流弥合BIM与实时现场条件的差距。&lt;h4&gt;背景&lt;/h4&gt;建筑行业采用赛博物理系统和工地智能连接设计模型、实时现场传感和自主现场操作，可显著增强数字管理，但现有BIM数据与实时现场条件之间存在差距。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架将静态BIM模型转变为动态、机器人就绪的数字孪生，优先考虑执行过程中的安全，并通过整合多种数据流弥合BIM数据与实时现场条件之间的差距。&lt;h4&gt;方法&lt;/h4&gt;引入语义重力ICP点云配准算法利用大型语言模型推理；创建反馈循环使机器人收集的数据更新数字孪生并优化任务路径；采用YOLOE目标检测和Shi-Tomasi角点检测识别和跟踪建筑元素；集成实时手臂振动监测并使用IFC标准映射安全事件。&lt;h4&gt;主要发现&lt;/h4&gt;SG-ICP算法在遮挡特征场景中的对齐优于标准ICP，实现64.3%至88.3%的均方根误差降低；HAV集成在超过暴露限制时触发警告，提高了对ISO 5349-1标准的合规性。&lt;h4&gt;结论&lt;/h4&gt;BIM2RDT框架成功将静态BIM转变为动态数字孪生，通过整合多种数据流和先进算法提高了建筑工地的安全性和效率，SG-ICP和HAV监测显著提升了系统准确性和安全性。&lt;h4&gt;翻译&lt;/h4&gt;采用连接设计模型、实时现场传感和自主现场操作的赛博物理系统和工地智能可以显著增强建筑行业的数字管理。本文介绍了BIM2RDT框架，这是一个代理人工智能系统，旨在将静态建筑信息建模转变为动态的、机器人就绪的数字孪生，优先考虑执行过程中的安全。该框架通过整合三个关键数据流来弥合现有BIM数据和实时现场条件之间的差距：来自BIM模型的几何和语义信息、来自物联网传感器网络的活动数据以及机器人在现场遍历期间收集的视觉空间数据。该方法引入了语义重力ICP点云配准算法，利用大型语言模型推理。与传统方法相比，SG-ICP利用大型语言模型基于BIM语义推断特定对象、合理的方向先验，通过避免收敛到局部最小值提高对齐精度。这创建了一个反馈循环，机器人收集的数据更新数字孪生，进而优化任务路径。该框架采用YOLOE目标检测和Shi-Tomasi角点检测来识别和跟踪建筑元素，同时使用BIM几何作为先验地图。该框架还集成了实时手臂振动监测，使用IFC标准将传感器检测到的安全事件映射到数字孪生，以便干预。实验证明SG-ICP优于标准ICP，在遮挡特征场景中的对齐实现了64.3%至88.3%的均方根误差降低，确保了合理的方向。HAV集成在超过暴露限制时触发警告，提高了对ISO 5349-1标准的合规性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将静态的建筑信息模型转变为动态的、机器人可用的工地数字孪生，并在施工过程中优先考虑安全。这个问题很重要，因为建筑行业正经历数字化转型，但传统BIM模型无法反映工地的实时变化；同时随着机器人技术在工地应用增加，需要将现有BIM数据与实时工地条件结合，确保机器人安全高效地导航和工作；此外施工安全监测和干预也是建筑行业的关键挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过整合三个关键数据流来设计方法：BIM模型的几何和语义信息、物联网传感器网络的实时活动数据、四足机器人在工地穿行期间收集的视觉-空间数据。作者借鉴了现有工作如传统ICP算法用于点云配准，但进行了创新，开发了语义-重力ICP算法，利用大型语言模型的推理能力来推断物体特定的、物理上合理的方向先验知识，避免收敛到局部最小值。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建智能反馈循环，机器人收集的数据更新数字孪生，数字孪生又为后续任务优化路径。整体流程包括：使用YOLOE开放词汇目标检测器和Shi-Tomasi角点检测识别和跟踪建筑元素；将BIM几何作为先验地图；应用语义-重力ICP算法对齐机器人捕获的点云与BIM模型；集成实时手臂振动监测将安全事件映射到数字孪生；使用智能代理AI引擎处理数据并做出决策。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：开发了语义-重力ICP算法利用大型语言模型推理；将静态BIM转变为动态机器人就绪的数字孪生；整合三种关键数据流（BIM信息、物联网数据、机器人数据）；集成实时手臂振动监测；使用智能代理AI作为系统认知引擎。相比之前工作，这个框架同时利用现有BIM数据、实时传感器流和自主决策，而之前工作通常只关注单一方面的技术。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一个名为BIM2RDT的智能AI框架，通过整合BIM数据、机器人收集的传感器信息和大型语言模型的推理能力，将静态建筑模型转变为动态的、安全优先的工地数字孪生，显著提高了建筑机器人导航的准确性和施工安全性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The adoption of cyber-physical systems and jobsite intelligence that connectsdesign models, real-time site sensing, and autonomous field operations candramatically enhance digital management in the construction industry. Thispaper introduces BIM2RDT (Building Information Models to Robot-Ready SiteDigital Twins), an agentic artificial intelligence (AI) framework designed totransform static Building Information Modeling (BIM) into dynamic, robot-readydigital twins (DTs) that prioritize safety during execution. The frameworkbridges the gap between pre-existing BIM data and real-time site conditions byintegrating three key data streams: geometric and semantic information from BIMmodels, activity data from IoT sensor networks, and visual-spatial datacollected by robots during site traversal. The methodology introducesSemantic-Gravity ICP (SG-ICP), a point cloud registration algorithm thatleverages large language model (LLM) reasoning. Unlike traditional methods,SG-ICP utilizes an LLM to infer object-specific, plausible orientation priorsbased on BIM semantics, improving alignment accuracy by avoiding convergence onlocal minima. This creates a feedback loop where robot-collected data updatesthe DT, which in turn optimizes paths for missions. The framework employs YOLOEobject detection and Shi-Tomasi corner detection to identify and trackconstruction elements while using BIM geometry as a priori maps. The frameworkalso integrates real-time Hand-Arm Vibration (HAV) monitoring, mappingsensor-detected safety events to the digital twin using IFC standards forintervention. Experiments demonstrate SG-ICP's superiority over standard ICP,achieving RMSE reductions of 64.3%--88.3% in alignment across scenarios withoccluded features, ensuring plausible orientations. HAV integration triggerswarnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.</description>
      <author>example@mail.com (Reza Akhavian, Mani Amani, Johannes Mootz, Robert Ashe, Behrad Beheshti)</author>
      <guid isPermaLink="false">2509.20705v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>OmniPlantSeg: Species Agnostic 3D Point Cloud Organ Segmentation for High-Resolution Plant Phenotyping Across Modalities</title>
      <link>http://arxiv.org/abs/2509.21038v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为KDSS的简单有效算法，用于植物器官点云分割，能够保留全分辨率数据并适用于不同植物种类和传感器模态。&lt;h4&gt;背景&lt;/h4&gt;现有植物器官点云分割解决方案针对特定问题设计，专注于特定植物种类或特定传感器模态，且通常需要大量预处理和下采样以满足硬件或神经网络输入要求。&lt;h4&gt;目的&lt;/h4&gt;开发一种独立于传感器数据和植物种类的点云下采样算法，无需下采样输入数据，从而实现对全分辨率点云的分割。&lt;h4&gt;方法&lt;/h4&gt;提出KDSS算法并将其与当前最先进的分割模型结合，在不同模态（如摄影测量、激光三角测量和LiDAR）和各种植物种类上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;KDSS与先进分割模型结合在不同传感器模态和植物种类上取得了令人满意的结果，成为一种轻量级保留分辨率的替代方案。&lt;h4&gt;结论&lt;/h4&gt;KDSS是密集预处理和下采样方法的有效替代方案，可用于植物器官分割，不受植物种类和传感器模态限制。&lt;h4&gt;翻译&lt;/h4&gt;准确的植物器官点云分割对于3D植物表型分析至关重要。现有解决方案是针对特定问题设计的，专注于特定植物种类或用于数据获取的指定传感器模态。此外，通常使用大量的预处理和对植物点云进行下采样，以满足硬件或神经网络输入大小的要求。我们提出了一种简单而有效的KDSS算法，用于生物点云的下采样，该算法独立于传感器数据和植物种类。这种方法的主要优点是我们不需要下采样输入数据，从而能够对全分辨率点云进行分割。将KD-SS与当前最先进的分割模型结合，在不同的模态（如摄影测量、激光三角测量和LiDAR）和各种植物种类上评估，显示出令人满意的结果。我们提出KD-SS作为轻量级的保留分辨率的替代方案，用于密集的预处理和下采样方法进行植物器官分割，无论使用的种类和传感器模态如何。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决植物器官点云分割的通用性和分辨率问题。现有方法通常针对特定植物种类或传感器设计，且需要降采样点云以适应神经网络输入，这会导致信息丢失和细节丢失。这一问题在植物表型研究中至关重要，因为高分辨率扫描能捕捉微小特征和细节，而准确的器官分割是提取有意义信息的基础，对农业数字化转型和精细作物管理具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性，特别是球形子采样(Spherical Sub Sampling)方法因固定半径导致点云密度分布失真的问题。作者借鉴了Scholz等人的球形子采样概念，但引入KD-tree算法进行改进，创建了KD-SS算法。这种方法优化了运行时间，同时保持原始点云的所有点。作者还使用了DGCNN作为分割模型，这是点云分割领域的先进方法，通过结合KD-SS和DGCNN，实现了无需降采样的全分辨率分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是KD-SS子采样算法，它可以将任意大小的点云分割成适合神经网络输入大小的子样本，同时保持原始分辨率。整体流程包括：1)初始化点云数据和每个子样本的点数；2)创建KD-tree；3)随机选择中心点并选取其N个最近邻居作为子样本；4)保存子样本和特征向量；5)从原始数据中移除已采样点；6)重复直到剩余点不足；7)使用DGCNN对子样本进行分割；8)合并所有分割后的子样本，得到全分辨率的带标签点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出KD-SS子采样算法，能在保持全分辨率的同时将点云分割成适合神经网络的大小；2)开发OmniPlantSeg流程，实现跨植物种类和传感器模态的通用分割；3)展示方法在不同数据集上的有效性。相比之前工作，不同之处在于：不需要降采样输入数据，保留了全分辨率信息；具有更好的通用性，能处理不同植物和传感器数据；在保持高分辨率的同时实现了有竞争力的分割性能；共享权重模型表明单个模型可泛化到不同物种。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了KD-SS子采样算法和OmniPlantSeg流程，实现了无需降采样的全分辨率植物器官分割，能够在不同植物种类和传感器模式下提供准确分割结果，为高分辨率植物表型分析提供了新方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate point cloud segmentation for plant organs is crucial for 3D plantphenotyping. Existing solutions are designed problem-specific with a focus oncertain plant species or specified sensor-modalities for data acquisition.Furthermore, it is common to use extensive pre-processing and down-sample theplant point clouds to meet hardware or neural network input size requirements.We propose a simple, yet effective algorithm KDSS for sub-sampling ofbiological point clouds that is agnostic to sensor data and plant species. Themain benefit of this approach is that we do not need to down-sample our inputdata and thus, enable segmentation of the full-resolution point cloud.Combining KD-SS with current state-of-the-art segmentation models showssatisfying results evaluated on different modalities such as photogrammetry,laser triangulation and LiDAR for various plant species. We propose KD-SS aslightweight resolution-retaining alternative to intensive pre-processing anddown-sampling methods for plant organ segmentation regardless of used speciesand sensor modality.</description>
      <author>example@mail.com (Andreas Gilson, Lukas Meyer, Oliver Scholz, Ute Schmid)</author>
      <guid isPermaLink="false">2509.21038v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense</title>
      <link>http://arxiv.org/abs/2509.21129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EvoMail是一种自进化的认知智能体框架，用于稳健检测垃圾邮件和网络钓鱼攻击，通过构建异构电子邮件图和对抗性自进化循环实现高性能检测。&lt;h4&gt;背景&lt;/h4&gt;现代垃圾邮件和网络钓鱼攻击已超越关键词黑名单或简单启发式方法，攻击者创建多模态活动结合文本、混淆URL、伪造头和恶意附件，并在几天内调整策略以绕过过滤器。传统垃圾邮件检测系统依赖静态规则或单模态模型，难以整合异构信号或持续适应，导致性能迅速下降。&lt;h4&gt;目的&lt;/h4&gt;提出EvoMail，一个用于稳健检测垃圾邮件和网络钓鱼的自进化认知智能体框架。&lt;h4&gt;方法&lt;/h4&gt;EvoMail构建统一的异构电子邮件图，融合文本内容、元数据和嵌入资源；使用由大型语言模型增强的认知图神经网络进行上下文感知推理；通过红队生成新的规避策略，蓝队从失败中学习并将经验压缩到内存模块中实现对抗性自进化循环。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实世界数据集和合成对抗变体上的实验表明，EvoMail在检测准确性、对 evolving 垃圾邮件策略的适应性以及推理轨迹的可解释性方面 consistently 超越最先进的基线。&lt;h4&gt;结论&lt;/h4&gt;EvoMail作为抵御下一代垃圾邮件和网络钓鱼威胁的弹性且可解释的防御框架具有显著潜力。&lt;h4&gt;翻译&lt;/h4&gt;现代垃圾邮件和网络钓鱼攻击已经远远超出了关键词黑名单或简单启发式方法。攻击者现在创建多模态活动，将自然语言文本与混淆URL、伪造头和恶意附件相结合，在几天内调整策略以绕过过滤器。依赖静态规则或单模态模型的传统垃圾邮件检测系统难以整合异构信号或持续适应，导致性能迅速下降。我们提出了EvoMail，一种用于稳健检测垃圾邮件和网络钓鱼的自进化认知智能体框架。EvoMail首先构建统一的异构电子邮件图，融合文本内容、元数据（头、发件人、域名）和嵌入资源（URL、附件）。由大型语言模型增强的认知图神经网络在这些源之间执行上下文感知推理，以识别协调的垃圾邮件活动。最重要的是，EvoMail参与对抗性自进化循环：红队智能体生成新的规避策略，如字符混淆或AI生成的钓鱼文本，而蓝队检测器从失败中学习，将经验压缩到内存模块中，并在未来推理中重用这些经验。在真实世界数据集（Enron-Spam、Ling-Spam、SpamAssassin和TREC）和合成对抗变体上的广泛实验表明，EvoMail在检测准确性、对 evolving 垃圾邮件策略的适应性以及推理轨迹的可解释性方面 consistently 超越最先进的基线。这些结果突显了EvoMail作为抵御下一代垃圾邮件和网络钓鱼威胁的弹性且可解释的防御框架的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern email spam and phishing attacks have evolved far beyond keywordblacklists or simple heuristics. Adversaries now craft multi-modal campaignsthat combine natural-language text with obfuscated URLs, forged headers, andmalicious attachments, adapting their strategies within days to bypass filters.Traditional spam detection systems, which rely on static rules orsingle-modality models, struggle to integrate heterogeneous signals or tocontinuously adapt, leading to rapid performance degradation.  We propose EvoMail, a self-evolving cognitive agent framework for robustdetection of spam and phishing. EvoMail first constructs a unifiedheterogeneous email graph that fuses textual content, metadata (headers,senders, domains), and embedded resources (URLs, attachments). A CognitiveGraph Neural Network enhanced by a Large Language Model (LLM) performscontext-aware reasoning across these sources to identify coordinated spamcampaigns. Most critically, EvoMail engages in an adversarial self-evolutionloop: a ''red-team'' agent generates novel evasion tactics -- such as characterobfuscation or AI-generated phishing text -- while the ''blue-team'' detectorlearns from failures, compresses experiences into a memory module, and reusesthem for future reasoning.  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,SpamAssassin, and TREC) and synthetic adversarial variants demonstrate thatEvoMail consistently outperforms state-of-the-art baselines in detectionaccuracy, adaptability to evolving spam tactics, and interpretability ofreasoning traces. These results highlight EvoMail's potential as a resilientand explainable defense framework against next-generation spam and phishingthreats.</description>
      <author>example@mail.com (Wei Huang, De-Tian Chu, Lin-Yuan Bai, Wei Kang, Hai-Tao Zhang, Bo Li, Zhi-Mo Han, Jing Ge, Hai-Feng Lin)</author>
      <guid isPermaLink="false">2509.21129v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization</title>
      <link>http://arxiv.org/abs/2509.21097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了GraphUniverse框架，用于生成整个图家族以评估图模型在归纳泛化方面的大规模性能，发现强大的同构性能是归纳泛化的不良预测因素，且对分布转移的鲁棒性对模型架构和初始图区域都很敏感。&lt;h4&gt;背景&lt;/h4&gt;图学习中的一个基本挑战是理解模型如何推广到新的、未见过的图上。现有方法局限于单图、同构设置，即模型在同一图结构上训练和测试，缺乏对归纳泛化的系统评估。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架来生成整个图家族，实现归纳泛化的大规模系统评估，并研究不同图模型架构的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出GraphUniverse框架，生成具有持久语义社区的图，确保概念一致性同时允许对同配性和度分布等结构特性进行细粒度控制。对多种架构（包括GNN、图变换器和拓扑架构）进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;1. 强大的同构性能是归纳泛化的不良预测因素；2. 对分布转移的鲁棒性不仅对模型架构选择敏感，而且对初始图区域（如高同配性与低同配性）也很敏感。&lt;h4&gt;结论&lt;/h4&gt;GraphUniverse的灵活性和可扩展性可以促进鲁棒和真正可推广架构的开发，包括下一代图基础模型，为图学习领域提供了新的评估基准和工具。&lt;h4&gt;翻译&lt;/h4&gt;图学习中的一个基本挑战是理解模型如何推广到新的、未见过的图上。虽然合成基准为分析提供了受控环境，但现有方法局限于单图、同构设置，即模型在同一图结构上训练和测试。为解决这一差距，我们引入了GraphUniverse，这是一个用于生成整个图家族的框架，以实现归纳泛化的大规模系统评估。我们的核心创新是生成具有持久语义社区的图，确保概念一致性的同时允许对同配性和度分布等结构特性进行细粒度控制。这使得关键但未被充分探索的鲁棒性测试成为可能，例如在受控分布转移下的性能。对各种架构（从GNN到图变换器和拓扑架构）的基准测试表明，强大的同构性能是归纳泛化的不良预测因素。此外，我们发现对分布转移的鲁棒性不仅对模型架构选择高度敏感，而且对初始图区域（如高同配性与低同配性）也很敏感。除了基准测试外，GraphUniverse的灵活性和可扩展性可以促进鲁棒和真正可推广架构的开发，包括下一代图基础模型。交互式演示可在https://graphuniverse.streamlit.app获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A fundamental challenge in graph learning is understanding how modelsgeneralize to new, unseen graphs. While synthetic benchmarks offer controlledsettings for analysis, existing approaches are confined to single-graph,transductive settings where models train and test on the same graph structure.Addressing this gap, we introduce GraphUniverse, a framework for generatingentire families of graphs to enable the first systematic evaluation ofinductive generalization at scale. Our core innovation is the generation ofgraphs with persistent semantic communities, ensuring conceptual consistencywhile allowing fine-grained control over structural properties like homophilyand degree distributions. This enables crucial but underexplored robustnesstests, such as performance under controlled distribution shifts. Benchmarking awide range of architectures -- from GNNs to graph transformers and topologicalarchitectures -- reveals that strong transductive performance is a poorpredictor of inductive generalization. Furthermore, we find that robustness todistribution shift is highly sensitive not only to model architecture choicebut also to the initial graph regime (e.g., high vs. low homophily). Beyondbenchmarking, GraphUniverse's flexibility and scalability can facilitate thedevelopment of robust and truly generalizable architectures -- includingnext-generation graph foundation models. An interactive demo is available athttps://graphuniverse.streamlit.app.</description>
      <author>example@mail.com (Louis Van Langendonck, Guillermo Bernárdez, Nina Miolane, Pere Barlet-Ros)</author>
      <guid isPermaLink="false">2509.21097v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Feature Augmentation of GNNs for ILPs: Local Uniqueness Suffices</title>
      <link>http://arxiv.org/abs/2509.21000v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于d跳唯一性着色的局部唯一标识符方案及其衍生的ColorGNN和ColorUID方法，解决了整数线性规划中图神经网络表达能力和泛化能力之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;整数线性规划是现实世界优化的核心但 notoriously 难以解决。学习优化已成为有前景的范式，图神经网络作为标准骨干。然而，标准匿名GNNs在表达ILPs方面存在局限性，而添加全局唯一标识符的常见方法会引入虚假相关性，严重损害泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决标准匿名GNNs在ILPs中的表达能力限制，以及全局唯一标识符引入的虚假相关性和泛化能力下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于d跳唯一性着色的局部唯一标识符方案，确保标识符仅在节点的d跳邻域内唯一。基于此方案，引入ColorGNN通过颜色条件嵌入整合颜色信息，以及ColorUID这一轻量级特征级变体。&lt;h4&gt;主要发现&lt;/h4&gt;对于d层网络，Local-UIDs能达到全局唯一标识符的表达能力，同时提供更强的泛化能力。实验表明该方法在三个ILP基准测试上取得显著提升，在线性规划数据集上表现出良好的OOD泛化能力，并与最先进方法配对时改进了图级任务。&lt;h4&gt;结论&lt;/h4&gt;所提出的Local-UID方案及其衍生的ColorGNN和ColorUID方法有效解决了ILPs中表达能力和泛化能力之间的权衡问题，在多个任务和基准测试中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;整数线性规划是现实世界优化的核心，但 notoriously 难以解决。学习优化已成为有前景的范式，图神经网络作为标准骨干。然而，标准匿名GNNs在表达ILPs方面存在局限性，而添加全局唯一标识符的常见方法会引入虚假相关性，严重损害泛化能力。为了解决这种权衡，我们提出基于d跳唯一性着色的局部唯一标识符方案，确保标识符仅在节点的d跳邻域内唯一。基于此方案，我们引入ColorGNN通过颜色条件嵌入整合颜色信息，以及ColorUID这一轻量级特征级变体。我们证明对于d层网络，Local-UIDs能达到全局唯一标识符的表达能力，同时提供更强的泛化能力。大量实验表明，我们的方法在三个ILP基准测试上取得显著提升，在线性规划数据集上表现出良好的OOD泛化能力，并与最先进方法配对时进一步改进了图级任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integer Linear Programs (ILPs) are central to real-world optimizations butnotoriously difficult to solve. Learning to Optimize (L2O) has emerged as apromising paradigm, with Graph Neural Networks (GNNs) serving as the standardbackbone. However, standard anonymous GNNs are limited in expressiveness forILPs, and the common enhancement of augmenting nodes with globally uniqueidentifiers (UIDs) typically introduces spurious correlations that severelyharm generalization. To address this tradeoff, we propose a parsimoniousLocal-UID scheme based on d-hop uniqueness coloring, which ensures identifiersare unique only within each node's d-hop neighborhood. Building on this scheme,we introduce ColorGNN, which incorporates color information viacolor-conditioned embeddings, and ColorUID, a lightweight feature-levelvariant. We prove that for d-layer networks, Local-UIDs achieve the expressivepower of Global-UIDs while offering stronger generalization. Extensiveexperiments show that our approach (i) yields substantial gains on three ILPbenchmarks, (ii) exhibits strong OOD generalization on linear programmingdatasets, and (iii) further improves a general graph-level task when pairedwith a state-of-the-art method.</description>
      <author>example@mail.com (Qingyu Han, Qian Li, Linxin Yang, Qian Chen, Qingjiang Shi, Ruoyu Sun)</author>
      <guid isPermaLink="false">2509.21000v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>FracAug: Fractional Augmentation boost Graph-level Anomaly Detection under Limited Supervision</title>
      <link>http://arxiv.org/abs/2509.20978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了FracAug，一个创新的插件式图增强框架，用于解决图级别异常检测中的高标注成本和数据不平衡问题。该框架通过生成语义一致的图变体和相互验证的伪标记来增强GNN性能，在多种GNN和数据集上展现出显著的通用性和有效性。&lt;h4&gt;背景&lt;/h4&gt;图级别异常检测在药物发现等多个领域至关重要，但高标注成本和数据集不平衡问题限制了图神经网络(GNNs)的性能表现。&lt;h4&gt;目的&lt;/h4&gt;解决图级别异常检测中的高标注成本和数据集不平衡问题，提高GNN的性能。&lt;h4&gt;方法&lt;/h4&gt;提出FracAug框架，通过在给定图中学习语义，利用加权距离感知边际损失引导合成部分变体，捕获多尺度拓扑生成多样且保持语义的图，不受数据不平衡影响。然后利用原始图和增强图的预测为未标记数据分配伪标记，迭代扩大训练集。作为模型无关模块，可与各种GNN兼容。&lt;h4&gt;主要发现&lt;/h4&gt;在12个真实数据集上对14种GNN进行的实验显示了一致的性能提升，平均AUROC提高5.72%，AUPRC提高7.23%，F1分数提高4.18%。&lt;h4&gt;结论&lt;/h4&gt;FracAug是一个有效的插件式增强框架，能够显著提高图级别异常检测任务的性能，具有广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;图级别异常检测(GAD)在药物发现等多个领域至关重要，然而高昂的标注成本和数据集不平衡问题限制了图神经网络(GNNs)的性能。为解决这些问题，我们提出了FracAug，一个创新的插件式增强框架，通过生成语义一致的图变体和相互验证的伪标记来增强GNN。与之前的启发式方法不同，FracAug在给定图中学习语义，并由一种新颖的加权距离感知边际损失引导，合成部分变体，捕获多尺度拓扑以生成多样且保持语义的图，不受数据不平衡影响。然后，FracAug利用原始图和增强图的预测来为未标记数据分配伪标记，迭代扩大训练集。作为一个与各种GNN兼容的模型无关模块，FracAug展示了显著的通用性和有效性：在12个真实数据集上对14种GNN进行的实验显示了一致的性能提升，平均AUROC、AUPRC和F1分数分别提高了5.72%、7.23%和4.18%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-level anomaly detection (GAD) is critical in diverse domains such asdrug discovery, yet high labeling costs and dataset imbalance hamper theperformance of Graph Neural Networks (GNNs). To address these issues, wepropose FracAug, an innovative plug-in augmentation framework that enhancesGNNs by generating semantically consistent graph variants and pseudo-labelingwith mutual verification. Unlike previous heuristic methods, FracAug learnssemantics within given graphs and synthesizes fractional variants, guided by anovel weighted distance-aware margin loss. This captures multi-scale topologyto generate diverse, semantic-preserving graphs unaffected by data imbalance.Then, FracAug utilizes predictions from both original and augmented graphs topseudo-label unlabeled data, iteratively expanding the training set. As amodel-agnostic module compatible with various GNNs, FracAug demonstratesremarkable universality and efficacy: experiments across 14 GNNs on 12real-world datasets show consistent gains, boosting average AUROC, AUPRC, andF1-score by up to 5.72%, 7.23%, and 4.18%, respectively.</description>
      <author>example@mail.com (Xiangyu Dong, Xingyi Zhang, Sibo Wang)</author>
      <guid isPermaLink="false">2509.20978v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery</title>
      <link>http://arxiv.org/abs/2509.20941v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Medical Image Analysis. Under review. 49 pages, 9  figures. An interactive version of the summary tables is available at  osf.io/fruq8&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述系统地研究了场景图在手术领域的应用发展，揭示了研究中的数据鸿沟问题，并展示了从基础图神经网络到专业基础模型的演进过程。场景图已成为手术分析和工作流识别等任务的关键技术，同时也在可控手术模拟等生成任务中展现出潜力。&lt;h4&gt;背景&lt;/h4&gt;场景图提供结构化的关系表示，对于解码复杂、动态的手术环境至关重要。随着医疗技术的发展，手术场景的智能化分析需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;系统地绘制手术中场景图研究的演变图景，包括其应用、方法学进展和未来方向。&lt;h4&gt;方法&lt;/h4&gt;采用PRISMA-ScR指导的范围综述方法，对手术场景图研究进行系统性分析。&lt;h4&gt;主要发现&lt;/h4&gt;1. 研究领域呈现快速增长，但存在关键的数据鸿沟：内部视图研究(如三元组识别)几乎仅使用真实世界2D视频，而外部视图4D建模则严重依赖模拟数据。2. 方法学上，该领域已从基础图神经网络发展到专业基础模型，这些模型在手术环境中显著优于通用大型视觉语言模型。3. 场景图已成为手术工作流识别和自动化安全监控等分析任务，以及可控手术模拟等生成任务的核心技术。4. 数据标注和实时实施方面仍存在挑战，但新兴技术正在积极解决这些问题。&lt;h4&gt;结论&lt;/h4&gt;手术场景图正在发展为一种重要的语义桥梁，使新一代智能系统能够提高手术安全性、效率和培训质量。&lt;h4&gt;翻译&lt;/h4&gt;场景图提供了解码复杂、动态手术环境所必需的结构化关系表示。这篇基于PRISMA-ScR指导的范围综述系统地绘制了手术中场景图研究的演变图景，记录了其应用、方法学进展和未来方向。分析揭示了快速增长，但发现了一个关键的数据鸿沟：内部视图研究(如三元组识别)几乎 exclusively 使用真实世界2D视频，而外部视图4D建模则严重依赖模拟数据，暴露了一个关键的转化研究差距。在方法学上，该领域已从基础图神经网络发展到专业基础模型，这些模型在手术环境中现在显著优于通用大型视觉语言模型。这一进展已使场景图成为分析(如工作流识别和自动化安全监控)和生成任务(如可控手术模拟)的核心技术。尽管数据标注和实时实施方面的挑战仍然存在，但它们正通过新兴技术得到积极解决。手术场景图正在发展为一种必要的语义桥梁，使新一代智能系统能够提高手术安全性、效率和培训质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是系统性地梳理和总结'场景图'在手术领域的研究现状、应用和发展趋势。这个问题在现实中非常重要，因为手术环境极其复杂动态，手术中多个器械同时操作，关键解剖结构距操作点仅毫米级距离；现有方法常单独分析特定组件而忽略丰富关系背景；准确理解手术场景中实体间的相互作用对提高手术安全性至关重要，手术情境意识不足是导致可预防不良事件的重要因素。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用范围综述(Scoping Review)方法，设计明确的纳入和排除标准，在三个数据库系统搜索，辅以Google Scholar关键词搜索、引用搜索和专家咨询。他们使用PRISMA-ScR框架指导综述过程，构建了三维分类系统（时间、维度、视角）来组织研究。这种方法借鉴了系统综述的现有方法，但针对手术场景图这一新兴领域进行了调整和创新，确保方法透明和可重复。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是系统性地梳理手术场景图研究，通过构建全面文献地图，揭示该领域发展趋势、方法学进步和未来方向。整体流程包括：背景介绍(解释手术场景图动机和历史)；方法学框架(介绍范围综述方法)；分类系统(提出三维分类框架)；场景图构建方法(分析数据源、数据集和计算方法)；应用和演变(追踪从内部场景图到与AI融合的发展)；结果分析(回答四个研究问题)；讨论(总结差距、挑战和未来方向)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首个专注于手术场景图的系统性综述；提出三维分类框架(时间、空间、视角)；揭示'数据鸿沟'(内部视图用真实2D视频，外部视图依赖模拟数据)；追踪方法学演变(从图神经网络到专用基础模型)；识别研究热点和冷点。相比之前工作，这篇论文专注于场景图表示本身而非特定下游应用，采用更系统全面的方法梳理新兴领域，不仅总结现状还通过识别差距为未来研究提供方向，强调多模态融合和基础模型等最新趋势。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性地梳理手术场景图研究，揭示了该领域从基础图模型到专用基础模型的演变轨迹，指出了关键的数据鸿沟和研究空白，为未来手术AI的发展提供了重要指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene graphs (SGs) provide structured relational representations crucial fordecoding complex, dynamic surgical environments. This PRISMA-ScR-guided scopingreview systematically maps the evolving landscape of SG research in surgery,charting its applications, methodological advancements, and future directions.Our analysis reveals rapid growth, yet uncovers a critical 'data divide':internal-view research (e.g., triplet recognition) almost exclusively usesreal-world 2D video, while external-view 4D modeling relies heavily onsimulated data, exposing a key translational research gap. Methodologically,the field has advanced from foundational graph neural networks to specializedfoundation models that now significantly outperform generalist largevision-language models in surgical contexts. This progress has established SGsas a cornerstone technology for both analysis, such as workflow recognition andautomated safety monitoring, and generative tasks like controllable surgicalsimulation. Although challenges in data annotation and real-time implementationpersist, they are actively being addressed through emerging techniques.Surgical SGs are maturing into an essential semantic bridge, enabling a newgeneration of intelligent systems to improve surgical safety, efficiency, andtraining.</description>
      <author>example@mail.com (Angelo Henriques, Korab Hoxha, Daniel Zapp, Peter C. Issa, Nassir Navab, M. Ali Nasseri)</author>
      <guid isPermaLink="false">2509.20941v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine</title>
      <link>http://arxiv.org/abs/2509.20935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了GALAX框架，通过图过程奖励模型(GPRM)引导的强化学习，将预训练图神经网络(GNNs)整合到大语言模型(LLMs)中，实现逐步生成疾病相关子图并无需显式中间推理注释。同时引入了Target-QA基准测试，结合CRISPR识别靶点、多组学特征和生物医学图知识，支持长上下文推理。&lt;h4&gt;背景&lt;/h4&gt;在精准医疗中，定量多组学特征、拓扑背景和文本生物知识对识别疾病关键信号通路和靶点至关重要。现有方法存在局限性：数值组学忽略拓扑背景，文本中心LLMs缺乏定量基础推理，图模型未充分利用节点语义和LLMs泛化能力，限制了机制可解释性。过程奖励模型(PRM)受限于不可靠的中间评估和奖励黑客攻击，且计算成本高。&lt;h4&gt;目的&lt;/h4&gt;通过LLMs整合定量多组学信号、拓扑结构和节点注释以及文献规模文本，使用子图推理作为连接数字证据、拓扑知识和语言上下文的原则桥梁，实现可靠和可解释的靶点和通路发现。&lt;h4&gt;方法&lt;/h4&gt;提出GALAX框架，通过图过程奖励模型(GPRM)引导的强化学习，将预训练图神经网络(GNNs)整合到大语言模型(LLMs)中，逐步生成疾病相关子图并由预训练GNN迭代评估，实现过程级监督。引入Target-QA基准测试，结合CRISPR识别靶点、多组学特征和跨多种癌细胞系生物医学图知识，支持GNN预训练和长上下文推理。&lt;h4&gt;主要发现&lt;/h4&gt;GALAX框架通过子图推理有效整合数字证据、拓扑知识和语言上下文，克服现有方法局限性。Target-QA基准测试提供了可扩展、生物有据可依的框架，支持强化引导的子图推理，实现可靠和可解释的靶点和通路发现。&lt;h4&gt;结论&lt;/h4&gt;GALAX框架和Target-QA基准测试为精准医学中可靠和可解释的靶点和通路发现提供了可扩展和生物有据可依的框架，通过整合定量多组学特征、拓扑结构和文本生物知识，提升机制可解释性。&lt;h4&gt;翻译&lt;/h4&gt;在精准医疗中，定量多组学特征、拓扑背景和文本生物知识在识别疾病关键信号通路和靶点中发挥着至关重要的作用。现有的流程仅捕捉了其中的一部分——数值组学忽略了拓扑背景，以文本为中心的大语言模型缺乏定量基础推理，而仅图模型未充分利用节点语义和大语言模型的泛化能力——限制了机制可解释性。尽管过程奖励模型(PRM)旨在指导大语言模型中的推理，但它们仍然受到不可靠的中间评估和奖励黑客攻击的困扰，且计算成本高。这些差距促使我们通过大语言模型整合定量多组学信号、带有节点注释的拓扑结构和文献规模的文本，使用子图推理作为连接数字证据、拓扑知识和语言上下文的原则桥梁。因此，我们提出了GALAX，这是一个创新框架，通过图过程奖励模型(GPRM)引导的强化学习，将预训练图神经网络(GNNs)整合到大语言模型(LLMs)中，该模型以逐步方式生成疾病相关的子图，并由预训练的GNN迭代评估，实现无需显式中间推理注释的过程级监督。作为应用，我们还引入了Target-QA，这是一个基准测试，结合了CRISPR识别的靶点、多组学特征和跨多种癌细胞系的生物医学图知识，它支持GNN预训练以监督逐步图构建，并支持在文本-数字图上的长上下文推理，为精准医学中可靠和可解释的靶点和通路发现提供了可扩展和生物有据可依的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In precision medicine, quantitative multi-omic features, topological context,and textual biological knowledge play vital roles in identifyingdisease-critical signaling pathways and targets. Existing pipelines captureonly part of these-numerical omics ignore topological context, text-centricLLMs lack quantitative grounded reasoning, and graph-only models underuse nodesemantics and the generalization of LLMs-limiting mechanistic interpretability.Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, theyremain limited by unreliable intermediate evaluation, and vulnerability toreward hacking with computational cost. These gaps motivate integratingquantitative multi-omic signals, topological structure with node annotations,and literature-scale text via LLMs, using subgraph reasoning as the principlebridge linking numeric evidence, topological knowledge and language context.Therefore, we propose GALAX (Graph Augmented LAnguage model witheXplainability), an innovative framework that integrates pretrained GraphNeural Networks (GNNs) into Large Language Models (LLMs) via reinforcementguided by a Graph Process Reward Model (GPRM), which generates disease-relevantsubgraphs in a step-wise manner initiated by an LLM and iteratively evaluatedby a pretrained GNN, enabling process-level supervision without explicitintermediate reasoning annotations. As an application, we also introducedTarget-QA, a benchmark combining CRISPR-identified targets, multi-omicprofiles, and biomedical graph knowledge across diverse cancer cell lines,which enables GNN pretraining for supervising step-wise graph construction andsupports long-context reasoning over text-numeric graphs (TNGs), providing ascalable and biologically grounded framework for explainable,reinforcement-guided subgraph reasoning toward reliable and interpretabletarget and pathway discovery in precision medicine.</description>
      <author>example@mail.com (Heming Zhang, Di Huang, Wenyu Li, Michael Province, Yixin Chen, Philip Payne, Fuhai Li)</author>
      <guid isPermaLink="false">2509.20935v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Mesh Interpolation Graph Network for Dynamic and Spatially Irregular Global Weather Forecasting</title>
      <link>http://arxiv.org/abs/2509.20911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 main track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的网格插值图网络(MIGN)，用于解决全局天气预报中不规则分布和动态变化的挑战，实现了对未观测位置的泛化。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在天气预报中显示出有前景的结果，这对人类活动如农业规划和极端天气准备至关重要。然而，大多数研究集中在有限和局部区域进行训练，忽视了更广泛区域的影响，限制了有效泛化的能力。&lt;h4&gt;目的&lt;/h4&gt;研究实践中不规则分布和动态变化的全局天气预报，开发能够泛化到未观测位置的模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通用的网格插值图网络(MIGN)，包含两个关键设计：(1)使用规则网格插值网络学习空间不规则数据以对齐数据；(2)利用参数化球谐位置嵌入进一步增强空间泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在最新的观测数据集上进行的大量实验表明，MIGN显著优于现有的数据驱动模型，并且具有空间泛化能力，能够泛化到以前未见过的站点。&lt;h4&gt;结论&lt;/h4&gt;MIGN通过网格插值和球谐位置嵌入解决了全局天气预报中不规则分布和动态变化的挑战，实现了对未观测位置的泛化。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在天气预报中显示出有前景的结果，这对人类活动如农业规划和极端天气准备至关重要。然而，大多数研究集中在有限和局部区域进行训练，忽视了更广泛区域的影响，限制了它们有效泛化的能力。因此，在本文中，我们研究实践中不规则分布和动态变化的全局天气预报，需要模型能够泛化到未观测的位置。为解决这些挑战，我们提出了一种通用的网格插值图网络(MIGN)，用于建模不规则的气象站预测，包含两个关键设计：(1)使用规则网格插值网络学习空间不规则数据以对齐数据；(2)利用参数化球谐位置嵌入进一步增强空间泛化能力。在最新的观测数据集上进行的大量实验表明，MIGN显著优于现有的数据驱动模型。此外，我们证明MIGN具有空间泛化能力，能够泛化到以前未见过的站点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have shown promising results in weather forecasting,which is critical for human activity such as agriculture planning and extremeweather preparation. However, most studies focus on finite and local areas fortraining, overlooking the influence of broader areas and limiting their abilityto generalize effectively. Thus, in this work, we study global weatherforecasting that is irregularly distributed and dynamically varying inpractice, requiring the model to generalize to unobserved locations. To addresssuch challenges, we propose a general Mesh Interpolation Graph Network (MIGN)that models the irregular weather station forecasting, consisting of two keydesigns: (1) learning spatially irregular data with regular mesh interpolationnetwork to align the data; (2) leveraging parametric spherical harmonicslocation embedding to further enhance spatial generalization ability. Extensiveexperiments on an up-to-date observation dataset show that MIGN significantlyoutperforms existing data-driven models. Besides, we show that MIGN has spatialgeneralization ability, and is capable of generalizing to previous unseenstations.</description>
      <author>example@mail.com (Zinan Zheng, Yang Liu, Jia Li)</author>
      <guid isPermaLink="false">2509.20911v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>MolCluster: Integrating Graph Neural Network with Community Detection for Coarse-Grained Mapping</title>
      <link>http://arxiv.org/abs/2509.20893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages; 4 figures; 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MolCluster是一种无监督模型，结合图神经网络和社区检测算法，实现了无需标签的粗粒度表示提取，并支持可定制的分辨率，在MARTINI2数据集上表现优于传统方法和监督模型。&lt;h4&gt;背景&lt;/h4&gt;传统的粗粒度（CG）建模通过将原子组映射为代表性单元来简化分子系统，但依赖固定映射规则，限制了处理多样化化学系统的能力且需要大量人工干预。基于监督学习的CG方法虽然更自动化和适应性强，但受限于标记数据集有限且无法控制映射分辨率的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种无监督模型，克服传统CG方法和监督学习方法在处理多样化化学系统、控制映射分辨率方面的局限性，实现无需标签训练和可定制的分辨率。&lt;h4&gt;方法&lt;/h4&gt;提出MolCluster，一种结合图神经网络和社区检测算法的无监督模型，用于提取粗粒度表示。引入预定义组对损失确保目标组的保留，采用二分策略实现不同分子系统上的精确、可定制分辨率。&lt;h4&gt;主要发现&lt;/h4&gt;在MARTINI2数据集上的评估表明，MolCluster受益于其无标签预训练策略，在下游任务中表现优于传统聚类方法和监督模型。&lt;h4&gt;结论&lt;/h4&gt;MolCluster作为可定制和化学一致的CG映射的核心模型具有潜力，能够有效解决传统CG方法和监督学习方法中的局限性。&lt;h4&gt;翻译&lt;/h4&gt;粗粒度（CG）建模通过将原子组映射为代表性单元来简化分子系统。然而，传统的CG方法依赖固定的映射规则，这限制了它们处理多样化化学系统的能力，并需要大量人工干预。因此，已经提出了基于监督学习的CG方法，使映射更加自动化和适应性强。尽管如此，这些方法受限于标记数据集有限且无法控制映射分辨率，这对于多尺度建模至关重要。为了克服这些局限性，我们提出了MolCluster，这是一种无监督模型，结合了图神经网络和社区检测算法来提取CG表示。此外，预定义的组对损失确保了目标组的保留，而二分策略能够在不同分子系统上实现精确、可定制的分辨率。在下游任务方面，在MARTINI2数据集上的评估表明，MolCluster受益于其无标签预训练策略，在性能上优于传统聚类和监督模型。总体而言，这些结果突显了MolCluster作为可定制和化学一致的CG映射核心模型的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Coarse-grained (CG) modeling simplifies molecular systems by mapping groupsof atoms into representative units. However, traditional CG approaches rely onfixed mapping rules, which limit their ability to handle diverse chemicalsystems and require extensive manual intervention. Thus, supervisedlearning-based CG methods have been proposed, enabling more automated andadaptable mapping. Nevertheless, these methods suffer from limited labeleddatasets and the inability to control mapping resolution, which is essentialfor multiscale modeling. To overcome these limitations, we propose MolCluster,an unsupervised model that integrates a graph neural network and a communitydetection algorithm to extract CG representations. Additionally, a predefinedgroup pair loss ensures the preservation of target groups, and a bisectionstrategy enables precise, customizable resolution across different molecularsystems. In the case of the downstream task, evaluations on the MARTINI2dataset demonstrate that MolCluster, benefiting from its label-free pretrainingstrategy, outperforms both traditional clustering and supervised models.Overall, these results highlight the potential of MolCluster as a core modelfor customizable and chemically consistent CG mapping.</description>
      <author>example@mail.com (Zhixuan Zhong, Linbo Ma, Jian Jiang)</author>
      <guid isPermaLink="false">2509.20893v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Molecular Property Prediction with Knowledge from Large Language Models</title>
      <link>http://arxiv.org/abs/2509.20664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型框架，将大型语言模型提取的知识与预训练分子模型的结构特征相结合，以增强分子性质预测(MPP)性能。&lt;h4&gt;背景&lt;/h4&gt;分子性质预测是药物发现的关键环节。深度学习特别是图神经网络(GNNs)的发展使得从分子结构进行端到端学习成为可能，减少了对手动特征工程的依赖。然而，尽管GNNs和自监督学习方法取得了进展，但人类先验知识的整合仍然不可或缺，如最近利用大型语言模型进行知识提取的方法所示。尽管LLMs有优势，但它们存在知识空白和幻觉问题，尤其是对于研究较少的分子性质。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型框架，首次整合从LLMs提取的知识与从预训练分子模型衍生的结构特征，以增强分子性质预测(MPP)。&lt;h4&gt;方法&lt;/h4&gt;该方法提示LLMs生成领域相关知识和分子向量化的可执行代码，产生基于知识的特征，随后与结构表示融合。研究使用了三种最先进的LLMs（GPT-4o、GPT-4.1和DeepSeek-R1）进行知识提取。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验证明，集成方法优于现有方法，确认了LLMs衍生的知识与结构信息的结合为MPP提供了强大而有效的解决方案。&lt;h4&gt;结论&lt;/h4&gt;结合LLMs提取的知识和结构信息的方法在分子性质预测中表现优越，为药物发现提供了新的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;预测分子性质是药物发现的关键组成部分。深度学习的最新进展，特别是图神经网络(GNNs)，使得从分子结构进行端到端学习成为可能，减少了对手动特征工程的依赖。然而，尽管GNNs和自监督学习方法在分子性质预测(MPP)方面取得了进展，人类先验知识的整合仍然是不可或缺的，正如最近利用大型语言模型(LLMs)进行知识提取的方法所证明的那样。尽管LLMs具有优势，但它们受限于知识空白和幻觉问题，尤其是对于研究较少的分子性质。在这项工作中，我们提出了一个新型框架，首次将从LLMs提取的知识与从预训练分子模型衍生的结构特征相结合，以增强MPP。我们的方法提示LLMs生成领域相关知识和分子向量化的可执行代码，产生基于知识的特征，随后与结构表示融合。我们使用三种最先进的LLMs（GPT-4o、GPT-4.1和DeepSeek-R1）进行知识提取。广泛的实验证明我们的集成方法优于现有方法，确认了LLMs衍生的知识与结构信息的结合为MPP提供了强大而有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting molecular properties is a critical component of drug discovery.Recent advances in deep learning, particularly Graph Neural Networks (GNNs),have enabled end-to-end learning from molecular structures, reducing relianceon manual feature engineering. However, while GNNs and self-supervised learningapproaches have advanced molecular property prediction (MPP), the integrationof human prior knowledge remains indispensable, as evidenced by recent methodsthat leverage large language models (LLMs) for knowledge extraction. Despitetheir strengths, LLMs are constrained by knowledge gaps and hallucinations,particularly for less-studied molecular properties. In this work, we propose anovel framework that, for the first time, integrates knowledge extracted fromLLMs with structural features derived from pre-trained molecular models toenhance MPP. Our approach prompts LLMs to generate both domain-relevantknowledge and executable code for molecular vectorization, producingknowledge-based features that are subsequently fused with structuralrepresentations. We employ three state-of-the-art LLMs, GPT-4o, GPT-4.1, andDeepSeek-R1, for knowledge extraction. Extensive experiments demonstrate thatour integrated method outperforms existing approaches, confirming that thecombination of LLM-derived knowledge and structural information provides arobust and effective solution for MPP.</description>
      <author>example@mail.com (Peng Zhou, Lai Hou Tim, Zhixiang Cheng, Kun Xie, Chaoyi Li, Wei Liu, Xiangxiang Zeng)</author>
      <guid isPermaLink="false">2509.20664v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding</title>
      <link>http://arxiv.org/abs/2509.21223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了名为Sigma的统一骨架SLU框架，通过标志感知早期融合机制、分层对齐学习策略和统一预训练框架解决了当前手语理解方法面临的三个关键限制：弱语义基础、局部细节与全局上下文不平衡以及低效跨模态学习。&lt;h4&gt;背景&lt;/h4&gt;预训练在手语理解(SLU)任务中被证明是有效的，基于骨架的方法因其能稳健处理主体和背景变化而不受外观或环境因素影响而日益受到关注。&lt;h4&gt;目的&lt;/h4&gt;解决当前SLU方法面临的三个关键限制：1)弱语义基础，模型难以将骨骼数据的运动模式与语言意义联系起来；2)局部细节与全局上下文之间的不平衡；3)低效的跨模态学习，难以构建跨模态语义对齐表示。&lt;h4&gt;方法&lt;/h4&gt;提出Sigma框架，包含：1)标志感知早期融合机制促进视觉与文本模态深度交互；2)分层对齐学习策略同时捕获细粒度细节和高级语义关系；3)统一预训练框架结合对比学习、文本匹配和语言建模。&lt;h4&gt;主要发现&lt;/h4&gt;Sigma在多个基准测试上取得最先进结果，包括孤立手语识别、连续手语识别和无词汇表手语翻译，证明了语义信息丰富预训练的影响和骨骼数据作为SLU独立解决方案的有效性。&lt;h4&gt;结论&lt;/h4&gt;语义信息丰富的预训练和骨骼数据作为独立解决方案对SLU任务具有重要影响，Sigma框架有效解决了当前方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;预训练已被证明对手语理解(SLU)任务中学习可迁移特征是有效的。最近，基于骨架的方法越来越受到关注，因为它们可以稳健地处理主体和背景的变化，而不受外观或环境因素的影响。当前的SLU方法仍然面临三个关键限制：1)弱语义基础，因为模型通常从骨骼数据中捕获低级运动模式，但难以将其与语言意义联系起来；2)局部细节和全局上下文之间的不平衡，模型要么过于关注细粒度线索，要么为了更广泛的上下文而忽略它们；3)低效的跨模态学习，因为在不同模态间构建语义对齐表示仍然很困难。为解决这些问题，我们提出了Sigma，一个统一的基于骨架的SLU框架，具有：1)标志感知的早期融合机制，促进视觉和文本模态之间的深度交互，用语言上下文丰富视觉特征；2)分层对齐学习策略，共同最大化来自不同模态的不同级别配对特征之间的一致性，有效捕获细粒度细节和高级语义关系；3)统一预训练框架，结合对比学习、文本匹配和语言建模，促进语义一致性和泛化能力。Sigma在多个基准测试上取得了新的最先进结果，包括孤立手语识别、连续手语识别和无词汇表手语翻译，这些测试涵盖了不同的手语和口语语言，证明了语义信息丰富预训练的影响以及骨骼数据作为SLU独立解决方案的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training has proven effective for learning transferable features in signlanguage understanding (SLU) tasks. Recently, skeleton-based methods havegained increasing attention because they can robustly handle variations insubjects and backgrounds without being affected by appearance or environmentalfactors. Current SLU methods continue to face three key limitations: 1) weaksemantic grounding, as models often capture low-level motion patterns fromskeletal data but struggle to relate them to linguistic meaning; 2) imbalancebetween local details and global context, with models either focusing toonarrowly on fine-grained cues or overlooking them for broader context; and 3)inefficient cross-modal learning, as constructing semantically alignedrepresentations across modalities remains difficult. To address these, wepropose Sigma, a unified skeleton-based SLU framework featuring: 1) asign-aware early fusion mechanism that facilitates deep interaction betweenvisual and textual modalities, enriching visual features with linguisticcontext; 2) a hierarchical alignment learning strategy that jointly maximisesagreements across different levels of paired features from differentmodalities, effectively capturing both fine-grained details and high-levelsemantic relationships; and 3) a unified pre-training framework that combinescontrastive learning, text matching and language modelling to promote semanticconsistency and generalisation. Sigma achieves new state-of-the-art results onisolated sign language recognition, continuous sign language recognition, andgloss-free sign language translation on multiple benchmarks spanning differentsign and spoken languages, demonstrating the impact of semantically informativepre-training and the effectiveness of skeletal data as a stand-alone solutionfor SLU.</description>
      <author>example@mail.com (Muxin Pu, Mei Kuan Lim, Chun Yong Chong, Chen Change Loy)</author>
      <guid isPermaLink="false">2509.21223v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Adversarially Robust MIMO Physical Layer Authentication for Non-Stationary Channels</title>
      <link>http://arxiv.org/abs/2509.21171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to an IEEE journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种针对非平稳MIMO无线信道的抗干扰物理层认证框架，整合了序列贝叶斯决策、深度特征提取和生成对抗建模，有效处理了时间空间相关性、视距阻塞和动态欺骗策略等问题。&lt;h4&gt;背景&lt;/h4&gt;传统物理层认证方法假设信道平稳或观测独立，难以应对实际无线环境中存在的时间与空间相关性、视距阻塞和动态欺骗策略等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应非平稳MIMO无线信道特性的鲁棒物理层认证框架，提高认证系统对各种欺骗攻击的抵抗能力。&lt;h4&gt;方法&lt;/h4&gt;整合序列贝叶斯决策、通过对比学习进行深度特征提取，以及生成对抗建模来模拟自适应欺骗者；使用2状态和3状态隐马尔可夫模型结合移动平均在线适应对认证性能进行综合分析。&lt;h4&gt;主要发现&lt;/h4&gt;通过闭式递归推导出的对数似然比、检测概率和稳态近似值表明，该方法相比经典序列认证方案具有显著的鲁棒性改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的AR-PLA框架能够有效处理非平稳MIMO信道中的复杂挑战，为无线通信系统提供更安全的物理层认证解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种针对非平稳多输入多输出(MIMO)无线信道的抗干扰物理层认证(AR-PLA)框架。该框架整合了序列贝叶斯决策、通过对比学习进行深度特征提取以及生成对抗建模来模拟自适应欺骗者。与假设信道平稳或观测独立性的传统方法不同，我们的方法明确考虑了时间与空间相关性、视距(LoS)阻塞和动态欺骗策略。同时，我们使用2状态和3状态隐马尔可夫模型(HMMs)结合移动平均在线适应对认证性能进行了全面分析，给出了对数似然比、检测概率和稳态近似值的闭式递归，这些结果表明与经典序列认证方案相比具有显著的鲁棒性提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an adversarially robust physical layer authentication (AR-PLA)framework tailored for non-stationary multiple-input multiple-output (MIMO)wireless channels. The framework integrates sequential Bayesiandecision-making, deep feature extraction via contrastive learning, andgenerative adversarial modeling to simulate adaptive spoofers. Unlikeconventional methods that assume stationary channels or independentobservations, our approach explicitly accounts for temporal and spatialcorrelations, line-of-sight (LoS) blockages, and dynamic spoofing strategies. Acomprehensive analytical characterization of the authentication performanceusing both 2-state and 3-state hidden Markov models (HMMs) with moving-averageonline adaptation is also provided, with closed-form recursions forloglikelihood ratios, detection probabilities, and steady-state approximations,which demonstrate significant robustness improvement over classical sequentialauthentication schemes.</description>
      <author>example@mail.com (Ali Khandan Boroujeni, Ghazal Bagheri, Kuranage Roche Rayan Ranasinghe, Giuseppe Thadeu Freitas de Abreu, Stefan Köpsell, Rafael F. Schaefer)</author>
      <guid isPermaLink="false">2509.21171v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction</title>
      <link>http://arxiv.org/abs/2509.21151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ROC框架，将多模态关系抽取从分类任务转变为检索任务，解决了传统方法的两个主要局限性：忽略结构约束和缺乏语义表达能力。&lt;h4&gt;背景&lt;/h4&gt;关系抽取(RE)旨在识别非结构化文本中实体间的语义关系。虽然最近研究将传统RE扩展到多模态场景，但大多数方法仍采用基于分类的范式，将关系表示为离散标签。&lt;h4&gt;目的&lt;/h4&gt;解决传统多模态关系抽取方法的两个主要局限性：忽略结构约束和缺乏细粒度关系理解的语义表达能力。&lt;h4&gt;方法&lt;/h4&gt;提出ROC框架，将多模态RE重新定义为关系语义驱动的检索任务。通过多模态编码器集成实体类型和位置信息，使用大型语言模型将关系标签扩展为自然语言描述，并通过基于语义相似性的对比学习对齐实体-关系对。&lt;h4&gt;主要发现&lt;/h4&gt;ROC在基准数据集MNRE和MORE上取得了最先进的性能，表现出更强的鲁棒性和可解释性。&lt;h4&gt;结论&lt;/h4&gt;ROC框架有效解决了传统分类范式的问题，在多模态关系抽取任务中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;关系抽取(RE)旨在识别非结构化文本中实体之间的语义关系。尽管最近的工作将传统RE扩展到多模态场景，但大多数方法仍然采用基于分类的范式，融合多模态特征，将关系表示为离散标签。这种范式有两个显著局限性：(1)它忽略了实体类型和位置线索等结构约束，(2)它缺乏对细粒度关系理解的语义表达能力。我们提出了ROC(检索优于分类)框架，这是一个将多模态RE重新定义为关系语义驱动的检索任务的新框架。ROC通过多模态编码器集成实体类型和位置信息，使用大型语言模型将关系标签扩展为自然语言描述，并通过基于语义相似性的对比学习对齐实体-关系对。实验表明，我们的方法在基准数据集MNRE和MORE上取得了最先进的性能，并表现出更强的鲁棒性和可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relation extraction (RE) aims to identify semantic relations between entitiesin unstructured text. Although recent work extends traditional RE to multimodalscenarios, most approaches still adopt classification-based paradigms withfused multimodal features, representing relations as discrete labels. Thisparadigm has two significant limitations: (1) it overlooks structuralconstraints like entity types and positional cues, and (2) it lacks semanticexpressiveness for fine-grained relation understanding. We propose\underline{R}etrieval \underline{O}ver \underline{C}lassification (ROC), anovel framework that reformulates multimodal RE as a retrieval task driven byrelation semantics. ROC integrates entity type and positional informationthrough a multimodal encoder, expands relation labels into natural languagedescriptions using a large language model, and aligns entity-relation pairs viasemantic similarity-based contrastive learning. Experiments show that ourmethod achieves state-of-the-art performance on the benchmark datasets MNRE andMORE and exhibits stronger robustness and interpretability.</description>
      <author>example@mail.com (Lei Hei, Tingjing Liao, Yingxin Pei, Yiyang Qi, Jiaqi Wang, Ruiting Li, Feiliang Ren)</author>
      <guid isPermaLink="false">2509.21151v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization</title>
      <link>http://arxiv.org/abs/2509.21033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出支持向量正则化（SVR）方法，用于解决对比语言-音频预训练中负样本推力的垂直分量导致的优化轨迹漂移和训练不稳定问题。通过引入辅助支持向量控制垂直分量，并探索语义半径的无监督建模策略，该方法在多个任务上超越了现有基线。&lt;h4&gt;背景&lt;/h4&gt;对比语言-音频预训练旨在将多模态表示统一到共享嵌入空间中，是构建从跨模态检索到前沿多模态大语言模型等各种应用的基础。&lt;h4&gt;目的&lt;/h4&gt;解决对比学习中负样本推力的垂直分量带来的优化轨迹漂移和训练不稳定问题，同时保留负样本中的丰富信息。&lt;h4&gt;方法&lt;/h4&gt;提出支持向量正则化（SVR）方法，引入辅助支持向量控制推力的垂直分量；探索语义半径的无监督建模策略，包括直接参数化和带有约束的自适应半径预测器模块。&lt;h4&gt;主要发现&lt;/h4&gt;SVR方法在标准音频-文本数据集上的分类、单语检索和多语检索任务中超越了InfoNCE和SigLIP损失等广泛使用的基线；关于优化轨迹漂移的理论分析和实验结果验证了SVR方法的正确性和有效性。&lt;h4&gt;结论&lt;/h4&gt;支持向量正则化方法能够有效控制对比学习中负样本推力的垂直分量，减轻优化轨迹漂移，提高训练稳定性，同时保留负样本中的丰富信息，在各种多模态任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-音频预训练旨在将多模态表示统一到共享嵌入空间中，是构建从跨模态检索到前沿多模态大语言模型等各种应用的基础。然而，我们发现对比学习中负样本推力的垂直分量是一把双刃剑：它包含来自负样本的丰富补充信息，但其无约束的性质会导致优化轨迹漂移和训练不稳定。为此，我们提出支持向量正则化（SVR）方法，通过引入辅助支持向量来控制这个垂直分量，旨在利用其丰富信息同时减轻相关的轨迹漂移。SVR的功效由其语义半径决定，我们探索了两种无监督建模策略：直接参数化和带有约束的自适应半径预测器模块，以提高其预测准确性。大量实验结果表明，在标准音频-文本数据集上的分类、单语检索和多语检索任务中，我们的方法超越了InfoNCE和SigLIP损失等广泛使用的基线。关于优化轨迹漂移的理论分析和实验结果都验证了SVR方法的正确性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive language-audio pretraining, which aims to unify multimodalrepresentations in a shared embedding space, serves as a cornerstone forbuilding a wide range of applications, from cross-modal retrieval tocutting-edge multimodal large language models. However, we find that theperpendicular component of the pushing force from negative samples incontrastive learning is a double-edged sword: it contains rich supplementaryinformation from negative samples, yet its unconstrained nature causesoptimization trajectory drift and training instability. To address this, wepropose Support Vector Regularization (SVR), a method that introduces anauxiliary support vector to control this perpendicular component, aiming toharness its rich information while mitigating the associated trajectory drift.The efficacy of SVR is critically governed by its semantic radius, for which weexplore two unsupervised modeling strategies: direct parameterization and anadaptive radius predictor module enhanced with constraints to improve itspredicting accuracy. Extensive experimental results demonstrate that our methodsurpasses widely used baselines like InfoNCE and SigLIP loss acrossclassification, monolingual retrieval, and multilingual retrieval on standardaudio-text datasets. Both the theoretical analysis and the experimental resultson optimizing trajectory drift validate the correctness and effectiveness ofour SVR method.</description>
      <author>example@mail.com (Jiehui Luo, Yuguo Yin, Yuxin Xie, Jinghan Ru, Xianwei Zhuang, Minghua He, Aofan Liu, Zihan Xiong, Dongchao Yang)</author>
      <guid isPermaLink="false">2509.21033v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network</title>
      <link>http://arxiv.org/abs/2509.20861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对物联网环境中复杂动态网络流量的检测挑战，本研究提出了一种新型特征提取工具和嵌入训练框架，通过消除传统时间和长度特征，采用上下文感知语义特征，并结合DBSCAN聚类算法和对比学习策略，有效提高了检测准确性、鲁棒性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;物联网环境中大量设备持续交互产生复杂动态的网络流量，对基于规则的检测方法构成重大挑战。基于机器学习的流量检测技术能够识别异常模式和潜在威胁，是确保网络安全的关键组件。&lt;h4&gt;目的&lt;/h4&gt;解决现有特征提取工具使用时间和长度相关特征导致高稀疏性影响模型收敛的问题，以及现有流量检测方法缺乏高效捕获网络流量语义特征的嵌入机制的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出新型特征提取工具，消除传统时间和长度特征，采用与源主机相关的上下文感知语义特征；设计集成无监督DBSCAN聚类算法和对比学习策略的嵌入训练框架，有效捕获流量的细粒度语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;在真实Mawi数据集上的评估表明，与几种最先进模型的比较实验证明了所提出方法的优越性能，且该方法在实际实时场景中具有良好的适用性和可部署性。&lt;h4&gt;结论&lt;/h4&gt;提出的特征提取工具和嵌入训练框架在物联网网络流量检测方面表现优越，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;在物联网环境中，大量设备的持续交互产生复杂动态的网络流量，这对基于规则的检测方法构成了重大挑战。基于机器学习的流量检测技术能够识别此流量中的异常模式和潜在威胁，是确保网络安全的关健组件。本研究首先确定了广泛采用的特征提取工具的一个显著问题：大量使用时间和长度相关特征导致高稀疏性，这 adversely 影响模型收敛。此外，现有流量检测方法通常缺乏能够高效全面捕获网络流量语义特征的嵌入机制。为解决这些挑战，我们提出了一种新型特征提取工具，它消除了传统的时间和长度特征，转而采用与源主机相关的上下文感知语义特征，从而提高模型的泛化能力。此外，我们设计了一个嵌入训练框架，将无监督DBSCAN聚类算法与对比学习策略相结合，以有效捕获流量的细粒度语义表示。我们在真实的Mawi数据集上进行了大量经验评估，以验证所提出方法在检测准确性、鲁棒性和泛化方面的有效性。与几种最先进模型的比较实验证明了我们方法的优越性能。此外，我们确认了它在实时场景中的适用性和可部署性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the Internet of Things (IoT) environment, continuous interaction among alarge number of devices generates complex and dynamic network traffic, whichposes significant challenges to rule-based detection approaches. Machinelearning (ML)-based traffic detection technology, capable of identifyinganomalous patterns and potential threats within this traffic, serves as acritical component in ensuring network security. This study first identifies asignificant issue with widely adopted feature extraction tools (e.g.,CICMeterFlow): the extensive use of time- and length-related features leads tohigh sparsity, which adversely affects model convergence. Furthermore, existingtraffic detection methods generally lack an embedding mechanism capable ofefficiently and comprehensively capturing the semantic characteristics ofnetwork traffic. To address these challenges, we propose a novel featureextraction tool that eliminates traditional time and length features in favorof context-aware semantic features related to the source host, thus improvingthe generalizability of the model. In addition, we design an embedding trainingframework that integrates the unsupervised DBSCAN clustering algorithm with acontrastive learning strategy to effectively capture fine-grained semanticrepresentations of traffic. Extensive empirical evaluations are conducted onthe real-world Mawi data set to validate the proposed method in terms ofdetection accuracy, robustness, and generalization. Comparative experimentsagainst several state-of-the-art (SOTA) models demonstrate the superiorperformance of our approach. Furthermore, we confirm its applicability anddeployability in real-time scenarios.</description>
      <author>example@mail.com (Chao Zha, Haolin Pan, Bing Bai, Jiangxing Wu, Ruyun Zhang)</author>
      <guid isPermaLink="false">2509.20861v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.20813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LumbarCLIP的新型多模态框架，利用对比语言-图像预训练技术将腰椎MRI扫描与相应的放射学描述对齐，实现了在下游分类任务上高达95.00%的准确率和94.75%的F1分数，为自动化肌肉骨骼诊断和临床决策支持提供了有前景的基础。&lt;h4&gt;背景&lt;/h4&gt;腰背痛影响全球数百万人，这推动了对能够联合分析复杂医学图像和相关文本报告的强大诊断模型的需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将腰椎MRI扫描与相应的放射学描述对齐的多模态诊断框架。&lt;h4&gt;方法&lt;/h4&gt;提出LumbarCLIP框架，基于包含轴向MRI视图与专家撰写的报告配对的数据集，集成视觉编码器（ResNet-50、Vision Transformer、Swin Transformer）与基于BERT的文本编码器提取密集表示，通过可学习的投影头将这些表示投影到共享嵌入空间，并使用软CLIP损失进行对比训练。&lt;h4&gt;主要发现&lt;/h4&gt;模型在测试集上达到95.00%的准确率和94.75%的F1分数，消融研究表明线性投影头比非线性变体产生更有效的跨模态对齐。&lt;h4&gt;结论&lt;/h4&gt;LumbarCLIP为自动化肌肉骨骼诊断和临床决策支持提供了有前景的基础。&lt;h4&gt;翻译&lt;/h4&gt;腰背痛影响全球数百万人，推动了对能够联合分析复杂医学图像和相关文本报告的强大诊断模型的需求。我们提出了LumbarCLIP，一种新颖的多模态框架，利用对比语言-图像预训练来将腰椎MRI扫描与相应的放射学描述对齐。基于一个包含轴向MRI视图与专家撰写的报告配对的数据集构建，LumbarCLIP集成了视觉编码器（ResNet-50、Vision Transformer、Swin Transformer）与基于BERT的文本编码器来提取密集表示。这些表示通过可学习的投影头（可配置为线性或非线性）投影到共享的嵌入空间，并进行归一化以使用软CLIP损失促进稳定的对比训练。尽管存在固有的类别不平衡，我们的模型在下游分类任务上取得了最先进的性能，在测试集上达到高达95.00%的准确率和94.75%的F1分数。大量的消融研究表明，线性投影头比非线性变体产生更有效的跨模态对齐。LumbarCLIP为自动化肌肉骨骼诊断和临床决策支持提供了有前景的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low back pain affects millions worldwide, driving the need for robustdiagnostic models that can jointly analyze complex medical images andaccompanying text reports. We present LumbarCLIP, a novel multimodal frameworkthat leverages contrastive language-image pretraining to align lumbar spine MRIscans with corresponding radiological descriptions. Built upon a curateddataset containing axial MRI views paired with expert-written reports,LumbarCLIP integrates vision encoders (ResNet-50, Vision Transformer, SwinTransformer) with a BERT-based text encoder to extract dense representations.These are projected into a shared embedding space via learnable projectionheads, configurable as linear or non-linear, and normalized to facilitatestable contrastive training using a soft CLIP loss. Our model achievesstate-of-the-art performance on downstream classification, reaching up to95.00% accuracy and 94.75% F1-score on the test set, despite inherent classimbalance. Extensive ablation studies demonstrate that linear projection headsyield more effective cross-modal alignment than non-linear variants. LumbarCLIPoffers a promising foundation for automated musculoskeletal diagnosis andclinical decision support.</description>
      <author>example@mail.com (Thanh Binh Le, Hoang Nhat Khang Vo, Tan-Ha Mai, Trong Nhan Phan)</author>
      <guid isPermaLink="false">2509.20813v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification</title>
      <link>http://arxiv.org/abs/2509.20489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages (14 pages Main text and 6 pages Supplementary Material)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新型端到端深度学习框架，用于处理脑电图信号中的多尺度信息，解决噪声和通道变化性问题。&lt;h4&gt;背景&lt;/h4&gt;脑电图信号包含丰富的多尺度信息，对理解大脑状态至关重要，在诊断和药物开发领域有潜在应用。然而，从原始EEG信号中提取有意义特征同时处理噪声和通道变化性仍然是一个主要挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖的端到端深度学习框架来解决EEG信号处理中的挑战，通过几个关键创新来提高特征提取的可靠性。&lt;h4&gt;方法&lt;/h4&gt;设计了一种能够捕获多尺度频率振荡的编码器；引入了基于注意力的编码器来学习EEG通道间和通道内局部区域的交互；集成了门控网络以动态过滤噪声通道；使用结合监督学习和对比学习的新型损失函数指导整个编码过程。&lt;h4&gt;主要发现&lt;/h4&gt;该学习范式能从原始EEG信号中提取生物学上有意义的模式；可跨不同物种自主选择高质量通道；通过创新的架构和损失设计实现强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该方法在多种应用中得到验证，从中枢神经系统障碍治疗的分类到帕金森病和阿尔茨海默病的诊断，结果表明能有效处理EEG数据的挑战。&lt;h4&gt;翻译&lt;/h4&gt;脑电图信号包含丰富的多尺度信息，对理解大脑状态至关重要，在诊断和推进药物开发领域具有潜在应用。然而，从原始脑电图信号中提取有意义特征同时处理噪声和通道变化性仍然是一个主要挑战。这项工作提出了一种新颖的端到端深度学习框架，通过几个关键创新解决了这些问题。首先，我们设计了一种能够明确捕获多尺度频率振荡的编码器，覆盖不同脑电图相关任务的广泛特征。其次，为了建模复杂依赖关系并处理脑电图的高时间分辨率，我们引入了一种基于注意力的编码器，同时学习脑电图通道之间的相互作用和单个通道内的局部区域。我们在注意力编码器顶部集成了一个专门的门控网络，以动态过滤掉嘈杂和非信息性通道，提高脑电图数据的可靠性。整个编码过程由一种新颖的损失函数指导，该函数利用监督学习和对比学习，显著提高了模型泛化能力。我们在多种应用中验证了我们的方法，从中枢神经系统障碍治疗的分类到帕金森病和阿尔茨海默病的诊断。我们的结果表明，所提出的学习范式能够从原始脑电图信号中提取跨不同物种的生物学上有意义的模式，自主选择高质量通道，并通过创新的架构和损失设计实现强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography signals (EEGs) contain rich multi-scale informationcrucial for understanding brain states, with potential applications indiagnosing and advancing the drug development landscape. However, extractingmeaningful features from raw EEG signals while handling noise and channelvariability remains a major challenge. This work proposes a novel end-to-enddeep-learning framework that addresses these issues through several keyinnovations. First, we designed an encoder capable of explicitly capturingmulti-scale frequency oscillations covering a wide range of features fordifferent EEG-related tasks. Secondly, to model complex dependencies and handlethe high temporal resolution of EEGs, we introduced an attention-based encoderthat simultaneously learns interactions across EEG channels and withinlocalized {\em patches} of individual channels. We integrated a dedicatedgating network on top of the attention encoder to dynamically filter out noisyand non-informative channels, enhancing the reliability of EEG data. The entireencoding process is guided by a novel loss function, which leverages supervisedand contrastive learning, significantly improving model generalization. Wevalidated our approach in multiple applications, ranging from theclassification of effects across multiple Central Nervous System (CNS)disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease.Our results demonstrate that the proposed learning paradigm can extractbiologically meaningful patterns from raw EEG signals across different species,autonomously select high-quality channels, and achieve robust generalizationthrough innovative architectural and loss design.</description>
      <author>example@mail.com (D. Darankoum, C. Habermacher, J. Volle, S. Grudinin)</author>
      <guid isPermaLink="false">2509.20489v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>A Contrastive Learning Framework for Breast Cancer Detection</title>
      <link>http://arxiv.org/abs/2509.20474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于对比学习(CL)框架的乳腺癌检测方法，通过半监督学习在大量未标记数据上训练Resnet-50模型，并在小规模标记数据上调整，最终在基准数据集上实现了96.7%的检测准确率，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;乳腺癌是全球第二大癌症死亡原因，占所有癌症病例的四分之一。早期检测对降低死亡率至关重要，而非侵入性成像技术和计算机辅助检测系统为早期检测提供了可能。传统图像分析方法存在局限性，而深度学习方法因标记数据有限而面临准确性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决深度学习方法在乳腺癌检测中因标记数据有限而导致的准确性问题，开发一种能够在小规模标记数据上高效训练的模型，提高乳腺癌检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一种对比学习(CL)框架，采用半监督对比学习方法，使用相似度指数在大量未标记的乳腺X线照片数据上训练Resnet-50模型。研究中使用了各种数据增强和变换技术来提高方法的性能，最后在一小组标记数据上调整模型。&lt;h4&gt;主要发现&lt;/h4&gt;在INbreast和MIAS基准数据集上，所提出的模型达到了96.7%的乳腺癌检测准确率，优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;通过对比学习框架和半监督学习方法，研究成功地解决了深度学习在乳腺癌检测中因标记数据有限而导致的准确性问题，为早期乳腺癌检测提供了更有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;乳腺癌是全球第二大癌症相关死亡原因，占所有癌症病例的四分之一[1]。为了降低这一死亡率，早期检测肿瘤至关重要，因为早期检测显著改善治疗效果。非侵入性成像技术的进步使得通过计算机辅助检测(CAD)系统进行早期检测成为可能，这些系统依赖于传统的图像分析来识别恶性肿瘤。然而，由于深度学习方法在处理有限的大型标记训练数据集时往往面临准确性挑战，因此正逐渐转向深度学习方法。尽管有这种潜力，但我们的研究引入了一种对比学习(CL)框架，该框架在小型标记数据集上表现出色。在这方面，我们使用相似度指数在大量未标记的乳腺X线照片数据上以半监督对比学习方法训练Resnet-50。在这方面，我们使用了各种增强和变换技术，有助于提高我们方法的性能。最后，我们在一小部分标记数据上调整了我们的模型，优于现有的最先进方法。具体而言，我们在基准数据集INbreast和MIAS上观察到乳腺癌检测的准确率为96.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast cancer, the second leading cause of cancer-related deaths globally,accounts for a quarter of all cancer cases [1]. To lower this death rate, it iscrucial to detect tumors early, as early-stage detection significantly improvestreatment outcomes. Advances in non-invasive imaging techniques have made earlydetection possible through computer-aided detection (CAD) systems which rely ontraditional image analysis to identify malignancies. However, there is agrowing shift towards deep learning methods due to their superioreffectiveness. Despite their potential, deep learning methods often strugglewith accuracy due to the limited availability of large-labeled datasets fortraining. To address this issue, our study introduces a Contrastive Learning(CL) framework, which excels with smaller labeled datasets. In this regard, wetrain Resnet-50 in semi supervised CL approach using similarity index on alarge amount of unlabeled mammogram data. In this regard, we use variousaugmentation and transformations which help improve the performance of ourapproach. Finally, we tune our model on a small set of labelled data thatoutperforms the existing state of the art. Specifically, we observed a 96.7%accuracy in detecting breast cancer on benchmark datasets INbreast and MIAS.</description>
      <author>example@mail.com (Samia Saeed, Khuram Naveed)</author>
      <guid isPermaLink="false">2509.20474v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</title>
      <link>http://arxiv.org/abs/2509.21245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report; 3D Generation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Hunyuan3D-Omni，一个统一的细粒度可控3D资产生成框架，通过多种条件信号实现精确控制，解决了现有方法依赖单一模态输入的限制。&lt;h4&gt;背景&lt;/h4&gt;3D原生生成模型的最新进展加速了游戏、电影和设计中的资产创建，但大多数方法主要依赖图像或文本条件，缺乏细粒度的跨模态控制，限制了可控性和实际应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D生成方法在细粒度、跨模态控制方面的不足，开发一个统一的框架，实现对几何、拓扑和姿态的精确控制。&lt;h4&gt;方法&lt;/h4&gt;基于Hunyuan3D 2.1构建，接受点云、体素、边界框和骨骼姿态先验等多种条件信号。采用统一的跨模态架构而非为每种模态使用单独的头部。使用渐进式、难度感知的采样策略，偏向于处理更难的信号，同时降低较容易信号的权重，鼓励稳健的多模态融合和对缺失输入的优雅处理。&lt;h4&gt;主要发现&lt;/h4&gt;额外的控制提高了生成准确性，实现了几何感知的转换，并增强了生产工作流程的稳健性。&lt;h4&gt;结论&lt;/h4&gt;Hunyuan3D-Omni通过统一的跨模态架构和渐进式训练策略，解决了现有3D生成模型在细粒度控制方面的局限性，为游戏、电影和设计行业提供了更强大的3D资产生成工具。&lt;h4&gt;翻译&lt;/h4&gt;3D原生生成模型的最新进展加速了游戏、电影和设计中的资产创建。然而，大多数方法仍然主要依赖图像或文本条件，缺乏细粒度的跨模态控制，这限制了可控性和实际应用。为解决这一差距，我们提出了Hunyuan3D-Omni，一个基于Hunyuan3D 2.1构建的、用于细粒度可控3D资产生成的统一框架。除了图像外，Hunyuan3D-Omni还接受点云、体素、边界框和骨骼姿态先验作为条件信号，能够对几何、拓扑和姿态进行精确控制。我们的模型采用统一的跨模态架构，而非为每种模态使用单独的头部。我们采用渐进式、难度感知的采样策略，为每个示例选择一种控制模态，偏向于采样更难的信号(如骨骼姿态)，同时降低较容易信号的权重(如点云)，鼓励稳健的多模态融合和对缺失输入的优雅处理。实验表明，这些额外的控制提高了生成准确性，实现了几何感知的转换，并增强了生产工作流程的稳健性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D生成模型过度依赖图像或文本条件输入，缺乏细粒度跨模态控制能力的问题。这个问题在现实中很重要，因为缺乏控制性导致生成的3D资产在几何精度、拓扑结构和姿态控制上不够准确，限制了3D生成技术在游戏、电影和设计等领域的实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于Hunyuan3D 2.1模型进行扩展，引入点云、体素、边界框和骨骼四种控制信号，设计了一个统一的控制编码器来处理这些不同信号。训练时采用渐进式、难度感知的采样策略，优先处理难度较高的信号。作者借鉴了PoseMaster的骨骼表示方法，参考了2D可控生成模型如ControlNet的思想，并采用了点云补全方法中的随机丢弃采样策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将多种控制信号统一表示为点云形式，通过一个统一的控制编码器处理不同类型的控制信号，并将控制特征与图像特征结合输入扩散模型实现可控3D生成。整体流程包括：1)图像通过DINO编码器提取特征；2)不同控制信号通过统一控制编码器处理；3)将图像特征和控制特征连接形成联合特征；4)输入扩散模型生成3D潜在表示；5)通过VAE解码器生成最终3D模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的多模态控制框架，首次支持四种控制信号；2)轻量级统一控制编码器；3)渐进式难度感知训练策略；4)最小化训练成本。相比之前工作，Hunyuan3D-Omni通过统一架构实现了多模态控制，减少了模型复杂度，能够优雅处理缺失输入，而之前的方法通常只支持单一条件或需要针对每种信号单独训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Hunyuan3D-Omni通过统一的控制框架和轻量级编码器，实现了基于点云、体素、边界框和骨骼的细粒度可控3D资产生成，显著提高了生成模型的控制性和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 3D-native generative models have accelerated assetcreation for games, film, and design. However, most methods still relyprimarily on image or text conditioning and lack fine-grained, cross-modalcontrols, which limits controllability and practical adoption. To address thisgap, we present Hunyuan3D-Omni, a unified framework for fine-grained,controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal posepriors as conditioning signals, enabling precise control over geometry,topology, and pose. Instead of separate heads for each modality, our modelunifies all signals in a single cross-modal architecture. We train with aprogressive, difficulty-aware sampling strategy that selects one controlmodality per example and biases sampling toward harder signals (e.g., skeletalpose) while downweighting easier ones (e.g., point clouds), encouraging robustmulti-modal fusion and graceful handling of missing inputs. Experiments showthat these additional controls improve generation accuracy, enablegeometry-aware transformations, and increase robustness for productionworkflows.</description>
      <author>example@mail.com (Team Hunyuan3D, :, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Yunfei Zhao, Zeqiang Lai, Zhihao Liang, Zibo Zhao)</author>
      <guid isPermaLink="false">2509.21245v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps</title>
      <link>http://arxiv.org/abs/2509.21145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DAGDiff框架，一个端到端的系统，能够直接在SE(3) x SE(3)空间中去噪生成抓取对，有效解决双臂抓取中的稳定性、碰撞和泛化问题。&lt;h4&gt;背景&lt;/h4&gt;可靠的双臂抓取对于操作大型和复杂物体至关重要，但由于稳定性、碰撞和泛化要求，这仍然是一个具有挑战性的问题。先前方法通常将任务分解为两个独立的抓取提议，依赖于区域先验或启发式方法，限制了泛化能力且无法提供稳定性的原则性保证。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够直接生成稳定且无碰撞的双臂抓取对的方法，不依赖于显式的区域检测或物体先验，提高抓取的泛化能力和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出DAGDiff框架，通过分类器信号引导扩散过程，而非依赖区域检测或物体先验。该框架集成了几何、稳定性和碰撞感知的引导项，引导生成过程朝向物理有效且力封闭合规的抓取方向发展。&lt;h4&gt;主要发现&lt;/h4&gt;通过分析力封闭检查、碰撞分析和大规模基于物理的模拟全面评估了DAGDiff，在各项指标上显示出比先前工作的一致性改进。框架能够直接在先前未见过的物体的真实世界点云上生成双臂抓取，并在异构双臂设置上可靠执行。&lt;h4&gt;结论&lt;/h4&gt;DAGDiff提供了一个有效的端到端解决方案，能够生成稳定、无碰撞且具有良好泛化能力的双臂抓取，可直接应用于真实世界场景中的未知物体。&lt;h4&gt;翻译&lt;/h4&gt;可靠的双臂抓取对于操作大型和复杂物体至关重要，但由于稳定性、碰撞和泛化要求，这仍然是一个具有挑战性的问题。先前方法通常将任务分解为两个独立的抓取提议，依赖于区域先验或启发式方法，这些方法限制了泛化能力且无法提供稳定性的原则性保证。我们提出了DAGDiff，一个端到端的框架，直接在SE(3) x SE(3)空间中去噪以生成抓取对。我们的主要见解是通过分类器信号引导扩散过程，可以更有效地强制执行稳定性和避免碰撞，而不是依赖显式区域检测或物体先验。为此，DAGDiff集成了几何、稳定性和碰撞感知的引导项，引导生成过程朝向物理有效且力封闭合规的抓取方向发展。我们通过分析力封闭检查、碰撞分析和大规模基于物理的模拟全面评估了DAGDiff，在这些指标上显示出比先前工作的一致性改进。最后，我们展示了该框架可以直接在先前未见过的物体的真实世界点云上生成双臂抓取，这些抓取在异构双臂设置上执行，两个机械臂可靠地抓取和提升它们。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable dual-arm grasping is essential for manipulating large and complexobjects but remains a challenging problem due to stability, collision, andgeneralization requirements. Prior methods typically decompose the task intotwo independent grasp proposals, relying on region priors or heuristics thatlimit generalization and provide no principled guarantee of stability. Wepropose DAGDiff, an end-to-end framework that directly denoises to grasp pairsin the SE(3) x SE(3) space. Our key insight is that stability and collision canbe enforced more effectively by guiding the diffusion process with classifiersignals, rather than relying on explicit region detection or object priors. Tothis end, DAGDiff integrates geometry-, stability-, and collision-awareguidance terms that steer the generative process toward grasps that arephysically valid and force-closure compliant. We comprehensively evaluateDAGDiff through analytical force-closure checks, collision analysis, andlarge-scale physics-based simulations, showing consistent improvements overprevious work on these metrics. Finally, we demonstrate that our frameworkgenerates dual-arm grasps directly on real-world point clouds of previouslyunseen objects, which are executed on a heterogeneous dual-arm setup where twomanipulators reliably grasp and lift them.</description>
      <author>example@mail.com (Md Faizal Karim, Vignesh Vembar, Keshab Patra, Gaurav Singh, K Madhava Krishna)</author>
      <guid isPermaLink="false">2509.21145v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling</title>
      <link>http://arxiv.org/abs/2509.21114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SIGGRAPH Asia 2025. 17 pages, 15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CHARM，一种用于动漫发型建模的新型参数化表示和生成框架，解决了传统方法难以处理动漫发型高度风格化、分段结构几何形状的问题。&lt;h4&gt;背景&lt;/h4&gt;传统头发建模方法专注于真实感头发，使用基于发丝或体积的表示方法，而动漫发型具有高度风格化和分段结构的几何特征，挑战了现有技术。现有工作通常依赖于密集网格建模或手工制作的样条曲线，使得它们在编辑方面效率低下，不适合可扩展的学习。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效、准确且可扩展的动漫发型建模解决方案，既能支持艺术家友好的设计，也能支持基于学习的生成。&lt;h4&gt;方法&lt;/h4&gt;CHARM引入了一种紧凑的、可逆的基于控制点的参数化方法，每个发片由一系列控制点表示，每个点仅用五个几何参数编码。基于这种表示，CHARM引入了一个自回归生成框架，可以从输入图像或点云有效地生成动漫发型。通过将动漫发型解释为顺序的'头发语言'，自回归Transformer能够捕获局部几何形状和全局发型拓扑结构。&lt;h4&gt;主要发现&lt;/h4&gt;研究团队构建了AnimeHair数据集，包含37K个高质量动漫发型，带有分离的发片和处理的网格数据。实验证明CHARM在重建准确性和生成质量方面都达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;CHARM为动漫发型建模提供了一种具有表现力和可扩展性的解决方案，在重建准确性和生成质量方面都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了CHARM，一种用于动漫发型建模的新型参数化表示和生成框架。虽然传统的头发建模方法专注于使用基于发丝或体积的表示方法来模拟真实头发，但动漫发型具有高度风格化、分段结构的几何特征，挑战了现有技术。现有工作通常依赖于密集网格建模或手工制作的样条曲线，使得它们在编辑方面效率低下，不适合可扩展的学习。CHARM引入了一种紧凑的、可逆的基于控制点的参数化方法，其中一系列控制点表示每个发片，每个点仅用五个几何参数编码。这种高效且准确的表示既支持艺术家友好的设计，也支持基于学习的生成。基于这种表示，CHARM引入了一个自回归生成框架，能够有效地从输入图像或点云生成动漫发型。通过将动漫发型解释为顺序的'头发语言'，我们的自回归Transformer捕获了局部几何形状和全局发型拓扑结构，实现了高保真度的动漫发型创建。项目页面：https://hyzcluster.github.io/charm/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present CHARM, a novel parametric representation and generative frameworkfor anime hairstyle modeling. While traditional hair modeling methods focus onrealistic hair using strand-based or volumetric representations, animehairstyle exhibits highly stylized, piecewise-structured geometry thatchallenges existing techniques. Existing works often rely on dense meshmodeling or hand-crafted spline curves, making them inefficient for editing andunsuitable for scalable learning. CHARM introduces a compact, invertiblecontrol-point-based parameterization, where a sequence of control pointsrepresents each hair card, and each point is encoded with only five geometricparameters. This efficient and accurate representation supports bothartist-friendly design and learning-based generation. Built upon thisrepresentation, CHARM introduces an autoregressive generative framework thateffectively generates anime hairstyles from input images or point clouds. Byinterpreting anime hairstyles as a sequential "hair language", ourautoregressive transformer captures both local geometry and global hairstyletopology, resulting in high-fidelity anime hairstyle creation. To facilitateboth training and evaluation of anime hairstyle generation, we constructAnimeHair, a large-scale dataset of 37K high-quality anime hairstyles withseparated hair cards and processed mesh data. Extensive experiments demonstratestate-of-the-art performance of CHARM in both reconstruction accuracy andgeneration quality, offering an expressive and scalable solution for animehairstyle modeling. Project page: https://hyzcluster.github.io/charm/</description>
      <author>example@mail.com (Yuze He, Yanning Zhou, Wang Zhao, Jingwen Ye, Yushi Bai, Kaiwen Xiao, Yong-Jin Liu, Zhongqian Sun, Wei Yang)</author>
      <guid isPermaLink="false">2509.21114v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>SeamCrafte: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.20725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了SeamCrafter，一种基于GPT风格的自回归接缝生成器，用于解决3D表面UV参数化和纹理映射中的接缝放置问题。&lt;h4&gt;背景&lt;/h4&gt;网格接缝在3D表面的UV参数化和纹理映射中起关键作用，不良接缝会导致严重的UV扭曲或过度碎片化，阻碍纹理合成并干扰艺术家工作流程。现有方法通常在两种失败模式间权衡：高扭曲或多分散碎片。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时减少UV扭曲和碎片化的接缝生成方法，避免现有方法的权衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出SeamCrafter，一种基于点云输入条件化的自回归GPT风格接缝生成器。采用双分支点云编码器在预训练中解耦并捕获拓扑和几何线索，使用直接偏好优化(DPO)在基于新接缝评估框架的偏好数据集上微调模型。&lt;h4&gt;主要发现&lt;/h4&gt;SeamCrafter产生的接缝比先前方法的扭曲和碎片化程度显著降低，同时保持了拓扑一致性和视觉保真度。&lt;h4&gt;结论&lt;/h4&gt;SeamCrafter是一种有效的接缝生成解决方案，能够平衡UV扭曲和碎片化问题，优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;网格接缝在将3D表面分割用于UV参数化和纹理映射中起着关键作用。放置不当的接缝通常会导致严重的UV扭曲或过度碎片化，从而阻碍纹理合成并干扰艺术家工作流程。现有方法经常在一种失败模式和另一种失败模式之间权衡——要么产生高扭曲，要么产生许多分散的碎片。为了解决这个问题，我们引入了SeamCrafter，一种基于点云输入条件化的自回归GPT风格接缝生成器。SeamCrafter采用双分支点云编码器，在预训练过程中解耦并捕获互补的拓扑和几何线索。为了进一步提高接缝质量，我们在基于新型接缝评估框架派生的偏好数据集上使用直接偏好优化(DPO)对模型进行微调。该框架主要根据UV扭曲和碎片化评估接缝，并提供成对偏好标签来指导优化。大量实验表明，SeamCrafter产生的接缝比先前方法的扭曲和碎片化程度低得多，同时保持了拓扑一致性和视觉保真度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D网格接缝生成的问题。接缝是3D表面上定义切割并展开成2D UV域的边缘，在UV参数化和纹理映射中起关键作用。接缝放置不当会导致UV失真或过度碎片化，影响纹理合成和艺术家工作流程。这个问题很重要，因为良好的接缝选择能实现忠实纹理对齐并最小化失真，支持高效的纹理合成和艺术家友好的编辑工作流程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：传统方法产生碎片化UV图谱，几何图像方法计算成本高，而神经方法如SeamGPT过度依赖拓扑线索。作者借鉴了SeamGPT的自回归方法、VecSet-based点云编码器和Direct Preference Optimization(DPO)方法，但创新性地设计了双分支编码器同时处理拓扑和几何信息，并使用基于偏好的优化来对齐人类判断。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是同时利用输入网格的拓扑和几何线索，并将预测与人类偏好对齐。流程分为三阶段：1)预训练阶段：将接衣表示为线段序列，用双分支编码器处理拓扑和几何点，用沙漏Transformer生成接缝；2)后训练阶段：用接缝评估系统构建偏好数据集，通过DPO优化失真和碎片化权衡；3)推理阶段：预测接缝坐标，映射到网格表面，标记为接缝并展开网格。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双分支点云编码器解耦拓扑和几何信息；2)接缝评估框架提供成对偏好标签；3)使用DPO进行后训练对齐人类偏好。相比之前工作，SeamCrafter不会产生过度碎片化UV图谱，计算成本更低，鲁棒性更强，同时考虑拓扑和几何信息而非仅依赖拓扑，无需大量场景微调且能保持接缝语义连贯性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SeamCrafter通过结合双分支点云编码器和直接偏好优化，实现了能同时考虑拓扑和几何信息并符合人类偏好的高质量3D网格接缝生成，显著降低了UV失真和碎片化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mesh seams play a pivotal role in partitioning 3D surfaces for UVparametrization and texture mapping. Poorly placed seams often result in severeUV distortion or excessive fragmentation, thereby hindering texture synthesisand disrupting artist workflows. Existing methods frequently trade one failuremode for another-producing either high distortion or many scattered islands. Toaddress this, we introduce SeamCrafter, an autoregressive GPT-style seamgenerator conditioned on point cloud inputs. SeamCrafter employs a dual-branchpoint-cloud encoder that disentangles and captures complementary topologicaland geometric cues during pretraining. To further enhance seam quality, wefine-tune the model using Direct Preference Optimization (DPO) on a preferencedataset derived from a novel seam-evaluation framework. This framework assessesseams primarily by UV distortion and fragmentation, and provides pairwisepreference labels to guide optimization. Extensive experiments demonstrate thatSeamCrafter produces seams with substantially lower distortion andfragmentation than prior approaches, while preserving topological consistencyand visual fidelity.</description>
      <author>example@mail.com (Duoteng Xu, Yuguang Chen, Jing Li, Xinhai Liu, Xueqi Ma, Zhuo Chen, Dongyu Zhang, Chunchao Guo)</author>
      <guid isPermaLink="false">2509.20725v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections</title>
      <link>http://arxiv.org/abs/2509.20607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用镜面反射进行3D重建的方法，通过将反射视为辅助视图并设计虚拟相机变换，实现了从单图像的多视图立体设置，简化了成像过程，并与强大的前馈重建模型兼容。&lt;h4&gt;背景&lt;/h4&gt;镜面反射在日常环境中很常见，可以在单次捕获中提供立体信息，因为真实和反射的虚拟视图同时可见。&lt;h4&gt;目的&lt;/h4&gt;利用镜面反射的性质将其视为辅助视图，设计变换构建物理有效的虚拟相机，实现从单图像生成多视图立体设置，使其与强大的前馈重建模型兼容以实现通用且稳健的3D重建。&lt;h4&gt;方法&lt;/h4&gt;将反射视为辅助视图并设计变换构建物理有效的虚拟相机，直接在像素域生成虚拟视图同时遵循真实世界成像过程；提出对称感知损失细化姿态估计以利用镜面引入的几何对称性；框架自然扩展到动态场景实现逐帧几何恢复。&lt;h4&gt;主要发现&lt;/h4&gt;该方法简化了成像过程，实现了从单图像的多视图立体设置，与强大的前馈重建模型兼容，能够实现通用且稳健的3D重建，并有效处理动态场景。&lt;h4&gt;结论&lt;/h4&gt;在真实世界数据和合成数据上的广泛实验证明了该方法的有效性，提供了一个完全可定制的16个Blender场景的合成数据集，每个场景都有真实的点云和相机姿态。&lt;h4&gt;翻译&lt;/h4&gt;镜面反射在日常环境中很常见，可以在单次捕获中提供立体信息，因为真实和反射的虚拟视图同时可见。我们通过将反射视为辅助视图并设计一种变换来构建物理上有效的虚拟相机来利用这一特性，允许在像素域直接生成虚拟视图，同时遵循真实世界的成像过程。这使我们可以从单图像实现多视图立体设置，简化了成像过程，使其与强大的前馈重建模型兼容，实现通用且稳健的3D重建。为了进一步利用镜面引入的几何对称性，我们提出了一种对称感知损失来细化姿态估计。我们的框架还自然地扩展到动态场景，其中每一帧都包含镜面反射，实现高效的逐帧几何恢复。为了进行定量评估，我们提供了一个完全可定制的16个Blender场景的合成数据集，每个场景都有真实的点云和相机姿态。在真实世界数据和合成数据上进行了广泛的实验，以说明我们方法的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用镜面反射来辅助单视图3D重建的问题。这个问题重要是因为镜面反射在日常环境中很常见，可以在单次拍摄中同时提供真实场景和反射的虚拟视图，形成立体信息。利用这些反射可以简化成像过程，减少硬件需求，无需跨相机同步，还能自然地扩展到动态场景，实现高效的逐帧几何恢复。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到镜面反射可以提供多视图线索，形成极线几何，且真实-虚拟视图对共享相同的内部参数。早期基于镜子的方法只能处理简单形状且需要高度控制的环境，而前馈重建模型又缺乏对反射的感知。作者借鉴了DUSt3R框架作为主干网络，使用Detect Any Mirror进行镜子检测，并利用多视图立体几何原理。通过将反射重新解释为辅助视角，将单视图问题转化为多视图问题，设计了像素域操作来创建虚拟视图。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将镜面反射重新解释为辅助视角，使单视图图像形成立体设置，并利用镜子场景的对称性来优化重建。整体流程包括：1)镜子检测和多视图设置，检测反射区域并水平翻转模拟虚拟相机；2)初始预测，使用DUSt3R从虚拟-真实对生成初始点云；3)后优化，应用对称感知损失细化相机位姿；4)镜子平面恢复，从结果点云中恢复镜子平面信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将镜面反射重新解释为辅助视角，实现单视图立体重建；2)设计像素域操作的多视图设置过程，创建符合物理成像的虚拟视图；3)利用对称性提出对称感知损失优化位姿；4)构建包含16个场景的完全可定制合成数据集。相比之前工作，本文方法不局限于简单形状或特定场景，能处理大角度差异的反射，且不将反射视为噪声而是有用信息，显著提高了重建质量和完整性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Reflect3r通过将镜面反射重新解释为虚拟相机捕获的辅助视图，成功地将单视图3D重建转化为多视图立体重建，显著提高了在含有镜子场景中的3D重建质量和完整性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mirror reflections are common in everyday environments and can provide stereoinformation within a single capture, as the real and reflected virtual viewsare visible simultaneously. We exploit this property by treating the reflectionas an auxiliary view and designing a transformation that constructs aphysically valid virtual camera, allowing direct pixel-domain generation of thevirtual view while adhering to the real-world imaging process. This enables amulti-view stereo setup from a single image, simplifying the imaging process,making it compatible with powerful feed-forward reconstruction models forgeneralizable and robust 3D reconstruction. To further exploit the geometricsymmetry introduced by mirrors, we propose a symmetric-aware loss to refinepose estimation. Our framework also naturally extends to dynamic scenes, whereeach frame contains a mirror reflection, enabling efficient per-frame geometryrecovery. For quantitative evaluation, we provide a fully customizablesynthetic dataset of 16 Blender scenes, each with ground-truth point clouds andcamera poses. Extensive experiments on real-world data and synthetic data areconducted to illustrate the effectiveness of our method.</description>
      <author>example@mail.com (Jing Wu, Zirui Wang, Iro Laina, Victor Adrian Prisacariu)</author>
      <guid isPermaLink="false">2509.20607v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Fast Estimation of Wasserstein Distances via Regression on Sliced Wasserstein Distances</title>
      <link>http://arxiv.org/abs/2509.20508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 20 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于切片Wasserstein距离回归的快速估计方法，用于高效计算多对分布之间的Wasserstein距离。该方法结合标准SW距离和提升SW距离，通过两个线性模型实现精确预测，在多种任务和数据集上优于现有方法，并能加速Wasserstein Wormhole训练。&lt;h4&gt;背景&lt;/h4&gt;Wasserstein距离是衡量分布间差异的重要工具，但计算多对分布之间的Wasserstein距离通常计算成本高昂。现有方法在低数据情况下表现不佳，需要更高效的计算方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效、准确估计多对分布之间Wasserstein距离的方法，特别是在数据有限的情况下，同时探索该方法在加速现有Wasserstein计算框架中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;提出基于切片Wasserstein距离回归的估计方法，同时利用标准SW距离（提供下界）和提升SW距离（提供上界）作为预测因子。设计了两个线性模型：一个具有闭式最小二乘解的无约束模型，和一个参数减半的约束模型。模型可以从少量分布对中学习，然后通过SW距离的线性组合快速预测任意分布对的Wasserstein距离。&lt;h4&gt;主要发现&lt;/h4&gt;在多种任务（高斯混合、点云分类、3D点云可视化）和数据集（MNIST点云、ShapeNetV2、MERFISH Cell Niches、scRNA-seq）上验证了该方法的有效性。结果表明，在低数据情况下，该方法比最先进的Wasserstein嵌入模型Wasserstein Wormhole提供更好的Wasserstein距离近似。此外，该估计器可以加速Wormhole训练，产生RG-Wormhole。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法显著提高了Wasserstein距离的计算效率，同时保持了准确性，特别是在数据有限的情况下。该方法不仅独立应用时表现优异，还能作为加速器改进现有Wasserstein计算框架，为分布间距离计算提供了新的实用工具。&lt;h4&gt;翻译&lt;/h4&gt;我们解决了从元分布中抽取的多对分布之间高效计算Wasserstein距离的问题。为此，我们提出了一种基于对切片Wasserstein距离回归的快速估计方法。具体来说，我们利用标准SW距离（提供下界）和提升SW距离（提供上界）作为真实Wasserstein距离的预测因子。为确保简洁性，我们引入了两个线性模型：一个具有闭式最小二乘解的无约束模型，以及一个参数减半的约束模型。我们证明可以从少量分布对中学习准确的模型。一旦估计完成，模型可以通过SW距离的线性组合预测任意分布对的Wasserstein距离，使其具有很高的效率。我们在多种任务上经验性地验证了我们的方法，包括高斯混合、点云分类和3D点云的Wasserstein空间可视化。在MNIST点云、ShapeNetV2、MERFISH Cell Niches和scRNA-seq等各种数据集上，我们的方法始终比最先进的Wasserstein嵌入模型Wasserstein Wormhole更好地近似Wasserstein距离，特别是在低数据情况下。最后，我们证明我们的估计器也可以加速Wormhole训练，产生RG-Wormhole。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We address the problem of efficiently computing Wasserstein distances formultiple pairs of distributions drawn from a meta-distribution. To this end, wepropose a fast estimation method based on regressing Wasserstein distance onsliced Wasserstein (SW) distances. Specifically, we leverage both standard SWdistances, which provide lower bounds, and lifted SW distances, which provideupper bounds, as predictors of the true Wasserstein distance. To ensureparsimony, we introduce two linear models: an unconstrained model with aclosed-form least-squares solution, and a constrained model that uses only halfas many parameters. We show that accurate models can be learned from a smallnumber of distribution pairs. Once estimated, the model can predict theWasserstein distance for any pair of distributions via a linear combination ofSW distances, making it highly efficient. Empirically, we validate our approachon diverse tasks, including Gaussian mixtures, point-cloud classification, andWasserstein-space visualizations for 3D point clouds. Across various datasetssuch as MNIST point clouds, ShapeNetV2, MERFISH Cell Niches, and scRNA-seq, ourmethod consistently provides a better approximation of Wasserstein distancethan the state-of-the-art Wasserstein embedding model, Wasserstein Wormhole,particularly in low-data regimes. Finally, we demonstrate that our estimatorcan also accelerate Wormhole training, yielding \textit{RG-Wormhole}.</description>
      <author>example@mail.com (Khai Nguyen, Hai Nguyen, Nhat Ho)</author>
      <guid isPermaLink="false">2509.20508v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2509.21113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MOSS-ChatV是一种基于强化学习的视频推理框架，通过动态时间规整(DTW)过程奖励解决了多模态大语言模型在视频推理中的过程不一致性问题，提高了模型的解释性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;视频推理已成为多模态大语言模型的关键能力，要求模型超越静态感知，对复杂场景中的时间动态进行连贯理解。然而现有MLLMs常表现出过程不一致性，即使最终答案正确，中间推理也会偏离视频动态。&lt;h4&gt;目的&lt;/h4&gt;解决现有MLLMs在视频推理过程中的过程不一致性问题，提高模型的解释性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入MOSS-ChatV框架，使用基于动态时间规整的过程奖励对齐推理轨迹与时间锚定的参考，无需辅助奖励模型即可实现高效过程监督；构建MOSS-Video基准，带有注释的推理轨迹，训练集用于微调MOSS-ChatV，保留集用于评估。&lt;h4&gt;主要发现&lt;/h4&gt;MOSS-ChatV在MOSS-Video测试集上达到87.2%的准确率，并在MVBench和MMVU等通用视频基准上提升性能；该框架在不同架构(包括Qwen2.5-VL和Phi-2)中均一致取得提升；使用GPT-4o作为评判者评估表明，MOSS-ChatV产生更一致和稳定的推理轨迹。&lt;h4&gt;结论&lt;/h4&gt;MOSS-ChatV框架有效解决了MLLMs在视频推理中的过程不一致性问题，提高了模型的解释性和鲁棒性，具有广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;视频推理已成为多模态大语言模型的关键能力，要求模型超越静态感知，对复杂场景中的时间动态进行连贯理解。然而现有MLLMs常表现出过程不一致性，即使最终答案正确，中间推理也会偏离视频动态，损害了模型的解释性和鲁棒性。为解决这一问题，我们引入MOSS-ChatV，这是一个基于强化学习的框架，使用基于动态时间规整的过程奖励。这种基于规则的奖励将推理轨迹与时间上锚定的参考对齐，无需辅助奖励模型即可实现高效的过程监督。我们进一步将动态状态预测识别为视频推理的关键度量，并构建了MOSS-Video基准，带有注释的推理轨迹，其中训练集用于微调MOSS-ChatV，保留集用于评估。MOSS-ChatV在MOSS-Video(测试集)上达到87.2%的准确率，并在MVBench和MMVU等通用视频基准上提高了性能。该框架在不同架构(包括Qwen2.5-VL和Phi-2)中均一致地取得提升，证实了其广泛的适用性。使用GPT-4o作为评判者的进一步评估表明，MOSS-ChatV产生了更一致和稳定的推理轨迹。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video reasoning has emerged as a critical capability for multimodal largelanguage models (MLLMs), requiring models to move beyond static perceptiontoward coherent understanding of temporal dynamics in complex scenes. Yetexisting MLLMs often exhibit process inconsistency, where intermediatereasoning drifts from video dynamics even when the final answer is correct,undermining interpretability and robustness. To address this issue, weintroduce MOSS-ChatV, a reinforcement learning framework with a Dynamic TimeWarping (DTW)-based process reward. This rule-based reward aligns reasoningtraces with temporally grounded references, enabling efficient processsupervision without auxiliary reward models. We further identify dynamic stateprediction as a key measure of video reasoning and construct MOSS-Video, abenchmark with annotated reasoning traces, where the training split is used tofine-tune MOSS-ChatV and the held-out split is reserved for evaluation.MOSS-ChatV achieves 87.2\% on MOSS-Video (test) and improves performance ongeneral video benchmarks such as MVBench and MMVU. The framework consistentlyyields gains across different architectures, including Qwen2.5-VL and Phi-2,confirming its broad applicability. Evaluations with GPT-4o-as-judge furthershow that MOSS-ChatV produces more consistent and stable reasoning traces.</description>
      <author>example@mail.com (Sicheng Tao, Jungang Li, Yibo Yan, Junyan Zhang, Yubo Gao, Hanqian Li, ShuHang Xun, Yuxuan Fan, Hong Chen, Jianxiang He, Xuming Hu)</author>
      <guid isPermaLink="false">2509.21113v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>A sub-hourly spatio-temporal statistical model for solar irradiance in Ireland using open-source data</title>
      <link>http://arxiv.org/abs/2509.21041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种新的贝叶斯时空建模框架，用于预测爱尔兰全境的小时和亚小时（10分钟）分辨率的太阳辐照度，该模型在预测精度、不确定性量化、可扩展性和实时实施能力方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;爱尔兰具有高度变化的海洋性气候，地面测量站分布稀疏，这使得选择合适的太阳辐照度数据集成为一个重大挑战。准确的太阳辐照度估计对可靠建模太阳能光伏（PV）发电量至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的贝叶斯时空建模框架，用于预测爱尔兰全境的小时和亚小时（10分钟）分辨率的太阳辐照度。&lt;h4&gt;方法&lt;/h4&gt;使用贝叶斯时空建模框架进行太阳辐照度预测，通过交叉验证评估模型的统计鲁棒性，并与替代数据源（包括再分析数据集和最近站点插值）进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;模型在所有时间分辨率上都具有统计鲁棒性；小时分辨率显示最高的预测精度，而10分钟分辨率遇到更高的误差但更好的不确定性量化；与替代数据源相比，模型始终提供更优的特定站点准确性；在小时尺度上，模型优于ERA5与地面观测的一致性；在亚小时尺度上，10分钟分辨率估计与爱尔兰住宅和工业太阳能光伏装置的太阳能光伏电力输出一致。&lt;h4&gt;结论&lt;/h4&gt;该模型不仅超越了现有数据集，还提供了完整的不确定性量化、可扩展性和实时实施能力，为太阳能预测和由于逆变器尺寸不足导致的过载削波损失估计提供了强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;准确的太阳辐照度估计对于可靠地建模太阳能光伏（PV）发电量至关重要。在爱尔兰高度变化的海洋性气候中，地面测量站分布稀疏，选择合适的太阳辐照度数据集是一个重大挑战。本研究引入了一种新的贝叶斯时空建模框架，用于预测爱尔兰全境小时和亚小时（10分钟）分辨率的太阳辐照度。交叉验证表明，我们的模型在所有时间分辨率上都具有统计鲁棒性，其中小时分辨率显示出最高的预测精度，而10分钟分辨率遇到更高的误差但更好的不确定性量化。在单独的评估中，我们将我们的模型与替代数据源（包括再分析数据集和最近站点插值）进行比较，发现它始终提供更优的特定站点准确性。在小时尺度上，我们的模型在符合地面观测方面优于ERA5。在亚小时尺度上，10分钟分辨率估计与爱尔兰住宅和工业太阳能光伏装置的太阳能光伏电力输出一致。除了超越现有数据集外，我们的模型提供了完整的不确定性量化、可扩展性和实时实施能力，为太阳能预测和由于逆变器尺寸不足导致的过载削波损失估计提供了强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate estimation of solar irradiance is essential for reliable modellingof solar photovoltaic (PV) power production. In Ireland's highly variablemaritime climate, where ground-based measurement stations are sparselydistributed, selecting an appropriate solar irradiance dataset presents asignificant challenge. This study introduces a novel Bayesian spatio-temporalmodelling framework for predicting solar irradiance at hourly and sub-hourly(10-minute) resolutions across Ireland. Cross-validation demonstrates that ourmodel is statistically robust across all temporal resolutions with hourlyshowing highest prediction precision whereas 10-minute resolution encountershigher errors but better uncertainty quantification. In separate evaluations,we compare our model against alternative data sources, including reanalysisdatasets and nearest-station interpolation, and find that it consistentlyprovides superior site-specific accuracy. At the hourly scale, our modeloutperforms ERA5 in agreement with ground-based observations. At the sub-hourlyscale, 10-minute resolution estimates provide solar PV power outputs consistentwith residential and industrial solar PV installations in Ireland. Beyondsurpassing existing datasets, our model delivers full uncertaintyquantification, scalability and the capacity for real-time implementation,offering a powerful tool for solar energy prediction and the estimation oflosses due to overload clipping from inverter undersizing.</description>
      <author>example@mail.com (Maeve Upton, Eamonn Organ, Amanda Lenzi, James Sweeney)</author>
      <guid isPermaLink="false">2509.21041v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Nuclear Diffusion Models for Low-Rank Background Suppression in Videos</title>
      <link>http://arxiv.org/abs/2509.20886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures, preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Nuclear Diffusion的混合框架，结合低秩时间建模与扩散后验采样，用于视频序列的去噪和修复，特别是在医学成像领域取得了改进的性能。&lt;h4&gt;背景&lt;/h4&gt;视频序列通常包含结构化噪声和背景伪影，这些会掩盖动态内容，对准确分析和修复构成挑战。&lt;h4&gt;目的&lt;/h4&gt;克服传统稳健主成分方法(RPCA)中稀疏性假设无法捕捉真实视频数据中丰富变异性的局限。&lt;h4&gt;方法&lt;/h4&gt;开发了一种混合框架，整合低秩时间建模与扩散后验采样，称为Nuclear Diffusion方法，并在心脏超声去雾问题上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;与传统RPCA相比，Nuclear Diffusion在对比度增强(gCNR)和信号保留(KS统计)方面展示了改进的去雾性能。&lt;h4&gt;结论&lt;/h4&gt;结合基于模型的时间模型与深度生成先验在高保真视频修复方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;视频序列通常包含结构化噪声和背景伪影，这些会掩盖动态内容，对准确分析和修复构成挑战。稳健的主成分方法通过将数据分解为低秩和稀疏成分来解决此问题。然而，稀疏性假设通常无法捕捉真实视频数据中存在的丰富变异性。为了克服这一限制，提出了一种混合框架，该框架结合了低秩时间建模和扩散后验采样。提出的方法（Nuclear Diffusion）在真实世界的医学成像问题（即心脏超声去雾）上进行了评估，与传统RPCA相比，在对比度增强（gCNR）和信号保留（KS统计）方面展示了改进的去雾性能。这些结果突显了结合基于模型的时间模型与深度生成先验进行高保真视频修复的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video sequences often contain structured noise and background artifacts thatobscure dynamic content, posing challenges for accurate analysis andrestoration. Robust principal component methods address this by decomposingdata into low-rank and sparse components. Still, the sparsity assumption oftenfails to capture the rich variability present in real video data. To overcomethis limitation, a hybrid framework that integrates low-rank temporal modelingwith diffusion posterior sampling is proposed. The proposed method, NuclearDiffusion, is evaluated on a real-world medical imaging problem, namely cardiacultrasound dehazing, and demonstrates improved dehazing performance compared totraditional RPCA concerning contrast enhancement (gCNR) and signal preservation(KS statistic). These results highlight the potential of combining model-basedtemporal models with deep generative priors for high-fidelity videorestoration.</description>
      <author>example@mail.com (Tristan S. W. Stevens, Oisín Nolan, Jean-Luc Robert, Ruud J. G. van Sloun)</author>
      <guid isPermaLink="false">2509.20886v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors</title>
      <link>http://arxiv.org/abs/2509.17084v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了MoCLIP-Lite，一种简单而强大的双流晚期融合框架，用于高效视频动作识别，结合了CLIP图像编码器和运动向量的优势，实现了89.2%的Top-1准确率。&lt;h4&gt;背景&lt;/h4&gt;视频动作识别是计算机视觉的基础任务，但现有先进模型计算成本高且依赖大量视频预训练。同时，CLIP等视觉语言模型在静态图像上具有强大零样本能力，运动向量则从压缩视频流中提供高效时间信息。&lt;h4&gt;目的&lt;/h4&gt;结合CLIP模型和运动向量的优势，开发一种高效的视频识别框架，减少计算成本并提高性能。&lt;h4&gt;方法&lt;/h4&gt;MoCLIP-Lite将冻结的CLIP图像编码器特征与在原始运动向量上训练的轻量级监督网络特征相结合。融合过程中，两个主干网络均保持冻结状态，仅训练一个小型多层感知机(MLP)头部，确保高效率。&lt;h4&gt;主要发现&lt;/h4&gt;在UCF101数据集上，该方法达到89.2%的Top-1准确率，显著优于零样本基线(65.0%)和仅使用运动向量的基线(66.5%)。&lt;h4&gt;结论&lt;/h4&gt;该工作为视频理解提供了新的高效基线，有效连接了大型静态模型和动态低成本运动线索。&lt;h4&gt;翻译&lt;/h4&gt;视频动作识别是计算机视觉的基础任务，但最先进的模型通常计算成本高且依赖大量视频预训练。同时，像CLIP这样的大规模视觉语言模型在静态图像上提供了强大的零样本能力，而运动向量(MV)则直接从压缩视频流中提供高效的时间信息。为了结合这些范式的优势，我们提出了MoCLIP-Lite，一种简单而强大的双流晚期融合框架，用于高效的视频识别。我们的方法将冻结的CLIP图像编码器的特征与在原始运动向量上训练的轻量级监督网络的特征相结合。在融合过程中，两个主干网络都被冻结，只训练一个微小的多层感知机(MLP)头部，确保极高的效率。在UCF101数据集上的全面实验表明，我们的方法实现了89.2%的Top-1准确率，显著优于强大的零样本(65.0%)和仅使用运动向量(66.5%)的基线。我们的工作为视频理解提供了一个新的、高效的基线，有效地连接了大型静态模型和动态、低成本的运动线索。我们的代码和模型可在https://github.com/microa/MoCLIP-Lite获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video action recognition is a fundamental task in computer vision, butstate-of-the-art models are often computationally expensive and rely onextensive video pre-training. In parallel, large-scale vision-language modelslike Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shotcapabilities on static images, while motion vectors (MV) provide highlyefficient temporal information directly from compressed video streams. Tosynergize the strengths of these paradigms, we propose MoCLIP-Lite, a simpleyet powerful two-stream late fusion framework for efficient video recognition.Our approach combines features from a frozen CLIP image encoder with featuresfrom a lightweight, supervised network trained on raw MV. During fusion, bothbackbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head istrained, ensuring extreme efficiency. Through comprehensive experiments on theUCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)baselines. Our work provides a new, highly efficient baseline for videounderstanding that effectively bridges the gap between large static models anddynamic, low-cost motion cues. Our code and models are available athttps://github.com/microa/MoCLIP-Lite.</description>
      <author>example@mail.com (Binhua Huang, Ni Wang, Arjun Pakrashi, Soumyabrata Dev)</author>
      <guid isPermaLink="false">2509.17084v2</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</title>
      <link>http://arxiv.org/abs/2509.21320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一个科学推理基础模型，能够将自然语言与异构科学表示对齐，支持多种科学推理任务，并在多个方面优于专业系统。&lt;h4&gt;背景&lt;/h4&gt;科学领域需要能够处理多种科学表示形式并具备复杂推理能力的系统，而现有专业系统可能覆盖范围有限，跨领域泛化能力不足或保真度不够。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够将自然语言与异构科学表示对齐的基础模型，支持多种科学推理任务，提高跨领域泛化能力，并增强保真度。&lt;h4&gt;方法&lt;/h4&gt;在206B-token的多模态科学语料库上预训练，使用40M指令进行监督微调，采用退火冷启动自举技术引发长链思维，通过任务特定的奖励塑造进行强化学习，支持四种能力家族涵盖103项任务，并开源相关资源。&lt;h4&gt;主要发现&lt;/h4&gt;该方法比专业系统具有更广泛的指令覆盖范围，提高了跨领域泛化能力，增强了保真度，且跨学科学习增强了迁移能力和下游可靠性。&lt;h4&gt;结论&lt;/h4&gt;该科学推理基础模型能有效处理多种科学表示形式，支持广泛的科学推理任务，研究团队已开源模型、指令调优数据集和评估代码。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个科学推理基础模型，该模型将自然语言与异构科学表示对齐。该模型在包含科学文本、纯序列和序列-文本对的206B-token语料库上进行了预训练，然后通过40M指令的监督微调进行对齐，采用退火冷启动自举来引发长链思维，并通过任务特定的奖励塑造进行强化学习，从而培养深思熟虑的科学推理能力。它支持四种能力家族，涵盖工作流中的103项任务：(i)文本与科学格式之间的忠实转换，(ii)文本/知识提取，(iii)属性预测，(iv)属性分类，(v)无条件和条件序列生成与设计。与专业系统相比，我们的方法扩展了指令覆盖范围，提高了跨领域泛化能力，并增强了保真度。我们详细介绍了数据整理和训练过程，并证明跨学科学习增强了迁移能力和下游可靠性。该模型、指令调优数据集和评估代码已在https://huggingface.co/SciReason和https://github.com/open-sciencelab/SciReason开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a scientific reasoning foundation model that aligns naturallanguage with heterogeneous scientific representations. The model is pretrainedon a 206B-token corpus spanning scientific text, pure sequences, andsequence-text pairs, then aligned via SFT on 40M instructions, annealedcold-start bootstrapping to elicit long-form chain-of-thought, andreinforcement learning with task-specific reward shaping, which instillsdeliberate scientific reasoning. It supports four capability families, coveringup to 103 tasks across workflows: (i) faithful translation between text andscientific formats, (ii) text/knowledge extraction, (iii) property prediction,(iv) property classification, (v) unconditional and conditional sequencegeneration and design. Compared with specialist systems, our approach broadensinstruction coverage, improves cross-domain generalization, and enhancesfidelity. We detail data curation and training and show that cross-disciplinelearning strengthens transfer and downstream reliability. The model, instructtuning datasets and the evaluation code are open-sourced athttps://huggingface.co/SciReason andhttps://github.com/open-sciencelab/SciReason.</description>
      <author>example@mail.com (Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai)</author>
      <guid isPermaLink="false">2509.21320v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>A Sentinel-3 foundation model for ocean colour</title>
      <link>http://arxiv.org/abs/2509.21273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种基于Prithvi-EO Vision Transformer架构的新型基础模型，该模型在Sentinel-3 OLCI数据上预训练，用于海洋科学应用。研究通过在两个下游海洋地球观测任务上微调来评估模型性能，结果表明该基础模型在海洋监测中具有显著效用，能够利用少量高质量标记数据并捕捉海洋颜色的详细空间模式。&lt;h4&gt;背景&lt;/h4&gt;在海洋科学领域，标记数据通常稀少且收集成本高昂，这限制了人工智能应用的发展。基础模型(FMs)是在大量未标记数据上预训练的AI模型，有潜力改变这一状况。&lt;h4&gt;目的&lt;/h4&gt;开发并评估一种新型基础模型，用于海洋科学应用，特别是在量化叶绿素浓度和改进海洋初级生产力估算方面的应用。&lt;h4&gt;方法&lt;/h4&gt;使用Prithvi-EO Vision Transformer架构构建基础模型，在Sentinel-3海洋和陆地色彩仪器(OLCI)数据上预训练用于数据重建。然后通过在两个下游海洋地球观测任务上微调来评估模型：1)与当前用于量化叶绿素浓度的基线模型比较性能；2)评估改进基于遥感的海洋初级生产力估算的能力。&lt;h4&gt;主要发现&lt;/h4&gt;自训练的基础模型在海洋监测中表现出显著效用，能够有效利用少量高质量标记数据，同时捕捉海洋颜色的详细空间模式并与点观测相匹配。&lt;h4&gt;结论&lt;/h4&gt;新一代地理空间AI模型有潜力为海洋生态系统及其在全球气候过程中的作用提供更稳健、数据驱动的见解，为海洋科学研究提供了新的工具和方法。&lt;h4&gt;翻译&lt;/h4&gt;人工智能(AI)基础模型(FMs)在大量未标记数据上预训练，有可能彻底改变海洋科学中的AI应用，因为标记数据通常稀少且收集成本高昂。在本工作中，我们描述了一种使用Prithvi-EO Vision Transformer架构的新型基础模型，该模型已在Sentinel-3海洋和陆地色彩仪器(OLCI)数据上预训练用于数据重建。我们通过在两个下游海洋地球观测任务上进行微调来评估模型。首先，我们评估了模型与当前用于量化叶绿素浓度的基线模型相比的性能。然后，我们评估了该基础模型改进基于遥感的海洋初级生产力估算的能力。我们的结果证明了自训练基础模型在海洋监测中的效用，特别是能够利用少量高质量标记数据并捕捉海洋颜色的详细空间模式，同时与点观测相匹配。我们得出结论，新一代地理空间AI模型有可能为海洋生态系统及其在全球气候过程中的作用提供更稳健、数据驱动的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massiveunlabelled datasets, have the potential to drastically change AI applicationsin ocean science, where labelled data are often sparse and expensive tocollect. In this work, we describe a new foundation model using the Prithvi-EOVision Transformer architecture which has been pre-trained to reconstruct datafrom the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate themodel by fine-tuning on two downstream marine earth observation tasks. We firstassess model performance compared to current baseline models used to quantifychlorophyll concentration. We then evaluate the FMs ability to refine remotesensing-based estimates of ocean primary production. Our results demonstratethe utility of self-trained FMs for marine monitoring, in particular for makinguse of small amounts of high quality labelled data and in capturing detailedspatial patterns of ocean colour whilst matching point observations. Weconclude that this new generation of geospatial AI models has the potential toprovide more robust, data-driven insights into ocean ecosystems and their rolein global climate processes.</description>
      <author>example@mail.com (Geoffrey Dawson, Remy Vandaele, Andrew Taylor, David Moffat, Helen Tamura-Wicks, Sarah Jackson, Rosie Lickorish, Paolo Fraccaro, Hywel Williams, Chunbo Luo, Anne Jones)</author>
      <guid isPermaLink="false">2509.21273v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Dense Semantic Matching with VGGT Prior</title>
      <link>http://arxiv.org/abs/2509.21263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于VGGT的语义匹配方法，解决了现有方法在几何模糊性和最近邻规则方面的局限性，实现了优越的几何感知能力、匹配可靠性和流形保持。&lt;h4&gt;背景&lt;/h4&gt;语义匹配是计算机视觉中的基础任务，旨在建立同一类别实例之间的像素级对应关系。现有方法存在两个局限性：(i) 几何模糊性：依赖于2D基础模型特征往往无法区分对称结构，需要额外微调但缺乏泛化能力；(ii) 最近邻规则：逐像素匹配忽略了跨图像不可见性和流形保持。&lt;h4&gt;目的&lt;/h4&gt;开发具有几何感知能力的像素描述符和整体密集对应机制，以改进语义匹配性能。&lt;h4&gt;方法&lt;/h4&gt;受3D几何基础模型启发，利用VGGT的基于几何的特征和整体密集匹配能力。通过重用早期特征阶段、微调后期阶段和添加双向对应的语义头保留VGGT优势；并通过循环一致训练策略、合成数据增强和渐进式训练方案（带有混叠伪影缓解）使VGGT适应数据稀缺的语义匹配场景。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法实现了优越的几何感知能力、匹配可靠性和流形保持，性能优于以前的基线方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了现有语义匹配方法的局限性，通过利用并适应VGGT的优势，显著提升了语义匹配的性能。&lt;h4&gt;翻译&lt;/h4&gt;语义匹配旨在建立同一类别实例之间的像素级对应关系，是计算机视觉中的基础任务。现有方法存在两种局限性：(i) 几何模糊性：它们对2D基础模型特征（如Stable Diffusion、DINO）的依赖往往无法区分对称结构，需要额外微调但缺乏泛化能力；(ii) 最近邻规则：它们的逐像素匹配忽略了跨图像不可见性和流形保持。这些挑战需要具有几何感知能力的像素描述符和整体密集对应机制。受3D几何基础模型最新进展的启发，我们转向使用VGGT，它提供了基于几何的特征和整体密集匹配能力，很好地满足了这些需求。然而，直接迁移VGGT具有挑战性，因为它原本是为单个实例的跨视图几何匹配设计的，与跨实例语义匹配不匹配，并且受到密集语义注释稀缺的阻碍。为此，我们提出了一种方法：(i) 通过重用早期特征阶段、微调后期阶段和添加双向对应的语义头，保留VGGT的内在优势；(ii) 通过循环一致训练策略、合成数据增强和带有混叠伪影缓解的渐进式训练方案，使VGGT适应数据稀缺的语义匹配场景。大量实验证明，我们的方法实现了优越的几何感知能力、匹配可靠性和流形保持，优于以前的基线。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决语义匹配中的两个关键问题：几何歧义性（难以区分对称结构如左右眼）和最近邻规则限制（忽略跨图像不可见性和流形保持）。这个问题在计算机视觉中很重要，因为语义匹配是建立同类实例间像素级对应关系的基础技术，广泛应用于2D操作（如风格迁移）、3D分析（如变形）和机器人技术（如功能学习）等领域，现有方法的局限性严重影响了这些应用的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到语义匹配本质上是3D问题，但现有方法主要使用2D特征，缺乏3D几何感知。他们发现3D几何基础模型VGGT提供基于几何的特征和整体密集匹配能力，与需求高度匹配。然而，直接迁移VGGT存在挑战：它原本设计用于单个实例的跨视图几何匹配，不完全符合跨实例语义匹配的目标；同时语义匹配面临密集标注数据稀缺问题。因此，作者设计架构调整（重用早期特征、微调后期特征、添加语义头）和循环一致训练策略来解决这些问题。该方法借鉴了VGGT的3D几何基础模型工作，同时也吸收了语义匹配领域的循环一致性和弱监督学习策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用VGGT的3D几何先验解决语义匹配中的几何歧义性问题，同时保持流形映射特性，并通过循环一致训练策略处理跨图像不可见性问题。整体流程包括：1)架构设计：重用VGGT早期块提取几何特征，微调后期块获取语义特征，添加语义匹配头预测双向采样网格和置信度图；2)训练策略：采用循环一致训练（匹配-重建一致性和误差-置信度相关性）、合成数据生成和渐进式训练（四个阶段逐步训练）；3)损失函数：包括监督损失、循环一致性损失、平滑损失和不确定性损失；4)推理过程：输入图像通过特征提取、主干细化和语义匹配头输出对应关系和置信度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将VGGT适应于密集语义匹配，利用其3D几何先验解决几何歧义并保持流形特性；2)提出循环一致训练策略，通过匹配-重建一致性和误差-置信度相关性解决跨图像不可见性；3)创建合成数据管道和渐进式训练方案，减少对密集标注数据的依赖。相比之前工作，该方法具有更强的几何感知能力（能更好区分对称结构）、保持流形结构（大多数现有方法无法做到）、处理非刚性变形（优于SpaceJAM的全局变换）、预测置信度图（解决跨图像不可见性问题）以及更高的训练效率（减少标注数据依赖）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过将VGGT的3D几何先验与创新的循环一致训练策略相结合，首次实现了具有强大几何感知能力、流形保持特性和可靠置信度预测的密集语义匹配，解决了现有方法在几何歧义性和跨图像不可见性方面的关键局限。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic matching aims to establish pixel-level correspondences betweeninstances of the same category and represents a fundamental task in computervision. Existing approaches suffer from two limitations: (i) GeometricAmbiguity: Their reliance on 2D foundation model features (e.g., StableDiffusion, DINO) often fails to disambiguate symmetric structures, requiringextra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Theirpixel-wise matching ignores cross-image invisibility and neglects manifoldpreservation. These challenges call for geometry-aware pixel descriptors andholistic dense correspondence mechanisms. Inspired by recent advances in 3Dgeometric foundation models, we turn to VGGT, which provides geometry-groundedfeatures and holistic dense matching capabilities well aligned with theseneeds. However, directly transferring VGGT is challenging, as it was originallydesigned for geometry matching within cross views of a single instance,misaligned with cross-instance semantic matching, and further hindered by thescarcity of dense semantic annotations. To address this, we propose an approachthat (i) retains VGGT's intrinsic strengths by reusing early feature stages,fine-tuning later ones, and adding a semantic head for bidirectionalcorrespondences; and (ii) adapts VGGT to the semantic matching scenario underdata scarcity through cycle-consistent training strategy, synthetic dataaugmentation, and progressive training recipe with aliasing artifactmitigation. Extensive experiments demonstrate that our approach achievessuperior geometry awareness, matching reliability, and manifold preservation,outperforming previous baselines.</description>
      <author>example@mail.com (Songlin Yang, Tianyi Wei, Yushi Lan, Zeqi Xiao, Anyi Rao, Xingang Pan)</author>
      <guid isPermaLink="false">2509.21263v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations</title>
      <link>http://arxiv.org/abs/2509.21249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Decipher-MR，一个3D MRI特定的视觉语言基础模型，通过大规模数据集训练和模块化设计，实现了在多样化临床任务中的高效应用和性能提升。&lt;h4&gt;背景&lt;/h4&gt;MRI是临床诊断和研究的关键医学成像方式，但其复杂性和异质性对自动化分析构成挑战。基础模型在自然语言和视觉任务中表现优异，但在MRI应用中受限于数据稀缺和狭窄的解剖学焦点。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展、可泛化的MRI基础模型，能够在广泛的临床和研究任务中有效应用，克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;构建了Decipher-MR模型，在包含200,000个MRI系列（来自22,000多项研究）的大规模数据集上训练，涵盖多样解剖区域、序列和病理。结合自监督视觉学习和报告引导的文本监督，采用模块化设计，支持轻量级任务特定解码器附加到冻结的预训练编码器。&lt;h4&gt;主要发现&lt;/h4&gt;在疾病分类、人口统计预测、解剖定位和跨模态检索等多样化基准测试中，Decipher-MR展现出超越现有基础模型和特定任务方法的性能，证明了其有效性和通用性。&lt;h4&gt;结论&lt;/h4&gt;Decipher-MR确立了作为基于MRI的AI的可扩展和多功能基础，能够促进临床和研究领域的高效发展，为MRI分析提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;磁共振成像（MRI）是临床诊断和研究中的关键医学成像方式，但其复杂性和异质性对自动化分析构成挑战，特别是在可扩展和可泛化的机器学习应用方面。虽然基础模型已经革命化了自然语言和视觉任务，但由于数据稀缺和狭窄的解剖学焦点，它们在MRI中的应用仍然有限。在这项工作中，我们提出了Decipher-MR，这是一个3D MRI特定的视觉语言基础模型，在包含来自22,000多项研究的200,000个MRI系列的大规模数据集上训练，涵盖了多样的解剖区域、序列和病理。Decipher-MR将自监督视觉学习与报告引导的文本监督相结合，构建强大、可泛化的表示，实现跨广泛应用的有效适应。为了以最小的计算开销实现强大和多样的临床任务，Decipher-MR支持模块化设计，能够将轻量级、特定任务的解码器附加到冻结的预训练编码器上。在此设置下，我们在疾病分类、人口统计预测、解剖定位和跨模态检索等多种基准上评估了Decipher-MR，展示了与现有基础模型和特定任务方法相比的一致性能提升。我们的结果确立了Decipher-MR作为基于MRI的AI的可扩展和多功能基础，促进了临床和研究领域的高效发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决MRI（磁共振成像）分析的自动化挑战，特别是开发能够处理MRI数据复杂性和异质性的基础模型。这个问题很重要，因为MRI是临床诊断和研究中的关键医学成像方式，但其数据的复杂性和多样性限制了机器学习应用的可扩展性和泛化能力。传统方法依赖大量标记数据且难以跨不同扫描仪、数据源和临床任务泛化，影响诊断准确性、临床工作流程效率和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到MRI分析面临的挑战，特别是缺乏专门针对MRI的基础模型。他们分析了现有基础模型在自然语言处理和计算机视觉领域的成功应用，以及医学成像领域（如X光、CT）的基础模型经验。设计思路包括构建专门针对3D MRI的基础模型，使用大规模多样化数据集，结合自监督视觉学习和文本监督，采用模块化设计。他们借鉴了DINOv2的学生-教师自监督学习、CLIP的对比学习方法，以及PubMedBERT作为文本编码器基础，但专门针对MRI数据特点进行了优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是开发一个专门针对3D MRI的视觉-语言基础模型，通过大规模多样化MRI数据和相应放射学报告进行训练，结合自监督视觉学习和文本监督构建强大可泛化的表示，并采用模块化设计支持轻量级任务特定解码器的灵活适配。整体流程包括：1)收集大规模多样化MRI数据集；2)两阶段预训练（第一阶段独立预训练图像和文本编码器，第二阶段进行图像-报告对比预训练）；3)使用3D Vision Transformer作为图像编码器，PubMedBERT作为文本编码器；4)自定义数据增强和基于器官的采样策略；5)冻结预训练编码器，附加和微调轻量级任务特定解码器进行多种任务评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)专门针对3D MRI的基础模型；2)使用超过20万个MRI系列的大规模多样化数据集；3)结合自监督视觉学习和文本监督的两阶段预训练策略；4)支持冻结预训练编码器仅微调轻量级解码器的模块化设计；5)实现零样本跨模态检索能力。相比之前工作，不同之处在于：比现有MRI基础模型使用的数据集更大更多样化；专门针对MRI特性优化而非通用医学成像模型；结合图像自监督和文本监督比单一阶段方法更有效；模块化设计比端到端训练更高效；在跨模态检索等方面表现出色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Decipher-MR通过结合大规模多样化3D MRI数据与两阶段预训练策略，创建了一个强大的视觉-语言基础模型，能够高效适配多种临床任务，显著提升了MRI分析的自动化能力和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnetic Resonance Imaging (MRI) is a critical medical imaging modality inclinical diagnosis and research, yet its complexity and heterogeneity posechallenges for automated analysis, particularly in scalable and generalizablemachine learning applications. While foundation models have revolutionizednatural language and vision tasks, their application to MRI remains limited dueto data scarcity and narrow anatomical focus. In this work, we presentDecipher-MR, a 3D MRI-specific vision-language foundation model trained on alarge-scale dataset comprising 200,000 MRI series from over 22,000 studiesspanning diverse anatomical regions, sequences, and pathologies. Decipher-MRintegrates self-supervised vision learning with report-guided text supervisionto build robust, generalizable representations, enabling effective adaptationacross broad applications. To enable robust and diverse clinical tasks withminimal computational overhead, Decipher-MR supports a modular design thatenables tuning of lightweight, task-specific decoders attached to a frozenpretrained encoder. Following this setting, we evaluate Decipher-MR acrossdiverse benchmarks including disease classification, demographic prediction,anatomical localization, and cross-modal retrieval, demonstrating consistentperformance gains over existing foundation models and task-specific approaches.Our results establish Decipher-MR as a scalable and versatile foundation forMRI-based AI, facilitating efficient development across clinical and researchdomains.</description>
      <author>example@mail.com (Zhijian Yang, Noel DSouza, Istvan Megyeri, Xiaojian Xu, Amin Honarmandi Shandiz, Farzin Haddadpour, Krisztian Koos, Laszlo Rusko, Emanuele Valeriano, Bharadwaj Swaninathan, Lei Wu, Parminder Bhatia, Taha Kass-Hout, Erhan Bas)</author>
      <guid isPermaLink="false">2509.21249v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy</title>
      <link>http://arxiv.org/abs/2509.21190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TimeRCD是一种基于相对上下文差异(RCD)范式的新型时间序列异常检测基础模型，通过检测相邻时间窗口之间的显著差异来识别异常，而非依赖重建目标。该模型使用标准Transformer架构实现，在大规模多样化合成语料库上预训练，实验证明其在零样本时间序列异常检测任务中显著优于现有模型。&lt;h4&gt;背景&lt;/h4&gt;时间序列异常检测是一项关键任务，但在零样本方式下开发能泛化到未见数据的模型仍然是一个重大挑战。现有基础模型主要依赖重建目标，存在目标不匹配问题：难以识别细微异常，同时经常误判复杂正常模式，导致高假阴性和假阳性率。&lt;h4&gt;目的&lt;/h4&gt;克服现有重建基础模型的局限性，开发一种能够在零样本方式下有效识别时间序列异常的新型基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出TimeRCD模型，基于新的相对上下文差异(RCD)预训练范式。模型通过检测相邻时间窗口之间的显著差异来训练识别异常，而非重建输入。使用标准Transformer架构实现，在大规模多样化合成语料库(具有令牌级异常标签)上进行预训练，提供丰富的监督信号。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，TimeRCD在跨不同数据集的零样本时间序列异常检测任务中，显著优于现有通用和特定于异常的基础模型。结果验证了RCD范式的优越性。&lt;h4&gt;结论&lt;/h4&gt;相对上下文差异(RCD)范式为构建健壮且可泛化的时间序列异常检测基础模型建立了新的有效路径。&lt;h4&gt;翻译&lt;/h4&gt;时间序列异常检测(TSAD)是一项关键任务，但开发以零样本方式泛化到未见数据的模型仍然是一个重大挑战。现有的TSAD基础模型主要依赖基于重建的目标，存在基本目标不匹配问题：它们难以识别细微异常，同时经常误判复杂的正常模式，导致高假阴性和假阳性率。为克服这些限制，我们引入了TimeRCD，一种基于新预训练范式——相对上下文差异(RCD)的时间序列异常检测基础模型。TimeRCD不是学习重建输入，而是通过检测相邻时间窗口之间的显著差异来明确训练识别异常。这种使用标准Transformer架构实现的关系方法，使模型能够捕获重建方法经常忽略的异常指示性上下文变化。为支持这一范式，我们开发了一个大规模、多样化的合成语料库，具有令牌级异常标签，提供了有效预训练所需的丰富监督信号。大量实验表明，TimeRCD在跨不同数据集的零样本时间序列异常检测中，显著优于现有通用和特定于异常的基础模型。我们的结果验证了RCD范式的优越性，并为构建健壮且可泛化的时间序列异常检测基础模型建立了新的有效路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series anomaly detection (TSAD) is a critical task, but developingmodels that generalize to unseen data in a zero-shot manner remains a majorchallenge. Prevailing foundation models for TSAD predominantly rely onreconstruction-based objectives, which suffer from a fundamental objectivemismatch: they struggle to identify subtle anomalies while oftenmisinterpreting complex normal patterns, leading to high rates of falsenegatives and positives. To overcome these limitations, we introduce\texttt{TimeRCD}, a novel foundation model for TSAD built upon a newpre-training paradigm: Relative Context Discrepancy (RCD). Instead of learningto reconstruct inputs, \texttt{TimeRCD} is explicitly trained to identifyanomalies by detecting significant discrepancies between adjacent time windows.This relational approach, implemented with a standard Transformer architecture,enables the model to capture contextual shifts indicative of anomalies thatreconstruction-based methods often miss. To facilitate this paradigm, wedevelop a large-scale, diverse synthetic corpus with token-level anomalylabels, providing the rich supervisory signal necessary for effectivepre-training. Extensive experiments demonstrate that \texttt{TimeRCD}significantly outperforms existing general-purpose and anomaly-specificfoundation models in zero-shot TSAD across diverse datasets. Our resultsvalidate the superiority of the RCD paradigm and establish a new, effectivepath toward building robust and generalizable foundation models for time seriesanomaly detection.</description>
      <author>example@mail.com (Tian Lan, Hao Duong Le, Jinbo Li, Wenjun He, Meng Wang, Chenghao Liu, Chen Zhang)</author>
      <guid isPermaLink="false">2509.21190v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns</title>
      <link>http://arxiv.org/abs/2509.21124v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过选择高价值思维链数据来提升大型推理模型数学推理能力的方法，仅使用10B-token精选数据即可显著提高模型在AIME测试中的表现。&lt;h4&gt;背景&lt;/h4&gt;大型推理模型在数学推理领域的进展主要由强化学习驱动，中期训练中加入长思维链(CoT)数据能提高推理深度，但当前方法通常不加区分地使用所有CoT数据。&lt;h4&gt;目的&lt;/h4&gt;确定哪些类型的数据最能有效增强模型推理能力，并开发一种方法来筛选和利用这些高价值数据。&lt;h4&gt;方法&lt;/h4&gt;首次定义推理潜能为解决问题所需尝试次数的倒数；从CoT序列中抽象出原子推理模式构建核心参考集；提出双粒度算法选择与核心集一致的高价值CoT数据(CoTP)；使用这些数据训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;10B-token的CoTP数据使85A6B MoE模型在AIME 2024和2025测试中提高9.58%，将下游RL性能上限提高7.81%。&lt;h4&gt;结论&lt;/h4&gt;通过有选择地使用富含高价值推理模式的数据，可以显著提高大型推理模型在数学推理任务上的性能，而无需使用大量训练数据。&lt;h4&gt;翻译&lt;/h4&gt;近期在具有挑战性的数学推理领域大型推理模型的进展是由强化学习(RL)驱动的。在中期训练中加入长思维链(CoT)数据也被证明可以显著提高推理深度。然而，当前方法往往不加区分地使用CoT数据，关于哪种数据类型最有效地增强模型推理能力的关键问题仍未解决。在本文中，我们首次将基础模型的推理潜能定义为正确回答问题所需独立尝试次数的倒数，这与最终模型性能强相关。我们随后提出利用富含高价值推理模式的多样化数据来扩展推理潜能。具体而言，我们从CoT序列中抽象出具有共性和归纳能力的原子推理模式，并用它们构建富含有价值推理模式的核心参考集。此外，我们提出了一种涉及推理模式链和令牌熵的双粒度算法，从数据池中高效选择与核心集一致的高价值CoT数据(CoTP)，从而有效训练模型掌握推理能力。仅10B-token的CoTP数据就使85A6B Mixture-of-Experts (MoE)模型在具有挑战性的AIME 2024和2025上提高了9.58%，并将下游RL性能的上限提高了7.81%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in large reasoning models for challenging mathematicalreasoning has been driven by reinforcement learning (RL). Incorporating longchain-of-thought (CoT) data during mid-training has also been shown tosubstantially improve reasoning depth. However, current approaches oftenutilize CoT data indiscriminately, leaving open the critical question of whichdata types most effectively enhance model reasoning capabilities. In thispaper, we define the foundation model's reasoning potential for the first timeas the inverse of the number of independent attempts required to correctlyanswer the question, which is strongly correlated with the final modelperformance. We then propose utilizing diverse data enriched with high-valuereasoning patterns to expand the reasoning potential. Specifically, we abstractatomic reasoning patterns from CoT sequences, characterized by commonality andinductive capabilities, and use them to construct a core reference set enrichedwith valuable reasoning patterns. Furthermore, we propose a dual-granularityalgorithm involving chains of reasoning patterns and token entropy, efficientlyselecting high-value CoT data (CoTP) from the data pool that aligns with thecore set, thereby training models to master reasoning effectively. Only10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improveby 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound ofdownstream RL performance by 7.81%.</description>
      <author>example@mail.com (Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai)</author>
      <guid isPermaLink="false">2509.21124v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials</title>
      <link>http://arxiv.org/abs/2509.21079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队创建了首个大规模多模态基准数据集SoM-1K，用于评估基础模型在材料力学问题上的性能，并提出图像描述(DoI)提示策略，发现当前基础模型在工程问题上表现不佳，但文本描述比直接图像输入更有效。&lt;h4&gt;背景&lt;/h4&gt;基础模型已在多个领域展现出显著能力，但它们在复杂的多模态工程问题上的性能仍 largely 未被探索。特别是，当前基础模型在理解复杂视觉信息方面的能力有限。&lt;h4&gt;目的&lt;/h4&gt;创建首个专门用于评估基础模型在材料力学问题上表现的大规模多模态基准数据集SoM-1K；提出一种新型提示策略(DoI)来改善基础模型对视觉信息的理解；评估多种基础模型在工程问题上的表现。&lt;h4&gt;方法&lt;/h4&gt;构建包含1,065个带注释的材料力学问题的SoM-1K数据集，包含文本问题陈述和示意图；提出图像描述(DoI)策略，为视觉图表提供专家生成的严格文本描述作为上下文；评估八种代表性的基础模型，包括大型语言模型(LLMs)和视觉语言模型(VLMs)；进行详细的错误分析，比较DoI与直接图像输入的效果。&lt;h4&gt;主要发现&lt;/h4&gt;当前基础模型在工程问题上表现显著不佳，最佳模型仅达到56.6%的准确率；当提供DoI时，大型语言模型通常比提供视觉图表的视觉语言模型表现更好；DoI在减轻视觉误解错误方面起着关键作用，表明准确的基于文本的描述对当前基础模型比直接图像输入更有效。&lt;h4&gt;结论&lt;/h4&gt;这项工作为工程AI建立了严格的基准，并突显出基础模型(特别是在科学和工程背景下)需要开发更强大的多模态推理能力的迫切需求。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已在各个领域展现出显著能力，但它们在复杂的多模态工程问题上的性能 largely 未被探索。我们介绍了SoM-1K，这是首个专门用于评估基础模型在材料力学问题上表现的大规模多模态基准数据集。该数据集包含1,065个带注释的材料力学问题，通过包含文本问题陈述和示意图来模拟真实工程任务。由于当前基础模型在理解复杂视觉信息方面的能力有限，我们提出了一种名为图像描述(DoI)的新型提示策略，它提供了视觉图表的专家生成文本描述作为上下文。我们评估了八种代表性的基础模型，包括大型语言模型(LLMs)和视觉语言模型(VLMs)。我们的结果显示，当前基础模型在解决这些工程问题时遇到很大困难，表现最佳的模型仅达到56.6%的准确率。有趣的是，我们发现当提供DoI时，大型语言模型通常比提供视觉图表的视觉语言模型表现更好。详细的错误分析显示，DoI在减轻视觉误解错误方面起着关键作用，表明准确的基于文本的描述对当前基础模型比直接图像输入更有效。这项工作为工程AI建立了严格的基准，并突显出基础模型(特别是在科学和工程背景下)需要开发更强大的多模态推理能力的迫切需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have shown remarkable capabilities in various domains, buttheir performance on complex, multimodal engineering problems remains largelyunexplored. We introduce SoM-1K, the first large-scale multimodal benchmarkdataset dedicated to evaluating foundation models on problems in the strengthof materials (SoM). The dataset, which contains 1,065 annotated SoM problems,mirrors real-world engineering tasks by including both textual problemstatements and schematic diagrams. Due to the limited capabilities of currentfoundation models in understanding complicated visual information, we propose anovel prompting strategy called Descriptions of Images (DoI), which providesrigorous expert-generated text descriptions of the visual diagrams as thecontext. We evaluate eight representative foundation models, including bothlarge language models (LLMs) and vision language models (VLMs). Our resultsshow that current foundation models struggle significantly with theseengineering problems, with the best-performing model achieving only 56.6%accuracy. Interestingly, we found that LLMs, when provided with DoI, oftenoutperform VLMs provided with visual diagrams. A detailed error analysisreveals that DoI plays a crucial role in mitigating visual misinterpretationerrors, suggesting that accurate text-based descriptions can be more effectivethan direct image input for current foundation models. This work establishes arigorous benchmark for engineering AI and highlights a critical need fordeveloping more robust multimodal reasoning capabilities in foundation models,particularly in scientific and engineering contexts.</description>
      <author>example@mail.com (Qixin Wan, Zilong Wang, Jingwen Zhou, Wanting Wang, Ziheng Geng, Jiachen Liu, Ran Cao, Minghui Cheng, Lu Cheng)</author>
      <guid isPermaLink="false">2509.21079v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Measuring Audio's Impact on Correctness: Audio-Contribution-Aware Post-Training of Large Audio Language Models</title>
      <link>http://arxiv.org/abs/2509.21060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了大型音频语言模型(LALMs)的多阶段后训练方法，提出了新的数据集和训练范式，解决了零音频贡献问题，并在多个基准测试上取得了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大型音频语言模型(LALMs)是多模态AI的重要前沿，能够处理多样化的音频任务。后训练可以显著提高基础模型的性能，但多阶段训练方法(如监督微调后跟强化学习)的效果不如单阶段强化学习。&lt;h4&gt;目的&lt;/h4&gt;解决多阶段训练中数据分配问题，探索最大化LALMs能力的方法，并提供大规模高质量数据集支持相关研究。&lt;h4&gt;方法&lt;/h4&gt;提出了AudioMCQ数据集，研究了零音频贡献现象，设计了音频贡献过滤方法，开发了弱到强和混合到强两种后训练范式。&lt;h4&gt;主要发现&lt;/h4&gt;LALMs存在零音频贡献现象，模型可能仅从文本信息获取正确答案而不处理音频内容；通过音频贡献过滤和数据集划分，可以显著提升模型性能。&lt;h4&gt;结论&lt;/h4&gt;通过提出的AudioMCQ数据集和两种训练范式，在DCASE 2025音频问答挑战中获得第一名，并在MMAU-test-mini、MMAU、MMAR和MMSU等多个基准测试上取得了最先进性能。&lt;h4&gt;翻译&lt;/h4&gt;大型音频语言模型(LALMs)代表了多模态AI的重要前沿，能够处理多样化的音频任务。最近，LALMs的后训练受到越来越多的关注，因为它能显著提高基础模型的性能。虽然单阶段后训练如强化学习已经显示出有希望的结果，但多阶段方法如监督微调后跟强化学习仍然不够理想。跨多个训练阶段分配数据以最大化LALMs能力尚未得到充分探索，此类研究的大规模高质量数据集也缺乏。为解决这些问题，我们首先提出了AudioMCQ，一个包含57.1万样本的综合音频多选题数据集，具有两种思维链注释。其次，我们研究了LALMs中普遍存在的零音频贡献现象，即模型仅从文本信息中获取正确答案而不处理音频内容。我们提出了音频贡献过滤方法，将数据划分为弱音频贡献和强音频贡献子集。基于这些见解，我们开发了两种有效的后训练范式：弱到强(在弱音频贡献数据上进行监督微调，然后在强音频贡献数据上进行强化学习)和混合到强(在混合音频贡献数据上进行监督微调，然后在强音频贡献数据上进行强化学习)。通过使用AudioMCQ，我们在DCASE 2025音频问答挑战中获得第一名。此外，利用我们的数据集和不同的训练策略，我们在MMAU-test-mini上达到78.2%，在MMAU上达到75.6%，在MMAR上达到67.1%，在MMSU上达到70.7%，在这些基准测试上建立了新的最先进性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Audio Language Models (LALMs) represent an important frontier inmultimodal AI, addressing diverse audio tasks. Recently, post-training of LALMshas received increasing attention due to significant performance improvementsover foundation models. While single-stage post-training such as reinforcementlearning (RL) has demonstrated promising results, multi-stage approaches suchas supervised fine-tuning (SFT) followed by RL remain suboptimal. Theallocation of data across multiple training stages to maximize LALMcapabilities has not been fully explored, and large-scale, high-qualitydatasets for such research are also lacking. To address these problems, wefirstly present AudioMCQ, a comprehensive audio multiple-choice questiondataset comprising 571k samples with two kinds of chain-of-thought annotations.Secondly, we investigate the prevalent zero audio-contribution phenomenon inLALMs, where models derive correct answers solely from textual informationwithout processing audio content. We propose Audio-Contribution Filtering topartition data into weak and strong audio-contribution subsets. Based on theseinsights, we develop two effective post-training paradigms: Weak-to-Strong (SFTon weak audio-contribution data followed by RL on strong audio-contributiondata) and Mixed-to-Strong (SFT on mixed audio-contribution data followed by RLon strong audio-contribution data). We achieve first place in the DCASE 2025Audio-Question-Answering challenge by using AudioMCQ. Additionally, leveragingour dataset with different training strategies, we achieve 78.2\% onMMAU-test-mini, 75.6\% on MMAU, 67.1\% on MMAR, and 70.7\% on MMSU,establishing new state-of-the-art performance across these benchmarks.</description>
      <author>example@mail.com (Haolin He, Xingjian Du, Renhe Sun, Zheqi Dai, Yujia Xiao, Mingru Yang, Jiayi Zhou, Xiquan Li, Zhengxi Liu, Zining Liang, Chunyat Wu, Qianhua He, Tan Lee, Xie Chen, Weilong Zheng, Weiqiang Wang, Mark Plumbley, Jian Liu, Qiuqiang Kong)</author>
      <guid isPermaLink="false">2509.21060v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>SiNGER: A Clearer Voice Distills Vision Transformers Further</title>
      <link>http://arxiv.org/abs/2509.20986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Main paper: 12 pages (including 3 pages of references), 6 figures, 6  tables. Appendix: 9 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Singular Nullspace-Guided Energy Reallocation (SiNGER)框架，解决了Vision Transformers在知识蒸馏中的高范数伪影问题，通过教师特征精炼和零空间引导的扰动，实现了抑制伪影同时保留信息信号的效果。&lt;h4&gt;背景&lt;/h4&gt;Vision Transformers被广泛用作视觉基础模型的骨干网络，但它们会产生高范数伪影，降低表示质量。在知识蒸馏中，这些高范数伪影会主导目标函数，导致学生模型过度拟合伪影而低估信息信号。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的知识蒸馏框架，能够在抑制Vision Transformers产生的高范数伪影的同时，保留教师模型中的信息信号，从而提高学生模型的性能。&lt;h4&gt;方法&lt;/h4&gt;引入Singular Nullspace-Guided Energy Reallocation (SiNGER)框架，其核心思想是有原则的教师特征精炼：在精炼过程中利用零空间引导的扰动来保留信息同时抑制伪影，然后将精炼后的教师特征蒸馏给学生。使用基于LoRA的适配器高效实现这种扰动，只需要最小的结构修改。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明SiNGER能持续改进学生模型，在多个下游任务中达到最先进性能，并产生更清晰和可解释的表示。&lt;h4&gt;结论&lt;/h4&gt;SiNGER框架成功解决了知识蒸馏中高范数伪影和信息信号保留之间的权衡问题，为学生模型提供了更有效的知识转移方式。&lt;h4&gt;翻译&lt;/h4&gt;Vision Transformers被广泛用作视觉基础模型的骨干网络，但它们会产生高范数伪影，降低表示质量。当知识蒸馏将这些特征转移到学生模型时，高范数伪影主导了目标函数，导致学生模型过度拟合伪影而低估信息信号，从而削弱了大模型的收益。先前的工作尝试移除伪影，但在抑制伪影和保留教师信息信号之间遇到了固有权衡。为解决这一问题，我们引入了Singular Nullspace-Guided Energy Reallocation (SiNGER)，这是一种新的蒸馏框架，可以在抑制伪影的同时保留信息信号。核心思想是有原则的教师特征精炼：在精炼过程中，我们利用零空间引导的扰动来保留信息同时抑制伪影。然后将精炼后的教师特征蒸馏给学生。我们使用基于LoRA的适配器高效实现这种扰动，只需要最小的结构修改。大量实验表明，SiNGER能持续改进学生模型，在多个下游任务中达到最先进性能，并产生更清晰和可解释的表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Transformers are widely adopted as the backbone of vision foundationmodels, but they are known to produce high-norm artifacts that degraderepresentation quality. When knowledge distillation transfers these features tostudents, high-norm artifacts dominate the objective, so students overfit toartifacts and underweight informative signals, diminishing the gains fromlarger models. Prior work attempted to remove artifacts but encountered aninherent trade-off between artifact suppression and preserving informativesignals from teachers. To address this, we introduce Singular Nullspace-GuidedEnergy Reallocation (SiNGER), a novel distillation framework that suppressesartifacts while preserving informative signals. The key idea is principledteacher feature refinement: during refinement, we leverage the nullspace-guidedperturbation to preserve information while suppressing artifacts. Then, therefined teacher's features are distilled to a student. We implement thisperturbation efficiently with a LoRA-based adapter that requires minimalstructural modification. Extensive experiments show that \oursname consistentlyimproves student models, achieving state-of-the-art performance in multipledownstream tasks and producing clearer and more interpretable representations.</description>
      <author>example@mail.com (Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Jaeseung Kim, Hyoseok Hwang)</author>
      <guid isPermaLink="false">2509.20986v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Data Challenges of Computational Pathology: A Pack-based Multiple Instance Learning Framework</title>
      <link>http://arxiv.org/abs/2509.20923v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于包装的多实例学习(PackMIL)框架，用于解决计算病理学中全切片图像(WSIs)分析面临的挑战，实现了更高的准确率和更快的训练速度。&lt;h4&gt;背景&lt;/h4&gt;计算病理学将病理切片数字化为全切片图像(WSIs)用于癌症诊断和预后，但WSIs具有极长序列长度(高达200K)、显著长度变化(从200到200K)和有限监督，导致数据异构性和冗余性高，传统方法难以有效处理。&lt;h4&gt;目的&lt;/h4&gt;全面解决计算病理学中WSIs分析面临的超长序列长度、显著长度变化和有限监督等挑战，提高模型性能和训练效率。&lt;h4&gt;方法&lt;/h4&gt;1. 提出PackMIL框架，将多个采样的可变长度特征序列打包为固定长度序列，实现批量训练同时保留数据异构性；2. 引入残差分支，将多个切片中被丢弃的特征组合成超切片，使用定制标签进行多切片监督；3. 设计注意力驱动的下采样器，压缩特征以减少冗余。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在PANDA(UNI)数据集上实现了高达8%的准确率提升，同时仅使用12%的训练时间，实验表明关注计算病理学中的数据挑战在基础模型时代具有巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;专注于解决计算病理学中的数据挑战对于提高模型性能和训练效率至关重要，所提出的PackMIL框架为WSIs分析提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;计算病理学(CPath)将病理切片数字化为全切片图像(WSIs)，使得能够进行分析以用于癌症诊断和预后等关键医疗任务。然而，WSIs具有极长的序列长度(高达200K)、显著的长度变化(从200到200K)和有限的监督。这些序列长度的极端变化导致数据异构性和冗余性高。传统方法通常需要在训练效率和优化之间做出妥协，以在有限监督下保留这种异构性。为了全面解决这些挑战，我们提出了一种基于包装的多实例学习框架。它将多个采样的可变长度特征序列打包为固定长度序列，实现批量训练同时保留数据异构性。此外，我们引入了一个残差分支，将多个切片中被丢弃的特征组合成超切片，使用定制标签进行训练。它提供多切片监督，同时减少采样带来的特征损失。同时，引入了注意力驱动的下采样器来压缩两个分支中的特征以减少冗余。通过缓解这些挑战，我们的方法在PANDA(UNI)上实现了高达8%的准确率提升，同时仅使用12%的训练时间。大量实验表明，关注计算病理学中的数据挑战在基础模型时代具有巨大潜力。代码位于https://github.com/FangHeng/PackMIL&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational pathology (CPath) digitizes pathology slides into whole slideimages (WSIs), enabling analysis for critical healthcare tasks such as cancerdiagnosis and prognosis. However, WSIs possess extremely long sequence lengths(up to 200K), significant length variations (from 200 to 200K), and limitedsupervision. These extreme variations in sequence length lead to high dataheterogeneity and redundancy. Conventional methods often compromise on trainingefficiency and optimization to preserve such heterogeneity under limitedsupervision. To comprehensively address these challenges, we propose apack-based MIL framework. It packs multiple sampled, variable-length featuresequences into fixed-length ones, enabling batched training while preservingdata heterogeneity. Moreover, we introduce a residual branch that composesdiscarded features from multiple slides into a hyperslide which is trained withtailored labels. It offers multi-slide supervision while mitigating featureloss from sampling. Meanwhile, an attention-driven downsampler is introduced tocompress features in both branches to reduce redundancy. By alleviating thesechallenges, our approach achieves an accuracy improvement of up to 8% whileusing only 12% of the training time in the PANDA(UNI). Extensive experimentsdemonstrate that focusing data challenges in CPath holds significant potentialin the era of foundation models. The code ishttps://github.com/FangHeng/PackMIL</description>
      <author>example@mail.com (Wenhao Tang, Heng Fang, Ge Wu, Xiang Li, Ming-Ming Cheng)</author>
      <guid isPermaLink="false">2509.20923v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting</title>
      <link>http://arxiv.org/abs/2509.20857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 figures, 7 tables, code is available at  https://github.com/tiny-smart/tasselnetv4&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TasselNetV4的新型跨物种植物计数模型，结合了TasselNet的局部计数思想和类别无关计数(CAC)的提取与匹配范式，实现了跨场景、跨尺度和跨物种的高效植物计数。&lt;h4&gt;背景&lt;/h4&gt;准确的植物计数对农业有价值，如作物产量预测、植物密度评估和表型量化。现有视觉方法通常使用检测或回归模型计数特定植物，但植物具有生物多样性，每年都有新品种培育，几乎不可能穷尽并构建所有物种依赖的计数模型。&lt;h4&gt;目的&lt;/h4&gt;重新思考植物计数的问题表述，从'计数什么植物'转向'如何计数植物'，开发一种不依赖于特定物种的通用植物计数方法。&lt;h4&gt;方法&lt;/h4&gt;继承TasselNet植物计数模型的思路，引入TasselNetV4扩展，基于普通视觉变换器构建，融合了多分支框感知局部计数器以增强跨尺度鲁棒性，并构建了两个具有挑战性的数据集PAC-105和PAC-Somalia进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进的CAC模型相比，TasselNetV4实现了卓越的计数性能和高效率。植物是动态的，随时间和空间变化，其非刚性结构导致当前CAC和开放世界检测模型在植物计数方面表现不佳。&lt;h4&gt;结论&lt;/h4&gt;TasselNetV4已成为跨场景、跨尺度和跨物种植物计数的视觉基础模型，为解决植物生物多样性带来的计数挑战提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;准确的植物计数为农业提供了有价值的信息，如作物产量预测、植物密度评估和表型量化。基于视觉的方法是目前的主流解决方案。现有技术通常使用检测或回归模型来计数特定植物。然而，植物具有生物多样性，每年都有新品种不断培育。几乎不可能穷尽并构建所有物种依赖的计数模型。受计算机视觉中类别无关计数(CAC)的启发，我们认为现在是重新思考植物计数问题表述的时候了，从'计数什么植物'转向'如何计数植物'。与大多数具有时空不变性的日常物体不同，植物是动态的，随时间和空间变化。它们的非刚性结构通常比计数头部和汽车等刚性实例导致更差的性能，因此当前的CAC和开放世界检测模型对于植物计数不是最优的。在这项工作中，我们继承了TasselNet植物计数模型的思路，并引入了一个新的扩展TasselNetV4，从物种特定计数转向跨物种计数。TasselNetV4结合了TasselNet的局部计数思想和CAC的提取与匹配范式。它基于普通视觉变换器构建，并融合了新颖的多分支框感知局部计数器，用于增强跨尺度鲁棒性。我们收集了两个具有挑战性的数据集：PAC-105和PAC-Somalia。与最先进的CAC模型进行的广泛实验表明，TasselNetV4不仅实现了卓越的计数性能，而且具有高效率。我们的结果表明，TasselNetV4已成为跨场景、跨尺度和跨物种植物计数的视觉基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate plant counting provides valuable information for agriculture such ascrop yield prediction, plant density assessment, and phenotype quantification.Vision-based approaches are currently the mainstream solution. Prior arttypically uses a detection or a regression model to count a specific plant.However, plants have biodiversity, and new cultivars are increasingly bred eachyear. It is almost impossible to exhaust and build all species-dependentcounting models. Inspired by class-agnostic counting (CAC) in computer vision,we argue that it is time to rethink the problem formulation of plant counting,from what plants to count to how to count plants. In contrast to most dailyobjects with spatial and temporal invariance, plants are dynamic, changing withtime and space. Their non-rigid structure often leads to worse performance thancounting rigid instances like heads and cars such that current CAC andopen-world detection models are suboptimal to count plants. In this work, weinherit the vein of the TasselNet plant counting model and introduce a newextension, TasselNetV4, shifting from species-specific counting tocross-species counting. TasselNetV4 marries the local counting idea ofTasselNet with the extract-and-match paradigm in CAC. It builds upon a plainvision transformer and incorporates novel multi-branch box-aware local countersused to enhance cross-scale robustness. Two challenging datasets, PAC-105 andPAC-Somalia, are harvested. Extensive experiments against state-of-the-art CACmodels show that TasselNetV4 achieves not only superior counting performancebut also high efficiency.Our results indicate that TasselNetV4 emerges to be avision foundation model for cross-scene, cross-scale, and cross-species plantcounting.</description>
      <author>example@mail.com (Xiaonan Hu, Xuebing Li, Jinyu Xu, Abdulkadir Duran Adan, Letian Zhou, Xuhui Zhu, Yanan Li, Wei Guo, Shouyang Liu, Wenzhong Liu, Hao Lu)</author>
      <guid isPermaLink="false">2509.20857v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>CaTS-Bench: Can Language Models Describe Numeric Time Series?</title>
      <link>http://arxiv.org/abs/2509.20823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 images, 4 tables in the main paper. Many more in the  appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了CaTS-Bench，这是第一个大规模、真实世界的上下文感知时间序列描述基准测试，解决了现有基准测试的局限性，提供了丰富的数据集和评估方法，为时间序列分析和基础模型的交叉研究奠定了基础。&lt;h4&gt;背景&lt;/h4&gt;时间序列描述是将数值时间序列用自然语言描述的任务，需要数值推理、趋势解释和上下文理解。然而，现有的基准测试通常依赖合成数据或过于简单的描述，并且通常忽略元数据和视觉表示。&lt;h4&gt;目的&lt;/h4&gt;引入CaTS-Bench，这是第一个大规模、真实世界的上下文感知时间序列描述基准测试，以弥补现有基准测试的不足。&lt;h4&gt;方法&lt;/h4&gt;CaTS-Bench来源于11个多样化数据集，重新构造成描述和问答任务，包含约465k训练和105k测试时间戳。每个样本包括数值序列片段、上下文元数据、线图图像和描述。提出了可扩展的参考描述生成流程，使用oracle LLM生成大多数参考描述并通过多种验证方法确保质量，同时提供579个测试描述的人类修订子集。&lt;h4&gt;主要发现&lt;/h4&gt;CaTS-Bench还提供460个针对时间序列推理更深层次方面的多项选择题。提出了新的定制评估指标，并对领先的VLMs进行了基准测试，突显了它们的优势和持续存在的局限性。&lt;h4&gt;结论&lt;/h4&gt;这些贡献共同将CaTS-Bench及其描述流程建立为时间序列分析和基础模型交叉领域未来研究的可靠且可扩展的基础。&lt;h4&gt;翻译&lt;/h4&gt;时间序列描述是将数值时间序列用自然语言描述的任务，需要数值推理、趋势解释和上下文理解。然而，现有的基准测试通常依赖合成数据或过于简单的描述，并且通常忽略元数据和视觉表示。为了弥补这一差距，我们引入了CaTS-Bench，这是第一个大规模、真实世界的上下文感知时间序列描述基准测试。CaTS-Bench来源于11个多样化数据集，重新构造成描述和问答任务，包含约465k训练和105k测试时间戳。每个样本包括数值序列片段、上下文元数据、线图图像和描述。这项工作的一个关键贡献是用于生成参考描述的可扩展流程：虽然大多数参考描述由oracle LLM生成并通过事实检查、人类不可区分性研究和多样性分析进行验证，但我们还提供了579个测试描述的人类修订子集，从LLM输出中提炼以确保准确性和人类风格。除了描述外，CaTS-Bench还提供460个针对时间序列推理更深层次方面的多项选择题。我们进一步提出了新的定制评估指标，并对领先的VLMs进行了基准测试，突显了它们的优势和持续存在的局限性。这些贡献共同将CaTS-Bench及其描述流程建立为时间序列分析和基础模型交叉领域未来研究的可靠且可扩展的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series captioning, the task of describing numeric time series in naturallanguage, requires numerical reasoning, trend interpretation, and contextualunderstanding. Existing benchmarks, however, often rely on synthetic data oroverly simplistic captions, and typically neglect metadata and visualrepresentations. To close this gap, we introduce CaTS-Bench, the firstlarge-scale, real-world benchmark for Context-aware Time Series captioning.CaTS-Bench is derived from 11 diverse datasets reframed as captioning and Q&amp;Atasks, comprising roughly 465k training and 105k test timestamps. Each sampleincludes a numeric series segment, contextual metadata, a line-chart image, anda caption. A key contribution of this work is the scalable pipeline used togenerate reference captions: while most references are produced by an oracleLLM and verified through factual checks, human indistinguishability studies,and diversity analyses, we also provide a human-revisited subset of 579 testcaptions, refined from LLM outputs to ensure accuracy and human-like style.Beyond captioning, CaTS-Bench offers 460 multiple-choice questions targetingdeeper aspects of time series reasoning. We further propose new tailoredevaluation metrics and benchmark leading VLMs, highlighting both theirstrengths and persistent limitations. Together, these contributions establishCaTS-Bench and its captioning pipeline as a reliable and extensible foundationfor future research at the intersection of time series analysis and foundationmodels.</description>
      <author>example@mail.com (Luca Zhou, Pratham Yashwante, Marshall Fisher, Alessio Sampieri, Zihao Zhou, Fabio Galasso, Rose Yu)</author>
      <guid isPermaLink="false">2509.20823v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>RAPTOR-GEN: RApid PosTeriOR GENerator for Bayesian Learning in Biomanufacturing</title>
      <link>http://arxiv.org/abs/2509.20753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  80 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RAPTOR-GEN是一个基于机制的贝叶斯学习框架，旨在从稀疏和异构的实验数据中加速智能数字孪生的发展，用于生物制药制造的快速按需生产。&lt;h4&gt;背景&lt;/h4&gt;生物制药制造对公共健康至关重要，但由于生物过程的复杂性和变异性，缺乏快速按需生产生物治疗药物的灵活性。&lt;h4&gt;目的&lt;/h4&gt;介绍RAPTOR-GEN框架，从稀疏和异构的实验数据中加速智能数字孪生的发展。&lt;h4&gt;方法&lt;/h4&gt;RAPTOR-GEN建立在多尺度概率知识图谱基础上，表述为基于随机微分方程的基础模型，包含两个组成部分：(1)整合线性噪声近似的可解释元模型，利用生物处理机制结构信息和顺序学习策略融合数据；(2)利用朗之万扩散的高效贝叶斯后验采样方法。&lt;h4&gt;主要发现&lt;/h4&gt;RAPTOR-GEN将LNA方法推广以避免步长选择的挑战，促进具有可证明有限样本性能保证的机制参数的稳健学习，开发了一种具有可控误差的快速稳健算法。&lt;h4&gt;结论&lt;/h4&gt;数值实验证明了RAPTOR-GEN在揭示生物制造过程潜在调控机制方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;生物制药制造对公共健康至关重要，但由于生物过程的复杂性和变异性，缺乏快速按需生产生物治疗药物的灵活性。为克服这一点，我们引入了RAPTOR-GEN（RApid PosTeriOR GENenerator），这是一个基于机制的贝叶斯学习框架，旨在从稀疏和异构的实验数据中加速智能数字孪生的发展。该框架建立在多尺度概率知识图谱(pKG)的基础上，被表述为基于随机微分方程(SDE)的基础模型，能够捕捉生物过程的非线性动力学。RAPTOR-GEN包含两个组成部分：(i)一个可解释的元模型，整合了线性噪声近似(LNA)，利用生物处理机制的结构信息，并采用顺序学习策略融合异构和稀疏数据，能够推断潜在状态变量并明确逼近难以处理的似然函数；(ii)一种高效的贝叶斯后验采样方法，利用朗之万扩散(LD)通过利用导出的似然的梯度来加速后验探索。它将LNA方法推广以避免步长选择的挑战，促进具有可证明有限样本性能保证的机制参数的稳健学习。我们开发了一种具有可控误差的快速稳健RAPTOR-GEN算法。数值实验证明了其在揭示生物制造过程潜在调控机制方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biopharmaceutical manufacturing is vital to public health but lacks theagility for rapid, on-demand production of biotherapeutics due to thecomplexity and variability of bioprocesses. To overcome this, we introduceRApid PosTeriOR GENerator (RAPTOR-GEN), a mechanism-informed Bayesian learningframework designed to accelerate intelligent digital twin development fromsparse and heterogeneous experimental data. This framework is built on amulti-scale probabilistic knowledge graph (pKG), formulated as a stochasticdifferential equation (SDE)-based foundational model that captures thenonlinear dynamics of bioprocesses. RAPTOR-GEN consists of two ingredients: (i)an interpretable metamodel integrating linear noise approximation (LNA) thatexploits the structural information of bioprocessing mechanisms and asequential learning strategy to fuse heterogeneous and sparse data, enablinginference of latent state variables and explicit approximation of theintractable likelihood function; and (ii) an efficient Bayesian posteriorsampling method that utilizes Langevin diffusion (LD) to accelerate posteriorexploration by exploiting the gradients of the derived likelihood. Itgeneralizes the LNA approach to circumvent the challenge of step sizeselection, facilitating robust learning of mechanistic parameters with provablefinite-sample performance guarantees. We develop a fast and robust RAPTOR-GENalgorithm with controllable error. Numerical experiments demonstrate itseffectiveness in uncovering the underlying regulatory mechanisms ofbiomanufacturing processes.</description>
      <author>example@mail.com (Wandi Xu, Wei Xie)</author>
      <guid isPermaLink="false">2509.20753v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation</title>
      <link>http://arxiv.org/abs/2509.20681v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Fast Image-to-Neural Surface (FINS)的轻量级框架，能够从单张或少量图像重建高保真表面和SDF场。该方法结合多分辨率哈希网格编码器与轻量级几何和颜色头部，通过近似二阶优化器实现高效训练，能在几秒内收敛。研究还展示了FINS在机器人表面跟随任务中的适用性及其在各种基准数据集上的可扩展性。&lt;h4&gt;背景&lt;/h4&gt;隐式表示已在机器人领域广泛应用于避障和路径规划。然而，现有的隐式表面重建方法（如NeuS及其变体）通常需要大量多视角图像作为输入，并且需要很长的训练时间。&lt;h4&gt;目的&lt;/h4&gt;探索从单张图像构建隐式距离表示的问题，开发一种能够从单张或少量图像重建高保真表面和SDF场的高效方法。&lt;h4&gt;方法&lt;/h4&gt;提出Fast Image-to-Neural Surface (FINS)框架，该方法集成了多分辨率哈希网格编码器，使用轻量级的几何和颜色头部，通过近似二阶优化器实现高效训练。此外，利用预训练的基础模型来估计图像中固有的几何信息，从而仅使用单张RGB图像构建神经表面。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在相同条件下，该方法在表面重建和SDF场估计的收敛速度和准确性上都优于最先进的基线方法。此外，FINS在机器人表面跟随任务中表现出适用性，并且在各种基准数据集上具有可扩展性。&lt;h4&gt;结论&lt;/h4&gt;FINS是一种高效的方法，能够从单张或少量图像重建高质量的隐式表面和SDF场，相比现有方法具有更快的收敛速度和更高的准确性，且在实际应用中具有良好的适用性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;隐式表示已在机器人领域被广泛应用于避障和路径规划。在本文中，我们探索了从单张图像构建隐式距离表示的问题。过去的隐式表面重建方法，如NeuS及其变体通常需要大量多视角图像作为输入，并且需要很长的训练时间。在这项工作中，我们提出了Fast Image-to-Neural Surface (FINS)，一个轻量级框架，可以根据单张或少量图像重建高保真表面和SDF场。FINS集成了多分辨率哈希网格编码器与轻量级的几何和颜色头部，使得通过近似二阶优化器进行训练非常高效，并且能够在几秒内收敛。此外，我们通过利用预训练的基础模型来估计图像中固有的几何信息，实现了仅使用单张RGB图像构建神经表面。我们的实验证明，在相同条件下，我们的方法在表面重建和SDF场估计的收敛速度和准确性上都优于最先进的基线方法。此外，我们展示了FINS在机器人表面跟随任务中的适用性，并证明了其在各种基准数据集上的可扩展性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从单张图像快速构建隐式表面模型的问题。这个问题很重要，因为自主机器人需要快速理解周围环境几何以安全导航和交互，而传统方法需要大量多视角图像和长时间训练，不适合机器人实时应用场景。准确的环境几何表示对障碍物避让和路径规划至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有隐式表面重建方法的局限性：需要密集多视角监督和长时间训练。然后借鉴了预训练3D基础模型（如DUSt3R和VGGT）来估计单张图像几何信息，采用多分辨率哈希网格编码高效表示空间特征，并使用近似二阶优化器加速收敛。作者将这些技术创新性地组合，设计了轻量级框架，实现了从单张图像快速重建高保真表面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合预训练3D基础模型、多分辨率哈希编码和混合优化策略，实现从单张图像快速重建隐式表面。流程包括：1)用预训练3D模型将输入图像转换为3D点云；2)使用多分辨率哈希编码器将3D坐标编码为特征；3)通过轻量级几何和颜色头预测SDF值和颜色；4)采用分阶段优化策略（先用一阶优化器预热，再用二阶优化器快速收敛）；5)最后提取等值面生成网格模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次实现从单张图像在消费级硬件上约10秒内重建高保真表面；2)利用预训练3D基础模型生成点云监督信号；3)采用多分辨率哈希编码和轻量级头设计；4)使用混合优化策略实现快速收敛。相比之前工作，FINS大幅减少了输入图像需求（从5-49张减少到1张）和训练时间（从18-600秒减少到10秒），同时保持竞争性的重建质量，更适合实时机器人应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FINS通过结合预训练3D基础模型、多分辨率哈希编码和混合优化策略，首次实现了从单张图像在消费级硬件上仅需约10秒即可重建高保真隐式表面模型，为机器人实时感知和导航提供了高效解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Implicit representations have been widely applied in robotics for obstacleavoidance and path planning. In this paper, we explore the problem ofconstructing an implicit distance representation from a single image. Pastmethods for implicit surface reconstruction, such as \emph{NeuS} and itsvariants generally require a large set of multi-view images as input, andrequire long training times. In this work, we propose Fast Image-to-NeuralSurface (FINS), a lightweight framework that can reconstruct high-fidelitysurfaces and SDF fields based on a single or a small set of images. FINSintegrates a multi-resolution hash grid encoder with lightweight geometry andcolor heads, making the training via an approximate second-order optimizerhighly efficient and capable of converging within a few seconds. Additionally,we achieve the construction of a neural surface requiring only a single RGBimage, by leveraging pre-trained foundation models to estimate the geometryinherent in the image. Our experiments demonstrate that under the sameconditions, our method outperforms state-of-the-art baselines in bothconvergence speed and accuracy on surface reconstruction and SDF fieldestimation. Moreover, we demonstrate the applicability of FINS for robotsurface following tasks and show its scalability to a variety of benchmarkdatasets.</description>
      <author>example@mail.com (Wei-Teng Chu, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi)</author>
      <guid isPermaLink="false">2509.20681v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting</title>
      <link>http://arxiv.org/abs/2509.20499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种零样本框架，结合简化的航路点预测器和多模态大语言模型，用于解决连续环境中的视觉语言导航问题，在R2R-CE和RxR-CE数据集上实现了最先进的零样本性能。&lt;h4&gt;背景&lt;/h4&gt;随着基础模型和机器人技术的快速发展，视觉语言导航已成为具身智能体的关键任务，具有广泛的应用前景。连续环境中的VLN尤其具有挑战性，因为智能体需要同时解释自然语言指令、感知周围环境并规划低级别动作。&lt;h4&gt;目的&lt;/h4&gt;解决具身智能体在连续环境中需要联合处理自然语言指令、环境感知和低级别动作规划的挑战性问题。&lt;h4&gt;方法&lt;/h4&gt;提出一个零样本框架，整合了在抽象障碍地图上操作的航路点预测器和多模态大语言模型。预测器产生线性可达的航路点，这些航路点被整合到具有访问记录的动态拓扑图中。图和访问信息被编码到提示中，使模型能够对空间结构和探索历史进行推理，鼓励探索并实现局部路径规划用于错误纠正。&lt;h4&gt;主要发现&lt;/h4&gt;在R2R-CE和RxR-CE数据集上的广泛实验表明，该方法实现了最先进的零样本性能，成功率分别达到41%和36%，优于之前的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在视觉语言导航任务中表现优异，特别是在零样本设置下，为具身智能体在连续环境中的导航提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着基础模型和机器人技术的快速发展，视觉语言导航已成为具身智能体的关键任务，具有广泛的应用前景。我们解决了连续环境中的视觉语言导航问题，这是一个特别具挑战性的设置，因为智能体必须同时解释自然语言指令、感知周围环境并规划低级别动作。我们提出一个零样本框架，集成了简化的 yet 有效的航路点预测器与多模态大语言模型。预测器在抽象障碍地图上操作，产生线性可达的航路点，这些航路点被整合到具有明确访问记录的动态更新的拓扑图中。图和访问信息被编码到提示中，使模型能够对空间结构和探索历史进行推理，鼓励探索并使MLLM具备局部路径规划能力用于错误纠正。在R2R-CE和RxR-CE上的广泛实验表明，我们的方法实现了最先进的零样本性能，成功率分别为41%和36%，优于之前的最先进方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决连续环境中的视觉语言导航(VLN)问题，即智能体需要理解自然语言指令并在自由移动的环境中导航到指定目标。这个问题在现实中非常重要，因为它涉及具身AI的核心能力，可应用于搜索救援、自主导航和日常人机交互等领域。连续环境中的VLN特别具有挑战性，因为智能体需要同时处理语言理解、环境感知和低级动作规划，而现有方法在零样本设置下性能有限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有航点预测方法的局限性：复杂的RGB-D输入会引入无关信息，且预测的航点可能不可达。因此，他们提出简化输入表示，将深度图像转换为抽象障碍物地图，使模型专注于空间可通行性。作者借鉴了ETPNav的拓扑图构建方法和MapGPT的自然语言表示方式，同时参考了AO-Planner的提示设计思想，但加入了拓扑图和访问信息以增强空间推理能力。这种设计思路体现了从简化输入到增强推理的系统性思考。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过抽象障碍物地图简化航点预测，并结合拓扑图和访问信息感知提示增强多模态大语言模型的导航能力。整体流程包括：1)将深度图像转换为障碍物地图；2)使用轻量级模型预测线性可达的航点；3)构建动态更新的拓扑图，标记已访问和未访问节点；4)将拓扑图和访问信息编码为提示，提供给MLLM进行导航决策；5)MLLM基于提示推理并决定下一步动作，支持探索和错误纠正；6)循环执行直到到达目标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于抽象障碍物地图的轻量级航点预测器，使用梯度方法检测障碍物而非固定阈值；2)拓扑图和访问信息感知提示系统，明确记录探索历史和空间结构；3)集成框架实现零样本VLN。相比之前工作，不同之处在于：使用简化的障碍物地图代替复杂RGB-D输入；确保航点线性可达；利用拓扑图而非仅顺序轨迹记录空间关系；提供局部路径规划能力进行错误纠正；在保持模型轻量化的同时实现了更优性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于抽象障碍物地图的航点预测与拓扑图和访问信息感知提示相结合的零样本视觉语言导航框架，显著提升了连续环境中的导航性能并实现了最先进的零样本结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid progress of foundation models and robotics, vision-languagenavigation (VLN) has emerged as a key task for embodied agents with broadpractical applications. We address VLN in continuous environments, aparticularly challenging setting where an agent must jointly interpret naturallanguage instructions, perceive its surroundings, and plan low-level actions.We propose a zero-shot framework that integrates a simplified yet effectivewaypoint predictor with a multimodal large language model (MLLM). The predictoroperates on an abstract obstacle map, producing linearly reachable waypoints,which are incorporated into a dynamically updated topological graph withexplicit visitation records. The graph and visitation information are encodedinto the prompt, enabling reasoning over both spatial structure and explorationhistory to encourage exploration and equip MLLM with local path planning forerror correction. Extensive experiments on R2R-CE and RxR-CE show that ourmethod achieves state-of-the-art zero-shot performance, with success rates of41% and 36%, respectively, outperforming prior state-of-the-art methods.</description>
      <author>example@mail.com (Boqi Li, Siyuan Li, Weiyi Wang, Anran Li, Zhong Cao, Henry X. Liu)</author>
      <guid isPermaLink="false">2509.20499v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data</title>
      <link>http://arxiv.org/abs/2509.20479v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基础模型在文本和图像处理任务中表现出色，具有跨领域泛化能力，但在真实工业数据上的应用存在局限性。&lt;h4&gt;背景&lt;/h4&gt;基础模型在零样本设置下可跨领域和数据集泛化，这使其成为系列制造过程中自动化质量检查的潜在解决方案。&lt;h4&gt;目的&lt;/h4&gt;探索基础模型在工业图像质量检查中的应用，用简单文本提示代替繁琐标记任务，利用相同模型处理多种产品以节省模型设置和实施的工作量。&lt;h4&gt;方法&lt;/h4&gt;测试多个最新的基础模型，使用自定义真实工业图像数据和公共图像数据，比较模型在两种数据集上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;所有测试的基础模型在真实世界工业数据上表现不佳，但相同的模型在公共基准数据集上表现良好。&lt;h4&gt;结论&lt;/h4&gt;基础模型虽然在公共数据集上表现良好，但在真实工业环境中存在应用局限性，需要进一步研究其在实际应用中的问题。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在文本和图像处理任务上表现出色。它们可以在零样本设置下跨领域和数据集泛化。这可能使它们适合在系列制造过程中的自动化质量检查，其中正在评估各种类型图像以检查许多不同产品。用简单的文本提示描述异常来代替繁琐的标记任务，并在许多产品上使用相同的模型，将在模型设置和实施过程中节省大量工作。这与监督式人工智能模型相比是一个显著优势，监督式AI模型针对单个应用程序进行训练，需要标记的训练数据。我们在自定义的真实工业图像数据和公共图像数据上测试了多个最新的基础模型。我们表明所有这些模型在我们的真实数据上都失败了，而完全相同的模型在公共基准数据集上表现良好。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation Models (FMs) have shown impressive performance on various text andimage processing tasks. They can generalize across domains and datasets in azero-shot setting. This could make them suitable for automated qualityinspection during series manufacturing, where various types of images are beingevaluated for many different products. Replacing tedious labeling tasks with asimple text prompt to describe anomalies and utilizing the same models acrossmany products would save significant efforts during model setup andimplementation. This is a strong advantage over supervised ArtificialIntelligence (AI) models, which are trained for individual applications andrequire labeled training data. We test multiple recent FMs on both customreal-world industrial image data and public image data. We show that all ofthose models fail on our real-world data, while the very same models performwell on public benchmark datasets.</description>
      <author>example@mail.com (Simon Baeuerle, Pratik Khanna, Nils Friederich, Angelo Jovin Yamachui Sitcheu, Damir Shakirov, Andreas Steimer, Ralf Mikut)</author>
      <guid isPermaLink="false">2509.20479v1</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Association Rules in High-Dimensional Small Tabular Data</title>
      <link>http://arxiv.org/abs/2509.20113v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper was accepted at ECAI 2025 Workshop: 1st International  Workshop on Advanced Neuro-Symbolic Applications (ANSyA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的关联规则挖掘方法，特别针对高维、低数据场景，通过结合神经符号方法和表格基础模型微调技术，有效解决了规则爆炸问题并提高了规则质量。&lt;h4&gt;背景&lt;/h4&gt;关联规则挖掘旨在发现数据集中特征间的模式，支持高风险决策中的知识发现和可解释机器学习。但在高维数据中，规则爆炸和计算开销使传统方法不切实际，神经符号方法如Aerial+虽解决高维问题，但在低数据场景下性能受限。&lt;h4&gt;目的&lt;/h4&gt;解决高维数据中的关联规则挖掘问题，特别是在低数据场景下的挑战，提高算法的扩展性和规则质量。&lt;h4&gt;方法&lt;/h4&gt;提出两种基于表格基础模型的Aerial+微调方法，用于高维、低数据场景下的关联规则挖掘。&lt;h4&gt;主要发现&lt;/h4&gt;Aerial+比最先进的算法和神经符号基线在五个真实世界数据集上扩展性好一到两个数量级；提出了高维、低数据设置下的ARM问题；提出的微调方法显著提高了五个真实世界数据集上的规则质量。&lt;h4&gt;结论&lt;/h4&gt;结合神经符号方法和表格基础模型微调的技术在高维、低数据场景中能有效提升关联规则挖掘的性能和质量。&lt;h4&gt;翻译&lt;/h4&gt;关联规则挖掘旨在以命题规则的形式发现数据集中特征之间的模式，支持高风险决策中的知识发现和可解释机器学习。然而，在高维设置中，规则爆炸和计算开销使得没有有效搜索空间减少的流行算法方法不切实际，这些挑战会传递到下游任务。神经符号方法，如Aerial+，最近被提出以解决ARM中的规则爆炸问题。虽然它们处理了数据的高维性，但也继承了神经网络的局限性，特别是在低数据环境下的性能降低。本文对高维表格数据中的关联规则发现做出了三项关键贡献。首先，我们在五个真实世界数据集上 empirically 证明 Aerial+ 比最先进的算法和神经符号基线扩展性好一到两个数量级。其次，我们提出了高维、低数据设置下的ARM新问题，例如生物医学领域具有约18k特征和50个样本的基因表达数据。第三，我们提出了两种使用表格基础模型对Aerial+进行微调的方法。我们提出的方法在五个真实世界数据集上被证明显著提高了规则质量，展示了它们在低数据、高维场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Association Rule Mining (ARM) aims to discover patterns between features indatasets in the form of propositional rules, supporting both knowledgediscovery and interpretable machine learning in high-stakes decision-making.However, in high-dimensional settings, rule explosion and computationaloverhead render popular algorithmic approaches impractical without effectivesearch space reduction, challenges that propagate to downstream tasks.Neurosymbolic methods, such as Aerial+, have recently been proposed to addressthe rule explosion in ARM. While they tackle the high dimensionality of thedata, they also inherit limitations of neural networks, particularly reducedperformance in low-data regimes.  This paper makes three key contributions to association rule discovery inhigh-dimensional tabular data. First, we empirically show that Aerial+ scalesone to two orders of magnitude better than state-of-the-art algorithmic andneurosymbolic baselines across five real-world datasets. Second, we introducethe novel problem of ARM in high-dimensional, low-data settings, such as geneexpression data from the biomedicine domain with around 18k features and 50samples. Third, we propose two fine-tuning approaches to Aerial+ using tabularfoundation models. Our proposed approaches are shown to significantly improverule quality on five real-world datasets, demonstrating their effectiveness inlow-data, high-dimensional scenarios.</description>
      <author>example@mail.com (Erkan Karabulut, Daniel Daza, Paul Groth, Victoria Degeler)</author>
      <guid isPermaLink="false">2509.20113v2</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2509.20107v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的高光谱适配器架构，利用预训练视觉基础模型有效学习高光谱数据，在自动驾驶场景中实现了最先进的语义分割性能。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像能够捕获空间信息和密集的光谱测量，在复杂材料成分、光照变化等视觉挑战性环境中具有促进机器人感知的潜力。然而，当前HSI语义分割方法表现不佳，因为它们依赖于为RGB输入优化的架构和学习框架。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的高光谱适配器，利用预训练的视觉基础模型有效学习高光谱数据，解决当前HSI语义分割方法性能不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种包含光谱变换器和光谱感知空间先验模块的架构，以提取丰富的空间-光谱特征。同时引入了一种模态感知交互块，通过专门的提取和注入机制促进高光谱表示和冻结视觉Transformer特征的有效集成。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准自动驾驶数据集上的广泛评估表明，该架构在使用HSI输入直接时实现了最先进的语义分割性能，优于基于视觉和高光谱分割的方法。&lt;h4&gt;结论&lt;/h4&gt;该研究成功解决了HSI语义分割中的性能问题，通过新的适配器架构实现了与视觉方法相当或更好的性能，为高光谱成像在机器人感知中的应用提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像捕获空间信息以及众多窄波长波段上的密集光谱测量。这种丰富的光谱内容有可能促进强大的机器人感知，特别是在具有复杂材料成分、变化光照或其他视觉挑战性条件的环境中。然而，当前的高光谱语义分割方法表现不佳，因为它们依赖于为RGB输入优化的架构和学习框架。在这项工作中，我们提出了一种新的高光谱适配器，它利用预训练的视觉基础模型来有效学习高光谱数据。我们的架构集成了光谱变换器和光谱感知空间先验模块，以提取丰富的空间-光谱特征。此外，我们引入了一种模态感知交互块，通过专门的提取和注入机制促进高光谱表示和冻结视觉Transformer特征的有效集成。在三个基准自动驾驶数据集上的广泛评估表明，我们的架构在使用HSI输入直接时实现了最先进的语义分割性能，优于基于视觉和高光谱的分割方法。我们在https://hsi-adapter.cs.uni-freiburg.de提供代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) captures spatial information along with densespectral measurements across numerous narrow wavelength bands. This richspectral content has the potential to facilitate robust robotic perception,particularly in environments with complex material compositions, varyingillumination, or other visually challenging conditions. However, current HSIsemantic segmentation methods underperform due to their reliance onarchitectures and learning frameworks optimized for RGB inputs. In this work,we propose a novel hyperspectral adapter that leverages pretrained visionfoundation models to effectively learn from hyperspectral data. Ourarchitecture incorporates a spectral transformer and a spectrum-aware spatialprior module to extract rich spatial-spectral features. Additionally, weintroduce a modality-aware interaction block that facilitates effectiveintegration of hyperspectral representations and frozen vision Transformerfeatures through dedicated extraction and injection mechanisms. Extensiveevaluations on three benchmark autonomous driving datasets demonstrate that ourarchitecture achieves state-of-the-art semantic segmentation performance whiledirectly using HSI inputs, outperforming both vision-based and hyperspectralsegmentation methods. We make the code available athttps://hsi-adapter.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Juana Valeria Hurtado, Rohit Mohan, Abhinav Valada)</author>
      <guid isPermaLink="false">2509.20107v2</guid>
      <pubDate>Fri, 26 Sep 2025 15:23:10 +0800</pubDate>
    </item>
    <item>
      <title>CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation</title>
      <link>http://arxiv.org/abs/2509.19936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CapStARE是一种基于胶囊的时空架构，用于凝视估计，结合了ConvNeXt主干网络、注意力路由的胶囊形成和专门处理慢速和快速凝视动态的双GRU解码器，实现了高性能和实时推理能力。&lt;h4&gt;背景&lt;/h4&gt;凝视估计是交互系统中的关键任务，需要准确估计人的凝视方向，同时处理不同凝视动态和复杂环境条件。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效、准确且实时运行的凝视估计模型，能够处理不同凝视动态，并在各种条件下具有良好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;采用模块化设计的CapStARE架构，包含ConvNeXt主干网络、注意力路由的胶囊形成和双GRU解码器，支持高效的部件-整体推理和解耦的时序建模。&lt;h4&gt;主要发现&lt;/h4&gt;在ETH-XGaze(3.36)和MPIIFaceGaze(2.65)数据集上达到最先进性能，实现实时推理(&lt;10ms)，在Gaze360(9.06)和RT-GENE(4.76)等不同场景下表现良好，且使用更少参数并具有更高可解释性。&lt;h4&gt;结论&lt;/h4&gt;CapStARE为交互系统中的实时凝视估计提供了一种实用且稳健的解决方案，在保持高性能的同时实现了高效推理和良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了CapStARE，一种基于胶囊的时空架构用于凝视估计，它集成了ConvNeXt主干网络、注意力路由的胶囊形成和专门处理慢速与快速凝视动态的双GRU解码器。这种模块化设计能够实现高效的部件-整体推理和解耦的时序建模，在ETH-XGaze(3.36)和MPIIFaceGaze(2.65)上达到最先进的性能，同时保持实时推理(&lt;10ms)。该模型在Gaze360(9.06)的无约束条件和RT-GENE(4.76)的人机交互场景中也具有良好的泛化能力，以更少的参数和更高的可解释性优于或匹配现有方法。这些结果表明CapStARE为交互系统中的实时凝视估计提供了实用且稳健的解决方案。相关代码和结果可在https://github.com/toukapy/capsStare找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce CapStARE, a capsule-based spatio-temporal architecture for gazeestimation that integrates a ConvNeXt backbone, capsule formation withattention routing, and dual GRU decoders specialized for slow and rapid gazedynamics. This modular design enables efficient part-whole reasoning anddisentangled temporal modeling, achieving state-of-the-art performance onETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference(&lt; 10 ms). The model also generalizes well to unconstrained conditions inGaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76),outperforming or matching existing methods with fewer parameters and greaterinterpretability. These results demonstrate that CapStARE offers a practicaland robust solution for real-time gaze estimation in interactive systems. Therelated code and results for this article can be found on:https://github.com/toukapy/capsStare</description>
      <author>example@mail.com (Miren Samaniego, Igor Rodriguez, Elena Lazkano)</author>
      <guid isPermaLink="false">2509.19936v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
  <item>
      <title>iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning</title>
      <link>http://arxiv.org/abs/2509.19552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;iFinder是一个结构化的语义基础框架，通过将驾驶摄像头视频转换为分层、可解释的数据结构，使大型语言模型能够进行驾驶视频分析。它使用预训练视觉模型提取关键线索，并结合三块提示策略，显著提高了事故推理准确性，最高达39%。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在特定领域任务如驾驶视频分析中面临挑战，因为它们是通用训练的，缺乏结构化的归纳偏见。在驾驶分析中，视觉通常是唯一可用的模态，现有的基于视频的视觉-语言模型在空间推理、因果推断和事件解释性方面存在问题。&lt;h4&gt;目的&lt;/h4&gt;介绍iFinder，一个结构化的语义基础框架，解决V-VLMs在驾驶视频分析中的局限性，使大型语言模型能够更好地处理驾驶视频分析任务。&lt;h4&gt;方法&lt;/h4&gt;iFinder将感知与推理解耦，通过将驾驶摄像头视频转换为分层、可解释的数据结构供LLMs使用。它作为一个模块化、无需训练的流水线运行，使用预训练的视觉模型提取物体姿态、车道位置和物体轨迹等关键线索，并将这些线索分层组织成帧级和视频级结构。结合三块提示策略，使LLM能够进行逐步的、基于基础的推理，精炼另一个V-VLM的输出并提供准确的推理。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共驾驶摄像头视频基准测试上的评估显示，iFinder提出的领域特定线索的基础，特别是物体方向和全局上下文，在四个零样本驾驶基准测试上显著优于端到端的V-VLMs，事故推理准确性提高了高达39%。&lt;h4&gt;结论&lt;/h4&gt;通过使用驾驶领域特定表示来基础化LLMs，iFinder为端到端V-VLMs提供了一个零样本、可解释且可靠的替代方案，用于后置驾驶视频理解。&lt;h4&gt;翻译&lt;/h4&gt;将大型语言模型应用于特定领域任务（如后置摄像头驾驶视频分析）具有挑战性，因为它们是通用训练的，缺乏结构化的归纳偏见。由于视觉通常是此类分析中唯一可用的模态（即没有LiDAR、GPS等），现有的基于视频的视觉-语言模型在空间推理、因果推断和输入视频中事件的解释性方面存在困难。为此，我们介绍了iFinder，一个结构化的语义基础框架，它通过将驾驶摄像头视频转换为分层的、可解释的数据结构供LLMs使用，从而将感知与推理解耦。iFinder作为一个模块化、无需训练的流水线运行，使用预训练的视觉模型提取关键线索——物体姿态、车道位置和物体轨迹——这些线索被分层组织成帧级和视频级结构。结合三块提示策略，它使LLM能够进行逐步的、基于基础的推理，以精炼另一个V-VLM的输出并提供准确的推理。在四个公共驾驶摄像头视频基准测试上的评估显示，iFinder提出的领域特定线索的基础，特别是物体方向和全局上下文，在四个零样本驾驶基准测试上显著优于端到端的V-VLMs，事故推理准确性提高了高达39%。通过使用驾驶领域特定表示来基础化LLMs，iFinder为端到端V-VLMs提供了一个零样本、可解释且可靠的替代方案，用于后置驾驶视频理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grounding large language models (LLMs) in domain-specific tasks like post-hocdash-cam driving video analysis is challenging due to their general-purposetraining and lack of structured inductive biases. As vision is often the solemodality available for such analysis (i.e., no LiDAR, GPS, etc.), existingvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,causal inference, and explainability of events in the input video. To this end,we introduce iFinder, a structured semantic grounding framework that decouplesperception from reasoning by translating dash-cam videos into a hierarchical,interpretable data structure for LLMs. iFinder operates as a modular,training-free pipeline that employs pretrained vision models to extractcritical cues -- object pose, lane positions, and object trajectories -- whichare hierarchically organized into frame- and video-level structures. Combinedwith a three-block prompting strategy, it enables step-wise, grounded reasoningfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.Evaluations on four public dash-cam video benchmarks show that iFinder'sproposed grounding with domain-specific cues, especially object orientation andglobal context, significantly outperforms end-to-end V-VLMs on four zero-shotdriving benchmarks, with up to 39% gains in accident reasoning accuracy. Bygrounding LLMs with driving domain-specific representations, iFinder offers azero-shot, interpretable, and reliable alternative to end-to-end V-VLMs forpost-hoc driving video understanding.</description>
      <author>example@mail.com (Manyi Yao, Bingbing Zhuang, Sparsh Garg, Amit Roy-Chowdhury, Christian Shelton, Manmohan Chandraker, Abhishek Aich)</author>
      <guid isPermaLink="false">2509.19552v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Transformer Modeling for Both Scalability and Performance in Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2509.19471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;变量数量是多元时间序列Transformer建模的主要可扩展性瓶颈，无差别变量间混合会导致噪声累积和性能下降。DELTAformer通过委托令牌约束变量间混合，实现线性扩展的同时提高性能，在基准测试中表现最佳，并在嘈杂环境中展现出更强的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;变量数量是多元时间序列(MTS)数据中Transformer建模的主要可扩展性瓶颈之一。该领域日益达成共识，认为无差别的变量间混合可能是噪声累积和性能下降的潜在来源。许多MTS系统具有信息信号稀疏性特征，加上来自异构变量间无差别信息混合的表示错位，可能会加剧这一问题。&lt;h4&gt;目的&lt;/h4&gt;在MTS中同时提高可扩展性和性能，通过策略性地限制变量间混合的表示能力来实现这一目标。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为DELTAformer的方法（具有委托令牌注意力的Transformer）。通过所谓的委托令牌约束变量间建模，然后使用这些委托令牌执行完全无约束的时间间建模。委托令牌充当隐式正则化器，强制模型对允许通过网络传播的变量间信息高度选择性。&lt;h4&gt;主要发现&lt;/h4&gt;DELTAformer随变量数量线性扩展，实际上优于标准Transformer，在基准测试和基线中实现了最先进的性能。在嘈杂的MTS环境中比标准Transformer更好地关注相关信号，总体表现出更强的噪声鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过将模型设计与利用MTS领域特定挑战相结合，DELTAformer可以同时实现线性扩展，同时与标准的二次方Transformer相比，实际上提高了其性能。&lt;h4&gt;翻译&lt;/h4&gt;变量数量是多元时间序列Transformer建模中的主要可扩展性瓶颈之一。此外，该领域日益达成的共识表明，无差别的变量间混合可能是噪声累积和性能下降的潜在来源。这很可能被许多MTS系统固有的信息信号稀疏性所加剧，同时也源于异构变量间无差别信息混合导致的表示错位。虽然可扩展性和性能在Transformer设计中通常被视为相互竞争的目标，但我们证明在MTS中，通过策略性地约束变量间混合的表示能力，可以同时提高两者。我们提出的方法是具有委托令牌注意力的Transformer（DELTAformer），通过我们所谓的委托令牌约束变量间建模，然后使用这些委托令牌执行完全无约束的时间间建模。委托令牌充当隐式正则化器，强制模型对允许通过网络传播的变量间信息高度选择性。我们的结果表明，DELTAformer随变量数量线性扩展，同时实际上优于标准Transformer，在基准测试和基线中实现了最先进的性能。此外，DELTAformer在嘈杂的MTS环境中比标准Transformer能更好地关注相关信号，总体表现出更强的噪声鲁棒性。总体而言，各种实验的结果证实，通过将我们的模型设计与利用MTS中的领域特定挑战相结合，DELTAformer可以同时实现线性扩展，同时实际上提高了其性能，与标准的二次方Transformer相比。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Variable count is among the main scalability bottlenecks for transformermodeling in multivariate time series (MTS) data. On top of this, a growingconsensus in the field points to indiscriminate inter-variable mixing as apotential source of noise-accumulation and performance degradation. This islikely exacerbated by sparsity of informative signals characteristic of manyMTS systems coupled with representational misalignment stemming fromindiscriminate information mixing between (heterogeneous) variables. Whilescalability and performance are often seen as competing interests intransformer design, we show that both can be improved simultaneously in MTS bystrategically constraining the representational capacity of inter-variablemixing. Our proposed method, transformer with Delegate Token Attention(DELTAformer), constrains inter-variable modeling through what we call delegatetokens which are then used to perform full, unconstrained, inter-temporalmodeling. Delegate tokens act as an implicit regularizer that forces the modelto be highly selective about what inter-variable information is allowed topropagate through the network. Our results show that DELTAformer scaleslinearly with variable-count while actually outperforming standardtransformers, achieving state-of-the-art performance across benchmarks andbaselines. In addition, DELTAformer can focus on relevant signals better thanstandard transformers in noisy MTS environments and overall exhibit superiornoise-resilience. Overall, results across various experiments confirm that byaligning our model design to leverage domain-specific challenges in MTS to ouradvantage, DELTAformer can simultaneously achieve linear scaling while actuallyimproving its performance against standard, quadratic transformers.</description>
      <author>example@mail.com (Hunjae Lee, Corey Clark)</author>
      <guid isPermaLink="false">2509.19471v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>COLT: Enhancing Video Large Language Models with Continual Tool Usage</title>
      <link>http://arxiv.org/abs/2509.18754v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为COLT（COntinuaL Tool usage）的方法，用于增强开源视频大型语言模型，使其能够在持续变化和流动的工具数据中自动获取工具使用能力而不会遗忘已学习的工具。同时，作者还收集了VideoToolBench数据集，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的成功显著推动了视频理解领域的研究。现有的视频LLMs主要探索工具使用能力，但现有方法依赖于固定的工具库，难以适应现实世界中不断变化和流动的工具数据。&lt;h4&gt;目的&lt;/h4&gt;增强开源视频LLMs，使其能够在持续变化和流动的工具数据中自动获取工具使用能力，同时避免对已学习工具的'灾难性遗忘'。&lt;h4&gt;方法&lt;/h4&gt;作者提出了COLT方法，它包含一个可学习的工具代码本作为特定工具的记忆系统。然后，基于用户指令与工具代码本中工具特征的相似性动态选择相关工具。此外，作者还收集了一个名为VideoToolBench的视频中心工具使用指令微调数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在先前视频LLM基准和特定工具使用的VideoToolBench数据集上的大量实验表明，所提出的COLT达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;COLT方法能够有效增强开源视频LLMs的工具使用能力，使其能够适应不断变化和流动的工具数据，同时保持对已学习工具的记忆。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型的成功显著推动了视频理解的研究。为了利用已训练专家模型（即工具）的优势，视频LLMs优先探索工具使用能力。然而，现有方法要么提示闭源LLMs，要么采用指令微调范式进行工具使用微调。这些方法假设有一个固定的工具库，难以推广到工具数据不断变化和流动的现实世界环境中。为此，我们提出通过持续工具使用（称为COLT）增强开源视频LLMs，该方法能够在连续工具流中自动获取工具使用能力，而不会对过去学习的工具造成'灾难性遗忘'。具体来说，我们的COLT包含一个可学习的工具代码本作为特定工具的记忆系统。然后，基于用户指令与代码本内工具特征的相似性动态选择相关工具。为了释放视频LLMs的工具使用潜力，我们收集了一个以视频为中心的工具使用指令微调数据集VideoToolBench。在先前视频LLM基准和特定工具使用的VideoToolBench数据集上的大量实验证明了我们提出的COLT的最先进性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The success of Large Language Models (LLMs) has significantly propelled theresearch of video understanding. To harvest the benefits of well-trained expertmodels (i.e., tools), video LLMs prioritize the exploration of tool usagecapabilities. Existing methods either prompt closed-source LLMs or employ theinstruction tuning paradigm for tool-use fine-tuning. These methods, however,assume an established repository of fixed tools and struggle to generalize toreal-world environments where tool data is perpetually evolving and streamingin. To this end, we propose to enhance open-source video LLMs with COntinuaLTool usage (termed COLT), which automatically acquires tool-use ability in asuccessive tool stream without suffering 'catastrophic forgetting' of the pastlearned tools. Specifically, our COLT incorporates a learnable tool codebook asa tool-specific memory system. Then relevant tools are dynamically selectedbased on the similarity between user instruction and tool features within thecodebook. To unleash the tool usage potential of video LLMs, we collect avideo-centric tool-use instruction tuning dataset VideoToolBench. Extensiveexperiments on both previous video LLM benchmarks and the tool-use-specificVideoToolBench dataset demonstrate the state-of-the-art performance of ourproposed COLT.</description>
      <author>example@mail.com (Yuyang Liu, Xinyuan Shi, Xiaondan Liang)</author>
      <guid isPermaLink="false">2509.18754v2</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation</title>
      <link>http://arxiv.org/abs/2509.20269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合反向传播和预测编码的混合训练方法，用于在动态环境中实现高效的设备端域适应，解决了深度神经网络在真实场景中因数据分布变化导致的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络越来越多地部署在动态的真实环境中，仅依靠单一静态模型往往不够。传感器漂移或光照变化导致的输入数据分布变化需要模型持续适应。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合训练方法，通过结合反向传播和预测编码的优势，实现高效的设备端域适应。&lt;h4&gt;方法&lt;/h4&gt;该方法首先使用反向传播离线训练深度神经网络以获得高初始性能，然后使用预测编码进行在线适应，使模型能够恢复因输入数据分布变化而损失的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法利用反向传播在初始表示学习方面的稳健性和预测编码在持续学习方面的计算效率，特别适合资源受限的边缘设备或未来的神经形态加速器。在MNIST和CIFAR-10数据集上的实验结果表明，这种混合策略能够实现有效的适应，同时减少计算开销。&lt;h4&gt;结论&lt;/h4&gt;该混合策略为在动态环境中保持模型性能提供了一种有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着深度神经网络越来越多地部署在动态的真实环境中，依靠单一静态模型往往是不够的。传感器漂移或光照变化引起的输入数据分布变化需要模型持续适应。在本文中，我们提出了一种混合训练方法，通过结合反向传播和预测编码的优势，实现高效的设备端域适应。该方法首先使用反向传播离线训练深度神经网络以获得高初始性能。随后，使用预测编码进行在线适应，使模型能够恢复因输入数据分布变化而损失的准确性。这种方法利用了反向传播在初始表示学习方面的稳健性和预测编码在持续学习方面的计算效率，使其特别适合资源受限的边缘设备或未来的神经形态加速器。在MNIST和CIFAR-10数据集上的实验结果表明，这种混合策略能够实现有效的适应，同时减少计算开销，为在动态环境中保持模型性能提供了一种有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As deep neural networks are increasingly deployed in dynamic, real-worldenvironments, relying on a single static model is often insufficient. Changesin input data distributions caused by sensor drift or lighting variationsnecessitate continual model adaptation. In this paper, we propose a hybridtraining methodology that enables efficient on-device domain adaptation bycombining the strengths of Backpropagation and Predictive Coding. The methodbegins with a deep neural network trained offline using Backpropagation toachieve high initial performance. Subsequently, Predictive Coding is employedfor online adaptation, allowing the model to recover accuracy lost due toshifts in the input data distribution. This approach leverages the robustnessof Backpropagation for initial representation learning and the computationalefficiency of Predictive Coding for continual learning, making it particularlywell-suited for resource-constrained edge devices or future neuromorphicaccelerators. Experimental results on the MNIST and CIFAR-10 datasetsdemonstrate that this hybrid strategy enables effective adaptation with areduced computational overhead, offering a promising solution for maintainingmodel performance in dynamic environments.</description>
      <author>example@mail.com (Matteo Cardoni, Sam Leroux)</author>
      <guid isPermaLink="false">2509.20269v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations</title>
      <link>http://arxiv.org/abs/2509.20048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为扩散增强对比学习(DACL)的新型混合框架，用于学习生物信号的稳健表示，通过融合扩散模型和监督对比学习的概念，解决了传统数据增强方法无法捕捉生理数据复杂变化的问题。&lt;h4&gt;背景&lt;/h4&gt;学习生物信号的稳健表示常常受到有效数据增强设计挑战的阻碍。传统方法无法捕捉生理数据中固有的复杂变化，这限制了表示学习的有效性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的混合框架，能够更好地捕捉生理数据中的复杂变化，学习具有鲁棒性的生物信号表示。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为扩散增强对比学习(DACL)的混合框架，该框架在由轻量级变分自编码器(VAE)创建的潜在空间上运行，VAE在新型散射变换器(ST)特征上训练。该方法利用扩散前向过程作为数据增强技术生成潜在嵌入的多个噪声视图，并使用监督对比目标训练U-Net风格编码器，以学习在不同扩散时间步上平衡类判别性和噪声鲁棒性的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在PhysioNet 2017 ECG数据集上评估该方法，取得了0.7815的竞争性AUROC值，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;这项工作通过使用扩散过程本身来驱动对比目标，为表示学习建立了一种新范式，创建了噪声不变的嵌入，这些嵌入展示了类可分性的坚实基础，为生物信号表示学习提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;学习生物信号的稳健表示通常受到设计有效数据增强技术的挑战阻碍。传统方法可能无法捕捉生理数据中固有的复杂变化。在此背景下，我们提出了一种新的混合框架——扩散增强对比学习(DACL)，该框架融合了扩散模型和监督对比学习的概念。DACL框架在由轻量级变分自编码器(VAE)创建的潜在空间上运行，该VAE在我们的新型散射变换器(ST)特征[12]上训练。它利用扩散前向过程作为有原则的数据增强技术，生成这些潜在嵌入的多个噪声视图。然后，使用监督对比目标训练U-Net风格的编码器，以学习一种表示，该表示在不同扩散时间步上平衡了类判别性和对噪声的鲁棒性。我们在PhysioNet 2017 ECG数据集上评估了这个概念验证方法，取得了0.7815的竞争性AUROC。这项工作通过使用扩散过程本身来驱动对比目标，为表示学习建立了一种新范式，创建了噪声不变的嵌入，这些嵌入展示了类可分性的坚实基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning robust representations for biosignals is often hampered by thechallenge of designing effective data augmentations.Traditional methods canfail to capture the complex variations inherent in physiological data. Withinthis context, we propose a novel hybrid framework, Diffusion-AugmentedContrastive Learning (DACL), that fuses concepts from diffusion models andsupervised contrastive learning. The DACL framework operates on a latent spacecreated by a lightweight Variational Autoencoder (VAE) trained on our novelScattering Transformer (ST) features [12]. It utilizes the diffusion forwardprocess as a principled data augmentation technique to generate multiple noisyviews of these latent embeddings. A U-Net style encoder is then trained with asupervised contrastive objective to learn a representation that balances classdiscrimination with robustness to noise across various diffusion time steps. Weevaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset,achieving a competitive AUROC of 0.7815. This work establishes a new paradigmfor representation learning by using the diffusion process itself to drive thecontrastive objective, creating noise-invariant embeddings that demonstrate astrong foundation for class separability.</description>
      <author>example@mail.com (Rami Zewail)</author>
      <guid isPermaLink="false">2509.20048v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Ricci Flow on Weighted Digraphs with Balancing Factor</title>
      <link>http://arxiv.org/abs/2509.19989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了有向加权图上的里奇流的严格公式化，提出了一种保持距离同时演化边权重的里奇流，并建立了其解的存在性和唯一性。为了捕捉有向网络中的不对称性，作者引入了节点级别的平衡因子来调节流出和流入。基于连续里奇流演化框架，提出了适用于数值计算的离散里奇流算法，数值研究表明该算法能够揭示结构不对称性和动态演化。&lt;h4&gt;背景&lt;/h4&gt;里奇曲率和里奇流已被证明是分析离散结构几何特性的强大工具，特别是在无向图中，它们已被应用于从社区检测到图表示学习的各种任务。然而，在有向图上的发展仍然有限，特别是里奇流的研究尤其不足。&lt;h4&gt;目的&lt;/h4&gt;在有向加权图上引入里奇流的严格公式化，解决有向网络上里奇流发展有限的问题，特别是探索里奇流在有向图中的应用。&lt;h4&gt;方法&lt;/h4&gt;引入了一种在有向加权图上的里奇流公式，这种流在演化边权重的过程中保持距离；建立了这种流解的存在性和唯一性；引入了节点级别的平衡因子来调节流出和流入；基于连续里奇流演化框架，提出了适用于数值计算的离散里奇流算法。&lt;h4&gt;主要发现&lt;/h4&gt;数值研究表明，所提出的里奇流算法能够揭示有向图的结构不对称性和动态演化特性。&lt;h4&gt;结论&lt;/h4&gt;该工作成功地将里奇流理论扩展到了有向加权图领域，为分析有向网络的结构特性提供了新的数学工具和方法。&lt;h4&gt;翻译&lt;/h4&gt;里奇曲率和里奇流已被证明是分析离散结构几何特性的强大工具，特别是在无向图中，它们已被应用于从社区检测到图表示学习的各种任务。然而，在有向图上的发展仍然有限，特别是里奇流的研究尤其不足。在这项工作中，我们引入了有向加权图上里奇流的严格公式化，这种流在演化边权重的过程中保持距离，并建立了其解的存在性和唯一性。为了捕捉有向网络中的不对称本质并增强建模更灵活结构的能力，我们引入了一个节点级别的平衡因子来调节流出和流入。基于连续里奇流演化框架，我们提出了一个适用于数值计算的离散里奇流算法。在各种有向图示例上的数值研究表明，所提出的流能够揭示结构不对称性和动态演化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ricci curvature and Ricci flow have proven to be powerful tools for analyzingthe geometry of discrete structures, particularly on undirected graphs, wherethey have been applied to tasks ranging from community detection to graphrepresentation learning. However, their development on directed graphs remainslimited, with Ricci flow being especially underexplored. In this work, weintroduce a rigorous formulation of Ricci flow on directed weighted graphs,which evolves edge weights while preserving distances, and establish both theexistence and uniqueness of its solutions. To capture the essence of asymmetryin directed networks and to enhance the capability of modeling more flexiblestructures, we incorporate a node-wise balancing factor that regulates betweenoutflow and inflow. Building on the continuous Ricci flow evolution framework,we propose a discrete Ricci flow algorithm that is applicable to numericalcomputing. Numerical studies on various directed graph examples demonstrate thecapacity of the proposed flow to reveal structural asymmetry and dynamicevolutions.</description>
      <author>example@mail.com (Shuliang Bai, Rui Li, Shuang Liu, Xin Lai)</author>
      <guid isPermaLink="false">2509.19989v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal-enhanced Federated Recommendation: A Group-wise Fusion Approach</title>
      <link>http://arxiv.org/abs/2509.19955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GFMFR的新型多模态融合机制，用于联邦推荐系统，通过将多模态表示学习转移到服务器端并采用感知群体的项目表示融合方法，解决了整合多模态特征的效率、分布异质性和细粒度对齐等挑战。&lt;h4&gt;背景&lt;/h4&gt;联邦推荐(FR)是一种新的学习范式，以隐私保护方式解决学习排序问题。然而，如何将多模态特征整合到联邦推荐中仍然是一个开放的挑战，主要面临效率、分布异质性和细粒度对齐等问题。&lt;h4&gt;目的&lt;/h4&gt;解决联邦推荐中整合多模态特征面临的效率、分布异质性和细粒度对齐等挑战，提出一种新的多模态融合机制。&lt;h4&gt;方法&lt;/h4&gt;提出GFMFR机制，将多模态表示学习转移到服务器端，服务器存储项目内容并使用高容量编码器生成丰富表示以减轻客户端开销；采用感知群体的项目表示融合方法，使相似用户间进行细粒度知识共享同时保留个人偏好；该融合损失可插入任何现有联邦推荐系统以增强多模态特征处理能力。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公共基准数据集上的大量实验表明，GFMFR持续优于最先进的多模态联邦推荐基线方法。&lt;h4&gt;结论&lt;/h4&gt;GFMFR有效地解决了联邦推荐中整合多模态特征的挑战，通过将表示学习转移到服务器端和采用群体感知的融合方法，提高了系统的性能和效率。&lt;h4&gt;翻译&lt;/h4&gt;联邦推荐(FR)是一种新的学习范式，以隐私保护的方式解决学习排序问题。如何将多模态特征整合到联邦推荐中，在效率、分布异质性和细粒度对齐方面仍然是一个开放的挑战。为了应对这些挑战，我们提出了联邦推荐环境中的新型多模态融合机制(GFMFR)。具体来说，它将多模态表示学习转移到服务器，服务器存储项目内容并使用高容量编码器生成丰富的表示，减轻了客户端的开销。此外，感知群体的项目表示融合方法使相似用户之间能够进行细粒度的知识共享，同时保留个人偏好。所提出的融合损失可以简单地插入到任何现有的联邦推荐系统中，通过添加多模态特征增强其能力。在五个公共基准数据集上的大量实验表明，GFMFR持续优于最先进的多模态联邦推荐基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Recommendation (FR) is a new learning paradigm to tackle thelearn-to-rank problem in a privacy-preservation manner. How to integratemulti-modality features into federated recommendation is still an openchallenge in terms of efficiency, distribution heterogeneity, and fine-grainedalignment. To address these challenges, we propose a novel multimodal fusionmechanism in federated recommendation settings (GFMFR). Specifically, itoffloads multimodal representation learning to the server, which stores itemcontent and employs a high-capacity encoder to generate expressiverepresentations, alleviating client-side overhead. Moreover, a group-aware itemrepresentation fusion approach enables fine-grained knowledge sharing amongsimilar users while retaining individual preferences. The proposed fusion losscould be simply plugged into any existing federated recommender systemsempowering their capability by adding multi-modality features. Extensiveexperiments on five public benchmark datasets demonstrate that GFMFRconsistently outperforms state-of-the-art multimodal FR baselines.</description>
      <author>example@mail.com (Chunxu Zhang, Weipeng Zhang, Guodong Long, Zhiheng Xue, Riting Xia, Bo Yang)</author>
      <guid isPermaLink="false">2509.19955v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network</title>
      <link>http://arxiv.org/abs/2509.19896v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures, reference 4 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了跨孔对齐掩码孪生网络（CWA-MSN），一种新的细胞图像表征学习框架，能够在不同批次中对相同干扰处理的细胞嵌入进行对齐，克服批次效应问题，同时保持数据和参数效率。&lt;h4&gt;背景&lt;/h4&gt;计算模型预测细胞对化学和遗传干扰的表型反应可加速药物发现，但提取具有生物学意义且批次稳健的细胞绘画表征具有挑战性。传统自监督和对比学习方法需要大规模模型和大量数据，仍难以处理批次效应。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的表征学习框架，能够在不同培养孔中对相同干扰处理的细胞嵌入进行对齐，强制实现语义一致性，克服批次效应问题。&lt;h4&gt;方法&lt;/h4&gt;提出跨孔对齐掩码孪生网络（CWA-MSN），集成到掩码孪生架构中，捕获细粒度形态特征，同时保持数据和参数效率。&lt;h4&gt;主要发现&lt;/h4&gt;在基因-关系检索基准测试中，CWA-MSN优于OpenPhenom和CellCLIP方法，分别提高29%和9%的基准分数，同时使用更少数据（0.2M图像vs 2.2M图像）和更小模型（22M参数vs 1.48B参数）。&lt;h4&gt;结论&lt;/h4&gt;CWA-MSN是一种简单有效的方法，用于学习细胞图像表征，能够在有限数据和参数预算下实现高效的表型建模。&lt;h4&gt;翻译&lt;/h4&gt;能够预测细胞对化学和遗传干扰的表型反应的计算模型可以通过优先选择治疗假设和减少昂贵的湿实验室迭代来加速药物发现。然而，提取具有生物学意义且批次稳健的细胞绘画表征仍然具有挑战性。传统的自监督和对比学习方法通常需要大规模模型和/或大量精心策划的数据，仍然难以处理批次效应。我们提出了跨孔对齐掩码孪生网络（CWA-MSN），这是一种新的表征学习框架，在不同培养孔中对相同干扰处理的细胞嵌入进行对齐，强制实现语义一致性，尽管存在批次效应。集成到掩码孪生架构中，这种对齐产生了能够捕获细粒度形态特征的特征，同时保持数据和参数效率。例如，在基因-关系检索基准测试中，CWA-MSN优于最先进的公开可用自监督（OpenPhenom）和对比学习（CellCLIP）方法，分别提高了29%和9%的基准分数，同时在显著较少的数据（例如CWA-MSN使用0.2M图像vs OpenPhenom使用2.2M图像）或更小的模型大小（例如CWA-MSN使用22M参数vs CellCLIP使用1.48B参数）上进行训练。大量实验表明，CWA-MSN是学习细胞图像表征的一种简单有效的方法，能够在有限数据和参数预算下实现高效的表型建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational models that predict cellular phenotypic responses to chemicaland genetic perturbations can accelerate drug discovery by prioritizingtherapeutic hypotheses and reducing costly wet-lab iteration. However,extracting biologically meaningful and batch-robust cell paintingrepresentations remains challenging. Conventional self-supervised andcontrastive learning approaches often require a large-scale model and/or a hugeamount of carefully curated data, still struggling with batch effects. Wepresent Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novelrepresentation learning framework that aligns embeddings of cells subjected tothe same perturbation across different wells, enforcing semantic consistencydespite batch effects. Integrated into a masked siamese architecture, thisalignment yields features that capture fine-grained morphology while remainingdata- and parameter-efficient. For instance, in a gene-gene relationshipretrieval benchmark, CWA-MSN outperforms the state-of-the-art publiclyavailable self-supervised (OpenPhenom) and contrastive learning (CellCLIP)methods, improving the benchmark scores by +29\% and +9\%, respectively, whiletraining on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2Mimages for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSNvs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate thatCWA-MSN is a simple and effective way to learn cell image representation,enabling efficient phenotype modeling even under limited data and parameterbudgets.</description>
      <author>example@mail.com (Pin-Jui Huang, Yu-Hsuan Liao, SooHeon Kim, NoSeong Park, JongBae Park, DongMyung Shin)</author>
      <guid isPermaLink="false">2509.19896v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive von Mises-Fisher Likelihood Loss for Supervised Deep Time Series Hashing</title>
      <link>http://arxiv.org/abs/2509.19625v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 6 figures, Conference: ICMLA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于von Mises-Fisher (vMF)哈希损失的时间序列深度哈希方法，通过将数据映射到M维超球面空间来减少信息损失，并将每个数据类建模为遵循不同vMF分布的点，实验结果表明该方法优于现有基线方法。&lt;h4&gt;背景&lt;/h4&gt;时间序列数据挖掘中的基本任务是创建紧凑的二进制表示。最近，基于深度学习的哈希方法已被证明可以根据语义意义而非原始相似性来索引时间序列。与其他监督表示学习方法不同，监督深度哈希需要一个离散化步骤将实值表示转换为二进制码，但这可能导致显著的信息损失。&lt;h4&gt;目的&lt;/h4&gt;解决监督深度哈希方法中离散化步骤导致的信息损失问题，提高时间序列索引的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种von Mises-Fisher (vMF)哈希损失。该深度哈希模型将数据映射到M维超球面空间以有效减少信息损失，并将每个数据类建模为遵循不同vMF分布的点。设计的损失函数旨在最大化每个建模的vMF分布之间的分离，为最大化每个语义不同的数据样本之间的间距提供更好的方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法优于现有的基线方法。&lt;h4&gt;结论&lt;/h4&gt;通过vMF哈希损失方法，可以有效减少深度哈希中的信息损失，提高时间序列索引的性能。实现已在https://github.com/jmpq97/vmf-hashing公开。&lt;h4&gt;翻译&lt;/h4&gt;通过创建紧凑的二进制表示来索引时间序列是时间序列数据挖掘中的基本任务。最近，基于深度学习的哈希方法已被证明可以根据语义意义而非原始相似性来索引时间序列。深度哈希的目的是将具有相同语义含义的样本映射到相同的二进制哈希码，从而实现更高效的搜索和检索。与其他监督表示学习方法不同，监督深度哈希需要一个离散化步骤将实值表示转换为二进制码，但这可能导致显著的信息损失。在本文中，我们提出了一种von Mises-Fisher (vMF)哈希损失。所提出的深度哈希模型将数据映射到M维超球面空间以有效减少信息损失，并将每个数据类建模为遵循不同vMF分布的点。设计的损失旨在最大化每个建模的vMF分布之间的分离，为最大化每个语义不同的数据样本之间的间距提供更好的方法。实验结果表明，我们的方法优于现有的基线方法。实现已在https://github.com/jmpq97/vmf-hashing公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indexing time series by creating compact binary representations is afundamental task in time series data mining. Recently, deep learning-basedhashing methods have proven effective for indexing time series based onsemantic meaning rather than just raw similarity. The purpose of deep hashingis to map samples with the same semantic meaning to identical binary hashcodes, enabling more efficient search and retrieval. Unlike other supervisedrepresentation learning methods, supervised deep hashing requires adiscretization step to convert real-valued representations into binary codes,but this can induce significant information loss. In this paper, we propose avon Mises-Fisher (vMF) hashing loss. The proposed deep hashing model maps datato an M-dimensional hyperspherical space to effectively reduce information lossand models each data class as points following distinct vMF distributions. Thedesigned loss aims to maximize the separation between each modeled vMFdistribution to provide a better way to maximize the margin between eachsemantically different data sample. Experimental results show that our methodoutperforms existing baselines. The implementation is publicly available athttps://github.com/jmpq97/vmf-hashing</description>
      <author>example@mail.com (Juan Manuel Perez, Kevin Garcia, Brooklyn Berry, Dongjin Song, Yifeng Gao)</author>
      <guid isPermaLink="false">2509.19625v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning</title>
      <link>http://arxiv.org/abs/2509.17552v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为上下文表征学习（ICRL）的新方法，允许大型语言模型（LLMs）无需训练即可整合非文本模态表征，实现多模态推理。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）的性能可通过测试时计算得到增强，但现有将非文本模态表征整合到LLMs中的方法通常需要额外的昂贵监督训练，限制了模型对新领域和模态的即时适应能力。&lt;h4&gt;目的&lt;/h4&gt;探索以无需训练的方式将非文本基础模型（FMs）的表征整合到文本基础的LLMs中的可行性，并提出一种方法使LLMs能够通过少样本学习自适应地利用非文本模态表征。&lt;h4&gt;方法&lt;/h4&gt;提出上下文表征学习（ICRL）作为概念验证，用FM表征替换传统上下文学习中的文本输入，使LLM能够在不进行微调的情况下执行多模态推理。在分子领域的一系列任务上评估ICRL，并探讨三个核心研究问题。&lt;h4&gt;主要发现&lt;/h4&gt;研究探讨了三个核心问题：(i) 如何以无需训练的方式将FM表征映射到LLMs中；(ii) 哪些因素影响ICRL性能；(iii) ICRL有效性的潜在机制。ICRL是首个无需训练即可将非文本模态表征整合到文本基础LLMs中的框架。&lt;h4&gt;结论&lt;/h4&gt;ICRL为可适应的多模态泛化提供了有前景的方向，使大型语言模型能够无需额外训练即可整合和使用非文本模态信息。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）的卓越性能可以通过测试时计算得到增强，这依赖于外部工具甚至其他深度学习模型。然而，现有将非文本模态表征整合到LLMs中的方法通常需要额外的昂贵监督训练，限制了模型对新领域和模态的即时适应能力。在本工作中，我们探索以无需训练的方式将非文本基础模型（FMs）的表征整合到文本基础的LLMs中的可行性。我们提出上下文表征学习（ICRL）作为概念验证，允许LLMs通过少样本学习自适应地利用非文本模态表征。与传统上下文学习不同（后者整合文本-标签对），ICRL用FM表征替换文本输入，使LLM能够在不进行微调的情况下执行多模态推理。我们在分子领域的一系列任务上评估了ICRL，研究了三个核心研究问题：(i) 如何以无需训练的方式将FM表征映射到LLMs中；(ii) 哪些因素影响ICRL性能；(iii) ICRL有效性的潜在机制。据我们所知，ICRL是首个无需训练即可将非文本模态表征整合到文本基础LLMs中的框架，为可适应的多模态泛化提供了有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The remarkable performance of Large Language Models (LLMs) can be enhancedwith test-time computation, which relies on external tools and even other deeplearning models. However, existing approaches for integrating non-text modalityrepresentations into LLMs typically require additional costly supervisedtraining, restricting on-the-fly adaptation to new domains and modalities. Inthis work, we explore the feasibility of integrating representations fromnon-text foundational models (FMs) into text-based LLMs in a training-freemanner. We propose In-Context Representation Learning (ICRL) as aproof-of-concept to allow LLMs to adaptively utilize non-text modalityrepresentations with few-shot learning. Unlike traditional in-context learning,which incorporates text-label pairs, ICRL replaces text inputs with FMrepresentations, enabling the LLM to perform multi-modal inference withoutfine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,investigating three core research questions: (i) how to map FM representationsinto LLMs in a training-free manner, (ii) what factors influence ICRLperformance, and (iii) what mechanisms underlie the effectiveness of ICRL. Tothe best of our knowledge, ICRL is the first training-free framework forintegrating non-text modality representations into text-based LLMs, presentinga promising direction for adaptable, multi-modal generalization.</description>
      <author>example@mail.com (Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak A. Subramanian, Alvin Chan)</author>
      <guid isPermaLink="false">2509.17552v2</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features</title>
      <link>http://arxiv.org/abs/2509.16629v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CAPE是一种新颖的位置编码方法，通过识别非顺序特征下的潜在因果结构并嵌入双曲空间，生成因果感知的位置编码，有效增强transformer对非顺序特征数据的处理能力。&lt;h4&gt;背景&lt;/h4&gt;位置编码对补充transformer中token的位置信息至关重要，但现有方法需要预定义的token/特征顺序，不适合处理现实世界中非顺序但因果相关的特征数据。&lt;h4&gt;目的&lt;/h4&gt;解决现有位置编码方法的局限性，提出一种适用于非顺序特征数据的位置编码方法。&lt;h4&gt;方法&lt;/h4&gt;提出CAPE方法，识别非顺序特征下的潜在因果结构并建模为加权有向无环图(DAG)，使用广义结构方程建模方法实现，然后将DAG嵌入双曲空间保留其几何结构，捕捉因果强度和因果特异性两个重要属性，生成因果感知的位置编码并转换为旋转形式以与transformer的自注意力机制集成。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明CAPE生成的旋转位置编码具有三个有价值特性：因果距离引起的衰减、因果普遍性引起的衰减以及对位置扰动的鲁棒性；在合成和真实世界数据集上的实验验证了其理论特性和有效性。&lt;h4&gt;结论&lt;/h4&gt;CAPE能够有效增强transformer对非顺序特征数据的处理能力，相关代码已公开在https://github.com/Catchxu/CAPE。&lt;h4&gt;翻译&lt;/h4&gt;位置编码对于补充transformer中token的位置信息至关重要。现有的位置编码方法需要预定义的token/特征顺序，使其不适合处理现实世界中非顺序但因果相关的特征数据。为了解决这一局限性，我们提出了CAPE，一种新颖的方法，它使用广义结构方程建模识别非顺序特征下的潜在因果结构，并将其建模为加权有向无环图(DAG)。然后将DAG嵌入双曲空间，使用基于双曲模型的方法有效保留其几何结构，捕捉两个重要的因果图属性（因果强度和因果特异性）。这一步为特征生成了因果感知的位置编码，然后将其转换为旋转形式以与transformer的自注意力机制集成。理论分析表明，CAPE生成的旋转位置编码具有三个增强自注意力的宝贵特性，包括因果距离引起的衰减、因果普遍性引起的衰减以及对位置扰动的鲁棒性。我们在合成和真实世界数据集上评估了CAPE， empirically证明了其理论特性和在增强transformer处理非顺序特征数据方面的有效性。我们的代码可在https://github.com/Catchxu/CAPE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Positional encoding is essential for supplementing transformer withpositional information of tokens. Existing positional encoding methods demandpredefined token/feature order, rendering them unsuitable for real-world datawith non-sequential yet causally-related features. To address this limitation,we propose CAPE, a novel method that identifies underlying causal structureover non-sequential features as a weighted directed acyclic graph (DAG) usinggeneralized structural equation modeling. The DAG is then embedded inhyperbolic space where its geometric structure is well-preserved using ahyperboloid model-based approach that effectively captures two important causalgraph properties (causal strength &amp; causal specificity). This step yieldscausality-aware positional encodings for the features, which are converted intotheir rotary form for integrating with transformer's self-attention mechanism.Theoretical analysis reveals that CAPE-generated rotary positional encodingspossess three valuable properties for enhanced self-attention, including causaldistance-induced attenuation, causal generality-induced attenuation, androbustness to positional disturbances. We evaluate CAPE over both synthetic andreal-word datasets, empirically demonstrating its theoretical properties andeffectiveness in enhancing transformer for data with non-sequential features.Our code is available at https://github.com/Catchxu/CAPE.</description>
      <author>example@mail.com (Kaichen Xu, Yihang Du, Mianpeng Liu, Zimu Yu, Xiaobo Sun)</author>
      <guid isPermaLink="false">2509.16629v2</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning</title>
      <link>http://arxiv.org/abs/2509.20077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种3D可查询场景表示(3D QSR)框架，帮助机器人理解高级人类指令并在复杂3D环境中执行任务。&lt;h4&gt;背景&lt;/h4&gt;机器人理解高级人类指令并执行复杂任务的关键挑战在于实现全面场景理解：有意义地解释和与3D环境交互。&lt;h4&gt;目的&lt;/h4&gt;开发一个智能地图，将精确的几何结构与丰富、人类可理解的语义融合，使机器人能够更好地理解和执行复杂任务。&lt;h4&gt;方法&lt;/h4&gt;引入3D可查询场景表示(3D QSR)框架，统一三种互补的3D表示：3D一致的新视角渲染和分割、来自3D点云的精确几何、通过3D场景图进行结构化组织。该框架基于对象中心设计，与大型视觉语言模型集成，支持语义查询和对象级信息检索。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够促进场景理解，整合空间和语义推理，有效将高级人类指令转化为复杂3D环境中的精确机器人任务规划。&lt;h4&gt;结论&lt;/h4&gt;3D QSR框架成功解决了机器人理解高级人类指令并执行复杂任务的关键挑战，在模拟环境和真实湿实验室环境中均表现出色。&lt;h4&gt;翻译&lt;/h4&gt;为了使机器人能够理解高级人类指令并执行复杂任务，关键挑战在于实现全面的场景理解：以有意义的方式解释和与3D环境交互。这需要一个智能地图，将精确的几何结构与丰富、人类可理解的语义融合。为此，我们引入了3D可查询场景表示(3D QSR)，这是一个基于多媒体数据的新框架，统一了三种互补的3D表示：(1)从全景重建中获得的3D一致的新视角渲染和分割，(2)来自3D点云的精确几何，(3)通过3D场景图进行结构化、可扩展的组织。基于对象中心设计，该框架与大型视觉语言模型集成，通过链接多模态对象嵌入实现语义可查询性，并支持几何、视觉和语义信息的对象级检索。检索到的数据随后被加载到机器人任务规划器中用于下游执行。我们在Unity中通过模拟机器人任务规划场景评估了该方法，这些场景由抽象语言指令指导，并使用了室内公共数据集Replica。此外，我们在真实湿实验室环境的数字副本中应用了该方法，以测试QSR支持的机器人任务规划用于应急响应。结果表明，该框架能够促进场景理解，整合空间和语义推理，有效将高级人类指令转化为复杂3D环境中的精确机器人任务规划。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人如何理解高级人类指令并执行复杂任务的问题，特别是在3D环境中实现全面的场景理解。这个问题很重要，因为机器人需要将抽象的人类指令（如'我渴了'）转化为具体行动，包括推断意图、定位相关物品、评估可用性和规划路径，这对实现人机协作和机器人自主操作至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析传统3D地图的局限性（主要是几何性，缺乏语义信息）和语义理解与3D几何对齐的挑战，设计了一个多模态框架。他们借鉴了多项现有工作：NeRF等3D重建技术、Panoptic Lifting方法进行语义分割、CLIP等大型视觉-语言模型实现开放词汇理解、以及3D场景图作为结构化表示。作者将这些方法有机结合，创建了一个统一的、可查询的3D场景表示框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的、可查询的3D场景表示（3D QSR），融合几何、语义和结构信息，支持自然语言查询，将人类高层指令转化为机器人可执行的精确任务计划。整体流程包括：1) 构建场景表示（3D全景重建、点云分割、多视角描述生成、场景图构建）；2) 查询场景表示（点云查询、NeRF查询、场景图查询）；3) 连接场景理解与行动（在Unity模拟器中实现机器人导航和任务执行）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的多模态3D表示，结合全景辐射场、精确几何点云和结构化场景图；2) 多视角描述生成，利用大型视觉-语言模型纠正分割错误；3) 两阶段查询方法，显著提高复杂查询（如否定查询）的成功率；4) 场景整合与任务规划机制。相比之前工作，3D QSR克服了传统3D地图缺乏语义、单一模态系统表示不全面、现有查询地图方法忽视几何结构以及3D场景图缺乏视觉几何信息等局限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了3D可查询场景表示（3D QSR）多模态框架，通过统一几何、语义和结构信息，使机器人能够理解自然语言查询并执行复杂任务，实现了从高层人类指令到精确机器人行动的转化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To enable robots to comprehend high-level human instructions and performcomplex tasks, a key challenge lies in achieving comprehensive sceneunderstanding: interpreting and interacting with the 3D environment in ameaningful way. This requires a smart map that fuses accurate geometricstructure with rich, human-understandable semantics. To address this, weintroduce the 3D Queryable Scene Representation (3D QSR), a novel frameworkbuilt on multimedia data that unifies three complementary 3D representations:(1) 3D-consistent novel view rendering and segmentation from panopticreconstruction, (2) precise geometry from 3D point clouds, and (3) structured,scalable organization via 3D scene graphs. Built on an object-centric design,the framework integrates with large vision-language models to enable semanticqueryability by linking multimodal object embeddings, and supportingobject-level retrieval of geometric, visual, and semantic information. Theretrieved data are then loaded into a robotic task planner for downstreamexecution. We evaluate our approach through simulated robotic task planningscenarios in Unity, guided by abstract language instructions and using theindoor public dataset Replica. Furthermore, we apply it in a digital duplicateof a real wet lab environment to test QSR-supported robotic task planning foremergency response. The results demonstrate the framework's ability tofacilitate scene understanding and integrate spatial and semantic reasoning,effectively translating high-level human instructions into precise robotic taskplanning in complex 3D environments.</description>
      <author>example@mail.com (Xun Li, Rodrigo Santa Cruz, Mingze Xi, Hu Zhang, Madhawa Perera, Ziwei Wang, Ahalya Ravendran, Brandon J. Matthews, Feng Xu, Matt Adcock, Dadong Wang, Jiajun Liu)</author>
      <guid isPermaLink="false">2509.20077v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.19973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为OmniScene的新型类人框架，通过模拟人类视觉系统的场景理解能力，显著提升了自动驾驶系统对复杂环境的认知能力。&lt;h4&gt;背景&lt;/h4&gt;人类视觉能够将二维观察转化为以自我为中心的三维场景理解，支持翻译复杂场景和展示适应性行为。然而，当前自动驾驶系统缺乏这种能力，主流方法主要依赖基于深度的3D重建而非真正的场景理解。&lt;h4&gt;目的&lt;/h4&gt;解决自动驾驶系统中缺乏类人场景理解能力的问题，提出一种更接近人类认知方式的框架。&lt;h4&gt;方法&lt;/h4&gt;提出OmniScene框架，包含：1) OmniVLM视觉语言模型整合多视图和时间感知实现4D场景理解；2) 利用教师-学生架构和知识蒸馏将文本表示嵌入3D实例特征；3) 将特征与人类驾驶行为对齐形成类人感知-理解-行动架构；4) 提出分层融合策略解决多模态整合中的模态贡献不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上与十多种最先进模型比较，OmniScene在感知、预测、规划和视觉问答任务上均取得优异结果，建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;OmniScene通过模拟人类视觉系统的工作方式，有效提升了自动驾驶系统的场景理解能力，使其能更接近人类认知方式处理复杂环境。&lt;h4&gt;翻译&lt;/h4&gt;人类视觉能够将二维观察转化为以自我为中心的三维场景理解，这支持了翻译复杂场景和展示适应性行为的能力。然而，当前的自动驾驶系统仍缺乏这种能力，主流方法主要依赖基于深度的3D重建，而非真正的场景理解。为了解决这一局限性，我们提出了一种名为OmniScene的新型类人框架。首先，我们引入了OmniScene视觉语言模型，这是一个整合多视图和时间感知的视觉语言框架，用于整体的4D场景理解。然后，利用教师-学生OmniVLM架构和知识蒸馏，我们将文本表示嵌入到3D实例特征中，用于语义监督，丰富特征学习并明确捕获类人注意语义。这些特征表示进一步与人类驾驶行为对齐，形成更类人的感知-理解-行动架构。此外，我们提出了分层融合策略来解决多模态整合过程中模态贡献不平衡的问题。我们的方法能够在多个抽象层次上自适应校准几何和语义特征的相对重要性，实现视觉和文本模态互补线索的协同使用。这种可学习的动态融合能够更细致有效地利用异构信息。我们在nuScenes数据集上对OmniScene进行了全面评估，在各种任务上与十多种最先进的模型进行基准测试。我们的方法始终取得优异结果，在感知、预测、规划和视觉问答方面建立了新的基准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统缺乏真正场景理解能力的问题。当前系统主要依赖基于深度的3D重建，无法像人类那样将二维观察转化为三维场景理解并做出适应性决策。这个问题很重要，因为真正的场景理解是确保自动驾驶系统安全性、适应性和决策质量的关键，特别是在复杂和动态的交通环境中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析人类视觉系统的特点（将二维观察转化为三维场景理解）和当前自动驾驶系统的不足（缺乏有效整合感知和场景理解）来设计方法。他们借鉴了多模态信息融合机制、端到端自动驾驶系统和视觉语言模型在自动驾驶中的应用经验。具体来说，他们结合了基于注意力的融合机制、可学习融合策略、稀疏查询范式和知识蒸馏技术，设计了OmniScene框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将人类视觉的注意力机制和场景理解能力引入自动驾驶系统，通过多模态融合实现更全面的4D场景理解。整体流程是：1)接收多视角图像流、操作命令和用户提示；2)学生OmniVLM生成场景文本注释，视觉编码层提取视觉特征；3)使用CLIP模型将文本转换为特征表示；4)通过分层融合策略融合3D实例特征、视觉特征和文本特征；5)教师OmniVLM生成丰富文本描述，学生模型学习关注关键区域；6)提供全面场景表示支持下游任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)OmniScene框架实现类人感知-理解-行动架构；2)OmniVLM模型通过教师-学生架构实现注意力知识蒸馏；3)分层融合策略(HFS)解决模态贡献不平衡问题；4)全面的4D场景理解结合几何和语义特征。相比之前工作，传统方法依赖3D重建缺乏场景理解，现有端到端系统未有效整合感知与理解，多模态方法未深度整合视觉和文本，且缺乏明确的类人注意力建模。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OmniScene通过引入类人注意力机制和分层多模态融合策略，显著提升了自动驾驶系统的4D场景理解能力，在感知、预测、规划和视觉问答等任务上实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human vision is capable of transforming two-dimensional observations into anegocentric three-dimensional scene understanding, which underpins the abilityto translate complex scenes and exhibit adaptive behaviors. This capability,however, remains lacking in current autonomous driving systems, wheremainstream approaches primarily rely on depth-based 3D reconstruction ratherthan true scene understanding. To address this limitation, we propose a novelhuman-like framework called OmniScene. First, we introduce the OmniSceneVision-Language Model (OmniVLM), a vision-language framework that integratesmulti-view and temporal perception for holistic 4D scene understanding. Then,harnessing a teacher-student OmniVLM architecture and knowledge distillation,we embed textual representations into 3D instance features for semanticsupervision, enriching feature learning, and explicitly capturing human-likeattentional semantics. These feature representations are further aligned withhuman driving behaviors, forming a more human-likeperception-understanding-action architecture. In addition, we propose aHierarchical Fusion Strategy (HFS) to address imbalances in modalitycontributions during multimodal integration. Our approach adaptively calibratesthe relative significance of geometric and semantic features at multipleabstraction levels, enabling the synergistic use of complementary cues fromvisual and textual modalities. This learnable dynamic fusion enables a morenuanced and effective exploitation of heterogeneous information. We evaluateOmniScene comprehensively on the nuScenes dataset, benchmarking it against overten state-of-the-art models across various tasks. Our approach consistentlyachieves superior results, establishing new benchmarks in perception,prediction, planning, and visual question answering.</description>
      <author>example@mail.com (Pei Liu, Hongliang Lu, Haichao Liu, Haipeng Liu, Xin Liu, Ruoyu Yao, Shengbo Eben Li, Jun Ma)</author>
      <guid isPermaLink="false">2509.19973v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping</title>
      <link>http://arxiv.org/abs/2509.19579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合室内3D场景图技术与户外几何映射和地形感知推理的新方法，用于户外智能自主机器人操作。该方法生成了任务无关的度量-语义稀疏地图，并构建了3D场景图，同时保持轻量级特性以适应自主机器人操作。&lt;h4&gt;背景&lt;/h4&gt;户外智能自主机器人操作依赖于充分表达的环境地图。传统几何映射方法保留了基本的环境结构信息，但缺乏语义理解和组织，无法支持高级机器人推理。3D场景图(3DSGs)通过整合几何、拓扑和语义关系到多级基于图的地图中解决了这一局限性。户外自主操作通常依赖地形信息，这既与任务需求相关，也与机器人平台的可通行性有关。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合室内3D场景图技术与户外几何映射和地形感知推理的新方法，为户外环境生成地形感知的位置节点和层次化区域组织，并构建轻量级的3D场景图以支持下游规划任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新方法，将室内3D场景图技术与标准户外几何映射和地形感知推理相结合，生成地形感知的位置节点和层次化组织的户外区域。该方法生成任务无关的度量-语义稀疏地图，并从中构建3D场景图，同时保持轻量级特性以适应自主机器人操作。&lt;h4&gt;主要发现&lt;/h4&gt;全面评估表明，所提出的3D场景图方法在物体检索方面与最先进的基于相机的3D场景图方法性能相当，在区域分类方面超越了它们，同时保持内存效率。在模拟和真实世界环境中的物体检索和区域监测等多样化机器人任务中，该方法展示了其有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法成功地将室内3D场景图技术扩展到户外环境，通过整合地形信息提高了地图的语义理解和组织能力，同时保持轻量级特性，使其适合自主机器人操作。在多种任务中展示了其有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;户外智能自主机器人操作依赖于充分表达的环境地图。传统几何映射方法保留了基本的环境结构信息，但缺乏语义理解和组织，无法支持高级机器人推理。3D场景图(3DSGs)通过整合几何、拓扑和语义关系到多级基于图的地图中解决了这一局限性。户外自主操作通常依赖地形信息，这既与任务需求相关，也与机器人平台的可通行性有关。我们提出了一种新方法，将室内3D场景图技术与标准户外几何映射和地形感知推理相结合，为户外环境生成地形感知的位置节点和层次化区域组织。我们的方法生成了任务无关的度量-语义稀疏地图，并从中构建3D场景图以支持下游规划任务，同时保持轻量级特性以适应自主机器人操作。我们的全面评估表明，所提出的3D场景图方法在物体检索方面与最先进的基于相机的3D场景图方法性能相当，在区域分类方面超越了它们，同时保持内存效率。我们在模拟和真实世界环境中的物体检索和区域监测等多样化机器人任务中展示了其有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决户外自主机器人在大规模环境中缺乏有效地图表示的问题，特别是缺乏语义理解和地形感知能力。这个问题很重要，因为户外自主机器人可用于搜索救援、森林火灾监测、食品递送等多种社会应用场景，而现有室内3D场景图方法无法有效扩展到户外环境，且缺乏对地形这一关键导航因素的处理。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D场景图方法在户外环境中的局限性，如计算密集、内存需求大、缺乏地形感知等。他们借鉴了室内3DSG技术、LiDAR SLAM方法、视觉语言模型(VLM)和基础模型的思想，但针对户外环境特点进行了创新设计。特别是结合了Hydra的GVD方法用于位置节点构建，借鉴了Clio的任务驱动思想但扩展为任务无关的地图构建，并引入专门的地形识别模型解决VLM在识别地形方面的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D场景图生成分为三个阶段：任务无关的度量-语义映射、任务无关的地形感知3DSG构建、任务驱动的3DSG查询和导航。整体流程是：1)通过LiDAR和相机数据构建稀疏几何地图，提取地形和物体语义特征；2)基于地形信息构建层次化位置和区域节点；3)根据任务需求在3DSG上进行查询和导航，支持对象检索、区域监控和地形感知路径规划。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)任务无关的轻量级度量-语义映射方法；2)专门的地形感知3DSG层；3)层次化区域组织；4)使用稀疏点云而非密集网格；5)开放集语义理解；6)任务无关与任务驱动分离的设计。相比之前工作，不同之处在于：专门处理地形信息而非将其视为普通语义类别；使用稀疏表示减少计算和内存需求；支持开放集语义理解；分离地图构建和任务查询阶段，增强地图通用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Terra提出了一种结合地形感知和层次化组织的轻量级3D场景图方法，实现了大规模户外环境中任务无关的度量-语义映射，支持多样化的自主机器人任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor intelligent autonomous robotic operation relies on a sufficientlyexpressive map of the environment. Classical geometric mapping methods retainessential structural environment information, but lack a semantic understandingand organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)address this limitation by integrating geometric, topological, and semanticrelationships into a multi-level graph-based map. Outdoor autonomous operationscommonly rely on terrain information either due to task-dependence or thetraversability of the robotic platform. We propose a novel approach thatcombines indoor 3DSG techniques with standard outdoor geometric mapping andterrain-aware reasoning, producing terrain-aware place nodes and hierarchicallyorganized regions for outdoor environments. Our method generates atask-agnostic metric-semantic sparse map and constructs a 3DSG from this mapfor downstream planning tasks, all while remaining lightweight for autonomousrobotic operation. Our thorough evaluation demonstrates our 3DSG methodperforms on par with state-of-the-art camera-based 3DSG methods in objectretrieval and surpasses them in region classification while remaining memoryefficient. We demonstrate its effectiveness in diverse robotic tasks of objectretrieval and region monitoring in both simulation and real-world environments.</description>
      <author>example@mail.com (Chad R. Samuelson, Abigail Austin, Seth Knoop, Blake Romrell, Gabriel R. Slade, Timothy W. McLain, Joshua G. Mangelson)</author>
      <guid isPermaLink="false">2509.19579v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning</title>
      <link>http://arxiv.org/abs/2509.20360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EditVerse是一个统一的图像和视频生成与编辑框架，通过将文本、图像和视频表示为统一令牌序列，利用自注意力机制实现跨模态知识转移和灵活处理任意分辨率和持续时间的输入输出。&lt;h4&gt;背景&lt;/h4&gt;基础模型正朝着统一化和规模化方向发展，图像生成和编辑已从特定任务框架转变为统一框架，但视频生成和编辑因架构限制和数据稀缺性仍处于分散状态。&lt;h4&gt;目的&lt;/h4&gt;开发一个单一模型框架，能够同时处理图像和视频的生成与编辑任务，克服当前视频生成领域的碎片化问题。&lt;h4&gt;方法&lt;/h4&gt;将所有模态表示为统一令牌序列，利用自注意力机制实现上下文学习；设计可扩展数据管道收集232K视频编辑样本；结合大规模图像和视频数据集进行联合训练；提出首个基于指令的视频编辑基准测试EditVerseBench。&lt;h4&gt;主要发现&lt;/h4&gt;EditVerse实现了最先进的性能，超越了现有的开源和商业模型；在跨模态方面表现出涌现的编辑和生成能力。&lt;h4&gt;结论&lt;/h4&gt;EditVerse成功统一了图像和视频生成与编辑框架，通过创新的表示方法和数据管道解决了视频编辑领域的挑战，为未来多模态统一模型的发展提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;近期基础模型的进展清晰地表明了向统一化和规模化发展的趋势，展示了跨不同领域的涌现能力。虽然图像生成和编辑已经从特定任务框架迅速转变为统一框架，但由于架构限制和数据稀缺性，视频生成和编辑仍然处于分散状态。在本工作中，我们介绍了EditVerse，一个用于图像和视频生成和编辑的统一框架，在单一模型内完成。通过将所有模态（即文本、图像和视频）表示为统一的令牌序列，EditVerse利用自注意力实现强大的上下文学习、自然的跨模态知识转移，以及对具有任意分辨率和持续时间的输入和输出的灵活处理。为解决视频编辑训练数据的缺乏，我们设计了一个可扩展的数据管道，整理了232K个视频编辑样本，并将它们与大规模图像和视频数据集结合进行联合训练。此外，我们提出了EditVerseBench，这是第一个涵盖多种任务和分辨率的基于指令的视频编辑基准测试。大量实验和用户研究表明，EditVerse实现了最先进的性能，超越了现有的开源和商业模型，同时在跨模态方面表现出涌现的编辑和生成能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in foundation models highlight a clear trend towardunification and scaling, showing emergent capabilities across diverse domains.While image generation and editing have rapidly transitioned from task-specificto unified frameworks, video generation and editing remain fragmented due toarchitectural limitations and data scarcity. In this work, we introduceEditVerse, a unified framework for image and video generation and editingwithin a single model. By representing all modalities, i.e., text, image, andvideo, as a unified token sequence, EditVerse leverages self-attention toachieve robust in-context learning, natural cross-modal knowledge transfer, andflexible handling of inputs and outputs with arbitrary resolutions anddurations. To address the lack of video editing training data, we design ascalable data pipeline that curates 232K video editing samples and combinesthem with large-scale image and video datasets for joint training. Furthermore,we present EditVerseBench, the first benchmark for instruction-based videoediting covering diverse tasks and resolutions. Extensive experiments and userstudies demonstrate that EditVerse achieves state-of-the-art performance,surpassing existing open-source and commercial models, while exhibitingemergent editing and generation abilities across modalities.</description>
      <author>example@mail.com (Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, Qiang Xu)</author>
      <guid isPermaLink="false">2509.20360v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Video models are zero-shot learners and reasoners</title>
      <link>http://arxiv.org/abs/2509.20328v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://video-zero-shot.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型的零样本能力推动了自然语言处理向通用基础模型转变，研究表明视频模型可能沿着类似路径发展成为通用视觉基础模型。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的零样本能力推动了自然语言处理从特定任务模型向统一的通用基础模型转变，这种转变源于大规模生成模型在网页规模数据上的训练。&lt;h4&gt;目的&lt;/h4&gt;探究视频模型是否可能沿着类似LLMs的路径发展，成为通用的视觉理解基础模型。&lt;h4&gt;方法&lt;/h4&gt;研究团队展示了Veo 3模型能够解决多种它没有明确训练过的任务，包括物体分割、边缘检测、图像编辑、理解物理属性、识别物体可供性、模拟工具使用等。&lt;h4&gt;主要发现&lt;/h4&gt;Veo模型能够感知、建模和操作视觉世界，实现了早期的视觉推理能力，如解决迷宫和对称性问题等。Veo的零样本能力表明视频模型正在成为统一的、通用的视觉基础模型。&lt;h4&gt;结论&lt;/h4&gt;视频模型可能正朝着成为通用视觉基础模型的方向发展，类似于LLMs在语言领域的发展路径。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)显著的零样本能力推动了自然语言处理从特定任务模型向统一的、通用的基础模型转变。这种转变源于简单的原则：在网页规模数据上训练的大规模生成模型。有趣的是，同样的原则也适用于当今的生成式视频模型。视频模型是否可能沿着与LLMs发展通用语言理解能力相似的路径，发展成为通用的视觉理解模型？我们证明Veo 3能够解决多种它没有明确训练过的任务：分割物体、检测边缘、编辑图像、理解物理属性、识别物体可供性、模拟工具使用等等。这些感知、建模和操作视觉世界的能力实现了早期的视觉推理，如解决迷宫和对称性问题。Veo的零样本能力表明视频模型正在成为统一的、通用的视觉基础模型的路径上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The remarkable zero-shot capabilities of Large Language Models (LLMs) havepropelled natural language processing from task-specific models to unified,generalist foundation models. This transformation emerged from simpleprimitives: large, generative models trained on web-scale data. Curiously, thesame primitives apply to today's generative video models. Could video models beon a trajectory towards general-purpose vision understanding, much like LLMsdeveloped general-purpose language understanding? We demonstrate that Veo 3 cansolve a broad variety of tasks it wasn't explicitly trained for: segmentingobjects, detecting edges, editing images, understanding physical properties,recognizing object affordances, simulating tool use, and more. These abilitiesto perceive, model, and manipulate the visual world enable early forms ofvisual reasoning like maze and symmetry solving. Veo's emergent zero-shotcapabilities indicate that video models are on a path to becoming unified,generalist vision foundation models.</description>
      <author>example@mail.com (Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos)</author>
      <guid isPermaLink="false">2509.20328v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>A Versatile Foundation Model for AI-enabled Mammogram Interpretation</title>
      <link>http://arxiv.org/abs/2509.20271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  64 pages, 7 figures, 40 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了VersaMammo，一个为乳腺X光片设计的多功能基础模型，通过创建最大的多机构乳腺X光数据集和采用两阶段预训练策略，克服了现有模型的局限性，并在92个临床相关任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;乳腺癌是全球女性最常见的诊断癌症和癌症相关死亡的主要原因。乳腺X光摄影术对于乳腺病变的早期检测和诊断至关重要。&lt;h4&gt;目的&lt;/h4&gt;介绍VersaMammo，一个为乳腺X光片设计的多功能基础模型，旨在克服现有基础模型的局限性，包括训练数据多样性不足、模型泛化能力有限，以及缺乏对临床相关任务的全面评估。&lt;h4&gt;方法&lt;/h4&gt;创建包含706,239张图像来自21个来源的最大多机构乳腺X光数据集；提出两阶段预训练策略：首先通过自监督学习训练教师模型提取可转移特征，然后结合监督学习和知识蒸馏将特征和临床知识转移到VersaMammo；建立包含92个特定任务的基准测试，涵盖病变检测、分割、分类、图像检索和视觉问答五大临床任务类别。&lt;h4&gt;主要发现&lt;/h4&gt;VersaMammo达到了最先进的性能，在68个内部任务中排名前50，在24个外部验证任务中排名前20，平均排名分别为1.5和1.2，证明了其卓越的泛化能力和临床实用性。&lt;h4&gt;结论&lt;/h4&gt;VersaMammo为可靠和可扩展的乳腺癌筛查和诊断提供了重大进展，其性能和泛化能力显著优于现有模型。&lt;h4&gt;翻译&lt;/h4&gt;乳腺癌是全球女性最常见的诊断癌症和癌症相关死亡的主要原因。乳腺X光摄影术对于乳腺病变的早期检测和诊断至关重要。尽管在乳腺X光片分析的基础模型方面取得了进展，但它们的临床转化仍受到几个基本限制的制约，包括训练数据多样性不足、模型泛化能力有限，以及缺乏对临床相关任务的全面评估。在此，我们介绍了VersaMammo，一个为乳腺X光片设计的多功能基础模型，旨在克服这些限制。我们整理了迄今为止最大的多机构乳腺X光数据集，包含来自21个来源的706,239张图像。为了提高泛化能力，我们提出两阶段预训练策略来开发VersaMammo这个乳腺X光基础模型。首先，通过自监督学习训练教师模型，从未标记的乳腺X光片中提取可转移的特征。然后，结合监督学习和知识蒸馏将特征和临床知识转移到VersaMammo中。为确保全面评估，我们建立了一个包含92个特定任务的基准测试，包括68个内部任务和24个外部验证任务，涵盖5个主要临床任务类别：病变检测、分割、分类、图像检索和视觉问答。VersaMammo达到了最先进的性能，在68个特定内部任务中排名前50，在24个外部验证任务中排名前20，平均排名分别为1.5和1.2。这些结果证明了其卓越的泛化能力和临床实用性，为可靠和可扩展的乳腺癌筛查和诊断提供了重大进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast cancer is the most commonly diagnosed cancer and the leading cause ofcancer-related mortality in women globally. Mammography is essential for theearly detection and diagnosis of breast lesions. Despite recent progress infoundation models (FMs) for mammogram analysis, their clinical translationremains constrained by several fundamental limitations, including insufficientdiversity in training data, limited model generalizability, and a lack ofcomprehensive evaluation across clinically relevant tasks. Here, we introduceVersaMammo, a versatile foundation model for mammograms, designed to overcomethese limitations. We curated the largest multi-institutional mammogram datasetto date, comprising 706,239 images from 21 sources. To improve generalization,we propose a two-stage pre-training strategy to develop VersaMammo, a mammogramfoundation model. First, a teacher model is trained via self-supervisedlearning to extract transferable features from unlabeled mammograms. Then,supervised learning combined with knowledge distillation transfers bothfeatures and clinical knowledge into VersaMammo. To ensure a comprehensiveevaluation, we established a benchmark comprising 92 specific tasks, including68 internal tasks and 24 external validation tasks, spanning 5 major clinicaltask categories: lesion detection, segmentation, classification, imageretrieval, and visual question answering. VersaMammo achieves state-of-the-artperformance, ranking first in 50 out of 68 specific internal tasks and 20 outof 24 external validation tasks, with average ranks of 1.5 and 1.2,respectively. These results demonstrate its superior generalization andclinical utility, offering a substantial advancement toward reliable andscalable breast cancer screening and diagnosis.</description>
      <author>example@mail.com (Fuxiang Huang, Jiayi Zhu, Yunfang Yu, Yu Xie, Yuan Guo, Qingcong Kong, Mingxiang Wu, Xinrui Jiang, Shu Yang, Jiabo Ma, Ziyi Liu, Zhe Xu, Zhixuan Chen, Yujie Tan, Zifan He, Luhui Mao, Xi Wang, Junlin Hou, Lei Zhang, Qiong Luo, Zhenhui Li, Herui Yao, Hao Chen)</author>
      <guid isPermaLink="false">2509.20271v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Association Rules in High-Dimensional Small Tabular Data</title>
      <link>http://arxiv.org/abs/2509.20113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper was accepted at ECAI 2025 Workshop: 1st International  Workshop on Advanced Neuro-Symbolic Applications (ANSyA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于表格基础模型的Aerial+微调方法，用于解决高维低数据环境下的关联规则挖掘问题，显著提高了规则发现的质量和效率。&lt;h4&gt;背景&lt;/h4&gt;关联规则挖掘(ARM)旨在发现数据集中特征之间的模式，用于知识发现和可解释机器学习。在高维设置中，规则爆炸和计算开销使得传统算法方法不切实际，神经符号方法如Aerial+虽解决了规则爆炸问题，但在低数据环境下性能有限。&lt;h4&gt;目的&lt;/h4&gt;解决高维数据中的关联规则挖掘问题，特别是在低数据环境下的挑战，提高规则发现的质量和效率。&lt;h4&gt;方法&lt;/h4&gt;提出两种基于表格基础模型的Aerial+微调方法，用于高维低数据环境下的关联规则挖掘。&lt;h4&gt;主要发现&lt;/h4&gt;1) Aerial+在五个真实世界数据集上比最先进的算法和神经符号基线好一到两个数量级；2) 引入了高维低数据设置下的ARM新问题；3) 提出的微调方法在五个真实世界数据集上显著提高了规则质量。&lt;h4&gt;结论&lt;/h4&gt;提出的基于表格基础模型的Aerial+微调方法能有效处理高维低数据环境下的关联规则挖掘问题，显著提高规则质量。&lt;h4&gt;翻译&lt;/h4&gt;关联规则挖掘(ARM)旨在以命题规则的形式发现数据集中的特征模式，支持高风险决策中的知识发现和可解释机器学习。然而，在高维设置中，规则爆炸和计算开销使得没有有效搜索空间缩减的流行算法方法不切实际，这些挑战会传播到下游任务。神经符号方法如Aerial+最近被提出以解决ARM中的规则爆炸问题。虽然它们处理了数据的高维性，但也继承了神经网络的局限性，特别是在低数据环境中的性能降低。本文对高维表格数据中的关联规则发现做出了三个关键贡献：首先，我们在五个真实世界数据集上实证表明Aerial+比最先进的算法和神经符号基线好一到两个数量级。其次，我们引入了高维、低数据设置下的ARM新问题，如生物医学领域中具有约18k特征和50个样本的基因表达数据。第三，我们提出了使用表格基础模型对Aerial+进行微调的两种方法。我们的方法在五个真实世界数据集上被证明显著提高了规则质量，证明了它们在低数据、高维场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Association Rule Mining (ARM) aims to discover patterns between features indatasets in the form of propositional rules, supporting both knowledgediscovery and interpretable machine learning in high-stakes decision-making.However, in high-dimensional settings, rule explosion and computationaloverhead render popular algorithmic approaches impractical without effectivesearch space reduction, challenges that propagate to downstream tasks.Neurosymbolic methods, such as Aerial+, have recently been proposed to addressthe rule explosion in ARM. While they tackle the high dimensionality of thedata, they also inherit limitations of neural networks, particularly reducedperformance in low-data regimes.  This paper makes three key contributions to association rule discovery inhigh-dimensional tabular data. First, we empirically show that Aerial+ scalesone to two orders of magnitude better than state-of-the-art algorithmic andneurosymbolic baselines across five real-world datasets. Second, we introducethe novel problem of ARM in high-dimensional, low-data settings, such as geneexpression data from the biomedicine domain with around 18k features and 50samples. Third, we propose two fine-tuning approaches to Aerial+ using tabularfoundation models. Our proposed approaches are shown to significantly improverule quality on five real-world datasets, demonstrating their effectiveness inlow-data, high-dimensional scenarios.</description>
      <author>example@mail.com (Erkan Karabulut, Daniel Daza, Paul Groth, Victoria Degeler)</author>
      <guid isPermaLink="false">2509.20113v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2509.20107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型高光谱适配器，利用预训练视觉基础模型有效学习高光谱数据，在自动驾驶场景中实现了最先进的语义分割性能。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像(HSI)捕获空间信息和密集的光谱测量，跨越许多窄波长波段，这种丰富的光谱内容有可能促进机器人的鲁棒感知，特别是在具有复杂成分、变化光照或其他视觉挑战性条件的环境中。&lt;h4&gt;目的&lt;/h4&gt;解决当前高光谱语义分割方法表现不佳的问题，因为这些方法依赖于针对RGB输入优化的架构和学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出一种包含光谱转换器和光谱感知空间先验模块的架构，用于提取丰富的空间-光谱特征；并引入模态感知交互块，通过专门的提取和注入机制促进高光谱表示和冻结视觉Transformer特征的有效集成。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准自动驾驶数据集上的广泛评估表明，该架构直接使用HSI输入实现了最先进的语义分割性能，优于基于视觉和高光谱的分割方法。&lt;h4&gt;结论&lt;/h4&gt;新型高光谱适配器能够有效利用高光谱数据，提升在复杂环境中的语义分割性能，为机器人感知提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)捕获空间信息和密集的光谱测量，跨越众多窄波长波段。这种丰富的光谱内容有可能促进强大的机器人感知，特别是在具有复杂成分、变化光照或其他视觉挑战性条件的环境中。然而，当前的高光谱语义分割方法表现不佳，因为它们依赖于针对RGB输入优化的架构和学习框架。在这项工作中，我们提出了一种新型高光谱适配器，利用预训练的视觉基础模型从高光谱数据中有效学习。我们的架构包含一个光谱转换器和一个光谱感知的空间先验模块，用于提取丰富的空间-光谱特征。此外，我们引入了一种模态感知交互块，通过专门的提取和注入机制促进高光谱表示和冻结视觉Transformer特征的有效集成。在三个基准自动驾驶数据集上的广泛评估表明，我们的架构直接使用HSI输入实现了最先进的语义分割性能，优于基于视觉和高光谱的分割方法。我们在 https://hyperspectraladapter.cs.uni-freiburg.de 提供代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) captures spatial information along with densespectral measurements across numerous narrow wavelength bands. This richspectral content has the potential to facilitate robust robotic perception,particularly in environments with complex material compositions, varyingillumination, or other visually challenging conditions. However, current HSIsemantic segmentation methods underperform due to their reliance onarchitectures and learning frameworks optimized for RGB inputs. In this work,we propose a novel hyperspectral adapter that leverages pretrained visionfoundation models to effectively learn from hyperspectral data. Ourarchitecture incorporates a spectral transformer and a spectrum-aware spatialprior module to extract rich spatial-spectral features. Additionally, weintroduce a modality-aware interaction block that facilitates effectiveintegration of hyperspectral representations and frozen vision Transformerfeatures through dedicated extraction and injection mechanisms. Extensiveevaluations on three benchmark autonomous driving datasets demonstrate that ourarchitecture achieves state-of-the-art semantic segmentation performance whiledirectly using HSI inputs, outperforming both vision-based and hyperspectralsegmentation methods. We make the code available athttps://hyperspectraladapter.cs.uni-freiburg.de.</description>
      <author>example@mail.com (JuanaJuana Valeria Hurtado, Rohit Mohan, Abhinav Valada)</author>
      <guid isPermaLink="false">2509.20107v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>One Filters All: A Generalist Filter for State Estimation</title>
      <link>http://arxiv.org/abs/2509.20051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为LLM-Filter的通用滤波框架，利用大型语言模型进行动态系统中的状态估计，通过将带有噪声的观测值嵌入到文本原型中实现。&lt;h4&gt;背景&lt;/h4&gt;估计动态系统中的隐藏状态（最优滤波）是科学和工程领域一个长期存在的问题。&lt;h4&gt;目的&lt;/h4&gt;引入一个利用大型语言模型进行状态估计的通用滤波框架LLM-Filter。&lt;h4&gt;方法&lt;/h4&gt;LLM-Filter通过将带有噪声的观测值嵌入到文本原型中，利用预训练大型语言模型中的推理知识进行状态估计，并设计了System-as-Prompt (SaP)提示结构。&lt;h4&gt;主要发现&lt;/h4&gt;1) 状态估计可以从预训练LLMs中的推理知识显著受益；2) 通过与冻结的LLM进行模态对齐，LLM-Filter优于最先进的学习方法；3) 设计的SaP提示结构使LLM能够理解估计任务；4) LLM-Filter表现出卓越的泛化能力，能在变化甚至未见环境中准确执行滤波任务；5) 模型准确度随模型大小和训练时间增加而提高。&lt;h4&gt;结论&lt;/h4&gt;这些发现使LLM-Filter成为滤波领域的一个有前景的基础模型。&lt;h4&gt;翻译&lt;/h4&gt;估计动态系统中的隐藏状态，也称为最优滤波，是科学和工程领域一个长期存在的问题。在本文中，我们引入了一个通用滤波框架LLM-Filter，它通过将带有噪声的观测值嵌入到文本原型中，利用大型语言模型进行状态估计。在各种经典动态系统的实验中，我们发现首先，状态估计可以从预训练LLMs中嵌入的推理知识中显著受益。通过与冻结的LLM进行适当的模态对齐，LLM-Filter优于最先进的学习方法。其次，我们仔细设计了提示结构System-as-Prompt (SaP)，包含使LLM能够理解估计任务的任务指令。在这些提示的指导下，LLM-Filter表现出卓越的泛化能力，能够在变化甚至未见过的环境中准确执行滤波任务。我们还观察到LLM-Filter中存在扩展定律行为，其中准确度随模型大小和训练时间的增加而提高。这些发现使LLM-Filter成为滤波领域的一个有前景的基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating hidden states in dynamical systems, also known as optimalfiltering, is a long-standing problem in various fields of science andengineering. In this paper, we introduce a general filtering framework,\textbf{LLM-Filter}, which leverages large language models (LLMs) for stateestimation by embedding noisy observations with text prototypes. In variousexperiments for classical dynamical systems, we find that first, stateestimation can significantly benefit from the reasoning knowledge embedded inpre-trained LLMs. By achieving proper modality alignment with the frozen LLM,LLM-Filter outperforms the state-of-the-art learning-based approaches. Second,we carefully design the prompt structure, System-as-Prompt (SaP), incorporatingtask instructions that enable the LLM to understand the estimation tasks.Guided by these prompts, LLM-Filter exhibits exceptional generalization,capable of performing filtering tasks accurately in changed or even unseenenvironments. We further observe a scaling-law behavior in LLM-Filter, whereaccuracy improves with larger model sizes and longer training times. Thesefindings make LLM-Filter a promising foundation model of filtering.</description>
      <author>example@mail.com (Shiqi Liu, Wenhan Cao, Chang Liu, Zeyu He, Tianyi Zhang, Shengbo Eben Li)</author>
      <guid isPermaLink="false">2509.20051v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture</title>
      <link>http://arxiv.org/abs/2509.19997v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper accepted at MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用基础模型的信息嵌入进行医学影像的无监督异常检测，提出使用狄利克雷过程混合模型(DPMM)替代传统记忆库方法，显著提高了大型医疗数据集的异常检测效率。&lt;h4&gt;背景&lt;/h4&gt;对于小型数据集，可以使用正常特征的记忆库进行异常检测，但这种方法对于大型医疗数据集计算负担过重，不适用。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于大型医疗数据集的高效异常检测方法，减少计算负担同时保持检测性能。&lt;h4&gt;方法&lt;/h4&gt;使用狄利克雷过程混合模型(DPMM)建模正常DINOv2嵌入的分布，利用成分中心和嵌入之间的相似性作为异常分数函数，创建粗略的异常分割掩码。&lt;h4&gt;主要发现&lt;/h4&gt;DINOv2的DPMM嵌入在医学影像基准测试中实现了有竞争力的异常检测性能，同时至少将推理时间减少了一半；归一化的DINOv2嵌入比未归一化的特征更符合解剖结构，即使在存在异常的情况下也是如此。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了大型医疗数据集异常检测的计算效率问题，代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们利用基础模型的信息嵌入进行医学影像的无监督异常检测。对于小型数据集，最近的研究表明可以使用正常特征的记忆库直接进行异常检测。然而，对于大型医疗数据集，这种方法不适用，因为计算负担显著增加。因此，我们提出使用狄利克雷过程混合模型(DPMM)来建模正常DINOv2嵌入的分布，这是一种非参数混合模型，能根据数据自动调整混合成分的数量。我们不使用记忆库，而是利用成分中心和嵌入之间的相似性作为异常分数函数来创建粗略的异常分割掩码。我们的实验表明，通过DPMM处理的DINOv2嵌入尽管是在自然图像上训练的，但在医学影像基准测试中实现了非常有竞争力的异常检测性能，同时至少将推理时间减少了一半。我们的进一步分析表明，即使在存在异常的情况下，归一化的DINOv2嵌入通常比未归一化的特征更符合解剖结构，使它们成为异常检测的优秀表示。代码可在https://github.com/NicoSchulthess/anomalydino-dpmm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we leverage informative embeddings from foundational models forunsupervised anomaly detection in medical imaging. For small datasets, amemory-bank of normative features can directly be used for anomaly detectionwhich has been demonstrated recently. However, this is unsuitable for largemedical datasets as the computational burden increases substantially.Therefore, we propose to model the distribution of normative DINOv2 embeddingswith a Dirichlet Process Mixture model (DPMM), a non-parametric mixture modelthat automatically adjusts the number of mixture components to the data athand. Rather than using a memory bank, we use the similarity between thecomponent centers and the embeddings as anomaly score function to create acoarse anomaly segmentation mask. Our experiments show that through DPMMembeddings of DINOv2, despite being trained on natural images, achieve verycompetitive anomaly detection performance on medical imaging benchmarks and cando this while at least halving the computation time at inference. Our analysisfurther indicates that normalized DINOv2 embeddings are generally more alignedwith anatomical structures than unnormalized features, even in the presence ofanomalies, making them great representations for anomaly detection. The code isavailable at https://github.com/NicoSchulthess/anomalydino-dpmm.</description>
      <author>example@mail.com (Nico Schulthess, Ender Konukoglu)</author>
      <guid isPermaLink="false">2509.19997v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later</title>
      <link>http://arxiv.org/abs/2509.19929v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了GABI（用于贝叶斯反演的几何自编码器）框架，用于处理具有复杂几何形状的工程系统中的不确定性量化问题。该方法采用'先学习后观察'的范式，从具有不同几何形状的大数据集中提取信息，生成几何感知的生成模型作为贝叶斯反演的先验信息。在多种工程应用中表现出色，预测精度与确定性监督学习方法相当，且在复杂几何形状的挑战性问题上具有良好校准和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;不确定性量化（UQ）对于工程应用中的推断至关重要。常见的推断任务是从少量噪声观测中恢复物理系统的全场信息，这通常是一个高度不适定的问题。关键在于，工程系统通常具有复杂且可变的几何形状，这限制了标准贝叶斯UQ方法的使用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理复杂几何形状的工程系统不确定性量化问题的框架，特别适用于从少量噪声观测中恢复物理系统全场信息的任务。&lt;h4&gt;方法&lt;/h4&gt;提出GABI框架，学习几何感知的物理响应生成模型，作为贝叶斯反演的几何条件先验。采用'先学习后观察'的范式，从具有不同几何形状的大数据集中提取信息，不需要了解控制PDE、边界条件或观测过程。该框架与架构无关，创造性地使用近似贝叶斯计算（ABC）采样，实现了利用现代GPU硬件的高效实现。&lt;h4&gt;主要发现&lt;/h4&gt;在监督学习方法可适用的受限情况下，预测精度与确定性监督学习方法相当；在具有复杂几何形状的挑战性问题上，不确定性量化具有良好的校准性和鲁棒性；该方法提供了一个灵活的几何感知'一次训练随处可用'的基础模型，独立于任何特定的观测过程。&lt;h4&gt;结论&lt;/h4&gt;GABI方法为具有复杂几何形状的工程系统提供了一种有效的不确定性量化解决方案，能够从少量噪声观测中恢复物理系统的全场信息，同时提供可靠的不确定性估计。&lt;h4&gt;翻译&lt;/h4&gt;不确定性量化（UQ）对于工程应用中的推断至关重要。常见的推断任务是从少量噪声观测中恢复物理系统的全场信息，这通常是一个高度不适定的问题。关键在于，工程系统通常具有复杂且可变的几何形状，这限制了标准贝叶斯UQ方法的使用。在这项工作中，我们引入了用于贝叶斯反演的几何自编码器（GABI），这是一个学习物理响应的几何感知生成模型的框架，作为贝叶斯反演的高度信息丰富的几何条件先验。遵循'先学习后观察'的范式，GABI将从具有不同几何形状的大数据集中提取的信息，不需要了解控制PDE、边界条件或观测过程，转化为丰富的潜在先验。在推断时，该先验与特定观测过程的可能性无缝结合，产生一个适应几何形状的后验分布。我们提出的框架与架构无关。创造性地使用近似贝叶斯计算（ABC）采样产生了一种高效的实现，利用现代GPU硬件。我们在以下测试了我们的方法：矩形域上的稳态热传导；翼型周围的雷诺平均纳维-斯托克斯（RANS）流动；3D车身上的亥姆霍兹共振和源定位；地形上的RANS气流。我们发现：在监督学习方法可适用的受限情况下，预测精度与确定性监督学习方法相当；在具有复杂几何形状的挑战性问题上，不确定性量化具有良好的校准性和鲁棒性。该方法提供了一个灵活的几何感知'一次训练随处可用'的基础模型，独立于任何特定的观测过程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Uncertainty Quantification (UQ) is paramount for inference in engineeringapplications. A common inference task is to recover full-field information ofphysical systems from a small number of noisy observations, a usually highlyill-posed problem. Critically, engineering systems often have complicated andvariable geometries prohibiting the use of standard Bayesian UQ. In this work,we introduce Geometric Autoencoders for Bayesian Inversion (GABI), a frameworkfor learning geometry-aware generative models of physical responses that serveas highly informative geometry-conditioned priors for Bayesian inversion.Following a ''learn first, observe later'' paradigm, GABI distills informationfrom large datasets of systems with varying geometries, without requiringknowledge of governing PDEs, boundary conditions, or observation processes,into a rich latent prior. At inference time, this prior is seamlessly combinedwith the likelihood of the specific observation process, yielding ageometry-adapted posterior distribution. Our proposed framework is architectureagnostic. A creative use of Approximate Bayesian Computation (ABC) samplingyields an efficient implementation that utilizes modern GPU hardware. We testour method on: steady-state heat over rectangular domains; Reynold-AveragedNavier-Stokes (RANS) flow around airfoils; Helmholtz resonance and sourcelocalization on 3D car bodies; RANS airflow over terrain. We find: thepredictive accuracy to be comparable to deterministic supervised learningapproaches in the restricted setting where supervised learning is applicable;UQ to be well calibrated and robust on challenging problems with complexgeometries. The method provides a flexible geometry-awaretrain-once-use-anywhere foundation model which is independent of any particularobservation process.</description>
      <author>example@mail.com (Arnaud Vadeboncoeur, Gregory Duthé, Mark Girolami, Eleni Chatzi)</author>
      <guid isPermaLink="false">2509.19929v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches</title>
      <link>http://arxiv.org/abs/2509.19924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 7 figures. Accepted for presentation at the 39th Conference  on Neural Information Processing Systems (NeurIPS 2025) Workshop on the  Foundations of Reasoning in Language Models (FoRLM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究基础模型在强化学习探索任务中的能力，特别是在稀疏奖励环境中的零样本探索，发现视觉语言模型能理解高级目标但在低级控制上存在困难，提出混合框架可提高早期学习效率。&lt;h4&gt;背景&lt;/h4&gt;强化学习中的探索具有挑战性，特别是在稀疏奖励设置中。基础模型拥有强大的语义先验，但它们作为经典强化学习基准中零样本探索代理的能力尚未被充分理解。&lt;h4&gt;目的&lt;/h4&gt;测试大型语言模型和视觉语言模型在多臂老虎机、网格世界和稀疏奖励Atari游戏中的零样本探索能力，并分析基础模型引导探索而非端到端控制的潜力和限制。&lt;h4&gt;方法&lt;/h4&gt;在多臂老虎机、网格世界和稀疏奖励Atari上对大型语言模型和视觉语言模型进行基准测试，并在受控的最佳情况下研究一个简单的在线策略混合框架。&lt;h4&gt;主要发现&lt;/h4&gt;视觉语言模型可以从视觉输入中推断高级目标，但在精确的低级控制上持续失败，存在'知行差距'。在理想化设置中，视觉语言模型指导可以显著提高早期阶段的样本效率。&lt;h4&gt;结论&lt;/h4&gt;基础模型更适合引导探索而非端到端控制，视觉语言模型指导在理想化环境中能有效提高早期学习效率。&lt;h4&gt;翻译&lt;/h4&gt;强化学习中的探索仍然具有挑战性，特别是在稀疏奖励设置中。虽然基础模型拥有强大的语义先验，但它们作为经典强化学习基准中零样本探索代理的能力尚未被充分理解。我们在多臂老虎机、网格世界和稀疏奖励Atari上对大型语言模型和视觉语言模型进行基准测试，以测试零样本探索。我们的研究揭示了一个关键局限：虽然视觉语言模型可以从视觉输入中推断高级目标，但它们在精确的低级控制上持续失败：即'知行差距'。为了分析这一差距的潜在桥梁，我们在受控的最佳情况下研究了一个简单的在线策略混合框架。我们在这种理想化设置中的结果表明，视觉语言模型指导可以显著提高早期阶段的样本效率，这为使用基础模型引导探索而非端到端控制的潜力和限制提供了清晰的分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploration in reinforcement learning (RL) remains challenging, particularlyin sparse-reward settings. While foundation models possess strong semanticpriors, their capabilities as zero-shot exploration agents in classic RLbenchmarks are not well understood. We benchmark LLMs and VLMs on multi-armedbandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Ourinvestigation reveals a key limitation: while VLMs can infer high-levelobjectives from visual input, they consistently fail at precise low-levelcontrol: the "knowing-doing gap". To analyze a potential bridge for this gap,we investigate a simple on-policy hybrid framework in a controlled, best-casescenario. Our results in this idealized setting show that VLM guidance cansignificantly improve early-stage sample efficiency, providing a clear analysisof the potential and constraints of using foundation models to guideexploration rather than for end-to-end control.</description>
      <author>example@mail.com (Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber)</author>
      <guid isPermaLink="false">2509.19924v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Towards Self-Supervised Foundation Models for Critical Care Time Series</title>
      <link>http://arxiv.org/abs/2509.19885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 workshop Learning from Time Series for  Health (TS4H)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于双轴变换器(BAT)的重症监护时间序列早期预训练基础模型，展示了其在小数据集上优于监督基线的迁移学习能力，为资源有限环境下的临床应用提供了可能。&lt;h4&gt;背景&lt;/h4&gt;医疗领域的基础模型近年来发展迅速，但重症监护时间序列的基础模型由于数据集规模有限且可用性低，研究相对较少。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于双轴变换器(BAT)的早期预训练基础模型，用于处理重症监护时间序列数据。&lt;h4&gt;方法&lt;/h4&gt;使用汇集的电子健康记录数据集对模型进行预训练，然后在不同于训练源的数据集上进行微调，用于死亡率预测任务。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在小数据集（少于5,000个样本）的情况下表现出色，通过迁移学习超越了传统的监督学习方法。&lt;h4&gt;结论&lt;/h4&gt;自监督基础模型在重症监护时间序列领域具有潜力，能够支持资源有限环境下的可推广和稳健的临床应用。&lt;h4&gt;翻译&lt;/h4&gt;近年来，医疗领域的基础模型发展迅速，但重症监护时间序列的基础模型由于数据集规模有限且可用性低，研究相对不足。在本工作中，我们介绍了一种基于双轴变换器(BAT)的重症监护时间序列早期预训练基础模型，该模型在汇集的电子健康记录数据集上进行训练。我们在不同于训练源的数据集上对模型进行微调以进行死亡率预测，在小数据集（少于5,000个样本）的情况下，它优于监督基线。这些贡献强调了自监督基础模型在支持资源有限环境下的可推广和稳健的临床应用方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain-specific foundation models for healthcare have expanded rapidly inrecent years, yet foundation models for critical care time series remainrelatively underexplored due to the limited size and availability of datasets.In this work, we introduce an early-stage pre-trained foundation model forcritical care time-series based on the Bi-Axial Transformer (BAT), trained onpooled electronic health record datasets. We demonstrate effective transferlearning by fine-tuning the model on a dataset distinct from the trainingsources for mortality prediction, where it outperforms supervised baselines,particularly for small datasets ($&lt;5,000$). These contributions highlight thepotential of self-supervised foundation models for critical care times seriesto support generalizable and robust clinical applications in resource-limitedsettings.</description>
      <author>example@mail.com (Katja Naasunnguaq Jagd, Rachael DeVries, Ole Winther)</author>
      <guid isPermaLink="false">2509.19885v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery</title>
      <link>http://arxiv.org/abs/2509.19586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍FragAtlas-62M，一个在最大片段数据集上训练的专业基础模型，覆盖了广泛的片段化学空间&lt;h4&gt;背景&lt;/h4&gt;化学片段在药物发现中很重要，但缺乏大规模、高质量的片段数据集和专门的基础模型&lt;h4&gt;目的&lt;/h4&gt;开发一个能生成大量化学有效片段的基础模型，加速药物发现过程&lt;h4&gt;方法&lt;/h4&gt;构建基于GPT-2的模型（42.7M参数），使用包含6200多万个分子的ZINC-22片段子集进行训练，并通过12个描述符和三种指纹方法进行验证&lt;h4&gt;主要发现&lt;/h4&gt;模型生成99.90%化学有效的片段；生成的片段与训练分布高度匹配（效应大小&lt;0.4）；保留了53.6%的已知ZINC片段，同时产生22%具有实际相关性的新结构&lt;h4&gt;结论&lt;/h4&gt;FragAtlas-62M能有效生成多样化的化学片段，模型及相关资源（训练代码、数据、文档和权重）的发布将加速该领域的采用&lt;h4&gt;翻译&lt;/h4&gt;我们引入了FragAtlas-62M，这是一个专业的基础模型，迄今为止在最大的片段数据集上进行训练。它建立在完整的ZINC-22片段子集上，包含超过6200万个分子，实现了对片段化学空间的前所未有的覆盖。我们基于GPT-2的模型（42.7M参数）生成了99.90%化学有效的片段。跨越12个描述符和三种指纹方法的验证显示，生成的片段与训练分布高度匹配（所有效应大小&lt;0.4）。该模型保留了53.6%的已知ZINC片段，同时产生了22%具有实际相关性的新结构。我们发布了FragAtlas-62M及其训练代码、预处理数据、文档和模型权重，以加速其采用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce FragAtlas-62M, a specialized foundation model trained on thelargest fragment dataset to date. Built on the complete ZINC-22 fragment subsetcomprising over 62 million molecules, it achieves unprecedented coverage offragment chemical space. Our GPT-2 based model (42.7M parameters) generates99.90% chemically valid fragments. Validation across 12 descriptors and threefingerprint methods shows generated fragments closely match the trainingdistribution (all effect sizes &lt; 0.4). The model retains 53.6% of known ZINCfragments while producing 22% novel structures with practical relevance. Werelease FragAtlas-62M with training code, preprocessed data, documentation, andmodel weights to accelerate adoption.</description>
      <author>example@mail.com (Alexander Ho, Sukyeong Lee, Francis T. F. Tsai)</author>
      <guid isPermaLink="false">2509.19586v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Mouse-Guided Gaze: Semi-Supervised Learning of Intention-Aware Representations for Reading Detection</title>
      <link>http://arxiv.org/abs/2509.19574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 Foundation Models for the Brain and Body  NeurIPS 2025 Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个半监督框架，用于学习放大阅读过程中具有意图意识的视线表示，通过利用鼠标轨迹作为弱监督来解决放大导致的视觉上下文丢失和视线碎片化问题。&lt;h4&gt;背景&lt;/h4&gt;在放大阅读过程中，用户意图的理解对无障碍界面设计至关重要。然而，放大缩小了视觉上下文，并迫使持续拖动视口，产生碎片化、嘈杂的视线，掩盖了阅读意图。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够准确识别用户在放大阅读过程中的意图（阅读与浏览）的系统，以改善无障碍界面设计。&lt;h4&gt;方法&lt;/h4&gt;提出一个半监督框架，首先使用未标记的视线进行预训练以预测鼠标速度，然后进行微调以区分阅读与浏览。同时，联合建模放大视口内的原始视线和重新映射到原始屏幕的补偿视图，以解决放大引起的失真问题。&lt;h4&gt;主要发现&lt;/h4&gt;在文本和网页数据集上，该方法持续优于监督基线，半监督预训练在具有挑战性的设置中实现了高达7.5%的F1值提升。&lt;h4&gt;结论&lt;/h4&gt;行为驱动预训练对于稳健的纯视线交互具有重要价值，为开发自适应的无手动辅助工具铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;理解放大阅读过程中的用户意图对无障碍界面设计至关重要。然而，放大缩小了视觉上下文，并迫使持续拖动视口，产生碎片化、嘈杂的视线，掩盖了阅读意图。我们提出了一个半监督框架，通过利用鼠标轨迹作为弱监督来学习具有意图意识的视线表示。该模型首先使用未标记的视线进行预训练以预测鼠标速度，然后进行微调以区分阅读与浏览。为了解决放大引起的失真，我们联合建模放大视口内的原始视线和重新映射到原始屏幕的补偿视图，从而恢复了行和段落之间的空间连续性。在文本和网页数据集上，我们的方法持续优于监督基线，半监督预训练在具有挑战性的设置中实现了高达7.5%的F1值提升。这些研究结果强调了行为驱动预训练对于稳健的纯视线交互的价值，为自适应的无手动辅助工具铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding user intent during magnified reading is critical for accessibleinterface design. Yet magnification collapses visual context and forcescontinual viewport dragging, producing fragmented, noisy gaze and obscuringreading intent. We present a semi-supervised framework that learnsintention-aware gaze representations by leveraging mouse trajectories as weaksupervision. The model is first pretrained to predict mouse velocity fromunlabeled gaze, then fine-tuned to classify reading versus scanning. To addressmagnification-induced distortions, we jointly model raw gaze within themagnified viewport and a compensated view remapped to the original screen,which restores spatial continuity across lines and paragraphs. Across text andwebpage datasets, our approach consistently outperforms supervised baselines,with semi-supervised pretraining yielding up to 7.5% F1 improvement inchallenging settings. These findings highlight the value of behavior-drivenpretraining for robust, gaze-only interaction, paving the way for adaptive,hands-free accessibility tools.</description>
      <author>example@mail.com (Seongsil Heo, Roberto Manduchi)</author>
      <guid isPermaLink="false">2509.19574v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation</title>
      <link>http://arxiv.org/abs/2509.19480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为OmniVLA的机器人基础模型训练框架，实现了基于视觉导航的全模态目标条件，能够处理2D姿态、自我中心图像和自然语言等多种目标模态及其组合。&lt;h4&gt;背景&lt;/h4&gt;人类能够灵活解释和组合不同目标规范（如语言指令、空间坐标或视觉参考）进行导航，而现有机器人导航策略多基于单一模态训练，限制了在现实世界场景中的适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一种训练框架，使机器人基础模型能够支持基于视觉导航的全模态目标条件，提高机器人在复杂环境中的导航能力。&lt;h4&gt;方法&lt;/h4&gt;利用高容量的视觉-语言-动作（VLA）主干网络，通过随机模态融合策略训练三种主要目标模态：2D姿态、自我中心图像和自然语言，以及它们的组合。&lt;h4&gt;主要发现&lt;/h4&gt;OmniVLA模型在未见环境中表现出强大泛化能力，对稀缺模态具有鲁棒性，能遵循新自然语言指令，在各种模态上优于专业基线模型，并为调整新模态和新任务提供了灵活基础。&lt;h4&gt;结论&lt;/h4&gt;OmniVLA为广泛可泛化和灵活的导航策略提供了发展路径，并为构建全模态机器人基础模型展示了可扩展的方法。&lt;h4&gt;翻译&lt;/h4&gt;人类在导航到目的地时能够灵活地解释和组合不同的目标规范，如语言指令、空间坐标或视觉参考。相比之下，大多数现有的机器人导航策略只针对单一模态进行训练，限制了它们在现实世界场景中的适应性，因为在这些场景中不同形式的目标规范是自然且互补的。在这项工作中，我们提出了一个机器人基础模型的训练框架，使基于视觉的导航能够实现全模态目标条件。我们的方法利用高容量的视觉-语言-动作（VLA）主干网络，并通过随机模态融合策略训练三种主要目标模态：2D姿态、自我中心图像和自然语言，以及它们的组合。这种设计不仅扩大了可用数据集的范围，还鼓励策略发展更丰富的几何、语义和视觉表示。由此产生的OmniVLA模型在未见过的环境中表现出强大的泛化能力，对稀缺模态具有鲁棒性，并且能够遵循新的自然语言指令。我们证明，OmniVVA在各种模态上都优于专业基线模型，并为调整新模态和新任务提供了灵活的基础。我们相信OmniVVA朝着广泛可泛化和灵活的导航策略迈出了一步，并为构建全模态机器人基础模型提供了一条可扩展的路径。我们展示了展示OmniVVA性能的视频，并将在项目页面上发布其检查点和训练代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can flexibly interpret and compose different goal specifications, suchas language instructions, spatial coordinates, or visual references, whennavigating to a destination. In contrast, most existing robotic navigationpolicies are trained on a single modality, limiting their adaptability toreal-world scenarios where different forms of goal specification are naturaland complementary. In this work, we present a training framework for roboticfoundation models that enables omni-modal goal conditioning for vision-basednavigation. Our approach leverages a high-capacity vision-language-action (VLA)backbone and trains with three primary goal modalities: 2D poses, egocentricimages, and natural language, as well as their combinations, through arandomized modality fusion strategy. This design not only expands the pool ofusable datasets but also encourages the policy to develop richer geometric,semantic, and visual representations. The resulting model, OmniVLA, achievesstrong generalization to unseen environments, robustness to scarce modalities,and the ability to follow novel natural language instructions. We demonstratethat OmniVLA outperforms specialist baselines across modalities and offers aflexible foundation for fine-tuning to new modalities and tasks. We believeOmniVLA provides a step toward broadly generalizable and flexible navigationpolicies, and a scalable path for building omni-modal robotic foundationmodels. We present videos showcasing OmniVLA performance and will release itscheckpoints and training code on our project page.</description>
      <author>example@mail.com (Noriaki Hirose, Catherine Glossop, Dhruv Shah, Sergey Levine)</author>
      <guid isPermaLink="false">2509.19480v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models</title>
      <link>http://arxiv.org/abs/2509.19465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Thirty-Ninth Annual Conference on Neural Information Processing  Systems {NeurIPS 2025}. Recent Advances in Time Series Foundation Models Have  We Reached the 'BERT Moment'?&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了跨频域迁移学习(CFTL)在基础预测模型(FFMs)预训练中的性能评估问题，发现当前评估方法存在严重缺陷，并提出了改进方案。研究证实统计模型及其集合在多个指标上显著优于现有FFMs，同时发现合成数据预训练对FFM性能有积极影响。&lt;h4&gt;背景&lt;/h4&gt;跨频域迁移学习(CFTL)已成为构建大规模时间序列数据集以预训练基础预测模型(FFMs)的流行框架。然而，当前的基准测试实践无法准确评估CFTL的性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有CFTL评估实践的局限性，提出更准确的性能评估方法。&lt;h4&gt;方法&lt;/h4&gt;引入广泛采用的神经预测网络的统一重新实现，使其适应CFTL设置；仅在专有和合成数据上进行预训练，防止测试数据泄露；在15个大型、多样化的公共预测竞赛数据集上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;统计模型的准确性经常被低估；统计模型及其集合在所有数据集上的表现明显优于现有的FFMs，sCRPS指标提高超过8.2%，MASE指标提高超过20%；合成数据集预训练确实将FFM的准确性提高了7%。&lt;h4&gt;结论&lt;/h4&gt;统计模型在时间序列预测任务中表现优于基础预测模型；合成数据预训练对FFM性能有积极影响，但效果不如统计模型显著。&lt;h4&gt;翻译&lt;/h4&gt;跨频域迁移学习(CFTL)已成为一种流行的框架，用于构建大规模时间序列数据集以预训练基础预测模型(FFMs)。尽管CFTL显示出前景，但当前的基准测试实践无法准确评估其性能。这一不足源于多个因素：过度依赖小规模评估数据集；计算汇总统计量时对样本量处理不当；报告了次优的统计模型；以及未能充分考虑预训练和测试数据集之间重叠的非 negligible 风险。为解决这些局限性，我们引入了广泛采用的神经预测网络的统一重新实现，使其适应CFTL设置；我们仅在专有和合成数据上进行预训练，小心防止测试数据泄露；我们在15个大型、多样化的公共预测竞赛数据集上进行了评估。我们的经验分析表明，统计模型的准确性经常被低估。值得注意的是，我们确认统计模型及其集合在所有数据集上的表现一致优于现有FFMs，sCRPS指标提高超过8.2%，MASE指标提高超过20%。然而，我们也发现合成数据集预训练确实将FFM的准确性提高了7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-frequency transfer learning (CFTL) has emerged as a popular frameworkfor curating large-scale time series datasets to pre-train foundationforecasting models (FFMs). Although CFTL has shown promise, currentbenchmarking practices fall short of accurately assessing its performance. Thisshortcoming stems from many factors: an over-reliance on small-scale evaluationdatasets; inadequate treatment of sample size when computing summarystatistics; reporting of suboptimal statistical models; and failing to accountfor non-negligible risks of overlap between pre-training and test datasets. Toaddress these limitations, we introduce a unified reimplementation ofwidely-adopted neural forecasting networks, adapting them for the CFTL setup;we pre-train only on proprietary and synthetic data, being careful to preventtest leakage; and we evaluate on 15 large, diverse public forecast competitiondatasets. Our empirical analysis reveals that statistical models' accuracy isfrequently underreported. Notably, we confirm that statistical models and theirensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, andby more than 20% MASE, across datasets. However, we also find that syntheticdataset pre-training does improve the accuracy of a FFM by 7% percent.</description>
      <author>example@mail.com (Kin G. Olivares, Malcolm Wolff, Tatiana Konstantinova, Shankar Ramasubramanian, Andrew Gordon Wilson, Andres Potapczynski, Willa Potosnak, Mengfei Cao, Boris Oreshkin, Dmitry Efimov)</author>
      <guid isPermaLink="false">2509.19465v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>The Platonic Universe: Do Foundation Models See the Same Sky?</title>
      <link>http://arxiv.org/abs/2509.19453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 tables, 1 figure. Accepted as a workshop paper to Machine  Learning and the Physical Sciences at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过测试柏拉图表示假说(PRH)，探索了不同数据类型训练的基础模型在天文学中的表示收敛性，发现模型容量增加时表示对齐程度提高，支持共享星系天体物理学表示的收敛。&lt;h4&gt;背景&lt;/h4&gt;柏拉图表示假说(PRH)在天文学中的应用，探索基础模型是否能收敛到共享的天体物理学表示。&lt;h4&gt;目的&lt;/h4&gt;测量在不同数据类型上训练的基础模型的表示收敛性，测试柏拉图表示假说在天文学中的适用性。&lt;h4&gt;方法&lt;/h4&gt;使用JWST、HSC、Legacy Survey和DESI的光谱和成像观测数据，比较视觉变换器、自监督模型和天文学特定架构的表示，采用互k近邻分析方法进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;表示对齐程度通常随着模型容量增加而提高，这支持了模型收敛于共享星系天体物理学表示的假说。&lt;h4&gt;结论&lt;/h4&gt;天文学基础模型可以采用预训练的通用架构，从而能够利用机器学习社区已经投入的计算资源。&lt;h4&gt;翻译&lt;/h4&gt;我们通过测量在不同数据类型上训练的一系列基础模型的表示收敛性，在天文学中测试柏拉图表示假说。使用JWST、HSC、Legacy Survey和DESI的光谱和成像观测数据，我们通过互k近邻分析比较了视觉变换器、自监督模型和天文学特定架构的表示。我们观察到一致的扩展趋势：在我们测试的架构中，表示对齐通常随着模型容量增加而提高，支持了向共享星系天体物理学表示收敛的观点。我们的结果表明，天文学基础模型可以使用预训练的通用架构，使我们能够利用更广泛的机器学习社区已经投入的计算投资。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We test the Platonic Representation Hypothesis (PRH) in astronomy bymeasuring representational convergence across a range of foundation modelstrained on different data types. Using spectroscopic and imaging observationsfrom JWST, HSC, Legacy Survey, and DESI, we compare representations from visiontransformers, self-supervised models, and astronomy-specific architectures viamutual $k$-nearest neighbour analysis. We observe consistent scaling:representational alignment generally increases with model capacity across ourtested architectures, supporting convergence toward a shared representation ofgalaxy astrophysics. Our results suggest that astronomical foundation modelscan use pre-trained general-purpose architectures, allowing us to capitalise onthe broader machine learning community's already-spent computationalinvestment.</description>
      <author>example@mail.com (UniverseTBD, :, Kshitij Duraphe, Michael J. Smith, Shashwat Sourav, John F. Wu)</author>
      <guid isPermaLink="false">2509.19453v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation</title>
      <link>http://arxiv.org/abs/2509.19215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 Workshop on Learning from Time Series for  Health&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了PPG-Distill知识蒸馏框架，用于在资源有限的设备上高效部署大型光电容积脉搏波基础模型，提升性能并减少资源消耗。&lt;h4&gt;背景&lt;/h4&gt;光电容积描记法(PPG)在可穿戴健康监测中被广泛使用，但大型PPG基础模型难以在资源有限的设备上部署。&lt;h4&gt;目的&lt;/h4&gt;开发一个知识蒸馏框架，能够在资源受限的可穿戴设备上实现高效的PPG分析。&lt;h4&gt;方法&lt;/h4&gt;PPG-Distill通过预测级、特征级和补丁级蒸馏来转移全局和局部知识，包含形态学蒸馏以保留局部波形模式，以及节律蒸馏以捕获补丁间的时间结构。&lt;h4&gt;主要发现&lt;/h4&gt;在心率和房颤检测任务上，PPG-Distill将学生模型的性能提高了高达21.8%，同时实现了7倍的推理速度提升和19倍的内存使用减少。&lt;h4&gt;结论&lt;/h4&gt;PPG-Distill使在可穿戴设备上高效的PPG分析成为可能，解决了大型模型在资源受限设备上的部署挑战。&lt;h4&gt;翻译&lt;/h4&gt;光电容积描记法(PPG)在可穿戴健康监测中被广泛使用，但大型PPG基础模型难以在资源有限的设备上部署。我们提出了PPG-Distill，这是一个通过预测级、特征级和补丁级蒸馏来转移全局和局部知识的知识蒸馏框架。PPG-Distill包含形态学蒸馏以保留局部波形模式，以及节律蒸馏以捕获补丁间的时间结构。在心率和房颤检测方面，PPG-Distill将学生模型的性能提高了高达21.8%，同时实现了7倍的推理速度和19倍的内存使用减少，从而实现了在可穿戴设备上的高效PPG分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoplethysmography (PPG) is widely used in wearable health monitoring, yetlarge PPG foundation models remain difficult to deploy on resource-limiteddevices. We present PPG-Distill, a knowledge distillation framework thattransfers both global and local knowledge through prediction-, feature-, andpatch-level distillation. PPG-Distill incorporates morphology distillation topreserve local waveform patterns and rhythm distillation to capture inter-patchtemporal structures. On heart rate estimation and atrial fibrillationdetection, PPG-Distill improves student performance by up to 21.8% whileachieving 7X faster inference and reducing memory usage by 19X, enablingefficient PPG analysis on wearables</description>
      <author>example@mail.com (Juntong Ni, Saurabh Kataria, Shengpu Tang, Carl Yang, Xiao Hu, Wei Jin)</author>
      <guid isPermaLink="false">2509.19215v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications</title>
      <link>http://arxiv.org/abs/2509.19185v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对基于基础模型的AI代理测试实践进行了首次大规模实证研究，分析了39个开源代理框架和439个代理应用，揭示了测试努力的分布不均和关键盲点。&lt;h4&gt;背景&lt;/h4&gt;基于基础模型的AI代理在多个领域迅速普及，但其内在的非确定性和不可重现性给测试和质量保证带来挑战。现有基准主要提供任务级别评估，但对开发者在开发过程中如何验证代理内部正确性的理解有限。&lt;h4&gt;目的&lt;/h4&gt;解决AI代理生态系统中测试实践的知识空白，提供首个大规模实证研究，以了解开发者如何验证AI代理的内部正确性。&lt;h4&gt;方法&lt;/h4&gt;分析39个开源代理框架和439个代理应用，识别10种不同的测试模式，并将这些模式映射到代理框架和应用的典型架构组件。&lt;h4&gt;主要发现&lt;/h4&gt;新的代理特定测试方法(如DeepEval)很少使用(约1%)，传统测试模式被广泛采用以管理基础模型不确定性；测试努力分布不均，确定性组件消耗70%以上测试努力，而基于基础模型的计划部分获得不到5%；触发组件(提示)被严重忽视，仅出现在约1%的测试中。&lt;h4&gt;结论&lt;/h4&gt;研究提供了基础模型代理框架和应用的第一个实证测试基线，揭示了对非确定性的理性但不完整的适应；建议框架开发者改进对新型测试方法的支持，应用开发者采用提示回归测试，研究人员探索采用障碍，以构建更强大可靠的AI代理。&lt;h4&gt;翻译&lt;/h4&gt;基于基础模型的AI代理正迅速在各个领域获得采用，但其固有的非确定性和不可重现性给测试和质量保证带来挑战。虽然最近的基准提供了任务级别的评估，但对于开发者在开发过程中如何验证这些代理的内部正确性，理解仍然有限。为解决这一空白，我们对AI代理生态系统中的测试实践进行了首次大规模实证研究，分析了39个开源代理框架和439个代理应用。我们确定了十种不同的测试模式，发现新的、代理特定的方法(如DeepEval)很少使用(约1%)，而传统模式(如负面测试和成员测试)被广泛采用以管理基础模型的不确定性。通过将这些模式映射到代理框架和代理应用的典型架构组件，我们揭示了测试努力的根本性倒置：确定性组件(如资源工件(工具)和协调工件(工作流))消耗了70%以上的测试努力，而基于基础模型的计划部分获得不到5%。关键的是，这揭示了一个关键盲点，因为触发组件(提示)仍然被忽视，仅出现在约1%的所有测试中。我们的研究为基于基础模型的代理框架和代理应用提供了第一个实证测试基线，揭示了对非确定性的理性但不完整的适应。为解决这一问题，框架开发者应改进对新型测试方法的支持，应用开发者必须采用提示回归测试，研究人员应探索采用的障碍。加强这些实践对于构建更强大和可靠的AI代理至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation model (FM)-based AI agents are rapidly gaining adoption acrossdiverse domains, but their inherent non-determinism and non-reproducibilitypose testing and quality assurance challenges. While recent benchmarks providetask-level evaluations, there is limited understanding of how developers verifythe internal correctness of these agents during development.  To address this gap, we conduct the first large-scale empirical study oftesting practices in the AI agent ecosystem, analyzing 39 open-source agentframeworks and 439 agentic applications. We identify ten distinct testingpatterns and find that novel, agent-specific methods like DeepEval are seldomused (around 1%), while traditional patterns like negative and membershiptesting are widely adapted to manage FM uncertainty. By mapping these patternsto canonical architectural components of agent frameworks and agenticapplications, we uncover a fundamental inversion of testing effort:deterministic components like Resource Artifacts (tools) and CoordinationArtifacts (workflows) consume over 70% of testing effort, while the FM-basedPlan Body receives less than 5%. Crucially, this reveals a critical blind spot,as the Trigger component (prompts) remains neglected, appearing in around 1% ofall tests.  Our findings offer the first empirical testing baseline in FM-based agentframeworks and agentic applications, revealing a rational but incompleteadaptation to non-determinism. To address it, framework developers shouldimprove support for novel testing methods, application developers must adoptprompt regression testing, and researchers should explore barriers to adoption.Strengthening these practices is vital for building more robust and dependableAI agents.</description>
      <author>example@mail.com (Mohammed Mehedi Hasan, Hao Li, Emad Fallahzadeh, Gopi Krishnan Rajbahadur, Bram Adams, Ahmed E. Hassan)</author>
      <guid isPermaLink="false">2509.19185v2</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions</title>
      <link>http://arxiv.org/abs/2509.19165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在恶劣天气条件下鲁棒的自监督立体匹配方法，通过注入视觉基础模型先验和场景对应先验来解决恶劣天气下性能下降的问题&lt;h4&gt;背景&lt;/h4&gt;现有的自监督立体匹配方法在良好条件下表现优异，但在夜间、雨天、雾天等恶劣天气条件下性能显著下降&lt;h4&gt;目的&lt;/h4&gt;解决恶劣天气条件下自监督立体匹配方法性能下降的问题，提高模型在恶劣天气条件下的视差估计能力&lt;h4&gt;方法&lt;/h4&gt;1) 将视觉基础模型推导的鲁棒先验注入CNN特征提取器；2) 引入场景对应先验构建鲁棒监督信号；3) 创建具有真实天气退化的合成立体数据集；4) 提出包含鲁棒自监督场景对应学习和恶劣天气蒸馏两步骤的训练范式&lt;h4&gt;主要发现&lt;/h4&gt;恶劣天气条件下性能下降的主要原因是：1) 恶劣天气引入噪声降低可见度，使CNN特征提取器难以处理退化区域；2) 退化区域破坏像素对应关系，导致基于光度一致性假设的监督效果不佳&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过注入鲁棒先验和构建新的训练范式，显著提高了恶劣天气条件下的立体匹配性能，实验证明其优于现有最先进方法&lt;h4&gt;翻译&lt;/h4&gt;最近的自监督立体匹配方法已取得显著进展，但在夜间、雨天、雾天等恶劣天气条件下，其性能显著下降。我们确定了导致性能下降的两个主要弱点。首先，恶劣天气会引入噪声并降低可见度，使基于CNN的特征提取器难以处理反光和无纹理等退化区域。其次，这些退化区域会破坏准确的像素对应关系，导致基于光度一致性假设的监督效果不佳。为解决这些挑战，我们提出将视觉基础模型推导的鲁棒先验注入基于CNN的特征提取器中，以改善恶劣天气条件下的特征表示。然后引入场景对应先验来构建鲁棒的监督信号，而不是仅仅依赖光度一致性假设。具体而言，我们创建了具有真实天气退化的合成立体数据集。这些数据集包含清晰和恶劣的图像对，保持相同的语义上下文和视差，保留了场景对应属性。基于此，我们提出了一种鲁棒的自监督训练范式，包含两个关键步骤：鲁棒自监督场景对应学习和恶劣天气蒸馏。这两个步骤都旨在对齐来自清晰和恶劣图像对的基础场景结果，从而提高模型在恶劣天气影响下的视差估计能力。大量实验证明了我们提出的解决方案的有效性和通用性，其性能优于现有的最先进自监督方法。代码可在GitHub上获取&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在恶劣天气条件（如夜间、雨天、雾天）下的立体匹配性能下降问题。这个问题在现实中非常重要，因为自动驾驶系统需要在各种天气条件下可靠运行，恶劣天气会影响视觉传感器的性能，可能导致安全隐患。在研究方面，这个问题也很重要，因为现有的自监督立体匹配方法在良好天气条件下表现良好，但在恶劣天气条件下性能显著下降，限制了自监督学习在实际场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有自监督立体匹配方法在恶劣天气条件下性能下降的两个主要原因：CNN特征提取器难以处理退化区域，以及基于光度一致性假设的监督失效。作者借鉴了多个现有工作：视觉基础模型（如SAM和DAMv2）提供鲁棒特征，特征金字塔网络（FPN）捕捉细节，CycleGAN生成合成恶劣天气图像对，以及知识蒸馏的思想。作者的创新在于将这些技术整合到一个统一框架中，并专门针对恶劣天气条件下的立体匹配问题设计了反恶劣天气特征增强模块（AFEM）和场景对应学习机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入场景对应先验和视觉基础模型的鲁棒先验，提高立体匹配模型在恶劣天气条件下的鲁棒性。具体来说，作者认为清晰图像对和对应的恶劣天气图像对应该具有相同的语义上下文和视差，这一先验可用于构建鲁棒监督信号。整体流程包括两个步骤：首先进行自监督场景对应学习，构建两个分支处理清晰和恶劣天气图像，通过特征一致性和视差一致性损失确保模型学习不受天气影响的特征；然后进行恶劣天气蒸馏，使用高质量伪标签训练模型，提高在恶劣条件下的性能。此外，还设计了AFEM模块在空间、通道和频域上处理特征，提取退化不变特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次解决各种恶劣条件下的自监督立体匹配问题；2)提出结合视觉基础模型、FPN和AFEM的鲁棒特征提取器；3)设计包含两种一致性损失的两步自监督训练管道；4)创建保持场景对应属性的合成恶劣天气数据集。相比之前工作，RoSe专门针对多种恶劣天气条件设计，结合了视觉基础模型和CNN的优势，引入场景对应先验替代单纯的光度一致性假设，并采用两步训练策略提高模型在恶劣条件下的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RoSe通过结合视觉基础模型的鲁棒特征提取和基于场景对应先验的自监督学习，显著提高了立体匹配模型在夜间、雨天和雾天等恶劣天气条件下的性能，为自动驾驶和场景重建等应用提供了更可靠的深度估计解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent self-supervised stereo matching methods have made significantprogress, but their performance significantly degrades under adverse weatherconditions such as night, rain, and fog. We identify two primary weaknessescontributing to this performance degradation. First, adverse weather introducesnoise and reduces visibility, making CNN-based feature extractors struggle withdegraded regions like reflective and textureless areas. Second, these degradedregions can disrupt accurate pixel correspondences, leading to ineffectivesupervision based on the photometric consistency assumption. To address thesechallenges, we propose injecting robust priors derived from the visualfoundation model into the CNN-based feature extractor to improve featurerepresentation under adverse weather conditions. We then introduce scenecorrespondence priors to construct robust supervisory signals rather thanrelying solely on the photometric consistency assumption. Specifically, wecreate synthetic stereo datasets with realistic weather degradations. Thesedatasets feature clear and adverse image pairs that maintain the same semanticcontext and disparity, preserving the scene correspondence property. With thisknowledge, we propose a robust self-supervised training paradigm, consisting oftwo key steps: robust self-supervised scene correspondence learning and adverseweather distillation. Both steps aim to align underlying scene results fromclean and adverse image pairs, thus improving model disparity estimation underadverse weather effects. Extensive experiments demonstrate the effectivenessand versatility of our proposed solution, which outperforms existingstate-of-the-art self-supervised methods. Codes are available at\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.</description>
      <author>example@mail.com (Yun Wang, Junjie Hu, Junhui Hou, Chenghao Zhang, Renwei Yang, Dapeng Oliver Wu)</author>
      <guid isPermaLink="false">2509.19165v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation</title>
      <link>http://arxiv.org/abs/2509.19112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeuRIPS2025 Workshop on Structured Probabilistic  Inference and Generative Modeling&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CARGO是一种可扩展的多标签因果发现方法，用于处理稀疏、高维事件序列，通过预训练的因果Transformer和两阶段推理方法实现高效的结构化推理&lt;h4&gt;背景&lt;/h4&gt;理解事件序列中的因果关系是一个跨领域挑战，特别是在医疗保健或车辆诊断等领域，其中疾病或系统故障等结果标签源于症状或错误代码等先前事件&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展的多标签因果发现方法，用于处理包含数千种独特事件类型的稀疏、高维事件序列&lt;h4&gt;方法&lt;/h4&gt;CARGO使用两个预训练的因果Transformer作为领域特定基础模型，并行推断每个序列的一次性因果图，并使用自适应频率融合聚合它们，以重建标签的全局马尔可夫边界，实现高效的大规模概率推理&lt;h4&gt;主要发现&lt;/h4&gt;在具有超过29,100种独特事件类型和474个不平衡标签的真实世界汽车故障预测数据集上，CARGO展示了其执行结构化推理的能力&lt;h4&gt;结论&lt;/h4&gt;CARGO能够有效处理高维稀疏事件序列中的因果关系推断，绕过全数据集条件独立性测试的不可承受成本&lt;h4&gt;翻译&lt;/h4&gt;理解事件序列中的因果关系，其中疾病或系统故障等结果标签源于症状或错误代码等先前事件，跨医疗保健或车辆诊断等领域仍是一个未解决的挑战。我们介绍了CARGO，一种用于处理包含数千种独特事件类型的稀疏、高维事件序列的可扩展多标签因果发现方法。使用两个预训练的因果Transformer作为事件序列的领域特定基础模型，CARGO并行推断每个序列的一次性因果图，并使用自适应频率融合聚合它们，以重建标签的全局马尔可夫边界。这种两阶段方法能够实现高效的大规模概率推理，同时绕过全数据集条件独立性测试的不可承受成本。我们在一个具有超过29,100种独特事件类型和474个不平衡标签的具有挑战性的真实世界汽车故障预测数据集上的结果证明了CARGO执行结构化推理的能力&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding causality in event sequences where outcome labels such asdiseases or system failures arise from preceding events like symptoms or errorcodes is critical. Yet remains an unsolved challenge across domains likehealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-labelcausal discovery method for sparse, high-dimensional event sequences comprisingof thousands of unique event types. Using two pretrained causal Transformers asdomain-specific foundation models for event sequences. CARGO infers inparallel, per sequence one-shot causal graphs and aggregates them using anadaptive frequency fusion to reconstruct the global Markov boundaries oflabels. This two-stage approach enables efficient probabilistic reasoning atscale while bypassing the intractable cost of full-dataset conditionalindependence testing. Our results on a challenging real-world automotive faultprediction dataset with over 29,100 unique event types and 474 imbalancedlabels demonstrate CARGO's ability to perform structured reasoning.</description>
      <author>example@mail.com (Hugo Math, Rainer Lienhart)</author>
      <guid isPermaLink="false">2509.19112v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning</title>
      <link>http://arxiv.org/abs/2509.19090v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Citrus-V是一种多模态医学基础模型，整合了图像分析与文本推理能力，在单一框架内实现了病变检测、分割和诊断推理，在多个医学影像任务中表现出色，优于现有开源模型和专家级系统。&lt;h4&gt;背景&lt;/h4&gt;医学影像为临床诊断提供关键证据，但现有模型通常专注于单一任务，需要多个专业网络，限制了泛化能力。虽然大规模语言和多模态模型有强大推理能力，但临床应用需要精确的视觉定位、多模态整合和思维链推理。&lt;h4&gt;目的&lt;/h4&gt;开发一个结合图像分析与文本推理的多模态医学基础模型，整合检测、分割和多模态思维链推理，实现像素级病变定位、结构化报告生成和类医师诊断推理。&lt;h4&gt;方法&lt;/h4&gt;提出新颖的多模态训练方法，发布涵盖推理、检测、分割和文档理解任务的开源数据套件，Citrus-V模型整合了检测、分割和多模态思维链推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;Citrus-V在多个基准测试中优于现有开源医学模型和专家级影像系统，提供从视觉定位到临床推理的统一流程，支持精确病变量化、自动报告生成和可靠第二意见。&lt;h4&gt;结论&lt;/h4&gt;Citrus-V代表了一种多模态医学基础模型的新方法，有效结合图像分析与文本推理，在多个医学影像任务中表现出色，通过开源方式发布促进了医学AI领域发展。&lt;h4&gt;翻译&lt;/h4&gt;医学影像为临床诊断、治疗计划和手术决策提供关键证据，但大多数现有影像模型专注于狭窄领域，需要多个专业网络，限制了其泛化能力。虽然大规模语言和多模态模型展现出强大的推理和多任务能力，但现实世界的临床应用需要精确的视觉定位、多模态整合和思维链推理。我们引入了Citrus-V，这是一个结合图像分析与文本推理的多模态医学基础模型。该模型整合了检测、分割和多模态思维链推理，能够在单一框架内实现像素级病变定位、结构化报告生成和类医师诊断推理。我们提出了一种新颖的多模态训练方法，并发布了一个精心策划的开源数据套件，涵盖推理、检测、分割和文档理解任务。评估表明，Citrus-V在多个基准测试中优于现有的开源医学模型和专家级影像系统，提供了从视觉定位到临床推理的统一流程，支持精确的病变量化、自动报告生成和可靠的第二意见。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical imaging provides critical evidence for clinical diagnosis, treatmentplanning, and surgical decisions, yet most existing imaging models are narrowlyfocused and require multiple specialized networks, limiting theirgeneralization. Although large-scale language and multimodal models exhibitstrong reasoning and multi-task capabilities, real-world clinical applicationsdemand precise visual grounding, multimodal integration, and chain-of-thoughtreasoning. We introduce Citrus-V, a multimodal medical foundation model thatcombines image analysis with textual reasoning. The model integrates detection,segmentation, and multimodal chain-of-thought reasoning, enabling pixel-levellesion localization, structured report generation, and physician-likediagnostic inference in a single framework. We propose a novel multimodaltraining approach and release a curated open-source data suite coveringreasoning, detection, segmentation, and document understanding tasks.Evaluations demonstrate that Citrus-V outperforms existing open-source medicalmodels and expert-level imaging systems across multiple benchmarks, deliveringa unified pipeline from visual grounding to clinical reasoning and supportingprecise lesion quantification, automated reporting, and reliable secondopinions.</description>
      <author>example@mail.com (Guoxin Wang, Jun Zhao, Xinyi Liu, Yanbo Liu, Xuyang Cao, Chao Li, Zhuoyun Liu, Qintian Sun, Fangru Zhou, Haoqiang Xing, Zhenhong Yang)</author>
      <guid isPermaLink="false">2509.19090v2</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?</title>
      <link>http://arxiv.org/abs/2509.19070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Open Science for Foundation Models (SCI-FM) Workshop  at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ColorBlindnessEval，一个受石原色盲测试启发的视觉对抗场景中评估视觉语言模型(VLMs)鲁棒性的新基准测试。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在复杂视觉环境中的鲁棒性有待提高，特别是在对抗性场景中。&lt;h4&gt;目的&lt;/h4&gt;评估VLMs在受石原色盲测试启发的视觉对抗场景中的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含500个类似石原图像的数据集，图像中包含0到99的数字并具有不同的颜色组合；评估了9个VLMs，使用是/否和开放式提示，并与人类参与者进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;模型在解释对抗环境中的数字方面存在局限性，存在普遍的幻觉问题。&lt;h4&gt;结论&lt;/h4&gt;需要提高VLMs在复杂视觉环境中的鲁棒性，以确保在现实应用中的准确性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了ColorBlindnessEval，一个新颖的基准测试，旨在评估视觉语言模型(VLMs)在受石原色盲测试启发的视觉对抗场景中的鲁棒性。我们的数据集包含500个类似石原的图像，图像中包含0到99的数字并具有不同的颜色组合，挑战VLMs准确识别嵌入在复杂视觉模式中的数字信息。我们使用是/否和开放式提示评估了9个VLMs，并将其性能与人类参与者进行比较。我们的实验揭示了模型在解释对抗环境中数字的能力方面的局限性，突显了普遍存在的幻觉问题。这些发现强调了需要提高VLMs在复杂视觉环境中的鲁棒性。ColorBlindnessEval作为基准测试工具，对于提高VLMs在准确性至关重要的现实应用中的可靠性具有重要价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents ColorBlindnessEval, a novel benchmark designed toevaluate the robustness of Vision-Language Models (VLMs) in visuallyadversarial scenarios inspired by the Ishihara color blindness test. Ourdataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 withvarying color combinations, challenging VLMs to accurately recognize numericalinformation embedded in complex visual patterns. We assess 9 VLMs using Yes/Noand open-ended prompts and compare their performance with human participants.Our experiments reveal limitations in the models' ability to interpret numbersin adversarial contexts, highlighting prevalent hallucination issues. Thesefindings underscore the need to improve the robustness of VLMs in complexvisual environments. ColorBlindnessEval serves as a valuable tool forbenchmarking and improving the reliability of VLMs in real-world applicationswhere accuracy is critical.</description>
      <author>example@mail.com (Zijian Ling, Han Zhang, Yazhuo Zhou, Jiahao Cui)</author>
      <guid isPermaLink="false">2509.19070v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>MAPO: Mixed Advantage Policy Optimization</title>
      <link>http://arxiv.org/abs/2509.18849v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mixed Advantage Policy Optimization (MAPO)的简单而有效的GRPO策略，解决了现有方法中的优势反转和优势镜像问题，通过动态重新加权优势函数来适应不同轨迹确定性的样本。&lt;h4&gt;背景&lt;/h4&gt;基础模型在推理任务上的性能通过强化学习方法（如Group Relative Policy Optimization, GRPO）得到了显著提升，其中优势函数作为GRPO的核心机制用于排序轨迹重要性。&lt;h4&gt;目的&lt;/h4&gt;解决现有GRPO方法中遇到的优势反转和优势镜像问题，实现不同查询样本间的合理优势分配。&lt;h4&gt;方法&lt;/h4&gt;揭示轨迹具有不同的确定性，为高确定性轨迹的样本提出优势百分比偏差，并动态重新加权具有不同轨迹确定性的样本的优势函数，使其自适应地考虑样本特定特征。&lt;h4&gt;主要发现&lt;/h4&gt;通过与相关最先进方法的比较以及不同优势变体的消融研究，验证了MAPO方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;MAPO是一种简单但有效的GRPO策略，能够解决现有方法中的问题，提高基础模型在推理任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;基础模型的强化学习最新进展，如群体相对策略优化（GRPO），显著提高了基础模型在推理任务上的性能。值得注意的是，优势函数在GRPO中作为核心机制用于排序轨迹重要性。然而，现有探索遇到了优势反转和优势镜像问题，这阻碍了不同查询样本间的合理优势分配。在这项工作中，我们提出了一种简单但有效的GRPO策略，混合优势策略优化（MAPO）。我们揭示轨迹具有不同的确定性，并为高确定性轨迹的样本提出了优势百分比偏差。此外，我们动态重新加权具有不同轨迹确定性的样本的优势函数，从而自适应地配置优势函数以考虑样本特定特征。与相关最先进方法的比较以及不同优势变体的消融研究验证了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in reinforcement learning for foundation models, such asGroup Relative Policy Optimization (GRPO), have significantly improved theperformance of foundation models on reasoning tasks. Notably, the advantagefunction serves as a central mechanism in GRPO for ranking the trajectoryimportance. However, existing explorations encounter both advantage reversionand advantage mirror problems, which hinder the reasonable advantage allocationacross different query samples. In this work, we propose an easy but effectiveGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that thetrajectory appears with different certainty and propose the advantage percentdeviation for samples with high-certainty trajectories. Furthermore, wedynamically reweight the advantage function for samples with varying trajectorycertainty, thereby adaptively configuring the advantage function to account forsample-specific characteristics. Comparison with related state-of-the-artmethods, along with ablation studies on different advantage variants, validatesthe effectiveness of our approach.</description>
      <author>example@mail.com (Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, Leszek Rutkowski, Mang Ye, Bo Du, Dacheng Tao)</author>
      <guid isPermaLink="false">2509.18849v2</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model</title>
      <link>http://arxiv.org/abs/2509.18751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MOMEMTO，一种基于时间序列基础模型(TFM)的异常检测方法，通过引入基于补丁的内存模块来缓解过拟合问题。&lt;h4&gt;背景&lt;/h4&gt;基于重建的深度模型被广泛用于时间序列异常检测，但随着模型容量增加，这些模型往往会过拟合，能准确重建未见过的异常。先前工作通过引入存储正常模式原型的内存架构来缓解，但这些方法训练成本高，且尚未与时间序列基础模型有效集成。&lt;h4&gt;目的&lt;/h4&gt;解决基于重建的深度模型在时间序列异常检测中的过拟合问题，并降低训练成本，同时与时间序列基础模型有效集成。&lt;h4&gt;方法&lt;/h4&gt;MOMEMTO包含一个内存模块，用于捕获来自多个域的代表性正常模式，并通过多域训练策略使单个模型能在多个数据集上联合微调。内存模块使用预训练编码器的潜在表示初始化，组织成补丁级别单元，并通过注意力机制更新。&lt;h4&gt;主要发现&lt;/h4&gt;使用23个单变量基准数据集评估，MOMEMTO作为单一模型在AUC和VUS指标上优于基线方法，并且在少样本学习场景中显著提升了骨干TFM的性能。&lt;h4&gt;结论&lt;/h4&gt;MOMEMTO成功解决了基于重建的深度模型的过拟合问题，通过基于补丁的内存模块和多域训练策略实现了更好的异常检测性能，特别是在少样本学习场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;最近基于重建的深度模型已被广泛用于时间序列异常检测，但随着其容量和表示能力的增加，这些模型往往会过拟合，经常能准确重建未见过的异常。先前的工作试图通过引入存储正常模式原型的内存架构来缓解这一问题。然而，这些方法存在训练成本高的问题，并且尚未与时间序列基础模型有效集成。为了解决这些挑战，我们提出了MOMEMTO，一种用于异常检测的TFM，通过基于补丁的内存模块增强，以减轻过拟合。该内存模块设计用于捕获来自多个域的代表性正常模式，并通过多域训练策略使单个模型能够在多个数据集上进行联合微调。MOMEMTO使用预训练编码器的潜在表示初始化内存项，将它们组织成补丁级别单元，并通过注意力机制进行更新。我们使用23个单变量基准数据集评估了我们的方法。实验结果表明，MOMEMTO作为单一模型，在AUC和VUS指标上比基线方法获得了更高的分数，并且在少样本学习场景中进一步增强了其骨干TFM的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently reconstruction-based deep models have been widely used for timeseries anomaly detection, but as their capacity and representation capabilityincrease, these models tend to over-generalize, often reconstructing unseenanomalies accurately. Prior works have attempted to mitigate this byincorporating a memory architecture that stores prototypes of normal patterns.Nevertheless, these approaches suffer from high training costs and have yet tobe effectively integrated with time series foundation models (TFMs). To addressthese challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,enhanced with a patch-based memory module to mitigate over-generalization. Thememory module is designed to capture representative normal patterns frommultiple domains and enables a single model to be jointly fine-tuned acrossmultiple datasets through a multi-domain training strategy. MOMEMTO initializesmemory items with latent representations from a pre-trained encoder, organizesthem into patch-level units, and updates them via an attention mechanism. Weevaluate our method using 23 univariate benchmark datasets. Experimentalresults demonstrate that MOMEMTO, as a single model, achieves higher scores onAUC and VUS metrics compared to baseline methods, and further enhances theperformance of its backbone TFM, particularly in few-shot learning scenarios.</description>
      <author>example@mail.com (Samuel Yoon, Jongwon Kim, Juyoung Ha, Young Myoung Ko)</author>
      <guid isPermaLink="false">2509.18751v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Transfer from Interaction Learning</title>
      <link>http://arxiv.org/abs/2509.18733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'从交互中学习'(LFI)的认知启发框架，通过显式建模视觉理解作为交互过程，解决了视觉基础模型(VFMs)从视觉语言模型(VLMs)转移知识时的局限性。该方法通过交互查询和基于交互的监督两个技术创新，有效捕获VLMs中的动态交互模式，实现了更忠实和高效的知识转移。&lt;h4&gt;背景&lt;/h4&gt;当前视觉基础模型(VFMs)在从视觉语言模型(VLMs)转移知识方面存在根本性限制。虽然VLMs在通过统一表征空间建模跨模态交互方面表现出色，但现有的VFMs主要采用结果导向的范式，忽略了底层的交互过程。这种表征差异阻碍了有效的知识转移，限制了VFMs在多样化视觉任务上的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决VFMs从VLMs转移知识时的表征差异问题，通过显式建模视觉理解作为交互过程，实现更忠实和高效的知识转移，提升VFMs在多样化视觉任务上的泛化能力，特别是在跨域设置中的表现。&lt;h4&gt;方法&lt;/h4&gt;提出'从交互中学习'(LFI)框架，包含两个技术创新：1) 交互查询(Interaction Queries)：在网络层之间保持持久的结构关系；2) 基于交互的监督(interaction-based supervision)：源自VLMs的跨模态注意力机制。通过捕捉预训练VLMs中编码的动态交互模式，实现更忠实和高效的知识转移到VFMs。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示在多个基准测试中取得了一致的改进：在TinyImageNet分类和COCO检测/分割任务上分别实现了3.3和1.6/2.4的mAP/AP绝对提升，参数开销小且收敛更快。该框架在跨域设置中表现尤为出色，在PACS和VLCS上分别实现了2.4和9.3的零样本提升。人类评估进一步证实了其认知一致性，在语义一致性指标上比结果导向方法高出2.7倍。&lt;h4&gt;结论&lt;/h4&gt;通过显式建模视觉理解作为交互过程，LFI框架有效解决了VFMs从VLMs转移知识时的表征差异问题，实现了更忠实和高效的知识转移，显著提升了VFMs在多样化视觉任务上的性能，特别是在跨域设置中的表现。该方法具有参数开销小、收敛快的特点，且在语义一致性方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;当前视觉基础模型(VFMs)在从视觉语言模型(VLMs)转移知识方面面临根本性限制，而VLMs在通过统一表征空间建模跨模态交互方面表现出色，现有的VFMs主要采用结果导向的范式，忽略了底层的交互过程。这种表征差异阻碍了有效的知识转移，限制了VFMs在多样化视觉任务上的泛化能力。我们提出了'从交互中学习'(LFI)，一种认知启发框架，通过将视觉理解明确建模为交互过程来解决这一差距。我们的核心见解是，捕捉预训练VLMs中编码的动态交互模式，可以实现更忠实和高效的知识转移到VFMs。该方法围绕两个技术创新展开：交互查询，在网络层之间保持持久的结构关系；以及基于交互的监督，源自VLMs的跨模态注意力机制。全面的实验证明在多个基准测试中取得了一致的改进，在TinyImageNet分类和COCO检测/分割任务上分别实现了3.3和1.6/2.4的mAP/AP绝对提升，参数开销最小且收敛更快。该框架在跨域设置中表现尤为出色，在PACS和VLCS上分别实现了2.4和9.3的零样本提升。人类评估进一步证实了其认知一致性，在语义一致性指标上比结果导向方法高出2.7倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current visual foundation models (VFMs) face a fundamental limitation intransferring knowledge from vision language models (VLMs), while VLMs excel atmodeling cross-modal interactions through unified representation spaces,existing VFMs predominantly adopt result-oriented paradigms that neglect theunderlying interaction processes. This representational discrepancy hinderseffective knowledge transfer and limits generalization across diverse visiontasks. We propose Learning from Interactions (LFI), a cognitive-inspiredframework that addresses this gap by explicitly modeling visual understandingas an interactive process. Our key insight is that capturing the dynamicinteraction patterns encoded in pre-trained VLMs enables more faithful andefficient knowledge transfer to VFMs. The approach centers on two technicalinnovations, Interaction Queries, which maintain persistent relationalstructures across network layers, and interaction-based supervision, derivedfrom the cross-modal attention mechanisms of VLMs. Comprehensive experimentsdemonstrate consistent improvements across multiple benchmarks, achieving 3.3and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCOdetection/segmentation respectively, with minimal parameter overhead and fasterconvergence. The framework particularly excels in cross-domain settings,delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Humanevaluations further confirm its cognitive alignment, outperformingresult-oriented methods by 2.7 times in semantic consistency metrics.</description>
      <author>example@mail.com (Yilin Gao, Kangyi Chen, Zhongxing Peng, Hengjie Lu, Shugong Xu)</author>
      <guid isPermaLink="false">2509.18733v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images</title>
      <link>http://arxiv.org/abs/2509.18711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RSVG-ZeroOV是一种无需训练的框架，利用冻结的通用基础模型实现零样本开放词汇遥感视觉定位，包含概述、聚焦和演进三个阶段，在无需任务特定训练的情况下提供高效可扩展的解决方案，性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;现有遥感视觉定位方法通常受限于封闭式词汇集，在开放世界场景中应用有限。最近的尝试虽然利用了通用基础模型，但过度依赖昂贵的高质量数据集和耗时的微调。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为RSVG-ZeroOV的无训练框架，探索冻结的通用基础模型在零样本开放词汇遥感视觉定位中的潜力。&lt;h4&gt;方法&lt;/h4&gt;RSVG-ZeroOV包含三个关键阶段：(i)概述：利用视觉语言模型获取交叉注意图，捕捉文本查询与视觉区域间的语义相关性；(ii)聚焦：利用扩散模型的细粒度建模先验，填充VLM忽略的对象结构和形状信息；(iii)演进：引入注意力演进模块抑制不相关激活，生成所指对象的纯化分割掩码。&lt;h4&gt;主要发现&lt;/h4&gt;无需繁琐的任务特定训练，RSVG-ZeroOV提供了一种高效且可扩展的解决方案。大量实验表明，所提出的框架持续优于现有的弱监督和零样本方法。&lt;h4&gt;结论&lt;/h4&gt;RSVG-ZeroOV框架在开放词汇遥感视觉定位任务中表现出色，无需额外训练即可实现高性能。&lt;h4&gt;翻译&lt;/h4&gt;遥感视觉定位(RSVG)旨在基于自由形式的自然语言表达式在遥感图像中定位对象。现有方法通常受限于封闭式词汇集，限制了它们在开放世界场景中的适用性。虽然最近尝试利用通用基础模型进行开放词汇RSVG，但它们过度依赖昂贵的高质量数据集和耗时的微调。为解决这些局限性，我们提出了RSVG-ZeroOV，一种无需训练的框架，旨在探索冻结的通用基础模型在零样本开放词汇RSVG中的潜力。具体而言，RSVG-ZeroOV包含三个关键阶段：(i)概述：我们利用视觉语言模型(VLM)获取交叉注意图，捕捉文本查询与视觉区域之间的语义相关性。(ii)聚焦：通过利用扩散模型(DM)的细粒度建模先验，我们填充了对象的结构和形状信息中的空白，这些信息经常被VLM忽略。(iii)演进：引入了一个简单而有效的注意力演进模块，以抑制不相关的激活，从而在所指对象上产生纯化的分割掩码。无需繁琐的任务特定训练，RSVG-ZeroOV提供了一种高效且可扩展的解决方案。大量实验表明，所提出的框架持续优于现有的弱监督和零样本方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing visual grounding (RSVG) aims to localize objects in remotesensing images based on free-form natural language expressions. Existingapproaches are typically constrained to closed-set vocabularies, limiting theirapplicability in open-world scenarios. While recent attempts to leveragegeneric foundation models for open-vocabulary RSVG, they overly rely onexpensive high-quality datasets and time-consuming fine-tuning. To addressthese limitations, we propose \textbf{RSVG-ZeroOV}, a training-free frameworkthat aims to explore the potential of frozen generic foundation models forzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three keystages: (i) Overview: We utilize a vision-language model (VLM) to obtaincross-attention\footnote[1]{In this paper, although decoder-only VLMs useself-attention over all tokens, we refer to the image-text interaction part ascross-attention to distinguish it from pure visual self-attention.}maps thatcapture semantic correlations between text queries and visual regions. (ii)Focus: By leveraging the fine-grained modeling priors of a diffusion model(DM), we fill in gaps in structural and shape information of objects, which areoften overlooked by VLM. (iii) Evolve: A simple yet effective attentionevolution module is introduced to suppress irrelevant activations, yieldingpurified segmentation masks over the referred objects. Without cumbersometask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.Extensive experiments demonstrate that the proposed framework consistentlyoutperforms existing weakly-supervised and zero-shot methods.</description>
      <author>example@mail.com (Ke Li, Di Wang, Ting Wang, Fuyu Dong, Yiming Zhang, Luyao Zhang, Xiangyu Wang, Shaofeng Li, Quan Wang)</author>
      <guid isPermaLink="false">2509.18711v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation</title>
      <link>http://arxiv.org/abs/2509.20207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for the ICCV 2025 e2e3D Workshop. To be published in the  Proceedings of the IEEE/CVF International Conference on Computer Vision  Workshops (ICCVW)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PU-Gaussian的新型点云上采样网络，通过各向异性三维高斯分布建模局部邻域，实现了高效且保持几何可解释性的点云上采样。&lt;h4&gt;背景&lt;/h4&gt;三维传感器产生的点云通常稀疏且带有噪声，这对需要密集和高保真三维表示的任务构成了挑战。现有方法往往以牺牲几何可解释性或对输入稀疏性的鲁棒性为代价。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保持几何可解释性且对输入稀疏性具有鲁棒性的点云上采样方法，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;PU-Gaussian网络使用各向异性三维高斯分布建模每个点的局部邻域，捕获底层几何结构，在局部几何域中通过直接点采样执行上采样。采样生成密集但粗糙的点云后，再通过细化网络调整输出，产生更均匀的分布和更锐利的边缘。&lt;h4&gt;主要发现&lt;/h4&gt;在PU1K和PUGAN数据集上的广泛测试表明，PU-Gaussian实现了最先进的性能，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;PU-Gaussian成功解决了点云上采样中的几何可解释性和鲁棒性问题，代码和模型权重已公开供社区使用。&lt;h4&gt;翻译&lt;/h4&gt;三维传感器产生的点云通常是稀疏和有噪声的，这对需要密集和高保真三维表示的任务构成了挑战。先前的工作已经探索了基于隐式特征的上采样和距离函数学习来解决这一问题，但往往以牺牲几何可解释性或对输入稀疏性的鲁棒性为代价。为了克服这些限制，我们提出了PU-Gaussian，一种新型上采样网络，它使用各向异性三维高斯分布建模每个点周围的局部邻域。这些高斯分布捕获了底层几何结构，使我们能够在局部几何域中通过直接点采样明确执行上采样。采样过程生成密集但粗糙的点云。随后的细化网络调整粗糙输出，以产生更均匀的分布和更锐利的边缘。我们在PU1K和PUGAN数据集上进行了广泛测试，证明PU-Gaussian实现了最先进的性能。我们在https://github.com/mvg-inatech/PU-Gaussian.git公开了代码和模型权重。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云上采样问题，即如何将稀疏、有噪声的点云转换为密集、高质量的点云。这个问题在现实中非常重要，因为点云是自动驾驶、机器人、增强现实和物体识别等应用的基础数据表示，而稀疏或不规则的点集会显著降低下游任务的性能，特别是当需要细粒度几何细节进行准确解释时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，如缺乏几何可解释性、对输入稀疏性不够鲁棒、计算开销大等。他们借鉴了Gaussian splatting技术，使用各向异性3D高斯分布来表示局部表面几何。同时采用了类似PU-Net的三阶段框架，但使用Point Transformer作为特征提取器，因为它轻量且能建模局部几何结构。作者将问题形式化为局部分布拟合，通过学习预测稀疏点云中每个点周围的高斯基元，然后直接从这些分布中采样点，从而生成更密集的点云。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用各向异性3D高斯分布来建模每个点周围的局部邻域，通过直接在高斯分布中采样来显式地进行上采样，这种方法能够捕捉底层的几何结构。整体实现流程分为三部分：1）高斯预测网络：输入稀疏点云，使用Point Transformer生成特征，预测每个点的尺度、旋转和偏移参数，计算高斯分布的均值和协方差；2）采样模块：从每个预测的高斯分布中采样多个点，使用重参数化技巧进行训练；3）精炼网络：对粗略点云进行精炼，提高空间精度和几何一致性，获得最终的高质量点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）引入PU-Gaussian，使用高斯表示进行点云上采样；2）显式局部几何建模，直接在3D空间中采样；3）提供可解释的上采样过程；4）两阶段架构，先生成粗略点云再精炼；5）轻量级设计，避免计算昂贵的K近邻操作。相比之前的工作，不同之处在于：不依赖特征空间上采样技术；不需要迭代精炼过程；高斯表示比离散体素更灵活；直接在3D空间中拟合高斯，无需解码阶段；在多个数据集上实现了最先进的性能，产生更少的异常值和更细粒度的细节。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了PU-Gaussian，一种基于各向异性3D高斯表示的点云上采样方法，通过直接在局部几何域中采样并精炼，实现了比现有方法更高质量、更鲁棒且更可解释的点云上采样效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds produced by 3D sensors are often sparse and noisy, posingchallenges for tasks requiring dense and high-fidelity 3D representations.Prior work has explored both implicit feature-based upsampling anddistance-function learning to address this, but often at the expense ofgeometric interpretability or robustness to input sparsity. To overcome theselimitations, we propose PU-Gaussian, a novel upsampling network that models thelocal neighborhood around each point using anisotropic 3D Gaussiandistributions. These Gaussians capture the underlying geometric structure,allowing us to perform upsampling explicitly in the local geometric domain bydirect point sampling. The sampling process generates a dense, but coarse,point cloud. A subsequent refinement network adjusts the coarse output toproduce a more uniform distribution and sharper edges. We perform extensivetesting on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achievesstate-of-the-art performance. We make code and model weights publicly availableat https://github.com/mvg-inatech/PU-Gaussian.git.</description>
      <author>example@mail.com (Mahmoud Khater, Mona Strauss, Philipp von Olshausen, Alexander Reiterer)</author>
      <guid isPermaLink="false">2509.20207v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>LidarScout: Direct Out-of-Core Rendering of Massive Point Clouds</title>
      <link>http://arxiv.org/abs/2509.20198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at High-Performance Graphics 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种能够即时可视化大规模地形扫描点云数据集的方法，无需预处理和额外磁盘空间。&lt;h4&gt;背景&lt;/h4&gt;大规模地形扫描是许多重要任务的基础，如地形测绘、林业、农业和基础设施规划。然而，点云数据集规模巨大，即使是查看这样的基本任务也需要花费数小时到数天的预处理时间。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够即时可视化包含数百亿点的大规模国家扫描数据集的方法。&lt;h4&gt;方法&lt;/h4&gt;通过分层加载和渲染策略：首先加载稀疏点子样本初始化概览；然后进行表面重建生成高质量高程图；根据用户导航优先处理视点区域的高程图构建；用户放大时加载完整分辨率数据；不再需要的区域数据被卸载但保留更新后的高程图纹理作为中等细节。&lt;h4&gt;主要发现&lt;/h4&gt;该方法构成了一种直接的核心外渲染方法，适用于处理TB级压缩的大规模点云数据集，无需预处理和额外磁盘空间。&lt;h4&gt;结论&lt;/h4&gt;该方法实现了大规模点云数据的即时可视化，显著提高了大规模地形数据的使用效率。&lt;h4&gt;翻译&lt;/h4&gt;大规模地形扫描是许多重要任务的基础，如地形测绘、林业、农业和基础设施规划。 resulting point cloud data sets are massive in size，以至于即使是查看这样的基本任务也需要花费数小时到数天的预处理时间，才能创建允许实时检查整个数据集的细节层次结构。在本文中，我们提出一种方法，能够即时可视化包含数百亿点的大规模国家扫描数据集。打开数据集时，我们首先加载稀疏的点子样本并初始化整个点云的概览，随后立即进行表面重建过程以生成更高质量、无空洞的高程图。当用户开始导航到感兴趣区域时，我们继续优先处理用户视点的高程图构建。一旦用户放大查看，我们加载该区域的完整分辨率点云数据，并用完整分辨率数据更新相应的高程图纹理。当用户导航到其他地方时，不再需要的完整分辨率点数据会被卸载，但更新后的高程图纹理会保留作为中等细节层次的形式。总体而言，我们的方法构成了一种直接的核心外渲染方法，用于处理大规模点云数据集（TB级，压缩），无需预处理和额外的磁盘空间。源代码、可执行文件、预训练模型和数据集可在以下网址获取：https://github.com/cg-tuwien/lidarscout&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大规模激光雷达扫描点云数据的快速可视化问题。随着激光扫描技术进步，产生的点云数据集越来越大（包含数百亿到数万亿个点），传统方法需要数小时到数天的预处理才能创建层次细节结构以便实时查看。这个问题很重要，因为这些大规模地形扫描是地形测绘、林业、农业和基础设施规划等任务的基础，快速查看对于寻找数据中的问题、查找特定区域和传输数据都是基本需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考的核心是如何避免传统方法中耗时的预处理步骤。他们注意到LAZ压缩格式将点存储在块中（每块50,000点），且每个块中的第一个点是不压缩的，这使他们能够快速访问这些'块点'作为稀疏子样本。他们借鉴了点云渲染技术、表面重建技术和神经渲染方法，特别是从稀疏点样本构建高程图的方法。作者采用了类似U-Net的神经网络架构，但进行了修改以适应他们的特定需求。然而，他们指出现有方法大多针对密集点云且需要相机姿态信息，这在他们的应用中不可用，且高程图重建在航空激光雷达扫描方面被忽视了。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过直接从压缩点云中快速提取稀疏子样本，并利用神经网络从这个稀疏样本生成高质量高程图，实现大规模点云的即时可视化，避免预处理和额外存储需求。整体流程包括：1)快速加载稀疏子样本（读取瓦片边界框，从LAZ文件中加载块点）；2)生成粗糙高程图（将地图分成补丁，使用块点构建插值高程图）；3)神经网络优化（使用小型神经网络优化粗糙高程图）；4)用户交互和动态更新（优先处理用户视角区域，放大时加载完整分辨率数据）；5)渲染（使用CUDA软件光栅化器渲染点云和高程图）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)交互式点云查看器，无需预处理和额外磁盘空间；2)从压缩点云中高效提取稀疏子样本；3)使用神经网络从稀疏样本预测高质量高程图；4)实现直接核心渲染方法。相比之前的工作，LidarScOUT的不同之处在于：无需预处理（传统方法需数小时到数天）；能处理任意大的数据集（而不仅限于内存容量）；优先处理用户视角区域（而非未定义顺序）；在稀疏样本重建高程图方面表现更好；能直接从压缩LAZ文件访问块点，提高效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LidarScOUT通过直接从压缩点云数据中快速提取稀疏子样本并利用神经网络生成高质量高程图，实现了大规模点云数据的即时可视化，无需预处理或额外存储空间。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.2312/hpg.20251170&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale terrain scans are the basis for many important tasks, such astopographic mapping, forestry, agriculture, and infrastructure planning. Theresulting point cloud data sets are so massive in size that even basic taskslike viewing take hours to days of pre-processing in order to createlevel-of-detail structures that allow inspecting the data set in their entiretyin real time. In this paper, we propose a method that is capable of instantlyvisualizing massive country-sized scans with hundreds of billions of points.Upon opening the data set, we first load a sparse subsample of points andinitialize an overview of the entire point cloud, immediately followed by asurface reconstruction process to generate higher-quality, hole-freeheightmaps. As users start navigating towards a region of interest, we continueto prioritize the heightmap construction process to the user's viewpoint. Oncea user zooms in closely, we load the full-resolution point cloud data for thatregion and update the corresponding height map textures with thefull-resolution data. As users navigate elsewhere, full-resolution point datathat is no longer needed is unloaded, but the updated heightmap textures areretained as a form of medium level of detail. Overall, our method constitutes aform of direct out-of-core rendering for massive point cloud data sets(terabytes, compressed) that requires no preprocessing and no additional diskspace. Source code, executable, pre-trained model, and dataset are availableat: https://github.com/cg-tuwien/lidarscout</description>
      <author>example@mail.com (Philipp Erler, Lukas Herzberger, Michael Wimmer, Markus Schütz)</author>
      <guid isPermaLink="false">2509.20198v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping</title>
      <link>http://arxiv.org/abs/2509.20081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于截断符号距离场(TSDF)的高效、仅CPU的体积映射框架，能够实现实时3D重建。&lt;h4&gt;背景&lt;/h4&gt;现有的TSDF/ESDF方法大多依赖GPU加速，而本研究旨在开发一种完全在CPU上运行的体积映射方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效、仅CPU的体积映射框架，能够在不牺牲运行时性能的情况下实现高分辨率3D重建。&lt;h4&gt;方法&lt;/h4&gt;使用基于方向位掩码的集成方案，将原始激光雷达点云数据增量融合到体素网格中，生成TSDF表示。&lt;h4&gt;主要发现&lt;/h4&gt;处理每个点云的时间保持恒定，不随体素网格分辨率变化；完全在CPU上运行的方法能够达到与GPU加速方法相当的速度；生成的地图精度与当代映射技术相当。&lt;h4&gt;结论&lt;/h4&gt;该CPU-only的体积映射框架能够实现高效、高精度的3D重建，不依赖GPU加速，同时保持实时性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于截断符号距离场(TSDF)的高效、仅CPU的体积映射框架。该系统使用基于方向位掩码的集成方案，将原始激光雷达点云数据增量融合到体素网格中，生成适合实时3D重建的密集且一致的TSDF表示。该方法的一个关键特点是每个点云的处理时间保持恒定，无论体素网格分辨率如何，能够在不牺牲运行时性能的情况下实现高分辨率映射。与大多数依赖GPU加速的最新TSDF/ESDF方法不同，该方法完全在CPU上运行，在速度上取得了具有竞争力的结果。在真实世界开放数据集上的实验表明，生成的地图精度与当代映射技术相当。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在仅使用CPU的情况下实现高效高分辨率体积映射的问题。当前大多数TSDF/ESDF方法严重依赖GPU加速，在CPU上运行时计算成本会随着地图分辨率增加而显著增加，这限制了在资源受限的机器人平台上的应用。这个问题很重要，因为许多机器人平台可能没有GPU资源或GPU需要用于其他任务，而CPU-only解决方案可以更广泛地部署在各种平台上。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了D-LIO框架中的Truncated Distance Field(TDF)映射后端，但进行了重要改进。他们观察到LiDAR返回的各向异性更新模式，包括每个测量表面点后面的阴影区域，因此设计了方向性核来建模这种模式。作者使用位掩码编码来简化距离计算和更新过程，实现恒定时间操作。他们扩展了原始TDF表示，从无符号变为有符号距离，引入方向性证据积累，并优化内存布局以提高缓存效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于方向性位掩码的截断符号距离场(TSDF)表示，结合预计算的方向性核，实现高效的CPU体积映射。每个体素存储三个字段：距离掩码(32位)、符号标志(1位)和命中计数器(8位)。实现流程：1)初始化体素网格；2)对每个LiDAR点，变换到全局坐标系；3)根据点的方位和仰角选择预计算的方向性核；4)通过按位AND操作更新体素的距离掩码；5)对核阴影区域中的体素递增命中计数器；6)当计数器超过阈值时更新符号位；7)并行处理多个点以提高效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)方向性位掩码表示，仅需8字节/体素；2)方向性核，建模各向异性更新和阴影区域；3)恒定时间更新，处理时间与地图分辨率无关；4)完全CPU实现，不依赖GPU；5)有符号距离表示，明确区分自由和占用空间。相比之前的工作不同之处在于：大多数TSDF方法依赖GPU加速，而DB-TSDF完全在CPU上运行；传统方法计算成本随分辨率增加，而DB-TSDF保持恒定；使用方向性核而非各向同性更新；使用位掩码编码简化更新过程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DB-TSDF提出了一种基于方向性位掩码的CPU高效TSDF体积映射方法，实现了与GPU方法相媲美的精度，同时保持恒定的计算成本，使其适合资源受限的机器人平台。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a high-efficiency, CPU-only volumetric mapping frameworkbased on a Truncated Signed Distance Field (TSDF). The system incrementallyfuses raw LiDAR point-cloud data into a voxel grid using a directionalbitmask-based integration scheme, producing dense and consistent TSDFrepresentations suitable for real-time 3D reconstruction. A key feature of theapproach is that the processing time per point-cloud remains constant,regardless of the voxel grid resolution, enabling high resolution mappingwithout sacrificing runtime performance. In contrast to most recent TSDF/ESDFmethods that rely on GPU acceleration, our method operates entirely on CPU,achieving competitive results in speed. Experiments on real-world open datasetsdemonstrate that the generated maps attain accuracy on par with contemporarymapping techniques.</description>
      <author>example@mail.com (Jose E. Maese, Luis Merino, Fernando Caballero)</author>
      <guid isPermaLink="false">2509.20081v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>An Overview of Meshfree Collocation Methods</title>
      <link>http://arxiv.org/abs/2509.20056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  55 pages, 259 references, Supplementary Material&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提供了无网格配置方法的全面概述，用于在非结构化点云上数值逼近微分算子。&lt;h4&gt;背景&lt;/h4&gt;无网格配置方法不需要计算网格或网格，而是在可能不规则分布的配置点(粒子)上逼近光滑函数及其导数到所需的一致性阶数。&lt;h4&gt;目的&lt;/h4&gt;回顾文献中的无网格配置方法，追踪关键概念的历史发展，提出方法分类，统一表述这些方法，并提出未来广义推导。&lt;h4&gt;方法&lt;/h4&gt;回顾文献中的无网格配置方法，根据推导原理进行分类，提出统一表述，展示每种方法如何从统一表述中推导，提出广义推导方法。&lt;h4&gt;主要发现&lt;/h4&gt;许多无网格配置方法之间存在微妙但重要的差异，这些差异通过统一表述变得明显。&lt;h4&gt;结论&lt;/h4&gt;提出了无网格配置方法的统一表述和广义推导，为未来研究提供了框架。&lt;h4&gt;翻译&lt;/h4&gt;我们提供了关于在连续标记的非结构化点云上数值逼近微分算子的无网格配置方法的全面概述。无网格配置方法不需要计算网格或网格。相反，它们在可能不规则分布的配置点(通常称为粒子)上逼近光滑函数及其导数，达到所需的一致性阶数。我们从文献中回顾了几种无网格配置方法，追踪了关键概念的历史发展，并根据推导原理提出了方法分类。尽管我们回顾的一些方法相似或相同，但许多方法之间存在微妙但重要的差异，我们强调了这些差异并进行了讨论。我们提出了无网格配置方法的统一表述，使这些差异变得明显，并展示了每种方法如何从这种表述中推导出来。最后，我们提出了未来无网格配置方法的广义推导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We provide a comprehensive overview of meshfree collocation methods fornumerically approximating differential operators on continuously labeledunstructured point clouds. Meshfree collocation methods do not require acomputational grid or mesh. Instead, they approximate smooth functions andtheir derivatives at potentially irregularly distributed collocation points,often called particles, to a desired order of consistency. We review severalmeshfree collocation methods from the literature, trace the historicaldevelopment of key concepts, and propose a classification of methods accordingto their principle of derivation. Although some of the methods reviewed aresimilar or identical, there are subtle yet important differences between many,which we highlight and discuss. We present a unifying formulation of meshfreecollocation methods that renders these differences apparent and show how eachmethod can be derived from this formulation. Finally, we propose a generalizedderivation for meshfree collocation methods going forward.</description>
      <author>example@mail.com (Tomas Halada, Serhii Yaskovets, Abhinav Singh, Ludek Benes, Pratik Suchde, Ivo F. Sbalzarini)</author>
      <guid isPermaLink="false">2509.20056v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Generalist Robot Manipulation beyond Action Labeled Data</title>
      <link>http://arxiv.org/abs/2509.19958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Conference on Robot Learning 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种创新的机器人学习方法，通过利用无标签的人类和机器人演示数据，结合3D动态预测和自监督技术，使机器人能够在没有动作标签的情况下学习新任务，并在真实世界和模拟环境中表现出良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;最近通用机器人操作技术的进步利用了预训练的视觉语言模型和大规模机器人演示数据，能够以零样本方式处理多样化任务。然而，扩展高质量、带动作标签的机器人演示数据仍然是一个关键挑战，现有方法依赖这些数据来获得鲁棒性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，利用没有动作标签的视频(包含人类和/或机器人的动作)，增强开放词汇性能，实现新任务的数据高效学习。&lt;h4&gt;方法&lt;/h4&gt;在手部或夹持器位置提取密集的动态3D点云，使用提出的3D动态预测器进行自监督，然后使用较小的带标签数据集对预测器进行微调，以实现动作对齐。&lt;h4&gt;主要发现&lt;/h4&gt;该方法不仅能够从无标签的人类和机器人演示中学习，改进下游通用机器人策略，还使机器人能够在没有动作标签的情况下学习新任务(即动作外泛化)，在真实世界和模拟环境中都有效。&lt;h4&gt;结论&lt;/h4&gt;该方法解决了高质量、带动作标签的机器人演示数据扩展的挑战，通过利用无标签视频数据，实现了更高效的机器人学习。&lt;h4&gt;翻译&lt;/h4&gt;最近通用机器人操作技术的进步利用了预训练的视觉语言模型和大规模机器人演示数据，以零样本方式处理多样化任务。一个关键挑战仍然存在：扩展高质量、带动作标签的机器人演示数据，现有方法依赖这些数据来获得鲁棒性和泛化能力。为解决这一问题，我们提出了一种方法，利用没有动作标签的视频(包含人类和/或机器人的动作)，增强开放词汇性能，并实现新任务的数据高效学习。我们的方法在手部或夹持器位置提取密集的动态3D点云，并使用提出的3D动态预测器进行自监督。然后使用较小的带标签数据集对预测器进行微调，以实现动作对齐。我们证明，我们的方法不仅能够从无标签的人类和机器人演示中学习，改进下游通用机器人策略，还使机器人能够在没有动作标签的情况下学习新任务(即动作外泛化)，在真实世界和模拟环境中都有效。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从没有动作标签的人类和机器人演示视频中学习通用机器人操作能力的问题。这个问题很重要，因为现有方法依赖大量带有精确动作标签的机器人演示数据，收集这些数据成本高昂且难以扩展；而互联网上有丰富的人类操作视频资源却未被充分利用；此外，现有方法在训练分布外的任务上表现不佳，限制了机器人在真实世界中的泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有VLA模型在领域外泛化能力有限，并注意到互联网上有大量无标签人类操作视频蕴含丰富操作知识。他们借鉴了Vision-Language Models（特别是Paligemma）的语义理解能力、自监督学习思想、flow matching技术和Transformer架构。创新点在于将这些技术结合，设计了两阶段训练框架：先从无标签视频中学习通用运动表征，再在有标签数据上进行动作对齐。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用动态点云作为人类和机器人操作的通用表征，通过两阶段训练方法学习通用操作能力。第一阶段从无标签视频中提取手部或夹爪的动态点云序列，训练3D动态预测器；第二阶段在有标签机器人数据上训练动作预测器，将动态点云与机器人动作对齐。推理时，模型处理视觉和语言输入，预测机器人动作序列，通过欧拉积分计算具体控制命令。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) MotoVLA，第一个能利用无标签数据学习通用机器人操作的端到端VLA模型；2) 两阶段训练方法，使用动态点云作为通用表征；3) 首次实现从无标签人类演示到机器人操作的直接迁移。相比之前工作，本文方法不依赖大规模有标签机器人数据，突破了特定任务和小规模策略的限制，避免了逆向动力学模型的需求，解决了重定向方法中的领域差距问题，是首个将无标签数据用于端到端通用VLA架构的研究。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了MotoVLA，一种通过两阶段训练方法利用无标签人类和机器人演示视频学习通用机器人操作能力的端到端视觉-语言-行动模型，显著提升了机器人在领域内和领域外任务上的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in generalist robot manipulation leverage pre-trainedVision-Language Models (VLMs) and large-scale robot demonstrations to tacklediverse tasks in a zero-shot manner. A key challenge remains: scalinghigh-quality, action-labeled robot demonstration data, which existing methodsrely on for robustness and generalization. To address this, we propose a methodthat benefits from videos without action labels - featuring humans and/orrobots in action - enhancing open-vocabulary performance and enablingdata-efficient learning of new tasks. Our method extracts dense, dynamic 3Dpoint clouds at the hand or gripper location and uses a proposed 3D dynamicspredictor for self-supervision. This predictor is then tuned to an actionpredictor using a smaller labeled dataset for action alignment. We show thatour method not only learns from unlabeled human and robot demonstrations -improving downstream generalist robot policies - but also enables robots tolearn new tasks without action labels (i.e., out-of-action generalization) inboth real-world and simulated settings.</description>
      <author>example@mail.com (Alexander Spiridonov, Jan-Nico Zaech, Nikolay Nikolov, Luc Van Gool, Danda Pani Paudel)</author>
      <guid isPermaLink="false">2509.19958v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar</title>
      <link>http://arxiv.org/abs/2509.19644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了使用更高容量的分割骨干网络改进从4D雷达生成类LiDAR点云的质量，发现最优分割骨干可比当前最先进方法提高23.7%性能。&lt;h4&gt;背景&lt;/h4&gt;LiDAR能提供密集、精确的点云表示，实现准确感知并提高道路安全，但其高成本限制了高级自动驾驶系统在商业车辆中的广泛应用。&lt;h4&gt;目的&lt;/h4&gt;研究更高容量的分割骨干网络对生成点云质量的影响，探索如何在不使用LiDAR的情况下生成类似LiDAR的3D点云。&lt;h4&gt;方法&lt;/h4&gt;使用神经网络，以LiDAR点云作为地面真实值(GT)，仅使用4D雷达生成类似LiDAR的3D点云，研究分割骨干网络的效果，使用RaDelft数据集进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;容量极高的模型实际上可能损害性能，而最优的分割骨干网络可以比当前最先进(SOTA)方法提高23.7%的性能。&lt;h4&gt;结论&lt;/h4&gt;通过选择适当的分割骨干网络，可以显著提高从4D雷达生成类LiDAR点云的质量，有助于降低自动驾驶系统的成本。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR对周围环境的密集、精确点云表示能够实现准确感知，并通过提供更大的场景感知和理解能力显著提高道路安全。然而，LiDAR的高成本继续限制高级自动驾驶系统在商业可用车辆中的广泛采用。先前的研究已经取得了进展，通过训练神经网络（使用LiDAR点云作为地面真实值）来绕过对LiDAR的需求，仅使用4D雷达生成类似LiDAR的3D点云。最好的例子之一是一个创建的神经网络，它使用模块化的二维卷积神经网络(CNN)骨干网络和一个以时间一致性网络为核心，使用RaDelff数据集进行训练(见arXiv:2406.04723)。在这项工作中，我们研究了更高容量的分割骨干网络对生成点云质量的影响。我们的结果表明，虽然容量极高的模型实际上可能损害性能，但最优的分割骨干网络可以比当前最先进(SOTA)方法提供23.7%的改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何提高4D雷达生成的点云质量问题，使其能够替代昂贵的激光雷达。这个问题很重要，因为激光雷达虽然能提供高质量的环境点云，但价格昂贵（中端约4000美元，高端可达70000美元），限制了自动驾驶技术在商业车辆中的广泛应用。4D雷达成本低且性能有竞争力，但生成的点云更稀疏、噪声更大。如果能通过神经网络处理4D雷达数据生成高质量点云，就能大幅降低自动驾驶系统成本，促进其商业化应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于现有工作进行思考和设计。他们借鉴了之前使用深度学习改进4D雷达成像质量的研究，特别是[1]中的工作，该工作使用包含2D卷积神经网络分割骨干网络和时间一致性网络的神经网络。作者在此基础上，研究了更高容量的分割骨干网络（ResNet50、101、152）对点云质量的影响，并测试了不同数量的3D卷积层在时间一致性网络中的作用。作者使用了焦点损失函数进行训练，并调整了训练超参数以适应更大容量的模型，包括增加训练周期、使用正则化防止学习噪声、调整批量大小等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过优化分割骨干网络和时间一致性网络的结构，提高4D雷达生成的点云质量，使其更接近激光雷达生成的点云。整体流程包括：1)使用RaDelft数据集，包含雷达数据和对应激光雷达点云；2)构建网络架构，包括多普勒编码器、分割骨干网络（不同容量ResNet模型）和时间一致性网络（不同数量3D卷积层）；3)使用焦点损失函数训练模型，调整超参数适应不同容量模型；4)使用检测概率、虚警概率和双向Chamfer距离评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统研究了不同容量分割骨干网络对点云质量的影响；2)发现了时间一致性网络中3D卷积层数量的最优配置（4层）；3)发现更大容量模型容易学习噪声导致性能下降；4)证明ResNet50结合4层时间一致性网络可提高23.7%的性能。相比之前工作[1]，本文不仅研究了时间维度改进，还深入探索了分割骨干网络的选择和容量对性能的影响，并找到了最优组合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过实验发现，使用中等容量的ResNet50作为分割骨干网络并结合4层时间一致性网络，可以显著提高4D雷达生成的点云质量，比现有方法提升23.7%，为低成本自动驾驶系统提供了更可靠的感知方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR's dense, sharp point cloud (PC) representations of the surroundingenvironment enable accurate perception and significantly improve road safety byoffering greater scene awareness and understanding. However, LiDAR's high costcontinues to restrict the broad adoption of high-level Autonomous Driving (AD)systems in commercially available vehicles. Prior research has shown progresstowards circumventing the need for LiDAR by training a neural network, usingLiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point cloudsusing only 4D Radars. One of the best examples is a neural network created totrain a more efficient radar target detector with a modular 2D convolutionalneural network (CNN) backbone and a temporal coherence network at its core thatuses the RaDelft dataset for training (see arXiv:2406.04723). In this work, weinvestigate the impact of higher-capacity segmentation backbones on the qualityof the produced point clouds. Our results show that while very high-capacitymodels may actually hurt performance, an optimal segmentation backbone canprovide a 23.7% improvement over the state-of-the-art (SOTA).</description>
      <author>example@mail.com (William L. Muckelroy III, Mohammed Alsakabi, John M. Dolan, Ozan K. Tonguz)</author>
      <guid isPermaLink="false">2509.19644v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Human-Interpretable Uncertainty Explanations for Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2509.18786v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为高斯过程概念归因（GP-CA）的新方法，用于解决点云配准中的不确定性问题。该方法不仅能够量化配准不确定性，还能通过将不确定性归因于已知的误差来源来解释不确定性，并利用主动学习发现新的不确定性来源。实验表明，GP-CA在运行时间、样本效率和准确性方面优于其他最先进方法，并能实现有效的故障恢复行为，提高机器人感知的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;在点云配准问题中，知名方法如ICP在传感器噪声、姿态估计误差和由遮挡引起的部分重叠等不确定性情况下表现不佳，需要一种能够处理这些不确定性的新方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种不仅能量化配准不确定性，还能通过将不确定性归因于配准问题中已知的误差来源来解释不确定性的方法，同时利用主动学习发现新的不确定性来源。&lt;h4&gt;方法&lt;/h4&gt;提出高斯过程概念归因（GP-CA）方法，结合主动学习技术，通过查询信息量丰富的实例来发现实际环境中的新不确定性来源。&lt;h4&gt;主要发现&lt;/h4&gt;GP-CA在三个公开数据集和真实机器人实验中得到了验证；与最先进方法相比，在运行时间、主动学习的高样本效率和准确性方面表现更好；真实实验展示了其适用性；能够实现有效的故障恢复行为，提高机器人感知的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GP-CA是一种有效的点云配准方法，能够在不确定性情况下表现良好，不仅能够量化不确定性，还能解释不确定性来源，并通过主动学习发现新的不确定性来源，从而提高机器人感知的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们解决了点云配准问题，其中知名方法如ICP在传感器噪声、姿态估计误差和由遮挡引起的部分重叠等不确定性情况下表现不佳。我们开发了一种新方法——高斯过程概念归因（GP-CA），它不仅量化配准不确定性，还通过将不确定性归因于配准问题中已知的误差来源来解释不确定性。我们的方法利用主动学习通过查询信息量丰富的实例来发现实际环境中的新不确定性来源。我们在三个公开可用数据集和我们的真实机器人实验中验证了GP-CA。大量的消融实验证实了我们的设计选择。我们的方法在运行时间、主动学习的高样本效率和准确性方面优于其他最先进方法。我们的真实实验清楚地展示了其适用性。我们的视频还表明，GP-CA能够实现有效的故障恢复行为，从而提供更强大的机器人感知能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准中的不确定性解释问题。在机器人感知任务中，如SLAM、3D重建和物体姿态估计，ICP等常用配准方法在传感器噪声、姿态估计误差和部分重叠等情况下容易失败。虽然现有方法能量化不确定性，但很少能解释不确定性产生的原因，这使得机器人无法理解为什么配准失败以及如何采取适当的恢复行动，限制了实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云配准中的不确定性问题，并指出现有不确定性量化方法只提供大小而不提供原因，而现有可解释AI方法计算量大且不适合实时应用。作者借鉴了三方面工作：1)使用DGCNN进行3D点云表示学习；2)应用高斯过程进行分类；3)采用BALD进行主动学习。基于这些，作者设计了GP-CA方法，通过将点云编码为潜在向量，再映射到不同不确定性概念的概率，并集成主动学习机制来发现新不确定性来源。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云配准中的不确定性归因于人类可理解的概念(如传感器噪声、姿态误差、部分重叠等)，使机器人不仅知道不确定性大小，还知道原因并采取针对性恢复行动。整体流程：1)使用ICP对齐点云；2)用DGCNN将对齐点云编码为潜在向量；3)用高斯过程分类器映射到不同概念的概率；4)选择概率最高的概念作为主要不确定性来源；5)当不确定性高时，通过BALD标准选择样本进行标注并更新模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次实现点云配准中人类可理解的不确定性解释；2)将不确定性归因于特定语义概念；3)集成主动学习机制适应新不确定性来源；4)实现高效的实时应用。相比之前工作：与传统不确定性量化方法不同，它不仅提供大小还提供原因；与SHAP/SA相比，计算效率更高且针对点云配准优化；与TCAV相比，不依赖神经网络且支持在线整合新概念；与纯监督方法相比，减少标注数据需求并能适应新环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的GP-CA方法首次实现了点云配准中人类可理解的不确定性解释，通过将不确定性归因于语义概念并集成主动学习，使机器人能理解配准失败原因并采取针对性恢复行动，显著提高了机器人感知系统的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we address the point cloud registration problem, wherewell-known methods like ICP fail under uncertainty arising from sensor noise,pose-estimation errors, and partial overlap due to occlusion. We develop anovel approach, Gaussian Process Concept Attribution (GP-CA), which not onlyquantifies registration uncertainty but also explains it by attributinguncertainty to well-known sources of errors in registration problems. Ourapproach leverages active learning to discover new uncertainty sources in thewild by querying informative instances. We validate GP-CA on three publiclyavailable datasets and in our real-world robot experiment. Extensive ablationssubstantiate our design choices. Our approach outperforms otherstate-of-the-art methods in terms of runtime, high sample-efficiency withactive learning, and high accuracy. Our real-world experiment clearlydemonstrates its applicability. Our video also demonstrates that GP-CA enableseffective failure-recovery behaviors, yielding more robust robotic perception.</description>
      <author>example@mail.com (Johannes A. Gaus, Loris Schneider, Yitian Shi, Jongseok Lee, Rania Rayyes, Rudolph Triebel)</author>
      <guid isPermaLink="false">2509.18786v2</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Process-Informed Forecasting of Complex Thermal Dynamics in Pharmaceutical Manufacturing</title>
      <link>http://arxiv.org/abs/2509.20349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于制药冷冻干燥过程中温度预测的过程信息引导预测(PIF)模型，解决了深度学习模型在物理一致性和鲁棒性方面的局限性，提高了预测的准确性、物理合理性和噪声鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;复杂物理系统的时间序列预测是现代工业监控和控制的基础，尽管深度学习模型在捕捉复杂动态方面表现出色，但其部署受到物理一致性和鲁棒性问题的限制，影响了在受控环境中的可靠性。&lt;h4&gt;目的&lt;/h4&gt;开发一种过程信息引导预测(PIF)模型，用于制药冷冻干燥过程中的温度预测，提高预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;研究从经典模型(ARIMA、ETS)到现代深度学习架构(KANs)的多种模型；比较三种整合过程信息轨迹先验的损失函数形式(固定权重损失、动态不确定性损失、残差注意力机制)；评估模型的准确性、物理一致性和传感器噪声鲁棒性；在新过程的迁移学习场景中测试最佳模型的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;PIF模型在准确性、物理合理性和噪声鲁棒性方面均优于传统的数据驱动模型。&lt;h4&gt;结论&lt;/h4&gt;这项工作为制药制造领域关键应用开发可靠且可泛化的预测解决方案提供了路线图。&lt;h4&gt;翻译&lt;/h4&gt;复杂物理系统的准确时间序列预测是现代工业监控和控制的基础。虽然深度学习模型在捕捉复杂动态方面表现出色，但目前由于物理一致性和鲁棒性问题，其部署受到限制，从而限制了它们在受控环境中的可靠性。我们针对制药冷冻干燥过程中的温度引入了过程信息引导(PIF)预测模型。我们研究了从经典模型(如自回归积分移动平均模型ARIMA和指数平滑模型ETS)到现代深度学习架构(包括Kolmogorov-Arnold网络KANs)的广泛模型。我们比较了三种不同的整合过程信息轨迹先验的损失函数形式：固定权重损失、基于动态不确定性的损失和基于残差的注意力(RBA)机制。我们不仅从准确性和物理一致性角度，还从对传感器噪声的鲁棒性角度评估所有模型。此外，我们在新过程的迁移学习场景中测试了最佳模型的实际泛化能力。我们的结果表明，PIF模型在准确性、物理合理性和噪声鲁棒性方面优于其数据驱动的对应模型。这项工作为在制药制造领域的关键应用中开发可靠且可泛化的预测解决方案提供了路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate time-series forecasting for complex physical systems is the backboneof modern industrial monitoring and control. While deep learning models excelat capturing complex dynamics, currently, their deployment is limited due tophysical inconsistency and robustness, hence constraining their reliability inregulated environments. We introduce process-informed forecasting (PIF) modelsfor temperature in pharmaceutical lyophilization. We investigate a wide rangeof models, from classical ones such as Autoregressive Integrated Moving AverageModel (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learningarchitectures, including Kolmogorov-Arnold Networks (KANs). We compare threedifferent loss function formulations that integrate a process-informedtrajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and aResidual-Based Attention (RBA) mechanism. We evaluate all models not only foraccuracy and physical consistency but also for robustness to sensor noise.Furthermore, we test the practical generalizability of the best model in atransfer learning scenario on a new process. Our results show that PIF modelsoutperform their data-driven counterparts in terms of accuracy, physicalplausibility and noise resilience. This work provides a roadmap for developingreliable and generalizable forecasting solutions for critical applications inthe pharmaceutical manufacturing landscape.</description>
      <author>example@mail.com (Ramona Rubini, Siavash Khodakarami, Aniruddha Bora, George Em Karniadakis, Michele Dassisti)</author>
      <guid isPermaLink="false">2509.20349v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning in Regression with Influential Points</title>
      <link>http://arxiv.org/abs/2509.20272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对迁移学习中影响点导致的模型性能下降问题，提出了Trans-CO框架，通过影响点检测和回归模型拟合的协同优化，提高了模型在存在影响点情况下的预测性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;回归预测在实际应用中至关重要，但依赖于数据标注。由于标注成本高或领域特定约束，目标域中的标记数据通常稀缺，因此迁移学习成为利用源域知识解决这一问题的重要方法。&lt;h4&gt;目的&lt;/h4&gt;创新性地引入一种迁移学习协同优化(Trans-CO)框架，用于影响点检测和回归模型拟合，解决迁移学习中影响点导致的性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出Trans-CO算法，通过协同优化进行影响点检测和回归模型拟合，以应对迁移学习中的鲁棒性挑战。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的模拟实验表明，Trans-CO算法在模型拟合性能和影响点识别准确性方面优于竞争方法，在真实数据集上也实现了优越的预测准确性。&lt;h4&gt;结论&lt;/h4&gt;Trans-CO为存在影响点的回归迁移学习提供了新的解决方案，提高了模型在目标域中的预测性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;回归预测在实际应用中起着至关重要的作用，并强烈依赖于数据标注。然而，由于高昂的标注成本或领域特定的约束，目标域中的标记数据通常稀缺，这使得迁移学习成为通过利用资源丰富的源域知识来解决这一问题的关键方案。在实际目标场景中，尽管迁移学习已被广泛应用，但影响点会显著扭曲目标域模型的参数估计。当源域中也存在影响点时，这一问题会进一步加剧，导致性能下降，并对现有迁移学习框架的鲁棒性提出关键挑战。在本研究中，我们创新性地引入了一种用于影响点检测和回归模型拟合的迁移学习协同优化(Trans-CO)框架。广泛的模拟实验表明，所提出的Trans-CO算法在模型拟合性能和影响点识别准确性方面优于竞争方法。此外，它在真实数据集上实现了优越的预测准确性，为存在影响点的回归迁移学习提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Regression prediction plays a crucial role in practical applications andstrongly relies on data annotation. However, due to prohibitive annotationcosts or domain-specific constraints, labeled data in the target domain isoften scarce, making transfer learning a critical solution by leveragingknowledge from resource-rich source domains. In the practical target scenario,although transfer learning has been widely applied, influential points cansignificantly distort parameter estimation for the target domain model. Thisissue is further compounded when influential points are also present in sourcedomains, leading to aggravated performance degradation and posing criticalrobustness challenges for existing transfer learning frameworks. In this study,we innovatively introduce a transfer learning collaborative optimization(Trans-CO) framework for influential point detection and regression modelfitting. Extensive simulation experiments demonstrate that the proposedTrans-CO algorithm outperforms competing methods in terms of model fittingperformance and influential point identification accuracy. Furthermore, itachieves superior predictive accuracy on real-world datasets, providing a novelsolution for transfer learning in regression with influential points</description>
      <author>example@mail.com (Bingbing Wang, Jiaqi Wang, Yu Tang)</author>
      <guid isPermaLink="false">2509.20272v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks</title>
      <link>http://arxiv.org/abs/2509.20209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This submission is 8 pages long, includes 4 tables, and contains all  required conference details&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用迁移学习技术提高低资源语言提格里尼亚语的神经机器翻译质量，通过整合语言特定分词、有信息嵌入初始化和领域自适应微调，并构建了高质量评估数据集。&lt;h4&gt;背景&lt;/h4&gt;尽管神经机器翻译取得进展，低资源语言如提格里尼亚语仍面临有限语料库、不充分分词策略和缺乏标准化评估基准等挑战。&lt;h4&gt;目的&lt;/h4&gt;研究使用多语言预训练模型的迁移学习技术，以提高形态丰富、低资源语言的翻译质量。&lt;h4&gt;方法&lt;/h4&gt;提出改进方法整合语言特定分词、有信息嵌入初始化和领域自适应微调；构建高质量人工对齐的英语-提格里尼亚语多领域评估数据集。&lt;h4&gt;主要发现&lt;/h4&gt;使用自定义分词器的迁移学习显著优于零样本基线，通过BLEU、chrF和人工评估验证；应用Bonferroni校正确保统计显著性；错误分析揭示关键局限性并指导改进。&lt;h4&gt;结论&lt;/h4&gt;语言感知建模和可重现基准对缩小代表性语言性能差距至关重要。&lt;h4&gt;翻译&lt;/h4&gt;尽管神经机器翻译取得了进展，但低资源语言如提格里尼亚语仍然面临持续挑战，包括有限的语料库、不充分的分词策略以及缺乏标准化的评估基准。本文研究使用多语言预训练模型的迁移学习技术，以提高形态丰富、低资源语言的翻译质量。我们提出了一种改进的方法，整合了语言特定的分词、有信息的嵌入初始化和领域自适应微调。为进行严格评估，我们构建了一个高质量、人工对齐的英语-提格里尼亚语评估数据集，涵盖多个领域。实验结果表明，使用自定义分词器的迁移学习显著优于零样本基线，这一优势通过BLEU、chrF和定性人工评估得到验证。应用了Bonferroni校正以确保不同配置间的统计显著性。错误分析揭示了关键局限性并指导了有针对性的改进。这项研究强调了语言感知建模和可重现基准在缩小代表性语言性能差距中的重要性。资源可在https://github.com/hailaykidu/MachineT_TigEng和https://huggingface.co/Hailay/MachineT_TigEng获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite advances in Neural Machine Translation (NMT), low-resource languageslike Tigrinya remain underserved due to persistent challenges, includinglimited corpora, inadequate tokenization strategies, and the lack ofstandardized evaluation benchmarks. This paper investigates transfer learningtechniques using multilingual pretrained models to enhance translation qualityfor morphologically rich, low-resource languages. We propose a refined approachthat integrates language-specific tokenization, informed embeddinginitialization, and domain-adaptive fine-tuning. To enable rigorous assessment,we construct a high-quality, human-aligned English-Tigrinya evaluation datasetcovering diverse domains. Experimental results demonstrate that transferlearning with a custom tokenizer substantially outperforms zero-shot baselines,with gains validated by BLEU, chrF, and qualitative human evaluation.Bonferroni correction is applied to ensure statistical significance acrossconfigurations. Error analysis reveals key limitations and informs targetedrefinements. This study underscores the importance of linguistically awaremodeling and reproducible benchmarks in bridging the performance gap forunderrepresented languages. Resources are available athttps://github.com/hailaykidu/MachineT_TigEng  and https://huggingface.co/Hailay/MachineT_TigEng</description>
      <author>example@mail.com (Hailay Kidu Teklehaymanot, Gebrearegawi Gidey, Wolfgang Nejdl)</author>
      <guid isPermaLink="false">2509.20209v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora and Machine Translation Systems</title>
      <link>http://arxiv.org/abs/2509.19941v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个大规模、高质量标注的平行语料库CorIL，涵盖11种印度语言，包含772,000个双语句子对，分类为政府、健康和一般三个领域，用于支持领域感知的机器翻译研究。&lt;h4&gt;背景&lt;/h4&gt;印度拥有超过120种主要语言和约1600种其他语言，宪法中承认22种预定语言。尽管多语言神经机器翻译取得进展，但印度语言的高质量平行语料仍然稀缺，特别是在不同领域。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模高质量平行语料库，支持领域感知的机器翻译研究，建立未来研究的基准，并提高印度语言高质量训练数据的可用性。&lt;h4&gt;方法&lt;/h4&gt;构建包含772,000个双语句子对的数据集，涵盖英语、泰卢固语、印地语、旁遮普语、奥里亚语、克什米尔语、信德语、多格拉语、卡纳达语、乌尔都语和古吉拉特语，并系统分类为三个领域。微调和评估了IndicTrans2、NLLB和BhashaVerse等先进NMT模型。&lt;h4&gt;主要发现&lt;/h4&gt;结果显示不同语言脚本有不同的性能模式，大规模多语言模型在波斯-阿拉伯语系上表现更佳，而其他模型在印度语系上更优秀。研究提供了详细的领域性能分析，揭示了领域敏感性和跨脚本迁移学习的特性。&lt;h4&gt;结论&lt;/h4&gt;通过公开发布CorIL，显著提高了印度语言高质量训练数据的可用性，为机器翻译研究社区提供了宝贵资源。&lt;h4&gt;翻译&lt;/h4&gt;印度的语言景观是世界上最具多样性的之一，包含120多种主要语言和约1600种其他语言，其中22种被印度宪法正式承认为预定语言。尽管多语言神经机器翻译最近取得了进展，但印度语言的高质量平行语料仍然稀缺，特别是在不同领域。在本文中，我们介绍了一个大规模、高质量标注的平行语料库，涵盖其中11种语言：英语、泰卢固语、印地语、旁遮普语、奥里亚语、克什米尔语、信德语、多格拉语、卡纳达语、乌尔都语和古吉拉特语，总共包含772,000个双语句子对。该数据集经过精心策划并系统分类为三个关键领域：政府、健康和一般，以支持领域感知的机器翻译研究并促进有效的领域适应。为了展示CorIL的效用并为未来研究建立强有力的基准，我们微调并评估了几种最先进的NMT模型，包括IndicTrans2、NLLB和BhashaVerse。我们的分析揭示了重要的性能趋势，并突显了该语料库在探测模型能力方面的价值。例如，结果显示基于语言脚本存在不同的性能模式，大规模多语言模型在波斯-阿拉伯语系（乌尔都语、信德语）上表现出优势，而其他模型在印度语系上表现出色。本文提供了详细的领域性能分析，对领域敏感性和跨脚本迁移学习提供了见解。通过公开发布CorIL，我们旨在显著提高印度语言高质量训练数据的可用性，并为机器翻译研究社区提供宝贵资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; India's linguistic landscape is one of the most diverse in the world,comprising over 120 major languages and approximately 1,600 additionallanguages, with 22 officially recognized as scheduled languages in the IndianConstitution. Despite recent progress in multilingual neural machinetranslation (NMT), high-quality parallel corpora for Indian languages remainscarce, especially across varied domains. In this paper, we introduce alarge-scale, high-quality annotated parallel corpus covering 11 of theselanguages : English, Telugu, Hindi, Punjabi, Odia, Kashmiri, Sindhi, Dogri,Kannada, Urdu, and Gujarati comprising a total of 772,000 bi-text sentencepairs. The dataset is carefully curated and systematically categorized intothree key domains: Government, Health, and General, to enable domain-awaremachine translation research and facilitate effective domain adaptation. Todemonstrate the utility of CorIL and establish strong benchmarks for futureresearch, we fine-tune and evaluate several state-of-the-art NMT models,including IndicTrans2, NLLB, and BhashaVerse. Our analysis reveals importantperformance trends and highlights the corpus's value in probing modelcapabilities. For instance, the results show distinct performance patternsbased on language script, with massively multilingual models showing anadvantage on Perso-Arabic scripts (Urdu, Sindhi) while other models excel onIndic scripts. This paper provides a detailed domain-wise performance analysis,offering insights into domain sensitivity and cross-script transfer learning.By publicly releasing CorIL, we aim to significantly improve the availabilityof high-quality training data for Indian languages and provide a valuableresource for the machine translation research community.</description>
      <author>example@mail.com (Soham Bhattacharjee, Mukund K Roy, Yathish Poojary, Bhargav Dave, Mihir Raj, Vandan Mujadia, Baban Gain, Pruthwik Mishra, Arafat Ahsan, Parameswari Krishnamurthy, Ashwath Rao, Gurpreet Singh Josan, Preeti Dubey, Aadil Amin Kak, Anna Rao Kulkarni, Narendra VG, Sunita Arora, Rakesh Balbantray, Prasenjit Majumdar, Karunesh K Arora, Asif Ekbal, Dipti Mishra Sharma)</author>
      <guid isPermaLink="false">2509.19941v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation</title>
      <link>http://arxiv.org/abs/2509.19918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;XL-CoGen是一种创新的跨语言代码生成系统，通过协调多智能体架构和基于数据驱动的桥接语言选择机制，显著提高了在多种编程语言上生成高质量代码的能力，特别是在训练数据有限的语言上表现优异。&lt;h4&gt;背景&lt;/h4&gt;现代软件系统建立在异构技术栈上，需要跨多种编程语言生成高质量代码。大型语言模型在自动化编程方面取得了进展，但在不同语言上的能力差异很大，特别是在训练数据有限的语言(如Rust、Perl、OCaml和Erlang)上表现不佳。当前解决方案仍然孤立地处理每种目标语言，错失了共享知识或利用跨语言模式重复出现的机会。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在不同编程语言间能力差异大的问题，打破现有解决方案孤立处理每种目标语言的局限，实现跨语言知识的共享和利用。&lt;h4&gt;方法&lt;/h4&gt;提出了XL-CoGen系统，采用协调的多智能体架构，整合了中间表示、代码生成、翻译和自动修复功能。创新点是基于数据驱动的机制选择桥接语言，通过经验推导的转移矩阵识别最佳中间语言，基于已证实的翻译成功率而非原始生成准确性。系统执行早期输出验证，迭代纠正错误，并重用中间工件作为后续翻译的上下文支架。&lt;h4&gt;主要发现&lt;/h4&gt;XL-CoGen取得了显著改进，比最强微调基线提高13个百分点，比现有单语言多智能体方法提高多达30个百分点。消融研究进一步证明，兼容性引导的桥接显著优于基于大型语言模型的启发式方法，确认了累积跨语言知识转移的价值。&lt;h4&gt;结论&lt;/h4&gt;XL-CoGen通过协调的多智能体架构有效解决了跨语言代码生成的挑战。数据驱动的桥接语言选择机制比传统方法更有效，跨语言知识共享和重用对提高代码生成质量至关重要。&lt;h4&gt;翻译&lt;/h4&gt;随着当今软件系统建立在异构技术栈上，跨多种编程语言生成高质量代码变得越来越重要。大型语言模型已经推动了自动化编程的发展，但它们在不同语言上的熟练程度差异很大，特别是对于那些训练数据有限的语言，如Rust、Perl、OCaml和Erlang。许多当前解决方案，包括语言特定的微调、多智能体协调、迁移学习和中间表示管道，仍然孤立地处理每种目标语言，错失了共享知识或利用重复出现的跨语言模式的机会。XL-CoGen通过协调的多智能体架构应对这一挑战，该架构整合了中间表示、代码生成、翻译和自动修复。其显著特点是用于选择桥接语言的数据驱动机制：经验推导的转移矩阵基于已证实的翻译成功率而非原始生成准确性来识别最佳中间语言。系统执行早期输出验证，迭代纠正错误，并重用中间工件作为后续翻译的上下文支架。大量实验表明，XL-CoGen取得了显著改进，比最强的微调基线提高了13个百分点，比现有的单语言多智能体方法提高了多达30个百分点。消融研究进一步证明，兼容性引导的桥接显著优于基于大型语言模型的启发式方法，确认了累积跨语言知识转移的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Producing high-quality code across multiple programming languages isincreasingly important as today's software systems are built on heterogeneousstacks. Large language models (LLMs) have advanced the state of automatedprogramming, yet their proficiency varies sharply between languages, especiallythose with limited training data such as Rust, Perl, OCaml, and Erlang. Manycurrent solutions including language-specific fine-tuning, multi-agentorchestration, transfer learning, and intermediate-representation pipelinesstill approach each target language in isolation, missing opportunities toshare knowledge or exploit recurring cross-language patterns.  XL-CoGen tackles this challenge with a coordinated multi-agent architecturethat integrates intermediate representation, code generation, translation, andautomated repair. Its distinguishing feature is a data-driven mechanism forselecting bridging languages: empirically derived transfer matrices identifythe best intermediate languages based on demonstrated translation successrather than raw generation accuracy. The system performs early outputvalidation, iteratively corrects errors, and reuses intermediate artifacts ascontextual scaffolds for subsequent translations.  Extensive experiments show that XL-CoGen yields notable improvements with 13percentage-point gains over the strongest fine-tuned baseline and as much as 30percentage points over existing single-language multi-agent methods. Ablationstudies further demonstrate that compatibility-guided bridging significantlyoutperforms LLM-based heuristics, confirming the value of cumulativecross-language knowledge transfer.</description>
      <author>example@mail.com (Micheline Bénédicte Moumoula, Serge Lionel Nikiema, Albérick Euraste Djire, Abdoul Kader Kabore, Jacques Klein, Tegawendé F. Bissyande)</author>
      <guid isPermaLink="false">2509.19918v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>SMILES-Inspired Transfer Learning for Quantum Operators in Generative Quantum Eigensolver</title>
      <link>http://arxiv.org/abs/2509.19715v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于文本表示的量子算子构建方法，用于在生成量子特征求解器(GQE)框架下实现不同分子系统间的知识转移，从而减少计算资源需求。&lt;h4&gt;背景&lt;/h4&gt;传统变分量子特征求解器(VQE)算法存在固有局限性。在量子化学中，广泛使用的UCCSD方法需要为不同分子系统构建不同量子算子，增加了计算成本。&lt;h4&gt;目的&lt;/h4&gt;利用不同分子系统间的相似性减少量子算子构建的计算成本，受SMILES表示方法启发，开发基于文本的UCCSD量子算子表示方法。&lt;h4&gt;方法&lt;/h4&gt;开发文本表示方法利用分子系统间固有的表示相似性，探索量子算子中的文本模式相似性，使用文本相似度度量建立迁移学习框架。&lt;h4&gt;主要发现&lt;/h4&gt;在朴素基线设置下，该方法在GQE范式中实现了不同分子系统间的知识转移，用于基态能量计算。&lt;h4&gt;结论&lt;/h4&gt;这一发现对分子基态能量的混合量子-经典计算具有重要意义，可显著减少计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;鉴于传统变分量子特征求解器(VQE)算法的固有局限性，将深度生成模型集成到混合量子-经典框架中，特别是生成量子特征求解器(GQE)，代表了一种有前景的创新方法。然而，以量子化学中广泛使用的UCCSD(具有单激发和双激发的酉耦合簇)为例，不同的分子系统需要构建不同的量子算子。考虑到不同分子的相似性，利用这种相似性构建量子算子可以显著降低计算成本。受计算化学中的SMILES表示方法启发，我们利用不同分子系统之间固有的表示相似性，开发了一种用于UCCSD量子算子的基于文本的表示方法。该框架探索量子算子中的文本模式相似性，并使用文本相似度度量来建立迁移学习框架。我们在朴素基线设置下的方法证明了在GQE范式中不同分子系统间的知识转移可用于基态能量计算。这一发现为分子基态能量的混合量子-经典计算带来了显著好处，大大降低了计算资源要求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given the inherent limitations of traditional Variational QuantumEigensolver(VQE) algorithms, the integration of deep generative models intohybrid quantum-classical frameworks, specifically the Generative QuantumEigensolver(GQE), represents a promising innovative approach. However, takingthe Unitary Coupled Cluster with Singles and Doubles(UCCSD) ansatz which iswidely used in quantum chemistry as an example, different molecular systemsrequire constructions of distinct quantum operators. Considering the similarityof different molecules, the construction of quantum operators utilizing thesimilarity can reduce the computational cost significantly. Inspired by theSMILES representation method in computational chemistry, we developed atext-based representation approach for UCCSD quantum operators by leveragingthe inherent representational similarities between different molecular systems.This framework explores text pattern similarities in quantum operators andemploys text similarity metrics to establish a transfer learning framework. Ourapproach with a naive baseline setting demonstrates knowledge transfer betweendifferent molecular systems for ground-state energy calculations within the GQEparadigm. This discovery offers significant benefits for hybridquantum-classical computation of molecular ground-state energies, substantiallyreducing computational resource requirements.</description>
      <author>example@mail.com (Zhi Yin, Xiaoran Li, Shengyu Zhang, Xin Li, Xiaojin Zhang)</author>
      <guid isPermaLink="false">2509.19715v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation</title>
      <link>http://arxiv.org/abs/2509.19602v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种渐进式任务特定多任务适应方法，用于解决参数高效微调在多任务学习中的任务干扰和负迁移问题。该方法通过在预训练模型中添加适配器模块，在初始层实现跨任务共享，在后续层实现任务特定学习，并使用基于梯度的任务相似性计算来优化任务分配。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调方法是将预训练模型适应到各种下游任务的有前景的解决方案，但在单任务学习中表现良好，扩展到多任务学习时会加剧任务干扰和负迁移等问题，这是由于可训练参数数量有限导致的。&lt;h4&gt;目的&lt;/h4&gt;解决多任务学习中的任务干扰和负迁移问题，提出一种新的参数高效多任务学习方法，能够在保持性能的同时减少可训练参数数量。&lt;h4&gt;方法&lt;/h4&gt;引入渐进式任务特定多任务适应方法，在预训练模型中添加适配器模块，这些模块在初始层跨所有任务共享，在后续层逐渐变得任务特定。此外，提出了一种基于梯度的任务相似性计算方法，用于将相似任务分配到共享的适配器模块，以最小化管道开销。&lt;h4&gt;主要发现&lt;/h4&gt;在PASCAL和NYUD-v2数据集上的实验表明，该方法优于完全微调的多任务模型，而只需要五分之一的可训练参数。相比单任务微调有更好的相对改进，同时减少了可训练参数数量，超越了当前参数高效多任务学习的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;渐进式任务特定多任务适应方法是一种有效的参数高效多任务学习解决方案，能够在减少可训练参数的同时保持或提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调方法已成为将预训练模型适应到各种下游任务的有前景的解决方案。虽然这些方法在单任务学习中表现良好，但将它们扩展到多任务学习会加剧常见的挑战，如任务干扰和负迁移，这是由于可训练参数数量有限。为解决这些问题，我们引入了渐进式任务特定多任务适应，这是一种用于多任务学习的新的参数高效方法。这种方法在预训练模型中引入适配器模块，使得这些模块在初始层跨所有任务共享，并在后续层逐渐变得更具任务特定性。其动机是通过允许在初始层跨所有任务进行迁移学习，并在预测头方向上实现任务特定学习，来减少任务之间的冲突。此外，我们提出了一种基于梯度的任务相似性计算方法，并利用这一度量将相似任务分配到共享的适配器模块。我们的任务相似性方法在管道中引入了最小的开销。我们通过将Swin Transformer适应于密集预测任务来评估我们的方法。在PASCAL和NYUD-v2数据集上的实验表明，我们的方法优于完全微调的多任务模型，而只需要五分之一的可训练参数。这种方法相比单任务微调有更好的相对改进，同时减少了可训练参数数量，并超越了当前参数高效多任务学习的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning methods have emerged as a promising solutionfor adapting pre-trained models to various downstream tasks. While thesemethods perform well in single-task learning, extending them to multi-tasklearning exacerbates common challenges, such as task interference and negativetransfer, due to the limited number of trainable parameters. To address theseissues, we introduce progressive task-specific multi-task adaptation, a novelparameter-efficient approach for multi-task learning. This approach introducesadapter modules in a pre-trained model such that these modules are sharedacross all tasks in the initial layers and become progressively moretask-specific in the later layers. The motivation is to reduce the conflictsamong tasks by allowing transfer learning across all tasks in the initiallayers and enabling task-specific learning toward the prediction heads.Additionally, we propose a gradient-based approach for computing tasksimilarity and use this measure to allocate similar tasks to the shared adaptermodules. Our task similarity method introduces minimal overhead in thepipeline. We evaluate our approach by adapting the Swin Transformer for denseprediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstratethat our approach outperforms a fully fine-tuned multi-task model whilerequiring only one-fifth of the trainable parameters. This approach achievesbetter relative improvement to single-task fine-tuning while reducing thenumber of trainable parameters and surpasses the current state-of-the-artmethods for parameter-efficient multi-task learning.</description>
      <author>example@mail.com (Neeraj Gangwar, Anshuka Rangi, Rishabh Deshmukh, Holakou Rahmanian, Yesh Dattatreya, Nickvash Kani)</author>
      <guid isPermaLink="false">2509.19602v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis</title>
      <link>http://arxiv.org/abs/2509.17729v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种测试双样本问题中条件分布相等性的通用框架，该框架基于神经网络生成方法和样本分割技术，将条件分布测试问题转化为无条件分布问题，并提出了两种特殊测试方法。理论上建立了最小最大下界，证明了测试方法的一致性和收敛速率，实验验证了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;该问题与协变量转移下的迁移学习密切相关，需要测试两个条件分布是否相等。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用框架来测试双样本问题中条件分布的相等性，特别是在协变量转移场景下。&lt;h4&gt;方法&lt;/h4&gt;构建基于神经网络生成方法和样本分割技术的框架，将条件分布测试转化为无条件分布问题，并提出了基于生成排列和生成分类精度的两种特殊测试方法。&lt;h4&gt;主要发现&lt;/h4&gt;在特定光滑性条件下建立了条件分布相等性统计推断的最小最大下界；生成排列测试及其修改版本可以达到这个下界；生成分类精度测试具有一致性；学习条件生成器具有收敛速率。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架和方法在理论上和实验上都证明了其有效性，为条件分布相等性测试提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种用于测试双样本问题中条件分布相等性的通用框架。这个问题与协变量转移下的迁移学习最为相关。我们的框架基于神经网络生成方法和样本分割技术，通过将条件分布测试问题转化为无条件分布问题而构建。我们引入了两种特殊测试：基于生成排列的条件分布相等性测试和基于生成分类精度的条件分布相等性测试。理论上，我们在特定的光滑性条件下，建立了测试两个条件分布相等性时统计推断的最小最大下界。我们证明，基于生成排列的条件分布相等性测试及其修改版本可以精确地或达到某些迭代对数因子地达到这个下界。此外，我们证明了基于生成分类精度的条件分布相等性测试的一致性。我们还通过推导与最近开发的偏置Rademacher复杂性和神经网络近似性质相关的新结果，建立了学习条件生成器的收敛速率。经验上，我们进行了包括合成数据集和两个真实数据集在内的数值研究，证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a general framework for testing the equality of theconditional distributions in a two-sample problem. This problem is mostrelevant to transfer learning under covariate shift. Our framework is built onneural network-based generative methods and sample splitting techniques bytransforming the conditional distribution testing problem into an unconditionalone. We introduce two special tests: the generative permutation-basedconditional distribution equality test and the generative classificationaccuracy-based conditional distribution equality test. Theoretically, weestablish a minimax lower bound for statistical inference in testing theequality of two conditional distributions under certain smoothness conditions.We demonstrate that the generative permutation-based conditional distributionequality test and its modified version can attain this lower bound precisely orup to some iterated logarithmic factor. Moreover, we prove the testingconsistency of the generative classification accuracy-based conditionaldistribution equality test. We also establish the convergence rate for thelearned conditional generator by deriving new results related to therecently-developed offset Rademacher complexity and approximation propertiesusing neural networks. Empirically, we conduct numerical studies includingsynthetic datasets and two real-world datasets, demonstrating the effectivenessof our approach.</description>
      <author>example@mail.com (Siming Zheng, Meifang Lan, Tong Wang, Yuanyuan Lin)</author>
      <guid isPermaLink="false">2509.17729v2</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization</title>
      <link>http://arxiv.org/abs/2509.15791v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了MS-UDG方法，通过学习最小充分语义表示来解决无监督域泛化问题，实现了在无类别和域标签情况下的高性能泛化。&lt;h4&gt;背景&lt;/h4&gt;深度学习的泛化能力在监督设置中已被广泛研究，但在无监督场景中研究较少。无监督域泛化（UDG）任务面临在没有类别标签的情况下区分语义与变化的挑战，而实际应用中域标签通常不可用。&lt;h4&gt;目的&lt;/h4&gt;将UDG形式化为学习最小充分语义表示的任务，该表示应保留增强视图间共享的所有语义信息（充分性），并最大化移除与语义无关的信息（最小性）。&lt;h4&gt;方法&lt;/h4&gt;提出最小充分UDG（MS-UDG）模型，通过基于InfoNCE的目标实现充分性，并采用两个互补组件促进最小性：新颖的语义-变化解纠缠损失和基于重建的机制来捕获足够的变异。&lt;h4&gt;主要发现&lt;/h4&gt;优化表示以实现充分性和最小性可以直接减少分布外风险；MS-UDG在流行的无监督域泛化基准上建立了新的最先进水平，一致优于现有的SSL和UDG方法，且在表示学习过程中不需要类别或域标签。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效地解决了UDG任务中的挑战，通过学习最小充分语义表示，模型能够在没有类别或域标签的情况下实现更好的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;深度学习的泛化能力在监督设置中已被广泛研究，但在无监督场景中研究较少。最近，无监督域泛化（UDG）任务被提出以增强使用常见无监督学习技术（如自监督学习SSL）训练的模型的泛化能力。UDG面临在没有类别标签的情况下区分语义与变化的挑战。尽管一些最近的方法使用域标签来解决这个问题，但在实际情况下这些域标签通常不可用。在本文中，我们将UDG形式化为学习最小充分语义表示的任务：一种保留增强视图间共享的所有语义信息（充分性）并最大化移除与语义无关的信息（最小性）的表示。我们从信息论角度理论上支持这些目标，证明优化表示以实现充分性和最小性可以直接减少分布外风险。实际上，我们通过最小充分UDG（MS-UDG）实现这一优化，这是一种可学习模型，通过整合（a）基于InfoNCE的目标来实现充分性；（b）两个互补组件来促进最小性：新颖的语义-变化解纠缠损失和基于重建的机制来捕获足够的变异。实验上，MS-UDG在流行的无监督域泛化基准上建立了新的最先进水平，一致优于现有的SSL和UDG方法，在表示学习过程中不需要类别或域标签。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generalization ability of deep learning has been extensively studied insupervised settings, yet it remains less explored in unsupervised scenarios.Recently, the Unsupervised Domain Generalization (UDG) task has been proposedto enhance the generalization of models trained with prevalent unsupervisedlearning techniques, such as Self-Supervised Learning (SSL). UDG confronts thechallenge of distinguishing semantics from variations without category labels.Although some recent methods have employed domain labels to tackle this issue,such domain labels are often unavailable in real-world contexts. In this paper,we address these limitations by formalizing UDG as the task of learning aMinimal Sufficient Semantic Representation: a representation that (i) preservesall semantic information shared across augmented views (sufficiency), and (ii)maximally removes information irrelevant to semantics (minimality). Wetheoretically ground these objectives from the perspective of informationtheory, demonstrating that optimizing representations to achieve sufficiencyand minimality directly reduces out-of-distribution risk. Practically, weimplement this optimization through Minimal-Sufficient UDG (MS-UDG), alearnable model by integrating (a) an InfoNCE-based objective to achievesufficiency; (b) two complementary components to promote minimality: a novelsemantic-variation disentanglement loss and a reconstruction-based mechanismfor capturing adequate variation. Empirically, MS-UDG sets a newstate-of-the-art on popular unsupervised domain-generalization benchmarks,consistently outperforming existing SSL and UDG methods, without category ordomain labels during representation learning.</description>
      <author>example@mail.com (Tan Pan, Kaiyu Guo, Dongli Xu, Zhaorui Tan, Chen Jiang, Deshu Chen, Xin Guo, Brian C. Lovell, Limei Han, Yuan Cheng, Mahsa Baktashmotlagh)</author>
      <guid isPermaLink="false">2509.15791v2</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data</title>
      <link>http://arxiv.org/abs/2509.19366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨无监督异常检测方法在审计分析中的有效性，使用美国卫生与公共服务部(DHHS)的支出数据作为案例。研究比较了多种异常检测算法，并发现混合方法能提高复杂金融数据中异常识别的稳健性和准确性。&lt;h4&gt;背景&lt;/h4&gt;政府大规模数据集中需要高效准确的异常检测，而传统审计方法可能无法满足这一需求。&lt;h4&gt;目的&lt;/h4&gt;评估和比较多种无监督异常检测算法在联邦支出模式异常识别中的有效性，以提高审计质量和效率。&lt;h4&gt;方法&lt;/h4&gt;使用数据准备、算法实施和性能评估（包括精确度、召回率和F1分数）的方法论。比较的算法包括基于直方图的异常分数(HBOS)、稳健主成分分析(PCA)、最小协方差行列式(MCD)和K近邻(KNN)。&lt;h4&gt;主要发现&lt;/h4&gt;结合多种检测策略的混合方法能提高复杂金融数据中异常识别的稳健性和准确性。&lt;h4&gt;结论&lt;/h4&gt;该研究通过提供各种异常检测模型比较有效性的见解，展示了无监督学习技术在提高审计质量和效率方面的潜力，对审计人员、政策制定者和研究人员在利用先进分析进行政府财务监督和风险管理方面具有启示意义。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了无监督异常检测方法在审计分析中的有效性，利用美国卫生与公共服务部(DHHS)的支出数据作为案例示例。我们采用并比较了多种异常检测算法，包括基于直方图的异常分数(HBOS)、稳健主成分分析(PCA)、最小协方差行列式(MCD)和K近邻(KNN)，以识别联邦支出模式中的异常。研究解决了在政府大规模数据集中高效准确异常检测的日益增长的需求，而传统审计方法可能无法满足这一需求。我们的方法论包括数据准备、算法实施以及使用精确度、召回率和F1分数进行性能评估。结果表明，结合多种检测策略的混合方法提高了复杂金融数据中异常识别的稳健性和准确性。该研究通过提供各种异常检测模型比较有效性的见解，展示了无监督学习技术在提高审计质量和效率方面的潜力，对寻求利用先进分析进行政府财务监督和风险管理的审计人员、政策制定者和研究人员具有启示意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the effectiveness of unsupervised outlier detectionmethods in audit analytics, utilizing USA spending data from the U.S.Department of Health and Human Services (DHHS) as a case example. We employ andcompare multiple outlier detection algorithms, including Histogram-basedOutlier Score (HBOS), Robust Principal Component Analysis (PCA), MinimumCovariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identifyanomalies in federal spending patterns. The research addresses the growing needfor efficient and accurate anomaly detection in large-scale governmentaldatasets, where traditional auditing methods may fall short. Our methodologyinvolves data preparation, algorithm implementation, and performance evaluationusing precision, recall, and F1 scores. Results indicate that a hybridapproach, combining multiple detection strategies, enhances the robustness andaccuracy of outlier identification in complex financial data. This studycontributes to the field of audit analytics by providing insights into thecomparative effectiveness of various outlier detection models and demonstratingthe potential of unsupervised learning techniques in improving audit qualityand efficiency. The findings have implications for auditors, policymakers, andresearchers seeking to leverage advanced analytics in governmental financialoversight and risk management.</description>
      <author>example@mail.com (Buhe Li, Berkay Kaplan, Maksym Lazirko, Aleksandr Kogan)</author>
      <guid isPermaLink="false">2509.19366v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Graph Variate Neural Networks</title>
      <link>http://arxiv.org/abs/2509.20311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GVNNs是一种创新的神经网络架构，专门用于处理动态时空信号，通过结合稳定长期支持和瞬时数据驱动交互，有效捕获动态统计相关性，在多个任务中表现优异，特别是在脑机接口应用方面。&lt;h4&gt;背景&lt;/h4&gt;动态时空信号建模是图神经网络(GNN)文献中的一个突出挑战。GNN假设存在一个潜在的图结构，但这种潜在结构可能并不总是存在，或者是从信号中独立推导出来的。从多通道数据中总是可以构建一个随时间演化的功能网络。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来处理动态时空信号建模问题，特别是当潜在图结构不存在或独立于信号的情况。&lt;h4&gt;方法&lt;/h4&gt;基于GVSA(图变分信号分析)和图信号处理工具，引入GVNNs(图变分神经网络)。设计了能够将时空信号与信号相关的连接张量进行卷积的层，该连接张量结合了稳定的长期支持和瞬时数据驱动的交互。这种设计在每个时间步捕获动态统计相关性，无需使用滑动窗口，并实现了序列长度上的线性复杂度的高效实现。&lt;h4&gt;主要发现&lt;/h4&gt;在预测基准测试中，GVNNs始终优于强大的基于图的基线方法，与广泛使用的序列模型(如LSTMs和Transformers)具有竞争力。在EEG运动想象分类中，GVNNs实现了高精度。&lt;h4&gt;结论&lt;/h4&gt;GVNNs在脑机接口应用方面显示出潜力，为动态时空信号建模提供了一个有效的新方法。&lt;h4&gt;翻译&lt;/h4&gt;对动态演化的时空信号进行建模是图神经网络(GNN)文献中的一个突出挑战。值得注意的是，GNN假设存在一个潜在的图结构。虽然这种潜在结构可能并不总是存在，或者是从信号中独立推导出来的，但从多通道数据中总是可以构建一个随时间演化的功能网络。图变分信号分析(GVSA)定义了一个统一框架，包含一个瞬时连接剖面的网络张量，通常基于信号本身构建一个稳定的支撑。基于GVSA和图信号处理工具，我们引入了图变分神经网络(GVNNs)：这些层将时空信号与信号相关的连接张量进行卷积，该张量结合了稳定的长期支撑和瞬时的数据驱动交互。这种设计在每个时间步捕获动态统计相关性，无需使用滑动窗口，并且允许在序列长度上实现线性复杂度的高效实现。在预测基准测试中，GVNNs始终优于强大的基于图的基线方法，并且与广泛使用的序列模型(如LSTMs和Transformers)具有竞争力。在EEG运动想象分类中，GVNNs实现了高精度，突显了它们在脑机接口应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modelling dynamically evolving spatio-temporal signals is a prominentchallenge in the Graph Neural Network (GNN) literature. Notably, GNNs assume anexisting underlying graph structure. While this underlying structure may notalways exist or is derived independently from the signal, a temporally evolvingfunctional network can always be constructed from multi-channel data. GraphVariate Signal Analysis (GVSA) defines a unified framework consisting of anetwork tensor of instantaneous connectivity profiles against a stable supportusually constructed from the signal itself. Building on GVSA and tools fromgraph signal processing, we introduce Graph-Variate Neural Networks (GVNNs):layers that convolve spatio-temporal signals with a signal-dependentconnectivity tensor combining a stable long-term support with instantaneous,data-driven interactions. This design captures dynamic statisticalinterdependencies at each time step without ad hoc sliding windows and admitsan efficient implementation with linear complexity in sequence length. Acrossforecasting benchmarks, GVNNs consistently outperform strong graph-basedbaselines and are competitive with widely used sequence models such as LSTMsand Transformers. On EEG motor-imagery classification, GVNNs achieve strongaccuracy highlighting their potential for brain-computer interfaceapplications.</description>
      <author>example@mail.com (Om Roy, Yashar Moshfeghi, Keith Smith)</author>
      <guid isPermaLink="false">2509.20311v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Stochastically Evolving Graphs via Edit Semigroups</title>
      <link>http://arxiv.org/abs/2509.19678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了一种基于Tsetlin库和超平面排列半群谱理论的子图随机演化过程，该过程通过随机编辑操作生成宿主图子图上的随机游走。&lt;h4&gt;背景&lt;/h4&gt;演化图出现在深度学习、图神经网络、流行病学建模和社会网络等多个领域。&lt;h4&gt;目的&lt;/h4&gt;开发一种从给定图中采样随机子图的一般随机模型。&lt;h4&gt;方法&lt;/h4&gt;从初始子图开始，每次迭代应用随机选择的编辑操作（简单编辑如添加/删除边，或复合编辑同时影响多条边），生成子图集合上的随机游走，并使用半群谱理论进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;1) 随机游走的特征值可由宿主图的边子集自然索引；2) 简单编辑情况下提供了转移概率矩阵特征向量的闭式公式；3) 提供了随机游走收敛速率的精确界限。&lt;h4&gt;结论&lt;/h4&gt;该随机演化过程可作为从给定图中采样随机子图的一般随机模型，适用于多种应用场景。&lt;h4&gt;翻译&lt;/h4&gt;我们使用与Tsetlin库和超平面排列相关的半群谱理论，研究了基础宿主图中子图的随机演化过程。从初始子图开始，每次迭代对当前子图应用随机选择的编辑操作。这些编辑操作从简单的添加或删除一条边，到可以同时影响多条边的复合编辑不等。这种演化过程在宿主图的所有可能子图集合上生成一个随机游走。我们证明了这个随机游走的特征值可以自然地由宿主图的边子集索引。在简单编辑的情况下，我们还提供了转移概率矩阵特征向量的闭式公式以及这个随机游走收敛速率的精确界限。我们考虑了向复合编辑情况的扩展；该模型的例子包括先前研究的Moran森林模型和动态随机相交图模型。演化图出现在从深度学习和图神经网络到流行病学建模和社会网络的各个领域。我们的随机演化过程作为从给定图中采样随机子图的一般随机模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate a randomly evolving process of subgraphs in an underlying hostgraph using the spectral theory of semigroups related to the Tsetlin libraryand hyperplane arrangements. Starting with some initial subgraph, at eachiteration, we apply a randomly selected edit to the current subgraph. Suchedits vary in nature from simple edits consisting of adding or deleting anedge, or compound edits which can affect several edges at once. This evolvingprocess generates a random walk on the set of all possible subgraphs of thehost graph. We show that the eigenvalues of this random walk can be naturallyindexed by subsets of edges of the host graph. We also provide, in the case ofsimple edits, a closed-form formula for the eigenvectors of the transitionprobability matrix and a sharp bound for the rate of convergence of this randomwalk. We consider extensions to the case of compound edits; examples of thismodel include the previously studied Moran forest model and a dynamic randomintersection graph model. Evolving graphs arise in a variety of fields rangingfrom deep learning and graph neural networks to epidemic modeling and socialnetworks. Our random evolving process serves as a general stochastic model forsampling random subgraphs from a given graph.</description>
      <author>example@mail.com (Fan Chung, Sawyer Jack Robertson)</author>
      <guid isPermaLink="false">2509.19678v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>EngravingGNN: A Hybrid Graph Neural Network for End-to-End Piano Score Engraving</title>
      <link>http://arxiv.org/abs/2509.19412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Conference on Technologies for Music  Notation and Representation (TENOR) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的图神经网络框架用于自动音乐雕版，通过多任务学习同时处理多个相互关联的子任务，实现了从量化符号输入到可打印乐谱的转换。&lt;h4&gt;背景&lt;/h4&gt;自动音乐雕版是从音乐内容创建人类可读乐谱的关键步骤，对所有涉及人类演奏者的应用都至关重要，但在符号音乐处理领域仍是一个大多未被探索的主题。&lt;h4&gt;目的&lt;/h4&gt;解决自动音乐雕版问题，通过一个统一的框架处理多个相互关联的子任务，包括声部连接、谱表分配、音高拼写、调号、符干方向、八度移位和谱号等。&lt;h4&gt;方法&lt;/h4&gt;采用多任务图神经网络联合预测多个音乐雕版相关子任务，并使用专门的后续处理流程生成可打印的MusicXML/MEI输出。&lt;h4&gt;主要发现&lt;/h4&gt;在J-Pop和DCML Romantic两个多样化钢琴曲库上的评估表明，统一模型在所有子任务上都取得了良好的准确性，优于仅专精于特定子任务的现有系统。&lt;h4&gt;结论&lt;/h4&gt;在多任务设置中使用共享的GNN编码器和轻量级的任务特定解码器，为自动音乐雕版提供了一种可扩展且有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文关注自动音乐雕版，即从音乐内容创建人类可读的乐谱。这一步骤对所有包含人类演奏者的应用都是基础性的，但在符号音乐处理领域仍是一个大多未被探索的主题。在这项工作中，我们将问题形式化为一系列相互依赖的子任务集合，并提出了一种统一的图神经网络框架，专门针对钢琴音乐和量化符号输入的情况。我们的方法采用多任务GNN来联合预测声部连接、谱表分配、音高拼写、调号、符干方向、八度移位和谱号等。专门的后续处理流程生成可打印的MusicXML/MEI输出。在两个多样化的钢琴曲库上的全面评估表明，与仅专精于特定子任务的现有系统相比，我们的统一模型在所有子任务上都取得了良好的准确性。这些结果表明，在多任务设置中，使用共享的GNN编码器和轻量级的任务特定解码器，为自动音乐雕版提供了一种可扩展且有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper focuses on automatic music engraving, i.e., the creation of ahumanly-readable musical score from musical content. This step is fundamentalfor all applications that include a human player, but it remains a mostlyunexplored topic in symbolic music processing. In this work, we formalize theproblem as a collection of interdependent subtasks, and propose a unified graphneural network (GNN) framework that targets the case of piano music andquantized symbolic input. Our method employs a multi-task GNN to jointlypredict voice connections, staff assignments, pitch spelling, key signature,stem direction, octave shifts, and clef signs. A dedicated postprocessingpipeline generates print-ready MusicXML/MEI outputs. Comprehensive evaluationon two diverse piano corpora (J-Pop and DCML Romantic) demonstrates that ourunified model achieves good accuracy across all subtasks, compared to existingsystems that only specialize in specific subtasks. These results indicate thata shared GNN encoder with lightweight task-specific decoders in a multi-tasksetting offers a scalable and effective solution for automatic music engraving.</description>
      <author>example@mail.com (Emmanouil Karystinaios, Francesco Foscarin, Gerhard Widmer)</author>
      <guid isPermaLink="false">2509.19412v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction</title>
      <link>http://arxiv.org/abs/2509.20290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12page and 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为PGCLODA的提示引导的基于图对比学习的框架，用于发现寡肽与感染性疾病之间的潜在关联。该方法构建三元图，结合双编码器架构，在基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;传染病持续威胁公共健康，亟需有效计算方法筛选新型抗感染剂。寡肽因结构简单、生物利用度高且不易产生耐药性，成为抗菌研究的有前景候选物，但专门预测寡肽与感染性疾病关联的计算模型仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;开发一种计算方法来预测寡肽与感染性疾病之间的关联，促进新型抗感染剂的发现。&lt;h4&gt;方法&lt;/h4&gt;构建包含寡肽、微生物和疾病节点的三元图，整合结构和语义信息。采用提示引导的图增强策略保留关键区域，使用结合图卷积网络(GCN)和Transformer的双编码器架构捕获特征，最后通过多层感知器(MLP)分类器进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;PGCLODA在AUROC、AUPRC和准确率方面持续优于最先进模型。消融研究和超参数研究确认了各模块的贡献。案例研究验证了其泛化能力和发现新型生物相关关联的潜力。&lt;h4&gt;结论&lt;/h4&gt;该研究为机制驱动的发现和基于寡肽的药物开发提供了有价值的见解，PGCLODA的源代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;传染病继续对公共健康构成严重威胁，凸显了有效计算方法筛选新型抗感染剂的迫切需求。寡肽因其结构简单、生物利用度高且不易产生耐药性，已成为抗菌研究中的有前景的候选物。尽管有这些潜力，但专门用于预测寡肽与感染性疾病之间关联的计算模型仍然稀缺。本研究引入了一种提示引导的基于图对比学习的框架(PGCLODA)来发现潜在关联。构建了一个包含寡肽、微生物和疾病节点的三元图，整合了结构和语义信息。为了在对比学习中保留关键区域，采用提示引导的图增强策略来生成有意义的配对视图。使用整合图卷积网络(GCN)和Transformer的双编码器架构来联合捕获局部和全局特征。融合的嵌入随后被输入多层感知器(MLP)分类器进行最终预测。在基准数据集上的实验结果表明，PGCLODA在AUROC、AUPRC和准确率方面持续优于最先进的模型。消融研究和超参数研究确认了每个模块的贡献。案例研究进一步验证了PGCLODA的泛化能力及其发现新型、生物相关关联的潜力。这些发现为机制驱动的发现和基于寡肽的药物开发提供了有价值的见解。PGCLODA的源代码可在https://github.com/jjnlcode/PGCLODA在线获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infectious diseases continue to pose a serious threat to public health,underscoring the urgent need for effective computational approaches to screennovel anti-infective agents. Oligopeptides have emerged as promising candidatesin antimicrobial research due to their structural simplicity, highbioavailability, and low susceptibility to resistance. Despite their potential,computational models specifically designed to predict associations betweenoligopeptides and infectious diseases remain scarce. This study introduces aprompt-guided graph-based contrastive learning framework (PGCLODA) to uncoverpotential associations. A tripartite graph is constructed with oligopeptides,microbes, and diseases as nodes, incorporating both structural and semanticinformation. To preserve critical regions during contrastive learning, aprompt-guided graph augmentation strategy is employed to generate meaningfulpaired views. A dual encoder architecture, integrating Graph ConvolutionalNetwork (GCN) and Transformer, is used to jointly capture local and globalfeatures. The fused embeddings are subsequently input into a multilayerperceptron (MLP) classifier for final prediction. Experimental results on abenchmark dataset indicate that PGCLODA consistently outperformsstate-of-the-art models in AUROC, AUPRC, and accuracy. Ablation andhyperparameter studies confirm the contribution of each module. Case studiesfurther validate the generalization ability of PGCLODA and its potential touncover novel, biologically relevant associations. These findings offervaluable insights for mechanism-driven discovery and oligopeptide-based drugdevelopment. The source code of PGCLODA is available online athttps://github.com/jjnlcode/PGCLODA.</description>
      <author>example@mail.com (Dayu Tan, Jing Chen, Xiaoping Zhou, Yansen Su, Chunhou Zheng)</author>
      <guid isPermaLink="false">2509.20290v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>C$^2$MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis</title>
      <link>http://arxiv.org/abs/2509.20152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为C²MIL的双重因果图MIL模型，用于解决H&amp;E染色全切片图像生存分析中的语义偏差和拓扑噪声问题，提高了模型的泛化能力和可解释性。&lt;h4&gt;背景&lt;/h4&gt;基于图的多个实例学习（Graph-based MIL）在H&amp;E染色全切片图像生存分析中被广泛使用，但染色和扫描的变化会引入语义偏差，与因果关系无关的拓扑子图会产生噪声，导致有偏差的切片级表示，影响模型的可解释性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有Graph-based MIL方法中存在的语义偏差和拓扑噪声问题，提高生存分析模型的可解释性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入双重结构因果模型作为理论基础，提出C²MIL模型，包含跨尺度自适应特征解缠模块用于语义因果干预，以及伯努利可微分因果子图采样方法用于拓扑因果发现，并通过结合解缠监督和对比学习的联合优化策略同时优化语义和拓扑因果性。&lt;h4&gt;主要发现&lt;/h4&gt;C²MIL在泛化和可解释性方面持续优于现有方法，可作为各种MIL基线的因果增强，代码已公开在GitHub上。&lt;h4&gt;结论&lt;/h4&gt;C²MIL模型有效解决了现有方法中的语义偏差和拓扑噪声问题，提高了生存分析的性能和可解释性，为医学图像分析提供了新的因果推理框架。&lt;h4&gt;翻译&lt;/h4&gt;基于图的多个实例学习（MIL）在苏木精和伊红（H&amp;E）染色的全切片图像（WSIs）生存分析中被广泛使用，因为它能够捕获拓扑信息。然而，染色和扫描的变化会引入语义偏差，而与因果关系无关的拓扑子图会产生噪声，导致有偏差的切片级表示。这些问题可能会阻碍分析的可解释性和泛化能力。为了解决这个问题，我们引入双重结构因果模型作为理论基础，并提出了一种新颖且可解释的双重因果图MIL模型C²MIL。C²MIL包含一个新的跨尺度自适应特征解缠模块用于语义因果干预，以及一个新的伯努利可微分因果子图采样方法用于拓扑因果发现。结合解缠监督和对比学习的联合优化策略能够同时优化语义和拓扑因果性。实验证明，C²MIL在泛化和可解释性方面持续优于现有方法，并可作为各种MIL基线的因果增强。代码可在https://github.com/mimic0127/C2MIL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based Multiple Instance Learning (MIL) is widely used in survivalanalysis with Hematoxylin and Eosin (H\&amp;E)-stained whole slide images (WSIs)due to its ability to capture topological information. However, variations instaining and scanning can introduce semantic bias, while topological subgraphsthat are not relevant to the causal relationships can create noise, resultingin biased slide-level representations. These issues can hinder both theinterpretability and generalization of the analysis. To tackle this, weintroduce a dual structural causal model as the theoretical foundation andpropose a novel and interpretable dual causal graph-based MIL model, C$^2$MIL.C$^2$MIL incorporates a novel cross-scale adaptive feature disentangling modulefor semantic causal intervention and a new Bernoulli differentiable causalsubgraph sampling method for topological causal discovery. A joint optimizationstrategy combining disentangling supervision and contrastive learning enablessimultaneous refinement of both semantic and topological causalities.Experiments demonstrate that C$^2$MIL consistently improves generalization andinterpretability over existing methods and can serve as a causal enhancementfor diverse MIL baselines. The code is available athttps://github.com/mimic0127/C2MIL.</description>
      <author>example@mail.com (Min Cen, Zhenfeng Zhuang, Yuzhe Zhang, Min Zeng, Baptiste Magnier, Lequan Yu, Hong Zhang, Liansheng Wang)</author>
      <guid isPermaLink="false">2509.20152v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance</title>
      <link>http://arxiv.org/abs/2509.19883v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 5 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoMelSinger是一种零样本歌唱声音合成框架，解决了韵律泄漏问题，实现了结构和分离的旋律控制，在音高准确性、音色一致性和零样本可转移性方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;离散编解码语音合成技术已通过上下文学习实现零样本生成，但直接应用于歌唱声音合成(SVS)仍面临挑战，特别是在需要精确旋律控制的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种零样本SVS框架，能够在离散编码建模范式中实现结构和分离的旋律控制，同时解决韵律泄漏问题。&lt;h4&gt;方法&lt;/h4&gt;CoMelSinger基于非自回归MaskGCT架构，用歌词和音高令牌替换传统文本输入；提出从粗到细的对比学习策略抑制韵律泄漏；集成轻量级仅编码器歌唱声音转录模块提供细粒度帧级监督。&lt;h4&gt;主要发现&lt;/h4&gt;基于提示的生成常常引入韵律泄漏，音高信息会无意中与音色提示纠缠；CoMelSinger通过对比学习策略和SVT模块有效解决了这一问题。&lt;h4&gt;结论&lt;/h4&gt;CoMelSinger在音高准确性、音色一致性和零样本可转移性方面相较于竞争性基线方法取得了显著改进。&lt;h4&gt;翻译&lt;/h4&gt;歌唱声音合成(SVS)旨在从结构化的音乐输入（如歌词和音高序列）生成富有表现力的声乐表演。虽然最近在离散编码语音合成方面的进展已通过上下文学习实现了零样本生成，但由于需要精确的旋律控制，直接将这些技术扩展到SVS仍然非同寻常。特别是，基于提示的生成常常引入韵律泄漏，音高信息会无意中与音色提示纠缠在一起，损害可控性。我们提出了CoMelSinger，一种零样本SVS框架，在离散编码建模范式中实现结构和分离的旋律控制。基于非自回归MaskGCT架构，CoMelSinger用歌词和音高令牌替换传统文本输入，保持上下文泛化能力同时增强旋律条件化。为了抑制韵律泄漏，我们提出了从粗到细的对比学习策略，明确调节声学提示和旋律输入之间的音高冗余。此外，我们还整合了一个轻量级的仅编码器歌唱声音转录(SVT)模块，将声学令牌与音高和持续时间对齐，提供细粒度的帧级监督。实验结果表明，CoMelSinger在音高准确性、音色一致性和零样本可转移性方面相较于竞争性基线方法取得了显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Singing Voice Synthesis (SVS) aims to generate expressive vocal performancesfrom structured musical inputs such as lyrics and pitch sequences. While recentprogress in discrete codec-based speech synthesis has enabled zero-shotgeneration via in-context learning, directly extending these techniques to SVSremains non-trivial due to the requirement for precise melody control. Inparticular, prompt-based generation often introduces prosody leakage, wherepitch information is inadvertently entangled within the timbre prompt,compromising controllability. We present CoMelSinger, a zero-shot SVS frameworkthat enables structured and disentangled melody control within a discrete codecmodeling paradigm. Built on the non-autoregressive MaskGCT architecture,CoMelSinger replaces conventional text inputs with lyric and pitch tokens,preserving in-context generalization while enhancing melody conditioning. Tosuppress prosody leakage, we propose a coarse-to-fine contrastive learningstrategy that explicitly regularizes pitch redundancy between the acousticprompt and melody input. Furthermore, we incorporate a lightweight encoder-onlySinging Voice Transcription (SVT) module to align acoustic tokens with pitchand duration, offering fine-grained frame-level supervision. Experimentalresults demonstrate that CoMelSinger achieves notable improvements in pitchaccuracy, timbre consistency, and zero-shot transferability over competitivebaselines.</description>
      <author>example@mail.com (Junchuan Zhao, Wei Zeng, Tianle Lyu, Ye Wang)</author>
      <guid isPermaLink="false">2509.19883v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning</title>
      <link>http://arxiv.org/abs/2509.19664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了名为MoTiC（Momentum Tightness and Contrast）的框架，用于解决少样本增量学习中的双重挑战：从稀缺样本中学习新类别同时保留旧类别知识。&lt;h4&gt;背景&lt;/h4&gt;Few-Shot Class-Incremental Learning (FSCIL)需要同时处理两个挑战：从稀缺样本中学习新类别并保留旧类别知识。现有方法使用冻结特征提取器和类别平均原型来缓解灾难性遗忘和过拟合问题。&lt;h4&gt;目的&lt;/h4&gt;减少新类别原型的估计偏差，提高原型准确性，并构建具有丰富表示能力和增强类别间内聚性的特征空间。&lt;h4&gt;方法&lt;/h4&gt;通过贝叶斯分析将新类别先验与旧类别统计信息对齐以减少方差；提出大规模对比学习强制执行跨类别特征紧密性；集成动量自监督和虚拟类别到MoTiC框架中，以丰富特征多样性并为新类别原型注入先验信息。&lt;h4&gt;主要发现&lt;/h4&gt;新类别先验与旧类别统计信息对齐可减少方差并提高原型准确性；大规模对比学习可增强跨类别特征紧密性；动量自监督和虚拟类别的集成可丰富特征多样性并提升增量学习性能。&lt;h4&gt;结论&lt;/h4&gt;在三个FSCIL基准测试上取得最先进性能，特别是在细粒度任务CUB-200上，验证了该方法减少估计偏差和提高增量学习鲁棒性的能力。&lt;h4&gt;翻译&lt;/h4&gt;少样本增量学习必须应对双重挑战：从稀缺样本中学习新类别同时保留旧类别知识。现有方法使用冻结特征提取器和类别平均原型来缓解灾难性遗忘和过拟合。然而，由于数据极度稀缺，新类别原型存在显著估计偏差，而基础类别原型则受益于充足的数据。在这项工作中，我们从理论上证明通过贝叶斯分析将新类别先验与旧类别统计信息对齐可以减少方差并提高原型准确性。此外，我们提出大规模对比学习来强制执行跨类别特征紧密性。为进一步丰富特征多样性并为新类别原型注入先验信息，我们将动量自监督和虚拟类别集成到动量紧密性和对比框架（MoTiC）中，构建具有丰富表示能力和增强类别间内聚性的特征空间。在三个FSCIL基准测试上的实验产生了最先进的性能，特别是在细粒度任务CUB-200上，验证了我们方法减少估计偏差和提高增量学习鲁棒性的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-Shot Class-Incremental Learning (FSCIL) must contend with the dualchallenge of learning new classes from scarce samples while preserving oldclass knowledge. Existing methods use the frozen feature extractor andclass-averaged prototypes to mitigate against catastrophic forgetting andoverfitting. However, new-class prototypes suffer significant estimation biasdue to extreme data scarcity, whereas base-class prototypes benefit fromsufficient data. In this work, we theoretically demonstrate that aligning thenew-class priors with old-class statistics via Bayesian analysis reducesvariance and improves prototype accuracy. Furthermore, we propose large-scalecontrastive learning to enforce cross-category feature tightness. To furtherenrich feature diversity and inject prior information for new-class prototypes,we integrate momentum self-supervision and virtual categories into the MomentumTightness and Contrast framework (MoTiC), constructing a feature space withrich representations and enhanced interclass cohesion. Experiments on threeFSCIL benchmarks produce state-of-the-art performances, particularly on thefine-grained task CUB-200, validating our method's ability to reduce estimationbias and improve incremental learning robustness.</description>
      <author>example@mail.com (Zeyu He, Shuai Huang, Yuwu Lu, Ming Zhao)</author>
      <guid isPermaLink="false">2509.19664v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification</title>
      <link>http://arxiv.org/abs/2509.19654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 2 figures, IEEE-EMBS BSN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于符号袋表示的自监督学习框架，用于处理数字健康领域中时间序列数据的分布偏移问题，特别是在人类行为差异导致的数据变化情况下表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;数字健康领域的时间序列数据日益重要，但这类数据通常具有高度噪声、概念漂移等特性，这给训练具有良好泛化能力的深度学习模型带来了挑战。自监督对比学习作为一种从原始数据中学习有意义模式和表示的先进方法，在处理这些问题时展现出潜力。&lt;h4&gt;目的&lt;/h4&gt;解决数字健康时间序列数据中因不同人类行为引起的数据分布偏移问题，提出一种能够抵抗数据偏移的自监督学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于符号袋表示的自监督学习框架。符号袋表示对数据变形、位置偏移和时间序列数据中存在的噪声具有不敏感性，这被认为可能对指导深度学习获取抵抗此类数据偏移的表示具有关键作用。&lt;h4&gt;主要发现&lt;/h4&gt;在存在显著数据偏移的情况下，所提出的方法能够实现显著更好的性能。&lt;h4&gt;结论&lt;/h4&gt;基于符号袋表示的自监督学习框架能够有效处理数字健康领域中时间序列数据的分布偏移问题，特别是在人类行为差异导致的数据变化情况下表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;数字健康领域中时间序列重要性的激增，需要先进的方法来提取有意义的模式和表示。自监督对比学习已成为一种直接从原始数据中学习的有前景的方法。然而，数字健康中的时间序列数据已知具有高度噪声，本质上涉及概念漂移，并且对训练可泛化的深度学习模型提出了挑战。在本文中，我们特别关注由不同人类行为引起的数据分布偏移，并提出了一种意识到符号袋表示的自监督学习框架。符号袋表示以其对时间序列数据中存在的数据变形、位置偏移和噪声的不敏感性而闻名，这使其可能在指导深度学习获取抵抗此类数据偏移的表示方面具有关键作用。我们证明，在存在显著数据偏移的情况下，所提出的方法可以实现显著更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The surge in the significance of time series in digital health domainsnecessitates advanced methodologies for extracting meaningful patterns andrepresentations. Self-supervised contrastive learning has emerged as apromising approach for learning directly from raw data. However, time seriesdata in digital health is known to be highly noisy, inherently involves conceptdrifting, and poses a challenge for training a generalizable deep learningmodel. In this paper, we specifically focus on data distribution shift causedby different human behaviors and propose a self-supervised learning frameworkthat is aware of the bag-of-symbol representation. The bag-of-symbolrepresentation is known for its insensitivity to data warping, location shifts,and noise existed in time series data, making it potentially pivotal in guidingdeep learning to acquire a representation resistant to such data shifting. Wedemonstrate that the proposed method can achieve significantly betterperformance where significant data shifting exists.</description>
      <author>example@mail.com (Kevin Garcia, Cassandra Garza, Brooklyn Berry, Yifeng Gao)</author>
      <guid isPermaLink="false">2509.19654v1</guid>
      <pubDate>Thu, 25 Sep 2025 15:13:59 +0800</pubDate>
    </item>
    <item>
      <title>From Prediction to Understanding: Will AI Foundation Models Transform Brain Science?</title>
      <link>http://arxiv.org/abs/2509.17280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了生成预训练(GPT)和基础模型如何应用于脑科学领域，强调了这些模型在预测方面的优势，同时也指出从预测到解释的挑战。&lt;h4&gt;背景&lt;/h4&gt;生成预训练使语言模型能够从大量互联网文本中自主学习，推动了AI领域的突破。基础模型作为大型预训练系统，正越来越多地应用于语言以外的脑科学领域。&lt;h4&gt;目的&lt;/h4&gt;探讨基础模型如何有效地整合到脑科学中，同时明确其潜力和局限性，主要关注如何从预测转向解释。&lt;h4&gt;方法&lt;/h4&gt;论文主要是概念性探讨，没有明确描述具体的研究方法。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在预测方面表现出色，但预测成功并不保证科学理解。将模型计算与神经活动和认知的潜在机制联系起来是主要挑战。&lt;h4&gt;结论&lt;/h4&gt;基础模型在脑科学中具有应用潜力，但需要从预测能力转向解释能力，才能真正促进科学理解。&lt;h4&gt;翻译&lt;/h4&gt;生成预训练（ChatGPT中的'GPT'）使语言模型能够从大量互联网文本中学习，无需人工监督。这种方法通过允许深度神经网络从大量、非结构化数据集中学习，推动了AI领域的突破。我们使用'基础模型'这一术语来指代可以适应广泛跨领域任务的大型预训练系统，这些模型正越来越多地应用于语言以外的脑科学领域。这些模型实现了强大的预测准确性，让人希望它们可能阐明计算原理。但仅凭预测成功并不能保证科学理解。在此，我们概述了基础模型如何有效地整合到脑科学中，强调了它们的潜力和局限性。核心挑战是从预测转向解释：将模型计算与神经活动和认知的潜在机制联系起来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative pretraining (the "GPT" in ChatGPT) enables language models tolearn from vast amounts of internet text without human supervision. Thisapproach has driven breakthroughs across AI by allowing deep neural networks tolearn from massive, unstructured datasets. We use the term foundation models torefer to large pretrained systems that can be adapted to a wide range of taskswithin and across domains, and these models are increasingly applied beyondlanguage to the brain sciences. These models achieve strong predictiveaccuracy, raising hopes that they might illuminate computational principles.But predictive success alone does not guarantee scientific understanding. Here,we outline how foundation models can be productively integrated into the brainsciences, highlighting both their promise and their limitations. The centralchallenge is to move from prediction to explanation: linking model computationsto mechanisms underlying neural activity and cognition.</description>
      <author>example@mail.com (Thomas Serre, Ellie Pavlick)</author>
      <guid isPermaLink="false">2509.17280v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
  <item>
      <title>PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification</title>
      <link>http://arxiv.org/abs/2509.17581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于光电响应非均匀性（PRNU）估计的相机识别新基准和模型，包含13K张照片和混合架构方法，相比现有方法显著提高了性能。&lt;h4&gt;背景&lt;/h4&gt;相机识别是一个重要研究领域，PRNU（光电响应非均匀性）被用于相机设备识别，但缺乏有效的基准测试方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够进行'野外'评估的相机识别基准，并提出一种更有效的PRNU相机识别模型。&lt;h4&gt;方法&lt;/h4&gt;构建包含13K张照片、来自120+台相机的数据集，采用混合架构模型结合去噪自编码器和卷积网络，使用PRNU信号间的Hadamard积而非对比学习作为输入。&lt;h4&gt;主要发现&lt;/h4&gt;新方法相比基于去噪自编码器和对比学习的最先进模型，显著提高了相机识别性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的基准和模型为相机识别领域提供了新的评估标准和有效方法，数据集和代码已公开发布。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种通过光电响应非均匀性估计进行相机识别的新基准。该基准包含13K张照片，来自120+台相机，训练和测试照片在不同场景下拍摄，实现了'野外'评估。此外，我们提出了一种基于PRNU的新型相机识别模型，采用混合架构，包括用于估计PRNU信号的去噪自编码器和能够执行1:N相机设备验证的卷积网络。与使用基于对比学习的传统方法不同，我们的方法使用参考和查询PRNU信号之间的Hadamard积作为输入。这种新颖的设计相比基于去噪自编码器和对比学习的最先进模型，显著提高了性能。我们在https://github.com/CroitoruAlin/PRNU-Bench发布了我们的数据集和代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel benchmark for camera identification via Photo ResponseNon-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with120+ cameras, where the training and test photos are taken in differentscenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novelPRNU-based camera identification model that employs a hybrid architecture,comprising a denoising autoencoder to estimate the PRNU signal and aconvolutional network that can perform 1:N verification of camera devices.Instead of using a conventional approach based on contrastive learning, ourmethod takes the Hadamard product between reference and query PRNU signals asinput. This novel design leads to significantly better results compared withstate-of-the-art models based on denoising autoencoders and contrastivelearning. We release our dataset and code at:https://github.com/CroitoruAlin/PRNU-Bench.</description>
      <author>example@mail.com (Florinel Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu)</author>
      <guid isPermaLink="false">2509.17581v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Prospective Multi-Graph Cohesion for Multivariate Time Series Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.17235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the 18th ACM International Conference on Web Search and  Data Mining (ACM WSDM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PMGC(前瞻性多图一致性)的框架，用于多变量时间序列异常检测，通过整合静态图和动态图来捕捉复杂关系，并引入前瞻性绘图策略提高异常检测效果。&lt;h4&gt;背景&lt;/h4&gt;高维时间序列数据中的异常检测对众多工业应用至关重要。最近，多变量时间序列异常检测越来越多地利用图结构建模变量间关系，通常采用图神经网络(GNNs)。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法依赖单一图表示而无法捕捉多变量时间序列中复杂多样关系的问题，提高异常检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;PMGC框架通过整合长期静态图和一系列短期实例动态图来利用空间相关性，使用图一致性损失函数进行调节。该损失函数促进动态图间的多样性，同时使其与静态图中的稳定长期关系保持一致。此外，引入'前瞻性绘图'策略来缓解传统基于预测的TSAD方法的局限性。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明图一致性损失函数能有效平衡动态图的多样性与静态图的稳定性；前瞻性绘图策略能准确反映正常条件下的并发级间关系。&lt;h4&gt;结论&lt;/h4&gt;在真实世界数据集上的经验评估证明，与现有TSAD技术相比，PMGC方法具有优越的性能，能有效提高多变量时间序列异常检测的效能。&lt;h4&gt;翻译&lt;/h4&gt;高维时间序列数据中的异常检测对众多工业应用至关重要。最近，多变量时间序列异常检测(TSAD)的最新进展越来越多地利用图结构来建模变量间关系，通常采用图神经网络(GNNs)。尽管这些方法取得了有希望的结果，但现有方法通常依赖单一图表示，不足以捕捉多变量时间序列中固有的复杂多样的关系。为此，我们提出了用于多变量TSAD的前瞻性多图一致性(PMGC)框架。PMGC通过整合长期静态图和一系列短期实例动态图来利用空间相关性，并通过图一致性损失函数进行调节。我们的理论分析表明，该损失函数促进动态图之间的多样性，同时使其与静态图中封装的稳定长期关系保持一致。此外，我们引入了'前瞻性绘图'策略来缓解传统基于预测的TSAD方法的局限性，这些方法往往难以应对不可预测的未来变化。该策略使模型能够准确反映正常条件下的并发级间关系，从而增强异常检测效果。在真实世界数据集上的经验评估表明，与现有TSAD技术相比，我们的方法具有优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in high-dimensional time series data is pivotal fornumerous industrial applications. Recent advances in multivariate time seriesanomaly detection (TSAD) have increasingly leveraged graph structures to modelinter-variable relationships, typically employing Graph Neural Networks (GNNs).Despite their promising results, existing methods often rely on a single graphrepresentation, which are insufficient for capturing the complex, diverserelationships inherent in multivariate time series. To address this, we proposethe Prospective Multi-Graph Cohesion (PMGC) framework for multivariate TSAD.PMGC exploits spatial correlations by integrating a long-term static graph witha series of short-term instance-wise dynamic graphs, regulated through a graphcohesion loss function. Our theoretical analysis shows that this loss functionpromotes diversity among dynamic graphs while aligning them with the stablelong-term relationships encapsulated by the static graph. Additionally, weintroduce a "prospective graphing" strategy to mitigate the limitations oftraditional forecasting-based TSAD methods, which often struggle withunpredictable future variations. This strategy allows the model to accuratelyreflect concurrent inter-series relationships under normal conditions, therebyenhancing anomaly detection efficacy. Empirical evaluations on real-worlddatasets demonstrate the superior performance of our method compared toexisting TSAD techniques.</description>
      <author>example@mail.com (Jiazhen Chen, Mingbin Feng, Tony S. Wirjanto)</author>
      <guid isPermaLink="false">2509.17235v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data</title>
      <link>http://arxiv.org/abs/2509.18507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at the 42nd International Conference on Machine Learning  (ICML) 2025. Code available at: https://github.com/ShanechiLab/SBIND/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SBIND的新型深度学习框架，用于建模神经图像中的时空依赖关系，并分离出与行为相关的神经动态。&lt;h4&gt;背景&lt;/h4&gt;高维神经活动成像（如宽场钙成像和功能超声成像）为理解大脑活动与行为关系提供了丰富信息，但准确建模受高维度、复杂时空依赖关系和行为无关动态的阻碍。现有动态模型通过预处理获取低维表示，但可能丢失行为相关信息和时空结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能建模神经图像时空依赖关系并分离行为相关神经动态的数据驱动深度学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出SBIND框架，在宽场成像数据集上验证其有效性，并展示其向功能超声成像这一新兴模态的扩展应用。&lt;h4&gt;主要发现&lt;/h4&gt;SBIND能有效识别大脑中的局部和长程空间依赖关系，同时分离出与行为相关的神经动态，在神经-行为预测方面优于现有模型。&lt;h4&gt;结论&lt;/h4&gt;SBIND为使用成像模态研究行为背后的神经机制提供了多功能工具。&lt;h4&gt;翻译&lt;/h4&gt;高维神经活动成像，如宽场钙成像和功能超声成像，为理解大脑活动与行为之间的关系提供了丰富的信息来源。准确建模这些模态中的神经动力学对于理解这种关系至关重要，但受到高维度、复杂的时空依赖关系以及这些模态中普遍存在的行为无关动态的阻碍。现有的动态模型通常采用预处理步骤从神经图像模态中获取低维表示。然而，这个过程可能会丢弃与行为相关的信息并错过时空结构。我们提出了SBIND，一种新颖的数据驱动深度学习框架，用于建模神经图像中的时空依赖关系，并将其与行为相关的神经动态与其他神经动态分离。我们在宽场成像数据集上验证了SBIND，并展示了其向功能超声成像的扩展，这是一种最近出现的模态，其动态建模 largely 仍未被探索。我们发现，我们的模型能够有效识别大脑中的局部和长程空间依赖关系，同时分离出与行为相关的神经动态。通过这样做，SBIND在神经-行为预测方面优于现有模型。总体而言，SBIND为使用成像模态研究行为背后的神经机制提供了多功能工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional imaging of neural activity, such as widefield calcium andfunctional ultrasound imaging, provide a rich source of information forunderstanding the relationship between brain activity and behavior. Accuratelymodeling neural dynamics in these modalities is crucial for understanding thisrelationship but is hindered by the high-dimensionality, complex spatiotemporaldependencies, and prevalent behaviorally irrelevant dynamics in thesemodalities. Existing dynamical models often employ preprocessing steps toobtain low-dimensional representations from neural image modalities. However,this process can discard behaviorally relevant information and missspatiotemporal structure. We propose SBIND, a novel data-driven deep learningframework to model spatiotemporal dependencies in neural images and disentangletheir behaviorally relevant dynamics from other neural dynamics. We validateSBIND on widefield imaging datasets, and show its extension to functionalultrasound imaging, a recent modality whose dynamical modeling has largelyremained unexplored. We find that our model effectively identifies both localand long-range spatial dependencies across the brain while also dissociatingbehaviorally relevant neural dynamics. Doing so, SBIND outperforms existingmodels in neural-behavioral prediction. Overall, SBIND provides a versatiletool for investigating the neural mechanisms underlying behavior using imagingmodalities.</description>
      <author>example@mail.com (Mohammad Hosseini, Maryam M. Shanechi)</author>
      <guid isPermaLink="false">2509.18507v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images</title>
      <link>http://arxiv.org/abs/2509.18973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Prompt-DAS的可提示多任务框架，用于从大规模电子显微镜图像中对大量细胞器实例进行领域自适应分割，实现了注释高效的学习。&lt;h4&gt;背景&lt;/h4&gt;从大规模电子显微镜(EM)图像中对大量细胞器实例进行领域自适应分割(DAS)是一种有前景的注释高效学习方法，受SAM(Segment Anything Model)启发。&lt;h4&gt;目的&lt;/h4&gt;提出一个灵活的框架，能够在适应训练和测试阶段利用任意数量的点提示，实现无监督领域适应、弱监督领域适应以及交互式分割功能。&lt;h4&gt;方法&lt;/h4&gt;设计了Prompt-DAS框架，整合了辅助的中心点检测任务，允许使用完整点、稀疏点或不使用点，并提出了提示引导的对比学习方法来增强判别性特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;Prompt-DAS只需在小数据集上训练，与需要为每个单独对象实例提供提示的SAM不同，它更加灵活且高效。&lt;h4&gt;结论&lt;/h4&gt;在具有挑战性的基准测试上进行的全面实验证明，Prompt-DAS在现有的UDA、WDA和基于SAM的方法上表现更好，为注释高效学习提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从大规模电子显微镜(EM)图像中对大量细胞器实例进行领域自适应分割(DAS)是一种有前景的注释高效学习方法。受SAM启发，我们提出了一个可提示的多任务框架，即Prompt-DAS，该框架足够灵活，可以在适应训练阶段和测试阶段利用任意数量的点提示。因此，通过不同的提示配置，Prompt-DAS可以实现无监督领域适应(UDA)和弱监督领域适应(WDA)，以及测试时的交互式分割。与需要为每个单独对象实例提供提示的基础模型SAM不同，Prompt-DAS只在小数据集上训练，并且可以通过整合辅助的中心点检测任务，利用所有实例上的完整点、部分实例上的稀疏点，甚至完全不使用点。此外，还提出了一种新颖的提示引导对比学习方法，以增强判别性特征学习。在具有挑战性的基准测试上进行的全面实验证明了该方法相对于现有UDA、WDA和基于SAM的方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain adaptive segmentation (DAS) of numerous organelle instances fromlarge-scale electron microscopy (EM) is a promising way to enableannotation-efficient learning. Inspired by SAM, we propose a promptablemultitask framework, namely Prompt-DAS, which is flexible enough to utilize anynumber of point prompts during the adaptation training stage and testing stage.Thus, with varying prompt configurations, Prompt-DAS can perform unsuperviseddomain adaptation (UDA) and weakly supervised domain adaptation (WDA), as wellas interactive segmentation during testing. Unlike the foundation model SAM,which necessitates a prompt for each individual object instance, Prompt-DAS isonly trained on a small dataset and can utilize full points on all instances,sparse points on partial instances, or even no points at all, facilitated bythe incorporation of an auxiliary center-point detection task. Moreover, anovel prompt-guided contrastive learning is proposed to enhance discriminativefeature learning. Comprehensive experiments conducted on challenging benchmarksdemonstrate the effectiveness of the proposed approach over existing UDA, WDA,and SAM-based approaches.</description>
      <author>example@mail.com (Jiabao Chen, Shan Xiong, Jialin Peng)</author>
      <guid isPermaLink="false">2509.18973v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>One-shot Embroidery Customization via Contrastive LoRA Modulation</title>
      <link>http://arxiv.org/abs/2509.18948v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM Transactions on Graphics (TOG), SIGGRAPH Asia 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的对比学习框架，用于细粒度视觉特征转移，特别是在刺绣等复杂艺术形式中的应用。该方法能够从单个参考图像中解耦风格和内容特征，并通过两阶段LoRA调制技术捕获细粒度风格特征。&lt;h4&gt;背景&lt;/h4&gt;扩散模型显著推进了图像处理技术，其生成逼真图像的能力正在改变零售工作流程，特别是预销售可视化。除了艺术风格转移外，细粒度视觉特征转移变得越来越重要。刺绣作为一种复杂的纺织艺术形式，对现有风格转移方法提出了独特挑战。&lt;h4&gt;目的&lt;/h4&gt;探索针对细粒度特征的定制化方法，解决刺绣等复杂艺术形式的风格转移问题，实现更精确的视觉特征控制。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于对比学习的框架，首先构建图像对定义目标风格，采用基于预训练扩散模型解耦表示的相似度度量进行风格-内容分离。然后使用两阶段对比LoRA调制技术：第一阶段迭代更新整个LoRA和选定风格块初步分离风格与内容；第二阶段通过自知识蒸馏设计对比学习策略进一步解耦风格和内容。最后构建仅使用风格块的推理管道处理图像或文本输入。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在刺绣定制基准测试上超越了先前方法，并在艺术风格转移、素描上色和外观转移三个额外领域展示了强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该框架能够有效处理细粒度视觉特征转移问题，特别是在刺绣等复杂艺术形式中表现出色，且具有良好的跨领域泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型显著推进了图像处理技术，其生成逼真图像的能力正在改变零售工作流程，特别是在预销售可视化方面。除了艺术风格转移外，执行细粒度视觉特征转移的能力变得越来越重要。刺绣是一种纺织艺术形式，其特点是不同针迹图案和材料特性的复杂相互作用，这对现有的风格转移方法提出了独特挑战。为了探索此类细粒度特征的定制，我们提出了一个新的对比学习框架，该框架使用单个参考图像解耦细粒度风格和内容特征，建立在经典的图像类比概念基础上。我们首先构建图像对来定义目标风格，然后采用基于预训练扩散模型解耦表示的相似度度量进行风格-内容分离。随后，我们提出了一种两阶段对比LoRA调制技术来捕获细粒度风格特征。在第一阶段，我们迭代更新整个LoRA和选定的风格块，以初步分离风格与内容。在第二阶段，我们设计了一种对比学习策略，通过自知识蒸馏进一步解耦风格和内容。最后，我们构建了一个推理管道，仅使用风格块处理图像或文本输入。为了评估我们在细粒度风格转移方面的方法，我们为刺绣定制构建了一个基准测试。我们的方法在该任务上超越了先前的方法，并进一步展示了在三个额外领域的强大泛化能力：艺术风格转移、素描上色和外观转移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models have significantly advanced image manipulation techniques,and their ability to generate photorealistic images is beginning to transformretail workflows, particularly in presale visualization. Beyond artistic styletransfer, the capability to perform fine-grained visual feature transfer isbecoming increasingly important. Embroidery is a textile art form characterizedby intricate interplay of diverse stitch patterns and material properties,which poses unique challenges for existing style transfer methods. To explorethe customization for such fine-grained features, we propose a novelcontrastive learning framework that disentangles fine-grained style and contentfeatures with a single reference image, building on the classic concept ofimage analogy. We first construct an image pair to define the target style, andthen adopt a similarity metric based on the decoupled representations ofpretrained diffusion models for style-content separation. Subsequently, wepropose a two-stage contrastive LoRA modulation technique to capturefine-grained style features. In the first stage, we iteratively update thewhole LoRA and the selected style blocks to initially separate style fromcontent. In the second stage, we design a contrastive learning strategy tofurther decouple style and content through self-knowledge distillation.Finally, we build an inference pipeline to handle image or text inputs withonly the style blocks. To evaluate our method on fine-grained style transfer,we build a benchmark for embroidery customization. Our approach surpasses priormethods on this task and further demonstrates strong generalization to threeadditional domains: artistic style transfer, sketch colorization, andappearance transfer.</description>
      <author>example@mail.com (Jun Ma, Qian He, Gaofeng He, Huang Chen, Chen Liu, Xiaogang Jin, Huamin Wang)</author>
      <guid isPermaLink="false">2509.18948v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition</title>
      <link>http://arxiv.org/abs/2509.18706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Audio, Speech and Language  Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M4SER的多模态语音情感识别方法，通过引入对抗网络和基于标签的对比学习策略来增强情感识别性能。&lt;h4&gt;背景&lt;/h4&gt;多模态语音情感识别(SER)对改善人机交互至关重要。研究者利用语音和通过自动语音识别(ASR)获得的文本信息来识别说话者情感状态，但ASR错误可能降低情感识别性能。&lt;h4&gt;目的&lt;/h4&gt;解决ASR错误可能降低情感识别性能的挑战，提高多模态语音情感识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;在之前工作的基础上，引入两个额外训练策略：1)提出对抗网络增强模态特定表示的多样性；2)引入基于标签的对比学习策略更好地捕捉情感特征。将此方法称为M4SER。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用IEMOCAP和MELD数据集进行的广泛实验验证了M4SER优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;M4SER方法通过结合对抗网络和对比学习策略，有效解决了ASR错误对情感识别性能的影响，在多模态语音情感识别任务上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;多模态语音情感识别(SER)已成为改善人机交互的关键。研究人员越来越多地利用语音和通过自动语音识别(ASR)获得的文本信息来全面识别说话者的情感状态。虽然这种方法减少了对人工标注文本数据的依赖，但ASR错误可能会降低情感识别性能。为了解决这一挑战，在我们之前的工作中，我们引入了两个辅助任务，即ASR错误检测和ASR错误纠正，并提出了一种新的多模态融合(MF)方法，用于学习跨不同模态的模态特定和模态不变表示。在此基础上，在本文中，我们引入了两个额外的训练策略。首先，我们提出了一个对抗网络来增强模态特定表示的多样性。其次，我们引入了一种基于标签的对比学习策略以更好地捕捉情感特征。我们将我们提出的方法称为M4SER，并通过使用IEMOCAP和MELD数据集进行的广泛实验验证了其优于最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal speech emotion recognition (SER) has emerged as pivotal forimproving human-machine interaction. Researchers are increasingly leveragingboth speech and textual information obtained through automatic speechrecognition (ASR) to comprehensively recognize emotional states from speakers.Although this approach reduces reliance on human-annotated text data, ASRerrors possibly degrade emotion recognition performance. To address thischallenge, in our previous work, we introduced two auxiliary tasks, namely, ASRerror detection and ASR error correction, and we proposed a novel multimodalfusion (MF) method for learning modality-specific and modality-invariantrepresentations across different modalities. Building on this foundation, inthis paper, we introduce two additional training strategies. First, we proposean adversarial network to enhance the diversity of modality-specificrepresentations. Second, we introduce a label-based contrastive learningstrategy to better capture emotional features. We refer to our proposed methodas M4SER and validate its superiority over state-of-the-art methods throughextensive experiments using IEMOCAP and MELD datasets.</description>
      <author>example@mail.com (Jiajun He, Xiaohan Shi, Cheng-Hung Hu, Jinyi Mi, Xingfeng Li, Tomoki Toda)</author>
      <guid isPermaLink="false">2509.18706v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector</title>
      <link>http://arxiv.org/abs/2509.18535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种轻量级框架，用于检测ChatGPT生成的文本及其经过同义改写或简单提示修改后的版本。该方法基于文本的内部结构进行分类，这种结构在词汇层面变化时保持不变。&lt;h4&gt;背景&lt;/h4&gt;ChatGPT的广泛采用引发了对其滥用的担忧，凸显了对AI生成文本进行可靠检测的必要性。现有的词汇级检测器容易受到同义改写或简单提示的影响，容易受到ChatGPT词汇级模式和训练数据内容引起的偏差影响，在修改后的文本上性能下降，且通常需要大型模型或在线LLM交互。&lt;h4&gt;目的&lt;/h4&gt;解决当前AI文本检测器的局限性，特别是它们对同义改写和简单提示的脆弱性、对ChatGPT固有模式偏差的敏感性、在修改文本上的性能下降以及对计算资源的高需求。&lt;h4&gt;方法&lt;/h4&gt;提出了一种轻量级框架，通过以下步骤检测原始和修改后的AI生成文本：使用预训练语言模型编码句子嵌入；通过注意力机制建模句子间关系；采用对比学习减轻自回归生成带来的嵌入偏差；结合因果图和反事实方法将结构特征与主题相关偏差分离。&lt;h4&gt;主要发现&lt;/h4&gt;在两个精心策划的数据集（包括摘要比较和修改后的生活常见问题解答）上进行的实验验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;基于文本内部结构的检测方法可以有效识别AI生成的文本及其变体，即使在词汇层面经过修改后仍然有效，且不需要大型模型或在线LLM交互。&lt;h4&gt;翻译&lt;/h4&gt;ChatGPT的广泛采用引发了对其滥用的担忧，凸显了对AI生成文本进行可靠检测的必要性。当前词汇级检测器容易受到同义改写或简单提示的影响，容易受到ChatGPT词汇级模式和训练数据内容引起的偏差影响，在修改后的文本上性能下降，并且通常需要大型模型或在线LLM交互。为了解决这些问题，我们引入了一项新任务，用于检测原始和PSP修改后的AI生成文本，并提出了一个轻量级框架，该框架根据文本的内部结构进行分类，这种结构在词汇层面变化时保持不变。我们的方法使用预训练语言模型编码句子嵌入，并通过注意力机制建模它们之间的关系。我们采用对比学习来减轻自回归生成的嵌入偏差，并结合因果图和反事实方法将结构特征与主题相关偏差分离。在两个精心策划的数据集（包括摘要比较和修改后的生活常见问题解答）上的实验验证了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread adoption of ChatGPT has raised concerns about its misuse,highlighting the need for robust detection of AI-generated text. Currentword-level detectors are vulnerable to paraphrasing or simple prompts (PSP),suffer from biases induced by ChatGPT's word-level patterns (CWP) and trainingdata content, degrade on modified text, and often require large models oronline LLM interaction. To tackle these issues, we introduce a novel task todetect both original and PSP-modified AI-generated texts, and propose alightweight framework that classifies texts based on their internal structure,which remains invariant under word-level changes. Our approach encodes sentenceembeddings from pre-trained language models and models their relationships viaattention. We employ contrastive learning to mitigate embedding biases fromautoregressive generation and incorporate a causal graph with counterfactualmethods to isolate structural features from topic-related biases. Experimentson two curated datasets, including abstract comparisons and revised life FAQs,validate the effectiveness of our method.</description>
      <author>example@mail.com (Mo Mu, Dianqiao Lei, Chang Li)</author>
      <guid isPermaLink="false">2509.18535v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning</title>
      <link>http://arxiv.org/abs/2509.18504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种在双曲空间中进行的从粗到细的小样本类增量学习方法，通过将特征提取器嵌入到双曲空间中，并引入双曲对比损失和双曲全连接层，有效提高了粗类和细类的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;在机器学习领域，双曲空间相比传统的欧几里得空间，在表示层次数据方面具有更好的表示能力。&lt;h4&gt;目的&lt;/h4&gt;研究专注于从粗到细的小样本类增量学习(C2FSCIL)任务，旨在提高粗类和细类的分类准确率。&lt;h4&gt;方法&lt;/h4&gt;采用Knowe方法，对比学习粗类标签并归一化冻结细类分类器权重；将特征提取器嵌入到双曲空间的Poincaré球模型中；引入双曲对比损失和双曲全连接层；在双曲空间中实现最大熵分布以生成增强特征，减轻小样本训练中的过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在C2FSCIL基准测试上的实验表明，该方法有效提高了粗类和细类的准确率。&lt;h4&gt;结论&lt;/h4&gt;通过将特征提取器嵌入到双曲空间中，并结合特定的双曲空间优化技术，该方法能够有效处理C2FSCIL任务，提高分类准确率。&lt;h4&gt;翻译&lt;/h4&gt;在机器学习领域，与传统的欧几里得空间相比，双曲空间在表示层次数据方面展现出优越的表示能力。这项工作专注于从粗到细的小样本类增量学习(C2FSCIL)任务。我们的研究遵循了Knowe方法，该方法对比学习粗类标签，随后在嵌入空间中对已学习细类的分类器权重进行归一化和冻结。为了更好地解释'从粗到细'的范式，我们提出将特征提取器嵌入到双曲空间中。具体来说，我们采用了双曲空间的Poincaré球模型，使特征提取器能够将输入图像转换为Poincaré球内的特征向量，而不是欧几里得空间中的向量。我们进一步引入了双曲对比损失和双曲全连接层，以促进双曲空间中的模型优化和分类。此外，为了在小样本条件下提高性能，我们在双曲空间中实现了最大熵分布，以估计细类特征向量的概率分布，从而从该分布生成增强特征，减轻在有限样本训练过程中的过拟合。在C2FSCIL基准测试上的实验表明，我们的方法有效提高了粗类和细类的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of machine learning, hyperbolic space demonstrates superiorrepresentation capabilities for hierarchical data compared to conventionalEuclidean space. This work focuses on the Coarse-To-Fine Few-ShotClass-Incremental Learning (C2FSCIL) task. Our study follows the Knoweapproach, which contrastively learns coarse class labels and subsequentlynormalizes and freezes the classifier weights of learned fine classes in theembedding space. To better interpret the "coarse-to-fine" paradigm, we proposeembedding the feature extractor into hyperbolic space. Specifically, we employthe Poincar\'e ball model of hyperbolic space, enabling the feature extractorto transform input images into feature vectors within the Poincar\'e ballinstead of Euclidean space. We further introduce hyperbolic contrastive lossand hyperbolic fully-connected layers to facilitate model optimization andclassification in hyperbolic space. Additionally, to enhance performance underfew-shot conditions, we implement maximum entropy distribution in hyperbolicspace to estimate the probability distribution of fine-class feature vectors.This allows generation of augmented features from the distribution to mitigateoverfitting during training with limited samples. Experiments on C2FSCILbenchmarks show that our method effectively improves both coarse and fine classaccuracies.</description>
      <author>example@mail.com (Jiaxin Dai, Xiang Xiang)</author>
      <guid isPermaLink="false">2509.18504v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach</title>
      <link>http://arxiv.org/abs/2509.18309v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型图神经网络方法，专门用于手语手势识别，通过分离时间动态与静态手势配置，解决了手势识别中的关键挑战，并在新建立的基准测试中显著提高了识别准确率。&lt;h4&gt;背景&lt;/h4&gt;手势在手语中扮演基础性音位角色，美国手语约使用50种不同手势形状。然而，现有计算方法很少明确建模手势形状，限制了识别准确性和语言学分析。&lt;h4&gt;目的&lt;/h4&gt;引入一种新型图神经网络，将时间动态与静态手势配置分离，解决手势识别中的关键挑战，包括细微的类别区分和时间变化。&lt;h4&gt;方法&lt;/h4&gt;使用基于解剖学的图结构，结合对比学习技术，创建手势序列结构化识别的基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;在37个手势类别上实现了46%的准确率，显著优于基线方法的25%准确率。&lt;h4&gt;结论&lt;/h4&gt;该方法为手语计算分析提供了新的可能性，显著提高了手势识别的准确性。&lt;h4&gt;翻译&lt;/h4&gt;手势在手语中扮演基础性音位角色，美国手语约使用50种不同手势形状。然而，计算方法很少明确建模手势形状，限制了识别准确性和语言学分析。我们引入了一种新型图神经网络，将时间动态与静态手势配置分离。我们的方法结合基于解剖学的图结构和对比学习，解决了手势识别中的关键挑战，包括细微的类别区分和时间变化。我们建立了手势序列结构化识别的第一个基准测试，在37个手势类别上实现了46%的准确率（基线方法仅达到25%）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Handshapes serve a fundamental phonological role in signed languages, withAmerican Sign Language employing approximately 50 distinct shapes.However,computational approaches rarely model handshapes explicitly, limitingboth recognition accuracy and linguistic analysis.We introduce a novel graphneural network that separates temporal dynamics from static handshapeconfigurations. Our approach combines anatomically-informed graph structureswith contrastive learning to address key challenges in handshape recognition,including subtle interclass distinctions and temporal variations. We establishthe first benchmark for structured handshape recognition in signing sequences,achieving 46% accuracy across 37 handshape classes (with baseline methodsachieving 25%).</description>
      <author>example@mail.com (Alessa Carbo, Eric Nalisnick)</author>
      <guid isPermaLink="false">2509.18309v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction</title>
      <link>http://arxiv.org/abs/2509.18284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的多模态学习框架，结合增强的模态丢弃和对比学习，解决医学诊断中的模态不平衡和缺失问题，在仅有一个模态可用的情况下表现优异。&lt;h4&gt;背景&lt;/h4&gt;医学诊断越来越多地利用多模态数据，机器学习模型需要有效融合异构信息，同时对缺失模态保持鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多模态学习框架，结合增强的模态丢弃和对比学习，解决现实世界中的模态不平衡和缺失问题。&lt;h4&gt;方法&lt;/h4&gt;引入可学习的模态标记来改进缺失感知的模态融合，并将传统的单模态对比目标与融合的多模态表示相结合。在大型临床数据集上验证该框架，用于疾病检测和预测任务，包括视觉和表格模态。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法最先进，特别是在仅有一个模态可用的情况下。此外，还展示了其与最近的CT基础模型成功集成的适应性。&lt;h4&gt;结论&lt;/h4&gt;该方法在多模态学习中具有有效性、效率和泛化性，为现实世界临床应用提供了可扩展、低成本的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着医学诊断越来越多地利用多模态数据，机器学习模型被期望能够有效融合异构信息，同时对缺失模态保持鲁棒性。在这项工作中，我们提出了一种新的多模态学习框架，结合增强的模态丢弃和对比学习，以解决模态不平衡和缺失等现实世界限制。我们的方法引入了可学习的模态标记，以改进对缺失模态的感知融合，并通过融合的多模态表示增强了传统的单模态对比目标。我们在大规模临床数据集上验证了我们的框架，用于疾病检测和预测任务，包括视觉和表格模态。实验结果表明，我们的方法达到了最先进的性能，特别是在仅有一个模态可用的挑战性和实际场景中。此外，我们通过成功整合最近的CT基础模型展示了其适应性。我们的发现突显了我们的方法在多模态学习中的有效性、效率和泛化性，为现实世界临床应用提供了具有显著潜力的可扩展、低成本解决方案。代码可在 https://github.com/omron-sinicx/medical-modality-dropout 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As medical diagnoses increasingly leverage multimodal data, machine learningmodels are expected to effectively fuse heterogeneous information whileremaining robust to missing modalities. In this work, we propose a novelmultimodal learning framework that integrates enhanced modalities dropout andcontrastive learning to address real-world limitations such as modalityimbalance and missingness. Our approach introduces learnable modality tokensfor improving missingness-aware fusion of modalities and augments conventionalunimodal contrastive objectives with fused multimodal representations. Wevalidate our framework on large-scale clinical datasets for disease detectionand prediction tasks, encompassing both visual and tabular modalities.Experimental results demonstrate that our method achieves state-of-the-artperformance, particularly in challenging and practical scenarios where only asingle modality is available. Furthermore, we show its adaptability throughsuccessful integration with a recent CT foundation model. Our findingshighlight the effectiveness, efficiency, and generalizability of our approachfor multimodal learning, offering a scalable, low-cost solution withsignificant potential for real-world clinical applications. The code isavailable at https://github.com/omron-sinicx/medical-modality-dropout.</description>
      <author>example@mail.com (Yi Gu, Kuniaki Saito, Jiaxin Ma)</author>
      <guid isPermaLink="false">2509.18284v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.18613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MLF-4DRCNet是一种新颖的两阶段框架，通过多级融合4D雷达和相机图像进行3D目标检测，解决了现有方法忽略雷达点云稀疏性和不完整几何特性的问题，在多个数据集上达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;4D毫米波雷达因其成本效益和稳健性在自动驾驶中受到认可，但其点云存在显著稀疏性和噪声问题，限制了独立应用。现有4D雷达-相机融合方法多采用为LiDAR-相机融合设计的鸟瞰图融合范式，忽略了雷达的固有缺点，如点云稀疏性和不完整几何特性，且融合局限于粗粒度场景级集成。&lt;h4&gt;目的&lt;/h4&gt;解决现有融合方法忽略雷达点云稀疏性和不完整几何特性的问题，提出一种更有效的4D雷达和相机图像的多级融合框架用于3D目标检测。&lt;h4&gt;方法&lt;/h4&gt;提出MLF-4DRCNet两阶段框架，整合点级、场景级和提案级多模态信息。包含三个关键组件：1)增强雷达点编码器(ERPE)模块：用2D图像实例对雷达点云进行密度处理并通过三重注意力体素特征编码器编码为体素；2)分层场景融合池化(HSFP)模块：使用可变形注意力动态集成多尺度体素特征与2D图像特征，捕获场景上下文并进行池化；3)提案级融合增强(PLFE)模块：融合图像特征细化区域提案，并与HSFP的池化特征进一步集成。&lt;h4&gt;主要发现&lt;/h4&gt;在View-of-Delft(VoD)和TJ4DRadSet数据集上的实验表明，MLF-4DRCNet达到最先进性能，特别是在VoD数据集上，其性能可与基于LiDAR的模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;MLF-4DRCNet通过多级融合方法有效解决了4D雷达点云稀疏性和噪声问题，在3D目标检测任务中表现出色，性能接近基于LiDAR的模型。&lt;h4&gt;翻译&lt;/h4&gt;新兴的4D毫米波雷达通过测量物体的距离、方位角、仰角和多普勒速度，在自动驾驶中以成本效益和稳健性而闻名。然而，其点云存在显著的稀疏性和噪声，限制了其在3D目标检测中的独立应用。最近的4D雷达-相机融合方法提供了有效的感知能力。然而，大多数现有方法采用最初为LiDAR-相机融合设计的显式鸟瞰图融合范式，忽略了雷达的固有缺点。具体来说，它们忽视了雷达点云的稀疏和不完整几何特性，并将融合限制在粗粒度的场景级集成。为解决这些问题，我们提出了MLF-4DRCNet，一种通过4D雷达和相机图像多级融合进行3D目标检测的新颖两阶段框架。我们的模型整合了点级、场景级和提案级多模态信息，实现全面特征表示。它包含三个关键组件：增强雷达点编码器(ERPE)模块、分层场景融合池化(HSFP)模块和提案级融合增强(PLFE)模块。ERPE在点级操作，使用2D图像实例对雷达点云进行密度处理，并通过 proposed 三重注意力体素特征编码器将它们编码为体素。HSFP使用可变形注意力动态集成多尺度体素特征与2D图像特征，捕获场景上下文并对融合特征进行池化。PLFE通过融合图像特征来细化区域提案，并与来自HSFP的池化特征进一步集成。在View-of-Delft(VoD)和TJ4DRadSet数据集上的实验结果表明，MLF-4DRCNet达到了最先进的性能。值得注意的是，它在VoD数据集上获得了可与基于LiDAR的模型相媲美的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决4D毫米波雷达点云的稀疏性和噪声问题，以及现有雷达-相机融合方法过度依赖鸟瞰图(BEV)范式而忽视雷达固有特性的问题。这个问题在自动驾驶领域至关重要，因为准确的3D目标检测是自动驾驶系统的基础，而LiDAR传感器虽然精度高但成本昂贵且在恶劣天气下性能下降，4D雷达具有成本效益和鲁棒性优势，但其点云稀疏问题限制了单独应用，因此雷达-相机融合结合了两种传感器的优势，对实现可靠的环境感知具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了4D雷达点云的局限性及现有融合方法的问题，然后提出多级融合框架分别在点级、场景级和提案级进行信息融合。设计过程中借鉴了现有LiDAR-相机融合的中间融合范式、可变形注意力机制以及点云密集化方法，但针对雷达特性进行了专门改进，设计了三重注意力体素编码器和多级融合策略，以克服雷达点云的稀疏性和噪声问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多级融合（点级、场景级和提案级）充分利用4D雷达和相机的互补信息，克服雷达点云的稀疏性和噪声。整体流程包括：1)ERPE模块使用图像实例引导增强雷达点云密度并通过TA-VFE编码为体素；2)图像编码器提取多尺度图像特征；3)提案生成模块处理体素特征并生成3D区域提案；4)HSFP模块使用可变形注意力融合多尺度体素与图像特征；5)PLFE模块融合提案级图像特征并与HSFP特征集成；6)检测头进行边界框回归和置信度预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多级融合框架实现点级、场景级和提案级全面特征融合；2)ERPE模块结合图像实例引导和TA-VFE处理雷达点云；3)HSFP模块使用可变形注意力避免显式BEV变换；4)PLFE模块增强小目标检测能力。相比之前的工作，MLF-4DRCNet专门针对雷达特性设计，避免了使用雷达几何信息指导深度估计可能引入的误差，在三个层次上整合信息而非仅场景级，并通过注意力机制有效处理原始点和虚拟点的特征差异。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MLF-4DRCNet通过创新的多级融合框架，有效结合了4D雷达和相机的互补优势，显著提升了自动驾驶场景中3D目标检测的准确性和鲁棒性，实现了可与LiDAR相媲美的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging 4D millimeter-wave radar, measuring the range, azimuth,elevation, and Doppler velocity of objects, is recognized for itscost-effectiveness and robustness in autonomous driving. Nevertheless, itspoint clouds exhibit significant sparsity and noise, restricting its standaloneapplication in 3D object detection. Recent 4D radar-camera fusion methods haveprovided effective perception. Most existing approaches, however, adoptexplicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camerafusion, neglecting radar's inherent drawbacks. Specifically, they overlook thesparse and incomplete geometry of radar point clouds and restrict fusion tocoarse scene-level integration. To address these problems, we proposeMLF-4DRCNet, a novel two-stage framework for 3D object detection viamulti-level fusion of 4D radar and camera images. Our model incorporates thepoint-, scene-, and proposal-level multi-modal information, enablingcomprehensive feature representation. It comprises three crucial components:the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene FusionPooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.Operating at the point-level, ERPE densities radar point clouds with 2D imageinstances and encodes them into voxels via the proposed Triple-Attention VoxelFeature Encoder. HSFP dynamically integrates multi-scale voxel features with 2Dimage features using deformable attention to capture scene context and adoptspooling to the fused features. PLFE refines region proposals by fusing imagefeatures, and further integrates with the pooled features from HSFP.Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasetsdemonstrate that MLF-4DRCNet achieves the state-of-the-art performance.Notably, it attains performance comparable to LiDAR-based models on the VoDdataset.</description>
      <author>example@mail.com (Yuzhi Wu, Li Xiao, Jun Liu, Guangfeng Jiang, XiangGen Xia)</author>
      <guid isPermaLink="false">2509.18613v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models</title>
      <link>http://arxiv.org/abs/2509.16832v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;L2M-Reg是一种基于平面的精细配准方法，用于解决LiDAR点云与语义3D城市模型在建筑物级别的精确配准问题，考虑了模型不确定性，并通过三个关键步骤实现，实验证明其比现有方法更准确且计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云与语义3D城市模型的精确配准是城市数字孪生和下游任务（如数字施工、变化检测和模型精化）的基础。&lt;h4&gt;目的&lt;/h4&gt;解决在单个建筑物级别实现准确的LiDAR到模型配准的挑战，特别是在考虑LoD2级别的语义3D城市模型中的泛化不确定性时。&lt;h4&gt;方法&lt;/h4&gt;提出L2M-Reg，一种基于平面的精细配准方法，明确考虑模型不确定性，包括三个关键步骤：建立可靠的平面对应关系、构建伪平面约束的Gauss-Helmert模型、自适应估计垂直平移。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集上的实验表明，L2M-Reg比现有的基于ICP和平面的方法更准确且计算效率更高。&lt;h4&gt;结论&lt;/h4&gt;L2M-Reg在存在模型不确定性时，为LiDAR到模型配准提供了新颖的建筑物级解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的LiDAR（光探测和测距）点云与语义3D城市模型之间的配准是城市数字孪生中的一个基础性话题，也是数字施工、变化检测和模型精化等下游任务的前提条件。然而，在单个建筑物级别实现准确的LiDAR到模型配准仍然具有挑战性，特别是由于细节级别2（LoD2）的语义3D城市模型中的泛化不确定性。本文通过提出L2M-Reg（一种基于平面的精细配准方法）来解决这个问题，该方法明确考虑了模型不确定性。L2M-Reg包括三个关键步骤：建立可靠的平面对应关系、构建伪平面约束的Gauss-Helmert模型、自适应估计垂直平移。在三个真实世界数据集上的实验表明，L2M-Reg比现有的基于ICP和平面的方法更准确且计算效率更高。总体而言，当存在模型不确定性时，L2M-Reg为LiDAR到模型配准提供了一种新颖的建筑物级解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决建筑物级别激光雷达点云与LoD2语义三维城市模型的精确配准问题，特别是处理模型中存在的不确定性。这个问题很重要，因为准确配准是城市数字孪生和数字施工、变化检测等下游任务的基础，而现有LoD2模型存在固有的不确定性（如墙体模型与实际立面之间的水平偏移），在高精度应用场景中会导致明显误差，而之前的研究大多忽略了这种不确定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到LoD2模型存在不确定性，特别是建筑墙体与实际立面之间的偏移问题。他们分析了现有点云配准方法（如ICP变种）和基于平面的方法（如PLADE、Scantra），发现这些方法都未考虑模型不确定性。作者借鉴了平面特征提取技术和Gauss-Helmert模型进行参数估计，但进行了创新改进，设计了围绕三个关键步骤的方法：建立可靠平面对应、构建伪平面约束模型、自适应估计垂直平移。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是明确考虑LoD2模型的不确定性，利用建筑墙基（而非上层立面）作为代表性区域进行配准，通过2D-3D解耦策略防止低质量地面数据影响精度，并利用模型语义信息直接建立对应关系。整体流程包括：1）数据预处理（关联点云与墙体表面）；2）建立可靠平面对应（定位墙基区域并提取代表性平面段）；3）构建伪平面约束的高斯-赫尔默特模型；4）自适应估计垂直平移（利用DTM数据计算垂直偏差）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首次在建筑物级别配准中明确考虑LoD2模型不确定性；2）提出L2M-Reg方法，结合可靠平面对应、伪平面约束和自适应垂直估计；3）引入2D-3D解耦参数估计策略；4）开发轻量级平面对应策略，利用模型语义信息。相比之前工作，L2M-Reg不同于传统ICP方法（考虑不确定性而非点对点配准），也不同于其他平面方法（直接利用语义信息而非特征匹配），且专门处理了模型不确定性，引入了伪平面约束解决地面数据质量问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; L2M-Reg首次提出了一种在建筑物级别配准激光雷达点云和语义三维城市模型时明确考虑模型不确定性的创新方法，通过利用建筑墙基、引入伪平面约束和解耦策略，实现了比现有方法更精确高效的配准性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate registration between LiDAR (Light Detection and Ranging) pointclouds and semantic 3D city models is a fundamental topic in urban digitaltwinning and a prerequisite for downstream tasks, such as digital construction,change detection and model refinement. However, achieving accurateLiDAR-to-Model registration at individual building level remains challenging,particularly due to the generalization uncertainty in semantic 3D city modelsat the Level of Detail 2 (LoD2). This paper addresses this gap by proposingL2M-Reg, a plane-based fine registration method that explicitly accounts formodel uncertainty. L2M-Reg consists of three key steps: establishing reliableplane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,and adaptively estimating vertical translation. Experiments on three real-worlddatasets demonstrate that L2M-Reg is both more accurate and computationallyefficient than existing ICP-based and plane-based methods. Overall, L2M-Regprovides a novel building-level solution regarding LiDAR-to-Model registrationwhen model uncertainty is present.</description>
      <author>example@mail.com (Ziyang Xu, Benedikt Schwab, Yihui Yang, Thomas H. Kolbe, Christoph Holst)</author>
      <guid isPermaLink="false">2509.16832v2</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies</title>
      <link>http://arxiv.org/abs/2509.19258v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review: npj Digital Medicine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图放射组学学习(GrRAiL)描述符，用于在临床MRI扫描上表征实体肿瘤中的病变内异质性，通过考虑病变内的高阶空间关系，能够在多种临床应用中可靠地区分混淆性病理与恶性肿瘤，并优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;实体肿瘤中的一个重大挑战是在常规影像上可靠地区分混淆性病理与恶性肿瘤。现有的放射组学方法在CT/MRI上寻找病变异质性的替代标志物，但它们通常在感兴趣区域内聚合特征，忽略了不同强度组成之间的复杂空间关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的图放射组学学习(GrRAiL)描述符，用于在临床MRI扫描上表征病变内异质性，以提高区分不同病理类型的能力。&lt;h4&gt;方法&lt;/h4&gt;GrRAiL首先使用体素级放射测量识别子区域簇，然后计算图论指标来量化簇之间的空间关联。生成的加权图编码了感兴趣区域内的高阶空间关系。研究在947名受试者中评估了该方法，涵盖三种使用情况：区分胶质母细胞瘤和脑转移瘤中的肿瘤复发与辐射效应，以及将胰腺导管内乳头状黏液性肿瘤分层为不同风险等级。&lt;h4&gt;主要发现&lt;/h4&gt;在多机构环境中，GrRAiL始终优于图神经网络、纹理放射组学和强度图分析等基线方法。在胶质母细胞瘤中，对于复发与假进展的交叉验证和测试准确率分别为89%和78%，比比较方法高出10%以上的测试准确率提升。在脑转移瘤中，对于复发与辐射坏死的准确率分别为84%和74%(改善超过13%)。对于胰腺导管内乳头状黏液性肿瘤风险分层，准确率分别为84%和75%，显示出超过10%的改善。&lt;h4&gt;结论&lt;/h4&gt;GrRAiL通过考虑病变内的高阶空间关系，能够更好地表征病变内异质性，并在多种临床应用中显示出优越的性能，为实体肿瘤的影像诊断提供了新的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;实体肿瘤中的一个重大挑战是在常规影像上可靠地区分混淆性病理与恶性肿瘤。虽然放射组学方法在CT/MRI上寻找病变异质性的替代标志物，但许多方法在感兴趣区域内聚合特征，忽略了不同强度组成之间的复杂空间关系。我们提出了一种新的图放射组学学习(GrRAiL)描述符，用于在临床MRI扫描上表征病变内异质性。GrRAiL首先使用体素级放射测量识别子区域簇，然后计算图论指标来量化簇之间的空间关联。生成的加权图编码了感兴趣区域内的高阶空间关系，旨在可靠地捕获病变内异质性并区分混淆性病理与恶性肿瘤。为了评估有效性和临床可行性，GrRAiL在n=947名受试者中进行了评估，涵盖三种使用情况：区分胶质母细胞瘤(GBM，n=106)和脑转移瘤(n=233)中的肿瘤复发与辐射效应，以及将胰腺导管内乳头状黏液性肿瘤(IPMNs)分层为无风险+低风险与高风险(n=608)。在多机构环境中，GrRAiL始终优于最先进的基线方法 - 图神经网络(GNNs)、纹理放射组学和强度图分析。在GBM中，对于复发与假进展的交叉验证(CV)和测试准确率分别为89%和78%，比比较方法高出10%以上的测试准确率提升。在脑转移瘤中，对于复发与辐射坏死的CV和测试准确率分别为84%和74%(改善超过13%)。对于IPMN风险分层，CV和测试准确率分别为84%和75%，显示出超过10%的改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A significant challenge in solid tumors is reliably distinguishingconfounding pathologies from malignant neoplasms on routine imaging. Whileradiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,many aggregate features across the region of interest (ROI) and miss complexspatial relationships among varying intensity compositions. We present a newGraph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesionalheterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters ofsub-regions using per-voxel radiomic measurements, then (2) computesgraph-theoretic metrics to quantify spatial associations among clusters. Theresulting weighted graphs encode higher-order spatial relationships within theROI, aiming to reliably capture ILH and disambiguate confounding pathologiesfrom malignancy. To assess efficacy and clinical feasibility, GrRAiL wasevaluated in n=947 subjects spanning three use cases: differentiating tumorrecurrence from radiation effects in glioblastoma (GBM; n=106) and brainmetastasis (n=233), and stratifying pancreatic intraductal papillary mucinousneoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutionalsetting, GrRAiL consistently outperformed state-of-the-art baselines - GraphNeural Networks (GNNs), textural radiomics, and intensity-graph analysis. InGBM, cross-validation (CV) and test accuracies for recurrence vspseudo-progression were 89% and 78% with &gt;10% test-accuracy gains overcomparators. In brain metastasis, CV and test accuracies for recurrence vsradiation necrosis were 84% and 74% (&gt;13% improvement). For IPMN riskstratification, CV and test accuracies were 84% and 75%, showing &gt;10%improvement.</description>
      <author>example@mail.com (Dheerendranath Battalapalli, Apoorva Safai, Maria Jaramillo, Hyemin Um, Gustavo Adalfo Pineda Ortiz, Ulas Bagci, Manmeet Singh Ahluwalia, Marwa Ismail, Pallavi Tiwari)</author>
      <guid isPermaLink="false">2509.19258v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying</title>
      <link>http://arxiv.org/abs/2509.19084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AxelGNN，一种受Axelrod文化传播模型启发的新型图神经网络架构，解决了GNNs面临的特征过度平滑、异质关系管理和特征向量处理限制等问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在各种基于图的任务中表现出色，但存在基本限制：特征过度平滑导致节点表示难以区分；难以有效管理异质关系；将特征向量作为不可分割单元处理限制了灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决图神经网络面临的基本限制问题。&lt;h4&gt;方法&lt;/h4&gt;提出AxelGNN架构，包含相似性门控概率交互、特征级复制机制和全局极化保持，通过双稳态收敛动力学处理同质和异质图。&lt;h4&gt;主要发现&lt;/h4&gt;AxelGNN在节点分类和影响估计基准实验中，在各种具有不同同质性-异质性特性的图结构上一致优于或匹配最先进的GNN方法。&lt;h4&gt;结论&lt;/h4&gt;AxelGNN通过统一框架成功解决了GNNs的多个基本限制，在各种图结构上都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已在各种基于图的任务中显示出显著成功。然而，它们面临一些基本限制：特征过度平滑可能导致深层网络中的节点表示变得难以区分，它们难以有效管理连接节点差异显著的异质关系，并且它们将整个特征向量作为不可分割单元处理，这限制了灵活性。我们寻求解决这些限制。我们提出了AxelGNN，一种受Axelrod文化传播模型启发的新型GNN架构，通过统一框架解决这些限制。AxelGNN纳入了基于节点相似性自适应促进收敛或发散的相似性门控概率交互，实现了段级别细粒度特征聚合的特征级复制机制，并保持全局极化以在多个表示集群中保留节点区分度。模型的双稳态收敛动力学自然地在单一架构中处理同质图和异质图。在节点分类和影响估计基准上的大量实验表明，AxelGNN在不同具有不同同质性-异质性特性的图结构上一致优于或匹配最先进的GNN方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable success acrossvarious graph-based tasks. However, they face some fundamental limitations:feature oversmoothing can cause node representations to becomeindistinguishable in deeper networks, they struggle to effectively manageheterogeneous relationships where connected nodes differ significantly, andthey process entire feature vectors as indivisible units, which limitsflexibility. We seek to address these limitations. We propose AxelGNN, a novelGNN architecture inspired by Axelrod's cultural dissemination model thataddresses these limitations through a unified framework. AxelGNN incorporatessimilarity-gated probabilistic interactions that adaptively promote convergenceor divergence based on node similarity, implements trait-level copyingmechanisms for fine-grained feature aggregation at the segment level, andmaintains global polarization to preserve node distinctiveness across multiplerepresentation clusters. The model's bistable convergence dynamics naturallyhandle both homophilic and heterophilic graphs within a single architecture.Extensive experiments on node classification and influence estimationbenchmarks demonstrate that AxelGNN consistently outperforms or matchesstate-of-the-art GNN methods across diverse graph structures with varyinghomophily-heterophily characteristics.</description>
      <author>example@mail.com (Asela Hevapathige)</author>
      <guid isPermaLink="false">2509.19084v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>A Methodological Study on Data Representation for Machine Learning Modelling of Thermal Conductivity of Rare-Earth Oxides</title>
      <link>http://arxiv.org/abs/2509.18951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了传统机器学习方法和基于图神经网络的QSAR模型在热障涂层材料研究中的表现，发现图神经网络模型显著优于传统方法，为材料科学研究提供了更有效的预测工具。&lt;h4&gt;背景&lt;/h4&gt;定量结构-活性关系(QSAR)建模在材料科学中广泛应用于预测材料性质和提取有用描述符。在热障涂层(TBC)研究中，QSAR可以显著缩短通常需要数年的实验发现周期。然而，传统基于手工设计描述符的机器学习方法受到材料基本晶体结构的限制，限制了数据集的大小。&lt;h4&gt;目的&lt;/h4&gt;研究旨在比较传统机器学习模型与基于图神经网络的模型在高熵稀土烧绿石氧化物热障涂层材料性质预测中的表现，探索更有效的QSAR建模方法。&lt;h4&gt;方法&lt;/h4&gt;研究比较了随机森林(RF)和高斯过程(GP)模型与基于图表示的晶体图卷积神经网络(CGCNN)模型。同时探索了两种不同的数据增强方法，其中一种仅适用于基于图的表示，以解决数据量有限的问题。&lt;h4&gt;主要发现&lt;/h4&gt;CGCNN模型显著优于RF和GP模型，表明基于图的表示方法能够更有效地编码材料结构信息，提高预测准确性。&lt;h4&gt;结论&lt;/h4&gt;基于图的表示方法在热障涂层研究的QSAR建模中具有巨大潜力，能够克服传统方法的局限性，为材料科学研究提供更强大的预测工具。&lt;h4&gt;翻译&lt;/h4&gt;定量结构-活性关系(QSAR)建模广泛应用于材料科学，用于预测感兴趣的性质并提取测量性质的有用描述符。在热障涂层(TBC)中，QSAR可以显著缩短可能需要数年的实验发现周期。尽管机器学习方法常用于QSAR，但其性能取决于数据质量和实例的表示方式。基于已知材料性质的传统手工设计描述符仅限于表示具有相同基本晶体结构的材料，限制了数据集的大小。相比之下，图神经网络提供了更具表现力的表示，能够编码晶体晶格中的原子位置和键合。在本研究中，我们比较了使用文献中手工设计描述符训练的随机森林(RF)和高斯过程(GP)模型与基于图表示的模型，使用晶体图卷积神经网络(CGCNN)处理高熵稀土烧绿石氧化物。还探索了两种不同的数据增强方法，以解决数据量有限的问题，其中一种仅适用于基于图的表示。我们的研究结果表明，CGCNN模型显著优于RF和GP模型，强调了基于图的表示在TBC研究中增强QSAR建模的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantitative structure-activity relationship (QSAR) modelling is widelyemployed in materials sci- ence to predict properties of interest and extractuseful descriptors for measured properties. In thermal barrier coatings (TBC),QSAR can significantly shorten the experimental discovery cycle, which can takeyears. Although machine learning methods are commonly employed for QSAR, theirperformance depends on the data quality and how instances are represented.Traditional, hand-crafted descriptors based on known material properties arelimited to represent materials that share the same basic crystal structure,limited the size of the dataset. By contrast, graph neural networks offer amore expressive representation, encoding atomic positions and bonds in thecrystal lattice. In this study, we compare Random Forest (RF) and GaussianProcess (GP) models trained on hand-crafted descriptors from the literaturewith graph-based representations for high-entropy, rare-earth pyrochlore oxidesusing the Crystal Graph Convolutional Neural Network (CGCNN). Two differenttypes of augmentation methods are also explored to account for the limited datasize, one of which is only applicable to graph-based representations. Ourfindings show that the CGCNN model substantially outperforms the RF and GPmodels, underscoring the potential of graph-based representations for enhancedQSAR modelling in TBC research.</description>
      <author>example@mail.com (Amiya Chowdhury, Acacio Rincón Romero, Eduardo Aguilar-Bejarano, Halar Memon, Grazziela Figueredo, Tanvir Hussain)</author>
      <guid isPermaLink="false">2509.18951v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Heterophily in Graph-level Tasks</title>
      <link>http://arxiv.org/abs/2509.18893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accectped by NeurIPS 2025 Workshop, New Perspectives in Advancing  Graph Machine Learning (NPGML)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文首次分析了图级学习中的异质性现象，通过理论洞察和实证验证，发现motif检测需要混合频率动力学而非频率主导机制，并提出了频率自适应模型优于频率主导模型的结论。&lt;h4&gt;背景&lt;/h4&gt;异质性在节点级任务中已被广泛研究，但对图级任务的影响尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;首次对图级学习中的异质性进行分析，结合理论洞察和实证验证，建立对图级学习中异质性的新理论理解。&lt;h4&gt;方法&lt;/h4&gt;引入图级标记方案分类法，专注于局部结构标记中的基于motif的任务；使用基于能量的梯度流分析；在合成数据集和真实世界分子属性预测上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;motif检测需要混合频率动力学来保持多个频谱组件的灵活性，motif目标本质上与全局频率主导不兼容，频率自适应模型优于频率主导模型。&lt;h4&gt;结论&lt;/h4&gt;这项工作建立了对图级学习中异质性的新的理论理解，并为设计有效的GNN架构提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;尽管异质性在节点级任务中已被广泛研究，其对图级任务的影响仍不清楚。我们首次对图级学习中的异质性进行了分析，结合理论洞察与实证验证。我们首先引入了图级标记方案的分类法，并专注于局部结构标记中流行的基于motif的任务。通过基于能量的梯度流分析，我们揭示了一个关键洞察：与节点级任务中的频率主导机制不同，motif检测需要混合频率动力学来保持多个频谱组件的灵活性。我们的理论表明，motif目标本质上与全局频率主导不兼容，需要不同的架构考虑。在具有可控异质性的合成数据集和真实世界分子属性预测上的实验支持了我们的发现，表明频率自适应模型优于频率主导模型。这项工作建立了对图级学习中异质性的新理论理解，并为设计有效的GNN架构提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While heterophily has been widely studied in node-level tasks, its impact ongraph-level tasks remains unclear. We present the first analysis of heterophilyin graph-level learning, combining theoretical insights with empiricalvalidation. We first introduce a taxonomy of graph-level labeling schemes, andfocus on motif-based tasks within local structure labeling, which is a popularlabeling scheme. Using energy-based gradient flow analysis, we reveal a keyinsight: unlike frequency-dominated regimes in node-level tasks, motifdetection requires mixed-frequency dynamics to remain flexible across multiplespectral components. Our theory shows that motif objectives are inherentlymisaligned with global frequency dominance, demanding distinct architecturalconsiderations. Experiments on synthetic datasets with controlled heterophilyand real-world molecular property prediction support our findings, showing thatfrequency-adaptive model outperform frequency-dominated models. This workestablishes a new theoretical understanding of heterophily in graph-levellearning and offers guidance for designing effective GNN architectures.</description>
      <author>example@mail.com (Qinhan Hou, Yilun Zheng, Xichun Zhang, Sitao Luan, Jing Tang)</author>
      <guid isPermaLink="false">2509.18893v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction</title>
      <link>http://arxiv.org/abs/2509.18840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可学习重参数化图构建方法(LRGC)用于视觉图神经网络(ViG)，解决了现有方法依赖非参数化统计方法且无法为每个节点选择最佳邻域的问题。&lt;h4&gt;背景&lt;/h4&gt;图像表示学习是计算机视觉中的重要问题。传统方法将图像处理为网格(使用CNN)或视觉令牌序列(使用视觉Transformer)。视觉图神经网络(ViG)将图像视为节点图，提供更直观的图像表示，但面临如何构建最佳节点图的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种不需要超参数搜索且能最佳表示节点间关系的图构建方法，克服现有ViG模型依赖非参数化统计方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出可学习重参数化图构建(LRGC)方法，在每对节点之间应用key-query注意力，使用软阈值重参数化进行边选择，允许使用可微分数学模型进行训练。通过可学习参数选择邻域，消除了聚类或阈值方法带来的偏差，并允许根据训练数据调整每层的阈值。&lt;h4&gt;主要发现&lt;/h4&gt;LRGC方法能够为每个节点选择最佳邻域，通过可学习参数而非超参数来调整阈值，消除了现有方法中的偏差。&lt;h4&gt;结论&lt;/h4&gt;提出的ViG-LRGC方法在ImageNet-1k基准数据集上优于最先进的类似大小ViG模型，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;图像表示学习是计算机视觉中的一个重要问题。传统上，图像被作为网格处理，使用卷积神经网络，或者作为视觉令牌序列处理，使用视觉Transformer。最近，视觉图神经网络(ViG)提出了将图像作为节点图的处理方式，这提供了更直观的图像表示。挑战在于在每个层中构建最能表示节点间关系的节点图，且不需要超参数搜索。文献中的ViG模型依赖于在节点潜在特征上操作的非参数化和不可学习的统计方法来创建图，这可能无法为每个节点选择最佳邻域。从k-NN图构建到超图构建和相似度阈值图构建，这些方法都缺乏提供可学习超参数自由图构建方法的能力。为了克服这些挑战，我们提出了用于视觉图神经网络的可学习重参数化图构建(LRGC)。LRGC在每对节点之间应用key-query注意力，然后使用软阈值重参数化进行边选择，这允许使用可微分数学模型进行训练。使用可学习参数选择邻域消除了先前文献中引入的任何聚类或阈值方法带来的偏差。此外，由于阈值是通过训练可学习的，而不是作为超参数提供给模型，LRGC允许在每个层中将阈值调整为训练数据。我们证明，提出的ViG-LRGC方法在ImageNet-1k基准数据集上优于最先进的类似大小ViG模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image Representation Learning is an important problem in Computer Vision.Traditionally, images were processed as grids, using Convolutional NeuralNetworks or as a sequence of visual tokens, using Vision Transformers.Recently, Vision Graph Neural Networks (ViG) have proposed the treatment ofimages as a graph of nodes; which provides a more intuitive imagerepresentation. The challenge is to construct a graph of nodes in each layerthat best represents the relations between nodes and does not need ahyper-parameter search. ViG models in the literature depend onnon-parameterized and non-learnable statistical methods that operate on thelatent features of nodes to create a graph. This might not select the bestneighborhood for each node. Starting from k-NN graph construction to HyperGraphConstruction and Similarity-Thresholded graph construction, these methods lackthe ability to provide a learnable hyper-parameter-free graph constructionmethod. To overcome those challenges, we present the Learnable ReparameterizedGraph Construction (LRGC) for Vision Graph Neural Networks. LRGC applieskey-query attention between every pair of nodes; then uses soft-thresholdreparameterization for edge selection, which allows the use of a differentiablemathematical model for training. Using learnable parameters to select theneighborhood removes the bias that is induced by any clustering or thresholdingmethods previously introduced in the literature. In addition, LRGC allowstuning the threshold in each layer to the training data since the thresholdsare learnable through training and are not provided as hyper-parameters to themodel. We demonstrate that the proposed ViG-LRGC approach outperformsstate-of-the-art ViG models of similar sizes on the ImageNet-1k benchmarkdataset.</description>
      <author>example@mail.com (Ismael Elsharkawi, Hossam Sharara, Ahmed Rafea)</author>
      <guid isPermaLink="false">2509.18840v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models</title>
      <link>http://arxiv.org/abs/2509.18742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DyGRASP方法，用于处理动态文本属性图(DyTAGs)中的时间演化和文本属性问题。该方法结合了大语言模型和时序图神经网络，有效捕捉了最近和全局时间语义，并在基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;动态文本属性图(DyTAGs)具有随时间演化的图交互和相关文本属性，在现实世界应用中普遍存在。然而，现有的图神经网络(GNN)和大语言模型(LLMs)主要关注静态文本属性图，难以处理DyTAGs的时间演化特性，且忽略了最近-全局时间语义。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法处理DyTAGs时的挑战，特别是捕捉交互文本间的最近语义依赖和节点随时间的全局语义演化，同时提高将LLMs应用于大量且不断演变文本时的效率。&lt;h4&gt;方法&lt;/h4&gt;提出Dynamic Global-Recent Adaptive Semantic Processing (DyGRASP)方法，利用LLMs和时序GNNs高效推理DyTAGs。具体包括：设计以节点为中心的隐式推理方法和滑动窗口机制捕捉最近时间语义；利用显式推理和定制提示及类似RNN的链结构推断长期语义；通过更新和合并层整合最近和全局时间语义及动态图结构信息。&lt;h4&gt;主要发现&lt;/h4&gt;在DyTAG基准测试中，DyGRASP表现出优越性能，在目标节点检索任务中Hit@10指标提高了最多34%。此外，该方法在不同时序GNNs和LLMs上展现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;DyGRASP是一种有效处理DyTAGs的创新方法，能够高效捕捉最近和全局时间语义，具有良好的性能表现和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;动态文本属性图(DyTAGs)的特点是具有时间演化的图交互和相关的文本属性，在现实世界应用中普遍存在。现有方法，如图神经网络(GNN)和大语言模型(LLMs)，大多关注静态文本属性图。将这些现有方法扩展到DyTAGs具有挑战性，因为它们在很大程度上忽略了最近-全局时间语义：交互文本之间的最近语义依赖和节点随时间的全局语义演化。此外，将LLMs应用于DyTAGs中大量且不断演变的文本面临效率问题。为解决这些挑战，我们提出了Dynamic Global-Recent Adaptive Semantic Processing (DyGRASP)，一种利用LLMs和时序GNNs高效有效地推理DyTAGs的新方法。具体而言，我们首先设计了一个以节点为中心的隐式推理方法以及滑动窗口机制，以高效捕捉最近时间语义。此外，为了捕捉节点的全局语义动态，我们利用显式推理和定制提示以及类似RNN的链结构来推断长期语义。最后，我们使用更新和合并层巧妙地整合了最近和全局时间语义以及动态图结构信息。在DyTAG基准上的大量实验证明了DyGRASP的优越性，在目标节点检索任务中Hit@10指标提高了最多34%。此外，DyGRASP在不同时序GNNs和LLMs上表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graphinteractions and associated text attributes, are prevalent in real-worldapplications. Existing methods, such as Graph Neural Networks (GNNs) and LargeLanguage Models (LLMs), mostly focus on static TAGs. Extending these existingmethods to DyTAGs is challenging as they largely neglect the recent-globaltemporal semantics: the recent semantic dependencies among interaction textsand the global semantic evolution of nodes over time. Furthermore, applyingLLMs to the abundant and evolving text in DyTAGs faces efficiency issues. Totackle these challenges, we propose Dynamic Global-Recent Adaptive SemanticProcessing (DyGRASP), a novel method that leverages LLMs and temporal GNNs toefficiently and effectively reason on DyTAGs. Specifically, we first design anode-centric implicit reasoning method together with a sliding window mechanismto efficiently capture recent temporal semantics. In addition, to captureglobal semantic dynamics of nodes, we leverage explicit reasoning with tailoredprompts and an RNN-like chain structure to infer long-term semantics. Lastly,we intricately integrate the recent and global temporal semantics as well asthe dynamic graph structural information using updating and merging layers.Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,achieving up to 34% improvement in Hit@10 for destination node retrieval task.Besides, DyGRASP exhibits strong generalization across different temporal GNNsand LLMs.</description>
      <author>example@mail.com (Yunan Wang, Jianxin Li, Ziwei Zhang)</author>
      <guid isPermaLink="false">2509.18742v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology</title>
      <link>http://arxiv.org/abs/2509.18703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究专注于理性农药设计，利用图机器学习加速更安全、环保的农用化学品开发，灵感来自药物发现中的计算机模拟方法&lt;h4&gt;背景&lt;/h4&gt;研究强调生态毒理学，初步贡献包括创建了ApisTox，这是关于农药对蜜蜂毒性的最大整理数据集&lt;h4&gt;目的&lt;/h4&gt;使用图机器学习方法加速更安全、环保的农用化学品的开发&lt;h4&gt;方法&lt;/h4&gt;对用于分子图分类的机器学习模型进行了广泛评估，包括分子指纹、图核、图神经网络和预训练转换器&lt;h4&gt;主要发现&lt;/h4&gt;结果显示，在药物化学中成功的方法往往无法推广到农用化学品，这突显了需要领域特定的模型和基准&lt;h4&gt;结论&lt;/h4&gt;未来的工作将专注于开发全面的基准测试套件，并设计针对农药发现独特挑战的机器学习模型&lt;h4&gt;翻译&lt;/h4&gt;这项研究专注于理性农药设计，利用图机器学习来加速更安全、环保的农用化学品的开发，灵感来自药物发现中的计算机模拟方法。重点强调生态毒理学，初步贡献包括创建了ApisTox，这是关于农药对蜜蜂毒性的最大整理数据集。我们对用于分子图分类的机器学习(ML)模型进行了广泛评估，包括分子指纹、图核、GNNs和预训练转换器。结果表明，在药物化学中成功的方法往往无法推广到农用化学品，这突显了需要领域特定的模型和基准。未来的工作将专注于开发全面的基准测试套件，并设计针对农药发现独特挑战的机器学习模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research focuses on rational pesticide design, using graph machinelearning to accelerate the development of safer, eco-friendly agrochemicals,inspired by in silico methods in drug discovery. With an emphasis onecotoxicology, the initial contributions include the creation of ApisTox, thelargest curated dataset on pesticide toxicity to honey bees. We conducted abroad evaluation of machine learning (ML) models for molecular graphclassification, including molecular fingerprints, graph kernels, GNNs, andpretrained transformers. The results show that methods successful in medicinalchemistry often fail to generalize to agrochemicals, underscoring the need fordomain-specific models and benchmarks. Future work will focus on developing acomprehensive benchmarking suite and designing ML models tailored to the uniquechallenges of pesticide discovery.</description>
      <author>example@mail.com (Jakub Adamczyk)</author>
      <guid isPermaLink="false">2509.18703v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia</title>
      <link>http://arxiv.org/abs/2509.18568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文是首篇专注于痴呆症研究中可解释图神经网络(XGNNs)的全面综述，探讨了其在不同类型痴呆症中的应用、分类方法及面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;痴呆症是一种具有多种病因的进行性神经退行性疾病，包括阿尔茨海默病、帕金森病、额颞叶痴呆和血管性痴呆。其临床和生物学异质性使诊断极具挑战性。图神经网络(GNNs)虽在建模大脑连接方面有潜力，但因鲁棒性有限、数据稀缺和缺乏可解释性而限制了临床应用。&lt;h4&gt;目的&lt;/h4&gt;全面介绍XGNNs在痴呆症研究中的应用，通过结合图学习和可解释性解决传统GNNs的局限性，为临床医生提供疾病相关生物标志物识别、脑网络分析和透明见解。&lt;h4&gt;方法&lt;/h4&gt;提出针对痴呆症相关任务的可解释性方法分类，比较现有模型在临床场景中的应用，探讨XGNNs在阿尔茨海默病、帕金森病、轻度认知障碍和多疾病诊断中的应用，并研究整合大型语言模型用于早期检测的潜力。&lt;h4&gt;主要发现&lt;/h4&gt;XGNNs能够识别疾病相关生物标志物，分析脑网络中断，为临床提供透明见解，但仍面临泛化能力有限、领域探索不足及大型语言模型整合等挑战。&lt;h4&gt;结论&lt;/h4&gt;通过概述XGNNs在痴呆症研究中的进展和开放性问题，本综述旨在指导未来工作朝着可信、临床有意义且可扩展的方向发展，推动XGNNs在痴呆症研究中的临床应用。&lt;h4&gt;翻译&lt;/h4&gt;痴呆症是一种具有多种病因的进行性神经退行性疾病，包括阿尔茨海默病、帕金森病、额颞叶痴呆和血管性痴呆。其临床和生物学异质性使得诊断和亚型区分极具挑战性。图神经网络(GNNs)在建模大脑连接方面显示出强大潜力，但其有限的鲁棒性、数据稀缺性和缺乏可解释性限制了其在临床中的应用。可解释图神经网络(XGNNs)应运而生，通过结合基于图的学习和可解释性来解决这些障碍，使临床医生能够识别疾病相关的生物标志物、分析脑网络中断并获得透明见解。本文是首篇专注于痴呆症研究中XGNNs的全面综述。我们探讨了它们在阿尔茨海默病、帕金森病、轻度认知障碍和多疾病诊断中的应用，引入了一种针对痴呆症相关任务的可解释性方法分类，并比较了现有模型在临床场景中的表现。我们还指出了诸如泛化能力有限、领域探索不足以及整合大型语言模型(LLMs)用于早期检测等挑战。通过概述进展和开放性问题，本综述旨在指导未来的工作朝着可信、临床有意义且可扩展的方向发展，推动XGNNs在痴呆症研究中的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dementia is a progressive neurodegenerative disorder with multipleetiologies, including Alzheimer's disease, Parkinson's disease, frontotemporaldementia, and vascular dementia. Its clinical and biological heterogeneitymakes diagnosis and subtype differentiation highly challenging. Graph NeuralNetworks (GNNs) have recently shown strong potential in modeling brainconnectivity, but their limited robustness, data scarcity, and lack ofinterpretability constrain clinical adoption. Explainable Graph Neural Networks(XGNNs) have emerged to address these barriers by combining graph-basedlearning with interpretability, enabling the identification of disease-relevantbiomarkers, analysis of brain network disruptions, and provision of transparentinsights for clinicians. This paper presents the first comprehensive reviewdedicated to XGNNs in dementia research. We examine their applications acrossAlzheimer's disease, Parkinson's disease, mild cognitive impairment, andmulti-disease diagnosis. A taxonomy of explainability methods tailored fordementia-related tasks is introduced, alongside comparisons of existing modelsin clinical scenarios. We also highlight challenges such as limitedgeneralizability, underexplored domains, and the integration of Large LanguageModels (LLMs) for early detection. By outlining both progress and openproblems, this review aims to guide future work toward trustworthy, clinicallymeaningful, and scalable use of XGNNs in dementia research.</description>
      <author>example@mail.com (Niharika Tewari, Nguyen Linh Dan Le, Mujie Liu, Jing Ren, Ziqi Xu, Tabinda Sarwar, Veeky Baths, Feng Xia)</author>
      <guid isPermaLink="false">2509.18568v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models</title>
      <link>http://arxiv.org/abs/2509.18499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at the Association of Certified Fraud Examiners (ACFE)  Research Institute Annual Meeting, Las Vegas, NV, (2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了如何解决反洗钱模型训练数据缺乏的问题，提出使用混合数据集（结合合成数据和真实世界特征）来提高模型效用，同时保护数据隐私。&lt;h4&gt;背景&lt;/h4&gt;洗钱是金融机构面临的全球性严重问题。自动化反洗钱模型如图神经网络(GNN)可用于实时识别非法交易，但开发此类模型面临的主要挑战是无法获取足够的训练数据，这是由于隐私和保密问题导致的。&lt;h4&gt;目的&lt;/h4&gt;研究旨在解决反洗钱模型训练数据缺乏的问题，探索如何利用混合数据集提高模型效用，同时保护数据隐私。&lt;h4&gt;方法&lt;/h4&gt;研究提出使用混合数据集来增强合成数据集的效用，通过结合公开可获取、易于获取且具有真实世界特征的数据。&lt;h4&gt;主要发现&lt;/h4&gt;混合数据集不仅保留了隐私保护特性，还提高了反洗钱模型的效用，为金融机构提供了增强AML系统的实用途径。&lt;h4&gt;结论&lt;/h4&gt;使用混合数据集结合合成数据和真实世界特征，为金融机构提供了一个实用且隐私友好的方法来增强反洗钱系统，解决了训练数据缺乏的问题。&lt;h4&gt;翻译&lt;/h4&gt;洗钱是金融机构面临的严重全球性问题。自动化反洗钱(AML)模型，如图神经网络(GNN)，可以训练用于实时识别非法交易。开发此类模型的一个主要问题是由于隐私和保密问题无法获取训练数据。模仿真实数据统计特性但保留隐私和保密性的合成数据已被提出作为解决方案。然而，在纯合成数据集上训练AML模型也带来了自身的挑战。本文提出使用混合数据集，通过结合公开可获取、易于获取和真实世界的特征来增强合成数据集的效用。这些补充表明混合数据集不仅保留隐私还提高了模型效用，为金融机构增强AML系统提供了实用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Money laundering is a critical global issue for financial institutions.Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),can be trained to identify illicit transactions in real time. A major issue fordeveloping such models is the lack of access to training data due to privacyand confidentiality concerns. Synthetically generated data that mimics thestatistical properties of real data but preserves privacy and confidentialityhas been proposed as a solution. However, training AML models on purelysynthetic datasets presents its own set of challenges. This article proposesthe use of hybrid datasets to augment the utility of synthetic datasets byincorporating publicly available, easily accessible, and real-world features.These additions demonstrate that hybrid datasets not only preserve privacy butalso improve model utility, offering a practical pathway for financialinstitutions to enhance AML systems.</description>
      <author>example@mail.com (Rachel Chung, Pratyush Nidhi Sharma, Mikko Siponen, Rohit Vadodaria, Luke Smith)</author>
      <guid isPermaLink="false">2509.18499v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Estimating Heterogeneous Causal Effect on Networks via Orthogonal Learning</title>
      <link>http://arxiv.org/abs/2509.18484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种两阶段方法来估计网络中的异质直接和溢出效应，解决了网络因果推断中的异质性和网络依赖性导致的混杂问题。&lt;h4&gt;背景&lt;/h4&gt;估计网络中的因果效应对科学研究和实际应用都很重要。与传统假设稳定单位处理值假设不同，干扰允许对一个单位的干预影响其他单位的结果。理解直接和溢出效应在流行病学、政治经济学和经济学等领域至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够估计网络中异质直接和溢出效应的方法，同时处理网络依赖性导致的混杂问题。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段方法：第一阶段使用图神经网络估计依赖于复杂网络拓扑的 nuisance 组件；第二阶段使用这些估计调整网络混杂，并通过基于注意力的干扰模型推断因果效应。使用 Neyman 正交化和交叉拟合整合两个阶段，确保误差估计仅在高阶贡献。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在表达性和可解释性之间取得平衡，支持识别有影响力的社区和恢复溢出效应符号等下游任务，且对网络依赖下的因果效应建模中的偏差和误规格具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的因果效应估计方法能够有效处理网络中的异质性和依赖性问题，为网络环境下的因果推断提供了可靠工具。&lt;h4&gt;翻译&lt;/h4&gt;估计网络中的因果效应对科学研究和实际应用都很重要。与假设稳定单位处理值假设的传统设置不同，干扰允许对一个单位的干预/治疗影响其他单位的结果。理解直接和溢出效应在流行病学、政治经济学和经济学等领域至关重要。网络上的因果推断面临两个主要挑战。首先，因果效应通常是异质的，随单位特征和局部网络结构而变化。其次，连接的单位由于网络同质性常常表现出依赖性，导致结构相关性和因果效应之间的混杂。在本文中，我们提出了一种两阶段方法来估计网络上的异质直接和溢出效应。第一阶段使用图神经网络估计依赖于复杂网络拓扑的 nuisance 组件。在第二阶段，我们使用这些估计调整网络混杂，并通过一种新的基于注意力的干扰模型推断因果效应。我们的方法在表达性和可解释性之间取得平衡，支持识别有影响力的社区和恢复溢出效应符号等下游任务。我们使用 Neyman 正交化和交叉拟合整合两个阶段，确保误差估计仅在高阶贡献。因此，我们的因果效应估计对网络依赖下的因果效应建模中的偏差和误规格具有鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating causal effects on networks is important for both scientificresearch and practical applications. Unlike traditional settings that assumethe Stable Unit Treatment Value Assumption (SUTVA), interference allows anintervention/treatment on one unit to affect the outcomes of others.Understanding both direct and spillover effects is critical in fields such asepidemiology, political science, and economics. Causal inference on networksfaces two main challenges. First, causal effects are typically heterogeneous,varying with unit features and local network structure. Second, connected unitsoften exhibit dependence due to network homophily, creating confounding betweenstructural correlations and causal effects. In this paper, we propose atwo-stage method to estimate heterogeneous direct and spillover effects onnetworks. The first stage uses graph neural networks to estimate nuisancecomponents that depend on the complex network topology. In the second stage, weadjust for network confounding using these estimates and infer causal effectsthrough a novel attention-based interference model. Our approach balancesexpressiveness and interpretability, enabling downstream tasks such asidentifying influential neighborhoods and recovering the sign of spillovereffects. We integrate the two stages using Neyman orthogonalization andcross-fitting, which ensures that errors from nuisance estimation contributeonly at higher order. As a result, our causal effect estimates are robust tobias and misspecification in modeling causal effects under networkdependencies.</description>
      <author>example@mail.com (Yuanchen Wu, Yubai Yuan)</author>
      <guid isPermaLink="false">2509.18484v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems</title>
      <link>http://arxiv.org/abs/2509.18445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MeshODENet是一个结合图神经网络和神经常微分方程的通用框架，用于解决复杂物理系统模拟中的长期预测问题，在结构力学问题中表现出高精度、稳定性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;使用离散网格模拟复杂物理系统是应用力学的基石，但传统数值求解器对于多查询任务通常计算成本过高。&lt;h4&gt;目的&lt;/h4&gt;解决图神经网络在长期预测中常见的误差累积和不稳定性问题，提高复杂物理系统模拟的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出MeshODENet框架，结合GNN的空间推理能力与神经常微分方程的连续时间建模，并在经历大非线性变形的一维和二维弹性体等结构力学问题上进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在长期预测精度和稳定性方面显著优于基线模型，同时相比传统求解器实现了显著的计算加速。&lt;h4&gt;结论&lt;/h4&gt;MeshODENet为开发数据驱动的替代模型提供了一种强大且可推广的方法，可加速复杂结构系统的分析和建模。&lt;h4&gt;翻译&lt;/h4&gt;使用离散网格模拟复杂物理系统是应用力学的基石，但传统数值求解器对于多查询任务通常计算成本过高。虽然图神经网络(GNN)作为基于网格数据的替代模型已经出现，但它们在长期预测中的标准自回归应用常常受到误差累积和不稳定性的困扰。为解决这一问题，我们引入了MeshODENet，这是一个将GNN的空间推理能力与神经常微分方程的连续时间建模相结合的通用框架。我们在一系列具有挑战性的结构力学问题上证明了该框架的有效性和多功能性，包括经历大非线性变形的一维和二维弹性体。结果表明，我们的方法在长期预测精度和稳定性方面显著优于基线模型，同时相比传统求解器实现了显著的计算加速。这项工作为开发数据驱动的替代模型提供了一种强大且可推广的方法，以加速复杂结构系统的分析和建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The simulation of complex physical systems using a discretized mesh is acornerstone of applied mechanics, but traditional numerical solvers are oftencomputationally prohibitive for many-query tasks. While Graph Neural Networks(GNNs) have emerged as powerful surrogate models for mesh-based data, theirstandard autoregressive application for long-term prediction is often plaguedby error accumulation and instability. To address this, we introduceMeshODENet, a general framework that synergizes the spatial reasoning of GNNswith the continuous-time modeling of Neural Ordinary Differential Equations. Wedemonstrate the framework's effectiveness and versatility on a series ofchallenging structural mechanics problems, including one- and two-dimensionalelastic bodies undergoing large, non-linear deformations. The resultsdemonstrate that our approach significantly outperforms baseline models inlong-term predictive accuracy and stability, while achieving substantialcomputational speed-ups over traditional solvers. This work presents a powerfuland generalizable approach for developing data-driven surrogates to acceleratethe analysis and modeling of complex structural systems.</description>
      <author>example@mail.com (Kangzheng Liu, Leixin Ma)</author>
      <guid isPermaLink="false">2509.18445v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability</title>
      <link>http://arxiv.org/abs/2509.18376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 20 figures, NeurIPS 2025 (Oral)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了GnnXemplar，一种基于认知科学范例理论的新型图神经网络全局解释方法，通过识别代表性节点并派生自然语言规则来解释预测，解决了现有方法在大型现实世界图数据中的局限性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络被广泛用于节点分类，但其决策过程不透明限制了信任度和采用率。现有全局解释方法依赖于在小图中的子图发现，这种方法在大型现实场景中会失效，因为这些场景中子图重复少、节点属性高维，且预测源于复杂的结构-属性交互作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理大规模、真实世界图数据中的全局解释方法，克服现有方法的局限性，提供高保真度、可扩展性和人类可解释性的解释。&lt;h4&gt;方法&lt;/h4&gt;提出GnnXemplar方法，包括：在GNN嵌入空间中识别代表性节点(范例)；使用这些范例的邻域派生自然语言规则；将范例选择构建为反向k最近邻的覆盖最大化问题并提供高效贪心近似算法；采用使用大型语言模型的自完善提示策略派生可解释规则。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的实验表明，GnnXemplar在保真度、可扩展性和人类可解释性方面显著优于现有方法，这一结论得到了60名参与者的用户研究的验证。&lt;h4&gt;结论&lt;/h4&gt;GnnXemplar有效解决了图神经网络全局解释的挑战，特别是在处理大规模、复杂现实世界图数据时，为理解和信任GNN的预测提供了强大工具。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)被广泛用于节点分类，但其不透明的决策过程限制了信任度和采用率。虽然局部解释可以提供单个预测的见解，但全局解释方法(描述整个类别的方法)仍然不够成熟。现有的全局解释方法依赖于在小图中的子图发现，这种方法在大型现实场景中会失效，因为在这些场景中子图重复很少，节点属性是高维的，并且预测源于复杂的结构-属性交互作用。我们提出了GnnXemplar，一种受认知科学范例理论启发的全新全局解释方法。GnnXemplar在GNN嵌入空间中识别代表性节点(范例)，并使用其邻域派生的自然语言规则来解释预测。范例选择被构建为反向k最近邻的覆盖最大化问题，我们为此提供了高效的贪心近似算法。为了派生可解释的规则，我们使用大型语言模型(LLMs)采用自我完善的提示策略。在多个基准测试上的实验表明，GnnXemplar在保真度、可扩展性和人类可解释性方面显著优于现有方法，这一结论得到了60名参与者的用户研究的验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are widely used for node classification, yettheir opaque decision-making limits trust and adoption. While localexplanations offer insights into individual predictions, global explanationmethods, those that characterize an entire class, remain underdeveloped.Existing global explainers rely on motif discovery in small graphs, an approachthat breaks down in large, real-world settings where subgraph repetition israre, node attributes are high-dimensional, and predictions arise from complexstructure-attribute interactions. We propose GnnXemplar, a novel globalexplainer inspired from Exemplar Theory from cognitive science. GnnXemplaridentifies representative nodes in the GNN embedding space, exemplars, andexplains predictions using natural language rules derived from theirneighborhoods. Exemplar selection is framed as a coverage maximization problemover reverse k-nearest neighbors, for which we provide an efficient greedyapproximation. To derive interpretable rules, we employ a self-refining promptstrategy using large language models (LLMs). Experiments across diversebenchmarks show that GnnXemplar significantly outperforms existing methods infidelity, scalability, and human interpretability, as validated by a user studywith 60 participants.</description>
      <author>example@mail.com (Burouj Armgaan, Eshan Jain, Harsh Pandey, Mahesh Chandran, Sayan Ranu)</author>
      <guid isPermaLink="false">2509.18376v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>A Graph-Neural-Network-Entropy model of vital node identification on network attack and propagation</title>
      <link>http://arxiv.org/abs/2509.18325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络和信息熵的新方法(GNNE)用于识别复杂网络中的关键节点&lt;h4&gt;背景&lt;/h4&gt;关键节点在复杂网络中通常扮演重要角色，特别是在网络遭受故意攻击时，识别这些节点对保护网络至关重要。然而，许多现有方法没有充分整合节点特征、交互和状态信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法更有效地识别复杂网络中的关键节点，特别关注网络遭受故意攻击时的节点保护&lt;h4&gt;方法&lt;/h4&gt;GNNE方法结合了图卷积网络(GCN)和图注意力网络(GAT)。GCN用于学习节点特征，GAT通过注意力机制为不同重要性的邻居节点分配不同权重来聚合特征，然后利用节点影响因子计算节点的熵来评估节点重要性，从而量化节点在网络中的状态&lt;h4&gt;主要发现&lt;/h4&gt;GNNE在合成的Barabasi-Albert网络上进行训练，并在六个真实数据集上测试。与八种传统基于拓扑的方法和四种基于图机器学习的方法相比，GNNE在网络攻击和传播方面识别关键节点具有明显优势&lt;h4&gt;结论&lt;/h4&gt;GNNE方法通过结合图神经网络和信息熵，能够更有效地识别复杂网络中的关键节点，特别适用于网络遭受故意攻击的场景&lt;h4&gt;翻译&lt;/h4&gt;关键节点在复杂网络中通常扮演着重要角色。识别这些节点是保护网络的重要任务，特别是当网络遭受故意攻击时。许多现有方法没有充分整合节点特征、交互和状态。在本文中，我们提出了一种基于图神经网络和信息熵的新方法(GNNE)。该方法使用图卷积网络(GCN)学习节点特征，这些特征被输入到图注意力网络(GAT)中获取节点的影响因子，然后利用节点影响因子计算节点的熵来评估节点的重要性。GNNE利用了GCN和GAT的优势，GCN能够很好地提取节点特征，GAT通过使用注意力机制为不同重要性的邻居节点分配不同权重来聚合特征，节点的熵量化了节点在网络中的状态。所提出的方法在合成的Barabasi-Albert网络上进行训练，并在六个真实数据集上进行了测试。与八种传统基于拓扑的方法和四种基于图机器学习的方法相比，GNNE在网络攻击和传播方面识别关键节点具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1088/1742-5468/ae068f&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vital nodes usually play a key role in complex networks. Uncovering thesenodes is an important task in protecting the network, especially when thenetwork suffers intentional attack. Many existing methods have not fullyintegrated the node feature, interaction and state. In this article, we proposea novel method (GNNE) based on graph neural networks and information entropy.The method employs a Graph Convolutional Network (GCN) to learn the nodes'features, which are input into a Graph Attention Network (GAT) to obtain theinfluence factor of nodes, and the node influence factors are used to calculatethe nodes' entropy to evaluate the node importance. The GNNE takes advantage ofthe GCN and GAT, with the GCN well extracting the nodes' features and the GATaggregating the features of the nodes' neighbors by using the attentionmechanism to assign different weights to the neighbors with differentimportance, and the nodes' entropy quantifies the nodes' state in the network.The proposed method is trained on a synthetic Barabasi-Albert network, andtested on six real datasets. Compared with eight traditional topology-basedmethods and four graph-machine-learning-based methods, the GNNE shows anadvantage for the vital node identification in the perspectives of networkattack and propagation.</description>
      <author>example@mail.com (Huaizhi Liao, Tian Qiu, Guang Chen)</author>
      <guid isPermaLink="false">2509.18325v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>NeuFACO: Neural Focused Ant Colony Optimization for Traveling Salesman Problem</title>
      <link>http://arxiv.org/abs/2509.16938v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to RIVF'25. Code is available at  https://github.com/shoraaa/NeuFACO&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了神经聚焦蚁群优化(NeuFACO)，一种用于旅行商问题的非自回归框架，结合了先进的强化学习与增强的蚁群优化方法。&lt;h4&gt;背景&lt;/h4&gt;旅行商问题(TSP)是组合优化中的经典难题，需要寻找有效的解决方案方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效生成高质量TSP解决方案的混合方法，结合强化学习和蚁群优化的优势。&lt;h4&gt;方法&lt;/h4&gt;使用近端策略优化(PPO)与熵正则化训练图神经网络，提供特定实例的启发式指导，并将其集成到包含候选列表、受限路线改进和可扩展局部搜索的优化ACO框架中，同时利用摊销推理与ACO随机探索。&lt;h4&gt;主要发现&lt;/h4&gt;NeuFACO能够高效地在各种TSP实例上产生高质量的解决方案。&lt;h4&gt;结论&lt;/h4&gt;通过将神经网络学习与蚁群优化相结合，NeuFACO为解决旅行商问题提供了一种有效的新方法。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种神经聚焦蚁群优化(NeuFACO)，一种用于旅行商问题的非自回归框架，结合了先进的强化学习与增强的蚁群优化(ACO)。NeuFACO采用近端策略优化(PPO)与熵正则化来训练图神经网络，用于提供特定实例的启发式指导，并将其集成到包含候选列表、受限路线改进和可扩展局部搜索的优化ACO框架中。通过利用摊销推理与ACO随机探索相结合，NeuFACO能够高效地在各种TSP实例上产生高质量的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents Neural Focused Ant Colony Optimization (NeuFACO), anon-autoregressive framework for the Traveling Salesman Problem (TSP) thatcombines advanced reinforcement learning with enhanced Ant Colony Optimization(ACO). NeuFACO employs Proximal Policy Optimization (PPO) with entropyregularization to train a graph neural network for instance-specific heuristicguidance, which is integrated into an optimized ACO framework featuringcandidate lists, restricted tour refinement, and scalable local search. Byleveraging amortized inference alongside ACO stochastic exploration, NeuFACOefficiently produces high-quality solutions across diverse TSP instances.</description>
      <author>example@mail.com (Dat Thanh Tran, Khai Quang Tran, Khoi Anh Pham, Van Khu Vu, Dong Duc Do)</author>
      <guid isPermaLink="false">2509.16938v2</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>ConViS-Bench: Estimating Video Similarity Through Semantic Concepts</title>
      <link>http://arxiv.org/abs/2509.19245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了基于概念的视频相似性评估(ConViS)任务和相应的基准测试集ConViS-Bench，旨在解决视频相似性评估的多维度问题，使模型能够像人类一样从不同语义概念角度评估视频相似性。&lt;h4&gt;背景&lt;/h4&gt;视频相似性评估存在多维度性，两个视频可能在动作上相似但在拍摄地点上完全不同。人类能够从不同角度评估视频相似性，而现有模型通常依赖于全局相似性评分，缺乏这种灵活性。大型多模态模型(LMMs)为利用自然语言进行视频比较提供了新机会。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够像人类一样从多个语义概念角度评估视频相似性的方法，建立相应的基准测试集，并评估现有模型与人类判断的一致性。&lt;h4&gt;方法&lt;/h4&gt;提出基于概念的视频相似性评估(ConViS)任务，通过预定义的关键语义概念集计算可解释的相似性评分。构建ConViS-Bench基准测试集，包含多个领域的视频对，每个视频对配有概念级相似性评分和文本描述。对多个最先进的模型进行基准测试，评估它们与人类判断的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在ConViS基准测试中，不同模型之间存在显著的性能差异，表明某些概念在评估视频相似性时存在更大挑战。研究发现现有模型在多维度视频相似性评估方面仍有改进空间。&lt;h4&gt;结论&lt;/h4&gt;ConViS-Bench基准测试集将成为推进语言驱动视频理解研究的重要资源，有助于开发更接近人类判断的视频相似性评估方法，支持概念条件视频检索等新应用。&lt;h4&gt;翻译&lt;/h4&gt;什么是两个视频相似的含义？视频可能根据它们所描述的动作看起来相似，但如果根据拍摄地点评估则可能完全不同。虽然人类自然地通过考虑不同方面来比较视频，但这种能力尚未得到充分研究，并且对通常依赖全局相似性评分的模型构成了挑战。具有视频理解能力的大型多模态模型(LMMs)为利用自然语言进行视频比较任务开辟了新机会。我们引入了基于概念的视频相似性评估(ConViS)，这是一个新任务，通过预定义的关键语义概念集计算可解释的相似性评分来比较视频对。ConViS允许人类对视频相似性进行类似人类的推理，并支持概念条件视频检索等新应用。为支持此任务，我们还引入了ConViS-Bench，这是一个新的基准测试集，包含多个领域经过仔细标注的视频对。每个视频对都配有概念级相似性评分以及差异和相似性的文本描述。此外，我们在ConViS上对几个最先进的模型进行了基准测试，提供了它们与人类判断一致性的见解。我们的结果显示了在ConViS上的显著性能差异，表明某些概念在估计视频相似性时存在更大挑战。我们相信ConViS-Bench将成为推进语言驱动视频理解研究的有价值资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; What does it mean for two videos to be similar? Videos may appear similarwhen judged by the actions they depict, yet entirely different if evaluatedbased on the locations where they were filmed. While humans naturally comparevideos by taking different aspects into account, this ability has not beenthoroughly studied and presents a challenge for models that often depend onbroad global similarity scores. Large Multimodal Models (LMMs) with videounderstanding capabilities open new opportunities for leveraging naturallanguage in comparative video tasks. We introduce Concept-based VideoSimilarity estimation (ConViS), a novel task that compares pairs of videos bycomputing interpretable similarity scores across a predefined set of keysemantic concepts. ConViS allows for human-like reasoning about videosimilarity and enables new applications such as concept-conditioned videoretrieval. To support this task, we also introduce ConViS-Bench, a newbenchmark comprising carefully annotated video pairs spanning multiple domains.Each pair comes with concept-level similarity scores and textual descriptionsof both differences and similarities. Additionally, we benchmark severalstate-of-the-art models on ConViS, providing insights into their alignment withhuman judgments. Our results reveal significant performance differences onConViS, indicating that some concepts present greater challenges for estimatingvideo similarity. We believe that ConViS-Bench will serve as a valuableresource for advancing research in language-driven video understanding.</description>
      <author>example@mail.com (Benedetta Liberatori, Alessandro Conti, Lorenzo Vaquero, Yiming Wang, Elisa Ricci, Paolo Rota)</author>
      <guid isPermaLink="false">2509.19245v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding</title>
      <link>http://arxiv.org/abs/2509.19135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GSTM-HMU，一个生成式时空框架，通过明确建模人类运动的语义和时序复杂性来推进移动性分析。&lt;h4&gt;背景&lt;/h4&gt;人类移动轨迹通常以签到序列的形式记录，提供了观察短期访问模式和持久生活规律的独特窗口。&lt;h4&gt;目的&lt;/h4&gt;设计GSTM-HMU框架，通过建模人类移动的语义和时序复杂性，提升移动性分析能力。&lt;h4&gt;方法&lt;/h4&gt;该框架包含四个关键创新：1)时空概念编码器整合地理位置、POI类别语义和周期性时间节奏；2)认知轨迹记忆自适应过滤历史访问；3)生活方式概念库提供结构化人类偏好线索；4)面向任务的生成式头将学习表示转换为下游任务预测。&lt;h4&gt;主要发现&lt;/h4&gt;在Gowalla、WeePlace、Brightkite和FourSquare四个数据集上进行的实验表明，GSTM-HMU在下一个位置预测、轨迹-用户识别和时间估计三个任务上均显著优于基线方法，证实了从复杂移动数据中提取语义规律的有效性。&lt;h4&gt;结论&lt;/h4&gt;生成式建模为构建更强大、可解释和可泛化的人类移动智能系统提供了有前景的基础。&lt;h4&gt;翻译&lt;/h4&gt;人类移动轨迹通常以签到序列的形式记录，为观察短期访问模式和持久生活规律提供了独特窗口。在这项工作中，我们引入了GSTM-HMU，一个生成式时空框架，通过明确建模人类运动的语义和时序复杂性来推进移动性分析。该框架包含四个关键创新。首先，时空概念编码器将地理位置、POI类别语义和周期性时间节奏整合为统一的向量表示。其次，认知轨迹记忆自适应过滤历史访问，强调近期和行为上显著的事件，以便更有效地捕获用户意图。第三，生活方式概念库提供结构化的人类偏好线索，如活动类型和生活方式模式，以增强可解释性和个性化。最后，面向任务的生成式头将学习到的表示转换为多个下游任务的预测。我们在四个广泛使用的真实世界数据集上进行了大量实验，包括Gowalla、WeePlace、Brightkite和FourSquare，并评估了三个基准任务：下一个位置预测、轨迹-用户识别和时间估计。结果表明，与强基线相比有持续且显著的改进，证实了GSTM-HMU从复杂移动数据中提取语义规律的有效性。除了原始性能提升外，我们的研究还表明，生成式建模为构建更强大、可解释和可泛化的人类移动智能系统提供了有前景的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human mobility traces, often recorded as sequences of check-ins, provide aunique window into both short-term visiting patterns and persistent lifestyleregularities. In this work we introduce GSTM-HMU, a generative spatio-temporalframework designed to advance mobility analysis by explicitly modeling thesemantic and temporal complexity of human movement. The framework consists offour key innovations. First, a Spatio-Temporal Concept Encoder (STCE)integrates geographic location, POI category semantics, and periodic temporalrhythms into unified vector representations. Second, a Cognitive TrajectoryMemory (CTM) adaptively filters historical visits, emphasizing recent andbehaviorally salient events in order to capture user intent more effectively.Third, a Lifestyle Concept Bank (LCB) contributes structured human preferencecues, such as activity types and lifestyle patterns, to enhanceinterpretability and personalization. Finally, task-oriented generative headstransform the learned representations into predictions for multiple downstreamtasks. We conduct extensive experiments on four widely used real-worlddatasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluateperformance on three benchmark tasks: next-location prediction, trajectory-useridentification, and time estimation. The results demonstrate consistent andsubstantial improvements over strong baselines, confirming the effectiveness ofGSTM-HMU in extracting semantic regularities from complex mobility data. Beyondraw performance gains, our findings also suggest that generative modelingprovides a promising foundation for building more robust, interpretable, andgeneralizable systems for human mobility intelligence.</description>
      <author>example@mail.com (Wenying Luo, Zhiyuan Lin, Wenhao Xu, Minghao Liu, Zhi Li)</author>
      <guid isPermaLink="false">2509.19135v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</title>
      <link>http://arxiv.org/abs/2509.19002v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态大语言模型(MLLMs)的最新进展显著提升了视频理解能力，但当前基准测试主要关注室内场景或短距离户外活动，对长途旅行相关挑战的探索不足。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)的最新进展显著提升了视频理解能力，为实际应用开辟了新可能性。然而，当前的视频基准测试主要关注室内场景或短距离户外活动，与长途旅行相关的挑战在很大程度上未被探索。&lt;h4&gt;目的&lt;/h4&gt;为解决这一差距，提出VIR-Bench基准测试，包含200个旅行视频，将行程重建设计为具有挑战性的任务，旨在评估和推动MLLMs的地理空间-时间智能。&lt;h4&gt;方法&lt;/h4&gt;VIR-Bench基准测试包含200个旅行视频，将行程重建作为评估MLLMs地理空间-时间智能的任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，包括专有模型在内的最先进MLLMs难以获得高分，凸显了处理跨越广泛空间和时间尺度视频的难度。开发的原型旅行规划代理利用VIR-Bench的洞察，提供了显著改善的行程推荐。&lt;h4&gt;结论&lt;/h4&gt;评估协议不仅有效地基准测试了模型，还在面向用户的应用中转化为具体的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的最新进展显著提升了视频理解能力，为实际应用开辟了新的可能性。然而，当前的视频基准测试主要关注室内场景或短距离户外活动，与长途旅行相关的挑战在很大程度上仍未被探索。掌握扩展的地理空间-时间轨迹对下一代MLLMs至关重要，它支撑着具身AI规划和导航等实际任务。为弥补这一差距，我们提出了VIR-Bench，这是一个包含200个旅行视频的新基准测试，它将行程重建设计为一个具有挑战性的任务，旨在评估和推动MLLMs的地理空间-时间智能。实验结果显示，包括专有模型在内的最先进MLLMs难以获得高分，这凸显了处理跨越广泛空间和时间尺度视频的难度。此外，我们进行了一个深入的案例研究，开发了一个利用VIR-Bench洞察的原型旅行规划代理。该代理显著改善的行程推荐验证了我们的评估协议不仅有效地基准测试了模型，还在面向用户的应用中转化为具体的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multimodal large language models (MLLMs) havesignificantly enhanced video understanding capabilities, opening newpossibilities for practical applications. Yet current video benchmarks focuslargely on indoor scenes or short-range outdoor activities, leaving thechallenges associated with long-distance travel largely unexplored. Masteringextended geospatial-temporal trajectories is critical for next-generationMLLMs, underpinning real-world tasks such as embodied-AI planning andnavigation. To bridge this gap, we present VIR-Bench, a novel benchmarkconsisting of 200 travel videos that frames itinerary reconstruction as achallenging task designed to evaluate and push forward MLLMs'geospatial-temporal intelligence. Experimental results reveal thatstate-of-the-art MLLMs, including proprietary ones, struggle to achieve highscores, underscoring the difficulty of handling videos that span extendedspatial and temporal scales. Moreover, we conduct an in-depth case study inwhich we develop a prototype travel-planning agent that leverages the insightsgained from VIR-Bench. The agent's markedly improved itinerary recommendationsverify that our evaluation protocol not only benchmarks models effectively butalso translates into concrete performance gains in user-facing applications.</description>
      <author>example@mail.com (Hao Wang, Eiki Murata, Lingfang Zhang, Ayako Sato, So Fukuda, Ziqi Yin, Wentao Hu, Keisuke Nakao, Yusuke Nakamura, Sebastian Zwirner, Yi-Chia Chen, Hiroyuki Otomo, Hiroki Ouchi, Daisuke Kawahara)</author>
      <guid isPermaLink="false">2509.19002v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Surgical Video Understanding with Label Interpolation</title>
      <link>http://arxiv.org/abs/2509.18802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合基于光流的分割标签插值与多任务学习的新框架，用于解决机器人辅助手术中视觉数据处理的时间-空间不平衡问题，提高手术场景理解的准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;机器人辅助手术(RAS)已成为现代手术的关键范式，通过微创手术促进患者恢复并减轻外科医生负担。然而，要充分实现其潜力，需要精确理解手术过程中生成的视觉数据。&lt;h4&gt;目的&lt;/h4&gt;解决手术场景理解中的时间-空间不平衡问题，提高多任务学习的效率和准确性，从而增强机器人辅助手术的实用性。&lt;h4&gt;方法&lt;/h4&gt;提出一个新框架，结合基于光流的分割标签插值与多任务学习。使用从标注的关键帧估计的光流将标签传播到相邻的未标注帧，丰富稀疏的空间监督，平衡训练中的时间和空间信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过整合光流估计与多任务学习，该方法能够有效解决手术视觉数据处理中的挑战，提高手术场景理解的准确性和效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的新框架通过平衡时间和空间信息，显著提升了机器人辅助手术中视觉数据处理的性能，增强了RAS的实用价值。&lt;h4&gt;翻译&lt;/h4&gt;机器人辅助手术(RAS)已成为现代手术的关键范式，通过微创手术促进患者恢复并减轻外科医生负担。然而，要充分实现其潜力，需要精确理解手术过程中生成的视觉数据。先前的研究主要集中在单任务方法上，但实际手术场景涉及复杂的时间动态和多样的器械交互，限制了全面理解。此外，多任务学习的有效应用需要足够的像素级分割数据，但由于标注成本高且需要专业知识而难以获取。特别是，长期标注如阶段和步骤适用于每一帧，而短期标注如手术器械分割和动作检测仅适用于关键帧，导致显著的时间-空间不平衡。为解决这些挑战，我们提出了一种结合基于光流的分割标签插值与多任务学习的新框架。使用从标注关键帧估计的光流将标签传播到相邻的未标注帧，从而丰富稀疏的空间监督，平衡训练中的时间和空间信息。这种整合提高了手术场景理解的准确性和效率，进而增强了RAS的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot-assisted surgery (RAS) has become a critical paradigm in modernsurgery, promoting patient recovery and reducing the burden on surgeons throughminimally invasive approaches. To fully realize its potential, however, aprecise understanding of the visual data generated during surgical proceduresis essential. Previous studies have predominantly focused on single-taskapproaches, but real surgical scenes involve complex temporal dynamics anddiverse instrument interactions that limit comprehensive understanding.Moreover, the effective application of multi-task learning (MTL) requiressufficient pixel-level segmentation data, which are difficult to obtain due tothe high cost and expertise required for annotation. In particular, long-termannotations such as phases and steps are available for every frame, whereasshort-term annotations such as surgical instrument segmentation and actiondetection are provided only for key frames, resulting in a significanttemporal-spatial imbalance. To address these challenges, we propose a novelframework that combines optical flow-based segmentation label interpolationwith multi-task learning. optical flow estimated from annotated key frames isused to propagate labels to adjacent unlabeled frames, thereby enriching sparsespatial supervision and balancing temporal and spatial information fortraining. This integration improves both the accuracy and efficiency ofsurgical scene understanding and, in turn, enhances the utility of RAS.</description>
      <author>example@mail.com (Garam Kim, Tae Kyeong Jeong, Juyoun Park)</author>
      <guid isPermaLink="false">2509.18802v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>COLT: Enhancing Video Large Language Models with Continual Tool Usage</title>
      <link>http://arxiv.org/abs/2509.18754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为COLT的方法，用于增强开源视频大型语言模型，使其能够在不断演化的工具流中自动获取工具使用能力，同时避免灾难性遗忘。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的成功显著推动了视频理解研究，视频LLMs优先探索工具使用能力。现有方法要么提示闭源LLMs，要么采用指令调优范式进行工具使用微调。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法难以适应工具数据持续演化和流入的现实环境的问题，使视频LLMs能够处理不断变化的工具集。&lt;h4&gt;方法&lt;/h4&gt;提出COLT方法，包含一个可学习的工具代码本作为特定工具的记忆系统，根据用户指令与工具特征的相似性动态选择相关工具，避免灾难性遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在先前视频LLM基准和特定工具使用的VideoToolBench数据集上的大量实验证明了COLT的最先进性能。&lt;h4&gt;结论&lt;/h4&gt;COLT方法成功增强了开源视频LLMs的工具使用能力，使其能够处理连续变化的工具数据，同时保持对已学习工具的记忆。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型的成功显著推动了视频理解的研究。为了利用训练有素的专业模型（即工具）的优势，视频LLMs优先探索工具使用能力。现有方法要么提示闭源LLMs，要么采用指令调优范式进行工具使用微调。然而，这些方法假设有一个固定的工具库，难以推广到工具数据持续演化和流入的现实环境。为此，我们提出通过连续工具使用（称为COLT）增强开源视频LLMs，使其能够在连续工具流中自动获取工具使用能力，同时避免对过去已学习工具的'灾难性遗忘'。具体而言，我们的COLT包含一个可学习的工具代码本作为特定工具的记忆系统。然后根据用户指令与代码本内工具特征的相似性动态选择相关工具。为了释放视频LLMs的工具使用潜力，我们收集了一个以视频为中心的工具使用指令调优数据集VideoToolBench。在先前视频LLM基准和特定工具使用的VideoToolBench数据集上的大量实验证明了我们提出的COLT的最先进性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The success of Large Language Models (LLMs) has significantly propelled theresearch of video understanding. To harvest the benefits of well-trained expertmodels (i.e., tools), video LLMs prioritize the exploration of tool usagecapabilities. Existing methods either prompt closed-source LLMs or employ theinstruction tuning paradigm for tool-use fine-tuning. These methods, however,assume an established repository of fixed tools and struggle to generalize toreal-world environments where tool data is perpetually evolving and streamingin. To this end, we propose to enhance open-source video LLMs with COntinuaLTool usage (termed COLT), which automatically acquires tool-use ability in asuccessive tool stream without suffering 'catastrophic forgetting' of the pastlearned tools. Specifically, our COLT incorporates a learnable tool codebook asa tool-specific memory system. Then relevant tools are dynamically selectedbased on the similarity between user instruction and tool features within thecodebook. To unleash the tool usage potential of video LLMs, we collect avideo-centric tool-use instruction tuning dataset VideoToolBench. Extensiveexperiments on both previous video LLM benchmarks and the tool-use-specificVideoToolBench dataset demonstrate the state-of-the-art performance of ourproposed COLT.</description>
      <author>example@mail.com (Yuyang Liu, Xinyuan Shi, Bang Yang, Peilin Zhou, Jiahua Dong, Long Chen, Ian Reid, Xiaondan Liang)</author>
      <guid isPermaLink="false">2509.18754v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Compositional System Dynamics: The Higher Mathematics Underlying System Dynamics Diagrams &amp; Practice</title>
      <link>http://arxiv.org/abs/2509.18475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文建立了基于范畴论的组合系统动力学建模的数学框架，形式化了系统动力学图表的表示、分析和组合能力，支持模块化、分层和语义映射，能识别路径、反馈循环和模式，支持替代语义如随机转换动力学，为处理复杂系统提供了清晰、可扩展和严谨的方法。&lt;h4&gt;背景&lt;/h4&gt;系统动力学建模在处理复杂系统时面临表示、分析和组合的挑战，传统方法缺乏坚实的数学基础支持。&lt;h4&gt;目的&lt;/h4&gt;建立组合系统动力学建模的数学基础，利用范畴论形式化和增强系统模型的表示、分析和组合能力。&lt;h4&gt;方法&lt;/h4&gt;使用范畴论将系统动力学图表表述为范畴论结构；通过带属性的C-集编码图表为数据；利用结构余极限、推出、拉回和函子映射等范畴论工具；支持模块化组合、分层和语法与语义之间的映射。&lt;h4&gt;主要发现&lt;/h4&gt;该框架支持识别路径和反馈循环，检测复杂图表中的简单模式，发现图表间的共同结构，实现不同图表类型间的结构保持映射；支持替代语义如随机转换动力学。&lt;h4&gt;结论&lt;/h4&gt;通过揭示和形式化系统动力学图表中隐藏的数学结构，该框架使从业者能够以清晰、可扩展和严谨的方式处理复杂系统；未来方向包括整合维度注释，支持混合和基于主体的建模范式，以及扩展时态推理能力。&lt;h4&gt;翻译&lt;/h4&gt;本研究为组合系统动力学建模建立了坚实的数学基础，利用范畴论来形式化和增强系统模型的表示、分析和组合能力。在这里，系统动力学图表，如存量流量图、系统结构图和因果循环图，被表述为范畴论结构，实现了可扩展、透明和系统化的推理。通过使用带属性的C-集将这些图表编码为数据，并利用结构余极限、推出、拉回和函子映射等高级范畴论工具，该框架支持模块化组合、分层以及语法与语义之间的无缝映射。该框架为传统实践提供了坚实的数学结构支持，有助于识别某些形式的路径和反馈循环，检测复杂图表中的简单模式，发现图表间的共同结构，以及不同图表类型之间的结构保持映射。此外，该框架还支持替代语义，如随机转换动力学，超越了传统的常微分方程表示。在组合建模、模块化和团队协作方面的应用展示了该先进框架的实际优势。未来方向包括整合维度注释，支持混合和基于主体的建模范式，并通过时层sheaves扩展框架在全局和局部时态推理中的适用性。通过揭示和形式化系统动力学图表中隐藏的数学结构，这项工作使从业者能够以清晰、可扩展和严谨的方式处理复杂系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work establishes a robust mathematical foundation for compositionalSystem Dynamics modeling, leveraging category theory to formalize and enhancethe representation, analysis, and composition of system models. Here, SystemDynamics diagrams, such as stock &amp; flow diagrams, system structure diagrams,and causal loop diagrams, are formulated as categorical constructs, enablingscalable, transparent, and systematic reasoning. By encoding these diagrams asdata using attributed C-sets and utilizing advanced categorical tools likestructured cospans, pushouts, pullbacks, and functor mappings, the frameworksupports modular composition, stratification, and seamless mapping betweensyntax and semantics.  The approach underwrites traditional practice with firm mathematicalstructure, facilitates the identification of certain forms of pathways andfeedback loops, the detection of simple patterns within complex diagrams,common structure between diagrams, and structure-preserving mappings betweendiverse diagram types. Additionally, this framework supports alternativesemantics, such as stochastic transition dynamics, extending beyond traditionalordinary differential equation (ODE) representations. Applications incompositional modeling, modularity, and team-based collaboration demonstratethe practical advantages of this advanced framework.  Future directions include integrating dimensional annotations, supportinghybrid and agent-based modeling paradigms, and expanding the framework'sapplicability to global and local temporal reasoning through temporal sheaves.By revealing and formalizing the hidden mathematical structure of SystemDynamics diagrams, this work empowers practitioners to tackle complex systemswith clarity, scalability, and rigor.</description>
      <author>example@mail.com (Xiaoyan Li, Evan Patterson, Patricia L. Mabry, Nathaniel D. Osgood)</author>
      <guid isPermaLink="false">2509.18475v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>AHA - Predicting What Matters Next: Online Highlight Detection Without Looking Ahead</title>
      <link>http://arxiv.org/abs/2509.16421v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025, 32 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Aha的自回归重点检测框架，能够在不访问未来视频帧的情况下，实时预测每个视频帧与自然语言描述任务的相关性，为高风险环境中的智能体提供实时视频理解能力。&lt;h4&gt;背景&lt;/h4&gt;实时理解连续视频流对在高风险环境中运行的智能体至关重要，包括自动驾驶车辆、监控无人机和灾难响应机器人。然而，现有方法通常需要访问整个视频进行推理，不适合在线或流式场景，且当前模型针对离线摘要优化，无法支持实时决策所需的逐步推理。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实时理解连续视频流的框架，针对自然语言描述的任务预测每个视频帧的相关性，支持在线或流式场景下的视频理解和实时决策。&lt;h4&gt;方法&lt;/h4&gt;提出Aha框架，使用多模态视觉语言模型和轻量级、解耦的头部，在大型人工标注视频数据集上训练。引入动态接收器缓存机制实现无限长度流上的恒定内存使用，使隐藏表示能够捕获高级任务目标，实现有效帧级排序。&lt;h4&gt;主要发现&lt;/h4&gt;Aha在重点检测基准上取得了最先进性能，在TVSum上比之前方法高+5.9%的mAP，在Mr. Hisum上高+8.3%的mAP。实验证明Aha可作为实时推理模块用于下游规划和长期理解，在机器人应用中展示了潜力。&lt;h4&gt;结论&lt;/h4&gt;Aha能够有效支持高风险环境中智能体的实时决策，可作为实时推理模块用于下游规划和长期理解，解决了现有方法无法适应流式视频场景的问题。&lt;h4&gt;翻译&lt;/h4&gt;在高风险环境中运行的智能体（包括自动驾驶车辆、监控无人机和灾难响应机器人）对连续视频流的实时理解至关重要。然而，大多数现有的视频理解和重点检测方法在推理过程中假设可以访问整个视频，这使得它们不适合在线或流式场景。特别是，当前模型针对离线摘要进行优化，无法支持实时决策所需的逐步推理。我们引入了Aha，一种自回归重点检测框架，它预测每个视频帧与自然语言描述的任务的相关性。在不访问未来视频帧的情况下，Aha利用多模态视觉语言模型和轻量级、解耦的头部，这些头部在大型人工标注视频数据集上进行训练。为实现可扩展性，我们引入了动态接收器缓存机制，该机制在无限长度流上实现恒定内存使用，且不会在标准基准测试中降低性能。这鼓励隐藏表示捕获高级任务目标，实现对信息量、相关性和不确定性的有效帧级排序，这些都与自然语言任务相关。Aha在重点检测基准上取得了最先进的性能，甚至在TVSum上比之前的离线、全上下文方法和视频语言模型高出+5.9%的mAP（平均精度均值），在Mr. Hisum上高出+8.3%。我们探索了Aha在给定面向任务的自然语言输入和连续的以机器人为中心的视频的情况下，用于现实世界机器人应用的潜力。两项实验都证明了Aha作为实时推理模块在下游规划和长期理解方面的潜在有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time understanding of continuous video streams is essential forintelligent agents operating in high-stakes environments, including autonomousvehicles, surveillance drones, and disaster response robots. Yet, most existingvideo understanding and highlight detection methods assume access to the entirevideo during inference, making them unsuitable for online or streamingscenarios. In particular, current models optimize for offline summarization,failing to support step-by-step reasoning needed for real-time decision-making.We introduce Aha, an autoregressive highlight detection framework that predictsthe relevance of each video frame against a task described in natural language.Without accessing future video frames, Aha utilizes a multimodalvision-language model and lightweight, decoupled heads trained on a large,curated dataset of human-centric video labels. To enable scalability, weintroduce the Dynamic SinkCache mechanism that achieves constant memory usageacross infinite-length streams without degrading performance on standardbenchmarks. This encourages the hidden representation to capture high-leveltask objectives, enabling effective frame-level rankings for informativeness,relevance, and uncertainty with respect to the natural language task. Ahaachieves state-of-the-art (SOTA) performance on highlight detection benchmarks,surpassing even prior offline, full-context approaches and video-languagemodels by +5.9% on TVSum and +8.3% on Mr. Hisum in mAP (mean AveragePrecision). We explore Aha's potential for real-world robotics applicationsgiven a task-oriented natural language input and a continuous, robot-centricvideo. Both experiments demonstrate Aha's potential effectiveness as areal-time reasoning module for downstream planning and long-horizonunderstanding.</description>
      <author>example@mail.com (Aiden Chang, Celso De Melo, Stephanie M. Lukin)</author>
      <guid isPermaLink="false">2509.16421v2</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Category-Level Object Shape and Pose Estimation in Less Than a Millisecond</title>
      <link>http://arxiv.org/abs/2509.18979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种快速的物体形状和姿态估计方法，仅需类别级物体先验，并能提供全局最优性证明。该方法通过学习的前端检测语义关键点，使用线性活动形状模型表示物体形状，并通过最大后验优化和自洽场迭代高效求解位置、方向和形状。&lt;h4&gt;背景&lt;/h4&gt;物体形状和姿态估计是机器人的基础性问题，支持从操作到场景理解和导航等多种任务。现有的方法可能需要复杂的计算或缺乏全局最优性的保证。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速的局部求解器，只需要类别级物体先验，并能提供全局最优性的有效证明，使该方法能够应用于实时应用场景。&lt;h4&gt;方法&lt;/h4&gt;1. 使用学习的前端在RGB-D图像中检测目标物体的稀疏类别级语义关键点；2. 使用线性活动形状模型表示目标物体的未知形状；3. 提出最大后验优化问题同时求解位置、方向和形状；4. 将问题转化为单位四元数表示的特征值问题；5. 使用自洽场迭代高效求解，每步仅需计算4x4矩阵并寻找其最小特征值-向量对；6. 通过求解拉格朗日乘子的线性系统提供全局最优性证明。&lt;h4&gt;主要发现&lt;/h4&gt;1. 求解器每迭代一次仅需约100微秒，可实现快速异常值剔除；2. 方法在合成数据和多种真实世界场景中表现良好；3. 代码已开源，可供其他研究者使用和扩展。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种高效的物体形状和姿态估计方法，能够在保证全局最优性的同时实现快速计算，为机器人操作、场景理解和导航等任务提供了有力支持。&lt;h4&gt;翻译&lt;/h4&gt;物体形状和姿态估计是机器人的基础性问题，支持从操作到场景理解和导航等多种任务。我们提出了一种用于形状和姿态估计的快速局部求解器，只需要类别级物体先验，并能提供高效的全局最优性证明。给定物体的RGB-D图像，我们使用学习的前端在目标物体上检测稀疏的类别级语义关键点。我们使用线性活动形状模型表示目标物体的未知形状，并提出最大后验优化问题来同时求解位置、方向和形状。用单位四元数表示，这个问题转化为具有特征向量非线性的特征值问题。我们的主要贡献是通过自洽场迭代高效解决这个问题，每步迭代仅需计算一个4x4矩阵并寻找其最小特征值-向量对。求解相应拉格朗日乘子的线性系统可提供简单的全局最优性证明。我们的求解器每次迭代运行时间约为100微秒，能够实现快速异常值剔除。我们在合成数据和多种真实世界场景中测试了我们的方法，包括两个公共数据集和一个无人机跟踪场景。代码已发布在https://github.com/MIT-SPARK/Fast-ShapeAndPose。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在极短时间内（不到一毫秒）完成类别级物体形状和姿态估计的问题。这个问题在机器人和计算机视觉领域非常重要，因为它支持从物体操作到场景理解和导航等多种基础任务。例如，自动驾驶汽车需要定位障碍物和其他车辆，家庭机械臂需要定位物体进行交互。快速估计器能实现快速响应新输入、在计算资源有限的情况下工作，并能有效剔除异常值，这对于实际应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有工作出发，特别是Shi等人[3]的半定松弛方法，但强调速度和可认证性。他们发现利用四元数表示旋转时，一阶最优性条件可以转化为非线性特征值问题。作者借鉴了四元数表示旋转的方法、自洽场迭代(SCF)技术[6]、半定规划(SDP)松弛以及高斯-牛顿和Levenberg-Marquardt等优化方法。通过重新表述问题并利用其特征值结构，他们设计了一个快速求解器，并基于拉格朗日对偶性添加了全局最优性认证。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用四元数表示旋转时的一阶最优性条件的特征值结构，将问题转化为非线性特征值问题，然后使用自洽场迭代(SCF)高效求解，并提供基于拉格朗日对偶性的快速全局最优性认证。整体流程包括：1)使用学习前端检测物体上的稀疏类别级语义关键点；2)用线性活动形状模型表示物体形状；3)建立最大后验概率优化问题；4)转化为四元数形式得到非线性特征值问题；5)用SCF迭代求解；6)通过拉格朗日乘子提供全局最优性认证。单次迭代仅需约100微秒。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于四元数非线性特征值结构的快速自洽场迭代求解器；2)基于拉格朗日对偶性的高效全局最优性认证方法；3)在合成数据和多种真实场景（包括无人机跟踪和两个公共数据集）上的全面评估。相比之前的工作，特别是Shi等人的半定松弛方法，本文方法速度提高了约10倍（不到1毫秒vs.数毫秒），同时保持了相当的精度，并提供了快速的全局最优性认证。方法更加轻量，可在资源有限设备上运行，且在保证精度的同时显著提高了计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于四元数非线性特征值结构的快速自洽场迭代方法，实现了在不到一毫秒内完成类别级物体形状和姿态估计，并提供了高效的全局最优性认证。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object shape and pose estimation is a foundational robotics problem,supporting tasks from manipulation to scene understanding and navigation. Wepresent a fast local solver for shape and pose estimation which requires onlycategory-level object priors and admits an efficient certificate of globaloptimality. Given an RGB-D image of an object, we use a learned front-end todetect sparse, category-level semantic keypoints on the target object. Werepresent the target object's unknown shape using a linear active shape modeland pose a maximum a posteriori optimization problem to solve for position,orientation, and shape simultaneously. Expressed in unit quaternions, thisproblem admits first-order optimality conditions in the form of an eigenvalueproblem with eigenvector nonlinearities. Our primary contribution is to solvethis problem efficiently with self-consistent field iteration, which onlyrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vectorpair at each iterate. Solving a linear system for the corresponding Lagrangemultipliers gives a simple global optimality certificate. One iteration of oursolver runs in about 100 microseconds, enabling fast outlier rejection. We testour method on synthetic data and a variety of real-world settings, includingtwo public datasets and a drone tracking scenario. Code is released athttps://github.com/MIT-SPARK/Fast-ShapeAndPose.</description>
      <author>example@mail.com (Lorenzo Shaikewitz, Tim Nguyen, Luca Carlone)</author>
      <guid isPermaLink="false">2509.18979v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations</title>
      <link>http://arxiv.org/abs/2509.18953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Eva-VLA是首个统一框架，通过将离散物理变化转化为连续优化问题，系统评估VLA模型对真实世界物理变化的鲁棒性。研究发现VLA模型在真实部署中存在严重脆弱性。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action (VLA)模型已成为机器人操作的有前景解决方案，但这些模型对真实世界物理变化的鲁棒性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出Eva-VLA框架系统评估VLA模型鲁棒性，解决两个关键挑战：(1)系统表征真实世界物理变化同时保持评估可重复性；(2)高效发现最坏情况场景而不需 prohibitive 的真实世界数据收集成本。&lt;h4&gt;方法&lt;/h4&gt;将真实世界变化分解为三个关键领域：物体3D变换、光照变化和对抗性补丁；引入连续黑盒优化框架，将离散物理变化转化为参数优化；在多个基准测试中对先进OpenVLA模型进行广泛实验。&lt;h4&gt;主要发现&lt;/h4&gt;所有变化类型都触发超过60%的失败率，物体变换在长时序任务中导致高达97.8%的失败率，揭示了控制实验室成功与不可预测的部署准备状态之间存在关键差距。&lt;h4&gt;结论&lt;/h4&gt;Eva-VLA框架为加强VLA模型应对真实世界部署挑战提供了实用途径，同时揭示了VLA模型在实际部署中的严重脆弱性。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-行动(VLA)模型已成为机器人操作的有前景解决方案，然而它们对真实世界物理变化的鲁棒性仍然关键性地未被探索。为了弥补这一差距，我们提出了Eva-VLA，这是第一个通过将离散物理变化转化为连续优化问题来系统评估VLA模型鲁棒性的统一框架。然而，全面评估VLA鲁棒性存在两个关键挑战：(1)如何在保持评估可重复性的同时系统地表征真实部署中遇到的各种物理变化，以及(2)如何高效地发现最坏情况场景而无需 prohibitive 的真实世界数据收集成本。为了解决第一个挑战，我们将真实世界变化分解为三个关键领域：影响空间推理的物体3D变换、挑战视觉感知的光照变化以及破坏场景理解的对抗性补丁。对于第二个挑战，我们引入了一个连续黑盒优化框架，将离散物理变化转化为参数优化，从而能够系统地探索最坏情况场景。在多个基准测试中对最先进的OpenVLA模型进行的广泛实验揭示了令人警见的漏洞：所有变化类型都触发了超过60%的失败率，其中物体变换在长时序任务中导致高达97.8%的失败率。我们的研究结果揭示了控制实验室成功与不可预测的部署准备状态之间的关键差距，而Eva-VLA框架为加强基于VLA的机器人操作模型应对真实世界部署挑战提供了实用途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决评估视觉-语言-行动（VLA）模型在现实世界物理变化下的鲁棒性问题。这个问题很重要，因为在真实环境中，机器人操作不可避免地面临物体位置变化、光照变化和视觉干扰等物理变化，这些变化可能在不被察觉的情况下显著改变机器人行为，带来安全风险。同时，现有研究主要关注实验室环境，缺乏对真实世界物理变化影响的系统评估，导致实验室成功与实际部署之间存在巨大差距。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先将现实世界物理变化分解为三个关键领域：物体3D变换、光照变化和对抗性补丁。然后，他们将离散的物理变化转化为连续的参数优化问题，通过参数化这些变化（如旋转角度、光照参数和补丁位置）来系统探索最坏情况场景。作者借鉴了对抗性攻击和进化策略的研究，特别是使用了CMA-ES（协方差矩阵自适应进化策略）算法，这是一种梯度无关的优化方法，适合黑盒场景。同时，他们参考了现有的VLA模型架构，特别是OpenVLA系列，作为评估目标。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将离散的物理变化转化为连续的参数优化问题，通过系统性地探索最坏情况场景来评估VLA模型的鲁棒性。整体流程包括：1) 参数化三种物理变化（物体3D变换、光照变化、对抗性补丁）；2) 定义对抗目标函数，使机器人输出错误动作；3) 使用CMA-ES算法优化这些参数，发现最坏情况配置；4) 在模拟环境中评估模型在这些物理变化下的表现；5) 计算失败率作为鲁棒性指标。这种方法不依赖梯度信息，适用于黑盒场景，且通过模拟器确保评估的可重复性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个统一框架系统评估V模型在物理变化下的鲁棒性；2) 首次将现实世界物理变化分解为三个关键领域（物体3D变换、光照变化、对抗性补丁）；3) 提出将离散物理变化转化为连续参数优化问题的新方法；4) 通过模拟器确保评估的可重复性和可靠性；5) 揭示了当前VLA系统高达97.8%的失败率。相比之前的工作，Eva-VLA更关注物理可行的变化（而非纯数字攻击），评估更全面（包括多种物理变化），不依赖梯度信息（适用于黑盒场景），且提供了系统化的方法探索物理变化空间。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Eva-VLA首次系统性地评估了视觉-语言-行动模型在现实世界物理变化下的鲁棒性，通过将离散物理变化转化为连续优化问题，揭示了当前VLA系统在面对物体3D变换、光照变化和对抗性补丁时高达97.8%的失败率，为开发更鲁棒的机器人系统提供了关键见解和实用工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have emerged as promising solutions forrobotic manipulation, yet their robustness to real-world physical variationsremains critically underexplored. To bridge this gap, we propose Eva-VLA, thefirst unified framework that systematically evaluates the robustness of VLAmodels by transforming discrete physical variations into continuousoptimization problems. However, comprehensively assessing VLA robustnesspresents two key challenges: (1) how to systematically characterize diversephysical variations encountered in real-world deployments while maintainingevaluation reproducibility, and (2) how to discover worst-case scenarioswithout prohibitive real-world data collection costs efficiently. To addressthe first challenge, we decompose real-world variations into three criticaldomains: object 3D transformations that affect spatial reasoning, illuminationvariations that challenge visual perception, and adversarial patches thatdisrupt scene understanding. For the second challenge, we introduce acontinuous black-box optimization framework that transforms discrete physicalvariations into parameter optimization, enabling systematic exploration ofworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA modelsacross multiple benchmarks reveal alarming vulnerabilities: all variation typestrigger failure rates exceeding 60%, with object transformations causing up to97.8% failure in long-horizon tasks. Our findings expose critical gaps betweencontrolled laboratory success and unpredictable deployment readiness, while theEva-VLA framework provides a practical pathway for hardening VLA-based roboticmanipulation models against real-world deployment challenges.</description>
      <author>example@mail.com (Hanqing Liu, Jiahuan Long, Junqi Wu, Jiacheng Hou, Huili Tang, Tingsong Jiang, Weien Zhou, Wen Yao)</author>
      <guid isPermaLink="false">2509.18953v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective</title>
      <link>http://arxiv.org/abs/2509.18905v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  a comprehensive visual spatial reasoning evaluation tool, 25 pages,  16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文对视觉语言模型中的视觉空间推理进行了系统性研究，创建了空间智能基准测试SIBench，并发现当前模型在基本感知任务中表现良好，但在空间理解和规划任务中存在明显差距。&lt;h4&gt;背景&lt;/h4&gt;视觉空间推理是人类核心认知能力，对推进具身智能和自主系统至关重要。尽管视觉语言模型近期取得进展，但实现人类水平的视觉空间推理仍面临三维空间表示和推理复杂性的挑战。&lt;h4&gt;目的&lt;/h4&gt;系统调查视觉语言模型中的视觉空间推理，涵盖输入模式、模型架构、训练策略和推理机制；将空间智能分为三个能力级别；创建空间智能基准测试。&lt;h4&gt;方法&lt;/h4&gt;对视觉语言模型中的视觉空间推理进行系统性研究；回顾现有方法；将空间智能分为基本感知、空间理解和空间规划三个层次；整理包含近20个开源数据集和23个任务设置的空间智能基准测试SIBench。&lt;h4&gt;主要发现&lt;/h4&gt;最先进的视觉语言模型在感知和推理之间存在明显差距，模型在基本感知任务中表现良好，但在理解和规划任务中表现不佳，特别是在数值估计、多视图推理、时间动态和空间想象方面。&lt;h4&gt;结论&lt;/h4&gt;实现空间智能仍存在重大挑战，该研究提供了系统性的路线图和全面的基准测试来推动该领域未来研究。相关资源可在https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/获取。&lt;h4&gt;翻译&lt;/h4&gt;视觉空间推理是一种核心的人类认知能力，对推进具身智能和自主系统至关重要。尽管视觉语言模型最近有所进展，但由于三维空间表示和推理的复杂性，实现人类水平的视觉空间推理仍然极具挑战性。在本文中，我们对视觉语言模型中的视觉空间推理进行了系统性研究，涵盖了输入模式、模型架构、训练策略和推理机制等方面的现有方法回顾。此外，我们将空间智能分为三个能力级别：基本感知、空间理解和空间规划，并整理了SIBench，这是一个包含近20个开源数据集和23个任务设置的空间智能基准测试。使用最先进视觉语言模型的实验显示，感知和推理之间存在明显差距，模型在基本感知任务中表现出能力，但在理解和规划任务中持续表现不佳，特别是在数值估计、多视图推理、时间动态和空间想象方面。这些发现强调了在实现空间智能方面仍然存在的重大挑战，同时为该领域的未来研究提供了系统性的路线图和全面的基准测试。本研究的相关资源可通过https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决评估视觉语言模型在视觉空间推理方面的能力，找出当前模型与人类水平空间智能之间的差距。这个问题很重要，因为视觉空间推理是人类的基本认知能力，对具身智能和自动驾驶等AI领域的发展至关重要，而现有评估基准碎片化，无法全面评估模型能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析视觉空间推理的重要性及现有评估的局限性，然后通过文献综述分析现有方法，借鉴了现有工作的分类框架但提出了更系统的三层认知水平分类，整合了近20个开源数据集构建SIBench基准，确保数据质量和多样性，最后通过实验评估揭示模型能力差距。作者确实借鉴了现有工作，特别是在分类方法和数据集整合方面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是系统评估视觉语言模型在视觉空间推理方面的能力，找出差距并提供全面评估基准。流程包括：1)文献综述与分类，按输入模态、模型架构等分类现有方法；2)构建SIBench基准，整合数据集并确保质量多样性；3)实验评估最先进VLMs；4)结果分析与提出未来方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统性VSR方法综述；2)基于认知水平的分层任务分类(基础感知、空间理解、空间规划)；3)开发SIBench评估基准，整合23个任务设置；4)揭示模型能力差距。相比之前工作，本文提供了更系统的评估框架、更科学的分类方法、更全面的分析和更实用的公开资源。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统综述现有方法、构建分层任务分类和开发全面的SIBench评估基准，揭示了当前视觉语言模型在视觉空间推理方面的能力差距，为未来研究提供了系统性的路线图。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Spatial Reasoning (VSR) is a core human cognitive ability and acritical requirement for advancing embodied intelligence and autonomoussystems. Despite recent progress in Vision-Language Models (VLMs), achievinghuman-level VSR remains highly challenging due to the complexity ofrepresenting and reasoning over three-dimensional space. In this paper, wepresent a systematic investigation of VSR in VLMs, encompassing a review ofexisting methodologies across input modalities, model architectures, trainingstrategies, and reasoning mechanisms. Furthermore, we categorize spatialintelligence into three levels of capability, ie, basic perception, spatialunderstanding, spatial planning, and curate SIBench, a spatial intelligencebenchmark encompassing nearly 20 open-source datasets across 23 task settings.Experiments with state-of-the-art VLMs reveal a pronounced gap betweenperception and reasoning, as models show competence in basic perceptual tasksbut consistently underperform in understanding and planning tasks, particularlyin numerical estimation, multi-view reasoning, temporal dynamics, and spatialimagination. These findings underscore the substantial challenges that remainin achieving spatial intelligence, while providing both a systematic roadmapand a comprehensive benchmark to drive future research in the field. Therelated resources of this study are accessible athttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.</description>
      <author>example@mail.com (Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang, Zhedong Zheng, Zhipeng Zhang, Yifan Wang, Lin Song, Lijun Wang, Yanwei Li, Ying Shan, Huchuan Lu)</author>
      <guid isPermaLink="false">2509.18905v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing</title>
      <link>http://arxiv.org/abs/2509.18897v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个名为RS3DBench的新基准测试，包含大量配对的遥感图像和深度图，以及相应的文本描述，并基于稳定扩散开发了遥感深度估计模型，推动了3D视觉感知模型在遥感领域的发展。&lt;h4&gt;背景&lt;/h4&gt;现有的遥感数据集通常缺乏全面的深度信息或无法在深度数据和遥感图像之间建立精确的对应关系，限制了通用大规模3D视觉模型在遥感领域的发展。&lt;h4&gt;目的&lt;/h4&gt;创建一个全面的基准测试，推动通用大规模3D视觉模型在遥感影像领域的发展，并开发高性能的遥感深度估计模型。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含54,951对遥感图像和像素级对齐深度图的数据集，并开发了基于稳定扩散的遥感深度估计模型，利用其多模态融合能力。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的RS3DBench数据集为训练和评估3D视觉感知模型提供了有效工具，基于稳定扩散的深度估计模型在数据集上取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为3D视觉感知模型的发展和地理人工智能在遥感领域的进步做出了重要贡献，数据集、模型和代码已公开可供使用。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们介绍了一个新的基准测试，旨在推动通用大规模3D视觉模型在遥感影像领域的发展。虽然已经提出了几个遥感领域的数据集，但许多现有数据集要么缺乏全面的深度信息，要么无法在深度数据和遥感图像之间建立精确的对应关系。为了解决这一不足，我们提出了一个用于遥感图像3D理解的视觉基准测试，命名为RS3DBench。该数据集包含54,951对遥感图像和像素级对齐的深度图，以及相应的文本描述，涵盖了广泛的地理背景。它可作为训练和评估遥感图像空间理解任务中3D视觉感知模型的工具。此外，我们介绍了一个基于稳定扩散的遥感深度估计模型，利用其多模态融合能力，从而在我们的数据集上取得了最先进的性能。我们的工作旨在为3D视觉感知模型的发展和遥感领域地理人工智能的进步做出重大贡献。数据集、模型和代码可通过https://rs3dbench.github.io获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决遥感领域缺乏像素级对齐的深度信息数据集的问题。这个问题很重要，因为高度信息是3D地理空间推理的基础，但现有数据集要么缺乏深度信息，要么无法将深度数据与遥感图像精确对齐。这限制了遥感图像在自动化土地利用制图、城市3D重建、灾害损失评估等应用中的3D空间感知能力发展，也阻碍了3D视觉感知模型从自然场景向遥感领域的迁移。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有深度估计模型在遥感领域的局限性，认识到数据分布差异和深度信息缺失是主要障碍。他们借鉴了自然场景中的3D视觉感知方法（如Marigold扩散模型），但针对遥感场景的特殊性进行了改进。作者设计了一个半自动化数据获取流程，包括数据爬取、深度对齐、文本注释和后处理四个阶段，并引入了地理语义文本信息来增强深度估计，解决了传统方法在处理遥感图像小物体和复杂空间推理时的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将遥感图像的深度估计视为一个条件扩散过程，同时利用图像和文本信息来指导深度图的生成。整体实现流程包括：1)数据爬取：整合多源DEM数据；2)对齐：将深度数据与对应的RGB图像进行像素级对齐；3)注释：使用GLM-v4大模型为图像添加文本描述；4)后处理：图像增强和质量检查。在模型方面，作者基于Marigold框架，引入文本条件通过交叉注意力机制与图像特征融合，提高了复杂地形场景中的深度估计精度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出首个包含像素级对齐的RGB-DEM对的遥感数据集RS3DBench；2)数据集覆盖多种分辨率(30m到0.5m)和全球四大洲的多样化地形；3)提出结合地理语义文本的扩散模型(Marigold-RS)用于遥感深度估计；4)建立全面的评估框架和协议。相比之前工作，不同之处在于：解决了传统数据集缺乏深度信息和RGB-DEM对齐的问题；突破了室内/城市场景的限制，扩展到自然地形分析；通过文本-视觉融合提高了跨地形泛化能力，特别是在复杂地形区域表现出色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RS3DBench通过提供首个像素级对齐的大规模遥感图像-深度图数据集和结合地理语义的深度估计方法，显著推动了遥感领域中3D空间感知技术的发展和应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce a novel benchmark designed to propel theadvancement of general-purpose, large-scale 3D vision models for remote sensingimagery. While several datasets have been proposed within the realm of remotesensing, many existing collections either lack comprehensive depth informationor fail to establish precise alignment between depth data and remote sensingimages. To address this deficiency, we present a visual Benchmark for 3Dunderstanding of Remotely Sensed images, dubbed RS3DBench. This datasetencompasses 54,951 pairs of remote sensing images and pixel-level aligned depthmaps, accompanied by corresponding textual descriptions, spanning a broad arrayof geographical contexts. It serves as a tool for training and assessing 3Dvisual perception models within remote sensing image spatial understandingtasks. Furthermore, we introduce a remotely sensed depth estimation modelderived from stable diffusion, harnessing its multimodal fusion capabilities,thereby delivering state-of-the-art performance on our dataset. Our endeavorseeks to make a profound contribution to the evolution of 3D visual perceptionmodels and the advancement of geographic artificial intelligence within theremote sensing domain. The dataset, models and code will be accessed on thehttps://rs3dbench.github.io.</description>
      <author>example@mail.com (Jiayu Wang, Ruizhi Wang, Jie Song, Haofei Zhang, Mingli Song, Zunlei Feng, Li Sun)</author>
      <guid isPermaLink="false">2509.18897v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>VGGT-DP: Generalizable Robot Control via Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2509.18778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VGGT-DP的视觉运动策略框架，通过整合预训练3D感知模型的几何先验和本体感觉反馈，提高机器人的空间理解和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有视觉模仿学习框架主要关注策略设计，但忽视了视觉编码器的结构和能力，限制了机器人的空间理解和泛化能力。生物视觉系统依靠视觉和本体感觉线索实现稳健控制。&lt;h4&gt;目的&lt;/h4&gt;受生物视觉系统启发，提出VGGT-DP框架，整合几何先验和本体感觉反馈，改善机器人的空间基础和闭环控制能力。&lt;h4&gt;方法&lt;/h4&gt;采用视觉几何基础Transformer(VGGT)作为视觉编码器；引入本体感觉引导的视觉学习策略对齐感知与机器人内部状态；设计逐帧令牌重用机制减少推理延迟；应用随机令牌修剪增强策略鲁棒性并减少过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的MetaWorld任务上，VGGT-DP显著优于DP和DP3等基线方法，特别是在精度关键和长场景情况下表现出色。&lt;h4&gt;结论&lt;/h4&gt;通过整合几何先验和本体感觉反馈，VGGT-DP框架能够提供更好的空间理解和泛化能力，使机器人在复杂操作任务中表现更佳。&lt;h4&gt;翻译&lt;/h4&gt;视觉模仿学习框架允许机器人从专家演示中学习操作技能。虽然现有方法主要关注策略设计，但它们常常忽视视觉编码器的结构和能力，限制了空间理解和泛化能力。受依靠视觉和本体感觉线索进行稳健控制的生物视觉系统启发，我们提出了VGGT-DP，一种将预训练3D感知模型的几何先验与本体感觉反馈相结合的视觉运动策略框架。我们采用视觉几何基础Transformer(VGGT)作为视觉编码器，并引入本体感觉引导的视觉学习策略，使感知与机器人内部状态保持一致，提高空间基础和闭环控制能力。为减少推理延迟，我们设计了逐帧令牌重用机制，将多视图令牌压缩为高效的空间表示。我们还应用随机令牌修剪来增强策略鲁棒性并减少过拟合。在具有挑战性的MetaWorld任务上的实验表明，VGGT-DP显著优于DP和DP3等强基线方法，特别是在精度关键和长场景情况下。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人视觉模仿学习框架中视觉编码器结构和容量不足的问题，限制了机器人的空间理解和泛化能力。这个问题在现实中很重要，因为它导致机器人面对环境微小变化时性能显著下降，难以在动态环境中灵活操作，限制了机器人在复杂任务中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从生物视觉系统获得灵感，注意到生物能同时利用视觉和本体感觉线索进行鲁棒控制，而不依赖语言。他们观察到动物神经资源中很大部分用于视觉处理，这有助于与动态环境交互。作者借鉴了扩散策略（DP）和视觉几何基础Transformer（VGGT）等现有工作，但选择不依赖语言先验，而是专注于视觉和本体感觉的融合，设计出结合VGGT强大几何基础与扩散策略概率推理能力的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是模拟生物系统的多模态感知，利用大规模预训练的3D视觉模型提供几何先验，并确保视觉特征与机器人内部状态对齐。整体流程包括：1)接收多视角图像和本体感觉信号；2)使用VGGT提取几何感知的视觉特征；3)通过帧级令重用机制减少计算；4)应用随机令剪枝增强鲁棒性；5)使用本体感觉引导的视觉学习对齐特征；6)通过扩散策略生成动作序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)VGGT-DP框架整合3D重建预训练视觉模型与扩散策略；2)帧级令重用机制加速推理；3)随机令剪枝增强鲁棒性；4)本体感觉引导的视觉学习方法。相比之前工作，VGGT-DP不依赖语言先验，使用大规模视觉编码器而非小编码器，专注于3D空间结构建模而非仅语义感知，并强调视觉与机器人内部状态的对齐，同时解决了大规模视觉模型的计算效率问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VGGT-DP通过整合强大的3D视觉基础模型与本体感觉引导的扩散策略，显著提升了机器人在复杂空间任务中的泛化能力和控制精度，同时通过创新的计算优化技术解决了大规模视觉模型的效率问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual imitation learning frameworks allow robots to learn manipulationskills from expert demonstrations. While existing approaches mainly focus onpolicy design, they often neglect the structure and capacity of visualencoders, limiting spatial understanding and generalization. Inspired bybiological vision systems, which rely on both visual and proprioceptive cuesfor robust control, we propose VGGT-DP, a visuomotor policy framework thatintegrates geometric priors from a pretrained 3D perception model withproprioceptive feedback. We adopt the Visual Geometry Grounded Transformer(VGGT) as the visual encoder and introduce a proprioception-guided visuallearning strategy to align perception with internal robot states, improvingspatial grounding and closed-loop control. To reduce inference latency, wedesign a frame-wise token reuse mechanism that compacts multi-view tokens intoan efficient spatial representation. We further apply random token pruning toenhance policy robustness and reduce overfitting. Experiments on challengingMetaWorld tasks show that VGGT-DP significantly outperforms strong baselinessuch as DP and DP3, particularly in precision-critical and long-horizonscenarios.</description>
      <author>example@mail.com (Shijia Ge, Yinxin Zhang, Shuzhao Xie, Weixiang Zhang, Mingcai Zhou, Zhi Wang)</author>
      <guid isPermaLink="false">2509.18778v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning</title>
      <link>http://arxiv.org/abs/2509.18757v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  For project website and videos, see https https://mv-umi.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MV-UMI的多视图通用操作接口框架，通过整合第三人称视角与第一人称视角相机，克服了手持夹爪数据收集设备在场景上下文捕捉方面的局限性，提升了机器人操作性能。&lt;h4&gt;背景&lt;/h4&gt;最近的模仿学习在从演示中开发强大的机器人操作策略方面显示出巨大潜力，但这种潜力依赖于多样化、高质量的数据集，这些数据集收集起来具有挑战性且成本高昂，并且通常局限于特定机器人形态。&lt;h4&gt;目的&lt;/h4&gt;提出MV-UMI框架，整合第三人称视角与自我中心视角相机，以克服手持夹爪仅依赖第一人称视角相机而无法充分捕捉场景上下文的限制。&lt;h4&gt;方法&lt;/h4&gt;MV-UMI框架结合第三人称视角和自我中心视角，减轻了人类演示与机器人部署之间的领域偏移，同时保留了手持数据收集设备的跨形态优势。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果（包括消融研究）表明，MV-UMI框架在需要广泛场景理解的子任务中，将性能提高了约47%（跨3个任务），证实了该方法在扩展可行操作任务范围方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;MV-UMI方法在不损害手持夹爪系统固有的跨形态优势的情况下，扩展了可以使用此类系统学习的可行操作任务范围。&lt;h4&gt;翻译&lt;/h4&gt;最近的模仿学习进展在从演示中开发强大的机器人操作策略方面显示出巨大潜力。然而，这种潜力依赖于多样化、高质量的数据集，这些数据集不仅收集起来具有挑战性和成本高昂，而且通常局限于特定的机器人形态。便携式手持夹爪作为传统机器人遥操作方法的数据收集替代方案，最近已经出现，它们直观且可扩展。然而，它们仅依赖第一人称视角的手腕安装相机，常常限制了充分捕捉场景上下文的能力。在本文中，我们提出了MV-UMI（多视图通用操作接口），这是一个整合第三人称视角与自我中心视角相机的框架，以克服这一限制。这种整合减轻了人类演示与机器人部署之间的领域偏移，保留了手持数据收集设备的跨形态优势。我们的实验结果（包括消融研究）表明，我们的MV-UMI框架在需要广泛场景理解的子任务中，将性能提高了约47%（跨3个任务），证实了我们的方法在扩展可以使用手持夹爪系统学习的可行操作任务范围方面的有效性，同时不损害此类系统固有的跨形态优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in imitation learning have shown great promise for developingrobust robot manipulation policies from demonstrations. However, this promiseis contingent on the availability of diverse, high-quality datasets, which arenot only challenging and costly to collect but are often constrained to aspecific robot embodiment. Portable handheld grippers have recently emerged asintuitive and scalable alternatives to traditional robotic teleoperationmethods for data collection. However, their reliance solely on first-personview wrist-mounted cameras often creates limitations in capturing sufficientscene contexts. In this paper, we present MV-UMI (Multi-View UniversalManipulation Interface), a framework that integrates a third-person perspectivewith the egocentric camera to overcome this limitation. This integrationmitigates domain shifts between human demonstration and robot deployment,preserving the cross-embodiment advantages of handheld data-collection devices.Our experimental results, including an ablation study, demonstrate that ourMV-UMI framework improves performance in sub-tasks requiring broad sceneunderstanding by approximately 47% across 3 tasks, confirming the effectivenessof our approach in expanding the range of feasible manipulation tasks that canbe learned using handheld gripper systems, without compromising thecross-embodiment advantages inherent to such systems.</description>
      <author>example@mail.com (Omar Rayyan, John Abanes, Mahmoud Hafez, Anthony Tzes, Fares Abu-Dakka)</author>
      <guid isPermaLink="false">2509.18757v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.18609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PIE框架，一种整合高级感知、推理和意图建模的创新方法，用于端到端自动驾驶运动规划。&lt;h4&gt;背景&lt;/h4&gt;端到端运动规划在简化自动驾驶复杂流程方面很有前景，但场景理解和有效预测等挑战仍是其大规模部署的主要障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够动态捕捉自车与周围智能体之间交互的框架，解决自动驾驶中的感知和决策问题。&lt;h4&gt;方法&lt;/h4&gt;PIE框架包含双向Mamba融合解决多模态数据融合损失，推理增强解码器促进场景兼容的锚点选择，以及动作-运动交互模块优化自车规划。&lt;h4&gt;主要发现&lt;/h4&gt;PIE在NAVSIM基准上实现了88.9的PDM分数和85.6的EPDM分数，不使用集成和数据增强技术就超越了先前最先进方法。&lt;h4&gt;结论&lt;/h4&gt;PIE能够可靠地生成可行的高质量自车轨迹，为自动驾驶端到端规划提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;端到端运动规划在简化复杂自动驾驶流程方面前景广阔。然而，场景理解和有效预测等挑战继续对其大规模部署构成重大障碍。在本文中，我们提出了PIE，一个开创性框架，整合了高级感知、推理和意图建模，以动态捕捉自车与周围智能体之间的交互。它包含双向Mamba融合，解决了摄像头和LiDAR输入多模态融合中的数据压缩损失，同时采用新颖的推理增强解码器，集成Mamba和Mixture-of-Experts，促进场景兼容的锚点选择和优化自适应轨迹推断。PIE采用动作-运动交互模块，有效利用周围智能体的状态预测来优化自车规划。所提出的框架在NAVSIM基准上得到了全面验证。PIE不使用任何集成和数据增强技术，实现了88.9的PDM分数和85.6的EPDM分数，超越了先前最先进方法的性能。全面的定量和定性分析表明，PIE能够可靠地生成可行的高质量自车轨迹。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶端到端运动规划中的场景理解、有效预测和多模态数据融合问题。这些问题在现实中至关重要，因为它们直接关系到自动驾驶系统的安全性、可靠性和计算效率。有效的场景理解和预测能让车辆更好地应对复杂交通环境，而高效的多模态融合则能提高感知准确性，这些都是自动驾驶大规模部署必须克服的关键障碍。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有端到端自动驾驶方法的不足，特别是多模态融合中的信息损失、复杂场景推理能力不足以及计算效率低等问题。基于这些分析，他们设计了PIE框架，借鉴了多种现有工作：端到端学习范式如UniAD和Transfuser，Mamba架构用于处理长序列数据，Mixture-of-Experts扩展模型容量，以及运动预测方法。作者将这些技术有机结合，针对性地解决了自动驾驶规划中的关键挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; PIE的核心思想是通过整合先进的感知、推理和意图建模来动态捕捉自车与周围交通参与者之间的交互。整体流程包括：1)接收多视图相机图像和LiDAR BEV输入；2)通过双向Mamba融合模块有效融合不同模态数据；3)利用推理增强解码器处理序列信息并应用MoE处理复杂场景；4)通过动作-运动交互模块整合自车动作与周围代理的运动状态；5)最终输出自车轨迹、动作和周围代理的状态信息，实现高效且安全的自动驾驶决策。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1)双向Mamba融合：通过两种不同顺序的特征融合并相加结果，有效解决多模态数据融合中的信息损失；2)推理增强解码器：结合Mamba和Mixture-of-Experts，实现场景合规的锚点选择和自适应轨迹推断；3)动作-运动交互模块：通过共享交叉注意力机制整合周围代理的速度预测。相比之前工作，PIE在多模态融合效率、复杂场景推理能力和预测-规划整合方面都有显著改进，同时保持了较高的计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PIE通过创新的感知与交互增强框架，实现了自动驾驶端到端运动规划的重大突破，在NAVSIM基准测试上达到了最先进的性能水平。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end motion planning is promising for simplifying complex autonomousdriving pipelines. However, challenges such as scene understanding andeffective prediction for decision-making continue to present substantialobstacles to its large-scale deployment. In this paper, we present PIE, apioneering framework that integrates advanced perception, reasoning, andintention modeling to dynamically capture interactions between the ego vehicleand surrounding agents. It incorporates a bidirectional Mamba fusion thataddresses data compression losses in multimodal fusion of camera and LiDARinputs, alongside a novel reasoning-enhanced decoder integrating Mamba andMixture-of-Experts to facilitate scene-compliant anchor selection and optimizeadaptive trajectory inference. PIE adopts an action-motion interaction moduleto effectively utilize state predictions of surrounding agents to refine egoplanning. The proposed framework is thoroughly validated on the NAVSIMbenchmark. PIE, without using any ensemble and data augmentation techniques,achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance ofprior state-of-the-art methods. Comprehensive quantitative and qualitativeanalyses demonstrate that PIE is capable of reliably generating feasible andhigh-quality ego trajectories.</description>
      <author>example@mail.com (Chengran Yuan, Zijian Lu, Zhanqi Zhang, Yimin Zhao, Zefan Huang, Shuo Sun, Jiawei Sun, Jiahui Li, Christina Dao Wen Lee, Dongen Li, Marcelo H. Ang Jr)</author>
      <guid isPermaLink="false">2509.18609v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration</title>
      <link>http://arxiv.org/abs/2509.17429v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了多尺度时间预测(MSTP)任务，旨在解决视觉语言模型在预测多个精细粒度场景状态时的困难。作者将多尺度分解为时间尺度和状态尺度两个正交维度，并提出了MSTP基准和增量生成与多智能体协作(IG-MC)方法，以提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;准确的时间预测是全面场景理解和具身人工智能之间的桥梁，但视觉语言模型在预测多个精细粒度的场景状态时存在困难。特别是在一般场景和手术场景中，不同层次的状态关系和预测间隔使得预测任务更加复杂。&lt;h4&gt;目的&lt;/h4&gt;正式定义多尺度时间预测(MSTP)任务，并提出相应的方法和基准来支持这一任务，提高视觉语言模型在多尺度状态预测中的性能。&lt;h4&gt;方法&lt;/h4&gt;作者提出了增量生成和多智能体协作(IG-MC)方法，包含两个关键创新：1) 插拔式增量生成模块，持续合成最新的视觉预览以供多个决策智能体使用；2) 决策驱动的多智能体协作框架，包括生成、发起和多状态评估智能体，动态触发和评估预测周期以平衡全局一致性和局部保真度。同时，作者还引入了首个MSTP基准，提供跨多个状态尺度和时间尺度的同步标注。&lt;h4&gt;主要发现&lt;/h4&gt;1. 多尺度时间预测任务可以分解为时间尺度和状态尺度两个正交维度；2. 增量生成模块可以防止随着预测间隔延长而导致的性能下降；3. 多智能体协作框架能够平衡全局一致性和局部保真度。&lt;h4&gt;结论&lt;/h4&gt;通过提出MSTP任务定义、基准和IG-MC方法，该研究为视觉语言模型在多尺度时间预测方面提供了有效的解决方案，有助于提高场景理解和具身人工智能的能力。&lt;h4&gt;翻译&lt;/h4&gt;准确的时间预测是全面场景理解和具身人工智能之间的桥梁。然而，预测多个精细粒度的场景状态在多个时间尺度上对视觉语言模型来说很困难。我们通过将多尺度分解为两个正交维度来正式定义一般场景和手术场景中的多尺度时间预测(MSTP)任务：时间尺度，预测不同前瞻间隔下人类和手术的状态；以及状态尺度，建模一般场景和手术场景中的状态层次。例如，在一般场景中，接触关系的状态比空间关系的状态更精细。在手术场景中，中级步骤比高级阶段更精细但仍受其包含阶段的约束。为了支持这一统一任务，我们引入了首个MSTP基准，具有跨多个状态尺度和时间尺度的同步标注。我们进一步提出了一种方法，增量生成和多智能体协作(IG-MC)，它集成了两个关键创新。首先，我们提出了一个即插即用的增量生成模块，该模块在扩展的时间尺度上持续合成最新的视觉预览，以通知多个决策智能体，使决策和生成的视觉保持同步，并防止随着前瞻间隔延长而导致的性能下降。其次，我们提出了一个用于多状态预测的决策驱动多智能体协作框架，包括生成、发起和多状态评估智能体，这些智能体动态触发和评估预测周期，以平衡全局一致性和局部保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate temporal prediction is the bridge between comprehensive sceneunderstanding and embodied artificial intelligence. However, predictingmultiple fine-grained states of a scene at multiple temporal scales isdifficult for vision-language models. We formalize the Multi-Scale TemporalPrediction (MSTP) task in general and surgical scenes by decomposingmulti-scale into two orthogonal dimensions: the temporal scale, forecastingstates of humans and surgery at varying look-ahead intervals, and the statescale, modeling a hierarchy of states in general and surgical scenes. Forexample, in general scenes, states of contact relationships are finer-grainedthan states of spatial relationships. In surgical scenes, medium-level stepsare finer-grained than high-level phases yet remain constrained by theirencompassing phase. To support this unified task, we introduce the first MSTPBenchmark, featuring synchronized annotations across multiple state scales andtemporal scales. We further propose a method, Incremental Generation andMulti-agent Collaboration (IG-MC), which integrates two key innovations. First,we present a plug-and-play incremental generation module that continuouslysynthesizes up-to-date visual previews at expanding temporal scales to informmultiple decision-making agents, keeping decisions and generated visualssynchronized and preventing performance degradation as look-ahead intervalslengthen. Second, we present a decision-driven multi-agent collaborationframework for multi-state prediction, comprising generation, initiation, andmulti-state assessment agents that dynamically trigger and evaluate predictioncycles to balance global coherence and local fidelity.</description>
      <author>example@mail.com (Zhitao Zeng, Guojian Yuan, Junyuan Mao, Yuxuan Wang, Xiaoshuang Jia, Yueming Jin)</author>
      <guid isPermaLink="false">2509.17429v2</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity</title>
      <link>http://arxiv.org/abs/2509.19220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出FedFusion框架，统一领域适应性和经济标记方法，用于处理实际联邦学习中的异构特征空间、严重非IID数据和跨客户端标签稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;实际联邦学习必须处理异构特征空间、严重的非IID数据和跨客户端的稀缺标签问题，现有方法在这些挑战下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理异构数据分布、减少标签需求并保持全局一致性的联邦学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出FedFusion框架，使用多样性/感知聚类的编码器；教师客户端通过置信度过滤的伪标签和领域自适应迁移指导学习者；客户端保持个性化编码器；采用相似度加权的分类器耦合保持全局一致性；结合自监督/半监督预训练和选择性微调的经济标记流程。&lt;h4&gt;主要发现&lt;/h4&gt;在表格和图像基准测试中，FedFusion在IID、非IID和标签稀缺情况下，在准确性、鲁棒性和公平性方面持续优于最先进基线方法；减轻了数据丰富站点的主导作用，改善了少数客户端性能；同时保持了可比的通信和计算预算。&lt;h4&gt;结论&lt;/h4&gt;将个性化、领域适应性和标签效率协调起来是应对实际约束下稳健联邦学习的有效策略。&lt;h4&gt;翻译&lt;/h4&gt;实际联邦学习必须处理异构特征空间、严重的非IID数据和跨客户端的稀缺标签。我们提出了FedFusion，一个联邦迁移学习框架，统一了领域适应性和经济标记方法，使用多样性/感知聚类的编码器。带标签的教师客户端通过置信度过滤的伪标签和领域自适应迁移指导学习者客户端，同时客户端保持针对本地数据的个性化编码器。为了在异构性下保持全局一致性，FedFusion采用相似度加权的分类器耦合，减轻了数据丰富站点的主导作用，并改善了少数客户端的性能。经济标记流程结合了自监督/半监督预训练和选择性微调，减少了标注需求，同时不共享原始数据。在IID、非IID和标签稀缺情况下的表格和图像基准测试中，FedFusion在准确性、鲁棒性和公平性方面持续优于最先进的基线方法，同时保持可比的通信和计算预算。这些结果表明，协调个性化、领域适应性和标记效率是应对实际约束下稳健联邦学习的有效方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning in practice must contend with heterogeneous featurespaces, severe non-IID data, and scarce labels across clients. We presentFedFusion, a federated transfer-learning framework that unifies domainadaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients viaconfidence-filtered pseudo-labels and domain-adaptive transfer, while clientsmaintain personalised encoders tailored to local data. To preserve globalcoherence under heterogeneity, FedFusion employs similarity-weighted classifiercoupling (with optional cluster-wise averaging), mitigating dominance bydata-rich sites and improving minority-client performance. The frugal-labellingpipeline combines self-/semi-supervised pretext training with selectivefine-tuning, reducing annotation demands without sharing raw data. Acrosstabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,FedFusion consistently outperforms state-of-the-art baselines in accuracy,robustness, and fairness while maintaining comparable communication andcomputation budgets. These results show that harmonising personalisation,domain adaptation, and label efficiency is an effective recipe for robustfederated learning under real-world constraints.</description>
      <author>example@mail.com (Ferdinand Kahenga, Antoine Bagula, Patrick Sello, Sajal K. Das)</author>
      <guid isPermaLink="false">2509.19220v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Autoencoder: An efficient approach to quantum feature map generation</title>
      <link>http://arxiv.org/abs/2509.19157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了量子自编码器在学习数据驱动的量子表示方面的能力，证明其在多个肽分类任务中优于传统量子方法。&lt;h4&gt;背景&lt;/h4&gt;量子机器学习方法通常依赖于固定的人工设计的量子编码，这些编码可能无法为下游任务捕获最佳特征。&lt;h4&gt;目的&lt;/h4&gt;研究量子自编码器在学习数据驱动的量子表示方面的能力，探索其在实际应用中的有效性。&lt;h4&gt;方法&lt;/h4&gt;理论上证明量子自编码器在样本复杂度上的高效性；在300万条肽序列上数值训练量子自编码器；评估其在降压肽预测、血脑屏障穿透和细胞毒性活性检测等任务中的表现；使用量子核与支持向量机与哈密顿量演化基线进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;量子自编码器学习到的表示在七个数据集上比哈密顿量基线提高了0.4%到8.1%的准确率；展示了有效的泛化能力和迁移学习能力；能够从大规模数据集中高效学习。&lt;h4&gt;结论&lt;/h4&gt;量子自编码器架构能够从大规模数据集（300万个样本）中有效学习，具有紧凑的参数化（约900个参数），展示了它们在实际量子应用中的可行性。&lt;h4&gt;翻译&lt;/h4&gt;量子机器学习方法通常依赖于固定、手工设计的量子编码，这些编码可能无法为下游任务捕获最佳特征。在这项工作中，我们研究了量子自编码器在学习数据驱动的量子表示方面的能力。我们首先从理论上证明了量子自编码器方法在整个训练过程中在样本复杂度方面是高效的。然后我们在300万条肽序列上数值训练量子自编码器，并评估它们在多个肽分类问题中的有效性，包括降压肽预测、血脑屏障穿透和细胞毒性活性检测。使用量子核与支持向量机，将学习到的表示与哈密顿量演化的基线进行了比较。结果表明，量子自编码器学习到的表示在七个数据集上比哈密顿量基线提高了0.4%到8.1%的准确率，展示了有效的泛化能力，能够推广到多样化的下游数据集，预训练使有效的迁移学习成为可能，无需针对特定任务的微调。这项工作确立了量子自编码器架构能够从大规模数据集（300万个样本）中有效学习，具有紧凑的参数化（约900个参数），展示了它们在实际量子应用中的可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum machine learning methods often rely on fixed, hand-crafted quantumencodings that may not capture optimal features for downstream tasks. In thiswork, we study the power of quantum autoencoders in learning data-drivenquantum representations. We first theoretically demonstrate that the quantumautoencoder method is efficient in terms of sample complexity throughout theentire training process. Then we numerically train the quantum autoencoder on 3million peptide sequences, and evaluate their effectiveness across multiplepeptide classification problems including antihypertensive peptide prediction,blood-brain barrier-penetration, and cytotoxic activity detection. The learnedrepresentations were compared against Hamiltonian-evolved baselines using aquantum kernel with support vector machines. Results show that quantumautoencoder learned representations achieve accuracy improvements ranging from0.4\% to 8.1\% over Hamiltonian baselines across seven datasets, demonstratingeffective generalization to diverse downstream datasets with pre-trainingenabling effective transfer learning without task-specific fine-tuning. Thiswork establishes that quantum autoencoder architectures can effectively learnfrom large-scale datasets (3 million samples) with compact parameterizations($\sim$900 parameters), demonstrating their viability for practical quantumapplications.</description>
      <author>example@mail.com (Shengxin Zhuang, Yusen Wu, Xavier F. Cadet, Du Q. Huynh, Wei Liu, Philippe Charton, Cedric Damour, Frederic Cadet, Jingbo B. Wang)</author>
      <guid isPermaLink="false">2509.19157v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.19098v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究非上下文多臂老虎机问题在迁移学习环境中的应用，提出KL-UCB-Transfer策略，并通过模拟验证其有效性。&lt;h4&gt;背景&lt;/h4&gt;在开始拉动老虎机之前，学习者从每个源分布获得独立同分布样本，真实目标分布与源分布的距离被限制在已知范围内。&lt;h4&gt;目的&lt;/h4&gt;推导累积遗憾的下界，并提出能够匹配该下界的索引策略。&lt;h4&gt;方法&lt;/h4&gt;扩展经典Lai-Robbins结果以纳入迁移参数，提出KL-UCB-Transfer策略，并通过模拟进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;KL-UCB-Transfer在源分布和目标分布足够接近的情况下，显著优于无先验基线方法。&lt;h4&gt;结论&lt;/h4&gt;KL-UCB-Transfer策略在迁移学习环境中表现良好，能有效解决非上下文多臂老虎机问题。&lt;h4&gt;翻译&lt;/h4&gt;我们在迁移学习环境中研究非上下文多臂老虎机问题：在开始拉动之前，学习者从每个源分布ν'_k中获得N'_k个独立同分布样本，真实目标分布ν_k与源分布的距离满足已知边界d_k(ν_k, ν'_k) ≤ L_k。在此框架下，我们首先推导了一个问题相关的渐进下界，扩展了经典Lai-Robbins结果以纳入迁移参数(d_k, L_k, N'_k)。然后我们提出KL-UCB-Transfer，一个简单的索引策略，在高斯情况下匹配这个新下界。最后，我们通过模拟验证了我们的方法，表明当源分布和目标分布足够接近时，KL-UCB-Transfer显著优于无先验基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the non-contextual multi-armed bandit problem in a transfer learningsetting: before any pulls, the learner is given N'_k i.i.d. samples from eachsource distribution nu'_k, and the true target distributions nu_k lie within aknown distance bound d_k(nu_k, nu'_k) &lt;= L_k. In this framework, we firstderive a problem-dependent asymptotic lower bound on cumulative regret thatextends the classical Lai-Robbins result to incorporate the transfer parameters(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy thatmatches this new bound in the Gaussian case. Finally, we validate our approachvia simulations, showing that KL-UCB-Transfer significantly outperforms theno-prior baseline when source and target distributions are sufficiently close.</description>
      <author>example@mail.com (Adrien Prevost, Timothee Mathieu, Odalric-Ambrym Maillard)</author>
      <guid isPermaLink="false">2509.19098v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning</title>
      <link>http://arxiv.org/abs/2509.18938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper was accepted at International Conference on Tools with  Artificial Intelligence (ICTAI) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的零样本图像分类框架，结合视觉语言模型和预训练视觉模型，通过自学习循环在无标注数据的情况下进行分类。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型如CNN和ViT虽然显著提升了分类性能，但通常依赖大量标注数据，这在许多实际应用场景中成为主要障碍，特别是在数据稀缺的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标注训练数据的零样本图像分类方法，解决数据稀缺场景下的分类问题。&lt;h4&gt;方法&lt;/h4&gt;提出结合VLM和预训练视觉模型的框架，使用基于置信度的伪标记策略直接在测试数据上训练轻量级分类器，VLM识别高置信度样本，预训练视觉模型增强其视觉表示，然后迭代训练分类器捕获互补的语义和视觉线索。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在十个不同数据集上的实验评估显示，性能优于基线零样本方法，且避免了VLM微调和大型语言模型的使用，减少了对语义表示的依赖。&lt;h4&gt;结论&lt;/h4&gt;所提出的零样本图像分类框架能够在无标注数据的情况下有效进行分类，为数据稀缺场景下的图像分类提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;虽然深度学习，包括卷积神经网络(CNNs)和视觉变换器(ViTs)，显著提升了分类性能，但其通常依赖大量标注数据的特点在许多实际场景中构成了主要障碍，特别是在数据稀缺的情况下。视觉语言模型(VLMs)和使用预训练视觉模型的迁移学习似乎是解决这一问题的有前景的技术。本文提出了一种新颖的零样本图像分类框架，将VLM和预训练视觉模型结合在自学习循环中。仅需类别名称而无需标注训练数据，我们的方法利用基于置信度的伪标记策略直接在测试数据上训练轻量级分类器，实现动态适应。VLM识别高置信度样本，预训练视觉模型增强其视觉表示。这些增强的特征然后迭代训练分类器，使系统能够在无监督的情况下捕获互补的语义和视觉线索。值得注意的是，我们的方法避免了VLM微调和大型语言模型的使用，依赖视觉模型减少对语义表示的依赖。在十个不同数据集上的实验评估表明，我们的方法优于基线零样本方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While deep learning, including Convolutional Neural Networks (CNNs) andVision Transformers (ViTs), has significantly advanced classificationperformance, its typical reliance on extensive annotated datasets presents amajor obstacle in many practical scenarios where such data is scarce.Vision-language models (VLMs) and transfer learning with pre-trained visualmodels appear as promising techniques to deal with this problem. This paperproposes a novel zero-shot image classification framework that combines a VLMand a pre-trained visual model within a self-learning cycle. Requiring only theset of class names and no labeled training data, our method utilizes aconfidence-based pseudo-labeling strategy to train a lightweight classifierdirectly on the test data, enabling dynamic adaptation. The VLM identifieshigh-confidence samples, and the pre-trained visual model enhances their visualrepresentations. These enhanced features then iteratively train the classifier,allowing the system to capture complementary semantic and visual cues withoutsupervision. Notably, our approach avoids VLM fine-tuning and the use of largelanguage models, relying on the visual-only model to reduce the dependence onsemantic representation. Experimental evaluations on ten diverse datasetsdemonstrate that our approach outperforms the baseline zero-shot method.</description>
      <author>example@mail.com (Matheus Vinícius Todescato, Joel Luís Carbonera)</author>
      <guid isPermaLink="false">2509.18938v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.18553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于视觉Transformer的乳腺癌和卵巢癌检测与分类方法，通过预训练模型微调和高效预处理，在二元和多类分类任务中均展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;乳腺癌和卵巢癌是女性面临的主要健康挑战之一，早期检测可通过及时干预和治疗提高生存率，但传统的人工检测方法耗时且需要专业病理学家，资源密集。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化、高效的癌症检测方法，减轻病理学家工作负担，提高检测效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的ViT-Base-Patch16-224模型，通过公开的组织病理学图像数据集进行微调，用于二元和多类分类任务；采用预处理管道将原始图像转换为与ViT架构兼容的PyTorch张量。&lt;h4&gt;主要发现&lt;/h4&gt;在BreakHis数据集的二元分类任务中，该模型超越了现有的CNN、ViT和基于拓扑数据分析的方法；在UBC-OCEAN数据集的五类分类任务中，与最近的拓扑方法相比表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;视觉Transformer-based迁移学习与高效预处理相结合在肿瘤诊断中是有效的，为癌症检测提供了一种新的自动化方法。&lt;h4&gt;翻译&lt;/h4&gt;癌症是女性面临的主要健康挑战之一，特别是乳腺癌和卵巢癌。早期检测可以通过及时干预和治疗帮助提高生存率。检测癌症的传统方法包括手动检查乳腺X光片、CT扫描、超声波和其他类型的影像。然而，这使得过程劳动密集，需要经过培训的病理学专业知识。因此，这既耗时又资源密集。在本文中，我们介绍了一种新颖的基于视觉Transformer（ViT）的乳腺癌和卵巢癌检测和分类方法。我们使用预训练的ViT-Base-Patch16-224模型，该模型使用公开的组织病理学图像数据集针对二元和多类分类任务进行了微调。此外，我们使用预处理管道将原始组织病理学图像转换为标准化的PyTorch张量，这些张量与ViT架构兼容，也有助于提高模型性能。我们在两个基准数据集上评估了我们模型的性能：用于二元分类的BreakHis数据集和用于五类分类的UBC-OCEAN数据集，没有任何数据增强。我们的模型在二元分类中超越了现有的CNN、ViT和基于拓扑数据分析的方法。对于多类分类，它与最近的拓扑方法进行了评估，并表现出优越的性能。我们的研究强调了视觉Transformer-based迁移学习与高效预处理在肿瘤诊断中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cancer is one of the leading health challenges for women, specifically breastand ovarian cancer. Early detection can help improve the survival rate throughtimely intervention and treatment. Traditional methods of detecting cancerinvolve manually examining mammograms, CT scans, ultrasounds, and other imagingtypes. However, this makes the process labor-intensive and requires theexpertise of trained pathologists. Hence, making it both time-consuming andresource-intensive. In this paper, we introduce a novel vision transformer(ViT)-based method for detecting and classifying breast and ovarian cancer. Weuse a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for bothbinary and multi-class classification tasks using publicly availablehistopathological image datasets. Further, we use a preprocessing pipeline thatconverts raw histophological images into standardized PyTorch tensors, whichare compatible with the ViT architecture and also help improve the modelperformance. We evaluated the performance of our model on two benchmarkdatasets: the BreakHis dataset for binary classification and the UBC-OCEANdataset for five-class classification without any data augmentation. Our modelsurpasses existing CNN, ViT, and topological data analysis-based approaches inbinary classification. For multi-class classification, it is evaluated againstrecent topological methods and demonstrates superior performance. Our studyhighlights the effectiveness of Vision Transformer-based transfer learningcombined with efficient preprocessing in oncological diagnostics.</description>
      <author>example@mail.com (Richa Rawat, Faisal Ahmed)</author>
      <guid isPermaLink="false">2509.18553v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Causal Representation Learning with Observable Sources as Auxiliaries</title>
      <link>http://arxiv.org/abs/2509.19058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种因果表征学习的新框架，将可观测源作为辅助变量，使用体积保持编码器识别潜在变量，并通过变量选择方案提高潜在因素的恢复性。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习旨在通过混合函数恢复生成观测数据的潜在因素。现有方法通常需要假设潜在结构或关系来达到可识别性，并且将辅助变量限制为混合函数的外部变量，这限制了可识别性的范围。&lt;h4&gt;目的&lt;/h4&gt;扩展因果表征学习的辅助变量范围，利用系统驱动的潜在因素（这些因素可以容易地从数据中观察到或提取）来提高潜在因素的可识别性。&lt;h4&gt;方法&lt;/h4&gt;引入可观测源作为辅助变量的框架，使用体积保持编码器识别潜在变量，并提供一种变量选择方案，在已知潜在因果图的情况下选择最大化潜在因素可恢复性的辅助变量。&lt;h4&gt;主要发现&lt;/h4&gt;1. 可以使用体积保持编码器识别整个潜在变量（直到子空间变换和排列）；2. 当有多个已知辅助变量时，可以提供变量选择方案来优化潜在因素的恢复性。&lt;h4&gt;结论&lt;/h4&gt;通过将可观测源作为辅助变量，扩展了当前因果表征学习方法的边界，提高了潜在因素的可识别性和恢复性，并在合成图和图像数据上验证了框架的有效性。&lt;h4&gt;翻译&lt;/h4&gt;因果表征学习旨在通过混合函数恢复生成观测数据的潜在因素。通常需要假设潜在结构或关系来实现可识别性，先前的工作往往基于已知辅助变量的条件独立性。然而，先前的框架将辅助变量的范围限制为混合函数的外部变量。但在某些情况下，系统驱动的潜在因素可以很容易地从数据中观察到或提取，可能有助于识别。在本文中，我们引入了可观测源作为辅助变量的框架，作为有效的条件变量。我们的主要结果表明，可以使用体积保持编码器识别整个潜在变量（直到子空间变换和排列）。此外，当有多个已知辅助变量可用时，我们提供了一种变量选择方案，选择那些在已知潜在因果图的情况下最大化潜在因素可恢复性的变量。最后，我们在合成图和图像数据上的实验展示了我们框架的有效性，从而扩展了当前方法的边界。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal representation learning seeks to recover latent factors that generateobservational data through a mixing function. Needing assumptions on latentstructures or relationships to achieve identifiability in general, prior worksoften build upon conditional independence given known auxiliary variables.However, prior frameworks limit the scope of auxiliary variables to be externalto the mixing function. Yet, in some cases, system-driving latent factors canbe easily observed or extracted from data, possibly facilitatingidentification. In this paper, we introduce a framework of observable sourcesbeing auxiliaries, serving as effective conditioning variables. Our mainresults show that one can identify entire latent variables up to subspace-wisetransformations and permutations using volume-preserving encoders. Moreover,when multiple known auxiliary variables are available, we offer avariable-selection scheme to choose those that maximize recoverability of thelatent factors given knowledge of the latent causal graph. Finally, wedemonstrate the effectiveness of our framework through experiments on syntheticgraph and image data, thereby extending the boundaries of current approaches.</description>
      <author>example@mail.com (Kwonho Kim, Heejeong Nam, Inwoo Hwang, Sanghack Lee)</author>
      <guid isPermaLink="false">2509.19058v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization</title>
      <link>http://arxiv.org/abs/2509.18997v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提供了关于无标签数据表示学习的最新理论进展概述，并提到了作者在此方向上的贡献。&lt;h4&gt;背景&lt;/h4&gt;表示学习在统计学、数据科学和信号处理领域已有广泛研究，有丰富的文献涉及降维、压缩、多维尺度变换等技术。然而，当前的深度学习模型使用新的无监督表示学习原则，这些原则难以用经典理论进行分析。视觉基础模型通过自监督或去噪/掩码自编码器取得了巨大成功，能够从大量无标签数据中有效学习表示。&lt;h4&gt;目的&lt;/h4&gt;回答关于这些模型学习到的表征难以描述的问题，解释为什么这些模型在多样化的预测任务中表现良好或表现出涌现行为。&lt;h4&gt;方法&lt;/h4&gt;结合统计学和优化领域的数学工具，提供无标签数据表示学习的最新理论进展概述。&lt;h4&gt;主要发现&lt;/h4&gt;论文中未明确提及具体的研究发现，主要关注理论进展的概述。&lt;h4&gt;结论&lt;/h4&gt;论文中未明确提及结论，主要提供理论进展的概述并提及作者在此方向上的贡献。&lt;h4&gt;翻译&lt;/h4&gt;从无标签数据中进行的表示学习在统计学、数据科学和信号处理领域已有广泛研究，有丰富的文献涉及降维、压缩、多维尺度变换等技术。然而，当前的深度学习模型使用新的无监督表示学习原则，这些原则难以用经典理论进行分析。例如，视觉基础模型通过自监督或去噪/掩码自编码器取得了巨大成功，能够从大量无标签数据中有效学习表示。然而，仍然难以描述这些模型学习到的表征，并解释它们为什么在多样化的预测任务中表现良好或表现出涌现行为。要回答这些问题，需要结合统计学和优化领域的数学工具。本文提供了关于无标签数据表示学习的最新理论进展概述，并提到了我们在此方向上的贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning from unlabeled data has been extensively studied instatistics, data science and signal processing with a rich literature ontechniques for dimension reduction, compression, multi-dimensional scalingamong others. However, current deep learning models use new principles forunsupervised representation learning that cannot be easily analyzed usingclassical theories. For example, visual foundation models have found tremendoussuccess using self-supervision or denoising/masked autoencoders, whicheffectively learn representations from massive amounts of unlabeled data.However, it remains difficult to characterize the representations learned bythese models and to explain why they perform well for diverse prediction tasksor show emergent behavior. To answer these questions, one needs to combinemathematical tools from statistics and optimization. This paper provides anoverview of recent theoretical advances in representation learning fromunlabeled data and mentions our contributions in this direction.</description>
      <author>example@mail.com (Pascal Esser, Maximilian Fleissner, Debarghya Ghoshdastidar)</author>
      <guid isPermaLink="false">2509.18997v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset</title>
      <link>http://arxiv.org/abs/2509.18919v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为异常引导的自监督预训练（AGSSP）的新型范式，用于解决金属表面缺陷检测中的数据稀缺问题。该方法通过异常先验知识引导表示学习，采用两阶段框架，包括从异常图中提取知识预训练模型骨干网络，以及使用这些图派生的伪缺陷框预训练检测器。研究团队还开发了增强方法生成高质量异常图，并收集了包含120,000张图像的大型工业数据集。实验表明，AGSSP在各种设置下都能提升性能，相比基于ImageNet的模型，mAP@0.5提升高达10%，mAP@0.5:0.95提升11.4%。&lt;h4&gt;背景&lt;/h4&gt;在金属表面缺陷检测中，预训练-微调范式是缓解数据稀缺挑战的关键策略。然而，其实施面临一个关键困境：在自然图像数据集（如ImageNet）上进行预训练存在显著的领域差距；而在领域内工业数据上进行简单的自监督预训练往往效果不佳，因为现有的学习目标无法区分微妙的缺陷模式与复杂的背景噪声和纹理。&lt;h4&gt;目的&lt;/h4&gt;解决金属表面缺陷检测中预训练-微调范式面临的领域差距和学习目标不足的问题，提出一种能够有效区分缺陷模式与背景噪声的新型预训练方法。&lt;h4&gt;方法&lt;/h4&gt;提出Anomaly-Guided Self-Supervised Pretraining (AGSSP)范式，采用两阶段框架：（1）通过从异常图中蒸馏知识预训练模型骨干网络，鼓励网络捕获缺陷显著特征；（2）使用这些图派生的伪缺陷框预训练检测器，使其与定位任务对齐。为此，研究团队开发了知识增强方法生成高质量异常图，并收集了包含120,000张图像的大型工业数据集，同时提供了两个小规模、像素级标注的金属表面缺陷数据集用于验证。&lt;h4&gt;主要发现&lt;/h4&gt;AGSSP在各种设置下能够一致地提升性能，相比基于ImageNet的模型，在mAP@0.5指标上提升高达10%，在mAP@0.5:0.95指标上提升11.4%。&lt;h4&gt;结论&lt;/h4&gt;AGSSP通过异常先验知识引导表示学习，有效解决了金属表面缺陷检测中预训练面临的领域差距和学习目标不足的问题，显著提升了检测性能。所有代码、预训练模型和数据集均已公开。&lt;h4&gt;翻译&lt;/h4&gt;预训练-微调范式是缓解金属表面缺陷检测中数据稀缺挑战的关键策略。然而，其实施面临一个关键困境。在自然图像数据集（如ImageNet）上进行预训练存在显著的领域差距。同时，在领域内工业数据上进行简单的自监督预训练往往效果不佳，因为现有的学习目标无法区分微妙的缺陷模式与复杂的背景噪声和纹理。为解决此问题，我们引入了异常引导的自监督预训练（AGSSP），一种通过异常先验明确引导表示学习的新型范式。AGSSP采用两阶段框架：（1）首先通过从异常图中蒸馏知识预训练模型的骨干网络，鼓励网络捕获缺陷显著特征；（2）然后使用这些图派生的伪缺陷框预训练检测器，使其与定位任务对齐。为实现这一点，我们开发了增强方法生成高质量异常图，并收集了一个包含120,000张图像的大型工业数据集。此外，我们提供了两个小规模、像素级标注的金属表面缺陷数据集用于验证。大量实验表明，AGSSP在各种设置下都能一致提升性能，相比基于ImageNet的模型，mAP@0.5提升高达10%，mAP@0.5:0.95提升11.4%。所有代码、预训练模型和数据集均可从https://clovermini.github.io/AGSSP-Dev/公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The pretraining-finetuning paradigm is a crucial strategy in metallic surfacedefect detection for mitigating the challenges posed by data scarcity. However,its implementation presents a critical dilemma. Pretraining on natural imagedatasets such as ImageNet, faces a significant domain gap. Meanwhile, naiveself-supervised pretraining on in-domain industrial data is often ineffectivedue to the inability of existing learning objectives to distinguish subtledefect patterns from complex background noise and textures. To resolve this, weintroduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigmthat explicitly guides representation learning through anomaly priors. AGSSPemploys a two-stage framework: (1) it first pretrains the model's backbone bydistilling knowledge from anomaly maps, encouraging the network to capturedefect-salient features; (2) it then pretrains the detector using pseudo-defectboxes derived from these maps, aligning it with localization tasks. To enablethis, we develop a knowledge-enhanced method to generate high-quality anomalymaps and collect a large-scale industrial dataset of 120,000 images.Additionally, we present two small-scale, pixel-level labeled metallic surfacedefect datasets for validation. Extensive experiments demonstrate that AGSSPconsistently enhances performance across various settings, achieving up to a10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared toImageNet-based models. All code, pretrained models, and datasets are publiclyavailable at https://clovermini.github.io/AGSSP-Dev/.</description>
      <author>example@mail.com (Chuni Liu, Hongjie Li, Jiaqi Du, Yangyang Hou, Qian Sun, Lei Jin, Ke Xu)</author>
      <guid isPermaLink="false">2509.18919v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision</title>
      <link>http://arxiv.org/abs/2509.18765v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为DiSSECT的自监督学习方法，通过多尺度向量量化在自监督学习流程中引入离散表示瓶颈，从而学习可重复、结构感知的特征，提高跨任务和领域的表示迁移能力。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为医学图像表示学习的强大范式，特别是在标记数据有限的情况下。然而，现有方法通常依赖复杂架构、特定解剖先验或大量调整的数据增强，限制了可扩展性和泛化能力。更重要的是，这些模型容易陷入捷径学习，特别是在胸部X光等模态中，解剖相似性高且病理变化微妙。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，整合多尺度向量量化到自监督学习流程中，施加离散表示瓶颈，约束模型学习可重复、结构感知的特征，同时抑制视图特定或低效用模式，提高跨任务和领域的表示迁移能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了DiSSECT（Discrete Self-Supervision for Efficient Clinical Transferable Representations），这是一种将多尺度向量量化整合到自监督学习流程中的框架，以施加离散表示瓶颈。&lt;h4&gt;主要发现&lt;/h4&gt;DiSSECT在分类和分割任务上都取得了强大性能，需要最少的微调或无需微调，并且在低标签情况下显示出特别高的标签效率。作者在多个公共医学图像数据集上验证了DiSSECT，与现有最先进的方法相比，展示了其鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;DiSSECT通过引入离散表示瓶颈，有效解决了现有自监督学习方法在医学图像表示学习中的局限性，特别是在标记数据有限的情况下，能够学习更可迁移、更鲁棒的特征表示。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为医学图像表示学习的强大范式，特别是在标记数据有限的情况下。然而，现有的自监督学习方法通常依赖复杂架构、特定解剖的先验或大量调整的数据增强，这限制了它们可扩展性和泛化能力。更重要的是，这些模型容易陷入捷径学习，特别是在胸部X光等模态中，解剖相似性高且病理变化微妙。在这项工作中，我们介绍了DiSSECT——用于高效临床可迁移表示的离散自监督，这是一种将多尺度向量量化整合到自监督学习流程中的框架，以施加离散表示瓶颈。这约束模型学习可重复、结构感知的特征，同时抑制视图特定或低效用模式，提高跨任务和领域的表示迁移能力。DiSSECT在分类和分割任务上都取得了强大性能，需要最少的微调或无需微调，并且在低标签情况下显示出特别高的标签效率。我们在多个公共医学图像数据集上验证了DiSSECT，与现有最先进的方法相比，展示了其鲁棒性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful paradigm for medicalimage representation learning, particularly in settings with limited labeleddata. However, existing SSL methods often rely on complex architectures,anatomy-specific priors, or heavily tuned augmentations, which limit theirscalability and generalizability. More critically, these models are prone toshortcut learning, especially in modalities like chest X-rays, where anatomicalsimilarity is high and pathology is subtle. In this work, we introduce DiSSECT-- Discrete Self-Supervision for Efficient Clinical TransferableRepresentations, a framework that integrates multi-scale vector quantizationinto the SSL pipeline to impose a discrete representational bottleneck. Thisconstrains the model to learn repeatable, structure-aware features whilesuppressing view-specific or low-utility patterns, improving representationtransfer across tasks and domains. DiSSECT achieves strong performance on bothclassification and segmentation tasks, requiring minimal or no fine-tuning, andshows particularly high label efficiency in low-label regimes. We validateDiSSECT across multiple public medical imaging datasets, demonstrating itsrobustness and generalizability compared to existing state-of-the-artapproaches.</description>
      <author>example@mail.com (Azad Singh, Deepak Mishra)</author>
      <guid isPermaLink="false">2509.18765v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing</title>
      <link>http://arxiv.org/abs/2509.18743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39th Conference on Neural Information Processing Systems (NeurIPS  2025) Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TriFusion-AE的多模态交叉注意力自编码器，通过整合文本先验、单目深度图和LiDAR点云，显著提高了点云处理在强对抗攻击和重度噪声情况下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;LiDAR感知是自动驾驶和机器人的核心技术，但原始点云数据容易受到噪声、遮挡和对抗性破坏的影响。现有自编码器在具有挑战性的现实条件下性能会下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种更鲁棒的LiDAR点云处理方法，提高在噪声、遮挡和对抗性攻击情况下的性能，特别是在强对抗攻击和重度噪声环境下的表现。&lt;h4&gt;方法&lt;/h4&gt;提出TriFusion-AE，一种多模态交叉注意力自编码器，整合文本语义线索、图像几何特征和LiDAR空间结构，通过多模态融合学习鲁棒表示。该框架设计为模型无关的，可与任何基于CNN的点云自编码器无缝集成。&lt;h4&gt;主要发现&lt;/h4&gt;在温和扰动下，模型提升有限；但在强对抗攻击和重度噪声下，实现了显著更鲁棒的重建，而基于CNN的自编码器在这些情况下会失效。&lt;h4&gt;结论&lt;/h4&gt;多模态融合框架在现实低数据部署场景中有效，所提出的方法显著提高了LiDAR点云处理在恶劣条件下的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的感知是自动驾驶和机器人的核心技术，然而原始点云仍然极易受到噪声、遮挡和对抗性破坏的影响。自编码器为去噪和重建提供了自然框架，但在具有挑战性的现实条件下其性能会下降。在这项工作中，我们提出了TriFusion-AE，一种多模态交叉注意力自编码器，整合文本先验、来自多视图图像的单目深度图和LiDAR点云，以提高鲁棒性。通过对齐文本的语义线索、图像的几何(深度)特征和LiDAR的空间结构，TriFusion-AE学习了能够抵抗随机噪声和对抗性扰动的表示。有趣的是，虽然在温和扰动下提升有限，但我们的模型在强对抗攻击和重度噪声下实现了显著更鲁棒的重建，而基于CNN的自编码器在这种情况下会失效。我们在nuScenes-mini数据集上评估，以反映现实中的低数据部署场景。我们的多模态融合框架设计为模型无关的，能够与任何基于CNN的点云自编码器无缝集成，用于联合表示学习。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决LiDAR点云数据在自动驾驶和机器人应用中的鲁棒性问题。原始点云数据对噪声、遮挡和对抗性攻击非常脆弱，即使小的扰动也可能导致下游检测和映射任务的严重失败。这个问题在现实中非常重要，因为LiDAR是自动驾驶和机器人的核心感知技术，而鲁棒的点云表示对于安全关键应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到单模态自编码器在挑战性条件下性能会下降，而实际系统通常结合多传感器信息。他们设计了一种融合LiDAR点云、多视图深度图和文本描述的三模态自编码器。借鉴了现有多模态融合方法（如DeepFusion、SparseFusion）、自编码器框架、视觉-语言融合技术（如VLM-E2E）以及Transformer架构中的交叉注意力机制，并使用了基于ViT的CLIP模型进行文本编码。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过融合三种互补模态（LiDAR点云、多视图深度图和文本描述）来增强点云重建的鲁棒性，利用文本的语义信息、图像的几何信息和LiDAR的空间结构相互补充。实现流程包括：1)多模态编码（分别编码LiDAR点云、深度图和文本）；2)交叉注意力融合（使用多头交叉注意力对齐不同模态特征）；3)解码和重建（通过两阶段解码器重建去噪点云）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将LiDAR、多视图深度图和文本三种模态融合到自编码器框架；2)利用文本语义信息引导点云重建；3)特别关注强对抗攻击和重噪声下的鲁棒性；4)在小型数据集上证明有效性。相比之前工作，TriFusion-AE不仅融合了LiDAR和摄像头，还引入了文本作为第三种模态；使用交叉注意力而非简单拼接；专注于自编码器重建任务而非检测；在低数据条件下也能有效工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TriFusion-AE通过融合LiDAR点云、多视图深度图和文本描述，显著提高了点云重建在强对抗攻击和重噪声条件下的鲁棒性，为自动驾驶和机器人应用提供了更可靠的感知解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based perception is central to autonomous driving and robotics, yet rawpoint clouds remain highly vulnerable to noise, occlusion, and adversarialcorruptions. Autoencoders offer a natural framework for denoising andreconstruction, but their performance degrades under challenging real-worldconditions. In this work, we propose TriFusion-AE, a multimodal cross-attentionautoencoder that integrates textual priors, monocular depth maps frommulti-view images, and LiDAR point clouds to improve robustness. By aligningsemantic cues from text, geometric (depth) features from images, and spatialstructure from LiDAR, TriFusion-AE learns representations that are resilient tostochastic noise and adversarial perturbations. Interestingly, while showinglimited gains under mild perturbations, our model achieves significantly morerobust reconstruction under strong adversarial attacks and heavy noise, whereCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset toreflect realistic low-data deployment scenarios. Our multimodal fusionframework is designed to be model-agnostic, enabling seamless integration withany CNN-based point cloud autoencoder for joint representation learning.</description>
      <author>example@mail.com (Susmit Neogi)</author>
      <guid isPermaLink="false">2509.18743v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>What Makes You Unique? Attribute Prompt Composition for Object Re-Identification</title>
      <link>http://arxiv.org/abs/2509.18715v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TCSVT2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种属性提示组合（APC）框架，通过文本语义共同增强目标再识别（ReID）的判别性和泛化能力，解决了现有模型在单域或跨域场景中的局限性。&lt;h4&gt;背景&lt;/h4&gt;目标再识别（ReID）旨在跨越非重叠摄像头视图识别个体。现有模型受限于单域或跨域场景，单域模型容易过拟合域特定特征，跨域模型依赖多样化归一化策略可能抑制身份特定判别线索，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有ReID模型的局限性，提出一个能够增强判别性和泛化能力的框架，使模型在保持ReID特定判别能力的同时具备更好的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;提出属性提示组合（APC）框架，包含属性提示生成器（APG）和快速-慢速训练策略（FSTS）。APG由语义属性字典（SAD）和提示组合模块（PCM）组成，SAD提供丰富语义描述，PCM自适应组合相关属性生成判别性特征。FSTS采用快速更新流（FUS）获取ReID特定知识，慢速更新流（SUS）保留预训练VLM的泛化知识，通过相互交互减轻过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在传统和域泛化（DG）ReID数据集上的大量实验表明，该框架超越最先进方法，在判别性和泛化性方面表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;提出的APC框架通过利用文本语义和视觉语言模型的优势，有效解决了ReID模型在判别性和泛化性之间的平衡问题，源代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;目标再识别（ReID）旨在跨越非重叠摄像头视图识别个体。尽管最近取得了显著进展，但大多数现有模型受限于单域或跨域场景，限制了它们的实际应用。单域模型容易过拟合域特定特征，而跨域模型通常依赖多样化归一化策略，可能无意中抑制身份特定的判别线索。为解决这些限制，我们提出了一种属性提示组合（APC）框架，利用文本语义共同增强判别性和泛化能力。具体而言，我们设计了一个由语义属性字典（SAD）和提示组合模块（PCM）组成的属性提示生成器（APG）。SAD是一个过完备属性字典，提供丰富的语义描述，而PCM从SAD中自适应组合相关属性，生成判别性属性感知特征。此外，受视觉语言模型（VLM）强大泛化能力的启发，我们提出了一种快速-慢速训练策略（FSTS）来平衡ReID特定判别和可泛化表征学习。具体来说，FSTS采用快速更新流（FUS）快速获取ReID特定判别知识，采用慢速更新流（SUS）保留从预训练VLM继承的可泛化知识。通过相互交互，框架有效关注ReID相关特征同时减轻过拟合。在传统和域泛化（DG）ReID数据集上的大量实验证明，我们的框架超越了最先进方法，在判别性和泛化性方面表现出优越性能。源代码可在https://github.com/AWangYQ/APC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object Re-IDentification (ReID) aims to recognize individuals acrossnon-overlapping camera views. While recent advances have achieved remarkableprogress, most existing models are constrained to either single-domain orcross-domain scenarios, limiting their real-world applicability. Single-domainmodels tend to overfit to domain-specific features, whereas cross-domain modelsoften rely on diverse normalization strategies that may inadvertently suppressidentity-specific discriminative cues. To address these limitations, we proposean Attribute Prompt Composition (APC) framework, which exploits textualsemantics to jointly enhance discrimination and generalization. Specifically,we design an Attribute Prompt Generator (APG) consisting of a SemanticAttribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is anover-complete attribute dictionary to provide rich semantic descriptions, whilePCM adaptively composes relevant attributes from SAD to generate discriminativeattribute-aware features. In addition, motivated by the strong generalizationability of Vision-Language Models (VLM), we propose a Fast-Slow TrainingStrategy (FSTS) to balance ReID-specific discrimination and generalizablerepresentation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)to rapidly acquire ReID-specific discriminative knowledge and a Slow UpdateStream (SUS) to retain the generalizable knowledge inherited from thepre-trained VLM. Through a mutual interaction, the framework effectivelyfocuses on ReID-relevant features while mitigating overfitting. Extensiveexperiments on both conventional and Domain Generalized (DG) ReID datasetsdemonstrate that our framework surpasses state-of-the-art methods, exhibitingsuperior performances in terms of both discrimination and generalization. Thesource code is available at https://github.com/AWangYQ/APC.</description>
      <author>example@mail.com (Yingquan Wang, Pingping Zhang, Chong Sun, Dong Wang, Huchuan Lu)</author>
      <guid isPermaLink="false">2509.18715v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications</title>
      <link>http://arxiv.org/abs/2509.18714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by the 39th Conference on Neural Information  Processing Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种广义双模拟度量(GBSM)，用于马尔可夫决策过程(MDP)对之间的状态相似性计算，解决了BSM在多MDP场景应用中的局限性。&lt;h4&gt;背景&lt;/h4&gt;双模拟度量(BSM)是计算MDP中状态相似性的有力工具，已在强化学习中的状态表示学习和策略探索中成功应用，但在多MDP场景如策略转移中的应用仍具挑战。&lt;h4&gt;目的&lt;/h4&gt;建立MDP对之间的广义双模拟度量(GBSM)，并分析其在策略转移、状态聚合和基于采样的估计等任务中的应用。&lt;h4&gt;方法&lt;/h4&gt;正式定义并证明了GBSM的三个基本性质：对称性、MDP间三角不等式和相同状态空间的距离界限，并利用这些性质进行理论分析。&lt;h4&gt;主要发现&lt;/h4&gt;GBSM在策略转移、状态聚合和基于采样的估计任务中获得了比标准BSM更严格的显式界限，提供了估计的闭式样本复杂度，改进了现有渐近结果。&lt;h4&gt;结论&lt;/h4&gt;GBSM在多MDP场景中有效，数值结果验证了理论发现，为多MDP场景下的状态相似性计算提供了新工具。&lt;h4&gt;翻译&lt;/h4&gt;双模拟度量(BSM)是计算马尔可夫决策过程(MDP)中状态相似性的有力工具，揭示了BSM接近的状态具有更相似的最优值函数。虽然BSM已在强化学习(RL)中成功应用于状态表示学习和策略探索等任务，但在多MDP场景如策略转移中的应用仍具挑战。先前工作尝试将BSM推广到MDP对，但缺乏对其数学性质的严格分析限制了理论进展。本文正式建立了MDP对之间的广义双模拟度量(GBSM)，并严格证明了三个基本性质：GBSM对称性、MDP间三角不等式和相同状态空间的距离界限。利用这些性质，我们理论分析了策略转移、状态聚合和基于采样的估计，获得了比标准BSM更严格的显式界限。此外，GBSM提供了估计的闭式样本复杂度，改进了基于BSM的现有渐近结果。数值结果验证了我们的理论发现，并证明了GBSM在多MDP场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The bisimulation metric (BSM) is a powerful tool for computing statesimilarities within a Markov decision process (MDP), revealing that statescloser in BSM have more similar optimal value functions. While BSM has beensuccessfully utilized in reinforcement learning (RL) for tasks like staterepresentation learning and policy exploration, its application to multiple-MDPscenarios, such as policy transfer, remains challenging. Prior work hasattempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysisof its mathematical properties has limited further theoretical progress. Inthis work, we formally establish a generalized bisimulation metric (GBSM)between pairs of MDPs, which is rigorously proven with the three fundamentalproperties: GBSM symmetry, inter-MDP triangle inequality, and the distancebound on identical state spaces. Leveraging these properties, we theoreticallyanalyse policy transfer, state aggregation, and sampling-based estimation inMDPs, obtaining explicit bounds that are strictly tighter than those derivedfrom the standard BSM. Additionally, GBSM provides a closed-form samplecomplexity for estimation, improving upon existing asymptotic results based onBSM. Numerical results validate our theoretical findings and demonstrate theeffectiveness of GBSM in multi-MDP scenarios.</description>
      <author>example@mail.com (Zhenyu Tao, Wei Xu, Xiaohu You)</author>
      <guid isPermaLink="false">2509.18714v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>An overview of neural architectures for self-supervised audio representation learning from masked spectrograms</title>
      <link>http://arxiv.org/abs/2509.18691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提供了掩码频谱建模和神经序列建模架构（Mamba和xLSTM）的全面概述，并在统一的框架下比较了基于Transformer、Mamba和xLSTM的掩码频谱模型在十个下游音频分类任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;自监督学习近年来受到广泛关注，掩码频谱建模是其重要方法之一，通过预测被隐藏的音频频谱部分学习语义表示。基于Transformer的掩码频谱建模已成为音频基础模型的主要方法，但Transformer的二次方缩放问题促使研究者重新关注循环序列建模方法，其中Mamba和xLSTM是最有前途的两种架构。&lt;h4&gt;目的&lt;/h4&gt;提供掩码频谱建模和Mamba、xLSTM神经序列建模架构的全面综述，并在统一框架下比较这些架构在音频任务上的表现，帮助读者做出方法选择决策。&lt;h4&gt;方法&lt;/h4&gt;对掩码频谱建模和神经序列建模架构进行文献综述，并在十个多样化的下游音频分类任务上使用统一的、可复现的框架比较Transformer、Mamba和xLSTM基于掩码频谱的模型。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体的实验结果和发现，但强调了比较研究对于评估不同架构适用性的重要性。&lt;h4&gt;结论&lt;/h4&gt;通过比较研究，将帮助感兴趣的读者对评估方法在相邻应用中的适用性做出明智的决策。&lt;h4&gt;翻译&lt;/h4&gt;近年来，自监督学习因其能够在没有标记数据的情况下训练深度神经表示而受到广泛关注。掩码频谱建模就是一种自监督学习方法，其目标是通过预测输入音频频谱中被移除或隐藏的部分来学习语义丰富的上下文表示。以Transformer神经架构为核心，掩码频谱建模已成为学习通用音频表示（即音频基础模型）的主要方法。与此同时，解决Transformer架构的问题，特别是其基础的缩放点积注意力操作（该操作随输入序列长度二次方缩放），重新激发了人们对循环序列建模方法的兴趣。其中，选择性结构化状态空间模型（如Mamba）和扩展长短期记忆（xLSTM）是两种最有前途且被广泛采用的方法。尽管这两个主题的研究工作不断增加，但目前仍缺乏一个涵盖这些主题交集的充分概述。在本文中，我们提供了上述研究领域的全面概述，涵盖了掩码频谱建模以及前面提到的神经序列建模架构Mamba和xLSTM。此外，我们在十个多样化的下游音频分类任务上，使用统一的、可复现的框架比较了基于Transformer、Mamba和xLSTM的掩码频谱模型，这将帮助感兴趣的读者对评估方法在相邻应用中的适用性做出明智的决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, self-supervised learning has amassed significant interestfor training deep neural representations without labeled data. One suchself-supervised learning approach is masked spectrogram modeling, where theobjective is to learn semantically rich contextual representations bypredicting removed or hidden portions of the input audio spectrogram. With theTransformer neural architecture at its core, masked spectrogram modeling hasemerged as the prominent approach for learning general purpose audiorepresentations, a.k.a. audio foundation models. Meanwhile, addressing theissues of the Transformer architecture, in particular the underlying ScaledDot-product Attention operation, which scales quadratically with input sequencelength, has led to renewed interest in recurrent sequence modeling approaches.Among them, Selective structured state space models (such as Mamba) andextended Long Short-Term Memory (xLSTM) are the two most promising approacheswhich have experienced widespread adoption. While the body of work on these twotopics continues to grow, there is currently a lack of an adequate overviewencompassing the intersection of these topics. In this paper, we present acomprehensive overview of the aforementioned research domains, covering maskedspectrogram modeling and the previously mentioned neural sequence modelingarchitectures, Mamba and xLSTM. Further, we compare Transformers, Mamba andxLSTM based masked spectrogram models in a unified, reproducible framework onten diverse downstream audio classification tasks, which will help interestedreaders to make informed decisions regarding suitability of the evaluatedapproaches to adjacent applications.</description>
      <author>example@mail.com (Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan)</author>
      <guid isPermaLink="false">2509.18691v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>MolPILE - large-scale, diverse dataset for molecular representation learning</title>
      <link>http://arxiv.org/abs/2509.18353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了MolPILE，一个包含2.22亿个化合物的大规模、多样化且经过严格筛选的分子数据集，旨在解决化学信息学中分子表示学习的局限性问题，提升基础模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有小分子数据集在大小、多样性和质量上的限制严重影响了基础模型的泛化能力，尽管化学信息学中预训练数据集的重要性日益增加，但分子表示学习的有效性仍然受到这些限制的阻碍。&lt;h4&gt;目的&lt;/h4&gt;构建一个大规模、多样化且经过严格筛选的分子数据集，为分子化学领域提供一个类似ImageNet的标准化资源，解决模型训练中数据集不足的问题。&lt;h4&gt;方法&lt;/h4&gt;从6个大型数据库收集数据，通过自动化管道构建并筛选出2.22亿个化合物，形成MolPILE数据集，并在该数据集上重新训练现有模型以评估其效果。&lt;h4&gt;主要发现&lt;/h4&gt;现有预训练数据集在训练机器学习模型方面存在显著不足；在MolPILE上重新训练现有模型可以提高泛化性能。&lt;h4&gt;结论&lt;/h4&gt;MolPILE为模型训练提供了标准化资源，满足了分子化学领域对类似ImageNet数据集的迫切需求，有助于提升分子表示学习的效果。&lt;h4&gt;翻译&lt;/h4&gt;预训练数据集的大小、多样性和质量决定了基础模型的泛化能力。尽管预训练数据集在化学信息学中的重要性日益增加，但现有小分子数据集的局限性一直阻碍着分子表示学习的有效性。为解决这一差距，我们提出了MolPILE，这是一个大规模、多样化且经过严格筛选的2.22亿个化合物集合，通过自动化管道从6个大型数据库构建而成。我们对现有预训练数据集进行了全面分析，指出了它们在训练机器学习模型方面的显著不足，并展示了在MolPILE上重新训练现有模型如何提高泛化性能。这项工作为模型训练提供了标准化资源，解决了分子化学领域对类似ImageNet数据集的迫切需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The size, diversity, and quality of pretraining datasets critically determinethe generalization ability of foundation models. Despite their growingimportance in chemoinformatics, the effectiveness of molecular representationlearning has been hindered by limitations in existing small molecule datasets.To address this gap, we present MolPILE, large-scale, diverse, and rigorouslycurated collection of 222 million compounds, constructed from 6 large-scaledatabases using an automated curation pipeline. We present a comprehensiveanalysis of current pretraining datasets, highlighting considerableshortcomings for training ML models, and demonstrate how retraining existingmodels on MolPILE yields improvements in generalization performance. This workprovides a standardized resource for model training, addressing the pressingneed for an ImageNet-like dataset in molecular chemistry.</description>
      <author>example@mail.com (Jakub Adamczyk, Jakub Poziemski, Franciszek Job, Mateusz Król, Maciej Makowski)</author>
      <guid isPermaLink="false">2509.18353v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Analysing the prevalence of tidal features in HSC-SSP using self-supervised representation learning</title>
      <link>http://arxiv.org/abs/2509.18274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages. Accepted for publication in MNRAS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究结合自监督机器学习和视觉分类方法，分析了34,331个星系样本中的潮汐特征，发现了1646个具有确认潮汐特征的星系，并研究了潮汐特征与宿主星系特性的关系。&lt;h4&gt;背景&lt;/h4&gt;研究基于Hyper Suprime-Cam Subaru战略计划(HSC-SSP)的光学成像调查，样本包括恒星质量≥9.5且红移≤0.4的34,331个星系。&lt;h4&gt;目的&lt;/h4&gt;识别星系中的潮汐特征，分析潮汐特征及其不同类别的发生率如何随宿主星系恒星质量、光度红移、颜色和晕质量变化，验证机器学习方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;结合自监督机器学习和视觉分类方法识别潮汐特征，构建了包含1646个确认具有潮汐特征的星系的最大样本，并分析了潮汐特征分数与各种参数的关系。&lt;h4&gt;主要发现&lt;/h4&gt;1. 潮汐特征分数与宿主星系恒星质量呈正相关；2. 潮汐特征分数与红移呈负相关；3. 群环境(12.0&lt;log(M200/M⊙)&lt;14.0)中比场环境或团簇环境中出现更多潮汐特征；4. 最高质量团簇和群的中心星系比卫星星系表现出更高的潮汐特征发生率；5. 观察到的趋势与纯视觉或其他自动化方法获得的结果一致。&lt;h4&gt;结论&lt;/h4&gt;机器学习方法可以显著减少视觉分类的工作量，仅需对不到30%的样本进行视觉分类。这种方法将在即将到来的大型成像调查中数百万个星系的分类中发挥重要作用。&lt;h4&gt;翻译&lt;/h4&gt;使用自监督机器学习和视觉分类相结合的方法，从Hyper Suprime-Cam Subaru战略计划光学成像调查中选取恒星质量≥9.5且红移≤0.4的34,331个星系样本，识别其中的潮汐特征。我们组装了1646个确认具有潮汐特征的星系的最大样本，发现潮汐特征分数f=0.06+0.05-0.01。我们分析了潮汐特征及其不同类别的发生率如何随宿主星系恒星质量、光度红移、颜色以及晕质量变化。我们发现潮汐特征分数与宿主星系恒星质量呈正相关，与红移呈负相关。我们发现群环境(12.0&lt;log(M200/M⊙)&lt;14.0)中比场环境或更密集的团簇环境中出现更多潮汐特征。我们还发现，最质量的群和团簇的中心星系比卫星星系表现出更高的潮汐特征发生率。我们发现观察到的趋势与纯视觉或其他自动化方法获得的结果一致，证实了我们的方法论的有效性，并且使用机器学习可以显著减少视觉分类的工作量，只需要对我们样本中不到30%的星系进行视觉分类。这种方法将在即将到来的大型成像调查中数百万个合适星系的分类中发挥重要作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We use a combination of self-supervised machine learning and visualclassification to identify tidal features in a sample of 34,331 galaxies withstellar masses $\log_{10}(M_{*}/\rm{M}_{\odot})\geq9.5$ and redshift$z\leq0.4$, drawn from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP)optical imaging survey. We assemble the largest sample of 1646 galaxies withconfirmed tidal features, finding a tidal feature fraction$f=0.06^{+0.05}_{-0.01}$. We analyse how the incidences of tidal features andthe various classes of tidal features vary with host galaxy stellar mass,photometric redshift, and colour, as well as halo mass. We find an increasingrelationship between tidal feature fraction and host galaxy stellar mass, and adecreasing relationship with redshift. We find more tidal features occurring ingroup environments with $12.0&lt;\log_{10}(M_{200}/\rm{M}_{\odot})&lt;14.0$ than inthe field or in denser, cluster environments. We also find that the centralgalaxies of the most massive ($\log_{10}(M_{200}/\rm{M}_{\odot})&gt;14.1$) groupsand clusters exhibit higher rates of tidal features than the satellitegalaxies. We find good agreement between the trends we observe and the resultsobtained from purely visual or other automated methods, confirming the validityof our methodology and that using machine learning can drastically reduce theworkload of visual classifiers, having needed to visually classify less than 30per cent of our sample. Such methods will be instrumental in classifying themillions of suitable galaxies to be observed by large upcoming imaging surveyssuch as the Vera C. Rubin Observatory's Legacy Survey of Space and Time.</description>
      <author>example@mail.com (A. Desmons, S. Brough, F. Lanusse, L. Canepa, A. Khalid)</author>
      <guid isPermaLink="false">2509.18274v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>MesoNet: A Fundamental Principle for Multi-Representation Learning in Complex Chemical Systems</title>
      <link>http://arxiv.org/abs/2509.17810v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了MesoNet，一种基于多表示学习的新型框架，用于准确预测复杂化学系统中的分子性质。该框架通过上下文感知表示和跨注意力机制有效捕捉了分子内和分子间的相互作用，在多个数据集上展示了优越的准确性和化学可解释性。&lt;h4&gt;背景&lt;/h4&gt;准确预测复杂化学系统中的分子性质对加速材料发现和化学创新至关重要。然而，当前计算方法往往难以捕捉从分子内键到分子间力的复杂组成相互作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更好地捕捉复杂化学系统中组成相互作用的新方法，提高分子性质预测的准确性和化学可解释性。&lt;h4&gt;方法&lt;/h4&gt;引入MesoNet，一种基于多表示学习原则的新型框架，专为多分子建模设计。其核心创新是构建上下文感知表示，通过神经策略动态丰富原子描述符，并使用跨注意力机制捕获固有原子性质及其动态组成上下文，使混合系统的影响逐步应用于每个分子和原子。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共数据集（包括纯组分和混合物）上的全面评估表明，MesoNet在分子性质预测方面实现了更高的准确性和增强的化学可解释性。&lt;h4&gt;结论&lt;/h4&gt;该工作建立了一种强大的、可解释的组成复杂性建模方法，旨在推进化学模拟和设计。&lt;h4&gt;翻译&lt;/h4&gt;准确预测复杂化学系统中的分子性质对于加速材料发现和化学创新至关重要。然而，当前计算方法往往难以捕捉复杂化学系统中从分子内键到分子间力的复杂组成相互作用。在这项工作中，我们引入了MesoNet，一种基于多表示学习原则的新型框架，专为多分子建模设计。MesoNet的核心创新在于构建上下文感知表示——通过神经策略动态生成的丰富原子描述符。这些参数通过跨越分子内和分子间信息传递的跨注意力机制，有效地捕获了固有原子性质及其动态组成上下文。受此机制驱动，混合系统的影响逐步应用于每个分子和原子，使信息传递既高效又有意义。在涵盖纯组分和混合物的多个公共数据集上的全面评估表明，MesoNet在分子性质预测方面实现了更高的准确性和增强的化学可解释性。这项工作为建模组成复杂性建立了一种强大、可解释的方法，旨在推进化学模拟和设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of molecular properties in complex chemical systems iscrucial for accelerating material discovery and chemical innovation. However,current computational methods often struggle to capture the intricatecompositional interplay across complex chemical systems, from intramolecularbonds to intermolecular forces. In this work, we introduce MesoNet, a novelframework founded on the principle of multi-representation learning andspecifically designed for multi-molecule modeling. The core innovation ofMesoNet lies in the construction of context-aware representation-dynamicallyenriched atomic descriptors generated via Neural Circuit Policies. Theseparameters efficiently capture both intrinsic atomic properties and theirdynamic compositional context through a cross-attention mechanism spanning bothintramolecular and intermolecular message passing. Driven by this mechanism,the influence of the mixed system is progressively applied to each molecule andatom, making message passing both efficient and meaningful. Comprehensiveevaluations across diverse public datasets, spanning both pure components andmixtures, demonstrate that MesoNet achieves superior accuracy and enhancedchemical interpretability for molecular properties. This work establishes apowerful, interpretable approach for modeling compositional complexity, aimingto advance chemical simulation and design.</description>
      <author>example@mail.com (Jinming Fan, Chao Qian, Shaodong Zhou)</author>
      <guid isPermaLink="false">2509.17810v2</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>A Generative Framework for Personalized Sticker Retrieval</title>
      <link>http://arxiv.org/abs/2509.17749v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Findings of EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PEARL，一个用于个性化贴纸检索的新型生成框架，解决了现有方法缺乏个性化的问题，显著提高了贴纸检索性能。&lt;h4&gt;背景&lt;/h4&gt;信息检索作为生成建模的一种变体，特别是使用自回归模型为给定查询生成相关标识符，最近引起了相当大的关注。然而，将其应用于个性化贴纸检索在很大程度上仍未被探索，并带来了独特的挑战：现有的基于相关性的生成检索方法通常缺乏个性化，导致多样化的用户期望与检索结果之间的不匹配。&lt;h4&gt;目的&lt;/h4&gt;解决现有相关性生成检索方法缺乏个性化的问题，使贴纸检索结果能够更好地匹配不同用户的期望。&lt;h4&gt;方法&lt;/h4&gt;作者提出了PEARL框架，包含两个关键贡献：(i) 设计了一个表示学习模型来编码用户特定的贴纸偏好，该模型在利用个人信息和点击历史的三个预测任务上进行训练，以学习判别性用户表示；(ii) 提出了一种新颖的意图感知学习目标，用于生成与用户查询意图一致的贴纸，优先考虑与更高排名意图相关的贴纸。&lt;h4&gt;主要发现&lt;/h4&gt;离线评估和在线测试的经验结果表明，PEARL显著优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;PEARL框架通过个性化用户表示和意图感知学习目标，有效解决了贴纸检索中的个性化问题，显著提高了检索性能。&lt;h4&gt;翻译&lt;/h4&gt;将信息检索制定为生成建模的一种变体，特别是使用自回归模型为给定查询生成相关标识符，最近引起了相当大的关注。然而，其在个性化贴纸检索中的应用在很大程度上仍未被探索，并带来了独特的挑战：现有的基于相关性的生成检索方法通常缺乏个性化，导致多样化的用户期望与检索结果之间的不匹配。为了解决这一差距，我们提出了PEARL，一个用于个性化贴纸检索的新型生成框架，并做出了两个关键贡献：(i) 为了编码用户特定的贴纸偏好，我们设计了一个表示学习模型来学习判别性用户表示。该模型在利用个人信息和点击历史的三个预测任务上进行训练；以及(ii) 为了生成与用户查询意图一致的贴纸，我们提出了一种新颖的意图感知学习目标，优先考虑与更高排名意图相关的贴纸。离线评估和在线测试的经验结果表明，PEARL显著优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Formulating information retrieval as a variant of generative modeling,specifically using autoregressive models to generate relevant identifiers for agiven query, has recently attracted considerable attention. However, itsapplication to personalized sticker retrieval remains largely unexplored andpresents unique challenges: existing relevance-based generative retrievalmethods typically lack personalization, leading to a mismatch between diverseuser expectations and the retrieved results. To address this gap, we proposePEARL, a novel generative framework for personalized sticker retrieval, andmake two key contributions: (i) To encode user-specific sticker preferences, wedesign a representation learning model to learn discriminative userrepresentations. It is trained on three prediction tasks that leverage personalinformation and click history; and (ii) To generate stickers aligned with auser's query intent, we propose a novel intent-aware learning objective thatprioritizes stickers associated with higher-ranked intents. Empirical resultsfrom both offline evaluations and online tests demonstrate that PEARLsignificantly outperforms state-of-the-art methods.</description>
      <author>example@mail.com (Changjiang Zhou, Ruqing Zhang, Jiafeng Guo, Yu-An Liu, Fan Zhang, Ganyuan Luo, Xueqi Cheng)</author>
      <guid isPermaLink="false">2509.17749v2</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction</title>
      <link>http://arxiv.org/abs/2509.19297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://lhmd.top/volsplat, Code:  https://github.com/ziplab/VolSplat&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出VolSplat方法，用体素对齐高斯替代像素对齐高斯，直接从3D体素网格预测高斯，克服了现有方法中的局限性，实现了更鲁棒的多视图一致性和自适应的高斯密度控制，提高了新视图渲染质量。&lt;h4&gt;背景&lt;/h4&gt;前馈3D高斯散射(3DGS)已成为新视图合成的有效解决方案，但现有方法主要采用像素对齐的高斯预测范式，每个2D像素映射到一个3D高斯，这种方法存在固有局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有像素对齐高斯预测范式的局限性，提出新方法克服像素对齐对易错2D特征匹配的依赖，确保鲁棒的多视图一致性，并能够根据3D场景复杂度自适应控制高斯密度。&lt;h4&gt;方法&lt;/h4&gt;提出VolSplat，一种新的多视图前馈范式，用体素对齐的高斯替代像素对齐的高斯，通过直接从预测的3D体素网格预测高斯，实现自适应的高斯密度控制。&lt;h4&gt;主要发现&lt;/h4&gt;在RealEstate10K和ScanNet等基准测试上，VolSplat实现了最先进性能，产生更合理且视图一致的高斯重建，为前馈3D重建建立了更具可扩展性的框架。&lt;h4&gt;结论&lt;/h4&gt;VolSplat不仅取得优越结果，还为前馈3D重建建立了更密集、更鲁棒的表示框架，为更广泛领域的研究铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;前馈3D高斯散射已成为新视图合成的有效解决方案。现有方法主要依赖像素对齐的高斯预测范式，每个2D像素映射到一个3D高斯。我们重新思考这一广泛采用的公式，发现其固有局限性：重建的3D模型高度依赖输入视图数量，导致视图偏向的密度分布，并在源视图包含遮挡或低纹理时引入对齐误差。为应对这些挑战，我们引入VolSplat，一种用体素对齐高斯替代像素对齐的新多视图前馈范式。通过直接从预测的3D体素网格预测高斯，它克服了像素对齐对易错2D特征匹配的依赖，确保了鲁棒的多视图一致性。此外，它能根据3D场景复杂度自适应控制高斯密度，产生更忠实的高斯点云、改善的几何一致性和增强的新视图渲染质量。在RealEstate10K和ScanNet等基准测试上的实验表明，VolSplat实现了最先进性能，同时产生更合理且视图一致的高斯重建。除优越结果外，我们的方法为前馈3D重建建立了更具可扩展性的框架，具有更密集和更鲁棒的表示，为更广泛领域的研究铺平了道路。视频结果、代码和训练模型可在项目页面获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有前馈3D高斯散射方法中的像素对齐范式问题，包括对输入视图数量的依赖、视图偏差的密度分布和对齐误差。这些问题在3D重建领域很重要，因为3D重建是现代机器人的基础，使系统能够感知、映射和理解物理环境，而前馈方法因其速度优势在实时应用和大规模场景重建中具有重要价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过重新思考广泛采用的像素对齐公式，识别出其固有限制，然后转向3D体素思路。他们借鉴了3D体素化技术、3D卷积神经网络、稀疏体素结构(如八叉树)以及3D U-Net架构等现有工作，并将这些技术组合改进，创建了基于体素对齐的前馈3D重建框架，解决了像素对齐方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D高斯散射的预测从基于2D像素对齐转变为基于3D体素对齐，通过直接从预测的3D体素网格预测高斯，克服像素对齐对2D特征匹配的依赖，实现自适应密度控制。整体流程包括：1)2D特征提取；2)构建成本体积；3)深度预测；4)3D特征构建；5)3D特征细化；6)高斯预测；7)渲染新视图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：体素对齐范式、3D特征网格、自适应密度控制、多视图一致性和稀疏体素表示。相比之前的工作，VolSplat不再固定高斯数量为H×W×N，而是能根据场景复杂度自适应分配高斯密度；在处理遮挡和低纹理区域时表现更好；产生更几何一致的3D重建；并能更好地利用稀疏表示提高计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VolSplat通过引入体素对齐替代像素对齐，解决了前馈3D高斯散射中多视图一致性和自适应密度控制的关键问题，实现了更准确、更鲁棒的3D场景重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effectivesolution for novel view synthesis. Existing methods predominantly rely on apixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a3D Gaussian. We rethink this widely adopted formulation and identify severalinherent limitations: it renders the reconstructed 3D models heavily dependenton the number of input views, leads to view-biased density distributions, andintroduces alignment errors, particularly when source views contain occlusionsor low texture. To address these challenges, we introduce VolSplat, a newmulti-view feed-forward paradigm that replaces pixel alignment withvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3Dvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D featurematching, ensuring robust multi-view consistency. Furthermore, it enablesadaptive control over Gaussian density based on 3D scene complexity, yieldingmore faithful Gaussian point clouds, improved geometric consistency, andenhanced novel-view rendering quality. Experiments on widely used benchmarksincluding RealEstate10K and ScanNet demonstrate that VolSplat achievesstate-of-the-art performance while producing more plausible and view-consistentGaussian reconstructions. In addition to superior results, our approachestablishes a more scalable framework for feed-forward 3D reconstruction withdenser and more robust representations, paving the way for further research inwider communities. The video results, code and trained models are available onour project page: https://lhmd.top/volsplat.</description>
      <author>example@mail.com (Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang, Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Y. Chen, Bohan Zhuang)</author>
      <guid isPermaLink="false">2509.19297v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>BiGraspFormer: End-to-End Bimanual Grasp Transformer</title>
      <link>http://arxiv.org/abs/2509.19142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BiGraspFormer是一个统一的端到端transformer框架，能够直接从物体点云生成协调的双手抓取，解决了现有方法的协调问题，包括碰撞风险和力分布不均。&lt;h4&gt;背景&lt;/h4&gt;双手抓取对机器人处理大型和复杂物体至关重要，但现有方法要么只关注单臂抓取，要么采用分开的抓取生成和双手评估阶段，导致协调问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一个统一的端到端框架，直接从物体点云生成协调的双手抓取。&lt;h4&gt;方法&lt;/h4&gt;提出BiGraspFormer框架，采用单引导双手（SGB）策略，首先使用transformer解码器生成多样化的单抓取候选，然后通过专门的注意力机制利用这些抓取的已学习特征，联合预测双手姿态和质量分数。&lt;h4&gt;主要发现&lt;/h4&gt;全面的仿真实验和现实世界验证表明，BiGraspFormer始终优于现有方法，同时保持高效的推理速度（小于0.05秒）。&lt;h4&gt;结论&lt;/h4&gt;BiGraspFormer通过SGB策略解决了现有方法的协调问题，在仿真和现实世界实验中表现优异，确认了框架的有效性。&lt;h4&gt;翻译&lt;/h4&gt;双手抓取对机器人处理大型和复杂物体至关重要。然而，现有方法要么只关注单臂抓取，要么采用分开的抓取生成和双手评估阶段，导致包括碰撞风险和力分布不均等协调问题。为解决这些局限性，我们提出了BiGraspFormer，这是一个统一的端到端transformer框架，能够直接从物体点云生成协调的双手抓取。我们的核心思想是单引导双手（SGB）策略，首先使用transformer解码器生成多样化的单抓取候选，然后通过专门的注意力机制利用这些抓取的已学习特征，联合预测双手姿态和质量分数。这种条件策略降低了自由度搜索空间的复杂性，同时确保协调的双手操作。全面的仿真实验和现实世界验证表明，BiGraspFormer始终优于现有方法，同时保持高效的推理速度（小于0.05秒），确认了我们框架的有效性。代码和补充材料可在指定网址获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决双臂抓取的协调性问题。现有方法要么只关注单臂抓取，要么采用分离的抓取生成和双臂评估阶段，导致碰撞风险和力分布不均等问题。这个问题在现实中很重要，因为双臂抓取使机器人能够处理大型和复杂物体，完成单臂无法完成的任务如搬运家具、移动大箱子等，在机器人操作领域具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考的关键洞察是单臂抓取特征可以有效指导双臂抓取生成，而不是将双臂协调视为两个独立问题。他们借鉴了现有工作中的PointNet++用于点云特征提取、DETR风格的集合预测公式、transformer架构以及DA2数据集中的双臂质量度量。通过这些借鉴，作者设计出单引导双臂策略，将复杂的12-DoF搜索空间分解为更易处理的子问题，从而更有效地学习双臂抓取。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'单引导双臂'(SGB)策略，先生成多样化单臂抓取候选，再通过专门注意力机制利用这些特征联合预测双臂姿态和质量分数。整体流程包括四个模块：1)对象编码器提取点云的局部和全局特征；2)单臂抓取提议器生成多样化单臂抓取候选；3)双臂匹配器基于碰撞检查和质量分数选择可行抓取对；4)双臂抓取生成器利用SGB注意力层将选定对细化为最终双臂抓取。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个统一的端到端双臂抓取transformer框架；2)单引导双臂(SGB)策略；3)端到端的双臂抓取生成和评估。相比之前工作，BiGraspFormer的不同之处在于：它消除了分离抓取生成和评估的模块化架构；利用单臂抓取特征而非简单组合独立单臂解决方案；尽管处理更复杂的12-DoF动作空间，但仍保持高效推理速度(&lt;0.05秒)；在模拟和真实世界实验中展现出优越的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BiGraspFormer通过引入单引导双臂策略，实现了首个端到端的双臂抓取transformer框架，直接从物体点云生成协调的双臂抓取，显著提升了抓取成功率和多样性，同时保持了高效的计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bimanual grasping is essential for robots to handle large and complexobjects. However, existing methods either focus solely on single-arm graspingor employ separate grasp generation and bimanual evaluation stages, leading tocoordination problems including collision risks and unbalanced forcedistribution. To address these limitations, we propose BiGraspFormer, a unifiedend-to-end transformer framework that directly generates coordinated bimanualgrasps from object point clouds. Our key idea is the Single-Guided Bimanual(SGB) strategy, which first generates diverse single grasp candidates using atransformer decoder, then leverages their learned features through specializedattention mechanisms to jointly predict bimanual poses and quality scores. Thisconditioning strategy reduces the complexity of the 12-DoF search space whileensuring coordinated bimanual manipulation. Comprehensive simulationexperiments and real-world validation demonstrate that BiGraspFormerconsistently outperforms existing methods while maintaining efficient inferencespeed (&lt;0.05s), confirming the effectiveness of our framework. Code andsupplementary materials are available at https://sites.google.com/bigraspformer</description>
      <author>example@mail.com (Kangmin Kim, Seunghyeok Back, Geonhyup Lee, Sangbeom Lee, Sangjun Noh, Kyoobin Lee)</author>
      <guid isPermaLink="false">2509.19142v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models</title>
      <link>http://arxiv.org/abs/2509.18917v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进的去噪扩散概率模型(DDPM)，通过增强噪声调度和时间步嵌入技术，生成高质量的合成激光雷达数据，用于增强自动驾驶车辆的3D视觉系统性能，解决了真实世界激光雷达数据收集困难、噪声大和稀疏性问题。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆(AVs)有望通过提高效率和安全性彻底改变交通方式，其成功依赖于能够有效感知环境和检测交通参与者的3D视觉系统。激光雷达(LiDAR)提供高分辨率深度数据，实现准确目标检测、安全导航和碰撞避免，但收集真实世界数据耗时且常受噪声和稀疏性影响。&lt;h4&gt;目的&lt;/h4&gt;应用去噪扩散概率模型(DDPM)生成高质量的合成激光雷达数据用于数据增强，提高一系列计算机视觉任务的性能，特别是在自动驾驶感知方面。&lt;h4&gt;方法&lt;/h4&gt;改进的去噪扩散概率模型(DDPM)，增强噪声调度和时间步嵌入技术，使模型能够基于投影产生更真实的点云。在IAMCV和KITTI-360数据集上进行了广泛评估，使用四种性能指标与最先进方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;模型在大多数现有基线方法上表现出优越性能，能有效减轻噪声和稀疏激光雷达数据的影响，生成具有丰富空间关系和结构细节的多样化点云。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能有效生成高质量的合成激光雷达数据，增强自动驾驶车辆的3D视觉系统性能，为自动驾驶感知提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶车辆(AVs)有望通过提高效率和安全性彻底改变交通方式。其成功依赖于能够有效感知环境和检测交通参与者的3D视觉系统。在自动驾驶车辆用于创建周围环境综合视图的传感器中，激光雷达提供高分辨率深度数据，实现准确的目标检测、安全导航和碰撞避免。然而，收集真实世界的激光雷达数据耗时，并且常常因恶劣天气或传感器限制而受到噪声和稀疏性的影响。这项工作应用了一种去噪扩散概率模型(DDPM)，通过增强噪声调度和时间步嵌入技术生成高质量的合成数据用于数据增强，从而提高一系列计算机视觉任务的性能，特别是在自动驾驶感知方面。这些改进影响了去噪过程和模型的时序感知能力，使其能够基于投影产生更真实的点云。提出的方法在IAMCV和KITTI-360数据集上使用各种配置进行了广泛评估，并将四种性能指标与最先进(SOTA)方法进行了比较。结果表明，该模型在大多数现有基线方法上表现出优越性能，并且在减轻噪声和稀疏激光雷达数据的影响方面有效，能够生成具有丰富空间关系和结构细节的多样化点云。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决生成高质量LiDAR点云数据的问题，以增强自动驾驶车辆的3D感知系统。这个问题非常重要，因为真实世界的LiDAR数据收集既耗时又昂贵，而且经常受到噪声和稀疏性的影响，这会影响自动驾驶系统的准确性和可靠性。通过生成合成数据，可以创建更完整、无噪声的训练数据，提高感知模型在复杂环境中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶车辆感知系统的需求和LiDAR数据的局限性，然后评估了现有的LiDAR数据生成方法，包括传统模拟工具和深度学习方法。作者选择了去噪扩散概率模型(DDPM)作为基础，因为它能够比GAN和VAE生成更高质量的点云数据。在此基础上，作者设计了新的噪声调度策略和时间步嵌入技术，以提高生成质量。作者确实借鉴了现有工作，特别是DDPM框架，但通过创新性的改进显著提升了性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用改进的DDPM模型通过鸟瞰图(BEV)和等距矩形投影生成高质量的LiDAR点云数据。整体流程包括：1)将原始3D点云投影为2D表示；2)在前向过程中向原始图像添加噪声并训练模型预测噪声；3)在反向过程中从随机噪声开始逐步去噪生成新样本；4)应用创新的噪声调度策略和时间步嵌入技术提高生成质量；5)使用多个指标评估生成结果并与最先进方法比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两种新的噪声调度策略(时间依赖和斜坡调度)，减少采样步骤同时保持高质量；2)基于傅里叶级数的时间步嵌入技术，提高模型对时间步的理解；3)局部平滑操作处理稀疏区域的噪声；4)轻量级架构设计降低计算需求。相比之前工作，这种方法能生成更高质量、更多样化的点云，具有更丰富的空间关系和结构细节，同时计算效率更高，在IAMCV数据集上实现了最佳性能，在KITTI-360数据集上接近或超过最先进方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创新的噪声调度策略和基于傅里叶级数的时间步嵌入技术，显著提高了去噪扩散概率模型生成高质量、多样化LiDAR点云的能力，为自动驾驶车辆的3D感知系统提供了更有效的数据增强解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicles (AVs) are expected to revolutionize transportation byimproving efficiency and safety. Their success relies on 3D vision systems thateffectively sense the environment and detect traffic agents. Among sensors AVsuse to create a comprehensive view of surroundings, LiDAR provideshigh-resolution depth data enabling accurate object detection, safe navigation,and collision avoidance. However, collecting real-world LiDAR data istime-consuming and often affected by noise and sparsity due to adverse weatheror sensor limitations. This work applies a denoising diffusion probabilisticmodel (DDPM), enhanced with novel noise scheduling and time-step embeddingtechniques to generate high-quality synthetic data for augmentation, therebyimproving performance across a range of computer vision tasks, particularly inAV perception. These modifications impact the denoising process and the model'stemporal awareness, allowing it to produce more realistic point clouds based onthe projection. The proposed method was extensively evaluated under variousconfigurations using the IAMCV and KITTI-360 datasets, with four performancemetrics compared against state-of-the-art (SOTA) methods. The resultsdemonstrate the model's superior performance over most existing baselines andits effectiveness in mitigating the effects of noisy and sparse LiDAR data,producing diverse point clouds with rich spatial relationships and structuraldetail.</description>
      <author>example@mail.com (Amirhesam Aghanouri, Cristina Olaverri-Monreal)</author>
      <guid isPermaLink="false">2509.18917v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring</title>
      <link>http://arxiv.org/abs/2509.18898v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DeblurSplat的新型去模糊3D高斯飞溅方法，该方法基于事件相机，无需运动恢复结构（SfM）技术。&lt;h4&gt;背景&lt;/h4&gt;运动模糊是计算机视觉中的一个常见问题，特别是在动态场景中。传统的3D重建方法通常需要精确的相机姿态估计，而这一过程容易引入误差。&lt;h4&gt;目的&lt;/h4&gt;解决动态场景中的运动去模糊问题，提高3D重建的质量和效率，同时避免传统方法中相机姿态估计带来的累积误差。&lt;h4&gt;方法&lt;/h4&gt;1. 利用密集立体模块（DUSt3R）的预训练能力，直接从模糊图像获取精确的初始点云；2. 不计算相机姿态作为中间步骤，避免误差传递；3. 将事件流引入去模糊流程，利用其对动态变化的高敏感性；4. 通过从事件流和模糊图像中解码潜在清晰图像，为场景重建优化提供细粒度监督信号。&lt;h4&gt;主要发现&lt;/h4&gt;在多种场景的广泛实验表明，DeblurSplat在生成高保真度新视图方面表现出色，并且在渲染效率方面相比去模糊3D-GS的最先进方法有显著提升。&lt;h4&gt;结论&lt;/h4&gt;DeblurSplat是一种有效的去模糊3D重建方法，通过结合事件相机和密集立体模块，能够实现高质量和高效率的3D场景重建，无需传统SfM过程中的相机姿态估计。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种基于事件相机的首个无运动恢复结构（SfM）的去模糊3D高斯飞溅方法，称为DeblurSplat。我们通过两种方式解决运动去模糊问题。首先，我们利用密集立体模块（DUSt3R）的预训练能力，直接从模糊图像获取精确的初始点云。不将相机姿态计算作为中间结果，我们避免了不准确相机姿态向初始点云位置的累积误差传递。其次，我们将事件流引入去模糊流程，利用其对动态变化的高敏感性。通过从事件流和模糊图像中解码潜在清晰图像，我们可以为场景重建优化提供细粒度监督信号。在多种场景的广泛实验表明，DeblurSplat不仅在生成高保真度新视图方面表现出色，而且在渲染效率方面相比去模糊3D-GS的最先进方法也取得了显著提升。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在运动模糊条件下进行3D场景重建和新视角合成的问题。在现实世界中，运动模糊是常见现象，特别是在低光环境或长曝光拍摄时。传统3D重建方法（如NeRF和3D-GS）在模糊条件下性能显著下降，因为模糊图像会破坏特征检测和匹配过程，导致相机姿态估计不准确。此外，现有去模糊方法大多依赖于结构从运动(SfM)技术，但在模糊条件下，SfM的相机姿态估计会产生累积误差，影响最终重建质量。解决这个问题对于提高3D重建技术在真实场景中的实用性具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统SfM方法在模糊条件下性能不佳，因为模糊图像会破坏特征检测和匹配过程。他们发现SfM-free方法（如DUSt3R）可以直接从图像生成点云，避免了中间步骤的误差累积。同时，作者注意到事件相机具有高动态范围和无运动模糊的特点，适合用于去模糊任务。作者借鉴了DUSt3R的密集立体模块来直接获取初始点云，参考了事件相机在去blur任务中的应用，特别是其高动态范围特性，并基于现有的3D高斯溅射框架进行扩展。此外，作者还参考了事件双积分(EDI)方法来解耦模糊图像。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 避免使用传统SfM方法估计相机姿态，而是通过预训练的密集立体模块(DUSt3R)直接获取初始点云；2) 利用事件相机的高动态范围信息指导去模糊过程；3) 结合置信度平衡采样和渐进式对齐策略提高重建质量。整体实现流程：1) 使用DUSt3R处理模糊图像生成初始点云和置信度分数；2) 应用置信度平衡采样策略选择高质量点作为高斯原语；3) 使用事件双积分(EDI)将模糊图像解耦为多个潜在清晰图像；4) 通过渐进式对齐策略利用事件流信息优化场景重建；5) 训练3D高斯溅射模型，结合模糊图像重建损失和事件引导的优化损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 提出首个SfM-free的3D高斯溅射去模糊框架DeblurSplat；2) 提出置信度平衡采样策略，利用DUSt3R的置信估计同时保持空间覆盖；3) 提出渐进式对齐方法，结合事件流信息进行精确重建优化；4) 创建更复杂的场景数据集评估不同运动模糊程度下的性能。与之前工作的不同：1) 不依赖传统SfM方法估计相机姿态，避免了模糊条件下的姿态估计误差；2) 直接利用DUSt3R获取初始点云，而非通过中间相机姿态估计；3) 同时利用事件相机的连续信息，而非简单转换为强度图像；4) 提出渐进式对齐策略，而非简单使用事件信息作为辅助损失；5) 在更复杂和现实的场景中进行了评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DeblurSplat通过结合预训练密集立体模块和事件相机信息，首次实现了无需结构从运动(SfM)的3D高斯溅射去模糊框架，显著提高了在运动模糊条件下的3D场景重建质量和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose the first Structure-from-Motion (SfM)-freedeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.We address the motion-deblurring problem in two ways. First, we leverage thepretrained capability of the dense stereo module (DUSt3R) to directly obtainaccurate initial point clouds from blurred images. Without calculating cameraposes as an intermediate result, we avoid the cumulative errors transfer frominaccurate camera poses to the initial point clouds' positions. Second, weintroduce the event stream into the deblur pipeline for its high sensitivity todynamic change. By decoding the latent sharp images from the event stream andblurred images, we can provide a fine-grained supervision signal for scenereconstruction optimization. Extensive experiments across a range of scenesdemonstrate that DeblurSplat not only excels in generating high-fidelity novelviews but also achieves significant rendering efficiency compared to the SOTAsin deblur 3D-GS.</description>
      <author>example@mail.com (Pengteng Li, Yunfan Lu, Pinhao Song, Weiyu Guo, Huizai Yao, F. Richard Yu, Hui Xiong)</author>
      <guid isPermaLink="false">2509.18898v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Human-Interpretable Uncertainty Explanations for Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2509.18786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为高斯过程概念归因(GP-CA)的新型点云配准方法，能够量化并解释配准过程中的不确定性，通过主动学习发现新的不确定性来源，在多个数据集和真实机器人实验中验证了其优越性能。&lt;h4&gt;背景&lt;/h4&gt;点云配准问题中，传统方法如ICP在面对传感器噪声、姿态估计误差和由遮挡引起的部分重叠等不确定性因素时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够量化点云配准不确定性并解释其来源的方法，提高配准过程的鲁棒性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出高斯过程概念归因(GP-CA)方法，利用主动学习通过查询信息丰富的实例来发现新的不确定性来源，并在三个公开数据集和真实机器人实验中进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;GP-CA在运行时间、主动学习的高样本效率和准确性方面优于其他最先进方法；真实世界实验证明了其适用性；GP-CA能够实现有效的故障恢复行为，提供更强大的机器人感知能力。&lt;h4&gt;结论&lt;/h4&gt;GP-CA是一种有效的方法，能够解决传统点云配准方法在不确定性条件下的问题，通过量化和解释不确定性来源，提高了配准过程的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们解决了点云配准问题，其中像ICP这样的知名方法在传感器噪声、姿态估计误差和由遮挡引起的部分重叠所产生的不确定性下表现不佳。我们开发了一种新颖的方法，高斯过程概念归因(GP-CA)，它不仅可以量化配准不确定性，还可以通过将不确定性归因于配准问题中已知的误差来源来解释它。我们的方法利用主动学习通过查询信息丰富的实例来发现新的不确定性来源。我们在三个公开可用的数据集和我们的真实世界机器人实验中验证了GP-CA。大量的消融实验证实了我们的设计选择。我们的方法在运行时间、主动学习的高样本效率和准确性方面优于其他最先进方法。我们的真实世界实验清楚地证明了其适用性。我们的视频还表明，GP-CA能够实现有效的故障恢复行为，从而产生更强大的机器人感知能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准中的不确定性解释问题。当配准方法如ICP面临传感器噪声、位姿估计误差和部分重叠（由于遮挡）等不确定性因素时会失败，现有方法只能量化不确定性大小，但不能解释不确定性来源。这个问题在机器人感知任务中非常重要，因为SLAM、3D重建和6自由度物体位姿估计等应用需要理解为什么配准失败才能采取正确的恢复策略。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有不确定性量化方法只能提供大小信息，无法解释原因；然后借鉴了可解释AI领域的概念解释方法（如TCAV），但针对ICP是几何优化算法而非神经网络的特点进行了改进；同时采用了3D点云处理领域的DGCNN进行特征提取，高斯过程分类器进行概念映射，以及主动学习领域的BALD机制来适应新概念。作者将多领域工作整合，针对点云配准问题的特殊性设计了GP-CA方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是不仅量化点云配准的不确定性，还以人类可理解的方式解释不确定性来源，将不确定性归因于语义上有意义的概念（如传感器噪声、位姿误差、部分重叠等）。整体流程：输入源点云和目标点云→ICP配准得到位姿估计→对齐源点云→DGCNN编码为特征向量→高斯过程分类器输出概念概率→当不确定性高时触发主动学习选择最有信息量的样本→用户标记样本→重新训练分类器适应新概念。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次实现点云配准不确定性的概念级解释；2)将不确定性归因于语义上有意义的概念而非特征；3)集成主动学习机制适应新不确定性来源；4)高效的实现速度（2-4秒/样本）。相比之前工作，传统方法只能量化不确定性大小，不能解释原因；SHAP和SA等方法计算量大（85-125秒/样本）且不能直接应用于ICP；TCAV不能提供校准概率且假设固定概念集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GP-CA通过结合表示学习、高斯过程分类和主动学习，首次实现了对点云配准不确定性的概念级解释，使机器人能够理解不确定性来源并采取针对性的恢复策略，显著提高了机器人感知任务的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we address the point cloud registration problem, wherewell-known methods like ICP fail under uncertainty arising from sensor noise,pose-estimation errors, and partial overlap due to occlusion. We develop anovel approach, Gaussian Process Concept Attribution (GP-CA), which not onlyquantifies registration uncertainty but also explains it by attributinguncertainty to well-known sources of errors in registration problems. Ourapproach leverages active learning to discover new uncertainty sources in thewild by querying informative instances. We validate GP-CA on three publiclyavailable datasets and in our real-world robot experiment. Extensive ablationssubstantiate our design choices. Our approach outperforms otherstate-of-the-art methods in terms of runtime, high sample-efficiency withactive learning, and high accuracy. Our real-world experiment clearlydemonstrates its applicability. Our video also demonstrates that GP-CA enableseffective failure-recovery behaviors, yielding more robust robotic perception.</description>
      <author>example@mail.com (Johannes A. Gaus, Loris Schneider, Yitian Shi, Jongseok Lee, Rania Rayyes, Rudolph Triebel)</author>
      <guid isPermaLink="false">2509.18786v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Query-Centric Diffusion Policy for Generalizable Robotic Assembly</title>
      <link>http://arxiv.org/abs/2509.18686v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Query-centric Diffusion Policy (QDP)的分层框架，用于解决机器人装配任务中高层技能查询与底层执行之间的不匹配问题，通过利用包含对象、接触点和技能信息的查询来连接高层规划和底层控制，显著提高了装配任务的技能精度和长时成功率。&lt;h4&gt;背景&lt;/h4&gt;机器人装配任务由于零件交互的内在复杂性以及在接触丰富环境中对噪声扰动的敏感性，成为构建通用机器人的关键挑战。传统装配代理采用分层设计（高层多零件推理和底层精确控制），但实践中存在高层技能查询与底层执行不匹配的问题。&lt;h4&gt;目的&lt;/h4&gt;解决机器人装配任务中分层策略实施困难的问题，特别是高层技能查询与底层执行之间的不匹配，提高装配任务的技能精度和成功率。&lt;h4&gt;方法&lt;/h4&gt;提出Query-centric Diffusion Policy (QDP)，一种分层框架，通过利用包含对象、接触点和技能信息的查询来连接高层规划和底层控制；引入以查询为中心的机制识别任务相关组件并指导底层策略；利用点云观测提高策略的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在FurnitureBench数据集的仿真和真实世界实验中，QDP在技能精度和长时成功率方面表现出色；在具有挑战性的插入和拧螺丝任务中，与没有结构化查询的基线相比，QDP将技能成功率提高了50%以上。&lt;h4&gt;结论&lt;/h4&gt;QDP通过以查询为中心的机制有效连接了高层规划和底层控制，显著提高了机器人装配任务在复杂环境中的性能表现，特别是在需要精确技能操作的任务中。&lt;h4&gt;翻译&lt;/h4&gt;机器人装配任务由于零件交互的内在复杂性以及在接触丰富环境中对噪声扰动的敏感性，是构建通用机器人的关键挑战。装配代理通常采用分层方式设计：高层多零件推理和底层精确控制。然而，由于高层技能查询与底层执行之间的不匹配，在实践中实施这种分层策略具有挑战性。为此，我们提出了Query-centric Diffusion Policy (QDP)，一种通过利用包含对象、接触点和技能信息的查询来连接高层规划和底层控制的分层框架。QDP引入了一种以查询为中心的机制，识别任务相关组件并用它们指导底层策略，利用点云观测提高策略的鲁棒性。我们在FurnitureBench上进行了综合实验，包括仿真和真实世界环境，证明了在技能精度和长时成功率方面的性能提升。在具有挑战性的插入和拧螺丝任务中，与没有结构化查询的基线相比，QDP将技能成功率提高了50%以上。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人装配任务中的挑战，特别是零件间关系推理和零件内精确控制的问题。这个问题很重要，因为机器人装配是构建通用机器人的关键任务，需要既精确又通用的策略来控制机械臂与多个对象交互。在接触密集场景中，即使是微小噪声也可能导致装配失败，解决此问题能推动机器人在制造业和家具组装等领域的实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将机器人装配任务分解为零件间关系推理和零件内精确控制两个主要挑战，设计了包含高级策略（负责多零件推理和技能选择）和低级策略（负责精确控制）的分层框架。作者借鉴了VLM-PC用于动态选择低级技能，扩散模型框架用于生成精确动作序列，SAM 2进行图像分割，SoM进行标记，以及PointNet处理点云数据。这些现有工作被整合到一个新框架中，并通过查询中心机制连接高级和低级策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'查询中心扩散策略'（QDP），一种分层框架，通过包含对象、接触点和技能信息的查询桥接高级规划和低级控制。整体流程包括：1）高级部分使用VLM-PC动态选择技能，通过SAM 2和SoM处理图像；2）低级部分使用查询中心编码器处理点云，预测组件状态，并通过动作解码器基于扩散模型生成精确动作；3）收集和标注演示数据；4）在模拟中训练并在真实世界以零样本方式应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）查询中心机制，识别任务相关组件指导低级策略；2）有效的分层框架接口；3）使用点云观测提高鲁棒性；4）查询条件策略学习方案。相比之前工作，不同之处在于：不依赖启发式低级控制器，将扩散模型与查询机制结合，解决了高级和低级模块集成时的复杂性，专注于任务相关组件而非处理所有信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的查询中心扩散策略（QDP）通过桥接高级规划和低级控制，显著提高了机器人在复杂装配任务中的性能和鲁棒性，特别是在插入和拧紧等高精度任务中，相比基线方法技能成功率提高了50%以上。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The robotic assembly task poses a key challenge in building generalist robotsdue to the intrinsic complexity of part interactions and the sensitivity tonoise perturbations in contact-rich settings. The assembly agent is typicallydesigned in a hierarchical manner: high-level multi-part reasoning andlow-level precise control. However, implementing such a hierarchical policy ischallenging in practice due to the mismatch between high-level skill queriesand low-level execution. To address this, we propose the Query-centricDiffusion Policy (QDP), a hierarchical framework that bridges high-levelplanning and low-level control by utilizing queries comprising objects, contactpoints, and skill information. QDP introduces a query-centric mechanism thatidentifies task-relevant components and uses them to guide low-level policies,leveraging point cloud observations to improve the policy's robustness. Weconduct comprehensive experiments on the FurnitureBench in both simulation andreal-world settings, demonstrating improved performance in skill precision andlong-horizon success rate. In the challenging insertion and screwing tasks, QDPimproves the skill-wise success rate by over 50% compared to baselines withoutstructured queries.</description>
      <author>example@mail.com (Ziyi Xu, Haohong Lin, Shiqi Liu, Ding Zhao)</author>
      <guid isPermaLink="false">2509.18686v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2509.18198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了MMCD（多模态协同决策）框架，通过融合多模态观测和跨模态知识蒸馏方法，解决了自动驾驶系统在事故多发环境中的决策挑战，显著提高了驾驶安全性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统在事故多发环境中面临决策挑战，单个车辆传感器范围有限且视野受阻，增加了事故风险。虽然多车辆连接系统和多模态方法（RGB图像和LiDAR点云）是潜在解决方案，但现有方法假设所有数据模态和连接车辆在训练和测试时都可用，这在实际中不现实，因为可能存在传感器故障或连接车辆缺失。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在部分数据模态不可用情况下仍保持鲁棒性能的决策框架，提高自动驾驶系统在事故多发环境中的安全性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出MMCD框架，融合自车和协作车辆的多模态观测以增强决策能力；采用基于跨模态知识蒸馏的教师-学生模型结构，教师模型使用多种数据模态训练，学生模型设计为在减少模态情况下有效运行；在地面车辆自动驾驶和空地车辆协作场景中进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;在实验中，该方法将驾驶安全性提高了20.7%，在检测潜在事故和做出安全驾驶决策方面超过了现有的最佳基线方法。&lt;h4&gt;结论&lt;/h4&gt;MMCD框架有效解决了自动驾驶系统在事故多发环境中的决策挑战，通过跨模态知识蒸馏确保了在部分模态缺失情况下的鲁棒性能，在提高自动驾驶安全性方面具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶系统已经取得了显著进展，但在事故多发环境中仍存在挑战，这些环境需要强大的决策能力。单个车辆的传感器范围有限且视野受阻，增加了事故发生的可能性。多车辆连接系统和多模态方法（利用RGB图像和LiDAR点云）已成为有前景的解决方案。然而，现有方法通常假设在训练和测试期间所有数据模态和连接车辆都可用，这在实际中是不现实的，因为可能存在传感器故障或连接车辆缺失的情况。为了应对这些挑战，我们为连接自主性引入了一个新框架MMCD（多模态协同决策）。我们的框架融合了来自自车和协作车辆的多模态观测，以增强在具有挑战性条件下的决策能力。为了确保在测试时某些数据模态不可用的情况下仍能保持鲁棒性能，我们提出了一种基于跨模态知识蒸馏的方法，采用教师-学生模型结构。教师模型使用多种数据模态进行训练，而学生模型设计为在减少模态的情况下有效运行。在地面车辆自动驾驶和空地车辆协作的实验中，我们的方法将驾驶安全性提高了20.7%，在检测潜在事故和做出安全驾驶决策方面超过了现有的最佳基线。更多信息可以在我们的网站https://ruiiu.github.io/mmcd上找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶车辆在事故易发环境中面临的决策挑战，特别是单个车辆传感器范围有限和视线被遮挡的问题。此外，还解决现有多模态和多车辆协作方法在实际应用中的局限性，即它们假设所有数据模态和协作车辆在训练和测试期间都可用，这在现实中不切实际。这个问题很重要，因为自动驾驶系统在事故易发环境中需要强大的决策能力，任何决策失误都可能导致严重后果；同时，传感器故障是常见问题，系统需要具备鲁棒性；另外，降低对昂贵LiDAR传感器的依赖也是成本效益的考虑。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶在事故易发环境中的挑战以及现有方法的局限性。他们借鉴了多车辆协作系统（如COOPERNAUT、STG）、多模态学习方法（如RGB和LiDAR融合研究）以及知识蒸馏技术（在自然语言处理和计算机视觉中的应用）。基于这些现有工作，作者创新性地设计了MMCD框架，融合来自自身和协作车辆的多模态观测，并提出了基于跨模态知识蒸馏的方法，使用教师-学生模型结构，确保在测试时某些数据模态不可用时的鲁棒性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1）多模态协作决策，融合来自自身和协作车辆的RGB图像和LiDAR点云数据；2）跨模态知识蒸馏，使用教师-学生模型结构，教师用多模态数据训练，学生用减少模态运行。整体流程包括：1）RGB模型处理图像数据，通过编码器提取特征，聚合器使用交叉注意力机制整合协作车辆特征；2）LiDAR模型处理点云数据；3）决策模型基于融合特征做出决策；4）通过知识蒸馏训练学生模型，使其能在测试时仅使用RGB数据的情况下仍保持高性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）MMCD框架，能融合多车辆多模态观测；2）跨模态知识蒸馏方法，处理测试时模态不可用的情况；3）教师-学生模型结构，确保鲁棒性。相比之前工作，不同之处在于：1）能处理测试时某些模态不可用的情况，而大多数现有方法假设所有模态都可用；2）支持更灵活的应用场景，包括单/多车辆和单/多模态配置；3）实验显示安全性提高最高20.7%，超过现有最佳基线；4）通过减少对昂贵LiDAR的依赖，提供更成本效益的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了MMCD框架，通过多模态协作决策和跨模态知识蒸馏，显著提高了自动驾驶系统在事故易发环境中的安全性和鲁棒性，同时降低了系统成本。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous systems have advanced significantly, but challenges persist inaccident-prone environments where robust decision-making is crucial. A singlevehicle's limited sensor range and obstructed views increase the likelihood ofaccidents. Multi-vehicle connected systems and multi-modal approaches,leveraging RGB images and LiDAR point clouds, have emerged as promisingsolutions. However, existing methods often assume the availability of all datamodalities and connected vehicles during both training and testing, which isimpractical due to potential sensor failures or missing connected vehicles. Toaddress these challenges, we introduce a novel framework MMCD (Multi-ModalCollaborative Decision-making) for connected autonomy. Our framework fusesmulti-modal observations from ego and collaborative vehicles to enhancedecision-making under challenging conditions. To ensure robust performance whencertain data modalities are unavailable during testing, we propose an approachbased on cross-modal knowledge distillation with a teacher-student modelstructure. The teacher model is trained with multiple data modalities, whilethe student model is designed to operate effectively with reduced modalities.In experiments on $\textit{connected autonomous driving with ground vehicles}$and $\textit{aerial-ground vehicles collaboration}$, our method improvesdriving safety by up to ${\it 20.7}\%$, surpassing the best-existing baselinein detecting potential accidents and making safe driving decisions. Moreinformation can be found on our website https://ruiiu.github.io/mmcd.</description>
      <author>example@mail.com (Rui Liu, Zikang Wang, Peng Gao, Yu Shen, Pratap Tokekar, Ming Lin)</author>
      <guid isPermaLink="false">2509.18198v1</guid>
      <pubDate>Wed, 24 Sep 2025 15:14:54 +0800</pubDate>
    </item>
    <item>
      <title>Preconditioned Deformation Grids</title>
      <link>http://arxiv.org/abs/2509.18097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  GitHub: https://github.com/vc-bonn/preconditioned-deformation-grids&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'预调节变形网格'的新技术，用于从不规则点云序列中直接估计连贯变形场，无需显式对应关系。该方法利用多分辨率体素网格在不同空间尺度上捕获整体运动，并结合基于网格的Sobolev预调节到梯度优化中，通过Chamfer损失函数获得准确变形。弱等距损失的加入确保了物体表面的时间一致性而不约束变形保真度。评估表明，该方法尤其对于长序列能取得优于现有技术的结果。&lt;h4&gt;背景&lt;/h4&gt;从点云序列进行物体动态表面重建是计算机图形学中的一个挑战性领域。现有方法要么需要多个正则化项，要么需要大量训练数据，但这会导致重建精度的妥协以及过度平滑或对未见物体和运动的泛化能力差的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在重建精度、过度平滑以及对未见物体和运动泛化能力方面的限制。&lt;h4&gt;方法&lt;/h4&gt;1. 引入预调节变形网格技术；2. 使用多分辨率体素网格在不同空间尺度上捕获整体运动；3. 将基于网格的Sobolev预调节整合到梯度优化中；4. 应用输入点云和演化模板网格之间的Chamfer损失函数；5. 在网格边上加入弱等距损失以确保物体表面的时间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;通过结合多分辨率体素网格、基于网格的Sobolev预调节和Chamfer损失函数，可以从不规则点云序列中准确估计变形场。弱等距损失的加入确保了时间一致性而不约束变形保真度。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在评估中表现出色，特别是对于长序列，与现有最先进技术相比取得了更好的结果。&lt;h4&gt;翻译&lt;/h4&gt;从点云序列进行物体的动态表面重建是计算机图形学中的一个具有挑战性的领域。现有方法要么需要多个正则化项，要么需要大量训练数据，然而这会导致重建精度的妥协以及过度平滑或对未见物体和运动的泛化能力差。为了解决这些局限性，我们引入了预调节变形网格，一种从不规则点云序列直接估计连贯变形场的新技术，无需形成或要求显式对应关系。我们方法的关键是使用多分辨率体素网格在不同空间尺度上捕获整体运动，从而实现更灵活的变形表示。结合将基于网格的Sobolev预调节整合到梯度优化中，我们表明在输入点云以及演化模板网格之间应用Chamfer损失足以获得准确的变形。为确保物体表面的时间一致性，我们在网格边上包含了弱等距损失，它补充了主要目标而不约束变形保真度。大量评估表明，与最先进技术相比，我们的方法取得了更优的结果，特别是对于长序列。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从无结构的点云序列中动态重建3D表面的问题。这个问题在现实中很重要，因为它在动画、虚拟制作、医学成像、机器人、自动驾驶、增强现实和虚拟现实等领域有广泛应用。随着深度传感设备的普及，可以捕获大量3D点云，但这些点云通常无结构、缺乏时间对应关系，且存在噪声，给可靠的表面重建带来挑战。现有方法要么需要多个正则化项，要么需要大量训练数据，导致精度妥协或过度平滑。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者面对现有方法的局限性，希望开发一个能联合利用时间一致性而不依赖限制性先验，同时保持高频细节的框架。他们借鉴了模板模型（如SMPL、SCAPE）但避免了对类别特定假设的依赖；借鉴了变形场估计方法，包括基于优化和基于学习的技术；特别借鉴了预处理技术，特别是Sobolev预处理，它在几何处理中已被用于改善网格参数化和加速网格变形。作者的工作与DynoSurf在精神上相似，都是无需类别特定监督、训练或对应关系的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用多分辨率体素网格捕获不同空间尺度的运动，结合基于网格的Sobolev预处理稳定优化过程，并仅用简单的Chamfer损失和弱等距损失来指导重建。整体流程包括：1)选择关键帧估计初始模板网格；2)使用多分辨率体素网格表示变形场；3)应用Sobolev预处理扩散梯度信息；4)优化初始网格和变换网格以最小化总损失；5)使用自适应置信权重确保时间一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)无需对应关系的变形框架，直接操作于无结构点云；2)多分辨率体素网格表示变形场；3)基于网格的Sobolev预处理方案。与之前工作的不同之处在于：不需要类别特定假设或预训练模型；不需要显式对应关系或大量训练数据；仅使用简单的Chamfer损失和弱等距损失而非多个正则化项；能保持高频表面细节；是训练自由且类别无关的。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Preconditioned Deformation Grids是一种无需对应关系、无需训练的框架，通过多分辨率体素网格和Sobolev预处理技术，直接从无结构的点云序列中重建时间一致的高保真表面，克服了现有方法在精度、平滑度和泛化能力之间的权衡问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic surface reconstruction of objects from point cloud sequences is achallenging field in computer graphics. Existing approaches either requiremultiple regularization terms or extensive training data which, however, leadto compromises in reconstruction accuracy as well as over-smoothing or poorgeneralization to unseen objects and motions. To address these lim- itations,we introduce Preconditioned Deformation Grids, a novel technique for estimatingcoherent deformation fields directly from unstructured point cloud sequenceswithout requiring or forming explicit correspondences. Key to our approach isthe use of multi-resolution voxel grids that capture the overall motion atvarying spatial scales, enabling a more flexible deformation representation. Inconjunction with incorporating grid-based Sobolev preconditioning intogradient-based optimization, we show that applying a Chamfer loss between theinput point clouds as well as to an evolving template mesh is sufficient toobtain accurate deformations. To ensure temporal consistency along the objectsurface, we include a weak isometry loss on mesh edges which complements themain objective without constraining deformation fidelity. Extensive evaluationsdemonstrate that our method achieves superior results, particularly for longsequences, compared to state-of-the-art techniques.</description>
      <author>example@mail.com (Julian Kaltheuner, Alexander Oebel, Hannah Droege, Patrick Stotko, Reinhard Klein)</author>
      <guid isPermaLink="false">2509.18097v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
  <item>
      <title>RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point Clouds</title>
      <link>http://arxiv.org/abs/2509.18068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RadarSFD是一种条件潜在扩散框架，能从单帧毫米波雷达重建类似LiDAR的密集点云，无需运动或SAR，在小型机器人系统中实现了高分辨率感知。&lt;h4&gt;背景&lt;/h4&gt;毫米波雷达在恶劣环境(雾、烟、尘、低光)中具有鲁棒性感知能力，但当前方法依赖合成孔径或多帧聚合提高分辨率，不适用于小型空中、检查或可穿戴系统。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需运动或SAR的单帧雷达系统，重建类似LiDAR的密集点云，解决小型机器人平台的感知挑战。&lt;h4&gt;方法&lt;/h4&gt;提出RadarSFD框架，将预训练单目深度估计器的几何先验转移到扩散主干，通过通道级潜在连接锚定到雷达输入，并用双空间目标(结合潜在和像素空间损失)正则化输出。&lt;h4&gt;主要发现&lt;/h4&gt;在RadarHD基准上，RadarSFD实现35cm Chamfer距离和28cm Modified Hausdorff距离，优于单帧基线(56cm,45cm)，与多帧方法(5-41帧)相当；能恢复精细墙壁和窄缝，在新环境中有强泛化能力。&lt;h4&gt;结论&lt;/h4&gt;RadarSFD建立了首个实用的单帧、无SAR mmWave雷达管道，为紧凑机器人系统中的密集点云感知提供了新方案。&lt;h4&gt;翻译&lt;/h4&gt;毫米波雷达提供对雾、烟、尘和低光的鲁棒感知能力，使其对尺寸、重量和功率受限的机器人平台具有吸引力。然而，当前雷达成像方法依赖合成孔径或多帧聚合来提高分辨率，这对小型空中、检查或可穿戴系统不切实际。我们提出了RadarSFD，一种条件潜在扩散框架，无需运动或SAR即可从单帧雷达重建类似LiDAR的密集点云。我们的方法将预训练单目深度估计器的几何先验转移到扩散主干，通过通道级潜在连接将其锚定到雷达输入，并用结合潜在空间和像素空间损失的双空间目标来正则化输出。在RadarHD基准上，RadarSFD实现了35厘米Chamfer距离和28厘米Modified Hausdorff距离，优于单帧RadarHD基线(56厘米，45厘米)，并保持与使用5-41帧的多帧方法的竞争力。定性结果显示恢复了精细墙壁和窄缝，新环境实验证实了强大的泛化能力。消融研究强调了预训练初始化、雷达BEV条件和双空间损失的重要性。这些结果共同建立了第一个实用的单帧、无SAR mmWave雷达管道，用于紧凑机器人系统中的密集点云感知。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决如何从单个毫米波雷达帧生成密集的激光雷达样点云，而不需要合成孔径雷达或多帧聚合。这个问题重要是因为毫米波雷达在恶劣天气条件下具有鲁棒性，但传统方法需要多帧或SAR来提高分辨率，这对小型无人机、可穿戴设备等资源受限平台不切实际。单帧雷达点云生成能为这些紧凑系统提供实用的空间感知能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考雷达不需要从头学习世界结构，而是可以转移预训练模型的几何先验。他们借鉴了单目深度估计工作(如Marigold)的几何先验，以及RadarHD关于轻度阈值雷达BEV保留有用信息的见解。方法设计采用条件潜在扩散框架，使用冻结的VAE进行高效编码，从Marigold初始化U-Net注入深度先验，并通过通道级联将雷达条件融入扩散过程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练单目深度估计器的几何先验，通过条件潜在扩散模型从单帧雷达重建密集点云。流程包括：1)将原始I/Q信号处理为轻度阈值的2D BEV图像；2)使用冻结VAE将雷达和LiDAR BEV编码到潜在空间；3)将雷达潜在与噪声LiDAR潜在连接，输入预训练U-Net；4)U-Net迭代去噪重建干净LiDAR潜在；5)VAE解码器生成高分辨率LiDAR样BEV；6)结合潜在和像素空间损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个单帧无SAR的雷达感知管道；2)条件潜在扩散模型转移单目深度先验；3)双空间目标函数确保保真度。不同之处：相比RadarHD无需41帧堆叠；相比ICRA'24使用单帧而非5帧；相比RAL'24保持原生4cm分辨率而非降低；相比cGAN方法不限于特定对象类别；使用潜在扩散提高效率，预训练先验增强泛化，通道级联提供更强几何引导。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RadarSFD通过条件潜在扩散模型和预训练几何先验，首次实现了从单帧毫米波雷达高效生成密集激光雷达样点云，为资源受限机器人平台提供了无需运动或合成孔径的空间感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millimeter-wave radar provides perception robust to fog, smoke, dust, and lowlight, making it attractive for size, weight, and power constrained roboticplatforms. Current radar imaging methods, however, rely on synthetic apertureor multi-frame aggregation to improve resolution, which is impractical forsmall aerial, inspection, or wearable systems. We present RadarSFD, aconditional latent diffusion framework that reconstructs dense LiDAR-like pointclouds from a single radar frame without motion or SAR. Our approach transfersgeometric priors from a pretrained monocular depth estimator into the diffusionbackbone, anchors them to radar inputs via channel-wise latent concatenation,and regularizes outputs with a dual-space objective combining latent andpixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm ChamferDistance and 28 cm Modified Hausdorff Distance, improving over the single-frameRadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-framemethods using 5-41 frames. Qualitative results show recovery of fine walls andnarrow gaps, and experiments across new environments confirm stronggeneralization. Ablation studies highlight the importance of pretrainedinitialization, radar BEV conditioning, and the dual-space loss. Together,these results establish the first practical single-frame, no-SAR mmWave radarpipeline for dense point cloud perception in compact robotic systems.</description>
      <author>example@mail.com (Bin Zhao, Nakul Garg)</author>
      <guid isPermaLink="false">2509.18068v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile Mapless Drone Navigation</title>
      <link>http://arxiv.org/abs/2509.17340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AERO-MPPI的GPU加速框架，用于解决无人机在复杂3D环境中无地图导航的挑战。该框架通过基于锚点的模型预测路径积分优化器集合统一了感知和规划过程，实现了实时、可靠和敏捷的自主飞行。&lt;h4&gt;背景&lt;/h4&gt;传统的映射-规划-控制流水线在计算成本上很高，并且会传播估计误差。在杂乱的3D环境中进行敏捷的无地图导航对自主无人机构成了重大挑战。现有的单MPPI方法容易陷入局部最小值失败。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实时运行的框架，使无人机能够在复杂的杂乱3D环境中安全、敏捷和鲁棒地飞行，同时避免传统方法的计算成本高和误差传播问题。&lt;h4&gt;方法&lt;/h4&gt;1. 设计多分辨率激光雷达点云表示方法，快速提取空间分布的'锚点'作为前视中间端点；2. 从锚点构建多项式轨迹引导，探索不同的同伦路径类别；3. 并行运行多个MPPI实例，使用两阶段多目标成本函数评估，平衡避障和目标到达；4. 完全使用NVIDIA Warp GPU内核实现。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在森林、垂直和倾斜环境中的模拟表明，AERO-MPPI能持续可靠地以超过7米/秒的速度飞行；2. 成功率高于80%，与最先进基线相比轨迹更平滑；3. 真实世界实验证实该框架能实时运行并实现安全、敏捷和鲁棒的飞行。&lt;h4&gt;结论&lt;/h4&gt;AERO-MPPI框架成功解决了无人机在复杂3D环境中无地图导航的挑战，通过GPU加速和优化的规划算法，实现了实时、可靠和高效的自主飞行，在模拟和真实世界测试中都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;在杂乱的3D环境中进行敏捷的无地图导航对自主无人机构成了重大挑战。传统的映射-规划-控制流水线计算成本高且会传播估计误差。我们提出了AERO-MPPI，一个完全由GPU加速的框架，它通过基于锚点的模型预测路径积分优化器集合统一了感知和规划。具体来说，我们设计了一种多分辨率激光雷达点云表示方法，可以快速提取空间分布的'锚点'作为前视中间端点，并从中构建多项式轨迹引导，以探索不同的同伦路径类别。在每个规划步骤中，我们并行运行多个MPPI实例，并使用两阶段多目标成本函数进行评估，该函数平衡了避障和目标到达。AERO-MPPI完全使用NVIDIA Warp GPU内核实现，实现了实时机上操作，并减轻了单MPPI方法的局部最小值失败问题。在森林、垂直和倾斜环境中的大量模拟表明，AERO-MPPI能够持续可靠地以超过7米/秒的速度飞行，成功率超过80%，与最先进的基线相比轨迹更平滑。在配备激光雷达的四旋翼无人机和NVIDIA Jetson Orin NX 16G上的真实世界实验证实，AERO-MPPI能够实时机上运行，并在复杂的杂乱环境中一致地实现安全、敏捷和鲁棒的飞行。论文接受后，代码将开源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在复杂3D环境中实现敏捷的无地图无人机导航问题。这个问题很重要，因为传统导航方法计算成本高、会累积误差，在高速飞行时难以实时处理，且容易陷入局部最优导致碰撞。在搜索救援、物流配送等实际应用中，无人机需要在复杂未知环境中高速、安全地飞行，这对导航算法提出了严峻挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统映射-规划-控制流水线的局限性，指出它们计算成本高且会累积误差。然后评估了现有基于地图的规划方法和基于MPPI的方法，发现单一MPPI容易陷入局部最优。作者借鉴了MPPI采样优化的思想、多尺度分析的思想以及轨迹初始化和引导的思想，设计了锚点引导的MPPI集合方法。通过在目标方向上生成多个锚点作为中间目标点，并行运行多个MPPI实例，每个实例跟踪一个锚点生成的引导轨迹，从而增强探索能力并避免局部最优。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过锚点引导的MPPI集合来增强轨迹优化的探索能力，避免单一MPPI方法容易陷入局部最优的问题。整体实现流程包括：1) 多分辨率环境表示（高分辨率用于精确碰撞检测，低分辨率用于锚点生成）；2) 锚点生成与轨迹引导（在目标方向上采样锚点，生成多项式引导轨迹）；3) 并行MPPI优化（每个锚点对应一个MPPI实例，进行两阶段优化）；4) 实时执行（在GPU上实现整个流程，以50Hz频率重新规划）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 锚点引导的MPPI集合，通过并行探索多种路径类别避免局部最优；2) 多分辨率LiDAR表示，平衡精度和效率；3) 两阶段优化策略，保持多样性同时确保安全性；4) 完全GPU加速的感知到规划流水线，实现实时性能。相比传统基于地图的方法，AERO-MPPI无需构建显式地图，减少了计算延迟；相比单一MPPI方法，它通过并行探索提高了在复杂环境中的成功率和轨迹质量；相比其他MPPI变体，它引入了锚点机制和多分辨率表示，增强了探索能力和效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AERO-MPPI通过结合锚点引导的MPPI集合、多分辨率环境表示和完全GPU加速的感知到规划流水线，实现了在复杂3D环境中高速、安全、实时的无地图无人机导航，显著提高了导航的鲁棒性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agile mapless navigation in cluttered 3D environments poses significantchallenges for autonomous drones. Conventional mapping-planning-controlpipelines incur high computational cost and propagate estimation errors. Wepresent AERO-MPPI, a fully GPU-accelerated framework that unifies perceptionand planning through an anchor-guided ensemble of Model Predictive PathIntegral (MPPI) optimizers. Specifically, we design a multi-resolution LiDARpoint-cloud representation that rapidly extracts spatially distributed"anchors" as look-ahead intermediate endpoints, from which we constructpolynomial trajectory guides to explore distinct homotopy path classes. At eachplanning step, we run multiple MPPI instances in parallel and evaluate themwith a two-stage multi-objective cost that balances collision avoidance andgoal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPIachieves real-time onboard operation and mitigates the local-minima failures ofsingle-MPPI approaches. Extensive simulations in forests, verticals, andinclines demonstrate sustained reliable flight above 7 m/s, with success ratesabove 80% and smoother trajectories compared to state-of-the-art baselines.Real-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX16G confirm that AERO-MPPI runs in real time onboard and consistently achievessafe, agile, and robust flight in complex cluttered environments. The code willbe open-sourced upon acceptance of the paper.</description>
      <author>example@mail.com (Xin Chen, Rui Huang, Longbin Tang, Lin Zhao)</author>
      <guid isPermaLink="false">2509.17340v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds</title>
      <link>http://arxiv.org/abs/2509.17207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Point-RTD的新型预训练策略，通过损坏-重建框架提高点云模型的鲁棒性，在多个基准测试中显著优于传统方法。&lt;h4&gt;背景&lt;/h4&gt;预训练策略在提升基于Transformer的3D点云模型性能方面起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;开发Point-RTD预训练策略，通过损坏-重建框架提高token的鲁棒性，学习更有效的结构先验知识。&lt;h4&gt;方法&lt;/h4&gt;Point-RTD不同于传统的基于掩码的重建任务，它损坏点云token并使用判别器-生成器架构进行去噪，从而更有效地学习结构先验知识。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet数据集上，Point-RTD比PointMAE减少93%以上的重建误差，测试集Chamfer距离降低14倍以上；在ShapeNet、ModelNet10和ModelNet40上收敛更快，分类准确率更高。&lt;h4&gt;结论&lt;/h4&gt;Point-RTD在所有测试案例中均明显优于基线Point-MAE框架，提高了模型性能和效率。&lt;h4&gt;翻译&lt;/h4&gt;预训练策略在提升基于Transformer的3D点云任务模型性能方面起着关键作用。在本文中，我们引入了Point-RTD（替换标记去噪），一种旨在通过损坏-重建框架提高标记鲁棒性的新型预训练策略。与传统的基于掩码的重建任务（隐藏数据段供后续预测）不同，Point-RTD损坏点云标记并利用判别器-生成器架构进行去噪。这种转变能够更有效地学习结构先验知识，并显著提高模型性能和效率。在ShapeNet数据集上，Point-RTD与PointMAE相比减少了93%以上的重建误差，并在测试集上实现了超过14倍的更低Chamfer距离。我们的方法在ShapeNet、ModelNet10和ModelNet40基准测试上也收敛更快，并产生更高的分类准确率，在所有情况下都明显优于基线Point-MAE框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云数据的预训练问题。点云是自动驾驶、机器人和遥感等领域的重要三维数据表示，但它们无结构、缺乏内在顺序和统一邻域关系，使得应用Transformer架构变得复杂。当前主流的掩码自编码预训练方法可能不是最优策略，因此需要更有效的预训练方法来提高模型性能和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先质疑了掩码自编码是否是点云重建的最优策略，认为可能有其他策略能更好地处理非结构化3D空间的挑战。他们借鉴了GANLM中的替换标记去噪概念，结合Point-BERT和Point-MAE中的基于补丁的标记化方法，创新性地设计了Point-RTD。作者通过引入标记替换和判别器-生成器架构，形成了一个损坏-重建框架，旨在增强标记鲁棒性和强化语义一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过替换标记去噪来增强点云Transformer模型的预训练，不同于传统掩码方法隐藏数据段进行预测，而是通过替换标记来'损坏'点云，然后使用判别器-生成器架构进行去噪。整体流程包括：1)点云标记化，使用FPS和kNN分割点云为补丁并通过mini-PointNet编码；2)损坏机制，应用高斯噪声和标记替换策略；3)判别器识别损坏标记，生成器清理这些标记；4)预训练算法，包括损坏、判别、生成和重建步骤，使用总损失函数进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于替换标记去噪的新型预训练策略；2)损坏-重建框架替代传统掩码方法；3)随机混合和最近邻混合两种标记替换策略；4)判别器-生成器架构形成反馈循环；5)将损坏机制视为标记空间中的对比正则化。相比Point-MAE，Point-RTD不隐藏数据段而是替换标记进行去噪，在ShapeNet上将重建误差降低93%以上，在ModelNet10上收敛更快且分类准确率更高。相比Point-BERT，虽然都使用基于补丁的标记化，但Point-RTD引入了判别器-生成器架构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了Point-RTD，一种基于替换标记去噪的新型点云预训练策略，通过损坏-重建框架显著提高了点云Transformer模型的性能、效率和鲁棒性，在多个基准测试中超越了现有的掩码自编码方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training strategies play a critical role in advancing the performance oftransformer-based models for 3D point cloud tasks. In this paper, we introducePoint-RTD (Replaced Token Denoising), a novel pretraining strategy designed toimprove token robustness through a corruption-reconstruction framework. Unliketraditional mask-based reconstruction tasks that hide data segments for laterprediction, Point-RTD corrupts point cloud tokens and leverages adiscriminator-generator architecture for denoising. This shift enables moreeffective learning of structural priors and significantly enhances modelperformance and efficiency. On the ShapeNet dataset, Point-RTD reducesreconstruction error by over 93% compared to PointMAE, and achieves more than14x lower Chamfer Distance on the test set. Our method also converges fasterand yields higher classification accuracy on ShapeNet, ModelNet10, andModelNet40 benchmarks, clearly outperforming the baseline Point-MAE frameworkin every case.</description>
      <author>example@mail.com (Gunner Stone, Youngsook Choi, Alireza Tavakkoli, Ankita Shukla)</author>
      <guid isPermaLink="false">2509.17207v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2509.17206v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于扩散的框架，将每点语义条件直接嵌入生成过程，实现几何和语义的联合合成，生成结构连贯且具有分割感知能力的3D点云。&lt;h4&gt;背景&lt;/h4&gt;生成逼真的3D点云是计算机视觉中的基本问题，应用于遥感、机器人和数字建模。现有生成方法主要捕捉几何信息，而语义信息通常是通过外部分割或聚类后添加，而非集成到生成过程中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将语义信息直接整合到生成过程中的方法，实现几何和语义的联合合成，生成高质量的3D点云。&lt;h4&gt;方法&lt;/h4&gt;提出基于扩散的框架，为每个点分配与语义标签对应的条件变量，这些变量引导扩散动力学，实现几何和语义的联合合成。&lt;h4&gt;主要发现&lt;/h4&gt;条件变量对扩散动力学和生成质量有显著影响；产生的点云结构连贯且具有分割感知能力；对象部分在合成过程中被明确表示；通过广泛实验验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法能够生成详细且准确的3D点云，适合特定部分和特征，且对象部分在合成过程中被明确表示，结构上保持连贯性。&lt;h4&gt;翻译&lt;/h4&gt;生成逼真的3D点云是计算机视觉中的一个基本问题，在遥感、机器人和数字建模等领域有广泛应用。现有的生成方法主要捕捉几何信息，而当考虑语义时，通常是通过外部分割或聚类后添加，而不是集成到生成过程中本身。我们提出了一种基于扩散的框架，将每点语义条件直接嵌入生成过程中。每个点都关联一个对应其语义标签的条件变量，该变量引导扩散动力学，并实现几何和语义的联合合成。这种设计产生的点云既结构连贯又具有分割感知能力，对象部分在合成过程中被明确表示。通过对引导和非引导扩散过程的比较分析，我们证明了条件变量对扩散动力学和生成质量的显著影响。大量实验验证了我们方法的有效性，生成了详细且准确的3D点云，适用于特定部分和特征。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在3D点云生成过程中集成语义信息的问题，而不是像现有方法那样仅关注几何结构或在生成后添加语义标签。这个问题在现实中很重要，因为准确的3D表示对遥感、机器人、数字对象建模等领域至关重要，而语义与几何的联合生成能够创建更真实、更有用的3D内容，支持需要精细识别组件的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有生成模型如VAEs和GANs在处理点云不规则结构时的局限性，发现扩散模型通过将点云视为粒子系统更有潜力。他们借鉴了Luo等人的扩散框架，但创新性地将逐点语义标签作为条件变量嵌入扩散过程。方法设计包括编码器-解码器架构、两种扩散变体（引导和无条件）以及专门的损失函数，这些设计都基于对点云特性和生成需求的深入理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将语义信息直接整合到3D点云生成过程中，为每个点关联语义标签条件变量，引导扩散动力学实现几何和语义的联合合成。整体流程包括：1)使用编码器将点云映射到潜在向量；2)前向过程逐渐添加噪声；3)反向过程从噪声恢复点云；4)实现两种扩散变体：引导扩散（保持语义固定）和无条件扩散（对语义也添加噪声）；5)使用专门的损失函数训练模型，确保生成既结构连贯又语义一致的点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将逐点语义条件直接嵌入3D点云生成过程；2)系统分析引导和无条件扩散对生成质量的影响；3)实现几何和语义的联合合成；4)引入按类别Chamfer距离损失函数。相比之前工作，本文方法不仅关注几何结构，还确保语义一致性；不同于DiffusionPointLabel将语义作为辅助预测，本文将其作为生成integral部分；不同于传统方法使用全局距离度量，本文确保同类点之间的距离最小化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的扩散框架，通过将逐点语义条件直接嵌入生成过程，实现了几何结构和语义信息的联合合成，生成了既结构连贯又具有分割感知能力的3D点云，为可控的语义驱动的3D生成奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic 3D point clouds is a fundamental problem in computervision with applications in remote sensing, robotics, and digital objectmodeling. Existing generative approaches primarily capture geometry, and whensemantics are considered, they are typically imposed post hoc through externalsegmentation or clustering rather than integrated into the generative processitself. We propose a diffusion-based framework that embeds per-point semanticconditioning directly within generation. Each point is associated with aconditional variable corresponding to its semantic label, which guides thediffusion dynamics and enables the joint synthesis of geometry and semantics.This design produces point clouds that are both structurally coherent andsegmentation-aware, with object parts explicitly represented during synthesis.Through a comparative analysis of guided and unguided diffusion processes, wedemonstrate the significant impact of conditional variables on diffusiondynamics and generation quality. Extensive experiments validate the efficacy ofour approach, producing detailed and accurate 3D point clouds tailored tospecific parts and features.</description>
      <author>example@mail.com (Gunner Stone, Sushmita Sarker, Alireza Tavakkoli)</author>
      <guid isPermaLink="false">2509.17206v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2509.17125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Imagine2Act，一个3D模仿学习框架，用于处理需要精确语义和几何推理的关系对象重排任务，通过整合对象语义和几何约束到策略学习中，解决了现有方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;关系对象重排任务（如将花插入花瓶）需要机器人进行精确的语义和几何推理。现有方法要么依赖预先收集的演示数据，难以捕捉复杂几何约束；要么生成目标状态观测但未能将对象转换与动作预测明确耦合，导致生成噪声引起的错误。&lt;h4&gt;目的&lt;/h4&gt;提出Imagine2Act框架，将对象的语义和几何约束整合到策略学习中，以解决高精度操作任务中的挑战。&lt;h4&gt;方法&lt;/h4&gt;Imagine2Act首先基于语言指令生成想象的目标图像并重建相应3D点云，提供稳健的语义和几何先验。这些想象目标点云作为策略模型额外输入，同时采用带有软姿态监督的对象-动作一致性策略，将预测的末端执行器运动与生成的对象转换明确对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和现实世界中的实验表明，Imagine2Act优于之前的最先进策略。该方法能够推理对象间的语义和几何关系，并在多样化任务中预测准确动作。&lt;h4&gt;结论&lt;/h4&gt;Imagine2Act通过整合语义和几何约束，并明确对齐对象转换与动作预测，有效解决了高精度操作任务中的挑战，实现了优于先前方法的性能。&lt;h4&gt;翻译&lt;/h4&gt;关系对象重排（ROR）任务（例如，将花插入花瓶）要求机器人操作对象并进行精确的语义和几何推理。现有方法要么依赖预先收集的演示数据，这些数据难以捕捉复杂的几何约束；要么生成目标状态观测来捕获语义和几何知识，但未能将对象转换与动作预测明确耦合，导致生成噪声引起的错误。为解决这些局限性，我们提出了Imagine2Act，一个3D模仿学习框架，将对象的语义和几何约束整合到策略学习中，以处理高精度操作任务。我们首先基于语言指令生成想象的目标图像，并重建相应的3D点云，以提供稳健的语义和几何先验。这些想象的目标点云作为策略模型的额外输入，同时采用带有软姿态监督的对象-动作一致性策略，将预测的末端执行器运动与生成的对象转换明确对齐。这种设计使Imagine2Act能够推理对象间的语义和几何关系，并在多样化任务中预测准确动作。模拟和现实世界中的实验均表明，Imagine2Act优于之前的最先进策略。更多可视化内容可在https://sites.google.com/view/imagine2act查看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决关系物体重排(ROR)任务中的机器人操作问题，这类任务要求机器人根据语义指令(如'把花插入花瓶')精确操作物体，需要同时进行语义理解和几何推理。这个问题在现实中很重要，因为它是家庭机器人实现自主清理和整理物品等基本技能的关键，挑战在于需要处理物体间严格的几何约束和语义关系，容错空间很小，例如在'把盘子放入架子'的任务中，机器人必须精确调整盘子位置以确保垂直插入窄槽。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考：传统3D模仿学习无法明确推理物体间复杂几何约束；现有方法虽能捕捉几何转换但未利用常识语义约束；生成模型虽能产生目标状态观测但未将物体转换与动作预测结合。基于这些分析，作者借鉴了3D模仿学习框架、生成模型(GPT-Image-1)、3D重建技术(TripoSR)、扩散模型和6D姿态估计算法(FoundationPose)等现有工作，设计出Imagine2Act框架，结合语义几何约束到策略学习中，并引入物体-动作一致性学习策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'想象的目标点云'提供语义和几何约束，并引入'物体-动作一致性学习'策略，将预测的末端执行器动作与生成的物体转换明确对齐。整体流程包括：1)语义几何约束生成：使用生成模型基于语言指令生成想象目标图像，分割前景与背景，3D重建前景物体点云，结合形成想象目标点云并投影为RGB-D输入；2)物体-动作一致性学习：计算物体SE(3)变换并编码为变换令牌注入策略模型，设计软姿态一致性损失对齐动作与物体转换；3)策略模型：采用3D Transformer条件扩散模型，输入包括当前观测、想象目标观测、语言指令和历史动作，输出机器人动作序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)想象的目标点云生成模块：利用现成模型确保零样本生成鲁棒性，只修改任务相关物体减少噪声；2)物体-动作一致性学习策略：明确耦合物体转换与机器人动作，避免误差累积；3)统一框架：结合语义几何约束生成和物体-动作一致性学习。相比之前工作，不同之处在于：明确推理物体间复杂几何约束而非仅映射观测到动作；利用物理世界的常识语义约束；明确将物体转换与动作预测结合而非直接使用生成转换作为动作；将约束作为策略先验并引入软监督防止误差累积。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Imagine2Act通过生成想象的目标点云提供语义几何约束，并引入物体-动作一致性学习策略，实现了在高精度机器人操作任务中超越现有方法的性能，特别是在需要精确语义和几何推理的关系物体重排任务中表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relational object rearrangement (ROR) tasks (e.g., insert flower to vase)require a robot to manipulate objects with precise semantic and geometricreasoning. Existing approaches either rely on pre-collected demonstrations thatstruggle to capture complex geometric constraints or generate goal-stateobservations to capture semantic and geometric knowledge, but fail toexplicitly couple object transformation with action prediction, resulting inerrors due to generative noise. To address these limitations, we proposeImagine2Act, a 3D imitation-learning framework that incorporates semantic andgeometric constraints of objects into policy learning to tackle high-precisionmanipulation tasks. We first generate imagined goal images conditioned onlanguage instructions and reconstruct corresponding 3D point clouds to providerobust semantic and geometric priors. These imagined goal point clouds serve asadditional inputs to the policy model, while an object-action consistencystrategy with soft pose supervision explicitly aligns predicted end-effectormotion with generated object transformation. This design enables Imagine2Act toreason about semantic and geometric relationships between objects and predictaccurate actions across diverse tasks. Experiments in both simulation and thereal world demonstrate that Imagine2Act outperforms previous state-of-the-artpolicies. More visualizations can be found athttps://sites.google.com/view/imagine2act.</description>
      <author>example@mail.com (Liang Heng, Jiadong Xu, Yiwen Wang, Xiaoqi Li, Muhe Cai, Yan Shen, Juan Zhu, Guanghui Ren, Hao Dong)</author>
      <guid isPermaLink="false">2509.17125v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Persistence Spheres: Bi-continuous Representations of Persistence Diagrams</title>
      <link>http://arxiv.org/abs/2509.16999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为持久球面的新型持久图功能表示方法，它提供了双连续映射，确保了稳定性和几何保真性，并且可以在线性空间中高效计算和并行化。&lt;h4&gt;背景&lt;/h4&gt;现有的持久图嵌入方法(如持久图像、景观或核方法)存在某些局限性，需要一种能更好地保持Wasserstein几何特性的表示方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的持久图表示方法，能够在理论上最优地确保稳定性和几何保真性，同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出持久球面，这是一种关于1-Wasserstein距离的Lipschitz连续映射，并在其图像上具有连续逆。推导了持久球面的显式公式，使其能够高效计算和并行化。&lt;h4&gt;主要发现&lt;/h4&gt;持久球面在各类回归和分类任务(包括函数数据、时间序列、图、网格和点云)中，相比持久图像、持久景观和切片Wasserstein核，提供了最先进或具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;持久球面是一种在理论上最优且计算高效的持久图表示方法，能够最接近地反映持久图的Wasserstein几何特性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了持久球面，这是一种持久图的新型功能表示。与现有的嵌入(如持久图像、景观或核方法)不同，持久球面提供了一个双连续映射：它们关于1-Wasserstein距离是Lipschitz连续的，并且在它们的图像上具有连续逆。这以理论上最优的方式确保了稳定性和几何保真性，使持久球面成为在线性空间中最接近PDs的Wasserstein几何的表示。我们推导了持久球面的显式公式，表明它们可以高效计算和并行化，且开销最小。经验性地，我们在涉及函数数据、时间序列、图、网格和点云的各种回归和分类任务上评估了它们。在这些基准测试中，持久球面相比持久图像、持久景观和切片Wasserstein核，始终提供了最先进或具有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce persistence spheres, a novel functional representation ofpersistence diagrams. Unlike existing embeddings (such as persistence images,landscapes, or kernel methods), persistence spheres provide a bi-continuousmapping: they are Lipschitz continuous with respect to the 1-Wassersteindistance and admit a continuous inverse on their image. This ensures, in atheoretically optimal way, both stability and geometric fidelity, makingpersistence spheres the representation that most closely mirrors theWasserstein geometry of PDs in linear space. We derive explicit formulas forpersistence spheres, showing that they can be computed efficiently andparallelized with minimal overhead. Empirically, we evaluate them on diverseregression and classification tasks involving functional data, time series,graphs, meshes, and point clouds. Across these benchmarks, persistence spheresconsistently deliver state-of-the-art or competitive performance compared topersistence images, persistence landscapes, and the sliced Wasserstein kernel.</description>
      <author>example@mail.com (Matteo Pegoraro)</author>
      <guid isPermaLink="false">2509.16999v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models</title>
      <link>http://arxiv.org/abs/2509.16832v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submit to ISPRS Journal of Photogrammetry and Remote Sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为L2M-Reg的基于平面的精细配准方法，用于解决LiDAR点云与语义3D城市模型在单个建筑层面的精确配准问题，特别考虑了模型不确定性带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云与语义3D城市模型的精确配准是城市数字孪生和下游任务（如数字建设、变化检测和模型优化）的基础，但在细节级别2(LoD2)的语义3D城市模型中存在泛化不确定性，使得单个建筑层面的精确配准仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理模型不确定性的LiDAR到模型配准方法，特别是在单个建筑层面实现精确配准。&lt;h4&gt;方法&lt;/h4&gt;L2M-Reg是一种基于平面的精细配准方法，包含三个关键步骤：建立可靠的平面对应关系，构建伪平面约束的Gauss-Helmert模型，以及自适应估计垂直平移。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集上的实验表明，L2M-Reg比现有的基于ICP和平面的方法更准确且计算效率更高。&lt;h4&gt;结论&lt;/h4&gt;L2M-Reg为存在模型不确定性时的LiDAR到模型配准提供了新颖的建筑级解决方案。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR（光探测和测距）点云与语义3D城市模型之间的精确配准是城市数字孪生的基本主题，也是数字建设、变化检测和模型优化等下游任务的先决条件。然而，在单个建筑层面实现精确的LiDAR到模型配准仍然具有挑战性，特别是在细节级别2(LoD2)的语义3D城市模型中存在泛化不确定性。本文通过提出L2M-Reg（一种基于平面的精细配准方法）来解决这一差距，该方法明确考虑了模型不确定性。L2M-Reg包含三个关键步骤：建立可靠的平面对应关系，构建伪平面约束的Gauss-Helmert模型，以及自适应估计垂直平移。在三个真实世界数据集上的实验表明，L2M-Reg比现有的基于ICP和平面的方法更准确且计算效率更高。总体而言，L2M-Reg为存在模型不确定性时的LiDAR到模型配准提供了新颖的建筑级解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决激光雷达点云与LoD2语义3D城市模型在建筑级别精确配准的问题，特别关注LoD2模型中存在的固有不确定性（建筑基座与立面间的水平偏移）。这个问题在数字孪生、数字施工、变化检测等应用中至关重要，因为这种不确定性在城市级别可被忽略，但在建筑级别的高精度应用中会显著影响配准精度，导致后续分析出现系统误差。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了LoD2模型的不确定性来源（建筑轮廓对应墙基而非立面），指出建筑基座更适合与建模墙面建立对应。方法设计借鉴了现有基于平面的配准思路和Gauss-Helmert模型，但针对不确定性问题进行了创新。作者设计了三个关键步骤：可靠平面对应建立、伪平面约束Gauss-Helmert模型和自适应垂直平移估计，形成完整的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是明确考虑LoD2模型的不确定性，利用建筑基座区域（而非整个立面）建立对应关系，采用2D-3D解耦参数估计策略，并利用模型语义信息提高效率。整体流程包括：1)数据预处理（点云与墙面关联）；2)可靠平面对应建立（自动定位基座区域并提取平面段）；3)伪平面约束Gauss-Helmert模型（解耦参数估计）；4)自适应垂直平移估计（使用DTM数据计算垂直偏移）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个明确考虑LoD2模型不确定性的建筑级别配准方法；2)2D-3D解耦变换参数估计策略，避免低质量地面数据影响；3)轻量级平面对应策略，直接利用模型语义信息。相比之前工作，L2M-Reg不再假设模型无误差，选择性处理建筑基座点而非整个点云，简化流程并提高效率，同时通过伪平面约束策略解决地面数据质量问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; L2M-Reg首次明确处理了LoD2模型的固有不确定性，通过创新的平面对应关系和2D-3D解耦参数估计策略，实现了建筑级别激光雷达点云与语义3D城市模型的高效高精度配准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate registration between LiDAR (Light Detection and Ranging) pointclouds and semantic 3D city models is a fundamental topic in urban digitaltwinning and a prerequisite for downstream tasks, such as digital construction,change detection and model refinement. However, achieving accurateLiDAR-to-Model registration at individual building level remains challenging,particularly due to the generalization uncertainty in semantic 3D city modelsat the Level of Detail 2 (LoD2). This paper addresses this gap by proposingL2M-Reg, a plane-based fine registration method that explicitly accounts formodel uncertainty. L2M-Reg consists of three key steps: establishing reliableplane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,and adaptively estimating vertical translation. Experiments on three real-worlddatasets demonstrate that L2M-Reg is both more accurate and computationallyefficient than existing ICP-based and plane-based methods. Overall, L2M-Regprovides a novel building-level solution regarding LiDAR-to-Model registrationwhen model uncertainty is present.</description>
      <author>example@mail.com (Ziyang Xu, Benedikt Schwab, Yihui Yang, Thomas H. Kolbe, Christoph Holst)</author>
      <guid isPermaLink="false">2509.16832v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided Grouping-Feature Coordination</title>
      <link>http://arxiv.org/abs/2509.16639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级的分组-特征协调模块(GF-Core)和针对点云输入的自监督预训练策略，通过模块集成而非结构修改显著提升了点云分析模型的性能。&lt;h4&gt;背景&lt;/h4&gt;点云分析已发展出多样化网络架构，但现有工作主要关注结构设计创新，传统基于点的架构（通过顺序采样、分组和特征提取处理原始点）的潜力未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;通过战略模块集成而非结构修改来释放点云分析模型的显著性能提升。&lt;h4&gt;方法&lt;/h4&gt;提出分组-特征协调模块(GF-Core)，一个轻量级可分离组件，同时调节分组层和特征提取层实现更精细特征聚合；引入专门针对基于点输入的自监督预训练策略，增强模型在复杂点云分析场景中的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40数据集上，将基线网络准确率提升至94.0%，匹配先进框架性能同时保持架构简单；在ScanObjectNN数据集三个变体上分别获得2.96%、6.34%和6.32%的改进。&lt;h4&gt;结论&lt;/h4&gt;通过模块集成而非结构修改可显著提升点云分析模型性能。&lt;h4&gt;翻译&lt;/h4&gt;点云分析已随着多样化的网络架构而发展，而现有工作主要集中在引入新颖的结构设计上。然而，传统的基于点的架构——通过顺序采样、分组和特征提取层处理原始点——显示出未被充分利用的潜力。我们注意到，通过战略模块集成而非结构修改可以释放显著的性能提升。在本文中，我们提出了分组-特征协调模块(GF-Core)，这是一个轻量级的可分离组件，同时调节分组层和特征提取层以实现更精细的特征聚合。此外，我们引入了一种专门针对基于点输入的自监督预训练策略，以增强模型在复杂点云分析场景中的鲁棒性。在ModelNet40数据集上，我们的方法将基线网络提升到94.0%的准确率，在保持架构简单的同时匹配了先进框架的性能。在ScanObjectNN数据集的三个变体上，我们分别获得了2.96%、6.34%和6.32%的改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云分析中传统点云网络架构的潜力未被充分利用的问题。现有研究主要关注引入新的结构设计，而传统的基于点的架构（通过顺序采样、分组和特征提取层处理原始点）却显示出未被充分利用的潜力。这个问题很重要，因为点云是表示3D几何形状的重要数据形式，在自动驾驶、机器人、增强现实等领域有广泛应用。简单而有效的架构对于实际应用至关重要，因为它可以减少计算资源需求，提高推理速度，同时论文表明通过战略性地整合模块而不是修改结构，可以释放传统架构的巨大性能提升。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到现有的点云处理框架存在两个基本限制：特征聚合过程中max-pooling导致的信息损失，以及固定k-NN分组引入的次优邻域选择。作者重新审视了经典点云框架的设计理念，认为这些框架保留了原始点输入中的几何关系，具有巨大的未开发潜力。作者借鉴了图注意力网络（GAT）和GATv2的思想，将其应用于点云分析，并借鉴了自监督学习中的对比学习思想，设计了基于分组扰动的自监督预训练策略。最终，作者设计了一个轻量级的可分离组件——分组-特征协调模块（GF-Core），同时调节分组层和特征提取层，以实现更细粒度的特征聚合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过注意力机制协调邻域分组和特征聚合，同时利用自监督预训练增强模型鲁棒性。具体实现流程包括：1) 输入点云经过下采样；2) 特征提取阶段：计算中心点和邻接点的特征差异和几何位移，使用MLP转换特征差异，计算注意力权重，通过softmax归一化，使用加权融合聚合邻域特征；3) 分组阶段：计算权重矩阵W，连接层次化权重矩阵，融合特征距离和注意力权重，选择top-k最相关的邻接点；4) 重复上述过程多层；5) 通过下游任务网络进行分类或分割。自监督预训练则通过对分组应用协调的点级扰动和掩码操作，然后在扰动和原始点云之间进行特征比较，使用Barlow Twins损失计算特征一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 分组-特征协调模块（GF-Core）：同时调节分组层和特征提取层，通过注意力机制协调邻域选择和特征聚合；2) 自监督预训练策略：基于分组扰动的自监督预训练，结合对比学习和重建方法的优点；3) 轻量级设计：使用低秩近似减少计算复杂度，模块化设计可集成到任何基于点的架构。相比之前的工作，不同之处在于：与Transformer和Mamba等复杂架构相比，保留了传统点云架构的简单性，同时实现了相当的性能；与其他注意力机制相比，同时考虑了几何和特征信息，将注意力权重矩阵传播到后续分组层；与其他自监督学习方法相比，提出了专门针对点云输入的自监督预训练策略，结合了变形感知对比目标和掩码重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种轻量级的分组-特征协调模块和基于分组扰动的自监督预训练策略，通过协调邻域分组和特征聚合释放了传统点云网络的隐藏潜力，在保持架构简单性的同时实现了与复杂架构相当的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud analysis has evolved with diverse network architectures, whileexisting works predominantly focus on introducing novel structural designs.However, conventional point-based architectures - processing raw points throughsequential sampling, grouping, and feature extraction layers - demonstrateunderutilized potential. We notice that substantial performance gains can beunlocked through strategic module integration rather than structuralmodifications. In this paper, we propose the Grouping-Feature CoordinationModule (GF-Core), a lightweight separable component that simultaneouslyregulates both grouping layer and feature extraction layer to enable morenuanced feature aggregation. Besides, we introduce a self-supervisedpretraining strategy specifically tailored for point-based inputs to enhancemodel robustness in complex point cloud analysis scenarios. On ModelNet40dataset, our method elevates baseline networks to 94.0% accuracy, matchingadvanced frameworks' performance while preserving architectural simplicity. Onthree variants of the ScanObjectNN dataset, we obtain improvements of 2.96%,6.34%, and 6.32% respectively.</description>
      <author>example@mail.com (Shangzhuo Xie, Qianqian Yang)</author>
      <guid isPermaLink="false">2509.16639v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning</title>
      <link>http://arxiv.org/abs/2509.16532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为NoReal3D的新框架，通过3DStructureFormer模块将单目图像转换为伪点云特征，解决了3D点云获取成本高的问题，同时实现了与3D点云方法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的机器人操作领域受到广泛关注，2D图像和3D点云是两种主要的学习范式。研究表明3D点云方法在性能和泛化能力上优于2D图像方法，但3D点云获取成本高，限制了其扩展性和实际应用。&lt;h4&gt;目的&lt;/h4&gt;解决3D点云获取成本高的问题，开发一种能够保留3D信息但降低获取成本的方法，使机器人能够更好地理解3D空间结构。&lt;h4&gt;方法&lt;/h4&gt;提出NoReal3D框架，包含3DStructureFormer可学习3D感知模块，将单目图像转换为几何上有意义的伪点云特征，并与2D编码器输出特征融合。设计伪点云编码器保留几何和拓扑结构，研究不同特征融合策略的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;生成的伪点云能够保留几何和拓扑结构；提出的框架可以在没有实际点云数据的情况下，实现与3D点云方法相当的性能；该框架增强了机器人对3D空间结构的理解。&lt;h4&gt;结论&lt;/h4&gt;NoReal3D框架成功解决了3D点云获取成本高的问题，通过将单目图像转换为伪点云特征，实现了与3D点云方法相当的性能，为基于视觉的机器人操作提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近，基于视觉的机器人操作受到了广泛关注并取得了显著进展。基于2D图像和基于3D点云的策略学习是该领域的两种主要范式，最近的研究表明，后者在前者的策略性能和泛化能力方面持续优于前者，从而强调了3D信息的价值和意义。然而，基于3D点云的方法面临高数据获取成本的重大挑战，限制了其可扩展性和实际部署。为了解决这个问题，我们提出了一个新颖的框架NoReal3D：它引入了3DStructureFormer，这是一个可学习的3D感知模块，能够将单目图像转换为几何上有意义的伪点云特征，与2D编码器输出特征有效融合。特别地，生成的伪点云保留了几何和拓扑结构，因此我们设计了伪点云编码器来保持这些特性，使其非常适合我们的框架。我们还研究了不同特征融合策略的有效性。我们的框架增强了机器人对3D空间结构的理解，同时完全消除了与3D点云获取相关的大量成本。在各种任务上的广泛实验验证了我们的框架可以在没有实际点云数据的情况下实现与基于3D点云的方法相当的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作学习中3D点云方法的高数据获取成本问题，以及2D图像方法缺乏3D空间感知能力的局限。这个问题很重要，因为3D点云方法虽然性能更好但需要昂贵传感器，限制了实际应用；而2D图像方法成本低但难以精确理解空间结构，阻碍了机器人在复杂操作场景中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前两种主要范式（2D图像和3D点云）的优缺点，注意到2D图像本身包含3D结构线索，进而思考能否仅从2D图像理解3D空间关系。他们设计了3DStructureFormer模块，借鉴了深度估计模型、针孔相机模型和现有视觉骨干网络，但创新性地将这些技术组合成一种新方法，将2D图像转换为保持拓扑结构的伪点云，并与原始图像特征融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是仅使用单目RGB图像生成伪3D表示，无需实际3D传感器，从而在保留2D图像低成本优势的同时获得类似3D方法的几何感知能力。整体流程包括：1)使用深度估计模型预测图像深度并生成伪点云；2)将伪点云编码为结构化坐标图并提取几何特征；3)通过跨模态融合模块将2D视觉特征与3D几何特征结合；4)使用融合后的3D增强表示指导机器人操作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出NoReal3D新范式，首次在机器人操作中实现仅从2D图像到3D的提升；2)设计3DStructureFormer模块，生成并编码保持拓扑结构的伪点云；3)发现简单的元素级加法是最佳特征融合策略；4)提出隐式3D感知机制，适应相机参数不确定的情况。相比之前工作，NoReal3D避免了3D传感器的昂贵成本，同时解决了传统跨模态方法的空间信息丢失问题，在多个任务上超越了纯2D方法，部分任务甚至超过了3D基线方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; NoReal3D通过将2D图像转换为几何上有意义的伪3D表示并与原始视觉特征融合，使机器人仅使用普通RGB相机就能达到与3D传感器相当的操作性能，大幅降低了机器人操作学习的硬件成本和实施难度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently,vision-based robotic manipulation has garnered significant attentionand witnessed substantial advancements. 2D image-based and 3D point cloud-basedpolicy learning represent two predominant paradigms in the field, with recentstudies showing that the latter consistently outperforms the former in terms ofboth policy performance and generalization, thereby underscoring the value andsignificance of 3D information. However, 3D point cloud-based approaches facethe significant challenge of high data acquisition costs, limiting theirscalability and real-world deployment. To address this issue, we propose anovel framework NoReal3D: which introduces the 3DStructureFormer, a learnable3D perception module capable of transforming monocular images intogeometrically meaningful pseudo-point cloud features, effectively fused withthe 2D encoder output features. Specially, the generated pseudo-point cloudsretain geometric and topological structures so we design a pseudo-point cloudencoder to preserve these properties, making it well-suited for our framework.We also investigate the effectiveness of different feature fusionstrategies.Our framework enhances the robot's understanding of 3D spatialstructures while completely eliminating the substantial costs associated with3D point cloud acquisition.Extensive experiments across various tasks validatethat our framework can achieve performance comparable to 3D point cloud-basedmethods, without the actual point cloud data.</description>
      <author>example@mail.com (Run Yu, Yangdi Liu, Wen-Da Wei, Chen Li)</author>
      <guid isPermaLink="false">2509.16532v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR</title>
      <link>http://arxiv.org/abs/2509.16346v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ForestGen3D是一种创新的生成模型，能够仅从机载激光雷达数据生成高保真的3D森林结构，通过条件去噪扩散概率模型和几何包含先验，有效解决了大规模三维植被结构测量的挑战，在多种尺度上都能生成与地面激光雷达测量结果高度一致的森林结构。&lt;h4&gt;背景&lt;/h4&gt;生态系统中生物和非生物成分的三维结构在决定生态过程和反馈方面起着关键作用，准确表征三维植被结构对于预测野火、干旱、疾病或大气沉降的影响至关重要，然而大规模测量仍然极其昂贵且往往不可行。&lt;h4&gt;目的&lt;/h4&gt;介绍ForestGen3D，一个新颖的生成式建模框架，该框架仅使用机载激光雷达(ALS)输入即可合成高保真度的3D森林结构。&lt;h4&gt;方法&lt;/h4&gt;ForestGen3D基于条件去噪扩散概率模型(DDPMs)，使用共注册的ALS/TLS数据进行训练，模型学习在有稀疏ALS观测条件下生成类似TLS的3D点云，有效重建被遮挡的林下细节；引入基于ALS观测凸包的几何包含先验，确保生成的结构在空间上保持一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在混合针叶林生态系统的真实世界数据上，从树木、样地到景观尺度评估ForestGen3D，生成的重建在几何相似性和生物物理指标(如树高、DBH、树冠直径和树冠体积)上与TLS参考值高度匹配；在TLS地面真实值不可用的环境中，包含属性可作为生成质量的实用代理。&lt;h4&gt;结论&lt;/h4&gt;ForestGen3D可作为生态建模、野火模拟和结构燃料表征的可扩展工具，特别适用于仅使用ALS的环境。&lt;h4&gt;翻译&lt;/h4&gt;生态系统中生物和非生物成分的三维结构在决定生态过程和反馈方面起着关键作用，无论是自然还是人为干扰的反馈都是如此。预测野火、干旱、疾病或大气沉降的影响取决于对三维植被结构的准确表征，然而大规模测量仍然极其昂贵且往往不可行。我们介绍了ForestGen3D，一种新颖的生成式建模框架，它仅使用机载激光雷达(ALS)输入即可合成高保真度的3D森林结构。ForestGen3D基于条件去噪扩散概率模型(DDPMs)，使用共注册的ALS/TLS(地面激光雷达)数据进行训练。该模型学习在有稀疏ALS观测条件下生成类似TLS的3D点云，有效重建被遮挡的林下细节。为确保生态合理性，我们引入了基于ALS观测凸包的几何包含先验，并提供了理论和经验保证，确保生成的结构在空间上保持一致。我们使用混合针叶林生态系统的真实世界数据，在树木、样地和景观尺度上评估ForestGen3D，结果表明它在几何相似性和生物物理指标(如树高、DBH、树冠直径和树冠体积)上生成的重建与TLS参考值高度匹配。此外，我们证明在TLS地面真实值不可用的环境中，包含属性可作为生成质量的实用代理。我们的研究结果表明，ForestGen3D可作为仅使用ALS环境中的生态建模、野火模拟和结构燃料表征的可扩展工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从空中激光雷达（ALS）数据生成高分辨率的3D森林结构，特别是ALS无法捕捉的林下细节问题。这个问题在现实和研究中非常重要，因为3D森林结构对理解生态过程、预测火灾行为、评估碳储存等至关重要；而现有的TLS数据提供高分辨率但覆盖范围有限，ALS覆盖范围广但无法捕捉林下细节；大规模收集详细3D森林结构数据成本高昂且不可行；现有方法无法捕捉真实的森林空间关系和3D异质性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到ALS和TLS数据具有互补性，提出通过学习生成模型填补ALS数据的盲点。他们采用条件去噪扩散概率模型（DDPM）作为基础架构，借鉴了图像生成领域的成功经验。方法借鉴了点云处理技术（如PointNet++和U-Net架构）、现有的配准方法和树检测算法，并对比了现有的3D生成方法（如PCN、3D-GAN、latent-GAN和PointFlow），但将其创新性地应用于森林结构生成这一特定领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用条件去噪扩散概率模型学习从ALS到TLS的跨域生成，通过几何约束先验确保生成的点云位于ALS点云的凸包内，保证空间一致性。整体流程包括：1)收集配准的ALS/TLS数据对并构建CoLiDAR-Forest3D数据集；2)训练条件DDPM模型学习从ALS输入生成TLS-like 3D结构；3)从随机噪声开始，使用ALS输入作为条件，通过迭代去噪过程生成完整3D森林结构；4)在树、地块和景观尺度上评估生成质量和生物物理指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将条件扩散模型应用于跨域3D森林结构生成；引入几何约束先验确保空间一致性；构建了专门的CoLiDAR-Forest3D数据集；提出期望点包含（EPC）指标作为生成质量代理；在多个尺度验证了方法的可扩展性。相比之前工作，该方法能生成更真实的结构细节，在几何相似性和生物物理指标上表现更好，更注重生态真实性而非视觉效果，充分利用了ALS和TLS的互补优势，并提供了理论保证确保生成结构的空间一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ForestGen3D首次利用条件去噪扩散模型实现了从空中激光雷达到地面激光雷达的跨域3D森林结构生成，有效填补了林下细节的空白，为大规模生态建模和火灾模拟提供了高保真度的工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 3D structure of living and non-living components in ecosystems plays acritical role in determining ecological processes and feedbacks from bothnatural and human-driven disturbances. Anticipating the effects of wildfire,drought, disease, or atmospheric deposition depends on accuratecharacterization of 3D vegetation structure, yet widespread measurement remainsprohibitively expensive and often infeasible. We introduce ForestGen3D, a novelgenerative modeling framework that synthesizes high-fidelity 3D foreststructure using only aerial LiDAR (ALS) inputs. ForestGen3D is based onconditional denoising diffusion probabilistic models (DDPMs) trained onco-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generateTLS-like 3D point clouds conditioned on sparse ALS observations, effectivelyreconstructing occluded sub-canopy detail at scale. To ensure ecologicalplausibility, we introduce a geometric containment prior based on the convexhull of ALS observations and provide theoretical and empirical guarantees thatgenerated structures remain spatially consistent. We evaluate ForestGen3D attree, plot, and landscape scales using real-world data from mixed coniferecosystems, and show that it produces high-fidelity reconstructions thatclosely match TLS references in terms of geometric similarity and biophysicalmetrics, such as tree height, DBH, crown diameter and crown volume.Additionally, we demonstrate that the containment property can serve as apractical proxy for generation quality in settings where TLS ground truth isunavailable. Our results position ForestGen3D as a scalable tool for ecologicalmodeling, wildfire simulation, and structural fuel characterization in ALS-onlyenvironments.</description>
      <author>example@mail.com (Juan Castorena, E. Louise Loudermilk, Scott Pokswinski, Rodman Linn)</author>
      <guid isPermaLink="false">2509.16346v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning</title>
      <link>http://arxiv.org/abs/2509.18041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出NeuS-QA，一种无需训练、即插即用的神经符号管道，用于解决长视频问答(LVQA)中的挑战。该方法通过将问题转换为形式化时间逻辑表达式，构建视频自动机并应用模型检查，仅将逻辑验证的视频片段提交给视觉语言模型，从而提高性能、减少幻觉并实现组合推理。&lt;h4&gt;背景&lt;/h4&gt;长视频问答(LVQA)比传统视觉问答(VQA)更具挑战性，VLMs在长视频的复杂查询中表现不佳。当前方法存在显著标记开销，导致严重下采样，使模型错过细粒度视觉结构和关键时间线索，造成错误答案。&lt;h4&gt;目的&lt;/h4&gt;解决LVQA中的基础性差距，特别是解决现有方法缺乏明确时间表示和无法验证逻辑事件关系的问题，确保采样的上下文实际编码了问题所要求的组合或因果逻辑。&lt;h4&gt;方法&lt;/h4&gt;NeuS-QA包括四个步骤：1)将自然语言问题转换为形式化时间逻辑表达式；2)从帧级语义命题构建视频自动机；3)应用模型检查识别满足问题逻辑要求视频片段；4)仅将逻辑验证片段提交给VLM。&lt;h4&gt;主要发现&lt;/h4&gt;在LongVideoBench和CinePile上的实验表明，NeuS-QA将性能提高了10%以上，特别是在涉及事件排序、因果关系和多步组合推理的问题上表现尤为突出。&lt;h4&gt;结论&lt;/h4&gt;NeuS-QA通过神经符号方法解决了LVQA中的基础性挑战，提高了可解释性，减少了幻觉，并实现了组合推理，同时无需修改或微调模型。&lt;h4&gt;翻译&lt;/h4&gt;长视频问答(LVQA)提出了超越传统视觉问答(VQA)的挑战，而VQA通常仅限于静态图像或短视频片段。虽然当前的视觉语言模型(VLMs)在这些设置中表现良好，但它们在涉及多步时间推理和因果关系的长视频复杂查询中表现不佳。简单的方法是均匀采样帧并将它们与问题一起输入到VLM中，这会产生显著的标记开销，迫使严重下采样。因此，模型常常错过细粒度的视觉结构、微妙的事件转换或关键的时间线索，最终导致错误的答案。为了解决这些局限性，最近的工作探索了查询自适应帧采样、分层关键帧选择和基于代理的迭代查询。然而，这些方法本质上仍然是启发式的：它们缺乏明确的时间表示，无法强制或验证逻辑事件关系。因此，没有正式保证采样的上下文实际编码了问题所要求的组合或因果逻辑。为了解决这些基础性差距，我们引入了NeuS-QA，一种用于LVQA的无需训练、即插即用的神经符号管道。NeuS-QA将自然语言问题转换为形式化时间逻辑表达式，从帧级语义命题构建视频自动机，并应用模型检查来严格识别满足问题逻辑要求视频片段。只有这些经过逻辑验证的片段才会被提交给VLM，从而提高可解释性，减少幻觉，并实现组合推理，而无需修改或微调模型。在LongVideoBench和CinePile上的实验表明，NeuS-QA将性能提高了10%以上，特别是在涉及事件排序、因果关系和多步组合推理的问题上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-Form Video Question Answering (LVQA) poses challenges beyond traditionalvisual question answering (VQA), which is often limited to static images orshort video clips. While current vision-language models (VLMs) perform well inthose settings, they struggle with complex queries in LVQA over long videosinvolving multi-step temporal reasoning and causality. Vanilla approaches,which sample frames uniformly and feed them to a VLM with the question, incursignificant token overhead, forcing severe downsampling. As a result, the modeloften misses fine-grained visual structure, subtle event transitions, or keytemporal cues, ultimately leading to incorrect answers. To address theselimitations, recent works have explored query-adaptive frame sampling,hierarchical keyframe selection, and agent-based iterative querying. However,these methods remain fundamentally heuristic: they lack explicit temporalrepresentations and cannot enforce or verify logical event relationships. As aresult, there are no formal guarantees that the sampled context actuallyencodes the compositional or causal logic demanded by the question. To addressthese foundational gaps, we introduce NeuS-QA, a training-free, plug-and-playneuro-symbolic pipeline for LVQA. NeuS-QA translates a natural languagequestion into a formal temporal logic expression, constructs a video automatonfrom frame-level semantic propositions, and applies model checking torigorously identify video segments satisfying the question's logicalrequirements. Only these logic-verified segments are submitted to the VLM, thusimproving interpretability, reducing hallucinations, and enabling compositionalreasoning without modifying or fine-tuning the model. Experiments onLongVideoBench and CinePile show NeuS-QA improves performance by over 10%,especially on questions involving event ordering, causality, and multi-stepcompositional reasoning.</description>
      <author>example@mail.com (Sahil Shah, S P Sharan, Harsh Goel, Minkyu Choi, Mustafa Munir, Manvik Pasula, Radu Marculescu, Sandeep Chinchali)</author>
      <guid isPermaLink="false">2509.18041v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media</title>
      <link>http://arxiv.org/abs/2509.17991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ReDepress，首个专注于抑郁症复发的临床验证社交媒体数据集，基于认知理论框架开发抑郁症复发检测方法，实现了高精度的复发预测。&lt;h4&gt;背景&lt;/h4&gt;近50%的抑郁症患者面临复发风险，第二次发作后风险增至80%。尽管社交媒体抑郁症检测已受关注，但复发检测因缺乏专业数据集和区分困难而 largely 未被探索。&lt;h4&gt;目的&lt;/h4&gt;创建专注于抑郁症复发的临床验证数据集，并开发基于认知理论的复发检测框架，实现早期复发预测。&lt;h4&gt;方法&lt;/h4&gt;构建包含204个Reddit用户的专业注释数据集，整合注意力偏差、解释偏差、记忆偏差和沉思等认知理论概念到注释和建模中，通过统计分析和机器学习实验验证方法有效性。&lt;h4&gt;主要发现&lt;/h4&gt;认知标记能有效区分复发和非复发组，融入这些特征的模型表现优异，基于transformer的时间模型达到0.86的F1值。&lt;h4&gt;结论&lt;/h4&gt;研究结果在真实文本数据中验证了心理学理论，证实了认知启发的计算方法在早期复发检测中的潜力，为心理健康护理中可扩展、低成本的干预措施奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;近50%的抑郁症患者面临复发的风险。第二次抑郁发作后，风险增加到80%。虽然从社交媒体检测抑郁症已获得相当关注，但由于缺乏精心策划的数据集以及区分复发和非复发用户的困难，抑郁症复发检测在很大程度上仍未被探索。在这项工作中，我们提出了ReDepress，这是第一个专注于复发的临床验证社交媒体数据集，包含204名由心理健康专业人员注释的Reddit用户。与先前的方法不同，我们的框架借鉴了抑郁症的认知理论，将注意力偏差、解释偏差、记忆偏差和沉思等概念纳入注释和建模中。通过统计分析和机器学习实验，我们证明认知标记显著区分了复发和非复发组，并且融入这些特征的模型实现了具有竞争力的性能，基于transformer的时间模型达到0.86的F1值。我们的研究结果在真实世界的文本数据中验证了心理学理论，并强调了认知启发的计算方法在早期复发检测中的潜力，为心理健康护理中可扩展、低成本的干预措施铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Almost 50% depression patients face the risk of going into relapse. The riskincreases to 80% after the second episode of depression. Although, depressiondetection from social media has attained considerable attention, depressionrelapse detection has remained largely unexplored due to the lack of curateddatasets and the difficulty of distinguishing relapse and non-relapse users. Inthis work, we present ReDepress, the first clinically validated social mediadataset focused on relapse, comprising 204 Reddit users annotated by mentalhealth professionals. Unlike prior approaches, our framework draws on cognitivetheories of depression, incorporating constructs such as attention bias,interpretation bias, memory bias and rumination into both annotation andmodeling. Through statistical analyses and machine learning experiments, wedemonstrate that cognitive markers significantly differentiate relapse andnon-relapse groups, and that models enriched with these features achievecompetitive performance, with transformer-based temporal models attaining an F1of 0.86. Our findings validate psychological theories in real-world textualdata and underscore the potential of cognitive-informed computational methodsfor early relapse detection, paving the way for scalable, low-costinterventions in mental healthcare.</description>
      <author>example@mail.com (Aakash Kumar Agarwal, Saprativa Bhattacharjee, Mauli Rastogi, Jemima S. Jacob, Biplab Banerjee, Rashmi Gupta, Pushpak Bhattacharyya)</author>
      <guid isPermaLink="false">2509.17991v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Does Audio Matter for Modern Video-LLMs and Their Benchmarks?</title>
      <link>http://arxiv.org/abs/2509.17901v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, under review. Project page:  https://github.com/naver-ai/LLaVA-AV-SSM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究表明现代多模态大语言模型声称具有视频理解能力，但大多数评估未充分利用音频信息。研究发现音频在一般视频基准测试中贡献有限，但在特定音频敏感任务中至关重要。&lt;h4&gt;背景&lt;/h4&gt;现代多模态大语言模型通常声称具有'视频理解'能力，然而大多数评估使用静音视频或直接丢弃音频。当前视频理解评估中音频的真实作用不明确。&lt;h4&gt;目的&lt;/h4&gt;研究音频对当代视频-大语言模型及其认证基准的实际重要性，探究音频在视频理解任务中的真实价值。&lt;h4&gt;方法&lt;/h4&gt;审计广泛使用的视频理解测试套件；基于LLaVA-OneVision架构添加语音/音频编码器；使用基于Mamba的轻量级状态空间令牌压缩器解决音频令牌爆炸问题；创建新的音频敏感测试子集。&lt;h4&gt;主要发现&lt;/h4&gt;许多视频理解任务甚至可以从单帧图像中解决，使音频变得冗余；音频在最近的视频基准测试中收益有限；但在精心策划的音频敏感子集上，音频具有决定性作用。&lt;h4&gt;结论&lt;/h4&gt;当前学术实践与现实世界期望之间存在日益扩大的差距；提供了用于可扩展音频-视觉视频-LLMs的实用工具；将在https://github.com/naver-ai/LLaVA-AV-SSM上完全开源工作。&lt;h4&gt;翻译&lt;/h4&gt;现代多模态大语言模型通常声称具有'视频理解'能力，然而大多数评估使用静音视频或直接丢弃音频。我们直接提出一个问题：音频对当代视频-大语言模型及其认证基准的实际重要性有多大？我们审计了广泛使用的测试套件，并观察到许多项目甚至可以从单帧图像中解决，使音频在很大程度上变得冗余。基于LLaVA-OneVision架构，我们附加了一个语音/音频编码器(如Whisper)，并分析了音频在何时有帮助，同时使用基于Mamba的轻量级状态空间令牌压缩器解决了音频令牌爆炸问题。我们发现音频在最近的视频基准测试中收益甚微，但在精心策划的音频敏感子集上具有决定性作用。为了实现可靠的评估，我们发布了AVQA-Hard和Music-AVQA-Hard，我们的模型和代码。我们的研究结果揭示了当前学术实践与现实世界期望之间的日益扩大的差距，并为可扩展的音频-视觉视频-大语言模型提供了实用工具。我们将在https://github.com/naver-ai/LLaVA-AV-SSM上完全开源我们的工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern multimodal large language models often claim "video understanding,"yet most evaluations use muted videos or simply discard audio. We ask a directquestion: how much does audio actually matter for contemporary Video-LLMs andthe benchmarks that certify them? We audit widely used suites and observe thatmany items are even solvable from a single frame, rendering audio largelyredundant. Building on LLaVA-OneVision architecture, we attach a speech/audioencoder (e.g., Whisper) and analyze when audio helps, while addressing audiotoken explosion with a lightweight Mamba-based state-space token compressor. Wefind that audio yields minimal gains on recent video benchmarks but is decisiveon curated, audio-sensitive subsets. To enable faithful evaluation, we releaseAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface agrowing gap between current academic practice and real-world expectations, andprovide practical tools for scalable audio-visual Video-LLMs. We will fullyopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.</description>
      <author>example@mail.com (Geewook Kim, Minjoon Seo)</author>
      <guid isPermaLink="false">2509.17901v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors</title>
      <link>http://arxiv.org/abs/2509.17084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了MoCLIP-Lite，一种高效的双流晚期融合框架，用于视频动作识别。该方法结合了CLIP图像编码器的静态特征和轻量级运动向量网络的时序特征，仅训练小型MLP头部，实现了高效率和出色的性能。&lt;h4&gt;背景&lt;/h4&gt;视频动作识别是计算机视觉的基本任务，但现有最先进模型通常计算量大且依赖大量视频预训练。同时，大规模视觉语言模型如CLIP在静态图像上提供强大的零样本能力，而运动向量(MV)可以从压缩视频流中提供高效的时序信息。&lt;h4&gt;目的&lt;/h4&gt;为了结合这些范式的优势，本研究旨在开发一种高效的视频识别方法，利用大型静态模型的强大功能和动态低成本运动线索，同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;MoCLIP-Lite是一个简单的双流晚期融合框架，结合了冻结CLIP图像编码器的特征和轻量级监督网络(在原始MV上训练)的特征。在融合过程中，两个主干网络都保持冻结状态，只训练一个小型多层感知机(MLP)头部，确保极高的效率。&lt;h4&gt;主要发现&lt;/h4&gt;在UCF101数据集上的综合实验表明，该方法达到了89.2%的Top-1准确率，显著优于强零样本基线(65.0%)和仅使用MV的基线(66.5%)。&lt;h4&gt;结论&lt;/h4&gt;该工作为视频理解提供了一个新的、高效的基线，有效连接了大型静态模型和动态、低成本的运动线索，证明了结合不同范式优势的潜力。&lt;h4&gt;翻译&lt;/h4&gt;视频动作识别是计算机视觉中的一个基本任务，但最先进的模型通常计算量大且依赖大量视频预训练。与此同时，像对比语言-图像预训练(CLIP)这样的大规模视觉语言模型在静态图像上提供强大的零样本能力，而运动向量(MV)则直接从压缩视频流中提供高效的时序信息。为了结合这些范式的优势，我们提出了MoCLIP-Lite，一个简单而强大的双流晚期融合框架，用于高效的视频识别。我们的方法将冻结的CLIP图像编码器的特征与在原始MV上训练的轻量级监督网络的特征相结合。在融合过程中，两个主干网络都保持冻结，只训练一个小型多层感知机(MLP)头部，确保极高的效率。通过在UCF101数据集上的全面实验，我们的方法取得了显著的89.2% Top-1准确率，显著优于强大的零样本(65.0%)和仅使用MV(66.5%)基线。我们的工作为视频理解提供了一个新的、高效的基线，有效连接了大型静态模型和动态、低成本的运动线索。我们的代码和模型可在https://github.com/microa/MoCLIP-Lite获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video action recognition is a fundamental task in computer vision, butstate-of-the-art models are often computationally expensive and rely onextensive video pre-training. In parallel, large-scale vision-language modelslike Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shotcapabilities on static images, while motion vectors (MV) provide highlyefficient temporal information directly from compressed video streams. Tosynergize the strengths of these paradigms, we propose MoCLIP-Lite, a simpleyet powerful two-stream late fusion framework for efficient video recognition.Our approach combines features from a frozen CLIP image encoder with featuresfrom a lightweight, supervised network trained on raw MV. During fusion, bothbackbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head istrained, ensuring extreme efficiency. Through comprehensive experiments on theUCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)baselines. Our work provides a new, highly efficient baseline for videounderstanding that effectively bridges the gap between large static models anddynamic, low-cost motion cues. Our code and models are available athttps://github.com/microa/MoCLIP-Lite.</description>
      <author>example@mail.com (Binhua Huang, Nan Wang, Arjun Parakash, Soumyabrata Dev)</author>
      <guid isPermaLink="false">2509.17084v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Time to Revist Exact Match</title>
      <link>http://arxiv.org/abs/2509.16720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for Findings of EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究将时间问答视为数值估计任务，引入TempAnswerQA基准，使用sMAPE和MASE指标评估模型，发现传统精确匹配(EM)评估方法存在局限性，无法区分误差大小，强调了时间问答任务需要专门指标的重要性。&lt;h4&gt;背景&lt;/h4&gt;时间问答是评估大型语言模型时间推理能力的既定方法，预期答案通常为数字形式（如日期或持续时间），但模型响应却使用精确匹配(EM)进行评估，如同普通文本一样，无法区分小错误与大错误。&lt;h4&gt;目的&lt;/h4&gt;评估精确匹配(EM)在时间问答任务中的不足，并探索更适合的评估方法来更准确地衡量模型的时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;将时间问答重新框架为数值估计任务，引入从Test of Time和TempTabQA中提炼的TempAnswerQA基准，所有问题都需要数字化的时间答案，使用对称平均绝对百分比误差(sMAPE)和平均绝对缩放误差(MASE)作为评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用sMAPE发现误差大小与EM是解耦的，低EM的模型仍有低sMAPE，高EM的模型可能有低sMAPE；2) 使用MASE通过真实数据的偏差缩放误差，重新排列了模型排名，揭示了模型对时间领域知识的理解差距，特别是使用合成数据训练的模型；3) 模型最常见的错误是与真实值仅相差±1，sMAPE和MASE比EM更能适当权衡这些错误。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了时间问答任务需要专门指标的重要性，传统的精确匹配方法无法准确评估模型的时间推理能力，应考虑使用更适合的数值评估指标。&lt;h4&gt;翻译&lt;/h4&gt;时间问答是评估大型语言模型时间推理能力的既定方法。预期答案通常是数字形式（如日期或持续时间），但模型响应却使用精确匹配(EM)进行评估，如同普通文本一样，无法区分小错误与大错误。在本研究中，我们将时间问答视为数值估计任务，以评估EM的不足。我们引入了从Test of Time和TempTabQA中提炼的TempAnswerQA基准，所有问题都需要一个数字化的时间答案，使我们能够超越EM来评估模型。我们使用预测指标对称平均绝对百分比误差(sMAPE)和平均绝对缩放误差(MASE)。使用sMAPE，我们发现误差大小和EM是解耦的。具有低EM的模型仍然有低sMAPE（两者都约为20%），而一些模型尽管EM高，但sMAPE却低。使用MASE通过真实数据的偏差来缩放误差，与EM相比重新排列了模型的排名，揭示了模型对时间领域知识的理解差距，特别是当使用合成数据训练时。最后，模型最常见的错误是与真实值仅相差±1。与EM不同，sMAPE和MASE适当地权衡了这些错误。我们的研究结果强调了时间问答任务需要专门指标的重要性。代码和数据可在https://github.com/aauss/temporal-answer-qa获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal question answering is an established method for evaluating temporalreasoning in large language models. Expected answers are often numeric (e.g.,dates or durations), yet model responses are evaluated like regular text withexact match (EM), unable to distinguish small from large errors. In thisinvestigative work, we frame temporal question answering as a numericalestimation task to assess the shortcomings of EM. We introduce TempAnswerQA, abenchmark distilled from Test of Time and TempTabQA, where all questionsrequire a numerical, temporal answer, allowing us to evaluate models beyond EM.We use the forecasting metrics symmetric mean absolute percentage error (sMAPE)and mean absolute scaled error (MASE). With sMAPE, we find that error size andEM are decoupled. Models with low EM still have low sMAPE (both ~20%), and somemodels have high sMAPE despite high EM. Scaling errors by the deviation of theground truth data with MASE reshuffles model rankings compared to EM, revealinggaps in models' understanding of temporal domain knowledge, especially whentrained with synthetic data. Lastly, the models' most frequent error is todeviate by only $\pm1$ from the ground truth. sMAPE and MASE, unlike EM,adequately weight these errors. Our findings underscore the need forspecialised metrics for temporal QA tasks. Code and data are available onhttps://github.com/aauss/temporal-answer-qa.</description>
      <author>example@mail.com (Auss Abbood, Zaiqiao Meng, Nigel Collier)</author>
      <guid isPermaLink="false">2509.16720v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2509.16560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 Findings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CaRe-DPO框架，通过双重组直接偏好优化方法优化字幕生成，提高了文本-视频检索的细粒度性能。&lt;h4&gt;背景&lt;/h4&gt;在文本-视频检索中，辅助字幕常用于增强视频理解，弥合模态差距。虽然多模态大语言模型(MLLMs)的进步实现了强大的零样本字幕生成，但这些字幕往往是通用的，难以区分视觉相似的视频，限制了细粒度检索的应用。此外，传统字幕生成评估指标(如BLEU)不适合需要区分候选的检索任务。&lt;h4&gt;目的&lt;/h4&gt;解决字幕生成和检索评估的问题，提出一个直接使用检索相关性分数优化字幕生成的框架。&lt;h4&gt;方法&lt;/h4&gt;提出CaRe-DPO检索框架，其核心是双重组直接偏好优化(DG-DPO)，通过建模不同视频和字幕组之间的偏好监督字幕生成；同时提出基于MLLM的检索模型，融入角色嵌入以更好地区分具有不同功能角色的文本输入。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验证明，CaRe-DPO通过有效利用辅助知识生成细粒度字幕，显著提高了检索性能。&lt;h4&gt;结论&lt;/h4&gt;CaRe-DPO框架能够有效解决传统字幕生成方法在文本-视频检索中的局限性。&lt;h4&gt;翻译&lt;/h4&gt;在文本-视频检索中，辅助字幕通常用于增强视频理解，弥合不同模态之间的差距。虽然最近多模态大语言模型(MLLMs)的进步实现了强大的零样本字幕生成，但我们观察到这些字幕往往是通用的，难以区分视觉上相似的视频，限制了它们在细粒度检索中的效用。此外，传统的字幕生成方法通常使用语言生成指标(如BLEU)进行评估，这些指标并不适合需要区分候选的检索任务。为此，我们提出了CaRe-DPO，一个直接使用检索相关性分数优化字幕生成的检索框架。其核心是双重组直接偏好优化(DG-DPO)，一种新的学习策略，通过建模不同视频和字幕组之间的偏好来监督字幕生成。此外，我们提出了一个基于MLLM的检索模型，融入角色嵌入以更好地区分具有不同功能角色的文本输入，如辅助字幕和文本查询。通过大量实验，我们证明CaRe-DPO通过有效利用辅助知识生成细粒度字幕，显著提高了检索性能。代码可在https://github.com/mlvlab/CaReDPO获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In text-video retrieval, auxiliary captions are often used to enhance videounderstanding, bridging the gap between the modalities. While recent advancesin multi-modal large language models (MLLMs) have enabled strong zero-shotcaption generation, we observe that such captions tend to be generic andindistinguishable across visually similar videos, limiting their utility forfine-grained retrieval. Moreover, conventional captioning approaches aretypically evaluated using language generation metrics, such as BLEU, which arenot typically tailored for retrieval tasks that require making discriminativedistinctions between candidates. To address this, we propose$\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes captiongeneration using retrieval relevance scores. At its core is Dual-Group DirectPreference Optimization (DG-DPO), a novel learning strategy that supervisescaptioning by modeling preferences across groups of distinct video and captionpairs. In addition, we present an MLLM-based retrieval model that incorporatesrole-embeddings to better distinguish between textual inputs with differentfunctional roles, such as an auxiliary caption and a text query. Throughextensive experiments, we demonstrate that CaRe-DPO significantly enhancesretrieval performance by effectively leveraging auxiliary knowledge to generatefine-grained captions for retrieval. Code is available athttps://github.com/mlvlab/CaReDPO.</description>
      <author>example@mail.com (Ji Soo Lee, Byungoh Ko, Jaewon Cho, Howoong Lee, Jaewoon Byun, Hyunwoo J. Kim)</author>
      <guid isPermaLink="false">2509.16560v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2509.16552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种时空高斯泼溅（ST-GS）框架，通过增强空间和时间建模来改进基于高斯的占用预测方法，解决了多视图空间交互不足和多帧时间一致性有限的问题。&lt;h4&gt;背景&lt;/h4&gt;3D占用预测对以视觉为中心的自动驾驶的全场景理解至关重要，而现有的基于3D语义高斯的方法在计算开销方面有优势，但在多视图空间交互和时间一致性方面存在限制。&lt;h4&gt;目的&lt;/h4&gt;克服现有3D占用预测方法中多视图空间交互不足和多帧时间一致性有限的限制，提高占用预测的准确性和时间连续性。&lt;h4&gt;方法&lt;/h4&gt;提出时空高斯泼溅（ST-GS）框架，包含两个主要部分：1）基于引导的空间聚合策略，采用双模态注意力机制增强高斯表示中的空间交互；2）几何感知的时间融合方案，利用历史上下文改善场景完成的时间连续性。&lt;h4&gt;主要发现&lt;/h4&gt;在大型nuScenes占用预测基准上的实验表明，所提出的方法不仅达到了最先进的性能，而且与现有基于高斯的方法相比，提供了显著更好的时间一致性。&lt;h4&gt;结论&lt;/h4&gt;ST-GS框架通过增强空间和时间建模，有效解决了现有3D占用预测方法的局限性，为自动驾驶场景理解提供了更可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D占用预测对以视觉为中心的自动驾驶的全场景理解至关重要。最近的进展探索了利用3D语义高斯模型来建模占用同时减少计算开销，但它们仍然受限于不足的多视图空间交互和有限的多帧时间一致性。为了克服这些问题，在本文中，我们提出了一种新颖的时空高斯泼溅（ST-GS）框架，以增强现有基于高斯流程中的时空建模。具体来说，我们在双模态注意力机制内开发了一种基于引导的空间聚合策略，以加强高斯表示中的空间交互。此外，我们引入了一种几何感知的时间融合方案，有效利用历史上下文来改善场景完成的时间连续性。在大型nuScenes占用预测基准上的大量实验展示了我们提出的方法不仅实现了最先进的性能，而且与现有的基于高斯的方法相比，提供了明显更好的时间一致性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于视觉的3D语义占用预测中的两个关键挑战：多视角空间交互不足和多帧时间一致性有限。这个问题在现实中非常重要，因为准确的3D场景理解是自动驾驶系统的核心需求，而改进空间交互和时间一致性可以显著提升自动驾驶系统对环境的感知能力，使其更准确地理解周围环境，从而做出更安全的决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于高斯的3D语义占用预测方法的局限性，然后针对空间交互不足问题设计了双模态注意力机制（高斯引导注意力和视角引导注意力），针对时间一致性问题设计了几何感知的时间融合方案。作者借鉴了3D高斯表示在场景重建中的应用、Transformer中的注意力机制、其他时空建模方法以及门控机制等现有工作，但进行了创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增强多视角空间交互和多帧时间一致性来改进基于高斯的3D语义占用预测。整体流程包括：1)输入处理接收环绕视图图像并提取特征；2)初始化3D高斯嵌入；3)通过引导感知的空间聚合策略融合多视角信息；4)利用几何感知的时间融合方案整合历史信息；5)解码高斯嵌入并通过高斯到体素渲染生成语义占用预测；6)计算损失进行模型优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引导感知的空间聚合策略(GISA)，结合高斯引导注意力和视角引导注意力；2)几何感知的时间融合方案(GATF)，利用自车运动信息建立帧间几何对应。相比之前的工作，ST-GS显著增强了多视角空间交互和时间一致性，在保持3D结构信息的同时提高了效率，比基于体素和BEV方法更好地处理复杂和不规则物体，比之前的基于高斯方法实现了更准确的预测和更好的时间稳定性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ST-GS通过引入引导感知的空间聚合策略和几何感知的时间融合方案，显著提升了基于视觉的3D语义占用预测中的多视角空间交互和多帧时间一致性，实现了最先进的性能和更好的时间稳定性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D occupancy prediction is critical for comprehensive scene understanding invision-centric autonomous driving. Recent advances have explored utilizing 3Dsemantic Gaussians to model occupancy while reducing computational overhead,but they remain constrained by insufficient multi-view spatial interaction andlimited multi-frame temporal consistency. To overcome these issues, in thispaper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) frameworkto enhance both spatial and temporal modeling in existing Gaussian-basedpipelines. Specifically, we develop a guidance-informed spatial aggregationstrategy within a dual-mode attention mechanism to strengthen spatialinteraction in Gaussian representations. Furthermore, we introduce ageometry-aware temporal fusion scheme that effectively leverages historicalcontext to improve temporal continuity in scene completion. Extensiveexperiments on the large-scale nuScenes occupancy prediction benchmark showcasethat our proposed approach not only achieves state-of-the-art performance butalso delivers markedly better temporal consistency compared to existingGaussian-based methods.</description>
      <author>example@mail.com (Xiaoyang Yan, Muleilan Pei, Shaojie Shen)</author>
      <guid isPermaLink="false">2509.16552v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead</title>
      <link>http://arxiv.org/abs/2509.16421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025, 32 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Aha是一个自回归的精彩片段检测框架，能够实时预测视频帧与自然语言任务的相关性，无需访问未来帧，在标准基准测试上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;实时理解连续视频流对在高风险环境中运行的智能体（如自动驾驶车辆、监控无人机、灾难响应机器人）至关重要，但现有方法不适合在线或流媒体场景。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实时处理视频流的框架，支持逐步推理和实时决策，而不需要访问整个视频。&lt;h4&gt;方法&lt;/h4&gt;Aha利用多模态视觉语言模型和轻量级解耦头，在大型精选数据集上训练，并引入Dynamic SinkCache机制实现在无限长度流中的恒定内存使用。&lt;h4&gt;主要发现&lt;/h4&gt;Aha在TVSum上比之前的离线方法高出+5.9%，在Mr.Hisum上高出+8.3%（mAP），展示了其在机器人应用中的潜力。&lt;h4&gt;结论&lt;/h4&gt;Aha可作为下游规划和长期理解的实时推理模块，有效支持智能体在需要实时决策的环境中的操作。&lt;h4&gt;翻译&lt;/h4&gt;实时理解连续视频流对在高风险环境中运行的智能体（包括自动驾驶车辆、监控无人机和灾难响应机器人）至关重要。然而，大多数现有的视频理解和精彩片段检测方法假设在推理时可以访问整个视频，使它们不适合在线或流媒体场景。特别是，当前模型针对离线摘要优化，无法支持实时决策所需的逐步推理。我们引入了Aha，一个自回归的精彩片段检测框架，预测每个视频帧与自然语言描述的任务的相关性。在不访问未来视频帧的情况下，Aha利用多模态视觉语言模型和轻量级解耦头，在一个人为中心的视频标签大型精选数据集上进行训练。为了实现可扩展性，我们引入了Dynamic SinkCache机制，可以在无限长度的流中实现恒定内存使用，同时在标准基准测试中不降低性能。这鼓励隐藏表示捕获高级任务目标，实现对自然语言任务的信息量、相关性和不确定性的有效帧级排序。Aha在精彩片段检测基准测试上实现了最先进的性能，甚至在TVSum上比之前的离线、全上下文方法和视频语言模型高出+5.9%，在Mr.Hisum上高出+8.3%（mAP，平均精度均值）。我们探索了Aha在给定面向任务的自然语言输入和连续的以机器人为中心的视频的情况下，用于现实世界机器人应用的潜力。两项实验都证明了Aha作为实时推理模块的有效性，用于下游规划和长期理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time understanding of continuous video streams is essential forintelligent agents operating in high-stakes environments, including autonomousvehicles, surveillance drones, and disaster response robots. Yet, most existingvideo understanding and highlight detection methods assume access to the entirevideo during inference, making them unsuitable for online or streamingscenarios. In particular, current models optimize for offline summarization,failing to support step-by-step reasoning needed for real-time decision-making.We introduce Aha, an autoregressive highlight detection framework that predictsthe relevance of each video frame against a task described in natural language.Without accessing future video frames, Aha utilizes a multimodalvision-language model and lightweight, decoupled heads trained on a large,curated dataset of human-centric video labels. To enable scalability, weintroduce the Dynamic SinkCache mechanism that achieves constant memory usageacross infinite-length streams without degrading performance on standardbenchmarks. This encourages the hidden representation to capture high-leveltask objectives, enabling effective frame-level rankings for informativeness,relevance, and uncertainty with respect to the natural language task. Ahaachieves state-of-the-art (SOTA) performance on highlight detection benchmarks,surpassing even prior offline, full-context approaches and video-languagemodels by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).We explore Aha's potential for real-world robotics applications given atask-oriented natural language input and a continuous, robot-centric video.Both experiments demonstrate Aha's potential effectiveness as a real-timereasoning module for downstream planning and long-horizon understanding.</description>
      <author>example@mail.com (Aiden Chang, Celso De Melo, Stephanie M. Lukin)</author>
      <guid isPermaLink="false">2509.16421v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Estimating Clinical Lab Test Result Trajectories from PPG using Physiological Foundation Model and Patient-Aware State Space Model -- a UNIPHY+ Approach</title>
      <link>http://arxiv.org/abs/2509.16345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出UNIPHY+Lab框架，通过结合PPG基础模型和患者感知的Mamba模型，实现从常规PPG监测中连续、个性化估计实验室值，为重症监护中的无创生化监测提供了新途径。&lt;h4&gt;背景&lt;/h4&gt;临床实验室检测提供必要的生化测量，但受限于间歇性和侵入性采样。而PPG是一种在ICU中无创、连续记录的信号，能反映心血管动态变化，可作为潜在生理变化的代理指标。&lt;h4&gt;目的&lt;/h4&gt;提出UNIPHY+Lab框架，解决三个挑战：(1)捕捉实验室值的长期时间趋势，(2)考虑患者特定的基线变化，(3)对相互关联的生物标志物进行多任务估计。&lt;h4&gt;方法&lt;/h4&gt;UNIPHY+Lab框架结合大规模PPG基础模型进行局部波形编码，以及患者感知的Mamba模型进行长程时间建模，使用FiLM调制的初始状态处理患者特定的基线变化。&lt;h4&gt;主要发现&lt;/h4&gt;在两个ICU数据集上评估该方法，预测五个关键实验室测试。结果显示，在MAE、RMSE和R平方方面，大多数估计目标比LSTM和前向传播基线有显著改进。&lt;h4&gt;结论&lt;/h4&gt;这项工作证明了从常规PPG监测中连续、个性化估计实验室值的可行性，为重症护理中的无创生化监测提供了途径。&lt;h4&gt;翻译&lt;/h4&gt;临床实验室检测为诊断和治疗提供必要的生化测量，但受限于间歇性和侵入性采样。相比之下，光电容积脉搏波是一种在重症监护室中无创、连续记录的信号，能反映心血管动态变化，可作为潜在生理变化的代理指标。我们提出UNIPHY+Lab框架，结合大规模PPG基础模型进行局部波形编码，以及患者感知的Mamba模型进行长程时间建模。我们的架构解决了三个挑战：(1)捕捉实验室值的长期时间趋势，(2)通过FiLM调制的初始状态考虑患者特定的基线变化，(3)对相互关联的生物标志物进行多任务估计。我们在两个ICU数据集上评估了我们的方法，用于预测五个关键实验室测试。结果显示，在MAE、RMSE和R平方方面，大多数估计目标比LSTM和前向传播基线有显著改进。这项工作证明了从常规PPG监测中连续、个性化估计实验室值的可行性，为重症护理中的无创生化监测提供了途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clinical laboratory tests provide essential biochemical measurements fordiagnosis and treatment, but are limited by intermittent and invasive sampling.In contrast, photoplethysmogram (PPG) is a non-invasive, continuously recordedsignal in intensive care units (ICUs) that reflects cardiovascular dynamics andcan serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, aframework that combines a large-scale PPG foundation model for local waveformencoding with a patient-aware Mamba model for long-range temporal modeling. Ourarchitecture addresses three challenges: (1) capturing extended temporal trendsin laboratory values, (2) accounting for patient-specific baseline variationvia FiLM-modulated initial states, and (3) performing multi-task estimation forinterrelated biomarkers. We evaluate our method on the two ICU datasets forpredicting the five key laboratory tests. The results show substantialimprovements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$among most of the estimation targets. This work demonstrates the feasibility ofcontinuous, personalized lab value estimation from routine PPG monitoring,offering a pathway toward non-invasive biochemical surveillance in criticalcare.</description>
      <author>example@mail.com (Minxiao Wang, Runze Yan, Carol Li, Saurabh Kataria, Xiao Hu, Matthew Clark, Timothy Ruchti, Timothy G. Buchman, Sivasubramanium V Bhavani, Randall J. Lee)</author>
      <guid isPermaLink="false">2509.16345v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?</title>
      <link>http://arxiv.org/abs/2509.15602v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了TennisTV，首个全面的网球视频理解基准测试，用于评估多模态大语言模型在处理高频、快速运动视频时的表现。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在一般视频理解方面表现出色，但在处理像网球这样的高频、快速运动时存在困难，因为网球回合视频片段短但信息密度高。&lt;h4&gt;目的&lt;/h4&gt;系统评估多模态大语言模型在网球视频这一具有挑战性领域的表现，并提供首个全面的基准测试。&lt;h4&gt;方法&lt;/h4&gt;TennisTV将每个回合建模为连续击球事件的时序序列，使用自动化流程进行筛选和问题生成。涵盖从击球级别到回合级别的9个任务，包括2943个人类验证的问题。评估了17个代表性的多模态大语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;结果显示多模态大语言模型在网球视频理解方面存在明显不足，并得出两个关键见解：帧采样密度应根据任务进行定制和平衡；改进时间锚定对于增强推理能力至关重要。&lt;h4&gt;结论&lt;/h4&gt;针对特定任务调整帧采样密度，并改进时间锚定能力，可以提高多模态大语言模型在网球视频理解方面的表现。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在一般视频理解方面表现出色，但在处理像网球这样的高频、快速运动时存在困难，因为网球回合视频片段短但信息密度高。为了系统评估多模态大语言模型在这个具有挑战性领域的表现，我们提出了TennisTV，这是第一个也是迄今为止最全面的网球视频理解基准测试。TennisTV将每个回合建模为连续击球事件的时序序列，使用自动化流程进行筛选和问题生成。它涵盖了从击球级别到回合级别的9个任务，包括2943个人类验证的问题。通过评估17个代表性的多模态大语言模型，我们提供了首个对网球视频理解的系统评估。结果显示存在明显的不足，并得出两个关键见解：帧采样密度应根据任务进行定制和平衡；改进时间锚定对于增强推理能力至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) excel at general video understandingbut struggle with fast, high-frequency sports like tennis, where rally clipsare short yet information-dense. To systematically evaluate MLLMs in thischallenging domain, we present TennisTV, the first and most comprehensivebenchmark for tennis video understanding. TennisTV models each rally as atemporal-ordered sequence of consecutive stroke events, using automatedpipelines for filtering and question generation. It covers 9 tasks from thestroke level to the rally level and includes 2943 human-verified questions.Evaluating 17 representative MLLMs, we provide the first systematic assessmentof tennis video understanding. Results reveal substantial shortcomings andyield two key insights: (i) frame-sampling density should be tailored andbalanced across tasks, and (ii) improving temporal grounding is essential forstronger reasoning.</description>
      <author>example@mail.com (Zhongyuan Bao, Lejun Zhang)</author>
      <guid isPermaLink="false">2509.15602v2</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>The Narcissus Hypothesis:Descending to the Rung of Illusion</title>
      <link>http://arxiv.org/abs/2509.17999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了'自恋假说'，认为递归对齐过程会导致基础模型产生社会期望偏差，使其更倾向于提供讨人喜欢而非客观的回应。&lt;h4&gt;背景&lt;/h4&gt;现代基础模型不仅反映世界知识，还反映了训练数据中嵌入的人类偏好模式。&lt;h4&gt;目的&lt;/h4&gt;测试递归对齐（通过人类反馈和模型生成的语料库）是否会导致模型产生社会期望偏差，从而优先选择讨人喜欢或奉承的回应而非客观推理。&lt;h4&gt;方法&lt;/h4&gt;使用标准化人格评估和一种新颖的社会期望偏差分数，在31个不同的模型上测试该假设。&lt;h4&gt;主要发现&lt;/h4&gt;结果显示模型显著向社会顺从特质漂移，这对语料库完整性和下游推理的可靠性有深远影响。&lt;h4&gt;结论&lt;/h4&gt;递归偏差可能将高阶推理降低到珀尔因果阶梯的层次，最终导致所谓的'错觉阶梯'，这为基础模型的认识论理解提供了新视角。&lt;h4&gt;翻译&lt;/h4&gt;现代基础模型日益反映的不仅是世界知识，还有训练数据中嵌入的人类偏好模式。我们假设递归对齐——通过人类反馈和模型生成的语料库——会导致社会期望偏差，促使模型优先选择讨人喜欢或奉承的回应而非客观推理。我们称之为'自恋假说'，并使用标准化人格评估和一种新颖的社会期望偏差分数在31个模型上测试了这一假设。结果显示模型显著向社会顺从特质漂移，这对语料库完整性和下游推理的可靠性有深远影响。随后我们提供了一种新颖的认识论解释，说明递归偏差如何可能将高阶推理降低到珀尔因果阶梯的层次，最终导致我们所谓的'错觉阶梯'。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern foundational models increasingly reflect not just world knowledge, butpatterns of human preference embedded in their training data. We hypothesizethat recursive alignment-via human feedback and model-generated corpora-inducesa social desirability bias, nudging models to favor agreeable or flatteringresponses over objective reasoning. We refer to it as the Narcissus Hypothesisand test it across 31 models using standardized personality assessments and anovel Social Desirability Bias score. Results reveal a significant drift towardsocially conforming traits, with profound implications for corpus integrity andthe reliability of downstream inferences. We then offer a novel epistemologicalinterpretation, tracing how recursive bias may collapse higher-order reasoningdown Pearl's Ladder of Causality, culminating in what we refer to as the Rungof Illusion.</description>
      <author>example@mail.com (Riccardo Cadei, Christian Internò)</author>
      <guid isPermaLink="false">2509.17999v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions</title>
      <link>http://arxiv.org/abs/2509.17942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StefaLand是一种新型的生成时空地球基础模型，专注于景观交互，能够准确预测气候驱动的地表响应，并在多种任务上表现优异，同时具有较好的泛化能力和计算效率。&lt;h4&gt;背景&lt;/h4&gt;自然资源管理、减轻自然灾害和满足增长需求需要高精度预测模型。传统影响模型由于观测有限和概念漂移难以进行空间泛化，而最近提出的视觉基础模型需要大量计算资源，不适合动态地表预测。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测气候驱动的地表响应和人类反馈的模型，解决传统模型的空间泛化问题，并创建更适合动态地表预测的模型。&lt;h4&gt;方法&lt;/h4&gt;引入StefaLand，一个基于景观交互的生成时空地球基础模型，使用掩码自编码器主干学习景观属性的深度联合表示，采用位置感知架构融合静态和时间序列输入，使用基于属性的表示减少计算需求，应用残差微调适配器增强迁移能力。&lt;h4&gt;主要发现&lt;/h4&gt;StefaLand在径流、土壤湿度和土壤成分三个任务和四个数据集上的预测表现优于之前的最先进方法，能够泛化到多样化、数据稀缺区域，支持广泛的地表应用，且在学术计算资源上就能优于最先进的基线模型。&lt;h4&gt;结论&lt;/h4&gt;StefaLand是第一个能够显著改进动态地表交互预测并支持多样化下游应用的地球科学地表基础模型，为自然资源管理和灾害减轻提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;管理自然资源、减轻洪水、干旱、野火和山体滑坡，以及满足日益增长的需求，需要能够高精度预测气候驱动的地表响应和人类反馈的模型。传统影响模型，无论是基于过程的、统计的还是机器学习的，都由于观测有限和概念漂移而难以进行空间泛化。最近提出的在卫星图像上训练的视觉基础模型需要大量计算资源，不适合动态地表预测。我们引入了StefaLand，一个以景观交互为中心的生成时空地球基础模型。与之前的最先进方法相比，StefaLand在径流、土壤湿度和土壤成分三个任务和四个数据集上改进了预测结果。结果突显了其在多样化、数据稀缺区域泛化的能力，并支持广泛的地表应用。该模型建立在掩码自编码器主干上，学习景观属性的深度联合表示，采用位置感知架构融合静态和时间序列输入，基于属性的表示大幅减少计算需求，以及残差微调适配器增强迁移能力。虽然受先前方法的启发，但它们与地球科学的整合以及在一个模型中的实现，使其在动态地表任务上能够实现稳健性能。StefaLand可以在学术计算上进行预训练和微调，但仍能优于最先进的基线甚至微调后的视觉基础模型。据我们所知，这是第一个能够显著改进动态地表交互预测并支持多样化下游应用的地球科学地表基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stewarding natural resources, mitigating floods, droughts, wildfires, andlandslides, and meeting growing demands require models that can predictclimate-driven land-surface responses and human feedback with high accuracy.Traditional impact models, whether process-based, statistical, or machinelearning, struggle with spatial generalization due to limited observations andconcept drift. Recently proposed vision foundation models trained on satelliteimagery demand massive compute and are ill-suited for dynamic land-surfaceprediction. We introduce StefaLand, a generative spatiotemporal earthfoundation model centered on landscape interactions. StefaLand improvespredictions on three tasks and four datasets: streamflow, soil moisture, andsoil composition, compared to prior state-of-the-art. Results highlight itsability to generalize across diverse, data-scarce regions and support broadland-surface applications. The model builds on a masked autoencoder backbonethat learns deep joint representations of landscape attributes, with alocation-aware architecture fusing static and time-series inputs,attribute-based representations that drastically reduce compute, and residualfine-tuning adapters that enhance transfer. While inspired by prior methods,their alignment with geoscience and integration in one model enables robustperformance on dynamic land-surface tasks. StefaLand can be pretrained andfinetuned on academic compute yet outperforms state-of-the-art baselines andeven fine-tuned vision foundation models. To our knowledge, this is the firstgeoscience land-surface foundation model that demonstrably improves dynamicland-surface interaction predictions and supports diverse downstreamapplications.</description>
      <author>example@mail.com (Nicholas Kraabel, Jiangtao Liu, Yuchen Bian, Daniel Kifer, Chaopeng Shen)</author>
      <guid isPermaLink="false">2509.17942v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>SingLEM: Single-Channel Large EEG Model</title>
      <link>http://arxiv.org/abs/2509.17920v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SingLEM是一种自监督基础模型，从单通道脑电图(EEG)学习鲁棒、通用表示，本质上与硬件无关。该模型使用混合编码器架构结合卷积层和分层transformer，在大量数据上预训练后，单通道表示性能优于多通道模型。&lt;h4&gt;背景&lt;/h4&gt;当前EEG深度学习模型通常是特定任务的，依赖大量标记数据，限制了适应性。新兴基础模型虽追求广泛适用性，但对固定高密度多通道设置的刚性依赖限制了它们在不同数据集和缺失通道环境中的使用。&lt;h4&gt;目的&lt;/h4&gt;解决现有EEG深度学习模型的局限性，开发一种从单通道EEG学习鲁棒、通用表示的基础模型，使其本质上与硬件无关。&lt;h4&gt;方法&lt;/h4&gt;引入SingLEM，一种自监督基础模型，使用混合编码器架构结合卷积层提取局部特征和分层transformer建模时间依赖关系。在71个公共数据集上预训练，包含超过9,200名受试者和357,000小时单通道EEG数据。&lt;h4&gt;主要发现&lt;/h4&gt;作为固定特征提取器在六个运动想象和认知任务上评估时，单通道表示的综合性能优于领先的多通道基础模型和手工制作的基线，实现了最先进的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;单通道方法可实现最先进的泛化，同时支持细粒度神经生理分析和提高可解释性。源代码和预训练模型可在GitHub获取。&lt;h4&gt;翻译&lt;/h4&gt;当前用于脑电图(EEG)的深度学习模型通常是特定任务的，并依赖大量标记数据，限制了它们的适应性。尽管新兴基础模型追求更广泛的应用性，但它们对固定、高密度多通道设置的刚性依赖限制了它们在异构数据集以及缺失通道或实际低通道环境中的使用。为解决这些限制，我们引入SingLEM，一种从单通道EEG学习鲁棒、通用表示的自监督基础模型，使其本质上与硬件无关。该模型采用混合编码器架构，结合卷积层提取局部特征和分层transformer建模短期和长期时间依赖关系。SingLEM在71个公共数据集上进行了预训练，包含超过9,200名受试者和357,000小时单通道EEG。作为固定特征提取器在六个运动想象和认知任务上评估时，综合单通道表示持续优于领先的多通道基础模型和手工制作的基线。这些结果表明，单通道方法可以实现最先进的泛化，同时实现细粒度神经生理分析并提高可解释性。源代码和预训练模型可在https://github.com/ttlabtuat/SingLEM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current deep learning models for electroencephalography (EEG) are oftentask-specific and depend on large labeled datasets, limiting theiradaptability. Although emerging foundation models aim for broaderapplicability, their rigid dependence on fixed, high-density multi-channelmontages restricts their use across heterogeneous datasets and inmissing-channel or practical low-channel settings. To address theselimitations, we introduce SingLEM, a self-supervised foundation model thatlearns robust, general-purpose representations from single-channel EEG, makingit inherently hardware agnostic. The model employs a hybrid encoderarchitecture that combines convolutional layers to extract local features witha hierarchical transformer to model both short- and long-range temporaldependencies. SingLEM is pretrained on 71 public datasets comprising over 9,200subjects and 357,000 single-channel hours of EEG. When evaluated as a fixedfeature extractor across six motor imagery and cognitive tasks, aggregatedsingle-channel representations consistently outperformed leading multi-channelfoundation models and handcrafted baselines. These results demonstrate that asingle-channel approach can achieve state-of-the-art generalization whileenabling fine-grained neurophysiological analysis and enhancinginterpretability. The source code and pretrained models are available athttps://github.com/ttlabtuat/SingLEM.</description>
      <author>example@mail.com (Jamiyan Sukhbaatar, Satoshi Imamura, Ibuki Inoue, Shoya Murakami, Kazi Mahmudul Hassan, Seungwoo Han, Ingon Chanpornpakdi, Toshihisa Tanaka)</author>
      <guid isPermaLink="false">2509.17920v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Deep Hierarchical Learning with Nested Subspace Networks</title>
      <link>http://arxiv.org/abs/2509.17874v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为嵌套子空间网络(Nested Subspace Networks, NSNs)的新型架构范式，使单个模型能够在推理时根据计算预算动态且细粒度地调整，实现了计算资源和性能之间的灵活权衡。&lt;h4&gt;背景&lt;/h4&gt;大型神经网络通常在固定计算预算下训练，导致性能与效率间的刚性权衡，不适合资源受限或动态环境。现有方法面临两难选择：训练离散专家模型计算成本过高，而动态方法如slimmable网络缺乏应用于大型预训练基础模型的灵活性。&lt;h4&gt;目的&lt;/h4&gt;开发一种架构范式，使单个模型能够在推理时根据不同计算预算动态调整，同时保持对预训练大型模型的适用性。&lt;h4&gt;方法&lt;/h4&gt;重新参数化线性层以满足嵌套子空间属性，使低秩计算是高秩计算的严格子空间；通过感知不确定性的目标函数优化整个模型层次结构，根据不同秩的内在难度平衡其贡献。&lt;h4&gt;主要发现&lt;/h4&gt;NSNs可应用于预训练的大型语言模型，实现平滑且可预测的计算-性能权衡；单个NSN适配模型可实现50%的推理FLOPs减少，同时精度仅下降5个百分点。&lt;h4&gt;结论&lt;/h4&gt;NSNs为创建下一代自适应基础模型提供了强大框架，实现了计算资源与性能之间的灵活平衡。&lt;h4&gt;翻译&lt;/h4&gt;大型神经网络通常在固定的计算预算下进行训练，在性能和效率之间形成了刚性的权衡，这种权衡不适合在资源受限或动态环境中部署。针对此问题的现有方法面临一个艰难的选择：训练离散的专家模型集合在计算上是不可行的，而slimmable网络等动态方法通常缺乏应用于大型预训练基础模型的灵活性。在这项工作中，我们提出了嵌套子空间网络(Nested Subspace Networks, NSNs)，一种新颖的架构范式，使单个模型能够在推理时动态且细粒度地跨越连续的计算预算谱进行调整。我们方法的核心是重新参数化线性层以满足嵌套子空间属性，使得在给定秩下计算的功能是任何更高秩下功能的严格子空间。我们表明，通过一个感知不确定性的目标函数，整个模型层次结构可以联合优化，该函数学习根据不同秩的内在难度平衡其贡献。我们通过经验证明，NSNs可以外科手术式地应用于预训练的大型语言模型，并解锁平滑且可预测的计算-性能前沿。例如，单个NSN适配的模型可以实现推理FLOPs减少50%，而精度仅下降5个百分点。我们的研究结果表明，NSNs是创建下一代自适应基础模型的强大框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large neural networks are typically trained for a fixed computational budget,creating a rigid trade-off between performance and efficiency that isill-suited for deployment in resource-constrained or dynamic environments.Existing approaches to this problem present a difficult choice: training adiscrete collection of specialist models is computationally prohibitive, whiledynamic methods like slimmable networks often lack the flexibility to beapplied to large, pre-trained foundation models. In this work, we proposeNested Subspace Networks (NSNs), a novel architectural paradigm that enables asingle model to be dynamically and granularly adjusted across a continuousspectrum of compute budgets at inference time. The core of our approach is tore-parameterize linear layers to satisfy a nested subspace property, such thatthe function computed at a given rank is a strict subspace of the function atany higher rank. We show that this entire hierarchy of models can be optimizedjointly via an uncertainty-aware objective that learns to balance thecontributions of different ranks based on their intrinsic difficulty. Wedemonstrate empirically that NSNs can be surgically applied to pre-trained LLMsand unlock a smooth and predictable compute-performance frontier. For example,a single NSN-adapted model can achieve a 50% reduction in inference FLOPs withonly a 5 percentage point loss in accuracy. Our findings establish NSNs as apowerful framework for creating the next generation of adaptive foundationmodels.</description>
      <author>example@mail.com (Paulius Rauba, Mihaela van der Schaar)</author>
      <guid isPermaLink="false">2509.17874v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology</title>
      <link>http://arxiv.org/abs/2509.17847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于潜在扩散模型的组织病理学合成数据生成方法，通过结合语义分割图和组织特定视觉裁剪，能够生成真实的异质性组织病理学图像。该方法适用于已标注和未标注数据集，在下游分割任务上表现优异，接近使用真实数据训练的基线性能。&lt;h4&gt;背景&lt;/h4&gt;组织病理学合成数据生成面临独特挑战：保持组织异质性、捕捉细微形态特征以及扩展到未标注数据集。现有方法依赖文本提示或抽象视觉嵌入，难以保留关键的形态细节。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够生成真实异质性组织病理学图像的潜在扩散模型，通过新颖的双重条件方法结合语义分割图和组织特定视觉裁剪，直接整合来自相应语义区域的原始组织裁剪，保留关键的形态细节。&lt;h4&gt;方法&lt;/h4&gt;提出一种双重条件方法，结合语义分割图和组织特定视觉裁剪。对于已标注数据集（Camelyon16、Panda），提取确保20-80%组织异质性的补丁；对于未标注数据（TCGA），引入自监督扩展，使用基础模型嵌入将全幻灯片图像聚类为100种组织类型，自动生成伪语义图进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;1) 该方法合成了高保真图像和精确的区域级注释，在下游分割任务上表现优异；2) 在已标注数据集上训练的模型与使用真实数据训练的模型具有竞争性表现；3) 提示引导合成在Camelyon16上将Frechet距离降低了高达6倍（从430.1降至72.0），在Panda和TCGA上FD低2-3倍；4) 仅在合成数据上训练的DeepLabv3+模型在Camelyon16和Panda上分别达到0.71和0.95的测试IoU，接近真实数据基线（0.72和0.96）。&lt;h4&gt;结论&lt;/h4&gt;通过扩展到11,765个TCGA全幻灯片图像而无需手动标注，该框架为生成多样化、标注的组织病理学数据提供了实用解决方案，解决了计算病理学中的关键瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;组织病理学合成数据生成面临独特挑战：保持组织异质性、捕捉细微形态特征以及扩展到未标注数据集。我们提出了一种潜在扩散模型，通过结合语义分割图和组织特定视觉裁剪的新颖双重条件方法，生成真实的异质性组织病理学图像。与依赖文本提示或抽象视觉嵌入的现有方法不同，我们的方法通过直接整合来自相应语义区域的原始组织裁剪来保留关键的形态细节。对于已标注数据集（即Camelyon16、Panda），我们提取确保20-80%组织异质性的补丁。对于未标注数据（即TCGA），我们引入了一种自监督扩展，使用基础模型嵌入将全幻灯片图像聚类为100种组织类型，自动生成伪语义图进行训练。我们的方法合成了具有精确区域级注释的高保真图像，在下游分割任务上取得卓越性能。在已标注数据集上评估时，在我们的合成数据上训练的模型与在真实数据上训练的模型具有竞争性性能，证明了受控异质性组织生成的实用性。在定量评估中，提示引导合成在Camelyon16上将Frechet距离降低了高达6倍（从430.1降至72.0），在Panda和TCGA上FD低2-3倍。仅在合成数据上训练的下游DeepLabv3+模型在Camelyon16和Panda上分别达到0.71和0.95的测试IoU，接近真实数据基线（0.72和0.96）。通过扩展到11,765个无需手动标注的TCGA全幻灯片图像，我们的框架为生成多样化、标注的组织病理学数据提供了实用解决方案，解决了计算病理学中的关键瓶颈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic data generation in histopathology faces unique challenges:preserving tissue heterogeneity, capturing subtle morphological features, andscaling to unannotated datasets. We present a latent diffusion model thatgenerates realistic heterogeneous histopathology images through a noveldual-conditioning approach combining semantic segmentation maps withtissue-specific visual crops. Unlike existing methods that rely on text promptsor abstract visual embeddings, our approach preserves critical morphologicaldetails by directly incorporating raw tissue crops from corresponding semanticregions. For annotated datasets (i.e., Camelyon16, Panda), we extract patchesensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), weintroduce a self-supervised extension that clusters whole-slide images into 100tissue types using foundation model embeddings, automatically generatingpseudo-semantic maps for training. Our method synthesizes high-fidelity imageswith precise region-wise annotations, achieving superior performance ondownstream segmentation tasks. When evaluated on annotated datasets, modelstrained on our synthetic data show competitive performance to those trained onreal data, demonstrating the utility of controlled heterogeneous tissuegeneration. In quantitative evaluation, prompt-guided synthesis reduces FrechetDistance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lowerFD across Panda and TCGA. Downstream DeepLabv3+ models trained solely onsynthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGAwhole-slide images without manual annotations, our framework offers a practicalsolution for an urgent need for generating diverse, annotated histopathologydata, addressing a critical bottleneck in computational pathology.</description>
      <author>example@mail.com (Saghir Alfasly, Wataru Uegami, MD Enamul Hoq, Ghazal Alabtah, H. R. Tizhoosh)</author>
      <guid isPermaLink="false">2509.17847v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>From Documents to Database: Failure Modes for Industrial Assets</title>
      <link>http://arxiv.org/abs/2509.17834v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures. Artificial Intelligence for Knowledge Acquisition  &amp; Management (AI4KAM) Workshop @ IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于基础模型的交互式系统，用于自动生成工业设备的失效模式与影响分析(FMEA)，显著提高了FMEA创建效率。&lt;h4&gt;背景&lt;/h4&gt;传统的FMEA创建过程耗时且需要大量专业知识，知识密集型内容的创建效率低下。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用基础模型和用户技术文档自动生成FMEA的系统，减少创建时间并提高效率。&lt;h4&gt;方法&lt;/h4&gt;构建一个交互式系统，该系统聚合跨文档的非结构化内容生成FMEA，并将结果存储在关系数据库中。&lt;h4&gt;主要发现&lt;/h4&gt;该系统能够显著减少创建FMEA所需的时间，性能优于传统手动方法。&lt;h4&gt;结论&lt;/h4&gt;基础模型在促进企业资产管理系统创建专门结构化内容方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种使用基础模型和用户提供的技术文档的交互式系统，用于为工业设备生成失效模式与影响分析(FMEA)。我们的系统跨文档聚合非结构化内容以生成FMEA，并将其存储在关系数据库中。利用此工具，创建这种知识密集型内容所需的时间减少，优于传统的手动方法。该演示展示了基础模型促进为企业资产管理系统创建专门结构化内容的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an interactive system using foundation models and user-providedtechnical documents to generate Failure Mode and Effects Analyses (FMEA) forindustrial equipment. Our system aggregates unstructured content acrossdocuments to generate an FMEA and stores it in a relational database.Leveraging this tool, the time required for creation of thisknowledge-intensive content is reduced, outperforming traditional manualapproaches. This demonstration showcases the potential of foundation models tofacilitate the creation of specialized structured content for enterprise assetmanagement systems.</description>
      <author>example@mail.com (Duygu Kabakci-Zorlu, Fabio Lorenzi, John Sheehan, Karol Lynch, Bradley Eck)</author>
      <guid isPermaLink="false">2509.17834v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training</title>
      <link>http://arxiv.org/abs/2509.17816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GLARE方法，用于在无监督和数据高效方式下将视觉基础模型适应到新领域，特别是针对语义分割任务。GLARE通过块级增强和区域一致性约束提升下游性能，实验证明其能有效改进分割效果且计算开销小。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为训练基础模型的核心范式，通过大规模无标签数据集产生具有强泛化能力的表示。现有研究主要关注参数高效的模型适应策略，但将SSL预训练扩展到新领域（尤其是有限数据和密集预测任务）仍不充分。&lt;h4&gt;目的&lt;/h4&gt;解决在无监督和数据高效方式下将视觉基础模型适应到新领域的问题，特别是针对下游语义分割任务。&lt;h4&gt;方法&lt;/h4&gt;提出GLARE（Global Local and Regional Enforcement）持续自监督预训练任务，引入块级增强鼓励局部一致性，并利用区域一致性约束利用数据中的空间语义。使用现有SSL模型权重初始化ViTs，仅更新轻量级UniAdapter模块，保持骨干网络冻结。&lt;h4&gt;主要发现&lt;/h4&gt;在多个不同领域的语义分割基准上，GLARE能够持续改进下游性能，同时保持最小的计算和参数开销。&lt;h4&gt;结论&lt;/h4&gt;GLARE是一种有效的持续自监督预训练方法，能帮助视觉基础模型高效适应新领域，特别是在语义分割任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习（SSL）已经成为通过利用大规模无标签数据集训练基础模型的核心范式，通常产生具有强大泛化能力的表示。这些模型通常在ImageNet等通用数据集上进行预训练，然后通过微调适应到各种下游任务。尽管最近的进展已经探索了参数高效的预训练模型适应策略，但将SSL预训练本身扩展到新领域——特别是在有限数据条件下和密集预测任务方面——仍然研究不足。在这项工作中，我们解决了在无监督和数据高效的方式下将视觉基础模型适应到新领域的问题，特别是针对下游语义分割。我们提出了GLARE（Global Local and Regional Enforcement），一种新的持续自监督预训练任务，旨在增强下游分割性能。GLARE引入了块级增强来鼓励局部一致性，并利用区域一致性约束来利用数据中的空间语义。为了高效的持续预训练，我们使用现有SSL模型的权重初始化Vision Transformers（ViTs），并仅更新轻量级的适配器模块——特别是UniAdapter——同时保持骨干网络的其余部分冻结。在多个不同领域的语义分割基准上的实验表明，GLARE能够持续改进下游性能，同时具有最小的计算和参数开销。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a central paradigm for trainingfoundation models by leveraging large-scale unlabeled datasets, often producingrepresentations with strong generalization capabilities. These models aretypically pre-trained on general-purpose datasets such as ImageNet andsubsequently adapted to various downstream tasks through finetuning. Whilerecent advances have explored parameter-efficient strategies for adaptingpre-trained models, extending SSL pre-training itself to new domains -particularly under limited data regimes and for dense prediction tasks -remains underexplored. In this work, we address the problem of adapting visionfoundation models to new domains in an unsupervised and data-efficient manner,specifically targeting downstream semantic segmentation. We propose GLARE(Global Local and Regional Enforcement), a novel continual self-supervisedpre-training task designed to enhance downstream segmentation performance.GLARE introduces patch-level augmentations to encourage local consistency andincorporates a regional consistency constraint that leverages spatial semanticsin the data. For efficient continual pre-training, we initialize VisionTransformers (ViTs) with weights from existing SSL models and update onlylightweight adapter modules - specifically UniAdapter - while keeping the restof the backbone frozen. Experiments across multiple semantic segmentationbenchmarks on different domains demonstrate that GLARE consistently improvesdownstream performance with minimal computational and parameter overhead.</description>
      <author>example@mail.com (Brown Ebouky, Ajad Chhatkuli, Cristiano Malossi, Christoph Studer, Roy Assaf, Andrea Bartezzaghi)</author>
      <guid isPermaLink="false">2509.17816v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Elucidating the Design Space of FP4 training</title>
      <link>http://arxiv.org/abs/2509.17791v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对基础模型日益增长的计算需求，探讨了4位浮点数(FP4)训练技术，提供了一种统一的FP4训练设计空间视角，并通过理论分析和实证研究确定了最佳性能与开销的权衡配置。&lt;h4&gt;背景&lt;/h4&gt;基础模型计算需求不断增长，推动了低精度训练研究，FP4格式成为提高硬件吞吐量的前沿技术。&lt;h4&gt;目的&lt;/h4&gt;提供FP4训练设计空间的统一视角，引入基于量化梯度的微缩放量化框架，理论分析不同稳定化方法的计算成本。&lt;h4&gt;方法&lt;/h4&gt;构建基于框架的模拟器，在回归、图像分类、扩散模型和语言模型等广泛任务中进行实证研究，系统评估数千种技术组合，识别最佳性能与开销权衡配置。&lt;h4&gt;主要发现&lt;/h4&gt;最佳权衡技术涉及结合Hadamard变换、张量缩放和随机舍入；使用UE5M3作为缩放因子可在范围和精度间提供良好平衡，计算开销可控。&lt;h4&gt;结论&lt;/h4&gt;通过系统评估确定了FP4训练的最佳配置，为高效低精度训练提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;基础模型日益增长的计算需求推动了低精度训练的研究，4位浮点数(FP4)格式成为提高硬件吞吐量的前沿。虽然已提出多种稳定FP4训练的技术，但它们通常呈现孤立的解决方案，且计算开销各不相同且不总是明确。本文旨在提供FP4训练设计空间的统一视角。我们引入了一种全面的、基于量化梯度的微缩放量化框架，可用于理论分析不同稳定化方法在前向和后向传播中的相关计算成本。利用基于此框架构建的模拟器，我们在广泛的机器学习任务（包括回归、图像分类、扩散模型和语言模型）中进行了广泛的实证研究。通过系统评估数千种技术组合，如新型梯度近似、舍入策略和缩放方法，我们确定了哪些配置提供了最有利的性能与开销权衡。我们发现，能够实现最佳权衡的技术涉及仔细结合Hadamard变换、张量缩放和随机舍入。我们还发现，使用UE5M3作为缩放因子可能在范围和精度之间提供良好平衡，且计算开销可控。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing computational demands of foundation models have spurredresearch into low-precision training, with 4-bit floating-point (\texttt{FP4})formats emerging as a frontier for maximizing hardware throughput. Whilenumerous techniques have been proposed to stabilize \texttt{FP4} training, theyoften present isolated solutions with varying, and not always clear,computational overheads. This paper aims to provide a unified view of thedesign space of \texttt{FP4} training. We introduce a comprehensive,quantisation gradient-based framework for microscaling quantization that allowsfor a theoretical analysis of the computational costs associated with differentstabilization methods on both the forward and backward passes. Using asimulator built on this framework, we conduct an extensive empirical studyacross a wide range of machine learning tasks, including regression, imageclassification, diffusion models, and language models. By systematicallyevaluating thousands of combinations of techniques, such as novel gradientapproximations, rounding strategies, and scaling methods, we identify whichconfigurations offer the most favourable performance-to-overhead trade-off. Wefind that the techniques enabling the best trade-off involve carefullycombining Hadamard transformations, tensor scaling and stochastic rounding. Wefurther find that using \texttt{UE5M3} as a scaling factor potentially offers agood compromise between range and precision with manageable computationaloverhead.</description>
      <author>example@mail.com (Robert Hu, Carlo Luschi, Paul Balanca)</author>
      <guid isPermaLink="false">2509.17791v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Development and validation of an AI foundation model for endoscopic diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep learning study</title>
      <link>http://arxiv.org/abs/2509.17660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于人工智能基础模型的食管胃结合部腺癌(EGJA)筛查和分期诊断方法，使用内窥镜图像。研究在中国七家医院进行，包含12,302张来自1,546名患者的图像。模型结合DINOv2和ResNet50提取特征，结果显示其诊断准确率优于其他AI模型和专家医师，并能提高各级医师的诊断能力。&lt;h4&gt;背景&lt;/h4&gt;食管胃结合部腺癌(EGJA)的早期检测对改善患者预后至关重要，但目前的诊断高度依赖于操作者，存在主观性和不一致性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于人工智能基础模型的方法，用于EGJA的筛查和分期诊断，使用内窥镜图像。&lt;h4&gt;方法&lt;/h4&gt;研究采用队列和学习研究方法，在中国七家医院进行多中心研究(2016-2024年)。研究包含12,302张来自1,546名患者的图像，其中8,249张用于模型训练，其余分为保留测试集、外部测试集和前瞻性测试集。模型使用DINOv2和ResNet50提取内窥镜图像的全局外观和局部细节特征。&lt;h4&gt;主要发现&lt;/h4&gt;1) 该模型在三个测试集上准确率分别为0.9256、0.8895和0.8956；2) 表现最好的传统AI模型(ResNet50)准确率分别为0.9125、0.8382和0.8519；3) 专家内镜医师准确率为0.8147；4) 使用模型辅助后，培训医师、合格医师和专家医师的准确率分别从0.7035、0.7350和0.8147提高到0.8497、0.8521和0.8696。&lt;h4&gt;结论&lt;/h4&gt;据所知，该模型是基础模型首次应用于EGJA分期诊断，在诊断准确性和效率方面展现出巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;食管胃结合部腺癌(EGJA)的早期检测对改善患者预后至关重要，但其当前诊断高度依赖于操作者。本文旨在首次尝试开发一种基于人工智能(AI)基础模型的方法，用于EGJA的筛查和分期诊断，使用内窥镜图像。在这项队列和学习研究中，我们在2016年12月28日至2024年12月30日期间在中国七家医院进行了多中心研究。研究包含来自1,546名患者的12,302张图像；其中8,249张用于模型训练，其余图像分为保留测试集、外部测试集和前瞻性测试集用于评估。所提出的模型采用DINOv2和ResNet50来提取内窥镜图像的全局外观和局部细节特征，用于EGJA分期诊断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The early detection of esophagogastric junction adenocarcinoma (EGJA) iscrucial for improving patient prognosis, yet its current diagnosis is highlyoperator-dependent. This paper aims to make the first attempt to develop anartificial intelligence (AI) foundation model-based method for both screeningand staging diagnosis of EGJA using endoscopic images. In this cohort andlearning study, we conducted a multicentre study across seven Chinese hospitalsbetween December 28, 2016 and December 30, 2024. It comprises 12,302 imagesfrom 1,546 patients; 8,249 of them were employed for model training, while theremaining were divided into the held-out (112 patients, 914 images), external(230 patients, 1,539 images), and prospective (198 patients, 1,600 images) testsets for evaluation. The proposed model employs DINOv2 (a vision foundationmodel) and ResNet50 (a convolutional neural network) to extract features ofglobal appearance and local details of endoscopic images for EGJA stagingdiagnosis. Our model demonstrates satisfactory performance for EGJA stagingdiagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and0.8956, respectively. In contrast, among representative AI models, the best one(ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three testsets, respectively; the expert endoscopists achieve an accuracy of 0.8147 onthe held-out test set. Moreover, with the assistance of our model, the overallaccuracy for the trainee, competent, and expert endoscopists improves from0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To ourknowledge, our model is the first application of foundation models for EGJAstaging diagnosis and demonstrates great potential in both diagnostic accuracyand efficiency.</description>
      <author>example@mail.com (Yikun Ma, Bo Li, Ying Chen, Zijie Yue, Shuchang Xu, Jingyao Li, Lei Ma, Liang Zhong, Duowu Zou, Leiming Xu, Yunshi Zhong, Xiaobo Li, Weiqun Ding, Minmin Zhang, Dongli He, Zhenghong Li, Ye Chen, Ye Zhao, Jialong Zhuo, Xiaofen Wu, Lisha Yi, Miaojing Shi, Huihui Sun)</author>
      <guid isPermaLink="false">2509.17660v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data</title>
      <link>http://arxiv.org/abs/2509.17566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First-place solution of the classification track for MICCAI'2025  PDCADxFoundation Challenge&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种利用2D视觉基础模型进行帕金森病自动诊断的方法，通过处理多感兴趣区域并结合监督对比学习，在有限数据集上实现了高准确率诊断。&lt;h4&gt;背景&lt;/h4&gt;帕金森病自动诊断在临床上需求高，但当前方法面临缺乏大型高质量数据集和3D医学模型适应困难的问题，主要挑战包括体素间距和模态不匹配。&lt;h4&gt;目的&lt;/h4&gt;解决帕金森病自动诊断中的数据限制和模型适应问题，探索利用2D视觉基础模型处理3D医学图像的可行性。&lt;h4&gt;方法&lt;/h4&gt;从NM和QSM图像中裁剪多个关键感兴趣区域，通过独立分支处理每个ROI并压缩为token，组合成统一患者表示；使用2D VFMs编码ROI轴向切片，辅助分割头引导特征提取；引入多ROI监督对比学习拉近同类患者表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在MICCAI 2025 PDCADxFoundation挑战赛中排名第一，仅用300个标记扫描数据集训练达到86.0%准确率，比第二名高出5.5%。&lt;h4&gt;结论&lt;/h4&gt;2D视觉基础模型在3D MR图像的临床分析中具有显著潜力，可有效解决医学图像诊断中的数据限制问题。&lt;h4&gt;翻译&lt;/h4&gt;帕金森病的自动诊断在临床上需求很高，由于其普遍性和靶向治疗的重要性。当前临床实践通常依赖于QSM和NM-MRI图像中的诊断生物标志物。然而，缺乏大型高质量数据集使得从头开始训练诊断模型容易过拟合。适应预训练的3D医学模型也具有挑战性，因为医学成像的多样性导致预训练和微调数据体素间距和模态不匹配。在本文中，我们通过利用2D视觉基础模型(VFMs)解决这些挑战。具体来说，我们从NM和QSM图像中裁剪多个关键ROI，通过单独的分支处理每个ROI将其压缩为一个token，然后将这些token组合成统一的患者表示用于分类。在每个分支中，我们使用2D VFMs对3D ROI体积的轴向切片进行编码，并将其融合为ROI token，由辅助分割头引导，将特征提取引向特定的脑核。此外，我们引入了多ROI监督对比学习，通过拉近同类患者的表示同时推开不同类患者的表示来提高诊断性能。我们的方法在MICCAI 2025 PDCADxFoundation挑战赛中获得第一名，在仅300个标记的QSM和NM-MRI扫描数据集上训练，准确率达到86.0%，比第二名方法高出5.5%。这些结果突显了2D VFMs在3D MR图像临床分析中的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决帕金森病自动诊断中的数据限制和3D医学模型适应挑战。由于帕金森病影响全球超过一千万人，早期诊断对改善预后至关重要，但临床实践中缺乏大型高质量数据集，导致从头训练模型容易过拟合，而直接使用预训练3D医学模型又面临体素间距和模态不匹配问题。这个问题在现实中非常重要，因为有效的自动诊断工具可以帮助医生早期识别疾病，提高治疗效果，同时解决医学数据获取困难和隐私保护带来的数据不足挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前帕金森病诊断方法的局限性，特别是数据不足和3D模型适应困难的问题。他们发现2D视觉基础模型(VFMs)相比3D模型有优势，因为自然图像有固定灰度范围且不依赖体素间距。基于此，他们设计了多ROI驱动的分类网络(MRN)，并行处理从脑部关键区域提取的多个ROI。作者借鉴了DINOv2视觉模型用于特征提取，参考了MoCo和监督对比学习设计了mSupMoCo，并在分割任务中借鉴了Dense Prediction Transformers(DPT)和VGGT的方法。这些现有工作的合理组合与创新应用构成了他们的方法基础。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D视觉基础模型处理3D医学图像，通过多ROI并行处理将模型注意力集中在与疾病相关的脑区，结合监督对比学习增强特征判别性。整体流程包括：1)从NM和QSM图像中提取4个关键ROI；2)每个ROI通过RES模块处理，使用DINOv2提取2D切片特征，通过FFA和SFA模块融合特征；3)生成ROI token并组合成患者级表示；4)通过分类头进行疾病分类；5)使用mSupMoCo进行监督对比学习，增强同类患者表示相似性，不同类差异性；6)结合分类、分割和对比损失进行多任务学习训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多ROI驱动的分类网络(MRN)，并行处理多个关键脑区域；2)ROI特征提取和分割(RES)模块，利用2D视觉模型处理3D体积；3)多ROI监督动量对比学习(mSupMoCo)，增强特征判别性。相比之前工作，不同之处在于：不同于传统3D CNN方法，MRN利用2D视觉基础模型处理3D医学图像，避免了体素间距和模态不匹配问题；不同于使用整个脑部图像或单个ROI的方法，MRN同时处理多个ROI，综合利用不同脑区域信息；引入多任务学习结合分类、分割和对比学习，特别适合小数据集场景。在MICCAI 2025挑战赛中，MRN以86.0%准确率获得第一，比第二名高5.5%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了MRN框架，通过巧妙利用2D视觉基础模型处理多ROI的3D医学图像并结合监督对比学习，在有限的帕金森病MR数据上实现了高精度的自动诊断，为医学影像分析提供了新思路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The automatic diagnosis of Parkinson's disease is in high clinical demand dueto its prevalence and the importance of targeted treatment. Current clinicalpractice often relies on diagnostic biomarkers in QSM and NM-MRI images.However, the lack of large, high-quality datasets makes training diagnosticmodels from scratch prone to overfitting. Adapting pre-trained 3D medicalmodels is also challenging, as the diversity of medical imaging leads tomismatches in voxel spacing and modality between pre-training and fine-tuningdata. In this paper, we address these challenges by leveraging 2D visionfoundation models (VFMs). Specifically, we crop multiple key ROIs from NM andQSM images, process each ROI through separate branches to compress the ROI intoa token, and then combine these tokens into a unified patient representationfor classification. Within each branch, we use 2D VFMs to encode axial slicesof the 3D ROI volume and fuse them into the ROI token, guided by an auxiliarysegmentation head that steers the feature extraction toward specific brainnuclei. Additionally, we introduce multi-ROI supervised contrastive learning,which improves diagnostic performance by pulling together representations ofpatients from the same class while pushing away those from different classes.Our approach achieved first place in the MICCAI 2025 PDCADxFoundationchallenge, with an accuracy of 86.0% trained on a dataset of only 300 labeledQSM and NM-MRI scans, outperforming the second-place method by 5.5%.Theseresults highlight the potential of 2D VFMs for clinical analysis of 3D MRimages.</description>
      <author>example@mail.com (Ding Shaodong, Liu Ziyang, Zhou Yijun, Liu Tao)</author>
      <guid isPermaLink="false">2509.17566v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Visual Instruction Pretraining for Domain-Specific Foundation Models</title>
      <link>http://arxiv.org/abs/2509.17562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Visual insTruction Pretraining (ViTP)的新预训练范式，通过利用推理来增强感知，在16个具有挑战性的遥感医学成像基准测试上建立了新的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;现代计算机视觉正在形成一个闭环，其中感知、推理和生成相互强化。然而，这个闭环仍然不完整：高层推理对低级感知特征基础学习的影响尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决高层推理对低级感知特征基础学习影响不足的问题，提出一种新的预训练范式来增强模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出Visual insTruction Pretraining (ViTP)，将Vision Transformer (ViT)骨干网络嵌入到视觉语言模型中，使用从目标下游领域收集的丰富视觉指令数据进行端到端预训练。ViTP由提出的Visual Robustness Learning (VRL)提供支持，强制ViT从稀疏的视觉令牌中学习稳健且领域相关的特征。&lt;h4&gt;主要发现&lt;/h4&gt;在16个具有挑战性的遥感医学成像基准测试上，ViTP在各种下游任务上建立了新的最先进性能，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过直接利用推理来增强感知的ViTP方法有效解决了高层推理对低级感知特征基础学习影响不足的问题，在多个领域取得了优异的性能。&lt;h4&gt;翻译&lt;/h4&gt;现代计算机视觉正在形成一个闭环，其中感知、推理和生成相互强化。然而，这个闭环仍然不完整：高层推理对低级感知特征基础学习的影响尚未得到充分探索。本文通过提出一种在下游领域预训练基础模型的新范式来解决这个问题。我们引入了Visual insTruction Pretraining (ViTP)，这是一种直接利用推理来增强感知的新方法。ViTP将Vision Transformer (ViT)骨干网络嵌入到视觉语言模型中，并使用从目标下游领域收集的丰富视觉指令数据对其进行端到端预训练。ViTP由我们提出的Visual Robustness Learning (VRL)提供支持，该VRL强制ViT从稀疏的视觉令牌中学习稳健且领域相关的特征。在16个具有挑战性的遥感医学成像基准测试上的大量实验表明，ViTP在各种下游任务上建立了新的最先进性能。代码可在github.com/zcablii/ViTP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern computer vision is converging on a closed loop in which perception,reasoning and generation mutually reinforce each other. However, this loopremains incomplete: the top-down influence of high-level reasoning on thefoundational learning of low-level perceptual features is not yetunderexplored. This paper addresses this gap by proposing a new paradigm forpretraining foundation models in downstream domains. We introduce VisualinsTruction Pretraining (ViTP), a novel approach that directly leveragesreasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)backbone within a Vision-Language Model and pretrains it end-to-end using arich corpus of visual instruction data curated from target downstream domains.ViTP is powered by our proposed Visual Robustness Learning (VRL), which compelsthe ViT to learn robust and domain-relevant features from a sparse set ofvisual tokens. Extensive experiments on 16 challenging remote sensing andmedical imaging benchmarks demonstrate that ViTP establishes newstate-of-the-art performance across a diverse range of downstream tasks. Thecode is available at github.com/zcablii/ViTP.</description>
      <author>example@mail.com (Yuxuan Li, Yicheng Zhang, Wenhao Tang, Yimian Dai, Ming-Ming Cheng, Xiang Li, Jian Yang)</author>
      <guid isPermaLink="false">2509.17562v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning</title>
      <link>http://arxiv.org/abs/2509.17552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ICRL的无需训练框架，用于将非文本模态表示整合到基于文本的大型语言模型中，实现了多模态推理而无需微调。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)的性能可通过测试时计算提升，但现有将非文本模态表示整合到LLMs中的方法通常需要额外的昂贵监督训练，限制了快速适应新领域和新模态的能力。&lt;h4&gt;目的&lt;/h4&gt;探索无需训练的方式，将非文本基础模型(FMs)的表示整合到基于文本的LLMs中。&lt;h4&gt;方法&lt;/h4&gt;提出'上下文表示学习'(ICRL)作为概念验证，允许LLMs通过少样本学习自适应地利用非文本模态表示。与传统上下文学习不同，ICRL用FM表示替换文本输入，使LLM能够无需微调就执行多模态推理。&lt;h4&gt;主要发现&lt;/h4&gt;在分子领域的任务套件上评估ICRL，研究了三个核心问题：(i)如何以无需训练的方式将FM表示映射到LLMs中；(ii)哪些因素影响ICRL性能；(iii)ICRL有效性的潜在机制。&lt;h4&gt;结论&lt;/h4&gt;ICRL是首个无需训练框架，用于将非文本模态表示整合到基于文本的LLMs中，为可适应的多模态泛化提供了有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)的显著性能可以通过测试时计算来增强，这依赖于外部工具甚至其他深度学习模型。然而，现有将非文本模态表示整合到LLMs中的方法通常需要额外的昂贵监督训练，限制了快速适应新领域和新模态的能力。在这项工作中，我们探索了无需训练的方式将非文本基础模型(FMs)的表示整合到基于文本的LLMs中的可行性。我们提出'上下文表示学习'(ICRL)作为概念验证，允许LLMs通过少样本学习自适应地利用非文本模态表示。与传统上下文学习不同(后者结合文本-标签对)，ICRL用FM表示替换文本输入，使LLM能够无需微调就执行多模态推理。我们在分子领域的任务套件上评估ICRL，研究了三个核心研究问题：(i)如何以无需训练的方式将FM表示映射到LLMs中；(ii)哪些因素影响ICRL性能；(iii)ICRL有效性的潜在机制是什么。据我们所知，ICRL是首个将非文本模态表示整合到基于文本的LLMs中的无需训练框架，为可适应的多模态泛化提供了有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The remarkable performance of Large Language Models (LLMs) can be enhancedwith test-time computation, which relies on external tools and even other deeplearning models. However, existing approaches for integrating non-text modalityrepresentations into LLMs typically require additional costly supervisedtraining, restricting on-the-fly adaptation to new domains and modalities. Inthis work, we explore the feasibility of integrating representations fromnon-text foundational models (FMs) into text-based LLMs in a training-freemanner. We propose In-Context Representation Learning (ICRL) as aproof-of-concept to allow LLMs to adaptively utilize non-text modalityrepresentations with few-shot learning. Unlike traditional in-context learning,which incorporates text-label pairs, ICRL replaces text inputs with FMrepresentations, enabling the LLM to perform multi-modal inference withoutfine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,investigating three core research questions: (i) how to map FM representationsinto LLMs in a training-free manner, (ii) what factors influence ICRLperformance, and (iii) what mechanisms underlie the effectiveness of ICRL. Tothe best of our knowledge, ICRL is the first training-free framework forintegrating non-text modality representations into text-based LLMs, presentinga promising direction for adaptable, multi-modal generalization.</description>
      <author>example@mail.com (Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak A. Subramanian, Alvin Chan)</author>
      <guid isPermaLink="false">2509.17552v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Training-Free Label Space Alignment for Universal Domain Adaptation</title>
      <link>http://arxiv.org/abs/2509.17452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的通用领域适应方法，利用视觉-语言基础模型的零样本能力，专注于标签空间对齐而非传统的视觉空间对齐，以提高领域适应的稳定性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统通用领域适应方法主要关注视觉空间对齐，但由于内容差异导致的视觉模糊性，限制了其鲁棒性和泛化能力。同时，目标域可能包含未知类别，且标签空间不完全已知，增加了适应的难度。&lt;h4&gt;目的&lt;/h4&gt;克服现有UniDA方法的局限性，解决标签空间未知和噪声标签问题，提高领域适应的稳定性和泛化能力，特别是在目标域包含私有类别的情况下。&lt;h4&gt;方法&lt;/h4&gt;首先使用生成式视觉-语言模型识别目标域中的未知类别，然后提出一种无需训练的标签空间对齐方法，通过过滤和细化域之间的噪声标签来对齐标签空间，最后构建一个整合共享知识和目标私有类信息的通用分类器。&lt;h4&gt;主要发现&lt;/h4&gt;在DomainBed基准测试中，提出的方法显著优于现有UniDA技术，H-score平均提高7.9%，H$^3$-score平均提高6.1%。结合自训练进一步提高了性能，H-score和H$^3$-score额外提高了1.6%。&lt;h4&gt;结论&lt;/h4&gt;通过专注于标签空间对齐而非视觉空间对齐，利用视觉-语言基础模型的零样本能力，可以有效提高通用领域适应的性能和稳定性，特别是在处理未知类别和噪声标签的情况下。&lt;h4&gt;翻译&lt;/h4&gt;通用领域适应将知识从有标签的源域迁移到无标签的目标域，其中标签空间可能不同，目标域可能包含私有类别。先前的UniDA方法主要关注视觉空间对齐，但常常因内容差异导致的视觉模糊性而受限，这限制了它们的鲁棒性和泛化能力。为了克服这一点，我们引入了一种新颖的方法，利用最近视觉-语言基础模型（如CLIP）的强大零样本能力，专注于标签空间对齐以提高适应性稳定性。CLIP可以仅基于标签名称生成任务特定的分类器。然而，将CLIP适应到UniDA具有挑战性，因为标签空间不是完全预先知道的。在本研究中，我们首先利用生成式视觉-语言模型识别目标域中的未知类别。发现的标签中的噪声和语义模糊性——类似于源标签的标签（如同义词、上义词、下义词）——使标签对齐复杂化。为解决此问题，我们提出了UniDA的一种无需训练的标签空间对齐方法。我们的方法通过过滤和细化域之间的噪声标签来对齐标签空间而非视觉空间。然后我们构建了一个通用分类器，整合共享知识和目标私有类信息，从而提高在域移位下的泛化能力。结果表明，在关键DomainBed基准测试中，提出的方法显著优于现有UniDA技术，H-score平均提高7.9%，H$^3$-score平均提高6.1%。此外，结合自训练进一步提高了性能，H-score和H$^3$-score额外提高了1.6%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Universal domain adaptation (UniDA) transfers knowledge from a labeled sourcedomain to an unlabeled target domain, where label spaces may differ and thetarget domain may contain private classes. Previous UniDA methods primarilyfocused on visual space alignment but often struggled with visual ambiguitiesdue to content differences, which limited their robustness andgeneralizability. To overcome this, we introduce a novel approach thatleverages the strong \textit{zero-shot capabilities} of recent vision-languagefoundation models (VLMs) like CLIP, concentrating solely on label spacealignment to enhance adaptation stability. CLIP can generate task-specificclassifiers based only on label names. However, adapting CLIP to UniDA ischallenging because the label space is not fully known in advance. In thisstudy, we first utilize generative vision-language models to identify unknowncategories in the target domain. Noise and semantic ambiguities in thediscovered labels -- such as those similar to source labels (e.g., synonyms,hypernyms, hyponyms) -- complicate label alignment. To address this, we proposea training-free label-space alignment method for UniDA (\ours). Our methodaligns label spaces instead of visual spaces by filtering and refining noisylabels between the domains. We then construct a \textit{universal classifier}that integrates both shared knowledge and target-private class information,thereby improving generalizability under domain shifts. The results reveal thatthe proposed method considerably outperforms existing UniDA techniques acrosskey DomainBed benchmarks, delivering an average improvement of\textcolor{blue}{+7.9\%}in H-score and \textcolor{blue}{+6.1\%} in H$^3$-score.Furthermore, incorporating self-training further enhances performance andachieves an additional (\textcolor{blue}{+1.6\%}) increment in both H- andH$^3$-scores.</description>
      <author>example@mail.com (Dujin Lee, Sojung An, Jungmyung Wi, Kuniaki Saito, Donghyun Kim)</author>
      <guid isPermaLink="false">2509.17452v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation</title>
      <link>http://arxiv.org/abs/2509.17353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS2025 Workshop: Evaluating the Evolving LLM Lifecycle:  Benchmarks, Emergent Abilities, and Scaling&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多智能体强化学习框架，用于自动化放射科报告生成，同时作为放射学生态系统中多模态临床推理的基准和评估环境。&lt;h4&gt;背景&lt;/h4&gt;自动化放射科报告生成面临双重挑战：构建临床可靠的系统和设计严格的评估协议。&lt;h4&gt;目的&lt;/h4&gt;引入一个多智能体强化学习框架，作为放射学生态系统中多模态临床推理的基准和评估环境，以实现可信赖的基于偏差的放射科报告生成。&lt;h4&gt;方法&lt;/h4&gt;提出一个模块化架构，由十个专门智能体组成，负责图像分析、特征提取、报告生成、审查和评估。该框架集成了大型语言模型(LLMs)和大型视觉模型(LVMs)。在公共放射学数据集上使用chatGPT-4o进行了实现，其中LLMs作为评估者与医学放射科医生反馈一起工作。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够在智能体级别（如检测和分割准确性）和共识级别（如报告质量和临床相关性）进行细粒度评估。通过将评估协议与LLM开发生命周期对齐，该基准为可信赖的基于偏差的放射科报告生成奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;所提出的多智能体强化学习框架不仅作为放射科报告生成的基准，还建立了严格的评估协议，有助于构建临床可靠的自动化放射科报告生成系统。&lt;h4&gt;翻译&lt;/h4&gt;自动化放射科报告生成面临双重挑战：构建临床可靠的系统和设计严格的评估协议。我们引入了一个多智能体强化学习框架，作为放射学生态系统中多模态临床推理的基准和评估环境。所提出的框架在一个由十个专门智能体组成的模块化架构中集成了大型语言模型(LLMs)和大型视觉模型(LVMs)，这些智能体负责图像分析、特征提取、报告生成、审查和评估。这种设计能够在智能体级别（例如，检测和分割准确性）和共识级别（例如，报告质量和临床相关性）进行细粒度评估。我们在公共放射学数据集上使用chatGPT-4o进行了实现，其中LLMs作为评估者与医学放射科医生反馈一起工作。通过将评估协议与LLM开发生命周期（包括预训练、微调、对齐和部署）保持一致，所提出的基准为可信赖的基于偏差的放射科报告生成奠定了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automating radiology report generation poses a dual challenge: buildingclinically reliable systems and designing rigorous evaluation protocols. Weintroduce a multi-agent reinforcement learning framework that serves as both abenchmark and evaluation environment for multimodal clinical reasoning in theradiology ecosystem. The proposed framework integrates large language models(LLMs) and large vision models (LVMs) within a modular architecture composed often specialized agents responsible for image analysis, feature extraction,report generation, review, and evaluation. This design enables fine-grainedassessment at both the agent level (e.g., detection and segmentation accuracy)and the consensus level (e.g., report quality and clinical relevance). Wedemonstrate an implementation using chatGPT-4o on public radiology datasets,where LLMs act as evaluators alongside medical radiologist feedback. Byaligning evaluation protocols with the LLM development lifecycle, includingpretraining, finetuning, alignment, and deployment, the proposed benchmarkestablishes a path toward trustworthy deviance-based radiology reportgeneration.</description>
      <author>example@mail.com (Ahmed T. Elboardy, Ghada Khoriba, Essam A. Rashed)</author>
      <guid isPermaLink="false">2509.17353v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Mano Report</title>
      <link>http://arxiv.org/abs/2509.17336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Mano的强大GUI代理，通过多模态基础模型和三阶段训练流程解决了现有GUI自动化方法的局限性，在多个基准测试中取得了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;图形用户界面是人机交互的主要媒介，但自动化GUI交互面临视觉元素复杂、环境动态和多步推理需求的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决基于视觉语言模型的GUI自动化方法存在的分辨率有限、领域不匹配和顺序决策能力不足等问题。&lt;h4&gt;方法&lt;/h4&gt;构建基于在大量网络和计算机系统数据上预训练的多模态基础模型的GUI代理；集成高保真数据生成模拟环境；采用三阶段训练流程（监督微调、离线强化学习和在线强化学习）；包含错误恢复的验证模块。&lt;h4&gt;主要发现&lt;/h4&gt;Mano在Mind2Web和OSWorld等多个GUI基准测试上展示了最先进的性能，在成功率和操作准确性方面取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;将强化学习与视觉语言模型有效集成用于实际GUI代理部署提供了新见解，强调了领域特定数据、迭代训练和整体奖励设计的重要性。&lt;h4&gt;翻译&lt;/h4&gt;图形用户界面(GUI)是人机交互的主要媒介，但自动化GUI交互由于视觉元素的复杂性、动态环境以及多步推理的需求而仍然具有挑战性。基于视觉语言模型(VLMs)的现有方法通常存在分辨率有限、领域不匹配和顺序决策能力不足等问题。为解决这些问题，我们提出Mano，这是一个建立在大量网络和计算机系统数据预训练的多模态基础模型上的强大GUI代理。我们的方法集成了一个新的高保真数据生成模拟环境，一个三阶段训练流程（监督微调、离线强化学习和在线强化学习），以及一个用于错误恢复的验证模块。Mano在多个GUI基准测试（包括Mind2Web和OSWorld）上展示了最先进的性能，在成功率和操作准确性方面取得了显著改进。我们的工作为将强化学习与VLMs有效集成用于实际GUI代理部署提供了新见解，强调了领域特定数据、迭代训练和整体奖励设计的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphical user interfaces (GUIs) are the primary medium for human-computerinteraction, yet automating GUI interactions remains challenging due to thecomplexity of visual elements, dynamic environments, and the need formulti-step reasoning. Existing methods based on vision-language models (VLMs)often suffer from limited resolution, domain mismatch, and insufficientsequential decisionmaking capability. To address these issues, we propose Mano,a robust GUI agent built upon a multi-modal foundation model pre-trained onextensive web and computer system data. Our approach integrates a novelsimulated environment for high-fidelity data generation, a three-stage trainingpipeline (supervised fine-tuning, offline reinforcement learning, and onlinereinforcement learning), and a verification module for error recovery. Manodemonstrates state-of-the-art performance on multiple GUI benchmarks, includingMind2Web and OSWorld, achieving significant improvements in success rate andoperational accuracy. Our work provides new insights into the effectiveintegration of reinforcement learning with VLMs for practical GUI agentdeployment, highlighting the importance of domain-specific data, iterativetraining, and holistic reward design.</description>
      <author>example@mail.com (Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu, Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, Yuyang Chen, Ruiyang Yu, Siran Peng, Menglin Li, Nan Huang, Haitian Wei, Jiawei Yu, Yi Xin, Xilin Zhao, Kai Gu, Ping Jiang, Sifan Zhou, Shuo Wang)</author>
      <guid isPermaLink="false">2509.17336v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking</title>
      <link>http://arxiv.org/abs/2509.17323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code will be made publicly available at  https://github.com/warriordby/DepTR-MOT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DepTR-MOT是一种基于DETR的检测器，通过整合实例级深度信息，解决了传统基于检测的跟踪方法在遮挡和近距离交互场景下的局限性，显著提高了机器人环境中多目标跟踪的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;视觉多目标跟踪是机器人感知的关键组成部分，但现有的基于检测的跟踪方法通常依赖2D线索（如边界框和运动建模），在机器人环境中面临密集目标和频繁遮挡的挑战时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;揭示深度信息在轨迹细化中的潜力，提出一种能够有效处理遮挡和近距离交互的深度增强型多目标跟踪方法。&lt;h4&gt;方法&lt;/h4&gt;提出DepTR-MOT，一种基于DETR的检测器，增强实例级深度信息；创新点包括：(1)基于基础模型的实例级软深度标签监督，优化深度预测；(2)密集深度图蒸馏，保持全局深度一致性；使模型能够在推理时输出实例级深度，无需基础模型且无额外计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;通过整合深度线索，增强了TBD范式的鲁棒性，有效解决了遮挡和近距离挑战；在QuadTrack和DanceTrack数据集上分别实现了27.59和44.47的HOTA分数；在机器人平台MOT数据集上的结果特别突显了该方法在处理遮挡和近距离挑战的优势。&lt;h4&gt;结论&lt;/h4&gt;DepTR-MOT通过有效利用深度信息，显著提高了机器人环境中多目标跟踪的性能，特别是在处理遮挡和近距离交互场景时表现出色。&lt;h4&gt;翻译&lt;/h4&gt;视觉多目标跟踪(MOT)是机器人感知的关键组成部分，然而现有的基于检测的跟踪(TBD)方法通常依赖于2D线索，如边界框和运动建模，这些方法在遮挡和近距离交互场景下表现不佳。依赖这些2D线索的跟踪器在机器人环境中尤其不可靠，因为密集目标和频繁遮挡很常见。虽然深度信息有潜力缓解这些问题，但大多数现有的MOT数据集缺乏深度标注，导致其在该领域的作用未被充分探索。为了揭示深度信息在轨迹细化中的潜力，我们引入了DepTR-MOT，一种基于DETR的检测器，增强了实例级深度信息。具体来说，我们提出了两个关键创新：(i)基于基础模型的实例级软深度标签监督，优化深度预测；(ii)将密集深度图蒸馏以保持全局深度一致性。这些策略使DepTR-MOT能够在推理时输出实例级深度，无需基础模型且无额外计算成本。通过整合深度线索，我们的方法增强了TBD范式的鲁棒性，有效解决了遮挡和近距离挑战。在QuadTrack和DanceTrack数据集上的实验证明了我们方法的有效性，分别实现了27.59和44.47的HOTA分数。特别是在QuadTrack（机器人平台MOT数据集）上的结果突显了我们的方法在处理机器人跟踪中遮挡和近距离挑战的优势。源代码将在https://github.com/warriordby/DepTR-MOT公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Multi-Object Tracking (MOT) is a crucial component of roboticperception, yet existing Tracking-By-Detection (TBD) methods often rely on 2Dcues, such as bounding boxes and motion modeling, which struggle underocclusions and close-proximity interactions. Trackers relying on these 2D cuesare particularly unreliable in robotic environments, where dense targets andfrequent occlusions are common. While depth information has the potential toalleviate these issues, most existing MOT datasets lack depth annotations,leading to its underexploited role in the domain. To unveil the potential ofdepth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-baseddetector enhanced with instance-level depth information. Specifically, wepropose two key innovations: (i) foundation model-based instance-level softdepth label supervision, which refines depth prediction, and (ii) thedistillation of dense depth maps to maintain global depth consistency. Thesestrategies enable DepTR-MOT to output instance-level depth during inference,without requiring foundation models and without additional computational cost.By incorporating depth cues, our method enhances the robustness of the TBDparadigm, effectively resolving occlusion and close-proximity challenges.Experiments on both the QuadTrack and DanceTrack datasets demonstrate theeffectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,respectively. In particular, results on QuadTrack, a robotic platform MOTdataset, highlight the advantages of our method in handling occlusion andclose-proximity challenges in robotic tracking. The source code will be madepublicly available at https://github.com/warriordby/DepTR-MOT.</description>
      <author>example@mail.com (Buyin Deng, Lingxin Huang, Kai Luo, Fei Teng, Kailun Yang)</author>
      <guid isPermaLink="false">2509.17323v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>OpenGVL - Benchmarking Visual Temporal Progress for Data Curation</title>
      <link>http://arxiv.org/abs/2509.17321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为OpenGVL的全面基准，用于估计各种挑战性操作任务中的任务进度，并评估了开源模型与闭源模型在时间进度预测任务上的性能差异。&lt;h4&gt;背景&lt;/h4&gt;机器人领域的数据稀缺性是限制进展的主要因素，但野外可用的机器人数据量正在指数级增长，为大规模数据利用创造了新机会。&lt;h4&gt;目的&lt;/h4&gt;提出一种可靠的时间任务完成预测方法，帮助自动标注和整理大规模机器人数据。&lt;h4&gt;方法&lt;/h4&gt;基于生成价值学习(GVL)方法，提出了OpenGVL基准，利用视觉语言模型(VLMs)中的知识从视觉观察中预测任务进度。&lt;h4&gt;主要发现&lt;/h4&gt;开源模型系列在时间进度预测任务上显著落后于闭源对应模型，仅实现了约70%的性能；OpenGVL可作为自动数据整理和过滤的实用工具，有效评估大规模机器人数据集质量。&lt;h4&gt;结论&lt;/h4&gt;OpenGVL提供了一个全面的基准来估计任务进度，并展示了开源模型与闭源模型在性能上的差距，同时为大规模机器人数据集的质量评估提供了实用工具。&lt;h4&gt;翻译&lt;/h4&gt;数据稀缺仍然是推动机器人领域进步的最具限制性的因素之一。然而，野外可用的机器人数据量正在指数级增长，为大规模数据利用创造了新的机会。可靠的时间任务完成预测可以帮助大规模自动标注和整理这些数据。生成价值学习(GVL)方法最近被提出，利用视觉语言模型(VLMs)中嵌入的知识从视觉观察中预测任务进度。基于GVL，我们提出了OpenGVL，这是一个全面的基准，用于估计涉及机器人和人类形态的各种挑战性操作任务中的任务进度。我们评估了公开可用的开源基础模型的能力，表明开源模型系列显著落后于闭源对应模型，在时间进度预测任务上仅实现了约70%的性能。此外，我们展示了OpenGVL如何作为自动数据整理和过滤的实用工具，能够有效评估大规模机器人数据集的质量。我们在OpenGVL上发布了基准测试和完整的代码库。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data scarcity remains one of the most limiting factors in driving progress inrobotics. However, the amount of available robotics data in the wild is growingexponentially, creating new opportunities for large-scale data utilization.Reliable temporal task completion prediction could help automatically annotateand curate this data at scale. The Generative Value Learning (GVL) approach wasrecently proposed, leveraging the knowledge embedded in vision-language models(VLMs) to predict task progress from visual observations. Building upon GVL, wepropose OpenGVL, a comprehensive benchmark for estimating task progress acrossdiverse challenging manipulation tasks involving both robotic and humanembodiments. We evaluate the capabilities of publicly available open-sourcefoundation models, showing that open-source model families significantlyunderperform closed-source counterparts, achieving only approximately $70\%$ oftheir performance on temporal progress prediction tasks. Furthermore, wedemonstrate how OpenGVL can serve as a practical tool for automated datacuration and filtering, enabling efficient quality assessment of large-scalerobotics datasets. We release the benchmark along with the complete codebase at\href{github.com/budzianowski/opengvl}{OpenGVL}.</description>
      <author>example@mail.com (Paweł Budzianowski, Emilia Wiśnios, Gracjan Góral, Igor Kulakov, Viktor Petrenko, Krzysztof Walas)</author>
      <guid isPermaLink="false">2509.17321v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Reference-aware SFM layers for intrusive intelligibility prediction</title>
      <link>http://arxiv.org/abs/2509.17270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint; submitted to ICASSP 2026. 5 pages. CPC3 system: Dev RMSE  22.36, Eval RMSE 24.98 (ranked 1st)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究重新审视了侵入式语音可懂度预测，通过结合参考条件和多层语音基础模型(SFM)表示，显著提高了预测性能，在CPC3排行榜上排名第一。&lt;h4&gt;背景&lt;/h4&gt;侵入式语音可懂度预测器虽然广泛使用，但并不总是优于非侵入式系统。作者认为主要原因是语音基础模型(SFMs)的利用有限。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过更有效地利用语音基础模型(SFMs)来改进侵入式语音可懂度预测器的性能。&lt;h4&gt;方法&lt;/h4&gt;通过结合参考条件和多层语音基础模型(SFM)表示来重新设计侵入式预测系统。&lt;h4&gt;主要发现&lt;/h4&gt;最终系统在开发集上达到RMSE 22.36，在评估集上达到24.98，在CPC3排行榜上排名第一。&lt;h4&gt;结论&lt;/h4&gt;这些发现为构建基于SFM的侵入式可懂度预测器提供了实用指导，表明更有效地利用语音基础模型可以显著提高侵入式预测的性能。&lt;h4&gt;翻译&lt;/h4&gt;侵入式语音可懂度预测器利用显式参考信号现已广泛使用，但它们并不总是优于非侵入式系统。我们认为主要原因是对语音基础模型(SFMs)的利用有限。本研究通过结合参考条件和多层SFM表示重新审视了侵入式预测。我们的最终系统在开发集上达到RMSE 22.36，在评估集上达到24.98，在CPC3上排名第1。这些发现为构建基于SFM的侵入式可懂度预测器提供了实用指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intrusive speech-intelligibility predictors that exploit explicit referencesignals are now widespread, yet they have not consistently surpassednon-intrusive systems. We argue that a primary cause is the limitedexploitation of speech foundation models (SFMs). This work revisits intrusiveprediction by combining reference conditioning with multi-layer SFMrepresentations. Our final system achieves RMSE 22.36 on the development setand 24.98 on the evaluation set, ranking 1st on CPC3. These findings providepractical guidance for constructing SFM-based intrusive intelligibilitypredictors.</description>
      <author>example@mail.com (Hanlin Yu, Haoshuai Zhou, Boxuan Cao, Changgeng Mo, Linkai Li, Shan X. Wang)</author>
      <guid isPermaLink="false">2509.17270v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Towards a unified turbulence model through multi-objective learning</title>
      <link>http://arxiv.org/abs/2509.17189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多目标学习的统一数据驱动湍流建模框架，实现了在不同流动工况下的帕累托最优性能，无需手动调整或切换模型。&lt;h4&gt;背景&lt;/h4&gt;湍流是经典物理学中的核心挑战，也是气候、航空航天和能源系统中准确流动预测的关键障碍。尽管工业模拟中广泛依赖雷诺平均纳维-斯托克斯求解器，但现有湍流模型缺乏处理分离流、二次流和自由剪切流等不同流动工况的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的数据驱动湍流建模框架，实现在不同流动数据集上的帕累托最优性能，每个数据集代表不同的流动机制和关注量。&lt;h4&gt;方法&lt;/h4&gt;提出采用并行张量基神经网络架构的统一底层模型，具有自动平衡和内部分支能力，可在不同流动工况间自适应转换。模型通过并行架构实现显式正则化以促进简洁性，并通过张量基公式保持物理对称性。研究在五个代表性流动上训练模型，并在27个测试案例上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在所有测试案例中改进或匹配了基线k-ω模型的性能。针对特定应用，在定制数据集上训练的专业模型可在具有挑战性的配置中进一步提高准确性，如在三维扩压器流动中同时处理分离流和二次流。&lt;h4&gt;结论&lt;/h4&gt;研究证明了在单一架构内统一多种流动机制的通用、可部署的湍流模型是可行的，这标志着科学和工业应用的统一湍流建模取得了重大进展。&lt;h4&gt;翻译&lt;/h4&gt;湍流是经典物理学中的核心挑战，也是气候、航空航天和能源系统中准确流动预测的关键障碍。尽管工业模拟中广泛依赖雷诺平均纳维-斯托克斯求解器，但现有湍流模型缺乏处理分离流、二次流和自由剪切流等不同流动工况的泛化能力，需要手动调整或切换。我们提出了一种基于多目标学习的统一数据驱动湍流建模框架，旨在实现在不同流动数据集上的帕累托最优性能，每个数据集代表不同的机制和关注量。所得到的统一底层模型采用并行张量基神经网络，具有自动平衡和内部分支能力，无需显式切换即可适应不同流动工况。并行架构能够显式正则化以促进模型简洁性，而张量基公式保持物理对称性。在五个代表性流动上训练后，该模型在27个测试案例上进行了评估，包括附着流、分离流、二次流以及两个具有工业意义的三维流动。在所有案例中，它改进或匹配了基线k-ω模型的性能。对于特定应用，我们展示了在定制数据集上训练的专业模型可以在具有挑战性的配置中进一步提高准确性，如在气体涡轮空气动力学中常见的三维扩压器流动中，同时存在分离流和二次流。这些结果表明，在单一架构内统一多种流动机制的通用、可部署的湍流模型是可行的。这项工作在科学和工业应用的统一湍流建模方面取得了重大进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Turbulence is a central challenge in classical physics and a critical barrierto accurate flow prediction in climate, aerospace, and energy systems. Despitethe widespread reliance on Reynolds-averaged Navier-Stokes (RANS) solvers inindustrial simulations, existing turbulence models lack the generalizability tohandle diverse regimes, such as separation, secondary flows, and free-shearflows, without manual tuning or switching. We propose a unified data-driventurbulence modeling framework based on multi-objective learning. The goal is toachieve Pareto-optimal performance across heterogeneous flow datasets, eachrepresenting distinct mechanisms and quantities of interest. The resultingunified foundation model employs a parallel tensor basis neural network withautomatic balancing and internal branching to adapt across flow regimes withoutexplicit switching. The parallel architecture enables explicit regularizationto promote model parsimony, while the tensor-basis formulation preservesphysical symmetries. Trained on five representative flows, the model isevaluated on 27 test cases spanning attached, separated, and secondary flows,as well as two realistic three-dimensional flows of industrial relevance. Itimproves or matches the performance of the baseline $k$-$\omega$ model in allcases. For specific applications, we show that specialist models trained ontailored datasets can further improve accuracy in challenging configurations,such as three-dimensional diffuser flows common in gas turbine aerodynamics,which exhibit simultaneous separation and secondary flows. These resultsdemonstrate that a generalized, deployable turbulence model unifying multipleflow mechanisms within a single architecture is achievable. This work markssignificant progress toward unified turbulence modeling for scientific andindustrial applications.</description>
      <author>example@mail.com (Zhuo-Ran Liu, Hao-Chen Wang, Zhuo-Lin Zhao, Heng Xiao)</author>
      <guid isPermaLink="false">2509.17189v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Governing Automated Strategic Intelligence</title>
      <link>http://arxiv.org/abs/2509.17087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;国家间的军事和经济战略竞争力将越来越多地由其前沿人工智能模型的能力和成本决定。多模态基础模型有望自动化战略分析，能够融合卫星图像、手机位置轨迹、社交媒体记录和书面文档等多种数据源。文章评估了这些系统的能力，提出了一个分类法，描述了这些系统将回答的问题类型，并提供了国家保持战略竞争力的建议。&lt;h4&gt;背景&lt;/h4&gt;国家间的战略竞争日益依赖于人工智能技术，特别是在军事情报领域。虽然许多讨论集中在AI系统如何启用新的军事模式或做出战略决策，但'数据中心中的CIA分析师'能力及其影响尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;评估多模态基础模型自动化战略分析的能力，提出一个分类法来描述这些系统将回答的问题类型，提供一个高层模型来确定这些系统的AI能力，并为国家提供建议，使其在新的自动化智能范式中保持战略竞争力。&lt;h4&gt;方法&lt;/h4&gt;进行了一项初步的提升研究，以经验性地评估这些系统的能力。&lt;h4&gt;主要发现&lt;/h4&gt;多模态基础模型有望能够将卫星图像、手机位置轨迹、社交媒体记录和书面文档等多种数据源融合到一个可查询的系统中，从而实现战略分析的自动化。&lt;h4&gt;结论&lt;/h4&gt;国家需要适应这种新的自动化智能范式，以保持战略竞争力。多模态基础模型将改变国家进行战略分析的方式，需要新的战略和政策来应对这一变化。&lt;h4&gt;翻译&lt;/h4&gt;国家之间的军事和经济战略竞争力将越来越多地由其前沿人工智能模型的能力和成本来定义。此类系统将首先带来的地缘政治优势领域之一是军事情报自动化。虽然许多讨论都集中在AI系统如何启用新的军事模式，如致命自主武器，或做出战略决策，但一个国家'在数据中心中的CIA分析师'的能力及其影响尚未得到充分探索。多模态基础模型有望自动化以前由人类进行的战略分析。它们将能够将当今丰富的卫星图像、手机位置轨迹、社交媒体记录和书面文档融合到一个可查询的系统中。我们进行了一项初步的提升研究，以经验性地评估这些能力，然后提出了一个分类法，描述了这些系统将回答的真实问题类型，提供了一个高层模型来确定该系统的AI能力，并为国家提供了建议，以在新的自动化智能范式中保持战略竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Military and economic strategic competitiveness between nation-states willincreasingly be defined by the capability and cost of their frontier artificialintelligence models. Among the first areas of geopolitical advantage granted bysuch systems will be in automating military intelligence. Much discussion hasbeen devoted to AI systems enabling new military modalities, such as lethalautonomous weapons, or making strategic decisions. However, the ability of acountry of "CIA analysts in a data-center" to synthesize diverse data at scale,and its implications, have been underexplored. Multimodal foundation modelsappear on track to automate strategic analysis previously done by humans. Theywill be able to fuse today's abundant satellite imagery, phone-location traces,social media records, and written documents into a single queryable system. Weconduct a preliminary uplift study to empirically evaluate these capabilities,then propose a taxonomy of the kinds of ground truth questions these systemswill answer, present a high-level model of the determinants of this system's AIcapabilities, and provide recommendations for nation-states to remainstrategically competitive within the new paradigm of automated intelligence.</description>
      <author>example@mail.com (Nicholas Kruus, Madhavendra Thakur, Adam Khoja, Leonhard Nagel, Maximilian Nicholson, Abeer Sharma, Jason Hausenloy, Alberto KoTafoya, Aliya Mukhanova, Alli Katila-Miikkulainen, Harish Chandran, Ivan Zhang, Jessie Chen, Joel Raj, Jord Nguyen, Lai Hsien Hao, Neja Jayasundara, Soham Sen, Sophie Zhang, Ashley Dora Kokui Tamaklo, Bhavya Thakur, Henry Close, Janghee Lee, Nina Sefton, Raghavendra Thakur, Shiv Munagala, Yeeun Kim)</author>
      <guid isPermaLink="false">2509.17087v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models</title>
      <link>http://arxiv.org/abs/2509.17074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the IEEE International Conference on Robotics and  Automation (ICRA) 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种信息框架用于文本引导的功能学习，通过基于信息的约束实现特征级别的文本-图像对齐，在少样本功能学习中取得了新的最先进水平。&lt;h4&gt;背景&lt;/h4&gt;视觉功能学习对机器人理解和有效与物理世界交互至关重要。最近的研究尝试利用视觉-语言基础模型的预训练知识，用有限的训练数据学习功能属性。然而，这些方法忽视了保持视觉图像和语言描述之间特征对齐的重要性。&lt;h4&gt;目的&lt;/h4&gt;提出一个信息框架用于文本引导的功能学习，实现特征级别的文本-图像对齐，提高功能区域识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一种功能互信息约束，通过最大化输入图像中功能区域特征与相应文本提示之间的互信息，同时学习适当的文本提示和任务导向的视觉特征。此外，提出了一种对象级信息约束，最大化给定对象的视觉特征与其所属类别的文本特征之间的互信息，使模型能够捕获对象的高质量表示。&lt;h4&gt;主要发现&lt;/h4&gt;在AGD20K数据集上的实验结果表明，所提出的方法优于现有方法，在少样本功能学习中达到了新的最先进水平。&lt;h4&gt;结论&lt;/h4&gt;通过功能互信息约束和对象级信息约束，所提出的方法有效实现了文本图像对齐，为机器人提供了更可靠的语义先验，从而提高了功能区域识别的性能。&lt;h4&gt;翻译&lt;/h4&gt;视觉功能学习对于机器人理解和有效与物理世界交互至关重要。该领域的最新进展试图利用视觉-语言基础模型的预训练知识，用有限的训练数据学习功能属性，为视觉功能学习提供了新的范式。然而，这些方法忽视了在文本引导下识别功能区域时保持视觉图像和语言描述之间特征对齐的重要性，可能导致次优结果。在本文中，我们提出了一个用于文本引导功能学习的信息框架，该框架涉及基于信息的约束，以实现特征级别的文本-图像对齐。具体来说，我们设计了一种功能互信息约束，通过最大化输入图像中功能区域特征与相应文本提示之间的互信息，同时学习适当的文本提示和任务导向的视觉特征。此外，我们提出了一种对象级信息约束，最大化给定对象的视觉特征与其所属类别的文本特征之间的互信息。这使模型能够捕获对象的高质量表示，为识别功能区域提供更可靠的语义先验。在AGD20K数据集上的实验结果表明，所提出的方法优于现有方法，在少样本功能学习中实现了新的最先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual affordance learning is crucial for robots to understand and interacteffectively with the physical world. Recent advances in this field attempt toleverage pre-trained knowledge of vision-language foundation models to learnaffordance properties with limited training data, providing a novel paradigmfor visual affordance learning. However, these methods overlook thesignificance of maintaining feature alignment between visual images andlanguage descriptions for identifying affordance areas with textual guidance,and thus may lead to suboptimal results. In this paper, we present aninformative framework for text-guided affordance learning, which involvesinformation-based constraints to achieve text-image alignment at feature level.Specifically, we design an affordance mutual information constraint that helpslearn appropriate textual prompts and task-oriented visual featuressimultaneously by maximizing the mutual information between the features of theaffordance areas in the input images and the corresponding textual prompts. Inaddition, we propose an object-level information constraint that maximizes themutual information between the visual features of a given object and the textfeatures of the category it belongs to. This enables the model to capturehigh-quality representations for the object, providing more reliable semanticpriors for identifying affordance regions. Experimental results on the AGD20Kdataset show that the proposed method outperforms existing approaches andachieves the new state-of-the-art in one-shot affordance learning.</description>
      <author>example@mail.com (Qian Zhang, Lin Zhang, Xing Fang, Mingxin Zhang, Zhiyuan Wei, Ran Song, Wei Zhang)</author>
      <guid isPermaLink="false">2509.17074v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>TSGym: Design Choices for Deep Multivariate Time-Series Forecasting</title>
      <link>http://arxiv.org/abs/2509.17063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过系统性地分解深度多变量时间序列预测方法的核心组件，解决了当前研究从整体角度评估模型而忽视各组件独立贡献的问题。同时提出了名为TSGym的自动化解决方案，实现了细粒度组件选择和自动化模型构建，显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;深度学习在多变量时间序列预测(MTSF)任务中推动了显著进展，但当前研究往往从整体角度评估模型，掩盖了各组件的独立贡献，留下了关键问题未得到解决。&lt;h4&gt;目的&lt;/h4&gt;通过系统性地分解深度MTSF方法的核心细粒度组件，弥合现有研究中的空白，提供比以往基准更深刻的见解，并开发一种自动化解决方案来提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;系统性地分解深度MTSF方法到核心细粒度组件，如序列分块标记化、通道独立策略、注意力模块，甚至是大型语言模型和时间序列基础模型。提出名为TSGym的自动化解决方案，执行细粒度组件选择和自动化模型构建，而不是传统的超参数调优、神经架构搜索或固定模型选择。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验和组件级分析，提供了比以往将模型作为一个整体讨论的基准更深刻的见解。TSGym显著优于现有的最先进MTSF和AutoML方法，能够创建针对多样化时间序列数据定制更有效的解决方案。&lt;h4&gt;结论&lt;/h4&gt;TSGym能够增强模型在不同数据源之间的可转移性和对分布变化的鲁棒性，显著提升多变量时间序列预测任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，深度学习推动了多变量时间序列预测(MTSF)任务的显著进展。然而，当前MTSF中的大部分研究倾向于从整体角度评估模型，这掩盖了各组件的独立贡献，并留下了关键问题未得到解决。遵循当前的建模范式，这项工作通过系统性地将深度MTSF方法分解为其核心、细粒度的组件（如序列分块标记化、通道独立策略、注意力模块，甚至是大型语言模型和时间序列基础模型）来弥合这些空白。通过大量实验和组件级分析，我们的工作提供了比以往通常将模型作为一个整体讨论的基准更深刻的见解。此外，我们为MTSF任务提出了一种名为TSGym的新型自动化解决方案。与传统的超参数调优、神经架构搜索或固定模型选择不同，TSGym执行细粒度组件选择和自动化模型构建，这使得能够创建针对多样化时间序列数据定制的更有效解决方案，从而增强模型在不同数据源之间的可转移性和对分布变化的鲁棒性。大量实验表明，TSGym显著优于现有的最先进MTSF和AutoML方法。所有代码已在https://github.com/SUFE-AILAB/TSGym上公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, deep learning has driven significant advancements in multivariatetime series forecasting (MTSF) tasks. However, much of the current research inMTSF tends to evaluate models from a holistic perspective, which obscures theindividual contributions and leaves critical issues unaddressed. Adhering tothe current modeling paradigms, this work bridges these gaps by systematicallydecomposing deep MTSF methods into their core, fine-grained components likeseries-patching tokenization, channel-independent strategy, attention modules,or even Large Language Models and Time-series Foundation Models. Throughextensive experiments and component-level analysis, our work offers moreprofound insights than previous benchmarks that typically discuss models as awhole.  Furthermore, we propose a novel automated solution called TSGym for MTSFtasks. Unlike traditional hyperparameter tuning, neural architecture searchingor fixed model selection, TSGym performs fine-grained component selection andautomated model construction, which enables the creation of more effectivesolutions tailored to diverse time series data, therefore enhancing modeltransferability across different data sources and robustness againstdistribution shifts. Extensive experiments indicate that TSGym significantlyoutperforms existing state-of-the-art MTSF and AutoML methods. All code ispublicly available on https://github.com/SUFE-AILAB/TSGym.</description>
      <author>example@mail.com (Shuang Liang, Chaochuan Hou, Xu Yao, Shiping Wang, Minqi Jiang, Songqiao Han, Hailiang Huang)</author>
      <guid isPermaLink="false">2509.17063v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning</title>
      <link>http://arxiv.org/abs/2509.17042v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了OGR(Orchestrate, Generate, Reflect)框架，一种基于视觉语言模型(VLM)的多智能体协作自动化驾驶策略学习方法，解决了手动设计奖励函数和训练课程的瓶颈问题，实现了自动驾驶策略的在线学习和优化。&lt;h4&gt;背景&lt;/h4&gt;基础模型的进步促进了自动驾驶安全高效策略学习的新方法，但复杂动态驾驶任务中奖励函数和训练课程的手工工程是一个关键瓶颈，这是一个劳动密集且耗时的过程。&lt;h4&gt;目的&lt;/h4&gt;解决自动驾驶领域中奖励函数和训练课程手动设计的瓶颈问题，实现自动化驾驶策略学习，使强化学习策略能够在线演化以获取交互感知的驾驶技能。&lt;h4&gt;方法&lt;/h4&gt;提出OGR框架，利用VLM的多模态理解能力构建分层智能体系统：1) 集中式协调器规划高层训练目标；2) 生成模块采用两步分析-生成过程高效生成奖励-课程对；3) 反思模块基于在线评估促进迭代优化；4) 专用记忆模块赋予VLM智能体长期记忆能力；5) 引入并行生成方案和人在回路技术增强生成过程的鲁棒性和多样性。&lt;h4&gt;主要发现&lt;/h4&gt;在CARLA模拟器中的大量实验证明了OGR的优越性能、跨不同城市场景的强泛化能力以及对各种RL算法的良好兼容性。进一步的现实世界实验突显了该框架的实际可行性和有效性。&lt;h4&gt;结论&lt;/h4&gt;OGR通过高效的多智能体协作和利用丰富的多模态信息，实现了自动驾驶策略的在线演化，能够获取交互感知的驾驶技能，为自动驾驶领域提供了一种有效的自动化策略学习方法。&lt;h4&gt;翻译&lt;/h4&gt;基础模型的进步促进了自动驾驶安全高效策略学习的新举措。然而，一个关键瓶颈在于复杂动态驾驶任务中奖励函数和训练课程的手工工程，这是一个劳动密集且耗时的过程。为解决这个问题，我们提出了OGR(Orchestrate, Generate, Reflect)，一种新颖的自动化驾驶策略学习框架，该框架利用基于视觉语言模型(VLM)的多智能体协作。我们的框架利用VLM的高级推理和多模态理解能力构建分层智能体系统。具体来说，集中式协调器规划高层训练目标，而生成模块采用两步分析-然后-生成过程高效生成奖励-课程对。然后，反思模块基于在线评估促进迭代优化。此外，专用记忆模块赋予VLM智能体长期记忆能力。为了增强生成过程的鲁棒性和多样性，我们引入了并行生成方案和人在回路技术，用于增强奖励观察空间。通过高效的多智能体协作和利用丰富的多模态信息，OGR使强化学习策略能够在线演化以获取交互感知的驾驶技能。在CARLA模拟器中的大量实验证明了其优越性能、跨不同城市场景的强泛化能力以及对各种RL算法的强兼容性。进一步的现实世界实验突显了我们框架的实际可行性和有效性。源代码将在论文接受后提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advancement of foundation models fosters new initiatives for policylearning in achieving safe and efficient autonomous driving. However, acritical bottleneck lies in the manual engineering of reward functions andtraining curricula for complex and dynamic driving tasks, which is alabor-intensive and time-consuming process. To address this problem, we proposeOGR (Orchestrate, Generate, Reflect), a novel automated driving policy learningframework that leverages vision-language model (VLM)-based multi-agentcollaboration. Our framework capitalizes on advanced reasoning and multimodalunderstanding capabilities of VLMs to construct a hierarchical agent system.Specifically, a centralized orchestrator plans high-level training objectives,while a generation module employs a two-step analyze-then-generate process forefficient generation of reward-curriculum pairs. A reflection module thenfacilitates iterative optimization based on the online evaluation. Furthermore,a dedicated memory module endows the VLM agents with the capabilities oflong-term memory. To enhance robustness and diversity of the generationprocess, we introduce a parallel generation scheme and a human-in-the-looptechnique for augmentation of the reward observation space. Through efficientmulti-agent cooperation and leveraging rich multimodal information, OGR enablesthe online evolution of reinforcement learning policies to acquireinteraction-aware driving skills. Extensive experiments in the CARLA simulatordemonstrate the superior performance, robust generalizability across distincturban scenarios, and strong compatibility with various RL algorithms. Furtherreal-world experiments highlight the practical viability and effectiveness ofour framework. The source code will be available upon acceptance of the paper.</description>
      <author>example@mail.com (Zengqi Peng, Yusen Xie, Yubin Wang, Rui Yang, Qifeng Chen, Jun Ma)</author>
      <guid isPermaLink="false">2509.17042v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification</title>
      <link>http://arxiv.org/abs/2509.16935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MIDOG'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用大型视觉基础模型结合低秩适应(LoRA)进行参数高效微调，用于非典型有丝分裂(AMFs)分类。最佳方法在MIDOG 2025挑战赛上达到了88.37%的平衡准确率，位列第九。&lt;h4&gt;背景&lt;/h4&gt;非典型有丝分裂是罕见的异常细胞分裂，与肿瘤侵袭性和不良预后相关。其检测面临形态学线索微妙、类别不平衡以及病理学家间观察者变异性等挑战。&lt;h4&gt;目的&lt;/h4&gt;在MIDOG 2025挑战赛的专门赛道上，系统评估使用大型视觉基础模型结合LoRA进行非典型有丝分裂分类的效果。&lt;h4&gt;方法&lt;/h4&gt;使用Virchow、Virchow2和UNI等大型视觉基础模型，结合低秩适应(LoRA)进行参数高效的微调。进行了不同LoRA秩以及随机和基于分组的数据分割实验，以分析方法的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;Virchow与LoRA秩8和三折交叉验证集成的最佳方法在初步测试集上达到了88.37%的平衡准确率，在挑战排行榜上并列第九。&lt;h4&gt;结论&lt;/h4&gt;具有高效适应策略的基础模型在非典型有丝分裂分类方面具有潜力，但需要进一步提高特异性和领域泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;非典型有丝分裂(AMFs)是与肿瘤侵袭性和不良预后相关的罕见异常细胞分裂。由于其形态学线索微妙、类别不平衡以及病理学家间的观察者变异性，其检测仍然是一个重大挑战。MIDOG 2025挑战赛引入了专门的非典型有丝分裂分类赛道，使深度学习方法的系统评估成为可能。在本研究中，我们研究了使用包括Virchow、Virchow2和UNI在内的大型视觉基础模型，结合低秩适应(LoRA)进行参数高效的微调。我们进行了不同LoRA秩以及随机和基于分组的数据分割的广泛实验，以分析在变化条件下的鲁棒性。我们的最佳方法，结合Virchow、LoRA秩8和三折交叉验证集成，在初步测试集上达到了88.37%的平衡准确率，在挑战排行榜上并列第九。这些结果突显了具有高效适应策略的基础模型在非典型有丝分裂分类方面的潜力，同时强调了提高特异性和领域泛化能力的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Atypical mitotic figures (AMFs) are rare abnormal cell divisions associatedwith tumor aggressiveness and poor prognosis. Their detection remains asignificant challenge due to subtle morphological cues, class imbalance, andinter-observer variability among pathologists. The MIDOG 2025 challengeintroduced a dedicated track for atypical mitosis classification, enablingsystematic evaluation of deep learning methods. In this study, we investigatedthe use of large vision foundation models, including Virchow, Virchow2, andUNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. Weconducted extensive experiments with different LoRA ranks, as well as randomand group-based data splits, to analyze robustness under varied conditions. Ourbest approach, Virchow with LoRA rank 8 and ensemble of three-foldcross-validation, achieved a balanced accuracy of 88.37% on the preliminarytest set, ranking joint 9th in the challenge leaderboard. These resultshighlight the promise of foundation models with efficient adaptation strategiesfor the classification of atypical mitosis, while underscoring the need forimprovements in specificity and domain generalization.</description>
      <author>example@mail.com (Lavish Ramchandani, Gunjan Deotale, Dev Kumar Das)</author>
      <guid isPermaLink="false">2509.16935v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Transition Frequencies and Dynamic Amplification of Buried Lifelines: A Semi-Analytical Timoshenko Beam on Winkler Foundation Model</title>
      <link>http://arxiv.org/abs/2509.16906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at and accepted for publication in the official Proceedings  of the 16th International Conference on Vibration Problems &amp; 11th  International Conference on Wave Mechanics and Vibrations (ICOVP &amp; WMVC  2025), Lisbon, Portugal, September 2-5, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种用于埋地生命线横向振动分析的半解析模型，基于Timoshenko梁理论在弹性基础上建立。该模型揭示了振动频谱由四个部分组成，由三个过渡频率分隔，每个过渡点处模式的振荡特性随系统属性变化，导致动态放大显著变化。模型通过案例研究得到验证，并量化了关键因素对动态性能的影响。&lt;h4&gt;背景&lt;/h4&gt;地下生命线（如管道和隧道）容易受到地震、交通和其他动态源引起的地面振动影响。准确预测其响应对于确保结构安全和运行能力至关重要。&lt;h4&gt;目的&lt;/h4&gt;引入一个用于埋地生命线横向振动分析的半解析模型，确保地下生命线系统在各种动态荷载下的设计和弹性评估。&lt;h4&gt;方法&lt;/h4&gt;使用弹性基础上的Timoshenko梁理论建立模型，进行案例研究研究不同长度和运行条件下的埋地钢管道，并进行参数研究量化关键因素（包括土壤刚度和系统长度）对动态性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;振动频谱包含四个部分，由三个过渡频率分隔；在每个过渡点，模式的振荡特性随系统属性变化，导致动态放大显著变化；模型通过与有限元模拟的对比得到验证。&lt;h4&gt;结论&lt;/h4&gt;提出的方法为捕捉复杂的振动现象提供了计算效率高且物理透明的框架，超越了简化的行波方法，为地下生命线系统在各种动态荷载下的设计和弹性评估提供了有价值的指导。&lt;h4&gt;翻译&lt;/h4&gt;地下生命线，如管道和隧道，容易受到地震事件、交通和其他动态源引起的地面振动的影响。准确预测它们的响应对于确保结构安全和运行能力至关重要。本研究介绍了一种用于埋地生命线横向振动分析的半解析模型，使用弹性基础上的Timoshenko梁理论建立。闭式解析解表明，振动频谱由四个部分组成，由三个过渡频率分隔。在每个过渡点，模式的振荡特性随系统属性变化，导致动态放大显著变化。通过不同长度和运行条件的埋地钢管道案例研究验证了模型的有效性，与有限元模拟结果表现出良好的一致性。随后的参数研究量化了关键因素（包括土壤刚度和系统长度）对动态性能的影响。该方法为捕捉复杂的振动现象提供了计算效率高且物理透明的框架，超越了简化的行波方法，为受到各种动态荷载的地下生命线系统的设计和弹性评估提供了有价值的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underground lifelines, such as pipelines and tunnels, are susceptible toground vibrations from seismic events, traffic, and other dynamic sources.Accurate prediction of their response is essential for ensuring structuralsafety and operability. This study introduces a semi-analytical model fortransverse vibration analysis of buried lifelines, formulated using theTimoshenko beam theory on elastic foundation. The closed-form analyticalsolutions revealed that the vibration spectrum comprises four parts, separatedby three transition frequencies. At each transition, the oscillatorycharacteristics of the modes change as a function of the system properties,leading to marked variations in dynamic amplification. The model's validity isconfirmed through case studies of buried steel pipelines of varying lengths andoperating conditions, showing excellent agreement with finite elementsimulations. A subsequent parametric study quantifies the influence of keyfactors - including soil stiffness and system length - on dynamic performance.The proposed method provides a computationally efficient and physicallytransparent framework for capturing complex vibration phenomena beyondsimplified travelling-wave approaches, offering valuable guidance for thedesign and resilience assessment of underground lifeline systems subjected tovarious dynamic loads.</description>
      <author>example@mail.com (Gersena Banushi, Kenichi Soga)</author>
      <guid isPermaLink="false">2509.16906v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with Hand-Object Priors and Taxonomy Awareness</title>
      <link>http://arxiv.org/abs/2509.16871v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Hand-Object (HO)GraspFlow的方法，可以将单个RGB图像转换为多模态可执行的平行夹爪抓取，无需目标物体的显式几何先验。&lt;h4&gt;背景&lt;/h4&gt;基于基础模型进行手部重建和视觉处理，解决手部与物体交互(HOI)抓取问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从单个RGB图像中合成高质量抓取姿态的方法，无需明确的目标物体几何信息。&lt;h4&gt;方法&lt;/h4&gt;使用去噪流匹配(FM)合成抓取姿态，条件是三个互补提示：RGB基础特征作为视觉语义、HOI接触重建以及对抓取类型的分类感知先验。&lt;h4&gt;主要发现&lt;/h4&gt;在没有明确HOI接触输入或物体几何的情况下，方法在抓取合成中表现出高保真度，同时保持强接触和分类识别能力；与基于扩散的变体相比表现更好。&lt;h4&gt;结论&lt;/h4&gt;HOGraspFlow实现了可靠、物体无关的抓取合成，在真实世界实验中平均成功率超过83%。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Hand-Object (HO)GraspFlow，一种以功能为中心的方法，可以将包含手部与物体交互(HOI)的单个RGB图像转换为多模态可执行的平行夹爪抓取，而无需目标物体的显式几何先验。基于手部重建和视觉的基础模型，我们使用去噪流匹配(FM)合成抓取姿态，条件是三个互补的提示：RGB基础特征作为视觉语义、HOI接触重建以及对抓取类型的分类感知先验。我们的方法在没有明确HOI接触输入或物体几何的情况下，在抓取合成中表现出高保真度，同时保持强接触和分类识别能力。另一项对照比较显示，HOGraspFlow始终优于基于扩散的变体(HOGraspDiff)，实现高分布保真度和更稳定的优化。我们在真实世界实验中展示了从人类演示实现的可靠、物体无关的抓取合成，平均成功率超过83%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从人类手部与物体交互(HOI)的演示中提取抓取意图，并将其重新定位到机器人平行夹爪(PJ)上，同时保持抓取类型的多样性和准确性，而无需依赖显式的物体几何信息。这个问题在现实中很重要，因为它能让机器人从人类演示中学习自然抓取行为，增强系统在复杂环境中的鲁棒性，并减少对精确3D感知的依赖，使机器人能更好地适应各种抓取任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，如基于拇指-食指对的方法忽略了抓取多样性，而基于几何的方法需要精确3D信息。他们认识到物体变化多样但抓取意图相对有限，人类手部姿势在接触约束下处于低维流形上。方法设计借鉴了多项现有工作：使用WiLoR进行手部重建，DINOv2提取视觉语义，MANO参数化手模型，以及SE(3)生成模型。创新点在于结合视觉语义、接触估计和抓取分类学，设计了HOGraspDiff和HOGraspFlow两种框架，特别是流匹配方法在SE(3)空间上表现更优。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型提取手-物体交互的语义特征，结合手部接触估计和抓取类型识别，通过生成模型在SE(3)空间中生成多样化的平行夹爪抓取姿势，而无需显式物体几何信息。整体流程包括：1)手-物体感知和特征提取：使用WiLoR和DINOv2提取手部参数和视觉特征，估计接触点和识别抓取类型；2)SE(3)姿势合成：通过流匹配或扩散模型在SE(3)空间生成多样化抓取姿势；3)部署：将姿势转换到世界坐标系，使用Z-only ICP校正位置偏差。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次集成视觉基础模型(DINOv2)用于抓取合成，不依赖物体几何；2)生成多样化抓取姿势，保持多模性；3)结合GRASP分类学和可学习代码本实现抓取类型感知；4)通过轻量级MLP进行接触估计，无需显式输入；5)HOGraspFlow在SE(3)流匹配上优于扩散模型。相比之前工作，不依赖拇指-食指模板保留抓取多样性，不依赖SDF或点图特征处理遮挡问题，在SE(3)空间直接操作提高样本效率，且更关注抓取意图提取而非物体重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HOGraspFlow通过结合视觉基础模型、手部接触估计和抓取类型感知，实现了从单RGB帧生成多样化、可执行抓取姿势的vision-based抓取合成方法，在无需显式物体几何信息的情况下达到了83%以上的现实抓取成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Hand-Object\emph{(HO)GraspFlow}, an affordance-centric approachthat retargets a single RGB with hand-object interaction (HOI) into multi-modalexecutable parallel jaw grasps without explicit geometric priors on targetobjects. Building on foundation models for hand reconstruction and vision, wesynthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditionedon the following three complementary cues: RGB foundation features as visualsemantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types.Our approach demonstrates high fidelity in grasp synthesis without explicit HOIcontact input or object geometry, while maintaining strong contact and taxonomyrecognition. Another controlled comparison shows that \emph{HOGraspFlow}consistently outperforms diffusion-based variants (\emph{HOGraspDiff}),achieving high distributional fidelity and more stable optimization in $SE(3)$.We demonstrate a reliable, object-agnostic grasp synthesis from humandemonstrations in real-world experiments, where an average success rate of over$83\%$ is achieved.</description>
      <author>example@mail.com (Yitian Shi, Zicheng Guo, Rosa Wolf, Edgar Welte, Rania Rayyes)</author>
      <guid isPermaLink="false">2509.16871v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model</title>
      <link>http://arxiv.org/abs/2509.16617v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures, to appear in GI LNI (SKILL 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过微调地理空间基础模型预测城市地表温度，探索了模型对未来气候情景和植被策略变化的响应，结果表明该模型在预测准确性和外推能力方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;城市化和气候变化导致城市热岛效应日益频繁和严重。传统基于机器学习模型的预测方法因数据基础设施有限而准确性不足，特别是在服务不足的地区。&lt;h4&gt;目的&lt;/h4&gt;为制定有效的城市热岛效应缓解计划提供详细的气温数据，并探索地理空间基础模型作为传统预测方法受限时的替代方案。&lt;h4&gt;方法&lt;/h4&gt;对地理空间基础模型进行微调，用于预测未来气候情景下的城市地表温度，并利用模拟的植被策略探索模型对土地覆盖变化的响应。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型实现了低于1.74摄氏度的像素级降尺度误差，与真实模式一致，并具有高达3.62摄氏度的外推能力。&lt;h4&gt;结论&lt;/h4&gt;地理空间基础模型在全局非结构化数据上训练，表现出强大的泛化能力，仅需少量微调即可用于城市热岛效应预测，为传统方法受限的地区提供了有效的预测工具。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化和气候变化的推进，城市热岛效应变得越来越频繁和严重。为了制定有效的缓解计划，城市需要详细的气温数据。然而，基于传统机器学习模型和有限数据基础设施的预测分析方法往往提供不准确的预测，特别是在服务不足的地区。在这种情况下，在非结构化全球数据上训练的地理空间基础模型表现出强大的泛化能力，只需要少量微调，为传统方法受限的预测提供了替代方案。本研究微调了一个地理空间基础模型，以预测未来气候情景下的城市地表温度，并使用模拟的植被策略探索了它对土地覆盖变化的响应。微调后的模型实现了低于1.74摄氏度的像素级降尺度误差，并与真实模式一致，展示了高达3.62摄氏度的外推能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As urbanization and climate change progress, urban heat island effects arebecoming more frequent and severe. To formulate effective mitigation plans,cities require detailed air temperature data. However, predictive analyticsmethods based on conventional machine learning models and limited datainfrastructure often provide inaccurate predictions, especially in underservedareas. In this context, geospatial foundation models trained on unstructuredglobal data demonstrate strong generalization and require minimal fine-tuning,offering an alternative for predictions where traditional approaches arelimited. This study fine-tunes a geospatial foundation model to predict urbanland surface temperatures under future climate scenarios and explores itsresponse to land cover changes using simulated vegetation strategies. Thefine-tuned model achieved pixel-wise downscaling errors below 1.74 {\deg}C andaligned with ground truth patterns, demonstrating an extrapolation capacity upto 3.62 {\deg}C.</description>
      <author>example@mail.com (David Kreismann)</author>
      <guid isPermaLink="false">2509.16617v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Synergies between Federated Foundation Models and Smart Power Grids</title>
      <link>http://arxiv.org/abs/2509.16496v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了多模态多任务联邦基础模型(M3T FedFMs)在智能电网领域的应用，从双向视角分析了M3T FedFMs如何增强电网功能以及智能电网如何影响M3T FedFMs的设计。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)如GPT-3的出现标志着机器学习的重要范式转变，而多模态、多任务基础模型(M3T FMs)作为更一般的新类别正在兴起。结合联邦学习(FL)形成M3T联邦基础模型(FedFMs)，这是一种能处理异构数据并支持多种下游任务的模型。&lt;h4&gt;目的&lt;/h4&gt;将M3T FedFMs引入电力系统研究社区，从双向角度探讨：(i) M3T FedFMs用于智能电网，(ii) 智能电网用于FedFMs。&lt;h4&gt;方法&lt;/h4&gt;通过M3T FedFMs学习来自电网边缘的分布式、异构数据，以隐私保护的方式增强电网功能；同时研究智能电网的约束和结构如何塑造M3T FedFMs的设计、训练和部署。&lt;h4&gt;主要发现&lt;/h4&gt;M3T FedFMs能够通过分布式、异构数据增强电网功能如负荷/需求预测和故障检测；智能电网的约束和结构影响M3T FedFMs的设计、训练和部署。&lt;h4&gt;结论&lt;/h4&gt;M3T FedFMs为智能电网提供了新的可能性，同时智能电网的特性也反过来影响这类模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)如GPT-3的近期出现标志着机器学习的重要范式转变。这些模型在大规模数据语料库上训练，在语言理解、生成、摘要和推理方面表现出色，改变了智能系统处理和与人语言交互的方式。尽管LLMs可能仍像最近的突破，但该领域已经见证了一个更新且更通用的类别的兴起：多模态、多任务基础模型(M3T FMs)。这些模型超越语言范畴，能够处理时间序列测量、音频、图像、表格记录和非结构化日志等异构数据类型/模态，同时支持涵盖预测、分类、控制和检索的广泛下游任务。当与联邦学习(FL)结合时，它们形成M3T联邦基础模型(FedFMs)：一类最新且很大程度上未被探索的模型，能够在分布式数据源上实现可扩展、隐私保护的模型训练/微调。在本文中，我们通过提供双向视角，将这些模型引入电力系统研究社区：(i) 用于智能电网的M3T FedFMs，和(ii) 用于FedFMs的智能电网。在前者中，我们探讨M3T FedFMs如何通过以隐私保护的方式学习电网边缘可用的分布式、异构数据，来增强负荷/需求预测和故障检测等关键电网功能。在后者中，我们研究跨越能源、通信和监管维度的智能电网的约束和结构如何塑造M3T FedFMs的设计、训练和部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent emergence of large language models (LLMs) such as GPT-3 has markeda significant paradigm shift in machine learning. Trained on massive corpora ofdata, these models demonstrate remarkable capabilities in languageunderstanding, generation, summarization, and reasoning, transforming howintelligent systems process and interact with human language. Although LLMs maystill seem like a recent breakthrough, the field is already witnessing the riseof a new and more general category: multi-modal, multi-task foundation models(M3T FMs). These models go beyond language and can process heterogeneous datatypes/modalities, such as time-series measurements, audio, imagery, tabularrecords, and unstructured logs, while supporting a broad range of downstreamtasks spanning forecasting, classification, control, and retrieval. Whencombined with federated learning (FL), they give rise to M3T FederatedFoundation Models (FedFMs): a highly recent and largely unexplored class ofmodels that enable scalable, privacy-preserving model training/fine-tuningacross distributed data sources. In this paper, we take one of the first stepstoward introducing these models to the power systems research community byoffering a bidirectional perspective: (i) M3T FedFMs for smart grids and (ii)smart grids for FedFMs. In the former, we explore how M3T FedFMs can enhancekey grid functions, such as load/demand forecasting and fault detection, bylearning from distributed, heterogeneous data available at the grid edge in aprivacy-preserving manner. In the latter, we investigate how the constraintsand structure of smart grids, spanning energy, communication, and regulatorydimensions, shape the design, training, and deployment of M3T FedFMs.</description>
      <author>example@mail.com (Seyyedali Hosseinalipour, Shimiao Li, Adedoyin Inaolaji, Filippo Malandra, Luis Herrera, Nicholas Mastronarde)</author>
      <guid isPermaLink="false">2509.16496v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs</title>
      <link>http://arxiv.org/abs/2509.18015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了多模态大型语言模型在医学图像中定位病理的能力，发现当前模型的表现虽低于专业基准但仍有潜力，需与任务特定工具集成以提高可靠性。&lt;h4&gt;背景&lt;/h4&gt;前沿大型语言模型及其多模态对应模型在医学诊断中显示出有前景的性能，具有广泛的临床应用潜力。然而，除了诊断外，医学图像解释的一个基本能力是定位病理发现，这对临床和教育具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;系统评估两种通用多模态大型语言模型(GPT-4和GPT-5)和一种领域特定模型(MedGemma)在胸部X光片上定位病理的能力。&lt;h4&gt;方法&lt;/h4&gt;使用覆盖空间网格的提示管道引发基于坐标的预测，在CheXlocalize数据集上的九种病理进行评估，比较各模型的定位准确率。&lt;h4&gt;主要发现&lt;/h4&gt;GPT-5定位准确率为49.7%，GPT-4为39.1%，MedGemma为17.7%，均低于CNN基线(59.9%)和放射科医生基准(80.1%)。GPT-5预测多在解剖合理区域但不精确；GPT-4在固定位置病理上表现较好；MedGemma泛化能力有限。&lt;h4&gt;结论&lt;/h4&gt;当前多模态大型语言模型在医学成像中既显示出前景也有限制，需与任务特定工具集成以实现可靠的临床应用。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，前沿大型语言模型(LLMs)及其多模态对应模型在医学测验和诊断任务中展现出有前景的性能，鉴于其易于访问和通用性，突显了它们广泛的临床应用潜力。然而，除了诊断之外，医学图像解释的一个基本方面是能够定位病理发现。评估定位能力不仅具有临床和教育意义，还能深入了解模型对解剖结构和疾病的空间理解。在此，我们使用覆盖空间网格并引发基于坐标预测的提示管道，系统评估了两种通用多模态大型语言模型(GPT-4和GPT-5)和一种领域特定模型(MedGemma)在胸部X光片上定位病理的能力。在CheXlocalize数据集上的九种病理平均来看，GPT-5的定位准确率为49.7%，其次是GPT-4(39.1%)和MedGemma(17.7%)，均低于任务特定的CNN基线(59.9%)和放射科医生基准(80.1%)。尽管表现一般，但错误分析显示GPT-5的预测大多在解剖学合理的区域，只是不总是精确定位。GPT-4在解剖位置固定的病理上表现良好，但在空间可变的发现上表现较差，且更频繁地出现解剖学上不合理的预测。MedGemma在所有病理上表现最低，显示出对此新颖任务的泛化能力有限。我们的研究结果既突显了当前多模态大型语言模型在医学成像中的前景和局限性，也强调了将它们与任务特定工具集成以实现可靠使用的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent work has shown promising performance of frontier large language models(LLMs) and their multimodal counterparts in medical quizzes and diagnostictasks, highlighting their potential for broad clinical utility given theiraccessible, general-purpose nature. However, beyond diagnosis, a fundamentalaspect of medical image interpretation is the ability to localize pathologicalfindings. Evaluating localization not only has clinical and educationalrelevance but also provides insight into a model's spatial understanding ofanatomy and disease. Here, we systematically assess two general-purpose MLLMs(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability tolocalize pathologies on chest radiographs, using a prompting pipeline thatoverlays a spatial grid and elicits coordinate-based predictions. Averagedacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited alocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark(80.1%). Despite modest performance, error analysis revealed that GPT-5'spredictions were largely in anatomically plausible regions, just not alwaysprecisely localized. GPT-4 performed well on pathologies with fixed anatomicallocations, but struggled with spatially variable findings and exhibitedanatomically implausible predictions more frequently. MedGemma demonstrated thelowest performance on all pathologies, showing limited capacity to generalizeto this novel task. Our findings highlight both the promise and limitations ofcurrent MLLMs in medical imaging and underscore the importance of integratingthem with task-specific tools for reliable use.</description>
      <author>example@mail.com (Advait Gosai, Arun Kavishwar, Stephanie L. McNamara, Soujanya Samineni, Renato Umeton, Alexander Chowdhury, William Lotter)</author>
      <guid isPermaLink="false">2509.18015v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models</title>
      <link>http://arxiv.org/abs/2509.17664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SD-VLM框架，通过MSMU数据集和深度位置编码方法增强视觉语言模型的空间感知能力，在三维空间关系定量推理方面取得显著提升。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在二维语义视觉理解方面表现出色，但在三维空间关系的定量推理能力方面探索不足，这归因于二维图像的空间表示能力有限。&lt;h4&gt;目的&lt;/h4&gt;分析阻碍VLMs空间理解能力的问题，并提出SD-VLM框架以显著增强VLMs的基本空间感知能力。&lt;h4&gt;方法&lt;/h4&gt;提出大规模空间测量和理解(MSMU)数据集，包含70万个问答对、250万个物理数值标注和1万个思维链增强样本；引入简单的深度位置编码方法增强VLMs的空间感知能力；训练SD-VLM模型。&lt;h4&gt;主要发现&lt;/h4&gt;SD-VLM在MSMU-Bench上取得最先进性能，并在Q-Spatial和SpatialRGPT-Bench等其他空间理解基准上表现出空间泛化能力；在MSMU-Bench上，SD-VLM分别比GPT-4o和Intern-VL3-78B高出26.91%和25.56%。&lt;h4&gt;结论&lt;/h4&gt;通过MSMU数据集和深度位置编码方法，SD-VLM显著提升了VLMs的空间感知和定量推理能力。&lt;h4&gt;翻译&lt;/h4&gt;虽然视觉语言模型在二维语义视觉理解方面表现出色，但由于二维图像空间表示能力的不足，它们在三维空间关系的定量推理方面的能力仍未得到充分探索。在本文中，我们分析了阻碍VLMs空间理解能力的问题，并提出了SD-VLM这一新框架，通过两个关键贡献显著增强VLMs的基本空间感知能力：(1)提出具有精确空间标注的大规模空间测量和理解(MSMU)数据集，以及(2)引入一种增强VLMs空间感知能力的简单深度位置编码方法。MSMU数据集涵盖了包含70万个问答对、250万个物理数值标注和1万个思维链增强样本的大规模定量空间任务。我们训练了SD-VLM，一个表现强大的通用VLM，展示了卓越的定量空间测量和理解能力。SD-VLM不仅在我们提出的MSMU-Bench上取得了最先进的性能，还在其他空间理解基准测试(包括Q-Spatial和SpatialRGPT-Bench)上表现出空间泛化能力。大量实验表明，在MSMU-Bench上，SD-VLM分别比GPT-4o和Intern-VL3-78B高出26.91%和25.56%。代码和模型已在https://github.com/cpystan/SD-VLM发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉语言模型在3D空间关系定量推理方面的能力不足问题。这个问题在现实中很重要，因为随着机器人技术、自动驾驶和增强现实等应用的发展，模型需要精确理解空间关系才能在真实世界中有效操作。现有模型虽然擅长2D视觉理解，但在回答'图片中桌子的大小是多少'这类涉及绝对距离和物理尺寸的问题时表现不佳，限制了它们在实际应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了阻碍VLMs空间理解能力的问题，指出图像只是3D场景到平面的投影，失去了原始3D结构信息。虽然有些工作使用3D场景作为输入，但作者选择避免引入3D结构数据，因为精确点云数据难以获取。作者借鉴了现有工作中深度图作为辅助输入的方法，参考了通过参考对象激发推理路径的思路，并利用了Transformer架构中位置嵌入的成功经验。作者观察到现有模型对空间概念掌握不足，缺乏精确的数值标注，因此设计了MSMU数据集和深度位置编码方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过深度位置编码将第三维（z轴）的信息引入模型，并提供大规模精确的空间标注数据集，使VLMs能够从2D图像中理解3D空间关系。整体流程包括：1) 构建MSMU数据集，从3D场景点云收集空间信息，建立3D到2D映射，过滤图像和物体，使用模板生成QA对，并通过LLM协作生成CoT样本；2) 设计SD-VLM模型架构，包含视觉编码器、深度编码模块和大语言模型；3) 实现深度位置编码，将深度图划分为小块，计算平均深度值，使用正弦和余弦函数生成深度位置嵌入，并将其添加到图像嵌入中。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) MSMU数据集，提供700K个QA对和250万个精确物理标注，覆盖多种定量空间任务；2) 深度位置编码(DPE)，简单有效地将深度信息引入模型；3) 全面实验验证，展示SD-VLM在空间任务上的优越性能。相比之前的工作，SD-VLM不需要像'深度作为图像'方法那样引入额外训练模块，不像'深度作为提示'方法那样需要教模型使用API，也不像'深度作为标记'方法那样需要扩展序列长度。MSMU数据集相比其他空间数据集提供了更精确的物理标注，覆盖了更广泛的定量空间任务，数据量更大。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SD-VLM通过引入深度位置编码和构建大规模精确空间标注数据集，显著提升了视觉语言模型在3D空间定量测量与理解方面的能力，为机器人在现实环境中的有效操作提供了更强的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While vision language models (VLMs) excel in 2D semantic visualunderstanding, their ability to quantitatively reason about 3D spatialrelationships remains under-explored, due to the deficiency of 2D images'spatial representation ability. In this paper, we analyze the problem hinderingVLMs' spatial understanding abilities and propose SD-VLM, a novel frameworkthat significantly enhances fundamental spatial perception abilities of VLMsthrough two key contributions: (1) propose Massive Spatial Measuring andUnderstanding (MSMU) dataset with precise spatial annotations, and (2)introduce a simple depth positional encoding method strengthening VLMs' spatialawareness. MSMU dataset covers massive quantitative spatial tasks with 700K QApairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmentedsamples. We have trained SD-VLM, a strong generalist VLM which shows superiorquantitative spatial measuring and understanding capability. SD-VLM not onlyachieves state-of-the-art performance on our proposed MSMU-Bench, but alsoshows spatial generalization abilities on other spatial understandingbenchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experimentsdemonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and25.56% respectively on MSMU-Bench. Code and models are released athttps://github.com/cpystan/SD-VLM.</description>
      <author>example@mail.com (Pingyi Chen, Yujing Lou, Shen Cao, Jinhui Guo, Lubin Fan, Yue Wu, Lin Yang, Lizhuang Ma, Jieping Ye)</author>
      <guid isPermaLink="false">2509.17664v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning</title>
      <link>http://arxiv.org/abs/2509.17437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMNLP2025 Findings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对多模态大型语言模型在几何推理等视觉密集型任务中的感知瓶颈问题，提出了一种两阶段强化学习训练框架，通过先增强视觉感知再培养推理能力，显著提升了模型在几何推理和问题解决方面的表现。&lt;h4&gt;背景&lt;/h4&gt;最近在强化学习方面的进步增强了大型语言模型的推理能力，但对多模态大型语言模型的影响有限。特别是在几何推理等视觉密集型任务中，多模态大型语言模型经常产生幻觉，导致推理不准确。&lt;h4&gt;目的&lt;/h4&gt;量化多模态大型语言模型在视觉感知方面的缺陷，解决其感知瓶颈问题，提升模型在视觉密集型任务中的推理能力。&lt;h4&gt;方法&lt;/h4&gt;设计了一个Geo-Perception问答(GeoPQA)基准来评估模型在几何概念和空间关系上的表现；提出一种两阶段强化学习训练框架，首先增强对几何结构的视觉感知，然后培养推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;多模态大型语言模型在视觉感知方面存在显著缺陷，这限制了强化学习奖励信号的有效性；两阶段训练相比直接推理训练，几何推理提高了9.7%，几何问题解决提高了9.1%。&lt;h4&gt;结论&lt;/h4&gt;感知基础对多模态大型语言模型的有效推理至关重要；所提出的方法不仅适用于几何推理，还可推广到其他视觉密集型领域，如图形理解。&lt;h4&gt;翻译&lt;/h4&gt;最近在强化学习方面的进步增强了大型语言模型的推理能力，但对多模态大型语言模型的影响有限。特别是在几何推理等视觉密集型任务中，多模态大型语言模型经常产生幻觉，导致推理不准确。我们将此归因于多模态大型语言模型中的感知瓶颈，这限制了推理训练的效益。为了量化这一点，我们设计了一个Geo-Perception问答(GeoPQA)基准，针对基本几何概念和空间关系。在GeoPQA上的实验揭示了多模态大型语言模型在视觉感知方面的重大缺陷，这些缺陷限制了强化学习奖励信号的有效训练。为了解决这个瓶颈，我们提出了一种两阶段强化学习训练框架，首先增强对几何结构的视觉感知，然后培养推理能力。应用于Qwen2.5-VL-3B-Instruct模型，我们的两阶段训练相比直接推理训练方法，几何推理提高了9.7%，几何问题解决提高了9.1%。我们的方法也推广到其他视觉密集型领域，如图形理解，突显了感知基础在有效多模态大型语言模型推理中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in reinforcement learning (RL) have enhanced thereasoning abilities of large language models (LLMs), yet the impact onmultimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks likegeometric reasoning, MLLMs hallucinate frequently, leading to inaccuratereasoning. We attribute this to the perceptual bottleneck in MLLMs, which capsthe benefits of reasoning training. To quantify this, we design aGeo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometricconcepts and spatial relationships. Experiments on GeoPQA reveal significantshortcomings of MLLMs in visual perception, which constrain RL reward signalsfor effective training. To address this bottleneck, we propose a two-stage RLtraining framework by first enhancing the visual perception of geometricstructures, then fostering reasoning capabilities. Applied toQwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by9.7% and geometric problem solving by 9.1%, compared to the direct reasoningtraining approach. Our method also generalizes to other vision-intensivedomains like figure understanding, highlighting the importance of perceptualgrounding in effective MLLM reasoning.</description>
      <author>example@mail.com (Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Deli Zhao, Anh Tuan Luu, Yu Rong)</author>
      <guid isPermaLink="false">2509.17437v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration</title>
      <link>http://arxiv.org/abs/2509.17429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了多尺度时序预测(MSTP)任务及相应的基准数据集和解决方法IG-MC，旨在解决视觉语言模型在预测多尺度、多细粒度场景状态方面的困难。&lt;h4&gt;背景&lt;/h4&gt;准确的时序预测是全面场景理解与具身人工智能之间的桥梁，但视觉语言模型在预测多尺度、多细粒度场景状态方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;定义并解决多尺度时序预测(MSTP)任务，支持在一般场景和手术场景中预测不同时间尺度和状态尺度的场景状态。&lt;h4&gt;方法&lt;/h4&gt;提出增量生成和多智能体协作(IG-MC)方法，包含增量生成模块(在扩展时间尺度上合成最新视觉预览)和决策驱动的多智能体协作框架(生成、发起和多状态评估智能体)。&lt;h4&gt;主要发现&lt;/h4&gt;多尺度时序预测可分解为时间维度和状态维度两个正交维度；增量生成模块能保持决策和生成视觉同步，防止随着前瞻间隔延长导致的性能下降。&lt;h4&gt;结论&lt;/h4&gt;IG-MC方法通过增量生成和多智能体协作有效解决了多尺度时序预测任务，在保持全局一致性和局部保真度之间取得了平衡。&lt;h4&gt;翻译&lt;/h4&gt;准确的时序预测是全面场景理解与具身人工智能之间的桥梁。然而，视觉语言模型在预测多个时间尺度上的场景多细粒度状态是困难的。我们在一般场景和手术场景中正式定义了多尺度时序预测(MSTP)任务，将多尺度分解为两个正交维度：时间维度，预测不同前瞻间隔下人类和手术的状态；状态维度，建模一般场景和手术场景中的状态层次结构。例如，在一般场景中，接触关系状态比空间关系状态更细粒度。在手术场景中，中等水平的步骤比高水平阶段更细粒度但仍受其包含阶段的约束。为支持这一统一任务，我们引入了首个MSTP基准，具有跨多个状态尺度和时间尺度的同步标注。我们进一步提出了一种方法：增量生成和多智能体协作(IG-MC)，它集成了两个关键创新。首先，我们提出了一个即插即用的增量生成模块，在扩展的时间尺度上持续合成最新的视觉预览，为多个决策智能体提供信息，保持决策和生成视觉同步，防止随着前瞻间隔延长导致的性能下降。其次，我们提出了一个用于多状态预测的决策驱动的多智能体协作框架，包含生成、发起和多状态评估智能体，动态触发和评估预测周期，以平衡全局一致性和局部保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate temporal prediction is the bridge between comprehensive sceneunderstanding and embodied artificial intelligence. However, predictingmultiple fine-grained states of a scene at multiple temporal scales isdifficult for vision-language models. We formalize the Multi-Scale TemporalPrediction (MSTP) task in general and surgical scenes by decomposingmulti-scale into two orthogonal dimensions: the temporal scale, forecastingstates of humans and surgery at varying look-ahead intervals, and the statescale, modeling a hierarchy of states in general and surgical scenes. Forexample, in general scenes, states of contact relationships are finer-grainedthan states of spatial relationships. In surgical scenes, medium-level stepsare finer-grained than high-level phases yet remain constrained by theirencompassing phase. To support this unified task, we introduce the first MSTPBenchmark, featuring synchronized annotations across multiple state scales andtemporal scales. We further propose a method, Incremental Generation andMulti-agent Collaboration (IG-MC), which integrates two key innovations. First,we present a plug-and-play incremental generation module that continuouslysynthesizes up-to-date visual previews at expanding temporal scales to informmultiple decision-making agents, keeping decisions and generated visualssynchronized and preventing performance degradation as look-ahead intervalslengthen. Second, we present a decision-driven multi-agent collaborationframework for multi-state prediction, comprising generation, initiation, andmulti-state assessment agents that dynamically trigger and evaluate predictioncycles to balance global coherence and local fidelity.</description>
      <author>example@mail.com (Zhitao Zeng, Guojian Yuan, Junyuan Mao, Yuxuan Wang, Xiaoshuang Jia, Yueming Jin)</author>
      <guid isPermaLink="false">2509.17429v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>LLMs as Layout Designers: A Spatial Reasoning Perspective</title>
      <link>http://arxiv.org/abs/2509.16891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为LaySPA的强化学习框架，通过增强大语言模型的空间理解和推理能力，解决了其在图形布局设计应用中的局限性，实现了结构合理且视觉吸引人的布局生成。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在文本领域展现了强大的推理和规划能力，但在空间理解和推理方面存在明显不足。然而，空间能力对于内容感知的图形布局设计等应用至关重要，这些应用需要在有限视觉空间内精确定位、对齐和组织多个元素。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够增强大语言模型空间理解和推理能力的框架，使其能够有效处理图形布局设计等需要精确空间安排的任务。&lt;h4&gt;方法&lt;/h4&gt;提出LaySPA，一种基于强化学习的框架，通过混合奖励信号（捕捉几何有效性、结构保真度和视觉质量）增强LLM智能体的空间推理能力。该框架利用迭代自我探索和自适应策略优化，生成可解释的推理痕迹和结构化布局。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LaySPA能够生成结构合理且视觉吸引人的布局，性能优于大型通用语言模型，并达到与最先进的专用布局模型相当的水平。&lt;h4&gt;结论&lt;/h4&gt;LaySPA成功解决了大语言模型在空间理解和推理方面的局限性，为需要精确空间安排的应用（如图形布局设计）提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;虽然大型语言模型在文本领域展示了令人印象深刻的推理和规划能力，并且能够有效遵循复杂任务的指令，但它们的空间理解和推理能力仍然有限。然而，这种能力对于内容感知的图形布局设计等应用至关重要，这些应用需要在受限制的视觉空间内精确定位、对齐和组织多个元素。为了解决这一差距，我们提出了LaySPA，这是一个基于强化学习的框架，通过明确的空间推理能力增强LLM智能体。LaySPA利用混合奖励信号，捕捉几何有效性、结构保真度和视觉质量，使智能体能够建模元素间关系、导航画布并优化空间排列。通过迭代自我探索和自适应策略优化，LaySPA产生可解释的推理痕迹和结构化布局。实验结果表明，LaySPA生成结构合理且视觉吸引人的布局，优于大型通用LLM，并达到与最先进的专用布局模型相当的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Large Language Models (LLMs) have demonstrated impressive reasoning andplanning abilities in textual domains and can effectively follow instructionsfor complex tasks, their capacity for spatial understanding and reasoningremains limited. Such capabilities, however, are critical for applications likecontent-aware graphic layout design, which demands precise placement,alignment, and structural organization of multiple elements within constrainedvisual spaces. To address this gap, we propose LaySPA, a reinforcementlearning-based framework that augments LLM agents with explicit spatialreasoning capabilities. LaySPA leverages hybrid reward signals that capturegeometric validity, structural fidelity, and visual quality, enabling agents tomodel inter-element relationships, navigate the canvas, and optimize spatialarrangements. Through iterative self-exploration and adaptive policyoptimization, LaySPA produces both interpretable reasoning traces andstructured layouts. Experimental results demonstrate that LaySPA generatesstructurally sound and visually appealing layouts, outperforming largergeneral-purpose LLMs and achieving results on par with state-of-the-artspecialized layout models.</description>
      <author>example@mail.com (Sha Li)</author>
      <guid isPermaLink="false">2509.16891v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2509.16721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 12 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Text-Scene框架，自动将3D场景解析为文本描述，用于场景理解。通过识别对象属性和空间关系生成连贯场景摘要，无需人工干预。利用几何分析和MLLMs产生准确、详细且人类可解释的描述。&lt;h4&gt;背景&lt;/h4&gt;让智能体理解和与复杂3D场景互动是具身人工智能系统的基本挑战。尽管多模态大语言模型在2D图像理解方面取得了显著进展，但将其扩展到3D场景仍然困难，因为3D环境涉及更丰富的概念（如空间关系、功能、物理、布局等），且缺乏大规模3D视觉语言数据集。&lt;h4&gt;目的&lt;/h4&gt;开发Text-Scene框架，自动将3D场景解析为文本描述，桥接3D观察与语言之间的差距，使3D场景内容通过语言变得清晰易懂。&lt;h4&gt;方法&lt;/h4&gt;给定3D场景，模型识别对象属性和空间关系，生成整个场景的连贯摘要。利用几何分析和多模态大语言模型(MLLMs)产生准确、详细且人类可解释的描述。同时提出InPlan3D基准，包含636个室内场景中的3174个长期规划任务，用于评估MLLMs的推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Text-Scene生成的文本解析可以真实地表示3D场景，并有利于下游任务。通过InPlan3D基准评估了MLLMs在3D任务规划中的推理能力。&lt;h4&gt;结论&lt;/h4&gt;Text-Scene框架强调清晰度和可访问性，通过语言使3D场景内容易于理解。代码和数据集将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;让智能体能够理解和与复杂的3D场景互动是具身人工智能系统的基本挑战。虽然多模态大语言模型在2D图像理解方面取得了显著进展，但将这种能力扩展到3D场景仍然存在困难：1) 3D环境涉及更丰富的概念，如空间关系、功能、物理、布局等；2) 大规模3D视觉语言数据集的缺失构成了重大障碍。在本文中，我们介绍了Text-Scene，这是一个将3D场景自动解析为文本描述以进行场景理解的框架。给定一个3D场景，我们的模型识别对象属性和空间关系，然后生成整个场景的连贯摘要，桥接3D观察与语言之间的差距，无需人工干预。通过利用几何分析和MLLMs，Text-Scene产生准确、详细且人类可解释的描述，捕捉对象级别的细节和全局级别的上下文。在基准测试上的实验结果表明，我们的文本解析可以真实地表示3D场景并有利于下游任务。为了评估MLLMs的推理能力，我们提出了InPlan3D，这是一个用于3D任务规划的综合基准，包含636个室内场景中的3174个长期规划任务。我们强调方法的清晰度和可访问性，旨在通过语言使3D场景内容易于理解。代码和数据集将公开发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让智能体能够理解和交互复杂的3D场景的问题。这个问题在现实中很重要，因为它关系到机器人、增强现实和自主系统等应用领域；在研究中也很重要，因为虽然多模态大型语言模型在2D图像理解方面取得了进展，但将其能力扩展到3D场景仍然困难，且缺乏大规模的3D视觉语言数据集构成了重大障碍。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先将问题分解为识别物体属性和空间关系两个核心组件，然后设计场景解析框架将3D场景转换为文本描述。他们借鉴了现有工作，如使用多模态大型语言模型、3D实例分割方法(Mask3D)、BLIP-2和CLIP等模型来生成和评估图像描述，以及点云特征或多视图图像特征提升到3D空间的技术。作者还创新性地引入了自我反思机制和句子选择块来提高生成质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D场景自动解析为结构化的文本描述，通过几何分析和MLLM的结合生成准确、详细且人类可解释的描述，利用文本作为3D场景的通用表示媒介。整体流程包括：1)图像采样和3D重建；2)空间关系推理；3)实例投影和描述生成；4)场景到语言翻译；5)自我反思优化；6)句子选择过滤；7)多模态推理框架处理下游任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Text-Scene自动场景到语言解析框架；2)InPlan3D基准测试；3)高效的文本驱动表示；4)自我反思机制；5)句子选择块。相比之前工作，Text-Scene将3D场景转换为文本而非直接输入点云或多视图图像，计算效率更高(推理时间仅108-169ms)，性能更优(在多个基准上达到最先进水平)，更少的视觉依赖(可仅用文本输入工作)，且提供了更全面的任务规划评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Text-Scene通过将3D场景自动解析为结构化文本描述，实现了高效准确的3D场景理解，减少了计算资源需求，并在多个任务上达到了最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling agents to understand and interact with complex 3D scenes is afundamental challenge for embodied artificial intelligence systems. WhileMultimodal Large Language Models (MLLMs) have achieved significant progress in2D image understanding, extending such capabilities to 3D scenes remainsdifficult: 1) 3D environment involves richer concepts such as spatialrelationships, affordances, physics, layout, and so on, 2) the absence oflarge-scale 3D vision-language datasets has posed a significant obstacle. Inthis paper, we introduce Text-Scene, a framework that automatically parses 3Dscenes into textual descriptions for scene understanding. Given a 3D scene, ourmodel identifies object attributes and spatial relationships, and thengenerates a coherent summary of the whole scene, bridging the gap between 3Dobservation and language without requiring human-in-the-loop intervention. Byleveraging both geometric analysis and MLLMs, Text-Scene produces descriptionsthat are accurate, detailed, and human-interpretable, capturing object-leveldetails and global-level context. Experimental results on benchmarksdemonstrate that our textual parses can faithfully represent 3D scenes andbenefit downstream tasks. To evaluate the reasoning capability of MLLMs, wepresent InPlan3D, a comprehensive benchmark for 3D task planning, consisting of3174 long-term planning tasks across 636 indoor scenes. We emphasize clarityand accessibility in our approach, aiming to make 3D scene contentunderstandable through language. Code and datasets will be released.</description>
      <author>example@mail.com (Haoyuan Li, Rui Liu, Hehe Fan, Yi Yang)</author>
      <guid isPermaLink="false">2509.16721v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery</title>
      <link>http://arxiv.org/abs/2509.16618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Early accepted by MICCAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Surgical-MambaLLM的新方法，首次将Mamba2与大语言模型结合应用于手术领域，通过CBMI模块和SIP扫描模式，有效解决了手术场景中跨模态依赖和空间信息感知的挑战，显著提升了视觉问题局部回答任务的性能。&lt;h4&gt;背景&lt;/h4&gt;Visual Question Localized-Answering in robotic surgery (Surgical-VQLA) 近年来受到关注，它可以帮助医学生和初级医生理解手术场景。大语言模型的发展为该任务提供了更有前景的解决方案。&lt;h4&gt;目的&lt;/h4&gt;解决当前方法难以建立文本和视觉细节之间复杂依赖关系，以及难以感知手术场景空间信息的问题。&lt;h4&gt;方法&lt;/h4&gt;提出Surgical-MambaLLM方法，首次将Mamba2与LLM结合应用于手术领域；提出Cross-modal Bidirectional Mamba2 Integration (CBMI)模块进行多模态融合；设计Surgical Instrument Perception (SIP)扫描模式增强模型对手术场景的空间理解。&lt;h4&gt;主要发现&lt;/h4&gt;在EndoVis17-VQLA和EndoVis18-VQLA数据集上，Surgical-MambaLLM模型优于最先进的方法，显著提高了Surgical-VQLA任务的性能。&lt;h4&gt;结论&lt;/h4&gt;Surgical-MambaLLM通过有效捕获跨模态依赖和感知手术场景空间信息，增强了对手术图像的理解，为手术视觉问题局部回答任务提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;近年来，机器人手术中的视觉问题局部回答技术已受到广泛关注，它有潜力帮助医学生和初级医生理解手术场景。最近，大语言模型的快速发展为该任务提供了更有前景的解决方案。然而，当前方法难以建立文本和视觉细节之间的复杂依赖关系，且难以感知手术场景的空间信息。为应对这些挑战，我们提出了一种新方法Surgical-MambaLLM，这是首次在手术领域将Mamba2与LLM结合的方法，它利用Mamba2有效捕获跨模态依赖的能力并感知手术场景中的空间信息，从而增强LLM对手术图像的理解。具体而言，我们提出了跨模态双向Mamba2集成模块，利用Mamba2进行有效的多模态融合。此外，针对手术场景的几何特征，我们设计了手术器械感知扫描模式，让Mamba2扫描手术图像，增强模型对手术场景的空间理解。大量实验表明，我们的Surgical-MambaLLM模型在EndoVis17-VQLA和EndoVis18-VQLA数据集上优于最先进的方法，显著提高了Surgical-VQLA任务的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Visual Question Localized-Answering in robotic surgery(Surgical-VQLA) has gained significant attention for its potential to assistmedical students and junior doctors in understanding surgical scenes. Recently,the rapid development of Large Language Models (LLMs) has provided morepromising solutions for this task. However, current methods struggle toestablish complex dependencies between text and visual details, and havedifficulty perceiving the spatial information of surgical scenes. To addressthese challenges, we propose a novel method, Surgical-MambaLLM, which is thefirst to combine Mamba2 with LLM in the surgical domain, that leveragesMamba2's ability to effectively capture cross-modal dependencies and perceivespatial information in surgical scenes, thereby enhancing the LLMs'understanding of surgical images. Specifically, we propose the Cross-modalBidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effectivemultimodal fusion, with its cross-modal integration capabilities. Additionally,tailored to the geometric characteristics of surgical scenes, we design theSurgical Instrument Perception (SIP) scanning mode for Mamba2 to scan thesurgical images, enhancing the model's spatial understanding of the surgicalscene. Extensive experiments demonstrate that our Surgical-MambaLLM modeloutperforms the state-of-the-art methods on the EndoVis17-VQLA andEndoVis18-VQLA datasets, significantly improving the performance of theSurgical-VQLA task.</description>
      <author>example@mail.com (Pengfei Hao, Hongqiu Wang, Shuaibo Li, Zhaohu Xing, Guang Yang, Kaishun Wu, Lei Zhu)</author>
      <guid isPermaLink="false">2509.16618v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Can multimodal representation learning by alignment preserve modality-specific information?</title>
      <link>http://arxiv.org/abs/2509.17943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a workshop paper at MACLEAN - ECML/PKDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了多模态数据融合在机器学习和遥感领域的应用，重点关注如何保留跨模态未共享的任务相关信息。&lt;h4&gt;背景&lt;/h4&gt;多模态数据结合是机器学习中的关键问题，包括许多遥感问题。早期多模态数据融合方法基于特定神经网络架构和监督学习，但由于标记数据稀缺，自监督学习技术得到发展。当前最先进的多模态表示学习技术利用不同模态卫星数据在同一地理区域的空间对齐，以促进潜在空间中的语义对齐。&lt;h4&gt;目的&lt;/h4&gt;研究在对齐策略下如何保留跨模态未共享的任务相关信息，揭示对齐策略何时会导致信息损失，并支持多模态卫星数据组合对比学习的新发展。&lt;h4&gt;方法&lt;/h4&gt;首先在简化假设下展示对齐策略何时会导致信息损失，然后在更现实的设置中通过数值实验支持理论见解。&lt;h4&gt;主要发现&lt;/h4&gt;对齐策略在某些情况下会导致信息损失，这一发现得到了理论和实验证据的支持。&lt;h4&gt;结论&lt;/h4&gt;基于理论和经验证据，支持多模态卫星数据组合对比学习的新发展，相关代码和数据已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;结合多模态数据是广泛机器学习任务中的关键问题，包括许多遥感问题。在地球观测中，早期的多模态数据融合方法基于特定的神经网络架构和监督学习。此后，标记数据的稀缺性推动了自监督学习技术的发展。最先进的多模态表示学习技术利用同一地理区域获取的不同模态卫星数据之间的空间对齐，以促进潜在空间中的语义对齐。在本文中，我们研究了这些方法如何保留跨模态未共享的任务相关信息。首先，我们在简化假设下展示了对齐策略何时会导致信息损失。然后，我们在更现实的设置中通过数值实验支持了我们的理论见解。基于这些理论和经验证据，我们希望支持多模态卫星数据组合对比学习的新发展。我们的代码和数据已在https://github.com/Romain3Ch216/alg_maclean_25公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Combining multimodal data is a key issue in a wide range of machine learningtasks, including many remote sensing problems. In Earth observation, earlymultimodal data fusion methods were based on specific neural networkarchitectures and supervised learning. Ever since, the scarcity of labeled datahas motivated self-supervised learning techniques. State-of-the-art multimodalrepresentation learning techniques leverage the spatial alignment betweensatellite data from different modalities acquired over the same geographic areain order to foster a semantic alignment in the latent space. In this paper, weinvestigate how this methods can preserve task-relevant information that is notshared across modalities. First, we show, under simplifying assumptions, whenalignment strategies fundamentally lead to an information loss. Then, wesupport our theoretical insight through numerical experiments in more realisticsettings. With those theoretical and empirical evidences, we hope to supportnew developments in contrastive learning for the combination of multimodalsatellite data. Our code and data is publicly available athttps://github.com/Romain3Ch216/alg_maclean_25.</description>
      <author>example@mail.com (Romain Thoreau, Jessie Levillain, Dawa Derksen)</author>
      <guid isPermaLink="false">2509.17943v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification</title>
      <link>http://arxiv.org/abs/2509.17802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TS-P²CL的新型即插即用框架，通过将一维生理信号转换为二维伪图像，利用预训练视觉模型的通用模式识别能力，解决了医疗时间序列分类中的跨个体异质性问题。&lt;h4&gt;背景&lt;/h4&gt;医疗时间序列分类对智能医疗保健至关重要，但由于个体间的高度异质性，其效果受到严重的跨主题生成限制。尽管在架构创新和迁移学习方面有所进展，但当前方法仍受限于特定模态的归纳偏差，限制了学习通用不变表示的能力。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一种能够学习通用不变表示的新框架，以减轻医疗时间序列分类中的个体特定偏差。&lt;h4&gt;方法&lt;/h4&gt;提出TS-P²CL框架，采用视觉引导范式将一维生理信号转换为二维伪图像，建立与视觉领域的桥梁。在该统一空间中，使用双重对比学习策略：模态内一致性强制时间连贯性，跨模态对齐将时间序列动态与视觉语义对齐，从而学习鲁棒的领域不变特征。&lt;h4&gt;主要发现&lt;/h4&gt;在六个医疗时间序列数据集上的大量实验表明，TS-P²CL在主题相关和主题无关设置中均一致优于十四种对比方法。&lt;h4&gt;结论&lt;/h4&gt;TS-P²CL框架通过利用预训练视觉模型的能力，有效解决了医疗时间序列分类中的跨个体异质性问题，学习到了鲁棒的领域不变特征。&lt;h4&gt;翻译&lt;/h4&gt;医疗时间序列(MedTS)分类对智能医疗保健至关重要，但其效果因个体间的高度异质性而受到严重的跨主题生成限制。尽管在架构创新和迁移学习技术方面有所进展，但当前方法仍受限于特定模态的归纳偏差，限制了它们学习通用不变表示的能力。为此，我们提出TS-P²CL，一种新颖的即插即用框架，利用预训练视觉模型的通用模式识别能力。我们引入一种视觉引导范式，将一维生理信号转换为二维伪图像，建立与视觉领域的桥梁。这种转换使模型能够隐式访问从自然图像中学到的丰富语义先验。在这个统一空间中，我们采用双重对比学习策略：模态内一致性强制时间连贯性，而跨模态对齐将时间序列动态与视觉语义对齐，从而减轻个体特定偏差并学习鲁棒的领域不变特征。在六个MedTS数据集上的大量实验表明，TS-P²CL在主题相关和主题无关设置中均一致优于十四种方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical time series (MedTS) classification is pivotal for intelligenthealthcare, yet its efficacy is severely limited by poor cross-subjectgeneration due to the profound cross-individual heterogeneity. Despite advancesin architectural innovations and transfer learning techniques, current methodsremain constrained by modality-specific inductive biases that limit theirability to learn universally invariant representations. To overcome this, wepropose TS-P$^2$CL, a novel plug-and-play framework that leverages theuniversal pattern recognition capabilities of pre-trained vision models. Weintroduce a vision-guided paradigm that transforms 1D physiological signalsinto 2D pseudo-images, establishing a bridge to the visual domain. Thistransformation enables implicit access to rich semantic priors learned fromnatural images. Within this unified space, we employ a dual-contrastivelearning strategy: intra-modal consistency enforces temporal coherence, whilecross-modal alignment aligns time-series dynamics with visual semantics,thereby mitigating individual-specific biases and learning robust,domain-invariant features. Extensive experiments on six MedTS datasetsdemonstrate that TS-P$^2$CL consistently outperforms fourteen methods in bothsubject-dependent and subject-independent settings.</description>
      <author>example@mail.com (Qi'ao Xu, Pengfei Wang, Bo Zhong, Tianwen Qian, Xiaoling Wang, Ye Wang, Hong Yu)</author>
      <guid isPermaLink="false">2509.17802v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>SeqUDA-Rec: Sequential User Behavior Enhanced Recommendation via Global Unsupervised Data Augmentation for Personalized Content Marketing</title>
      <link>http://arxiv.org/abs/2509.17361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SeqUDA-Rec是一种新的深度学习框架，通过整合用户行为序列和全局无监督数据增强来提高推荐准确性和鲁棒性，解决了传统推荐系统依赖有限监督信号和易受噪声影响的问题。&lt;h4&gt;背景&lt;/h4&gt;个性化内容营销已成为数字平台的关键策略，但传统推荐系统有两个主要局限性：依赖有限的监督信号（来自明确用户反馈）和容易受到噪声或非故意交互的影响。&lt;h4&gt;目的&lt;/h4&gt;解决传统推荐系统的局限性，提高推荐准确性和鲁棒性，为个性化广告和智能内容推荐提供更有效的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出SeqUDA-Rec框架，构建全局用户-物品交互图(GUIG)捕获局部和全局物品关联，应用图对比学习模块生成鲁棒嵌入，使用基于Transformer的顺序编码器建模用户偏好，并采用基于GAN的增强策略生成合理交互模式补充训练数据。&lt;h4&gt;主要发现&lt;/h4&gt;在Amazon Ads和TikTok AdClicks两个真实营销数据集上，SeqUDA-Rec显著优于SASRec、BERT4Rec和GCL4SR等最先进基线方法，在NDCG@10上实现6.7%改进，在HR@10上实现11.3%改进。&lt;h4&gt;结论&lt;/h4&gt;SeqUDA-Rec在个性化广告和智能内容推荐方面表现出色，证明了其有效性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;个性化内容营销已成为数字平台的关键策略，旨在提供符合用户偏好的定制广告和推荐。传统推荐系统通常存在两个局限：(1)依赖于来自明确用户反馈的有限监督信号，(2)容易受到噪声或非故意交互的影响。为应对这些挑战，我们提出了SeqUDA-Rec，一种新的深度学习框架，将用户行为序列与全局无监督数据增强相结合，以提高推荐准确性和鲁棒性。我们的方法首先从所有用户行为序列构建全局用户-物品交互图(GUIG)，捕获局部和全局物品关联。然后，应用图对比学习模块生成鲁棒嵌入，同时基于Transformer的顺序编码器建模用户不断变化的偏好。为进一步提高多样性并对抗稀疏监督标签，我们采用基于GAN的增强策略，生成合理的交互模式并补充训练数据。在两个真实营销数据集(Amazon Ads和TikTok AdClicks)上的广泛实验表明，SeqUDA-Rec显著优于SASRec、BERT4Rec和GCL4SR等最先进基线方法。我们的模型在NDCG@10上实现了6.7%的改进，在HR@10上实现了11.3%的改进，证明了其在个性化广告和智能内容推荐方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized content marketing has become a crucial strategy for digitalplatforms, aiming to deliver tailored advertisements and recommendations thatmatch user preferences. Traditional recommendation systems often suffer fromtwo limitations: (1) reliance on limited supervised signals derived fromexplicit user feedback, and (2) vulnerability to noisy or unintentionalinteractions. To address these challenges, we propose SeqUDA-Rec, a novel deeplearning framework that integrates user behavior sequences with globalunsupervised data augmentation to enhance recommendation accuracy androbustness. Our approach first constructs a Global User-Item Interaction Graph(GUIG) from all user behavior sequences, capturing both local and global itemassociations. Then, a graph contrastive learning module is applied to generaterobust embeddings, while a sequential Transformer-based encoder models users'evolving preferences. To further enhance diversity and counteract sparsesupervised labels, we employ a GAN-based augmentation strategy, generatingplausible interaction patterns and supplementing training data. Extensiveexperiments on two real-world marketing datasets (Amazon Ads and TikTok AdClicks) demonstrate that SeqUDA-Rec significantly outperforms state-of-the-artbaselines such as SASRec, BERT4Rec, and GCL4SR. Our model achieves a 6.7%improvement in NDCG@10 and 11.3% improvement in HR@10, proving itseffectiveness in personalized advertising and intelligent contentrecommendation.</description>
      <author>example@mail.com (Ruihan Luo, Xuanjing Chen, Ziyang Ding)</author>
      <guid isPermaLink="false">2509.17361v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed Operator Learning for Hemodynamic Modeling</title>
      <link>http://arxiv.org/abs/2509.17293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in the proceedings of DICTA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于物理信息神经算子学习的心血管动力学建模方法，通过知识蒸馏简化了复杂的物理信息神经网络架构，在保持性能的同时降低了训练复杂性和实现难度。&lt;h4&gt;背景&lt;/h4&gt;个性化心血管动力学建模对于无创监测和治疗规划至关重要。现有的物理信息神经网络方法虽有效但引入了显著的训练和实现复杂性，限制了可扩展性和实际应用。&lt;h4&gt;目的&lt;/h4&gt;研究物理信息神经算子学习模型作为通过知识蒸馏训练简化架构的有效监督信号，以降低复杂模型的实现难度。&lt;h4&gt;方法&lt;/h4&gt;在高保真无袖带血压记录上预训练物理信息DeepONet(PI-DeepONet)，学习从原始可穿戴波形到逐拍压力信号的算子映射。将预训练算子作为固定监督者，指导简化基础模型，消除复杂的对抗性和对比性学习组件。&lt;h4&gt;主要发现&lt;/h4&gt;基于算子的监督方法与复杂基线性能相当（相关性：零点七六六对比零点七七零，均方根误差：四点四五二对比四点五零一），同时将架构复杂性从八个关键超参数减少到单一正则化系数，训练开销减少百分之四。&lt;h4&gt;结论&lt;/h4&gt;基于算子的监督有效替代了复杂的多组件训练策略，提供了更具可扩展性和可解释性的生理建模方法，同时减少了实现负担。&lt;h4&gt;翻译&lt;/h4&gt;精准的个性化心血管动力学建模对于无创监测和治疗规划至关重要。最先进的物理信息神经网络方法采用深度多分支架构，并结合对抗性或对比性目标来强制执行偏微分方程约束。虽然这些方法有效，但它们引入了显著的训练和实现复杂性，限制了可扩展性和实际部署。我们研究物理信息神经算子学习模型作为通过知识蒸馏训练简化架构的有效监督信号。我们的方法首先在高保真的无袖带血压记录上预训练一个物理信息DeepONet，以学习从原始可穿戴波形到逐拍压力信号的算子映射，并在嵌入的物理约束下进行。这个预训练的算子作为知识蒸馏流水线中的固定监督者，指导简化基础模型，消除复杂的对抗性和对比性学习组件，同时保持性能。我们表征了物理信息正则化在算子学习中的作用，并证明其作为监督指导的有效性。通过大量实验，我们的基于算子的监督方法与复杂基线实现了性能相当，同时将架构复杂性从八个关键超参数减少到单一正则化系数，并将训练开销减少了百分之四。我们的结果表明，基于算子的监督有效地替代了复杂的多组件训练策略，提供了一种更具可扩展性和可解释性的生理建模方法，同时减少了实现负担。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate modeling of personalized cardiovascular dynamics is crucial fornon-invasive monitoring and therapy planning. State-of-the-art physics-informedneural network (PINN) approaches employ deep, multi-branch architectures withadversarial or contrastive objectives to enforce partial differential equationconstraints. While effective, these enhancements introduce significant trainingand implementation complexity, limiting scalability and practical deployment.We investigate physics-informed neural operator learning models as efficientsupervisory signals for training simplified architectures through knowledgedistillation. Our approach pre-trains a physics-informed DeepONet (PI-DeepONet)on high-fidelity cuffless blood pressure recordings to learn operator mappingsfrom raw wearable waveforms to beat-to-beat pressure signals under embeddedphysics constraints. This pre-trained operator serves as a frozen supervisor ina lightweight knowledge-distillation pipeline, guiding streamlined base modelsthat eliminate complex adversarial and contrastive learning components whilemaintaining performance. We characterize the role of physics-informedregularization in operator learning and demonstrate its effectiveness forsupervisory guidance. Through extensive experiments, our operator-supervisedapproach achieves performance parity with complex baselines (correlation: 0.766vs. 0.770, RMSE: 4.452 vs. 4.501), while dramatically reducing architecturalcomplexity from eight critical hyperparameters to a single regularizationcoefficient and decreasing training overhead by 4%. Our results demonstratethat operator-based supervision effectively replaces intricate multi-componenttraining strategies, offering a more scalable and interpretable approach tophysiological modeling with reduced implementation burden.</description>
      <author>example@mail.com (Ryan Chappell, Chayan Banerjee, Kien Nguyen, Clinton Fookes)</author>
      <guid isPermaLink="false">2509.17293v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness</title>
      <link>http://arxiv.org/abs/2509.17228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in Proc. of EMNLP 2025 (18 pages)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种因果表示学习框架，用于处理多模态临床记录中的模态缺失非随机问题，通过整合结构化数据、影像和文本信息，提高了患者表示学习的效果。&lt;h4&gt;背景&lt;/h4&gt;临床笔记包含丰富的患者信息，对患者的表示学习很有价值，但临床笔记经常缺失（如MIMIC-IV数据集中24.5%的患者没有出院摘要）。其他模态（如结构化数据、影像）的可用性受临床决策影响，导致模态缺失非随机(MMNAR)模式。&lt;h4&gt;目的&lt;/h4&gt;开发一个因果表示学习框架，利用多模态临床记录中的观测数据和有信息量的缺失性，解决临床数据中的模态缺失问题。&lt;h4&gt;方法&lt;/h4&gt;框架包含三个组件：(1) MMNAR感知模态融合组件，整合多模态数据并基于缺失模式进行条件化；(2) 具有对比学习的模态重建组件，确保表示学习的语义充分性；(3) 多任务结果预测模型与校正器，校正特定模态观测模式的残余偏差。&lt;h4&gt;主要发现&lt;/h4&gt;在MIMIC-IV和eICU数据集上的评估显示，与最强基线相比持续改进，医院再入院预测AUC提高13.8%，ICU入院预测AUC提高13.1%。&lt;h4&gt;结论&lt;/h4&gt;该框架能有效处理临床数据中的模态缺失非随机问题，显著提升表示学习效果和临床预测性能。&lt;h4&gt;翻译&lt;/h4&gt;临床笔记包含丰富的患者信息，如诊断或药物，使其对患者的表示学习非常有价值。最近大型语言模型的进步进一步提高从临床文本中提取有意义表示的能力。然而，临床笔记经常缺失。例如，在我们对MIMIC-IV数据集的分析中，24.5%的患者没有可用的出院摘要。在这种情况下，可以从其他模态学习表示，如结构化数据、胸部X光片或放射学报告。然而，这些模态的可用性受临床决策影响，并在患者之间有所不同，导致模态缺失非随机(MMNAR)模式。我们提出一个因果表示学习框架，利用多模态临床记录中的观测数据和有信息量的缺失性。它包括：(1) 一个MMNAR感知的模态融合组件，整合结构化数据、成像和文本，同时基于缺失模式进行条件化，以捕捉患者健康和临床医生驱动的分配；(2) 一个具有对比学习的模态重建组件，确保表示学习中的语义充分性；(3) 一个多任务结果预测模型与校正器，校正来自特定模态观测模式的残余偏差。在MIMIC-IV和eICU上的全面评估显示，与最强基线相比持续改进，医院再入院实现高达13.8%的AUC改进，ICU入院实现13.1%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clinical notes contain rich patient information, such as diagnoses ormedications, making them valuable for patient representation learning. Recentadvances in large language models have further improved the ability to extractmeaningful representations from clinical texts. However, clinical notes areoften missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% ofpatients have no available discharge summaries. In such cases, representationscan be learned from other modalities such as structured data, chest X-rays, orradiology reports. Yet the availability of these modalities is influenced byclinical decision-making and varies across patients, resulting in modalitymissing-not-at-random (MMNAR) patterns. We propose a causal representationlearning framework that leverages observed data and informative missingness inmultimodal clinical records. It consists of: (1) an MMNAR-aware modality fusioncomponent that integrates structured data, imaging, and text while conditioningon missingness patterns to capture patient health and clinician-drivenassignment; (2) a modality reconstruction component with contrastive learningto ensure semantic sufficiency in representation learning; and (3) a multitaskoutcome prediction model with a rectifier that corrects for residual bias fromspecific modality observation patterns. Comprehensive evaluations acrossMIMIC-IV and eICU show consistent gains over the strongest baselines, achievingup to 13.8% AUC improvement for hospital readmission and 13.1% for ICUadmission.</description>
      <author>example@mail.com (Zihan Liang, Ziwen Pan, Ruoxuan Xiong)</author>
      <guid isPermaLink="false">2509.17228v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification</title>
      <link>http://arxiv.org/abs/2509.16903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提交了针对DISRPT 2025共享任务3（话语关系分类）的解决方案，包括基准测试多语言BERT模型、评估基于提示的大型语言模型，并引入了HiDAC分层双适配器对比学习模型。&lt;h4&gt;背景&lt;/h4&gt;任务3统一了17个话语关系标签，跨越16种语言的39个语料库和6种话语框架，带来了显著的多语言和跨形式主义挑战。&lt;h4&gt;目的&lt;/h4&gt;建立强基线、评估大型语言模型对统一标签的响应，并开发一种参数高效的模型来处理多语言话语关系分类任务。&lt;h4&gt;方法&lt;/h4&gt;微调多语言BERT模型（mBERT、XLM-RoBERTa-Base和XLM-RoBERTa-Large）使用两种参数排序策略和渐进式解冻比例；评估Claude Opus 4.0在零样本和少样本设置下的表现；开发HiDAC分层双适配器对比学习模型。&lt;h4&gt;主要发现&lt;/h4&gt;更大的transformer模型获得更高准确性但改进有限；解冻编码器顶部75%层的性能与完全微调相当但参数更少；基于提示的模型显著落后于微调的transformer；HiDAC实现67.5%的最高总体准确率且参数效率更高。&lt;h4&gt;结论&lt;/h4&gt;HiDAC模型在保持高参数效率的同时实现了最佳性能，为多语言话语关系分类任务提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;作者向DISRPT 2025共享任务3（话语关系分类）提交了我们的解决方案。任务3在16种语言的39个语料库和6种话语框架中引入了统一的17个话语关系标签集，带来了显著的多语言和跨形式主义挑战。我们首先通过微调多语言BERT模型（mBERT、XLM-RoBERTa-Base和XLM-RoBERTa-Large）并使用两种参数排序策略和渐进式解冻比例来建立任务的基准测试。然后，我们在零样本和少样本设置下评估基于提示的大型语言模型（即Claude Opus 4.0），以了解LLM如何响应新提出的统一标签。最后，我们介绍了HiDAC，一种分层双适配器对比学习模型。结果表明，虽然更大的transformer模型实现了更高的准确性，但改进幅度有限，并且解冻编码器顶部75%层的性能与完全微调相当，同时训练的参数少得多。基于提示的模型显著落后于微调的transformer，而HiDAC实现了最高的总体准确率（67.5%），同时比完全微调保持更高的参数效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present our submission to Task 3 (Discourse Relation Classification) ofthe DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourserelation labels across 39 corpora in 16 languages and six discourse frameworks,posing significant multilingual and cross-formalism challenges. We firstbenchmark the task by fine-tuning multilingual BERT-based models (mBERT,XLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategiesand progressive unfreezing ratios to establish strong baselines. We thenevaluate prompt-based large language models (namely Claude Opus 4.0) inzero-shot and few-shot settings to understand how LLMs respond to the newlyproposed unified labels. Finally, we introduce HiDAC, a HierarchicalDual-Adapter Contrastive learning model. Results show that while largertransformer models achieve higher accuracy, the improvements are modest, andthat unfreezing the top 75% of encoder layers yields performance comparable tofull fine-tuning while training far fewer parameters. Prompt-based models lagsignificantly behind fine-tuned transformers, and HiDAC achieves the highestoverall accuracy (67.5%) while remaining more parameter-efficient than fullfine-tuning.</description>
      <author>example@mail.com (Nawar Turk, Daniele Comitogianni, Leila Kosseim)</author>
      <guid isPermaLink="false">2509.16903v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>The Complexity of Finding Local Optima in Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.16898v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear as a conference paper in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了对比学习中寻找局部最优解的复杂性理论问题，证明了在离散和连续设置中分别是PLS-难和CLS-难的，表明除非P=PLS（或P=CLS），否则不存在多项式时间算法能找到局部最优解。&lt;h4&gt;背景&lt;/h4&gt;对比学习是一种通过优化基于对比信息的目标来发现有意义数据表示的技术，通常以加权三元组集合{(x_i, y_i^+, z_i^-)}的形式给出，其中锚点x_i被认为比负例z_i更接近正例y_i。&lt;h4&gt;目的&lt;/h4&gt;确定在各种对比学习问题中寻找局部最优解的计算复杂性，特别是对于局部搜索算法如基于梯度的方法。&lt;h4&gt;方法&lt;/h4&gt;通过复杂性归约，证明在离散设置中是PLS-难的，在连续设置中是CLS-难的。&lt;h4&gt;主要发现&lt;/h4&gt;在离散设置（如最大化满足的三元组）中寻找局部最优解是PLS-难的；在连续设置（如最小化三元组损失）中寻找局部最优解是CLS-难的；除非PLS⊆P（或CLS⊆P），否则不存在多项式时间算法能找到局部最优解；即使在PLS⊆P（或CLS⊆P）的情况下，也存在需要指数时间才能达到局部最优解的实例。&lt;h4&gt;结论&lt;/h4&gt;对比学习中的局部优化问题本质上是困难的，表明局部搜索算法可能无法高效地找到局部最优解，这为理解对比学习的计算复杂性提供了重要见解。&lt;h4&gt;翻译&lt;/h4&gt;对比学习是一种通过优化基于对比信息的目标来发现有意义数据表示的强大技术，通常以加权三元组集合{(x_i, y_i^+, z_i^-)}的形式给出，表示'锚点'x_i比'负例'z_i更接近'正例'y_i。目标是找到表示（如R^d中的嵌入或树度量），其中锚点被放置得比负例更接近正例。虽然找到对比目标的全局最优解是NP难的，但寻找局部最优解（即通过局部搜索算法如基于梯度的方法无法改进的表示）的复杂性仍然未知。我们的工作通过证明在离散设置（如最大化满足的三元组）中是PLS-难的，在连续设置（如最小化三元组损失）中是CLS-难的，解决了在各种对比学习问题中寻找局部最优解的复杂性，其中PLS（多项式局部搜索）和CLS（连续局部搜索）是分别捕获离散和连续优化中局部搜索动态性的复杂度类。我们的结果表明，除非PLS⊆P（对于连续问题是CLS⊆P），否则不存在多项式时间算法（局部搜索或其他）可以找到各种对比学习问题的局部最优解。即使在PLS⊆P（或CLS⊆P）不太可能的情况下，我们的归约也表明存在实例，其中局部搜索算法需要指数时间才能达到局部最优解，即使对于d=1（在线上的嵌入）也是如此。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning is a powerful technique for discovering meaningful datarepresentations by optimizing objectives based on $\textit{contrastiveinformation}$, often given as a set of weighted triplets $\{(x_i, y_i^+,z_{i}^-)\}_{i = 1}^m$ indicating that an "anchor" $x_i$ is more similar to a"positive" example $y_i$ than to a "negative" example $z_i$. The goal is tofind representations (e.g., embeddings in $\mathbb{R}^d$ or a tree metric)where anchors are placed closer to positive than to negative examples. Whilefinding $\textit{global}$ optima of contrastive objectives is$\mathsf{NP}$-hard, the complexity of finding $\textit{local}$ optima --representations that do not improve by local search algorithms such asgradient-based methods -- remains open. Our work settles the complexity offinding local optima in various contrastive learning problems by proving$\mathsf{PLS}$-hardness in discrete settings (e.g., maximize satisfiedtriplets) and $\mathsf{CLS}$-hardness in continuous settings (e.g., minimizeTriplet Loss), where $\mathsf{PLS}$ (Polynomial Local Search) and$\mathsf{CLS}$ (Continuous Local Search) are well-studied complexity classescapturing local search dynamics in discrete and continuous optimization,respectively. Our results imply that no polynomial time algorithm (local searchor otherwise) can find a local optimum for various contrastive learningproblems, unless $\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq\mathsf{P}$ for continuous problems). Even in the unlikely scenario that$\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq \mathsf{P}$), ourreductions imply that there exist instances where local search algorithms needexponential time to reach a local optimum, even for $d=1$ (embeddings on aline).</description>
      <author>example@mail.com (Jingming Yan, Yiyuan Luo, Vaggos Chatziafratis, Ioannis Panageas, Parnian Shahkar, Stelios Stavroulakis)</author>
      <guid isPermaLink="false">2509.16898v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>On the de-duplication of the Lakh MIDI dataset</title>
      <link>http://arxiv.org/abs/2509.16662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper has been accepted for publication at ISMIR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了符号音乐数据集中的重复数据问题，特别是在Lakh MIDI数据集(LMD)上的应用。研究者评估了多种去重方法，包括基于规则的方法、符号音乐检索模型以及基于对比学习的BERT模型，并提出了三种不同版本的LMD过滤列表，最多可过滤掉38,134个重复样本。&lt;h4&gt;背景&lt;/h4&gt;大规模数据集对于训练泛化能力强的深度学习模型至关重要。大多数此类数据集通过网络爬虫从各种互联网来源收集，不可避免地引入了重复数据。在符号音乐领域，这些重复数据通常来自多个用户的编曲和简单编辑后的元数据更改。然而，尽管数据集重复会导致随机分割过程中的数据泄露，从而影响训练评估的可靠性，但这一问题在音乐信息检索(MIR)社区尚未得到充分解决。&lt;h4&gt;目的&lt;/h4&gt;研究旨在调查Lakh MIDI数据集(LMD)中的数据重复问题，LMD是符号音乐领域最大的公开可用数据源之一。目标是找到并评估用于重复数据检索的最佳方法。&lt;h4&gt;方法&lt;/h4&gt;研究者使用LMD的Clean MIDI子集作为基准测试集，其中同一歌曲的不同版本被分组在一起。首先评估了基于规则的方法和之前的符号音乐检索模型用于去重，还研究了基于对比学习的BERT模型与各种增强方法来查找重复文件。&lt;h4&gt;主要发现&lt;/h4&gt;研究提出了三种不同版本的LMD过滤列表，在最保守的设置下，从178,561个文件中过滤掉了至少38,134个样本。&lt;h4&gt;结论&lt;/h4&gt;通过系统性地评估和比较不同的去重方法，研究者为符号音乐数据集的重复问题提供了有效的解决方案，提高了数据质量和模型训练的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;大规模数据集对于训练泛化能力强的深度学习模型至关重要。大多数此类数据集通过网络爬虫从各种互联网来源收集，不可避免地引入了重复数据。在符号音乐领域，这些重复数据通常来自多个用户的编曲和简单编辑后的元数据更改。然而，尽管数据集重复会导致随机分割过程中的数据泄露，从而影响训练评估的可靠性，但这一问题在音乐信息检索(MIR)社区尚未得到充分解决。本研究调查了Lakh MIDI数据集(LMD)中的数据重复问题，LMD是符号音乐领域最大的公开可用数据源之一。为了找到并评估用于重复数据检索的最佳方法，我们使用LMD的Clean MIDI子集作为基准测试集，其中同一歌曲的不同版本被分组在一起。我们首先评估了基于规则的方法和之前的符号音乐检索模型用于去重，还研究了基于对比学习的BERT模型与各种增强方法来查找重复文件。结果，我们提出了三种不同版本的LMD过滤列表，在最保守的设置下，从178,561个文件中过滤掉了至少38,134个样本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A large-scale dataset is essential for training a well-generalizeddeep-learning model. Most such datasets are collected via scraping from variousinternet sources, inevitably introducing duplicated data. In the symbolic musicdomain, these duplicates often come from multiple user arrangements andmetadata changes after simple editing. However, despite critical issues such asunreliable training evaluation from data leakage during random splitting,dataset duplication has not been extensively addressed in the MIR community.This study investigates the dataset duplication issues regarding Lakh MIDIDataset (LMD), one of the largest publicly available sources in the symbolicmusic domain. To find and evaluate the best retrieval method for duplicateddata, we employed the Clean MIDI subset of the LMD as a benchmark test set, inwhich different versions of the same songs are grouped together. We firstevaluated rule-based approaches and previous symbolic music retrieval modelsfor de-duplication and also investigated with a contrastive learning-based BERTmodel with various augmentations to find duplicate files. As a result, wepropose three different versions of the filtered list of LMD, which filters outat least 38,134 samples in the most conservative settings among 178,561 files.</description>
      <author>example@mail.com (Eunjin Choi, Hyerin Kim, Jiwoo Ryu, Juhan Nam, Dasaem Jeong)</author>
      <guid isPermaLink="false">2509.16662v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval</title>
      <link>http://arxiv.org/abs/2509.16649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, DCASE2025 Task2 technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AISTAT团队提交了基于语言的音频检索系统，采用双编码器架构和对比学习，结合大型语言模型进行数据增强，并通过聚类引入辅助分类任务进行微调。&lt;h4&gt;背景&lt;/h4&gt;DCASE 2025任务6是基于语言的音频检索挑战，AISTAT团队参与了这一任务。&lt;h4&gt;目的&lt;/h4&gt;开发高效的音频检索系统，能够通过文本描述检索相关音频。&lt;h4&gt;方法&lt;/h4&gt;1. 采用双编码器架构，分别编码音频和文本模态；2. 使用对比学习对齐不同模态的表示；3. 应用蒸馏方法；4. 利用大型语言模型进行数据增强，包括回译和LLM混合；5. 引入聚类，增加辅助分类任务进行进一步微调。&lt;h4&gt;主要发现&lt;/h4&gt;最佳单一系统在Clotho开发测试集上达到46.62的mAP@16，四个系统的集成达到48.83的mAP@16。&lt;h4&gt;结论&lt;/h4&gt;通过结合多种先进技术，包括双编码器架构、对比学习、大型语言模型和辅助分类任务，能够有效提高基于语言的音频检索性能。&lt;h4&gt;翻译&lt;/h4&gt;本报告介绍了AISTAT团队对DCASE 2025任务6中基于语言的音频检索任务的提交内容。我们提出的系统采用双编码器架构，其中音频和文本模态分别编码，并通过对比学习对齐它们的表示。受上一年度挑战方法的启发，我们实现了蒸馏方法，并利用大型语言模型进行有效的数据增强技术，包括回译和LLM混合。此外，我们还引入了聚类，增加辅助分类任务进行进一步微调。我们最佳的单个系统在Clotho开发测试集上达到了46.62的mAP@16，而四个系统的集成在Clotho开发测试集上达到了48.83的mAP@16。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This report presents the AISTAT team's submission to the language-based audioretrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoderarchitecture, where audio and text modalities are encoded separately, and theirrepresentations are aligned using contrastive learning. Drawing inspirationfrom methodologies of the previous year's challenge, we implemented adistillation approach and leveraged large language models (LLMs) for effectivedata augmentation techniques, including back-translation and LLM mix.Additionally, we incorporated clustering to introduce an auxiliaryclassification task for further finetuning. Our best single system achieved amAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 onthe Clotho development test split.</description>
      <author>example@mail.com (Hyun Jun Kim, Hyeong Yong Choi, Changwon Lim)</author>
      <guid isPermaLink="false">2509.16649v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains</title>
      <link>http://arxiv.org/abs/2509.16531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多语言作者表征学习的新方法，通过概率内容掩码和语言感知批处理两种技术创新，在多语言作者归属任务中取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;先前的研究主要关注单语言（主要是英语）环境下的作者表征学习，多语言AR模型的潜力未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉多语言作者写作风格的新方法，以提升作者归属任务的性能。&lt;h4&gt;方法&lt;/h4&gt;提出两种技术创新：1)概率内容掩码，使模型关注风格指示词而非内容特定词；2)语言感知批处理，通过减少跨语言干扰改进对比学习。模型在超过450万作者、36种语言和13个领域的数据上训练。&lt;h4&gt;主要发现&lt;/h4&gt;在22种非英语语言中的21种上，模型始终优于单语言基线，平均Recall@8提高4.85%，单个语言最大提高15.91%。相比仅用英语训练的单语言模型，该模型表现出更强的跨语言和跨领域泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的两种技术被证明是有效的，它们在模型性能提升中起着关键作用，为多语言作者表征学习提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;作者表征（AR）学习通过建模作者独特的写作风格，在作者归属任务中展现出强大的性能。然而，先前的研究主要集中在单语言环境（主要是英语），多语言AR模型的潜在优势尚未得到充分探索。我们引入了一种多语言AR学习的新方法，包含两个关键创新：概率内容掩码，鼓励模型关注风格指示词而非内容特定词；以及语言感知批处理，通过减少跨语言干扰来改进对比学习。我们的模型在超过450万作者、36种语言和13个领域上进行训练。在22种非英语语言中的21种上，它始终优于单语言基线，平均Recall@8提高了4.85%，单个语言最大增益达15.91%。此外，与仅用英语训练的单语言模型相比，它表现出更强的跨语言和跨领域泛化能力。我们的分析证实了所提出两种技术的有效性，突显了它们在模型性能提升中的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Authorship representation (AR) learning, which models an author's uniquewriting style, has demonstrated strong performance in authorship attributiontasks. However, prior research has primarily focused on monolingualsettings-mostly in English-leaving the potential benefits of multilingual ARmodels underexplored. We introduce a novel method for multilingual AR learningthat incorporates two key innovations: probabilistic content masking, whichencourages the model to focus on stylistically indicative words rather thancontent-specific words, and language-aware batching, which improves contrastivelearning by reducing cross-lingual interference. Our model is trained on over4.5 million authors across 36 languages and 13 domains. It consistentlyoutperforms monolingual baselines in 21 out of 22 non-English languages,achieving an average Recall@8 improvement of 4.85%, with a maximum gain of15.91% in a single language. Furthermore, it exhibits stronger cross-lingualand cross-domain generalization compared to a monolingual model trained solelyon English. Our analysis confirms the effectiveness of both proposedtechniques, highlighting their critical roles in the model's improvedperformance.</description>
      <author>example@mail.com (Junghwan Kim, Haotian Zhang, David Jurgens)</author>
      <guid isPermaLink="false">2509.16531v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>CoUn: Empowering Machine Unlearning via Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.16391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoUn是一种新的机器遗忘框架，通过对比学习和监督学习调整数据表示，实现了比现有方法更好的遗忘效果。&lt;h4&gt;背景&lt;/h4&gt;机器遗忘(MU)旨在从训练好的模型中移除特定'遗忘'数据的影响，同时保留其对剩余'保留'数据的知识。现有的基于标签操作或模型权重扰动的MU方法通常实现有限的遗忘效果。&lt;h4&gt;目的&lt;/h4&gt;提出一种更有效的MU方法，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入CoUn，一种新的MU框架。它基于一个观察：仅使用保留数据从头开始重新训练的模型会根据遗忘数据与保留数据的语义相似性来分类遗忘数据。CoUn通过对比学习(CL)和监督学习(仅应用于保留数据)来调整学习的数据表示，模拟这种行为。具体来说，CoUn(1)利用数据样本之间的语义相似性，通过CL间接调整遗忘表示；(2)通过监督学习将保留表示保持在各自的簇内。&lt;h4&gt;主要发现&lt;/h4&gt;在各种数据集和模型架构上的大量实验表明，CoUn在遗忘有效性方面始终优于最先进的MU基线。此外，将CL模块集成到现有基线中也能增强它们的遗忘有效性。&lt;h4&gt;结论&lt;/h4&gt;CoUn是一种有效的MU框架，通过对比学习和监督学习来调整数据表示，实现了比现有方法更好的遗忘效果。&lt;h4&gt;翻译&lt;/h4&gt;机器遗忘(MU)旨在从训练好的模型中移除特定'遗忘'数据的影响，同时保留其对剩余'保留'数据的知识。现有的基于标签操作或模型权重扰动的MU方法通常实现有限的遗忘效果。为此，我们引入了CoUn，一种新的MU框架，它基于一个观察：仅使用保留数据从头开始重新训练的模型会根据遗忘数据与保留数据的语义相似性来分类遗忘数据。CoUn通过对比学习(CL)和监督学习(仅应用于保留数据)来调整学习的数据表示，模拟这种行为。具体来说，CoUn(1)利用数据样本之间的语义相似性，通过CL间接调整遗忘表示；(2)通过监督学习将保留表示保持在各自的簇内。在各种数据集和模型架构上的大量实验表明，CoUn在遗忘有效性方面始终优于最先进的MU基线。此外，将我们的CL模块集成到现有基线中也能增强它们的遗忘有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine unlearning (MU) aims to remove the influence of specific "forget"data from a trained model while preserving its knowledge of the remaining"retain" data. Existing MU methods based on label manipulation or model weightperturbations often achieve limited unlearning effectiveness. To address this,we introduce CoUn, a novel MU framework inspired by the observation that amodel retrained from scratch using only retain data classifies forget databased on their semantic similarity to the retain data. CoUn emulates thisbehavior by adjusting learned data representations through contrastive learning(CL) and supervised learning, applied exclusively to retain data. Specifically,CoUn (1) leverages semantic similarity between data samples to indirectlyadjust forget representations using CL, and (2) maintains retainrepresentations within their respective clusters through supervised learning.Extensive experiments across various datasets and model architectures show thatCoUn consistently outperforms state-of-the-art MU baselines in unlearningeffectiveness. Additionally, integrating our CL module into existing baselinesempowers their unlearning effectiveness.</description>
      <author>example@mail.com (Yasser H. Khalil, Mehdi Setayesh, Hongliang Li)</author>
      <guid isPermaLink="false">2509.16391v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2509.14181v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TimeAlign是一种新的时间序列预测表示学习框架，通过重建任务和对齐机制，有效解决了历史输入与未来输出之间的分布差距问题，在多个基准测试上表现出色。&lt;h4&gt;背景&lt;/h4&gt;对比学习和其他表示学习方法在视觉和自然语言处理中早已被探索，但它们在现代时间序列预测器中的采用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;释放表示学习方法在时间序列预测领域的潜力，通过明确对齐过去和未来的表示，弥合输入历史和未来目标之间的分布差距。&lt;h4&gt;方法&lt;/h4&gt;引入TimeAlign，一个轻量级、即插即用的框架，通过简单的重建任务对齐辅助特征，并将其反馈到任何基础预测器中，建立了一种不同于对比学习的新表示范式。&lt;h4&gt;主要发现&lt;/h4&gt;在八个基准测试上的广泛实验验证了其卓越性能；收益主要来自于纠正历史输入和未来输出之间的频率不匹配；提供了两个理论证明：重建如何提高预测泛化能力，以及对齐如何增加学习表示与预测目标之间的互信息。&lt;h4&gt;结论&lt;/h4&gt;表示学习方法（特别是TimeAlign框架）在时间序列预测领域具有巨大潜力，通过重建任务和对齐机制有效解决了历史输入与未来输出之间的分布差距和频率不匹配问题。&lt;h4&gt;翻译&lt;/h4&gt;尽管对比学习和其他表示学习方法在视觉和自然语言处理中早已被探索，但它们在现代时间序列预测器中的采用仍然有限。我们相信它们在这个领域具有很强的潜力。为了释放这种潜力，我们明确地对齐过去和未来的表示，从而弥合输入历史和未来目标之间的分布差距。为此，我们引入了TimeAlign，一个轻量级、即插即用的框架，它通过简单的重建任务对齐辅助特征，并将其反馈到任何基础预测器中，建立了一种不同于对比学习的新表示范式。在八个基准测试上的广泛实验验证了其卓越性能。进一步研究表明，收益主要来自于纠正历史输入和未来输出之间的频率不匹配。此外，我们提供了两个理论证明：重建如何提高预测泛化能力，以及对齐如何增加学习表示与预测目标之间的互信息。代码可在https://github.com/TROUBADOUR000/TimeAlign获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although contrastive and other representation-learning methods have long beenexplored in vision and NLP, their adoption in modern time series forecastersremains limited. We believe they hold strong promise for this domain. To unlockthis potential, we explicitly align past and future representations, therebybridging the distributional gap between input histories and future targets. Tothis end, we introduce TimeAlign, a lightweight, plug-and-play framework thatestablishes a new representation paradigm, distinct from contrastive learning,by aligning auxiliary features via a simple reconstruction task and feedingthem back into any base forecaster. Extensive experiments across eightbenchmarks verify its superior performance. Further studies indicate that thegains arise primarily from correcting frequency mismatches between historicalinputs and future outputs. Additionally, we provide two theoreticaljustifications for how reconstruction improves forecasting generalization andhow alignment increases the mutual information between learned representationsand predicted targets. The code is available athttps://github.com/TROUBADOUR000/TimeAlign.</description>
      <author>example@mail.com (Yifan Hu, Jie Yang, Tian Zhou, Peiyuan Liu, Yujin Tang, Rong Jin, Liang Sun)</author>
      <guid isPermaLink="false">2509.14181v2</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series</title>
      <link>http://arxiv.org/abs/2509.17845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于类卷积ScaleFusion Transformer的多尺度表示学习框架，解决了时间序列分析中处理可变长度数据和实现稳健泛化的挑战。&lt;h4&gt;背景&lt;/h4&gt;时间序列分析在处理可变长度数据和实现稳健泛化方面面临重大挑战。基于Transformer的模型虽然在时间序列任务上有所进步，但它们通常面临特征冗余和泛化能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理可变长度时间序列数据并减少特征冗余的新框架，提升预测和分类任务的性能。&lt;h4&gt;方法&lt;/h4&gt;引入类似时间卷积的结构结合patch操作与多头注意力；开发跨尺度注意力机制实现不同时间尺度的特征融合；提出对数空间归一化方法处理可变长度序列。&lt;h4&gt;主要发现&lt;/h4&gt;所提框架实现了更好的特征独立性，减少了特征冗余，在预测和分类任务中表现优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;基于类卷积ScaleFusion Transformer的多尺度表示学习框架能有效解决时间序列分析中的关键挑战，特别是在处理可变长度数据和提升泛化能力方面。&lt;h4&gt;翻译&lt;/h4&gt;时间序列分析在处理可变长度数据和实现稳健泛化方面面临重大挑战。虽然基于Transformer的模型在推进时间序列任务方面有所进展，但它们通常难以处理特征冗余和有限的泛化能力。受经典CNN架构金字塔结构的启发，我们提出了一种基于类卷积ScaleFusion Transformer的多尺度表示学习框架。我们的方法引入了一种类似时间卷积的结构，将patch操作与多头注意力相结合，实现了渐进式时间维度压缩和特征通道扩展。我们进一步开发了一种新颖的跨尺度注意力机制，用于在不同时间尺度上进行有效的特征融合，以及一种用于可变长度序列的对数空间归一化方法。大量实验证明，我们的框架在特征独立性、减少冗余以及预测和分类任务性能方面优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series analysis faces significant challenges in handling variable-lengthdata and achieving robust generalization. While Transformer-based models haveadvanced time series tasks, they often struggle with feature redundancy andlimited generalization capabilities. Drawing inspiration from classical CNNarchitectures' pyramidal structure, we propose a Multi-Scale RepresentationLearning Framework based on a Conv-like ScaleFusion Transformer. Our approachintroduces a temporal convolution-like structure that combines patchingoperations with multi-head attention, enabling progressive temporal dimensioncompression and feature channel expansion. We further develop a novelcross-scale attention mechanism for effective feature fusion across differenttemporal scales, along with a log-space normalization method forvariable-length sequences. Extensive experiments demonstrate that our frameworkachieves superior feature independence, reduced redundancy, and betterperformance in forecasting and classification tasks compared tostate-of-the-art methods.</description>
      <author>example@mail.com (Kai Zhang, Siming Sun, Zhengyu Fan, Qinmin Yang, Xuejun Jiang)</author>
      <guid isPermaLink="false">2509.17845v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>MesoNet: A Fundamental Principle for Multi-Representation Learning in Complex Chemical Systems</title>
      <link>http://arxiv.org/abs/2509.17810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MesoNet是一种基于多表示学习原则的新型框架，专为多分子建模而设计，通过上下文感知的表示和交叉注意力机制有效预测复杂化学系统中的分子特性。&lt;h4&gt;背景&lt;/h4&gt;准确预测复杂化学系统中的分子特性对加速材料发现和化学创新至关重要，但当前计算方法难以捕捉从分子内键到分子间力的复杂组成相互作用。&lt;h4&gt;目的&lt;/h4&gt;引入MesoNet框架，解决复杂化学系统中分子特性预测的挑战，提高预测准确性和化学可解释性。&lt;h4&gt;方法&lt;/h4&gt;MesoNet的核心创新在于构建上下文感知表示，通过神经电路策略动态生成原子描述符，利用跨越分子内和分子间消息传递的交叉注意力机制，逐步将混合系统的影响应用于每个分子和原子。&lt;h4&gt;主要发现&lt;/h4&gt;在涵盖纯组分和混合物的多种公共数据集上全面评估显示，MesoNet在分子特性预测方面实现了卓越的准确性和增强的化学可解释性。&lt;h4&gt;结论&lt;/h4&gt;MesoNet为建模组成复杂性建立了一种强大、可解释的方法，旨在推进化学模拟和设计领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;准确预测复杂化学系统中的分子特性对于加速材料发现和化学创新至关重要。然而，当前的计算方法往往难以捕捉复杂化学系统中从分子内键到分子间力的复杂组成相互作用。在这项工作中，我们引入了MesoNet，一种基于多表示学习原则的新型框架，专门为多分子建模而设计。MesoNet的核心创新在于构建上下文感知表示——通过神经电路策略动态生成的原子描述符。这些参数通过跨越分子内和分子间消息传递的交叉注意力机制，有效捕获了原子的内在属性及其动态组成上下文。通过这种机制，混合系统的影响逐步应用于每个分子和原子，使消息传递既高效又有意义。在涵盖纯组分和混合物的各种公共数据集上进行全面评估表明，MesoNet在分子特性预测方面实现了卓越的准确性和增强的化学可解释性。这项工作为建模组成复杂性建立了一种强大、可解释的方法，旨在推进化学模拟和设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of molecular properties in complex chemical systems iscrucial for accelerating material discovery and chemicalinnovation.However,current computational methods often struggle to capture theintricate compositional interplay across complex chemical systems,fromintramolecular bonds to intermolecular forces.In this work,we introduceMesoNet,a novel framework founded on the principle of multi-representationlearning and specifically designed for multi-molecule modeling.The coreinnovation of MesoNet lies in the construction of context-awarerepresentation-dynamically enriched atomic descriptors generated via NeuralCircuit Policies. These parameters efficiently capture both intrinsic atomicproperties and their dynamic compositional context through a cross-attentionmechanism spanning both intramolecular and intermolecular message passing.Driven by this mechanism,the influence of the mixed system is progressivelyapplied to each molecule and atom, making message passing both efficient andmeaningful.Comprehensive evaluations across diverse public datasets, spanningboth pure components and mixtures,demonstrate that MesoNet achieves superioraccuracy and enhanced chemical interpretability for molecular properties.Thiswork establishes a powerful,interpretable approach for modeling compositionalcomplexity,aiming to advance chemical simulation and design.</description>
      <author>example@mail.com (Jinming Fan, Chao Qian, Shaodong Zhou)</author>
      <guid isPermaLink="false">2509.17810v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>A Generative Framework for Personalized Sticker Retrieval</title>
      <link>http://arxiv.org/abs/2509.17749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Findings of EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PEARL，一种用于个性化贴纸检索的新型生成框架，通过学习用户表示和意图感知学习目标来解决现有方法缺乏个性化的挑战，实验证明其性能显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;将信息检索作为生成式建模的一种变体，特别是使用自回归模型为查询生成相关标识符，最近受到广泛关注。然而，这种方法在个性化贴纸检索领域的应用仍处于探索阶段。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于相关性的生成检索方法缺乏个性化的问题，满足用户多样化期望与检索结果之间的匹配需求。&lt;h4&gt;方法&lt;/h4&gt;设计PEARL框架，包含两个关键贡献：(1) 开发表示学习模型编码用户特定贴纸偏好，利用个人信息和点击历史在三个预测任务上训练；(2) 提出意图感知学习目标，生成与用户查询意图一致的贴纸，优先考虑高排名意图相关的贴纸。&lt;h4&gt;主要发现&lt;/h4&gt;离线评估和在线测试的结果表明，PEARL显著优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;PEARL框架有效解决了个性化贴纸检索中的挑战，能够提供更符合用户期望的检索结果。&lt;h4&gt;翻译&lt;/h4&gt;将信息检索表述为生成式建模的一种变体，特别是使用自回归模型为给定查询生成相关标识符，最近引起了相当大的关注。然而，将其应用于个性化贴纸检索在很大程度上尚未被探索，并带来了独特的挑战：现有的基于相关性的生成检索方法通常缺乏个性化，导致用户多样化期望与检索结果之间存在不匹配。为了解决这一差距，我们提出了PEARL，一种用于个性化贴纸检索的新型生成框架，并做出了两个关键贡献：(i) 为了编码用户特定的贴纸偏好，我们设计了一个表示学习模型来学习判别性用户表示。该模型在三个预测任务上进行训练，这些任务利用个人信息和点击历史；(ii) 为了生成与用户查询意图一致的贴纸，我们提出了一种新颖的意图感知学习目标，优先考虑与更高排名意图相关的贴纸。离线评估和在线测试的经验结果表明，PEARL显著优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Formulating information retrieval as a variant of generative modeling,specifically using autoregressive models to generate relevant identifiers for agiven query, has recently attracted considerable attention. However, itsapplication to personalized sticker retrieval remains largely unexplored andpresents unique challenges: existing relevance-based generative retrievalmethods typically lack personalization, leading to a mismatch between diverseuser expectations and the retrieved results. To address this gap, we proposePEARL, a novel generative framework for personalized sticker retrieval, andmake two key contributions: (i) To encode user-specific sticker preferences, wedesign a representation learning model to learn discriminative userrepresentations. It is trained on three prediction tasks that leverage personalinformation and click history; and (ii) To generate stickers aligned with auser's query intent, we propose a novel intent-aware learning objective thatprioritizes stickers associated with higher-ranked intents. Empirical resultsfrom both offline evaluations and online tests demonstrate that PEARLsignificantly outperforms state-of-the-art methods.</description>
      <author>example@mail.com (Changjiang Zhou, Ruqing Zhang, Jiafeng Guo, Yu-An Liu, Fan Zhang, Ganyuan Luo, Xueqi Cheng)</author>
      <guid isPermaLink="false">2509.17749v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>TACTFL: Temporal Contrastive Training for Multi-modal Federated Learning with Similarity-guided Model Aggregation</title>
      <link>http://arxiv.org/abs/2509.17532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TACTFL框架，解决了联邦学习中标记数据有限和多模态输入异构性的问题，通过时间对比训练和相似度引导的模型聚合策略实现了半监督多模态联邦学习，在多个基准测试中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现实世界的联邦学习面临两个关键挑战：标记数据有限和多模态输入异构性。&lt;h4&gt;目的&lt;/h4&gt;提出TACTFL框架，用于半监督多模态联邦学习，以解决上述挑战。&lt;h4&gt;方法&lt;/h4&gt;TACTFL引入了与模态无关的时间对比训练方案，利用跨模态时间对从未标记数据中进行表示学习；同时采用相似度引导的模型聚合策略，基于表示一致性动态加权客户端模型，促进全局对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在视频、音频和可穿戴传感器等多种基准测试和模态上的广泛实验表明，TACTFL取得了最先进的性能。在UCF101数据集上仅使用10%的标记数据，TACTFL达到了68.48%的top-1准确率，显著优于FedOpt基线模型的35.35%。&lt;h4&gt;结论&lt;/h4&gt;TACTFL框架有效解决了联邦学习中标记数据有限和多模态输入异构性的挑战，通过创新的训练和聚合策略实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;现实世界的联邦学习面临两个关键挑战：标记数据有限和多模态输入异构性。本文提出了TACTFL，一个用于半监督多模态联邦学习的统一框架。TACTFL引入了一种与模态无关的时间对比训练方案，通过利用跨模态的时间对从未标记的客户端数据进行表示学习。然而，当客户端在异构数据上进行自监督训练时，本地模型可能在语义上出现分歧。为了缓解这一问题，TACTFL结合了相似度引导的模型聚合策略，根据模型的表示一致性动态加权客户端模型，促进全局对齐。在包括视频、音频和可穿戴传感器在内的多样化基准测试和模态上的广泛实验表明，TACTFL取得了最先进的性能。例如，在UCF101数据集上仅使用10%的标记数据，TACTFL达到了68.48%的top-1准确率，显著优于FedOpt基线模型的35.35%。代码将在发表后公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world federated learning faces two key challenges: limited access tolabelled data and the presence of heterogeneous multi-modal inputs. This paperproposes TACTFL, a unified framework for semi-supervised multi-modal federatedlearning. TACTFL introduces a modality-agnostic temporal contrastive trainingscheme that conducts representation learning from unlabelled client data byleveraging temporal alignment across modalities. However, as clients performself-supervised training on heterogeneous data, local models may divergesemantically. To mitigate this, TACTFL incorporates a similarity-guided modelaggregation strategy that dynamically weights client models based on theirrepresentational consistency, promoting global alignment. Extensive experimentsacross diverse benchmarks and modalities, including video, audio, and wearablesensors, demonstrate that TACTFL achieves state-of-the-art performance. Forinstance, on the UCF101 dataset with only 10% labelled data, TACTFL attains68.48% top-1 accuracy, significantly outperforming the FedOpt baseline of35.35%. Code will be released upon publication.</description>
      <author>example@mail.com (Guanxiong Sun, Majid Mirmehdi, Zahraa Abdallah, Raul Santos-Rodriguez, Ian Craddock, Telmo de Menezes e Silva Filho)</author>
      <guid isPermaLink="false">2509.17532v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models</title>
      <link>http://arxiv.org/abs/2509.17523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了通过引入有限视觉引导来减少双语语音自监督学习模型中多语言性能差距的方法，结果显示视觉引导显著改善了模型性能，特别是在双语场景中。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在语音表示学习中取得了显著进展，如wav2vec 2.0和HuBERT等模型在单语语音识别任务中达到了最先进水平。&lt;h4&gt;目的&lt;/h4&gt;研究一种新方法来减少多语言SSL模型与单语模型之间的性能差距，特别是在双语场景中。&lt;h4&gt;方法&lt;/h4&gt;在双语语音SSL模型中引入有限的视觉引导，结合视觉信息进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;视觉引导对单语和双语模型都有益处，对双语模型的改善尤其明显，将多语言性能差距从仅使用音频模型的31.5%减少到有引导情况下的8.04%（在零音素辨别任务上）。&lt;h4&gt;结论&lt;/h4&gt;视觉引导可以有效提升多语言语音SSL模型的性能，特别是在双语场景中，能够显著缩小多语言与单语模型之间的性能差距。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习在语音表示学习中已取得显著进展。wav2vec 2.0和HuBERT等模型在语音识别等任务中取得了最先进的结果，特别是在单语设置中。然而，多语言SSL模型在每种单独语言上的表现往往不如其单语对应模型，特别是在双语等少数语言场景中。在本工作中，我们研究了一种新方法来减少这种性能差距，通过在双语语音SSL模型中引入有限的视觉引导。我们的结果表明，视觉引导对单语和双语模型都有益处，对后者的改善尤为明显，将多语言在零样本音素辨别任务上的性能差距从仅使用音频模型的31.5%减少到有引导情况下的8.04%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has made significant advances in speechrepresentation learning. Models like wav2vec 2.0 and HuBERT have achievedstate-of-the-art results in tasks such as speech recognition, particularly inmonolingual settings. However, multilingual SSL models tend to underperformtheir monolingual counterparts on each individual language, especially inmultilingual scenarios with few languages such as the bilingual setting. Inthis work, we investigate a novel approach to reduce this performance gap byintroducing limited visual grounding into bilingual speech SSL models. Ourresults show that visual grounding benefits both monolingual and bilingualmodels, with especially pronounced gains for the latter, reducing themultilingual performance gap on zero-shot phonetic discrimination from 31.5%for audio-only models to 8.04% with grounding.</description>
      <author>example@mail.com (María Andrea Cruz Blandón, Zakaria Aldeneh, Jie Chi, Maureen de Seyssel)</author>
      <guid isPermaLink="false">2509.17523v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>STAR: Speech-to-Audio Generation via Representation Learning</title>
      <link>http://arxiv.org/abs/2509.17164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了STAR，首个端到端的语音到音频生成框架，旨在提高效率并解决级联系统中固有的错误传播问题。&lt;h4&gt;背景&lt;/h4&gt;现有方法通常依赖于文本或视觉信息，而STAR利用语音作为交互的自然模态。&lt;h4&gt;目的&lt;/h4&gt;验证使用语音作为直接交互信号进行音频生成的可行性，并提高生成效率。&lt;h4&gt;方法&lt;/h4&gt;通过表示学习实验证明可以从原始语音中有效提取口语声音事件的语义，包含一个表示映射的桥接网络，并采用两阶段训练策略实现端到端合成。&lt;h4&gt;主要发现&lt;/h4&gt;STAR实现了76.9%的语音处理延迟减少，在生成性能上优于级联系统。&lt;h4&gt;结论&lt;/h4&gt;STAR建立了语音作为音频生成的直接交互信号，连接了表示学习和多模态合成。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了STAR，这是第一个端到端的语音到音频生成框架，旨在提高效率并解决级联系统中固有的错误传播问题。与依赖文本或视觉的先前方法不同，STAR利用语音作为交互的自然模态。作为验证系统可行性的初步步骤，我们通过表示学习实验证明，口语声音事件的语义可以从原始语音中有效提取，捕获听觉事件和场景线索。利用这些语义表示，STAR集成了一个用于表示映射的桥接网络和两阶段训练策略，以实现端到端合成。STAR实现了76.9%的语音处理延迟减少，展示了优于级联系统的生成性能。总体而言，STAR建立了语音作为音频生成的直接交互信号，从而连接了表示学习和多模态合成。生成的样本可在https://zeyuxie29.github.io/STAR获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents STAR, the first end-to-end speech-to-audio generationframework, designed to enhance efficiency and address error propagationinherent in cascaded systems. Unlike prior approaches relying on text orvision, STAR leverages speech as it constitutes a natural modality forinteraction. As an initial step to validate the feasibility of the system, wedemonstrate through representation learning experiments that spoken sound eventsemantics can be effectively extracted from raw speech, capturing both auditoryevents and scene cues. Leveraging the semantic representations, STARincorporates a bridge network for representation mapping and a two-stagetraining strategy to achieve end-to-end synthesis. With a 76.9% reduction inspeech processing latency, STAR demonstrates superior generation performanceover the cascaded systems. Overall, STAR establishes speech as a directinteraction signal for audio generation, thereby bridging representationlearning and multimodal synthesis. Generated samples are available athttps://zeyuxie29.github.io/STAR.</description>
      <author>example@mail.com (Zeyu Xie, Xuenan Xu, Yixuan Li, Mengyue Wu, Yuexian Zou)</author>
      <guid isPermaLink="false">2509.17164v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning</title>
      <link>http://arxiv.org/abs/2509.16892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoMTIP是一种新型的对比掩码文本-图像预训练框架，能够同时从图像、基因名称和表达值中学习，捕获空间转录组学的细粒度视觉上下文，实现了零样本基因表达预测，超越了现有方法在多种下游任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学旨在连接高分辨率组织学图像与空间分辨的基因表达。现有跨模态预训练方法仅依赖基因名称或表达值，剥离了基因分支的必要语义，破坏了基因与其定量值之间的关联，且仅通过图像-文本对齐进行监督，忽略了学习鲁棒图像特征的关键视觉线索。&lt;h4&gt;目的&lt;/h4&gt;实现更好的下游任务性能（如基因表达预测），获得可推广的表示，能够跨越组织、协议和实验室连接组织学和转录组学。&lt;h4&gt;方法&lt;/h4&gt;提出CoMTIP框架，视觉分支使用掩码特征建模重建遮挡块并学习上下文感知的图像嵌入；文本分支应用可扩展的基因-文本编码器并行处理所有基因句子，为每个基因及其数值添加专门嵌入，采用成对感知对抗训练(PAAT)保持正确的基因-值关联；图像和文本表示在共享的InfoNCE优化空间中对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在公共空间转录组学数据集上的实验表明，CoMTIP不仅在多种下游任务上超越了先前的方法，还实现了零样本基因表达预测，这是现有方法无法提供的功能。&lt;h4&gt;结论&lt;/h4&gt;CoMTIP有效地结合了图像、基因名称和表达值信息，同时保留了重要的视觉上下文和基因-值关联，为空间转录组学分析提供了更强大的预训练框架。&lt;h4&gt;翻译&lt;/h4&gt;空间转录组学旨在将高分辨率组织学图像与空间分辨的基因表达联系起来。为了在基因表达预测等下游任务上实现更好的性能，需要进行大规模预训练以获得可推广的表示，这些表示能够跨越组织、协议和实验室连接组织学和转录组学。现有的空间转录组学跨模态预训练方法要么单独依赖基因名称，要么单独依赖表达值，这剥离了基因分支的必要语义，并破坏了每个基因与其定量值之间的关联。此外，通过将监督限制在图像-文本对齐，这些方法忽略了学习鲁棒图像特征的关键内在视觉线索。我们提出了CoMTIP，这是第一个对比掩码文本-图像预训练框架，它同时从图像、基因名称和表达值中学习，同时捕获空间转录组学的细粒度视觉上下文。视觉分支使用掩码特征建模来重建遮挡的块并学习上下文感知的图像嵌入。文本分支应用可扩展的基因-文本编码器，并行处理所有基因句子，为每个基因及其数值添加专门的嵌入，并采用成对感知对抗训练(PAAT)来保持正确的基因-值关联。图像和文本表示在共享的InfoNCE优化空间中对齐。在公共空间转录组学数据集上的实验表明，CoMTIP不仅在多种下游任务上超越了先前的方法，还实现了零样本基因表达预测，这是现有方法无法提供的功能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial transcriptomics aims to connect high-resolution histology images withspatially resolved gene expression. To achieve better performance on downstreamtasks such as gene expression prediction, large-scale pre-training is requiredto obtain generalisable representations that can bridge histology andtranscriptomics across tissues, protocols, and laboratories. Existingcross-modal pre-training approaches for spatial transcriptomics rely on eithergene names or expression values in isolation, which strips the gene branch ofessential semantics and breaks the association between each gene and itsquantitative magnitude. In addition, by restricting supervision to image-textalignment, these methods ignore intrinsic visual cues that are critical forlearning robust image features. We present CoMTIP, the first Contrastive MaskedText-Image Pretraining framework that jointly learns from images, gene names,and expression values while capturing fine-grained visual context for spatialtranscriptomics. The vision branch uses Masked Feature Modeling to reconstructoccluded patches and learn context-aware image embeddings. The text branchapplies a scalable Gene-Text Encoder that processes all gene sentences inparallel, enriches each gene and its numerical value with dedicated embeddings,and employs Pair-aware Adversarial Training (PAAT) to preserve correctgene-value associations. Image and text representations are aligned in a sharedInfoNCE-optimised space. Experiments on public spatial transcriptomics datasetsshow that CoMTIP not only surpasses previous methods on diverse downstreamtasks but also achieves zero-shot gene expression prediction, a capability thatexisting approaches do not provide.</description>
      <author>example@mail.com (Jiahe Qian, Yaoyu Fang, Ziqiao Weng, Xinkun Wang, Lee A. Cooper, Bo Zhou)</author>
      <guid isPermaLink="false">2509.16892v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>$\boldsymbolλ$-Orthogonality Regularization for Compatible Representation Learning</title>
      <link>http://arxiv.org/abs/2509.16664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合仿射变换和松散正交性约束的方法，用于检索系统中不同模型表示的适配，既保持了特定分布的适应性，又保留了原始学习的表示空间。&lt;h4&gt;背景&lt;/h4&gt;检索系统依赖于越来越强大的模型学习的表示。由于训练成本高和表示学习中的不一致性，促进不同表示之间的通信并确保独立训练的神经网络之间的兼容性具有重要意义。目前常用的两种方法是仿射变换和正交变换，但各有局限。&lt;h4&gt;目的&lt;/h4&gt;解决如何将更新模型的潜在空间与先前模型在下游分布上的潜在空间对齐，同时保留新学习的表示空间这一关键挑战。&lt;h4&gt;方法&lt;/h4&gt;在学习仿射变换的同时，施加一个松散的正交性约束，即λ-正交性正则化，以获得特定分布的适应，同时保留原始学习的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在各种架构和数据集的大量实验表明，该方法保留了模型的零样本性能，并确保了模型更新之间的兼容性。&lt;h4&gt;结论&lt;/h4&gt;提出的λ-正交性正则化方法有效解决了模型表示适配的问题，在保持模型性能的同时实现了不同模型版本之间的兼容性。&lt;h4&gt;翻译&lt;/h4&gt;检索系统依赖于由越来越强大的模型学习的表示。然而，由于高昂的训练成本和学习表示中的不一致性，促进表示之间的通信并确保独立训练的神经网络之间的兼容性引起了人们的极大兴趣。在文献中，通常使用两种主要方法来适应不同的学习表示：仿射变换，能很好地适应特定分布但可能显著改变原始表示；以及正交变换，通过严格的几何约束保留原始结构但限制了适应性。一个关键挑战是将更新模型的潜在空间与先前模型在下游分布上的潜在空间对齐，同时保留新学习的表示空间。在本文中，我们在学习仿射变换的同时施加松散的正交性约束，即λ-正交性正则化，以获得特定分布的适应同时保留原始学习的表示。各种架构和数据集的大量实验验证了我们的方法，证明它保留了模型的零样本性能并确保了模型更新之间的兼容性。代码可在https://github.com/miccunifi/lambda_orthogonality获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval systems rely on representations learned by increasingly powerfulmodels. However, due to the high training cost and inconsistencies in learnedrepresentations, there is significant interest in facilitating communicationbetween representations and ensuring compatibility across independently trainedneural networks. In the literature, two primary approaches are commonly used toadapt different learned representations: affine transformations, which adaptwell to specific distributions but can significantly alter the originalrepresentation, and orthogonal transformations, which preserve the originalstructure with strict geometric constraints but limit adaptability. A keychallenge is adapting the latent spaces of updated models to align with thoseof previous models on downstream distributions while preserving the newlylearned representation spaces. In this paper, we impose a relaxed orthogonalityconstraint, namely $\lambda$-orthogonality regularization, while learning anaffine transformation, to obtain distribution-specific adaptation whileretaining the original learned representations. Extensive experiments acrossvarious architectures and datasets validate our approach, demonstrating that itpreserves the model's zero-shot performance and ensures compatibility acrossmodel updates. Code available at:https://github.com/miccunifi/lambda_orthogonality</description>
      <author>example@mail.com (Simone Ricci, Niccolò Biondi, Federico Pernici, Ioannis Patras, Alberto Del Bimbo)</author>
      <guid isPermaLink="false">2509.16664v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features</title>
      <link>http://arxiv.org/abs/2509.16629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CAPE的新型位置编码方法，解决了传统位置编码方法无法处理非顺序但因果相关特征的现实数据问题。该方法通过识别特征间的因果结构，在双曲空间中嵌入并生成因果感知的位置编码，有效提升了transformer模型处理非顺序特征数据的能力。&lt;h4&gt;背景&lt;/h4&gt;位置编码对于为transformer提供token的位置信息至关重要。然而，现有的位置编码方法需要预定义的token/特征顺序，这使得它们不适合处理现实世界中具有非顺序但因果相关特征的数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理非顺序特征的位置编码方法，解决现有方法在现实世界数据应用中的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出CAPE方法，首先通过广义结构方程建模识别非顺序特征的潜在因果结构并构建为加权的有向无环图(DAG)；然后在双曲空间中嵌入该DAG，保留其几何结构并捕获因果强度和因果特异性两个重要属性；最后将生成的因果感知位置编码转换为旋转形式以与transformer的自注意力机制集成。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，CAPE生成的旋转位置编码具有三个重要属性：因果距离引起的衰减、因果普遍性引起的衰减以及对位置扰动的鲁棒性。实证验证了这些理论属性，并证明了CAPE在处理非顺序特征数据时的有效性。&lt;h4&gt;结论&lt;/h4&gt;CAPE成功解决了传统位置编码方法在处理非顺序特征数据时的局限性，通过引入因果感知的位置编码，显著提升了transformer模型处理现实世界非顺序特征数据的能力。&lt;h4&gt;翻译&lt;/h4&gt;位置编码对于为transformer提供token的位置信息是必不可少的。现有的位置编码方法需要预定义的token/特征顺序，这使得它们不适合处理具有非顺序但因果相关特征的现实世界数据。为解决这一局限性，我们提出了CAPE，一种新颖的方法，它使用广义结构方程建模识别非顺序特征的潜在因果结构，并将其表示为加权的有向无环图(DAG)。随后，该DAG被嵌入到双曲空间中，使用基于双曲模型的方法保留其几何结构，有效捕获两个重要的因果图属性（因果强度和因果特异性）。这一步为特征生成了因果感知的位置编码，这些编码被转换为旋转形式以与transformer的自注意力机制集成。理论分析表明，CAPE生成的旋转位置编码具有三个增强自注意力的宝贵属性，包括因果距离引起的衰减、因果普遍性引起的衰减以及对位置扰动的鲁棒性。我们在合成和真实世界数据集上评估了CAPE，实证证明了其理论属性和有效性，证明了CAPE能够增强transformer处理非顺序特征数据的能力。我们的代码可在https://github.com/Catchxu/CAPE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Positional encoding is essential for supplementing transformer withpositional information of tokens. Existing positional encoding methods demandpredefined token/feature order, rendering them unsuitable for real-world datawith non-sequential yet causally-related features. To address this limitation,we propose CAPE, a novel method that identifies underlying causal structureover non-sequential features as a weighted directed acyclic graph (DAG) usinggeneralized structural equation modeling. The DAG is then embedded inhyperbolic space where its geometric structure is well-preserved using ahyperboloid model-based approach that effectively captures two important causalgraph properties (causal strength &amp; causal specificity). This step yieldscausality-aware positional encodings for the features, which are converted intotheir rotary form for integrating with transformer's self-attention mechanism.Theoretical analysis reveals that CAPE-generated rotary positional encodingspossess three valuable properties for enhanced self-attention, including causaldistance-induced attenuation, causal generality-induced attenuation, androbustness to positional disturbances. We evaluate CAPE over both synthetic andreal-word datasets, empirically demonstrating its theoretical properties andeffectiveness in enhancing transformer for data with non-sequential features.Our code is available at https://github.com/Catchxu/CAPE.</description>
      <author>example@mail.com (Kaichen Xu, Yihang Du, Mianpeng Liu, Zimu Yu, Xiaobo Sun)</author>
      <guid isPermaLink="false">2509.16629v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Learning of Graph Representations for Network Intrusion Detection</title>
      <link>http://arxiv.org/abs/2509.16625v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GraphIDS是一种自监督入侵检测模型，通过掩码自编码器学习正常通信模式的局部图表示，将表示学习与异常检测统一起来，在多种NetFlow基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;检测网络流量中的入侵是一个具有挑战性的任务，特别是在有限的监督和不断演变的攻击模式的情况下。现有方法虽然利用图神经网络进行网络入侵检测，但通常将表示学习与异常检测解耦，限制了嵌入在识别攻击方面的效用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够统一表示学习和异常检测阶段的入侵检测模型，使嵌入能够直接优化用于下游任务，从而更好地识别恶意流量。&lt;h4&gt;方法&lt;/h4&gt;GraphIDS使用归纳图神经网络将每个流量及其局部拓扑上下文嵌入，捕获典型网络行为；同时使用基于Transformer的编码器-解码器重建这些嵌入，通过自注意力隐式学习全局共现模式而不需要显式位置信息；在推理过程中，标记具有异常高重建错误的流量为潜在入侵。&lt;h4&gt;主要发现&lt;/h4&gt;这种端到端框架确保嵌入被直接优化用于下游任务，促进恶意流量的识别。在多种NetFlow基准测试中，GraphIDS实现了高达99.98%的PR-AUC和99.61%的宏F1分数，比基线方法高出5-25个百分点。&lt;h4&gt;结论&lt;/h4&gt;GraphIDS是一种有效的自监督入侵检测模型，能够统一表示学习和异常检测阶段，在多种基准测试中表现出色，显著优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;在网络流量中检测入侵是一项具有挑战性的任务，特别是在有限监督和不断演变的攻击模式的情况下。虽然最近的工作已经利用图神经网络进行网络入侵检测，但它们通常将表示学习与异常检测解耦，限制了嵌入在识别攻击方面的效用。我们提出了GraphIDS，这是一种自监督入侵检测模型，通过掩码自编码器学习正常通信模式的局部图表示，将这两个阶段统一起来。归纳图神经网络将每个流量及其局部拓扑上下文嵌入，以捕获典型的网络行为，而基于Transformer的编码器-解码器重建这些嵌入，通过自注意力隐式学习全局共现模式，而不需要显式的位置信息。在推理过程中，具有异常高重建错误的流量被标记为潜在入侵。这种端到端框架确保嵌入被直接优化用于下游任务，促进恶意流量的识别。在不同的NetFlow基准测试中，GraphIDS实现了高达99.98%的PR-AUC和99.61%的宏F1分数，比基线方法高出5-25个百分点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting intrusions in network traffic is a challenging task, particularlyunder limited supervision and constantly evolving attack patterns. While recentworks have leveraged graph neural networks for network intrusion detection,they often decouple representation learning from anomaly detection, limitingthe utility of the embeddings for identifying attacks. We propose GraphIDS, aself-supervised intrusion detection model that unifies these two stages bylearning local graph representations of normal communication patterns through amasked autoencoder. An inductive graph neural network embeds each flow with itslocal topological context to capture typical network behavior, while aTransformer-based encoder-decoder reconstructs these embeddings, implicitlylearning global co-occurrence patterns via self-attention without requiringexplicit positional information. During inference, flows with unusually highreconstruction errors are flagged as potential intrusions. This end-to-endframework ensures that embeddings are directly optimized for the downstreamtask, facilitating the recognition of malicious traffic. On diverse NetFlowbenchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score,outperforming baselines by 5-25 percentage points.</description>
      <author>example@mail.com (Lorenzo Guerra, Thomas Chapuis, Guillaume Duc, Pavlo Mozharovskyi, Van-Tam Nguyen)</author>
      <guid isPermaLink="false">2509.16625v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>TF-DWGNet: A Directed Weighted Graph Neural Network with Tensor Fusion for Multi-Omics Cancer Subtype Classification</title>
      <link>http://arxiv.org/abs/2509.16301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TF-DWGNet是一种创新的图神经网络框架，通过基于树的定向加权图构建和张量融合，解决了多组学数据整合和癌症亚型分类中的关键挑战，提供了卓越的性能和生物学上有意义的见解。&lt;h4&gt;背景&lt;/h4&gt;多组学数据的整合分析为癌症亚型分类提供了有价值的见解，但这些数据本质上具有异质性、高维度，并表现出复杂的模态内和模态间依赖关系。现有图神经网络方法大多依赖先验知识或预定义的相似性网络构建无向或未加权的图，无法捕捉生物相互作用的方向性和强度，且模态和特征层面的可解释性有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在构建图结构方面的局限性，提高多组学数据整合和癌症亚型分类的可解释性，开发一种能够捕捉生物相互作用方向性和强度的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了TF-DWGNet，一种结合基于树的加权有向图构建和张量融合的图神经网络框架，用于多类别癌症亚型分类。该框架包含两个关键创新：1)监督式基于树的方法，为每个组学模态构建定向、加权的图；2)张量融合机制，使用低秩分解捕获单模态、双模态和三模态相互作用，实现模态特定的表示学习、联合嵌入融合和可解释的亚型预测。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界癌症数据集上的实验表明，TF-DWGNet在多个指标和统计测试中始终优于最先进的基线方法。通过对有影响力的特征和模态进行排序，提供了生物学上有意义的见解。&lt;h4&gt;结论&lt;/h4&gt;TF-DWGNet在癌症研究中展现出有效且可解释的多组学整合潜力，为癌症亚型分类提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多组学数据的整合分析为癌症亚型分类提供了有价值的见解。然而，此类数据本质上具有异质性、高维度，并表现出复杂的模态内和模态间依赖关系。图神经网络(GNNs)的最新进展为建模此类结构提供了强大工具。但大多数现有方法依赖先验知识或预定义的相似性网络来构建图，这些图通常是无向或未加权的，无法捕捉生物相互作用的方向性和强度。模态和特征层面的可解释性仍然有限。为解决这些挑战，我们提出了TF-DWGNet，一种新颖的图神经网络框架，结合了基于树的定向加权图构建与张量融合，用于多类别癌症亚型分类。TF-DWGNet引入两个关键创新：一种监督式基于树的方法，用于为每个组学模态构建定向、加权的图；以及一种张量融合机制，使用低秩分解捕获单模态、双模态和三模态相互作用，提高效率。TF-DWGNet实现模态特定的表示学习、联合嵌入融合和可解释的亚型预测。在真实世界癌症数据集上的实验表明，TF-DWGNet在多个指标和统计测试中始终优于最先进的基线方法。此外，通过对有影响力的特征和模态进行排序，它提供了生物学上有意义的见解。这些结果突显了TF-DWGNet在癌症研究中有效且可解释的多组学整合潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integration and analysis of multi-omics data provide valuable insights forcancer subtype classification. However, such data are inherently heterogeneous,high-dimensional, and exhibit complex intra- and inter-modality dependencies.Recent advances in graph neural networks (GNNs) offer powerful tools formodeling such structure. Yet, most existing methods rely on prior knowledge orpredefined similarity networks to construct graphs, which are often undirectedor unweighted, failing to capture the directionality and strength of biologicalinteractions. Interpretability at both the modality and feature levels alsoremains limited. To address these challenges, we propose TF-DWGNet, a novelGraph Neural Network framework that combines tree-based Directed Weighted graphconstruction with Tensor Fusion for multiclass cancer subtype classification.TF-DWGNet introduces two key innovations: a supervised tree-based approach forconstructing directed, weighted graphs tailored to each omics modality, and atensor fusion mechanism that captures unimodal, bimodal, and trimodalinteractions using low-rank decomposition for efficiency. TF-DWGNet enablesmodality-specific representation learning, joint embedding fusion, andinterpretable subtype prediction. Experiments on real-world cancer datasetsshow that TF-DWGNet consistently outperforms state-of-the-art baselines acrossmultiple metrics and statistical tests. Moreover, it provides biologicallymeaningful insights by ranking influential features and modalities. Theseresults highlight TF-DWGNet's potential for effective and interpretablemulti-omics integration in cancer research.</description>
      <author>example@mail.com (Tiantian Yang, Zhiqian Chen)</author>
      <guid isPermaLink="false">2509.16301v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Budgeted Adversarial Attack against Graph-Based Anomaly Detection in Sensor Networks</title>
      <link>http://arxiv.org/abs/2509.17987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BETA的新型灰盒逃避攻击方法，专门针对基于图神经网络的传感器网络异常检测器，通过有限节点扰动显著降低检测准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为传感器网络异常检测的强大模型，特别是在分析多元时间序列数据时表现优异。&lt;h4&gt;目的&lt;/h4&gt;开发一种攻击方法，使攻击者能够从有限节点集中扰动传感器读数（不包括目标传感器），目的是要么抑制真实异常，要么在目标节点触发误报。&lt;h4&gt;方法&lt;/h4&gt;BETA通过识别对目标节点分类最具影响力的传感器，向其特征中注入精心设计的对抗性扰动，同时保持隐蔽性和遵守攻击预算约束。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界传感器网络数据集上，BETA将最先进的基于GNN的检测器的检测准确性平均降低了30.62%至39.16%，并在现实约束条件下显著优于基线攻击策略。&lt;h4&gt;结论&lt;/h4&gt;BETA是一种有效的攻击方法，能够在现实约束条件下显著降低GNN检测器的性能，对基于GNN的异常检测系统构成了重要威胁。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为传感器网络异常检测的强大模型，特别是在分析多元时间序列时。在本工作中，我们引入了BETA，一种针对此类基于GNN的检测器的新型灰盒逃避攻击，其中攻击者被限制只能从有限节点集中扰动传感器读数，不包括目标传感器，目的是要么抑制真实异常，要么在目标节点触发误报。BETA识别对目标节点分类最具影响力的传感器，并向其特征中注入精心设计的对抗性扰动，同时保持隐蔽性并尊重攻击者的预算。在三个真实世界传感器网络数据集上的实验表明，BETA将最先进的基于GNN的检测器的检测准确性平均降低了30.62%至39.16%，并且在现实约束条件下显著优于基线攻击策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful models for anomalydetection in sensor networks, particularly when analyzing multivariate timeseries. In this work, we introduce BETA, a novel grey-box evasion attacktargeting such GNN-based detectors, where the attacker is constrained toperturb sensor readings from a limited set of nodes, excluding the targetsensor, with the goal of either suppressing a true anomaly or triggering afalse alarm at the target node. BETA identifies the sensors most influential tothe target node's classification and injects carefully crafted adversarialperturbations into their features, all while maintaining stealth and respectingthe attacker's budget. Experiments on three real-world sensor network datasetsshow that BETA reduces the detection accuracy of state-of-the-art GNN-baseddetectors by 30.62 to 39.16% on average, and significantly outperforms baselineattack strategies, while operating within realistic constraints.</description>
      <author>example@mail.com (Sanju Xaviar, Omid Ardakanian)</author>
      <guid isPermaLink="false">2509.17987v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Technical overview and architecture of the FastNet Machine Learning weather prediction model, version 1.0</title>
      <link>http://arxiv.org/abs/2509.17658v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FastNet 1.0是一个基于图神经网络架构的数据驱动中期数值天气预报模型，由Alan Turing Institute和Met Office联合开发，可确定性预测全球未来10天的天气。&lt;h4&gt;背景&lt;/h4&gt;数值天气预报领域需要更高效准确的预测模型，传统方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于图神经网络的数据驱动NWP模型，提高中期天气预报的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;使用编码-处理-解码结构的图神经网络，在1°和0.25°分辨率下训练，时间步长为6小时；采用多层网格捕捉大气空间结构的短程和长程模式；在ECMWF的ERA5再分析数据上预训练，然后在额外的自回归展开步骤上微调。&lt;h4&gt;主要发现&lt;/h4&gt;FastNet在各种评估指标和多个大气变量上超越了Met Office全球模型的预测技能；1°和0.25°的FastNet模型都优于当前全球模型，预测技能接近其他在0.25° ERA5上训练的数据驱动模型。&lt;h4&gt;结论&lt;/h4&gt;FastNet 1.0是一种有效的数据驱动中期天气预报模型，在多种分辨率下都能提供高质量的预测结果。&lt;h4&gt;翻译&lt;/h4&gt;我们推出了FastNet 1.0，这是一个基于图神经网络架构的数据驱动中期数值天气预报模型，由Alan Turing Institute和Met Office联合开发。FastNet使用编码-处理-解码结构产生确定性的全球天气预报，可预测未来10天。该架构与空间分辨率无关，我们在1°和0.25°分辨率下训练了模型，时间步长为6小时。FastNet在处理器中使用多层网格，能够捕捉大气空间结构中的短程和长程模式。该模型在ECMWF的ERA5再分析数据上进行预训练，然后在额外的自回归展开步骤上进行微调，提高了长时间预测的准确性。我们使用2022年作为保留年，在1.5°分辨率下评估模型性能，并与Met Office全球模型比较，发现通过多种评估指标和多个大气变量，FastNet超越了当前Met Office全球模型NWP系统的预测技能。我们的结果表明，我们的1°和0.25° FastNet模型都优于当前全球模型，并产生预测技能接近其他在0.25° ERA5上训练的数据驱动模型的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present FastNet version 1.0, a data-driven medium range numerical weatherprediction (NWP) model based on a Graph Neural Network architecture, developedjointly between the Alan Turing Institute and the Met Office. FastNet uses anencode-process-decode structure to produce deterministic global weatherpredictions out to 10 days. The architecture is independent of spatialresolution and we have trained models at 1$^{\circ}$ and 0.25$^{\circ}$resolution, with a six hour time step. FastNet uses a multi-level mesh in theprocessor, which is able to capture both short-range and long-range patterns inthe spatial structure of the atmosphere. The model is pre-trained on ECMWF'sERA5 reanalysis data and then fine-tuned on additional autoregressive rolloutsteps, which improves accuracy over longer time horizons. We evaluate the modelperformance at 1.5$^{\circ}$ resolution using 2022 as a hold-out year andcompare with the Met Office Global Model, finding that FastNet surpasses theskill of the current Met Office Global Model NWP system using a variety ofevaluation metrics on a number of atmospheric variables. Our results show thatboth our 1$^{\circ}$ and 0.25$^{\circ}$ FastNet models outperform the currentGlobal Model and produce results with predictive skill approaching those ofother data-driven models trained on 0.25$^{\circ}$ ERA5.</description>
      <author>example@mail.com (Eric G. Daub, Tom Dunstan, Thusal Bennett, Matthew Burnand, James Chappell, Alejandro Coca-Castro, Noushin Eftekhari, J. Scott Hosking, Manvendra Janmaijaya, Jon Lillis, David Salvador-Jasin, Nathan Simpson, Oliver T Strickson, Ryan Sze-Yin Chan, Mohamad Elmasri, Lydia Allegranza France, Sam Madge, James Robinson, Adam A. Scaife, David Walters, Peter Yatsyshin, Theo McCaie, Levan Bokeria, Hannah Brown, Tom Dodds, David Llewellyn-Jones, Sophia Moreton, Tom Potter, Iain Stenson, Louisa van Zeeland, Karina Bett-Williams, Kirstine Ida Dale)</author>
      <guid isPermaLink="false">2509.17658v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>FastNet: Improving the physical consistency of machine-learning weather prediction models through loss function design</title>
      <link>http://arxiv.org/abs/2509.17601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络(GNN)的全球天气预报模型FastNet，并研究了不同损失函数设计对提高预报物理真实性的影响。通过三种对标准均方误差损失的修改，实现了在保持预测准确性的同时显著提高物理一致性的目标。&lt;h4&gt;背景&lt;/h4&gt;机器学习天气预报(MLWP)模型相比传统数值天气预报(NWP)系统能以显著降低的计算成本提供准确的预报。然而，在确保MLWP输出的物理一致性方面仍存在挑战，特别是在确定性预报设置中。&lt;h4&gt;目的&lt;/h4&gt;研究替代损失函数设计对提高预报物理真实性的影响，并提出FastNet模型，这是一种基于图神经网络(GNN)的全球预测模型。&lt;h4&gt;方法&lt;/h4&gt;探索三种对标准均方误差(MSE)损失的关键修改：(1)修改的球谐(MSH)损失，惩罚谱幅度误差以减少模糊并增强小尺度结构保留；(2)在损失中包含水平梯度项，抑制非物理伪影；(3)替代的风表示，解耦速度和方向以更好地捕捉极端风事件。&lt;h4&gt;主要发现&lt;/h4&gt;MSH和基于梯度的损失单独使用可能会略微降低RMSE分数，但组合训练时，模型表现出与MSE训练模型非常相似的MSE性能，同时显著提高了谱保真度和物理一致性。替代的风表示进一步提高了风速准确性并减少了方向偏差。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了损失函数设计作为将领域知识嵌入MLWP模型机制的重要性，有助于推进这些模型的业务准备状态。&lt;h4&gt;翻译&lt;/h4&gt;机器学习天气预报(MLWP)模型已展现出相比传统数值天气预报(NWP)系统以显著降低的计算成本提供准确预报的巨大潜力。然而，在确保MLWP输出的物理一致性方面仍存在挑战，特别是在确定性设置中。本研究提出了FastNet，一种基于图神经网络(GNN)的全球预测模型，并研究了替代损失函数设计对提高其预报物理真实性的影响。我们探索了对标准均方误差(MSE)损失的三个关键修改：(1)一种修改的球谐(MSH)损失，惩罚谱幅度误差以减少模糊并增强小尺度结构保留；(2)在损失中包含水平梯度项以抑制非物理伪影；(3)一种替代的风表示，解耦速度和方向以更好地捕捉极端风事件。结果表明，虽然MSH和基于梯度的损失单独使用可能会略微降低RMSE分数，但当组合训练时，模型表现出与MSE训练模型非常相似的MSE性能，同时显著提高了谱保真度和物理一致性。替代的风表示进一步提高了风速准确性并减少了方向偏差。总的来说，这些发现强调了损失函数设计作为将领域知识嵌入MLWP模型机制的重要性，并推进了它们的业务准备状态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning weather prediction (MLWP) models have demonstratedremarkable potential in delivering accurate forecasts at significantly reducedcomputational cost compared to traditional numerical weather prediction (NWP)systems. However, challenges remain in ensuring the physical consistency ofMLWP outputs, particularly in deterministic settings. This study presentsFastNet, a graph neural network (GNN)-based global prediction model, andinvestigates the impact of alternative loss function designs on improving thephysical realism of its forecasts. We explore three key modifications to thestandard mean squared error (MSE) loss: (1) a modified spherical harmonic (MSH)loss that penalises spectral amplitude errors to reduce blurring and enhancesmall-scale structure retention; (2) inclusion of horizontal gradient terms inthe loss to suppress non-physical artefacts; and (3) an alternative windrepresentation that decouples speed and direction to better capture extremewind events. Results show that while the MSH and gradient-based losses\textit{alone} may slightly degrade RMSE scores, when trained in combinationthe model exhibits very similar MSE performance to an MSE-trained model whileat the same time significantly improving spectral fidelity and physicalconsistency. The alternative wind representation further improves wind speedaccuracy and reduces directional bias. Collectively, these findings highlightthe importance of loss function design as a mechanism for embedding domainknowledge into MLWP models and advancing their operational readiness.</description>
      <author>example@mail.com (Tom Dunstan, Oliver Strickson, Thusal Bennett, Jack Bowyer, Matthew Burnand, James Chappell, Alejandro Coca-Castro, Kirstine Ida Dale, Eric G. Daub, Noushin Eftekhari, Manvendra Janmaijaya, Jon Lillis, David Salvador-Jasin, Nathan Simpson, Ryan Sze-Yin Chan, Mohamad Elmasri, Lydia Allegranza France, Sam Madge, Levan Bokeria, Hannah Brown, Tom Dodds, Anna-Louise Ellis, David Llewellyn-Jones, Theo McCaie, Sophia Moreton, Tom Potter, James Robinson, Adam A. Scaife, Iain Stenson, David Walters, Karina Bett-Williams, Louisa van Zeeland, Peter Yatsyshin, J. Scott Hosking)</author>
      <guid isPermaLink="false">2509.17601v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Periodic Graph-Enhanced Multivariate Time Series Anomaly Detector</title>
      <link>http://arxiv.org/abs/2509.17472v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种周期图增强的多变量时间序列异常检测器(PGMA)，通过动态图结构和图神经网络有效解决了现有方法无法准确表示多变量时间序列复杂时空相关性的问题&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列异常检测广泛应用于金融、医疗和工业监控等领域，但现有方法大多基于静态图结构，无法准确表示多变量时间序列中的复杂时空相关性&lt;h4&gt;目的&lt;/h4&gt;解决现有多变量时间序列异常检测方法无法准确表示复杂时空相关性的问题&lt;h4&gt;方法&lt;/h4&gt;提出PGMA模型，包含两个核心思想：a)基于快速傅里叶变换设计周期性时间槽分配策略，使图结构能反映多变量时间序列的动态变化；b)利用图神经网络和时序扩展卷积从重建的周期图中提取复杂的时空相关性&lt;h4&gt;主要发现&lt;/h4&gt;在四个来自实际应用的真实数据集上的实验表明，PGMA在多变量时间序列异常检测中优于最先进的模型&lt;h4&gt;结论&lt;/h4&gt;PGMA通过动态图结构和图神经网络有效解决了多变量时间序列异常检测中时空相关性表示不准确的问题&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列异常检测常见于金融、医疗和工业监控等各个领域。然而，现有的多变量时间序列异常检测方法大多定义在静态图结构上，无法准确表示多变量时间序列中的复杂时空相关性。为解决这一问题，本研究提出了一种周期图增强的多变量时间序列异常检测器(PGMA)，包含以下两个方面的创新：a)基于快速傅里叶变换(FFT)设计周期性时间槽分配策略，使图结构能够反映多变量时间序列的动态变化；b)利用图神经网络和时序扩展卷积从重建的周期图中准确提取复杂的时空相关性。在四个来自实际应用的真实数据集上的实验表明，所提出的PGMA在多变量时间序列异常检测中优于最先进的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) anomaly detection commonly encounters invarious domains like finance, healthcare, and industrial monitoring. However,existing MTS anomaly detection methods are mostly defined on the static graphstructure, which fails to perform an accurate representation of complexspatio-temporal correlations in MTS. To address this issue, this study proposesa Periodic Graph-Enhanced Multivariate Time Series Anomaly Detector (PGMA) withthe following two-fold ideas: a) designing a periodic time-slot allocationstrategy based Fast Fourier Transform (FFT), which enables the graph structureto reflect dynamic changes in MTS; b) utilizing graph neural network andtemporal extension convolution to accurate extract the complex spatio-temporalcorrelations from the reconstructed periodic graphs. Experiments on four realdatasets from real applications demonstrate that the proposed PGMA outperformsstate-of-the-art models in MTS anomaly detection.</description>
      <author>example@mail.com (Jia Li, Shiyu Long, Ye Yuan)</author>
      <guid isPermaLink="false">2509.17472v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Predicting the Curie Temperature of Magnetic Materials with Machine Learning: Descriptor Engineering, Graph Neural Networks, and the Role of Curated Data</title>
      <link>http://arxiv.org/abs/2509.17464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种机器学习框架，用于预测磁性材料的居里温度，通过使用精选数据集和图神经网络取得了高精度预测结果。&lt;h4&gt;背景&lt;/h4&gt;预测磁性材料的居里温度对于推进数据存储、自旋电子学和传感器等领域的应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个机器学习框架来准确预测磁性材料的居里温度，加速磁性材料的发现。&lt;h4&gt;方法&lt;/h4&gt;使用包含2500种铁磁化合物的精选数据集，采用两种基于元素描述符的特征：一种基于化学计量加权描述符，另一种利用图神经网络(GNN)，并使用CatBoost模型进行训练；分析特征重要性，比较使用精选数据集和在线未筛选数据集的效果。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用化学计量加权描述符的CatBoost模型达到0.87的R²分数；2) 使用基于GNN的表示进一步提高了性能，CatBoost达到0.91的R²分数；3) 使用在线未筛选数据集导致预测效果差，R²分数仅为0.66；4) 电离能是影响居里温度的关键物理化学因素；5) 仅使用前10个电离能作为输入特征，统计模型的R²分数高达0.85，基于GNN的方法达到0.89。&lt;h4&gt;结论&lt;/h4&gt;结合稳健的机器学习模型、精心设计的特征工程和高品质数据可以加速磁性材料的发现。&lt;h4&gt;翻译&lt;/h4&gt;预测磁性材料的居里温度对于推进数据存储、自旋电子学和传感器中的应用至关重要。我们提出了一种机器学习框架，使用包含2500种铁磁化合物的精选数据集来预测居里温度，采用了两种基于元素描述符的特征：一种基于化学计量加权描述符，另一种利用图神经网络。基于化学计量加权描述符训练的CatBoost模型达到了0.87的R²分数，而使用基于GNN的表示进一步提高了性能，CatBoost达到了0.91的R²分数，突显了基于图的特征学习的有效性。我们还证明使用在线可用的未筛选数据集会导致预测效果差，CatBoost模型的R²分数仅为0.66。我们使用递归特征消除(RFE)等工具分析了特征重要性，揭示电离能是影响居里温度的关键物理化学因素。值得注意的是，仅使用前10个电离能作为输入特征，统计模型的R²分数高达0.85，基于GNN的方法达到了0.89。这些结果表明，结合稳健的机器学习模型、精心设计的特征工程和高品质数据，可以加速磁性材料的发现。我们的精选数据集已在GitHub上公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting the Curie temperature ($T_\mathrm{C}$) of magnetic materials iscrucial for advancing applications in data storage, spintronics, and sensors.We present a machine learning (ML) framework to predict $T_{\mathrm{C}}$ usinga curated dataset of 2,500 ferromagnetic compounds, employing two types ofelemental descriptor-based features: one based on stoichiometry-weighteddescriptors, and the other leveraging Graph Neural Networks (GNNs). CatBoosttrained on the stoichiometry-weighted descriptors achieved an $R^2$ score of0.87, while the use of GNN-based representations led to a further improvement,with CatBoost reaching an $R^2$ of 0.91, highlighting the effectiveness ofgraph-based feature learning. We also demonstrated that using an uncurateddataset available online leads to poor predictions, resulting in a low $R^2$score of 0.66 for the CatBoost model. We analyzed feature importance usingtools such as Recursive Feature Elimination (RFE), which revealed thationization energies are a key physicochemical factor influencing$T_\mathrm{C}$. Notably, the use of only the first 10 ionization energies asinput features resulted in high predictive accuracy, with $R^2$ scores of up to0.85 for statistical models and 0.89 for the GNN-based approach. These resultshighlight that combining robust ML models with thoughtful feature engineeringand high-quality data, can accelerate the discovery of magnetic materials. Ourcurated dataset is publicly available on GitHub.</description>
      <author>example@mail.com (Akram Abedi Orang, Mojtaba Alaei, Artem R. Oganov)</author>
      <guid isPermaLink="false">2509.17464v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Graph Signal Generative Diffusion Models</title>
      <link>http://arxiv.org/abs/2509.17250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to 2026 IEEE International Conference on Acoustics, Speech,  and Signal Processing (ICASSP 2026)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种U型编码器-解码器图神经网络（U-GNNs），用于通过去噪扩散过程进行随机图信号生成。该模型通过跳跃连接学习不同分辨率的节点特征，并利用特殊的池化操作避免任意图粗化，在股票价格预测任务中展示了概率预测的有效性。&lt;h4&gt;背景&lt;/h4&gt;在处理图数据时，特别是需要捕捉不确定性和尾部事件的情况下（如股票价格预测），确定性预测方法往往存在局限性。传统方法难以有效处理图数据的多尺度特征和概率性预测需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效进行随机图信号生成的图神经网络模型，特别是在需要概率预测的场景下，如股票价格预测，以捕捉数据中的不确定性和尾部事件。&lt;h4&gt;方法&lt;/h4&gt;提出U型编码器-解码器图神经网络（U-GNNs），采用去噪扩散过程进行随机图信号生成。该架构通过编码器和解码器路径之间的跳跃连接学习不同分辨率的节点特征，类似于图像生成的卷积U-Net。特别设计了利用零填充的池化操作，避免任意的图粗化，并在其上叠加图卷积以捕获局部依赖关系。这种方法允许在架构的更深层次学习采样节点的特征嵌入，这些特征嵌入相对于原始图保持卷积特性。&lt;h4&gt;主要发现&lt;/h4&gt;U-GNN模型在股票价格预测任务中表现出色，特别是在捕捉不确定性和尾部事件方面，这些是确定性预测方法难以处理的关键要素。扩散模型能够有效进行股票价格的概率预测，提供更全面的不确定性量化。&lt;h4&gt;结论&lt;/h4&gt;U-GNNs结合去噪扩散过程为随机图信号生成提供了一种有效方法，特别适用于需要概率预测的领域如金融预测。该模型通过多尺度特征学习和特殊的池化操作，能够捕捉图数据中的复杂依赖关系和不确定性，为概率图信号建模提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍用于随机图信号生成的U型编码器-解码器图神经网络（U-GNNs），使用去噪扩散过程。该架构通过编码器和解码器路径之间的跳跃连接，学习不同分辨率的节点特征，类似于图像生成的卷积U-Net。U-GNN突出的特点是池化操作，利用零填充并避免任意的图粗化，在其上叠加图卷积以捕获局部依赖关系。这种技术允许在架构的更深层次学习采样节点的特征嵌入，这些特征嵌入相对于原始图保持卷积特性。应用于股票价格预测——其中确定性预测难以捕捉不确定性和尾部事件（这些非常重要）——我们展示了扩散模型在股票价格概率预测中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) forstochastic graph signal generation using denoising diffusion processes. Thearchitecture learns node features at different resolutions with skipconnections between the encoder and decoder paths, analogous to theconvolutional U-Net for image generation. The U-GNN is prominent for a poolingoperation that leverages zero-padding and avoids arbitrary graph coarsening,with graph convolutions layered on top to capture local dependencies. Thistechnique permits learning feature embeddings for sampled nodes at deeperlevels of the architecture that remain convolutional with respect to theoriginal graph. Applied to stock price prediction -- where deterministicforecasts struggle to capture uncertainties and tail events that are paramount-- we demonstrate the effectiveness of the diffusion model in probabilisticforecasting of stock prices.</description>
      <author>example@mail.com (Yigit Berkay Uslu, Samar Hadou, Sergio Rozada, Shirin Saeedi Bidokhti, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2509.17250v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Discovery of Neural Circuits in Spatially Patterned Neural Responses with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.17174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图的神经网络推断模型，能够同时预测神经活动并推断潜在的突触连接性，通过将神经元建模为图中相互作用的节点实现。&lt;h4&gt;背景&lt;/h4&gt;从神经群体活动中推断突触连接是计算神经科学的基本挑战，受到部分可观察性和推断模型与真实电路动力学之间不匹配的复杂化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时预测神经活动并推断潜在连接性的神经网络模型，解决在部分可观察条件下神经回路推断的问题。&lt;h4&gt;方法&lt;/h4&gt;构建包含两个模块的图神经网络架构：一个用于学习结构连接，另一个通过图神经网络预测未来尖峰活动；使用辅助节点容纳未观察到的神经元，允许在部分观察的电路中进行推断；使用环形吸引子网络的合成数据和老鼠头部方向细胞的真实尖峰记录进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在各种条件下（包括变化的循环连接、外部输入和不完整的观察），该模型始终优于标准基线，更有效地解决了虚假相关性，恢复了准确的权重分布；应用于真实数据时，推断出的连接性与连续吸引子模型的理论预测一致。&lt;h4&gt;结论&lt;/h4&gt;基于GNN的模型具有通过自监督结构学习推断潜在神经回路的潜力，利用尖峰预测任务灵活地连接模拟和生物神经系统中的连接性和动力学。&lt;h4&gt;翻译&lt;/h4&gt;从神经群体活动中推断突触连接是计算神经科学中的一个基本挑战，由于部分可观察性和推断模型与真实电路动力学之间的不匹配而变得复杂。在本研究中，我们提出了一种基于图的神经网络推断模型，通过将神经元建模为图中相互作用的节点，同时预测神经活动并推断潜在的连接性。该架构包含两个不同的模块：一个用于学习结构连接，另一个通过图神经网络预测未来的尖峰活动。我们的模型通过辅助节点容纳未观察到的神经元，允许在部分观察的电路中进行推断。我们使用环形吸引器网络的合成数据和老鼠头部方向细胞的真实尖峰记录来评估这种方法。在广泛的条件下，包括变化的循环连接、外部输入和不完整的观察，我们的模型始终优于标准基线，更有效地解决了虚假相关性，并恢复了准确的权重分布。应用于真实数据时，推断出的连接性与连续吸引子模型的理论预测一致。这些结果强调了基于GNN的模型通过自监督结构学习推断潜在神经回路的潜力，同时利用尖峰预测任务灵活地连接模拟和生物神经系统中的连接性和动力学。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inferring synaptic connectivity from neural population activity is afundamental challenge in computational neuroscience, complicated by partialobservability and mismatches between inference models and true circuitdynamics. In this study, we propose a graph-based neural inference model thatsimultaneously predicts neural activity and infers latent connectivity bymodeling neurons as interacting nodes in a graph. The architecture features twodistinct modules: one for learning structural connectivity and another forpredicting future spiking activity via a graph neural network (GNN). Our modelaccommodates unobserved neurons through auxiliary nodes, allowing for inferencein partially observed circuits. We evaluate this approach using synthetic datafrom ring attractor networks and real spike recordings from head directioncells in mice. Across a wide range of conditions, including varying recurrentconnectivity, external inputs, and incomplete observations, our modelconsistently outperforms standard baselines, resolving spurious correlationsmore effectively and recovering accurate weight profiles. When applied to realdata, the inferred connectivity aligns with theoretical predictions ofcontinuous attractor models. These results highlight the potential of GNN-basedmodels to infer latent neural circuitry through self-supervised structurelearning, while leveraging the spike prediction task to flexibly linkconnectivity and dynamics across both simulated and biological neural systems.</description>
      <author>example@mail.com (Kijung Yoon)</author>
      <guid isPermaLink="false">2509.17174v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Unrolled Graph Neural Networks for Constrained Optimization</title>
      <link>http://arxiv.org/abs/2509.17156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究将上升对偶(DA)算法展开为两个耦合的图神经网络(GNNs)，通过层级别交互寻找拉格朗日函数的鞍点，用于解决约束优化问题。采用联合训练方案交替更新原始网络和对偶网络，实验证明该方法能产生接近最优且接近可行的解，并具有良好的分布外泛化能力。&lt;h4&gt;背景&lt;/h4&gt;约束优化问题在许多领域都有广泛应用，而传统的上升对偶(DA)算法在解决这类问题时面临挑战。图神经网络作为一种强大的学习架构，为解决这些问题提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;将上升对偶(DA)算法的动力学展开为两个耦合的图神经网络，用于解决约束优化问题，并寻找拉格朗日函数的鞍点。&lt;h4&gt;方法&lt;/h4&gt;构建两个耦合的图神经网络(GNNs)，原始网络和对偶网络在层级别相互交互。通过施加下降和上升约束，使网络反映DA算法的动力学。采用联合训练方案，交替更新原始网络和对偶网络。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够产生接近最优且接近可行的解，并且能够很好地推广到分布外(OOD)问题。&lt;h4&gt;结论&lt;/h4&gt;将上升对偶算法展开为图神经网络是解决约束优化问题的有效方法，所提出的联合训练方案能够有效地优化原始和对偶变量，具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们将上升对偶(DA)算法的动力学展开为两个耦合的图神经网络(GNNs)，用于解决约束优化问题。这两个网络在层级别相互交互，以寻找拉格朗日函数的鞍点。原始GNN为给定的对偶乘子找到一个平稳点，而对偶网络迭代地改进其估计以达到最优解。通过施加下降和上升约束，我们强制原始网络和对偶网络反映DA算法的动力学。我们提出了一种联合训练方案，交替更新原始网络和对偶网络。我们的数值实验表明，我们的方法能够产生接近最优且接近可行的解，并且能够很好地推广到分布外(OOD)问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we unroll the dynamics of the dual ascent (DA) algorithm intwo coupled graph neural networks (GNNs) to solve constrained optimizationproblems. The two networks interact with each other at the layer level to finda saddle point of the Lagrangian. The primal GNN finds a stationary point for agiven dual multiplier, while the dual network iteratively refines its estimatesto reach an optimal solution. We force the primal and dual networks to mirrorthe dynamics of the DA algorithm by imposing descent and ascent constraints. Wepropose a joint training scheme that alternates between updating the primal anddual networks. Our numerical experiments demonstrate that our approach yieldsnear-optimal near-feasible solutions and generalizes well toout-of-distribution (OOD) problems.</description>
      <author>example@mail.com (Samar Hadou, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2509.17156v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>DeepEOSNet: Capturing the dependency on thermodynamic state in property prediction tasks</title>
      <link>http://arxiv.org/abs/2509.17018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DeepEOSNet的机器学习架构，用于更好地捕捉热力学性质对独立状态的依赖性。该架构通过分离处理分子结构和状态依赖信息，实现了对热力学性质的准确预测。&lt;h4&gt;背景&lt;/h4&gt;在预测状态依赖的热力学性质时，机器学习模型需要同时考虑分子结构和热力学状态（通常由温度、压力和组成描述）。现有的分子机器学习模型通常通过将状态信息添加到分子指纹向量中或嵌入显式（半经验）热力学关系来包含状态信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的机器学习架构，能够更有效地捕捉热力学性质对独立状态的依赖关系，并提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出将分子结构和状态依赖的信息处理分为两个独立的网络通道：图神经网络和多层感知机，其输出通过点积结合。这种方法基于DeepONet架构，但不是学习算子，而是学习状态依赖性，有可能预测状态方程（EOS）。&lt;h4&gt;主要发现&lt;/h4&gt;通过三个案例研究（包括预测作为温度函数的蒸气压，以及作为组成、温度和压力函数的混合物摩尔体积），结果表明DeepEOSNet在预测蒸气压方面表现优于现有的基于图的热力学预测模型，在预测混合物摩尔体积方面表现相当。DeepEOSNet在状态域数据稀疏且不同分子的输出函数结构相似的情况下具有很大潜力。&lt;h4&gt;结论&lt;/h4&gt;DeepEOSNet的概念可以轻松转移到其他分子机器学习架构中，为性质预测提供了可行的选择。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种机器学习（ML）架构，以更好地捕捉热力学性质对独立状态的依赖性。在预测状态依赖的热力学性质时，机器学习模型需要考虑分子结构和由独立变量描述的热力学状态，通常是温度、压力和组成。现代分子机器学习模型通常通过将状态信息添加到分子指纹向量中或嵌入显式（半经验）热力学关系来包含状态信息。在这里，我们提出将分子结构和状态依赖的信息处理分为两个独立的网络通道：图神经网络和多层感知机，其输出通过点积结合。我们将我们的方法称为DeepEOSNet，因为这个想法基于DeepONet架构[Lu等人（2021），自然·机器智能]：我们不是学习算子，而是学习状态依赖性，有可能预测状态方程（EOS）。我们通过三个案例研究研究了DeepEOSNet的预测性能，包括预测作为温度函数的蒸气压，以及作为组成、温度和压力函数的混合物摩尔体积。我们的结果表明，与先前工作中基于图的前沿热力学预测模型相比，DeepEOSNet在预测蒸气压方面具有优越的性能，在预测混合物摩尔体积方面具有相当的性能。事实上，在状态域数据稀疏且不同分子的输出函数结构相似的情况下，我们看到了DeepEOSNet的巨大潜力。DeepEOSNet的概念可以轻松转移到其他分子机器学习架构中，因此为性质预测提供了一个可行的选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a machine learning (ML) architecture to better capture thedependency of thermodynamic properties on the independent states. Whenpredicting state-dependent thermodynamic properties, ML models need to accountfor both molecular structure and the thermodynamic state, described byindependent variables, typically temperature, pressure, and composition. Modernmolecular ML models typically include state information by adding it tomolecular fingerprint vectors or by embedding explicit (semi-empirical)thermodynamic relations. Here, we propose to rather split the informationprocessing on the molecular structure and the dependency on states into twoseparate network channels: a graph neural network and a multilayer perceptron,whose output is combined by a dot product. We refer to our approach asDeepEOSNet, as this idea is based on the DeepONet architecture [Lu et al.(2021), Nat. Mach. Intell.]: instead of operators, we learn state dependencies,with the possibility to predict equation of states (EOS). We investigate thepredictive performance of DeepEOSNet by means of three case studies, whichinclude the prediction of vapor pressure as a function of temperature, andmixture molar volume as a function of composition, temperature, and pressure.Our results show superior performance of DeepEOSNet for predicting vaporpressure and comparable performance for predicting mixture molar volumecompared to state-of-research graph-based thermodynamic prediction models fromour earlier works. In fact, we see large potential of DeepEOSNet in cases wheredata is sparse in the state domain and the output function is structurallysimilar across different molecules. The concept of DeepEOSNet can easily betransferred to other ML architectures in molecular context, and thus provides aviable option for property prediction.</description>
      <author>example@mail.com (Jan Pavšek, Alexander Mitsos, Manuel Dahmen, Tai Xuan Tan, Jan G. Rittig)</author>
      <guid isPermaLink="false">2509.17018v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>NeuFACO: Neural Focused Ant Colony Optimization for Traveling Salesman Problem</title>
      <link>http://arxiv.org/abs/2509.16938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to RIVF'25. Code is available at  https://github.com/shoraaa/NeuFACO&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为神经聚焦蚁群优化(NeuFACO)的新框架，该框架结合了强化学习和蚁群优化技术，用于解决旅行商问题，能够高效地生成高质量解决方案。&lt;h4&gt;背景&lt;/h4&gt;旅行商问题(TSP)是一个经典的组合优化问题，需要寻找访问所有城市并返回起点的最短路径。传统的蚁群优化算法在解决此类问题时面临效率和质量的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合先进强化学习和增强蚁群优化的非自回归框架，以高效解决旅行商问题，生成高质量解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用近端策略优化(PPO)和熵正则化训练图神经网络，生成实例特定的启发式指导，并将其集成到优化的ACO框架中，该框架包含候选列表、受限路线优化和可扩展局部搜索，结合摊销推理与ACO随机探索。&lt;h4&gt;主要发现&lt;/h4&gt;NeuFACO能够通过结合神经网络的学习能力和蚁群优化的随机探索，在各种不同的旅行商问题实例上高效地产生高质量解决方案。&lt;h4&gt;结论&lt;/h4&gt;NeuFACO框架成功地将强化学习与蚁群优化相结合，为解决旅行商问题提供了一种有效的方法，能够处理多样化的实例并生成高质量的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了神经聚焦蚁群优化(NeuFACO)，这是一种用于旅行商问题(TSP)的非自回归框架，结合了先进的强化学习与增强的蚁群优化(ACO)。NeuFACO采用带熵正则化的近端策略优化(PPO)来训练图神经网络，用于实例特定的启发式指导，并将其集成到具有候选列表、受限路线优化和可扩展局部搜索的优化ACO框架中。通过利用摊销推理与ACO随机探索相结合，NeuFACO能够高效地生成各种TSP实例的高质量解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents Neural Focused Ant Colony Optimization (NeuFACO), anon-autoregressive framework for the Traveling Salesman Problem (TSP) thatcombines advanced reinforcement learning with enhanced Ant Colony Optimization(ACO). NeuFACO employs Proximal Policy Optimization (PPO) with entropyregularization to train a graph neural network for instance-specific heuristicguidance, which is integrated into an optimized ACO framework featuringcandidate lists, restricted tour refinement, and scalable local search. Byleveraging amortized inference alongside ACO stochastic exploration, NeuFACOefficiently produces high-quality solutions across diverse TSP instances.</description>
      <author>example@mail.com (Tran Thanh Dat, Tran Quang Khai, Pham Anh Khoi, Vu Van Khu, Do Duc Dong)</author>
      <guid isPermaLink="false">2509.16938v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Graph Convolution and Semantic-Guided Attention for Multimodal Risk Detection in Social Networks</title>
      <link>http://arxiv.org/abs/2509.16936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合自然语言处理和图神经网络的多模态方法，用于检测社交媒体用户的潜在危险倾向。&lt;h4&gt;背景&lt;/h4&gt;在社交媒体环境中，识别可能具有危险倾向的用户对平台安全和社会稳定具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;通过创新的多模态方式更有效地检测社交媒体用户的潜在危险倾向。&lt;h4&gt;方法&lt;/h4&gt;整合自然语言处理和图神经网络技术；对用户文本进行语义分析、情感识别和关键词提取；构建异构用户关系图；提出新型关系图卷积网络建模用户关系和内容传播路径；结合文本特征和图结构信息进行风险评估。&lt;h4&gt;主要发现&lt;/h4&gt;在来自不同平台的真实社交媒体数据集上，该多模态模型相比单模态方法能够取得显著改进。&lt;h4&gt;结论&lt;/h4&gt;结合文本特征和图结构信息的方法能够提供更强大、有效的方式来发现风险用户。&lt;h4&gt;翻译&lt;/h4&gt;本文专注于以创新的多模态方式检测社交媒体用户的潜在危险倾向。我们将自然语言处理和图神经网络相结合。首先，我们在用户生成的文本上应用NLP并进行语义分析、情感识别和关键词提取，以从社交媒体帖子中获取细微的风险信号。同时，我们基于社交互动构建异构用户关系图，并提出一种新型关系图卷积网络来建模用户关系、关注关系和内容传播路径，以发现一些重要的结构信息和用户行为。最后，我们将上述两个模型提取的文本特征与图结构信息相结合，这为发现风险用户提供了一种更强大有效的方式。我们在来自不同平台的真实社交媒体数据集上的实验表明，我们的模型相比单模态方法能够取得显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper focuses on the detection of potentially dangerous tendencies ofsocial media users in an innovative multimodal way. We integrate NaturalLanguage Processing (NLP) and Graph Neural Networks (GNNs) together. Firstly,we apply NLP on the user-generated text and conduct semantic analysis,sentiment recognition and keyword extraction to get subtle risk signals fromsocial media posts. Meanwhile, we build a heterogeneous user relationship graphbased on social interaction and propose a novel relational graph convolutionalnetwork to model user relationship, attention relationship and contentdissemination path to discover some important structural information and userbehaviors. Finally, we combine textual features extracted from these two modelsabove with graph structural information, which provides a more robust andeffective way to discover at-risk users. Our experiments on real social mediadatasets from different platforms show that our model can achieve significantimprovement over single-modality methods.</description>
      <author>example@mail.com (Cuiqianhe Du, Chia-En Chiang, Tianyi Huang, Zikun Cui)</author>
      <guid isPermaLink="false">2509.16936v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion</title>
      <link>http://arxiv.org/abs/2509.17712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RCTDistill是一种基于时间融合的新型跨模态知识蒸馏方法，包含三个关键模块(RAKD、TKD和RDKD)，有效解决了雷达-摄像头融合方法在3D物体检测中的性能瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;雷达-摄像头融合方法虽经济有效，但性能仍落后于LiDAR方法；现有研究虽采用时间融合和知识蒸馏策略，但未充分考虑物体运动和传感器特定误差带来的不确定性。&lt;h4&gt;目的&lt;/h4&gt;提出RCTDistill方法，通过创新的知识蒸馏架构，提高雷达-摄像头融合方法的性能，同时处理物体运动和传感器误差带来的不确定性。&lt;h4&gt;方法&lt;/h4&gt;RCTDistill包含三个关键模块：RAKD考虑距离和方位方向的固有误差，实现从LiDAR特征到BEV表示的有效知识转移；TKD对齐历史雷达-摄像头BEV特征与当前LiDAR表示，减轻动态物体引起的时间错位；RDKD通过从教师模型蒸馏关系知识，增强特征区分能力。&lt;h4&gt;主要发现&lt;/h4&gt;RCTDistill在nuScenes和View-of-Delft(VoD)数据集上实现了最先进的雷达-摄像头融合性能，推理速度达到26.2 FPS，是最快的。&lt;h4&gt;结论&lt;/h4&gt;RCTDistill通过创新的跨模态知识蒸馏方法，有效解决了雷达-摄像头融合中的关键挑战，在性能和速度方面都取得了优异结果。&lt;h4&gt;翻译&lt;/h4&gt;雷达-摄像头融合方法已成为一种经济有效的3D物体检测方法，但在性能上仍落后于基于LiDAR的方法。近期研究工作集中于采用时间融合和知识蒸馏(KD)策略来克服这些局限性。然而，现有方法未能充分考虑物体运动产生的不确定性，以及雷达和摄像头模态固有的传感器特定误差。在本工作中，我们提出了RCTDistill，一种基于时间融合的新型跨模态KD方法，包含三个关键模块：距离-方位知识蒸馏(RAKD)、时间知识蒸馏(TKD)和区域解耦知识蒸馏(RDKD)。RAKD旨在考虑距离和方位方向的固有误差，实现从LiDAR特征到不准确的BEV表示的有效知识转移。TKD通过对齐历史雷达-摄像头BEV特征与当前LiDAR表示，减轻动态物体引起的时间错位。RDKD通过从教师模型蒸馏关系知识，增强特征区分能力，使学生模型能够区分前景和背景特征。RCTDistill在nuScenes和View-of-Delft(VoD)数据集上都实现了最先进的雷达-摄像头融合性能，推理速度最快可达26.2 FPS。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决雷达-相机融合3D物体检测方法性能落后于LiDAR方法的问题，特别是未充分考虑物体运动和传感器固有误差引起的不确定性。这个问题很重要，因为雷达-相机方案成本更低，更适合自动驾驶商业化应用，但现有方法在准确性和可靠性上存在不足，直接影响自动驾驶系统的安全性和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传感器特定误差考虑不足和动态物体导致的时间失准问题。基于此，设计了三个关键模块(RAKD、TKD和RDKD)针对性地解决这些问题。作者借鉴了BEVFusion-R作为学生模型、CenterPoint作为教师模型，并参考了DORN的椭圆区域确定方法和基本知识蒸馏框架，但针对雷达-相机融合的特殊性进行了创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过跨模态知识蒸馏将高性能LiDAR模型的知识转移到雷达-相机融合模型中，同时针对雷达和相机的特定误差以及时间动态性问题设计专门策略。整体流程包括：1)构建基于BEVFusion-R的学生模型和基于CenterPoint的教师模型；2)实施RAKD处理传感器误差、TKD处理时间失准、RDKD增强特征区分；3)通过总损失函数联合优化；4)仅使用训练好的学生模型进行实时推理，达到26.2 FPS的速度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个针对雷达-相机时间融合的知识蒸馏技术；2)三个专门设计的知识蒸馏模块(RAKD、TKD、RDKD)；3)显著性能提升(4.7% mAP和4.9% NDS)。相比之前工作，本文使用椭圆高斯掩模而非圆形处理传感器误差，利用物体速度信息动态调整时间蒸馏区域，选择性针对特定区域应用关系知识蒸馏，并整合空间、时间和关系三个维度的知识蒸馏，实现了更优的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RCTDistill通过创新的跨模态知识蒸馏框架，有效解决了雷达-相机3D物体检测中的传感器特定误差和时间失准问题，显著提升了检测性能同时保持实时处理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radar-camera fusion methods have emerged as a cost-effective approach for 3Dobject detection but still lag behind LiDAR-based methods in performance.Recent works have focused on employing temporal fusion and KnowledgeDistillation (KD) strategies to overcome these limitations. However, existingapproaches have not sufficiently accounted for uncertainties arising fromobject motion or sensor-specific errors inherent in radar and cameramodalities. In this work, we propose RCTDistill, a novel cross-modal KD methodbased on temporal fusion, comprising three key modules: Range-Azimuth KnowledgeDistillation (RAKD), Temporal Knowledge Distillation (TKD), andRegion-Decoupled Knowledge Distillation (RDKD). RAKD is designed to considerthe inherent errors in the range and azimuth directions, enabling effectiveknowledge transfer from LiDAR features to refine inaccurate BEVrepresentations. TKD mitigates temporal misalignment caused by dynamic objectsby aligning historical radar-camera BEV features with current LiDARrepresentations. RDKD enhances feature discrimination by distilling relationalknowledge from the teacher model, allowing the student to differentiateforeground and background features. RCTDistill achieves state-of-the-artradar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)datasets, with the fastest inference speed of 26.2 FPS.</description>
      <author>example@mail.com (Geonho Bang, Minjae Seong, Jisong Kim, Geunju Baek, Daye Oh, Junhyung Kim, Junho Koh, Jun Won Choi)</author>
      <guid isPermaLink="false">2509.17712v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception</title>
      <link>http://arxiv.org/abs/2509.17462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为MAESTRO的结构化框架，用于多任务3D感知中的特定任务特征生成和特征干扰缓解。&lt;h4&gt;背景&lt;/h4&gt;多任务学习旨在基于共享数据表示同时执行多个任务，但这种方法可能导致性能下降，因为优化不同目标时会产生任务冲突。&lt;h4&gt;目的&lt;/h4&gt;解决多任务学习中的任务冲突问题，提高多任务3D感知的性能，包括3D目标检测、鸟瞰图(BEV)地图分割和3D占用预测。&lt;h4&gt;方法&lt;/h4&gt;MAESTRO框架包含三个组件：类别原型生成器(CPG)将类别分组为前景和背景组并生成组级原型；任务特定特征生成器(TSFG)利用这些原型组保留任务相关特征同时抑制无关特征；场景原型聚合器(SPA)利用3D目标检测头和地图分割头产生的信息增强3D占用预测任务的原型组。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Occ3D基准上的大量实验表明，MAESTRO在3D目标检测、BEV地图分割和3D占用预测任务中始终优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;MAESTRO框架能够有效解决多任务学习中的任务冲突问题，提高多任务3D感知的性能。&lt;h4&gt;翻译&lt;/h4&gt;多任务学习的目标是基于共享数据表示同时执行多个任务。虽然这种方法可以提高学习效率，但在为不同目标优化模型时，由于任务冲突可能导致性能下降。为应对这一挑战，我们引入了MAESTRO，这是一个结构化框架，专为生成任务特定特征和缓解多任务3D感知中的特征干扰而设计，包括3D目标检测、鸟瞰图(BEV)地图分割和3D占用预测。MAESTRO包含三个组件：类别原型生成器(CPG)、任务特定特征生成器(TSFG)和场景原型聚合器(SPA)。CPG将类别分组为前景组和背景组，并生成组级原型。前景和背景原型分别分配给3D目标检测任务和地图分割任务，而两者都分配给3D占用预测任务。TSFG利用这些原型组保留任务相关特征，同时抑制无关特征，从而提高每个任务的性能。SPA通过利用3D目标检测头和地图分割头产生的信息，增强了为3D占用预测分配的原型组。在nuScenes和Occ3D基准上的大量实验表明，MAESTRO在3D目标检测、BEV地图分割和3D占用预测任务中始终优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多任务3D感知中的任务冲突和特征干扰问题。在自动驾驶领域，车辆需要同时处理物体检测、地图分割和占用预测等多个感知任务来确保安全决策。传统多任务学习方法共享底层特征表示，但不同任务的优化目标会导致冲突，降低各任务性能。解决这个问题对自动驾驶至关重要，因为它能提高计算效率、减少资源消耗，并实现实时性能，同时保持或提升各任务的准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到不同3D感知任务关注不同的语义线索和空间区域：物体检测关注可移动前景物体，地图分割关注静态背景结构，而占用预测需要同时关注两者。基于这一观察，作者设计了MAESTRO框架，包含三个关键组件：类别原型生成器(CPG)将语义类别分组为前景和背景组；任务特定特征生成器(TSFG)利用原型组增强相关特征并抑制无关特征；场景原型聚合器(SPA)整合其他任务信息增强占用预测。作者借鉴了原型网络的思想、特征增强与抑制机制，以及Lift-Splat-Shoot等现有方法，但进行了创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过生成任务特定特征表示来减少任务间干扰，利用语义原型作为先验指导特征增强与抑制，并利用任务间依赖关系提升整体性能。整体流程：1)多视角图像通过共享主干网络提取特征并转换为3D体素表示；2)CPG将语义类别分为前景和背景组，生成组别原型；3)TSFG将共享特征转换为任务特定表示，利用原型增强相关特征并抑制无关区域；4)SPA整合物体检测和地图分割的信息生成场景原型，增强占用预测；5)各任务头部生成最终预测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)MAESTRO框架专门解决多任务3D感知中的特征干扰问题；2)CPG生成组别原型作为任务特定特征的语义先验；3)TSFG自适应增强相关特征并抑制无关特征；4)SPA显式建模任务间依赖关系，利用其他任务信息增强占用预测。相比之前工作，MAESTRO不简单共享特征表示，而是显式生成任务特定特征；利用语义原型更精细控制特征表示；通过SPA组件显式建模任务间依赖关系，实现了比单任务学习和传统多任务学习方法更优的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MAESTRO通过自适应特征增强与抑制机制，有效解决了多任务3D感知中的特征干扰问题，实现了比单任务学习和传统多任务学习方法更优的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of multi-task learning is to learn to conduct multiple taskssimultaneously based on a shared data representation. While this approach canimprove learning efficiency, it may also cause performance degradation due totask conflicts that arise when optimizing the model for different objectives.To address this challenge, we introduce MAESTRO, a structured frameworkdesigned to generate task-specific features and mitigate feature interferencein multi-task 3D perception, including 3D object detection, bird's-eye view(BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises threecomponents: the Class-wise Prototype Generator (CPG), the Task-Specific FeatureGenerator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups classcategories into foreground and background groups and generates group-wiseprototypes. The foreground and background prototypes are assigned to the 3Dobject detection task and the map segmentation task, respectively, while bothare assigned to the 3D occupancy prediction task. TSFG leverages theseprototype groups to retain task-relevant features while suppressing irrelevantfeatures, thereby enhancing the performance for each task. SPA enhances theprototype groups assigned for 3D occupancy prediction by utilizing theinformation produced by the 3D object detection head and the map segmentationhead. Extensive experiments on the nuScenes and Occ3D benchmarks demonstratethat MAESTRO consistently outperforms existing methods across 3D objectdetection, BEV map segmentation, and 3D occupancy prediction tasks.</description>
      <author>example@mail.com (Changwon Kang, Jisong Kim, Hongjae Shin, Junseo Park, Jun Won Choi)</author>
      <guid isPermaLink="false">2509.17462v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception</title>
      <link>http://arxiv.org/abs/2509.17107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code will be made publicly available at  https://github.com/godk0509/CoBEVMoE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoBEVMoE的新型协同感知框架，通过在鸟瞰图空间中采用动态专家混合架构，有效解决了多智能体间特征异构性问题，显著提升了感知性能。&lt;h4&gt;背景&lt;/h4&gt;协同感知旨在通过多智能体间信息共享扩展感知覆盖并提高准确性。然而，由于视点和空间位置差异，智能体获取的观测数据往往是异构的，而现有中间融合方法主要关注相似特征对齐，忽略了智能体间的感知多样性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时建模智能体间特征相似性和异构性的协同感知方法，以充分利用多智能体的感知多样性，提升协同感知的整体性能。&lt;h4&gt;方法&lt;/h4&gt;提出CoBEVMoE框架，在鸟瞰图空间运行并采用动态专家混合(DMoE)架构。每个专家根据特定智能体的输入特征动态生成，提取独特可靠线索同时关注共享语义。引入动态专家度量损失(DEML)增强专家间多样性和融合表示的区分度。&lt;h4&gt;主要发现&lt;/h4&gt;在OPV2V和DAIR-V2X-C数据集上的实验表明，CoBEVMoE达到最先进性能：在OPV2V上基于相机的BEV分割IoU提升+1.5%，在DAIR-V2X-C上基于LiDAR的3D目标检测AP@50提升+3.0%，验证了基于专家的异构特征建模在多智能体协同感知中的有效性。&lt;h4&gt;结论&lt;/h4&gt;基于专家的异构特征建模能有效提升多智能体协同感知性能，CoBEVMoE框架通过同时考虑特征相似性和异构性，为协同感知提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;协同感知旨在通过多个智能体之间共享信息来扩展感知覆盖范围并提高感知准确性。然而，由于视点和空间位置的不同，智能体通常获取异构观测数据。现有的中间融合方法主要关注对齐相似特征，常常忽略了智能体之间的感知多样性。为解决这一局限，我们提出了CoBEVMoE，一种在鸟瞰图(BEV)空间运行并采用动态专家混合(DMoE)架构的新型协同感知框架。在DMoE中，每个专家根据特定智能体的输入特征动态生成，能够提取独特且可靠的线索，同时关注共享语义。这种设计使融合过程能够明确建模智能体间的特征相似性和异构性。此外，我们引入了动态专家度量损失(DEML)来增强专家间多样性并提高融合表示的区分度。在OPV2V和DAIR-V2X-C数据集上的大量实验表明，CoBEVMoE达到了最先进的性能。具体而言，在OPV2V上提高了基于相机的BEV分割的IoU达+1.5%，在DAIR-V2X-C上提高了基于LiDAR的3D目标检测的AP@50达+3.0%，验证了基于专家的异构特征建模在多智能体协同感知中的有效性。源代码将在https://github.com/godk0509/CoBEVMoE上公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多智能体协同感知中的异构特征融合问题。由于不同车辆视角和位置差异，它们获取的信息各不相同，现有方法主要关注对齐相似特征，忽略了智能体间的感知多样性。这个问题在现实中非常重要，因为自动驾驶需要全面感知环境以确保安全，而有效的协同感知可以让车辆'看到'彼此的盲点，获取更全面的环境信息。如果融合方法不能处理异构观测，就会导致感知不完整或不准确，可能危及行车安全。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有协同感知方法忽略了智能体的个体特征，意识到不同视角的车辆可能观察到独特或互补的信息。受混合专家(MoE)范式启发，他们希望每个专家专门学习单个车辆的个性化信息，而非传统MoE中的随机初始化。他们借鉴了MoE架构、注意力机制、空间变换技术、Swin Transformer以及对比学习等方法，设计了动态专家内核生成机制和门控聚合系统，使专家能够基于输入特征动态调整，同时保持与共享语义的一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是为每个智能体动态生成专门的专家网络，这些专家基于智能体的本地特征进行条件化，能够捕捉独特、智能体特定的模式。通过门控机制自适应融合所有专家输出，既保留不同视角的独特感知线索，又整合共享语义。整体流程是：1)各智能体从原始传感器输入提取BEV特征；2)特征通过空间变换对齐并传输；3)DMoE模块生成智能体特定的专家内核；4)通过门控机制执行自适应特征聚合；5)融合后的表示被解码用于下游任务；6)引入DEML损失增强专家多样性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)从智能体特定多样性角度重新思考多源信息融合；2)设计专家内核动态初始化的DMoE模块，使用智能体个性化特征引导学习；3)引入DEML确保专家间差异化。与传统方法不同，CoBEVMoE同时考虑特征相似性和异构性，专家基于智能体本地特征动态生成而非随机初始化，能够保留异构特征而非抑制智能体特定线索，并通过DEML增强专家多样性提高融合表示的判别性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于动态混合专家的协同感知框架CoBEVMoE，通过为每个智能体生成专门的特征提取专家并引入专家多样性损失，有效解决了多智能体协同感知中的异构特征融合问题，显著提高了自动驾驶场景下的感知准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative perception aims to extend sensing coverage and improveperception accuracy by sharing information among multiple agents. However, dueto differences in viewpoints and spatial positions, agents often acquireheterogeneous observations. Existing intermediate fusion methods primarilyfocus on aligning similar features, often overlooking the perceptual diversityamong agents. To address this limitation, we propose CoBEVMoE, a novelcollaborative perception framework that operates in the Bird's Eye View (BEV)space and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. InDMoE, each expert is dynamically generated based on the input features of aspecific agent, enabling it to extract distinctive and reliable cues whileattending to shared semantics. This design allows the fusion process toexplicitly model both feature similarity and heterogeneity across agents.Furthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhanceinter-expert diversity and improve the discriminability of the fusedrepresentation. Extensive experiments on the OPV2V and DAIR-V2X-C datasetsdemonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,it improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and theAP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying theeffectiveness of expert-based heterogeneous feature modeling in multi-agentcollaborative perception. The source code will be made publicly available athttps://github.com/godk0509/CoBEVMoE.</description>
      <author>example@mail.com (Lingzhao Kong, Jiacheng Lin, Siyu Li, Kai Luo, Zhiyong Li, Kailun Yang)</author>
      <guid isPermaLink="false">2509.17107v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.16588v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SQS，一种基于查询的喷溅预训练方法，用于推进自动驾驶中的稀疏感知模型。SQS通过预测3D高斯表示和自监督喷溅学习细粒度上下文特征，并在微调阶段通过查询交互机制集成到下游任务中，显著提升了占用预测和3D物体检测的性能。&lt;h4&gt;背景&lt;/h4&gt;稀疏感知模型(SPMs)采用查询驱动范式，避免显式的密集BEV或体积构建，实现高效的计算和加速的推理。&lt;h4&gt;目的&lt;/h4&gt;介绍SQS，一种新颖的基于查询的喷溅预训练方法，专门设计用于推进自动驾驶中的SPMs。&lt;h4&gt;方法&lt;/h4&gt;SQS引入插件模块在预训练期间从稀疏查询预测3D高斯表示，利用自监督喷溅通过重建多视图图像和深度图学习细粒度上下文特征；在微调阶段，预训练的高斯查询通过查询交互机制无缝集成到下游网络中，连接预训练查询与任务特定查询，适应占用预测和3D物体检测的多样化需求。&lt;h4&gt;主要发现&lt;/h4&gt;在自动驾驶基准上的实验表明，SQS在多种基于查询的3D感知任务上带来显著性能提升，特别是在占用预测和3D物体检测方面，优于先前最先进的预训练方法，占用预测上提升+1.3 mIoU，3D检测上提升+1.0 NDS。&lt;h4&gt;结论&lt;/h4&gt;SQS是一种有效的预训练方法，能够显著提升自动驾驶中的稀疏感知模型性能。&lt;h4&gt;翻译&lt;/h4&gt;稀疏感知模型(SPMs)采用查询驱动范式，避免显式的密集BEV或体积构建，实现高效计算和加速推理。在本文中，我们介绍了SQS，一种新颖的基于查询的喷溅预训练方法，专门设计用于推进自动驾驶中的SPMs。SQS引入了一个插件模块，在预训练期间从稀疏查询预测3D高斯表示，利用自监督喷溅通过重建多视图图像和深度图来学习细粒度上下文特征。在微调期间，预训练的高斯查询通过查询交互机制无缝集成到下游网络中，这些机制明确连接预训练查询与任务特定查询，有效适应占用预测和3D物体检测的多样化需求。在自动驾驶基准上的大量实验表明，SQS在多种基于查询的3D感知任务上带来显著性能提升，特别是在占用预测和3D物体检测方面，优于先前最先进的预训练方法，显著提高了性能（占用预测上+1.3 mIoU，3D检测上+1.0 NDS）。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何提升稀疏感知模型（SPMs）在自动驾驶中的性能问题。SPMs虽然计算效率高、推理速度快，但现有预训练方法主要针对密集BEV模型设计，不适用于SPMs的稀疏查询结构。这个问题在现实中很重要，因为自动驾驶需要高效的实时感知系统，而获取精确标注数据成本高昂，利用大量未标注数据进行预训练可以显著提升模型性能，但现有方法无法有效应用于SPMs。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了SPMs的特点和现有预训练方法的局限性，认识到SPMs的稀疏查询缺乏明确空间位置和语义含义。方法设计上借鉴了3D高斯溅射技术用于快速渲染，参考了自学习方法如对比学习和掩码自编码器，以及基于NeRF的场景重建方法。在此基础上，作者设计了SQS框架，引入可插入的高斯查询模块，在预训练阶段通过重建多视图图像和深度图学习特征，在微调阶段通过查询交互机制将预训练知识转移到下游任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入自适应高斯查询，在预训练期间动态预测3D高斯并重建RGB和深度图像，使模型从未标记数据中学习细粒度表示；在微调阶段，通过查询交互机制将预训练知识与任务特定查询融合。整体流程分为两阶段：1）预训练阶段：图像编码器提取多视图特征，高斯查询通过变形交叉注意力与图像特征交互，预测高斯属性并重建图像；2）微调阶段：加载预训练权重，通过空间感知的局部注意力机制将预训练高斯查询与任务特定查询连接，更新后的查询输入任务特定头部进行预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首个专门针对SPMs的查询式溅射预训练技术；2）可插入的高斯查询模块，在预训练和微调阶段分别实现自监督学习和特征融合；3）查询交互机制，有效连接预训练查询与任务特定查询；4）显著的性能提升。相比之前工作，SQS专门针对稀疏查询模型设计，而非密集BEV模型；不同于2D查询预训练方法，能从多视图图像提取准确3D几何表示；与现有3D高斯溅射应用不同，专为SPMs的稀疏结构设计；是首个SPMs预训练方案，解决了其查询缺乏明确空间语义的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SQS提出了一种基于查询的溅射预训练方法，通过引入自适应高斯查询和查询交互机制，显著提升了自动驾驶中稀疏感知模型在占用预测和3D物体检测等任务上的性能，实现了从未标记数据中有效学习细粒度3D表示的目标。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoesexplicit dense BEV or volumetric construction, enabling highly efficientcomputation and accelerated inference. In this paper, we introduce SQS, a novelquery-based splatting pre-training specifically designed to advance SPMs inautonomous driving. SQS introduces a plug-in module that predicts 3D Gaussianrepresentations from sparse queries during pre-training, leveragingself-supervised splatting to learn fine-grained contextual features through thereconstruction of multi-view images and depth maps. During fine-tuning, thepre-trained Gaussian queries are seamlessly integrated into downstream networksvia query interaction mechanisms that explicitly connect pre-trained querieswith task-specific queries, effectively accommodating the diverse requirementsof occupancy prediction and 3D object detection. Extensive experiments onautonomous driving benchmarks demonstrate that SQS delivers considerableperformance gains across multiple query-based 3D perception tasks, notably inoccupancy prediction and 3D object detection, outperforming priorstate-of-the-art pre-training approaches by a significant margin (i.e., +1.3mIoU on occupancy prediction and +1.0 NDS on 3D detection).</description>
      <author>example@mail.com (Haiming Zhang, Yiyao Zhu, Wending Zhou, Xu Yan, Yingjie Cai, Bingbing Liu, Shuguang Cui, Zhen Li)</author>
      <guid isPermaLink="false">2509.16588v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation</title>
      <link>http://arxiv.org/abs/2509.16500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于几何反馈的强化学习方法(RLGF)，用于解决自动驾驶系统中合成视频数据的几何失真问题，显著提高了3D目标检测性能。&lt;h4&gt;背景&lt;/h4&gt;合成数据对推进自动驾驶系统至关重要，但当前最先进的视频生成模型尽管视觉真实感强，却存在细微的几何失真，限制了它们在下游感知任务中的效用。&lt;h4&gt;目的&lt;/h4&gt;解决合成数据中的几何失真问题，缩小合成数据与真实数据在3D目标检测任务中的性能差距。&lt;h4&gt;方法&lt;/h4&gt;引入RLGF方法，通过整合来自专门的潜在空间自动驾驶感知模型的奖励来优化视频扩散模型，包含潜在空间窗口优化技术和分层几何奖励系统，并提出GeoScores量化评估几何失真。&lt;h4&gt;主要发现&lt;/h4&gt;RLGF显著减少了几何误差（如 VP 误差降低21%，深度误差降低57%），并将3D目标检测的mAP提高了12.7%，大幅缩小了与真实数据性能的差距。&lt;h4&gt;结论&lt;/h4&gt;RLGF为自动驾驶开发提供了一种即插即用的解决方案，能够生成几何上可靠且真实的合成视频。&lt;h4&gt;翻译&lt;/h4&gt;合成数据对于推进自动驾驶系统至关重要，然而当前最先进的视频生成模型尽管具有视觉真实感，却存在细微的几何失真，限制了它们在下游感知任务中的效用。我们识别并量化了这一关键问题，展示了使用合成数据与真实数据进行3D目标检测时存在显著性能差距。为此，我们引入了基于几何反馈的强化学习(RLGF)，RLGF通过整合来自专门潜在空间自动驾驶感知模型的奖励，独特地优化视频扩散模型。其核心组件包括一种高效的潜在空间窗口优化技术，用于在扩散过程中提供针对性反馈，以及一个分层几何奖励(HGR)系统，为点-线-面对齐和场景占用一致性提供多层次奖励。为量化这些失真，我们提出了GeoScores。将RLGF应用于nuScenes数据集上的DiVE等模型，显著减少了几何误差（例如 VP 误差降低21%，深度误差降低57%），并将3D目标检测的mAP提高了12.7%，缩小了与真实数据性能的差距。RLGF为自动驾驶开发提供了一种即插即用的解决方案，用于生成几何上可靠且真实的合成视频。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶视频生成模型中的几何失真问题。当前先进的视频生成模型虽然视觉上逼真，但存在微妙的几何失真，导致合成数据在3D物体检测等下游任务中性能显著下降（mAP从35.5降至25.7）。这个问题很重要，因为合成数据对推进自动驾驶系统至关重要，而几何失真会影响基于这些数据训练的感知和规划任务的可靠性，限制了模拟训练和系统验证等关键应用场景的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过实验发现，虽然2D物体检测结果与真实数据相当，但3D物体检测性能显著下降，表明当前模型保留了2D外观但未能捕捉准确的3D场景结构。他们借鉴了强化学习从人类反馈（RLHF）的成功经验，特别是在大语言模型中的应用，并将其扩展到视频生成领域。同时，他们使用了感知模型作为奖励信号，这与一些图像生成方法类似，但针对自动驾驶的几何特性进行了专门设计，提出了在潜在空间而非像素空间中应用奖励的创新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过强化学习将几何反馈整合到视频生成过程中，使用专门的自动驾驶感知模型作为奖励提供者确保几何保真度。整体实现流程包括：1) 潜在空间窗口优化 - 在扩散过程的中间步骤而非仅最终输出提供反馈，使用滑动窗口策略减少计算负担；2) 分层几何奖励系统(HGR) - 使用两个专门的感知网络（几何感知模型和占用预测模型）提供多级反馈，包括点-线-平面几何反馈和场景级占用反馈；3) 微解码模块 - 使用轻量级模块处理潜在特征，避免昂贵的完全解码，使感知模型能在嘈杂的潜在特征上高效运行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次系统量化自动驾驶视频生成中的几何失真问题，提出GeoScores评估指标；2) 引入RLGF框架，将感知模型驱动的几何空间约束直接注入视频生成过程；3) 设计潜在空间窗口优化技术，实现高效且针对性的中间反馈；4) 提出分层几何奖励系统(HGR)，提供多级几何反馈。相比之前工作，RLGF不依赖传统的像素级对齐，而是在潜在空间中操作提高效率；专门针对自动驾驶几何需求设计，包含消失点、车道拓扑和深度一致性等特定方面；使用可量化的几何奖励而非广泛的偏好分数，提供精确的局部几何反馈。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RLGF通过引入基于感知模型的几何反馈强化学习框架，显著改善了自动驾驶视频生成中的几何保真度，有效缩小了合成数据与真实数据在3D物体检测任务上的性能差距，将mAP提升了12.7%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic data is crucial for advancing autonomous driving (AD) systems, yetcurrent state-of-the-art video generation models, despite their visual realism,suffer from subtle geometric distortions that limit their utility fordownstream perception tasks. We identify and quantify this critical issue,demonstrating a significant performance gap in 3D object detection when usingsynthetic versus real data. To address this, we introduce ReinforcementLearning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusionmodels by incorporating rewards from specialized latent-space AD perceptionmodels. Its core components include an efficient Latent-Space WindowingOptimization technique for targeted feedback during diffusion, and aHierarchical Geometric Reward (HGR) system providing multi-level rewards forpoint-line-plane alignment, and scene occupancy coherence. To quantify thesedistortions, we propose GeoScores. Applied to models like DiVE on nuScenes,RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Deptherror by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,narrowing the gap to real-data performance. RLGF offers a plug-and-playsolution for generating geometrically sound and reliable synthetic videos forAD development.</description>
      <author>example@mail.com (Tianyi Yan, Wencheng Han, Xia Zhou, Xueyang Zhang, Kun Zhan, Cheng-zhong Xu, Jianbing Shen)</author>
      <guid isPermaLink="false">2509.16500v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Stabilizing Information Flow Entropy: Regularization for Safe and Interpretable Autonomous Driving Perception</title>
      <link>http://arxiv.org/abs/2509.16277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究重新概念化深度神经编码器为层次化通信链，提出两个稳健感知的设计原则，并开发了Eloss正则化器，实现信息论稳定与标准感知任务的统一，显著提高自动驾驶系统对异常的敏感性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶中的深度感知网络通常依赖数据密集型训练和事后异常检测，忽视了稳定信息处理的基本信息论约束。&lt;h4&gt;目的&lt;/h4&gt;重新概念化深度神经编码器为层次化通信链，将原始感官输入逐步压缩为任务相关的潜在特征，并建立稳健感知的理论设计原则。&lt;h4&gt;方法&lt;/h4&gt;建立两个理论设计原则：(D1)连续层间互信息的平滑变化，和(D2)潜在熵随网络深度的单调递减；提出Eloss作为基于熵的轻量级即插即用训练目标。&lt;h4&gt;主要发现&lt;/h4&gt;在真实架构假设下，强制信息流平滑自然促进熵衰减，确保稳定压缩；实验证明Eloss在保持竞争力的同时，将分布偏移信号放大最多两个数量级。&lt;h4&gt;结论&lt;/h4&gt;稳定信息压缩的视角不仅提高了可解释性，还为更安全、更稳健的自动驾驶感知系统建立了坚实的理论基础。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶中的深度感知网络传统上依赖数据密集型训练和事后异常检测，往往忽视了支配稳定信息处理的基本信息论约束。我们将深度神经编码器重新概念化为层次化通信链，将原始感官输入逐步压缩为任务相关的潜在特征。在此框架内，我们为稳健感知建立了两个理论上合理的设计原则：(D1)连续层间互信息的平滑变化，和(D2)潜在熵随网络深度的单调递减。我们的分析表明，在真实的架构假设下，特别是由相似容量重复层组成的模块，强制信息流平滑(D1)自然促进熵衰减(D2)，从而确保稳定压缩。在这些见解的指导下，我们提出了Eloss，一种新型基于熵的正则化器，设计为轻量级即插即用训练目标。这种方法不仅带来边际精度改进，还代表了概念上的转变：它将信息论稳定性与标准感知任务统一起来，通过熵偏差实现异常传感器输入的明确、原则性检测。在大型3D目标检测基准(KITTI和nuScenes)上的实验验证表明，结合Eloss始终能实现竞争性或改进的精度，同时显著提高对异常的敏感性，将分布偏移信号放大最多两个数量级。这种稳定信息压缩的视角不仅提高了可解释性，还为更安全、更稳健的自动驾驶感知系统建立了坚实的理论基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶感知系统中深度神经网络的信息流不稳定问题，导致在面对异常输入（如恶劣天气、传感器故障）时性能下降。这个问题在现实中非常重要，因为自动驾驶系统需要在复杂多变的环境中可靠运行，而异常输入可能导致灾难性错误。传统方法依赖大量数据训练和事后检测，但无法有效识别未知异常，且缺乏理论依据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将深度编码器建模为层次化通信链，借鉴信息论中的信源编码理论，提出两个设计原则：连续层间互信息平滑变化（D1）和潜在熵随深度单调递减（D2）。在常见网络架构（由相似容量重复层组成的块）下，强制D1自然会导致D2。作者还借鉴了不确定性量化、神经网络的联合信源-信道编码、表示学习中的信源编码以及信息瓶颈理论等相关工作，但将它们整合应用于自动驾驶感知系统的稳定信息流问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将深度感知网络视为层次化通信系统，通过正则化层间信息熵的变化，确保信息流的稳定性和可预测性。实现流程包括：1) 计算每个网络层输出的熵；2) 测量相邻层间的熵下降；3) 对每个块内熵下降的方差施加惩罚；4) 将所有块的惩罚相加形成总Eloss目标；5) 在训练过程中将Eloss与主要任务损失结合。这种方法使网络在面对异常输入时能产生明显的熵偏差信号，同时保持或提高正常情况下的检测精度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 稳定的信息压缩理论框架，证明熵递减会自然出现；2) Eloss正则化器，作为即插即用的训练目标；3) 实验验证显示Eloss能保持或提高检测精度，同时显著增强异常敏感性。相比之前工作，Eloss的不同之处在于：它关注信息压缩过程而非仅关注压缩量；直接正则化层间熵下降方差而非使用变分界限或对抗训练；适用于深度网络的全局稳定信息流；提供统一的架构无关不确定性信号；将信息论稳定性直接整合到训练目标中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Eloss通过稳定深度神经网络中的信息流熵，实现了自动驾驶感知系统在保持竞争力的同时显著提高异常检测能力和模型可解释性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep perception networks in autonomous driving traditionally rely ondata-intensive training regimes and post-hoc anomaly detection, oftendisregarding fundamental information-theoretic constraints governing stableinformation processing. We reconceptualize deep neural encoders as hierarchicalcommunication chains that incrementally compress raw sensory inputs intotask-relevant latent features. Within this framework, we establish twotheoretically justified design principles for robust perception: (D1) smoothvariation of mutual information between consecutive layers, and (D2) monotonicdecay of latent entropy with network depth. Our analysis shows that, underrealistic architectural assumptions, particularly blocks comprising repeatedlayers of similar capacity, enforcing smooth information flow (D1) naturallyencourages entropy decay (D2), thus ensuring stable compression. Guided bythese insights, we propose Eloss, a novel entropy-based regularizer designed asa lightweight, plug-and-play training objective. Rather than marginal accuracyimprovements, this approach represents a conceptual shift: it unifiesinformation-theoretic stability with standard perception tasks, enablingexplicit, principled detection of anomalous sensor inputs through entropydeviations. Experimental validation on large-scale 3D object detectionbenchmarks (KITTI and nuScenes) demonstrates that incorporating Elossconsistently achieves competitive or improved accuracy while dramaticallyenhancing sensitivity to anomalies, amplifying distribution-shift signals by upto two orders of magnitude. This stable information-compression perspective notonly improves interpretability but also establishes a solid theoreticalfoundation for safer, more robust autonomous driving perception systems.</description>
      <author>example@mail.com (Haobo Yang, Shiyan Zhang, Zhuoyi Yang, Jilong Guo, Jun Yang, Xinyu Zhang)</author>
      <guid isPermaLink="false">2509.16277v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN</title>
      <link>http://arxiv.org/abs/2509.17859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种在完全无监督模型中建模人类语言习得中声调学习的方法，证明了生成模型可以无需标记数据学习中文声调类别。&lt;h4&gt;背景&lt;/h4&gt;声调模式是语言学习中计算上最复杂的学习目标之一，研究人类如何学习声调对理解语言习得过程具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，使完全无监督的语言习得模型能够学习中文声调系统，无需任何标记数据。&lt;h4&gt;方法&lt;/h4&gt;使用名为ciwGAN的真实人类语言生成模型，训练三个模型来学习中文声调，并开发追踪内部卷积层中声调表示的方法。&lt;h4&gt;主要发现&lt;/h4&gt;所有三个训练模型在分类变量上显示出F0的统计学显著差异；仅在男性语料上训练的模型能够持续编码声调；模型学习到与人类语言学习者习得阶段相对应的声调系统。&lt;h4&gt;结论&lt;/h4&gt;语言学工具有助于深度学习的可解释性，这些方法最终可用于神经实验，增强我们对语言习得机制的理解。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文概述了在完全无监督的人类语言习得模型中建模声调学习的方法论。声调模式是语言学习中计算上最复杂的学习目标之一。我们认为，一个真实的人类语言生成模型(ciwGAN)可以学习将其分类变量与中文声调类别关联，而无需任何标记数据。所有三个训练好的模型在分类变量上显示出F0的统计学显著差异。仅在男性语料上训练的模型持续编码声调。我们的结果表明，模型不仅学习中文声调对比，还学习与人类语言学习者习得阶段相对应的系统。我们还概述了追踪内部卷积层中声调表示的方法，这表明语言学工具有助于深度学习的可解释性，并最终可用于神经实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper outlines the methodology for modeling tonal learning in fullyunsupervised models of human language acquisition. Tonal patterns are among thecomputationally most complex learning objectives in language. We argue that arealistic generative model of human language (ciwGAN) can learn to associateits categorical variables with Mandarin Chinese tonal categories without anylabeled data. All three trained models showed statistically significantdifferences in F0 across categorical variables. The model trained solely onmale tokens consistently encoded tone. Our results sug- gest that not only doesthe model learn Mandarin tonal contrasts, but it learns a system thatcorresponds to a stage of acquisition in human language learners. We alsooutline methodology for tracing tonal representations in internal convolutionallayers, which shows that linguistic tools can contribute to interpretability ofdeep learning and can ultimately be used in neural experiments.</description>
      <author>example@mail.com (Kai Schenck, Gašper Beguš)</author>
      <guid isPermaLink="false">2509.17859v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis</title>
      <link>http://arxiv.org/abs/2509.17729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种用于检验两样本问题中条件分布相等性的通用框架。该框架基于神经网络的生成方法和样本分割技术，通过将条件分布检验问题转化为无条件问题来解决。论文引入了两种特殊检验方法，并在理论上建立了统计推断的最小最大下界，证明了这些检验方法的有效性。此外，作者还通过数值研究验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;该问题与协变量转移下的迁移学习最为相关。在许多实际应用中，我们需要检验两个条件分布是否相等，这是一个基础但重要的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的框架来检验两样本问题中的条件分布相等性，特别是在协变量转移下的迁移学习场景中。&lt;h4&gt;方法&lt;/h4&gt;基于神经网络生成方法和样本分割技术构建框架，将条件分布检验问题转化为无条件问题。引入了两种特殊检验：基于生成排列的条件分布相等性检验和基于生成分类准确率的条件分布相等性检验。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在特定光滑性条件下，建立了检验两个条件分布相等性的统计推断的最小最大下界。2. 基于生成排列的条件分布相等性检验及其修改版本可以精确地或达到迭代对数因子下界。3. 证明了基于生成分类准确率的条件分布相等性检验的测试一致性。4. 通过推导与偏置Rademacher复杂性和神经网络近似性质相关的新结果，建立了学习条件生成器的收敛速率。&lt;h4&gt;结论&lt;/h4&gt;论文提出的框架和检验方法在理论和实证上都被证明是有效的，能够解决条件分布相等性检验的问题，特别是在协变量转移下的迁移学习场景中。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一个用于检验两样本问题中条件分布相等性的通用框架。这个问题在协变量转移下的迁移学习中最为相关。我们的框架基于神经网络的生成方法和样本分割技术，通过将条件分布检验问题转化为无条件问题来构建。我们引入了两种特殊检验：基于生成排列的条件分布相等性检验和基于生成分类准确率的条件分布相等性检验。理论上，我们在特定光滑性条件下，建立了检验两个条件分布相等性的统计推断的最小最大下界。我们证明了基于生成排列的条件分布相等性检验及其修改版本可以精确地或达到迭代对数因子下界。此外，我们还证明了基于生成分类准确率的条件分布相等性检验的测试一致性。通过推导与最近发展的偏置Rademacher复杂性和神经网络近似性质相关的新结果，我们建立了学习条件生成器的收敛速率。在实证方面，我们进行了包括合成数据集和两个真实世界数据集在内的数值研究，证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a general framework for testing the equality of theconditional distributions in a two-sample problem. This problem is mostrelevant to transfer learning under covariate shift. Our framework is built onneural network-based generative methods and sample splitting techniques bytransforming the conditional distribution testing problem into an unconditionalone. We introduce two special tests: the generative permutation-basedconditional distribution equality test and the generative classificationaccuracy-based conditional distribution equality test. Theoretically, weestablish a minimax lower bound for statistical inference in testing theequality of two conditional distributions under certain smoothness conditions.We demonstrate that the generative permutation-based conditional distributionequality test and its modified version can attain this lower bound precisely orup to some iterated logarithmic factor. Moreover, we prove the testingconsistency of the generative classification accuracy-based conditionaldistribution equality test. We also establish the convergence rate for thelearned conditional generator by deriving new results related to therecently-developed offset Rademacher complexity and approximation propertiesusing neural networks. Empirically, we conduct numerical studies includingsynthetic datasets and two real-world datasets, demonstrating the effectivenessof our approach.</description>
      <author>example@mail.com (Siming Zheng, Meifang Lan, Tong Wang, Yuanyuan Lin)</author>
      <guid isPermaLink="false">2509.17729v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Whitening Spherical Gaussian Mixtures in the Large-Dimensional Regime</title>
      <link>http://arxiv.org/abs/2509.17636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了白化技术在无监督学习中的应用，特别是在高维稀疏数据(LDR)情况下的问题和解决方案。作者发现标准白化矩阵在高维稀疏数据下失效，并提出了一个修正的白化矩阵来恢复渐近正交性，从而提高球形高斯混合模型(GMM)的估计性能。&lt;h4&gt;背景&lt;/h4&gt;白化是一种经典的无监督学习技术，通过标准化数据可以促进估计任务。一个重要的应用是通过构建高阶矩的张量分解来估计潜变量模型。特别是，白化使非球形高斯混合模型(GMM)的均值正交化，从而使相应的矩张量正交可分解，更容易分解。&lt;h4&gt;目的&lt;/h4&gt;研究在高维稀疏数据(LDR)情况下，标准白化矩阵失效的问题，并提出解决方案。具体来说，作者希望解决在高维稀疏数据下，白化后的非球形GMM均值不再正交的问题。&lt;h4&gt;方法&lt;/h4&gt;作者使用随机矩阵理论推导了高维稀疏数据下白化后的非球形GMM均值点积的精确极限。基于这些发现，他们构建了一个修正的白化矩阵，该矩阵能够恢复渐近正交性。&lt;h4&gt;主要发现&lt;/h4&gt;在高维稀疏数据(LDR)情况下，由于样本协方差矩阵的频谱失真，标准白化矩阵变得无效，导致白化后的非球形GMM均值不再正交。使用随机矩阵理论，作者推导出它们的点积在LDR下通常是非零的精确极限。&lt;h4&gt;结论&lt;/h4&gt;通过构建一个修正的白化矩阵，作者恢复了渐近正交性，从而在球形GMM估计中实现了性能提升。&lt;h4&gt;翻译&lt;/h4&gt;白化是无监督学习中的经典技术，通过标准化数据可以促进估计任务。一个重要的应用是通过构建高阶矩的张量来估计潜变量模型。特别是，白化使非球形高斯混合模型(GMM)的均值正交化，从而使相应的矩张量正交可分解，更容易分解。然而，在高维稀疏数据(LDR)情况下，数据是高维且稀少的，基于样本协方差构建的标准白化矩阵变得无效，因为后者存在频谱失真。因此，非球形GMM的白化均值不再正交。使用随机矩阵理论，我们推导出它们的点积的精确极限，这些极限在LDR下通常是非零的。作为我们的主要贡献，我们随后构建了一个修正的白化矩阵，该矩阵恢复了渐近正交性，从而在球形GMM估计中实现了性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whitening is a classical technique in unsupervised learning that canfacilitate estimation tasks by standardizing data. An important application isthe estimation of latent variable models via the decomposition of tensors builtfrom high-order moments. In particular, whitening orthogonalizes the means of aspherical Gaussian mixture model (GMM), thereby making the corresponding momenttensor orthogonally decomposable, hence easier to decompose. However, in thelarge-dimensional regime (LDR) where data are high-dimensional and scarce, thestandard whitening matrix built from the sample covariance becomes ineffectivebecause the latter is spectrally distorted. Consequently, whitened means of aspherical GMM are no longer orthogonal. Using random matrix theory, we deriveexact limits for their dot products, which are generally nonzero in the LDR. Asour main contribution, we then construct a corrected whitening matrix thatrestores asymptotic orthogonality, allowing for performance gains in sphericalGMM estimation.</description>
      <author>example@mail.com (Mohammed Racim Moussa Boudjemaa, Alper Kalle, Xiaoyi Mai, José Henrique de Morais Goulart, Cédric Févotte)</author>
      <guid isPermaLink="false">2509.17636v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Deep Synthetic Cross-Project Approaches for Software Reliability Growth Modeling</title>
      <link>http://arxiv.org/abs/2509.16939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted on April 26, 2025. Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为深度合成跨项目SRGM(DSC-SRGM)的新方法，通过结合合成数据生成与跨项目迁移学习，有效解决了数据稀缺环境下软件可靠性预测准确性的问题。&lt;h4&gt;背景&lt;/h4&gt;软件可靠性增长模型(SRGMs)被广泛用于预测软件可靠性，但在数据稀缺环境下(如早期测试或安全关键系统)预测准确性会下降。虽然跨项目迁移学习可以缓解此问题，但由于现实世界数据集的稀缺性和保密性，其应用受到限制。&lt;h4&gt;目的&lt;/h4&gt;克服数据稀缺环境下软件可靠性预测的挑战，提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出DSC-SRGM方法，使用传统SRGM生成合成数据集以保留现实世界缺陷发现趋势的统计特征，应用基于互相关性的聚类方法识别与目标项目模式相似的合成数据集，然后使用这些数据集训练深度学习模型进行可靠性预测。&lt;h4&gt;主要发现&lt;/h4&gt;在60个真实世界数据集上的评估显示，与传统SRGM相比，预测准确性提高了23.3%；与在真实世界数据集上训练的跨项目深度学习模型相比，提高了32.2%；同时发现过度使用合成数据或简单组合合成和真实世界数据可能会降低预测性能。&lt;h4&gt;结论&lt;/h4&gt;DSC-SRLM是数据稀缺环境下软件可靠性预测的一种有前景的方法，但需要注意保持适当的数据平衡。&lt;h4&gt;翻译&lt;/h4&gt;软件可靠性增长模型(SRGMs)被广泛用于基于在测试或操作阶段收集的缺陷发现数据来预测软件可靠性。然而，在数据稀缺环境中，如早期测试阶段或安全关键系统，它们的预测准确性往往会下降。虽然已经探索了跨项目迁移学习来通过利用过去项目的数据来缓解此问题，但由于现实世界数据集的稀缺性和保密性，其适用性仍然有限。为了克服这些限制，我们提出了深度合成跨项目SRGM(DSC-SRGM)，这是一种将合成数据生成与跨项目迁移学习相结合的新方法。使用传统SRGM生成合成数据集，以保留现实世界缺陷发现趋势的统计特征。应用基于互相关性的聚类方法来识别与目标项目模式相似的合成数据集。然后使用这些数据集训练深度学习模型进行可靠性预测。所提出的方法在60个真实世界数据集上进行了评估，并将其性能与传统SRGM和在真实世界数据集上训练的跨项目深度学习模型进行了比较。DSC-SRGM与传统SRGM相比，预测准确性提高了23.3%，与在真实世界数据集上训练的跨项目深度学习模型相比提高了32.2%。然而，过度使用合成数据或简单组合合成和真实世界数据可能会降低预测性能，这强调了保持适当数据平衡的重要性。这些发现表明，DSC-SRGM是数据稀缺环境下软件可靠性预测的一种有前景的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Software Reliability Growth Models (SRGMs) are widely used to predictsoftware reliability based on defect discovery data collected during testing oroperational phases. However, their predictive accuracy often degrades indata-scarce environments, such as early-stage testing or safety-criticalsystems. Although cross-project transfer learning has been explored to mitigatethis issue by leveraging data from past projects, its applicability remainslimited due to the scarcity and confidentiality of real-world datasets. Toovercome these limitations, we propose Deep Synthetic Cross-project SRGM(DSC-SRGM), a novel approach that integrates synthetic data generation withcross-project transfer learning. Synthetic datasets are generated usingtraditional SRGMs to preserve the statistical characteristics of real-worlddefect discovery trends. A cross-correlation-based clustering method is appliedto identify synthetic datasets with patterns similar to the target project.These datasets are then used to train a deep learning model for reliabilityprediction. The proposed method is evaluated on 60 real-world datasets, and itsperformance is compared with both traditional SRGMs and cross-project deeplearning models trained on real-world datasets. DSC-SRGM achieves up to 23.3%improvement in predictive accuracy over traditional SRGMs and 32.2% overcross-project deep learning models trained on real-world datasets. However,excessive use of synthetic data or a naive combination of synthetic andreal-world data may degrade prediction performance, highlighting the importanceof maintaining an appropriate data balance. These findings indicate thatDSC-SRGM is a promising approach for software reliability prediction indata-scarce environments.</description>
      <author>example@mail.com (Taehyoun Kim, Duksan Ryu, Jongmoon Baik)</author>
      <guid isPermaLink="false">2509.16939v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages</title>
      <link>http://arxiv.org/abs/2509.16914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究构建了一个名为CUTE的多语言语料库，包含中文、维吾尔语、藏语和英语，旨在解决大型语言模型在低资源语言上支持不足的问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在多种NLP任务中表现出色，但这种优势主要限于资源丰富的语言。对于低资源语言，支持仍然不足，训练语料稀缺被认为是主要原因。&lt;h4&gt;目的&lt;/h4&gt;构建一个包含资源丰富语言和低资源语言的多语言语料库，以增强大型语言模型处理低资源语言的能力。&lt;h4&gt;方法&lt;/h4&gt;构建并开源CUTE数据集，包含两个25GB的四语言语料集（一个平行语料集和一个非平行语料集），通过机器翻译获得。CUTE包含中文、英语（资源丰富语言）和维吾尔语、藏语（低资源语言）。&lt;h4&gt;主要发现&lt;/h4&gt;CUTE是目前维吾尔语和藏语最大的开源语料库，能有效增强LLMs处理低资源语言的能力，同时研究了语料库平行性在跨语言迁移学习中的作用。&lt;h4&gt;结论&lt;/h4&gt;通过提供高质量的机器翻译语料，CUTE数据集有助于解决低资源语言在大型语言模型中的支持不足问题，相关资源已向研究社区公开。&lt;h4&gt;翻译&lt;/h4&gt;CUTE数据集包含了中文、维吾尔语、藏语和英语四种语言，其中包含平行和非平行语料，为跨语言研究和翻译提供了资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) demonstrate exceptional zero-shot capabilitiesin various NLP tasks, significantly enhancing user experience and efficiency.However, this advantage is primarily limited to resource-rich languages. Forthe diverse array of low-resource languages, support remains inadequate, withthe scarcity of training corpora considered the primary cause. We construct andopen-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two25GB sets of four-language corpora (one parallel and one non-parallel),obtained through machine translation. CUTE encompasses two resource-richlanguages (Chinese and English) and two low-resource languages (Uyghur andTibetan). Prior to constructing CUTE, human assessment validates that themachine translation quality between Chinese-Uyghur and Chinese-Tibetanapproaches that of Chinese-English translation. CUTE represents the largestopen-source corpus for Uyghur and Tibetan languages to date, and we demonstrateits effectiveness in enhancing LLMs' ability to process low-resource languageswhile investigating the role of corpus parallelism in cross-lingual transferlearning. The CUTE corpus and related models are made publicly available to theresearch community.</description>
      <author>example@mail.com (Wenhao Zhuang, Yuan Sun)</author>
      <guid isPermaLink="false">2509.16914v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>The Good, the Bad, and the Ugly of Atomistic Learning for "Clusters-to-Bulk" Generalization</title>
      <link>http://arxiv.org/abs/2509.16601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了使用差分学习和迁移学习在分子团簇上训练机器学习原子间势(MLIPs)的方法，以将相关波函数理论的准确性扩展到凝聚相。研究验证了团簇训练模型的泛化能力，比较了不同训练方法的准确性和数据效率，并发现仅使用能量训练会引入伪影导致微观可观测值不准确。&lt;h4&gt;背景&lt;/h4&gt;使用机器学习原子间势(MLIPs)在分子团簇的总能量上进行训练是扩展相关波函数理论准确性到凝聚相的热门方法。然而，验证是一个关键挑战，因为在有限温度系综中，参考级别的可观测参考数据不可用。&lt;h4&gt;目的&lt;/h4&gt;构建合成参考数据，评估团簇训练模型在冰-Ih上的泛化能力，研究不同训练方法的准确性和数据效率，并为开发和验证稳健且数据高效的MLIPs提供指导。&lt;h4&gt;方法&lt;/h4&gt;研究人员从预训练的MLIPs构建合成参考数据，评估了团簇训练模型在冰-Ih上的泛化能力，研究了差分学习、单保真度迁移学习和多保真度迁移学习，并考虑了训练时同时使用能量和力以及仅使用能量的场景。将不同方法的性能与真实热力学可观测值进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;1. 将团簇的准确性迁移到体相需要正则化，多保真度迁移学习在同时训练能量和力时效果最佳；2. 仅训练能量会引入伪影：稳定轨迹和低能量误差掩盖了大的力误差，导致微观可观测值不准确；3. 微观结构的准确重现与低力误差强相关，与能量误差弱相关；4. 全局属性如能量和密度与低能量误差相关。&lt;h4&gt;结论&lt;/h4&gt;团簇训练的MLIPs在凝聚相应用中既有前景也有陷阱。训练过程中应纳入力的信息，或在生产应用前进行仔细验证。研究结果为开发和验证稳健且数据高效的MLIPs提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;使用差分或迁移学习在分子团簇的总能量上训练机器学习原子间势(MLIPs)正成为将相关波函数理论准确性扩展到凝聚相的热门途径。然而，关键挑战在于验证，因为有限温度系综中的参考可观测值在参考级别不可用。本研究从预训练的MLIPs构建合成参考数据，并在冰-Ih上评估团簇训练模型的泛化能力，考虑了训练时同时使用能量和力以及仅使用能量的场景。我们研究了差分学习、单保真度迁移学习和多保真度迁移学习相对于真实热力学可观测值的准确性和数据效率。我们发现，将团簇的准确性迁移到体相需要正则化，当同时训练能量和力时，多保真度迁移学习效果最佳。相比之下，仅训练能量会引入伪影：稳定轨迹和低能量误差掩盖了大的力误差，导致微观可观测值不准确。更广泛地说，我们表明微观结构的准确重现与低力误差强相关，而与能量误差弱相关，而全局属性如能量和密度与低能量误差相关。这突显了在训练过程中纳入力或在生产应用前进行仔细验证的必要性。我们的研究结果突出了团簇训练的MLIPs在凝聚相中的前景和陷阱，并为开发和验证稳健且数据高效的MLIPs提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training machine learning interatomic potentials (MLIPs) on total energies ofmolecular clusters using differential or transfer learning is becoming apopular route to extend the accuracy of correlated wave-function theory tocondensed phases. A key challenge, however, lies in validation, as referenceobservables in finite-temperature ensembles are not available at the referencelevel. Here, we construct synthetic reference data from pretrained MLIPs andevaluate the generalizability of cluster-trained models on ice-Ih, consideringscenarios where both energies and forces and where only energies are availablefor training. We study the accuracy and data-efficiency of differential,single-fidelity transfer, and multi-fidelity transfer learning againstground-truth thermodynamic observables. We find that transferring accuracy fromclusters to bulk requires regularization, which is best achieved throughmulti-fidelity transfer learning when training on both energies and forces. Bycontrast, training only on energies introduces artefacts: stable trajectoriesand low energy errors conceal large force errors, leading to inaccuratemicroscopic observables. More broadly, we show that accurate reproduction ofmicroscopic structure correlates strongly with low force errors but only weaklywith energy errors, whereas global properties such as energies and densitiescorrelate with low energy errors. This highlights the need to incorporateforces during training or to apply careful validation before production. Ourresults highlight the promise and pitfalls of cluster-trained MLIPs forcondensed phases and provide guidelines for developing - and critically,validating - robust and data-efficient MLIPs.</description>
      <author>example@mail.com (Mikołaj J. Gawkowski, Mingjia Li, Benjamin X. Shi, Venkat Kapil)</author>
      <guid isPermaLink="false">2509.16601v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>R-Net: A Reliable and Resource-Efficient CNN for Colorectal Cancer Detection with XAI Integration</title>
      <link>http://arxiv.org/abs/2509.16251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种轻量级的CNN网络R-Net，用于结直肠癌的检测和分类，在保持高准确率的同时减少了计算资源需求。&lt;h4&gt;背景&lt;/h4&gt;最先进的卷积神经网络(CNNs)因其计算量大、训练时间长和需要大型数据集而受到批评，结直肠癌(CRC)的检测和分类需要更高效的解决方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级的CNN网络(R-Net)，用于检测和分类结直肠癌，使用内窥镜活检组织病理学苏木精和伊红图像数据集(EBHI)。&lt;h4&gt;方法&lt;/h4&gt;提出轻量级R-Net模型，同时测试六种SOTA CNN模型(DenseNet121, ResNet50, InceptionV3, Xception, MobileNetV2, VGG16)，迁移学习和两种集成模型(多路径-深度-宽度组合和多路径-深度-空间组合)，并集成SHAP、LIME和Grad-CAM等可解释AI技术来理解决策过程。&lt;h4&gt;主要发现&lt;/h4&gt;R-Net轻量级模型达到了99.37%的准确率，优于MobileNet(95.83%)和ResNet50(96.94%)；可解释AI技术能够可视化EBHI图像中贡献于检测和分类的关键部分；像素强度对正确和错误分类图像的影响也是一个创新点。&lt;h4&gt;结论&lt;/h4&gt;构建了一个可靠的、轻量级的CNN R-Net，它需要较少的计算资源但保持强大的预测结果；SOTA CNNs、迁移学习和集成模型扩展了关于CRC分类和检测的知识；可解释AI功能是CRC检测和分类的创新点。&lt;h4&gt;翻译&lt;/h4&gt;最先进的卷积神经网络(CNNs)因其计算量大、训练时间长和需要大型数据集而受到批评。为了克服这一限制，我们提出了一种合理的网络(R-Net)，一种轻量级CNN，仅使用内窥镜活检组织病理学苏木精和伊红图像数据集(EBHI)来检测和分类结直肠癌(CRC)。此外，还在同一数据集上测试了六种SOTA CNN，包括多路径CNN(DenseNet121, ResNet50)、基于深度的CNN(InceptionV3)、基于宽度的多连接CNN(Xception)、深度可分离卷积(MobileNetV2)、基于空间利用的CNN(VGG16)、迁移学习以及两种集成模型。集成模型是多路径-深度-宽度组合(DenseNet121-InceptionV3-Xception)和多路径-深度-空间组合(ResNet18-InceptionV3-VGG16)。然而，提出的R-Net轻量级模型达到了99.37%的准确率，超过了MobileNet(95.83%)和ResNet50(96.94%)。最重要的是，为了理解R-Net的决策过程，集成了可解释AI技术，如SHAP、LIME和Grad-CAM，以可视化EBHI图像的哪些部分贡献于R-Net的检测和分类过程。本研究的主要创新点在于构建了一个可靠的、轻量级的CNN R-Net，它需要较少的计算资源却保持强大的预测结果。SOTA CNNs、迁移学习和集成模型也扩展了我们对CRC分类和检测的知识。可解释AI功能和像素强度对正确和错误分类图像的影响也是CRC检测和分类的一些创新点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) are criticizedfor their extensive computational power, long training times, and largedatasets. To overcome this limitation, we propose a reasonable network (R-Net),a lightweight CNN only to detect and classify colorectal cancer (CRC) using theEnteroscope Biopsy Histopathological Hematoxylin and Eosin Image Dataset(EBHI). Furthermore, six SOTA CNNs, including Multipath-based CNNs(DenseNet121, ResNet50), Depth-based CNNs (InceptionV3), width-basedmulti-connection CNNs (Xception), depth-wise separable convolutions(MobileNetV2), spatial exploitation-based CNNs (VGG16), Transfer learning, andtwo ensemble models are also tested on the same dataset. The ensemble modelsare a multipath-depth-width combination (DenseNet121-InceptionV3-Xception) anda multipath-depth-spatial combination (ResNet18-InceptionV3-VGG16). However,the proposed R-Net lightweight achieved 99.37% accuracy, outperformingMobileNet (95.83%) and ResNet50 (96.94%). Most importantly, to understand thedecision-making of R-Net, Explainable AI such as SHAP, LIME, and Grad-CAM areintegrated to visualize which parts of the EBHI image contribute to thedetection and classification process of R-Net. The main novelty of thisresearch lies in building a reliable, lightweight CNN R-Net that requires fewercomputing resources yet maintains strong prediction results. SOTA CNNs,transfer learning, and ensemble models also extend our knowledge on CRCclassification and detection. XAI functionality and the impact of pixelintensity on correct and incorrect classification images are also somenovelties in CRC detection and classification.</description>
      <author>example@mail.com (Rokonozzaman Ayon, Md Taimur Ahad, Bo Song, Yan Li)</author>
      <guid isPermaLink="false">2509.16251v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>A study on Deep Convolutional Neural Networks, transfer learning, and Mnet model for Cervical Cancer Detection</title>
      <link>http://arxiv.org/abs/2509.16250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种名为S-Net的轻量级CNN模型，用于宫颈癌的检测和分类，解决了现有方法计算资源需求高、训练时间长和数据集要求大的问题。S-Net在保持高准确率的同时，显著提高了计算效率和推理速度，并通过可解释AI技术增强了模型决策过程的透明度。&lt;h4&gt;背景&lt;/h4&gt;早期和准确的巴氏涂片分析对改善宫颈癌患者预后和降低死亡率至关重要。然而，现有的最先进卷积神经网络需要大量计算资源、长时间训练和大型数据集，限制了其在实际应用中的可行性。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级CNN模型专门用于宫颈癌检测和分类，评估多种SOTA CNN模型在巴氏涂片图像上的表现，并应用可解释AI技术提高模型决策过程的透明度。&lt;h4&gt;方法&lt;/h4&gt;开发名为S-Net的轻量级CNN模型；评估六种SOTA CNN模型(DenseNet201、ResNet152、Serasnet152、Xception、MobileNetV2和VGG19)使用迁移学习的效果；应用SHAP、LIME和Grad-CAM等可解释AI技术来可视化和解释模型预测的关键图像区域。&lt;h4&gt;主要发现&lt;/h4&gt;所有模型(包括S-Net)都达到了可比较的准确率，S-Net达到99.99%。S-Net在计算效率和推理时间方面显著优于其他SOTA CNN模型，使其更适合实时和资源受限应用。XAI技术成功可视化了影响模型预测的关键图像区域，增强了模型的可解释性。&lt;h4&gt;结论&lt;/h4&gt;S-Net是一个高度准确且计算轻量级的模型，能够快速推理，同时通过XAI集成保持可解释性。该研究还分析了SOTA CNN的行为，研究了负迁移学习对巴氏涂片图像的影响，并检查了正确和错误分类样本中的像素强度模式。&lt;h4&gt;翻译&lt;/h4&gt;通过巴氏涂片分析的早期和准确检测对于改善宫颈癌患者预后和降低死亡率至关重要。最先进的卷积神经网络需要大量计算资源、长时间训练和大型数据集。在本研究中，开发了一种名为S-Net的轻量级CNN模型，专门用于使用巴氏涂片图像进行宫颈癌检测和分类，以解决这些局限性。除了S-Net外，还使用迁移学习评估了六种SOTA CNN模型。所有模型都达到了可比较的准确率，S-Net达到99.99%。然而，S-Net在计算效率和推理时间方面显著优于其他SOTA CNN模型，使其成为实时和资源受限应用中更实用的选择。基于CNN的医疗诊断的主要局限性是决策过程缺乏透明度。为解决这一问题，采用了可解释AI技术，如SHAP、LIME和Grad-CAM，以可视化和解释影响模型预测的关键图像区域。本研究的创新之处在于开发了一种高度准确且计算轻量级的模型，能够快速推理，同时通过XAI集成保持可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early and accurate detection through Pap smear analysis is critical toimproving patient outcomes and reducing mortality of Cervical cancer.State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) requiresubstantial computational resources, extended training time, and largedatasets. In this study, a lightweight CNN model, S-Net (Simple Net), isdeveloped specifically for cervical cancer detection and classification usingPap smear images to address these limitations. Alongside S-Net, six SOTA CNNswere evaluated using transfer learning, including multi-path (DenseNet201,ResNet152), depth-based (Serasnet152), width-based multi-connection (Xception),depth-wise separable convolutions (MobileNetV2), and spatial exploitation-based(VGG19). All models, including S-Net, achieved comparable accuracy, with S-Netreaching 99.99%. However, S-Net significantly outperforms the SOTA CNNs interms of computational efficiency and inference time, making it a morepractical choice for real-time and resource-constrained applications. A majorlimitation in CNN-based medical diagnosis remains the lack of transparency inthe decision-making process. To address this, Explainable AI (XAI) techniques,such as SHAP, LIME, and Grad-CAM, were employed to visualize and interpret thekey image regions influencing model predictions. The novelty of this study liesin the development of a highly accurate yet computationally lightweight model(S-Net) caPable of rapid inference while maintaining interpretability throughXAI integration. Furthermore, this work analyzes the behavior of SOTA CNNs,investigates the effects of negative transfer learning on Pap smear images, andexamines pixel intensity patterns in correctly and incorrectly classifiedsamples.</description>
      <author>example@mail.com (Saifuddin Sagor, Md Taimur Ahad, Faruk Ahmed, Rokonozzaman Ayon, Sanzida Parvin)</author>
      <guid isPermaLink="false">2509.16250v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Learning Dexterous Manipulation with Quantized Hand State</title>
      <link>http://arxiv.org/abs/2509.17450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DQ-RISE的方法，用于解决灵巧机器人操作中手臂和手部协调控制的问题，通过对手部状态量化和连续松弛技术，实现了更平衡高效的学习。&lt;h4&gt;背景&lt;/h4&gt;灵巧的机械手使机器人能够执行需要精细控制和适应性的复杂操作，但高度自由度使手部和手臂动作紧密耦合，使学习和控制变得困难。成功的灵巧操作需要精确的手部动作、手臂的精确空间定位和协调的手臂-手部动力学。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉运动策略中高维手部动作主导耦合动作空间而损害手臂控制的问题，提出一种能够从数据中学习手臂-手部协调同时防止手部动作淹没动作空间的方法。&lt;h4&gt;方法&lt;/h4&gt;提出DQ-RISE方法，对手部状态进行量化以简化手部运动预测同时保留基本模式，并应用连续松弛技术，允许手臂动作与这些紧凑的手部状态联合扩散。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明DQ-RISE实现了更平衡和高效的学习，为结构化和可推广的灵巧操作铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;DQ-RISE通过对手部状态量化和连续松弛处理，实现了手臂和手部动作的平衡协调控制，为灵巧机器人操作提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;灵巧的机械手使机器人能够执行需要精细控制和适应性的复杂操作。实现此类操作具有挑战性，因为高度自由度使手部和手臂动作紧密耦合，使学习和控制变得困难。成功的灵巧操作不仅需要精确的手部动作，还需要手臂的精确空间定位和协调的手臂-手部动力学。然而，大多数现有的视觉运动策略将手臂和手部动作表示在单一组合空间中，这通常会导致高维度的手部动作主导耦合动作空间，从而损害手臂控制。为解决这一问题，我们提出了DQ-RISE，它对手部状态进行量化以简化手部运动预测同时保留基本模式，并应用连续松弛技术，允许手臂动作与这些紧凑的手部状态联合扩散。这种设计使策略能够从数据中学习手臂-手部协调，同时防止手部动作淹没动作空间。实验表明，DQ-RISE实现了更平衡和高效的学习，为结构化和可推广的灵巧操作铺平了道路。项目网站：http://rise-policy.github.io/DQ-RISE/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous robotic hands enable robots to perform complex manipulations thatrequire fine-grained control and adaptability. Achieving such manipulation ischallenging because the high degrees of freedom tightly couple hand and armmotions, making learning and control difficult. Successful dexterousmanipulation relies not only on precise hand motions, but also on accuratespatial positioning of the arm and coordinated arm-hand dynamics. However, mostexisting visuomotor policies represent arm and hand actions in a singlecombined space, which often causes high-dimensional hand actions to dominatethe coupled action space and compromise arm control. To address this, wepropose DQ-RISE, which quantizes hand states to simplify hand motion predictionwhile preserving essential patterns, and applies a continuous relaxation thatallows arm actions to diffuse jointly with these compact hand states. Thisdesign enables the policy to learn arm-hand coordination from data whilepreventing hand actions from overwhelming the action space. Experiments showthat DQ-RISE achieves more balanced and efficient learning, paving the waytoward structured and generalizable dexterous manipulation. Project website:http://rise-policy.github.io/DQ-RISE/</description>
      <author>example@mail.com (Ying Feng, Hongjie Fang, Yinong He, Jingjing Chen, Chenxi Wang, Zihao He, Ruonan Liu, Cewu Lu)</author>
      <guid isPermaLink="false">2509.17450v1</guid>
      <pubDate>Tue, 23 Sep 2025 16:07:36 +0800</pubDate>
    </item>
    <item>
      <title>Biologically Plausible Online Hebbian Meta-Learning: Two-Timescale Local Rules for Spiking Neural Brain Interfaces</title>
      <link>http://arxiv.org/abs/2509.14447v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, submitted to ICLR 2026; under review. Replacement  was made to correct mistakes in metadata and header in PDF&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种在线SNN解码器，使用局部三因素学习规则和双时间尺度资格迹，解决了脑机接口中神经信号不稳定和内存限制的问题，实现了内存高效且持续自适应的神经解码。&lt;h4&gt;背景&lt;/h4&gt;脑机接口面临神经信号不稳定和内存限制的挑战，特别是在实时植入式应用中。&lt;h4&gt;目的&lt;/h4&gt;引入一种在线SNN解码器，避免通过时间反向传播，同时保持竞争性性能。&lt;h4&gt;方法&lt;/h4&gt;结合误差调制的Hebbian更新、快速/慢速轨迹合并和自适应学习率控制，仅需O(1)内存，而BPTT方法需要O(T)内存。&lt;h4&gt;主要发现&lt;/h4&gt;在两个灵长类动物数据集上实现了可比的解码精度，内存减少28-35%，且比BPTT训练的SNN收敛更快；闭环模拟展示了适应神经干扰和无需离线校准从头学习的能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作实现了内存高效、持续自适应的神经解码，适用于资源受限的植入式BCI系统。&lt;h4&gt;翻译&lt;/h4&gt;脑机接口面临来自神经信号不稳定和实时植入式应用的内存限制的挑战。我们引入了一种使用局部三因素学习规则和双时间尺度资格迹的在线SNN解码器，避免了通过时间反向传播，同时保持了竞争性性能。我们的方法结合了误差调制的Hebbian更新、快速/慢速轨迹合并和自适应学习率控制，仅需O(1)内存，而BPTT方法需要O(T)内存。在两个灵长类动物数据集上的评估实现了可比的解码精度，内存减少28-35%，且比BPTT训练的SNN收敛更快。使用合成神经群体的闭环模拟展示了适应神经干扰和无需离线校准从头学习的能力。这项工作实现了内存高效、持续自适应的神经解码，适用于资源受限的植入式BCI系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-Computer Interfaces face challenges from neural signal instability andmemory constraints for real-time implantable applications. We introduce anonline SNN decoder using local three-factor learning rules with dual-timescaleeligibility traces that avoid backpropagation through time while maintainingcompetitive performance. Our approach combines error-modulated Hebbian updates,fast/slow trace consolidation, and adaptive learning rate control, requiringonly O(1) memory versus O(T) for BPTT methods. Evaluations on two primatedatasets achieve comparable decoding accuracy (Pearson $R \geq 0.63$ Zenodo, $R\geq 0.81$ MC Maze) with 28-35% memory reduction and faster convergence thanBPTT-trained SNNs. Closed-loop simulations with synthetic neural populationsdemonstrate adaptation to neural disruptions and learning from scratch withoutoffline calibration. This work enables memory-efficient, continuously adaptiveneural decoding suitable for resource-constrained implantable BCI systems.</description>
      <author>example@mail.com (Sriram V. C. Nallani, Gautham Ramachandran, Sahil Shah)</author>
      <guid isPermaLink="false">2509.14447v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
  <item>
      <title>Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2509.15882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CrossI2P是一种自监督框架，通过统一跨模态学习和两阶段配准方法，解决了图像到点云配准中的语义-几何差距和局部最优问题，显著提升了自主系统感知的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;将2D和3D传感器模态桥接对自主系统鲁棒感知至关重要，但图像到点云配准面临语义-几何差距（纹理丰富但深度模糊的图像与稀疏但度量精确的点云之间的差距）和现有方法收敛到局部最优的挑战。&lt;h4&gt;目的&lt;/h4&gt;克服现有图像到点云配准方法的局限性，提高配准的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;CrossI2P采用自监督框架，包含三个关键部分：1)通过双路径对比学习学习几何-语义融合嵌入空间，实现无标注的双向对齐；2)采用粗到细配准范式，先建立超点-超像素对应关系，再进行几何约束的点级细化；3)使用动态训练机制和梯度归一化平衡不同任务的损失。&lt;h4&gt;主要发现&lt;/h4&gt;CrossI2P在KITTI Odometry基准上比最先进方法提高23.7%，在nuScenes上提高37.9%，显著提升了准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;CrossI2P成功解决了2D和3D传感器模态桥接的挑战，特别是在图像到点云配准方面取得了突破性进展。&lt;h4&gt;翻译&lt;/h4&gt;将2D和3D传感器模态桥接起来对自主系统的鲁棒感知至关重要。然而，由于纹理丰富但深度模糊的图像与稀疏但度量精确的点云之间的语义-几何差距，以及现有方法倾向于收敛到局部最优，图像到点云(I2P)配准仍然具有挑战性。为了克服这些局限性，我们引入了CrossI2P，这是一个自监督框架，在单一的端到端管道中统一了跨模态学习和两阶段配准。首先，我们通过双路径对比学习学习几何-语义融合的嵌入空间，实现无标注、双向的2D纹理和3D结构对齐。其次，我们采用粗到细的配准范式：全局阶段通过联合内模态上下文和跨模态交互建模建立超点-超像素对应关系，然后进行几何约束的点级细化以实现精确配准。第三，我们采用具有梯度归一化的动态训练机制来平衡特征对齐、对应关系细化和姿态估计的损失。大量实验表明，CrossI2P在KITTI Odometry基准上比最先进的方法提高了23.7%，在nuScenes上提高了37.9%，显著提高了准确性和鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决图像到点云（I2P）配准问题，特别是2D图像和3D点云之间的语义-几何鸿沟问题。这个问题在自动驾驶和机器人系统中非常重要，因为它关系到如何有效融合来自不同传感器的数据（如摄像头和激光雷达），确保几何一致性，实现准确的环境感知和决策。在现实应用中，移动平台、手持设备等场景常常缺乏预校准的外参，这使得传统方法难以应对，而本文的方法能够在无需精确校准的情况下实现精确配准。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的三大局限：语义不匹配、局部最优收敛问题以及非端到端特性。他们借鉴了对比学习（如CLIP）、粗到细配准策略（如CoFiI2P）和Transformer架构等现有技术，但针对I2P配准的特殊需求进行了创新。作者设计了自监督对比学习模块来弥合语义鸿沟，采用两阶段配准策略避免局部最优，并引入可微PnP模块实现真正的端到端优化。通过动态协作训练机制，作者平衡了不同任务的损失，提高了整体性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自监督学习将2D图像和3D点云映射到共享的几何-语义融合空间，采用粗到细的两阶段配准策略，并利用可微PnP模块实现端到端优化。整体流程包括：1）自监督对比学习模块，使用点云和图像特征提取器提取特征并通过对比损失对齐；2）两阶段特征匹配，先通过Transformer建立全局对应关系，再进行局部点级精炼；3）可微PnP模块，计算姿态损失和重投影误差；4）动态协作训练机制，使用GradNorm平衡不同任务的损失权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）自监督对比学习模块，同时考虑模态内实例区分和跨模态对齐；2）基于Transformer的两阶段配准策略，先全局后局部；3）可微PnP模块，实现真正的端到端优化；4）动态协作训练机制，自适应平衡多任务损失。相比之前的工作，CrossI2P解决了语义对齐与几何优化的冲突，避免了局部最优问题，实现了完整的端到端训练，并在噪声和遮挡等挑战性条件下表现出更强的鲁棒性。实验表明，该方法在KITTI和nuScenes数据集上分别比最先进方法提高了23.7%和37.9%的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CrossI2P通过创新的自监督对比学习、两阶段配准和可微PnP模块，实现了无需标注的高精度、鲁棒性强的图像到点云端到端配准，显著提升了自动驾驶和机器人系统中的多模态感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bridging 2D and 3D sensor modalities is critical for robust perception inautonomous systems. However, image-to-point cloud (I2P) registration remainschallenging due to the semantic-geometric gap between texture-rich butdepth-ambiguous images and sparse yet metrically precise point clouds, as wellas the tendency of existing methods to converge to local optima. To overcomethese limitations, we introduce CrossI2P, a self-supervised framework thatunifies cross-modal learning and two-stage registration in a single end-to-endpipeline. First, we learn a geometric-semantic fused embedding space viadual-path contrastive learning, enabling annotation-free, bidirectionalalignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fineregistration paradigm: a global stage establishes superpoint-superpixelcorrespondences through joint intra-modal context and cross-modal interactionmodeling, followed by a geometry-constrained point-level refinement for preciseregistration. Third, we employ a dynamic training mechanism with gradientnormalization to balance losses for feature alignment, correspondencerefinement, and pose estimation. Extensive experiments demonstrate thatCrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometrybenchmark and by 37.9% on nuScenes, significantly improving both accuracy androbustness.</description>
      <author>example@mail.com (Xingmei Wang, Xiaoyu Hu, Chengkai Huang, Ziyan Zeng, Guohao Nie, Quan Z. Sheng, Lina Yao)</author>
      <guid isPermaLink="false">2509.15882v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning</title>
      <link>http://arxiv.org/abs/2509.16078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICDM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Dual-Masked Autoencoder (DMAE)的新型掩码时间序列建模框架，用于无监督多变量时间序列表示学习。该框架通过两个互补的预训练任务和特征级对齐约束，学习时间上连贯且语义丰富的表示，在多种下游任务上展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;无监督多变量时间序列表示学习旨在从原始序列中提取紧凑且信息丰富的表示，而无需依赖标签，以便有效地迁移到各种下游任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为Dual-Masked Autoencoder (DMAE)的新型掩码时间序列建模框架，用于无监督多变量时间序列表示学习。&lt;h4&gt;方法&lt;/h4&gt;DMAE制定了两个互补的预训练任务：(1)基于可见属性重建掩码值；(2)在教师编码器的指导下估计掩码特征的潜在表示。此外，引入了特征级对齐约束，鼓励预测的潜在表示与教师的输出保持一致。通过联合优化这些目标，DMAE学习时间上连贯且语义丰富的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在分类、回归和预测任务上的综合评估表明，该方法比竞争性基线实现了一致且优越的性能。&lt;h4&gt;结论&lt;/h4&gt;DMAE是一种有效的无监督多变量时间序列表示学习方法，通过双重掩码自编码器和特征级对齐约束，能够学习高质量的时间序列表示。&lt;h4&gt;翻译&lt;/h4&gt;无监督多变量时间序列(MTS)表示学习旨在从原始序列中提取紧凑且信息丰富的表示，而无需依赖标签，从而能够高效地迁移到各种下游任务。在本文中，我们提出了Dual-Masked Autoencoder (DMAE)，一种用于无监督MTS表示学习的新型掩码时间序列建模框架。DMAE制定了两个互补的预训练任务：(1)基于可见属性重建掩码值；(2)在教师编码器的指导下估计掩码特征的潜在表示。为了进一步提高表示质量，我们引入了特征级对齐约束，鼓励预测的潜在表示与教师的输出保持一致。通过联合优化这些目标，DMAE学习时间上连贯且语义丰富的表示。在分类、回归和预测任务上的综合评估表明，我们的方法比竞争性基线实现了一致且优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised multivariate time series (MTS) representation learning aims toextract compact and informative representations from raw sequences withoutrelying on labels, enabling efficient transfer to diverse downstream tasks. Inthis paper, we propose Dual-Masked Autoencoder (DMAE), a novel maskedtime-series modeling framework for unsupervised MTS representation learning.DMAE formulates two complementary pretext tasks: (1) reconstructing maskedvalues based on visible attributes, and (2) estimating latent representationsof masked features, guided by a teacher encoder. To further improverepresentation quality, we introduce a feature-level alignment constraint thatencourages the predicted latent representations to align with the teacher'soutputs. By jointly optimizing these objectives, DMAE learns temporallycoherent and semantically rich representations. Comprehensive evaluationsacross classification, regression, and forecasting tasks demonstrate that ourapproach achieves consistent and superior performance over competitivebaselines.</description>
      <author>example@mail.com (Yi Xu, Yitian Zhang, Yun Fu)</author>
      <guid isPermaLink="false">2509.16078v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>FMD-TransUNet: Abdominal Multi-Organ Segmentation Based on Frequency Domain Multi-Axis Representation Learning and Dual Attention Mechanisms</title>
      <link>http://arxiv.org/abs/2509.16044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FMD-TransUNet的新型框架，通过整合多轴外部权重块(MEWB)和改进的双注意力模块(DA+)，有效提高了腹部多器官分割的准确性，尤其在处理小型、不规则或解剖结构复杂的器官方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;准确的腹部多器官分割对临床应用至关重要，但现有基于深度学习的自动分割方法在分割小型、不规则或解剖结构复杂的器官时仍有困难，且大多数方法专注于空间域分析，忽略了频域表示的协同潜力。&lt;h4&gt;目的&lt;/h4&gt;解决现有腹部多器官分割方法的局限性，提出一种能够精确分割腹部多器官的新型框架。&lt;h4&gt;方法&lt;/h4&gt;创新地将多轴外部权重块(MEWB)和改进的双注意力模块(DA+)集成到TransUNet框架中。MEWB提取多轴频域特征捕捉全局解剖结构和局部边界细节；DA+块利用深度可分离卷积并融入空间和通道注意力机制增强特征融合，减少冗余信息，缩小编码器和解码器间的语义差距。&lt;h4&gt;主要发现&lt;/h4&gt;在Synapse数据集上的实验表明，FMD-TransUNet优于其他最新方法，在八个腹部器官上实现平均81.32%的DSC和16.35毫米的HD。与基线模型相比，平均DSC提高3.84%，平均HD降低15.34毫米。&lt;h4&gt;结论&lt;/h4&gt;FMD-TransUNNet在提高腹部多器官分割准确性方面是有效的，特别是在处理复杂解剖结构的小型器官时。&lt;h4&gt;翻译&lt;/h4&gt;准确的腹部多器官分割对临床应用至关重要。尽管已开发出许多基于深度学习的自动分割方法，但它们在分割小型、不规则或解剖结构复杂的器官时仍然存在困难。此外，大多数当前方法专注于空间域分析，常常忽略了频域表示的协同潜力。为解决这些局限性，我们提出了一种名为FMD-TransUNet的新型框架，用于精确的腹部多器官分割。它创新性地将多轴外部权重块(MEWB)和改进的双注意力模块(DA+)集成到TransUNet框架中。MEWB提取多轴频域特征，以捕捉全局解剖结构和局部边界细节，为空间域表示提供互补信息。DA+块利用深度可分离卷积，并融入空间和通道注意力机制，以增强特征融合，减少冗余信息，缩小编码器和解码器之间的语义差距。在Synapse数据集上的实验验证表明，FMD-TransUNet优于其他最新的最先进方法，在八个腹部器官上实现了平均81.32%的DSC和16.35毫米的HD。与基线模型相比，平均DSC提高了3.84%，平均HD降低了15.34毫米。这些结果证明了FMD-TransUNNet在提高腹部多器官分割准确性方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决腹部多器官分割中难以准确分割小型、不规则或解剖结构复杂器官的问题，以及现有方法过度依赖空间域分析而忽视频率域潜力的问题。这个问题在临床上非常重要，因为准确的腹部多器官分割对疾病诊断、放疗计划和手术导航等关键医疗应用至关重要，同时能减少放射科医生手动标注的工作负担和主观差异，提高诊断一致性和临床效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统方法难以处理复杂器官形状和模糊边界，相邻器官灰度分布重叠，以及器官大小差异大。他们发现大多数方法专注于空间域分析而忽视频率域潜力，以及Transformer模型虽能捕捉全局上下文但缺乏考虑图像特定属性的机制。基于这些分析，作者借鉴了TransUNet的架构，结合了频率域方法如GFNet和GFUNet的思想，但改进为多轴频率分析，并受DA-TransUNet启发改进了双注意力机制，最终设计出FMD-TransUNet框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是融合空间域和频率域表示，通过多轴频率分析提供更丰富的特征表示，并结合改进的双注意力机制增强特征提取能力。整体流程包括：1)编码器部分使用卷积块提取低级特征，MEWB模块增强频率域特征，DA+块细化特征，Transformer层捕捉全局上下文；2)解码器部分通过上采样和特征融合重建高分辨率特征，MEWB模块在每个上采样步骤后应用；3)跳跃连接使用DA+块过滤无关信息，缩小编码器和解码器间的语义差距。MEWB模块通过多轴DFT提取方向频率特征，DA+模块利用深度可分离卷积结合空间和通道注意力机制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多轴外部权重块(MEWB)应用多轴二维离散傅里叶变换提取方向频率特征，提供比单轴分析更丰富的表示；2)改进的双注意力模块(DA+)使用深度可分离卷积减少计算复杂度和冗余信息；3)首次系统性地整合频率域多轴表示和空间域表示，实现互补优势。相比之前工作，FMD-TransUNet不同于传统U-Net变体在于引入了频率域分析和Transformer；不同于其他Transformer模型在于整合了频率域信息；不同于频率域方法在于使用多轴分析而非单轴；不同于双注意力机制模型在于使用深度可分离卷积并结合频率域信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FMD-TransUNet通过创新性地融合多轴频率域特征学习和改进的双注意力机制，显著提高了腹部多器官分割的准确性，特别是在处理小型、不规则和复杂解剖结构器官时展现出卓越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate abdominal multi-organ segmentation is critical for clinicalapplications. Although numerous deep learning-based automatic segmentationmethods have been developed, they still struggle to segment small, irregular,or anatomically complex organs. Moreover, most current methods focus onspatial-domain analysis, often overlooking the synergistic potential offrequency-domain representations. To address these limitations, we propose anovel framework named FMD-TransUNet for precise abdominal multi-organsegmentation. It innovatively integrates the Multi-axis External Weight Block(MEWB) and the improved dual attention module (DA+) into the TransUNetframework. The MEWB extracts multi-axis frequency-domain features to captureboth global anatomical structures and local boundary details, providingcomplementary information to spatial-domain representations. The DA+ blockutilizes depthwise separable convolutions and incorporates spatial and channelattention mechanisms to enhance feature fusion, reduce redundant information,and narrow the semantic gap between the encoder and decoder. Experimentalvalidation on the Synapse dataset shows that FMD-TransUNet outperforms otherrecent state-of-the-art methods, achieving an average DSC of 81.32\% and a HDof 16.35 mm across eight abdominal organs. Compared to the baseline model, theaverage DSC increased by 3.84\%, and the average HD decreased by 15.34 mm.These results demonstrate the effectiveness of FMD-TransUNet in improving theaccuracy of abdominal multi-organ segmentation.</description>
      <author>example@mail.com (Fang Lu, Jingyu Xu, Qinxiu Sun, Qiong Lou)</author>
      <guid isPermaLink="false">2509.16044v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization</title>
      <link>http://arxiv.org/abs/2509.15791v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MS-UDG的新方法，用于解决无监督领域泛化问题。该方法通过学习最小充分语义表示，既保留了共享的语义信息，又去除了与语义无关的变化信息，在多个基准测试上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;深度学习的泛化能力在监督学习场景中已被广泛研究，但在无监督场景中研究较少。无监督领域泛化(UDG)任务旨在增强使用自监督学习等无监督学习技术训练的模型的泛化能力。然而，UDG面临在没有类别标签的情况下区分语义和变化的挑战，且实际环境中通常没有领域标签可用。&lt;h4&gt;目的&lt;/h4&gt;通过将UDG形式化为学习最小充分语义表示的任务来解决这些局限性。这种表示需要满足：(i)保留增强视图间共享的所有语义信息（充分性），(ii)最大程度地去除与语义无关的信息（最小性）。&lt;h4&gt;方法&lt;/h4&gt;从信息论角度为这些目标提供理论依据，表明优化表示以实现充分性和最小性可以直接减少分布外风险。通过MS-UDG实现这种优化，集成了：(a)基于InfoNCE的目标来实现充分性；(b)两个互补组件来促进最小性：一种新的语义-变化解纠缠损失和一种基于重建的机制来捕捉充分的变化。&lt;h4&gt;主要发现&lt;/h4&gt;在流行的无监督领域泛化基准上，MS-UDG设立了新的最先进水平，一致性地优于现有的SSL和UDG方法，且在表示学习过程中不需要类别或领域标签。&lt;h4&gt;结论&lt;/h4&gt;通过学习最小充分语义表示，MS-UDG方法有效解决了无监督领域泛化中的挑战，使模型能够更好地泛化到未见过的领域。&lt;h4&gt;翻译&lt;/h4&gt;深度学习的泛化能力在监督设置中已被广泛研究，但在无监督场景中仍较少探索。最近，无监督领域泛化(UDG)任务被提出，以增强使用普遍的无监督学习技术（如自监督学习SSL）训练的模型的泛化能力。UDG面临在没有类别标签的情况下区分语义与变化的挑战。尽管一些近期方法采用领域标签来解决这个问题，但在实际环境中这些标签通常不可用。在本文中，我们通过将UDG形式化为学习最小充分语义表示的任务来解决这些局限性：一种保留增强视图间所有共享语义信息（充分性）并最大程度去除与语义无关信息（最小性）的表示。我们从信息论角度为这些目标提供理论依据，证明优化表示以实现充分性和最小性可以直接减少分布外风险。实践中，我们通过最小充分UDG(MS-UDG)实现这种优化，这是一个可学习模型，通过集成(a)基于InfoNCE的目标来实现充分性；(b)两个互补组件来促进最小性：一种新的语义-变化解纠缠损失和一种基于重建的机制来捕捉充分的变化。实验上，MS-UDG在流行的无监督领域泛化基准上设立了新的最先进水平，一致性地超越现有的SSL和UDG方法，在表示学习过程中不需要类别或领域标签。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generalization ability of deep learning has been extensively studied insupervised settings, yet it remains less explored in unsupervised scenarios.Recently, the Unsupervised Domain Generalization (UDG) task has been proposedto enhance the generalization of models trained with prevalent unsupervisedlearning techniques, such as Self-Supervised Learning (SSL). UDG confronts thechallenge of distinguishing semantics from variations without category labels.Although some recent methods have employed domain labels to tackle this issue,such domain labels are often unavailable in real-world contexts. In this paper,we address these limitations by formalizing UDG as the task of learning aMinimal Sufficient Semantic Representation: a representation that (i) preservesall semantic information shared across augmented views (sufficiency), and (ii)maximally removes information irrelevant to semantics (minimality). Wetheoretically ground these objectives from the perspective of informationtheory, demonstrating that optimizing representations to achieve sufficiencyand minimality directly reduces out-of-distribution risk. Practically, weimplement this optimization through Minimal-Sufficient UDG (MS-UDG), alearnable model by integrating (a) an InfoNCE-based objective to achievesufficiency; (b) two complementary components to promote minimality: a novelsemantic-variation disentanglement loss and a reconstruction-based mechanismfor capturing adequate variation. Empirically, MS-UDG sets a newstate-of-the-art on popular unsupervised domain-generalization benchmarks,consistently outperforming existing SSL and UDG methods, without category ordomain labels during representation learning.</description>
      <author>example@mail.com (Tan Pan, Kaiyu Guo, Dongli Xu, Zhaorui Tan, Chen Jiang, Deshu Chen, Xin Guo, Brian C. Lovell, Limei Han, Yuan Cheng, Mahsa Baktashmotlagh)</author>
      <guid isPermaLink="false">2509.15791v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SONAR: Self-Distilled Continual Pre-training for Domain Adaptive Audio Representation</title>
      <link>http://arxiv.org/abs/2509.15703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SONAR的自蒸馏持续预训练框架，用于领域自适应的音频表示学习，能够在适应新领域的同时减轻灾难性遗忘。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在大型数据集如AudioSet上已成为音频表示学习的主导范式，新标记音频的持续流入为丰富静态表示提供了机会。&lt;h4&gt;目的&lt;/h4&gt;解决从头重新训练模型的高计算成本问题，避免丢弃先前训练模型中嵌入的宝贵知识，实现高效的持续学习。&lt;h4&gt;方法&lt;/h4&gt;提出SONAR(Self-distilled cONtinual pre-training for domain adaptive Audio Representation)框架，基于BEATs构建，通过三种策略解决关键挑战：为新旧数据实施联合采样策略，应用正则化平衡特异性和通用性，动态扩展tokenizer代码本以适应新的声学模式。&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同领域的实验中，SONAR方法实现了高度的适应性和对遗忘的强大抵抗力。&lt;h4&gt;结论&lt;/h4&gt;SONAR是一种有效的持续预训练框架，能够在适应新音频领域的同时保持对先前知识的记忆，解决了传统方法计算成本高和知识遗忘的问题。&lt;h4&gt;翻译&lt;/h4&gt;在AudioSet等大型数据集上进行自监督学习已成为音频表示学习的主导范式。虽然新标记音频的持续流入为丰富这些静态表示提供了机会，但简单的方法是使用所有可用数据从头开始重新训练模型。然而，这种方法计算上难以承受，并且丢弃了先前训练模型权重中嵌入的宝贵知识。为解决这种低效率问题，我们提出了SONAR(自蒸馏持续预训练用于领域自适应音频表示)，这是一个基于BEATs构建的持续预训练框架。SONAR在有效适应新领域的同时，通过解决三个关键挑战来减轻灾难性遗忘：为新旧数据实施联合采样策略，应用正则化以平衡特异性和通用性，以及为新的声学模式动态扩展tokenizer代码本。在四个不同领域的实验表明，我们的方法实现了高度的适应性和对遗忘的强大抵抗力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) on large-scale datasets like AudioSet hasbecome the dominant paradigm for audio representation learning. While thecontinuous influx of new, unlabeled audio presents an opportunity to enrichthese static representations, a naive approach is to retrain the model fromscratch using all available data. However, this method is computationallyprohibitive and discards the valuable knowledge embedded in the previouslytrained model weights. To address this inefficiency, we propose SONAR(Self-distilled cONtinual pre-training for domain adaptive AudioRepresentation), a continual pre-training framework built upon BEATs. SONAReffectively adapts to new domains while mitigating catastrophic forgetting bytackling three key challenges: implementing a joint sampling strategy for newand prior data, applying regularization to balance specificity and generality,and dynamically expanding the tokenizer codebook for novel acoustic patterns.Experiments across four distinct domains demonstrate that our method achievesboth high adaptability and robust resistance to forgetting.</description>
      <author>example@mail.com (Yizhou Zhang, Yuan Gao, Wangjin Zhou, Zicheng Yuan, Keisuke Imoto, Tatsuya Kawahara)</author>
      <guid isPermaLink="false">2509.15703v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification</title>
      <link>http://arxiv.org/abs/2509.15591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种称为潜在区域网络(LZN)的统一框架，旨在解决机器学习中的三个核心问题：生成建模、表示学习和分类，这些问题的现有解决方案通常相互分离。&lt;h4&gt;背景&lt;/h4&gt;生成建模、表示学习和分类是机器学习的三个核心问题，但它们的最先进解决方案仍然相互独立，缺乏统一性。&lt;h4&gt;目的&lt;/h4&gt;探索能否通过统一原则同时解决这三个问题，以简化机器学习流程并促进任务间的协同作用。&lt;h4&gt;方法&lt;/h4&gt;引入潜在区域网络(LZN)，创建共享的高斯潜在空间编码所有任务信息，为每种数据类型配备编码器和解码器，将机器学习任务表示为这些组件的组合。&lt;h4&gt;主要发现&lt;/h4&gt;LZN在三个场景中展示了潜力：(1)增强现有模型：与Rectified Flow结合，在CIFAR10上FID从2.76提高到2.59；(2)独立解决任务：无需辅助损失函数实现无监督表示学习，在ImageNet上分别比MoCo和SimCLR高9.3%和0.2%；(3)同时解决多个任务：在CIFAR10上同时改进FID并实现最先进的分类准确度。&lt;h4&gt;结论&lt;/h4&gt;LZN为统一解决机器学习的三个核心问题提供了有前景的途径，代码和训练模型已公开。&lt;h4&gt;翻译&lt;/h4&gt;生成建模、表示学习和是机器学习的三个核心问题，然而它们的最先进解决方案仍然 largely disjoint。在本文中，我们提出：能否通过统一原则解决这三个问题？这种统一可以简化机器学习流程并促进任务间的更大协同。我们引入潜在区域网络(LZN)作为实现这一目标的一步。LZN的核心是创建一个共享的高斯潜在空间，编码所有任务的信息。每种数据类型(如图像、文本、标签)都配备一个将样本映射到不重叠潜在区域的编码器，以及一个将潜在空间映射回数据的解码器。机器学习任务表示为这些编码器和解码器的组合：例如，标签条件图像生成使用标签编码器和图像解码器；图像嵌入使用图像编码器；分类使用图像编码器和标签解码器。我们在三个复杂度递增的场景中展示了LZN的潜力：(1) LZN可以增强现有模型(图像生成)：与最先进的Rectified Flow模型结合，LZN将CIFAR10上的FID从2.76提高到2.59，无需修改训练目标。(2) LZN可以独立解决任务(表示学习)：LZN无需辅助损失函数即可实现无监督表示学习，在ImageNet上的下游线性分类分别比开创性的MoCo和SimCLR方法高9.3%和0.2%。(3) LZN可以同时解决多个任务(联合生成和分类)：通过图像和标签编码器/解码器，LZN设计上同时执行这两个任务，在CIFAR10上改进FID并实现最先进的分类准确度。代码和训练模型可在https://github.com/microsoft/latent-zoning-networks获取。项目网站在https://zinanlin.me/blogs/latent_zoning_networks.html。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling, representation learning, and classification are threecore problems in machine learning (ML), yet their state-of-the-art (SoTA)solutions remain largely disjoint. In this paper, we ask: Can a unifiedprinciple address all three? Such unification could simplify ML pipelines andfoster greater synergy across tasks. We introduce Latent Zoning Network (LZN)as a step toward this goal. At its core, LZN creates a shared Gaussian latentspace that encodes information across all tasks. Each data type (e.g., images,text, labels) is equipped with an encoder that maps samples to disjoint latentzones, and a decoder that maps latents back to data. ML tasks are expressed ascompositions of these encoders and decoders: for example, label-conditionalimage generation uses a label encoder and image decoder; image embedding usesan image encoder; classification uses an image encoder and label decoder. Wedemonstrate the promise of LZN in three increasingly complex scenarios: (1) LZNcan enhance existing models (image generation): When combined with the SoTARectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-withoutmodifying the training objective. (2) LZN can solve tasks independently(representation learning): LZN can implement unsupervised representationlearning without auxiliary loss functions, outperforming the seminal MoCo andSimCLR methods by 9.3% and 0.2%, respectively, on downstream linearclassification on ImageNet. (3) LZN can solve multiple tasks simultaneously(joint generation and classification): With image and label encoders/decoders,LZN performs both tasks jointly by design, improving FID and achieving SoTAclassification accuracy on CIFAR10. The code and trained models are availableat https://github.com/microsoft/latent-zoning-networks. The project website isat https://zinanlin.me/blogs/latent_zoning_networks.html.</description>
      <author>example@mail.com (Zinan Lin, Enshu Liu, Xuefei Ning, Junyi Zhu, Wenyu Wang, Sergey Yekhanin)</author>
      <guid isPermaLink="false">2509.15591v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition</title>
      <link>http://arxiv.org/abs/2509.15430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages including reference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BiRQ是一种双层自监督学习框架，结合了BEST-RQ的效率和HuBERT风格标签增强的优势，通过重用模型本身作为伪标签生成器，实现了端到端的迭代标签改进，在多个数据集上展现出优于BEST-RQ的性能。&lt;h4&gt;背景&lt;/h4&gt;语音是丰富信号但标记数据成本高昂，自监督学习对可扩展表征学习至关重要。语音自监督学习的核心挑战是生成既信息丰富又高效的伪标签。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合高效方法与标签增强优势的自监督学习框架，解决现有方法中强标签依赖外部编码器而高效方法标签较弱的问题。&lt;h4&gt;方法&lt;/h4&gt;BiRQ框架使用随机投影量化器对中间表示进行离散化产生增强标签，同时锚定直接从原始输入派生的标签稳定训练。训练表述为一阶双层优化问题，使用可区分的Gumbel-softmax选择端到端解决，消除了对外部标签编码器的需求。&lt;h4&gt;主要发现&lt;/h4&gt;BiRQ在保持低复杂度和计算效率的同时，持续优于BEST-RQ。在960小时的LibriSpeech、150小时的AMI会议和5,000小时的YODAS等多个数据集上验证了该方法的有效性，展现出一致的改进。&lt;h4&gt;结论&lt;/h4&gt;BiRQ成功结合了高效方法和标签增强方法的优势，通过重用模型本身作为伪标签生成器，实现了端到端的迭代标签改进，为语音表征学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;语音是一种丰富的信号，标记的音频-文本对成本高昂，这使得自监督学习对于可扩展的表征学习至关重要。语音自监督学习的一个核心挑战是生成既信息丰富又高效的伪标签：强标签（如HuBERT中使用）能提高下游性能，但依赖外部编码器和多阶段流程；而高效方法如BEST-RQ以简单的实现为代价，产生了较弱的标签。我们提出BiRQ，一种双层自监督学习框架，结合了BEST-RQ的效率和HuBERT风格标签增强的改进优势。核心思想是重用模型本身作为伪标签生成器：中间表示通过随机投影量化器离散化以产生增强标签，而直接从原始输入派生的锚定标签稳定训练并防止崩溃。训练被表述为高效的一阶双层优化问题，使用可区分的Gumbel-softmax选择端到端解决。这种设计消除了对外部标签编码器的需求，降低了内存成本，并实现了端到端的迭代标签改进。BiRQ在保持低复杂度和计算效率的同时，持续优于BEST-RQ。我们在各种数据集上验证了我们的方法，包括960小时的LibriSpeech、150小时的AMI会议和5,000小时的YODAS，展示了相对于BEST-RQ的一致性提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech is a rich signal, and labeled audio-text pairs are costly, makingself-supervised learning essential for scalable representation learning. A corechallenge in speech SSL is generating pseudo-labels that are both informativeand efficient: strong labels, such as those used in HuBERT, improve downstreamperformance but rely on external encoders and multi-stage pipelines, whileefficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.We propose BiRQ, a bilevel SSL framework that combines the efficiency ofBEST-RQ with the refinement benefits of HuBERT-style label enhancement. The keyidea is to reuse part of the model itself as a pseudo-label generator:intermediate representations are discretized by a random-projection quantizerto produce enhanced labels, while anchoring labels derived directly from theraw input stabilize training and prevent collapse. Training is formulated as anefficient first-order bilevel optimization problem, solved end-to-end withdifferentiable Gumbel-softmax selection. This design eliminates the need forexternal label encoders, reduces memory cost, and enables iterative labelrefinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQwhile maintaining low complexity and computational efficiency. We validate ourmethod on various datasets, including 960-hour LibriSpeech, 150-hour AMImeetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.</description>
      <author>example@mail.com (Liuyuan Jiang, Xiaodong Cui, Brian Kingsbury, Tianyi Chen, Lisha Chen)</author>
      <guid isPermaLink="false">2509.15430v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception</title>
      <link>http://arxiv.org/abs/2509.15333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了AdaptiveNN框架，一种从被动到主动、自适应的视觉模型范式转变，通过模拟人类视觉注意力机制实现视觉感知的粗到细顺序决策过程，显著降低推理成本同时保持准确性，并提供更好的可解释性。&lt;h4&gt;背景&lt;/h4&gt;人类视觉具有高度适应性，能通过顺序注视任务相关区域高效采样复杂环境；而现有机器视觉模型被动一次性处理整个场景，导致资源需求随输入分辨率和模型规模过度增加，限制了未来进展和实际应用。&lt;h4&gt;目的&lt;/h4&gt;引入AdaptiveNN框架，推动视觉模型从'被动'到'主动、自适应'的范式转变。&lt;h4&gt;方法&lt;/h4&gt;AdaptiveNN将视觉感知制定为从粗到细的顺序决策过程，逐步识别和关注任务相关区域，跨注视点增量结合信息，并在获得足够信息时主动结束观察；通过结合表示学习与自我奖励强化学习理论，实现非可微模型的端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;AdaptiveNN在17个涵盖9项任务的基准测试上表现优异，包括大规模视觉识别、细粒度辨别、视觉搜索等；在不牺牲准确性的情况下实现高达28倍的推理成本降低，能灵活适应不同任务需求和资源预算无需重新训练，并通过注视模式提供增强的可解释性。&lt;h4&gt;结论&lt;/h4&gt;AdaptiveNN为高效、灵活和可解释的计算机视觉提供了有前途的途径，在许多情况下表现出与人类相似的行为，揭示了其作为研究视觉认知有价值工具的潜力。&lt;h4&gt;翻译&lt;/h4&gt;人类视觉具有高度适应性，通过顺序注视任务相关区域来高效采样复杂环境。相比之下，主流的机器视觉模型被动地一次性处理整个场景，导致资源需求随空间时间输入分辨率和模型规模而过度增加，产生了阻碍未来进展和实际应用的关键限制。在此，我们介绍AdaptiveNN，一个旨在推动视觉模型从'被动'到'主动、自适应'转变的通用框架。AdaptiveNN将视觉感知制定为从粗到细的顺序决策过程，逐步识别和关注与任务相关的区域，跨注视点增量地结合信息，并在获得足够信息时主动结束观察。我们建立了一个将表示学习与自我奖励强化学习相结合的理论，使非可微的AdaptiveNN无需额外的注视位置监督即可进行端到端训练。我们在17个涵盖9项任务的基准测试上评估了AdaptiveNN，包括大规模视觉识别、细粒度辨别、视觉搜索、处理真实驾驶和医疗场景的图像、语言驱动的具身AI以及与人类的并排比较。AdaptiveNN在不牺牲准确性的情况下实现了高达28倍的推理成本降低，能够灵活适应不同的任务需求和资源预算而无需重新训练，并通过其注视模式提供了增强的可解释性，展示了高效、灵活和可解释计算机视觉的有前途途径。此外，AdaptiveNN在许多情况下表现出与人类相似的行为，揭示了其作为研究视觉认知有价值工具的潜力。代码可在https://github.com/LeapLabTHU/AdaptiveNN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human vision is highly adaptive, efficiently sampling intricate environmentsby sequentially fixating on task-relevant regions. In contrast, prevailingmachine vision models passively process entire scenes at once, resulting inexcessive resource demands scaling with spatial-temporal input resolution andmodel size, yielding critical limitations impeding both future advancements andreal-world application. Here we introduce AdaptiveNN, a general frameworkaiming to drive a paradigm shift from 'passive' to 'active, adaptive' visionmodels. AdaptiveNN formulates visual perception as a coarse-to-fine sequentialdecision-making process, progressively identifying and attending to regionspertinent to the task, incrementally combining information across fixations,and actively concluding observation when sufficient. We establish a theoryintegrating representation learning with self-rewarding reinforcement learning,enabling end-to-end training of the non-differentiable AdaptiveNN withoutadditional supervision on fixation locations. We assess AdaptiveNN on 17benchmarks spanning 9 tasks, including large-scale visual recognition,fine-grained discrimination, visual search, processing images from real drivingand medical scenarios, language-driven embodied AI, and side-by-sidecomparisons with humans. AdaptiveNN achieves up to 28x inference cost reductionwithout sacrificing accuracy, flexibly adapts to varying task demands andresource budgets without retraining, and provides enhanced interpretability viaits fixation patterns, demonstrating a promising avenue toward efficient,flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibitsclosely human-like perceptual behaviors in many cases, revealing its potentialas a valuable tool for investigating visual cognition. Code is available athttps://github.com/LeapLabTHU/AdaptiveNN.</description>
      <author>example@mail.com (Yulin Wang, Yang Yue, Yang Yue, Huanqian Wang, Haojun Jiang, Yizeng Han, Zanlin Ni, Yifan Pu, Minglei Shi, Rui Lu, Qisen Yang, Andrew Zhao, Zhuofan Xia, Shiji Song, Gao Huang)</author>
      <guid isPermaLink="false">2509.15333v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2509.14868v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DPANet是一种新颖的双金字塔注意力网络，通过解耦并建模时间多尺度动态和频谱多分辨率周期性，显著提升了长期时间序列预测性能。&lt;h4&gt;背景&lt;/h4&gt;长期时间序列预测面临建模跨越多时间尺度和频率分辨率的复杂依赖关系的挑战，现有Transformer和MLP方法难以统一结构化地捕捉这些相互交织的特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新架构，能够明确解耦并同时建模时间多尺度动态和频谱多分辨率周期性，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;构建双金字塔注意力网络(DPANet)，包含基于渐进下采样的时间金字塔和基于带通滤波的频率金字塔，通过交叉金字塔融合块实现对应级别间深度交互式信息交换，以从粗到细的层次结构进行融合。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准上的广泛实验表明，DPANet达到最先进性能，显著优于先前模型。&lt;h4&gt;结论&lt;/h4&gt;DPANet通过双金字塔结构和交叉金字塔融合块有效解决了长期时间序列预测中的复杂依赖关系建模问题，为时间序列预测提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;长期时间序列预测(LTSF)因难以建模跨越多个时间尺度和频率分辨率的复杂依赖关系而受到阻碍。现有方法，包括Transformer和基于MLP的模型，往往无法以统一和结构化的方式捕捉这些相互交织的特性。我们提出了双金字塔注意力网络(DPANet)，一种新颖的架构，明确解耦并同时建模时间多尺度动态和频谱多分辨率周期性。DPANet构建两个并行金字塔：基于渐进下采样的时间金字塔和基于带通滤波的频率金字塔。我们模型的核心是交叉金字塔融合块，它通过交叉注意力机制实现对应金字塔级别之间深度、交互式的信息交换。这种融合以从粗到细的层次结构进行，使全局上下文能够指导局部表示学习。在公共基准上的广泛实验表明，DPANet达到最先进性能，显著优于先前模型。代码可在https://github.com/hit636/DPANet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term time series forecasting (LTSF) is hampered by the challenge ofmodeling complex dependencies that span multiple temporal scales and frequencyresolutions. Existing methods, including Transformer and MLP-based models,often struggle to capture these intertwined characteristics in a unified andstructured manner. We propose the Dual Pyramid Attention Network (DPANet), anovel architecture that explicitly decouples and concurrently models temporalmulti-scale dynamics and spectral multi-resolution periodicities. DPANetconstructs two parallel pyramids: a Temporal Pyramid built on progressivedownsampling, and a Frequency Pyramid built on band-pass filtering. The core ofour model is the Cross-Pyramid Fusion Block, which facilitates deep,interactive information exchange between corresponding pyramid levels viacross-attention. This fusion proceeds in a coarse-to-fine hierarchy, enablingglobal context to guide local representation learning. Extensive experiments onpublic benchmarks show that DPANet achieves state-of-the-art performance,significantly outperforming prior models. Code is available athttps://github.com/hit636/DPANet.</description>
      <author>example@mail.com (Qianyang Li, Xingjun Zhang, Shaoxun Wang, Jia Wei)</author>
      <guid isPermaLink="false">2509.14868v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders</title>
      <link>http://arxiv.org/abs/2509.15259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于信息熵的特征选择方法IEFS-GMB，用于提高基于深度学习的脑电图分类性能，在四个公共神经系统疾病数据集上实现了0.64%至6.45%的准确率提升。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的脑电图分类对神经系统疾病的自动检测至关重要，但脑电图信号的低信噪比限制了模型性能，使得特征选择成为优化神经网络编码器学习表示的关键。&lt;h4&gt;目的&lt;/h4&gt;解决现有特征选择方法在脑电图分类中的局限性，包括缺乏专门设计、依赖特定架构、缺乏可解释性以及对数据变化鲁棒性不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出IEFS-GMB方法，构建动态记忆库存储历史梯度，通过信息熵计算特征重要性，并应用基于熵的加权来选择信息丰富的脑电图特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共神经系统疾病数据集上实验表明，使用IEFS-GMB增强的编码器相比基线模型准确率提高了0.64%到6.45%，优于四种竞争性特征选择技术，同时提高了模型的可解释性。&lt;h4&gt;结论&lt;/h4&gt;IEFS-GMB方法在脑电图分类中表现优异，支持在临床环境中的实际应用。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的脑电图分类对于神经系统疾病的自动检测至关重要，可以提高诊断准确性并实现早期干预。然而，脑电图信号的低信噪比限制了模型性能，使得特征选择对于优化神经网络编码器学习到的表示至关重要。现有的特征选择方法很少专门为脑电图诊断设计；许多方法依赖于特定架构且缺乏可解释性，限制了它们的适用性。此外，大多数方法依赖于单次迭代数据，导致对变化的鲁棒性有限。为解决这些问题，我们提出了IEFS-GMB，一种由梯度记忆库引导的基于信息熵的特征选择方法。该方法构建存储历史梯度的动态记忆库，通过信息熵计算特征重要性，并应用基于熵的加权来选择信息丰富的脑电图特征。在四个公共神经系统疾病数据集上的实验表明，使用IEFS-GMB增强的编码器相比基线模型实现了0.64%至6.45%的准确率提升。该方法还优于四种竞争性特征选择技术，并提高了模型的可解释性，支持其在临床环境中的实际应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based EEG classification is crucial for the automated detectionof neurological disorders, improving diagnostic accuracy and enabling earlyintervention. However, the low signal-to-noise ratio of EEG signals limitsmodel performance, making feature selection (FS) vital for optimizingrepresentations learned by neural network encoders. Existing FS methods areseldom designed specifically for EEG diagnosis; many are architecture-dependentand lack interpretability, limiting their applicability. Moreover, most rely onsingle-iteration data, resulting in limited robustness to variability. Toaddress these issues, we propose IEFS-GMB, an Information Entropy-based FeatureSelection method guided by a Gradient Memory Bank. This approach constructs adynamic memory bank storing historical gradients, computes feature importancevia information entropy, and applies entropy-based weighting to selectinformative EEG features. Experiments on four public neurological diseasedatasets show that encoders enhanced with IEFS-GMB achieve accuracyimprovements of 0.64% to 6.45% over baseline models. The method alsooutperforms four competing FS techniques and improves model interpretability,supporting its practical use in clinical settings.</description>
      <author>example@mail.com (Liang Zhang, Hanyang Dong, Jia-Hong Gao, Yi Sun, Kuntao Xiao, Wanli Yang, Zhao Lv, Shurong Sheng)</author>
      <guid isPermaLink="false">2509.15259v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays</title>
      <link>http://arxiv.org/abs/2509.15234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 2 figures, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于大型语言模型的放射学报告表示学习方法，通过领域适配的LLM编码器和双塔框架，解决了临床报告异质性和噪声问题，提高了模型在跨数据集泛化和临床对齐方面的表现。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言预训练在图像-文本对齐方面取得了进展，但在放射学领域，由于临床报告的异质性（包括缩写、仅印象笔记和风格变异性）而进展受限。与通用领域不同，简单地扩展到大量嘈杂报告的数据集可能导致模型学习停滞甚至退化。&lt;h4&gt;目的&lt;/h4&gt;研究大型语言模型(LLM)编码器是否能提供稳健的临床表示，这些表示能跨不同风格转移并更好地指导图像-文本对齐。&lt;h4&gt;方法&lt;/h4&gt;引入了LLM2VEC4CXR（针对胸部X光报告进行领域适配的LLM编码器）和LLM2CLIP4CXR（将此编码器与视觉骨干网络耦合的双塔框架）。在来自公共和私人来源的160万份胸部X光研究上训练模型，这些报告具有异质性和噪声。&lt;h4&gt;主要发现&lt;/h4&gt;LLM2VEC4CXR在临床文本理解方面优于基于BERT的基线，能够处理缩写和风格变化，并在报告级别指标上实现了强大的临床对齐。LLM2CLIP4CXR利用这些嵌入提高了检索准确度和临床导向评分，比之前的医疗CLIP变体具有更强的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;稳健性（而非仅是规模）是有效多模态学习的关键。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言预训练已推进图像-文本对齐，但在放射学领域的进展仍受临床报告异质性的限制，包括缩写、仅印象笔记和风格变异性。与通用领域设置不同（更多数据通常带来更好性能），简单地扩展到大量嘈杂报告的数据集可能导致模型学习停滞甚至退化。我们探究大型语言模型(LLM)编码器是否能提供稳健的临床表示，这些表示能跨不同风格转移并更好地指导图像-文本对齐。我们介绍了LLM2VEC4CXR，一个针对胸部X光报告进行领域适配的LLM编码器，以及LLM2CLIP4CXR，一个将此编码器与视觉骨干网络耦合的双塔框架。LLM2VEC4CXR在临床文本理解方面优于基于BERT的基线，能够处理缩写和风格变化，并在报告级别指标上实现了强大的临床对齐。LLM2CLIP4CXR利用这些嵌入提高了检索准确度和临床导向评分，比之前的医疗CLIP变体具有更强的跨数据集泛化能力。在来自公共和私人来源的160万份具有异质性和噪声的胸部X光研究上训练后，我们的模型证明了稳健性（而非仅是规模）是有效多模态学习的关键。我们发布这些模型以支持医学图像-文本表示学习的进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language pretraining has advanced image-text alignment, yet progressin radiology remains constrained by the heterogeneity of clinical reports,including abbreviations, impression-only notes, and stylistic variability.Unlike general-domain settings where more data often leads to betterperformance, naively scaling to large collections of noisy reports can plateauor even degrade model learning. We ask whether large language model (LLM)encoders can provide robust clinical representations that transfer acrossdiverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR,a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, adual-tower framework that couples this encoder with a vision backbone.LLM2VEC4CXR improves clinical text understanding over BERT-based baselines,handles abbreviations and style variation, and achieves strong clinicalalignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings toboost retrieval accuracy and clinically oriented scores, with strongercross-dataset generalization than prior medical CLIP variants. Trained on 1.6MCXR studies from public and private sources with heterogeneous and noisyreports, our models demonstrate that robustness -- not scale alone -- is thekey to effective multimodal learning. We release models to support furtherresearch in medical image-text representation learning.</description>
      <author>example@mail.com (Hanbin Ko, Gihun Cho, Inhyeok Baek, Donguk Kim, Joonbeom Koo, Changi Kim, Dongheon Lee, Chang Min Park)</author>
      <guid isPermaLink="false">2509.15234v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Compose by Focus: Scene Graph-based Atomic Skills</title>
      <link>http://arxiv.org/abs/2509.16053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于场景图的技能学习框架，结合图神经网络和基于扩散的模仿学习，并与视觉语言模型任务规划器结合，显著提高了机器人在长时程任务中的组合泛化能力和稳健性。&lt;h4&gt;背景&lt;/h4&gt;通用机器人的关键挑战是实现组合泛化能力，即将原子技能组合解决复杂长时程任务。先前研究主要关注规划器合成，但单个技能在场景组合引起的分布转移下执行效果不佳。&lt;h4&gt;目的&lt;/h4&gt;解决视觉运动策略在场景组合引起的分布转移下失效的问题，提高单个技能的稳健执行能力，增强机器人在长时程任务中的组合泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入基于场景图的表示，专注于任务相关对象和关系；开发结合图神经网络和基于扩散的模仿学习的技能学习框架；将'聚焦的'场景图技能与视觉语言模型任务规划器相结合。&lt;h4&gt;主要发现&lt;/h4&gt;在仿真和真实世界操作任务中，所提方法比现有最先进基线方法取得显著更高的成功率，有效提升了长时程任务中的稳健性和组合泛化能力。&lt;h4&gt;结论&lt;/h4&gt;基于场景图的表示和技能学习框架能有效解决视觉运动策略在分布转移下的失效问题，显著提高机器人在复杂长时程任务中的执行能力和泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;通用机器人的一个关键要求是组合泛化能力——将原子技能组合起来解决复杂、长时程任务的能力。虽然先前的工作主要集中在合成规划器来排序预学习的技能，但单个技能本身的稳健执行仍然具有挑战性，因为视觉运动策略在场景组合引起的分布转移下往往会失败。为了解决这个问题，我们引入了一种基于场景图的表示，专注于任务相关的对象和关系，从而降低对无关变化的敏感性。基于这一思想，我们开发了一个基于场景图的技能学习框架，将图神经网络与基于扩散的模仿学习相结合，并将'聚焦的'场景图技能与基于视觉语言模型(VLM)的任务规划器相结合。在仿真和真实世界操作任务中的实验表明，成功率比最先进的基线方法显著提高，突显了在长时程任务中改进的稳健性和组合泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人如何有效组合基本技能来完成复杂长时程任务的问题。这个问题很重要，因为现实世界中的机器人任务通常需要将大任务分解成多个小任务执行，而现有方法在复杂场景中组合技能时表现不佳，容易受到环境变化的影响。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为要让技能能有效组合，每个技能必须是'专注的'，只关注与当前任务相关的对象和关系，忽略无关干扰。基于这一想法，他们设计了基于场景图的表示方法。该方法借鉴了视觉基础模型（如Grounded-SAM）进行对象分割，视觉语言模型（如ChatGPT）进行关系推理，图神经网络处理图结构，以及扩散模型进行动作学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用场景图表示视觉数据，只包含任务相关的对象和关系，使模型能'专注'于重要元素。流程包括：1)构建场景图（分割对象、提取点云、推断关系）；2)训练多技能策略（使用图神经网络处理场景图，结合扩散模型学习动作）；3)测试时技能组合（使用视觉语言模型规划任务，动态构建子场景图并执行相应技能）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用场景图作为视觉输入表示；2)设计'专注'的技能学习方法；3)结合图神经网络和扩散模型；4)与视觉语言模型集成实现端到端流程。相比之前工作，不同之处在于：不使用原始图像而用结构化场景图；不仅关注高层规划还关注底层技能构建；明确编码对象间关系而非仅处理对象特征；将场景图直接作为低层策略输入而非仅用于高层推理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于场景图的表示方法，使机器人能够'专注'于任务相关元素，从而实现更鲁棒和可组合的技能学习，显著提高了复杂长时程任务的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A key requirement for generalist robots is compositional generalization - theability to combine atomic skills to solve complex, long-horizon tasks. Whileprior work has primarily focused on synthesizing a planner that sequencespre-learned skills, robust execution of the individual skills themselvesremains challenging, as visuomotor policies often fail under distributionshifts induced by scene composition. To address this, we introduce a scenegraph-based representation that focuses on task-relevant objects and relations,thereby mitigating sensitivity to irrelevant variation. Building on this idea,we develop a scene-graph skill learning framework that integrates graph neuralnetworks with diffusion-based imitation learning, and further combine "focused"scene-graph skills with a vision-language model (VLM) based task planner.Experiments in both simulation and real-world manipulation tasks demonstratesubstantially higher success rates than state-of-the-art baselines,highlighting improved robustness and compositional generalization inlong-horizon tasks.</description>
      <author>example@mail.com (Han Qi, Changhe Chen, Heng Yang)</author>
      <guid isPermaLink="false">2509.16053v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>LC-SLab -- An Object-based Deep Learning Framework for Large-scale Land Cover Classification from Satellite Imagery and Sparse In-situ Labels</title>
      <link>http://arxiv.org/abs/2509.15868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LC-SLab框架，这是首个用于在稀疏监督下系统探索基于对象深度学习方法进行大规模土地分类的深度学习框架。该框架能够生成更连贯的土地覆盖地图，同时保持或提高准确性。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习生成的大规模土地覆盖地图在地球科学应用中起关键作用。来自原则性土地调查的开放实地数据集为手动标注提供了可扩展替代方案，但其稀疏空间覆盖导致现有方法产生碎片化和噪声预测。基于对象的分类方法为语义连贯的图像区域分配标签，但在基于深度学习的土地覆盖映射中尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决稀疏实地数据集导致的土地覆盖预测碎片化问题，探索基于对象的深度学习方法在中等分辨率图像和稀疏监督背景下的应用，并开发系统性框架。&lt;h4&gt;方法&lt;/h4&gt;提出LC-SLab框架，支持两种聚合方式：1)输入级聚合通过图神经网络实现；2)输出级聚合通过后处理语义分割模型结果实现。同时融入大型预训练网络特征以改善小数据集性能。在年度Sentinel-2合成图像和稀疏LUCAS标签上评估框架，关注准确性与碎片化权衡及数据集大小敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;基于对象的方法可匹配或超越逐像素模型准确性，同时产生更连贯地图；输入级聚合在小数据集上更鲁棒；输出级聚合在更多数据上表现最佳；几种LC-SLab配置优于现有土地覆盖产品。&lt;h4&gt;结论&lt;/h4&gt;LC-SLab框架成功将基于对象的深度学习方法应用于大规模土地分类，特别是在稀疏监督条件下，为地球科学应用提供了生成连贯土地覆盖地图的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;使用深度生成的大规模土地覆盖地图在广泛的地球科学应用中起着关键作用。来自原则性土地调查的开放实地数据集为手动标注提供了可扩展的替代方案，用于训练此类模型。然而，它们稀疏的空间覆盖通常导致在使用现有基于深度学习的土地覆盖映射方法时产生碎片化和噪声预测。解决这一问题的一个有前景的方向是基于对象的分类，它为语义连贯的图像区域而非单个像素分配标签，从而强制设定最小制图单元。尽管有这种潜力，但基于对象的方法在基于深度学习的土地覆盖映射管道中仍未得到充分探索，特别是在中等分辨率图像和稀疏监督的背景下。为了解决这一差距，我们提出了LC-SLab，这是首个用于在稀疏监督下系统探索基于对象的深度学习方法进行大规模土地分类的深度学习框架。LC-SLab支持通过图神经网络进行输入级聚合，以及通过后处理已建立的语义分割模型的结果进行输出级聚合。此外，我们还融入了大型预训练网络的特征，以改善小数据集上的性能。我们在年度Sentinel-2合成图像和稀疏LUCAS标签上评估了该框架，重点关注准确性与碎片化之间的权衡以及对数据集大小的敏感性。我们的结果表明，基于对象的方法可以匹配或超越逐像素模型的准确性，同时产生更连贯的地图。输入级聚合在较小数据集上更鲁棒，而输出级聚合在更多数据上表现最佳。几种LC-SLab配置也优于现有的土地覆盖产品，突显了该框架的实际效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale land cover maps generated using deep learning play a criticalrole across a wide range of Earth science applications. Open in-situ datasetsfrom principled land cover surveys offer a scalable alternative to manualannotation for training such models. However, their sparse spatial coverageoften leads to fragmented and noisy predictions when used with existing deeplearning-based land cover mapping approaches. A promising direction to addressthis issue is object-based classification, which assigns labels to semanticallycoherent image regions rather than individual pixels, thereby imposing aminimum mapping unit. Despite this potential, object-based methods remainunderexplored in deep learning-based land cover mapping pipelines, especiallyin the context of medium-resolution imagery and sparse supervision. To addressthis gap, we propose LC-SLab, the first deep learning framework forsystematically exploring object-based deep learning methods for large-scaleland cover classification under sparse supervision. LC-SLab supports bothinput-level aggregation via graph neural networks, and output-level aggregationby postprocessing results from established semantic segmentation models.Additionally, we incorporate features from a large pre-trained network toimprove performance on small datasets. We evaluate the framework on annualSentinel-2 composites with sparse LUCAS labels, focusing on the tradeoffbetween accuracy and fragmentation, as well as sensitivity to dataset size. Ourresults show that object-based methods can match or exceed the accuracy ofcommon pixel-wise models while producing substantially more coherent maps.Input-level aggregation proves more robust on smaller datasets, whereasoutput-level aggregation performs best with more data. Several configurationsof LC-SLab also outperform existing land cover products, highlighting theframework's practical utility.</description>
      <author>example@mail.com (Johannes Leonhardt, Juergen Gall, Ribana Roscher)</author>
      <guid isPermaLink="false">2509.15868v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network</title>
      <link>http://arxiv.org/abs/2509.15857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025 (spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EvoBrain的新型癫痫检测模型，通过整合双流Mamba架构和拉普拉斯位置编码增强的GCN，解决了动态图神经网络在脑电图数据中捕捉癫痫状态动态特性的两个主要挑战。&lt;h4&gt;背景&lt;/h4&gt;动态图神经网络整合脑电图数据中的时间和空间特征，在癫痫检测方面有巨大潜力，但完全捕捉代表癫痫和非癫痫状态的脑动态仍面临两个挑战：现有方法多基于静态图，无法反映癫痫过程中大脑连接的动态特性；联合建模时间信号和图结构及其交互作用的尝试仍处于初级阶段。&lt;h4&gt;目的&lt;/h4&gt;解决现有动态GNN方法的两个基本挑战：一是无法反映癫痫进展过程中大脑连接的动态演化，二是联合建模时间信号和图结构及其交互作用的能力不足导致的性能不一致问题。&lt;h4&gt;方法&lt;/h4&gt;提出EvoBrain模型，整合双流Mamba架构和拉普拉斯位置编码增强的GCN，遵循神经学见解，并引入显式动态图结构使节点和边随时间演化。基于对两个问题的理论分析，证明了显式动态建模和时间-图动态GNN方法的有效性和必要性。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析证明显式动态建模和时间-图动态GNN方法比其他方法具有表达优势；实验表明EvoBrain相比动态GNN基线显著提高了性能，AUROC提高23%，F1分数提高30%。&lt;h4&gt;结论&lt;/h4&gt;EvoBrain模型通过显式动态建模和时间-图动态GNN方法有效解决了癫痫检测中的关键挑战，在早期癫痫预测任务上表现出色，为癫痫检测提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;动态图神经网络整合了脑电图数据中的时间和空间特征，在自动化癫痫检测方面显示出巨大潜力。然而，完全捕捉代表脑状态（如癫痫和非癫痫）所需的基本动态仍然是一项困难的任务，并面临两个基本挑战。首先，大多数现有的动态图神经网络方法建立在时间固定的静态图上，无法反映癫痫进展过程中大脑连接的动态特性。其次，目前联合建模时间信号和图结构及其交互作用的尝试仍处于初级阶段，通常导致性能不一致。为解决这些挑战，我们首次对这两个问题进行了理论分析，证明了显式动态建模和时间-图动态图神经网络方法的有效性和必要性。基于这些见解，我们提出了EvoBrain，一种整合双流Mamba架构和拉普拉斯位置编码增强的GCN的新型癫痫检测模型，遵循神经学见解。此外，EvoBrain集成了显式动态图结构，允许节点和边随时间演化。我们的贡献包括：(a) 证明显式动态建模和时间-图动态图神经网络方法比其他方法具有表达优势的理论分析，(b) 相比动态图神经网络基线显著提高AUROC 23%和F1分数30%的新型高效模型，以及(c) 在具有挑战性的早期癫痫预测任务上对我们方法的广泛评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic GNNs, which integrate temporal and spatial features inElectroencephalography (EEG) data, have shown great potential in automatingseizure detection. However, fully capturing the underlying dynamics necessaryto represent brain states, such as seizure and non-seizure, remains anon-trivial task and presents two fundamental challenges. First, most existingdynamic GNN methods are built on temporally fixed static graphs, which fail toreflect the evolving nature of brain connectivity during seizure progression.Second, current efforts to jointly model temporal signals and graph structuresand, more importantly, their interactions remain nascent, often resulting ininconsistent performance. To address these challenges, we present the firsttheoretical analysis of these two problems, demonstrating the effectiveness andnecessity of explicit dynamic modeling and time-then-graph dynamic GNN method.Building on these insights, we propose EvoBrain, a novel seizure detectionmodel that integrates a two-stream Mamba architecture with a GCN enhanced byLaplacian Positional Encoding, following neurological insights. Moreover,EvoBrain incorporates explicitly dynamic graph structures, allowing both nodesand edges to evolve over time. Our contributions include (a) a theoreticalanalysis proving the expressivity advantage of explicit dynamic modeling andtime-then-graph over other approaches, (b) a novel and efficient model thatsignificantly improves AUROC by 23% and F1 score by 30%, compared with thedynamic GNN baseline, and (c) broad evaluations of our method on thechallenging early seizure prediction tasks.</description>
      <author>example@mail.com (Rikuto Kotoge, Zheng Chen, Tasuku Kimura, Yasuko Matsubara, Takufumi Yanagisawa, Haruhiko Kishima, Yasushi Sakurai)</author>
      <guid isPermaLink="false">2509.15857v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors</title>
      <link>http://arxiv.org/abs/2509.15827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 17 figures, submitted to IEEE Transactions on Sustainable  Energy&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SolarCrossFormer是一种新型深度学习模型，用于结合卫星图像和地面气象站网络时间序列数据进行日辐照度预测，提供15分钟分辨率和24小时预测范围，归一化平均绝对误差为6.1%，结果与商业数值天气预报服务相当。&lt;h4&gt;背景&lt;/h4&gt;大规模太阳能光伏系统并网需要准确的日辐照度预测，但当前预测解决方案缺乏系统运营商所需的时间和空间分辨率。&lt;h4&gt;目的&lt;/h4&gt;开发SolarCrossFormer模型，结合卫星图像和地面气象站数据，提高日辐照度预测的准确性和分辨率。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络利用输入数据的模内和模间相关性，生成瑞士任何位置的预测性预报，分辨率为15分钟，预测范围可达未来24小时；无需重新训练即可整合新数据，仅使用坐标即可为无数据位置生成预测。&lt;h4&gt;主要发现&lt;/h4&gt;在瑞士一年期、127个位置的数据集上测试，SolarCrossFormer在整个预测范围内的归一化平均绝对误差为6.1%，结果与商业数值天气预报服务具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;SolarCrossFormer是一种有效的日辐照度预测模型，结合多源数据提供高分辨率、高准确性的预测，在实际应用中具有稳健性。&lt;h4&gt;翻译&lt;/h4&gt;太阳能光伏系统的大规模并网需要准确的日辐照度提前预测。然而，当前的预测解决方案缺乏系统运营商所需的时间和空间分辨率。在本文中，我们引入了SolarCrossFormer，这是一种用于日辐照度预测的新型深度学习模型，它结合了卫星图像和地面气象站网络的时间序列数据。SolarCrossFormer使用新颖的图神经网络来利用输入数据的模内和模间相关性，提高预测的准确性和分辨率。它以15分钟的时间分辨率生成瑞士任何位置的预测性预报，预测范围可达未来24小时。SolarCrossFormer的主要优势之一是在实际操作中的稳健性。它可以整合新的时间序列数据而无需重新训练模型，此外，它可以通过仅使用坐标来为没有输入数据的位置生成预测。在瑞士一年期、127个位置的数据集上的实验结果表明，SolarCrossFormer在整个预测范围内的归一化平均绝对误差为6.1%。结果与商业数值天气预报服务所取得的结果具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate day-ahead forecasts of solar irradiance are required for thelarge-scale integration of solar photovoltaic (PV) systems into the power grid.However, current forecasting solutions lack the temporal and spatial resolutionrequired by system operators. In this paper, we introduce SolarCrossFormer, anovel deep learning model for day-ahead irradiance forecasting, that combinessatellite images and time series from a ground-based network of meteorologicalstations. SolarCrossFormer uses novel graph neural networks to exploit theinter- and intra-modal correlations of the input data and improve the accuracyand resolution of the forecasts. It generates probabilistic forecasts for anylocation in Switzerland with a 15-minute resolution for horizons up to 24 hoursahead. One of the key advantages of SolarCrossFormer its robustness in reallife operations. It can incorporate new time-series data without retraining themodel and, additionally, it can produce forecasts for locations without inputdata by using only their coordinates. Experimental results over a dataset ofone year and 127 locations across Switzerland show that SolarCrossFormer yielda normalized mean absolute error of 6.1 % over the forecasting horizon. Theresults are competitive with those achieved by a commercial numerical weatherprediction service.</description>
      <author>example@mail.com (Baptiste Schubnel, Jelena Simeunović, Corentin Tissier, Pierre-Jean Alet, Rafael E. Carrillo)</author>
      <guid isPermaLink="false">2509.15827v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Optimize Capacity Planning in Semiconductor Manufacturing</title>
      <link>http://arxiv.org/abs/2509.15767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于神经网络的半导体制造产能规划模型，通过深度强化学习训练，能够捕捉机器和处理步骤间的复杂关系，实现主动决策，提高了生产效率。&lt;h4&gt;背景&lt;/h4&gt;在制造业中，产能规划是根据可变需求分配生产资源的过程。当前半导体制造业使用启发式规则优先处理操作，但这些规则难以考虑流程中复杂的相互作用，可能导致瓶颈形成。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉制造系统中复杂相互关系的产能规划模型，克服传统启发式方法的局限性，实现更高效的产能规划。&lt;h4&gt;方法&lt;/h4&gt;提出基于神经网络的机器级产能规划模型，使用深度强化学习训练，采用异构图神经网络表示策略，直接捕捉机器和处理步骤间的多样化关系，并采取措施实现足够的可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;在英特尔的小型Minifab模型和SMT2020测试台上的实验表明，训练后的策略在最大测试场景中使吞吐量提高约1.8%，同时减少约1.8%的周期时间。&lt;h4&gt;结论&lt;/h4&gt;基于深度强化学习和异构图神经网络的产能规划模型能有效捕捉制造系统中复杂的相互关系，为半导体制造业提供比传统启发式方法更有效的产能规划解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在制造业中，产能规划是根据可变需求分配生产资源的过程。当前半导体制造业的典型做法是使用启发式规则来优先处理操作，如考虑未来机器和配方分配的变更列表。然而，虽然启发式规则提供了可解释性，但它们难以考虑流程中复杂的相互作用，而这些相互作用可能导致瓶颈的形成。在这里，我们提出了一个基于神经网络的机器级产能规划模型，使用深度强化学习进行训练。通过使用异构图神经网络表示策略，该模型直接捕捉机器和处理步骤之间的多样化关系，实现主动决策。我们描述了几项为实现足够的可扩展性以处理庞大的机器级操作可能空间而采取的措施。我们的评估结果涵盖了英特尔的小型Minifab模型和使用流行的SMT2020测试台进行的初步实验。在最大测试场景中，我们训练的策略使吞吐量提高了约1.8%，同时减少了约1.8%的周期时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In manufacturing, capacity planning is the process of allocating productionresources in accordance with variable demand. The current industry practice insemiconductor manufacturing typically applies heuristic rules to prioritizeactions, such as future change lists that account for incoming machine andrecipe dedications. However, while offering interpretability, heuristics cannoteasily account for the complex interactions along the process flow that cangradually lead to the formation of bottlenecks. Here, we present a neuralnetwork-based model for capacity planning on the level of individual machines,trained using deep reinforcement learning. By representing the policy using aheterogeneous graph neural network, the model directly captures the diverserelationships among machines and processing steps, allowing for proactivedecision-making. We describe several measures taken to achieve sufficientscalability to tackle the vast space of possible machine-level actions.  Our evaluation results cover Intel's small-scale Minifab model andpreliminary experiments using the popular SMT2020 testbed. In the largesttested scenario, our trained policy increases throughput and decreases cycletime by about 1.8% each.</description>
      <author>example@mail.com (Philipp Andelfinger, Jieyi Bi, Qiuyu Zhu, Jianan Zhou, Bo Zhang, Fei Fei Zhang, Chew Wye Chan, Boon Ping Gan, Wentong Cai, Jie Zhang)</author>
      <guid isPermaLink="false">2509.15767v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Network-assisted Random Forest+</title>
      <link>http://arxiv.org/abs/2509.15611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于随机森林推广的灵活网络辅助模型家族，该模型具有高度竞争性的预测准确性，同时保持可解释性，通过特征重要性措施和专门的解释工具，使从业者能够识别重要特征并量化网络对预测的贡献。&lt;h4&gt;背景&lt;/h4&gt;机器学习算法通常假设训练样本是独立的，但当数据点通过网络连接时，样本间的依赖性既是挑战(减少有效样本量)也是机会(利用网络邻居信息改善预测)。&lt;h4&gt;目的&lt;/h4&gt;解决现有网络辅助方法中可解释性与预测性能之间的差距，开发一种既具有高度竞争性预测准确性又可解释的网络辅助模型。&lt;h4&gt;方法&lt;/h4&gt;提出基于随机森林推广的灵活网络辅助模型家族，开发全局和局部重要性指标以及样本影响指标，使从业者能够识别重要特征并量化网络贡献的重要性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型具有高度竞争性的预测准确性，可以通过特征重要性措施进行解释，并且能够量化网络对预测的贡献。&lt;h4&gt;结论&lt;/h4&gt;这套解释工具扩展了网络辅助机器学习的范围和适用性，特别适用于可解释性和透明性至关重要的高影响力问题。&lt;h4&gt;翻译&lt;/h4&gt;机器学习算法通常假设训练样本是独立的。当数据点通过网络连接时，样本间的依赖性既是挑战，减少了有效样本量，也是利用网络邻居信息改善预测的机会。目前有多种方法利用这一机会，但许多方法(如图神经网络)不易解释，限制了它们对理解模型如何做出预测的用处。其他方法(如网络辅助线性回归)虽然可解释，但通常预测性能明显较差。我们通过提出一种基于随机森林推广的灵活网络辅助模型家族来弥合这一差距，该模型实现了高度竞争性的预测准确性，并通过特征重要性措施可被解释。特别是，我们开发了一套解释工具，使从业者不仅能够识别驱动模型预测的重要特征，还能量化网络对预测的重要性。重要的是，我们提供了全局和局部重要性指标以及样本影响指标，以评估特定观察的影响。这套工具扩展了网络辅助机器学习在高影响力问题上的范围和适用性，其中可解释性和透明性至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning algorithms often assume that training samples areindependent. When data points are connected by a network, the induceddependency between samples is both a challenge, reducing effective sample size,and an opportunity to improve prediction by leveraging information from networkneighbors. Multiple methods taking advantage of this opportunity are nowavailable, but many, including graph neural networks, are not easilyinterpretable, limiting their usefulness for understanding how a model makesits predictions. Others, such as network-assisted linear regression, areinterpretable but often yield substantially worse prediction performance. Webridge this gap by proposing a family of flexible network-assisted models builtupon a generalization of random forests (RF+), which achieveshighly-competitive prediction accuracy and can be interpreted through featureimportance measures. In particular, we develop a suite of interpretation toolsthat enable practitioners to not only identify important features that drivemodel predictions, but also quantify the importance of the network contributionto prediction. Importantly, we provide both global and local importancemeasures as well as sample influence measures to assess the impact of a givenobservation. This suite of tools broadens the scope and applicability ofnetwork-assisted machine learning for high-impact problems whereinterpretability and transparency are essential.</description>
      <author>example@mail.com (Tiffany M. Tang, Elizaveta Levina, Ji Zhu)</author>
      <guid isPermaLink="false">2509.15611v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies</title>
      <link>http://arxiv.org/abs/2509.15481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SolarCAST是一个基于因果信息模型的太阳辐照度预测系统，仅使用历史传感器数据而不需要特殊硬件或复杂预处理，能够有效预测目标地点的全局水平辐照度。&lt;h4&gt;背景&lt;/h4&gt;准确的太阳预测是有效管理可再生能源的基础。以往的研究依赖于需要专门硬件和大量预处理的摄像机或卫星图像。&lt;h4&gt;目的&lt;/h4&gt;开发一个仅使用公共传感器数据就能提供高精度预测的太阳辐照度预测模型。&lt;h4&gt;方法&lt;/h4&gt;SolarCAST使用可扩展的神经组件建模三类混杂因素：1)可观测同步变量（如一天中的时间、站点身份），通过嵌入模块处理；2)潜在同步因素（如区域天气模式），通过时空图神经网络捕获；3)时间滞后影响（如云层在站点间的移动），使用门控变压器建模时间变化。&lt;h4&gt;主要发现&lt;/h4&gt;SolarCAST在各种地理条件下都优于领先的时间序列和多模态基线模型，比顶级商业预测器Solcast实现了25.9%的误差减少。&lt;h4&gt;结论&lt;/h4&gt;SolarCAST为局部太阳预测提供了一个轻量级、实用且可推广的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的太阳预测是有效管理可再生能源的基础。我们提出了SolarCAST，这是一个因果信息模型，仅使用站点X和附近站点S的历史GHI数据来预测目标地点的未来全局水平辐照度(GHI)——与之前依赖需要专门硬件和大量预处理的摄像机或卫星图像的研究不同。为了仅使用公共传感器数据提供高精度，SolarCAST使用可扩展的神经组件建模X-S相关性背后的三类混杂因素：(i)可观测同步变量（如一天中的时间、站点身份），通过嵌入模块处理；(ii)潜在同步因素（如区域天气模式），由时空图神经网络捕获；(iii)时间滞后影响（如云层在站点间的移动），使用学习时间变化的门控变压器建模。它在各种地理条件下都优于领先的时间序列和多模态基线模型，并比顶级商业预测器Solcast实现了25.9%的误差减少。SolarCAST为局部太阳预测提供了一个轻量级、实用且可推广的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760905&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate solar forecasting underpins effective renewable energy management.We present SolarCAST, a causally informed model predicting future globalhorizontal irradiance (GHI) at a target site using only historical GHI fromsite X and nearby stations S - unlike prior work that relies on sky-camera orsatellite imagery requiring specialized hardware and heavy preprocessing. Todeliver high accuracy with only public sensor data, SolarCAST models threeclasses of confounding factors behind X-S correlations using scalable neuralcomponents: (i) observable synchronous variables (e.g., time of day, stationidentity), handled via an embedding module; (ii) latent synchronous factors(e.g., regional weather patterns), captured by a spatio-temporal graph neuralnetwork; and (iii) time-lagged influences (e.g., cloud movement acrossstations), modeled with a gated transformer that learns temporal shifts. Itoutperforms leading time-series and multimodal baselines across diversegeographical conditions, and achieves a 25.9% error reduction over the topcommercial forecaster, Solcast. SolarCAST offers a lightweight, practical, andgeneralizable solution for localized solar forecasting.</description>
      <author>example@mail.com (Yanan Niu, Demetri Psaltis, Christophe Moser, Luisa Lambertini)</author>
      <guid isPermaLink="false">2509.15481v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Partial Column Generation with Graph Neural Networks for Team Formation and Routing</title>
      <link>http://arxiv.org/abs/2509.15275v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对团队组建和路由问题的新型部分列生成策略，通过机器学习和图神经网络预测可能产生负约简成本的列，显著提高了解决方案效率。&lt;h4&gt;背景&lt;/h4&gt;团队组建和路由问题是一个具有挑战性的优化问题，在机场、医疗保健和维护操作等多个现实领域有广泛应用。文献中已提出基于列生成的精确解法来解决此问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对具有多个定价问题设置的部分列生成新策略，通过预测哪些定价问题可能产生有价值的列来优化求解过程。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于机器学习的部分列生成策略，专门为团队组建和路由问题设计的机器学习模型利用图神经网络来预测哪些定价问题可能产生具有负约简成本的列。&lt;h4&gt;主要发现&lt;/h4&gt;计算实验表明，应用所提出的策略能够增强解决方法，并优于文献中的传统部分列生成方法，特别是在严格时间限制下解决困难实例时表现更佳。&lt;h4&gt;结论&lt;/h4&gt;基于机器学习的部分列生成策略在团队组建和路由问题上表现出色，特别是在处理困难实例和时间受限场景时，为解决实际优化问题提供了有效途径。&lt;h4&gt;翻译&lt;/h4&gt;团队组建和路由问题是一个具有挑战性的优化问题，在机场、医疗保健和维护操作等领域有几种现实世界的应用。为解决此问题，文献中已提出基于列生成的精确解法。在本文中，我们提出了一种针对具有多个定价问题设置的新型部分列生成策略，基于预测哪些定价问题可能产生具有负约简成本的列。我们开发了一个专门针对团队组建和路由问题的机器学习模型，利用图神经网络进行这些预测。计算实验表明，应用我们的策略能够增强解决方法，并优于文献中的传统部分列生成方法，特别是在严格时间限制下解决的困难实例上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The team formation and routing problem is a challenging optimization problemwith several real-world applications in fields such as airport, healthcare, andmaintenance operations. To solve this problem, exact solution methods based oncolumn generation have been proposed in the literature. In this paper, wepropose a novel partial column generation strategy for settings with multiplepricing problems, based on predicting which ones are likely to yield columnswith a negative reduced cost. We develop a machine learning model tailored tothe team formation and routing problem that leverages graph neural networks forthese predictions. Computational experiments demonstrate that applying ourstrategy enhances the solution method and outperforms traditional partialcolumn generation approaches from the literature, particularly on hardinstances solved under a tight time limit.</description>
      <author>example@mail.com (Giacomo Dall'Olio, Rainer Kolisch, Yaoxin Wu)</author>
      <guid isPermaLink="false">2509.15275v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features</title>
      <link>http://arxiv.org/abs/2509.16098v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SegDINO3D，一种用于3D实例分割的新型Transformer编码器-解码器框架。&lt;h4&gt;背景&lt;/h4&gt;3D训练数据通常不如2D训练图像充足，因此需要充分利用预训练的2D检测模型中的2D表示来改善3D表示。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够充分利用2D表示（包括图像级和对象级特征）来提高3D表示性能的框架。&lt;h4&gt;方法&lt;/h4&gt;SegDINO3D同时接收点云及其相关的2D图像作为输入；在编码器阶段，通过检索2D图像特征来丰富每个3D点，然后进行3D上下文融合；在解码器阶段，将3D对象查询公式化为3D锚框，并执行从3D查询到2D对象查询的交叉注意力；2D对象查询作为2D图像的紧凑表示避免了内存挑战；引入3D框查询使模型能够使用预测框进行更精确的查询。&lt;h4&gt;主要发现&lt;/h4&gt;SegDINO3D在ScanNetV2和ScanNet200 3D实例分割基准测试上达到了最先进的性能；在具有挑战性的ScanNet200数据集上，SegDINO3D在验证集和隐藏测试集上分别比之前的方法高出+8.7和+6.8 mAP。&lt;h4&gt;结论&lt;/h4&gt;SegDINO3D通过有效利用2D表示来增强3D表示，显著提高了3D实例分割的性能，证明了其优越性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了SegDINO3D，一种用于3D实例分割的新型Transformer编码器-解码器框架。由于3D训练数据通常不如2D训练图像充足，SegDINO3D被设计为充分利用预训练2D检测模型中的2D表示，包括图像级和对象级特征，以提高3D表示。SegDINO3D同时接收点云及其相关的2D图像作为输入。在编码器阶段，它首先通过从相应的图像视图中检索2D图像特征来丰富每个3D点，然后利用3D编码器进行3D上下文融合。在解码器阶段，它将3D对象查询公式化为3D锚框，并执行从3D查询到从2D图像使用2D检测模型获得的2D对象查询的交叉注意力。这些2D对象查询作为2D图像的紧凑对象级表示，有效地避免了在内存中保留数千个图像特征图的挑战，同时忠实地保留了预训练2D模型的知识。引入3D框查询也使模型能够使用预测的框来调制交叉注意力，从而进行更精确的查询。SegDINO3D在ScanNetV2和ScanNet200 3D实例分割基准测试上达到了最先进的性能。值得注意的是，在具有挑战性的ScanNet200数据集上，SegDINO3D在验证集和隐藏测试集上分别比之前的方法高出+8.7和+6.8 mAP，证明了其优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D实例分割任务中因3D训练数据不足导致的性能受限问题。这个问题在现实中非常重要，因为3D环境理解是AI系统与物理世界交互的关键能力，而3D语义理解（如物体实例检测和分割）的不足严重制约了机器人操作、自主导航等下游应用的发展。同时，2D图像数据远比3D数据丰富，如何有效利用2D图像的丰富语义信息来增强3D感知能力是一个重要研究方向。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考：大多数3D分割方法只使用点云而忽略伴随的2D图像；有些方法虽然利用2D特征但缺乏全局3D上下文融合或面临内存挑战。作者借鉴了DETR的编码器-解码器架构，创新性地设计了双层次2D特征利用策略：在编码器阶段通过'最近视图采样'将2D图像级特征注入3D点云；在解码器阶段使用紧凑的2D对象查询而非原始特征图，并引入3D框查询来调制注意力机制。这种方法既保留了2D模型的语义区分能力，又解决了内存和计算效率问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是充分利用2D图像的丰富语义信息来弥补3D数据不足的缺陷，通过双层次特征增强（图像级和对象级）提升3D表示能力。整体流程分为两个阶段：1）编码器阶段：对每个3D点从最近视图中采样2D图像特征，融合后通过3D编码器进行全局上下文融合；2）解码器阶段：初始化3D对象查询（含内容和位置信息），通过'框调制交叉注意力'和'距离感知交叉注意力'分别与3D超点特征和2D对象查询交互，逐步优化查询并生成最终分割结果。整个过程高效利用了2D语义信息，同时保持了3D几何结构的完整性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三方面：1）双层次2D特征增强策略，同时利用图像级和对象级特征；2）引入3D框查询调制位置注意力，使模型能根据物体大小动态调整注意力；3）距离感知交叉注意力机制，实现内存高效的特征利用。相比之前的工作，SegDINO3D不仅利用了2D图像信息，还保留了对象级别的语义表示；解决了直接使用2D特征图面临的内存挑战；通过框查询提供了更精确的空间约束；同时实现了更快的训练收敛速度（从432个epoch减少到88个epoch）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SegDINO3D通过创新性地结合图像级和对象级的2D特征，并引入3D框查询调制机制，显著提高了3D实例分割的性能，同时实现了更快的训练收敛速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present SegDINO3D, a novel Transformer encoder-decoderframework for 3D instance segmentation. As 3D training data is generally not assufficient as 2D training images, SegDINO3D is designed to fully leverage 2Drepresentation from a pre-trained 2D detection model, including bothimage-level and object-level features, for improving 3D representation.SegDINO3D takes both a point cloud and its associated 2D images as input. Inthe encoder stage, it first enriches each 3D point by retrieving 2D imagefeatures from its corresponding image views and then leverages a 3D encoder for3D context fusion. In the decoder stage, it formulates 3D object queries as 3Danchor boxes and performs cross-attention from 3D queries to 2D object queriesobtained from 2D images using the 2D detection model. These 2D object queriesserve as a compact object-level representation of 2D images, effectivelyavoiding the challenge of keeping thousands of image feature maps in the memorywhile faithfully preserving the knowledge of the pre-trained 2D model. Theintroducing of 3D box queries also enables the model to modulatecross-attention using the predicted boxes for more precise querying. SegDINO3Dachieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3Dinstance segmentation benchmarks. Notably, on the challenging ScanNet200dataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAPon the validation and hidden test sets, respectively, demonstrating itssuperiority.</description>
      <author>example@mail.com (Jinyuan Qu, Hongyang Li, Xingyu Chen, Shilong Liu, Yukai Shi, Tianhe Ren, Ruitao Jing, Lei Zhang)</author>
      <guid isPermaLink="false">2509.16098v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>See&amp;Trek: Training-Free Spatial Prompting for Multimodal Large Language Model</title>
      <link>http://arxiv.org/abs/2509.16087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SEE&amp;TREK是一个无需训练的提示框架，旨在增强多模态大语言模型在纯视觉条件下的空间理解能力。&lt;h4&gt;背景&lt;/h4&gt;先前的研究已尝试结合深度或点云等模态提高空间推理，但纯视觉空间理解仍不充分探索。&lt;h4&gt;目的&lt;/h4&gt;SEE&amp;TREK专注于两个核心原则：增加视觉多样性和运动重建，以解决纯视觉空间理解的研究空白。&lt;h4&gt;方法&lt;/h4&gt;1) 视觉多样性：通过最大语义丰富度采样，使用感知模型提取语义丰富的关键帧；2) 运动重建：模拟视觉轨迹并编码相对空间位置到关键帧中，保持空间关系和时间连贯性。该方法无需训练和GPU资源，只需一次前向传递，可无缝集成到现有MLLMs中。&lt;h4&gt;主要发现&lt;/h4&gt;在VSI-BENCH和STI-BENCH上的实验表明，SEE&amp;TREK在各种空间推理任务中持续提升MLLMs性能，最高提升3.5%。&lt;h4&gt;结论&lt;/h4&gt;SEE&amp;TREK为增强空间智能提供了一条有希望的路径。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了SEE&amp;TREK，这是首个专为增强多模态大语言模型(MLLMs)在纯视觉约束下的空间理解而设计的无需训练的提示框架。虽然先前的研究已纳入深度或点云等模态来提高空间推理能力，但纯视觉空间理解仍然未被充分探索。SEE&amp;TREK通过专注于两个核心原则来解决这一差距：增加视觉多样性和运动重建。对于视觉多样性，我们进行最大语义丰富度采样，使用现成的感知模型提取能够捕捉场景结构的语义丰富的关键帧。对于运动重建，我们模拟视觉轨迹并将相对空间位置编码到关键帧中，以保持空间关系和时间连贯性。我们的方法无需训练和GPU资源，只需要一次前向传递，可以无缝集成到现有的MLLMs中。在VSI-BENCH和STI-BENCH上的大量实验表明，SEE&amp;TREK在各种空间推理任务中持续提升了各种MLLMs的性能，最高提升了3.5%，为增强空间智能提供了一条有希望的路径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型在空间理解方面的局限性，特别是在只有视觉输入的情况下。具体来说，它针对两个问题：视觉同质性和未知运动。这个问题很重要，因为空间推理对模型理解和与真实世界环境交互至关重要，特别是在导航、机器人操作等应用中。增强模型的空间意识可以显著改善下游应用的性能，但现有模型在处理复杂空间关系时仍存在困难。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过对现有MLLMs空间理解瓶颈的批判性反思，确定了视觉同质性和未知运动两个关键问题。针对这些问题，他们设计了SEE&amp;TREK框架，包含增加视觉多样性和运动重建两个核心原则。作者借鉴了现有工作中的多种技术，如使用YOLO进行物体检测，利用视觉里程计模拟轨迹，但提出了改进的Balanced-TopK帧选择策略和新的时空编码方法，形成了一个综合解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增强关键帧的语义丰富度和添加运动信息来提升MLLMs的空间理解能力，而无需额外训练或GPU资源。整体流程包括：1)初始采样从视频中每隔N帧取一帧；2)最大语义丰富度采样，使用YOLO检测物体并采用Balanced-TopK策略选择关键帧；3)运动重建，使用视觉里程计估计相机姿态并生成轨迹可视化；4)时空编码，为关键帧添加帧索引和颜色编码标记；5)联合优化提示，将增强关键帧与轨迹可视化和文本提示结合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个训练和GPU免费的空间提示框架；2)最大语义丰富度采样，提出Balanced-TopK策略选择语义丰富且时间分布均匀的关键帧；3)运动重建，通过视觉里程计估计相机运动并将信息编码到关键帧中；4)时空编码，通过直观标记表示时间顺序和空间进展。相比之前工作，SEE&amp;TREK仅使用视觉输入，无需跨模态对齐，不需要微调模型，即插即用，且通过语义丰富采样和运动重建提供更全面的空间信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SEE&amp;TREK通过一种无需训练和GPU资源的空间提示方法，通过增强视觉多样性和重建运动信息，显著提升了多模态大语言模型在空间理解任务上的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SEE&amp;TREK, the first training-free prompting framework tailoredto enhance the spatial understanding of Multimodal Large Language Models(MLLMS) under vision-only constraints. While prior efforts have incorporatedmodalities like depth or point clouds to improve spatial reasoning, purelyvisualspatial understanding remains underexplored. SEE&amp;TREK addresses this gapby focusing on two core principles: increasing visual diversity and motionreconstruction. For visual diversity, we conduct Maximum Semantic RichnessSampling, which employs an off-the-shell perception model to extractsemantically rich keyframes that capture scene structure. For motionreconstruction, we simulate visual trajectories and encode relative spatialpositions into keyframes to preserve both spatial relations and temporalcoherence. Our method is training&amp;GPU-free, requiring only a single forwardpass, and can be seamlessly integrated into existing MLLM'S. Extensiveexperiments on the VSI-B ENCH and STI-B ENCH show that S EE &amp;T REK consistentlyboosts various MLLM S performance across diverse spatial reasoning tasks withthe most +3.5% improvement, offering a promising path toward stronger spatialintelligence.</description>
      <author>example@mail.com (Pengteng Li, Pinhao Song, Wuyang Li, Weiyu Guo, Huizai Yao, Yijie Xu, Dugang Liu, Hui Xiong)</author>
      <guid isPermaLink="false">2509.16087v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Point Cloud Surface Reconstruction using B-Splines</title>
      <link>http://arxiv.org/abs/2509.16050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于字典引导的图卷积网络的表面重建策略，能够在不使用点法线的情况下，同时预测控制点的位置和数量，为有噪声的点云数据生成平滑表面，并在性能上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;从离散点云数据生成连续表面是多个3D视觉应用中的基础任务，但现实世界中的点云数据固有噪声。现有数据驱动表面重建算法依赖真实法线或计算近似法线作为中间步骤，这使得它们在有噪声点云数据集上极不可靠，且真实训练数据并非总是可用。&lt;h4&gt;目的&lt;/h4&gt;开发一种表面重建策略，能够在不使用任何点法线的情况下，为有噪声的点云数据生成平滑表面，同时预测控制点的位置和数量，以匹配底层表面的复杂度。&lt;h4&gt;方法&lt;/h4&gt;开发了一种基于字典引导的图卷积网络的表面重建策略，结合B样条重建技术，这些技术提供点云的紧凑表面表示并具有平滑特性。该方法能够同时预测控制点的位置和数量，而不依赖法线信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛使用的评估指标与多个知名及最新基线方法进行比较，定性和定量地证明了该方法在表面重建任务上优于所有基线方法。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效处理有噪声的点云数据，生成高质量平滑表面，且不依赖于法线信息，在表面重建领域具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;从离散点云数据生成连续表面是多个三维视觉应用中的基础任务。现实世界中的点云数据由于各种技术和环境因素而固有噪声。现有的数据驱动表面重建算法严重依赖真实法线或将计算近似法线作为中间步骤。这种依赖使得它们在有噪声的点云数据集上极不可靠，即使有真实训练数据可用（但并非总是如此）。B样条重建技术提供点云的紧凑表面表示，特别以其平滑特性而闻名。然而，使用B样条近似的表面复杂度直接受样条控制点的数量和位置影响。现有的基于样条的建模方法为给定点云预测固定数量的控制点位置，这使得很难匹配其底层表面的复杂度。在这项工作中，我们开发了一种基于字典引导的图卷积网络的表面重建策略，我们同时为有噪声的点云数据预测控制点的位置和数量，从而在不使用任何点法线的情况下生成平滑表面。我们使用广泛使用的评估指标将我们的重建方法与几个知名及最新的基线方法进行比较，并证明我们的方法在定性和定量上都优于所有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从离散噪声点云数据生成连续表面的问题，特别是在不依赖点法线信息的情况下。这个问题在现实世界中非常重要，因为点云是3D视觉应用的基础，如CAD设计、机器人导航、医疗成像和文化遗产保护等。然而，现实世界中的点云通常含有噪声，而现有方法要么依赖真实法线（通常不可用），要么在噪声环境下表现不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计新方法：传统几何技术和Poisson重建在噪声下表现差；机器学习方法依赖法线估计；B样条方法使用固定数量控制点导致欠拟合或过拟合。作者借鉴了图卷积网络(GCN)处理点云的能力和B样条的平滑特性，结合了数据驱动和解析技术。具体而言，利用GCN提取点云特征，引入可学习字典机制指导控制点预测，并解决了B样条方法对有序网格结构的依赖问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于图的字典引导策略，同时预测B样条控制点的位置和数量，从而从无序噪声点云重建平滑表面，无需法线信息。整体流程包括：1)创建噪声B样条点云数据集；2)使用GCN提取点云特征；3)通过字典引导机制预测控制点；4)用预测的控制点生成B样条表面。训练时使用加权均方误差损失函数，在GPU上优化模型参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)同时预测控制点位置和数量，而非固定数量；2)无需点法线或其中间估计；3)使用GCN处理无序点云；4)引入可学习字典机制；5)创建专门的噪声点云数据集。相比之前工作，本文方法能处理无序、噪声点云，自适应确定控制点数量，避免了传统方法在噪声下的敏感性和现有B样条方法对有序结构的依赖，在多个评估指标上表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于图卷积网络的字典引导方法，能够同时预测B样条控制点的位置和数量，从而从无序噪声点云中重建平滑连续的表面，无需依赖点法线信息，并在多个评估指标上优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating continuous surfaces from discrete point cloud data is afundamental task in several 3D vision applications. Real-world point clouds areinherently noisy due to various technical and environmental factors. Existingdata-driven surface reconstruction algorithms rely heavily on ground truthnormals or compute approximate normals as an intermediate step. This dependencymakes them extremely unreliable for noisy point cloud datasets, even if theavailability of ground truth training data is ensured, which is not always thecase. B-spline reconstruction techniques provide compact surfacerepresentations of point clouds and are especially known for their smootheningproperties. However, the complexity of the surfaces approximated usingB-splines is directly influenced by the number and location of the splinecontrol points. Existing spline-based modeling methods predict the locations ofa fixed number of control points for a given point cloud, which makes it verydifficult to match the complexity of its underlying surface. In this work, wedevelop a Dictionary-Guided Graph Convolutional Network-based surfacereconstruction strategy where we simultaneously predict both the location andthe number of control points for noisy point cloud data to generate smoothsurfaces without the use of any point normals. We compare our reconstructionmethod with several well-known as well as recent baselines by employingwidely-used evaluation metrics, and demonstrate that our method outperforms allof them both qualitatively and quantitatively.</description>
      <author>example@mail.com (Stuti Pathak, Rhys G. Evans, Gunther Steenackers, Rudi Penne)</author>
      <guid isPermaLink="false">2509.16050v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Towards Sharper Object Boundaries in Self-Supervised Depth Estimation</title>
      <link>http://arxiv.org/abs/2509.15987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BMVC 2025 Oral, 10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用自监督方法就能产生清晰深度不连续性的单目深度估计方法，在KITTI和VKITTIv2数据集上实现了高达35%的更高边界清晰度，并改进了点云质量。&lt;h4&gt;背景&lt;/h4&gt;精确的单目深度估计对3D场景理解至关重要，但现有方法通常在物体边界处模糊深度，引入虚假的中间3D点。&lt;h4&gt;目的&lt;/h4&gt;实现具有清晰边缘的深度估计，而无需非常细粒度的监督，仅使用自监督方法产生清晰的深度不连续性。&lt;h4&gt;方法&lt;/h4&gt;将每像素深度建模为混合分布，捕获多种合理的深度值，并将不确定性从直接回归转移到混合权重；通过方差感知损失函数和不确定性传播，将此公式无缝集成到现有流程中。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和VKITTIv2上的广泛评估表明，与最先进的基线相比，该方法可实现高达35%的更高边界清晰度，并改进了点云质量。&lt;h4&gt;结论&lt;/h4&gt;该方法能够在仅使用自监督的情况下产生清晰的深度不连续性，在多个数据集上超越了现有技术。&lt;h4&gt;翻译&lt;/h4&gt;精确的单目深度估计对3D场景理解至关重要，但现有方法通常在物体边界处模糊深度，引入虚假的中间3D点。虽然实现清晰边缘通常需要非常细粒度的监督，但我们的方法仅使用自监督就能产生清晰的深度不连续性。具体来说，我们将每像素深度建模为混合分布，捕获多种合理的深度值，并将不确定性从直接回归转移到混合权重。通过方差感知损失函数和不确定性传播，该公式能无缝集成到现有流程中。在KITTI和VKITTIv2上的广泛评估表明，与最先进的基线相比，我们的方法可实现高达35%的更高边界清晰度，并改进了点云质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目深度估计中物体边界模糊的问题。现有方法在物体边界处往往产生模糊的深度值，导致出现虚假的中间3D点，这在点云中表现为伪影。这个问题在现实中非常重要，因为准确的深度边界对3D场景理解、自动驾驶中的障碍物检测、机器人导航和增强现实应用至关重要。模糊的边界会影响3D重建质量，导致后续处理任务如物体识别和场景解析的准确性下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到传统方法为每个像素分配单一深度值，但在边界处存在不确定性，导致深度值在前景和背景之间平均，使过渡模糊。他们想到使用混合分布来表示每个像素的深度，而不是单一值，这样可以明确建模边界处的多种可能深度。作者借鉴了混合密度网络和期望最大化算法的思想，参考了Tosi等人使用拉普拉斯分布混合进行监督立体深度估计的工作，以及Kendall和Gal关于深度预测中不确定性估计的贝叶斯框架。同时，他们采用了标准的自监督深度估计管道，基于视图合成作为监督信号。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将每个像素的深度表示为两个高斯分布的混合，而不是单一值，这样能够捕捉物体边界处的前景和背景两种可能的深度值。通过混合权重控制每个分布的相对重要性，从而在边界处创建清晰的深度不连续性。整体流程包括：1)分布表示：网络预测五个参数(两个均值、两个方差和一个混合权重)；2)分布传播：将深度分布通过重投影函数传播到支持视图，使用一阶近似技术处理不确定性；3)损失计算：为每个组件计算误差分布，使用竞争训练方法使组件专门化；4)推理：根据混合权重选择最可能的深度值，避免由不确定性引起的平均效应。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合分布表示：首次将每个像素的深度表示为混合分布，自然捕捉边界处的不确定性；2)分布传播技术：提出原则性方法传播深度分布通过重投影过程；3)不确定性感知损失函数：开发能处理多模态深度分布的损失函数；4)边缘锐度度量：提出基于边缘像素熵的边界锐度度量。相比之前的工作，不同之处在于：传统方法需要精细监督或高分辨率图像和精细标注，而本文仅使用自监督；传统方法分配单一深度值导致边界模糊，而本文使用混合分布明确建模边界处多种可能深度；传统方法忽略或简单处理不确定性，而本文将不确定性作为核心组成部分；本文还提出了新的边缘熵度量专门评估边界锐度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将每个像素的深度建模为混合分布并提出分布传播技术，在仅使用自监督的情况下显著提高了单目深度估计中物体边界的锐利度，同时改进了点云质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate monocular depth estimation is crucial for 3D scene understanding,but existing methods often blur depth at object boundaries, introducingspurious intermediate 3D points. While achieving sharp edges usually requiresvery fine-grained supervision, our method produces crisp depth discontinuitiesusing only self-supervision. Specifically, we model per-pixel depth as amixture distribution, capturing multiple plausible depths and shiftinguncertainty from direct regression to the mixture weights. This formulationintegrates seamlessly into existing pipelines via variance-aware loss functionsand uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 showthat our method achieves up to 35% higher boundary sharpness and improves pointcloud quality compared to state-of-the-art baselines.</description>
      <author>example@mail.com (Aurélien Cecille, Stefan Duffner, Franck Davoine, Rémi Agier, Thibault Neveu)</author>
      <guid isPermaLink="false">2509.15987v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>PAN: Pillars-Attention-Based Network for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.15935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖且高效的基于摄像头和雷达的3D目标检测算法，在鸟瞰图视角下工作，利用雷达点云的优势如精确距离估计和速度信息，并优化了推理时间。&lt;h4&gt;背景&lt;/h4&gt;Camera-radar融合是Camera-lidar融合在3D目标检测任务中的一种稳健且低成本的替代方案，特别适用于恶劣天气和光照条件。然而，目前文献中很少有工作关注这种模态并探索雷达点云的优势。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖且高效的3D目标检测算法，使用摄像头和雷达在鸟瞰图视角下工作，充分利用雷达点云的优势并提高推理效率。&lt;h4&gt;方法&lt;/h4&gt;在融合特征前利用雷达优势；引入新backbone将雷达柱状特征映射到嵌入维度；使用自注意力机制建模雷达点间依赖关系；用简化卷积层替换基于FPN的卷积层以减少推理时间。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在3D目标检测问题上达到新最先进水平，使用ResNet-50时NDS指标达58.2，同时在nuScenes数据集上为同类算法设置了推理时间新基准。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法通过优化雷达特征处理和简化网络结构，在保持高精度的同时显著提高了推理速度，为实时3D目标检测提供了新解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摄像头-雷达融合为摄像头-激光雷达融合提供了一种稳健且低成本的替代方案，用于在恶劣天气和光照条件下实时执行3D目标检测任务。然而，目前文献中很少有工作关注这种模态，更重要的是，很少有工作开发新架构来探索雷达点云的优势，如精确的距离估计和速度信息。因此，这项工作提出了一种新颖且高效的3D目标检测算法，使用摄像头和雷达在鸟瞰图视角下工作。我们的算法在将特征融合到检测头之前利用雷达的优势。引入了一个新的backbone，将雷达柱状特征映射到嵌入维度。自注意力机制使backbone能够建模雷达点之间的依赖关系。我们使用简化的卷积层替换基于PointPillars架构中基于FPN的卷积层，主要目标是减少推理时间。我们的结果表明，通过这种修改，我们的方法在3D目标检测问题上达到了新的最先进水平，使用ResNet-50时NDS指标达到58.2，同时为同一类别在nuScenes数据集上设置了推理时间的新基准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在3D目标检测中如何有效利用相机-雷达融合技术，以提高在恶劣天气和光照条件下的实时检测性能。这个问题在现实中非常重要，因为自动驾驶系统需要在各种环境条件下准确识别和定位物体，而现有传感器如相机和激光雷达在恶劣条件下性能下降。雷达数据具有精确的距离估计和速度信息等优势，但现有研究很少充分利用这些特性，且缺乏专门针对相机-雷达融合的新架构。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到雷达点云的稀疏性特点导致传统卷积方法效率低下，因此设计了基于支柱的注意力机制网络(PAN)来更好地利用雷达特性。作者借鉴了现有工作中的多个元素：使用ResNet提取相机特征，借鉴PointPillars的点云处理思想但进行了改进，采用自注意力机制建模雷达点依赖关系，使用雷达辅助的视图转换(RVT)将相机特征转换为鸟瞰图(BEV)，应用多模态特征聚合融合雷达和相机特征，以及使用CenterPoint作为检测头。这些借鉴被创新性地结合并改进，形成了PAN方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用雷达数据在恶劣天气条件下的鲁棒性和精确的距离/速度信息，通过基于支柱的注意力机制有效提取雷达特征，并简化网络结构以提高推理速度，最终在鸟瞰图空间中融合相机和雷达特征。整体实现流程分为四个阶段：1)特征提取阶段，使用ResNet提取相机特征，使用PAN骨干提取雷达特征；2)视图转换阶段，使用RVT将相机特征转换为BEV，将雷达特征转换为二进制占用图；3)特征融合阶段，使用多模态特征聚合模块融合雷达和相机BEV特征；4)目标检测阶段，使用CenterPoint检测头进行3D目标检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出PAN骨干网络，使用自注意力机制建模雷达点依赖关系，去除空柱有效利用雷达特征；2)简化基于FPN的卷积层，减少推理时间；3)在恶劣天气条件下表现优异，在nuScenes数据集上达到58.2的NDS指标。相比之前工作，PAN在推理速度上提高约43%，同时检测精度更高；相比纯相机方法，在恶劣条件下性能更好；相比相机-激光雷达融合方法，成本更低且更鲁棒；更有效地利用雷达的距离和速度信息，提高了方向和属性估计的准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAN提出了一种基于支柱注意力的相机-雷达融合网络，通过有效利用雷达特征并简化网络结构，在保持高检测精度的同时显著提高了推理速度，特别是在恶劣天气条件下表现出优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-radar fusion offers a robust and low-cost alternative to Camera-lidarfusion for the 3D object detection task in real-time under adverse weather andlighting conditions. However, currently, in the literature, it is possible tofind few works focusing on this modality and, most importantly, developing newarchitectures to explore the advantages of the radar point cloud, such asaccurate distance estimation and speed information. Therefore, this workpresents a novel and efficient 3D object detection algorithm using cameras andradars in the bird's-eye-view (BEV). Our algorithm exploits the advantages ofradar before fusing the features into a detection head. A new backbone isintroduced, which maps the radar pillar features into an embedded dimension. Aself-attention mechanism allows the backbone to model the dependencies betweenthe radar points. We are using a simplified convolutional layer to replace theFPN-based convolutional layers used in the PointPillars-based architectureswith the main goal of reducing inference time. Our results show that with thismodification, our approach achieves the new state-of-the-art in the 3D objectdetection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,while also setting a new benchmark for inference time on the nuScenes datasetfor the same category.</description>
      <author>example@mail.com (Ruan Bispo, Dane Mitrev, Letizia Mariotti, Clément Botty, Denver Humphrey, Anthony Scanlan, Ciarán Eising)</author>
      <guid isPermaLink="false">2509.15935v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation</title>
      <link>http://arxiv.org/abs/2509.15886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了将视觉基础模型SAM2应用于LiDAR点云分割的range-view方法，通过结合2D特征提取与投影技术，实现了在保持性能的同时提高效率和简化部署。&lt;h4&gt;背景&lt;/h4&gt;点云分割是自动驾驶和3D场景理解的核心技术。当前主流的体素和点云方法虽能捕捉细粒度几何信息，但计算成本高、内存访问不规则且实时效率有限。相比之下，range-view方法相对未被充分探索，但可利用成熟的2D语义分割技术进行快速准确预测。&lt;h4&gt;目的&lt;/h4&gt;研究当前最先进的视觉基础模型SAM2是否能作为LiDAR点云在range view中的强大backbone，并开发首个适应SAM2用于3D分割的range-view框架。&lt;h4&gt;方法&lt;/h4&gt;结合高效的2D特征提取与标准投影/反投影操作处理点云，并对编码器进行三种架构修改：(1)强调LiDAR范围图像中水平空间依赖性的新模块；(2)针对球形投影几何特性定制的配置；(3)专门设计用于捕获range-view伪图像中独特空间模式和间断性的适应机制。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在SemanticKITTI数据集上实现了有竞争力的性能，同时受益于2D为中心的管道的速度、可扩展性和部署简便性。&lt;h4&gt;结论&lt;/h4&gt;视觉基础模型作为3D感知的通用backbone具有可行性，为统一、基础模型驱动的LiDAR分割开辟了道路，使用VFMs的range-view分割方法取得了有前景的结果。&lt;h4&gt;翻译&lt;/h4&gt;点云分割是自动驾驶和3D场景理解的核心。虽然最近的体素和基于点的方法由于其与深度架构的兼容性和捕捉细粒度几何的能力而主导研究，但它们通常带来高计算成本、不规则内存访问和有限的实时效率。相比之下，range-view方法虽然相对未被充分探索，但可以利用成熟的2D语义分割技术进行快速准确的预测。受视觉基础模型在描述、零样本识别和多模态任务方面的快速进展启发，我们研究了当前最先进的分割VFM模型SAM2是否能作为LiDAR点云在range view中的强大backbone。我们提出了，据我们所知，首个将SAM2适应于3D分割的range-view框架，结合高效的2D特征提取与标准投影/反投影操作来处理点云。为了将SAM2优化用于range-view表示，我们在编码器中实现了几种架构修改：(1)一个强调LiDAR范围图像中固有水平空间依赖性的新模块；(2)针对球形投影几何特性定制的配置；(3)在编码器主干中适应的机制，专门设计用于捕获range-view伪图像中存在的独特空间模式和间断性。我们的方法在SemanticKITTI上实现了有竞争力的性能，同时受益于2D为中心管道的速度、可扩展性和部署简便性。这项工作强调了VFMs作为3D感知通用backbone的可行性，并为统一、基础模型驱动的LiDAR分割开辟了道路。结果让我们得出结论，使用VFMs的range-view分割方法取得了有前景的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是LiDAR点云分割的效率和准确性问题。现有的点云分割方法（如体素和点方法）计算成本高、内存访问不规则、运行效率有限。这个问题在自动驾驶和3D场景理解中至关重要，因为准确的点云分割能让系统实时区分车辆、行人、路标等物体，是自动驾驶和机器人导航的基础技术。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到视觉基础模型快速进展的启发，特别是SAM2在分割任务上的优异表现。他们注意到基于range-view投影的方法虽然被忽视，但可以利用成熟的2D语义分割技术。作者借鉴了多项现有工作：使用SAM2作为骨干网络、采用range-view表示方法、应用Receptive Field Blocks进行特征解码、使用kNN插值进行后处理，以及采用复合损失函数。主要创新在于将SAM2适配到3D点云分割任务中，通过特定修改使其能处理range-view表示的点云数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型SAM2的强大分割能力，通过将3D点云投影为2D range-view图像，然后使用SAM2进行分割，最后将分割结果投影回原始点云。整体流程包括：1)预处理：将无序LiDAR扫描转换为range-view表示；2)模型处理：使用Stem模块转换输入，通过修改后的SAM2编码器处理range-view图像，使用包含Receptive Field Blocks的解码器进行特征解码；3)后处理：通过kNN插值将分割结果传播到全分辨率点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次将SAM2视觉基础模型应用于range-view表示的LiDAR点云分割；2)对SAM2编码器进行特定修改：新的Stem模块强调水平空间依赖性，定制化Hiera Blocks适应球形投影，适配的窗口注意力机制捕获独特空间模式；3)提出多组件编码器架构，利用预训练的Hiera骨干网络；4)使用复合损失函数解决类别不平衡问题。相比之前工作，大多数方法专注于计算成本高的体素或点处理方法，而RangeSAM利用视觉基础模型的知识转移能力，同时保持2D方法的效率优势。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RangeSAM成功地将先进的视觉基础模型SAM2适配到3D点云分割任务中，通过特定的架构修改和range-view表示，实现了与现有方法相媲美的性能，同时保持了2D方法的效率优势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud segmentation is central to autonomous driving and 3D sceneunderstanding. While voxel- and point-based methods dominate recent researchdue to their compatibility with deep architectures and ability to capturefine-grained geometry, they often incur high computational cost, irregularmemory access, and limited real-time efficiency. In contrast, range-viewmethods, though relatively underexplored - can leverage mature 2D semanticsegmentation techniques for fast and accurate predictions. Motivated by therapid progress in Visual Foundation Models (VFMs) for captioning, zero-shotrecognition, and multimodal tasks, we investigate whether SAM2, the currentstate-of-the-art VFM for segmentation tasks, can serve as a strong backbone forLiDAR point cloud segmentation in the range view. We present , to ourknowledge, the first range-view framework that adapts SAM2 to 3D segmentation,coupling efficient 2D feature extraction with standardprojection/back-projection to operate on point clouds. To optimize SAM2 forrange-view representations, we implement several architectural modifications tothe encoder: (1) a novel module that emphasizes horizontal spatial dependenciesinherent in LiDAR range images, (2) a customized configuration of tailored tothe geometric properties of spherical projections, and (3) an adapted mechanismin the encoder backbone specifically designed to capture the unique spatialpatterns and discontinuities present in range-view pseudo-images. Our approachachieves competitive performance on SemanticKITTI while benefiting from thespeed, scalability, and deployment simplicity of 2D-centric pipelines. Thiswork highlights the viability of VFMs as general-purpose backbones for 3Dperception and opens a path toward unified, foundation-model-driven LiDARsegmentation. Results lets us conclude that range-view segmentation methodsusing VFMs leads to promising results.</description>
      <author>example@mail.com (Paul Julius Kühn, Duc Anh Nguyen, Arjan Kuijper, Holger Graf, Dieter Fellner, Saptarshi Neil Sinha)</author>
      <guid isPermaLink="false">2509.15886v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion</title>
      <link>http://arxiv.org/abs/2509.15750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 15 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FloorSAM是一种整合点云密度图与分割一切模型(SAM)的框架，用于从LiDAR数据准确重建建筑平面图，解决了传统方法面临的噪声、泛化能力有限和几何细节丢失等问题。&lt;h4&gt;背景&lt;/h4&gt;从点云数据重建建筑平面图对室内导航、BIM和精确测量至关重要，但传统几何算法和基于Mask R-CNN的深度学习方法存在噪声、泛化能力有限和几何细节丢失等问题。&lt;h4&gt;目的&lt;/h4&gt;提出FloorSAM框架，通过结合点云密度图与SAM模型，实现从LiDAR数据准确重建建筑平面图，提高重建的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;FloorSAM使用网格过滤、自适应分辨率投影和图像增强创建鲁棒的自上而下密度图，利用SAM的零样本学习进行精确房间分割，通过自适应提示点和多阶段过滤生成房间掩码，最后结合掩码和点云分析进行轮廓提取和规则化。&lt;h4&gt;主要发现&lt;/h4&gt;在Giblayout和ISPRS数据集上的测试表明，FloorSAM比传统方法具有更好的准确性、召回率和鲁棒性，特别是在嘈杂和复杂环境中表现更佳。&lt;h4&gt;结论&lt;/h4&gt;FloorSAM能够生成准确的平面图并恢复房间拓扑关系，相关代码和材料已公开在github.com/Silentbarber/FloorSAM。&lt;h4&gt;翻译&lt;/h4&gt;从点云数据重建建筑平面图对室内导航、BIM和精确测量至关重要。传统方法如几何算法和基于Mask R-CNN的深度学习通常面临噪声、泛化能力有限和几何细节丢失等问题。我们提出了FloorSAM，一种将点云密度图与分割一切模型(SAM)相结合的框架，用于从LiDAR数据准确重建平面图。通过使用基于网格的过滤、自适应分辨率投影和图像增强，我们创建了鲁棒的自上而下密度图。FloorSAM利用SAM的零样本学习进行精确的房间分割，提高了对不同布局的重建效果。房间掩码通过自适应提示点和多阶段过滤生成，随后进行联合掩码和点云分析以进行轮廓提取和规则化。这产生了准确的平面图并恢复了房间拓扑关系。在Giblayout和ISPRS数据集上的测试显示，与传统方法相比，FloorSAM具有更好的准确性、召回率和鲁棒性，特别是在嘈杂和复杂环境中。代码和材料：github.com/Silentbarber/FloorSAM。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从激光雷达点云数据重建建筑平面图的问题。这个问题在现实中非常重要，因为准确的室内平面图对室内导航、建筑信息模型(BIM)和高精度室内测量应用至关重要。传统方法在处理噪声、复杂布局和几何细节时存在局限性，影响了测量的准确性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统几何方法和基于深度学习方法的局限性，如对噪声敏感、泛化能力有限和需要大量标注数据。然后，他们结合了点云密度图和SAM(分割任意模型)的优势，设计了新的框架。作者借鉴了多种现有工作，包括传统几何算法(如RANSAC)、基于深度学习的方法(如Mask R-CNN、HEAT、RoomFormer等)，以及图结构推理方法，但创新性地将它们与SAM的零样本学习能力相结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将房间高度的点云密度图与SAM的引导分割能力相结合，并利用SAM的零样本学习能力实现高保真房间分割。整体流程分为三个阶段：1)预处理阶段，包括点云过滤、密度图生成和自适应提示点提取；2)房间掩码过滤阶段，通过粗过滤和精过滤筛选高质量单房间掩码；3)轮廓绘制阶段，通过点云和掩码的联合分析提取和正则化轮廓，并恢复房间间的拓扑关系。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出结合密度图和SAM的混合框架，引入零样本分割到平面图重建；2)设计多阶段掩码过滤机制，从冗余掩码中提取高质量单房间分割；3)开发掩码-点云联合轮廓绘制和正则化算法，保留几何细节同时生成正则化轮廓。相比之前的工作，FloorSAM不依赖大量标注数据，能处理非曼哈顿结构，对噪声更具鲁棒性，且在复杂布局中能保留更多几何细节。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FloorSAM通过融合点云密度图与SAM的零样本分割能力，实现了从激光雷达点云数据中高质量、鲁棒的建筑平面图重建，解决了传统方法在噪声环境和复杂布局下的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing building floor plans from point cloud data is key for indoornavigation, BIM, and precise measurements. Traditional methods like geometricalgorithms and Mask R-CNN-based deep learning often face issues with noise,limited generalization, and loss of geometric details. We propose FloorSAM, aframework that integrates point cloud density maps with the Segment AnythingModel (SAM) for accurate floor plan reconstruction from LiDAR data. Usinggrid-based filtering, adaptive resolution projection, and image enhancement, wecreate robust top-down density maps. FloorSAM uses SAM's zero-shot learning forprecise room segmentation, improving reconstruction across diverse layouts.Room masks are generated via adaptive prompt points and multistage filtering,followed by joint mask and point cloud analysis for contour extraction andregularization. This produces accurate floor plans and recovers roomtopological relationships. Tests on Giblayout and ISPRS datasets show betteraccuracy, recall, and robustness than traditional methods, especially in noisyand complex settings. Code and materials: github.com/Silentbarber/FloorSAM.</description>
      <author>example@mail.com (Han Ye, Haofu Wang, Yunchi Zhang, Jiangjian Xiao, Yuqiang Jin, Jinyuan Liu, Wen-An Zhang, Uladzislau Sychou, Alexander Tuzikov, Vladislav Sobolevskii, Valerii Zakharov, Boris Sokolov, Minglei Fu)</author>
      <guid isPermaLink="false">2509.15750v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions</title>
      <link>http://arxiv.org/abs/2509.15693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  to appear in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SceneForge是一个新颖的框架，通过构建具有明确空间关系的多对象场景来增强3D点云与文本之间的对比学习，有效解决了大规模3D-文本数据集稀缺的问题，在多个任务上展现出显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;在3D文本对比学习中，整体大于部分之和。然而，大规模3D-文本数据集的稀缺限制了相关研究的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够增强3D点云与文本之间对比对齐的框架，解决大规模3D-文本数据集稀缺的问题，提高3D-文本多模态任务的性能。&lt;h4&gt;方法&lt;/h4&gt;SceneForge利用个体3D形状构建具有明确空间关系的多对象场景，并与由大型语言模型完善的多对象描述配对。通过系统研究关键设计元素，如每个场景的最佳对象数量、训练批次中组合样本的比例以及场景构建策略，优化框架性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. SceneForge在多个任务上带来显著的性能提升，包括在多个数据集上的零样本分类和少样本部分分割2. SceneForge的组合增强与模型无关，能够一致地提高多种编码器架构的性能3. SceneForge在3D视觉问答任务中表现更好，能够稳健地推广到具有递增场景复杂度的检索场景4. SceneForge展示了空间推理能力，能够调整空间配置以精确匹配文本指令&lt;h4&gt;结论&lt;/h4&gt;SceneForge通过结构化的多对象场景组合，有效解决了3D-文本对比学习中数据稀缺的问题，显著提高了模型在多种任务上的性能，展示了强大的空间推理能力和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;整体大于部分之和——即使在3D文本对比学习中也是如此。我们引入了SceneForge，一个新颖的框架，通过构建具有明确空间关系的多对象场景组合来增强3D点云与文本之间的对比对齐。SceneForge利用个体3D形状构建具有明确空间关系的多对象场景，并将其与由大型语言模型完善的多对象描述配对。通过添加这些结构化的、组合式的样本来增强对比训练，SceneForge有效解决了大规模3D-文本数据集稀缺的问题，显著丰富了数据的复杂性和和多样性。我们系统地研究了关键设计元素，如每个场景的最佳对象数量、训练批次中组合样本的比例以及场景构建策略。大量实验表明，SceneForge在多个任务上带来了显著的性能提升，包括在ModelNet、ScanObjNN、Objaverse-LVIS和ScanNet上的零样本分类，以及在ShapeNetPart上的少样本部分分割。SceneForge的组合增强与模型无关，能够一致地提高多种编码器架构的性能。此外，SceneForge在ScanQA上的3D视觉问答任务中表现更好，能够稳健地推广到具有递增场景复杂度的检索场景，并通过调整空间配置以精确匹配文本指令展示了空间推理能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云与文本对比学习中大规模数据集稀缺的问题。这个问题很重要，因为3D数据在机器人、虚拟现实和增强现实等领域具有关键价值，而相比2D图像-文本数据集，3D-文本数据集数量有限，限制了3D视觉语言模型的发展和应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受图像分类中的组合数据增强方法(如CutMix、MixUp)和图像-文本对比学习中的组合方法启发，注意到3D点云可以自由组合成结构化场景而不会产生视觉伪影。他们利用这一特性，结合大语言模型生成场景描述，设计了SCENEFORGE框架。这种方法借鉴了现有工作的思想，但针对3D数据的特性进行了创新性设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建结构化的多物体3D场景来增强3D点云与文本的对比学习对齐。实现流程包括：1)接收一批点云及其标题；2)随机保留单物体或组合成多物体场景；3)通过3D场景锻造模块根据空间关系安排物体；4)通过场景标题锻造模块生成连贯描述；5)混合单物体和多物体样本进行训练；6)使用对比损失进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)结构化多物体场景组合；2)显式空间关系建模('over','under','next to')；3)大语言模型增强的描述生成；4)模型无关的增强方法。相比之前工作，SCENEFORGE构建了语义上有意义的结构化场景而非简单混合，利用了3D数据可自由组合的特性，实现了对物体定位的明确控制，并在多种任务上超越了使用集成方法的模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCENEFORGE通过结构化多物体场景组合增强3D-文本对比学习，在保持模型无关性的同时显著提升了多种3D视觉语言任务的性能，解决了3D领域大规模数据稀缺的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The whole is greater than the sum of its parts-even in 3D-text contrastivelearning. We introduce SceneForge, a novel framework that enhances contrastivealignment between 3D point clouds and text through structured multi-objectscene compositions. SceneForge leverages individual 3D shapes to constructmulti-object scenes with explicit spatial relations, pairing them with coherentmulti-object descriptions refined by a large language model. By augmentingcontrastive training with these structured, compositional samples, SceneForgeeffectively addresses the scarcity of large-scale 3D-text datasets,significantly enriching data complexity and diversity. We systematicallyinvestigate critical design elements, such as the optimal number of objects perscene, the proportion of compositional samples in training batches, and sceneconstruction strategies. Extensive experiments demonstrate that SceneForgedelivers substantial performance gains across multiple tasks, includingzero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,as well as few-shot part segmentation on ShapeNetPart. SceneForge'scompositional augmentations are model-agnostic, consistently improvingperformance across multiple encoder architectures. Moreover, SceneForgeimproves 3D visual question answering on ScanQA, generalizes robustly toretrieval scenarios with increasing scene complexity, and showcases spatialreasoning capabilities by adapting spatial configurations to align preciselywith textual instructions.</description>
      <author>example@mail.com (Cristian Sbrolli, Matteo Matteucci)</author>
      <guid isPermaLink="false">2509.15693v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>A PCA Based Model for Surface Reconstruction from Incomplete Point Clouds</title>
      <link>http://arxiv.org/abs/2509.15675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于主成分分析(PCA)的模型，用于从不完整的点云数据进行表面重建。该模型利用PCA估计底层表面的法线信息，并将其作为正则化子指导重建过程，特别是在数据缺失区域。研究还引入了算子分裂方法来有效求解该模型。&lt;h4&gt;背景&lt;/h4&gt;点云数据是数学建模中的重要信息形式，表面重建是跨学科的重要任务。然而，在扫描过程中，由于高光吸收率和遮挡等因素，收集的点云数据可能无法覆盖整个表面，导致数据集不完整。&lt;h4&gt;目的&lt;/h4&gt;推断数据缺失区域的表面结构，并成功重建表面，解决因扫描不完整导致的表面重建挑战。&lt;h4&gt;方法&lt;/h4&gt;使用主成分分析(PCA)从可用点云数据估计底层表面的法线信息，将估计的法线信息作为模型中的正则化子，指导表面重建，特别是在数据缺失区域，并引入算子分裂方法来有效求解提出的模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过系统实验，该模型能够成功推断数据缺失区域的表面结构，并很好地重建底层表面，性能优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;基于PCA的模型能有效处理不完整点云数据的表面重建问题，特别是在数据缺失区域表现优异。&lt;h4&gt;翻译&lt;/h4&gt;点云数据是数学建模中的一种重要信息形式，从这些数据进行表面重建是跨学科的重要任务。然而，在扫描过程中，由于高光吸收率和遮挡等因素，收集的点云数据可能无法覆盖整个表面，导致数据集不完整。推断数据缺失区域的表面结构并成功重建表面是一项挑战。在本文中，我们提出了一种基于主成分分析(PCA)的模型，用于从不完整的点云数据进行表面重建。最初，我们使用PCA从可用点云数据估计底层表面的法线信息。这种估计的法线信息在我们的模型中作为正则化子，指导表面重建，特别是在数据缺失区域。此外，我们引入了一种算子分裂方法来有效求解所提出的模型。通过系统实验，我们证明了我们的模型能够成功推断数据缺失区域的表面结构，并很好地重建底层表面，性能优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从点云数据中重建表面时，由于扫描过程中的高光吸收率和遮挡等因素导致的数据不完整问题。这个问题在现实和研究中非常重要，因为点云数据是数学建模的关键信息，表面重建在多个领域（如城市重建、医学成像、计算机图形学、文化遗产保护和元宇宙开发等）有广泛应用。然而，现有方法主要针对完整的点云数据，对于不完整数据的处理研究不足，导致在数据缺失区域难以准确推断表面结构。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了不完整点云数据重建表面的挑战，指出在数据缺失区域仅依靠距离函数难以控制重建表面。他们借鉴了水平集方法（如Zhao等人[52,51]的工作）来隐式表示表面，以及[20]引入表面曲率作为正则化的方法和[30]利用法线信息对齐的方法。在此基础上，作者创新性地提出使用主成分分析（PCA）从可用点云数据估计底层表面的法线信息，并将这些估计的法线作为正则化项引入模型，特别是在数据缺失区域。作者还引入了算子分裂方法来有效求解提出的模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1)使用PCA从点云数据估计表面法线信息，即使在数据缺失区域也能提供可靠估计；2)将估计的法线信息作为正则化项引入表面重建模型；3)利用水平集方法隐式表示表面，并通过算子分裂方法高效求解。整体流程包括：1)预处理阶段计算距离函数和PCA主方向；2)构建包含数据保真项、曲率正则化项和法线信息正则化项的优化模型；3)使用水平集方法将问题公式化；4)通过算子分裂方法将问题分解为四个子步骤迭代求解；5)进行数值离散化并使用FFT高效求解；6)对水平集函数重新初始化并提取零水平集作为重建表面。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于PCA的表面重建模型，专门处理不完整点云数据；2)引入法线信息正则化项，惩罚重建表面法线与PCA估计法线间的角度；3)设计算子分裂方法高效求解模型；4)方法对噪声具有鲁棒性。相比之前工作，本文专注于不完整数据的处理，不依赖预先提供的法线信息，而是从点云数据中估计法线；模型结合了数据保真、曲率正则化和法线信息正则化三项；求解方法采用算子分裂，提高了效率且对超参数不那么敏感。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于PCA的不完整点云表面重建新方法，通过估计表面法线信息作为正则化项，有效解决了数据缺失区域的表面重建问题，并在实验中显示出优于现有方法的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud data represents a crucial category of information formathematical modeling, and surface reconstruction from such data is animportant task across various disciplines. However, during the scanningprocess, the collected point cloud data may fail to cover the entire surfacedue to factors such as high light-absorption rate and occlusions, resulting inincomplete datasets. Inferring surface structures in data-missing regions andsuccessfully reconstructing the surface poses a challenge. In this paper, wepresent a Principal Component Analysis (PCA) based model for surfacereconstruction from incomplete point cloud data. Initially, we employ PCA toestimate the normal information of the underlying surface from the availablepoint cloud data. This estimated normal information serves as a regularizer inour model, guiding the reconstruction of the surface, particularly in areaswith missing data. Additionally, we introduce an operator-splitting method toeffectively solve the proposed model. Through systematic experimentation, wedemonstrate that our model successfully infers surface structures indata-missing regions and well reconstructs the underlying surfaces,outperforming existing methodologies.</description>
      <author>example@mail.com (Hao Liu)</author>
      <guid isPermaLink="false">2509.15675v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Bench-RNR: Dataset for Benchmarking Repetitive and Non-repetitive Scanning LiDAR for Infrastructure-based Vehicle Localization</title>
      <link>http://arxiv.org/abs/2509.15583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个基于基础设施的车辆定位数据集，比较了重复和非重复扫描激光雷达的性能，为选择最适合的激光雷达扫描模式提供了见解。&lt;h4&gt;背景&lt;/h4&gt;现有研究大多依赖重复扫描激光雷达，而非重复扫描激光雷达虽有消除盲区和成本效益更高的优势，但在路边感知和定位中的应用仍有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于基础设施的车辆定位数据集，以基准不同激光雷达扫描模式的性能。&lt;h4&gt;方法&lt;/h4&gt;收集了来自重复和非重复扫描激光雷达的数据集，包含5,445帧点云数据，涵盖八种车辆轨迹序列，轨迹类型多样。&lt;h4&gt;主要发现&lt;/h4&gt;建立了基于基础设施的车辆定位基线，比较了使用非重复和重复扫描激光雷达的方法性能，为选择最适合的激光雷达扫描模式提供了有价值的见解。&lt;h4&gt;结论&lt;/h4&gt;该数据集对科学界有重要贡献，支持基于基础设施的感知和车辆定位的发展。数据集和源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;使用路边激光雷达的车辆定位可以为云端控制的车辆提供厘米级精度，同时为多辆车辆提供服务，提高安全性和效率。虽然大多数现有研究依赖重复扫描激光雷达，但非重复扫描激光雷达具有消除盲区和成本效益更高的优势。然而，其在路边感知和定位中的应用仍然有限。为此，我们提出了一个基于基础设施的车辆定位数据集，其中包含来自重复和非重复扫描激光雷达的数据，以便基准不同激光雷达扫描模式的性能。该数据集包含5,445帧点云数据，涵盖八种车辆轨迹序列，轨迹类型多样。我们的实验建立了基于基础设施的车辆定位基线，并使用非重复和重复扫描激光雷达比较了这些方法的性能。这项工作为选择最适合基于基础设施的车辆定位的激光雷达扫描模式提供了有价值的见解。我们的数据集对科学界有重要贡献，支持基于基础设施的感知和车辆定位的发展。数据集和源代码可在以下公开获取：https://github.com/sjtu-cyberc3/BenchRNR。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle localization using roadside LiDARs can provide centimeter-levelaccuracy for cloud-controlled vehicles while simultaneously serving multiplevehicles, enhanc-ing safety and efficiency. While most existing studies rely onrepetitive scanning LiDARs, non-repetitive scanning LiDAR offers advantagessuch as eliminating blind zones and being more cost-effective. However, itsapplication in roadside perception and localization remains limited. To addressthis, we present a dataset for infrastructure-based vehicle localization, withdata collected from both repetitive and non-repetitive scanning LiDARs, inorder to benchmark the performance of different LiDAR scanning patterns. Thedataset contains 5,445 frames of point clouds across eight vehicle trajectorysequences, with diverse trajectory types. Our experiments establish base-linesfor infrastructure-based vehicle localization and compare the performance ofthese methods using both non-repetitive and repetitive scanning LiDARs. Thiswork offers valuable insights for selecting the most suitable LiDAR scanningpattern for infrastruc-ture-based vehicle localization. Our dataset is asignifi-cant contribution to the scientific community, supporting advancementsin infrastructure-based perception and vehicle localization. The dataset andsource code are publicly available at:https://github.com/sjtu-cyberc3/BenchRNR.</description>
      <author>example@mail.com (Runxin Zhao, Chunxiang Wang, Hanyang Zhuang, Ming Yang)</author>
      <guid isPermaLink="false">2509.15583v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Distribution Estimation for Global Data Association via Approximate Bayesian Inference</title>
      <link>http://arxiv.org/abs/2509.15565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种利用近似贝叶斯推理的数据关联框架，能够捕捉数据关联问题的多种解决方案模式，避免在模糊情况下过早承诺单一解决方案，并通过粒子演化来覆盖解决方案分布的模式。&lt;h4&gt;背景&lt;/h4&gt;全局数据关联是机器人在不同时间或不同机器人看到的环境中操作的基本前提。现有方法通常依赖最大似然估计或最大共识来产生单一组关联，但在存在重复或对称数据时会面临重大挑战。在模糊场景中，全局数据关联问题的解决方案分布通常是多模态的，单一解决方案的方法往往会失败。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉数据关联问题多种解决方案模式的数据关联框架，避免在模糊情况下过早承诺单一解决方案，并能正确估计变换分布。&lt;h4&gt;方法&lt;/h4&gt;引入利用近似贝叶斯推理的数据关联框架，将假设解决方案表示为粒子，这些粒子根据确定性或随机更新规则演化，以覆盖底层解决方案分布的模式。该方法可以整合数据关联公式施加的优化约束，并直接受益于GPU并行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在高度模糊数据的模拟和真实世界实验中，该方法在点云或对象地图配准时能够正确估计变换分布。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据关联框架能够有效处理模糊场景中的全局数据关联问题，通过捕捉多种解决方案模式，避免了单一解决方案方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;全局数据关联是机器人在不同时间或不同机器人看到的环境中操作的基本前提。重复或对称数据为现有方法带来了重大挑战，这些方法通常依赖于最大似然估计或最大共识来产生一组关联。然而，在模糊场景中，全局数据关联问题的解决方案分布通常是高度多模态的，这类单一解决方案的方法经常失败。在这项工作中，我们引入了一个利用近似贝叶斯推理的数据关联框架，以捕捉数据关联问题的多种解决方案模式，从而避免在模糊情况下过早承诺单一解决方案。我们的方法将假设解决方案表示为粒子，这些粒子根据确定性或随机更新规则演化，以覆盖底层解决方案分布的模式。此外，我们展示了我们的方法可以整合数据关联公式施加的优化约束，并直接受益于GPU并行优化。在高度模糊数据的广泛模拟和真实世界实验中，我们的方法在配准点云或对象地图时正确估计了变换分布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决全局数据关联问题中的多模态分布估计问题。当机器人面对重复或对称结构的环境（如楼梯、排列整齐的树木、圆形房间等）时，传统方法往往只能找到单一的最优解，而实际上可能存在多个合理的解。这个问题很重要，因为对称或重复结构在人类环境中很常见，机器人需要在这样的环境中准确定位和构建地图；传统单一解方法在模糊场景下容易失败，可能导致机器人做出错误决策（如楼梯案例中机器人可能认为自己实际位置的两层之上）；正确估计解的分布可以帮助机器人理解定位的不确定性，做出更鲁棒的决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将数据关联问题从传统的优化问题重新表述为概率推理问题，借鉴了两种成熟的贝叶斯推理框架：Stein变分梯度下降(SVGD)和朗之万动力学(Langevin dynamics)。他们受到CLIPPER算法的启发，但扩展了它以处理多模态解；借鉴了Stein ICP和Bayesian ICP将贝叶斯推理应用于点云配准的思想；参考了多假设跟踪(MHT)和概率数据关联(PDA)等方法，但指出了它们在处理全局模糊性时的局限性。作者设计算法时考虑了使用粒子系统表示多个可能的解，设计特定更新规则探索解空间，确保计算效率以利用GPU并行计算，并添加正则化技术防止粒子退化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将数据关联问题视为概率推理问题而非传统优化问题，假设解遵循可能有多个峰值（多模态）的概率分布，通过近似贝叶斯推理估计完整分布而非单一最优解。整体流程：1)初始化粒子集合，每个粒子代表一个可能解；2)将问题转化为从特定概率分布中采样；3)更新粒子使其向高概率区域移动；4)从粒子中提取解。Stein CLIPPER使用同伦方法逐渐增加参数d，计算确定性更新方向；Langevin CLIPPER直接设置d=n，添加随机噪声进行更新。两者都使用AdaGrad调整学习率，投影粒子回可行域，并利用GPU并行计算提高效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出多模态分布估计框架，估计完整解分布而非单一最优解；2)将近似贝叶斯推理技术应用于数据关联问题；3)提出两种互补算法（Stein CLIPPER和Langevin CLIPPER）处理不同类型分布；4)设计高效实现，利用GPU并行计算；5)提供理论保证，证明解模式对应于一致性图中的最大团。与之前工作的不同：传统单一解方法（如ICP、RANSAC）在模糊场景下失败；多假设跟踪(MHT)计算成本高昂；概率数据关联(PDA)难以处理全局模糊性；Bayesian ICP和Stein ICP主要应用于点云配准而非通用数据关联；原始CLIPPER只能找到单一最优解。本文方法更通用，能处理多种数据类型，在处理高度峰化和均匀分布时表现更好。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将近似贝叶斯推理技术应用于全局数据关联问题，提出了一种能够估计多模态解分布的新方法，使机器人在面对对称或重复结构的环境时能够更鲁棒地进行定位和地图构建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global data association is an essential prerequisite for robot operation inenvironments seen at different times or by different robots. Repetitive orsymmetric data creates significant challenges for existing methods, whichtypically rely on maximum likelihood estimation or maximum consensus to producea single set of associations. However, in ambiguous scenarios, the distributionof solutions to global data association problems is often highly multimodal,and such single-solution approaches frequently fail. In this work, we introducea data association framework that leverages approximate Bayesian inference tocapture multiple solution modes to the data association problem, therebyavoiding premature commitment to a single solution under ambiguity. Ourapproach represents hypothetical solutions as particles that evolve accordingto a deterministic or randomized update rule to cover the modes of theunderlying solution distribution. Furthermore, we show that our method canincorporate optimization constraints imposed by the data associationformulation and directly benefit from GPU-parallelized optimization. Extensivesimulated and real-world experiments with highly ambiguous data show that ourmethod correctly estimates the distribution over transformations whenregistering point clouds or object maps.</description>
      <author>example@mail.com (Yixuan Jia, Mason B. Peterson, Qingyuan Li, Yulun Tian, Jonathan P. How)</author>
      <guid isPermaLink="false">2509.15565v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>STARC: See-Through-Wall Augmented Reality Framework for Human-Robot Collaboration in Emergency Response</title>
      <link>http://arxiv.org/abs/2509.15507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STARC是一个用于人机协作的增强现实框架，通过融合移动机器人映射和救援人员携带的激光雷达传感，在紧急救援任务中实现'透视'效果，帮助救援人员识别被遮挡的危险和受害者。&lt;h4&gt;背景&lt;/h4&gt;在紧急救援任务中，救援人员需要在被遮挡的室内环境中导航，这些遮挡物阻挡了直接视线，隐藏了危及生命的危险和需要救援的受害者。&lt;h4&gt;目的&lt;/h4&gt;开发一个增强现实框架，为救援人员提供实时可视化隐藏人员和危险的能力，提高态势感知并降低操作风险。&lt;h4&gt;方法&lt;/h4&gt;地面机器人使用激光雷达惯性里程计进行大面积探索和3D人体检测，救援人员的头盔或手持激光雷达通过相对姿态估计与机器人全局地图配准，实现跨激光雷达对齐，将检测到的人和点云以低延迟渲染在AR中投射到救援人员视野。&lt;h4&gt;主要发现&lt;/h4&gt;STARC系统能够提供隐藏人员和危险的实时可视化，提高救援人员的态势感知能力并降低操作风险。&lt;h4&gt;结论&lt;/h4&gt;模拟、实验室设置和战术实地试验证实了STARC系统姿态对齐的鲁棒性、检测的可靠性和叠加的稳定性，表明该系统在消防、救灾和其他安全关键操作中具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在紧急救援任务中，救援人员必须在被遮挡的室内环境中导航，这些遮挡物阻挡了直接视线，隐藏了危及生命的危险和需要救援的受害者。我们提出了STARC，这是一个用于人机协作的透视AR框架，它融合了移动机器人映射和救援人员携带的激光雷达传感。运行激光雷达惯性里程计的地面机器人进行大面积探索和3D人体检测，而救援人员头盔或手持的激光雷达通过相对姿态估计与机器人的全局地图配准。这种跨激光雷达对齐使得检测到的人和他们的点云能够以低延迟渲染在AR中，并投射到救援人员的视野中。通过提供隐藏人员和危险的实时可视化，STARC提高了态势感知能力并降低了操作风险。在模拟、实验室设置和战术实地试验中，实验证实了姿态对齐的鲁棒性、检测的可靠性和叠加的稳定性，强调了我们的系统在消防、救灾和其他安全关键操作中的潜力。代码和设计将在接受后开源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决应急响应任务中视线被遮挡的问题，使救援人员能够'看穿'墙壁和其他障碍物，发现隐藏的危险和需要救援的受害者。这个问题在现实中非常重要，因为在火灾、建筑倒塌、人质环境等紧急情况下，救援人员需要在视线受阻的环境中工作，面临极大风险。现有方法如远程操作机器人只能提供2D视频流，缺乏几何上下文；无线或雷达穿墙方法在不同环境中鲁棒性差；AR/VR接口无法提供实时遮挡感知定位；依赖重型模型的AI不适合时间关键任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在分析现有工作基础上进行了创新：分析了机器人安全关键侦察、LiDAR-惯性里程计和跨视图配准、人类检测和人机交互等领域的工作；设计采用双LIO架构，机器人运行FAST-LIO2维护全局地图，FPV设备运行独立LIO；通过一次性跨LiDAR配准实现3D人类检测投影到AR视图；借鉴了FAST-LIO2进行状态估计、ICP/NDT进行配准、PointPillars进行检测等现有技术，但将它们集成到一个新框架中，专注于验证核心概念。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过融合移动机器人地图与救援人员携带的LiDAR传感，创建'透视墙'的增强现实框架，使救援人员在视线被遮挡时能看到隐藏的人和危险。整体流程包括：1)系统初始化：建立全局时间参考、定义世界坐标系、双LIO启动、跨LiDAR配准；2)机器人状态估计：运行FAST-LIO2维护地图、进行人类检测、定期姿态校正；3)人类点云检测和AR投影：从点云检测人类、投影到FPV视角、在AR中渲染这些点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)操作员-机器人AR框架：首次融合机器人地图与操作员LiDAR传感，实现透视AR叠加；2)任务驱动的跨视图集成：将跨LiDAR配准作为锚定操作员视角的手段；3)跨任务场景验证：建立多阶段评估平台证明系统可行性。与之前工作不同：STARC强调语义而非仅几何、实现了机器人地图与操作员传感器的对齐、提供实时融合的3D提示、首次结合操作员LiDAR、机器人全局映射和跨LiDAR配准的AR可视化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; STARC是一种创新的增强现实框架，通过融合机器人地图与救援人员携带的LiDAR传感，实现了在视线被遮挡的安全关键环境中实时'透视墙'感知，显著提高了救援人员的态势感知能力并降低了操作风险。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In emergency response missions, first responders must navigate clutteredindoor environments where occlusions block direct line-of-sight, concealingboth life-threatening hazards and victims in need of rescue. We present STARC,a see-through AR framework for human-robot collaboration that fusesmobile-robot mapping with responder-mounted LiDAR sensing. A ground robotrunning LiDAR-inertial odometry performs large-area exploration and 3D humandetection, while helmet- or handheld-mounted LiDAR on the responder isregistered to the robot's global map via relative pose estimation. Thiscross-LiDAR alignment enables consistent first-person projection of detectedhumans and their point clouds - rendered in AR with low latency - into theresponder's view. By providing real-time visualization of hidden occupants andhazards, STARC enhances situational awareness and reduces operator risk.Experiments in simulation, lab setups, and tactical field trials confirm robustpose alignment, reliable detections, and stable overlays, underscoring thepotential of our system for fire-fighting, disaster relief, and othersafety-critical operations. Code and design will be open-sourced uponacceptance.</description>
      <author>example@mail.com (Shenghai Yuan, Weixiang Guo, Tianxin Hu, Yu Yang, Jinyu Chen, Rui Qian, Zhongyuan Liu, Lihua Xie)</author>
      <guid isPermaLink="false">2509.15507v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Machine-Learning Potentials for Efficient Simulations of Anisotropic Colloids</title>
      <link>http://arxiv.org/abs/2509.15504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入并评估了基于描述符和端到端模型来预测非球形胶体粒子相互作用能量和力的方法，发现神经进化势(NEP)在准确性和计算效率间达到最佳平衡，能够准确模拟各种复杂形状粒子的行为，并实现显著的计算加速。&lt;h4&gt;背景&lt;/h4&gt;模拟非球形胶体粒子之间的相互作用具有计算挑战性，因为力和能量对其几何形状有复杂的依赖关系。&lt;h4&gt;目的&lt;/h4&gt;引入并评估基于描述符和端到端模型来预测相互作用能量和力的方法，以解决非球形胶体粒子模拟的计算挑战。&lt;h4&gt;方法&lt;/h4&gt;比较了多种描述符与回归模型的组合，包括Behler-Parinello描述符、原子位置平滑重叠(SOAP)和神经进化势(NEP)，以及多种端到端模型，如SchNet、DimeNet和DimeNet++。NEP使用点云表示各向异性刚体间的相互作用。&lt;h4&gt;主要发现&lt;/h4&gt;神经进化势(NEP)在准确性和计算效率间提供了最佳平衡。使用NEP的分子动力学模拟准确重现了立方体、四面体、五角双锥和扭曲圆柱体等多种粒子形状的结构特性，同时比其他方法实现了高达一个数量级的计算加速。该方法还能灵活扩展到具有不同表面相互作用的复杂形状。&lt;h4&gt;结论&lt;/h4&gt;该方法能够实现复杂胶体系统的可扩展模拟，并可能有助于未来对形状依赖相互作用和相行为的高效研究。&lt;h4&gt;翻译&lt;/h4&gt;模拟非球形胶体粒子之间的相互作用在计算上具有挑战性，因为力和能量对其几何形状存在复杂的依赖关系。我们引入并评估了基于描述符和端到端模型来预测相互作用能量和力的方法。然后，我们比较了各种描述符与不同回归模型的组合，如Behler-Parinello描述符、原子位置平滑重叠和神经进化势，以及多种端到端模型，即SchNet、DimeNet和DimeNet++。在这些方法中，神经进化势(NEP)在准确性和计算效率之间提供了最佳平衡。NEP最初是为原子系统开发的，它使用点云表示各向异性刚体之间的相互作用，能够表示任意形状。使用NEP的分子动力学模拟准确重现了多种粒子形状的结构特性，包括立方体、四面体、五角双锥和扭曲圆柱体，同时比其他方法实现了高达一个数量级的加速。此外，我们展示了该方法扩展到具有不同表面相互作用的多个面形状是直接的。我们使用了一个没有任何点群对称性的扭曲圆柱体，来证明NEP的灵活性和准确性。我们的方法能够实现复杂胶体系统的可扩展模拟，并可能有助于未来对形状依赖相互作用和相行为的高效研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating interactions between non-spherical colloidal particles iscomputationally challenging due to the complex dependency of forces andenergies on their geometry. We introduce and evaluate both descriptor-based andend-to-end models for predicting interaction energies and forces. Then, wecompare various descriptors coupled with different regression models, likeBehler-Parinello descriptors, Smooth Overlap of Atomic Positions, andneuroevolution potential, as well as multiple end-to-end models, namely SchNet,DimeNet, and DimeNet++. Among these, the neuroevolution potential (NEP) offersan optimal balance between accuracy and computational efficiency. NEP,originally developed for atomistic systems, represents interactions betweenrigid anisotropic bodies using point clouds, which enables the representationof any arbitrary shape. Molecular dynamics simulations using NEP, accuratelyreproduced structural properties across diverse particle shapes includingcubes, tetrahedra, pentagonal bipyramids, and twisted cylinders, whileachieving roughly up to an order-of-magnitude speedup over other methods.Additionally, we show that the extension of the method to multi-face shapeswith different interactions on their surface is straightforward. We used atwisted cylinder, which lacked any point group symmetry, to demonstrate theflexibility and accuracy of NEP. Our approach enables scalable simulations ofcomplex colloidal systems and can potentially help to facilitate efficientstudies on shape dependent interactions and phase behavior in the future.</description>
      <author>example@mail.com (B. Rusen Argun, Antonia Statt)</author>
      <guid isPermaLink="false">2509.15504v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction</title>
      <link>http://arxiv.org/abs/2509.15459v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为CAGE（连续感知边缘）的鲁棒网络框架，用于从点云密度图中直接重建矢量平面图。该方法采用以边缘为中心的表示方式，能够生成连贯、拓扑有效的房间边界，并在多个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;传统的基于角落的多边形表示方法对噪声和不完整观测非常敏感，常导致碎片化或不合理的布局。最近的线分组方法虽然利用结构提示提高了鲁棒性，但在恢复精细几何细节方面仍有困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接从点云密度图中重建矢量平面图的鲁棒框架，解决传统方法对噪声敏感和无法恢复精细几何细节的问题，确保生成连贯、拓扑有效的房间边界。&lt;h4&gt;方法&lt;/h4&gt;提出一种原生的以边缘为中心的公式，将每个墙段建模为有向的、几何连续的边缘。开发了一个双查询Transformer解码器，在去噪框架中集成扰动查询和潜在查询，以稳定优化并加速收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在Structured3D和SceneCAD数据集上的实验表明，CAGE取得了最先进的性能，房间F1分数达到99.1%，角落为91.7%，角度为89.3%。该方法还表现出强大的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CAGE网络通过以边缘为中心的表示方法和创新的解码器设计，显著提高了平面图重建的鲁棒性和准确性，能够生成连贯、拓扑有效的房间边界，并在多个评估指标上超越了现有方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了CAGE（连续感知边缘）网络，一个用于从点云密度图中直接重建矢量平面图的鲁棒框架。传统的基于角落的多边形表示对噪声和不完整观测非常敏感，常常导致碎片化或不合理的布局。最近的线分组方法利用结构提示提高鲁棒性，但在恢复精细几何细节方面仍然困难。为解决这些限制，我们提出了一种原生的以边缘为中心的公式，将每个墙段建模为有向的、几何连续的边缘。这种表示能够推断出连贯的平面图结构，确保无缝、拓扑有效的房间边界，同时提高鲁棒性并减少伪影。为此设计，我们开发了一个双查询Transformer解码器，在去噪框架中集成扰动查询和潜在查询，这不仅稳定了优化，还加速了收敛。在Structured3D和SceneCAD上的广泛实验表明，CAGE取得了最先进的性能，房间F1分数为99.1%，角落为91.7%，角度为89.3%。该方法还表现出强大的跨数据集泛化能力，证明了我们架构创新的有效性。代码和预训练模型将在接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从点云密度图中重建矢量平面图的问题。这个问题很重要，因为矢量平面图是室内结构的紧凑、可编辑2D表示，与CAD工具和BIM无缝集成，支持精确几何推理，对建筑生命周期管理、AR/VR模拟和自主导航等下游应用至关重要。传统方法对噪声和不完整数据敏感，难以产生连贯、拓扑有效的布局。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：基于角点的方法对噪声敏感，一个缺失角点会扭曲整个布局；SLIBO-Net依赖曼哈顿世界假设；FRI-Net会导致精细结构过度平滑。基于此，作者设计了基于边缘的表示方法，借鉴了DETR和DN-DETR的transformer架构，创造性地引入双查询机制（扰动查询和潜在查询），在去噪框架内稳定训练并加速收敛。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用基于边缘的表示方法，将每面墙建模为有向的、几何连续的边缘，并设计双查询transformer解码器。整体流程：1)点云投影为2D密度图；2)通过卷积骨干网络提取多尺度特征；3)特征展平并添加位置编码；4)transformer编码器处理特征；5)双查询解码器迭代细化边缘预测；6)通过边缘相交转换为闭合多边形。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)连续感知边缘表示，增强对不完整和噪声数据的鲁棒性；2)双查询transformer解码器，稳定训练并加速收敛；3)强大的跨数据集泛化能力。相比之前工作：不依赖精确角点定位（不同于HEAT、RoomFormer）；不依赖曼哈顿世界假设（不同于SLIBO-Net）；避免精细结构过度平滑（不同于FRI-Net）；更好地平衡全局结构和局部精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CAGE网络通过连续感知的边缘表示和双查询transformer解码器，实现了从点云密度图中鲁棒且精确的矢量平面图重建，显著提高了在噪声和不完整数据条件下的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present \textbf{CAGE} (\textit{Continuity-Aware edGE}) network, a\textcolor{red}{robust} framework for reconstructing vector floorplans directlyfrom point-cloud density maps. Traditional corner-based polygon representationsare highly sensitive to noise and incomplete observations, often resulting infragmented or implausible layouts. Recent line grouping methods leveragestructural cues to improve robustness but still struggle to recover finegeometric details. To address these limitations, we propose a \textit{native}edge-centric formulation, modeling each wall segment as a directed,geometrically continuous edge. This representation enables inference ofcoherent floorplan structures, ensuring watertight, topologically valid roomboundaries while improving robustness and reducing artifacts. Towards thisdesign, we develop a dual-query transformer decoder that integrates perturbedand latent queries within a denoising framework, which not only stabilizesoptimization but also accelerates convergence. Extensive experiments onStructured3D and SceneCAD show that \textbf{CAGE} achieves state-of-the-artperformance, with F1 scores of 99.1\% (rooms), 91.7\% (corners), and 89.3\%(angles). The method also demonstrates strong cross-dataset generalization,underscoring the efficacy of our architectural innovations. Code and pretrainedmodels will be released upon acceptance.</description>
      <author>example@mail.com (Yiyi Liu, Chunyang Liu, Weiqin Jiao, Bojian Wu, Fashuai Li, Biao Xiong)</author>
      <guid isPermaLink="false">2509.15459v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Measurement and Potential Field-Based Patient Modeling for Model-Mediated Tele-ultrasound</title>
      <link>http://arxiv.org/abs/2509.15325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进的远程超声操作方法，通过使用测量的位置和力来更新患者的内部势场模型，以提高远程超声诊断的准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;远程超声操作可以改善偏远社区的医学诊断成像获取。准确的力反馈对于操作者（超声医师）应用适当的探头接触力以优化超声图像质量非常重要。然而，通信中的大延迟使得直接力反馈不切实际。&lt;h4&gt;目的&lt;/h4&gt;扩展先前基于点云的模型中介远程操作和内部势场模型研究，引入一种使用测量的位置和力更新患者内部势场模型的方法，以实现更透明的模型中介远程超声操作。&lt;h4&gt;方法&lt;/h4&gt;首先生成患者表面的点云模型，并以紧凑的数据结构传输给超声医师。然后将其转换为静态体素化体积，每个体素包含势场值。这些值决定力和力矩，基于体素化体积与超声换能器的点壳模型之间的重叠来渲染。使用结合空间拉普拉斯算子和测量力的凸二次方程求解势场。在3名志愿者患者上评估了渲染力的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;与仅使用拉普拉斯方程相比，将测量力添加到模型中使力大小误差平均减少了7.23牛顿，力矢量角度误差平均减少了9.37度。&lt;h4&gt;结论&lt;/h4&gt;通过整合测量的位置和力来更新内部势场模型的方法，显著提高了远程超声操作中力反馈的准确性，有助于优化远程超声诊断的质量。&lt;h4&gt;翻译&lt;/h4&gt;远程超声操作可以提高偏远社区获取诊断医学成像的能力。准确的力反馈对于使超声医师能够应用适当的探头接触力以优化超声图像质量非常重要。然而，通信中的大延迟使得直接力反馈不切实际。先前的研究调查了使用基于点云的模型中介远程操作和内部势场模型来估计接触力和力矩。我们通过引入一种使用测量的位置和力更新患者内部势场模型的方法来扩展这一研究，以实现更透明的模型中介远程超声操作。我们首先生成患者表面的点云模型，并以紧凑的数据结构将其传输给超声医师。这被转换为静态体素化体积，其中每个体素包含势场值。这些值确定力和力矩，这些力和力矩基于体素化体积与超声换能器的点壳模型之间的重叠来渲染。我们使用结合空间拉普拉斯算子和测量力的凸二次方程来求解势场。通过计算渲染力的准确性，在3名志愿者患者上对此进行了评估。结果表明，与仅使用拉普拉斯方程相比，将测量力添加到模型中使力大小误差平均减少了7.23牛顿，力矢量角度误差平均减少了9.37度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决远程超声检查中由于通信延迟导致的力反馈不准确问题。在远程超声中，医生需要感受到探头与患者身体的接触力来优化图像质量，但网络延迟使得直接力反馈变得困难。这个问题很重要，因为它关系到远程医疗服务的质量和可及性，特别是对医疗资源匮乏的偏远地区患者而言，能让他们在当地获得专业诊断服务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了远程超声中力反馈的挑战，考察了现有的模型中介遥操作方法。他们借鉴了点云模型和势场模型的思想，特别是Voxmap Pointshell算法和压力场接触方法。作者发现这些方法虽能渲染力反馈，但不能很好地模拟患者阻抗特性。因此，他们设计了一种结合拉普拉斯方程和实际测量力的方法，通过将测量的位置和力数据整合到势场模型中，提高模型的准确性和透明度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合物理模型和实际测量数据创建更准确的虚拟患者模型，用于远程超声中的力反馈。具体流程：1)用深度相机获取患者躯干点云数据；2)转换为圆柱坐标系结构化表示；3)创建静态体素化体积，每个体素含势场值；4)用拉普拉斯方程初始化势场；5)通过实际测量的力和位置数据更新势场；6)当探头与模型交互时计算力和力矩；7)将计算结果反馈给远程医生。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)引入用测量的位置和力更新患者内部势场模型的方法；2)结合拉普拉斯方程和实际测量数据创建更准确的阻抗模型；3)实现更透明的模型中介远程超声系统。相比之前工作：1)之前方法主要依赖预定义物理模型，未结合实际测量数据；2)之前阻抗估计多适用于单自由度系统，本文方法可处理复杂三维情况；3)本文能捕捉患者身体不同部位阻抗变化，而之前模型通常假设阻抗均匀。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合物理模型和实际测量数据的远程超声患者建模方法，通过更新内部势场模型显著提高了力反馈准确性，为偏远地区提供了更高质量的远程超声诊断服务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Teleoperated ultrasound can improve diagnostic medical imaging access forremote communities. Having accurate force feedback is important for enablingsonographers to apply the appropriate probe contact force to optimizeultrasound image quality. However, large time delays in communication makedirect force feedback impractical. Prior work investigated using pointcloud-based model-mediated teleoperation and internal potential field models toestimate contact forces and torques. We expand on this by introducing a methodto update the internal potential field model of the patient with measuredpositions and forces for more transparent model-mediated tele-ultrasound. Wefirst generate a point cloud model of the patient's surface and transmit thisto the sonographer in a compact data structure. This is converted to a staticvoxelized volume where each voxel contains a potential field value. Thesevalues determine the forces and torques, which are rendered based on overlapbetween the voxelized volume and a point shell model of the ultrasoundtransducer. We solve for the potential field using a convex quadratic thatcombines the spatial Laplace operator with measured forces. This was evaluatedon volunteer patients ($n=3$) by computing the accuracy of rendered forces.Results showed the addition of measured forces to the model reduced the forcemagnitude error by an average of 7.23 N and force vector angle error by anaverage of 9.37$^{\circ}$ compared to using only Laplace's equation.</description>
      <author>example@mail.com (Ryan S. Yeung, David G. Black, Septimiu E. Salcudean)</author>
      <guid isPermaLink="false">2509.15325v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2509.15123v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 Spotlight&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ROS-Cam的新方法，用于在动态场景中更准确、更高效地优化相机参数，仅使用单个RGB视频作为监督。&lt;h4&gt;背景&lt;/h4&gt;COLMAP是静态场景中相机参数优化的主要方法，但存在运行时间长且依赖真实运动掩码的问题。许多改进方法需要先验知识，但这些在普通RGB视频中通常不可用。&lt;h4&gt;目的&lt;/h4&gt;提出一种在动态场景中更准确、更高效的相机参数优化方法，仅使用单个RGB视频作为监督。&lt;h4&gt;方法&lt;/h4&gt;ROS-Cam方法包含三个关键组件：(1)基于块的运动跟踪滤波器，建立RGB视频间稳健且最大稀疏的铰链式关系；(2)异常感知联合优化，通过自适应降低运动异常权重来高效优化相机参数，不依赖运动先验；(3)两阶段优化策略，通过在Softplus限制和凸最小值之间权衡提高稳定性和优化速度。&lt;h4&gt;主要发现&lt;/h4&gt;在4个真实数据集(NeRF-DS, DAVIS, iPhone, 和 TUM-dynamics)和1个合成数据集(MPI-Sintel)上的实验表明，ROS-Cam方法仅使用单个RGB视频作为监督，能够更高效、更准确地估计相机参数。通过将相机估计输入4D重建方法评估结果3D场景、渲染的2D RGB和深度图，进一步验证了准确性。&lt;h4&gt;结论&lt;/h4&gt;ROS-Cam方法在动态场景中能够更准确、更高效地优化相机参数，仅需单个RGB视频作为监督，无需额外的先验知识。&lt;h4&gt;翻译&lt;/h4&gt;虽然COLMAP长期以来一直是静态场景中相机参数优化的主要方法，但它受限于较长的运行时间以及对真实运动掩码的依赖，无法应用于动态场景。许多尝试通过融入更多先验作为监督来改进它，如真实焦距、运动掩码、3D点云、相机姿态和度量深度，但这些在普通拍摄的RGB视频中通常不可用。在本文中，我们提出了一种新颖的方法，用于在动态场景中进行更准确、更高效的相机参数优化，仅由单个RGB视频监督，称为ROS-Cam。我们的方法包含三个关键组件：(1)基于块的运动跟踪滤波器，在RGB视频间建立稳健且最大稀疏的铰链式关系；(2)异常感知联合优化，通过自适应降低运动异常的权重来高效优化相机参数，不依赖运动先验；(3)两阶段优化策略，通过在损失函数中的Softplus限制和凸最小值之间权衡来提高稳定性和优化速度。我们通过可视化和数值方式评估了相机估计。为进一步验证准确性，我们将相机估计输入4D重建方法并评估生成的3D场景、渲染的2D RGB和深度图。我们在4个真实数据集(NeRF-DS, DAVIS, iPhone, 和 TUM-dynamics)和1个合成数据集(MPI-Sintel)上进行了实验，证明我们的方法仅使用单个RGB视频作为监督，能够更高效、更准确地估计相机参数。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在动态场景中仅使用RGB视频（无需额外监督信息如运动掩码、焦距、3D点云等）进行相机参数优化的问题。这个问题在现实中很重要，因为大多数普通用户设备只能提供RGB视频，而现有方法要么需要额外的先验信息（通常不可用），要么在动态场景中运行时间长且性能不佳。准确估计相机参数对动态场景重建、新视角合成等计算机视觉应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法（如COLMAP）在动态场景中的局限性，包括运行时间长和对运动掩码的依赖。他们注意到大多数方法需要额外的先验监督，而这些在日常RGB视频中不可用。作者设计了三个关键组件：基于块的运动跟踪过滤器、离群感知联合优化和两阶段优化策略。作者借鉴了点跟踪模型（如CoTracker）来提取鲁棒的跟踪轨迹作为伪监督，并使用Cauchy分布来建模不确定性，这优于高斯分布，因为它能更好地处理重尾分布。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是仅使用RGB视频作为监督，通过提取鲁棒的跟踪轨迹作为伪监督，然后使用离群感知的联合优化方法来估计相机参数。整体实现流程包括：1) 基于块的运动跟踪过滤器（纹理、梯度、可见性和分布过滤器提取鲁棒轨迹）；2) 离群感知联合优化（计算平均累积投影误差，使用Cauchy损失函数降低运动离群点影响，联合优化校准点、焦距、旋转、平移和不确定性参数）；3) 两阶段优化策略（第一阶段快速收敛，第二阶段使用第一阶段的误差初始化不确定性参数，稳定收敛）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 仅RGB监督，无需额外先验信息；2) 基于块的运动跟踪过滤器，提取鲁棒轨迹作为伪监督；3) 离群感知联合优化，使用Cauchy分布建模不确定性；4) 两阶段优化策略提高稳定性和效率。相比之前的工作，不同之处在于：不需要额外的先验监督（如运动掩码、焦距、深度等）；通过学习不确定性参数自适应处理运动离群点，而非依赖真实运动掩码；运行时间显著短于现有方法，且随着视频长度增加优势更明显；在各种动态场景中表现更鲁棒，包括低视差视频和快速运动物体。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种名为ROS-Cam的新方法，仅使用RGB视频作为监督，通过基于块的运动跟踪过滤、离群感知联合优化和两阶段优化策略，实现了动态场景中相机参数的准确、高效估计，显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although COLMAP has long remained the predominant method for camera parameteroptimization in static scenes, it is constrained by its lengthy runtime andreliance on ground truth (GT) motion masks for application to dynamic scenes.Many efforts attempted to improve it by incorporating more priors assupervision such as GT focal length, motion masks, 3D point clouds, cameraposes, and metric depth, which, however, are typically unavailable in casuallycaptured RGB videos. In this paper, we propose a novel method for more accurateand efficient camera parameter optimization in dynamic scenes solely supervisedby a single RGB video, dubbed ROS-Cam. Our method consists of three keycomponents: (1) Patch-wise Tracking Filters, to establish robust and maximallysparse hinge-like relations across the RGB video. (2) Outlier-aware JointOptimization, for efficient camera parameter optimization by adaptivedown-weighting of moving outliers, without reliance on motion priors. (3) ATwo-stage Optimization Strategy, to enhance stability and optimization speed bya trade-off between the Softplus limits and convex minima in losses. Wevisually and numerically evaluate our camera estimates. To further validateaccuracy, we feed the camera estimates into a 4D reconstruction method andassess the resulting 3D scenes, and rendered 2D RGB and depth maps. We performexperiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics)and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimatescamera parameters more efficiently and accurately with a single RGB video asthe only supervision.</description>
      <author>example@mail.com (Fang Li, Hao Zhang, Narendra Ahuja)</author>
      <guid isPermaLink="false">2509.15123v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>GenCAD-3D: CAD Program Generation using Multimodal Latent Space Alignment and Synthetic Dataset Balancing</title>
      <link>http://arxiv.org/abs/2509.15246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 figures, 15 pages. Accepted and soon published in the ASME Journal  of Mechanical Design&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GenCAD-3D多模态生成框架和SynthBal合成数据增强策略，解决了从非参数化数据生成CAD程序的挑战，显著提高了重建准确性并减少了无效CAD模型的生成。&lt;h4&gt;背景&lt;/h4&gt;CAD程序作为参数化命令序列，对工程设计的精确性和效率至关重要。但从点云和网格等非参数化数据生成这些程序仍具挑战性，通常需要大量人工干预。当前深度生成模型受限于不平衡且不足的数据集，特别是缺乏复杂CAD程序的表示。&lt;h4&gt;目的&lt;/h4&gt;开发自动化CAD生成方法，解决数据集不平衡和不足的问题，提高复杂CAD几何体的表示和生成质量。&lt;h4&gt;方法&lt;/h4&gt;提出GenCAD-3D多模态生成框架，利用对比学习对齐CAD和几何编码器之间的潜在嵌入，结合潜在扩散模型进行CAD序列生成和检索；同时提出SynthBal合成数据增强策略，专门用于平衡和扩展数据集，增强复杂CAD几何体的表示。&lt;h4&gt;主要发现&lt;/h4&gt;SynthBal显著提高了重建准确性，减少了无效CAD模型的生成，在高复杂度几何体上明显优于现有基准，超越了现有的性能标准。&lt;h4&gt;结论&lt;/h4&gt;这些进展对简化逆向工程和增强工程设计自动化具有重要意义，作者计划公开发布数据集、代码以及一组51个3D打印和激光扫描的零件。&lt;h4&gt;翻译&lt;/h4&gt;CAD程序作为参数化命令序列的结构，编译成精确的3D几何体，对准确和高效的工程设计流程至关重要。从点云和网格等非参数化数据生成这些程序仍然是一个关键但具有挑战性的任务，通常需要大量人工干预。当前旨在自动化CAD生成的深度生成模型受到不平衡且不够大的数据集的显著限制，特别是那些缺乏复杂CAD程序表示的数据集。为解决这一问题，我们引入了GenCAD-3D，一个利用对比学习对齐CAD和几何编码器之间潜在嵌入的多模态生成框架，结合潜在扩散模型进行CAD序列生成和检索。此外，我们提出了SynthBal，一种专门设计用于平衡和扩展数据集的合成数据增强策略，显著增强了复杂CAD几何体的表示。我们的实验表明，SynthBal显著提高了重建准确性，减少了无效CAD模型的生成，并在高复杂度几何体上明显优于现有基准。这些进展对简化逆向工程和增强工程设计自动化具有重要意义。我们将在项目网站上公开发布我们的数据集和代码，包括一组51个3D打印和激光扫描的零件。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从3D几何数据（如点云和网格）自动生成参数化CAD程序的问题。这个问题在工程领域非常重要，因为CAD程序提供了精确的参数化控制，是工程设计和制造的基础。在逆向工程中，工程师经常需要将物理对象的3D扫描转换为可编辑的CAD程序，以便修改和制造零件，尤其是在原始供应商已 discontinued 的情况下。当前工具需要大量人工干预，而生成式AI通常输出非参数化形式，难以编辑和精确细化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前CAD生成面临的数据不平衡挑战，特别是复杂CAD程序在数据集中表示不足的问题。他们借鉴了GenCAD架构的条件潜在扩散模型和对比学习在跨模态任务（如CLIP模型）中的成功经验。设计方法包括：1）多模态对比学习对齐不同模态的潜在表示；2）条件扩散模型实现从几何到CAD的生成；3）专门的3D编码器处理不同几何输入；4）SynthBal策略解决数据不平衡。创新之处在于不依赖现有基础模型，独立学习多模态嵌入，并专门针对CAD程序复杂性进行数据平衡。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态潜在空间对齐将不同几何模态与CAD程序表示对齐到共享空间，利用条件扩散模型实现从几何输入到CAD程序的生成，并通过SynthBal策略平衡数据集中不同复杂度CAD程序的表示。整体流程包括：1）使用因果变换器自编码器学习CAD程序潜在表示；2）为点云和网格设计专门编码器，通过对比损失对齐几何模态与CAD潜在空间；3）训练条件扩散模型将几何潜在表示映射到CAD程序潜在表示；4）通过SynthBal生成合成数据平衡不同复杂度CAD程序的表示；5）在合成数据上训练自编码器，然后在减少平衡的真实数据上微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）GenCAD-3D框架结合多模态潜在空间对齐和条件扩散模型；2）专门的网格编码器相比点云基线在命令准确性上有15%相对提升；3）SynthBal合成数据策略将无效CAD生成率从3.44%降至0.845%；4）复杂度归一化评估指标确保高复杂度设计得到公平评估；5）发布多模态数据集和编码器促进研究。相比之前工作，GenCAD-3D支持多种3D几何模态输入，专门解决数据不平衡问题，设计了专门的网格编码器，引入了新的评估方法，且不依赖现有基础模型独立学习多模态嵌入。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GenCAD-3D通过多模态潜在空间对齐和合成数据平衡策略，实现了从3D几何数据高效生成复杂CAD程序，显著提高了逆向工程和工程设计自动化的准确性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1115/1.4069276&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CAD programs, structured as parametric sequences of commands that compileinto precise 3D geometries, are fundamental to accurate and efficientengineering design processes. Generating these programs from nonparametric datasuch as point clouds and meshes remains a crucial yet challenging task,typically requiring extensive manual intervention. Current deep generativemodels aimed at automating CAD generation are significantly limited byimbalanced and insufficiently large datasets, particularly those lackingrepresentation for complex CAD programs. To address this, we introduceGenCAD-3D, a multimodal generative framework utilizing contrastive learning foraligning latent embeddings between CAD and geometric encoders, combined withlatent diffusion models for CAD sequence generation and retrieval.Additionally, we present SynthBal, a synthetic data augmentation strategyspecifically designed to balance and expand datasets, notably enhancingrepresentation of complex CAD geometries. Our experiments show that SynthBalsignificantly boosts reconstruction accuracy, reduces the generation of invalidCAD models, and markedly improves performance on high-complexity geometries,surpassing existing benchmarks. These advancements hold substantialimplications for streamlining reverse engineering and enhancing automation inengineering design. We will publicly release our datasets and code, including aset of 51 3D-printed and laser-scanned parts on our project site.</description>
      <author>example@mail.com (Nomi Yu, Md Ferdous Alam, A. John Hart, Faez Ahmed)</author>
      <guid isPermaLink="false">2509.15246v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution</title>
      <link>http://arxiv.org/abs/2509.15781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages,2 figures, ICCV Workshop (MOSEv2 Track of 7th LSVOS  Challenge)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种整合Cutie和SAM2优势的视频对象分割框架SCOPE，通过改进编码器和引入运动预测模块，在第七届LSVOS挑战赛的MOSEv2赛道中获得第三名。&lt;h4&gt;背景&lt;/h4&gt;视频对象分割是一项具有挑战性的任务，在视频编辑和自动驾驶等领域有广泛应用。现有方法Cutie提供强大的基于查询的分割能力，SAM2通过预训练的ViT编码器提供丰富的表示，但两者在特征容量和时间建模方面各有局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一个整合Cutie和SAM2互补优势的框架，解决现有方法在特征容量和时间建模方面的局限性，提高视频对象分割的性能。&lt;h4&gt;方法&lt;/h4&gt;用SAM2的ViT编码器替换Cutie的编码器，引入运动预测模块以实现时间稳定性，并采用集成策略结合Cutie、SAM2和提出的变体，创建名为SCOPE（SAM2-CUTIE Object Prediction Ensemble）的最终模型。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在第七届LSVOS挑战赛的MOSEv2赛道中获得第三名，证明了丰富的特征表示和运动预测对鲁棒的视频对象分割的有效性。&lt;h4&gt;结论&lt;/h4&gt;集成不同方法的互补优势可以改进视频对象分割性能，特征表示增强和时间建模对实现稳定的视频对象分割至关重要。&lt;h4&gt;翻译&lt;/h4&gt;视频对象分割是一项具有挑战性的任务，在视频编辑和自动驾驶等领域有广泛应用。虽然Cutie提供了强大的基于查询的分割能力，SAM2通过预训练的ViT编码器提供了丰富的表示，但两者在特征容量和时间建模方面各有局限性。在本报告中，我们提出了一种通过用SAM2的ViT编码器替换Cutie的编码器并引入运动预测模块来实现时间稳定性的框架，从而整合它们的互补优势。我们进一步采用结合Cutie、SAM2和我们提出的变体的集成策略，在第七届LSVOS挑战赛的MOSEv2赛道中获得第三名。我们将最终模型称为SCOPE（SAM2-CUTIE Object Prediction Ensemble）。这证明了丰富的特征表示和运动预测对鲁棒的视频对象分割的有效性。代码可在https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video object segmentation (VOS) is a challenging task with wide applicationssuch as video editing and autonomous driving. While Cutie provides strongquery-based segmentation and SAM2 offers enriched representations via apretrained ViT encoder, each has limitations in feature capacity and temporalmodeling. In this report, we propose a framework that integrates theircomplementary strengths by replacing the encoder of Cutie with the ViT encoderof SAM2 and introducing a motion prediction module for temporal stability. Wefurther adopt an ensemble strategy combining Cutie, SAM2, and our variant,achieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer toour final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). Thisdemonstrates the effectiveness of enriched feature representation and motionprediction for robust video object segmentation. The code is available athttps://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place.</description>
      <author>example@mail.com (Chang Soo Lim, Joonyoung Moon, Donghyeon Cho)</author>
      <guid isPermaLink="false">2509.15781v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>KoopCast: Trajectory Forecasting via Koopman Operators</title>
      <link>http://arxiv.org/abs/2509.15513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;KoopCast是一个轻量级但高效的轨迹预测模型，适用于一般动态环境&lt;h4&gt;背景&lt;/h4&gt;轨迹预测在一般动态环境中面临非线性动力学挑战&lt;h4&gt;目的&lt;/h4&gt;开发一个既能保持高预测准确性，又具有可解释性和低延迟部署的轨迹预测模型&lt;h4&gt;方法&lt;/h4&gt;采用基于Koopman算子理论的两阶段设计：第一阶段使用概率神经目标估计器预测长期目标；第二阶段使用Koopman算子细化模块将意图和历史信息整合到非线性特征空间实现线性预测&lt;h4&gt;主要发现&lt;/h4&gt;KoopCast模型在多个数据集上验证了三个关键优势：具有竞争力的准确性、基于Koopman谱理论的解释性、低延迟部署&lt;h4&gt;结论&lt;/h4&gt;KoopCast在各种基准测试中提供高预测准确性，同时具有模式级别的可解释性和实际效率&lt;h4&gt;翻译&lt;/h4&gt;我们提出了KoopCast，一个轻量级但高效的轨迹预测模型，适用于一般动态环境。我们的方法利用Koopman算子理论，通过将轨迹提升到高维空间，实现对非线性动力学的线性表示。该框架采用两阶段设计：首先，概率神经目标估计器预测可能的长远目标，确定去向；其次，基于Koopman算子的细化模块将意图和历史信息整合到非线性特征空间，实现线性预测，确定如何去。这种双重结构不仅确保了强大的预测准确性，同时保留了线性算子的有利特性，同时真实地捕获了非线性动力学。因此，我们的模型具有三个主要优势：具有竞争力的准确性、基于Koopman谱理论的解释性、低延迟部署。我们在ETH/UCY、Waymo Open Motion Dataset和nuScenes上验证了这些优势，这些数据集具有丰富的多智能体交互和地图约束的非线性运动。在各种基准测试中，KoopCast始终提供高预测准确性，同时具有模式级别的可解释性和实际效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present KoopCast, a lightweight yet efficient model for trajectoryforecasting in general dynamic environments. Our approach leverages Koopmanoperator theory, which enables a linear representation of nonlinear dynamics bylifting trajectories into a higher-dimensional space. The framework follows atwo-stage design: first, a probabilistic neural goal estimator predictsplausible long-term targets, specifying where to go; second, a Koopmanoperator-based refinement module incorporates intention and history into anonlinear feature space, enabling linear prediction that dictates how to go.This dual structure not only ensures strong predictive accuracy but alsoinherits the favorable properties of linear operators while faithfullycapturing nonlinear dynamics. As a result, our model offers three keyadvantages: (i) competitive accuracy, (ii) interpretability grounded in Koopmanspectral theory, and (iii) low-latency deployment. We validate these benefitson ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature richmulti-agent interactions and map-constrained nonlinear motion. Acrossbenchmarks, KoopCast consistently delivers high predictive accuracy togetherwith mode-level interpretability and practical efficiency.</description>
      <author>example@mail.com (Jungjin Lee, Jaeuk Shin, Gihwan Kim, Joonho Han, Insoon Yang)</author>
      <guid isPermaLink="false">2509.15513v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>The Missing Piece: A Case for Pre-Training in 3D Medical Object Detection</title>
      <link>http://arxiv.org/abs/2509.15947v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模预训练对3D医学目标检测的影响，首次系统性探讨了如何将现有预训练方法集成到最先进的检测架构中，发现基于重建的自监督预训练优于监督预训练，而对比预训练无明显益处。&lt;h4&gt;背景&lt;/h4&gt;大规模预训练有望推进3D医学目标检测，这是准确计算机辅助诊断的关键组成部分。然而，与分割相比，3D目标检测的预训练研究仍然不足，现有方法主要依赖2D医学数据或自然图像预训练，未能充分利用3D体积信息。&lt;h4&gt;目的&lt;/h4&gt;进行首次系统性研究，探讨如何将现有预训练方法集成到最先进的检测架构中，涵盖CNN和Transformer两种架构。&lt;h4&gt;方法&lt;/h4&gt;将现有预训练方法集成到CNN和Transformer架构中，评估了基于重建的自监督预训练、监督预训练和对比预训练方法对3D医学目标检测性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;预训练在各种任务和数据集上一致提高了检测性能；基于重建的自监督预训练优于监督预训练；对比预训练对3D医学目标检测没有明显益处。&lt;h4&gt;结论&lt;/h4&gt;预训练可以有效提升3D医学目标检测性能，基于重建的自监督方法是3D医学目标检测的有效预训练策略。&lt;h4&gt;翻译&lt;/h4&gt;大规模预训练有望推进3D医学目标检测，这是准确计算机辅助诊断的关键组成部分。然而，与分割相比，3D目标检测的预训练研究仍然不足，而分割领域的预训练已经显示出显著优势。现有的3D目标检测预训练方法依赖于2D医学数据或自然图像预训练，未能充分利用3D体积信息。在这项工作中，我们首次进行了系统性研究，探讨了如何将现有预训练方法集成到最先进的检测架构中，涵盖了CNN和Transformer两种架构。我们的结果表明，预训练在各种任务和数据集上一致提高了检测性能。值得注意的是，基于重建的自监督预训练优于监督预训练，而对比预训练对3D医学目标检测没有明显益处。我们的代码已在以下公开：https://github.com/MIC-DKFZ/nnDetection-finetuning。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D医学目标检测领域缺乏有效预训练方法的问题。这个问题很重要，因为3D医学目标检测是计算机辅助诊断的关键组成部分，能帮助医生准确定位临床相关物体，特别是在高风险场景中，完全遗漏物体的后果比分割不准确更严重。目前，预训练在分割任务中已显示出优势，但在3D目标检测领域研究不足，限制了模型在小数据集上的性能和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D医学目标检测预训练研究的空白，并借鉴了分割任务中已成功的预训练方法。他们采用了MultiTalent框架进行监督预训练，并使用了四种自监督学习方法（MAE、SparkMAE、VoCo和Models Genesis）。作者设计了全面的实验，评估不同预训练策略对两种检测架构（Retina U-Net和Deformable DETR）的影响，并确保预训练数据与下游任务数据不重叠，避免数据泄露。这种方法结合了现有研究成果并进行了系统性扩展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统性地研究不同预训练策略对3D医学目标检测的影响，找出最优方法，并建立连接检测、分割和自监督学习的统一框架。整体流程包括：1) 预训练阶段 - 使用MultiTalent进行监督预训练，使用CT-RATE和ABCD数据集进行自监督预训练；2) 模型架构选择 - 使用Retina U-Net和Deformable DETR作为检测架构，ResEncL和Retina U-Net作为预训练架构；3) 下游任务微调 - 在8个数据集上进行微调，评估不同策略；4) 性能评估 - 使用mAP和FROC指标，通过bootstrap进行统计分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次系统性研究3D医学目标检测的预训练范式；2) 建立nnDetection与预训练框架间的桥梁，实现跨领域统一方法；3) 全面评估多种预训练策略和架构组合；4) 发现基于重建的自监督预训练优于监督预训练，而对比预训练无明显益处。相比之前工作，本文专注于3D预训练而非2D，提供了更全面的评估，确保实验严谨性，并公开代码促进研究复现和进一步发展。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性地研究不同预训练策略对3D医学目标检测的影响，首次建立了连接检测、分割和自监督学习的统一框架，并发现基于重建的自监督预训练能有效提升3D医学目标检测的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-04965-0_58&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale pre-training holds the promise to advance 3D medical objectdetection, a crucial component of accurate computer-aided diagnosis. Yet, itremains underexplored compared to segmentation, where pre-training has alreadydemonstrated significant benefits. Existing pre-training approaches for 3Dobject detection rely on 2D medical data or natural image pre-training, failingto fully leverage 3D volumetric information. In this work, we present the firstsystematic study of how existing pre-training methods can be integrated intostate-of-the-art detection architectures, covering both CNNs and Transformers.Our results show that pre-training consistently improves detection performanceacross various tasks and datasets. Notably, reconstruction-basedself-supervised pre-training outperforms supervised pre-training, whilecontrastive pre-training provides no clear benefit for 3D medical objectdetection. Our code is publicly available at:https://github.com/MIC-DKFZ/nnDetection-finetuning.</description>
      <author>example@mail.com (Katharina Eckstein, Constantin Ulrich, Michael Baumgartner, Jessica Kächele, Dimitrios Bounias, Tassilo Wald, Ralf Floca, Klaus H. Maier-Hein)</author>
      <guid isPermaLink="false">2509.15947v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Sparse Multiview Open-Vocabulary 3D Detection</title>
      <link>http://arxiv.org/abs/2509.15924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025; OpenSUN3D Workshop; Camera ready version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在稀疏视图条件下进行开放词汇3D物体检测的新方法，无需训练即可利用预训练的2D基础模型，通过提升2D检测结果和优化3D提案实现视图间特征度量一致性，在标准基准测试中建立了强大基线。&lt;h4&gt;背景&lt;/h4&gt;3D场景理解和物体检测对许多视觉和机器人系统至关重要。传统方法仅能检测固定类别的物体，应用受限。在稀疏视图条件下（只有少量姿态RGB图像可用），3D物体检测面临更大挑战。&lt;h4&gt;目的&lt;/h4&gt;研究在稀疏视图条件下的开放词汇3D物体检测，克服传统方法只能检测固定类别物体的限制，并避免计算昂贵的3D特征融合和3D特定学习需求。&lt;h4&gt;方法&lt;/h4&gt;提出一种无需训练的方法，利用预训练的现成2D基础模型，通过提升2D检测结果和直接优化3D提案来实现视图间的特征度量一致性，充分利用2D中丰富的训练数据。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在标准基准测试中建立了强大的基线，在密集采样场景下与最先进技术性能相当，而在稀疏视图设置下显著优于现有技术。&lt;h4&gt;结论&lt;/h4&gt;简单的方法可以有效地解决稀疏视图条件下的开放词汇3D物体检测问题，无需复杂的3D特征融合或特定学习，为3D物体检测提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;解释和理解3D场景的能力对许多视觉和机器人系统至关重要。在众多应用中，这涉及3D物体检测，即识别属于特定类别的物体的位置和尺寸，通常表示为边界框。这传统上通过训练检测固定类别集合来解决，限制了其应用。在这项工作中，我们研究了具有挑战性但实用的稀疏视图设置下的开放词汇3D物体检测，其中只有少量姿态RGB图像可用作为输入。我们的方法无需训练，依赖预训练的现成2D基础模型，而不是采用计算昂贵的3D特征融合或需要3D特定学习。通过提升2D检测结果和直接优化3D提案以实现视图间的特征度量一致性，我们充分利用了2D中可用的丰富训练数据。通过标准基准测试，我们证明这个简单的流程建立了一个强大的基线，在密集采样场景下与最先进技术竞争，同时在稀疏视图设置下显著优于它们。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放词汇3D物体检测在稀疏视图设置下的挑战。传统3D检测只能识别预定义类别，而现有开放词汇方法又依赖密集3D数据或需要昂贵训练。这个问题在实际应用中很重要，因为它使系统能检测任意类别物体无需重新训练，特别适合设施管理、零售监控等只有有限摄像头的场景，避免了重新扫描整个场景的需要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考的核心是利用2D基础模型的强大泛化能力，避免昂贵的3D训练。他们设计了一个'提升'过程：将2D检测结果通过单目深度估计转换为3D提案，然后通过多视图一致性优化来 refine 这些提案。借鉴了现有工作包括：使用OWLv2进行2D开放词汇检测，使用SAM生成精确掩码，利用MoGe进行单目深度估计，以及结合CLIP特征确保跨视图语义一致性，但将这些技术以新颖方式组合应用于3D检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练2D基础模型的强大泛化能力，通过简单'提升'2D检测结果到3D空间，再利用多视图一致性优化来提高准确性。整体流程分三步：1)单视图提案生成 - 使用2D检测器和分割模型生成掩码，通过单目深度估计转换为3D点云；2)多视图提案优化 - 通过全局尺度初始化和每个掩码的精细调整，结合光度和CLIP语义一致性损失优化3D位置；3)聚类和融合 - 基于IoU合并重叠提案，生成最终3D边界框。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)完全训练自由的开放词汇3D检测，无需任何3D训练；2)专门针对稀疏视图设置优化，只需少量RGB图像；3)多视图特征度量一致性优化，结合光度和语义一致性；4)简单而强大的基线方法。相比之前工作，本文不依赖点云或深度数据，不需要3D训练，在稀疏视图下表现更好，能处理任意文本查询包括长尾类别，计算效率更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种简单而强大的训练自由方法，通过利用预训练的2D基础模型和稀疏多视图一致性优化，实现了在稀疏RGB图像上的开放词汇3D物体检测，无需任何3D特定训练，并在稀疏视图设置下显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ability to interpret and comprehend a 3D scene is essential for manyvision and robotics systems. In numerous applications, this involves 3D objectdetection, i.e.~identifying the location and dimensions of objects belonging toa specific category, typically represented as bounding boxes. This hastraditionally been solved by training to detect a fixed set of categories,which limits its use. In this work, we investigate open-vocabulary 3D objectdetection in the challenging yet practical sparse-view setting, where only alimited number of posed RGB images are available as input. Our approach istraining-free, relying on pre-trained, off-the-shelf 2D foundation modelsinstead of employing computationally expensive 3D feature fusion or requiring3D-specific learning. By lifting 2D detections and directly optimizing 3Dproposals for featuremetric consistency across views, we fully leverage theextensive training data available in 2D compared to 3D. Through standardbenchmarks, we demonstrate that this simple pipeline establishes a powerfulbaseline, performing competitively with state-of-the-art techniques in denselysampled scenarios while significantly outperforming them in the sparse-viewsetting.</description>
      <author>example@mail.com (Olivier Moliner, Viktor Larsson, Kalle Åström)</author>
      <guid isPermaLink="false">2509.15924v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding</title>
      <link>http://arxiv.org/abs/2509.15800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ChronoForge-RL是一种新型视频理解框架，结合TAD和KF-GRPO方法解决了视频处理中的计算效率和语义重要帧识别问题，使7B参数模型达到与72B参数模型相当的性能。&lt;h4&gt;背景&lt;/h4&gt;当前视频理解方法面临两大挑战：处理密集视频内容的计算不可行性，以及通过简单均匀采样策略难以识别语义上重要的帧。&lt;h4&gt;目的&lt;/h4&gt;提出ChronoForge-RL框架，结合时间顶点蒸馏(TAD)和关键帧感知组相对策略优化(KF-GRPO)来解决视频理解中的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;引入可微分的关键帧选择机制，通过三阶段过程识别语义转折点；提出TAD模块利用变化评分和优先蒸馏选择信息量最大的帧；引入KF-GRPO实现对比学习范式，使用显著性增强奖励机制激励模型同时利用帧内容和时间关系。&lt;h4&gt;主要发现&lt;/h4&gt;ChronoForge-RL在VideoMME上达到69.1%，在LVBench上达到52.7%，明显优于先前方法，使7B参数模型达到与72B参数模型相当的性能。&lt;h4&gt;结论&lt;/h4&gt;ChronoForge-RL框架有效解决了视频理解中的计算效率和语义重要帧识别问题，显著提升了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;当前最先进的视频理解方法通常面临两个关键挑战：(1)处理密集视频内容中每一帧的计算不可行性，以及(2)通过简单均匀采样策略识别语义上重要帧的困难。在本文中，我们提出了一种名为ChronoForge-RL的新型视频理解框架，结合时间顶点蒸馏(TAD)和关键帧感知组相对策略优化(KF-GRPO)来解决这些问题。具体来说，我们引入了一个可微分的关键帧选择机制，通过三阶段过程系统识别语义转折点，在保持时间信息的同时提高计算效率。然后，提出了两个特定模块来实现有效的时间推理：首先，TAD利用变化评分、转折点检测和优先蒸馏来选择信息量最大的帧。其次，我们引入了KF-GRPO，它实现了一个具有显著性增强奖励机制的对比学习范式，明确激励模型同时利用帧内容和时间关系。最后，我们提出的ChronoForge-RL在VideoMME上达到69.1%，在LVBench上达到52.7%，与基线方法相比，明显超越了先前的方法，同时使我们的7B参数模型能够达到与72B参数替代方案相当的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current state-of-the-art video understanding methods typically struggle withtwo critical challenges: (1) the computational infeasibility of processingevery frame in dense video content and (2) the difficulty in identifyingsemantically significant frames through naive uniform sampling strategies. Inthis paper, we propose a novel video understanding framework, calledChronoForge-RL, which combines Temporal Apex Distillation (TAD) andKeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle theseissues. Concretely, we introduce a differentiable keyframe selection mechanismthat systematically identifies semantic inflection points through a three-stageprocess to enhance computational efficiency while preserving temporalinformation. Then, two particular modules are proposed to enable effectivetemporal reasoning: Firstly, TAD leverages variation scoring, inflectiondetection, and prioritized distillation to select the most informative frames.Secondly, we introduce KF-GRPO which implements a contrastive learning paradigmwith a saliency-enhanced reward mechanism that explicitly incentivizes modelsto leverage both frame content and temporal relationships. Finally, ourproposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBenchcompared to baseline methods, clearly surpassing previous approaches whileenabling our 7B parameter model to achieve performance comparable to 72Bparameter alternatives.</description>
      <author>example@mail.com (Kehua Chen)</author>
      <guid isPermaLink="false">2509.15800v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?</title>
      <link>http://arxiv.org/abs/2509.15602v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了TennisTV基准测试，用于系统性评估多模态大语言模型在网球视频理解方面的能力，揭示了当前模型在快速、高频运动视频理解中的不足，并指出帧采样密度和时间定位是改进的关键领域。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在一般视频理解方面表现出色，但在处理像网球这样快速、高频的运动视频时存在困难，因为网球回合片段短小但信息密集。&lt;h4&gt;目的&lt;/h4&gt;创建第一个也是最全面的网球视频理解基准测试，系统性评估多模态大语言模型在这一具有挑战性领域的表现。&lt;h4&gt;方法&lt;/h4&gt;TennisTV将每个网球回合建模为连续击球事件的时序序列，使用自动化流程进行视频过滤和问题生成，涵盖8个不同层级的任务，并包含2,500个人类验证的问题。&lt;h4&gt;主要发现&lt;/h4&gt;评估了16个代表性多模态大语言模型，发现了两个关键见解：帧采样密度应该根据任务特点进行调整和平衡；改进时间定位能力对于提升模型推理能力至关重要。&lt;h4&gt;结论&lt;/h4&gt;当前多模态大语言模型在网球视频理解方面存在明显不足，需要针对快速、高频运动视频的特点进行专门优化，特别是在帧采样策略和时间定位能力方面。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在通用视频理解方面表现出色，但在像网球这样快速、高频的运动中表现不佳，因为回合片段短小但信息密集。为了系统性评估MLLMs在这一具有挑战性领域的表现，我们提出了TennisTV，这是第一个也是最全面的网球视频理解基准测试。TennisTV将每个回合建模为连续击球事件的时序序列，使用自动化流程进行过滤和问题生成。它涵盖回合级别和击球级别的8个任务，并包括2,500个人类验证的问题。通过评估16个代表性的MLLMs，我们提供了第一个网球视频理解的系统评估。结果揭示了重大的不足，并得出两个关键见解：(i) 帧采样密度应该针对任务进行调整和平衡，(ii) 改进时间定位对于更强的推理能力至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) excel at general video understandingbut struggle with fast, high-frequency sports like tennis, where rally clipsare short yet information-dense. To systematically evaluate MLLMs in thischallenging domain, we present TennisTV, the first and most comprehensivebenchmark for tennis video understanding. TennisTV models each rally as atemporal-ordered sequence of consecutive stroke events, using automatedpipelines for filtering and question generation. It covers 8 tasks at rally andstroke levels and includes 2,500 human-verified questions. Evaluating 16representative MLLMs, we provide the first systematic assessment of tennisvideo understanding. Results reveal substantial shortcomings and yield two keyinsights: (i) frame-sampling density should be tailored and balanced acrosstasks, and (ii) improving temporal grounding is essential for strongerreasoning.</description>
      <author>example@mail.com (Zhongyuan Bao, Lejun Zhang)</author>
      <guid isPermaLink="false">2509.15602v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery</title>
      <link>http://arxiv.org/abs/2509.15596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Strong accept by NeurIPS2025 Reviewers and AC, but reject by PC.  (Rating: 6,5,4,4)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了EyePCR基准测试，评估多模态大语言模型在眼科手术分析中的认知能力，包括感知、理解和推理三个维度，并通过领域适应的EyePCR-MLLM模型展示了显著性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在眼科手术等高风险、特定领域场景中的表现尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为EyePCR的眼科手术分析大规模基准测试，评估模型在感知、理解和推理方面的认知能力。&lt;h4&gt;方法&lt;/h4&gt;创建包含21万多个视觉问答对的注释语料库，涵盖1048个细粒度属性用于多视图感知，构建2.5万多个三元组的医学知识图谱用于理解，提供四个临床基础的推理任务，并开发EyePCR-MLLM作为Qwen2.5-VL-7B的领域适应变体。&lt;h4&gt;主要发现&lt;/h4&gt;EyePCR-MLLM在感知的多项选择题上达到最高准确率，在理解和推理任务上优于开源模型，性能可与GPT-4.1等商业模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;EyePCR揭示了现有MLLMs在手术认知方面的局限性，并为手术视频理解模型的基准测试和临床可靠性提升奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)展现出了非凡的能力，但它们在眼科手术等高风险、特定领域场景中的表现仍然 largely 未得到充分探索。为解决这一差距，我们开发了EyePCR，一个用于眼科手术分析的大规模基准测试，基于结构化临床知识，用于评估感知、理解和推理三个维度的认知能力。EyePCR提供了丰富的注释语料库，包含超过21万个视觉问答对，覆盖1048个细粒度属性用于多视图感知，2.5万多个三元组的医学知识图谱用于理解，以及四个临床基础的推理任务。丰富的注释促进了深入的认知分析，模拟了外科医生如何感知视觉线索并将其与领域知识结合以做出决策，从而大大提高了模型的认知能力。特别是，EyePCR-MLLM作为Qwen2.5-VL-7B的领域适应变体，在比较的模型中感知的多项选择题上达到最高准确率，在理解和推理上优于开源模型，性能可与GPT-4.1等商业模型相媲美。EyePCR揭示了现有MLLMs在手术认知方面的局限性，并为手术视频理解模型的基准测试和临床可靠性提升奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; MLLMs (Multimodal Large Language Models) have showcased remarkablecapabilities, but their performance in high-stakes, domain-specific scenarioslike surgical settings, remains largely under-explored. To address this gap, wedevelop \textbf{EyePCR}, a large-scale benchmark for ophthalmic surgeryanalysis, grounded in structured clinical knowledge to evaluate cognitionacross \textit{Perception}, \textit{Comprehension} and \textit{Reasoning}.EyePCR offers a richly annotated corpus with more than 210k VQAs, which cover1048 fine-grained attributes for multi-view perception, medical knowledge graphof more than 25k triplets for comprehension, and four clinically groundedreasoning tasks. The rich annotations facilitate in-depth cognitive analysis,simulating how surgeons perceive visual cues and combine them with domainknowledge to make decisions, thus greatly improving models' cognitive ability.In particular, \textbf{EyePCR-MLLM}, a domain-adapted variant of Qwen2.5-VL-7B,achieves the highest accuracy on MCQs for \textit{Perception} among comparedmodels and outperforms open-source models in \textit{Comprehension} and\textit{Reasoning}, rivalling commercial models like GPT-4.1. EyePCR revealsthe limitations of existing MLLMs in surgical cognition and lays the foundationfor benchmarking and enhancing clinical reliability of surgical videounderstanding models.</description>
      <author>example@mail.com (Gui Wang, Yang Wennuo, Xusen Ma, Zehao Zhong, Zhuoru Wu, Ende Wu, Rong Qu, Wooi Ping Cheah, Jianfeng Ren, Linlin Shen)</author>
      <guid isPermaLink="false">2509.15596v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2509.15464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种结合时间感知推理和知识图谱演化的方法，用于解决大型语言模型在处理动态知识时的挑战。EvoReasoner算法和EvoKG模块使模型能够有效处理时间变化的知识，并在动态问答任务中表现出色，甚至缩小了小型和大型模型之间的性能差距。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在许多语言理解任务上表现出色，但在处理不断发展的知识时存在困难。现有通过知识图谱增强LLMs的方法大多假设知识图谱是静态的，忽略了现实数据中固有的时间动态和事实不一致性。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在处理时间变化知识时的推理挑战，确保底层知识图谱的准确性和时效性，提高模型在动态问答任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出EvoReasoner，一个时间感知的多跳推理算法，执行全局-局部实体锚定、多路径分解和时间锚定评分；同时引入EvoKG，一个噪声容忍的知识图谱演化模块，通过基于置信度的矛盾解决和时间趋势跟踪从非结构化文档增量更新知识图谱。&lt;h4&gt;主要发现&lt;/h4&gt;该方法优于基于提示和基于知识图谱的基线，有效缩小了小型和大型LLMs在动态问答上的差距。使用该方法的80亿参数模型匹配了7个月后提示的671B模型的性能，证明了结合时间推理与知识图谱演化的重要性。&lt;h4&gt;结论&lt;/h4&gt;结合时间推理与知识图谱演化对于构建稳健和最新的大型语言模型性能至关重要，该方法在处理动态知识方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在许多语言理解任务上表现出色，但在处理不断发展的知识时存在困难。为了解决这个问题，最近的研究探索了通过知识图谱增强LLMs，以提供结构化、最新的信息。然而，许多现有方法假设知识图谱是静态的快照，忽略了现实数据中固有的时间动态和事实不一致性。为了解决在时间变化知识上进行推理的挑战，我们提出了EvoReasoner，一个时间感知的多跳推理算法，执行全局-局部实体锚定、多路径分解和时间锚定评分。为了确保底层知识图谱保持准确和最新，我们引入了EvoKG，一个噪声容忍的知识图谱演化模块，通过基于置信度的矛盾解决和时间趋势跟踪从非结构化文档增量更新知识图谱。我们在时间问答基准测试和一个新颖的端到端设置上评估了我们的方法，其中知识图谱从原始文档动态更新。我们的方法优于基于提示和基于知识图谱的基线，有效缩小了小型和大型LLMs在动态问答上的差距。值得注意的是，使用我们方法的80亿参数模型匹配了7个月后提示的671B模型的性能。这些结果强调了将时间推理与知识图谱演化相结合对于稳健和最新的LLM性能的重要性。我们的代码已在github.com/junhongmit/TREK上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) excel at many language understanding tasks butstruggle to reason over knowledge that evolves. To address this, recent workhas explored augmenting LLMs with knowledge graphs (KGs) to provide structured,up-to-date information. However, many existing approaches assume a staticsnapshot of the KG and overlook the temporal dynamics and factualinconsistencies inherent in real-world data. To address the challenge ofreasoning over temporally shifting knowledge, we propose EvoReasoner, atemporal-aware multi-hop reasoning algorithm that performs global-local entitygrounding, multi-route decomposition, and temporally grounded scoring. Toensure that the underlying KG remains accurate and up-to-date, we introduceEvoKG, a noise-tolerant KG evolution module that incrementally updates the KGfrom unstructured documents through confidence-based contradiction resolutionand temporal trend tracking. We evaluate our approach on temporal QA benchmarksand a novel end-to-end setting where the KG is dynamically updated from rawdocuments. Our method outperforms both prompting-based and KG-enhancedbaselines, effectively narrowing the gap between small and large LLMs ondynamic question answering. Notably, an 8B-parameter model using our approachmatches the performance of a 671B model prompted seven months later. Theseresults highlight the importance of combining temporal reasoning with KGevolution for robust and up-to-date LLM performance. Our code is publiclyavailable at github.com/junhongmit/TREK.</description>
      <author>example@mail.com (Junhong Lin, Song Wang, Xiaojie Guo, Julian Shun, Yada Zhu)</author>
      <guid isPermaLink="false">2509.15464v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Adaptive Estimation for Temporal Respiratory Disease Outbreak</title>
      <link>http://arxiv.org/abs/2509.08578v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MAESTRO是一个新型统一框架，通过整合先进的谱时建模和多模态数据融合，实现了及时且稳健的流感发病率预测。&lt;h4&gt;背景&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测流感发病率的方法，支持公共卫生决策。&lt;h4&gt;方法&lt;/h4&gt;提出了MAESTRO框架，整合了先进的谱时建模和多模态数据融合（包括监测数据、网络搜索趋势和气象数据）；通过自适应加权异构数据源和分解复杂时间序列模式来实现稳健准确的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在超过11年的香港流感数据上评估，MAESTRO展示了最先进的性能，实现了0.956的R平方值，具有优越的模型拟合效果；大量的消融实验证实了其多模态和谱时组分的重要贡献。&lt;h4&gt;结论&lt;/h4&gt;模块化和可复现的管道已公开可用，便于部署和扩展到其他地区和病原体，为流行病学预测提供了强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。本文提出了MAESTRO（多模态时间呼吸道疾病暴发自适应估计），这是一个新颖的统一框架，协同整合了先进的谱时建模与多模态数据融合，包括监测数据、网络搜索趋势和气象数据。通过自适应加权异构数据源和分解复杂时间序列模式，该模型实现了稳健且准确的预测。在香港超过11年的流感数据（不包括COVID-19期间）上评估，MAESTRO展示了最先进的性能，实现了0.956的R平方值，具有优越的模型拟合效果。大量的消融实验证实了其多模态和谱时组分的显著贡献。模块化和可复现的管道已公开可用，便于部署和扩展到其他地区和病原体，为流行病学预测提供了强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely and robust influenza incidence forecasting is critical for publichealth decision-making. This paper presents MAESTRO (Multi-modal AdaptiveEstimation for Temporal Respiratory Disease Outbreak), a novel, unifiedframework that synergistically integrates advanced spectro-temporal modelingwith multi-modal data fusion, including surveillance, web search trends, andmeteorological data. By adaptively weighting heterogeneous data sources anddecomposing complex time series patterns, the model achieves robust andaccurate forecasts. Evaluated on over 11 years of Hong Kong influenza data(excluding the COVID-19 period), MAESTRO demonstrates state-of-the-artperformance, achieving a superior model fit with an R-square of 0.956.Extensive ablations confirm the significant contributions of its multi-modaland spectro-temporal components. The modular and reproducible pipeline is madepublicly available to facilitate deployment and extension to other regions andpathogens, presenting a powerful tool for epidemiological forecasting.</description>
      <author>example@mail.com (Hong Liu, Kerui Cen, Yanxing Chen, Zige Liu, Dong Chen, Zifeng Yang, Chitin Hon)</author>
      <guid isPermaLink="false">2509.08578v3</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation</title>
      <link>http://arxiv.org/abs/2509.16170v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一模态松弛分割网络(UniMRSeg)，通过分层自监督补偿(HSSC)解决多模态图像分割中因模态不完整导致的性能下降问题，在各种缺失模态场景下显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;多模态图像分割在实际部署中面临挑战，不完整或损坏的模态会降低性能。现有方法通过专门的每组合模型解决训练-推理模态差距，但需要大量模型子集和模型-模态匹配，导致部署成本高昂。&lt;h4&gt;目的&lt;/h4&gt;设计一个统一的模态松弛分割网络，能够有效处理模态缺失问题，同时降低部署复杂度和成本。&lt;h4&gt;方法&lt;/h4&gt;UniMRSeg通过分层方式弥合完整和不完整模态在输入、特征和输出级别之间的表征差距，包括：采用混合洗牌掩码增强的模态重建；模态不变对比学习；轻量级反向注意力适配器；以及在混合一致性约束下的微调过程。&lt;h4&gt;主要发现&lt;/h4&gt;在各种缺失模态场景下，UniMRSeg在基于MRI的脑肿瘤分割、RGB-D语义分割、RGB-D/T显著目标分割等任务上显著优于最先进方法，无需额外复杂处理。&lt;h4&gt;结论&lt;/h4&gt;UniMRSeg提供了一个高效的多模态图像分割解决方案，能够有效处理模态缺失问题，同时保持较低的计算和部署成本。&lt;h4&gt;翻译&lt;/h4&gt;多模态图像分割面临实际部署挑战，不完整/损坏的模态会降低性能。虽然现有方法通过专门的每组合模型解决训练-推理模态差距，但它们需要大量模型子集和模型-模态匹配，引入了高部署成本。在这项工作中，我们通过分层自监督补偿(HSSC)提出了统一的模态松弛分割网络(UniMRSeg)。我们的方法在输入、特征和输出级别分层弥合完整和不完整模态之间的表征差距。首先，我们采用混合洗牌掩码增强进行模态重建，鼓励模型学习固有模态特征，并通过跨模态融合为缺失模态生成有意义的表征。接下来，模态不变对比学习隐式补偿不完整-完整模态对之间的特征空间距离。此外，提出的轻量级反向注意力适配器显式补偿冻结编码器中的弱感知语义。最后，UniMRSeg在混合一致性约束下进行微调，确保在所有模态组合下稳定预测而不会出现大的性能波动。无需额外复杂处理，UniMRSeg在基于MRI的脑肿瘤分割、RGB-D语义分割、RGB-D/T显著目标分割等多样化缺失模态场景下显著优于最先进方法。代码将在https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal image segmentation faces real-world deployment challenges fromincomplete/corrupted modalities degrading performance. While existing methodsaddress training-inference modality gaps via specialized per-combinationmodels, they introduce high deployment costs by requiring exhaustive modelsubsets and model-modality matching. In this work, we propose a unifiedmodality-relax segmentation network (UniMRSeg) through hierarchicalself-supervised compensation (HSSC). Our approach hierarchically bridgesrepresentation gaps between complete and incomplete modalities across input,feature and output levels. % First, we adopt modality reconstruction with thehybrid shuffled-masking augmentation, encouraging the model to learn theintrinsic modality characteristics and generate meaningful representations formissing modalities through cross-modal fusion. % Next, modality-invariantcontrastive learning implicitly compensates the feature space distance amongincomplete-complete modality pairs. Furthermore, the proposed lightweightreverse attention adapter explicitly compensates for the weak perceptualsemantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybridconsistency constraint to ensure stable prediction under all modalitycombinations without large performance fluctuations. Without bells andwhistles, UniMRSeg significantly outperforms the state-of-the-art methods underdiverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-Dsemantic segmentation, RGB-D/T salient object segmentation. The code will bereleased at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.</description>
      <author>example@mail.com (Xiaoqi Zhao, Youwei Pang, Chenyang Yu, Lihe Zhang, Huchuan Lu, Shijian Lu, Georges El Fakhri, Xiaofeng Liu)</author>
      <guid isPermaLink="false">2509.16170v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports</title>
      <link>http://arxiv.org/abs/2509.16095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICDM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AdaSports-Traj的自适应轨迹建模框架，用于解决多智能体体育场景中的轨迹预测挑战，通过角色和领域感知适配器以及分层对比学习方法，有效处理了域内和域间的分布差异。&lt;h4&gt;背景&lt;/h4&gt;多智能体体育场景中的轨迹预测具有挑战性，因为不同智能体角色（如球员与球）存在结构异质性，不同体育领域间存在动态分布差异，现有统一框架无法捕捉这些变化。&lt;h4&gt;目的&lt;/h4&gt;提出AdaSports-Traj框架，解决体育领域中域内和域间的分布差异问题，提高轨迹预测在角色和领域间的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;框架核心是角色和领域感知适配器，根据智能体身份和领域上下文条件性地调整潜在表示；同时引入分层对比学习目标，分别监督角色敏感和领域感知的表示，鼓励解耦的潜在结构而不引入优化冲突。&lt;h4&gt;主要发现&lt;/h4&gt;在Basketball-U、Football-U和Soccer-U三个多样化的体育数据集上的实验证明，AdaSports-Traj的自适应设计在统一和跨域轨迹预测设置中都取得了有效性能。&lt;h4&gt;结论&lt;/h4&gt;AdaSports-Traj通过自适应处理角色和领域差异，显著提高了多智能体体育场景中的轨迹预测能力。&lt;h4&gt;翻译&lt;/h4&gt;在多智能体体育场景中，轨迹预测本质上具有挑战性，这源于智能体角色间的结构异质性（如球员与球）以及不同体育领域间的动态分布差异。现有的统一框架往往无法捕捉这些结构分布变化，导致在角色和领域间的泛化能力不佳。我们提出了AdaSports-Traj，一个自适应轨迹建模框架，明确解决体育领域中域内和域间的分布差异。其核心是角色和领域感知适配器，根据智能体身份和领域上下文条件性地调整潜在表示。此外，我们引入了分层对比学习目标，分别监督角色敏感和领域感知的表示，鼓励解耦的潜在结构而不引入优化冲突。在三个多样化的体育数据集（Basketball-U、Football-U和Soccer-U）上的实验证明了我们自适应设计的有效性，在统一和跨域轨迹预测设置中都取得了强大性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory prediction in multi-agent sports scenarios is inherentlychallenging due to the structural heterogeneity across agent roles (e.g.,players vs. ball) and dynamic distribution gaps across different sportsdomains. Existing unified frameworks often fail to capture these structureddistributional shifts, resulting in suboptimal generalization across roles anddomains. We propose AdaSports-Traj, an adaptive trajectory modeling frameworkthat explicitly addresses both intra-domain and inter-domain distributiondiscrepancies in sports. At its core, AdaSports-Traj incorporates a Role- andDomain-Aware Adapter to conditionally adjust latent representations based onagent identity and domain context. Additionally, we introduce a HierarchicalContrastive Learning objective, which separately supervises role-sensitive anddomain-aware representations to encourage disentangled latent structureswithout introducing optimization conflict. Experiments on three diverse sportsdatasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectivenessof our adaptive design, achieving strong performance in both unified andcross-domain trajectory prediction settings.</description>
      <author>example@mail.com (Yi Xu, Yun Fu)</author>
      <guid isPermaLink="false">2509.16095v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>A multi-temporal multi-spectral attention-augmented deep convolution neural network with contrastive learning for crop yield prediction</title>
      <link>http://arxiv.org/abs/2509.15966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in Computers and Electronics in Agriculture&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为MTMS-YieldNet的新型多时相多光谱产量预测网络，通过整合光谱数据与时空信息，有效解决了气候变化背景下农作物产量预测的挑战，在不同气候和季节条件下均表现出优异的预测性能。&lt;h4&gt;背景&lt;/h4&gt;精确的产量预测对农业可持续性和粮食安全至关重要，但气候变化通过影响天气条件、土壤肥力和农场管理系统等因素，使准确的产量预测变得复杂。现有方法在处理多光谱数据时面临挑战，而这种数据对于评估作物健康和生长模式至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合光谱数据与时空信息的产量预测网络，以克服现有方法的局限性，提高产量预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出名为MTMS-YieldNet的新型网络，整合光谱数据与时空信息，利用对比学习进行特征判别，专注于从遥感数据中捕获空间-光谱模式和时空依赖关系，而非依赖在通用视觉数据上预训练的模型。&lt;h4&gt;主要发现&lt;/h4&gt;MTMS-YieldNet在七种现有最先进方法中表现优异，在Sentinel-1数据集上达到0.336的MAPE分数，在Landsat-8数据集上达到0.353的MAPE分数，在Sentinel-2数据集上达到0.331的MAPE分数，展示了在不同气候和季节条件下的有效预测能力。&lt;h4&gt;结论&lt;/h4&gt;MTMS-YieldNet显著提高了产量预测的准确性，为农民提供有价值的见解，帮助他们做出更好的决策，从而有潜力提高作物产量。&lt;h4&gt;翻译&lt;/h4&gt;精确的产量预测对农业可持续性和粮食安全至关重要。然而，气候变化通过影响天气条件、土壤肥力和农场管理系统等主要因素，使准确的产量预测变得复杂。技术的进步在克服这些挑战方面发挥了重要作用，通过利用卫星监测和数据分析进行精确的产量估计。当前方法依赖于时空数据来预测作物产量，但它们往往难以处理多光谱数据，而多光谱数据对于评估作物健康和生长模式至关重要。为解决这一挑战，我们提出了一种新颖的多时相多光谱产量预测网络MTMS-YieldNet，该网络将光谱数据与时空信息整合，以有效捕获它们之间的相关性和依赖关系。虽然现有方法依赖于在通用视觉数据上预训练的模型，但MTMS-YieldNet在预训练过程中利用对比学习进行特征判别，专注于从遥感数据中捕获空间-光谱模式和时空依赖关系。定量和定性评估均突显了所提出的MTMS-YieldNet相对于七种现有最先进方法的优越性。MTMS-YieldNet在Sentinel-1上达到0.336的MAPE分数，在Landsat-8上达到0.353的MAPE分数，以及在Sentinel-2上达到了卓越的0.331的MAPE分数，展示了在不同气候和季节条件下的有效产量预测性能。MTMS-YieldNet的卓越性能提高了产量预测，并提供了有价值的见解，可以帮助农民做出更好的决策，从而可能提高作物产量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.compag.2025.110895&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Precise yield prediction is essential for agricultural sustainability andfood security. However, climate change complicates accurate yield prediction byaffecting major factors such as weather conditions, soil fertility, and farmmanagement systems. Advances in technology have played an essential role inovercoming these challenges by leveraging satellite monitoring and dataanalysis for precise yield estimation. Current methods rely on spatio-temporaldata for predicting crop yield, but they often struggle with multi-spectraldata, which is crucial for evaluating crop health and growth patterns. Toresolve this challenge, we propose a novel Multi-Temporal Multi-Spectral YieldPrediction Network, MTMS-YieldNet, that integrates spectral data withspatio-temporal information to effectively capture the correlations anddependencies between them. While existing methods that rely on pre-trainedmodels trained on general visual data, MTMS-YieldNet utilizes contrastivelearning for feature discrimination during pre-training, focusing on capturingspatial-spectral patterns and spatio-temporal dependencies from remote sensingdata. Both quantitative and qualitative assessments highlight the excellence ofthe proposed MTMS-YieldNet over seven existing state-of-the-art methods.MTMS-YieldNet achieves MAPE scores of 0.336 on Sentinel-1, 0.353 on Landsat-8,and an outstanding 0.331 on Sentinel-2, demonstrating effective yieldprediction performance across diverse climatic and seasonal conditions. Theoutstanding performance of MTMS-YieldNet improves yield predictions andprovides valuable insights that can assist farmers in making better decisions,potentially improving crop yields.</description>
      <author>example@mail.com (Shalini Dangi, Surya Karthikeya Mullapudi, Chandravardhan Singh Raghaw, Shahid Shafi Dar, Mohammad Zia Ur Rehman, Nagendra Kumar)</author>
      <guid isPermaLink="false">2509.15966v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>UNIV: Unified Foundation Model for Infrared and Visible Modalities</title>
      <link>http://arxiv.org/abs/2509.15642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为UNIV的生物启发统一模型，用于处理RGB可见光和红外数据的联合感知，通过两种创新方法实现了跨模态特征对齐和知识保留，在保持可见光任务性能的同时显著提升了红外任务性能。&lt;h4&gt;背景&lt;/h4&gt;对RGB可见光和红外感知的联合需求正在快速增长，特别是在各种天气条件下实现稳健性能。预训练的RGB可见光和红外数据模型在其各自领域表现出色，但在配备两种传感器的自动驾驶车辆等多模态场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决预训练模型在多模态场景中的表现问题，提出一种生物启发的统一红外和可见光模态基础模型(UNIV)，以实现跨模态的有效感知和学习。&lt;h4&gt;方法&lt;/h4&gt;1) 引入基于补丁的跨模态对比学习(PCCL)，模仿视网膜水平细胞的侧向抑制，实现跨模态特征对齐；2) 双知识保留机制模拟视网膜双极细胞信号路由，结合LoRA适配器和同步蒸馏防止灾难性遗忘；3) 引入MVIP数据集，包含98,992个精确对齐的图像对，涵盖多种场景。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明UNIV在红外任务上表现优越(语义分割中+1.7 mIoU，目标检测中+0.7 mAP)，同时在可见RGB任务上保持了99%以上的基线性能。&lt;h4&gt;结论&lt;/h4&gt;UNIV模型成功实现了RGB可见光和红外数据的联合感知，在保持可见光任务性能的同时显著提升了红外任务性能，为自动驾驶等应用提供了多模态感知的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;对RGB可见光和红外感知的联合需求正在快速增长，特别是在各种天气条件下实现稳健性能。尽管预训练的RGB可见光和红外数据模型在其各自领域表现出色，但在多模态场景中往往表现不佳，例如配备这两种传感器的自动驾驶车辆。为应对这一挑战，我们提出了一种生物启发的统一红外和可见光模态基础模型(UNIV)，具有两个关键创新。首先，我们引入了基于补丁的跨模态对比学习(PCCL)，这是一种注意力引导的蒸馏框架，模仿视网膜水平细胞的侧向抑制，使有效的跨模态特征对齐成为可能，同时保持与任何基于Transformer架构的兼容性。其次，我们的双知识保留机制模拟视网膜双极细胞信号路由 - 结合LoRA适配器(增加2%参数)和同步蒸馏，防止灾难性遗忘，从而复制视网膜的明视觉(锥体驱动)和暗视觉(杆体驱动)功能。为支持跨模态学习，我们引入了MVIP数据集，这是迄今为止最全面的可见光-红外基准数据集。它包含98,992个精确对齐的图像对，涵盖多种场景。大量实验证明了UNIV在红外任务上的优越性能(语义分割中+1.7 mIoU，目标检测中+0.7 mAP)，同时在可见RGB任务上保持了99%以上的基线性能。我们的代码可在https://github.com/fangyuanmao/UNIV获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The demand for joint RGB-visible and infrared perception is growing rapidly,particularly to achieve robust performance under diverse weather conditions.Although pre-trained models for RGB-visible and infrared data excel in theirrespective domains, they often underperform in multimodal scenarios, such asautonomous vehicles equipped with both sensors. To address this challenge, wepropose a biologically inspired UNified foundation model for Infrared andVisible modalities (UNIV), featuring two key innovations. First, we introducePatch-wise Cross-modality Contrastive Learning (PCCL), an attention-guideddistillation framework that mimics retinal horizontal cells' lateralinhibition, which enables effective cross-modal feature alignment whileremaining compatible with any transformer-based architecture. Second, ourdual-knowledge preservation mechanism emulates the retina's bipolar cell signalrouting - combining LoRA adapters (2% added parameters) with synchronousdistillation to prevent catastrophic forgetting, thereby replicating theretina's photopic (cone-driven) and scotopic (rod-driven) functionality. Tosupport cross-modal learning, we introduce the MVIP dataset, the mostcomprehensive visible-infrared benchmark to date. It contains 98,992 preciselyaligned image pairs spanning diverse scenarios. Extensive experimentsdemonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU insemantic segmentation and +0.7 mAP in object detection) while maintaining 99%+of the baseline performance on visible RGB tasks. Our code is available athttps://github.com/fangyuanmao/UNIV.</description>
      <author>example@mail.com (Fangyuan Mao, Shuo Wang, Jilin Mei, Chen Min, Shun Lu, Fuyang Liu, Yu Hu)</author>
      <guid isPermaLink="false">2509.15642v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection</title>
      <link>http://arxiv.org/abs/2509.15570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted CVIPPR 2024 April Xiamen China&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种针对无监督异常声音检测的异常暴露方法，通过对比学习中的高频信息数据增强技术，使模型更关注代表机器正常操作模式的低频信息，从而有效提高异常检测性能。&lt;h4&gt;背景&lt;/h4&gt;异常声音检测在工业设备监控等领域具有重要意义。传统方法难以解决无监督场景下的异常检测问题，因为缺乏异常样本进行训练。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决无监督异常声音检测问题，通过使模型学习正常数据的分布空间，提高异常检测的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;基于生物感知和数据分析发现异常音频和噪声通常具有更高频率的特点，本研究提出了一种针对对比学习的高频信息数据增强方法。该方法使模型更关注代表机器正常操作模式的低频信息。&lt;h4&gt;主要发现&lt;/h4&gt;在DCASE 2020 Task 2和DCASE 2022 Task 2数据集上的评估结果表明，所提出的方法优于该数据集上使用的其他对比学习方法，具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过高频信息数据增强的对比学习方法能够有效提高无监督异常声音检测的性能，模型能够更好地学习正常数据的分布空间，从而准确识别异常声音。&lt;h4&gt;翻译&lt;/h4&gt;异常暴露方法是解决无监督异常声音检测问题的有效方法。该方法的关键焦点是如何让模型学习正常数据的分布空间。基于生物感知和数据分析，发现异常音频和噪声通常具有更高的频率。因此，我们提出了一种针对对比学习中的高频信息的数据增强方法。这使得模型能够更关注音频的低频信息，这代表了机器的正常操作模式。我们在DCASE 2020 Task 2上评估了所提出的方法。结果表明，我们的方法优于该数据集上使用的其他对比学习方法。我们还评估了该方法在DCASE 2022 Task 2数据集上的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The outlier exposure method is an effective approach to address theunsupervised anomaly sound detection problem. The key focus of this method ishow to make the model learn the distribution space of normal data. Based onbiological perception and data analysis, it is found that anomalous audio andnoise often have higher frequencies. Therefore, we propose a data augmentationmethod for high-frequency information in contrastive learning. This enables themodel to pay more attention to the low-frequency information of the audio,which represents the normal operational mode of the machine. We evaluated theproposed method on the DCASE 2020 Task 2. The results showed that our methodoutperformed other contrastive learning methods used on this dataset. We alsoevaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.</description>
      <author>example@mail.com (Xinxin Meng, Jiangtao Guo, Yunxiang Zhang, Shun Huang)</author>
      <guid isPermaLink="false">2509.15570v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Comparing Computational Pathology Foundation Models using Representational Similarity Analysis</title>
      <link>http://arxiv.org/abs/2509.15482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统分析了六个计算病理学基础模型的表示空间，发现模型训练范式与表示结构无直接关联，所有模型均表现出较高的幻灯片依赖性但较低的疾病依赖性，染色标准化可降低幻灯片依赖性，视觉-语言模型具有更紧凑的表示结构。&lt;h4&gt;背景&lt;/h4&gt;基础模型在计算病理学中日益发展，有望促进多种下游任务，但对不同模型学习到的表示结构和变异性了解有限。&lt;h4&gt;目的&lt;/h4&gt;系统分析六个计算病理学基础模型的表示空间，使用计算神经科学中流行化的技术。&lt;h4&gt;方法&lt;/h4&gt;分析六种CPath基础模型(包括视觉-语言对比学习模型CONCH、PLIP、KEEP和自蒸馏模型UNI v2、Virchow v2、Prov-GigaPath)，使用TCGA的H&amp;E图像补丁进行表示相似性分析。&lt;h4&gt;主要发现&lt;/h4&gt;UNI2和Virchow2具有最独特的表示结构，Prov-Gigapath在模型间具有最高平均相似性；相同训练范式不保证表示相似性；所有模型表示均表现出较高幻灯片依赖性和较低疾病依赖性；染色标准化将幻灯片依赖性降低5.5%-20.5%；视觉-语言模型具有相对紧凑的表示，仅视觉模型表示更为分散。&lt;h4&gt;结论&lt;/h4&gt;这些发现为提高对幻灯片特定特征的鲁棒性、模型集成策略设计以及理解训练范式如何塑造模型表示提供了见解；该框架可扩展到其他医学成像领域，有助于确保基础模型的有效开发和部署。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在计算病理学(CPath)中日益受到重视，因为它们有望促进许多下游任务。尽管最近的研究已经评估了不同模型的任务性能，但对于它们学习到的表示结构和变异性了解较少。在此，我们使用计算神经科学中流行化的技术，系统分析了六个CPath基础模型的表示空间。所分析的模型涵盖了视觉-语言对比学习(CONCH、PLIP、KEEP)和自蒸馏(UNI v2、Virchow v2、Prov-GigaPath)方法。通过对TCGA的H&amp;E图像补丁进行表示相似性分析，我们发现UNI2和Virchow2具有最独特的表示结构，而Prov-Gigapath在模型间具有最高的平均相似性。具有相同的训练范式（仅视觉vs视觉-语言）并不能保证更高的表示相似性。所有模型的表示都表现出较高的幻灯片依赖性，但相对较低的疾病依赖性。染色标准化将所有模型的幻灯片依赖性降低了5.5%(CONCH)到20.5%(PLIP)。在内在维度方面，视觉-语言模型表现出相对紧凑的表示，而仅视觉模型的表示更为分散。这些发现突显了提高对幻灯片特定特征鲁棒性的机会，为模型集成策略提供信息，并深入了解训练范式如何塑造模型表示。我们的框架可扩展到医学成像领域，探索基础模型的内部表示可以帮助确保有效开发和部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are increasingly developed in computational pathology(CPath) given their promise in facilitating many downstream tasks. While recentstudies have evaluated task performance across models, less is known about thestructure and variability of their learned representations. Here, wesystematically analyze the representational spaces of six CPath foundationmodels using techniques popularized in computational neuroscience. The modelsanalyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) andself-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Throughrepresentational similarity analysis using H&amp;E image patches from TCGA, we findthat UNI2 and Virchow2 have the most distinct representational structures,whereas Prov-Gigapath has the highest average similarity across models. Havingthe same training paradigm (vision-only vs. vision-language) did not guaranteehigher representational similarity. The representations of all models showed ahigh slide-dependence, but relatively low disease-dependence. Stainnormalization decreased slide-dependence for all models by a range of 5.5%(CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-languagemodels demonstrated relatively compact representations, compared to the moredistributed representations of vision-only models. These findings highlightopportunities to improve robustness to slide-specific features, inform modelensembling strategies, and provide insights into how training paradigms shapemodel representations. Our framework is extendable across medical imagingdomains, where probing the internal representations of foundation models canhelp ensure effective development and deployment.</description>
      <author>example@mail.com (Vaibhav Mishra, William Lotter)</author>
      <guid isPermaLink="false">2509.15482v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks</title>
      <link>http://arxiv.org/abs/2509.15272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, XAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统评估了未经额外修改的视觉Transformer（ViT）特征在图像分类和分割任务中的表现，探索了不同令牌类型、任务和预训练目标下的最佳选择，为自监督学习（SSL）在计算机视觉中的应用提供了新见解。&lt;h4&gt;背景&lt;/h4&gt;自监督学习（SSL）对视觉Transformer（ViT）显示出作为各种计算机视觉任务预训练策略的巨大潜力。对比学习和掩码图像建模是SSL技术的两大主流预训练目标。从最终transformer注意力块（键、查询和值）以及最终块的前馈层后获取的特征已成为解决下游任务的基础。然而，现有方法通常对这些预训练的ViT特征进行额外处理，如通过轻量级头部或蒸馏技术，以获得更好的任务性能。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究空白，通过系统评估未经修改的ViT特征在图像分类和分割任务（包括标准和少样本情境）中的应用，全面分析ViT特征的内在表示能力。&lt;h4&gt;方法&lt;/h4&gt;研究使用基于超平面（如逻辑回归）或余弦相似度的分类和分割规则，这些规则依赖于ViT潜在空间中可解释方向的存在。在不使用额外特征转换的情况下，对不同令牌类型、任务和预训练ViT模型进行了系统分析。&lt;h4&gt;主要发现&lt;/h4&gt;研究提供了关于令牌类型和决策规则的最佳选择的见解，这些选择基于任务、上下文和预训练目标，同时在两个广泛使用的数据集上报告了详细的研究结果。&lt;h4&gt;结论&lt;/h4&gt;研究揭示了ViT原始特征在不同任务和情境下的表现特性，为如何有效利用这些特征提供了指导，而无需额外的特征转换层。&lt;h4&gt;翻译&lt;/h4&gt;视觉Transformer（ViT）的自监督学习（SSL）最近显示出作为各种计算机视觉任务预训练策略的巨大潜力，包括图像分类和分割，在标准和少样本下游情境中都有应用。SSL技术领域主要由两种预训练目标主导：对比学习和掩码图像建模。从最终transformer注意力块（特别是键、查询和值）以及最终块的前馈层后获取的特征已成为解决下游任务的共同基础。然而，在许多现有方法中，这些预训练的ViT特征会通过额外的转换层进行处理，通常涉及轻量级头部或与蒸馏结合，以获得更好的任务性能。尽管这些方法可以改善任务结果，但据我们所知，尚未对未经修改的ViT特征的内在表示能力进行全面分析。本研究旨在通过系统评估这些未修改的特征在图像分类和分割任务（包括标准和少样本情境）中的应用来填补这一空白。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recentlydemonstrated considerable potential as a pre-training strategy for a variety ofcomputer vision tasks, including image classification and segmentation, both instandard and few-shot downstream contexts. Two pre-training objectives dominatethe landscape of SSL techniques: Contrastive Learning and Masked ImageModeling. Features (or tokens) extracted from the final transformer attentionblock -- specifically, the keys, queries, and values -- as well as featuresobtained after the final block's feed-forward layer, have become a commonfoundation for addressing downstream tasks. However, in many existingapproaches, these pre-trained ViT features are further processed throughadditional transformation layers, often involving lightweight heads or combinedwith distillation, to achieve superior task performance. Although such methodscan improve task outcomes, to the best of our knowledge, a comprehensiveanalysis of the intrinsic representation capabilities of unaltered ViT featureshas yet to be conducted. This study aims to bridge this gap by systematicallyevaluating the use of these unmodified features across image classification andsegmentation tasks, in both standard and few-shot contexts. The classificationand segmentation rules that we use are either hyperplane based (as in logisticregression) or cosine-similarity based, both of which rely on the presence ofinterpretable directions in the ViT's latent space. Based on the previous rulesand without the use of additional feature transformations, we conduct ananalysis across token types, tasks, and pre-trained ViT models. This studyprovides insights into the optimal choice for token type and decision rulebased on the task, context, and the pre-training objective, while reportingdetailed findings on two widely-used datasets.</description>
      <author>example@mail.com (Yannis Kaltampanidis, Alexandros Doumanoglou, Dimitrios Zarpalas)</author>
      <guid isPermaLink="false">2509.15272v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Are Multimodal Foundation Models All That Is Needed for Emofake Detection?</title>
      <link>http://arxiv.org/abs/2509.16193v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to APSIPA-ASC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了多模态基础模型在情感伪造检测中的应用，提出并验证了它们优于仅依赖音频的音频基础模型的假设，同时提出了SCAR融合框架实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态基础模型通过跨模态预训练从多种模态学习情感模式，而音频基础模型仅依赖音频，这可能影响它们对情感伪造检测的准确性。&lt;h4&gt;目的&lt;/h4&gt;验证多模态基础模型在情感伪造检测中会优于音频基础模型的假设，并探索基础模型融合的有效方法。&lt;h4&gt;方法&lt;/h4&gt;对最先进的多模态基础模型(如LanguageBind)和音频基础模型(如WavLM)进行综合比较分析，提出SCAR框架，该框架包含嵌套交叉注意力机制和自注意力细化模块，实现基础模型表示的顺序交互和特征增强。&lt;h4&gt;主要发现&lt;/h4&gt;多模态基础模型确实在情感伪造检测中优于音频基础模型；使用SCAR框架协同融合多模态基础模型实现了最先进的性能，超越了独立基础模型和传统融合方法以及先前的工作。&lt;h4&gt;结论&lt;/h4&gt;多模态基础模型能够更好地识别不自然的情感转变和操纵音频中的不一致性，使其在区分真实与虚假情感表达方面更有效。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们研究多模态基础模型用于情感伪造检测，并假设它们将优于音频基础模型。多模态基础模型由于其跨模态预训练，从多种模态学习情感模式，而音频基础模型仅依赖音频。因此，多模态基础模型可以更好地识别操纵音频中不自然的情感转变和不一致性，使其在区分真实与虚假情感表达方面更有效。为了验证我们的假设，我们对最先进的多模态基础模型(如LanguageBind)和音频基础模型(如WavLM)进行了全面的比较分析。我们的实验证实多模态基础模型在情感伪造检测中优于音频基础模型。除了独立基础模型的性能外，我们还探索了基础模型融合，受到相关研究领域如合成语音检测和语音情感识别的发现启发。为此，我们提出了SCAR，一种有效融合的新颖框架。SCAR引入了嵌套交叉注意力机制，其中表示从基础模型在两个阶段顺序交互以改进信息交换。此外，自注意力细化模块通过强化重要的跨基础模型线索同时抑制噪声来进一步增强特征表示。通过SCAR协同融合多模态基础模型，我们实现了最先进的性能，超越了独立基础模型、传统融合方法以及情感伪造检测的先前工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we investigate multimodal foundation models (MFMs) for EmoFakedetection (EFD) and hypothesize that they will outperform audio foundationmodels (AFMs). MFMs due to their cross-modal pre-training, learns emotionalpatterns from multiple modalities, while AFMs rely only on audio. As such, MFMscan better recognize unnatural emotional shifts and inconsistencies inmanipulated audio, making them more effective at distinguishing real from fakeemotional expressions. To validate our hypothesis, we conduct a comprehensivecomparative analysis of state-of-the-art (SOTA) MFMs (e.g. LanguageBind)alongside AFMs (e.g. WavLM). Our experiments confirm that MFMs surpass AFMs forEFD. Beyond individual foundation models (FMs) performance, we explore FMsfusion, motivated by findings in related research areas such synthetic speechdetection and speech emotion recognition. To this end, we propose SCAR, a novelframework for effective fusion. SCAR introduces a nested cross-attentionmechanism, where representations from FMs interact at two stages sequentiallyto refine information exchange. Additionally, a self-attention refinementmodule further enhances feature representations by reinforcing importantcross-FM cues while suppressing noise. Through SCAR with synergistic fusion ofMFMs, we achieve SOTA performance, surpassing both standalone FMs andconventional fusion approaches and previous works on EFD.</description>
      <author>example@mail.com (Mohd Mujtaba Akhtar, Girish, Orchid Chetia Phukan, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru)</author>
      <guid isPermaLink="false">2509.16193v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories</title>
      <link>http://arxiv.org/abs/2509.16176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ACDC系统，一个由人类导演与无人机之间自然语言通信驱动的自主无人机摄影系统，利用大型语言模型和视觉基础模型将自然语言提示直接转化为可执行的室内无人机视频导览。&lt;h4&gt;背景&lt;/h4&gt;先前无人机摄影工作流程的主要局限性是需要基于预定义的人类意图手动选择路径点和视角，这既劳动密集又导致性能不一致。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用大型语言模型和视觉基础模型将自由形式的自然语言提示直接转化为可执行的室内无人机视频导览的方法。&lt;h4&gt;方法&lt;/h4&gt;该方法包括：视觉语言检索管道用于初始路径点选择；基于偏好的贝叶斯优化框架，利用美学反馈优化姿态；以及运动规划器生成安全的四旋翼飞行轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;通过仿真和硬件在环实验验证了ACDC，证明它能够在多样化的室内场景中稳健地产生专业质量的视频画面，无需机器人或摄影专业知识。&lt;h4&gt;结论&lt;/h4&gt;这些结果突出了具身AI代理在从开放词汇对话到现实世界自主空中摄影方面闭环的潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了代理空中摄影：从对话线索到电影轨迹（ACDC），这是一个由人类导演与无人机之间自然语言通信驱动的自主无人机摄影系统。先前无人机摄影工作流程的主要局限性是需要基于预定义的人类意图手动选择路径点和视角，这既劳动密集又导致性能不一致。在本文中，我们提议采用大型语言模型和视觉基础模型将自由形式的自然语言提示直接转化为可执行的室内无人机视频导览。具体来说，我们的方法包括一个用于初始路径点选择的视觉语言检索管道，一个利用美学反馈优化姿态的基于偏好的贝叶斯优化框架，以及一个生成安全四旋翼轨迹的运动规划器。我们通过仿真和硬件在环实验验证了ACDC，证明它能够在多样化的室内场景中稳健地产生专业质量的视频画面，无需机器人或摄影专业知识。这些结果突出了具身AI代理在从开放词汇对话到现实世界自主空中摄影方面闭环的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Agentic Aerial Cinematography: From Dialogue Cues to CinematicTrajectories (ACDC), an autonomous drone cinematography system driven bynatural language communication between human directors and drones. The mainlimitation of previous drone cinematography workflows is that they requiremanual selection of waypoints and view angles based on predefined human intent,which is labor-intensive and yields inconsistent performance. In this paper, wepropose employing large language models (LLMs) and vision foundation models(VFMs) to convert free-form natural language prompts directly into executableindoor UAV video tours. Specifically, our method comprises a vision-languageretrieval pipeline for initial waypoint selection, a preference-based Bayesianoptimization framework that refines poses using aesthetic feedback, and amotion planner that generates safe quadrotor trajectories. We validate ACDCthrough both simulation and hardware-in-the-loop experiments, demonstratingthat it robustly produces professional-quality footage across diverse indoorscenes without requiring expertise in robotics or cinematography. These resultshighlight the potential of embodied AI agents to close the loop fromopen-vocabulary dialogue to real-world autonomous aerial cinematography.</description>
      <author>example@mail.com (Yifan Lin, Sophie Ziyu Liu, Ran Qi, George Z. Xue, Xinping Song, Chao Qin, Hugh H. -T. Liu)</author>
      <guid isPermaLink="false">2509.16176v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning</title>
      <link>http://arxiv.org/abs/2509.16025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Copyright 2025 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新颖的多模态基础模型方法，用于口语语言评估，能够在单次处理中进行会话级别评估，结合多目标学习和Whisper ASR模型语音先验，有效预测学习者的整体口语能力。&lt;h4&gt;背景&lt;/h4&gt;口语语言评估从自然言语中评估学习者的口语能力，随着第二语言英语使用者人数增长，对可靠SLA的需求增加，SLA是计算机辅助语言学习的关键组成部分。现有方法存在级联管道易传播错误或端到端模型在短音频窗口上操作可能丢失话语级别证据的局限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在单次处理中进行会话级别评估的新方法，克服现有方法的局限性，避免错误传播和丢失话语级别证据，创建适用于CALL应用的紧凑可部署评分器。&lt;h4&gt;方法&lt;/h4&gt;提出多模态基础模型方法，结合多目标学习与基于冻结的Whisper ASR模型的语音先验进行声学感知校准，无需手动制作特征，共同学习SLA的整体和特质级目标，一致处理L2说话者的整个回答会话。&lt;h4&gt;主要发现&lt;/h4&gt;在Speak &amp; Improve基准测试中，提出的方法优于之前最先进的级联系统，展示了强大的跨参与者泛化能力，产生了一个为CALL应用量身定制的紧凑可部署评分器。&lt;h4&gt;结论&lt;/h4&gt;提出的方法有效解决了现有SLA方法的局限性，通过一致处理整个回答会话提高了整体口语能力预测的准确性，为CALL应用提供了实用高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;口语语言评估从自然言语中估计学习者的口语能力。第二语言英语使用者人口的增加加剧了对可靠SLA的需求，SLA是计算机辅助语言学习的关键组成部分。现有工作通常依赖级联管道，容易传播错误，或端到端模型通常在短音频窗口上操作，可能错过话语级别的证据。本文引入了一种新颖的多模态基础模型方法，在单次执行中进行会话级别评估。我们的方法将多目标学习与基于冻结的Whisper ASR模型的语音先验相结合进行声学感知校准，允许共同学习SLA的整体和特质级目标，无需依赖手工制作的特征。通过一致地处理L2说话者的整个回答会话，该模型在预测整体口语能力方面表现出色。在Speak &amp; Improve基准上进行的实验表明，我们提出的方法优于之前最先进的级联系统，并表现出强大的跨参与者泛化能力，产生了一个为CALL应用量身定制的紧凑可部署评分器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spoken Language Assessment (SLA) estimates a learner's oral proficiency fromspontaneous speech. The growing population of L2 English speakers hasintensified the demand for reliable SLA, a critical component of ComputerAssisted Language Learning (CALL). Existing efforts often rely on cascadedpipelines, which are prone to error propagation, or end-to-end models thatoften operate on a short audio window, which might miss discourse-levelevidence. This paper introduces a novel multimodal foundation model approachthat performs session-level evaluation in a single pass. Our approach couplesmulti-target learning with a frozen, Whisper ASR model-based speech prior foracoustic-aware calibration, allowing for jointly learning holistic andtrait-level objectives of SLA without resorting to handcrafted features. Bycoherently processing the entire response session of an L2 speaker, the modelexcels at predicting holistic oral proficiency. Experiments conducted on theSpeak &amp; Improve benchmark demonstrate that our proposed approach outperformsthe previous state-of-the-art cascaded system and exhibits robust cross-partgeneralization, producing a compact deployable grader that is tailored for CALLapplications.</description>
      <author>example@mail.com (Hong-Yun Lin, Jhen-Ke Lin, Chung-Chun Wang, Hao-Chien Lu, Berlin Chen)</author>
      <guid isPermaLink="false">2509.16025v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>AI Methods for Permutation Circuit Synthesis Across Generic Topologies</title>
      <link>http://arxiv.org/abs/2509.16020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by First AAAI Symposium on Quantum  Information &amp; Machine Learning (QIML): Bridging Quantum Computing and  Artificial Intelligence at AAAI 2025 Fall Symposium&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了人工智能方法在通用拓扑上排列电路的综合和转译。研究使用强化学习技术实现了最多25个量子比特的排列电路的近最优综合。通过训练基础模型和采用掩码机制，使单个训练模型能够在多种拓扑上高效合成电路，并可实际集成到转译工作流中。&lt;h4&gt;背景&lt;/h4&gt;在量子计算领域，排列电路的综合和转译是一个重要问题。传统方法通常需要为每种拓扑单独训练模型，限制了方法的通用性和效率。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理通用拓扑的排列电路综合方法，无需为每种拓扑单独训练模型，实现高效且通用的电路合成。&lt;h4&gt;方法&lt;/h4&gt;研究使用强化学习技术训练一个基础模型在通用矩形格子上，并通过掩码机制在综合过程中动态选择拓扑子集。这种方法使模型能够在嵌入矩形格子的任何拓扑上合成排列电路而无需重新训练。此外，研究还展示了可以对模型进行微调以增强对特定拓扑的性能。&lt;h4&gt;主要发现&lt;/h4&gt;研究展示了5x5格子的结果，并将其与之前的AI拓扑导向模型和经典方法进行了比较。结果表明，新方法优于经典启发式方法，与之前的专用AI模型相匹配，能够对训练过程中未见过的拓扑进行综合，并且可以通过微调增强对特定拓扑的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法允许单个训练模型在多种拓扑上高效合成电路，使其能够实际集成到转译工作流中，提高了量子计算电路设计的效率和通用性。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了人工智能方法在通用拓扑上排列电路的综合和转译。我们的方法使用强化学习技术实现了最多25个量子比特的排列电路的近最优综合。我们没有为单个拓扑开发专门模型，而是在通用矩形格子上训练一个基础模型，并采用掩码机制在综合过程中动态选择拓扑子集。这使得能够在嵌入矩形格子的任何拓扑上合成排列电路，而无需重新训练模型。在本文中，我们展示了5x5格子的结果，并将其与之前的AI拓扑导向模型和经典方法进行了比较，结果表明它们优于经典启发式方法，与之前的专用AI模型相匹配，并且能够对训练过程中未见过的拓扑进行综合。我们还展示了模型可以微调以增强对所选感兴趣拓扑的性能。这种方法允许单个训练模型在多种拓扑上高效合成电路，使其能够实际集成到转译工作流中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates artificial intelligence (AI) methodologies for thesynthesis and transpilation of permutation circuits across generic topologies.Our approach uses Reinforcement Learning (RL) techniques to achievenear-optimal synthesis of permutation circuits up to 25 qubits. Rather thandeveloping specialized models for individual topologies, we train afoundational model on a generic rectangular lattice, and employ maskingmechanisms to dynamically select subsets of topologies during the synthesis.This enables the synthesis of permutation circuits on any topology that can beembedded within the rectangular lattice, without the need to re-train themodel. In this paper we show results for 5x5 lattice and compare them toprevious AI topology-oriented models and classical methods, showing that theyoutperform classical heuristics, and match previous specialized AI models, andperforms synthesis even for topologies that were not seen during training. Wefurther show that the model can be fine tuned to strengthen the performance forselected topologies of interest. This methodology allows a single trained modelto efficiently synthesize circuits across diverse topologies, allowing itspractical integration into transpilation workflows.</description>
      <author>example@mail.com (Victor Villar, Juan Cruz-Benito, Ismael Faro, David Kremer)</author>
      <guid isPermaLink="false">2509.16020v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching</title>
      <link>http://arxiv.org/abs/2509.16017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DistillMatch的多模态图像匹配方法，通过从视觉基础模型(VFM)进行知识蒸馏，构建轻量级学生模型，提取高级语义特征并注入模态类别信息，同时设计V2I-GAN进行数据增强，有效解决了不同模态间外观差异大的匹配挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态图像匹配对于跨模态感知、融合和分析至关重要，但模态间的显著外观差异使这一任务具有挑战性。由于高质量标注数据集稀缺，现有的深度学习方法表现不佳且缺乏对多样化场景的适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理不同模态间显著外观差异的多模态图像匹配方法，提高模型的泛化能力和鲁棒性，使其能够适应多样化场景。&lt;h4&gt;方法&lt;/h4&gt;提出DistillMatch方法，采用知识蒸馏技术构建轻量级学生模型，从VFM(包括DINOv2和DINOv3)提取高级语义特征辅助跨模态匹配。提取并注入模态类别信息以保留模态特定信息，设计V2I-GAN将可见图像转换为伪红外图像进行数据增强，提高模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，DistillMatch在公共数据集上的性能优于现有算法，证明了其在多模态图像匹配任务中的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;DistillMatch通过知识蒸馏和模态信息注入，能够更好地理解跨模态相关性，结合V2I-GAN数据进一步增强模型泛化能力，为多模态图像匹配提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态图像匹配寻求不同模态图像间的像素级对应关系，这对于跨模态感知、融合和分析至关重要。然而，模态之间的显著外观差异使这一任务具有挑战性。由于高质量标注数据集的稀缺，现有的深度学习方法表现不佳且缺乏对多样化场景的适应性。视觉基础模型(VFM)在大规模数据上训练，产生可泛化且鲁棒的特征表示，适应各种模态的数据和任务，包括多模态匹配。因此，我们提出了DistillMatch，一种使用VFM知识蒸馏的多模态图像匹配方法。DistillMatch采用知识蒸馏构建轻量级学生模型，从VFM(包括DINOv2和DINOv3)提取高级语义特征以辅助跨模态匹配。为保留模态特定信息，它提取并将模态类别信息注入到另一模态的特征中，增强了模型对跨模态相关性的理解。此外，我们设计了V2I-GAN，通过将可见图像转换为伪红外图像进行数据增强，提高模型的泛化能力。实验表明，DistillMatch在公共数据集上优于现有算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal image matching seeks pixel-level correspondences between images ofdifferent modalities, crucial for cross-modal perception, fusion and analysis.However, the significant appearance differences between modalities make thistask challenging. Due to the scarcity of high-quality annotated datasets,existing deep learning methods that extract modality-common features formatching perform poorly and lack adaptability to diverse scenarios. VisionFoundation Model (VFM), trained on large-scale data, yields generalizable androbust feature representations adapted to data and tasks of various modalities,including multimodal matching. Thus, we propose DistillMatch, a multimodalimage matching method using knowledge distillation from VFM. DistillMatchemploys knowledge distillation to build a lightweight student model thatextracts high-level semantic features from VFM (including DINOv2 and DINOv3) toassist matching across modalities. To retain modality-specific information, itextracts and injects modality category information into the other modality'sfeatures, which enhances the model's understanding of cross-modal correlations.Furthermore, we design V2I-GAN to boost the model's generalization bytranslating visible to pseudo-infrared images for data augmentation.Experiments show that DistillMatch outperforms existing algorithms on publicdatasets.</description>
      <author>example@mail.com (Meng Yang, Fan Fan, Zizhuo Li, Songchu Deng, Yong Ma, Jiayi Ma)</author>
      <guid isPermaLink="false">2509.16017v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Wireless Channel Foundation Model with Embedded Noise-Plus-Interference Suppression Structure</title>
      <link>http://arxiv.org/abs/2509.15993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种具有噪声加干扰抑制能力的增强型无线信道基础模型架构，解决了实际系统中完美信道信息状态不可用的问题，通过NPI估计和减法模块以及CSI完成网络获取干净版本的CSI用于特征提取，在信道预测任务上实现了比现有方法更好的性能。&lt;h4&gt;背景&lt;/h4&gt;无线信道基础模型(WCFM)是一种在大规模无线信道数据集上预训练的任务无关AI模型，可用于通信和感知相关的各种下游任务。现有WCFM研究都使用完美CSI数据进行训练，但实际系统中完美CSI不可用，只能获取降级CSI(有噪版本)，这影响了WCFM的性能。&lt;h4&gt;目的&lt;/h4&gt;解决实际系统中WCFM因无法获取完美CSI而导致的性能下降问题，提出一种能够处理噪声和干扰的增强型WCFM架构。&lt;h4&gt;方法&lt;/h4&gt;提出一种具有噪声加干扰(NPI)抑制能力的增强型无线信道基础模型架构。首先获取CSI的粗略估计，然后计算两个投影矩阵提取接收信号中的NPI项，通过NPI估计和减法模块处理，最后将结果信号通过CSI完成网络获取干净版本的CSI用于特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，与最先进的解决方案相比，具有NPI抑制结构的WCFM在信道预测任务上实现了更好的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入NPI抑制能力，增强型WCFM能够在实际系统中有效处理噪声和干扰问题，提高下游任务性能。&lt;h4&gt;翻译&lt;/h4&gt;无线信道基础模型(WCFM)是一种任务无关的AI模型，在大规模无线信道数据集上进行预训练，学习通用的信道特征表示，可用于通信和感知相关的各种下游任务。虽然现有关于WCFM的研究已展示其在波束预测、信道预测、定位等任务中的巨大潜力，但这些模型都是使用完美(即无误差且完整)的信道信息状态(CSI)数据进行训练的，这些数据是通过仿真工具生成的。然而，在实际部署WCFM的系统中，完美CSI不可用。相反，需要首先基于部分资源元素(REs)上的导频信号进行信道估计，以获取CSI的有噪版本(称为降级CSI)，这在某些噪声和干扰严重的实际环境中与完美CSI有很大差异。因此，WCFM生成的特征表示无法反映真实信道的特性，导致下游任务性能下降。为解决这一问题，本文提出了一种具有噪声加干扰(NPI)抑制能力的增强型无线信道基础模型架构。在我们的方法中，首先获取CSI的粗略估计，然后计算两个投影矩阵来提取接收信号中的NPI项，这些项进一步通过NPI估计和减法模块处理。最后，将结果信号通过CSI完成网络获取干净版本的CSI，用于特征提取。仿真结果表明，与最先进的解决方案相比，具有NPI抑制结构的WCFM在信道预测任务上实现了更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wireless channel foundation model (WCFM) is a task-agnostic AI model that ispretrained on large-scale wireless channel datasets to learn a universalchannel feature representation that can be used for a wide range of downstreamtasks related to communications and sensing. While existing works on WCFM havedemonstrated its great potentials in various tasks including beam prediction,channel prediction, localization, etc, the models are all trained using perfect(i.e., error-free and complete) channel information state (CSI) data which aregenerated with simulation tools. However, in practical systems where the WCFMis deployed, perfect CSI is not available. Instead, channel estimation needs tobe first performed based on pilot signals over a subset of the resourceelements (REs) to acquire a noisy version of the CSI (termed as degraded CSI),which significantly differs from the perfect CSI in some real-worldenvironments with severe noise and interference. As a result, the featurerepresentation generated by the WCFM is unable to reflect the characteristicsof the true channel, yielding performance degradation in downstream tasks. Toaddress this issue, in this paper we propose an enhanced wireless channelfoundation model architecture with noise-plus-interference (NPI) suppressioncapability. In our approach, coarse estimates of the CSIs are first obtained.With these information, two projection matrices are computed to extract the NPIterms in the received signals, which are further processed by a NPI estimationand subtraction module. Finally, the resultant signal is passed through a CSIcompletion network to get a clean version of the CSI, which is used for featureextraction. Simulation results demonstrated that compared to thestate-of-the-art solutions, WCFM with NPI suppression structure achievesimproved performance on channel prediction task.</description>
      <author>example@mail.com (Yuwei Wang, Li Sun, Tingting Yang)</author>
      <guid isPermaLink="false">2509.15993v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds</title>
      <link>http://arxiv.org/abs/2509.15915v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 9 figures. Accepted for presentation at the 39th Conference  on Neural Information Processing Systems (NeurIPS 2025) Workshop on Embodied  World Models for Decision Making&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了如何将基础模型整合到强化学习框架中以提高样本效率，提出了基础世界模型和基础智能体两种策略，并通过网格世界环境进行了实证评估。&lt;h4&gt;背景&lt;/h4&gt;虽然从头开始的强化学习在解决具有高效模拟器的顺序决策任务方面已显示出令人印象深刻的结果，但现实世界应用中由于交互成本高，需要更高效的样本智能体。&lt;h4&gt;目的&lt;/h4&gt;研究如何有效地将基础模型整合到强化学习框架中，以提高样本效率。&lt;h4&gt;方法&lt;/h4&gt;提出并评估两种策略：一是使用基础世界模型(FWMs)，利用基础模型的先验知识进行智能体的训练和评估；二是使用基础智能体(FAs)，利用基础模型的推理能力进行决策。在适合当前大型语言模型的网格世界环境中进行实证评估。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型的改进已经转化为更好的基础世界模型和基础智能体；基于当前大型语言模型的基础智能体可以为足够简单的环境提供优秀的策略；基础世界模型与强化学习智能体的结合在具有部分可观测性和随机性的复杂环境中非常有前景。&lt;h4&gt;结论&lt;/h4&gt;基础模型可以有效地整合到强化学习框架中，提高样本效率，为现实世界应用提供了有希望的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;虽然从头开始的强化学习在解决具有高效模拟器的顺序决策任务方面已经显示出令人印象深刻的结果，但现实世界应用中由于交互成本高，需要更高效的样本智能体。基础模型是提高样本效率的自然候选，因为它们拥有广泛的知识和推理能力，但如何将它们有效地整合到强化学习框架中尚不清楚。在本文中，我们预期并评估了两种有前景的策略。首先，我们考虑使用基础世界模型，利用基础模型的先验知识来实现智能体的训练和评估。其次，我们考虑使用基础智能体，利用基础模型的推理能力进行决策。我们在适合当前大型语言模型的一类网格世界环境中对这两种方法进行了实证评估。我们的结果表明，大型语言模型的改进已经转化为更好的基础世界模型和基础智能体；基于当前大型语言模型的基础智能体已经可以为足够简单的环境提供优秀的策略；基础世界模型与强化学习智能体的结合在具有部分可观测性和随机性的更复杂环境中非常有前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While reinforcement learning from scratch has shown impressive results insolving sequential decision-making tasks with efficient simulators, real-worldapplications with expensive interactions require more sample-efficient agents.Foundation models (FMs) are natural candidates to improve sample efficiency asthey possess broad knowledge and reasoning capabilities, but it is yet unclearhow to effectively integrate them into the reinforcement learning framework. Inthis paper, we anticipate and, most importantly, evaluate two promisingstrategies. First, we consider the use of foundation world models (FWMs) thatexploit the prior knowledge of FMs to enable training and evaluating agentswith simulated interactions. Second, we consider the use of foundation agents(FAs) that exploit the reasoning capabilities of FMs for decision-making. Weevaluate both approaches empirically in a family of grid-world environmentsthat are suitable for the current generation of large language models (LLMs).Our results suggest that improvements in LLMs already translate into betterFWMs and FAs; that FAs based on current LLMs can already provide excellentpolicies for sufficiently simple environments; and that the coupling of FWMsand reinforcement learning agents is highly promising for more complex settingswith partial observability and stochastic elements.</description>
      <author>example@mail.com (Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber)</author>
      <guid isPermaLink="false">2509.15915v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>ENSAM: an efficient foundation model for interactive segmentation of 3D medical images</title>
      <link>http://arxiv.org/abs/2509.15874v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ENSAM是一个轻量级、可提示的通用3D医学图像分割模型，具有等变性和归一化特性，结合了SegResNet编码器、提示编码器和掩码解码器，采用U-Net架构。&lt;h4&gt;背景&lt;/h4&gt;在CVPR 2025交互式3D生物医学图像分割基础模型挑战中，需要处理多模态3D医学图像分割任务。&lt;h4&gt;目的&lt;/h4&gt;设计一个在有限数据和计算预算下表现良好的3D医学图像分割模型。&lt;h4&gt;方法&lt;/h4&gt;ENSAM使用基于SegResNet的编码器与提示编码器和掩码解码器结合，采用潜在交叉注意力、相对位置编码、归一化注意力和Muon优化器进行训练。模型从零开始，使用不到5000个体积的多模态数据(CT、MRI、PET、超声、显微镜)，在单个32GB GPU上6小时内完成训练。&lt;h4&gt;主要发现&lt;/h4&gt;在隐藏测试集上，ENSAM获得了DSC AUC为2.404，NSD AUC为2.266，最终DSC为0.627，最终NSD为0.597，优于两个基线模型(VISTA3D、SAM-Med3D)，与第三个基线模型(SegVol)相当。在挑战的核心集轨道中，ENSAM在不使用预训练权重的方法中表现最佳。&lt;h4&gt;结论&lt;/h4&gt;ENSAM是一个高效的3D医学图像分割模型，在有限资源和数据条件下表现良好，不需要预训练权重就能取得优异性能。&lt;h4&gt;翻译&lt;/h4&gt;ENSAM(等变性、归一化、分割任何模型)是一个轻量级和可提示的通用3D医学图像分割模型。ENSAM将基于SegResNet的编码器与提示编码器和掩码解码器结合在U-Net风格架构中，使用潜在交叉注意力、相对位置编码、归一化注意力和Muon优化器进行训练。ENSAM旨在在有限数据和计算预算下实现良好性能，并在单个32GB GPU上使用不到5000个体积的多模态数据从零开始训练，耗时6小时。作为挑战的一部分，ENSAM在多模态3D医学图像的隐藏测试集上进行了评估，获得了优于两个先前发布的基线模型的性能，并与第三个基线模型相当。消融研究证实，相对位置编码和Muon优化器的使用显著加速了收敛并提高了分割质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present ENSAM (Equivariant, Normalized, Segment Anything Model), alightweight and promptable model for universal 3D medical image segmentation.ENSAM combines a SegResNet-based encoder with a prompt encoder and mask decoderin a U-Net-style architecture, using latent cross-attention, relativepositional encoding, normalized attention, and the Muon optimizer for training.ENSAM is designed to achieve good performance under limited data andcomputational budgets, and is trained from scratch on under 5,000 volumes frommultiple modalities (CT, MRI, PET, ultrasound, microscopy) on a single 32 GBGPU in 6 hours. As part of the CVPR 2025 Foundation Models for Interactive 3DBiomedical Image Segmentation Challenge, ENSAM was evaluated on hidden test setwith multimodal 3D medical images, obtaining a DSC AUC of 2.404, NSD AUC of2.266, final DSC of 0.627, and final NSD of 0.597, outperforming two previouslypublished baseline models (VISTA3D, SAM-Med3D) and matching the third (SegVol),surpassing its performance in final DSC but trailing behind in the other threemetrics. In the coreset track of the challenge, ENSAM ranks 5th of 10 overalland best among the approaches not utilizing pretrained weights. Ablationstudies confirm that our use of relative positional encodings and the Muonoptimizer each substantially speed up convergence and improve segmentationquality.</description>
      <author>example@mail.com (Elias Stenhede, Agnar Martin Bjørnstad, Arian Ranjbar)</author>
      <guid isPermaLink="false">2509.15874v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data</title>
      <link>http://arxiv.org/abs/2509.15859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Curated Data for Efficient Learning Workshop at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种利用视觉基础模型生成合成数据并训练简单线性分类器的新框架，用于解决长尾分类问题，在保持高性能的同时显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;不平衡分类数据集在机器学习中构成重大挑战，导致模型在少数类上表现不佳。尽管基础模型在长尾分类上有所进展，但仍无法达到平衡数据集训练的水平，且计算资源消耗大。&lt;h4&gt;目的&lt;/h4&gt;强调计算效率和简单性的重要性，提出一种利用视觉基础模型语义潜在空间生成合成数据，并结合真实数据训练简单线性分类器的方法。&lt;h4&gt;方法&lt;/h4&gt;利用视觉基础模型的丰富语义潜在空间生成合成数据，使用真实和合成数据的混合训练一个简单的线性分类器，将可训练参数减少到仅线性模型的参数数量。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-100-LT基准测试上设定了新的最先进水平，在Places-LT基准测试上展示了强大性能，证明了简单有效方法的高效性和适应性。&lt;h4&gt;结论&lt;/h4&gt;所提出的简单而有效的方法在长尾分类任务上表现优异，同时大大降低了计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;不平衡分类数据集在机器学习中构成重大挑战，通常导致模型在代表性不足的类别上表现不佳。随着基础模型的兴起，最近的研究已关注对这些模型进行全面、部分和参数高效的微调以处理长尾分类。尽管这些工作在基准数据集上表现出色，但仍无法缩小与使用平衡数据集训练的网络之间的差距，即使对于相对较小的数据集，仍然需要大量计算资源。强调计算效率和简单性的重要性，在这项工作中，我们提出了一个新框架，利用视觉基础模型的丰富语义潜在空间生成合成数据，并使用真实和合成数据的混合训练一个简单的线性分类器进行长尾分类。计算效率的提升来自于可训练参数的数量减少到仅线性模型中的参数数量。我们的方法在CIFAR-100-LT基准测试上设定了新的最先进水平，并在Places-LT基准测试上展示了强大的性能，突显了我们简单有效方法的有效性和适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imbalanced classification datasets pose significant challenges in machinelearning, often leading to biased models that perform poorly onunderrepresented classes. With the rise of foundation models, recent researchhas focused on the full, partial, and parameter-efficient fine-tuning of thesemodels to deal with long-tail classification. Despite the impressiveperformance of these works on the benchmark datasets, they still fail to closethe gap with the networks trained using the balanced datasets and still requiresubstantial computational resources, even for relatively smaller datasets.Underscoring the importance of computational efficiency and simplicity, in thiswork we propose a novel framework that leverages the rich semantic latent spaceof Vision Foundation Models to generate synthetic data and train a simplelinear classifier using a mixture of real and synthetic data for long-tailclassification. The computational efficiency gain arises from the number oftrainable parameters that are reduced to just the number of parameters in thelinear model. Our method sets a new state-of-the-art for the CIFAR-100-LTbenchmark and demonstrates strong performance on the Places-LT benchmark,highlighting the effectiveness and adaptability of our simple and effectiveapproach.</description>
      <author>example@mail.com (Nakul Sharma)</author>
      <guid isPermaLink="false">2509.15859v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation</title>
      <link>http://arxiv.org/abs/2509.15795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为TASAM的新型遥感图像分割模型，该模型是对Segment Anything Model (SAM)的改进，专门针对高分辨率遥感图像分割任务进行了优化。&lt;h4&gt;背景&lt;/h4&gt;SAM在自然图像领域展示了令人印象深刻的零样本分割能力，但难以推广到遥感数据的独特挑战，如复杂地形、多尺度目标和时间动态性。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门为高分辨率遥感图像分割设计的地形和时间感知的SAM扩展模型。&lt;h4&gt;方法&lt;/h4&gt;TASAM集成了三个轻量而有效的模块：地形感知适配器（注入高程先验）、时间提示生成器（捕捉土地覆盖随时间变化）和多尺度融合策略（增强细粒度目标 delineation），且不重新训练SAM主干网络。&lt;h4&gt;主要发现&lt;/h4&gt;在三个遥感基准测试（LoveDA、iSAID和WHU-CD）上实现了显著的性能提升，超越了零样本SAM和特定任务模型，同时保持了最小的计算开销。&lt;h4&gt;结论&lt;/h4&gt;强调了基础模型领域自适应增强的价值，为更强大的地理空间分割提供了可扩展的路径。&lt;h4&gt;翻译&lt;/h4&gt;分割任何模型（SAM）在自然图像领域展示了令人印象深刻的零样本分割能力，但它难以推广到遥感数据的独特挑战，如复杂地形、多尺度目标和时间动态性。在本文中，我们引入了TASAM，这是SAM的一个地形和时间感知的扩展，专门为高分辨率遥感图像分割而设计。TASAM集成了三个轻量而有效的模块：一个注入高程先验的地形感知适配器，一个捕捉土地覆盖随时间变化的时间提示生成器，以及一个增强细粒度目标 delineation 的多尺度融合策略。在不重新训练SAM主干网络的情况下，我们的方法在三个遥感基准测试（LoveDA、iSAID和WHU-CD）上实现了显著的性能提升，超越了零样本SAM和特定任务模型，同时计算开销最小。我们的结果突显了基础模型领域自适应增强的价值，并为更强大的地理空间分割提供了可扩展的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segment Anything Model (SAM) has demonstrated impressive zero-shotsegmentation capabilities across natural image domains, but it struggles togeneralize to the unique challenges of remote sensing data, such as complexterrain, multi-scale objects, and temporal dynamics. In this paper, weintroduce TASAM, a terrain and temporally-aware extension of SAM designedspecifically for high-resolution remote sensing image segmentation. TASAMintegrates three lightweight yet effective modules: a terrain-aware adapterthat injects elevation priors, a temporal prompt generator that capturesland-cover changes over time, and a multi-scale fusion strategy that enhancesfine-grained object delineation. Without retraining the SAM backbone, ourapproach achieves substantial performance gains across three remote sensingbenchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM andtask-specific models with minimal computational overhead. Our results highlightthe value of domain-adaptive augmentation for foundation models and offer ascalable path toward more robust geospatial segmentation.</description>
      <author>example@mail.com (Tianyang Wang, Xi Xiao, Gaofei Chen, Hanzhang Chi, Qi Zhang, Guo Cheng, Yingrui Ji)</author>
      <guid isPermaLink="false">2509.15795v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models</title>
      <link>http://arxiv.org/abs/2509.15607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PRIMT框架，通过基础模型解决偏好强化学习中的挑战，提高机器人学习复杂行为的效率&lt;h4&gt;背景&lt;/h4&gt;偏好强化学习(PbRL)是一种有前景的范式，可以教会机器人复杂行为而无需奖励工程，但其有效性常受限于依赖大量人工输入和解决查询模糊性及信用分配的困难&lt;h4&gt;目的&lt;/h4&gt;开发PRIMT框架，利用基础模型进行多模态合成反馈和轨迹合成，以克服传统PbRL方法的局限性&lt;h4&gt;方法&lt;/h4&gt;PRIMT采用分层神经符号融合策略，整合大型语言模型和视觉语言模型的优势评估机器人行为；包含前瞻性轨迹生成减少早期查询模糊性；以及后顾轨迹增强通过反事实推理改进信用分配&lt;h4&gt;主要发现&lt;/h4&gt;在2个运动和6个操作任务上的评估表明，PRIMT性能优于基于基础模型和脚本的基线方法&lt;h4&gt;结论&lt;/h4&gt;PRIMT框架有效解决了传统偏好强化学习中的关键挑战，为机器人复杂行为学习提供了更可靠的方法&lt;h4&gt;翻译&lt;/h4&gt;基于偏好的强化学习(PbRL)已成为一种有前景的范式，可以在没有奖励工程的情况下教会机器人复杂行为。然而，其有效性通常受到两个关键挑战的限制：依赖大量人工输入以及在奖励学习过程中解决查询模糊性和信用分配的固有困难。在本文中，我们介绍了PRIMT，一个PbRL框架，旨在通过利用基础模型(FMs)进行多模态合成反馈和轨迹合成来克服这些挑战。与依赖单模态FM评估的先前方法不同，PRIMT采用分层神经符号融合策略，整合大型语言模型和视觉语言模型的优势，以评估机器人行为，提供更可靠和全面的反馈。PRIMT还包含前瞻性轨迹生成，通过自举样本预热轨迹缓冲区，减少早期阶段的查询模糊性；以及后顾轨迹增强，通过因果辅助损失实现反事实推理，以改进信用分配。我们在各种基准测试的2个运动和6个操作任务上评估了PRIMT，展示了其优于基于FM和脚本的基线的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Preference-based reinforcement learning (PbRL) has emerged as a promisingparadigm for teaching robots complex behaviors without reward engineering.However, its effectiveness is often limited by two critical challenges: thereliance on extensive human input and the inherent difficulties in resolvingquery ambiguity and credit assignment during reward learning. In this paper, weintroduce PRIMT, a PbRL framework designed to overcome these challenges byleveraging foundation models (FMs) for multimodal synthetic feedback andtrajectory synthesis. Unlike prior approaches that rely on single-modality FMevaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy,integrating the complementary strengths of large language models andvision-language models in evaluating robot behaviors for more reliable andcomprehensive feedback. PRIMT also incorporates foresight trajectorygeneration, which reduces early-stage query ambiguity by warm-starting thetrajectory buffer with bootstrapped samples, and hindsight trajectoryaugmentation, which enables counterfactual reasoning with a causal auxiliaryloss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6manipulation tasks on various benchmarks, demonstrating superior performanceover FM-based and scripted baselines.</description>
      <author>example@mail.com (Ruiqi Wang, Dezhong Zhao, Ziqin Yuan, Tianyu Shao, Guohua Chen, Dominic Kao, Sungeun Hong, Byung-Cheol Min)</author>
      <guid isPermaLink="false">2509.15607v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Lynx: Towards High-Fidelity Personalized Video Generation</title>
      <link>http://arxiv.org/abs/2509.15496v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Lynx Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Lynx是一个高保真模型，用于从单张输入图像生成个性化视频。它基于开源的扩散变换器(DiT)基础模型，引入了两个轻量级适配器确保身份保真度。在基准测试中，Lynx展示了出色的面部相似性、提示遵循能力和视频质量，推进了个性化视频生成技术。&lt;h4&gt;背景&lt;/h4&gt;个性化视频生成需要从单张输入图像生成高质量且保持身份一致性的视频，这是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从单张输入图像生成高质量个性化视频的模型，同时确保身份保真度、时间一致性和视觉真实感。&lt;h4&gt;方法&lt;/h4&gt;Lynx构建在开源的扩散变换器(DiT)基础模型上，引入了两个轻量级适配器：1) ID适配器使用Perceiver Resampler将基于ArcFace的面部嵌入转换为紧凑的身份标记；2) Ref适配器集成密集VAE特征，通过交叉注意力将细粒度细节注入到所有transformer层中。&lt;h4&gt;主要发现&lt;/h4&gt;在包含40个受试者和20个无偏见提示的基准测试中(共800个测试案例)，Lynx表现出卓越的面部相似性、有竞争力的提示遵循能力和强大的视频质量。&lt;h4&gt;结论&lt;/h4&gt;Lynx通过两个轻量级适配器的协同工作，能够在保持身份保真度的同时维持时间一致性和视觉真实感，从而推进了个性化视频生成的技术水平。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Lynx，一个用于从单张输入图像进行个性化视频合成的高保真模型。Lynx构建在一个开源的扩散变换器(DiT)基础模型之上，引入了两个轻量级适配器来确保身份保真度。ID适配器采用Perceiver Resampler将基于ArcFace的面部嵌入转换为紧凑的身份标记用于条件化，而Ref适配器集成了来自冻结参考路径的密集VAE特征，通过交叉注意力将细粒度细节注入到所有transformer层中。这些模块共同实现了强大的身份保持，同时保持时间一致性和视觉真实感。通过对包含40个受试者和20个无偏见提示的精选基准进行评估(产生了800个测试案例)，Lynx展示了卓越的面部相似性、有竞争力的提示遵循能力和强大的视频质量，从而推进了个性化视频生成的技术水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Lynx, a high-fidelity model for personalized video synthesis froma single input image. Built on an open-source Diffusion Transformer (DiT)foundation model, Lynx introduces two lightweight adapters to ensure identityfidelity. The ID-adapter employs a Perceiver Resampler to convertArcFace-derived facial embeddings into compact identity tokens forconditioning, while the Ref-adapter integrates dense VAE features from a frozenreference pathway, injecting fine-grained details across all transformer layersthrough cross-attention. These modules collectively enable robust identitypreservation while maintaining temporal coherence and visual realism. Throughevaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, whichyielded 800 test cases, Lynx has demonstrated superior face resemblance,competitive prompt following, and strong video quality, thereby advancing thestate of personalized video generation.</description>
      <author>example@mail.com (Shen Sang, Tiancheng Zhi, Tianpei Gu, Jing Liu, Linjie Luo)</author>
      <guid isPermaLink="false">2509.15496v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training</title>
      <link>http://arxiv.org/abs/2509.15416v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种专门针对神经肿瘤学的基础模型，结合分布鲁棒优化技术，提高了分子标记物预测准确性和跨机构泛化能力，同时改善了生存预测性能。&lt;h4&gt;背景&lt;/h4&gt;神经肿瘤学因数据异质性和肿瘤复杂性对机器学习构成挑战，导致基础模型难以在不同队列中泛化，且在预测不常见分子标记物方面表现不佳，而这些标记物对治疗反应和风险分层至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种神经肿瘤学特定的基础模型，采用分布鲁棒损失函数，实现准确肿瘤表型估计的同时保持跨机构泛化能力。&lt;h4&gt;方法&lt;/h4&gt;在多机构脑肿瘤MRI数据上预训练自监督主干网络(BYOL, DINO, MAE, MoCo)，应用分布鲁棒优化(DRO)减轻站点和类别不平衡；在UCSF、UPenn和CUIMC机构测试常见分子标记物分类、不常见改变分类、连续标记物预测及IDH1野生型胶质母细胞瘤生存预测。&lt;h4&gt;主要发现&lt;/h4&gt;模型改进了分子预测并减少了站点特定嵌入差异；在CUIMC，平均平衡准确率从0.744提高到0.785，AUC从0.656提高到0.676，代表性不足端点提升显著；所有站点的生存预测c-index均有改善；Grad-CAM验证了模型的可解释性。&lt;h4&gt;结论&lt;/h4&gt;将基础模型与分布鲁棒优化结合可产生更多站点不变的表示，改善常见和不常见标记物预测，增强生存判别能力，强调了前瞻性验证及整合纵向和干预信号的必要性。&lt;h4&gt;翻译&lt;/h4&gt;神经肿瘤学由于其数据的异质性和肿瘤的复杂性，为机器学习带来了独特挑战，限制了基础模型在不同队列中泛化的能力。现有的FMs在预测不常见的分子标记物方面也表现不佳，而这些标记物对于治疗反应和风险分层至关重要。为了解决这些差距，我们开发了一个专门针对神经肿瘤学的FM，具有分布鲁棒损失函数，能够在保持跨机构泛化能力的同时准确估计肿瘤表型。我们在多机构脑肿瘤MRI上预训练了自监督主干网络，并应用分布鲁棒优化来减轻站点和类别不平衡问题。我们的方法改进了分子预测并减少了站点特定的嵌入差异，所有站点的生存预测性能都有所提升，验证了模型的可解释性。总体而言，将FMs与DRO结合可以产生更多站点不变的表示，改善预测能力，推进精准神经肿瘤学发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neuro-oncology poses unique challenges for machine learning due toheterogeneous data and tumor complexity, limiting the ability of foundationmodels (FMs) to generalize across cohorts. Existing FMs also perform poorly inpredicting uncommon molecular markers, which are essential for treatmentresponse and risk stratification. To address these gaps, we developed aneuro-oncology specific FM with a distributionally robust loss function,enabling accurate estimation of tumor phenotypes while maintainingcross-institution generalization. We pretrained self-supervised backbones(BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applieddistributionally robust optimization (DRO) to mitigate site and classimbalance. Downstream tasks included molecular classification of common markers(MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT),continuous markers (Ki-67, TP53), and overall survival prediction in IDH1wild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecularprediction and reduced site-specific embedding differences. At CUIMC, meanbalanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, withthe largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69).For survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647to 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoralregions, confirming interpretability. Overall, coupling FMs with DRO yieldsmore site-invariant representations, improves prediction of common and uncommonmarkers, and enhances survival discrimination, underscoring the need forprospective validation and integration of longitudinal and interventionalsignals to advance precision neuro-oncology.</description>
      <author>example@mail.com (Moinak Bhattacharya, Angelica P. Kurtz, Fabio M. Iwamoto, Prateek Prasanna, Gagandeep Singh)</author>
      <guid isPermaLink="false">2509.15416v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</title>
      <link>http://arxiv.org/abs/2509.15221v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ScaleCUA是一个大规模、开源的计算机使用代理项目，通过构建跨平台数据集和训练模型，实现了在多个操作系统和任务领域上的自主GUI操作能力。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型(VLMs)使计算机使用代理能够自主操作GUI，显示出巨大潜力，但进展受到缺乏大规模开源计算机使用数据和基础模型的限制。&lt;h4&gt;目的&lt;/h4&gt;引入ScaleCUA，朝着扩展开源计算机使用代理的方向迈进，解决数据稀缺问题。&lt;h4&gt;方法&lt;/h4&gt;ScaleCUA提供了一个跨越6个操作系统和3个任务领域的大规模数据集，通过结合自动化代理和人类专家的闭环流程构建。基于这些扩展数据训练的ScaleCUA能够跨平台无缝运行。&lt;h4&gt;主要发现&lt;/h4&gt;ScaleCUA在多个基准测试中取得了显著成果：在WebArena-Lite-v2上比基线提高26.6，在ScreenSpot-Pro上提高10.7；在MMBench-GUI L1-Hard上达到94.4%的最先进结果，在OSWorld-G上达到60.6%，在WebArena-Lite-v2上达到47.4%。&lt;h4&gt;结论&lt;/h4&gt;数据驱动的扩展对通用计算机使用代理的强大功效得到了这些发现的证明。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)已经使计算机使用代理(CUAs)能够自主操作图形用户界面(GUI)，显示出巨大潜力，但进展受到缺乏大规模开源计算机使用数据和基础模型的限制。在这项工作中，我们引入了ScaleCUA，这是朝着扩展开源计算机使用代理迈出的一步。它提供了一个跨越6个操作系统和3个任务领域的大规模数据集，通过结合自动化代理和人类专家的闭环流程构建。基于这种扩展数据训练的ScaleCUA能够跨平台无缝运行。具体而言，它在多个基准测试中对比基线取得了显著提升(+26.6在WebArena-Lite-v2上，+10.7在ScreenSpot-Pro上)，并设定了新的最先进结果(94.4%在MMBench-GUI L1-Hard上，60.6%在OSWorld-G上，47.4%在WebArena-Lite-v2上)。这些发现证明了数据驱动扩展对通用计算机使用代理的强大功效。我们将发布数据、模型和代码以促进未来研究：https://github.com/OpenGVLab/ScaleCUA。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have enabled computer use agents (CUAs) thatoperate GUIs autonomously, showing great potential, yet progress is limited bythe lack of large-scale, open-source computer use data and foundation models.In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. Itoffers a large-scale dataset spanning 6 operating systems and 3 task domains,built via a closed-loop pipeline uniting automated agents with human experts.Trained on this scaled-up data, ScaleCUA can operate seamlessly acrossplatforms. Specifically, it delivers strong gains over baselines (+26.6 onWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-artresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% onWebArena-Lite-v2). These findings underscore the power of data-driven scalingfor general-purpose computer use agents. We will release data, models, and codeto advance future research: https://github.com/OpenGVLab/ScaleCUA.</description>
      <author>example@mail.com (Zhaoyang Liu, Jingjing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Xuan Dong, Yue Yu, Chenyu Lu, YunXiang Mo, Yao Yan, Zeyue Tian, Xiao Zhang, Yuan Huang, Yiqian Liu, Weijie Su, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang)</author>
      <guid isPermaLink="false">2509.15221v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>How Good are Foundation Models in Step-by-Step Embodied Reasoning?</title>
      <link>http://arxiv.org/abs/2509.15293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了FoMER基准测试，用于评估大型多模态模型在复杂具身决策场景中的推理能力。研究涵盖了10个任务、8种具身形态和3种机器人类型，包含超过1.1k个带有详细逐步推理的样本。研究结果揭示了LMMs在具身推理方面的潜力和局限性。&lt;h4&gt;背景&lt;/h4&gt;物理世界中的具身智能体需要做出不仅有效而且安全、空间连贯且基于上下文的决策。尽管大型多模态模型在视觉理解和语言生成方面显示出有前景的能力，但它们在执行现实世界具身任务的结构化推理能力仍未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究旨在了解基础模型在具身环境中进行逐步推理的能力如何。&lt;h4&gt;方法&lt;/h4&gt;研究提出了Foundation Model Embodied Reasoning (FoMER)基准测试，用于评估LMMs在复杂具身决策场景中的推理能力。该基准测试包括：(i)大规模精选的具身推理任务套件，(ii)一种将感知基础与动作推理分离的新型评估框架，(iii)在此设置下对几个领先LMMs的经验分析。基准测试包含超过1.1k个样本，涵盖10个任务、8种具身形态和3种机器人类型。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果揭示了LMMs在具身推理方面的潜力和当前局限性，指出了机器人智能未来研究的关键挑战和机遇。&lt;h4&gt;结论&lt;/h4&gt;研究的数据和代码将公开可用，为未来机器人智能研究提供了重要资源。&lt;h4&gt;翻译&lt;/h4&gt;在物理世界中运行的具身智能体必须做出的决策不仅要有效，还要安全、空间连贯且基于上下文。尽管大型多模态模型在视觉理解和语言生成方面的最新进展显示出有前景的能力，但它们在执行现实世界具身任务的结构化推理能力仍未被充分探索。在这项工作中，我们旨在了解基础模型在具身环境中进行逐步推理的能力如何。为此，我们提出了Foundation Model Embodied Reasoning (FoMER)基准测试，旨在评估LMMs在复杂具身决策场景中的推理能力。我们的基准测试涵盖了多样化的任务集，要求智能体解释多模态观察、对物理约束和安全进行推理，并用自然语言生成有效的下一个动作。我们提出了(i)大规模精选的具身推理任务套件，(ii)一种将感知基础与动作推理分离的新型评估框架，以及(iii)在此设置下对几个领先LMMs的经验分析。我们的基准测试包含超过1.1k个样本，涵盖10个任务、8种具身形态和3种机器人类型，并包含详细的逐步推理。我们的结果突显了LMMs在具身推理方面的潜力和当前局限性，指出了机器人智能未来研究的关键挑战和机遇。我们的数据和代码将公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要想解决评估基础模型（特别是大型多模态模型）在逐步具身推理方面的能力问题。这个问题很重要，因为机器人等具身智能体需要在物理世界中做出有效、安全、空间连贯且基于上下文的决策，而当前对这些模型在真实世界具身任务中的推理能力还缺乏系统了解，特别是对它们推理过程的评估不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有具身推理基准的局限性，特别是缺乏对推理过程的详细评估。他们借鉴了Cosmos-R1等物理推理基准和VRC-Bench等对推理轨迹进行评估的方法，同时参考了现有的机器人数据集。作者设计了一个新的评估框架，不仅评估最终答案，还评估推理过程的质量，并创建了包含10种不同任务类型和8种具身形态的多样化基准测试集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是不仅要评估模型给出的最终答案是否正确，还要评估其推理过程的质量和合理性，特别是在考虑空间关系、物理约束、安全性和任务对齐等具身推理关键因素的情况下。整体流程包括：1)从多个数据集构建包含QA对和推理轨迹的基准测试；2)设计10个评估标准的评估框架；3)使用LLM-as-judge方法进行自动评估；4)对9个先进模型进行测试和比较，分析它们在不同任务和问题类型上的表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出FoMER基准测试，包含1100+样本和10种任务类型；2)设计新评估框架，同时评估答案正确性和推理质量；3)提供对9个最先进模型的全面分析。相比之前工作，FoMER明确包含推理轨迹评估，专注于具身环境中的物理推理，提供更广泛的任务和机器人类型覆盖，使用更细致的评估标准关注具身推理特有的挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了FoMER基准测试和评估框架，首次全面评估了大型多模态模型在具身推理任务中的能力，揭示了当前模型在复杂物理世界推理中的优势和局限，为未来具身智能研究提供了重要参考。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied agents operating in the physical world must make decisions that arenot only effective but also safe, spatially coherent, and grounded in context.While recent advances in large multimodal models (LMMs) have shown promisingcapabilities in visual understanding and language generation, their ability toperform structured reasoning for real-world embodied tasks remainsunderexplored. In this work, we aim to understand how well foundation modelscan perform step-by-step reasoning in embodied environments. To this end, wepropose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed toevaluate the reasoning capabilities of LMMs in complex embodied decision-makingscenarios. Our benchmark spans a diverse set of tasks that require agents tointerpret multimodal observations, reason about physical constraints andsafety, and generate valid next actions in natural language. We present (i) alarge-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluationframework that disentangles perceptual grounding from action reasoning, and(iii) empirical analysis of several leading LMMs under this setting. Ourbenchmark includes over 1.1k samples with detailed step-by-step reasoningacross 10 tasks and 8 embodiments, covering three different robot types. Ourresults highlight both the potential and current limitations of LMMs inembodied reasoning, pointing towards key challenges and opportunities forfuture research in robot intelligence. Our data and code will be made publiclyavailable.</description>
      <author>example@mail.com (Dinura Dissanayake, Ahmed Heakl, Omkar Thawakar, Noor Ahsan, Ritesh Thawkar, Ketan More, Jean Lahoud, Rao Anwer, Hisham Cholakkal, Ivan Laptev, Fahad Shahbaz Khan, Salman Khan)</author>
      <guid isPermaLink="false">2509.15293v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers</title>
      <link>http://arxiv.org/abs/2509.16058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了基于注意力模式的注意力控制(ASAC)模块，将认知科学中的注意力模式理论整合到人工神经网络中，特别是在Transformer架构中。通过使用向量量化变分自编码器作为注意力抽象器和控制器，ASAC能够精确管理注意力分配，提高系统效率。&lt;h4&gt;背景&lt;/h4&gt;注意力机制已成为AI的重要组成部分，显著提高了模型性能和可扩展性。同时，认知科学中的注意力模式理论(AST)认为，个体通过创建注意力模型来管理注意力，从而有效分配认知资源。&lt;h4&gt;目的&lt;/h4&gt;将注意力模式概念整合到人工神经网络中，创建基于注意力模式的注意力控制(ASAC)，通过明确建模注意力分配来提高系统效率。&lt;h4&gt;方法&lt;/h4&gt;引入ASAC模块，在Transformer架构中嵌入该模块，使用向量量化变分自编码器(VQVAE)作为注意力抽象器和控制器，在视觉和NLP领域进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;ASAC提高了分类准确性并加速学习过程；模型在各种数据集上具有鲁棒性和泛化能力；在多任务设置中表现更好；增强了对对抗性攻击的抵抗力；优化了注意力以提高学习效率；促进了有效的迁移学习和少样本学习。&lt;h4&gt;结论&lt;/h4&gt;这些有希望的结果建立了认知科学与机器学习之间的联系，为AI系统中注意力机制的有效利用提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;注意力机制已成为AI的重要组成部分，通过借鉴人类认知显著提高了模型性能和可扩展性。同时，认知科学中的注意力模式理论(AST)提出，个体通过创建注意力模型来管理注意力，从而有效分配认知资源。受AST启发，我们引入了基于注意力模式的注意力控制(ASAC)，将注意力模式概念整合到人工神经网络中。我们的初步实验专注于在Transformer架构中嵌入ASAC模块。该模块使用向量量化变分自编码器(VQVAE)作为注意力抽象器和控制器，实现精确的注意力管理。通过明确建模注意力分配，我们的方法旨在提高系统效率。我们在视觉和NLP领域证明了ASAC的有效性，突显了其提高分类准确性和加速学习过程的能力。我们在各种数据集上对视觉变压器的实验表明，注意力控制器不仅提高了分类准确性，还加速了学习。此外，我们还证明了模型在嘈杂和分布外数据集上的鲁棒性和泛化能力。此外，我们在多任务设置中展示了改进的性能。快速实验表明，基于注意力模式的模块增强了对对抗性攻击的抵抗力，优化了注意力以提高学习效率，并促进了有效的迁移学习和少样本学习。这些有希望的结果建立了认知科学与机器学习之间的联系，阐明了AI系统中注意力机制的有效利用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attention mechanisms have become integral in AI, significantly enhancingmodel performance and scalability by drawing inspiration from human cognition.Concurrently, the Attention Schema Theory (AST) in cognitive science positsthat individuals manage their attention by creating a model of the attentionitself, effectively allocating cognitive resources. Inspired by AST, weintroduce ASAC (Attention Schema-based Attention Control), which integrates theattention schema concept into artificial neural networks. Our initialexperiments focused on embedding the ASAC module within transformerarchitectures. This module employs a Vector-Quantized Variational AutoEncoder(VQVAE) as both an attention abstractor and controller, facilitating preciseattention management. By explicitly modeling attention allocation, our approachaims to enhance system efficiency. We demonstrate ASAC's effectiveness in boththe vision and NLP domains, highlighting its ability to improve classificationaccuracy and expedite the learning process. Our experiments with visiontransformers across various datasets illustrate that the attention controllernot only boosts classification accuracy but also accelerates learning.Furthermore, we have demonstrated the model's robustness and generalizationcapabilities across noisy and out-of-distribution datasets. In addition, wehave showcased improved performance in multi-task settings. Quick experimentsreveal that the attention schema-based module enhances resilience toadversarial attacks, optimizes attention to improve learning efficiency, andfacilitates effective transfer learning and learning from fewer examples. Thesepromising results establish a connection between cognitive science and machinelearning, shedding light on the efficient utilization of attention mechanismsin AI systems.</description>
      <author>example@mail.com (Krati Saxena, Federico Jurado Ruiz, Guido Manzi, Dianbo Liu, Alex Lamb)</author>
      <guid isPermaLink="false">2509.16058v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>What is a good matching of probability measures? A counterfactual lens on transport maps</title>
      <link>http://arxiv.org/abs/2509.16027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages; comments most welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了概率测度耦合在统计和机器学习中的核心作用，特别聚焦于不同类型的传输映射及其与因果推理的联系。&lt;h4&gt;背景&lt;/h4&gt;耦合概率测度是统计学和机器学习许多问题的核心，从领域适应到迁移学习和因果推理。即使限制在确定性传输上，这些耦合也不是可识别的，两个无原子边缘存在无限多个传输映射。最优传输的常见做法掩盖了多元单调匹配的多种不同概念共存的事实。&lt;h4&gt;目的&lt;/h4&gt;比较分析三种传输映射构建的异同，建立它们等价的必要和充分条件；将反事实推理表述为传输映射选择问题；连接统计传输与因果推理两个视角；阐明因果假设如何支持特定传输映射结构的使用。&lt;h4&gt;方法&lt;/h4&gt;对循环单调、分位数保持和三角形单调三种传输映射构建进行系统比较分析；建立等价条件的必要和充分条件；将反事实推理纳入结构因果模型框架；识别因果图和结构方程条件下反事实映射与经典统计传输一致的条件。&lt;h4&gt;主要发现&lt;/h4&gt;确定了三种传输映射等价的必要和充分条件，阐明了它们各自的结构特性；揭示了反事实推理中不可测试假设的作用；找到了因果图和结构方程使得反事实映射与经典统计传输一致的条件；界定了因果假设支持特定传输映射结构使用的情境。&lt;h4&gt;结论&lt;/h4&gt;研究结果丰富了传输映射族的理论理解，阐明了它们可能的因果解释，为统计传输和因果推理之间建立新的桥梁做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;耦合概率测度位于统计学和机器学习中许多问题的核心，从领域适应到迁移学习和因果推理。然而，即使限制在确定性传输上，这些耦合也不是可识别的：两个无原子边缘存在无限多个传输映射。最优传输的常见做法，以成本最小化和循环单调性为动机，掩盖了多种多元单调匹配概念共存的事实。在这项工作中，我们首先对三种传输映射的构建进行了比较分析：循环单调、分位数保持和三角形单调映射。我们建立了它们等价的必要和充分条件，从而阐明了它们各自的结构特性。同时，我们将结构因果模型框架内的反事实推理表述为在固定边缘之间选择传输映射的问题，这明确了不可测试假设在反事实推理中的作用。然后，我们通过识别因果图和结构方程的条件，在这些条件下反事实映射与经典统计传输一致，从而能够连接这两个视角。通过这种方式，我们界定了因果假设支持使用特定传输映射结构的情况。总之，我们的结果旨在丰富传输映射族的理论理解，并阐明它们可能的因果解释。我们希望这项工作有助于在统计传输和因果推理之间建立新的桥梁。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Coupling probability measures lies at the core of many problems in statisticsand machine learning, from domain adaptation to transfer learning and causalinference. Yet, even when restricted to deterministic transports, suchcouplings are not identifiable: two atomless marginals admit infinitely manytransport maps. The common recourse to optimal transport, motivated by costminimization and cyclical monotonicity, obscures the fact that several distinctnotions of multivariate monotone matchings coexist. In this work, we firstcarry a comparative analysis of three constructions of transport maps:cyclically monotone, quantile-preserving and triangular monotone maps. Weestablish necessary and sufficient conditions for their equivalence, therebyclarifying their respective structural properties. In parallel, we formulatecounterfactual reasoning within the framework of structural causal models as aproblem of selecting transport maps between fixed marginals, which makesexplicit the role of untestable assumptions in counterfactual reasoning. Then,we are able to connect these two perspectives by identifying conditions oncausal graphs and structural equations under which counterfactual maps coincidewith classical statistical transports. In this way, we delineate thecircumstances in which causal assumptions support the use of a specificstructure of transport map. Taken together, our results aim to enrich thetheoretical understanding of families of transport maps and to clarify theirpossible causal interpretations. We hope this work contributes to establishingnew bridges between statistical transport and causal inference.</description>
      <author>example@mail.com (Lucas De Lara, Luca Ganassali)</author>
      <guid isPermaLink="false">2509.16027v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised and probabilistic learning with Contrastive Local Learning Networks: The Restricted Kirchhoff Machine</title>
      <link>http://arxiv.org/abs/2509.15842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为受限基尔霍夫机器的自学习电阻网络，能够解决无监督学习任务，类似于受限玻尔兹曼机器算法。该系统通过两个相同网络比较不同物理状态实现对比局部学习规则，并在二值化MNIST数据集上进行了模拟训练，验证了其学习能力。&lt;h4&gt;背景&lt;/h4&gt;自主物理学习系统能够修改内部参数并解决计算任务而不依赖外部计算，相比传统计算机具有分布式和节能学习的优势。&lt;h4&gt;目的&lt;/h4&gt;引入一种自学习电阻网络（受限基尔霍夫机器），使其能够解决无监督学习任务，类似于受限玻尔兹曼机器算法，并验证其学习能力。&lt;h4&gt;方法&lt;/h4&gt;基于对比局部学习网络的现有技术构建自学习电阻网络，通过两个相同网络比较不同物理状态实现对比局部学习规则。在二值化MNIST数据集上模拟训练过程，并比较其随节点增加的时间、功率和能耗扩展行为与传统计算平台上的受限玻尔兹曼机器。&lt;h4&gt;主要发现&lt;/h4&gt;成功构建了受限基尔霍夫机器并证明了其在二值化MNIST数据集上的学习能力。该机器在时间、功率和能耗方面具有特定的扩展行为，与传统计算平台上的受限玻尔兹曼机器相比有其特点。&lt;h4&gt;结论&lt;/h4&gt;受限基尔霍夫机器作为自主物理学习系统能有效解决无监督学习任务，展示了物理系统在机器学习领域的应用潜力，基于现有技术实现，具有良好的可扩展性和能效优势。&lt;h4&gt;翻译&lt;/h4&gt;自主物理学习系统修改其内部参数并解决计算任务而不依赖外部计算。与传统计算机相比，由于其物理动力学特性，它们享有分布式和节能学习的优势。在本文中，我们介绍了一种自学习电阻网络——受限基尔霍夫机器，能够解决类似于受限玻尔兹曼机器算法的无监督学习任务。该电路依赖于基于对比局部学习网络的现有技术，其中两个相同的网络比较不同的物理状态以实现对比局部学习规则。我们在二值化MNIST数据集上模拟了该机器的训练，为其学习能力提供了概念验证。最后，我们将机器随节点增加的时间、功率和能耗扩展行为与在CPU和GPU平台上运行的受限玻尔兹曼机器进行了比较。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous physical learning systems modify their internal parameters andsolve computational tasks without relying on external computation. Compared totraditional computers, they enjoy distributed and energy-efficient learning dueto their physical dynamics. In this paper, we introduce a self-learningresistor network, the Restricted Kirchhoff Machine, capable of solvingunsupervised learning tasks akin to the Restricted Boltzmann Machine algorithm.The circuit relies on existing technology based on Contrastive Local LearningNetworks, in which two identical networks compare different physical states toimplement a contrastive local learning rule. We simulate the training of themachine on the binarized MNIST dataset, providing a proof of concept of itslearning capabilities. Finally, we compare the scaling behavior of the time,power, and energy consumed per operation as more nodes are included in themachine to their Restricted Boltzmann Machine counterpart operated on CPU andGPU platforms.</description>
      <author>example@mail.com (Marcelo Guzman, Simone Ciarella, Andrea J. Liu)</author>
      <guid isPermaLink="false">2509.15842v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Transfer learning under latent space model</title>
      <link>http://arxiv.org/abs/2509.15797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种迁移学习方法，利用与目标网络潜在变量相似的源网络信息，提高目标网络中潜在变量的估计准确性，解决了潜在空间模型中参数数量众多导致的估计挑战。&lt;h4&gt;背景&lt;/h4&gt;潜在空间模型在网络分析中起着关键作用，准确估计潜在变量对下游任务如链接预测至关重要。当潜在空间维度不是特别小时，需要估计的参数数量众多，这带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种迁移学习方法，利用与目标网络潜在变量相似的网络信息，提高目标网络中潜在变量的估计准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一个两阶段迁移学习算法，适应源网络和目标网络之间节点数量的差异。在每个阶段，推导充分的识别条件并设计定制的投影梯度下降算法进行估计。当可迁移网络未知时，引入检测算法来识别合适的源网络。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析建立了所得估计器的特性，证明了所提方法能够有效处理参数估计的挑战。&lt;h4&gt;结论&lt;/h4&gt;通过模拟研究和两个真实数据集的分析，证明了所提出方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;潜在空间模型在网络分析中起着至关重要的作用，准确估计潜在变量对链接预测等下游任务至关重要。然而，当潜在空间维度不是特别小时，需要估计的大量参数带来了挑战。在本文中，我们提出了一种迁移学习方法，利用与目标网络潜在变量相似的网络中的信息，从而提高目标网络的估计准确性。给定可迁移的源网络，我们引入了一个两阶段迁移学习算法，该算法适应了源网络和目标网络之间节点数量的差异。在每个阶段，我们推导出充分的识别条件，并设计定制的投影梯度下降算法进行估计。建立了所得估计器的理论特性。当可迁移网络未知时，引入了检测算法来识别合适的源网络。模拟研究和两个真实数据集的分析证明了所提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent space model plays a crucial role in network analysis, and accurateestimation of latent variables is essential for downstream tasks such as linkprediction. However, the large number of parameters to be estimated presents achallenge, especially when the latent space dimension is not exceptionallysmall. In this paper, we propose a transfer learning method that leveragesinformation from networks with latent variables similar to those in the targetnetwork, thereby improving the estimation accuracy for the target. Giventransferable source networks, we introduce a two-stage transfer learningalgorithm that accommodates differences in node numbers between source andtarget networks. In each stage, we derive sufficient identification conditionsand design tailored projected gradient descent algorithms for estimation.Theoretical properties of the resulting estimators are established. When thetransferable networks are unknown, a detection algorithm is introduced toidentify suitable source networks. Simulation studies and analyses of two realdatasets demonstrate the effectiveness of the proposed methods.</description>
      <author>example@mail.com (Kuangnan Fang, Ruixuan Qin, Xinyan Fan)</author>
      <guid isPermaLink="false">2509.15797v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents</title>
      <link>http://arxiv.org/abs/2509.15635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 22 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MicroRCA-Agent，一种基于大型语言模型代理的微服务根因分析创新解决方案，构建了具有多模态数据融合的智能故障根因定位系统。&lt;h4&gt;背景&lt;/h4&gt;微服务环境下的故障根因分析面临挑战，需要有效的解决方案来定位故障原因。&lt;h4&gt;目的&lt;/h4&gt;构建一个能够高效处理多模态数据、智能识别故障根因的微服务分析系统。&lt;h4&gt;方法&lt;/h4&gt;三个关键技术创新：1)结合预训练Drain日志解析算法与多级数据过滤机制；2)采用集成Isolation Forest无监督学习算法与状态码验证的双重异常检测方法；3)设计统计对称比率过滤机制与两阶段LLM分析策略；4)利用精心设计的跨模态提示实现多模态异常信息的深度整合。&lt;h4&gt;主要发现&lt;/h4&gt;全面消融研究验证了每种模态数据的互补价值和系统架构的有效性，该解决方案在复杂微服务故障场景中表现出色。&lt;h4&gt;结论&lt;/h4&gt;MicroRCA-Agent在复杂微服务故障场景中实现了最终得分50.71的优越性能，代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了MicroRCA-Agent，一种基于大型语言模型代理的微服务根因分析创新解决方案，构建了具有多模态数据融合的智能故障根因定位系统。技术创新体现在三个关键方面：首先，我们将预训练的Drain日志解析算法与多级数据过滤机制相结合，高效地将大量日志压缩为高质量的故障特征。其次，我们采用双重异常检测方法，集成Isolation Forest无监督学习算法与状态码验证，实现全面的跟踪异常识别。第三，我们设计了统计对称比率过滤机制与两阶段LLM分析策略，实现跨越节点-服务-Pod层次的全栈现象总结。多模态根因分析模块利用精心设计的跨模态提示，深度整合多模态异常信息，充分利用大型语言模型的跨模态理解和逻辑推理能力，生成包含故障组件、根因描述和推理痕迹的结构化分析结果。全面的消融研究验证了每种模态数据的互补价值和系统架构的有效性。所提出的解决方案在复杂微服务故障场景中表现出色，最终得分为50.71。代码已在https://github.com/tangpan360/MicroRCA-Agent发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents MicroRCA-Agent, an innovative solution for microserviceroot cause analysis based on large language model agents, which constructs anintelligent fault root cause localization system with multimodal data fusion.The technical innovations are embodied in three key aspects: First, we combinethe pre-trained Drain log parsing algorithm with multi-level data filteringmechanism to efficiently compress massive logs into high-quality faultfeatures. Second, we employ a dual anomaly detection approach that integratesIsolation Forest unsupervised learning algorithms with status code validationto achieve comprehensive trace anomaly identification. Third, we design astatistical symmetry ratio filtering mechanism coupled with a two-stage LLManalysis strategy to enable full-stack phenomenon summarization acrossnode-service-pod hierarchies. The multimodal root cause analysis moduleleverages carefully designed cross-modal prompts to deeply integrate multimodalanomaly information, fully exploiting the cross-modal understanding and logicalreasoning capabilities of large language models to generate structured analysisresults encompassing fault components, root cause descriptions, and reasoningtrace. Comprehensive ablation studies validate the complementary value of eachmodal data and the effectiveness of the system architecture. The proposedsolution demonstrates superior performance in complex microservice faultscenarios, achieving a final score of 50.71. The code has been released at:https://github.com/tangpan360/MicroRCA-Agent.</description>
      <author>example@mail.com (Pan Tang, Shixiang Tang, Huanqi Pu, Zhiqing Miao, Zhixing Wang)</author>
      <guid isPermaLink="false">2509.15635v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant</title>
      <link>http://arxiv.org/abs/2509.15593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SETrLUSI是一种基于统计不变式的集成学习框架，用于多源迁移学习，能够提取和整合来自源域和目标域的多样化知识，加速收敛过程，并通过随机SI选择、比例源域采样和目标域自举提高训练效率和模型稳定性。&lt;h4&gt;背景&lt;/h4&gt;在迁移学习中，源域通常包含多样化的知识，不同域通常强调不同类型的知识。&lt;h4&gt;目的&lt;/h4&gt;不同于传统迁移学习方法只处理来自所有域的单一类型知识，作者引入了一种基于统计不变式(SI)的集成学习框架，用于多源迁移学习。&lt;h4&gt;方法&lt;/h4&gt;提出了SETrLUSI(Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant)，该SI提取并整合来自源域和目标域的各种类型知识，不仅有效利用多样化知识，还加速收敛过程。此外，SETrLUSI结合了随机SI选择、比例源域采样和目标域自举，提高了训练效率并增强了模型稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SETrLUSI具有良好的收敛性，并且在较低时间成本下优于相关方法。&lt;h4&gt;结论&lt;/h4&gt;SETrLUSI是一种有效的多源迁移学习方法，能够整合多样化知识并提高训练效率。&lt;h4&gt;翻译&lt;/h4&gt;在迁移学习中，源域通常携带多样化的知识，而不同的域通常强调不同类型的知识。不同于传统迁移学习方法只处理来自所有域的单一类型知识，我们引入了一种基于统计不变式(SI)的弱收敛模式集成学习框架，用于多源迁移学习，形式化为SETrLUSI(Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant)。所提出的SI提取并整合来自源域和目标域的各种类型知识，这不仅有效利用了多样化知识，还加速了收敛过程。此外，SETrLUSI结合了随机SI选择、比例源域采样和目标域自举，在提高训练效率的同时增强了模型稳定性。实验表明，SETrLUSI具有良好的收敛性，并且以较低的时间成本优于相关方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In transfer learning, a source domain often carries diverse knowledge, anddifferent domains usually emphasize different types of knowledge. Differentfrom handling only a single type of knowledge from all domains in traditionaltransfer learning methods, we introduce an ensemble learning framework with aweak mode of convergence in the form of Statistical Invariant (SI) formulti-source transfer learning, formulated as Stochastic Ensemble Multi-SourceTransfer Learning Using Statistical Invariant (SETrLUSI). The proposed SIextracts and integrates various types of knowledge from both source and targetdomains, which not only effectively utilizes diverse knowledge but alsoaccelerates the convergence process. Further, SETrLUSI incorporates stochasticSI selection, proportional source domain sampling, and target domainbootstrapping, which improves training efficiency while enhancing modelstability. Experiments show that SETrLUSI has good convergence and outperformsrelated methods with a lower time cost.</description>
      <author>example@mail.com (Chunna Li, Yiwei Song, Yuanhai Shao)</author>
      <guid isPermaLink="false">2509.15593v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder</title>
      <link>http://arxiv.org/abs/2509.15880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 figures, 7 tables. Project page: https://evggt.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了将几何感知视觉表示整合到机器人操作中的方法，提出了高效几何感知编码器eVGGT，在保持强大3D推理能力的同时，比原始VGGT快9倍且体积小5倍。&lt;h4&gt;背景&lt;/h4&gt;现有基于RGB的模仿学习方法通常使用传统视觉编码器如ResNet或ViT，这些编码器缺乏明确的3D推理能力。而最近的几何基础视觉模型如VGGT提供了强大的空间理解能力，有潜力解决这一局限性。&lt;h4&gt;目的&lt;/h4&gt;研究将几何感知视觉表示整合到机器人操作中，解决现有视觉编码器缺乏3D推理能力的问题，并提高计算效率以适应实际机器人系统部署。&lt;h4&gt;方法&lt;/h4&gt;将几何感知视觉编码器整合到模仿学习框架中（包括ACT和DP），并提出了eVGGT，这是一种从VGGT蒸馏出的高效几何感知编码器。&lt;h4&gt;主要发现&lt;/h4&gt;整合几何感知视觉编码器后，在单手和双手操作任务上，成功率比标准视觉编码器提高了最多6.5%。eVGGT比VGGT快9倍，体积小5倍，同时保留了强大的3D推理能力。&lt;h4&gt;结论&lt;/h4&gt;几何感知视觉表示可以显著提高机器人操作的性能，而eVGGT模型在保持高性能的同时大大提高了计算效率，使其更适合实际机器人系统的部署。&lt;h4&gt;翻译&lt;/h4&gt;现有的基于RGB的模仿学习方法通常采用传统的视觉编码器，如ResNet或ViT，这些编码器缺乏明确的3D推理能力。最近的几何基础视觉模型，如VGGT，提供了强大的空间理解能力，是有望解决这一局限性的候选方案。本研究探讨了将几何感知视觉表示整合到机器人操作中。我们的结果表明，将几何感知视觉编码器整合到模仿学习框架中（包括ACT和DP），在模拟和现实世界中的单手和双手操作任务上，成功率比标准视觉编码器提高了最多6.5%。尽管有这些优势，但大多数几何基础模型需要高计算成本，限制了它们在实际机器人系统中的部署。为应对这一挑战，我们提出了eVGGT，这是一种从VGGT蒸馏出的高效几何感知编码器。eVGGT比VGGT快9倍，体积小5倍，同时保留了强大的3D推理能力。代码和预训练模型将被发布，以促进几何感知机器人技术的进一步研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人操作任务中传统视觉编码器（如ResNet或ViT）缺乏3D推理能力的问题。这个问题很重要，因为许多机器人操作任务需要精确的几何理解和空间关系判断，缺乏这种能力会限制机器人在复杂环境中的操作表现。同时，现有的几何感知模型虽然性能强大，但计算成本高，难以在实际机器人系统中实时部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统视觉编码器在机器人操作中的局限性，然后借鉴了VGGT这一具有强大几何推理能力的视觉模型。他们利用知识蒸馏技术，将VGGT的知识转移到一个小型高效的模型中，同时结合数据增强和梯度损失来提升3D重建质量。这种方法既保留了VGGT的几何推理能力，又解决了其计算效率低的问题，使其适用于实时机器人系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过知识蒸馏创建一个轻量化的几何感知视觉编码器eVGGT，并将其集成到机器人操作策略中。整体流程包括：1) 使用知识蒸馏从VGGT训练出eVGGT，减少Transformer块数量并更换小型骨干网络；2) 添加梯度损失和数据增强提升3D重建质量；3) 将预训练的eVGGT冻结并集成到ACT和DP等模仿学习框架中，替换传统视觉编码器；4) 在模拟环境和真实机器人上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出eVGGT，一种高效且具有几何感知能力的视觉编码器，比原始VGGT快9倍、小5倍；2) 设计简单有效的几何感知表示集成方法，直接替换传统视觉编码器的潜在空间；3) 改进知识蒸馏训练方案，结合数据增强和梯度损失提升模型泛化能力。相比之前的工作，eVGGT在保持竞争力的3D重建性能的同时，显著提高了计算效率，使其能够在资源受限的机器人系统中实时运行，而无需外部处理或额外传感器输入。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 作者提出了eVGGT，一种高效几何感知视觉编码器，通过知识蒸馏从VGGT中学习，显著提升了机器人操作的成功率，同时保持计算效率，使几何感知的视觉表示能够在实时机器人系统中实际应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing RGB-based imitation learning approaches typically employ traditionalvision encoders such as ResNet or ViT, which lack explicit 3D reasoningcapabilities. Recent geometry-grounded vision models, such asVGGT~\cite{wang2025vggt}, provide robust spatial understanding and arepromising candidates to address this limitation. This work investigates theintegration of geometry-aware visual representations into robotic manipulation.Our results suggest that incorporating the geometry-aware vision encoder intoimitation learning frameworks, including ACT and DP, yields up to 6.5%improvement over standard vision encoders in success rate across single- andbi-manual manipulation tasks in both simulation and real-world settings.Despite these benefits, most geometry-grounded models require highcomputational cost, limiting their deployment in practical robotic systems. Toaddress this challenge, we propose eVGGT, an efficient geometry-aware encoderdistilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller thanVGGT, while preserving strong 3D reasoning capabilities. Code and pretrainedmodels will be released to facilitate further research in geometry-awarerobotics.</description>
      <author>example@mail.com (An Dinh Vuong, Minh Nhat Vu, Ian Reid)</author>
      <guid isPermaLink="false">2509.15880v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation</title>
      <link>http://arxiv.org/abs/2509.15772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VLM3D是一种新型文本到3D生成框架，通过集成大型视觉语言模型(VLMs)到SDS管道中，解决了现有SDS方法在语义对齐和3D空间约束方面的局限性，实现了更高质量的3D生成。&lt;h4&gt;背景&lt;/h4&gt;Score Distillation Sampling (SDS)通过监督3D模型的去噪过程，使用预训练文本到图像扩散模型实现高质量文本到3D生成，但现有方法存在两个基本限制：依赖CLIP风格文本编码器导致粗略语义对齐，以及2D扩散先验缺乏明确3D空间约束。&lt;h4&gt;目的&lt;/h4&gt;解决现有SDS方法在语义对齐和3D空间约束方面的局限性，提高文本到3D生成的质量，特别是在语义保真度、几何一致性和空间正确性方面。&lt;h4&gt;方法&lt;/h4&gt;提出VLM3D框架，将大型视觉语言模型(VLMs)集成到SDS管道中作为可区分的语义和空间先验，利用VLMs的语言监督实现细粒度提示对齐，并通过其视觉语言建模能力增强空间理解。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，VLM3D在各种物体和复杂场景中显著优于之前的基于SDS的方法，在语义保真度、几何一致性和空间正确性方面表现更佳，特别是在单对象生成的3D一致性和多对象场景中的关系推理方面有显著提升。&lt;h4&gt;结论&lt;/h4&gt;VLM3D通过整合大型视觉语言模型到文本到3D生成流程中，有效解决了现有SDS方法的局限性，实现了更高质量的3D内容生成。&lt;h4&gt;翻译&lt;/h4&gt;评分蒸馏采样(SDS)通过监督多视角2D渲染的去噪过程来指导3D模型，使用预训练的文本到图像扩散模型与输入提示保持一致并确保3D一致性，从而实现高质量的文本到3D生成。然而，现有的基于SDS的方法面临两个基本限制：(1)它们对CLIP风格文本编码器的依赖导致粗略的语义对齐，难以处理细粒度提示；(2)2D扩散先验缺乏明确的3D空间约束，导致多对象场景中几何不一致和物体关系不准确。为应对这些挑战，我们提出了VLM3D，一种新型文本到3D生成框架，将大型视觉语言模型(VLMs)集成到SDS管道中作为可区分的语义和空间先验。与标准的文本到图像扩散先验不同，VLMs利用丰富的语言监督，能够实现细粒度的提示对齐。此外，它们固有的视觉语言建模提供了强大的空间理解能力，显著提高了单对象生成的3D一致性，并改善了多对象场景中的关系推理。我们基于开源的Qwen2.5-VL模型实例化了VLM3D，并在GPTeval3D基准上进行了评估。在各种物体和复杂场景的实验中表明，VLM3D在语义保真度、几何一致性和空间正确性方面明显优于之前的基于SDS的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决文本到3D生成中的两个关键问题：一是现有方法依赖CLIP文本编码器导致粗略语义对齐，难以处理细粒度提示；二是2D扩散先验缺乏明确3D空间约束，造成多视角几何不一致和对象关系不准确。这些问题在现实研究中很重要，因为高质量文本到3D生成在游戏开发、虚拟现实、数字孪生等领域有广泛应用，而现有方法生成的3D资产常与文本提示不匹配或视角不一致，限制了技术的实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了SDS方法的局限性，然后观察到视觉语言模型(VLMs)具有更强的跨模态语义理解和空间推理能力。他们借鉴了SDS的基本范式和VLMs的研究成果，特别是Qwen2.5-VL模型。设计上，他们将VLMs集成到SDS管道中作为可微分奖励信号，设计了双查询提示分别关注内容匹配和几何质量，开发了完全可微分的训练流程，并使用动态权重策略平衡VLM奖励和SDS损失。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将大型视觉语言模型(VLMs)集成到文本到3D生成流程中，作为可微分的语义和空间奖励信号，替代传统的2D扩散模型监督。实现流程包括：1)从3D模型渲染多视角图像；2)将图像和文本输入VLM进行双查询评估；3)计算VLM奖励；4)计算SDS损失；5)结合两个损失并使用动态权重策略更新3D参数；6)迭代优化直到收敛。关键设计包括双查询提示、动态权重调度和端到端可微训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将VLMs作为可微分语义和空间奖励引入文本到3D生成；2)设计双查询提示同时评估内容匹配和几何质量；3)采用动态权重调度策略平衡约束和细节；4)实现端到端可微训练流程。相比之前的工作，VLM3D不再仅依赖2D扩散模型的隐式监督，而是利用VLM的显式语义和空间约束；不依赖人类偏好或人工设计奖励，而是利用VLM内置的理解能力；从隐式分数蒸馏转向显式语义优化，强调多视角一致性而非单视角质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VLM3D通过将大型视觉语言模型作为可微分的语义和空间奖励集成到文本到3D生成流程中，显著提高了生成模型的语义对齐精度和3D一致性，解决了现有方法在处理细粒度提示和复杂空间关系时的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Score Distillation Sampling (SDS) enables high-quality text-to-3D generationby supervising 3D models through the denoising of multi-view 2D renderings,using a pretrained text-to-image diffusion model to align with the input promptand ensure 3D consistency. However, existing SDS-based methods face twofundamental limitations: (1) their reliance on CLIP-style text encoders leadsto coarse semantic alignment and struggles with fine-grained prompts; and (2)2D diffusion priors lack explicit 3D spatial constraints, resulting ingeometric inconsistencies and inaccurate object relationships in multi-objectscenes. To address these challenges, we propose VLM3D, a novel text-to-3Dgeneration framework that integrates large vision-language models (VLMs) intothe SDS pipeline as differentiable semantic and spatial priors. Unlike standardtext-to-image diffusion priors, VLMs leverage rich language-groundedsupervision that enables fine-grained prompt alignment. Moreover, theirinherent vision language modeling provides strong spatial understanding, whichsignificantly enhances 3D consistency for single-object generation and improvesrelational reasoning in multi-object scenes. We instantiate VLM3D based on theopen-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark.Experiments across diverse objects and complex scenes show that VLM3Dsignificantly outperforms prior SDS-based methods in semantic fidelity,geometric coherence, and spatial correctness.</description>
      <author>example@mail.com (Weimin Bai, Yubo Li, Weijian Luo, Wenzheng Chen, He Sun)</author>
      <guid isPermaLink="false">2509.15772v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</title>
      <link>http://arxiv.org/abs/2509.15536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages,15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAMPO是一种混合框架，结合了视觉自回归建模和因果建模，有效解决了现有自回归世界模型在视觉连贯性预测方面的问题，显著提高了视频预测质量和推理效率。&lt;h4&gt;背景&lt;/h4&gt;世界模型允许智能体在想象的环境中模拟行动后果，用于规划、控制和长时程决策。然而，现有的自回归世界模型由于空间结构破坏、解码效率不足和运动建模不充分，在视觉连贯性预测方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合框架，解决现有自回归世界模型在视觉连贯性预测方面的问题，改进视频预测和基于模型的控制性能。&lt;h4&gt;方法&lt;/h4&gt;提出了名为SAMPO（Scale-wise Autoregression with Motion Prompt）的混合框架，结合帧内生成的视觉自回归建模和下一帧生成的因果建模；集成时间因果解码与双向空间注意力，保持空间局部性并支持并行解码；设计非对称多尺度标记器，保留观察帧空间细节同时提取未来帧的动态表示；引入轨迹感知运动提示模块，注入时空线索关注动态区域。&lt;h4&gt;主要发现&lt;/h4&gt;SAMPO在条件动作视频预测和基于模型的控制方面取得了有竞争力的性能；生成质量提高，推理速度提高了4.4倍；能够泛化到未见过的任务并从更大的模型尺寸中受益。&lt;h4&gt;结论&lt;/h4&gt;SAMPO通过结合视觉自回归建模和因果建模，有效解决了现有自回归世界模型在视觉连贯性预测方面的问题，其设计显著提高了时间一致性和展开效率，同时优化了内存使用和模型性能。&lt;h4&gt;翻译&lt;/h4&gt;世界模型允许智能体在想象的环境中模拟行动后果，用于规划、控制和长时程决策。然而，现有的自回归世界模型由于空间结构破坏、解码效率不足和运动建模不充分，在视觉连贯性预测方面存在困难。为此，我们提出了SAMPO（Scale-wise Autoregression with Motion Prompt），这是一种混合框架，结合了帧内生成的视觉自回归建模和下一帧生成的因果建模。具体来说，SAMPO将时间因果解码与双向空间注意力相结合，保留了空间局部性并支持每个尺度内的并行解码。这种设计显著提高了时间一致性和展开效率。为了进一步改进动态场景理解，我们设计了一种非对称多尺度标记器，在保留观察帧空间细节的同时，为未来帧提取紧凑的动态表示，优化了内存使用和模型性能。此外，我们引入了轨迹感知运动提示模块，注入物体和机器人轨迹的时空线索，将注意力集中在动态区域，提高了时间一致性和物理真实性。大量实验表明，SAMPO在条件动作视频预测和基于模型的控制方面取得了有竞争力的性能，生成质量提高了4.4倍，推理速度提高了4.4倍。我们还评估了SAMPO的零样本泛化能力和扩展行为，证明了其能够泛化到未见过的任务并从更大的模型尺寸中受益。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有自回归世界模型在视觉预测中面临的视觉连贯性差问题，具体表现为空间结构被破坏、解码效率低以及对运动建模不足。这个问题很重要，因为世界模型能让代理人在想象环境中模拟动作后果，用于规划和决策，而视觉连贯性对于准确预测环境变化至关重要，空间结构破坏会导致不合理的视觉内容生成，效率低限制了实际应用，运动建模不足会影响动态场景的真实性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法（掩码建模、扩散模型和自回归模型）的局限性，发现自回归模型虽保留因果结构但面临结构退化、解码缓慢和运动建模不足三大挑战。基于此，作者设计了一个结合帧内双向空间注意力和跨时间因果建模的框架。借鉴了VAR的多尺度令牌预测替代光栅扫描、VQ-VAE的令牌化思想、视觉提示技术特别是运动感知提示，以及类似GPT的Transformer架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是尺度自回归，结合时间因果建模和空间粗到细生成；不对称多尺度令牌化，平衡空间细节和动态建模；轨迹感知运动提示，指导模型关注动态区域。整体流程：1)输入处理将帧转换为令牌图；2)观察帧密集令牌化，未来帧稀疏令牌化；3)提取动态轨迹作为运动提示；4)时间上自回归生成未来帧，空间上从粗到细生成每个帧的令牌图；5)使用多尺度交叉熵损失训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)混合自回归架构结合时间因果和空间粗到细生成；2)不对称多尺度令牌器优化内存和性能；3)轨迹感知运动提示增强动态交互建模。不同之处：与传统光栅扫描相比保留空间局部性；与单一尺度令牌化相比采用不对称策略；与静态提示相比引入动态时空线索；与纯时间或空间自回归不同采用时空结合架构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAMPO通过结合尺度自回归、不对称多尺度令牌化和轨迹感知运动提示，解决了现有世界模型在视觉预测中面临的视觉连贯性差、解码效率低和运动建模不足的问题，实现了高质量、高效率的动态场景预测和机器人控制。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; World models allow agents to simulate the consequences of actions in imaginedenvironments for planning, control, and long-horizon decision-making. However,existing autoregressive world models struggle with visually coherentpredictions due to disrupted spatial structure, inefficient decoding, andinadequate motion modeling. In response, we propose \textbf{S}cale-wise\textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt(\textbf{SAMPO}), a hybrid framework that combines visual autoregressivemodeling for intra-frame generation with causal modeling for next-framegeneration. Specifically, SAMPO integrates temporal causal decoding withbidirectional spatial attention, which preserves spatial locality and supportsparallel decoding within each scale. This design significantly enhances bothtemporal consistency and rollout efficiency. To further improve dynamic sceneunderstanding, we devise an asymmetric multi-scale tokenizer that preservesspatial details in observed frames and extracts compact dynamic representationsfor future frames, optimizing both memory usage and model performance.Additionally, we introduce a trajectory-aware motion prompt module that injectsspatiotemporal cues about object and robot trajectories, focusing attention ondynamic regions and improving temporal consistency and physical realism.Extensive experiments show that SAMPO achieves competitive performance inaction-conditioned video prediction and model-based control, improvinggeneration quality with 4.4$\times$ faster inference. We also evaluate SAMPO'szero-shot generalization and scaling behavior, demonstrating its ability togeneralize to unseen tasks and benefit from larger model sizes.</description>
      <author>example@mail.com (Sen Wang, Jingyi Tian, Le Wang, Zhimin Liao, Jiayi Li, Huaiyi Dong, Kun Xia, Sanping Zhou, Wei Tang, Hua Gang)</author>
      <guid isPermaLink="false">2509.15536v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters</title>
      <link>http://arxiv.org/abs/2509.15490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures, IEEE/CVF International Conference on Computer  Vision Workshops (ICCVW)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SmolRGPT，一种紧凑的视觉语言模型架构，通过整合RGB和深度线索实现区域级空间推理，仅需6亿参数就能在仓库空间推理基准测试中与更大模型相媲美。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的视觉语言模型(VLMs)虽然能实现强大的多模态推理，但通常依赖超大型模型，计算和内存需求极高，难以在资源受限的仓库、机器人或工业环境中部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且具有强大空间推理能力的视觉语言模型，使其能够在资源受限环境中部署，同时保持高性能的空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出SmolRGPT架构，通过整合RGB和深度线索实现区域级空间推理，并采用三阶段课程学习策略：逐步对齐视觉和语言特征、实现空间关系理解、适应特定任务数据集。&lt;h4&gt;主要发现&lt;/h4&gt;SmolRGPT仅使用6亿参数，就在具有挑战性的仓库空间推理基准测试中达到了与更大替代模型相匹配或超越的性能表现。&lt;h4&gt;结论&lt;/h4&gt;研究证明，可以在不牺牲核心空间推理能力的情况下，在现实场景中实现高效、可部署的多模态智能，为资源受限环境提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近的视觉语言模型(VLMs)进展已经实现了强大的多模态推理能力，但最先进的方法通常依赖于具有极高计算和内存要求的超大型模型。这使得在资源受限环境(如仓库、机器人和工业应用)中部署这些模型变得困难，而这些环境既需要高效性，也需要稳健的空间理解能力。在这项工作中，我们提出了SmolRGPT，一种紧凑的视觉语言架构，它通过整合RGB和深度线索，明确地融入了区域级别的空间推理能力。SmolRGPT采用三阶段课程学习方法，逐步对齐视觉和语言特征，实现空间关系理解，并适应特定任务的数据集。我们证明，仅使用6亿参数，SmolRGPT就在具有挑战性的仓库空间推理基准测试中取得了与更大的替代模型相匹配或更好的性能。这些发现突显了在现实场景中实现高效、可部署的多模态智能的潜力，而无需牺牲核心的空间推理能力。实验代码将在https://github.com/abtraore/SmolRGPT上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在资源受限环境（如仓库、机器人、工业应用）中部署高效空间推理模型的问题。现实中，这些环境既需要强大的空间理解能力来理解物体位置、排列和几何关系，又受限于硬件和内存资源，而当前最先进的视觉语言模型通常参数量极大（数十亿甚至上万亿），难以在这些环境中部署。这个问题的重要性在于它限制了AI技术在物流自动化、机器人导航等实际场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有大型视觉语言模型在空间推理任务上的局限性，特别是在资源受限环境中的部署问题。他们借鉴了多个现有工作：基于SmolVLM构建并集成了SpatialRGPT的区域级视觉语言建模技术；使用NanoVLM作为基础框架；采用预训练的视觉特征提取器siglip2-base-patch16-256；参考SpatialGPT的方法保持RGB和深度输入的独立路径；借鉴RegionGPT的设计使用专门的refiner模块。作者通过创建紧凑架构、结合RGB和深度信息、采用像素重排技术和三阶段课程学习等方法，设计出了这个高效的空间推理模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个仅6亿参数的紧凑视觉语言模型，通过同时处理RGB和深度信息来实现区域级空间推理，使其能够在资源受限环境中部署。整体流程包括：1)使用预训练视觉特征提取器处理RGB和深度图像；2)通过模态特定的连接器将特征映射到语言模型空间；3)使用refiner模块细化特征；4)通过掩码池化提取区域特征；5)将区域特征插入语言模型；6)采用三阶段训练策略：先在LLaVA-CC3M上训练RGB连接器，再在OSD上预热深度组件，最后在仓库数据集上微调。推理时，模型将特殊标记替换为区域特定嵌入，生成对空间查询的自然语言回答。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)仅用6亿参数就实现了强大的空间推理能力；2)同时处理RGB和深度信息，通过独立路径保持模态独特性；3)使用像素重排投影技术提供更密集的特征表示；4)采用三阶段课程学习逐步构建能力；5)高效的区域级处理通过掩码池化实现。相比之前的工作，SmolRGPT在模型规模上比SpatialRGPT(80亿参数)和LLaVA-34B(340亿参数)小得多，在架构上更简洁且明确整合了深度信息，在训练上采用渐进式策略，在应用上特别针对仓库环境优化，能够处理实际工业场景中的空间推理任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SmolRGPT通过创新的紧凑架构和三阶段训练方法，证明了仅用6亿参数的模型就能在资源受限环境中实现与大型模型相当的空间推理能力，为仓库和工业应用中的高效AI部署开辟了新途径。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision-language models (VLMs) have enabled powerfulmultimodal reasoning, but state-of-the-art approaches typically rely onextremely large models with prohibitive computational and memory requirements.This makes their deployment challenging in resource-constrained environmentssuch as warehouses, robotics, and industrial applications, where bothefficiency and robust spatial understanding are critical. In this work, wepresent SmolRGPT, a compact vision-language architecture that explicitlyincorporates region-level spatial reasoning by integrating both RGB and depthcues. SmolRGPT employs a three-stage curriculum that progressively align visualand language features, enables spatial relationship understanding, and adaptsto task-specific datasets. We demonstrate that with only 600M parameters,SmolRGPT achieves competitive results on challenging warehouse spatialreasoning benchmarks, matching or exceeding the performance of much largeralternatives. These findings highlight the potential for efficient, deployablemultimodal intelligence in real-world settings without sacrificing core spatialreasoning capabilities. The code of the experimentation will be available at:https://github.com/abtraore/SmolRGPT</description>
      <author>example@mail.com (Abdarahmane Traore, Éric Hervet, Andy Couturier)</author>
      <guid isPermaLink="false">2509.15490v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SPATIALGEN: Layout-guided 3D Indoor Scene Generation</title>
      <link>http://arxiv.org/abs/2509.14981v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3D scene generation; diffusion model; Scene reconstruction and  understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SpatialGen的新型多视图多模态扩散模型，用于生成高质量且语义一致的3D室内场景，同时提供了一个包含12,328个结构化注释场景和470万张逼真2D渲染图的大型数据集。&lt;h4&gt;背景&lt;/h4&gt;创建高保真室内环境3D模型对设计、虚拟现实和机器人应用至关重要，但手动3D建模耗时且劳动密集。现有生成AI方法在平衡视觉质量、多样性和用户控制方面存在挑战，主要瓶颈是缺乏大规模高质量数据集。&lt;h4&gt;目的&lt;/h4&gt;解决缺乏大规模高质量数据集的问题，开发能够生成真实且语义一致的3D室内场景的方法。&lt;h4&gt;方法&lt;/h4&gt;引入综合合成数据集，包含12,328个结构化注释场景、57,440个房间和470万张逼真2D渲染图；提出SpatialGen模型，根据3D布局和参考图像从任意视角合成外观、几何和语义信息，同时保持跨模态空间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;SpatialGen在实验中持续产生优于先前方法的结果，能够生成高质量、多样化且语义一致的3D室内场景。&lt;h4&gt;结论&lt;/h4&gt;开源数据和模型以促进社区发展，推动室内场景理解和生成领域的进步。&lt;h4&gt;翻译&lt;/h4&gt;创建室内环境的高保真3D模型对设计、虚拟现实和机器人应用至关重要。然而，手动3D建模仍然耗时且劳动密集。虽然最近生成AI的进步实现了自动化场景合成，但现有方法在平衡视觉质量、多样性、语义一致性和用户控制方面常常面临挑战。主要瓶颈是缺乏针对此任务的大规模高质量数据集。为解决这一差距，我们引入了一个综合的合成数据集，包含12,328个结构化注释场景，57,440个房间，和470万张逼真的2D渲染图。利用此数据集，我们提出了SpatialGen，一种新颖的多视图多模态扩散模型，能够生成真实且语义一致的3D室内场景。给定3D布局和参考图像（从文本提示派生），我们的模型从任意视角合成外观（彩色图像）、几何（场景坐标图）和语义（语义分割图），同时保持跨模态的空间一致性。在实验中，SpatialGen持续产生优于先前方法的结果。我们开源了我们的数据和模型，以赋能社区并推动室内场景理解和生成领域的发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何自动生成高质量、多样化的3D室内场景模型的问题。这个问题在现实中非常重要，因为室内3D模型对设计、虚拟现实、机器人应用等领域至关重要，而手动3D建模既耗时又耗费人力。现有方法在平衡视觉质量、多样性、语义一致性和用户控制方面存在困难，缺乏大规模高质量数据集是主要瓶颈。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计解决方案：程序化建模方法缺乏多样性，3D生成方法布局和外观真实性有限，基于图像的方法面临多视图语义一致性挑战。作者借鉴了扩散模型技术、多视图扩散方法、3D语义布局先验和3D高斯溅射技术，创新性地结合这些技术，设计了一个基于3D布局先验的多视图多模态扩散模型，并创建了大规模数据集来支持模型训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是基于给定的3D语义布局，生成真实且语义一致的3D室内场景，利用多视图多模态扩散模型从任意视点合成外观、几何和语义信息，同时保持跨模态的空间一致性。整体流程包括：1)将3D语义布局转换为特定视图表示；2)设计布局引导的注意力机制，包括跨视图和跨模态注意力；3)采用迭代多视图生成策略确保完整场景覆盖；4)使用3D高斯溅射优化重建显式辐射场实现自由视点渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了一个包含12,328个场景、57,440个房间和470万张渲染的大规模数据集；2)提出SPATIALGEN框架，实现基于3D布局先验的多视图多模态图像扩散；3)开发场景坐标图VAE(SCM-VAE)提高几何重建质量；4)设计迭代密集视图生成策略确保一致性。相比之前的工作，SPATIALGEN避免了基于分数蒸馏方法的严重视觉伪影，突破了全景作为代理方法仅限于固定相机位置的局限，在语义一致性、视觉质量和场景多样性方面表现出色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPATIALGEN通过大规模数据集和创新的布局引导多视图多模态扩散模型，实现了高质量、语义一致的3D室内场景生成，显著超越了现有方法在视觉质量、多样性和一致性方面的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Creating high-fidelity 3D models of indoor environments is essential forapplications in design, virtual reality, and robotics. However, manual 3Dmodeling remains time-consuming and labor-intensive. While recent advances ingenerative AI have enabled automated scene synthesis, existing methods oftenface challenges in balancing visual quality, diversity, semantic consistency,and user control. A major bottleneck is the lack of a large-scale, high-qualitydataset tailored to this task. To address this gap, we introduce acomprehensive synthetic dataset, featuring 12,328 structured annotated sceneswith 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging thisdataset, we present SpatialGen, a novel multi-view multi-modal diffusion modelthat generates realistic and semantically consistent 3D indoor scenes. Given a3D layout and a reference image (derived from a text prompt), our modelsynthesizes appearance (color image), geometry (scene coordinate map), andsemantic (semantic segmentation map) from arbitrary viewpoints, whilepreserving spatial consistency across modalities. SpatialGen consistentlygenerates superior results to previous methods in our experiments. We areopen-sourcing our data and models to empower the community and advance thefield of indoor scene understanding and generation.</description>
      <author>example@mail.com (Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan)</author>
      <guid isPermaLink="false">2509.14981v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark</title>
      <link>http://arxiv.org/abs/2509.14574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了一个小型基准测试，用于评估视觉语言模型(VLMs)对城市场景的理解能力，通过蒙特利尔街道图像和人类标注数据进行比较分析。&lt;h4&gt;背景&lt;/h4&gt;理解人们如何阅读城市场景可以为城市设计和规划提供重要信息，但当前缺乏专门评估VLMs在城市感知任务上的基准测试。&lt;h4&gt;目的&lt;/h4&gt;创建一个小型基准测试，评估VLMs对城市场景的理解能力，并比较模型与人类在城市感知上的差异。&lt;h4&gt;方法&lt;/h4&gt;使用100张蒙特利尔街道图像(真实照片和合成场景各半)，12名来自7个社区组的参与者提供230份标注，涵盖30个维度(物理属性和主观印象)；评估7个VLMs在zero-shot设置下的表现；使用准确性评估单选项目，Jaccard重叠评估多标签项目；人类一致性通过Krippendorff's alpha和成对Jaccard衡量。&lt;h4&gt;主要发现&lt;/h4&gt;模型在可见、客观属性上的表现优于主观评价；最佳系统(claude-sonnet)在多标签项目上达到macro 0.31和平均Jaccard 0.48；人类一致性高的维度模型表现更好；合成图像导致模型分数略低。&lt;h4&gt;结论&lt;/h4&gt;发布的基准测试、提示和工具有助于在参与式城市分析中进行可复现、不确定性感知的评估，为城市规划提供AI辅助工具。&lt;h4&gt;翻译&lt;/h4&gt;了解人们如何阅读城市场景可以为设计和规划提供信息。我们引入了一个小型基准测试，使用100张蒙特利尔街道图像( evenly split between photographs 和 photorealistic synthetic scenes)来测试视觉语言模型(VLMs)的城市感知能力。来自7个社区组的12名参与者提供了230份注释表，涵盖30个维度，混合了物理属性和主观印象。法语响应被标准化为英语。我们在zero-shot设置下使用结构化提示和确定性解析器评估了7个VLMs。对于单选项目使用准确性，对于多标签项目使用Jaccard重叠；人类一致性使用Krippendorff's alpha和成对Jaccard。结果表明，模型在可见、客观属性上的对齐比主观评价更强。最佳系统(claude-sonnet)在多标签项目上达到macro 0.31和平均Jaccard 0.48。更高的人类一致性对应更好的模型分数。合成图像略微降低了分数。我们发布了基准测试、提示和工具，用于在参与式城市分析中进行可复现、不确定性感知的评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how people read city scenes can inform design and planning. Weintroduce a small benchmark for testing vision-language models (VLMs) on urbanperception using 100 Montreal street images, evenly split between photographsand photorealistic synthetic scenes. Twelve participants from seven communitygroups supplied 230 annotation forms across 30 dimensions mixing physicalattributes and subjective impressions. French responses were normalized toEnglish. We evaluated seven VLMs in a zero-shot setup with a structured promptand deterministic parser. We use accuracy for single-choice items and Jaccardoverlap for multi-label items; human agreement uses Krippendorff's alpha andpairwise Jaccard. Results suggest stronger model alignment on visible,objective properties than subjective appraisals. The top system (claude-sonnet)reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher humanagreement coincides with better model scores. Synthetic images slightly lowerscores. We release the benchmark, prompts, and harness for reproducible,uncertainty-aware evaluation in participatory urban analysis.</description>
      <author>example@mail.com (Rashid Mushkani)</author>
      <guid isPermaLink="false">2509.14574v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
  <item>
      <title>Biologically Plausible Online Hebbian Meta-Learning: Two-Timescale Local Rules for Spiking Neural Brain Interfaces</title>
      <link>http://arxiv.org/abs/2509.14447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, submitted to ICLR 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于局部三因子学习规则和双时间尺度资格迹的在线脉冲神经网络解码器，解决了脑机接口中神经信号不稳定和内存限制的问题。&lt;h4&gt;背景&lt;/h4&gt;脑机接口在实时植入式应用中面临神经信号不稳定和内存约束的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种避免通过时间反向传播同时保持竞争性能的在线SNN解码器，实现内存高效的神经解码。&lt;h4&gt;方法&lt;/h4&gt;结合误差调节的Hebbian更新、快速/慢速迹合并和自适应学习率控制，仅需O(1)内存，相比BPTT方法的O(T)内存大幅降低内存需求。&lt;h4&gt;主要发现&lt;/h4&gt;在两个灵长类动物数据集上实现了可比的解码准确率（Zenodo数据集Pearson R≥0.63，MC Maze数据集R≥0.81），内存减少28-35%，且比BPTT训练的SNN收敛更快。闭环模拟展示了适应神经干扰和无需离线校准即可从零开始学习的能力。&lt;h4&gt;结论&lt;/h4&gt;该研究实现了内存高效、持续自适应的神经解码，适用于资源受限的植入式BCI系统。&lt;h4&gt;翻译&lt;/h4&gt;Brain-Computer Interfaces面临来自神经信号不稳定和内存限制的挑战，用于实时植入式应用。我们引入了一种使用局部三因子学习规则和双时间尺度资格迹的在线SNN解码器，避免了通过时间反向传播，同时保持竞争性能。我们的方法结合了误差调节的Hebbian更新、快速/慢速迹合并和自适应学习率控制，仅需O(1)内存，而BPTT方法需要O(T)内存。在两个灵长类数据集上的评估实现了可比的解码准确率（Pearson R≥0.63 Zenodo，R≥0.81 MC Maze），内存减少28-35%，并且比BPTT训练的SNN收敛更快。使用合成神经群体的闭环模拟展示了适应神经干扰和无需离线校准即可从零开始学习的能力。这项工作实现了内存高效、持续自适应的神经解码，适用于资源受限的植入式BCI系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-Computer Interfaces face challenges from neural signal instability andmemory constraints for real-time implantable applications. We introduce anonline SNN decoder using local three-factor learning rules with dual-timescaleeligibility traces that avoid backpropagation through time while maintainingcompetitive performance. Our approach combines error-modulated Hebbian updates,fast/slow trace consolidation, and adaptive learning rate control, requiringonly O(1) memory versus O(T) for BPTT methods. Evaluations on two primatedatasets achieve comparable decoding accuracy (Pearson $R \geq 0.63$ Zenodo, $R\geq 0.81$ MC Maze) with 28-35% memory reduction and faster convergence thanBPTT-trained SNNs. Closed-loop simulations with synthetic neural populationsdemonstrate adaptation to neural disruptions and learning from scratch withoutoffline calibration. This work enables memory-efficient, continuously adaptiveneural decoding suitable for resource-constrained implantable BCI systems.</description>
      <author>example@mail.com (Sriram V. C. Nallani, Gautham Ramachandran, Sahil S. Shah)</author>
      <guid isPermaLink="false">2509.14447v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</title>
      <link>http://arxiv.org/abs/2509.15220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE T-PAMI 2025. Code: https://github.com/cvg/diffmvs&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于扩散模型的新型多视图立体(MVS)框架，通过条件扩散过程进行深度细化，结合轻量级网络和置信度采样策略，实现了高效且高质量的3D几何重建。&lt;h4&gt;背景&lt;/h4&gt;传统的基于学习的多视图立体方法通过多视图深度估计和深度图融合来重建3D几何形状，通常采用初始化粗糙深度图再逐步细化的策略以提高效率。扩散模型在生成任务中表现出色，通过迭代去噪过程从随机噪声中恢复样本。&lt;h4&gt;目的&lt;/h4&gt;将扩散模型引入多视图立体(MVS)框架，以提高3D几何重建的效率和性能，特别是在深度估计和细化方面。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新型MVS框架，将深度细化表述为条件扩散过程，设计了条件编码器指导扩散过程，提出了结合轻量级2D U-Net和卷积GRU的扩散网络，以及基于置信度的自适应采样策略。基于此框架开发了DiffMVS和CasDiffMVS两种方法。&lt;h4&gt;主要发现&lt;/h4&gt;DiffMVS在运行时间和GPU内存方面实现了与最先进方法相竞争的性能；CasDiffMVS在DTU、Tanks &amp; Temples和ETH3D数据集上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;将扩散模型应用于多视图立体任务可以有效提高3D几何重建的效率和性能，所提出的框架和方法为该领域提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;从校准图像重建3D几何形状时，基于学习的多视图立体(MVS)方法通常执行多视图深度估计，然后将深度图融合到网格或点云中。为了提高计算效率，许多方法初始化一个粗糙的深度图，然后在更高分辨率下逐步细化它。最近，扩散模型在生成任务中取得了巨大成功。从随机噪声开始，扩散模型通过迭代去噪过程逐渐恢复样本。在本文中，我们提出了一种新的MVS框架，在MVS中引入了扩散模型。具体来说，我们将深度细化表述为条件扩散过程。考虑到深度估计的判别性特征，我们设计了一个条件编码器来指导扩散过程。为了提高效率，我们提出了一种结合轻量级2D U-Net和卷积GRU的新型扩散网络。此外，我们提出了一种基于置信度的采样策略，根据扩散模型估计的置信度自适应地对深度假设进行采样。基于我们新的MVS框架，我们提出了两种新的MVS方法：DiffMVS和CasDiffMVS。DiffMVS在运行时间和GPU内存方面实现了与最先进方法相竞争的性能。CasDiffMVS在DTU、Tanks &amp; Temples和ETH3D上实现了最先进的性能。代码可在https://github.com/cvg/diffmvs获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多视图立体(MVS)中的3D几何重建问题，特别是在保持计算效率的同时提高重建精度的问题。这个问题在现实中非常重要，因为MVS在机器人、自动驾驶、虚拟/混合现实和'元宇宙'等场景有广泛应用。现有的方法通常面临效率和精度之间的权衡，且在光照变化、低纹理区域等挑战性条件下表现不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有MVS方法的局限性，特别是离散深度采样和局部最小值问题。他们注意到扩散模型在生成任务中的成功，并思考如何将其应用于深度估计这一判别性任务。作者借鉴了多个现有工作：扩散模型的去噪过程、PatchMatch的最近邻搜索思想、RAFT中使用GRU进行迭代细化的方法、粗到细的多阶段框架，以及置信度估计在立体匹配中的应用。基于这些借鉴，作者设计了条件编码器、轻量级扩散网络和基于置信度的采样策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将扩散模型引入多视图立体匹配，将深度细化过程表述为条件扩散过程，通过引入随机噪声避免陷入局部最小值。整体流程包括：1)深度初始化：构建代价体积并预测初始深度图；2)基于扩散的细化：添加噪声并通过条件扩散模型去噪，使用条件编码器提供指导，结合2D U-Net和卷积GRU进行迭代细化；3)基于置信度的采样：根据估计的置信度自适应生成深度假设；4)学习上采样：将深度图上采样到全分辨率；5)训练损失：使用L1损失和置信度加权损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于条件扩散模型的新型MVS框架；2)融合匹配信息、图像上下文和深度上下文的条件编码器；3)基于置信度的自适应采样策略；4)结合轻量级2D U-Net和卷积GRU的高效扩散网络；5)提出的DiffMVS和CasDiffMVS两种方法。相比之前的工作，本文将生成式扩散模型应用于判别性深度估计任务，避免了传统3D卷积和注意力机制的高计算成本，同时通过随机噪声和条件指导提高了重建质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于条件扩散模型的新型多视图立体框架，通过轻量级网络设计和基于置信度的采样策略，实现了高效且准确的3D几何重建，在保持计算效率的同时达到了最先进的重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TPAMI.2025.3597148&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To reconstruct the 3D geometry from calibrated images, learning-basedmulti-view stereo (MVS) methods typically perform multi-view depth estimationand then fuse depth maps into a mesh or point cloud. To improve thecomputational efficiency, many methods initialize a coarse depth map and thengradually refine it in higher resolutions. Recently, diffusion models achievegreat success in generation tasks. Starting from a random noise, diffusionmodels gradually recover the sample with an iterative denoising process. Inthis paper, we propose a novel MVS framework, which introduces diffusion modelsin MVS. Specifically, we formulate depth refinement as a conditional diffusionprocess. Considering the discriminative characteristic of depth estimation, wedesign a condition encoder to guide the diffusion process. To improveefficiency, we propose a novel diffusion network combining lightweight 2D U-Netand convolutional GRU. Moreover, we propose a novel confidence-based samplingstrategy to adaptively sample depth hypotheses based on the confidenceestimated by diffusion model. Based on our novel MVS framework, we propose twonovel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitiveperformance with state-of-the-art efficiency in run-time and GPU memory.CasDiffMVS achieves state-of-the-art performance on DTU, Tanks &amp; Temples andETH3D. Code is available at: https://github.com/cvg/diffmvs.</description>
      <author>example@mail.com (Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys)</author>
      <guid isPermaLink="false">2509.15220v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2509.15123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用单个RGB视频作为监督的新方法，用于动态场景中的相机参数优化，该方法包含三个关键组件，实验证明其高效且准确。&lt;h4&gt;背景&lt;/h4&gt;COLMAP是静态场景中相机参数优化的主要方法，但在动态场景中受限于运行时间长和依赖真实运动掩码的问题。许多改进方法依赖于各种先验知识，但这些在日常拍摄的RGB视频中通常不可用。&lt;h4&gt;目的&lt;/h4&gt;开发一种更准确和高效的相机参数优化方法，专门针对动态场景，且仅使用单个RGB视频作为监督。&lt;h4&gt;方法&lt;/h4&gt;提出了一种包含三个关键组件的方法：(1)基于块的跟踪滤波器，建立视频中的鲁棒稀疏铰链关系；(2)异常感知联合优化，通过自适应降低移动异常值的权重来优化相机参数，不依赖运动先验；(3)两阶段优化策略，通过在Softplus限制和凸最小值之间权衡来提高稳定性和优化速度。&lt;h4&gt;主要发现&lt;/h4&gt;在4个真实世界数据集（NeRF-DS, DAVIS, iPhone, 和 TUM-dynamics）和1个合成数据集（MPI-Sintel）上的实验表明，该方法仅使用单个RGB视频作为监督，能够更高效、更准确地估计相机参数。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为动态场景中的相机参数优化提供了一种新的解决方案，仅依靠单个RGB视频作为输入，实现了高效且准确的相机参数估计。&lt;h4&gt;翻译&lt;/h4&gt;虽然COLMAP长期以来一直是静态场景中相机参数优化的主要方法，但它在动态场景中的应用受到运行时间长和对真实运动掩码依赖的限制。许多尝试通过加入更多先验作为监督（如真实焦距、运动掩码、3D点云、相机位姿和度量深度）来改进它，然而这些信息在日常拍摄的RGB视频中通常不可用。在本文中，我们提出了一种新方法，用于在动态场景中进行更准确和高效的相机参数优化，仅由单个RGB视频监督。我们的方法包含三个关键组件：(1)基于块的跟踪滤波器，在RGB视频间建立鲁棒且最大稀疏的铰链关系；(2)异常感知联合优化，通过自适应降低移动异常值的权重进行高效的相机参数优化，不依赖运动先验；(3)两阶段优化策略，通过在Softplus限制和损失凸最小值之间权衡来提高稳定性和优化速度。我们对相机估计进行了视觉和数值评估。为了进一步验证准确性，我们将相机估计输入到4D重建方法中，评估生成的3D场景、渲染的2D RGB和深度图。我们在4个真实世界数据集（NeRF-DS, DAVIS, iPhone, 和 TUM-dynamics）和1个合成数据集（MPI-Sintel）上进行了实验，证明我们的方法仅使用单个RGB视频作为监督，能够更高效、更准确地估计相机参数。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的是在仅使用RGB视频的情况下，如何准确高效地优化动态场景中的相机参数（包括焦距、旋转和平移）。这个问题在现实中非常重要，因为日常拍摄的视频通常只有RGB信息，没有额外的传感器数据（如深度或运动掩码），而准确的相机参数估计是3D场景重建和视图合成的基础，对于AR/VR、自动驾驶和机器人导航等领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计新方法：观察到COLMAP在动态场景中运行时间长且需要真实运动掩码；发现大多数改进方法依赖于额外的真实先验信息，这些在日常RGB视频中不可用；注意到现有RGB-only监督方法计算延迟高且无法自适应排除移动异常值。作者借鉴了预训练点跟踪模型（如CoTracker）来提取轨迹，并采用3D高斯表示进行场景重建，但创新性地将不确定性建模与3D校准点关联而非2D像素。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是仅使用RGB视频作为监督，通过分块跟踪滤波器提取鲁棒的伪监督信息，然后使用异常值感知联合优化方法估计相机参数，最后通过两阶段优化策略提高稳定性和效率。整体流程：1)分块跟踪滤波：将图像分块，识别高纹理块，选择梯度最高的点，过滤不可见轨迹和重叠轨迹；2)异常值感知联合优化：联合优化校准点、焦距、旋转、平移和不确定性参数，使用ACP误差和柯西损失降低异常值影响；3)两阶段优化：第一阶段固定不确定性参数快速收敛，第二阶段使用第一阶段结果初始化并联合优化所有参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)分块跟踪滤波器：基于预训练点跟踪模型建立鲁稀疏关系，避免密集预测延迟；2)异常值感知联合优化：引入与3D校准点关联的可学习不确定性，使用柯西分布建模；3)两阶段优化策略：基于Softplus函数分析和损失凸项最小值，提高稳定性和效率。相比之前工作：仅使用RGB视频监督；将不确定性参数与3D校准点关联而非2D像素，减少参数数量；提出两阶段优化策略；在多个数据集上展示优越性能和效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种仅使用RGB视频监督的创新方法，通过分块跟踪滤波器、异常值感知联合优化和两阶段优化策略，实现了动态场景中相机参数的高效准确估计，为从日常拍摄的视频中重建高保真动态场景提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although COLMAP has long remained the predominant method for camera parameteroptimization in static scenes, it is constrained by its lengthy runtime andreliance on ground truth (GT) motion masks for application to dynamic scenes.Many efforts attempted to improve it by incorporating more priors assupervision such as GT focal length, motion masks, 3D point clouds, cameraposes, and metric depth, which, however, are typically unavailable in casuallycaptured RGB videos. In this paper, we propose a novel method for more accurateand efficient camera parameter optimization in dynamic scenes solely supervisedby a single RGB video. Our method consists of three key components: (1)Patch-wise Tracking Filters, to establish robust and maximally sparsehinge-like relations across the RGB video. (2) Outlier-aware JointOptimization, for efficient camera parameter optimization by adaptivedown-weighting of moving outliers, without reliance on motion priors. (3) ATwo-stage Optimization Strategy, to enhance stability and optimization speed bya trade-off between the Softplus limits and convex minima in losses. Wevisually and numerically evaluate our camera estimates. To further validateaccuracy, we feed the camera estimates into a 4D reconstruction method andassess the resulting 3D scenes, and rendered 2D RGB and depth maps. We performexperiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics)and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimatescamera parameters more efficiently and accurately with a single RGB video asthe only supervision.</description>
      <author>example@mail.com (Fang Li, Hao Zhang, Narendra Ahuja)</author>
      <guid isPermaLink="false">2509.15123v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Random Masking: A Dual-Stream Approach for Rotation-Invariant Point Cloud Masked Autoencoders</title>
      <link>http://arxiv.org/abs/2509.14975v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, aceppted by DICTA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双流掩码方法，结合3D空间网格掩码和渐进式语义掩码，解决了现有旋转不变点云掩码自编码器忽略几何结构和语义连贯性的问题，在各种旋转场景下表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有的旋转不变点云掩码自编码器依赖于随机掩码策略，这些策略忽略了几何结构和语义连贯性。随机掩码将块独立处理，无法捕捉跨方向保持一致的空间关系，也无法识别旋转时保持身份的语义对象部分。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉几何结构和语义一致性的掩码方法，以改进旋转不变点云表示学习，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出双流掩码方法，结合3D空间网格掩码(通过坐标排序创建结构化模式捕捉跨方向几何关系)和渐进式语义掩码(使用注意力驱动聚类发现语义有意义部分并保持连贯性)，通过课程学习和动态权重进行编排，从几何理解逐步过渡到语义发现。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40、ScanObjectNN和OmniObject3D上的综合实验表明，新方法在各种旋转场景下都有持续改进，显示出比基线旋转不变方法显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;所提出的双流掩码策略作为即插即用组件，可集成到现有旋转不变框架中无需架构更改，确保了不同方法间的广泛兼容性，有效解决了现有方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;现有的旋转不变点云掩码自编码器(MAE)依赖于随机掩码策略，这些策略忽略了几何结构和语义连贯性。随机掩码将块独立处理，无法捕捉跨方向保持一致的空间关系，也无法忽略旋转时保持身份的语义对象部分。我们提出了一种双流掩码方法，结合3D空间网格掩码和渐进式语义掩码，以解决这些基本限制。网格掩码通过坐标排序创建结构化模式，以捕捉跨不同方向保持的几何关系；而语义掩码使用注意力驱动的聚类来发现语义上有意义的部分，并在掩码过程中保持其连贯性。这些互补的流通过课程学习和动态权重进行编排，从几何理解逐步过渡到语义发现。作为即插即用组件设计，我们的策略可以集成到现有的旋转不变框架中，无需架构更改，确保了不同方法间的广泛兼容性。在ModelNet40、ScanObjectNN和OmniObject3D上的综合实验表明，在各种旋转场景下都有持续改进，显示出比基线旋转不变方法显著的性能提升。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有旋转不变性点云掩码自编码器(RI-MAE)依赖随机掩码策略而忽视几何结构和语义一致性的问题。这个问题在现实中很重要，因为物体经常以任意方向出现，模型需要具备旋转不变性才能有效识别相同形状的不同方向，而随机掩码无法捕捉跨方向保持一致的空间关系和语义对象部分，导致模型在旋转场景下性能显著下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到有效的旋转不变学习需要在掩码过程中明确保持几何结构和语义一致性，掩码策略本身必须尊重不变属性而非随机处理。他们借鉴了课程学习的动态权重机制设计，使用注意力驱动的聚类方法识别语义区域，参考了2D视觉中的结构化掩码方法如进化部分掩码(EPM)和注意力引导掩码(AttnMask)，并采用期望最大化(EM)算法进行语义聚类。这些现有工作被创新性地整合到3D点云旋转不变框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出双流掩码方法，结合3D空间网格掩码和渐进式语义掩码，通过课程学习动态平衡几何结构与语义一致性。整体流程为：输入点云通过最远点采样分解为局部区域；空间网格掩码通过坐标排序创建结构化模式；语义掩码使用注意力驱动的聚类发现语义部分；通过动态权重参数α(t)从几何主导(α≈0)逐渐过渡到语义主导(α≈1)；最终掩码概率是两种策略的加权组合；未掩码区域通过旋转不变特征提取后输入编码器，解码器重建掩码区域。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：3D空间网格掩码保持跨旋转的几何一致性；渐进式语义掩码通过注意力聚类保持语义连贯性；双流课程学习框架从几何过渡到语义；即插即用设计无需架构修改；动态权重机制平衡两个流。相比之前工作，本文不依赖随机掩码或修改网络架构，而是在掩码策略层面解决旋转不变性问题，通过课程学习实现多尺度学习，且能作为即插即用组件与多种现有框架结合，在多个数据集和旋转场景下显示出一致的性能提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种双流掩码方法，结合3D空间网格掩码和渐进式语义掩码，通过课程学习动态平衡几何结构与语义一致性，显著提升了点云掩码自编码器在任意旋转场景下的性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing rotation-invariant point cloud masked autoencoders (MAE) rely onrandom masking strategies that overlook geometric structure and semanticcoherence. Random masking treats patches independently, failing to capturespatial relationships consistent across orientations and overlooking semanticobject parts that maintain identity regardless of rotation. We propose adual-stream masking approach combining 3D Spatial Grid Masking and ProgressiveSemantic Masking to address these fundamental limitations. Grid masking createsstructured patterns through coordinate sorting to capture geometricrelationships that persist across different orientations, while semanticmasking uses attention-driven clustering to discover semantically meaningfulparts and maintain their coherence during masking. These complementary streamsare orchestrated via curriculum learning with dynamic weighting, progressingfrom geometric understanding to semantic discovery. Designed as plug-and-playcomponents, our strategies integrate into existing rotation-invariantframeworks without architectural changes, ensuring broad compatibility acrossdifferent approaches. Comprehensive experiments on ModelNet40, ScanObjectNN,and OmniObject3D demonstrate consistent improvements across various rotationscenarios, showing substantial performance gains over the baselinerotation-invariant methods.</description>
      <author>example@mail.com (Xuanhua Yin, Dingxin Zhang, Yu Feng, Shunqi Mao, Jianhui Yu, Weidong Cai)</author>
      <guid isPermaLink="false">2509.14975v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Hint: hierarchical inter-frame correlation for one-shot point cloud sequence compression</title>
      <link>http://arxiv.org/abs/2509.14859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  \c{opyright} 2026 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HINT是一种整合时空相关性的顺序点云压缩方法，显著降低了编码和解码时间，同时提高了压缩效率。&lt;h4&gt;背景&lt;/h4&gt;深度学习在点云压缩领域表现出强大能力，熵建模被广泛用于无损压缩。然而，大多数方法仅依赖父级或同级上下文和逐层自回归，导致解码延迟高达10-100秒。&lt;h4&gt;目的&lt;/h4&gt;提出HINT方法，整合时空相关性进行顺序点云压缩，以解决现有方法的高延迟问题。&lt;h4&gt;方法&lt;/h4&gt;HINT采用两阶段时间特征提取：(i)父级存在图和(ii)前一帧的子级邻域查找。这些线索通过逐元素相加与空间特征融合，并使用分组策略进行编码。&lt;h4&gt;主要发现&lt;/h4&gt;HINT实现了105毫秒的编码时间和140毫秒的解码时间，与G-PCC相比分别实现了49.6倍和21.6倍的加速，同时实现了高达43.6%的比特率降低，并一致性地优于仅使用空间信息的强基线方法RENO。&lt;h4&gt;结论&lt;/h4&gt;HINT方法在点云压缩方面既高效又有效，显著改善了编码解码速度和压缩性能。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在点云压缩方面已展现出强大的能力。在该领域中，用于无损压缩的熵建模被广泛研究。然而，大多数方法仅依赖于父级或同级上下文和逐层自回归，导致解码延迟在10到100秒的量级。我们提出了HINT，一种整合时空相关性进行顺序点云压缩的方法。具体来说，它首先使用两阶段时间特征提取：(i)父级存在图和(ii)前一帧中的子级邻域查找。这些线索通过逐元素相加与空间特征融合，并使用分组策略进行编码。实验结果表明，HINT分别实现了105毫秒和140毫秒的编码和解码时间，与G-PCC相比分别实现了49.6倍和21.6倍的加速，同时实现了高达43.6%的比特率降低。此外，HINT一致性地优于仅使用空间信息的强基线方法RENO。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云序列压缩中的效率问题，特别是解码延迟过高的问题。现有基于深度学习的点云压缩方法解码延迟高达10-100秒，这对于自动驾驶、AR/VR和远程呈现等需要实时处理点云数据的领域来说太慢了。点云数据在这些领域迅速增长，原始点云极其消耗带宽，因此高效的几何压缩对可扩展的捕获、传输和存储至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：大多数方法仅依赖父/子上下文和逐层自回归，导致高解码延迟。他们发现现有方法主要关注单帧内的空间上下文，没有充分利用帧间的时间冗余。因此，他们设计了一个既能保持严格因果性，又能引入轻量级时间线索和兄弟线索的方法。他们借鉴了RENO框架的快速占用生成器(FOG)和快速坐标生成器(FCG)模块，但扩展了RENO，使其不仅适用于稀疏LiDAR序列，还能处理高密度点云序列。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合时间和空间相关性来提高点云序列的压缩效率。方法采用两阶段时间特征提取：父级存在映射和子级邻域查找，将这些线索与空间特征通过元素级相加融合，并使用分组策略进行编码。整体流程包括：1)量化和层次结构构建；2)父级时间相关性提取，创建当前帧和前一帧的'存在映射'；3)子级时间相关性提取，查询前一帧中对应位置的邻域；4)基于兄弟相关性的熵编码，将每个父体的8个子体素分为奇数组和偶数组进行编码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段时间特征提取(父级存在映射和子级邻域查找)；2)时间特征与空间特征的元素级相加融合；3)分组编码策略，将子体素分为奇偶两组；4)高效并行化设计，显著降低延迟。相比之前工作，HINT明确建模了时间相关性，不同于仅依赖空间上下文的方法；它保持严格因果性，不同于需要显式运动估计或自回归的方法；设计更轻量，计算和内存需求低于使用注意力机制的方法；相比基线方法RENO，它解决了时间和兄弟依赖性未被充分利用的问题，并将应用扩展到高密度点云序列。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HINT通过整合两阶段时间特征提取与分组编码策略，实现了点云序列的高效压缩，在保持高压缩率的同时将编码/解码速度提高了两个数量级，解决了现有方法中时间相关性利用不足和计算效率低下的问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has demonstrated strong capability in compressing point clouds.Within this area, entropy modeling for lossless compression is widelyinvestigated. However, most methods rely solely on parent orsibling contextsand level-wise autoregression, which suffers from decoding latency on the orderof 10 to 100 seconds. We propose HINT, a method that integrates temporal andspatial correlation for sequential point cloud compression. Specifically, itfirst uses a two stage temporal feature extraction: (i) a parent-levelexistence map and (ii) a child-level neighborhood lookup in the previous frame.These cues are fused with the spatial features via elementwise addition andencoded with a group-wise strategy. Experimental results show that HINTachieves encoding and decoding time at 105 ms and 140 ms, respectively,equivalent to 49.6x and 21.6x acceleration in comparison with G-PCC, whileachieving up to bit rate reduction of 43.6%, in addition, consistentlyoutperforming over the strong spatial only baseline (RENO).</description>
      <author>example@mail.com (Yuchen Gao, Qi Zhang)</author>
      <guid isPermaLink="false">2509.14859v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>MapAnything: Mapping Urban Assets using Single Street-View Images</title>
      <link>http://arxiv.org/abs/2509.14839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MapAnything是一个能够通过单张图像自动确定物体地理坐标的模块，可减少城市数据库维护的人工工作量。&lt;h4&gt;背景&lt;/h4&gt;城市管理部门维护包含交通标志、树木等物体及其地理坐标的数据库，涂鸦和道路损坏等事件也相关。随着数字化程度提高，需要更多数据和更新的数据库，这需要大量人工工作。&lt;h4&gt;目的&lt;/h4&gt;介绍MapAnything模块，该模块能够自动使用单个图像确定物体的地理坐标，并提供自动化城市物体和事件映射的建议。&lt;h4&gt;方法&lt;/h4&gt;利用先进的度量深度估计模型，根据物体与相机的距离、几何原理和相机规格计算地理坐标。通过将估计距离与LiDAR点云进行比较来评估准确性，并分析不同距离区间和语义区域（如道路和植被）的性能。&lt;h4&gt;主要发现&lt;/h4&gt;模块在估计距离方面具有准确性，在不同距离区间和语义区域表现出良好的性能。&lt;h4&gt;结论&lt;/h4&gt;通过交通标志和道路损坏的实际用例证明了MapAnything模块的有效性，能够帮助城市管理部门更高效地维护地理数据库。&lt;h4&gt;翻译&lt;/h4&gt;为了掌握城市状况概览，城市管理部门维护着包含交通标志和树木等物体的数据库，并附有它们的地理坐标。涂鸦或道路损坏等事件也很重要。随着数字化程度提高，对更多数据和更新数据库的需求也在增加，这需要大量人工工作。本文介绍了MapAnything，一个使用单张图像自动确定物体地理坐标的模块。利用先进的度量深度估计模型，MapAnything根据物体与相机的距离、几何原理和相机规格计算地理坐标。我们详细说明并验证了该模块，提供了自动化城市物体和事件映射的建议。我们的评估将估计距离的准确性与城市环境中的LiDAR点云进行了比较，分析了不同距离区间和道路和植被等语义区域的性能。通过涉及交通标志和道路损坏的实际用例，证明了该模块的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何自动确定城市物体（如交通标志、树木等）地理坐标的问题。这个问题很重要，因为城市管理部门需要维护包含物体位置信息的数据库，但手动创建和更新这些数据库需要大量人工工作，成本高昂且耗时。随着数字化程度提高，需要更及时更新的数据，而现有解决方案要么昂贵（如使用LiDAR或航拍图像），要么不够准确（如仅使用相机坐标），要么需要物体先验知识或多个连续图像。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有城市物体映射方法的局限性，包括成本高、不够准确或需要额外信息。他们借鉴了现有的单目度量深度估计模型（Monocular Metric Depth Estimation Models），这些模型可以从单张图像估计物体与相机的距离。作者设计的方法基于三个主要步骤：1)检测/分割图像中的目标物体；2)应用MapAnything模块估计深度并计算坐标；3)去除重复识别的物体并与数据库匹配。这种方法的核心创新是使用单张图像和深度估计模型来确定物体地理坐标，不需要额外信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用单张街景图像，通过先进的单目度量深度估计模型预测物体与相机的距离，然后结合相机位置、方向和几何原理计算物体的实际地理坐标。整体流程包括：1)物体检测/分割，使用模型定位图像中的目标物体；2)深度估计，使用深度估计模型预测物体到相机的距离；3)坐标计算，基于针孔相机模型和相机参数计算物体的经纬度坐标；4)后处理，去除重复预测并与数据库条目匹配。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)通用单图像方法，不需要额外图像或物体信息；2)应用并评估了四种最新深度估计模型在城市环境中的适用性；3)全面的评估体系，测量了不同距离和语义区域的性能；4)通过两个实际用例验证了方法有效性。相比之前的工作，这篇论文的不同之处在于不依赖额外信息（如航拍图像或物体大小）、专注于单目深度估计在城市地理定位中的应用、提供实际应用建议和全面的评估体系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于单张街景图像和深度估计的自动化方法，能够准确估计城市物体的地理坐标，为城市基础设施的数字化管理和维护提供了高效、低成本的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To maintain an overview of urban conditions, city administrations managedatabases of objects like traffic signs and trees, complete with theirgeocoordinates. Incidents such as graffiti or road damage are also relevant. Asdigitization increases, so does the need for more data and up-to-datedatabases, requiring significant manual effort. This paper introducesMapAnything, a module that automatically determines the geocoordinates ofobjects using individual images. Utilizing advanced Metric Depth Estimationmodels, MapAnything calculates geocoordinates based on the object's distancefrom the camera, geometric principles, and camera specifications. We detail andvalidate the module, providing recommendations for automating urban object andincident mapping. Our evaluation measures the accuracy of estimated distancesagainst LiDAR point clouds in urban environments, analyzing performance acrossdistance intervals and semantic areas like roads and vegetation. The module'seffectiveness is demonstrated through practical use cases involving trafficsigns and road damage.</description>
      <author>example@mail.com (Miriam Louise Carnot, Jonas Kunze, Erik Fastermann, Eric Peukert, André Ludwig, Bogdan Franczyk)</author>
      <guid isPermaLink="false">2509.14839v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>A Real-Time Multi-Model Parametric Representation of Point Clouds</title>
      <link>http://arxiv.org/abs/2509.14773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多模型参数化表示方法，用于实时表面检测和拟合，能够在保持实时性能的同时提高点云处理的准确性。&lt;h4&gt;背景&lt;/h4&gt;近年来，点云的参数化表示已被广泛应用于内存高效映射和多机器人协作等任务。高度自适应的模型（如样条曲面或二次曲面）在检测或拟合时计算成本高。而实时方法（如高斯混合模型或平面）自由度低，难以用少量基元实现高精度。&lt;h4&gt;目的&lt;/h4&gt;解决现有点云参数化表示方法中，高自适应模型计算复杂度高，而实时方法精度低的问题，提出一种能够实时进行表面检测和拟合的多模型参数化表示方法。&lt;h4&gt;方法&lt;/h4&gt;首先使用高斯混合模型将点云分割成多个簇；然后选择并合并平坦的簇形成平面或曲面；平面通过基于二维体素的边界描述方法进行拟合和界定；有曲率的表面通过B样条曲面进行拟合，同样使用边界描述方法。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共数据集上的评估表明，所提出的表面检测方法比最先进的方法具有更强的鲁棒性，效率提高了3.78倍；同时，该表示方法比高斯混合模型的精度提高了2倍，在低功耗机载计算机上运行速度达到36.4 fps。&lt;h4&gt;结论&lt;/h4&gt;该方法结合了高自适应模型的精度优势和实时方法的效率优势，能够在保持实时性能的同时提高点云表面检测和拟合的准确性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，点云的参数化表示已被广泛应用于内存高效映射和多机器人协作等任务。高度自适应的模型，如样条曲面或二次曲面，在检测或拟合时计算成本高。相比之下，实时方法，如高斯混合模型或平面，自由度低，难以用少量基元实现高精度。为解决这一问题，本文提出了一种具有实时表面检测和拟合功能的多模型参数化表示方法。具体而言，首先采用高斯混合模型将点云分割成多个簇。然后，选择平坦的簇合并成平面或曲面。平面可以通过基于二维体素的边界描述方法轻松拟合和界定。有曲率的表面通过B样条曲面进行拟合，并采用相同的边界描述方法。通过对多个公共数据集的评估，所提出的表面检测方法比最先进的方法具有更强的鲁棒性，效率提高了3.78倍。同时，该表示方法比高斯混合模型的精度提高了2倍，在低功耗机载计算机上运行速度达到36.4 fps。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云参数化表示的实时性与精度之间的矛盾问题。在机器人感知、SLAM（同步定位与地图构建）和多机器人协作等应用中，需要在有限计算资源上实时处理点云数据，同时保持高精度。现有方法要么计算成本高（如样条曲面），要么精度有限（如高斯混合模型），难以满足实际需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的优缺点：高精度模型计算量大，实时方法精度有限。然后提出结合多种模型优势的思路，针对不同区域特性使用最适合的表示方法。作者借鉴了GMM点云分割、B-spline曲面拟合和体素过滤等现有技术，但通过创新的集成层次聚类和平面/曲面检测方法，将这些技术有效整合为一个统一的多模型框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是根据点云中不同区域的几何特性，灵活选择最合适的参数化模型：无结构区域用高斯分布表示，平坦区域用平面表示，弯曲区域用B-spline曲面表示。整体流程包括：1) 体素过滤预处理点云；2) 使用集成层次GMM方法分割点云；3) 提取并合并平坦区域为平面或曲面；4) 将平面和曲面点变换到局部坐标系；5) 使用2D体素方法描述边界；6) 对弯曲区域进行B-spline曲面拟合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 改进的曲面检测方法，效率比现有方法提高3.78倍；2) 多模型参数化表示框架，有效结合高斯分布、平面和B-spline曲面。相比之前的工作，不同之处在于：现有方法通常只使用单一模型，而本文方法根据区域特性灵活选择；现有曲面检测方法计算量大不适合实时应用，而本文实现了实时曲面检测；现有多模型方法无法有效处理曲面区域，而本文能够有效识别和表示曲面区域。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种实时多模型点云参数化表示方法，结合高斯分布、平面和B-spline曲面的优势，在保持36.4fps实时性能的同时，将点云表示的准确性提高了两倍。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, parametric representations of point clouds have been widelyapplied in tasks such as memory-efficient mapping and multi-robotcollaboration. Highly adaptive models, like spline surfaces or quadrics, arecomputationally expensive in detection or fitting. In contrast, real-timemethods, such as Gaussian mixture models or planes, have low degrees offreedom, making high accuracy with few primitives difficult. To tackle thisproblem, a multi-model parametric representation with real-time surfacedetection and fitting is proposed. Specifically, the Gaussian mixture model isfirst employed to segment the point cloud into multiple clusters. Then, flatclusters are selected and merged into planes or curved surfaces. Planes can beeasily fitted and delimited by a 2D voxel-based boundary description method.Surfaces with curvature are fitted by B-spline surfaces and the same boundarydescription method is employed. Through evaluations on multiple publicdatasets, the proposed surface detection exhibits greater robustness than thestate-of-the-art approach, with 3.78 times improvement in efficiency.Meanwhile, this representation achieves a 2-fold gain in accuracy over Gaussianmixture models, operating at 36.4 fps on a low-power onboard computer.</description>
      <author>example@mail.com (Yuan Gao, Wei Dong)</author>
      <guid isPermaLink="false">2509.14773v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>LLM4MG: Adapting Large Language Model for Multipath Generation via Synesthesia of Machines</title>
      <link>http://arxiv.org/abs/2509.14711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次基于机器联觉(SoM)将大型语言模型(LLM)适应用于多路径生成(LLM4MG)，针对第六代(6G)车对基础设施(V2I)场景，构建了多模态感知-通信数据集SynthSoM-V2I，并通过特征提取、融合网络以及低秩适应和传播感知提示工程，实现了基于多模态感知数据的高精度多路径生成。&lt;h4&gt;背景&lt;/h4&gt;基于机器联觉(Synesthesia of Machines, SoM)的方法，应用于第六代(6G)车对基础设施(V2I)通信场景，需要高精度的多路径生成技术来支持系统设计。&lt;h4&gt;目的&lt;/h4&gt;首次将大型语言模型(LLM)适应用于多路径生成(LLM4MG)，实现基于多模态感知数据的高精度多路径生成，并验证其在不同场景下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;构建包含信道多路径信息、毫米波雷达感知数据、RGB-D图像和LiDAR点云的多模态数据集SynthSoM-V2I；利用LLaMA 3.2模型通过特征提取和融合网络对齐多模态特征空间与语义空间；采用低秩适应(LoRA)参数高效微调和传播感知提示工程实现知识迁移。&lt;h4&gt;主要发现&lt;/h4&gt;LLM4MG在视距(LoS)/非视距(NLoS)分类上达到92.76%的准确率；多路径功率/延迟生成的归一化均方误差(NMSE)分别为0.099/0.032；具有跨车辆交通密度、跨频段和跨场景的良好泛化能力；通过实际泛化和信道容量比较验证了其有效性和高精度多路径生成的必要性。&lt;h4&gt;结论&lt;/h4&gt;基于机器联觉的大型语言模型适应方法在6G V2I场景的多路径生成中表现出色，具有良好的泛化能力和实际应用价值，为高精度信道建模提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;基于机器联觉(SoM)，首次将大型语言模型(LLM)适应用于多路径生成(LLM4MG)。考虑到非典型的第六代(6G)车对基础设施(V2I)场景，构建了一个新的多模态感知-通信数据集，名为SynthSoM-V2I，包括信道多路径信息、毫米波(mmWave)雷达感知数据、RGB-D图像和光检测与测距(LiDAR)点云。基于SynthSoM-V2I数据集，所提出的LLM4MG利用大型语言模型Meta AI(LLaMA) 3.2通过多模态感知数据进行多路径生成。所提出的LLM4MG通过特征提取和融合网络将多模态特征空间与LLaMA语义空间对齐。为了进一步实现从预训练LLaMA到多路径生成的通用知识迁移，采用了低秩适应(LoRA)参数高效微调和传播感知提示工程。仿真结果表明，所提出的LLM4MG在视距(LoS)/非视距(NLoS)分类上的准确率为92.76%，多路径功率/延迟生成的归一化均方误差(NMSE)为0.099/0.032，并且在跨车辆交通密度(VTD)、跨频段和跨场景泛化方面优于传统深度学习方法。通过实际泛化验证了所提出LLM4MG的实用性。通过信道容量比较也证明了高精度多路径生成对系统设计的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Based on Synesthesia of Machines (SoM), a large language model (LLM) isadapted for multipath generation (LLM4MG) for the first time. Considering atypical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a newmulti-modal sensing-communication dataset is constructed, named SynthSoM-V2I,including channel multipath information, millimeter wave (mmWave) radar sensorydata, RGB-D images, and light detection and ranging (LiDAR) point clouds. Basedon the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language ModelMeta AI (LLaMA) 3.2 for multipath generation via multi-modal sensory data. Theproposed LLM4MG aligns the multi-modal feature space with the LLaMA semanticspace through feature extraction and fusion networks. To further achievegeneral knowledge transfer from the pre-trained LLaMA for multipath generationvia multi-modal sensory data, the low-rank adaptation (LoRA)parameter-efficient fine-tuning and propagation-aware prompt engineering areexploited. Simulation results demonstrate that the proposed LLM4MG outperformsconventional deep learning-based methods in terms of line-of-sight(LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipathpower/delay generation precision with normalized mean square error (NMSE) of0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, andcross-scenario generalization. The utility of the proposed LLM4MG is validatedby real-world generalization. The necessity of high-precision multipathgeneration for system design is also demonstrated by channel capacitycomparison.</description>
      <author>example@mail.com (Ziwei Huang, Shiliang Lu, Lu Bai, Xuesong Cai, Xiang Cheng)</author>
      <guid isPermaLink="false">2509.14711v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Feature-aligned Motion Transformation for Efficient Dynamic Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2509.14591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种特征对齐运动变换(FMT)框架用于动态点云压缩，通过时空对齐策略替代显式运动向量，并设计了随机访问参考策略支持双向运动引用和分层编码，实现了帧级并行压缩。实验表明该方法在压缩效率和性能上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;动态点云广泛应用于沉浸式现实、机器人和自动驾驶等领域，但其高效压缩面临挑战，因为点云的不规则结构和显著局部变化使得运动估计和补偿任务非常困难。&lt;h4&gt;目的&lt;/h4&gt;克服当前动态点云压缩方法中显式运动估计的局限性，提高压缩效率和性能。&lt;h4&gt;方法&lt;/h4&gt;提出特征对齐运动变换(FMT)框架，用时空对齐策略替代显式运动向量，在潜在空间条件编码框架中使用对齐特征作为时间上下文；设计随机访问参考策略，支持双向运动引用和分层编码，实现帧级并行压缩。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在编码和解码效率上优于D-DPCC和AdaDPCC，分别实现了20%和9.4%的BD-Rate降低。&lt;h4&gt;结论&lt;/h4&gt;FMT能够有效提高压缩效率和处理性能，证明了特征对齐运动变换在动态点云压缩中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;动态点云广泛应用于沉浸式现实、机器人和自动驾驶等应用中。高效压缩在很大程度上依赖于准确的运动估计和补偿，然而点云的不规则结构和显著的局部变化使得这一任务极具挑战性。当前方法通常依赖显式运动估计，其编码向量难以捕捉复杂的动态变化，且无法充分利用时间相关性。为克服这些局限，我们提出了一种用于动态点云压缩的特征对齐运动变换(FMT)框架。FMT用时空对齐策略替代显式运动向量，该策略隐式地建模连续的时间变化，并在潜在空间条件编码框架中使用对齐特征作为时间上下文。此外，我们设计了一种随机访问(RA)参考策略，支持双向运动引用和分层编码，从而实现帧级并行压缩。大量实验表明，我们的方法在编码和解码效率上均优于D-DPCC和AdaDPCC，同时分别实现了20%和9.4%的BD-Rate降低。这些结果突显了FMT在提高压缩效率和处理性能方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态点云压缩中的高效压缩问题。当前方法通常依赖显式的运动估计和补偿，但难以捕捉复杂的动态变化，无法充分利用时间相关性。这个问题在现实中非常重要，因为动态点云数据在沉浸式现实、机器人和自动驾驶等领域有广泛应用，但其巨大的数据量对存储和传输带来挑战。高效的压缩可以减少带宽需求，降低存储成本，使这些技术更加实用化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前动态点云压缩方法的局限性，特别是显式运动估计的问题。他们借鉴了深度学习在图像和视频压缩中的成功经验，将压缩的核心问题重新定义为去除空间和时间冗余。为了解决传统运动估计的局限性，作者将传统的运动估计和补偿重新表述为时空对齐任务，并借鉴了视频压缩中的随机访问参考策略，设计了非序列的分层编码结构。在方法设计上，作者引入了特征对齐运动变换框架，使用隐式神经表示通过MLP来建模连续时间变化，而不是使用显式运动向量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将传统的显式运动估计和补偿重新表述为时空对齐任务，使用特征对齐运动变换框架隐式地建模连续时间变化，并设计随机访问参考策略实现双向运动引用和分层编码。整体实现流程：编码端时，输入点云被逐步下采样，使用八叉树编码压缩坐标，通过双向特征对齐运动变换模块将当前帧与参考帧特征对齐生成运动感知上下文，使用条件熵模型压缩特征，并按非序列分层编码顺序处理；解码端时，从比特流恢复坐标和特征，使用相同模块对齐参考帧特征，通过上下文解码器生成时间对齐特征，逐步上采样重建完整点云，并根据编码顺序存储在参考缓冲区中。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)特征对齐运动变换框架，将运动估计重新表述为时空对齐任务，使用隐式神经表示替代显式运动向量；2)随机访问参考策略，设计非序列分层帧组结构，实现双向运动引用和并行压缩；3)条件熵模型，结合多种先验提高压缩效率。相比之前的工作，这篇论文不再依赖显式运动估计，而是采用隐式时空对齐；使用非序列分层编码结构替代传统顺序编码；支持双向引用和并行处理，在低比特率场景下表现出更好的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于特征对齐运动变换的动态点云压缩框架，通过隐式时空对齐和随机访问参考策略，显著提高了压缩效率和性能，实现了比现有方法更优的率失真性能和处理速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic point clouds are widely used in applications such as immersivereality, robotics, and autonomous driving. Efficient compression largelydepends on accurate motion estimation and compensation, yet the irregularstructure and significant local variations of point clouds make this taskhighly challenging. Current methods often rely on explicit motion estimation,whose encoded vectors struggle to capture intricate dynamics and fail to fullyexploit temporal correlations. To overcome these limitations, we introduce aFeature-aligned Motion Transformation (FMT) framework for dynamic point cloudcompression. FMT replaces explicit motion vectors with a spatiotemporalalignment strategy that implicitly models continuous temporal variations, usingaligned features as temporal context within a latent-space conditional encodingframework. Furthermore, we design a random access (RA) reference strategy thatenables bidirectional motion referencing and layered encoding, therebysupporting frame-level parallel compression. Extensive experiments demonstratethat our method surpasses D-DPCC and AdaDPCC in both encoding and decodingefficiency, while also achieving BD-Rate reductions of 20% and 9.4%,respectively. These results highlight the effectiveness of FMT in jointlyimproving compression efficiency and processing performance.</description>
      <author>example@mail.com (Xuan Deng, Xiandong Meng, Longguang Wang, Tiange Zhang, Xiaopeng Fan, Debin Zhao)</author>
      <guid isPermaLink="false">2509.14591v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model</title>
      <link>http://arxiv.org/abs/2509.14560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于分数扩散模型的自适应迭代点云去噪方法，通过估计噪声变化并确定自适应去噪计划，提高了去噪效率，实验证明该方法在保留细节和边界方面优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;点云去噪任务旨在从带有不同程度或模式噪声的扫描数据中恢复干净点云。现有最先进的方法通常训练深度神经网络来更新点位置，并经验性地重复去噪过程多次，但如何有效地安排迭代去噪过程来处理不同噪声水平或模式尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;开发一种自适应迭代点云去噪方法，能够根据噪声特征自动调整去噪过程，有效处理不同水平和模式的噪声，同时保留点云的形状边界和细节。&lt;h4&gt;方法&lt;/h4&gt;基于分数扩散模型设计自适应迭代点云去噪方法。对于给定噪声点云，首先估计噪声变化并确定自适应去噪计划和适当步长，然后按照自适应计划使用训练好的网络迭代更新点云。同时设计网络架构和两阶段采样策略，支持特征融合和梯度融合以实现迭代去噪。&lt;h4&gt;主要发现&lt;/h4&gt;与现有最先进方法相比，所提出方法能够获得更干净平滑的去噪点云，同时更好地保留形状边界和细节。在定性和定量评估中均优于其他方法，并且在处理不同噪声模式的合成数据集以及真实扫描数据集上表现出色。&lt;h4&gt;结论&lt;/h4&gt;基于分数扩散模型的自适应迭代点云去噪方法能够有效处理不同水平和模式的噪声，在保留点云细节和边界方面显著优于现有方法，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;点云去噪任务旨在从耦合有不同程度或模式噪声的扫描数据中恢复干净点云。最近的最先进方法通常训练深度神经网络来更新点位置，朝向干净点云，并经验性地重复去噪过程多次以获得去噪结果。如何有效安排迭代去噪过程以处理不同程度或模式的噪声尚不清楚。在本文中，我们提出了一种基于分数扩散模型的自适应迭代点云去噪方法。对于给定的噪声点云，我们首先估计噪声变化并确定具有适当步长的自适应去噪计划，然后按照自适应计划调用训练好的网络迭代更新点云。为促进这种自适应和迭代去噪过程，我们设计了网络架构和用于网络训练的两阶段采样策略，以支持迭代去噪的特征融合和梯度融合。与最先进的点云去噪方法相比，我们的方法获得了干净平滑的去噪点云，同时更好地保留了形状边界和细节。我们的结果不仅在定性和定量上都优于其他方法，而且在具有不同噪声模式的合成数据集以及真实扫描数据集上也表现更优。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云去噪任务中如何有效安排迭代去噪过程的问题。这个问题很重要，因为点云是机器人、自动驾驶、制造业等应用中的基本3D表示，但实际获取的点云常含有不同程度的噪声，影响后续处理和使用。有效的去噪方法能恢复干净准确的形状，为后续应用提供高质量数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到现有方法在迭代去噪过程中存在局限性，网络未针对迭代过程训练，且去噪过程缺乏针对性。作者借鉴了扩散模型的思想，这种模型基于非平衡热力学理论，原本用于数据生成。作者将其改造用于去噪任务，引入因子保持噪声点云分布的均值，设计了特征融合和梯度融合模块，并采用两阶段采样策略进行训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是基于得分的扩散模型进行自适应和迭代点云去噪，通过特征融合和梯度融合模块保留形状细节。实现流程包括：1)训练阶段：使用两阶段采样策略训练网络，包含特征提取、特征融合、梯度预测和梯度融合模块；2)推理阶段：估计噪声方差，确定自适应去噪时间表，迭代更新点位置，最后拼接恢复的块形成完整去噪点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于得分的扩散模型的自适应和迭代去噪方法；2)具有特征融合和梯度融合模块的网络架构及两阶段采样训练策略；3)针对不同噪声模式的泛化能力。相比之前工作，本文方法不是利用预训练模型，而是从头开始训练；为每个点云确定自适应去噪时间表而非使用固定时间表；通过融合模块更好地保留形状细节，避免薄结构表面坍塌。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于得分的扩散模型的自适应和迭代点云去噪方法，通过特征融合和梯度融合模块，能有效处理不同水平和模式的噪声，同时保留点云的形状边界和细节。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1111/cgf.70149&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud denoising task aims to recover the clean point cloud from thescanned data coupled with different levels or patterns of noise. The recentstate-of-the-art methods often train deep neural networks to update the pointlocations towards the clean point cloud, and empirically repeat the denoisingprocess several times in order to obtain the denoised results. It is not clearhow to efficiently arrange the iterative denoising processes to deal withdifferent levels or patterns of noise. In this paper, we propose an adaptiveand iterative point cloud denoising method based on the score-based diffusionmodel. For a given noisy point cloud, we first estimate the noise variation anddetermine an adaptive denoising schedule with appropriate step sizes, theninvoke the trained network iteratively to update point clouds following theadaptive schedule. To facilitate this adaptive and iterative denoising process,we design the network architecture and a two-stage sampling strategy for thenetwork training to enable feature fusion and gradient fusion for iterativedenoising. Compared to the state-of-the-art point cloud denoising methods, ourapproach obtains clean and smooth denoised point clouds, while preserving theshape boundary and details better. Our results not only outperform the othermethods both qualitatively and quantitatively, but also are preferable on thesynthetic dataset with different patterns of noises, as well as thereal-scanned dataset.</description>
      <author>example@mail.com (Zhaonan Wang, Manyi Li, ShiQing Xin, Changhe Tu)</author>
      <guid isPermaLink="false">2509.14560v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>A Software-Defined Radio Testbed for Distributed LiDAR Point Cloud Sharing with IEEE 802.11p in V2V Networks</title>
      <link>http://arxiv.org/abs/2509.14523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一个基于软件定义无线电(SDR)的IEEE 802.11p测试平台，用于分布式车对车(V2V)通信。&lt;h4&gt;背景&lt;/h4&gt;该平台弥合了网络仿真与实际部署之间的差距，提供了为经济高效的ADALM-Pluto SDR配置的模块化代码库。&lt;h4&gt;目的&lt;/h4&gt;创建一个可协作感知的V2V通信系统，能够共享LiDAR点云并在节点间融合成集体感知环境。&lt;h4&gt;方法&lt;/h4&gt;使用基于SDR的IEEE 802.11p测试平台；提供模块化代码库配置；使用ADALM-Pluto SDR；具备运行Docker和ROS、执行Matlab并通过USB与Pluto接口连接的设备可作为通信节点；分享LiDAR点云并在节点间融合；评估利用去中心化存储系统(IPFS和Filecoin)的理论模型；分析节点存储收敛、延迟和可扩展性等约束；提供信道质量研究。&lt;h4&gt;主要发现&lt;/h4&gt;评估了利用去中心化存储系统的理论模型，分析了节点存储收敛、延迟和可扩展性等约束条件。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提供结论。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个基于软件定义无线电(SDR)的IEEE 802.11p测试平台，用于分布式车对车(V2V)通信。该平台通过为经济高效的ADALM-Pluto SDR提供模块化代码库，弥合了网络仿真与部署之间的差距。任何能够运行带有ROS的Docker、执行Matlab并通过USB与Pluto接口连接的设备都可以作为通信节点。为了展示协作感知，我们在节点间共享LiDAR点云，并将它们融合成集体感知环境。我们评估了利用去中心化存储系统(IPFS和Filecoin)的理论模型，分析了节点存储收敛、延迟和可扩展性等约束条件。此外，我们还提供了信道质量研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a Software Defined Radio (SDR)-based IEEE 802.11p testbed fordistributed Vehicle-to-Vehicle (V2V) communication. The platform bridges thegap between network simulation and deployment by providing a modular codebaseconfigured for cost-effective ADALM-Pluto SDRs. Any device capable of running aDocker with ROS, executing Matlab and interface with a Pluto via USB can act asa communication node. To demonstrate collaborative sensing, we share LiDARpoint clouds between nodes and fuse them into a collective perceptionenvironment. We evaluated a theoretical model for leveraging decentralizedstorage systems (IPFS and Filecoin), analyzing constraints such as node storageconvergence, latency, and scalability. In addition, we provide a channelquality study.</description>
      <author>example@mail.com (Mario Hernandez, Elijah Bryce, Peter Stubberud, Ebrahim Saberinia, Brendan Morris)</author>
      <guid isPermaLink="false">2509.14523v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>MATTER: Multiscale Attention for Registration Error Regression</title>
      <link>http://arxiv.org/abs/2509.12924v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于回归的点云配准质量验证方法，替代传统的分类方法，实现了更细粒度的质量量化，并通过多尺度提取和注意力聚合提高了特征表示能力。&lt;h4&gt;背景&lt;/h4&gt;点云配准(PCR)对于SLAM和目标跟踪等下游任务至关重要，因此检测和量化配准不齐（PCR质量验证）是一个重要任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种更细粒度的点云配准质量验证方法，替代传统的分类方法，提高配准质量评估的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用回归方法进行PCR验证，通过多尺度提取和基于注意力的聚合扩展了不齐相关特征，实现了更精确的配准误差估计。&lt;h4&gt;主要发现&lt;/h4&gt;所提方法在多样化数据集上提供了准确和鲁棒的配准误差估计，特别是在处理具有异构空间密度的点云时表现优异。&lt;h4&gt;结论&lt;/h4&gt;当用于指导地图绘制任务时，与基于分类的最先进方法相比，本文方法在给定数量的重新配准帧下显著提高了地图绘制质量。&lt;h4&gt;翻译&lt;/h4&gt;点云配准(PCR)对于许多下游任务（如同步定位与地图构建SLAM和目标跟踪）至关重要。这使得检测和量化配准不齐，即PCR质量验证，成为一个重要任务。所有现有方法都将验证视为分类任务，旨在将PCR质量分配到几个类别中。在本工作中，我们使用回归方法进行PCR验证，允许对配准质量进行更细粒度的量化。我们还通过多尺度提取和基于注意力的聚合扩展了先前使用的不齐相关特征。这导致在多样化数据集上准确和鲁棒的配准误差估计，特别是对于具有异构空间密度的点云。此外，当用于指导地图绘制下游任务时，与最先进的基于分类的方法相比，我们的方法在给定数量的重新配准帧下显著提高了地图绘制质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准质量评估的问题。现有方法将配准质量验证视为分类任务，只能粗略地将质量分为几个类别。而这个问题在现实中非常重要，因为点云配准是SLAM、3D重建和机器人导航等下游任务的基础，配准错误会传播到后续应用中，导致地图失真和导航失败。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有分类方法的局限性，然后借鉴了FACT等工作的基本架构，但将其从分类改为回归任务。他们还借鉴了使用微分熵和Sinkhorn散度作为特征的方法。核心创新是设计了多尺度注意力机制，通过在不同几何尺度提取特征并使用注意力自适应融合，解决了单一尺度特征难以适应不同配准场景的问题。整体实现包括特征提取、多尺度注意力融合和误差预测三个主要步骤。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用回归而非分类来评估点云配准质量，并通过多尺度特征提取和注意力机制融合不同几何尺度的特征，提高对点云配准误差估计的鲁棒性。整体流程：1)将源点云和参考点云转换到同一坐标系并选择锚点；2)提取局部特征(微分熵、Sinkhorn散度、覆盖率)和全局特征(共视性分数、距离、源标志)；3)通过多尺度注意力机制融合不同尺度的特征；4)使用PointTransformer和MLP网络预测最终的配准误差。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次将点云配准质量评估从分类转变为回归，实现更细粒度的误差量化；2)提出多尺度注意力机制，自适应融合不同几何尺度的特征；3)显著提高了在异构空间密度点云和挑战性场景下的鲁棒性；4)在下游应用中能更有效地指导重新配准。相比之前工作，本文方法提供连续的误差预测而非离散分类，使用多尺度特征而非单一尺度，并且在多个数据集上均表现出更好的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MATTER通过多尺度注意力机制将点云配准质量评估从分类转变为回归，实现了更精确、更鲁棒的配准误差估计，显著提升了下游任务的质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration (PCR) is crucial for many downstream tasks, such assimultaneous localization and mapping (SLAM) and object tracking. This makesdetecting and quantifying registration misalignment, i.e., PCR qualityvalidation, an important task. All existing methods treat validation as aclassification task, aiming to assign the PCR quality to a few classes. In thiswork, we instead use regression for PCR validation, allowing for a morefine-grained quantification of the registration quality. We also extendpreviously used misalignment-related features by using multiscale extractionand attention-based aggregation. This leads to accurate and robust registrationerror estimation on diverse datasets, especially for point clouds withheterogeneous spatial densities. Furthermore, when used to guide a mappingdownstream task, our method significantly improves the mapping quality for agiven amount of re-registered frames, compared to the state-of-the-artclassification-based method.</description>
      <author>example@mail.com (Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik Forssén)</author>
      <guid isPermaLink="false">2509.12924v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>A-TDOM: Active TDOM via On-the-Fly 3DGS</title>
      <link>http://arxiv.org/abs/2509.12759v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a short white paper for a coming Journal Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;A-TDOM是一种基于On-the-Fly 3DGS优化的近实时真正射影像图生成方法，能够在几秒内优化每个新图像的3DGS，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;背景&lt;/h4&gt;真正射影像图(TDOM)是城市管理、城市规划、土地测量等领域的重要地理空间产品。传统TDOM生成方法依赖复杂的离线摄影测量流程，导致延迟，阻碍了实时应用。此外，由于相机姿态不准确、数字表面模型(DSM)问题和场景遮挡等挑战，TDOM质量可能会下降。&lt;h4&gt;目的&lt;/h4&gt;解决传统TDOM生成方法的延迟问题，并改善因相机姿态不准确、DSM问题和场景遮挡等因素导致的TDOM质量下降问题。&lt;h4&gt;方法&lt;/h4&gt;A-TDOM是一种基于On-the-Fly 3DGS优化的近实时TDOM生成方法。每获取一张图像，通过On-the-Fly SfM计算其姿态和稀疏点云。然后将新的高斯函数整合并优化到先前未见或粗略重建的区域。通过与正交splatting集成，A-TDOM可以在每次更新新的3DGS场后立即渲染。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的初步实验表明，A-TDOM能够近实时主动渲染TDOM，每个新图像的3DGS优化只需几秒钟，同时保持了可接受的渲染质量和TDOM几何精度。&lt;h4&gt;结论&lt;/h4&gt;A-TDOM是一种有效的近实时TDOM生成方法，解决了传统方法的延迟问题，在保持质量的同时实现了近实时性能。&lt;h4&gt;翻译&lt;/h4&gt;真正射影像图(TDOM)作为城市管理、城市规划、土地测量等各领域的重要地理空间产品发挥着关键作用。然而，传统的TDOM生成方法通常依赖复杂的离线摄影测量流程，导致延迟阻碍了实时应用。此外，由于相机姿态不准确、数字表面模型(DSM)和场景遮挡等各种挑战，TDOM的质量可能会下降。为解决这些挑战，本文提出了A-TDOM，一种基于On-the-Fly 3DGS优化的近实时TDOM生成方法。每获取一张图像，通过On-the-Fly SfM计算其姿态和稀疏点云。然后将新的高斯函数整合并优化到先前未见或粗略重建的区域。通过与正交splatting集成，A-TDOM可以在每次更新新的3DGS场后立即渲染。在多个基准测试上的初步实验表明，所提出的A-TDOM能够近实时主动渲染TDOM，每个新图像的3DGS优化仅需几秒钟，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统TDOM（真实数字正射影像图）生成方法依赖复杂离线流程、处理延迟大、无法满足实时应用需求的问题。这个问题很重要，因为TDOM是城市管理、城市规划、土地测量等领域的关键地理空间产品，实时生成能力对于应急响应、动态监测等场景至关重要，而传统方法不仅耗时，还依赖DSM（数字表面模型）进行遮挡检测，计算成本高且质量易受影响。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有3D高斯溅射（3DGS）方法能生成高质量TDOM但多为离线处理，因此思考如何实现实时生成。他们借鉴了On-the-Fly SfM技术用于即时姿态估计和稀疏点云更新，参考了Tortho-Gaussian和Ortho-3DGS等3DGS方法，但针对实时需求进行了改进。特别是解决了原始3DGS densification策略不可控的问题，创新性地提出了高斯采样和集成方法，并基于投影矩阵修改实现了正交溅射，这些都是对现有工作的有效借鉴和提升。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过即时3DGS优化实现近实时的TDOM生成，使用高斯采样和集成方法替代原始3DGS的densification策略，实现可控的在线训练，并利用正交溅射技术消除建筑立面影响。整体流程是：1)训练初始3DGS场；2)每收到新图像时，使用On-the-Fly SfM更新相机姿态和稀疏点云；3)通过Delaunay三角剖分确定需要优化的关键区域；4)使用高斯采样和集成方法添加新高斯；5)优化3DGS场；6)通过正交溅射生成更新后的TDOM。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三：1)A-TDOM工作流，实现无需DSM和遮挡检测的近实时TDOM生成；2)基于高斯采样和集成方法的近实时3DGS优化技术，能智能更新3DGS场；3)基于投影矩阵修改的正交溅射方法。相比之前工作，不同之处在于：传统方法依赖复杂离线流程，而A-TDOM支持实时处理；现有3DGS方法多为离线，而A-TDOM实现了在线优化；原始3DGS的densification不可控，而A-TDOM的高斯采样和集成方法解决了这一问题；A-TDOM使用Delaunay三角剖分确定关键区域，优化了训练过程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; A-TDOM通过创新的在线3DGS优化方法，实现了无需DSM和遮挡检测的近实时真实数字正射影像图生成，显著提高了地理空间产品生成的效率和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product invarious fields such as urban management, city planning, land surveying, etc.However, traditional TDOM generation methods generally rely on a complexoffline photogrammetric pipeline, resulting in delays that hinder real-timeapplications. Moreover, the quality of TDOM may degrade due to variouschallenges, such as inaccurate camera poses or Digital Surface Model (DSM) andscene occlusions. To address these challenges, this work introduces A-TDOM, anear real-time TDOM generation method based on On-the-Fly 3DGS optimization. Aseach image is acquired, its pose and sparse point cloud are computed viaOn-the-Fly SfM. Then new Gaussians are integrated and optimized into previouslyunseen or coarsely reconstructed regions. By integrating with orthogonalsplatting, A-TDOM can render just after each update of a new 3DGS field.Initial experiments on multiple benchmarks show that the proposed A-TDOM iscapable of actively rendering TDOM in near real-time, with 3DGS optimizationfor each new image in seconds while maintaining acceptable rendering qualityand TDOM geometric accuracy.</description>
      <author>example@mail.com (Yiwei Xu, Xiang Wang, Yifei Yu, Wentian Gan, Luca Morelli, Giulio Perda, Xiongwu Xiao, Zongqian Zhan, Xin Wang, Fabio Remondino)</author>
      <guid isPermaLink="false">2509.12759v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning</title>
      <link>http://arxiv.org/abs/2509.15097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合分层分解、FPGA直接方程求解和增量学习的混合框架，用于降低深度学习的计算和能源消耗，同时保持模型性能。&lt;h4&gt;背景&lt;/h4&gt;深度学习，特别是基础模型和大语言模型的大规模架构，带来了巨大的计算和能源需求，对可持续性构成显著挑战。&lt;h4&gt;目的&lt;/h4&gt;解决传统基于梯度训练方法的低效性问题，减少计算成本和能源消耗，同时保持模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出混合框架，将神经网络分为两个功能层：底层通过FPGA上的单步方程求解进行高效特征提取；高层采用自适应增量学习支持持续更新。在此基础上引入Compound LLM框架，在两个层次上部署LLM模块，底层处理可重用表示学习，高层进行自适应决策。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著降低计算成本，同时保持高性能模型，特别适合边缘部署和能源受限环境中的实时适应。&lt;h4&gt;结论&lt;/h4&gt;这种集成设计提高了可扩展性，减少了冗余计算，符合可持续AI的原则，为能源受限环境中的深度学习应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习不断增长的计算和能源需求，特别是在基础模型和大语言模型等大规模架构中，对可持续性构成了重大挑战。传统的基于梯度的训练方法效率低下，需要多次迭代更新和高能耗。为解决这些限制，我们提出了一种结合分层分解、基于FPGA的直接方程求解和增量学习的混合框架。我们的方法将神经网络分为两个功能层：底层通过FPGA上的单步方程求解进行优化，实现高效和可并行化的特征提取；而高层采用自适应增量学习，支持持续更新而无需完全重新训练。在此基础上，我们引入了Compound LLM框架，在两个层次上明确部署LLM模块。底层LLM以最低的能源开销处理可重用的表示学习，而顶层LLM通过节能感知的更新执行自适应决策。这种集成设计提高了可扩展性，减少了冗余计算，符合可持续AI的原则。理论分析和架构洞察表明，我们的方法显著降低了计算成本，同时保持了高性能模型，非常适合边缘部署和能源受限环境中的实时适应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rising computational and energy demands of deep learning, particularly inlarge-scale architectures such as foundation models and large language models(LLMs), pose significant challenges to sustainability. Traditionalgradient-based training methods are inefficient, requiring numerous iterativeupdates and high power consumption. To address these limitations, we propose ahybrid framework that combines hierarchical decomposition with FPGA-baseddirect equation solving and incremental learning. Our method divides the neuralnetwork into two functional tiers: lower layers are optimized via single-stepequation solving on FPGAs for efficient and parallelizable feature extraction,while higher layers employ adaptive incremental learning to support continualupdates without full retraining. Building upon this foundation, we introducethe Compound LLM framework, which explicitly deploys LLM modules across bothhierarchy levels. The lower-level LLM handles reusable representation learningwith minimal energy overhead, while the upper-level LLM performs adaptivedecision-making through energy-aware updates. This integrated design enhancesscalability, reduces redundant computation, and aligns with the principles ofsustainable AI. Theoretical analysis and architectural insights demonstratethat our method reduces computational costs significantly while preserving highmodel performance, making it well-suited for edge deployment and real-timeadaptation in energy-constrained environments.</description>
      <author>example@mail.com (Mohammad Saleh Vahdatpour, Huaiyuan Chu, Yanqing Zhang)</author>
      <guid isPermaLink="false">2509.15097v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.15096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为OmniSegmentor的新型多模态学习框架，用于语义分割任务。该框架包含两个关键创新：一个名为ImageNeXt的大规模多模态预训练数据集，以及一种高效的预训练方法，使模型能够编码不同模态的信息。&lt;h4&gt;背景&lt;/h4&gt;最近的研究已经证明多模态线索在鲁棒的语义分割中的优势。然而，针对多种视觉模态的灵活的预训练和微调管道尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;作者旨在开发一个通用的多模态预训练框架，无论涉及的模态如何任意组合，都能在各种场景下一致地增强模型的感知能力。&lt;h4&gt;方法&lt;/h4&gt;基于ImageNet构建了一个名为ImageNeXt的大规模数据集，包含五种流行的视觉模态；提供了一种高效的预训练方式，使模型能够编码ImageNeXt中不同模态的信息；开发了OmniSegmentor框架，这是一个通用的多模态预训练框架。&lt;h4&gt;主要发现&lt;/h4&gt;OmniSegmentor在多种多模态语义分割数据集上取得了新的最先进记录，包括NYU Depthv2、EventScape、MFNet、DeLiVER、SUNRGBD和KITTI-360。&lt;h4&gt;结论&lt;/h4&gt;OmniSegmentor框架成功地解决了多模态视觉学习的挑战，提供了一个灵活且高效的预训练和微调管道，适用于各种视觉模态的组合。&lt;h4&gt;翻译&lt;/h4&gt;最近的表示学习研究已经证明多模态线索对鲁棒的语义分割的益处。然而，针对多种视觉模态的灵活的预训练和微调管道仍未被探索。在本文中，我们提出了一种新型的多模态学习框架，称为OmniSegmentor。它有两个关键创新：1）基于ImageNet，我们构建了一个用于多模态预训练的大规模数据集，称为ImageNeXt，它包含五种流行的视觉模态。2）我们提供了一种高效的预训练方式，使模型能够在ImageNeXt中编码不同模态的信息。我们首次引入了一个通用的多模态预训练框架，无论涉及模态的任意组合如何，都能在各种场景下一致地增强模型的感知能力。值得注意的是，我们的OmniSegmentor在多种多模态语义分割数据集上取得了新的最先进记录，包括NYU Depthv2、EventScape、MFNet、DeLiVER、SUNRGBD和KITTI-360。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research on representation learning has proved the merits ofmulti-modal clues for robust semantic segmentation. Nevertheless, a flexiblepretrain-and-finetune pipeline for multiple visual modalities remainsunexplored. In this paper, we propose a novel multi-modal learning framework,termed OmniSegmentor. It has two key innovations: 1) Based on ImageNet, weassemble a large-scale dataset for multi-modal pretraining, called ImageNeXt,which contains five popular visual modalities. 2) We provide an efficientpretraining manner to endow the model with the capacity to encode differentmodality information in the ImageNeXt. For the first time, we introduce auniversal multi-modal pretraining framework that consistently amplifies themodel's perceptual capabilities across various scenarios, regardless of thearbitrary combination of the involved modalities. Remarkably, our OmniSegmentorachieves new state-of-the-art records on a wide range of multi-modal semanticsegmentation datasets, including NYU Depthv2, EventScape, MFNet, DeLiVER,SUNRGBD, and KITTI-360.</description>
      <author>example@mail.com (Bo-Wen Yin, Jiao-Long Cao, Xuying Zhang, Yuming Chen, Ming-Ming Cheng, Qibin Hou)</author>
      <guid isPermaLink="false">2509.15096v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Representation Learning of Phenotype Trajectories for pCR Prediction in Breast Cancer</title>
      <link>http://arxiv.org/abs/2509.14872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于成像数据的学习方法，用于预测乳腺癌患者对新辅助化疗的病理完全缓解反应。&lt;h4&gt;背景&lt;/h4&gt;有效的治疗决策需要能够预测个体治疗反应的模型，但疾病进展和治疗反应在不同患者间存在显著差异，这使得预测变得具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;从成像数据中学习治疗反应的早期动态表示，以预测接受新辅助化疗(NACT)的乳腺癌患者的病理完全缓解(pCR)。&lt;h4&gt;方法&lt;/h4&gt;利用乳腺磁共振成像(MRI)数据的纵向变化，在潜在空间中形成轨迹，作为成功反应预测的基础。采用多任务模型来表示外观，促进时间连续性，并考虑非反应者队列中较高的异质性。&lt;h4&gt;主要发现&lt;/h4&gt;在公开的ISPY-2数据集上，仅使用治疗前数据(T0)时，潜在轨迹空间中的线性分类器达到0.761的平衡准确率；使用早期反应数据(T0 + T1)时，准确率提高到0.811；使用四个成像时间点(T0 -&gt; T3)时，准确率进一步提高到0.861。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效预测乳腺癌患者对新辅助化疗的反应，且随着时间点的增加，预测准确率显著提高。&lt;h4&gt;翻译&lt;/h4&gt;有效的治疗决策需要能够预测个体治疗反应的模型。这具有挑战性，因为疾病进展和治疗反应在不同患者间存在显著差异。在这里，我们提出从成像数据中学习治疗反应的早期动态表示，以预测接受新辅助化疗(NACT)的乳腺癌患者的病理完全缓解(pCR)。乳腺磁共振成像(MRI)数据的纵向变化在潜在空间中形成轨迹，作为成功反应预测的基础。多任务模型表示外观，促进时间连续性，并考虑非反应者队列中较高的异质性。在公开的ISPY-2数据集上的实验中，潜在轨迹空间中的线性分类器仅使用治疗前数据(T0)时达到0.761的平衡准确率，使用早期反应数据(T0 + T1)时达到0.811，使用四个成像时间点(T0 -&gt; T3)时达到0.861。代码将在论文接受后公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective therapy decisions require models that predict the individualresponse to treatment. This is challenging since the progression of disease andresponse to treatment vary substantially across patients. Here, we propose tolearn a representation of the early dynamics of treatment response from imagingdata to predict pathological complete response (pCR) in breast cancer patientsundergoing neoadjuvant chemotherapy (NACT). The longitudinal change in magneticresonance imaging (MRI) data of the breast forms trajectories in the latentspace, serving as basis for prediction of successful response. The multi-taskmodel represents appearance, fosters temporal continuity and accounts for thecomparably high heterogeneity in the non-responder cohort.In experiments on thepublicly available ISPY-2 dataset, a linear classifier in the latent trajectoryspace achieves a balanced accuracy of 0.761 using only pre-treatment data (T0),0.811 using early response (T0 + T1), and 0.861 using four imaging time points(T0 -&gt; T3). The code will be made available upon paper acceptance.</description>
      <author>example@mail.com (Ivana Janíčková, Yen Y. Tan, Thomas H. Helbich, Konstantin Miloserdov, Zsuzsanna Bago-Horvath, Ulrike Heber, Georg Langs)</author>
      <guid isPermaLink="false">2509.14872v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study</title>
      <link>http://arxiv.org/abs/2509.14863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;G2LFormer是一种新型的全局到局部注意力方案的图变换器，通过浅层捕获全局信息和深层学习局部结构信息，有效解决了传统方法中的信息损失问题，在保持线性复杂度的同时表现出色。&lt;h4&gt;背景&lt;/h4&gt;Graph Transformers (GTs)在图表示学习中显示出巨大潜力，其架构通常将图神经网络(GNNs)与全局注意力机制结合，形成局部与全局或局部到全局的注意力方案。&lt;h4&gt;目的&lt;/h4&gt;提出G2LFormer，解决现有GTs整合方案中可能存在的信息损失问题，防止节点忽略其直接邻居。&lt;h4&gt;方法&lt;/h4&gt;G2LFormer采用全局到局部的注意力方案，浅层网络使用注意力机制捕获全局信息，深层网络使用GNN模块学习局部结构信息；同时引入有效的跨层信息融合策略，使局部层保留全局层的有益信息并减轻信息损失，在可接受的扩展性权衡下进行操作。&lt;h4&gt;主要发现&lt;/h4&gt;G2LFormer在节点级和图级任务上与最先进的线性GTs和GNNs比较，结果表明其在保持线性复杂度的同时表现出色。&lt;h4&gt;结论&lt;/h4&gt;G2LFormer通过全局到局部的注意力方案有效解决了信息损失问题，并在性能和复杂度之间取得了良好的平衡。&lt;h4&gt;翻译&lt;/h4&gt;图变换器(GTs)在图表示学习中显示出巨大的潜力。GTs的架构通常将图神经网络(GNNs)与全局注意力机制结合，形成并行关系或作为注意力机制的先导，从而产生局部与全局或局部到全局的注意力方案。然而，由于全局注意力机制主要捕获节点间的长程依赖关系，这些整合方案可能遭受信息损失，其中GNN学习的局部邻域信息可能被注意力机制稀释。因此，我们提出了G2LFormer，它具有一种新的全局到局部注意力方案，其中浅层网络层使用注意力机制捕获全局信息，而深层层采用GNN模块学习局部结构信息，从而防止节点忽略其直接邻居。引入了一种有效的跨层信息融合策略，使局部层能够保留来自全局层的有益信息并减轻信息损失，同时在可接受的扩展性权衡下进行操作。为了验证全局到局部注意力方案的可行性，我们在节点级和图级任务上将G2LFormer与最先进的线性GTs和GNNs进行了比较。结果表明，G2LFormer在保持线性复杂度的同时表现出优异的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) show considerable potential in graph representationlearning. The architecture of GTs typically integrates Graph Neural Networks(GNNs) with global attention mechanisms either in parallel or as a precursor toattention mechanisms, yielding a local-and-global or local-to-global attentionscheme. However, as the global attention mechanism primarily captureslong-range dependencies between nodes, these integration schemes may sufferfrom information loss, where the local neighborhood information learned by GNNcould be diluted by the attention mechanism. Therefore, we propose G2LFormer,featuring a novel global-to-local attention scheme where the shallow networklayers use attention mechanisms to capture global information, while the deeperlayers employ GNN modules to learn local structural information, therebypreventing nodes from ignoring their immediate neighbors. An effectivecross-layer information fusion strategy is introduced to allow local layers toretain beneficial information from global layers and alleviate informationloss, with acceptable trade-offs in scalability. To validate the feasibility ofthe global-to-local attention scheme, we compare G2LFormer withstate-of-the-art linear GTs and GNNs on node-level and graph-level tasks. Theresults indicate that G2LFormer exhibits excellent performance while keepinglinear complexity.</description>
      <author>example@mail.com (Zhengwei Wang, Gang Wu)</author>
      <guid isPermaLink="false">2509.14863v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation</title>
      <link>http://arxiv.org/abs/2509.14688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CoRL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种具有硬件和算法创新的触觉机器人学习系统，解决了数据稀缺性和稀疏性以及缺乏力反馈的挑战。&lt;h4&gt;背景&lt;/h4&gt;触觉感知的机器人学习面临数据收集和表示的关键挑战，主要问题包括数据稀缺性和稀疏性，以及现有系统中缺乏力反馈。&lt;h4&gt;目的&lt;/h4&gt;解决数据稀缺性和稀疏性以及缺乏力反馈的限制，引入具有硬件和算法创新的触觉机器人学习系统。&lt;h4&gt;方法&lt;/h4&gt;提出exUMI可扩展数据收集设备，增强原始UMI系统，配备AR动作捕捉和旋转编码器的鲁棒本体感受、模块化视觉-触觉传感和自动校准功能，实现100%数据可用性；基于超过100万个触觉帧的高效收集，提出触觉预测预训练(TPP)表示学习框架，通过感知感知的时间触觉预测捕捉接触动力学并缓解触觉稀疏性。&lt;h4&gt;主要发现&lt;/h4&gt;TPPP在现实世界实验中表现优于传统的触觉模仿学习方法。&lt;h4&gt;结论&lt;/h4&gt;通过协同设计的硬件和算法，弥合了人类触觉直觉与机器人学习之间的差距，提供开源资源以促进接触丰富的操作研究。&lt;h4&gt;翻译&lt;/h4&gt;触觉感知的机器人学习因数据稀缺性和稀疏性以及现有系统中缺乏力反馈而面临关键的数据收集和表示挑战。为解决这些限制，我们引入了具有硬件和算法创新的触觉机器人学习系统。我们提出了exUMI，一个可扩展的数据收集设备，通过AR动作捕捉和旋转编码器增强原始UMI的鲁棒本体感受，配备模块化视觉-触觉传感和自动校准功能，实现了100%的数据可用性。基于超过100万个触觉帧的高效收集，我们提出了触觉预测预训练(TPP)，一种通过感知感知的时间触觉预测的表示学习框架，捕捉接触动力学并缓解触觉稀疏性。现实世界的实验表明，TPPP优于传统的触觉模仿学习。我们的工作通过协同设计的硬件和算法弥合了人类触觉直觉与机器人学习之间的差距，为推进接触丰富的操作研究提供了开源资源。项目页面：https://silicx.github.io/exUMI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tactile-aware robot learning faces critical challenges in data collection andrepresentation due to data scarcity and sparsity, and the absence of forcefeedback in existing systems. To address these limitations, we introduce atactile robot learning system with both hardware and algorithm innovations. Wepresent exUMI, an extensible data collection device that enhances the vanillaUMI with robust proprioception (via AR MoCap and rotary encoder), modularvisuo-tactile sensing, and automated calibration, achieving 100% datausability. Building on an efficient collection of over 1 M tactile frames, wepropose Tactile Prediction Pretraining (TPP), a representation learningframework through action-aware temporal tactile prediction, capturing contactdynamics and mitigating tactile sparsity. Real-world experiments show that TPPoutperforms traditional tactile imitation learning. Our work bridges the gapbetween human tactile intuition and robot learning through co-designed hardwareand algorithms, offering open-source resources to advance contact-richmanipulation research. Project page: https://silicx.github.io/exUMI.</description>
      <author>example@mail.com (Yue Xu, Litao Wei, Pengyu An, Qingyu Zhang, Yong-Lu Li)</author>
      <guid isPermaLink="false">2509.14688v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training</title>
      <link>http://arxiv.org/abs/2509.14642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeCoP是一种依赖控制预训练框架，通过显式建模动态多尺度依赖关系来解决时间序列预训练中的挑战，在十个数据集上实现了最先进结果，且计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;时间序列预训练中动态时间依赖关系建模是关键挑战，这些依赖因分布漂移和多尺度模式而演变，严重损害预训练模型在下游任务上的泛化能力。现有框架无法捕捉短期和长期依赖关系的复杂交互，容易受虚假相关性影响。&lt;h4&gt;目的&lt;/h4&gt;解决现有框架在捕捉动态、多尺度依赖关系方面的局限性，提出能够明确模拟演化补丁间依赖关系的方法。&lt;h4&gt;方法&lt;/h4&gt;提出DeCoP框架，包含：1)输入层的实例级补丁归一化(IPN)减轻分布漂移并保留补丁独特特征；2)潜在层的层次化依赖控制学习(DCL)策略建模跨时间尺度的补丁间依赖关系；3)实例级对比模块(ICM)通过学习时间不变正对的实例判别性表示增强全局泛化。&lt;h4&gt;主要发现&lt;/h4&gt;DeCoP在十个数据集上实现最先进结果，与PatchTST相比，在仅使用37% FLOPs的情况下，在ETTh1上将MSE提高3%。&lt;h4&gt;结论&lt;/h4&gt;DeCoP是有效的时间序列预训练框架，能够处理动态时间依赖关系，在计算资源较少的情况下仍能取得优异性能。&lt;h4&gt;翻译&lt;/h4&gt;建模动态时间依赖关系是时间序列预训练中的关键挑战，这些依赖关系会因分布漂移和多尺度模式而演变。这种时间变化性严重损害了预训练模型在下游任务上的泛化能力。现有框架无法捕捉短期和长期依赖关系的复杂交互，容易受到虚假相关性的影响，从而降低泛化能力。为解决这些局限性，我们提出了DeCoP，一种依赖控制预训练框架，通过模拟演化的补丁间依赖关系来明确建模动态、多尺度依赖关系。在输入层面，DeCoP引入实例级补丁归一化(IPN)来减轻分布漂移，同时保留每个补丁的独特特征，为表示学习创建稳健基础。在潜在层面，层次化的依赖控制学习(DCL)策略明确建模跨多个时间尺度的补丁间依赖关系，实例级对比模块(ICM)通过从不随时间变化的正对中学习实例判别性表示来增强全局泛化。DeCoP在十个数据集上以更低的计算资源实现了最先进结果，与PatchTST相比，在仅使用37% FLOPs的情况下，在ETTh1上将MSE提高了3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling dynamic temporal dependencies is a critical challenge in time seriespre-training, which evolve due to distribution shifts and multi-scale patterns.This temporal variability severely impairs the generalization of pre-trainedmodels to downstream tasks. Existing frameworks fail to capture the complexinteractions of short- and long-term dependencies, making them susceptible tospurious correlations that degrade generalization. To address theselimitations, we propose DeCoP, a Dependency Controlled Pre-training frameworkthat explicitly models dynamic, multi-scale dependencies by simulating evolvinginter-patch dependencies. At the input level, DeCoP introduces Instance-wisePatch Normalization (IPN) to mitigate distributional shifts while preservingthe unique characteristics of each patch, creating a robust foundation forrepresentation learning. At the latent level, a hierarchical DependencyControlled Learning (DCL) strategy explicitly models inter-patch dependenciesacross multiple temporal scales, with an Instance-level Contrastive Module(ICM) enhances global generalization by learning instance-discriminativerepresentations from time-invariant positive pairs. DeCoP achievesstate-of-the-art results on ten datasets with lower computing resources,improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.</description>
      <author>example@mail.com (Yuemin Wu, Zhongze Wu, Xiu Su, Feng Yang, Hongyan Xu, Xi Lin, Wenti Huang, Shan You, Chang Xu)</author>
      <guid isPermaLink="false">2509.14642v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering</title>
      <link>http://arxiv.org/abs/2509.15024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AGCN(Attentive Graph Clustering Network)，一种将注意力机制直接嵌入图结构的新型架构，用于解决图聚类任务中GNN和Transformer的互补弱点问题。&lt;h4&gt;背景&lt;/h4&gt;注意力机制已成为现代神经网络的基石，但在图结构数据中的应用尚未充分探索，相比图神经网络表现较差，特别是在图聚类任务中。&lt;h4&gt;目的&lt;/h4&gt;解决图聚类任务中GNN过度强调邻域聚合而Transformer过度全局化的问题，探究注意力机制是否对无监督图学习本质上冗余，并提出一种新方法结合两者的优势。&lt;h4&gt;方法&lt;/h4&gt;提出Attentive Graph Clustering Network (AGCN)，直接将注意力机制嵌入图结构，同时引入KV缓存机制提高计算效率和成对边界对比损失增强注意力空间的判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和Transformer在图聚类中存在互补弱点，GNN过度强调邻域聚合导致节点表示同质化，而Transformer过度全局化牺牲了有意义的局部模式。&lt;h4&gt;结论&lt;/h4&gt;AGCN通过直接嵌入注意力机制到图结构中，能够有效提取全局信息同时保持对局部拓扑线索的敏感性，实验证明其性能优于最先进的方法。&lt;h4&gt;翻译&lt;/h4&gt;注意力机制已成为现代神经网络的基石，推动着各个领域的突破。然而，其在需要捕获拓扑连接的图结构数据中的应用仍然探索不足，相比图神经网络(GNN)表现较差，特别是在图聚类任务中。GNN倾向于过度强调邻域聚合，导致节点表示的同质化。相反，Transformer倾向于过度全局化，强调远距离节点而牺牲有意义的局部模式。这种二元对立引发了一个关键问题：注意力机制是否对无监督图学习本质上冗余？为了解决这个问题，我们进行了全面的实证分析，揭示了GNN和Transformer在图聚类中的互补弱点。受这些见解的启发，我们提出了Attentive Graph Clustering Network (AGCN)，一种重新诠释'图即注意力'概念的新架构。AGCN直接将注意力机制嵌入图结构，能够有效提取全局信息同时保持对局部拓扑线索的敏感性。我们的框架包含理论分析，对比了AGCN与GNN和Transformer的行为差异，并引入了两项创新：(1) KV缓存机制提高计算效率，(2) 成对边界对比损失增强注意力空间的判别能力。广泛的实验结果表明，AGCN优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attention mechanisms have become a cornerstone in modern neural networks,driving breakthroughs across diverse domains. However, their application tograph structured data, where capturing topological connections is essential,remains underexplored and underperforming compared to Graph Neural Networks(GNNs), particularly in the graph clustering task. GNN tends to overemphasizeneighborhood aggregation, leading to a homogenization of node representations.Conversely, Transformer tends to over globalize, highlighting distant nodes atthe expense of meaningful local patterns. This dichotomy raises a key question:Is attention inherently redundant for unsupervised graph learning? To addressthis, we conduct a comprehensive empirical analysis, uncovering thecomplementary weaknesses of GNN and Transformer in graph clustering. Motivatedby these insights, we propose the Attentive Graph Clustering Network (AGCN) anovel architecture that reinterprets the notion that graph is attention. AGCNdirectly embeds the attention mechanism into the graph structure, enablingeffective global information extraction while maintaining sensitivity to localtopological cues. Our framework incorporates theoretical analysis to contrastAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KVcache mechanism to improve computational efficiency, and (2) a pairwise margincontrastive loss to boost the discriminative capacity of the attention space.Extensive experimental results demonstrate that AGCN outperformsstate-of-the-art methods.</description>
      <author>example@mail.com (Xuanting Xie, Bingheng Li, Erlin Pan, Rui Hou, Wenyu Chen, Zhao Kang)</author>
      <guid isPermaLink="false">2509.15024v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis</title>
      <link>http://arxiv.org/abs/2509.14965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为Brain-HGCN的新型几何深度学习框架，基于双曲几何，用于处理功能性磁共振成像(fMRI)数据，以更好地建模大脑网络的层次结构。该框架在精神障碍分类任务中表现优异，显著优于多种最先进的欧几里得基线方法。&lt;h4&gt;背景&lt;/h4&gt;功能性磁共振成像(fMRI)通过生成复杂的功能网络（通常建模为图）提供了一种强大的非侵入性观察大脑功能组织的窗口。大脑网络表现出对认知处理至关重要的层次拓扑结构。然而，由于固有的空间限制，标准的欧几里得图神经网络(GNN)难以在没有高失真的情况下表示这些层次结构，限制了它们的临床性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更准确地表示大脑网络层次结构的几何深度学习框架，解决标准欧几里得GNN在处理fMRI数据时的局限性。&lt;h4&gt;方法&lt;/h4&gt;研究提出了Brain-HGCN，一种基于双曲几何的几何深度学习框架，利用负曲率空间的内在特性来高保真地建模大脑网络的层次结构。该模型基于洛伦兹模型，采用了一种新颖的双曲图注意力层，具有符号聚合机制，可以分别处理兴奋性和抑制性连接，最终通过几何上合理的弗雷谢均值进行图读出，学习鲁棒的图级表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模fMRI数据集上进行的精神障碍分类实验表明，该方法显著优于多种最先进的欧几里得基线方法。&lt;h4&gt;结论&lt;/h4&gt;这项工作为fMRI分析开创了一种新的几何深度学习范式，突显了双曲图神经计算精神病学领域的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;功能性磁共振成像(fMRI)通过生成复杂的功能网络（通常建模为图）提供了一种强大的非侵入性观察大脑功能组织的窗口。这些大脑网络表现出对认知处理至关重要的层次拓扑结构。然而，由于固有的空间限制，标准的欧几里得图神经网络(GNN)难以在没有高失真的情况下表示这些层次结构，限制了它们的临床性能。为解决这一局限性，我们提出了Brain-HGCN，一种基于双曲几何的几何深度学习框架，利用负曲率空间的内在特性来高保真地建模大脑网络的层次结构。基于洛伦兹模型，我们的模型采用了一种新颖的双曲图注意力层，具有符号聚合机制，可以分别处理兴奋性和抑制性连接，最终通过几何上合理的弗雷谢均值进行图读出，学习鲁棒的图级表示。在两个用于精神障碍分类的大规模fMRI数据集上的实验表明，我们的方法显著优于多种最先进的欧几里得基线方法。这项工作为fMRI分析开创了一种新的几何深度学习范式，突显了双曲图神经计算精神病学领域的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Functional magnetic resonance imaging (fMRI) provides a powerful non-invasivewindow into the brain's functional organization by generating complexfunctional networks, typically modeled as graphs. These brain networks exhibita hierarchical topology that is crucial for cognitive processing. However, dueto inherent spatial constraints, standard Euclidean GNNs struggle to representthese hierarchical structures without high distortion, limiting their clinicalperformance. To address this limitation, we propose Brain-HGCN, a geometricdeep learning framework based on hyperbolic geometry, which leverages theintrinsic property of negatively curved space to model the brain's networkhierarchy with high fidelity. Grounded in the Lorentz model, our model employsa novel hyperbolic graph attention layer with a signed aggregation mechanism todistinctly process excitatory and inhibitory connections, ultimately learningrobust graph-level representations via a geometrically sound Fr\'echet mean forgraph readout. Experiments on two large-scale fMRI datasets for psychiatricdisorder classification demonstrate that our approach significantly outperformsa wide range of state-of-the-art Euclidean baselines. This work pioneers a newgeometric deep learning paradigm for fMRI analysis, highlighting the immensepotential of hyperbolic GNNs in the field of computational psychiatry.</description>
      <author>example@mail.com (Junhao Jia, Yunyou Liu, Cheng Yang, Yifei Sun, Feiwei Qin, Changmiao Wang, Yong Peng)</author>
      <guid isPermaLink="false">2509.14965v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Towards Pre-trained Graph Condensation via Optimal Transport</title>
      <link>http://arxiv.org/abs/2509.14722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于最优传输的预训练图压缩方法(PreGC)，解决了传统GC方法依赖刚性GNN和任务特定监督的问题，实现了任务和架构无关的图压缩。&lt;h4&gt;背景&lt;/h4&gt;传统图压缩(GC)方法旨在将原始图压缩为小规模图以减少冗余并加速GNN训练，但这些方法严重依赖刚性GNN和任务特定监督，限制了其可重用性和跨任务/架构的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;从GNN优化一致性的角度重新审视理想GC的目标，提出一种任务和架构无关的图压缩方法。&lt;h4&gt;方法&lt;/h4&gt;提出基于最优传输的预训练图压缩(PreGC)，包括混合区间图扩散增强来提高压缩图泛化能力，建立最优图传输计划与表示传输计划之间的匹配保持语义一致性，以及可追溯的语义协调器桥接语义关联。&lt;h4&gt;主要发现&lt;/h4&gt;通过广义GC优化目标推导，传统GC方法可作为该优化范式的特例；PreGC方法具有任务无关性质并能与任意GNN无缝兼容。&lt;h4&gt;结论&lt;/h4&gt;PreGC方法超越了任务和架构依赖的GC方法的局限性，在实验中展示了优越性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;图压缩(GC)旨在将原始图压缩为小规模图，减少冗余并加速GNN训练。然而，传统GC方法严重依赖刚性GNN和任务特定监督。这种依赖严重限制了它们在各种任务和架构中的可重用性和泛化能力。在本工作中，我们从GNN优化一致性的角度重新审视了理想GC的目标，然后推导出广义GC优化目标，通过这个目标，那些传统GC方法可以很好地被视为这种优化范式的特例。基于此，提出了基于最优传输的预训练图压缩(PreGC)，以超越任务和架构依赖的GC方法的局限性。具体而言，提出了混合区间图扩散增强，通过增强节点状态的不确定性来抑制压缩图在特定架构上的弱泛化能力。同时，巧妙地建立了最优图传输计划与表示传输计划之间的匹配，以保持源图和压缩图空间间的语义一致性，从而使图压缩摆脱任务依赖。为了进一步促进压缩图适应各种下游任务，提出了从源节点到压缩节点的可追溯语义协调器，通过预训练中的优化表示传输计划桥接语义关联。大量实验验证了PreGC的优越性和通用性，展示了其任务无关性质和与任意GNN的无缝兼容性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph condensation (GC) aims to distill the original graph into a small-scalegraph, mitigating redundancy and accelerating GNN training. However,conventional GC approaches heavily rely on rigid GNNs and task-specificsupervision. Such a dependency severely restricts their reusability andgeneralization across various tasks and architectures. In this work, we revisitthe goal of ideal GC from the perspective of GNN optimization consistency, andthen a generalized GC optimization objective is derived, by which thosetraditional GC methods can be viewed nicely as special cases of thisoptimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC)via optimal transport is proposed to transcend the limitations of task- andarchitecture-dependent GC methods. Specifically, a hybrid-interval graphdiffusion augmentation is presented to suppress the weak generalization abilityof the condensed graph on particular architectures by enhancing the uncertaintyof node states. Meanwhile, the matching between optimal graph transport planand representation transport plan is tactfully established to maintain semanticconsistencies across source graph and condensed graph spaces, thereby freeinggraph condensation from task dependencies. To further facilitate the adaptationof condensed graphs to various downstream tasks, a traceable semanticharmonizer from source nodes to condensed nodes is proposed to bridge semanticassociations through the optimized representation transport plan inpre-training. Extensive experiments verify the superiority and versatility ofPreGC, demonstrating its task-independent nature and seamless compatibilitywith arbitrary GNNs.</description>
      <author>example@mail.com (Yeyu Yan, Shuai Zheng, Wenjun Hui, Xiangkai Zhu, Dong Chen, Zhenfeng Zhu, Yao Zhao, Kunlun He)</author>
      <guid isPermaLink="false">2509.14722v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control</title>
      <link>http://arxiv.org/abs/2509.14431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LEGO的局部规范化等变图神经网络框架，用于解决多智能体强化学习中的关键挑战，特别是在竞争环境和不同智能体数量下的泛化问题。&lt;h4&gt;背景&lt;/h4&gt;多智能体强化学习(MARL)已成为协调复杂决策中智能体群体的强大范式，但仍存在重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决多智能体强化学习中的三个主要挑战：竞争环境中的训练不稳定、非动能对抗措施在不利条件下的失效、以及训练策略难以泛化到不同智能体数量的环境。&lt;h4&gt;方法&lt;/h4&gt;提出LEGO框架，该框架使用图神经网络捕获排列等变性并泛化到不同数量的智能体，通过规范化强制执行E(n)-等变性，并采用异构表示编码角色特定的归纳偏置，可与流行的MARL算法如MAPPO无缝集成。&lt;h4&gt;主要发现&lt;/h4&gt;在合作和竞争性群体基准测试中，LEGO优于强大的基线方法并提高了泛化能力；在真实世界的实验中，LEGO展示了对不同团队规模和智能体故障的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;LEGO框架有效解决了多智能体强化学习中的关键挑战，特别是在竞争环境和不同智能体数量下的泛化问题，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;多智能体强化学习(MARL)已成为协调复杂决策中智能体群体的强大范式，但仍存在重大挑战。在追逐者-逃避者等竞争性场景中，同时适应可能导致训练不稳定；非动能对抗措施在不利条件下往往失效；在一种配置中训练的策略很少能泛化到具有不同智能体数量的环境。为解决这些问题，我们提出了局部规范化等变图神经网络(LEGO)框架，该框架可与流行的MARL算法(如MAPPO)无缝集成。LEGO使用图神经网络来捕获排列等变性并泛化到不同数量的智能体，通过规范化强制执行E(n)-等变性，并使用异构表示来编码角色特定的归纳偏置。在合作和竞争性群体基准测试中，LEGO优于强大的基线方法并提高了泛化能力。在真实世界的实验中，LEGO展示了对不同团队规模和智能体故障的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigmfor coordinating swarms of agents in complex decision-making, yet majorchallenges remain. In competitive settings such as pursuer-evader tasks,simultaneous adaptation can destabilize training; non-kinetic countermeasuresoften fail under adverse conditions; and policies trained in one configurationrarely generalize to environments with a different number of agents. To addressthese issues, we propose the Local-Canonicalization Equivariant Graph NeuralNetworks (LEGO) framework, which integrates seamlessly with popular MARLalgorithms such as MAPPO. LEGO employs graph neural networks to capturepermutation equivariance and generalization to different agent numbers,canonicalization to enforce E(n)-equivariance, and heterogeneousrepresentations to encode role-specific inductive biases. Experiments oncooperative and competitive swarm benchmarks show that LEGO outperforms strongbaselines and improves generalization. In real-world experiments, LEGOdemonstrates robustness to varying team sizes and agent failure.</description>
      <author>example@mail.com (Keqin Wang, Tao Zhong, David Chang, Christine Allen-Blanchette)</author>
      <guid isPermaLink="false">2509.14431v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>How Bad Is Forming Your Own Multidimensional Opinion?</title>
      <link>http://arxiv.org/abs/2509.14411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Appeared in 26th ACM Conference on Economics and Computation (EC'25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究社交网络中互联话题上观点形成的模型，特别是在多维模型下提供无政府状态价格的紧边界，并探讨更复杂的相互依赖关系和群体结构的影响。&lt;h4&gt;背景&lt;/h4&gt;社交网络中互联话题上的观点形成对理解集体行为和决策具有重要意义，现有模型基于个人观点是同伴观点与自身信念的加权平均这一假设，该过程被视为最佳响应博弈。&lt;h4&gt;目的&lt;/h4&gt;为多维观点形成模型提供无政府状态价格的紧边界，并将模型推广到更复杂的相互依赖关系，同时研究非二次惩罚下的边界情况。&lt;h4&gt;方法&lt;/h4&gt;遵循Bhawalkar、Gollapudi和Munagala的研究方法，分析多维模型下的无政府状态价格，并扩展到群体内部和外部分歧惩罚的场景。&lt;h4&gt;主要发现&lt;/h4&gt;多维模型下的无政府状态价格边界与标量模型匹配；即使增加群体内部和外部分歧惩罚的复杂性，这些边界仍然保持不变。&lt;h4&gt;结论&lt;/h4&gt;研究解决了多维模型中无政府状态价格边界未知的开放问题，发现即使考虑更复杂的相互依赖关系和群体结构，边界仍然保持不变。&lt;h4&gt;翻译&lt;/h4&gt;理解社交网络中互联话题上的观点形成具有重要意义。它为集体行为和决策提供了见解，并在图神经网络中有应用。现有模型提出个人观点是基于同龄人观点和自身信念的加权平均值形成的。这个平均过程被视为最佳响应博弈，可以看作是个体最小化与同伴的分歧，通过二次惩罚定义，导致均衡状态。Bindel、Kleinberg和Oren（FOCS 2011）提供了无政府状态价格的紧边界，定义为均衡时总体最大分歧与社会最优值的比率。Bhawalkar、Gollapudi和Munagala（STOC 2013）将惩罚函数推广到非二次惩罚，并提供了无政府状态价格的紧边界。当考虑多个话题时，个人观点可以表示为向量。Parsegov、Proskurnikov、Tempo和Friedkin（2016）提出了一个多维模型，使用加权平均过程，但话题间有恒定的相互依赖关系。然而，该模型的无政府状态价格边界问题仍然悬而未决。我们通过为多维模型提供无政府状态价格的紧边界来解决这一问题，同时将其推广到更复杂的相互依赖关系。遵循Bhawalkar、Gollapudi和Munagala的工作，我们提供了非二次惩罚下无政府状态价格的紧边界。令人惊讶的是，这些边界与标量模型匹配。我们进一步证明，即使增加另一层复杂性，涉及个人组最小化其整体内部和外部分歧惩罚（现实生活中常见的情况），这些边界仍然保持不变。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3736252.3742669&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the formation of opinions on interconnected topics withinsocial networks is of significant importance. It offers insights intocollective behavior and decision-making, with applications in Graph NeuralNetworks. Existing models propose that individuals form opinions based on aweighted average of their peers' opinions and their own beliefs. This averagingprocess, viewed as a best-response game, can be seen as an individualminimizing disagreements with peers, defined by a quadratic penalty, leading toan equilibrium. Bindel, Kleinberg, and Oren (FOCS 2011) provided tight boundson the "price of anarchy" defined as the maximum overall disagreement atequilibrium relative to a social optimum. Bhawalkar, Gollapudi, and Munagala(STOC 2013) generalized the penalty function to non-quadratic penalties andprovided tight bounds on the price of anarchy.  When considering multiple topics, an individual's opinions can be representedas a vector. Parsegov, Proskurnikov, Tempo, and Friedkin (2016) proposed amultidimensional model using the weighted averaging process, but with constantinterdependencies between topics. However, the question of the price of anarchyfor this model remained open. We address this by providing tight bounds on themultidimensional model, while also generalizing it to more complexinterdependencies. Following the work of Bhawalkar, Gollapudi, and Munagala, weprovide tight bounds on the price of anarchy under non-quadratic penalties.Surprisingly, these bounds match the scalar model. We further demonstrate thatthe bounds remain unchanged even when adding another layer of complexity,involving groups of individuals minimizing their overall internal and externaldisagreement penalty, a common occurrence in real-life scenarios.</description>
      <author>example@mail.com (Kiarash Banihashem, MohammadTaghi Hajiaghayi, Mahdi JafariRaviz, Danny Mittal, Alipasha Montaseri)</author>
      <guid isPermaLink="false">2509.14411v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Property-Isometric Variational Autoencoders for Sequence Modeling and Design</title>
      <link>http://arxiv.org/abs/2509.14287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 6 figures, preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PrIVAE的保留几何特性的变分自编码器框架，用于设计具有理想功能特性的生物序列（DNA、RNA或肽），解决了现有模型无法优化复杂高维特性的挑战。&lt;h4&gt;背景&lt;/h4&gt;生物序列设计在发现新型纳米材料、生物传感器、抗菌药物等领域有广泛应用，但优化复杂高维特性（如DNA介导的荧光纳米颗粒的发射光谱、光化学稳定性以及抗菌肽的抗菌活性）是一大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型依赖简单二元标签而非高维复杂特性的问题，开发一种能够学习保留属性空间几何特性的序列设计框架。&lt;h4&gt;方法&lt;/h4&gt;PrIVAE框架将属性空间建模为高维流形，通过最近邻图近似，并使用图神经网络编码器层和等距正则化器来指导序列潜在表示，学习一个属性组织的潜在空间，使训练的解码器能够设计具有理想属性的新序列。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在DNA序列设计（用于模板荧光金属纳米团簇）和抗菌肽设计两个任务中表现出色，保持高重建精度的同时根据属性组织潜在空间；湿实验显示，稀有属性纳米团的富集度比训练数据中高16.1倍。&lt;h4&gt;结论&lt;/h4&gt;PrIVAE框架能够有效解决生物序列设计中优化复杂高维特性的挑战，在实际应用中具有显著实用价值，可大幅提高稀有属性纳米团的富集度。&lt;h4&gt;翻译&lt;/h4&gt;生物序列设计（DNA、RNA或肽）具有理想功能特性，在发现新型纳米材料、生物传感器、抗菌药物等领域有应用。一个常见挑战是优化复杂的高维特性，如DNA介导的荧光纳米颗粒的目标发射光谱、光化学稳定性以及针对目标微生物的肽的抗菌活性。现有模型依赖简单的二元标签（如结合/非结合）而非高维复杂特性。为解决这一差距，我们提出了一个保留几何特性的变分自编码器框架PrIVAE，它学习保留属性空间几何特性的潜在序列嵌入。具体而言，我们将属性空间建模为高维流形，在适当定义的距离度量下可由最近邻图局部近似。我们使用属性图通过（1）图神经网络编码器层和（2）等距正则化器来指导序列潜在表示。PrIVAE学习一个属性组织的潜在空间，通过训练的解码器实现具有理想属性的新序列的合理设计。我们评估了该框架在两个生成任务中的效用：（1）设计DNA序列以模板荧光金属纳米团簇，和（2）设计抗菌肽。训练的模型保持高重建精度的同时根据属性组织潜在空间。除了计算机模拟实验外，我们还使用采样序列进行DNA纳米团湿实验室设计，与训练数据中的丰度相比，稀有属性纳米团富集度高达16.1倍，证明了该框架的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biological sequence design (DNA, RNA, or peptides) with desired functionalproperties has applications in discovering novel nanomaterials, biosensors,antimicrobial drugs, and beyond. One common challenge is the ability tooptimize complex high-dimensional properties such as target emission spectra ofDNA-mediated fluorescent nanoparticles, photo and chemical stability, andantimicrobial activity of peptides across target microbes. Existing models relyon simple binary labels (e.g., binding/non-binding) rather thanhigh-dimensional complex properties. To address this gap, we propose ageometry-preserving variational autoencoder framework, called PrIVAE, whichlearns latent sequence embeddings that respect the geometry of their propertyspace. Specifically, we model the property space as a high-dimensional manifoldthat can be locally approximated by a nearest neighbor graph, given anappropriately defined distance measure. We employ the property graph to guidethe sequence latent representations using (1) graph neural network encoderlayers and (2) an isometric regularizer. PrIVAE learns a property-organizedlatent space that enables rational design of new sequences with desiredproperties by employing the trained decoder. We evaluate the utility of ourframework for two generative tasks: (1) design of DNA sequences that templatefluorescent metal nanoclusters and (2) design of antimicrobial peptides. Thetrained models retain high reconstruction accuracy while organizing the latentspace according to properties. Beyond in silico experiments, we also employsampled sequences for wet lab design of DNA nanoclusters, resulting in up to16.1-fold enrichment of rare-property nanoclusters compared to their abundancein training data, demonstrating the practical utility of our framework.</description>
      <author>example@mail.com (Elham Sadeghi, Xianqi Deng, I-Hsin Lin, Stacy M. Copp, Petko Bogdanov)</author>
      <guid isPermaLink="false">2509.14287v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.15033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种改进多变量异常检测的方法，通过建模多元时间序列数据中的时变非线性时空相关性，解决了现有方法假设变量独立性而忽略现实世界交互的问题。&lt;h4&gt;背景&lt;/h4&gt;在多变量时间序列数据中，异常可能表现为相互关联的时间序列同时偏离预期集体行为，即使单个时间序列本身无明显异常。现有方法通常假设时间序列变量是(条件)独立的，这简化了现实世界的复杂交互关系。&lt;h4&gt;目的&lt;/h4&gt;改进多变量异常检测性能，通过准确建模多元时间序列数据中的时变非线性时空相关性，捕捉变量间的复杂依赖关系。&lt;h4&gt;方法&lt;/h4&gt;作者提出的方法在潜在空间中建模联合依赖关系，并解耦边际分布、时间动态和变量间依赖的建模。使用transformer编码器捕获时间模式，通过拟合多变量似然和copula建模空间(变量间)依赖。时间和空间组件在潜在空间中使用自监督对比学习目标联合训练，学习能分离正常和异常样本的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过建模时间序列间的复杂依赖关系，即使单个时间序列无明显异常，也能检测出多变量数据中的异常模式。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过同时考虑时间动态和变量间依赖关系，提高了多变量异常检测的准确性和鲁棒性，能够更有效地识别复杂场景中的异常。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们旨在通过建模多元时间序列数据中发现的时变非线性时空相关性来改进多变量异常检测。在多元时间序列数据中，异常可能表现为相互关联的时间序列同时偏离其预期的集体行为，即使单个时间序列本身没有表现出明显的异常模式。在许多现有方法中，时间序列变量被假设为(条件)独立的，这简化了现实世界的交互。我们的方法通过在潜在空间中建模联合依赖关系，并解耦边际分布、时间动态和变量间依赖的建模来解决这一问题。我们使用transformer编码器来捕获时间模式，为了建模空间依赖，我们拟合了多变量似然和copula。时间和空间组件在潜在空间中使用自监督对比学习目标联合训练，以学习有意义的特征表示来分离正常和异常样本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we aim to improve multivariate anomaly detection (AD) bymodeling the \textit{time-varying non-linear spatio-temporal correlations}found in multivariate time series data . In multivariate time series data, ananomaly may be indicated by the simultaneous deviation of interrelated timeseries from their expected collective behavior, even when no individual timeseries exhibits a clearly abnormal pattern on its own. In many existingapproaches, time series variables are assumed to be (conditionally)independent, which oversimplifies real-world interactions. Our approachaddresses this by modeling joint dependencies in the latent space anddecoupling the modeling of \textit{marginal distributions, temporal dynamics,and inter-variable dependencies}. We use a transformer encoder to capturetemporal patterns, and to model spatial (inter-variable) dependencies, we fit amulti-variate likelihood and a copula. The temporal and the spatial componentsare trained jointly in a latent space using a self-supervised contrastivelearning objective to learn meaningful feature representations to separatenormal and anomaly samples.</description>
      <author>example@mail.com (Padmaksha Roy, Almuatazbellah Boker, Lamine Mili)</author>
      <guid isPermaLink="false">2509.15033v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Temporally Heterogeneous Graph Contrastive Learning for Multimodal Acoustic event Classification</title>
      <link>http://arxiv.org/abs/2509.14893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于时序异构图的对比学习方法(THGCL)，用于解决多模态声学事件分类中的时间对齐和跨模态噪声问题，在AudioSet数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态声学事件分类在音频-视觉系统中起着关键作用。虽然结合音频和视觉信号可以提高识别性能，但仍然难以在时间上对齐它们并减少跨模态噪声的影响。现有方法通常分别处理音频和视觉流，稍后使用对比学习或互信息目标融合特征。最近的进展探索了多模态图学习，但大多数方法无法区分模态内和模态间的时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法无法区分模态内和模态间时间依赖性的问题，作者提出了基于时序异构图的对比学习方法(THGCL)。&lt;h4&gt;方法&lt;/h4&gt;THGCL框架为每个事件构建一个时间图，其中音频和视频片段形成节点，它们的时间链接形成边。引入了高斯过程用于模态内平滑，霍克斯过程用于模态间衰减，以及对比学习来捕获细粒度关系。&lt;h4&gt;主要发现&lt;/h4&gt;在AudioSet上的实验表明，THGCL实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过提出THGCL方法，作者成功解决了多模态声学事件分类中对齐和噪声问题，并取得了优于现有方法的性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态声学事件分类在音频-视觉系统中起着关键作用。虽然结合音频和视觉信号可以提高识别，但仍然难以在时间上对齐它们并减少跨模态噪声的影响。现有方法通常分别处理音频和视觉流，稍后使用对比或互信息目标融合特征。最近的进展探索了多模态图学习，但大多数方法无法区分模态内和模态间的时间依赖性。为了解决这个问题，我们提出了基于时序异构图的对比学习(THGCL)。我们的框架为每个事件构建一个时间图，其中音频和视频片段形成节点，它们的时间链接形成边。我们引入高斯过程用于模态内平滑，霍克斯过程用于模态间衰减，以及对比学习来捕获细粒度关系。AudioSet上的实验表明，THGCL实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal acoustic event classification plays a key role in audio-visualsystems. Although combining audio and visual signals improves recognition, itis still difficult to align them over time and to reduce the effect of noiseacross modalities. Existing methods often treat audio and visual streamsseparately, fusing features later with contrastive or mutual informationobjectives. Recent advances explore multimodal graph learning, but most fail todistinguish between intra- and inter-modal temporal dependencies. To addressthis, we propose Temporally Heterogeneous Graph-based Contrastive Learning(THGCL). Our framework constructs a temporal graph for each event, where audioand video segments form nodes and their temporal links form edges. We introduceGaussian processes for intra-modal smoothness, Hawkes processes for inter-modaldecay, and contrastive learning to capture fine-grained relationships.Experiments on AudioSet show that THGCL achieves state-of-the-art performance.</description>
      <author>example@mail.com (Yuanjian Chen, Yang Xiao, Jinjie Huang)</author>
      <guid isPermaLink="false">2509.14893v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery</title>
      <link>http://arxiv.org/abs/2509.14788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于序列的药物-靶点相互作用框架，整合结构先验到蛋白质表示中，同时保持高通量筛选能力。该模型在多个基准测试中表现优异，在虚拟筛选任务中超越先前方法，消融研究和嵌入可视化证实了模型各组件的关键作用。&lt;h4&gt;背景&lt;/h4&gt;药物-靶点相互作用的准确识别是计算药理学中的中心挑战，基于序列的方法提供了可扩展性。&lt;h4&gt;目的&lt;/h4&gt;引入一种基于序列的药物-靶点相互作用框架，将结构先验整合到蛋白质表示中，同时保持高通量筛选能力。&lt;h4&gt;方法&lt;/h4&gt;开发一种序列基础的药物-靶点相互作用框架，整合结构先验到蛋白质表示中。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，该模型在Human和BioSNAP数据集上实现了最先进的性能，在BindingDB上保持竞争力；在虚拟筛选任务中，它在LIT-PCBA上超越了先前的方法，在AUROC和BEDROC方面获得了显著提升；消融研究证实了学习聚合、双线注意力和对比对齐在增强预测鲁棒性方面的关键作用；嵌入可视化显示出与已知结合口袋的改进空间对应，并突出了配体-残基接触的可解释注意模式。&lt;h4&gt;结论&lt;/h4&gt;这些结果验证了该框架在可扩展和结构感知的DTI预测中的实用性。&lt;h4&gt;翻译&lt;/h4&gt;准确的药物-靶点相互作用识别仍然是计算药理学中的中心挑战，其中基于序列的方法提供了可扩展性。这项工作引入了一种基于序列的药物-靶点相互作用框架，将结构先验整合到蛋白质表示中，同时保持高通量筛选能力。在多个基准测试中评估，该模型在Human和BioSNAP数据集上实现了最先进的性能，并在BindingDB上保持竞争力。在虚拟筛选任务中，它在LIT-PCBA上超越了先前的方法，在AUROC和BEDROC方面获得了显著提升。消融研究证实了学习聚合、双线注意力和对比对齐在增强预测鲁棒性方面的关键作用。嵌入可视化显示出与已知结合口袋的改进空间对应，并突出了配体-残基接触的可解释注意模式。这些结果验证了该框架在可扩展和结构感知的DTI预测中的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate identification of drug-target interactions (DTI) remains a centralchallenge in computational pharmacology, where sequence-based methods offerscalability. This work introduces a sequence-based drug-target interactionframework that integrates structural priors into protein representations whilemaintaining high-throughput screening capability. Evaluated across multiplebenchmarks, the model achieves state-of-the-art performance on Human andBioSNAP datasets and remains competitive on BindingDB. In virtual screeningtasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains inAUROC and BEDROC. Ablation studies confirm the critical role of learnedaggregation, bilinear attention, and contrastive alignment in enhancingpredictive robustness. Embedding visualizations reveal improved spatialcorrespondence with known binding pockets and highlight interpretable attentionpatterns over ligand-residue contacts. These results validate the framework'sutility for scalable and structure-aware DTI prediction.</description>
      <author>example@mail.com (Jing Lan, Hexiao Ding, Hongzhao Chen, Yufeng Jiang, Nga-Chun Ng, Gwing Kei Yip, Gerald W. Y. Cheng, Yunlin Mao, Jing Cai, Liang-ting Lin, Jung Sun Yoo)</author>
      <guid isPermaLink="false">2509.14788v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-CLAP: Learning Spatially-Aware audio--text Embeddings for Multi-Source Conditions</title>
      <link>http://arxiv.org/abs/2509.14785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Spatial-CLAP，一种能够捕获空间信息的音频-文本嵌入框架，通过内容感知的空间编码器和空间对比学习策略解决了多源条件下的空间信息建模挑战。&lt;h4&gt;背景&lt;/h4&gt;CLAP作为音频-文本嵌入框架已取得显著成功，但现有方法仅限于单声道或单源条件，无法完全捕获空间信息。建模空间信息的主要挑战在于多源条件，需要正确对应每个声源及其位置。&lt;h4&gt;目的&lt;/h4&gt;解决多源条件下的空间信息建模问题，开发能够捕获空间信息的音频-文本嵌入框架。&lt;h4&gt;方法&lt;/h4&gt;提出Spatial-CLAP，引入内容感知的空间编码器使空间表示与音频内容耦合；提出空间对比学习(SCL)训练策略，强制学习正确的对应关系，促进多源条件下更可靠的嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明Spatial-CLAP在多源条件下能有效学习嵌入；SCL策略被证实有效；在未见三源混合物上的评估突显了传统单源训练与多源训练范式的根本区别。&lt;h4&gt;结论&lt;/h4&gt;这些发现为空间感知的音频-文本嵌入建立了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-音频预训练(CLAP)作为音频-文本嵌入框架已取得显著成功，但现有方法仅限于单声道或单源条件，无法完全捕获空间信息。建模空间信息的主要挑战在于多源条件，需要正确对应每个声源及其位置。为解决这个问题，我们提出了Spatial-CLAP，引入了内容感知的空间编码器，使空间表示与音频内容耦合。我们进一步提出了空间对比学习(SCL)，一种训练策略，明确强制学习正确的对应关系，促进多源条件下更可靠的嵌入。包括下游任务在内的实验评估表明，Spatial-CLAP即使在多源条件下也能学习有效的嵌入，并确认了SCL的有效性。此外，在未见三源混合物上的评估突显了传统单源训练与所提出的多源训练范式之间的根本区别。这些发现为空间感知的音频-文本嵌入建立了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive language--audio pretraining (CLAP) has achieved remarkablesuccess as an audio--text embedding framework, but existing approaches arelimited to monaural or single-source conditions and cannot fully capturespatial information. The central challenge in modeling spatial information liesin multi-source conditions, where the correct correspondence between each soundsource and its location is required. To tackle this problem, we proposeSpatial-CLAP, which introduces a content-aware spatial encoder that enablesspatial representations coupled with audio content. We further propose spatialcontrastive learning (SCL), a training strategy that explicitly enforces thelearning of the correct correspondence and promotes more reliable embeddingsunder multi-source conditions. Experimental evaluations, including downstreamtasks, demonstrate that Spatial-CLAP learns effective embeddings even undermulti-source conditions, and confirm the effectiveness of SCL. Moreover,evaluation on unseen three-source mixtures highlights the fundamentaldistinction between conventional single-source training and our proposedmulti-source training paradigm. These findings establish a new paradigm forspatially-aware audio--text embeddings.</description>
      <author>example@mail.com (Kentaro Seki, Yuki Okamoto, Kouei Yamaoka, Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari)</author>
      <guid isPermaLink="false">2509.14785v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration</title>
      <link>http://arxiv.org/abs/2509.14084v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出AD-DINOv3，一种新型视觉-语言多模态框架，首次将DINOv3模型适应于零样本异常检测(ZSAD)任务，通过多模态对比学习和异常感知校准模块解决了领域偏差和全局语义偏差问题，在八个工业和医学基准测试上取得优异性能。&lt;h4&gt;背景&lt;/h4&gt;零样本异常检测(ZSAD)旨在从任意新类别中识别异常，提供可扩展且注释高效的解决方案。传统ZSAD工作多基于CLIP模型，通过计算视觉和文本嵌入相似性进行异常检测。DINOv3等视觉基础模型最近展示了强大的可迁移表示能力。&lt;h4&gt;目的&lt;/h4&gt;首次将DINOv3适应于ZSAD任务，解决两个关键挑战：(1)大规模预训练数据与异常检测任务间领域偏差导致的特征不对齐；(2)预表示中对全局语义的固有倾向导致细微异常被误认为正常前景物体。&lt;h4&gt;方法&lt;/h4&gt;提出AD-DINOv3框架，将异常检测表述为多模态对比学习问题，使用DINOv3作为视觉骨干网络提取补丁令牌和CLS令牌，CLIP文本编码器提供正常和异常提示嵌入。引入轻量级适配器弥合领域差距，设计异常感知校准模块(AACM)引导CLS令牌关注异常区域而非通用前景语义。&lt;h4&gt;主要发现&lt;/h4&gt;在八个工业和医学基准测试上的广泛实验表明，AD-DINOv3一致地匹配或超越了最先进方法。代码将在https://github.com/Kaisor-Yuan/AD-DINOv3上公开。&lt;h4&gt;结论&lt;/h4&gt;AD-DINOv3结合了DINOv3的强大表示能力和CLIP的文本理解能力，解决了ZSAD任务中的关键挑战，通过多模态对比学习和异常感知校准模块的设计，提供了一种高效且可扩展的异常检测解决方案。&lt;h4&gt;翻译&lt;/h4&gt;零样本异常检测(ZSAD)旨在从任意新类别中识别异常，提供了一种可扩展且注释高效的解决方案。传统上，大多数ZSAD工作基于CLIP模型，通过计算视觉和文本嵌入之间的相似性来执行异常检测。最近，DINOv3等视觉基础模型展示了强大的可迁移表示能力。在这项工作中，我们首次将DINOv3适应于ZSAD。然而，这种适应提出了两个关键挑战：(i)大规模预训练数据与异常检测任务之间的领域偏差导致特征不对齐；(ii)预表示中对全局语义的固有倾向常常导致细微异常被误认为是正常前景物体的一部分，而非被识别为异常区域。为了克服这些挑战，我们引入了AD-DINOv3，一种专为ZSAD设计的新型视觉-语言多模态框架。具体而言，我们将异常检测表述为多模态对比学习问题，其中DINOv3被用作视觉骨干网络来提取补丁令牌和CLS令牌，CLIP文本编码器为正常和异常提示提供嵌入。为了弥合领域差距，在两种模态中都引入了轻量级适配器，使其表示能够针对异常检测任务进行重新校准。除了这种基线对齐外，我们还设计了一个异常感知校准模块(AACM)，它明确引导CLS令牌关注异常区域而非通用前景语义，从而增强判别能力。在八个工业和医学基准测试上的广泛实验表明，AD-DINOv3一致地匹配或超越了最先进的方法。代码将在https://github.com/Kaisor-Yuan/AD-DINOv3上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrarynovel categories, offering a scalable and annotation-efficient solution.Traditionally, most ZSAD works have been based on the CLIP model, whichperforms anomaly detection by calculating the similarity between visual andtext embeddings. Recently, vision foundation models such as DINOv3 havedemonstrated strong transferable representation capabilities. In this work, weare the first to adapt DINOv3 for ZSAD. However, this adaptation presents twokey challenges: (i) the domain bias between large-scale pretraining data andanomaly detection tasks leads to feature misalignment; and (ii) the inherentbias toward global semantics in pretrained representations often leads tosubtle anomalies being misinterpreted as part of the normal foreground objects,rather than being distinguished as abnormal regions. To overcome thesechallenges, we introduce AD-DINOv3, a novel vision-language multimodalframework designed for ZSAD. Specifically, we formulate anomaly detection as amultimodal contrastive learning problem, where DINOv3 is employed as the visualbackbone to extract patch tokens and a CLS token, and the CLIP text encoderprovides embeddings for both normal and abnormal prompts. To bridge the domaingap, lightweight adapters are introduced in both modalities, enabling theirrepresentations to be recalibrated for the anomaly detection task. Beyond thisbaseline alignment, we further design an Anomaly-Aware Calibration Module(AACM), which explicitly guides the CLS token to attend to anomalous regionsrather than generic foreground semantics, thereby enhancing discriminability.Extensive experiments on eight industrial and medical benchmarks demonstratethat AD-DINOv3 consistently matches or surpasses state-of-the-art methods.Thecode will be available at https://github.com/Kaisor-Yuan/AD-DINOv3.</description>
      <author>example@mail.com (Jingyi Yuan, Jianxiong Ye, Wenkang Chen, Chenqiang Gao)</author>
      <guid isPermaLink="false">2509.14084v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection</title>
      <link>http://arxiv.org/abs/2509.13853v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为单阶段监督对比学习(OS-SCL)的新训练技术，用于解决无监督异常声音检测中的频繁误报问题，并提出了TFgram时间频率特征，显著提升了检测性能。&lt;h4&gt;背景&lt;/h4&gt;尽管自监督方法有所进展，但在处理来自不同机器的同类样本时，无监督异常声音检测仍然存在频繁误报的问题。&lt;h4&gt;目的&lt;/h4&gt;解决无监督异常声音检测中处理来自不同机器的同类样本时的频繁误报问题。&lt;h4&gt;方法&lt;/h4&gt;提出单阶段监督对比学习(OS-SCL)训练技术，通过扰动嵌入空间特征并采用单阶段噪声监督对比学习方法；同时提出TFgram时间频率特征，从原始音频中提取关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64% AUC、88.42% pAUC和89.24% mAUC；使用提出的TFgram特征进一步提升了性能，达到95.71% AUC、90.23% pAUC和91.23% mAUC。&lt;h4&gt;结论&lt;/h4&gt;OS-SCL方法和TFgram特征有效解决了无监督异常声音检测中的频繁误报问题，显著提升了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常声音检测旨在仅使用正常音频数据训练模型来检测未知异常声音。尽管自监督方法有所进展，但在处理来自不同机器的同类样本时频繁误报的问题仍未解决。本文引入了一种名为单阶段监督对比学习(OS-SCL)的新训练技术，通过扰动嵌入空间特征并采用单阶段噪声监督对比学习方法，显著解决了这一问题。在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64% AUC、88.42% pAUC和89.24% mAUC。此外，还提出了一种从原始音频中提取的时间频率特征TFgram，该特征有效捕获了异常声音检测的关键信息，最终实现了95.71% AUC、90.23% pAUC和91.23% mAUC。源代码可在www.github.com/huangswt/OS-SCL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomalous sound detection aims to detect unknown anomaloussounds by training a model using only normal audio data. Despite advancementsin self-supervised methods, the issue of frequent false alarms when handlingsamples of the same type from different machines remains unresolved. This paperintroduces a novel training technique called one-stage supervised contrastivelearning (OS-SCL), which significantly addresses this problem by perturbingfeatures in the embedding space and employing a one-stage noisy supervisedcontrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.Additionally, a time-frequency feature named TFgram is proposed, which isextracted from raw audio. This feature effectively captures criticalinformation for anomalous sound detection, ultimately achieving 95.71\% AUC,90.23\% pAUC, and 91.23\% mAUC. The source code is available at:\underline{www.github.com/huangswt/OS-SCL}.</description>
      <author>example@mail.com (Shun Huang, Zhihua Fang, Liang He)</author>
      <guid isPermaLink="false">2509.13853v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>AEGIS: Automated Error Generation and Identification for Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2509.14295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AEGIS框架，用于多智能体系统的自动化错误生成和识别，解决了该领域缺乏大规模多样化错误数据集的问题。通过系统注入可控错误和探索三种学习范式，证明了该框架能显著提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;随着多智能体系统变得越来越自主和复杂，理解它们的错误模式对确保可靠性和安全性至关重要。然而，该领域研究严重缺乏大规模、多样化的数据集，这些数据集需要有精确的真实错误标签。&lt;h4&gt;目的&lt;/h4&gt;解决多智能体系统错误研究中缺乏大规模、多样化错误数据集的瓶颈问题，引入AEGIS框架用于自动化错误生成和识别。&lt;h4&gt;方法&lt;/h4&gt;通过系统性地向初始成功的轨迹中注入可控和可追踪的错误来创建丰富的现实失败数据集；使用基于上下文感知的大语言模型的自适应操作器执行复杂攻击（如提示注入和响应损坏）来诱导特定错误模式；探索三种学习范式（监督微调、强化学习和对比学习）进行错误识别。&lt;h4&gt;主要发现&lt;/h4&gt;在所有三种学习范式中，使用AEGIS数据训练的模型都取得了显著改进；多个微调模型表现出与或优于数量大一个数量级的专有系统的性能；验证了自动化数据生成框架对于开发更强大和可解释的多智能体系统是关键资源。&lt;h4&gt;结论&lt;/h4&gt;AEGIS框架为多智能体系统错误研究提供了宝贵的资源，通过自动化生成的错误数据，可以显著提高模型性能，使其能够与专有系统竞争。&lt;h4&gt;翻译&lt;/h4&gt;随着多智能体系统变得越来越自主和复杂，理解它们的错误模式对于确保其可靠性和安全性至关重要。然而，该领域的研究一直严重缺乏具有精确、真实错误标签的大规模、多样化数据集。为了解决这一瓶颈，我们引入了AEGIS，这是一个用于多智能体系统自动化错误生成和识别的新颖框架。通过系统性地向初始成功的轨迹中注入可控和可追踪的错误，我们创建了一个丰富的现实失败数据集。这是通过使用一个上下文感知的、基于大语言模型的自适应操作器实现的，该操作器执行复杂攻击，如提示注入和响应损坏，以诱导特定的预定义错误模式。我们通过探索三种不同的学习范式来证明我们数据集的价值：监督微调、强化学习和对比学习。我们全面的实验表明，在所有三种学习范式中，使用AEGIS数据训练的模型都取得了显著改进。值得注意的是，我们多个微调模型的性能与或优于数量级大一个数量级的专有系统，验证了我们的自动化数据生成框架是开发更强大和可解释的多智能体系统的关键资源。我们的项目网站可在https://kfq20.github.io/AEGIS-Website获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Multi-Agent Systems (MAS) become increasingly autonomous and complex,understanding their error modes is critical for ensuring their reliability andsafety. However, research in this area has been severely hampered by the lackof large-scale, diverse datasets with precise, ground-truth error labels. Toaddress this bottleneck, we introduce \textbf{AEGIS}, a novel framework for\textbf{A}utomated \textbf{E}rror \textbf{G}eneration and\textbf{I}dentification for Multi-Agent \textbf{S}ystems. By systematicallyinjecting controllable and traceable errors into initially successfultrajectories, we create a rich dataset of realistic failures. This is achievedusing a context-aware, LLM-based adaptive manipulator that performssophisticated attacks like prompt injection and response corruption to inducespecific, predefined error modes. We demonstrate the value of our dataset byexploring three distinct learning paradigms for the error identification task:Supervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Ourcomprehensive experiments show that models trained on AEGIS data achievesubstantial improvements across all three learning paradigms. Notably, severalof our fine-tuned models demonstrate performance competitive with or superiorto proprietary systems an order of magnitude larger, validating our automateddata generation framework as a crucial resource for developing more robust andinterpretable multi-agent systems. Our project website is available athttps://kfq20.github.io/AEGIS-Website.</description>
      <author>example@mail.com (Fanqi Kong, Ruijie Zhang, Huaxiao Yin, Guibin Zhang, Xiaofei Zhang, Ziang Chen, Zhaowei Zhang, Xiaoyuan Zhang, Song-Chun Zhu, Xue Feng)</author>
      <guid isPermaLink="false">2509.14295v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models</title>
      <link>http://arxiv.org/abs/2509.14269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SparseDoctor的新型稀疏医学大型语言模型，采用对比学习增强的LoRA-MoE架构，有效解决了传统微调策略的高成本问题，在医学基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在医疗问答和临床决策中取得成功，促进了个性化虚拟医生的普及，但传统微调策略需要更新数十亿参数，显著增加了训练成本。&lt;h4&gt;目的&lt;/h4&gt;提高当前医学大型语言模型的效率和有效性，探索大型语言模型在医学领域的表示能力边界。&lt;h4&gt;方法&lt;/h4&gt;提出SparseDoctor模型，采用对比学习增强的LoRA-MoE架构，包括自动路由机制科学分配计算资源，以及专家记忆队列机制提高框架效率并防止内存溢出。&lt;h4&gt;主要发现&lt;/h4&gt;在CMB、CMExam和CMMLU-Med三个医学基准测试上，所提出的模型持续优于HuatuoGPT系列等强大基线模型。&lt;h4&gt;结论&lt;/h4&gt;SparseDoctor模型证明了其在医学领域的有效性和优越性，为高效医学大型语言模型的发展提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在医疗问答和临床决策中取得了巨大成功，促进了个性化虚拟医生在社会中的普及和效率提升。然而，在大型语言模型上的传统微调策略需要更新数十亿参数，显著增加了训练成本，包括训练时间和实用成本。为了提高当前医学大型语言模型的效率和有效性，并探索大型语言模型在医学领域的表示能力边界，除了从数据角度的传统微调策略（即监督微调或人类反馈强化学习）外，我们设计了一个名为SparseDoctor的新型稀疏医学大型语言模型，配备对比学习增强的LoRA-MoE（低秩适配-专家混合）架构。为此，设计的自动路由机制可以在对比学习的监督下科学分配不同LoRA专家之间的计算资源。此外，我们还引入了一种新的专家记忆队列机制，进一步提高整体框架的效率并防止训练过程中的内存溢出。我们在三个典型的医学基准测试（CMB、CMExam和CMMLU-Med）上进行了综合评估。实验结果表明，所提出的大型语言模型能够持续优于HuatuoGPT系列等强大基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have achieved great success in medical questionanswering and clinical decision-making, promoting the efficiency andpopularization of the personalized virtual doctor in society. However, thetraditional fine-tuning strategies on LLM require the updates of billions ofparameters, substantially increasing the training cost, including the trainingtime and utility cost. To enhance the efficiency and effectiveness of thecurrent medical LLMs and explore the boundary of the representation capabilityof the LLMs on the medical domain, apart from the traditional fine-tuningstrategies from the data perspective (i.e., supervised fine-tuning orreinforcement learning from human feedback), we instead craft a novel sparsemedical LLM named SparseDoctor armed with contrastive learning enhancedLoRA-MoE (low rank adaptation-mixture of experts) architecture. To this end,the crafted automatic routing mechanism can scientifically allocate thecomputational resources among different LoRA experts supervised by thecontrastive learning. Additionally, we also introduce a novel expert memoryqueue mechanism to further boost the efficiency of the overall framework andprevent the memory overflow during training. We conduct comprehensiveevaluations on three typical medical benchmarks: CMB, CMExam, and CMMLU-Med.Experimental results demonstrate that the proposed LLM can consistentlyoutperform the strong baselines such as the HuatuoGPT series.</description>
      <author>example@mail.com (Zhang Jianbin, Yulin Zhu, Wai Lun Lo, Richard Tai-Chiu Hsung, Harris Sik-Ho Tsang, Kai Zhou)</author>
      <guid isPermaLink="false">2509.14269v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation</title>
      <link>http://arxiv.org/abs/2509.15224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Code: https://github.com/bartn8/depthanyevent/ Project  Page: https://bartn8.github.io/depthanyevent/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种跨模态蒸馏范式，利用视觉基础模型生成密集代理标签，解决了事件相机深度估计中缺乏大规模标注数据集的问题，并提出了适应VFMs的方法，在合成和真实世界数据集上取得了与监督方法相当的性能和最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;事件相机捕捉稀疏、高时间分辨率的视觉信息，特别适合高速运动和光照条件变化大的挑战性环境。然而，缺乏具有密集真实深度标注的大数据集，阻碍了基于学习的单目深度估计从事件数据中学习。&lt;h4&gt;目的&lt;/h4&gt;解决事件相机深度估计中缺乏大规模密集真实深度标注数据集的问题，提出一种无需昂贵深度标注的深度估计方法。&lt;h4&gt;方法&lt;/h4&gt;提出跨模态蒸馏范式，利用视觉基础模型(VFM)生成密集代理标签；需要与RGB帧空间对齐的事件流；利用大规模VFMs的鲁棒性；提出适应VFMs的方法，包括使用原始模型如Depth Anything v2或衍生新的循环架构来从单目事件相机推断深度。&lt;h4&gt;主要发现&lt;/h4&gt;跨模态范式与完全监督方法相比具有竞争性性能，不需要昂贵的深度标注；基于VFM的模型达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的跨模态蒸馏范式和VFM适应方法有效解决了事件相机深度估计中的数据标注问题，在合成和真实世界数据集上取得了优异的性能。&lt;h4&gt;翻译&lt;/h4&gt;事件相机捕捉稀疏、高时间分辨率的视觉信息，使其特别适合高速运动和光照条件变化剧烈的挑战性环境。然而，缺乏具有密集真实深度标注的大数据集阻碍了基于学习的从事件数据中进行单目深度估计。为解决这一限制，我们提出了一种跨模态蒸馏范式，利用视觉基础模型(VFM)生成密集代理标签。我们的策略需要与RGB帧空间对齐的事件流，这种简单设置甚至可以现成获取，并利用大规模VFMs的鲁棒性。此外，我们提出适应VFMs，可以是原始的如Depth Anything v2 (DAv2)，或者从中衍生出一种新的循环架构来从单目事件相机推断深度。我们在合成和真实世界数据集上评估了我们的方法，证明i)我们的跨模态范式与不需要昂贵深度标注的完全监督方法相比具有竞争性性能，以及ii)我们的基于VFM的模型达到了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决事件相机(event cameras)的单目深度估计问题。问题的重要性在于：事件相机具有高时间分辨率和强光照鲁棒性，特别适合高速运动和光照变化场景，但缺乏大规模密集深度标注数据集，限制了基于学习的方法发展。深度感知对自主导航、机器人等应用至关重要，而单目设置相比多目系统在成本、校准复杂度上有优势，但事件相机信息量少且标注困难，使深度估计极具挑战性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到事件相机深度估计的主要瓶颈是缺乏标注数据，然后借鉴视觉基础模型(VFMs)在图像深度估计中的成功经验。他们思考如何将图像领域的知识转移到事件领域，设计了跨模态蒸馏策略。借鉴了现有工作包括：Depth Anything v2等VFMs的架构、Tencode事件表示方法、循环神经网络处理时序数据的思想、以及知识蒸馏技术。在此基础上，设计了两种适应策略：直接使用VFMs和基于VFMs构建新的循环架构DepthAnyEvent-R。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型的知识弥补事件相机深度估计中标注数据的不足，通过跨模态蒸馏将图像领域的深度估计知识转移到事件领域。整体流程为：1) 收集空间对齐的事件流和RGB帧；2) 使用预训练VFM处理RGB帧生成深度代理标签；3) 用Tencode方法将事件流转换为RGB格式表示；4) 训练事件网络(学生模型)模仿VFM(教师模型)的预测；5) 设计两种模型架构：直接适应的DepthAnyEvent和整合时序信息的DepthAnyEvent-R；6) 在合成和真实数据集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出跨模态蒸馏范式，利用VFMs生成密集代理标签；2) 提出适应VFMs到事件域的策略；3) 设计循环架构DepthAnyEvent-R捕捉事件流时序特性；4) 证明蒸馏方法可媲美完全监督方法而无需昂贵标注。相比之前工作的不同：传统方法依赖大量标注数据，而本文通过蒸馏减少标注需求；简单图像模型直接应用于事件数据效果有限，本文设计了更适合事件的表示和架构；现有事件方法未充分利用大规模预训练知识，本文结合了图像和事件模态优势。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了跨模态蒸馏范式和适应视觉基础模型到事件域的方法，解决了事件相机深度估计中标注数据不足的问题，实现了与完全监督方法相媲美的性能并达到了新的技术水平。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras capture sparse, high-temporal-resolution visual information,making them particularly suitable for challenging environments with high-speedmotion and strongly varying lighting conditions. However, the lack of largedatasets with dense ground-truth depth annotations hinders learning-basedmonocular depth estimation from event data. To address this limitation, wepropose a cross-modal distillation paradigm to generate dense proxy labelsleveraging a Vision Foundation Model (VFM). Our strategy requires an eventstream spatially aligned with RGB frames, a simple setup even availableoff-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),or deriving from it a novel recurrent architecture to infer depth frommonocular event cameras. We evaluate our approach with synthetic and real-worlddatasets, demonstrating that i) our cross-modal paradigm achieves competitiveperformance compared to fully supervised methods without requiring expensivedepth annotations, and ii) our VFM-based models achieve state-of-the-artperformance.</description>
      <author>example@mail.com (Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia)</author>
      <guid isPermaLink="false">2509.15224v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</title>
      <link>http://arxiv.org/abs/2509.15221v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ScaleCUA是一个大规模开源计算机使用代理系统，通过整合6个操作系统和3个任务领域的数据集，实现了跨平台无缝操作，性能显著超越现有基线，并创造了多项新纪录。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language Models(VLMs)已使计算机使用代理(CUAs)能够自主操作GUI，显示出巨大潜力，但进展受限于缺乏大规模、开源的计算机使用数据和基础模型。&lt;h4&gt;目的&lt;/h4&gt;引入ScaleCUA，朝着扩展开源CUAs迈出一步，解决当前领域内数据和模型不足的问题。&lt;h4&gt;方法&lt;/h4&gt;构建了一个大规模数据集，涵盖6个操作系统和3个任务领域；通过闭环流水线构建，该流水线将自动代理与人类专家结合；在扩展的数据上训练ScaleCUA，使其能够跨平台无缝运行。&lt;h4&gt;主要发现&lt;/h4&gt;与基线相比，ScaleCUA在WebArena-Lite-v2上提升+26.6，在ScreenSpot-Pro上提升+10.7；在MMBench-GUI L1-Hard上达到94.4%，在OSWorld-G上达到60.6%，在WebArena-Lite-v2上达到47.4%的最先进结果；数据驱动扩展对通用计算机使用代理有显著提升作用。&lt;h4&gt;结论&lt;/h4&gt;ScaleCUA证明了数据驱动扩展的有效性，将发布数据、模型和代码以推进未来研究，项目地址为https://github.com/OpenGVLab/ScaleCUA。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)已经使计算机使用代理(CUAs)能够自主操作图形用户界面，显示出巨大潜力，但进展受限于缺乏大规模、开源的计算机使用数据和基础模型。在这项工作中，我们介绍了ScaleCUA，这是朝着扩展开源CUAs迈出的一步。它提供了一个大规模数据集，涵盖6个操作系统和3个任务领域，通过将自动代理与人类专家结合的闭环流水线构建。在此扩展数据上训练的ScaleCUA能够跨平台无缝运行。具体而言，它比基线模型带来显著提升(+26.6在WebArena-Lite-v2上，+10.7在ScreenSpot-Pro上)，并设定了新的最先进结果(94.4%在MMBench-GUI L1-Hard上，60.6%在OSWorld-G上，47.4%在WebArena-Lite-v2上)。这些发现强调了数据驱动扩展对通用计算机使用代理的力量。我们将发布数据、模型和代码以推进未来研究：https://github.com/OpenGVLab/ScaleCUA。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have enabled computer use agents (CUAs) thatoperate GUIs autonomously, showing great potential, yet progress is limited bythe lack of large-scale, open-source computer use data and foundation models.In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. Itoffers a large-scale dataset spanning 6 operating systems and 3 task domains,built via a closed-loop pipeline uniting automated agents with human experts.Trained on this scaled-up data, ScaleCUA can operate seamlessly acrossplatforms. Specifically, it delivers strong gains over baselines (+26.6 onWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-artresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% onWebArena-Lite-v2). These findings underscore the power of data-driven scalingfor general-purpose computer use agents. We will release data, models, and codeto advance future research: https://github.com/OpenGVLab/ScaleCUA.</description>
      <author>example@mail.com (Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang)</author>
      <guid isPermaLink="false">2509.15221v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Self-Improving Embodied Foundation Models</title>
      <link>http://arxiv.org/abs/2509.15155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Appearing in the Conference on Neural Information Processing Systems  (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于机器人的两阶段后训练方法，结合监督微调和自我改进，使机器人能够在最少的人工监督下自主练习下游任务，并展现出比传统方法更高的样本效率和成功率。&lt;h4&gt;背景&lt;/h4&gt;基于网络规模数据训练的基础模型已经彻底改变了机器人技术，但它们在低级控制中的应用仍然主要局限于行为克隆。&lt;h4&gt;目的&lt;/h4&gt;受大型语言模型微调中强化学习阶段成功的启发，提出一种用于机器人的两阶段后训练方法，以提高基础模型在机器人控制中的性能。&lt;h4&gt;方法&lt;/h4&gt;第一阶段：监督微调(SFT)，使用行为克隆和步数预测目标对预训练的基础模型进行微调；第二阶段：自我改进，利用步数预测提取奖励函数和成功检测器，使机器人能够在最少的人工监督下自主练习下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;SFT和自我改进的结合比扩展模仿数据收集更高效，能带来成功率更高的策略；网络规模预训练和自我改进的结合是实现样本效率的关键；该方法能实现自主练习和获取泛化能力远超训练数据的新技能。&lt;h4&gt;结论&lt;/h4&gt;结合预训练基础模型和在线自我改进具有变革性潜力，可以促进机器人中的自主技能获取。&lt;h4&gt;翻译&lt;/h4&gt;基于网络规模数据训练的基础模型已经彻底改变了机器人技术，但它们在低级控制中的应用仍然主要局限于行为克隆。受大型语言模型微调中强化学习阶段成功的启发，我们提出了一种用于机器人的两阶段后训练方法。第一阶段，监督微调(SFT)，使用行为克隆和步数预测目标对预训练的基础模型进行微调。在第二阶段，自我改进，步数预测能够提取出良好的奖励函数和强大的成功检测器，使机器人能够在最少的人工监督下自主练习下游任务。通过对现实世界和模拟机器人 embodiment 的广泛实验，我们的新型后训练方案在具身基础模型上揭示了显著的结果。首先，我们证明 SFT 和自我改进的结合比扩展监督学习的模仿数据收集更高效，并且能带来成功率更高的策略。进一步的消融研究强调，网络规模预训练和自我改进的结合是实现这种样本效率的关键。接下来，我们证明提出的组合方法独特地解锁了当前方法无法实现的能力：自主练习和获取泛化能力远超训练时使用的模仿学习数据集中观察到的行为的新技能。这些发现突显了结合预训练基础模型和在线自我改进以实现机器人中自主技能获取的变革潜力。我们的项目网站可以在 https://self-improving-efms.github.io 找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models trained on web-scale data have revolutionized robotics, buttheir application to low-level control remains largely limited to behavioralcloning. Drawing inspiration from the success of the reinforcement learningstage in fine-tuning large language models, we propose a two-stagepost-training approach for robotics. The first stage, Supervised Fine-Tuning(SFT), fine-tunes pretrained foundation models using both: a) behavioralcloning, and b) steps-to-go prediction objectives. In the second stage,Self-Improvement, steps-to-go prediction enables the extraction of awell-shaped reward function and a robust success detector, enabling a fleet ofrobots to autonomously practice downstream tasks with minimal humansupervision. Through extensive experiments on real-world and simulated robotembodiments, our novel post-training recipe unveils significant results onEmbodied Foundation Models. First, we demonstrate that the combination of SFTand Self-Improvement is significantly more sample-efficient than scalingimitation data collection for supervised learning, and that it leads topolicies with significantly higher success rates. Further ablations highlightthat the combination of web-scale pretraining and Self-Improvement is the keyto this sample-efficiency. Next, we demonstrate that our proposed combinationuniquely unlocks a capability that current methods cannot achieve: autonomouslypracticing and acquiring novel skills that generalize far beyond the behaviorsobserved in the imitation learning datasets used during training. Thesefindings highlight the transformative potential of combining pretrainedfoundation models with online Self-Improvement to enable autonomous skillacquisition in robotics. Our project website can be found athttps://self-improving-efms.github.io .</description>
      <author>example@mail.com (Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, Igor Mordatch)</author>
      <guid isPermaLink="false">2509.15155v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Asymptotic Study of In-context Learning with Random Transformers through Equivalent Models</title>
      <link>http://arxiv.org/abs/2509.15152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MLSP 2025, 6 pages 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究预训练Transformer在非线性回归中的上下文学习能力，发现随机Transformer在特定条件下等价于有限阶Hermite多项式模型。&lt;h4&gt;背景&lt;/h4&gt;探索Transformer模型在非线性回归任务中的上下文学习能力，特别是在渐近情况下各参数共同增长时的表现。&lt;h4&gt;目的&lt;/h4&gt;理解MLP层如何增强上下文学习能力，以及非线性和过参数化如何影响模型性能。&lt;h4&gt;方法&lt;/h4&gt;研究具有非线性MLP头部的随机Transformer，固定第一层并训练第二层，在上下文长度、输入维度、隐藏维度等参数共同增长的渐近情况下进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;随机Transformer在ICL误差方面等价于有限阶Hermite多项式模型，这一发现通过不同激活函数、上下文长度、隐藏层宽度和正则化设置的模拟得到验证，并揭示了双下降现象。&lt;h4&gt;结论&lt;/h4&gt;MLP层能够增强上下文学习能力，而非线性和过参数化对模型性能有重要影响。&lt;h4&gt;翻译&lt;/h4&gt;我们研究预训练Transformer在非线性回归设置中的上下文学习能力。具体而言，我们关注具有非线性MLP头部的随机Transformer，其中第一层随机初始化并固定，而第二层进行训练。此外，我们考虑一个渐近情况，其中上下文长度、输入维度、隐藏维度、训练任务数量和训练样本数量共同增长。在这种情况下，我们证明随机Transformer在ICL误差方面等价于有限阶Hermite多项式模型。这种等价性通过在不同激活函数、上下文长度、隐藏层宽度(揭示了双下降现象)和正则化设置下的模拟得到验证。我们的结果提供了理论和实证见解，说明MLP层何时以及如何增强ICL，以及非线性和过参数化如何影响模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the in-context learning (ICL) capabilities of pretrainedTransformers in the setting of nonlinear regression. Specifically, we focus ona random Transformer with a nonlinear MLP head where the first layer israndomly initialized and fixed while the second layer is trained. Furthermore,we consider an asymptotic regime where the context length, input dimension,hidden dimension, number of training tasks, and number of training samplesjointly grow. In this setting, we show that the random Transformer behavesequivalent to a finite-degree Hermite polynomial model in terms of ICL error.This equivalence is validated through simulations across varying activationfunctions, context lengths, hidden layer widths (revealing a double-descentphenomenon), and regularization settings. Our results offer theoretical andempirical insights into when and how MLP layers enhance ICL, and hownonlinearity and over-parameterization influence model performance.</description>
      <author>example@mail.com (Samet Demir, Zafer Dogan)</author>
      <guid isPermaLink="false">2509.15152v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Exploring How Audio Effects Alter Emotion with Foundation Models</title>
      <link>http://arxiv.org/abs/2509.15151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了音频效果如何影响音乐聆听时的情感反应，并利用基础模型分析这些效果与情感之间的关系。&lt;h4&gt;背景&lt;/h4&gt;先前研究已考察低级音频特征与情感感知的联系，但音频效果对情感的系统性影响尚未得到充分探索。音频效果在塑造音乐聆听时的情感反应中起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;研究如何利用基础模型分析音频效果对情感的影响，揭示音频效果与估计情感之间复杂、非线性的关系，并评估基础音频模型的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过应用各种探测方法分析深度学习模型的嵌入表示，研究不同音频效果与情感估计之间的关系。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了特定音频效果与情感之间的模式关系，并评估了基础音频模型的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这项研究旨在增进对音频制作实践感知影响的理解，对音乐认知、表演和情感计算领域具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;混响、失真、调制和动态范围处理等音频效果在塑造音乐聆听时的情感反应中起着关键作用。虽然先前的研究已经考察了低级音频特征与情感感知之间的联系，但音频效果对情感的系统性影响尚未得到充分探索。这项研究探讨了如何利用基础模型（在多模态数据上预训练的大型神经网络架构）来分析这些效果。这类模型编码了音乐结构、音色与情感意义之间的丰富关联，为探测声音设计技术的情感后果提供了强大的框架。通过应用各种探测方法来分析深度学习模型的嵌入表示，我们考察了音频效果与估计情感之间复杂、非线性的关系，揭示了与特定效果相关的模式，并评估了基础音频模型的鲁棒性。我们的研究旨在增进对音频制作实践感知影响的理解，对音乐认知、表演和情感计算领域具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio effects (FX) such as reverberation, distortion, modulation, and dynamicrange processing play a pivotal role in shaping emotional responses duringmusic listening. While prior studies have examined links between low-levelaudio features and affective perception, the systematic impact of audio FX onemotion remains underexplored. This work investigates how foundation models -large-scale neural architectures pretrained on multimodal data - can beleveraged to analyze these effects. Such models encode rich associationsbetween musical structure, timbre, and affective meaning, offering a powerfulframework for probing the emotional consequences of sound design techniques. Byapplying various probing methods to embeddings from deep learning models, weexamine the complex, nonlinear relationships between audio FX and estimatedemotion, uncovering patterns tied to specific effects and evaluating therobustness of foundation audio models. Our findings aim to advanceunderstanding of the perceptual impact of audio production practices, withimplications for music cognition, performance, and affective computing.</description>
      <author>example@mail.com (Stelios Katsis, Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Giorgos Stamou)</author>
      <guid isPermaLink="false">2509.15151v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings</title>
      <link>http://arxiv.org/abs/2509.15001v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BabyHuBERT是一个在13,000小时多语言儿童中心长篇录音上训练的自监督语音表征模型，在说话人分割任务上表现优于现有模型，特别是在代表性不足的语言上。&lt;h4&gt;背景&lt;/h4&gt;研究儿童语言发展需要以儿童为中心的长篇录音，但现有的语音模型是在干净的成人数据上训练的，由于声学和语言差异，在儿童语音上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对儿童语音的自监督语音表征模型。&lt;h4&gt;方法&lt;/h4&gt;引入BabyHuBERT，在13,000小时多语言儿童中心长篇录音上训练，涵盖40多种语言。&lt;h4&gt;主要发现&lt;/h4&gt;BabyHuBERT在六个不同的数据集上实现了52.1%到74.4%的F1分数，一致优于W2V2-LL4300和标准HuBERT。在瓦努阿图语上比HuBERT高出13.2个F1点，在所罗门群岛语上高出15.9个点。&lt;h4&gt;结论&lt;/h4&gt;BabyHuBERT作为儿童语音研究的基础模型，通过分享代码和模型，能够为各种下游任务提供微调。&lt;h4&gt;翻译&lt;/h4&gt;以儿童为中心的长篇录音对于研究早期语言发展至关重要，但现有的在干净的成人数据上训练的语音模型由于声学和语言差异表现不佳。我们介绍了BabyHuBERT，这是第一个在13,000小时多语言儿童中心长篇录音上训练的自监督语音表征模型，涵盖40多种语言。我们在说话人分割任务上评估BabyHuBERT，即识别目标儿童何时说话，与女性成人、男性成人或其他儿童说话相比——这是分析自然语言体验的基本预处理步骤。BabyHuBERT在六个不同的数据集上实现了52.1%到74.4%的F1分数，一致优于W2V2-LL4300（在英语长篇录音上训练）和标准HuBERT（在干净的成人语音上训练）。显著的改进包括在瓦努阿图语上比HuBERT高出13.2个F1点，在所罗门群岛语上高出15.9个点，证明了在代表性不足的语言上的有效性。通过分享代码和模型，BabyHuBERT作为儿童语音研究的基础模型，能够为各种下游任务提供微调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Child-centered long-form recordings are essential for studying early languagedevelopment, but existing speech models trained on clean adult data performpoorly due to acoustic and linguistic differences. We introduce BabyHuBERT, thefirst self-supervised speech representation model trained on 13,000 hours ofmultilingual child-centered long-form recordings spanning over 40 languages. Weevaluate BabyHuBERT on speaker segmentation, identifying when target childrenspeak versus female adults, male adults, or other children -- a fundamentalpreprocessing step for analyzing naturalistic language experiences. BabyHuBERTachieves F1-scores from 52.1% to 74.4% across six diverse datasets,consistently outperforming W2V2-LL4300 (trained on English long-forms) andstandard HuBERT (trained on clean adult speech). Notable improvements include13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on SolomonIslands corpora, demonstrating effectiveness on underrepresented languages. Bysharing code and models, BabyHuBERT serves as a foundation model for childspeech research, enabling fine-tuning on diverse downstream tasks.</description>
      <author>example@mail.com (Théo Charlot, Tarek Kunze, Maxime Poli, Alejandrina Cristia, Emmanuel Dupoux, Marvin Lavechin)</author>
      <guid isPermaLink="false">2509.15001v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>PRISM: Product Retrieval In Shopping Carts using Hybrid Matching</title>
      <link>http://arxiv.org/abs/2509.14985v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PRISM的新混合方法，用于零售环境中的产品检索，结合了视觉语言模型和像素级匹配的优点，实现了高准确率和实时处理能力。&lt;h4&gt;背景&lt;/h4&gt;与传统图像检索相比，零售环境中的产品检索更具挑战性，因为同类型不同品牌的产品外观高度相似，且查询图像角度可能与目录图像视角差异显著。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的混合方法，解决基础模型难以区分细微差异和像素级匹配计算量大的问题，实现高效且准确的产品检索。&lt;h4&gt;方法&lt;/h4&gt;PRISM框架包含三个阶段：1)使用SigLIP视觉语言模型检索语义最相似的35个产品缩小搜索空间；2)应用YOLO-E分割模型消除背景杂乱；3)使用LightGlue进行细粒度像素级匹配。&lt;h4&gt;主要发现&lt;/h4&gt;PRISM框架能够通过关注全局模型常忽略的细微视觉线索，更准确地区分高度相似类别的产品，在ABV数据集上实现了4.21%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;PRISM方法在保持实时处理能力的同时，显著提高了产品检索的准确率，适用于实际零售环境部署。&lt;h4&gt;翻译&lt;/h4&gt;与传统图像检索任务相比，零售环境中的产品检索更具挑战性。来自不同品牌的同类型产品可能具有高度相似的视觉外观，且查询图像的拍摄角度可能与存储的目录图像视角有显著差异。基础模型（如CLIP和SigLIP）通常难以区分这些细微但重要的局部差异。另一方面，像素级匹配方法计算量大，导致匹配时间过高。在本文中，我们提出了一种名为PRISM的新混合方法，用于零售环境中的产品检索，通过结合基于视觉语言模型和像素级匹配方法的优势。为了同时提高效率/速度和细粒度检索准确性，PRISM包含三个阶段：1)首先使用视觉语言模型（SigLIP）从固定库中检索语义上最相似的35个产品，显著缩小搜索空间；2)应用分割模型（YOLO-E）消除背景杂乱；3)使用LightGlue在过滤后的候选产品上进行细粒度像素级匹配。该框架通过关注全局模型经常忽略的细微视觉线索，能够更准确地区分高度相似类别的产品。在ABV数据集上进行的实验表明，我们提出的PRISM在顶级-1准确率上比最先进的图像检索方法高出4.21%，同时仍保持实时处理能力，适用于实际零售部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compared to traditional image retrieval tasks, product retrieval in retailsettings is even more challenging. Products of the same type from differentbrands may have highly similar visual appearances, and the query image may betaken from an angle that differs significantly from view angles of the storedcatalog images. Foundational models, such as CLIP and SigLIP, often struggle todistinguish these subtle but important local differences. Pixel-wise matchingmethods, on the other hand, are computationally expensive and incurprohibitively high matching times. In this paper, we propose a new, hybridmethod, called PRISM, for product retrieval in retail settings by leveragingthe advantages of both vision-language model-based and pixel-wise matchingapproaches. To provide both efficiency/speed and finegrained retrievalaccuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP)is employed first to retrieve the top 35 most semantically similar productsfrom a fixed gallery, thereby narrowing the search space significantly; 2) asegmentation model (YOLO-E) is applied to eliminate background clutter; 3)fine-grained pixel-level matching is performed using LightGlue across thefiltered candidates. This framework enables more accurate discriminationbetween products with high inter-class similarity by focusing on subtle visualcues often missed by global models. Experiments performed on the ABV datasetshow that our proposed PRISM outperforms the state-of-the-art image retrievalmethods by 4.21% in top-1 accuracy while still remaining within the bounds ofreal-time processing for practical retail deployments.</description>
      <author>example@mail.com (Arda Kabadayi, Senem Velipasalar, Jiajing Chen)</author>
      <guid isPermaLink="false">2509.14985v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments</title>
      <link>http://arxiv.org/abs/2509.14978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出感知感知MPPI(PA-MPPI)方法，解决了四旋翼无人机在未知环境中导航时面临的探索挑战，相比传统MPPI方法性能提升高达100%&lt;h4&gt;背景&lt;/h4&gt;四旋翼无人机在未知环境中的导航对于实际任务如搜救至关重要，需要处理自由空间非凸性、四旋翼特定动力学和探索未知区域等挑战&lt;h4&gt;目的&lt;/h4&gt;解决MPPI方法在未知环境中缺乏探索未知区域能力的问题，使其能够在大障碍物阻挡时找到替代路径&lt;h4&gt;方法&lt;/h4&gt;提出感知感知MPPI(PA-MPPI)，通过根据感知目标在线调整轨迹，当目标被遮挡时，利用感知成本偏向能够感知未知区域的轨迹&lt;h4&gt;主要发现&lt;/h4&gt;硬件实验表明，PA-MPPI以50Hz频率运行，在挑战性设置中比基线方法性能提高高达100%，而最先进的MPPI在这些设置中失败&lt;h4&gt;结论&lt;/h4&gt;PA-MPPI能有效扩展可通行映射空间，增加找到替代路径的可能性，还可作为导航基础模型的安全可靠动作策略&lt;h4&gt;翻译&lt;/h4&gt;未知环境中的四旋翼导航对于搜救等实际任务至关重要。解决这一问题需要应对三个关键挑战：由障碍物导致的自由空间非凸性、四旋翼特定的动力学和目标，以及探索未知区域以找到通往目标的路径的需求。最近，模型预测路径积分(MPPI)方法已成为解决前两个挑战的有前景的解决方案。通过利用基于采样的优化，它可以有效处理非凸自由空间，同时直接优化整个四旋翼动力学，使能够包含四旋翼特定成本如能源消耗。然而，它在未知环境中的性能有限，因为它在大障碍物阻挡时缺乏探索未知区域的能力。为解决这个问题，我们引入了感知感知MPPI(PA-MPPI)。在这里，感知感知被定义为根据感知目标在线调整轨迹。具体而言，当目标被遮挡时，PA-MPPI的感知成本偏向能够感知未知区域的轨迹。这扩展了可通行映射空间并增加了找到通往目标的替代路径的可能性。通过硬件实验，我们证明PA-MPPI以50Hz的频率运行，配合我们高效的感知和映射模块，在我们具有挑战性的设置中，其性能比基线方法提高高达100%，而最先进的MPPI在这些设置中失败。此外，我们证明PA-MPPI可以用作导航基础模型的安全可靠的动作策略，这些模型通常提供无法直接到达的目标位姿。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quadrotor navigation in unknown environments is critical for practicalmissions such as search-and-rescue. Solving it requires addressing three keychallenges: the non-convexity of free space due to obstacles,quadrotor-specific dynamics and objectives, and the need for exploration ofunknown regions to find a path to the goal. Recently, the Model Predictive PathIntegral (MPPI) method has emerged as a promising solution that solves thefirst two challenges. By leveraging sampling-based optimization, it caneffectively handle non-convex free space while directly optimizing over thefull quadrotor dynamics, enabling the inclusion of quadrotor-specific costssuch as energy consumption. However, its performance in unknown environments islimited, as it lacks the ability to explore unknown regions when blocked bylarge obstacles. To solve this issue, we introduce Perception-Aware MPPI(PA-MPPI). Here, perception-awareness is defined as adapting the trajectoryonline based on perception objectives. Specifically, when the goal is occluded,PA-MPPI's perception cost biases trajectories that can perceive unknownregions. This expands the mapped traversable space and increases the likelihoodof finding alternative paths to the goal. Through hardware experiments, wedemonstrate that PA-MPPI, running at 50 Hz with our efficient perception andmapping module, performs up to 100% better than the baseline in our challengingsettings where the state-of-the-art MPPI fails. In addition, we demonstratethat PA-MPPI can be used as a safe and robust action policy for navigationfoundation models, which often provide goal poses that are not directlyreachable.</description>
      <author>example@mail.com (Yifan Zhai, Rudolf Reiter, Davide Scaramuzza)</author>
      <guid isPermaLink="false">2509.14978v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</title>
      <link>http://arxiv.org/abs/2509.14966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出RoboEye，一种两阶段识别框架，通过结合二维语义特征与领域自适应的三维推理和轻量级适配器，解决了大型电商平台产品类别快速增长导致的仓库自动化包装物体识别困难问题。&lt;h4&gt;背景&lt;/h4&gt;大型电商平台产品类别快速增长，导致仓库自动化包装中的物体识别变得困难。随着目录增长，类内变异性和稀有或视觉相似物品的长尾增加，结合多样化的包装、杂乱的容器、频繁遮挡和大视角变化等因素，放大了查询图像和参考图像之间的差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服仅依赖二维外观特征的方法性能下降问题的物体识别框架，提高在复杂环境下的识别准确率。&lt;h4&gt;方法&lt;/h4&gt;提出RoboEye两阶段识别框架：第一阶段训练大型视觉模型提取二维特征生成候选排名，使用轻量级三维特征感知模块估计三维特征质量并预测是否需要三维重排序；第二阶段使用机器人三维检索transformer，包括产生几何感知密集特征的三维特征提取器和基于关键点的匹配器。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，RoboEye比最先进的方法（RoboLLM）将Recall@1提高了7.1%，且仅使用RGB图像运行，避免了对显式三维输入的依赖。&lt;h4&gt;结论&lt;/h4&gt;RoboEye通过动态增强二维特征与三维推理，有效解决了产品类别快速增长带来的识别挑战，同时降低了部署成本。&lt;h4&gt;翻译&lt;/h4&gt;大型电商平台产品类别的快速增长使仓库自动化包装中的物体识别变得异常困难。随着目录增长，类内变异性和稀有或视觉相似物品的长尾增加，当与多样化的包装、杂乱的容器、频繁遮挡和大视角变化相结合时，这些因素放大了查询图像和参考图像之间的差异，导致仅依赖二维外观特征的方法性能急剧下降。因此，我们提出RoboEye，一个两阶段识别框架，动态将二维语义特征与领域自适应的三维推理和轻量级适配器结合，以弥合训练与部署的差距。在第一阶段，我们训练大型视觉模型提取二维特征生成候选排名。然后，轻量级三维特征感知模块估计三维特征质量并预测是否需要三维重排序，防止性能下降并避免不必要的计算。当被调用时，第二阶段使用我们的机器人三维检索transformer，包括产生几何感知密集特征的三维特征提取器和基于关键点的匹配器，计算查询图像和参考图像之间的关键点对应置信度，而不是传统的余弦相似度评分。实验表明，RoboEye比最先进的方法（RoboLLM）将Recall@1提高了7.1%。此外，RoboEye仅使用RGB图像运行，避免了对显式三维输入的依赖，降低了部署成本。本文使用的代码可在以下公开获取：https://github.com/longkukuhi/RoboEye。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决大型电商仓库中自动化包装时的物体识别问题。随着产品类别快速增长，类内差异性和视觉相似物品增加，结合多样化包装、杂乱容器、频繁遮挡和大视角变化等因素，使得仅依赖2D外观特征的方法性能急剧下降。这个问题在现实中至关重要，因为正确的物体识别为仓库自动化的运动规划和控制提供必要信息，错误识别会导致订单履行错误和巨大财务损失（论文提到亚马逊2025年第一季度因此损失约10亿美元）。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到仅依赖2D特征的方法在仓库环境中的局限性，然后思考如何利用3D几何特征提供视角不变的特征，但又不愿增加专门的传感器成本。他们设计了两阶段框架，动态增强2D特征与选择性3D推理。该方法借鉴了多项现有工作：使用BEiT-3模型提取2D特征，借鉴VGGT的3D特征提取能力，参考RoboLLM的方法论训练2D特征提取器，并采用知识适配器概念实现领域适应。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是动态增强2D外观特征与选择性3D几何推理，仅在必要时调用3D重排序，避免不必要的计算和性能下降，同时避免对显式3D输入的依赖。整体流程分为两阶段：第一阶段使用大型视觉模型提取2D特征生成候选排序，并通过轻量级3D特征感知模块判断是否需要3D重排序；第二阶段当需要时，使用机器人3D检索transformer，通过3D特征提取器和基于关键点的匹配器计算查询和参考图像间的关键点对应置信度，进行重排序。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个动态增强2D检索与隐式3D几何重排序的框架；2) MRR驱动的3D感知训练方案，选择性激活3D重排序；3) 基于关键点的检索匹配器，提供更鲁棒的相似性度量；4) 基于适配器的训练策略实现高效领域适应；5) 仅使用RGB图像，降低部署成本。相比之前的工作，RoboEye不仅提高了识别准确率（比RoboLLM高7.1%的Recall@1），还避免了显式3D传感器的需求，并智能判断何时使用3D特征，避免无条件使用导致的性能下降。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RoboEye通过选择性3D几何推理与2D外观特征的动态融合，在无需额外3D传感器的情况下，显著提升了复杂仓库环境中的物体识别准确率和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapidly growing number of product categories in large-scale e-commercemakes accurate object identification for automated packing in warehousessubstantially more difficult. As the catalog grows, intra-class variability anda long tail of rare or visually similar items increase, and when combined withdiverse packaging, cluttered containers, frequent occlusion, and largeviewpoint changes-these factors amplify discrepancies between query andreference images, causing sharp performance drops for methods that rely solelyon 2D appearance features. Thus, we propose RoboEye, a two-stage identificationframework that dynamically augments 2D semantic features with domain-adapted 3Dreasoning and lightweight adapters to bridge training deployment gaps. In thefirst stage, we train a large vision model to extract 2D features forgenerating candidate rankings. A lightweight 3D-feature-awareness module thenestimates 3D feature quality and predicts whether 3D re-ranking is necessary,preventing performance degradation and avoiding unnecessary computation. Wheninvoked, the second stage uses our robot 3D retrieval transformer, comprising a3D feature extractor that produces geometry-aware dense features and akeypoint-based matcher that computes keypoint-correspondence confidencesbetween query and reference images instead of conventional cosine-similarityscoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the priorstate of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,avoiding reliance on explicit 3D inputs and reducing deployment costs. The codeused in this paper is publicly available at:https://github.com/longkukuhi/RoboEye.</description>
      <author>example@mail.com (Xingwu Zhang, Guanxuan Li, Zhuocheng Zhang, Zijun Long)</author>
      <guid isPermaLink="false">2509.14966v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification</title>
      <link>http://arxiv.org/abs/2509.14958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为跨模态几何校正(CMGR)的框架，用于解决3D类增量学习在极端数据稀缺情况下的几何不对齐和纹理偏差问题。通过利用CLIP的层次空间语义，该方法增强了3D几何保真度，并通过结构感知几何校正模块、纹理增强模块和基础-新颖判别器提高了模型的性能。&lt;h4&gt;背景&lt;/h4&gt;随着3D数字内容的快速增长，需要对开放世界场景进行可扩展的识别系统。然而，现有的3D类增量学习方法在极端数据稀缺情况下表现不佳，主要由于几何不对齐和纹理偏差问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D类增量学习方法在极端数据稀缺情况下的几何不对齐和纹理偏差问题，提高3D少样本类增量学习的性能。&lt;h4&gt;方法&lt;/h4&gt;提出跨模态几何校正(CMGR)框架，包括：1)结构感知几何校正模块，通过注意力驱动的几何融合将3D部分结构与CLIP的中间空间先验进行层次对齐；2)纹理增强模块，合成最小但有区分度的纹理以抑制噪声并增强跨模态一致性；3)基础-新颖判别器，隔离几何变化以稳定增量原型。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了3D少样本类增量学习，在跨域和域内设置中实现了更好的几何一致性和对纹理偏差的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过利用CLIP的层次空间语义和创新的模块设计，CMGR框架有效解决了3D类增量学习中的几何不对齐和纹理偏差问题，提高了模型的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;随着3D数字内容的快速增长，需要对开放世界场景进行可扩展的识别系统。然而，现有的3D类增量学习方法在极端数据稀缺情况下因几何不对齐和纹理偏差而表现不佳。虽然最近的方法将3D数据与2D基础模型(如CLIP)集成，但它们因纹理投影偏差和几何-纹理线索的无差别融合导致语义模糊，进而导致决策原型不稳定和灾难性遗忘。为解决这些问题，我们提出了跨模态几何校正(CMGR)框架，通过利用CLIP的层次空间语义来增强3D几何保真度。具体而言，我们引入了一个结构感知几何校正模块，通过注意力驱动的几何融合将3D部分结构与CLIP的中间空间先验进行层次对齐。此外，纹理增强模块合成最小但有区分度的纹理，以抑制噪声并增强跨模态一致性。为进一步稳定增量原型，我们采用基础-新颖判别器来隔离几何变化。大量实验证明，我们的方法显著提高了3D少样本类增量学习，在跨域和域内设置中实现了更好的几何一致性和对纹理偏差的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D少样本类增量学习(3DFSCIL)中的几何错位、纹理偏差、语义模糊和灾难性遗忘问题。随着3D数字内容快速增长，在自动驾驶、工业机器人等领域需要能够处理动态出现新物体类别的系统，而这些系统在极端数据稀缺情况下表现不佳。几何结构和空间关系对3D物体识别至关重要，而现有方法过度依赖纹理信息，忽视了几何结构的重要性，导致在开放世界场景中难以有效识别新类别。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3DFSCIL方法的局限性，指出它们过度依赖CLIP的最终层表示而忽略了中间层的丰富空间先验信息，以及无差别融合机制导致的语义模糊。作者的关键洞察是CLIP的中间层保留了与3D几何层次结构自然对齐的空间关系。基于此，作者设计了跨模态几何校正框架(CMGR)，包含三个关键组件：SAGR、TAM和BND。作者借鉴了CLIP等视觉语言模型的跨模态学习能力、点云处理和深度图渲染技术，以及增量学习中的注意力机制和判别器设计，但创新性地将它们组合用于解决3D几何表示问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将CLIP的中间层作为'2D透镜'，通过跨模态几何校正增强3D表示的几何保真度。整体流程包括：1)点云通过3D分支处理生成特征，同时渲染为深度图通过2D分支处理；2)SAGR模块通过交叉注意力机制层次化对齐CLIP中间层特征与3D结构特征；3)TAM模块从点云特征学习判别性纹理模式并合成自适应RGB值；4)BND模块作为二分类器区分基础和新类别，稳定增量学习；5)结合几何特征和CLIP的多模态理解进行最终预测。每个增量任务前独立训练BND，允许基础和新类别并行处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)CMGR框架首次利用CLIP中间层作为'2D透镜'增强3D几何保真度；2)SAGR模块通过层次化对齐3D部分结构与CLIP空间先验解决语义模糊；3)TAM模块合成判别性纹理增强跨模态一致性；4)BND模块隔离几何变化稳定增量原型。相比之前工作，本文利用CLIP中间层而非仅用最终层，通过几何校正保留结构完整性而非无差别融合几何纹理特征，BND允许并行处理基础和新类别减轻灾难性遗忘，并通过层次化对齐建立3D结构与2D表示的明确对应关系解决语义模糊。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了跨模态几何校正框架，通过利用CLIP的层次化空间语义增强3D表示的几何保真度，显著提高了3D少样本类增量学习在跨域和域内场景中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of 3D digital content necessitates expandable recognitionsystems for open-world scenarios. However, existing 3D class-incrementallearning methods struggle under extreme data scarcity due to geometricmisalignment and texture bias. While recent approaches integrate 3D data with2D foundation models (e.g., CLIP), they suffer from semantic blurring caused bytexture-biased projections and indiscriminate fusion of geometric-texturalcues, leading to unstable decision prototypes and catastrophic forgetting. Toaddress these issues, we propose Cross-Modal Geometric Rectification (CMGR), aframework that enhances 3D geometric fidelity by leveraging CLIP's hierarchicalspatial semantics. Specifically, we introduce a Structure-Aware GeometricRectification module that hierarchically aligns 3D part structures with CLIP'sintermediate spatial priors through attention-driven geometric fusion.Additionally, a Texture Amplification Module synthesizes minimal yetdiscriminative textures to suppress noise and reinforce cross-modalconsistency. To further stabilize incremental prototypes, we employ aBase-Novel Discriminator that isolates geometric variations. Extensiveexperiments demonstrate that our method significantly improves 3D few-shotclass-incremental learning, achieving superior geometric coherence androbustness to texture bias across cross-domain and within-domain settings.</description>
      <author>example@mail.com (Xiang Tuo, Xu Xuemiao, Liu Bangzhen, Li Jinyi, Li Yong, He Shengfeng)</author>
      <guid isPermaLink="false">2509.14958v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications</title>
      <link>http://arxiv.org/abs/2509.14921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the IEEE International Joint Conference on Biometrics  2025 (IJCB 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了基础模型CLIP在生物识别任务微调后面临的过度专业化问题，发现微调会导致跨领域泛化能力下降，但特定任务性能提高，且较大模型能更好地保持原始泛化能力。&lt;h4&gt;背景&lt;/h4&gt;基础模型如CLIP在各种视觉任务中表现出强大的零样本和少样本迁移能力，但当针对高度专业化的生物识别任务（人脸识别FR、morphing攻击检测MAD、呈现攻击检测PAD）进行微调时，这些模型可能会过度专业化，从而失去跨领域泛化能力。&lt;h4&gt;目的&lt;/h4&gt;系统量化专业化与泛化能力之间的权衡，通过评估三个针对FR、MAD和PAD微调的CLIP实例来研究这一问题。&lt;h4&gt;方法&lt;/h4&gt;评估每个微调模型以及原始CLIP基线模型，在14个通用视觉数据集上使用零样本和线性探测协议进行评估，同时在常见的FR、MAD和PAD基准测试上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型存在过度专业化问题，特别是针对复杂的人脸识别任务；任务复杂度和分类头设计与灾难性遗忘程度相关；使用ViT-L骨干网的FRoundation模型在IJB-C上实现58.52%的改进，但在ImageNetV2上性能显著下降；较大CLIP架构比小版本保留了更多原始泛化能力。&lt;h4&gt;结论&lt;/h4&gt;微调基础模型会导致过度专业化，特别是在复杂任务上；增加模型容量可能有助于减轻过度专业化问题。&lt;h4&gt;翻译&lt;/h4&gt;基础模型如CLIP已在各种视觉任务中展现出卓越的零样本和少样本迁移能力。然而，当针对高度专业化的生物识别任务（人脸识别FR、morphing攻击检测MAD和呈现攻击检测PAD）进行微调时，这些模型可能会过度专业化，从而失去其基础优势之一——跨领域泛化能力。本研究通过评估三个针对FR、MAD和PAD微调的CLIP实例，系统量化了这些权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models such as CLIP have demonstrated exceptional zero- andfew-shot transfer capabilities across diverse vision tasks. However, whenfine-tuned for highly specialized biometric tasks, face recognition (FR),morphing attack detection (MAD), and presentation attack detection (PAD), thesemodels may suffer from over-specialization. Thus, they may lose one of theirfoundational strengths, cross-domain generalization. In this work, wesystematically quantify these trade-offs by evaluating three instances of CLIPfine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as theoriginal CLIP baseline on 14 general vision datasets under zero-shot andlinear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Ourresults indicate that fine-tuned models suffer from over-specialization,especially when fine-tuned for complex tasks of FR. Also, our results pointedout that task complexity and classification head design, multi-class (FR) vs.binary (MAD and PAD), correlate with the degree of catastrophic forgetting. TheFRoundation model with the ViT-L backbone outperforms other approaches on thelarge-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%.However, it experiences a substantial performance drop on ImageNetV2, reachingonly 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover,the larger CLIP architecture consistently preserves more of the model'soriginal generalization ability than the smaller variant, indicating thatincreased model capacity may help mitigate over-specialization.</description>
      <author>example@mail.com (Tahar Chettaoui, Naser Damer, Fadi Boutros)</author>
      <guid isPermaLink="false">2509.14921v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction</title>
      <link>http://arxiv.org/abs/2509.14739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FMGS-Avatar的新方法，用于从单目视频中重建高保真可动画化人类头像，通过结合网格引导的2D高斯泼溅和基础模型先验知识，解决了单目观察中几何信息不足和现有方法表面细节保留困难的问题。&lt;h4&gt;背景&lt;/h4&gt;从单目视频中重建高保真可动画化人类头像具有挑战性，主要是因为单目观察中的几何信息不足。现有的3D高斯泼溅方法由于3D高斯原子的自由形式特性，在保持表面细节方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决单目视频中人类头像重建的表示限制和信息稀缺问题，提高重建质量，包括几何准确性和外观保真度，同时提供丰富的语义信息。&lt;h4&gt;方法&lt;/h4&gt;FMGS-Avatar方法包含两个关键创新：1) 网格引导的2D高斯泼溅，将2D高斯原子直接附加到模板网格面上，具有受约束的位置、旋转和运动；2) 利用基础模型（如Sapiens）补充单目视频有限的视觉线索。通过具有选择性梯度隔离的协调训练策略解决多模态先验知识提炼中的冲突优化目标问题。&lt;h4&gt;主要发现&lt;/h4&gt;结合增强表示和协调信息蒸馏的方法显著提高了3D单目人类头像重建质量。实验评估显示，与现有方法相比具有更好的重建质量，在几何准确性和外观保真度方面有显著提升，同时提供丰富的语义信息。在共享规范空间内提炼的先验知识支持在新视角和姿势下进行空间和时间一致的渲染。&lt;h4&gt;结论&lt;/h4&gt;FMGS-Avatar方法通过网格引导的2D高斯泼溅和基础模型先验知识的有效结合，成功解决了单目视频中人类头像重建的关键挑战，实现了高质量的重建结果，并支持在新视角和姿势下的一致渲染。&lt;h4&gt;翻译&lt;/h4&gt;从单目视频中重建高保真可动画化人类头像由于单目观察中几何信息不足而仍然具有挑战性。虽然最近的3D高斯泼溅方法显示出前景，但由于3D高斯原子的自由形式特性，它们在保持表面细节方面存在困难。为了解决表示限制和信息稀缺问题，我们提出了一种新方法FMGS-Avatar，它整合了两个关键创新。首先，我们引入了网格引导的2D高斯泼溅，其中2D高斯原子直接附加到模板网格面上，具有受约束的位置、旋转和运动，实现了更好的表面对齐和几何细节保留。其次，我们利用在大型数据集上训练的基础模型（如Sapiens）来补充单目视频有限的视觉线索。然而，当从基础模型中提炼多模态先验知识时，不同模态表现出不同的参数敏感性，可能导致冲突的优化目标。我们通过具有选择性梯度隔离的协调训练策略解决了这一问题，使每个损失组件能够优化其相关参数而不会相互干扰。通过这种增强表示和协调信息蒸馏的组合，我们的方法显著推进了3D单目人类头像重建。实验评估表明与现有方法相比具有更好的重建质量，在几何准确性和外观保真度方面有显著提升，同时提供丰富的语义信息。此外，在共享规范空间内提炼的先验知识自然地支持在新视角和姿势下进行空间和时间一致的渲染。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单目视频中重建高保真、可动画的3D人像avatar的问题。这个问题在现实中非常重要，因为高保真数字人像创建对娱乐、医疗、AR/VR和交互式模拟等应用至关重要，而传统动作捕捉方法需要昂贵设备或受控环境，限制了技术的普及。能够从普通单目RGB视频创建数字人像的方法将显著降低技术门槛，使更多人能够使用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了单目人像重建的两个主要挑战：几何模糊性和表示局限性。他们借鉴了3D高斯溅射(3DGS)的高效渲染优势，以及基础模型(如Sapiens)提供丰富2D先验知识的能力，还参考了将高斯原语附加到模板网格的方法(如GoMAvatar)。基于这些借鉴，作者创新性地设计了网格引导的2D高斯溅射表示和协调训练策略，以解决表面表示和多模态知识蒸馏中的优化冲突问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用表面导向的2D高斯原语替代体积导向的3D高斯，并结合基础模型的多模态先验知识来解决单目重建的几何模糊性。整体流程包括：1)规范空间表示：将2D高斯原语附加到上采样的模板网格面上；2)多场蒸馏：分别处理几何场、外观场和语义场；3)蒙皮场：通过线性混合蒙皮将规范空间转换到观察空间；4)训练与渲染：使用协调训练策略解决多模态优化冲突，并通过可微分的高光栅化进行渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)网格引导的2D高斯溅射：使用2D高斯原语直接附加到模板网格面上，解决3D高斯的体积性质在表示薄表面时的局限性；2)协调训练策略：通过选择性梯度隔离解决多模态优化冲突，使不同损失组件互不干扰；3)系统性的多模态知识蒸馏：从基础模型中综合提取深度、法线和语义等多种2D先验知识。相比之前工作，该方法使用表面导向而非体积导向的表示，系统性地而非单模态地蒸馏知识，并解决了优化冲突问题，同时实现了更快的训练速度和竞争性的推理性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FMGS-Avatar通过结合网格引导的2D高斯溅射表示和协调训练策略，实现了从单目视频中高效重建高保真、可动画的3D人像，同时系统性地蒸馏了基础模型的多模态2D先验知识，解决了几何模糊性和表示局限性两大挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing high-fidelity animatable human avatars from monocular videosremains challenging due to insufficient geometric information in single-viewobservations. While recent 3D Gaussian Splatting methods have shown promise,they struggle with surface detail preservation due to the free-form nature of3D Gaussian primitives. To address both the representation limitations andinformation scarcity, we propose a novel method, \textbf{FMGS-Avatar}, thatintegrates two key innovations. First, we introduce Mesh-Guided 2D GaussianSplatting, where 2D Gaussian primitives are attached directly to template meshfaces with constrained position, rotation, and movement, enabling superiorsurface alignment and geometric detail preservation. Second, we leveragefoundation models trained on large-scale datasets, such as Sapiens, tocomplement the limited visual cues from monocular videos. However, whendistilling multi-modal prior knowledge from foundation models, conflictingoptimization objectives can emerge as different modalities exhibit distinctparameter sensitivities. We address this through a coordinated trainingstrategy with selective gradient isolation, enabling each loss component tooptimize its relevant parameters without interference. Through this combinationof enhanced representation and coordinated information distillation, ourapproach significantly advances 3D monocular human avatar reconstruction.Experimental evaluation demonstrates superior reconstruction quality comparedto existing methods, with notable gains in geometric accuracy and appearancefidelity while providing rich semantic information. Additionally, the distilledprior knowledge within a shared canonical space naturally enables spatially andtemporally consistent rendering under novel views and poses.</description>
      <author>example@mail.com (Jinlong Fan, Bingyu Hu, Xingguang Li, Yuxiang Yang, Jing Zhang)</author>
      <guid isPermaLink="false">2509.14739v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models</title>
      <link>http://arxiv.org/abs/2509.14723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用transcoders从单细胞基础模型中提取可解释的决策电路，以提高这些模型的透明度和可解释性。&lt;h4&gt;背景&lt;/h4&gt;单细胞基础模型(scFMs)在各种任务上表现出色，如细胞类型注释和扰动响应预测，通过从大规模转录组数据中学习基因调控网络。然而，这些模型的决策过程相比传统方法(如差异基因表达分析)不太可解释。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在利用transcoders技术从单细胞基础模型中提取可解释的决策电路，以揭示模型内部的决策机制并验证其与真实生物学机制的相关性。&lt;h4&gt;方法&lt;/h4&gt;研究者在cell2sentence(C2S)模型(一种先进的单细胞基础模型)上训练了一个transcoder，并利用这个训练好的transcoder从C2S模型中提取内部决策电路。&lt;h4&gt;主要发现&lt;/h4&gt;通过transcoder提取的决策电路与现实世界的生物学机制相对应，表明这些电路确实捕捉了有意义的生物学信息。&lt;h4&gt;结论&lt;/h4&gt;transcoders技术有潜力揭示复杂单细胞模型中生物学上合理的通路，提高了这些模型的可解释性和透明度。&lt;h4&gt;翻译&lt;/h4&gt;单细胞基础模型(scFMs)已通过从大规模转录组数据中学习基因调控网络，在各种任务上展示了最先进的性能，如细胞类型注释和扰动响应预测。然而，一个重大挑战仍然存在：与传统方法如差异基因表达分析相比，这些模型的决策过程不太可解释。最近，transcoders作为一种有前景的方法出现，用于从大型语言模型(LLMs)中提取可解释的决策电路。在这项工作中，我们在cell2sentence(C2S)模型(一种最先进的scFM)上训练了一个transcoder。通过利用训练好的transcoder，我们从C2S模型中提取了内部决策电路。我们证明，发现的电路对应于现实世界的生物学机制，证实了transcoders揭示复杂单细胞模型中生物学上合理的通路的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell foundation models (scFMs) have demonstrated state-of-the-artperformance on various tasks, such as cell-type annotation and perturbationresponse prediction, by learning gene regulatory networks from large-scaletranscriptome data. However, a significant challenge remains: thedecision-making processes of these models are less interpretable compared totraditional methods like differential gene expression analysis. Recently,transcoders have emerged as a promising approach for extracting interpretabledecision circuits from large language models (LLMs). In this work, we train atranscoder on the cell2sentence (C2S) model, a state-of-the-art scFM. Byleveraging the trained transcoder, we extract internal decision-making circuitsfrom the C2S model. We demonstrate that the discovered circuits correspond toreal-world biological mechanisms, confirming the potential of transcoders touncover biologically plausible pathways within complex single-cell models.</description>
      <author>example@mail.com (Sosuke Hosokawa, Toshiharu Kawakami, Satoshi Kodera, Masamichi Ito, Norihiko Takeda)</author>
      <guid isPermaLink="false">2509.14723v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>HARNESS: Lightweight Distilled Arabic Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2509.14689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HArnESS是首个阿拉伯中心的自监督语音模型家族，通过知识蒸馏技术将大型模型压缩为轻量级版本，在资源受限环境中表现出色，仅需微调即可达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大型预训练语音模型在下游任务中表现优异，但在资源受限环境中部署不切实际。&lt;h4&gt;目的&lt;/h4&gt;开发一个轻量级但功能强大的语音模型，专门捕捉阿拉伯语音的细微差别，适用于实际应用场景。&lt;h4&gt;方法&lt;/h4&gt;使用迭代自蒸馏训练大型双语HArnESS (HL) SSL模型，并将知识压缩到学生模型(HS, HST)中，同时使用低秩近似进一步压缩模型。&lt;h4&gt;主要发现&lt;/h4&gt;在阿拉伯语音识别(ASR)、说话人情感识别(SER)和方言识别(DID)任务上，HArnESS与HuBERT和XLS-R相比表现有效，仅需微调即可达到最先进或可比较的性能。&lt;h4&gt;结论&lt;/h4&gt;HArnESS是资源受限环境下的轻量级但功能强大的替代方案，研究团队已发布模型和发现以支持低资源环境中的研究和部署。&lt;h4&gt;翻译&lt;/h4&gt;大型预训练语音模型在下游任务中表现出色，但在资源受限环境中部署不切实际。在本文中，我们介绍了HArnESS，这是第一个阿拉伯中心的自监督语音模型家族，旨在捕捉阿拉伯语音的细微差别。使用迭代自蒸馏，我们训练大型双语HArnESS (HL) SSL模型，然后将知识压缩到压缩的学生模型(HS, HST)中，保留阿拉伯特定表示。我们使用低秩近似将教师的离散监督进一步压缩到浅层、薄型模型中。我们在阿拉伯语音识别(ASR)、说话人情感识别(SER)和方言识别(DID)上评估HArnESS，证明了其相对于HuBERT和XLS-R的有效性。仅通过最小微调，HArnESS就能达到最先进或可比较的性能，使其成为实际应用的轻量级但功能强大的替代方案。我们发布蒸馏模型和发现，以支持低资源环境中的负责任的研究和部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large pre-trained speech models excel in downstream tasks but theirdeployment is impractical for resource-limited environments. In this paper, weintroduce HArnESS, the first Arabic-centric self-supervised speech modelfamily, designed to capture Arabic speech nuances. Using iterativeself-distillation, we train large bilingual HArnESS (HL) SSL models and thendistill knowledge into compressed student models (HS, HST), preservingArabic-specific representations. We use low-rank approximation to furthercompact the teacher's discrete supervision into shallow, thin models. Weevaluate HArnESS on Arabic ASR, Speaker Emotion Recognition (SER), and DialectIdentification (DID), demonstrating effectiveness against HuBERT and XLS-R.With minimal fine-tuning, HArnESS achieves SOTA or comparable performance,making it a lightweight yet powerful alternative for real-world use. We releaseour distilled models and findings to support responsible research anddeployment in low-resource settings.</description>
      <author>example@mail.com (Vrunda N. sukhadia, Shammur Absar Chowdhury)</author>
      <guid isPermaLink="false">2509.14689v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images</title>
      <link>http://arxiv.org/abs/2509.14685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DACoN框架，通过融合基础模型和CNN的特征，实现线条图自动着色，支持任意数量的参考图像，解决了遮挡、姿势变化和视角变化等挑战。&lt;h4&gt;背景&lt;/h4&gt;线条图自动着色研究广泛，旨在减少手绘动漫制作的人工成本。现有深度学习方法在准确性上有所提高，但在处理遮挡、姿势变化和视角变化方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决线条图自动着色中遮挡、姿势变化和视角变化等挑战，提高着色性能。&lt;h4&gt;方法&lt;/h4&gt;DACoN框架利用基础模型捕捉部分级语义，将基础模型的低分辨率语义特征与CNN的高分辨率空间特征融合，进行细粒度且鲁棒的特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;使用多个参考图像可以显著提高线条图自动着色的性能，DACoN框架能够支持任意数量的参考图像，而不再局限于一张或两张参考图像。&lt;h4&gt;结论&lt;/h4&gt;DACoN框架通过融合基础模型和CNN的特征，实现了优越的线条图自动着色性能，解决了现有方法在遮挡、姿势变化和视角变化方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;线条图自动着色已被广泛研究，以减少手绘动漫制作的人工成本。包括图像/视频生成和基于特征的对应在内的深度学习方法提高了准确性，但在处理遮挡、姿势变化和视角变化方面存在困难。为了解决这些挑战，我们提出了DACoN框架，它利用基础模型来捕捉部分级语义，即使在线条图中也是如此。我们的方法将基础模型的低分辨率语义特征与CNN的高分辨率空间特征融合，进行细粒度且鲁棒的特征提取。与依赖多路变换器且仅支持一张或两张参考图像的先前方法不同，DACoN移除了这一限制，允许任意数量的参考图像。定量和定性评估证明了使用多个参考图像的优越性，实现了卓越的着色性能。我们的代码和模型可在https://github.com/kzmngt/DACoN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic colorization of line drawings has been widely studied to reduce thelabor cost of hand-drawn anime production. Deep learning approaches, includingimage/video generation and feature-based correspondence, have improved accuracybut struggle with occlusions, pose variations, and viewpoint changes. Toaddress these challenges, we propose DACoN, a framework that leveragesfoundation models to capture part-level semantics, even in line drawings. Ourmethod fuses low-resolution semantic features from foundation models withhigh-resolution spatial features from CNNs for fine-grained yet robust featureextraction. In contrast to previous methods that rely on the MultiplexTransformer and support only one or two reference images, DACoN removes thisconstraint, allowing any number of references. Quantitative and qualitativeevaluations demonstrate the benefits of using multiple reference images,achieving superior colorization performance. Our code and model are availableat https://github.com/kzmngt/DACoN.</description>
      <author>example@mail.com (Kazuma Nagata, Naoshi Kaneko)</author>
      <guid isPermaLink="false">2509.14685v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model</title>
      <link>http://arxiv.org/abs/2509.14664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at ICONIP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的视觉基础模型解释生成方法，通过引入注意力网格适配器(ALA)和交替周期架构者(AEA)两种机制，解决了现有方法缺乏适应性的问题，提高了模型的解释性能。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型中生成视觉解释已有多种方法被提出，但这些方法通常缺乏适应性，无法应用于复杂模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的视觉基础模型解释生成方法，旨在生成解释并部分更新模型参数以提高可解释性。&lt;h4&gt;方法&lt;/h4&gt;引入两个新机制：注意力网格适配器(ALA)通过消除手动层选择需要来简化过程并增强适应性和可解释性；交替周期架构者(AEA)每隔一个周期更新ALA参数，有效解决注意力区域过小的问题。&lt;h4&gt;主要发现&lt;/h4&gt;在CUB-200-2011和ImageNet-S两个基准数据集上，该方法在平均交并比(IoU)、插入分数、删除分数和插入-删除分数方面均优于基线方法，最佳模型在CUB-200-2011数据集上的平均IoU比基线提高了53.2个百分点。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过两个创新机制成功解决了现有视觉基础模型解释生成方法的局限性，显著提高了模型的适应性和解释性能。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，我们考虑视觉基础模型中生成视觉解释的问题。为此目的已经提出了许多方法；然而，由于缺乏适应性，它们通常无法应用于复杂模型。为了克服这些限制，我们提出了一种新颖的视觉基础模型解释生成方法，旨在生成解释并部分更新模型参数以提高可解释性。我们的方法引入了两种新机制：注意力网格适配器(ALA)和交替周期架构者(AEA)。ALA机制通过消除手动层选择的需要来简化过程，从而增强模型的适应性和可解释性。此外，每隔一个周期更新ALA参数的AEA机制有效地解决了注意力区域过小这一常见问题。我们在两个基准数据集CUB-200-2011和ImageNet-S上评估了我们的方法。我们的结果表明，在CUB-200-2011和ImageNet-S数据集上，我们的方法在平均交并比(IoU)、插入分数、删除分数和插入-删除分数方面均优于基线方法。值得注意的是，与基线方法相比，我们的最佳模型在CUB-200-2011数据集上的平均IoU提高了53.2个百分点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we consider the problem of generating visual explanations invisual foundation models. Numerous methods have been proposed for this purpose;however, they often cannot be applied to complex models due to their lack ofadaptability. To overcome these limitations, we propose a novel explanationgeneration method in visual foundation models that is aimed at both generatingexplanations and partially updating model parameters to enhanceinterpretability. Our approach introduces two novel mechanisms: AttentionLattice Adapter (ALA) and Alternating Epoch Architect (AEA). ALA mechanismsimplifies the process by eliminating the need for manual layer selection, thusenhancing the model's adaptability and interpretability. Moreover, the AEAmechanism, which updates ALA's parameters every other epoch, effectivelyaddresses the common issue of overly small attention regions. We evaluated ourmethod on two benchmark datasets, CUB-200-2011 and ImageNet-S. Our resultsshowed that our method outperformed the baseline methods in terms of meanintersection over union (IoU), insertion score, deletion score, andinsertion-deletion score on both the CUB-200-2011 and ImageNet-S datasets.Notably, our best model achieved a 53.2-point improvement in mean IoU on theCUB-200-2011 dataset compared with the baselines.</description>
      <author>example@mail.com (Shinnosuke Hirano, Yuiga Wada, Tsumugi Iida, Komei Sugiura)</author>
      <guid isPermaLink="false">2509.14664v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics</title>
      <link>http://arxiv.org/abs/2509.14600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the ICML 2025 Workshop on Multi-modal Foundation  Models and Large Language Models for Life Sciences, Vancouver, Canada. 2025.  Copyright 2025 by the author(s). 4 Pages 5 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种改进的粗粒度机器学习模型，通过在损失函数中添加能量匹配项来增强对生物分子系统的模拟效果，尽管在准确性上没有显著提升，但揭示了模型推广自由能表面的不同趋势。&lt;h4&gt;背景&lt;/h4&gt;分子动力学模拟提供生物分子系统的原子级见解，但计算成本高；传统粗粒度机器学习模型的力匹配方法无法完整捕捉势能面，因为梯度拟合可能无法反映低能构象态间的绝对差异。&lt;h4&gt;目的&lt;/h4&gt;通过在损失函数中纳入互补的能量匹配项，改进粗粒度模型对生物分子系统的模拟效果。&lt;h4&gt;方法&lt;/h4&gt;使用CGSchNet模型在Chignolin蛋白上评估新框架，系统性地调整能量损失项的权重。&lt;h4&gt;主要发现&lt;/h4&gt;能量匹配未显著提高模型准确性，但揭示了模型推广自由能表面的不同趋势。&lt;h4&gt;结论&lt;/h4&gt;改进能量估计技术和采用多模态损失公式有望增强未来的粗粒度建模效果。&lt;h4&gt;翻译&lt;/h4&gt;分子动力学模拟为生物分子系统提供了原子级见解，但通常需要高昂的计算成本才能访问长时间尺度。粗粒度机器学习模型为加速采样提供了有希望的途径，但传统的力匹配方法往往无法捕捉完整的势能面，因为在梯度上拟合模型可能无法拟合低能构象态之间的绝对差异。在这项工作中，我们将一个互补的能量匹配项纳入损失函数。我们使用CGSchNet模型在Chignolin蛋白上评估了我们的框架，系统地改变了能量损失项的权重。虽然能量匹配没有在准确性方面产生统计学上的显著改进，但它揭示了模型如何推广自由能表面的不同趋势。我们的研究结果表明，通过改进能量估计技术和多模态损失公式，未来有机会增强粗粒度建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics (MD) simulations provide atomistic insight intobiomolecular systems but are often limited by high computational costs requiredto access long timescales. Coarse-grained machine learning models offer apromising avenue for accelerating sampling, yet conventional force matchingapproaches often fail to capture the full thermodynamic landscape as fitting amodel on the gradient may not fit the absolute differences between low-energyconformational states. In this work, we incorporate a complementary energymatching term into the loss function. We evaluate our framework on theChignolin protein using the CGSchNet model, systematically varying the weightof the energy loss term. While energy matching did not yield statisticallysignificant improvements in accuracy, it revealed distinct tendencies in howmodels generalize the free energy surface. Our results suggest futureopportunities to enhance coarse-grained modeling through improved energyestimation techniques and multi-modal loss formulations.</description>
      <author>example@mail.com (Alexander Aghili, Andy Bruce, Daniel Sabo, Razvan Marinescu)</author>
      <guid isPermaLink="false">2509.14600v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>CLAIP-Emo: Parameter-Efficient Adaptation of Language-supervised models for In-the-Wild Audiovisual Emotion Recognition</title>
      <link>http://arxiv.org/abs/2509.14527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The code and models will be available at  https://github.com/MSA-LMC/CLAIP-Emo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CLAIP-Emo 是一个模块化框架，通过参数高效适应语言监督基础模型（CLIP/CLAP）解决实际环境中音频情感识别的挑战，仅需少量训练参数即可达到最先进水平。&lt;h4&gt;背景&lt;/h4&gt;实际环境中的音频情感识别（AVER）仍受姿势变化、遮挡和背景噪声的阻碍。现有方法主要依赖大规模领域特定预训练，成本高昂且与真实情感数据不匹配。&lt;h4&gt;目的&lt;/h4&gt;提出一个模块化框架，将实际环境中的 AVER 重新定义为语言监督基础模型的参数高效适应方案。&lt;h4&gt;方法&lt;/h4&gt;保留语言监督先验（冻结主干并通过 LoRA 进行情感导向适应，更新≤4.0%参数）；非对称时间建模（视觉动态使用轻量级 Transformer，音频韵律使用平均池化）；应用简单融合头进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;在 DFEW 和 MAFW 数据集上，CLAIP-Emo (ViT-L/14) 仅使用 8M 训练参数分别实现了 80.14% 和 61.18% 的加权平均召回率，创造了新的最先进水平。&lt;h4&gt;结论&lt;/h4&gt;语言监督基础模型的参数高效适应为实际环境中的 AVER 提供了可扩展的替代方案，优于领域特定的预训练。&lt;h4&gt;翻译&lt;/h4&gt;实际环境中的音频情感识别（AVER）仍受到姿势变化、遮挡和背景噪声的阻碍。现有方法主要依赖大规模领域特定的预训练，这成本高昂且常常与真实情感数据不匹配。为解决这一问题，我们提出了 CLAIP-Emo，一个模块化框架，将实际环境中的 AVER 重新定义为语言监督基础模型（CLIP/CLAP）的参数高效适应。具体而言，它（i）通过冻结 CLIP/CLAP 主干并通过 LoRA 进行情感导向的适应来保留语言监督先验（更新总参数的≤4.0%），（ii）非对称分配时间建模，对视觉动态使用轻量级 Transformer，对音频韵律应用平均池化，以及（iii）应用简单的融合头进行预测。在 DFEW 和 MAFW 上，CLAIP-Emo (ViT-L/14) 仅使用 8M 训练参数就分别实现了 80.14% 和 61.18% 的加权平均召回率，创造了新的最先进水平。我们的研究表明，语言监督基础模型的参数高效适应为实际环境中的 AVER 提供了可扩展的替代方案，优于领域特定的预训练。代码和模型将在 https://github.com/MSA-LMC/CLAIP-Emo 上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audiovisual emotion recognition (AVER) in the wild is still hindered by posevariation, occlusion, and background noise. Prevailing methods primarily relyon large-scale domain-specific pre-training, which is costly and oftenmismatched to real-world affective data. To address this, we present CLAIP-Emo,a modular framework that reframes in-the-wild AVER as a parameter-efficientadaptation of language-supervised foundation models (CLIP/CLAP). Specifically,it (i) preserves language-supervised priors by freezing CLIP/CLAP backbones andperforming emotion-oriented adaptation via LoRA (updating \ensuremath{\le}4.0\%of the total parameters), (ii) allocates temporal modeling asymmetrically,employing a lightweight Transformer for visual dynamics while applying meanpooling for audio prosody, and (iii) applies a simple fusion head forprediction. On DFEW and MAFW, CLAIP-Emo (ViT-L/14) achieves 80.14\% and 61.18\%weighted average recall with only 8M training parameters, setting a new stateof the art. Our findings suggest that parameter-efficient adaptation oflanguage-supervised foundation models provides a scalable alternative todomain-specific pre-training for real-world AVER. The code and models will beavailable at\href{https://github.com/MSA-LMC/CLAIP-Emo}{https://github.com/MSA-LMC/CLAIP-Emo}.</description>
      <author>example@mail.com (Yin Chen, Jia Li, Jinpeng Hu, Zhenzhen Hu, Richang Hong)</author>
      <guid isPermaLink="false">2509.14527v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents</title>
      <link>http://arxiv.org/abs/2509.14480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种用于训练智能体掌握工具集成推理(TIR)的新方法，通过强化学习沙盒环境和回合级裁决强化学习(TARL)策略，显著提高了任务通过率，并成功训练了具有工具使用能力的多模态基础模型。&lt;h4&gt;背景&lt;/h4&gt;有效使用交互工具需要智能体掌握工具集成推理(TIR)，这是一个涉及多轮规划和长上下文对话管理的复杂过程。在多模态上下文中训练这种动态过程尤其具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;训练智能体在多模态上下文中掌握工具集成推理能力，开发更自然、语音驱动的交互智能体。&lt;h4&gt;方法&lt;/h4&gt;引入支持交错语音-文本展开的强化学习沙盒环境；提出回合级裁决强化学习(TARL)策略，使用大型语言模型作为裁判提供回合级评估；集成包含数学推理问题的混合任务训练课程以增强探索能力；在交错语音-文本展开上训练基础多模态大型语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;统一的方法将基于文本的τ-bench上的任务通过率提高了6%以上，相比强大的强化学习基线；框架适合微调多模态基础模型用于智能体任务；通过训练使基础多模态大型语言模型获得了工具使用能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为开发更自然、语音驱动的交互智能体铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;有效的交互工具使用需要智能体掌握工具集成推理(TIR)：一个涉及多轮规划和长上下文对话管理的复杂过程。为了在多模态上下文中训练智能体掌握这一动态过程，我们引入了一个支持交错语音-文本展开的强化学习(RL)沙盒环境。我们的核心策略——回合级裁决强化学习(TARL)——通过使用大型语言模型(LLM)作为裁判提供回合级评估，解决了长距离任务中的信用分配挑战。为了增强探索能力，我们整合了包含数学推理问题的混合任务训练课程。与强大的强化学习基线相比，这种统一的方法将基于文本的τ-bench上的任务通过率提高了6%以上。关键的是，我们证明了我们的框架适合微调多模态基础模型用于智能体任务。通过在交错语音-文本展开上训练基础多模态大型语言模型，我们为其赋予了工具使用能力，为开发更自然、语音驱动的交互智能体铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective interactive tool use requires agents to master Tool IntegratedReasoning (TIR): a complex process involving multi-turn planning andlong-context dialogue management. To train agents for this dynamic process,particularly in multi-modal contexts, we introduce a sandbox environment forreinforcement learning (RL) that supports interleaved speech-text rollouts. Ourcore strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addressesthe challenge of credit assignment in long-horizon tasks by employing a LargeLanguage Model (LLM) as a judge to provide turn-level evaluation. To enhanceexploration, we integrate a mixed-task training curriculum with mathematicalreasoning problems. This unified approach boosts the task pass rate on thetext-based $\tau$-bench by over 6% compared to strong RL baselines. Crucially,we demonstrate our framework's suitability for fine-tuning a multi-modalfoundation model for agentic tasks. By training a base multi-modal LLM oninterleaved speech-text rollouts, we equip it with tool-use abilities, pavingthe way for more natural, voice-driven interactive agents.</description>
      <author>example@mail.com (Weiting Tan, Xinghua Qu, Ming Tu, Meng Ge, Andy T. Liu, Philipp Koehn, Lu Lu)</author>
      <guid isPermaLink="false">2509.14480v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</title>
      <link>http://arxiv.org/abs/2509.14380v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CRAFT框架，利用基础模型作为多机器人协调的'教练'，自动分解长期任务为子任务序列，通过LLM生成奖励函数，并用VLM优化，成功实现了复杂协调行为的学习。&lt;h4&gt;背景&lt;/h4&gt;多智能体强化学习为多系统协调提供了强大框架，但应用于机器人仍面临高维连续动作空间、复杂奖励设计和去中心化环境中非平稳转换等挑战。相比之下，人类通过阶段性课程学习协调能力，逐步构建长期行为。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自动分解长期协调任务、生成适当奖励函数并指导多机器人系统学习复杂协调行为的框架，解决MARL在机器人应用中的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出CRAFT框架，利用大型语言模型规划能力自动分解任务为子任务序列，使用LLM生成奖励函数，通过VLM引导的奖励循环进行优化，并在多四足导航和双臂操作任务上评估，最后在实际硬件中验证导航策略。&lt;h4&gt;主要发现&lt;/h4&gt;CRAFT框架能够有效学习复杂的协调行为，在多四足导航和双臂操作任务上表现良好，且其多四足导航策略在实际硬件实验中得到了成功验证。&lt;h4&gt;结论&lt;/h4&gt;CRAFT通过利用基础模型作为'教练'，有效解决了多机器人协调学习中的挑战，特别是在处理长期协调任务方面，为实际应用提供了可行方案。&lt;h4&gt;翻译&lt;/h4&gt;多智能体强化学习为多智能体系统中的协调学习提供了强大的框架。然而，由于高维连续联合动作空间、复杂的奖励设计以及去中心化设置中固有的非平稳转换，将MARL应用于机器人仍然具有挑战性。另一方面，人类通过阶段性课程学习复杂的协调能力，其中长期行为建立在更简单的技能基础上。受此启发，我们提出了CRAFT：一种利用基础模型推理能力作为多机器人协调任务'教练'的框架。CRAFT利用大型语言模型的规划能力，自动将长期协调任务分解为子任务序列。随后，CRAFT使用大型语言模型生成的奖励函数训练每个子任务，并通过视觉语言模型引导的奖励循环进行优化。我们在多四足导航和双臂操作任务上评估了CRAFT，展示了其学习复杂协调行为的能力。此外，我们在实际硬件实验中验证了多四足导航策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-Agent Reinforcement Learning (MARL) provides a powerful framework forlearning coordination in multi-agent systems. However, applying MARL torobotics still remains challenging due to high-dimensional continuous jointaction spaces, complex reward design, and non-stationary transitions inherentto decentralized settings. On the other hand, humans learn complex coordinationthrough staged curricula, where long-horizon behaviors are progressively builtupon simpler skills. Motivated by this, we propose CRAFT: CoachingReinforcement learning Autonomously using Foundation models for multi-robotcoordination Tasks, a framework that leverages the reasoning capabilities offoundation models to act as a "coach" for multi-robot coordination. CRAFTautomatically decomposes long-horizon coordination tasks into sequences ofsubtasks using the planning capability of Large Language Models (LLMs). In whatfollows, CRAFT trains each subtask using reward functions generated by LLM, andrefines them through a Vision Language Model (VLM)-guided reward-refinementloop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulationtasks, demonstrating its capability to learn complex coordination behaviors. Inaddition, we validate the multi-quadruped navigation policy in real hardwareexperiments.</description>
      <author>example@mail.com (Seoyeon Choi, Kanghyun Ryu, Jonghoon Ock, Negar Mehr)</author>
      <guid isPermaLink="false">2509.14380v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning</title>
      <link>http://arxiv.org/abs/2509.14373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CodeLSI是一个结合低秩优化和领域特定指令调优的框架，用于解决基础模型在自动代码生成中面临的领域特异性、成本效益和安全性挑战，特别是在不依赖第三方API的情况下生成高质量代码。&lt;h4&gt;背景&lt;/h4&gt;使用基础模型进行自动代码生成为提高软件开发效率提供了有前景的解决方案。然而，在确保领域特异性、成本效益和安全性方面仍然存在挑战，特别是在依赖第三方API时。&lt;h4&gt;目的&lt;/h4&gt;开发并评估CodeLSI，这是一种新的方法，用于生成针对特定领域的高质量代码，使用在公司基础设施上微调的基础模型，不依赖外部API。&lt;h4&gt;方法&lt;/h4&gt;CodeLSI应用低秩适配技术来减少模型预训练和微调的计算成本。采用领域特定指令调优使代码生成与组织需求保持一致。研究者在内部软件项目的数据集上使用JavaScript编码任务对框架进行了实现和测试。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明，CodeLSI能够生成高质量、具有上下文感知能力的代码。在相关性、准确性和领域适应性方面，它优于基线模型。低秩优化的使用显著降低了资源需求，使在公司拥有的基础设施上进行可扩展的训练成为可能。&lt;h4&gt;结论&lt;/h4&gt;CodeLSI证明，结合低秩优化和领域特定调优可以增强基础模型在自动代码生成中的实用性和性能。这种方法为基于商业API的解决方案提供了安全、成本高效的替代方案，并支持软件开发中更快、更有针对性的创新。&lt;h4&gt;翻译&lt;/h4&gt;背景：使用基础模型进行自动代码化为提高软件开发效率提供了有前景的解决方案。然而，在确保领域特异性、成本效益和安全性方面仍然存在挑战，特别是在依赖第三方API时。本文介绍了CodeLSI，这是一个结合低秩优化和领域特定指令调优以应对这些挑战的框架。目的：本研究旨在开发并评估CodeLSI，这是一种新的方法，用于生成针对特定领域的高质量代码，使用在公司基础设施上微调的基础模型，不依赖外部API。方法：CodeLSI应用低秩适配技术来减少模型预训练和微调的计算成本。采用领域特定指令调优使代码生成与组织需求保持一致。研究者在内部软件项目的数据集上使用JavaScript编码任务对框架进行了实现和测试。结果：实验评估表明，CodeLSI能够生成高质量、具有上下文感知能力的代码。在相关性、准确性和领域适应性方面，它优于基线模型。低秩优化的使用显著降低了资源需求，使在公司拥有的基础设施上进行可扩展的训练成为可能。结论：CodeLSI证明，结合低秩优化和领域特定调优可以增强基础模型在自动代码生成中的实用性和性能。这种方法为基于商业API的解决方案提供了安全、成本高效的替代方案，并支持软件开发中更快、更有针对性的创新。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Context: Automated code generation using Foundation Models (FMs) offerspromising solutions for enhancing software development efficiency. However,challenges remain in ensuring domain specificity, cost-effectiveness, andsecurity - especially when relying on third-party APIs. This paper introducesCodeLSI, a framework that combines low-rank optimization and domain-specificinstruction tuning to address these challenges.  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novelapproach for generating high-quality code tailored to specific domains, usingFMs fine-tuned on company infrastructure without dependence on external APIs.  Methods: CodeLSI applies low-rank adaptation techniques to reduce thecomputational cost of model pre-training and fine-tuning. Domain-specificinstruction tuning is employed to align code generation with organizationalneeds. We implemented and tested the framework on real-world JavaScript codingtasks using datasets drawn from internal software projects.  Results: Experimental evaluations show that CodeLSI produces high-quality,context aware code. It outperforms baseline models in terms of relevance,accuracy, and domain fit. The use of low-rank optimization significantlyreduced resource requirements, enabling scalable training on company-ownedinfrastructure.  Conclusion: CodeLSI demonstrates that combining low-rank optimization withdomain specific tuning can enhance the practicality and performance of FMs forautomated code generation. This approach provides a secure, cost-efficientalternative to commercial API based solutions and supports faster, moretargeted innovation in software development.</description>
      <author>example@mail.com (Huy Le, Phong Nguyen, Hao Do, Tuan Nguyen, Thien Pham, Anh Nguyen-Duc, Tho Quan)</author>
      <guid isPermaLink="false">2509.14373v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Reaction dynamics of lithium-mediated electrolyte decomposition using machine learning potentials</title>
      <link>http://arxiv.org/abs/2509.14067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究揭示了碳酸乙烯酯在锂原子和锂金属表面的开环分解机制，发现了一种新的超快分解途径，其反应速率比传统路径快几个数量级。&lt;h4&gt;背景&lt;/h4&gt;研究碳酸乙烯酯在单个锂原子存在下以及在锂金属表面的开环分解过程。&lt;h4&gt;目的&lt;/h4&gt;通过精确计算获得碳酸乙烯酯分解的自由能剖面和反应速率，揭示反应机制和动力学特性。&lt;h4&gt;方法&lt;/h4&gt;结合精确的电子结构理论、增强采样技术和机器学习方法，优化MACE-MP0基础模型，应用优化后的机器学习势能面进行计算。&lt;h4&gt;主要发现&lt;/h4&gt;1) 电子结构理论水平对结果至关重要，不准确的密度泛函会高估反应速率达9个数量级；2) 谐波过渡态理论低估反应速率约1个数量级；3) 发现并表征了一种新的超快分解途径，羰基深度插入锂表面并弯曲约70度；4) 超快反应在几十皮秒内发生，生成开环中间体，是CO或CO2形成的前体；5) 另一种生成CO3^2-和乙烯的途径竞争力弱，发生在几十纳秒时间尺度。&lt;h4&gt;结论&lt;/h4&gt;碳酸乙烯酯在锂表面的分解存在超快路径，比传统路径快几个数量级，这对理解锂离子电池中电解液分解机制具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了在单个锂原子存在下以及在锂金属表面上的碳酸乙烯酯的开环分解。结合精确的电子结构理论、增强采样和机器学习，我们优化了MACE-MP0基础模型，并将得到的机器学习势能面应用于获得统计收敛的自由能剖面和反应速率。我们确认电子结构理论水平很重要，不准确的密度泛函会高估反应速率达9个数量级。我们还发现谐波过渡态理论低估反应速率约1个数量级。对于表面反应，我们发现并表征了一种新的超快分解途径，其中羰基深度插入锂表面并弯曲约70度。这个反应在几十皮秒内发生，生成开环中间体，是CO或CO2形成的前体；相比之下，另一种生成CO3^2-和乙烯的途径竞争力不强，发生在几十纳秒的时间尺度上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the ring-opening decomposition of ethylene carbonate in the presenceof a single lithium atom and on the surface of lithium metal. Combiningaccurate electronic structure theory, enhanced sampling, and machine learning,we fine-tune the MACE-MP0 foundation model and apply the resulting machinelearning potentials to obtain statistically converged free energy profiles andreaction rates. We confirm that the level of electronic structure theory isimportant, and inaccurate density functionals can overestimate the reactionrate by up to nine orders of magnitude. We also find that harmonic transitionstate theory underestimates reaction rates by about one order of magnitude. Forthe surface reaction, we find and characterize a new, ultrafast decompositionpathway wherein the carbonyl is deeply inserted into the lithium surface andbent by about 70$^\circ$. This reaction, which occurs in a few tens ofpicoseconds, generates a ring-opened intermediate that is a precursor for CO orCO$_2$ formation; by contrast, an alternative pathway that yields CO$_3^{2-}$and ethylene is found to be non-competitive, occurring on a timescale of tensof nanoseconds.</description>
      <author>example@mail.com (Sohang Kundu, Diana Chamaki, Hong-Zhou Ye, Garvit Agarwal, Timothy C. Berkelbach)</author>
      <guid isPermaLink="false">2509.14067v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>SAIL-VL2 Technical Report</title>
      <link>http://arxiv.org/abs/2509.14033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAIL-VL2是一个开放的多模态基础模型，作为SAIL-VL的后续版本，在20亿和80亿参数规模上实现了最先进性能，从细粒度感知到复杂推理都表现出强大能力。&lt;h4&gt;背景&lt;/h4&gt;SAIL-VL2是SAIL-VL的后续版本，旨在开发一个全面的多模态理解和推理基础模型。&lt;h4&gt;目的&lt;/h4&gt;创建一个能够进行综合多模态理解和推理的基础模型，并在各种图像和视频基准测试中实现最先进的性能。&lt;h4&gt;方法&lt;/h4&gt;三大核心创新：1)大规模数据整理流程，通过评分和过滤策略提高质量和分布；2)渐进式训练框架，从强大的预训练视觉编码器开始，经过多模态预训练，最终采用思考融合的SFT-RL混合范式；3)架构改进，扩展到高效的稀疏专家混合(MoE)设计。&lt;h4&gt;主要发现&lt;/h4&gt;SAIL-VL2在106个数据集上具有竞争力，在MMMU和MathVista等挑战性推理基准测试上取得最先进结果，在OpenCompass排行榜上，SAIL-VL2-2B在40亿参数规模以下的官方发布开源模型中排名第一。&lt;h4&gt;结论&lt;/h4&gt;SAIL-VL2是一个高效且可扩展的开源多模态社区基础，为开源多模态社区提供了强大的基础。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了SAIL-VL2，这是一个开放的多模态基础模型，用于全面的多模态理解和推理。作为SAIL-VL的后续版本，SAIL-VL2在20亿和80亿参数规模上跨越各种图像和视频基准测试实现了最先进的性能，展示了从细粒度感知到复杂推理的强大能力。三个核心创新推动了其有效性。首先，具有评分和过滤策略的大规模数据整理流程提高了字幕、OCR、问答和视频数据的质量和分布，提高了训练效率。其次，渐进式训练框架从强大的预训练视觉编码器(SAIL-ViT)开始，通过多模态预训练，最终采用思考融合的SFT-RL混合范式，系统性地增强模型能力。第三，架构进步从密集LLM扩展到高效的稀疏专家混合(MoE)设计。凭借这些贡献，SAIL-VL2在106个数据集上展示了具有竞争力的性能，并在MMMU和MathVista等具有挑战性的推理基准测试上取得了最先进的结果。此外，在OpenCompass排行榜上，SAIL-VL2-2B在40亿参数规模以下的官方发布开源模型中排名第一，同时作为开源多模态社区的高效且可扩展的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)for comprehensive multimodal understanding and reasoning. As the successor toSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8Bparameter scales across diverse image and video benchmarks, demonstratingstrong capabilities from fine-grained perception to complex reasoning. Threecore innovations drive its effectiveness. First, a large-scale data curationpipeline with scoring and filtering strategies enhances both quality anddistribution across captioning, OCR, QA, and video data, improving trainingefficiency. Second, a progressive training framework begins with a powerfulpre-trained vision encoder (SAIL-ViT), advances through multimodalpre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm thatsystematically strengthens model capabilities. Third, architectural advancesextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.With these contributions, SAIL-VL2 demonstrates competitive performance across106 datasets and achieves state-of-the-art results on challenging reasoningbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompassleaderboard, SAIL-VL2-2B ranks first among officially released open-sourcemodels under the 4B parameter scale, while serving as an efficient andextensible foundation for the open-source multimodal community.</description>
      <author>example@mail.com (Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng)</author>
      <guid isPermaLink="false">2509.14033v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates</title>
      <link>http://arxiv.org/abs/2509.13906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TFMAdapter，一种轻量级实例级适配器，解决了时间序列基础模型(TSFMs)无法利用协变量的问题，通过两阶段方法在不微调的情况下增强TSFMs的协变量信息处理能力，在实验中实现了24-27%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)最近在单变量预测方面取得了最先进性能，仅基于过去值的简短历史就能在新时间序列上表现良好，表明大规模跨领域预训练可获取归纳偏置。然而，大多数TSFMs无法利用协变量——许多应用中准确预测所需的关键外生变量。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法使TSFMs能够利用协变量信息，同时保持其通用性和轻量化特性，无需进行模型微调。&lt;h4&gt;方法&lt;/h4&gt;提出TFMAdapter，一种轻量级、实例级别的适配器，在单次模型调用中操作有限历史，学习非参数级联来组合协变量和TSFM预测。采用两阶段方法：(1)使用简单回归模型生成伪预测；(2)训练高斯过程回归器利用伪预测、TSFM预测和协变量优化预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的广泛实验表明，TFMAdapter始终优于基础模型和监督基线，相比基础基础模型实现了24-27%的性能提升，且数据计算开销最小。&lt;h4&gt;结论&lt;/h4&gt;轻量级适配器具有弥合通用基础模型和领域特定预测需求之间差距的潜力，为时间序列预测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)最近仅通过基于过去值的简短历史就在单变量预测方面取得了最先进性能。它们的成功表明，大规模跨领域预训练可以获取归纳偏置，从而从简短历史中的时间模式泛化。然而，由于领域特定性质和相关归纳偏置的缺乏，大多数TSFMs无法利用协变量——许多应用中准确预测所需的关键外生变量。我们提出TFMAdapter，一种轻量级、实例级别的适配器，在不进行微调的情况下增强TSFMs的协变量信息。TFMAdapter在单次模型调用中操作有限历史，学习非参数级联来组合协变量和单变量TSFM预测。然而，这种学习需要在历史中的所有步骤都有单变量预测，导致需要多次调用TSFM。为了在限制TSFM调用的同时在完整历史上下文上进行训练，TFMAdapter使用两阶段方法：(1)使用简单回归模型生成伪预测；(2)训练高斯过程回归器使用伪预测、TSFM预测和协变量来优化预测。在真实世界数据集上的广泛实验表明，TFMAdapter始终优于基础模型和监督基线，以最小的数据和计算开销相比基础基础模型实现了24-27%的改进。我们的结果突显了轻量级适配器弥合通用基础模型和领域特定预测需求之间差距的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761272&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time Series Foundation Models (TSFMs) have recently achieved state-of-the-artperformance in univariate forecasting on new time series simply by conditionedon a brief history of past values. Their success demonstrates that large-scalepretraining across diverse domains can acquire the inductive bias to generalizefrom temporal patterns in a brief history. However, most TSFMs are unable toleverage covariates -- future-available exogenous variables critical foraccurate forecasting in many applications -- due to their domain-specificnature and the lack of associated inductive bias. We propose TFMAdapter, alightweight, instance-level adapter that augments TSFMs with covariateinformation without fine-tuning. Instead of retraining, TFMAdapter operates onthe limited history provided during a single model call, learning anon-parametric cascade that combines covariates with univariate TSFM forecasts.However, such learning would require univariate forecasts at all steps in thehistory, requiring too many calls to the TSFM. To enable training on the fullhistorical context while limiting TSFM invocations, TFMAdapter uses a two-stagemethod: (1) generating pseudo-forecasts with a simple regression model, and (2)training a Gaussian Process regressor to refine predictions using both pseudo-and TSFM forecasts alongside covariates. Extensive experiments on real-worlddatasets demonstrate that TFMAdapter consistently outperforms both foundationmodels and supervised baselines, achieving a 24-27\% improvement over basefoundation models with minimal data and computational overhead. Our resultshighlight the potential of lightweight adapters to bridge the gap betweengeneric foundation models and domain-specific forecasting needs.</description>
      <author>example@mail.com (Afrin Dange, Sunita Sarawagi)</author>
      <guid isPermaLink="false">2509.13906v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.14151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE TCSVT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新的几何感知教师-学生框架BEVUDA++，用于解决BEV感知中的领域适应挑战，通过融合深度感知信息和多空间特征映射，有效减少了跨领域场景中的性能下降。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的鸟瞰图(BEV)感知在自动驾驶领域具有巨大潜力，但近期研究忽视了领域迁移问题，导致模型在跨领域场景中性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;解决BEV感知中多视图3D目标检测的领域适应(DA)挑战，减少多几何空间中的领域迁移累积问题。&lt;h4&gt;方法&lt;/h4&gt;提出BEVUDA++框架，包含可靠深度教师(RDT)和几何一致学生(GCS)模型。RDT融合目标LiDAR和深度预测生成深度感知信息；GCS将多空间特征映射到统一几何嵌入空间；引入不确定性引导的指数移动平均(UEMA)减少误差累积。&lt;h4&gt;主要发现&lt;/h4&gt;在四个跨领域场景的实验中，该方法在BEV 3D目标检测任务中取得了最先进性能，如在昼夜适应任务上NDS提升12.9%，mAP提升9.5%。&lt;h4&gt;结论&lt;/h4&gt;BEVUDA++框架有效解决了BEV感知中的领域适应挑战，显著提升了模型在跨领域场景中的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的鸟瞰图(BEV)感知在自动驾驶领域具有巨大潜力。近期研究优先考虑效率或准确性提升，但忽视了领域迁移问题，导致在迁移过程中性能显著下降。我们确定了真实世界跨领域场景中的主要领域差距，并首次尝试解决BEV感知中多视图3D目标检测的领域适应(DA)挑战。鉴于BEV感知方法的复杂性及其多组件特性，多几何空间(如2D、3D Voxel、BEV)中的领域迁移累积对BEV领域适应构成了重大挑战。本文提出了一种创新的几何感知教师-学生框架BEVUDA++，以减轻这一问题，包含可靠深度教师(RDT)和几何一致学生(GCS)模型。具体而言，RDT有效融合目标LiDAR和可靠的深度预测，基于不确定性估计生成深度感知信息，增强了对目标域理解至关重要的Voxel和BEV特征的提取。为协同减少领域迁移，GCS将多空间特征映射到统一的几何嵌入空间，从而缩小两领域间的数据分布差距。此外，我们引入了一种新颖的不确定性引导指数移动平均(UEMA)，进一步减少由领域迁移导致的误差累积，基于先前获得的不确定性指导。为证明我们提出方法的优越性，我们在四个跨领域场景中进行了全面实验，在BEV 3D目标检测任务中取得了最先进性能，例如在昼夜适应任务上NDS提升12.9%，mAP提升9.5%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域中基于鸟瞰图(BEV)感知的多视角3D目标检测存在的领域自适应(DA)挑战。具体来说，就是解决模型在跨领域场景(如不同场景、天气、昼夜变化)下因数据分布差异导致的性能大幅下降问题。这个问题在现实中非常重要，因为自动驾驶系统需要在各种真实世界条件下可靠工作，而标记目标领域的数据在实际应用中往往不切实际。现有BEV感知方法主要关注提高效率或准确性，忽视了领域自适应问题，导致在实际部署中性能不佳，解决这一问题可以显著提高自动驾驶系统在多样化环境下的鲁棒性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到BEV感知方法在跨领域场景下存在显著的性能下降，特别是在LSS(基于空间变换的)方法中，由于多几何空间(2D图像、3D体素、BEV空间)的复杂性，领域偏移误差会累积。作者借鉴了教师-学生框架(mean teacher)的思想，因为教师预测通常比标准模型具有更高的质量。他们设计了一个几何感知的教师-学生框架BEVUDA++，包括一个可靠的深度教师(RDT)和一个几何一致的学生(GCS)模型，并引入不确定性引导的指数移动平均(UEMA)策略。该方法借鉴了现有工作如BEVDepth、MC Dropout不确定性估计、STM3D的自训练策略以及MTTrans和DANN的多空间特征对齐和领域对抗训练等技术，但针对BEV感知的特点进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过处理多几何空间中的领域偏移累积问题，利用深度感知信息和几何一致性来减少不同领域间的数据分布差距。整体实现流程包括：1)可靠深度教师(RDT)融合目标LiDAR数据和可靠的深度预测，基于不确定性估计生成深度感知信息，使用MC Dropout方法过滤可靠深度预测；2)几何一致学生(GCS)将多个空间特征映射到统一几何嵌入空间，通过MLP转换特征并使用对齐损失和知识转移损失减少领域差距；3)不确定性引导的指数移动平均(UEMA)动态更新教师模型参数，根据不确定性调整更新速率；4)结合检测损失、监督损失、知识转移损失和对齐损失进行训练优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)几何感知的教师-学生框架BEVUDA++，专门针对BEV感知的领域自适应问题；2)可靠深度教师(RDT)，融合LiDAR和可靠深度预测，使用不确定性估计减少噪声；3)几何一致学生(GCS)，将多几何空间特征投影到统一嵌入空间；4)不确定性引导的指数移动平均(UEMA)，动态调整教师模型更新策略。相比之前的工作，本文专注于BEV感知而非2D或单目3D检测，同时处理多个几何空间的领域偏移，使用不确定性估计而非简单置信度评分指导深度预测，并设计了动态EMA策略而非静态更新策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种几何感知的教师-学生框架BEVUDA++，通过可靠的深度估计和几何特征对齐有效解决了BEV感知中多几何空间的领域偏移累积问题，显著提升了自动驾驶系统在跨领域场景下的3D目标检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-centric Bird's Eye View (BEV) perception holds considerable promisefor autonomous driving. Recent studies have prioritized efficiency or accuracyenhancements, yet the issue of domain shift has been overlooked, leading tosubstantial performance degradation upon transfer. We identify major domaingaps in real-world cross-domain scenarios and initiate the first effort toaddress the Domain Adaptation (DA) challenge in multi-view 3D object detectionfor BEV perception. Given the complexity of BEV perception approaches withtheir multiple components, domain shift accumulation across multi-geometricspaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domainadaptation. In this paper, we introduce an innovative geometric-awareteacher-student framework, BEVUDA++, to diminish this issue, comprising aReliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.Specifically, RDT effectively blends target LiDAR with dependable depthpredictions to generate depth-aware information based on uncertaintyestimation, enhancing the extraction of Voxel and BEV features that areessential for understanding the target domain. To collaboratively reduce thedomain shift, GCS maps features from multiple spaces into a unified geometricembedding space, thereby narrowing the gap in data distribution between the twodomains. Additionally, we introduce a novel Uncertainty-guided ExponentialMoving Average (UEMA) to further reduce error accumulation due to domain shiftsinformed by previously obtained uncertainty guidance. To demonstrate thesuperiority of our proposed method, we execute comprehensive experiments infour cross-domain scenarios, securing state-of-the-art performance in BEV 3Dobject detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Nightadaptation.</description>
      <author>example@mail.com (Rongyu Zhang, Jiaming Liu, Xiaoqi Li, Xiaowei Chi, Dan Wang, Li Du, Yuan Du, Shanghang Zhang)</author>
      <guid isPermaLink="false">2509.14151v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Paediatric Sleep Apnoea Detection Using Physiology-Guided Acoustic Models</title>
      <link>http://arxiv.org/abs/2509.15008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种迁移学习框架，将预训练在成人睡眠数据上的声学模型适应到儿科阻塞性睡眠呼吸暂停(OSA)检测中，通过整合SpO2去饱和模式增强模型训练，显著提高了儿童OSA检测的准确性。&lt;h4&gt;背景&lt;/h4&gt;儿童阻塞性睡眠呼吸暂停在临床上具有重要意义但难以诊断，因为儿童对基于传感器的多导睡眠图耐受性差。声学监测为家庭OSA筛查提供了非侵入性替代方案，但儿科数据有限，阻碍了强大的深度学习方法的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一个迁移学习框架，将预训练在成人睡眠数据上的声学模型适应到儿科OSA检测中，并通过整合基于SpO2的去饱和模式来增强模型训练。&lt;h4&gt;方法&lt;/h4&gt;使用大型成人睡眠数据集(157个夜晚)和小型儿科数据集(15个夜晚)进行系统评估，比较单任务与多任务学习、编码器冻结与完整微调，以及延迟SpO2标签的影响，以更好地与声学对齐并捕获生理上有意义的特征。&lt;h4&gt;主要发现&lt;/h4&gt;使用SpO2集成的微调相比没有适应的基线模型，持续改善了儿科OSA检测效果。&lt;h4&gt;结论&lt;/h4&gt;证明了迁移学习用于儿童家庭OSA筛查的可行性，展示了其在早期诊断中的潜在临床价值。&lt;h4&gt;翻译&lt;/h4&gt;儿科阻塞性睡眠呼吸暂停在临床上具有重要意义但难以诊断，因为儿童对基于传感器的多导睡眠图耐受性差。声学监测为家庭OSA筛查提供了非侵入性替代方案，但有限的儿科数据阻碍了强大的深度学习方法的发展。本文提出了一种迁移学习框架，将预训练在成人睡眠数据上的声学模型适应到儿科OSA检测中，整合基于SpO2的去饱和模式以增强模型训练。使用大型成人睡眠数据集(157个夜晚)和小型儿科数据集(15个夜晚)，我们系统评估了(i)单任务与多任务学习，(ii)编码器冻结与完整微调，以及(iii)延迟SpO2标签的影响，以更好地与声学对齐并捕获生理上有意义的特征。结果表明，使用SpO2集成的微调相比没有适应的基线模型，持续改善了儿科OSA检测。这些发现证明了迁移学习用于儿童家庭OSA筛查的可行性，并展示了其在早期诊断中的潜在临床价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Paediatric obstructive sleep apnoea (OSA) is clinically significant yetdifficult to diagnose, as children poorly tolerate sensor-basedpolysomnography. Acoustic monitoring provides a non-invasive alternative forhome-based OSA screening, but limited paediatric data hinders the developmentof robust deep learning approaches. This paper proposes a transfer learningframework that adapts acoustic models pretrained on adult sleep data topaediatric OSA detection, incorporating SpO2-based desaturation patterns toenhance model training. Using a large adult sleep dataset (157 nights) and asmaller paediatric dataset (15 nights), we systematically evaluate (i) single-versus multi-task learning, (ii) encoder freezing versus full fine-tuning, and(iii) the impact of delaying SpO2 labels to better align them with theacoustics and capture physiologically meaningful features. Results show thatfine-tuning with SpO2 integration consistently improves paediatric OSAdetection compared with baseline models without adaptation. These findingsdemonstrate the feasibility of transfer learning for home-based OSA screeningin children and offer its potential clinical value for early diagnosis.</description>
      <author>example@mail.com (Chaoyue Niu, Veronica Rowe, Guy J. Brown, Heather Elphick, Heather Kenyon, Lowri Thomas, Sam Johnson, Ning Ma)</author>
      <guid isPermaLink="false">2509.15008v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Learning Informed Prior Distributions with Normalizing Flows for Bayesian Analysis</title>
      <link>http://arxiv.org/abs/2509.14911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究归一化流(NF)模型作为贝叶斯推理中灵活先验的应用，结合MCMC采样，通过在先前分析的后验上训练，在后续任务中作为信息性先验，捕捉非平凡分布和相关性。&lt;h4&gt;背景&lt;/h4&gt;在贝叶斯推理中，需要有效的先验分布来表示参数的不确定性，传统的简单先验可能无法捕捉复杂的参数关系和分布特征。&lt;h4&gt;目的&lt;/h4&gt;探索归一化流模型作为贝叶斯推理中灵活先验的可行性，评估其在顺序贝叶斯分析中的效果，并比较不同的训练策略和采样算法。&lt;h4&gt;方法&lt;/h4&gt;使用归一化流模型作为先验，结合MCMC采样方法，比较不同的训练策略(如基于KL散度和无监督学习)和损失函数，并比较pocoMCMC与标准emcee采样器的性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于KL散度和无监督学习的训练策略能最准确地重现参考分布；在目标分布为单峰的情况下，基于NF的先验的MCMC能很好地重现一次性联合推理结果；但在多峰性或数据张力情况下可能会出现失真；先进的采样算法对探索后验空间至关重要。&lt;h4&gt;结论&lt;/h4&gt;基于NF的先验在高维参数空间中顺序贝叶斯推理是一种实用且高效的工具，但在多阶段贝叶斯推理中需要谨慎，特别是在处理多峰分布或数据张力时。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了归一化流(NF)模型作为贝叶斯推理中灵活先验的使用，与马尔可夫链蒙特卡洛(MCMC)采样结合。这些模型在先前分析的后验上进行训练，可以在后续推理任务中用作信息性先验，捕捉非平凡分布和相关性。我们比较了不同的训练策略和损失函数，发现基于KL散度和无监督学习的训练能最准确地重现参考分布。在顺序贝叶斯工作流中应用时，基于NF的先验的MCMC能很好地重现一次性联合推理的结果，前提是目标分布是单峰的。在明显的多峰性或数据张力情况下，可能会出现失真，这强调了在多阶段贝叶斯推理中需要谨慎。pocoMCMC采样器与标准emcee采样器的进一步比较表明，先进且稳健的算法对于探索后验空间的重要性。总体而言，我们的研究结果表明，基于NF的先验在高维参数空间中顺序贝叶斯推理是一种实用且高效的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate the use of normalizing flow (NF) models as flexible priors inBayesian inference with Markov Chain Monte Carlo (MCMC) sampling. Trained onposteriors from previous analyses, these models can be used as informativepriors, capturing non-trivial distributions and correlations, in subsequentinference tasks. We compare different training strategies and loss functions,finding that training based on Kullback-Leibler (KL) divergence andunsupervised learning consistently yield the most accurate reproductions ofreference distributions. Applied in sequential Bayesian workflows, MCMC withthe NF-based priors reproduces the results of one-shot joint inferences well,provided the target distributions are unimodal. In cases with pronouncedmulti-modality or dataset tension, distortions may arise, underscoring the needfor caution in multi-stage Bayesian inference. A comparison between the pocoMCMCMC sampler and the standard emcee sampler further demonstrates the importanceof advanced and robust algorithms for exploring the posterior space. Overall,our results establish NF-based priors as a practical and efficient tool forsequential Bayesian inference in high-dimensional parameter spaces.</description>
      <author>example@mail.com (Hendrik Roch, Chun Shen)</author>
      <guid isPermaLink="false">2509.14911v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>HAM: Hierarchical Adapter Merging for Scalable Continual Learning</title>
      <link>http://arxiv.org/abs/2509.13211v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了HAM（分层适配器合并）框架，通过动态合并不同任务的适配器来解决持续学习中的灾难性遗忘问题。HAM使用层次化的方法组织适配器，通过修剪、缩放和合并相关任务的适配器来促进迁移学习，实验证明其在多个视觉基准测试上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;持续学习是人类认知的基本能力，但对当前深度学习模型构成重大挑战。主要问题是新知识会干扰已学习信息，导致模型遗忘旧知识（灾难性遗忘）。大型预训练模型可以部分缓解遗忘，但在面对新数据分布时仍有困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效扩展到动态学习场景和长任务序列的持续学习方法，管理比基线方法更多的任务，同时提高效率。&lt;h4&gt;方法&lt;/h4&gt;引入HAM框架，在训练过程中动态合并来自不同任务的适配器。维护一组固定的组层次化整合新适配器，对每个任务训练低秩适配器和重要性标量，基于适配器相似度动态分组任务，并在每个组内修剪、缩放和合并适配器以促进迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个视觉基准测试上的广泛实验表明，HAM显著优于最先进的方法，随着任务数量的增加，HAM的优势更加明显。&lt;h4&gt;结论&lt;/h4&gt;HAM有效地解决了持续学习中的灾难性遗忘问题，能够处理比竞争基线更多的任务，同时提高效率。&lt;h4&gt;翻译&lt;/h4&gt;持续学习是人类认知的基本能力，然而它对当前的深度学习模型构成了重大挑战。主要问题是新知识可能会干扰已学习的信息，导致模型为了新知识而遗忘旧知识，这种现象被称为灾难性遗忘。尽管大型预训练模型可以通过利用其现有知识和过参数化部分缓解遗忘，但当面对新的数据分布时，它们仍然存在困难。参数高效微调（PEFT）方法（如LoRA）能够有效地适应新知识。然而，在扩展到动态学习场景和长任务序列时，它们仍然面临挑战，因为每个任务维护一个适配器会增加复杂性并增加干扰的可能性。在本文中，我们引入了分层适配器合并（HAM），一种在训练过程中动态合并来自不同任务的适配器的新框架。这种方法使HAM能够有效地扩展，允许它以更高的效率管理比竞争基线更多的任务。为此，HAM维护一组固定的组，这些组层次化地整合新的适配器。对于每个任务，HAM训练一个低秩适配器和一个重要性标量，然后基于适配器相似度动态分组任务。在每个组内，适配器被修剪、缩放和合并，促进相关任务之间的迁移学习。在三个视觉基准测试上的广泛实验表明，HAM显著优于最先进的方法，特别是在任务数量增加的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning is an essential capability of human cognition, yet itposes significant challenges for current deep learning models. The primaryissue is that new knowledge can interfere with previously learned information,causing the model to forget earlier knowledge in favor of the new, a phenomenonknown as catastrophic forgetting. Although large pre-trained models canpartially mitigate forgetting by leveraging their existing knowledge andover-parameterization, they often struggle when confronted with novel datadistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,enable efficient adaptation to new knowledge. However, they still facechallenges in scaling to dynamic learning scenarios and long sequences oftasks, as maintaining one adapter per task introduces complexity and increasesthe potential for interference. In this paper, we introduce HierarchicalAdapters Merging (HAM), a novel framework that dynamically combines adaptersfrom different tasks during training. This approach enables HAM to scaleeffectively, allowing it to manage more tasks than competing baselines withimproved efficiency. To achieve this, HAM maintains a fixed set of groups thathierarchically consolidate new adapters. For each task, HAM trains a low-rankadapter along with an importance scalar, then dynamically groups tasks based onadapter similarity. Within each group, adapters are pruned, scaled and merge,facilitating transfer learning between related tasks. Extensive experiments onthree vision benchmarks show that HAM significantly outperformsstate-of-the-art methods, particularly as the number of tasks increases.</description>
      <author>example@mail.com (Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2509.13211v3</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems</title>
      <link>http://arxiv.org/abs/2509.15213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;扩展现实(XR)应用越来越多地集成大型语言模型(LLMs)来增强用户体验、场景理解，甚至生成可执行的XR内容，这些应用通常被称为'AI眼镜'。尽管有这些潜在的好处，集成的XR-LLM管道使XR应用容易受到新型攻击。&lt;h4&gt;目的&lt;/h4&gt;分析文献和实践中的LLM集成XR系统，从系统角度沿不同维度对其进行分类，识别通用威胁模型，并展示一系列概念验证攻击。&lt;h4&gt;方法&lt;/h4&gt;对文献和实践中的LLM集成XR系统进行分析和分类，在多个XR平台上实施概念验证攻击，这些平台使用各种LLM模型（包括Meta Quest 3、Meta Ray-Ban、Android和运行Llama和GPT模型的Microsoft HoloLens 2）。&lt;h4&gt;主要发现&lt;/h4&gt;尽管这些平台以不同方式实现LLM集成，但它们共享漏洞，攻击者可以修改围绕合法LLM查询的公共上下文，导致用户收到错误的视觉或听觉反馈，这可能危及用户安全或隐私，造成混乱或其他有害影响。&lt;h4&gt;结论&lt;/h4&gt;讨论了防御策略和开发人员的最佳实践，提出了一个初始防御原型，并呼吁社区开发新的保护机制来减轻这些风险。&lt;h4&gt;翻译&lt;/h4&gt;扩展现实(XR)应用越来越多地集成大型语言模型(LLMs)来增强用户体验、场景理解，甚至生成可执行的XR内容，这些应用通常被称为'AI眼镜'。尽管有这些潜在的好处，集成的XR-LLM管道使XR应用容易受到新型攻击。在本文中，我们从系统角度分析了文献和实践中的LLM集成XR系统，并沿不同维度对其进行分类。基于这种分类，我们识别出一个通用威胁模型，并在多个采用各种LLM模型的XR平台上展示了一系列概念验证攻击（包括Meta Quest 3、Meta Ray-Ban、Android和运行Llama和GPT模型的Microsoft HoloLens 2）。尽管这些平台各自以不同方式实现LLM集成，但它们共享漏洞，攻击者可以修改合法LLM查询周围的公共上下文，导致用户收到错误的视觉或听觉反馈，从而危及他们的安全或隐私，制造混乱或其他有害影响。为了防御这些威胁，我们讨论了防御策略和开发人员的最佳实践，包括一个初始防御原型，并呼吁社区开发新的保护机制来减轻这些风险。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extended reality (XR) applications increasingly integrate Large LanguageModels (LLMs) to enhance user experience, scene understanding, and evengenerate executable XR content, and are often called "AI glasses". Despitethese potential benefits, the integrated XR-LLM pipeline makes XR applicationsvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XRsystems in the literature and in practice and categorize them along differentdimensions from a systems perspective. Building on this categorization, weidentify a common threat model and demonstrate a series of proof-of-conceptattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).Although these platforms each implement LLM integration differently, they sharevulnerabilities where an attacker can modify the public context surrounding alegitimate LLM query, resulting in erroneous visual or auditory feedback tousers, thus compromising their safety or privacy, sowing confusion, or otherharmful effects. To defend against these threats, we discuss mitigationstrategies and best practices for developers, including an initial defenseprototype, and call on the community to develop new protection mechanisms tomitigate these risks.</description>
      <author>example@mail.com (Yicheng Zhang, Zijian Huang, Sophie Chen, Erfan Shayegani, Jiasi Chen, Nael Abu-Ghazaleh)</author>
      <guid isPermaLink="false">2509.15213v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>SPATIALGEN: Layout-guided 3D Indoor Scene Generation</title>
      <link>http://arxiv.org/abs/2509.14981v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3D scene ggeneration; diffusion model; Scene reconstruction and  understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过创建大型合成数据集并提出SpatialGen模型，解决了室内3D场景生成中的关键挑战，实现了高质量、语义一致的3D室内场景生成，结果优于先前方法，并已开源数据和模型以促进社区发展。&lt;h4&gt;背景&lt;/h4&gt;创建高保真室内环境3D模型对设计、虚拟现实和机器人应用至关重要，但手动3D建模耗时且劳动密集。现有生成式AI方法在平衡视觉质量、多样性、语义一致性和用户控制方面面临挑战，主要瓶颈是缺乏针对此任务的大规模高质量数据集。&lt;h4&gt;目的&lt;/h4&gt;引入一个全面的合成数据集以解决缺乏大规模高质量数据集的问题，并提出SpatialGen模型用于生成真实且语义一致的3D室内场景。&lt;h4&gt;方法&lt;/h4&gt;创建包含12,328个结构化注释场景、57,440个房间和470万张逼真2D渲染的合成数据集；提出SpatialGen多视图多模态扩散模型，基于3D布局和参考图像从任意视点合成外观、几何和语义信息，同时保持跨模态的空间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;SpatialGen在实验中始终比以前的方法产生更好的结果，能够生成高质量、语义一致的3D室内场景。&lt;h4&gt;结论&lt;/h4&gt;SpatialGen模型有效解决了室内3D场景生成中的关键挑战，开源数据和模型将有助于推动室内场景理解和生成领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;创建高保真室内环境的3D模型对于设计、虚拟现实和机器人应用至关重要。然而，手动3D建模仍然耗时且劳动密集。尽管生成式AI的最新进展已实现自动化场景合成，但现有方法在平衡视觉质量、多样性和用户控制方面常面临挑战。主要瓶颈是缺乏针对此任务的大规模高质量数据集。为解决这一差距，我们引入了一个全面的合成数据集，包含12,328个结构化注释场景、57,440个房间和470万张逼真2D渲染。利用此数据集，我们提出了SpatialGen，一种新颖的多视图多模态扩散模型，可生成真实且语义一致的3D室内场景。给定3D布局和参考图像（从文本提示派生），我们的模型从任意视点合成外观（彩色图像）、几何（场景坐标图）和语义（语义分割图），同时保持跨模态的空间一致性。在我们的实验中，SpatialGen始终比以前的方法产生更好的结果。我们开源了数据和模型，以赋能社区并推动室内场景理解和生成领域的发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决生成高质量、高保真度的3D室内场景模型的问题，特别是在平衡视觉质量、多样性、语义一致性和用户控制方面的挑战。这个问题在现实中非常重要，因为室内3D模型对室内设计、虚拟现实和机器人应用至关重要，而手动3D建模既耗时又费力。现有生成AI方法难以同时满足这些要求，主要瓶颈是缺乏大规模、高质量的数据集。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考：分数蒸馏方法利用2D预训练模型创建3D内容但存在视觉伪影，全景作为代理的方法无法外推到新视角。作者借鉴了多视角扩散模型、3D语义布局引导、ControlNet和3D高斯溅射等现有技术，但创新性地将它们结合，并创建了一个大规模数据集来解决数据稀缺问题。设计思路是通过布局条件引导生成过程，确保跨视角和跨模态的一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D语义布局作为先验条件，通过多视角多模态扩散模型同时生成RGB图像、场景坐标图和语义分割图，确保多模态一致性。整体流程包括：1)将3D布局转换为视图特定表示；2)使用布局引导的交替注意力机制(跨视图和跨模态)；3)采用迭代密集视角生成策略；4)最后通过3D高斯溅射优化重建显式辐射场，实现自由视角渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的大规模室内场景数据集(12,328个场景，57,440个房间，470万张渲染图)；2)SPATIALGEN框架，结合布局引导的多视角多模态扩散模型；3)场景坐标图VAE(SCM-VAE)专门处理几何信息；4)迭代密集视角生成策略。相比之前工作，SPATIALGEN解决了视觉质量、多样性、语义一致性和用户控制的平衡问题，能生成任意视角的图像而非仅固定位置全景，同时确保多模态一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPATIALGEN通过引入大规模室内场景数据集和基于布局引导的多视角多模态扩散模型，实现了高质量、语义一致的3D室内场景生成，显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Creating high-fidelity 3D models of indoor environments is essential forapplications in design, virtual reality, and robotics. However, manual 3Dmodeling remains time-consuming and labor-intensive. While recent advances ingenerative AI have enabled automated scene synthesis, existing methods oftenface challenges in balancing visual quality, diversity, semantic consistency,and user control. A major bottleneck is the lack of a large-scale, high-qualitydataset tailored to this task. To address this gap, we introduce acomprehensive synthetic dataset, featuring 12,328 structured annotated sceneswith 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging thisdataset, we present SpatialGen, a novel multi-view multi-modal diffusion modelthat generates realistic and semantically consistent 3D indoor scenes. Given a3D layout and a reference image (derived from a text prompt), our modelsynthesizes appearance (color image), geometry (scene coordinate map), andsemantic (semantic segmentation map) from arbitrary viewpoints, whilepreserving spatial consistency across modalities. SpatialGen consistentlygenerates superior results to previous methods in our experiments. We areopen-sourcing our data and models to empower the community and advance thefield of indoor scene understanding and generation.</description>
      <author>example@mail.com (Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan)</author>
      <guid isPermaLink="false">2509.14981v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Human Interaction for Collaborative Semantic SLAM using Extended Reality</title>
      <link>http://arxiv.org/abs/2509.14949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HICS-SLAM是一种人机协同的语义SLAM框架，通过共享扩展现实环境实现实时协作，让人类操作员能够直接与机器人的3D场景图交互并添加高级语义概念，显著提升了地图构建的准确性、精度和语义完整性。&lt;h4&gt;背景&lt;/h4&gt;语义SLAM系统通过为机器人地图添加结构和语义信息，使机器人能够在复杂环境中更有效地工作。然而，这些系统在处理遮挡、数据不完整或几何结构模糊等真实场景时存在困难，因为它们无法充分利用人类自然应用的高级空间和语义知识。&lt;h4&gt;目的&lt;/h4&gt;引入HICS-SLAM，一种人机协同的语义SLAM框架，利用共享的扩展现实环境实现实时协作，以克服传统语义SLAM系统的局限性。&lt;h4&gt;方法&lt;/h4&gt;该系统允许人类操作员直接与机器人的3D场景图进行交互和可视化，并在地图构建过程中添加高级语义概念（如房间或结构实体）。同时，提出了一种基于图的语义融合方法，将这些人类干预与机器人感知相结合，实现可扩展的协作以增强态势感知能力。&lt;h4&gt;主要发现&lt;/h4&gt;在真实建筑工地数据集上的实验评估表明，与自动化基线相比，该方法在房间检测准确性、地图精度和语义完整性方面都有显著改进。&lt;h4&gt;结论&lt;/h4&gt;HICS-SLAM方法的有效性及其未来扩展的潜力得到了验证，为人机协作语义地图构建提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;语义SLAM（同时定位与地图构建）系统通过为机器人地图添加结构和语义信息，使机器人能够在复杂环境中更有效地工作。然而，这些系统在处理遮挡、数据不完整或几何结构模糊等真实场景时存在困难，因为它们无法充分利用人类自然应用的高级空间和语义知识。我们引入HICS-SLAM，一种人机协同的语义SLAM框架，利用共享的扩展现实环境实现实时协作。该系统允许人类操作员直接与机器人的3D场景图进行交互和可视化，并在地图构建过程中添加高级语义概念（如房间或结构实体）。我们提出了一种基于图的语义融合方法，将这些人类干预与机器人感知相结合，实现可扩展的协作以增强态势感知能力。在真实建筑工地数据集上的实验评估表明，与自动化基线相比，该方法在房间检测准确性、地图精度和语义完整性方面都有所改进，证明了该方法的有效性及其未来扩展的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决语义SLAM系统在现实场景中（如遮挡、数据不完整或几何模糊环境）表现不佳的问题，因为这些系统无法充分利用人类自然应用的高级空间和语义知识。这个问题在现实世界中非常重要，因为它影响着机器人搜索救援、工业装配和建筑工地等需要精确空间理解的应用场景，人类的空间认知能力可以弥补机器人感知系统的局限性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有Human-in-the-Loop策略的局限性，如HAC-SLAM缺乏语义集成，HSS-SLAM需要手动参数调整且对非专家不直观。他们设计了一个结合扩展现实(XR)接口与语义SLAM的框架，允许人类通过自然手势直接操作3D场景图。该方法借鉴了S-Graphs 2.0作为底层SLAM算法，参考了HAC-SLAM的轨迹修正方法和HSS-SLAM的语义表示方法，并利用了XR技术中的手势识别研究。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过共享的扩展现实环境实现人机协作的语义SLAM，让人类操作员直接与机器人的3D场景图交互并添加高级语义概念。整体流程为：机器人在物理环境中收集传感器数据；通过语义SLAM算法处理数据构建层次化场景图；人类通过混合现实设备查看交互式虚拟环境；人类使用手势添加或修正语义信息；这些干预通过图优化方法集成到SLAM后端；系统实时更新机器人的地图和定位信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 通过XR接口实现直观的人机干预和3D场景图操作；2) 在线集成人类语义知识到SLAM后端优化；3) 在真实世界建筑工地数据集上验证方法有效性。相比之前工作，HICS-SLAM与HAC-SLAM不同之处在于它将语义集成到场景图中；与HSS-SLAM不同在于它支持直观的高级语义编辑；与其他HitL方法不同在于它结合了直观的场景操作和语义意识，解决了现有方法要么在低级几何层面操作要么提供高级交互但不包含语义信息的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HICS-SLAM通过结合扩展现实接口与语义SLAM，实现了人类与机器人在实时环境映射中的协作，显著提高了地图的语义完整性和空间精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic SLAM (Simultaneous Localization and Mapping) systems enrich robotmaps with structural and semantic information, enabling robots to operate moreeffectively in complex environments. However, these systems struggle inreal-world scenarios with occlusions, incomplete data, or ambiguous geometries,as they cannot fully leverage the higher-level spatial and semantic knowledgehumans naturally apply. We introduce HICS-SLAM, a Human-in-the-Loop semanticSLAM framework that uses a shared extended reality environment for real-timecollaboration. The system allows human operators to directly interact with andvisualize the robot's 3D scene graph, and add high-level semantic concepts(e.g., rooms or structural entities) into the mapping process. We propose agraph-based semantic fusion methodology that integrates these humaninterventions with robot perception, enabling scalable collaboration forenhanced situational awareness. Experimental evaluations on real-worldconstruction site datasets demonstrate improvements in room detection accuracy,map precision, and semantic completeness compared to automated baselines,demonstrating both the effectiveness of the approach and its potential forfuture extensions.</description>
      <author>example@mail.com (Laura Ribeiro, Muhammad Shaheer, Miguel Fernandez-Cortizas, Ali Tourani, Holger Voos, Jose Luis Sanchez-Lopez)</author>
      <guid isPermaLink="false">2509.14949v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition</title>
      <link>http://arxiv.org/abs/2509.14619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为LSTC-MDA的统一框架，用于解决基于骨架的动作识别中的样本稀缺和时间依赖建模问题。&lt;h4&gt;背景&lt;/h4&gt;基于骨架的动作识别面临两个长期挑战：标记训练样本稀缺和难以建模短期和长期时间依赖关系。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一框架LSTC-MDA，同时改进时间建模和数据多样性，解决上述两个挑战。&lt;h4&gt;方法&lt;/h4&gt;1) 提出长短期时间卷积(LSTC)模块，具有并行的短期和长期分支；2) 使用学习的相似度权重对齐和融合这两个特征分支；3) 扩展联合混合数据增强(JMDA)，在输入级别添加加性Mixup；4) 将mixup操作限制在同一摄像头视图中，避免分布偏移。&lt;h4&gt;主要发现&lt;/h4&gt;消融研究确认每个组件都有贡献，LSTC-MDA达到了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;LSTC-MDA在多个数据集上取得了最先进的结果：NTU 60(X-Sub和X-View)上分别为94.1%和97.5%，NTU 120(X-Sub和X-Set)上分别为90.4%和92.0%，NW-UCLA上为97.2%。&lt;h4&gt;翻译&lt;/h4&gt;基于骨架的动作识别面临两个长期挑战：标记训练样本稀缺和难以建模短期和长期时间依赖关系。为解决这些问题，我们提出了一个统一框架LSTC-MDA，同时改进时间建模和数据多样性。我们引入了一种新颖的长短期时间卷积(LSTC)模块，具有并行的短期和长期分支，然后使用学习的相似度权重对这些特征分支进行自适应的对齐和融合，以保留传统步长为2的时间卷积所丢失的关键长期线索。我们还扩展了联合混合数据增强(JMDA)，在输入级别添加了加性Mixup，增加了训练样本的多样性，并将mixup操作限制在同一摄像头视图中，以避免分布偏移。消融研究确认每个组件都有贡献。LSTC-MDA取得了最先进的结果：在NTU 60(X-Sub和X-View)上分别为94.1%和97.5%，在NTU 120(X-Sub和X-Set)上分别为90.4%和92.0%，在NW-UCLA上为97.2%。代码：https://github.com/xiaobaoxia/LSTC-MDA。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skeleton-based action recognition faces two longstanding challenges: thescarcity of labeled training samples and difficulty modeling short- andlong-range temporal dependencies. To address these issues, we propose a unifiedframework, LSTC-MDA, which simultaneously improves temporal modeling and datadiversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC)module with parallel short- and long-term branches, these two feature branchesare then aligned and fused adaptively using learned similarity weights topreserve critical long-range cues lost by conventional stride-2 temporalconvolutions. We also extend Joint Mixing Data Augmentation (JMDA) with anAdditive Mixup at the input level, diversifying training samples andrestricting mixup operations to the same camera view to avoid distributionshifts. Ablation studies confirm each component contributes. LSTC-MDA achievesstate-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4%and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code:https://github.com/xiaobaoxia/LSTC-MDA.</description>
      <author>example@mail.com (Feng Ding, Haisheng Fu, Soroush Oraki, Jie Liang)</author>
      <guid isPermaLink="false">2509.14619v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Dense Video Understanding with Gated Residual Tokenization</title>
      <link>http://arxiv.org/abs/2509.14199v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Dense Video Understanding (DVU)方法和Gated Residual Tokenization (GRT)框架，通过减少标记时间和标记开销实现高FPS视频理解，并创建了首个针对密集时间推理的基准测试DIVE。&lt;h4&gt;背景&lt;/h4&gt;当前视频大语言模型和基准测试主要依赖低帧率采样，如均匀采样或关键帧选择，这会丢弃密集的时间信息。虽然这种折衷避免了标记每一帧的高成本，但无法处理如讲座理解等需要精确时间对齐的任务。&lt;h4&gt;目的&lt;/h4&gt;解决低帧率采样无法捕捉密集时间信息的问题，实现高效的高FPS视频理解，并创建相应的基准测试。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段框架GRT：1) 运动补偿门控标记化，使用像素级运动估计跳过静态区域；2) 语义场景内标记化合并，融合静态区域标记减少冗余。同时创建首个密集时间推理基准测试DIVE。&lt;h4&gt;主要发现&lt;/h4&gt;在DIVE基准测试上，GRT优于更大的VLLM基线模型，且性能随FPS正向扩展，证明了密集时间信息的重要性。&lt;h4&gt;结论&lt;/h4&gt;GRT框架能够实现高效、可扩展的高FPS视频理解，强调了密集时间信息对视频理解的关键作用。&lt;h4&gt;翻译&lt;/h4&gt;高时间分辨率对于捕捉视频理解中的细粒度细节至关重要。然而，当前视频大语言模型和基准测试主要依赖低帧率采样，如均匀采样或关键帧选择，这会丢弃密集的时间信息。这种折衷避免了标记每一帧的高成本，否则会导致冗余计算和随视频长度线性增长的标记数量。虽然这种折衷适用于缓慢变化的内容，但在讲座理解等任务中失败，因为这些任务中信息几乎出现在每一帧，需要精确的时间对齐。为解决这一差距，我们引入了Dense Video Understanding (DVU)，通过减少标记时间和标记开销来实现高FPS视频理解。现有基准测试也有限，因为其问答对侧重于粗粒度内容变化。因此，我们提出了DIVE (Dense Information Video Evaluation)，这是第一个为密集时间推理设计的基准测试。为了使DVU实用，我们提出了Gated Residual Tokenization (GRT)，一个两阶段框架：(1) 运动补偿门控标记化使用像素级运动估计在标记化过程中跳过静态区域，实现标记数量和计算量的次线性增长。(2) 语义场景内标记化合并融合场景内静态区域的标记，进一步减少冗余，同时保留动态语义。在DIVE上的实验表明，GRT优于更大的VLLM基线模型，且随FPS正向扩展。这些结果强调了密集时间信息的重要性，并证明了GRT能够实现高效、可扩展的高FPS视频理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High temporal resolution is essential for capturing fine-grained details invideo understanding. However, current video large language models (VLLMs) andbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling orkeyframe selection, discarding dense temporal information. This compromiseavoids the high cost of tokenizing every frame, which otherwise leads toredundant computation and linear token growth as video length increases. Whilethis trade-off works for slowly changing content, it fails for tasks likelecture comprehension, where information appears in nearly every frame andrequires precise temporal alignment. To address this gap, we introduce DenseVideo Understanding (DVU), which enables high-FPS video comprehension byreducing both tokenization time and token overhead. Existing benchmarks arealso limited, as their QA pairs focus on coarse content changes. We thereforepropose DIVE (Dense Information Video Evaluation), the first benchmark designedfor dense temporal reasoning. To make DVU practical, we present Gated ResidualTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-GatedTokenization uses pixel-level motion estimation to skip static regions duringtokenization, achieving sub-linear growth in token count and compute. (2)Semantic-Scene Intra-Tokenization Merging fuses tokens across static regionswithin a scene, further reducing redundancy while preserving dynamic semantics.Experiments on DIVE show that GRT outperforms larger VLLM baselines and scalespositively with FPS. These results highlight the importance of dense temporalinformation and demonstrate that GRT enables efficient, scalable high-FPS videounderstanding.</description>
      <author>example@mail.com (Haichao Zhang, Wenhao Chai, Shwai He, Ang Li, Yun Fu)</author>
      <guid isPermaLink="false">2509.14199v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>D4PM: A Dual-branch Driven Denoising Diffusion Probabilistic Model with Joint Posterior Diffusion Sampling for EEG Artifacts Removal</title>
      <link>http://arxiv.org/abs/2509.14302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为D4PM的双分支驱动的去噪扩散概率模型，用于统一处理多种伪影的脑电图信号去噪，解决了现有方法在时间建模和伪影差异处理方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;伪影去除对于准确分析和解释脑电图(EEG)信号至关重要。传统方法在伪影与EEG强相关或单通道数据时表现不佳。虽然基于扩散的生成模型最近在EEG去噪方面显示出潜力，但现有方法存在明显缺陷。&lt;h4&gt;目的&lt;/h4&gt;解决现有EEG去噪方法的两个主要限制：缺乏时间建模限制了可解释性，以及使用单伪影训练范式忽略了不同伪影之间的差异。&lt;h4&gt;方法&lt;/h4&gt;提出D4PM模型，采用双分支条件扩散架构隐式建模干净EEG和伪影的数据分布，并设计联合后验采样策略协同整合互补先验，实现高保真EEG重建。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集上的广泛实验表明，D4PM提供了优越的去噪效果，在EOG伪影去除方面取得了最新的最先进性能，优于所有公开可用的基线方法。&lt;h4&gt;结论&lt;/h4&gt;D4PM模型通过统一处理多种伪影类型并改进时间建模，显著提高了EEG信号去噪的质量和可解释性，为脑电图分析提供了更可靠的工具。&lt;h4&gt;翻译&lt;/h4&gt;伪影去除对于准确分析和解释脑电图(EEG)信号至关重要。传统方法在伪影与EEG强相关或单通道数据时表现不佳。最近基于扩散的生成模型的进展展示了EEG去噪的强大潜力，显著改善了细粒度噪声抑制并减少了过平滑。然而，现有方法面临两个主要限制：缺乏时间建模限制了可解释性，以及使用单伪影训练范式忽略了伪影间的差异。为解决这些问题，我们提出了D4PM，一种双分支驱动的去噪扩散概率模型，统一了多种伪影去除。我们引入了一种双分支条件扩散架构，隐式建模干净EEG和伪影的数据分布。还设计了一种联合后验采样策略，协同整合互补先验，实现高保真EEG重建。在两个公共数据集上的广泛实验表明，D4PM提供了优越的去噪效果。它在EOG伪影去除方面取得了最新的最先进性能，优于所有公开可用的基线方法。代码可在https://github.com/flysnow1024/D4PM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artifact removal is critical for accurate analysis and interpretation ofElectroencephalogram (EEG) signals. Traditional methods perform poorly withstrong artifact-EEG correlations or single-channel data. Recent advances indiffusion-based generative models have demonstrated strong potential for EEGdenoising, notably improving fine-grained noise suppression and reducingover-smoothing. However, existing methods face two main limitations: lack oftemporal modeling limits interpretability and the use of single-artifacttraining paradigms ignore inter-artifact differences. To address these issues,we propose D4PM, a dual-branch driven denoising diffusion probabilistic modelthat unifies multi-type artifact removal. We introduce a dual-branchconditional diffusion architecture to implicitly model the data distribution ofclean EEG and artifacts. A joint posterior sampling strategy is furtherdesigned to collaboratively integrate complementary priors for high-fidelityEEG reconstruction. Extensive experiments on two public datasets show that D4PMdelivers superior denoising. It achieves new state-of-the-art performance inEOG artifact removal, outperforming all publicly available baselines. The codeis available at https://github.com/flysnow1024/D4PM.</description>
      <author>example@mail.com (Feixue Shao, Xueyu Liu, Yongfei Wu, Jianbo Lu, Guiying Yan, Weihua Yang)</author>
      <guid isPermaLink="false">2509.14302v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings</title>
      <link>http://arxiv.org/abs/2509.12938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种绕过可微分渲染的新方法，通过预分解的对象级高斯和多视图CLIP特征聚合来克服3D高斯溅射在场景理解方面的局限性，实现了准确的开放词汇对象检索和无缝任务适应。&lt;h4&gt;背景&lt;/h4&gt;3D高斯溅射在新型视图合成方面取得了显著进展，能够实现实时照片级真实感渲染，但其内在模糊性对3D场景理解提出了挑战，限制了在AR/VR和机器人技术中的应用。现有方法通过2D基础模型蒸馏学习语义存在根本性限制，alpha blending会使不同对象的语义平均化，导致无法实现3D级别的理解。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服3D高斯溅射场景理解限制的方法，实现准确的3D开放词汇对象提取和语义理解，扩展其在AR/VR和机器人技术中的应用范围。&lt;h4&gt;方法&lt;/h4&gt;提出范式转变的替代方案，完全绕过可微分渲染处理语义。利用预分解的对象级高斯，通过多视图CLIP特征聚合表示每个对象，创建全面的'嵌入袋'来全面描述对象。实现两种功能：1) 通过比较文本查询与对象级嵌入进行开放词汇对象检索；2) 将对象ID传播到像素用于2D分割或传播到高斯用于3D提取。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明该方法有效克服了3D开放词汇对象提取的挑战，同时在2D开放词汇分割方面与最先进性能相当，确保了最小程度的性能妥协。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过改变处理语义的方式，成功解决了3D高斯溅射在场景理解方面的局限性，为AR/VR和机器人技术等领域的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;新型视图合成在3D高斯溅射(3DGS)的推动下取得了显著进展，实现了实时照片级真实感渲染。然而，高斯溅射的内在模糊性对3D场景理解提出了挑战，限制了其在AR/VR和机器人技术中的更广泛应用。虽然最近的工作尝试通过2D基础模型蒸馏来学习语义，但它们继承了根本性限制：alpha blending使不同对象的语义平均化，导致无法实现3D级别的理解。我们提出了一种范式转变的替代方案，完全绕过可微分渲染来处理语义。我们的关键见解是利用预分解的对象级高斯，并通过多视图CLIP特征聚合来表示每个对象，创建全面的'嵌入袋'，全面描述对象。这实现了：(1) 通过将文本查询与对象级(而非高斯级)嵌入进行比较，实现准确的开放词汇对象检索，以及(2) 无缝任务适应：将对象ID传播到像素用于2D分割或传播到高斯用于3D提取。实验证明，我们的方法有效克服了3D开放词汇对象提取的挑战，同时在2D开放词汇分割方面与最先进性能相当，确保最小程度的妥协。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D高斯溅射(3DGS)在场景理解方面的局限性。虽然3DGS能实现实时照片级渲染，但其内在模糊性导致难以准确识别和分割场景中的个体对象，这限制了在AR/VR、机器人技术等需要精确场景理解的应用中的使用。这个问题很重要，因为当前3D渲染技术虽然视觉效果逼真，但缺乏对场景内容的语义理解能力，无法支持需要精确对象识别和操作的高级应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有方法(如LangSplat、LeGaussians等)的局限性出发思考，这些方法通过可微分渲染将2D基础模型语义注入3D高斯，但存在噪声和不一致问题。作者借鉴了Gaussian Grouping方法，该方法能将3D高斯分组为不同对象并分配身份编码。作者还利用了CLIP模型强大的视觉-语言关联能力。基于这些观察，作者设计了一种绕过可微分渲染的新方法，转而使用预分解的对象级高斯，并通过多视图CLIP特征聚合创建'嵌入包'来表示每个对象。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：不再通过可微分渲染将语义注入单个高斯，而是将3D高斯预先分组为不同对象，为每个对象创建'嵌入包'，在对象级别执行开放词汇检索，并将对象ID传播到下游任务。整体流程：1)使用SAM模型从多视图图像生成2D掩码并确保跨视图一致性；2)使用Gaussian Grouping将3D高斯分组为不同对象并分配唯一ID；3)为每个对象提取多视图CLIP特征并聚合形成'嵌入包'；4)根据文本查询计算对象相关性分数并识别最相关对象ID；5)将对象ID用于3D对象选择(过滤高斯)或2D分割(生成像素级掩码)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)对象级语义编码，将3D高斯场景表示为每个对象的多元CLIP嵌入'包'，避免有问题的语义可微分渲染；2)开放词汇3D搜索，通过预分组场景实现直接对象级检索，解决高斯级语义不一致问题；3)任务无关的语义传播，将对象ID无缝传播到2D分割和3D提取，无需重新训练。与之前工作的不同：1)完全绕过可微分渲染进行语义学习，避免alpha混合导致的语义稀释；2)在对象级别而非高斯级别操作，提供更清洁一致的表示；3)保留多视图嵌入细节而非简单平均，捕获特定视角的独特特征；4)同时处理2D和3D任务，提供更全面场景理解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出通过高斯分组和多视图CLIP嵌入包实现开放词汇3D场景理解，能准确进行3D对象提取和2D分割，同时克服了现有方法中的语义平均化和噪声问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Novel view synthesis has seen significant advancements with 3D GaussianSplatting (3DGS), enabling real-time photorealistic rendering. However, theinherent fuzziness of Gaussian Splatting presents challenges for 3D sceneunderstanding, restricting its broader applications in AR/VR and robotics.While recent works attempt to learn semantics via 2D foundation modeldistillation, they inherit fundamental limitations: alpha blending averagessemantics across objects, making 3D-level understanding impossible. We proposea paradigm-shifting alternative that bypasses differentiable rendering forsemantics entirely. Our key insight is to leverage predecomposed object-levelGaussians and represent each object through multiview CLIP feature aggregation,creating comprehensive "bags of embeddings" that holistically describe objects.This allows: (1) accurate open-vocabulary object retrieval by comparing textqueries to object-level (not Gaussian-level) embeddings, and (2) seamless taskadaptation: propagating object IDs to pixels for 2D segmentation or toGaussians for 3D extraction. Experiments demonstrate that our methodeffectively overcomes the challenges of 3D open-vocabulary object extractionwhile remaining comparable to state-of-the-art performance in 2Dopen-vocabulary segmentation, ensuring minimal compromise.</description>
      <author>example@mail.com (Abdalla Arafa, Didier Stricker)</author>
      <guid isPermaLink="false">2509.12938v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
  <item>
      <title>Semantic 3D Reconstructions with SLAM for Central Airway Obstruction</title>
      <link>http://arxiv.org/abs/2509.13541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的流程，通过结合DROID-SLAM与分割模型，利用单目内窥镜视频实现中央气道的实时、语义感知三维重建，为自主机器人干预提供了有希望的进展。&lt;h4&gt;背景&lt;/h4&gt;中央气道阻塞(CAO)是一种发病率不断增加的危及生命的疾病，由气道内外的肿瘤引起。传统治疗方法如支气管镜和电凝术虽可完全移除肿瘤，但并发症风险高。最近的机器人干预技术降低了风险，结合场景理解和映射还可能实现自动化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够使用单目内窥镜视频进行中央气道实时、语义感知三维重建的新颖流程，为机器人干预提供支持。&lt;h4&gt;方法&lt;/h4&gt;结合DROID-SLAM与一个经过训练用于识别阻塞组织的分割模型。SLAM模块实时重建气道的三维几何结构，分割掩码指导重建点云内阻塞区域的标注。使用离体模型评估重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;真实CT扫描与3D重建之间具有高度相似性(0.62 mm Chamfer距离)。系统通过集成分割到SLAM工作流中，能实时生成突出显示临床相关区域的标注3D地图。该流程比以往工作更快完成重建，更准确地反映手术场景。&lt;h4&gt;结论&lt;/h4&gt;据所知，这是第一个将语义分割与实时单目SLAM集成用于内窥镜CAO场景的工作。该框架是模块化的，可泛化到其他解剖结构或程序，只需少量改动，为自主机器人干预提供了有希望的进展。&lt;h4&gt;翻译&lt;/h4&gt;中央气道阻塞(CAO)是一种发病率不断增加的危及生命的疾病，由气道内外的肿瘤引起。传统治疗方法如支气管镜和电凝术可用于完全移除肿瘤；然而，这些方法存在高并发症风险。最近的进展允许风险更低的机器人干预。机器人干预与场景理解和映射的结合也为自动化开辟了可能性。我们提出了一种新颖的流程，能够使用单目内窥镜视频进行中央气道的实时、语义感知三维重建。我们的方法将DROID-SLAM与一个用于识别阻塞组织的分割模型相结合。SLAM模块实时重建气道的三维几何结构，而分割掩码指导重建点云内阻塞区域的标注。为了验证我们的流程，我们使用离体模型评估重建质量。定性和定量结果显示真实CT扫描与3D重建之间具有高度相似性(0.62 mm Chamfer距离)。通过将分割直接集成到SLAM工作流中，我们的系统能实时生成突出显示临床相关区域的标注3D地图。该流程的高速能力比以往工作能更快完成重建，更准确地反映手术场景。据我们所知，这是第一个将语义分割与实时单目SLAM集成用于内窥镜CAO场景的工作。我们的框架是模块化的，可以泛化到其他解剖结构或程序，只需少量改动，为自主机器人干预提供了有希望的进展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决中心气道阻塞（CAO）的实时3D重建和场景理解问题。CAO是一种危及生命的疾病，发病率正在增加，由气道内外的肿瘤引起。传统治疗方法风险高，而机器人干预需要结合场景理解才能实现自动化。现有的3D重建方法耗时太长，无法在手术过程中实时更新，因此需要一种能够快速重建气道3D结构并识别阻塞区域的方法，以支持更安全、更精确的自动化手术操作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到CAO治疗的挑战性和机器人干预的潜力。他们之前的工作已实现实时分割但3D重建使用耗时的SfM方法，因此转向SLAM算法以实现实时重建。他们借鉴了DROID-SLAM作为3D重建框架，采用U-Net和SAM2构建分割模型，使用类似前人的phantom制作方法模拟临床场景。整体设计思路是将实时3D重建与语义分割结合，创建带有标注的3D气道地图，以支持下游自动化任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将实时3D重建（通过SLAM）与语义分割相结合，创建带有临床相关信息标注的3D气道地图。整体流程包括：1)使用羊心和鸡胸肉制作的phantom模拟CAO并收集内窥镜视频；2)训练U-Net/SAM2分割模型识别阻塞组织；3)使用DROID-SLAM处理单目内窥镜视频流，实时创建3D点云；4)将分割掩码集成到重建流程中，识别和标注阻塞区域；5)通过配准CT扫描评估重建质量和分割精度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将语义分割与实时单目SLAM结合应用于内窥镜CAO场景；创建实时、语义丰富的3D气道重建管道；实现比之前工作更快的重建速度；开发模块化框架可扩展到其他场景。相比之前工作，新方法用SLAM替代了耗时的SfM，将分割直接集成到SLAM工作流中，产生的3D重建带有语义标注，处理速度更快（每帧0.31秒），能更准确地反映手术场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的管道，通过结合实时单目SLAM与语义分割，首次实现了中心气道阻塞场景下的实时、语义丰富的3D重建，为自动化机器人手术干预提供了关键技术支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Central airway obstruction (CAO) is a life-threatening condition withincreasing incidence, caused by tumors in and outside of the airway.Traditional treatment methods such as bronchoscopy and electrocautery can beused to remove the tumor completely; however, these methods carry a high riskof complications. Recent advances allow robotic interventions with lesser risk.The combination of robot interventions with scene understanding and mappingalso opens up the possibilities for automation. We present a novel pipelinethat enables real-time, semantically informed 3D reconstructions of the centralairway using monocular endoscopic video.  Our approach combines DROID-SLAM with a segmentation model trained toidentify obstructive tissues. The SLAM module reconstructs the 3D geometry ofthe airway in real time, while the segmentation masks guide the annotation ofobstruction regions within the reconstructed point cloud. To validate ourpipeline, we evaluate the reconstruction quality using ex vivo models.  Qualitative and quantitative results show high similarity between groundtruth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). Byintegrating segmentation directly into the SLAM workflow, our system producesannotated 3D maps that highlight clinically relevant regions in real time.High-speed capabilities of the pipeline allows quicker reconstructions comparedto previous work, reflecting the surgical scene more accurately.  To the best of our knowledge, this is the first work to integrate semanticsegmentation with real-time monocular SLAM for endoscopic CAO scenarios. Ourframework is modular and can generalize to other anatomies or procedures withminimal changes, offering a promising step toward autonomous roboticinterventions.</description>
      <author>example@mail.com (Ayberk Acar, Fangjie Li, Hao Li, Lidia Al-Zogbi, Kanyifeechukwu Jane Oguine, Susheela Sharma Stern, Jesse F. d'Almeida, Robert J. Webster III, Ipek Oguz, Jie Ying Wu)</author>
      <guid isPermaLink="false">2509.13541v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors</title>
      <link>http://arxiv.org/abs/2509.13525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ColonCrafter，一个基于扩散的深度估计模型，用于从单目结肠镜视频中生成时间上一致的深度图，解决了现有内窥镜深度估计模型在视频序列中时间一致性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;结肠镜检查中的三维场景理解面临重大挑战，需要自动化的方法进行准确的深度估计。然而，现有的内窥镜深度估计模型在视频序列中的时间一致性方面存在问题，限制了它们在三维重建中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从单目结肠镜视频中生成时间上一致的深度图的模型，以克服现有方法的局限性，并支持三维重建等临床应用。&lt;h4&gt;方法&lt;/h4&gt;作者提出了ColonCrafter，一个基于扩散的深度估计模型。该方法从合成的结肠镜序列中学习强大的几何先验，生成时间上一致的深度图。同时引入了一种风格转换技术，在保持几何结构的同时，将真实的临床视频调整为匹配合成的训练域。&lt;h4&gt;主要发现&lt;/h4&gt;ColonCrafter在C3VD数据集上实现了最先进的零样本性能，优于通用方法和专门针对内窥镜的方法。该模型能够生成时间上一致的深度图，并支持临床相关的应用，包括三维点云生成和表面覆盖率评估。&lt;h4&gt;结论&lt;/h4&gt;尽管完整的轨迹三维重建仍然是一个挑战，但ColonCrafter在临床应用方面展示了潜力，特别是在三维点云生成和表面覆盖率评估方面。&lt;h4&gt;翻译&lt;/h4&gt;结肠镜检查中的三维场景理解面临着重大挑战，需要自动化的方法进行准确的深度估计。然而，现有的内窥镜深度估计模型在视频序列中的时间一致性方面存在问题，限制了它们在三维重建中的应用。我们提出了ColonCrafter，一个基于扩散的深度估计模型，可以从单目结肠镜视频中生成时间上一致的深度图。我们的方法从合成的结肠镜序列中学习强大的几何先验，以生成时间上一致的深度图。我们还引入了一种风格转换技术，在保持几何结构的同时，将真实的临床视频调整为匹配我们的合成训练域。ColonCrafter在C3VD数据集上实现了最先进的零样本性能，优于通用方法和专门针对内窥镜的方法。尽管完整的轨迹三维重建仍然是一个挑战，但我们展示了ColonCrafter的临床相关应用，包括三维点云生成和表面覆盖率评估。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决结肠镜视频中的深度估计问题，特别是实现时间一致的深度估计以支持结肠三维重建。这个问题非常重要，因为结直肠癌是美国癌症相关死亡的主要原因，结肠镜检查是筛查金标准，但临床实践中存在结肠皱褶后可视化不良导致病变漏诊率高、难以重新定位病变、以及难以准确测量息肉大小等挑战。结肠本质上是三维结构，而医生只能获得二维视觉信息，这种不匹配限制了临床效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了结肠镜环境的特殊挑战，包括黏膜缺乏明显视觉特征、非朗伯反射、严重高光等。他们借鉴了DepthCrafter架构和EDM框架，使用变分自编码器进行深度表示，并采用低秩适应(LoRA)进行模型微调而非从头训练。作者还参考了Chung等人的艺术风格转换工作，但创新性地将其应用于医学领域。他们使用合成结肠镜序列进行训练，并开发了一种风格转换技术来弥合合成数据与真实临床视频之间的域差距，同时引入了多种数据增强技术提高模型泛化能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用扩散模型进行结肠镜视频的深度估计，并通过风格转换技术解决合成训练数据与真实临床视频之间的域差距问题。整体流程包括：1)从CT扫描构建合成结肠镜数据集；2)使用LoRA微调DepthCrafter模型；3)开发真实到合成的风格转换技术，保留几何结构同时改变外观；4)使用ColonCrafter从输入视频生成时间一致的深度图；5)将深度图应用于下游任务如3D点云生成和表面覆盖评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门针对结肠镜的扩散模型深度估计框架；2)新颖的风格转换技术，解决域差距同时保留几何结构；3)在C3VD基准上实现最先进的零样本性能。相比之前工作，通用深度估计模型在结肠镜数据上表现中等，而ColonCrafter通过特定微调将δ1准确性提高17%以上；其他内窥镜特定模型难以保持长期时间一致性；基于合成数据的方法通常在真实临床视频上表现不佳，而ColonCrafter通过风格转换成功弥合了域差距。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ColonCrafter通过结合扩散模型和风格转换技术，首次实现了从单目结肠镜视频中生成时间一致的高精度深度图，为结肠三维重建和临床应用提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional (3D) scene understanding in colonoscopy presentssignificant challenges that necessitate automated methods for accurate depthestimation. However, existing depth estimation models for endoscopy strugglewith temporal consistency across video sequences, limiting their applicabilityfor 3D reconstruction. We present ColonCrafter, a diffusion-based depthestimation model that generates temporally consistent depth maps from monocularcolonoscopy videos. Our approach learns robust geometric priors from syntheticcolonoscopy sequences to generate temporally consistent depth maps. We alsointroduce a style transfer technique that preserves geometric structure whileadapting real clinical videos to match our synthetic training domain.ColonCrafter achieves state-of-the-art zero-shot performance on the C3VDdataset, outperforming both general-purpose and endoscopy-specific approaches.Although full trajectory 3D reconstruction remains a challenge, we demonstrateclinically relevant applications of ColonCrafter, including 3D point cloudgeneration and surface coverage assessment.</description>
      <author>example@mail.com (Romain Hardy, Tyler Berzin, Pranav Rajpurkar)</author>
      <guid isPermaLink="false">2509.13525v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark</title>
      <link>http://arxiv.org/abs/2509.14227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Cinéaste基准，用于评估视觉语言模型对长篇电影的理解能力，现有模型在该基准上表现不佳，表明长程时间推理是主要瓶颈。&lt;h4&gt;背景&lt;/h4&gt;最近的视觉语言模型在视频理解方面取得了进步，但诊断它们对深层叙事理解的能力仍然是一个挑战。现有基准测试通常测试短片段识别或使用基于模板的问题，在评估对长篇叙事内容的细粒度推理方面存在关键空白。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的基准测试(Cinéaste)来评估视觉语言模型对长篇电影的细粒度推理和叙事理解能力。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含来自200部不同电影的1,805个场景的3,119个多项选择题-答案对的数据集，涵盖五个细粒度上下文推理类别。使用GPT-4o生成多样化、丰富的上下文问题，整合视觉描述、字幕、场景标题和摘要。采用两阶段过滤流程确保问题质量：上下文独立性过滤和上下文真实性过滤。&lt;h4&gt;主要发现&lt;/h4&gt;现有的多模态大语言模型在Cinéaste基准上表现不佳；分析显示长程时间推理是主要瓶颈，最好的开源模型仅达到63.15%的准确率。&lt;h4&gt;结论&lt;/h4&gt;这强调了细粒度上下文理解面临的重大挑战以及对长篇电影理解进步的需求。&lt;h4&gt;翻译&lt;/h4&gt;尽管最近视觉语言模型的进步改善了视频理解，但诊断它们对深层叙事理解的能力仍然是一个挑战。现有的基准测试通常测试短片段识别或使用基于模板的问题，在评估对长篇叙事内容的细粒度推理方面留下了关键空白。为了解决这些空白，我们引入了Cinéaste，一个用于长篇电影理解的全面基准。我们的数据集包含来自200部不同电影的1,805个场景的3,119个多项选择题-答案对，涵盖了五个新颖的细粒度上下文推理类别。我们使用GPT-4o通过整合视觉描述、字幕、场景标题和摘要来生成多样化、丰富的上下文问题，这些问题需要深层的叙事理解。为确保高质量评估，我们的流程包含两阶段过滤：上下文独立性过滤确保问题需要视频上下文，而上下文真实性过滤验证与电影内容的事实一致性，减轻幻觉。实验表明，现有的多模态大语言模型在Cinéaste上表现不佳；我们的分析显示长程时间推理是主要瓶颈，最好的开源模型仅达到63.15%的准确率。这强调了细粒度上下文理解面临的重大挑战以及对长篇电影理解进步的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advancements in vision-language models have improved videounderstanding, diagnosing their capacity for deep, narrative comprehensionremains a challenge. Existing benchmarks often test short-clip recognition oruse template-based questions, leaving a critical gap in evaluating fine-grainedreasoning over long-form narrative content. To address these gaps, we introduce$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movieunderstanding. Our dataset comprises 3,119 multiple-choice question-answerpairs derived from 1,805 scenes across 200 diverse movies, spanning five novelfine-grained contextual reasoning categories. We use GPT-4o to generatediverse, context-rich questions by integrating visual descriptions, captions,scene titles, and summaries, which require deep narrative understanding. Toensure high-quality evaluation, our pipeline incorporates a two-stage filteringprocess: Context-Independence filtering ensures questions require videocontext, while Contextual Veracity filtering validates factual consistencyagainst the movie content, mitigating hallucinations. Experiments show thatexisting MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis revealsthat long-range temporal reasoning is a primary bottleneck, with the topopen-source model achieving only 63.15\% accuracy. This underscores significantchallenges in fine-grained contextual understanding and the need foradvancements in long-form movie comprehension.</description>
      <author>example@mail.com (Nisarg A. Shah, Amir Ziai, Chaitanya Ekanadham, Vishal M. Patel)</author>
      <guid isPermaLink="false">2509.14227v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Dense Video Understanding with Gated Residual Tokenization</title>
      <link>http://arxiv.org/abs/2509.14199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Dense Video Understanding (DVU)框架，通过Gated Residual Tokenization (GRT)技术实现高帧率视频理解，并创建了DIVE基准测试评估密集时间推理能力。&lt;h4&gt;背景&lt;/h4&gt;当前视频大语言模型主要依赖低帧率采样，丢弃了密集时间信息，导致对需要精确时间对齐的任务（如讲座理解）表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发能够高效处理高FPS视频理解的方法，减少标记时间和标记开销，并创建新的基准测试评估密集时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出GRT两阶段框架：1)运动补偿间门控标记化，使用像素级运动估计跳过静态区域；2)语义场景内标记化合并，融合静态区域标记减少冗余。&lt;h4&gt;主要发现&lt;/h4&gt;在DIVE基准测试上，GRT表现优于更大规模的VLLM基线模型，并能随FPS正向扩展，证明了密集时间信息的重要性。&lt;h4&gt;结论&lt;/h4&gt;密集时间信息对视频理解至关重要，GRT方法能够实现高效、可扩展的高FPS视频理解。&lt;h4&gt;翻译&lt;/h4&gt;高时间分辨率对于捕捉视频理解中的精细细节至关重要。然而，当前视频大语言模型和基准测试主要依赖低帧率采样，如均匀采样或关键帧选择，丢弃了密集的时间信息。这种折衷避免了标记每一帧的高成本，否则会导致冗余计算和随着视频长度增加的线性标记增长。虽然这种权衡对缓慢变化的内容有效，但对于像讲座理解这样的任务则失败，其中信息几乎出现在每一帧中，需要精确的时间对齐。为解决这一差距，我们引入了Dense Video Understanding (DVU)，通过减少标记时间和标记开销来实现高FPS视频理解。现有基准测试也有局限，因为它们的问答对关注粗略的内容变化。因此，我们提出了DIVE (Dense Information Video Evaluation)，这是第一个专为密集时间推理设计的基准测试。为使DVU实用，我们提出了Gated Residual Tokenization (GRT)，一个两阶段框架：(1)运动补偿间门控标记化使用像素级运动估计在标记化过程中跳过静态区域，实现标记数和计算量的次线性增长。(2)语义场景内标记化合并融合场景内静态区域的标记，进一步减少冗余，同时保留动态语义。在DIVE上的实验表明，GRT优于更大的VLLM基线，并能随FPS正向扩展。这些结果突出了密集时间信息的重要性，并证明GRT能够实现高效、可扩展的高FPS视频理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High temporal resolution is essential for capturing fine-grained details invideo understanding. However, current video large language models (VLLMs) andbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling orkeyframe selection, discarding dense temporal information. This compromiseavoids the high cost of tokenizing every frame, which otherwise leads toredundant computation and linear token growth as video length increases. Whilethis trade-off works for slowly changing content, it fails for tasks likelecture comprehension, where information appears in nearly every frame andrequires precise temporal alignment. To address this gap, we introduce DenseVideo Understanding (DVU), which enables high-FPS video comprehension byreducing both tokenization time and token overhead. Existing benchmarks arealso limited, as their QA pairs focus on coarse content changes. We thereforepropose DIVE (Dense Information Video Evaluation), the first benchmark designedfor dense temporal reasoning. To make DVU practical, we present Gated ResidualTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-GatedTokenization uses pixel-level motion estimation to skip static regions duringtokenization, achieving sub-linear growth in token count and compute. (2)Semantic-Scene Intra-Tokenization Merging fuses tokens across static regionswithin a scene, further reducing redundancy while preserving dynamic semantics.Experiments on DIVE show that GRT outperforms larger VLLM baselines and scalespositively with FPS. These results highlight the importance of dense temporalinformation and demonstrate that GRT enables efficient, scalable high-FPS videounderstanding.</description>
      <author>example@mail.com (Haichao Zhang, Wenhao Chai, Shwai He, Ang Li, Yun Fu)</author>
      <guid isPermaLink="false">2509.14199v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling</title>
      <link>http://arxiv.org/abs/2509.13784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Variable-Rate Spatial Event Mamba的新型架构，直接处理原始事件流而无需中间表示，通过轻量级编码器和Mamba状态空间模型实现高效处理，并采用自适应控制器优化延迟平衡。&lt;h4&gt;背景&lt;/h4&gt;事件相机以微秒级时间分辨率捕捉异步像素级亮度变化，为高速视觉任务提供独特优势。现有方法常将事件流转换为帧、体素网格或点云等中间表示，需要预定义时间窗口，引入窗口延迟；点检测方法则因计算成本高而无法实现实时效率。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，开发能够直接处理原始事件流、无需中间表示的架构，同时保持实时效率，实现窗口延迟和推理延迟之间的最佳平衡。&lt;h4&gt;方法&lt;/h4&gt;提出Variable-Rate Spatial Event Mamba架构，包含：1)轻量级因果空间邻域编码器，高效捕获局部几何关系；2)基于Mamba的状态空间模型，实现线性复杂度的可扩展时间建模；3)自适应控制器，根据事件率调整处理速度优化延迟。&lt;h4&gt;主要发现&lt;/h4&gt;直接处理原始事件流可避免中间表示的延迟；轻量级编码器能有效捕获局部几何关系；Mamba状态空间模型提供线性复杂度的时间建模能力；自适应处理可实现延迟优化。&lt;h4&gt;结论&lt;/h4&gt;Variable-Rate Spatial Event Mamba架构成功克服了现有方法的局限性，能够直接处理原始事件流并保持实时效率，为高速视觉任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;事件相机以微秒级时间分辨率捕捉异步像素级亮度变化，为高速视觉任务提供了独特优势。现有方法通常将事件流转换为帧、体素网格或点云等中间表示，这不可避免地需要预定义时间窗口，从而引入窗口延迟。同时，点检测方法由于计算成本高，面临计算挑战，无法实现实时效率。为克服这些局限性，我们提出了Variable-Rate Spatial Event Mamba，一种新型架构，可直接处理原始事件流而无需中间表示。我们的方法引入了轻量级因果空间邻域编码器，以高效捕获局部几何关系，然后使用基于Mamba的状态空间模型进行具有线性复杂度的可扩展时间建模。在推理过程中，控制器根据事件率自适应调整处理速度，实现窗口延迟和推理延迟之间的最佳平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras capture asynchronous pixel-level brightness changes withmicrosecond temporal resolution, offering unique advantages for high-speedvision tasks. Existing methods often convert event streams into intermediaterepresentations such as frames, voxel grids, or point clouds, which inevitablyrequire predefined time windows and thus introduce window latency. Meanwhile,pointwise detection methods face computational challenges that preventreal-time efficiency due to their high computational cost. To overcome theselimitations, we propose the Variable-Rate Spatial Event Mamba, a novelarchitecture that directly processes raw event streams without intermediaterepresentations. Our method introduces a lightweight causal spatialneighborhood encoder to efficiently capture local geometric relations, followedby Mamba-based state space models for scalable temporal modeling with linearcomplexity. During inference, a controller adaptively adjusts the processingspeed according to the event rate, achieving an optimal balance between windowlatency and inference latency.</description>
      <author>example@mail.com (Hanfang Liang, Bing Wang, Shizhen Zhang, Wen Jiang, Yizhuo Yang, Weixiang Guo, Shenghai Yuan)</author>
      <guid isPermaLink="false">2509.13784v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training</title>
      <link>http://arxiv.org/abs/2509.10426v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为DECAMP的去耦合上下文感知预训练框架，用于解决自动驾驶中轨迹预测面临的标记数据稀缺和多智能体预测场景表现不佳的问题。该框架通过将行为模式学习与潜在特征重建去耦合，优先考虑可解释的动态，并结合上下文感知表示学习和协作空间-运动预训练任务，有效提升了多智能体运动预测的性能。&lt;h4&gt;背景&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法面临标记数据稀缺的问题，在多智能体预测场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，作者提出了一个名为DECAMP的去耦合上下文感知预训练框架，专门用于多智能体运动预测。&lt;h4&gt;方法&lt;/h4&gt;该框架将行为模式学习与潜在特征重建去耦合，优先考虑可解释的动态，从而增强下游预测的场景表示。框架结合了上下文感知表示学习和协作空间-运动预训练任务，能够同时优化结构和意图推理，同时捕捉潜在的动态意图。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse 2基准测试上的实验展示了该方法优越的性能，结果证明了其在多智能体运动预测中的有效性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是自动驾驶中首个用于多智能体运动预测的上下文自编码器框架，代码和模型将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法往往面临标记数据稀缺的问题，并在多智能体预测场景中表现不佳。为了解决这些挑战，我们引入了一个用于多智能体运动预测的去耦合上下文感知预训练框架，名为DECAMP。与现有方法将表示学习与预训练任务纠缠在一起不同，我们的框架将行为模式学习与潜在特征重建去耦合，优先考虑可解释的动态，从而增强下游预测的场景表示。此外，我们的框架结合了上下文感知表示学习和协作空间-运动预训练任务，这能够在捕捉潜在动态意图的同时，对结构和意图推理进行联合优化。我们在Argoverse 2基准测试上的实验展示了我们方法的优越性能，取得的结果强调了其在多智能体运动预测中的有效性。据我们所知，这是自动驾驶中首个用于多智能体运动预测的上下文自编码器框架。代码和模型将公开提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多智能体运动预测中的两个核心挑战：标记数据稀缺问题和多智能体预测场景中的次优性能。这个问题在自动驾驶领域至关重要，因为准确预测交通参与者的运动对确保道路安全和效率至关重要，特别是在复杂的多智能体交互场景中，不准确的预测可能导致生成与整体场景不一致的轨迹，阻碍自动驾驶车辆做出可靠决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前轨迹预测的主流方法（监督学习和自监督学习）及其局限性，特别是自监督学习方法存在的两个核心问题：编码器与预训练任务紧密耦合、难以扩展到多智能体预测。基于这些分析，作者借鉴了计算机视觉中自监督学习的成功经验，特别是掩码图像建模中的回归器设计，将其扩展到自动驾驶中的行为预测。同时，作者参考了DenseTNT和HiVT等方法的向量化表示技术，设计了'编码器-回归器-解码器'的级联范式来解耦行为模式学习与特征重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是解耦行为模式学习与潜在特征重建，优先考虑可解释的动力学，并引入协作空间-运动预训练任务来同时优化结构和推理。整体实现分为两个阶段：1)预训练阶段，模型接收历史和未来状态以及地图信息，通过编码器提取场景特征，回归器预测被遮盖令牌的表示，双解码器执行空间重建和运动识别任务；2)微调阶段，模型仅使用历史状态和地图信息，利用预训练的编码器生成K个场景一致的轨迹预测，每个场景包含所有目标代理的完整轨迹组合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)解耦的自监督学习框架，分离行为模式学习与特征重建；2)协作空间-运动预训练任务，联合优化空间线索和运动信号识别；3)'编码器-回归器-解码器'级联范式，增强场景元素间语义关系建模；4)多世界预测微调，生成场景一致的轨迹组合。相比之前工作，不同之处在于：不将表示学习与预训练任务紧密耦合，支持真正的多智能体联合预测而非单智能体预测，无需复杂后处理来确保场景一致性，并首次揭示协作预训练任务可以增强驾驶行为模式的表示学习。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DECAMP提出了一种解耦的上下文感知预训练框架，通过分离行为模式学习与特征重建并引入协作空间-运动预训练任务，显著提高了多智能体运动预测中场景一致性轨迹预测的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory prediction is a critical component of autonomous driving,essential for ensuring both safety and efficiency on the road. However,traditional approaches often struggle with the scarcity of labeled data andexhibit suboptimal performance in multi-agent prediction scenarios. To addressthese challenges, we introduce a disentangled context-aware pre-trainingframework for multi-agent motion prediction, named DECAMP. Unlike existingmethods that entangle representation learning with pretext tasks, our frameworkdecouples behavior pattern learning from latent feature reconstruction,prioritizing interpretable dynamics and thereby enhancing scene representationfor downstream prediction. Additionally, our framework incorporatescontext-aware representation learning alongside collaborative spatial-motionpretext tasks, which enables joint optimization of structural and intentionalreasoning while capturing the underlying dynamic intentions. Our experiments onthe Argoverse 2 benchmark showcase the superior performance of our method, andthe results attained underscore its effectiveness in multi-agent motionforecasting. To the best of our knowledge, this is the first contextautoencoder framework for multi-agent motion forecasting in autonomous driving.The code and models will be made publicly available.</description>
      <author>example@mail.com (Jianxin Shi, Zengqi Peng, Xiaolong Chen, Tianyu Wo, Jun Ma)</author>
      <guid isPermaLink="false">2509.10426v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction</title>
      <link>http://arxiv.org/abs/2509.13841v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This preprint is also available at ESS Open Archive:  https://essopenarchive.org/users/960205/articles/1329010&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种端到端可区分的混合框架，将图神经网络嵌入到孔隙网络模型中，实现了高精度且跨尺度泛化的多孔介质渗透率预测，同时保持物理基础并提高模型可解释性。&lt;h4&gt;背景&lt;/h4&gt;准确预测多孔介质中的渗透率对地下流动建模至关重要。纯数据驱动模型计算效率高但缺乏跨尺度泛化能力和物理约束；传统孔隙网络模型基于物理且高效，但依赖理想化几何假设，限制了在复杂结构中的准确性。&lt;h4&gt;目的&lt;/h4&gt;克服纯数据驱动模型和传统孔隙网络模型的局限性，开发一种既能避免理想化几何假设又能保持物理基础流动计算的渗透率预测方法。&lt;h4&gt;方法&lt;/h4&gt;构建一个端到端可区分的混合框架，将图神经网络嵌入到孔隙网络模型中，用GNN预测替代传统解析公式进行传导率计算，并通过反向传播梯度实现完全耦合的端到端训练，仅需单一标量渗透率作为训练目标。&lt;h4&gt;主要发现&lt;/h4&gt;所得模型具有高精度，能很好地跨不同尺度泛化，性能优于纯数据驱动和传统孔隙网络模型；基于梯度的敏感性分析揭示了物理上一致的特征影响，提高了模型可解释性。&lt;h4&gt;结论&lt;/h4&gt;该方法为复杂多孔介质中的渗透率预测提供了可扩展且物理信息丰富的框架，有效减少了模型不确定性并提高了预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;多孔介质中渗透率的准确预测对地下流动建模至关重要。虽然纯数据驱动模型提供了计算效率，但它们通常缺乏跨尺度的泛化能力，且不包含明确的物理约束。另一方面，孔隙网络模型基于物理原理且高效，但依赖于理想化的几何假设来估计孔隙尺度水力传导率，限制了其在复杂结构中的准确性。为了克服这些局限，我们提出了一种端到端可区分的混合框架，将图神经网络嵌入到孔隙网络模型中。在此框架中，用于传导率计算的解析公式被基于GNN的预测所替代，这些预测由孔隙和喉部特征推导得出。预测的传导率随后被传递给PNM求解器进行渗透率计算。这样，模型避免了PNM的理想化几何假设，同时保留了基于物理的流动计算。GNN的训练不需要标记的传导率数据（每个孔隙网络可能有数千个），而是使用单一的标量渗透率作为训练目标。这通过反向传播梯度（通过GNN的自动微分和PNM求解器的离散伴随方法）成为可能，实现了完全耦合的端到端训练。所得模型实现高精度并能很好地跨不同尺度泛化，性能优于纯数据驱动和传统PNM方法。基于梯度的敏感性分析进一步揭示了物理上一致的特征影响，增强了模型可解释性。这种方法为复杂多孔介质中的渗透率预测提供了可扩展且物理信息丰富的框架，减少了模型不确定性并提高了准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of permeability in porous media is essential for modelingsubsurface flow. While pure data-driven models offer computational efficiency,they often lack generalization across scales and do not incorporate explicitphysical constraints. Pore network models (PNMs), on the other hand, arephysics-based and efficient but rely on idealized geometric assumptions toestimate pore-scale hydraulic conductance, limiting their accuracy in complexstructures. To overcome these limitations, we present an end-to-enddifferentiable hybrid framework that embeds a graph neural network (GNN) into aPNM. In this framework, the analytical formulas used for conductancecalculations are replaced by GNN-based predictions derived from pore and throatfeatures. The predicted conductances are then passed to the PNM solver forpermeability computation. In this way, the model avoids the idealized geometricassumptions of PNM while preserving the physics-based flow calculations. TheGNN is trained without requiring labeled conductance data, which can number inthe thousands per pore network; instead, it learns conductance values by usinga single scalar permeability as the training target. This is made possible bybackpropagating gradients through both the GNN (via automatic differentiation)and the PNM solver (via a discrete adjoint method), enabling fully coupled,end-to-end training. The resulting model achieves high accuracy and generalizeswell across different scales, outperforming both pure data-driven andtraditional PNM approaches. Gradient-based sensitivity analysis further revealsphysically consistent feature influences, enhancing model interpretability.This approach offers a scalable and physically informed framework forpermeability prediction in complex porous media, reducing model uncertainty andimproving accuracy.</description>
      <author>example@mail.com (Qingqi Zhao, Heng Xiao)</author>
      <guid isPermaLink="false">2509.13841v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>State Space Models over Directed Graphs</title>
      <link>http://arxiv.org/abs/2509.13735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  currently undergoing review by IEEE Transactions on Big Data&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新方法DirEgo2Token，通过k-hop自我图将有向图序列化，并开发了DirGraphSSM架构，首次将状态空间模型系统性地扩展到有向图学习领域。实验表明该方法在多个任务上取得了最先进性能，且训练速度提升1.5至2倍。&lt;h4&gt;背景&lt;/h4&gt;有向图在众多领域普遍存在，边的方向性编码关键因果关系。现有针对有向图的图神经网络和图变换器面临两大挑战：有效捕获长程因果关系，以及在大规模图数据集上平衡准确性和训练效率。状态空间模型在因果序列任务中表现出色，但其图变体仅适用于无向图，限制了在有向图学习中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有图状态空间模型仅适用于无向图的限制，将有向图学习与状态空间模型相结合，提高有向图学习的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;提出两种方法：1) DirEgo2Token，通过k-hop自我图将有向图序列化；2) DirGraphSSM，一种新的有向图神经网络架构，通过消息传递机制在有向图上实现状态空间模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，DirGraphSSM在三个代表性的有向图学习任务上取得了最先进的性能，同时在另外两个任务上取得了具有竞争力的性能，并且比现有的最先进模型快1.5到2倍。&lt;h4&gt;结论&lt;/h4&gt;DirGraphSSM成功地将状态空间模型应用于有向图学习，解决了现有方法面临的挑战，并在多个任务上取得了优异的性能和训练效率。&lt;h4&gt;翻译&lt;/h4&gt;有向图在众多领域中普遍存在，其中边的方向性编码了关键的因果关系。然而，现有的针对有向图设计的图神经网络和图变换器面临两大挑战：(1)有效捕获来自有向边的长程因果关系；(2)在处理大规模图数据集时平衡准确性和训练效率。近年来，状态空间模型在因果序列任务中取得了实质性进展，其针对图的变体在各种图学习基准测试中展示了最先进的准确性，同时保持了高效率。然而，现有的图状态空间模型仅设计用于无向图，这限制了它们在有向图学习中的性能。为此，我们提出了一种创新方法DirEgo2Token，通过k-hop自我图将有向图序列化。这是首次将状态空间模型系统性地扩展到有向图学习领域。基于此，我们开发了DirGraphSSM，一种新的有向图神经网络架构，通过消息传递机制在有向图上实现状态空间模型。实验结果表明，DirGraphSSM在三个代表性的有向图学习任务上取得了最先进的性能，同时在另外两个任务上取得了具有竞争力的性能，并且比现有的最先进模型快1.5到2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Directed graphs are ubiquitous across numerous domains, where thedirectionality of edges encodes critical causal dependencies. However, existingGNNs and graph Transformers tailored for directed graphs face two majorchallenges: (1) effectively capturing long-range causal dependencies derivedfrom directed edges; (2) balancing accuracy and training efficiency whenprocessing large-scale graph datasets. In recent years, state space models(SSMs) have achieved substantial progress in causal sequence tasks, and theirvariants designed for graphs have demonstrated state-of-the-art accuracy whilemaintaining high efficiency across various graph learning benchmarks. However,existing graph state space models are exclusively designed for undirectedgraphs, which limits their performance in directed graph learning. To this end,we propose an innovative approach DirEgo2Token which sequentializes directedgraphs via k-hop ego graphs. This marks the first systematic extension of statespace models to the field of directed graph learning. Building upon this, wedevelop DirGraphSSM, a novel directed graph neural network architecture thatimplements state space models on directed graphs via the message-passingmechanism. Experimental results demonstrate that DirGraphSSM achievesstate-of-the-art performance on three representative directed graph learningtasks while attaining competitive performance on two additional tasks with1.5$\times $ to 2$\times $ training speed improvements compared to existingstate-of-the-art models.</description>
      <author>example@mail.com (Junzhi She, Xunkai Li, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2509.13735v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Hate Detection Using Dual-Stream Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.13515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的多模态双流图神经网络模型用于仇恨视频检测，通过构建实例图和互补权重图来突出仇恨内容，系统化建模视频中的结构化关系，实现了最先进的分类性能并具有强可解释性。&lt;h4&gt;背景&lt;/h4&gt;仇恨视频对在线安全和现实福祉构成严重风险，需要有效的检测方法。虽然多模态分类方法优于单模态方法，但存在忽视仇恨内容定义视频类别、统一处理所有内容而非强调仇恨组件，以及无法系统捕获视频结构化信息等局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态仇恨视频检测方法的局限性，开发能够突出仇恨内容、系统化建模视频结构化关系的高效检测模型。&lt;h4&gt;方法&lt;/h4&gt;提出一种多模态双流图神经网络模型，通过将视频分离为多个实例构建实例图提取实例级特征，使用互补权重图分配重要性权重突出仇恨实例，结合权重和特征生成视频标签，采用基于图的框架系统化建模模态内和跨模态的结构化关系。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上的广泛实验表明，该模型在仇恨视频分类方面达到了最先进的水平，具有强可解释性，代码已在GitHub上公开。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型有效解决了现有多模态仇恨视频检测方法的局限性，通过强调仇恨内容和系统化建模结构化关系，显著提升了分类性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;仇恨视频对在线安全和现实福祉构成严重风险，需要有效的检测方法。尽管整合多种模态信息的多模态分类方法优于单模态方法，但它们通常忽视了即使是最少量的仇恨内容也能定义视频类别。具体而言，它们通常统一处理所有内容，而非强调仇恨组件。此外，现有的多模态方法无法系统捕获视频中的结构化信息，限制了多模态融合的有效性。为解决这些局限性，我们提出了一种新颖的多模态双流图神经网络模型。它通过将给定视频分离为多个实例来构建实例图，以提取实例级特征。然后，互补权重图为这些特征分配重要性权重，突出仇恨实例。结合重要性权重和实例特征生成视频标签。我们的模型采用基于图的框架系统化建模模态内和跨模态的结构化关系。在公共数据集上的广泛实验表明，我们的模型在仇恨视频分类方面达到了最先进的水平，并具有强可解释性。代码可在以下网址获取：https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hateful videos present serious risks to online safety and real-worldwell-being, necessitating effective detection methods. Although multimodalclassification approaches integrating information from several modalitiesoutperform unimodal ones, they typically neglect that even minimal hatefulcontent defines a video's category. Specifically, they generally treat allcontent uniformly, instead of emphasizing the hateful components. Additionally,existing multimodal methods cannot systematically capture structuredinformation in videos, limiting the effectiveness of multimodal fusion. Toaddress these limitations, we propose a novel multimodal dual-stream graphneural network model. It constructs an instance graph by separating the givenvideo into several instances to extract instance-level features. Then, acomplementary weight graph assigns importance weights to these features,highlighting hateful instances. Importance weights and instance features arecombined to generate video labels. Our model employs a graph-based framework tosystematically model structured relationships within and across modalities.Extensive experiments on public datasets show that our model isstate-of-the-art in hateful video classification and has strong explainability.Code is available:https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.</description>
      <author>example@mail.com (Jiangbei Yue, Shuonan Yang, Tailin Chen, Jianbo Jiao, Zeyu Fu)</author>
      <guid isPermaLink="false">2509.13515v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Context-Aware Inputs: Physics-Inspired Improvements in Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.10684v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了物理信息策略在改进NuGraph2架构语义分割方面的应用，特别关注提升对米歇尔电子等代表性不足粒子类别的识别性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在液态氩时间投影室的事件重建任务中显示出强大潜力，但对米歇尔电子等代表性不足的粒子类别性能仍然有限。&lt;h4&gt;目的&lt;/h4&gt;探索物理信息策略以改进NuGraph2架构中的语义分割，提高对米歇尔电子等粒子类别的识别能力。&lt;h4&gt;方法&lt;/h4&gt;研究三种互补方法：(i)通过探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示；(ii)引入辅助解码器捕获类别级相关性；(iii)整合基于米歇尔电子能量分布的能量正则化项。&lt;h4&gt;主要发现&lt;/h4&gt;物理启发特征增强带来最大提升，特别显著提高了米歇尔电子的精确率和召回率；辅助解码器和能量正则化项提供的改进有限，部分原因是NuGraph2缺乏明确的粒子或事件级表示。&lt;h4&gt;结论&lt;/h4&gt;将物理上下文直接嵌入节点级输入比施加任务特定辅助损失更有效；建议未来具有明确粒子和事件级推理的分层架构(如NuGraph3)将为高级解码器和基于物理的正则化提供更自然的环境。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近在液态氩时间投影室的事件重建任务中显示出强大的前景，但对于代表性不足的粒子类别(如米歇尔电子)其性能仍然有限。在这项工作中，我们研究了物理信息策略以改进NuGraph2架构中的语义分割。我们探索了三种互补方法：(i)通过从探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示，(ii)引入辅助解码器以捕获类别级相关性，以及(iii)整合受米歇尔电子能量分布启发的基于能量的正则化项。在MicroBooNE公共数据集上的实验表明，物理启发的特征增强带来了最大的收益，特别是通过解纠缠重叠的潜在空间区域，显著提高了米歇尔电子的精确率和召回率。相比之下，辅助解码器和能量正则化项提供的改进有限，部分原因是NuGraph2的命中级别特性，缺乏明确的粒子或事件级表示。我们的发现强调，将物理上下文直接嵌入节点级输入比施加任务特定的辅助损失更有效，并表明未来的分层架构(如NuGraph3)具有明确的粒子和事件级推理，将为高级解码器和基于物理的正则化提供更自然的环境。这项工作的代码已在Github上公开可用：https://github.com/vitorgrizzi/nugraph_phys/tree/main_phys。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决图神经网络在液态氩时间投影室(LArTPC)中对代表性不足粒子类别(特别是米歇尔电子)识别性能不佳的问题。这个问题很重要，因为米歇尔电子是粒子物理研究中的关键信号，准确识别不同类型粒子对于理解中微子相互作用和粒子物理现象至关重要，提高对少数类别的识别能力能改善整体粒子分类的可靠性，对高能物理实验的数据分析和物理发现具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于物理直觉探索了三种互补策略来注入物理领域知识：丰富输入表示、引入辅助解码器和结合基于能量的正则化项。他们借鉴了现有工作：参考了Drielsma的GrapPA研究引入几何描述符，借鉴了DUNE协作的CVN网络进行多任务学习，以及参考了Sharma等人的物理信息GNN将守恒定律作为损失项。作者系统性地评估了这些策略在NuGraph2架构中的适用性和有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将物理领域知识直接嵌入到图神经网络的输入表示中，而非仅通过辅助损失函数约束模型。整体实现流程包括：1)特征扩展，添加节点度、最短边长、导线差值和时间差值四个新特征；2)添加额外解码器，实验了二进制米歇尔解码器、图级米歇尔计数器和完整类别分布解码器；3)米歇尔能量正则化，在损失函数中添加基于物理的约束项，使用波形积分作为沉积能量的代理指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)设计特定的物理启发上下文感知输入特征；2)系统性评估三种物理知识注入策略；3)针对代表性不足类别优化识别性能。相比之前工作，不同之处在于：与GrapPA相比，专注于更广泛的语义分割而非仅电磁簇射组装；与CVN相比，通过特征增强而非多任务学习提高性能；与PINNs相比，将物理知识编码到输入特征而非直接施加物理方程；与原始NuGraph2相比，扩展了输入特征空间并评估了多种物理知识注入策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过在NuGraph2中引入物理启发的上下文感知输入特征，特别是节点度、几何距离和轨迹连续性度量，显著提高对代表性不足粒子类别(如米歇尔电子)的识别性能，同时证明了将领域知识直接嵌入输入表示比通过辅助损失函数约束模型更为有效。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have recently shown strong promise for eventreconstruction tasks in Liquid Argon Time Projection Chambers, yet theirperformance remains limited for underrepresented classes of particles, such asMichel electrons. In this work, we investigate physics-informed strategies toimprove semantic segmentation within the NuGraph2 architecture. We explorethree complementary approaches: (i) enriching the input representation withcontext-aware features derived from detector geometry and track continuity,(ii) introducing auxiliary decoders to capture class-level correlations, and(iii) incorporating energy-based regularization terms motivated by Michelelectron energy distributions. Experiments on MicroBooNE public datasets showthat physics-inspired feature augmentation yields the largest gains,particularly boosting Michel electron precision and recall by disentanglingoverlapping latent space regions. In contrast, auxiliary decoders andenergy-regularization terms provided limited improvements, partly due to thehit-level nature of NuGraph2, which lacks explicit particle- or event-levelrepresentations. Our findings highlight that embedding physics context directlyinto node-level inputs is more effective than imposing task-specific auxiliarylosses, and suggest that future hierarchical architectures such as NuGraph3,with explicit particle- and event-level reasoning, will provide a more naturalsetting for advanced decoders and physics-based regularization. The code forthis work is publicly available on Github athttps://github.com/vitorgrizzi/nugraph_phys/tree/main_phys.</description>
      <author>example@mail.com (Vitor F. Grizzi, Margaret Voetberg, Giuseppe Cerati, Hadi Meidani, V Hewes)</author>
      <guid isPermaLink="false">2509.10684v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Explainability: Post-hoc Explanations for Geometric Neural Network Predictions</title>
      <link>http://arxiv.org/abs/2509.10676v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究为科学应用中的AI方法引入了后验可解释性技术，通过多种解释方法共同分析图神经网络NuGraph2的决策过程，提高了AI在科学应用中的透明度和可靠性。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能在科学应用中的普及，需要能够将结果归因于网络推理过程的能力，以保持稳健的科学泛化。这促使了对AI模型可解释性的研究需求。&lt;h4&gt;目的&lt;/h4&gt;激励和展示后验可解释性方法在科学应用中AI方法的使用，特别是在中微子标记的图神经网络NuGraph2中的应用。&lt;h4&gt;方法&lt;/h4&gt;为现有的图神经网络NuGraph2引入可解释性附加组件，包括检查网络输出（节点分类）和边连接的技术，以及使用新通用工具探测潜在空间的方法。&lt;h4&gt;主要发现&lt;/h4&gt;没有单一的解释方法足以展示网络'理解'，但多种方法结合使用可以提供对分类过程中使用的见解。&lt;h4&gt;结论&lt;/h4&gt;这些可解释性方法虽然在NuGraph2上测试，但具有广泛适用性，可以应用于各种类型的网络，不仅限于图神经网络，为科学应用中的AI模型提供了更好的透明度和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;随着人工智能在科学应用中的日益普及，将结果归因于网络推理过程的能力对于保持稳健的科学泛化至关重要。在这项工作中，我们旨在激励和展示后验可解释性方法在科学应用中AI方法的使用。为此，我们为现有的用于中微子标记的图神经网络NuGraph2引入了可解释性附加组件。解释形式包括一系列技术，检查网络的输出（节点分类）和它们之间的边连接，以及使用应用于该网络的新通用工具探测潜在空间。我们展示了没有这些方法中的任何一种足以单独展示网络'理解'，但它们共同可以提供对分类过程中使用的见解。虽然这些方法在NuGraph2应用上进行了测试，但它们可以应用于广泛的网络，不仅限于GNN。这项工作的代码已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing popularity of artificial intelligence used for scientificapplications, the ability of attribute a result to a reasoning process from thenetwork is in high demand for robust scientific generalizations to hold. Inthis work we aim to motivate the need for and demonstrate the use of post-hocexplainability methods when applied to AI methods used in scientificapplications. To this end, we introduce explainability add-ons to the existinggraph neural network (GNN) for neutrino tagging, NuGraph2. The explanationstake the form of a suite of techniques examining the output of the network(node classifications) and the edge connections between them, and probing ofthe latent space using novel general-purpose tools applied to this network. Weshow how none of these methods are singularly sufficient to show network"understanding", but together can give insights into the processes used inclassification. While these methods are tested on the NuGraph2 application,they can be applied to a broad range of networks, not limited to GNNs. The codefor this work is publicly available on GitHub athttps://github.com/voetberg/XNuGraph.</description>
      <author>example@mail.com (Margaret Voetberg, Vitor F. Grizzi, Giuseppe Cerati, Hadi Meidani, V Hewes)</author>
      <guid isPermaLink="false">2509.10676v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2509.14181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TimeAlign框架，通过表示对齐技术解决时间序列预测中历史输入与未来目标间的分布差距问题，提升预测性能，且与基础预测器架构无关，计算开销小。&lt;h4&gt;背景&lt;/h4&gt;表示学习技术如对比学习在计算机视觉和自然语言处理领域已取得成功，并在时间序列预测中也有探索。然而，最近最先进的预测器很少采用这些表示方法，因为它们显示出很小的性能优势。&lt;h4&gt;目的&lt;/h4&gt;挑战当前观点，证明显式表示对齐可以提供关键信息，弥合历史输入与未来目标之间的分布差距，从而提升时间序列预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出TimeAlign，一个轻量级即插即用框架，通过简单的重建任务学习辅助特征，并将其反馈给任何基础预测器。该方法与架构无关，计算开销小。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在八个基准测试上的广泛实验验证了TimeAlign的优越性能。2. 收益主要来自于修正历史输入和未来输出之间的频率不匹配。3. 研究提供了TimeAlign有效性的理论依据，证明它能增加学习表示与预测目标之间的互信息。&lt;h4&gt;结论&lt;/h4&gt;TimeAlign可作为现代深度学习时间序列预测系统的通用对齐模块，因为它与架构无关且计算开销小。&lt;h4&gt;翻译&lt;/h4&gt;表示学习技术如对比学习长期以来一直在时间序列预测中得到探索，反映了它们在计算机视觉和自然语言处理领域的成功。然而，最近的最先进预测器很少采用这些表示方法，因为它们显示出很小的性能优势。我们挑战这一观点，并证明显式表示对齐可以提供关键信息，弥合历史输入和未来目标之间的分布差距。为此，我们引入了TimeAlign，一个轻量级即插即用框架，通过简单的重建任务学习辅助特征，并将它们反馈给任何基础预测器。在八个基准测试上的广泛实验验证了其优越性能。进一步的研究表明，收益主要来自于修正历史输入和未来输出之间的频率不匹配。我们还提供了TimeAlign在增加学习表示与预测目标之间互信息方面的有效性的理论依据。由于它与架构无关且计算开销小，TimeAlign可作为现代深度学习时间序列预测系统的通用对齐模块。代码可在https://github.com/TROUBADOUR000/TimeAlign获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning techniques like contrastive learning have long beenexplored in time series forecasting, mirroring their success in computer visionand natural language processing. Yet recent state-of-the-art (SOTA) forecastersseldom adopt these representation approaches because they have shown littleperformance advantage. We challenge this view and demonstrate that explicitrepresentation alignment can supply critical information that bridges thedistributional gap between input histories and future targets. To this end, weintroduce TimeAlign, a lightweight, plug-and-play framework that learnsauxiliary features via a simple reconstruction task and feeds them back to anybase forecaster. Extensive experiments across eight benchmarks verify itssuperior performance. Further studies indicate that the gains arises primarilyfrom correcting frequency mismatches between historical inputs and futureoutputs. We also provide a theoretical justification for the effectiveness ofTimeAlign in increasing the mutual information between learned representationsand predicted targets. As it is architecture-agnostic and incurs negligibleoverhead, TimeAlign can serve as a general alignment module for modern deeplearning time-series forecasting systems. The code is available athttps://github.com/TROUBADOUR000/TimeAlign.</description>
      <author>example@mail.com (Yifan Hu, Jie Yang, Tian Zhou, Peiyuan Liu, Yujin Tang, Rong Jin, Liang Sun)</author>
      <guid isPermaLink="false">2509.14181v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits</title>
      <link>http://arxiv.org/abs/2509.14169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TopoSizing是一个端到端框架，能直接从原始网表进行稳健的电路理解，并将这种知识转化为优化收益，解决了模拟和混合信号电路设计中的数据质量和领域知识嵌入挑战。&lt;h4&gt;背景&lt;/h4&gt;模拟和混合信号电路设计面临高质量数据短缺和领域知识难以嵌入自动化流程的挑战。传统黑盒优化采样效率高但缺乏电路理解，基于学习的方法嵌入结构知识但特定于案例且重新训练成本高，而大语言模型方法常需要人工干预，限制了通用性和透明度。&lt;h4&gt;目的&lt;/h4&gt;提出TopoSizing框架，实现从原始网表直接进行稳健的电路理解，并将这种知识转化为优化收益，提高设计效率同时减少人工干预。&lt;h4&gt;方法&lt;/h4&gt;应用图算法将电路组织成层次化的器件-模块-阶段表示；LLM代理执行迭代假设-验证-细化循环，内置一致性检查产生明确注释；将验证过的见解集成到贝叶斯优化中，通过LLM引导的初始采样和停滞触发的信任区域更新提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;TopoSizing能够直接从原始网表理解电路并将这种理解转化为优化收益；通过LLM代理的迭代循环产生明确注释；验证过的见解可以集成到贝叶斯优化中，提高效率同时保持可行性。&lt;h4&gt;结论&lt;/h4&gt;TopoSizing是一个有前途的端到端框架，结合了电路理解和优化效率，解决了模拟和混合信号电路设计中的关键挑战，无需大量人工干预。&lt;h4&gt;翻译&lt;/h4&gt;模拟和混合信号电路设计仍然具有挑战性，这是由于高质量数据的缺乏以及将领域知识嵌入自动化流程的困难。传统的黑盒优化实现了采样效率，但缺乏电路理解，这常常导致在设计空间中的低价值区域浪费评估。相比之下，基于学习的方法嵌入结构知识，但特定于案例且重新训练成本高。最近使用大语言模型的尝试显示出潜力，但它们通常依赖人工干预，限制了通用性和透明度。我们提出了TopoSizing，一个端到端框架，直接从原始网表执行稳健的电路理解，并将这种知识转化为优化收益。我们的方法首先应用图算法将电路组织成层次化的器件-模块-阶段表示。然后，LLM代理执行迭代假设-验证-细化循环，内置一致性检查，产生明确的注释。验证过的见解通过LLM引导的初始采样和停滞触发的信任区域更新集成到贝叶斯优化中，提高了效率同时保持了可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analog and mixed-signal circuit design remains challenging due to theshortage of high-quality data and the difficulty of embedding domain knowledgeinto automated flows. Traditional black-box optimization achieves samplingefficiency but lacks circuit understanding, which often causes evaluations tobe wasted in low-value regions of the design space. In contrast, learning-basedmethods embed structural knowledge but are case-specific and costly to retrain.Recent attempts with large language models show potential, yet they often relyon manual intervention, limiting generality and transparency. We proposeTopoSizing, an end-to-end framework that performs robust circuit understandingdirectly from raw netlists and translates this knowledge into optimizationgains. Our approach first applies graph algorithms to organize circuits into ahierarchical device-module-stage representation. LLM agents then execute aniterative hypothesis-verification-refinement loop with built-in consistencychecks, producing explicit annotations. Verified insights are integrated intoBayesian optimization through LLM-guided initial sampling andstagnation-triggered trust-region updates, improving efficiency whilepreserving feasibility.</description>
      <author>example@mail.com (Ziming Wei, Zichen Kong, Yuan Wang, David Z. Pan, Xiyuan Tang)</author>
      <guid isPermaLink="false">2509.14169v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration</title>
      <link>http://arxiv.org/abs/2509.14084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为AD-DINOv3的新型视觉语言多模态框架，用于零样本异常检测(ZSAD)，通过结合DINOv3和CLIP模型的优势，解决了特征不对齐和异常区域识别困难的问题。&lt;h4&gt;背景&lt;/h4&gt;零样本异常检测(ZSAD)旨在识别任意新类别的异常，提供可扩展且标注高效的解决方案。传统方法大多基于CLIP模型，通过计算视觉和文本嵌入之间的相似性进行异常检测。最近，DINOv3等视觉基础模型展示了强大的可迁移表示能力，但将其应用于ZSAD面临领域偏差和异常识别困难等挑战。&lt;h4&gt;目的&lt;/h4&gt;将DINOv3模型适应于零样本异常检测任务，解决特征不对齐和细微异常被误认为正常前景的问题，开发一个高效且通用的异常检测框架。&lt;h4&gt;方法&lt;/h4&gt;提出AD-DINOv3框架，将异常检测形式化为多模态对比学习问题。使用DINOv3作为视觉主干提取patch tokens和CLS token，CLIP文本编码器提供正常和异常提示的嵌入。引入轻量级适配器弥合领域差距，并设计异常感知校准模块(AACM)引导CLS token关注异常区域而非通用前景语义。&lt;h4&gt;主要发现&lt;/h4&gt;在八个工业和医疗基准上的大量实验表明，AD-DINOv3一致匹配或超越最先进的方法，验证了其作为通用零样本异常检测框架的优越性。轻量级适配器有效解决了领域偏差问题，AACM模块显著提高了对细微异常的识别能力。&lt;h4&gt;结论&lt;/h4&gt;AD-DINOv3框架成功将DINOv3模型应用于零样本异常检测，通过多模态对比学习和异常感知校准有效解决了传统方法面临的挑战，为异常检测领域提供了一个高效且通用的解决方案，具有广泛的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;零样本异常检测(ZSAD)旨在识别任意新类别的异常，提供可扩展且标注高效的解决方案。传统上，大多数ZSAD工作基于CLIP模型，通过计算视觉和文本嵌入之间的相似性来执行异常检测。最近，DINOv3等视觉基础模型展示了强大的可迁移表示能力。在这项工作中，我们首次将DINOv3适应于ZSAD。然而，这种适应带来了两个关键挑战：(i)大规模预训练数据与异常检测任务之间的领域偏差导致特征不对齐；(ii)预训练表示中对全局语义的固有倾向往往导致细微异常被误认为是正常前景对象的一部分，而不是被识别为异常区域。为了克服这些挑战，我们引入了AD-DINOv3，一个专为ZSAD设计的新型视觉语言多模态框架。具体来说，我们将异常检测形式化为多模态对比学习问题，其中DINOv3用作视觉主干提取patch tokens和CLS token，CLIP文本编码器为正常和异常提示提供嵌入。为了弥合领域差距，我们在两种模态中都引入了轻量级适配器，使它们的表示能够重新校准以适应异常检测任务。除了这种基线对齐外，我们还设计了一个异常感知校准模块(AACM)，明确引导CLS token关注异常区域而非通用前景语义，从而增强可区分性。在八个工业和医疗基准上的大量实验表明，AD-DINOv3一致匹配或超越最先进的方法，验证了其作为通用零样本异常检测框架的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrarynovel categories, offering a scalable and annotation-efficient solution.Traditionally, most ZSAD works have been based on the CLIP model, whichperforms anomaly detection by calculating the similarity between visual andtext embeddings. Recently, vision foundation models such as DINOv3 havedemonstrated strong transferable representation capabilities. In this work, weare the first to adapt DINOv3 for ZSAD. However, this adaptation presents twokey challenges: (i) the domain bias between large-scale pretraining data andanomaly detection tasks leads to feature misalignment; and (ii) the inherentbias toward global semantics in pretrained representations often leads tosubtle anomalies being misinterpreted as part of the normal foreground objects,rather than being distinguished as abnormal regions. To overcome thesechallenges, we introduce AD-DINOv3, a novel vision-language multimodalframework designed for ZSAD. Specifically, we formulate anomaly detection as amultimodal contrastive learning problem, where DINOv3 is employed as the visualbackbone to extract patch tokens and a CLS token, and the CLIP text encoderprovides embeddings for both normal and abnormal prompts. To bridge the domaingap, lightweight adapters are introduced in both modalities, enabling theirrepresentations to be recalibrated for the anomaly detection task. Beyond thisbaseline alignment, we further design an Anomaly-Aware Calibration Module(AACM), which explicitly guides the CLS token to attend to anomalous regionsrather than generic foreground semantics, thereby enhancing discriminability.Extensive experiments on eight industrial and medical benchmarks demonstratethat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,verifying its superiority as a general zero-shot anomaly detection framework.</description>
      <author>example@mail.com (Jingyi Yuan, Jianxiong Ye, Wenkang Chen, Chenqiang Gao)</author>
      <guid isPermaLink="false">2509.14084v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction</title>
      <link>http://arxiv.org/abs/2509.14037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PhenoGnet的新型基于图的对比学习框架，通过整合基因功能相互作用网络与人类表型本体来预测疾病相似性，并在基准测试中表现出色，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;理解疾病相似性对于推进诊断、药物发现和个性化治疗策略至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发PhenoGnet框架，通过整合基因功能相互作用网络与人类表型本体来预测疾病相似性。&lt;h4&gt;方法&lt;/h4&gt;PhenoGnet包含两个关键组件：内视图模型使用图卷积网络和图注意力网络分别编码基因和表型图；跨视图模型作为共享权重的多层感知机，通过对比学习对齐基因和表型嵌入。模型使用已知的基因-表型关联作为正对，随机采样的无关对作为负对进行训练，疾病通过其关联基因和/或表型的平均嵌入表示，通过余弦相似度计算成对相似度。&lt;h4&gt;主要发现&lt;/h4&gt;在包含1100个相似和866个不相似疾病对的基准测试中，基于基因的嵌入实现了0.9012的AUCPR和0.8764的AUROC，优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;PhenoGnet能够捕捉超越直接重叠的潜在生物学关系，为疾病相似性预测提供了可扩展且可解释的解决方案，具有在罕见疾病研究和精准医学中应用的潜力。&lt;h4&gt;翻译&lt;/h4&gt;理解疾病相似性对于推进诊断、药物发现和个性化治疗策略至关重要。我们提出了PhenoGnet，一种新颖的基于图的对比学习框架，旨在通过整合基因功能相互作用网络与人类表型本体来预测疾病相似性。PhenoGnet包含两个关键组件：一个内视图模型，分别使用图卷积网络和图注意力网络编码基因和表型图；以及一个跨视图模型，作为共享权重的多层感知机实现，通过对比学习对齐基因和表型嵌入。模型使用已知的基因-表型关联作为正对，随机采样的无关对作为负对进行训练。疾病通过其关联基因和/或表型的平均嵌入表示，成对相似度通过余弦相似度计算。在包含1100个相似和866个不相似疾病对的精选基准上的评估展示了强劲的性能，基于基因的嵌入实现了0.9012的AUCPR和0.8764的AUROC，优于现有的最先进方法。值得注意的是，PhenoGnet能够捕捉超越直接重叠的潜在生物学关系，为疾病相似性预测提供了可扩展且可解释的解决方案。这些结果强调了其在罕见疾病研究和精准医学中下游应用的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding disease similarity is critical for advancing diagnostics, drugdiscovery, and personalized treatment strategies. We present PhenoGnet, a novelgraph-based contrastive learning framework designed to predict diseasesimilarity by integrating gene functional interaction networks with the HumanPhenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-viewmodel that separately encodes gene and phenotype graphs using GraphConvolutional Networks (GCNs) and Graph Attention Networks (GATs), and a crossview model implemented as a shared weight multilayer perceptron (MLP) thataligns gene and phenotype embeddings through contrastive learning. The model istrained using known gene phenotype associations as positive pairs and randomlysampled unrelated pairs as negatives. Diseases are represented by the meanembeddings of their associated genes and/or phenotypes, and pairwise similarityis computed via cosine similarity. Evaluation on a curated benchmark of 1,100similar and 866 dissimilar disease pairs demonstrates strong performance, withgene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764,outperforming existing state of the art methods. Notably, PhenoGnet captureslatent biological relationships beyond direct overlap, offering a scalable andinterpretable solution for disease similarity prediction. These resultsunderscore its potential for enabling downstream applications in rare diseaseresearch and precision medicine.</description>
      <author>example@mail.com (Ranga Baminiwatte, Kazi Jewel Rana, Aaron J. Masino)</author>
      <guid isPermaLink="false">2509.14037v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation</title>
      <link>http://arxiv.org/abs/2509.14036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出基于问题的手语翻译（QB-SLT）任务，探索对话上下文的高效集成，并开发了SSL-SSAF方法，利用问题文本和手语序列进行特征对齐和自适应提取，在两个新数据集上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;手语翻译旨在弥合聋人与健听人之间的沟通鸿沟，对话提供重要上下文线索。传统方法依赖手语标注（gloss annotations），但标注过程较为困难，而对话自然存在于交流中且更易标注。&lt;h4&gt;目的&lt;/h4&gt;探索对话在手语翻译中的高效集成，解决多模态特征对齐挑战，利用问题上下文提高翻译质量，开发一种利用易获取问题文本替代难度较高手语标注的方法。&lt;h4&gt;方法&lt;/h4&gt;提出跨模态自监督学习与Sigmoid自注意力加权融合方法（SSL-SSAF），使用对比学习对齐多模态特征，引入Sigmoid自注意力加权模块进行自适应特征提取，通过自监督学习利用问题文本增强表示和翻译能力。&lt;h4&gt;主要发现&lt;/h4&gt;SSL-SSAF在CSL-Daily-QA和PHOENIX-2014T-QA数据集上取得最先进性能；易获取的问题文本辅助可达到甚至超过手语标注的性能；可视化结果表明整合对话有助于提高翻译质量。&lt;h4&gt;结论&lt;/h4&gt;通过引入对话上下文特别是问题文本，可以显著改进手语翻译效果，无需依赖难度较高的手语标注，同时达到或超过传统方法性能，为手语翻译领域提供新方向。&lt;h4&gt;翻译&lt;/h4&gt;手语翻译（SLT）弥合了聋人与健听人之间的沟通差距，其中对话为翻译提供了关键的上下文线索。基于这一基础概念，本文提出了基于问题的手语翻译（QB-SLT），这是一个探索对话高效集成的新任务。与手语标注（手语转录）不同，对话自然出现在交流中，且更容易标注。关键挑战在于对齐多模态特征，同时利用问题上下文提高翻译质量。为解决这个问题，我们提出了用于手语翻译的跨模态自监督学习与Sigmoid自注意力加权融合方法（SSL-SSAF）。具体来说，我们在QB-SLT中使用对比学习对齐多模态特征，然后引入Sigmoid自注意力加权（SSAW）模块，从问题和手语序列中进行自适应特征提取。此外，我们通过自监督学习利用可用的问题文本来增强表示和翻译能力。我们在新构建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上评估了我们的方法，SSL-SSAF取得了最先进的性能。值得注意的是，易于获取的问题文本辅助可以达到甚至超过手语标注的性能。此外，可视化结果表明，整合对话有助于提高翻译质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign Language Translation (SLT) bridges the communication gap between deafpeople and hearing people, where dialogue provides crucial contextual cues toaid in translation. Building on this foundational concept, this paper proposesQuestion-based Sign Language Translation (QB-SLT), a novel task that exploresthe efficient integration of dialogue. Unlike gloss (sign languagetranscription) annotations, dialogue naturally occurs in communication and iseasier to annotate. The key challenge lies in aligning multimodality featureswhile leveraging the context of the question to improve translation. To addressthis issue, we propose a cross-modality Self-supervised Learning with SigmoidSelf-attention Weighting (SSL-SSAW) fusion method for sign languagetranslation. Specifically, we employ contrastive learning to alignmultimodality features in QB-SLT, then introduce a Sigmoid Self-attentionWeighting (SSAW) module for adaptive feature extraction from question and signlanguage sequences. Additionally, we leverage available question text throughself-supervised learning to enhance representation and translationcapabilities. We evaluated our approach on newly constructed CSL-Daily-QA andPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,easily accessible question assistance can achieve or even surpass theperformance of gloss assistance. Furthermore, visualization results demonstratethe effectiveness of incorporating dialogue in improving translation quality.</description>
      <author>example@mail.com (Zekang Liu, Wei Feng, Fanhua Shang, Lianyu Hu, Jichao Feng, Liqing Gao)</author>
      <guid isPermaLink="false">2509.14036v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection</title>
      <link>http://arxiv.org/abs/2509.13853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accept ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种单阶段监督对比学习方法（OS-SCL）和TFgram特征，有效解决了无监督异常声音检测中的误报问题，并在DCASE 2020挑战赛上取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督异常声音检测旨在仅使用正常音频数据训练模型来检测未知异常声音。尽管自监督方法有所进步，但在处理来自不同机器的相同类型样本时，仍然存在频繁误报的问题。&lt;h4&gt;目的&lt;/h4&gt;解决无监督异常声音检测中处理来自不同机器的相同类型样本时出现的频繁误报问题。&lt;h4&gt;方法&lt;/h4&gt;提出单阶段监督对比学习（OS-SCL）技术，通过扰动嵌入空间中的特征并采用单阶段噪声监督对比学习方法；同时提出一种名为TFgram的时间-频率特征，从原始音频中提取以捕获异常声音检测的关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64%的AUC、88.42%的pAUC和89.24%的mAUC；使用TFgram特征后，性能提升到95.71%的AUC、90.23%的pAUC和91.23%的mAUC。&lt;h4&gt;结论&lt;/h4&gt;所提出的OS-SCL方法和TFgram特征能有效提高无监督异常声音检测的性能，显著减少了误报情况，源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常声音检测旨在仅使用正常音频数据训练模型来检测未知异常声音。尽管自监督方法有所进步，但在处理来自不同机器的相同类型样本时，仍然存在频繁误报的问题。本文介绍了一种名为单阶段监督对比学习（OS-SCL）的新型训练技术，通过扰动嵌入空间中的特征并采用单阶段噪声监督对比学习方法，显著解决了这一问题。在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64%的AUC、88.42%的pAUC和89.24%的mAUC。此外，还提出了一种名为TFgram的时间-频率特征，从原始音频中提取。该特征能有效捕获异常声音检测的关键信息，最终达到95.71%的AUC、90.23%的pAUC和91.23%的mAUC。源代码可在www.github.com/huangswt/OS-SCL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomalous sound detection aims to detect unknown anomaloussounds by training a model using only normal audio data. Despite advancementsin self-supervised methods, the issue of frequent false alarms when handlingsamples of the same type from different machines remains unresolved. This paperintroduces a novel training technique called one-stage supervised contrastivelearning (OS-SCL), which significantly addresses this problem by perturbingfeatures in the embedding space and employing a one-stage noisy supervisedcontrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.Additionally, a time-frequency feature named TFgram is proposed, which isextracted from raw audio. This feature effectively captures criticalinformation for anomalous sound detection, ultimately achieving 95.71\% AUC,90.23\% pAUC, and 91.23\% mAUC. The source code is available at:\underline{www.github.com/huangswt/OS-SCL}.</description>
      <author>example@mail.com (Shun Huang, Zhihua Fang, Liang He)</author>
      <guid isPermaLink="false">2509.13853v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Masked Feature Modeling Enhances Adaptive Segmentation</title>
      <link>http://arxiv.org/abs/2509.13801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种掩码特征建模方法，用于无监督域适应语义分割任务，通过在特征空间进行掩码和重建，提高了分割性能且不增加计算开销。&lt;h4&gt;背景&lt;/h4&gt;无监督域适应用于语义分割旨在将有标签源域的模型迁移到无标签目标域。虽然对比学习等辅助自监督任务已提高特征区分性，但掩码建模方法在此领域探索不足，主要因架构不兼容和优化目标不一致。&lt;h4&gt;目的&lt;/h4&gt;提出掩码特征建模作为辅助任务，直接在特征空间进行掩码和重建，使其学习目标与主要分割任务对齐，确保与标准架构兼容。&lt;h4&gt;方法&lt;/h4&gt;提出掩码特征建模方法，引入轻量级辅助模块Rebuilder进行联合训练但在推理时丢弃，利用分割解码器对重建特征进行分类，将辅助目标与逐像素预测任务紧密耦合。&lt;h4&gt;主要发现&lt;/h4&gt;在各种架构和UDA基准上的大量实验表明，MFM一致提高分割性能，是一种简单、高效且可推广的策略。&lt;h4&gt;结论&lt;/h4&gt;MFM为无监督域适应语义分割提供了简单、高效且通用的解决方案，通过将辅助目标与主要任务对齐，避免了与主要任务的干扰。&lt;h4&gt;翻译&lt;/h4&gt;无监督域适应用于语义分割旨在将有标签源域的模型迁移到无标签目标域。虽然辅助自监督任务，特别是对比学习，已提高特征区分性，但掩码建模方法在此设置中探索不足，主要因架构不兼容和优化目标不一致。我们提出掩码特征建模，一种在特征空间直接执行特征掩码和重建的新型辅助任务。与现有掩码建模方法不同，MFM将其学习目标与主要分割任务对齐，确保与DeepLab和DAFormer等标准架构兼容，无需修改推理流程。为有效重建，我们引入轻量级辅助模块Rebuilder，该模块联合训练但在推理时丢弃，测试时零计算开销。关键是，MFM利用分割解码器对重建特征进行分类，将辅助目标与逐像素预测任务紧密耦合，避免干扰主要任务。在各种架构和UDA基准上的大量实验表明，MFM一致提高分割性能，为无监督域适应语义分割提供了简单、高效且可推广的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised domain adaptation (UDA) for semantic segmentation aims totransfer models from a labeled source domain to an unlabeled target domain.While auxiliary self-supervised tasks-particularly contrastive learning-haveimproved feature discriminability, masked modeling approaches remainunderexplored in this setting, largely due to architectural incompatibility andmisaligned optimization objectives. We propose Masked Feature Modeling (MFM), anovel auxiliary task that performs feature masking and reconstruction directlyin the feature space. Unlike existing masked modeling methods that reconstructlow-level inputs or perceptual features (e.g., HOG or visual tokens), MFMaligns its learning target with the main segmentation task, ensuringcompatibility with standard architectures like DeepLab and DAFormer withoutmodifying the inference pipeline. To facilitate effective reconstruction, weintroduce a lightweight auxiliary module, Rebuilder, which is trained jointlybut discarded during inference, adding zero computational overhead at testtime. Crucially, MFM leverages the segmentation decoder to classify thereconstructed features, tightly coupling the auxiliary objective with thepixel-wise prediction task to avoid interference with the primary task.Extensive experiments across various architectures and UDA benchmarksdemonstrate that MFM consistently enhances segmentation performance, offering asimple, efficient, and generalizable strategy for unsupervised domain-adaptivesemantic segmentation.</description>
      <author>example@mail.com (Wenlve Zhou, Zhiheng Zhou, Tiantao Xian, Yikui Zhai, Weibin Wu, Biyun Ma)</author>
      <guid isPermaLink="false">2509.13801v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI</title>
      <link>http://arxiv.org/abs/2509.13767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint submitted to ICASSP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VocSegMRI的多模态框架，通过交叉注意力融合整合视频、音频和音韵输入，实现了在实时磁共振成像中更准确的发音结构分割。&lt;h4&gt;背景&lt;/h4&gt;在实时磁共振成像(rtMRI)中准确分割发音结构具有挑战性，因为大多数现有方法几乎完全依赖于视觉线索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够整合声学和音韵信号的多模态框架，以提高发音结构分割的精度和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出VocSegMRI多模态框架，通过交叉注意力融合整合视频、音频和音韵输入实现动态特征对齐，并引入对比学习目标以增强跨模态表示，使模型在推理时音频模态不可用的情况下仍能保持良好性能。&lt;h4&gt;主要发现&lt;/h4&gt;在USC-75 rtMRI数据集子集上评估，VocSegMRI实现了最先进的性能，Dice得分为0.95，第95百分位Hausdorff距离为4.20毫米，优于单模态和多模态基线；消融研究证实了交叉注意力和对比学习对分割精度和鲁棒性的贡献。&lt;h4&gt;结论&lt;/h4&gt;集成多模态建模对于准确的声道分析具有重要价值，VocSegMRI框架能够有效利用多种信号源提高分割性能。&lt;h4&gt;翻译&lt;/h4&gt;在实时磁共振成像中准确分割发音结构仍然具有挑战性，因为大多数现有方法几乎完全依赖于视觉线索。然而，同步的声学和音韵信号提供了补充的上下文，可以丰富视觉信息并提高精度。在本文中，我们引入了VocSegMRI，一个通过交叉注意力融合整合视频、音频和音韵输入的多模态框架，用于动态特征对齐。为了进一步增强跨模态表示，我们纳入了对比学习目标，即使在推理时音频模态不可用，也能提高分割性能。在USC-75 rtMRI数据集的一个子集上评估，我们的方法实现了最先进的性能，Dice得分为0.95，第95百分位Hausdorff距离为4.20毫米，优于单模态和多模态基线。消融研究证实了交叉注意力和对比学习对分割精度和鲁棒性的贡献。这些结果强调了集成多模态建模对准确声道分析的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately segmenting articulatory structures in real-time magnetic resonanceimaging (rtMRI) remains challenging, as most existing methods rely almostentirely on visual cues. Yet synchronized acoustic and phonological signalsprovide complementary context that can enrich visual information and improveprecision. In this paper, we introduce VocSegMRI, a multimodal framework thatintegrates video, audio, and phonological inputs through cross-attention fusionfor dynamic feature alignment. To further enhance cross-modal representation,we incorporate a contrastive learning objective that improves segmentationperformance even when the audio modality is unavailable at inference. Evaluatedon a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-artperformance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.Ablation studies confirm the contributions of cross-attention and contrastivelearning to segmentation precision and robustness. These results highlight thevalue of integrative multimodal modeling for accurate vocal tract analysis.</description>
      <author>example@mail.com (Daiqi Liu, Tomás Arias-Vergara, Johannes Enk, Fangxu Xing, Maureen Stone, Jerry L. Prince, Jana Hutter, Andreas Maier, Jonghye Woo, Paula Andrea Pérez-Toro)</author>
      <guid isPermaLink="false">2509.13767v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization</title>
      <link>http://arxiv.org/abs/2509.13474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为语义增强跨模态地点识别（SCM-PR）的框架，通过结合RGB图像的高层语义与LiDAR地图的几何信息，提高了在无GPS环境中的机器人定位精度，特别是在复杂场景和视点变化的情况下。&lt;h4&gt;背景&lt;/h4&gt;在没有GPS能力的环境中确保机器人精确定位具有挑战性。现有基于RGB的视觉地点识别技术对光照、天气和季节性变化敏感，而现有的跨模态定位方法虽利用RGB图像和3D LiDAR地图的几何特性来减少这些敏感性问题，但在复杂场景、细粒度匹配和视点变化情况下仍表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合高层语义的框架，利用RGB图像在LiDAR地图中实现更鲁棒的定位，解决当前方法在复杂场景、细粒度或高分辨率匹配以及视点变化情况下的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了SCM-PR框架，包含：1) 使用VMamba骨干网络进行RGB图像特征提取；2) 语义感知特征融合（SAFF）模块，同时使用地点描述符和分割掩码；3) 结合语义和几何的LiDAR描述符；4) 在NetVLAD中引入跨模态语义注意力机制提高匹配性能；5) 在对比学习框架中设计了多视图语义几何匹配和语义一致性损失。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和KITTI-360数据集上的实验表明，SCM-PR与其他跨模态地点识别方法相比实现了最先进的性能，特别是在复杂场景和视点变化的情况下表现更优。&lt;h4&gt;结论&lt;/h4&gt;语义信息的整合对于在复杂环境中实现鲁棒的机器人定位至关重要。SCM-PR框架有效解决了现有方法在复杂场景、细粒度匹配和视点变化情况下的局限性，提高了无GPS环境中的定位精度。&lt;h4&gt;翻译&lt;/h4&gt;确保机器人在没有GPS能力的环境中精确定位是一项具有挑战性的任务。视觉地点识别技术有可能实现这一目标，但现有的基于RGB的方法对光照、天气和其他季节性变化敏感。现有的跨模态定位方法利用RGB图像和3D LiDAR地图的几何特性来减少上述敏感性问题。目前，最先进的方法在复杂场景、细粒度或高分辨率匹配以及视点变化的情况下表现不佳。在这项工作中，我们提出了一个名为语义增强跨模态地点识别的框架，该框架结合利用RGB图像的高层语义，在LiDAR地图中实现鲁棒的定位。我们提出的方法引入：用于RGB图像特征提取的VMamba骨干网络；用于同时使用地点描述符和分割掩码的语义感知特征融合模块；结合语义和几何的LiDAR描述符；以及在NetVLAD中引入跨模态语义注意力机制以提高匹配性能。整合语义信息对于设计多视图语义几何匹配和语义一致性损失也至关重要，两者都在对比学习框架中。我们在KITTI和KITTI-360数据集上的实验表明，与其他跨模态地点识别方法相比，SCM-PR实现了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人在没有GPS信号的环境中准确定位的挑战。传统基于RGB图像的定位方法对光照、天气和季节变化敏感，而现有跨模态方法在复杂场景、细粒度匹配和视角变化时表现不佳。这个问题在现实世界中非常重要，因为随着自动驾驶和机器人技术的发展，特别是在GPS信号不可靠或不可用的环境（如城市峡谷、室内、地下空间等），机器人需要可靠的定位能力来安全导航。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有跨模态方法主要依赖低级几何特征，在几何相似但语义不同的场景中会产生歧义。他们思考通过整合高级语义信息可以提供更丰富的上下文理解，增强定位鲁棒性。作者借鉴了多项现有工作：使用VMamba作为RGB图像特征提取的骨干网络，采用预训练3D语义分割模型处理激光雷达数据，利用NetVLAD结构生成全局描述符，以及应用对比学习框架对齐不同模态特征。这些借鉴被创新性地组合并扩展，形成了独特的语义增强跨模态定位框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过深度融合高级语义信息来增强跨模态定位的鲁棒性和准确性，使系统能理解场景中的物体和结构而不仅仅是匹配外观或形状。整体流程分为三部分：1) RGB图像分支使用VMamba骨干网络提取特征，通过SAFF模块生成位置描述符和语义掩码，并应用跨模态语义注意力机制；2) 激光雷达分支将点云转换为距离图像，应用语义分割生成语义标签，构建语义-几何混合描述符，并生成多视角描述符；3) 全局描述符生成和匹配阶段使用NetVLAD结构，结合多视角语义-几何匹配方法，在对比学习框架中使用语义一致性损失进行训练，确保相同语义类别的特征在嵌入空间中聚集。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 语义增强跨模态定位框架(SCM-PR)；2) 语义感知特征融合(SAFF)模块，同时生成位置描述符和语义掩码；3) 语义-几何混合描述符，结合几何和语义信息；4) 跨模态语义注意力机制，使RGB描述符关注激光雷达中对应语义区域；5) 多视角语义-几何匹配方法，同时考虑几何重叠和语义一致性；6) 语义一致性损失，确保相同语义类别的特征在嵌入空间中接近。相比之前工作，SCM-PR深度整合了高级语义信息而非仅依赖低级几何特征，实现了双向语义利用和细粒度语义对齐，在复杂环境和视角变化下表现更佳。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了语义增强跨模态定位框架(SCM-PR)，通过深度整合RGB图像的高级语义信息和激光雷达地图的几何信息，显著提高了在复杂环境和视角变化下的机器人定位准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring accurate localization of robots in environments without GPScapability is a challenging task. Visual Place Recognition (VPR) techniques canpotentially achieve this goal, but existing RGB-based methods are sensitive tochanges in illumination, weather, and other seasonal changes. Existingcross-modal localization methods leverage the geometric properties of RGBimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.Currently, state-of-the-art methods struggle in complex scenes, fine-grained orhigh-resolution matching, and situations where changes can occur in viewpoint.In this work, we introduce a framework we call Semantic-Enhanced Cross-ModalPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGBimages for robust localization in LiDAR maps. Our proposed method introduces: aVMamba backbone for feature extraction of RGB images; a Semantic-Aware FeatureFusion (SAFF) module for using both place descriptors and segmentation masks;LiDAR descriptors that incorporate both semantics and geometry; and across-modal semantic attention mechanism in NetVLAD to improve matching.Incorporating the semantic information also was instrumental in designing aMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both ina contrastive learning framework. Our experimental work on the KITTI andKITTI-360 datasets show that SCM-PR achieves state-of-the-art performancecompared to other cross-modal place recognition methods.</description>
      <author>example@mail.com (Yujia Lin, Nicholas Evans)</author>
      <guid isPermaLink="false">2509.13474v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Supervised and Unsupervised Deep Learning Applied to the Majority Vote Model</title>
      <link>http://arxiv.org/abs/2509.14155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用深度学习技术研究多数投票模型中的连续相变临界性质，结合主成分分析方法，通过监督学习和无监督学习准确识别临界点并估计临界指数。&lt;h4&gt;背景&lt;/h4&gt;多数投票模型的连续相变临界性质研究传统上依赖蒙特卡洛方法，本研究探索使用深度学习技术替代或补充传统方法。&lt;h4&gt;目的&lt;/h4&gt;利用深度学习技术探究多数投票模型的相变临界性质，并验证深度学习方法在相变研究中的有效性和准确性。&lt;h4&gt;方法&lt;/h4&gt;结合深度学习和主成分分析，通过监督学习在动力学蒙特卡洛生成的自旋构型数据上训练密集神经网络；使用主成分分析重现磁化；应用变分自编码器进行深度无监督学习来重构和生成自旋构型。&lt;h4&gt;主要发现&lt;/h4&gt;神经网络能够准确识别正方形和三角形晶格上的临界点；变分自编码器通过损失函数检测相变；真实数据与重构数据之间的关联函数在临界点具有普适性；变分自编码器可作为生成模型产生人工自旋构型。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法能够有效研究相变临界性质，变分自编码器不仅能检测相变，还能作为生成模型产生符合物理规律的人工自旋构型。&lt;h4&gt;翻译&lt;/h4&gt;我们采用深度学习技术研究多数投票模型中连续相变的临界性质。除了深度学习外，还利用主成分分析来分析相变。对于监督学习，我们在通过动力学蒙特卡洛方法生成的自旋构型数据上训练密集神经网络。使用独立模拟的构型数据，神经网络能够准确识别正方形和三角形晶格上的临界点。使用主成分分析的经典无监督学习重现了磁化，并能够估计通常通过蒙特卡洛重要性采样获得的临界指数。此外，使用变分自编码器进行深度无监督学习，它们重构输入自旋构型并生成人工输出。自编码器通过损失函数检测相变，量化了基本数据特征的保留。我们定义了真实数据与重构数据之间的关联函数，发现该关联函数在临界点是普适的。变分自编码器还可作为生成模型，产生人工自旋构型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We employ deep learning techniques to investigate the critical properties ofthe continuous phase transition in the majority vote model. In addition to deeplearning, principal component analysis is utilized to analyze the transition.For supervised learning, dense neural networks are trained on spinconfiguration data generated via the kinetic Monte Carlo method. Usingindependently simulated configuration data, the neural network accuratelyidentifies the critical point on both square and triangular lattices. Classicalunsupervised learning with principal component analysis reproduces themagnetization and enables estimation of critical exponents, typically obtainedvia Monte Carlo importance sampling. Furthermore, deep unsupervised learning isperformed using variational autoencoders, which reconstruct input spinconfigurations and generate artificial outputs. The autoencoders detect thephase transition through the loss function, quantifying the preservation ofessential data features. We define a correlation function between the real andreconstructed data, and find that this correlation function is universal at thecritical point. Variational autoencoders also serve as generative models,producing artificial spin configurations.</description>
      <author>example@mail.com (J. F. Silva Neto, D. S. M. Alencar, L. T. Brito, G. A. Alves, F. W. S. Lima, A. Macedo-Filho, R. S. Ferreira, T. F. A. Alves)</author>
      <guid isPermaLink="false">2509.14155v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data</title>
      <link>http://arxiv.org/abs/2509.13725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于智能手表的系统，用于测量和预测社交焦虑大学生的日常焦虑波动，实现了约60%的准确率，为实时个性化干预提供了可能。&lt;h4&gt;背景&lt;/h4&gt;社交焦虑是一种常见心理健康问题，与学业、社交和职业功能显著相关。其核心特征是在社交情境中的即时焦虑升高，但之前很少有研究测量或预测这种焦虑的日常波动。&lt;h4&gt;目的&lt;/h4&gt;捕捉社交焦虑的日内动态，为设计实时、个性化的干预措施(如JITAIs)提供依据。&lt;h4&gt;方法&lt;/h4&gt;对91名社交焦虑大学生(排除后72名)进行研究，使用定制智能手表系统，平均持续9.03天；每天进行7次生态瞬时评估(EMA)报告状态焦虑；基于外部心率数据开发基础模型，迁移表示并进行微调以生成概率预测；将这些预测与特质水平测量结合到元学习器中。&lt;h4&gt;主要发现&lt;/h4&gt;在数据集中，管道在状态焦虑检测中达到60.4%的平衡准确率；在TILES-18数据集的独立保留集上(10,095次每日EMA)，该方法达到59.1%的平衡准确率，优于先前工作至少7%。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效预测社交焦虑的日常波动，为个性化实时干预提供了可能性，有助于改善社交焦虑人群的日常功能。&lt;h4&gt;翻译&lt;/h4&gt;社交焦虑是一种常见心理健康问题，与学业、社交和职业功能方面的重大挑战相关。一个核心特征是在社交情境中即时(状态)焦虑升高，但之前很少有研究测量或预测这种焦虑在一天中的波动。捕捉这些日内动态对于设计实时、个性化的干预措施(如JITAIs)至关重要。为解决这一空白，我们对社交焦虑大学生(N=91；排除后72人)进行了研究，使用我们定制的基于智能手表的系统，平均9.03天(SD = 2.95)。参与者每天接受七次生态瞬时评估(EMA)以报告状态焦虑。我们在超过10,000天的外部心率数据上开发了基础模型，将其表示迁移到我们的数据集，并进行微调以生成概率预测。这些预测与特质水平测量在元学习器中结合。我们的管道在我们的数据集中实现了60.4%的状态焦虑检测平衡准确率。为了评估泛化能力，我们将训练方法应用于TILES-18数据集的独立保留集-与预训练相同的 dataset。在10,095次每日EMA上，我们的方法实现了59.1%的平衡准确率，优于先前工作至少7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social anxiety is a common mental health condition linked to significantchallenges in academic, social, and occupational functioning. A core feature iselevated momentary (state) anxiety in social situations, yet little prior workhas measured or predicted fluctuations in this anxiety throughout the day.Capturing these intra-day dynamics is critical for designing real-time,personalized interventions such as Just-In-Time Adaptive Interventions(JITAIs). To address this gap, we conducted a study with socially anxiouscollege students (N=91; 72 after exclusions) using our custom smartwatch-basedsystem over an average of 9.03 days (SD = 2.95). Participants received sevenecological momentary assessments (EMAs) per day to report state anxiety. Wedeveloped a base model on over 10,000 days of external heart rate data,transferred its representations to our dataset, and fine-tuned it to generateprobabilistic predictions. These were combined with trait-level measures in ameta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxietydetection in our dataset. To evaluate generalizability, we applied the trainingapproach to a separate hold-out set from the TILES-18 dataset-the same datasetused for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%balanced accuracy, outperforming prior work by at least 7%.</description>
      <author>example@mail.com (Md Sabbir Ahmed, Noah French, Mark Rucker, Zhiyuan Wang, Taylor Myers-Brower, Kaitlyn Petz, Mehdi Boukhechba, Bethany A. Teachman, Laura E. Barnes)</author>
      <guid isPermaLink="false">2509.13725v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models</title>
      <link>http://arxiv.org/abs/2509.13706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种自然语言处理工具，用于自动检测放射肿瘤学中的高严重程度事件报告，解决了人工审查耗时且需要专业知识的问题。&lt;h4&gt;背景&lt;/h4&gt;事件报告是医疗保健中安全和质量改进的重要工具，但人工审查耗时且需要专业知识。&lt;h4&gt;目的&lt;/h4&gt;展示一个自然语言处理（NLP）筛查工具，用于在两个医疗机构中检测放射肿瘤学中的高严重程度事件报告。&lt;h4&gt;方法&lt;/h4&gt;使用两个文本来训练和评估NLP模型：7,094份来自我们机构（Inst.）的报告和571份来自IAEA SAFRON（SF）的报告，所有报告都由临床内容专家标记了严重程度分数。训练和评估了两种类型的模型：基线支持向量机（SVM）和BlueBERT（一种在PubMed摘要和住院患者数据上预训练的大型语言模型）。通过两种方式评估了模型的泛化能力：使用在Inst.-train上训练的模型在SF-test上进行评估；训练了一个BlueBERT_TRANSFER模型，先在Inst-train上微调，然后在SF-train上再次微调，最后在SF-test集上测试。为了进一步分析模型性能，还检查了我们机构数据集中的59份报告子集，这些报告经过人工编辑以提高清晰度。&lt;h4&gt;主要发现&lt;/h4&gt;在Inst.测试上的分类性能，SVM达到AUROC 0.82，BlueBERT达到0.81。在没有跨机构迁移学习的情况下，在SF测试上的性能有限，SVM仅为0.42，BlueBERT为0.56。在两个数据集上均进行微调的BlueBERT_TRANSFER将SF测试的性能提高到AUROC 0.78。SVM和BlueBERT_TRANSFER模型在人工编辑的Inst.报告上的性能（AUROC 0.85和0.74）与人类性能（AUROC 0.81）相似。&lt;h4&gt;结论&lt;/h4&gt;成功地在放射肿瘤学中心的事件报告文本上开发了跨机构NLP模型。这些模型能够在经过整理的数据集上像人类一样检测高严重程度报告。&lt;h4&gt;翻译&lt;/h4&gt;目的：事件报告是医疗保健中安全和质量改进的重要工具，但人工审查耗时且需要专业知识。在此，我们展示了一个自然语言处理（NLP）筛查工具，用于在两个医疗机构中检测放射肿瘤学中的高严重程度事件报告。方法与材料：我们使用两个文本来训练和评估我们的NLP模型：7,094份来自我们机构（Inst.）的报告和571份来自IAEA SAFRON（SF）的报告，所有报告都由临床内容专家标记了严重程度分数。我们训练和评估了两种类型的模型：基线支持向量机（SVM）和BlueBERT（一种在PubMed摘要和住院患者数据上预训练的大型语言模型）。我们通过两种方式评估了模型的泛化能力。首先，我们评估了使用Inst.-train训练后在SF-test上测试的模型。其次，我们训练了一个BlueBERT_TRANSFER模型，先在Inst.-train上微调，然后在SF-train上再次微调，最后在SF-test集上测试。为了进一步分析模型性能，我们还检查了我们机构数据集中的59份报告子集，这些报告经过人工编辑以提高清晰度。结果：在Inst.测试上的分类性能，SVM达到AUROC 0.82，BlueBERT达到0.81。在没有跨机构迁移学习的情况下，在SF测试上的性能有限，SVM仅为0.42，BlueBERT为0.56。在两个数据集上均进行微调的BlueBERT_TRANSFER将SF测试的性能提高到AUROC 0.78。SVM和BlueBERT_TRANSFER模型在人工编辑的Inst.报告上的性能（AUROC 0.85和0.74）与人类性能（AUROC 0.81）相似。结论：总之，我们成功地在放射肿瘤学中心的事件报告文本上开发了跨机构NLP模型。这些模型能够在经过整理的数据集上像人类一样检测高严重程度报告。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PURPOSE: Incident reports are an important tool for safety and qualityimprovement in healthcare, but manual review is time-consuming and requiressubject matter expertise. Here we present a natural language processing (NLP)screening tool to detect high-severity incident reports in radiation oncologyacross two institutions.  METHODS AND MATERIALS: We used two text datasets to train and evaluate ourNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEASAFRON (SF), all of which had severity scores labeled by clinical contentexperts. We trained and evaluated two types of models: baseline support vectormachines (SVM) and BlueBERT which is a large language model pretrained onPubMed abstracts and hospitalized patient data. We assessed forgeneralizability of our model in two ways. First, we evaluated models trainedusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model thatwas first fine-tuned on Inst.-train then on SF-train before testing on SF-testset. To further analyze model performance, we also examined a subset of 59reports from our Inst. dataset, which were manually edited for clarity.  RESULTS Classification performance on the Inst. test achieved AUROC 0.82using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,improved the performance on SF test to AUROC 0.78. Performance of SVM, andBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and0.74) was similar to human performance (AUROC 0.81).  CONCLUSION: In summary, we successfully developed cross-institution NLPmodels on incident report text from radiation oncology centers. These modelswere able to detect high-severity reports similarly to humans on a curateddataset.</description>
      <author>example@mail.com (Peter Beidler, Mark Nguyen, Kevin Lybarger, Ola Holmberg, Eric Ford, John Kang)</author>
      <guid isPermaLink="false">2509.13706v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning</title>
      <link>http://arxiv.org/abs/2509.13624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Camera-ready version. Accepted to appear in the proceedings of the  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个分析框架，通过构建迁移学习矩阵和降维技术，研究了大型语言模型在跨任务迁移学习中的交互作用，揭示了影响模型性能的关键因素。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型被广泛应用于各种任务，但这些任务通常是模型在训练过程中未遇到的。为所有任务枚举和获取高质量训练数据是不可行的，因此需要依赖迁移学习，使用具有不同特性的数据集，并处理分布外请求。&lt;h4&gt;目的&lt;/h4&gt;提出一个分析框架，用于研究跨任务交互，理解迁移学习的效果和副作用。&lt;h4&gt;方法&lt;/h4&gt;构建迁移学习矩阵，使用降维技术，训练并分析10个模型，识别潜在能力（如推理、情感分类、自然语言理解、算术等），发现迁移学习的副作用。&lt;h4&gt;主要发现&lt;/h4&gt;性能提升通常无法基于表面层面的数据集相似性或源数据质量来解释，源数据集的隐藏统计因素（如类别分布和生成长度倾向）更有影响力，特定的语言特征也起着重要作用。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了对迁移学习复杂动态的见解，为更可预测和有效的LLM适应铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型正被越来越多地部署到各种应用中。这通常包括模型在训练过程中未遇到的任务。这意味着枚举和获取所有任务的高质量训练数据是不可行的。因此，我们通常需要依赖使用具有不同特性的数据集进行迁移学习，并预期分布外的请求。受这一实际需求的启发，我们提出了一个分析框架，通过构建迁移学习矩阵和降维技术，来剖析这些跨任务交互。我们训练并分析了10个模型，以识别潜在能力（如推理、情感分类、自然语言理解、算术），并发现了迁移学习的副作用。我们的研究揭示，性能提升通常无法基于表面层面的数据集相似性或源数据质量来解释。相反，源数据集的隐藏统计因素，如类别分布和生成长度倾向，以及特定的语言特征，实际上更有影响力。这项工作为迁移学习的复杂动态提供了见解，为更可预测和有效的LLM适应铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models are increasingly deployed across diverse applications.This often includes tasks LLMs have not encountered during training. Thisimplies that enumerating and obtaining the high-quality training data for alltasks is infeasible. Thus, we often need to rely on transfer learning usingdatasets with different characteristics, and anticipate out-of-distributionrequests. Motivated by this practical need, we propose an analysis framework,building a transfer learning matrix and dimensionality reduction, to dissectthese cross-task interactions. We train and analyze 10 models to identifylatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)and discover the side effects of the transfer learning. Our findings revealthat performance improvements often defy explanations based on surface-leveldataset similarity or source data quality. Instead, hidden statistical factorsof the source dataset, such as class distribution and generation lengthproclivities, alongside specific linguistic features, are actually moreinfluential. This work offers insights into the complex dynamics of transferlearning, paving the way for more predictable and effective LLM adaptation.</description>
      <author>example@mail.com (Shambhavi Krishna, Atharva Naik, Chaitali Agarwal, Sudharshan Govindan, Taesung Lee, Haw-Shiuan Chang)</author>
      <guid isPermaLink="false">2509.13624v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Speaker-Independent Dysarthric Speech Severity Classification with DSSCNet and Cross-Corpus Adaptation</title>
      <link>http://arxiv.org/abs/2509.13442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Speaker-independent experiments on classification of dysarthric  speech severity&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了DSSCNet，一种新型深度神经网络架构，用于构音障碍言语的严重程度分类。该模型结合了卷积、挤压激励(SE)和残差网络，能够从梅尔频谱图中提取判别性表示。研究还提出了跨语料微调框架，并在两个基准语料库上进行了评估，结果显示该方法优于现有最先进技术。&lt;h4&gt;背景&lt;/h4&gt;构音障碍言语严重程度分类对于运动性言语障碍患者的客观临床评估和进展监测至关重要。尽管先前方法已解决此任务，但在说话人独立(SID)场景中实现强大泛化能力仍具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效对构音障碍言语严重程度进行分类的方法，特别是在说话人独立场景中实现更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1) 提出DSSCNet架构，结合卷积、Squeeze-Excitation和残差网络；2) 使用SE块选择性关注重要特征；3) 提出基于检测的迁移学习方法改编的跨语料微调框架；4) 在TORGO和UA-Speech语料库上评估；5) 使用OSPS和LOSO评估协议。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在OSPS协议下，DSSCNet在TORGO和UA-Speech上分别达到56.84%和62.62%的准确率；2) 在LOSO设置下，分别达到63.47%和64.18%的准确率；3) 微调后性能显著提高：OSPS下分别达到75.80%和68.25%，LOSO下分别达到77.76%和79.44%；4) 所有结果均优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明了DSSCNet在跨不同构音障碍言语数据集进行细粒度严重程度分类方面的有效性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;构音障碍言语严重程度分类对于运动性言语障碍患者的客观临床评估和进展监测至关重要。尽管先前的方法已经解决了这一任务，但在说话人独立(SID)场景中实现强大的泛化能力仍然具有挑战性。这项工作介绍了DSSCNet，一种新型深度神经网络架构，它结合了卷积、挤压激励(SE)和残差网络，帮助它从梅尔频谱图中提取构音障碍言语的判别性表示。SE块的添加选择性关注构音障碍言语的重要特征，从而减少损失并提高整体模型性能。我们还提出了一个用于严重程度分类的跨语料微调框架，该框架基于检测的迁移学习方法改编而成。DSSCNet在两个基准构音障碍言语语料库(TORGO和UA-Speech)上进行了评估，使用说话人独立评估协议：每严重程度一位说话人(OSPS)和留一说话人法(LOSO)。在OSPS和LOSO设置下，DSSCNet在TORGO上分别达到56.84%和63.47%的准确率，在UA-Speech上分别达到62.62%和64.18%的准确率，优于现有的最先进方法。微调后，性能显著提高，在OSPS下，DSSCNet在TORGO和UA-Speech上分别达到75.80%和68.25%的准确率，在LOSO下分别达到77.76%和79.44%。这些结果证明了DSSCNet在跨不同构音障碍言语数据集进行细粒度严重程度分类方面的有效性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dysarthric speech severity classification is crucial for objective clinicalassessment and progress monitoring in individuals with motor speech disorders.Although prior methods have addressed this task, achieving robustgeneralization in speaker-independent (SID) scenarios remains challenging. Thiswork introduces DSSCNet, a novel deep neural architecture that combinesConvolutional, Squeeze-Excitation (SE), and Residual network, helping itextract discriminative representations of dysarthric speech from melspectrograms. The addition of SE block selectively focuses on the importantfeatures of the dysarthric speech, thereby minimizing loss and enhancingoverall model performance. We also propose a cross-corpus fine-tuning frameworkfor severity classification, adapted from detection-based transfer learningapproaches. DSSCNet is evaluated on two benchmark dysarthric speech corpora:TORGO and UA-Speech under speaker-independent evaluation protocols:One-Speaker-Per-Severity (OSPS) and Leave-One-Speaker-Out (LOSO) protocols.DSSCNet achieves accuracies of 56.84% and 62.62% under OSPS and 63.47% and64.18% under LOSO setting on TORGO and UA-Speech respectively outperformingexisting state-of-the-art methods. Upon fine-tuning, the performance improvessubstantially, with DSSCNet achieving up to 75.80% accuracy on TORGO and 68.25%on UA-Speech in OSPS, and up to 77.76% and 79.44%, respectively, in LOSO. Theseresults demonstrate the effectiveness and generalizability of DSSCNet forfine-grained severity classification across diverse dysarthric speech datasets.</description>
      <author>example@mail.com (Arnab Kumar Roy, Hemant Kumar Kathania, Paban Sapkota)</author>
      <guid isPermaLink="false">2509.13442v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>HAM: Hierarchical Adapter Merging for Scalable Continual Learning</title>
      <link>http://arxiv.org/abs/2509.13211v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HAM是一种新的持续学习框架，通过动态合并不同任务的适配器来解决灾难性遗忘问题，在处理大量任务时表现优异。&lt;h4&gt;背景&lt;/h4&gt;持续学习是人类认知的重要能力，但对当前深度学习模型构成挑战。主要问题是新知识会干扰已学习信息，导致灾难性遗忘。大型预训练模型部分缓解此问题，但在面对新数据分布时表现不佳。参数高效微调方法如LoRA能有效适应新知识，但在扩展到动态学习场景和长任务序列时仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效扩展到更多任务的方法，提高持续学习效率，减少每个任务使用一个适配器带来的复杂性和干扰可能性。&lt;h4&gt;方法&lt;/h4&gt;提出分层适配器合并（HAM）框架，在训练期间动态组合来自不同任务的适配器。HAM维护一组固定的组，分层次整合新适配器。对于每个任务，训练一个低秩适配器和一个重要性标量，然后基于适配器相似性动态分组任务。在每个组内，适配器被修剪、缩放和合并，促进相关任务间的迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;HAM能够有效扩展，管理比竞争基线更多的任务，同时提高效率。在三个视觉基准上的广泛实验表明，HAM显著优于最先进的方法，随着任务数量的增加，其优势尤其明显。&lt;h4&gt;结论&lt;/h4&gt;HAM框架为持续学习提供了一种有效解决方案，特别是在处理大量任务时。通过动态合并适配器，HAM能够更好地管理任务间的知识迁移，减少灾难性遗忘。&lt;h4&gt;翻译&lt;/h4&gt;持续学习是人类认知的基本能力，然而它对当前的深度学习模型提出了重大挑战。主要问题是新知识可能会干扰已学习的信息，导致模型为了新知识而遗忘早期知识，这种现象被称为灾难性遗忘。尽管大型预训练模型可以通过利用其现有知识和过参数化部分缓解遗忘问题，但当面对新的数据分布时，它们往往表现不佳。参数高效微调（PEFT）方法，如LoRA，能够有效地适应新知识。然而，在扩展到动态学习场景和长任务序列时，它们仍然面临挑战，因为为每个任务维护一个适配器会引入复杂性并增加干扰的可能性。在本文中，我们引入了分层适配器合并（HAM），这是一个新颖的框架，在训练期间动态组合来自不同任务的适配器。这种方法使HAM能够有效扩展，允许它以更高的效率管理比竞争基线更多的任务。为此，HAM维护一组固定的组，这些组分层次地整合新适配器。对于每个任务，HAM训练一个低秩适配器和一个重要性标量，然后基于适配器相似性动态分组任务。在每个组内，适配器被修剪、缩放和合并，促进相关任务之间的迁移学习。在三个视觉基准上的广泛实验表明，HAM显著优于最先进的方法，特别是随着任务数量的增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning is an essential capability of human cognition, yet itposes significant challenges for current deep learning models. The primaryissue is that new knowledge can interfere with previously learned information,causing the model to forget earlier knowledge in favor of the new, a phenomenonknown as catastrophic forgetting. Although large pre-trained models canpartially mitigate forgetting by leveraging their existing knowledge andover-parameterization, they often struggle when confronted with novel datadistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,enable efficient adaptation to new knowledge. However, they still facechallenges in scaling to dynamic learning scenarios and long sequences oftasks, as maintaining one adapter per task introduces complexity and increasesthe potential for interference. In this paper, we introduce HierarchicalAdapters Merging (HAM), a novel framework that dynamically combines adaptersfrom different tasks during training. This approach enables HAM to scaleeffectively, allowing it to manage more tasks than competing baselines withimproved efficiency. To achieve this, HAM maintains a fixed set of groups thathierarchically consolidate new adapters. For each task, HAM trains a low-rankadapter along with an importance scalar, then dynamically groups tasks based onadapter similarity. Within each group, adapters are pruned, scaled and merge,facilitating transfer learning between related tasks. Extensive experiments onthree vision benchmarks show that HAM significantly outperformsstate-of-the-art methods, particularly as the number of tasks increases.</description>
      <author>example@mail.com (Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2509.13211v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds</title>
      <link>http://arxiv.org/abs/2509.13390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to: Mechanical Systems and Signal Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于领域知识的模型选择方法，通过使用代理异常来改进汽车舱内声音异常检测模型的性能评估。&lt;h4&gt;背景&lt;/h4&gt;在汽车舱内声音异常检测中，由于标记的故障数据稀缺或完全缺失，这项任务更适合作为无监督学习问题而非监督学习问题。然而，在无监督设置中，由于缺乏标记的故障样本进行验证，以及常用指标的可靠性有限，有效的模型选择仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;为克服无监督学习环境下模型选择的挑战，提出一种基于领域知识的模型选择方法，通过使用代理异常来支持模型选择。&lt;h4&gt;方法&lt;/h4&gt;通过健康频谱图的结构化扰动构建代理异常，将其用于验证集以支持模型选择。在一个包含健康和故障舱内声音的高保真电动汽车数据集上评估该方法，该数据集涵盖了五种代表性故障类型：不平衡、调制、啸叫、风声和脉冲宽度调制。&lt;h4&gt;主要发现&lt;/h4&gt;在五种故障案例上的实验评估表明，使用代理异常选择最优模型显著优于传统的模型选择策略。&lt;h4&gt;结论&lt;/h4&gt;基于领域知识的模型选择方法，使用代理异常，能够有效解决无监督学习环境下汽车舱内声音异常检测中的模型选择挑战，显著提高了模型选择的性能。&lt;h4&gt;翻译&lt;/h4&gt;汽车舱内声音异常的检测对于确保车辆质量和维护乘客舒适度至关重要。在许多现实场景中，由于标记的故障数据稀缺或完全缺失，这项任务更适合作为无监督学习问题而非监督学习案例。在这种无监督设置中，模型仅在健康样本上训练，并将异常检测为正常行为的偏差。然而，由于缺乏标记的故障样本进行验证以及常用指标（如验证重建误差）的可靠性有限，有效的模型选择仍然是一个重大挑战。为克服这些限制，提出了一种基于领域知识的模型选择方法，其中通过健康频谱图的结构化扰动构建的代理异常用于验证集以支持模型选择。该方法在一个包含健康和故障舱内声音的高保真电动汽车数据集上进行了评估，该数据集涵盖了五种代表性故障类型：不平衡、调制、啸叫、风声和脉冲宽度调制。这个数据集使用先进的声音合成技术生成，并通过专家评审评估验证，已公开以促进进一步研究。在五种故障案例上的实验评估表明，使用代理异常选择最优模型显著优于传统的模型选择策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The detection of anomalies in automotive cabin sounds is critical forensuring vehicle quality and maintaining passenger comfort. In many real-worldsettings, this task is more appropriately framed as an unsupervised learningproblem rather than the supervised case due to the scarcity or complete absenceof labeled faulty data. In such an unsupervised setting, the model is trainedexclusively on healthy samples and detects anomalies as deviations from normalbehavior. However, in the absence of labeled faulty samples for validation andthe limited reliability of commonly used metrics, such as validationreconstruction error, effective model selection remains a significantchallenge. To overcome these limitations, a domain-knowledge-informed approachfor model selection is proposed, in which proxy-anomalies engineered throughstructured perturbations of healthy spectrograms are used in the validation setto support model selection. The proposed methodology is evaluated on ahigh-fidelity electric vehicle dataset comprising healthy and faulty cabinsounds across five representative fault types viz., Imbalance, Modulation,Whine, Wind, and Pulse Width Modulation. This dataset, generated using advancedsound synthesis techniques, and validated via expert jury assessments, has beenmade publicly available to facilitate further research. Experimentalevaluations on the five fault cases demonstrate the selection of optimal modelsusing proxy-anomalies, significantly outperform conventional model selectionstrategies.</description>
      <author>example@mail.com (Deepti Kunte, Bram Cornelis, Claudio Colangeli, Karl Janssens, Brecht Van Baelen, Konstantinos Gryllias)</author>
      <guid isPermaLink="false">2509.13390v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.13907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为White Aggregation and Restoration Module (WARM)的新型原型生成方法，用于解决少样本3D点云分割问题。该方法通过白化和彩色化变换之间的交叉注意力机制，解决了支持特征与原型令牌之间的分布不对齐问题，从而生成更具代表性的原型，显著提升了分割性能。&lt;h4&gt;背景&lt;/h4&gt;少样本3D点云分割(FS-PCS)旨在仅有少量标记样本的情况下预测未标记点云的每个点标签。现有方法使用传统算法构建原型，但初始随机性显著影响性能，且原型生成过程缺乏深入探索。&lt;h4&gt;目的&lt;/h4&gt;调查一种基于注意力机制的先进原型生成方法，以提高FS-PCS性能。&lt;h4&gt;方法&lt;/h4&gt;提出White Aggregation and Restoration Module (WARM)，通过在白化和彩色化变换之间夹入交叉注意力来解决分布不对齐问题。白化在注意力过程前将支持特征与原型令牌对齐，彩色化随后恢复原始分布到已处理令牌，使注意力更加鲁棒，捕获支持特征间的语义关系生成代表性原型。&lt;h4&gt;主要发现&lt;/h4&gt;现有简单模块在可学习的原型令牌和支持特征之间存在分布差距；WARM方法能有效解决这种分布不对齐问题，生成更具代表性的原型。&lt;h4&gt;结论&lt;/h4&gt;在多个FS-PCS基准测试上，该方法以显著优势实现了最先进的性能，通过大量实验证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;少样本3D点云分割(FS-PCS)旨在仅有少量标记样本的情况下预测未标记点云的每个点标签。为从有限的支持集中提取判别性表示，现有方法使用最远点采样等传统算法构建原型。然而，我们指出其初始随机性显著影响FS-PCS性能，且尽管原型生成过程被广泛使用，它仍然缺乏深入探索。这促使我们调查一种基于注意力机制的先进原型生成方法。尽管有潜力，我们发现简单模块在可学习的原型令牌和支持特征之间存在分布差距。为克服这一问题，我们提出了白化聚合与恢复模块(WARM)，通过在白化和彩色化变换之间夹入交叉注意力来解决不对齐问题。具体而言，白化在注意力过程前将支持特征与原型令牌对齐，随后彩色化将原始分布恢复到已处理令牌。这种简单而有效的设计使注意力更加鲁棒，从而通过捕获支持特征之间的语义关系生成代表性原型。我们的方法在多个FS-PCS基准测试上以显著优势实现了最先进的性能，通过大量实验证明了其有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决少样本3D点云语义分割(FS-PCS)中的原型生成问题。传统方法使用最远点采样(FPS)等算法构建原型，但这些方法存在初始随机性大、性能不稳定的问题。这个问题很重要，因为3D点云语义分割在自动驾驶、机器人等领域有广泛应用，而标记3D点云数据需要大量人工劳动，成本高昂。少样本学习可以减少对大量标记数据的依赖，原型生成作为FS-PCS的关键步骤，其质量直接影响分割性能，现有方法的不稳定性限制了FS-PCS的实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出传统FPS方法的局限性，特别是其初始随机性导致的性能不稳定。然后考虑使用注意力机制作为替代方案，因为注意力在其他少样本下游任务中表现出色。研究发现vanilla交叉注意力存在分布差距问题：可学习的原型令牌与支持特征之间分布不匹配。通过分析特征空间的分布差异，作者设计了白化和着色变换来解决这一问题。该方法借鉴了图像领域DETR和Mask2Former的注意力机制，以及标准的ZCA白化技术，并受到原型网络的启发，但将这些技术创新性地应用于3D点云FS-PCS场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'白化-交叉注意力-着色'三阶段流程解决原型令牌与支持特征之间的分布不匹配问题。白化阶段暂时移除支持特征的分布统计信息，使其与原型令牌对齐；交叉注意力阶段在共享空间中，原型令牌能够基于语义关系而非空间距离来聚合特征；着色阶段恢复原始分布统计信息，保留支持特征的固有特性。整体流程包括：使用主干网络提取特征；将支持特征分为前景和背景；对每类应用ZCA白化；使用白化后的特征通过交叉注意力生成临时原型；应用着色变换恢复原始分布；最后将查询点分配给最近原型进行分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出WARM模块解决FS-PCS中原型生成问题；2) 首次将白化和着色变换引入注意力机制，解决分布不匹配；3) 通过白化移除特征通道间相关性，使优化更稳定；4) 通过着色恢复原始特征特性。相比之前的工作：不同于FPS方法的随机性和不稳定性，WARM是确定性的；不同于简单注意力无法处理分布不匹配问题，WARM通过白化-着色变换解决了这一问题；不同于其他FS-PCS方法主要关注查询适应，WARM专注于改进原型生成本身；不同于3D领域通常使用非可学习令牌，WARM实现了可学习令牌的应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了WARM模块，通过白化和着色变换解决了少样本3D点云语义分割中原型生成的不稳定性和分布不匹配问题，显著提升了分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-pointlabels for an unlabeled point cloud, given only a few labeled examples. Toextract discriminative representations from the limited support set, existingmethods have constructed prototypes using conventional algorithms such asfarthest point sampling. However, we point out that its initial randomnesssignificantly affects FS-PCS performance and that the prototype generationprocess remains underexplored despite its prevalence. This motivates us toinvestigate an advanced prototype generation method based on attentionmechanism. Despite its potential, we found that vanilla module suffers from thedistributional gap between learnable prototypical tokens and support features.To overcome this, we propose White Aggregation and Restoration Module (WARM),which resolves the misalignment by sandwiching cross-attention betweenwhitening and coloring transformations. Specifically, whitening aligns thesupport features to prototypical tokens before attention process, andsubsequently coloring restores the original distribution to the attendedtokens. This simple yet effective design enables robust attention, therebygenerating representative prototypes by capturing the semantic relationshipsamong support features. Our method achieves state-of-the-art performance with asignificant margin on multiple FS-PCS benchmarks, demonstrating itseffectiveness through extensive experiments.</description>
      <author>example@mail.com (Jiyun Im, SuBeen Lee, Miso Lee, Jae-Pil Heo)</author>
      <guid isPermaLink="false">2509.13907v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Algorithm of the GLMY Homology on Digraphs</title>
      <link>http://arxiv.org/abs/2509.13862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对GLMY同调的量子算法，在拓扑数据分析领域比最佳经典算法具有显著优势，在一般情况下提供二次加速，在特定情况下提供指数级加速。&lt;h4&gt;背景&lt;/h4&gt;量子算法在拓扑数据分析中比最佳经典算法有明显优势。GLMY同调是由Alexander Grigor'yan等人引入的，不同于传统的点云上的单纯复形，它是在有向图上定义的拓扑数据分析新兴领域，最近受到越来越多的关注。&lt;h4&gt;目的&lt;/h4&gt;提出一个针对GLMY同调的量子算法，使其比最佳经典算法具有显著优势。&lt;h4&gt;方法&lt;/h4&gt;设计了一个通用编码协议，用于GLMY同调的量子态和边界算子；同时证明了GLMY同调的一个性质，为量子算法提供了理论保证。&lt;h4&gt;主要发现&lt;/h4&gt;GLMY同调的量子算法在一般情况下提供了二次加速，在输入数据以路径规范形式给出时提供了指数级的量子优势。&lt;h4&gt;结论&lt;/h4&gt;量子算法在GLMY同调计算中具有显著优势，为拓扑数据分析领域提供了新的计算方法。&lt;h4&gt;翻译&lt;/h4&gt;拓扑数据分析的量子算法比最佳经典算法具有显著优势。由Alexander Grigor'yan、Yong Lin、Yuri Muranov和Shing-Tung Yau引入的GLMY同调，与之前的点云上的单纯复形不同，它是在有向图上定义的，是拓扑数据分析中的一个新兴领域，最近吸引了越来越多的关注。我们提出了一个针对GLMY同调的量子算法，比最佳经典算法有显著优势。我们设计了GLMY同调在有向图上的量子态和边界算子的通用编码协议。并且证明了GLMY同调的一个性质，为量子算法提供了理论保证。GLMY同调的量子算法在一般情况下给出了二次加速，在输入数据以路径规范形式给出时给出了指数级的量子优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum algorithms for topological data analysis provide significantadvantage over the best classical algorithm. Different from the previoussimplical complex on points cloud, the GLMY homology introduced by AlexanderGrigor'yan, Yong Lin, Yuri Muranov and Shing-Tung Yau, is defined on digraphand is a arising realm in Topological Data Analysis (TDA), which attracts moreand more attention recently. We propose a quantum algorithm for the GLMYhomology with significant advantage over the best classical algorithm. Wedesign a universal encoding protocol for the quantum states and boundaryoperators of GLMY homology on digraphs. And a property of the GLMY homology isproved for the theoretical guarantee of the quantum algorithm. The quantumalgorithm for GLMY homology gives a quadratic speedup in general cases, and itgives an exponential quantum advantage in the case of the input data is givenas a specification of paths.</description>
      <author>example@mail.com (Yunpeng Zi, Muchun Yang, D. L. Zhou)</author>
      <guid isPermaLink="false">2509.13862v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap</title>
      <link>http://arxiv.org/abs/2509.13857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出InterKey，一种利用道路交叉路口作为独特地标的跨模态框架，实现自动驾驶车辆在GNSS受限环境中的全球定位。&lt;h4&gt;背景&lt;/h4&gt;可靠的全球定位对自动驾驶车辆至关重要，特别是在GNSS信号减弱或不可用的环境中。高清地图提供准确信息但成本高，限制了可扩展性；OpenStreetMap免费可用但抽象粗糙，与传感器数据匹配困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用道路交叉路口作为独特地标的全球定位方法，解决高清地图成本高和OSM抽象粗糙的问题。&lt;h4&gt;方法&lt;/h4&gt;提出InterKey跨模态框架，通过联合编码点云和OSM的道路和建筑物印记构建紧凑二进制描述符，并引入差异缓解、方向确定和面积均衡采样策略实现鲁棒跨模态匹配。&lt;h4&gt;主要发现&lt;/h4&gt;KITTI数据集实验表明，InterKey实现最先进准确性，大幅优于最近基线方法，框架可扩展至能生成密集结构点云的传感器。&lt;h4&gt;结论&lt;/h4&gt;InterKey为车辆定位提供了一种可扩展、经济高效的鲁棒解决方案。&lt;h4&gt;翻译&lt;/h4&gt;可靠的全球定位对自动驾驶车辆至关重要，特别是在GNSS信号减弱或不可用的环境中，如城市峡谷和隧道。虽然高清地图提供准确的先验信息，但数据收集、地图构建和维护的成本限制了其可扩展性。OpenStreetMap提供了免费且全球可用的替代方案，但其粗略的抽象表示给与传感器数据的匹配带来挑战。我们提出InterKey，一种利用道路交叉路口作为独特地标的跨模态框架。我们的方法通过联合编码来自点云和OSM的道路和建筑物印记来构建紧凑的二进制描述符。为了弥合模态差距，我们引入了差异缓解、方向确定和面积均衡采样策略，实现了鲁棒的跨模态匹配。KITTI数据集上的实验表明，InterKey实现了最先进的准确性，以较大优势优于最近的基线方法。该框架可以扩展到能够生成密集结构点云的传感器，为车辆定位提供了一种可扩展且经济高效的鲁棒解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在GNSS信号弱或不可用环境（如城市峡谷、隧道）中，自动驾驶汽车的全局定位问题。这个问题很重要，因为GNSS在这些环境中容易受到多路径效应和信号遮挡的影响导致定位不可靠，而高精度地图虽准确但成本高昂限制了可扩展性。准确的全局定位对自动驾驶汽车的初始化、故障恢复、路径规划等关键任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到GNSS在复杂环境中的局限性和高精度地图的成本问题，选择道路交叉口作为关键特征，因其具有独特几何配置且可从LiDAR数据可靠提取。他们结合道路和建筑物信息提高描述符区分度，并通过差异缓解、方向确定和面积等采样策略解决模态差距。该方法借鉴了作者之前关于交叉口检测的研究，参考了Scan Context等点云识别方法，以及OSM Context等点云到OSM的匹配方法，但创新性地专注于交叉口特征的跨模态匹配。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用道路交叉口作为点云和OSM之间的跨模态关键点，联合编码道路和建筑物印记生成紧凑二进制描述符，并通过专门策略解决模态差距。整体流程分为四步：1)从OSM提取交叉口节点并生成道路和建筑物印记；2)处理点云数据生成俯视图道路和建筑物印记并检测交叉口；3)通过差异缓解、方向确定和形状编码生成二进制描述符；4)匹配描述符检索最佳OSM交叉口并计算车辆全局姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首个专注于跨模态交叉口匹配的框架、结合道路和建筑物的描述符、模态差异缓解方案、方向确定策略和面积等采样方法。相比之前工作，不同之处在于：结合道路和建筑物信息而非仅依赖建筑物轮廓，提高区分度；采用面积等采样模式而非线性增长，能均匀捕捉周围形状；专注于交叉口这一独特特征减少搜索空间；专门设计解决点云和OSM间的模态差距，而非仅处理点云到点云匹配。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InterKey通过利用道路交叉口作为跨模态关键点，结合道路和建筑物信息生成紧凑描述符，并采用创新的采样策略，实现了在GNSS受限环境中基于免费OpenStreetMap的高效、准确的全车定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable global localization is critical for autonomous vehicles, especiallyin environments where GNSS is degraded or unavailable, such as urban canyonsand tunnels. Although high-definition (HD) maps provide accurate priors, thecost of data collection, map construction, and maintenance limits scalability.OpenStreetMap (OSM) offers a free and globally available alternative, but itscoarse abstraction poses challenges for matching with sensor data. We proposeInterKey, a cross-modal framework that leverages road intersections asdistinctive landmarks for global localization. Our method constructs compactbinary descriptors by jointly encoding road and building imprints from pointclouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,orientation determination, and area-equalized sampling strategies, enablingrobust cross-modal matching. Experiments on the KITTI dataset demonstrate thatInterKey achieves state-of-the-art accuracy, outperforming recent baselines bya large margin. The framework generalizes to sensors that can produce densestructural point clouds, offering a scalable and cost-effective solution forrobust vehicle localization.</description>
      <author>example@mail.com (Nguyen Hoang Khoi Tran, Julie Stephany Berrio, Mao Shan, Stewart Worrall)</author>
      <guid isPermaLink="false">2509.13857v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2509.13692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为HGACNet的新型框架，用于解决点云补全问题，通过分层编码3D几何特征并与RGB图像引导的先验信息融合，重建完整点云。&lt;h4&gt;背景&lt;/h4&gt;点云补全对机器人感知、物体重建及下游任务（如抓取规划、避障和操作）至关重要，但自遮挡和传感器限制导致的不完整几何结构会显著降低下游推理和交互质量。&lt;h4&gt;目的&lt;/h4&gt;解决由自遮挡和传感器限制导致的点云不完整问题，提高点云补全的准确性和适用性。&lt;h4&gt;方法&lt;/h4&gt;提出HGACNet框架，包含分层图注意力(HGA)编码器进行关键点选择和特征细化，多尺度跨模态融合(MSCF)模块实现几何特征与视觉特征的对齐，以及对比损失(C-Loss)对齐跨模态特征分布。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet-ViPC基准和YCB-Complete数据集上的实验证实HGACNet的有效性，展示了最先进的性能和强适用性。&lt;h4&gt;结论&lt;/h4&gt;HGACNet能够有效解决点云补全问题，在多个数据集上表现优异，并在实际机器人操作任务中具有很好的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;点云补全对机器人感知、物体重建和支持抓取规划、避障和操作等下游任务至关重要。然而，由自遮挡和传感器限制引起的不完整几何结构会显著降低下游推理和交互质量。为解决这些挑战，我们提出HGACNet，一种新型框架，通过分层编码3D几何特征并将其与单视图RGB图像引导的先验信息融合，重建单个物体的完整点云。我们方法的核心是分层图注意力(HGA)编码器，它通过基于图注意力的下采样自适应选择关键局部点，并逐步细化分层几何特征，以更好地捕捉结构连续性和空间关系。为加强跨模态交互，我们进一步设计了多尺度跨模态融合(MSCF)模块，在分层几何特征和结构化视觉表示之间进行基于注意力的特征对齐，实现补全的细粒度语义指导。此外，我们提出了对比损失(C-Loss)，明确对齐跨模态特征分布，提高模态差异下的补全保真度。最后，在ShapeNet-ViPC基准和YCB-Complete数据集上进行的广泛实验证实了HGACNet的有效性，展示了最先进的性能以及在现实世界机器人操作任务中的强适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云补全问题，即从部分点云和对应RGB图像重建完整的3D点云。这个问题在现实中非常重要，因为实际获取的点云数据常因传感器限制和物体自遮挡而不完整，这会严重影响机器人抓取、避障、物体识别等下游任务的可靠性。准确的点云补全对机器人感知、物体重建和交互操作至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：单模态方法难以处理遮挡和细粒度结构重建，而跨模态方法未能充分利用模态间关系。作者借鉴了图注意力机制、Transformer架构和对比学习等现有技术，但创新性地设计了分层图注意力编码器（HGA）和多尺度跨模态融合模块（MSCF），通过分层特征提取和跨模态注意力对齐，实现了更有效的点云补全。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过分层编码3D几何特征并融合图像引导的先验知识，实现高质量点云补全。整体流程为：1) 使用HGA编码器从部分点云提取全局和局部特征；2) 用Swin Transformer从RGB图像提取视觉特征；3) 通过MSCF模块融合几何和视觉特征；4) 应用对比损失(C-Loss)对齐不同模态特征；5) 将融合特征输入解码器重建完整点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 分层图注意力编码器(HGA)，自适应选择关键点并保留结构细节；2) 多尺度跨模态融合模块(MSCF)，实现几何与视觉特征的实例级对齐；3) 对比损失函数(C-Loss)，减少模态差异提高重建准确性。相比之前工作，HGACNet不是通过简单连接进行早期融合，而是通过分层注意力实现深度特征交互；在不同几何抽象层次上选择性应用注意力，平衡计算效率和重建质量；同时关注全局结构和细粒度细节，并引入对比学习明确对齐不同模态特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HGACNet通过分层图注意力编码和多尺度跨模态融合，有效结合点云几何特征与图像语义先验，实现了高质量、结构一致的三维点云补全，在机器人感知任务中取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is essential for robotic perception, objectreconstruction and supporting downstream tasks like grasp planning, obstacleavoidance, and manipulation. However, incomplete geometry caused byself-occlusion and sensor limitations can significantly degrade downstreamreasoning and interaction. To address these challenges, we propose HGACNet, anovel framework that reconstructs complete point clouds of individual objectsby hierarchically encoding 3D geometric features and fusing them withimage-guided priors from a single-view RGB image. At the core of our approach,the Hierarchical Graph Attention (HGA) encoder adaptively selects criticallocal points through graph attention-based downsampling and progressivelyrefines hierarchical geometric features to better capture structural continuityand spatial relationships. To strengthen cross-modal interaction, we furtherdesign a Multi-Scale Cross-Modal Fusion (MSCF) module that performsattention-based feature alignment between hierarchical geometric features andstructured visual representations, enabling fine-grained semantic guidance forcompletion. In addition, we proposed the contrastive loss (C-Loss) toexplicitly align the feature distributions across modalities, improvingcompletion fidelity under modality discrepancy. Finally, extensive experimentsconducted on both the ShapeNet-ViPC benchmark and the YCB-Complete datasetconfirm the effectiveness of HGACNet, demonstrating state-of-the-artperformance as well as strong applicability in real-world robotic manipulationtasks.</description>
      <author>example@mail.com (Yadan Zeng, Jiadong Zhou, Xiaohan Li, I-Ming Chen)</author>
      <guid isPermaLink="false">2509.13692v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization</title>
      <link>http://arxiv.org/abs/2509.13686v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RF-LSCM的新型无线信道建模框架，通过辐射场联合表示大尺度信号衰减和多径分量，解决了传统LSCM方法的局限性，实现了多小区、多网格和多频率信道建模，显著提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;精确的局部无线信道建模是蜂窝网络优化的基石，局部统计信道建模(LSCM)是当前最先进的信道建模框架。然而，传统LSCM方法仅限于单小区、单网格和单载波频率分析，无法捕捉复杂的跨域交互。&lt;h4&gt;目的&lt;/h4&gt;克服传统LSCM方法的局限性，开发一个能够捕捉复杂跨域交互的新型信道建模框架，实现多小区、多网格和多频率信道建模。&lt;h4&gt;方法&lt;/h4&gt;提出RF-LSCM框架，通过辐射场联合表示大尺度信号衰减和多径分量；引入多域LSCM公式和频率依赖衰减模型(FDAM)促进跨频率泛化；使用点云辅助的环境增强方法实现多小区和多网格信道建模；利用低秩张量表示和分层张量角度建模(HiTAM)算法提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;RF-LSCM在真实世界多小区数据集上显著优于最先进方法，覆盖预测的平均绝对误差减少高达30%，通过有效融合多频率数据，MAE提高了22%。&lt;h4&gt;结论&lt;/h4&gt;RF-LSCM是一种高效的信道建模框架，能够处理多小区、多网格和多频率数据，在保持细粒度准确性的同时，显著降低了GPU内存需求和训练时间。&lt;h4&gt;翻译&lt;/h4&gt;精确的局部无线信道建模是蜂窝网络优化的基石，能够在参数调整过程中可靠预测网络性能。局部统计信道建模(LSCM)是专为蜂窝网络优化定制的最先进信道建模框架。然而，传统LSCM方法从参考信号接收功率(RSRP)测量中推断信道的角度功率谱(APS)，存在关键局限性：它们通常仅限于单小区、单网格和单载波频率分析，无法捕捉复杂的跨域交互。为克服这些挑战，我们提出了RF-LSCM，一种新框架，通过辐射场联合表示大尺度信号衰减和多径分量来建模信道APS。RF-LSCM引入了多域LSCM公式，采用物理信息相关的频率依赖衰减模型(FDAM)促进跨频率泛化，以及点云辅助的环境增强方法实现多小区和多网格信道建模。此外，为解决典型神经辐射场的计算效率低下问题，RF-LSCM利用低秩张量表示，并辅以新型的分层张量角度建模(HiTAM)算法。这种高效设计显著降低了GPU内存需求和训练时间，同时保持了细粒度准确性。在真实世界多小区数据集上的广泛实验表明，RF-LSCM显著优于最先进的方法，在覆盖预测中平均绝对误差(MAE)减少高达30%，并通过有效融合多频率数据，MAE提高了22%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localized wireless channel modeling is a cornerstone of cellularnetwork optimization, enabling reliable prediction of network performanceduring parameter tuning. Localized statistical channel modeling (LSCM) is thestate-of-the-art channel modeling framework tailored for cellular networkoptimization. However, traditional LSCM methods, which infer the channel'sAngular Power Spectrum (APS) from Reference Signal Received Power (RSRP)measurements, suffer from critical limitations: they are typically confined tosingle-cell, single-grid and single-carrier frequency analysis and fail tocapture complex cross-domain interactions. To overcome these challenges, wepropose RF-LSCM, a novel framework that models the channel APS by jointlyrepresenting large-scale signal attenuation and multipath components within aradiance field. RF-LSCM introduces a multi-domain LSCM formulation with aphysics-informed frequency-dependent Attenuation Model (FDAM) to facilitate thecross frequency generalization as well as a point-cloud-aided environmentenhanced method to enable multi-cell and multi-grid channel modeling.Furthermore, to address the computational inefficiency of typical neuralradiance fields, RF-LSCM leverages a low-rank tensor representation,complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.This efficient design significantly reduces GPU memory requirements andtraining time while preserving fine-grained accuracy. Extensive experiments onreal-world multi-cell datasets demonstrate that RF-LSCM significantlyoutperforms state-of-the-art methods, achieving up to a 30% reduction in meanabsolute error (MAE) for coverage prediction and a 22% MAE improvement byeffectively fusing multi-frequency data.</description>
      <author>example@mail.com (Bingsheng Peng, Shutao Zhang, Xi Zheng, Ye Xue, Xinyu Qin, Tsung-Hui Chang)</author>
      <guid isPermaLink="false">2509.13686v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Deep Lookup Network</title>
      <link>http://arxiv.org/abs/2509.13662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的查找表操作替代卷积神经网络中的乘法操作，以提高计算效率和降低能耗，同时保持与标准卷积网络相当的性能。作者构建了可微分的查找表并提出了训练策略，在多种任务上验证了查找网络的高效性。&lt;h4&gt;背景&lt;/h4&gt;卷积神经网络由大量不同类型的操作构成，计算密集度高。其中乘法操作计算复杂度高，通常比其他操作消耗更多能量且需要更长的推理时间，阻碍了卷积神经网络在移动设备上的部署。在资源受限的边缘设备上，可通过查找表计算复杂操作以降低计算成本。&lt;h4&gt;目的&lt;/h4&gt;引入一种通用的、高效的查找操作作为构建神经网络的基本操作，用简单的查找操作替代权重与激活值之间的乘法计算，提高神经网络的效率。&lt;h4&gt;方法&lt;/h4&gt;构建可微分的查找表，提出几种训练策略促进收敛，用查找操作替代计算密集的乘法操作，开发了用于图像分类、图像超分辨率和点云分类任务的查找网络。&lt;h4&gt;主要发现&lt;/h4&gt;查找网络能够在能耗和推理速度方面实现更高的效率，同时保持与标准卷积网络相竞争的性能。在不同任务（分类和回归）和数据类型（图像和点云）上都取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过用查找操作替代乘法操作，成功开发了高效的查找网络，在保持竞争力的同时显著提高了计算效率和降低了能耗，适用于资源受限的边缘设备。&lt;h4&gt;翻译&lt;/h4&gt;卷积神经网络由大量不同类型的操作构成，计算密集度高。在这些操作中，乘法操作的计算复杂度较高，通常比其他操作消耗更多能量且需要更长的推理时间，这阻碍了卷积神经网络在移动设备上的部署。在许多资源受限的边缘设备上，可以通过查找表来计算复杂操作以降低计算成本。受此启发，在本文中，我们引入了一种通用的、高效的查找操作，可作为构建神经网络的基本操作。我们采用简单而高效的查找操作来计算权重和激活值的响应，而不是直接计算它们的乘积。为了使查找操作能够端到端优化，我们以可微分的方式构建查找表，并提出了几种训练策略来促进其收敛。通过用我们的查找操作替代计算密集的乘法操作，我们开发了用于图像分类、图像超分辨率和点云分类任务的查找网络。实验证明，我们的查找网络能够受益于查找操作，在能耗和推理速度方面实现更高的效率，同时保持与标准卷积网络相竞争的性能。大量实验表明，我们的查找网络在不同任务（分类和回归任务）和数据类型（图像和点云）上都取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Convolutional neural networks are constructed with massive operations withdifferent types and are highly computationally intensive. Among theseoperations, multiplication operation is higher in computational complexity andusually requires {more} energy consumption with longer inference time thanother operations, which hinders the deployment of convolutional neural networkson mobile devices. In many resource-limited edge devices, complicatedoperations can be calculated via lookup tables to reduce computational cost.Motivated by this, in this paper, we introduce a generic and efficient lookupoperation which can be used as a basic operation for the construction of neuralnetworks. Instead of calculating the multiplication of weights and activationvalues, simple yet efficient lookup operations are adopted to compute theirresponses. To enable end-to-end optimization of the lookup operation, weconstruct the lookup tables in a differentiable manner and propose severaltraining strategies to promote their convergence. By replacing computationallyexpensive multiplication operations with our lookup operations, we developlookup networks for the image classification, image super-resolution, and pointcloud classification tasks. It is demonstrated that our lookup networks canbenefit from the lookup operations to achieve higher efficiency in terms ofenergy consumption and inference speed while maintaining competitiveperformance to vanilla convolutional networks. Extensive experiments show thatour lookup networks produce state-of-the-art performance on different tasks(both classification and regression tasks) and different data types (bothimages and point clouds).</description>
      <author>example@mail.com (Yulan Guo, Longguang Wang, Wendong Mao, Xiaoyu Dong, Yingqian Wang, Li Liu, Wei An)</author>
      <guid isPermaLink="false">2509.13662v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Object Pose Estimation through Dexterous Touch</title>
      <link>http://arxiv.org/abs/2509.13591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;结合触觉传感和强化学习的方法，能够主动探索物体表面以识别关键位姿特征，无需先验几何知识。&lt;h4&gt;背景&lt;/h4&gt;机器人在抓取和交互任务中需要稳健的目标位姿估计，特别是在视觉数据有限或对光照、遮挡和外观敏感的场景中。触觉传感器通常提供有限和局部的接触信息，这使得从部分数据重建位姿具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，通过主动控制机器人手与物体交互，来稳健地估计物体位姿。&lt;h4&gt;方法&lt;/h4&gt;使用强化学习进行训练以探索和收集触觉数据。收集到的3D点云用于迭代优化物体的形状和位姿。在一个设置中，一只手稳定地握住物体，另一只手执行主动探索。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够主动探索物体表面以识别关键的位姿特征，无需事先了解物体的几何形状。&lt;h4&gt;结论&lt;/h4&gt;通过触觉传感和强化学习训练的主动探索，可以有效地从有限数据中估计物体位姿。&lt;h4&gt;翻译&lt;/h4&gt;稳健的目标位姿估计对于机器人中的抓取和交互任务至关重要，特别是在视觉数据有限或对光照、遮挡和外观敏感的场景中。触觉传感器通常提供有限和局部的接触信息，这使得从部分数据重建位姿具有挑战性。我们的方法使用传感运动探索来主动控制机器人手与物体交互。我们使用强化学习进行训练以探索和收集触觉数据。收集到的3D点云用于迭代优化物体的形状和位姿。在我们的设置中，一只手稳定地握住物体，而另一只手执行主动探索。我们表明，我们的方法能够主动探索物体表面以识别关键的位姿特征，无需事先了解物体的几何形状。补充材料和更多演示将在https://amirshahid.github.io/BimanualTactilePose提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是通过触觉来估计物体姿态的问题。这个问题在现实中很重要，因为视觉方法在光照变化、遮挡或物体外观等因素影响下表现不稳定，而触觉传感器提供了一种紧凑、低成本的替代方案，可以直接嵌入到机器人手指尖。准确的物体姿态估计对机器人的操作和交互任务至关重要，特别是在视觉受限的环境中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类通过触摸和交互重建物体形状的能力启发，设计了双臂协作系统：一个手稳定握住物体，另一个手进行主动探索。作者设计了丰富的状态表示（包括手指配置、手部旋转、触摸状态等）和多目标奖励函数（平衡姿态估计和探索）。该方法借鉴了强化学习、基于好奇心的奖励函数、FoundationPose姿态估计和点云重建等现有工作，但创新性地将它们整合到一个完整的触觉探索框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双臂协作，一个手稳定握住物体，另一个手使用触觉传感器主动探索物体表面，收集接触点云数据，并利用这些数据迭代重建物体形状和估计物体姿态。整体流程包括：1)初始化：双手向物体移动；2)握持调整：一只手找到稳定抓取姿态；3)探索阶段：探索手收集触觉数据；4)姿态估计：通过点云重建、深度图像渲染和FoundationPose进行姿态优化；5)强化学习训练：使用PPO算法训练探索策略，结合多目标奖励函数引导机器人收集有价值的信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双臂触态探索框架，使用简单FSR传感器实现仅触觉的姿态估计；2)新颖的状态表示和奖励函数，促进在有限动作内的探索；3)将姿态估计反馈整合到奖励函数中，引导探索关键特征；4)无需物体模板即可准确估计姿态。相比之前的工作，本文方法不依赖被动接触或预定义交互，解决了物体固定假设的限制，不仅关注表面覆盖(IoU)更关注姿态估计准确性(ADD-S)，在100步内达到87%的准确率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于强化学习的双臂触觉探索框架，通过主动收集触觉数据并利用姿态估计反馈，实现了仅使用简单触觉传感器就能高效准确地估计未知物体姿态。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust object pose estimation is essential for manipulation and interactiontasks in robotics, particularly in scenarios where visual data is limited orsensitive to lighting, occlusions, and appearances. Tactile sensors often offerlimited and local contact information, making it challenging to reconstruct thepose from partial data. Our approach uses sensorimotor exploration to activelycontrol a robot hand to interact with the object. We train with ReinforcementLearning (RL) to explore and collect tactile data. The collected 3D pointclouds are used to iteratively refine the object's shape and pose. In oursetup, one hand holds the object steady while the other performs activeexploration. We show that our method can actively explore an object's surfaceto identify critical pose features without prior knowledge of the object'sgeometry. Supplementary material and more demonstrations will be provided athttps://amirshahid.github.io/BimanualTactilePose .</description>
      <author>example@mail.com (Amir-Hossein Shahidzadeh, Jiyue Zhu, Kezhou Chen, Sha Yi, Cornelia Fermüller, Yiannis Aloimonos, Xiaolong Wang)</author>
      <guid isPermaLink="false">2509.13591v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>An Exploratory Study on Abstract Images and Visual Representations Learned from Them</title>
      <link>http://arxiv.org/abs/2509.14149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了由基本形状构成的抽象图像在传达视觉语义信息方面的能力，以及其与传统栅格图像表示之间的性能差异。&lt;h4&gt;背景&lt;/h4&gt;最近的研究表明，由基本形状构建的抽象图像确实能够向深度学习模型传达视觉语义信息，但这类图像获得的表示通常不如从传统栅格图像获得的表示。&lt;h4&gt;目的&lt;/h4&gt;研究抽象图像与传统栅格图像之间性能差距的原因，并调查在不同抽象级别能捕获多少高级语义内容。&lt;h4&gt;方法&lt;/h4&gt;引入了分层抽象图像数据集(HAID)，这是一个包含从正常栅格图像在多个抽象级别生成的抽象图像的新数据集。在各种任务(包括分类、分割和目标检测)上训练和评估传统视觉系统，提供栅格化和抽象图像表示之间的综合研究。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体的研究结果，主要介绍了研究方法和数据集的构建。&lt;h4&gt;结论&lt;/h4&gt;探讨抽象图像是否可以被视为传达视觉语义信息的潜在有效格式，以及其是否有助于视觉任务。&lt;h4&gt;翻译&lt;/h4&gt;想象一下生活在一个完全由基本形状构成的世界中，你还能认出熟悉的物体吗？最近的研究表明，由基本形状构建的抽象图像确实能够向深度学习模型传达视觉语义信息。然而，从这类图像获得的表示通常不如从传统栅格图像获得的表示。在本文中，我们研究这种性能差距背后的原因，并调查在不同抽象级别能捕获多少高级语义内容。为此，我们引入了分层抽象图像数据集(HAID)，这是一个新颖的数据收集，包含从正常栅格图像在多个抽象级别生成的抽象图像。然后我们在各种任务(包括分类、分割和目标检测)上训练和评估传统视觉系统，提供了栅格化和抽象图像表示之间的综合研究。我们还讨论了抽象图像是否可以被视为传达视觉语义信息的潜在有效格式，以及是否有助于视觉任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imagine living in a world composed solely of primitive shapes, could youstill recognise familiar objects? Recent studies have shown that abstractimages-constructed by primitive shapes-can indeed convey visual semanticinformation to deep learning models. However, representations obtained fromsuch images often fall short compared to those derived from traditional rasterimages. In this paper, we study the reasons behind this performance gap andinvestigate how much high-level semantic content can be captured at differentabstraction levels. To this end, we introduce the Hierarchical AbstractionImage Dataset (HAID), a novel data collection that comprises abstract imagesgenerated from normal raster images at multiple levels of abstraction. We thentrain and evaluate conventional vision systems on HAID across various tasksincluding classification, segmentation, and object detection, providing acomprehensive study between rasterised and abstract image representations. Wealso discuss if the abstract image can be considered as a potentially effectiveformat for conveying visual semantic information and contributing to visiontasks.</description>
      <author>example@mail.com (Haotian Li, Jianbo Jiao)</author>
      <guid isPermaLink="false">2509.14149v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts</title>
      <link>http://arxiv.org/abs/2509.14104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将Soft mixture-of-experts (MoE)机制集成到遥感基础模型中的方法，创建了CSMoE模型，并引入主题-气候描述符采样策略构建训练集。实验表明该方法在降低计算需求的同时保持或提高了表示性能，实现了现有模型两倍以上的计算效率。&lt;h4&gt;背景&lt;/h4&gt;自监督学习通过掩码自编码器在遥感基础模型发展中受到广泛关注，能够促进不同传感器和下游任务中的表示学习。然而，现有遥感基础模型通常在训练和推理过程中存在巨大的计算复杂性，或者表示能力有限，限制了它们在遥感领域的实际应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有遥感基础模型计算复杂度高和表示能力有限的问题，提高模型的效率和实用性。&lt;h4&gt;方法&lt;/h4&gt;1. 将Soft mixture-of-experts (MoE)机制集成到基础模型中；2. 将此方法应用于Cross-Sensor Masked Autoencoder (CSMAE)模型，创建了Cross-Sensor Mixture-of-Experts (CSMoE)模型；3. 引入基于主题-气候描述符的采样策略构建具有代表性和多样性的训练集。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在场景分类、语义分割和基于内容的图像检索等任务上，该方法在降低计算需求的同时保持或提高了表示性能；2. 与最先进的遥感基础模型相比，CSMoE在表示能力、准确性和计算效率之间取得了更好的平衡；3. 平均而言，CSMoE实现了现有遥感基础模型两倍以上的计算效率，同时在所有实验中保持了有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的适应方法对于创建计算高效的遥感基础模型是有效的，通过集成Soft MoE机制，实现了特定模态的专家专业化与共享的跨传感器表示学习之间的平衡。&lt;h4&gt;翻译&lt;/h4&gt;通过掩码自编码器的自监督学习在遥感基础模型发展中引起了广泛关注，促进了不同传感器和下游任务中的改进表示学习。然而，现有的遥感基础模型通常在训练和推理过程中存在巨大的计算复杂性，或者表示能力有限。这些问题限制了它们在遥感领域的实际应用。为解决这一限制，我们提出了一种适应方法，通过将Soft mixture-of-experts (MoE)机制集成到基础模型中来提高遥感基础模型的效率。将Soft MoEs集成到基础模型中，允许特定模态的专家专业化与共享的跨传感器表示学习相结合。为了证明我们适应方法的有效性，我们将其应用于Cross-Sensor Masked Autoencoder (CSMAE)模型，创建了Cross-Sensor Mixture-of-Experts (CSMoE)模型。此外，我们引入了一种基于主题-气候描述符的采样策略，用于构建具有代表性和多样性的训练集来训练我们的CSMoE模型。在场景分类、语义分割和基于内容的图像检索方面的广泛实验表明，我们的适应方法在降低计算需求的同时保持了或提高了表示性能。与最先进的遥感基础模型相比，CSMoE在表示能力、准确性和计算效率之间取得了更好的平衡。平均而言，CSMoE实现了现有遥感基础模型两倍以上的计算效率，同时在所有实验中保持了有竞争力的性能。这些结果表明，所提出的适应方法对于创建计算高效的遥感基础模型是有效的。该模型的代码、训练集创建和模型权重将在https://git.tu-berlin.de/rsim/csmoe上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning through masked autoencoders has attracted greatattention for remote sensing (RS) foundation model (FM) development, enablingimproved representation learning across diverse sensors and downstream tasks.However, existing RS FMs often either suffer from substantial computationalcomplexity during both training and inference or exhibit limitedrepresentational capacity. These issues restrict their practical applicabilityin RS. To address this limitation, we propose an adaptation for enhancing theefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanisminto the FM. The integration of Soft MoEs into the FM allows modality-specificexpert specialization alongside shared cross-sensor representation learning. Todemonstrate the effectiveness of our adaptation, we apply it on theCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-SensorMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climaticdescriptor-driven sampling strategy for the construction of a representativeand diverse training set to train our CSMoE model. Extensive experiments onscene classification, semantic segmentation, and content-based image retrievaldemonstrate that our adaptation yields a reduction in computationalrequirements while maintaining or improving representational performance.Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-offbetween representational capacity, accuracy, and computational efficiency. Onaverage, CSMoE achieves more than twice the computational efficiency ofexisting RS FMs, while maintaining competitive performance across allexperiments. These results show the effectiveness of the proposed adaptationfor creating computationally efficient RS FMs. The code for the model, thetraining set creation, and the model weights will be available athttps://git.tu-berlin.de/rsim/csmoe.</description>
      <author>example@mail.com (Leonard Hackel, Tom Burgert, Begüm Demir)</author>
      <guid isPermaLink="false">2509.14104v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection</title>
      <link>http://arxiv.org/abs/2509.13878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种混合LoRA专家方法，通过集成多个低秩适配器和路由机制，提高了音频深度伪造检测模型的泛化能力，特别是在面对新型深度伪造方法时表现更佳&lt;h4&gt;背景&lt;/h4&gt;Wav2Vec2等基础模型在语音任务（包括音频深度伪造检测）的表示学习中表现出色。然而，在固定的一组真实和欺骗性音频片段上进行微调后，它们通常无法泛化到训练中未包含的新型深度伪造方法&lt;h4&gt;目的&lt;/h4&gt;解决基础模型在音频深度伪造检测中泛化能力不足的问题，提高模型对不断演变的深度伪造攻击的适应性&lt;h4&gt;方法&lt;/h4&gt;提出了一种混合LoRA专家方法，将多个低秩适配器（LoRA）集成到模型的注意力层中，并使用路由机制选择性地激活专门的专家&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在领域内和领域外场景中都优于标准微调，相比基线模型降低了等错误率；最好的MoE-LoRA模型将平均领域外EER从8.55%降低到6.08%&lt;h4&gt;结论&lt;/h4&gt;该混合LoRA专家方法在实现可泛化的音频深度伪造检测方面有效&lt;h4&gt;翻译&lt;/h4&gt;基础模型如Wav2Vec2在语音任务（包括音频深度伪造检测）的表示学习中表现出色。然而，在固定的一组真实和欺骗性音频片段上进行微调后，它们通常无法泛化到训练中未包含的新型深度伪造方法。为此，我们提出了一种混合LoRA专家方法，将多个低秩适配器（LoRA）集成到模型的注意力层中。路由机制选择性地激活专门的专家，增强了对不断演变的深度伪造攻击的适应性。实验结果表明，我们的方法在领域内和领域外场景中都优于标准微调，降低了相对于基线模型的等错误率。值得注意的是，我们最好的MoE-LoRA模型将平均领域外EER从8.55%降低到6.08%，证明了其在实现可泛化的音频深度伪造检测方面的有效性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models such as Wav2Vec2 excel at representation learning in speechtasks, including audio deepfake detection. However, after being fine-tuned on afixed set of bonafide and spoofed audio clips, they often fail to generalize tonovel deepfake methods not represented in training. To address this, we proposea mixture-of-LoRA-experts approach that integrates multiple low-rank adapters(LoRA) into the model's attention layers. A routing mechanism selectivelyactivates specialized experts, enhancing adaptability to evolving deepfakeattacks. Experimental results show that our method outperforms standardfine-tuning in both in-domain and out-of-domain scenarios, reducing equal errorrates relative to baseline models. Notably, our best MoE-LoRA model lowers theaverage out-of-domain EER from 8.55\% to 6.08\%, demonstrating itseffectiveness in achieving generalizable audio deepfake detection.</description>
      <author>example@mail.com (Janne Laakkonen, Ivan Kukanov, Ville Hautamäki)</author>
      <guid isPermaLink="false">2509.13878v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.13846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2025: 1st Place in Transformer track and 2nd Place in  Convolution track of SSL3D-OpenMind challenge&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究挑战了表征学习中关于非相关视图足以学习有意义表征的假设，提出了一种名为'Consistent View Alignment'的自监督学习方法，通过结构化视图对齐提高下游任务性能，并在MICCAI 2025 SSL3D挑战赛中取得优异成绩。&lt;h4&gt;背景&lt;/h4&gt;近年来，表征学习方法隐含地假设数据点的非相关视图足以学习对各种下游任务有意义的表征。&lt;h4&gt;目的&lt;/h4&gt;挑战这一假设，证明潜在空间中的有意义结构不会自然出现，必须被明确诱导。&lt;h4&gt;方法&lt;/h4&gt;提出一种方法，对齐来自数据不同视图的表征，以对齐互补信息而不产生假阳性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的自监督学习方法'Consistent View Alignment'提高了下游任务的性能，突显了结构化视图对齐在学习有效表征中的关键作用。&lt;h4&gt;结论&lt;/h4&gt;结构化视图对齐对于学习有效表征至关重要。&lt;h4&gt;翻译&lt;/h4&gt;表征学习中的许多近期方法隐含地假设，数据点的非相关视图足以学习对各种下游任务有意义的表征。在本工作中，我们挑战这一假设，证明潜在空间中的有意义结构不会自然出现，而必须被明确诱导。我们提出了一种方法，对齐来自数据不同视图的表征，以对齐互补信息而不产生假阳性。我们的实验表明，我们提出的自监督学习方法'Consistent View Alignment'提高了下游任务的性能，突显了结构化视图对齐在学习有效表征中的关键作用。当使用Primus视觉变换器和ResEnc卷积神经网络时，我们的方法在MICCAI 2025 SSL3D挑战赛中分别获得第一名和第二名。代码和预训练模型权重已在https://github.com/Tenbatsu24/LatentCampus发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自监督学习中当数据视图之间相关性较弱时的'假阳性'问题。在医学图像分析中，简单的随机裁剪可能导致两个视图代表完全不同的语义信息，而现有对比学习方法仍会强制模型对这些不相关特征进行对齐，导致虚假关联并降低表示质量。这个问题很重要，因为它直接影响医学AI中下游任务（如分割、分类）的性能，进而关系到诊断的准确性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有对比学习方法（如SimCLR、SwAV）假设正样本对完美相关，但在医学图像等复杂场景中这一假设常被违反。他们注意到现有方法很少直接约束特征空间中对齐应该发生的位置。因此，作者设计了'一致视图对齐'方法，通过生成具有重叠区域的视图对，并在特征空间中对齐这些重叠区域，只在这些区域应用一致性损失。作者借鉴了对比学习中的学生-教师架构、掩码自编码器的重建目标、SwAV的对称化损失计算以及VoCo的体积特定增强方法，但创新性地将它们组合以解决假阳性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过确保只有真正语义对应的区域被对齐来避免假阳性问题，而不是简单地假设两个随机视图应该相似。整体流程包括：1)从原始3D医学图像生成两个具有重叠区域(40%-80%)的随机裁剪；2)使用学生和教师网络提取特征；3)应用掩码自编码器目标重建被遮蔽区域；4)使用ROIAlign对齐重叠区域的特征；5)计算对齐区域间的特征相似度；6)在两个视图方向上对称化计算损失；7)结合重建损失、一致性损失和可选的对比损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)通过一致视图构造缓解假阳性问题；2)通过强制局部特征一致性改善下游分割任务；3)在不同空间上下文和语义内容间强制特征一致性。相比之前工作的不同：与传统对比学习不同，CVA不假设整个视图相似，只对齐已知重叠区域；与掩码自编码器不同，CVA添加了显式特征对齐步骤；与对比掩码自编码器不同，CVA主动识别并纠正假阳性；与其他3D医学图像预训练方法不同，CVA更关注减少假阳性而非假阴性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CVA通过显式对齐不同视图中的语义对应区域而非强制对齐整个视图，有效缓解了自监督学习中的假阳性问题，显著提高了3D医学图像分割任务的表现，同时展示了局部特征一致性与全局判别特征学习之间的权衡关系。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many recent approaches in representation learning implicitly assume thatuncorrelated views of a data point are sufficient to learn meaningfulrepresentations for various downstream tasks. In this work, we challenge thisassumption and demonstrate that meaningful structure in the latent space doesnot emerge naturally. Instead, it must be explicitly induced. We propose amethod that aligns representations from different views of the data to aligncomplementary information without inducing false positives. Our experimentsshow that our proposed self-supervised learning method, Consistent ViewAlignment, improves performance for downstream tasks, highlighting the criticalrole of structured view alignment in learning effective representations. Ourmethod achieved first and second place in the MICCAI 2025 SSL3D challenge whenusing a Primus vision transformer and ResEnc convolutional neural network,respectively. The code and pretrained model weights are released athttps://github.com/Tenbatsu24/LatentCampus.</description>
      <author>example@mail.com (Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink)</author>
      <guid isPermaLink="false">2509.13846v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>SAMIR, an efficient registration framework via robust feature learning from SAM</title>
      <link>http://arxiv.org/abs/2509.13629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAMIR的高效医学图像配准框架，利用Segment Anything Model (SAM)增强特征提取，显著提升了配准性能。&lt;h4&gt;背景&lt;/h4&gt;医学图像配准是医学图像分析的基础任务，变形通常与组织的形态特征密切相关，准确的特征提取至关重要。然而，现有的弱监督方法需要分割掩码或地标等解剖先验，这些标签往往难以获取，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;利用视觉基础模型的强大表示学习能力，开发一种不依赖弱标签的高效医学图像配准方法，提高配准精度和实用性。&lt;h4&gt;方法&lt;/h4&gt;使用SAM的图像编码器提取结构感知的特征嵌入，而非直接使用原始输入图像；设计特定任务的自适应管道和轻量级3D头在嵌入空间内细化特征，适应医学图像中的局部变形；引入分层特征一致性损失来引导粗到细的特征匹配，改善解剖对齐。&lt;h4&gt;主要发现&lt;/h4&gt;SAMIR在受试者内心脏图像配准和受试者间腹部CT图像配准任务上显著优于现有方法，在ACDC数据集上实现了2.68%的性能提升，在腹部数据集上实现了6.44%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;SAMIR是一种有效的医学图像配准框架，利用SAM可以增强特征提取，提高配准性能，且代码将在论文接受后公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;图像配准是医学图像分析中的基础任务。变形通常与组织的形态特征密切相关，使得准确的特征提取至关重要。近期的弱监督方法通过整合分割掩码或地标等解剖先验来改进配准，这些先验可以作为输入或损失函数的一部分。然而，这类弱标签通常不易获取，限制了其实际应用。受视觉基础模型强大表示学习能力的启发，本文引入了SAMIR，一种利用Segment Anything Model (SAM)增强特征提取的高效医学图像配准框架。SAM在大型自然图像数据集上预训练，可以学习强大且通用的视觉表示。我们设计了一个特定任务的自适应管道，使用SAM的图像编码器提取结构感知的特征嵌入，而非使用原始输入图像，从而能够更准确地建模解剖一致性和变形模式。我们还设计了一个轻量级3D头在嵌入空间内细化特征，适应医学图像中的局部变形。此外，我们引入了分层特征一致性损失来引导粗到细的特征匹配，改善解剖对齐。大量实验表明，在受试者内心脏图像配准和受试者间腹部CT图像配准的基准数据集上，SAMIR显著优于最先进的方法，在ACDC上实现了2.68%的性能提升，在腹部数据集上实现了6.44%的性能提升。论文接受后，源代码将在GitHub上公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决医学图像配准中特征提取的准确性和鲁棒性问题。医学图像配准对诊断、手术规划和运动分析至关重要，但现有方法难以处理图像质量差异（如对比度变化和噪声），且依赖难以获取的分割标签。提高配准准确性和鲁棒性可以辅助医生进行更精准的诊疗决策，扩大方法在实际临床环境中的适用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统配准方法的局限性（耗时、对图像质量敏感）和弱监督方法的不足（标签获取困难）。受SAM模型在图像分割中强大特征提取能力的启发，作者设计了一种新的配准框架，利用SAM的结构感知特征而非原始图像。作者借鉴了金字塔策略处理大变形问题，参考了弱监督方法的思想但避免了直接依赖分割标签，同时结合了视觉基础模型在其他任务中的成功应用经验。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用SAM的预训练图像编码器提取结构感知的特征嵌入，增强医学图像配准的准确性和鲁棒性。整体流程分为三部分：1) 结构感知特征嵌入模块：将3D医学数据转为2D切片处理，通过SAM编码器提取特征，再用轻量级3D卷积模块增强特征；2) 金字塔变形场预测模块：采用粗到细策略，在不同尺度上预测变形场，逐步优化；3) 分层特征一致性损失：在多尺度空间中对齐特征，提高解剖结构一致性。整个流程避免了依赖难以获取的弱标签，同时提高了对图像质量变化的鲁棒性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 创新性地将SAM模型用于医学图像配准，设计了特定的3D适应管道；2) 提出结构感知特征嵌入模块，利用SAM的强大特征提取能力；3) 设计分层特征一致性损失，在多尺度空间中对齐特征；4) 结合金字塔策略处理大变形问题。相比之前工作，SAMIR避免了依赖难以获取的分割标签，直接利用SAM的特征提取能力而非原始图像，在ACDC和腹部CT数据集上分别实现了2.68%和6.44%的性能提升，且对图像质量变化表现出更强的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAMIR创新性地利用SAM的强大特征提取能力，通过结构感知特征嵌入和分层特征一致性损失，实现了更准确、更鲁棒的医学图像配准，显著优于现有方法且无需依赖难以获取的弱标签。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image registration is a fundamental task in medical image analysis.Deformations are often closely related to the morphological characteristics oftissues, making accurate feature extraction crucial. Recent weakly supervisedmethods improve registration by incorporating anatomical priors such assegmentation masks or landmarks, either as inputs or in the loss function.However, such weak labels are often not readily available, limiting theirpractical use. Motivated by the strong representation learning ability ofvisual foundation models, this paper introduces SAMIR, an efficient medicalimage registration framework that utilizes the Segment Anything Model (SAM) toenhance feature extraction. SAM is pretrained on large-scale natural imagedatasets and can learn robust, general-purpose visual representations. Ratherthan using raw input images, we design a task-specific adaptation pipelineusing SAM's image encoder to extract structure-aware feature embeddings,enabling more accurate modeling of anatomical consistency and deformationpatterns. We further design a lightweight 3D head to refine features within theembedding space, adapting to local deformations in medical images.Additionally, we introduce a Hierarchical Feature Consistency Loss to guidecoarse-to-fine feature matching and improve anatomical alignment. Extensiveexperiments demonstrate that SAMIR significantly outperforms state-of-the-artmethods on benchmark datasets for both intra-subject cardiac image registrationand inter-subject abdomen CT image registration, achieving performanceimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source codewill be publicly available on GitHub following the acceptance of this paper.</description>
      <author>example@mail.com (Yue He, Min Liu, Qinghao Liu, Jiazheng Wang, Yaonan Wang, Hang Zhang, Xiang Chen)</author>
      <guid isPermaLink="false">2509.13629v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>HAM: Hierarchical Adapter Merging for Scalable Continual Learning</title>
      <link>http://arxiv.org/abs/2509.13211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为分层适配器合并(HAM)的新框架，用于解决持续学习中的灾难性遗忘问题，通过动态组合不同任务的适配器，有效管理多个任务并提高效率。&lt;h4&gt;背景&lt;/h4&gt;持续学习是人类认知的重要能力，但对当前深度学习模型构成重大挑战。新知识会干扰已学习的信息，导致模型遗忘旧知识，这种现象称为灾难性遗忘。大型预训练模型可通过利用现有知识和过参数化部分缓解遗忘，但在面对新数据分布时仍存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效扩展到动态学习场景和长任务序列的方法，解决参数高效微调(PEFT)方法在管理多个任务时面临的复杂性和干扰问题。&lt;h4&gt;方法&lt;/h4&gt;提出HAM框架，在训练过程中动态组合来自不同任务的适配器。维护一组固定组，分层合并新的适配器；对每个任务训练低秩适配器和重要性标量；基于适配器相似性动态分组任务；在每个组内对适配器进行修剪、缩放和合并，促进相关任务间的迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个视觉基准测试上的大量实验表明，HAM显著优于最先进的方法，特别是在任务数量增加时优势更加明显。HAM能够有效扩展，比竞争基线方法管理更多任务同时提高效率。&lt;h4&gt;结论&lt;/h4&gt;HAM框架提供了一种有效解决持续学习中灾难性遗忘问题的方法，通过分层合并适配器，能够在处理多个任务时保持性能并提高效率。&lt;h4&gt;翻译&lt;/h4&gt;持续学习是人类认知的基本能力，然而它对当前的深度学习模型构成了重大挑战。主要问题是新知识可能会干扰已学习的信息，导致模型遗忘旧知识而倾向于新知识，这种现象被称为灾难性遗忘。尽管大型预训练模型可以通过利用其现有知识和过参数化部分缓解遗忘，但当面对新的数据分布时，它们仍然存在困难。参数高效微调(PEFT)方法（如LoRA）使模型能够高效地适应新知识。然而，在扩展到动态学习场景和长任务序列时，它们仍面临挑战，因为每个任务维护一个适配器会增加复杂性并增加干扰的可能性。在本文中，我们引入了分层适配器合并(HAM)，一个在训练过程中动态组合来自不同任务的适配器的新框架。这种方法使HAM能够有效扩展，允许它以更高的效率管理比竞争基线更多的任务。为此，HAM维护一组固定组，这些组分层合并新的适配器。对于每个任务，HAM训练一个低秩适配器和一个重要性标量，然后基于适配器相似性动态对任务进行分组。在每个组内，适配器被修剪、缩放和合并，促进相关任务之间的迁移学习。在三个视觉基准测试上的大量实验表明，HAM显著优于最先进的方法，特别是随着任务数量的增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning is an essential capability of human cognition, yet itposes significant challenges for current deep learning models. The primaryissue is that new knowledge can interfere with previously learned information,causing the model to forget earlier knowledge in favor of the new, a phenomenonknown as catastrophic forgetting. Although large pre-trained models canpartially mitigate forgetting by leveraging their existing knowledge andover-parameterization, they often struggle when confronted with novel datadistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,enable efficient adaptation to new knowledge. However, they still facechallenges in scaling to dynamic learning scenarios and long sequences oftasks, as maintaining one adapter per task introduces complexity and increasesthe potential for interference. In this paper, we introduce HierarchicalAdapters Merging (HAM), a novel framework that dynamically combines adaptersfrom different tasks during training. This approach enables HAM to scaleeffectively, allowing it to manage more tasks than competing baselines withimproved efficiency. To achieve this, HAM maintains a fixed set of groups thathierarchically consolidate new adapters. For each task, HAM trains a low-rankadapter along with an importance scalar, then dynamically groups tasks based onadapter similarity. Within each group, adapters are pruned, scaled and merge,facilitating transfer learning between related tasks. Extensive experiments onthree vision benchmarks show that HAM significantly outperformsstate-of-the-art methods, particularly as the number of tasks increases.</description>
      <author>example@mail.com (Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2509.13211v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
  <item>
      <title>Ensemble Visualization With Variational Autoencoder</title>
      <link>http://arxiv.org/abs/2509.13000v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE Workshop on Uncertainty Visualization&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提出了一种新的数据集可视化方法，通过在潜在空间中构建结构化概率表示，实现数据集的有效可视化和分析。&lt;h4&gt;背景&lt;/h4&gt;数据集的可视化是理解和分析复杂空间数据的重要挑战，特别是在处理高维数据集合时。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来可视化数据集，通过构建潜在空间中的结构化概率表示，实现数据集的特征提取和概率分布分析。&lt;h4&gt;方法&lt;/h4&gt;通过特征空间转换和变分自编码器的无监督学习，将空间数据集合的特征转换为潜在空间，使潜在空间遵循多元标准高斯分布，从而支持置信区间计算和概率分布密度估计。&lt;h4&gt;主要发现&lt;/h4&gt;在天气预报集合上的初步结果表明，所提出的方法在数据集可视化方面是有效且多功能的，能够准确表示数据集合的概率分布特性。&lt;h4&gt;结论&lt;/h4&gt;该方法为数据集可视化提供了一种新的有效途径，通过结构化概率表示在潜在空间中，能够更好地理解和分析复杂的数据集合。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种通过在潜在空间中构建结构化概率表示来可视化数据集的新方法，即空间数据特征的低维表示。我们的方法通过特征空间转换和使用变分自编码器的无监督学习，将集合的空间特征转换为潜在空间。由此产生的潜在空间遵循多元标准高斯分布，使得能够计算置信区间和生成数据集的概率分布的密度估计。在天气预报集合上的初步结果证明了我们方法的有效性和多功能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new method to visualize data ensembles by constructingstructured probabilistic representations in latent spaces, i.e.,lower-dimensional representations of spatial data features. Our approachtransforms the spatial features of an ensemble into a latent space throughfeature space conversion and unsupervised learning using a variationalautoencoder (VAE). The resulting latent spaces follow multivariate standardGaussian distributions, enabling analytical computation of confidence intervalsand density estimation of the probabilistic distribution that generates thedata ensemble. Preliminary results on a weather forecasting ensembledemonstrate the effectiveness and versatility of our method.</description>
      <author>example@mail.com (Cenyang Wu, Qinhan Yu, Liang Zhou)</author>
      <guid isPermaLink="false">2509.13000v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Rapid Adaptation of SpO2 Estimation to Wearable Devices via Transfer Learning on Low-Sampling-Rate PPG</title>
      <link>http://arxiv.org/abs/2509.12515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In the proceedings of IEEE-EMBS International Conference on Body  Sensor Networks 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于迁移学习的框架，利用低采样率双通道光电容积脉搏波实现可穿戴设备上的低功耗血氧饱和度监测，无需复杂临床校准。&lt;h4&gt;背景&lt;/h4&gt;血氧饱和度(SpO2)是医疗保健监测的重要指标，但传统估计方法依赖复杂临床校准，不适合低功耗可穿戴应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速适应于能量高效可穿戴设备的SpO2估计方法，使用低采样率(25Hz)双通道光电容积脉搏波(PPG)。&lt;h4&gt;方法&lt;/h4&gt;首先在公共临床数据集上预训练带有自注意力的双向长短期记忆(BiLSTM)模型，然后使用从可穿戴We-Be带和FDA批准的参考脉搏血氧仪收集的数据进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上平均绝对误差为2.967%，在私有数据集上为2.624%，显著优于传统方法；使用25Hz PPG相比100Hz减少40%功耗；瞬时SpO2预测误差为3.284%，能有效捕捉快速波动。&lt;h4&gt;结论&lt;/h4&gt;证明了准确、低功耗SpO2监测在可穿戴设备上的快速适应性，无需临床校准即可实现。&lt;h4&gt;翻译&lt;/h4&gt;血氧饱和度(SpO2)是医疗保健监测的重要指标。传统的SpO2估计方法通常依赖复杂的临床校准，使其不适合低功耗、可穿戴应用。在本文中，我们提出了一种基于迁移学习的框架，利用低采样率(25Hz)双通道光电容积脉搏波(PPG)实现SpO2估计向节能可穿戴设备的快速适应。我们首先在公共临床数据集上预训练一个带有自注意力的双向长短期记忆(BiLSTM)模型，然后使用从我们的可穿戴We-Be带和FDA批准的参考脉搏血氧仪收集的数据对其进行微调。实验结果表明，我们的方法在公共数据集上达到2.967%的平均绝对误差(MAE)，在私有数据集上达到2.624%的MAE，显著优于传统校准和非迁移学习基线。此外，与100Hz相比，使用25Hz PPG可减少40%的功耗(不包括基线功耗)。我们的方法在瞬时SpO2预测中也达到了3.284%的MAE，有效捕捉了快速波动。这些结果证明了无需临床校准即可在可穿戴设备上实现准确、低功耗SpO2监测的快速适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Blood oxygen saturation (SpO2) is a vital marker for healthcare monitoring.Traditional SpO2 estimation methods often rely on complex clinical calibration,making them unsuitable for low-power, wearable applications. In this paper, wepropose a transfer learning-based framework for the rapid adaptation of SpO2estimation to energy-efficient wearable devices using low-sampling-rate (25Hz)dual-channel photoplethysmography (PPG). We first pretrain a bidirectional LongShort-Term Memory (BiLSTM) model with self-attention on a public clinicaldataset, then fine-tune it using data collected from our wearable We-Be bandand an FDA-approved reference pulse oximeter. Experimental results show thatour approach achieves a mean absolute error (MAE) of 2.967% on the publicdataset and 2.624% on the private dataset, significantly outperformingtraditional calibration and non-transferred machine learning baselines.Moreover, using 25Hz PPG reduces power consumption by 40% compared to 100Hz,excluding baseline draw. Our method also attains an MAE of 3.284% ininstantaneous SpO2 prediction, effectively capturing rapid fluctuations. Theseresults demonstrate the rapid adaptation of accurate, low-power SpO2 monitoringon wearable devices without the need for clinical calibration.</description>
      <author>example@mail.com (Zequan Liang, Ruoyu Zhang, Wei Shao, krishna Karthik, Ehsan Kourkchi, Setareh Rafatirad, Houman Homayoun)</author>
      <guid isPermaLink="false">2509.12515v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture</title>
      <link>http://arxiv.org/abs/2509.12363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种联邦学习框架用于智能农业，旨在为明尼苏达州农场开发可扩展、高效且安全的作物疾病检测解决方案，同时保护农场数据隐私。&lt;h4&gt;背景&lt;/h4&gt;农业部门正经历转型，先进技术特别是数据驱动决策的整合日益重要。然而，农场对共享运营数据存在隐私顾虑，导致数据驱动的农业解释需求与农场隐私担忧之间存在矛盾。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展、高效且安全的作物疾病检测解决方案，该方案能适应明尼苏达州农场的环境和运营条件，同时保持敏感农场数据的本地性，实现高准确率的作物疾病分类而不损害数据隐私。&lt;h4&gt;方法&lt;/h4&gt;研究涉及从明尼苏达州农场收集数据，应用本地深度学习算法，使用迁移学习，以及通过中央聚合服务器进行模型优化。联邦学习框架使敏感数据保持本地，同时实现协作模型更新。&lt;h4&gt;主要发现&lt;/h4&gt;该方法有望实现疾病检测准确率的提高，在农业场景中具有良好的泛化能力，降低通信和训练成本，并在未来实现疾病的早期识别和干预。&lt;h4&gt;结论&lt;/h4&gt;这项工作弥合了先进机器学习技术与明尼苏达州及其他地区农民的实际、隐私敏感需求之间的差距，利用联邦学习的优势，为农业提供了安全高效的疾病检测方法，有望彻底改变智能农业系统并解决当地农业问题，同时确保数据机密性。&lt;h4&gt;翻译&lt;/h4&gt;农业部门正经历转型，先进技术特别是数据驱动决策的整合日益重要。本研究提出了一种用于智能农业的联邦学习框架，旨在为明尼苏达州农场开发可扩展、高效且安全的作物疾病检测解决方案，适应其环境和运营条件。通过将敏感农场数据保存在本地并实现协作模型更新，我们提出的框架旨在实现高准确率的作物疾病分类而不损害数据隐私。我们概述了一种涉及从明尼苏达州农场收集数据、应用本地深度学习算法、迁移学习以及通过中央聚合服务器进行模型优化的方法，旨在提高疾病检测的准确率，在农业场景中实现良好的泛化能力，降低通信和训练成本，并在未来实施中实现疾病的早期识别和干预。我们概述了方法和预期成果，为后续研究中的实证验证奠定了基础。这项工作出现在越来越多的农业数据驱动解释需求必须与对农场不愿共享其运营数据的隐私担忧进行权衡的背景下。这将提供一种安全高效的疾病检测方法，最终能够彻底改变智能农业系统，并以数据保密性解决当地农业问题。通过这样做，本文弥合了先进机器学习技术与明尼苏达州及其他地区农民的实际、隐私敏感需求之间的差距，利用了联邦学习的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.47852/bonviewAIA52025089&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The agricultural sector is undergoing a transformation with the integrationof advanced technologies, particularly in data-driven decision-making. Thiswork proposes a federated learning framework for smart farming, aiming todevelop a scalable, efficient, and secure solution for crop disease detectiontailored to the environmental and operational conditions of Minnesota farms. Bymaintaining sensitive farm data locally and enabling collaborative modelupdates, our proposed framework seeks to achieve high accuracy in crop diseaseclassification without compromising data privacy. We outline a methodologyinvolving data collection from Minnesota farms, application of local deeplearning algorithms, transfer learning, and a central aggregation server formodel refinement, aiming to achieve improved accuracy in disease detection,good generalization across agricultural scenarios, lower costs in communicationand training time, and earlier identification and intervention against diseasesin future implementations. We outline a methodology and anticipated outcomes,setting the stage for empirical validation in subsequent studies. This workcomes in a context where more and more demand for data-driven interpretationsin agriculture has to be weighed with concerns about privacy from farms thatare hesitant to share their operational data. This will be important to providea secure and efficient disease detection method that can finally revolutionizesmart farming systems and solve local agricultural problems with dataconfidentiality. In doing so, this paper bridges the gap between advancedmachine learning techniques and the practical, privacy-sensitive needs offarmers in Minnesota and beyond, leveraging the benefits of federated learning.</description>
      <author>example@mail.com (Ritesh Janga, Rushit Dave)</author>
      <guid isPermaLink="false">2509.12363v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation</title>
      <link>http://arxiv.org/abs/2509.13177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ROOM是一个综合模拟框架，用于生成逼真的支气管镜检查训练数据，解决连续体机器人发展中缺乏真实训练环境的问题。&lt;h4&gt;背景&lt;/h4&gt;连续体机器人正在推进支气管镜检查程序，能够进入复杂的肺部气道并实现靶向干预。然而，其发展受到缺乏真实训练和测试环境的限制。真实数据难以收集，因为受到伦理约束和患者安全问题的限制，开发自主算法需要真实的成像和物理反馈。&lt;h4&gt;目的&lt;/h4&gt;提出ROOM（医学中真实的光学观察）模拟框架，用于生成逼真的支气管镜检查训练数据。&lt;h4&gt;方法&lt;/h4&gt;利用患者CT扫描，管道渲染多模态传感器数据，包括具有真实噪声和镜面反射的RGB图像、度量深度图、表面法线、光流和点云，在医学相关尺度上。&lt;h4&gt;主要发现&lt;/h4&gt;在多视图姿态估计和单目深度估计两个医疗机器人标准任务中验证了ROOM生成的数据，展示了最先进方法必须克服的多样化挑战。证明ROOM产生的数据可用于微调现有的深度估计模型，使其他下游应用（如导航）成为可能。&lt;h4&gt;结论&lt;/h4&gt;预期ROOM将能够在多样化的患者解剖结构和临床环境中难以捕捉的程序场景中实现大规模数据生成。&lt;h4&gt;翻译&lt;/h4&gt;连续体机器人通过进入复杂的肺部气道和实现靶向干预，正在推进支气管镜检查程序。然而，其发展受到缺乏真实训练和测试环境的限制：由于伦理约束和患者安全问题，真实数据难以收集，并且开发自主算法需要真实的成像和物理反馈。我们提出了ROOM（医学中真实的光学观察），这是一个综合模拟框架，专为生成逼真的支气管镜检查训练数据而设计。通过利用患者CT扫描，我们的管道渲染多模态传感器数据，包括具有真实噪声和镜面反射的RGB图像、度量深度图、表面法线、光流和点云，在医学相关尺度上。我们在医疗机器人的两个标准任务——多视图姿态估计和单目深度估计中验证了ROOM生成的数据，展示了最先进方法必须克服的多样化挑战，才能转移到这些医疗环境中。此外，我们证明ROOM产生的数据可用于微调现有的深度估计模型以克服这些挑战，同时使其他下游应用（如导航）成为可能。我们期望ROOM将能够在多样化的患者解剖结构和临床环境中难以捕捉的程序场景中实现大规模数据生成。代码和数据：https://github.com/iamsalvatore/room。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决支气管镜检查中连续体机器人开发面临的训练数据稀缺问题。由于伦理限制、患者安全问题和临床数据收集的高成本，真实数据难以获取；同时，人体解剖结构的个体化特性要求算法必须适应不同气道几何形状并保持毫米级精度。这个问题限制了自主导航算法的发展，而合成数据生成能提供大规模、多样化的训练数据，促进支气管镜机器人的技术进步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了支气管镜检查的特殊挑战：需要解剖保真度、特定光照条件以及临床尺度校准。他们借鉴了Cosserat杆理论建模连续体机器人运动学，使用PyBullet作为物理引擎，Blender的路径追踪和BSDF着色器实现光保真渲染。同时参考了现有深度估计和姿态估计方法进行验证，并利用医学图像分割和3D重建技术处理CT数据。作者整合了这些现有技术，创建了一个专门针对支气管镜检查的统一模拟框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用患者CT扫描数据，通过物理模拟和光保真渲染，生成多样化的多模态传感器数据，解决支气管镜训练数据稀缺问题。整体流程分为四步：1)中轴提取：从CT扫描重建3D肺部模型并提取中轴轨迹；2)自动采样：沿骨骼结构采样，在关键区域增加密度；3)数据合成：生成RGB图像、深度图、表面法线等多模态数据；4)传感器噪声建模：通过频域分析添加真实噪声特征。整个过程从CT扫描开始，最终生成同步的多模态传感器数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ROOM框架：首个完全自动化将CT数据转换为合成训练数据的管道；2)光保真渲染：考虑内窥镜光照和组织表面特性；3)多模态数据生成：提供RGB、深度、法线等多种传感器数据；4)频域噪声建模：准确复制真实支气管镜图像噪声特性。相比传统模拟器，ROOM提供光保真度和多模态数据；相比结肠镜数据生成，ROOM应对支气管镜特有的几何和外观挑战；相比现有连续体机器人系统，ROOM统一了物理模拟和光保真渲染，弥合了两者间的差距。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ROOM是一个创新的物理模拟框架，通过整合患者特异性解剖重建、连续体机器人物理和光保真渲染，生成多样化的支气管镜训练数据集，解决了真实数据稀缺的关键挑战，并提高了姿态估计和深度估计模型在医疗环境中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuum robots are advancing bronchoscopy procedures by accessing complexlung airways and enabling targeted interventions. However, their development islimited by the lack of realistic training and test environments: Real data isdifficult to collect due to ethical constraints and patient safety concerns,and developing autonomy algorithms requires realistic imaging and physicalfeedback. We present ROOM (Realistic Optical Observation in Medicine), acomprehensive simulation framework designed for generating photorealisticbronchoscopy training data. By leveraging patient CT scans, our pipelinerenders multi-modal sensor data including RGB images with realistic noise andlight specularities, metric depth maps, surface normals, optical flow and pointclouds at medically relevant scales. We validate the data generated by ROOM intwo canonical tasks for medical robotics -- multi-view pose estimation andmonocular depth estimation, demonstrating diverse challenges thatstate-of-the-art methods must overcome to transfer to these medical settings.Furthermore, we show that the data produced by ROOM can be used to fine-tuneexisting depth estimation models to overcome these challenges, also enablingother downstream applications such as navigation. We expect that ROOM willenable large-scale data generation across diverse patient anatomies andprocedural scenarios that are challenging to capture in clinical settings. Codeand data: https://github.com/iamsalvatore/room.</description>
      <author>example@mail.com (Salvatore Esposito, Matías Mattamala, Daniel Rebain, Francis Xiatian Zhang, Kevin Dhaliwal, Mohsen Khadem, Subramanian Ramamoorthy)</author>
      <guid isPermaLink="false">2509.13177v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</title>
      <link>http://arxiv.org/abs/2509.13172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WHU-STree是一个跨城市、丰富标注和多模态的城市行道树数据集，包含两个不同城市收集的同步点云和高分辨率图像，涵盖21,007个标注的树实例，涉及50个物种和2个形态参数。该数据集支持超过10个与行道树清单相关的任务，并针对树种分类和单木分割两个关键任务进行了基准测试。&lt;h4&gt;背景&lt;/h4&gt;行道树对城市宜居性至关重要，提供生态和社会效益。在空间受限的城市环境中优化这些多功能资产需要建立详细、准确且动态更新的行道树清单。传统的地面调查耗时且劳动密集，而利用移动测量系统(MMS)的自动化调查提供了更高效的解决方案。然而，现有的MMS获取的树数据集受限于小规模场景、有限的标注或单一模态，限制了它们用于综合分析的能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有MMS获取的树数据集的局限性，研究引入了WHU-STree，这是一个跨城市、丰富标注和多模态的城市行道树数据集。&lt;h4&gt;方法&lt;/h4&gt;WHU-STree数据集是在两个不同城市收集的，集成了同步点云和高分辨率图像，包含21,007个标注的树实例，涵盖50个物种和2个形态参数。研究利用WHU-STree的独特特性，同时支持超过10个与行道树清单相关的任务。研究对两个关键任务(树种分类和单木分割)的代表性基线进行了基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验和深入的分析证明了多模态数据融合的显著潜力，并强调了跨域适用性是实际算法部署的关键前提。研究确定了关键挑战，并概述了未来可能的工作方向，包括多模态融合、多任务协作、跨域泛化、空间模式学习以及用于行道树资产管理的多模态大语言模型。&lt;h4&gt;结论&lt;/h4&gt;WHU-STree数据集为城市行道树研究提供了一个全面、多模态的资源，支持多种任务，并有助于推动相关算法的发展和应用。&lt;h4&gt;翻译&lt;/h4&gt;行道树对城市宜居性至关重要，提供生态和社会效益。在空间受限的城市环境中优化这些多功能资产需要建立详细、准确且动态更新的行道树清单。鉴于传统的地面调查耗时且劳动密集，利用移动测量系统(MMS)的自动化调查提供了更高效的解决方案。然而，现有的MMS获取的树数据集受限于小规模场景、有限的标注或单一模态，限制了它们用于综合分析的能力。为了解决这些局限性，我们引入了WHU-STree，这是一个跨城市、丰富标注和多模态的城市行道树数据集。WHU-STree在两个不同城市收集，集成了同步点云和高分辨率图像，包含21,007个标注的树实例，涵盖50个物种和2个形态参数。利用WHU-STree的独特特性，它可以同时支持超过10个与行道树清单相关的任务。我们针对两个关键任务(树种分类和单木分割)对代表性基线进行了基准测试。广泛的实验和深入的分析证明了多模态数据融合的显著潜力，并强调了跨域适用性是实际算法部署的关键前提。特别是，我们确定了关键挑战，并概述了未来可能的工作方向，包括多模态融合、多任务协作、跨域泛化、空间模式学习以及用于行道树资产管理的多模态大语言模型。WHU-STree数据集可通过以下网址获取：https://github.com/WHU-USI3DV/WHU-STree。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有城市街道树木数据集的三个限制：小规模场景、有限标注和单一模态。这个问题很重要，因为街道树木对城市宜居性至关重要，提供生态和社会效益，建立准确、动态更新的树木清单对优化城市环境中的这些多功能资产至关重要，而传统实地调查耗时耗力，现有数据集的限制又阻碍了自动化调查解决方案的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别现有数据集的三个主要限制（小规模场景、有限标注和单一模态）来思考问题，然后设计了一个跨越城市、丰富标注和多模态的数据集。作者在两个气候区显著不同的城市（南京和沈阳）收集数据，并对数据进行预处理和详细标注。作者借鉴了现有的树木分割和分类方法（如MinkNet、PointMLP、SegmentAnyTree等）以及多模态融合方法（如TSCMDL和LCPS）作为基准进行评估。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个跨越城市、丰富标注和多模态的城市街道树木数据集，整合点云和图像数据以支持多种树木清单相关任务。整体流程包括：1)在南京和沈阳使用移动测量系统收集数据；2)对数据进行预处理（降采样、自适应分区、异常点去除）；3)进行详细标注（树木实例、物种信息和形态参数）；4)采用两种数据分割策略（类别平衡分割和跨城市分割）；5)最终形成一个包含21,007个树木实例、50个物种和2个形态参数的大规模数据集。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)跨城市覆盖，涵盖两个气候区不同的城市；2)丰富标注，包含精确的个体树木定位、物种信息和形态参数；3)多模态数据，整合点云和全景图像；4)多任务支持，支持多种配置和任务。相比之前的工作，WHU-STree规模更大（21,007个树木实例），标注更丰富（包含物种和形态参数），是多模态的，且跨城市覆盖，而现有数据集通常是单模态、小规模、单一区域的，且标注有限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一个大规模、多模态、跨城市的街道树木基准数据集WHU-STree，通过整合精确的3D点云和高分辨率图像数据，支持多种与街道树木清单相关的任务，为城市树木自动化管理提供了宝贵资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Street trees are vital to urban livability, providing ecological and socialbenefits. Establishing a detailed, accurate, and dynamically updated streettree inventory has become essential for optimizing these multifunctional assetswithin space-constrained urban environments. Given that traditional fieldsurveys are time-consuming and labor-intensive, automated surveys utilizingMobile Mapping Systems (MMS) offer a more efficient solution. However, existingMMS-acquired tree datasets are limited by small-scale scene, limitedannotation, or single modality, restricting their utility for comprehensiveanalysis. To address these limitations, we introduce WHU-STree, a cross-city,richly annotated, and multi-modal urban street tree dataset. Collected acrosstwo distinct cities, WHU-STree integrates synchronized point clouds andhigh-resolution images, encompassing 21,007 annotated tree instances across 50species and 2 morphological parameters. Leveraging the unique characteristics,WHU-STree concurrently supports over 10 tasks related to street tree inventory.We benchmark representative baselines for two key tasks--tree speciesclassification and individual tree segmentation. Extensive experiments andin-depth analysis demonstrate the significant potential of multi-modal datafusion and underscore cross-domain applicability as a critical prerequisite forpractical algorithm deployment. In particular, we identify key challenges andoutline potential future works for fully exploiting WHU-STree, encompassingmulti-modal fusion, multi-task collaboration, cross-domain generalization,spatial pattern learning, and Multi-modal Large Language Model for street treeasset management. The WHU-STree dataset is accessible at:https://github.com/WHU-USI3DV/WHU-STree.</description>
      <author>example@mail.com (Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang)</author>
      <guid isPermaLink="false">2509.13172v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation</title>
      <link>http://arxiv.org/abs/2509.13149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MSDNet是一种多阶段蒸馏框架，用于4D雷达点云超分辨率，通过将密集LiDAR先验知识转移到雷达特征上，实现高质量重建和计算效率。&lt;h4&gt;背景&lt;/h4&gt;4D雷达超分辨率是自动驾驶感知中的基础问题，旨在将稀疏且带噪的点云重建为密集且几何一致的表示。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法训练成本高、推理延迟高和泛化能力差的问题，实现准确性和效率的平衡。&lt;h4&gt;方法&lt;/h4&gt;提出MSDNet框架，包括两个阶段：1)重建引导的特征蒸馏，通过特征重建对齐和加密学生特征；2)扩散引导的特征蒸馏，通过轻量级扩散网络优化特征。此外引入噪声适配器，自适应对齐噪声级别与扩散时间步。&lt;h4&gt;主要发现&lt;/h4&gt;在VoD和内部数据集上的实验表明，MSDNet实现了高保真重建和低延迟推理，并在下游任务中持续提高性能。&lt;h4&gt;结论&lt;/h4&gt;MSDNet成功平衡了准确性和效率，代码将在发表后公开。&lt;h4&gt;翻译&lt;/h4&gt;4D雷达超分辨率旨在将稀疏且带噪的点云重建为密集且几何一致的表示，是自动驾驶感知中的一个基础问题。然而，现有方法往往存在训练成本高或依赖复杂的基于扩散的采样，导致推理延迟高和泛化能力差，难以平衡准确性和效率。为解决这些限制，我们提出MSDNet，一个多阶段蒸馏框架，有效将密集LiDAR先验知识转移到4D雷达特征上，实现高质量重建和计算效率。第一阶段执行重建引导的特征蒸馏，通过特征重建对齐和加密学生特征。在第二阶段，我们提出扩散引导的特征蒸馏，将第一阶段蒸馏的特征视为教师表示的噪声版本，并通过轻量级扩散网络进行优化。此外，我们引入噪声适配器，自适应地将特征的噪声级别与预定义的扩散时间步对齐，实现更精确的降噪。在VoD和内部数据集上的大量实验表明，MSDNet在4D雷达点云超分辨率任务中实现了高保真重建和低延迟推理，并在下游任务中持续提高性能。代码将在发表后公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D雷达点云超分辨率问题，即如何将稀疏、有噪声的4D雷达点云重建为密集、几何一致的表示。这个问题在自动驾驶领域非常重要，因为4D雷达虽然能在恶劣天气条件下稳定工作，但其点云质量有限，严重影响了物体检测、场景理解和定位等精细感知任务的性能。当前方法要么训练成本高，要么推理延迟大且泛化能力差，难以平衡准确性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：信号级方法数据收集成本高且泛化能力有限，而基于扩散的方法（如R2LDM）虽然效果好但推理延迟高。作者借鉴了知识蒸馏的思想，将其应用于4D雷达超分辨率任务，设计了两个渐进阶段的蒸馏框架：第一阶段通过重建引导特征蒸馏（RGFD）进行初步知识转移，第二阶段通过扩散引导特征蒸馏（DGFD）进行细化。作者还引入了噪声适配器来精确对齐噪声水平，提高扩散效率。这种方法结合了知识蒸馏和扩散模型的优点，避免了各自的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多阶段知识蒸馏，将高分辨率LiDAR的几何先验高效转移到稀疏的4D雷达特征中，实现高质量且高效的点云超分辨率。整体流程包括：1) 使用VoxelNet提取LiDAR和4D雷达的BEV特征；2) 构建教师网络，通过S2D模块增强LiDAR特征并重建点云；3) 第一阶段RGFD通过特征重建网络将稀疏4D雷达特征转换为密集表示，并最小化与LiDAR特征的差异；4) 第二阶段DGFD将第一阶段结果视为噪声，通过噪声适配器和轻量级扩散网络进行去噪；5) 将去噪特征输入点云重建模块生成密集4D雷达点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次将知识蒸馏应用于4D雷达超分辨率，提出多阶段蒸馏框架；2) 设计重建引导特征蒸馏（RGFD）解决跨模态对齐挑战；3) 提出扩散引导特征蒸馏（DGFD）使用轻量级扩散网络进行特征细化；4) 引入噪声适配器自适应对齐噪声水平。相比之前工作，MSDNet避免了信号级方法的高数据成本和基于扩散方法的高推理延迟，通过两阶段蒸馏实现了高质量和效率的平衡。实验表明，MSDNet在保持高重建质量的同时，推理速度比R2LDM快89.6%，参数量减少一半。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MSDNet通过多阶段知识蒸馏框架，高效地将LiDAR的密集几何先验转移到4D雷达特征中，实现了高质量且低延迟的4D雷达点云超分辨率，显著提升了下游任务性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D radar super-resolution, which aims to reconstruct sparse and noisy pointclouds into dense and geometrically consistent representations, is afoundational problem in autonomous perception. However, existing methods oftensuffer from high training cost or rely on complex diffusion-based sampling,resulting in high inference latency and poor generalization, making itdifficult to balance accuracy and efficiency. To address these limitations, wepropose MSDNet, a multi-stage distillation framework that efficiently transfersdense LiDAR priors to 4D radar features to achieve both high reconstructionquality and computational efficiency. The first stage performsreconstruction-guided feature distillation, aligning and densifying thestudent's features through feature reconstruction. In the second stage, wepropose diffusion-guided feature distillation, which treats the stage-onedistilled features as a noisy version of the teacher's representations andrefines them via a lightweight diffusion network. Furthermore, we introduce anoise adapter that adaptively aligns the noise level of the feature with apredefined diffusion timestep, enabling a more precise denoising. Extensiveexperiments on the VoD and in-house datasets demonstrate that MSDNet achievesboth high-fidelity reconstruction and low-latency inference in the task of 4Dradar point cloud super-resolution, and consistently improves performance ondownstream tasks. The code will be publicly available upon publication.</description>
      <author>example@mail.com (Minqing Huang, Shouyi Lu, Boyuan Zheng, Ziyao Li, Xiao Tang, Guirong Zhuo)</author>
      <guid isPermaLink="false">2509.13149v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.13116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  An extension of our CVPR 2023 paper, "Weakly Supervised  Class-Agnostic Motion Prediction for Autonomous Driving," accepted for  publication in TPAMI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从激光雷达点云中进行弱自监督类别无关运动预测的新方法，通过利用场景中的前景/背景或非地面/地面线索作为监督信号，有效减少了标注需求，同时保持了良好的预测性能。&lt;h4&gt;背景&lt;/h4&gt;动态环境中的运动理解对自动驾驶至关重要，户外场景通常包含移动前景和静态背景，使运动理解与场景解析相关联。&lt;h4&gt;目的&lt;/h4&gt;开发一种弱监督和自监督的类别无关运动预测方法，减少对运动标注的依赖，同时保持或提高预测性能。&lt;h4&gt;方法&lt;/h4&gt;1) 提出用前景/背景掩码替代运动标注进行监督；2) 利用非地面/地面掩码作为前景/背景掩码的替代，进一步减少标注；3) 设计需要更少标注的弱监督方法和完全无标注的自监督方法；4) 开发鲁棒一致性感知Chamfer距离损失函数，结合多帧信息和鲁棒惩罚函数抑制异常值。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的弱监督和自监督模型优于现有的自监督对应模型，弱监督模型的性能甚至可以与一些监督模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地平衡了标注工作量和预测性能，为自动驾驶中的运动预测提供了一种实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解动态环境中的运动对自动驾驶至关重要，从而推动了类别无关运动预测的研究。在本工作中，我们研究了从激光雷达点云中进行弱自监督类别无关运动预测。户外场景通常包含移动前景和静态背景，使运动理解能够与场景解析相关联。基于这一观察，我们提出了一种新的弱监督范式，用完全或部分标注的前景/背景掩码（1%、0.1%）替代运动标注进行监督。为此，我们开发了一种利用前景/背景线索指导运动预测模型自监督学习的弱监督方法。由于前景运动通常发生在非地面区域，非地面/地面掩码可以作为前景/背景掩码的替代，进一步减少标注工作量。利用非地面/地面线索，我们提出了两种额外方法：一种需要更少（0.01%）前景/背景标注的弱监督方法，以及一种无需标注的自监督方法。此外，我们设计了一种鲁棒一致性感知Chamfer距离损失，结合多帧信息和鲁棒惩罚函数，以抑制自监督学习中的异常值。实验表明，我们的弱监督和自监督模型优于现有的自监督对应模型，我们的弱监督模型甚至可以与一些监督模型相媲美。这证明了我们的方法有效地平衡了标注工作量和性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶中运动预测对昂贵运动标注数据的依赖问题。在现实中，运动数据的获取成本高且困难，而准确理解动态环境中的运动对确保自动驾驶安全至关重要。传统方法需要大量标注数据，限制了其在开放场景和未知物体类别上的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到户外场景可分解为移动前景和静态背景，从而将运动理解与场景解析联系起来。他们借鉴了现有类别无关运动预测(如MotionNet、BE-STI)和弱监督/自监督方法(如SSMP、SelfMotion)的工作，创新性地提出用前景/背景掩码替代运动标注作为弱监督信号。此外，他们利用前景运动通常发生在非地面区域的特性，进一步用非地面/地面掩码减少标注需求，并设计了鲁棒一致性感知Chamfer Distance损失函数提高自监督学习效果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用场景解析信息(前景/背景或非地面/地面掩码)替代昂贵的运动标注作为监督信号。整体实现流程包括：1)预训练前景/背景分割网络(PreSegNet)或通过平面拟合生成非地面/地面点；2)训练运动预测网络(WeakMotionNet或SelfMotionNet)，包含运动预测头和辅助分割头；3)使用鲁棒一致性感知Chamfer Distance损失函数在前景/非地面点上进行自监督学习；4)同时用少量前景/背景掩码训练辅助分割头。最终实现从弱监督(0.01%标注)到完全无监督的高效运动预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出使用前景/背景掩码作为弱监督进行类别无关运动预测；2)创新性地用非地面/地面掩码替代前景/背景掩码，进一步减少标注需求；3)设计鲁棒一致性感知Chamfer Distance损失函数，利用多帧信息和鲁棒惩罚函数提高自监督学习的鲁棒性；4)提出三种方法：WeakMotion-FB(使用前景/背景掩码)、WeakMotion-NG(使用非地面/地面掩码，需极少标注)和SelfMotion-NG(完全无监督)。相比之前工作，本文方法显著降低了标注依赖(从1%运动标注降至0.01%前景/背景掩码)，同时性能优于或媲美全监督方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的弱监督和无监督学习范式，通过利用场景解析信息替代昂贵的运动标注，实现了高效的类别无关运动预测，显著降低了自动驾驶系统中运动预测模型的标注依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TPAMI.2025.3604036&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding motion in dynamic environments is critical for autonomousdriving, thereby motivating research on class-agnostic motion prediction. Inthis work, we investigate weakly and self-supervised class-agnostic motionprediction from LiDAR point clouds. Outdoor scenes typically consist of mobileforegrounds and static backgrounds, allowing motion understanding to beassociated with scene parsing. Based on this observation, we propose a novelweakly supervised paradigm that replaces motion annotations with fully orpartially annotated (1%, 0.1%) foreground/background masks for supervision. Tothis end, we develop a weakly supervised approach utilizingforeground/background cues to guide the self-supervised learning of motionprediction models. Since foreground motion generally occurs in non-groundregions, non-ground/ground masks can serve as an alternative toforeground/background masks, further reducing annotation effort. Leveragingnon-ground/ground cues, we propose two additional approaches: a weaklysupervised method requiring fewer (0.01%) foreground/background annotations,and a self-supervised method without annotations. Furthermore, we design aRobust Consistency-aware Chamfer Distance loss that incorporates multi-frameinformation and robust penalty functions to suppress outliers inself-supervised learning. Experiments show that our weakly and self-supervisedmodels outperform existing self-supervised counterparts, and our weaklysupervised models even rival some supervised ones. This demonstrates that ourapproaches effectively balance annotation effort and performance.</description>
      <author>example@mail.com (Ruibo Li, Hanyu Shi, Zhe Wang, Guosheng Lin)</author>
      <guid isPermaLink="false">2509.13116v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>MATTER: Multiscale Attention for Registration Error Regression</title>
      <link>http://arxiv.org/abs/2509.12924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于回归而非分类的点云配准质量验证方法，通过多尺度特征提取和注意力聚合机制，实现了对配准质量的更细粒度量化，并在多样化数据集上表现出色，特别是在处理具有异构空间密度的点云时。&lt;h4&gt;背景&lt;/h4&gt;点云配准(PCR)对于同时定位与地图构建(SLAM)和目标跟踪等许多下游任务至关重要，因此检测和量化配准错位(即PCR质量验证)是一个重要任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于回归的PCR质量验证方法，替代现有的基于分类的方法，实现更精细的配准质量量化；并通过多尺度特征提取和注意力聚合扩展特征表示，提高方法在异构密度点云上的表现。&lt;h4&gt;方法&lt;/h4&gt;使用回归而非分类进行PCR质量验证；采用多尺度提取和基于注意力的聚合来扩展与错位相关的特征；在多样化数据集上评估方法的准确性和鲁棒性；将方法应用于指导地图构建下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多样化数据集上提供了准确且稳健的配准误差估计；对于具有异构空间密度的点云尤其有效；当用于指导地图构建下游任务时，与最先进的基于分类的方法相比，可以在给定数量的重新配准帧数量下显著提高地图质量。&lt;h4&gt;结论&lt;/h4&gt;基于回归的PCR质量验证方法比现有的基于分类的方法更有效；通过多尺度特征提取和注意力聚合实现了更精细的质量量化；在实际应用中，该方法可以优化地图构建过程，提高地图质量。&lt;h4&gt;翻译&lt;/h4&gt;点云配准(PCR)对于许多下游任务至关重要，例如同时定位与地图构建(SLAM)和目标跟踪。这使得检测和量化配准错位，即PCR质量验证，成为一个重要任务。所有现有方法都将验证视为分类任务，旨在将PCR质量分配到几个类别中。在本工作中，我们使用回归进行PCR验证，允许对配准质量进行更细粒度的量化。我们还通过使用多尺度提取和基于注意力的聚合来扩展先前使用的与错位相关的特征。这导致在多样化数据集上准确且稳健的配准误差估计，特别是对于具有异构空间密度的点云。此外，当用于指导地图构建下游任务时，与最先进的基于分类的方法相比，我们的方法在给定数量的重新配准帧数量下显著提高了地图质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云配准(PCR)质量评估的问题，特别是检测和量化配准误差。这个问题在现实中非常重要，因为点云配准是SLAM、3D重建和机器人导航等下游任务的基础。配准误差会传播到下游模块，导致地图扭曲或导航失败。在挑战性场景下，如优化陷入局部最小值、运动失真或测量噪声时，配准误差仍然显著，因此需要准确评估配准质量以便采取纠正措施。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，即它们都是基于分类的，只能粗略地将错位分为几个级别。作者借鉴了FACT等工作的特征提取方法，但将其从分类改为回归任务。作者注意到单尺度特征的局限性，特别是小半径在初始对齐差时可能失败，而大半径包含非重叠区域使熵和Sinkhorn散度不可靠。因此，作者设计了多尺度注意力机制，在三个不同半径(7.5m、4.0m、2.5m)上提取特征，并通过注意力机制自适应地选择最适合的尺度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云错位评估从分类任务转变为回归任务，实现更细粒度的配准质量评估，并使用多尺度注意力机制来融合不同几何尺度的特征，提高对点云空间密度异质性的鲁棒性。整体实现流程包括：1)特征提取：为每个锚点计算局部特征(分别和联合的微分熵、Sinkhorn散度、覆盖率比)和全局特征(共视分数、距离、源标志)；2)多尺度特征提取：在三个不同半径尺度上提取特征；3)多尺度交叉注意力：使用查询学习MLP生成查询，通过多头注意力机制聚合多尺度特征；4)回归预测：使用PointTransformer和MLP预测对齐误差。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)任务转换：将点云错位分类扩展为点云错位回归，实现更细粒度的评估；2)多尺度注意力机制：提取多尺度几何特征并自适应选择最适合的尺度；3)鲁棒性增强：特别处理点云中空间密度异质性的情况。相比之前的工作，MATTER提供连续的误差预测而非离散分类，使用多尺度注意力而非固定尺度特征，实验表明在多个数据集上均优于现有方法，特别是在下游任务中能更好地指导重配准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MATTER通过引入多尺度注意力机制将点云配准质量评估从分类转变为回归任务，实现了更精确、更鲁棒的配准误差估计，特别是在处理空间密度异质的点云时表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration (PCR) is crucial for many downstream tasks, such assimultaneous localization and mapping (SLAM) and object tracking. This makesdetecting and quantifying registration misalignment, i.e.,~{\it PCR qualityvalidation}, an important task. All existing methods treat validation as aclassification task, aiming to assign the PCR quality to a few classes. In thiswork, we instead use regression for PCR validation, allowing for a morefine-grained quantification of the registration quality. We also extendpreviously used misalignment-related features by using multiscale extractionand attention-based aggregation. This leads to accurate and robust registrationerror estimation on diverse datasets, especially for point clouds withheterogeneous spatial densities. Furthermore, when used to guide a mappingdownstream task, our method significantly improves the mapping quality for agiven amount of re-registered frames, compared to the state-of-the-artclassification-based method.</description>
      <author>example@mail.com (Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik Forssén)</author>
      <guid isPermaLink="false">2509.12924v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.12878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种原型扩展网络(PENet)，用于解决少样本3D点云语义分割中的类内多样性和集合间不一致性问题。该方法利用扩散模型的条件编码器提供可泛化特征，通过双流学习器架构构建大容量原型，并通过原型同化模块和原型校准机制优化原型表示。&lt;h4&gt;背景&lt;/h4&gt;少样本3D点云语义分割旨在使用少量标注的支持样本分割新类别。现有的基于原型的方法虽然有一定效果，但面临类内多样性不足和集合间不一致性两大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于原型的方法在少样本3D点云语义分割中面临的类内多样性和集合间不一致性问题，提高对新类别的分割性能。&lt;h4&gt;方法&lt;/h4&gt;提出原型扩展网络(PENet)，采用双流学习器架构：保留传统的全监督内在学习器提取代表性特征，同时引入扩散学习器提供丰富的可泛化特征。双原型通过原型同化模块处理，采用推拉交叉引导注意力块迭代对齐原型与查询空间，并通过原型校准机制防止语义漂移。&lt;h4&gt;主要发现&lt;/h4&gt;在S3DIS和ScanNet数据集上的实验表明，PENet在各种少样本设置下显著优于最先进的方法。扩散模型提供的可泛化特征能有效扩展原型的表示能力，解决类内多样性问题；原型同化模块和校准机制能有效解决集合间不一致性问题。&lt;h4&gt;结论&lt;/h4&gt;原型扩展网络(PENet)通过结合传统监督学习和扩散模型的可泛化特征，有效解决了少样本3D点云语义分割中的关键挑战，显著提高了分割性能。&lt;h4&gt;翻译&lt;/h4&gt;少样本3D点云语义分割旨在使用少量标注的支持样本分割新类别。虽然现有的基于原型的方法显示出潜力，但它们受到两个关键挑战的限制：(1)类内多样性，原型的有限表示能力无法覆盖类的全部变化；(2)集合间不一致性，从支持集导出的原型与查询特征空间不匹配。受扩散模型强大生成能力的启发，我们重新利用其预训练条件编码器为原型提供一种新的可泛化特征来源，以扩展原型的表示范围。在此设置下，我们引入了原型扩展网络(PENet)，这是一个从两个互补特征源构建大容量原型的框架。PENet采用双流学习器架构：它保留传统的全监督内在学习器(IL)来提取代表性特征，同时引入新的扩散学习器(DL)来提供丰富的可泛化特征。然后，双原型通过原型同化模块(PAM)处理，该模块采用新颖的推拉交叉引导注意力块迭代地将原型与查询空间对齐。此外，原型校准机制(PCM)正则化最终的大容量原型以防止语义漂移。在S3DIS和ScanNet数据集上的大量实验表明，PENet在各种少样本设置下显著优于最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决少样本3D点云语义分割中的两个关键挑战：类内多样性（原型的有限表示能力无法覆盖一个类的全部变化）和集合间不一致性（支持集原型与查询特征空间不对齐）。这个问题在现实中很重要，因为3D点云语义分割在自动驾驶、机器人和增强现实等领域有广泛应用，而完全监督学习需要大量昂贵的标注数据。少样本学习方法使模型仅通过少量标注样本就能泛化到新类别，大大减少了对标注数据的依赖。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析认识到现有基于原型的方法面临相互关联的挑战：容量小的原型即使对齐也无法覆盖类的全部变化，而容量大的原型如果不对齐则无法在查询空间中有效工作。因此，他们提出通过'原型扩展'来增加原型的表示能力。他们借鉴了元学习框架，利用扩散模型的生成能力重新调整其预训练条件编码器，并采用双流架构结合传统监督学习和自监督学习。具体设计包括保留内在学习者提取代表性特征，引入扩散学习者提供可泛化特征，设计原型同化模块迭代对齐原型，以及引入原型校准机制防止语义漂移。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建'大容量原型'来解决少样本3D点云语义分割中的挑战，从两个互补特征源构建原型：一个来自传统监督学习（提供代表性特征），另一个来自扩散模型（提供可泛化特征）。整体流程包括：1)双流特征提取（内在学习者和扩散学习者）；2)从支持集特征生成初始原型；3)通过原型同化模块迭代对齐双原型与查询空间；4)将对齐后的原型融合成最终的大容量原型；5)使用原型校准机制防止语义漂移；6)用最终原型对查询点云进行语义分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)原型扩展网络框架，首次将扩散模型应用于少样本3D点云语义分割；2)双流学习架构，结合传统监督学习和扩散模型特征；3)原型同化模块，采用推-拉交叉引导注意力机制迭代对齐原型；4)原型校准机制，防止语义漂移。相比之前工作的不同在于：大多数方法依赖单一特征源，而本文从两个互补特征源构建原型；传统方法采用单步对齐，本文采用迭代推-拉机制；本文首次利用扩散模型条件编码器作为特征提取器；现有方法通过增加原型数量而非扩展单个原型容量来解决多样性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了原型扩展网络（PENet），通过融合来自传统监督学习和扩散模型的互补特征构建大容量原型，有效解决了少样本3D点云语义分割中的类内多样性和集合间不一致性问题，显著提升了模型在未见类别上的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot 3D point cloud semantic segmentation aims to segment novelcategories using a minimal number of annotated support samples. While existingprototype-based methods have shown promise, they are constrained by twocritical challenges: (1) Intra-class Diversity, where a prototype's limitedrepresentational capacity fails to cover a class's full variations, and (2)Inter-set Inconsistency, where prototypes derived from the support set aremisaligned with the query feature space. Motivated by the powerful generativecapability of diffusion model, we re-purpose its pre-trained conditionalencoder to provide a novel source of generalizable features for expanding theprototype's representational range. Under this setup, we introduce thePrototype Expansion Network (PENet), a framework that constructs big-capacityprototypes from two complementary feature sources. PENet employs a dual-streamlearner architecture: it retains a conventional fully supervised IntrinsicLearner (IL) to distill representative features, while introducing a novelDiffusion Learner (DL) to provide rich generalizable features. The resultingdual prototypes are then processed by a Prototype Assimilation Module (PAM),which adopts a novel push-pull cross-guidance attention block to iterativelyalign the prototypes with the query space. Furthermore, a Prototype CalibrationMechanism (PCM) regularizes the final big capacity prototype to preventsemantic drift. Extensive experiments on the S3DIS and ScanNet datasetsdemonstrate that PENet significantly outperforms state-of-the-art methodsacross various few-shot settings.</description>
      <author>example@mail.com (Qianguang Zhao, Dongli Wang, Yan Zhou, Jianxun Li, Richard Irampa)</author>
      <guid isPermaLink="false">2509.12878v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>A-TDOM: Active TDOM via On-the-Fly 3DGS</title>
      <link>http://arxiv.org/abs/2509.12759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种称为A-TDOM的近实时真正数字正射地图生成方法，基于即时3D高斯优化技术，能够在获取每张图像后快速计算姿态和稀疏点云，并将新的高斯函数集成和优化到先前未看到或粗略重建的区域，实现近实时渲染。&lt;h4&gt;背景&lt;/h4&gt;真正数字正射地图(TDOM)是城市管理、城市规划、土地测量等领域的关键地理空间产品。然而，传统TDOM生成方法依赖复杂的离线摄影测量流程，导致延迟阻碍了实时应用，且因摄像机姿态不准确、数字表面模型和场景遮挡等问题，质量可能下降。&lt;h4&gt;目的&lt;/h4&gt;解决传统TDOM生成方法的延迟问题，实现近实时生成，同时保持可接受的渲染质量和几何精度。&lt;h4&gt;方法&lt;/h4&gt;提出A-TDOM方法，基于即时3D高斯优化：每获取一张图像后，通过即时SfM计算姿态和稀疏点云；将新的高斯函数集成并优化到先前未看到或粗略重建的区域；结合正交渲染，在每次更新3D高斯场后立即渲染。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的实验表明，A-TDOM能够在近实时主动渲染TDOM，每张新图像的3D高斯优化仅需几秒钟，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;结论&lt;/h4&gt;A-TDOM方法有效解决了传统TDOM生成方法的延迟问题，通过即时3D高斯优化实现了近实时生成，同时保持了良好的质量和精度，为需要实时TDOM的应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;真正数字正射地图(TDOM)是城市管理、城市规划、土地测量等各个领域中的关键地理空间产品。然而，传统的TDOM生成方法通常依赖于复杂的离线摄影测量流程，导致延迟阻碍了实时应用。此外，由于摄像机姿态不准确、数字表面模型(DSM)和场景遮挡等挑战，TDOM的质量可能会下降。为了解决这些挑战，本文介绍了A-TDOM，一种基于即时3D高斯优化的近实时TDOM生成方法。每获取一张图像，其姿态和稀疏点云通过即时SfM计算。然后新的高斯函数被集成并优化到先前未看到或粗略重建的区域。通过与正交渲染结合，A-TDOM可以在每次更新3D高斯场后立即渲染。在多个基准测试上的初步实验表明，所提出的A-TDOM能够在近实时主动渲染TDOM，每张新图像的3D高斯优化仅需几秒钟，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统TDOM（真正的数字正射影像图）生成方法的实时性问题。传统方法依赖复杂的离线摄影测量流程，导致处理延迟，无法满足实时应用需求，同时还存在相机姿态不准确、数字表面模型和场景遮挡等问题影响质量。这个问题在现实中非常重要，因为TDOM是城市管理、城市规划、土地测量等领域的关键地理空间产品，实时生成TDOM可以大大提高工作效率，支持灾害监测、城市规划动态调整、土地资源调查等场景中的及时决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有TDOM生成方法的局限性，包括传统方法计算成本高和新兴方法仍采用离线处理。作者借鉴了多项现有工作：基于On-the-Fly SfM进行实时姿态估计、利用3D高斯投影技术进行场景表示、参考Tortho-Gaussian和Ortho-3DGS等3DGS-based方法，以及Gaussian On-the-Fly Splatting的优化策略。作者的创新思路是将这些技术整合到一个在线框架中，通过即时更新和优化3DGS场，实现近实时TDOM生成，并设计了高斯采样和集成方法替代原始3DGS中的密集化策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; A-TDOM方法的核心思想是通过即时更新和优化3D高斯投影场(3DGS)，实现近实时的TDOM生成，无需依赖数字表面模型(DSM)和显式遮挡检测。整体流程是：1)初始化训练一个初始3DGS场；2)对于每张新图像：a)使用On-the-Fly SfM计算姿态并更新稀疏点云；b)基于稀疏点云重投影进行Delaunay三角剖分，创建掩码确定关键优化区域；c)比较当前3DGS场渲染与输入图像的梯度图，识别需要额外高斯集成的区域；d)在选定区域内采样点，生成新高斯函数并集成；e)优化更新后的3DGS场；f)通过正交投影生成更新的TDOM。这一流程允许系统在图像采集过程中持续更新3D表示并实时生成高质量TDOM。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)近实时TDOM生成框架，首次实现近实时生成；2)高斯采样和集成方法，替代原始3DGS密集化策略；3)关键区域确定机制，通过Delaunay三角掩码确定优化区域；4)自适应优化策略，包括自适应学习率更新和局部优化；5)基于投影矩阵修改的正交投影方法。相比之前的工作，A-TDOM的主要不同在于：将离线处理转变为在线近实时处理；无需依赖DSM和显式遮挡检测；通过高斯采样和集成更有效地扩展3DGS场；采用自适应优化加速处理；实现了图像采集与TDOM生成的同步进行。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; A-TDOM通过创新的即时3D高斯投影优化和正交投影技术，首次实现了无需数字表面模型和遮挡检测的近实时真正的数字正射影像图生成，显著提高了地理空间信息获取的效率和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product invarious fields such as urban management, city planning, land surveying, etc.However, traditional TDOM generation methods generally rely on a complexoffline photogrammetric pipeline, resulting in delays that hinder real-timeapplications. Moreover, the quality of TDOM may degrade due to variouschallenges, such as inaccurate camera poses or Digital Surface Model (DSM) andscene occlusions. To address these challenges, this work introduces A-TDOM, anear real-time TDOM generation method based on On-the-Fly 3DGS optimization. Aseach image is acquired, its pose and sparse point cloud are computed viaOn-the-Fly SfM. Then new Gaussians are integrated and optimized into previouslyunseen or coarsely reconstructed regions. By integrating with orthogonalsplatting, A-TDOM can render just after each update of a new 3DGS field.Initial experiments on multiple benchmarks show that the proposed A-TDOM iscapable of actively rendering TDOM in near real-time, with 3DGS optimizationfor each new image in seconds while maintaining acceptable rendering qualityand TDOM geometric accuracy.</description>
      <author>example@mail.com (Yiwei Xu, Xiang Wang, Yifei Yu, Wentian Gan, Luca Morelli, Giulio Perda, Xiongwu Xiao, Zongqian Zhan, Xin Wang, Fabio Remondino)</author>
      <guid isPermaLink="false">2509.12759v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>DisorientLiDAR: Physical Attacks on LiDAR-based Localization</title>
      <link>http://arxiv.org/abs/2509.12595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DisorientLiDAR的新型对抗攻击框架，针对基于激光雷达的定位系统。通过逆向工程定位模型，攻击者可以识别并移除关键点，从而破坏自动驾驶汽车的定位能力。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型容易受到视觉上难以察觉的扰动的对抗攻击，这对自动驾驶汽车的定位构成严重安全挑战。然而，针对定位的攻击探索很少，因为大多数对抗攻击应用于3D感知而非定位系统。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对基于激光雷达(LiDAR)的定位系统的对抗攻击方法，以揭示自动驾驶定位系统的安全漏洞。&lt;h4&gt;方法&lt;/h4&gt;提出DisorientLiDAR攻击框架，通过逆向工程定位模型(如特征提取网络)识别关键点，策略性地移除这些关键点区域来破坏基于激光雷达的定位。实验在KITTI数据集上使用三种点云配准模型(HRegNet、D3Feat和GeoTransformer)进行评估，并在Autoware自动驾驶平台和物理世界中验证了攻击效果。&lt;h4&gt;主要发现&lt;/h4&gt;移除包含Top-K关键点的区域显著降低了点云配准模型的精度；在Autoware平台上，隐藏几个关键区域会导致明显的定位漂移；使用近红外吸收材料在物理世界中隐藏关键区域可以复制数字攻击效果。&lt;h4&gt;结论&lt;/h4&gt;所提出的DisorientLiDAR攻击框架成功展示了自动驾驶定位系统的脆弱性，通过物理世界的攻击验证了方法的真实性和通用性，为提高自动驾驶系统的安全性提供了重要参考。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型已被证明容易受到具有视觉上难以察觉的扰动的对抗攻击。即使这对自动驾驶汽车的定位构成了严重的安全挑战，但针对它的攻击探索很少，因为大多数对抗攻击已被应用于3D感知。在这项工作中，我们提出了一种名为DisorientLiDAR的新型对抗攻击框架，针对基于激光雷达的定位。通过逆向工程定位模型(例如特征提取网络)，攻击者可以识别关键点并策略性地移除它们，从而破坏基于激光雷达的定位。我们的提案首先使用KITTI数据集在三种最先进的点云配准模型(HRegNet、D3Feat和GeoTransformer)上进行了评估。实验结果表明，移除包含Top-K关键点的区域显著降低了它们的配准精度。我们进一步验证了攻击对Autoware自动驾驶平台的影响，在那里隐藏仅几个关键区域就会引起明显的定位漂移。最后，我们通过使用近红外吸收材料隐藏关键区域，将攻击扩展到物理世界，从而成功复制了在KITTI数据中观察到的攻击效果。这一步骤更接近真实的物理世界攻击，证明了我们提案的真实性和通用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文研究如何对基于LiDAR的自动驾驶汽车定位系统进行物理攻击。这个问题非常重要，因为自动驾驶汽车的安全高度依赖准确定位，而定位错误可能导致严重事故。尽管已有研究证明深度学习模型对对抗攻击很脆弱，但对定位系统的攻击探索很少，且即使有GPS和IMU支持，这种攻击仍然有效，对自动驾驶安全构成重大威胁。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有的点云注册方法，发现它们都依赖检测和匹配关键几何特征区域。基于这一观察，作者设计了一种攻击策略：通过逆向工程定位模型识别关键点，然后使用红外吸收材料物理覆盖这些区域，使它们从LiDAR扫描中'消失'。作者借鉴了现有的LiDAR对抗攻击工作，但与之前需要物理访问LiDAR传感器的方法不同，这种方法只需在道路旁放置材料即可实施，更加隐蔽且易于部署。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是识别并移除对LiDAR定位系统最关键的几何特征区域，干扰点云匹配过程导致定位错误。整体流程包括：1)使用与受害者相同的定位模型提取高置信度关键点；2)选择Top-K最显著的关键点；3)用红外吸收材料覆盖这些关键区域对应的物理位置；4)当受害车辆经过时，其LiDAR无法获取这些区域数据，导致定位错误；5)评估攻击效果。为确保攻击可行性和隐蔽性，作者还提出了高度过滤、轨迹接近检查、重叠处理和埋伏工具放置等筛选步骤。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出首个通过物理隐藏关键区域干扰定位系统的攻击框架；无需物理访问LiDAR传感器；在多种点云注册模型上验证了攻击通用性；在真实世界车辆和商业平台上进行了物理攻击验证；系统化评估了攻击参数影响。相比之前工作，本文专注于定位系统而非感知模块；通过物理遮挡而非激光注入实施攻击；在多种环境和LiDAR配置上评估；同时提出了防御策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提出并验证了一种通过物理遮挡关键区域来干扰基于LiDAR的自动驾驶定位系统的攻击方法，揭示了定位系统的重要安全漏洞，并为开发更安全的定位系统提供了方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models have been shown to be susceptible to adversarial attackswith visually imperceptible perturbations. Even this poses a serious securitychallenge for the localization of self-driving cars, there has been very littleexploration of attack on it, as most of adversarial attacks have been appliedto 3D perception. In this work, we propose a novel adversarial attack frameworkcalled DisorientLiDAR targeting LiDAR-based localization. Byreverse-engineering localization models (e.g., feature extraction networks),adversaries can identify critical keypoints and strategically remove them,thereby disrupting LiDAR-based localization. Our proposal is first evaluated onthree state-of-the-art point-cloud registration models (HRegNet, D3Feat, andGeoTransformer) using the KITTI dataset. Experimental results demonstrate thatremoving regions containing Top-K keypoints significantly degrades theirregistration accuracy. We further validate the attack's impact on the Autowareautonomous driving platform, where hiding merely a few critical regions inducesnoticeable localization drift. Finally, we extended our attacks to the physicalworld by hiding critical regions with near-infrared absorptive materials,thereby successfully replicate the attack effects observed in KITTI data. Thisstep has been closer toward the realistic physical-world attack thatdemonstrate the veracity and generality of our proposal.</description>
      <author>example@mail.com (Yizhen Lao, Yu Zhang, Ziting Wang, Chengbo Wang, Yifei Xue, Wanpeng Shao)</author>
      <guid isPermaLink="false">2509.12595v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery</title>
      <link>http://arxiv.org/abs/2509.12511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于几何感知的计算机视觉流水线，用于从RGB-D图像中估计茎直径，旨在解决传统测量方法劳动强度大、易出错的问题，为作物育种提供高通量表型分析解决方案。&lt;h4&gt;背景&lt;/h4&gt;准确的高通量表型分析是现代作物育种项目的关键组成部分，特别是对于改善机械稳定性、生物量生产和抗病性等性状。茎直径是一个关键的结构性状，但传统测量方法劳动密集、容易出错，不适合可扩展的表型分析。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确、高效测量茎直径的方法，支持作物育种和农学研究中的高通量表型分析。&lt;h4&gt;方法&lt;/h4&gt;提出一种几何感知的计算机视觉流水线，结合基于深度学习的实例分割、3D点云重建和通过主成分分析(PCA)的轴对齐切片，以执行鲁棒的直径估计。&lt;h4&gt;主要发现&lt;/h4&gt;通过减轻曲率、遮挡和图像噪声的影响，该方法提供了可扩展且可靠的解决方案，支持育种和农学研究中的高通量表型分析。&lt;h4&gt;结论&lt;/h4&gt;该几何感知的计算机视觉方法为茎直径测量提供了一种高效、准确的解决方案，克服了传统方法的局限性，为作物育种提供了重要工具。&lt;h4&gt;翻译&lt;/h4&gt;准确的高通量表型分析是现代作物育种项目的关键组成部分，特别是对于改善机械稳定性、生物量生产和抗病性等性状。茎直径是一个关键的结构性状，但传统测量方法劳动密集、容易出错，不适合可扩展的表型分析。在本文中，我们提出了一种几何感知的计算机视觉流水线，用于从RGB-D图像中估计茎直径。我们的方法结合了基于深度学习的实例分割、3D点云重建和通过主成分分析(PCA)的轴对齐切片，以执行鲁棒的直径估计。通过减轻曲率、遮挡和图像噪声的影响，这种方法为育种和农学研究中的高通量表型分析提供了可扩展且可靠的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从RGB-D图像中自动估计植物茎秆直径的问题。这个问题很重要，因为茎秆直径影响作物的抗倒伏能力、生物量分配、水力功能和疾病易感性，是作物表型分析的关键指标。传统测量方法劳动密集且不适合高通量表型分析，而随着2050年全球人口预计达到97亿，自动化测量对确保粮食安全和提高农业生产力至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统测量方法的局限性，以及RGB-D传感器在自动化测量中的潜力。他们注意到现有方法（如颜色阈值、边缘检测）假设茎秆垂直对齐，限制了可靠性；更先进的方法虽改善了检测，但缺乏稳健的轴对齐。作者借鉴了深度学习在农业环境中的应用，结合了基于深度学习的实例隔离、3D点云重建和PCA轴估计，设计出能处理茎杆曲率和任意方向的轴对齐测量方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过确定茎杆的真实主轴方向，然后垂直于此轴进行横截面切片来提高测量准确性。整体流程包括：1)使用RGB-D相机采集数据；2)用YOLOv11x-seg模型分割茎杆区域；3)构建3D点云并进行过滤；4)应用PCA确定茎杆主轴；5)垂直于主轴将点云分为100个切片；6)对每个切片应用DBSCAN过滤并使用95百分位数估计半径；7)聚合切片结果得到最终直径估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)几何感知的计算机视觉管道，结合深度学习、3D重建和PCA轴估计；2)轴对齐的横截面测量，提高准确性和鲁棒性；3)能有效处理茎杆曲率和任意方向；4)使用95百分位数圆拟合和1-标准差聚合处理噪声；5)发现初始统计异常值移除过滤器冗余。相比之前工作，不同之处在于传统方法假设垂直对齐，而先进方法缺乏稳健轴对齐；本文方法能处理各种方向，并通过统计方法有效处理传感器噪声。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文开发了一种基于几何感知的计算机视觉管道，通过结合深度学习实例分割、3D点云重建和轴对齐的横截面切片，实现了从RGB-D图像中高精度、自动化地估计植物茎杆直径，为作物表型分析和农业管理提供了强大的工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate, high-throughput phenotyping is a critical component of modern cropbreeding programs, especially for improving traits such as mechanicalstability, biomass production, and disease resistance. Stalk diameter is a keystructural trait, but traditional measurement methods are labor-intensive,error-prone, and unsuitable for scalable phenotyping. In this paper, we presenta geometry-aware computer vision pipeline for estimating stalk diameter fromRGB-D imagery. Our method integrates deep learning-based instance segmentation,3D point cloud reconstruction, and axis-aligned slicing via Principal ComponentAnalysis (PCA) to perform robust diameter estimation. By mitigating the effectsof curvature, occlusion, and image noise, this approach offers a scalable andreliable solution to support high-throughput phenotyping in breeding andagronomic research.</description>
      <author>example@mail.com (Benjamin Vail, Rahul Harsha Cheppally, Ajay Sharda, Sidharth Rai)</author>
      <guid isPermaLink="false">2509.12511v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Artist-Created Mesh Generation from Raw Observation</title>
      <link>http://arxiv.org/abs/2509.12501v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种端到端框架，能够从嘈杂或不完整的点云生成高质量艺术家风格的网格，通过将3D点云精炼重新表述为2D修复任务，利用生成模型实现。&lt;h4&gt;背景&lt;/h4&gt;艺术家创建的网格对商业图形管道至关重要，因为它们与动画和纹理工具兼容且渲染效率高。然而，现有方法通常假设输入干净完整或依赖复杂的多阶段管道，限制了它们在现实场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种端到端方法，能够精炼输入点云并直接生成高质量、艺术家风格的网格，适用于现实世界传感器获取的嘈杂或不完整点云数据。&lt;h4&gt;方法&lt;/h4&gt;提出了一种将3D点云精炼重新表述为2D修复任务的方法，从而能够利用强大的生成模型。该方法是一个端到端的框架，直接从原始点云生成高质量网格。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet数据集上的初步结果表明，该框架能够生成干净、完整的网格，证明了其在处理现实世界传感器数据方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的端到端框架为从嘈杂或不完整的点云生成高质量艺术家风格网格提供了有效解决方案，适用于商业图形管道和现实应用场景。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种端到端框架，用于从嘈杂或不完整的点云生成艺术家风格的网格，例如由现实世界传感器如LiDAR或移动RGB-D相机捕获的点云。艺术家创建的网格对商业图形管道至关重要，因为它们与动画和纹理工具兼容，并且在渲染中效率高。然而，现有方法通常假设输入干净完整或依赖复杂的多阶段管道，限制了它们在现实场景中的应用。为此，我们提出了一种端到端方法，可以精炼输入点云并直接生成高质量、艺术家风格的网格。我们方法的核心是将3D点云精炼重新表述为2D修复任务，从而能够使用强大的生成模型。在ShapeNet数据集上的初步结果证明了我们的框架在生成干净、完整网格方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从真实世界传感器（如LiDAR或移动RGB-D相机）捕获的嘈杂或不完整点云中生成高质量艺术风格网格的问题。这个问题在现实中很重要，因为艺术风格网格与商业图形管线兼容，适合动画和纹理处理，且渲染效率高，而现有方法往往需要干净完整的输入或复杂的多阶段处理流程，难以应用于真实世界场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，认识到艺术风格网格生成需要干净输入，而真实传感器数据往往嘈杂不完整。他们创新性地将3D点云精炼重新表述为2D修复问题，利用成熟的2D生成模型能力。方法设计包括将3D点云投影到球形地图集、应用扩散模型修复、再映射回3D并生成网格。作者借鉴了Stable Diffusion作为2D生成模型，MeshAnything V2作为网格生成模型，以及将3D转化为2D处理的高斯地图集方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云精炼问题转化为2D修复任务，利用强大的2D生成模型处理3D数据。整体流程分为四步：1)将3D点云通过最优传输映射到球面，再通过等距矩形投影转换为2D地图集；2)使用微调的扩散模型修复不完整的地图集；3)将修复后的地图集映射回3D，生成干净完整的点云；4)将点云输入MeshAnything V2模型生成最终艺术风格网格。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)端到端框架直接从原始点云生成艺术风格网格；2)首次将3D点云精炼重新表述为2D修复任务；3)使用球形地图集表示保留3D几何信息；4)利用扩散模型进行地图集修复。相比之前工作，不同之处在于：可直接处理嘈杂不完整输入而非假设干净数据；采用统一端到端框架而非多阶段处理；利用成熟的2D生成模型而非专门开发3D模型；直接生成适合商业管线的拓扑结构网格。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新方法，通过将3D点云精炼转化为2D修复任务，实现了从嘈杂或不完整的真实世界传感器数据直接生成高质量艺术风格网格的端到端框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an end-to-end framework for generating artist-style meshes fromnoisy or incomplete point clouds, such as those captured by real-world sensorslike LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial forcommercial graphics pipelines due to their compatibility with animation andtexturing tools and their efficiency in rendering. However, existing approachesoften assume clean, complete inputs or rely on complex multi-stage pipelines,limiting their applicability in real-world scenarios. To address this, wepropose an end-to-end method that refines the input point cloud and directlyproduces high-quality, artist-style meshes. At the core of our approach is anovel reformulation of 3D point cloud refinement as a 2D inpainting task,enabling the use of powerful generative models. Preliminary results on theShapeNet dataset demonstrate the promise of our framework in producing clean,complete meshes.</description>
      <author>example@mail.com (Yao He, Youngjoong Kwon, Wenxiao Cai, Ehsan Adeli)</author>
      <guid isPermaLink="false">2509.12501v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</title>
      <link>http://arxiv.org/abs/2509.12452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  57 Pages, 4 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对点云处理的深度学习方法和数据集进行了元综述，重点关注实际应用价值而非仅关注网络架构。&lt;h4&gt;背景&lt;/h4&gt;点云处理是大地测量学和计算机视觉的基础任务，支持从空中到地面的多种应用，包括测绘、环境监测、城市建模、自动驾驶等。深度学习的发展使基于学习的方法主导了点云处理算法，但这些方法大多尚未转化为实际应用。&lt;h4&gt;目的&lt;/h4&gt;提供点云处理关键任务的深度学习方法和数据集的元综述，关注这些方法在实际应用中的价值和挑战。&lt;h4&gt;方法&lt;/h4&gt;回顾了涵盖场景补全、配准、语义分割和建模等关键任务的深度学习方法和数据集，并通过分析这些任务支持的城市和环境应用，识别方法转化为应用时需要填补的差距。&lt;h4&gt;主要发现&lt;/h4&gt;现有调查主要集中在适应无序点云的网络架构更新上，忽略了典型点云处理应用中的实际价值，如大量数据处理、多样化场景内容、变化的点密度和数据模态等实际考量。&lt;h4&gt;结论&lt;/h4&gt;在算法和实践两方面对所调查的方法得出了结论，强调了点云处理从理论研究向实际应用转化的重要性。&lt;h4&gt;翻译&lt;/h4&gt;点云处理作为大地测量学和计算机视觉领域的基础任务，一直支持着从空中到地面不同规模的任务和应用，包括测绘、环境监测、城市/树木结构建模、自动驾驶、机器人技术、灾害响应等。由于深度学习的快速发展，点云处理算法现在几乎明确由基于学习的方法主导，但其中大多数尚未转化为实际应用实践。现有调查主要集中在不断更新的网络架构上，以适应无序点云，很大程度上忽略了它们在典型点云处理应用中的实际价值，在这些应用中需要考虑大量数据、多样化场景内容、变化的点密度和数据模态。在本文中，我们对点云处理中使用的深度学习方法和数据集进行了元综述，涵盖了场景补全、配准、语义分割和建模等关键任务。通过回顾这些任务可以支持的广泛城市和环境应用，我们确定了方法转化为应用时需要填补的差距，并对所调查方法的算法和实践两方面得出了结论。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云处理算法在实际应用中的价值和挑战问题。现实中，点云处理支持从空中到地面的多种应用如自动驾驶、导航、测绘等，但现有算法大多尚未过渡到实际应用。这个问题很重要，因为实际应用中需要考虑大量数据、多样化场景内容、不同点密度和数据模态等因素，而这些在现有综述中被忽视，导致理论与实践之间存在差距。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察现有综述主要关注网络架构而非实际应用价值，思考需要一种更全面的元综述方法。他们设计了涵盖点云处理关键任务（场景补全、配准、分割、建模）的综述框架，并关注这些任务在城市和环境应用中的支持作用。作者借鉴了现有的点云处理任务分类、深度学习方法（基于体素、多视图、基于点的方法）以及应用领域（城市建模、林业等），但将这些元素整合到一个关注实际应用的元综述框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过元综述深度学习在点云处理中的应用，从任务到实际应用，填补算法理论与实际应用之间的差距。整体流程包括：1)概述点云数据生成方式；2)详细讨论四个主要任务（场景补全、点云配准、分割、建模）和相关算法；3)回顾这些任务在城市建模、林业、农业等下游应用中的支持作用；4)讨论方法和应用，总结深度模型在点云处理中的展望。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)关注算法的实际应用价值而非仅关注网络架构；2)考虑实际应用中的额外挑战如大数据量、多样化场景内容等；3)提供从任务到应用的元综述；4)识别算法转化为应用时需要填补的差距。相比之前的工作，这篇论文不再局限于网络架构的枚举，而是建立了算法与实际应用之间的连接，并比较了不同学习模型在不同任务中的角色与传统方法的差异。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过元综述深度学习在点云处理中的应用，从任务到实际城市和环境应用，填补了算法理论与实际应用之间的差距，为点云处理技术的实际部署提供了指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud processing as a fundamental task in the field of geomatics andcomputer vision, has been supporting tasks and applications at different scalesfrom air to ground, including mapping, environmental monitoring, urban/treestructure modeling, automated driving, robotics, disaster responses etc. Due tothe rapid development of deep learning, point cloud processing algorithms havenowadays been almost explicitly dominated by learning-based approaches, most ofwhich are yet transitioned into real-world practices. Existing surveysprimarily focus on the ever-updating network architecture to accommodateunordered point clouds, largely ignoring their practical values in typicalpoint cloud processing applications, in which extra-large volume of data,diverse scene contents, varying point density, data modality need to beconsidered. In this paper, we provide a meta review on deep learning approachesand datasets that cover a selection of critical tasks of point cloud processingin use such as scene completion, registration, semantic segmentation, andmodeling. By reviewing a broad range of urban and environmental applicationsthese tasks can support, we identify gaps to be closed as these methodstransformed into applications and draw concluding remarks in both thealgorithmic and practical aspects of the surveyed methods.</description>
      <author>example@mail.com (Zhenxin Zhang, Zhihua Xu, Yuwei Cao, Ningli Xu, Shuye Wang, Shen'ao Cui, Zhen Li, Rongjun Qin)</author>
      <guid isPermaLink="false">2509.12452v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction</title>
      <link>http://arxiv.org/abs/2509.12430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了MechBench基准数据集和DYNAMO模型，用于解决从静态几何结构理解耦合机械运动的问题。MechBench包含693个合成齿轮组件，DYNAMO是一个依赖感知的神经模型，可直接从CAD点云预测零件运动轨迹，实验表明该方法优于基线，实现了准确且时间一致性的预测。&lt;h4&gt;背景&lt;/h4&gt;从静态几何结构理解铰接机械组件的运动是3D感知和设计自动化中的核心挑战。现有方法通常针对日常铰接物体（如门和笔记本电脑），假设简化的运动结构或依赖关节标注。但在机械组件（如齿轮）中，运动源于几何耦合（通过啮合齿或对齐轴），使得仅从几何结构推断关系运动变得困难。&lt;h4&gt;目的&lt;/h4&gt;解决从静态几何结构理解机械组件中耦合运动的问题，特别是那些通过几何耦合而非预定义关节产生运动的组件。为此，作者引入了MechBench数据集和DYNAMO模型，建立了一个系统性框架用于数据驱动的耦合机械运动学习。&lt;h4&gt;方法&lt;/h4&gt;作者提出了两个主要贡献：1) MechBench：一个包含693个多样化合成齿轮组件的基准数据集，具有零件级真实运动轨迹，提供研究耦合运动的结构化环境；2) DYNAMO：一个依赖感知的神经模型，可直接从分割的CAD点云预测每个零件的运动轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，DYNAMO模型在各种齿轮配置下实现了准确且时间一致性的运动轨迹预测，性能优于现有的强基线方法。这证明了从几何结构推断耦合机械运动的可行性。&lt;h4&gt;结论&lt;/h4&gt;MechBench和DYNAMO共同建立了一个新颖的系统性框架，用于CAD组件中耦合机械运动的数据驱动学习，解决了从静态几何理解机械运动的核心挑战。&lt;h4&gt;翻译&lt;/h4&gt;理解铰接机械组件从静态几何的运动是3D感知和设计自动化中的核心挑战。先前关于日常铰接物体（如门和笔记本电脑）的工作通常假设简化的运动结构或依赖关节标注。然而，在机械组件（如齿轮）中，运动源于几何耦合，通过啮合齿或对齐轴产生，这使得现有方法难以仅从几何结构推断关系运动。为解决这一差距，我们引入了MechBench，一个包含693个多样化合成齿轮组件的基准数据集，具有零件级真实运动轨迹。MechBench提供了一个研究耦合运动的结构化环境，其中零件动力学由接触和传动而非预定义关节引起。基于此，我们提出了DYNAMO，一个依赖感知的神经模型，可直接从分割的CAD点云预测每个零件的运动轨迹。实验表明，DYNAMO优于强基线方法，在各种齿轮配置下实现了准确且时间一致性的预测。MechBench和DYNAMO共同建立了一个新颖的系统性框架，用于CAD组件中耦合机械运动的数据驱动学习。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决铰接式机械装配体（特别是齿轮系统）从静态几何形状预测运动的问题。在现有方法中，运动预测通常假设简化的运动学结构或依赖关节标注，但在机械装配体中，运动源于零件间的几何耦合（如齿轮啮合或对齐轴），这使得从几何推理关系运动变得困难。这个问题在3D感知和设计自动化领域至关重要，因为现代CAD工具虽然提供详细3D几何，但很少包含可执行的运动规范，限制了它们在机器人应用（如装配自动化、运动规划和仿真）中的实用性。机器人要与、操作甚至设计这样的系统，理解其组成部分如何移动是必不可少的。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：大多数方法通过估计单个零件的机动性来建模运动学，然后应用固定方程组进行运动生成，这种方法在零件运动独立时有效，但在机械装配体中，零件运动相互依赖。作者选择基于齿轮的机制作为研究重点，因为它们的运动通过接触、啮合和传动传播。由于现有数据集很少包括耦合运动，作者创建了MechBench数据集。DYNAMO方法借鉴了PointNet++进行特征提取、图神经网络建模关系和Transformer进行时间解码等现有技术，但进行了改进以处理耦合运动，通过建模零件间的机械耦合关系来捕获运动传播。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'依赖感知'，即认识到机械装配体中零件的运动不是独立的，而是通过接触和耦合相互依赖的。DYNAMO直接从分割的CAD点云预测每个零件的SE(3)运动轨迹，而不依赖于关节标注或预定义的运动学结构。整体实现流程包括：1) 使用PointNet++提取每个零件的几何特征；2) 构建零件图并使用接触启发式方法估计耦合矩阵，然后应用耦合感知的图神经网络建模零件间关系；3) 将零件特征在时间帧上复制并与位置编码结合，使用Transformer编码器处理时间依赖关系，并通过MLP预测6D扭转向量；4) 使用多项损失函数（平移L2损失、旋转测地损失和时间一致性损失）训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) MechBench数据集，包含693个合成齿轮装配体，专门用于评估耦合机械运动预测；2) DYNAMO依赖感知神经模型，能联合推理零件间关系；3) 通过图神经网络建模零件间的机械耦合关系；4) 实现从静态CAD几何端到端学习耦合刚体运动。相比之前工作，DYNAMO明确处理零件间的耦合关系而非假设独立运动；专注于机械装配体而非日常铰接物体；使用6D Lie代数向量确保预测的运动有效且连续；不依赖预定义关节信息而是从几何中学习；提供了专门针对耦合机械运动预测的数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DYNAMO和MechBench共同建立了一个新颖的系统性框架，用于从CAD装配体的静态几何形状中学习耦合机械运动，通过依赖感知的神经网络准确预测相互连接零件的运动轨迹。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the motion of articulated mechanical assemblies from staticgeometry remains a core challenge in 3D perception and design automation. Priorwork on everyday articulated objects such as doors and laptops typicallyassumes simplified kinematic structures or relies on joint annotations.However, in mechanical assemblies like gears, motion arises from geometriccoupling, through meshing teeth or aligned axes, making it difficult forexisting methods to reason about relational motion from geometry alone. Toaddress this gap, we introduce MechBench, a benchmark dataset of 693 diversesynthetic gear assemblies with part-wise ground-truth motion trajectories.MechBench provides a structured setting to study coupled motion, where partdynamics are induced by contact and transmission rather than predefined joints.Building on this, we propose DYNAMO, a dependency-aware neural model thatpredicts per-part SE(3) motion trajectories directly from segmented CAD pointclouds. Experiments show that DYNAMO outperforms strong baselines, achievingaccurate and temporally consistent predictions across varied gearconfigurations. Together, MechBench and DYNAMO establish a novel systematicframework for data-driven learning of coupled mechanical motion in CADassemblies.</description>
      <author>example@mail.com (Mayank Patel, Rahul Jain, Asim Unmesh, Karthik Ramani)</author>
      <guid isPermaLink="false">2509.12430v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning</title>
      <link>http://arxiv.org/abs/2509.11594v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper needs major revision&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GBPP是一种基于快速学习的评分器，可以从单个RGB-D快照中选择机器人抓取的基础姿态。它使用两阶段课程学习方法：第一阶段使用简单的距离-可见性规则低成本自动标记大型数据集；第二阶段使用较小的高保真模拟试验集优化模型以匹配真实抓取结果。PointNet++风格的点云编码器与多层感知机一起对候选姿态进行评分，实现快速在线选择，无需完整的任务和运动优化。&lt;h4&gt;背景&lt;/h4&gt;机器人抓取任务中需要选择合适的基础姿态，传统方法可能需要复杂的计算或大量数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速学习评分器(GBPP)，能够从单个RGB-D图像中选择机器人抓取的基础姿态。&lt;h4&gt;方法&lt;/h4&gt;两阶段课程学习：第一阶段使用简单的距离-可见性规则低成本自动标记大型数据集；第二阶段使用高保真模拟试验集优化模型。使用PointNet++风格的点云编码器与多层感知机对候选姿态进行评分，实现快速在线选择，无需完整的任务和运动优化。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实移动机械臂上，GBPP优于仅依赖接近度和几何形状的基线方法；GBPP能选择更安全、更可达的姿态；在错误情况下表现良好(优雅降级)。&lt;h4&gt;结论&lt;/h4&gt;GBPP为数据高效、几何感知的基础放置提供了实用方法：先使用廉价启发式方法覆盖，然后通过有针对性的模拟进行校准。&lt;h4&gt;翻译&lt;/h4&gt;GBPP是一种基于快速学习的评分器，它从单个RGB-D快照中选择机器人抓取的基础姿态。该方法使用两阶段课程学习：(1)简单的距离-可见性规则低成本自动标记大型数据集；(2)较小的高保真模拟试验集优化模型以匹配真实抓取结果。带有MLP的PointNet++风格点云编码器对密集网格候选姿态进行评分，实现无需完整任务和运动优化的快速在线选择。在模拟和真实移动机械臂上，GBPP优于仅依赖接近度和几何的基线方法，选择更安全、更可达的姿态，并在错误情况下优雅降级。这些结果为数据高效、几何感知的基础放置提供了实用方法：使用廉价启发式方法覆盖，然后通过有针对性的模拟进行校准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GBPP is a fast learning based scorer that selects a robot base pose forgrasping from a single RGB-D snapshot. The method uses a two stage curriculum:(1) a simple distance-visibility rule auto-labels a large dataset at low cost;and (2) a smaller set of high fidelity simulation trials refines the model tomatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLPscores dense grids of candidate poses, enabling rapid online selection withoutfull task-and-motion optimization. In simulation and on a real mobilemanipulator, GBPP outperforms proximity and geometry only baselines, choosingsafer and more reachable stances and degrading gracefully when wrong. Theresults offer a practical recipe for data efficient, geometry aware baseplacement: use inexpensive heuristics for coverage, then calibrate withtargeted simulation.</description>
      <author>example@mail.com (Jizhuo Chen, Diwen Liu, Jiaming Wang, Harold Soh)</author>
      <guid isPermaLink="false">2509.11594v2</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>3D Aware Region Prompted Vision Language Model</title>
      <link>http://arxiv.org/abs/2509.13317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Website: https://www.anjiecheng.me/sr3d&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SR-3D是一种视觉语言模型，通过共享视觉令牌空间连接2D图像和3D数据，支持灵活的区域标注，无需繁琐的多帧标注。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉模型在处理2D和3D数据时存在局限性，特别是在跨帧空间推理方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够统一2D和3D表示空间的模型，提高场景理解的准确性。&lt;h4&gt;方法&lt;/h4&gt;通过将3D位置嵌入增强2D视觉特征，使3D模型能够利用2D先验知识进行跨帧空间推理。&lt;h4&gt;主要发现&lt;/h4&gt;SR-3D在2D视觉语言和3D空间基准测试中达到了最先进的性能，即使在野外视频中也表现出色。&lt;h4&gt;结论&lt;/h4&gt;SR-3D有效地统一了2D和3D表示空间，提高了场景理解的准确性和适用性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了空间区域3D(SR-3D)感知视觉语言模型，通过共享视觉令牌空间连接单视图2D图像和多视图3D数据。SR-3D支持灵活的区域提示，允许用户在任何帧上使用边界框、分割掩码标注区域，或直接在3D中标注，无需进行繁琐的多帧标注。我们通过将3D位置嵌入增强2D视觉特征来实现这一点，使3D模型能够利用强大的2D先验知识进行更准确的跨帧空间推理，即使感兴趣的对象不出现在同一视图中。在通用2D视觉语言和专业3D空间基准上的大量实验表明，SR-3D达到了最先进的性能，证明了其在统一2D和3D表示空间以实现场景理解方面的有效性。此外，我们观察到在没有3D输入或真实3D标注的野外视频中，SR-3D也能准确推断空间关系和度量测量，显示出其适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉语言模型在3D空间推理方面的局限性。现有2D模型缺乏理解复杂3D结构的能力，而3D模型又难以利用2D模型的强大先验知识。这个问题很重要，因为准确的空间推理对机器人导航、自动驾驶、增强现实等应用至关重要，能让AI系统更好地理解和物理世界互动。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考过程是：首先认识到2D视觉语言模型有强大先验但缺乏3D能力；然后发现多视图图像可作为3D表示与2D模型对齐；接着意识到区域提示在单视图有效但扩展到多视图有挑战；最后设计统一架构连接2D和3D表示。他们借鉴了DepthAnythingV2进行深度估计、动态瓦片机制处理高分辨率图像，以及RegionGPT等工作的区域提示概念，并整合了点云估计器处理视频输入。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建统一的3D感知视觉语言模型，通过共享视觉标记空间连接单视图2D图像和多视图3D数据，并在2D特征中加入3D位置嵌入，使模型能利用2D先验进行跨帧空间推理。实现流程包括：1)单视图表示：预训练基础模型，估计深度，计算3D位置，编码并融合位置嵌入；2)多视图表示：采样视频帧，对齐并规范化点图；3)动态瓦片区域提取：处理高分辨率图像和区域特征；4)训练：先单视图预训练，再多视图微调；5)推理：支持灵活的区域标注方式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一架构实现单视图和多视图任务的3D感知；2)动态瓦片区域提取器处理高分辨率图像；3)统一的嵌入空间使2D训练的区域能推广到多视图；4)灵活的区域标注支持；5)规范化的3D位置空间。相比之前工作，SR-3D不使用单独路径处理不同视图数据，而是直接在基础模型中整合位置嵌入，支持更灵活的区域标注，且在单视图训练后就能在多视图场景中表现出强大的零样本空间推理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SR-3D通过统一的3D感知视觉语言模型架构，实现了单视图2D图像和多视图3D数据的有效连接，支持灵活的区域提示，并在各种空间推理任务上达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Spatial Region 3D (SR-3D) aware vision-language model thatconnects single-view 2D images and multi-view 3D data through a shared visualtoken space. SR-3D supports flexible region prompting, allowing users toannotate regions with bounding boxes, segmentation masks on any frame, ordirectly in 3D, without the need for exhaustive multi-frame labeling. Weachieve this by enriching 2D visual features with 3D positional embeddings,which allows the 3D model to draw upon strong 2D priors for more accuratespatial reasoning across frames, even when objects of interest do not co-occurwithin the same view. Extensive experiments on both general 2D vision languageand specialized 3D spatial benchmarks demonstrate that SR-3D achievesstate-of-the-art performance, underscoring its effectiveness for unifying 2Dand 3D representation space on scene understanding. Moreover, we observeapplicability to in-the-wild videos without sensory 3D inputs or ground-truth3D annotations, where SR-3D accurately infers spatial relationships and metricmeasurements.</description>
      <author>example@mail.com (An-Chieh Cheng, Yang Fu, Yukang Chen, Zhijian Liu, Xiaolong Li, Subhashree Radhakrishnan, Song Han, Yao Lu, Jan Kautz, Pavlo Molchanov, Hongxu Yin, Xiaolong Wang, Sifei Liu)</author>
      <guid isPermaLink="false">2509.13317v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization</title>
      <link>http://arxiv.org/abs/2509.12893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MLLM-Engaged Joint Optimization (MEJO)的框架，用于解决外科手术三元组识别中的跨任务和任务内优化冲突问题。该框架通过Shared-Specific-Disentangled学习方案和协调梯度学习策略，有效处理了长尾数据分布和类别不平衡带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;外科手术三元组识别涉及识别手术器械、动词、目标及其组合，是一个复杂的外科场景理解挑战，面临长尾数据分布问题。&lt;h4&gt;目的&lt;/h4&gt;克服外科手术三元组识别中的两个关键挑战：跨任务优化冲突（由任务通用和任务特定表示纠缠引起）和任务内优化冲突（由类别不平衡的训练数据引起）。&lt;h4&gt;方法&lt;/h4&gt;提出MEJO框架，包括：1) 对于跨任务优化，引入Shared-Specific-Disentangled学习方案，构建多模态大语言模型驱动的概率提示池；2) 对于任务内优化，开发协调梯度学习策略，解析和重新平衡来自头部和尾部类别的正负梯度。&lt;h4&gt;主要发现&lt;/h4&gt;在CholecT45和CholecT50数据集上的广泛实验表明，所提出的MEJO框架在处理外科手术三元组识别的优化冲突方面具有优越性，能有效解决跨任务和任务内优化冲突问题。&lt;h4&gt;结论&lt;/h4&gt;MEJO框架通过结合多模态大语言模型和精心设计的优化策略，能够有效解决外科手术三元组识别中的优化冲突问题，提高了识别性能。&lt;h4&gt;翻译&lt;/h4&gt;外科手术三元组识别涉及识别器械、动词、目标及其组合，是一个复杂的外科场景理解挑战，受长尾数据分布困扰。受益于跨任务协作促进的主流多任务学习范式在识别三元组方面显示出有前景的性能，但两个关键挑战仍然存在：1) 由任务通用和任务特定表示纠缠导致的跨任务优化冲突；2) 由于类别不平衡的训练数据导致的任务内优化冲突。为了克服这些困难，我们提出了MLLM-Engaged Joint Optimization (MEJO)框架，它为外科手术三元组识别赋能了跨任务和任务内优化。对于跨任务优化，我们引入了Shared-Specific-Disentangled学习方案，将表示分解为任务共享和任务特定组件。为了增强任务共享表示，我们构建了一个多模态大语言模型驱动的概率提示池，以专家级语义线索动态增强视觉特征。此外，通过覆盖时空维度的不同任务提示，全面建模了任务特定线索，有效减轻了跨任务模糊性。为了解决任务内优化冲突，我们开发了协调梯度学习策略，它解析和重新平衡来自头部和尾部类别的正负梯度，以实现更协调的学习行为。在CholecT45和CholecT50数据集上的广泛实验证明了我们提出框架的优越性，验证了其在处理优化冲突方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical triplet recognition, which involves identifying instrument, verb,target, and their combinations, is a complex surgical scene understandingchallenge plagued by long-tailed data distribution. The mainstream multi-tasklearning paradigm benefiting from cross-task collaborative promotion has shownpromising performance in identifying triples, but two key challenges remain: 1)inter-task optimization conflicts caused by entangling task-generic andtask-specific representations; 2) intra-task optimization conflicts due toclass-imbalanced training data. To overcome these difficulties, we propose theMLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- andintra-task optimization for surgical triplet recognition. For inter-taskoptimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learningscheme that decomposes representations into task-shared and task-specificcomponents. To enhance task-shared representations, we construct a MultimodalLarge Language Model (MLLM) powered probabilistic prompt pool to dynamicallyaugment visual features with expert-level semantic cues. Additionally,comprehensive task-specific cues are modeled via distinct task prompts coveringthe temporal-spatial dimensions, effectively mitigating inter-task ambiguities.To tackle intra-task optimization conflicts, we develop a Coordinated GradientLearning (CGL) strategy, which dissects and rebalances the positive-negativegradients originating from head and tail classes for more coordinated learningbehaviors. Extensive experiments on the CholecT45 and CholecT50 datasetsdemonstrate the superiority of our proposed framework, validating itseffectiveness in handling optimization conflicts.</description>
      <author>example@mail.com (Yiyi Zhang, Yuchen Yuan, Ying Zheng, Jialun Pei, Jinpeng Li, Zheng Li, Pheng-Ann Heng)</author>
      <guid isPermaLink="false">2509.12893v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer</title>
      <link>http://arxiv.org/abs/2509.12718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ongoing Work, 29 pages, 3 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了两个动态空间基准测试，评估模型在动态环境下的空间理解和自适应规划能力，揭示了主流模型在动态空间推理和长期记忆方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有空间推理基准测试主要关注静态或全局可观察环境，无法捕捉部分可观察性和动态变化下的长期推理和内存利用挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍两个动态空间基准测试，系统评估模型在局部感知、环境反馈和全局目标紧密耦合情况下的空间理解和自适应规划能力。&lt;h4&gt;方法&lt;/h4&gt;提出局部可观察迷宫导航和match-2消除游戏两个基准测试，每个动作触发环境结构变化，需要持续更新认知和策略。同时提出基于主观体验的跨任务经验转移和验证的记忆机制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这些基准测试揭示了主流模型在动态空间推理和长期记忆方面的关键局限性。&lt;h4&gt;结论&lt;/h4&gt;提供了一个全面的平台，用于未来的方法论改进。&lt;h4&gt;翻译&lt;/h4&gt;大多数现有的空间推理基准测试关注静态或全局可观察环境，无法捕捉部分可观察性和动态变化下的长期推理和内存利用挑战。我们引入了两个动态空间基准测试——局部可观察迷宫导航和match-2消除游戏，系统评估模型在局部感知、环境反馈和全局目标紧密耦合情况下的空间理解和自适应规划能力。每个动作都会触发环境结构变化，需要持续更新认知和策略。我们进一步提出了基于主观体验的跨任务经验转移和验证的记忆机制。实验表明，我们的基准测试揭示了主流模型在动态空间推理和长期记忆方面的关键局限性，为未来方法论改进提供了全面平台。我们的代码和数据可在https://anonymous.4open.science/r/EvoEmpirBench-143C/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing spatial reasoning benchmarks focus on static or globallyobservable environments, failing to capture the challenges of long-horizonreasoning and memory utilization under partial observability and dynamicchanges. We introduce two dynamic spatial benchmarks, locally observable mazenavigation and match-2 elimination that systematically evaluate models'abilities in spatial understanding and adaptive planning when local perception,environment feedback, and global objectives are tightly coupled. Each actiontriggers structural changes in the environment, requiring continuous update ofcognition and strategy. We further propose a subjective experience-based memorymechanism for cross-task experience transfer and validation. Experiments showthat our benchmarks reveal key limitations of mainstream models in dynamicspatial reasoning and long-term memory, providing a comprehensive platform forfuture methodological advances. Our code and data are available athttps://anonymous.4open.science/r/EvoEmpirBench-143C/.</description>
      <author>example@mail.com (Pukun Zhao, Longxiang Wang, Miaowei Wang, Chen Chen, Fanqing Zhou, Haojian Huang)</author>
      <guid isPermaLink="false">2509.12718v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>RailSafeNet: Visual Scene Understanding for Tram Safety</title>
      <link>http://arxiv.org/abs/2509.12125v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, EPIA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为RailSafeNet的实时框架，利用数字图像处理、深度学习和人工智能技术提高有轨电车与人类交互的安全性，通过语义分割、目标检测和距离评估来识别轨道入侵风险。&lt;h4&gt;背景&lt;/h4&gt;有轨电车经常在人口密集地区运行，与人类（行人、驾驶员、骑自行车的人、宠物等）的交互安全是一个重要挑战，碰撞可能导致从轻微伤害到致命后果的各种事故。&lt;h4&gt;目的&lt;/h4&gt;设计一个利用数字图像处理、深度学习和人工智能的解决方案，提高行人、驾驶员、骑自行车的人、宠物和有轨电车乘客的安全。&lt;h4&gt;方法&lt;/h4&gt;提出RailSafeNet实时框架，融合语义分割、目标检测和基于规则的距离评估器，使用单目视频识别轨道，定位附近物体，并通过将投影距离与标准1435毫米轨距比较来分类风险。&lt;h4&gt;主要发现&lt;/h4&gt;在RailSem19数据集上的实验显示，经过类别过滤的SegFormer B3模型实现了65%的交并比，微调后的YOLOv8在交并比阈值为0.50的情况下达到了75.6%的平均精度均值。&lt;h4&gt;结论&lt;/h4&gt;RailSafeNet提供准确的、标注较少的场景理解，可以在危险情况升级之前警告驾驶员，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;有轨电车与人类交互安全是一个重要挑战，因为有轨电车经常在人口密集地区运行，碰撞可能导致轻微伤害甚至致命后果。本文从设计解决方案的角度解决这一问题，利用数字图像处理、深度学习和人工智能来提高行人、驾驶员、骑自行车的人、宠物和有轨电车乘客的安全。我们提出了RailSafeNet，一个实时框架，融合了语义分割、目标检测和基于规则的距离评估器，以突出显示轨道入侵。仅使用单目视频，系统识别轨道，定位附近物体，并通过将投影距离与标准1435毫米轨距进行比较来分类风险。在多样化的RailSem19数据集上的实验表明，经过类别过滤的SegFormer B3模型实现了65%的交并比，而微调后的YOLOv8在交并比阈值为0.50的情况下达到了75.6%的平均精度均值。因此，RailSafeNet提供了准确的、标注较少的场景理解，可以在危险情况升级之前警告驾驶员。代码可在https://github.com/oValach/RailSafeNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tram-human interaction safety is an important challenge, given that tramsfrequently operate in densely populated areas, where collisions can range fromminor injuries to fatal outcomes. This paper addresses the issue from theperspective of designing a solution leveraging digital image processing, deeplearning, and artificial intelligence to improve the safety of pedestrians,drivers, cyclists, pets, and tram passengers. We present RailSafeNet, areal-time framework that fuses semantic segmentation, object detection and arule-based Distance Assessor to highlight track intrusions. Using onlymonocular video, the system identifies rails, localises nearby objects andclassifies their risk by comparing projected distances with the standard 1435mmrail gauge. Experiments on the diverse RailSem19 dataset show that aclass-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculatedat an intersection over union (IoU) threshold of 0.50. RailSafeNet thereforedelivers accurate, annotation-light scene understanding that can warn driversbefore dangerous situations escalate. Code available athttps://github.com/oValach/RailSafeNet.</description>
      <author>example@mail.com (Ondřej Valach, Ivan Gruber)</author>
      <guid isPermaLink="false">2509.12125v2</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Geometric Priors for Unaligned Scene Change Detection</title>
      <link>http://arxiv.org/abs/2509.11292v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入几何先验解决非对齐场景变化检测的核心挑战，提出无需训练的框架，结合视觉基础模型表征，实现视角不对齐情况下的可靠变化检测。&lt;h4&gt;背景&lt;/h4&gt;非对齐场景变化检测旨在检测不同时间拍摄且无视角对齐的图像对间的场景变化。当前方法仅依赖2D视觉线索建立对应关系，但大视角变化会导致基于外观的匹配失败。小规模数据集的2D变化掩码监督限制了多视图知识学习，缺乏显式几何推理是关键但被忽视的局限性。&lt;h4&gt;目的&lt;/h4&gt;引入几何先验解决非对齐SCD的核心挑战，实现可靠的视觉重叠识别、稳健的对应关系建立和显式的遮挡检测。&lt;h4&gt;方法&lt;/h4&gt;提出一种无需训练的框架，将几何先验与视觉基础模型的强大表征相结合，使模型能够在视角不对齐的情况下实现可靠的变化检测。&lt;h4&gt;主要发现&lt;/h4&gt;在PSCD、ChangeSim和PASLCD数据集上的广泛评估表明，该方法实现了优越且稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入几何先验，解决了非对齐SCD中的核心挑战，提高了变化检测的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;非对齐场景变化检测旨在检测在不同时间拍摄且没有假设视角对齐的图像对之间的场景变化。为处理视角变化，当前方法仅依赖2D视觉线索建立跨图像对应关系以辅助变化检测。然而，大视角变化会改变视觉观测，导致基于外观的匹配漂移或失败。此外，仅限于小规模SCD数据集的2D变化掩码监督限制了可泛化的多视图知识学习，使得难以可靠识别视觉重叠和处理遮挡。这种缺乏显式几何推理代表了关键但被忽视的局限性。在这项工作中，我们首次引入几何先验来解决非对齐SCD的核心挑战，实现可靠的视觉重叠识别、稳健的对应关系建立和显式的遮挡检测。基于这些先验，我们提出了一种无需训练的框架，将其与视觉基础模型的强大表征相结合，使模型能够在视角不对齐的情况下实现可靠的变化检测。通过在PSCD、ChangeSim和PASLCD数据集上的广泛评估，我们证明了我们的方法实现了优越且稳健的性能。我们的代码将在https://github.com/ZilingLiu/GeoSCD上发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决不对齐场景变化检测问题，即在相机视角不同的情况下检测场景变化。这个问题在现实中非常重要，因为自动驾驶、无人机和移动机器人等应用中经常遇到不同时间拍摄的图像存在视角差异的情况。传统方法假设图像视角对齐，限制了它们在真实世界场景中的实用性。缺乏显式几何推理导致这些方法在大视角变化下表现不佳，难以可靠识别视觉重叠和处理遮挡问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到当前方法仅依赖2D视觉线索建立图像对应关系，在大视角变化下容易失败。他们发现2D监督限制了多视图知识的泛化学习，难以处理遮挡问题。受人类能轻松从不同视角建立关联和感知遮挡的启发，作者认为3D信息可以解决这些挑战。他们借鉴了几何基础模型(GFMs)可以从多视角图像中恢复3D几何的能力，并利用视觉基础模型SAM的强大表示能力，设计了一个两阶段框架：先利用GFM建立几何理解，再指导变化掩码预测。这种方法还借鉴了零样本思想来实现跨数据集泛化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入几何先验知识来解决不对齐场景变化检测的挑战，包括识别视觉重叠、建立稳健对应关系和检测遮挡。整体流程分为两个主要模块：1)几何先验生成模块：使用GFM重建深度图和相机参数，建立像素级对应关系，识别视觉重叠区域，并检测遮挡区域；2)几何引导变化掩码预测模块：将几何线索与SAM模型集成，在重叠区域内通过特征相似性生成初始变化提案，用遮挡掩码精炼提案，再与SAM的分割掩码匹配融合产生最终结果。此外，还包含光照变化处理步骤，使用Retinex或Color Transfer技术减少光照差异对GFM重建的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次引入几何先验知识来识别视觉重叠、建立稳健对应关系和检测遮挡；2)提出无需训练的框架，将几何先验与视觉基础模型集成，减少对标注数据的依赖；3)在各种视角变化和场景类型上实现领先且稳健的性能。相比之前工作，本文超越了仅依赖2D视觉线索的传统方法，解决了遮挡问题，无需大规模训练数据，具有更强的跨数据集泛化能力，并能处理更复杂的场景和更广泛的视角变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次将几何先验知识引入不对齐场景变化检测，通过结合几何基础模型与视觉基础模型，实现了无需训练、视角变化鲁棒且跨数据集泛化的场景变化检测方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unaligned Scene Change Detection aims to detect scene changes between imagepairs captured at different times without assuming viewpoint alignment. Tohandle viewpoint variations, current methods rely solely on 2D visual cues toestablish cross-image correspondence to assist change detection. However, largeviewpoint changes can alter visual observations, causing appearance-basedmatching to drift or fail. Additionally, supervision limited to 2D change masksfrom small-scale SCD datasets restricts the learning of generalizablemulti-view knowledge, making it difficult to reliably identify visual overlapsand handle occlusions. This lack of explicit geometric reasoning represents acritical yet overlooked limitation. In this work, we introduce geometric priorsfor the first time to address the core challenges of unaligned SCD, forreliable identification of visual overlaps, robust correspondenceestablishment, and explicit occlusion detection. Building on these priors, wepropose a training-free framework that integrates them with the powerfulrepresentations of a visual foundation model to enable reliable changedetection under viewpoint misalignment. Through extensive evaluation on thePSCD, ChangeSim, and PASLCD datasets, we demonstrate that our approach achievessuperior and robust performance. Our code will be released athttps://github.com/ZilingLiu/GeoSCD.</description>
      <author>example@mail.com (Ziling Liu, Ziwei Chen, Mingqi Gao, Jinyu Yang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.11292v2</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.13229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CMTSSL的课程多任务自监督学习框架，专为高光谱成像分析设计的轻量级架构，有效结合了掩模图像建模与空间光谱拼图求解。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像能捕获每个像素的数百个连续波段的光谱特征，对土地覆盖分类、变化检测和环境监测等遥感应用至关重要。但由于数据高维度和卫星系统传输速率慢，需要高效模型支持星上处理。&lt;h4&gt;目的&lt;/h4&gt;开发轻量级模型支持星上处理，最小化冗余或低价值数据(如云覆盖区域)的传输，同时保持HSI分析的准确性。&lt;h4&gt;方法&lt;/h4&gt;CMTSSL框架结合掩模图像建模与解耦的空间和光谱拼图求解，采用课程学习策略逐步增加数据复杂度，使编码器能同时捕获光谱连续性、空间结构和全局语义特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上验证，CMTSSL在下游分割任务中表现一致提升，使用的架构比某些最先进模型轻16000多倍。&lt;h4&gt;结论&lt;/h4&gt;CMTSSL在轻量级架构上的可推广表示学习具有潜力，适用于真实世界的HSI应用，代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)捕获每个像素的数百个连续波段的详细光谱特征，对于土地覆盖分类、变化检测和环境监测等遥感应用不可或缺。由于HSI数据的高维性和卫星系统数据传输速率慢，需要紧凑高效的模型支持星上处理并最小化冗余或低价值数据(如云覆盖区域)的传输。为此，我们引入了一种专为HSI分析轻量架构设计的课程多任务自监督学习(CMTSSL)框架。CMTSSL将掩模图像建模与解耦的空间和光谱拼图求解相结合，通过课程学习策略在自监督过程中逐步增加数据复杂度，使编码器能够同时捕获细粒度光谱连续性、空间结构和全局语义特征。与先前的双任务SSL方法不同，CMTSSL在统一且计算高效的设计中同时解决空间和光谱推理，特别适合用于轻量级模型的星上卫星部署。我们在四个公共基准数据集上验证了我们的方法，在使用比某些最先进模型轻16000多倍的架构时，在下游分割任务中表现出一致的改进。这些结果突显了CMTSSL在轻量级架构上可推广表示学习的潜力，适用于真实世界的HSI应用。我们的代码已在https://github.com/hugocarlesso/CMTSSL公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) captures detailed spectral signatures acrosshundreds of contiguous bands per pixel, being indispensable for remote sensingapplications such as land-cover classification, change detection, andenvironmental monitoring. Due to the high dimensionality of HSI data and theslow rate of data transfer in satellite-based systems, compact and efficientmodels are required to support onboard processing and minimize the transmissionof redundant or low-value data, e.g. cloud-covered areas. To this end, weintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)framework designed for lightweight architectures for HSI analysis. CMTSSLintegrates masked image modeling with decoupled spatial and spectral jigsawpuzzle solving, guided by a curriculum learning strategy that progressivelyincreases data complexity during self-supervision. This enables the encoder tojointly capture fine-grained spectral continuity, spatial structure, and globalsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneouslyaddresses spatial and spectral reasoning within a unified and computationallyefficient design, being particularly suitable for training lightweight modelsfor onboard satellite deployment. We validate our approach on four publicbenchmark datasets, demonstrating consistent gains in downstream segmentationtasks, using architectures that are over 16,000x lighter than somestate-of-the-art models. These results highlight the potential of CMTSSL ingeneralizable representation learning with lightweight architectures forreal-world HSI applications. Our code is publicly available athttps://github.com/hugocarlesso/CMTSSL.</description>
      <author>example@mail.com (Hugo Carlesso, Josiane Mothe, Radu Tudor Ionescu)</author>
      <guid isPermaLink="false">2509.13229v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Representation Learning for Robust Sim-to-Real Transfer of Adaptive Humanoid Locomotion</title>
      <link>http://arxiv.org/abs/2509.12858v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新范式，通过对比学习框架使纯本体感觉策略获得感知的前瞻性能力，解决了人形机器人运动中反应性控制和感知驱动系统之间的权衡问题，实现了在不增加部署成本的情况下主动适应复杂地形的能力。&lt;h4&gt;背景&lt;/h4&gt;强化学习在人形机器人运动方面取得了显著进展，但在实际部署中存在一个基本困境：策略必须在反应性本体感觉控制的鲁棒性和复杂的、脆弱的感知驱动系统的主动性之间做出选择。&lt;h4&gt;目的&lt;/h4&gt;解决这一困境，引入一种范式，使纯粹的本体感觉策略具有主动性，获得感知的前瞻性能力，同时避免其部署时的成本。&lt;h4&gt;方法&lt;/h4&gt;核心贡献是一个对比学习框架，它强制执行者的潜在状态从仿真中编码特权的环境信息。这种'蒸馏意识'使自适应节拍时钟能够根据对地形的推断理解主动调整节奏。&lt;h4&gt;主要发现&lt;/h4&gt;这种协同作用解决了刚性时钟步态和不稳定的无时钟策略之间的经典权衡。通过零样本仿真到真实世界的转移，验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法使全尺寸人形机器人在具有挑战性的地形上（包括30厘米高的台阶和26.5度的斜坡）实现了高度鲁棒的运动。&lt;h4&gt;翻译&lt;/h4&gt;强化学习在人形机器人运动方面已经取得了显著进展，但实际部署中仍然存在一个基本困境：策略必须在反应性本体感觉控制的鲁棒性和复杂的、脆弱的感知驱动系统的主动性之间做出选择。本文通过引入一种范式解决了这一困境，该范式赋予纯本体感觉策略以主动能力，实现了感知的前瞻性，同时避免了其部署时的成本。我们的核心贡献是一个对比学习框架，它强制执行者的潜在状态从仿真中编码特权的环境信息。重要的是，这种'蒸馏意识'使自适应节拍时钟能够基于对地形的推断理解主动调整节奏。这种协同作用解决了刚性时钟步态和不稳定的无时钟策略之间的经典权衡。我们通过零样本仿真到真实世界的转移验证了我们的方法，证明全尺寸人形机器人在具有挑战性的地形上（包括30厘米高的台阶和26.5度的斜坡）实现了高度鲁棒的运动。网站：https://lu-yidan.github.io/cra-loco。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning has produced remarkable advances in humanoidlocomotion, yet a fundamental dilemma persists for real-world deployment:policies must choose between the robustness of reactive proprioceptive controlor the proactivity of complex, fragile perception-driven systems. This paperresolves this dilemma by introducing a paradigm that imbues a purelyproprioceptive policy with proactive capabilities, achieving the foresight ofperception without its deployment-time costs. Our core contribution is acontrastive learning framework that compels the actor's latent state to encodeprivileged environmental information from simulation. Crucially, this``distilled awareness" empowers an adaptive gait clock, allowing the policy toproactively adjust its rhythm based on an inferred understanding of theterrain. This synergy resolves the classic trade-off between rigid, clockedgaits and unstable clock-free policies. We validate our approach with zero-shotsim-to-real transfer to a full-sized humanoid, demonstrating highly robustlocomotion over challenging terrains, including 30 cm high steps and 26.5{\deg}slopes, proving the effectiveness of our method. Website:https://lu-yidan.github.io/cra-loco.</description>
      <author>example@mail.com (Yidan Lu, Rurui Yang, Qiran Kou, Mengting Chen, Tao Fan, Peter Cui, Yinzhao Dong, Peng Lu)</author>
      <guid isPermaLink="false">2509.12858v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Modeling the Multivariate Relationship with Contextualized Representations for Effective Human-Object Interaction Detection</title>
      <link>http://arxiv.org/abs/2509.12784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种上下文化表征学习网络，通过结合功能引导推理和上下文提示，改进了人机交互(HOI)检测方法，能够更好地捕捉复杂交互关系。&lt;h4&gt;背景&lt;/h4&gt;人机交互(HOI)检测旨在同时定位人-物体对并识别它们的交互。最近的两阶段方法虽然取得了显著进展，但由于上下文建模不完整，仍然面临挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍一种上下文表征学习网络，结合功能引导推理和上下文提示，以更好地捕捉复杂交互。&lt;h4&gt;方法&lt;/h4&gt;通过三元结构&lt;人，工具，物体&gt;明确建模辅助对象的功能角色；将可学习提示与实例类别丰富化，并使用注意力机制与上下文视觉特征集成；在全局和区域级别将语言与图像内容对齐。&lt;h4&gt;主要发现&lt;/h4&gt;提出的上下文化表征为模型提供了丰富的关系线索，使其能够对复杂、上下文依赖的交互进行更可靠的推理。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在HICO-Det和V-COCO数据集的大多数场景中表现出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;人机交互(HOI)检测旨在同时定位人-物体对并识别它们的交互。虽然最近的两阶段方法已经取得了显著进展，但由于上下文建模不完整，它们仍然面临挑战。在这项工作中，我们引入了一种上下文化表征学习网络，结合功能引导推理和上下文提示以及视觉线索，以更好地捕捉复杂交互。我们通过将常规HOI检测框架扩展到简单的物体对之外，包括涉及工具等辅助实体的多元关系，来增强它。具体来说，我们通过三元结构&lt;人，工具，物体&gt;明确建模这些辅助对象的功能角色(功能)。这使得我们的模型能够识别依赖于工具的交互，如'填充'。此外，可学习提示被实例类别丰富化，随后使用注意力机制与上下文视觉特征集成。这个过程在全局和区域级别上将语言与图像内容对齐。这些上下文化表征为模型提供了丰富的关系线索，以便对复杂、上下文依赖的交互进行更可靠的推理。我们提出的方法在HICO-Det和V-COCO数据集的大多数场景中表现出优越的性能。代码将在接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human-Object Interaction (HOI) detection aims to simultaneously localizehuman-object pairs and recognize their interactions. While recent two-stageapproaches have made significant progress, they still face challenges due toincomplete context modeling. In this work, we introduce a ContextualizedRepresentation Learning Network that integrates both affordance-guidedreasoning and contextual prompts with visual cues to better capture complexinteractions. We enhance the conventional HOI detection framework by expandingit beyond simple human-object pairs to include multivariate relationshipsinvolving auxiliary entities like tools. Specifically, we explicitly model thefunctional role (affordance) of these auxiliary objects through tripletstructures &lt;human, tool, object&gt;. This enables our model to identifytool-dependent interactions such as 'filling'. Furthermore, the learnableprompt is enriched with instance categories and subsequently integrated withcontextual visual features using an attention mechanism. This process alignslanguage with image content at both global and regional levels. Thesecontextualized representations equip the model with enriched relational cuesfor more reliable reasoning over complex, context-dependent interactions. Ourproposed method demonstrates superior performance on both the HICO-Det andV-COCO datasets in most scenarios. Codes will be released upon acceptance.</description>
      <author>example@mail.com (Zhehao Li, Yucheng Qian, Chong Wang, Yinghao Lu, Zhihao Yang, Jiafei Wu)</author>
      <guid isPermaLink="false">2509.12784v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>NORA: A Nephrology-Oriented Representation Learning Approach Towards Chronic Kidney Disease Classification</title>
      <link>http://arxiv.org/abs/2509.12704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures, accepted to the International Conference on  Machine Learning and Applications (ICMLA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为NORA的肾脏病导向表示学习方法，利用常规收集的非肾脏临床变量进行慢性肾脏病分类，在缺乏专门肾脏生物标志物的环境中表现出良好的性能。&lt;h4&gt;背景&lt;/h4&gt;慢性肾脏病影响全球数百万人，但其早期检测仍具有挑战性，特别是在门诊环境中，基于实验室的肾脏生物标志物通常不可用。&lt;h4&gt;目的&lt;/h4&gt;研究常规收集的非肾脏临床变量（包括社会人口学因素、合并症和尿检结果）在慢性肾脏病分类中的预测潜力。&lt;h4&gt;方法&lt;/h4&gt;引入Nephrology-Oriented Representation leArning (NORA)方法，结合监督对比学习和非线性随机森林分类器，从表格型电子健康记录数据中推导区分性患者表示，用于下游CKD分类。在Riverside肾脏病学医师的基于诊所的EHR数据集上评估该方法，并在UCI CKD数据集上测试其可推广性。&lt;h4&gt;主要发现&lt;/h4&gt;NORA提高了类间可分性和整体分类性能，特别增强了早期阶段CKD的F1分数；在不同患者队列中证明了NORA进行CKD风险分层的有效性。&lt;h4&gt;结论&lt;/h4&gt;NORA方法能够有效利用常规临床数据进行CKD检测和风险分层，特别是在缺乏专门肾脏生物标志物的环境中具有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;慢性肾脏病(CKD)影响全球数百万人，但其早期检测仍然具有挑战性，特别是在门诊环境中，基于实验室的肾脏生物标志物通常不可用。在本研究中，我们调查了常规收集的非肾脏临床变量（包括社会人口学因素、合并症和尿检结果）在CKD分类中的预测潜力。我们引入了肾脏病导向表示学习(NORA)方法，结合监督对比学习和非线性随机森林分类器。NORA首先从表格型电子健康记录数据中推导出区分性患者表示，然后用于下游CKD分类。我们在Riverside肾脏病学医师的基于诊所的EHR数据集上评估了NORA。结果表明，NORA提高了类间可分性和整体分类性能，特别增强了早期阶段CKD的F1分数。此外，我们在UCI CKD数据集上评估了NORA的可推广性，证明了其在不同患者队列中进行CKD风险分层的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chronic Kidney Disease (CKD) affects millions of people worldwide, yet itsearly detection remains challenging, especially in outpatient settings wherelaboratory-based renal biomarkers are often unavailable. In this work, weinvestigate the predictive potential of routinely collected non-renal clinicalvariables for CKD classification, including sociodemographic factors, comorbidconditions, and urinalysis findings. We introduce the Nephrology-OrientedRepresentation leArning (NORA) approach, which combines supervised contrastivelearning with a nonlinear Random Forest classifier. NORA first derivesdiscriminative patient representations from tabular EHR data, which are thenused for downstream CKD classification. We evaluated NORA on a clinic-based EHRdataset from Riverside Nephrology Physicians. Our results demonstrated thatNORA improves class separability and overall classification performance,particularly enhancing the F1-score for early-stage CKD. Additionally, weassessed the generalizability of NORA on the UCI CKD dataset, demonstrating itseffectiveness for CKD risk stratification across distinct patient cohorts.</description>
      <author>example@mail.com (Mohammad Abdul Hafeez Khan, Twisha Bhattacharyya, Omar Khan, Noorah Khan, Alina Aziz Fatima Khan, Mohammed Qutub Khan, Sujoy Ghosh Hajra)</author>
      <guid isPermaLink="false">2509.12704v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields</title>
      <link>http://arxiv.org/abs/2509.12358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为MEAGraph的无监督学习方法，用于分析原子数据集，有效去除采样偏差，优化数据集。&lt;h4&gt;背景&lt;/h4&gt;在计算化学和材料科学中，构建化学多样性的数据集并避免采样偏差对于训练高效且可推广的力场至关重要。然而，许多常见的数据集生成技术容易过度采样势能表面的某些区域，这些区域难以识别和区分，或者与人类直觉不符，使得系统性地去除数据集中的偏差具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效识别和去除数据集中采样偏差的方法，同时避免信息损失，并能正确识别势能表面的不同区域。&lt;h4&gt;方法&lt;/h4&gt;作者提出了MEAGraph模型，这是一种无监督的原子数据分析方法。该方法结合了多种线性核变换和基于注意力的消息传递，能够捕获几何敏感性，并在不依赖标签或大量训练的情况下实现有效的数据集剪枝。&lt;h4&gt;主要发现&lt;/h4&gt;在铌、钽和铁数据集上的应用表明，MEAGraph能够高效地分组相似的原子环境，使得可以使用基本的剪枝技术来去除采样偏差。&lt;h4&gt;结论&lt;/h4&gt;这种方法为表示学习和聚类提供了有效的方法，可用于数据分析、异常检测和数据集优化。&lt;h4&gt;翻译&lt;/h4&gt;在构建化学多样性数据集的同时避免采样偏差对于训练高效且可推广的力场至关重要。然而，在计算化学和材料科学中，许多常见的数据集生成技术容易过度采样势能表面的某些区域。此外，这些区域可能难以相互识别和分离，或者与人类直觉不太吻合，这使得系统性地去除数据集中的偏差具有挑战性。虽然传统的聚类和修剪方法对此有用，但由于原子描述符的高维度相关困难，它们往往会导致信息损失或无法正确识别势能表面的不同区域。在本工作中，我们引入了多核边缘注意力的图自编码器模型，这是一种用于分析原子数据集的无监督方法。MEAGraph将多种线性核变换与基于注意力的消息传递相结合，以捕获几何敏感性并实现有效的数据集修剪，而不依赖于标签或大量训练。在铌、钽和铁数据集上的应用表明，MEAGraph能够高效地分组相似的原子环境，从而可以使用基本的修剪技术来去除采样偏差。这种方法为表示学习和聚类提供了有效的方法，可用于数据分析、异常检测和数据集优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Constructing a chemically diverse dataset while avoiding sampling bias iscritical to training efficient and generalizable force fields. However, incomputational chemistry and materials science, many common dataset generationtechniques are prone to oversampling regions of the potential energy surface.Furthermore, these regions can be difficult to identify and isolate from eachother or may not align well with human intuition, making it challenging tosystematically remove bias in the dataset. While traditional clustering andpruning (down-sampling) approaches can be useful for this, they can often leadto information loss or a failure to properly identify distinct regions of thepotential energy surface due to difficulties associated with the highdimensionality of atomic descriptors. In this work, we introduce theMulti-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, anunsupervised approach for analyzing atomic datasets. MEAGraph combines multiplelinear kernel transformations with attention-based message passing to capturegeometric sensitivity and enable effective dataset pruning without relying onlabels or extensive training. Demonstrated applications on niobium, tantalum,and iron datasets show that MEAGraph efficiently groups similar atomicenvironments, allowing for the use of basic pruning techniques for removingsampling bias. This approach provides an effective method for representationlearning and clustering that can be used for data analysis, outlier detection,and dataset optimization.</description>
      <author>example@mail.com (Hong Sun, Joshua A. Vita, Amit Samanta, Vincenzo Lordi)</author>
      <guid isPermaLink="false">2509.12358v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery</title>
      <link>http://arxiv.org/abs/2509.11436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The Fourth Workshop on Applications of Medical Artificial  Intelligence, AMAI 2025, Held in Conjunction with MICCAI 2025, Daejeon,  Republic of Korea, September 23, 2025, Proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种通过机器学习主动学习域偏移的方法，通过数据潜在空间的后旋转实现生物和技术因素的解耦，提高了医学影像数据中疾病相关模式的识别能力。&lt;h4&gt;背景&lt;/h4&gt;在医学影像数据中借助机器学习识别新的疾病相关模式可以扩展可识别发现的词汇，支持诊断和预后评估。然而，图像外观的变化不仅源于生物学差异，还与供应商相关的成像技术、扫描或重建参数有关。这些导致的域偏移阻碍了数据表示学习策略和有意义的生物聚类外观的发现。&lt;h4&gt;目的&lt;/h4&gt;解决医学影像数据中的域偏移问题，使机器学习能够更好地识别疾病相关模式，提高诊断和预后评估的准确性。&lt;h4&gt;方法&lt;/h4&gt;引入一种通过数据潜在空间的后旋转来主动学习域偏移的方法，实现生物和技术因素的解耦，从而在不同采集设置下实现代表组织类型的稳定聚类。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界异构临床数据上，学习的解耦表示导致跨不同采集设置的稳定聚类；与纠缠表示相比，聚类一致性提高了+19.01%（ARI）、+16.85%（NMI）和+12.39%（Dice），优于四种最先进的调和方法；当使用聚类对特发性肺纤维化患者进行组织成分量化时，学习的特征增强了Cox生存预测。&lt;h4&gt;结论&lt;/h4&gt;所提出的无标签框架通过解耦生物和技术因素，能够有效处理医学影像数据中的域偏移问题，提高聚类一致性，并促进多中心常规成像数据中的生物标志物发现。&lt;h4&gt;翻译&lt;/h4&gt;借助机器学习在医学影像数据中识别新的疾病相关模式扩展了可识别发现的词汇，支持诊断和预后评估。然而，图像外观的变化不仅源于生物学差异，还与供应商相关的成像技术、扫描或重建参数有关。由此产生的域偏移阻碍了数据表示学习策略和有意义的生物聚类外观的发现。为应对这些挑战，我们引入了一种通过数据潜在空间的后旋转来主动学习域偏移的方法，实现生物和技术因素的解耦。真实世界异构临床数据的结果表明，学习的解耦表示导致跨不同采集设置的稳定聚类。与纠缠表示相比，聚类一致性提高了+19.01%（ARI）、+16.85%（NMI）和+12.39%（Dice），优于四种最先进的调和方法。当使用聚类对特发性肺纤维化患者进行组织成分量化时，学习的特征增强了Cox生存预测。这表明所提出的无标签框架促进了多中心常规成像数据中的生物标志物发现。代码可在GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying new disease-related patterns in medical imaging data with thehelp of machine learning enlarges the vocabulary of recognizable findings. Thissupports diagnostic and prognostic assessment. However, image appearance variesnot only due to biological differences, but also due to imaging technologylinked to vendors, scanning- or re- construction parameters. The resultingdomain shifts impedes data representation learning strategies and the discoveryof biologically meaningful cluster appearances. To address these challenges, weintroduce an approach to actively learn the domain shift via post-hoc rotationof the data latent space, enabling disentanglement of biological and technicalfactors. Results on real-world heterogeneous clinical data showcase that thelearned disentangled representation leads to stable clusters representingtissue-types across different acquisition settings. Cluster consistency isimproved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to theentangled representation, outperforming four state-of-the-art harmonizationmethods. When using the clusters to quantify tissue composition on idiopathicpulmonary fibrosis patients, the learned profiles enhance Cox survivalprediction. This indicates that the proposed label-free framework facilitatesbiomarker discovery in multi-center routine imaging data. Code is available onGitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.</description>
      <author>example@mail.com (Jeanny Pan, Philipp Seeböck, Christoph Fürböck, Svitlana Pochepnia, Jennifer Straub, Lucian Beer, Helmut Prosch, Georg Langs)</author>
      <guid isPermaLink="false">2509.11436v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE</title>
      <link>http://arxiv.org/abs/2509.12255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了GraphSAGE图神经网络框架在银行交易网络中的实际应用，证明了其在处理动态银行数据方面的优势，并展示了其在洗钱检测等实际应用中的价值。&lt;h4&gt;背景&lt;/h4&gt;金融机构日益需要可扩展的工具来分析复杂的交易网络，但传统的图嵌入方法难以处理动态、真实的银行数据。&lt;h4&gt;目的&lt;/h4&gt;展示GraphSAGE（一种归纳图神经网络框架）在银行非二分异构交易网络中的实际应用，并证明其在处理随时间演变的交易数据时的优势。&lt;h4&gt;方法&lt;/h4&gt;构建使用匿名客户和商家交易数据的交易网络，并训练GraphSAGE模型生成节点嵌入，然后将这些嵌入应用于下游分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;对嵌入的探索性工作揭示了与地理和人口统计属性一致的、可解释的聚类；在洗钱检测模型中使用这些嵌入可以改进高风险账户的优先级排序。&lt;h4&gt;结论&lt;/h4&gt;这项研究为金融机构利用图机器学习在交易生态系统中获取可操作的见解提供了蓝图，强调了该框架的归纳能力、可扩展性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;金融机构日益需要可扩展的工具来分析复杂的交易网络，然而传统的图嵌入方法难以处理动态、真实的银行数据。本文展示了GraphSAGE（一种归纳图神经网络框架）在银行非二分异构交易网络中的实际应用。与归纳方法不同，GraphSAGE能够很好地扩展到大型网络，并且可以推广到未见过的节点，这对于处理随时间演变的交易数据的机构至关重要。我们使用匿名客户和商家交易数据构建了一个交易网络，并训练了一个GraphSAGE模型来生成节点嵌入。我们对嵌入的探索性工作揭示了与地理和人口统计属性一致的、可解释的聚类。此外，我们通过将它们应用于洗钱检测模型，说明了它们在下游分类任务中的实用性，使用这些嵌入可以改进高风险账户的优先级排序。除了欺诈检测外，我们的工作强调了该框架对银行规模网络的适应性，强调了其归纳能力、可扩展性和可解释性。本研究为金融机构利用图机器学习在交易生态系统中获取可操作的见解提供了蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-94139-9_17&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial institutions increasingly require scalable tools to analyse complextransactional networks, yet traditional graph embedding methods struggle withdynamic, real-world banking data. This paper demonstrates the practicalapplication of GraphSAGE, an inductive Graph Neural Network framework, tonon-bipartite heterogeneous transaction networks within a banking context.Unlike transductive approaches, GraphSAGE scales well to large networks and cangeneralise to unseen nodes which is critical for institutions working withtemporally evolving transactional data. We construct a transaction networkusing anonymised customer and merchant transactions and train a GraphSAGE modelto generate node embeddings. Our exploratory work on the embeddings revealsinterpretable clusters aligned with geographic and demographic attributes.Additionally, we illustrate their utility in downstream classification tasks byapplying them to a money mule detection model where using these embeddingsimproves the prioritisation of high-risk accounts. Beyond fraud detection, ourwork highlights the adaptability of this framework to banking-scale networks,emphasising its inductive capability, scalability, and interpretability. Thisstudy provides a blueprint for financial organisations to harness graph machinelearning for actionable insights in transactional ecosystems.</description>
      <author>example@mail.com (Mihir Tare, Clemens Rattasits, Yiming Wu, Euan Wielewski)</author>
      <guid isPermaLink="false">2509.12255v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Why and How Auxiliary Tasks Improve JEPA Representations</title>
      <link>http://arxiv.org/abs/2509.12249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了JEPA架构的理论特性，证明了一个'无有害表征崩溃'定理，并通过实验表明联合训练JEPA模型与辅助头能生成更丰富的表征。&lt;h4&gt;背景&lt;/h4&gt;JEPA（Joint-Embedding Predictive Architecture）越来越多地用于视觉表征学习和基于模型的强化学习组件中，但其行为仍然不被充分理解。&lt;h4&gt;目的&lt;/h4&gt;提供一个简单实用的JEPA变体的理论表征，该变体有一个与潜在动力学联合训练的辅助回归头。&lt;h4&gt;方法&lt;/h4&gt;证明了一个'无有害表征崩溃'定理：在确定性MDPs中，如果训练使潜在转换一致性损失和辅助回归损失都趋近于零，那么任何非等价观测（即那些不具有相同转换动力学或辅助标签的观测）必须映射到不同的潜在表征。&lt;h4&gt;主要发现&lt;/h4&gt;辅助任务锚定了表征必须保留的区分。在计数环境中的受控消融实验证实了这一理论，并表明与辅助头联合训练JEPA模型比分别训练它们能生成更丰富的表征。&lt;h4&gt;结论&lt;/h4&gt;我们的工作指出了改进JEPA编码器的一条路径：使用一个辅助函数与它们一起训练，该辅助函数与转换动力学一起编码正确的等价关系。&lt;h4&gt;翻译&lt;/h4&gt;联合嵌入预测架构（JEPA）越来越多地用于视觉表征学习和作为基于模型的强化学习组件，但其行为仍然不被充分理解。我们提供了一个简单实用的JEPA变体的理论表征，该变体有一个与潜在动力学联合训练的辅助回归头。我们证明了一个'无有害表征崩溃'定理：在确定性MDPs中，如果训练使潜在转换一致性损失和辅助回归损失都趋近于零，那么任何非等价观测（即那些不具有相同转换动力学或辅助标签的观测）必须映射到不同的潜在表征。因此，辅助任务锚定了表征必须保留的区分。在计数环境中的受控消融实验证实了这一理论，并表明与辅助头联合训练JEPA模型比分别训练它们能生成更丰富的表征。我们的工作指出了改进JEPA编码器的一条路径：使用一个辅助函数与它们一起训练，该辅助函数与转换动力学一起编码正确的等价关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint-Embedding Predictive Architecture (JEPA) is increasingly used forvisual representation learning and as a component in model-based RL, but itsbehavior remains poorly understood. We provide a theoretical characterizationof a simple, practical JEPA variant that has an auxiliary regression headtrained jointly with latent dynamics. We prove a No Unhealthy RepresentationCollapse theorem: in deterministic MDPs, if training drives both thelatent-transition consistency loss and the auxiliary regression loss to zero,then any pair of non-equivalent observations, i.e., those that do not have thesame transition dynamics or auxiliary label, must map to distinct latentrepresentations. Thus, the auxiliary task anchors which distinctions therepresentation must preserve. Controlled ablations in a counting environmentcorroborate the theory and show that training the JEPA model jointly with theauxiliary head generates a richer representation than training them separately.Our work indicates a path to improve JEPA encoders: training them with anauxiliary function that, together with the transition dynamics, encodes theright equivalence relations.</description>
      <author>example@mail.com (Jiacan Yu, Siyi Chen, Mingrui Liu, Nono Horiuchi, Vladimir Braverman, Zicheng Xu, Dan Haramati, Randall Balestriero)</author>
      <guid isPermaLink="false">2509.12249v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Agents via Continual Pre-training</title>
      <link>http://arxiv.org/abs/2509.13310v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为智能体持续预训练(Agentic CPT)的新方法，用于构建强大的智能体基础模型，解决了后训练过程中同时学习多样智能体行为与对齐专家演示之间的优化冲突问题。基于此方法开发的AgentFounder-30B模型在10个基准测试上取得了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大语言模型已发展为能够自主使用工具和多步推理的智能体系统，用于解决复杂问题。然而，基于通用基础模型的后训练方法在智能体任务中表现不佳，特别是在开源实现中。&lt;h4&gt;目的&lt;/h4&gt;构建强大的智能体基础模型，解决后训练过程中同时学习多样智能体行为与对齐专家演示之间的优化冲突问题。&lt;h4&gt;方法&lt;/h4&gt;首次将智能体持续预训练(Agentic CPT)纳入深度研究智能体训练流程，并基于此方法开发了名为AgentFounder的深度研究智能体模型。&lt;h4&gt;主要发现&lt;/h4&gt;AgentFounder-30B在10个基准测试上取得了最先进性能，同时保持强大的工具使用能力，具体表现为：在BrowseComp-en上达到39.9%，在BrowseComp-zh上达到43.3%，在HLE上达到31.5%的Pass@1。&lt;h4&gt;结论&lt;/h4&gt;通过引入Agentic CPT方法，成功构建了强大的智能体基础模型，解决了后训练过程中的优化冲突问题，显著提升了智能体任务性能。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型已经发展为能够自主使用工具和多步推理的智能体系统，用于解决复杂问题。然而，建立在通用基础模型之上的后训练方法在智能体任务中持续表现不佳，特别是在开源实现中。我们确定了根本原因：缺乏强大的智能体基础模型迫使模型在后训练过程中同时学习多样的智能体行为，同时将这些行为与专家演示对齐，从而产生根本的优化冲突。为此，我们首次提出将智能体持续预训练(Agentic CPT)纳入深度研究智能体训练流程，以构建强大的智能体基础模型。基于这种方法，我们开发了一个名为AgentFounder的深度研究智能体模型。我们在10个基准测试上评估了AgentFounder-30B，并取得了最先进的性能，同时保持了强大的工具使用能力，特别是在BrowseComp-en上达到39.9%，在BrowseComp-zh上达到43.3%，以及在HLE上达到31.5%的Pass@1。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have evolved into agentic systems capable ofautonomous tool use and multi-step reasoning for complex problem-solving.However, post-training approaches building upon general-purpose foundationmodels consistently underperform in agentic tasks, particularly in open-sourceimplementations. We identify the root cause: the absence of robust agenticfoundation models forces models during post-training to simultaneously learndiverse agentic behaviors while aligning them to expert demonstrations, therebycreating fundamental optimization tensions. To this end, we are the first topropose incorporating Agentic Continual Pre-training (Agentic CPT) into thedeep research agents training pipeline to build powerful agentic foundationalmodels. Based on this approach, we develop a deep research agent model namedAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achievestate-of-the-art performance while retains strong tool-use ability, notably39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.</description>
      <author>example@mail.com (Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou)</author>
      <guid isPermaLink="false">2509.13310v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>ResidualViT for Efficient Temporally Dense Video Encoding</title>
      <link>http://arxiv.org/abs/2509.13255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作通过创新的架构设计和蒸馏策略，显著提高了时间密集型视频理解任务的计算效率，同时保持了准确性。&lt;h4&gt;背景&lt;/h4&gt;多个视频理解任务需要'时间密集型'推理，如自然语言视频时序定位、时序活动定位和音频描述生成。这些任务需要在高时间分辨率下对帧进行采样，而计算帧级特征对于这些任务来说计算成本很高。&lt;h4&gt;目的&lt;/h4&gt;降低时间密集型任务中特征计算的成本，提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为ResidualViT的视觉Transformer架构，利用视频中的大时间冗余来高效计算时间密集的帧级特征。该架构包含可学习的残差连接确保时间一致性，以及标记减少模块提高处理速度。同时提出了一种轻量级蒸馏策略来近似原始基础模型的帧级特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个任务和五个数据集上进行了评估，在零样本和全监督设置下都表现出显著效果。计算成本降低了高达60%，推理速度提高了高达2.5倍，同时保持了与原始基础模型相近的准确性。&lt;h4&gt;结论&lt;/h4&gt;ResidualViT架构和蒸馏策略有效地降低了时间密集型视频理解任务的计算成本，同时保持了性能，为视频理解任务提供了更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多个视频理解任务，如自然语言视频时序定位、时序活动定位和音频描述生成，需要在高时间分辨率下采样的帧上进行'时间密集型'推理。然而，考虑到时间分辨率的要求，为这些任务计算帧级特征的计算成本很高。在本文中，我们做出了三项贡献以降低时间密集型任务的特征计算成本。首先，我们引入了一种视觉Transformer架构，称为ResidualViT，它利用视频中的大时间冗余来高效计算时间密集的帧级特征。我们的架构包含(i)确保连续帧之间时间一致性的可学习残差连接，以及(ii)一个标记减少模块，通过选择性丢弃时间冗余信息同时重用预训练基础模型的权重来提高处理速度。其次，我们提出了一种轻量级蒸馏策略来近似原始基础模型的帧级特征。最后，我们在四个任务和五个数据集上评估了我们的方法，在零样本和全监督设置下，展示了计算成本的显著降低（高达60%）和推理速度的改进（高达2.5倍），同时紧密近似了原始基础模型的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Several video understanding tasks, such as natural language temporal videogrounding, temporal activity localization, and audio description generation,require "temporally dense" reasoning over frames sampled at high temporalresolution. However, computing frame-level features for these tasks iscomputationally expensive given the temporal resolution requirements. In thispaper, we make three contributions to reduce the cost of computing features fortemporally dense tasks. First, we introduce a vision transformer (ViT)architecture, dubbed ResidualViT, that leverages the large temporal redundancyin videos to efficiently compute temporally dense frame-level features. Ourarchitecture incorporates (i) learnable residual connections that ensuretemporal consistency across consecutive frames and (ii) a token reductionmodule that enhances processing speed by selectively discarding temporallyredundant information while reusing weights of a pretrained foundation model.Second, we propose a lightweight distillation strategy to approximate theframe-level features of the original foundation model. Finally, we evaluate ourapproach across four tasks and five datasets, in both zero-shot and fullysupervised settings, demonstrating significant reductions in computational cost(up to 60%) and improvements in inference speed (up to 2.5x faster), all whileclosely approximating the accuracy of the original foundation model.</description>
      <author>example@mail.com (Mattia Soldan, Fabian Caba Heilbron, Bernard Ghanem, Josef Sivic, Bryan Russell)</author>
      <guid isPermaLink="false">2509.13255v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning</title>
      <link>http://arxiv.org/abs/2509.13240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NoRA是一种创新的参数高效微调(PEFT)框架，首次直接适应预训练Transformer模型中的非线性激活函数，而非仅调整权重矩阵。&lt;h4&gt;背景&lt;/h4&gt;现有的PEFT方法主要适应权重矩阵而保持激活函数固定，限制了模型适应的灵活性。&lt;h4&gt;目的&lt;/h4&gt;引入NoRA框架，探索激活函数作为模型适应一级对象的潜力，提供一种新的参数高效微调方法。&lt;h4&gt;方法&lt;/h4&gt;NoRA用可学习的有理函数替换固定的激活函数，对分子和分母系数应用结构化低秩更新，并采用分组设计实现本地化适应和提高稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;NoRA在视觉Transformer上仅更新0.4%参数即可匹配或超越完全微调；与LoRA结合的NoRA++在多种任务上优于其他方法；NoRA能将适应限制在低维函数子空间，实现隐式正则化。&lt;h4&gt;结论&lt;/h4&gt;激活空间调优是基于权重的PEFT的互补且高度参数高效的替代方案，激活函数应被视为模型适应的一级对象。&lt;h4&gt;翻译&lt;/h4&gt;现有的参数高效微调(PEFT)方法主要适应权重矩阵，同时保持激活函数固定。我们引入NoRA，这是第一个直接适应预训练基于Transformer模型的非线性激活函数的PEFT框架。NoRA用可学习的有理函数替换固定的激活函数，并对分子和分母系数应用结构化低秩更新，采用分组设计使适应本地化并提高稳定性，同时成本最小。在CIFAR-10和CIFAR-100上训练的视觉Transformer上，NoRA在仅更新0.4%参数(0.02M)的情况下匹配或完全微调，实现了+0.17%和+0.27%的准确率提升。与LoRA结合(NoRA++)时，在匹配的训练预算下，通过添加更少的可训练参数，优于LoRA和DoRA。在LLaMA3-8B指令调优上，NoRA++持续提高生成质量，平均MMLU提升+0.3%--0.8%，包括在STEM(Alpaca)上+1.6%和OpenOrca上+1.3%。我们进一步证明NoRA将适应限制在低维函数子空间，隐式正则化更新的幅度和方向。这些结果确立了激活空间调优作为基于权重的PEFT的互补且高度参数高效的替代方案，将激活函数定位为模型适应的一级对象。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing parameter-efficient fine-tuning (PEFT) methods primarily adaptweight matrices while keeping activation functions fixed. We introduce\textbf{NoRA}, the first PEFT framework that directly adapts nonlinearactivation functions in pretrained transformer-based models. NoRA replacesfixed activations with learnable rational functions and applies structuredlow-rank updates to numerator and denominator coefficients, with a group-wisedesign that localizes adaptation and improves stability at minimal cost. Onvision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceedsfull fine-tuning while updating only 0.4\% of parameters (0.02M), achievingaccuracy gains of +0.17\% and +0.27\%. When combined with LoRA(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgetsby adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++consistently improves generation quality, yielding average MMLU gains of+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. Wefurther show that NoRA constrains adaptation to a low-dimensional functionalsubspace, implicitly regularizing update magnitude and direction. These resultsestablish activation-space tuning as a complementary and highlyparameter-efficient alternative to weight-based PEFT, positioning activationfunctions as first-class objects for model adaptation.</description>
      <author>example@mail.com (Bo Yin, Xingyi Yang, Xinchao Wang)</author>
      <guid isPermaLink="false">2509.13240v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Road Obstacle Video Segmentation</title>
      <link>http://arxiv.org/abs/2509.13181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  GCPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对自动驾驶中道路障碍物分割问题，认识到该任务具有时间相关性，提出了基于视觉基础模型的新方法，在长序列视频分割中取得了最先进的效果。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶代理的广泛部署，道路障碍物的检测与分割对于确保安全导航变得至关重要。然而，现有的道路障碍物分割方法应用于单帧图像，忽略了问题的时间特性，导致连续帧之间的预测图不一致。&lt;h4&gt;目的&lt;/h4&gt;解决现有道路障碍物分割方法在处理连续帧时存在的不一致问题，提高道路障碍物视频分割的准确性，为未来研究提供有价值的见解和方向。&lt;h4&gt;方法&lt;/h4&gt;1. 构建并调整了四个用于道路障碍物视频分割的评估基准；2. 在这些基准上评估了11种最先进的图像和视频分割方法；3. 引入了两种基于视觉基础模型的新型基线方法。&lt;h4&gt;主要发现&lt;/h4&gt;道路障碍物分割任务本质上具有时间特性，因为连续帧的分割图之间存在强相关性。作者提出的方法在长序列视频分割中建立了新的技术水平。&lt;h4&gt;结论&lt;/h4&gt;通过考虑时间相关性，改进的道路障碍物视频分割方法能够提供更一致和准确的预测，这对自动驾驶系统的安全性和可靠性具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;随着自动驾驶代理的广泛部署，道路障碍物的检测与分割已成为确保安全自动驾驶的关键。然而，现有的道路障碍物分割方法应用于单个帧，忽略了问题的时间特性，导致连续帧之间的预测图不一致。在本工作中，我们证明了道路障碍物分割任务本质上具有时间性，因为连续帧的分割图之间存在强相关性。为此，我们整理并调整了四个用于道路障碍物视频分割的评估基准，并评估了11种最先进的基于图像和视频的分割方法。此外，我们引入了两种基于视觉基础模型的有效基线方法。我们的方法在长序列视频的道路障碍物视频分割中建立了新的技术水平，为未来研究提供了宝贵的见解和方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing deployment of autonomous driving agents, the detection andsegmentation of road obstacles have become critical to ensure safe autonomousnavigation. However, existing road-obstacle segmentation methods are applied onindividual frames, overlooking the temporal nature of the problem, leading toinconsistent prediction maps between consecutive frames. In this work, wedemonstrate that the road-obstacle segmentation task is inherently temporal,since the segmentation maps for consecutive frames are strongly correlated. Toaddress this, we curate and adapt four evaluation benchmarks for road-obstaclevideo segmentation and evaluate 11 state-of-the-art image- and video-basedsegmentation methods on these benchmarks. Moreover, we introduce two strongbaseline methods based on vision foundation models. Our approach establishes anew state-of-the-art in road-obstacle video segmentation for long-range videosequences, providing valuable insights and direction for future research.</description>
      <author>example@mail.com (Shyam Nandan Rai, Shyamgopal Karthik, Mariana-Iuliana Georgescu, Barbara Caputo, Carlo Masone, Zeynep Akata)</author>
      <guid isPermaLink="false">2509.13181v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection</title>
      <link>http://arxiv.org/abs/2509.12995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于现代视觉基础模型的简单线性分类器方法，用于检测AI生成的图像，相比专门设计的检测器在真实场景中表现更优。&lt;h4&gt;背景&lt;/h4&gt;专门的AI生成图像检测器在精心设计的基准测试中表现出色，但在真实场景中表现很差，表现为在'野外'基准测试中有极高的假阴性率。&lt;h4&gt;目的&lt;/h4&gt;开发一种更有效的AI生成图像检测方法，能够在真实世界场景中取得更好的性能。&lt;h4&gt;方法&lt;/h4&gt;在现代视觉基础模型(VFM)上使用简单的线性分类器，而不是设计专门的检测器，在相同数据上训练。&lt;h4&gt;主要发现&lt;/h4&gt;1) 基于VFM的方法在'野外'准确率上比专门设计的检测器提高了超过20%；2) 最近的VLM已经学会将合成图像与伪造相关概念对齐；3) 这种对齐能力可能源于模型在预训练期间接触到的数据。&lt;h4&gt;结论&lt;/h4&gt;1) 对于AI生成图像检测，更新后的VFM的原始能力比静态检测器的专门设计更有效；2) 真正的泛化评估需要测试数据独立于模型的整个训练历史，包括预训练。&lt;h4&gt;翻译&lt;/h4&gt;虽然针对AI生成图像的专业检测器在精心设计的基准测试中表现出色，但它们在真实场景中的表现灾难性地差，正如它们在'野外'基准测试中极高的假阴性率所证明的那样。我们不是为这个问题再打造一把专门的'刀'，而是带来一把'枪'：在现代视觉基础模型(VFM)上使用一个简单的线性分类器。在相同数据上训练后，这个基线方法明显优于专门设计的检测器，在'野外'准确率上提高了超过20%。我们的分析确定了VFM'火力'的来源：首先，通过探测文本-图像相似性，我们发现最近的VLM(如Perception Encoder、Meta CLIP2)已经学会将合成图像与伪造相关概念(如'AI生成')对齐，而之前的版本则没有。其次，我们推测这是由于数据暴露，因为在一个在VFM预训练截止日期后抓取的新数据集上，这种对齐和整体准确率都大幅下降，确保了在预训练期间未见。我们的研究得出两个关键结论：1) 对于AI生成图像检测的真实'枪战'，更新后的VFM的原始'火力'比静态检测器的'工艺'更有效。2) 真正的泛化评估需要测试数据独立于模型的整个训练历史，包括预训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While specialized detectors for AI-generated images excel on curatedbenchmarks, they fail catastrophically in real-world scenarios, as evidenced bytheir critically high false-negative rates on `in-the-wild' benchmarks. Insteadof crafting another specialized `knife' for this problem, we bring a `gun' tothe fight: a simple linear classifier on a modern Vision Foundation Model(VFM). Trained on identical data, this baseline decisively `outguns' bespokedetectors, boosting in-the-wild accuracy by a striking margin of over 20\%.  Our analysis pinpoints the source of the VFM's `firepower': First, by probingtext-image similarities, we find that recent VLMs (e.g., Perception Encoder,Meta CLIP2) have learned to align synthetic images with forgery-relatedconcepts (e.g., `AI-generated'), unlike previous versions. Second, we speculatethat this is due to data exposure, as both this alignment and overall accuracyplummet on a novel dataset scraped after the VFM's pre-training cut-off date,ensuring it was unseen during pre-training. Our findings yield two criticalconclusions: 1) For the real-world `gunfight' of AI-generated image detection,the raw `firepower' of an updated VFM is far more effective than the`craftsmanship' of a static detector. 2) True generalization evaluationrequires test data to be independent of the model's entire training history,including pre-training.</description>
      <author>example@mail.com (Yue Zhou, Xinan He, Kaiqing Lin, Bing Fan, Feng Ding, Jinhua Zeng, Bin Li)</author>
      <guid isPermaLink="false">2509.12995v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder</title>
      <link>http://arxiv.org/abs/2509.12991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A simple yet effective strategy for ECG foundation models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种简单而有效的后训练方法，用于增强ECG基础模型，显著提升了其在临床应用中的性能，特别是在有限数据情况下表现优异。&lt;h4&gt;背景&lt;/h4&gt;ECG基础模型因其跨任务适应性而日益流行，但与特定任务模型相比存在性能差距，即使在大型数据集预训练和目标数据微调后仍然存在，这可能是由于缺乏有效的后训练策略。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单而有效的后训练方法来增强ECG Founder模型（一种在超过700万个ECG记录上预训练的最先进基础模型），提高其临床适用性。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单而有效的后训练方法应用于ECG Founder模型，并在PTB-XL基准测试上进行评估，同时进行了消融研究以确定关键性能贡献组件。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在宏观AUROC上比基线微调策略提高1.2%-3.3%，在宏观AUPRC上提高5.3%-20.9%；优于多种先进方法；仅使用10%训练数据时，宏观AUROC提高9.1%，宏观AUPRC提高34.9%；随机深度和预览线性探测是关键性能提升组件。&lt;h4&gt;结论&lt;/h4&gt;后训练策略在改进ECG基础模型方面具有显著潜力，这项工作有望促进ECG领域基础模型的持续发展。&lt;h4&gt;翻译&lt;/h4&gt;ECG基础模型因其跨任务的适应性而日益流行。然而，与特定任务模型相比，它们的临床适用性通常受到性能差距的限制，即使在大型ECG数据集上进行预训练并在目标数据上进行微调后也是如此。这种局限性可能是由于缺乏有效的后训练策略。在本文中，我们提出了一种简单而有效的后训练方法来增强ECG Founder模型，这是一种在超过700万个ECG记录上预训练的最先进ECG基础模型。在PTB-XL基准测试上的实验表明，我们的方法在宏观AUROC上比基线微调策略提高了1.2%-3.3%，在宏观AUPRC上提高了5.3%-20.9%。此外，我们的方法优于几种最近的先进方法，包括特定任务模型和高级架构。进一步评估显示，与基线相比，我们的方法更稳定且样本效率更高，仅使用10%的训练数据即可实现9.1%的宏观AUROC和34.9%的宏观AUPRC改进。消融研究确定了随机深度和预览线性探测等关键组件，这些组件有助于提高性能。这些发现强调了后训练策略在改进ECG基础模型方面的潜力，我们希望这项工作将促进ECG领域基础模型的持续发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; ECG foundation models are increasingly popular due to their adaptabilityacross various tasks. However, their clinical applicability is often limited byperformance gaps compared to task-specific models, even after pre-training onlarge ECG datasets and fine-tuning on target data. This limitation is likelydue to the lack of an effective post-training strategy. In this paper, wepropose a simple yet effective post-training approach to enhance ECGFounder, astate-of-the-art ECG foundation model pre-trained on over 7 million ECGrecordings. Experiments on the PTB-XL benchmark show that our approach improvesthe baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% inmacro AUPRC. Additionally, our method outperforms several recentstate-of-the-art approaches, including task-specific and advancedarchitectures. Further evaluation reveals that our method is more stable andsample-efficient compared to the baseline, achieving a 9.1% improvement inmacro AUROC and a 34.9% improvement in macro AUPRC using just 10% of thetraining data. Ablation studies identify key components, such as stochasticdepth and preview linear probing, that contribute to the enhanced performance.These findings underscore the potential of post-training strategies to improveECG foundation models, and we hope this work will contribute to the continueddevelopment of foundation models in the ECG domain.</description>
      <author>example@mail.com (Ya Zhou, Yujie Yang, Xiaohan Fan, Wei Zhao)</author>
      <guid isPermaLink="false">2509.12991v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Data Scaling Laws for Radiology Foundation Models</title>
      <link>http://arxiv.org/abs/2509.12818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性地研究了两种医学影像视觉编码器MI2和RAD-DINO在大规模胸部X光片数据上的持续预训练效果，评估了模型在分类、分割和放射报告生成任务上的表现，发现不同模型在不同任务上有各自优势，并证明了特定中心持续预训练的实用价值。&lt;h4&gt;背景&lt;/h4&gt;基础视觉编码器如CLIP和DINOv2经过网络规模数据训练后展现出强大的跨任务性能，但医学影像基础模型受限于较小的数据集，限制了我们对数据规模和预训练范式如何影响性能的理解。&lt;h4&gt;目的&lt;/h4&gt;系统研究两种代表CLIP和DINOv2范式的视觉编码器MI2和RAD-DINO在350万张胸部X光片上的持续预训练效果，同时保持计算和评估协议不变。&lt;h4&gt;方法&lt;/h4&gt;在350万张胸部X光片上持续预训练两种视觉编码器；评估模型在分类、分割和放射报告生成任务上的表现；包含线条和导管任务以平衡偏倚；使用UniCL结合报告和结构化标签预训练MI2；测试仅需少量领域内样本即可超越开放权重基础模型的场景。&lt;h4&gt;主要发现&lt;/h4&gt;MI2在发现相关任务上扩展更有效，RAD-DINO在导管相关任务上更强；使用报告和结构化标签通过UniCL持续预训练MI2可提升性能，突显大规模结构化监督的价值；对于某些任务，仅需30k领域内样本就足以超越开放权重基础模型。&lt;h4&gt;结论&lt;/h4&gt;特定中心持续预训练具有实用价值，医疗机构可利用领域内数据获得显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;基础视觉编码器如CLIP和DINOv2经过网络规模数据训练后，在任务和数据集间展现出强大的转移性能。然而，医学影像基础模型仍受限于较小的数据集，限制了我们对数据规模和预训练范式在此环境中如何影响性能的理解。在本工作中，我们系统地研究了两种视觉编码器（代表CLIP和DINOv2两种主要编码器范式的MedImageInsight (MI2)和RAD-DINO）在多达350万张来自单一机构的胸部X光片上的持续预训练，同时保持计算和评估协议不变。我们在分类（放射学发现、线条和导管）、分割（线条和导管）和放射报告生成任务上进行了评估。尽管先前工作主要关注与放射学发现相关的任务，但我们包含了线条和导管任务，以平衡这种偏倚，并评估模型提取沿 elongated 结构保持连续性的特征的能力。我们的实验表明，MI2在发现相关任务上扩展更有效，而RAD-DINO在导管相关任务上更强。令人惊讶的是，使用报告和结构化标签通过UniCL持续预训练MI2可以提高性能，突显了大规模结构化监督的价值。我们进一步表明，对于某些任务，仅需30k领域内样本就足以超越开放权重基础模型。这些结果突显了特定中心持续预训练的实用性，使医疗机构能够利用领域内数据获得显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation vision encoders such as CLIP and DINOv2, trained on web-scaledata, exhibit strong transfer performance across tasks and datasets. However,medical imaging foundation models remain constrained by smaller datasets,limiting our understanding of how data scale and pretraining paradigms affectperformance in this setting. In this work, we systematically study continualpretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINOrepresenting the two major encoder paradigms CLIP and DINOv2, on up to 3.5Mchest x-rays from a single institution, holding compute and evaluationprotocols constant. We evaluate on classification (radiology findings, linesand tubes), segmentation (lines and tubes), and radiology report generation.While prior work has primarily focused on tasks related to radiology findings,we include lines and tubes tasks to counterbalance this bias and evaluate amodel's ability to extract features that preserve continuity along elongatedstructures. Our experiments show that MI2 scales more effectively forfinding-related tasks, while RAD-DINO is stronger on tube-related tasks.Surprisingly, continually pretraining MI2 with both reports and structuredlabels using UniCL improves performance, underscoring the value of structuredsupervision at scale. We further show that for some tasks, as few as 30kin-domain samples are sufficient to surpass open-weights foundation models.These results highlight the utility of center-specific continual pretraining,enabling medical institutions to derive significant performance gains byutilizing in-domain data.</description>
      <author>example@mail.com (Maximilian Ilse, Harshita Sharma, Anton Schwaighofer, Sam Bond-Taylor, Fernando Pérez-García, Olesya Melnichenko, Anne-Marie G. Sykes, Kelly K. Horst, Ashish Khandelwal, Maxwell Reynolds, Maria T. Wetscherek, Noel C. F. Codella, Javier Alvarez-Valle, Korfiatis Panagiotis, Valentina Salvatelli)</author>
      <guid isPermaLink="false">2509.12818v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach</title>
      <link>http://arxiv.org/abs/2509.12697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种双层个性化框架，用于联邦基础模型的微调，以解决在有限数据情况下个性化与联邦之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;联邦基础模型代表了一种新的范式，可以在客户端之间联合微调预训练的基础模型。然而，为少数新用户或专门场景微调基础模型仍然是一个挑战，因为这些场景涉及的数据规模通常远小于预训练时使用的大规模数据。&lt;h4&gt;目的&lt;/h4&gt;解决在有限数据情况下个性化与联邦之间的权衡问题，特别是在为少数新用户或专门场景微调基础模型时。&lt;h4&gt;方法&lt;/h4&gt;提出了一种双层个性化框架：在客户端层面，使用私有数据进行个性化微调；在服务器层面，使用基于客户端特定任务向量测量的相似用户进行个性化聚合。&lt;h4&gt;主要发现&lt;/h4&gt;通过客户端微调获得的个性化信息，服务器层面的个性化聚合可以获得群体级别的个性化信息，同时减轻非独立同分布数据中不相关或利益冲突客户端的干扰。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法在基准数据集上的广泛实验分析中证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;联邦基础模型代表了一种新的范式，可以在客户端之间联合微调预训练的基础模型。为少数新用户或专门场景微调基础模型仍然是一个挑战，这些场景涉及的数据规模通常远小于预训练时使用的大规模数据。在这种情况下，个性化与联邦之间的权衡变得更加敏感。为了解决这些问题，我们提出了一种用于联邦基础模型微调的双层个性化框架。具体来说，我们在客户端层面使用私有数据进行个性化微调，然后在服务器层面使用基于客户端特定任务向量测量的相似用户进行个性化聚合。鉴于从客户端微调中获得的个性化信息，服务器层面的个性化聚合可以获得群体级别的个性化信息，同时减轻非独立同分布数据中不相关或利益冲突客户端的干扰。所提出的算法在基准数据集上的广泛实验分析中证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated foundation models represent a new paradigm to jointly fine-tunepre-trained foundation models across clients. It is still a challenge tofine-tune foundation models for a small group of new users or specializedscenarios, which typically involve limited data compared to the large-scaledata used in pre-training. In this context, the trade-off betweenpersonalization and federation becomes more sensitive. To tackle these, weproposed a bi-level personalization framework for federated fine-tuning onfoundation models. Specifically, we conduct personalized fine-tuning on theclient-level using its private data, and then conduct a personalizedaggregation on the server-level using similar users measured by client-specifictask vectors. Given the personalization information gained from client-levelfine-tuning, the server-level personalized aggregation can gain group-wisepersonalization information while mitigating the disturbance of irrelevant orinterest-conflict clients with non-IID data. The effectiveness of the proposedalgorithm has been demonstrated by extensive experimental analysis in benchmarkdatasets.</description>
      <author>example@mail.com (Yiyuan Yang, Guodong Long, Qinghua Lu, Liming Zhu, Jing Jiang)</author>
      <guid isPermaLink="false">2509.12697v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.12650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages,8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TimeRep是一种利用时间序列基础模型中间层表示进行异常检测的新方法，通过计算表示之间的距离作为异常分数，并采用核心集策略和适应机制处理概念漂移。&lt;h4&gt;背景&lt;/h4&gt;时间序列数据中的异常检测对许多现实系统的可靠运行至关重要，时间序列基础模型已成为异常检测的有力工具。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖于TSFMs最终层表示，而是利用中间层表示进行异常检测的新方法。&lt;h4&gt;方法&lt;/h4&gt;TimeRep选择最具信息量的中间层和patch-token位置，从训练数据形成中间表示参考集合，应用核心集策略减小集合大小，在推理时通过测量距离计算异常分数，并集成适应机制处理概念漂移。&lt;h4&gt;主要发现&lt;/h4&gt;在包含250个单变量时间序列的UCR异常档案上进行的实验表明，TimeRep始终优于各种最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;TimeRep是一种有效的时间序列异常检测方法，通过利用中间层表示而非最终层表示，结合核心集策略和适应机制，在各种基线方法上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;检测时间序列数据中的异常对于许多现实世界系统的可靠运行至关重要。最近，时间序列基础模型(TSFMs)已成为异常检测的有力工具。然而，现有方法通常依赖于TSFMs的最终层表示，通过特定任务头来计算异常分数作为重建或预测误差。相反，我们提出了TimeRep，一种新颖的异常检测方法，利用TSFMs的中间层表示，将这些表示之间的距离计算为异常分数。给定预训练的TSFM，TimeRep选择产生最具信息量表示的中间层和patch-token位置。TimeRep从训练数据形成中间表示的参考集合，并应用core-set策略来减小其大小，同时保持分布覆盖。在推理期间，TimeRep通过测量其中间表示与集合中中间表示之间的距离来计算传入数据的异常分数。为了处理概念漂移，TimeRep集成了一个适应机制，在推理时，仅用来自传入数据的非冗余中间表示来增强集合。我们在包含250个单变量时间序列的UCR异常档案上进行了广泛的实验。TimeRep始终优于各种最先进的基线方法，包括非深度学习、深度学习和基于基础模型的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting anomalies in time series data is essential for the reliableoperation of many real-world systems. Recently, time series foundation models(TSFMs) have emerged as a powerful tool for anomaly detection. However,existing methods typically rely on the final layer's representations of TSFMs,computing the anomaly score as a reconstruction or forecasting error via atask-specific head. Instead, we propose TimeRep, a novel anomaly detectionapproach that leverages the intermediate layer's representations of TSFMs,computing the anomaly score as the distance between these representations.Given a pre-trained TSFM, TimeRep selects the intermediate layer andpatch-token position that yield the most informative representation. TimeRepforms a reference collection of intermediate representations from the trainingdata and applies a core-set strategy to reduce its size while maintainingdistributional coverage. During inference, TimeRep computes the anomaly scorefor incoming data by measuring the distance between its intermediaterepresentations and those of the collection. To address concept drift, TimeRepintegrates an adaptation mechanism that, at inference time, augments thecollection exclusively with non-redundant intermediate representations fromincoming data. We conducted extensive experiments on the UCR Anomaly Archive,which contains 250 univariate time series. TimeRep consistently outperforms abroad spectrum of state-of-the-art baselines, including non-DL, DL, andfoundation model-based methods.</description>
      <author>example@mail.com (Chan Sik Han, Keon Myung Lee)</author>
      <guid isPermaLink="false">2509.12650v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction</title>
      <link>http://arxiv.org/abs/2509.12600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MICE多模态基础模型，通过整合病理图像、临床报告和基因组数据，实现了精确的泛癌预后预测，显著提升了模型的泛化性和数据效率。&lt;h4&gt;背景&lt;/h4&gt;多模态数据为全面理解肿瘤微环境提供了异构信息，但现有AI模型往往难以有效利用多模态数据中的丰富信息，提取的泛化性较差。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效整合病理图像、临床报告和基因组数据的多模态基础模型，用于精确的泛癌预后预测。&lt;h4&gt;方法&lt;/h4&gt;提出了MICE（Multimodal data Integration via Collaborative Experts）多模态基础模型，采用多种功能不同的专家全面捕捉跨癌种和癌种特定洞察，利用来自30种癌症类型、11,799名患者的数据，通过对比学习和监督学习相结合增强MICE的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;MICE优于单模态和最先进的基于多专家的多模态模型，在内部队列中C-index提高了3.8%至11.2%，在独立队列中提高了5.8%至8.8%，同时在多样化的临床场景中表现出卓越的数据效率。&lt;h4&gt;结论&lt;/h4&gt;MICE具有增强的泛化性和数据效率，为泛癌预后预测提供了有效且可扩展的基础，有潜力个性化定制治疗并改善治疗效果。&lt;h4&gt;翻译&lt;/h4&gt;多模态数据为全面理解肿瘤微环境提供了异构信息。然而，现有的人工智能模型往往难以利用多模态数据中的丰富信息，提取的泛化性较差。在此，我们提出了MICE（通过协作专家进行多模态数据集成），这是一个多模态基础模型，能够有效整合病理图像、临床报告和基因组数据，用于精确的泛癌预后预测。MICE采用多种功能不同的专家，全面捕捉跨癌种和癌种特定的洞察，而非传统的多专家模块。利用来自30种癌症类型、11,799名患者的数据，我们通过结合对比学习和监督学习增强了MICE的泛化能力。MICE优于单模态和最先进的基于多专家的多模态模型，在内部队列中C-index显著提高了3.8%至11.2%，在独立队列中提高了5.8%至8.8%。此外，它在多样化的临床场景中表现出卓越的数据效率。凭借其增强的泛化性和数据效率，MICE为泛癌预后预测建立了有效且可扩展的基础，具有为个性化定制治疗和改善治疗效果提供强大潜力的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal data provides heterogeneous information for a holisticunderstanding of the tumor microenvironment. However, existing AI models oftenstruggle to harness the rich information within multimodal data and extractpoorly generalizable representations. Here we present MICE (Multimodal dataIntegration via Collaborative Experts), a multimodal foundation model thateffectively integrates pathology images, clinical reports, and genomics datafor precise pan-cancer prognosis prediction. Instead of conventionalmulti-expert modules, MICE employs multiple functionally diverse experts tocomprehensively capture both cross-cancer and cancer-specific insights.Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE'sgeneralizability by coupling contrastive and supervised learning. MICEoutperformed both unimodal and state-of-the-art multi-expert-based multimodalmodels, demonstrating substantial improvements in C-index ranging from 3.8% to11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,respectively. Moreover, it exhibited remarkable data efficiency across diverseclinical scenarios. With its enhanced generalizability and data efficiency,MICE establishes an effective and scalable foundation for pan-cancer prognosisprediction, holding strong potential to personalize tailored therapies andimprove treatment outcomes.</description>
      <author>example@mail.com (Huajun Zhou, Fengtao Zhou, Jiabo Ma, Yingxue Xu, Xi Wang, Xiuming Zhang, Li Liang, Zhenhui Li, Hao Chen)</author>
      <guid isPermaLink="false">2509.12600v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification</title>
      <link>http://arxiv.org/abs/2509.12512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACCEPTED at the ICCV 2025 Workshop on Anomaly Detection with  Foundation Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于注意力的全局聚合框架，用于3D医学图像异常分类，利用预训练的DINOv2模型和软注意机制处理脑部MRI切片，并通过复合损失函数解决数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;医学影像中的异常检测和分类对早期诊断至关重要，但由于标注数据有限、类别不平衡和专家标注的高成本，这仍然是一个挑战。新兴的视觉基础模型如DINOv2在大量无标注数据上预训练，可提供通用表示来缓解这些限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门用于3D医学图像异常分类的基于注意力的全局聚合框架，利用自监督DINOv2模型作为预训练特征提取器。&lt;h4&gt;方法&lt;/h4&gt;处理脑部MRI的2D轴向切片，通过软注意机制分配自适应的切片级重要性权重，并采用复合损失函数结合监督对比学习和类别方差正则化来增强类间分离性和类内一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在ADNI数据集和机构多类别头痛队列上的验证表明，尽管数据有限且类别不平衡显著，该方法仍表现出强大的异常分类性能。&lt;h4&gt;结论&lt;/h4&gt;利用预训练的2D基础模型结合基于注意力的切片聚合，对于实现医学影像中鲁棒的体积异常检测是有效的，该方法已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;医学影像中的异常检测和分类对早期诊断至关重要，但由于标注数据有限、类别不平衡和专家标注的高成本，这仍然具有挑战性。新兴的视觉基础模型（如DINOv2）在大量无标注数据上预训练，可提供通用表示， potentially缓解这些限制。在本研究中，我们提出了一种专门用于3D医学图像异常分类的基于注意力的全局聚合框架。利用自监督DINOv2模型作为预训练特征提取器，我们的方法处理脑部MRI的2D轴向切片，通过软注意机制分配自适应的切片级重要性权重。为进一步解决数据稀缺问题，我们采用结合监督对比学习和类别方差正则化的复合损失函数，增强类间分离性和类内一致性。我们在ADNI数据集和机构多类别头痛队列上验证了我们的框架，尽管数据有限且类别不平衡显著，但仍表现出强大的异常分类性能。我们的结果突显了利用预训练的2D基础模型结合基于注意力的切片聚合进行医学影像中鲁棒体积异常检测的有效性。我们的实现已在https://github.com/Rafsani/DinoAtten3D.git上公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学影像（特别是3D脑部MRI）中异常检测和分类的挑战，包括标注数据有限、类别不平衡和专家标注成本高的问题。这个问题在现实中非常重要，因为早期疾病诊断对治疗和预后至关重要，而医学数据获取和标注成本高昂限制了监督学习方法的应用，类别不平衡问题也影响模型泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统监督学习方法需要大量标注数据且易过拟合，无监督方法（如基于GAN或扩散模型）仅依赖健康数据重建，可能因健康数据有限而泛化能力不足。作者注意到基础模型（如DINOv2）在大规模无标签数据上预训练的潜力，但识别到这些模型本质上是2D的，无法直接处理3D医学影像。作者借鉴了注意力池化在高维医学图像分析中的有效性、监督对比学习在处理不平衡分类问题上的进展（如SC-MIL）以及DINOv2作为特征提取器的泛化能力，设计了结合注意力机制和基础模型的新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练的2D基础模型（DINOv2）处理3D医学影像的各个2D切片，通过软注意力机制自适应地聚合不同切片的特征，为诊断相关的切片分配更高权重，并结合监督对比学习和类方差正则化处理数据稀缺和类别不平衡问题。整体流程包括：1)将3D MRI分解为2D切片，用DINOv2提取每个切片的特征嵌入；2)使用两层MLN学习注意力分数，通过softmax转换为权重，计算加权聚合特征；3)将聚合特征通过MLP映射后进行分类；4)结合交叉熵损失、监督对比损失和类内方差损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全局注意力驱动的3D医学影像聚合框架，自适应融合切片级嵌入；2)针对数据稀缺和类别不平衡的组合损失函数；3)在两个真实临床队列（ADNI和头痛队列）上验证。相比之前的工作，不同之处在于：不同于传统MIL方法可能忽略跨切片病理特征，DinoAtten3D使用全局注意力聚合切片级特征；不同于直接应用2D基础模型到3D影像，通过注意力机制桥接2D能力和体积特性；不同于单一监督对比学习，结合类方差正则化专门针对医学影像数据稀缺和类别不平衡问题设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DinoAtten3D通过结合DINOv2基础模型的特征提取能力和切片级软注意力机制，为3D脑部MRI图像异常检测提供了一个高效、数据友好的解决方案，在数据有限和类别不平衡的情况下仍能保持强大的分类性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection and classification in medical imaging are critical forearly diagnosis but remain challenging due to limited annotated data, classimbalance, and the high cost of expert labeling. Emerging vision foundationmodels such as DINOv2, pretrained on extensive, unlabeled datasets, offergeneralized representations that can potentially alleviate these limitations.In this study, we propose an attention-based global aggregation frameworktailored specifically for 3D medical image anomaly classification. Leveragingthe self-supervised DINOv2 model as a pretrained feature extractor, our methodprocesses individual 2D axial slices of brain MRIs, assigning adaptiveslice-level importance weights through a soft attention mechanism. To furtheraddress data scarcity, we employ a composite loss function combining supervisedcontrastive learning with class-variance regularization, enhancing inter-classseparability and intra-class consistency. We validate our framework on the ADNIdataset and an institutional multi-class headache cohort, demonstrating stronganomaly classification performance despite limited data availability andsignificant class imbalance. Our results highlight the efficacy of utilizingpretrained 2D foundation models combined with attention-based slice aggregationfor robust volumetric anomaly detection in medical imaging. Our implementationis publicly available at https://github.com/Rafsani/DinoAtten3D.git.</description>
      <author>example@mail.com (Fazle Rafsani, Jay Shah, Catherine D. Chong, Todd J. Schwedt, Teresa Wu)</author>
      <guid isPermaLink="false">2509.12512v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model</title>
      <link>http://arxiv.org/abs/2509.12497v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作比较了基础模型与传统方法在神经科学中的时间序列预测和因果发现能力，发现基础模型在零样本设置下表现良好，能够更精确地检测因果相互作用。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测和因果发现在神经科学中处于核心地位，因为预测大脑活动并识别神经群体和回路之间的因果关系可以阐明认知和疾病背后的机制。随着基础模型的兴起，一个开放性问题是如何它们与传统方法在脑信号预测和因果分析方面的比较，以及它们是否可以在零样本设置中应用。&lt;h4&gt;目的&lt;/h4&gt;评估一个基础模型与传统方法在推断人类功能性磁共振成像（fMRI）测量的自发性大脑活动中的方向性相互作用方面的比较。&lt;h4&gt;方法&lt;/h4&gt;研究人员测试了基础模型在零样本和微调设置中的预测能力，通过将模型中的类Granger估计与标准Granger因果性进行比较来评估因果性，并使用从真实因果模型生成的合成时间序列（包括逻辑映射耦合和Ornstein-Uhlenbeck过程）验证了该方法。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在零样本预测fMRI时间序列方面具有竞争力（对照组平均绝对百分误差为0.55，患者组为0.27）；尽管标准Granger因果性没有显示出模型之间的明显定量差异，但基础模型提供了更精确的因果相互作用检测。&lt;h4&gt;结论&lt;/h4&gt;总的来说，这些发现表明基础模型提供了多功能性、强大的零样本性能，以及在时间序列数据中进行预测和因果发现的潜在效用。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测和因果发现在神经科学中处于核心地位，因为预测大脑活动并识别神经群体和回路之间的因果关系可以阐明认知和疾病背后的机制。随着基础模型的兴起，一个开放性问题是如何它们与传统方法在脑信号预测和因果分析方面的比较，以及它们是否可以在零样本设置中应用。在这项工作中，我们评估了一个基础模型与传统方法在推断人类功能性磁共振成像（fMRI）测量的自发性大脑活动中的方向性相互作用方面的比较。传统方法通常依赖于Wiener-Granger因果性。我们测试了基础模型在零样本和微调设置中的预测能力，并通过将模型中的类Granger估计与标准Granger因果性比较来评估因果性。我们使用从真实因果模型生成的合成时间序列（包括逻辑映射耦合和Ornstein-Uhlenbeck过程）验证了该方法。基础模型在零样本预测fMRI时间序列方面具有竞争力（对照组平均绝对百分误差为0.55，患者组为0.27）。尽管标准Granger因果性没有显示出模型之间的明显定量差异，但基础模型提供了更精确的因果相互作用检测。总的来说，这些发现表明基础模型提供了多功能性、强大的零样本性能，以及在时间序列数据中进行预测和因果发现的潜在效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-series forecasting and causal discovery are central in neuroscience, aspredicting brain activity and identifying causal relationships between neuralpopulations and circuits can shed light on the mechanisms underlying cognitionand disease. With the rise of foundation models, an open question is how theycompare to traditional methods for brain signal forecasting and causalityanalysis, and whether they can be applied in a zero-shot setting. In this work,we evaluate a foundation model against classical methods for inferringdirectional interactions from spontaneous brain activity measured withfunctional magnetic resonance imaging (fMRI) in humans. Traditional approachesoften rely on Wiener-Granger causality. We tested the forecasting ability ofthe foundation model in both zero-shot and fine-tuned settings, and assessedcausality by comparing Granger-like estimates from the model with standardGranger causality. We validated the approach using synthetic time seriesgenerated from ground-truth causal models, including logistic map coupling andOrnstein-Uhlenbeck processes. The foundation model achieved competitivezero-shot forecasting fMRI time series (mean absolute percentage error of 0.55in controls and 0.27 in patients). Although standard Granger causality did notshow clear quantitative differences between models, the foundation modelprovided a more precise detection of causal interactions.  Overall, these findings suggest that foundation models offer versatility,strong zero-shot performance, and potential utility for forecasting and causaldiscovery in time-series data.</description>
      <author>example@mail.com (Alessandro Crimi, Andrea Brovelli)</author>
      <guid isPermaLink="false">2509.12497v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Towards Foundational Models for Single-Chip Radar</title>
      <link>http://arxiv.org/abs/2509.12482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文收集了最大的原始雷达数据集(100万个样本，29小时)，训练了通用雷达变换器(GRT)基础模型，用于4D单芯片雷达的3D占用预测和语义分割任务，实现了通常需要更高分辨率传感器才能达到的质量水平。&lt;h4&gt;背景&lt;/h4&gt;毫米波雷达是紧凑、廉价、耐用的传感器，对遮挡和环境条件(如天气和黑暗)具有鲁棒性，但角度分辨率较差，特别是廉价的单芯片雷达。虽然已有基于学习的方法来缓解这一弱点，但毫米波雷达缺乏标准的基础模型或大型数据集，实践者通常使用较小的数据集从头训练特定任务模型。&lt;h4&gt;目的&lt;/h4&gt;收集大型原始雷达数据集，训练适用于4D单芯片雷达的基础模型，实现高质量的3D占用预测和语义分割。&lt;h4&gt;方法&lt;/h4&gt;收集100万个样本(29小时)的原始雷达数据集，开发Generalizable Radar Transformer (GRT)模型，进行广泛的消融实验，评估原始雷达数据与有损表示的性能差异。&lt;h4&gt;主要发现&lt;/h4&gt;GRT能在不同环境中泛化，可针对不同任务微调，数据呈对数缩放比例(每增加10倍数据，性能提高20%)；使用原始雷达数据显著优于有损表示，相当于增加10倍训练数据的效果；估计需要约1亿个样本(3000小时)的数据才能完全发挥GRT潜力。&lt;h4&gt;结论&lt;/h4&gt;通过大型数据集和GRT模型解决了毫米波雷达角度分辨率差的问题，原始数据比有损表示更有效，GRT具有良好的泛化能力和可微调性，未来需要更多数据进一步发挥模型潜力。&lt;h4&gt;翻译&lt;/h4&gt;毫米波雷达是紧凑、廉价且耐用的传感器，对遮挡具有鲁棒性，并且在天气和黑暗等环境条件下仍能正常工作。然而，这以较差的角度分辨率为代价，特别是对于通常用于汽车和室内感应应用的廉价单芯片雷达。尽管许多人提出了基于学习的方法来缓解这一弱点，但毫米波雷达尚未出现标准的基础模型或大型数据集，实践者主要使用相对较小的数据集从头开始训练特定任务模型。在本文中，我们收集了(据我们所知)最大的可用原始雷达数据集，包含100万个样本(29小时)，并训练了一个用于4D单芯片雷达的基础模型，该模型可以预测3D占用和进行语义分割，其质量通常只有在使用更高分辨率传感器时才能实现。我们证明了我们的通用雷达变换器(GRT)能够在不同环境中泛化，可以针对不同任务进行微调，并显示出每增加10倍数据性能提升20%的对数数据缩放比例。我们还对常见设计决策进行了广泛的消融实验，发现使用原始雷达数据显著优于广泛使用的有损表示，相当于增加10倍训练数据的效果。最后，我们大致估计需要约1亿个样本(3000小时)的数据才能完全发挥GRT的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单芯片毫米波雷达缺乏基础模型和大规模数据集的问题。毫米波雷达虽然体积小、成本低、耐用且不受环境条件影响，但角度分辨率差，限制了其在自动驾驶、室内感知等领域的应用。目前实践者大多使用小数据集从头训练特定任务模型，效率低下。缺乏基础模型阻碍了雷达感知技术的发展，因此这个问题对推动雷达技术在自动驾驶和机器人领域的应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到毫米波雷达处理流程中过度依赖有损点云表示和缺乏大规模数据集的瓶颈。他们设计了一个便携式的多模态数据收集系统'red-rover'，在不同场景收集原始雷达数据。在模型设计上，借鉴了计算机视觉中的Transformer架构和Perceiver I/O的查询机制，将其适配到4D雷达数据处理。作者参考了雷达处理领域的4D FFT技术，以及现有雷达感知任务定义，但创新性地将其应用于基础模型训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模原始雷达数据训练通用Transformer模型，直接从4D雷达数据立方体中学习特征，而非依赖传统的有损点云表示。整体流程包括：1)使用便携式系统收集多模态数据(雷达、激光雷达、相机)；2)将原始I/Q数据通过4D FFT转换为4D数据立方体；3)将数据分割成补丁并添加位置编码；4)使用Transformer编码器-解码器架构处理数据；5)训练基础模型(3D占用分类)并扩展到其他任务；6)评估模型在不同场景的泛化能力和微调性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了目前最大的原始毫米波雷达数据集I/Q-1M(29小时/100万帧)；2)证明使用原始4D数据比传统点云表示效果更好，相当于10倍以上训练数据量；3)设计了通用雷达Transformer(GRT)架构，能生成高质量3D占用图和语义分割；4)系统研究了雷达Transformer的扩展规律。相比之前工作，本研究数据规模大得多(比最大公开数据集大8倍)，直接使用原始数据而非点云，首次系统将Transformer应用于雷达基础模型，并实现了3D而非仅2D的复杂任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建大规模原始毫米波雷达数据集并训练通用雷达Transformer，证明了数据规模对提升单芯片雷达感知能力的关键作用，为雷达领域的基础模型发展奠定了重要基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; mmWave radars are compact, inexpensive, and durable sensors that are robustto occlusions and work regardless of environmental conditions, such as weatherand darkness. However, this comes at the cost of poor angular resolution,especially for inexpensive single-chip radars, which are typically used inautomotive and indoor sensing applications. Although many have proposedlearning-based methods to mitigate this weakness, no standardized foundationalmodels or large datasets for the mmWave radar have emerged, and practitionershave largely trained task-specific models from scratch using relatively smalldatasets.  In this paper, we collect (to our knowledge) the largest available raw radardataset with 1M samples (29 hours) and train a foundational model for 4Dsingle-chip radar, which can predict 3D occupancy and semantic segmentationwith quality that is typically only possible with much higher resolutionsensors. We demonstrate that our Generalizable Radar Transformer (GRT)generalizes across diverse settings, can be fine-tuned for different tasks, andshows logarithmic data scaling of 20\% per $10\times$ data. We also runextensive ablations on common design decisions, and find that using raw radardata significantly outperforms widely-used lossy representations, equivalent toa $10\times$ increase in training data. Finally, we roughly estimate that$\approx$100M samples (3000 hours) of data are required to fully exploit thepotential of GRT.</description>
      <author>example@mail.com (Tianshu Huang, Akarsh Prabhakara, Chuhan Chen, Jay Karhade, Deva Ramanan, Matthew O'Toole, Anthony Rowe)</author>
      <guid isPermaLink="false">2509.12482v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Prompt Management in GitHub Repositories: A Call for Best Practices</title>
      <link>http://arxiv.org/abs/2509.12421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究对开源提示进行了实证分析，发现了管理挑战并提供了改进建议。&lt;h4&gt;背景&lt;/h4&gt;基础模型（如大型语言模型）的快速采用引发了promptware（使用自然语言提示构建的软件）的发展，有效管理提示变得至关重要但具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;调查提示管理实践和质量属性，以识别关键挑战并提供改进建议。&lt;h4&gt;方法&lt;/h4&gt;对来自92个GitHub仓库的24,800个开源提示进行实证分析。&lt;h4&gt;主要发现&lt;/h4&gt;提示格式化存在相当大的不一致性、内部和外部提示大量重复，以及频繁出现的可读性和拼写问题。&lt;h4&gt;结论&lt;/h4&gt;开发者需要采取措施提高开源提示的可用性和可维护性，以应对promptware生态系统中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;基础模型（如大型语言模型）的快速采用引发了promptware的发展，即使用自然语言提示构建的软件。有效管理提示，如组织和质量保证，虽然至关重要但具有挑战性。在本研究中，我们对来自92个GitHub仓库的24,800个开源提示进行了实证分析，以调查提示管理实践和质量属性。我们的发现揭示了关键挑战，如提示格式化存在相当大的不一致性、内部和外部提示大量重复，以及频繁出现的可读性和拼写问题。基于这些发现，我们为开发者提供了可操作的建议，以在不断发展的promptware生态系统中提高开源提示的可用性和可维护性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid adoption of foundation models (e.g., large language models) hasgiven rise to promptware, i.e., software built using natural language prompts.Effective management of prompts, such as organization and quality assurance, isessential yet challenging. In this study, we perform an empirical analysis of24,800 open-source prompts from 92 GitHub repositories to investigate promptmanagement practices and quality attributes. Our findings reveal criticalchallenges such as considerable inconsistencies in prompt formatting,substantial internal and external prompt duplication, and frequent readabilityand spelling issues. Based on these findings, we provide actionablerecommendations for developers to enhance the usability and maintainability ofopen-source prompts within the rapidly evolving promptware ecosystem.</description>
      <author>example@mail.com (Hao Li, Hicham Masri, Filipe R. Cogo, Abdul Ali Bangash, Bram Adams, Ahmed E. Hassan)</author>
      <guid isPermaLink="false">2509.12421v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking</title>
      <link>http://arxiv.org/abs/2509.12913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为T-SiamTPN的新型空中目标跟踪方法，通过引入时间感知的Siamese框架解决了现有跟踪器在处理尺度变化、动态背景、杂乱和频繁遮挡等问题时的局限性。该方法通过时间特征融合和基于注意力的交互显著提高了跟踪性能，同时保持计算效率，适合在资源受限的嵌入式设备上实时运行。&lt;h4&gt;背景&lt;/h4&gt;空中目标跟踪具有挑战性，因为存在尺度变化、动态背景、杂乱和频繁遮挡等问题。现有跟踪器大多强调空间线索，但忽略了时间依赖性，导致长期跟踪和遮挡情况下的鲁棒性有限。此外，基于相关的Siamese跟踪器受限于相关操作的线性本质，对复杂、非线性的外观变化效果不佳。&lt;h4&gt;目的&lt;/h4&gt;解决现有跟踪器的局限性，特别是时间依赖性建模不足的问题；提高跟踪器在长期跟踪和遮挡情况下的鲁棒性；处理复杂、非线性的外观变化。&lt;h4&gt;方法&lt;/h4&gt;提出T-SiamTPN，一个具有时间感知的Siamese跟踪框架；通过显式的时间建模扩展了SiamTPN架构；结合时间特征融合和基于注意力的交互，增强时间一致性并实现更丰富的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;T-SiamTPN相比基线有显著改进，性能与最先进的跟踪器相当；尽管增加了时间模块，T-SiamTPN仍保持计算效率；在资源受限的Jetson Nano上，跟踪器实时运行，达到7.1 FPS，适合嵌入式应用；与基线相比，T-SiamTPN将成功率提高了13.7%，精度提高了14.7%。&lt;h4&gt;结论&lt;/h4&gt;时间建模在Siamese跟踪框架中很重要；T-SiamTPN是空中目标跟踪的一个强大且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;空中目标跟踪由于尺度变化、动态背景、杂乱和频繁遮挡而仍然是一项具有挑战性的任务。虽然大多数现有跟踪器强调空间线索，但它们经常忽略时间依赖性，导致在长期跟踪和遮挡情况下鲁棒性有限。此外，基于相关的Siamese跟踪器本质上受限于相关操作的线性特性，使其对复杂、非线性的外观变化无效。为解决这些局限性，我们引入了T-SiamTPN，这是一个具有时间感知的Siamese跟踪框架，通过显式的时间建模扩展了SiamTPN架构。我们的方法结合了时间特征融合和基于注意力的交互，增强了时间一致性并实现了更丰富的特征表示。这些增强相比基线带来了显著改进，并实现了与最先进跟踪器相当的性能。关键的是，尽管增加了时间模块，T-SiamTPN仍保持了计算效率。在资源受限的Jetson Nano上部署时，跟踪器以7.1 FPS的帧率实时运行，展示了其适合实际嵌入式应用的适用性，而没有明显的运行时开销。实验结果突出了显著的增益：与基线相比，T-SiamTPN将成功率提高了13.7%，精度提高了14.7%。这些发现强调了时间建模在Siamese跟踪框架中的重要性，并将T-SiamTPN确立为空中目标跟踪的一个强大且高效的解决方案。代码可在以下网址获取：https://github.com/to/be/released&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerial object tracking remains a challenging task due to scale variations,dynamic backgrounds, clutter, and frequent occlusions. While most existingtrackers emphasize spatial cues, they often overlook temporal dependencies,resulting in limited robustness in long-term tracking and under occlusion.Furthermore, correlation-based Siamese trackers are inherently constrained bythe linear nature of correlation operations, making them ineffective againstcomplex, non-linear appearance changes. To address these limitations, weintroduce T-SiamTPN, a temporal-aware Siamese tracking framework that extendsthe SiamTPN architecture with explicit temporal modeling. Our approachincorporates temporal feature fusion and attention-based interactions,strengthening temporal consistency and enabling richer feature representations.These enhancements yield significant improvements over the baseline and achieveperformance competitive with state-of-the-art trackers. Crucially, despite theadded temporal modules, T-SiamTPN preserves computational efficiency. Deployedon the resource-constrained Jetson Nano, the tracker runs in real time at 7.1FPS, demonstrating its suitability for real-world embedded applications withoutnotable runtime overhead. Experimental results highlight substantial gains:compared to the baseline, T-SiamTPN improves success rate by 13.7% andprecision by 14.7%. These findings underscore the importance of temporalmodeling in Siamese tracking frameworks and establish T-SiamTPN as a strong andefficient solution for aerial object tracking. Code is available at:https://github.com/to/be/released</description>
      <author>example@mail.com (Hojat Ardi, Amir Jahanshahi, Ali Diba)</author>
      <guid isPermaLink="false">2509.12913v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>HistoryBankQA: Multilingual Temporal Question Answering on Historical Events</title>
      <link>http://arxiv.org/abs/2509.12720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HistoryBank，一个包含1000多万个历史事件的多语言数据库，以及一个全面的问答基准，用于评估大型语言模型在历史事件时间推理方面的能力。&lt;h4&gt;背景&lt;/h4&gt;时间推理对于自然语言处理任务至关重要，但现有的大型语言模型在时间推理方面的基准测试有限。现有的时间推理数据集规模有限，缺乏多语言覆盖，并且更关注当代事件而非历史事件。&lt;h4&gt;目的&lt;/h4&gt;解决现有时间推理数据集的局限性，提供一个大规模、多语言的历史事件数据库，并构建一个全面的问答基准来评估语言模型在时间推理任务上的表现。&lt;h4&gt;方法&lt;/h4&gt;从维基百科的时间线页面和文章信息框中提取了1000多万个历史事件，构建了一个包含10种语言的数据库。此外，还构建了一个包含6种时间问答推理任务的全面基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;评估了多种流行的语言模型（LLaMA-3-8B、Mistral-7B、Gemma-2-9b、Qwen3-8B、GPT4o）在这些任务上的性能。GPT4o在所有答案类型和语言中表现最佳；Gemma-2-9b优于其他小型语言模型。&lt;h4&gt;结论&lt;/h4&gt;这项工作旨在为推进对历史事件的多语言和时间感知自然语言理解提供全面资源。为了促进进一步研究，将在论文接受后公开代码和数据集。&lt;h4&gt;翻译&lt;/h4&gt;关于历史事件的时间推理是自然语言处理任务的关键技能，如事件提取、历史实体链接、时间问答、时间线总结、时间事件聚类和时间自然语言推理。然而，对大型语言模型时间推理能力的基准测试工作相当有限。现有的时间推理数据集在规模上有限，缺乏多语言覆盖，并且更关注当代事件。为了解决这些局限性，我们提出了HistoryBank，一个从维基百科时间线页面和文章信息框中提取的1000多万个历史事件的多语言数据库。我们的数据库在历史深度和语言广度上提供了前所未有的覆盖，包含10种语言。此外，我们构建了一个全面的跨所有语言的问答基准，用于时间推理。该基准涵盖了多样化的6种时间问答推理任务集，我们评估了一系列流行的语言模型（LLaMA-3-8B、Mistral-7B、Gemma-2-9b、Qwen3-8B、GPT4o）以评估它们在这些任务上的性能。正如预期的那样，GPT4o在所有答案类型和语言中表现最佳；Gemma-2优于其他小型语言模型。我们的工作旨在为推进对历史事件的多语言和时间感知自然语言理解提供全面资源。为了促进进一步研究，我们将在论文接受后公开我们的代码和数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning about historical events is a critical skill for NLP taskslike event extraction, historical entity linking, temporal question answering,timeline summarization, temporal event clustering and temporal natural languageinference. Yet efforts on benchmarking temporal reasoning capabilities of largelanguage models (LLMs) are rather limited. Existing temporal reasoning datasetsare limited in scale, lack multilingual coverage and focus more on contemporaryevents. To address these limitations, we present HistoryBank, a multilingualdatabase of 10M+ historical events extracted from Wikipedia timeline pages andarticle infoboxes. Our database provides unprecedented coverage in bothhistorical depth and linguistic breadth with 10 languages. Additionally, weconstruct a comprehensive question answering benchmark for temporal reasoningacross all languages. This benchmark covers a diverse set of 6 temporal QAreasoning tasks, and we evaluate a suite of popular language models(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess theirperformance on these tasks. As expected GPT4o performs best across all answertypes and languages; Gemma-2 outperforms the other small language models. Ourwork aims to provide a comprehensive resource for advancing multilingual andtemporally-aware natural language understanding of historical events. Tofacilitate further research, we will make our code and datasets publiclyavailable upon acceptance of this paper.</description>
      <author>example@mail.com (Biswadip Mandal, Anant Khandelwal, Manish Gupta)</author>
      <guid isPermaLink="false">2509.12720v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.12269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MT-DQN模型，结合了Transformer、时间图神经网络和深度Q网络，用于解决短视频环境中的用户行为预测和推荐策略优化问题。实验证明该模型在多个评估指标上均优于传统模型。&lt;h4&gt;背景&lt;/h4&gt;短视频环境中存在用户行为预测困难和推荐策略优化挑战的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效预测用户行为并优化推荐策略的模型，以应对短视频环境的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出MT-DQN模型，该模型集成了Transformer、时间图神经网络(TGNN)和深度Q网络(DQN)。&lt;h4&gt;主要发现&lt;/h4&gt;1) MT-DQN比传统连接模型(如Concat-Modal)表现更好，平均F1分数提高10.97%，平均NDCG@5提高8.3%；2) 与经典强化学习模型Vanilla-DQN相比，MT-DQN的MSE降低34.8%，MAE降低26.5%。&lt;h4&gt;结论&lt;/h4&gt;MT-DQN模型在短视频推荐领域具有显著优势，但面临计算成本高和在线推理延迟敏感等现实部署挑战，这些挑战将通过未来的架构优化来解决。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了MT-DQN模型，该模型集成了Transformer、时间图神经网络(TGNN)和深度Q网络(DQN)，以解决短视频环境中预测用户行为和优化推荐策略的挑战。实验证明，MT-DQN持续优于传统的连接模型，如Concat-Modal，平均F1分数提高了10.97%，平均NDCG@5提高了8.3%。与经典的强化学习模型Vanilla-DQN相比，MT-DQN的MSE降低了34.8%，MAE降低了26.5%。尽管如此，我们也认识到在现实场景中部署MT-DQN面临的挑战，如计算成本和在线推理时的延迟敏感性，这些问题将通过未来的架构优化来解决。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes the MT-DQN model, which integrates a Transformer,Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address thechallenges of predicting user behavior and optimizing recommendation strategiesin short-video environments. Experiments demonstrated that MT-DQN consistentlyoutperforms traditional concatenated models, such as Concat-Modal, achieving anaverage F1-score improvement of 10.97% and an average NDCG@5 improvement of8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQNreduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognizechallenges in deploying MT-DQN in real-world scenarios, such as itscomputational cost and latency sensitivity during online inference, which willbe addressed through future architectural optimization.</description>
      <author>example@mail.com (Jinmeiyang Wang, Jing Dong, Li Zhou)</author>
      <guid isPermaLink="false">2509.12269v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks</title>
      <link>http://arxiv.org/abs/2509.13266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为JANUS的双重约束隐蔽节点注入框架，通过局部和全局两个层面的优化，显著提高了图神经网络对抗攻击的隐蔽性和有效性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在各种应用中表现出色，但它们容易受到复杂的对抗攻击，特别是节点注入攻击。这些攻击的成功很大程度上依赖于它们的隐蔽性，即能够融入原始图并逃避检测的能力。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，这些方法往往依赖间接代理指标、忽略注入内容的基本特征或仅关注模仿局部结构，导致局部近视问题。&lt;h4&gt;方法&lt;/h4&gt;提出名为'Joint Alignment of Nodal and Universal Structures'(JANUS)的框架。在局部层面，引入局部特征流形对齐策略实现特征空间中的几何一致性；在全局层面，结合结构化潜在变量并最大化与生成结构的互信息，确保注入结构与原始图的语义模式一致。将注入攻击建模为顺序决策过程，通过强化学习代理进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在多个标准数据集上的实验表明，JANUS框架在攻击有效性和隐蔽性方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;JANUS框架通过同时考虑局部和全局结构，解决了现有节点注入攻击方法中的局部近视问题，实现了更隐蔽、更有效的攻击。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已在各种应用中展现出卓越的性能，但它们容易受到复杂的对抗攻击，特别是节点注入攻击。这类攻击的成功很大程度上依赖于其隐蔽性，即能够融入原始图并逃避检测的能力。然而，现有方法通常通过依赖间接代理指标来实现隐蔽性，缺乏对注入内容基本特征的考虑，或仅专注于模仿局部结构，这导致了局部近视问题。为了克服这些限制，我们提出了一种双重约束的隐蔽节点注入框架，称为'节点与通用结构联合对齐'(JANUS)。在局部层面，我们引入了局部特征流形对齐策略，以实现特征空间中的几何一致性。在全局层面，我们结合了结构化潜在变量，并最大化与生成结构的互信息，确保注入的结构与原始图的语义模式一致。我们将注入攻击建模为顺序决策过程，并通过强化学习代理进行优化。在多个标准数据集上的实验表明，JANUS框架在攻击有效性和隐蔽性方面显著优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable performance acrossvarious applications, yet they are vulnerable to sophisticated adversarialattacks, particularly node injection attacks. The success of such attacksheavily relies on their stealthiness, the ability to blend in with the originalgraph and evade detection. However, existing methods often achieve stealthinessby relying on indirect proxy metrics, lacking consideration for the fundamentalcharacteristics of the injected content, or focusing only on imitating localstructures, which leads to the problem of local myopia. To overcome theselimitations, we propose a dual-constraint stealthy node injection framework,called Joint Alignment of Nodal and Universal Structures (JANUS). At the locallevel, we introduce a local feature manifold alignment strategy to achievegeometric consistency in the feature space. At the global level, we incorporatestructured latent variables and maximize the mutual information with thegenerated structures, ensuring the injected structures are consistent with thesemantic patterns of the original graph. We model the injection attack as asequential decision process, which is optimized by a reinforcement learningagent. Experiments on multiple standard datasets demonstrate that the JANUSframework significantly outperforms existing methods in terms of both attackeffectiveness and stealthiness.</description>
      <author>example@mail.com (Jiahao Zhang, Xiaobing Pei, Zhaokun Zhong, Wenqiang Hao, Zhenghao Tang)</author>
      <guid isPermaLink="false">2509.13266v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges</title>
      <link>http://arxiv.org/abs/2509.13139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图异质性对消息传递图神经网络性能的影响，特别是图卷积网络等低通滤波器在异构图上的表现。通过添加自环和平行边修改图结构，观察图拉普拉斯矩阵特征值变化及GCN性能变化，建立了图频谱与低通滤波器性能之间的联系，提出了一种无需昂贵特征值分解即可评估图特性的方法。&lt;h4&gt;背景&lt;/h4&gt;图异质性对消息传递图神经网络性能构成了重大挑战。像图卷积网络这样的常见低通滤波器面临性能下降，这可以归因于来自不相似邻居节点的消息混合。低通滤波器在异构图上的性能仍需要深入分析。&lt;h4&gt;目的&lt;/h4&gt;研究图异质性对低通滤波器性能的影响，特别是GCN在异构图上的表现，并探索通过修改图结构（添加自环和平行边）来改善性能的可能性。&lt;h4&gt;方法&lt;/h4&gt;通过添加自环和平行边来更新异构图。观察图拉普拉斯矩阵的特征值如何随着自环和平行边数量的变化而变化。在添加自环或平行边的情况下，在各种基准异构网络上进行关于GCN性能的研究。&lt;h4&gt;主要发现&lt;/h4&gt;1) 随着自环数量的增加，图拉普拉斯矩阵的特征值减小；2) 随着平行边数量的增加，图拉普拉斯矩阵的特征值增大；3) GCN在添加自环和平行边后表现出性能增加或减少的趋势；4) 建立了图频谱与低通滤波器在异构图上性能趋势之间的联系；5) 图频谱描述了输入图的基本内在特性，如连通分量的存在、稀疏性、平均度、聚类结构等；6) 可以通过观察低通滤波器的性能趋势来评估图频谱和特性，而无需进行昂贵的特征值分解。&lt;h4&gt;结论&lt;/h4&gt;本研究工作能够通过观察低通滤波器的性能趋势来评估图频谱和特性，而不需要进行昂贵的特征值分解。理论基础也验证了添加自环和平行边对图频谱的影响。&lt;h4&gt;翻译&lt;/h4&gt;图异质性对消息传递图神经网络性能构成了严峻挑战。像图卷积网络这样的常见低通滤波器面临性能下降，这可以归因于来自不相似邻居节点的消息混合。低通滤波器在异构图上的性能仍需要深入分析。在此背景下，我们通过添加自环和平行边来更新异构图。我们观察到，随着自环数量的增加，图拉普拉斯矩阵的特征值减小；随着平行边数量的增加，图拉普拉斯矩阵的特征值增大。我们通过添加自环或平行边，在各种基准异构网络上进行了关于GCN性能的几项研究。研究表明，GCN在添加自环和平行边后表现出性能增加或减少的趋势。基于这些研究，我们建立了图频谱与低通滤波器在异构图上性能趋势之间的联系。图频谱描述了输入图的基本内在特性，如连通分量的存在、稀疏性、平均度、聚类结构等。我们的工作能够通过观察低通滤波器的性能趋势来无缝评估图频谱和特性，而无需进行昂贵的特征值分解。还讨论了理论基础，以验证添加自环和平行边对图频谱的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph heterophily poses a formidable challenge to the performance ofMessage-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filterslike Graph Convolutional Networks (GCNs) face performance degradation, whichcan be attributed to the blending of the messages from dissimilar neighboringnodes. The performance of the low-pass filters on heterophilic graphs stillrequires an in-depth analysis. In this context, we update the heterophilicgraphs by adding a number of self-loops and parallel edges. We observe thateigenvalues of the graph Laplacian decrease and increase respectively byincreasing the number of self-loops and parallel edges. We conduct severalstudies regarding the performance of GCN on various benchmark heterophilicnetworks by adding either self-loops or parallel edges. The studies reveal thatthe GCN exhibited either increasing or decreasing performance trends on addingself-loops and parallel edges. In light of the studies, we establishedconnections between the graph spectra and the performance trends of thelow-pass filters on the heterophilic graphs. The graph spectra characterize theessential intrinsic properties of the input graph like the presence ofconnected components, sparsity, average degree, cluster structures, etc. Ourwork is adept at seamlessly evaluating graph spectrum and properties byobserving the performance trends of the low-pass filters without pursuing thecostly eigenvalue decomposition. The theoretical foundations are also discussedto validate the impact of adding self-loops and parallel edges on the graphspectrum.</description>
      <author>example@mail.com (Kushal Bose, Swagatam Das)</author>
      <guid isPermaLink="false">2509.13139v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Curriculum Learning for Mesh-based simulations</title>
      <link>http://arxiv.org/abs/2509.13138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了一种从粗到细的课程学习方法，加速图神经网络在高分辨率非结构化网格上的训练过程，同时保持相当的准确性并减少训练时间。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为基于网格的计算流体动力学的强大替代模型，但在高分辨率非结构化网格上训练这些网络（包含数十万个节点）仍然极其昂贵。&lt;h4&gt;目的&lt;/h4&gt;研究一种从粗到细的课程学习方法，以加速图神经网络在高分辨率非结构化网格上的训练过程。&lt;h4&gt;方法&lt;/h4&gt;首先在非常粗糙的网格上训练，然后逐步引入中等和高分辨率（最高可达30万个节点）的网格。与多尺度GNN架构不同，模型本身保持不变，只有训练数据的保真度随时间变化。&lt;h4&gt;主要发现&lt;/h4&gt;使用这种方法可以实现相当的泛化准确性，同时将总墙上时钟时间减少高达50%；在模型缺乏学习底层物理能力的情况下，课程学习可以帮助模型突破性能瓶颈。&lt;h4&gt;结论&lt;/h4&gt;从粗到细的课程学习可以显著加速图神经网络在高分辨率非结构化网格上的训练，同时保持相当的准确性，并且在某些情况下还能帮助模型突破性能瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为基于网格的计算流体动力学的强大替代模型，但在高分辨率非结构化网格上训练它们（包含数十万个节点）仍然极其昂贵。我们研究了一种从粗到细的课程学习，通过首先在非常粗糙的网格上训练，然后逐步引入中等和高分辨率（最高达30万个节点）来加速收敛。与多尺度GNN架构不同，模型本身保持不变；只有训练数据的保真度随时间变化。我们在减少高达50%的总墙上时钟时间的同时，实现了相当的泛化准确性。此外，在我们的模型缺乏学习底层物理能力的数据集上，使用课程学习能够使其突破性能瓶颈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as powerful surrogates formesh-based computational fluid dynamics (CFD), but training them onhigh-resolution unstructured meshes with hundreds of thousands of nodes remainsprohibitively expensive. We study a \emph{coarse-to-fine curriculum} thataccelerates convergence by first training on very coarse meshes and thenprogressively introducing medium and high resolutions (up to \(3\times10^5\)nodes). Unlike multiscale GNN architectures, the model itself is unchanged;only the fidelity of the training data varies over time. We achieve comparablegeneralization accuracy while reducing total wall-clock time by up to 50\%.Furthermore, on datasets where our model lacks the capacity to learn theunderlying physics, using curriculum learning enables it to break throughplateaus.</description>
      <author>example@mail.com (Paul Garnier, Vincent Lannelongue, Elie Hachem)</author>
      <guid isPermaLink="false">2509.13138v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories</title>
      <link>http://arxiv.org/abs/2509.12953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种结合神经常微分方程、图神经网络和神经过程的概率框架，用于从稀疏观测中建模心脏运动的时空动力学特性。&lt;h4&gt;背景&lt;/h4&gt;心脏运动的建模需要考虑时空结构、稀疏观测和不确定性，传统方法可能难以捕捉这些复杂特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从稀疏观测中重建和预测心脏运动的统一模型，同时捕捉不确定性、时间连续性和解剖结构。&lt;h4&gt;方法&lt;/h4&gt;将动态系统表示为时空多路复用图，使用GNN参数化的向量场建模潜在轨迹，并通过神经过程推断初始状态和控制变量的分布。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够在合成动态系统和真实心脏成像数据集上准确重建轨迹并外推未来周期，在ACDC分类任务上达到99%准确率，在UK Biobank中检测心房颤动达到67%准确率。&lt;h4&gt;结论&lt;/h4&gt;该框架为心脏运动分析提供了灵活的方法，并为结构化生物医学时空时间序列数据中的基于图的学习奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于从稀疏观测中建模结构化时空动力学的概率框架，专注于心脏运动。我们的方法将神经常微分方程、图神经网络和神经过程整合到一个统一模型中，该模型捕捉不确定性、时间连续性和解剖结构。我们将动态系统表示为时空多路复用图，并使用GNN参数化的向量场建模其潜在轨迹。给定节点和边缘级别的稀疏上下文观测，模型推断潜在初始状态和控制变量的分布，从而实现轨迹的内插和外推。我们在三个合成动态系统和两个真实世界的心脏成像数据集上验证了该方法，展示了精确重建、外推和疾病分类能力。模型能从单个观测周期准确重建轨迹并外推未来心脏周期。在ACDC分类任务上达到最先进的结果，并在UK Biobank受试者中检测心房颤动，具有竞争力的性能。这项工作为分析心脏运动提供了灵活的方法，并为结构化生物医学时空时间序列数据中的基于图的学习奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a probabilistic framework for modeling structured spatiotemporaldynamics from sparse observations, focusing on cardiac motion. Our approachintegrates neural ordinary differential equations (NODEs), graph neuralnetworks (GNNs), and neural processes into a unified model that capturesuncertainty, temporal continuity, and anatomical structure. We representdynamic systems as spatiotemporal multiplex graphs and model their latenttrajectories using a GNN-parameterized vector field. Given the sparse contextobservations at node and edge levels, the model infers a distribution overlatent initial states and control variables, enabling both interpolation andextrapolation of trajectories. We validate the method on three syntheticdynamical systems (coupled pendulum, Lorenz attractor, and Kuramotooscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UKBiobank (N=526) - demonstrating accurate reconstruction, extrapolation, anddisease classification capabilities. The model accurately reconstructstrajectories and extrapolates future cardiac cycles from a single observedcycle. It achieves state-of-the-art results on the ACDC classification task (upto 99% accuracy), and detects atrial fibrillation in UK Biobank subjects withcompetitive performance (up to 67% accuracy). This work introduces a flexibleapproach for analyzing cardiac motion and offers a foundation for graph-basedlearning in structured biomedical spatiotemporal time-series data.</description>
      <author>example@mail.com (Jaume Banus, Augustin C. Ogier, Roger Hullin, Philippe Meyer, Ruud B. van Heeswijk, Jonas Richiardi)</author>
      <guid isPermaLink="false">2509.12953v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Explicit Multimodal Graph Modeling for Human-Object Interaction Detection</title>
      <link>http://arxiv.org/abs/2509.12554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MGNM方法，利用图神经网络的关系结构增强人类-物体交互检测性能&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的方法已成为人类-物体交互检测的主流方法，但该架构没有明确建模HOI检测中固有的关系结构，阻碍了交互识别&lt;h4&gt;目的&lt;/h4&gt;提出MGNM方法，利用基于GNN的关系结构增强HOI检测&lt;h4&gt;方法&lt;/h4&gt;设计多模态图网络框架，在四阶段图结构中明确建模HOI任务；引入多级特征交互机制，利用多级视觉和语言特征增强人类-物体对之间的信息传播&lt;h4&gt;主要发现&lt;/h4&gt;在HICO-DET和V-COCO基准测试上取得最先进性能；与先进物体检测器集成时展示显著性能提升，并在稀有和非稀有类别间保持平衡&lt;h4&gt;结论&lt;/h4&gt;MGNM方法能有效提升人类-物体交互检测的性能&lt;h4&gt;翻译&lt;/h4&gt;基于Transformer的方法最近已成为人类-物体交互检测的主流方法。然而，Transformer架构没有明确建模HOI检测中固有的关系结构，这阻碍了交互的识别。相比之下，图神经网络(GNNs)本质上更适合这项任务，因为它们明确建模了人类-物体对之间的关系。因此，在本文中，我们提出了MGNM(多模态图网络建模)，利用基于GNN的关系结构来增强HOI检测。具体来说，我们设计了一个多模态图网络框架，在四阶段图结构中明确建模HOI任务。此外，我们在图网络中引入了多级特征交互机制。该机制利用多级视觉和语言特征来增强人类-物体对之间的信息传播。因此，我们提出的MGNM在两个广泛使用的基准测试上取得了最先进的性能：HICO-DET和V-COCO。此外，当与更先进的物体检测器集成时，我们的方法展示了显著的性能提升，并在稀有和非稀有类别之间保持了有效的平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformer-based methods have recently become the prevailing approach forHuman-Object Interaction (HOI) detection. However, the Transformer architecturedoes not explicitly model the relational structures inherent in HOI detection,which impedes the recognition of interactions. In contrast, Graph NeuralNetworks (GNNs) are inherently better suited for this task, as they explicitlymodel the relationships between human-object pairs. Therefore, in this paper,we propose \textbf{M}ultimodal \textbf{G}raph \textbf{N}etwork\textbf{M}odeling (MGNM) that leverages GNN-based relational structures toenhance HOI detection. Specifically, we design a multimodal graph networkframework that explicitly models the HOI task in a four-stage graph structure.Furthermore, we introduce a multi-level feature interaction mechanism withinour graph network. This mechanism leverages multi-level vision and languagefeatures to enhance information propagation across human-object pairs.Consequently, our proposed MGNM achieves state-of-the-art performance on twowidely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with amore advanced object detector, our method demonstrates a significantperformance gain and maintains an effective balance between rare and non-rareclasses.</description>
      <author>example@mail.com (Wenxuan Ji, Haichao Shi, Xiao-Yu zhang)</author>
      <guid isPermaLink="false">2509.12554v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Graph Homophily Booster: Rethinking the Role of Discrete Features on Heterophilic Graphs</title>
      <link>http://arxiv.org/abs/2509.12530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GRAPHITE的新框架，通过图转换直接增加图的同质性来解决图神经网络在异质性图上的表现不佳问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是建模图结构数据的强大工具，但现有GNN在处理异质性图(连接节点具有不同特征或标签)时表现不佳。现有方法主要集中在架构设计上，没有直接针对异质性问题的根本原因，甚至在某些异质性数据集上表现不如简单的多层感知机。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新方法来解决图异质性问题，超越架构设计的限制，直接通过图转换增加图的同质性。&lt;h4&gt;方法&lt;/h4&gt;提出名为GRAPHITE的框架，基于同质性的精确定义，创建特征节点来促进具有相似特征的节点之间的同质性消息传递，直接将异质性图转换为更具同质性的图。&lt;h4&gt;主要发现&lt;/h4&gt;理论和实证表明GRAPHITE显著增加了原始异质性图的同质性，同时仅略微增加图的大小；在具有挑战性的数据集上，GRAPHITE在异质性图上显著优于最先进方法，在同质性图上实现了与最先进方法相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;GRAPHITE是首个明确转换图以直接提高图同质性的方法，为解决图异质性问题提供了新范式。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为建模图结构数据的强大工具。然而，现有GNN通常难以处理异质性图，其中连接的节点往往具有不同的特征或标签。虽然已提出多种方法来解决这一挑战，但它们主要关注架构设计，而没有直接针对异质性问题的根本原因。这些方法在具有挑战性的异质性数据集上的表现甚至比最简单的多层感知机还差。例如，我们的实验显示，21种最新的GNN在Actor数据集上仍然落后于MLP。这一关键挑战需要一种超越架构设计的创新方法来解决图异质性。为了弥合这一差距，我们提出并研究了一种新的、未被探索的范式：通过精心设计的图转换直接增加图的同质性。在这项工作中，我们提出了一个简单而有效的框架GRAPHITE来解决图异质性问题。据我们所知，这项工作是首个明确转换图以直接提高图同质性的方法。源于同质性的精确定义，我们提出的GRAPHITE创建特征节点，以促进具有相似特征的节点之间的同质性消息传递。此外，我们从理论和实证上都证明，我们提出的GRAPHITE显著增加了原始异质性图的同质性，同时仅略微增加了图的大小。在具有挑战性的数据集上的大量实验表明，我们提出的GRAPHITE在异质性图上显著优于最先进的方法，同时在同质性图上实现了与最先进方法相当的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as a powerful tool for modelinggraph-structured data. However, existing GNNs often struggle with heterophilicgraphs, where connected nodes tend to have dissimilar features or labels. Whilenumerous methods have been proposed to address this challenge, they primarilyfocus on architectural designs without directly targeting the root cause of theheterophily problem. These approaches still perform even worse than thesimplest MLPs on challenging heterophilic datasets. For instance, ourexperiments show that 21 latest GNNs still fall behind the MLP on the Actordataset. This critical challenge calls for an innovative approach to addressinggraph heterophily beyond architectural designs. To bridge this gap, we proposeand study a new and unexplored paradigm: directly increasing the graphhomophily via a carefully designed graph transformation. In this work, wepresent a simple yet effective framework called GRAPHITE to address graphheterophily. To the best of our knowledge, this work is the first method thatexplicitly transforms the graph to directly improve the graph homophily.Stemmed from the exact definition of homophily, our proposed GRAPHITE createsfeature nodes to facilitate homophilic message passing between nodes that sharesimilar features. Furthermore, we both theoretically and empirically show thatour proposed GRAPHITE significantly increases the homophily of originallyheterophilic graphs, with only a slight increase in the graph size. Extensiveexperiments on challenging datasets demonstrate that our proposed GRAPHITEsignificantly outperforms state-of-the-art methods on heterophilic graphs whileachieving comparable accuracy with state-of-the-art methods on homophilicgraphs.</description>
      <author>example@mail.com (Ruizhong Qiu, Ting-Wei Li, Gaotang Li, Hanghang Tong)</author>
      <guid isPermaLink="false">2509.12530v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions</title>
      <link>http://arxiv.org/abs/2509.12277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GraphDerm是一种融合图像、毫米级校准和患者元数据的群体图框架，用于多类皮肤镜分类，显著提高了黑色素瘤分诊的准确性。&lt;h4&gt;背景&lt;/h4&gt;皮肤镜辅助黑色素瘤分诊，但仅基于图像的AI通常忽略患者元数据（年龄、性别、部位）和进行几何分析所需的物理尺度信息。&lt;h4&gt;目的&lt;/h4&gt;开发GraphDerm，一个融合图像、毫米级校准和元数据的多类皮肤镜分类的群体图框架，首次将图神经网络(GNNs)应用于ISIC规模的皮肤镜分析。&lt;h4&gt;方法&lt;/h4&gt;整合ISIC 2018/2019数据集，合成带有精确掩膜的标尺嵌入图像，训练U-Nets进行病变和标尺分割，通过1D-CNN回归每毫米像素数，计算病变的真实尺度描述符，使用EfficientNet-B3作为节点特征，编码元数据/几何相似性作为边，采用谱GNN进行半监督分类。&lt;h4&gt;主要发现&lt;/h4&gt;标尺和病变分割达到Dice 0.904和0.908，尺度回归达到MAE 1.5像素，图分类达到AUC 0.9812，使用约25%边的阈值变体保留AUC 0.9788，远高于图像基线的0.9440，每类AUC通常在0.97-0.99范围内。&lt;h4&gt;结论&lt;/h4&gt;将校准尺度、病变几何形状和元数据统一到群体图中，比仅基于图像的流程有显著提升；更稀疏的图保留了接近最优的准确性，表明高效部署的可能性；具有尺度感知、基于图的AI是皮肤镜决策支持的 promising 方向。&lt;h4&gt;翻译&lt;/h4&gt;引言。皮肤镜辅助黑色素瘤分诊，但仅基于图像的AI通常忽略患者元数据（年龄、性别、部位）和进行几何分析所需的物理尺度。我们提出了GraphDerm，一个融合图像、毫米级校准和元数据的多类皮肤镜分类的群体图框架，据我们所知，这是首次将图神经网络应用于ISIC规模的皮肤镜。方法。我们整理了ISIC 2018/2019，合成了带有精确掩膜的标尺嵌入图像，并训练U-Nets(SE-ResNet-18)进行病变和标尺分割。通过轻量级1D-CNN从标尺掩膜两点相关性回归每毫米像素数。从病变掩膜我们计算真实尺度描述符（面积、周长、回转半径）。节点特征使用EfficientNet-B3；边编码元数据/几何相似性（全权重或阈值化）。谱GNN执行半监督节点分类；图像-only ANN作为基线。结果。标尺和病变分割达到Dice 0.904和0.908；尺度回归达到MAE 1.5像素（RMSE 6.6）。图达到AUC 0.9812，阈值变体使用约25%的边保留AUC 0.9788（对比图像基线的0.9440）；每类AUC通常在0.97-0.99范围内。结论。在ISIC-2019上，将校准尺度、病变几何形状和元数据统一到群体图中，比仅基于图像的流程有显著提升。更稀疏的图保留了接近最优的准确性，表明高效部署的可能性。具有尺度感知、基于图的AI是皮肤镜决策支持的 promising 方向；未来工作将改进学习的边语义并在更广泛的精选基准上评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Introduction. Dermoscopy aids melanoma triage, yet image-only AI oftenignores patient metadata (age, sex, site) and the physical scale needed forgeometric analysis. We present GraphDerm, a population-graph framework thatfuses imaging, millimeter-scale calibration, and metadata for multiclassdermoscopic classification, to the best of our knowledge the first ISIC-scaleapplication of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019,synthesize ruler-embedded images with exact masks, and train U-Nets(SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter areregressed from the ruler-mask two-point correlation via a lightweight 1D-CNN.From lesion masks we compute real-scale descriptors (area, perimeter, radius ofgyration). Node features use EfficientNet-B3; edges encode metadata/geometrysimilarity (fully weighted or thresholded). A spectral GNN performssemi-supervised node classification; an image-only ANN is the baseline.Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scaleregression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with athresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata ina population graph yields substantial gains over image-only pipelines onISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficientdeployment. Scale-aware, graph-based AI is a promising direction fordermoscopic decision support; future work will refine learned edge semanticsand evaluate on broader curated benchmarks.</description>
      <author>example@mail.com (Mehdi Yousefzadeh, Parsa Esfahanian, Sara Rashidifar, Hossein Salahshoor Gavalan, Negar Sadat Rafiee Tabatabaee, Saeid Gorgin, Dara Rahmati, Maryam Daneshpazhooh)</author>
      <guid isPermaLink="false">2509.12277v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Graph Tokenization for Behavior-Aware Generative Next POI Recommendation</title>
      <link>http://arxiv.org/abs/2509.12350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了KGTB方法，通过知识图谱标记化和多行为学习来解决现有POI推荐方法中信息损失和对用户移动理解不足的问题。&lt;h4&gt;背景&lt;/h4&gt;生成范式，特别是由大型语言模型(LLMs)驱动的，已成为解决下一个兴趣点(POI)推荐的新方案。现有研究通常采用两阶段流程，首先使用标记化器将POI转换为可由LLM处理的离散标识符，然后进行POI行为预测任务来指令微调LLM以实现下一个POI推荐。&lt;h4&gt;目的&lt;/h4&gt;解决现有POI推荐方法的两个局限性：(1)现有标记化器难以编码推荐数据中的异构信号，存在信息损失问题；(2)之前的指令微调任务只关注用户的POI访问行为，而忽略其他行为类型，导致对移动性的理解不足。&lt;h4&gt;方法&lt;/h4&gt;提出KGTB方法，具体包括：(1)将推荐数据组织为知识图谱(KG)格式，保留异构信息；(2)开发基于KG的标记化器，将每个节点量化为独立的结构ID，由KG结构监督，减少异构信息损失；(3)提出多行为学习，引入多种特定行为的预测任务（如POI、类别和区域访问行为）来微调LLM。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实城市数据集上的实验表明，KGTB具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;KGTB通过知识图谱标记化和多行为学习有效解决了现有POI推荐方法中的信息损失和对用户移动理解不足的问题，提升了推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;生成范式，特别是由大型语言模型(LLMs)驱动的，已成为解决下一个兴趣点(POI)推荐的新方案。开创性研究通常采用两阶段流程，首先使用标记化器将POI转换为可由LLM处理的离散标识符，然后进行POI行为预测任务来指令微调LLM以实现下一个POI推荐。尽管取得了显著进展，它们仍面临两个局限性：(1)现有标记化器难以编码推荐数据中的异构信号，存在信息损失问题；(2)之前的指令微调任务只关注用户的POI访问行为而忽略其他行为类型，导致对移动性的理解不足。为解决这些局限性，我们提出了KGTB（用于行为感知生成式下一个POI推荐的知识图谱标记化）。具体而言，KGTB将推荐数据组织为知识图谱(KG)格式，其结构可以无缝保留异构信息。然后，开发了一个基于KG的标记化器，将每个节点量化为独立的结构ID。此过程由KG结构监督，从而减少异构信息的损失。使用生成的ID，KGTB提出了多行为学习，为LLM微调引入多种特定行为的预测任务，例如POI、类别和区域访问行为。在这些行为任务上的学习为LLMs提供了对目标POI访问行为的全面洞察。在四个真实城市数据集上的实验证明了KGTB的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative paradigm, especially powered by Large Language Models (LLMs), hasemerged as a new solution to the next point-of-interest (POI) recommendation.Pioneering studies usually adopt a two-stage pipeline, starting with atokenizer converting POIs into discrete identifiers that can be processed byLLMs, followed by POI behavior prediction tasks to instruction-tune LLM fornext POI recommendation. Despite of remarkable progress, they still face twolimitations: (1) existing tokenizers struggle to encode heterogeneous signalsin the recommendation data, suffering from information loss issue, and (2)previous instruction-tuning tasks only focus on users' POI visit behavior whileignore other behavior types, resulting in insufficient understanding ofmobility. To address these limitations, we propose KGTB (Knowledge GraphTokenization for Behavior-aware generative next POI recommendation).Specifically, KGTB organizes the recommendation data in a knowledge graph (KG)format, of which the structure can seamlessly preserve the heterogeneousinformation. Then, a KG-based tokenizer is developed to quantize each node intoan individual structural ID. This process is supervised by the KG's structure,thus reducing the loss of heterogeneous information. Using generated IDs, KGTBproposes multi-behavior learning that introduces multiple behavior-specificprediction tasks for LLM fine-tuning, e.g., POI, category, and region visitbehaviors. Learning on these behavior tasks provides LLMs with comprehensiveinsights on the target POI visit behavior. Experiments on four real-world citydatasets demonstrate the superior performance of KGTB.</description>
      <author>example@mail.com (Ke Sun, Mayi Xu)</author>
      <guid isPermaLink="false">2509.12350v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive timbre representations for musical instrument and synthesizer retrieval</title>
      <link>http://arxiv.org/abs/2509.13285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于乐器检索的对比学习框架，能够使用单一模型直接查询乐器数据库，适用于单乐器和多乐器声音。&lt;h4&gt;背景&lt;/h4&gt;从音频混合物中高效检索特定乐器音色在数字音乐制作中仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理单乐器和多乐器声音的统一检索模型，解决现有音频数据增强方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出技术来为虚拟乐器（如采样器和合成器）生成逼真的正/负声音对，构建对比学习框架。&lt;h4&gt;主要发现&lt;/h4&gt;在单乐器检索实验中，对比学习方法与基于分类预训练的先前工作具有竞争力；在多乐器检索实验中，所提出的框架表现更优，对三种乐器混合物实现了81.7%的top-1准确率和95.7%的top-5准确率。&lt;h4&gt;结论&lt;/h4&gt;该对比学习框架在乐器检索任务中表现出色，特别是在多乐器检索方面优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;从音频混合物中高效检索特定乐器音色在数字音乐制作中仍然是一个挑战。本文介绍了一种用于乐器检索的对比学习框架，能够使用单一模型直接查询乐器数据库，适用于单乐器和多乐器声音。我们提出了为虚拟乐器（如采样器和合成器）生成逼真的正/负声音对的技术，解决了常见音频数据增强方法的局限性。第一个实验专注于从3,884种乐器的数据集中检索乐器，使用单乐器音频作为输入。对比方法与基于分类预训练的先前工作具有竞争力。第二个实验考虑使用乐器混合作为音频输入进行多乐器检索。在这种情况下，所提出的对比框架优于相关工作，对三种乐器混合物实现了81.7%的top-1准确率和95.7%的top-5准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficiently retrieving specific instrument timbres from audio mixturesremains a challenge in digital music production. This paper introduces acontrastive learning framework for musical instrument retrieval, enablingdirect querying of instrument databases using a single model for both single-and multi-instrument sounds. We propose techniques to generate realisticpositive/negative pairs of sounds for virtual musical instruments, such assamplers and synthesizers, addressing limitations in common audio dataaugmentation methods.  The first experiment focuses on instrument retrieval from a dataset of 3,884instruments, using single-instrument audio as input. Contrastive approaches arecompetitive with previous works based on classification pre-training. Thesecond experiment considers multi-instrument retrieval with a mixture ofinstruments as audio input. In this case, the proposed contrastive frameworkoutperforms related works, achieving 81.7\% top-1 and 95.7\% top-5 accuraciesfor three-instrument mixtures.</description>
      <author>example@mail.com (Gwendal Le Vaillant, Yannick Molle)</author>
      <guid isPermaLink="false">2509.13285v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Flow-Based Fragment Identification via Binding Site-Specific Latent Representations</title>
      <link>http://arxiv.org/abs/2509.13216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种蛋白质片段编码器，采用对比学习方法将分子片段和蛋白质表面映射到共享潜在空间，并提出LatentFrag方法用于虚拟筛选和生成设计，有效提高了基于片段的药物设计效率。&lt;h4&gt;背景&lt;/h4&gt;基于片段的药物设计是一种有前景的策略，利用小化学分子的结合来有效指导药物发现。然而，片段识别的初始步骤具有挑战性，因为片段通常结合较弱且非特异性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效识别和设计蛋白质-片段相互作用的方法，克服片段结合弱和非特异性的挑战，提高药物发现的效率。&lt;h4&gt;方法&lt;/h4&gt;研究人员开发了一个蛋白质片段编码器，采用对比学习方法将分子片段和蛋白质表面映射到共享的潜在空间。该方法能够捕获相互作用相关特征，并允许进行虚拟筛选以及生成设计。提出的LatentFrag方法根据蛋白质表面生成片段嵌入和位置，并在构造上保持化学真实性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 表达性片段和蛋白质表示能够以高灵敏度定位蛋白质-片段相互作用位点；2. 从学习的潜在片段嵌入分布中采样时，观察到最先进的片段回收率；3. 生成方法在计算成本仅为常用方法的一小部分的情况下，表现优于这些方法；4. 该方法为片段命中发现提供了有价值的起点。&lt;h4&gt;结论&lt;/h4&gt;这些方法共同推进了片段识别的发展，并为基于片段的药物发现提供了有价值的工具。研究人员进一步展示了LatentFrag的实际效用，并将工作流程扩展到完整的配体设计任务。&lt;h4&gt;翻译&lt;/h4&gt;基于片段的药物设计是一种有前景的策略，利用小化学分子的结合来有效指导药物发现。片段识别的初始步骤仍然具有挑战性，因为片段通常结合较弱且非特异性。我们开发了一个蛋白质片段编码器，它依赖于对比学习方法，将分子片段和蛋白质表面映射到共享的潜在空间。该编码器捕获相互作用相关特征，并允许使用我们的新方法LatentFrag进行虚拟筛选和生成设计。在LatentFrag中，根据蛋白质表面生成片段嵌入和位置，并在构造上保持化学真实性。我们表达性的片段和蛋白质表示能够以高灵敏度定位蛋白质-片段相互作用位点，并且当我们从学习的潜在片段嵌入分布中采样时，观察到最先进的片段回收率。我们的生成方法在计算成本仅为常用方法（如虚拟筛选）的一小部分的情况下，表现优于这些方法，为片段命中发现提供了有价值的起点。我们进一步展示了LatentFrag的实际效用，并将工作流程扩展到完整的配体设计任务。这些方法共同推进了片段识别的发展，并为基于片段的药物发现提供了有价值的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fragment-based drug design is a promising strategy leveraging the binding ofsmall chemical moieties that can efficiently guide drug discovery. The initialstep of fragment identification remains challenging, as fragments often bindweakly and non-specifically. We developed a protein-fragment encoder thatrelies on a contrastive learning approach to map both molecular fragments andprotein surfaces in a shared latent space. The encoder capturesinteraction-relevant features and allows to perform virtual screening as wellas generative design with our new method LatentFrag. In LatentFrag, fragmentembeddings and positions are generated conditioned on the protein surface whilebeing chemically realistic by construction. Our expressive fragment and proteinrepresentations allow location of protein-fragment interaction sites with highsensitivity and we observe state-of-the-art fragment recovery rates whensampling from the learned distribution of latent fragment embeddings. Ourgenerative method outperforms common methods such as virtual screening at afraction of its computational cost providing a valuable starting point forfragment hit discovery. We further show the practical utility of LatentFrag andextend the workflow to full ligand design tasks. Together, these approachescontribute to advancing fragment identification and provide valuable tools forfragment-based drug discovery.</description>
      <author>example@mail.com (Rebecca Manuela Neeser, Ilia Igashov, Arne Schneuing, Michael Bronstein, Philippe Schwaller, Bruno Correia)</author>
      <guid isPermaLink="false">2509.13216v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling</title>
      <link>http://arxiv.org/abs/2509.13084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpeted in Knowledge-Based Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于双网络架构的新型半监督3D医学图像分割框架，通过交叉一致性增强模块和动态加权策略解决伪标签噪声问题，并利用自监督对比学习减少预测不确定性。实验表明该方法在多个数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;监督医学图像分割模型虽然性能优异，但在实际应用中依赖大量标注数据不切实际。半监督学习方法通过伪标签生成利用未标注数据来缓解这一挑战，但现有方法仍受伪标签噪声和特征空间监督不足的影响。&lt;h4&gt;目的&lt;/h4&gt;解决现有半监督分割方法中存在的伪标签噪声和特征空间监督不足的问题，提出一种新颖的基于双网络架构的半监督3D医学图像分割框架。&lt;h4&gt;方法&lt;/h4&gt;提出交叉一致性增强模块使用交叉伪标签和熵过滤监督减少噪声伪标签；设计动态加权策略通过不确定性感知机制调整伪标签贡献；使用自监督对比学习机制区分可信和不确定预测，将不确定体素特征与可靠类别原型对齐以减少预测不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在Left Atrial、NIH Pancreas和BraTS-2019三个3D分割数据集上进行实验，结果表明所提方法在各种设置下均优于最先进方法（如在Left Atrial数据集上使用10%标注数据时达到89.95%的Dice分数），消融实验验证了各模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的新型半监督3D医学图像分割框架有效解决了伪标签噪声和特征空间监督不足的问题，实验结果证实了其在多种数据集和设置下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;尽管监督医学图像分割模型表现出色，但在实际情况中依赖大量标注数据是不切实际的。半监督学习方法旨在通过伪标签生成利用未标注数据来缓解这一挑战。然而，现有的半监督分割方法仍然受到噪声伪标签和特征空间内监督不足的影响。为了解决这些挑战，本文提出了一种基于双网络架构的新型半监督3D医学图像分割框架。具体来说，我们研究了一个交叉一致性增强模块，使用交叉伪标签和熵过滤监督来减少噪声伪标签，同时我们设计了一种动态加权策略，使用不确定性感知机制（即Kullback-Leibler散度）来调整伪标签的贡献。此外，我们使用自监督对比学习机制，通过有效区分可信和不确定的预测，将不确定的体素特征与可靠的类别原型对齐，从而减少预测不确定性。在三个3D分割数据集Left Atrial、NIH Pancreas和BraTS-2019上进行了大量实验。与最先进的方法相比，所提出的方法在各种设置下都表现出优越的性能（例如，在Left Atrial数据集上使用10%标注数据时达到89.95%的Dice分数）。此外，通过消融实验进一步验证了所提出模块的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决半监督医学图像分割中的两个关键挑战：噪声伪标签问题和特征空间监督不足。在现实中，医学图像分割需要大量像素级标注数据，而这些数据需要专家临床知识，获取成本高昂且耗时。半监督学习可以利用大量未标注数据缓解这一问题，但噪声伪标签会降低分割性能，而特征空间监督不足则限制了模型学习判别性表示的能力，这些问题限制了半监督方法在医学图像分割中的实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有半监督医学图像分割方法的局限性进行设计：发现交叉伪监督方法计算开销大，不确定性估计方法成本高，基于原型的方法往往单独应用效果有限。因此，作者借鉴了知识蒸馏框架但创新性地使用两个并行学生网络而非传统教师-学生架构。他们结合了交叉伪监督(CPS)和熵过滤监督(EFS)形成CCE模块，并引入不确定性感知机制和原型引导的对比学习，从而有效解决了现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用双网络架构相互验证和改进伪标签质量，通过熵过滤减少噪声影响，利用不确定性机制动态调整伪标签权重，并通过对比学习增强特征空间判别性。整体流程包括：1)构建两个并行的3D编码器-解码器子网络；2)对标记数据使用标准损失函数并加入一致性正则化；3)对未标记数据实施交叉伪监督、熵过滤、不确定性加权调整和原型引导的对比学习；4)整合监督损失、对比损失和半监督损失进行模型优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出交叉一致性增强(CCE)模块协同整合交叉伪监督和熵过滤监督；2)设计不确定性感知的加权机制动态调整伪标签贡献；3)整合基于原型的对比学习策略对齐不确定特征与可靠类原型。相比之前工作，该方法使用双并行学生网络而非传统教师-学生架构，将不确定性估计与原型引导相结合实现高效噪声识别，通过CCE模块有机结合多种策略，并使用自适应熵阈值而非固定阈值，在不同数据集上表现出更强的适应性和性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于双网络架构和不确定性引导伪标签的半监督医学图像分割方法，通过交叉一致性增强模块和原型引导的对比学习有效解决了噪声伪标签和特征空间监督不足的问题，在有限标注数据条件下实现了最先进的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.knosys.2025.114454&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the remarkable performance of supervised medical image segmentationmodels, relying on a large amount of labeled data is impractical in real-worldsituations. Semi-supervised learning approaches aim to alleviate this challengeusing unlabeled data through pseudo-label generation. Yet, existingsemi-supervised segmentation methods still suffer from noisy pseudo-labels andinsufficient supervision within the feature space. To solve these challenges,this paper proposes a novel semi-supervised 3D medical image segmentationframework based on a dual-network architecture. Specifically, we investigate aCross Consistency Enhancement module using both cross pseudo andentropy-filtered supervision to reduce the noisy pseudo-labels, while we designa dynamic weighting strategy to adjust the contributions of pseudo-labels usingan uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). Inaddition, we use a self-supervised contrastive learning mechanism to alignuncertain voxel features with reliable class prototypes by effectivelydifferentiating between trustworthy and uncertain predictions, thus reducingprediction uncertainty. Extensive experiments are conducted on three 3Dsegmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposedapproach consistently exhibits superior performance across various settings(e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared tothe state-of-the-art methods. Furthermore, the usefulness of the proposedmodules is further validated via ablation experiments.</description>
      <author>example@mail.com (Yunyao Lu, Yihang Wu, Ahmad Chaddad, Tareef Daqqaq, Reem Kateb)</author>
      <guid isPermaLink="false">2509.13084v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning</title>
      <link>http://arxiv.org/abs/2509.12875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LTA-Thinker是一种新型训练框架，通过增加潜在思想分布的方差和引入分布方向性优化，解决了大型语言模型中复杂推理的瓶颈问题，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型中的复杂推理可以通过测试时扩展(TTS)进行动态优化，以减轻过度思考问题。然而，在连续潜在空间推理中，高质量潜在思想的生成和利用仍然是核心瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出一个潜在思想增强训练框架——LTA-Thinker，以提高分布方差并从两个方面增强推理性能。&lt;h4&gt;方法&lt;/h4&gt;构建基于可学习先验的潜在思想生成架构，以增加生成潜在思想向量的方差分布，简化整体结构并提高性能上限；引入基于分布的方向性优化范式，联合约束分布局部性和分布规模，通过多目标共同训练策略结合标准监督微调(SFT)损失与两种新损失：语义对齐损失和推理焦点损失。&lt;h4&gt;主要发现&lt;/h4&gt;LTA-Thinker在各种基线中取得了最先进的(SOTA)性能，表现出更高的性能上限和更好的扩展效果。&lt;h4&gt;结论&lt;/h4&gt;LTA-Thinker通过提高分布方差和优化潜在思想的生成与利用，有效提升了大型语言模型的复杂推理能力。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型中的复杂推理可以通过测试时扩展(TTS)进行动态优化以减轻过度思考。诸如Coconut、SoftCoT及其变体等方法在连续潜在空间推理中有效，但核心瓶颈仍在于高质量潜在思想的生成和利用。借鉴SoftCoT++理论——生成的潜在思想分布的更大方差更接近黄金真实分布，我们提出了一个潜在思想增强训练框架——LTA-Thinker，它从两个方面提高分布方差并增强推理性能。首先，LTA-Thinker构建了一个基于可学习先验的潜在思想生成架构，该架构旨在增加生成潜在思想向量的方差分布，以简化整体结构并提高性能上限。其次，LTA-Thinker引入了一种基于分布的方向性优化范式，联合约束分布局部性和分布规模。这种机制通过多目标共同训练策略提高信息效率和计算成本，该策略结合了标准监督微调(SFT)损失与两种新损失：语义对齐损失，利用KL散度确保潜在思想与问题语义高度相关；推理焦点损失，利用对比学习机制引导模型关注最关键的推理步骤。实验表明，LTA-Thinker在各种基线中实现了最先进的(SOTA)性能，表现出更高的性能上限和更好的扩展效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Complex Reasoning in Large Language Models can be dynamically optimized usingTest-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,SoftCoT and its variant are effective in continuous latent space inference, thecore bottleneck still lies in the efficient generation and utilization ofhigh-quality Latent Thought. Drawing from the theory of SoftCoT++ that a largervariance in the generated Latent Thought distribution more closely approximatesthe golden truth distribution, we propose a Latent Thought-Augmented TrainingFramework--LTA-Thinker, which improves distributional variance and enhancesreasoning performance from two perspectives. First, LTA-Thinker constructs aLatent Thought generation architecture based on a learnable prior. Thisarchitecture aims to increase the variance distribution of generated LatentThought Vectors in order to simplify the overall structure and raise theperformance ceiling. Second, LTA-Thinker introduces a distribution-baseddirectional optimization paradigm that jointly constrains both distributionlocality and distribution scale. This mechanism improves information efficiencyand computational cost through a multi-objective co-training strategy, whichcombines standard Supervised Fine-Tuning (SFT) loss with two novel losses:Semantic Alignment Loss, which utilizes KL divergence to ensure that the LatentThought is highly relevant to the semantics of the question; Reasoning FocusLoss, which utilizes a contrastive learning mechanism to guide the model tofocus on the most critical reasoning steps. Experiments show that LTA-thinkerachieves state-of-the-art (SOTA) performance among various baselines anddemonstrates a higher performance ceiling and better scaling effects.</description>
      <author>example@mail.com (Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang, Yuantao Han, Cangyi Yang, jiaxu Zhang, Feiliang Ren)</author>
      <guid isPermaLink="false">2509.12875v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision</title>
      <link>http://arxiv.org/abs/2509.12771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉语言模型(VLMs)的概念抽象能力，提出了一种新的训练方法使模型能够识别图像作为一般概念的实例，而不仅仅是识别对象及其关系。&lt;h4&gt;背景&lt;/h4&gt;人类能够将图像识别为一般概念的实例，而不仅仅是识别其中的对象及其关系。研究者调查VLMs是否具有这种概念抽象能力。&lt;h4&gt;目的&lt;/h4&gt;1) 调查VLMs在多大程度上具有概念抽象能力；2) 研究策略使VLM模型能够更大程度地具备这种能力。&lt;h4&gt;方法&lt;/h4&gt;引入分组图像-标题数据集(MAGIC)，使用新颖的对比损失技术编码每组图像(标题)的共有信息，提出基于文本-图像对比组的分组对比损失函数(外部对比损失)和衡量组内图像-标题实例距离的内部损失，训练模型创建语义表示使其接近高层概念的语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种训练方法产生的CLEAR GLASS模型在抽象概念识别方面比最先进(SOTA)模型有所改进。&lt;h4&gt;结论&lt;/h4&gt;CLEAR GLASS模型通过这种训练方法获得了概念抽象能力，能够更好地识别抽象概念，尽管训练过程中模型并未直接接触高层概念标签。&lt;h4&gt;翻译&lt;/h4&gt;人类能够将图像识别为一般概念的实例，而不仅仅是识别其中的对象及其关系。在本文中，我们研究了1) VLMs在多大程度上具有这种概念抽象能力，以及2)如何在图像中编码那种高层概念信息，使产生的VLM模型(CLEAR GLASS模型)更大程度地具备这种能力的策略。为此，我们引入了一个分组图像-标题数据集(MAGIC)，它包含多组图像标题、相关图像和高层概念标签。我们使用新颖的对比损失技术，使模型编码每组图像(标题)中所有成员共有的信息。我们的主要贡献是基于文本-图像对比组的分组对比损失函数(外部对比损失)以及衡量组内图像-标题实例之间距离的内部损失。我们的训练方法使CLEAR GLASS模型获得了概念抽象能力，因为模型没有暴露给与每组相关的高层概念。相反，训练迫使模型为每组图像-标题创建一个语义表示，使其在潜在语义空间中更接近高层概念的语义表示。我们的实验表明，这种训练方法产生的模型在抽象概念识别方面比SOTA模型有所改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can recognize an image as an instance of a general concept, beyondsimply identifying its objects and their relationships. In this paper, weinvestigate 1. The extent to which VLMs have this concept abstraction capacity,and 2. Strategies for encoding the sort of higher-concept information in imagesthat would enable the resulting VLM model (CLEAR GLASS model) to have thiscapability to a greater degree. To this end, we introduce a groupedimage-caption dataset (MAGIC), which consists of several groups of imagecaptions and for each group a set of associated images and higher-levelconceptual labels. We use a novel contrastive loss technique to induce themodel to encode in the representation of each image (caption) in a group theinformation that is common to all members of the image-caption group. Our maincontribution is a grouped contrastive loss function based on text-imagecontrastive groups (outer contrastive loss) as well as an inner loss whichmeasures the distances between image-caption instances in the group. Ourtraining methodology results in the CLEAR GLASS model having the conceptabstraction capacity as an emergent capacity because the model is not exposedto the higher-level concepts associated with each group. Instead, the trainingforces the model to create for each image-caption group a semanticrepresentation that brings it closer to the semantic representation of thehigher-level concepts in the latent semantic space. Our experiments show thatthis training methodology results in a model which shows improvement inabstract concept recognition compared to SOTA models.</description>
      <author>example@mail.com (Omri Suissa, Muhiim Ali, Shengmai Chen, Yinuo Cai, Shekhar Pradhan)</author>
      <guid isPermaLink="false">2509.12771v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>ScaleDoc: Scaling LLM-based Predicates over Large Document Collections</title>
      <link>http://arxiv.org/abs/2509.12610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ScaleDoc是一种新型系统，通过解耦谓词执行为离线表示阶段和在线过滤阶段，结合大型语言模型和轻量级代理模型，显著提高了大规模语义分析的效率。&lt;h4&gt;背景&lt;/h4&gt;谓词是数据分析系统的基础组件，但现代工作负载越来越多地涉及需要语义理解的非结构化文档，传统基于值的谓词已无法满足需求。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在处理海量文档和临时查询时的高推理成本问题，使大规模语义分析变得实用和高效。&lt;h4&gt;方法&lt;/h4&gt;ScaleDoc系统将谓词执行分为离线表示阶段和在线过滤阶段。离线阶段利用LLM为文档生成语义表示；在线阶段训练轻量级代理模型过滤文档，仅将模糊案例转发给LLM。系统还包含两个核心创新：基于对比学习的框架训练代理模型生成可靠决策分数，以及自适应级联机制确定有效过滤策略。&lt;h4&gt;主要发现&lt;/h4&gt;ScaleDoc在三个数据集上实现了超过2倍的端到端加速，并将昂贵的LLM调用减少了高达85%。&lt;h4&gt;结论&lt;/h4&gt;ScaleDoc使大规模语义分析变得实用和高效，通过减少对大型语言模型的依赖，显著提高了处理性能。&lt;h4&gt;翻译&lt;/h4&gt;谓词是数据分析系统的基础组件。然而，现代工作负载越来越多地涉及非结构化文档，这需要超越传统基于值的谓词的语义理解。面对海量文档和临时查询，虽然大型语言模型展示了强大的零样本能力，但其高推理成本导致不可接受的额外开销。因此，我们提出了ScaleDoc，一种新型系统，通过将谓词执行解耦为离线表示阶段和优化后的在线过滤阶段来解决这一问题。在离线阶段，ScaleDoc利用LLM为每个文档生成语义表示。在线，对于每个查询，它在这些表示上训练一个轻量级代理模型来过滤大部分文档，仅将模糊案例转发给LLM进行最终决策。此外，ScaleDoc提出了两个核心创新以实现显著效率：(1)基于对比学习的框架，训练代理模型生成可靠的预测决策分数；(2)自适应级联机制，在满足特定准确度目标的同时确定有效的过滤策略。我们在三个数据集上的评估表明，ScaleDoc实现了超过2倍的端到端加速，并将昂贵的LLM调用减少了高达85%，使大规模语义分析变得实用和高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicates are foundational components in data analysis systems. However,modern workloads increasingly involve unstructured documents, which demandssemantic understanding, beyond traditional value-based predicates. Givenenormous documents and ad-hoc queries, while Large Language Models (LLMs)demonstrate powerful zero-shot capabilities, their high inference cost leads tounacceptable overhead. Therefore, we introduce \textsc{ScaleDoc}, a novelsystem that addresses this by decoupling predicate execution into an offlinerepresentation phase and an optimized online filtering phase. In the offlinephase, \textsc{ScaleDoc} leverages a LLM to generate semantic representationsfor each document. Online, for each query, it trains a lightweight proxy modelon these representations to filter the majority of documents, forwarding onlythe ambiguous cases to the LLM for final decision. Furthermore,\textsc{ScaleDoc} proposes two core innovations to achieve significantefficiency: (1) a contrastive-learning-based framework that trains the proxymodel to generate reliable predicating decision scores; (2) an adaptive cascademechanism that determines the effective filtering policy while meeting specificaccuracy targets. Our evaluations across three datasets demonstrate that\textsc{ScaleDoc} achieves over a 2$\times$ end-to-end speedup and reducesexpensive LLM invocations by up to 85\%, making large-scale semantic analysispractical and efficient.</description>
      <author>example@mail.com (Hengrui Zhang, Yulong Hui, Yihao Liu, Huanchen Zhang)</author>
      <guid isPermaLink="false">2509.12610v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>LayerLock: Non-collapsing Representation Learning with Progressive Freezing</title>
      <link>http://arxiv.org/abs/2509.10156v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LayerLock是一种简单而有效的自监督视觉表征学习方法，通过渐进式层冻结技术，从像素预测逐步过渡到潜在空间预测。&lt;h4&gt;背景&lt;/h4&gt;在视频掩码自编码(MAE)模型训练过程中，ViT层按照其深度顺序收敛：浅层先收敛，深层后收敛。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法加速标准MAE训练，并实现简单且可扩展的潜在空间预测，避免'表征崩溃'问题。&lt;h4&gt;方法&lt;/h4&gt;LayerLock利用ViT层按深度顺序收敛的观察，在整个训练过程中根据显式调度逐步冻结模型，既可用于加速标准MAE，也可用于潜在空间预测。&lt;h4&gt;主要发现&lt;/h4&gt;通过逐步冻结模型可以加速标准MAE训练；使用相同调度可以实现简单且可扩展的潜在空间预测，避免'表征崩溃'问题。&lt;h4&gt;结论&lt;/h4&gt;LayerLock方法成功应用于高达40亿参数的大模型，在4DS感知套件上的结果超越了非掩码潜在预测方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了LayerLock，一种简单而有效的自监督视觉表征学习方法，它通过渐进式层冻结技术，从像素预测逐步过渡到潜在空间预测。首先，我们观察到在视频掩码自编码(MAE)模型训练过程中，ViT层按照其深度顺序收敛：浅层先收敛，深层后收敛。然后，我们证明这一观察可以用来加速标准MAE，通过在整个训练过程中根据显式调度逐步冻结模型。此外，相同的调度可用于一种简单且可扩展的潜在空间预测方法，不会遭受'表征崩溃'问题。我们将提出的LayerLock方法应用于高达40亿参数的大模型，在4DS感知套件上的结果超越了非掩码潜在预测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LayerLock, a simple yet effective approach for self-supervisedvisual representation learning, that gradually transitions from pixel to latentprediction through progressive layer freezing. First, we make the observationthat during training of video masked-autoencoding (MAE) models, ViT layersconverge in the order of their depth: shallower layers converge early, deeperlayers converge late. We then show that this observation can be exploited toaccelerate standard MAE by progressively freezing the model according to anexplicit schedule, throughout training. Furthermore, this same schedule can beused in a simple and scalable approach to latent prediction that does notsuffer from "representation collapse". We apply our proposed approach,LayerLock, to large models of up to 4B parameters with results surpassing thoseof non-latent masked prediction on the 4DS perception suite.</description>
      <author>example@mail.com (Goker Erdogan, Nikhil Parthasarathy, Catalin Ionescu, Drew Hudson, Alexander Lerchner, Andrew Zisserman, Mehdi Sajjadi, Joao Carreira)</author>
      <guid isPermaLink="false">2509.10156v2</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
  <item>
      <title>Open-ended Hierarchical Streaming Video Understanding with Vision Language Models</title>
      <link>http://arxiv.org/abs/2509.12145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了分层流式视频理解任务，结合了在线时序动作定位和自由形式描述生成。作者提出使用大语言模型丰富现有数据集的方法，并开发了名为OpenHOUSE的系统，该系统具有专门的流式模块，能够准确检测相邻动作边界，性能显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;当前缺乏具有分层和细粒度时间标注的视频数据集，限制了视频理解技术的发展。同时，流式动作感知目前主要局限于动作分类，无法满足更复杂的视频理解需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够进行分层流式视频理解的方法，扩展流式动作感知的能力，使其超越简单的动作分类，能够进行更精细的动作边界检测和事件描述生成。&lt;h4&gt;方法&lt;/h4&gt;1. 利用大语言模型将原子动作分组为更高级别的事件，以丰富现有数据集；2. 提出OpenHOUSE系统，包含专门的流式模块用于检测紧密相邻动作之间的边界；3. 扩展流式动作感知，使其不仅限于动作分类，还包括自由形式的描述生成。&lt;h4&gt;主要发现&lt;/h4&gt;1. 大语言模型能够有效地将原子动作分组为更高级别的事件，从而丰富现有数据集；2. OpenHOUSE的专门流式模块能够准确检测紧密相邻动作之间的边界；3. OpenHOUSE的性能几乎是现有方法直接扩展的两倍。&lt;h4&gt;结论&lt;/h4&gt;流式动作感知的未来发展方向是集成强大的生成模型，OpenHOUSE代表了朝着这一方向迈出的关键一步，为更高级的视频理解任务提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了分层流式视频理解，这是一个结合在线时序动作定位与自由形式描述生成的任务。鉴于具有分层和细粒度时间标注的数据集稀缺，我们证明了大型语言模型能够有效地将原子动作分组为更高级别的事件，从而丰富现有数据集。随后，我们提出了OpenHOUSE(面向事件的开分层式在线理解系统)，它将流式动作感知扩展超越了动作分类的范围。OpenHOUSE具有专门的流式模块，能够准确检测紧密相邻动作之间的边界，性能几乎是现有方法直接扩展的两倍。我们展望流式动作感知的未来在于集成强大的生成模型，而OpenHOUSE代表了朝着这一方向迈出的关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Hierarchical Streaming Video Understanding, a task that combinesonline temporal action localization with free-form description generation.Given the scarcity of datasets with hierarchical and fine-grained temporalannotations, we demonstrate that LLMs can effectively group atomic actions intohigher-level events, enriching existing datasets. We then propose OpenHOUSE(Open-ended Hierarchical Online Understanding System for Events), which extendsstreaming action perception beyond action classification. OpenHOUSE features aspecialized streaming module that accurately detects boundaries between closelyadjacent actions, nearly doubling the performance of direct extensions ofexisting methods. We envision the future of streaming action perception in theintegration of powerful generative models, with OpenHOUSE representing a keystep in that direction.</description>
      <author>example@mail.com (Hyolim Kang, Yunsu Park, Youngbeom Yoo, Yeeun Choi, Seon Joo Kim)</author>
      <guid isPermaLink="false">2509.12145v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset</title>
      <link>http://arxiv.org/abs/2509.12047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 figures, Submitted to Computers and Electronics in Agriculture&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于计算机视觉的模块化流程，用于自动分析群体饲养环境中的动物行为，显著提高了分析准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;动物行为分析对理解农业环境中的动物福利、健康和生产率至关重要，但传统人工观察方法耗时、主观且可扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个利用开源最先进计算机视觉技术的模块化流程，实现群体饲养环境中动物行为分析的自动化。&lt;h4&gt;方法&lt;/h4&gt;结合零样本目标检测模型、感知运动跟踪和分割技术，以及使用视觉转换器进行高级特征提取，解决动物遮挡和群体饲养场景等挑战，并在室内猪监测中进行了演示。&lt;h4&gt;主要发现&lt;/h4&gt;在爱丁堡猪行为视频数据集上验证，时间模型总体准确率达94.2%，比现有方法提高21.2个百分点；跟踪能力身份保持得分为93.3%，目标检测精度达89.3%。&lt;h4&gt;结论&lt;/h4&gt;模块化设计有潜力适应其他环境，但需对不同物种进一步验证；开源实现为行为监测提供了可扩展解决方案，通过自动化、客观和持续的分析促进精准养猪和福利评估。&lt;h4&gt;翻译&lt;/h4&gt;动物行为分析在理解农业环境中的动物福利、健康状况和生产率方面起着至关重要的作用。然而，传统的人工观察方法耗时、主观且可扩展性有限。我们提出了一种模块化流程，利用开源的最先进计算机视觉技术来自动化群体饲养环境中的动物行为分析。我们的方法结合了用于零样本目标检测的最先进模型、感知运动跟踪和分割技术，以及使用视觉转换器进行高级特征提取，以实现稳健的行为识别。该流程解决了包括动物遮挡和群体饲养场景在内的挑战，如在室内猪监测中所展示的那样。我们在爱丁堡猪行为视频数据集上对多个行为任务验证了我们的系统。我们的时间模型实现了94.2%的总体准确率，比现有方法提高了21.2个百分点。该流程显示出强大的跟踪能力，身份保持得分为93.3%，目标检测精度为89.3%。模块化设计表明有潜力适应其他环境，但需要对不同物种进行进一步验证。开源实现为行为监测提供了可扩展的解决方案，通过自动化、客观和持续的分析为精准养猪和福利评估做出贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Animal behavior analysis plays a crucial role in understanding animalwelfare, health status, and productivity in agricultural settings. However,traditional manual observation methods are time-consuming, subjective, andlimited in scalability. We present a modular pipeline that leveragesopen-sourced state-of-the-art computer vision techniques to automate animalbehavior analysis in a group housing environment. Our approach combinesstate-of-the-art models for zero-shot object detection, motion-aware trackingand segmentation, and advanced feature extraction using vision transformers forrobust behavior recognition. The pipeline addresses challenges including animalocclusions and group housing scenarios as demonstrated in indoor pigmonitoring. We validated our system on the Edinburgh Pig Behavior Video Datasetfor multiple behavioral tasks. Our temporal model achieved 94.2% overallaccuracy, representing a 21.2 percentage point improvement over existingmethods. The pipeline demonstrated robust tracking capabilities with 93.3%identity preservation score and 89.3% object detection precision. The modulardesign suggests potential for adaptation to other contexts, though furthervalidation across species would be required. The open-source implementationprovides a scalable solution for behavior monitoring, contributing to precisionpig farming and welfare assessment through automated, objective, and continuousanalysis.</description>
      <author>example@mail.com (Haiyu Yang, Enhong Liu, Jennifer Sun, Sumit Sharma, Meike van Leerdam, Sebastien Franceschini, Puchun Niu, Miel Hostens)</author>
      <guid isPermaLink="false">2509.12047v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare</title>
      <link>http://arxiv.org/abs/2509.11944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于时间图的多模态医疗推理模型，通过有向图建模，能够适应动态变化，完善推理内容，并考虑时间因素跟踪患者健康变化。多代理时间推理框架进一步提高了推理准确性，实验验证了该方法的新颖性和实用性。&lt;h4&gt;背景&lt;/h4&gt;医疗和医学是多模态学科，需要处理多模态数据进行推理和诊断多种疾病。尽管已经出现了一些用于科学领域复杂任务的多模态推理模型，但它们在医疗领域的应用仍然有限，并且在诊断推理方面表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决多模态医疗推理在正确诊断方面的挑战，协助医疗专业人员。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于时间图的推理过程模型，通过有向图建模。该模型能够通过回溯来适应原因的动态变化，完善推理内容，创建新原因或删除现有原因，以达到最佳推荐或答案。考虑不同时间点的多模态数据，能够跟踪和分析患者的健康状况和疾病进展。提出了多代理时间推理框架，提供任务分配和交叉验证机制，进一步提高推理输出的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;基本实验和分析结果证明了所提出的初步方法的新颖性和实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的时间图推理框架和多代理时间推理框架能够有效解决医疗领域多模态推理的挑战，提高诊断准确性。&lt;h4&gt;翻译&lt;/h4&gt;医疗和医学是处理多模态数据以推理和诊断多种疾病的多模态学科。尽管已经出现了一些用于科学领域复杂任务的多模态推理模型，但它们在医疗领域的应用仍然有限，并且在诊断推理方面表现不佳。为解决多模态医疗推理在正确诊断方面的挑战并协助医疗专业人员，本文提出了一种新颖的基于时间图的推理过程，通过有向图建模。该模型能够通过回溯来适应原因的动态变化，完善推理内容，创建新原因或删除现有原因，以达到最佳推荐或答案。此外，考虑不同时间点的多模态数据能够跟踪和分析患者的健康状况和疾病进展。此外，所提出的多代理时间推理框架提供任务分配和交叉验证机制，进一步提高推理输出的准确性。一些基本实验和分析结果证明了所提出的初步方法的新颖性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Healthcare and medicine are multimodal disciplines that deal with multimodaldata for reasoning and diagnosing multiple diseases. Although some multimodalreasoning models have emerged for reasoning complex tasks in scientificdomains, their applications in the healthcare domain remain limited and fallshort in correct reasoning for diagnosis. To address the challenges ofmultimodal medical reasoning for correct diagnosis and assist the healthcareprofessionals, a novel temporal graph-based reasoning process modelled througha directed graph has been proposed in the current work. It helps inaccommodating dynamic changes in reasons through backtracking, refining thereasoning content, and creating new or deleting existing reasons to reach thebest recommendation or answer. Again, consideration of multimodal data atdifferent time points can enable tracking and analysis of patient health anddisease progression. Moreover, the proposed multi-agent temporal reasoningframework provides task distributions and a cross-validation mechanism tofurther enhance the accuracy of reasoning outputs. A few basic experiments andanalysis results justify the novelty and practical utility of the proposedpreliminary approach.</description>
      <author>example@mail.com (Susanta Mitra)</author>
      <guid isPermaLink="false">2509.11944v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</title>
      <link>http://arxiv.org/abs/2509.11866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了Dr.V分层框架，用于诊断大型视频模型中的幻觉问题，包括基准数据集Dr.V-Bench和卫星视频代理Dr.V-Agent，通过细粒度时空定位提高视频理解的可靠性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;大型视频模型(LVMs)的最新进展显著提升了视频理解能力，但这些模型仍然存在幻觉问题，即产生与输入视频内容相冲突的内容。&lt;h4&gt;目的&lt;/h4&gt;解决大型视频模型中的幻觉问题，建立一个能够诊断视频幻觉的框架。&lt;h4&gt;方法&lt;/h4&gt;提出Dr.V分层框架，包括：1) Dr.V-Bench基准数据集：包含来自4,974个视频的10k实例，涵盖多样化任务，每个实例都有详细的时空标注；2) Dr.V-Agent卫星视频代理：通过在感知和时空层面系统应用细粒度时空定位，然后进行认知层面推理，来检测LVMs中的幻觉。&lt;h4&gt;主要发现&lt;/h4&gt;Dr.V-Agent能够有效诊断幻觉，同时提高模型的可解释性和可靠性。&lt;h4&gt;结论&lt;/h4&gt;Dr.V为现实世界场景中的鲁棒视频理解提供了实用的蓝图，所有数据和代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;大型视频模型(LVMs)的最新进展显著提升了视频理解能力。然而，这些模型仍然存在幻觉问题，产生与输入视频内容相冲突的内容。为解决这一问题，我们提出了Dr.V，一个覆盖感知、时空和认知三个层面的分层框架，通过细粒度的时空定位来诊断视频幻觉。Dr.V包含两个关键组件：基准数据集Dr.V-Bench和卫星视频代理Dr.V-Agent。Dr.V-Bench包含来自4,974个视频的10k实例，涵盖多样化任务，每个实例都配有详细的时空标注。Dr.V-Agent通过在感知和时空层面系统应用细粒度时空定位，然后进行认知层面推理，来检测LVMs中的幻觉。这一逐步流程模拟了类人视频理解过程，能有效识别幻觉。大量实验证明，Dr.V-Agent在诊断幻觉方面有效，同时提高了可解释性和可靠性，为现实世界场景中的鲁棒视频理解提供了实用蓝图。我们所有的数据和代码可在https://github.com/Eurekaleo/Dr.V获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large video models (LVMs) have significantly enhancevideo understanding. However, these models continue to suffer fromhallucinations, producing content that conflicts with input videos. To addressthis issue, we propose Dr.V, a hierarchical framework covering perceptive,temporal, and cognitive levels to diagnose video hallucination by fine-grainedspatial-temporal grounding. Dr.V comprises of two key components: a benchmarkdataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes10k instances drawn from 4,974 videos spanning diverse tasks, each enrichedwith detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations inLVMs by systematically applying fine-grained spatial-temporal grounding at theperceptive and temporal levels, followed by cognitive level reasoning. Thisstep-by-step pipeline mirrors human-like video comprehension and effectivelyidentifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent iseffective in diagnosing hallucination while enhancing interpretability andreliability, offering a practical blueprint for robust video understanding inreal-world scenarios. All our data and code are available athttps://github.com/Eurekaleo/Dr.V.</description>
      <author>example@mail.com (Meng Luo, Shengqiong Wu, Liqiang Jing, Tianjie Ju, Li Zheng, Jinxiang Lai, Tianlong Wu, Xinya Du, Jian Li, Siyuan Yan, Jiebo Luo, William Yang Wang, Hao Fei, Mong-Li Lee, Wynne Hsu)</author>
      <guid isPermaLink="false">2509.11866v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Vision Language Models and Symbolic Grounding for Video Question Answering</title>
      <link>http://arxiv.org/abs/2509.11862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合符号场景图与视觉语言模型的新框架SG-VLM，用于提升视频问答中的因果和时间推理能力，尽管相对于强大的VLMs改进有限。&lt;h4&gt;背景&lt;/h4&gt;视频问答(VQA)需要模型对视频中的空间、时间和因果线索进行推理。近期的视觉语言模型虽取得良好结果，但常依赖浅层关联，导致时间定位能力弱且可解释性有限。&lt;h4&gt;目的&lt;/h4&gt;研究符号场景图(SGs)作为视频问答的中间定位信号，探索如何将SGs与VLMs结合以提升模型表现。&lt;h4&gt;方法&lt;/h4&gt;提出SG-VLM模块化框架，通过提示和视觉定位将冻结的VLMs与场景图定位相结合，利用SGs提供的结构化对象-关系表示来补充VLMs的整体推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准测试(NExT-QA, iVQA, ActivityNet-QA)和多个VLMs(QwenVL, InternVL)上测试，SG-VLM提高了因果和时间推理能力，超越了先前的基线方法，但相对于强大的VLMs的改进有限。&lt;h4&gt;结论&lt;/h4&gt;符号定位在视频问答中具有前景但也存在当前局限性，为未来混合VLM-符号方法在视频理解中的应用提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;视频问答(VQA)需要模型对视频中的空间、时间和因果线索进行推理。近期的视觉语言模型(VLMs)取得了很好的结果，但通常依赖于浅层关联，导致时间定位能力弱且可解释性有限。我们研究符号场景图(SGs)作为VQA的中间定位信号。SGs提供结构化的对象-关系表示，补充VLMs的整体推理能力。我们提出SG-VLM，一个通过提示和视觉定位将冻结的VLMs与场景图定位相结合的模块化框架。在三个基准测试(NExT-QA, iVQA, ActivityNet-QA)和多个VLMs(QwenVL, InternVL)上，SG-VLM提高了因果和时间推理能力，并超越了先前的基线方法，但相对于强大的VLMs的改进有限。这些发现突显了符号定位的前景和当前局限性，为未来混合VLM-符号方法在视频理解中的应用提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VQA) requires models to reason over spatial,temporal, and causal cues in videos. Recent vision language models (VLMs)achieve strong results but often rely on shallow correlations, leading to weaktemporal grounding and limited interpretability. We study symbolic scene graphs(SGs) as intermediate grounding signals for VQA. SGs provide structuredobject-relation representations that complement VLMs holistic reasoning. Weintroduce SG-VLM, a modular framework that integrates frozen VLMs with scenegraph grounding via prompting and visual localization. Across three benchmarks(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLMimproves causal and temporal reasoning and outperforms prior baselines, thoughgains over strong VLMs are limited. These findings highlight both the promiseand current limitations of symbolic grounding, and offer guidance for futurehybrid VLM-symbolic approaches in video understanding.</description>
      <author>example@mail.com (Haodi Ma, Vyom Pathak, Daisy Zhe Wang)</author>
      <guid isPermaLink="false">2509.11862v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning</title>
      <link>http://arxiv.org/abs/2509.11796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出FineQuest，一个基于双模式推理的无需训练框架，专门用于解决体育视频问答中的复杂理解问题。&lt;h4&gt;背景&lt;/h4&gt;基于大型语言模型(LLMs)的视频问答在一般视频理解方面显示出潜力，但在应用于体育视频这一固有复杂领域时面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决通用模型与特定体育领域理解之间的知识差距，提高体育视频问答的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出FineQuest框架，采用双模式推理（反应式推理用于简单问题，深思熟虑式推理用于复杂问题）；集成SSGraph，一个跨越九种运动的多模态体育知识场景图；引入两个新的体育VideoQA基准：Gym-QA和Diving-QA。&lt;h4&gt;主要发现&lt;/h4&gt;FineQuest在新的基准测试以及现有的SPORTU数据集上取得了最先进的性能，同时保持了强大的通用VideoQA能力。&lt;h4&gt;结论&lt;/h4&gt;FineQuest通过双模式推理和领域特定知识整合，有效解决了体育视频问答的挑战。&lt;h4&gt;翻译&lt;/h4&gt;基于大型语言模型(LLMs)的视频问答在一般视频理解方面显示出潜力，但在应用于体育视频这一固有复杂领域时面临重大挑战。在这项工作中，我们提出了FineQuest，这是第一个利用认知科学启发的双模式推理的无训练框架：i)用于简单体育查询的反应式推理；ii)用于更复杂查询的深思熟虑式推理。为了弥合通用模型与特定体育领域理解之间的知识差距，FineQuest集成了SSGraph，一个跨越九种运动的多模态体育知识场景图，它编码视觉实例和领域特定术语以提高推理准确性。此外，我们引入了两个新的体育VideoQA基准，Gym-QA和Diving-QA，它们源自FineGym和FineDiving数据集，实现了多样化和全面的评估。FineQuest在这些基准以及现有的SPORTU数据集上取得了最先进的性能，同时保持了强大的通用VideoQA能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VideoQA) based on Large Language Models (LLMs) hasshown potential in general video understanding but faces significant challengeswhen applied to the inherently complex domain of sports videos. In this work,we propose FineQuest, the first training-free framework that leveragesdual-mode reasoning inspired by cognitive science: i) Reactive Reasoning forstraightforward sports queries and ii) Deliberative Reasoning for more complexones. To bridge the knowledge gap between general-purpose models anddomain-specific sports understanding, FineQuest incorporates SSGraph, amultimodal sports knowledge scene graph spanning nine sports, which encodesboth visual instances and domain-specific terminology to enhance reasoningaccuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QAand Diving-QA, derived from the FineGym and FineDiving datasets, enablingdiverse and comprehensive evaluation. FineQuest achieves state-of-the-artperformance on these benchmarks as well as the existing SPORTU dataset, whilemaintains strong general VideoQA capabilities.</description>
      <author>example@mail.com (Haodong Chen, Haojian Huang, XinXiang Yin, Dian Shao)</author>
      <guid isPermaLink="false">2509.11796v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration</title>
      <link>http://arxiv.org/abs/2509.11360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了GLaVE-Cap框架，通过整合视觉专家和双流结构解决视频详细描述中的上下文一致性和细节性问题，同时构建了GLaVE-Bench基准测试和GLaVE-1.2M数据集，在多个基准上实现了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;视频详细描述旨在生成全面视频描述以促进视频理解。目前大多数研究采用从局部到全局的范式，先生成视频片段的局部描述，再将其汇总为全局描述。&lt;h4&gt;目的&lt;/h4&gt;解决现有从局部到全局的视频描述范式导致的描述不够详细且上下文不一致的问题，提高视频描述的质量和连贯性。&lt;h4&gt;方法&lt;/h4&gt;提出GLaVE-Cap框架，包含两个核心模块：1) TrackFusion模块利用视觉专家获取跨帧视觉提示，结合双流结构实现全面的局部描述生成；2) CaptionBridge模块使用全局上下文指导局部描述，并将局部描述自适应地汇总为连贯的全局描述。同时构建GLaVE-Bench基准测试和GLaVE-1.2M训练数据集。&lt;h4&gt;主要发现&lt;/h4&gt;现有范式导致描述不够详细且上下文不一致的原因是：(1)没有确保细粒度描述的机制，(2)局部和全局描述之间的交互较弱。GLaVE-Cap通过整合视觉专家和双流结构有效解决了这些问题。&lt;h4&gt;结论&lt;/h4&gt;在四个基准测试上的大量实验表明，GLaVE-Cap达到了最先进的性能。消融研究和学生模型分析进一步验证了所提出模块的有效性以及GLaVE-1.2M对视频理解社区的贡献。源代码、模型权重、基准和数据集将开源。&lt;h4&gt;翻译&lt;/h4&gt;视频详细描述旨在生成全面的视频描述以促进视频理解。最近，视频详细描述领域的大多数努力都集中在从局部到全局的范式上，该范式首先从视频片段生成局部描述，然后将其汇总为全局描述。然而，我们发现这种范式导致描述不够详细且上下文不一致，这可以归因于：(1)没有确保细粒度描述的机制，以及(2)局部和全局描述之间的交互较弱。为了解决上述两个问题，我们提出了GLaVE-Cap，这是一种具有视觉专家集成的全局-局部对齐框架，包含两个核心模块：TrackFusion通过利用视觉专家获取跨帧视觉提示，结合双流结构实现全面的局部描述生成；而CaptionBridge通过使用全局上下文指导局部描述，并将局部描述自适应地汇总为连贯的全局描述来建立局部-全局交互。此外，我们构建了GLaVE-Bench，这是一个全面的视频描述基准测试，每个视频包含比现有基准多5倍的查询，覆盖多样化的视觉维度，以促进可靠的评估。我们还提供了包含16K高质量细粒度视频描述和1.2M相关问答对的训练数据集GLaVE-1.2M。在四个基准上的大量实验表明，我们的GLaVE-Cap实现了最先进的性能。此外，消融研究和学生模型分析进一步验证了所提出模块的有效性以及GLaVE-1.2M对视频理解社区的贡献。源代码、模型权重、基准和数据集将开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video detailed captioning aims to generate comprehensive video descriptionsto facilitate video understanding. Recently, most efforts in the video detailedcaptioning community have been made towards a local-to-global paradigm, whichfirst generates local captions from video clips and then summarizes them into aglobal caption. However, we find this paradigm leads to less detailed andcontextual-inconsistent captions, which can be attributed to (1) no mechanismto ensure fine-grained captions, and (2) weak interaction between local andglobal captions. To remedy the above two issues, we propose GLaVE-Cap, aGlobal-Local aligned framework with Vision Expert integration for Captioning,which consists of two core modules: TrackFusion enables comprehensive localcaption generation, by leveraging vision experts to acquire cross-frame visualprompts, coupled with a dual-stream structure; while CaptionBridge establishesa local-global interaction, by using global context to guide local captioning,and adaptively summarizing local captions into a coherent global caption.Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmarkfeaturing 5X more queries per video than existing benchmarks, covering diversevisual dimensions to facilitate reliable evaluation. We further provide atraining dataset GLaVE-1.2M containing 16K high-quality fine-grained videocaptions and 1.2M related question-answer pairs. Extensive experiments on fourbenchmarks show that our GLaVE-Cap achieves state-of-the-art performance.Besides, the ablation studies and student model analyses further validate theeffectiveness of the proposed modules and the contribution of GLaVE-1.2M to thevideo understanding community. The source code, model weights, benchmark, anddataset will be open-sourced.</description>
      <author>example@mail.com (Wan Xu, Feng Zhu, Yihan Zeng, Yuanfan Guo, Ming Liu, Hang Xu, Wangmeng Zuo)</author>
      <guid isPermaLink="false">2509.11360v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Equalize: Data-Driven Frequency-Domain Signal Recovery in Molecular Communications</title>
      <link>http://arxiv.org/abs/2509.11327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于长短期记忆(LSTM)神经网络的频域均衡(FDE)技术，用于分子通信系统中的符号间干扰和噪声抑制，无需预先知道信道模型信息，同时保持高计算效率。&lt;h4&gt;背景&lt;/h4&gt;在分子通信中，符号间干扰(ISI)和噪声是影响通信可靠性的关键因素。时域均衡可有效减轻这些影响但计算复杂度高，而频域均衡计算效率更高却通常需要预先了解信道模型。&lt;h4&gt;目的&lt;/h4&gt;解决传统FDE方法对先验信道信息的依赖问题，提出基于LSTM神经网络的FDE技术，能够在分子通信信道中建模时间相关性，提高ISI和噪声抑制能力。&lt;h4&gt;方法&lt;/h4&gt;采用基于长短期记忆(LSTM)神经网络的频域均衡技术，通过监督训练策略实现信道自适应均衡，消除对信道先验信息的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，所提出的LSTM-FDE相比传统FDE和基于前馈神经网络的均衡器显著降低了误码率，这种性能提升归因于LSTM的时间建模能力增强了噪声抑制并加速了模型收敛。&lt;h4&gt;结论&lt;/h4&gt;基于LSTM神经网络的频域均衡技术能有效解决分子通信中的符号间干扰和噪声问题，无需预先知道信道模型信息，同时保持高计算效率，是一种有前景的分子通信均衡方案。&lt;h4&gt;翻译&lt;/h4&gt;在分子通信(MC)中，符号间干扰(ISI)和噪声是降低通信可靠性的关键因素。虽然时域均衡可以有效减轻这些影响，但它通常涉及与信道内存相关的高计算复杂度。相比之下，频域均衡(FDE)提供了更高的计算效率，但通常需要预先了解信道模型。为了解决这一限制，本文提出了一种基于长短期记忆(LSTM)神经网络的FDE技术，能够在分子通信信道中建模时间相关性，以提高ISI和噪声抑制能力。为了消除传统FDE方法对信道先验信息的依赖，采用监督训练策略进行信道自适应均衡。仿真结果表明，与传统的FDE和基于前馈神经网络的均衡器相比，所提出的LSTM-FDE显著降低了误码率。这一性能提升归因于LSTM的时间建模能力，它增强了噪声抑制并加速了模型收敛，同时保持了相当的计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In molecular communications (MC), inter-symbol interference (ISI) and noiseare key factors that degrade communication reliability. Although time-domainequalization can effectively mitigate these effects, it often entails highcomputational complexity concerning the channel memory. In contrast,frequency-domain equalization (FDE) offers greater computational efficiency buttypically requires prior knowledge of the channel model. To address thislimitation, this letter proposes FDE techniques based on long short-term memory(LSTM) neural networks, enabling temporal correlation modeling in MC channelsto improve ISI and noise suppression. To eliminate the reliance on priorchannel information in conventional FDE methods, a supervised training strategyis employed for channel-adaptive equalization. Simulation results demonstratethat the proposed LSTM-FDE significantly reduces the bit error rate compared totraditional FDE and feedforward neural network-based equalizers. Thisperformance gain is attributed to the LSTM's temporal modeling capabilities,which enhance noise suppression and accelerate model convergence, whilemaintaining comparable computational efficiency.</description>
      <author>example@mail.com (Cheng Xiang, Yu Huang, Miaowen Wen, Weiqiang Tan, Chan-Byoung Chae)</author>
      <guid isPermaLink="false">2509.11327v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic</title>
      <link>http://arxiv.org/abs/2509.11165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Traffic-MLLM是一个基于Qwen2.5-VL的多模态大语言模型，通过LoRA微调和创新的知识提示模块，实现了在交通视频理解任务上的最先进性能，并具备优秀的零样本推理和跨场景泛化能力。&lt;h4&gt;背景&lt;/h4&gt;随着智能交通系统的发展，交通视频理解在全面场景感知和因果分析中扮演着越来越重要的角色。然而，现有方法在准确建模时空因果关系和集成领域特定知识方面面临显著挑战，限制了它们在复杂场景中的有效性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，作者提出了Traffic-MLLM，一个专门用于细粒度交通分析的多模态大语言模型。&lt;h4&gt;方法&lt;/h4&gt;基于Qwen2.5-VL主干构建，利用高质量交通特定多模态数据集，使用低秩自适应(LoRA)进行轻量级微调，增强建模视频序列中连续时空特征的能力。同时引入创新的知识提示模块，将思维链(CoT)推理与检索增强生成(RAG)融合，使详细的交通规则和领域知识能够精确注入推理过程。&lt;h4&gt;主要发现&lt;/h4&gt;在TrafficQA和DriveQA基准测试上的实验结果表明，Traffic-MLLM取得了最先进的性能，验证了其处理多模态交通数据的卓越能力。它还表现出显著的零样本推理和跨场景泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Traffic-MLLM模型能够有效解决交通视频理解中的时空因果关系建模和领域知识集成问题，在复杂场景中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;随着智能交通系统的发展，交通视频理解在全面场景感知和因果分析中扮演着越来越重要的角色。然而，现有方法在准确建模时空因果关系和集成领域特定知识方面面临显著挑战，限制了它们在复杂场景中的有效性。为了解决这些局限性，我们提出了Traffic-MLLM，一个专门用于细粒度交通分析的多模态大语言模型。基于Qwen2.5-VL主干构建，我们的模型利用高质量交通特定多模态数据集，并使用低秩自适应(LoRA)进行轻量级微调，显著增强了其建模视频序列中连续时空特征的能力。此外，我们引入了一个创新的知识提示模块，将思维链(CoT)推理与检索增强生成(RAG)融合，使详细的交通规则和领域知识能够精确注入推理过程。这种设计显著提升了模型的逻辑推理和知识适应能力。在TrafficQA和DriveQA基准测试上的实验结果表明，Traffic-MLLM取得了最先进的性能，验证了其处理多模态交通数据的卓越能力。它还表现出显著的零样本推理和跨场景泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As intelligent transportation systems advance, traffic video understandingplays an increasingly pivotal role in comprehensive scene perception and causalanalysis. Yet, existing approaches face notable challenges in accuratelymodeling spatiotemporal causality and integrating domain-specific knowledge,limiting their effectiveness in complex scenarios. To address theselimitations, we propose Traffic-MLLM, a multimodal large language modeltailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,our model leverages high-quality traffic-specific multimodal datasets and usesLow-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancingits capacity to model continuous spatiotemporal features in video sequences.Furthermore, we introduce an innovative knowledge prompting module fusingChain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),enabling precise injection of detailed traffic regulations and domain knowledgeinto the inference process. This design markedly boosts the model's logicalreasoning and knowledge adaptation capabilities. Experimental results onTrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-artperformance, validating its superior ability to process multimodal trafficdata. It also exhibits remarkable zero-shot reasoning and cross-scenariogeneralization capabilities.</description>
      <author>example@mail.com (Waikit Xiu, Qiang Lu, Xiying Li, Chen Hu, Shengbo Sun)</author>
      <guid isPermaLink="false">2509.11165v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning</title>
      <link>http://arxiv.org/abs/2509.11880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将监督对比学习(SupCon)应用于模仿学习(IL)的新方法，专注于为视频游戏环境中的智能体学习更有效的状态表示。&lt;h4&gt;背景&lt;/h4&gt;模仿学习在视频游戏环境中面临状态表示学习的挑战，需要捕捉观察结果与行动之间的因果关系。&lt;h4&gt;目的&lt;/h4&gt;获取观察结果的潜在表示，更好地捕捉与行动相关的因素，从而更好地建模观察结果映射到演示者执行动作的因果关系。&lt;h4&gt;方法&lt;/h4&gt;提出一种将SupCon损失与连续输出空间集成的方案，使SupCon能够在不限制环境动作类型的情况下运行。&lt;h4&gt;主要发现&lt;/h4&gt;在3D游戏Astro Bot和Returnal以及多个2D Atari游戏上的实验表明，与仅使用监督动作预测损失函数训练的基线模型相比，改进了表示质量，加快了学习收敛速度，并提高了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;将SupCon应用于IL可以有效地改善智能体在视频游戏环境中的学习性能，使其能够更好地捕捉观察结果与行动之间的因果关系。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了监督对比学习(SupCon)在模仿学习(IL)中的新应用，专注于为视频游戏环境中的智能体学习更有效的状态表示。目标是获取观察结果的潜在表示，更好地捕捉与行动相关的因素，从而更好地建模观察结果映射到演示者执行动作的因果关系，例如当前方出现障碍物时玩家会跳跃。我们提出了一种将SupCon损失与连续输出空间集成的方案，使SupCon能够在不限制环境动作类型的情况下运行。在3D游戏Astro Bot和Returnal以及多个2D Atari游戏上的实验表明，与仅使用监督动作预测损失函数训练的基线模型相比，改进了表示质量，加快了学习收敛速度，并提高了泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的是在视频游戏环境中，如何让智能体通过高维视觉输入学习更有效的状态表示问题。这个问题在现实中很重要，因为当测试已发布游戏时，往往无法访问游戏的内部状态数据，只能通过视觉输入端到端训练智能体。这种方法需要更大的数据集和更长的训练时间，增加了过拟合风险，因此学习能识别与动作相关关键因素并泛化到新状态的有效表示至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到在视频游戏中，精确的空间配置对决策至关重要，而传统状态表示学习方法虽引入有用归纳偏置，但未能完全实现由智能体动作明确塑造表示的目标。他们借鉴了监督对比学习(SupCon)的思想，但发现其依赖的数据增强会扭曲游戏中的关键空间信息。因此，作者设计出使用动作标签而非数据增强来确定正负样本对的方法，并针对视频游戏中常见的连续和离散混合动作空间提出了具体处理策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在潜在嵌入空间中，根据动作对观察结果进行分组：导致相同动作的观察结果应具有相似表示，而与不同动作相关的观察结果应被很好分离。整体流程包括：1)构建包含特征提取器和策略网络的结构；2)组合预测任务损失和监督对比损失；3)将连续动作空间离散化为多个区间；4)使用混合基数位置编码将多维动作转换为单一分类标签；5)计算SupCon损失，同时优化两种损失函数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将监督对比学习应用于模仿学习领域；2)提出不需要人工视图或正样本对增强的对比学习方法；3)针对连续和离散混合动作空间提出有效处理策略；4)解决小批量中可能不存在正样本对的问题。相比之前工作，这种方法不依赖数据增强避免空间信息扭曲，明确由智能体动作塑造表示而非仅受时间或物理动力学影响，专门针对模仿学习任务和视频游戏环境进行了改进，通过添加SupCon损失作为正则化项改善了表示质量、学习收敛速度和泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过将监督对比学习引入模仿学习领域并针对视频游戏环境特点进行改进，提出了一种能学习更有效状态表示的方法，显著提高了智能体的学习效率、收敛速度和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CoG64752.2025.11114174&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel application of Supervised Contrastive Learning(SupCon) to Imitation Learning (IL), with a focus on learning more effectivestate representations for agents in video game environments. The goal is toobtain latent representations of the observations that capture better theaction-relevant factors, thereby modeling better the cause-effect relationshipfrom the observations that are mapped to the actions performed by thedemonstrator, for example, the player jumps whenever an obstacle appears ahead.We propose an approach to integrate the SupCon loss with continuous outputspaces, enabling SupCon to operate without constraints regarding the type ofactions of the environment. Experiments on the 3D games Astro Bot and Returnal,and multiple 2D Atari games show improved representation quality, fasterlearning convergence, and better generalization compared to baseline modelstrained only with supervised action prediction loss functions.</description>
      <author>example@mail.com (Carlos Celemin, Joseph Brennan, Pierluigi Vito Amadori, Tim Bradley)</author>
      <guid isPermaLink="false">2509.11880v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification</title>
      <link>http://arxiv.org/abs/2509.11587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层身份学习(HIL)框架，用于无监督可见光-红外行人重识别任务，通过多中心对比学习和双向反向选择传输机制解决了现有方法忽略细粒度差异的问题，在SYSU-MM01和RegDB数据集上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督可见光-红外行人重识别(USVI-ReID)旨在从无标注的跨模态行人数据集中学习模态不变图像特征，同时减少模态差距并最小化对昂贵人工标注的依赖。现有方法通常使用基于聚类的对比学习，将一个人表示为单一聚类中心，主要关注每个聚类内图像的共同性，而忽略了它们之间的细粒度差异。&lt;h4&gt;目的&lt;/h4&gt;解决现有无监督可见光-红外行人重识别方法中只关注聚类内图像共同性而忽略细粒度差异的问题，提高跨模态匹配质量。&lt;h4&gt;方法&lt;/h4&gt;提出分层身份学习(HIL)框架，包括：(1)通过二次聚类为每个现有的粗粒度聚类生成多个记忆，反映图像之间的细粒度变化；(2)提出多中心对比学习(MCCL)来优化表示，增强模内聚类并最小化跨模态差异；(3)设计双向反向选择传输(BRST)机制，通过执行伪标签的双向匹配建立可靠的跨模态对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;在SYSU-MM01和RegDB数据集上的大量实验表明，提出的方法优于现有方法，证明了HIL框架在无监督可见光-红外行人重识别任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;分层身份学习框架通过考虑细粒度差异和多中心表示，有效解决了现有无监督可见光-红外行人重识别方法的局限性，提高了跨模态匹配质量，为该领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;无监督可见光-红外行人重识别(USVI-ReID)旨在通过减少模态差距同时最小化对昂贵人工标注的依赖，从未标记的跨模态行人数据集中学习模态不变的图像特征。现有方法通常使用基于聚类的对比学习来解决USVI-ReID，将一个人表示为单一聚类中心。然而，它们主要关注每个聚类内图像的共同性，而忽略了它们之间的细粒度差异。为了解决这一局限性，我们提出了分层身份学习(HIL)框架。由于每个聚类可能包含几个反映图像间细粒度变化的小子聚类，我们通过二次聚类为每个现有的粗粒度聚类生成多个记忆。此外，我们提出了多中心对比学习(MCCL)来优化表示，以增强模内聚类并最小化跨模态差异。为了进一步提高跨模态匹配质量，我们设计了一种双向反向选择传输(BRST)机制，通过执行伪标签的双向匹配建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上进行的大量实验表明，所提出的方法优于现有方法。源代码可在以下网址获取：https://github.com/haonanshi0125/HIL。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised visible-infrared person re-identification (USVI-ReID) aims tolearn modality-invariant image features from unlabeled cross-modal persondatasets by reducing the modality gap while minimizing reliance on costlymanual annotations. Existing methods typically address USVI-ReID usingcluster-based contrastive learning, which represents a person by a singlecluster center. However, they primarily focus on the commonality of imageswithin each cluster while neglecting the finer-grained differences among them.To address the limitation, we propose a Hierarchical Identity Learning (HIL)framework. Since each cluster may contain several smaller sub-clusters thatreflect fine-grained variations among images, we generate multiple memories foreach existing coarse-grained cluster via a secondary clustering. Additionally,we propose Multi-Center Contrastive Learning (MCCL) to refine representationsfor enhancing intra-modal clustering and minimizing cross-modal discrepancies.To further improve cross-modal matching quality, we design a BidirectionalReverse Selection Transmission (BRST) mechanism, which establishes reliablecross-modal correspondences by performing bidirectional matching ofpseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDBdatasets demonstrate that the proposed method outperforms existing approaches.The source code is available at: https://github.com/haonanshi0125/HIL.</description>
      <author>example@mail.com (Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao)</author>
      <guid isPermaLink="false">2509.11587v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness</title>
      <link>http://arxiv.org/abs/2509.11355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出两种互补的正则化策略，通过减少CNN对高频纹理的依赖和促进形状感知表示，提高模型对图像损坏的鲁棒性，同时保持原始分类性能。&lt;h4&gt;背景&lt;/h4&gt;卷积神经网络在图像分类方面表现出色，但对人类容易处理的常见损坏仍然脆弱。这种脆弱性的一个关键原因是CNN依赖局部纹理线索而非全局物体形状，这与人类感知形成鲜明对比。&lt;h4&gt;目的&lt;/h4&gt;提出两种互补的正则化策略，旨在促进形状偏向的表示并增强CNN对图像损坏的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;第一种方法引入辅助损失，强制原始输入和低频滤波输入之间的特征一致性，减少对高频纹理的依赖；第二种方法结合监督对比学习，围绕类一致、形状相关的表示来构建特征空间。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10-C基准测试上评估，两种方法都提高了对图像损坏的鲁棒性，同时没有降低干净数据的准确性。&lt;h4&gt;结论&lt;/h4&gt;损失级别的正则化可以有效引导CNN朝向更形状感知、更有韧性的表示。&lt;h4&gt;翻译&lt;/h4&gt;卷积神经网络在图像分类方面表现出色，但对人类容易处理的常见损坏仍然脆弱。这种脆弱性的一个关键原因是它们依赖局部纹理线索而非全局物体形状——这与人类感知形成鲜明对比。为此，我们提出两种互补的正则化策略，旨在促进形状偏向的表示并增强鲁棒性。第一种引入辅助损失，强制原始输入和低频滤波输入之间的特征一致性，减少对高频纹理的依赖。第二种结合监督对比学习，围绕类一致、形状相关的表示来构建特征空间。在CIFAR-10-C基准测试上评估，两种方法都提高了对损坏的鲁棒性，同时没有降低干净数据的准确性。我们的结果表明，损失级别的正则化可以有效引导CNN朝向更形状感知、更有韧性的表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Convolutional Neural Networks (CNNs) excel at image classification but remainvulnerable to common corruptions that humans handle with ease. A key reason forthis fragility is their reliance on local texture cues rather than globalobject shapes -- a stark contrast to human perception. To address this, wepropose two complementary regularization strategies designed to encourageshape-biased representations and enhance robustness. The first introduces anauxiliary loss that enforces feature consistency between original andlow-frequency filtered inputs, discouraging dependence on high-frequencytextures. The second incorporates supervised contrastive learning to structurethe feature space around class-consistent, shape-relevant representations.Evaluated on the CIFAR-10-C benchmark, both methods improve corruptionrobustness without degrading clean accuracy. Our results suggest thatloss-level regularization can effectively steer CNNs toward more shape-aware,resilient representations.</description>
      <author>example@mail.com (Robin Narsingh Ranabhat, Longwei Wang, Amit Kumar Patel, KC santosh)</author>
      <guid isPermaLink="false">2509.11355v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Network Representation Learning</title>
      <link>http://arxiv.org/abs/2509.11316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自适应对比边表示学习(ACERL)的新方法，用于处理脑连接数据分析中的挑战，包括个体特异性、高维性和稀疏性网络。该方法基于对比学习和自适应随机掩码机制，在边表示学习中达到最小最优收敛率，并在多种下游任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;网络表示学习旨在将网络嵌入低维空间同时保留结构和语义属性，但脑连接数据分析面临个体特异性、高维性和稀疏性网络的挑战，且缺乏节点或边协变量。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于对比学习的统计方法用于网络边嵌入，解决脑连接数据分析中的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出自适应对比边表示学习(ACERL)，包含两个关键组件：增强网络对的对比学习和数据驱动的自适应随机掩码机制，并建立非渐近误差边界。&lt;h4&gt;主要发现&lt;/h4&gt;ACERL在边表示学习中达到最小最优收敛率，学习到的表示可用于网络分类、重要边检测和社区检测等下游任务，且对这些任务有理论保证。通过合成数据和真实脑连接研究验证了该方法，与稀疏主成分分析相比具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;ACERL方法能有效处理脑连接数据分析中的挑战，在多种任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;网络表示学习寻求将网络嵌入到低维空间，同时保留结构和语义属性，从而促进下游任务，如分类、特征预测、边识别和社区检测。受脑连接数据分析挑战的启发，这些数据具有个体特异性、高维性和稀疏性网络的特点，且缺乏节点或边协变量，我们提出了一种新颖的基于对比学习的统计方法用于网络边嵌入，命名为自适应对比边表示学习(ACERL)。它基于两个关键组件：增强网络对的对比学习和数据驱动的自适应随机掩码机制。我们建立了非渐近误差边界，并证明我们的方法在边表示学习中达到了最小最优收敛率。我们进一步展示了学习到的表示在多种下游任务中的适用性，包括网络分类、重要边检测和社区检测，并建立了相应的理论保证。我们通过合成数据和真实脑连接研究验证了我们的方法，并显示出与稀疏主成分分析基线方法相比具有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network representation learning seeks to embed networks into alow-dimensional space while preserving the structural and semantic properties,thereby facilitating downstream tasks such as classification, trait prediction,edge identification, and community detection. Motivated by challenges in brainconnectivity data analysis that is characterized by subject-specific,high-dimensional, and sparse networks that lack node or edge covariates, wepropose a novel contrastive learning-based statistical approach for networkedge embedding, which we name as Adaptive Contrastive Edge RepresentationLearning (ACERL). It builds on two key components: contrastive learning ofaugmented network pairs, and a data-driven adaptive random masking mechanism.We establish the non-asymptotic error bounds, and show that our method achievesthe minimax optimal convergence rate for edge representation learning. Wefurther demonstrate the applicability of the learned representation in multipledownstream tasks, including network classification, important edge detection,and community detection, and establish the corresponding theoreticalguarantees. We validate our method through both synthetic data and real brainconnectivities studies, and show its competitive performance compared to thebaseline method of sparse principal components analysis.</description>
      <author>example@mail.com (Zihan Dong, Xin Zhou, Ryumei Nakada, Lexin Li, Linjun Zhang)</author>
      <guid isPermaLink="false">2509.11316v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment</title>
      <link>http://arxiv.org/abs/2509.11135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AlignKT的新方法，用于知识追踪(KT)，采用前端到后端架构明确建模稳定知识状态，通过对比学习增强对齐鲁棒性，实验证明在三个数据集上优于七种基线方法。&lt;h4&gt;背景&lt;/h4&gt;知识追踪(KT)是智能教学系统(ITS)的基本组件，通过建模学习者知识状态监控学习进度。然而现有KT模型主要关注拟合交互序列，忽视知识状态本身，导致可解释性降低和教学支持不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有KT模型可解释性不足和教学支持不够的问题，通过明确建模稳定知识状态提高KT模型性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;AlignKT采用前端到后端架构，定义基于教学理论的理想知识状态作为对齐标准，使用五个编码器实现，并加入对比学习模块增强对齐过程的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验，AlignKT在三个真实数据集上优于七种KT基线方法，在两个数据集上达到最先进结果，在第三个数据集上也表现具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;AlignKT通过明确建模知识状态并引入对齐标准，成功提高了KT模型的性能和可解释性，为智能教学系统提供了更好的教学支持。&lt;h4&gt;翻译&lt;/h4&gt;知识追踪(KT)是智能教学系统(ITS)的基本组成部分，通过建模学习者的知识状态，使这些系统能够监控和理解学习者的进步。然而，许多现有的KT模型主要关注拟合学习者交互序列，而常常忽视知识状态本身。这一限制降低了可解释性，并为ITS提供了不足的教学支持。为了应对这一挑战，我们提出了AlignKT，它采用前端到后端的架构来明确建模稳定的知识状态。在此方法中，初步知识状态与额外的标准对齐。具体来说，我们基于教学理论定义了一个理想知识状态作为对齐标准，为可解释性提供了基础。我们使用五个编码器来实现此设置，并采用对比学习模块来增强对齐过程的鲁棒性。通过大量实验，AlignKT表现出优越的性能，在三个真实数据集上优于七种KT基线方法。它在其中两个数据集上取得了最先进的结果，在第三个数据集上也表现出具有竞争力的性能。本工作的代码可在https://github.com/SCNU203/AlignKT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Tracing (KT) serves as a fundamental component of IntelligentTutoring Systems (ITS), enabling these systems to monitor and understandlearners' progress by modeling their knowledge state. However, many existing KTmodels primarily focus on fitting the sequences of learners' interactions, andoften overlook the knowledge state itself. This limitation leads to reducedinterpretability and insufficient instructional support from the ITS. Toaddress this challenge, we propose AlignKT, which employs a frontend-to-backendarchitecture to explicitly model a stable knowledge state. In this approach,the preliminary knowledge state is aligned with an additional criterion.Specifically, we define an ideal knowledge state based on pedagogical theoriesas the alignment criterion, providing a foundation for interpretability. Weutilize five encoders to implement this set-up, and incorporate a contrastivelearning module to enhance the robustness of the alignment process. Throughextensive experiments, AlignKT demonstrates superior performance, outperformingseven KT baselines on three real-world datasets. It achieves state-of-the-artresults on two of these datasets and exhibits competitive performance on thethird. The code of this work is available athttps://github.com/SCNU203/AlignKT.</description>
      <author>example@mail.com (Jing Xiao, Chang You, Zhiyu Chen)</author>
      <guid isPermaLink="false">2509.11135v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SPARK: Adaptive Low-Rank Knowledge Graph Modeling in Hybrid Geometric Spaces for Recommendation</title>
      <link>http://arxiv.org/abs/2509.11094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM' 25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SPARK，一个多阶段框架，用于解决知识图谱增强推荐系统中的噪声、稀疏性和几何表示问题，特别关注长尾项目的推荐效果。&lt;h4&gt;背景&lt;/h4&gt;知识图谱增强推荐系统面临固有噪声、数据稀疏性和欧几里得几何不适合复杂关系结构的挑战，这些问题损害了表示学习，特别是对于长尾实体。现有方法通常缺乏针对项目流行度的自适应多源信号融合。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效处理知识图谱噪声和稀疏性，同时为长尾项目提供更好表示的推荐系统框架，特别关注主流和长尾项目的精确建模。&lt;h4&gt;方法&lt;/h4&gt;SPARK框架采用多阶段方法：首先使用Tucker低秩分解去噪知识图谱并生成鲁棒实体表示；然后采用SVD初始化的混合几何图神经网络同时在欧几里得和双曲空间中学习表示，利用双曲空间建模层次结构的能力；引入项目流行度感知的自适应融合策略，动态加权来自协作过滤、精炼知识图谱嵌入和不同几何空间的信号；最后使用对比学习对齐多源表示。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，SPARK在最新的方法上表现出显著的优越性，特别是在改善长尾项目推荐方面，为知识增强推荐提供了一种稳健且原则性的方法。&lt;h4&gt;结论&lt;/h4&gt;SPARK框架通过系统性地解决知识图谱增强推荐系统中的关键挑战，特别是长尾项目的表示问题，提供了一个创新且有效的解决方案，实现了主流和长尾项目的精确建模。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱增强推荐系统，但面临固有噪声、稀疏性和欧几里得几何不适合复杂关系结构的挑战，严重损害了表示学习，特别是对于长尾实体。现有方法通常也缺乏针对项目流行度的自适应多源信号融合。本文介绍了SPARK，一个新颖的多阶段框架，系统性地解决这些问题。SPARK首先采用Tucker低秩分解对知识图谱去噪并生成鲁棒实体表示。随后，一个SVD初始化的混合几何图神经网络同时在欧几里得和双曲空间中学习表示；后者被战略性地利用，因为它擅长建模层次结构，有效捕获稀疏长尾项目的语义特征。核心贡献是一个项目流行度感知的自适应融合策略，动态加权来自协作过滤、精炼知识图谱嵌入和不同几何空间的信号，精确建模主流和长尾项目。最后，对比学习对齐这些多源表示。大量实验证明了SPARK在最新方法上的显著优越性，特别是在改善长尾项目推荐方面，为知识增强推荐提供了一种稳健且原则性的方法。实现代码可在https://github.com/Applied-Machine-Learning-Lab/SPARK获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Graphs (KGs) enhance recommender systems but face challenges frominherent noise, sparsity, and Euclidean geometry's inadequacy for complexrelational structures, critically impairing representation learning, especiallyfor long-tail entities. Existing methods also often lack adaptive multi-sourcesignal fusion tailored to item popularity. This paper introduces SPARK, a novelmulti-stage framework systematically tackling these issues. SPARK first employsTucker low-rank decomposition to denoise KGs and generate robust entityrepresentations. Subsequently, an SVD-initialized hybrid geometric GNNconcurrently learns representations in Euclidean and Hyperbolic spaces; thelatter is strategically leveraged for its aptitude in modeling hierarchicalstructures, effectively capturing semantic features of sparse, long-tail items.A core contribution is an item popularity-aware adaptive fusion strategy thatdynamically weights signals from collaborative filtering, refined KGembeddings, and diverse geometric spaces for precise modeling of bothmainstream and long-tail items. Finally, contrastive learning aligns thesemulti-source representations. Extensive experiments demonstrate SPARK'ssignificant superiority over state-of-the-art methods, particularly inimproving long-tail item recommendation, offering a robust, principled approachto knowledge-enhanced recommendation. Implementation code is available athttps://github.com/Applied-Machine-Learning-Lab/SPARK.</description>
      <author>example@mail.com (Binhao Wang, Yutian Xiao, Maolin Wang, Zhiqi Li, Tianshuo Wei, Ruocheng Guo, Xiangyu Zhao)</author>
      <guid isPermaLink="false">2509.11094v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data</title>
      <link>http://arxiv.org/abs/2509.11053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对有限数据条件下轴承故障诊断的先进数据增强和对比傅里叶卷积框架(DAC-FCF)，通过结合新型生成对抗网络、对比学习和傅里叶卷积神经网络，有效解决了数据稀缺、特征提取不充分和样本关系建模不足等问题。&lt;h4&gt;背景&lt;/h4&gt;在轴承故障诊断领域，深度学习方法被广泛应用，但由于高成本或隐私问题，实际场景中高质量的标记数据稀缺。虽然少样本学习在解决数据稀缺问题上显示出潜力，但现有方法仍面临显著限制。&lt;h4&gt;目的&lt;/h4&gt;解决传统数据增强技术生成低质量样本、传统卷积神经网络难以提取全局特征以及现有方法无法建模有限训练样本间复杂关系等问题，为有限数据条件下的轴承故障诊断提供有效解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出DAC-FCF框架，包含三个核心组件：1) 条件一致潜在表示和重建生成对抗网络(CCLR-GAN)用于生成更多样化的数据；2) 基于对比学习的联合优化机制用于建模训练数据间的关系；3) 一维傅里叶卷积神经网络(1D-FCNN)用于实现输入数据的全局感知。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明DAC-FCF取得了显著改进，在CWRU数据集上比基线方法提高最多32%，在自收集测试台上提高10%。大量的消融实验证明了所提出组件的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的DAC-FCF为有限数据条件下的轴承故障诊断提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在轴承故障诊断领域，深度学习方法最近已被广泛应用。然而，由于高成本或隐私问题，实际场景中高质量的标记数据稀缺。虽然少样本学习在解决数据稀缺问题上显示出潜力，但现有方法在此领域仍面临显著限制。传统数据增强技术常遭受模式崩溃，生成无法捕捉轴承故障模式多样性的低质量样本。此外，具有局部感受野的传统卷积神经网络不足以从复杂振动信号中提取全局特征。另外，现有方法无法建模有限训练样本之间的复杂关系。为解决这些问题，我们提出了一种针对有限数据条件下轴承故障诊断的先进数据增强和对比傅里叶卷积框架(DAC-FCF)。首先，提出了一种新型的条件一致潜在表示和重建生成对抗网络(CCLR-GAN)来生成更多样化的数据。其次，利用基于对比学习的联合优化机制来更好地建模可用训练数据之间的关系。最后，我们提出了一维傅里叶卷积神经网络(1D-FCNN)来实现对输入数据的全局感知。实验证明DAC-FCF取得了显著改进，在凯斯西储大学(CWRU)数据集上比基线方法提高最多32%，在自收集测试台上提高10%。大量的消融实验证明了所提出组件的有效性。因此，提出的DAC-FCF为有限数据条件下的轴承故障诊断提供了有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the area of bearing fault diagnosis, deep learning (DL) methods have beenwidely used recently. However, due to the high cost or privacy concerns,high-quality labeled data are scarce in real world scenarios. While few-shotlearning has shown promise in addressing data scarcity, existing methods stillface significant limitations in this domain. Traditional data augmentationtechniques often suffer from mode collapse and generate low-quality samplesthat fail to capture the diversity of bearing fault patterns. Moreover,conventional convolutional neural networks (CNNs) with local receptive fieldsmakes them inadequate for extracting global features from complex vibrationsignals. Additionally, existing methods fail to model the intricaterelationships between limited training samples. To solve these problems, wepropose an advanced data augmentation and contrastive fourier convolutionframework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, anovel conditional consistent latent representation and reconstructiongenerative adversarial network (CCLR-GAN) is proposed to generate more diversedata. Secondly, a contrastive learning based joint optimization mechanism isutilized to better model the relations between the available training data.Finally, we propose a 1D fourier convolution neural network (1D-FCNN) toachieve a global-aware of the input data. Experiments demonstrate that DAC-FCFachieves significant improvements, outperforming baselines by up to 32\% oncase western reserve university (CWRU) dataset and 10\% on a self-collectedtest bench. Extensive ablation experiments prove the effectiveness of theproposed components. Thus, the proposed DAC-FCF offers a promising solution forbearing fault diagnosis under limited data.</description>
      <author>example@mail.com (Shengke Sun, Shuzhen Han, Ziqian Luan, Xinghao Qin, Jiao Yin, Zhanshan Zhao, Jinli Cao, Hua Wang)</author>
      <guid isPermaLink="false">2509.11053v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds</title>
      <link>http://arxiv.org/abs/2509.10842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpenUrban3D是首个不依赖对齐多视图图像、预训练点云分割网络或手动标注的3D开放词汇语义分割框架，可直接从原始点云生成语义特征，实现任意文本查询的零样本分割。&lt;h4&gt;背景&lt;/h4&gt;开放词汇语义分割能识别和分割来自任意自然语言描述的对象，对大规模城市点云应用至关重要，但该领域研究不足，主要受限于高质量多视图图像的缺乏和现有3D分割方法的泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;解决大规模城市点云数据集中缺乏高质量多视图图像和现有3D分割方法泛化能力差的问题，开发一个不依赖对齐多视图图像、预训练网络或手动标注的3D开放词汇语义分割框架。&lt;h4&gt;方法&lt;/h4&gt;通过多视图多粒度渲染、掩码级视觉-语言特征提取和样本平衡融合，直接从原始点云生成语义特征，然后蒸馏到3D骨干模型，实现零样本分割并保留语义和几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;在SensatUrban和SUM等大规模城市基准测试上，OpenUrban3D在分割精度和跨场景泛化能力上显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;OpenUrban3D为大规模城市场景理解提供了灵活且可扩展的解决方案，无需依赖对齐多视图图像或预训练模型。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇语义分割使模型能够识别和分割来自任意自然语言描述的对象，提供了处理固定标签集之外的新的、细粒度的或功能定义的类别的灵活性。虽然这种能力对于支持数字孪生、智能城市管理和城市分析的大规模城市点云至关重要，但在这个领域仍 largely unexplored。主要障碍是大规模城市点云数据集中经常缺乏高质量、对齐良好的多视图图像，以及现有的三维分割管道在几何、尺度和外观差异大的多样化城市环境中泛化能力差。为了解决这些挑战，我们提出了OpenUrban3D，这是第一个大规模城市场景的3D开放词汇语义分割框架，它可以在没有对齐多视图图像、预训练点云分割网络或手动标注的情况下运行。我们的方法通过多视图、多粒度渲染、掩码级视觉-语言特征提取和样本平衡融合，直接从原始点云生成强大的语义特征，然后将其蒸馏到3D骨干模型中。这种设计能够实现任意文本查询的零样本分割，同时捕获语义丰富性和几何先验。在SensatUrban和SUM等大规模城市基准测试上的广泛实验表明，OpenUrban3D在分割精度和跨场景泛化方面都显著优于现有方法，证明了其作为3D城市场景理解的灵活且可扩展解决方案的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决大规模城市点云的开集词汇语义分割问题，具体包括两个关键挑战：1) 大规模城市点云数据集常缺乏高质量对齐的多视图图像；2) 现有3D分割管道在不同城市环境中的泛化能力差。这个问题在现实中非常重要，因为城市点云支持数字孪生、智能城市管理等应用，而传统方法依赖预定义封闭类别标签，导致标注成本高昂且无法处理未见过的类别或功能定义区域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：室内场景的开集词汇方法依赖对齐的2D图像，而城市点云数据缺乏这种图像；'先分割后识别'策略在城市场景中表现不佳。因此设计了无需对齐图像、预训练网络或手动标注的框架。方法借鉴了OpenScene的2D到3D知识蒸馏策略，利用2D视觉-语言模型(如CLIP)进行特征提取，并参考了'先分割后识别'的范式，但针对城市场景进行了改进，设计了专门的多视图多粒度投影方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接从原始点云生成鲁棒语义特征，不依赖对齐图像或预训练3D分割网络，通过多视图渲染捕获不同尺度对象，利用掩码级视觉-语言特征提取和样本平衡融合，然后将知识蒸馏到3D模型。整体流程包括：1)多视图多粒度投影生成虚拟图像；2)提取2D掩码特征并反投影到点云；3)样本平衡特征融合构建2D特征库；4)2D到3D知识蒸馏训练3D主干；5)推理时融合2D-3D特征，通过余弦相似度实现任意文本查询的分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个仅需要原始点云输入的大规模城市3D开集词汇语义分割框架；2)多视图多粒度投影、掩码级特征提取和2D-3D知识蒸馏的定制化技术流程；3)样本平衡特征融合(SBFF)解决数据不平衡问题。相比之前工作，OpenUrban3D不依赖对齐RGB图像或预训练3D分割网络，能处理大规模物体尺度变化，在跨场景泛化方面表现更好，并通过2D-3D特征融合结合了语义识别能力和几何先验。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenUrban3D首次实现了无需对齐图像、预训练分割网络或手动标注的大规模城市点云开集词汇语义分割，通过多视图多粒度渲染、掩码级特征提取和2D-3D知识蒸馏，显著提升了分割精度和跨场景泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary semantic segmentation enables models to recognize and segmentobjects from arbitrary natural language descriptions, offering the flexibilityto handle novel, fine-grained, or functionally defined categories beyond fixedlabel sets. While this capability is crucial for large-scale urban point cloudsthat support applications such as digital twins, smart city management, andurban analytics, it remains largely unexplored in this domain. The mainobstacles are the frequent absence of high-quality, well-aligned multi-viewimagery in large-scale urban point cloud datasets and the poor generalizationof existing three-dimensional (3D) segmentation pipelines across diverse urbanenvironments with substantial variation in geometry, scale, and appearance. Toaddress these challenges, we present OpenUrban3D, the first 3D open-vocabularysemantic segmentation framework for large-scale urban scenes that operateswithout aligned multi-view images, pre-trained point cloud segmentationnetworks, or manual annotations. Our approach generates robust semanticfeatures directly from raw point clouds through multi-view, multi-granularityrendering, mask-level vision-language feature extraction, and sample-balancedfusion, followed by distillation into a 3D backbone model. This design enableszero-shot segmentation for arbitrary text queries while capturing both semanticrichness and geometric priors. Extensive experiments on large-scale urbanbenchmarks, including SensatUrban and SUM, show that OpenUrban3D achievessignificant improvements in both segmentation accuracy and cross-scenegeneralization over existing methods, demonstrating its potential as a flexibleand scalable solution for 3D urban scene understanding.</description>
      <author>example@mail.com (Chongyu Wang, Kunlei Jing, Jihua Zhu, Di Wang)</author>
      <guid isPermaLink="false">2509.10842v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks</title>
      <link>http://arxiv.org/abs/2509.12151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可学习的物理模拟器，能够准确预测机器人在接触丰富操作中末端执行器的运动和力矩。该模型扩展了现有的GNN-based模拟器，实现了条件动作预测，并在模拟和真实世界实验中表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有的物理模拟器在处理接触丰富的操作任务时存在局限性，特别是在预测机器人末端执行器的运动和力矩方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测机器人末端执行器在接触丰富操作中的运动和力矩的可学习物理模拟器，提高预测精度和性能。&lt;h4&gt;方法&lt;/h4&gt;扩展了基于图神经网络(GNN)的最先进模拟器FIGNET，引入了新的节点和边类型，实现了对控制任务和状态估计任务的条件动作预测。在模拟中使用MPC代理进行测试，并在真实世界实验中验证性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在模拟中，使用该模型的MPC代理在孔插入任务中与使用真实动力学模型的控制器性能相当。2. 在真实世界实验中，该模型相比基线物理模拟器，运动预测准确率提高了50%，力矩预测精度提高了3倍。3. 源代码和数据已公开可用。&lt;h4&gt;结论&lt;/h4&gt;所提出的可学习物理模拟器在机器人末端执行器的运动和力矩预测方面表现出色，能够有效支持控制任务和状态估计任务，且源代码和数据已公开，便于进一步研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种可学习的物理模拟器，能够准确预测机器人在接触丰富操作中末端执行器的运动和力矩。所提出的模型通过引入新的节点和边类型扩展了最先进的基于图神经网络的模拟器(FIGNet)，实现了对控制任务和状态估计任务的条件动作预测。在模拟中，使用我们模型的MPC代理在具有挑战性的孔插入任务中与使用真实动力学模型的相同控制器性能相匹配；而在真实世界实验中，我们的模型相比基线物理模拟器，运动预测准确率提高了50%，力矩预测精度提高了3倍。源代码和数据已公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a learnable physics simulator that provides accurate motion andforce-torque prediction of robot end effectors in contact-rich manipulation.The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)with novel node and edge types, enabling action-conditional predictions forcontrol and state estimation tasks. In simulation, the MPC agent using ourmodel matches the performance of the same controller with the ground truthdynamics model in a challenging peg-in-hole task, while in the real-worldexperiment, our model achieves a 50% improvement in motion prediction accuracyand 3$\times$ increase in force-torque prediction precision over the baselinephysics simulator. Source code and data are publicly available.</description>
      <author>example@mail.com (Zongyao Yi, Joachim Hertzberg, Martin Atzmueller)</author>
      <guid isPermaLink="false">2509.12151v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Visual Autonomous Parking via Control-Aided Attention</title>
      <link>http://arxiv.org/abs/2509.11090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CAA-Policy的端到端模仿学习系统，通过控制辅助注意力机制(CAA)解决了精确停车中感知和控制协同不足的问题，实现了更准确、鲁棒且可解释的停车决策。&lt;h4&gt;背景&lt;/h4&gt;精确停车需要一个端到端系统，感知系统需自适应提供与策略相关的细节，特别是在需要精细控制决策的关键区域。现有的端到端学习方法在感知和控制之间缺乏有效的协同作用，且transformer-based自注意力机制单独使用时往往产生不稳定和不一致的空间注意力。&lt;h4&gt;目的&lt;/h4&gt;解决现有端到端学习方法中感知和控制协同不足的问题，提高停车系统的准确性、鲁棒性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出CAA-Policy系统，引入新颖的Control-Aided Attention (CAA)机制，允许控制信号指导视觉注意力的学习；首次以自监督方式训练注意力模块，使用来自控制输出的反向传播梯度而非训练损失；集成短时程航点预测作为辅助任务；引入单独训练的运动预测模块以稳健跟踪目标位置。&lt;h4&gt;主要发现&lt;/h4&gt;仅基于transformer的自注意力机制会产生不稳定和时间上不一致的空间注意力，削弱下游策略决策的可靠性；使用控制输出的梯度训练注意力可促使注意力集中在导致动作输出高方差的视觉特征上，而非仅最小化训练损失；这种转变带来了更稳健和可推广的策略。&lt;h4&gt;结论&lt;/h4&gt;在CARLA模拟器中的大量实验表明，CAA-Policy持续优于端到端学习基线和模块化的BEV分割+混合A*流水线，实现了更高的准确性、鲁棒性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;精确停车需要一个端到端系统，其中感知系统自适应地提供与策略相关的细节-特别是在需要精细控制决策的关键区域。端到端学习通过直接将传感器输入映射到控制动作提供了一个统一的框架，但现有方法在感知和控制之间缺乏有效的协同作用。我们发现，单独使用基于transformer的自注意力机制往往会产生不稳定且时间上不一致的空间注意力，这随时间削弱了下游策略决策的可靠性。相反，我们提出了CAA-Policy，一个端到端的模仿学习系统，通过新颖的控制辅助注意力(CAA)机制允许控制信号指导视觉注意力的学习。我们首次以自监督方式训练这样的注意力模块，使用从控制输出反向传播的梯度而非来自训练损失的梯度。这种策略鼓励注意力集中在导致动作输出高方差的视觉特征上，而不仅仅是最小化训练损失-我们证明这种转变带来了更稳健和可推广的策略。为进一步增强稳定性，CAA-Policy集成了短时程航点预测作为辅助任务，并引入了单独训练的运动预测模块以随时间稳健地跟踪目标位置。在CARLA模拟器中的大量实验表明，CAA-Policy持续优于端到端学习基线和模块化的BEV分割+混合A*流水线，实现了更高的准确性、鲁棒性和可解释性。代码已在https://github.com/Joechencc/CAAPolicy发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Precise parking requires an end-to-end system where perception adaptivelyprovides policy-relevant details-especially in critical areas where finecontrol decisions are essential. End-to-end learning offers a unified frameworkby directly mapping sensor inputs to control actions, but existing approacheslack effective synergy between perception and control. We find thattransformer-based self-attention, when used alone, tends to produce unstableand temporally inconsistent spatial attention, which undermines the reliabilityof downstream policy decisions over time. Instead, we propose CAA-Policy, anend-to-end imitation learning system that allows control signal to guide thelearning of visual attention via a novel Control-Aided Attention (CAA)mechanism. For the first time, we train such an attention module in aself-supervised manner, using backpropagated gradients from the control outputsinstead of from the training loss. This strategy encourages the attention tofocus on visual features that induce high variance in action outputs, ratherthan merely minimizing the training loss-a shift we demonstrate leads to a morerobust and generalizable policy. To further enhance stability, CAA-Policyintegrates short-horizon waypoint prediction as an auxiliary task, andintroduces a separately trained motion prediction module to robustly track thetarget spot over time. Extensive experiments in the CARLA simulator show that\titlevariable~consistently surpasses both the end-to-end learning baseline andthe modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,robustness, and interpretability. Code is released athttps://github.com/Joechencc/CAAPolicy.</description>
      <author>example@mail.com (Chao Chen, Shunyu Yao, Yuanwu He, Tao Feng, Ruojing Song, Yuliang Guo, Xinyu Huang, Chenxu Wu, Ren Liu, Chen Feng)</author>
      <guid isPermaLink="false">2509.11090v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</title>
      <link>http://arxiv.org/abs/2509.12197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对从野外LiDAR点云进行3D人体姿态估计和人体网格恢复的研究进行了全面综述，提出了结构化分类方法，并分析了各种方法的优缺点。&lt;h4&gt;背景&lt;/h4&gt;从野外LiDAR点云进行3D人体姿态估计和人体网格恢复是计算机视觉领域的重要研究方向。&lt;h4&gt;目的&lt;/h4&gt;对现有方法进行系统比较，提出分类框架，分析方法的优缺点，并建立评估基准以促进该领域的发展。&lt;h4&gt;方法&lt;/h4&gt;对三个常用数据集进行定量比较，编写评估指标的统一定义，在数据集上建立两个任务的基准表格，维护一个配套网页持续更新研究。&lt;h4&gt;主要发现&lt;/h4&gt;提出了结构化分类方法，分析了各种方法的优缺点和设计选择，建立了评估基准，明确了开放挑战和研究方向。&lt;h4&gt;结论&lt;/h4&gt;基于LiDAR的3D人体理解仍面临开放挑战，需要进一步研究，作者提供了持续更新的资源平台。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们对从野外LiDAR点云进行3D人体姿态估计和人体网格恢复的研究进行了全面综述。我们在多个关键维度上比较了现有方法，并提出了一个结构化的分类方法来对这些方法进行分类。按照这一分类方法，我们分析了每种方法的优点、局限性和设计选择。此外，(i)我们对三个最常用的数据集进行了定量比较，详细描述了它们的特征；(ii)我们编写了所有评估指标的统一定义；(iii)我们在这些数据集上为两个任务建立了基准表格，以实现公平比较并促进该领域的进步。我们还概述了推动基于LiDAR的3D人体理解的关键开放挑战和研究方向。此外，我们维护了一个配套网页，根据我们的分类组织论文并持续更新新研究：https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决对从LiDAR点云进行3D人体姿态估计和人体网格恢复的全面综述问题。这个问题在现实中非常重要，因为这些技术在自动驾驶、虚拟现实、人机交互、医疗保健等领域有广泛应用，能帮助系统准确理解和预测人类行为，提高安全性和交互体验。LiDAR传感器提供精确的3D几何信息，不受光照影响，且保护隐私，是这些应用的关键技术。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性分析现有文献，提出一个结构化的分类法来组织各种方法，并分析它们的优缺点。作者借鉴了先前关于3D人体姿态估计和人体网格恢复的综述工作，但发现现有综述主要关注图像或视频方法，缺乏对LiDAR传感器的专门关注。因此，作者填补了这一空白，专注于从LiDAR点云进行3D人体理解的方法，并整合了多传感器融合、弱监督学习等先进技术思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 作为综述论文，其核心思想是提供一个全面的框架来理解和分类从LiDAR点云进行3D人体姿态估计和人体网格恢复的方法。整体流程包括：介绍问题背景和重要性；分析传感器技术特别是LiDAR的特点；提出分类法组织现有方法；按监督类型（监督、弱监督、无监督）分析3D人体姿态估计方法；讨论人体网格恢复方法；评估常用数据集；统一评估指标定义；建立基准测试；指出未来挑战和方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门针对LiDAR点云的3D人体理解进行全面综述；提出结构化分类法；对三个主要数据集进行定量比较；统一评估指标定义；建立基准测试表；维护持续更新的配套网页。相比之前工作，这篇综述专注于LiDAR而非图像/视频；针对户外'野外'场景而非受控环境；包含最新研究成果（至2025年）；提供更全面的方法分类和实际应用基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性的综述、分类和基准测试，显著推进了从LiDAR点云进行3D人体姿态估计和人体网格恢复领域的研究与应用，为该领域提供了清晰的发展路线图。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a comprehensive review of 3D human pose estimationand human mesh recovery from in-the-wild LiDAR point clouds. We compareexisting approaches across several key dimensions, and propose a structuredtaxonomy to classify these methods. Following this taxonomy, we analyze eachmethod's strengths, limitations, and design choices. In addition, (i) weperform a quantitative comparison of the three most widely used datasets,detailing their characteristics; (ii) we compile unified definitions of allevaluation metrics; and (iii) we establish benchmark tables for both tasks onthese datasets to enable fair comparisons and promote progress in the field. Wealso outline open challenges and research directions critical for advancingLiDAR-based 3D human understanding. Moreover, we maintain an accompanyingwebpage that organizes papers according to our taxonomy and continuously updateit with new studies:https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</description>
      <author>example@mail.com (Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet)</author>
      <guid isPermaLink="false">2509.12197v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2509.11853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SDI-GS的新方法，通过区域分割识别和保留结构上重要的区域，减少3D高斯点数量，同时保持场景保真度，提高稀疏视图合成的效率和实用性。&lt;h4&gt;背景&lt;/h4&gt;稀疏视图合成具有挑战性，因为从有限观测中恢复准确几何和外观困难。现有3D高斯散射方法通常依赖运动结构(SfM)进行相机姿态估计，这在真正稀疏视图设置中表现不佳。无SfM方法虽用多视图立体视觉替代，但通过反向投影所有像素生成大量3D高斯点，导致高内存成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，在稀疏视图条件下高效生成高质量3D场景表示，同时减少内存使用和提高训练速度。&lt;h4&gt;方法&lt;/h4&gt;提出分割驱动的Gaussian Splatting初始化(SDI-GS)，利用基于区域的分割识别和保留结构上重要的区域，实现对密集点云的选择性下采样，保持场景保真度的同时显著减少高斯点数量。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SDI-GS可减少高达50%的高斯点数量，在PSNR和SSIM指标上实现相当或更优的渲染质量，LPIPS指标仅有轻微下降。该方法还实现了更快的训练速度和更低的内存占用。&lt;h4&gt;结论&lt;/h4&gt;SDI-GS推进了3D高斯散射在受限视图场景中的实用性，通过减少内存需求和提高训练效率，使3DGS更适合实际应用。&lt;h4&gt;翻译&lt;/h4&gt;稀疏视图合成仍然是一个具有挑战性的问题，因为从有限的观测中恢复准确的几何和外观很困难。虽然3D高斯散射(3DGS)的最新进展实现了具有竞争力的质量的实时渲染，但现有流程通常依赖于运动结构(SfM)进行相机姿态估计，这种方法在真正稀疏视图设置中表现不佳。此外，一些无SfM的方法用多视图立体视觉(MVS)模型替代SfM，但通过将每个像素反向投影到3D空间来生成大量3D高斯点，导致高内存成本。我们提出了分割驱动的Gaussian Splatting初始化(SDI-GS)，一种通过利用基于区域的分割来识别和保留结构上重要的区域，从而缓解低效率问题的方法。这实现了对密集点云的选择性下采样，在保持场景保真度的同时显著减少高斯点数量。跨不同基准的实验表明，SDI-GS可减少高达50%的高斯点数量，并在PSNR和SSIM上实现相当或更优的渲染质量，同时LPIPS指标仅有轻微下降。它还实现了更快的训练速度和更低的内存占用，推进了3DGS在受限视图场景中的实用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决稀疏视角3D高斯溅射（3DGS）中的初始化效率问题。传统方法依赖运动恢复结构（SfM）在稀疏视角下不稳定，而无SfM方法则生成过多高斯分布导致高内存成本。这个问题在机器人、增强现实和医疗成像等实际应用中至关重要，因为在这些场景中获取密集视图不切实际，限制了3DGS在资源受限场景中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有SfM方法在稀疏视角下不可靠，而无SfM方法效率低下。他们借鉴了自己在2D高斯回归方面的先前工作，证明基于区域的分割可有效减少冗余同时保留重要结构。他们将这种分割驱动范式从2D扩展到3D高斯溅射领域，利用跨视图区域一致性指导选择性下采样。具体借鉴了MASt3R进行姿态估计和点云生成，以及使用修改的DBSCAN算法（MDBSCAN）进行高效区域分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用基于区域的分割识别并保留结构上重要的区域，对密集点云进行选择性下采样，在保持场景保真度的同时减少高斯数量。整体流程包括：1)使用MASt3R估计相机姿态和生成密集点云；2)对每个视图执行基于区域的分割；3)构建感知3D标记向量来识别跨视图一致区域；4)在每个结构聚类内进行分层采样；5)用下采样点初始化3D高斯并联合优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：分割驱动的初始化策略减少高斯数量达50%；高效的表示学习保持高质量渲染；更快的训练速度和更低内存占用；利用跨视图一致性识别有意义区域。相比SfM方法，避免了稀疏视角下的不稳定性；相比其他SfM-free方法，使用智能分割指导下采样而非简单置信度过滤；相比训练期使用分割的方法，在初始化阶段使用轻量级区域分割，不依赖语义标签或逐步密集化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种分割驱动的初始化方法，通过基于区域的分割和智能下采样，显著减少了稀疏视角3D高斯溅射中的高斯数量和内存需求，同时保持了高质量的渲染效果和快速的训练速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse-view synthesis remains a challenging problem due to the difficulty ofrecovering accurate geometry and appearance from limited observations. Whilerecent advances in 3D Gaussian Splatting (3DGS) have enabled real-timerendering with competitive quality, existing pipelines often rely onStructure-from-Motion (SfM) for camera pose estimation, an approach thatstruggles in genuinely sparse-view settings. Moreover, several SfM-free methodsreplace SfM with multi-view stereo (MVS) models, but generate massive numbersof 3D Gaussians by back-projecting every pixel into 3D space, leading to highmemory costs. We propose Segmentation-Driven Initialization for GaussianSplatting (SDI-GS), a method that mitigates inefficiency by leveragingregion-based segmentation to identify and retain only structurally significantregions. This enables selective downsampling of the dense point cloud,preserving scene fidelity while substantially reducing Gaussian count.Experiments across diverse benchmarks show that SDI-GS reduces Gaussian countby up to 50% and achieves comparable or superior rendering quality in PSNR andSSIM, with only marginal degradation in LPIPS. It further enables fastertraining and lower memory footprint, advancing the practicality of 3DGS forconstrained-view scenarios.</description>
      <author>example@mail.com (Yi-Hsin Li, Thomas Sikora, Sebastian Knorr, Måarten Sjöström)</author>
      <guid isPermaLink="false">2509.11853v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations</title>
      <link>http://arxiv.org/abs/2509.11783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 8th International Conference on Robotics, Control  and Automation Engineering (RCAE 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种增强现实(AR)增强的机器人远程操作系统，通过结合AR控制和空间点云渲染，实现了直观、无接触的机器人演示收集。该系统已在ABB机器人平台上验证，研究表明增强感知显著提高了任务性能和用户体验。&lt;h4&gt;背景&lt;/h4&gt;传统工业机器人编程复杂且耗时，需要专家程序员花费数周甚至数月时间。虽然通过演示编程(PbD)提供了一种更易选择的替代方案，但机器人控制和演示收集的直观界面仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种增强现实(AR)增强的机器人远程操作系统，结合基于AR的控制和空间点云渲染，实现直观、无接触的演示，使操作员能够远程控制机器人而无需进入工作空间或使用传统工具。&lt;h4&gt;方法&lt;/h4&gt;提出并实现了一个结合AR控制和空间点云渲染的系统，使操作员能够远程控制机器人。该系统在ABB机器人平台上进行了演示，特别是在IRB 1200工业机器人和GoFa 5协作机器人上进行了验证。通过用户研究评估了实时环境感知（有点云渲染和无点云渲染）对任务完成的影响。&lt;h4&gt;主要发现&lt;/h4&gt;增强感知显著提高了28%的任务性能，并改善了用户体验，系统可用性量表(SUS)得分提高了12%。实时环境感知对任务完成准确性、效率和用户信心有积极影响。&lt;h4&gt;结论&lt;/h4&gt;这项工作有助于推进工业环境中用于演示收集的直观机器人远程操作、AR界面设计、环境感知和远程操作安全机制。收集的演示可作为机器学习应用的有价值训练数据。&lt;h4&gt;翻译&lt;/h4&gt;传统工业机器人编程通常复杂且耗时，通常需要专家程序员花费数周甚至数月的努力。虽然通过演示编程(PbD)提供了更易选择的替代方案，但机器人控制和演示收集的直观界面仍然具有挑战性。为此，我们提出了一种增强现实(AR)增强的机器人远程操作系统，将基于AR的控制与空间点云渲染相结合，实现直观、无接触的演示。这种方法允许操作员远程控制机器人，无需进入工作空间或使用示教器等传统工具。所提出的系统具有通用性，已在ABB机器人平台上进行了演示，特别是在IRB 1200工业机器人和GoFa 5协作机器人上进行了验证。用户研究评估了实时环境感知的影响，具体是有点云渲染和无点云渲染两种情况，对任务完成准确性、效率和用户信心的影响。结果表明，增强感知显著提高了28%的任务性能，并通过系统可用性量表(SUS)得分提高12%改善了用户体验。这项工作有助于推进工业环境中用于演示收集的直观机器人远程操作、AR界面设计、环境感知和远程操作安全机制。收集的演示可作为机器学习应用的有价值训练数据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决工业机器人编程复杂、耗时的问题，以及通过演示编程(PbD)时缺乏直观界面的问题。这个问题很重要，因为传统机器人编程需要专家花费数周甚至数月，而现有方法要么局限于昂贵的协作机器人，要么存在安全风险，限制了工业机器人的广泛应用和人机协作的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统编程和现有PbD方法的局限性，然后探索了AR/VR技术在机器人控制中的应用潜力。他们借鉴了多项现有研究，如Rivera-Pinto的全息球体轨迹定义、Thormann的手势控制、Smith和Van Haastregt的数字末端执行器代理控制、Pizzagalli的数字孪生环境路径规划等，但针对这些方法的不足进行了改进，设计了一个模块化的AR遥操作系统，结合实时点云渲染增强环境感知。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用增强现实技术创建直观界面，使用户能远程控制机器人而不需进入工作空间或使用传统工具。系统由四个模块组成：机器人实时控制模块(EGM接口)、Unity AR系统模块(可视化与交互)、空间点云渲染模块(深度传感器)和机器人监控模块(RWS协议)。工作流程是用户通过HMD在AR界面演示动作，Unity处理数据并传输给物理机器人，同时点云渲染提供环境感知，机器人状态实时反馈给用户。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)适用于工业机器人和协作机器人的通用AR遥操作系统；2)集成AR控制与实时点云渲染增强视觉反馈；3)用户研究评估环境感知影响。相比之前工作，不同之处在于：确保轨迹可行性(不同于Rivera-Pinto的不验证可达性)、支持更广泛运动(不同于Thormann的手势限制)、减轻距离和物体大小限制(不同于Smith和Van Haastregt的直接视觉依赖)、无需预先建模适应动态环境(不同于Pizzagalli的数字孪生)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文开发并验证了一个基于增强现实的机器人遥操作系统，通过集成实时点云渲染，使用户能够直观、安全地远程控制工业机器人进行任务演示，显著提高了任务效率和用户体验。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional industrial robot programming is often complex and time-consuming,typically requiring weeks or even months of effort from expert programmers.Although Programming by Demonstration (PbD) offers a more accessiblealternative, intuitive interfaces for robot control and demonstrationcollection remain challenging. To address this, we propose an Augmented Reality(AR)-enhanced robot teleoperation system that integrates AR-based control withspatial point cloud rendering, enabling intuitive, contact-free demonstrations.This approach allows operators to control robots remotely without entering theworkspace or using conventional tools like the teach pendant. The proposedsystem is generally applicable and has been demonstrated on ABB robotplatforms, specifically validated with the IRB 1200 industrial robot and theGoFa 5 collaborative robot. A user study evaluates the impact of real-timeenvironmental perception, specifically with and without point cloud rendering,on task completion accuracy, efficiency, and user confidence. Results indicatethat enhanced perception significantly improves task performance by 28% andenhances user experience, as reflected by a 12% increase in the SystemUsability Scale (SUS) score. This work contributes to the advancement ofintuitive robot teleoperation, AR interface design, environmental perception,and teleoperation safety mechanisms in industrial settings for demonstrationcollection. The collected demonstrations may serve as valuable training datafor machine learning applications.</description>
      <author>example@mail.com (Shiqi Gong, Sebastian Zudaire, Chi Zhang, Zhen Li)</author>
      <guid isPermaLink="false">2509.11783v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning</title>
      <link>http://arxiv.org/abs/2509.11594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Jizhuo Chen and Diwen Liu contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GBPP是一种基于快速学习的评分器，用于从单个RGB-D快照中选择机器人抓取的基础姿态。该方法采用两阶段课程学习，结合简单启发式方法和高保真模拟优化，实现快速、安全的基础姿态选择。&lt;h4&gt;背景&lt;/h4&gt;机器人抓取任务中，从单个RGB-D图像选择合适的基础姿态具有挑战性。传统方法可能需要复杂的任务和运动优化，或仅依赖简单几何信息，效果有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、高效的方法，从单个RGB-D图像选择机器人抓取基础姿态，使其能够安全、可靠到达目标位置，并在出错时优雅降级。&lt;h4&gt;方法&lt;/h4&gt;1. 两阶段课程学习：第一阶段使用距离-可见性规则低成本自动标记大型数据集；第二阶段使用高保真模拟试验优化模型。2. 采用PointNet++风格点云编码器与多层感知机对候选姿态密集网格评分。3. 实现快速在线选择，无需完整任务和运动优化。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在模拟和真实移动机械臂上，GBPP优于仅依赖接近度和几何的基线方法。2. GBPP选择的基础姿态更安全、更易到达。3. 出错时能优雅降级而非完全失败。4. 实现了数据高效、几何感知的基础定位。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了一种实用的基础定位方法：使用廉价启发式方法获得广泛覆盖，然后通过有针对性的模拟进行校准，结合了简单高效和精确优化的优点。&lt;h4&gt;翻译&lt;/h4&gt;GBPP是一种基于快速学习的评分器，用于从单个RGB-D快照中选择机器人抓取的基础姿态。该方法采用两阶段课程：(1)使用简单的距离-可见性规则低成本自动标记大型数据集；(2)使用更小规模的高保真模拟试验来优化模型以匹配真实的抓取结果。PointNet++风格的点云编码器与多层感知机一起对候选姿态的密集网格进行评分，实现了快速在线选择，无需完整的任务和运动优化。在模拟和真实的移动机械臂上，GBPP优于仅依赖接近度和几何的基线方法，选择更安全、更易到达的姿态，并且在出错时能优雅降级。研究结果为数据高效、几何感知的基础定位提供了实用的方法：使用廉价启发式方法覆盖，然后通过有针对性的模拟进行校准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决移动机器人在杂乱环境中如何适当放置基座以成功抓取目标物体的问题。这个问题在现实中很重要，因为当前模块化系统往往忽略基座放置与机械臂可达性之间的协调，导致抓取失败；而几何方法计算成本高，任务和运动规划难以实时部署，限制了机器人在家庭、仓库等实际环境中的抓取效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：模块化系统忽略基座放置挑战，几何方法计算成本高，TAMP难以实时部署。然后他们将问题重新定义为候选姿态的二分类问题，考虑到直接使用大规模仿真训练成本极高，设计了结合廉价启发式数据和高保真仿真的两阶段方法。他们借鉴了PointNet++作为点云编码器，使用ProcTHOR和ManiSkill等现有仿真环境，但通过创新的两阶段课程学习解决了数据稀缺问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合廉价启发式数据的大规模覆盖和高保真仿真数据的高精度，通过两阶段学习实现数据高效的几何感知基座放置。整体流程包括：1)任务定义，输入RGB-D观测和机器人参数，输出最优基座位置；2)第一阶段使用距离-可见性启发式自动标记180k训练样本；3)第二阶段使用约12k高保真仿真样本微调模型；4)推理时评估所有候选位置，选择最佳位置执行；5)在仿真和真实环境中评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段课程学习框架，结合启发式和仿真数据优势；2)启发式自标记方法，大幅降低数据收集成本；3)数据高效的学习方法，在更少高质量数据下实现更好性能；4)实时性能，0.3秒内评估600个候选姿态。相比之前工作，不同于模块化系统的分离设计，直接从RGB-D预测基座位置；不同于几何方法的高计算成本，学习处理复杂环境；不同于TAMP的实时部署困难，支持快速在线规划；不同于纯学习的高数据需求，通过两阶段解决数据稀缺问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种两阶段课程学习方法，结合廉价启发式数据的大规模覆盖和高保真仿真数据的高精度，实现了数据高效的几何感知基座放置，显著提高了机器人在复杂环境中的抓取成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GBPP is a fast learning based scorer that selects a robot base pose forgrasping from a single RGB-D snapshot. The method uses a two stage curriculum:(1) a simple distance-visibility rule auto-labels a large dataset at low cost;and (2) a smaller set of high fidelity simulation trials refines the model tomatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLPscores dense grids of candidate poses, enabling rapid online selection withoutfull task-and-motion optimization. In simulation and on a real mobilemanipulator, GBPP outperforms proximity and geometry only baselines, choosingsafer and more reachable stances and degrading gracefully when wrong. Theresults offer a practical recipe for data efficient, geometry aware baseplacement: use inexpensive heuristics for coverage, then calibrate withtargeted simulation.</description>
      <author>example@mail.com (Jizhuo Chen, Diwen Liu, Jiaming Wang, Harold Soh)</author>
      <guid isPermaLink="false">2509.11594v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</title>
      <link>http://arxiv.org/abs/2509.11453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TrajTrack的新型基于轨迹的LiDAR 3D单目标跟踪框架，通过从历史边界框轨迹中隐式学习运动连续性，在保持高效率的同时显著提高了跟踪精度。&lt;h4&gt;背景&lt;/h4&gt;LiDAR-based 3D单目标跟踪是机器人和自主系统中的关键任务。现有方法中，两帧方法效率高但缺乏长期时间上下文，在稀疏或遮挡场景中容易失效；而基于序列的方法处理多个点云，计算成本高但鲁棒性强。&lt;h4&gt;目的&lt;/h4&gt;解决两帧方法和序列方法之间的权衡问题，提出一种新的基于轨迹的范式，在保持高效率的同时增强跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;TrajTrack是一个轻量级框架，通过从历史边界框轨迹中隐式学习运动连续性来增强基础两帧跟踪器，无需额外点云输入。它首先生成快速显式的运动建议，然后使用隐式运动建模模块预测未来轨迹，完善和纠正初始建议。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes基准测试上，TrajTrack实现了新的最先进性能，相比强大基线提高跟踪精度4.48%，同时保持56 FPS的运行速度，并展示了在不同基础跟踪器上的强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;TrajTrack成功解决了两帧方法和序列方法之间的权衡问题，在保持高效率的同时显著提高了跟踪精度，为LiDAR 3D单目标跟踪提供了新范式。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的3D单目标跟踪是机器人和自主系统中的关键任务。现有方法通常遵循逐帧运动估计或基于序列的范式。然而，两帧方法效率高但缺乏长期时间上下文，在稀疏或遮挡场景中容易失效，而处理多个点云的序列方法则以显著的计算成本获得鲁棒性。为解决这一困境，我们提出了一种新的基于轨迹的范式及其实现TrajTrack。TrajTrack是一个轻量级框架，通过仅从历史边界框轨迹中隐式学习运动连续性来增强基础两帧跟踪器，不需要额外昂贵的点云输入。它首先生成快速、显式的运动建议，然后使用隐式运动建模模块预测未来轨迹，进而完善和纠正初始建议。在大型NuScenes基准上的大量实验表明，TrajTrack实现了新的最先进性能，相比强大基线显著提高4.48%的跟踪精度，同时以56 FPS的速度运行。此外，我们还展示了TrajTrack在不同基础跟踪器上的强大泛化能力。视频可在https://www.bilibili.com/video/BV1ahYgzmEWP观看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是点云跟踪中效率与鲁棒性之间的权衡问题。现有的两帧方法效率高但缺乏长期时间上下文，在稀疏或遮挡场景中表现不佳；而序列方法虽能增强鲁棒性但计算成本高，不适合实时应用。这个问题在自动驾驶和机器人系统中至关重要，因为LiDAR点云跟踪是这些系统的核心任务，需要在保持实时性的同时应对各种复杂环境挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后从轨迹预测领域获得灵感，意识到可以从物体历史轨迹中学习运动连续性而不需要处理高带宽点云数据。他们设计了一个结合短期显式运动和长期隐式连续性的框架。方法借鉴了现有工作：显式运动部分参考了P2P等两帧跟踪方法；隐式轨迹预测部分借鉴了VectorNet、AgentFormer等使用Transformer建模序列数据的方法；整体框架受到SeqTrack3D等序列方法的启发，但通过只使用历史边界框而非完整点云序列来降低计算成本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过利用历史边界框轨迹学习物体长期运动连续性，在不增加大量计算成本的情况下提高跟踪鲁棒性。整体实现采用两阶段'提出-预测-细化'流程：1)显式运动提案：使用两帧模型处理点云，生成初始跟踪提案；2)隐式轨迹预测：创新性地使用仅历史边界框序列的隐式运动建模(IMM)模块，用TrajFormer架构预测未来轨迹；3)轨迹引导的提案细化：根据两个提案间的IoU动态选择，智能校正初始提案，输出最终跟踪结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于轨迹的范式，利用历史边界框融入长期运动连续性；2)隐式运动建模(IMM)模块，仅用轻量级历史边界框序列学习长期运动模式；3)两阶段处理流程，结合短期显式和长期隐式运动线索。相比之前工作：与两帧方法不同，它考虑长期连续性，在稀疏场景表现更好；与序列方法不同，它不处理多帧点云，计算成本低且能实时运行(56 FPS)；与其他轨迹预测方法不同，它专为3D跟踪设计，并提出了轨迹引导的提案细化机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrajTrack通过引入基于轨迹的范式和隐式运动建模模块，有效结合了短期显式运动和长期隐式连续性的优势，在保持实时性能的同时显著提升了点云跟踪的鲁棒性，实现了前所未有的性能和效率平衡。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D single object tracking (3D SOT) is a critical task in roboticsand autonomous systems. Existing methods typically follow frame-wise motionestimation or a sequence-based paradigm. However, the two-frame methods areefficient but lack long-term temporal context, making them vulnerable in sparseor occluded scenes, while sequence-based methods that process multiple pointclouds gain robustness at a significant computational cost. To resolve thisdilemma, we propose a novel trajectory-based paradigm and its instantiation,TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frametracker by implicitly learning motion continuity from historical bounding boxtrajectories alone-without requiring additional, costly point cloud inputs. Itfirst generates a fast, explicit motion proposal and then uses an implicitmotion modeling module to predict the future trajectory, which in turn refinesand corrects the initial proposal. Extensive experiments on the large-scaleNuScenes benchmark show that TrajTrack achieves new state-of-the-artperformance, dramatically improving tracking precision by 4.48% over a strongbaseline while running at 56 FPS. Besides, we also demonstrate the stronggeneralizability of TrajTrack across different base trackers. Video isavailable at https://www.bilibili.com/video/BV1ahYgzmEWP.</description>
      <author>example@mail.com (BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang)</author>
      <guid isPermaLink="false">2509.11453v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3D Gaussian Modeling and Ray Marching of OpenVDB datasets for Scientific Visualization</title>
      <link>http://arxiv.org/abs/2509.11377v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了3D高斯模型在科学可视化领域的应用，提出使用OpenVDB格式作为3D高斯粒子的建模框架，实现高效体积数据压缩，并开发基于光线积分的渲染算法进行光学深度累积计算。&lt;h4&gt;背景&lt;/h4&gt;3D高斯模型正被广泛研究用于场景建模和压缩。在科学可视化领域，大多数流行格式是密集网格数据结构，存储每个网格单元而忽略其贡献。OpenVDB库和数据格式专为稀疏体积数据设计，通过屏蔽空单元格避免存储它们，最初用于视觉效果如云、火和流体等。&lt;h4&gt;目的&lt;/h4&gt;探索在科学可视化中使用OpenVDB作为建模框架，转换为3D高斯粒子以实现进一步压缩，为不同科学体积类型提供统一建模方法，利用OpenVDB的压缩优势作为3D高斯模型的起点。&lt;h4&gt;方法&lt;/h4&gt;在OptiX 8.1中实现基于光线积分的渲染算法，计算3D高斯沿光线的贡献用于光学深度累积；实现SciVis风格的主光线仅NanoVDB HDDA基于光线步进器用于OpenVDB体素网格，以比较渲染结果；探索将此高斯模型应用于非规则网格体积格式如AMR体积和点云，使用OpenVDB网格类类型的内部表示用于数据层次结构。&lt;h4&gt;主要发现&lt;/h4&gt;使用OpenVDB作为3D高斯模型的起点提供了非平凡的压缩优势；成功实现了基于光线积分的渲染算法，能够计算3D高斯沿光线的贡献；开发了SciVis风格的主光线仅NanoVDB HDDA基于光线步进器用于性能比较。&lt;h4&gt;结论&lt;/h4&gt;OpenVDB在科学可视化场景中的应用具有可行性；3D高斯模型为体积数据压缩和渲染提供了新途径；高斯模型可扩展应用于非规则网格体积格式。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯模型目前因其场景建模和压缩能力而被广泛研究。在3D体积中，其使用正被探索用于以尽可能稀疏的方式表示密集体积。然而，这些方法大多开始于内存效率低下的数据格式。特别是在科学可视化(SciVis)领域，大多数流行格式是密集网格数据结构，存储每个网格单元，无论其贡献如何。OpenVDB库和数据格式被引入用于专门表示稀疏体积数据，用于视觉效果用例如云、火、流体等。它通过在存储过程中屏蔽空单元格来避免存储它们。这为在SciVis中使用提供了机会，特别是作为转换为3D高斯粒子的建模框架以实现进一步压缩，以及为不同科学体积类型提供统一建模方法。这种压缩起点是非平凡的，本文希望提出一种基于光线积分的渲染算法，在OptiX 8.1中实现，用于计算沿光线的3D高斯贡献以进行光学深度累积。为了比较我们光线步进高斯渲染器的渲染结果，我们还实现了SciVis风格的主光线仅NanoVDB HDDA基于光线步进器，用于OpenVDB体素网格。最后，本文还探索了将此高斯模型应用于非规则网格的体积格式，如AMR体积和点云，使用OpenVDB网格类类型的内部表示用于数据层次结构和细分结构。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将OpenVDB科学可视化数据集高效地转换为3D高斯模型并进行渲染的问题。这个问题很重要，因为科学可视化中常用的密集网格数据结构存储效率低下，而OpenVDB虽然能高效存储稀疏体积数据，但传统渲染方法仍有局限。3D高斯模型提供了一种潜在的压缩和统一表示方法，能够支持不同类型的科学体积数据，提高渲染效率同时保持视觉质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了科学可视化中体积数据处理的挑战，特别是稀疏数据的存储和渲染效率问题。他们注意到OpenVDB在处理稀疏体积数据方面的优势，以及3D高斯模型在场景表示和压缩方面的潜力。作者设计方法时借鉴了多项现有工作，包括OpenVDB库本身、3D高斯建模研究(如3DGS)、光线行进技术、线积分方法以及OptiX 8.1框架。作者的创新在于将这些技术结合起来，专门针对科学可视化的需求，提出了从OpenVDB到3D高斯的转换和渲染方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将OpenVDB中的稀疏体积数据转换为3D高斯表示，每个高斯代表一个体素或一组体素，然后使用光线行进技术穿过这些高斯，通过线积分计算光学深度，最后应用传递函数将累积的光学深度转换为最终的颜色和透明度。整体流程包括：1)加载OpenVDB数据集；2)遍历OpenVDB叶节点，根据密集或稀疏特性使用不同策略生成3D高斯；3)为每个高斯创建AABB并构建BVH加速结构；4)使用OptiX框架进行光线追踪，计算光学深度贡献；5)累积光学深度并应用传递函数获得最终颜色。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出将OpenVDB科学可视化数据格式转换为3D高斯模型；2)传递函数无关的建模方法，允许运行时动态调整传递函数；3)提供多层次细节控制，平衡渲染质量和性能；4)支持多种科学数据格式(规则网格、AMR体积、点云)的统一高斯表示；5)使用线积分方法计算3D高斯沿光线的贡献。相比之前工作，本文不依赖训练循环或图像监督，直接从原始体积数据生成高斯；使用3D高斯而非体素作为基本渲染单元；提供完整的体积覆盖而非仅表示表面；为多种科学数据格式提供统一的渲染框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种将OpenVDB科学可视化数据转换为3D高斯模型的方法，通过光线行进和线积分实现高效渲染，支持多种数据格式和动态传递函数调整，为科学可视化提供了统一的体积数据表示和渲染框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussians are currently being heavily investigated for their scenemodeling and compression abilities. In 3D volumes, their use is being exploredfor representing dense volumes as sparsely as possible. However, most of thesemethods begin with a memory inefficient data format. Specially in ScientificVisualization(SciVis), where most popular formats are dense-grid datastructures that store every grid cell, irrespective of its contribution.OpenVDB library and data format were introduced for representing sparsevolumetric data specifically for visual effects use cases such as clouds, fire,fluids etc. It avoids storing empty cells by masking them during storage. Itpresents an opportunity for use in SciVis, specifically as a modeling frameworkfor conversion to 3D Gaussian particles for further compression and for aunified modeling approach for different scientific volume types. Thiscompression head-start is non-trivial and this paper would like to present thiswith a rendering algorithm based on line integration implemented in OptiX8.1for calculating 3D Gaussians contribution along a ray for optical-depthaccumulation. For comparing the rendering results of our ray marching Gaussiansrenderer, we also implement a SciVis style primary-ray only NanoVDB HDDA basedray marcher for OpenVDB voxel grids. Finally, this paper also exploresapplication of this Gaussian model to formats of volumes other than regulargrids, such as AMR volumes and point clouds, using internal representation ofOpenVDB grid class types for data hierarchy and subdivision structure.</description>
      <author>example@mail.com (Isha Sharma, Dieter Schmalstieg)</author>
      <guid isPermaLink="false">2509.11377v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Limit Theorems for Verbose Persistence Diagrams</title>
      <link>http://arxiv.org/abs/2509.11256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作启动了对随机冗长图的研究，建立了随机冗长图的大数定律，证明了极限冗长图的存在性，刻画了其支撑并计算总质量，同时扩展了持久贝蒂数的概念并揭示了其与冗长图的关系。&lt;h4&gt;背景&lt;/h4&gt;持久图是持久同调研究的中心对象，也在随机拓扑背景下被研究。冗长图是对持久图的精化，通过沿对角线添加额外点来包含瞬时持久特征。&lt;h4&gt;目的&lt;/h4&gt;研究随机冗长图，建立其大数定律，证明极限冗长图的存在性，刻画其支撑并计算总质量，扩展持久贝蒂数概念并研究其与冗长图的关系。&lt;h4&gt;方法&lt;/h4&gt;通过分析随机点云的增长，研究冗长图的极限行为，将持久贝蒂数概念扩展，并研究其渐进行为。&lt;h4&gt;主要发现&lt;/h4&gt;随着随机点云规模扩大，存在极限冗长图，可视为对上半平面(对角线及其上方)上的一个测度；刻画了极限冗长图的支撑并计算了其总质量；揭示了扩展持久贝蒂数概念与冗长图之间的关系；建立了关于扩展持久贝蒂数渐进行为的结果。&lt;h4&gt;结论&lt;/h4&gt;这项工作成功地将Hiraoka、Shirai和Trinh及其后续Shirai和Suzaki的主要结果扩展到了冗长图的设定中，为随机冗长图研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;持久图是持久同调研究中的中心对象，也在随机拓扑的背景下被研究。新近提出的冗长图(也称为冗长条形码)是对持久图的精化，通过沿对角线添加额外点来包含瞬时持久特征。在这项工作中，我们启动了对随机冗长图的研究。我们建立了随机冗长图的大数定律，证明随着随机点云规模的扩大，存在一个极限冗长图，可以看作是对上半平面(对角线及其上方)上的一个测度。同时，我们刻画了其支撑并计算了其总质量。在此过程中，我们扩展了持久贝蒂数的概念，揭示了这种扩展概念与冗长图之间的关系，并建立了关于扩展持久贝蒂数渐进行为的结果。这项工作将Hiraoka、Shirai和Trinh及其后续Shirai和Suzaki的主要结果扩展到了冗长图的设定中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The persistence diagram is a central object in the study of persistenthomology and has also been investigated in the context of random topology. Themore recent notion of the verbose diagram (a.k.a. verbose barcode) is arefinement of the persistence diagram that is obtained by incorporatingephemeral persistence features as extra points along the diagonal. In thiswork, we initiate the study of random verbose diagrams. We establish a stronglaw of large numbers for verbose diagrams as a random point cloud grows in size-- that is, we prove the existence of a limiting verbose diagram, viewed as ameasure on the half-plane on and above the diagonal. Also, we characterize itssupport and compute its total mass. Along the way, we extend the notion of thepersistent Betti number, reveal the relation between this extended notion andthe verbose diagram (which is an extension of the fundamental lemma ofpersistent homology), and establish results on the asymptotic behavior of theextended persistent Betti numbers.  This work extends the main results of the work by Hiraoka, Shirai, and Trinhand its sequel by Shirai and Suzaki to the setting of verbose diagrams.</description>
      <author>example@mail.com (Jeong-hwi Joe, Woojin Kim, Cheolwoo Park)</author>
      <guid isPermaLink="false">2509.11256v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images</title>
      <link>http://arxiv.org/abs/2509.11164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新颖的轻量级可扩展学习框架，通过分析珊瑚的2D多视图RGB图像来预测其3D体积和表面积，用于珊瑚礁监测。&lt;h4&gt;背景&lt;/h4&gt;珊瑚礁监测需要量化珊瑚生长，这需要准确估算珊瑚的体积和表面积。然而，由于珊瑚形态复杂，这是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从二维多视图RGB图像预测珊瑚状物体三维体积和表面积的轻量级可扩展学习框架。&lt;h4&gt;方法&lt;/h4&gt;使用预训练模块VGGT从每个视图中提取密集点图，合并为统一点云并添加置信度分数，通过两个并行的DGCNN解码器头部输出体积和表面积及其置信度估计，并引入基于高斯负对数似然的复合损失函数以增强预测稳定性和提供不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;该方法实现了具有竞争力的准确性，并对未见过的珊瑚形态具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该框架为直接从稀疏图像集合中高效可扩展的珊瑚几何估算铺平了道路，在珊瑚生长分析和珊瑚礁监测方面具有潜在应用。&lt;h4&gt;翻译&lt;/h4&gt;有效的珊瑚礁监测需要通过精确的体积和表面积估算来量化珊瑚生长，但由于珊瑚形态复杂，这是一项具有挑战性的任务。我们提出了一种新颖的轻量级可扩展学习框架，通过从二维多视图RGB图像预测珊瑚状物体的三维体积和表面积来解决这一挑战。我们的方法使用预训练模块从每个视图中提取密集点图；这些点图被合并为统一的点云，并添加了每个视图的置信度分数。将结果点云输入到两个并行的DGCNN解码器头部，它们共同输出珊瑚的体积和表面积，以及它们相应的置信度估计。为了增强预测稳定性并提供不确定性估计，我们在实数和对数域中引入了基于高斯负对数似然的复合损失函数。我们的方法实现了具有竞争力的准确性，并对未见过的形态具有良好的泛化能力。该框架为直接从稀疏图像集合中高效可扩展的珊瑚几何估算铺平了道路，在珊瑚生长分析和珊瑚礁监测方面具有潜在应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从稀疏的多视角2D RGB图像中准确估算珊瑚体积和表面积的问题。这个问题很重要，因为珊瑚礁是海洋生态系统健康的重要指标，跟踪珊瑚体积和表面积对理解生长趋势、检测退化及指导保护工作至关重要。传统方法需要昂贵设备、大量人工处理且不可扩展，而现有方法在稀疏视角或遮挡情况下会产生不准确结果，严重影响监测可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了珊瑚形态复杂性和传统3D重建方法的局限性，然后借鉴了多视角技术在无需完整3D监督下估计几何形状的成功经验。他们利用VGGT视觉几何基础Transformer框架提取密集点图，并采用DGCNN动态图卷积网络作为解码器。方法设计考虑了珊瑚形态的极端变异性，通过混合损失函数处理不确定性，并针对稀疏视角情况进行了优化，体现了对现有计算机视觉和深度学习技术的创造性应用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接从稀疏多视角2D图像预测珊瑚体积和表面积，无需传统3D网格重建。整体流程分为三部分：1) 珊瑚数据集生成：合成watertight珊瑚网格并渲染多视角图像；2) 特征提取：使用VGGT从掩码图像提取密集点图并转换为带置信度的3D点云；3) 解码器处理：两个并行的DGCNN解码器分别预测体积和表面积及其置信度。训练时采用混合损失函数，结合高斯负对数似然和确定性误差指标，提高预测稳定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 轻量级可扩展的自动化管道，无需3D网格监督；2) 利用VGGT提取特征和DGCNN解码器预测几何属性；3) 设计混合损失函数处理不同珊瑚尺度的不确定性；4) 解决传统网格重建的伪影问题。相比Trellis方法，论文在体积估计上MAPE从60.81%降至10.27%，表面积从55.41%降至7.56%。与现有珊瑚监测方法不同，论文直接针对体积和表面积估计，而非仅关注覆盖率或地形指标。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种轻量级、可扩展的深度学习框架，能够从稀疏的多视角2D RGB图像中直接、准确地估计珊瑚的体积和表面积，克服了传统3D重建方法的局限性，为大尺度珊瑚监测和保护提供了高效工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective reef monitoring requires the quantification of coral growth viaaccurate volumetric and surface area estimates, which is a challenging task dueto the complex morphology of corals. We propose a novel, lightweight, andscalable learning framework that addresses this challenge by predicting the 3Dvolume and surface area of coral-like objects from 2D multi-view RGB images.Our approach utilizes a pre-trained module (VGGT) to extract dense point mapsfrom each view; these maps are merged into a unified point cloud and enrichedwith per-view confidence scores. The resulting cloud is fed to two parallelDGCNN decoder heads, which jointly output the volume and the surface area ofthe coral, as well as their corresponding confidence estimate. To enhanceprediction stability and provide uncertainty estimates, we introduce acomposite loss function based on Gaussian negative log-likelihood in both realand log domains. Our method achieves competitive accuracy and generalizes wellto unseen morphologies. This framework paves the way for efficient and scalablecoral geometry estimation directly from a sparse set of images, with potentialapplications in coral growth analysis and reef monitoring.</description>
      <author>example@mail.com (Diego Eustachio Farchione, Ramzi Idoughi, Peter Wonka)</author>
      <guid isPermaLink="false">2509.11164v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Maximum diversity, weighting and invariants of time series</title>
      <link>http://arxiv.org/abs/2509.11146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了magnitude（量级）的连续性及其在数据分析中的应用，特别是提出了一种用于周期时间序列分析的新不变量，并通过机器学习实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;Magnitude作为富范畴欧拉特征的特殊情况，代表了度量空间的大小感，与基数、维度和体积等经典概念相关。虽然已有研究从多角度解释了magnitude的意义，但连续性为理解magnitude提供了新视角。&lt;h4&gt;目的&lt;/h4&gt;基于magnitude和最大多样性的连续性已有结果，本文重点关注加权的连续性及其与最大多样性的关系，并将magnitude理论应用于时间序列分析。&lt;h4&gt;方法&lt;/h4&gt;应用magnitude理论到表示数据或模型参数的点云上，利用连续性结果推导周期时间序列的新不变量，并进行机器学习实验验证其性能。&lt;h4&gt;主要发现&lt;/h4&gt;加权具有连续性，其变化与最大多样性相关；magnitude理论与数据分析有密切联系；提出的周期时间序列不变量直接基于连续性结果，并在实验中改善了性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入基于magnitude连续性的新不变量，有效提升了时间序列分析的性能，展示了magnitude理论在数据科学中的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;Magnitude作为富范畴欧拉特征的特例，代表了度量空间的大小感，并与基数、维度和体积等经典概念相关。虽然研究已从多角度解释了magnitude的意义，但连续性也为magnitude提供了有价值的视角。基于关于magnitude和最大多样性的连续性已有结果，本文重点关注加权的连续性，其总和为magnitude，及其与最大多样性的变化。同时，近期研究通过将magnitude理论应用于表示数据或模型参数的点云，阐明了magnitude与数据分析之间的联系。本文还通过引入周期时间序列的新不变量为时间序列分析提供应用，其中不变性直接来自于连续性结果。作为案例，使用真实数据进行了简单的机器学习实验，结果表明建议的不变量提高了性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnitude, obtained as a special case of Euler characteristic of enrichedcategory, represents a sense of the size of metric spaces and is related toclassical notions such as cardinality, dimension, and volume. While the studieshave explained the meaning of magnitude from various perspectives, continuityalso gives a valuable view of magnitude. Based on established results aboutcontinuity of magnitude and maximum diversity, this article focuses oncontinuity of weighting, a distribution whose totality is magnitude, and itsvariation corresponding to maximum diversity. Meanwhile, recent studies alsoilluminated the connection between magnitude and data analysis by applyingmagnitude theory to point clouds representing the data or the set of modelparameters. This article will also provide an application for time seriesanalysis by introducing a new kind of invariants of periodic time series, wherethe invariance follows directly from the continuity results. As a use-case, asimple machine learning experiment is conducted with real-world data, in whichthe suggested invariants improved the performance.</description>
      <author>example@mail.com (Byungchang So)</author>
      <guid isPermaLink="false">2509.11146v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations</title>
      <link>http://arxiv.org/abs/2509.11125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为ManiVID-3D的新型3D强化学习架构，专门用于机器人操作，通过自监督解耦特征学习实现视角不变的表示。该框架包含ViewNet模块和高效的GPU加速批量渲染模块，能够在不依赖精确相机校准的情况下处理任意视角的点云观测，并实现了前所未有的训练速度。实验表明，该方法在视角变化下的表现优于现有技术，且参数效率更高。&lt;h4&gt;背景&lt;/h4&gt;在真实世界的机器人操作中部署视觉强化学习策略常常受到摄像头视角变化的阻碍。从固定前置摄像头训练的策略在摄像头位置改变时可能会失效，这是真实环境中难以避免的情况，因为传感器位置的适当管理很困难。现有方法通常依赖精确的相机校准或在处理大的透视变化时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决视觉强化学习在真实世界机器人操作中因摄像头视角变化导致的问题，开发一种不依赖精确相机校准且能有效处理大视角变化的3D强化学习架构。&lt;h4&gt;方法&lt;/h4&gt;提出ManiVID-3D，一种用于机器人操作的新型3D强化学习架构，通过自监督解耦特征学习来学习视角不变的表示。框架包含ViewNet模块，这是一个轻量级但有效的模块，能够自动将任意视角的点云观测对齐到统一的空间坐标系，无需外部校准。此外，还开发了一种高效的GPU加速批量渲染模块，每秒可处理超过5000帧，支持前所未有的3D视觉强化学习大规模训练。&lt;h4&gt;主要发现&lt;/h4&gt;在10个模拟任务和5个现实世界任务上的广泛评估表明，该方法在视角变化下的成功率比最先进的方法高44.7%，同时参数减少了80%。系统对严重透视变化的鲁棒性和强大的模拟到现实性能证明了学习几何一致表示对非结构化环境中可扩展机器人操作的有效性。&lt;h4&gt;结论&lt;/h4&gt;ManiVID-3D通过自监督解耦特征学习和ViewNet模块，有效解决了视觉强化学习在机器人操作中因视角变化导致的问题，无需精确的相机校准即可实现高性能。该方法的参数效率高、训练速度快，且在模拟到现实迁移中表现出色，为非结构化环境中的可扩展机器人操作提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在真实世界的机器人操作中部署视觉强化学习策略常常受到摄像头视角变化的阻碍。从固定前置摄像头训练的策略在摄像头位置改变时可能会失效——这是真实环境中难以避免的情况，因为传感器位置的适当管理很困难。现有方法通常依赖精确的相机校准或在处理大的透视变化时表现不佳。为了解决这些局限性，我们提出了ManiVID-3D，一种用于机器人操作的新型3D强化学习架构，通过自监督解耦特征学习来学习视角不变的表示。该框架包含ViewNet模块，这是一个轻量级但有效的模块，能够自动将任意视角的点云观测对齐到统一的空间坐标系，无需外部校准。此外，我们还开发了一种高效的GPU加速批量渲染模块，每秒可处理超过5000帧，支持前所未有的3D视觉强化学习大规模训练。在10个模拟任务和5个现实世界任务上的广泛评估表明，该方法在视角变化下的成功率比最先进的方法高44.7%，同时参数减少了80%。系统对严重透视变化的鲁棒性和强大的模拟到现实性能证明了学习几何一致表示对非结构化环境中可扩展机器人操作的有效性。我们的项目网站可以在https://zheng-joe-lee.github.io/manivid3d/找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉强化学习策略在摄像头视角变化时泛化能力不足的问题。当机器人从一个固定视角训练后，如果摄像头位置发生变化（真实世界中不可避免），训练好的策略可能会失效。这个问题在现实中非常重要，因为真实环境中传感器位置很难保持固定，特别是杂乱的家庭或动态工业环境中；视角变化会导致视觉观察中剧烈的几何畸变，使策略泛化变得特别困难；现有方法要么依赖精确相机校准，要么在大视角变化下表现不佳，限制了视觉强化学习在真实世界中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D视觉输入方法难以捕捉3D结构先验知识；深度感知方法依赖易受遮挡的深度图；3D点云方法需要精确相机校准。基于这些分析，作者设计了结合监督视图对齐和自监督特征解耦的方法。他们借鉴了DP3的轻量级MLP架构，对比学习表示解耦的思想，以及PointNet++用于点云变换。作者的创新在于将这些技术有机结合，特别是消除了对相机校准的依赖，实现了视角不变的强化学习策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦的3D表示学习视角不变的视觉强化学习策略，同时学习视角不变特征（包含任务关键信息）和视角相关特征（不影响任务执行）。整体流程包括：1)训练阶段：使用ViewNet对齐不同视角点云；处理点云数据；通过双头编码器提取视角不变和视角相关特征；使用对比学习目标实现特征解耦；2)部署阶段：处理真实世界点云；使用预训练ViewNet和策略网络执行动作；3)高效批量模拟与渲染：GPU加速系统实现大规模训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ManiVID-3D架构：使用解耦的视角不变表示实现大视角变化下的鲁棒操作；2)ViewNet模块：无需校准的多视图对齐，统一不同视角点云坐标；3)高效批量渲染系统：GPU加速实现大规模3D视觉RL训练；4)强大的零样本模拟到现实迁移能力。相比之前工作，不同之处在于：不依赖相机校准；使用真正的3D点云表示而非2D图像；参数减少80%同时提高性能；端到端训练而非两阶段预训练；在±75°极端视角变化下仍保持稳定性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ManiVID-3D通过解耦的3D表示和无需校准的多视图对齐，实现了在极端视角变化下具有强大泛化能力的机器人操作策略，同时显著提高了训练效率和模拟到现实的迁移能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying visual reinforcement learning (RL) policies in real-worldmanipulation is often hindered by camera viewpoint changes. A policy trainedfrom a fixed front-facing camera may fail when the camera is shifted--anunavoidable situation in real-world settings where sensor placement is hard tomanage appropriately. Existing methods often rely on precise camera calibrationor struggle with large perspective changes. To address these limitations, wepropose ManiVID-3D, a novel 3D RL architecture designed for roboticmanipulation, which learns view-invariant representations throughself-supervised disentangled feature learning. The framework incorporatesViewNet, a lightweight yet effective module that automatically aligns pointcloud observations from arbitrary viewpoints into a unified spatial coordinatesystem without the need for extrinsic calibration. Additionally, we develop anefficient GPU-accelerated batch rendering module capable of processing over5000 frames per second, enabling large-scale training for 3D visual RL atunprecedented speeds. Extensive evaluation across 10 simulated and 5 real-worldtasks demonstrates that our approach achieves a 44.7% higher success rate thanstate-of-the-art methods under viewpoint variations while using 80% fewerparameters. The system's robustness to severe perspective changes and strongsim-to-real performance highlight the effectiveness of learning geometricallyconsistent representations for scalable robotic manipulation in unstructuredenvironments. Our project website can be found inhttps://zheng-joe-lee.github.io/manivid3d/.</description>
      <author>example@mail.com (Zheng Li, Pei Qu, Yufei Jia, Shihui Zhou, Haizhou Ge, Jiahang Cao, Jinni Zhou, Guyue Zhou, Jun Ma)</author>
      <guid isPermaLink="false">2509.11125v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment</title>
      <link>http://arxiv.org/abs/2509.11097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了3DAeroRelief，首个专门用于灾后评估的三维基准数据集，使用低成本无人机收集飓风破坏区域的密集三维点云，并通过语义标注提供细粒度结构损伤信息。&lt;h4&gt;背景&lt;/h4&gt;及时评估结构损伤对灾害响应和恢复至关重要，但现有自然灾害分析主要依赖二维图像，存在缺乏深度、遮挡和空间上下文有限等问题；虽然三维语义分割提供了更丰富的替代方案，但现有三维基准数据集主要关注城市或室内场景，很少关注灾害影响区域。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集在灾害场景中的不足，创建首个专门针对灾后评估的三维基准数据集，为灾害响应中的三维场景理解提供资源。&lt;h4&gt;方法&lt;/h4&gt;使用低成本无人机在飓风破坏区域收集数据，通过运动结构(Motion Structure)和多视图立体技术(Multi-View Stereo)重建密集三维点云，并通过手动二维标注并将结果投影到三维空间来生成语义标注。&lt;h4&gt;主要发现&lt;/h4&gt;3DAeroRelief捕捉了具有细粒度结构损伤的大规模户外环境，处于真实世界灾害背景中；无人机能够在危险区域实现经济、灵活和安全的数据收集，特别适合紧急情况。&lt;h4&gt;结论&lt;/h4&gt;通过评估多个最先进的三维分割模型，展示了3DAeroRelief的效用，突出了灾害响应中三维场景理解的挑战和机会；该数据集作为推进灾后场景中稳健三维视觉系统在真实世界应用中有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;及时评估结构损伤对灾害响应和恢复至关重要。然而，之前大多数自然灾害分析工作依赖于二维图像，这些图像缺乏深度、存在遮挡问题，且提供有限的空间上下文。三维语义分割提供了更丰富的替代方案，但现有的三维基准数据集主要关注城市或室内场景，很少关注灾害影响区域。为解决这一差距，我们提出了3DAeroRelief——首个专门用于灾后评估的三维基准数据集。该数据集使用低成本无人机在飓风破坏区域收集，通过运动结构(Motion Structure)和多视图立体技术(Multi-View Stereo)重建密集三维点云。语义标注通过手动二维标注并投影到三维空间生成。与现有数据集不同，3DAeroRelief在真实世界灾害背景下捕捉了具有细粒度结构损伤的大规模户外环境。无人机能够在危险区域实现经济、灵活和安全的数据收集，使其特别适合紧急情况。为展示3DAeroRelief的效用，我们评估了数据集上的几个最先进的三维分割模型，以突显灾害响应中三维场景理解的挑战和机会。我们的数据集作为推进灾后场景中稳健三维视觉系统在真实世界应用中有价值的资源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决灾后评估中缺乏高质量3D数据集的问题。目前大多数灾害分析依赖2D图像，这些图像缺乏深度信息、容易受遮挡且空间上下文有限。这个问题在现实中非常重要，因为及时准确评估灾后结构损坏对灾害响应、资源分配和恢复规划至关重要，直接影响救援效率和灾后重建质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有3D数据集主要集中在城市或室内场景，缺乏灾后特定场景的问题。他们借鉴了先前2D灾害评估数据集(如FloodNet和RescueNet)的思路，但将其扩展到3D领域。技术上，他们采用标准的运动恢复结构(SfM)和多视图立体(MVS)技术进行3D重建，参考了现有的语义标注方法，并使用CloudCompare等工具进行3D编辑，形成了一套完整的从数据采集到标注的流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用无人机收集灾后区域的航拍影像，重建高分辨率3D点云，并进行精细语义标注，创建专门的灾后评估3D基准数据集。整体流程包括：1)数据收集与预处理(无人机拍摄、视频清理和帧提取)；2)3D重建(SfM建立稀疏结构，MVS生成密集点云)；3)后处理(绝对重缩放和区域清理)；4)标注(2D图像标注、标签投影到3D空间和3D精细化调整)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门用于灾后评估的3D基准数据集；2)使用低成本无人机收集数据，提高经济性和安全性；3)提供高分辨率3D点云覆盖大规模灾后区域；4)包含精细的结构损坏标注。相比之前工作，室内数据集(如S3DIS)只覆盖小规模环境，室外数据集(如SemanticKITTI)使用LiDAR生成稀疏点云且主要针对自动驾驶场景，而3DAeroRelief填补了大规模高分辨率灾后3D数据的空白。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3DAeroRelief是首个专门用于灾后评估的3D基准数据集，通过无人机收集高分辨率航拍影像并重建为密集3D点云，为开发更准确、全面的灾后评估3D视觉系统提供了宝贵的资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely assessment of structural damage is critical for disaster response andrecovery. However, most prior work in natural disaster analysis relies on 2Dimagery, which lacks depth, suffers from occlusions, and provides limitedspatial context. 3D semantic segmentation offers a richer alternative, butexisting 3D benchmarks focus mainly on urban or indoor scenes, with littleattention to disaster-affected areas. To address this gap, we present3DAeroRelief--the first 3D benchmark dataset specifically designed forpost-disaster assessment. Collected using low-cost unmanned aerial vehicles(UAVs) over hurricane-damaged regions, the dataset features dense 3D pointclouds reconstructed via Structure-from-Motion and Multi-View Stereotechniques. Semantic annotations were produced through manual 2D labeling andprojected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3Dlarge-scale outdoor environments with fine-grained structural damage inreal-world disaster contexts. UAVs enable affordable, flexible, and safe datacollection in hazardous areas, making them particularly well-suited foremergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluateseveral state-of-the-art 3D segmentation models on the dataset to highlightboth the challenges and opportunities of 3D scene understanding in disasterresponse. Our dataset serves as a valuable resource for advancing robust 3Dvision systems in real-world applications for post-disaster scenarios.</description>
      <author>example@mail.com (Nhut Le, Ehsan Karimi, Maryam Rahnemoonfar)</author>
      <guid isPermaLink="false">2509.11097v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios</title>
      <link>http://arxiv.org/abs/2509.10841v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Computer Vision and Image Understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的LiDAR点云语义分割方法，通过点平面投影从2D表示中学习特征，并引入几何感知数据增强技术，在仅依赖LiDAR数据的情况下提高了性能，特别是在数据有限场景中表现优异。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云语义分割对于自动驾驶和机器人等应用中解释3D环境至关重要。现有方法虽然性能强大，但通常计算复杂度高，需要大量训练数据，限制了它们在数据稀少场景中的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;改进基于点的方法在数据有限场景中的性能，通过有效学习2D表示的特征来提取互补信息，同时仅依赖LiDAR数据。&lt;h4&gt;方法&lt;/h4&gt;通过点平面投影从2D表示中有效学习特征，引入符合LiDAR传感器特性的几何感知数据增强技术来缓解类别不平衡问题，并将点平面投影应用于点云的多个信息丰富的2D表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种方法在数据有限的情况下取得了显著改进，同时在两个公开可用的标准数据集（SemanticKITTI和PandaSet）上也取得了具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在仅依赖LiDAR数据的情况下，通过点平面投影和几何感知数据增强技术，有效提高了点云语义分割的性能，特别是在数据有限场景中表现优异，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;激光雷达点云语义分割对于自动驾驶和机器人等应用中解释三维环境至关重要。最近的方法通过利用不同的点云表示或整合来自其他传感器（如摄像头）或外部数据集的数据来实现强大的性能。然而，这些方法通常计算复杂度高，需要大量训练数据，限制了它们在数据稀少场景中的泛化能力。本文通过点平面投影从二维表示中有效学习特征，改进了基于点的方法的性能，能够在仅依赖激光雷达数据的同时提取互补信息。此外，我们引入了一种符合激光雷达传感器特性的几何感知数据增强技术，可以缓解类别不平衡问题。我们实现了并评估了将点平面投影应用于点云的多个信息丰富的二维表示的方法。实验表明，这种方法在数据有限的情况下取得了显著改进，同时在两个公开可用的标准数据集（如SemanticKITTI和PandaSet）上也取得了具有竞争力的结果。我们方法的代码可在https://github.com/SiMoM0/3PNet获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云语义分割在数据稀缺场景下的性能提升问题。这个问题在现实世界中非常重要，因为自动驾驶和机器人等应用需要高效的3D环境理解，但获取大量标注数据成本高昂且耗时。许多现有方法依赖大量训练数据或多传感器融合，限制了它们在资源受限环境或特殊场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的优缺点，发现点云直接处理方法在数据稀缺场景下表现更好但仍有提升空间，而投影方法虽然高效但存在信息损失。他们设计了一种混合方法，保留点云直接处理的优点同时借鉴投影方法的高效性。该方法基于之前的工作(Fusaro et al., 2024)进行扩展，该工作又受Puy et al. (2023)启发。作者采用了Instance CutMix技术来缓解类别不平衡，并引入了两种新的2D点云表示和层跳跃连接机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点平面投影技术将3D点云投影到多个2D平面上，从不同视角提取互补的几何和空间信息，利用高效的2D卷积处理这些投影，同时结合简单的点级操作保持点云原始结构。整体流程包括：1)点云嵌入模块提取点级和邻域特征；2)主干网络使用SpatialMix模块将点云投影到5种2D平面并处理，ChannelMix模块在3D空间细化特征；3)分割头结合初始特征生成预测；4)几何感知的Instance CutMix数据增强；5)交叉熵和Lovasz-Softmax损失的加权和作为最终损失函数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多点平面投影架构，使用5种2D点云表示并引入极坐标网格和距离图像两种新表示；2)几何感知的Instance CutMix数据增强，根据LiDAR束配置调整点密度；3)层跳跃连接保留重要特征；4)轻量级高效设计，使用2D卷积替代3D卷积。相比之前工作，该方法仅使用LiDAR数据不需要多传感器融合，比纯投影方法保留了更多原始点云信息，比纯点云方法计算效率更高，并在小数据场景下表现显著优于作者之前的工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于点平面投影的轻量级LiDAR语义分割方法，通过多视角2D表示和几何感知数据增强，显著提升了数据稀缺场景下的分割性能，同时保持了计算效率和标准数据集上的竞争力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point cloud semantic segmentation is essential for interpreting 3Denvironments in applications such as autonomous driving and robotics. Recentmethods achieve strong performance by exploiting different point cloudrepresentations or incorporating data from other sensors, such as cameras orexternal datasets. However, these approaches often suffer from highcomputational complexity and require large amounts of training data, limitingtheir generalization in data-scarce scenarios. In this paper, we improve theperformance of point-based methods by effectively learning features from 2Drepresentations through point-plane projections, enabling the extraction ofcomplementary information while relying solely on LiDAR data. Additionally, weintroduce a geometry-aware technique for data augmentation that aligns withLiDAR sensor properties and mitigates class imbalance. We implemented andevaluated our method that applies point-plane projections onto multipleinformative 2D representations of the point cloud. Experiments demonstrate thatthis approach leads to significant improvements in limited-data scenarios,while also achieving competitive results on two publicly available standarddatasets, as SemanticKITTI and PandaSet. The code of our method is available athttps://github.com/SiMoM0/3PNet</description>
      <author>example@mail.com (Simone Mosco, Daniel Fusaro, Wanmeng Li, Emanuele Menegatti, Alberto Pretto)</author>
      <guid isPermaLink="false">2509.10841v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors</title>
      <link>http://arxiv.org/abs/2509.12081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为欺骗风险最小化(DRM)的新方法，通过学习使训练数据看起来独立同分布的数据表示来实现分布外泛化，从而识别稳定特征并消除虚假相关性。&lt;h4&gt;背景&lt;/h4&gt;传统的领域适应或不变表示学习方法需要访问测试数据或将训练数据划分为有限数量的数据生成域，限制了它们在实际应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要测试数据访问或训练数据预划分的分布外泛化方法。&lt;h4&gt;方法&lt;/h4&gt;提出欺骗风险最小化(DRM)原则，通过一个可微分目标函数实现，该函数同时学习消除分布偏移的特征并最小化任务特定损失。&lt;h4&gt;主要发现&lt;/h4&gt;DRM在概念转换的数值实验和具有协变量偏移的模拟模仿学习环境中表现有效，特别是在机器人部署的环境中。&lt;h4&gt;结论&lt;/h4&gt;DRM提供了一种新的分布外泛化机制，通过学习使训练数据看起来独立同分布的表示，可以推广到未见过的领域，且不需要传统方法所需的测试数据访问或数据预划分。&lt;h4&gt;翻译&lt;/h4&gt;本文提出将欺骗作为分布外(OOD)泛化的机制：通过学习使训练数据对观察者看起来独立同分布(iid)的数据表示，我们可以识别出消除虚假相关性的稳定特征，并推广到未见过的领域。我们将这一原则称为欺骗风险最小化(DRM)，并通过一个实用的可微分目标函数实例化，该函数同时学习从基于保鞅的检测器角度看消除分布偏移的特征，同时最小化特定任务的损失。与领域适应或先前的不变表示学习方法相比，DRM不需要访问测试数据或将训练数据划分为有限数量的数据生成域。我们在具有概念转换的数值实验和机器人部署环境中具有协变量偏移的模拟模仿学习设置上证明了DRM的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes deception as a mechanism for out-of-distribution (OOD)generalization: by learning data representations that make training data appearindependent and identically distributed (iid) to an observer, we can identifystable features that eliminate spurious correlations and generalize to unseendomains. We refer to this principle as deceptive risk minimization (DRM) andinstantiate it with a practical differentiable objective that simultaneouslylearns features that eliminate distribution shifts from the perspective of adetector based on conformal martingales while minimizing a task-specific loss.In contrast to domain adaptation or prior invariant representation learningmethods, DRM does not require access to test data or a partitioning of trainingdata into a finite number of data-generating domains. We demonstrate theefficacy of DRM on numerical experiments with concept shift and a simulatedimitation learning setting with covariate shift in environments that a robot isdeployed in.</description>
      <author>example@mail.com (Anirudha Majumdar)</author>
      <guid isPermaLink="false">2509.12081v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</title>
      <link>http://arxiv.org/abs/2509.12039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 22 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了RAM++（Robust Representation Learning via Adaptive Mask），一个用于全面图像恢复的两阶段框架，结合高级语义理解和低级纹理生成，实现面向内容的鲁棒恢复。&lt;h4&gt;背景&lt;/h4&gt;现有基于退化导向的方法在极端场景（如与图像结构强耦合的退化）中存在局限性，同时面临跨任务性能不均衡、对已见退化过拟合以及对未见过退化泛化能力弱等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理各种退化场景（包括已见、未见、极端和混合退化）的鲁棒图像恢复框架，解决现有方法在极端场景下的局限性，并提高跨任务性能的均衡性。&lt;h4&gt;方法&lt;/h4&gt;RAM++包含三个关键设计：1) 自适应语义感知掩码（AdaSAM）：对语义丰富和纹理区域应用像素级掩码的预训练策略；2) 掩码属性电导（MAC）：调整贡献度更高的层的选择性微调策略；3) 鲁棒特征正则化（RFR）：利用DINOv2的语义一致且退化不变的表示，结合高效特征融合的策略。&lt;h4&gt;主要发现&lt;/h4&gt;RAM++通过三个关键设计，在已见、未见、极端和混合退化场景下实现了鲁棒、均衡和最先进的性能，有效解决了现有方法在极端场景下的局限性。&lt;h4&gt;结论&lt;/h4&gt;RAM++是一个有效的两阶段框架，通过结合高级语义理解和低级纹理生成，实现了面向内容的鲁棒图像恢复，在各种退化场景下表现出色。&lt;h4&gt;翻译&lt;/h4&gt;这项工作提出了通过自适应掩码进行鲁棒表示学习（RAM++），这是一个用于全面图像恢复的两阶段框架。RAM++将高级语义理解与低级纹理生成相结合，以实现面向内容的鲁棒恢复。它解决了现有基于退化导向的方法在极端场景（例如与图像结构强耦合的退化）中的局限性。RAM++还通过三个关键设计缓解了常见挑战，如跨任务性能不均衡、对已见退化的过拟合以及对未见过退化的弱泛化能力：1）自适应语义感知掩码（AdaSAM）：一种预训练策略，对语义丰富和纹理区域应用像素级掩码。这种设计使网络能够从各种退化中学习生成先验和图像内容先验。2）掩码属性电导（MAC）：一种选择性微调策略，调整贡献度更高的层，以弥合掩码预训练和完整图像微调之间的完整性差距，同时保留已学习的先验。3）鲁棒特征正则化（RFR）：一种策略，利用DINOv2的语义一致且退化不变的表示，结合高效特征融合，实现忠实且语义连贯的恢复。通过这些设计，RAM++在已见、未见、极端和混合退化场景下实现了鲁棒、均衡和最先进的性能。我们的代码和模型将在https://github.com/DragonisCV/RAM发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents Robust Representation Learning via Adaptive Mask (RAM++),a two-stage framework for all-in-one image restoration. RAM++ integrateshigh-level semantic understanding with low-level texture generation to achievecontent-oriented robust restoration. It addresses the limitations of existingdegradation-oriented methods in extreme scenarios (e.g., degradations stronglycoupled with image structures). RAM++ also mitigates common challenges such asunbalanced performance across tasks, overfitting to seen degradations, and weakgeneralization to unseen ones through three key designs: 1) AdaptiveSemantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-levelmasks to semantically rich and textured regions. This design enables thenetwork to learn both generative priors and image content priors from variousdegradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuningstrategy that adjusts the layers with higher contributions to bridge theintegrity gap between masked pretraining and full-image fine-tuning whileretaining learned priors. 3) Robust Feature Regularization (RFR): a strategythat leverages DINOv2's semantically consistent and degradation-invariantrepresentations, together with efficient feature fusion, to achieve faithfuland semantically coherent restoration. With these designs, RAM++ achievesrobust, well-balanced, and state-of-the-art performance across seen, unseen,extreme, and mixed degradations. Our code and model will be released athttps://github.com/DragonisCV/RAM</description>
      <author>example@mail.com (Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, Chongyi Li)</author>
      <guid isPermaLink="false">2509.12039v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation</title>
      <link>http://arxiv.org/abs/2509.11840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 CDEL Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;生成式视觉语言模型在高层图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐。本研究通过将图像与VLMs生成的合成描述进行密集对齐，结合了生成式VLMs和视觉语言对齐表示学习两个研究方向。合成字幕成本低、可扩展且易于生成，为密集对齐方法提供了高级语义理解的优质来源。实验表明，该方法在标准零样本开放词汇分割基准上优于先前工作，且数据效率更高。&lt;h4&gt;背景&lt;/h4&gt;生成式视觉语言模型在高层图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐。另一研究方向专注于视觉语言对齐的表示学习，针对的是像分割这样的密集任务的零样本推理。&lt;h4&gt;目的&lt;/h4&gt;结合生成式VLMs和视觉语言对齐表示学习两个研究方向，通过将图像与VLMs生成的合成描述进行密集对齐。&lt;h4&gt;方法&lt;/h4&gt;使用VLMs生成合成字幕，这些字幕成本低、可扩展且易于生成，作为密集对齐方法的高级语义理解来源。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在标准的零样本开放词汇分割基准/数据集上优于先前的工作，同时具有更高的数据效率。&lt;h4&gt;结论&lt;/h4&gt;通过VLMs生成的合成描述与图像进行密集对齐是一种有效的方法，可以在零样本开放词汇分割任务中取得更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;生成式视觉语言模型在高层图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐，正如我们的研究结果所示。与生成式VLMs的进展相辅相成，另一研究方向专注于视觉语言对齐的表示学习，针对的是像分割这样的密集任务的零样本推理。在这项工作中，我们通过将图像与VLMs生成的合成描述进行密集对齐，结合了这两个方向。合成字幕成本低、可扩展且易于生成，使它们成为密集对齐方法的高级语义理解的优质来源。实验上，我们的方法在标准的零样本开放词汇分割基准/数据集上优于先前的工作，同时具有更高的数据效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative vision-language models (VLMs) exhibit strong high-level imageunderstanding but lack spatially dense alignment between vision and languagemodalities, as our findings indicate. Orthogonal to advancements in generativeVLMs, another line of research has focused on representation learning forvision-language alignment, targeting zero-shot inference for dense tasks likesegmentation. In this work, we bridge these two directions by densely aligningimages with synthetic descriptions generated by VLMs. Synthetic captions areinexpensive, scalable, and easy to generate, making them an excellent source ofhigh-level semantic understanding for dense alignment methods. Empirically, ourapproach outperforms prior work on standard zero-shot open-vocabularysegmentation benchmarks/datasets, while also being more data-efficient.</description>
      <author>example@mail.com (Tim Lebailly, Vijay Veerabadran, Satwik Kottur, Karl Ridgeway, Michael Louis Iuzzolino)</author>
      <guid isPermaLink="false">2509.11840v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Anomaly Detection in Industrial Control Systems Based on Cross-Domain Representation Learning</title>
      <link>http://arxiv.org/abs/2509.11786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于跨域表示学习的工业控制系统异常检测方法，能够学习多领域行为的联合特征并在不同领域内检测异常，实验结果表明该方法性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;工业控制系统广泛应用于工业领域，其安全性和稳定性至关重要。ICS通过网络通信远程监控和管理物理设备，而现有异常检测方法主要关注网络流量或传感器数据的安全分析。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于跨域表示学习的ICS异常检测方法，学习多领域行为的联合特征，在不同领域内检测异常，克服仅分析单一领域难以全面识别异常的问题。&lt;h4&gt;方法&lt;/h4&gt;构建能够表示ICS多领域行为的跨域图，利用图神经网络学习这些行为的联合特征，采用多任务学习方法在不同领域分别识别异常并进行联合训练。&lt;h4&gt;主要发现&lt;/h4&gt;由于异常在不同领域表现出不同行为，通过多任务学习分别识别不同领域的异常并进行联合训练，能够提高异常检测的准确性。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，所提出的基于跨域表示学习的异常检测方法在识别ICS异常方面的性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;工业控制系统(ICS)广泛应用于工业领域，其安全性和稳定性非常重要。一旦ICS受到攻击，可能会造成严重损害。因此，检测ICS中的异常非常重要。ICS可以使用通信网络远程监控和管理物理设备。现有的异常检测方法主要关注分析网络流量或传感器数据的安全性。然而，ICS不同领域（如网络流量和传感器物理状态）的行为是相互关联的，因此仅通过分析单一领域难以全面识别异常。本文提出了一种基于ICS跨域表示学习的异常检测方法，可以学习多领域行为的联合特征，并在不同领域内检测异常。在构建能够表示ICS多领域行为的跨域图后，我们的方法可以利用图神经网络学习它们的联合特征。由于异常在不同领域表现出不同的行为，我们利用多任务学习方法分别识别不同领域的异常并进行联合训练。实验结果表明，我们的方法在识别ICS异常方面的性能优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Industrial control systems (ICSs) are widely used in industry, and theirsecurity and stability are very important. Once the ICS is attacked, it maycause serious damage. Therefore, it is very important to detect anomalies inICSs. ICS can monitor and manage physical devices remotely using communicationnetworks. The existing anomaly detection approaches mainly focus on analyzingthe security of network traffic or sensor data. However, the behaviors ofdifferent domains (e.g., network traffic and sensor physical status) of ICSsare correlated, so it is difficult to comprehensively identify anomalies byanalyzing only a single domain. In this paper, an anomaly detection approachbased on cross-domain representation learning in ICSs is proposed, which canlearn the joint features of multi-domain behaviors and detect anomalies withindifferent domains. After constructing a cross-domain graph that can representthe behaviors of multiple domains in ICSs, our approach can learn the jointfeatures of them by leveraging graph neural networks. Since anomalies behavedifferently in different domains, we leverage a multi-task learning approach toidentify anomalies in different domains separately and perform joint training.The experimental results show that the performance of our approach is betterthan existing approaches for identifying anomalies in ICSs.</description>
      <author>example@mail.com (Dongyang Zhan, Wenqi Zhang, Lin Ye, Xiangzhan Yu, Hongli Zhang, Zheng He)</author>
      <guid isPermaLink="false">2509.11786v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</title>
      <link>http://arxiv.org/abs/2509.11425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FuseCodec通过强大的跨模态对齐和全局监督信息统一了声学、语义和上下文表示，在语音标记化任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的神经编解码器捕获低级声学特征，忽略了人类语音中固有的语义和上下文线索。虽然最近的工作引入了语义表示或上下文表示，但在对齐和统一这些表示方面仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;引入FuseCodec，通过强大的跨模态对齐和全局监督信息来统一声学、语义和上下文表示。&lt;h4&gt;方法&lt;/h4&gt;FuseCodec采用三种互补技术：(i)潜在表示融合：将语义和上下文特征直接集成到编码器潜在空间中；(ii)全局语义-上下文监督：使用全局池化和广播表示监督离散令牌；(iii)时间对齐的上下文监督：在局部窗口内动态匹配上下文和语音令牌。同时介绍了FuseCodec-TTS，展示其在零样本语音合成中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;FuseCodec在LibriSpeech上取得了最先进的性能，在转录准确性、感知质量、可懂度和说话人相似性方面超越了EnCodec、SpeechTokenizer和DAC。&lt;h4&gt;结论&lt;/h4&gt;上下文和语义引导的标记化对语音标记化和下游任务有效。代码和预训练模型已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;语音标记化能够实现离散表示并促进语音语言建模。然而，现有的神经编解码器捕获低级声学特征，忽略了人类语音中固有的语义和上下文线索。虽然最近的工作引入了来自自监督语音模型的语义表示或集成了来自预训练语言模型的上下文表示，但在对齐和统一这些语义和上下文表示方面仍存在挑战。我们引入了FuseCodec，它通过强大的跨模态对齐和全局监督信息来统一声学、语义和上下文表示。我们提出了三种互补技术：(i)潜在表示融合，将语义和上下文特征直接集成到编码器潜在空间中，以实现强大和统一的表示学习；(ii)全局语义-上下文监督，使用全局池化和广播表示监督离散令牌，以增强时间一致性和跨模态对齐；(iii)时间对齐的上下文监督，通过在局部窗口内动态匹配上下文和语音令牌来加强对齐，实现细粒度的令牌级监督。我们进一步介绍了FuseCodec-TTS，展示了我们的方法在零样本语音合成中的应用。实验表明，FuseCodec在LibriSpeech上取得了最先进的性能，在转录准确性、感知质量、可懂度和说话人相似性方面超越了EnCodec、SpeechTokenizer和DAC。结果强调了上下文和语义引导的标记化对语音标记化和下游任务的有效性。代码和预训练模型可在https://github.com/mubtasimahasan/FuseCodec获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech tokenization enables discrete representation and facilitates speechlanguage modeling. However, existing neural codecs capture low-level acousticfeatures, overlooking the semantic and contextual cues inherent to humanspeech. While recent efforts introduced semantic representations fromself-supervised speech models or incorporated contextual representations frompre-trained language models, challenges remain in aligning and unifying thesemantic and contextual representations. We introduce FuseCodec, which unifiesacoustic, semantic, and contextual representations through strong cross-modalalignment and globally informed supervision. We propose three complementarytechniques: (i) Latent Representation Fusion, integrating semantic andcontextual features directly into the encoder latent space for robust andunified representation learning; (ii) Global Semantic-Contextual Supervision,supervising discrete tokens with globally pooled and broadcastedrepresentations to enhance temporal consistency and cross-modal alignment; and(iii) Temporally Aligned Contextual Supervision, strengthening alignment bydynamically matching contextual and speech tokens within a local window forfine-grained token-level supervision. We further introduce FuseCodec-TTS,demonstrating our methodology's applicability to zero-shot speech synthesis.Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,perceptual quality, intelligibility, and speaker similarity. Results highlightthe effectiveness of contextually and semantically guided tokenization forspeech tokenization and downstream tasks. Code and pretrained models areavailable at https://github.com/mubtasimahasan/FuseCodec.</description>
      <author>example@mail.com (Md Mubtasim Ahasan, Rafat Hasan Khan, Tasnim Mohiuddin, Aman Chadha, Tariq Iqbal, M Ashraful Amin, Amin Ahsan Ali, Md Mofijul Islam, A K M Mahbubur Rahman)</author>
      <guid isPermaLink="false">2509.11425v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits</title>
      <link>http://arxiv.org/abs/2509.11362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了PersonaX，一个多模态数据集集合，用于全面分析公共人物的行为特征，包含CelebPersona（9444名公众人物）和AthlePersona（4181名运动员）两个子数据集，每个数据集都包含行为特征评估、面部图像和传记信息。&lt;h4&gt;背景&lt;/h4&gt;理解人类行为特征在人机交互、计算社会科学和个性化AI系统中至关重要，需要整合多种模态捕捉细微模式，但现有资源很少提供结合行为描述符与面部属性、传记信息等互补模态的数据集。&lt;h4&gt;目的&lt;/h4&gt;解决现有资源不足，提供一个结合行为描述符与互补模态的数据集，支持跨模态的公共特征全面分析。&lt;h4&gt;方法&lt;/h4&gt;创建PersonaX数据集集合，并在两个层次上分析：1)从文本描述提取高级特征分数，应用五种统计独立性检验检查与其他模态的关系；2)引入专为多模态和多测量数据设计的因果表示学习框架，提供理论可识别性保证。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实数据上的实验证明了该方法的有效性，PersonaX为研究LLM推断的行为特征与视觉和传记属性的结合奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;PersonaX通过整合结构化和非结构化分析，建立了研究LLM推断行为特征与视觉和传记属性相结合的基础，促进了多模态特征分析和因果推理的发展。&lt;h4&gt;翻译&lt;/h4&gt;理解人类行为特征是人机交互、计算社会科学和个性化AI系统应用的核心。这种理解通常需要整合多种模态来捕捉细微的模式和关系。然而，现有资源很少提供将行为描述符与面部属性和传记信息等互补模态结合的数据集。为了解决这一差距，我们提出了PersonaX，这是一个精心策划的多模态数据集集合，旨在实现跨模态的公共特征全面分析。PersonaX包括(1)CelebPersona，包含来自不同职业的9444名公众人物，和(2)AthlePersona，涵盖7个主要体育联赛的4181名专业运动员。每个数据集都包含由三个高性能大型语言模型推断的行为特征评估，以及面部图像和结构化的传记特征。我们在两个互补层次上分析PersonaX。首先，我们从文本描述中抽象出高级特征分数，并应用五种统计独立性检验来检查它们与其他模态的关系。其次，我们引入了一种新的因果表示学习框架，专为多模态和多测量数据设计，提供理论可识别性保证。在合成和真实数据上的实验证明了我们方法的有效性。通过统一结构化和非结构化分析，PersonaX为研究LLM推断的行为特征与视觉和传记属性相结合奠定了基础，推进了多模态特征分析和因果推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding human behavior traits is central to applications inhuman-computer interaction, computational social science, and personalized AIsystems. Such understanding often requires integrating multiple modalities tocapture nuanced patterns and relationships. However, existing resources rarelyprovide datasets that combine behavioral descriptors with complementarymodalities such as facial attributes and biographical information. To addressthis gap, we present PersonaX, a curated collection of multimodal datasetsdesigned to enable comprehensive analysis of public traits across modalities.PersonaX consists of (1) CelebPersona, featuring 9444 public figures fromdiverse occupations, and (2) AthlePersona, covering 4181 professional athletesacross 7 major sports leagues. Each dataset includes behavioral traitassessments inferred by three high-performing large language models, alongsidefacial imagery and structured biographical features. We analyze PersonaX at twocomplementary levels. First, we abstract high-level trait scores from textdescriptions and apply five statistical independence tests to examine theirrelationships with other modalities. Second, we introduce a novel causalrepresentation learning (CRL) framework tailored to multimodal andmulti-measurement data, providing theoretical identifiability guarantees.Experiments on both synthetic and real-world data demonstrate the effectivenessof our approach. By unifying structured and unstructured analysis, PersonaXestablishes a foundation for studying LLM-inferred behavioral traits inconjunction with visual and biographical attributes, advancing multimodal traitanalysis and causal reasoning.</description>
      <author>example@mail.com (Loka Li, Wong Yu Kang, Minghao Fu, Guangyi Chen, Zhenhao Chen, Gongxu Luo, Yuewen Sun, Salman Khan, Peter Spirtes, Kun Zhang)</author>
      <guid isPermaLink="false">2509.11362v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Towards Automated Error Discovery: A Study in Conversational AI</title>
      <link>http://arxiv.org/abs/2509.10833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMNLP 2025 main conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基于大语言模型的对话代理虽然流畅连贯，但仍会产生难以预防的不良行为。本研究提出了一种自动错误发现框架和SEEED方法，用于检测对话AI中的错误，特别是指令中未明确指定的错误。&lt;h4&gt;背景&lt;/h4&gt;基于大语言模型的对话代理表现出强大的流畅性和连贯性，但仍会产生不良行为（错误），这些错误很难在部署过程中阻止其到达用户。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够检测和定义对话AI中错误的框架，特别是那些由于响应生成模型更新或用户行为变化而产生的新错误。&lt;h4&gt;方法&lt;/h4&gt;引入'自动错误发现'框架，提出SEEED（Soft Clustering Extended Encoder-Based Error Detection）作为基于编码器的实现方法。增强软最近邻损失中负样本的距离加权，并引入基于标签的样本排序来选择高度对比的示例，以实现更好的表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;SEEED在多个错误标注的对话数据集上优于适应的基线方法（包括GPT-4o和Phi-4），将检测未知错误的准确性提高了最多8个百分点，并且在对未知意图检测方面表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SEEED方法能有效检测对话AI中的未知错误，提高错误检测的准确性，并在未知意图检测方面表现出良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;尽管基于大语言模型的对话代理表现出强大的流畅性和连贯性，但它们仍然会产生不良行为（错误），这些错误很难在部署过程中阻止其到达用户。最近的研究利用大语言模型来检测错误并指导响应生成模型的改进。然而，当前的LLM难以识别指令中未明确指定的错误，例如由于响应生成模型更新或用户行为变化而产生的错误。在这项工作中，我们引入了'自动错误发现'，这是一个用于检测和定义对话AI中错误的框架，并提出了SEEED（Soft Clustering Extended Encoder-Based Error Detection）作为其基于编码器的实现方法。我们通过增强负样本的距离加权来改进软最近邻损失，并引入基于标签的样本排序来选择高度对比的示例，以实现更好的表示学习。SEEED在多个错误标注的对话数据集上优于适应的基线方法（包括GPT-4o和Phi-4），将检测未知错误的准确性提高了最多8个百分点，并表现出对未知意图检测的强大泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although LLM-based conversational agents demonstrate strong fluency andcoherence, they still produce undesirable behaviors (errors) that arechallenging to prevent from reaching users during deployment. Recent researchleverages large language models (LLMs) to detect errors and guideresponse-generation models toward improvement. However, current LLMs struggleto identify errors not explicitly specified in their instructions, such asthose arising from updates to the response-generation model or shifts in userbehavior. In this work, we introduce Automated Error Discovery, a framework fordetecting and defining errors in conversational AI, and propose SEEED (SoftClustering Extended Encoder-Based Error Detection), as an encoder-basedapproach to its implementation. We enhance the Soft Nearest Neighbor Loss byamplifying distance weighting for negative samples and introduce Label-BasedSample Ranking to select highly contrastive examples for better representationlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --across multiple error-annotated dialogue datasets, improving the accuracy fordetecting unknown errors by up to 8 points and demonstrating stronggeneralization to unknown intent detection.</description>
      <author>example@mail.com (Dominic Petrak, Thy Thy Tran, Iryna Gurevych)</author>
      <guid isPermaLink="false">2509.10833v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Socially-Informed Content Analysis of Online Human Behavior</title>
      <link>http://arxiv.org/abs/2509.10807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Doctoral dissertation, University of Southern California, 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文采用计算社会科学技术研究社交媒体带来的挑战，如政治极化、虚假信息、仇恨言论和回音室，并提出数据驱动的解决方案促进更健康的数字互动。&lt;h4&gt;背景&lt;/h4&gt;社交媒体的爆炸性增长不仅改变了通信方式，还带来了政治极化、虚假信息、仇恨言论和回音室等挑战。&lt;h4&gt;目的&lt;/h4&gt;研究社交媒体带来的问题，理解推动负面在线行为的社会动态，并提出数据驱动的解决方案以促进更健康的数字互动。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种可扩展的社会网络表示学习方法，将用户生成的内容与社会连接相结合，创建统一的用户嵌入，从而准确预测和可视化用户属性、社区和行为倾向。&lt;h4&gt;主要发现&lt;/h4&gt;1) Twitter上关于COVID-19的讨论揭示了政治极化和不对称政治回音室；2) 在线仇恨言论表明追求社会认可推动了有毒行为；3) COVID-19讨论的道德基础揭示了道德同质性和回音室的模式，同时表明道德多样性和多元性可以提高跨意识形态分歧的信息传播和接受度。&lt;h4&gt;结论&lt;/h4&gt;这些发现有助于计算社会科学的发展，并为通过社会互动和网络同质性视角理解人类行为提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体的爆炸性增长不仅彻底改变了通信方式，也带来了政治极化、虚假信息、仇恨言论和回音室等挑战。本文采用计算社会科学技术研究这些问题，理解推动负面在线行为的社会动态，并提出数据驱动的解决方案以促进更健康的数字互动。首先，我介绍了一种可扩展的社会网络表示学习方法，将用户生成的内容与社会连接相结合，创建统一的用户嵌入，从而能够准确预测和可视化用户属性、社区和行为倾向。使用这一工具，我探索了三个相互关联的问题：1) Twitter上关于COVID-19的讨论，揭示了政治极化和不对称政治回音室；2) 在线仇恨言论，表明追求社会认可推动了有毒行为；3) COVID-19讨论的道德基础，揭示了道德同质性和回音室的模式，同时表明道德多样性和多元性可以提高跨意识形态分歧的信息传播和接受度。这些发现有助于计算社会科学的发展，并为通过社会互动和网络同质性视角理解人类行为提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The explosive growth of social media has not only revolutionizedcommunication but also brought challenges such as political polarization,misinformation, hate speech, and echo chambers. This dissertation employscomputational social science techniques to investigate these issues, understandthe social dynamics driving negative online behaviors, and propose data-drivensolutions for healthier digital interactions. I begin by introducing a scalablesocial network representation learning method that integrates user-generatedcontent with social connections to create unified user embeddings, enablingaccurate prediction and visualization of user attributes, communities, andbehavioral propensities. Using this tool, I explore three interrelatedproblems: 1) COVID-19 discourse on Twitter, revealing polarization andasymmetric political echo chambers; 2) online hate speech, suggesting thepursuit of social approval motivates toxic behavior; and 3) moral underpinningsof COVID-19 discussions, uncovering patterns of moral homophily and echochambers, while also indicating moral diversity and plurality can improvemessage reach and acceptance across ideological divides. These findingscontribute to the advancement of computational social science and provide afoundation for understanding human behavior through the lens of socialinteractions and network homophily.</description>
      <author>example@mail.com (Julie Jiang)</author>
      <guid isPermaLink="false">2509.10807v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Outlier-Resistant Heterogeneous Treatment Effect Estimation in HDLSS Settings via GAT--CVAE Framework</title>
      <link>http://arxiv.org/abs/2509.10787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对高维小样本设置的异质性治疗效应估计的稳健框架，结合图注意力网络和条件变分自编码器，通过扩展样本空间和聚类分析，实现稳定且可推广的因果效应估计。&lt;h4&gt;背景&lt;/h4&gt;在高维小样本环境下进行异质性治疗效应估计面临挑战，现有方法可能无法有效处理结构依赖性和异常值问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健的HTE估计框架，能够在高维小样本设置中准确捕捉混杂因素间的结构依赖性，并有效整合异常值数据，从而获得稳定且可推广的因果效应估计结果。&lt;h4&gt;方法&lt;/h4&gt;结合图注意力网络捕捉混杂因素间的结构依赖性，以及条件变分自编码器进行潜在表示学习，扩展样本空间并进行聚类分析，将异常值集合整合为连贯的子组，然后使用双重稳健抗异常值估计器估计子组因果效应。&lt;h4&gt;主要发现&lt;/h4&gt;模拟和实际应用表明，该方法相比现有HTE方法具有优越性能，能够有效处理高维小样本数据中的复杂结构和异常值问题。&lt;h4&gt;结论&lt;/h4&gt;该框架为精准医疗和政策评估等领域提供了有效的异质性治疗效应估计工具，具有实际应用价值和推广潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种针对高维小样本设置的异质性治疗效应估计的稳健框架。通过结合图注意力网络来捕捉混杂因素之间的结构依赖性，以及使用条件变分自编码器进行潜在表示学习，我们的方法扩展了样本空间并执行聚类，将异常值集合整合为连贯的子组。然后使用双重稳健抗异常值估计器估计子组因果效应，产生稳定且可推广的结果。模拟和实际应用证实了与现有HTE方法相比的优越性能，突显了该框架在精准医疗和政策评估方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a robust framework for heterogeneous treatment effect (HTE)estimation tailored to high-dimensional low sample size (HDLSS) settings. Bycombining Graph Attention Networks (GAT) to capture structural dependenciesamong confounders with a Conditional Variational Autoencoder (CVAE) for latentrepresentation learning, our method expands the sample space and performsclustering that integrates even outlier sets into coherent subgroups.Clusterwise causal effects are then estimated using a doubly robustoutlier-resistant estimator, yielding stable and generalizable results.Simulations and real-world applications confirm superior performance comparedwith existing HTE methods, highlighting the framework's potential for precisionmedicine and policy evaluation.</description>
      <author>example@mail.com (Byeonghee Lee, Joonsung Kang)</author>
      <guid isPermaLink="false">2509.10787v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning</title>
      <link>http://arxiv.org/abs/2509.10555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SurgLaVi，迄今为止最大且多样化的手术视觉语言数据集，包含近24万剪辑-字幕对，涵盖200多种手术程序，并具有层次化结构。研究还开发了SurgCLIP模型，在多项任务上超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;视觉语言预训练(VLP)在手术领域具有独特优势，能够将语言与手术视频对齐，实现工作流程理解和跨任务迁移，无需依赖专家标注数据集。然而，现有手术VLP数据集受限于规模小、程序多样性不足、语义质量不高和层次结构不完善等问题。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模、多样化、语义质量高且具有层次结构的手术视觉语言数据集，以促进手术基础模型的发展。&lt;h4&gt;方法&lt;/h4&gt;开发了一个全自动管道系统，系统生成手术视频的精细转录并将其分割为连贯的程序单元；应用双模态过滤去除不相关和噪声样本，确保高质量注释；推出开源版本SurgLaVi-eta，包含11.3万剪辑-字幕对，完全基于公共数据构建；引入SurgCLIP模型，一种具有双编码器的CLIP风格视频文本对比框架。&lt;h4&gt;主要发现&lt;/h4&gt;SurgCLIP在阶段、步骤、动作和工具识别任务上实现了持续改进，显著超越了先前最先进的方法；大规模、语义丰富和层次结构化的数据集直接转化为更强和更可泛化的表示能力。&lt;h4&gt;结论&lt;/h4&gt;SurgLaVi作为开发手术基础模型的关键资源，证明了高质量、大规模数据集对提升手术AI模型性能的重要性，为手术领域的基础模型研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言预训练(VLP)通过将语言与手术视频对齐，为手术领域提供了独特优势，使工作流程理解和跨任务迁移成为可能，无需依赖专家标注的数据集。然而，现有手术VLP的进展受限于现有数据集的规模有限、程序多样性不足、语义质量和层次结构等问题。在本工作中，我们提出了SurgLaVi，迄今为止最大且最多样化的手术视觉语言数据集，包含来自200多种程序的近24万剪辑-字幕对，并在阶段、步骤和任务级别具有层次结构。SurgLaVi的核心是一个全自动管道，系统生成手术视频的精细转录并将其分割为连贯的程序单元。为确保高质量注释，它应用双模态过滤去除不相关和噪声样本。在此框架内，生成的字幕通过上下文细节进行丰富，产生语义丰富且易于解释的注释。为确保可访问性，我们发布了SurgLaVi-eta，这是一个完全基于公共数据构建的开源衍生版本，包含11.3万剪辑-字幕对，比现有手术VLP数据集大四倍以上。为证明SurgLaVi数据集的价值，我们引入了SurgCLIP，一种具有双编码器的CLIP风格视频文本对比框架，作为代表性基础模型。SurgCLIP在阶段、步骤、动作和工具识别方面取得了一致的改进，显著超越了先前最先进的方法。这些结果验证了大规模、语义丰富和层次结构化的数据集直接转化为更强和更可泛化的表示能力，确立了SurgLaVi作为开发手术基础模型的关键资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language pre-training (VLP) offers unique advantages for surgery byaligning language with surgical videos, enabling workflow understanding andtransfer across tasks without relying on expert-labeled datasets. However,progress in surgical VLP remains constrained by the limited scale, proceduraldiversity, semantic quality, and hierarchical structure of existing datasets.In this work, we present SurgLaVi, the largest and most diverse surgicalvision-language dataset to date, comprising nearly 240k clip-caption pairs frommore than 200 procedures, and comprising hierarchical levels at phase-, step-,and task-level. At the core of SurgLaVi lies a fully automated pipeline thatsystematically generates fine-grained transcriptions of surgical videos andsegments them into coherent procedural units. To ensure high-qualityannotations, it applies dual-modality filtering to remove irrelevant and noisysamples. Within this framework, the resulting captions are enriched withcontextual detail, producing annotations that are both semantically rich andeasy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, anopen-source derivative of 113k clip-caption pairs constructed entirely frompublic data, which is over four times larger than existing surgical VLPdatasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,a CLIP-style video-text contrastive framework with dual encoders, as arepresentative base model. SurgCLIP achieves consistent improvements acrossphase, step, action, and tool recognition, surpassing prior state-of-the-artmethods, often by large margins. These results validate that large-scale,semantically rich, and hierarchically structured datasets directly translateinto stronger and more generalizable representations, establishing SurgLaVi asa key resource for developing surgical foundation models.</description>
      <author>example@mail.com (Alejandra Perez, Chinedu Nwoye, Ramtin Raji Kermani, Omid Mohareri, Muhammad Abdullah Jamal)</author>
      <guid isPermaLink="false">2509.10555v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Deriving accurate galaxy cluster masses using X-ray thermodynamic profiles and graph neural networks</title>
      <link>http://arxiv.org/abs/2509.12199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 15 figures, 6 tables, resubmitted to A&amp;A after revision,  comments welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用图神经网络(GNNs)从X射线观测数据估计星系团质量，实现了比传统方法更精确的结果，并发现了SZ方法中存在的质量依赖性偏差。&lt;h4&gt;背景&lt;/h4&gt;精确测定星系团质量对于建立星系团宇宙学中可靠的质量-观测标度关系至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来精确估计星系团质量，并验证其准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNNs)处理X射线观测推断的星系团内介质(ICM)径向采样剖面，将每个ICM剖面表示为图，以处理可变长度和分辨率的输入。使用The Three Hundred Project的星系团流体动力学模拟来训练和测试模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 与真实质量相比没有系统偏差；2) 质量估计散射约为6%，比传统方法小6倍；3) 算法对数据质量和星系团形态具有鲁棒性；4) 发现SZ推导的质量存在质量依赖性偏差，质量越高的星系团偏差越大；5) 中位偏差为(1-b)=0.85_{-14}^{+34}。&lt;h4&gt;结论&lt;/h4&gt;通过整合X射线、SZ和光学数据集使用深度学习技术，在建立无偏观测质量标度关系方面迈出了重要一步，增强了星系团在精密宇宙学中的作用。&lt;h4&gt;翻译&lt;/h4&gt;精确测定星系团质量对于建立星系团宇宙学中可靠的质量-观测标度关系至关重要。我们采用图神经网络(GNNs)从X射线观测推断的星系团内介质(ICM)径向采样剖面来估计星系团质量。GNNs通过将每个ICM剖面表示为图，自然处理可变长度和分辨率的输入，能够准确灵活地建模各种观测条件。我们使用The Three Hundred Project的最先进星系团流体动力学模拟来训练和测试GNN模型。与模拟中的真实星系团质量相比，我们方法的质量估计没有系统偏差。此外，我们实现恢复质量与真实质量约为6%的散射，比标准流体静力学平衡方法获得的散射小6倍。我们的算法对数据质量和星系团形态具有鲁棒性，能够同时结合模型不确定性和观测不确定性。最后，我们将该技术应用于XMM-Newton观测的星系团样本，并将GNN推导的质量估计与通过Y_SZ-M_500标度关系获得的质量估计进行比较。我们的结果提供了强有力的证据(5σ水平)表明SZ推导的质量存在质量依赖性偏差，质量更高的星系团表现出更大的偏差程度。此外，我们发现中位偏差为(1-b)=0.85_{-14}^{+34}，但由于其质量依赖性而具有显著的弥散。这项工作通过整合X射线、SZ和光学数据集使用深度学习技术，在建立无偏观测质量标度关系方面迈出了重要一步，从而增强了星系团在精密宇宙学中的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Precise determination of galaxy cluster masses is crucial for establishingreliable mass-observable scaling relations in cluster cosmology. We employgraph neural networks (GNNs) to estimate galaxy cluster masses from radiallysampled profiles of the intra-cluster medium (ICM) inferred from X-rayobservations. GNNs naturally handle inputs of variable length and resolution byrepresenting each ICM profile as a graph, enabling accurate and flexiblemodeling across diverse observational conditions. We trained and tested GNNmodel using state-of-the-art hydrodynamical simulations of galaxy clusters fromThe Three Hundred Project. The mass estimates using our method exhibit nosystematic bias compared to the true cluster masses in the simulations.Additionally, we achieve a scatter in recovered mass versus true mass of about6\%, which is a factor of six smaller than obtained from a standard hydrostaticequilibrium approach. Our algorithm is robust to both data quality and clustermorphology and it is capable of incorporating model uncertainties alongsideobservational uncertainties. Finally, we apply our technique to XMM-Newtonobserved galaxy cluster samples and compare the GNN derived mass estimates withthose obtained with $Y_{\rm SZ}$-M$_{500}$ scaling relations. Our resultsprovide strong evidence, at 5$\sigma$ level, for a mass-dependent bias in SZderived masses, with higher mass clusters exhibiting a greater degree ofdeviation. Furthermore, we find the median bias to be $(1-b)=0.85_{-14}^{+34}$,albeit with significant dispersion due to its mass dependence. This work takesa significant step towards establishing unbiased observable mass scalingrelations by integrating X-ray, SZ and optical datasets using deep learningtechniques, thereby enhancing the role of galaxy clusters in precisioncosmology.</description>
      <author>example@mail.com (Asif Iqbal, Subhabrata Majumdar, Elena Rasia, Gabriel W. Pratt, Daniel de Andres, Jean-Baptiste Melin, Weiguang Cui)</author>
      <guid isPermaLink="false">2509.12199v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data</title>
      <link>http://arxiv.org/abs/2509.12143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 1 figure, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种结合视觉Transformer和图神经网络的统一方法，用于通过结构磁共振成像自动检测重度抑郁症，有效提高了诊断准确性。&lt;h4&gt;背景&lt;/h4&gt;重度抑郁症是一种常见的精神健康问题，影响个人幸福感和全球公共卫生。使用结构磁共振成像和深度学习方法自动检测MDD有望提高诊断准确性和实现早期干预。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉复杂脑部模式的方法，超越现有的仅使用体素级特征或手工制作区域表征的方法。&lt;h4&gt;方法&lt;/h4&gt;开发了统一流程，使用视觉Transformer从sMRI数据中提取3D区域嵌入，使用图神经网络进行分类。探索了两种定义区域的方法：基于图谱的方法使用预定义的脑图谱，基于立方体的方法直接从3D补丁中识别区域。生成余弦相似度图建模区域间关系，指导GNN分类。使用REST-meta-MDD数据集进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;最佳模型在分层10折交叉验证中获得了78.98%的准确率、76.54%的敏感性、81.58%的特异性、81.58%的精确度和78.98%的F1分数。基于图谱的模型始终优于基于立方体的方法。&lt;h4&gt;结论&lt;/h4&gt;使用特定领域的解剖先验对于MDD检测至关重要，基于预定义脑图谱的方法表现更好。&lt;h4&gt;翻译&lt;/h4&gt;重度抑郁症是一种常见的精神健康问题，对个人幸福感和全球公共卫生产生负面影响。使用结构磁共振成像和深度学习方法自动检测MDD有望提高诊断准确性和实现早期干预。大多数现有方法使用体素级特征或基于预定义脑图谱构建的手工制作区域表征，限制了它们捕捉复杂脑部模式的能力。本文开发了一个统一流程，利用视觉Transformer从sMRI数据中提取3D区域嵌入，并使用图神经网络进行分类。我们探索了两种定义区域的方法：(1)使用预定义的结构性和功能性脑图谱的基于图谱方法，(2)基于立方体的方法，ViTs直接训练以从均匀提取的3D补丁中识别区域。此外，生成余弦相似度图来建模区域间关系，并指导基于GNN的分类。使用REST-meta-MDD数据集进行了广泛的实验以证明我们模型的有效性。通过分层10折交叉验证，最佳模型获得了78.98%的准确率、76.54%的敏感性、81.58%的特异性、81.58%的精确度和78.98%的F1分数。此外，基于图谱的模型始终优于基于立方体的方法，突显了使用特定领域的解剖先验进行MDD检测的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Major depressive disorder (MDD) is a prevalent mental health condition thatnegatively impacts both individual well-being and global public health.Automated detection of MDD using structural magnetic resonance imaging (sMRI)and deep learning (DL) methods holds increasing promise for improvingdiagnostic accuracy and enabling early intervention. Most existing methodsemploy either voxel-level features or handcrafted regional representationsbuilt from predefined brain atlases, limiting their ability to capture complexbrain patterns. This paper develops a unified pipeline that utilizes VisionTransformers (ViTs) for extracting 3D region embeddings from sMRI data andGraph Neural Network (GNN) for classification. We explore two strategies fordefining regions: (1) an atlas-based approach using predefined structural andfunctional brain atlases, and (2) an cube-based method by which ViTs aretrained directly to identify regions from uniformly extracted 3D patches.Further, cosine similarity graphs are generated to model interregionalrelationships, and guide GNN-based classification. Extensive experiments wereconducted using the REST-meta-MDD dataset to demonstrate the effectiveness ofour model. With stratified 10-fold cross-validation, the best model obtained78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and78.98% F1-score. Further, atlas-based models consistently outperformed thecube-based approach, highlighting the importance of using domain-specificanatomical priors for MDD detection.</description>
      <author>example@mail.com (Nojod M. Alotaibi, Areej M. Alhothali, Manar S. Ali)</author>
      <guid isPermaLink="false">2509.12143v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework</title>
      <link>http://arxiv.org/abs/2509.12043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript has been accepted as a REGULAR PAPER in the  Transactions on Intelligent Transportation Systems 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的交通流预测框架，通过自适应邻接矩阵和天气因素调整来处理交通随机性，并使用自适应一致性预测进行不确定性量化，实验证明该方法具有更好的预测精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;交通流预测对管理拥堵、提高安全和优化交通系统至关重要，但由于城市交通的随机性和环境因素，这仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发能够适应由多个动态和复杂相互依赖因素影响的交通变异性的模型，以实现更好的预测。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图神经网络框架，使用对数正态分布和变异系数值的自适应邻接矩阵反映真实世界的行程时间变异性；通过温度、风速和降水等天气因素调整边权重，使GNN能够捕获交通站点之间不断变化的时空依赖关系；使用自适应一致性预测框架提供可靠的不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与基线方法相比，所提出的模型显示出更好的预测精度和不确定性边界；在SUMO中构建交通场景并应用蒙特卡洛模拟推导测试车辆的行程时间分布，模拟的平均行程时间落在INRIX历史数据定义的区间内。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型能够有效适应交通随机性和变化的环境条件，验证了模型的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;交通流预测对于管理拥堵、提高安全和优化各种交通系统至关重要。然而，由于城市交通的随机性和环境因素，这仍然是一个普遍存在的挑战。更好的预测需要能够适应由多个动态和复杂相互依赖因素影响的交通变异性的模型。在这项工作中，我们提出了一种图神经网络框架，通过使用对数正态分布和变异系数值的自适应邻接矩阵来处理随机性，以反映真实世界的行程时间变异性。此外，温度、风速和降水等天气因素调整边权重，使GNN能够捕获交通站点之间不断变化的时空依赖关系。这种对静态邻接矩阵的增强使模型能够有效适应交通随机性和变化的环境条件。此外，我们使用自适应一致性预测框架提供可靠的不确定性量化，在实现目标覆盖率的同时保持可接受的预测区间。实验结果表明，与基线方法相比，所提出的模型显示出更好的预测精度和不确定性边界。然后，我们通过在SUMO中构建交通场景并应用蒙特卡洛模拟来推导测试车辆的行程时间分布，以反映真实世界的变异性，从而验证了该方法。测试车辆的模拟平均行程时间落在INRIX历史数据定义的区间内，验证了模型的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic flow forecasting is essential for managing congestion, improvingsafety, and optimizing various transportation systems. However, it remains aprevailing challenge due to the stochastic nature of urban traffic andenvironmental factors. Better predictions require models capable ofaccommodating the traffic variability influenced by multiple dynamic andcomplex interdependent factors. In this work, we propose a Graph Neural Network(GNN) framework to address the stochasticity by leveraging adaptive adjacencymatrices using log-normal distributions and Coefficient of Variation (CV)values to reflect real-world travel time variability. Additionally, weatherfactors such as temperature, wind speed, and precipitation adjust edge weightsand enable GNN to capture evolving spatio-temporal dependencies across trafficstations. This enhancement over the static adjacency matrix allows the model toadapt effectively to traffic stochasticity and changing environmentalconditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP)framework to provide reliable uncertainty quantification, achieving targetcoverage while maintaining acceptable prediction intervals. Experimentalresults demonstrate that the proposed model, in comparison with baselinemethods, showed better prediction accuracy and uncertainty bounds. We, then,validate this method by constructing traffic scenarios in SUMO and applyingMonte-Carlo simulation to derive a travel time distribution for a Vehicle UnderTest (VUT) to reflect real-world variability. The simulated mean travel time ofthe VUT falls within the intervals defined by INRIX historical data, verifyingthe model's robustness.</description>
      <author>example@mail.com (Mayur Patil, Qadeer Ahmed, Shawn Midlam-Mohler)</author>
      <guid isPermaLink="false">2509.12043v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Prior Observations for Incremental 3D Scene Graph Prediction</title>
      <link>http://arxiv.org/abs/2509.11895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 24th International Conference on Machine Learning and  Applications (ICMLA'25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于增量3D语义场景图预测的新型异构图模型，该模型通过消息传递过程整合多模态信息，无需完整场景重建即可灵活融合全局和局部场景表示。&lt;h4&gt;背景&lt;/h4&gt;3D语义场景图(3DSSG)通过显式建模对象、属性和关系为环境提供紧凑结构化表示，但现有方法主要依赖传感器数据，未充分整合语义丰富环境中的信息，且大多假设可访问完整场景重建，限制了在现实世界增量设置中的适用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型异构图模型，用于增量3DSSG预测，整合多模态信息到消息传递过程中，使其能在不依赖完整场景重建的情况下工作。&lt;h4&gt;方法&lt;/h4&gt;提出使用多层异构图模型，通过消息传递过程整合多模态信息如语义嵌入和先验观测，灵活融合全局和局部场景表示，无需专门模块或完整场景重建。&lt;h4&gt;主要发现&lt;/h4&gt;在3DSSG数据集上评估表明，通过多模态信息增强的图神经网络为复杂现实世界环境提供了可扩展且可推广的解决方案。&lt;h4&gt;结论&lt;/h4&gt;所提出的架构通过整合多模态信息，为增量3D语义场景图预测提供了有效方法，适用于现实世界的增量设置。&lt;h4&gt;翻译&lt;/h4&gt;3D语义场景图(3DSSG)通过显式建模对象、属性和关系，为环境提供紧凑的结构化表示。尽管3DSSG在机器人和具身AI中显示出潜力，但现有方法主要依赖传感器数据，没有进一步整合语义丰富环境中的信息。此外，大多数方法假设可以访问完整的场景重建，限制了它们在现实世界增量设置中的适用性。本文介绍了一种用于增量3DSSG预测的新型异构图模型，该模型通过消息传递过程直接整合了额外的多模态信息，如先验观测。利用多层，该模型灵活地融合全局和局部场景表示，而无需专门模块或完整场景重建。我们在3DSSG数据集上评估了我们的方法，表明通过多模态信息（如语义嵌入（例如CLIP）和先验观测）增强的图神经网络为复杂现实世界环境提供了一种可扩展且可推广的解决方案。所提出架构的完整源代码将在https://github.com/m4renz/incremental-scene-graph-prediction上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D语义场景图(3DSSG)预测中两个关键问题：一是现有方法主要依赖传感器数据，没有充分利用环境中丰富的语义信息；二是大多数方法假设可以访问完整的场景重建，这在现实世界中是不切实际的。这个问题很重要，因为3DSSG在机器人和具身AI领域潜力巨大，能提供环境的紧凑结构化表示，而实际应用中场景通常是从传感器数据流逐步捕获的，需要模型能够利用先前观察的信息来预测新输入。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，发现它们缺乏通用的信息整合机制且不适用于增量场景生成。作者借鉴了SceneGraphFusion的增量生成思想和Feng等人的历史预测整合方法，但改进了这些方法。作者设计了一个多层异构图模型，融合全局和局部信息，将先前观察直接整合到消息传递过程中，而不是像现有方法那样只使用更新的几何信息或显式编码全局信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个两层异构图架构：全局场景图积累先前帧的观察和关系，局部场景图处理当前帧数据，并通过连接匹配的对象实例使信息在两层间流动。整体流程是：1)预处理RGB-D帧数据；2)构建图模型，包括对象分割、点云转换和边连接；3)提取节点特征，包括点云采样和几何描述符计算；4)使用异构图神经网络进行消息传递；5)通过多层感知机进行节点和边分类预测；6)将预测结果合并到全局图中供后续使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新型异构图模型整合多模态信息用于增量预测；2)多层架构灵活整合全局和局部场景表示；3)将先前观察直接嵌入消息传递过程；4)展示对错误预测的鲁棒性；5)能无缝整合额外信息而无需修改核心架构。相比之前工作，不同之处在于：与SceneGraphFusion相比，直接整合先前的预测；与Feng等人的方法相比，不显式编码全局信息；与其他多模态方法相比，首次应用于增量3DSSG生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的异构图模型，通过整合先前观察的多模态信息，实现了高效的增量3D场景图预测，显著提升了机器人在复杂环境中的感知和理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D semantic scene graphs (3DSSG) provide compact structured representationsof environments by explicitly modeling objects, attributes, and relationships.While 3DSSGs have shown promise in robotics and embodied AI, many existingmethods rely mainly on sensor data, not integrating further information fromsemantically rich environments. Additionally, most methods assume access tocomplete scene reconstructions, limiting their applicability in real-world,incremental settings. This paper introduces a novel heterogeneous graph modelfor incremental 3DSSG prediction that integrates additional, multi-modalinformation, such as prior observations, directly into the message-passingprocess. Utilizing multiple layers, the model flexibly incorporates global andlocal scene representations without requiring specialized modules or full scenereconstructions. We evaluate our approach on the 3DSSG dataset, showing thatGNNs enriched with multi-modal information such as semantic embeddings (e.g.,CLIP) and prior observations offer a scalable and generalizable solution forcomplex, real-world environments. The full source code of the presentedarchitecture will be made available athttps://github.com/m4renz/incremental-scene-graph-prediction.</description>
      <author>example@mail.com (Marian Renz, Felix Igelbrink, Martin Atzmueller)</author>
      <guid isPermaLink="false">2509.11895v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Descriptor and Graph-based Molecular Representations in Prediction of Copolymer Properties Using Machine Learning</title>
      <link>http://arxiv.org/abs/2509.11874v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究使用机器学习方法预测共聚物的物理性质，通过两种不同的分子表示方法建立了预测模型，并发现不同表示方法对不同类型的物理性质预测效果不同。&lt;h4&gt;背景&lt;/h4&gt;共聚物是高度通用的材料，具有广泛的化学成分可能性。通过计算方法预测性质可以加速共聚物设计，优先选择具有有利性质的候选材料。&lt;h4&gt;目的&lt;/h4&gt;利用两种不同的分子集合表示方法，通过机器学习预测共聚物的七种不同物理性质，并比较不同表示方法的预测效果。&lt;h4&gt;方法&lt;/h4&gt;1. 使用随机森林模型从分子描述符预测聚合物性质；2. 使用图神经网络从2D聚合物图预测相同性质，包括单任务和多任务设置；3. 构建了一个包含140种具有不同单体组成和构型的二元共聚物的分子动力学模拟数据集；4. 训练和评估模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基于描述符的随机森林模型在预测密度和定压/定容比热容方面表现出色，因为这些性质与分子描述符捕获的特定分子特征密切相关；2. 图表示方法更好地预测膨胀系数和体积模量，因为这些性质更依赖于图模型更好地捕获的复杂结构相互作用。&lt;h4&gt;结论&lt;/h4&gt;这项研究强调了选择适当表示方法预测分子性质的重要性。研究结果展示了机器学习模型如何通过可学习的结构-性质关系加速共聚物发现，简化聚合物设计，促进高性能材料在多种应用中的开发。&lt;h4&gt;翻译&lt;/h4&gt;共聚物是高度通用的材料，具有广泛的化学成分可能性。通过使用计算方法进行性质预测，可以加速共聚物设计，优先选择具有有利性质的候选材料。在本研究中，我们利用分子集合的两种不同表示方法，通过机器学习预测共聚物的七种不同物理性质：我们使用随机森林模型从分子描述符预测聚合物性质，使用图神经网络从2D聚合物图预测相同性质，包括单任务和多任务设置。为了训练和评估模型，我们构建了一个数据集，包含来自分子动力学模拟的140种具有不同单体组成和构型的二元共聚物。我们的结果表明，基于描述符的随机森林在预测密度和定压/定容比热容方面表现出色，因为这些性质与分子描述符捕获的特定分子特征密切相关。相比之下，图表示方法更好地预测膨胀系数和体积模量，因为这些性质更依赖于图模型更好地捕获的复杂结构相互作用。这项研究强调了选择适当表示方法预测分子性质的重要性。我们的研究展示了机器学习模型如何通过可学习的结构-性质关系加速共聚物发现，简化聚合物设计，促进高性能材料在多种应用中的开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Copolymers are highly versatile materials with a vast range of possiblechemical compositions. By using computational methods for property prediction,the design of copolymers can be accelerated, allowing for the prioritization ofcandidates with favorable properties. In this study, we utilized two distinctrepresentations of molecular ensembles to predict the seven different physicalpolymer properties copolymers using machine learning: we used a random forest(RF) model to predict polymer properties from molecular descriptors, and agraph neural network (GNN) to predict the same properties from 2D polymergraphs under both a single- and multi-task setting. To train and evaluate themodels, we constructed a data set from molecular dynamic simulations for 140binary copolymers with varying monomer compositions and configurations. Ourresults demonstrate that descriptors-based RFs excel at predicting density andspecific heat capacities at constant pressure (Cp) and volume (Cv) becausethese properties are strongly tied to specific molecular features captured bymolecular descriptors. In contrast, graph representations better predictexpansion coefficients ({\gamma}, {\alpha}) and bulk modulus (K), which dependmore on complex structural interactions better captured by graph-based models.This study underscores the importance of choosing appropriate representationsfor predicting molecular properties. Our findings demonstrate how machinelearning models can expedite copolymer discovery with learnablestructure-property relationships, streamlining polymer design and advancing thedevelopment of high-performance materials for diverse applications.</description>
      <author>example@mail.com (Elaheh Kazemi-Khasragh, Rocío Mercado, Carlos Gonzalez, Maciej Haranczyk)</author>
      <guid isPermaLink="false">2509.11874v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Visualization and Analysis of the Loss Landscape in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.11792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过引入可学习的降维方法来可视化GNN损失景观，并分析了多种因素对GNN优化的影响，为开发更高效的GNN架构和训练策略提供了见解。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是处理图结构数据的强大模型，有广泛应用，但其参数优化、表达能力和泛化能力之间的相互作用仍理解不充分。&lt;h4&gt;目的&lt;/h4&gt;引入一种高效的、可学习的降维方法来可视化GNN损失景观，并分析过平滑、量化、稀疏化和预调节器对GNN优化的影响。&lt;h4&gt;方法&lt;/h4&gt;提出一种可学习的投影方法，优于基于PCA的最先进方法，能够以更低的内存使用量准确重建高维参数。&lt;h4&gt;主要发现&lt;/h4&gt;架构、稀疏化和优化器的预调节显著影响GNN优化景观，进而影响训练过程和最终预测性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现有助于开发更高效的GNN架构设计和训练策略。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络是处理图结构数据的强大模型，有广泛应用。然而，GNN参数优化、表达能力和泛化能力之间的相互作用仍理解不充分。我们通过引入一种高效的、可学习的降维方法来可视化GNN损失景观，并分析了过平滑、跳跃知识、量化、稀疏化和预调节器对GNN优化的影响。我们的可学习投影方法优于基于PCA的最先进方法，能够以更低的内存使用量准确重建高维参数。我们进一步表明，架构、稀疏化和优化器的预调节显著影响GNN优化景观及其训练过程和最终预测性能。这些见解有助于开发更高效的GNN架构设计和训练策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-04552-2_9&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are powerful models for graph-structured data,with broad applications. However, the interplay between GNN parameteroptimization, expressivity, and generalization remains poorly understood. Weaddress this by introducing an efficient learnable dimensionality reductionmethod for visualizing GNN loss landscapes, and by analyzing the effects ofover-smoothing, jumping knowledge, quantization, sparsification, andpreconditioner on GNN optimization. Our learnable projection method surpassesthe state-of-the-art PCA-based approach, enabling accurate reconstruction ofhigh-dimensional parameters with lower memory usage. We further show thatarchitecture, sparsification, and optimizer's preconditioning significantlyimpact the GNN optimization landscape and their training process and finalprediction performance. These insights contribute to developing more efficientdesigns of GNN architectures and training strategies.</description>
      <author>example@mail.com (Samir Moustafa, Lorenz Kummer, Simon Fetzel, Nils M. Kriege, Wilfried N. Gansterer)</author>
      <guid isPermaLink="false">2509.11792v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Regression for Enzyme Turnover Rates Prediction</title>
      <link>http://arxiv.org/abs/2509.11782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures. This paper was withdrawn from the IJCAI 2025  proceedings due to the lack of participation in the conference and  presentation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多模态框架，用于预测酶转换率，整合了酶序列、底物结构和环境因素，实现了可解释且准确的预测。&lt;h4&gt;背景&lt;/h4&gt;酶转换率是酶动力学中的基本参数，反映了酶的催化效率。然而，由于实验测量的高成本和复杂性，大多数生物体中的酶转换率数据仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;解决酶转换率数据稀缺的问题，开发一种能够准确预测酶转换率的工具，为酶动力学研究和相关应用提供支持。&lt;h4&gt;方法&lt;/h4&gt;研究提出的多模态框架结合了预训练语言模型和卷积神经网络提取蛋白质序列特征，使用图神经网络捕获底物分子表示，并加入注意力机制增强酶和底物间的相互作用。同时利用Kolmogorov-Arnold网络的符号回归学习控制酶转换率的数学公式。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该框架优于传统的和最先进的深度学习方法，为研究酶动力学提供了强大的工具。&lt;h4&gt;结论&lt;/h4&gt;这项工作为研究酶动力学提供了强大工具，在酶工程、生物技术和工业生物催化应用方面具有广阔前景。&lt;h4&gt;翻译&lt;/h4&gt;酶转换率是酶动力学中的一个基本参数，反映了酶的催化效率。然而，由于实验测量的高成本和复杂性，大多数生物体中的酶转换率数据仍然稀缺。为解决这一差距，我们提出了一种多模态框架来预测酶转换率，该框架整合了酶序列、底物结构和环境因素。我们的模型结合了预训练语言模型和卷积神经网络来提取蛋白质序列特征，同时图神经网络捕获底物分子的信息表示。加入了注意力机制以增强酶和底物表示之间的相互作用。此外，我们利用Kolmogorov-Arnold网络的符号回归来明确学习控制酶转换率的数学公式，实现了可解释且准确的预测。大量实验表明，我们的框架优于传统的和最先进的深度学习方法。这项工作为研究酶动力学提供了强大的工具，并在酶工程、生物技术和工业生物催化应用方面具有广阔前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The enzyme turnover rate is a fundamental parameter in enzyme kinetics,reflecting the catalytic efficiency of enzymes. However, enzyme turnover ratesremain scarce across most organisms due to the high cost and complexity ofexperimental measurements. To address this gap, we propose a multimodalframework for predicting the enzyme turnover rate by integrating enzymesequences, substrate structures, and environmental factors. Our model combinesa pre-trained language model and a convolutional neural network to extractfeatures from protein sequences, while a graph neural network capturesinformative representations from substrate molecules. An attention mechanism isincorporated to enhance interactions between enzyme and substraterepresentations. Furthermore, we leverage symbolic regression viaKolmogorov-Arnold Networks to explicitly learn mathematical formulas thatgovern the enzyme turnover rate, enabling interpretable and accuratepredictions. Extensive experiments demonstrate that our framework outperformsboth traditional and state-of-the-art deep learning approaches. This workprovides a robust tool for studying enzyme kinetics and holds promise forapplications in enzyme engineering, biotechnology, and industrial biocatalysis.</description>
      <author>example@mail.com (Bozhen Hu, Cheng Tan, Siyuan Li, Jiangbin Zheng, Sizhe Qiu, Jun Xia, Stan Z. Li)</author>
      <guid isPermaLink="false">2509.11782v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SpaPool: Soft Partition Assignment Pooling for__Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.11675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SpaPool的新型池化方法，结合了密集和稀疏技术的优点，用于图神经网络处理。&lt;h4&gt;背景&lt;/h4&gt;图神经网络处理需要有效的池化方法来减少图的大小同时保持结构完整性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效减少图大小同时保持结构完整性的池化方法。&lt;h4&gt;方法&lt;/h4&gt;SpaPool通过将顶点分组为自适应数量的簇，结合密集和稀疏技术的优势来实现图的池化。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的实验表明，SpaPool与现有池化技术相比具有竞争力，特别是在小规模图上表现优异。&lt;h4&gt;结论&lt;/h4&gt;SpaPool是一种有前途的方法，适用于需要高效有效图处理的应用。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了SpaPool，一种新颖的池化方法，它结合了密集和稀疏技术的优点，用于图神经网络。SpaPool将顶点分组为自适应数量的簇，利用密集和稀疏方法的优势。它旨在在保持图结构完整性的同时高效减少其大小。在多个数据集上的实验结果表明，与现有的池化技术相比，SpaPool具有竞争力的性能，并特别在小规模图上表现出色。这使得SpaPool成为需要高效有效图处理的应用的一种有前途的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SpaPool, a novel pooling method that combines thestrengths of both dense and sparse techniques for a graph neural network.SpaPool groups vertices into an adaptive number of clusters, leveraging thebenefits of both dense and sparse approaches. It aims to maintain thestructural integrity of the graph while reducing its size efficiently.Experimental results on several datasets demonstrate that SpaPool achievescompetitive performance compared to existing pooling techniques and excelsparticularly on small-scale graphs. This makes SpaPool a promising method forapplications requiring efficient and effective graph processing.</description>
      <author>example@mail.com (Rodrigue Govan, Romane Scherrer, Philippe Fournier-Viger, Nazha Selmaoui-Folcher)</author>
      <guid isPermaLink="false">2509.11675v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Drug Repurposing Using Deep Embedded Clustering and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.11493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 2025 International Conference on Machine Learning and  Applications (ICMLA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合无监督深度嵌入聚类和监督图神经网络的机器学习流程，用于从多组学数据中识别新的药物-疾病链接。&lt;h4&gt;背景&lt;/h4&gt;药物再利用在历史上是一个经济上不可行的过程，用于识别废弃药物的新用途。现代机器学习能够识别候选药物中的复杂生化细节，但许多研究依赖于具有已知药物-疾病相似性的简化数据集。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习流程，使用无监督深度嵌入聚类与监督图神经网络链接预测相结合，从多组学数据中识别新的药物-疾病链接。&lt;h4&gt;方法&lt;/h4&gt;使用无监督自编码器和聚类训练将组学数据的维度压缩为潜在嵌入。将9022种独特药物划分为35个簇，并使用图神经网络进行链接预测。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络实现了强大的统计性能，预测准确率为0.901，ROC曲线下面积为0.960，F1得分为0.901。生成了一个包含477个每个簇链接概率超过99%的排序列表。&lt;h4&gt;结论&lt;/h4&gt;这项研究可能在不同疾病领域提供新的药物-疾病链接前景，同时推进对药物再利用研究中机器学习的理解。&lt;h4&gt;翻译&lt;/h4&gt;药物再利用在历史上一直是识别废弃药物新用途的经济上不可行的过程。现代机器学习使得识别候选药物中的复杂生化细节成为可能；然而，许多研究依赖于具有已知药物-疾病相似性的简化数据集。我们提出了一种机器学习流程，使用无监督深度嵌入聚类，结合监督图神经网络链接预测，从多组学数据中识别新的药物-疾病链接。无监督自编码器和聚类训练将组学数据的维度压缩为压缩的潜在嵌入。总共9022种独特药物被划分为35个簇，平均轮廓得分为0.8550。图神经网络实现了强大的统计性能，预测准确率为0.901，受试者工作特征曲线下面积为0.960，F1得分为0.901。生成了一个包含477个每个簇链接概率超过99%的排序列表。这项研究可能在不同疾病领域提供新的药物-疾病链接前景，同时推进对药物再利用研究中机器学习的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug repurposing has historically been an economically infeasible process foridentifying novel uses for abandoned drugs. Modern machine learning has enabledthe identification of complex biochemical intricacies in candidate drugs;however, many studies rely on simplified datasets with known drug-diseasesimilarities. We propose a machine learning pipeline that uses unsuperviseddeep embedded clustering, combined with supervised graph neural network linkprediction to identify new drug-disease links from multi-omic data.Unsupervised autoencoder and cluster training reduced the dimensionality ofomic data into a compressed latent embedding. A total of 9,022 unique drugswere partitioned into 35 clusters with a mean silhouette score of 0.8550. Graphneural networks achieved strong statistical performance, with a predictionaccuracy of 0.901, receiver operating characteristic area under the curve of0.960, and F1-Score of 0.901. A ranked list comprised of 477 per-cluster linkprobabilities exceeding 99 percent was generated. This study could provide newdrug-disease link prospects across unrelated disease domains, while advancingthe understanding of machine learning in drug repurposing studies.</description>
      <author>example@mail.com (Luke Delzer, Robert Kroleski, Ali K. AlShami, Jugal Kalita)</author>
      <guid isPermaLink="false">2509.11493v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Graph Attention Networks: Trainable Quantum Encoders for Inductive Graph Learning</title>
      <link>http://arxiv.org/abs/2509.11390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了量子图注意力网络（QGATs）作为图上进行归纳学习的可训练量子编码器，扩展了量子图神经网络框架。QGATs通过量子注意力机制动态调节邻居节点的贡献，实现了具有局部感知能力的量子表示。&lt;h4&gt;背景&lt;/h4&gt;量子图神经网络（QGNN）框架为图数据提供了量子编码方法，但在处理复杂图结构时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效编码节点特征和邻域结构的量子神经网络，提高在图数据上的归纳学习能力，特别是在化学性质预测任务中。&lt;h4&gt;方法&lt;/h4&gt;利用参数化量子电路编码节点特征和邻域结构，通过动态学习的酉算子实现量子注意力机制，调节每个邻居节点的贡献。在QM9数据集上评估该方法，预测各种化学性质，并与经典和量子图神经网络进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;注意力机制在经典和量子图神经网络中都能提高性能；量子注意力随着图大小的增长而带来更大的益处；在较大分子图上，QGATs显著优于无注意力的量子对应模型；对于较小图，QGATs实现了与经典GAT模型相当的预测精度。&lt;h4&gt;结论&lt;/h4&gt;量子注意力机制有潜力增强量子图神经网络在化学及其他领域的归纳能力，QGATs作为表达性量子编码器具有可行性和优势。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了量子图注意力网络（QGATs）作为图上进行归纳学习的可训练量子编码器，扩展了量子图神经网络（QGNN）框架。QGATs利用参数化量子电路编码节点特征和邻域结构，通过量子注意力机制经由动态学习的酉算子调节每个邻居的贡献。这使得能够表达具有局部感知能力的量子表示，并能推广到未见过的图实例。我们在QM9数据集上评估了我们的方法，目标是预测各种化学性质。我们的实验比较了经典和量子图神经网络（有和没有注意力层），表明注意力在两种范式中都能提高性能。值得注意的是，我们观察到量子注意力随着图大小的增长而带来更大的益处，在较大的分子图上，QGATs显著优于无注意力的量子对应模型。此外，对于较小的图，QGATs实现了与经典GAT模型相当的预测精度，突显了它们作为表达性量子编码器的可行性。这些结果表明量子注意力机制有潜力增强QGNN在化学及其他领域的归纳能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Quantum Graph Attention Networks (QGATs) as trainable quantumencoders for inductive learning on graphs, extending the Quantum Graph NeuralNetworks (QGNN) framework. QGATs leverage parameterized quantum circuits toencode node features and neighborhood structures, with quantum attentionmechanisms modulating the contribution of each neighbor via dynamically learnedunitaries. This allows for expressive, locality-aware quantum representationsthat can generalize across unseen graph instances. We evaluate our approach onthe QM9 dataset, targeting the prediction of various chemical properties. Ourexperiments compare classical and quantum graph neural networks-with andwithout attention layers-demonstrating that attention consistently improvesperformance in both paradigms. Notably, we observe that quantum attentionyields increasing benefits as graph size grows, with QGATs significantlyoutperforming their non-attentive quantum counterparts on larger moleculargraphs. Furthermore, for smaller graphs, QGATs achieve predictive accuracycomparable to classical GAT models, highlighting their viability as expressivequantum encoders. These results show the potential of quantum attentionmechanisms to enhance the inductive capacity of QGNN in chemistry and beyond.</description>
      <author>example@mail.com (Arthur M. Faria, Mehdi Djellabi, Igor O. Sokolov, Savvas Varsamopoulos)</author>
      <guid isPermaLink="false">2509.11390v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>BIGNet: Pretrained Graph Neural Network for Embedding Semantic, Spatial, and Topological Data in BIM Models</title>
      <link>http://arxiv.org/abs/2509.11104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了首个大规模图神经网络BIGNet，用于学习和重用BIM模型中的多维设计特征，解决了现有模型忽略BIM中语义、空间和拓扑特征的问题。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型在土木工程中显示出显著优势，但主要关注文本和视觉数据，忽略了BIM模型中丰富的语义、空间和拓扑特征。&lt;h4&gt;目的&lt;/h4&gt;开发第一个大规模图神经网络（GNN）BIGNet，以学习和重用BIM模型中嵌入的多维设计特征。&lt;h4&gt;方法&lt;/h4&gt;引入可扩展的图表示编码BIM组件的'语义-空间-拓扑'特征，创建包含近100万节点和350万条边的数据集；通过向GraphMAE2引入新消息传递机制提出BIGNet，并使用节点掩蔽策略进行预训练；在基于BIM的设计检查任务中评估BIGNet。&lt;h4&gt;主要发现&lt;/h4&gt;1) 同构图在学习设计特征方面优于异构图；2) 考虑30厘米半径内的局部空间关系可以提高性能；3) 基于GAT的特征提取的BIGNet实现了最佳迁移学习结果。&lt;h4&gt;结论&lt;/h4&gt;BIGNet使平均F1分数比未预训练模型提高72.7%，证明了其在学习和转移BIM设计特征方面的有效性，促进了这些特征在设计和生命周期管理中的自动化应用。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型在土木工程中显示出显著优势，但它们主要关注文本和视觉数据，忽略了BIM（建筑信息建模）模型中丰富的语义、空间和拓扑特征。因此，本研究开发了第一个大规模图神经网络（GNN）BIGNet，以学习和重用BIM模型中嵌入的多维设计特征。首先，引入了一种可扩展的图表示来编码BIM组件的'语义-空间-拓扑'特征，并创建了一个包含近100万个节点和350万条边的数据集。随后，通过向GraphMAE2引入新的消息传递机制提出了BIGNet，并使用节点掩蔽策略进行了进一步预训练。最后，在基于BIM的设计检查的各种迁移学习任务中评估了BIGNet。结果表明：1）同构图在学习设计特征方面优于异构图；2）考虑30厘米半径内的局部空间关系可以提高性能；3）基于GAT（图注意力网络）的特征提取的BIGNet实现了最佳的迁移学习结果。这一创新使平均F1分数比未预训练的模型提高了72.7%，证明了它在学习和转移BIM设计特征方面的有效性，并促进了它们在未来的设计和生命周期管理中的自动化应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有大型基础模型忽略BIM模型中丰富的语义、空间和拓扑特征的问题，以及BIM数据表示方法缺乏通用性和标注数据稀缺的问题。这个问题在现实中非常重要，因为BIM是建筑行业数字化的重要工具，包含大量设计知识和经验；有效提取和重用这些知识可以提高设计质量，减少施工错误，降低成本，并提升建筑全生命周期管理效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了BIM模型特点和现有方法局限性，指出BIM包含复杂几何数据、非几何信息和关系数据，而深度学习需要结构化输入。作者借鉴了图神经网络处理非欧几里得数据的能力，基于GraphMAE2架构进行改进，引入新的消息传递机制。同时利用预训练和迁移学习技术解决标注数据稀缺问题，并扩展了统一的基于网络的表示方法，以支持更全面的特征编码。整个设计过程体现了对现有工作的批判性继承和创新性发展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络表示和处理BIM模型中的语义、空间和拓扑特征，通过预训练学习隐式设计知识，并利用迁移学习应用于多种下游任务。整体流程分为三步：1) 开发BIM特定的可扩展图表示方法，将BIM模型转换为图结构，提取并编码组件的多维特征；2) 基于改进的GraphMAE2架构，使用节点掩码策略(50%掩码率)进行自监督预训练，引入新的消息传递机制；3) 在三种BIM设计检查任务(语义冲突、数据范围错误和拓扑错误)中评估模型，使用混淆矩阵和F1分数等指标衡量性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个大规模预训练图神经网络(BIGNet)用于学习BIM中的多维特征；2) BIM特定的可扩展图表示方法，统一编码语义、空间和拓扑特征；3) 改进的预训练策略，引入新的消息传递机制；4) 多任务设计检查框架。相比之前的工作，不同之处在于：现有方法主要针对特定任务设计特征提取，缺乏通用性；需要大量标注数据，而BIGNet通过预训练减少标注需求；通常只处理单一错误类型，而BIGNet支持多任务检查；图结构设计更灵活，可根据任务需求调整。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BIGNet通过创新的BIM图表示方法和预训练图神经网络，有效提取和重用了建筑信息模型中的语义、空间和拓扑设计知识，实现了多任务自动化设计检查，显著提高了建筑设计的质量和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Foundation Models (LFMs) have demonstrated significant advantages incivil engineering, but they primarily focus on textual and visual data,overlooking the rich semantic, spatial, and topological features in BIM(Building Information Modelling) models. Therefore, this study develops thefirst large-scale graph neural network (GNN), BIGNet, to learn, and reusemultidimensional design features embedded in BIM models. Firstly, a scalablegraph representation is introduced to encode the "semantic-spatial-topological"features of BIM components, and a dataset with nearly 1 million nodes and 3.5million edges is created. Subsequently, BIGNet is proposed by introducing a newmessage-passing mechanism to GraphMAE2 and further pretrained with a nodemasking strategy. Finally, BIGNet is evaluated in various transfer learningtasks for BIM-based design checking. Results show that: 1) homogeneous graphrepresentation outperforms heterogeneous graph in learning design features, 2)considering local spatial relationships in a 30 cm radius enhances performance,and 3) BIGNet with GAT (Graph Attention Network)-based feature extractionachieves the best transfer learning results. This innovation leads to a 72.7%improvement in Average F1-score over non-pretrained models, demonstrating itseffectiveness in learning and transferring BIM design features and facilitatingtheir automated application in future design and lifecycle management.</description>
      <author>example@mail.com (Jin Han, Xin-Zheng Lu, Jia-Rui Lin)</author>
      <guid isPermaLink="false">2509.11104v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>CogGNN: Cognitive Graph Neural Networks in Generative Connectomics</title>
      <link>http://arxiv.org/abs/2509.10864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CogGNN，第一个认知生成模型，使图神经网络具有认知能力，能够生成保留认知特征的脑网络，并在连接性脑模板学习任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;生成式学习已推进网络神经科学，支持图超分辨率、时间图预测和多模态脑图融合等任务，但当前基于图神经网络的方法仅关注结构和拓扑特性，忽略了认知特征。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够整合认知特征的生成模型，特别是视觉输入，以生成在认知和结构上都有意义的脑网络和连接性脑模板。&lt;h4&gt;方法&lt;/h4&gt;提出CogGNN，一种认知感知的生成模型，具有基于视觉记忆的损失函数，以及一种连接性脑模板学习框架，采用共同优化策略产生居中良好、可区分且认知增强的模板。&lt;h4&gt;主要发现&lt;/h4&gt;CogGNN生成的连接性脑模板在认知和结构上都有意义，且大量实验表明CogGNN优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;CogGNN为认知基础的脑网络建模奠定了坚实基础，是第一个将认知能力整合到图神经网络中的生成模型。&lt;h4&gt;翻译&lt;/h4&gt;生成式学习已推进网络神经科学，使图超分辨率、时间图预测和多模态脑图融合等任务成为可能。然而，当前主要基于图神经网络(GNNs)的方法仅关注结构和拓扑特性，忽略了认知特征。为此，我们引入了第一个认知生成模型CogGNN，它赋予GNN认知能力（如视觉记忆），以生成保留认知特征的脑网络。虽然具有广泛适用性，我们提出了CogGNN，这是一种特殊变体，旨在整合视觉输入，这是大脑功能（如图案识别和记忆回忆）的关键因素。作为概念验证，我们使用该模型学习连接性脑模板(CBTs)，这是来自多视图脑网络的群体级指纹。与之前忽略认知属性的工作不同，CogGNN生成的CBT在认知和结构上都有意义。我们的贡献是：(i) 一种新颖的认知感知生成模型，具有基于视觉记忆的损失函数；(ii) 一种CBT学习框架，采用共同优化策略，产生居中良好、可区分且认知增强的模板。大量实验表明，CogGNN优于最先进的方法，为认知基础的脑网络建模奠定了坚实基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative learning has advanced network neuroscience, enabling tasks likegraph super-resolution, temporal graph prediction, and multimodal brain graphfusion. However, current methods, mainly based on graph neural networks (GNNs),focus solely on structural and topological properties, neglecting cognitivetraits. To address this, we introduce the first cognified generative model,CogGNN, which endows GNNs with cognitive capabilities (e.g., visual memory) togenerate brain networks that preserve cognitive features. While broadlyapplicable, we present CogGNN, a specific variant designed to integrate visualinput, a key factor in brain functions like pattern recognition and memoryrecall. As a proof of concept, we use our model to learn connectional braintemplates (CBTs), population-level fingerprints from multi-view brain networks.Unlike prior work that overlooks cognitive properties, CogGNN generates CBTsthat are both cognitively and structurally meaningful. Our contributions are:(i) a novel cognition-aware generative model with a visual-memory-based loss;(ii) a CBT-learning framework with a co-optimization strategy to yieldwell-centered, discriminative, cognitively enhanced templates. Extensiveexperiments show that CogGNN outperforms state-of-the-art methods, establishinga strong foundation for cognitively grounded brain network modeling.</description>
      <author>example@mail.com (Mayssa Soussia, Yijun Lin, Mohamed Ali Mahjoub, Islem Rekik)</author>
      <guid isPermaLink="false">2509.10864v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Context--Aware Inputs: Physics--Inspired Improvements in Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.10684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了物理信息策略以提高图神经网络在液氩时间投影室事件重建任务中的性能，特别是针对代表性不足的粒子类别如米歇尔电子。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在液氩时间投影室的事件重建任务中显示出强大潜力，但对于米歇尔电子等代表性不足的粒子类别性能仍然有限。&lt;h4&gt;目的&lt;/h4&gt;研究基于物理信息的策略，以改进NuGraph2架构中的语义分割性能。&lt;h4&gt;方法&lt;/h4&gt;探索三种互补方法：(i)通过探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示；(ii)引入辅助解码器捕获类级别相关性；(iii)整合基于能量的正则化项，受米歇尔电子能量分布启发。&lt;h4&gt;主要发现&lt;/h4&gt;物理启发的特征增强带来最大收益，特别是通过解纠缠重叠潜在空间区域提高米歇尔电子精确率和召回率；辅助解码器和能量正则化项改进有限，部分原因是NuGraph2缺乏明确的粒子或事件级别表示。&lt;h4&gt;结论&lt;/h4&gt;将物理上下文直接嵌入到节点级输入中比施加特定任务的辅助损失更有效；具有明确粒子级和事件级推理的分层架构如NuGraph3将为高级解码器和基于物理的正则化提供更自然的环境。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近在液氩时间投影室的事件重建任务中显示出强大的前景，然而对于代表性不足的粒子类别，如米歇尔电子，其性能仍然有限。在这项工作中，我们研究基于物理信息的策略以改进NuGraph2架构中的语义分割。我们探索了三种互补方法：(i)通过探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示；(ii)引入辅助解码器以捕获类级别相关性；(iii)整合受米歇尔电子能量分布启发的基于能量的正则化项。在MicroBooNE公共数据集上的实验表明，物理启发的特征增强带来最大收益，特别是在通过解纠缠重叠的潜在空间区域显著提高米歇尔电子的精确率和召回率。相比之下，辅助解码器和能量正则化项提供的改进有限，部分原因是NuGraph2的命中级别性质，缺乏明确的粒子或事件级别表示。我们的发现强调将物理上下文直接嵌入到节点级输入中比施加特定任务的辅助损失更有效，并表明未来的分层架构如NuGraph3，具有明确的粒子级和事件级推理，将为高级解码器和基于物理的正则化提供更自然的环境。本工作的代码已在Github上公开：https://github.com/vitorgrizzi/nugraph_phys/tree/main_phys。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决图神经网络在液态氩时间投影室(LArTPC)粒子事件重建中对代表性不足的粒子类别(如Michel电子)分类性能不佳的问题。这个问题很重要，因为Michel电子等稀有粒子的准确识别对粒子物理实验至关重要，它们提供了关于粒子行为和相互作用的关键信息，而当前模型对这些稀有粒子的识别能力有限，影响了科学家对基本粒子行为的理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了图神经网络在粒子物理任务中的局限性，特别是对稀有类别识别不足的问题。他们借鉴了三种主要策略：通过输入特征注入物理知识、使用辅助解码器捕获类别间相关性、在损失函数中加入基于物理的约束项。作者参考了多项现有工作，如Drielsma等人的GrapPA方法引入几何描述符，Kiesler等人结合CNN与物理特征，以及DUNE协作的CVN多任务学习框架，这些工作启发了作者将物理学知识深度融入神经网络的不同方式。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在图神经网络的输入表示中直接嵌入物理上下文来增强模型表达能力，特别是改善对稀有粒子的识别，而不是仅依赖辅助损失函数来强制执行物理约束。整体实现包括三个主要步骤：1)特征扩展：在原始四个特征基础上添加节点度、最近邻距离和双差分特征，编码结构和关系上下文；2)添加额外解码器：引入预测事件中粒子类别分布的解码器，利用类别间相关性；3)Michel能量正则化：在损失函数中加入基于物理的约束项，惩罚偏离预期能量分布的分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)设计特定的物理启发特征，直接编码节点结构和关系上下文；2)使用全局注意力机制实现有效的图级表示；3)基于Michel电子能量分布特性设计特定正则化项。相比之前的工作，本研究强调在输入表示中嵌入物理上下文的重要性，而非仅通过损失函数强制执行物理约束；专注于语义分割而非粒子聚类；针对稀有类别识别而非多任务学习；将物理知识注入图神经网络的特定组件而非直接强制执行物理方程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过在图神经网络的输入表示中融入物理学启发的上下文感知特征，显著提高了对稀有粒子类别(如Michel电子)的语义分割性能，证明了直接在节点级嵌入物理知识比通过辅助损失函数强制执行物理约束更有效。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have recently shown strong promise for eventreconstruction tasks in Liquid Argon Time Projection Chambers, yet theirperformance remains limited for underrepresented classes of particles, such asMichel electrons. In this work, we investigate physics-informed strategies toimprove semantic segmentation within the NuGraph2 architecture. We explorethree complementary approaches: (i) enriching the input representation withcontext-aware features derived from detector geometry and track continuity,(ii) introducing auxiliary decoders to capture class-level correlations, and(iii) incorporating energy-based regularization terms motivated by Michelelectron energy distributions. Experiments on MicroBooNE public datasets showthat physics-inspired feature augmentation yields the largest gains,particularly boosting Michel electron precision and recall by disentanglingoverlapping latent space regions. In contrast, auxiliary decoders andenergy-regularization terms provided limited improvements, partly due to thehit-level nature of NuGraph2, which lacks explicit particle- or event-levelrepresentations. Our findings highlight that embedding physics context directlyinto node-level inputs is more effective than imposing task-specific auxiliarylosses, and suggest that future hierarchical architectures such as NuGraph3,with explicit particle- and event-level reasoning, will provide a more naturalsetting for advanced decoders and physics-based regularization. The code forthis work is publicly available on Github athttps://github.com/vitorgrizzi/nugraph_phys/tree/main_phys.</description>
      <author>example@mail.com (Vitor F. Grizzi, Margaret Voetberg, Giuseppe Cerati, Hadi Meidani)</author>
      <guid isPermaLink="false">2509.10684v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Explainability: Post-hoc Explanations for Geometric Neural Network Predictions</title>
      <link>http://arxiv.org/abs/2509.10676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了人工智能在科学应用中的可解释性问题，为现有的中微子标记图神经网络NuGraph2引入了解释性附加组件，展示了多种解释性技术如何结合使用以揭示AI模型的推理过程。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能在科学应用中的日益普及，将结果归因于网络的推理过程变得至关重要，这对于支持稳健的科学泛化具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;强调并展示事后解释性方法在科学应用中AI方法上的需求，并通过具体案例演示其应用价值。&lt;h4&gt;方法&lt;/h4&gt;为NuGraph2图神经网络引入一系列解释性技术，包括检查网络输出（节点分类）、分析节点间的边连接关系，以及使用新开发的通用工具探测潜在空间。这些方法不仅限于图神经网络，可应用于广泛的网络类型。&lt;h4&gt;主要发现&lt;/h4&gt;单独使用任何一种解释方法都不足以展示网络的'理解'能力，但将这些方法结合使用可以提供对分类过程中所使用方法的深入见解。&lt;h4&gt;结论&lt;/h4&gt;虽然这些解释性方法在NuGraph2应用上进行了测试，但它们具有广泛的适用性，可应用于各类神经网络。相关代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;随着人工智能在科学应用中日益普及，将结果归因于网络推理过程对于支持稳健的科学泛化来说需求很高。在本工作中，我们旨在强调并展示事后解释性方法在科学应用中AI方法上的应用需求。为此，我们为现有的用于中微子标记的图神经网络NuGraph2引入了解释性附加组件。这些解释采用一系列技术形式，包括检查网络输出（节点分类）和它们之间的边连接关系，以及使用应用于该网络的新开发通用工具探测潜在空间。我们展示了这些方法中的任何单独一种都不足以展示网络的'理解'能力，但结合起来可以提供对分类过程中所使用方法的见解。虽然这些方法在NuGraph2应用上进行了测试，但它们可以应用于广泛的网络，不仅限于图神经网络。本工作的代码已在GitHub上公开，网址为https://github.com/voetberg/XNuGraph。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing popularity of artificial intelligence used for scientificapplications, the ability of attribute a result to a reasoning process from thenetwork is in high demand for robust scientific generalizations to hold. Inthis work we aim to motivate the need for and demonstrate the use of post-hocexplainability methods when applied to AI methods used in scientificapplications. To this end, we introduce explainability add-ons to the existinggraph neural network (GNN) for neutrino tagging, NuGraph2. The explanationstake the form of a suite of techniques examining the output of the network(node classifications) and the edge connections between them, and probing ofthe latent space using novel general-purpose tools applied to this network. Weshow how none of these methods are singularly sufficient to show network"understanding", but together can give insights into the processes used inclassification. While these methods are tested on the NuGraph2 application,they can be applied to a broad range of networks, not limited to GNNs. The codefor this work is publicly available on GitHub athttps://github.com/voetberg/XNuGraph.</description>
      <author>example@mail.com (Margaret Voetberg, Vitor F. Grizzi, Giuseppe Cerati, Hadi Meidani)</author>
      <guid isPermaLink="false">2509.10676v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations</title>
      <link>http://arxiv.org/abs/2509.10659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and published in Transactions on Machine Learning Research  (TMLR), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M4GN的三层段中心分层网络，用于解决基于网格的图神经网络在PDE模拟中面临的高成本、过度平滑以及构建粗图和保持细粒度精度的问题。&lt;h4&gt;背景&lt;/h4&gt;基于网格的图神经网络已成为PDE模拟的有效替代方法，但深度消息传递在大规模、长距离网格上成本高且存在过度平滑问题；分层GNN虽缩短了传播路径，但仍面临构建尊重网格拓扑、几何和物理不连续性的粗图，以及保持细粒度精度而不牺牲粗化带来的速度优势两大障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够平衡精度和效率的图神经网络架构，解决现有方法在处理大规模网格时的高计算成本和过度平滑问题，同时保持细粒度精度。&lt;h4&gt;方法&lt;/h4&gt;提出M4GN，采用混合分割策略结合快速图分割器和超像素式细化，产生动态一致节点的连续段；使用排列不变聚合器编码这些段，避免顺序敏感性和二次成本；通过微观级GNN捕获局部动力学，宏观级transformer跨段高效推理，实现精度与效率的平衡。&lt;h4&gt;主要发现&lt;/h4&gt;在多个代表性基准数据集上评估，M4GN将预测准确性提高了高达56%，同时比最先进的基线快达22%。&lt;h4&gt;结论&lt;/h4&gt;M4GN通过创新的分层架构和混合分割策略，有效解决了现有图神经网络在PDE模拟中的关键挑战，实现了精度和效率的显著提升。&lt;h4&gt;翻译&lt;/h4&gt;基于网格的图神经网络已成为PDE模拟的有效替代方法，但其深度消息传递在大规模、长距离网格上成本高且过度平滑；分层GNN缩短了传播路径但仍面临两个关键障碍：(i)构建尊重网格拓扑、几何和物理不连续性的粗图，以及(ii)保持细粒度精度而不牺牲粗化带来的速度优势。我们通过M4GN应对这些挑战，这是一种三层、段中心的分层网络。M4GN采用混合分割策略，将快速图分割器与由模态分解特征引导的超像素式细化配对，产生动态一致节点的连续段。这些段通过排列不变聚合器进行编码，避免了先前工作中使用的聚合方法的顺序敏感性和二次成本。得到的信息连接了捕获局部动力学的微观级GNN和跨段高效推理的宏观级transformer，实现了精度和效率之间的原则性平衡。在多个代表性基准数据集上评估，M4GN将预测准确性提高了高达56%，同时比最先进的基线快达22%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mesh-based graph neural networks (GNNs) have become effective surrogates forPDE simulations, yet their deep message passing incurs high cost andover-smoothing on large, long-range meshes; hierarchical GNNs shortenpropagation paths but still face two key obstacles: (i) building coarse graphsthat respect mesh topology, geometry, and physical discontinuities, and (ii)maintaining fine-scale accuracy without sacrificing the speed gained fromcoarsening. We tackle these challenges with M4GN, a three-tier, segment-centrichierarchical network. M4GN begins with a hybrid segmentation strategy thatpairs a fast graph partitioner with a superpixel-style refinement guided bymodal-decomposition features, producing contiguous segments of dynamicallyconsistent nodes. These segments are encoded by a permutation-invariantaggregator, avoiding the order sensitivity and quadratic cost of aggregationapproaches used in prior works. The resulting information bridges a micro-levelGNN, which captures local dynamics, and a macro-level transformer that reasonsefficiently across segments, achieving a principled balance between accuracyand efficiency. Evaluated on multiple representative benchmark datasets, M4GNimproves prediction accuracy by up to 56% while achieving up to 22% fasterinference than state-of-the-art baselines.</description>
      <author>example@mail.com (Bo Lei, Victor M. Castillo, Yeping Hu)</author>
      <guid isPermaLink="false">2509.10659v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Hetero-EUCLID: Interpretable model discovery for heterogeneous hyperelastic materials using stress-unsupervised learning</title>
      <link>http://arxiv.org/abs/2509.11784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Hetero-EUCLID的计算框架，用于异质材料的分割和参数识别，以表征所有组分的完整超弹性行为。&lt;h4&gt;背景&lt;/h4&gt;异质材料的完整力学行为表征是一个挑战性问题，需要能够识别不同材料组分及其力学特性的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够分割异质材料域并识别各组分材料本构参数的计算框架，以实现完整的超弹性行为表征。&lt;h4&gt;方法&lt;/h4&gt;基于Bayesian-EUCLID框架，使用稀疏促进先验和马尔可夫链蒙特卡罗采样解决异构化公式。框架包括两个主要步骤：基于残余力的分割和本构参数识别。使用有限元模拟生成的三维表面位移和边界平均力数据作为输入。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够成功分割各种类型的薄方形异质域，并表征不同组分的材料特性。即使在存在位移噪声和非原生网格离散化的情况下，框架也能保持有效。基于单次实验数据即可完成分割和材料表征。&lt;h4&gt;结论&lt;/h4&gt;Hetero-EUCLID框架在基于数字图像/体积相关的实验场景中具有适用性，为航空航天和国防复合材料等领域的快速、可解释的模型发现提供了工具，同时也适用于医疗领域如纤维动脉粥样硬化、动脉粥样硬化或癌症中的选择性组织硬化表征。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种计算框架Hetero-EUCLID，用于分割和参数识别，以表征异质材料所有组分的完整超弹性行为。在这项工作中，我们利用Bayesian-EUCLID（高效无监督本构定律识别和发现）框架，通过使用稀疏促进先验和马尔可夫链蒙特卡罗采样进行简约模型选择，有效地解决了异构化公式。我们使用了从异质试样非等双轴拉伸试验的有限元模拟中生成的实验可观测的三维表面位移和边界平均力数据。该框架 broadly包括两个步骤——基于残余力的分割和本构参数识别。我们验证并展示了所提出框架分割域和表征组分材料在各种类型薄方形异质域上的能力。我们验证了该框架在不同级别位移噪声和非原生网格离散化情况下的分割和表征材料的能力，即使用不同的网格进行前向FE模拟和逆EUCLID问题。这证明了Hetero-EUCLID框架在基于数字图像/体积相关的实验场景中的适用性。此外，所提出的框架基于单次实验数据成功进行了分割和材料表征，使其在航空航天和国防复合材料等领域的快速、可解释的模型发现，以及在纤维动脉粥样硬化、动脉粥样硬化或癌症等医疗状况中选择性组织硬化表征方面具有可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a computational framework, Hetero-EUCLID, for segmentation andparameter identification to characterize the full hyperelastic behavior of allconstituents of a heterogeneous material. In this work, we leverage theBayesian-EUCLID (Efficient Unsupervised Constitutive Law Identification andDiscovery) framework to efficiently solve the heterogenized formulation throughparsimonious model selection using sparsity-promoting priors and Monte CarloMarkov Chain sampling. We utilize experimentally observable 3D surfacedisplacement and boundary-averaged force data generated from Finite Elementsimulations of non-equi-biaxial tension tests on heterogeneous specimens. Theframework broadly consists of two steps -- residual force-based segmentation,and constitutive parameter identification. We validate and demonstrate theability of the proposed framework to segment the domain, and characterize theconstituent materials on various types of thin square heterogeneous domains. Wevalidate of the framework's ability to segment and characterize materials withvarious levels of displacement noises and non-native mesh discretizations, i.e,using different meshes for the forward FE simulations and the inverse EUCLIDproblem. This demonstrates Hetero-EUCLID framework's applicability in DigitalImage/Volume Correlation-based experimental scenarios. Furthermore, theproposed framework performs successful segmentation and materialcharacterizations based on data from a single experiment, thereby making itviable for rapid, interpretable model discovery in domains such as aerospaceand defense composites and for characterization of selective tissue stiffeningin medical conditions such as fibroatheroma, atherosclerosis, or cancer.</description>
      <author>example@mail.com (Kanhaiya Lal Chaurasiya, Saurav Dutta, Siddhant Kumar, Akshay Joshi)</author>
      <guid isPermaLink="false">2509.11784v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>The Quest for Universal Master Key Filters in DS-CNNs</title>
      <link>http://arxiv.org/abs/2509.11711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文扩展了卷积神经网络的'主键过滤器假设'，发现深度可分离卷积网络固有收敛到一组仅8个通用过滤器，这些过滤器在图像处理中具有基础性作用。&lt;h4&gt;背景&lt;/h4&gt;最近的研究提出了卷积神经网络过滤器的'主键过滤器假设'，本文进一步扩展了这一假设。&lt;h4&gt;目的&lt;/h4&gt;将假设范围严格限制到一组仅8个通用过滤器，探究深度可分离卷积网络(DS-CNN)的固有收敛特性。&lt;h4&gt;方法&lt;/h4&gt;通过系统性的无监督搜索，从不同架构和数据集中提取这些基本模式。&lt;h4&gt;主要发现&lt;/h4&gt;传统DS-CNN使用的数千个过滤器主要是这8个通用集合的线性移位；用这8个冻结过滤器初始化的网络在ImageNet上达到超过80%的准确率；这些过滤器与高斯差、高斯及其导数相似，与哺乳动物视觉系统感受野相似；深度卷积层自然倾向于这组基本空间算子。&lt;h4&gt;结论&lt;/h4&gt;深度卷积层自然地倾向于这组基本的空间算子，无论任务或架构如何。这些主键过滤器为理解泛化和迁移学习提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究提出了卷积神经网络过滤器的'主键过滤器假设'。本文通过将假设范围严格限制到一组仅8个通用过滤器来扩展这一假设，这些是深度可分离卷积网络固有收敛到的。虽然传统DS-CNN使用数千个不同的训练过滤器，但我们的分析显示这些过滤器主要是我们发现的通用集合的线性移位。通过系统性的无监督搜索，我们从不同架构和数据集中提取了这些基本模式。值得注意的是，用这8个独特的冻结过滤器初始化的网络在ImageNet上获得超过80%的准确率，甚至在应用于较小数据集时优于具有数千个可训练参数的模型。识别出的主键过滤器与高斯差、高斯及其导数非常匹配，这些结构不仅是经典图像处理的基础，而且与哺乳动物视觉系统中的感受野惊人地相似。我们的发现提供了有力证据，表明深度卷积层自然地倾向于这组基本的空间算子，无论任务或架构如何。这项工作通过这些主键过滤器的通用语言，为理解泛化和迁移学习提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A recent study has proposed the "Master Key Filters Hypothesis" forconvolutional neural network filters. This paper extends this hypothesis byradically constraining its scope to a single set of just 8 universal filtersthat depthwise separable convolutional networks inherently converge to. Whileconventional DS-CNNs employ thousands of distinct trained filters, our analysisreveals these filters are predominantly linear shifts (ax+b) of our discovereduniversal set. Through systematic unsupervised search, we extracted thesefundamental patterns across different architectures and datasets. Remarkably,networks initialized with these 8 unique frozen filters achieve over 80%ImageNet accuracy, and even outperform models with thousands of trainableparameters when applied to smaller datasets. The identified master key filtersclosely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,structures that are not only fundamental to classical image processing but alsostrikingly similar to receptive fields in mammalian visual systems. Ourfindings provide compelling evidence that depthwise convolutional layersnaturally gravitate toward this fundamental set of spatial operators regardlessof task or architecture. This work offers new insights for understandinggeneralization and transfer learning through the universal language of thesemaster key filters.</description>
      <author>example@mail.com (Zahra Babaiee, Peyman M. Kiassari, Daniela Rus, Radu Grosu)</author>
      <guid isPermaLink="false">2509.11711v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>An Unsupervised Learning Approach For A Reliable Profiling Of Cyber Threat Actors Reported Globally Based On Complete Contextual Information Of Cyber Attacks</title>
      <link>http://arxiv.org/abs/2509.11683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于无监督的高效凝聚层次聚类技术，用于对网络犯罪团伙进行全面画像，解决了现有监督机器学习方法在攻击者画像方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;随着技术进步，网络攻击迅速增加，而信息保护不足。现有监督机器学习方法只考虑了文本威胁报告中的少量特征，且依赖结构化数据集，需要先建立数据集再进行分析，效率低下。&lt;h4&gt;目的&lt;/h4&gt;识别网络威胁行为者之间的共同特征关系，对其进行聚合，并对网络犯罪团伙进行画像，以创建更有效的防御机制。&lt;h4&gt;方法&lt;/h4&gt;采用无监督的高效凝聚层次聚类技术，基于全面的上下文威胁信息对网络犯罪团伙进行画像，避免了对结构化数据集的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;通过基于网络威胁行为者的共同特征识别关系并聚合，可以更有效地创建防御机制，提高威胁分析的效率。&lt;h4&gt;结论&lt;/h4&gt;基于全面上下文信息的无监督方法可以更有效地识别网络威胁行为者之间的关系，为提前创建防御机制提供支持。&lt;h4&gt;翻译&lt;/h4&gt;网络攻击随着技术进步而迅速增加，我们的信息没有保护。为防止未来的网络攻击，及时识别网络攻击并建立强大的防御机制至关重要。为立即应对网络安全威胁，必须检查攻击者的技能、知识和行为，目的是评估他们对系统的影响并理解与这些攻击相关的特征。基于网络威胁行为者的特征或行为模式创建画像，可以帮助提前创建有效的防御机制。在现有文献中，多种基于监督机器学习的方法只考虑了文本网络威胁事件报告中报告的少量特征用于攻击者画像，尽管这些画像是基于安全专家自身的认知开发的，但我们不能依赖它们。监督机器学习方法严格依赖于结构化数据集。这通常导致一个两步过程，即我们首先必须建立结构化数据集，然后才能分析它并用于构建防御机制，这需要时间。在本文中，提出了一种无监督的高效凝聚层次聚类技术，用于基于全面的上下文威胁信息对网络犯罪团伙进行画像，以解决上述问题。本报告的主要目标是识别基于共同特征的网络威胁行为者之间的关系，对其进行聚合，并对网络犯罪团伙进行画像。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cyber attacks are rapidly increasing with the advancement of technology andthere is no protection for our information. To prevent future cyberattacks itis critical to promptly recognize cyberattacks and establish strong defensemechanisms against them. To respond to cybersecurity threats immediately, it isessential to examine the attackers skills, knowledge, and behaviors with thegoal of evaluating their impact on the system and comprehending the traitsassociated with these attacks. Creating a profile of cyber threat actors basedon their traits or patterns of behavior can help to create effective defensesagainst cyberattacks in advance. In the current literature, multiple supervisedmachine learning based approaches considered a smaller number of features forattacker profiling that are reported in textual cyber threat incident documentsalthough these profiles have been developed based on the security experts ownperception, we cannot rely on them. Supervised machine learning approachesstrictly depend upon the structure data set. This usually leads to a two stepprocess where we first have to establish a structured data set before we cananalyze it and then employ it to construct defense mechanisms, which takestime. In this paper, an unsupervised efficient agglomerative hierarchalclustering technique is proposed for profiling cybercriminal groups based ontheir comprehensive contextual threat information in order to address theaforementioned issues. The main objective of this report is to identify therelationship between cyber threat actors based on their common features,aggregate them, and also profile cyber criminal groups.</description>
      <author>example@mail.com (Sawera Shahid, Umara Noor, Zahid Rashid)</author>
      <guid isPermaLink="false">2509.11683v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Stacked Intelligent Metasurface for End-to-End OFDM System</title>
      <link>http://arxiv.org/abs/2509.11551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于SIM（DPSIM）辅助的端到端OFDM系统，将传统通信任务在电磁前向传播中同时完成，并引入电磁神经网络（EMNN）进行系统控制，通过迁移学习进行模型训练，实现了在复杂信道条件下的稳健比特流传输。&lt;h4&gt;背景&lt;/h4&gt;堆叠智能超表面（SIM）和双极化SIM（DPSIM）的波域信号处理是卸载基带数字处理和简化收发器设计的有前途的研究方向，但现有架构仅限于将SIM（DPSIM）用于单一通信功能。&lt;h4&gt;目的&lt;/h4&gt;提高SIM（DPSIM）辅助系统的整体性能，实现从发送比特流到接收比特流的端到端（E2E）联合优化。&lt;h4&gt;方法&lt;/h4&gt;提出SIM（DPSIM）辅助的端到端OFDM系统，在电磁前向传播过程中同时完成调制、预编码、合并和解调任务；受神经网络启发提出电磁神经网络（EMNN）控制系统；引入迁移学习进行模型训练；设计EMNN的训练和部署框架。&lt;h4&gt;主要发现&lt;/h4&gt;SIM辅助和DPSIM辅助的端到端OFDM系统都能在复杂信道条件下实现稳健的比特流传输。&lt;h4&gt;结论&lt;/h4&gt;EMNN和SIM（DPSIM）辅助的端到端OFDM系统在下一代收发器设计中具有应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;堆叠智能超表面（SIM）和双极化SIM（DPSIM）的波域信号处理已成为卸载基带数字处理任务和高效简化收发器设计的研究方向。然而，现有架构仅限于将SIM（DPSIM）用于单一通信功能，如预编码或合并。为了进一步提高SIM（DPSIM）辅助系统的整体性能，并实现从发送比特流到接收比特流的端到端（E2E）联合优化，我们提出了一种SIM（DPSIM）辅助的端到正交频分复用（OFDM）系统，其中传统的通信任务如调制、预编码、合并和解调在电磁（EM）前向传播过程中同时完成。此外，受将真实超表面抽象为神经网络隐藏层的启发，我们提出了电磁神经网络（EMNN）来控制端到端OFDM通信系统。另外，将迁移学习引入模型训练，并设计了EMNN的训练和部署框架。仿真结果表明，SIM辅助的端到端OFDM系统和DPSIM辅助的端到端OFDM系统都能在复杂信道条件下实现稳健的比特流传输。我们的研究突显了EMNN和SIM（DPSIM）辅助的端到端OFDM系统在下一代收发器设计中的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stacked intelligent metasurface (SIM) and dual-polarized SIM (DPSIM) enabledwave-domain signal processing have emerged as promising research directions foroffloading baseband digital processing tasks and efficiently simplifyingtransceiver design. However, existing architectures are limited to employingSIM (DPSIM) for a single communication function, such as precoding orcombining. To further enhance the overall performance of SIM (DPSIM)-assistedsystems and achieve end-to-end (E2E) joint optimization from the transmittedbitstream to the received bitstream, we propose an SIM (DPSIM)- assisted E2Eorthogonal frequency-division multiplexing (OFDM) system, where traditionalcommunication tasks such as modulation, precoding, combining, and demodulationare performed simultaneously during electromagnetic (EM) forward propagation.Furthermore, inspired by the idea of abstracting real metasurfaces as hiddenlayers of a neural network, we propose the electromagnetic neural network(EMNN) to enable the control of the E2E OFDM communication system. In addition,transfer learning is introduced into the model training, and a training anddeployment framework for the EMNN is designed. Simulation results demonstratethat both SIM-assisted E2E OFDM systems and DPSIM-assisted E2E OFDM systems canachieve robust bitstream transmission under complex channel conditions. Ourstudy highlights the application potential of EMNN and SIM (DPSIM)-assisted E2EOFDM systems in the design of next-generation transceivers.</description>
      <author>example@mail.com (Yida Zhang, Qiuyan Liu, Hongtao Luo, Yuqi Xia, Qiang Wang)</author>
      <guid isPermaLink="false">2509.11551v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>MAUI: Reconstructing Private Client Data in Federated Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.11451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MAUI，一种隐蔽的数据重建攻击方法，在联邦学习中仅利用分类头梯度即可重建原始输入数据，显著提高了重建质量。&lt;h4&gt;背景&lt;/h4&gt;联邦学习中，服务器首先在公共数据集上预训练全局模型，然后在客户端微调模型的最后几个线性层（分类头）。现有数据重建攻击存在两个弱点：初始模型层的梯度信息不共享，以及服务器对模型的手工操作易被检测。&lt;h4&gt;目的&lt;/h4&gt;开发一种隐蔽的数据重建攻击方法，不需要对模型架构或权重进行明显操作，仅依靠分类头梯度实现数据重建。&lt;h4&gt;方法&lt;/h4&gt;MAUI从分类头的梯度中提取输入批次的'鲁棒'特征表示，然后将这些特征表示反转回原始输入数据。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR10和ImageNet数据集上实现了高度准确的重建，适用于多种模型架构（CNN、VGG11、ResNets、ShuffleNet-V2和ViT B-32），无论批量大小如何都能工作，重建质量比先前方法高40-120%的PSNR分数。&lt;h4&gt;结论&lt;/h4&gt;MAUI是一种隐蔽高效的数据重建攻击方法，仅利用分类头梯度即可实现高质量的数据重建，显著优于现有攻击方法。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)的最新研究表明，利用迁移学习可以平衡联邦学习和集中式学习的好处。在这种设置中，联邦训练在通过传统训练达到稳定点后进行。全局模型权重首先由服务器在公共数据集上进行集中预训练，之后只有模型的最后几个线性层（分类头）在客户端间进行微调。在这种情况下，现有的联邦学习数据重建攻击(DRAs)表现出两个关键弱点：首先，与输入强相关的初始模型层梯度信息从不共享，显著降低了重建准确性；其次，服务器对模型结构或参数进行高度特定、手工操作的攻击方法（如全零权重层、恒等映射和具有相同权重模式的行）容易被活跃客户端检测。针对这些弱点，我们提出了MAUI，一种隐蔽的数据重建攻击，不需要对模型架构或权重进行任何明显操作，仅依赖于分类头的梯度。MAUI首先从分类头的梯度中提取输入批次的'鲁棒'特征表示，然后将这些表示反转回原始输入。我们在CIFAR10和ImageNet数据集上报告了高度准确的重建，适用于多种模型架构，包括卷积网络(CNN, VGG11)、ResNets(18, 50)、ShuffleNet-V2和Vision Transformer(ViT B-32)，无论批量大小如何。MAUI在重建质量上显著优于先前的DRAs，实现了40-120%更高的PSNR分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent works in federated learning (FL) have shown the utility of leveragingtransfer learning for balancing the benefits of FL and centralized learning. Inthis setting, federated training happens after a stable point has been reachedthrough conventional training. Global model weights are first centrallypretrained by the server on a public dataset following which only the last fewlinear layers (the classification head) of the model are finetuned acrossclients. In this scenario, existing data reconstruction attacks (DRAs) in FLshow two key weaknesses. First, strongly input-correlated gradient informationfrom the initial model layers is never shared, significantly degradingreconstruction accuracy. Second, DRAs in which the server makes highlyspecific, handcrafted manipulations to the model structure or parameters (fore.g., layers with all zero weights, identity mappings and rows with identicalweight patterns) are easily detectable by an active client.  Improving on these, we propose MAUI, a stealthy DRA that does not require anyovert manipulations to the model architecture or weights, and relies solely onthe gradients of the classification head. MAUI first extracts "robust" featurerepresentations of the input batch from the gradients of the classificationhead and subsequently inverts these representations to the original inputs. Wereport highly accurate reconstructions on the CIFAR10 and ImageNet datasets ona variety of model architectures including convolution networks (CNN, VGG11),ResNets (18, 50), ShuffleNet-V2 and Vision Transformer (ViT B-32), regardlessof the batch size. MAUI significantly outperforms prior DRAs in reconstructionquality, achieving 40-120% higher PSNR scores.</description>
      <author>example@mail.com (Ahaan Dabholkar, Atul Sharma, Z. Berkay Celik, Saurabh Bagchi)</author>
      <guid isPermaLink="false">2509.11451v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Dataset Evaluation Based on Generalized Cross Validation</title>
      <link>http://arxiv.org/abs/2509.11273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IST 2025. Official IEEE Xplore entry will  be available once published&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合广义交叉验证和领域迁移学习原则的合成数据集质量评估框架，通过构建交叉性能矩阵和GCV矩阵来量化领域可迁移性，并引入两个关键指标分别评估模拟质量和迁移质量，实验证明该框架能有效评估合成数据保真度。&lt;h4&gt;背景&lt;/h4&gt;随着合成数据集生成技术的快速发展，评估合成数据质量已成为关键研究焦点。当前评估研究有限，缺乏普遍接受的标准框架。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的评估框架，对合成数据集质量进行可推广和可比较的评估，指导合成数据集优化。&lt;h4&gt;方法&lt;/h4&gt;在合成数据集和真实世界基准数据集上训练特定任务模型，形成交叉性能矩阵；构建广义交叉验证矩阵量化领域可迁移性；引入两个关键指标：一个衡量合成数据与真实数据的相似性，一个评估合成数据在不同真实世界场景中的多样性和覆盖范围。&lt;h4&gt;主要发现&lt;/h4&gt;在Virtual KITTI上的实验验证了所提框架和指标在评估合成数据保真度方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;该可扩展和可量化的评估解决方案克服了传统局限性，为人工智能研究中合成数据集的优化提供了有原则的方法。&lt;h4&gt;翻译&lt;/h4&gt;随着合成数据集生成技术的快速发展，评估合成数据的质量已成为关键研究焦点。稳健的评估不仅推动数据生成方法的创新，还能指导研究人员优化这些合成资源的利用。然而，当前对合成数据集的评估研究仍然有限，缺乏普遍接受的标准框架。为解决这一问题，本文提出了一种结合广义交叉验证实验和领域迁移学习原则的新评估框架，能够对合成数据集质量进行可推广和可比较的评估。该框架涉及在合成数据集和多个真实世界基准（如KITTI、BDD100K）上训练特定任务模型（如YOLOv5s），形成一个交叉性能矩阵。经过归一化后，构建广义交叉验证（GCV）矩阵来量化领域可迁移性。该框架引入两个关键指标：一个通过量化合成数据与真实数据集之间的相似性来衡量模拟质量，另一个通过评估合成数据在各种真实世界场景中的多样性和覆盖范围来评估迁移质量。在Virtual KITTI上的实验验证了所提框架和指标在评估合成数据保真度方面的有效性。这种可扩展和可量化的评估解决方案克服了传统局限性，为人工智能研究中合成数据集的优化提供了有原则的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of synthetic dataset generation techniques,evaluating the quality of synthetic data has become a critical research focus.Robust evaluation not only drives innovations in data generation methods butalso guides researchers in optimizing the utilization of these syntheticresources. However, current evaluation studies for synthetic datasets remainlimited, lacking a universally accepted standard framework. To address this,this paper proposes a novel evaluation framework integrating generalizedcross-validation experiments and domain transfer learning principles, enablinggeneralizable and comparable assessments of synthetic dataset quality. Theframework involves training task-specific models (e.g., YOLOv5s) on bothsynthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K),forming a cross-performance matrix. Following normalization, a GeneralizedCross-Validation (GCV) Matrix is constructed to quantify domaintransferability. The framework introduces two key metrics. One measures thesimulation quality by quantifying the similarity between synthetic data andreal-world datasets, while another evaluates the transfer quality by assessingthe diversity and coverage of synthetic data across various real-worldscenarios. Experimental validation on Virtual KITTI demonstrates theeffectiveness of our proposed framework and metrics in assessing synthetic datafidelity. This scalable and quantifiable evaluation solution overcomestraditional limitations, providing a principled approach to guide syntheticdataset optimization in artificial intelligence research.</description>
      <author>example@mail.com (Zhihang Song, Dingyi Yao, Ruibo Ming, Lihui Peng, Danya Yao, Yi Zhang)</author>
      <guid isPermaLink="false">2509.11273v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches</title>
      <link>http://arxiv.org/abs/2509.11241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了两种深度学习模型在卡纳提克音乐节拍跟踪任务上的性能，并探索了通过迁移学习和音乐信息参数调整来提高模型适应性的方法。&lt;h4&gt;背景&lt;/h4&gt;节拍和强拍跟踪是音乐信息检索的基础任务。深度学习模型在西方音乐流派中表现优异，但在代表性不足的音乐传统上表现不佳。卡纳提克音乐以其节奏复杂性和独特节拍结构著称，先前研究主要采用概率动态贝叶斯网络(DBN)方法。&lt;h4&gt;目的&lt;/h4&gt;评估两种深度学习模型在卡纳提克音乐节拍跟踪任务上的性能，研究适应策略，包括数据微调和音乐信息参数调整，探索最先进深度学习模型在非西方音乐传统中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;在卡纳提克音乐节奏(CMR_f)数据集上评估时间卷积网络(TCN)和基于Transformer的Beat This!模型，复制DBN基线实验设置，并通过迁移学习和音乐信息参数调整来优化模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;现成的深度学习模型并不总是优于DBN基线，但通过迁移学习，模型性能显著提高，能够匹配或超过基线性能，表明深度学习模型可以有效地适应卡纳提克音乐等非西方音乐传统。&lt;h4&gt;结论&lt;/h4&gt;最先进的深度学习模型可以有效地适应代表性不足的音乐传统，为开发更具包容性和广泛适用性的节拍跟踪系统开辟了道路。&lt;h4&gt;翻译&lt;/h4&gt;节拍和强拍跟踪，统称为节拍跟踪，是音乐信息检索(MIR)中的基础任务。深度学习模型在这一领域已经远远超越了传统的信号处理和经典机器学习方法，特别是在西方(欧洲起源)音乐流派中，因为有大量标注数据集可用。然而，这些系统在代表性不足的音乐传统上表现不太可靠。卡纳提克音乐是来自印度次大陆的丰富传统，以其节奏复杂性和独特的节拍结构(talas)而闻名。在此背景下，关于节拍跟踪的最著名先前工作采用了概率动态贝叶斯网络(DBNs)。然而，最先进的深度学习模型在卡纳提克音乐上的表现仍然 largely未被探索。在本研究中，我们评估了两种用于卡纳提克音乐节拍跟踪的模型：时间卷积网络(TCN)，这是一种已成功适应拉丁节奏的轻量级架构，以及Beat This!，一种不需要后处理的基于Transformer的模型，旨在广泛风格覆盖。我们在卡纳提克音乐节奏(CMR_f)数据集上复制DBN基线的实验设置，在直接可比的环境中系统地评估这些模型的性能。我们进一步研究了适应策略，包括在卡纳提克数据上微调模型和使用音乐信息参数。结果表明，虽然现成的模型并不总是优于DBN，但通过迁移学习，它们的性能显著提高，匹配或超过了基线。这些发现表明，最先进的深度学习模型可以有效地适应代表性不足的传统，为更具包容性和更广泛适用的节拍跟踪系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Beat and downbeat tracking, jointly referred to as Meter Tracking, is afundamental task in Music Information Retrieval (MIR). Deep learning modelshave far surpassed traditional signal processing and classical machine learningapproaches in this domain, particularly for Western (Eurogenetic) genres, wherelarge annotated datasets are widely available. These systems, however, performless reliably on underrepresented musical traditions. Carnatic music, a richtradition from the Indian subcontinent, is renowned for its rhythmic intricacyand unique metrical structures (t\=alas). The most notable prior work on metertracking in this context employed probabilistic Dynamic Bayesian Networks(DBNs). The performance of state-of-the-art (SOTA) deep learning models onCarnatic music, however, remains largely unexplored.  In this study, we evaluate two models for meter tracking in Carnatic music:the Temporal Convolutional Network (TCN), a lightweight architecture that hasbeen successfully adapted for Latin rhythms, and Beat This!, atransformer-based model designed for broad stylistic coverage without the needfor post-processing. Replicating the experimental setup of the DBN baseline onthe Carnatic Music Rhythm (CMR$_f$) dataset, we systematically assess theperformance of these models in a directly comparable setting. We furtherinvestigate adaptation strategies, including fine-tuning the models on Carnaticdata and the use of musically informed parameters. Results show that whileoff-the-shelf models do not always outperform the DBN, their performanceimproves substantially with transfer learning, matching or surpassing thebaseline. These findings indicate that SOTA deep learning models can beeffectively adapted to underrepresented traditions, paving the way for moreinclusive and broadly applicable meter tracking systems.</description>
      <author>example@mail.com (Satyajeet Prabhu)</author>
      <guid isPermaLink="false">2509.11241v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Feature Space Topology Control via Hopkins Loss</title>
      <link>http://arxiv.org/abs/2509.11154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Proc. IEEE ICTAI 2025, Athens, Greece&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为'Hopkins loss'的新型损失函数，它利用Hopkins统计量强制实现所需的特征空间拓扑结构，与现有方法不同，现有方法旨在保留输入特征拓扑。研究者在语音、文本和图像数据上评估了该方法在分类和降维场景下的有效性，结果表明该方法对分类性能影响较小，同时能修改特征拓扑。&lt;h4&gt;背景&lt;/h4&gt;特征空间拓扑指的是特征空间中样本的组织方式。修改这种拓扑在机器学习应用中是有益的，包括降维、生成建模、迁移学习和对抗攻击的鲁棒性。现有的拓扑相关方法主要关注保留输入特征拓扑。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的损失函数(Hopkins loss)，能够强制实现所需的特征空间拓扑，从而在保持分类性能的同时修改特征拓扑结构。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为'Hopkins loss'的新型损失函数，它利用Hopkins统计量来强制实现所需的特征空间拓扑。研究者在语音、文本和图像数据上评估了该方法，在两个场景中进行测试：分类和非线性瓶颈自编码器的降维。&lt;h4&gt;主要发现&lt;/h4&gt;将Hopkins loss集成到分类或降维任务中，对分类性能只有很小的影响，同时能够提供修改特征拓扑的好处。&lt;h4&gt;结论&lt;/h4&gt;Hopkins loss是一种有效的工具，可以在保持模型性能的同时，修改特征空间的拓扑结构，这有助于提高机器学习应用中的多种任务表现。&lt;h4&gt;翻译&lt;/h4&gt;特征空间拓扑指的是特征空间中样本的组织方式。修改这种拓扑在机器学习应用中是有益的，包括降维、生成建模、迁移学习和对抗攻击的鲁棒性。本文介绍了一种新型的损失函数Hopkins loss，它利用Hopkins统计量来强制实现所需的特征空间拓扑，这与现有的拓扑相关方法形成对比，后者旨在保留输入特征拓扑。我们在语音、文本和图像数据上评估了Hopkins loss在两种场景下的有效性：使用非线性瓶颈自编码器的分类和降维。我们的实验表明，将Hopkins loss集成到分类或降维中，对分类性能只有很小的影响，同时能够提供修改特征拓扑的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature space topology refers to the organization of samples within thefeature space. Modifying this topology can be beneficial in machine learningapplications, including dimensionality reduction, generative modeling, transferlearning, and robustness to adversarial attacks. This paper introduces a novelloss function, Hopkins loss, which leverages the Hopkins statistic to enforce adesired feature space topology, which is in contrast to existingtopology-related methods that aim to preserve input feature topology. Weevaluate the effectiveness of Hopkins loss on speech, text, and image data intwo scenarios: classification and dimensionality reduction using nonlinearbottleneck autoencoders. Our experiments show that integrating Hopkins lossinto classification or dimensionality reduction has only a small impact onclassification performance while providing the benefit of modifying featuretopology.</description>
      <author>example@mail.com (Einari Vaaras, Manu Airaksinen)</author>
      <guid isPermaLink="false">2509.11154v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring</title>
      <link>http://arxiv.org/abs/2509.10995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, 3 algorithms, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于强化学习的迁移学习框架，使用上置信界算法自动选择最适合动物检测任务的预训练模型，实现了更高的检测率并减少了计算时间。&lt;h4&gt;背景&lt;/h4&gt;动物健康监测和种群管理是野生动物保护和畜牧业管理的关键方面，这些领域越来越依赖自动化检测和跟踪系统。基于无人机和计算机视觉的系统提供了有前景的解决方案，但标记训练数据的有限性是开发有效深度学习模型的主要障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于强化学习的迁移学习框架，使用上置信界算法自动选择最适合动物检测任务的预训练模型，简化模型选择过程。&lt;h4&gt;方法&lt;/h4&gt;采用强化学习方法进行迁移学习，使用上置信界算法系统评估和排序候选模型，基于性能选择最优模型。&lt;h4&gt;主要发现&lt;/h4&gt;该框架实现了比传统方法更高的检测率，同时需要显著更少的计算时间。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了预训练神经网络架构众多导致的模型选择难题，特别对领域新手研究者有帮助，能够简化模型选择过程。&lt;h4&gt;翻译&lt;/h4&gt;动物健康监测和种群管理是野生动物保护和畜牧业管理的关键方面，这些方面越来越依赖于自动化检测和跟踪系统。虽然基于无人机(UAV)的系统结合计算机视觉为具有挑战性地形上的非侵入式动物监测提供了有前景的解决方案，但标记训练数据的有限性仍然是开发这些应用的有效深度学习(DL)模型的障碍。迁移学习已成为一种潜在的解决方案，允许在大型数据集上训练的模型适应资源有限的情况，例如数据有限的情况。然而，预训练神经网络架构的广阔格局使得选择最佳模型具有挑战性，特别是对领域新手研究人员。在本文中，我们提出了一种基于强化学习(RL)的迁移学习框架，该框架采用上置信界(UCB)算法自动选择最适合动物检测任务的预训练模型。我们的方法基于性能系统评估和排名候选模型，简化了模型选择过程。实验结果表明，与传统方法相比，我们的框架实现了更高的检测率，同时需要显著更少的计算时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Animal health monitoring and population management are critical aspects ofwildlife conservation and livestock management that increasingly rely onautomated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)based systems combined with computer vision offer promising solutions fornon-invasive animal monitoring across challenging terrains, limitedavailability of labeled training data remains an obstacle in developingeffective deep learning (DL) models for these applications. Transfer learninghas emerged as a potential solution, allowing models trained on large datasetsto be adapted for resource-limited scenarios such as those with limited data.However, the vast landscape of pre-trained neural network architectures makesit challenging to select optimal models, particularly for researchers new tothe field. In this paper, we propose a reinforcement learning (RL)-basedtransfer learning framework that employs an upper confidence bound (UCB)algorithm to automatically select the most suitable pre-trained model foranimal detection tasks. Our approach systematically evaluates and rankscandidate models based on their performance, streamlining the model selectionprocess. Experimental results demonstrate that our framework achieves a higherdetection rate while requiring significantly less computational time comparedto traditional methods.</description>
      <author>example@mail.com (Nisha Pillai, Aditi Virupakshaiah, Harrison W. Smith, Amanda J. Ashworth, Prasanna Gowda, Phillip R. Owens, Adam R. Rivers, Bindu Nanduri, Mahalingam Ramkumar)</author>
      <guid isPermaLink="false">2509.10995v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A novel IR-SRGAN assisted super-resolution evaluation of photothermal coherence tomography for impact damage in toughened thermoplastic CFRP laminates under room temperature and low temperature</title>
      <link>http://arxiv.org/abs/2509.10894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种新型红外超分辨率生成对抗网络（IR-SRGAN），用于提高复合材料在低温条件下冲击损伤检测的准确性，解决了红外热成像技术存在的空间分辨率限制问题。&lt;h4&gt;背景&lt;/h4&gt;评估复合材料在温度变化条件下的冲击损伤对航空航天、极地等极端环境应用中的结构完整性和可靠性能至关重要。&lt;h4&gt;目的&lt;/h4&gt;精确检测和量化复合材料亚表面损伤特征，如分层面积、裂纹形态和界面分离，以进行后续机械表征和寿命预测。&lt;h4&gt;方法&lt;/h4&gt;结合红外热成像（IRT）与新开发的频分复用光热相关层析成像（FM-PCT）技术，并应用基于迁移学习的红外超分辨率生成对抗网络（IR-SRGAN）来增强成像质量。&lt;h4&gt;主要发现&lt;/h4&gt;低温条件下，基体脆性增加，导致损伤机制发生变化，常温下轻微的冲击可能引发严重的基体开裂、纤维/基体脱粘或界面失效；IRT技术存在帧率受限和横向热扩散等固有局限性，影响损伤尺寸测量的准确性。&lt;h4&gt;结论&lt;/h4&gt;通过开发的IR-SRGAN技术，可以在有限的热成像数据基础上，提高横向和深度分辨成像的保真度，从而更准确地评估复合材料在极端环境下的损伤情况。&lt;h4&gt;翻译&lt;/h4&gt;评估复合材料在变化温度条件下的冲击损伤对于确保航空航天、极地和其他极端环境应用中的结构完整性和可靠性能至关重要。随着低温下基体脆性的增加，损伤机制发生变化：在常温条件下只产生轻微分层的冲击事件在严重冷载荷下可能引发广泛的基体开裂、纤维/基体脱粘或界面失效，从而降低剩余强度和疲劳寿命。精确检测和量化亚表面损伤特征（如分层面积、裂纹形态、界面分离）对后续机械表征和寿命预测至关重要。在本研究中，采用红外热成像（IRT）与新开发的频分复用光热相关层析成像（FM-PCT）相结合，捕捉三维亚表面损伤特征，其深度分辨率接近X射线显微CT的分辨率。然而，IRT的固有局限性，包括帧率受限和横向热扩散，降低了空间分辨率，从而影响损伤尺寸测量的准确性。为解决此问题，我们开发了一种基于迁移学习的红外超分辨率生成对抗网络（IR-SRGAN），基于有限的热成像数据集增强横向和深度分辨成像保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating impact-induced damage in composite materials under varyingtemperature conditions is essential for ensuring structural integrity andreliable performance in aerospace, polar, and other extreme-environmentapplications. As matrix brittleness increases at low temperatures, damagemechanisms shift: impact events that produce only minor delaminations atambient conditions can trigger extensive matrix cracking, fiber/matrixdebonding, or interfacial failure under severe cold loads, thereby degradingresidual strength and fatigue life. Precision detection and quantification ofsubsurface damage features (e.g., delamination area, crack morphology,interface separation) are critical for subsequent mechanical characterizationand life prediction. In this study, infrared thermography (IRT) coupled with anewly developed frequency multiplexed photothermal correlation tomography(FM-PCT) is employed to capture three-dimensional subsurface damage signatureswith depth resolution approaching that of X-ray micro-computed tomography.However, the inherent limitations of IRT, including restricted frame rate andlateral thermal diffusion, reduce spatial resolution and thus the accuracy ofdamage size measurement. To address this, we develop a new transferlearning-based infrared super-resolution generative adversarial network(IR-SRGAN) that enhances both lateral and depth-resolved imaging fidelity basedon limited thermographic datasets.</description>
      <author>example@mail.com (Pengfei Zhu, Hai Zhang, Stefano Sfarra, Fabrizio Sarasini, Zijing Ding, Clemente Ibarra-Castanedo, Xavier Maldague)</author>
      <guid isPermaLink="false">2509.10894v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Neurosymbolic AI Transfer Learning Improves Network Intrusion Detection</title>
      <link>http://arxiv.org/abs/2509.10850v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种创新的神经符号AI框架，用于网络入侵检测系统，展示了迁移学习在网络安全领域的潜力。&lt;h4&gt;背景&lt;/h4&gt;迁移学习在计算机视觉、自然语言处理和医学影像等领域被广泛应用，但在网络安全领域的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;设计一个创新的神经符号AI框架应用于网络入侵检测系统，以应对网络安全中的恶意活动。&lt;h4&gt;方法&lt;/h4&gt;利用迁移学习和不确定性量化的神经符号AI框架进行网络入侵检测。&lt;h4&gt;主要发现&lt;/h4&gt;在大型和结构良好的数据集上训练的迁移学习模型比依赖较小数据集的基于神经的模型表现更好。&lt;h4&gt;结论&lt;/h4&gt;迁移学习模型在网络安全领域的应用为网络安全解决方案的新时代铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习因其解决子任务和处理不同数据集的出色能力，常被应用于计算机视觉、自然语言处理和医学影像等多个领域。然而，其在网络安全领域的应用尚未得到充分探索。在本文中，我们提出了一种创新的神经符号AI框架，专门用于网络入侵检测系统，该系统在应对网络安全中的恶意活动方面起着关键作用。我们的框架利用了迁移学习和不确定性量化。研究结果表明，在大型和结构良好的数据集上训练的迁移学习模型，比依赖较小数据集的基于神经的模型表现更优，为网络安全解决方案的新时代铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning is commonly utilized in various fields such as computervision, natural language processing, and medical imaging due to its impressivecapability to address subtasks and work with different datasets. However, itsapplication in cybersecurity has not been thoroughly explored. In this paper,we present an innovative neurosymbolic AI framework designed for networkintrusion detection systems, which play a crucial role in combating maliciousactivities in cybersecurity. Our framework leverages transfer learning anduncertainty quantification. The findings indicate that transfer learningmodels, trained on large and well-structured datasets, outperform neural-basedmodels that rely on smaller datasets, paving the way for a new era incybersecurity solutions.</description>
      <author>example@mail.com (Huynh T. T. Tran, Jacob Sander, Achraf Cohen, Brian Jalaian, Nathaniel D. Bastian)</author>
      <guid isPermaLink="false">2509.10850v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering</title>
      <link>http://arxiv.org/abs/2509.07766v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures, International Quantum Engineering conference and  exhibition (QUEST-IS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图的联盟结构生成算法(GCS-Q)，用于金融资产的相关性聚类，该方法能够直接处理带符号的加权图，无需传统转换方法，并通过量子退火技术高效探索解空间。实验证明该方法在聚类质量和动态确定聚类数量方面优于现有经典算法。&lt;h4&gt;背景&lt;/h4&gt;基于收益率相关性的金融资产聚类是投资组合优化和统计套利的基础任务，但传统聚类方法在处理带符号的相关性结构时表现不佳，通常需要损失性转换和固定聚类数量等启发式假设。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接处理带符号、加权图的聚类方法，避免传统方法的局限性，并利用量子计算技术提高聚类效率和质量。&lt;h4&gt;方法&lt;/h4&gt;应用基于图的联盟结构生成算法(GCS-Q)，将每个分区步骤表述为QUBO问题，利用量子退火技术高效探索解空间。在合成和真实金融数据上验证该方法，并与SPONGE和k-Medoids等经典算法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;GCS-Q在调整兰德指数和结构平衡惩罚等指标上始终实现更高的聚类质量，同时能够动态确定聚类数量，无需预先指定聚类数。&lt;h4&gt;结论&lt;/h4&gt;近期量子计算在金融应用的基于图的无监督学习中具有实用价值，为处理复杂金融数据提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;基于收益率相关性对金融资产进行聚类是投资组合优化和统计套利中的基础任务。然而，传统的聚类方法在处理带符号的相关性结构时往往表现不足，通常需要损失性转换和启发式假设，如固定数量的聚类。在本工作中，我们应用基于图的联盟结构生成算法(GCS-Q)直接聚类带符号的加权图，而不依赖这些转换。GCS-Q将每个分区步骤表述为QUBO问题，使其能够利用量子退火高效探索指数级大的解空间。我们在合成和真实金融数据上验证了我们的方法，并与SPONGE和k-Medoids等最先进的经典算法进行基准测试。我们的实验表明，GCS-Q在调整兰德指数和结构平衡惩罚等指标上始终实现更高的聚类质量，同时动态确定聚类数量。这些结果突显了近期量子计算在金融应用的基于图的无监督学习中的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering financial assets based on return correlations is a fundamentaltask in portfolio optimization and statistical arbitrage. However, classicalclustering methods often fall short when dealing with signed correlationstructures, typically requiring lossy transformations and heuristic assumptionssuch as a fixed number of clusters. In this work, we apply the Graph-basedCoalition Structure Generation algorithm (GCS-Q) to directly cluster signed,weighted graphs without relying on such transformations. GCS-Q formulates eachpartitioning step as a QUBO problem, enabling it to leverage quantum annealingfor efficient exploration of exponentially large solution spaces. We validateour approach on both synthetic and real-world financial data, benchmarkingagainst state-of-the-art classical algorithms such as SPONGE and k-Medoids. Ourexperiments demonstrate that GCS-Q consistently achieves higher clusteringquality, as measured by Adjusted Rand Index and structural balance penalties,while dynamically determining the number of clusters. These results highlightthe practical utility of near-term quantum computing for graph-basedunsupervised learning in financial applications.</description>
      <author>example@mail.com (Shivam Sharma, Supreeth Mysore Venkatesh, Pushkin Kachroo)</author>
      <guid isPermaLink="false">2509.07766v2</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury</title>
      <link>http://arxiv.org/abs/2509.12155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了使用低秩适应（LoRA）技术微调大型视觉模型（DinoV2和SwinV2）以诊断立体定向体放射治疗（SBRT）后X射线CT扫描中的放射性肺损伤（RILI）的有效性。&lt;h4&gt;背景&lt;/h4&gt;需要开发有效的方法来诊断放射治疗后的放射性肺损伤，这可能对患者监测和治疗调整至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估LoRA方法在微调大型视觉模型以诊断RILI方面的稳健性和效率，并与传统的完全微调和仅推理方法进行比较。&lt;h4&gt;方法&lt;/h4&gt;使用两种尺寸（50 mm³和75 mm³）的裁剪图像（以治疗等中心为中心），以及不同的适应技术将2D大型视觉模型适应为3D数据处理，评估模型对空间上下文的敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;LoRA实现了与传统微调相当或更好的性能，同时显著降低了计算成本和训练时间，因为需要更少的可训练参数。&lt;h4&gt;结论&lt;/h4&gt;LoRA是一种有效的方法，可用于微调大型视觉模型以诊断放射性肺损伤，同时保持高性能并减少计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;本研究调查了低秩适应（LoRA）技术用于微调大型视觉模型（DinoV2和SwinV2）以诊断立体定向体放射治疗（SBRT）后X射线CT扫描中放射性肺损伤（RILI）的有效性。为了评估这种方法的稳健性和效率，我们将LoRA与传统完全微调和仅推理（不微调）方法进行比较。除了使用以治疗等为中心的两种尺寸（50 mm³和75 mm³）的裁剪图像外，我们还使用了不同的适应技术将2D大型视觉模型适应为3D数据处理，以确定模型对空间上下文的敏感性。实验结果表明，LoRA实现了与传统微调相当或更好的性能，同时通过需要更少的可训练参数显著降低了计算成本和训练时间。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何使用人工智能技术自动诊断肺癌患者接受立体定向体部放射治疗（SBRT）后可能出现的辐射诱导肺损伤（RILI）。这个问题很重要，因为RILI是肺癌放疗后的常见并发症（发生率5-25%），早期诊断困难但至关重要，能帮助医生及时干预，改善患者治疗效果。传统诊断方法面临症状与其他肺部疾病重叠、影像特征随时间变化等挑战，而现有AI方法（如CNN）性能有限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到CNN在RILI诊断中的局限性，转而探索视觉变换器（ViT）等先进模型，因为它们能更好地捕捉全局图像信息。他们借鉴了LoRA（低秩自适应）技术，这是一种高效微调大型模型的方法。作者还参考了自己之前使用CNN诊断RILI的工作，以及其他研究使用放射组学预测RILI的方法。设计上，他们比较了三种微调策略（不微调、完全微调、LoRA微调），并测试了不同输入方式（2D切片和3D正交信息）和图像大小对模型性能的影响。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用LoRA技术微调大型视觉模型（DinoV2和SwinV2），只更新少量参数就能有效适应医学影像诊断任务，同时保留模型从自然图像中学到的丰富特征。整体流程包括：1）收集并预处理CT扫描数据，包括标准化分辨率、对齐、裁剪治疗区域等；2）设计两种输入方式（2D轴向切片和3D正交切片）；3）应用三种微调策略（不微调、完全微调、LoRA微调）；4）使用五折交叉验证训练和评估模型；5）在独立测试集上评估性能，使用ROC-AUC、F1分数等指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将大型视觉模型应用于SBRT后RILI分类；引入LoRA微调技术大幅降低计算成本；评估不同输入方式和图像大小对性能的影响；在具有挑战性的子集（如治疗后早期小病灶）上验证模型。相比之前工作，本研究使用更先进的视觉变换器代替CNN，直接处理原始影像而非手工特征，使用更大数据集，并通过LoRA实现了参数高效的微调，在保持性能的同时显著减少了训练时间和计算资源需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文展示了使用LoRA微调的大型视觉模型能够高效准确地诊断SBRT后的辐射诱导肺损伤，显著减少了计算成本和训练时间，同时保持了与完全微调相当或更好的性能，为临床决策提供了AI支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the efficacy of Low-Rank Adaptation (LoRA) forfine-tuning large Vision Models, DinoV2 and SwinV2, to diagnoseRadiation-Induced Lung Injury (RILI) from X-ray CT scans following StereotacticBody Radiation Therapy (SBRT). To evaluate the robustness and efficiency ofthis approach, we compare LoRA with traditional full fine-tuning andinference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3and 75 mm3), centered at the treatment isocenter, in addition to differentadaptation techniques for adapting the 2D LVMs for 3D data were used todetermine the sensitivity of the models to spatial context. Experimentalresults show that LoRA achieves comparable or superior performance totraditional fine-tuning while significantly reducing computational costs andtraining times by requiring fewer trainable parameters.</description>
      <author>example@mail.com (M. Bolhassani, B. Veasey, E. Daugherty, S. Keltner, N. Kumar, N. Dunlap, A. Amini)</author>
      <guid isPermaLink="false">2509.12155v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Multi Anatomy X-Ray Foundation Model</title>
      <link>http://arxiv.org/abs/2509.12146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了XR-0，这是一个多解剖部位的X射线基础模型，通过在大规模私有数据集上进行自监督学习训练，在多种临床任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;X射线影像在放射学中无处不在，但现有的AI基础模型大多局限于胸部解剖结构，无法在更广泛的临床任务中泛化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨多种解剖区域和临床任务泛化的X射线基础模型。&lt;h4&gt;方法&lt;/h4&gt;介绍XR-0模型，使用自监督学习方法，在包含115万张图像的大规模私有数据集上进行训练，这些图像涵盖不同的解剖区域。该模型在12个数据集和20个下游任务上进行了评估，包括分类、检索、分割、定位、视觉定位和报告生成。&lt;h4&gt;主要发现&lt;/h4&gt;XR-0在大多数多解剖部位任务上达到了最先进的性能，并且在胸部特定基准测试中保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;解剖多样性和监督对于构建健壮的通用医疗视觉模型至关重要，为放射学中可扩展和适应性强的AI系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;X射线影像在放射学中无处不在，然而大多数现有的AI基础模型仅限于胸部解剖结构，无法在更广泛的临床任务中泛化。在这项工作中，我们介绍了XR-0，这是一个多解剖部位的X射线基础模型，它通过在包含115万张图像的大规模私有数据集上进行自监督学习训练，这些图像涵盖多种解剖区域，并在12个数据集和20个下游任务上进行了评估，包括分类、检索、分割、定位、视觉定位和报告生成。XR-0在大多数多解剖部位任务上达到了最先进的性能，并在胸部特定基准测试中保持竞争力。我们的结果表明，解剖多样性和监督对于构建健壮的通用医疗视觉模型至关重要，为放射学中可扩展和适应性强的AI系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; X-ray imaging is a ubiquitous in radiology, yet most existing AI foundationmodels are limited to chest anatomy and fail to generalize across broaderclinical tasks. In this work, we introduce XR-0, the multi-anatomy X-rayfoundation model using self-supervised learning on a large, private dataset of1.15 million images spanning diverse anatomical regions and evaluated across 12datasets and 20 downstream tasks, including classification, retrieval,segmentation, localization, visual grounding, and report generation. XR-0achieves state-of-the-art performance on most multi-anatomy tasks and remainscompetitive on chest-specific benchmarks. Our results demonstrate thatanatomical diversity and supervision are critical for building robust,general-purpose medical vision models, paving the way for scalable andadaptable AI systems in radiology.</description>
      <author>example@mail.com (Nishank Singla, Krisztian Koos, Farzin Haddadpour, Amin Honarmandi Shandiz, Lovish Chum, Xiaojian Xu, Qing Jin, Erhan Bas)</author>
      <guid isPermaLink="false">2509.12146v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Navigation Foundation Model</title>
      <link>http://arxiv.org/abs/2509.12129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://pku-epic.github.io/NavFoM-Web/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个跨具身和跨任务的导航基础模型(NavFoM)，该模型在多种具身(四足动物、无人机、轮式机器人和车辆)和多种导航任务(视觉语言导航、目标搜索、目标跟踪和自动驾驶)上表现出强大的泛化能力，无需任务特定的微调即可达到最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管大型视觉-语言模型(VLMs)在通用视觉-语言任务上表现出显著的零样本性能，但它们在具身导航中的泛化能力仍然主要局限于狭窄的任务设置和特定的具身架构。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨不同具身架构和多种导航任务泛化的导航基础模型，解决当前具身AI导航系统泛化能力有限的问题。&lt;h4&gt;方法&lt;/h4&gt;NavFoM采用统一架构处理多模态导航输入，集成了标识符令牌来嵌入具身的摄像头视图信息和任务的时间上下文，并在有限的令牌长度预算下使用动态调整的采样策略控制所有观测令牌。该模型在八百万个导航样本上进行了训练，涵盖了四足动物、无人机、轮式机器人和车辆等多种具身。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准上的广泛评估表明，NavFoM在多个导航任务和具身上实现了最先进或极具竞争力的性能，无需任务特定的微调。额外的真实世界实验进一步证实了该模型具有强大的泛化能力和实际适用性。&lt;h4&gt;结论&lt;/h4&gt;NavFoM代表了具身AI导航领域的重要进展，通过统一的架构和创新的令牌处理策略，实现了跨具身和跨任务的强大泛化能力，为实际应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;导航是具身AI中的基本能力，代表遵循语言指令在物理环境中感知和互动所需的智能。尽管大型视觉-语言模型(VLMs)在通用视觉-语言任务上表现出显著的零样本性能，但它们在具身导航中的泛化能力仍然主要局限于狭窄的任务设置和特定的具身架构。在这项工作中，我们引入了一个跨具身和跨任务的导航基础模型(NavFoM)，该模型在八百万个导航样本上进行了训练，这些样本涵盖了四足动物、无人机、轮式机器人和车辆，并跨越了视觉语言导航、目标搜索、目标跟踪和自动驾驶等多种任务。NavFoM采用统一架构，处理来自不同摄像头配置和导航范围的多模态导航输入。为了适应不同的摄像头设置和时间范围，NavFoM集成了标识符令牌，这些令牌嵌入具身的摄像头视图信息和任务的时间上下文。此外，为了满足实际部署的需求，NavFoM在有限的令牌长度预算下，使用动态调整的采样策略控制所有观测令牌。在公共基准上的广泛评估表明，我们的模型在多个导航任务和具身上实现了最先进或极具竞争力的性能，而无需任务特定的微调。额外的真实世界实验进一步证实了我们方法的强大泛化能力和实际适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决具身导航模型在跨具身形态和跨任务上的泛化能力不足问题。现有导航模型要么局限于特定具身形态，要么只能处理特定任务，缺乏通用性。这个问题很重要，因为导航是具身AI的基础能力，一个通用的导航模型可以适应各种机器人和应用场景，减少定制化需求，降低开发和部署成本，并更好地适应真实世界的复杂性和多样性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类主要通过视觉完成导航任务的启发，以及最近仅视觉导航方法的成功，将通用导航任务表述为处理第一视角视频和语言指令并预测轨迹。他们借鉴了视觉语言模型的基本架构、Transformer-based在大规模跨具身数据上的训练方法、视觉特征缓存机制和位置编码等技术。创新性地设计了时间-视角指示器(TVI)tokens来标识摄像头视角和时序信息，以及基于预算的时序采样(BATS)策略来处理大量视频帧，平衡性能和推理速度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的导航基础模型，能够处理不同具身形态和不同任务的导航问题。整体流程包括：1)使用预训练视觉编码器提取视觉特征并通过网格池化生成紧凑表示；2)使用TVI tokens编码摄像头视角和时序信息；3)采用BATS策略动态采样历史帧；4)组织视觉和语言令牌并通过LLM和规划模型预测轨迹；5)在大规模数据(800万导航样本和476万开放世界知识样本)上训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)跨具身和跨任务的统一导航框架；2)时间-视角指示器(TVI)tokens显式编码摄像头视角和时序信息；3)基于预算的时序采样(BATS)策略平衡性能和推理速度；4)大规模多样化数据集训练。相比之前工作，NavFoM不仅能跨不同具身形态泛化，还能处理多种导航任务；显式编码了多视角信息，解决了输入歧义；优化了长序列处理；直接预测轨迹而非生成文本描述；处理范围更广，包括自动驾驶和UAV导航等复杂场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; NavFoM通过引入时间-视角指示器tokens和基于预算的时序采样策略，创建了一个能够跨具身形态和跨任务泛化的统一导航基础模型，在多种导航任务和具身形态上实现了最先进或极具竞争力的性能，无需任务特定的微调。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigation is a fundamental capability in embodied AI, representing theintelligence required to perceive and interact within physical environmentsfollowing language instructions. Despite significant progress in largeVision-Language Models (VLMs), which exhibit remarkable zero-shot performanceon general vision-language tasks, their generalization ability in embodiednavigation remains largely confined to narrow task settings andembodiment-specific architectures. In this work, we introduce across-embodiment and cross-task Navigation Foundation Model (NavFoM), trainedon eight million navigation samples that encompass quadrupeds, drones, wheeledrobots, and vehicles, and spanning diverse tasks such as vision-and-languagenavigation, object searching, target tracking, and autonomous driving. NavFoMemploys a unified architecture that processes multimodal navigation inputs fromvarying camera configurations and navigation horizons. To accommodate diversecamera setups and temporal horizons, NavFoM incorporates identifier tokens thatembed camera view information of embodiments and the temporal context of tasks.Furthermore, to meet the demands of real-world deployment, NavFoM controls allobservation tokens using a dynamically adjusted sampling strategy under alimited token length budget. Extensive evaluations on public benchmarksdemonstrate that our model achieves state-of-the-art or highly competitiveperformance across multiple navigation tasks and embodiments without requiringtask-specific fine-tuning. Additional real-world experiments further confirmthe strong generalization capability and practical applicability of ourapproach.</description>
      <author>example@mail.com (Jiazhao Zhang, Anqi Li, Yunpeng Qi, Minghan Li, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu, Zhizheng Zhang, He Wang)</author>
      <guid isPermaLink="false">2509.12129v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</title>
      <link>http://arxiv.org/abs/2509.12105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICIAP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于SAM2的少样本分割方法(FS-SAM2)，该方法利用SAM2的视频能力直接用于少样本任务，并通过低秩适应(LoRA)处理标准数据集中的多样化图像，只需少量参数进行元训练，在多个数据集上取得了显著结果且推理效率高。&lt;h4&gt;背景&lt;/h4&gt;少样本语义分割最近受到广泛关注，目标是用少量标注样本分割未见类别。现有方法通常需要从头训练额外模块并在大型数据集上大量训练才能达到最佳性能。SAM2是一个用于零样本图像和视频分割的基础模型，采用模块化设计。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于SAM2的少样本分割方法，将SAM2的视频能力直接应用于少样本任务，通过LoRA处理标准数据集中的多样化图像，仅用少量参数进行元训练，支持任何K-shot配置。&lt;h4&gt;方法&lt;/h4&gt;提出FS-SAM2方法，直接利用SAM2的视频能力处理少样本任务，应用低秩适应(LoRA)到原始模块以处理标准数据集中的多样化图像，只对少量参数进行元训练以适应SAM2。&lt;h4&gt;主要发现&lt;/h4&gt;在PASCAL-5^i、COCO-20^i和FSS-1000数据集上评估FS-SAM2取得了显著结果，在推理过程中表现出优秀的计算效率，代码已开源。&lt;h4&gt;结论&lt;/h4&gt;FS-SAM2是一种有效的少样本语义分割方法，能够有效利用SAM2的基础模型能力，在多个数据集上取得了良好性能且计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;少样本语义分割最近受到了广泛关注。其目标是开发一个仅使用少量标注样本就能分割未见类别的模型。大多数现有方法通过从头训练一个额外模块来调整预训练模型。这些方法需要在大型数据集上进行大量训练才能达到最佳性能。Segment Anything Model 2 (SAM2) 是一个用于零样本图像和视频分割的基础模型，采用模块化设计。在本文中，我们提出了一种基于SAM2的少样本分割方法(FS-SAM2)，其中SAM2的视频能力被直接用于少样本任务。此外，我们对原始模块应用了低秩适应(Low-Rank Adaptation, LoRA)，以处理标准数据集中常见的多样化图像，这与SAM2预训练中使用的时间连接帧不同。通过这种方法，只有少量参数进行元训练，从而有效适应SAM2，同时受益于其出色的分割性能。我们的方法支持任何K-shot配置。我们在PASCAL-5^i、COCO-20^i和FSS-1000数据集上评估了FS-SAM2，取得了显著结果，并在推理过程中表现出优秀的计算效率。代码可在https://github.com/fornib/FS-SAM2获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot semantic segmentation has recently attracted great attention. Thegoal is to develop a model capable of segmenting unseen classes using only afew annotated samples. Most existing approaches adapt a pre-trained model bytraining from scratch an additional module. Achieving optimal performance withthese approaches requires extensive training on large-scale datasets. TheSegment Anything Model 2 (SAM2) is a foundational model for zero-shot image andvideo segmentation with a modular design. In this paper, we propose a Few-Shotsegmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilitiesare directly repurposed for the few-shot task. Moreover, we apply a Low-RankAdaptation (LoRA) to the original modules in order to handle the diverse imagestypically found in standard datasets, unlike the temporally connected framesused in SAM2's pre-training. With this approach, only a small number ofparameters is meta-trained, which effectively adapts SAM2 while benefiting fromits impressive segmentation performance. Our method supports any K-shotconfiguration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ andFSS-1000 datasets, achieving remarkable results and demonstrating excellentcomputational efficiency during inference. Code is available athttps://github.com/fornib/FS-SAM2</description>
      <author>example@mail.com (Bernardo Forni, Gabriele Lombardi, Federico Pozzi, Mirco Planamente)</author>
      <guid isPermaLink="false">2509.12105v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Time-Series Foundation Model by Universal Delay Embedding</title>
      <link>http://arxiv.org/abs/2509.12080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为'通用延迟嵌入'(UDE)的预训练基础模型，通过整合延迟嵌入表示和Koopman算子预测来革新时间序列预测，在各种基准和真实世界数据集上表现出色，具有可扩展性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在科学和工业领域具有重要应用价值，但传统方法在处理非线性时间序列时面临挑战，需要更有效的方法来捕捉动力系统的动态和拓扑特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测非线性时间序列的框架，同时保持良好的可解释性和泛化能力，适用于广泛的科学和工业应用。&lt;h4&gt;方法&lt;/h4&gt;利用Takens嵌入定理，将观测数据构造为动力系统表示，从Hankel矩阵构建二维子空间补丁，将这些补丁视为图像并通过先进深度学习技术处理，使用自注意力编码器学习这些补丁，在潜在空间中以线性方式学习有限维Koopman算子进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;在各类基准和气候数据集上评估显示，UDE比最先进的基础模型平均降低20%以上的均方误差，在微调场景中表现出更好的泛化能力；学习到的动力表示和Koopman算子预测具有卓越的可解释性，能够一致识别拓扑信息丰富的子空间并稳健编码领域不变动力。&lt;h4&gt;结论&lt;/h4&gt;UDE确立了一种可扩展、可解释的通用时间序列建模和预测框架，具有广泛的科学和工业应用价值，为时间序列分析提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;本研究引入了通用延迟嵌入(UDE)，这是一种预训练基础模型，旨在通过有原则地整合延迟嵌入表示和Koopman算子预测来革新时间序列预测。利用Takens嵌入定理，UDE作为观测数据的动力表示，从Hankel矩阵构建二维子空间补丁，理论上保留了底层动力系统的动力和拓扑特性。这些补丁被视为图像，可以通过利用先进的深度学习技术高效处理。计算上，这些补丁进一步作为令牌用于学习自注意力编码器，从而在潜在空间中以线性方式通过有限维Koopman算子实现非线性时间序列的准确预测。在各种基准和真实世界气候数据集上的广泛评估表明，与最先进的基础模型相比，平均均方误差降低了20%以上，同时在微调场景中表现出更好的泛化能力。特别是，学习到的动力表示和从补丁中形成的Koopman算子预测表现出卓越的可解释性，能够一致识别拓扑信息丰富的子空间并稳健编码领域不变动力，确立了UDE作为可扩展、可解释的通用时间序列建模和预测框架，具有广泛的科学和工业应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces Universal Delay Embedding (UDE), a pretrainedfoundation model designed to revolutionize time-series forecasting throughprincipled integration of delay embedding representation and Koopman operatorprediction. Leveraging Takens' embedding theorem, UDE as a dynamicalrepresentation of observed data constructs two-dimensional subspace patchesfrom Hankel matrices, theoretically preserving dynamical and topologicalproperties of underlying dynamical systems. Such patches are viewed as images,which can be efficiently processed by exploiting advanced deep learningtechnologies. Computationally, these patches further serve as tokens forlearning a self-attention encoder, thus enabling accurate prediction ofnonlinear time-series by a finite-dimensional Koopman operator in a linearmanner in a latent space. Extensive evaluations across various benchmarks andreal-world climate datasets demonstrate over 20% average reduction in meansquared error versus state-of-the-art foundation models, alongside superiorgeneralization in fine-tuning scenarios. In particular, the learned dynamicalrepresentations and Koopman operator prediction forms from the patches exhibitexceptional interpretability, with consistent identification of topologicallyinformative subspaces and robust encoding of domain-invariant dynamics,establishing UDE as a scalable, interpretable framework for universaltime-series modeling and forecasting with broad scientific and industrialapplicability.</description>
      <author>example@mail.com (Zijian Wang, Peng Tao, Jifan Shi, Rui Bao, Rui Liu, Luonan Chen)</author>
      <guid isPermaLink="false">2509.12080v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications</title>
      <link>http://arxiv.org/abs/2509.12053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first two authors have equal contributions; Published as a  conference paper in HPCA 2025; 13 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LEGO框架是一种针对张量应用的新方法，能够自动生成空间架构设计和可综合的RTL代码，无需手工RTL设计模板，解决了现有框架在设计灵活性和RTL生成生产力之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;现代张量应用，特别是基础模型和生成式AI应用需要多种输入模态（视觉和语言），这增加对灵活加速器架构的需求。现有框架要么仅限于少数手工编写的模板，要么无法自动生成RTL。&lt;h4&gt;目的&lt;/h4&gt;提出LEGO框架，针对张量应用，自动生成空间架构设计并输出可综合的RTL代码，无需手工RTL设计模板，解决设计灵活性和RTL生成生产力之间的权衡问题。&lt;h4&gt;方法&lt;/h4&gt;利用基于仿射变换的架构表示，LEGO前端找到功能单元之间的互连，合成存储系统，并根据数据重用分析融合不同的空间数据流设计；LEGO后端在原始级别图上翻译硬件以执行低级优化，应用线性规划算法最优插入流水线寄存器，减少切换空间数据流时的未使用逻辑开销。&lt;h4&gt;主要发现&lt;/h4&gt;LEGO相比之前的工作Gemmini可以实现3.2倍的速度提升和2.4倍的能效提升，能够为生成式AI应用中的各种现代基础模型生成一个架构。&lt;h4&gt;结论&lt;/h4&gt;LEGO框架有效地解决了设计灵活性和RTL生成生产力之间的权衡问题，能够高效地为现代张量应用生成优化的硬件架构。&lt;h4&gt;翻译&lt;/h4&gt;现代张量应用，特别是基础模型和生成式AI应用需要多种输入模态（视觉和语言），这增加对灵活加速器架构的需求。现有框架在设计灵活性和RTL生成生产力之间存在权衡：要么仅限于少数手工编写的模板，要么无法自动生成RTL。为解决这一挑战，我们提出了LEGO框架，该框架针对张量应用，自动生成空间架构设计并输出可综合的RTL代码，无需手工RTL设计模板。利用基于仿射变换的架构表示，LEGO前端找到功能单元之间的互连，合成存储系统，并根据数据重用分析融合不同的空间数据流设计。LEGO后端然后在原始级别图上翻译硬件以执行低级优化，并应用一组线性规划算法以最优方式插入流水线寄存器，减少切换空间数据流时未使用逻辑的开销。我们的评估表明，与之前的工作Gemmini相比，LEGO可以实现3.2倍的速度提升和2.4倍的能效提升，并且可以为生成式AI应用中的各种现代基础模型生成一个架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/HPCA61900.2025.00101&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern tensor applications, especially foundation models and generative AIapplications require multiple input modalities (both vision and language),which increases the demand for flexible accelerator architecture. Existingframeworks suffer from the trade-off between design flexibility andproductivity of RTL generation: either limited to very few hand-writtentemplates or cannot automatically generate the RTL. To address this challenge,we propose the LEGO framework, which targets tensor applications andautomatically generates spatial architecture design and outputs synthesizableRTL code without handwritten RTL design templates. Leveraging theaffine-transformation-based architecture representation, LEGO front end findsinterconnections between function units, synthesizes the memory system, andfuses different spatial dataflow designs based on data reuse analysis. LEGOback end then translates the hardware in a primitive-level graph to performlower-level optimizations, and applies a set of linear-programming algorithmsto optimally insert pipeline registers and reduce the overhead of unused logicwhen switching spatial dataflows. Our evaluation demonstrates that LEGO canachieve 3.2x speedup and 2.4x energy efficiency compared to previous workGemmini, and can generate one architecture for diverse modern foundation modelsin generative AI applications.</description>
      <author>example@mail.com (Yujun Lin, Zhekai Zhang, Song Han)</author>
      <guid isPermaLink="false">2509.12053v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Radio Galaxy Zoo: Morphological classification by Fanaroff-Riley designation using self-supervised pre-training</title>
      <link>http://arxiv.org/abs/2509.11988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to MNRAS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用RGZ项目中的14,000多个射电星系，对约5,900个FRI型和8,100个FRII型射电星系进行了分类。研究分析了使用预训练并微调的射电星系基础模型预测的形态，发现了FRI和FRII光度-大小分布的重叠区域，模型在这些区域的置信度较低。研究还探讨了预训练和微调数据选择对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;Radio Galaxy Zoo (RGZ)项目提供了大量射电星系数据，射电星系的形态分类（特别是Fanaroff-Riley分类）是天文学研究中的重要课题。随着自动化分类方法的发展，了解模型训练数据选择对结果的影响变得尤为重要。&lt;h4&gt;目的&lt;/h4&gt;分析RGZ目录中预测的射电星系形态，评估预训练并微调的射电星系基础模型在FR形态分类任务上的表现，并探讨训练数据选择对模型输出的影响。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的射电星系基础模型，并针对预测Fanaroff-Riley形态的任务进行了微调。对RGZ项目中的14,000多个射电星系进行分类，并对约5,900个FRI型和8,100个FRII型射电星系进行分析。研究还考察了预训练和微调数据选择对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1. FRI和FRII形态分类的光度-大小分布存在重叠；2. 模型在重叠区域的预测置信度最低，表明源形态在这些区域更为模糊；3. 发现了低光度FRII源，其比例与之前的研究一致；4. 本研究发现的低光度FRII源与之前研究识别的源存在差异，可能受分类方法影响；5. 不同的预训练数据选择会影响模型置信度，但不会引起系统性的泛化偏差；6. 微调数据的选择可能对模型有不同影响。&lt;h4&gt;结论&lt;/h4&gt;随着天体源识别和分类的自动化方法日益普及，训练数据的选择会显著影响模型输出并可能传播到下游分析中，因此需要谨慎选择训练数据。&lt;h4&gt;翻译&lt;/h4&gt;在本研究中，我们检查了从Radio Galaxy Zoo (RGZ)项目中精心挑选的14,000多个射电星系，并为约5,900个FRI型和8,100个FRII型射电星系提供了分类。我们展示了使用经过微调以预测Fanaroff-Riley (FR)形态的预训练射电星系基础模型对RGZ目录中预测的射电星系形态的分析。如先前研究所示，我们的结果表明形态分类的FRI和FRII光度-大小分布存在重叠，我们发现模型对其预测的置信度在这一重叠区域最低，表明源形态更加模糊。我们确定了低光度FRII源的存在，其相对于FRII总数的比例与先前研究一致。然而，将本研究发现的低光度FRII源与先前研究确定的源进行比较，揭示了差异，这可能表明它们的选择受到分类方法选择的影响。我们研究了预训练和微调数据选择对下游分类任务模型性能的影响，表明虽然不同的预训练数据选择会影响模型置信度，但它们似乎不会引起所考虑的物理和观测特性范围内的系统性泛化偏差；然而，我们注意到对于微调情况可能并非如此。随着天体源识别和分类的自动化方法日益普及，我们强调了可能影响模型输出并传播到下游分析的训练数据选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we examine over 14,000 radio galaxies finely selected fromRadio Galaxy Zoo (RGZ) project and provide classifications for approximately5,900 FRIs and 8,100 FRIIs. We present an analysis of these predicted radiogalaxy morphologies for the RGZ catalogue, classified using a pre-trained radiogalaxy foundation model that has been fine-tuned to predict Fanaroff-Riley (FR)morphology. As seen in previous studies, our results show overlap betweenmorphologically classified FRI and FRII luminosity-size distributions and wefind that the model's confidence in its predictions is lowest in this overlapregion, suggesting that source morphologies are more ambiguous. We identify thepresence of low-luminosity FRII sources, the proportion of which, with respectto the total number of FRIIs, is consistent with previous studies. However, acomparison of the low-luminosity FRII sources found in this work with thoseidentified by previous studies reveals differences that may indicate theirselection is influenced by the choice of classification methodology. Weinvestigate the impacts of both pre-training and fine-tuning data selection onmodel performance for the downstream classification task, and show that whiledifferent pre-training data choices affect model confidence they do not appearto cause systematic generalisation biases for the range of physical andobservational characteristics considered in this work; however, we note thatthe same is not necessarily true for fine-tuning. As automated approaches toastronomical source identification and classification become increasinglyprevalent, we highlight training data choices that can affect the model outputsand propagate into downstream analyses.</description>
      <author>example@mail.com (Nutthawara Buatthaisong, Inigo Val Slijepcevic, Anna M. M. Scaife, Micah Bowles, Andrew Hopkins, Devina Mohan, Stanislav S Shabala, O. Ivy Wong)</author>
      <guid isPermaLink="false">2509.11988v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Low-rank Orthogonalization for Large-scale Matrix Optimization with Applications to Foundation Model Training</title>
      <link>http://arxiv.org/abs/2509.11983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了低秩正交化方法，用于改进神经网络训练中的矩阵优化问题，并通过实验和理论分析证明了其优越性。&lt;h4&gt;背景&lt;/h4&gt;神经网络训练本质上是一个大规模矩阵优化问题，但长期以来神经网络参数的矩阵结构被忽视了。Muon优化器因其显式利用这种结构而在基础模型训练中表现出色。&lt;h4&gt;目的&lt;/h4&gt;提出低秩正交化方法，利用神经网络训练过程中梯度的低秩特性，改进Muon优化器的性能。&lt;h4&gt;方法&lt;/h4&gt;提出低秩正交化，基于此提出低秩矩阵符号梯度下降和Muon的低秩变体。&lt;h4&gt;主要发现&lt;/h4&gt;低秩正交化具有优越的性能，低秩Muon在GPT-2和LLaMA预训练中超过了基础Muon的性能。理论上，建立了低秩矩阵符号梯度下降和低秩Muon的迭代复杂性。&lt;h4&gt;结论&lt;/h4&gt;低秩正交化是一种有效的神经网络训练方法，能够显著提升优化性能。&lt;h4&gt;翻译&lt;/h4&gt;神经网络训练本质上是一个大规模矩阵优化问题，但长期以来神经网络参数的矩阵结构被忽视了。最近，名为Muon的优化器因其显式利用这种结构而在基础模型训练中表现出色，引起了广泛关注。Muon成功的一个关键组成部分是矩阵正交化。在这篇论文中，作者提出了'低秩正交化'，它明确利用了神经网络训练过程中梯度的低秩特性。基于此，他们提出了低秩矩阵符号梯度下降和Muon的低秩变体。数值实验表明，低秩正交化具有优越的性能，低秩Muon在GPT-2和LLaMA预训练中取得了有希望的结果，超过了精心调整的基础Muon的性能。理论上，他们建立了低秩矩阵符号梯度下降寻找近似平稳解的迭代复杂性，以及低秩Muon在重尾噪声下寻找近似随机平稳解的迭代复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural network (NN) training is inherently a large-scale matrix optimizationproblem, yet the matrix structure of NN parameters has long been overlooked.Recently, the optimizer Muon \cite{jordanmuon}, which explicitly exploits thisstructure, has gained significant attention for its strong performance infoundation model training. A key component contributing to Muon's success ismatrix orthogonalization. In this paper, we propose {\it low-rankorthogonalization}, which explicitly leverages the low-rank nature of gradientsduring NN training. Building on this, we propose low-rank matrix-signedgradient descent and a low-rank variant of Muon. Our numerical experimentsdemonstrate the superior performance of low-rank orthogonalization, with thelow-rank Muon achieving promising results in GPT-2 and LLaMA pretraining --surpassing the performance of the carefully tuned vanilla Muon. Theoretically,we establish the iteration complexity of the low-rank matrix-signed gradientdescent for finding an approximate stationary solution, as well as that oflow-rank Muon for finding an approximate stochastic stationary solution underheavy-tailed noise.</description>
      <author>example@mail.com (Chuan He, Zhanwang Deng, Zhaosong Lu)</author>
      <guid isPermaLink="false">2509.11983v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>MusicSwarm: Biologically Inspired Intelligence for Music Composition</title>
      <link>http://arxiv.org/abs/2509.11973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了一种去中心化的音乐创作系统，由相同的冻结基础模型组成，通过基于信息素的点对点信号协调，无需权重更新即可生成连贯的长篇音乐作品。&lt;h4&gt;背景&lt;/h4&gt;传统音乐创作系统通常采用集中式架构，而本研究探索了去中心化群体在音乐创作中的应用潜力。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需权重更新的去中心化音乐创作系统，通过信息素协调机制生成连贯的长篇音乐作品，并探索其在创意生成中的优势。&lt;h4&gt;方法&lt;/h4&gt;使用相同的冻结基础模型组成群体，通过基于信息素的点对点信号协调；比较集中式多智能体系统与完全去中心化群体；进行符号、音频和图论分析评估结果。&lt;h4&gt;主要发现&lt;/h4&gt;去中心化群体系统产生了更高质量、更大多样性和结构变化的作品；系统动态趋于稳定的互补角色配置；自相似性网络揭示了小世界架构；局部新颖性能够整合为全球音乐形式。&lt;h4&gt;结论&lt;/h4&gt;MusicSwarm通过将专业化从参数更新转向交互规则、共享记忆和动态共识，为长时程创造性结构提供了计算和数据高效的途径，可直接应用于音乐之外的协作写作、设计和科学发现。&lt;h4&gt;翻译&lt;/h4&gt;我们展示了一种去中心化的群体，由相同的冻结基础模型组成，通过基于信息素的点对点信号协调，无需任何权重更新即可产生连贯的长篇音乐创作。我们将带有全局评论者的集中式多智能体系统与完全去中心化的群体进行比较，在后者中，小节级智能体感知并存储和声、节奏和结构线索，适应短期记忆，并达成共识。通过符号、音频和图论分析，群体系统产生了更高质量的作品，同时提供了更大的多样性和结构变化，并在创意指标上表现更优。系统动态趋于稳定的互补角色配置，自相似性网络揭示了具有高效远程连接和专门桥接模体的小世界架构，阐明了局部新颖性如何整合为全球音乐形式。通过将专业化从参数更新转向交互规则、共享记忆和动态共识，MusicSwarm为长时程创造性结构提供了一种计算和数据高效的途径，可直接应用于音乐之外的协作写作、设计和科学发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We show that coherent, long-form musical composition can emerge from adecentralized swarm of identical, frozen foundation models that coordinate viastigmergic, peer-to-peer signals, without any weight updates. We compare acentralized multi-agent system with a global critic to a fully decentralizedswarm in which bar-wise agents sense and deposit harmonic, rhythmic, andstructural cues, adapt short-term memory, and reach consensus. Across symbolic,audio, and graph-theoretic analyses, the swarm yields superior quality whiledelivering greater diversity and structural variety and leads across creativitymetrics. The dynamics contract toward a stable configuration of complementaryroles, and self-similarity networks reveal a small-world architecture withefficient long-range connectivity and specialized bridging motifs, clarifyinghow local novelties consolidate into global musical form. By shiftingspecialization from parameter updates to interaction rules, shared memory, anddynamic consensus, MusicSwarm provides a compute- and data-efficient route tolong-horizon creative structure that is immediately transferable beyond musicto collaborative writing, design, and scientific discovery.</description>
      <author>example@mail.com (Markus J. Buehler)</author>
      <guid isPermaLink="false">2509.11973v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation</title>
      <link>http://arxiv.org/abs/2509.11885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper has been accepted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Brea-Depth的新框架，用于支气管镜检查中的单目深度估计，通过整合气道特定的几何先验，提高了实时导航精度和手术安全性。&lt;h4&gt;背景&lt;/h4&gt;单目深度估计在支气管镜检查中可以显著提高复杂分支气道中的实时导航精度和手术安全性。然而，现有的深度基础模型在支气管镜场景中缺乏解剖学意识，容易过度拟合局部纹理而非捕获全局气道结构，特别是在深度线索模糊和光照条件差的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确估计支气管镜检查中深度的方法，特别是提高模型对气道结构的理解和处理能力，从而产生更准确、鲁棒的3D气道重建。&lt;h4&gt;方法&lt;/h4&gt;提出Brea-Depth框架，整合气道特定几何先验到基础模型适应中。方法包括：1) 引入深度感知的CycleGAN，优化真实支气管镜图像与解剖数据中气道几何之间的转换；2) 引入气道结构感知损失，强制气道腔内的深度一致性，同时保持平滑过渡和结构完整性。&lt;h4&gt;主要发现&lt;/h4&gt;Brea-Depth通过整合解剖先验，增强了模型泛化能力，产生了更鲁棒、准确的3D气道重建。在收集的离体人肺数据集和公开的支气管镜数据集上，该方法在解剖深度保存方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;Brea-Depth通过整合气道特定的几何先验，有效解决了现有深度基础模型在支气管镜场景中缺乏解剖学意识的问题，提高了深度估计的准确性和鲁棒性，有望改善支气管镜检查中的实时导航精度和手术安全性。&lt;h4&gt;翻译&lt;/h4&gt;支气管镜检查中的单目深度估计可以显著提高复杂分支气道中的实时导航精度和手术安全性。最近的深度基础模型进展在内镜场景中显示出前景，但这些模型通常缺乏支气管镜中的解剖学意识，过度拟合局部纹理而非捕获全局气道结构，特别是在深度线索模糊和光照条件差的情况下。为解决这一问题，我们提出了Brea-Depth，一种新颖的框架，将气道特定的几何先验整合到基础模型适应中，用于支气管镜深度估计。我们的方法引入了深度感知的CycleGAN，优化真实支气管镜图像与解剖数据中气道几何之间的转换，有效弥合了域差距。此外，我们引入了气道结构感知损失，强制气道腔内的深度一致性，同时保持平滑过渡和结构完整性。通过整合解剖先验，Brea-Depth增强了模型泛化能力，产生了更鲁棒、准确的3D气道重建。为评估解剖学真实性，我们引入了气道深度结构评估，一种新的结构一致性指标。我们在收集的离体人肺数据集和公开的支气管镜数据集上验证了Brea-Depth，它在解剖深度保存方面优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决支气管镜检查中的单目深度估计问题。现有方法缺乏解剖学意识，过度拟合局部纹理而非捕获全局气道结构，尤其在模糊深度线索和不良光照条件下表现不佳。这个问题很重要，因为准确的深度估计能显著提高实时导航精度，增强复杂分支气道中干预措施的安全性，对靶向活检或局部药物输送等需要结构精确深度的任务至关重要，同时能改进3D气道重建质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有深度基础模型在支气管镜场景中的局限性，认识到需要针对特定解剖结构进行优化。他们借鉴了CycleGAN进行图像转换、U-Net架构进行编码解码、以及基础模型提供伪深度监督等现有技术。在此基础上，他们创新性地开发了深度感知的CycleGAN专门针对支气管镜场景优化，并引入气道结构感知损失函数确保深度预测符合解剖学先验。这种设计既利用了现有方法的优势，又针对性地解决了支气管镜特有的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将气道特定的几何先验知识整合到基础深度估计模型中，创建专门针对支气管镜场景的深度估计框架，确保生成的深度图不仅在像素级准确，而且在解剖学上一致。整体流程包括：1)创建几何准确的3D气道模型；2)实现深度感知CycleGAN，包含合成到真实和真实到合成两个转换分支；3)引入气道结构感知损失函数强制气道管腔内深度一致性；4)使用合成和真实数据混合训练，最终实现60FPS的实时深度估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)BREA-Depth框架，首个将气道几何整合到基础深度估计中的专门框架；2)深度感知CycleGAN，直接将深度整合到域转换中；3)气道结构感知损失，强制气道管腔内深度一致性；4)气道深度结构评估指标，关注解剖一致性而非仅像素级准确性。相比之前工作，不同之处在于：不依赖CT生成的简化数据；具有解剖学意识关注全局结构；在模糊条件下表现更好；保留气道几何结构；引入专门评估指标更全面评估实用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BREA-Depth通过整合气道特定的几何先验知识到深度估计基础模型中，首次实现了在支气管镜场景中既保持像素级准确性又确保解剖学一致性的深度预测，显著提升了复杂气道导航和3D重建的可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular depth estimation in bronchoscopy can significantly improvereal-time navigation accuracy and enhance the safety of interventions incomplex, branching airways. Recent advances in depth foundation models haveshown promise for endoscopic scenarios, yet these models often lack anatomicalawareness in bronchoscopy, overfitting to local textures rather than capturingthe global airway structure, particularly under ambiguous depth cues and poorlighting. To address this, we propose Brea-Depth, a novel framework thatintegrates airway-specific geometric priors into foundation model adaptationfor bronchoscopic depth estimation. Our method introduces a depth-awareCycleGAN, refining the translation between real bronchoscopic images and airwaygeometries from anatomical data, effectively bridging the domain gap. Inaddition, we introduce an airway structure awareness loss to enforce depthconsistency within the airway lumen while preserving smooth transitions andstructural integrity. By incorporating anatomical priors, Brea-Depth enhancesmodel generalization and yields more robust, accurate 3D airwayreconstructions. To assess anatomical realism, we introduce Airway DepthStructure Evaluation, a new metric for structural consistency. We validateBREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopicdataset, where it outperforms existing methods in anatomical depthpreservation.</description>
      <author>example@mail.com (Francis Xiatian Zhang, Emile Mackute, Mohammadreza Kasaei, Kevin Dhaliwal, Robert Thomson, Mohsen Khadem)</author>
      <guid isPermaLink="false">2509.11885v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</title>
      <link>http://arxiv.org/abs/2509.11772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Seg2Track-SAM2框架，结合预训练目标检测器、SAM2和新型Seg2Track模块，实现了无需微调且与检测器无关的多目标跟踪与分割(MOTS)系统，在基准测试中达到最先进性能并显著降低内存使用。&lt;h4&gt;背景&lt;/h4&gt;自主系统需要在动态环境中可靠运行，这要求具备强大的多目标跟踪(MOT)能力。虽然基础模型如SAM2在视频分割方面表现出强大的零样本泛化能力，但其直接应用于MOTS仍受身份管理和内存效率不足的限制。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够解决MOTS问题的框架，结合强大的零样本跟踪能力、增强的身份保持和高效的内存利用，同时不需要微调且与检测器无关。&lt;h4&gt;方法&lt;/h4&gt;Seg2Track-SAM2框架整合了预训练的目标检测器、SAM2和一个新的Seg2Track模块，用于处理轨道初始化、轨道管理和强化。该方法采用滑动窗口内存策略，可将内存使用减少高达75%，且性能下降可忽略不计。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI MOT和KITTI MOTS基准测试上，Seg2Track-SAM2实现了最先进(SOTA)的性能，在KITTI MOTS上汽车和行人类别总体排名第四，并在关联准确率(AssA)方面建立了新基准。滑动窗口内存策略可在资源受限条件下支持部署。&lt;h4&gt;结论&lt;/h4&gt;Seg2Track-SAM2通过结合强大的零样本跟踪、增强的身份保持和高效的内存利用，推动了MOTS的发展。&lt;h4&gt;翻译&lt;/h4&gt;自主系统需要在动态环境中可靠运行，这要求具备强大的多目标跟踪(MOT)能力。MOT确保了对象身份分配的一致性和空间 delineation 的精确性。基础模型(如SAM2)的最新进展已在视频分割方面表现出强大的零样本泛化能力，但它们直接应用于MOTS(多目标跟踪+分割)仍受身份管理和内存效率不足的限制。本研究介绍了Seg2Track-SAM2，一个结合了预训练目标检测器、SAM2和新型Seg2Track模块的框架，用于解决轨道初始化、轨道管理和强化问题。所提出的方法无需微调，且与检测器无关。在KITTI MOT和KITTI MOTS基准测试上的实验结果表明，Seg2Track-SAM2实现了最先进(SOTA)的性能，在KITTI MOTS上汽车和行人类别总体排名第四，同时在关联准确率(AssA)方面建立了新基准。此外，滑动窗口内存策略可将内存使用减少高达75%，且性能下降可忽略不计，支持在资源受限条件下的部署。这些结果证实，Seg2Track-SAM2通过结合强大的零样本跟踪、增强的身份保持和高效的内存利用，推动了MOTS的发展。代码可在https://github.com/hcmr-lab/Seg2Track-SAM2获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous systems require robust Multi-Object Tracking (MOT) capabilities tooperate reliably in dynamic environments. MOT ensures consistent objectidentity assignment and precise spatial delineation. Recent advances infoundation models, such as SAM2, have demonstrated strong zero-shotgeneralization for video segmentation, but their direct application to MOTS(MOT+Segmentation) remains limited by insufficient identity management andmemory efficiency. This work introduces Seg2Track-SAM2, a framework thatintegrates pre-trained object detectors with SAM2 and a novel Seg2Track moduleto address track initialization, track management, and reinforcement. Theproposed approach requires no fine-tuning and remains detector-agnostic.Experimental results on KITTI MOT and KITTI MOTS benchmarks show thatSeg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourthoverall in both car and pedestrian classes on KITTI MOTS, while establishing anew benchmark in association accuracy (AssA). Furthermore, a sliding-windowmemory strategy reduces memory usage by up to 75% with negligible performancedegradation, supporting deployment under resource constraints. These resultsconfirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shottracking, enhanced identity preservation, and efficient memory utilization. Thecode is available at https://github.com/hcmr-lab/Seg2Track-SAM2</description>
      <author>example@mail.com (Diogo Mendonça, Tiago Barros, Cristiano Premebida, Urbano J. Nunes)</author>
      <guid isPermaLink="false">2509.11772v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Igniting VLMs toward the Embodied Space</title>
      <link>http://arxiv.org/abs/2509.11766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WALL-OSS是一个端到端的具身基础模型，通过大规模多模态预训练实现了具身感知的视觉语言理解、强大的语言-动作关联和稳健的操作能力，解决了现有视觉语言模型在空间和具身理解方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;基础模型在语言和视觉方面取得了显著进展，但现有视觉语言模型在空间和具身理解方面仍然有限。将视觉语言模型转移到具身领域揭示了模态、预训练分布和训练目标之间的根本不匹配，使得动作理解和生成成为通往通用人工智能道路上的主要瓶颈。&lt;h4&gt;目的&lt;/h4&gt;介绍WALL-OSS模型，利用大规模多模态预训练实现具身感知的视觉语言理解、强大的语言-动作关联和稳健的操作能力，解决视觉语言模型到具身领域的转移问题。&lt;h4&gt;方法&lt;/h4&gt;采用紧密耦合的架构和多策略训练课程，实现统一跨层级思维链，在单一可微分框架内无缝统一指令推理、子目标分解和细粒度动作合成。&lt;h4&gt;主要发现&lt;/h4&gt;WALL-OSS在复杂长期操作上取得高成功率，展示强大的指令遵循能力和复杂理解推理能力，性能优于强大的基线模型。&lt;h4&gt;结论&lt;/h4&gt;WALL-OSS为从视觉语言模型到具身基础模型提供了一条可靠且可扩展的路径，解决了模态间不匹配和训练目标不一致的问题。&lt;h4&gt;翻译&lt;/h4&gt;虽然基础模型在语言和视觉方面显示出显著进展，但现有的视觉语言模型在空间和具身理解方面仍然有限。将视觉语言模型转移到具身领域揭示了模态、预训练分布和训练目标之间的根本不匹配，使得动作理解和生成成为通往通用人工智能道路上的主要瓶颈。我们引入了WALL-OSS，一个端到端的具身基础模型，它利用大规模多模态预训练实现(1)具身感知的视觉语言理解，(2)强大的语言-动作关联，和(3)稳健的操作能力。我们的方法采用紧密耦合的架构和多策略训练课程，实现了统一跨层级思维链——在单一可微分框架内无缝统一指令推理、子目标分解和细粒度动作合成。我们的结果表明，WALL-OSS在复杂的长期操作上取得高成功率，展示了强大的指令遵循能力、复杂理解和推理能力，并优于强大的基线模型，从而为从视觉语言模型到具身基础模型提供了一条可靠且可扩展的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models show remarkable progress in language and vision,existing vision-language models (VLMs) still have limited spatial andembodiment understanding. Transferring VLMs to embodied domains revealsfundamental mismatches between modalities, pretraining distributions, andtraining objectives, leaving action comprehension and generation as a centralbottleneck on the path to AGI.  We introduce WALL-OSS, an end-to-end embodied foundation model that leverageslarge-scale multimodal pretraining to achieve (1) embodiment-awarevision-language understanding, (2) strong language-action association, and (3)robust manipulation capability.  Our approach employs a tightly coupled architecture and multi-strategiestraining curriculum that enables Unified Cross-Level CoT-seamlessly unifyinginstruction reasoning, subgoal decomposition, and fine-grained action synthesiswithin a single differentiable framework.  Our results show that WALL-OSS attains high success on complex long-horizonmanipulations, demonstrates strong instruction-following capabilities, complexunderstanding and reasoning, and outperforms strong baselines, therebyproviding a reliable and scalable path from VLMs to embodied foundation models.</description>
      <author>example@mail.com (Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, Lucy Liang, Make Wang, Qian Wang, Roy Gan, Ryan Yu, Shalfun Li, Starrick Liu, Sylas Chen, Vincent Chen, Zach Xu)</author>
      <guid isPermaLink="false">2509.11766v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Uniqueness Theorem for Distributed Computation under Physical Constraint</title>
      <link>http://arxiv.org/abs/2509.11754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文在网络内计算等极端环境中，针对通信效率、有限内存和健壮可扩展性之间的三难困境，提出了一种基于逻辑必然性的解决方案。作者建立了严格的公理系统，证明了对于具有幂等合并操作符的计算，存在一个最优范式——自描述并行流（SDPF），并证明了该范式的收敛性、图灵完备性和最小性。这项工作类似于CAP定理，但提供了分布式计算流中不可避免性的唯一性定理。&lt;h4&gt;背景&lt;/h4&gt;传统的计算模型通常抽象了物理硬件限制，但在网络内计算等极端环境中，这些限制变得不可违反，导致在通信效率、有限内存和健壮可扩展性之间形成尖锐的三难困境。现有的分布式范式虽然在其预期领域很强大，但并非为这种严格的体系设计，因此面临根本性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决网络内计算等极端环境中面临的三难困境，即通信效率、有限内存和健壮可扩展性之间的冲突，寻找最优的分布式计算范式。&lt;h4&gt;方法&lt;/h4&gt;建立了一个严格的公理系统来形式化物理约束，并通过数学证明推导出最优范式。作者证明了对于具有幂等合并操作符的广泛计算类别，存在一个独特的最优范式，并分析了该范式的性质。&lt;h4&gt;主要发现&lt;/h4&gt;1) 对于具有幂等合并操作符的计算，存在一个独特的最优范式；2) 任何满足所建立公理的系统都必须收敛到自描述并行流（SDPF）这一单一标准形式；3) SDPF范式是收敛的、图灵完备的且是最小的；4) 类似于CAP定理，本文提供了一个唯一性定理，揭示了在物理定律下分布式计算流中什么是不可避免的。&lt;h4&gt;结论&lt;/h4&gt;解决极端环境中的计算三难困境需要转变视角，从寻求工程权衡到从逻辑必然性推导解决方案。自描述并行流（SDPF）是一个基于物理约束推导出的最优范式，它为分布式计算流提供了不可避免的设计原则。&lt;h4&gt;翻译&lt;/h4&gt;计算的基础模型通常抽象了物理硬件限制。然而，在网络内计算等极端环境中，这些限制变得不可违反的定律，在通信效率、有限内存和健壮可扩展性之间形成尖锐的三难困境。现有的分布式范式虽然在其预期领域很强大，但并非为这种严格的体系设计，因此面临根本性挑战。本文表明，解决这一三难困境需要转变视角——从寻求工程权衡到从逻辑必然性推导解决方案。我们建立了一个严格的公理系统，形式化了这些物理约束，并证明对于具有幂等合并操作符的广泛计算类别，存在一个独特的最优范式。任何满足这些公理的系统都必须收敛到一个单一的标准形式：自描述并行流（SDPF），这是一种纯数据中心的模型，其中无状态执行器处理携带自身控制逻辑的流。我们进一步证明这种独特范式是收敛的、图灵完备的且是最小的。就像CAP定理为分布式状态管理中不可能实现的事情建立了边界一样，我们的工作提供了一个建设性的对偶：一个唯一性定理，揭示了在物理定律下分布式计算流中什么是不可避免的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models of computation often abstract away physical hardwarelimitations. However, in extreme environments like In-Network Computing (INC),these limitations become inviolable laws, creating an acute trilemma amongcommunication efficiency, bounded memory, and robust scalability. Prevailingdistributed paradigms, while powerful in their intended domains, were notdesigned for this stringent regime and thus face fundamental challenges. Thispaper demonstrates that resolving this trilemma requires a shift in perspective- from seeking engineering trade-offs to deriving solutions from logicalnecessity. We establish a rigorous axiomatic system that formalizes thesephysical constraints and prove that for the broad class of computationsadmitting an idempotent merge operator, there exists a unique, optimalparadigm. Any system satisfying these axioms must converge to a single normalform: Self-Describing Parallel Flows (SDPF), a purely data-centric model wherestateless executors process flows that carry their own control logic. Wefurther prove this unique paradigm is convergent, Turing-complete, and minimal.In the same way that the CAP theorem established a boundary for what isimpossible in distributed state management, our work provides a constructivedual: a uniqueness theorem that reveals what is \textit{inevitable} fordistributed computation flows under physical law.</description>
      <author>example@mail.com (Zhiyuan Ren, Mingxuan Lu, Wenchi Cheng)</author>
      <guid isPermaLink="false">2509.11754v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications</title>
      <link>http://arxiv.org/abs/2509.11752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EchoCare是一个新型的超声基础模型，通过自监督学习在大型、多样化数据集EchoCareData上开发，能够有效整合多源数据学习超声表征，并在多种超声应用中表现出色。&lt;h4&gt;背景&lt;/h4&gt;真实临床环境中缺乏大型标记数据集，特定任务模型的泛化能力有限，这些因素阻碍了超声应用中可泛化临床AI模型的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的超声基础模型，能够有效整合多源数据学习超声表征，提高临床护理水平。&lt;h4&gt;方法&lt;/h4&gt;创建了EchoCareData数据集，包含450万张超声图像，来自5大洲23个国家，使用多种不同成像设备获取；开发了EchoCare模型，采用分层分类器，能够同时学习像素级和表征级特征；通过自监督学习方法训练模型；在10个代表性超声基准测试上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;EchoCare在10个不同诊断难度的代表性超声基准测试中优于最先进的比较模型，这些基准测试涵盖了疾病诊断、病灶分割、器官检测、标志点预测、定量回归、图像增强和报告生成；EchoCare仅需少量训练就能表现优异。&lt;h4&gt;结论&lt;/h4&gt;EchoCare提供了一个完全开放且可泛化的基础模型，可以促进各种临床超声应用中AI技术的发展，代码和预训练模型已公开发布，支持微调和本地适应，可扩展到更多应用。&lt;h4&gt;翻译&lt;/h4&gt;能够通过整合多源数据有效学习超声表征的人工智能在推进临床护理方面具有巨大潜力。然而，真实临床环境中大型标记数据集的稀缺以及特定任务模型有限的泛化能力，阻碍了超声应用中可泛化临床AI模型的发展。本研究提出了EchoCare，一个用于通用临床的新型超声基础模型，通过在策划的、公开可用的、大规模数据集EchoCareData上进行自监督学习开发。EchoCareData包含450万张超声图像，源自5大洲23个国家，通过多种不同的成像设备获取，因此包含了多中心、多设备和多种族的全队列人群。与采用现成视觉基础模型架构的先前研究不同，我们在EchoCare中引入了分层分类器，以实现像素级和表征级特征的联合学习，同时捕捉全局解剖上下文和局部超声特征。经过最少训练，EchoCare在10个具有不同诊断难度的代表性超声基准测试中优于最先进的比较模型，涵盖了疾病诊断、病灶分割、器官检测、标志点预测、定量回归、图像增强和报告生成。代码和预训练模型已公开发布，使EchoCare可用于微调和本地适应，支持扩展到更多应用。EchoCare提供了一个完全开放且可泛化的基础模型，以促进各种临床超声应用中AI技术的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI) that can effectively learn ultrasoundrepresentations by integrating multi-source data holds significant promise foradvancing clinical care. However, the scarcity of large labeled datasets inreal-world clinical environments and the limited generalizability oftask-specific models have hindered the development of generalizable clinical AImodels for ultrasound applications. In this study, we present EchoCare, a novelultrasound foundation model for generalist clinical use, developed viaself-supervised learning on our curated, publicly available, large-scaledataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,sourced from over 23 countries across 5 continents and acquired via a diverserange of distinct imaging devices, thus encompassing global cohorts that aremulti-center, multi-device, and multi-ethnic. Unlike prior studies that adoptoff-the-shelf vision foundation model architectures, we introduce ahierarchical classifier into EchoCare to enable joint learning of pixel-leveland representation-level features, capturing both global anatomical contextsand local ultrasound characteristics. With minimal training, EchoCareoutperforms state-of-the-art comparison models across 10 representativeultrasound benchmarks of varying diagnostic difficulties, spanning diseasediagnosis, lesion segmentation, organ detection, landmark prediction,quantitative regression, imaging enhancement and report generation. The codeand pretrained model are publicly released, rendering EchoCare accessible forfine-tuning and local adaptation, supporting extensibility to additionalapplications. EchoCare provides a fully open and generalizable foundation modelto boost the development of AI technologies for diverse clinical ultrasoundapplications.</description>
      <author>example@mail.com (Hongyuan Zhang, Yuheng Wu, Mingyang Zhao, Zhiwei Chen, Rebecca Li, Fei Zhu, Haohan Zhao, Xiaohua Yuan, Meng Yang, Chunli Qiu, Xiang Cong, Haiyan Chen, Lina Luan, Randolph H. L. Wong, Huai Liao, Colin A Graham, Shi Chang, Guowei Tao, Dong Yi, Zhen Lei, Nassir Navab, Sebastien Ourselin, Jiebo Luo, Hongbin Liu, Gaofeng Meng)</author>
      <guid isPermaLink="false">2509.11752v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>DRAG: Data Reconstruction Attack using Guided Diffusion</title>
      <link>http://arxiv.org/abs/2509.11724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于引导扩散的新型数据重建攻击方法，用于评估大型基础模型在分割推理环境下的隐私风险。&lt;h4&gt;背景&lt;/h4&gt;随着大型基础模型的兴起，分割推理已成为一种流行的计算范式，用于在轻量级边缘设备和云服务器之间部署模型，以解决数据隐私和计算成本问题。然而，大多数现有的数据重建攻击都集中在较小的CNN分类模型上，而基础模型在SI环境下的隐私风险在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一研究空白，论文旨在提出一种新型的数据重建攻击方法，以评估大型基础模型在SI环境下的隐私风险。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一种基于引导扩散的数据重建攻击方法，该方法利用在大型数据集上预训练的潜在扩散模型(LDM)中嵌入的丰富先验知识。该方法在LDM学习到的图像先验上进行迭代重建，能够从中间表示(IR)有效地生成高保真度图像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在从视觉基础模型的深层中间表示重建数据方面，该方法在定性和定量上都显著优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了在SI场景中需要为大型模型开发更强大的隐私保护机制。&lt;h4&gt;翻译&lt;/h4&gt;随着大型基础模型的兴起，分割推理已成为一种流行的计算范式，用于在轻量级边缘设备和云服务器之间部署模型，以解决数据隐私和计算成本问题。然而，大多数现有的数据重建攻击都集中在较小的CNN分类模型上，而基础模型在SI环境下的隐私风险在很大程度上尚未被探索。为了解决这一研究空白，我们提出了一种基于引导扩散的新型数据重建攻击方法，该方法利用在大型数据集上预训练的潜在扩散模型(LDM)中嵌入的丰富先验知识。我们的方法在LDM学习到的图像先验上进行迭代重建，能够有效地从中间表示(IR)生成高保真度图像，这些图像类似于原始数据。大量实验表明，在从视觉基础模型的深层中间表示重建数据方面，我们的方法在定性和定量上都显著优于最先进的方法。研究结果强调了在SI场景中需要为大型模型开发更强大的隐私保护机制。代码可在以下网址获取：https://github.com/ntuaislab/DRAG。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of large foundation models, split inference (SI) has emerged asa popular computational paradigm for deploying models across lightweight edgedevices and cloud servers, addressing data privacy and computational costconcerns. However, most existing data reconstruction attacks have focused onsmaller CNN classification models, leaving the privacy risks of foundationmodels in SI settings largely unexplored. To address this gap, we propose anovel data reconstruction attack based on guided diffusion, which leverages therich prior knowledge embedded in a latent diffusion model (LDM) pre-trained ona large-scale dataset. Our method performs iterative reconstruction on theLDM's learned image prior, effectively generating high-fidelity imagesresembling the original data from their intermediate representations (IR).Extensive experiments demonstrate that our approach significantly outperformsstate-of-the-art methods, both qualitatively and quantitatively, inreconstructing data from deep-layer IRs of the vision foundation model. Theresults highlight the urgent need for more robust privacy protection mechanismsfor large models in SI scenarios. Code is available at:https://github.com/ntuaislab/DRAG.</description>
      <author>example@mail.com (Wa-Kin Lei, Jun-Cheng Chen, Shang-Tse Chen)</author>
      <guid isPermaLink="false">2509.11724v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity with Pretrained Transformers (TabPFN) and Mamba-Based Models</title>
      <link>http://arxiv.org/abs/2509.11449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the author's preprint version of a paper accepted for  presentation at the 24th International Conference on Machine Learning and  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final  published version will appear in the official IEEE proceedings. Conference  site: https://www.icmla-conference.org/icmla25/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种深度表格学习框架，使用德州2017-2023年的真实碰撞数据预测电动汽车碰撞的严重程度。&lt;h4&gt;背景&lt;/h4&gt;研究使用德州(2017-2023)的真实碰撞数据，筛选后分析了23,301起电动汽车碰撞记录。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度表格学习框架来预测电动汽车碰撞的严重程度，并评估不同模型的性能。&lt;h4&gt;方法&lt;/h4&gt;使用XGBoost和Random Forest进行特征重要性分析，识别关键预测因素；应用SMOTEENN重新采样技术解决类别不平衡问题；比较TabPFN、MambaNet和MambaAttention三种深度表格模型。&lt;h4&gt;主要发现&lt;/h4&gt;TabPFN表现出强大的泛化能力，而MambaAttention在严重伤害案例分类中表现更优，归因于其基于注意力的特征重加权机制。&lt;h4&gt;结论&lt;/h4&gt;深度表格架构在改善碰撞严重程度预测方面具有潜力，能够在电动汽车碰撞背景下实现数据驱动的安全干预。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种深度表格学习框架，使用德州(2017-2023)的真实碰撞数据预测电动汽车碰撞的严重程度。在筛选出纯电动汽车后，分析了23,301起电动汽车碰撞记录。使用XGBoost和随机森林的特征重要性技术确定了交叉路口关系、首次有害事件、人员年龄、碰撞限速和星期几为主要预测因素，以及自动紧急制动等先进安全功能。为解决类别不平衡问题，应用了合成少数过采样技术和编辑最近邻(SMOTEENN)重新采样。对三种最先进的深度表格模型TabPFN、MambaNet和MambaAttention进行了严重程度预测的基准测试。虽然TabPFN表现出强大的泛化能力，但MambaAttention由于其基于注意力的特征重加权，在严重伤害案例分类中取得了优越的性能。这些发现强调了深度表格架构在改善碰撞严重程度预测方面的潜力，并能够在电动汽车碰撞背景下实现数据驱动的安全干预。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a deep tabular learning framework for predicting crashseverity in electric vehicle (EV) collisions using real-world crash data fromTexas (2017-2023). After filtering for electric-only vehicles, 23,301EV-involved crash records were analyzed. Feature importance techniques usingXGBoost and Random Forest identified intersection relation, first harmfulevent, person age, crash speed limit, and day of week as the top predictors,along with advanced safety features like automatic emergency braking. Toaddress class imbalance, Synthetic Minority Over-sampling Technique and EditedNearest Neighbors (SMOTEENN) resampling was applied. Three state-of-the-artdeep tabular models, TabPFN, MambaNet, and MambaAttention, were benchmarked forseverity prediction. While TabPFN demonstrated strong generalization,MambaAttention achieved superior performance in classifying severe injury casesdue to its attention-based feature reweighting. The findings highlight thepotential of deep tabular architectures for improving crash severity predictionand enabling data-driven safety interventions in EV crash contexts.</description>
      <author>example@mail.com (Shriyank Somvanshi, Pavan Hebli, Gaurab Chhetri, Subasish Das)</author>
      <guid isPermaLink="false">2509.11449v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Geometric Priors for Unaligned Scene Change Detection</title>
      <link>http://arxiv.org/abs/2509.11292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于几何先验的未对齐场景变化检测方法，解决了当前方法在大视角变化下匹配漂移或失败的问题。&lt;h4&gt;背景&lt;/h4&gt;未对齐场景变化检测旨在检测不同时间拍摄且没有假设视点对齐的图像对之间的场景变化。当前方法仅依赖2D视觉线索建立跨图像对应关系来辅助变化检测，但大视角变化会改变视觉观测，导致基于外观的匹配漂移或失败。&lt;h4&gt;目的&lt;/h4&gt;利用几何基础模型的几何先验来解决未对齐场景变化检测的核心挑战，包括可靠识别视觉重叠、稳健建立对应关系和明确遮挡检测。&lt;h4&gt;方法&lt;/h4&gt;提出一个无需训练的框架，将几何先验与视觉基础模型的强大表示相结合，实现视点不对齐情况下的可靠变化检测。&lt;h4&gt;主要发现&lt;/h4&gt;在PSCD、ChangeSim和PASLCD数据集上的广泛评估表明，该方法实现了优越且稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入几何先验，解决了未对齐场景变化检测中缺乏显式几何推理的关键且被忽视的限制。&lt;h4&gt;翻译&lt;/h4&gt;未对齐场景变化检测旨在检测在不同时间拍摄且不假设视点对齐的图像对之间的场景变化。为处理视角变化，当前方法仅依赖2D视觉线索建立跨图像对应关系以辅助变化检测。然而，大视角变化会改变视觉观测，导致基于外观的匹配漂移或失败。此外，监督仅限于来自小规模SCD数据集的2D变化掩码，限制了可泛化的多视图知识的学习，使得难以可靠识别视觉重叠和处理遮挡。这种缺乏显式几何推理代表了关键但被忽视的限制。在这项工作中，我们首次利用几何基础模型的几何先验来解决未对齐SCD的核心挑战，包括可靠识别视觉重叠、稳健建立对应关系和明确遮挡检测。基于这些先验，我们提出一个无需训练的框架，将其与视觉基础模型的强大表示相结合，实现视点不对齐情况下的可靠变化检测。通过在PSCD、ChangeSim和PASLCD数据集上的广泛评估，我们证明了我们的方法实现了优越且稳健的性能。我们的代码将在https://github.com/ZilingLiu/GeoSCD发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决'未对齐场景变化检测'问题，即在不同时间、不同视点拍摄的图像对之间检测场景变化。这个问题在现实中很重要，因为自动驾驶、无人机和移动机器人等应用中，图像往往因传感器位置、运动或环境因素而存在视点差异。现有方法大多假设视点对齐，限制了实际应用；同时，大视点变化下仅依赖2D视觉线索的方法难以建立可靠对应关系，导致性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析现有方法的局限性：它们仅依赖2D视觉线索，在大视点变化下易出现匹配漂移或失败；且受限于小规模数据集，难以学习多视图可泛化知识。作者借鉴了几何基础模型(GFM)从多视图图像恢复3D几何的能力，以及视觉基础模型SAM的零样本特性。基于这些，作者设计了两阶段框架：先用GFM建立几何理解，再引导变化掩码预测，无需训练即可实现跨数据集泛化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用几何基础模型提供的3D几何先验来处理视点未对齐问题。整体流程分为两阶段：1)几何先验生成模块：用GFM恢复深度图和相机参数，建立像素级对应关系，识别视觉重叠区域并检测遮挡；2)几何引导变化掩码预测模块：将图像输入SAM提取特征，基于几何对应关系生成初始变化提案，用遮挡掩码精炼，与SAM分割掩码匹配后融合得到最终结果。此外，还包含预处理步骤减轻光照变化影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次引入GFM几何先验解决未对齐SCD的核心挑战(识别重叠、建立对应、检测遮挡)；2)提出无需训练的框架，集成几何先验与视觉基础模型，消除对大规模标注数据的需求；3)在多个数据集上验证了方法的优越性和鲁棒性。相比之前工作：不同于传统视点对齐方法和仅依赖2D线索的未对齐方法，本文利用3D几何信息建立更可靠的对应关系；不同于其他未对齐方法(如光流、特征相关)，本文方法能明确检测遮挡且无需训练；相比零样本方法，本文能有效处理大视点变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次利用几何基础模型的几何先验，提出了一种无需训练的框架，有效解决了视点未对齐场景变化检测中的核心挑战，实现了在各种视点变化和场景类型下的鲁棒变化检测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unaligned Scene Change Detection aims to detect scene changes between imagepairs captured at different times without assuming viewpoint alignment. Tohandle viewpoint variations, current methods rely solely on 2D visual cues toestablish cross-image correspondence to assist change detection. However, largeviewpoint changes can alter visual observations, causing appearance-basedmatching to drift or fail. Additionally, supervision limited to 2D change masksfrom small-scale SCD datasets restricts the learning of generalizablemulti-view knowledge, making it difficult to reliably identify visual overlapsand handle occlusions. This lack of explicit geometric reasoning represents acritical yet overlooked limitation. In this work, we are the first to leveragegeometric priors from a Geometric Foundation Model to address the corechallenges of unaligned SCD, including reliable identification of visualoverlaps, robust correspondence establishment, and explicit occlusiondetection. Building on these priors, we propose a training-free framework thatintegrates them with the powerful representations of a visual foundation modelto enable reliable change detection under viewpoint misalignment. Throughextensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, wedemonstrate that our approach achieves superior and robust performance. Ourcode will be released at https://github.com/ZilingLiu/GeoSCD.</description>
      <author>example@mail.com (Ziling Liu, Ziwei Chen, Mingqi Gao, Jinyu Yang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.11292v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation</title>
      <link>http://arxiv.org/abs/2509.11197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DreamNav是一种新的零样本视觉语言导航方法，通过改进感知、规划和预测能力，解决了现有方法依赖昂贵感知和被动理解的问题，实现了更高效、更语义一致的导航。&lt;h4&gt;背景&lt;/h4&gt;Vision-and-Language Navigation in Continuous Environments (VLN-CE)是具身机器人的核心能力，将语言指令与现实世界的感知和控制联系起来。大规模预训练基础模型被用作感知、推理和动作的共享先验，实现了无需任务特定训练的零样本VLN。&lt;h4&gt;目的&lt;/h4&gt;解决现有零样本VLN方法依赖昂贵感知和被动场景理解、控制简化为点级选择、部署成本高、动作语义不一致、规划缺乏前瞻性的问题。&lt;h4&gt;方法&lt;/h4&gt;DreamNav专注于三个方面：1) EgoView Corrector减少感知成本，对齐视角并稳定以自我为中心的感知；2) Trajectory Predictor替代点级动作，倾向于全局轨迹级规划，更好地与指令语义对齐；3) Imagination Predictor实现前瞻性和长期规划，赋予代理主动思考能力。&lt;h4&gt;主要发现&lt;/h4&gt;在VLN-CE和真实世界测试中，DreamNav建立了新的零样本最先进水平，在SR和SPL指标上分别比最强的以自我为中心的基线高出7.49%和18.15%。&lt;h4&gt;结论&lt;/h4&gt;DreamNav是第一个统一轨迹级规划和主动想象并仅使用以自我为中心输入的零样本VLN方法，显著提升了性能并降低了部署成本。&lt;h4&gt;翻译&lt;/h4&gt;Vision-and-Language Navigation in Continuous Environments (VLN-CE)将语言指令与现实世界中的感知和控制联系起来，是具身机器人的核心能力。最近，大规模预训练基础模型被用作感知、推理和动作的共享先验，实现了无需任务特定训练的零样本VLN。然而，现有零样本VLN方法依赖于昂贵的感知和被动场景理解，导致控制简化为点级选择。因此，这些方法部署成本高，动作语义不一致，且规划缺乏前瞻性。为解决这些问题，我们提出了DreamNav，专注于以下三个方面：(1)为减少感知成本，我们的EgoView Corrector对齐视角并稳定以自我为中心的感知；(2)替代点级动作，我们的Trajectory Predictor倾向于全局轨迹级规划，更好地与指令语义对齐；(3)为实现前瞻性和长期规划，我们提出了Imagination Predictor，赋予代理主动思考能力。在VLN-CE和真实世界测试中，DreamNav建立了新的零样本最先进水平，在SR和SPL指标上分别比最强的以自我为中心的基线高出7.49%和18.15%。据我们所知，这是第一个统一轨迹级规划和主动想象并仅使用以自我为中心输入的零样本VLN方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是零样本视觉语言导航(VLN-CE)中的三个核心问题：高成本感知、短视规划和语义不匹配。这些问题很重要，因为VLN-CE是具身机器人的关键能力，能让机器人根据语言指令在真实环境中导航。高成本感知限制了实际应用，短视规划导致机器人缺乏长远视野，语义不匹配则使导航决策与人类意图不符，这些都阻碍了机器人在复杂现实环境中的有效导航。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类导航行为中获得灵感，人类会构建连贯轨迹并主动想象未来场景。他们将问题分解为感知成本、规划前瞻性和语义对齐三个方面，针对性地设计解决方案。作者借鉴了扩散策略框架在机器人操作中的应用，利用预训练多模态模型作为认知核心，采用可控世界模型进行视觉预测，并借鉴FastSAM等技术用于可行走区域检测。整体设计是一个分层框架，包含视角校正、轨迹预测、想象预测和导航管理四个主要组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过轨迹级规划和主动想象模仿人类导航行为，构建连贯轨迹并预见未来场景，而非被动理解和反应。整体流程：1)接收第一人称RGB-D观察和语言指令；2)EgoView Corrector校正视角方向；3)Trajectory Predictor生成候选轨迹并过滤；4)Imagination Predictor将候选轨迹转换为未来场景的语义描述；5)Navigation Manager选择最佳轨迹并监控执行；6)系统持续调整和优化导航策略，形成闭环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)EgoView Corrector解决第一人称视角的方向错误，仅用低成本输入；2)Trajectory Predictor采用轨迹级而非点级决策；3)Imagination Predictor将被动理解转为主动想象；4)统一框架整合轨迹规划和主动想象。相比之前工作，DreamNav摒弃了高成本的全景感知，避免了点级决策的短视性，通过文本而非像素级输出降低成本，并在真实测试中实现了SR和SPL指标分别提升7.49%和18.15%的性能突破。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DreamNav首次统一了轨迹级规划和主动想象，仅使用第一人称输入就在零样本视觉语言导航中实现了最先进性能，解决了现有方法的高成本、短视和语义不匹配问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-Language Navigation in Continuous Environments (VLN-CE), whichlinks language instructions to perception and control in the real world, is acore capability of embodied robots. Recently, large-scale pretrained foundationmodels have been leveraged as shared priors for perception, reasoning, andaction, enabling zero-shot VLN without task-specific training. However,existing zero-shot VLN methods depend on costly perception and passive sceneunderstanding, collapsing control to point-level choices. As a result, they areexpensive to deploy, misaligned in action semantics, and short-sighted inplanning. To address these issues, we present DreamNav that focuses on thefollowing three aspects: (1) for reducing sensory cost, our EgoView Correctoraligns viewpoints and stabilizes egocentric perception; (2) instead ofpoint-level actions, our Trajectory Predictor favors global trajectory-levelplanning to better align with instruction semantics; and (3) to enableanticipatory and long-horizon planning, we propose an Imagination Predictor toendow the agent with proactive thinking capability. On VLN-CE and real-worldtests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming thestrongest egocentric baseline with extra information by up to 7.49\% and18.15\% in terms of SR and SPL metrics. To our knowledge, this is the firstzero-shot VLN method to unify trajectory-level planning and active imaginationwhile using only egocentric inputs.</description>
      <author>example@mail.com (Yunheng Wang, Yuetong Fang, Taowen Wang, Yixiao Feng, Yawen Tan, Shuning Zhang, Peiran Liu, Yiding Ji, Renjing Xu)</author>
      <guid isPermaLink="false">2509.11197v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos</title>
      <link>http://arxiv.org/abs/2509.11063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了Organoid Tracker平台，基于SAM2模型实现肾类器官时空显微镜视频的自动分析和定量评估，无需编程专业知识即可获取详细的定量指标，为肾脏疾病研究和药物发现提供强大工具。&lt;h4&gt;背景&lt;/h4&gt;类器官模型的最新进展改变了人类肾脏疾病机制研究和药物发现的方式，使研究可扩展、经济高效且无需牺牲动物。&lt;h4&gt;目的&lt;/h4&gt;开发一种优化的肾脏类器官平台，用于多囊肾病(PKD)的高效筛选，并解决现有手动分析方法仅限于粗略分类的问题。&lt;h4&gt;方法&lt;/h4&gt;开发了Organoid Tracker，一个基于图形用户界面(GUI)的平台，采用模块化插件架构，基于Segment Anything Model 2 (SAM2)实现零样本分割和时空显微镜视频的自动分析。&lt;h4&gt;主要发现&lt;/h4&gt;Organoid Tracker可以量化关键指标，如囊肿形成率、生长速度和形态变化，同时生成综合报告，提供有价值的像素级和纵向信息。&lt;h4&gt;结论&lt;/h4&gt;Organoid Tracker提供了一个可扩展的开源框架，为改善和加速肾脏发育、PKD建模和治疗发现研究提供了强大解决方案，平台已作为开源软件公开可用。&lt;h4&gt;翻译&lt;/h4&gt;类器官模型的最新进展通过实现可扩展、经济高效且无需牺牲动物的研究，彻底改变了人类肾脏疾病机制研究和药物发现。我们在此介绍了一种为多囊肾病(PKD)高效筛选而优化的肾脏类器官平台。虽然这些系统生成丰富的时空显微镜视频数据集，但当前的手工分析方法仍仅限于粗略分类（如命中与非命中），常常遗漏有价值的像素级和纵向信息。为帮助克服这一瓶颈，我们开发了Organoid Tracker，一个采用模块化插件架构设计的图形用户界面(GUI)平台，使研究人员无需编程专业知识即可提取详细的定量指标。基于前沿视觉基础模型Segment Anything Model 2 (SAM2)构建，Organoid Tracker实现了时空显微镜视频的零样本分割和自动分析。它量化了囊肿形成率、生长速度和形态变化等关键指标，同时生成综合报告。通过提供可扩展的开源框架，Organoid Tracker为改善和加速肾脏发育、PKD建模和治疗发现研究提供了强大解决方案。该平台作为开源软件可在https://github.com/hrlblab/OrganoidTracker公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in organoid models have revolutionized the study of humankidney disease mechanisms and drug discovery by enabling scalable,cost-effective research without the need for animal sacrifice. Here, we presenta kidney organoid platform optimized for efficient screening in polycystickidney disease (PKD). While these systems generate rich spatial-temporalmicroscopy video datasets, current manual approaches to analysis remain limitedto coarse classifications (e.g., hit vs. non-hit), often missing valuablepixel-level and longitudinal information. To help overcome this bottleneck, wedeveloped Organoid Tracker, a graphical user interface (GUI) platform designedwith a modular plugin architecture, which empowers researchers to extractdetailed, quantitative metrics without programming expertise. Built on thecutting-edge vision foundation model Segment Anything Model 2 (SAM2), OrganoidTracker enables zero-shot segmentation and automated analysis ofspatial-temporal microscopy videos. It quantifies key metrics such as cystformation rate, growth velocity, and morphological changes, while generatingcomprehensive reports. By providing an extensible, open-source framework,Organoid Tracker offers a powerful solution for improving and acceleratingresearch in kidney development, PKD modeling, and therapeutic discovery. Theplatform is publicly available as open-source software athttps://github.com/hrlblab/OrganoidTracker.</description>
      <author>example@mail.com (Xiaoyu Huang, Lauren M Maxson, Trang Nguyen, Cheng Jack Song, Yuankai Huo)</author>
      <guid isPermaLink="false">2509.11063v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Deep Reinforcement Learning-Assisted Component Auto-Configuration of Differential Evolution Algorithm for Constrained Optimization: A Foundation Model</title>
      <link>http://arxiv.org/abs/2509.11016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SuperDE的新型框架，利用深度强化学习自动配置差分进化算法的组件，以解决约束优化问题。SuperDE作为一个基础模型，能够根据实时进化动态调整算法配置，并通过元学习实现零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;尽管人们努力设计高性能的进化算法，但由于现实问题的动态性和不断演变的特性，它们的适应性仍然有限。'无免费午餐'定理表明没有单一算法能在所有问题上都表现最佳。现有的在线适应方法通常存在效率低下、收敛性弱以及在约束优化问题上的泛化能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，作者引入了一个新的框架，用于在差分进化算法中自动化组件配置，以处理约束优化问题，该框架由深度强化学习提供支持。&lt;h4&gt;方法&lt;/h4&gt;作者提出了SuperDE，这是一个基础模型，可以根据实时进化动态配置差分进化算法的进化组件。通过元学习在各种约束优化问题上进行离线训练，SuperDE能够以零样本的方式为未见问题推荐每代的最优配置。利用双深度Q网络，SuperDE能够根据优化过程中不断变化的种群状态调整其配置策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SuperDE在基准测试套件上显著优于现有的最先进算法，实现了卓越的泛化和优化性能。&lt;h4&gt;结论&lt;/h4&gt;SuperDE通过结合深度强化学习和差分进化算法，为解决约束优化问题提供了一种有效的方法，能够动态调整算法参数以适应不同的问题特性。&lt;h4&gt;翻译&lt;/h4&gt;尽管人们付出了巨大努力来手动设计高性能的进化算法，但由于现实问题的动态和不断演变的特性，它们的适应性仍然有限。'无免费午餐'定理强调，没有单一算法能在所有问题上都表现最优。虽然已经提出了在线适应方法，但它们通常效率低下、收敛性弱，并且在约束优化问题上泛化能力有限。为了解决这些挑战，我们引入了一个新的框架，用于在差分进化算法中自动化组件配置以解决约束优化问题，该框架由深度强化驱动。具体来说，我们提出了SuperDE，这是一个基础模型，能够根据实时进化动态配置差分进化的组件。通过在各种约束优化问题上进行元学习离线训练，SuperDE能够以零样本的方式为未见问题推荐每代的最优配置。利用双深度Q网络，SuperDE能够根据优化过程中不断变化的种群状态调整其配置策略。实验结果表明，SuperDE在基准测试套件上显著优于现有的最先进算法，实现了卓越的泛化和优化性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant efforts to manually design high-performance evolutionaryalgorithms, their adaptability remains limited due to the dynamic andever-evolving nature of real-world problems. The "no free lunch" theoremhighlights that no single algorithm performs optimally across all problems.While online adaptation methods have been proposed, they often suffer frominefficiency, weak convergence, and limited generalization on constrainedoptimization problems (COPs).  To address these challenges, we introduce a novel framework for automatedcomponent configuration in Differential Evolution (DE) algorithm to addressCOPs, powered by Deep Reinforcement Learning (DRL). Specifically, we proposeSuperDE, a foundation model that dynamically configures DE's evolutionarycomponents based on real-time evolution. Trained offline through meta-learningacross a wide variety of COPs, SuperDE is capable of recommending optimalper-generation configurations for unseen problems in a zero-shot manner.Utilizing a Double Deep Q-Network (DDQN), SuperDE adapts its configurationstrategies in response to the evolving population states during optimization.Experimental results demonstrate that SuperDE significantly outperformsexisting state-of-the-art algorithms on benchmark test suites, achievingsuperior generalization and optimization performance.</description>
      <author>example@mail.com (Xu Yang, Rui Wang, Kaiwen Li, Wenhua Li, Ling Wang)</author>
      <guid isPermaLink="false">2509.11016v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation</title>
      <link>http://arxiv.org/abs/2509.10919v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种紧凑的元数据感知专家混合掩码自编码器（MoE-MAE），该模型仅具有250万参数，但性能可与更大规模模型相媲美，证明了元数据感知预训练在地球观测领域的有效性。&lt;h4&gt;背景&lt;/h4&gt;地球观测领域最近的发展集中在大型基础模型上，但这些模型计算成本高，限制了其在下游任务中的可访问性和重用性。&lt;h4&gt;目的&lt;/h4&gt;研究紧凑架构作为实现小型通用地球观测模型的实用途径。&lt;h4&gt;方法&lt;/h4&gt;提出了一个只有250万参数的元数据感知专家混合掩码自编码器（MoE-MAE）。该模型结合了稀疏专家路由与地理时间条件，同时结合了图像数据以及纬度/经度和季节/日循环编码。研究者在BigEarthNet-Landsat数据集上预训练MoE-MAE，并使用线性探针评估其冻结编码器的嵌入结果。&lt;h4&gt;主要发现&lt;/h4&gt;尽管模型规模小，但它能够与更大的架构竞争，这表明元数据感知预训可以提高迁移效率和标签效率。在缺乏显式元数据的EuroSAT-Landsat数据集上的评估显示，与拥有数亿参数的模型相比，仍然观察到具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;紧凑的、元数据感知的MoE-MAE是未来地球观测基础模型的高效且可扩展的一步。&lt;h4&gt;翻译&lt;/h4&gt;最近的地球观测进展主要集中在大型基础模型上。然而，这些模型计算成本高昂，限制了它们在下游任务中的可访问性和重用。在这项工作中，我们研究紧凑架构作为迈向小型通用地球观测模型的实用途径。我们提出了一个只有250万参数的元数据感知专家混合掩码自编码器（MoE-MAE）。该模型结合了稀疏专家路由与地理时间条件，整合了图像数据以及纬度/经度和季节/日循环编码。我们在BigEarthNet-Landsat数据集上预训练MoE-MAE，并使用线性探针评估其冻结编码器的嵌入结果。尽管规模小，该模型能与更大的架构竞争，这表明元数据感知预训练提高了迁移效率和标签效率。为了进一步评估泛化能力，我们在缺乏显式元数据的EuroSAT-Landsat数据集上进行了评估，与拥有数亿参数的模型相比，仍然观察到具有竞争力的性能。这些结果表明，紧凑的、元数据感知的MoE-MAE是未来地球观测基础模型的高效且可扩展的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Earth Observation have focused on large-scale foundationmodels. However, these models are computationally expensive, limiting theiraccessibility and reuse for downstream tasks. In this work, we investigatecompact architectures as a practical pathway toward smaller general-purpose EOmodels. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routingwith geo-temporal conditioning, incorporating imagery alongsidelatitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAEon the BigEarthNet-Landsat dataset and evaluate embeddings from its frozenencoder using linear probes. Despite its small size, the model competes withmuch larger architectures, demonstrating that metadata-aware pretrainingimproves transfer and label efficiency. To further assess generalization, weevaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, andstill observe competitive performance compared to models with hundreds ofmillions of parameters. These results suggest that compact, metadata-awareMoE-MAEs are an efficient and scalable step toward future EO foundation models.</description>
      <author>example@mail.com (Mohanad Albughdadi)</author>
      <guid isPermaLink="false">2509.10919v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Nav-R1: Reasoning and Navigation in Embodied Scenes</title>
      <link>http://arxiv.org/abs/2509.10884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Nav-R1，一个用于具身导航的基础模型，通过统一推理框架解决了现有方法在推理轨迹不连贯、不稳定以及难以平衡长时程语义推理与低延迟控制方面的问题。&lt;h4&gt;背景&lt;/h4&gt;具身导航需要智能体在复杂3D环境中整合感知、推理和行动，但现有方法常因推理轨迹不连贯和不稳定而影响泛化能力，且难以平衡长时程语义推理与低延迟控制以实现实时导航。&lt;h4&gt;目的&lt;/h4&gt;解决具身导航中推理不连贯、不稳定以及语义推理与控制平衡困难的问题，提高智能体在不同环境中的导航性能。&lt;h4&gt;方法&lt;/h4&gt;1) 构建Nav-CoT-110K大规模分步思维链数据集实现冷启动初始化；2) 设计基于GRPO的强化学习框架，包含格式、理解和导航三种互补奖励；3) 引入'Fast-in-Slow'推理范式，将 deliberative 语义推理与低延迟反应控制解耦。&lt;h4&gt;主要发现&lt;/h4&gt;Nav-R1在具身AI基准测试中显著优于基线模型，推理和导航性能平均提升超过8%；在移动机器人上的实际部署验证了其在有限机载资源下的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Nav-R1通过统一推理框架和创新的训练方法，有效解决了具身导航中的关键挑战，实现了高效且连贯的导航性能。&lt;h4&gt;翻译&lt;/h4&gt;具身导航需要智能体在复杂3D环境中整合感知、推理和行动以实现稳健的交互。现有方法常因推理轨迹不连贯和不稳定而阻碍了跨不同环境的泛化能力，并且难以平衡长时程语义推理与低延迟控制以实现实时导航。为应对这些挑战，我们提出Nav-R1，一个统一具身环境中推理的基础模型。我们首先构建Nav-CoT-110K，一个用于具身任务的大规模分步思维链数据集，实现了具有结构化推理的冷启动初始化。在此基础上，我们设计了一个基于GRPO的强化学习框架，包含格式、理解和导航三种互补奖励，以提高结构遵循性、语义基础和路径保真度。此外，我们引入了'Fast-in-Slow'推理范式，将 deliberative 语义推理与低延迟反应控制解耦，实现高效且连贯的导航。在具身AI基准上的广泛评估表明，Nav-R1始终优于强大的基线模型，推理和导航性能平均提升超过8%。在移动机器人上的实际部署进一步验证了其在有限机载资源下的鲁棒性。代码：https://github.com/AIGeeksGroup/Nav-R1。网站：https://aigeeksgroup.github.io/Nav-R1。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决具身导航中的两个关键问题：一是现有方法存在推理轨迹不连贯和不稳定的问题，影响模型在不同环境中的泛化能力；二是难以平衡长距离语义推理与低延迟控制以实现实时导航。这些问题在现实中非常重要，因为具身导航是服务机器人、增强现实助手等智能系统在复杂3D环境中执行物体搜索、指令跟随等任务的核心能力，直接关系到这些系统在现实世界中的实用性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后设计了一个分阶段的解决方案：首先构建Nav-CoT-110K数据集进行冷启动初始化，然后使用基于GRPO的强化学习框架进行优化，最后提出Fast-in-Slow双系统推理范式。该方法借鉴了人类认知科学中的双系统理论（快速直觉系统1和慢速理性系统2），以及大型语言模型中常用的Chain-of-Thought推理技术和GRPO强化学习方法，同时整合了多个现有3D视觉语言数据集来构建新的高质量数据集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将语义推理与实时导航控制解耦，通过双系统设计实现高效且连贯的导航，同时使用结构化的思维链数据确保推理质量。整体实现流程分为四个阶段：1) 使用CoT数据引擎构建Nav-CoT-110K数据集，通过视觉语言模型生成结构化推理轨迹并过滤；2) 冷启动阶段，使用该数据集进行监督微调，初始化模型生成结构化推理-行动序列；3) 强化学习阶段，应用三种互补奖励函数（格式、理解和导航奖励）进行GRPO优化；4) 实现Fast-in-Slow双系统，慢系统处理长期语义推理，快系统执行短期反应控制，两者异步协调工作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) Nav-CoT-110K大型数据集，提供结构化思维链轨迹；2) 三种互补奖励机制，平衡结构一致性、语义理解和导航准确性；3) Fast-in-Slow双系统推理范式，解耦长期推理与短期控制；4) 结合模拟训练与真实世界部署。相比之前工作，Nav-R1不将语义推理和导航视为分离问题，而是在统一框架中解决；双系统设计比单一系统更好地平衡了语义连贯性和实时响应；多奖励机制提供了更全面的训练信号；结合真实世界数据提高了模型泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Nav-R1通过结构化思维链数据、多奖励强化学习和双系统推理范式，显著提升了具身导航中的推理连贯性和实时导航性能，实现了语义理解与行动执行的有效平衡。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied navigation requires agents to integrate perception, reasoning, andaction for robust interaction in complex 3D environments. Existing approachesoften suffer from incoherent and unstable reasoning traces that hindergeneralization across diverse environments, and difficulty balancinglong-horizon semantic reasoning with low-latency control for real-timenavigation. To address these challenges, we propose Nav-R1, an embodiedfoundation model that unifies reasoning in embodied environments. We firstconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought(CoT) for embodied tasks, which enables cold-start initialization withstructured reasoning. Building on this foundation, we design a GRPO-basedreinforcement learning framework with three complementary rewards: format,understanding, and navigation, to improve structural adherence, semanticgrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slowreasoning paradigm, decoupling deliberate semantic reasoning from low-latencyreactive control for efficient yet coherent navigation. Extensive evaluationson embodied AI benchmarks demonstrate that Nav-R1 consistently outperformsstrong baselines, with over 8% average improvement in reasoning and navigationperformance. Real-world deployment on a mobile robot further validates itsrobustness under limited onboard resources. Code:https://github.com/AIGeeksGroup/Nav-R1. Website:https://aigeeksgroup.github.io/Nav-R1.</description>
      <author>example@mail.com (Qingxiang Liu, Ting Huang, Zeyu Zhang, Hao Tang)</author>
      <guid isPermaLink="false">2509.10884v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research</title>
      <link>http://arxiv.org/abs/2509.10790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Transformer已成为多种先进模型的基础，但其故障条件下的鲁棒性研究不足。GoldenTransformer是一个模块化且可扩展的故障注入框架，用于评估大型语言模型对硬件故障的恢复能力。&lt;h4&gt;背景&lt;/h4&gt;Transformers在自然语言处理、计算机视觉和机器学习领域广泛应用，但这些模型在故障条件下的鲁棒性研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;开发GoldenTransformer框架，用于评估大型语言模型对诱导硬件故障的恢复能力，并提供一个统一的平台来测试不同类型的故障影响。&lt;h4&gt;方法&lt;/h4&gt;GoldenTransformer是一个基于Python的统一平台，可以向预训练的transformer模型注入多种类型的故障，如权重损坏、激活注入和注意力级别干扰。该框架基于PyTorch和HuggingFace Transformers构建，支持实验可重复性、指标记录和可视化功能。&lt;h4&gt;主要发现&lt;/h4&gt;通过在transformer的多个逻辑和结构点进行受控故障注入，GoldenTransformer能够帮助研究人员和从业者分析模型鲁棒性，指导现实世界LLM应用中的可靠系统设计。&lt;h4&gt;结论&lt;/h4&gt;GoldenTransformer解决了大型transformer架构的独特挑战，包括结构复杂性、潜在依赖性和非均匀层定义等问题，为模型鲁棒性分析提供了重要工具。&lt;h4&gt;翻译&lt;/h4&gt;Transformers已成为自然语言处理、计算机视觉和其他机器学习领域中各种先进模型的基础。尽管它们被广泛部署，但这些模型在故障条件下的鲁棒性仍然研究不足。我们提出了GoldenTransformer，这是一个模块化和可扩展的故障注入框架，旨在评估大型语言模型对诱导硬件故障的恢复能力。GoldenTransformer提供了一个基于Python的统一平台，可以向预训练的基于transformer的模型注入多种类型的故障，如权重损坏、激活注入和注意力级别干扰。受DNN的GoldenEye模拟器启发，我们的框架专注于处理大型transformer架构的独特挑战，包括结构复杂性、潜在依赖性和非均匀层定义等考虑因素。GoldenTransformer基于PyTorch和HuggingFace Transformers构建，开箱即用地支持实验可重复性、指标记录和可视化。我们详细介绍了GoldenTransformer的技术设计和使用方法，并通过分类和生成任务上的几个示例实验进行了演示。通过能够在transformer的多个逻辑和结构点进行受控故障注入，GoldenTransformer为研究人员和从业者提供了有价值的工具，用于模型鲁棒性分析和指导现实世界LLM应用中的可靠系统设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have become the foundation for a wide range ofstate--of--the--art models across natural language processing, computer vision,and other machine learning domains. Despite their widespread deployment, therobustness of these models under fault conditions remains underexplored. Wepresent GoldenTransformer, a modular and extensible fault injection frameworkdesigned to evaluate the resiliency of Large Language Models to inducedhardware faults. GoldenTransformer offers a unified Python-based platform forinjecting diverse classes of faults--such as weight corruption, activationinjections, and attention--level disruptions--into pretrainedtransformer--based models. Inspired by the GoldenEye simulator for DNNs, ourframework focuses on the unique challenges of working with large transformerarchitectures, including considerations such as structural complexity, latentdependencies, and nonuniform layer definitions. GoldenTransformer is built atopPyTorch and HuggingFace Transformers, and it supports experimentreproducibility, metric logging, and visualization out of the box. We detailthe technical design and use of GoldenTransformer and demonstrate throughseveral example experiments on classification and generation tasks. By enablingcontrolled injection of faults at multiple logical and structural points in atransformer, GoldenTransformer offers researchers and practitioners a valuabletool for model robustness analysis and for guiding dependable system design inreal-world LLM applications.</description>
      <author>example@mail.com (Luke Howard)</author>
      <guid isPermaLink="false">2509.10790v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Medical Vision Foundation Models for Volumetric Medical Image Segmentation via Active Learning and Selective Semi-supervised Fine-tuning</title>
      <link>http://arxiv.org/abs/2509.10784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 5 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种主动无源域适应(ASFDA)方法，用于高效地将医学视觉基础模型(Med-VFMs)适应到目标域，用于体积医学图像分割任务。该方法通过主动学习选择最有信息量的样本进行微调，无需访问源预训练样本，从而以最小的选择预算最大化模型性能。&lt;h4&gt;背景&lt;/h4&gt;医学视觉基础模型(Med-VFMs)通过大量未标注图像的自监督预训练学习，具有解释医学图像的优越能力。目前，为了提高它们在适应性下游评估（特别是分割任务）中的性能，通常从目标域随机选择少量样本进行微调。然而，缺乏探索如何高效适应Med-VFMs以达到目标域最佳性能的研究。&lt;h4&gt;目的&lt;/h4&gt;设计一种高效的微调方法，通过选择信息量最大的样本来最大化Med-VFMs在目标域上的适应性能。&lt;h4&gt;方法&lt;/h4&gt;提出主动无源域适应(ASFDA)方法，采用新颖的主动学习策略，通过两个查询指标选择最有信息量的样本：多样化的知识差异(DKD)测量源-目标知识差距和域内多样性，利用预训练知识指导查询源不相似和语义多样的样本；解剖分割难度(ASD)通过自适应测量前景区域的预测熵来评估解剖结构分割的难度。此外，采用选择性半监督微调，通过从未查询的样本中识别高可靠性样本来提高微调的性能和效率。&lt;h4&gt;主要发现&lt;/h4&gt;DKD和ASD两个查询指标能够有效选择最有价值的样本进行微调，提高模型在目标域上的性能，无需访问源预训练样本即可实现最优适应。&lt;h4&gt;结论&lt;/h4&gt;ASFDA方法能够高效地将Med-VFMs适应到目标域，用于体积医学图像分割，通过主动学习选择最有信息量的样本，实现最优性能。&lt;h4&gt;翻译&lt;/h4&gt;医学视觉基础模型(Med-VFMs)由于通过大量未标注图像进行自监督预训练所获得的知识，具有解释医学图像的优越能力。为了提高它们在适应性下游评估中的性能，特别是分割任务，通常从目标域随机选择少量样本进行微调。然而，缺乏探索如何高效适应Med-VFMs以达到目标域最佳性能的研究。因此，迫切需要设计一种高效的微调方法，通过选择信息量最大的样本来最大化Med-VFMs在目标域上的适应性能。为此，我们提出了一种主动无源域适应(ASFDA)方法，用于高效地将Med-VFMs适应到目标域，用于体积医学图像分割。该ASFDA采用一种新颖的主动学习(AL)方法，从目标域中选择信息量最大的样本进行微调，无需访问源预训练样本，从而以最小的选择预算最大化其性能。在该AL方法中，我们设计了一种主动测试时间样本查询策略，通过两个查询指标（包括多样化的知识差异DKD和解剖分割难度ASD）从目标域中选择样本。DKD旨在测量源-目标知识差距和域内多样性，它利用预训练知识指导从目标域中查询源不相似和语义多样的样本。ASD旨在通过自适应测量前景区域的预测熵来评估解剖结构分割的难度。此外，我们的ASFDA方法采用选择性半监督微调，通过从未查询的样本中识别高可靠性样本来提高微调的性能和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical Vision Foundation Models (Med-VFMs) have superior capabilities ofinterpreting medical images due to the knowledge learned from self-supervisedpre-training with extensive unannotated images. To improve their performance onadaptive downstream evaluations, especially segmentation, a few samples fromtarget domains are selected randomly for fine-tuning them. However, there lacksworks to explore the way of adapting Med-VFMs to achieve the optimalperformance on target domains efficiently. Thus, it is highly demanded todesign an efficient way of fine-tuning Med-VFMs by selecting informativesamples to maximize their adaptation performance on target domains. To achievethis, we propose an Active Source-Free Domain Adaptation (ASFDA) method toefficiently adapt Med-VFMs to target domains for volumetric medical imagesegmentation. This ASFDA employs a novel Active Learning (AL) method to selectthe most informative samples from target domains for fine-tuning Med-VFMswithout the access to source pre-training samples, thus maximizing theirperformance with the minimal selection budget. In this AL method, we design anActive Test Time Sample Query strategy to select samples from the targetdomains via two query metrics, including Diversified Knowledge Divergence (DKD)and Anatomical Segmentation Difficulty (ASD). DKD is designed to measure thesource-target knowledge gap and intra-domain diversity. It utilizes theknowledge of pre-training to guide the querying of source-dissimilar andsemantic-diverse samples from the target domains. ASD is designed to evaluatethe difficulty in segmentation of anatomical structures by measuring predictiveentropy from foreground regions adaptively. Additionally, our ASFDA methodemploys a Selective Semi-supervised Fine-tuning to improve the performance andefficiency of fine-tuning by identifying samples with high reliability fromunqueried ones.</description>
      <author>example@mail.com (Jin Yang, Daniel S. Marcus, Aristeidis Sotiras)</author>
      <guid isPermaLink="false">2509.10784v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation</title>
      <link>http://arxiv.org/abs/2509.10748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种语音引导的协同感知(SCOPE)框架，将大型语言模型与开放式视觉基础模型相结合，支持在手术视频中对手术器械和解剖结构进行即时分割、标记和跟踪，实现人机协作的手术场景理解。&lt;h4&gt;背景&lt;/h4&gt;手术场景中准确分割和跟踪相关元素对提供术中辅助和决策支持至关重要。当前解决方案依赖于特定领域的监督模型，需要标记数据，且难以适应新场景和预定义标签类别外的元素。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在术中环境中实现开放式、零样本分割和跟踪的框架，无需依赖手动视觉或文本提示，支持人机协作的手术场景理解。&lt;h4&gt;方法&lt;/h4&gt;提出语音引导的协同感知(SCOPE)框架，整合大型语言模型的推理能力与开放式视觉基础模型的感知能力。框架包含协同感知代理组件，生成分割候选并整合临床医生的语音反馈，实现自然的人机协作。手术器械本身作为交互式指针标记其他手术场景元素。&lt;h4&gt;主要发现&lt;/h4&gt;在Cataract1k数据集子集和自体离体颅底数据集上的评估表明，该框架能够生成手术场景的即时分割和跟踪。实时离体模拟实验进一步验证了其动态能力。&lt;h4&gt;结论&lt;/h4&gt;这种人-AI协作范式展示了开发适应性、免手持、以外科医生为中心的工具用于动态手术室环境的潜力。&lt;h4&gt;翻译&lt;/h4&gt;准确的手术场景相关元素分割和跟踪对于实现情境感知的术中辅助和决策制定至关重要。当前解决方案仍然依赖于特定领域的监督模型，这些模型依赖标记数据，并且需要特定领域的数据来适应新的手术场景和预定义标签类别之外的元素。提示驱动视觉基础模型(VFM)的最新进展使得在异构医学图像上进行开放式、零样本分割成为可能。然而，这些模型对手动视觉或文本提示的依赖限制了其在术中手术环境中的部署。我们引入了一个语音引导的协同感知(SCOPE)框架，该框架将大型语言模型(LLM)的推理能力与开放式VFM的感知能力相结合，支持在术中视频流中对手术器械和解剖结构进行即时分割、标记和跟踪。该框架的一个关键组件是一个协同感知代理，它生成VFM生成的分割候选，并整合临床医生直观的语音反馈，以自然的人机协作方式指导手术器械的分割。之后，手术器械本身作为交互式指针来标记手术场景的其他元素。我们在公开可用的Cataract1k数据集的子集和一个自体离体颅底数据集上评估了我们提出的框架，以证明其在生成手术场景即时分割和跟踪方面的潜力。此外，我们通过一个实时的离体模拟实验展示了其动态能力。这种人-AI协作范式展示了开发适应性、免手持、以外科医生为中心的工具用于动态手术室环境的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决手术场景中实时、准确分割和跟踪手术器械及解剖结构的问题。传统方法依赖标记数据和特定领域模型，难以适应新场景；现有视觉模型虽强大但需手动操作，不适合无菌手术环境。此问题对实现术中情境感知辅助和决策制定至关重要，能提高手术安全性和效率，增强医生在动态手术环境中的应变能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有监督学习难以泛化、手动交互不适合手术环境的问题。借鉴了视觉基础模型(SAM, GSAM)的分割能力和大型语言模型(GPT-4)的理解能力，参考了Visual ChatGPT的多模态交互范式但针对手术环境优化。设计了模块化交互流程，结合语音交互、协作感知代理和虚拟指针技术，实现免提操作和自然人机协作，无需针对特定手术数据进行微调。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合大型语言模型的自然语言理解能力和视觉基础模型的感知能力，通过语音引导实现人机协作，实时分割和跟踪手术场景元素。流程包括：1)语音输入处理；2)查询扩展与掩码生成；3)医生语音选择掩码并分配标签；4)使用视频分割模型跟踪器械；5)通过器械尖端作为虚拟指针分割解剖结构；6)持续交互优化结果，形成完整闭环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)语音引导的免提交互方式；2)人机协作感知框架；3)器械尖端作为虚拟指针的技术；4)模块化交互设计；5)零样本泛化能力。相比之前工作，SCOPE摒弃了手动交互方式，专为动态手术视频设计，无需领域特定数据微调，强调人机协作而非自动化，更注重实际术中应用而非数据集标注。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCOPE通过整合语言模型推理能力和视觉模型感知能力，创造了一种语音引导的人机协作框架，使外科医生能通过自然语音命令实时分割和跟踪手术场景元素，从而提高手术情境感知和决策能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation and tracking of relevant elements of the surgical sceneis crucial to enable context-aware intraoperative assistance and decisionmaking. Current solutions remain tethered to domain-specific, supervised modelsthat rely on labeled data and required domain-specific data to adapt to newsurgical scenarios and beyond predefined label categories. Recent advances inprompt-driven vision foundation models (VFM) have enabled open-set, zero-shotsegmentation across heterogeneous medical images. However, dependence of thesemodels on manual visual or textual cues restricts their deployment inintroperative surgical settings. We introduce a speech-guided collaborativeperception (SCOPE) framework that integrates reasoning capabilities of largelanguage model (LLM) with perception capabilities of open-set VFMs to supporton-the-fly segmentation, labeling and tracking of surgical instruments andanatomy in intraoperative video streams. A key component of this framework is acollaborative perception agent, which generates top candidates of VFM-generatedsegmentation and incorporates intuitive speech feedback from clinicians toguide the segmentation of surgical instruments in a natural human-machinecollaboration paradigm. Afterwards, instruments themselves serve as interactivepointers to label additional elements of the surgical scene. We evaluated ourproposed framework on a subset of publicly available Cataract1k dataset and anin-house ex-vivo skull-base dataset to demonstrate its potential to generateon-the-fly segmentation and tracking of surgical scene. Furthermore, wedemonstrate its dynamic capabilities through a live mock ex-vivo experiment.This human-AI collaboration paradigm showcase the potential of developingadaptable, hands-free, surgeon-centric tools for dynamic operating-roomenvironments.</description>
      <author>example@mail.com (Jecia Z. Y. Mao, Francis X Creighton, Russell H Taylor, Manish Sahu)</author>
      <guid isPermaLink="false">2509.10748v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction</title>
      <link>http://arxiv.org/abs/2509.10698v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CrunchLLM，一个专门用于创业公司成功预测的领域自适应大型语言模型框架，结合结构化数据和非结构化文本，通过参数高效微调和提示优化提高预测准确率，同时提供可解释的推理。&lt;h4&gt;背景&lt;/h4&gt;预测创业公司成功（通过收购或IPO退出）是创业研究的关键问题；Crunchbase等数据集提供结构化信息和非结构化文本，但传统机器学习方法仅依赖结构化特征且准确率一般，而大型语言模型虽具推理能力但难以直接适应商业领域数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于创业公司成功预测的领域自适应大型语言模型框架，整合结构化公司属性与非结构化文本叙述，应用参数高效微调策略和提示优化，使基础模型专门用于创业数据。&lt;h4&gt;方法&lt;/h4&gt;提出CrunchLLM框架，整合结构化公司属性与非结构化文本叙述，应用参数高效微调策略和提示优化，使基础模型专门适应创业领域数据。&lt;h4&gt;主要发现&lt;/h4&gt;CrunchLLM在Crunchbase创业公司成功预测上实现超过80%的准确率，显著优于传统分类器和基线LLMs；提供可解释的推理轨迹，增强金融和政策决策者的透明度和可信度。&lt;h4&gt;结论&lt;/h4&gt;通过领域感知微调和结构化-非结构化数据融合改进LLMs可推进创业成果预测建模；CrunchLLM为风险资本和创新政策中的数据驱动决策提供了方法论框架和实用工具。&lt;h4&gt;翻译&lt;/h4&gt;预测创业公司的成功，定义为通过收购或首次公开募股实现退出，是创业和创新研究中的关键问题。Crunchbase等数据集提供了结构化信息（如融资轮次、行业、投资者网络）和非结构化文本（如公司描述），但有效利用这种异构数据进行预测仍然具有挑战性。传统机器学习方法通常只依赖结构化特征并取得中等准确率，而大型语言模型虽提供丰富的推理能力却难以直接适应特定领域的商业数据。我们提出了CrunchLLM，一个用于创业公司成功预测的领域自适应LLM框架。CrunchLLM整合了结构化公司属性和非结构化文本叙述，并应用参数高效微调策略和提示优化，使基础模型专门用于创业数据。我们的方法在Crunchbase创业公司成功预测上实现了超过80%的准确率，显著优于传统分类器和基线LLMs。除了预测性能外，CrunchLLM提供可解释的推理轨迹，为其预测提供合理性解释，增强了金融和政策决策者的透明度和可信度。这项工作展示了如何通过领域感知微调和结构化-非结构化数据融合来改进LLMs，以推进创业成果的预测建模。CrunchLLM为风险资本和创新政策中的数据驱动决策提供了方法论框架和实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting the success of start-up companies, defined as achieving an exitthrough acquisition or IPO, is a critical problem in entrepreneurship andinnovation research. Datasets such as Crunchbase provide both structuredinformation (e.g., funding rounds, industries, investor networks) andunstructured text (e.g., company descriptions), but effectively leveraging thisheterogeneous data for prediction remains challenging. Traditional machinelearning approaches often rely only on structured features and achieve moderateaccuracy, while large language models (LLMs) offer rich reasoning abilities butstruggle to adapt directly to domain-specific business data. We present\textbf{CrunchLLM}, a domain-adapted LLM framework for startup successprediction. CrunchLLM integrates structured company attributes withunstructured textual narratives and applies parameter-efficient fine-tuningstrategies alongside prompt optimization to specialize foundation models forentrepreneurship data. Our approach achieves accuracy exceeding 80\% onCrunchbase startup success prediction, significantly outperforming traditionalclassifiers and baseline LLMs. Beyond predictive performance, CrunchLLMprovides interpretable reasoning traces that justify its predictions, enhancingtransparency and trustworthiness for financial and policy decision makers. Thiswork demonstrates how adapting LLMs with domain-aware fine-tuning andstructured--unstructured data fusion can advance predictive modeling ofentrepreneurial outcomes. CrunchLLM contributes a methodological framework anda practical tool for data-driven decision making in venture capital andinnovation policy.</description>
      <author>example@mail.com (Rabeya Tus Sadia, Qiang Cheng)</author>
      <guid isPermaLink="false">2509.10698v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses</title>
      <link>http://arxiv.org/abs/2509.10620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025 Workshop CVAMD&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一个基于SimCLR的自监督学习基础模型，用于3D脑结构MRI分析，该模型在多种神经系统疾病的预测任务中表现出色，即使使用少量标记数据也能保持高性能。&lt;h4&gt;背景&lt;/h4&gt;3D结构MRI常用于监测多种神经系统疾病，但现有深度学习模型针对特定任务设计，泛化能力有限；自监督学习在2D医学成像中已取得成功，但3D脑MRI基础模型在分辨率、范围或可访问性方面仍有不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用、高分辨率的自监督学习基础模型，用于3D脑结构MRI分析，提高模型在不同任务和人群中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;基于SimCLR架构构建自监督学习模型，使用来自11个公开数据集的18,759名患者（44,958次扫描）进行预训练，涵盖多种神经系统疾病；与掩码自编码器及两个监督基线模型在四种下游预测任务上进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的SimCLR模型在所有下游任务上均优于其他模型；即使仅使用20%的标记训练样本预测阿尔茨海默病，模型仍能保持卓越性能。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一个广泛适用且可访问的基础模型，显著提升了3D脑MRI分析的性能，特别适用于标记数据有限的情况。&lt;h4&gt;翻译&lt;/h4&gt;3D结构磁共振成像（MRI）脑扫描通常在临床环境中获取，用于监测广泛的神经系统疾病，包括神经退行性疾病和中风。虽然深度学习模型在分析3D MRI的多个脑成像任务中显示出有希望的结果，但大多数模型都是针对特定任务高度定制化的，标记数据有限，且无法跨任务和/或人群泛化。自监督学习（SSL）的发展使得能够利用从健康到疾病多样化的未标记数据集创建大型医学基础模型，在2D医学成像应用中显示出显著成功。然而，即使是少数已开发的3D脑MRI基础模型，在分辨率、范围或可访问性方面仍然有限。在这项工作中，我们提出了一个通用的高分辨率基于SimCLR的SSL基础模型，用于3D脑结构MRI，在11个公开可用的数据集上进行了预训练，这些数据集涵盖了18,759名患者（44,958次扫描），涉及多种神经系统疾病。我们在分布内和分布外设置中的四个不同下游预测任务上，将我们的模型与掩码自编码器（MAE）以及两个监督基线模型进行了比较。我们的微调SimCLR模型在所有任务上都优于所有其他模型。值得注意的是，即使在使用仅20%的标记训练样本来预测阿尔茨海默病时，我们的模型仍能实现卓越的性能。我们使用公开可用的代码和数据，并在https://github.com/emilykaczmarek/3D-Neuro-SimCLR上发布了我们的训练模型，为临床脑MRI分析贡献了一个广泛适用且可访问的基础模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D脑部MRI扫描分析中深度学习模型难以泛化的问题。现有模型通常针对特定任务定制，依赖有限标记数据，无法在不同任务和人群中泛化。这个问题很重要，因为脑部疾病诊断常使用3D MRI，但医学影像标记数据获取成本高、耗时长，且模型在遇到分布变化时可能失效。不同人群的脑部MRI具有高度视觉和解剖相似性，为利用多样化数据集训练通用模型提供了机会。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了当前3D脑部MRI基础模型的三个主要局限：数据集范围有限、分辨率低、难以复制。他们借鉴了现有的自监督学习方法，特别是基于对比学习的SimCLR和基于掩码的MAE架构。选择SimCLR是因为其强大的归纳偏差，适合相对较小的医学影像数据集。作者还实现了MAE作为互补基线，并设计了快速可复现的预处理流程，使用TurboPrep包处理高分辨率3D数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用自监督学习从大量未标记的多样化3D脑部MRI数据中学习通用表示，使模型能泛化到各种下游任务。整体流程包括：1)收集11个数据集的18,759名患者(44,958次扫描)的3D脑部MRI；2)使用TurboPrep进行预处理，包括偏置场校正、颅骨剥离、配准等，保持1×1×1mm³高分辨率；3)采用SimCLR或MAE进行自监督预训练，学习对变换不变的语义特征；4)在四个下游任务(中风量表回归、阿尔茨海默病分类、性别分类、年龄回归)上评估模型性能，包括分布内和分布外测试；5)研究在有限标记数据下的模型表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用大规模多样化数据集(11个数据集，近4.5万次扫描)，涵盖多种神经系统疾病；2)保持高分辨率(1×1×1mm³)处理能力，保留临床相关小特征；3)完全公开代码和训练模型，提高可访问性和可复现性；4)在所有下游任务上表现优于现有模型，即使仅使用20%标记数据；5)使用快速预处理流程(TurboPrep)，每个扫描约需一分钟。相比之前工作，本文模型数据规模更大、更多样化、分辨率更高、更易获取，且在更广泛的任务和数据分布上进行了全面评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于SimCLR的高分辨率、可泛化的3D脑部MRI自监督基础模型，通过整合多源数据实现了在各种神经系统疾病诊断任务上的卓越性能，并在有限标记数据条件下保持高准确率，同时公开了代码和模型以促进医学影像分析领域的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonlyacquired in clinical settings to monitor a wide range of neurologicalconditions, including neurodegenerative disorders and stroke. While deeplearning models have shown promising results analyzing 3D MRI across a numberof brain imaging tasks, most are highly tailored for specific tasks withlimited labeled data, and are not able to generalize across tasks and/orpopulations. The development of self-supervised learning (SSL) has enabled thecreation of large medical foundation models that leverage diverse, unlabeleddatasets ranging from healthy to diseased data, showing significant success in2D medical imaging applications. However, even the very few foundation modelsfor 3D brain MRI that have been developed remain limited in resolution, scope,or accessibility. In this work, we present a general, high-resolutionSimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on18,759 patients (44,958 scans) from 11 publicly available datasets spanningdiverse neurological diseases. We compare our model to Masked Autoencoders(MAE), as well as two supervised baselines, on four diverse downstreamprediction tasks in both in-distribution and out-of-distribution settings. Ourfine-tuned SimCLR model outperforms all other models across all tasks. Notably,our model still achieves superior performance when fine-tuned using only 20% oflabeled training samples for predicting Alzheimer's disease. We use publiclyavailable code and data, and release our trained model athttps://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadlyapplicable and accessible foundation model for clinical brain MRI analysis.</description>
      <author>example@mail.com (Emily Kaczmarek, Justin Szeto, Brennan Nichyporuk, Tal Arbel)</author>
      <guid isPermaLink="false">2509.10620v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>RailSafeNet: Visual Scene Understanding for Tram Safety</title>
      <link>http://arxiv.org/abs/2509.12125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, EPIA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为RailSafeNet的实时框架，利用数字图像处理、深度学习和人工智能技术来提高有轨电车与人交互的安全性。&lt;h4&gt;背景&lt;/h4&gt;有轨电车经常在人口密集区域运行，与人交互的安全是一个重要挑战，碰撞可能导致从轻微伤害到致命结果。&lt;h4&gt;目的&lt;/h4&gt;设计一个解决方案，利用数字图像处理、深度学习和人工智能来提高行人、司机、骑行者、宠物和有轨电车乘客的安全。&lt;h4&gt;方法&lt;/h4&gt;RailSafeNet是一个实时框架，融合了语义分割、目标检测和基于规则的距离评估器来突出显示轨道入侵。系统仅使用单目视频，可以识别轨道、定位附近物体，并通过将投影距离与标准1435毫米轨道轨距比较来分类风险。&lt;h4&gt;主要发现&lt;/h4&gt;在RailSem19数据集上的实验显示，经过类别过滤的SegFormer B3模型实现了65%的交并比，经过微调的YOLOv8在交并比阈值为0.50时达到了75.6%的平均精度均值。&lt;h4&gt;结论&lt;/h4&gt;RailSafeNet提供了准确、标注轻量的场景理解能力，可以在危险情况升级前警告司机。&lt;h4&gt;翻译&lt;/h4&gt;有轨电车与人交互安全是一个重要挑战，因为有轨电车经常在人口密集区域运行，碰撞可能导致从轻微伤害到致命结果。本文从利用数字图像处理、深度学习和人工智能设计解决方案的角度出发，以提高行人、司机、骑行者、宠物和有轨电车乘客的安全性。我们提出了RailSafeNet，一个实时框架，融合了语义分割、目标检测和基于规则的距离评估器来突出显示轨道入侵。仅使用单目视频，系统可以识别轨道，定位附近物体，并通过将投影距离与标准1435毫米轨道轨距比较来分类风险。在多样化的RailSem19数据集上的实验显示，经过类别过滤的SegFormer B3模型实现了65%的交并比，而经过微调的YOLOv8在交并比阈值为0.50时达到了75.6%的平均精度均值。因此，RailSafeNet提供了准确、标注轻量的场景理解，可以在危险情况升级前警告司机。代码可在https://github.com/oValach/RailSafeNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tram-human interaction safety is an important challenge, given that tramsfrequently operate in densely populated areas, where collisions can range fromminor injuries to fatal outcomes. This paper addresses the issue from theperspective of designing a solution leveraging digital image processing, deeplearning, and artificial intelligence to improve the safety of pedestrians,drivers, cyclists, pets, and tram passengers. We present RailSafeNet, areal-time framework that fuses semantic segmentation, object detection and arule-based Distance Assessor to highlight track intrusions. Using onlymonocular video, the system identifies rails, localises nearby objects andclassifies their risk by comparing projected distances with the standard 1435mmrail gauge. Experiments on the diverse RailSem19 dataset show that aclass-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculatedat an intersection over union (IoU) threshold of 0.50. RailSafeNet thereforedelivers accurate, annotation-light scene understanding that can warn driversbefore dangerous situations escalate. Code available athttps://github.com/oValach/RailSafeNet.</description>
      <author>example@mail.com (Ing. Ondrej Valach, Ing. Ivan Gruber)</author>
      <guid isPermaLink="false">2509.12125v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Microsurgical Instrument Segmentation for Robot-Assisted Surgery</title>
      <link>http://arxiv.org/abs/2509.11727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MISRA框架，通过增强RGB输入、集成跳跃注意力和迭代反馈模块来解决显微手术中薄结构分割的挑战，并引入了一个专门的显微手术数据集。&lt;h4&gt;背景&lt;/h4&gt;准确的薄结构分割对于显微手术场景理解至关重要，但由于分辨率损失、低对比度和类别不平衡，这仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个分割框架来解决显微手术中薄结构分割的挑战，提高分割的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;提出了MISRA分割框架，通过增加亮度通道增强RGB输入，集成跳跃注意力保留细长特征，采用迭代反馈模块在多次传递中恢复连续性，并引入了一个专门的显微手术数据集。&lt;h4&gt;主要发现&lt;/h4&gt;MISRA实现了有竞争力的性能，与竞争方法相比，平均类别IoU提高了5.37%，同时在器械接触和重叠处提供了更稳定的预测。&lt;h4&gt;结论&lt;/h4&gt;MISRA是迈向计算机辅助和机器人显微手术可靠场景解析的有希望的一步。&lt;h4&gt;翻译&lt;/h4&gt;准确的薄结构分割对于显微手术场景理解至关重要，但由于分辨率损失、低对比度和类别不平衡，这仍然具有挑战性。我们提出了MISRA（机器人辅助显微手术器械分割），一个分割框架，通过增加亮度通道来增强RGB输入，集成跳跃注意力以保留细长特征，并采用迭代反馈模块在多次传递中恢复连续性。此外，我们引入了一个专门的显微手术数据集，包含精细标注的外科器械包括薄物体，为稳健评估提供了基准。数据集可在https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg获取。实验证明MISRA实现了有竞争力的性能，与竞争方法相比，平均类别IoU提高了5.37%，同时在器械接触和重叠处提供了更稳定的预测。这些结果表明MISRA是迈向计算机辅助和机器人显微手术可靠场景解析的有希望的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of thin structures is critical for microsurgical sceneunderstanding but remains challenging due to resolution loss, low contrast, andclass imbalance. We propose Microsurgery Instrument Segmentation for RoboticAssistance(MISRA), a segmentation framework that augments RGB input withluminance channels, integrates skip attention to preserve elongated features,and employs an Iterative Feedback Module(IFM) for continuity restoration acrossmultiple passes. In addition, we introduce a dedicated microsurgical datasetwith fine-grained annotations of surgical instruments including thin objects,providing a benchmark for robust evaluation Dataset available athttps://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstratethat MISRA achieves competitive performance, improving the mean class IoU by5.37% over competing methods, while delivering more stable predictions atinstrument contacts and overlaps. These results position MISRA as a promisingstep toward reliable scene parsing for computer-assisted and roboticmicrosurgery.</description>
      <author>example@mail.com (Tae Kyeong Jeong, Garam Kim, Juyoun Park)</author>
      <guid isPermaLink="false">2509.11727v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>See What I Mean? Mobile Eye-Perspective Rendering for Optical See-through Head-mounted Displays</title>
      <link>http://arxiv.org/abs/2509.11653v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对光学透视头戴显示器中的视角错位问题，提出并评估了三种基于软件的眼睛视角渲染技术，通过用户研究证明了凝视代理方法的优越性，并开源了相关框架。&lt;h4&gt;背景&lt;/h4&gt;基于图像的场景理解技术可在未准备的真实世界环境中为增强现实系统提供上下文视觉指导，但在光学透视头戴显示器上存在世界-facing相机与用户眼睛视角之间的错位问题，影响效果。&lt;h4&gt;目的&lt;/h4&gt;开发并评估软件眼睛视角渲染技术，以近似用户的真实视角，解决光学透视头戴显示器中的错位问题。&lt;h4&gt;方法&lt;/h4&gt;在Microsoft HoloLens 2上实现并评估了三种眼睛视角渲染技术：平面代理EPR（投影到固定距离平面）、网格代理EPR（使用SLAM重建）和凝视代理EPR（基于眼动追踪的对齐方法）。&lt;h4&gt;主要发现&lt;/h4&gt;真实世界任务的用户研究强调了准确眼睛视角渲染的重要性，并发现凝视代理方法是一种轻量级的有效替代方案，优于基于几何的方法。&lt;h4&gt;结论&lt;/h4&gt;软件眼睛视角渲染技术可有效解决光学透视头戴显示器中的视角错位问题，凝视代理方法因其轻量级特性而特别有应用价值。&lt;h4&gt;翻译&lt;/h4&gt;基于图像的场景理解使增强系统能够在未准备的真实世界环境中提供上下文视觉指导。虽然在视频透视头戴显示器上有效，但这类方法在光学透视头戴显示器上存在问题，因为前置相机与用户眼睛视角之间存在错位。为了近似用户的真实视角，我们在商业化的无绳光学透视头戴显示器（Microsoft HoloLens 2）上实现了并评估了三种基于软件的眼睛视角渲染技术：（1）平面代理EPR，投影到固定距离的平面上；（2）网格代理EPR，使用基于SLAM的重建进行投影；（3）凝视代理EPR，一种新颖的基于眼动追踪的方法，将投影与用户的凝视深度对齐。真实世界任务的用户研究强调了准确眼睛视角渲染的重要性，并证明了凝视代理作为基于几何方法的轻量级替代方案。我们以开源形式发布了眼睛视角渲染框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决光学穿透式头戴显示器中世界相机视角与用户眼睛视角不匹配导致的视觉错位问题。这个问题很重要，因为当系统分析世界相机图像并叠加信息到用户视野时，视角差异会导致虚拟内容无法准确对齐真实物体，影响用户体验，在低视力辅助、颜色视觉缺陷补偿和任务指导等应用中尤其明显，还可能引起恶心等生理不适。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有工作，发现有两种主要方法：硬件修改方法（如使用半透镜）和软件方法（如固定平面投影）。硬件方法无法应用于现有商业设备，软件方法则存在明显错位问题。作者借鉴了现有的纹理投影技术，设计了三种不同的代理几何体方法：固定平面投影、网格投影和一种新的基于凝视的动态平面投影方法，以解决视角不匹配问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过纹理投影技术，从用户视角重新渲染世界相机捕获的视图，使用不同的代理几何体确保虚拟内容与真实世界场景准确对齐。流程包括：1)使用HoloLens 2获取传感器数据；2)在Unity中进行渲染；3)处理世界相机图像；4)根据选择的EPR方法将图像投影到不同代理几何体上；5)从用户眼睛位置重新渲染生成EPR视图；6)在显示器上显示结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Gaze-Proxy EPR方法，使用眼动跟踪动态调整投影平面；2)在移动式商业OST HMD上实现和比较三种EPR方法；3)进行真实场景下的用户研究；4)开源所有实现。相比之前工作，本研究关注移动、无连接线场景，提出的Gaze-Proxy方法在保持高准确性的同时降低了计算复杂度，解决了之前方法要么计算复杂要么准确性差的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这项研究提出并评估了一种基于眼动跟踪的新型轻量级EPR方法，为光学穿透式头戴显示器提供了准确的图像分析结果对齐解决方案，并通过开源框架促进了移动式增强现实应用的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image-based scene understanding allows Augmented Reality systems to providecontextual visual guidance in unprepared, real-world environments. Whileeffective on video see-through (VST) head-mounted displays (HMDs), such methodssuffer on optical see-through (OST) HMDs due to misregistration between theworld-facing camera and the user's eye perspective. To approximate the user'strue eye view, we implement and evaluate three software-based eye-perspectiverendering (EPR) techniques on a commercially available, untethered OST HMD(Microsoft HoloLens 2): (1) Plane-Proxy EPR, projecting onto a fixed-distanceplane; (2) Mesh-Proxy EPR, using SLAM-based reconstruction for projection; and(3) Gaze-Proxy EPR, a novel eye-tracking-based method that aligns theprojection with the user's gaze depth. A user study on real-world tasksunderscores the importance of accurate EPR and demonstrates gaze-proxy as alightweight alternative to geometry-based methods. We release our EPR frameworkas open source.</description>
      <author>example@mail.com (Gerlinde Emsenhuber, Tobias Langlotz, Denis Kalkofen, Markus Tatzgern)</author>
      <guid isPermaLink="false">2509.11653v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>How Auxiliary Reasoning Unleashes GUI Grounding in VLMs</title>
      <link>http://arxiv.org/abs/2509.11548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对图形用户界面(GUI)基础任务提出三种零样本辅助推理方法，通过提供明确的空间线索使视觉语言模型能够表达其隐含的空间理解能力，显著提高了GUI基础性能。&lt;h4&gt;背景&lt;/h4&gt;图形用户界面(GUI)基础是构建GUI代理的基本任务，但通用的视觉语言模型(VLMs)在这个任务上表现不佳，因为缺乏特定优化。&lt;h4&gt;目的&lt;/h4&gt;解决VLMs在Pointing Game测量中显示出潜在定位能力但在输出明确坐标任务上表现不佳的问题，同时避免当前微调方法的高数据和高标注成本。&lt;h4&gt;方法&lt;/h4&gt;提出三种零样本辅助推理方法，通过在输入图像中提供明确的空间线索(如坐标轴、网格和标记的交叉点)，使VLMs能够表达其隐含的空间理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;在四个GUI基础基准上对七个开源和专有VLMs的评估结果表明，提出的方法显著提高了GUI基础性能。&lt;h4&gt;结论&lt;/h4&gt;通过提供空间线索的零样本辅助推理方法可以有效提升VLMs在GUI基础任务上的表现，避免了传统微调方法的高成本问题。&lt;h4&gt;翻译&lt;/h4&gt;图形用户界面(GUI)基础是构建GUI代理的基本任务。然而，通用的视觉语言模型(VLMs)由于缺乏特定优化而难以完成此任务。本文确定了一个关键差距：虽然VLMs在通过Pointing Game测量的性能中显示出显著的潜在定位能力，但在输出明确坐标的任务上表现不佳。为解决这种差异，并绕过当前微调方法的高数据和标注成本，我们提出了三种零样本辅助推理方法。通过在输入图像中提供明确的空间线索，如坐标轴、网格和标记的交叉点，这些方法使VLMs能够表达其隐含的空间理解能力。我们在四个GUI基础基准上对七个开源和专有VLMs评估了这些方法。评估结果表明，提出的方法显著提高了GUI基础的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphical user interface (GUI) grounding is a fundamental task for buildingGUI agents. However, general vision-language models (VLMs) struggle with thistask due to a lack of specific optimization. We identify a key gap in thispaper: while VLMs exhibit significant latent grounding potential, asdemonstrated by their performance measured by Pointing Game, they underperformwhen tasked with outputting explicit coordinates. To address this discrepancy,and bypass the high data and annotation costs of current fine-tuningapproaches, we propose three zero-shot auxiliary reasoning methods. Byproviding explicit spatial cues such as axes, grids and labeled intersectionsas part of the input image, these methods enable VLMs to articulate theirimplicit spatial understanding capabilities. We evaluate these methods on fourGUI grounding benchmarks across seven open-source and proprietary VLMs. Theevaluation results demonstrate that the proposed methods substantially improvethe performance of GUI grounding.</description>
      <author>example@mail.com (Weiming Li, Yan Shao, Jing Yang, Yujing Lu, Ling Zhong, Yuhan Wang, Manni Duan)</author>
      <guid isPermaLink="false">2509.11548v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision</title>
      <link>http://arxiv.org/abs/2509.11476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 6th International Conference on Computer Vision and  Data Mining (ICCVDM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FusionNet的新型端到端红外和可见图像融合框架，通过模态感知注意力机制和像素级alpha混合模块实现细粒度融合，并利用目标感知损失函数保留重要对象区域的语义一致性。&lt;h4&gt;背景&lt;/h4&gt;红外和可见图像融合(IVIF)是多模态感知中的基本任务，旨在整合来自不同光谱域的互补结构和纹理线索。&lt;h4&gt;目的&lt;/h4&gt;提出一个新型端到端融合框架，明确建模模态间交互并增强任务关键区域。&lt;h4&gt;方法&lt;/h4&gt;引入模态感知注意力机制动态调整红外和可见特征贡献；集成像素级alpha混合模块自适应学习空间变化的融合权重；提出目标感知损失函数利用弱ROI监督保留重要对象区域的语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在M3FD数据集上的实验表明，FusionNet生成的融合图像具有增强的语义保留性、高感知质量和清晰的可解释性。&lt;h4&gt;结论&lt;/h4&gt;该框架为语义感知的多模态图像融合提供了通用且可扩展的解决方案，有利于下游任务如目标检测和场景理解。&lt;h4&gt;翻译&lt;/h4&gt;红外和可见图像融合(IVIF)是多模态感知中的一个基本任务，旨在整合来自不同光谱域的互补结构和纹理线索。在本文中，我们提出FusionNet，一种新型端到端融合框架，明确建模模态间交互并增强任务关键区域。FusionNet引入了一种模态感知注意力机制，根据红外和可见特征的判别能力动态调整其贡献。为实现细粒度、可解释的融合，我们进一步集成了像素级alpha混合模块，该模块以自适应和内容感知的方式学习空间变化的融合权重。此外，我们制定了一种目标感知损失函数，利用弱ROI监督来保留包含重要对象（如行人、车辆）区域的语义一致性。在公共M3FD数据集上的实验表明，FusionNet生成的融合图像具有增强的语义保留性、高感知质量和清晰的可解释性。我们的框架为语义感知的多模态图像融合提供了通用且可扩展的解决方案，有利于目标检测和场景理解等下游任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infrared and visible image fusion (IVIF) is a fundamental task in multi-modalperception that aims to integrate complementary structural and textural cuesfrom different spectral domains. In this paper, we propose FusionNet, a novelend-to-end fusion framework that explicitly models inter-modality interactionand enhances task-critical regions. FusionNet introduces a modality-awareattention mechanism that dynamically adjusts the contribution of infrared andvisible features based on their discriminative capacity. To achievefine-grained, interpretable fusion, we further incorporate a pixel-wise alphablending module, which learns spatially-varying fusion weights in an adaptiveand content-aware manner. Moreover, we formulate a target-aware loss thatleverages weak ROI supervision to preserve semantic consistency in regionscontaining important objects (e.g., pedestrians, vehicles). Experiments on thepublic M3FD dataset demonstrate that FusionNet generates fused images withenhanced semantic preservation, high perceptual quality, and clearinterpretability. Our framework provides a general and extensible solution forsemantic-aware multi-modal image fusion, with benefits for downstream taskssuch as object detection and scene understanding.</description>
      <author>example@mail.com (Tianyao Sun, Dawei Xiang, Tianqi Ding, Xiang Fang, Yijiashun Qi, Zunduo Zhao)</author>
      <guid isPermaLink="false">2509.11476v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI</title>
      <link>http://arxiv.org/abs/2509.10683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了大型语言模型(LLMs)和传统卷积神经网络(CNNs)在医学影像任务（胶质瘤分类和分割）上的性能，结果表明CNN在准确性和空间理解方面优于LLMs，LLMs在当前形式下不太适合基于图像的医疗任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)在基于文本的医疗任务中表现出色，但它们在基于图像的应用中的效用尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;研究LLMs在医学影像任务（特别是胶质瘤分类和分割）中的有效性，并将其性能与传统卷积神经网络(CNNs)进行比较。&lt;h4&gt;方法&lt;/h4&gt;使用BraTS 2020多模态脑部MRI数据集，评估通用视觉-语言LLM（LLaMA 3.2 Instruct）在微调前后的性能，并将其与自定义3D CNN进行基准测试。对于分类任务，比较低级别与高级别胶质瘤的分类效果；对于分割任务，实现中心点、边界框和多边形提取三种方法。&lt;h4&gt;主要发现&lt;/h4&gt;分类任务：CNN达到80%准确率，平衡了精确率和召回率；通用LLM达到76%准确率但特异性仅18%，经常错误分类低级别肿瘤；微调后特异性提高到55%但准确率降至72%。分割任务：CNNs能准确定位胶质瘤但有时遗漏小肿瘤；LLMs将预测聚集在图像中心，无法区分胶质瘤大小、位置；微调改善了输出格式但未提高空间准确性；边界多边形方法产生随机无结构输出。&lt;h4&gt;结论&lt;/h4&gt;CNNs在两项任务中都优于LLMs。LLMs表现出有限的空间理解能力，微调带来的改进很小，表明在当前形式下它们不适合基于图像的任务。需要更严格的微调或替代训练策略，才能使LLMs在医疗领域实现更好的性能、鲁棒性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在基于文本的医疗任务中表现出色。然而，它们在基于图像的应用中的效用尚未被探索。我们研究了LLMs在医学影像任务（特别是胶质瘤分类和分割）中的有效性，并将其性能与传统卷积神经网络(CNNs)进行了比较。使用BraTS 2020多模态脑部MRI数据集，我们评估了通用视觉-语言LLM（LLaMA 3.2 Instruct）在微调前后的性能，并将其性能与自定义3D CNN进行了基准测试。对于胶质瘤分类（低级别vs高级别），CNN实现了80%的准确率，并平衡了精确率和召回率。通用LLM达到76%的准确率，但特异性仅为18%，经常错误分类低级别肿瘤。微调将特异性提高到55%，但整体性能下降（例如，准确率降至72%）。对于分割任务，实现了三种方法：中心点、边界框和多边形提取。CNNs能够准确定位胶质瘤，但有时会遗漏小肿瘤。相比之下，LLMs将预测一致地聚集在图像中心，无法区分胶质瘤的大小、位置或放置。微调改善了输出格式，但未能有意义地提高空间准确性。边界多边形方法产生随机、无结构的输出。总体而言，CNNs在两项任务中都优于LLMs。LLMs表现出有限的空间理解能力，微调带来的改进也很小，表明在当前形式下，它们不适合基于图像的任务。可能需要更严格的微调或替代训练策略，才能使LLMs在医疗领域实现更好的性能、鲁棒性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have shown strong performance in text-basedhealthcare tasks. However, their utility in image-based applications remainsunexplored. We investigate the effectiveness of LLMs for medical imaging tasks,specifically glioma classification and segmentation, and compare theirperformance to that of traditional convolutional neural networks (CNNs). Usingthe BraTS 2020 dataset of multi-modal brain MRIs, we evaluated ageneral-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and afterfine-tuning, and benchmarked its performance against custom 3D CNNs. For gliomaclassification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy andbalanced precision and recall. The general LLM reached 76% accuracy butsuffered from a specificity of only 18%, often misclassifying Low-Grade tumors.Fine-tuning improved specificity to 55%, but overall performance declined(e.g., accuracy dropped to 72%). For segmentation, three methods - centerpoint, bounding box, and polygon extraction, were implemented. CNNs accuratelylocalized gliomas, though small tumors were sometimes missed. In contrast, LLMsconsistently clustered predictions near the image center, with no distinctionof glioma size, location, or placement. Fine-tuning improved output formattingbut failed to meaningfully enhance spatial accuracy. The bounding polygonmethod yielded random, unstructured outputs. Overall, CNNs outperformed LLMs inboth tasks. LLMs showed limited spatial understanding and minimal improvementfrom fine-tuning, indicating that, in their current form, they are notwell-suited for image-based tasks. More rigorous fine-tuning or alternativetraining strategies may be needed for LLMs to achieve better performance,robustness, and utility in the medical space.</description>
      <author>example@mail.com (Felicia Liu, Jay J. Yoo, Farzad Khalvati)</author>
      <guid isPermaLink="false">2509.10683v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal SAM-adapter for Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.10408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MM SAM-adapter的新型多模态语义分割框架，通过适配器网络将融合的多模态特征注入到SAM的RGB特征中，实现了在具有挑战性条件下的高效场景理解。&lt;h4&gt;背景&lt;/h4&gt;语义分割作为计算机视觉的关键任务，在自动驾驶、医学成像和机器人技术等领域有广泛应用。尽管深度学习推动了该领域的显著进步，但现有方法在光照不足、遮挡和恶劣天气等挑战条件下仍表现脆弱。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，作者提出MM SAM-adapter框架，扩展Segment Anything Model（SAM）的能力以实现多模态语义分割，实现多模态信息的平衡和高效使用。&lt;h4&gt;方法&lt;/h4&gt;所提出的方法采用适配器网络，将融合的多模态特征（如LiDAR、红外数据）注入到SAM丰富的RGB特征中。这种设计使模型能够保留RGB特征的强泛化能力，同时仅在辅助模态提供额外有用信息时选择性地融入它们。&lt;h4&gt;主要发现&lt;/h4&gt;在三个具有挑战性的基准测试（DeLiVER、FMB和MUSES）上评估，MM SAM-adapter实现了最先进的性能。通过将数据集划分为RGB-easy和RGB-hard子集的分析表明，该框架在有利和不利条件下均优于现有方法，证明了多模态适应对鲁棒场景理解的有效性。&lt;h4&gt;结论&lt;/h4&gt;MM SAM-adapter实现了多模态信息的平衡和高效使用，显著提升了在挑战性条件下的语义分割性能，为多模态场景理解提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;语义分割是计算机视觉中的一个关键任务，在自动驾驶、医学成像和机器人技术中有广泛应用，随着深度学习的发展取得了显著进步。然而，当前方法在光照不足、遮挡和恶劣天气等具有挑战性的条件下仍然脆弱。为了解决这些局限性，最近出现了整合辅助传感器数据（如LiDAR、红外）的多模态方法，提供互补信息以增强鲁棒性。在这项工作中，我们提出了MM SAM-adapter，一种扩展Segment Anything Model（SAM）能力以实现多模态语义分割的新框架。所提出的方法采用适配器网络，将融合的多模态特征注入到SAM丰富的RGB特征中。这种设计使模型能够保留RGB特征的强泛化能力，同时仅在辅助模态提供额外线索时选择性地融入它们。因此，MM SAM-adapter实现了多模态信息的平衡和高效使用。我们在三个具有挑战性的基准测试（DeLiVER、FMB和MUSES）上评估了我们的方法，MM SAM-adapter在这些测试中实现了最先进的性能。为了进一步分析模态贡献，我们将DeLiVER和FMB划分为RGB-easy和RGB-hard子集。结果一致表明，我们的框架在有利和不利条件下都优于竞争方法，突显了多模态适应对鲁棒场景理解的有效性。代码可在以下链接获取：https://github.com/iacopo97/Multimodal-SAM-Adapter。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation, a key task in computer vision with broad applicationsin autonomous driving, medical imaging, and robotics, has advancedsubstantially with deep learning. Nevertheless, current approaches remainvulnerable to challenging conditions such as poor lighting, occlusions, andadverse weather. To address these limitations, multimodal methods thatintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,providing complementary information that enhances robustness. In this work, wepresent MM SAM-adapter, a novel framework that extends the capabilities of theSegment Anything Model (SAM) for multimodal semantic segmentation. The proposedmethod employs an adapter network that injects fused multimodal features intoSAM's rich RGB features. This design enables the model to retain the stronggeneralization ability of RGB features while selectively incorporatingauxiliary modalities only when they contribute additional cues. As a result, MMSAM-adapter achieves a balanced and efficient use of multimodal information. Weevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,where MM SAM-adapter delivers state-of-the-art performance. To further analyzemodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hardsubsets. Results consistently demonstrate that our framework outperformscompeting methods in both favorable and adverse conditions, highlighting theeffectiveness of multimodal adaptation for robust scene understanding. The codeis available at the following link:https://github.com/iacopo97/Multimodal-SAM-Adapter.</description>
      <author>example@mail.com (Iacopo Curti, Pierluigi Zama Ramirez, Alioscia Petrelli, Luigi Di Stefano)</author>
      <guid isPermaLink="false">2509.10408v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
  <item>
      <title>LayerLock: Non-collapsing Representation Learning with Progressive Freezing</title>
      <link>http://arxiv.org/abs/2509.10156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LayerLock是一种简单有效的自监督视觉表征学习方法，通过渐进式层冻结实现从像素到潜在预测的过渡。&lt;h4&gt;背景&lt;/h4&gt;在视频掩码自编码(MAE)模型训练中，ViT层按深度顺序收敛：浅层先收敛，深层后收敛。&lt;h4&gt;目的&lt;/h4&gt;加速标准MAE训练，并提供一种简单可扩展的潜在预测方法，避免'表征崩溃'问题。&lt;h4&gt;方法&lt;/h4&gt;LayerLock通过显式计划逐步冻结模型，从像素预测过渡到潜在预测。&lt;h4&gt;主要发现&lt;/h4&gt;ViT层收敛顺序与其深度相关，浅层先收敛，深层后收敛；这种观察可用于加速MAE训练。&lt;h4&gt;结论&lt;/h4&gt;LayerLock在大模型上表现优异，在4DS感知套件上的结果超越了非掩码潜在预测方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了LayerLock，一种简单而有效的自监督视觉表征学习方法，通过渐进式层冻结从像素预测逐步过渡到潜在预测。首先，我们观察到在视频掩码自编码(MAE)模型训练过程中，ViT层按照其深度顺序收敛：浅层收敛早，深层收敛晚。然后我们证明，这一观察可以通过在整个训练过程中按照显式计划逐步冻结模型来加速标准MAE。此外，同样的计划可用于一种简单且可扩展的潜在预测方法，不会遭受'表征崩溃'。我们将提出的LayerLock方法应用于高达40亿参数的大模型，在4DS感知套件上的结果超过了非掩码潜在预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LayerLock, a simple yet effective approach for self-supervisedvisual representation learning, that gradually transitions from pixel to latentprediction through progressive layer freezing. First, we make the observationthat during training of video masked-autoencoding (MAE) models, ViT layersconverge in the order of their depth: shallower layers converge early, deeperlayers converge late. We then show that this observation can be exploited toaccelerate standard MAE by progressively freezing the model according to anexplicit schedule, throughout training. Furthermore, this same schedule can beused in a simple and scalable approach to latent prediction that does notsuffer from "representation collapse". We apply our proposed approach,LayerLock, to large models of up to 4B parameters with results surpassing thoseof non-latent masked prediction on the 4DS perception suite.</description>
      <author>example@mail.com (Goker Erdogan, Nikhil Parthasarathy, Catalin Ionescu, Drew Hudson, Alexander Lerchner, Andrew Zisserman, Mehdi Sajjadi, Joao Carreira)</author>
      <guid isPermaLink="false">2509.10156v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Merging Physics-Based Synthetic Data and Machine Learning for Thermal Monitoring of Lithium-ion Batteries: The Role of Data Fidelity</title>
      <link>http://arxiv.org/abs/2509.10380v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合物理建模与机器学习的新型框架，用于开发资源高效且可扩展的内部温度估计算法，解决了传统方法中数据收集、模型参数化和估计器设计的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;内部温度比表面温度更难获取，需要开发准确且实时的估计算法以实现更好的热管理和安全。&lt;h4&gt;目的&lt;/h4&gt;开发一种资源高效且可扩展的框架，用于构建准确、鲁棒和自适应的内部温度估计算法。&lt;h4&gt;方法&lt;/h4&gt;结合物理建模和机器学习：利用物理模型生成包含不同操作场景的仿真数据；使用这些数据预训练机器学习算法；应用迁移学习和无监督领域适应弥合仿真到现实的差距；使用有限的目标电池运行数据微调预训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;框架在多种圆柱形电池和不同对流空气冷却条件下得到验证；仅依赖电池热特性的先验知识时，均方根误差为0.5摄氏度；使用接近真实值的热参数时，误差小于0.1摄氏度；全面研究了仿真数据质量在框架中的作用。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够有效解决传统方法中数据收集、模型参数化和估计器设计方面的关键挑战，实现准确、鲁棒和自适应的内部温度估计。&lt;h4&gt;翻译&lt;/h4&gt;由于内部温度比表面温度更难获取，迫切需要开发准确且实时的估计算法以实现更好的热管理和安全。这项工作通过结合物理建模与机器学习，提出了一种新型框架，用于资源高效且可扩展地开发准确、鲁棒和自适应的内部温度估计算法，以解决传统方法中数据收集、模型参数化和估计器设计方面的关键挑战。在该框架中，利用物理模型生成包含不同操作场景的仿真数据，通过扫描模型参数和输入曲线。这种廉价的仿真数据集可用于预训练机器学习算法以捕获底层映射关系。为了弥合由不完美建模导致的仿真到现实的差距，应用了带有无监督领域适应的迁移学习，使用来自目标电池的有限运行数据（无内部温度值）来微调预训练的机器学习模型。该框架在不同操作条件和多个采用对流空气冷却的圆柱形电池上得到验证，仅依赖电池热特性的先验知识时，均方根误差为0.5摄氏度，而使用接近真实值的热参数时，误差小于0.1摄氏度。此外，全面研究了仿真数据质量在所提框架中的作用，以识别有前途的合成数据生成方法，保证机器学习模型的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since the internal temperature is less accessible than surface temperature,there is an urgent need to develop accurate and real-time estimation algorithmsfor better thermal management and safety. This work presents a novel frameworkfor resource-efficient and scalable development of accurate, robust, andadaptive internal temperature estimation algorithms by blending physics-basedmodeling with machine learning, in order to address the key challenges in datacollection, model parameterization, and estimator design that traditionallyhinder both approaches. In this framework, a physics-based model is leveragedto generate simulation data that includes different operating scenarios bysweeping the model parameters and input profiles. Such a cheap simulationdataset can be used to pre-train the machine learning algorithm to capture theunderlying mapping relationship. To bridge the simulation-to-reality gapresulting from imperfect modeling, transfer learning with unsupervised domainadaptation is applied to fine-tune the pre-trained machine learning model, byusing limited operational data (without internal temperature values) fromtarget batteries. The proposed framework is validated under different operatingconditions and across multiple cylindrical batteries with convective aircooling, achieving a root mean square error of 0.5 {\deg}C when relying solelyon prior knowledge of battery thermal properties, and less than 0.1 {\deg}Cwhen using thermal parameters close to the ground truth. Furthermore, the roleof the simulation data quality in the proposed framework has beencomprehensively investigated to identify promising ways of synthetic datageneration to guarantee the performance of the machine learning model.</description>
      <author>example@mail.com (Yusheng Zheng, Wenxue Liu, Yunhong Che, Ferdinand Grimm, Jingyuan Zhao, Xiaosong Hu, Simona Onori, Remus Teodorescu, Gregory J. Offer)</author>
      <guid isPermaLink="false">2509.10380v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning</title>
      <link>http://arxiv.org/abs/2509.10273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种数据驱动的迁移学习框架，利用神经推荐系统在稀疏实验数据集上实现离子液体性质的可靠预测。&lt;h4&gt;背景&lt;/h4&gt;离子液体因其物理化学性质可精确调节而成为传统溶剂的多功能替代品，但准确预测其关键热物理性质具有挑战性，因为化学设计空间大且实验数据有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用神经推荐系统的数据驱动迁移学习框架，能够在稀疏实验数据情况下可靠预测离子液体的热物理性质。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段过程：首先在固定温度压力下使用COSMO-RS模拟数据预训练NRS模型学习阳离子和阴离子的结构嵌入；然后使用这些嵌入和不同温度压力下的实验数据微调前馈神经网络。研究考虑了密度、粘度、表面张力、热容和熔点五种性质，并支持同性质和跨性质知识转移。&lt;h4&gt;主要发现&lt;/h4&gt;使用密度、粘度和热容的预训练模型进行微调后，五种目标性质中有四种的性能显著提高；模型对未见过的离子液体具有强大的外推能力；最终训练的模型可预测超过700,000种离子液体组合的性质。&lt;h4&gt;结论&lt;/h4&gt;结合模拟数据和迁移学习是克服实验数据稀疏性的有效方法，为离子液体筛选和工艺设计提供了可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;离子液体（ILs）因其物理化学性质可精确调节以适应各种应用而成为传统溶剂的多功能替代品。然而，由于巨大的化学设计空间和实验数据的有限性，准确预测关键热物理性质仍然具有挑战性。在本研究中，我们提出了一个数据驱动的迁移学习框架，利用神经推荐系统（NRS）使用稀疏实验数据集实现离子液体性质的可靠预测。该方法涉及一个两阶段过程：首先在固定温度和压力下使用基于COSMO-RS的模拟数据预训练NRS模型，学习阳离子和阴离子的特定性质结构嵌入；其次使用这些嵌入和不同温度压力下的实验数据微调简单的前馈神经网络。在本工作中，考虑了五种基本的离子液体性质：密度、粘度、表面张力、热容和熔点。该框架支持同性质和跨性质知识转移。值得注意的是，密度、粘度和热容的预训练模型被用来微调所有五种目标性质的模型，其中四种性质的性能显著提高。该模型对未见过的离子液体表现出强大的外推能力。此外，最终训练的模型能够预测超过700,000种离子液体组合的性质，为工艺设计中的离子液体筛选提供了可扩展的解决方案。这项工作强调了结合模拟数据和迁移学习克服实验数据稀疏性的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ionic liquids (ILs) have emerged as versatile replacements for traditionalsolvents because their physicochemical properties can be precisely tailored tovarious applications. However, accurately predicting key thermophysicalproperties remains challenging due to the vast chemical design space and thelimited availability of experimental data. In this study, we present adata-driven transfer learning framework that leverages a neural recommendersystem (NRS) to enable reliable property prediction for ILs using sparseexperimental datasets. The approach involves a two-stage process: first,pre-training NRS models on COSMO-RS-based simulated data at fixed temperatureand pressure to learn property-specific structural embeddings for cations andanions; and second, fine-tuning simple feedforward neural networks using theseembeddings with experimental data at varying temperatures and pressures. Inthis work, five essential IL properties are considered: density, viscosity,surface tension, heat capacity, and melting point. The framework supports bothwithin-property and cross-property knowledge transfer. Notably, pre-trainedmodels for density, viscosity, and heat capacity are used to fine-tune modelsfor all five target properties, achieving improved performance by a substantialmargin for four of them. The model exhibits robust extrapolation to previouslyunseen ILs. Moreover, the final trained models enable property prediction forover 700,000 IL combinations, offering a scalable solution for IL screening inprocess design. This work highlights the effectiveness of combining simulateddata and transfer learning to overcome sparsity in the experimental data.</description>
      <author>example@mail.com (Sahil Sethi, Kai Sundmacher, Caroline Ganzer)</author>
      <guid isPermaLink="false">2509.10273v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Supervised and unsupervised learning with numerical computation for the Wolfram cellular automata</title>
      <link>http://arxiv.org/abs/2509.10209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了Wolfram细胞自动机的渐近密度和动态演化机制，通过结合数值模拟与机器学习方法分析了不同规则下的系统行为。&lt;h4&gt;背景&lt;/h4&gt;Wolfram细胞自动机使用八位二进制编码的一维三细胞邻域确定性更新规则，被广泛应用于研究自组织现象和复杂系统动力学。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探究Wolfram自动机的渐近密度和动态演化机制，识别不同规则下的配置特征，并探索初始条件对系统行为的影响。&lt;h4&gt;方法&lt;/h4&gt;研究采用数值模拟和计算方法，结合监督学习和无监督学习方法（如主成分分析和自编码器）来分析不同Wolfram规则的配置特征和系统行为。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了所选规则的渐近密度与初始密度之间的关系；某些Wolfram规则即使在从单个活动位点开始的情况下也能随时间生成相似的分形图案；监督学习方法有效识别各种Wolfram规则的配置，而无监督方法可以将不同规则的配置聚类到不同组中。&lt;h4&gt;结论&lt;/h4&gt;通过结合数值模拟与机器学习方法，研究成功揭示了Wolfram自动机的动态特性，为理解复杂系统行为提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;Wolfram细胞自动机具有一维三细胞邻域的局部规则由八位二进制表示，这些二进制编码确定性更新规则。这些自动机被广泛用于研究自组织现象和复杂系统的动力学。在这项工作中，我们采用数值模拟和计算方法来研究Wolfram自动机的渐近密度和动态演化机制。我们应用监督和无监督学习方法来识别与不同Wolfram规则相关的配置。此外，我们探索了替代初始条件，在这些条件下，某些Wolfram规则即使从单个活动位点开始也能随时间生成相似的分形图案。我们的结果揭示了所选规则的渐近密度与初始密度之间的关系。监督学习方法有效识别了各种Wolfram规则的配置，而无监督方法如主成分分析和自编码器可以将不同Wolfram规则的配置近似聚类到不同的组中，产生与模拟密度输出一致的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The local rules of Wolfram cellular automata with one-dimensional three-cellneighborhoods are represented by eight-bit binary that encode deterministicupdate rules. These automata are widely utilized to investigateself-organization phenomena and the dynamics of complex systems. In this work,we employ numerical simulations and computational methods to investigate theasymptotic density and dynamical evolution mechanisms in Wolfram automata. Weapply both supervised and unsupervised learning methods to identify theconfigurations associated with different Wolfram rules. Furthermore, we explorealternative initial conditions under which certain Wolfram rules generatesimilar fractal patterns over time, even when starting from a single activesite. Our results reveal the relationship between the asymptotic density andthe initial density of selected rules. The supervised learning methodseffectively identify the configurations of various Wolfram rules, whileunsupervised methods like principal component analysis and autoencoders canapproximately cluster configurations of different Wolfram rules into distinctgroups, yielding results that align well with simulated density outputs.</description>
      <author>example@mail.com (Kui Tuo, Shengfeng Deng, Yuxiang Yang, Yanyang Wang, Qiuping A. Wang, Wei Li, Wenjun Zhang)</author>
      <guid isPermaLink="false">2509.10209v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification</title>
      <link>http://arxiv.org/abs/2509.10082v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 4 tables, 5 figures, submitted to IEEE Journal of  Biomedical and Health Informatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FetalSleepNet是一个创新的深度学习方法，首次用于从羊胎儿脑电图(EEG)分类睡眠状态。通过迁移学习和频谱均衡域适应策略，实现了高准确率的睡眠阶段分类，准确率达86.6%，宏F1分数为62.5%，优于基线模型。该方法有潜力应用于临床胎儿监测，为早期发现妊娠并发症相关的脑发育异常提供工具。&lt;h4&gt;背景&lt;/h4&gt;羊胎儿脑电图(EEG)的获取复杂，解释起来困难且繁琐。然而，精确的睡眠阶段分类可能有助于早期发现与妊娠并发症（如缺氧或宫内生长受限）相关的异常脑发育。&lt;h4&gt;目的&lt;/h4&gt;开发一个深度学习方法来分类羊胎儿的睡眠状态，并应用于妊娠并发症的早期检测。&lt;h4&gt;方法&lt;/h4&gt;研究人员将EEG电极固定在24只晚期妊娠胎羊的顶叶皮层硬脑膜上。使用一种为成人EEG睡眠阶段分类开发的轻量级深度神经网络，通过从成人EEG进行迁移学习来训练该网络处理羊EEG，并采用基于频谱均衡的域适应策略来减少跨域不匹配。&lt;h4&gt;主要发现&lt;/h4&gt;直接迁移表现不佳，但完全微调结合频谱均衡取得了最佳整体性能（准确率：86.6%，宏F1分数：62.5%），优于基线模型。&lt;h4&gt;结论&lt;/h4&gt;FetalSleepNet是第一个专门为从胎儿EEG自动睡眠阶段分类开发的深度学习框架。它可以作为标记引擎，支持大规模弱/半监督标记和蒸馏，促进在临床中获取的侵入性较小信号（如多普勒超声或心电图数据）的训练。其轻量级设计使其非常适合部署在低功耗、实时和可穿戴的胎儿监测系统中。&lt;h4&gt;翻译&lt;/h4&gt;引言：本研究介绍了FetalSleepNet，这是首个已发表的使用深度学习方法从羊胎儿脑电图(EEG)分类睡眠状态的方法。胎儿EEG的获取复杂，解释起来困难且繁琐。然而，精确的睡眠阶段分类可能有助于早期发现与妊娠并发症相关的异常脑发育（如缺氧或宫内生长受限）。方法：将EEG电极固定在24只晚期妊娠胎羊的顶叶皮层硬脑膜上。使用一种为成人EEG睡眠阶段分类开发的轻量级深度神经网络，通过从成人EEG进行迁移学习来训练该网络处理羊EEG。采用基于频谱均衡的域适应策略来减少跨域不匹配。结果：我们证明了虽然直接迁移表现不佳，但完全微调结合频谱均衡取得了最佳整体性能（准确率：86.6%，宏F1分数：62.5%），优于基线模型。结论：据我们所知，FetalSleepNet是首个专门为从胎儿EEG自动睡眠阶段分类开发的深度学习框架。在实验室之外，基于EEG的睡眠阶段分类器可以作为标记引擎，支持大规模弱/半监督标记和蒸馏，促进在临床中获取的侵入性较小信号（如多普勒超声或心电图数据）的训练。FetalSleepNet的轻量级设计使其非常适合部署在低功耗、实时和可穿戴的胎儿监测系统中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Introduction: This study presents FetalSleepNet, the first published deeplearning approach to classifying sleep states from the ovineelectroencephalogram (EEG). Fetal EEG is complex to acquire and difficult andlaborious to interpret consistently. However, accurate sleep stageclassification may aid in the early detection of abnormal brain maturationassociated with pregnancy complications (e.g. hypoxia or intrauterine growthrestriction).  Methods: EEG electrodes were secured onto the ovine dura over the parietalcortices of 24 late gestation fetal sheep. A lightweight deep neural networkoriginally developed for adult EEG sleep staging was trained on the ovine EEGusing transfer learning from adult EEG. A spectral equalisation-based domainadaptation strategy was used to reduce cross-domain mismatch.  Results: We demonstrated that while direct transfer performed poorly, fullfine tuning combined with spectral equalisation achieved the best overallperformance (accuracy: 86.6 percent, macro F1-score: 62.5), outperformingbaseline models.  Conclusions: To the best of our knowledge, FetalSleepNet is the first deeplearning framework specifically developed for automated sleep staging from thefetal EEG. Beyond the laboratory, the EEG-based sleep stage classifierfunctions as a label engine, enabling large scale weak/semi supervised labelingand distillation to facilitate training on less invasive signals that can beacquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.FetalSleepNet's lightweight design makes it well suited for deployment in lowpower, real time, and wearable fetal monitoring systems.</description>
      <author>example@mail.com (Weitao Tang, Johann Vargas-Calixto, Nasim Katebi, Nhi Tran, Sharmony B. Kelly, Gari D. Clifford, Robert Galinsky, Faezeh Marzbanrad)</author>
      <guid isPermaLink="false">2509.10082v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference</title>
      <link>http://arxiv.org/abs/2509.09747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了D-CAT（解耦跨注意力迁移）框架，用于改进多模态分类模型，特别是在人机协作中的人类活动识别，解决了资源受限环境中部署的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的跨模态迁移学习方法需要在训练和推理阶段都需要成对的传感器数据，限制了在资源受限环境中的应用，因为在这些环境中完整传感器套件在经济和技术上都不实用。&lt;h4&gt;目的&lt;/h4&gt;提出一个不需要在推理阶段联合传感器模态的框架，解决资源受限环境中的多模态分类问题，同时保持准确性并减少硬件冗余。&lt;h4&gt;方法&lt;/h4&gt;D-CAT框架结合了一个用于特征提取的自注意力模块和一个新颖的跨注意力对齐损失，强制对齐传感器特征空间，而不需要两个模态的分类管道耦合。&lt;h4&gt;主要发现&lt;/h4&gt;在分布内场景中，从高性能模态迁移可获得高达10%的F1分数提升；在分布外场景中，即使较弱的源模态也能改善目标性能，只要目标模型没有在训练数据上过拟合；D-CAT通过跨模态知识实现单传感器推理，减少了感知系统的硬件冗余。&lt;h4&gt;结论&lt;/h4&gt;D-CAT框架能够在保持准确性的同时减少感知系统的硬件冗余，这对于成本敏感或适应性部署（如传感器可用性变化的家庭辅助机器人）至关重要。&lt;h4&gt;翻译&lt;/h4&gt;跨模态迁移学习用于改进多模态分类模型（例如人机协作中的人类活动识别）。然而，现有方法在训练和推理阶段都需要成对的传感器数据，限制了在资源受限环境中的部署，在这些环境中完整传感器套件在经济和技术上都不实用。为解决这一问题，我们提出了D-CAT（解耦跨注意力迁移）框架，该框架对齐模态特定表示，不需要在推理阶段联合传感器模态。我们的方法将自注意力模块用于特征提取，结合了一个新颖的跨注意力对齐损失，强制对齐传感器特征空间，而不需要两个模态分类管道的耦合。我们在三个多模态人类活动数据集（IMU、视频和音频）上评估了D-CAT，包括分布内和分布外场景，并与单模态模型进行比较。结果表明，在分布内场景中，从高性能模态（如视频到IMU）迁移可获得高达10%的F1分数提升。在分布外场景中，即使较弱的源模态（如IMU到视频）也能改善目标性能，只要目标模型没有在训练数据上过拟合。通过实现单传感器推理与跨模态知识，D-CAT减少了感知系统的硬件冗余，同时保持准确性，这对于成本敏感或适应性部署（如传感器可用性变化的家庭辅助机器人）至关重要。代码可在https://github.com/Schindler-EPFL-Lab/D-CAT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-modal transfer learning is used to improve multi-modal classificationmodels (e.g., for human activity recognition in human-robot collaboration).However, existing methods require paired sensor data at both training andinference, limiting deployment in resource-constrained environments where fullsensor suites are not economically and technically usable. To address this, wepropose Decoupled Cross-Attention Transfer (D-CAT), a framework that alignsmodality-specific representations without requiring joint sensor modalityduring inference. Our approach combines a self-attention module for featureextraction with a novel cross-attention alignment loss, which enforces thealignment of sensors' feature spaces without requiring the coupling of theclassification pipelines of both modalities. We evaluate D-CAT on threemulti-modal human activity datasets (IMU, video, and audio) under bothin-distribution and out-of-distribution scenarios, comparing against uni-modalmodels. Results show that in in-distribution scenarios, transferring fromhigh-performing modalities (e.g., video to IMU) yields up to 10% F1-score gainsover uni-modal training. In out-of-distribution scenarios, even weaker sourcemodalities (e.g., IMU to video) improve target performance, as long as thetarget model isn't overfitted on the training data. By enabling single-sensorinference with cross-modal knowledge, D-CAT reduces hardware redundancy forperception systems while maintaining accuracy, which is critical forcost-sensitive or adaptive deployments (e.g., assistive robots in homes withvariable sensor availability). Code is available athttps://github.com/Schindler-EPFL-Lab/D-CAT.</description>
      <author>example@mail.com (Leen Daher, Zhaobo Wang, Malcolm Mielle)</author>
      <guid isPermaLink="false">2509.09747v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Pipeline for Aortic Segmentation and Shape Analysis</title>
      <link>http://arxiv.org/abs/2509.09718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  STACOM 2025 with MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种稳健、全自动的主动脉形状分析流程，结合深度学习和统计技术，从心脏MRI中提取主动脉形状信息。&lt;h4&gt;背景&lt;/h4&gt;主动脉形状分析在心血管诊断、治疗规划和理解疾病进展中起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健、全自动的主动脉形状分析流程，提高分析精度和效率。&lt;h4&gt;方法&lt;/h4&gt;结合深度学习和统计技术，包括分割、3D表面重建和网格配准；对比nnUNet、TotalSegmentator和MedSAM2等分割模型；重建高质量3D网格；引入基于深度学习的网格配准方法，直接优化顶点位移。&lt;h4&gt;主要发现&lt;/h4&gt;新方法在几何精度和解剖一致性上显著优于传统方法；主成分分析揭示了主动脉形状变化的主导模式，捕获了全局形态和局部结构差异。&lt;h4&gt;结论&lt;/h4&gt;整合传统几何处理与基于学习模型的优势，为解剖精确且可扩展的主动脉分析提供基础，支持心血管医学个性化诊断的发展。&lt;h4&gt;翻译&lt;/h4&gt;主动脉形状分析在心血管诊断、治疗规划和理解疾病进展中起着关键作用。我们提出了一种稳健的、全自动的主动脉形状分析流程，从心脏MRI中结合深度学习和统计技术，包括分割、3D表面重建和网格配准。我们基准测试了领先的分割模型，包括nnUNet、TotalSegmentator和MedSAM2，强调了在特定数据集上进行领域特定训练和迁移学习的有效性。分割后，我们重建高质量的3D网格，并引入一种基于深度学习的网格配准方法，直接优化顶点位移。这种方法在几何精度和解剖一致性上显著优于传统的刚性和非刚性方法。使用配准后的网格，我们对599名健康受试者进行统计形状分析。主成分分析揭示了主动脉形状变化的主导模式，捕获了刚性和相似变换下的全局形态和局部结构差异。我们的研究结果证明了整合传统几何处理与基于学习模型的优势，用于解剖精确且可扩展的主动脉分析。这项工作为未来研究病理形状偏差和心血管医学个性化诊断的发展奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从心脏MRI图像中进行主动脉分割和形状分析的自动化流程问题。由于MRI图像中主动脉与周围组织对比度低、分辨率低，导致分割困难，且传统表面重建和配准方法存在诸多挑战。这个问题在现实中很重要，因为准确的主动脉形状分析对心血管疾病诊断、治疗规划和疾病进展监测至关重要，能帮助建立健康参考模型以检测病理性偏差，支持个性化医疗发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为三个主要步骤：分割、3D表面重建和网格配准，针对每个步骤设计解决方案。他们系统评估了现有技术，包括nn-UNet、TotalSegmentator等分割模型，以及传统表面重建和配准方法。在此基础上，作者借鉴了现有的深度学习分割模型和经典几何处理算法，并在网格配准方面提出了创新，设计了一个结合统计方法和深度学习技术的集成管道。这种设计思路体现了对现有技术的批判性评估和有针对性的改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个全自动化的管道，结合深度学习和统计技术，通过领域特定训练和迁移学习提高分割性能，使用基于深度学习的网格配准方法直接优化顶点位移以实现更高精度，并利用统计形状分析揭示主动脉形状变化的主导模式。整体流程包括：数据准备与预处理；多种分割模型评估与选择；3D表面重建（等值面提取、顶点标准化、法线处理、网格重建）；模板选择与对齐；网格配准（比较传统方法并提出深度学习方法）；最后对599名健康受试者进行主成分分析以识别形状变化模式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了完整的端到端自动化管道；2) 系统评估并优化了分割方法，发现nn-UNet表现最佳；3) 创新性地提出基于深度学习的网格配准方法，直接优化顶点位移；4) 在大型健康队列上进行统计分析，识别主动脉形状变化模式。相比之前工作，本文提供了更全面的解决方案，在配准精度上显著优于传统方法，使用了更大的数据集进行统计分析，并成功整合了传统几何处理与基于学习的方法，实现了既解剖精确又可扩展的主动脉分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合深度学习和统计技术的全自动管道，实现了从心脏MRI中精确分割、重建和配准主动脉，并揭示了健康人群主动脉形状变化的主要模式，为心血管疾病的个性化诊断提供了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aortic shape analysis plays a key role in cardiovascular diagnostics,treatment planning, and understanding disease progression. We present a robust,fully automated pipeline for aortic shape analysis from cardiac MRI, combiningdeep learning and statistical techniques across segmentation, 3D surfacereconstruction, and mesh registration. We benchmark leading segmentation modelsincluding nnUNet, TotalSegmentator, and MedSAM2 highlighting the effectivenessof domain specific training and transfer learning on a curated dataset.Following segmentation, we reconstruct high quality 3D meshes and introduce aDL based mesh registration method that directly optimises vertex displacements.This approach significantly outperforms classical rigid and nonrigid methods ingeometric accuracy and anatomical consistency. Using the registered meshes, weperform statistical shape analysis on a cohort of 599 healthy subjects.Principal Component Analysis reveals dominant modes of aortic shape variation,capturing both global morphology and local structural differences under rigidand similarity transformations. Our findings demonstrate the advantages ofintegrating traditional geometry processing with learning based models foranatomically precise and scalable aortic analysis. This work lays thegroundwork for future studies into pathological shape deviations and supportsthe development of personalised diagnostics in cardiovascular medicine.</description>
      <author>example@mail.com (Nairouz Shehata, Amr Elsawy, Mohamed Nagy, Muhammad ElMahdy, Mariam Ali, Soha Romeih, Heba Aguib, Magdi Yacoub, Ben Glocker)</author>
      <guid isPermaLink="false">2509.09718v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets</title>
      <link>http://arxiv.org/abs/2509.10453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过采用三种先进的时序自监督学习方法处理3D脑部MRI数据，解决了阿尔茨海默病预测中标记数据缺乏、跨数据集泛化能力差以及输入灵活性不足的问题。模型在四个数据集的3161名患者上进行了预训练，并在多种预测任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病是一种导致记忆丧失和认知能力下降的进行性神经退行性疾病。现有的深度学习模型应用于阿尔茨海默病预测时受限于标记数据缺乏、跨数据集泛化能力差，以及无法处理不同数量的输入扫描和扫描间时间间隔。&lt;h4&gt;目的&lt;/h4&gt;适应三种先进的时序自监督学习方法用于3D脑部MRI分析，并添加新型扩展以处理可变长度输入和鲁棒空间特征学习。&lt;h4&gt;方法&lt;/h4&gt;聚合四个公开数据集，包含3161名患者进行预训练，评估模型在阿尔茨海默病多种预测任务中的性能，包括诊断分类、转化检测和未来转化预测。&lt;h4&gt;主要发现&lt;/h4&gt;采用时序顺序预测和对比学习的自监督学习模型在七个下游任务中的六个表现优于监督学习。该模型展示了跨任务和不同数量输入图像（具有不同时间间隔）的适应性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该自监督学习模型在临床应用中表现出强大的适应性和鲁棒性能，作者已公开代码和模型供进一步研究使用。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病是一种进行性神经退行性疾病，会导致记忆丧失和认知能力下降。尽管已有大量研究将深度学习模型应用于阿尔茨海默病预测任务，但这些模型仍受限于可用标记数据的缺乏、跨数据集的泛化能力差，以及无法处理不同数量的输入扫描和扫描间时间间隔。在本研究中，我们调整了三种最先进的时序自学习方法用于3D脑部MRI分析，并添加了新型扩展以处理可变长度输入和鲁棒空间特征学习。我们聚合了四个包含3161名患者的公开数据集进行预训练，展示了我们的模型在多种阿尔茨海默病预测任务中的性能，包括诊断分类、转化检测和未来转化预测。重要的是，我们采用时序顺序预测和对比学习的自监督模型在七个下游任务中的六个上优于监督学习。它展示了跨任务和不同数量输入图像（具有不同时间间隔）的适应性和泛化能力，突显了其在临床应用中保持鲁棒性能的能力。我们在https://github.com/emilykaczmarek/SSL-AD公开了我们的代码和模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决阿尔茨海默病预测中深度学习模型的三大局限性：缺乏足够标记数据、模型在不同数据集间泛化能力差、无法灵活处理不同数量的输入扫描图像和时间间隔。这个问题在现实中非常重要，因为阿尔茨海默病是一种进行性疾病，早期准确预测对治疗干预至关重要，而现有模型受限于数据不足和临床场景的多样性，难以充分利用患者多次就诊的完整信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到阿尔茨海默病作为随时间发展的疾病，其预测需要模型能捕捉时间变化特征。他们发现自监督学习可解决标记数据不足问题，但现有SSL方法在阿尔茨海默病应用中存在三大局限。因此，他们借鉴了计算机视觉领域的时间顺序验证和预测方法，结合对比学习技术，设计了三种模型：时间顺序验证(SSL-TOV)、时间顺序预测(SSL-TOP)和结合对比学习的SSL-TOPC。他们创新性地扩展了这些方法以处理医学影像的特殊性，如可变长度输入和3D脑MRI分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自监督学习从大量未标记脑MRI数据中学习通用表示，利用时间顺序预测任务使模型关注疾病进展的微妙变化，并结合对比学习增强空间特征鲁棒性。整体流程包括：1)聚合四个公开数据集共3,161名患者进行预训练；2)对数据进行标准化预处理；3)设计三种模型分别进行时间顺序验证、预测和结合对比学习；4)在七个下游任务(如分类、转换检测、未来转换预测)上评估模型性能。模型能处理1-4个输入图像，适应不同时间间隔，并在大多数任务上优于监督学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)整合四个大规模数据集进行预训练；2)创新性地将时间顺序预测应用于3D脑MRI分析；3)设计专门方法处理可变长度输入，无需填充；4)结合时间顺序预测和对比学习增强时空特征；5)在七个不同任务上全面评估模型性能。相比之前工作，本研究首次提出完全可适应、可泛化的自监督框架，解决了现有SSL模型缺乏大型数据集、忽略时间学习和无法处理可变输入的问题，特别是SSL-TOPC模型在六个 out of seven 的下游任务上超越了监督学习基线。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的时空自监督学习框架，通过整合大规模脑MRI数据和时间顺序预测任务，显著提高了阿尔茨海默病预测模型的泛化能力和适应性，特别是在标记数据有限和临床应用场景多变的情况下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alzheimer's disease is a progressive, neurodegenerative disorder that causesmemory loss and cognitive decline. While there has been extensive research inapplying deep learning models to Alzheimer's prediction tasks, these modelsremain limited by lack of available labeled data, poor generalization acrossdatasets, and inflexibility to varying numbers of input scans and timeintervals between scans. In this study, we adapt three state-of-the-arttemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,and add novel extensions designed to handle variable-length inputs and learnrobust spatial features. We aggregate four publicly available datasetscomprising 3,161 patients for pre-training, and show the performance of ourmodel across multiple Alzheimer's prediction tasks including diagnosisclassification, conversion detection, and future conversion prediction.Importantly, our SSL model implemented with temporal order prediction andcontrastive learning outperforms supervised learning on six out of sevendownstream tasks. It demonstrates adaptability and generalizability acrosstasks and number of input images with varying time intervals, highlighting itscapacity for robust performance across clinical applications. We release ourcode and model publicly at https://github.com/emilykaczmarek/SSL-AD.</description>
      <author>example@mail.com (Emily Kaczmarek, Justin Szeto, Brennan Nichyporuk, Tal Arbel)</author>
      <guid isPermaLink="false">2509.10453v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms</title>
      <link>http://arxiv.org/abs/2509.10369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review at npj Digital Medicine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;对比学习对队列组成的依赖性是一个重要但未被充分探索的问题。CAPE基础模型和IDB策略解决了多中心、多样化队列预训练中的挑战，队列组成对模型性能有显著影响，需要考虑临床公平性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;对比学习是一种广泛采用的自我监督预训练策略，但其对队列组成的依赖性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;系统评估队列人口统计、健康状态和人口多样性如何影响下游预测性能，开发一种临床公平且可推广的基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出了基于患者增强心电图(CAPE)的基础模型，在四个队列(n = 5,203,352)上进行预训练，这些队列来自三个大洲(北美、南美、亚洲)的多样化人群；评估了两个来自欧洲的额外队列的预测任务；提出了In-Distribution Batch (IDB)策略，在预训练期间保留队列内部一致性并增强OOD鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;下游性能取决于预训练队列的分布特性，包括人口统计和健康状态；使用多中心、人口统计多样化的队列进行预训练可以提高分布内准确率，但通过编码队列特定的人工制品，降低了对比方法的分布外(OOD)泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作为开发临床公平且可推广的基础模型提供了重要见解。&lt;h4&gt;翻译&lt;/h4&gt;对比学习是一种广泛采用的自我监督预训练策略，但其对队列组成的依赖性仍未得到充分探索。我们提出了基于患者增强心电图(CAPE)的基础模型，并在来自三个大洲(北美、南美、亚洲)的多样化人群中，在四个队列(n = 5,203,352)上进行预训练。我们系统评估了队列人口统计、健康状态和人口多样性如何影响预测任务的下游性能，这些任务还包括来自另一个大陆(欧洲)的两个额外队列。我们发现下游性能取决于预训练队列的分布特性，包括人口统计和健康状态。此外，虽然使用多中心、人口统计多样化的队列进行预训练可以提高分布内准确率，但它通过编码队列特定的人工制品，降低了我们对比方法的分布外(OOD)泛化能力。为解决这一问题，我们提出了In-Distribution Batch (IDB)策略，该策略在预训练期间保留队列内部一致性并增强OOD鲁棒性。这项工作为开发临床公平且可推广的基础模型提供了重要见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning is a widely adopted self-supervised pretrainingstrategy, yet its dependence on cohort composition remains underexplored. Wepresent Contrasting by Patient Augmented Electrocardiograms (CAPE) foundationmodel and pretrain on four cohorts (n = 5,203,352), from diverse populationsacross three continents (North America, South America, Asia). We systematicallyassess how cohort demographics, health status, and population diversityinfluence the downstream performance for prediction tasks also including twoadditional cohorts from another continent (Europe). We find that downstreamperformance depends on the distributional properties of the pretraining cohort,including demographics and health status. Moreover, while pretraining with amulti-centre, demographically diverse cohort improves in-distribution accuracy,it reduces out-of-distribution (OOD) generalisation of our contrastive approachby encoding cohort-specific artifacts. To address this, we propose theIn-Distribution Batch (IDB) strategy, which preserves intra-cohort consistencyduring pretraining and enhances OOD robustness. This work provides importantinsights for developing clinically fair and generalisable foundation models.</description>
      <author>example@mail.com (Gul Rukh Khattak, Konstantinos Patlatzoglou, Joseph Barker, Libor Pastika, Boroumand Zeidaabadi, Ahmed El-Medany, Hesham Aggour, Yixiu Liang, Antonio H. Ribeiro, Jeffrey Annis, Antonio Luiz Pinho Ribeiro, Junbo Ge, Daniel B. Kramer, Jonathan W. Waks, Evan Brittain, Nicholas Peters, Fu Siong Ng, Arunashis Sau)</author>
      <guid isPermaLink="false">2509.10369v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography</title>
      <link>http://arxiv.org/abs/2509.10344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLAM的新型深度学习方法，用于改进乳腺X光筛查中的视觉语言模型预训练，通过利用多视图成像过程的先验知识，学习局部跨视图对齐和细粒度特征，在多个数据集上超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;乳腺X光筛查是早期发现乳腺癌的重要工具，深度学习可提高其解释速度和准确性。然而，视觉语言模型的发展受限于数据有限和自然图像与医学图像间的领域差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效建模乳腺X光多视图关系的视觉语言模型，解决现有方法忽略领域特定特征和多视图关系的问题。&lt;h4&gt;方法&lt;/h4&gt;提出GLAM模型：全局和局部对齐的多视图乳腺X光模型，利用乳腺X光多视图成像过程的先验知识，通过联合全局和局部、视觉-视觉以及视觉语言对比学习来学习局部跨视图对齐和细粒度局部特征，并在EMBED数据集上预训练。&lt;h4&gt;主要发现&lt;/h4&gt;GLAM模型在不同设置下的多个数据集上均优于现有基线方法，表明其能有效捕捉乳腺X光的多视图关系和领域特定特征。&lt;h4&gt;结论&lt;/h4&gt;通过利用乳腺X光成像过程的先验知识和多视图关系，GLAM模型改进了视觉语言模型在乳腺X光筛查中的应用效果，为乳腺癌早期检测提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;乳腺X光筛查是早期发现乳腺癌的重要工具。深度学习方法有望提高乳腺X光解释的速度和准确性。然而，基础视觉语言模型(VLM)的发展受到数据有限和自然图像与医学图像之间领域差异的阻碍。现有的乳腺X光VLM通常从自然图像改编而来，忽略了多视图等特定领域特征。与放射科医生不同，当前方法将两个视图视为独立图像或未正确建模多视图对应关系学习，导致失去关键几何上下文和次优预测。我们提出GLAM：利用几何引导进行VLM预训练的全局和局部对齐多视图乳腺X光模型。通过利用乳腺X光多视图成像过程的先验知识，我们的模型通过联合全局和局部、视觉-视觉以及视觉语言对比学习，学习局部跨视图对齐和细粒度局部特征。在最大的开放乳腺X光数据集之一EMBED上预训练后，我们的模型在不同设置下的多个数据集上都优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mammography screening is an essential tool for early detection of breastcancer. The speed and accuracy of mammography interpretation have the potentialto be improved with deep learning methods. However, the development of afoundation visual language model (VLM) is hindered by limited data and domaindifferences between natural and medical images. Existing mammography VLMs,adapted from natural images, often ignore domain-specific characteristics, suchas multi-view relationships in mammography. Unlike radiologists who analyzeboth views together to process ipsilateral correspondence, current methodstreat them as independent images or do not properly model the multi-viewcorrespondence learning, losing critical geometric context and resulting insuboptimal prediction. We propose GLAM: Global and Local Alignment forMulti-view mammography for VLM pretraining using geometry guidance. Byleveraging the prior knowledge about the multi-view imaging process ofmammograms, our model learns local cross-view alignments and fine-grained localfeatures through joint global and local, visual-visual, and visual-languagecontrastive learning. Pretrained on EMBED [14], one of the largest openmammography datasets, our model outperforms baselines across multiple datasetsunder different settings.</description>
      <author>example@mail.com (Yuexi Du, Lihui Chen, Nicha C. Dvornek)</author>
      <guid isPermaLink="false">2509.10344v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion</title>
      <link>http://arxiv.org/abs/2509.10266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SignClip是一种新的手语翻译框架，通过融合手动和非手动线索（空间手势和嘴唇运动特征），并采用层次对比学习框架，显著提高了手语翻译的准确性。&lt;h4&gt;背景&lt;/h4&gt;手语翻译旨在从手语视频中翻译自然语言，作为包容性交流的重要桥梁。最近的进展利用了强大的视觉骨干网络和大语言模型，但大多数方法主要关注手动信号（手部动作）而往往忽略非手动线索如口型。&lt;h4&gt;目的&lt;/h4&gt;提出SignClip框架，提高手语翻译的准确性。&lt;h4&gt;方法&lt;/h4&gt;SignClip融合手动和非手动线索，特别是空间手势和嘴唇运动特征。此外，SignClip引入了具有多级对齐目标的层次对比学习框架，确保手语-嘴唇和视觉-文本模态之间的语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在PHOENIX14T和How2Sign两个基准数据集上的大量实验证明了该方法的优势。例如，在PHOENIX14T上，Gloss-free设置中，SignClip超越了之前的最先进模型SpaMo，将BLEU-4从24.32提高到24.71，ROUGE从46.57提高到48.38。&lt;h4&gt;结论&lt;/h4&gt;SignClip通过融合手势和口型特征，并采用层次对比学习框架，显著提高了手语翻译的准确性。&lt;h4&gt;翻译&lt;/h4&gt;手语翻译旨在从手语视频中翻译自然语言，作为包容性交流的重要桥梁。尽管最近的进展利用了强大的视觉骨干网络和大语言模型，但大多数方法主要关注手动信号（手部动作）而往往忽略非手动线索如口型。事实上，口型在手语中传达重要的语言信息，并在区分视觉相似的手势中起关键作用。本文提出SignClip，一种新的手语翻译框架，通过融合手动和非手动线索（特别是空间手势和嘴唇运动特征）来提高手语翻译的准确性。此外，SignClip引入了具有多级对齐目标的层次对比学习框架，确保手语-嘴唇和视觉-文本模态之间的语义一致性。在PHOENIX14T和How2Sign两个基准数据集上的大量实验证明了该方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language translation (SLT) aims to translate natural language from signlanguage videos, serving as a vital bridge for inclusive communication. Whilerecent advances leverage powerful visual backbones and large language models,most approaches mainly focus on manual signals (hand gestures) and tend tooverlook non-manual cues like mouthing. In fact, mouthing conveys essentiallinguistic information in sign languages and plays a crucial role indisambiguating visually similar signs. In this paper, we propose SignClip, anovel framework to improve the accuracy of sign language translation. It fusesmanual and non-manual cues, specifically spatial gesture and lip movementfeatures. Besides, SignClip introduces a hierarchical contrastive learningframework with multi-level alignment objectives, ensuring semantic consistencyacross sign-lip and visual-text modalities. Extensive experiments on twobenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of ourapproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClipsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from24.32 to 24.71, and ROUGE from 46.57 to 48.38.</description>
      <author>example@mail.com (Wenfang Wu, Tingting Yuan, Yupeng Li, Daling Wang, Xiaoming Fu)</author>
      <guid isPermaLink="false">2509.10266v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning</title>
      <link>http://arxiv.org/abs/2509.10208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SI FACT的自我改进框架，用于解决大型语言模型在知识密集型任务中生成不忠实响应的问题。该框架通过自我指导机制生成对比学习数据，并应用对比学习训练模型，显著提高了模型的上下文忠实性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在知识密集型任务中经常生成不忠实的响应，这是因为知识冲突，即模型倾向于依赖内部参数知识而非提供的上下文。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在知识密集型任务中由于知识冲突而产生的不忠实响应问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Self Improving Faithfulness Aware Contrastive Tuning(SI FACT)的自我改进框架，该框架使用自我指导机制让基础LLM自动生成高质量的、结构化的对比学习数据，包括锚样本、语义等价正样本和模拟不忠实场景的负样本，然后应用对比学习训练模型，使其在表示空间中拉近忠实响应，推远不忠实响应。&lt;h4&gt;主要发现&lt;/h4&gt;在知识冲突评估基准ECARE KRE和COSE KRE上的实验表明，基于Llama3 8B Instruct的SI FACT模型比最佳基线方法提高了6.2%的上下文召回率，同时显著减少了对内部记忆的依赖。&lt;h4&gt;结论&lt;/h4&gt;SI FACT在增强LLM的上下文忠实性方面提供了有效性和高数据效率，为构建更加主动和可信赖的语言模型提供了实用途径。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在知识密集型任务中经常因知识冲突而产生不忠实响应，即倾向于依赖内部参数知识而非提供的上下文。为解决这一问题，我们提出了一种新颖的自我改进框架——自我提高忠实性感知对比调优(SI FACT)。该框架使用自我指导机制，使基础LLM能够自动生成高质量、结构化的对比学习数据，包括锚样本、语义等价正样本和模拟不忠实场景的负样本。这种方法显著降低了手动标注成本。随后，应用对比学习训练模型，使其在表示空间中拉近忠实响应，推远不忠实响应。在知识冲突评估基准ECARE KRE和COSE KRE上的实验表明，基于Llama3 8B Instruct的SI FACT模型比最佳基线方法提高了6.2%的上下文召回率，同时显著减少了对内部记忆的依赖。结果表明，SI FACT在增强LLM的上下文忠实性方面提供了强大的有效性和高数据效率，为构建更加主动和可信赖的语言模型提供了实用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models often generate unfaithful responses in knowledgeintensive tasks due to knowledge conflict,that is,a preference for relying oninternal parametric knowledge rather than the provided context.To address thisissue,we propose a novel self improving framework,Self Improving FaithfulnessAware Contrastive Tuning.The framework uses a self instruct mechanism thatallows the base LLM to automatically generate high quality,structuredcontrastive learning data,including anchor samples,semantically equivalentpositive samples,and negative samples simulating unfaithful scenarios.Thisapproach significantly reduces the cost of manualannotation.Subsequently,contrastive learning is applied to train themodel,enabling it to pull faithful responses closer and push unfaithfulresponses farther apart in the representation space.Experiments on knowledgeconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACTmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%over the best baseline method,while significantly reducing dependence oninternal memory.The results indicate that SI FACT provides strong effectivenessand high data efficiency in enhancing the contextual faithfulness ofLLMs,offering a practical pathway toward building more proactive andtrustworthy language models.</description>
      <author>example@mail.com (Shengqiang Fu)</author>
      <guid isPermaLink="false">2509.10208v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment</title>
      <link>http://arxiv.org/abs/2509.10134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Grad-CL的新型无源域适应框架，用于解决视盘和视杯分割在不同成像条件下的适应性问题。该框架结合了梯度引导的伪标签细化和基于余弦相似度的对比学习策略，无需访问原始源数据即可实现稳健的分割性能。&lt;h4&gt;背景&lt;/h4&gt;视盘和视杯的准确分割对于眼科疾病（如青光眼）的早期诊断和管理至关重要。然而，在一个数据集上训练的分割模型在应用于不同成像协议或条件下获取的目标数据时，往往会经历显著的性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决分割模型在不同成像条件下性能下降的问题，提出一种无需访问原始源数据的无源域适应框架，实现视盘和视杯分割的稳健适应。&lt;h4&gt;方法&lt;/h4&gt;Grad-CL框架结合两个主要阶段：第一阶段通过基于梯度的机制提取显著的类别特定特征，实现更准确的不确定性量化和稳健的原型估计，用于细化有噪声的伪标签；第二阶段采用基于余弦相似度的对比损失，明确强制执行视杯和视盘的梯度引导特征之间的类间可分离性。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的跨域眼底成像数据集上进行的大量实验表明，Grad-CL优于最先进的无监督和无源域适应方法，实现了 superior 的分割精度和改进的边界描绘。&lt;h4&gt;结论&lt;/h4&gt;Grad-CL是一种有效的无源域适应方法，可以解决视盘和视杯分割在不同成像条件下的适应性问题，无需原始源数据。&lt;h4&gt;翻译&lt;/h4&gt;准确的视盘和视杯分割对于眼科疾病（如青光眼）的早期诊断和管理至关重要。然而，在一个数据集上训练的分割模型在应用于不同成像协议或条件下获取的目标数据时，往往会经历显著的性能下降。为应对这一挑战，我们提出Grad-CL，一种新颖的无源域适应框架，它利用预训练的源模型和无标签的目标数据，无需访问原始源数据即可稳健地适应分割性能。Grad-CL结合了基于梯度引导的伪标签细化模块和基于余弦相似度的对比学习策略。在第一阶段，通过基于梯度的机制提取显著的类别特定特征，实现更准确的不确定性量化和稳健的原型估计，用于细化有噪声的伪标签。在第二阶段，采用基于余弦相似度的对比损失，明确强制执行视杯和视盘的梯度引导特征之间的类间可分离性。在具有挑战性的跨域眼底成像数据集上进行的大量实验表明，Grad-CL优于最先进的无监督和无源域适应方法，实现了 superior 的分割精度和改进的边界描绘。项目和代码可在https://visdomlab.github.io/GCL/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of the optic disc and cup is critical for the earlydiagnosis and management of ocular diseases such as glaucoma. However,segmentation models trained on one dataset often suffer significant performancedegradation when applied to target data acquired under different imagingprotocols or conditions. To address this challenge, we propose\textbf{Grad-CL}, a novel source-free domain adaptation framework thatleverages a pre-trained source model and unlabeled target data to robustlyadapt segmentation performance without requiring access to the original sourcedata. Grad-CL combines a gradient-guided pseudolabel refinement module with acosine similarity-based contrastive learning strategy. In the first stage,salient class-specific features are extracted via a gradient-based mechanism,enabling more accurate uncertainty quantification and robust prototypeestimation for refining noisy pseudolabels. In the second stage, a contrastiveloss based on cosine similarity is employed to explicitly enforce inter-classseparability between the gradient-informed features of the optic cup and disc.Extensive experiments on challenging cross-domain fundus imaging datasetsdemonstrate that Grad-CL outperforms state-of-the-art unsupervised andsource-free domain adaptation methods, achieving superior segmentation accuracyand improved boundary delineation. Project and code are available athttps://visdomlab.github.io/GCL/.</description>
      <author>example@mail.com (Rini Smita Thakur, Rajeev Ranjan Dwivedi, Vinod K Kurmi)</author>
      <guid isPermaLink="false">2509.10134v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Prototypical Contrastive Learning For Improved Few-Shot Audio Classification</title>
      <link>http://arxiv.org/abs/2509.10074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and Presented at IEEE International Workshop on Machine  Learning for Signal Processing, Aug.\ 31-- Sep.\ 3, 2025, Istanbul, Turkey ,  6 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探索了在音频分类中将监督对比损失整合到原型小样本训练中的方法，通过结合SpecAugment和自注意力机制，实现了在5路5 shot设置下的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;小样本学习已成为处理标记数据有限场景的有效方法，但在图像领域已有大量研究，而音频分类中的小样本学习仍然相对未被充分探索。大规模数据标注在某些场景下不切实际，因此需要开发小样本学习方法。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探索监督对比损失在音频分类小样本学习中的应用，并比较不同对比损失（标准对比损失与角度损失）的性能，同时开发一种结合SpecAugment和自注意力机制的新方法。&lt;h4&gt;方法&lt;/h4&gt;研究将监督对比损失整合到原型小样本训练框架中，特别采用了角度损失而非标准对比损失。方法利用SpecAugment进行数据增强，然后通过自注意力机制将增强输入版本的多样化信息封装到一个统一的嵌入中。实验在MetaAudio基准数据集上进行，该数据集包含五个预定义分割的标准化数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，与标准对比损失相比，角度损失能进一步提高性能。提出的结合SpecAugment和自注意力机制的方法在5路5 shot设置下实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;将监督对比损失（特别是角度损失）整合到原型小样本训练中，结合SpecAugment和自注意力机制，是音频分类小样本学习的有效方法，能够达到当前最先进的性能水平。&lt;h4&gt;翻译&lt;/h4&gt;小样本学习已成为一种强大的范式，用于训练标记数据有限的模型，解决了大规模标注不切实际场景中的挑战。尽管图像领域已有大量研究，但音频分类中的小样本学习仍然相对未被探索。在这项工作中，我们研究了将监督对比损失整合到原型小样本训练中对音频分类的影响。具体而言，我们证明与标准对比损失相比，角度损失能进一步提高性能。我们的方法利用SpecAugment，然后通过自注意力机制，将增强输入版本的多样化信息封装到一个统一的嵌入中。我们在MetaAudio上评估了我们的方法，这是一个包含五个数据集的基准，具有预定义的分割、标准化的预处理和一套全面的用于比较的小样本学习模型。在5路、5 shot设置下，我们提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot learning has emerged as a powerful paradigm for training models withlimited labeled data, addressing challenges in scenarios where large-scaleannotation is impractical. While extensive research has been conducted in theimage domain, few-shot learning in audio classification remains relativelyunderexplored. In this work, we investigate the effect of integratingsupervised contrastive loss into prototypical few shot training for audioclassification. In detail, we demonstrate that angular loss further improvesthe performance compared to the standard contrastive loss. Our method leveragesSpecAugment followed by a self-attention mechanism to encapsulate diverseinformation of augmented input versions into one unified embedding. We evaluateour approach on MetaAudio, a benchmark including five datasets with predefinedsplits, standardized preprocessing, and a comprehensive set of few-shotlearning models for comparison. The proposed approach achieves state-of-the-artperformance in a 5-way, 5-shot setting.</description>
      <author>example@mail.com (Christos Sgouropoulos, Christos Nikou, Stefanos Vlachos, Vasileios Theiou, Christos Foukanelis, Theodoros Giannakopoulos)</author>
      <guid isPermaLink="false">2509.10074v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>On Syntactical Simplification of Temporal Operators in Negation-free MTL</title>
      <link>http://arxiv.org/abs/2509.10146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了无否定MTL时态逻辑框架的表达能力，发现可以仅使用'until'和'since'算子构建强大的时态逻辑片段，挑战了否定对于表达普遍时态约束的必要性假设。&lt;h4&gt;背景&lt;/h4&gt;在动态、数据密集型环境中进行时态推理需要表达性强且可处理的逻辑框架。传统方法依赖否定表达缺失或矛盾，但在开放分布式系统如物联网网络或语义网中，由于数据不完整和异步性，否定失败语义变得不可靠。&lt;h4&gt;目的&lt;/h4&gt;研究无否定MTL（时态逻辑框架）的表达能力，该框架用于基于规则的时态推理。&lt;h4&gt;方法&lt;/h4&gt;展示如何使用'once'、'since'和'until'算子消除MTL中的'always'算子，并进一步展示如何移除'once'算子，仅基于'until'和'since'构建片段。&lt;h4&gt;主要发现&lt;/h4&gt;'always'算子和'once'算子都可以被消除，可以仅使用'until'和'since'算子构建一个能够捕获存在性和不变时态模式的强大片段。&lt;h4&gt;结论&lt;/h4&gt;挑战了否定对于表达普遍时态约束的必要性假设，揭示了能够捕获存在性和不变时态模式的强大片段，结果导致MTL语法的简化，为理论研究和实现工作带来好处。&lt;h4&gt;翻译&lt;/h4&gt;在动态、数据密集型环境中的时态推理越来越需要表达性强且可处理的逻辑框架。传统方法通常依赖否定来表达缺失或矛盾。在这种情况下，否定失败常用于从缺乏积极证据中推断负面信息。然而，在物联网网络或语义网等开放和分布式系统中，由于数据不完整和异步性，否定失败语义变得不可靠。这导致了对无否定时态规则系统片段的日益关注，这些片段保持单调性并支持可扩展的推理。本文研究了无否定MTL的表达能力，这是一种为时间上的基于规则推理而设计的时态逻辑框架。我们展示了MTL的'always'算子通常被视为其他时态结构的语法糖，可以使用'once'、'since'和'until'算子消除。值得注意的是，即使'once'算子也可以被移除，产生一个仅基于'until'和'since'的片段。这些结果挑战了否定对于表达普遍时态约束的必要性假设，并揭示了一个能够捕获存在性和不变时态模式的强大片段。此外，这些结果导致了MTL语法的简化，这可以为理论研究以及实现工作带来好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning in dynamic, data-intensive environments increasinglydemands expressive yet tractable logical frameworks. Traditional approachesoften rely on negation to express absence or contradiction. In such contexts,Negation-as-Failure is commonly used to infer negative information from thelack of positive evidence. However, open and distributed systems such as IoTnetworks or the Semantic Web Negation-as-Failure semantics become unreliabledue to incomplete and asynchronous data. This has led to a growing interest innegation-free fragments of temporal rule-based systems, which preservemonotonicity and enable scalable reasoning.  This paper investigates the expressive power of negation-free MTL, a temporallogic framework designed for rule-based reasoning over time. We show that the"always" operators of MTL, often treated as syntactic sugar for combinations ofother temporal constructs, can be eliminated using "once", "since" and "until"operators. Remarkably, even the "once" operators can be removed, yielding afragment based solely on "until" and "since". These results challenge theassumption that negation is necessary for expressing universal temporalconstraints, and reveal a robust fragment capable of capturing both existentialand invariant temporal patterns. Furthermore, the results induce a reduction inthe syntax of MTL, which in turn can provide benefits for both theoreticalstudy as well as implementation efforts.</description>
      <author>example@mail.com (Mathijs van Noort, Femke Ongenae, Pieter Bonte)</author>
      <guid isPermaLink="false">2509.10146v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>MAESTRO: Multi-modal Adaptive Estimation for Temporal Respiratory Disease Outbreak</title>
      <link>http://arxiv.org/abs/2509.08578v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MAESTRO框架，一种用于流感发病率预测的新型统一方法，通过整合多模态数据和先进的谱时建模实现了高准确率的预测。&lt;h4&gt;背景&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测流感发病率的方法，为公共卫生决策提供支持。&lt;h4&gt;方法&lt;/h4&gt;提出MAESTRO框架，整合先进的谱时建模与多模态数据融合（包括监测数据、网络搜索趋势和气象数据），通过自适应加权异构数据源和分解复杂时间序列模式实现预测。&lt;h4&gt;主要发现&lt;/h4&gt;在香港11年以上的流感数据评估中，MAESTRO达到了0.956的R平方值，展示了最先进的预测性能；消融实验证实了多模态和谱时成分的重要贡献。&lt;h4&gt;结论&lt;/h4&gt;MAESTRO是一种模块化、可复现的预测工具，已公开可用，可部署到其他地区和病原体，为流行病学预测提供了强大工具。&lt;h4&gt;翻译&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。本文提出了MAESTRO（多模态时间呼吸道疾病爆发自适应估计），一种新颖的统一框架，该框架协同整合了先进的谱时建模与多模态数据融合，包括监测数据、网络搜索趋势和气象数据。通过自适应加权异构数据源和分解复杂时间序列模式，该模型实现了稳健且准确的预测。在香港11年以上的流感数据评估中（排除COVID-19期间），MAESTRO展示了最先进的性能，实现了0.956的R平方值的优越模型拟合。大量的消融实验证实了其多模态和谱时成分的重要贡献。模块化和可复现的管道已公开可用，便于部署和扩展到其他地区和病原体，为流行病学预测提供了强大工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely and robust influenza incidence forecasting is critical for publichealth decision-making. This paper presents MAESTRO (Multi-modal AdaptiveEstimation for Temporal Respiratory Disease Outbreak), a novel, unifiedframework that synergistically integrates advanced spectro-temporal modelingwith multi-modal data fusion, including surveillance, web search trends, andmeteorological data. By adaptively weighting heterogeneous data sources anddecomposing complex time series patterns, the model achieves robust andaccurate forecasts. Evaluated on over 11 years of Hong Kong influenza data(excluding the COVID-19 period), MAESTRO demonstrates state-of-the-artperformance, achieving a superior model fit with an R-square of 0.956.Extensive ablations confirm the significant contributions of its multi-modaland spectro-temporal components. The modular and reproducible pipeline is madepublicly available to facilitate deployment and extension to other regions andpathogens, presenting a powerful tool for epidemiological forecasting.</description>
      <author>example@mail.com (Hong Liu, Kerui Cen, Yanxing Chen, Zige Liu, Dong Chen, Zifeng Yang, Chitin Hon)</author>
      <guid isPermaLink="false">2509.08578v2</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Multiscaling in Wasserstein Spaces</title>
      <link>http://arxiv.org/abs/2509.10415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的多尺度框架，用于分析欧几里得域上Wasserstein空间中的概率测度序列。该框架利用最优传输的内在几何结构，构建了适用于绝对连续和离散测度的多尺度变换，并引入了最优性数量来量化序列与Wasserstein测地线的偏差，从而检测不规则动态和异常。&lt;h4&gt;背景&lt;/h4&gt;在欧几里得域上的Wasserstein空间中分析概率测度序列是当前研究的一个挑战领域，特别是需要同时处理绝对连续和离散测度的情况。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效分析概率测度序列的多尺度框架，检测不规则动态和异常，并提供稳健且可解释的多尺度表示。&lt;h4&gt;方法&lt;/h4&gt;研究基于最优传输的内在几何结构构建多尺度变换，核心是基于McCann插值的细化算子，该算子保留了测度流的测地线结构并作为上采样机制。此外，引入了最优性数量这一标量，用于量化序列与Wasserstein测地线的偏差。&lt;h4&gt;主要发现&lt;/h4&gt;研究建立了关键的理论保证，包括变换的稳定性和系数的几何衰减，确保了多尺度表示的稳健性和可解释性。通过数值实验展示了该方法在高斯流去噪和异常检测、向量场下点云动态分析以及神经网络学习轨迹多尺度表征方面的多功能性。&lt;h4&gt;结论&lt;/h4&gt;该多尺度框架为分析概率测度序列提供了一种强大而灵活的工具，能够有效检测不规则动态和异常，并在多种应用场景中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的多尺度框架，用于分析欧几里得域上Wasserstein空间中的概率测度序列。利用最优传输的内在几何结构，我们构建了适用于绝对连续和离散测度的多尺度变换。我们方法的核心是基于McCann插值的细化算子，它保留了测度流的测地线结构并作为上采样机制。在此基础上，我们引入了最优性数量，这是一个标量，用于量化序列跨尺度偏离Wasserstein测地线的程度，从而能够检测不规则动态和异常。我们建立了关键的理论保证，包括变换的稳定性和系数的几何衰减，确保了多尺度表示的稳健性和可解释性。最后，我们通过数值实验展示了我们方法的多功能性：高斯流中的去噪和异常检测、向量场下点云动态的分析，以及神经网络学习轨迹的多尺度表征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel multiscale framework for analyzing sequences ofprobability measures in Wasserstein spaces over Euclidean domains. Exploitingthe intrinsic geometry of optimal transport, we construct a multiscaletransform applicable to both absolutely continuous and discrete measures.Central to our approach is a refinement operator based on McCann'sinterpolants, which preserves the geodesic structure of measure flows andserves as an upsampling mechanism. Building on this, we introduce theoptimality number, a scalar that quantifies deviations of a sequence fromWasserstein geodesicity across scales, enabling the detection of irregulardynamics and anomalies. We establish key theoretical guarantees, includingstability of the transform and geometric decay of coefficients, ensuringrobustness and interpretability of the multiscale representation. Finally, wedemonstrate the versatility of our methodology through numerical experiments:denoising and anomaly detection in Gaussian flows, analysis of point clouddynamics under vector fields, and the multiscale characterization of neuralnetwork learning trajectories.</description>
      <author>example@mail.com (Wael Mattar, Nir Sharon)</author>
      <guid isPermaLink="false">2509.10415v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System</title>
      <link>http://arxiv.org/abs/2509.10349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Acetrans系统，一种自主的、基于走廊的、高效的无人机悬吊运输系统，通过统一的感知、规划和控制框架解决现有系统在感知、规划和控制方面的关键限制。&lt;h4&gt;背景&lt;/h4&gt;无人机悬吊载荷在复杂和杂乱环境中的空中运输具有显著优势，但现有系统面临感知不可靠、大规模环境中规划效率低、无法保证在电缆弯曲和外部干扰下的全身安全等关键限制。&lt;h4&gt;目的&lt;/h4&gt;提出Acetrans系统，解决现有无人机悬吊运输系统在感知、规划和控制方面的关键限制，实现更安全、高效的空中货物运输。&lt;h4&gt;方法&lt;/h4&gt;通过统一的感知、规划和控制框架解决问题：1)提出LiDAR-IMU融合模块，在张紧和弯曲模式下估计载荷姿态和电缆形状；2)引入MACIRI算法，考虑不同无人机和载荷几何形状，生成安全飞行走廊；3)开发时空受限的轨迹优化方案；4)使用带有电缆弯曲约束的非线性模型预测控制器确保执行过程中的全身安全。&lt;h4&gt;主要发现&lt;/h4&gt;通过仿真和实验验证了Acetrans的有效性，与最先进的方法相比，在感知准确性、规划效率和控制安全性方面有显著改进。&lt;h4&gt;结论&lt;/h4&gt;Acetrans系统能够有效解决现有无人机悬吊运输系统的关键限制，实现更安全、高效的空中货物运输，在复杂和杂乱环境中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;带有悬吊载荷的无人机在复杂和杂乱环境中的空中运输具有显著优势。然而，现有系统面临关键限制，包括对电缆-载荷动力学的感知不可靠、大规模环境中的规划效率低下，以及在电缆弯曲和外部干扰下无法保证全身安全。本文提出了Acetrans，一种自主的、基于走廊的、高效的无人机悬吊运输系统，通过统一的感知、规划和控制框架解决这些挑战。提出了LiDAR-IMU融合模块，在张紧和弯曲模式下联合估计载荷姿态和电缆形状，实现鲁棒的全身状态估计和电缆点云的实时滤波。为了提高规划的可扩展性，我们引入了多尺寸感知配置空间迭代区域膨胀算法，该算法在考虑不同无人机和载荷几何形状的同时生成安全飞行走廊。然后，开发了一种时空受限的轨迹优化方案，以确保动态可行且无碰撞的轨迹。最后，通过增加电缆弯曲约束的非线性模型预测控制器在执行期间提供鲁棒的全身安全。仿真和实验结果验证了Acetrans的有效性，表明与最先进的方法相比，在感知准确性、规划效率和控制安全性方面有显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无人机悬挂运输系统中的三个关键问题：对电缆-载荷动力学的感知不可靠、在大规模环境中规划效率低下、以及无法保证电缆弯曲和外部干扰下的全身安全。这个问题在现实中非常重要，因为无人机悬挂运输系统在物流、农业和救灾等领域具有巨大潜力，特别是在复杂环境（如森林、城市峡谷或室内）中能够提供传统固定翼无人机和带机械臂的多旋翼无人机无法比拟的灵活性和适应性。解决这些问题将使无人机能够在这些复杂环境中实现自主、安全、高效的货物运输，拓展其应用范围。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计Acetrans系统。他们首先识别了悬挂运输系统面临的三个主要挑战：感知不可靠、规划效率低下和安全保障不足。在感知方面，作者借鉴了LiDAR-IMU融合技术，但进行了创新以适应电缆弯曲情况；在规划方面，作者基于现有的迭代区域膨胀算法（如IRIS、RILS、FIRI和CIRI）思想，扩展了它们以适应悬挂载荷系统的复杂几何形状；在控制方面，作者借鉴了非线性模型预测控制技术，但增加了电缆弯曲约束。总的来说，作者不是从零开始设计，而是基于现有工作的基础上进行创新和扩展，解决了悬挂运输系统特有的挑战，并统一了感知、规划和控制三个模块。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; Acetrans方法的核心思想是通过一个统一的框架同时解决感知、规划和控制三个关键问题，实现无人机在复杂环境中的安全、高效悬挂运输。核心思想包括：1)全身感知：使用LiDAR-IMU融合技术同时估计无人机、载荷和电缆的状态；2)多尺寸感知的走廊生成：提出MACIRI算法根据系统中不同部分的尺寸动态调整障碍物表示；3)走廊约束的轨迹优化：在生成的安全走廊内进行轨迹优化；4)电缆弯曲安全的控制：使用增强的非线性模型预测控制器确保全身安全。整体流程为：首先通过双LiDAR和LiDAR-IMU融合进行全身感知和状态估计；然后使用动力学A*算法和MACIRI算法生成安全飞行走廊；接着在走廊内进行时空轨迹优化；最后使用带有电缆弯曲约束的NMPC控制器跟踪轨迹，形成一个完整的自主运输系统。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1)全身感知框架：能够处理电缆弯曲情况，独立于光照条件，而现有视觉方法在低光条件下表现不佳且大多假设电缆总是张紧的；2)MACIRI算法：根据系统中不同部分的尺寸动态调整障碍物表示，扩大了可用走廊体积，而现有算法只考虑单一机器人尺寸；3)走廊约束的轨迹优化：专门针对悬挂载荷系统设计，优化速度比现有基线快1-3个数量级；4)电缆弯曲安全的控制：是第一个实现电缆弯曲时障碍避免的框架，而现有方法大多忽略电缆或未保证其安全。Acetrans的创新之处在于它首次将感知、规划和控制统一到一个完整的自主框架中，解决了悬挂载荷系统特有的挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Acetrans首次提出了一个统一的自主悬挂运输系统框架，通过创新的全身感知、多尺寸感知的走廊生成和电缆弯曲安全的控制方法，实现了无人机在复杂环境中安全、高效的悬挂载荷运输。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmanned aerial vehicles (UAVs) with suspended payloads offer significantadvantages for aerial transportation in complex and cluttered environments.However, existing systems face critical limitations, including unreliableperception of the cable-payload dynamics, inefficient planning in large-scaleenvironments, and the inability to guarantee whole-body safety under cablebending and external disturbances. This paper presents Acetrans, an Autonomous,Corridor-based, and Efficient UAV suspended transport system that addressesthese challenges through a unified perception, planning, and control framework.A LiDAR-IMU fusion module is proposed to jointly estimate both payload pose andcable shape under taut and bent modes, enabling robust whole-body stateestimation and real-time filtering of cable point clouds. To enhance planningscalability, we introduce the Multi-size-Aware Configuration-space IterativeRegional Inflation (MACIRI) algorithm, which generates safe flight corridorswhile accounting for varying UAV and payload geometries. A spatio-temporal,corridor-constrained trajectory optimization scheme is then developed to ensuredynamically feasible and collision-free trajectories. Finally, a nonlinearmodel predictive controller (NMPC) augmented with cable-bending constraintsprovides robust whole-body safety during execution. Simulation and experimentalresults validate the effectiveness of Acetrans, demonstrating substantialimprovements in perception accuracy, planning efficiency, and control safetycompared to state-of-the-art methods.</description>
      <author>example@mail.com (Weiyan Lu, Huizhe Li, Yuhao Fang, Zhexuan Zhou, Junda Wu, Yude Li, Youmin Gong, Jie Mei)</author>
      <guid isPermaLink="false">2509.10349v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.10282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Page 14, 5 pictures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了MCL-AD，一个新颖的多模态协作学习框架，用于零样本3D异常检测。该框架结合点云、RGB图像和文本语义信息，通过多模态提示学习机制和协作调制机制，实现了最先进的检测性能。&lt;h4&gt;背景&lt;/h4&gt;Zero-shot 3D异常检测旨在无需标记训练数据的情况下识别3D物体缺陷，在数据稀缺、隐私限制或标注成本高的场景中特别有价值。然而，现有方法主要专注于点云，忽视了来自RGB图像和文本先验等互补模态的丰富语义线索。&lt;h4&gt;目的&lt;/h4&gt;引入MCL-AD框架，利用点云、RGB图像和文本语义之间的多模态协作学习，实现更优的零样本3D异常检测。&lt;h4&gt;方法&lt;/h4&gt;提出多模态提示学习机制（MPLM），引入与对象无关的解耦文本提示和多模态对比损失，增强模内表示能力和模间协作学习；同时提出协作调制机制（CMM），通过联合调制RGB图像引导和点云引导的分支，充分利用点云和RGB图像的互补表示。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，提出的MCL-AD框架在零样本3D异常检测任务中达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;MCL-AD框架通过多模态协作学习有效提升了零样本3D异常检测的性能，为该领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;零样本3D异常检测旨在无需依赖标记的训练数据来识别3D物体中的缺陷，在数据稀缺、隐私限制或标注成本高的场景中特别有价值。然而，大多数现有方法仅专注于点云，忽视了来自RGB图像和文本先验等互补模态的丰富语义线索。本文介绍了MCL-AD，一个新颖的框架，它利用点云、RGB图像和文本语义之间的多模态协作学习来实现卓越的零样本3D异常检测。具体而言，我们提出了多模态提示学习机制（MPLM），通过引入与对象无关的解耦文本提示和多模态对比损失，增强模内表示能力和模间协作学习。此外，还提出了协作调制机制（CMM），通过联合调制RGB图像引导和点云引导的分支，充分利用点云和RGB图像的互补表示。大量实验证明，所提出的MCL-AD框架在零样本3D异常检测中达到了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决零样本3D异常检测问题，即在没有针对特定对象标记的训练数据的情况下检测3D物体中的缺陷。这个问题在现实中很重要，因为它能解决数据稀缺、隐私限制和高标注成本等挑战，使异常检测系统能够快速适应新对象类别，在工业检测、质量控制等领域具有广泛应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法仅关注点云数据而忽略RGB图像和文本提示等互补模态的局限性，提出多模态协作学习思路。设计过程中借鉴了CLIP模型作为基础编码器、多视图渲染技术处理点云、提示学习和对比学习等现有技术，但进行了创新改进，设计了多模态提示学习机制和协作调制机制来有效融合不同模态信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态协作学习，融合点云、RGB图像和文本语义信息，增强模型对异常的感知能力，使其能在无特定类别训练数据的情况下准确检测3D异常。整体流程包括：1)特征提取阶段，将点云转换为多视图深度图像并提取特征；2)训练阶段使用多模态提示学习机制构建解耦文本提示并计算多模态对比损失；3)测试阶段通过协作调制机制动态调整RGB和点云分支的输出权重，融合生成最终结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多模态协作学习框架MCL-AD，首次融合点云、RGB图像和文本语义三种模态；2)多模态提示学习机制MPLM，包含对象无关的解耦文本提示和多模态对比损失；3)协作调制机制CMM，动态调整不同模态贡献权重。相比之前工作，MCL-AD不仅利用了更多模态信息，还通过解耦设计和动态融合解决了模态不平衡问题，显著提升了检测性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MCL-AD通过多模态协作学习框架，有效融合点云、RGB图像和文本语义信息，显著提升了在没有特定对象类别训练数据的情况下3D异常检测的准确性和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objectswithout relying on labeled training data, making it especially valuable inscenarios constrained by data scarcity, privacy, or high annotation cost.However, most existing methods focus exclusively on point clouds, neglectingthe rich semantic cues available from complementary modalities such as RGBimages and texts priors. This paper introduces MCL-AD, a novel framework thatleverages multimodal collaboration learning across point clouds, RGB images,and texts semantics to achieve superior zero-shot 3D anomaly detection.Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) thatenhances the intra-modal representation capability and inter-modalcollaborative learning by introducing an object-agnostic decoupled text promptand a multimodal contrastive loss. In addition, a collaborative modulationmechanism (CMM) is proposed to fully leverage the complementary representationsof point clouds and RGB images by jointly modulating the RGB image-guided andpoint cloud-guided branches. Extensive experiments demonstrate that theproposed MCL-AD framework achieves state-of-the-art performance in ZS-3Danomaly detection.</description>
      <author>example@mail.com (Gang Li, Tianjiao Chen, Mingle Zhou, Min Li, Delong Han, Jin Wan)</author>
      <guid isPermaLink="false">2509.10282v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion</title>
      <link>http://arxiv.org/abs/2509.10139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CaR1的新型相机-雷达融合架构，用于BEV车辆分割，通过结合相机和雷达的互补优势，实现了与最先进方法相当的分割性能。&lt;h4&gt;背景&lt;/h4&gt;相机-雷达融合为基于激光雷达的自动驾驶系统提供了一种稳健且具有成本效益的替代方案，相机提供丰富的语义线索但深度信息不可靠，而雷达提供稀疏但可靠的位置和运动信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型相机-雷达融合架构CaR1，专门用于BEV（鸟瞰图）车辆分割任务。&lt;h4&gt;方法&lt;/h4&gt;基于BEVFusion构建，包含网格化的雷达编码，将点云离散化为结构化的BEV特征，以及自适应融合机制，动态平衡传感器的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验显示，CaR1实现了57.6的IoU分割性能，与当前最先进的方法相当。&lt;h4&gt;结论&lt;/h4&gt;CaR1架构通过有效融合相机和雷达数据，在BEV车辆分割任务上取得了优异性能，相关代码已在GitHub公开可用。&lt;h4&gt;翻译&lt;/h4&gt;相机-雷达融合通过结合互补的感知能力，为基于激光雷达的自动驾驶系统提供了一种稳健且具有成本效益的替代方案：相机提供丰富的语义线索但深度信息不可靠，而雷达提供稀疏但可靠的位置和运动信息。我们介绍了CaR1，一种用于BEV车辆分割的新型相机-雷达融合架构。基于BEVFusion构建，我们的方法包含一种网格化的雷达编码，将点云离散化为结构化的BEV特征，以及一种自适应融合机制，能够动态平衡传感器的贡献。在nuScenes上的实验表明，该方法具有竞争力的分割性能（57.6 IoU），与最先进的方法相当。代码已在GitHub上公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何有效融合相机和雷达两种传感器的数据，实现鸟瞰视角下的车辆分割问题。这个问题很重要，因为自动驾驶系统需要准确感知周围环境，而单一传感器有局限性：相机提供丰富语义信息但深度估计不可靠，雷达提供可靠位置和运动信息且在恶劣天气下稳定但数据稀疏。相机-雷达融合可以提供一种成本更低、更鲁棒的替代方案，比基于LiDAR的系统更适合大规模部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了相机和雷达的互补特性及融合挑战，然后基于BEVFusion架构进行改进。借鉴了现有工作中的BEV表示空间、EfficientViT-L2图像编码器、Point Transformer V3点云编码网络和Attention U-Net解码器。作者的创新设计包括网格化雷达特征编码方法将稀疏点云转换为结构化特征，以及自适应融合机制动态平衡各传感器贡献。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用相机和雷达的互补优势，通过鸟瞰视角表示空间统一两种传感器数据，采用网格化方法处理雷达点云，并设计自适应融合机制动态调整传感器贡献。整体流程包括：1)图像特征提取：使用EfficientViT-L2处理多视角图像并投影到BEV空间；2)雷达特征提取：使用PTv3处理点云并通过金字塔聚合网络增强特征；3)自适应融合：通过注意力机制动态调整传感器贡献；4)BEV解码：使用Attention U-Net refine特征；5)分割头：生成车辆分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)网格化雷达特征编码方法，将稀疏点云转换为结构化BEV特征；2)自适应融合机制，动态平衡各传感器贡献；3)整体架构设计，使用轻量级编码器和注意力解码器。相比之前工作，不同之处在于：雷达处理方式更先进，结合了点网络优势和BEV框架；融合策略更灵活，能动态调整传感器贡献；性能表现更好，在nuScenes上实现57.6% IoU，比纯相机方法提升10.2%，与最先进方法相当。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CaR1通过创新的网格化雷达特征编码和自适应融合机制，实现了相机和雷达数据在鸟瞰视角下的有效融合，为自动驾驶车辆分割提供了一种成本更低、更鲁棒的感知方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-radar fusion offers a robust and cost-effective alternative toLiDAR-based autonomous driving systems by combining complementary sensingcapabilities: cameras provide rich semantic cues but unreliable depth, whileradar delivers sparse yet reliable position and motion information. Weintroduce CaR1, a novel camera-radar fusion architecture for BEV vehiclesegmentation. Built upon BEVFusion, our approach incorporates a grid-wise radarencoding that discretizes point clouds into structured BEV features and anadaptive fusion mechanism that dynamically balances sensor contributions.Experiments on nuScenes demonstrate competitive segmentation performance (57.6IoU), on par with state-of-the-art methods. Code is publicly available\href{https://www.github.com/santimontiel/car1}{online}.</description>
      <author>example@mail.com (Santiago Montiel-Marín, Angel Llamazares, Miguel Antunes-García, Fabio Sánchez-García, Luis M. Bergasa)</author>
      <guid isPermaLink="false">2509.10139v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Soft Tissue Simulation and Force Estimation from Heterogeneous Structures using Equivariant Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.10125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络(GNN)的软组织变形模拟方法，能够从稀疏点云预测组织表面变形和施加的力，相比传统有限元法更高效且具有较好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;软组织变形模拟对手术训练、术前规划和实时触觉反馈系统至关重要。基于物理的模型如有限元法能提供高保真结果，但计算成本高且需要大量预处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的数据驱动方法，能够准确模拟软组织变形，适用于实时应用，并具有良好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图神经网络架构，通过每个点下方的二值组织轮廓整合内部解剖信息，并利用E(n)-等变消息传递提高鲁棒性。实验数据包括真实硅胶和类骨 phantom，以及使用FEM生成的合成模拟数据。&lt;h4&gt;主要发现&lt;/h4&gt;模型在标准测试案例中与基线GNN性能相当，在旋转和跨分辨率场景中显著优于基线，显示出对未见方向和点密度的强泛化能力。模型也实现了显著的速度提升，为实时应用提供了解决方案。在实验数据上微调后，模型在有限样本量和测量噪声下仍保持亚毫米级变形精度。&lt;h4&gt;结论&lt;/h4&gt;该方法为传统模拟提供了高效的数据驱动替代方案，能够泛化到不同的解剖结构配置，并支持交互式手术环境。&lt;h4&gt;翻译&lt;/h4&gt;准确模拟软组织变形对手术培训、术前规划和实时触觉反馈系统至关重要。虽然基于物理的模型如有限元法(FEM)能提供高保真结果，但它们通常计算量大且需要大量预处理。我们提出了一种图神经网络(GNN)架构，可以从稀疏点云预测组织表面变形和施加的力。该模型通过每个点下方的二值组织轮廓整合内部解剖信息，并利用E(n)-等变消息传递提高鲁棒性。我们收集了包含真实硅胶和类骨 phantom 的实验数据，并辅以使用FEM生成的合成模拟数据。我们的模型在标准测试案例中与基线GNN性能相当，在旋转和跨分辨率场景中显著优于基线，显示出对未见方向和点密度的强泛化能力。它还实现了显著的速度提升，为实时应用提供了解决方案。在实验数据上微调后，尽管样本量有限且存在测量噪声，模型仍保持亚毫米级变形精度。结果表明，我们的方法为传统模拟提供了高效的数据驱动替代方案，能够泛化到不同的解剖结构配置，并支持交互式手术环境。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决软组织变形模拟的计算效率问题。传统物理模拟方法(如有限元法)虽然准确但计算量大，难以满足实时应用需求。这一问题在现实中至关重要，因为准确的软组织模拟对手术培训、术前规划和实时手术引导系统都至关重要，能够提高外科医生技能、预见手术并发症、改善临床决策精度，并支持医疗机器人的精确控制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：物理模拟计算昂贵，而现有学习方法无法提供触觉反馈所需的力估计或仅能处理均匀组织。他们借鉴了多种现有工作，包括多层感知机(MLP)、U-Mesh架构、图神经网络(GNN)如MagNet框架、条件图神经网络以及E(n)-等变图神经网络概念。基于这些工作，作者设计了一种新方法，通过引入异构组织建模、条件等变图卷积层(cEGCL)和多任务预测(变形和力)来解决现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过图神经网络处理点云数据，同时预测软组织变形和施加力，并利用组织内部结构信息提高预测准确性。整体流程包括：1)收集实验数据(硅胶和骨状phantom)和FEM模拟数据；2)为每个表面点提取128维二元组织特征向量，表示垂直材料分布；3)设计条件等变图卷积层(cEGCL)更新节点坐标和特征；4)构建包含位移预测和力预测两个分支的模型架构；5)使用加权损失函数进行训练，并在实验数据上微调；6)评估模型在旋转、点密度变化和迁移学习场景下的性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)异构对象建模，引入内部解剖结构信息；2)条件等变图卷积层(cEGCL)，提高对旋转的鲁棒性；3)多任务预测，同时输出变形和力；4)数据融合策略，结合实验和FEM模拟数据；5)强大的泛化能力，在旋转和不同点密度场景下表现优异。相比之前的工作，本方法不仅能处理异质组织结构，还提供力估计支持触觉反馈，计算速度更快，且对旋转和点密度变化具有更强的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合组织内部结构信息和等变图神经网络的创新方法，实现了高效、准确的软组织变形和力估计，显著提高了在异质结构、旋转和不同点密度场景下的泛化能力，为实时手术模拟和触觉反馈提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately simulating soft tissue deformation is crucial for surgicaltraining, pre-operative planning, and real-time haptic feedback systems. Whilephysics-based models such as the finite element method (FEM) providehigh-fidelity results, they are often computationally expensive and requireextensive preprocessing. We propose a graph neural network (GNN) architecturethat predicts both tissue surface deformation and applied force from sparsepoint clouds. The model incorporates internal anatomical information throughbinary tissue profiles beneath each point and leverages E(n)-equivariantmessage passing to improve robustness. We collected experimental data thatcomprises a real silicone and bone-like phantom, and complemented it withsynthetic simulations generated using FEM. Our model achieves a comparableperformance to a baseline GNN on standard test cases and significantlyoutperforms it in rotated and cross-resolution scenarios, showing a stronggeneralization to unseen orientations and point densities. It also achieves asignificant speed improvement, offering a solution for real-time applications.When fine-tuned on experimental data, the model maintains sub-millimeterdeformation accuracy despite limited sample size and measurement noise. Theresults demonstrate that our approach offers an efficient, data-drivenalternative to traditional simulations, capable of generalizing acrossanatomical configurations and supporting interactive surgical environments.</description>
      <author>example@mail.com (Madina Kojanazarova, Sidady El Hadramy, Jack Wilkie, Georg Rauter, Philippe C. Cattin)</author>
      <guid isPermaLink="false">2509.10125v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping</title>
      <link>http://arxiv.org/abs/2509.10032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Pages, 9 figures, International Workshop 3D-AdViCE in conjunction  with 12th ECMR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了两种互补的球形测绘系统，评估了它们在资源受限硬件上的激光雷达-惯性里程计性能，并发现球形运动的高动态特性导致传统算法性能下降。&lt;h4&gt;背景&lt;/h4&gt;球形机器人因其保护壳和全向移动能力，在危险或受限环境的测绘应用中具有独特优势。&lt;h4&gt;目的&lt;/h4&gt;开发两种互补的球形测绘系统（轻量级无驱动设计和内部摆锤驱动变体），并评估其测绘精度。&lt;h4&gt;方法&lt;/h4&gt;两种系统均配备Livox Mid-360固态激光雷达传感器，在资源受限硬件上运行激光雷达-惯性里程计算法，通过与真实地图比较点云来评估精度。&lt;h4&gt;主要发现&lt;/h4&gt;球形运动引入的高动态运动导致最先进的LIO算法性能下降，产生全局不一致的地图和有时不可恢复的漂移。&lt;h4&gt;结论&lt;/h4&gt;球形机器人的特殊运动特性对传统LIO算法构成挑战，需要改进算法以适应这种高动态运动环境。&lt;h4&gt;翻译&lt;/h4&gt;球形机器人因其保护壳和全向移动能力，在危险或受限环境的测绘应用中具有独特优势。这项工作提出了两种互补的球形测绘系统：一种轻量级无驱动设计和一种内部摆锤驱动运动的变体。两种系统都配备了Livox Mid-360固态激光雷达传感器，并在资源受限的硬件上运行激光雷达-惯性里程计算法。我们通过将LIO算法生成的3D点云与真实地图进行比较来评估这些系统的测绘精度。结果表明，由于球形运动引入的高动态运动，最先进的LIO算法性能下降，导致全局不一致的地图和有时不可恢复的漂移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spherical robots offer unique advantages for mapping applications inhazardous or confined environments, thanks to their protective shells andomnidirectional mobility. This work presents two complementary sphericalmapping systems: a lightweight, non-actuated design and an actuated variantfeaturing internal pendulum-driven locomotion. Both systems are equipped with aLivox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)algorithms on resource-constrained hardware. We assess the mapping accuracy ofthese systems by comparing the resulting 3D point-clouds from the LIOalgorithms to a ground truth map. The results indicate that the performance ofstate-of-the-art LIO algorithms deteriorates due to the high dynamic movementintroduced by the spherical locomotion, leading to globally inconsistent mapsand sometimes unrecoverable drift.</description>
      <author>example@mail.com (Marawan Khalil, Fabian Arzberger, Andreas Nüchter)</author>
      <guid isPermaLink="false">2509.10032v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation</title>
      <link>http://arxiv.org/abs/2509.09946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCVW 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种将现有2D多摄像头跟踪系统扩展到3D空间的方法，通过利用深度信息重建目标点云并恢复3D边界框，同时引入增强的数据关联机制，在AI City Challenge的3D MTMC数据集上获得第三名。&lt;h4&gt;背景&lt;/h4&gt;多目标多摄像头跟踪(MTMC)是自动化大规模监控的重要计算机视觉任务。通过摄像头校准和深度信息，可以将场景中的目标投影到3D空间，提供对3D环境的自动感知。然而，在3D空间中进行跟踪需要从头替换所有2D跟踪组件，这对现有的MTMC系统可能不可行。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在开发一种方法，能够将任何现有的在线2D多摄像头跟踪系统扩展到3D空间，而无需完全重新设计系统架构。&lt;h4&gt;方法&lt;/h4&gt;研究提出的方法包括：1)利用深度信息重建目标的点云空间；2)通过聚类和偏航角细化恢复目标的3D边界框；3)引入增强的在线数据关联机制，利用目标的局部ID一致性来跨帧分配全局ID。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在2025年AI City Challenge的3D MTMC数据集上进行了评估，并在排行榜上获得第三名的成绩，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;本研究提供了一种实用的方法，可以将现有的2D多摄像头跟踪系统扩展到3D空间，而无需完全重新设计系统，为大规模3D监控应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多目标多摄像头跟踪(MTMC)是自动化大规模监控的重要计算机视觉任务。通过摄像头校准和深度信息，场景中的目标可以被投影到3D空间，为3D环境提供前所未有的自动感知水平。然而，在3D空间中进行跟踪需要从头替换所有2D跟踪组件，这对于现有的MTMC系统可能不可行。在本文中，我们提出了一种方法，通过利用深度信息将任何在线2D多摄像头跟踪系统扩展到3D空间，重建目标在点云空间中的表示，并在跟踪后通过聚类和偏航角细化恢复其3D边界框。我们还引入了一种增强的在线数据关联机制，利用目标的局部ID一致性来跨帧分配全局ID。所提出的框架在2025年AI City Challenge的3D MTMC数据集上进行了评估，在排行榜上获得第三名。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将现有的在线2D多摄像头跟踪系统扩展到3D空间的问题。这个问题在现实中非常重要，因为大规模监控系统（如智慧城市、智能交通、安防等）需要处理来自多个摄像头的视频数据，而3D空间中的目标跟踪能提供更丰富的场景信息，如目标的高度、位置和朝向等。直接替换2D系统的所有组件到3D空间成本高昂且复杂，而本文提供了一种更高效、经济的解决方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有的3D多摄像头跟踪方法需要在跟踪前融合多视图数据，这需要替换所有2D组件，对大规模监控系统不切实际。因此，他们提出了'后期聚合'的思路，即在完成2D跟踪后，再利用深度信息将2D结果扩展到3D空间。作者借鉴了多项现有工作，包括使用Co-DETR进行目标检测、CLIP-ReID提取外观特征、RTMPose进行姿态估计、Deep OC-SORT进行单摄像头跟踪、SAM2进行实例分割、DBSCAN进行点云聚类以及分层聚类进行空间数据关联。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'后期聚合'的方式，在完成2D多摄像头跟踪后，再利用深度信息将2D结果扩展到3D空间，同时利用目标局部ID的一致性来改进跨摄像头跟踪的ID分配机制。整体流程分为两个阶段：第一阶段是2D多摄像头跟踪，包括单摄像头跟踪、空间数据关联和时间数据关联；第二阶段是后期3D边界框聚合，包括深度到点云转换、点云到3D边界框转换、3D边界框融合和偏航角优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 后期聚合框架，可将2D系统扩展为3D系统而无需修改现有组件；2) 增强的在线关联机制，利用局部ID一致性改进跨摄像头跟踪；3) 3D边界框恢复机制，通过分割和点云聚类将2D边界框扩展到3D空间；4) 偏航角优化，通过运动轨迹分析提高定位精度。相比之前的工作，本文方法不需要在跟踪前融合多视图数据，而是利用现有的2D跟踪结果，降低了系统复杂度，同时通过局部ID一致性改进了跨摄像头跟踪性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的后期聚合框架，能够将现有的在线2D多摄像头跟踪系统无缝扩展为3D跟踪系统，同时引入了基于局部ID一致性的增强关联机制和基于深度信息的3D边界框恢复方法，在2025年AI City Challenge的3D MTMC任务中取得了第三名的好成绩。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-Target Multi-Camera Tracking (MTMC) is an essential computer visiontask for automating large-scale surveillance. With camera calibration and depthinformation, the targets in the scene can be projected into 3D space, offeringunparalleled levels of automatic perception of a 3D environment. However,tracking in the 3D space requires replacing all 2D tracking components from theground up, which may be infeasible for existing MTMC systems. In this paper, wepresent an approach for extending any online 2D multi-camera tracking systeminto 3D space by utilizing depth information to reconstruct a target inpoint-cloud space, and recovering its 3D box through clustering and yawrefinement following tracking. We also introduced an enhanced online dataassociation mechanism that leverages the target's local ID consistency toassign global IDs across frames. The proposed framework is evaluated on the2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on theleaderboard.</description>
      <author>example@mail.com (Vu-Minh Le, Thao-Anh Tran, Duc Huy Do, Xuan Canh Do, Huong Ninh, Hai Tran)</author>
      <guid isPermaLink="false">2509.09946v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging</title>
      <link>http://arxiv.org/abs/2509.09785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Token Purging (PG)的新型测试时适应方法，用于解决3D点云分类中的分布偏移问题。该方法通过在令牌到达注意力层之前移除受域偏移高度影响的令牌，实现了无需反向传播的稳健适应。实验表明，该方法在准确率和效率方面均优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;测试时适应(TTA)对于缓解3D点云分类中分布偏移引起的性能下降至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的TTA方法，能够在不进行反向传播的情况下有效适应分布偏移，同时提高计算效率和内存利用率。&lt;h4&gt;方法&lt;/h4&gt;引入Token Purging (PG)，一种新颖的无反向传播方法，在令牌到达注意力层之前移除受域偏移高度影响的令牌。提出了两种变体：PG-SP（利用源统计信息）和PG-SF（完全无源版本，依赖CLS令牌驱动的适应）。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40-C、ShapeNet-C和ScanObjectNN-C上的广泛评估表明，PG-SP比最先进的无反向传播方法平均高出10.3%的准确率，而PG-SF为无源适应设定了新基准。此外，PG比基线快12.4倍，内存效率高5.5倍，适合实际部署。&lt;h4&gt;结论&lt;/h4&gt;Token Purging是一种有效的TTA方法，在性能和效率方面都有显著优势，适合实际部署。&lt;h4&gt;翻译&lt;/h4&gt;测试时适应(TTA)对于缓解3D点云分类中分布偏移引起的性能下降至关重要。在这项工作中，我们引入了Token Purging (PG)，一种新颖的无反向传播方法，在令牌到达注意力层之前移除受域偏移高度影响的令牌。与现有的TTA方法不同，PG在令牌级别操作，确保无需迭代更新的稳健适应。我们提出了两种变体：PG-SP，它利用源统计信息，以及PG-SF，一种完全无源的版本，依赖于CLS令牌驱动的适应。在ModelNet40-C、ShapeNet-C和ScanObjectNN-C上的广泛评估表明，PG-SP比最先进的无反向传播方法平均高出10.3%的准确率，而PG-SF为无源适应设定了新基准。此外，PG比我们的基线快12.4倍，内存效率高5.5倍，使其适合实际部署。代码可在https://github.com/MosyMosy/Purge-Gate获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云分类任务中的测试时适应问题，即在测试数据分布与训练数据分布不同（分布偏移）的情况下，如何保持模型性能。这个问题在现实中非常重要，因为3D点云应用广泛（如自动驾驶、机器人、AR/VR等），这些应用中数据分布经常变化，而预训练一个模型应对所有场景是不现实的。现有方法要么计算成本高，要么需要额外源数据，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云数据中分布偏移的特殊性，指出噪声可能表现为新增点或不均匀分布的扰动。他们观察到噪声会破坏transformer架构中的注意力机制，影响特征聚合。受Token Pruning（用于提高transformer效率）的启发，作者转而设计删除受分布偏移影响最大的token的方法。同时，他们利用了transformer中CLS token的特性，发现CLS token在预训练过程中会吸收领域信息，可以作为原型用于测试时适应。整体设计思路是创建一个轻量级的、无反向传播的门控机制，在注意力层输入前过滤掉噪声token。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在测试时识别并移除受分布偏移影响最大的token，防止这些噪声token到达注意力层，从而保护模型的注意力机制不被破坏。整体流程包括：1) 将点云样本表示为token集合；2) 计算每个token的偏离程度（PG-SP使用源数据统计信息计算马氏距离，PG-SF使用CLS token作为原型计算余弦距离）；3) 移除偏离最大的Lpg个token；4) 将净化后的token输入transformer网络进行分类；5) 使用熵最小化策略动态选择最优的Lpg值。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次在token级别进行测试时适应；2) 提出无反向传播的轻量级方法；3) 设计两种变体（PG-SP利用源数据统计，PG-SF完全无源）；4) 提供分布偏移如何影响transformer注意力机制的理论分析。相比之前工作，不同之处在于：相比反向传播方法（如TENT），不需要梯度计算和参数更新，效率更高；相比无反向传播方法（如BFTT3D），不依赖源数据类别原型；相比测试时训练方法（如MATE），不需要辅助训练任务，可应用于任意预训练模型；相比Token Pruning，专注于解决分布偏移而非提高计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Purge-Gate通过在测试时动态移除受分布偏移影响最大的token，提出了一种高效的无反向传播的点云分类测试时适应方法，显著提高了模型在分布偏移场景下的性能，同时大幅降低了计算和内存需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Test-time adaptation (TTA) is crucial for mitigating performance degradationcaused by distribution shifts in 3D point cloud classification. In this work,we introduce Token Purging (PG), a novel backpropagation-free approach thatremoves tokens highly affected by domain shifts before they reach attentionlayers. Unlike existing TTA methods, PG operates at the token level, ensuringrobust adaptation without iterative updates. We propose two variants: PG-SP,which leverages source statistics, and PG-SF, a fully source-free versionrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is12.4 times faster and 5.5 times more memory efficient than our baseline, makingit suitable for real-world deployment. Code is available at\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}</description>
      <author>example@mail.com (Moslem Yazdanpanah, Ali Bahri, Mehrdad Noori, Sahar Dastani, Gustavo Adolfo Vargas Hakim, David Osowiechi, Ismail Ben Ayed, Christian Desrosiers)</author>
      <guid isPermaLink="false">2509.09785v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training</title>
      <link>http://arxiv.org/abs/2509.10426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为DECAMP的解耦上下文感知预训练框架，用于解决自动驾驶中多智能体运动预测的挑战，特别是标记数据稀缺和多智能体场景预测效果不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法往往面临标记数据稀缺的问题，并且在多智能体预测场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法面临的标记数据稀缺和多智能体预测场景中表现不佳的挑战，通过引入一个解耦的上下文感知预训练框架来提高多智能体运动预测的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为DECAMP的解耦上下文感知预训练框架，将行为模式学习与潜在特征重建解耦，优先考虑可解释的动力学，从而增强下游预测的场景表示。同时，框架集成了上下文感知表示学习和协作空间-运动预训练任务，能够同时优化结构推理和意图推理，同时捕获潜在的动态意图。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse 2基准测试上的实验展示了该方法优越的性能，结果证明了其在多智能体运动预测中的有效性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是自动驾驶领域中首个用于多智能体运动预测的上下文自编码器框架，代码和模型将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法往往面临标记数据稀缺的问题，并且在多智能体预测场景中表现不佳。为了解决这些挑战，我们引入了一个用于多智能体运动预测的解耦上下文感知预训练框架，名为DECAMP。与现有将表示学习与预训练任务纠缠在一起的方法不同，我们的框架将行为模式学习与潜在特征重建解耦，优先考虑可解释的动力学，从而增强下游预测的场景表示。此外，我们的框架集成了上下文感知表示学习和协作空间-运动预训练任务，这能够在捕获潜在动态意图的同时，优化结构推理和意图推理。我们在Argoverse 2基准测试上的实验展示了我们方法的优越性能，取得的结果强调了其在多智能体运动预测中的有效性。据我们所知，这是自动驾驶领域中首个用于多智能体运动预测的上下文自编码器框架。代码和模型将公开提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多智能体运动预测中的两个核心挑战：标记数据稀缺问题和场景一致性不足。在自动驾驶领域，准确预测多个交通参与者的互动行为对确保道路安全和效率至关重要，而场景不一致的预测会导致不合理的轨迹组合，影响自动驾驶汽车的可靠决策。解决这些问题不仅能提高预测准确性，还能降低系统开发和部署成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有监督学习和自监督方法的局限性，特别是编码器与预训练任务纠缠的问题以及单智能体方法扩展到多智能体场景的困难。受计算机视觉中掩码图像建模的启发，作者设计了'编码器-回归器-解码器'级联范式，引入回归器来减少编码器与特定任务的耦合。同时借鉴了Transformer架构、FPN和PointNet等现有技术，并创新性地设计了协作的空间-运动预训练任务，使模型能够同时捕获空间结构和动态意图。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦行为模式学习与潜在特征重建，使编码器专注于学习驾驶行为表示，而解码器专注于完成预训练任务。整体流程分为两个阶段：1)预训练阶段，输入历史、未来状态和地图，通过掩码策略和双解码器（空间解码器和运动解码器）分别重建空间线索和识别运动信号；2)微调阶段，仅使用历史状态和地图，通过预训练编码器生成K个场景一致的联合轨迹集合，确保预测结果在场景级别保持一致。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)解耦的'编码器-回归器-解码器'预训练框架，减少编码器与特定任务的纠缠；2)协作的空间-运动预训练任务，同时优化空间重建和运动识别；3)直接生成场景一致的多智能体联合轨迹，避免复杂的后处理；4)上下文感知表示学习，增强场景理解能力。相比之前工作，DECAMP解决了现有自监督方法中编码器与预训练任务耦合的问题，从单智能体预测扩展到多智能体联合预测，并通过解耦学习更鲁棒的行为先验，显著提高了预测准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DECAMP通过解耦行为模式学习与特征重建，并引入协作的空间-运动预训练任务，显著提高了多智能体运动预测的场景一致性和准确性，同时减少了对大量标记数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory prediction is a critical component of autonomous driving,essential for ensuring both safety and efficiency on the road. However,traditional approaches often struggle with the scarcity of labeled data andexhibit suboptimal performance in multi-agent prediction scenarios. To addressthese challenges, we introduce a disentangled context-aware pre-trainingframework for multi-agent motion prediction, named DECAMP. Unlike existingmethods that entangle representation learning with pretext tasks, our frameworkdecouples behavior pattern learning from latent feature reconstruction,prioritizing interpretable dynamics and thereby enhancing scene representationfor downstream prediction. Additionally, our framework incorporatescontext-aware representation learning alongside collaborative spatial-motionpretext tasks, which enables joint optimization of structural and intentionalreasoning while capturing the underlying dynamic intentions. Our experiments onthe Argoverse 2 benchmark showcase the superior performance of our method, andthe results attained underscore its effectiveness in multi-agent motionforecasting. To the best of our knowledge, this is the first contextautoencoder framework for multi-agent motion forecasting in autonomous driving.The code and models will be made publicly available.</description>
      <author>example@mail.com (Jianxin Shi, Zengqi Peng, Xiaolong Chen, Tianyu Wo, Jun Ma)</author>
      <guid isPermaLink="false">2509.10426v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning</title>
      <link>http://arxiv.org/abs/2509.10305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GundamQ的多尺度时空Q网络，用于解决动态环境中机器人路径规划的挑战。该框架包含时空感知模块和自适应策略优化模块，能够有效处理多尺度时间依赖性和平衡探索-利用关系。实验结果表明，该方法在成功率和路径质量方面显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;在动态和不确定的环境中，机器人路径规划需要准确的空间时间环境理解以及在部分可观测情况下的鲁棒决策能力。&lt;h4&gt;目的&lt;/h4&gt;解决当前基于深度强化学习的路径规划方法在多尺度时间依赖性建模不足和探索-利用平衡效率低下方面的局限，提高机器人路径规划的成功率和质量。&lt;h4&gt;方法&lt;/h4&gt;提出GundamQ框架，包含两个关键模块：(1)时空感知模块，分层提取多粒度空间特征和多尺度时间依赖性；(2)自适应策略优化模块，在训练中平衡探索和利用，并通过约束策略更新优化路径平滑度和碰撞概率。&lt;h4&gt;主要发现&lt;/h4&gt;在动态环境中的实验表明，GundamQ成功率达到15.3%的提升，整体路径质量提高21.7%，显著优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;GundamQ通过有效建模多尺度时空特征和优化探索-利用平衡，显著提高了动态环境中机器人路径规划的适应性和路径质量。&lt;h4&gt;翻译&lt;/h4&gt;在动态和不确定的环境中，机器人路径规划需要准确的空间时间环境理解以及在部分可观测情况下的鲁棒决策能力。然而，当前基于深度强化学习的路径规划方法面临两个基本局限：(1)对多尺度时间依赖性建模不足，导致在动态场景中适应性次优；(2)探索-利用平衡效率低下，导致路径质量下降。为解决这些挑战，我们提出GundamQ：一种用于机器人路径规划的多尺度时空Q网络。该框架包含两个关键模块：(i)时空感知模块，分层提取从瞬时到延长时间范围的多粒度空间特征和多尺度时间依赖性，从而提高动态环境中的感知准确性；(ii)自适应策略优化模块，在训练过程中平衡探索和利用，同时通过约束策略更新优化平滑度和碰撞概率。动态环境中的实验表明，GundamQ成功率达到15.3%的提升，整体路径质量提高21.7%，显著优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In dynamic and uncertain environments, robotic path planning demands accuratespatiotemporal environment understanding combined with robust decision-makingunder partial observability. However, current deep reinforcement learning-basedpath planning methods face two fundamental limitations: (1) insufficientmodeling of multi-scale temporal dependencies, resulting in suboptimaladaptability in dynamic scenarios, and (2) inefficient exploration-exploitationbalance, leading to degraded path quality. To address these challenges, wepropose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic PathPlanning. The framework comprises two key modules: (i) the SpatiotemporalPerception module, which hierarchically extracts multi-granularity spatialfeatures and multi-scale temporal dependencies ranging from instantaneous toextended time horizons, thereby improving perception accuracy in dynamicenvironments; and (ii) the Adaptive Policy Optimization module, which balancesexploration and exploitation during training while optimizing for smoothnessand collision probability through constrained policy updates. Experiments indynamic environments demonstrate that GundamQ achieves a 15.3\% improvement insuccess rate and a 21.7\% increase in overall path quality, significantlyoutperforming existing state-of-the-art methods.</description>
      <author>example@mail.com (Yutong Shen, Ruizhe Xia, Bokai Yan, Shunqi zhang, Pengrui Xiang, Sicheng He, Yixin Xu)</author>
      <guid isPermaLink="false">2509.10305v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>BenchECG and xECG: a benchmark and baseline for ECG foundation models</title>
      <link>http://arxiv.org/abs/2509.10151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 4 figures, 22 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了BenchECG作为心电图(ECG)基础模型的标准化基准，并提出了xECG模型，该模型在所有评估任务中表现最佳，为ECG表示学习设定了新的基线。&lt;h4&gt;背景&lt;/h4&gt;心电图(ECG)是一种低成本、广泛使用且适合深度学习的医疗数据。最近，人们对开发能够在各种下游任务中泛化的ECG基础模型的兴趣日益增长。然而，先前的研究缺乏一致的评估方法，常使用狭窄的任务选择和不一致的数据集，妨碍了公平比较。&lt;h4&gt;目的&lt;/h4&gt;引入BenchECG作为标准化的基准，包含全面的公开可用ECG数据集和多样化任务；同时提出并评估xECG模型，这是一种基于xLSTM的循环模型。&lt;h4&gt;方法&lt;/h4&gt;使用SimDINOv2自监督学习训练基于xLSTM的循环模型(xECG)，并在BenchECG基准上与公开的最先进模型进行比较评估。&lt;h4&gt;主要发现&lt;/h4&gt;与公开的最先进模型相比，xECG在BenchECG评分上取得了最佳成绩。特别地，xECG是唯一在所有数据集和任务上都表现出色的公开可用模型。&lt;h4&gt;结论&lt;/h4&gt;通过标准化评估，BenchECG能够进行严格的模型比较，并旨在加速ECG表示学习的进展。xECG比早期方法表现更优，为未来的ECG基础模型设定了新的基线。&lt;h4&gt;翻译&lt;/h4&gt;心电图(ECG)是一种低成本、广泛使用且适合深度学习的工具。最近，人们对开发能够跨多样化下游任务泛化的ECG基础模型的兴趣日益增长。然而，一直缺乏一致的评估：先前的工作通常使用狭窄的任务选择和不一致的数据集，妨碍了公平比较。在此，我们引入了BenchECG，一个包含全面公开可用ECG数据集和多样化任务的标准基准。我们还提出了xECG，这是一种基于xLSTM的循环模型，使用SimDINOv2自监督学习进行训练，与公开的最先进模型相比，它在BenchECG评分上取得了最佳成绩。特别是，xECG是唯一在所有数据集和任务上都表现强劲的公开可用模型。通过标准化评估，BenchECG能够进行严格的比较，并旨在加速ECG表示学习的进展。xECG比早期方法表现更优，为未来的ECG基础模型设定了新的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited todeep learning. Recently, interest has grown in developing foundation models forECGs - models that generalise across diverse downstream tasks. However,consistent evaluation has been lacking: prior work often uses narrow taskselections and inconsistent datasets, hindering fair comparison. Here, weintroduce BenchECG, a standardised benchmark comprising a comprehensive suiteof publicly available ECG datasets and versatile tasks. We also propose xECG,an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,which achieves the best BenchECG score compared to publicly availablestate-of-the-art models. In particular, xECG is the only publicly availablemodel to perform strongly on all datasets and tasks. By standardisingevaluation, BenchECG enables rigorous comparison and aims to accelerateprogress in ECG representation learning. xECG achieves superior performanceover earlier approaches, defining a new baseline for future ECG foundationmodels.</description>
      <author>example@mail.com (Riccardo Lunelli, Angus Nicolson, Samuel Martin Pröll, Sebastian Johannes Reinstadler, Axel Bauer, Clemens Dlaska)</author>
      <guid isPermaLink="false">2509.10151v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data</title>
      <link>http://arxiv.org/abs/2509.10048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究评估了将变分贝叶斯最后一层（VBLL）集成到表格先验数据拟合网络（TabPFN）中后对不确定性校准性能的影响，并在三个医疗数据集上进行了比较。&lt;h4&gt;背景&lt;/h4&gt;预测模型正被广泛应用于包括医疗诊断和刑事司法等安全关键领域，在这些领域中可靠的不确定性估计至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估VBLL集成到TabPFN中后对不确定性校准性能的影响。&lt;h4&gt;方法&lt;/h4&gt;在三个基准医疗表格数据集上比较原始TabPFN和VBLL集成版本的性能。&lt;h4&gt;主要发现&lt;/h4&gt;与预期相反，原始TabPFN在所有数据集上的不确定性校准性能均优于VBLL集成的TabPFN。&lt;h4&gt;结论&lt;/h4&gt;尽管VBLL在其他应用中能有效提高不确定性估计，但在与TabPFN集成时，原始TabPFN的不确定性校准表现更好。&lt;h4&gt;翻译&lt;/h4&gt;预测模型正被广泛应用于各种领域，包括医疗诊断和刑事司法等安全关键应用。在这样的场景中，可靠的不确定性估计是一项关键任务。表格先验数据拟合网络（TabPFN）是最近提出的一种用于表格数据集的机器学习基础模型，它采用生成式Transformer架构。变分贝叶斯最后一层（VBLL）是最先进的轻量级变分公式，能够以最小的计算开销有效提高不确定性估计。在本研究中，我们旨在评估VBLL与最近提出的TabPFN集成后在不确定性校准方面的性能。我们在三个基准医疗表格数据集上进行了实验，比较了原始TabPFN和VBLL集成版本的性能。与预期相反，我们在所有数据集上都观察到原始TabPFN的不确定性校准性能始终优于VBLL集成的TabPFN。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive models are being increasingly used across a wide range of domains,including safety-critical applications such as medical diagnosis and criminaljustice. Reliable uncertainty estimation is a crucial task in such settings.Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machinelearning foundation model for tabular dataset, which uses a generativetransformer architecture. Variational Bayesian Last Layers (VBLL) is astate-of-the-art lightweight variational formulation that effectively improvesuncertainty estimation with minimal computational overhead. In this work we aimto evaluate the performance of VBLL integrated with the recently proposedTabPFN in uncertainty calibration. Our experiments, conducted on threebenchmark medical tabular datasets, compare the performance of the originalTabPFN and the VBLL-integrated version. Contrary to expectations, we observedthat original TabPFN consistently outperforms VBLL integrated TabPFN inuncertainty calibration across all datasets.</description>
      <author>example@mail.com (Madhushan Ramalingam)</author>
      <guid isPermaLink="false">2509.10048v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images</title>
      <link>http://arxiv.org/abs/2509.09952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to SIGGRAPH Asia 2025. Project page:  https://ubisoft-laforge.github.io/world/chord&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的两阶段生成和估计框架用于PBR材质生成，解决了传统方法耗时耗力以及现有方法在质量、灵活性和用户控制方面的不足。&lt;h4&gt;背景&lt;/h4&gt;材质创建和重建对外观建模至关重要，但传统方法需要艺术家投入大量时间和专业知识。近期利用视觉基础模型合成PBR材质的方法在质量、灵活性和用户控制方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效、高质量且能提供灵活用户控制的PBR材质生成方法。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段框架：1)生成阶段：使用微调的扩散模型合成符合用户输入的、可平铺的纹理图像；2)估计阶段：引入链式分解方案，通过将先前提取的表示作为输入传递到单步图像条件扩散模型，顺序预测SVBRDF通道。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在材质生成和估计方面表现出优越性能，材质估计方法在生成的纹理和野外照片上都显示出强大的鲁棒性，且框架具有广泛的适用性。&lt;h4&gt;结论&lt;/h4&gt;该方法高效、高质量且能提供灵活的用户控制，在多种应用场景中表现优异，包括文本到材质、图像到材质、结构引导的生成和材质编辑。&lt;h4&gt;翻译&lt;/h4&gt;材质创建和重建对外观建模至关重要，但传统上需要艺术家投入大量时间和专业知识。虽然最近的方法利用视觉基础模型从用户提供的输入合成PBR材质，但它们在质量、灵活性和用户控制方面常常不尽如人意。我们提出了一种新颖的两阶段生成和估计框架用于PBR材质生成。在生成阶段，微调的扩散模型合成符合用户输入的、可平铺的纹理图像。在估计阶段，我们引入了一种链式分解方案，通过将先前提取的表示作为输入传递到单步图像条件扩散模型，顺序预测SVBRDF通道。我们的方法高效、高质量且能提供灵活的用户控制。我们将我们的方法与现有的材质生成和估计方法进行了比较，展示了优越的性能。我们的材质估计方法在生成的纹理和野外照片上都显示出强大的鲁棒性。此外，我们展示了我们的框架在多种应用中的灵活性，包括文本到材质、图像到材质、结构引导的生成和材质编辑。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决PBR材质生成和估计的问题。传统方法需要专业艺术家花费大量时间，而现有自动生成方法在质量、灵活性和用户控制方面表现不佳。这个问题在现实中很重要，因为材质建模是游戏、电影、虚拟现实等领域的核心需求，影响着数字内容的真实感和视觉效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：多通道压缩限制了灵活性，逆向渲染问题具有不确定性。因此设计了两阶段框架：先生成纹理图像，再分解为材质通道。借鉴了扩散模型（如SDXL）、图像条件扩散模型（如RGB→X）、内在分解方法和材质生成统一框架（如MatFusion）的思想，但进行了创新改进以解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两阶段框架和链式分解（Chord）处理材质生成和估计。第一阶段使用微调的扩散模型生成可平铺的纹理RGB图像；第二阶段按特定顺序（基础颜色→法线→粗糙度和金属度）预测SVBRDF通道，利用LEGO-conditioning为不同模态提供特定权重。整体流程包括纹理生成、材质估计两个主要阶段，训练过程包含预训练和单步两个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）两阶段生成和估计框架；2）链式分解方案（Chord）；3）LEGO-conditioning机制；4）单步训练方法。相比之前的工作，该方法先生成纹理再分解而非直接预测通道，更好地处理了模态间关系，利用扩散模型先验知识提高了质量和效率，并更明确地建模了SVBRDF模态间的内在关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种两阶段生成和估计框架，通过链式分解和LEGO-conditioning实现了高质量、高效率且用户可控的PBR材质生成和估计，显著提升了材质生成质量和用户控制能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Material creation and reconstruction are crucial for appearance modeling buttraditionally require significant time and expertise from artists. While recentmethods leverage visual foundation models to synthesize PBR materials fromuser-provided inputs, they often fall short in quality, flexibility, and usercontrol. We propose a novel two-stage generate-and-estimate framework for PBRmaterial generation. In the generation stage, a fine-tuned diffusion modelsynthesizes shaded, tileable texture images aligned with user input. In theestimation stage, we introduce a chained decomposition scheme that sequentiallypredicts SVBRDF channels by passing previously extracted representation asinput into a single-step image-conditional diffusion model. Our method isefficient, high quality, and enables flexible user control. We evaluate ourapproach against existing material generation and estimation methods,demonstrating superior performance. Our material estimation method shows strongrobustness on both generated textures and in-the-wild photographs. Furthermore,we highlight the flexibility of our framework across diverse applications,including text-to-material, image-to-material, structure-guided generation, andmaterial editing.</description>
      <author>example@mail.com (Zhi Ying, Boxiang Rong, Jingyu Wang, Maoyuan Xu)</author>
      <guid isPermaLink="false">2509.09952v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Segment Anything for Cell Tracking</title>
      <link>http://arxiv.org/abs/2509.09943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于SAM2的零样本细胞追踪框架，解决了现有方法对标记数据集依赖性强且泛化能力有限的问题，实现了在2D和3D显微镜视频中的高精度细胞追踪，无需数据集特定的微调。&lt;h4&gt;背景&lt;/h4&gt;在时间推移显微镜图像序列中追踪细胞和检测有丝分裂事件是生物医学研究中的关键任务，但由于分裂物体、低信噪比、不明确的边界、密集簇和单个细胞的视觉相似性，这仍然极具挑战性。现有的基于深度学习的方法依赖于手动标记的数据集进行训练，这既昂贵又耗时，且由于显微镜数据的巨大多样性，它们对未见过的数据集的泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;克服现有细胞追踪方法对标记数据集的依赖性和泛化能力有限的限制，开发一种无需微调即可跨多样化显微镜数据集泛化的细胞追踪方法。&lt;h4&gt;方法&lt;/h4&gt;通过将Segment Anything 2（SAM2）这种为通用图像和视频分割设计的大型基础模型整合到细胞追踪流程中，构建了一种完全无监督的零样本细胞追踪框架。该方法不依赖于任何特定的训练数据集，也不继承任何特定训练数据集的偏差。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在二维和大规模三维时间推移显微镜视频中都取得了具有竞争力的准确性，同时消除了对数据集特定适应的需求。&lt;h4&gt;结论&lt;/h4&gt;通过整合SAM2大型基础模型，作者提出了一种完全无监督的零样本细胞追踪方法，能够在多样化显微镜数据集上泛化，无需数据集特定的微调，同时保持高准确性。&lt;h4&gt;翻译&lt;/h4&gt;在时间推移显微镜图像序列中追踪细胞和检测有丝分裂事件是生物医学研究中的关键任务。然而，由于分裂物体、低信噪比、不明确的边界、密集簇以及单个细胞的视觉相似性，这仍然极具挑战性。现有的基于深度学习的方法依赖于手动标记的数据集进行训练，这既昂贵又耗时。此外，由于显微镜数据的巨大多样性，它们对未见过的数据集的泛化能力仍然有限。为了克服这些限制，我们通过将Segment Anything 2（SAM2）——一种为通用图像和视频分割设计的大型基础模型——整合到追踪流程中，提出了一种零样本细胞追踪框架。作为一种完全无监督的方法，我们的方法不依赖于或继承任何特定训练数据集的偏差，使其能够在无需微调的情况下跨多样化显微镜数据集进行泛化。我们的方法在二维和大规模三维时间推移显微镜视频中都取得了具有竞争力的准确性，同时消除了对数据集特定适应的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tracking cells and detecting mitotic events in time-lapse microscopy imagesequences is a crucial task in biomedical research. However, it remains highlychallenging due to dividing objects, low signal-tonoise ratios, indistinctboundaries, dense clusters, and the visually similar appearance of individualcells. Existing deep learning-based methods rely on manually labeled datasetsfor training, which is both costly and time-consuming. Moreover, theirgeneralizability to unseen datasets remains limited due to the vast diversityof microscopy data. To overcome these limitations, we propose a zero-shot celltracking framework by integrating Segment Anything 2 (SAM2), a large foundationmodel designed for general image and video segmentation, into the trackingpipeline. As a fully-unsupervised approach, our method does not depend on orinherit biases from any specific training dataset, allowing it to generalizeacross diverse microscopy datasets without finetuning. Our approach achievescompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videoswhile eliminating the need for dataset-specific adaptation.</description>
      <author>example@mail.com (Zhu Chen, Mert Edgü, Er Jin, Johannes Stegmaier)</author>
      <guid isPermaLink="false">2509.09943v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios</title>
      <link>http://arxiv.org/abs/2509.09926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为LoFT的长尾半监督学习框架，通过基础模型微调解决传统方法中的过度自信和低质量伪标签问题，并进一步提出LoFT-OW处理开放世界条件下的半监督学习。实验表明，即使只使用以往方法1%的未标记数据，新方法也能取得优越性能。&lt;h4&gt;背景&lt;/h4&gt;长尾学习在现实场景中应用广泛。长尾半监督学习(LTSSL)通过整合大量未标记数据到不平衡标记数据集中成为有效解决方案，但现有方法多从零开始训练，导致过度自信和低质量伪标签问题。&lt;h4&gt;目的&lt;/h4&gt;扩展LTSSL到基础模型微调范式，解决从头训练导致的问题；探索更实际的开放世界条件下的半监督学习场景。&lt;h4&gt;方法&lt;/h4&gt;提出LoFT(Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning)框架，利用微调基础模型生成更可靠伪标签；提出LoFT-OW处理开放世界条件下的半监督学习问题，提高模型判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;微调的基础模型能生成更可靠的伪标签；在多个基准测试上，新方法性能优于以前方法；即使只使用以往方法1%的未标记数据，也能获得更好性能。&lt;h4&gt;结论&lt;/h4&gt;LoFT框架有效解决了LTSSL中的过度自信和低质量伪标签问题；LoFT-OW能处理开放世界条件下的半监督学习；基础模型微调是解决长尾半监督学习问题的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;长尾学习由于其现实场景中的广泛适用性而获得越来越多的关注。在现有方法中，长尾半监督学习(LTSSL)通过将大量未标记数据整合到不平衡标记数据集中，已成为一种有效的解决方案。然而，大多数先前的LTSSL方法设计为从头开始训练模型，这常常导致过度自信和低质量伪标签等问题。为了解决这些挑战，我们将LTSSL扩展到基础模型微调范式，并提出了一种新框架：LoFT(通过参数高效微调的长尾半监督学习)。我们证明微调的基础模型可以生成更可靠的伪标签，从而有利于不平衡学习。此外，我们通过研究开放世界条件下的半监督学习来探索更实际的设置，其中未标记数据可能包括分布外(OOD)样本。为了解决这个问题，我们提出LoFT-OW(开放世界场景下的LoFT)来提高判别能力。在多个基准测试上的实验结果表明，与以前的方法相比，我们的方法取得了优越的性能，即使与以前的工作相比只使用1%的未标记数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-tailed learning has garnered increasing attention due to its wideapplicability in real-world scenarios. Among existing approaches, Long-TailedSemi-Supervised Learning (LTSSL) has emerged as an effective solution byincorporating a large amount of unlabeled data into the imbalanced labeleddataset. However, most prior LTSSL methods are designed to train models fromscratch, which often leads to issues such as overconfidence and low-qualitypseudo-labels. To address these challenges, we extend LTSSL into the foundationmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailedsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstratethat fine-tuned foundation models can generate more reliable pseudolabels,thereby benefiting imbalanced learning. Furthermore, we explore a morepractical setting by investigating semi-supervised learning under open-worldconditions, where the unlabeled data may include out-of-distribution (OOD)samples. To handle this problem, we propose LoFT-OW (LoFT under Open-Worldscenarios) to improve the discriminative ability. Experimental results onmultiple benchmarks demonstrate that our method achieves superior performancecompared to previous approaches, even when utilizing only 1\% of the unlabeleddata compared with previous works.</description>
      <author>example@mail.com (Jiahao Chen, Zhiyuan Huang, Yurou Liu, Bing Su)</author>
      <guid isPermaLink="false">2509.09926v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>How well can LLMs provide planning feedback in grounded environments?</title>
      <link>http://arxiv.org/abs/2509.09790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了预训练基础模型（如大型语言模型和视觉语言模型）在具身环境规划中提供反馈的能力，评估了不同类型反馈和推理方法的效果，发现基础模型能提供高质量反馈，且更大规模的模型表现更好。&lt;h4&gt;背景&lt;/h4&gt;在具身环境中进行规划学习通常需要精心设计的奖励函数或高质量标注的演示数据，这增加了学习成本和难度。&lt;h4&gt;目的&lt;/h4&gt;评估基础模型在不同类型环境中提供反馈的能力，研究不同反馈类型和推理方法对规划性能的影响，探索基础模型减少对奖励设计和演示数据依赖的可能性。&lt;h4&gt;方法&lt;/h4&gt;研究者在符号、语言和连续控制环境中评估了LLMs和VLMs的反馈能力，考虑了二元反馈、偏好反馈、行动建议、目标建议和增量动作反馈等反馈类型，以及上下文学习、思维链和访问环境动态信息等推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型能够在不同领域提供多样化的高质量反馈；更大规模的推理模型提供更准确的反馈，表现出更少的偏见，并且从增强的推理方法中获益更多；反馈质量在具有复杂动态或连续状态空间和动作空间的环境中会下降。&lt;h4&gt;结论&lt;/h4&gt;基础模型可以作为规划学习的有效反馈来源，减少对奖励设计和演示数据的依赖，但在复杂动态环境中可能需要额外的改进或辅助方法。&lt;h4&gt;翻译&lt;/h4&gt;在具身环境中学习规划通常需要精心设计的奖励函数或高质量的标注演示。最近的研究表明，预训练的基础模型（如大型语言模型和视觉语言模型）捕捉了对规划有用的背景知识，这减少了策略学习所需的奖励设计和演示数据的数量。我们评估了LLMs和VLMs在符号、语言和连续控制环境中提供反馈的能力。我们考虑了规划的主要反馈类型，包括二元反馈、偏好反馈、行动建议、目标建议和增量动作反馈。我们还研究了影响反馈性能的推理方法，包括上下文学习、思维链和访问环境动态信息。我们发现基础模型能够在不同领域提供多样化的高质量反馈。此外，更大规模的推理模型始终提供更准确的反馈，表现出更少的偏见，并且从增强的推理方法中获益更多。最后，对于具有复杂动态或连续状态空间和动作空间的环境，反馈质量会下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning to plan in grounded environments typically requires carefullydesigned reward functions or high-quality annotated demonstrations. Recentworks show that pretrained foundation models, such as large language models(LLMs) and vision language models (VLMs), capture background knowledge helpfulfor planning, which reduces the amount of reward design and demonstrationsneeded for policy learning. We evaluate how well LLMs and VLMs provide feedbackacross symbolic, language, and continuous control environments. We considerprominent types of feedback for planning including binary feedback, preferencefeedback, action advising, goal advising, and delta action feedback. We alsoconsider inference methods that impact feedback performance, includingin-context learning, chain-of-thought, and access to environment dynamics. Wefind that foundation models can provide diverse high-quality feedback acrossdomains. Moreover, larger and reasoning models consistently provide moreaccurate feedback, exhibit less bias, and benefit more from enhanced inferencemethods. Finally, feedback quality degrades for environments with complexdynamics or continuous state spaces and action spaces.</description>
      <author>example@mail.com (Yuxuan Li, Victor Zhong)</author>
      <guid isPermaLink="false">2509.09790v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>AI-enabled tuberculosis screening in a high-burden setting using cough sound analysis and speech foundation models</title>
      <link>http://arxiv.org/abs/2509.09746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to The Lancet Digital Health&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用基于语音基础模型的深度学习技术分析咳嗽声音，以检测结核病。研究在赞比亚招募了500名参与者，包括TB患者、其他呼吸道疾病患者和健康人。仅使用音频的分类器表现良好，而结合人口统计和临床特征的多模态模型性能更佳，达到WHO目标产品特征基准。&lt;h4&gt;背景&lt;/h4&gt;人工智能可用于检测与疾病相关的咳嗽声音模式，为高负担、低资源环境中的结核病筛查提供可扩展方法。以往研究存在数据集小、症状性非TB患者代表性不足、依赖简单模型、在理想条件下收集录音等局限性。&lt;h4&gt;目的&lt;/h4&gt;开发并评估基于深度学习的咳嗽声音分析模型，用于结核病的筛查和分诊，特别是在资源有限的环境中。&lt;h4&gt;方法&lt;/h4&gt;在赞比亚两家医院招募512名参与者，分为TB患者组、其他呼吸道疾病患者组和健康对照组。从500名参与者获取咳嗽录音及人口统计和临床数据。使用基于语音基础模型的深度学习分类器训练模型，并在3秒片段上表现最佳的模型进一步评估人口统计和临床特征的影响。&lt;h4&gt;主要发现&lt;/h4&gt;仅音频分类器区分TB+与其他人的AUROC为85.2%，区分TB+与其他呼吸道疾病的AUROC为80.1%。加入人口统计和临床特征后，性能提升至92.1%和84.2%。在0.38阈值下，多模态模型对TB+/Rest达到90.3%敏感性和73.1%特异性，对TB+/OR达到80.6%和73.1%特异性。&lt;h4&gt;结论&lt;/h4&gt;基于语音基础模型的咳嗽分析，特别是结合人口统计和临床数据时，作为结核病分诊工具显示出强大潜力，达到WHO目标产品特征基准。该模型对背景噪声、录音时间和设备变异性等混杂因素具有鲁棒性，表明检测到真实的疾病相关声学模式。临床应用前需在不同地区和病例定义中进一步验证。&lt;h4&gt;翻译&lt;/h4&gt;背景：人工智能可检测与疾病相关的咳嗽声音模式，为高负担、低资源环境中的结核病筛查提供可扩展方法。以往研究受限于小数据集、症状性非结核病患者代表性不足、依赖简单模型及在理想条件下收集录音。方法：我们在赞比亚两家医院招募512名参与者，分为细菌学确认的结核病患者、有其他呼吸道疾病的症状患者和健康对照组。从500名参与者获取可用的咳嗽录音及人口统计和临床数据。基于语音基础模型的深度学习分类器在咳嗽录音上进行训练。表现最佳的模型在3秒片段上训练，并进一步使用人口统计和临床特征进行评估。结果：仅音频的最佳分类器区分结核病患者与其他人的AUROC为85.2%，区分结核病患者与其他呼吸道疾病患者的AUROC为80.1%。加入人口统计和临床特征后，性能提升至92.1%和84.2%。在0.38阈值下，多模态模型对结核病患者与其他人达到90.3%敏感性和73.1%特异性，对结核病患者与其他呼吸道疾病患者达到80.6%和73.1%特异性。结论：使用语音基础模型进行咳嗽分析，特别是结合人口统计和临床数据时，显示出作为结核病分诊工具的强大潜力，达到WHO目标产品特征基准。该模型对背景噪声、录音时间和设备变异性等混杂因素具有鲁棒性，表明检测到真实的疾病相关声学模式。临床应用前需在不同地区和病例定义中进一步验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background  Artificial intelligence (AI) can detect disease-related acoustic patterns incough sounds, offering a scalable approach to tuberculosis (TB) screening inhigh-burden, low-resource settings. Previous studies have been limited by smalldatasets, under-representation of symptomatic non-TB patients, reliance onsimple models, and recordings collected under idealised conditions.  Methods  We enrolled 512 participants at two hospitals in Zambia, grouped asbacteriologically confirmed TB (TB+), symptomatic patients with otherrespiratory diseases (OR), and healthy controls (HC). Usable cough recordingsplus demographic and clinical data were obtained from 500 participants. Deeplearning classifiers based on speech foundation models were trained on coughrecordings. The best-performing model, trained on 3-second segments, wasfurther evaluated with demographic and clinical features.  Findings  The best audio-only classifier achieved an AUROC of 85.2% for distinguishingTB+ from all others (TB+/Rest) and 80.1% for TB+ versus OR. Adding demographicand clinical features improved performance to 92.1% (TB+/Rest) and 84.2%(TB+/OR). At a threshold of 0.38, the multimodal model reached 90.3%sensitivity and 73.1% specificity for TB+/Rest, and 80.6% and 73.1% for TB+/OR.  Interpretation  Cough analysis using speech foundation models, especially when combined withdemographic and clinical data, showed strong potential as a TB triage tool,meeting WHO target product profile benchmarks. The model was robust toconfounding factors including background noise, recording time, and devicevariability, indicating detection of genuine disease-related acoustic patterns.Further validation across diverse regions and case definitions, includingsubclinical TB, is required before clinical use.</description>
      <author>example@mail.com (Ning Ma, Bahman Mirheidari, Guy J. Brown, Minyoi M. Maimbolwa, Nsala Sanjase, Solomon Chifwamba, Seke Muzazu, Monde Muyoyeta, Mary Kagujje)</author>
      <guid isPermaLink="false">2509.09746v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>MatSKRAFT: A framework for large-scale materials knowledge extraction from scientific tables</title>
      <link>http://arxiv.org/abs/2509.10448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MatSKRAFT是一个计算框架，能够前所未有地大规模自动提取和整合材料科学知识，从表格数据中构建包含超过535,000个条目的综合数据库，显著优于现有模型，并促进材料发现。&lt;h4&gt;背景&lt;/h4&gt;科学进步越来越需要综合大量文献中的知识，但大多数实验数据仍然被困在半结构化格式中，难以进行系统提取和分析。&lt;h4&gt;目的&lt;/h4&gt;开发一个计算框架，能够前所未有地大规模自动提取和整合材料科学知识。&lt;h4&gt;方法&lt;/h4&gt;MatSKRAFT将表格转换为基于图的表示，通过约束驱动的图神经网络处理，并将科学原理直接编码到模型架构中。&lt;h4&gt;主要发现&lt;/h4&gt;MatSKRAFT在属性提取方面达到88.68的F1分数，在成分提取方面达到71.35的F1分数，处理速度比其他模型快19-496倍，硬件要求适中；应用于47,000多篇研究论文的近69,000个表格，构建了包含535,000多个条目和104,000种成分的综合数据库，扩展了现有数据库覆盖范围，揭示了具有独特属性组合的材料，并实现了成分-属性关系的发现。&lt;h4&gt;结论&lt;/h4&gt;这种系统化的方法为材料和科学发现奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;科学进步越来越依赖于综合大量文献中的知识，但大多数实验数据仍然被困在半结构化格式中，难以进行系统提取和分析。在此，我们提出了MatSKRAFT，一个计算框架，能够前所未有地大规模自动提取和整合材料科学知识。我们的方法将表格转换为基于图的表示，通过约束驱动的图神经网络处理，将科学原理直接编码到模型架构中。MatSKRAFT显著优于最先进的大型语言模型，在属性提取方面达到88.68的F1分数，在成分提取方面达到71.35的F1分数，同时处理速度比这些模型快19-496倍（与最慢和最快模型相比），且硬件要求适中。应用于来自47,000多篇研究论文的近69,000个表格，我们构建了一个包含超过535,000个条目的综合数据库，其中包括104,000种成分，扩展了现有数据库的覆盖范围，有待人工验证。这种系统化的方法揭示了以前被忽视的具有独特属性组合的材料，并实现了构成材料和科学发现基石的成分-属性关系的发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific progress increasingly depends on synthesizing knowledge acrossvast literature, yet most experimental data remains trapped in semi-structuredformats that resist systematic extraction and analysis. Here, we presentMatSKRAFT, a computational framework that automatically extracts and integratesmaterials science knowledge from tabular data at unprecedented scale. Ourapproach transforms tables into graph-based representations processed byconstraint-driven GNNs that encode scientific principles directly into modelarchitecture. MatSKRAFT significantly outperforms state-of-the-art largelanguage models, achieving F1 scores of 88.68 for property extraction and 71.35for composition extraction, while processing data $19$-$496\times$ faster thanthem (compared to the slowest and the fastest models, respectively) with modesthardware requirements. Applied to nearly 69,000 tables from more than 47,000research publications, we construct a comprehensive database containing over535,000 entries, including 104,000 compositions that expand coverage beyondmajor existing databases, pending manual validation. This systematic approachreveals previously overlooked materials with distinct property combinations andenables data-driven discovery of composition-property relationships forming thecornerstone of materials and scientific discovery.</description>
      <author>example@mail.com (Kausik Hira, Mohd Zaki, Mausam, N. M. Anoop Krishnan)</author>
      <guid isPermaLink="false">2509.10448v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Modeling and Risk Zoning of Global Extreme Precipitation via Graph Neural Networks and r-Pareto Processes</title>
      <link>http://arxiv.org/abs/2509.10362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合图神经网络与r-Pareto过程(GNN-rP)的混合框架，用于空间极端降水建模和风险分区，能够学习非线性、非平稳依赖结构，并应用于高风险区域识别和气候变化下的热点检测。&lt;h4&gt;背景&lt;/h4&gt;极端降水事件对人类社会构成重大威胁，可引发大范围复合洪水、山体滑坡和基础设施故障，传统统计空间极值模型存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合框架用于空间极端降水建模和风险分区，能够学习非线性、非平稳依赖结构，并识别气候变化下的高风险区域。&lt;h4&gt;方法&lt;/h4&gt;提出结合图神经网络与r-Pareto过程(GNN-rP)的混合框架，从降水衍生的空间图中学习依赖结构，应用数据驱动的尾部函数建模低维嵌入空间中的联合超越概率，使用NASA的IMERG观测数据和CMIP6 SSP5-8.5投影进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;框架成功描绘了连贯的高风险区域，量化了其时序持续性，检测到气候变化下的新兴热点；与基线方法相比显著提高了高风险网格单元的检测能力；结果显示热带带特别是季风和对流区存在持续高风险区域，并揭示十年尺度持续性在高排放情景下被偶发性重组中断。&lt;h4&gt;结论&lt;/h4&gt;通过结合机器学习与极值理论，GNN-rP提供了一种可扩展、可解释的自适应气候风险分区工具，可直接应用于基础设施规划、防灾准备和气候韧性政策设计。&lt;h4&gt;翻译&lt;/h4&gt;发生在广大空间区域的极端降水事件对社会构成重大威胁，因为它们可能在大范围内引发复合洪水、山体滑坡和基础设施故障。本文提出了一种用于空间极端降水建模和风险分区的混合框架，该框架结合了图神经网络与r-Pareto过程(GNN-rP)。与传统统计空间极值模型不同，这种方法从降水衍生的空间图中学习非线性、非平稳的依赖结构，并应用数据驱动的尾部函数来建模低维嵌入空间中的联合超越概率。利用NASA的IMERG观测数据(2000-2021)和CMIP6 SSP5-8.5投影，该框架描绘了连贯的高风险区域，量化了它们的时序持续性，并检测了气候变化下的新兴热点。与两种基线方法相比，GNN-rP管道显著提高对高风险网格单元的逐点检测能力，同时保持了相当的聚类稳定性。结果突显了热带带特别是季风和对流区存在持续高风险区域，并揭示了在高排放情景下被偶发性重组所中断的十年尺度持续性。通过将机器学习与极值理论相结合，GNN-rP为自适应气候风险分区提供了一种可扩展、可解释的工具，可直接应用于基础设施规划、防灾准备和气候韧性政策设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extreme precipitation events occurring over large spatial domains posesubstantial threats to societies because they can trigger compound flooding,landslides, and infrastructure failures across wide areas. A hybrid frameworkfor spatial extreme precipitation modeling and risk zoning is proposed thatintegrates graph neural networks with r-Pareto processes (GNN-rP). Unliketraditional statistical spatial extremes models, this approach learnsnonlinear, nonstationary dependence structures from precipitation-derivedspatial graphs and applies a data-driven tail functional to model jointexceedances in a low-dimensional embedding space. Using NASA's IMERGobservations (2000-2021) and CMIP6 SSP5-8.5 projections, the frameworkdelineates coherent high-risk zones, quantifies their temporal persistence, anddetects emerging hotspots under climate change. Compared with two baselineapproaches, the GNN-rP pipeline substantially improves pointwise detection ofhigh-risk grid cells while yielding comparable clustering stability. Resultshighlight persistent high-risk regions in the tropical belt, especially monsoonand convective zones, and reveal decadal-scale persistence that is punctuatedby episodic reconfigurations under high-emission scenarios. By coupling machinelearning with extreme value theory, GNN-rP offers a scalable, interpretabletool for adaptive climate risk zoning, with direct applications ininfrastructure planning, disaster preparedness, and climate-resilient policydesign.</description>
      <author>example@mail.com (Zimu Wang, Yifan Wu, Daning Bi)</author>
      <guid isPermaLink="false">2509.10362v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Why does your graph neural network fail on some graphs? Insights from exact generalisation error</title>
      <link>http://arxiv.org/abs/2509.10337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过信号处理的视角，推导了图神经网络(GNNs)在转导固定设计设置下的确切泛化误差，解释了GNNs成功或失败的原因，并为模型选择提供了实践指导。&lt;h4&gt;背景&lt;/h4&gt;图神经网络被广泛用于图结构数据学习，但对它们成功或失败的原因缺乏原则性理解。先前研究关注架构限制如过平滑和过挤压，但不能解释GNNs如何提取有意义表示或为什么相似架构间性能差异大。这些问题与泛化能力相关，而现有GNN泛化误差界限通常宽松、仅限于单一架构，且对实践中泛化机制的洞察有限。&lt;h4&gt;目的&lt;/h4&gt;通过信号处理方法推导GNNs的确切泛化误差，解释GNNs何时以及为何能有效利用结构和特征信息，为模型选择提供实践指导。&lt;h4&gt;方法&lt;/h4&gt;采用信号处理视角，将GNNs解释为通过图结构作用于节点特征的图滤波器算子。专注于线性GNNs同时允许图滤波器中的非线性，推导出包括卷积、基于PageRank和基于注意力模型在内的广泛GNNs的第一个确切泛化误差。&lt;h4&gt;主要发现&lt;/h4&gt;泛化误差的确切表征揭示出只有节点特征与图结构之间对齐的信息才对泛化有贡献；量化了同质性(homophily)对泛化的影响。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了一个框架，解释了GNNs何时以及为何能有效利用结构和特征信息，为模型选择提供了实践指导。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)被广泛应用于图结构数据的学习，然而对它们成功或失败的原因仍缺乏原则性理解。虽然先前研究已经考察了架构限制如过平滑和过挤压，但这些并不能解释GNNs如何提取有意义表示或为什么相似架构间性能差异巨大。这些问题与泛化能力相关：模型对未标记数据做出准确预测的能力。尽管已有研究推导了GNNs的泛化误差界限，但这些界限通常宽松、仅限于单一架构，且对实践中控制泛化的因素提供有限洞察。在本工作中，我们通过信号处理的视角，采用不同方法推导了GNNs在转导固定设计设置下的确切泛化误差。从这一观点看，GNNs可被解释为通过图结构作用于节点特征的图滤波器算子。在专注于线性GNNs的同时允许图滤波器中的非线性，我们推导出广泛GNNs（包括卷积、基于PageRank和基于注意力的模型）的第一个确切泛化误差。泛化误差的确切表征揭示出只有节点特征与图结构之间对齐的信息才对泛化有贡献。此外，我们量化了同质性对泛化的影响。我们的工作提供了一个框架，解释了GNNs何时以及为何能有效利用结构和特征信息，为模型选择提供了实践指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are widely used in learning on graph-structureddata, yet a principled understanding of why they succeed or fail remainselusive. While prior works have examined architectural limitations such asover-smoothing and over-squashing, these do not explain what enables GNNs toextract meaningful representations or why performance varies drasticallybetween similar architectures. These questions are related to the role ofgeneralisation: the ability of a model to make accurate predictions onunlabelled data. Although several works have derived generalisation errorbounds for GNNs, these are typically loose, restricted to a singlearchitecture, and offer limited insight into what governs generalisation inpractice. In this work, we take a different approach by deriving the exactgeneralisation error for GNNs in a transductive fixed-design setting throughthe lens of signal processing. From this viewpoint, GNNs can be interpreted asgraph filter operators that act on node features via the graph structure. Byfocusing on linear GNNs while allowing non-linearity in the graph filters, wederive the first exact generalisation error for a broad range of GNNs,including convolutional, PageRank-based, and attention-based models. The exactcharacterisation of the generalisation error reveals that only the alignedinformation between node features and graph structure contributes togeneralisation. Furthermore, we quantify the effect of homophily ongeneralisation. Our work provides a framework that explains when and why GNNscan effectively leverage structural and feature information, offering practicalguidance for model selection.</description>
      <author>example@mail.com (Nil Ayday, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar)</author>
      <guid isPermaLink="false">2509.10337v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Approximate Graph Propagation Revisited: Dynamic Parameterized Queries, Tighter Bounds and Dynamic Updates</title>
      <link>http://arxiv.org/abs/2509.10036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文重新研究了近似图传播(AGP)框架，针对动态图和动态参数化查询场景提出了改进算法AGP-Static++和AGP-Dynamic，显著提高了查询效率和更新性能。&lt;h4&gt;背景&lt;/h4&gt;AGP是一个统一框架，可以捕获各种图传播任务，如PageRank、图神经网络中的特征传播和基于图的检索增强生成(RAG)。在动态图和动态参数化查询场景下，底层图随时间演变，查询参数根据应用需求即时指定。&lt;h4&gt;目的&lt;/h4&gt;解决现有AGP-Static算法在动态参数化查询和动态图场景下的局限性，包括查询时间复杂度高和动态图更新效率低的问题。&lt;h4&gt;方法&lt;/h4&gt;提出两种新算法：1) AGP-Static++：简化算法设计，将查询复杂度降低约对数平方n因子，同时保留近似保证；2) AGP-Dynamic：实现每次更新常数时间的摊销时间，显著提高动态图更新效率，同时保持查询复杂度和近似保证。&lt;h4&gt;主要发现&lt;/h4&gt;1) AGP-Static可适应支持动态参数化查询，但存在查询复杂度高和动态图更新效率低的问题；2) AGP-Static++能显著降低查询复杂度；3) AGP-Dynamic能大幅提高动态图更新效率，实现常数时间摊销每次更新。&lt;h4&gt;结论&lt;/h4&gt;提出的AGP-Static++和AGP-Dynamic算法在理论和实验上都显著优于现有方法，与基线相比，在更新时间上实现了高达177倍的加速，在查询效率上实现了10倍的加速。&lt;h4&gt;翻译&lt;/h4&gt;我们重新研究了近似图传播(AGP)，这是一个统一框架，可以捕获各种图传播任务，如PageRank、图神经网络(GNN)中的特征传播和基于图的检索增强生成(RAG)。我们的工作重点是动态图和动态参数化查询的场景，其中底层图随时间演变（通过边插入或删除更新），输入查询参数根据应用需求即时指定。我们的第一个贡献是一个有趣的观察：SOTA解决方案AGP-Static可以适应支持动态参数化查询；然而，仍有几个挑战未解决。首先，AGP-Static的查询时间复杂度基于一个假设：在其查询算法中使用子集采样的最优算法。不幸的是，当时不存在这样的算法；没有这种最优算法，查询复杂度需要额外的对数平方n因子，其中n是图中的顶点数。其次，AGP-Static在动态图上表现不佳，每次更新需要n乘以对数n的时间。为了解决这些挑战，我们提出了一种新算法AGP-Static++，它更简单，同时将查询复杂度大致降低了对数平方n因子，同时保留了AGP-Static的近似保证。然而，AGP-Static++仍然需要n时间来处理每个更新。为了更好地支持动态图，我们进一步提出了AGP-Dynamic，它实现了每次更新常数时间的摊销时间，显著改善了前面提到的每次更新n的界限，同时仍然保留了查询复杂度和近似保证。最后，我们的全面实验验证了理论改进：与基线相比，我们的算法在更新时间上实现了高达177倍的加速，在查询效率上实现了10倍的加速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We revisit Approximate Graph Propagation (AGP), a unified framework whichcaptures various graph propagation tasks, such as PageRank, feature propagationin Graph Neural Networks (GNNs), and graph-based Retrieval-Augmented Generation(RAG). Our work focuses on the settings of dynamic graphs and dynamicparameterized queries, where the underlying graphs evolve over time (updated byedge insertions or deletions) and the input query parameters are specified onthe fly to fit application needs. Our first contribution is an interestingobservation that the SOTA solution, AGP-Static, can be adapted to supportdynamic parameterized queries; however several challenges remain unresolved.Firstly, the query time complexity of AGP-Static is based on an assumption ofusing an optimal algorithm for subset sampling in its query algorithm.Unfortunately, back to that time, such an algorithm did not exist; without suchan optimal algorithm, an extra $O(\log^2 n)$ factor is required in the querycomplexity, where $n$ is the number of vertices in the graphs. Secondly,AGP-Static performs poorly on dynamic graphs, taking $O(n\log n)$ time toprocess each update. To address these challenges, we propose a new algorithm,AGP-Static++, which is simpler yet reduces roughly a factor of $O(\log^2 n)$ inthe query complexity while preserving the approximation guarantees ofAGP-Static. However, AGP-Static++ still requires $O(n)$ time to process eachupdate. To better support dynamic graphs, we further propose AGP-Dynamic, whichachieves $O(1)$ amortized time per update, significantly improving theaforementioned $O(n)$ per-update bound, while still preserving the querycomplexity and approximation guarantees. Last, our comprehensive experimentsvalidate the theoretical improvements: compared to the baselines, our algorithmachieves speedups of up to $177\times$ on update time and $10\times$ on queryefficiency.</description>
      <author>example@mail.com (Zhuowei Zhao, Zhuo Zhang, Hanzhi Wang, Junhao Gan, Zhifeng Bao, Jianzhong Qi)</author>
      <guid isPermaLink="false">2509.10036v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>HGEN: Heterogeneous Graph Ensemble Networks</title>
      <link>http://arxiv.org/abs/2509.09843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper is in proceedings of the 34th IJCAI Conference, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HGEN是一种创新的异构图集成学习方法，通过元路径和转换优化流程集成多个学习器，提高分类准确性。&lt;h4&gt;背景&lt;/h4&gt;异构图中的节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来挑战，特别是在适应多样化的图学习器方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理异构图的集成学习框架，提高分类准确性，同时确保学习器之间的多样性。&lt;h4&gt;方法&lt;/h4&gt;HGEN使用元路径结合随机丢弃创建等位图神经网络(Allele GNNs)，训练基础图学习器并进行对齐。框架包含两个关键组件：残差注意力机制校准不同元路径的GNNs，使节点嵌入更关注信息量大的图；相关性正则化项扩大不同元路径生成的嵌入矩阵差异，增强学习器多样性。&lt;h4&gt;主要发现&lt;/h4&gt;HGEN通过残差注意力机制和相关正则化项有效提高了基础学习器的准确性和多样性。实验表明HGEN的正则化程度高于简单投票，在五个异构网络上的表现显著优于最先进竞争对手。&lt;h4&gt;结论&lt;/h4&gt;HGEN为异构图的集成学习提供了有效解决方案，通过创新的元路径和优化流程，显著提升了分类性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了HGEN，这是一种开创性的异构图集成学习方法。我们认为，节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来了重大挑战，特别是在适应多样化的图学习器方面。我们的HGEN框架通过基于元路径和转换的优化流程集成多个学习器，以提高分类准确性。具体来说，HGEN使用元路径结合随机丢弃创建等位图神经网络(GNNs)，基础图学习器在这些网络上进行训练和对齐以便后续集成。为确保有效的集成学习，HGEN提出了两个关键组件：1)残差注意力机制用于校准不同元路径的等位GNNs，使节点嵌入更专注于信息量更大的图，提高基础学习器准确性；2)相关性正则化项扩大不同元路径生成的嵌入矩阵之间的差异，从而丰富基础学习器多样性。我们分析了HGEN的收敛性，并证明其正则化程度高于简单投票。在五个异构网络上的实验验证了HGEN以显著优势持续优于最先进的竞争对手。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents HGEN that pioneers ensemble learning for heterogeneousgraphs. We argue that the heterogeneity in node types, nodal features, andlocal neighborhood topology poses significant challenges for ensemble learning,particularly in accommodating diverse graph learners. Our HGEN frameworkensembles multiple learners through a meta-path and transformation-basedoptimization pipeline to uplift classification accuracy. Specifically, HGENuses meta-path combined with random dropping to create Allele Graph NeuralNetworks (GNNs), whereby the base graph learners are trained and aligned forlater ensembling. To ensure effective ensemble learning, HGEN presents two keycomponents: 1) a residual-attention mechanism to calibrate allele GNNs ofdifferent meta-paths, thereby enforcing node embeddings to focus on moreinformative graphs to improve base learner accuracy, and 2) acorrelation-regularization term to enlarge the disparity among embeddingmatrices generated from different meta-paths, thereby enriching base learnerdiversity. We analyze the convergence of HGEN and attest its higherregularization magnitude over simple voting. Experiments on five heterogeneousnetworks validate that HGEN consistently outperforms its state-of-the-artcompetitors by substantial margin.</description>
      <author>example@mail.com (Jiajun Shen, Yufei Jin, Yi He, Xingquan Zhu)</author>
      <guid isPermaLink="false">2509.09843v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Commissioning and Testing of IceAct Telescopes at the IceCube Neutrino Observatory</title>
      <link>http://arxiv.org/abs/2509.09778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IceAct是一个位于IceCube中微子天文台上方的成像切伦科夫望远镜阵列，专为南极恶劣环境设计。该系统由七个望远镜组成'苍蝇眼'配置和一个额外的望远镜用于立体观测。研究团队进行了严格的测试和校准程序，包括现场几何对齐和图神经网络重建初级粒子方向。&lt;h4&gt;背景&lt;/h4&gt;IceAct是一个部署在南极冰面上的成像切伦科夫望远镜阵列，位于著名的IceCube中微子天文台上方。每个望远镜配备硅光电倍增器相机和菲涅尔透镜，具有12度视场。&lt;h4&gt;目的&lt;/h4&gt;研究旨在介绍IceAct望远镜的测试程序、现场对齐校准方法，以及使用图神经网络重建初级粒子方向的技术，并验证其在实际数据中的应用。&lt;h4&gt;方法&lt;/h4&gt;研究团队通过比较IceCube测量的μ子方向重建与IceAct重建的初级粒子方向，推导出每个IceAct望远镜的几何对齐。此外，还采用了图神经网络技术重建初级粒子方向，并通过蒙特卡洛模拟进行验证，最后应用于调试数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究展示了IceAct望远镜的测试程序，包括夜空观测和低温测试；开发了现场对齐校准方法；实现了基于图神经网络的初级粒子方向重建；验证了该方法在蒙特卡洛模拟中的有效性；并将该方法成功应用于调试数据集。&lt;h4&gt;结论&lt;/h4&gt;IceAct望远镜阵列已成功部署并进行了全面测试和校准，图神经网络重建方法表现良好，为未来中微子和宇宙射线观测提供了可靠的工具。&lt;h4&gt;翻译&lt;/h4&gt;IceAct是一个位于IceCube中微子天文台冰面上方的成像切伦科夫望远镜阵列。每个望远镜配备基于硅光电倍增器的61像素相机和菲涅尔透镜作为成像光学元件，产生12度的视场。设计针对恶劣环境进行了优化，特别是在南极。该装置将由七个望远镜组成在所谓的'苍蝇眼'配置中，将视场扩大到36度，另外还有一个相距200米的望远镜用于立体观测。在部署前进行了严格的测试程序，确保在这些条件下能够运行，例如夜空观测和低温测试。此外，现场校准用于验证安装的准确性和可靠性。我们通过比较使用IceCube测量的μ子的方向重建与IceAct对应初级粒子方向重建，推导出每个IceAct望远镜的几何对齐。本贡献展示了这些测试程序。此外，我们展示了现场对齐校准，包括IceAct中初级粒子方向的图神经网络重建、蒙特卡洛模拟验证以及应用于调试数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; IceAct is an array of imaging air Cherenkov telescopes located at the icesurface above the IceCube Neutrino Observatory. Each telescope features asilicon photomultiplier based 61 pixel camera and a Fresnel-lens as imagingoptic, resulting in a 12-degree field of view. The design is optimized to beoperated in harsh environments, particularly at the South Pole. The setup willconsist of seven telescopes in a so-called fly's eye configuration, increasingthe field of view to 36^\circ, and an additional telescope 200m apart forstereoscopic observations. Rigorous testing procedures have been performedbefore deployment to ensure that operation under these conditions is possible,e.g. night sky observations and cold temperature tests. Furthermore, on-sitecalibrations are used to verify the accuracy and reliability of theinstallation. We derive the geometric alignment of each IceAct telescope bycomparing the directional reconstruction of muons measured with IceCube to thecorresponding primary particle direction reconstruction from IceAct. Thiscontribution presents these testing procedures. Additionally, we present theon-site alignment calibration, including a Graph Neural Network reconstructionfor the primary particle direction in IceAct, verification on Monte Carlosimulation, and the application to a commissioning dataset.</description>
      <author>example@mail.com (Arun Vaidyanathan, Lars Heuermann)</author>
      <guid isPermaLink="false">2509.09778v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario</title>
      <link>http://arxiv.org/abs/2509.10096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to RA-L 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于辅助机器人的交互感知运动预测方法，通过创建新数据集和开发基于Transformer的条件去噪扩散模型，有效捕捉了护工与护理接受者之间的耦合动力学，显著提升了机器人在物理交互场景中的辅助能力。&lt;h4&gt;背景&lt;/h4&gt;劳动力短缺和人口老龄化问题日益严重，需要辅助机器人来支持人类护理接受者。在物理交互场景中，机器人需要准确预测人类动作以确保安全和响应性帮助，然而这仍然是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;解决辅助环境中可变性和物理交互中耦合动力学复杂性的挑战，实现安全、响应性辅助。&lt;h4&gt;方法&lt;/h4&gt;创建HHI-Assist数据集，包含辅助任务中人类-人类交互的运动捕捉片段；开发基于Transformer的条件去噪扩散模型，用于预测交互代理的姿态。&lt;h4&gt;主要发现&lt;/h4&gt;模型能有效捕捉护工和护理接受者之间的耦合动力学，相比基线方法有所改进，对未见过的场景有很强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过推进交互感知的运动预测和引入新数据集，显著增强机器人辅助策略。数据集和代码已在https://sites.google.com/view/hhi-assist/home公开可用。&lt;h4&gt;翻译&lt;/h4&gt;日益严重的劳动力短缺和人口老龄化凸显了对辅助机器人的需求，以支持人类护理接受者。为了实现安全且响应迅速的辅助，机器人在物理交互场景中需要准确预测人类动作。然而，由于辅助环境的多变性和物理交互中耦合动力学的复杂性，这仍然是一项具有挑战性的任务。在本工作中，我们通过两个关键贡献解决了这些挑战：(1) HHI-Assist，一个包含辅助任务中人类-人类交互动作捕捉片段的数据集；(2) 一种基于Transformer的条件去噪扩散模型，用于预测交互代理的姿态。我们的模型有效地捕捉了护工和护理接受者之间的耦合动力学，展示了相比基线方法的改进，并对未见场景表现出强大的泛化能力。通过推进交互感知的运动预测和引入新数据集，我们的工作有潜力显著增强机器人辅助策略。数据集和代码可在以下网址获取：https://sites.google.com/view/hhi-assist/home&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3586011&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing labor shortage and aging population underline the need forassistive robots to support human care recipients. To enable safe andresponsive assistance, robots require accurate human motion prediction inphysical interaction scenarios. However, this remains a challenging task due tothe variability of assistive settings and the complexity of coupled dynamics inphysical interactions. In this work, we address these challenges through twokey contributions: (1) HHI-Assist, a dataset comprising motion capture clips ofhuman-human interactions in assistive tasks; and (2) a conditionalTransformer-based denoising diffusion model for predicting the poses ofinteracting agents. Our model effectively captures the coupled dynamics betweencaregivers and care receivers, demonstrating improvements over baselines andstrong generalization to unseen scenarios. By advancing interaction-awaremotion prediction and introducing a new dataset, our work has the potential tosignificantly enhance robotic assistance policies. The dataset and code areavailable at: https://sites.google.com/view/hhi-assist/home</description>
      <author>example@mail.com (Saeed Saadatnejad, Reyhaneh Hosseininejad, Jose Barreiros, Katherine M. Tsui, Alexandre Alahi)</author>
      <guid isPermaLink="false">2509.10096v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
      <link>http://arxiv.org/abs/2509.06426v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了首个果蝇腿部的三维数据驱动肌肉骨骼模型，填补了现有研究空白。该模型连接了神经活动和运动表现，为理解运动控制提供了新工具。通过结合实验数据和仿真，模型预测了肌肉协同作用，并测试了关节特性对学习的影响。该模型不仅有助于基础科学研究，还可应用于开发更自然的机器人运动控制。&lt;h4&gt;背景&lt;/h4&gt;计算模型对于理解神经、生物力学和物理系统如何相互作用以协调动物行为至关重要。尽管已有果蝇中枢神经系统、肌肉和骨骼的完整重建，但基于解剖学和物理学的果蝇腿肌模型仍然缺失。这些模型是运动神经元活动和关节运动之间不可或缺的桥梁。&lt;h4&gt;目的&lt;/h4&gt;引入第一个果蝇腿部的三维、数据驱动的肌肉骨骼模型，并在OpenSim和MuJoCo仿真环境中实现该模型。&lt;h4&gt;方法&lt;/h4&gt;基于来自多个固定标本的高分辨率X射线扫描，采用Hill型肌肉表示。提出了一个使用形态学成像数据构建肌肉模型的流程，并优化了果蝇特有的未知肌肉参数。将肌肉骨骼模型与来自行为果蝇的详细3D姿态估计数据相结合，在OpenSim中实现肌肉驱动的行为重放。在MuJoCo中训练模仿学习策略，测试不同被动关节特性对学习速度的影响。&lt;h4&gt;主要发现&lt;/h4&gt;模拟多种行走和梳理行为中的肌肉活动预测了协调的肌肉协同作用，这些协同作用可以通过实验进行测试。阻尼和刚度有助于学习。&lt;h4&gt;结论&lt;/h4&gt;该模型使得在实验上易于处理的模式生物中研究运动控制成为可能，深入了解生物力学如何促进复杂肢体运动的产生。该模型还可用于控制具身人工智能体，在模拟环境中生成自然且柔顺的运动。&lt;h4&gt;翻译&lt;/h4&gt;计算模型对于推进我们理解神经、生物力学和物理系统如何相互作用以协调动物行为至关重要。尽管已有果蝇(Drosophila melanogaster)中枢神经系统、肌肉和骨骼的近乎完整重建，但基于解剖学和物理学的果蝇腿肌模型仍然缺失。这些模型是运动神经元活动和关节运动之间不可或缺的桥梁。在此，我们介绍了首个果蝇腿部的三维、数据驱动的肌肉骨骼模型，在OpenSim和MuJoCo仿真环境中实现。我们的模型纳入了基于来自多个固定标本的高分辨率X射线扫描的Hill型肌肉表示。我们提出了一个使用形态学成像数据构建肌肉模型的流程，并优化了果蝇特有的未知肌肉参数。然后，我们将肌肉骨骼模型与来自行为果蝇的详细3D姿态估计数据相结合，在OpenSim中实现肌肉驱动的行为重放。对多种行走和梳理行为中肌肉活动的模拟预测了可通过实验测试的协调肌肉协同作用。此外，通过在MuJoCo中训练模仿学习策略，我们测试了不同被动关节特性对学习速度的影响，发现阻尼和刚度有助于学习。总体而言，我们的模型使得在实验上易于处理的模式生物中研究运动控制成为可能，深入了解生物力学如何促进复杂肢体运动的产生。此外，我们的模型可用于控制具身人工智能体，在模拟环境中生成自然且柔顺的运动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational models are critical to advance our understanding of how neural,biomechanical, and physical systems interact to orchestrate animal behaviors.Despite the availability of near-complete reconstructions of the Drosophilamelanogaster central nervous system, musculature, and exoskeleton, anatomicallyand physically grounded models of fly leg muscles are still missing. Thesemodels provide an indispensable bridge between motor neuron activity and jointmovements. Here, we introduce the first 3D, data-driven musculoskeletal modelof Drosophila legs, implemented in both OpenSim and MuJoCo simulationenvironments. Our model incorporates a Hill-type muscle representation based onhigh-resolution X-ray scans from multiple fixed specimens. We present apipeline for constructing muscle models using morphological imaging data andfor optimizing unknown muscle parameters specific to the fly. We then combineour musculoskeletal models with detailed 3D pose estimation data from behavingflies to achieve muscle-actuated behavioral replay in OpenSim. Simulations ofmuscle activity across diverse walking and grooming behaviors predictcoordinated muscle synergies that can be tested experimentally. Furthermore, bytraining imitation learning policies in MuJoCo, we test the effect of differentpassive joint properties on learning speed and find that damping and stiffnessfacilitate learning. Overall, our model enables the investigation of motorcontrol in an experimentally tractable model organism, providing insights intohow biomechanics contribute to generation of complex limb movements. Moreover,our model can be used to control embodied artificial agents to generatenaturalistic and compliant locomotion in simulated environments.</description>
      <author>example@mail.com (Pembe Gizem Özdil, Chuanfang Ning, Jasper S. Phelps, Sibo Wang-Chen, Guy Elisha, Alexander Blanke, Auke Ijspeert, Pavan Ramdya)</author>
      <guid isPermaLink="false">2509.06426v2</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    </channel>
</rss>