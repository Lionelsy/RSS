<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 22 Sep 2025 15:43:12 +0800</lastBuildDate>
    <item>
      <title>Biologically Plausible Online Hebbian Meta-Learning: Two-Timescale Local Rules for Spiking Neural Brain Interfaces</title>
      <link>http://arxiv.org/abs/2509.14447v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, submitted to ICLR 2026; under review. Replacement  was made to correct mistakes in metadata and header in PDF&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种在线SNN解码器，使用局部三因素学习规则和双时间尺度资格迹，解决了脑机接口中神经信号不稳定和内存限制的问题，实现了内存高效且持续自适应的神经解码。&lt;h4&gt;背景&lt;/h4&gt;脑机接口面临神经信号不稳定和内存限制的挑战，特别是在实时植入式应用中。&lt;h4&gt;目的&lt;/h4&gt;引入一种在线SNN解码器，避免通过时间反向传播，同时保持竞争性性能。&lt;h4&gt;方法&lt;/h4&gt;结合误差调制的Hebbian更新、快速/慢速轨迹合并和自适应学习率控制，仅需O(1)内存，而BPTT方法需要O(T)内存。&lt;h4&gt;主要发现&lt;/h4&gt;在两个灵长类动物数据集上实现了可比的解码精度，内存减少28-35%，且比BPTT训练的SNN收敛更快；闭环模拟展示了适应神经干扰和无需离线校准从头学习的能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作实现了内存高效、持续自适应的神经解码，适用于资源受限的植入式BCI系统。&lt;h4&gt;翻译&lt;/h4&gt;脑机接口面临来自神经信号不稳定和实时植入式应用的内存限制的挑战。我们引入了一种使用局部三因素学习规则和双时间尺度资格迹的在线SNN解码器，避免了通过时间反向传播，同时保持了竞争性性能。我们的方法结合了误差调制的Hebbian更新、快速/慢速轨迹合并和自适应学习率控制，仅需O(1)内存，而BPTT方法需要O(T)内存。在两个灵长类动物数据集上的评估实现了可比的解码精度，内存减少28-35%，且比BPTT训练的SNN收敛更快。使用合成神经群体的闭环模拟展示了适应神经干扰和无需离线校准从头学习的能力。这项工作实现了内存高效、持续自适应的神经解码，适用于资源受限的植入式BCI系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-Computer Interfaces face challenges from neural signal instability andmemory constraints for real-time implantable applications. We introduce anonline SNN decoder using local three-factor learning rules with dual-timescaleeligibility traces that avoid backpropagation through time while maintainingcompetitive performance. Our approach combines error-modulated Hebbian updates,fast/slow trace consolidation, and adaptive learning rate control, requiringonly O(1) memory versus O(T) for BPTT methods. Evaluations on two primatedatasets achieve comparable decoding accuracy (Pearson $R \geq 0.63$ Zenodo, $R\geq 0.81$ MC Maze) with 28-35% memory reduction and faster convergence thanBPTT-trained SNNs. Closed-loop simulations with synthetic neural populationsdemonstrate adaptation to neural disruptions and learning from scratch withoutoffline calibration. This work enables memory-efficient, continuously adaptiveneural decoding suitable for resource-constrained implantable BCI systems.</description>
      <author>example@mail.com (Sriram V. C. Nallani, Gautham Ramachandran, Sahil Shah)</author>
      <guid isPermaLink="false">2509.14447v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
  <item>
      <title>Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2509.15882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CrossI2P是一种自监督框架，通过统一跨模态学习和两阶段配准方法，解决了图像到点云配准中的语义-几何差距和局部最优问题，显著提升了自主系统感知的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;将2D和3D传感器模态桥接对自主系统鲁棒感知至关重要，但图像到点云配准面临语义-几何差距（纹理丰富但深度模糊的图像与稀疏但度量精确的点云之间的差距）和现有方法收敛到局部最优的挑战。&lt;h4&gt;目的&lt;/h4&gt;克服现有图像到点云配准方法的局限性，提高配准的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;CrossI2P采用自监督框架，包含三个关键部分：1)通过双路径对比学习学习几何-语义融合嵌入空间，实现无标注的双向对齐；2)采用粗到细配准范式，先建立超点-超像素对应关系，再进行几何约束的点级细化；3)使用动态训练机制和梯度归一化平衡不同任务的损失。&lt;h4&gt;主要发现&lt;/h4&gt;CrossI2P在KITTI Odometry基准上比最先进方法提高23.7%，在nuScenes上提高37.9%，显著提升了准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;CrossI2P成功解决了2D和3D传感器模态桥接的挑战，特别是在图像到点云配准方面取得了突破性进展。&lt;h4&gt;翻译&lt;/h4&gt;将2D和3D传感器模态桥接起来对自主系统的鲁棒感知至关重要。然而，由于纹理丰富但深度模糊的图像与稀疏但度量精确的点云之间的语义-几何差距，以及现有方法倾向于收敛到局部最优，图像到点云(I2P)配准仍然具有挑战性。为了克服这些局限性，我们引入了CrossI2P，这是一个自监督框架，在单一的端到端管道中统一了跨模态学习和两阶段配准。首先，我们通过双路径对比学习学习几何-语义融合的嵌入空间，实现无标注、双向的2D纹理和3D结构对齐。其次，我们采用粗到细的配准范式：全局阶段通过联合内模态上下文和跨模态交互建模建立超点-超像素对应关系，然后进行几何约束的点级细化以实现精确配准。第三，我们采用具有梯度归一化的动态训练机制来平衡特征对齐、对应关系细化和姿态估计的损失。大量实验表明，CrossI2P在KITTI Odometry基准上比最先进的方法提高了23.7%，在nuScenes上提高了37.9%，显著提高了准确性和鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决图像到点云（I2P）配准问题，特别是2D图像和3D点云之间的语义-几何鸿沟问题。这个问题在自动驾驶和机器人系统中非常重要，因为它关系到如何有效融合来自不同传感器的数据（如摄像头和激光雷达），确保几何一致性，实现准确的环境感知和决策。在现实应用中，移动平台、手持设备等场景常常缺乏预校准的外参，这使得传统方法难以应对，而本文的方法能够在无需精确校准的情况下实现精确配准。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的三大局限：语义不匹配、局部最优收敛问题以及非端到端特性。他们借鉴了对比学习（如CLIP）、粗到细配准策略（如CoFiI2P）和Transformer架构等现有技术，但针对I2P配准的特殊需求进行了创新。作者设计了自监督对比学习模块来弥合语义鸿沟，采用两阶段配准策略避免局部最优，并引入可微PnP模块实现真正的端到端优化。通过动态协作训练机制，作者平衡了不同任务的损失，提高了整体性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自监督学习将2D图像和3D点云映射到共享的几何-语义融合空间，采用粗到细的两阶段配准策略，并利用可微PnP模块实现端到端优化。整体流程包括：1）自监督对比学习模块，使用点云和图像特征提取器提取特征并通过对比损失对齐；2）两阶段特征匹配，先通过Transformer建立全局对应关系，再进行局部点级精炼；3）可微PnP模块，计算姿态损失和重投影误差；4）动态协作训练机制，使用GradNorm平衡不同任务的损失权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）自监督对比学习模块，同时考虑模态内实例区分和跨模态对齐；2）基于Transformer的两阶段配准策略，先全局后局部；3）可微PnP模块，实现真正的端到端优化；4）动态协作训练机制，自适应平衡多任务损失。相比之前的工作，CrossI2P解决了语义对齐与几何优化的冲突，避免了局部最优问题，实现了完整的端到端训练，并在噪声和遮挡等挑战性条件下表现出更强的鲁棒性。实验表明，该方法在KITTI和nuScenes数据集上分别比最先进方法提高了23.7%和37.9%的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CrossI2P通过创新的自监督对比学习、两阶段配准和可微PnP模块，实现了无需标注的高精度、鲁棒性强的图像到点云端到端配准，显著提升了自动驾驶和机器人系统中的多模态感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bridging 2D and 3D sensor modalities is critical for robust perception inautonomous systems. However, image-to-point cloud (I2P) registration remainschallenging due to the semantic-geometric gap between texture-rich butdepth-ambiguous images and sparse yet metrically precise point clouds, as wellas the tendency of existing methods to converge to local optima. To overcomethese limitations, we introduce CrossI2P, a self-supervised framework thatunifies cross-modal learning and two-stage registration in a single end-to-endpipeline. First, we learn a geometric-semantic fused embedding space viadual-path contrastive learning, enabling annotation-free, bidirectionalalignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fineregistration paradigm: a global stage establishes superpoint-superpixelcorrespondences through joint intra-modal context and cross-modal interactionmodeling, followed by a geometry-constrained point-level refinement for preciseregistration. Third, we employ a dynamic training mechanism with gradientnormalization to balance losses for feature alignment, correspondencerefinement, and pose estimation. Extensive experiments demonstrate thatCrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometrybenchmark and by 37.9% on nuScenes, significantly improving both accuracy androbustness.</description>
      <author>example@mail.com (Xingmei Wang, Xiaoyu Hu, Chengkai Huang, Ziyan Zeng, Guohao Nie, Quan Z. Sheng, Lina Yao)</author>
      <guid isPermaLink="false">2509.15882v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning</title>
      <link>http://arxiv.org/abs/2509.16078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICDM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Dual-Masked Autoencoder (DMAE)的新型掩码时间序列建模框架，用于无监督多变量时间序列表示学习。该框架通过两个互补的预训练任务和特征级对齐约束，学习时间上连贯且语义丰富的表示，在多种下游任务上展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;无监督多变量时间序列表示学习旨在从原始序列中提取紧凑且信息丰富的表示，而无需依赖标签，以便有效地迁移到各种下游任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为Dual-Masked Autoencoder (DMAE)的新型掩码时间序列建模框架，用于无监督多变量时间序列表示学习。&lt;h4&gt;方法&lt;/h4&gt;DMAE制定了两个互补的预训练任务：(1)基于可见属性重建掩码值；(2)在教师编码器的指导下估计掩码特征的潜在表示。此外，引入了特征级对齐约束，鼓励预测的潜在表示与教师的输出保持一致。通过联合优化这些目标，DMAE学习时间上连贯且语义丰富的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在分类、回归和预测任务上的综合评估表明，该方法比竞争性基线实现了一致且优越的性能。&lt;h4&gt;结论&lt;/h4&gt;DMAE是一种有效的无监督多变量时间序列表示学习方法，通过双重掩码自编码器和特征级对齐约束，能够学习高质量的时间序列表示。&lt;h4&gt;翻译&lt;/h4&gt;无监督多变量时间序列(MTS)表示学习旨在从原始序列中提取紧凑且信息丰富的表示，而无需依赖标签，从而能够高效地迁移到各种下游任务。在本文中，我们提出了Dual-Masked Autoencoder (DMAE)，一种用于无监督MTS表示学习的新型掩码时间序列建模框架。DMAE制定了两个互补的预训练任务：(1)基于可见属性重建掩码值；(2)在教师编码器的指导下估计掩码特征的潜在表示。为了进一步提高表示质量，我们引入了特征级对齐约束，鼓励预测的潜在表示与教师的输出保持一致。通过联合优化这些目标，DMAE学习时间上连贯且语义丰富的表示。在分类、回归和预测任务上的综合评估表明，我们的方法比竞争性基线实现了一致且优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised multivariate time series (MTS) representation learning aims toextract compact and informative representations from raw sequences withoutrelying on labels, enabling efficient transfer to diverse downstream tasks. Inthis paper, we propose Dual-Masked Autoencoder (DMAE), a novel maskedtime-series modeling framework for unsupervised MTS representation learning.DMAE formulates two complementary pretext tasks: (1) reconstructing maskedvalues based on visible attributes, and (2) estimating latent representationsof masked features, guided by a teacher encoder. To further improverepresentation quality, we introduce a feature-level alignment constraint thatencourages the predicted latent representations to align with the teacher'soutputs. By jointly optimizing these objectives, DMAE learns temporallycoherent and semantically rich representations. Comprehensive evaluationsacross classification, regression, and forecasting tasks demonstrate that ourapproach achieves consistent and superior performance over competitivebaselines.</description>
      <author>example@mail.com (Yi Xu, Yitian Zhang, Yun Fu)</author>
      <guid isPermaLink="false">2509.16078v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>FMD-TransUNet: Abdominal Multi-Organ Segmentation Based on Frequency Domain Multi-Axis Representation Learning and Dual Attention Mechanisms</title>
      <link>http://arxiv.org/abs/2509.16044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FMD-TransUNet的新型框架，通过整合多轴外部权重块(MEWB)和改进的双注意力模块(DA+)，有效提高了腹部多器官分割的准确性，尤其在处理小型、不规则或解剖结构复杂的器官方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;准确的腹部多器官分割对临床应用至关重要，但现有基于深度学习的自动分割方法在分割小型、不规则或解剖结构复杂的器官时仍有困难，且大多数方法专注于空间域分析，忽略了频域表示的协同潜力。&lt;h4&gt;目的&lt;/h4&gt;解决现有腹部多器官分割方法的局限性，提出一种能够精确分割腹部多器官的新型框架。&lt;h4&gt;方法&lt;/h4&gt;创新地将多轴外部权重块(MEWB)和改进的双注意力模块(DA+)集成到TransUNet框架中。MEWB提取多轴频域特征捕捉全局解剖结构和局部边界细节；DA+块利用深度可分离卷积并融入空间和通道注意力机制增强特征融合，减少冗余信息，缩小编码器和解码器间的语义差距。&lt;h4&gt;主要发现&lt;/h4&gt;在Synapse数据集上的实验表明，FMD-TransUNet优于其他最新方法，在八个腹部器官上实现平均81.32%的DSC和16.35毫米的HD。与基线模型相比，平均DSC提高3.84%，平均HD降低15.34毫米。&lt;h4&gt;结论&lt;/h4&gt;FMD-TransUNNet在提高腹部多器官分割准确性方面是有效的，特别是在处理复杂解剖结构的小型器官时。&lt;h4&gt;翻译&lt;/h4&gt;准确的腹部多器官分割对临床应用至关重要。尽管已开发出许多基于深度学习的自动分割方法，但它们在分割小型、不规则或解剖结构复杂的器官时仍然存在困难。此外，大多数当前方法专注于空间域分析，常常忽略了频域表示的协同潜力。为解决这些局限性，我们提出了一种名为FMD-TransUNet的新型框架，用于精确的腹部多器官分割。它创新性地将多轴外部权重块(MEWB)和改进的双注意力模块(DA+)集成到TransUNet框架中。MEWB提取多轴频域特征，以捕捉全局解剖结构和局部边界细节，为空间域表示提供互补信息。DA+块利用深度可分离卷积，并融入空间和通道注意力机制，以增强特征融合，减少冗余信息，缩小编码器和解码器之间的语义差距。在Synapse数据集上的实验验证表明，FMD-TransUNet优于其他最新的最先进方法，在八个腹部器官上实现了平均81.32%的DSC和16.35毫米的HD。与基线模型相比，平均DSC提高了3.84%，平均HD降低了15.34毫米。这些结果证明了FMD-TransUNNet在提高腹部多器官分割准确性方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决腹部多器官分割中难以准确分割小型、不规则或解剖结构复杂器官的问题，以及现有方法过度依赖空间域分析而忽视频率域潜力的问题。这个问题在临床上非常重要，因为准确的腹部多器官分割对疾病诊断、放疗计划和手术导航等关键医疗应用至关重要，同时能减少放射科医生手动标注的工作负担和主观差异，提高诊断一致性和临床效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统方法难以处理复杂器官形状和模糊边界，相邻器官灰度分布重叠，以及器官大小差异大。他们发现大多数方法专注于空间域分析而忽视频率域潜力，以及Transformer模型虽能捕捉全局上下文但缺乏考虑图像特定属性的机制。基于这些分析，作者借鉴了TransUNet的架构，结合了频率域方法如GFNet和GFUNet的思想，但改进为多轴频率分析，并受DA-TransUNet启发改进了双注意力机制，最终设计出FMD-TransUNet框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是融合空间域和频率域表示，通过多轴频率分析提供更丰富的特征表示，并结合改进的双注意力机制增强特征提取能力。整体流程包括：1)编码器部分使用卷积块提取低级特征，MEWB模块增强频率域特征，DA+块细化特征，Transformer层捕捉全局上下文；2)解码器部分通过上采样和特征融合重建高分辨率特征，MEWB模块在每个上采样步骤后应用；3)跳跃连接使用DA+块过滤无关信息，缩小编码器和解码器间的语义差距。MEWB模块通过多轴DFT提取方向频率特征，DA+模块利用深度可分离卷积结合空间和通道注意力机制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多轴外部权重块(MEWB)应用多轴二维离散傅里叶变换提取方向频率特征，提供比单轴分析更丰富的表示；2)改进的双注意力模块(DA+)使用深度可分离卷积减少计算复杂度和冗余信息；3)首次系统性地整合频率域多轴表示和空间域表示，实现互补优势。相比之前工作，FMD-TransUNet不同于传统U-Net变体在于引入了频率域分析和Transformer；不同于其他Transformer模型在于整合了频率域信息；不同于频率域方法在于使用多轴分析而非单轴；不同于双注意力机制模型在于使用深度可分离卷积并结合频率域信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FMD-TransUNet通过创新性地融合多轴频率域特征学习和改进的双注意力机制，显著提高了腹部多器官分割的准确性，特别是在处理小型、不规则和复杂解剖结构器官时展现出卓越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate abdominal multi-organ segmentation is critical for clinicalapplications. Although numerous deep learning-based automatic segmentationmethods have been developed, they still struggle to segment small, irregular,or anatomically complex organs. Moreover, most current methods focus onspatial-domain analysis, often overlooking the synergistic potential offrequency-domain representations. To address these limitations, we propose anovel framework named FMD-TransUNet for precise abdominal multi-organsegmentation. It innovatively integrates the Multi-axis External Weight Block(MEWB) and the improved dual attention module (DA+) into the TransUNetframework. The MEWB extracts multi-axis frequency-domain features to captureboth global anatomical structures and local boundary details, providingcomplementary information to spatial-domain representations. The DA+ blockutilizes depthwise separable convolutions and incorporates spatial and channelattention mechanisms to enhance feature fusion, reduce redundant information,and narrow the semantic gap between the encoder and decoder. Experimentalvalidation on the Synapse dataset shows that FMD-TransUNet outperforms otherrecent state-of-the-art methods, achieving an average DSC of 81.32\% and a HDof 16.35 mm across eight abdominal organs. Compared to the baseline model, theaverage DSC increased by 3.84\%, and the average HD decreased by 15.34 mm.These results demonstrate the effectiveness of FMD-TransUNet in improving theaccuracy of abdominal multi-organ segmentation.</description>
      <author>example@mail.com (Fang Lu, Jingyu Xu, Qinxiu Sun, Qiong Lou)</author>
      <guid isPermaLink="false">2509.16044v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization</title>
      <link>http://arxiv.org/abs/2509.15791v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MS-UDG的新方法，用于解决无监督领域泛化问题。该方法通过学习最小充分语义表示，既保留了共享的语义信息，又去除了与语义无关的变化信息，在多个基准测试上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;深度学习的泛化能力在监督学习场景中已被广泛研究，但在无监督场景中研究较少。无监督领域泛化(UDG)任务旨在增强使用自监督学习等无监督学习技术训练的模型的泛化能力。然而，UDG面临在没有类别标签的情况下区分语义和变化的挑战，且实际环境中通常没有领域标签可用。&lt;h4&gt;目的&lt;/h4&gt;通过将UDG形式化为学习最小充分语义表示的任务来解决这些局限性。这种表示需要满足：(i)保留增强视图间共享的所有语义信息（充分性），(ii)最大程度地去除与语义无关的信息（最小性）。&lt;h4&gt;方法&lt;/h4&gt;从信息论角度为这些目标提供理论依据，表明优化表示以实现充分性和最小性可以直接减少分布外风险。通过MS-UDG实现这种优化，集成了：(a)基于InfoNCE的目标来实现充分性；(b)两个互补组件来促进最小性：一种新的语义-变化解纠缠损失和一种基于重建的机制来捕捉充分的变化。&lt;h4&gt;主要发现&lt;/h4&gt;在流行的无监督领域泛化基准上，MS-UDG设立了新的最先进水平，一致性地优于现有的SSL和UDG方法，且在表示学习过程中不需要类别或领域标签。&lt;h4&gt;结论&lt;/h4&gt;通过学习最小充分语义表示，MS-UDG方法有效解决了无监督领域泛化中的挑战，使模型能够更好地泛化到未见过的领域。&lt;h4&gt;翻译&lt;/h4&gt;深度学习的泛化能力在监督设置中已被广泛研究，但在无监督场景中仍较少探索。最近，无监督领域泛化(UDG)任务被提出，以增强使用普遍的无监督学习技术（如自监督学习SSL）训练的模型的泛化能力。UDG面临在没有类别标签的情况下区分语义与变化的挑战。尽管一些近期方法采用领域标签来解决这个问题，但在实际环境中这些标签通常不可用。在本文中，我们通过将UDG形式化为学习最小充分语义表示的任务来解决这些局限性：一种保留增强视图间所有共享语义信息（充分性）并最大程度去除与语义无关信息（最小性）的表示。我们从信息论角度为这些目标提供理论依据，证明优化表示以实现充分性和最小性可以直接减少分布外风险。实践中，我们通过最小充分UDG(MS-UDG)实现这种优化，这是一个可学习模型，通过集成(a)基于InfoNCE的目标来实现充分性；(b)两个互补组件来促进最小性：一种新的语义-变化解纠缠损失和一种基于重建的机制来捕捉充分的变化。实验上，MS-UDG在流行的无监督领域泛化基准上设立了新的最先进水平，一致性地超越现有的SSL和UDG方法，在表示学习过程中不需要类别或领域标签。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generalization ability of deep learning has been extensively studied insupervised settings, yet it remains less explored in unsupervised scenarios.Recently, the Unsupervised Domain Generalization (UDG) task has been proposedto enhance the generalization of models trained with prevalent unsupervisedlearning techniques, such as Self-Supervised Learning (SSL). UDG confronts thechallenge of distinguishing semantics from variations without category labels.Although some recent methods have employed domain labels to tackle this issue,such domain labels are often unavailable in real-world contexts. In this paper,we address these limitations by formalizing UDG as the task of learning aMinimal Sufficient Semantic Representation: a representation that (i) preservesall semantic information shared across augmented views (sufficiency), and (ii)maximally removes information irrelevant to semantics (minimality). Wetheoretically ground these objectives from the perspective of informationtheory, demonstrating that optimizing representations to achieve sufficiencyand minimality directly reduces out-of-distribution risk. Practically, weimplement this optimization through Minimal-Sufficient UDG (MS-UDG), alearnable model by integrating (a) an InfoNCE-based objective to achievesufficiency; (b) two complementary components to promote minimality: a novelsemantic-variation disentanglement loss and a reconstruction-based mechanismfor capturing adequate variation. Empirically, MS-UDG sets a newstate-of-the-art on popular unsupervised domain-generalization benchmarks,consistently outperforming existing SSL and UDG methods, without category ordomain labels during representation learning.</description>
      <author>example@mail.com (Tan Pan, Kaiyu Guo, Dongli Xu, Zhaorui Tan, Chen Jiang, Deshu Chen, Xin Guo, Brian C. Lovell, Limei Han, Yuan Cheng, Mahsa Baktashmotlagh)</author>
      <guid isPermaLink="false">2509.15791v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SONAR: Self-Distilled Continual Pre-training for Domain Adaptive Audio Representation</title>
      <link>http://arxiv.org/abs/2509.15703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SONAR的自蒸馏持续预训练框架，用于领域自适应的音频表示学习，能够在适应新领域的同时减轻灾难性遗忘。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在大型数据集如AudioSet上已成为音频表示学习的主导范式，新标记音频的持续流入为丰富静态表示提供了机会。&lt;h4&gt;目的&lt;/h4&gt;解决从头重新训练模型的高计算成本问题，避免丢弃先前训练模型中嵌入的宝贵知识，实现高效的持续学习。&lt;h4&gt;方法&lt;/h4&gt;提出SONAR(Self-distilled cONtinual pre-training for domain adaptive Audio Representation)框架，基于BEATs构建，通过三种策略解决关键挑战：为新旧数据实施联合采样策略，应用正则化平衡特异性和通用性，动态扩展tokenizer代码本以适应新的声学模式。&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同领域的实验中，SONAR方法实现了高度的适应性和对遗忘的强大抵抗力。&lt;h4&gt;结论&lt;/h4&gt;SONAR是一种有效的持续预训练框架，能够在适应新音频领域的同时保持对先前知识的记忆，解决了传统方法计算成本高和知识遗忘的问题。&lt;h4&gt;翻译&lt;/h4&gt;在AudioSet等大型数据集上进行自监督学习已成为音频表示学习的主导范式。虽然新标记音频的持续流入为丰富这些静态表示提供了机会，但简单的方法是使用所有可用数据从头开始重新训练模型。然而，这种方法计算上难以承受，并且丢弃了先前训练模型权重中嵌入的宝贵知识。为解决这种低效率问题，我们提出了SONAR(自蒸馏持续预训练用于领域自适应音频表示)，这是一个基于BEATs构建的持续预训练框架。SONAR在有效适应新领域的同时，通过解决三个关键挑战来减轻灾难性遗忘：为新旧数据实施联合采样策略，应用正则化以平衡特异性和通用性，以及为新的声学模式动态扩展tokenizer代码本。在四个不同领域的实验表明，我们的方法实现了高度的适应性和对遗忘的强大抵抗力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) on large-scale datasets like AudioSet hasbecome the dominant paradigm for audio representation learning. While thecontinuous influx of new, unlabeled audio presents an opportunity to enrichthese static representations, a naive approach is to retrain the model fromscratch using all available data. However, this method is computationallyprohibitive and discards the valuable knowledge embedded in the previouslytrained model weights. To address this inefficiency, we propose SONAR(Self-distilled cONtinual pre-training for domain adaptive AudioRepresentation), a continual pre-training framework built upon BEATs. SONAReffectively adapts to new domains while mitigating catastrophic forgetting bytackling three key challenges: implementing a joint sampling strategy for newand prior data, applying regularization to balance specificity and generality,and dynamically expanding the tokenizer codebook for novel acoustic patterns.Experiments across four distinct domains demonstrate that our method achievesboth high adaptability and robust resistance to forgetting.</description>
      <author>example@mail.com (Yizhou Zhang, Yuan Gao, Wangjin Zhou, Zicheng Yuan, Keisuke Imoto, Tatsuya Kawahara)</author>
      <guid isPermaLink="false">2509.15703v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification</title>
      <link>http://arxiv.org/abs/2509.15591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种称为潜在区域网络(LZN)的统一框架，旨在解决机器学习中的三个核心问题：生成建模、表示学习和分类，这些问题的现有解决方案通常相互分离。&lt;h4&gt;背景&lt;/h4&gt;生成建模、表示学习和分类是机器学习的三个核心问题，但它们的最先进解决方案仍然相互独立，缺乏统一性。&lt;h4&gt;目的&lt;/h4&gt;探索能否通过统一原则同时解决这三个问题，以简化机器学习流程并促进任务间的协同作用。&lt;h4&gt;方法&lt;/h4&gt;引入潜在区域网络(LZN)，创建共享的高斯潜在空间编码所有任务信息，为每种数据类型配备编码器和解码器，将机器学习任务表示为这些组件的组合。&lt;h4&gt;主要发现&lt;/h4&gt;LZN在三个场景中展示了潜力：(1)增强现有模型：与Rectified Flow结合，在CIFAR10上FID从2.76提高到2.59；(2)独立解决任务：无需辅助损失函数实现无监督表示学习，在ImageNet上分别比MoCo和SimCLR高9.3%和0.2%；(3)同时解决多个任务：在CIFAR10上同时改进FID并实现最先进的分类准确度。&lt;h4&gt;结论&lt;/h4&gt;LZN为统一解决机器学习的三个核心问题提供了有前景的途径，代码和训练模型已公开。&lt;h4&gt;翻译&lt;/h4&gt;生成建模、表示学习和是机器学习的三个核心问题，然而它们的最先进解决方案仍然 largely disjoint。在本文中，我们提出：能否通过统一原则解决这三个问题？这种统一可以简化机器学习流程并促进任务间的更大协同。我们引入潜在区域网络(LZN)作为实现这一目标的一步。LZN的核心是创建一个共享的高斯潜在空间，编码所有任务的信息。每种数据类型(如图像、文本、标签)都配备一个将样本映射到不重叠潜在区域的编码器，以及一个将潜在空间映射回数据的解码器。机器学习任务表示为这些编码器和解码器的组合：例如，标签条件图像生成使用标签编码器和图像解码器；图像嵌入使用图像编码器；分类使用图像编码器和标签解码器。我们在三个复杂度递增的场景中展示了LZN的潜力：(1) LZN可以增强现有模型(图像生成)：与最先进的Rectified Flow模型结合，LZN将CIFAR10上的FID从2.76提高到2.59，无需修改训练目标。(2) LZN可以独立解决任务(表示学习)：LZN无需辅助损失函数即可实现无监督表示学习，在ImageNet上的下游线性分类分别比开创性的MoCo和SimCLR方法高9.3%和0.2%。(3) LZN可以同时解决多个任务(联合生成和分类)：通过图像和标签编码器/解码器，LZN设计上同时执行这两个任务，在CIFAR10上改进FID并实现最先进的分类准确度。代码和训练模型可在https://github.com/microsoft/latent-zoning-networks获取。项目网站在https://zinanlin.me/blogs/latent_zoning_networks.html。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling, representation learning, and classification are threecore problems in machine learning (ML), yet their state-of-the-art (SoTA)solutions remain largely disjoint. In this paper, we ask: Can a unifiedprinciple address all three? Such unification could simplify ML pipelines andfoster greater synergy across tasks. We introduce Latent Zoning Network (LZN)as a step toward this goal. At its core, LZN creates a shared Gaussian latentspace that encodes information across all tasks. Each data type (e.g., images,text, labels) is equipped with an encoder that maps samples to disjoint latentzones, and a decoder that maps latents back to data. ML tasks are expressed ascompositions of these encoders and decoders: for example, label-conditionalimage generation uses a label encoder and image decoder; image embedding usesan image encoder; classification uses an image encoder and label decoder. Wedemonstrate the promise of LZN in three increasingly complex scenarios: (1) LZNcan enhance existing models (image generation): When combined with the SoTARectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-withoutmodifying the training objective. (2) LZN can solve tasks independently(representation learning): LZN can implement unsupervised representationlearning without auxiliary loss functions, outperforming the seminal MoCo andSimCLR methods by 9.3% and 0.2%, respectively, on downstream linearclassification on ImageNet. (3) LZN can solve multiple tasks simultaneously(joint generation and classification): With image and label encoders/decoders,LZN performs both tasks jointly by design, improving FID and achieving SoTAclassification accuracy on CIFAR10. The code and trained models are availableat https://github.com/microsoft/latent-zoning-networks. The project website isat https://zinanlin.me/blogs/latent_zoning_networks.html.</description>
      <author>example@mail.com (Zinan Lin, Enshu Liu, Xuefei Ning, Junyi Zhu, Wenyu Wang, Sergey Yekhanin)</author>
      <guid isPermaLink="false">2509.15591v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition</title>
      <link>http://arxiv.org/abs/2509.15430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages including reference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BiRQ是一种双层自监督学习框架，结合了BEST-RQ的效率和HuBERT风格标签增强的优势，通过重用模型本身作为伪标签生成器，实现了端到端的迭代标签改进，在多个数据集上展现出优于BEST-RQ的性能。&lt;h4&gt;背景&lt;/h4&gt;语音是丰富信号但标记数据成本高昂，自监督学习对可扩展表征学习至关重要。语音自监督学习的核心挑战是生成既信息丰富又高效的伪标签。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合高效方法与标签增强优势的自监督学习框架，解决现有方法中强标签依赖外部编码器而高效方法标签较弱的问题。&lt;h4&gt;方法&lt;/h4&gt;BiRQ框架使用随机投影量化器对中间表示进行离散化产生增强标签，同时锚定直接从原始输入派生的标签稳定训练。训练表述为一阶双层优化问题，使用可区分的Gumbel-softmax选择端到端解决，消除了对外部标签编码器的需求。&lt;h4&gt;主要发现&lt;/h4&gt;BiRQ在保持低复杂度和计算效率的同时，持续优于BEST-RQ。在960小时的LibriSpeech、150小时的AMI会议和5,000小时的YODAS等多个数据集上验证了该方法的有效性，展现出一致的改进。&lt;h4&gt;结论&lt;/h4&gt;BiRQ成功结合了高效方法和标签增强方法的优势，通过重用模型本身作为伪标签生成器，实现了端到端的迭代标签改进，为语音表征学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;语音是一种丰富的信号，标记的音频-文本对成本高昂，这使得自监督学习对于可扩展的表征学习至关重要。语音自监督学习的一个核心挑战是生成既信息丰富又高效的伪标签：强标签（如HuBERT中使用）能提高下游性能，但依赖外部编码器和多阶段流程；而高效方法如BEST-RQ以简单的实现为代价，产生了较弱的标签。我们提出BiRQ，一种双层自监督学习框架，结合了BEST-RQ的效率和HuBERT风格标签增强的改进优势。核心思想是重用模型本身作为伪标签生成器：中间表示通过随机投影量化器离散化以产生增强标签，而直接从原始输入派生的锚定标签稳定训练并防止崩溃。训练被表述为高效的一阶双层优化问题，使用可区分的Gumbel-softmax选择端到端解决。这种设计消除了对外部标签编码器的需求，降低了内存成本，并实现了端到端的迭代标签改进。BiRQ在保持低复杂度和计算效率的同时，持续优于BEST-RQ。我们在各种数据集上验证了我们的方法，包括960小时的LibriSpeech、150小时的AMI会议和5,000小时的YODAS，展示了相对于BEST-RQ的一致性提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech is a rich signal, and labeled audio-text pairs are costly, makingself-supervised learning essential for scalable representation learning. A corechallenge in speech SSL is generating pseudo-labels that are both informativeand efficient: strong labels, such as those used in HuBERT, improve downstreamperformance but rely on external encoders and multi-stage pipelines, whileefficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.We propose BiRQ, a bilevel SSL framework that combines the efficiency ofBEST-RQ with the refinement benefits of HuBERT-style label enhancement. The keyidea is to reuse part of the model itself as a pseudo-label generator:intermediate representations are discretized by a random-projection quantizerto produce enhanced labels, while anchoring labels derived directly from theraw input stabilize training and prevent collapse. Training is formulated as anefficient first-order bilevel optimization problem, solved end-to-end withdifferentiable Gumbel-softmax selection. This design eliminates the need forexternal label encoders, reduces memory cost, and enables iterative labelrefinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQwhile maintaining low complexity and computational efficiency. We validate ourmethod on various datasets, including 960-hour LibriSpeech, 150-hour AMImeetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.</description>
      <author>example@mail.com (Liuyuan Jiang, Xiaodong Cui, Brian Kingsbury, Tianyi Chen, Lisha Chen)</author>
      <guid isPermaLink="false">2509.15430v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception</title>
      <link>http://arxiv.org/abs/2509.15333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了AdaptiveNN框架，一种从被动到主动、自适应的视觉模型范式转变，通过模拟人类视觉注意力机制实现视觉感知的粗到细顺序决策过程，显著降低推理成本同时保持准确性，并提供更好的可解释性。&lt;h4&gt;背景&lt;/h4&gt;人类视觉具有高度适应性，能通过顺序注视任务相关区域高效采样复杂环境；而现有机器视觉模型被动一次性处理整个场景，导致资源需求随输入分辨率和模型规模过度增加，限制了未来进展和实际应用。&lt;h4&gt;目的&lt;/h4&gt;引入AdaptiveNN框架，推动视觉模型从'被动'到'主动、自适应'的范式转变。&lt;h4&gt;方法&lt;/h4&gt;AdaptiveNN将视觉感知制定为从粗到细的顺序决策过程，逐步识别和关注任务相关区域，跨注视点增量结合信息，并在获得足够信息时主动结束观察；通过结合表示学习与自我奖励强化学习理论，实现非可微模型的端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;AdaptiveNN在17个涵盖9项任务的基准测试上表现优异，包括大规模视觉识别、细粒度辨别、视觉搜索等；在不牺牲准确性的情况下实现高达28倍的推理成本降低，能灵活适应不同任务需求和资源预算无需重新训练，并通过注视模式提供增强的可解释性。&lt;h4&gt;结论&lt;/h4&gt;AdaptiveNN为高效、灵活和可解释的计算机视觉提供了有前途的途径，在许多情况下表现出与人类相似的行为，揭示了其作为研究视觉认知有价值工具的潜力。&lt;h4&gt;翻译&lt;/h4&gt;人类视觉具有高度适应性，通过顺序注视任务相关区域来高效采样复杂环境。相比之下，主流的机器视觉模型被动地一次性处理整个场景，导致资源需求随空间时间输入分辨率和模型规模而过度增加，产生了阻碍未来进展和实际应用的关键限制。在此，我们介绍AdaptiveNN，一个旨在推动视觉模型从'被动'到'主动、自适应'转变的通用框架。AdaptiveNN将视觉感知制定为从粗到细的顺序决策过程，逐步识别和关注与任务相关的区域，跨注视点增量地结合信息，并在获得足够信息时主动结束观察。我们建立了一个将表示学习与自我奖励强化学习相结合的理论，使非可微的AdaptiveNN无需额外的注视位置监督即可进行端到端训练。我们在17个涵盖9项任务的基准测试上评估了AdaptiveNN，包括大规模视觉识别、细粒度辨别、视觉搜索、处理真实驾驶和医疗场景的图像、语言驱动的具身AI以及与人类的并排比较。AdaptiveNN在不牺牲准确性的情况下实现了高达28倍的推理成本降低，能够灵活适应不同的任务需求和资源预算而无需重新训练，并通过其注视模式提供了增强的可解释性，展示了高效、灵活和可解释计算机视觉的有前途途径。此外，AdaptiveNN在许多情况下表现出与人类相似的行为，揭示了其作为研究视觉认知有价值工具的潜力。代码可在https://github.com/LeapLabTHU/AdaptiveNN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human vision is highly adaptive, efficiently sampling intricate environmentsby sequentially fixating on task-relevant regions. In contrast, prevailingmachine vision models passively process entire scenes at once, resulting inexcessive resource demands scaling with spatial-temporal input resolution andmodel size, yielding critical limitations impeding both future advancements andreal-world application. Here we introduce AdaptiveNN, a general frameworkaiming to drive a paradigm shift from 'passive' to 'active, adaptive' visionmodels. AdaptiveNN formulates visual perception as a coarse-to-fine sequentialdecision-making process, progressively identifying and attending to regionspertinent to the task, incrementally combining information across fixations,and actively concluding observation when sufficient. We establish a theoryintegrating representation learning with self-rewarding reinforcement learning,enabling end-to-end training of the non-differentiable AdaptiveNN withoutadditional supervision on fixation locations. We assess AdaptiveNN on 17benchmarks spanning 9 tasks, including large-scale visual recognition,fine-grained discrimination, visual search, processing images from real drivingand medical scenarios, language-driven embodied AI, and side-by-sidecomparisons with humans. AdaptiveNN achieves up to 28x inference cost reductionwithout sacrificing accuracy, flexibly adapts to varying task demands andresource budgets without retraining, and provides enhanced interpretability viaits fixation patterns, demonstrating a promising avenue toward efficient,flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibitsclosely human-like perceptual behaviors in many cases, revealing its potentialas a valuable tool for investigating visual cognition. Code is available athttps://github.com/LeapLabTHU/AdaptiveNN.</description>
      <author>example@mail.com (Yulin Wang, Yang Yue, Yang Yue, Huanqian Wang, Haojun Jiang, Yizeng Han, Zanlin Ni, Yifan Pu, Minglei Shi, Rui Lu, Qisen Yang, Andrew Zhao, Zhuofan Xia, Shiji Song, Gao Huang)</author>
      <guid isPermaLink="false">2509.15333v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2509.14868v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DPANet是一种新颖的双金字塔注意力网络，通过解耦并建模时间多尺度动态和频谱多分辨率周期性，显著提升了长期时间序列预测性能。&lt;h4&gt;背景&lt;/h4&gt;长期时间序列预测面临建模跨越多时间尺度和频率分辨率的复杂依赖关系的挑战，现有Transformer和MLP方法难以统一结构化地捕捉这些相互交织的特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新架构，能够明确解耦并同时建模时间多尺度动态和频谱多分辨率周期性，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;构建双金字塔注意力网络(DPANet)，包含基于渐进下采样的时间金字塔和基于带通滤波的频率金字塔，通过交叉金字塔融合块实现对应级别间深度交互式信息交换，以从粗到细的层次结构进行融合。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准上的广泛实验表明，DPANet达到最先进性能，显著优于先前模型。&lt;h4&gt;结论&lt;/h4&gt;DPANet通过双金字塔结构和交叉金字塔融合块有效解决了长期时间序列预测中的复杂依赖关系建模问题，为时间序列预测提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;长期时间序列预测(LTSF)因难以建模跨越多个时间尺度和频率分辨率的复杂依赖关系而受到阻碍。现有方法，包括Transformer和基于MLP的模型，往往无法以统一和结构化的方式捕捉这些相互交织的特性。我们提出了双金字塔注意力网络(DPANet)，一种新颖的架构，明确解耦并同时建模时间多尺度动态和频谱多分辨率周期性。DPANet构建两个并行金字塔：基于渐进下采样的时间金字塔和基于带通滤波的频率金字塔。我们模型的核心是交叉金字塔融合块，它通过交叉注意力机制实现对应金字塔级别之间深度、交互式的信息交换。这种融合以从粗到细的层次结构进行，使全局上下文能够指导局部表示学习。在公共基准上的广泛实验表明，DPANet达到最先进性能，显著优于先前模型。代码可在https://github.com/hit636/DPANet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term time series forecasting (LTSF) is hampered by the challenge ofmodeling complex dependencies that span multiple temporal scales and frequencyresolutions. Existing methods, including Transformer and MLP-based models,often struggle to capture these intertwined characteristics in a unified andstructured manner. We propose the Dual Pyramid Attention Network (DPANet), anovel architecture that explicitly decouples and concurrently models temporalmulti-scale dynamics and spectral multi-resolution periodicities. DPANetconstructs two parallel pyramids: a Temporal Pyramid built on progressivedownsampling, and a Frequency Pyramid built on band-pass filtering. The core ofour model is the Cross-Pyramid Fusion Block, which facilitates deep,interactive information exchange between corresponding pyramid levels viacross-attention. This fusion proceeds in a coarse-to-fine hierarchy, enablingglobal context to guide local representation learning. Extensive experiments onpublic benchmarks show that DPANet achieves state-of-the-art performance,significantly outperforming prior models. Code is available athttps://github.com/hit636/DPANet.</description>
      <author>example@mail.com (Qianyang Li, Xingjun Zhang, Shaoxun Wang, Jia Wei)</author>
      <guid isPermaLink="false">2509.14868v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders</title>
      <link>http://arxiv.org/abs/2509.15259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于信息熵的特征选择方法IEFS-GMB，用于提高基于深度学习的脑电图分类性能，在四个公共神经系统疾病数据集上实现了0.64%至6.45%的准确率提升。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的脑电图分类对神经系统疾病的自动检测至关重要，但脑电图信号的低信噪比限制了模型性能，使得特征选择成为优化神经网络编码器学习表示的关键。&lt;h4&gt;目的&lt;/h4&gt;解决现有特征选择方法在脑电图分类中的局限性，包括缺乏专门设计、依赖特定架构、缺乏可解释性以及对数据变化鲁棒性不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出IEFS-GMB方法，构建动态记忆库存储历史梯度，通过信息熵计算特征重要性，并应用基于熵的加权来选择信息丰富的脑电图特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共神经系统疾病数据集上实验表明，使用IEFS-GMB增强的编码器相比基线模型准确率提高了0.64%到6.45%，优于四种竞争性特征选择技术，同时提高了模型的可解释性。&lt;h4&gt;结论&lt;/h4&gt;IEFS-GMB方法在脑电图分类中表现优异，支持在临床环境中的实际应用。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的脑电图分类对于神经系统疾病的自动检测至关重要，可以提高诊断准确性并实现早期干预。然而，脑电图信号的低信噪比限制了模型性能，使得特征选择对于优化神经网络编码器学习到的表示至关重要。现有的特征选择方法很少专门为脑电图诊断设计；许多方法依赖于特定架构且缺乏可解释性，限制了它们的适用性。此外，大多数方法依赖于单次迭代数据，导致对变化的鲁棒性有限。为解决这些问题，我们提出了IEFS-GMB，一种由梯度记忆库引导的基于信息熵的特征选择方法。该方法构建存储历史梯度的动态记忆库，通过信息熵计算特征重要性，并应用基于熵的加权来选择信息丰富的脑电图特征。在四个公共神经系统疾病数据集上的实验表明，使用IEFS-GMB增强的编码器相比基线模型实现了0.64%至6.45%的准确率提升。该方法还优于四种竞争性特征选择技术，并提高了模型的可解释性，支持其在临床环境中的实际应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based EEG classification is crucial for the automated detectionof neurological disorders, improving diagnostic accuracy and enabling earlyintervention. However, the low signal-to-noise ratio of EEG signals limitsmodel performance, making feature selection (FS) vital for optimizingrepresentations learned by neural network encoders. Existing FS methods areseldom designed specifically for EEG diagnosis; many are architecture-dependentand lack interpretability, limiting their applicability. Moreover, most rely onsingle-iteration data, resulting in limited robustness to variability. Toaddress these issues, we propose IEFS-GMB, an Information Entropy-based FeatureSelection method guided by a Gradient Memory Bank. This approach constructs adynamic memory bank storing historical gradients, computes feature importancevia information entropy, and applies entropy-based weighting to selectinformative EEG features. Experiments on four public neurological diseasedatasets show that encoders enhanced with IEFS-GMB achieve accuracyimprovements of 0.64% to 6.45% over baseline models. The method alsooutperforms four competing FS techniques and improves model interpretability,supporting its practical use in clinical settings.</description>
      <author>example@mail.com (Liang Zhang, Hanyang Dong, Jia-Hong Gao, Yi Sun, Kuntao Xiao, Wanli Yang, Zhao Lv, Shurong Sheng)</author>
      <guid isPermaLink="false">2509.15259v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays</title>
      <link>http://arxiv.org/abs/2509.15234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 2 figures, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于大型语言模型的放射学报告表示学习方法，通过领域适配的LLM编码器和双塔框架，解决了临床报告异质性和噪声问题，提高了模型在跨数据集泛化和临床对齐方面的表现。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言预训练在图像-文本对齐方面取得了进展，但在放射学领域，由于临床报告的异质性（包括缩写、仅印象笔记和风格变异性）而进展受限。与通用领域不同，简单地扩展到大量嘈杂报告的数据集可能导致模型学习停滞甚至退化。&lt;h4&gt;目的&lt;/h4&gt;研究大型语言模型(LLM)编码器是否能提供稳健的临床表示，这些表示能跨不同风格转移并更好地指导图像-文本对齐。&lt;h4&gt;方法&lt;/h4&gt;引入了LLM2VEC4CXR（针对胸部X光报告进行领域适配的LLM编码器）和LLM2CLIP4CXR（将此编码器与视觉骨干网络耦合的双塔框架）。在来自公共和私人来源的160万份胸部X光研究上训练模型，这些报告具有异质性和噪声。&lt;h4&gt;主要发现&lt;/h4&gt;LLM2VEC4CXR在临床文本理解方面优于基于BERT的基线，能够处理缩写和风格变化，并在报告级别指标上实现了强大的临床对齐。LLM2CLIP4CXR利用这些嵌入提高了检索准确度和临床导向评分，比之前的医疗CLIP变体具有更强的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;稳健性（而非仅是规模）是有效多模态学习的关键。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言预训练已推进图像-文本对齐，但在放射学领域的进展仍受临床报告异质性的限制，包括缩写、仅印象笔记和风格变异性。与通用领域设置不同（更多数据通常带来更好性能），简单地扩展到大量嘈杂报告的数据集可能导致模型学习停滞甚至退化。我们探究大型语言模型(LLM)编码器是否能提供稳健的临床表示，这些表示能跨不同风格转移并更好地指导图像-文本对齐。我们介绍了LLM2VEC4CXR，一个针对胸部X光报告进行领域适配的LLM编码器，以及LLM2CLIP4CXR，一个将此编码器与视觉骨干网络耦合的双塔框架。LLM2VEC4CXR在临床文本理解方面优于基于BERT的基线，能够处理缩写和风格变化，并在报告级别指标上实现了强大的临床对齐。LLM2CLIP4CXR利用这些嵌入提高了检索准确度和临床导向评分，比之前的医疗CLIP变体具有更强的跨数据集泛化能力。在来自公共和私人来源的160万份具有异质性和噪声的胸部X光研究上训练后，我们的模型证明了稳健性（而非仅是规模）是有效多模态学习的关键。我们发布这些模型以支持医学图像-文本表示学习的进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language pretraining has advanced image-text alignment, yet progressin radiology remains constrained by the heterogeneity of clinical reports,including abbreviations, impression-only notes, and stylistic variability.Unlike general-domain settings where more data often leads to betterperformance, naively scaling to large collections of noisy reports can plateauor even degrade model learning. We ask whether large language model (LLM)encoders can provide robust clinical representations that transfer acrossdiverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR,a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, adual-tower framework that couples this encoder with a vision backbone.LLM2VEC4CXR improves clinical text understanding over BERT-based baselines,handles abbreviations and style variation, and achieves strong clinicalalignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings toboost retrieval accuracy and clinically oriented scores, with strongercross-dataset generalization than prior medical CLIP variants. Trained on 1.6MCXR studies from public and private sources with heterogeneous and noisyreports, our models demonstrate that robustness -- not scale alone -- is thekey to effective multimodal learning. We release models to support furtherresearch in medical image-text representation learning.</description>
      <author>example@mail.com (Hanbin Ko, Gihun Cho, Inhyeok Baek, Donguk Kim, Joonbeom Koo, Changi Kim, Dongheon Lee, Chang Min Park)</author>
      <guid isPermaLink="false">2509.15234v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Compose by Focus: Scene Graph-based Atomic Skills</title>
      <link>http://arxiv.org/abs/2509.16053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于场景图的技能学习框架，结合图神经网络和基于扩散的模仿学习，并与视觉语言模型任务规划器结合，显著提高了机器人在长时程任务中的组合泛化能力和稳健性。&lt;h4&gt;背景&lt;/h4&gt;通用机器人的关键挑战是实现组合泛化能力，即将原子技能组合解决复杂长时程任务。先前研究主要关注规划器合成，但单个技能在场景组合引起的分布转移下执行效果不佳。&lt;h4&gt;目的&lt;/h4&gt;解决视觉运动策略在场景组合引起的分布转移下失效的问题，提高单个技能的稳健执行能力，增强机器人在长时程任务中的组合泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入基于场景图的表示，专注于任务相关对象和关系；开发结合图神经网络和基于扩散的模仿学习的技能学习框架；将'聚焦的'场景图技能与视觉语言模型任务规划器相结合。&lt;h4&gt;主要发现&lt;/h4&gt;在仿真和真实世界操作任务中，所提方法比现有最先进基线方法取得显著更高的成功率，有效提升了长时程任务中的稳健性和组合泛化能力。&lt;h4&gt;结论&lt;/h4&gt;基于场景图的表示和技能学习框架能有效解决视觉运动策略在分布转移下的失效问题，显著提高机器人在复杂长时程任务中的执行能力和泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;通用机器人的一个关键要求是组合泛化能力——将原子技能组合起来解决复杂、长时程任务的能力。虽然先前的工作主要集中在合成规划器来排序预学习的技能，但单个技能本身的稳健执行仍然具有挑战性，因为视觉运动策略在场景组合引起的分布转移下往往会失败。为了解决这个问题，我们引入了一种基于场景图的表示，专注于任务相关的对象和关系，从而降低对无关变化的敏感性。基于这一思想，我们开发了一个基于场景图的技能学习框架，将图神经网络与基于扩散的模仿学习相结合，并将'聚焦的'场景图技能与基于视觉语言模型(VLM)的任务规划器相结合。在仿真和真实世界操作任务中的实验表明，成功率比最先进的基线方法显著提高，突显了在长时程任务中改进的稳健性和组合泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人如何有效组合基本技能来完成复杂长时程任务的问题。这个问题很重要，因为现实世界中的机器人任务通常需要将大任务分解成多个小任务执行，而现有方法在复杂场景中组合技能时表现不佳，容易受到环境变化的影响。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为要让技能能有效组合，每个技能必须是'专注的'，只关注与当前任务相关的对象和关系，忽略无关干扰。基于这一想法，他们设计了基于场景图的表示方法。该方法借鉴了视觉基础模型（如Grounded-SAM）进行对象分割，视觉语言模型（如ChatGPT）进行关系推理，图神经网络处理图结构，以及扩散模型进行动作学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用场景图表示视觉数据，只包含任务相关的对象和关系，使模型能'专注'于重要元素。流程包括：1)构建场景图（分割对象、提取点云、推断关系）；2)训练多技能策略（使用图神经网络处理场景图，结合扩散模型学习动作）；3)测试时技能组合（使用视觉语言模型规划任务，动态构建子场景图并执行相应技能）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用场景图作为视觉输入表示；2)设计'专注'的技能学习方法；3)结合图神经网络和扩散模型；4)与视觉语言模型集成实现端到端流程。相比之前工作，不同之处在于：不使用原始图像而用结构化场景图；不仅关注高层规划还关注底层技能构建；明确编码对象间关系而非仅处理对象特征；将场景图直接作为低层策略输入而非仅用于高层推理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于场景图的表示方法，使机器人能够'专注'于任务相关元素，从而实现更鲁棒和可组合的技能学习，显著提高了复杂长时程任务的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A key requirement for generalist robots is compositional generalization - theability to combine atomic skills to solve complex, long-horizon tasks. Whileprior work has primarily focused on synthesizing a planner that sequencespre-learned skills, robust execution of the individual skills themselvesremains challenging, as visuomotor policies often fail under distributionshifts induced by scene composition. To address this, we introduce a scenegraph-based representation that focuses on task-relevant objects and relations,thereby mitigating sensitivity to irrelevant variation. Building on this idea,we develop a scene-graph skill learning framework that integrates graph neuralnetworks with diffusion-based imitation learning, and further combine "focused"scene-graph skills with a vision-language model (VLM) based task planner.Experiments in both simulation and real-world manipulation tasks demonstratesubstantially higher success rates than state-of-the-art baselines,highlighting improved robustness and compositional generalization inlong-horizon tasks.</description>
      <author>example@mail.com (Han Qi, Changhe Chen, Heng Yang)</author>
      <guid isPermaLink="false">2509.16053v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>LC-SLab -- An Object-based Deep Learning Framework for Large-scale Land Cover Classification from Satellite Imagery and Sparse In-situ Labels</title>
      <link>http://arxiv.org/abs/2509.15868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LC-SLab框架，这是首个用于在稀疏监督下系统探索基于对象深度学习方法进行大规模土地分类的深度学习框架。该框架能够生成更连贯的土地覆盖地图，同时保持或提高准确性。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习生成的大规模土地覆盖地图在地球科学应用中起关键作用。来自原则性土地调查的开放实地数据集为手动标注提供了可扩展替代方案，但其稀疏空间覆盖导致现有方法产生碎片化和噪声预测。基于对象的分类方法为语义连贯的图像区域分配标签，但在基于深度学习的土地覆盖映射中尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决稀疏实地数据集导致的土地覆盖预测碎片化问题，探索基于对象的深度学习方法在中等分辨率图像和稀疏监督背景下的应用，并开发系统性框架。&lt;h4&gt;方法&lt;/h4&gt;提出LC-SLab框架，支持两种聚合方式：1)输入级聚合通过图神经网络实现；2)输出级聚合通过后处理语义分割模型结果实现。同时融入大型预训练网络特征以改善小数据集性能。在年度Sentinel-2合成图像和稀疏LUCAS标签上评估框架，关注准确性与碎片化权衡及数据集大小敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;基于对象的方法可匹配或超越逐像素模型准确性，同时产生更连贯地图；输入级聚合在小数据集上更鲁棒；输出级聚合在更多数据上表现最佳；几种LC-SLab配置优于现有土地覆盖产品。&lt;h4&gt;结论&lt;/h4&gt;LC-SLab框架成功将基于对象的深度学习方法应用于大规模土地分类，特别是在稀疏监督条件下，为地球科学应用提供了生成连贯土地覆盖地图的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;使用深度生成的大规模土地覆盖地图在广泛的地球科学应用中起着关键作用。来自原则性土地调查的开放实地数据集为手动标注提供了可扩展的替代方案，用于训练此类模型。然而，它们稀疏的空间覆盖通常导致在使用现有基于深度学习的土地覆盖映射方法时产生碎片化和噪声预测。解决这一问题的一个有前景的方向是基于对象的分类，它为语义连贯的图像区域而非单个像素分配标签，从而强制设定最小制图单元。尽管有这种潜力，但基于对象的方法在基于深度学习的土地覆盖映射管道中仍未得到充分探索，特别是在中等分辨率图像和稀疏监督的背景下。为了解决这一差距，我们提出了LC-SLab，这是首个用于在稀疏监督下系统探索基于对象的深度学习方法进行大规模土地分类的深度学习框架。LC-SLab支持通过图神经网络进行输入级聚合，以及通过后处理已建立的语义分割模型的结果进行输出级聚合。此外，我们还融入了大型预训练网络的特征，以改善小数据集上的性能。我们在年度Sentinel-2合成图像和稀疏LUCAS标签上评估了该框架，重点关注准确性与碎片化之间的权衡以及对数据集大小的敏感性。我们的结果表明，基于对象的方法可以匹配或超越逐像素模型的准确性，同时产生更连贯的地图。输入级聚合在较小数据集上更鲁棒，而输出级聚合在更多数据上表现最佳。几种LC-SLab配置也优于现有的土地覆盖产品，突显了该框架的实际效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale land cover maps generated using deep learning play a criticalrole across a wide range of Earth science applications. Open in-situ datasetsfrom principled land cover surveys offer a scalable alternative to manualannotation for training such models. However, their sparse spatial coverageoften leads to fragmented and noisy predictions when used with existing deeplearning-based land cover mapping approaches. A promising direction to addressthis issue is object-based classification, which assigns labels to semanticallycoherent image regions rather than individual pixels, thereby imposing aminimum mapping unit. Despite this potential, object-based methods remainunderexplored in deep learning-based land cover mapping pipelines, especiallyin the context of medium-resolution imagery and sparse supervision. To addressthis gap, we propose LC-SLab, the first deep learning framework forsystematically exploring object-based deep learning methods for large-scaleland cover classification under sparse supervision. LC-SLab supports bothinput-level aggregation via graph neural networks, and output-level aggregationby postprocessing results from established semantic segmentation models.Additionally, we incorporate features from a large pre-trained network toimprove performance on small datasets. We evaluate the framework on annualSentinel-2 composites with sparse LUCAS labels, focusing on the tradeoffbetween accuracy and fragmentation, as well as sensitivity to dataset size. Ourresults show that object-based methods can match or exceed the accuracy ofcommon pixel-wise models while producing substantially more coherent maps.Input-level aggregation proves more robust on smaller datasets, whereasoutput-level aggregation performs best with more data. Several configurationsof LC-SLab also outperform existing land cover products, highlighting theframework's practical utility.</description>
      <author>example@mail.com (Johannes Leonhardt, Juergen Gall, Ribana Roscher)</author>
      <guid isPermaLink="false">2509.15868v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network</title>
      <link>http://arxiv.org/abs/2509.15857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025 (spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EvoBrain的新型癫痫检测模型，通过整合双流Mamba架构和拉普拉斯位置编码增强的GCN，解决了动态图神经网络在脑电图数据中捕捉癫痫状态动态特性的两个主要挑战。&lt;h4&gt;背景&lt;/h4&gt;动态图神经网络整合脑电图数据中的时间和空间特征，在癫痫检测方面有巨大潜力，但完全捕捉代表癫痫和非癫痫状态的脑动态仍面临两个挑战：现有方法多基于静态图，无法反映癫痫过程中大脑连接的动态特性；联合建模时间信号和图结构及其交互作用的尝试仍处于初级阶段。&lt;h4&gt;目的&lt;/h4&gt;解决现有动态GNN方法的两个基本挑战：一是无法反映癫痫进展过程中大脑连接的动态演化，二是联合建模时间信号和图结构及其交互作用的能力不足导致的性能不一致问题。&lt;h4&gt;方法&lt;/h4&gt;提出EvoBrain模型，整合双流Mamba架构和拉普拉斯位置编码增强的GCN，遵循神经学见解，并引入显式动态图结构使节点和边随时间演化。基于对两个问题的理论分析，证明了显式动态建模和时间-图动态GNN方法的有效性和必要性。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析证明显式动态建模和时间-图动态GNN方法比其他方法具有表达优势；实验表明EvoBrain相比动态GNN基线显著提高了性能，AUROC提高23%，F1分数提高30%。&lt;h4&gt;结论&lt;/h4&gt;EvoBrain模型通过显式动态建模和时间-图动态GNN方法有效解决了癫痫检测中的关键挑战，在早期癫痫预测任务上表现出色，为癫痫检测提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;动态图神经网络整合了脑电图数据中的时间和空间特征，在自动化癫痫检测方面显示出巨大潜力。然而，完全捕捉代表脑状态（如癫痫和非癫痫）所需的基本动态仍然是一项困难的任务，并面临两个基本挑战。首先，大多数现有的动态图神经网络方法建立在时间固定的静态图上，无法反映癫痫进展过程中大脑连接的动态特性。其次，目前联合建模时间信号和图结构及其交互作用的尝试仍处于初级阶段，通常导致性能不一致。为解决这些挑战，我们首次对这两个问题进行了理论分析，证明了显式动态建模和时间-图动态图神经网络方法的有效性和必要性。基于这些见解，我们提出了EvoBrain，一种整合双流Mamba架构和拉普拉斯位置编码增强的GCN的新型癫痫检测模型，遵循神经学见解。此外，EvoBrain集成了显式动态图结构，允许节点和边随时间演化。我们的贡献包括：(a) 证明显式动态建模和时间-图动态图神经网络方法比其他方法具有表达优势的理论分析，(b) 相比动态图神经网络基线显著提高AUROC 23%和F1分数30%的新型高效模型，以及(c) 在具有挑战性的早期癫痫预测任务上对我们方法的广泛评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic GNNs, which integrate temporal and spatial features inElectroencephalography (EEG) data, have shown great potential in automatingseizure detection. However, fully capturing the underlying dynamics necessaryto represent brain states, such as seizure and non-seizure, remains anon-trivial task and presents two fundamental challenges. First, most existingdynamic GNN methods are built on temporally fixed static graphs, which fail toreflect the evolving nature of brain connectivity during seizure progression.Second, current efforts to jointly model temporal signals and graph structuresand, more importantly, their interactions remain nascent, often resulting ininconsistent performance. To address these challenges, we present the firsttheoretical analysis of these two problems, demonstrating the effectiveness andnecessity of explicit dynamic modeling and time-then-graph dynamic GNN method.Building on these insights, we propose EvoBrain, a novel seizure detectionmodel that integrates a two-stream Mamba architecture with a GCN enhanced byLaplacian Positional Encoding, following neurological insights. Moreover,EvoBrain incorporates explicitly dynamic graph structures, allowing both nodesand edges to evolve over time. Our contributions include (a) a theoreticalanalysis proving the expressivity advantage of explicit dynamic modeling andtime-then-graph over other approaches, (b) a novel and efficient model thatsignificantly improves AUROC by 23% and F1 score by 30%, compared with thedynamic GNN baseline, and (c) broad evaluations of our method on thechallenging early seizure prediction tasks.</description>
      <author>example@mail.com (Rikuto Kotoge, Zheng Chen, Tasuku Kimura, Yasuko Matsubara, Takufumi Yanagisawa, Haruhiko Kishima, Yasushi Sakurai)</author>
      <guid isPermaLink="false">2509.15857v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors</title>
      <link>http://arxiv.org/abs/2509.15827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 17 figures, submitted to IEEE Transactions on Sustainable  Energy&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SolarCrossFormer是一种新型深度学习模型，用于结合卫星图像和地面气象站网络时间序列数据进行日辐照度预测，提供15分钟分辨率和24小时预测范围，归一化平均绝对误差为6.1%，结果与商业数值天气预报服务相当。&lt;h4&gt;背景&lt;/h4&gt;大规模太阳能光伏系统并网需要准确的日辐照度预测，但当前预测解决方案缺乏系统运营商所需的时间和空间分辨率。&lt;h4&gt;目的&lt;/h4&gt;开发SolarCrossFormer模型，结合卫星图像和地面气象站数据，提高日辐照度预测的准确性和分辨率。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络利用输入数据的模内和模间相关性，生成瑞士任何位置的预测性预报，分辨率为15分钟，预测范围可达未来24小时；无需重新训练即可整合新数据，仅使用坐标即可为无数据位置生成预测。&lt;h4&gt;主要发现&lt;/h4&gt;在瑞士一年期、127个位置的数据集上测试，SolarCrossFormer在整个预测范围内的归一化平均绝对误差为6.1%，结果与商业数值天气预报服务具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;SolarCrossFormer是一种有效的日辐照度预测模型，结合多源数据提供高分辨率、高准确性的预测，在实际应用中具有稳健性。&lt;h4&gt;翻译&lt;/h4&gt;太阳能光伏系统的大规模并网需要准确的日辐照度提前预测。然而，当前的预测解决方案缺乏系统运营商所需的时间和空间分辨率。在本文中，我们引入了SolarCrossFormer，这是一种用于日辐照度预测的新型深度学习模型，它结合了卫星图像和地面气象站网络的时间序列数据。SolarCrossFormer使用新颖的图神经网络来利用输入数据的模内和模间相关性，提高预测的准确性和分辨率。它以15分钟的时间分辨率生成瑞士任何位置的预测性预报，预测范围可达未来24小时。SolarCrossFormer的主要优势之一是在实际操作中的稳健性。它可以整合新的时间序列数据而无需重新训练模型，此外，它可以通过仅使用坐标来为没有输入数据的位置生成预测。在瑞士一年期、127个位置的数据集上的实验结果表明，SolarCrossFormer在整个预测范围内的归一化平均绝对误差为6.1%。结果与商业数值天气预报服务所取得的结果具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate day-ahead forecasts of solar irradiance are required for thelarge-scale integration of solar photovoltaic (PV) systems into the power grid.However, current forecasting solutions lack the temporal and spatial resolutionrequired by system operators. In this paper, we introduce SolarCrossFormer, anovel deep learning model for day-ahead irradiance forecasting, that combinessatellite images and time series from a ground-based network of meteorologicalstations. SolarCrossFormer uses novel graph neural networks to exploit theinter- and intra-modal correlations of the input data and improve the accuracyand resolution of the forecasts. It generates probabilistic forecasts for anylocation in Switzerland with a 15-minute resolution for horizons up to 24 hoursahead. One of the key advantages of SolarCrossFormer its robustness in reallife operations. It can incorporate new time-series data without retraining themodel and, additionally, it can produce forecasts for locations without inputdata by using only their coordinates. Experimental results over a dataset ofone year and 127 locations across Switzerland show that SolarCrossFormer yielda normalized mean absolute error of 6.1 % over the forecasting horizon. Theresults are competitive with those achieved by a commercial numerical weatherprediction service.</description>
      <author>example@mail.com (Baptiste Schubnel, Jelena Simeunović, Corentin Tissier, Pierre-Jean Alet, Rafael E. Carrillo)</author>
      <guid isPermaLink="false">2509.15827v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Optimize Capacity Planning in Semiconductor Manufacturing</title>
      <link>http://arxiv.org/abs/2509.15767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于神经网络的半导体制造产能规划模型，通过深度强化学习训练，能够捕捉机器和处理步骤间的复杂关系，实现主动决策，提高了生产效率。&lt;h4&gt;背景&lt;/h4&gt;在制造业中，产能规划是根据可变需求分配生产资源的过程。当前半导体制造业使用启发式规则优先处理操作，但这些规则难以考虑流程中复杂的相互作用，可能导致瓶颈形成。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉制造系统中复杂相互关系的产能规划模型，克服传统启发式方法的局限性，实现更高效的产能规划。&lt;h4&gt;方法&lt;/h4&gt;提出基于神经网络的机器级产能规划模型，使用深度强化学习训练，采用异构图神经网络表示策略，直接捕捉机器和处理步骤间的多样化关系，并采取措施实现足够的可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;在英特尔的小型Minifab模型和SMT2020测试台上的实验表明，训练后的策略在最大测试场景中使吞吐量提高约1.8%，同时减少约1.8%的周期时间。&lt;h4&gt;结论&lt;/h4&gt;基于深度强化学习和异构图神经网络的产能规划模型能有效捕捉制造系统中复杂的相互关系，为半导体制造业提供比传统启发式方法更有效的产能规划解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在制造业中，产能规划是根据可变需求分配生产资源的过程。当前半导体制造业的典型做法是使用启发式规则来优先处理操作，如考虑未来机器和配方分配的变更列表。然而，虽然启发式规则提供了可解释性，但它们难以考虑流程中复杂的相互作用，而这些相互作用可能导致瓶颈的形成。在这里，我们提出了一个基于神经网络的机器级产能规划模型，使用深度强化学习进行训练。通过使用异构图神经网络表示策略，该模型直接捕捉机器和处理步骤之间的多样化关系，实现主动决策。我们描述了几项为实现足够的可扩展性以处理庞大的机器级操作可能空间而采取的措施。我们的评估结果涵盖了英特尔的小型Minifab模型和使用流行的SMT2020测试台进行的初步实验。在最大测试场景中，我们训练的策略使吞吐量提高了约1.8%，同时减少了约1.8%的周期时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In manufacturing, capacity planning is the process of allocating productionresources in accordance with variable demand. The current industry practice insemiconductor manufacturing typically applies heuristic rules to prioritizeactions, such as future change lists that account for incoming machine andrecipe dedications. However, while offering interpretability, heuristics cannoteasily account for the complex interactions along the process flow that cangradually lead to the formation of bottlenecks. Here, we present a neuralnetwork-based model for capacity planning on the level of individual machines,trained using deep reinforcement learning. By representing the policy using aheterogeneous graph neural network, the model directly captures the diverserelationships among machines and processing steps, allowing for proactivedecision-making. We describe several measures taken to achieve sufficientscalability to tackle the vast space of possible machine-level actions.  Our evaluation results cover Intel's small-scale Minifab model andpreliminary experiments using the popular SMT2020 testbed. In the largesttested scenario, our trained policy increases throughput and decreases cycletime by about 1.8% each.</description>
      <author>example@mail.com (Philipp Andelfinger, Jieyi Bi, Qiuyu Zhu, Jianan Zhou, Bo Zhang, Fei Fei Zhang, Chew Wye Chan, Boon Ping Gan, Wentong Cai, Jie Zhang)</author>
      <guid isPermaLink="false">2509.15767v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Network-assisted Random Forest+</title>
      <link>http://arxiv.org/abs/2509.15611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于随机森林推广的灵活网络辅助模型家族，该模型具有高度竞争性的预测准确性，同时保持可解释性，通过特征重要性措施和专门的解释工具，使从业者能够识别重要特征并量化网络对预测的贡献。&lt;h4&gt;背景&lt;/h4&gt;机器学习算法通常假设训练样本是独立的，但当数据点通过网络连接时，样本间的依赖性既是挑战(减少有效样本量)也是机会(利用网络邻居信息改善预测)。&lt;h4&gt;目的&lt;/h4&gt;解决现有网络辅助方法中可解释性与预测性能之间的差距，开发一种既具有高度竞争性预测准确性又可解释的网络辅助模型。&lt;h4&gt;方法&lt;/h4&gt;提出基于随机森林推广的灵活网络辅助模型家族，开发全局和局部重要性指标以及样本影响指标，使从业者能够识别重要特征并量化网络贡献的重要性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型具有高度竞争性的预测准确性，可以通过特征重要性措施进行解释，并且能够量化网络对预测的贡献。&lt;h4&gt;结论&lt;/h4&gt;这套解释工具扩展了网络辅助机器学习的范围和适用性，特别适用于可解释性和透明性至关重要的高影响力问题。&lt;h4&gt;翻译&lt;/h4&gt;机器学习算法通常假设训练样本是独立的。当数据点通过网络连接时，样本间的依赖性既是挑战，减少了有效样本量，也是利用网络邻居信息改善预测的机会。目前有多种方法利用这一机会，但许多方法(如图神经网络)不易解释，限制了它们对理解模型如何做出预测的用处。其他方法(如网络辅助线性回归)虽然可解释，但通常预测性能明显较差。我们通过提出一种基于随机森林推广的灵活网络辅助模型家族来弥合这一差距，该模型实现了高度竞争性的预测准确性，并通过特征重要性措施可被解释。特别是，我们开发了一套解释工具，使从业者不仅能够识别驱动模型预测的重要特征，还能量化网络对预测的重要性。重要的是，我们提供了全局和局部重要性指标以及样本影响指标，以评估特定观察的影响。这套工具扩展了网络辅助机器学习在高影响力问题上的范围和适用性，其中可解释性和透明性至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning algorithms often assume that training samples areindependent. When data points are connected by a network, the induceddependency between samples is both a challenge, reducing effective sample size,and an opportunity to improve prediction by leveraging information from networkneighbors. Multiple methods taking advantage of this opportunity are nowavailable, but many, including graph neural networks, are not easilyinterpretable, limiting their usefulness for understanding how a model makesits predictions. Others, such as network-assisted linear regression, areinterpretable but often yield substantially worse prediction performance. Webridge this gap by proposing a family of flexible network-assisted models builtupon a generalization of random forests (RF+), which achieveshighly-competitive prediction accuracy and can be interpreted through featureimportance measures. In particular, we develop a suite of interpretation toolsthat enable practitioners to not only identify important features that drivemodel predictions, but also quantify the importance of the network contributionto prediction. Importantly, we provide both global and local importancemeasures as well as sample influence measures to assess the impact of a givenobservation. This suite of tools broadens the scope and applicability ofnetwork-assisted machine learning for high-impact problems whereinterpretability and transparency are essential.</description>
      <author>example@mail.com (Tiffany M. Tang, Elizaveta Levina, Ji Zhu)</author>
      <guid isPermaLink="false">2509.15611v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies</title>
      <link>http://arxiv.org/abs/2509.15481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SolarCAST是一个基于因果信息模型的太阳辐照度预测系统，仅使用历史传感器数据而不需要特殊硬件或复杂预处理，能够有效预测目标地点的全局水平辐照度。&lt;h4&gt;背景&lt;/h4&gt;准确的太阳预测是有效管理可再生能源的基础。以往的研究依赖于需要专门硬件和大量预处理的摄像机或卫星图像。&lt;h4&gt;目的&lt;/h4&gt;开发一个仅使用公共传感器数据就能提供高精度预测的太阳辐照度预测模型。&lt;h4&gt;方法&lt;/h4&gt;SolarCAST使用可扩展的神经组件建模三类混杂因素：1)可观测同步变量（如一天中的时间、站点身份），通过嵌入模块处理；2)潜在同步因素（如区域天气模式），通过时空图神经网络捕获；3)时间滞后影响（如云层在站点间的移动），使用门控变压器建模时间变化。&lt;h4&gt;主要发现&lt;/h4&gt;SolarCAST在各种地理条件下都优于领先的时间序列和多模态基线模型，比顶级商业预测器Solcast实现了25.9%的误差减少。&lt;h4&gt;结论&lt;/h4&gt;SolarCAST为局部太阳预测提供了一个轻量级、实用且可推广的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的太阳预测是有效管理可再生能源的基础。我们提出了SolarCAST，这是一个因果信息模型，仅使用站点X和附近站点S的历史GHI数据来预测目标地点的未来全局水平辐照度(GHI)——与之前依赖需要专门硬件和大量预处理的摄像机或卫星图像的研究不同。为了仅使用公共传感器数据提供高精度，SolarCAST使用可扩展的神经组件建模X-S相关性背后的三类混杂因素：(i)可观测同步变量（如一天中的时间、站点身份），通过嵌入模块处理；(ii)潜在同步因素（如区域天气模式），由时空图神经网络捕获；(iii)时间滞后影响（如云层在站点间的移动），使用学习时间变化的门控变压器建模。它在各种地理条件下都优于领先的时间序列和多模态基线模型，并比顶级商业预测器Solcast实现了25.9%的误差减少。SolarCAST为局部太阳预测提供了一个轻量级、实用且可推广的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760905&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate solar forecasting underpins effective renewable energy management.We present SolarCAST, a causally informed model predicting future globalhorizontal irradiance (GHI) at a target site using only historical GHI fromsite X and nearby stations S - unlike prior work that relies on sky-camera orsatellite imagery requiring specialized hardware and heavy preprocessing. Todeliver high accuracy with only public sensor data, SolarCAST models threeclasses of confounding factors behind X-S correlations using scalable neuralcomponents: (i) observable synchronous variables (e.g., time of day, stationidentity), handled via an embedding module; (ii) latent synchronous factors(e.g., regional weather patterns), captured by a spatio-temporal graph neuralnetwork; and (iii) time-lagged influences (e.g., cloud movement acrossstations), modeled with a gated transformer that learns temporal shifts. Itoutperforms leading time-series and multimodal baselines across diversegeographical conditions, and achieves a 25.9% error reduction over the topcommercial forecaster, Solcast. SolarCAST offers a lightweight, practical, andgeneralizable solution for localized solar forecasting.</description>
      <author>example@mail.com (Yanan Niu, Demetri Psaltis, Christophe Moser, Luisa Lambertini)</author>
      <guid isPermaLink="false">2509.15481v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Partial Column Generation with Graph Neural Networks for Team Formation and Routing</title>
      <link>http://arxiv.org/abs/2509.15275v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对团队组建和路由问题的新型部分列生成策略，通过机器学习和图神经网络预测可能产生负约简成本的列，显著提高了解决方案效率。&lt;h4&gt;背景&lt;/h4&gt;团队组建和路由问题是一个具有挑战性的优化问题，在机场、医疗保健和维护操作等多个现实领域有广泛应用。文献中已提出基于列生成的精确解法来解决此问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对具有多个定价问题设置的部分列生成新策略，通过预测哪些定价问题可能产生有价值的列来优化求解过程。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于机器学习的部分列生成策略，专门为团队组建和路由问题设计的机器学习模型利用图神经网络来预测哪些定价问题可能产生具有负约简成本的列。&lt;h4&gt;主要发现&lt;/h4&gt;计算实验表明，应用所提出的策略能够增强解决方法，并优于文献中的传统部分列生成方法，特别是在严格时间限制下解决困难实例时表现更佳。&lt;h4&gt;结论&lt;/h4&gt;基于机器学习的部分列生成策略在团队组建和路由问题上表现出色，特别是在处理困难实例和时间受限场景时，为解决实际优化问题提供了有效途径。&lt;h4&gt;翻译&lt;/h4&gt;团队组建和路由问题是一个具有挑战性的优化问题，在机场、医疗保健和维护操作等领域有几种现实世界的应用。为解决此问题，文献中已提出基于列生成的精确解法。在本文中，我们提出了一种针对具有多个定价问题设置的新型部分列生成策略，基于预测哪些定价问题可能产生具有负约简成本的列。我们开发了一个专门针对团队组建和路由问题的机器学习模型，利用图神经网络进行这些预测。计算实验表明，应用我们的策略能够增强解决方法，并优于文献中的传统部分列生成方法，特别是在严格时间限制下解决的困难实例上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The team formation and routing problem is a challenging optimization problemwith several real-world applications in fields such as airport, healthcare, andmaintenance operations. To solve this problem, exact solution methods based oncolumn generation have been proposed in the literature. In this paper, wepropose a novel partial column generation strategy for settings with multiplepricing problems, based on predicting which ones are likely to yield columnswith a negative reduced cost. We develop a machine learning model tailored tothe team formation and routing problem that leverages graph neural networks forthese predictions. Computational experiments demonstrate that applying ourstrategy enhances the solution method and outperforms traditional partialcolumn generation approaches from the literature, particularly on hardinstances solved under a tight time limit.</description>
      <author>example@mail.com (Giacomo Dall'Olio, Rainer Kolisch, Yaoxin Wu)</author>
      <guid isPermaLink="false">2509.15275v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features</title>
      <link>http://arxiv.org/abs/2509.16098v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SegDINO3D，一种用于3D实例分割的新型Transformer编码器-解码器框架。&lt;h4&gt;背景&lt;/h4&gt;3D训练数据通常不如2D训练图像充足，因此需要充分利用预训练的2D检测模型中的2D表示来改善3D表示。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够充分利用2D表示（包括图像级和对象级特征）来提高3D表示性能的框架。&lt;h4&gt;方法&lt;/h4&gt;SegDINO3D同时接收点云及其相关的2D图像作为输入；在编码器阶段，通过检索2D图像特征来丰富每个3D点，然后进行3D上下文融合；在解码器阶段，将3D对象查询公式化为3D锚框，并执行从3D查询到2D对象查询的交叉注意力；2D对象查询作为2D图像的紧凑表示避免了内存挑战；引入3D框查询使模型能够使用预测框进行更精确的查询。&lt;h4&gt;主要发现&lt;/h4&gt;SegDINO3D在ScanNetV2和ScanNet200 3D实例分割基准测试上达到了最先进的性能；在具有挑战性的ScanNet200数据集上，SegDINO3D在验证集和隐藏测试集上分别比之前的方法高出+8.7和+6.8 mAP。&lt;h4&gt;结论&lt;/h4&gt;SegDINO3D通过有效利用2D表示来增强3D表示，显著提高了3D实例分割的性能，证明了其优越性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了SegDINO3D，一种用于3D实例分割的新型Transformer编码器-解码器框架。由于3D训练数据通常不如2D训练图像充足，SegDINO3D被设计为充分利用预训练2D检测模型中的2D表示，包括图像级和对象级特征，以提高3D表示。SegDINO3D同时接收点云及其相关的2D图像作为输入。在编码器阶段，它首先通过从相应的图像视图中检索2D图像特征来丰富每个3D点，然后利用3D编码器进行3D上下文融合。在解码器阶段，它将3D对象查询公式化为3D锚框，并执行从3D查询到从2D图像使用2D检测模型获得的2D对象查询的交叉注意力。这些2D对象查询作为2D图像的紧凑对象级表示，有效地避免了在内存中保留数千个图像特征图的挑战，同时忠实地保留了预训练2D模型的知识。引入3D框查询也使模型能够使用预测的框来调制交叉注意力，从而进行更精确的查询。SegDINO3D在ScanNetV2和ScanNet200 3D实例分割基准测试上达到了最先进的性能。值得注意的是，在具有挑战性的ScanNet200数据集上，SegDINO3D在验证集和隐藏测试集上分别比之前的方法高出+8.7和+6.8 mAP，证明了其优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D实例分割任务中因3D训练数据不足导致的性能受限问题。这个问题在现实中非常重要，因为3D环境理解是AI系统与物理世界交互的关键能力，而3D语义理解（如物体实例检测和分割）的不足严重制约了机器人操作、自主导航等下游应用的发展。同时，2D图像数据远比3D数据丰富，如何有效利用2D图像的丰富语义信息来增强3D感知能力是一个重要研究方向。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考：大多数3D分割方法只使用点云而忽略伴随的2D图像；有些方法虽然利用2D特征但缺乏全局3D上下文融合或面临内存挑战。作者借鉴了DETR的编码器-解码器架构，创新性地设计了双层次2D特征利用策略：在编码器阶段通过'最近视图采样'将2D图像级特征注入3D点云；在解码器阶段使用紧凑的2D对象查询而非原始特征图，并引入3D框查询来调制注意力机制。这种方法既保留了2D模型的语义区分能力，又解决了内存和计算效率问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是充分利用2D图像的丰富语义信息来弥补3D数据不足的缺陷，通过双层次特征增强（图像级和对象级）提升3D表示能力。整体流程分为两个阶段：1）编码器阶段：对每个3D点从最近视图中采样2D图像特征，融合后通过3D编码器进行全局上下文融合；2）解码器阶段：初始化3D对象查询（含内容和位置信息），通过'框调制交叉注意力'和'距离感知交叉注意力'分别与3D超点特征和2D对象查询交互，逐步优化查询并生成最终分割结果。整个过程高效利用了2D语义信息，同时保持了3D几何结构的完整性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三方面：1）双层次2D特征增强策略，同时利用图像级和对象级特征；2）引入3D框查询调制位置注意力，使模型能根据物体大小动态调整注意力；3）距离感知交叉注意力机制，实现内存高效的特征利用。相比之前的工作，SegDINO3D不仅利用了2D图像信息，还保留了对象级别的语义表示；解决了直接使用2D特征图面临的内存挑战；通过框查询提供了更精确的空间约束；同时实现了更快的训练收敛速度（从432个epoch减少到88个epoch）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SegDINO3D通过创新性地结合图像级和对象级的2D特征，并引入3D框查询调制机制，显著提高了3D实例分割的性能，同时实现了更快的训练收敛速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present SegDINO3D, a novel Transformer encoder-decoderframework for 3D instance segmentation. As 3D training data is generally not assufficient as 2D training images, SegDINO3D is designed to fully leverage 2Drepresentation from a pre-trained 2D detection model, including bothimage-level and object-level features, for improving 3D representation.SegDINO3D takes both a point cloud and its associated 2D images as input. Inthe encoder stage, it first enriches each 3D point by retrieving 2D imagefeatures from its corresponding image views and then leverages a 3D encoder for3D context fusion. In the decoder stage, it formulates 3D object queries as 3Danchor boxes and performs cross-attention from 3D queries to 2D object queriesobtained from 2D images using the 2D detection model. These 2D object queriesserve as a compact object-level representation of 2D images, effectivelyavoiding the challenge of keeping thousands of image feature maps in the memorywhile faithfully preserving the knowledge of the pre-trained 2D model. Theintroducing of 3D box queries also enables the model to modulatecross-attention using the predicted boxes for more precise querying. SegDINO3Dachieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3Dinstance segmentation benchmarks. Notably, on the challenging ScanNet200dataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAPon the validation and hidden test sets, respectively, demonstrating itssuperiority.</description>
      <author>example@mail.com (Jinyuan Qu, Hongyang Li, Xingyu Chen, Shilong Liu, Yukai Shi, Tianhe Ren, Ruitao Jing, Lei Zhang)</author>
      <guid isPermaLink="false">2509.16098v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>See&amp;Trek: Training-Free Spatial Prompting for Multimodal Large Language Model</title>
      <link>http://arxiv.org/abs/2509.16087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SEE&amp;TREK是一个无需训练的提示框架，旨在增强多模态大语言模型在纯视觉条件下的空间理解能力。&lt;h4&gt;背景&lt;/h4&gt;先前的研究已尝试结合深度或点云等模态提高空间推理，但纯视觉空间理解仍不充分探索。&lt;h4&gt;目的&lt;/h4&gt;SEE&amp;TREK专注于两个核心原则：增加视觉多样性和运动重建，以解决纯视觉空间理解的研究空白。&lt;h4&gt;方法&lt;/h4&gt;1) 视觉多样性：通过最大语义丰富度采样，使用感知模型提取语义丰富的关键帧；2) 运动重建：模拟视觉轨迹并编码相对空间位置到关键帧中，保持空间关系和时间连贯性。该方法无需训练和GPU资源，只需一次前向传递，可无缝集成到现有MLLMs中。&lt;h4&gt;主要发现&lt;/h4&gt;在VSI-BENCH和STI-BENCH上的实验表明，SEE&amp;TREK在各种空间推理任务中持续提升MLLMs性能，最高提升3.5%。&lt;h4&gt;结论&lt;/h4&gt;SEE&amp;TREK为增强空间智能提供了一条有希望的路径。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了SEE&amp;TREK，这是首个专为增强多模态大语言模型(MLLMs)在纯视觉约束下的空间理解而设计的无需训练的提示框架。虽然先前的研究已纳入深度或点云等模态来提高空间推理能力，但纯视觉空间理解仍然未被充分探索。SEE&amp;TREK通过专注于两个核心原则来解决这一差距：增加视觉多样性和运动重建。对于视觉多样性，我们进行最大语义丰富度采样，使用现成的感知模型提取能够捕捉场景结构的语义丰富的关键帧。对于运动重建，我们模拟视觉轨迹并将相对空间位置编码到关键帧中，以保持空间关系和时间连贯性。我们的方法无需训练和GPU资源，只需要一次前向传递，可以无缝集成到现有的MLLMs中。在VSI-BENCH和STI-BENCH上的大量实验表明，SEE&amp;TREK在各种空间推理任务中持续提升了各种MLLMs的性能，最高提升了3.5%，为增强空间智能提供了一条有希望的路径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型在空间理解方面的局限性，特别是在只有视觉输入的情况下。具体来说，它针对两个问题：视觉同质性和未知运动。这个问题很重要，因为空间推理对模型理解和与真实世界环境交互至关重要，特别是在导航、机器人操作等应用中。增强模型的空间意识可以显著改善下游应用的性能，但现有模型在处理复杂空间关系时仍存在困难。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过对现有MLLMs空间理解瓶颈的批判性反思，确定了视觉同质性和未知运动两个关键问题。针对这些问题，他们设计了SEE&amp;TREK框架，包含增加视觉多样性和运动重建两个核心原则。作者借鉴了现有工作中的多种技术，如使用YOLO进行物体检测，利用视觉里程计模拟轨迹，但提出了改进的Balanced-TopK帧选择策略和新的时空编码方法，形成了一个综合解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增强关键帧的语义丰富度和添加运动信息来提升MLLMs的空间理解能力，而无需额外训练或GPU资源。整体流程包括：1)初始采样从视频中每隔N帧取一帧；2)最大语义丰富度采样，使用YOLO检测物体并采用Balanced-TopK策略选择关键帧；3)运动重建，使用视觉里程计估计相机姿态并生成轨迹可视化；4)时空编码，为关键帧添加帧索引和颜色编码标记；5)联合优化提示，将增强关键帧与轨迹可视化和文本提示结合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个训练和GPU免费的空间提示框架；2)最大语义丰富度采样，提出Balanced-TopK策略选择语义丰富且时间分布均匀的关键帧；3)运动重建，通过视觉里程计估计相机运动并将信息编码到关键帧中；4)时空编码，通过直观标记表示时间顺序和空间进展。相比之前工作，SEE&amp;TREK仅使用视觉输入，无需跨模态对齐，不需要微调模型，即插即用，且通过语义丰富采样和运动重建提供更全面的空间信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SEE&amp;TREK通过一种无需训练和GPU资源的空间提示方法，通过增强视觉多样性和重建运动信息，显著提升了多模态大语言模型在空间理解任务上的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SEE&amp;TREK, the first training-free prompting framework tailoredto enhance the spatial understanding of Multimodal Large Language Models(MLLMS) under vision-only constraints. While prior efforts have incorporatedmodalities like depth or point clouds to improve spatial reasoning, purelyvisualspatial understanding remains underexplored. SEE&amp;TREK addresses this gapby focusing on two core principles: increasing visual diversity and motionreconstruction. For visual diversity, we conduct Maximum Semantic RichnessSampling, which employs an off-the-shell perception model to extractsemantically rich keyframes that capture scene structure. For motionreconstruction, we simulate visual trajectories and encode relative spatialpositions into keyframes to preserve both spatial relations and temporalcoherence. Our method is training&amp;GPU-free, requiring only a single forwardpass, and can be seamlessly integrated into existing MLLM'S. Extensiveexperiments on the VSI-B ENCH and STI-B ENCH show that S EE &amp;T REK consistentlyboosts various MLLM S performance across diverse spatial reasoning tasks withthe most +3.5% improvement, offering a promising path toward stronger spatialintelligence.</description>
      <author>example@mail.com (Pengteng Li, Pinhao Song, Wuyang Li, Weiyu Guo, Huizai Yao, Yijie Xu, Dugang Liu, Hui Xiong)</author>
      <guid isPermaLink="false">2509.16087v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Point Cloud Surface Reconstruction using B-Splines</title>
      <link>http://arxiv.org/abs/2509.16050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于字典引导的图卷积网络的表面重建策略，能够在不使用点法线的情况下，同时预测控制点的位置和数量，为有噪声的点云数据生成平滑表面，并在性能上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;从离散点云数据生成连续表面是多个3D视觉应用中的基础任务，但现实世界中的点云数据固有噪声。现有数据驱动表面重建算法依赖真实法线或计算近似法线作为中间步骤，这使得它们在有噪声点云数据集上极不可靠，且真实训练数据并非总是可用。&lt;h4&gt;目的&lt;/h4&gt;开发一种表面重建策略，能够在不使用任何点法线的情况下，为有噪声的点云数据生成平滑表面，同时预测控制点的位置和数量，以匹配底层表面的复杂度。&lt;h4&gt;方法&lt;/h4&gt;开发了一种基于字典引导的图卷积网络的表面重建策略，结合B样条重建技术，这些技术提供点云的紧凑表面表示并具有平滑特性。该方法能够同时预测控制点的位置和数量，而不依赖法线信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛使用的评估指标与多个知名及最新基线方法进行比较，定性和定量地证明了该方法在表面重建任务上优于所有基线方法。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效处理有噪声的点云数据，生成高质量平滑表面，且不依赖于法线信息，在表面重建领域具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;从离散点云数据生成连续表面是多个三维视觉应用中的基础任务。现实世界中的点云数据由于各种技术和环境因素而固有噪声。现有的数据驱动表面重建算法严重依赖真实法线或将计算近似法线作为中间步骤。这种依赖使得它们在有噪声的点云数据集上极不可靠，即使有真实训练数据可用（但并非总是如此）。B样条重建技术提供点云的紧凑表面表示，特别以其平滑特性而闻名。然而，使用B样条近似的表面复杂度直接受样条控制点的数量和位置影响。现有的基于样条的建模方法为给定点云预测固定数量的控制点位置，这使得很难匹配其底层表面的复杂度。在这项工作中，我们开发了一种基于字典引导的图卷积网络的表面重建策略，我们同时为有噪声的点云数据预测控制点的位置和数量，从而在不使用任何点法线的情况下生成平滑表面。我们使用广泛使用的评估指标将我们的重建方法与几个知名及最新的基线方法进行比较，并证明我们的方法在定性和定量上都优于所有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从离散噪声点云数据生成连续表面的问题，特别是在不依赖点法线信息的情况下。这个问题在现实世界中非常重要，因为点云是3D视觉应用的基础，如CAD设计、机器人导航、医疗成像和文化遗产保护等。然而，现实世界中的点云通常含有噪声，而现有方法要么依赖真实法线（通常不可用），要么在噪声环境下表现不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计新方法：传统几何技术和Poisson重建在噪声下表现差；机器学习方法依赖法线估计；B样条方法使用固定数量控制点导致欠拟合或过拟合。作者借鉴了图卷积网络(GCN)处理点云的能力和B样条的平滑特性，结合了数据驱动和解析技术。具体而言，利用GCN提取点云特征，引入可学习字典机制指导控制点预测，并解决了B样条方法对有序网格结构的依赖问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于图的字典引导策略，同时预测B样条控制点的位置和数量，从而从无序噪声点云重建平滑表面，无需法线信息。整体流程包括：1)创建噪声B样条点云数据集；2)使用GCN提取点云特征；3)通过字典引导机制预测控制点；4)用预测的控制点生成B样条表面。训练时使用加权均方误差损失函数，在GPU上优化模型参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)同时预测控制点位置和数量，而非固定数量；2)无需点法线或其中间估计；3)使用GCN处理无序点云；4)引入可学习字典机制；5)创建专门的噪声点云数据集。相比之前工作，本文方法能处理无序、噪声点云，自适应确定控制点数量，避免了传统方法在噪声下的敏感性和现有B样条方法对有序结构的依赖，在多个评估指标上表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于图卷积网络的字典引导方法，能够同时预测B样条控制点的位置和数量，从而从无序噪声点云中重建平滑连续的表面，无需依赖点法线信息，并在多个评估指标上优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating continuous surfaces from discrete point cloud data is afundamental task in several 3D vision applications. Real-world point clouds areinherently noisy due to various technical and environmental factors. Existingdata-driven surface reconstruction algorithms rely heavily on ground truthnormals or compute approximate normals as an intermediate step. This dependencymakes them extremely unreliable for noisy point cloud datasets, even if theavailability of ground truth training data is ensured, which is not always thecase. B-spline reconstruction techniques provide compact surfacerepresentations of point clouds and are especially known for their smootheningproperties. However, the complexity of the surfaces approximated usingB-splines is directly influenced by the number and location of the splinecontrol points. Existing spline-based modeling methods predict the locations ofa fixed number of control points for a given point cloud, which makes it verydifficult to match the complexity of its underlying surface. In this work, wedevelop a Dictionary-Guided Graph Convolutional Network-based surfacereconstruction strategy where we simultaneously predict both the location andthe number of control points for noisy point cloud data to generate smoothsurfaces without the use of any point normals. We compare our reconstructionmethod with several well-known as well as recent baselines by employingwidely-used evaluation metrics, and demonstrate that our method outperforms allof them both qualitatively and quantitatively.</description>
      <author>example@mail.com (Stuti Pathak, Rhys G. Evans, Gunther Steenackers, Rudi Penne)</author>
      <guid isPermaLink="false">2509.16050v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Towards Sharper Object Boundaries in Self-Supervised Depth Estimation</title>
      <link>http://arxiv.org/abs/2509.15987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BMVC 2025 Oral, 10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用自监督方法就能产生清晰深度不连续性的单目深度估计方法，在KITTI和VKITTIv2数据集上实现了高达35%的更高边界清晰度，并改进了点云质量。&lt;h4&gt;背景&lt;/h4&gt;精确的单目深度估计对3D场景理解至关重要，但现有方法通常在物体边界处模糊深度，引入虚假的中间3D点。&lt;h4&gt;目的&lt;/h4&gt;实现具有清晰边缘的深度估计，而无需非常细粒度的监督，仅使用自监督方法产生清晰的深度不连续性。&lt;h4&gt;方法&lt;/h4&gt;将每像素深度建模为混合分布，捕获多种合理的深度值，并将不确定性从直接回归转移到混合权重；通过方差感知损失函数和不确定性传播，将此公式无缝集成到现有流程中。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和VKITTIv2上的广泛评估表明，与最先进的基线相比，该方法可实现高达35%的更高边界清晰度，并改进了点云质量。&lt;h4&gt;结论&lt;/h4&gt;该方法能够在仅使用自监督的情况下产生清晰的深度不连续性，在多个数据集上超越了现有技术。&lt;h4&gt;翻译&lt;/h4&gt;精确的单目深度估计对3D场景理解至关重要，但现有方法通常在物体边界处模糊深度，引入虚假的中间3D点。虽然实现清晰边缘通常需要非常细粒度的监督，但我们的方法仅使用自监督就能产生清晰的深度不连续性。具体来说，我们将每像素深度建模为混合分布，捕获多种合理的深度值，并将不确定性从直接回归转移到混合权重。通过方差感知损失函数和不确定性传播，该公式能无缝集成到现有流程中。在KITTI和VKITTIv2上的广泛评估表明，与最先进的基线相比，我们的方法可实现高达35%的更高边界清晰度，并改进了点云质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目深度估计中物体边界模糊的问题。现有方法在物体边界处往往产生模糊的深度值，导致出现虚假的中间3D点，这在点云中表现为伪影。这个问题在现实中非常重要，因为准确的深度边界对3D场景理解、自动驾驶中的障碍物检测、机器人导航和增强现实应用至关重要。模糊的边界会影响3D重建质量，导致后续处理任务如物体识别和场景解析的准确性下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到传统方法为每个像素分配单一深度值，但在边界处存在不确定性，导致深度值在前景和背景之间平均，使过渡模糊。他们想到使用混合分布来表示每个像素的深度，而不是单一值，这样可以明确建模边界处的多种可能深度。作者借鉴了混合密度网络和期望最大化算法的思想，参考了Tosi等人使用拉普拉斯分布混合进行监督立体深度估计的工作，以及Kendall和Gal关于深度预测中不确定性估计的贝叶斯框架。同时，他们采用了标准的自监督深度估计管道，基于视图合成作为监督信号。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将每个像素的深度表示为两个高斯分布的混合，而不是单一值，这样能够捕捉物体边界处的前景和背景两种可能的深度值。通过混合权重控制每个分布的相对重要性，从而在边界处创建清晰的深度不连续性。整体流程包括：1)分布表示：网络预测五个参数(两个均值、两个方差和一个混合权重)；2)分布传播：将深度分布通过重投影函数传播到支持视图，使用一阶近似技术处理不确定性；3)损失计算：为每个组件计算误差分布，使用竞争训练方法使组件专门化；4)推理：根据混合权重选择最可能的深度值，避免由不确定性引起的平均效应。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合分布表示：首次将每个像素的深度表示为混合分布，自然捕捉边界处的不确定性；2)分布传播技术：提出原则性方法传播深度分布通过重投影过程；3)不确定性感知损失函数：开发能处理多模态深度分布的损失函数；4)边缘锐度度量：提出基于边缘像素熵的边界锐度度量。相比之前的工作，不同之处在于：传统方法需要精细监督或高分辨率图像和精细标注，而本文仅使用自监督；传统方法分配单一深度值导致边界模糊，而本文使用混合分布明确建模边界处多种可能深度；传统方法忽略或简单处理不确定性，而本文将不确定性作为核心组成部分；本文还提出了新的边缘熵度量专门评估边界锐度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将每个像素的深度建模为混合分布并提出分布传播技术，在仅使用自监督的情况下显著提高了单目深度估计中物体边界的锐利度，同时改进了点云质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate monocular depth estimation is crucial for 3D scene understanding,but existing methods often blur depth at object boundaries, introducingspurious intermediate 3D points. While achieving sharp edges usually requiresvery fine-grained supervision, our method produces crisp depth discontinuitiesusing only self-supervision. Specifically, we model per-pixel depth as amixture distribution, capturing multiple plausible depths and shiftinguncertainty from direct regression to the mixture weights. This formulationintegrates seamlessly into existing pipelines via variance-aware loss functionsand uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 showthat our method achieves up to 35% higher boundary sharpness and improves pointcloud quality compared to state-of-the-art baselines.</description>
      <author>example@mail.com (Aurélien Cecille, Stefan Duffner, Franck Davoine, Rémi Agier, Thibault Neveu)</author>
      <guid isPermaLink="false">2509.15987v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>PAN: Pillars-Attention-Based Network for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.15935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖且高效的基于摄像头和雷达的3D目标检测算法，在鸟瞰图视角下工作，利用雷达点云的优势如精确距离估计和速度信息，并优化了推理时间。&lt;h4&gt;背景&lt;/h4&gt;Camera-radar融合是Camera-lidar融合在3D目标检测任务中的一种稳健且低成本的替代方案，特别适用于恶劣天气和光照条件。然而，目前文献中很少有工作关注这种模态并探索雷达点云的优势。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖且高效的3D目标检测算法，使用摄像头和雷达在鸟瞰图视角下工作，充分利用雷达点云的优势并提高推理效率。&lt;h4&gt;方法&lt;/h4&gt;在融合特征前利用雷达优势；引入新backbone将雷达柱状特征映射到嵌入维度；使用自注意力机制建模雷达点间依赖关系；用简化卷积层替换基于FPN的卷积层以减少推理时间。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在3D目标检测问题上达到新最先进水平，使用ResNet-50时NDS指标达58.2，同时在nuScenes数据集上为同类算法设置了推理时间新基准。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法通过优化雷达特征处理和简化网络结构，在保持高精度的同时显著提高了推理速度，为实时3D目标检测提供了新解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摄像头-雷达融合为摄像头-激光雷达融合提供了一种稳健且低成本的替代方案，用于在恶劣天气和光照条件下实时执行3D目标检测任务。然而，目前文献中很少有工作关注这种模态，更重要的是，很少有工作开发新架构来探索雷达点云的优势，如精确的距离估计和速度信息。因此，这项工作提出了一种新颖且高效的3D目标检测算法，使用摄像头和雷达在鸟瞰图视角下工作。我们的算法在将特征融合到检测头之前利用雷达的优势。引入了一个新的backbone，将雷达柱状特征映射到嵌入维度。自注意力机制使backbone能够建模雷达点之间的依赖关系。我们使用简化的卷积层替换基于PointPillars架构中基于FPN的卷积层，主要目标是减少推理时间。我们的结果表明，通过这种修改，我们的方法在3D目标检测问题上达到了新的最先进水平，使用ResNet-50时NDS指标达到58.2，同时为同一类别在nuScenes数据集上设置了推理时间的新基准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在3D目标检测中如何有效利用相机-雷达融合技术，以提高在恶劣天气和光照条件下的实时检测性能。这个问题在现实中非常重要，因为自动驾驶系统需要在各种环境条件下准确识别和定位物体，而现有传感器如相机和激光雷达在恶劣条件下性能下降。雷达数据具有精确的距离估计和速度信息等优势，但现有研究很少充分利用这些特性，且缺乏专门针对相机-雷达融合的新架构。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到雷达点云的稀疏性特点导致传统卷积方法效率低下，因此设计了基于支柱的注意力机制网络(PAN)来更好地利用雷达特性。作者借鉴了现有工作中的多个元素：使用ResNet提取相机特征，借鉴PointPillars的点云处理思想但进行了改进，采用自注意力机制建模雷达点依赖关系，使用雷达辅助的视图转换(RVT)将相机特征转换为鸟瞰图(BEV)，应用多模态特征聚合融合雷达和相机特征，以及使用CenterPoint作为检测头。这些借鉴被创新性地结合并改进，形成了PAN方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用雷达数据在恶劣天气条件下的鲁棒性和精确的距离/速度信息，通过基于支柱的注意力机制有效提取雷达特征，并简化网络结构以提高推理速度，最终在鸟瞰图空间中融合相机和雷达特征。整体实现流程分为四个阶段：1)特征提取阶段，使用ResNet提取相机特征，使用PAN骨干提取雷达特征；2)视图转换阶段，使用RVT将相机特征转换为BEV，将雷达特征转换为二进制占用图；3)特征融合阶段，使用多模态特征聚合模块融合雷达和相机BEV特征；4)目标检测阶段，使用CenterPoint检测头进行3D目标检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出PAN骨干网络，使用自注意力机制建模雷达点依赖关系，去除空柱有效利用雷达特征；2)简化基于FPN的卷积层，减少推理时间；3)在恶劣天气条件下表现优异，在nuScenes数据集上达到58.2的NDS指标。相比之前工作，PAN在推理速度上提高约43%，同时检测精度更高；相比纯相机方法，在恶劣条件下性能更好；相比相机-激光雷达融合方法，成本更低且更鲁棒；更有效地利用雷达的距离和速度信息，提高了方向和属性估计的准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAN提出了一种基于支柱注意力的相机-雷达融合网络，通过有效利用雷达特征并简化网络结构，在保持高检测精度的同时显著提高了推理速度，特别是在恶劣天气条件下表现出优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-radar fusion offers a robust and low-cost alternative to Camera-lidarfusion for the 3D object detection task in real-time under adverse weather andlighting conditions. However, currently, in the literature, it is possible tofind few works focusing on this modality and, most importantly, developing newarchitectures to explore the advantages of the radar point cloud, such asaccurate distance estimation and speed information. Therefore, this workpresents a novel and efficient 3D object detection algorithm using cameras andradars in the bird's-eye-view (BEV). Our algorithm exploits the advantages ofradar before fusing the features into a detection head. A new backbone isintroduced, which maps the radar pillar features into an embedded dimension. Aself-attention mechanism allows the backbone to model the dependencies betweenthe radar points. We are using a simplified convolutional layer to replace theFPN-based convolutional layers used in the PointPillars-based architectureswith the main goal of reducing inference time. Our results show that with thismodification, our approach achieves the new state-of-the-art in the 3D objectdetection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,while also setting a new benchmark for inference time on the nuScenes datasetfor the same category.</description>
      <author>example@mail.com (Ruan Bispo, Dane Mitrev, Letizia Mariotti, Clément Botty, Denver Humphrey, Anthony Scanlan, Ciarán Eising)</author>
      <guid isPermaLink="false">2509.15935v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation</title>
      <link>http://arxiv.org/abs/2509.15886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了将视觉基础模型SAM2应用于LiDAR点云分割的range-view方法，通过结合2D特征提取与投影技术，实现了在保持性能的同时提高效率和简化部署。&lt;h4&gt;背景&lt;/h4&gt;点云分割是自动驾驶和3D场景理解的核心技术。当前主流的体素和点云方法虽能捕捉细粒度几何信息，但计算成本高、内存访问不规则且实时效率有限。相比之下，range-view方法相对未被充分探索，但可利用成熟的2D语义分割技术进行快速准确预测。&lt;h4&gt;目的&lt;/h4&gt;研究当前最先进的视觉基础模型SAM2是否能作为LiDAR点云在range view中的强大backbone，并开发首个适应SAM2用于3D分割的range-view框架。&lt;h4&gt;方法&lt;/h4&gt;结合高效的2D特征提取与标准投影/反投影操作处理点云，并对编码器进行三种架构修改：(1)强调LiDAR范围图像中水平空间依赖性的新模块；(2)针对球形投影几何特性定制的配置；(3)专门设计用于捕获range-view伪图像中独特空间模式和间断性的适应机制。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在SemanticKITTI数据集上实现了有竞争力的性能，同时受益于2D为中心的管道的速度、可扩展性和部署简便性。&lt;h4&gt;结论&lt;/h4&gt;视觉基础模型作为3D感知的通用backbone具有可行性，为统一、基础模型驱动的LiDAR分割开辟了道路，使用VFMs的range-view分割方法取得了有前景的结果。&lt;h4&gt;翻译&lt;/h4&gt;点云分割是自动驾驶和3D场景理解的核心。虽然最近的体素和基于点的方法由于其与深度架构的兼容性和捕捉细粒度几何的能力而主导研究，但它们通常带来高计算成本、不规则内存访问和有限的实时效率。相比之下，range-view方法虽然相对未被充分探索，但可以利用成熟的2D语义分割技术进行快速准确的预测。受视觉基础模型在描述、零样本识别和多模态任务方面的快速进展启发，我们研究了当前最先进的分割VFM模型SAM2是否能作为LiDAR点云在range view中的强大backbone。我们提出了，据我们所知，首个将SAM2适应于3D分割的range-view框架，结合高效的2D特征提取与标准投影/反投影操作来处理点云。为了将SAM2优化用于range-view表示，我们在编码器中实现了几种架构修改：(1)一个强调LiDAR范围图像中固有水平空间依赖性的新模块；(2)针对球形投影几何特性定制的配置；(3)在编码器主干中适应的机制，专门设计用于捕获range-view伪图像中存在的独特空间模式和间断性。我们的方法在SemanticKITTI上实现了有竞争力的性能，同时受益于2D为中心管道的速度、可扩展性和部署简便性。这项工作强调了VFMs作为3D感知通用backbone的可行性，并为统一、基础模型驱动的LiDAR分割开辟了道路。结果让我们得出结论，使用VFMs的range-view分割方法取得了有前景的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是LiDAR点云分割的效率和准确性问题。现有的点云分割方法（如体素和点方法）计算成本高、内存访问不规则、运行效率有限。这个问题在自动驾驶和3D场景理解中至关重要，因为准确的点云分割能让系统实时区分车辆、行人、路标等物体，是自动驾驶和机器人导航的基础技术。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到视觉基础模型快速进展的启发，特别是SAM2在分割任务上的优异表现。他们注意到基于range-view投影的方法虽然被忽视，但可以利用成熟的2D语义分割技术。作者借鉴了多项现有工作：使用SAM2作为骨干网络、采用range-view表示方法、应用Receptive Field Blocks进行特征解码、使用kNN插值进行后处理，以及采用复合损失函数。主要创新在于将SAM2适配到3D点云分割任务中，通过特定修改使其能处理range-view表示的点云数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型SAM2的强大分割能力，通过将3D点云投影为2D range-view图像，然后使用SAM2进行分割，最后将分割结果投影回原始点云。整体流程包括：1)预处理：将无序LiDAR扫描转换为range-view表示；2)模型处理：使用Stem模块转换输入，通过修改后的SAM2编码器处理range-view图像，使用包含Receptive Field Blocks的解码器进行特征解码；3)后处理：通过kNN插值将分割结果传播到全分辨率点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次将SAM2视觉基础模型应用于range-view表示的LiDAR点云分割；2)对SAM2编码器进行特定修改：新的Stem模块强调水平空间依赖性，定制化Hiera Blocks适应球形投影，适配的窗口注意力机制捕获独特空间模式；3)提出多组件编码器架构，利用预训练的Hiera骨干网络；4)使用复合损失函数解决类别不平衡问题。相比之前工作，大多数方法专注于计算成本高的体素或点处理方法，而RangeSAM利用视觉基础模型的知识转移能力，同时保持2D方法的效率优势。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RangeSAM成功地将先进的视觉基础模型SAM2适配到3D点云分割任务中，通过特定的架构修改和range-view表示，实现了与现有方法相媲美的性能，同时保持了2D方法的效率优势。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud segmentation is central to autonomous driving and 3D sceneunderstanding. While voxel- and point-based methods dominate recent researchdue to their compatibility with deep architectures and ability to capturefine-grained geometry, they often incur high computational cost, irregularmemory access, and limited real-time efficiency. In contrast, range-viewmethods, though relatively underexplored - can leverage mature 2D semanticsegmentation techniques for fast and accurate predictions. Motivated by therapid progress in Visual Foundation Models (VFMs) for captioning, zero-shotrecognition, and multimodal tasks, we investigate whether SAM2, the currentstate-of-the-art VFM for segmentation tasks, can serve as a strong backbone forLiDAR point cloud segmentation in the range view. We present , to ourknowledge, the first range-view framework that adapts SAM2 to 3D segmentation,coupling efficient 2D feature extraction with standardprojection/back-projection to operate on point clouds. To optimize SAM2 forrange-view representations, we implement several architectural modifications tothe encoder: (1) a novel module that emphasizes horizontal spatial dependenciesinherent in LiDAR range images, (2) a customized configuration of tailored tothe geometric properties of spherical projections, and (3) an adapted mechanismin the encoder backbone specifically designed to capture the unique spatialpatterns and discontinuities present in range-view pseudo-images. Our approachachieves competitive performance on SemanticKITTI while benefiting from thespeed, scalability, and deployment simplicity of 2D-centric pipelines. Thiswork highlights the viability of VFMs as general-purpose backbones for 3Dperception and opens a path toward unified, foundation-model-driven LiDARsegmentation. Results lets us conclude that range-view segmentation methodsusing VFMs leads to promising results.</description>
      <author>example@mail.com (Paul Julius Kühn, Duc Anh Nguyen, Arjan Kuijper, Holger Graf, Dieter Fellner, Saptarshi Neil Sinha)</author>
      <guid isPermaLink="false">2509.15886v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion</title>
      <link>http://arxiv.org/abs/2509.15750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 15 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FloorSAM是一种整合点云密度图与分割一切模型(SAM)的框架，用于从LiDAR数据准确重建建筑平面图，解决了传统方法面临的噪声、泛化能力有限和几何细节丢失等问题。&lt;h4&gt;背景&lt;/h4&gt;从点云数据重建建筑平面图对室内导航、BIM和精确测量至关重要，但传统几何算法和基于Mask R-CNN的深度学习方法存在噪声、泛化能力有限和几何细节丢失等问题。&lt;h4&gt;目的&lt;/h4&gt;提出FloorSAM框架，通过结合点云密度图与SAM模型，实现从LiDAR数据准确重建建筑平面图，提高重建的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;FloorSAM使用网格过滤、自适应分辨率投影和图像增强创建鲁棒的自上而下密度图，利用SAM的零样本学习进行精确房间分割，通过自适应提示点和多阶段过滤生成房间掩码，最后结合掩码和点云分析进行轮廓提取和规则化。&lt;h4&gt;主要发现&lt;/h4&gt;在Giblayout和ISPRS数据集上的测试表明，FloorSAM比传统方法具有更好的准确性、召回率和鲁棒性，特别是在嘈杂和复杂环境中表现更佳。&lt;h4&gt;结论&lt;/h4&gt;FloorSAM能够生成准确的平面图并恢复房间拓扑关系，相关代码和材料已公开在github.com/Silentbarber/FloorSAM。&lt;h4&gt;翻译&lt;/h4&gt;从点云数据重建建筑平面图对室内导航、BIM和精确测量至关重要。传统方法如几何算法和基于Mask R-CNN的深度学习通常面临噪声、泛化能力有限和几何细节丢失等问题。我们提出了FloorSAM，一种将点云密度图与分割一切模型(SAM)相结合的框架，用于从LiDAR数据准确重建平面图。通过使用基于网格的过滤、自适应分辨率投影和图像增强，我们创建了鲁棒的自上而下密度图。FloorSAM利用SAM的零样本学习进行精确的房间分割，提高了对不同布局的重建效果。房间掩码通过自适应提示点和多阶段过滤生成，随后进行联合掩码和点云分析以进行轮廓提取和规则化。这产生了准确的平面图并恢复了房间拓扑关系。在Giblayout和ISPRS数据集上的测试显示，与传统方法相比，FloorSAM具有更好的准确性、召回率和鲁棒性，特别是在嘈杂和复杂环境中。代码和材料：github.com/Silentbarber/FloorSAM。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从激光雷达点云数据重建建筑平面图的问题。这个问题在现实中非常重要，因为准确的室内平面图对室内导航、建筑信息模型(BIM)和高精度室内测量应用至关重要。传统方法在处理噪声、复杂布局和几何细节时存在局限性，影响了测量的准确性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统几何方法和基于深度学习方法的局限性，如对噪声敏感、泛化能力有限和需要大量标注数据。然后，他们结合了点云密度图和SAM(分割任意模型)的优势，设计了新的框架。作者借鉴了多种现有工作，包括传统几何算法(如RANSAC)、基于深度学习的方法(如Mask R-CNN、HEAT、RoomFormer等)，以及图结构推理方法，但创新性地将它们与SAM的零样本学习能力相结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将房间高度的点云密度图与SAM的引导分割能力相结合，并利用SAM的零样本学习能力实现高保真房间分割。整体流程分为三个阶段：1)预处理阶段，包括点云过滤、密度图生成和自适应提示点提取；2)房间掩码过滤阶段，通过粗过滤和精过滤筛选高质量单房间掩码；3)轮廓绘制阶段，通过点云和掩码的联合分析提取和正则化轮廓，并恢复房间间的拓扑关系。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出结合密度图和SAM的混合框架，引入零样本分割到平面图重建；2)设计多阶段掩码过滤机制，从冗余掩码中提取高质量单房间分割；3)开发掩码-点云联合轮廓绘制和正则化算法，保留几何细节同时生成正则化轮廓。相比之前的工作，FloorSAM不依赖大量标注数据，能处理非曼哈顿结构，对噪声更具鲁棒性，且在复杂布局中能保留更多几何细节。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FloorSAM通过融合点云密度图与SAM的零样本分割能力，实现了从激光雷达点云数据中高质量、鲁棒的建筑平面图重建，解决了传统方法在噪声环境和复杂布局下的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing building floor plans from point cloud data is key for indoornavigation, BIM, and precise measurements. Traditional methods like geometricalgorithms and Mask R-CNN-based deep learning often face issues with noise,limited generalization, and loss of geometric details. We propose FloorSAM, aframework that integrates point cloud density maps with the Segment AnythingModel (SAM) for accurate floor plan reconstruction from LiDAR data. Usinggrid-based filtering, adaptive resolution projection, and image enhancement, wecreate robust top-down density maps. FloorSAM uses SAM's zero-shot learning forprecise room segmentation, improving reconstruction across diverse layouts.Room masks are generated via adaptive prompt points and multistage filtering,followed by joint mask and point cloud analysis for contour extraction andregularization. This produces accurate floor plans and recovers roomtopological relationships. Tests on Giblayout and ISPRS datasets show betteraccuracy, recall, and robustness than traditional methods, especially in noisyand complex settings. Code and materials: github.com/Silentbarber/FloorSAM.</description>
      <author>example@mail.com (Han Ye, Haofu Wang, Yunchi Zhang, Jiangjian Xiao, Yuqiang Jin, Jinyuan Liu, Wen-An Zhang, Uladzislau Sychou, Alexander Tuzikov, Vladislav Sobolevskii, Valerii Zakharov, Boris Sokolov, Minglei Fu)</author>
      <guid isPermaLink="false">2509.15750v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions</title>
      <link>http://arxiv.org/abs/2509.15693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  to appear in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SceneForge是一个新颖的框架，通过构建具有明确空间关系的多对象场景来增强3D点云与文本之间的对比学习，有效解决了大规模3D-文本数据集稀缺的问题，在多个任务上展现出显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;在3D文本对比学习中，整体大于部分之和。然而，大规模3D-文本数据集的稀缺限制了相关研究的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够增强3D点云与文本之间对比对齐的框架，解决大规模3D-文本数据集稀缺的问题，提高3D-文本多模态任务的性能。&lt;h4&gt;方法&lt;/h4&gt;SceneForge利用个体3D形状构建具有明确空间关系的多对象场景，并与由大型语言模型完善的多对象描述配对。通过系统研究关键设计元素，如每个场景的最佳对象数量、训练批次中组合样本的比例以及场景构建策略，优化框架性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. SceneForge在多个任务上带来显著的性能提升，包括在多个数据集上的零样本分类和少样本部分分割2. SceneForge的组合增强与模型无关，能够一致地提高多种编码器架构的性能3. SceneForge在3D视觉问答任务中表现更好，能够稳健地推广到具有递增场景复杂度的检索场景4. SceneForge展示了空间推理能力，能够调整空间配置以精确匹配文本指令&lt;h4&gt;结论&lt;/h4&gt;SceneForge通过结构化的多对象场景组合，有效解决了3D-文本对比学习中数据稀缺的问题，显著提高了模型在多种任务上的性能，展示了强大的空间推理能力和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;整体大于部分之和——即使在3D文本对比学习中也是如此。我们引入了SceneForge，一个新颖的框架，通过构建具有明确空间关系的多对象场景组合来增强3D点云与文本之间的对比对齐。SceneForge利用个体3D形状构建具有明确空间关系的多对象场景，并将其与由大型语言模型完善的多对象描述配对。通过添加这些结构化的、组合式的样本来增强对比训练，SceneForge有效解决了大规模3D-文本数据集稀缺的问题，显著丰富了数据的复杂性和和多样性。我们系统地研究了关键设计元素，如每个场景的最佳对象数量、训练批次中组合样本的比例以及场景构建策略。大量实验表明，SceneForge在多个任务上带来了显著的性能提升，包括在ModelNet、ScanObjNN、Objaverse-LVIS和ScanNet上的零样本分类，以及在ShapeNetPart上的少样本部分分割。SceneForge的组合增强与模型无关，能够一致地提高多种编码器架构的性能。此外，SceneForge在ScanQA上的3D视觉问答任务中表现更好，能够稳健地推广到具有递增场景复杂度的检索场景，并通过调整空间配置以精确匹配文本指令展示了空间推理能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云与文本对比学习中大规模数据集稀缺的问题。这个问题很重要，因为3D数据在机器人、虚拟现实和增强现实等领域具有关键价值，而相比2D图像-文本数据集，3D-文本数据集数量有限，限制了3D视觉语言模型的发展和应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受图像分类中的组合数据增强方法(如CutMix、MixUp)和图像-文本对比学习中的组合方法启发，注意到3D点云可以自由组合成结构化场景而不会产生视觉伪影。他们利用这一特性，结合大语言模型生成场景描述，设计了SCENEFORGE框架。这种方法借鉴了现有工作的思想，但针对3D数据的特性进行了创新性设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建结构化的多物体3D场景来增强3D点云与文本的对比学习对齐。实现流程包括：1)接收一批点云及其标题；2)随机保留单物体或组合成多物体场景；3)通过3D场景锻造模块根据空间关系安排物体；4)通过场景标题锻造模块生成连贯描述；5)混合单物体和多物体样本进行训练；6)使用对比损失进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)结构化多物体场景组合；2)显式空间关系建模('over','under','next to')；3)大语言模型增强的描述生成；4)模型无关的增强方法。相比之前工作，SCENEFORGE构建了语义上有意义的结构化场景而非简单混合，利用了3D数据可自由组合的特性，实现了对物体定位的明确控制，并在多种任务上超越了使用集成方法的模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCENEFORGE通过结构化多物体场景组合增强3D-文本对比学习，在保持模型无关性的同时显著提升了多种3D视觉语言任务的性能，解决了3D领域大规模数据稀缺的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The whole is greater than the sum of its parts-even in 3D-text contrastivelearning. We introduce SceneForge, a novel framework that enhances contrastivealignment between 3D point clouds and text through structured multi-objectscene compositions. SceneForge leverages individual 3D shapes to constructmulti-object scenes with explicit spatial relations, pairing them with coherentmulti-object descriptions refined by a large language model. By augmentingcontrastive training with these structured, compositional samples, SceneForgeeffectively addresses the scarcity of large-scale 3D-text datasets,significantly enriching data complexity and diversity. We systematicallyinvestigate critical design elements, such as the optimal number of objects perscene, the proportion of compositional samples in training batches, and sceneconstruction strategies. Extensive experiments demonstrate that SceneForgedelivers substantial performance gains across multiple tasks, includingzero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,as well as few-shot part segmentation on ShapeNetPart. SceneForge'scompositional augmentations are model-agnostic, consistently improvingperformance across multiple encoder architectures. Moreover, SceneForgeimproves 3D visual question answering on ScanQA, generalizes robustly toretrieval scenarios with increasing scene complexity, and showcases spatialreasoning capabilities by adapting spatial configurations to align preciselywith textual instructions.</description>
      <author>example@mail.com (Cristian Sbrolli, Matteo Matteucci)</author>
      <guid isPermaLink="false">2509.15693v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>A PCA Based Model for Surface Reconstruction from Incomplete Point Clouds</title>
      <link>http://arxiv.org/abs/2509.15675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于主成分分析(PCA)的模型，用于从不完整的点云数据进行表面重建。该模型利用PCA估计底层表面的法线信息，并将其作为正则化子指导重建过程，特别是在数据缺失区域。研究还引入了算子分裂方法来有效求解该模型。&lt;h4&gt;背景&lt;/h4&gt;点云数据是数学建模中的重要信息形式，表面重建是跨学科的重要任务。然而，在扫描过程中，由于高光吸收率和遮挡等因素，收集的点云数据可能无法覆盖整个表面，导致数据集不完整。&lt;h4&gt;目的&lt;/h4&gt;推断数据缺失区域的表面结构，并成功重建表面，解决因扫描不完整导致的表面重建挑战。&lt;h4&gt;方法&lt;/h4&gt;使用主成分分析(PCA)从可用点云数据估计底层表面的法线信息，将估计的法线信息作为模型中的正则化子，指导表面重建，特别是在数据缺失区域，并引入算子分裂方法来有效求解提出的模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过系统实验，该模型能够成功推断数据缺失区域的表面结构，并很好地重建底层表面，性能优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;基于PCA的模型能有效处理不完整点云数据的表面重建问题，特别是在数据缺失区域表现优异。&lt;h4&gt;翻译&lt;/h4&gt;点云数据是数学建模中的一种重要信息形式，从这些数据进行表面重建是跨学科的重要任务。然而，在扫描过程中，由于高光吸收率和遮挡等因素，收集的点云数据可能无法覆盖整个表面，导致数据集不完整。推断数据缺失区域的表面结构并成功重建表面是一项挑战。在本文中，我们提出了一种基于主成分分析(PCA)的模型，用于从不完整的点云数据进行表面重建。最初，我们使用PCA从可用点云数据估计底层表面的法线信息。这种估计的法线信息在我们的模型中作为正则化子，指导表面重建，特别是在数据缺失区域。此外，我们引入了一种算子分裂方法来有效求解所提出的模型。通过系统实验，我们证明了我们的模型能够成功推断数据缺失区域的表面结构，并很好地重建底层表面，性能优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从点云数据中重建表面时，由于扫描过程中的高光吸收率和遮挡等因素导致的数据不完整问题。这个问题在现实和研究中非常重要，因为点云数据是数学建模的关键信息，表面重建在多个领域（如城市重建、医学成像、计算机图形学、文化遗产保护和元宇宙开发等）有广泛应用。然而，现有方法主要针对完整的点云数据，对于不完整数据的处理研究不足，导致在数据缺失区域难以准确推断表面结构。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了不完整点云数据重建表面的挑战，指出在数据缺失区域仅依靠距离函数难以控制重建表面。他们借鉴了水平集方法（如Zhao等人[52,51]的工作）来隐式表示表面，以及[20]引入表面曲率作为正则化的方法和[30]利用法线信息对齐的方法。在此基础上，作者创新性地提出使用主成分分析（PCA）从可用点云数据估计底层表面的法线信息，并将这些估计的法线作为正则化项引入模型，特别是在数据缺失区域。作者还引入了算子分裂方法来有效求解提出的模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1)使用PCA从点云数据估计表面法线信息，即使在数据缺失区域也能提供可靠估计；2)将估计的法线信息作为正则化项引入表面重建模型；3)利用水平集方法隐式表示表面，并通过算子分裂方法高效求解。整体流程包括：1)预处理阶段计算距离函数和PCA主方向；2)构建包含数据保真项、曲率正则化项和法线信息正则化项的优化模型；3)使用水平集方法将问题公式化；4)通过算子分裂方法将问题分解为四个子步骤迭代求解；5)进行数值离散化并使用FFT高效求解；6)对水平集函数重新初始化并提取零水平集作为重建表面。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于PCA的表面重建模型，专门处理不完整点云数据；2)引入法线信息正则化项，惩罚重建表面法线与PCA估计法线间的角度；3)设计算子分裂方法高效求解模型；4)方法对噪声具有鲁棒性。相比之前工作，本文专注于不完整数据的处理，不依赖预先提供的法线信息，而是从点云数据中估计法线；模型结合了数据保真、曲率正则化和法线信息正则化三项；求解方法采用算子分裂，提高了效率且对超参数不那么敏感。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于PCA的不完整点云表面重建新方法，通过估计表面法线信息作为正则化项，有效解决了数据缺失区域的表面重建问题，并在实验中显示出优于现有方法的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud data represents a crucial category of information formathematical modeling, and surface reconstruction from such data is animportant task across various disciplines. However, during the scanningprocess, the collected point cloud data may fail to cover the entire surfacedue to factors such as high light-absorption rate and occlusions, resulting inincomplete datasets. Inferring surface structures in data-missing regions andsuccessfully reconstructing the surface poses a challenge. In this paper, wepresent a Principal Component Analysis (PCA) based model for surfacereconstruction from incomplete point cloud data. Initially, we employ PCA toestimate the normal information of the underlying surface from the availablepoint cloud data. This estimated normal information serves as a regularizer inour model, guiding the reconstruction of the surface, particularly in areaswith missing data. Additionally, we introduce an operator-splitting method toeffectively solve the proposed model. Through systematic experimentation, wedemonstrate that our model successfully infers surface structures indata-missing regions and well reconstructs the underlying surfaces,outperforming existing methodologies.</description>
      <author>example@mail.com (Hao Liu)</author>
      <guid isPermaLink="false">2509.15675v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Bench-RNR: Dataset for Benchmarking Repetitive and Non-repetitive Scanning LiDAR for Infrastructure-based Vehicle Localization</title>
      <link>http://arxiv.org/abs/2509.15583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个基于基础设施的车辆定位数据集，比较了重复和非重复扫描激光雷达的性能，为选择最适合的激光雷达扫描模式提供了见解。&lt;h4&gt;背景&lt;/h4&gt;现有研究大多依赖重复扫描激光雷达，而非重复扫描激光雷达虽有消除盲区和成本效益更高的优势，但在路边感知和定位中的应用仍有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于基础设施的车辆定位数据集，以基准不同激光雷达扫描模式的性能。&lt;h4&gt;方法&lt;/h4&gt;收集了来自重复和非重复扫描激光雷达的数据集，包含5,445帧点云数据，涵盖八种车辆轨迹序列，轨迹类型多样。&lt;h4&gt;主要发现&lt;/h4&gt;建立了基于基础设施的车辆定位基线，比较了使用非重复和重复扫描激光雷达的方法性能，为选择最适合的激光雷达扫描模式提供了有价值的见解。&lt;h4&gt;结论&lt;/h4&gt;该数据集对科学界有重要贡献，支持基于基础设施的感知和车辆定位的发展。数据集和源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;使用路边激光雷达的车辆定位可以为云端控制的车辆提供厘米级精度，同时为多辆车辆提供服务，提高安全性和效率。虽然大多数现有研究依赖重复扫描激光雷达，但非重复扫描激光雷达具有消除盲区和成本效益更高的优势。然而，其在路边感知和定位中的应用仍然有限。为此，我们提出了一个基于基础设施的车辆定位数据集，其中包含来自重复和非重复扫描激光雷达的数据，以便基准不同激光雷达扫描模式的性能。该数据集包含5,445帧点云数据，涵盖八种车辆轨迹序列，轨迹类型多样。我们的实验建立了基于基础设施的车辆定位基线，并使用非重复和重复扫描激光雷达比较了这些方法的性能。这项工作为选择最适合基于基础设施的车辆定位的激光雷达扫描模式提供了有价值的见解。我们的数据集对科学界有重要贡献，支持基于基础设施的感知和车辆定位的发展。数据集和源代码可在以下公开获取：https://github.com/sjtu-cyberc3/BenchRNR。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle localization using roadside LiDARs can provide centimeter-levelaccuracy for cloud-controlled vehicles while simultaneously serving multiplevehicles, enhanc-ing safety and efficiency. While most existing studies rely onrepetitive scanning LiDARs, non-repetitive scanning LiDAR offers advantagessuch as eliminating blind zones and being more cost-effective. However, itsapplication in roadside perception and localization remains limited. To addressthis, we present a dataset for infrastructure-based vehicle localization, withdata collected from both repetitive and non-repetitive scanning LiDARs, inorder to benchmark the performance of different LiDAR scanning patterns. Thedataset contains 5,445 frames of point clouds across eight vehicle trajectorysequences, with diverse trajectory types. Our experiments establish base-linesfor infrastructure-based vehicle localization and compare the performance ofthese methods using both non-repetitive and repetitive scanning LiDARs. Thiswork offers valuable insights for selecting the most suitable LiDAR scanningpattern for infrastruc-ture-based vehicle localization. Our dataset is asignifi-cant contribution to the scientific community, supporting advancementsin infrastructure-based perception and vehicle localization. The dataset andsource code are publicly available at:https://github.com/sjtu-cyberc3/BenchRNR.</description>
      <author>example@mail.com (Runxin Zhao, Chunxiang Wang, Hanyang Zhuang, Ming Yang)</author>
      <guid isPermaLink="false">2509.15583v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Distribution Estimation for Global Data Association via Approximate Bayesian Inference</title>
      <link>http://arxiv.org/abs/2509.15565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种利用近似贝叶斯推理的数据关联框架，能够捕捉数据关联问题的多种解决方案模式，避免在模糊情况下过早承诺单一解决方案，并通过粒子演化来覆盖解决方案分布的模式。&lt;h4&gt;背景&lt;/h4&gt;全局数据关联是机器人在不同时间或不同机器人看到的环境中操作的基本前提。现有方法通常依赖最大似然估计或最大共识来产生单一组关联，但在存在重复或对称数据时会面临重大挑战。在模糊场景中，全局数据关联问题的解决方案分布通常是多模态的，单一解决方案的方法往往会失败。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉数据关联问题多种解决方案模式的数据关联框架，避免在模糊情况下过早承诺单一解决方案，并能正确估计变换分布。&lt;h4&gt;方法&lt;/h4&gt;引入利用近似贝叶斯推理的数据关联框架，将假设解决方案表示为粒子，这些粒子根据确定性或随机更新规则演化，以覆盖底层解决方案分布的模式。该方法可以整合数据关联公式施加的优化约束，并直接受益于GPU并行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在高度模糊数据的模拟和真实世界实验中，该方法在点云或对象地图配准时能够正确估计变换分布。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据关联框架能够有效处理模糊场景中的全局数据关联问题，通过捕捉多种解决方案模式，避免了单一解决方案方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;全局数据关联是机器人在不同时间或不同机器人看到的环境中操作的基本前提。重复或对称数据为现有方法带来了重大挑战，这些方法通常依赖于最大似然估计或最大共识来产生一组关联。然而，在模糊场景中，全局数据关联问题的解决方案分布通常是高度多模态的，这类单一解决方案的方法经常失败。在这项工作中，我们引入了一个利用近似贝叶斯推理的数据关联框架，以捕捉数据关联问题的多种解决方案模式，从而避免在模糊情况下过早承诺单一解决方案。我们的方法将假设解决方案表示为粒子，这些粒子根据确定性或随机更新规则演化，以覆盖底层解决方案分布的模式。此外，我们展示了我们的方法可以整合数据关联公式施加的优化约束，并直接受益于GPU并行优化。在高度模糊数据的广泛模拟和真实世界实验中，我们的方法在配准点云或对象地图时正确估计了变换分布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决全局数据关联问题中的多模态分布估计问题。当机器人面对重复或对称结构的环境（如楼梯、排列整齐的树木、圆形房间等）时，传统方法往往只能找到单一的最优解，而实际上可能存在多个合理的解。这个问题很重要，因为对称或重复结构在人类环境中很常见，机器人需要在这样的环境中准确定位和构建地图；传统单一解方法在模糊场景下容易失败，可能导致机器人做出错误决策（如楼梯案例中机器人可能认为自己实际位置的两层之上）；正确估计解的分布可以帮助机器人理解定位的不确定性，做出更鲁棒的决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将数据关联问题从传统的优化问题重新表述为概率推理问题，借鉴了两种成熟的贝叶斯推理框架：Stein变分梯度下降(SVGD)和朗之万动力学(Langevin dynamics)。他们受到CLIPPER算法的启发，但扩展了它以处理多模态解；借鉴了Stein ICP和Bayesian ICP将贝叶斯推理应用于点云配准的思想；参考了多假设跟踪(MHT)和概率数据关联(PDA)等方法，但指出了它们在处理全局模糊性时的局限性。作者设计算法时考虑了使用粒子系统表示多个可能的解，设计特定更新规则探索解空间，确保计算效率以利用GPU并行计算，并添加正则化技术防止粒子退化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将数据关联问题视为概率推理问题而非传统优化问题，假设解遵循可能有多个峰值（多模态）的概率分布，通过近似贝叶斯推理估计完整分布而非单一最优解。整体流程：1)初始化粒子集合，每个粒子代表一个可能解；2)将问题转化为从特定概率分布中采样；3)更新粒子使其向高概率区域移动；4)从粒子中提取解。Stein CLIPPER使用同伦方法逐渐增加参数d，计算确定性更新方向；Langevin CLIPPER直接设置d=n，添加随机噪声进行更新。两者都使用AdaGrad调整学习率，投影粒子回可行域，并利用GPU并行计算提高效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出多模态分布估计框架，估计完整解分布而非单一最优解；2)将近似贝叶斯推理技术应用于数据关联问题；3)提出两种互补算法（Stein CLIPPER和Langevin CLIPPER）处理不同类型分布；4)设计高效实现，利用GPU并行计算；5)提供理论保证，证明解模式对应于一致性图中的最大团。与之前工作的不同：传统单一解方法（如ICP、RANSAC）在模糊场景下失败；多假设跟踪(MHT)计算成本高昂；概率数据关联(PDA)难以处理全局模糊性；Bayesian ICP和Stein ICP主要应用于点云配准而非通用数据关联；原始CLIPPER只能找到单一最优解。本文方法更通用，能处理多种数据类型，在处理高度峰化和均匀分布时表现更好。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将近似贝叶斯推理技术应用于全局数据关联问题，提出了一种能够估计多模态解分布的新方法，使机器人在面对对称或重复结构的环境时能够更鲁棒地进行定位和地图构建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global data association is an essential prerequisite for robot operation inenvironments seen at different times or by different robots. Repetitive orsymmetric data creates significant challenges for existing methods, whichtypically rely on maximum likelihood estimation or maximum consensus to producea single set of associations. However, in ambiguous scenarios, the distributionof solutions to global data association problems is often highly multimodal,and such single-solution approaches frequently fail. In this work, we introducea data association framework that leverages approximate Bayesian inference tocapture multiple solution modes to the data association problem, therebyavoiding premature commitment to a single solution under ambiguity. Ourapproach represents hypothetical solutions as particles that evolve accordingto a deterministic or randomized update rule to cover the modes of theunderlying solution distribution. Furthermore, we show that our method canincorporate optimization constraints imposed by the data associationformulation and directly benefit from GPU-parallelized optimization. Extensivesimulated and real-world experiments with highly ambiguous data show that ourmethod correctly estimates the distribution over transformations whenregistering point clouds or object maps.</description>
      <author>example@mail.com (Yixuan Jia, Mason B. Peterson, Qingyuan Li, Yulun Tian, Jonathan P. How)</author>
      <guid isPermaLink="false">2509.15565v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>STARC: See-Through-Wall Augmented Reality Framework for Human-Robot Collaboration in Emergency Response</title>
      <link>http://arxiv.org/abs/2509.15507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STARC是一个用于人机协作的增强现实框架，通过融合移动机器人映射和救援人员携带的激光雷达传感，在紧急救援任务中实现'透视'效果，帮助救援人员识别被遮挡的危险和受害者。&lt;h4&gt;背景&lt;/h4&gt;在紧急救援任务中，救援人员需要在被遮挡的室内环境中导航，这些遮挡物阻挡了直接视线，隐藏了危及生命的危险和需要救援的受害者。&lt;h4&gt;目的&lt;/h4&gt;开发一个增强现实框架，为救援人员提供实时可视化隐藏人员和危险的能力，提高态势感知并降低操作风险。&lt;h4&gt;方法&lt;/h4&gt;地面机器人使用激光雷达惯性里程计进行大面积探索和3D人体检测，救援人员的头盔或手持激光雷达通过相对姿态估计与机器人全局地图配准，实现跨激光雷达对齐，将检测到的人和点云以低延迟渲染在AR中投射到救援人员视野。&lt;h4&gt;主要发现&lt;/h4&gt;STARC系统能够提供隐藏人员和危险的实时可视化，提高救援人员的态势感知能力并降低操作风险。&lt;h4&gt;结论&lt;/h4&gt;模拟、实验室设置和战术实地试验证实了STARC系统姿态对齐的鲁棒性、检测的可靠性和叠加的稳定性，表明该系统在消防、救灾和其他安全关键操作中具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在紧急救援任务中，救援人员必须在被遮挡的室内环境中导航，这些遮挡物阻挡了直接视线，隐藏了危及生命的危险和需要救援的受害者。我们提出了STARC，这是一个用于人机协作的透视AR框架，它融合了移动机器人映射和救援人员携带的激光雷达传感。运行激光雷达惯性里程计的地面机器人进行大面积探索和3D人体检测，而救援人员头盔或手持的激光雷达通过相对姿态估计与机器人的全局地图配准。这种跨激光雷达对齐使得检测到的人和他们的点云能够以低延迟渲染在AR中，并投射到救援人员的视野中。通过提供隐藏人员和危险的实时可视化，STARC提高了态势感知能力并降低了操作风险。在模拟、实验室设置和战术实地试验中，实验证实了姿态对齐的鲁棒性、检测的可靠性和叠加的稳定性，强调了我们的系统在消防、救灾和其他安全关键操作中的潜力。代码和设计将在接受后开源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决应急响应任务中视线被遮挡的问题，使救援人员能够'看穿'墙壁和其他障碍物，发现隐藏的危险和需要救援的受害者。这个问题在现实中非常重要，因为在火灾、建筑倒塌、人质环境等紧急情况下，救援人员需要在视线受阻的环境中工作，面临极大风险。现有方法如远程操作机器人只能提供2D视频流，缺乏几何上下文；无线或雷达穿墙方法在不同环境中鲁棒性差；AR/VR接口无法提供实时遮挡感知定位；依赖重型模型的AI不适合时间关键任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在分析现有工作基础上进行了创新：分析了机器人安全关键侦察、LiDAR-惯性里程计和跨视图配准、人类检测和人机交互等领域的工作；设计采用双LIO架构，机器人运行FAST-LIO2维护全局地图，FPV设备运行独立LIO；通过一次性跨LiDAR配准实现3D人类检测投影到AR视图；借鉴了FAST-LIO2进行状态估计、ICP/NDT进行配准、PointPillars进行检测等现有技术，但将它们集成到一个新框架中，专注于验证核心概念。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过融合移动机器人地图与救援人员携带的LiDAR传感，创建'透视墙'的增强现实框架，使救援人员在视线被遮挡时能看到隐藏的人和危险。整体流程包括：1)系统初始化：建立全局时间参考、定义世界坐标系、双LIO启动、跨LiDAR配准；2)机器人状态估计：运行FAST-LIO2维护地图、进行人类检测、定期姿态校正；3)人类点云检测和AR投影：从点云检测人类、投影到FPV视角、在AR中渲染这些点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)操作员-机器人AR框架：首次融合机器人地图与操作员LiDAR传感，实现透视AR叠加；2)任务驱动的跨视图集成：将跨LiDAR配准作为锚定操作员视角的手段；3)跨任务场景验证：建立多阶段评估平台证明系统可行性。与之前工作不同：STARC强调语义而非仅几何、实现了机器人地图与操作员传感器的对齐、提供实时融合的3D提示、首次结合操作员LiDAR、机器人全局映射和跨LiDAR配准的AR可视化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; STARC是一种创新的增强现实框架，通过融合机器人地图与救援人员携带的LiDAR传感，实现了在视线被遮挡的安全关键环境中实时'透视墙'感知，显著提高了救援人员的态势感知能力并降低了操作风险。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In emergency response missions, first responders must navigate clutteredindoor environments where occlusions block direct line-of-sight, concealingboth life-threatening hazards and victims in need of rescue. We present STARC,a see-through AR framework for human-robot collaboration that fusesmobile-robot mapping with responder-mounted LiDAR sensing. A ground robotrunning LiDAR-inertial odometry performs large-area exploration and 3D humandetection, while helmet- or handheld-mounted LiDAR on the responder isregistered to the robot's global map via relative pose estimation. Thiscross-LiDAR alignment enables consistent first-person projection of detectedhumans and their point clouds - rendered in AR with low latency - into theresponder's view. By providing real-time visualization of hidden occupants andhazards, STARC enhances situational awareness and reduces operator risk.Experiments in simulation, lab setups, and tactical field trials confirm robustpose alignment, reliable detections, and stable overlays, underscoring thepotential of our system for fire-fighting, disaster relief, and othersafety-critical operations. Code and design will be open-sourced uponacceptance.</description>
      <author>example@mail.com (Shenghai Yuan, Weixiang Guo, Tianxin Hu, Yu Yang, Jinyu Chen, Rui Qian, Zhongyuan Liu, Lihua Xie)</author>
      <guid isPermaLink="false">2509.15507v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Machine-Learning Potentials for Efficient Simulations of Anisotropic Colloids</title>
      <link>http://arxiv.org/abs/2509.15504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入并评估了基于描述符和端到端模型来预测非球形胶体粒子相互作用能量和力的方法，发现神经进化势(NEP)在准确性和计算效率间达到最佳平衡，能够准确模拟各种复杂形状粒子的行为，并实现显著的计算加速。&lt;h4&gt;背景&lt;/h4&gt;模拟非球形胶体粒子之间的相互作用具有计算挑战性，因为力和能量对其几何形状有复杂的依赖关系。&lt;h4&gt;目的&lt;/h4&gt;引入并评估基于描述符和端到端模型来预测相互作用能量和力的方法，以解决非球形胶体粒子模拟的计算挑战。&lt;h4&gt;方法&lt;/h4&gt;比较了多种描述符与回归模型的组合，包括Behler-Parinello描述符、原子位置平滑重叠(SOAP)和神经进化势(NEP)，以及多种端到端模型，如SchNet、DimeNet和DimeNet++。NEP使用点云表示各向异性刚体间的相互作用。&lt;h4&gt;主要发现&lt;/h4&gt;神经进化势(NEP)在准确性和计算效率间提供了最佳平衡。使用NEP的分子动力学模拟准确重现了立方体、四面体、五角双锥和扭曲圆柱体等多种粒子形状的结构特性，同时比其他方法实现了高达一个数量级的计算加速。该方法还能灵活扩展到具有不同表面相互作用的复杂形状。&lt;h4&gt;结论&lt;/h4&gt;该方法能够实现复杂胶体系统的可扩展模拟，并可能有助于未来对形状依赖相互作用和相行为的高效研究。&lt;h4&gt;翻译&lt;/h4&gt;模拟非球形胶体粒子之间的相互作用在计算上具有挑战性，因为力和能量对其几何形状存在复杂的依赖关系。我们引入并评估了基于描述符和端到端模型来预测相互作用能量和力的方法。然后，我们比较了各种描述符与不同回归模型的组合，如Behler-Parinello描述符、原子位置平滑重叠和神经进化势，以及多种端到端模型，即SchNet、DimeNet和DimeNet++。在这些方法中，神经进化势(NEP)在准确性和计算效率之间提供了最佳平衡。NEP最初是为原子系统开发的，它使用点云表示各向异性刚体之间的相互作用，能够表示任意形状。使用NEP的分子动力学模拟准确重现了多种粒子形状的结构特性，包括立方体、四面体、五角双锥和扭曲圆柱体，同时比其他方法实现了高达一个数量级的加速。此外，我们展示了该方法扩展到具有不同表面相互作用的多个面形状是直接的。我们使用了一个没有任何点群对称性的扭曲圆柱体，来证明NEP的灵活性和准确性。我们的方法能够实现复杂胶体系统的可扩展模拟，并可能有助于未来对形状依赖相互作用和相行为的高效研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating interactions between non-spherical colloidal particles iscomputationally challenging due to the complex dependency of forces andenergies on their geometry. We introduce and evaluate both descriptor-based andend-to-end models for predicting interaction energies and forces. Then, wecompare various descriptors coupled with different regression models, likeBehler-Parinello descriptors, Smooth Overlap of Atomic Positions, andneuroevolution potential, as well as multiple end-to-end models, namely SchNet,DimeNet, and DimeNet++. Among these, the neuroevolution potential (NEP) offersan optimal balance between accuracy and computational efficiency. NEP,originally developed for atomistic systems, represents interactions betweenrigid anisotropic bodies using point clouds, which enables the representationof any arbitrary shape. Molecular dynamics simulations using NEP, accuratelyreproduced structural properties across diverse particle shapes includingcubes, tetrahedra, pentagonal bipyramids, and twisted cylinders, whileachieving roughly up to an order-of-magnitude speedup over other methods.Additionally, we show that the extension of the method to multi-face shapeswith different interactions on their surface is straightforward. We used atwisted cylinder, which lacked any point group symmetry, to demonstrate theflexibility and accuracy of NEP. Our approach enables scalable simulations ofcomplex colloidal systems and can potentially help to facilitate efficientstudies on shape dependent interactions and phase behavior in the future.</description>
      <author>example@mail.com (B. Rusen Argun, Antonia Statt)</author>
      <guid isPermaLink="false">2509.15504v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction</title>
      <link>http://arxiv.org/abs/2509.15459v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为CAGE（连续感知边缘）的鲁棒网络框架，用于从点云密度图中直接重建矢量平面图。该方法采用以边缘为中心的表示方式，能够生成连贯、拓扑有效的房间边界，并在多个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;传统的基于角落的多边形表示方法对噪声和不完整观测非常敏感，常导致碎片化或不合理的布局。最近的线分组方法虽然利用结构提示提高了鲁棒性，但在恢复精细几何细节方面仍有困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接从点云密度图中重建矢量平面图的鲁棒框架，解决传统方法对噪声敏感和无法恢复精细几何细节的问题，确保生成连贯、拓扑有效的房间边界。&lt;h4&gt;方法&lt;/h4&gt;提出一种原生的以边缘为中心的公式，将每个墙段建模为有向的、几何连续的边缘。开发了一个双查询Transformer解码器，在去噪框架中集成扰动查询和潜在查询，以稳定优化并加速收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在Structured3D和SceneCAD数据集上的实验表明，CAGE取得了最先进的性能，房间F1分数达到99.1%，角落为91.7%，角度为89.3%。该方法还表现出强大的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CAGE网络通过以边缘为中心的表示方法和创新的解码器设计，显著提高了平面图重建的鲁棒性和准确性，能够生成连贯、拓扑有效的房间边界，并在多个评估指标上超越了现有方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了CAGE（连续感知边缘）网络，一个用于从点云密度图中直接重建矢量平面图的鲁棒框架。传统的基于角落的多边形表示对噪声和不完整观测非常敏感，常常导致碎片化或不合理的布局。最近的线分组方法利用结构提示提高鲁棒性，但在恢复精细几何细节方面仍然困难。为解决这些限制，我们提出了一种原生的以边缘为中心的公式，将每个墙段建模为有向的、几何连续的边缘。这种表示能够推断出连贯的平面图结构，确保无缝、拓扑有效的房间边界，同时提高鲁棒性并减少伪影。为此设计，我们开发了一个双查询Transformer解码器，在去噪框架中集成扰动查询和潜在查询，这不仅稳定了优化，还加速了收敛。在Structured3D和SceneCAD上的广泛实验表明，CAGE取得了最先进的性能，房间F1分数为99.1%，角落为91.7%，角度为89.3%。该方法还表现出强大的跨数据集泛化能力，证明了我们架构创新的有效性。代码和预训练模型将在接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从点云密度图中重建矢量平面图的问题。这个问题很重要，因为矢量平面图是室内结构的紧凑、可编辑2D表示，与CAD工具和BIM无缝集成，支持精确几何推理，对建筑生命周期管理、AR/VR模拟和自主导航等下游应用至关重要。传统方法对噪声和不完整数据敏感，难以产生连贯、拓扑有效的布局。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：基于角点的方法对噪声敏感，一个缺失角点会扭曲整个布局；SLIBO-Net依赖曼哈顿世界假设；FRI-Net会导致精细结构过度平滑。基于此，作者设计了基于边缘的表示方法，借鉴了DETR和DN-DETR的transformer架构，创造性地引入双查询机制（扰动查询和潜在查询），在去噪框架内稳定训练并加速收敛。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用基于边缘的表示方法，将每面墙建模为有向的、几何连续的边缘，并设计双查询transformer解码器。整体流程：1)点云投影为2D密度图；2)通过卷积骨干网络提取多尺度特征；3)特征展平并添加位置编码；4)transformer编码器处理特征；5)双查询解码器迭代细化边缘预测；6)通过边缘相交转换为闭合多边形。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)连续感知边缘表示，增强对不完整和噪声数据的鲁棒性；2)双查询transformer解码器，稳定训练并加速收敛；3)强大的跨数据集泛化能力。相比之前工作：不依赖精确角点定位（不同于HEAT、RoomFormer）；不依赖曼哈顿世界假设（不同于SLIBO-Net）；避免精细结构过度平滑（不同于FRI-Net）；更好地平衡全局结构和局部精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CAGE网络通过连续感知的边缘表示和双查询transformer解码器，实现了从点云密度图中鲁棒且精确的矢量平面图重建，显著提高了在噪声和不完整数据条件下的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present \textbf{CAGE} (\textit{Continuity-Aware edGE}) network, a\textcolor{red}{robust} framework for reconstructing vector floorplans directlyfrom point-cloud density maps. Traditional corner-based polygon representationsare highly sensitive to noise and incomplete observations, often resulting infragmented or implausible layouts. Recent line grouping methods leveragestructural cues to improve robustness but still struggle to recover finegeometric details. To address these limitations, we propose a \textit{native}edge-centric formulation, modeling each wall segment as a directed,geometrically continuous edge. This representation enables inference ofcoherent floorplan structures, ensuring watertight, topologically valid roomboundaries while improving robustness and reducing artifacts. Towards thisdesign, we develop a dual-query transformer decoder that integrates perturbedand latent queries within a denoising framework, which not only stabilizesoptimization but also accelerates convergence. Extensive experiments onStructured3D and SceneCAD show that \textbf{CAGE} achieves state-of-the-artperformance, with F1 scores of 99.1\% (rooms), 91.7\% (corners), and 89.3\%(angles). The method also demonstrates strong cross-dataset generalization,underscoring the efficacy of our architectural innovations. Code and pretrainedmodels will be released upon acceptance.</description>
      <author>example@mail.com (Yiyi Liu, Chunyang Liu, Weiqin Jiao, Bojian Wu, Fashuai Li, Biao Xiong)</author>
      <guid isPermaLink="false">2509.15459v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Measurement and Potential Field-Based Patient Modeling for Model-Mediated Tele-ultrasound</title>
      <link>http://arxiv.org/abs/2509.15325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进的远程超声操作方法，通过使用测量的位置和力来更新患者的内部势场模型，以提高远程超声诊断的准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;远程超声操作可以改善偏远社区的医学诊断成像获取。准确的力反馈对于操作者（超声医师）应用适当的探头接触力以优化超声图像质量非常重要。然而，通信中的大延迟使得直接力反馈不切实际。&lt;h4&gt;目的&lt;/h4&gt;扩展先前基于点云的模型中介远程操作和内部势场模型研究，引入一种使用测量的位置和力更新患者内部势场模型的方法，以实现更透明的模型中介远程超声操作。&lt;h4&gt;方法&lt;/h4&gt;首先生成患者表面的点云模型，并以紧凑的数据结构传输给超声医师。然后将其转换为静态体素化体积，每个体素包含势场值。这些值决定力和力矩，基于体素化体积与超声换能器的点壳模型之间的重叠来渲染。使用结合空间拉普拉斯算子和测量力的凸二次方程求解势场。在3名志愿者患者上评估了渲染力的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;与仅使用拉普拉斯方程相比，将测量力添加到模型中使力大小误差平均减少了7.23牛顿，力矢量角度误差平均减少了9.37度。&lt;h4&gt;结论&lt;/h4&gt;通过整合测量的位置和力来更新内部势场模型的方法，显著提高了远程超声操作中力反馈的准确性，有助于优化远程超声诊断的质量。&lt;h4&gt;翻译&lt;/h4&gt;远程超声操作可以提高偏远社区获取诊断医学成像的能力。准确的力反馈对于使超声医师能够应用适当的探头接触力以优化超声图像质量非常重要。然而，通信中的大延迟使得直接力反馈不切实际。先前的研究调查了使用基于点云的模型中介远程操作和内部势场模型来估计接触力和力矩。我们通过引入一种使用测量的位置和力更新患者内部势场模型的方法来扩展这一研究，以实现更透明的模型中介远程超声操作。我们首先生成患者表面的点云模型，并以紧凑的数据结构将其传输给超声医师。这被转换为静态体素化体积，其中每个体素包含势场值。这些值确定力和力矩，这些力和力矩基于体素化体积与超声换能器的点壳模型之间的重叠来渲染。我们使用结合空间拉普拉斯算子和测量力的凸二次方程来求解势场。通过计算渲染力的准确性，在3名志愿者患者上对此进行了评估。结果表明，与仅使用拉普拉斯方程相比，将测量力添加到模型中使力大小误差平均减少了7.23牛顿，力矢量角度误差平均减少了9.37度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决远程超声检查中由于通信延迟导致的力反馈不准确问题。在远程超声中，医生需要感受到探头与患者身体的接触力来优化图像质量，但网络延迟使得直接力反馈变得困难。这个问题很重要，因为它关系到远程医疗服务的质量和可及性，特别是对医疗资源匮乏的偏远地区患者而言，能让他们在当地获得专业诊断服务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了远程超声中力反馈的挑战，考察了现有的模型中介遥操作方法。他们借鉴了点云模型和势场模型的思想，特别是Voxmap Pointshell算法和压力场接触方法。作者发现这些方法虽能渲染力反馈，但不能很好地模拟患者阻抗特性。因此，他们设计了一种结合拉普拉斯方程和实际测量力的方法，通过将测量的位置和力数据整合到势场模型中，提高模型的准确性和透明度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合物理模型和实际测量数据创建更准确的虚拟患者模型，用于远程超声中的力反馈。具体流程：1)用深度相机获取患者躯干点云数据；2)转换为圆柱坐标系结构化表示；3)创建静态体素化体积，每个体素含势场值；4)用拉普拉斯方程初始化势场；5)通过实际测量的力和位置数据更新势场；6)当探头与模型交互时计算力和力矩；7)将计算结果反馈给远程医生。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)引入用测量的位置和力更新患者内部势场模型的方法；2)结合拉普拉斯方程和实际测量数据创建更准确的阻抗模型；3)实现更透明的模型中介远程超声系统。相比之前工作：1)之前方法主要依赖预定义物理模型，未结合实际测量数据；2)之前阻抗估计多适用于单自由度系统，本文方法可处理复杂三维情况；3)本文能捕捉患者身体不同部位阻抗变化，而之前模型通常假设阻抗均匀。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合物理模型和实际测量数据的远程超声患者建模方法，通过更新内部势场模型显著提高了力反馈准确性，为偏远地区提供了更高质量的远程超声诊断服务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Teleoperated ultrasound can improve diagnostic medical imaging access forremote communities. Having accurate force feedback is important for enablingsonographers to apply the appropriate probe contact force to optimizeultrasound image quality. However, large time delays in communication makedirect force feedback impractical. Prior work investigated using pointcloud-based model-mediated teleoperation and internal potential field models toestimate contact forces and torques. We expand on this by introducing a methodto update the internal potential field model of the patient with measuredpositions and forces for more transparent model-mediated tele-ultrasound. Wefirst generate a point cloud model of the patient's surface and transmit thisto the sonographer in a compact data structure. This is converted to a staticvoxelized volume where each voxel contains a potential field value. Thesevalues determine the forces and torques, which are rendered based on overlapbetween the voxelized volume and a point shell model of the ultrasoundtransducer. We solve for the potential field using a convex quadratic thatcombines the spatial Laplace operator with measured forces. This was evaluatedon volunteer patients ($n=3$) by computing the accuracy of rendered forces.Results showed the addition of measured forces to the model reduced the forcemagnitude error by an average of 7.23 N and force vector angle error by anaverage of 9.37$^{\circ}$ compared to using only Laplace's equation.</description>
      <author>example@mail.com (Ryan S. Yeung, David G. Black, Septimiu E. Salcudean)</author>
      <guid isPermaLink="false">2509.15325v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2509.15123v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 Spotlight&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ROS-Cam的新方法，用于在动态场景中更准确、更高效地优化相机参数，仅使用单个RGB视频作为监督。&lt;h4&gt;背景&lt;/h4&gt;COLMAP是静态场景中相机参数优化的主要方法，但存在运行时间长且依赖真实运动掩码的问题。许多改进方法需要先验知识，但这些在普通RGB视频中通常不可用。&lt;h4&gt;目的&lt;/h4&gt;提出一种在动态场景中更准确、更高效的相机参数优化方法，仅使用单个RGB视频作为监督。&lt;h4&gt;方法&lt;/h4&gt;ROS-Cam方法包含三个关键组件：(1)基于块的运动跟踪滤波器，建立RGB视频间稳健且最大稀疏的铰链式关系；(2)异常感知联合优化，通过自适应降低运动异常权重来高效优化相机参数，不依赖运动先验；(3)两阶段优化策略，通过在Softplus限制和凸最小值之间权衡提高稳定性和优化速度。&lt;h4&gt;主要发现&lt;/h4&gt;在4个真实数据集(NeRF-DS, DAVIS, iPhone, 和 TUM-dynamics)和1个合成数据集(MPI-Sintel)上的实验表明，ROS-Cam方法仅使用单个RGB视频作为监督，能够更高效、更准确地估计相机参数。通过将相机估计输入4D重建方法评估结果3D场景、渲染的2D RGB和深度图，进一步验证了准确性。&lt;h4&gt;结论&lt;/h4&gt;ROS-Cam方法在动态场景中能够更准确、更高效地优化相机参数，仅需单个RGB视频作为监督，无需额外的先验知识。&lt;h4&gt;翻译&lt;/h4&gt;虽然COLMAP长期以来一直是静态场景中相机参数优化的主要方法，但它受限于较长的运行时间以及对真实运动掩码的依赖，无法应用于动态场景。许多尝试通过融入更多先验作为监督来改进它，如真实焦距、运动掩码、3D点云、相机姿态和度量深度，但这些在普通拍摄的RGB视频中通常不可用。在本文中，我们提出了一种新颖的方法，用于在动态场景中进行更准确、更高效的相机参数优化，仅由单个RGB视频监督，称为ROS-Cam。我们的方法包含三个关键组件：(1)基于块的运动跟踪滤波器，在RGB视频间建立稳健且最大稀疏的铰链式关系；(2)异常感知联合优化，通过自适应降低运动异常的权重来高效优化相机参数，不依赖运动先验；(3)两阶段优化策略，通过在损失函数中的Softplus限制和凸最小值之间权衡来提高稳定性和优化速度。我们通过可视化和数值方式评估了相机估计。为进一步验证准确性，我们将相机估计输入4D重建方法并评估生成的3D场景、渲染的2D RGB和深度图。我们在4个真实数据集(NeRF-DS, DAVIS, iPhone, 和 TUM-dynamics)和1个合成数据集(MPI-Sintel)上进行了实验，证明我们的方法仅使用单个RGB视频作为监督，能够更高效、更准确地估计相机参数。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在动态场景中仅使用RGB视频（无需额外监督信息如运动掩码、焦距、3D点云等）进行相机参数优化的问题。这个问题在现实中很重要，因为大多数普通用户设备只能提供RGB视频，而现有方法要么需要额外的先验信息（通常不可用），要么在动态场景中运行时间长且性能不佳。准确估计相机参数对动态场景重建、新视角合成等计算机视觉应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法（如COLMAP）在动态场景中的局限性，包括运行时间长和对运动掩码的依赖。他们注意到大多数方法需要额外的先验监督，而这些在日常RGB视频中不可用。作者设计了三个关键组件：基于块的运动跟踪过滤器、离群感知联合优化和两阶段优化策略。作者借鉴了点跟踪模型（如CoTracker）来提取鲁棒的跟踪轨迹作为伪监督，并使用Cauchy分布来建模不确定性，这优于高斯分布，因为它能更好地处理重尾分布。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是仅使用RGB视频作为监督，通过提取鲁棒的跟踪轨迹作为伪监督，然后使用离群感知的联合优化方法来估计相机参数。整体实现流程包括：1) 基于块的运动跟踪过滤器（纹理、梯度、可见性和分布过滤器提取鲁棒轨迹）；2) 离群感知联合优化（计算平均累积投影误差，使用Cauchy损失函数降低运动离群点影响，联合优化校准点、焦距、旋转、平移和不确定性参数）；3) 两阶段优化策略（第一阶段快速收敛，第二阶段使用第一阶段的误差初始化不确定性参数，稳定收敛）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 仅RGB监督，无需额外先验信息；2) 基于块的运动跟踪过滤器，提取鲁棒轨迹作为伪监督；3) 离群感知联合优化，使用Cauchy分布建模不确定性；4) 两阶段优化策略提高稳定性和效率。相比之前的工作，不同之处在于：不需要额外的先验监督（如运动掩码、焦距、深度等）；通过学习不确定性参数自适应处理运动离群点，而非依赖真实运动掩码；运行时间显著短于现有方法，且随着视频长度增加优势更明显；在各种动态场景中表现更鲁棒，包括低视差视频和快速运动物体。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种名为ROS-Cam的新方法，仅使用RGB视频作为监督，通过基于块的运动跟踪过滤、离群感知联合优化和两阶段优化策略，实现了动态场景中相机参数的准确、高效估计，显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although COLMAP has long remained the predominant method for camera parameteroptimization in static scenes, it is constrained by its lengthy runtime andreliance on ground truth (GT) motion masks for application to dynamic scenes.Many efforts attempted to improve it by incorporating more priors assupervision such as GT focal length, motion masks, 3D point clouds, cameraposes, and metric depth, which, however, are typically unavailable in casuallycaptured RGB videos. In this paper, we propose a novel method for more accurateand efficient camera parameter optimization in dynamic scenes solely supervisedby a single RGB video, dubbed ROS-Cam. Our method consists of three keycomponents: (1) Patch-wise Tracking Filters, to establish robust and maximallysparse hinge-like relations across the RGB video. (2) Outlier-aware JointOptimization, for efficient camera parameter optimization by adaptivedown-weighting of moving outliers, without reliance on motion priors. (3) ATwo-stage Optimization Strategy, to enhance stability and optimization speed bya trade-off between the Softplus limits and convex minima in losses. Wevisually and numerically evaluate our camera estimates. To further validateaccuracy, we feed the camera estimates into a 4D reconstruction method andassess the resulting 3D scenes, and rendered 2D RGB and depth maps. We performexperiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics)and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimatescamera parameters more efficiently and accurately with a single RGB video asthe only supervision.</description>
      <author>example@mail.com (Fang Li, Hao Zhang, Narendra Ahuja)</author>
      <guid isPermaLink="false">2509.15123v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>GenCAD-3D: CAD Program Generation using Multimodal Latent Space Alignment and Synthetic Dataset Balancing</title>
      <link>http://arxiv.org/abs/2509.15246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 figures, 15 pages. Accepted and soon published in the ASME Journal  of Mechanical Design&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GenCAD-3D多模态生成框架和SynthBal合成数据增强策略，解决了从非参数化数据生成CAD程序的挑战，显著提高了重建准确性并减少了无效CAD模型的生成。&lt;h4&gt;背景&lt;/h4&gt;CAD程序作为参数化命令序列，对工程设计的精确性和效率至关重要。但从点云和网格等非参数化数据生成这些程序仍具挑战性，通常需要大量人工干预。当前深度生成模型受限于不平衡且不足的数据集，特别是缺乏复杂CAD程序的表示。&lt;h4&gt;目的&lt;/h4&gt;开发自动化CAD生成方法，解决数据集不平衡和不足的问题，提高复杂CAD几何体的表示和生成质量。&lt;h4&gt;方法&lt;/h4&gt;提出GenCAD-3D多模态生成框架，利用对比学习对齐CAD和几何编码器之间的潜在嵌入，结合潜在扩散模型进行CAD序列生成和检索；同时提出SynthBal合成数据增强策略，专门用于平衡和扩展数据集，增强复杂CAD几何体的表示。&lt;h4&gt;主要发现&lt;/h4&gt;SynthBal显著提高了重建准确性，减少了无效CAD模型的生成，在高复杂度几何体上明显优于现有基准，超越了现有的性能标准。&lt;h4&gt;结论&lt;/h4&gt;这些进展对简化逆向工程和增强工程设计自动化具有重要意义，作者计划公开发布数据集、代码以及一组51个3D打印和激光扫描的零件。&lt;h4&gt;翻译&lt;/h4&gt;CAD程序作为参数化命令序列的结构，编译成精确的3D几何体，对准确和高效的工程设计流程至关重要。从点云和网格等非参数化数据生成这些程序仍然是一个关键但具有挑战性的任务，通常需要大量人工干预。当前旨在自动化CAD生成的深度生成模型受到不平衡且不够大的数据集的显著限制，特别是那些缺乏复杂CAD程序表示的数据集。为解决这一问题，我们引入了GenCAD-3D，一个利用对比学习对齐CAD和几何编码器之间潜在嵌入的多模态生成框架，结合潜在扩散模型进行CAD序列生成和检索。此外，我们提出了SynthBal，一种专门设计用于平衡和扩展数据集的合成数据增强策略，显著增强了复杂CAD几何体的表示。我们的实验表明，SynthBal显著提高了重建准确性，减少了无效CAD模型的生成，并在高复杂度几何体上明显优于现有基准。这些进展对简化逆向工程和增强工程设计自动化具有重要意义。我们将在项目网站上公开发布我们的数据集和代码，包括一组51个3D打印和激光扫描的零件。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从3D几何数据（如点云和网格）自动生成参数化CAD程序的问题。这个问题在工程领域非常重要，因为CAD程序提供了精确的参数化控制，是工程设计和制造的基础。在逆向工程中，工程师经常需要将物理对象的3D扫描转换为可编辑的CAD程序，以便修改和制造零件，尤其是在原始供应商已 discontinued 的情况下。当前工具需要大量人工干预，而生成式AI通常输出非参数化形式，难以编辑和精确细化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前CAD生成面临的数据不平衡挑战，特别是复杂CAD程序在数据集中表示不足的问题。他们借鉴了GenCAD架构的条件潜在扩散模型和对比学习在跨模态任务（如CLIP模型）中的成功经验。设计方法包括：1）多模态对比学习对齐不同模态的潜在表示；2）条件扩散模型实现从几何到CAD的生成；3）专门的3D编码器处理不同几何输入；4）SynthBal策略解决数据不平衡。创新之处在于不依赖现有基础模型，独立学习多模态嵌入，并专门针对CAD程序复杂性进行数据平衡。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态潜在空间对齐将不同几何模态与CAD程序表示对齐到共享空间，利用条件扩散模型实现从几何输入到CAD程序的生成，并通过SynthBal策略平衡数据集中不同复杂度CAD程序的表示。整体流程包括：1）使用因果变换器自编码器学习CAD程序潜在表示；2）为点云和网格设计专门编码器，通过对比损失对齐几何模态与CAD潜在空间；3）训练条件扩散模型将几何潜在表示映射到CAD程序潜在表示；4）通过SynthBal生成合成数据平衡不同复杂度CAD程序的表示；5）在合成数据上训练自编码器，然后在减少平衡的真实数据上微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）GenCAD-3D框架结合多模态潜在空间对齐和条件扩散模型；2）专门的网格编码器相比点云基线在命令准确性上有15%相对提升；3）SynthBal合成数据策略将无效CAD生成率从3.44%降至0.845%；4）复杂度归一化评估指标确保高复杂度设计得到公平评估；5）发布多模态数据集和编码器促进研究。相比之前工作，GenCAD-3D支持多种3D几何模态输入，专门解决数据不平衡问题，设计了专门的网格编码器，引入了新的评估方法，且不依赖现有基础模型独立学习多模态嵌入。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GenCAD-3D通过多模态潜在空间对齐和合成数据平衡策略，实现了从3D几何数据高效生成复杂CAD程序，显著提高了逆向工程和工程设计自动化的准确性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1115/1.4069276&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CAD programs, structured as parametric sequences of commands that compileinto precise 3D geometries, are fundamental to accurate and efficientengineering design processes. Generating these programs from nonparametric datasuch as point clouds and meshes remains a crucial yet challenging task,typically requiring extensive manual intervention. Current deep generativemodels aimed at automating CAD generation are significantly limited byimbalanced and insufficiently large datasets, particularly those lackingrepresentation for complex CAD programs. To address this, we introduceGenCAD-3D, a multimodal generative framework utilizing contrastive learning foraligning latent embeddings between CAD and geometric encoders, combined withlatent diffusion models for CAD sequence generation and retrieval.Additionally, we present SynthBal, a synthetic data augmentation strategyspecifically designed to balance and expand datasets, notably enhancingrepresentation of complex CAD geometries. Our experiments show that SynthBalsignificantly boosts reconstruction accuracy, reduces the generation of invalidCAD models, and markedly improves performance on high-complexity geometries,surpassing existing benchmarks. These advancements hold substantialimplications for streamlining reverse engineering and enhancing automation inengineering design. We will publicly release our datasets and code, including aset of 51 3D-printed and laser-scanned parts on our project site.</description>
      <author>example@mail.com (Nomi Yu, Md Ferdous Alam, A. John Hart, Faez Ahmed)</author>
      <guid isPermaLink="false">2509.15246v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution</title>
      <link>http://arxiv.org/abs/2509.15781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages,2 figures, ICCV Workshop (MOSEv2 Track of 7th LSVOS  Challenge)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种整合Cutie和SAM2优势的视频对象分割框架SCOPE，通过改进编码器和引入运动预测模块，在第七届LSVOS挑战赛的MOSEv2赛道中获得第三名。&lt;h4&gt;背景&lt;/h4&gt;视频对象分割是一项具有挑战性的任务，在视频编辑和自动驾驶等领域有广泛应用。现有方法Cutie提供强大的基于查询的分割能力，SAM2通过预训练的ViT编码器提供丰富的表示，但两者在特征容量和时间建模方面各有局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一个整合Cutie和SAM2互补优势的框架，解决现有方法在特征容量和时间建模方面的局限性，提高视频对象分割的性能。&lt;h4&gt;方法&lt;/h4&gt;用SAM2的ViT编码器替换Cutie的编码器，引入运动预测模块以实现时间稳定性，并采用集成策略结合Cutie、SAM2和提出的变体，创建名为SCOPE（SAM2-CUTIE Object Prediction Ensemble）的最终模型。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在第七届LSVOS挑战赛的MOSEv2赛道中获得第三名，证明了丰富的特征表示和运动预测对鲁棒的视频对象分割的有效性。&lt;h4&gt;结论&lt;/h4&gt;集成不同方法的互补优势可以改进视频对象分割性能，特征表示增强和时间建模对实现稳定的视频对象分割至关重要。&lt;h4&gt;翻译&lt;/h4&gt;视频对象分割是一项具有挑战性的任务，在视频编辑和自动驾驶等领域有广泛应用。虽然Cutie提供了强大的基于查询的分割能力，SAM2通过预训练的ViT编码器提供了丰富的表示，但两者在特征容量和时间建模方面各有局限性。在本报告中，我们提出了一种通过用SAM2的ViT编码器替换Cutie的编码器并引入运动预测模块来实现时间稳定性的框架，从而整合它们的互补优势。我们进一步采用结合Cutie、SAM2和我们提出的变体的集成策略，在第七届LSVOS挑战赛的MOSEv2赛道中获得第三名。我们将最终模型称为SCOPE（SAM2-CUTIE Object Prediction Ensemble）。这证明了丰富的特征表示和运动预测对鲁棒的视频对象分割的有效性。代码可在https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video object segmentation (VOS) is a challenging task with wide applicationssuch as video editing and autonomous driving. While Cutie provides strongquery-based segmentation and SAM2 offers enriched representations via apretrained ViT encoder, each has limitations in feature capacity and temporalmodeling. In this report, we propose a framework that integrates theircomplementary strengths by replacing the encoder of Cutie with the ViT encoderof SAM2 and introducing a motion prediction module for temporal stability. Wefurther adopt an ensemble strategy combining Cutie, SAM2, and our variant,achieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer toour final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). Thisdemonstrates the effectiveness of enriched feature representation and motionprediction for robust video object segmentation. The code is available athttps://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place.</description>
      <author>example@mail.com (Chang Soo Lim, Joonyoung Moon, Donghyeon Cho)</author>
      <guid isPermaLink="false">2509.15781v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>KoopCast: Trajectory Forecasting via Koopman Operators</title>
      <link>http://arxiv.org/abs/2509.15513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;KoopCast是一个轻量级但高效的轨迹预测模型，适用于一般动态环境&lt;h4&gt;背景&lt;/h4&gt;轨迹预测在一般动态环境中面临非线性动力学挑战&lt;h4&gt;目的&lt;/h4&gt;开发一个既能保持高预测准确性，又具有可解释性和低延迟部署的轨迹预测模型&lt;h4&gt;方法&lt;/h4&gt;采用基于Koopman算子理论的两阶段设计：第一阶段使用概率神经目标估计器预测长期目标；第二阶段使用Koopman算子细化模块将意图和历史信息整合到非线性特征空间实现线性预测&lt;h4&gt;主要发现&lt;/h4&gt;KoopCast模型在多个数据集上验证了三个关键优势：具有竞争力的准确性、基于Koopman谱理论的解释性、低延迟部署&lt;h4&gt;结论&lt;/h4&gt;KoopCast在各种基准测试中提供高预测准确性，同时具有模式级别的可解释性和实际效率&lt;h4&gt;翻译&lt;/h4&gt;我们提出了KoopCast，一个轻量级但高效的轨迹预测模型，适用于一般动态环境。我们的方法利用Koopman算子理论，通过将轨迹提升到高维空间，实现对非线性动力学的线性表示。该框架采用两阶段设计：首先，概率神经目标估计器预测可能的长远目标，确定去向；其次，基于Koopman算子的细化模块将意图和历史信息整合到非线性特征空间，实现线性预测，确定如何去。这种双重结构不仅确保了强大的预测准确性，同时保留了线性算子的有利特性，同时真实地捕获了非线性动力学。因此，我们的模型具有三个主要优势：具有竞争力的准确性、基于Koopman谱理论的解释性、低延迟部署。我们在ETH/UCY、Waymo Open Motion Dataset和nuScenes上验证了这些优势，这些数据集具有丰富的多智能体交互和地图约束的非线性运动。在各种基准测试中，KoopCast始终提供高预测准确性，同时具有模式级别的可解释性和实际效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present KoopCast, a lightweight yet efficient model for trajectoryforecasting in general dynamic environments. Our approach leverages Koopmanoperator theory, which enables a linear representation of nonlinear dynamics bylifting trajectories into a higher-dimensional space. The framework follows atwo-stage design: first, a probabilistic neural goal estimator predictsplausible long-term targets, specifying where to go; second, a Koopmanoperator-based refinement module incorporates intention and history into anonlinear feature space, enabling linear prediction that dictates how to go.This dual structure not only ensures strong predictive accuracy but alsoinherits the favorable properties of linear operators while faithfullycapturing nonlinear dynamics. As a result, our model offers three keyadvantages: (i) competitive accuracy, (ii) interpretability grounded in Koopmanspectral theory, and (iii) low-latency deployment. We validate these benefitson ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature richmulti-agent interactions and map-constrained nonlinear motion. Acrossbenchmarks, KoopCast consistently delivers high predictive accuracy togetherwith mode-level interpretability and practical efficiency.</description>
      <author>example@mail.com (Jungjin Lee, Jaeuk Shin, Gihwan Kim, Joonho Han, Insoon Yang)</author>
      <guid isPermaLink="false">2509.15513v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>The Missing Piece: A Case for Pre-Training in 3D Medical Object Detection</title>
      <link>http://arxiv.org/abs/2509.15947v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模预训练对3D医学目标检测的影响，首次系统性探讨了如何将现有预训练方法集成到最先进的检测架构中，发现基于重建的自监督预训练优于监督预训练，而对比预训练无明显益处。&lt;h4&gt;背景&lt;/h4&gt;大规模预训练有望推进3D医学目标检测，这是准确计算机辅助诊断的关键组成部分。然而，与分割相比，3D目标检测的预训练研究仍然不足，现有方法主要依赖2D医学数据或自然图像预训练，未能充分利用3D体积信息。&lt;h4&gt;目的&lt;/h4&gt;进行首次系统性研究，探讨如何将现有预训练方法集成到最先进的检测架构中，涵盖CNN和Transformer两种架构。&lt;h4&gt;方法&lt;/h4&gt;将现有预训练方法集成到CNN和Transformer架构中，评估了基于重建的自监督预训练、监督预训练和对比预训练方法对3D医学目标检测性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;预训练在各种任务和数据集上一致提高了检测性能；基于重建的自监督预训练优于监督预训练；对比预训练对3D医学目标检测没有明显益处。&lt;h4&gt;结论&lt;/h4&gt;预训练可以有效提升3D医学目标检测性能，基于重建的自监督方法是3D医学目标检测的有效预训练策略。&lt;h4&gt;翻译&lt;/h4&gt;大规模预训练有望推进3D医学目标检测，这是准确计算机辅助诊断的关键组成部分。然而，与分割相比，3D目标检测的预训练研究仍然不足，而分割领域的预训练已经显示出显著优势。现有的3D目标检测预训练方法依赖于2D医学数据或自然图像预训练，未能充分利用3D体积信息。在这项工作中，我们首次进行了系统性研究，探讨了如何将现有预训练方法集成到最先进的检测架构中，涵盖了CNN和Transformer两种架构。我们的结果表明，预训练在各种任务和数据集上一致提高了检测性能。值得注意的是，基于重建的自监督预训练优于监督预训练，而对比预训练对3D医学目标检测没有明显益处。我们的代码已在以下公开：https://github.com/MIC-DKFZ/nnDetection-finetuning。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D医学目标检测领域缺乏有效预训练方法的问题。这个问题很重要，因为3D医学目标检测是计算机辅助诊断的关键组成部分，能帮助医生准确定位临床相关物体，特别是在高风险场景中，完全遗漏物体的后果比分割不准确更严重。目前，预训练在分割任务中已显示出优势，但在3D目标检测领域研究不足，限制了模型在小数据集上的性能和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D医学目标检测预训练研究的空白，并借鉴了分割任务中已成功的预训练方法。他们采用了MultiTalent框架进行监督预训练，并使用了四种自监督学习方法（MAE、SparkMAE、VoCo和Models Genesis）。作者设计了全面的实验，评估不同预训练策略对两种检测架构（Retina U-Net和Deformable DETR）的影响，并确保预训练数据与下游任务数据不重叠，避免数据泄露。这种方法结合了现有研究成果并进行了系统性扩展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统性地研究不同预训练策略对3D医学目标检测的影响，找出最优方法，并建立连接检测、分割和自监督学习的统一框架。整体流程包括：1) 预训练阶段 - 使用MultiTalent进行监督预训练，使用CT-RATE和ABCD数据集进行自监督预训练；2) 模型架构选择 - 使用Retina U-Net和Deformable DETR作为检测架构，ResEncL和Retina U-Net作为预训练架构；3) 下游任务微调 - 在8个数据集上进行微调，评估不同策略；4) 性能评估 - 使用mAP和FROC指标，通过bootstrap进行统计分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次系统性研究3D医学目标检测的预训练范式；2) 建立nnDetection与预训练框架间的桥梁，实现跨领域统一方法；3) 全面评估多种预训练策略和架构组合；4) 发现基于重建的自监督预训练优于监督预训练，而对比预训练无明显益处。相比之前工作，本文专注于3D预训练而非2D，提供了更全面的评估，确保实验严谨性，并公开代码促进研究复现和进一步发展。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性地研究不同预训练策略对3D医学目标检测的影响，首次建立了连接检测、分割和自监督学习的统一框架，并发现基于重建的自监督预训练能有效提升3D医学目标检测的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-04965-0_58&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale pre-training holds the promise to advance 3D medical objectdetection, a crucial component of accurate computer-aided diagnosis. Yet, itremains underexplored compared to segmentation, where pre-training has alreadydemonstrated significant benefits. Existing pre-training approaches for 3Dobject detection rely on 2D medical data or natural image pre-training, failingto fully leverage 3D volumetric information. In this work, we present the firstsystematic study of how existing pre-training methods can be integrated intostate-of-the-art detection architectures, covering both CNNs and Transformers.Our results show that pre-training consistently improves detection performanceacross various tasks and datasets. Notably, reconstruction-basedself-supervised pre-training outperforms supervised pre-training, whilecontrastive pre-training provides no clear benefit for 3D medical objectdetection. Our code is publicly available at:https://github.com/MIC-DKFZ/nnDetection-finetuning.</description>
      <author>example@mail.com (Katharina Eckstein, Constantin Ulrich, Michael Baumgartner, Jessica Kächele, Dimitrios Bounias, Tassilo Wald, Ralf Floca, Klaus H. Maier-Hein)</author>
      <guid isPermaLink="false">2509.15947v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Sparse Multiview Open-Vocabulary 3D Detection</title>
      <link>http://arxiv.org/abs/2509.15924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025; OpenSUN3D Workshop; Camera ready version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在稀疏视图条件下进行开放词汇3D物体检测的新方法，无需训练即可利用预训练的2D基础模型，通过提升2D检测结果和优化3D提案实现视图间特征度量一致性，在标准基准测试中建立了强大基线。&lt;h4&gt;背景&lt;/h4&gt;3D场景理解和物体检测对许多视觉和机器人系统至关重要。传统方法仅能检测固定类别的物体，应用受限。在稀疏视图条件下（只有少量姿态RGB图像可用），3D物体检测面临更大挑战。&lt;h4&gt;目的&lt;/h4&gt;研究在稀疏视图条件下的开放词汇3D物体检测，克服传统方法只能检测固定类别物体的限制，并避免计算昂贵的3D特征融合和3D特定学习需求。&lt;h4&gt;方法&lt;/h4&gt;提出一种无需训练的方法，利用预训练的现成2D基础模型，通过提升2D检测结果和直接优化3D提案来实现视图间的特征度量一致性，充分利用2D中丰富的训练数据。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在标准基准测试中建立了强大的基线，在密集采样场景下与最先进技术性能相当，而在稀疏视图设置下显著优于现有技术。&lt;h4&gt;结论&lt;/h4&gt;简单的方法可以有效地解决稀疏视图条件下的开放词汇3D物体检测问题，无需复杂的3D特征融合或特定学习，为3D物体检测提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;解释和理解3D场景的能力对许多视觉和机器人系统至关重要。在众多应用中，这涉及3D物体检测，即识别属于特定类别的物体的位置和尺寸，通常表示为边界框。这传统上通过训练检测固定类别集合来解决，限制了其应用。在这项工作中，我们研究了具有挑战性但实用的稀疏视图设置下的开放词汇3D物体检测，其中只有少量姿态RGB图像可用作为输入。我们的方法无需训练，依赖预训练的现成2D基础模型，而不是采用计算昂贵的3D特征融合或需要3D特定学习。通过提升2D检测结果和直接优化3D提案以实现视图间的特征度量一致性，我们充分利用了2D中可用的丰富训练数据。通过标准基准测试，我们证明这个简单的流程建立了一个强大的基线，在密集采样场景下与最先进技术竞争，同时在稀疏视图设置下显著优于它们。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放词汇3D物体检测在稀疏视图设置下的挑战。传统3D检测只能识别预定义类别，而现有开放词汇方法又依赖密集3D数据或需要昂贵训练。这个问题在实际应用中很重要，因为它使系统能检测任意类别物体无需重新训练，特别适合设施管理、零售监控等只有有限摄像头的场景，避免了重新扫描整个场景的需要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考的核心是利用2D基础模型的强大泛化能力，避免昂贵的3D训练。他们设计了一个'提升'过程：将2D检测结果通过单目深度估计转换为3D提案，然后通过多视图一致性优化来 refine 这些提案。借鉴了现有工作包括：使用OWLv2进行2D开放词汇检测，使用SAM生成精确掩码，利用MoGe进行单目深度估计，以及结合CLIP特征确保跨视图语义一致性，但将这些技术以新颖方式组合应用于3D检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练2D基础模型的强大泛化能力，通过简单'提升'2D检测结果到3D空间，再利用多视图一致性优化来提高准确性。整体流程分三步：1)单视图提案生成 - 使用2D检测器和分割模型生成掩码，通过单目深度估计转换为3D点云；2)多视图提案优化 - 通过全局尺度初始化和每个掩码的精细调整，结合光度和CLIP语义一致性损失优化3D位置；3)聚类和融合 - 基于IoU合并重叠提案，生成最终3D边界框。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)完全训练自由的开放词汇3D检测，无需任何3D训练；2)专门针对稀疏视图设置优化，只需少量RGB图像；3)多视图特征度量一致性优化，结合光度和语义一致性；4)简单而强大的基线方法。相比之前工作，本文不依赖点云或深度数据，不需要3D训练，在稀疏视图下表现更好，能处理任意文本查询包括长尾类别，计算效率更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种简单而强大的训练自由方法，通过利用预训练的2D基础模型和稀疏多视图一致性优化，实现了在稀疏RGB图像上的开放词汇3D物体检测，无需任何3D特定训练，并在稀疏视图设置下显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ability to interpret and comprehend a 3D scene is essential for manyvision and robotics systems. In numerous applications, this involves 3D objectdetection, i.e.~identifying the location and dimensions of objects belonging toa specific category, typically represented as bounding boxes. This hastraditionally been solved by training to detect a fixed set of categories,which limits its use. In this work, we investigate open-vocabulary 3D objectdetection in the challenging yet practical sparse-view setting, where only alimited number of posed RGB images are available as input. Our approach istraining-free, relying on pre-trained, off-the-shelf 2D foundation modelsinstead of employing computationally expensive 3D feature fusion or requiring3D-specific learning. By lifting 2D detections and directly optimizing 3Dproposals for featuremetric consistency across views, we fully leverage theextensive training data available in 2D compared to 3D. Through standardbenchmarks, we demonstrate that this simple pipeline establishes a powerfulbaseline, performing competitively with state-of-the-art techniques in denselysampled scenarios while significantly outperforming them in the sparse-viewsetting.</description>
      <author>example@mail.com (Olivier Moliner, Viktor Larsson, Kalle Åström)</author>
      <guid isPermaLink="false">2509.15924v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding</title>
      <link>http://arxiv.org/abs/2509.15800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ChronoForge-RL是一种新型视频理解框架，结合TAD和KF-GRPO方法解决了视频处理中的计算效率和语义重要帧识别问题，使7B参数模型达到与72B参数模型相当的性能。&lt;h4&gt;背景&lt;/h4&gt;当前视频理解方法面临两大挑战：处理密集视频内容的计算不可行性，以及通过简单均匀采样策略难以识别语义上重要的帧。&lt;h4&gt;目的&lt;/h4&gt;提出ChronoForge-RL框架，结合时间顶点蒸馏(TAD)和关键帧感知组相对策略优化(KF-GRPO)来解决视频理解中的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;引入可微分的关键帧选择机制，通过三阶段过程识别语义转折点；提出TAD模块利用变化评分和优先蒸馏选择信息量最大的帧；引入KF-GRPO实现对比学习范式，使用显著性增强奖励机制激励模型同时利用帧内容和时间关系。&lt;h4&gt;主要发现&lt;/h4&gt;ChronoForge-RL在VideoMME上达到69.1%，在LVBench上达到52.7%，明显优于先前方法，使7B参数模型达到与72B参数模型相当的性能。&lt;h4&gt;结论&lt;/h4&gt;ChronoForge-RL框架有效解决了视频理解中的计算效率和语义重要帧识别问题，显著提升了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;当前最先进的视频理解方法通常面临两个关键挑战：(1)处理密集视频内容中每一帧的计算不可行性，以及(2)通过简单均匀采样策略识别语义上重要帧的困难。在本文中，我们提出了一种名为ChronoForge-RL的新型视频理解框架，结合时间顶点蒸馏(TAD)和关键帧感知组相对策略优化(KF-GRPO)来解决这些问题。具体来说，我们引入了一个可微分的关键帧选择机制，通过三阶段过程系统识别语义转折点，在保持时间信息的同时提高计算效率。然后，提出了两个特定模块来实现有效的时间推理：首先，TAD利用变化评分、转折点检测和优先蒸馏来选择信息量最大的帧。其次，我们引入了KF-GRPO，它实现了一个具有显著性增强奖励机制的对比学习范式，明确激励模型同时利用帧内容和时间关系。最后，我们提出的ChronoForge-RL在VideoMME上达到69.1%，在LVBench上达到52.7%，与基线方法相比，明显超越了先前的方法，同时使我们的7B参数模型能够达到与72B参数替代方案相当的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current state-of-the-art video understanding methods typically struggle withtwo critical challenges: (1) the computational infeasibility of processingevery frame in dense video content and (2) the difficulty in identifyingsemantically significant frames through naive uniform sampling strategies. Inthis paper, we propose a novel video understanding framework, calledChronoForge-RL, which combines Temporal Apex Distillation (TAD) andKeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle theseissues. Concretely, we introduce a differentiable keyframe selection mechanismthat systematically identifies semantic inflection points through a three-stageprocess to enhance computational efficiency while preserving temporalinformation. Then, two particular modules are proposed to enable effectivetemporal reasoning: Firstly, TAD leverages variation scoring, inflectiondetection, and prioritized distillation to select the most informative frames.Secondly, we introduce KF-GRPO which implements a contrastive learning paradigmwith a saliency-enhanced reward mechanism that explicitly incentivizes modelsto leverage both frame content and temporal relationships. Finally, ourproposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBenchcompared to baseline methods, clearly surpassing previous approaches whileenabling our 7B parameter model to achieve performance comparable to 72Bparameter alternatives.</description>
      <author>example@mail.com (Kehua Chen)</author>
      <guid isPermaLink="false">2509.15800v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?</title>
      <link>http://arxiv.org/abs/2509.15602v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了TennisTV基准测试，用于系统性评估多模态大语言模型在网球视频理解方面的能力，揭示了当前模型在快速、高频运动视频理解中的不足，并指出帧采样密度和时间定位是改进的关键领域。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在一般视频理解方面表现出色，但在处理像网球这样快速、高频的运动视频时存在困难，因为网球回合片段短小但信息密集。&lt;h4&gt;目的&lt;/h4&gt;创建第一个也是最全面的网球视频理解基准测试，系统性评估多模态大语言模型在这一具有挑战性领域的表现。&lt;h4&gt;方法&lt;/h4&gt;TennisTV将每个网球回合建模为连续击球事件的时序序列，使用自动化流程进行视频过滤和问题生成，涵盖8个不同层级的任务，并包含2,500个人类验证的问题。&lt;h4&gt;主要发现&lt;/h4&gt;评估了16个代表性多模态大语言模型，发现了两个关键见解：帧采样密度应该根据任务特点进行调整和平衡；改进时间定位能力对于提升模型推理能力至关重要。&lt;h4&gt;结论&lt;/h4&gt;当前多模态大语言模型在网球视频理解方面存在明显不足，需要针对快速、高频运动视频的特点进行专门优化，特别是在帧采样策略和时间定位能力方面。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在通用视频理解方面表现出色，但在像网球这样快速、高频的运动中表现不佳，因为回合片段短小但信息密集。为了系统性评估MLLMs在这一具有挑战性领域的表现，我们提出了TennisTV，这是第一个也是最全面的网球视频理解基准测试。TennisTV将每个回合建模为连续击球事件的时序序列，使用自动化流程进行过滤和问题生成。它涵盖回合级别和击球级别的8个任务，并包括2,500个人类验证的问题。通过评估16个代表性的MLLMs，我们提供了第一个网球视频理解的系统评估。结果揭示了重大的不足，并得出两个关键见解：(i) 帧采样密度应该针对任务进行调整和平衡，(ii) 改进时间定位对于更强的推理能力至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) excel at general video understandingbut struggle with fast, high-frequency sports like tennis, where rally clipsare short yet information-dense. To systematically evaluate MLLMs in thischallenging domain, we present TennisTV, the first and most comprehensivebenchmark for tennis video understanding. TennisTV models each rally as atemporal-ordered sequence of consecutive stroke events, using automatedpipelines for filtering and question generation. It covers 8 tasks at rally andstroke levels and includes 2,500 human-verified questions. Evaluating 16representative MLLMs, we provide the first systematic assessment of tennisvideo understanding. Results reveal substantial shortcomings and yield two keyinsights: (i) frame-sampling density should be tailored and balanced acrosstasks, and (ii) improving temporal grounding is essential for strongerreasoning.</description>
      <author>example@mail.com (Zhongyuan Bao, Lejun Zhang)</author>
      <guid isPermaLink="false">2509.15602v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery</title>
      <link>http://arxiv.org/abs/2509.15596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Strong accept by NeurIPS2025 Reviewers and AC, but reject by PC.  (Rating: 6,5,4,4)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了EyePCR基准测试，评估多模态大语言模型在眼科手术分析中的认知能力，包括感知、理解和推理三个维度，并通过领域适应的EyePCR-MLLM模型展示了显著性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在眼科手术等高风险、特定领域场景中的表现尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为EyePCR的眼科手术分析大规模基准测试，评估模型在感知、理解和推理方面的认知能力。&lt;h4&gt;方法&lt;/h4&gt;创建包含21万多个视觉问答对的注释语料库，涵盖1048个细粒度属性用于多视图感知，构建2.5万多个三元组的医学知识图谱用于理解，提供四个临床基础的推理任务，并开发EyePCR-MLLM作为Qwen2.5-VL-7B的领域适应变体。&lt;h4&gt;主要发现&lt;/h4&gt;EyePCR-MLLM在感知的多项选择题上达到最高准确率，在理解和推理任务上优于开源模型，性能可与GPT-4.1等商业模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;EyePCR揭示了现有MLLMs在手术认知方面的局限性，并为手术视频理解模型的基准测试和临床可靠性提升奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)展现出了非凡的能力，但它们在眼科手术等高风险、特定领域场景中的表现仍然 largely 未得到充分探索。为解决这一差距，我们开发了EyePCR，一个用于眼科手术分析的大规模基准测试，基于结构化临床知识，用于评估感知、理解和推理三个维度的认知能力。EyePCR提供了丰富的注释语料库，包含超过21万个视觉问答对，覆盖1048个细粒度属性用于多视图感知，2.5万多个三元组的医学知识图谱用于理解，以及四个临床基础的推理任务。丰富的注释促进了深入的认知分析，模拟了外科医生如何感知视觉线索并将其与领域知识结合以做出决策，从而大大提高了模型的认知能力。特别是，EyePCR-MLLM作为Qwen2.5-VL-7B的领域适应变体，在比较的模型中感知的多项选择题上达到最高准确率，在理解和推理上优于开源模型，性能可与GPT-4.1等商业模型相媲美。EyePCR揭示了现有MLLMs在手术认知方面的局限性，并为手术视频理解模型的基准测试和临床可靠性提升奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; MLLMs (Multimodal Large Language Models) have showcased remarkablecapabilities, but their performance in high-stakes, domain-specific scenarioslike surgical settings, remains largely under-explored. To address this gap, wedevelop \textbf{EyePCR}, a large-scale benchmark for ophthalmic surgeryanalysis, grounded in structured clinical knowledge to evaluate cognitionacross \textit{Perception}, \textit{Comprehension} and \textit{Reasoning}.EyePCR offers a richly annotated corpus with more than 210k VQAs, which cover1048 fine-grained attributes for multi-view perception, medical knowledge graphof more than 25k triplets for comprehension, and four clinically groundedreasoning tasks. The rich annotations facilitate in-depth cognitive analysis,simulating how surgeons perceive visual cues and combine them with domainknowledge to make decisions, thus greatly improving models' cognitive ability.In particular, \textbf{EyePCR-MLLM}, a domain-adapted variant of Qwen2.5-VL-7B,achieves the highest accuracy on MCQs for \textit{Perception} among comparedmodels and outperforms open-source models in \textit{Comprehension} and\textit{Reasoning}, rivalling commercial models like GPT-4.1. EyePCR revealsthe limitations of existing MLLMs in surgical cognition and lays the foundationfor benchmarking and enhancing clinical reliability of surgical videounderstanding models.</description>
      <author>example@mail.com (Gui Wang, Yang Wennuo, Xusen Ma, Zehao Zhong, Zhuoru Wu, Ende Wu, Rong Qu, Wooi Ping Cheah, Jianfeng Ren, Linlin Shen)</author>
      <guid isPermaLink="false">2509.15596v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2509.15464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种结合时间感知推理和知识图谱演化的方法，用于解决大型语言模型在处理动态知识时的挑战。EvoReasoner算法和EvoKG模块使模型能够有效处理时间变化的知识，并在动态问答任务中表现出色，甚至缩小了小型和大型模型之间的性能差距。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在许多语言理解任务上表现出色，但在处理不断发展的知识时存在困难。现有通过知识图谱增强LLMs的方法大多假设知识图谱是静态的，忽略了现实数据中固有的时间动态和事实不一致性。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在处理时间变化知识时的推理挑战，确保底层知识图谱的准确性和时效性，提高模型在动态问答任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出EvoReasoner，一个时间感知的多跳推理算法，执行全局-局部实体锚定、多路径分解和时间锚定评分；同时引入EvoKG，一个噪声容忍的知识图谱演化模块，通过基于置信度的矛盾解决和时间趋势跟踪从非结构化文档增量更新知识图谱。&lt;h4&gt;主要发现&lt;/h4&gt;该方法优于基于提示和基于知识图谱的基线，有效缩小了小型和大型LLMs在动态问答上的差距。使用该方法的80亿参数模型匹配了7个月后提示的671B模型的性能，证明了结合时间推理与知识图谱演化的重要性。&lt;h4&gt;结论&lt;/h4&gt;结合时间推理与知识图谱演化对于构建稳健和最新的大型语言模型性能至关重要，该方法在处理动态知识方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在许多语言理解任务上表现出色，但在处理不断发展的知识时存在困难。为了解决这个问题，最近的研究探索了通过知识图谱增强LLMs，以提供结构化、最新的信息。然而，许多现有方法假设知识图谱是静态的快照，忽略了现实数据中固有的时间动态和事实不一致性。为了解决在时间变化知识上进行推理的挑战，我们提出了EvoReasoner，一个时间感知的多跳推理算法，执行全局-局部实体锚定、多路径分解和时间锚定评分。为了确保底层知识图谱保持准确和最新，我们引入了EvoKG，一个噪声容忍的知识图谱演化模块，通过基于置信度的矛盾解决和时间趋势跟踪从非结构化文档增量更新知识图谱。我们在时间问答基准测试和一个新颖的端到端设置上评估了我们的方法，其中知识图谱从原始文档动态更新。我们的方法优于基于提示和基于知识图谱的基线，有效缩小了小型和大型LLMs在动态问答上的差距。值得注意的是，使用我们方法的80亿参数模型匹配了7个月后提示的671B模型的性能。这些结果强调了将时间推理与知识图谱演化相结合对于稳健和最新的LLM性能的重要性。我们的代码已在github.com/junhongmit/TREK上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) excel at many language understanding tasks butstruggle to reason over knowledge that evolves. To address this, recent workhas explored augmenting LLMs with knowledge graphs (KGs) to provide structured,up-to-date information. However, many existing approaches assume a staticsnapshot of the KG and overlook the temporal dynamics and factualinconsistencies inherent in real-world data. To address the challenge ofreasoning over temporally shifting knowledge, we propose EvoReasoner, atemporal-aware multi-hop reasoning algorithm that performs global-local entitygrounding, multi-route decomposition, and temporally grounded scoring. Toensure that the underlying KG remains accurate and up-to-date, we introduceEvoKG, a noise-tolerant KG evolution module that incrementally updates the KGfrom unstructured documents through confidence-based contradiction resolutionand temporal trend tracking. We evaluate our approach on temporal QA benchmarksand a novel end-to-end setting where the KG is dynamically updated from rawdocuments. Our method outperforms both prompting-based and KG-enhancedbaselines, effectively narrowing the gap between small and large LLMs ondynamic question answering. Notably, an 8B-parameter model using our approachmatches the performance of a 671B model prompted seven months later. Theseresults highlight the importance of combining temporal reasoning with KGevolution for robust and up-to-date LLM performance. Our code is publiclyavailable at github.com/junhongmit/TREK.</description>
      <author>example@mail.com (Junhong Lin, Song Wang, Xiaojie Guo, Julian Shun, Yada Zhu)</author>
      <guid isPermaLink="false">2509.15464v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Adaptive Estimation for Temporal Respiratory Disease Outbreak</title>
      <link>http://arxiv.org/abs/2509.08578v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MAESTRO是一个新型统一框架，通过整合先进的谱时建模和多模态数据融合，实现了及时且稳健的流感发病率预测。&lt;h4&gt;背景&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测流感发病率的方法，支持公共卫生决策。&lt;h4&gt;方法&lt;/h4&gt;提出了MAESTRO框架，整合了先进的谱时建模和多模态数据融合（包括监测数据、网络搜索趋势和气象数据）；通过自适应加权异构数据源和分解复杂时间序列模式来实现稳健准确的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在超过11年的香港流感数据上评估，MAESTRO展示了最先进的性能，实现了0.956的R平方值，具有优越的模型拟合效果；大量的消融实验证实了其多模态和谱时组分的重要贡献。&lt;h4&gt;结论&lt;/h4&gt;模块化和可复现的管道已公开可用，便于部署和扩展到其他地区和病原体，为流行病学预测提供了强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。本文提出了MAESTRO（多模态时间呼吸道疾病暴发自适应估计），这是一个新颖的统一框架，协同整合了先进的谱时建模与多模态数据融合，包括监测数据、网络搜索趋势和气象数据。通过自适应加权异构数据源和分解复杂时间序列模式，该模型实现了稳健且准确的预测。在香港超过11年的流感数据（不包括COVID-19期间）上评估，MAESTRO展示了最先进的性能，实现了0.956的R平方值，具有优越的模型拟合效果。大量的消融实验证实了其多模态和谱时组分的显著贡献。模块化和可复现的管道已公开可用，便于部署和扩展到其他地区和病原体，为流行病学预测提供了强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely and robust influenza incidence forecasting is critical for publichealth decision-making. This paper presents MAESTRO (Multi-modal AdaptiveEstimation for Temporal Respiratory Disease Outbreak), a novel, unifiedframework that synergistically integrates advanced spectro-temporal modelingwith multi-modal data fusion, including surveillance, web search trends, andmeteorological data. By adaptively weighting heterogeneous data sources anddecomposing complex time series patterns, the model achieves robust andaccurate forecasts. Evaluated on over 11 years of Hong Kong influenza data(excluding the COVID-19 period), MAESTRO demonstrates state-of-the-artperformance, achieving a superior model fit with an R-square of 0.956.Extensive ablations confirm the significant contributions of its multi-modaland spectro-temporal components. The modular and reproducible pipeline is madepublicly available to facilitate deployment and extension to other regions andpathogens, presenting a powerful tool for epidemiological forecasting.</description>
      <author>example@mail.com (Hong Liu, Kerui Cen, Yanxing Chen, Zige Liu, Dong Chen, Zifeng Yang, Chitin Hon)</author>
      <guid isPermaLink="false">2509.08578v3</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation</title>
      <link>http://arxiv.org/abs/2509.16170v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一模态松弛分割网络(UniMRSeg)，通过分层自监督补偿(HSSC)解决多模态图像分割中因模态不完整导致的性能下降问题，在各种缺失模态场景下显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;多模态图像分割在实际部署中面临挑战，不完整或损坏的模态会降低性能。现有方法通过专门的每组合模型解决训练-推理模态差距，但需要大量模型子集和模型-模态匹配，导致部署成本高昂。&lt;h4&gt;目的&lt;/h4&gt;设计一个统一的模态松弛分割网络，能够有效处理模态缺失问题，同时降低部署复杂度和成本。&lt;h4&gt;方法&lt;/h4&gt;UniMRSeg通过分层方式弥合完整和不完整模态在输入、特征和输出级别之间的表征差距，包括：采用混合洗牌掩码增强的模态重建；模态不变对比学习；轻量级反向注意力适配器；以及在混合一致性约束下的微调过程。&lt;h4&gt;主要发现&lt;/h4&gt;在各种缺失模态场景下，UniMRSeg在基于MRI的脑肿瘤分割、RGB-D语义分割、RGB-D/T显著目标分割等任务上显著优于最先进方法，无需额外复杂处理。&lt;h4&gt;结论&lt;/h4&gt;UniMRSeg提供了一个高效的多模态图像分割解决方案，能够有效处理模态缺失问题，同时保持较低的计算和部署成本。&lt;h4&gt;翻译&lt;/h4&gt;多模态图像分割面临实际部署挑战，不完整/损坏的模态会降低性能。虽然现有方法通过专门的每组合模型解决训练-推理模态差距，但它们需要大量模型子集和模型-模态匹配，引入了高部署成本。在这项工作中，我们通过分层自监督补偿(HSSC)提出了统一的模态松弛分割网络(UniMRSeg)。我们的方法在输入、特征和输出级别分层弥合完整和不完整模态之间的表征差距。首先，我们采用混合洗牌掩码增强进行模态重建，鼓励模型学习固有模态特征，并通过跨模态融合为缺失模态生成有意义的表征。接下来，模态不变对比学习隐式补偿不完整-完整模态对之间的特征空间距离。此外，提出的轻量级反向注意力适配器显式补偿冻结编码器中的弱感知语义。最后，UniMRSeg在混合一致性约束下进行微调，确保在所有模态组合下稳定预测而不会出现大的性能波动。无需额外复杂处理，UniMRSeg在基于MRI的脑肿瘤分割、RGB-D语义分割、RGB-D/T显著目标分割等多样化缺失模态场景下显著优于最先进方法。代码将在https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal image segmentation faces real-world deployment challenges fromincomplete/corrupted modalities degrading performance. While existing methodsaddress training-inference modality gaps via specialized per-combinationmodels, they introduce high deployment costs by requiring exhaustive modelsubsets and model-modality matching. In this work, we propose a unifiedmodality-relax segmentation network (UniMRSeg) through hierarchicalself-supervised compensation (HSSC). Our approach hierarchically bridgesrepresentation gaps between complete and incomplete modalities across input,feature and output levels. % First, we adopt modality reconstruction with thehybrid shuffled-masking augmentation, encouraging the model to learn theintrinsic modality characteristics and generate meaningful representations formissing modalities through cross-modal fusion. % Next, modality-invariantcontrastive learning implicitly compensates the feature space distance amongincomplete-complete modality pairs. Furthermore, the proposed lightweightreverse attention adapter explicitly compensates for the weak perceptualsemantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybridconsistency constraint to ensure stable prediction under all modalitycombinations without large performance fluctuations. Without bells andwhistles, UniMRSeg significantly outperforms the state-of-the-art methods underdiverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-Dsemantic segmentation, RGB-D/T salient object segmentation. The code will bereleased at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.</description>
      <author>example@mail.com (Xiaoqi Zhao, Youwei Pang, Chenyang Yu, Lihe Zhang, Huchuan Lu, Shijian Lu, Georges El Fakhri, Xiaofeng Liu)</author>
      <guid isPermaLink="false">2509.16170v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports</title>
      <link>http://arxiv.org/abs/2509.16095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICDM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AdaSports-Traj的自适应轨迹建模框架，用于解决多智能体体育场景中的轨迹预测挑战，通过角色和领域感知适配器以及分层对比学习方法，有效处理了域内和域间的分布差异。&lt;h4&gt;背景&lt;/h4&gt;多智能体体育场景中的轨迹预测具有挑战性，因为不同智能体角色（如球员与球）存在结构异质性，不同体育领域间存在动态分布差异，现有统一框架无法捕捉这些变化。&lt;h4&gt;目的&lt;/h4&gt;提出AdaSports-Traj框架，解决体育领域中域内和域间的分布差异问题，提高轨迹预测在角色和领域间的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;框架核心是角色和领域感知适配器，根据智能体身份和领域上下文条件性地调整潜在表示；同时引入分层对比学习目标，分别监督角色敏感和领域感知的表示，鼓励解耦的潜在结构而不引入优化冲突。&lt;h4&gt;主要发现&lt;/h4&gt;在Basketball-U、Football-U和Soccer-U三个多样化的体育数据集上的实验证明，AdaSports-Traj的自适应设计在统一和跨域轨迹预测设置中都取得了有效性能。&lt;h4&gt;结论&lt;/h4&gt;AdaSports-Traj通过自适应处理角色和领域差异，显著提高了多智能体体育场景中的轨迹预测能力。&lt;h4&gt;翻译&lt;/h4&gt;在多智能体体育场景中，轨迹预测本质上具有挑战性，这源于智能体角色间的结构异质性（如球员与球）以及不同体育领域间的动态分布差异。现有的统一框架往往无法捕捉这些结构分布变化，导致在角色和领域间的泛化能力不佳。我们提出了AdaSports-Traj，一个自适应轨迹建模框架，明确解决体育领域中域内和域间的分布差异。其核心是角色和领域感知适配器，根据智能体身份和领域上下文条件性地调整潜在表示。此外，我们引入了分层对比学习目标，分别监督角色敏感和领域感知的表示，鼓励解耦的潜在结构而不引入优化冲突。在三个多样化的体育数据集（Basketball-U、Football-U和Soccer-U）上的实验证明了我们自适应设计的有效性，在统一和跨域轨迹预测设置中都取得了强大性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory prediction in multi-agent sports scenarios is inherentlychallenging due to the structural heterogeneity across agent roles (e.g.,players vs. ball) and dynamic distribution gaps across different sportsdomains. Existing unified frameworks often fail to capture these structureddistributional shifts, resulting in suboptimal generalization across roles anddomains. We propose AdaSports-Traj, an adaptive trajectory modeling frameworkthat explicitly addresses both intra-domain and inter-domain distributiondiscrepancies in sports. At its core, AdaSports-Traj incorporates a Role- andDomain-Aware Adapter to conditionally adjust latent representations based onagent identity and domain context. Additionally, we introduce a HierarchicalContrastive Learning objective, which separately supervises role-sensitive anddomain-aware representations to encourage disentangled latent structureswithout introducing optimization conflict. Experiments on three diverse sportsdatasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectivenessof our adaptive design, achieving strong performance in both unified andcross-domain trajectory prediction settings.</description>
      <author>example@mail.com (Yi Xu, Yun Fu)</author>
      <guid isPermaLink="false">2509.16095v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>A multi-temporal multi-spectral attention-augmented deep convolution neural network with contrastive learning for crop yield prediction</title>
      <link>http://arxiv.org/abs/2509.15966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in Computers and Electronics in Agriculture&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为MTMS-YieldNet的新型多时相多光谱产量预测网络，通过整合光谱数据与时空信息，有效解决了气候变化背景下农作物产量预测的挑战，在不同气候和季节条件下均表现出优异的预测性能。&lt;h4&gt;背景&lt;/h4&gt;精确的产量预测对农业可持续性和粮食安全至关重要，但气候变化通过影响天气条件、土壤肥力和农场管理系统等因素，使准确的产量预测变得复杂。现有方法在处理多光谱数据时面临挑战，而这种数据对于评估作物健康和生长模式至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合光谱数据与时空信息的产量预测网络，以克服现有方法的局限性，提高产量预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出名为MTMS-YieldNet的新型网络，整合光谱数据与时空信息，利用对比学习进行特征判别，专注于从遥感数据中捕获空间-光谱模式和时空依赖关系，而非依赖在通用视觉数据上预训练的模型。&lt;h4&gt;主要发现&lt;/h4&gt;MTMS-YieldNet在七种现有最先进方法中表现优异，在Sentinel-1数据集上达到0.336的MAPE分数，在Landsat-8数据集上达到0.353的MAPE分数，在Sentinel-2数据集上达到0.331的MAPE分数，展示了在不同气候和季节条件下的有效预测能力。&lt;h4&gt;结论&lt;/h4&gt;MTMS-YieldNet显著提高了产量预测的准确性，为农民提供有价值的见解，帮助他们做出更好的决策，从而有潜力提高作物产量。&lt;h4&gt;翻译&lt;/h4&gt;精确的产量预测对农业可持续性和粮食安全至关重要。然而，气候变化通过影响天气条件、土壤肥力和农场管理系统等主要因素，使准确的产量预测变得复杂。技术的进步在克服这些挑战方面发挥了重要作用，通过利用卫星监测和数据分析进行精确的产量估计。当前方法依赖于时空数据来预测作物产量，但它们往往难以处理多光谱数据，而多光谱数据对于评估作物健康和生长模式至关重要。为解决这一挑战，我们提出了一种新颖的多时相多光谱产量预测网络MTMS-YieldNet，该网络将光谱数据与时空信息整合，以有效捕获它们之间的相关性和依赖关系。虽然现有方法依赖于在通用视觉数据上预训练的模型，但MTMS-YieldNet在预训练过程中利用对比学习进行特征判别，专注于从遥感数据中捕获空间-光谱模式和时空依赖关系。定量和定性评估均突显了所提出的MTMS-YieldNet相对于七种现有最先进方法的优越性。MTMS-YieldNet在Sentinel-1上达到0.336的MAPE分数，在Landsat-8上达到0.353的MAPE分数，以及在Sentinel-2上达到了卓越的0.331的MAPE分数，展示了在不同气候和季节条件下的有效产量预测性能。MTMS-YieldNet的卓越性能提高了产量预测，并提供了有价值的见解，可以帮助农民做出更好的决策，从而可能提高作物产量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.compag.2025.110895&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Precise yield prediction is essential for agricultural sustainability andfood security. However, climate change complicates accurate yield prediction byaffecting major factors such as weather conditions, soil fertility, and farmmanagement systems. Advances in technology have played an essential role inovercoming these challenges by leveraging satellite monitoring and dataanalysis for precise yield estimation. Current methods rely on spatio-temporaldata for predicting crop yield, but they often struggle with multi-spectraldata, which is crucial for evaluating crop health and growth patterns. Toresolve this challenge, we propose a novel Multi-Temporal Multi-Spectral YieldPrediction Network, MTMS-YieldNet, that integrates spectral data withspatio-temporal information to effectively capture the correlations anddependencies between them. While existing methods that rely on pre-trainedmodels trained on general visual data, MTMS-YieldNet utilizes contrastivelearning for feature discrimination during pre-training, focusing on capturingspatial-spectral patterns and spatio-temporal dependencies from remote sensingdata. Both quantitative and qualitative assessments highlight the excellence ofthe proposed MTMS-YieldNet over seven existing state-of-the-art methods.MTMS-YieldNet achieves MAPE scores of 0.336 on Sentinel-1, 0.353 on Landsat-8,and an outstanding 0.331 on Sentinel-2, demonstrating effective yieldprediction performance across diverse climatic and seasonal conditions. Theoutstanding performance of MTMS-YieldNet improves yield predictions andprovides valuable insights that can assist farmers in making better decisions,potentially improving crop yields.</description>
      <author>example@mail.com (Shalini Dangi, Surya Karthikeya Mullapudi, Chandravardhan Singh Raghaw, Shahid Shafi Dar, Mohammad Zia Ur Rehman, Nagendra Kumar)</author>
      <guid isPermaLink="false">2509.15966v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>UNIV: Unified Foundation Model for Infrared and Visible Modalities</title>
      <link>http://arxiv.org/abs/2509.15642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为UNIV的生物启发统一模型，用于处理RGB可见光和红外数据的联合感知，通过两种创新方法实现了跨模态特征对齐和知识保留，在保持可见光任务性能的同时显著提升了红外任务性能。&lt;h4&gt;背景&lt;/h4&gt;对RGB可见光和红外感知的联合需求正在快速增长，特别是在各种天气条件下实现稳健性能。预训练的RGB可见光和红外数据模型在其各自领域表现出色，但在配备两种传感器的自动驾驶车辆等多模态场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决预训练模型在多模态场景中的表现问题，提出一种生物启发的统一红外和可见光模态基础模型(UNIV)，以实现跨模态的有效感知和学习。&lt;h4&gt;方法&lt;/h4&gt;1) 引入基于补丁的跨模态对比学习(PCCL)，模仿视网膜水平细胞的侧向抑制，实现跨模态特征对齐；2) 双知识保留机制模拟视网膜双极细胞信号路由，结合LoRA适配器和同步蒸馏防止灾难性遗忘；3) 引入MVIP数据集，包含98,992个精确对齐的图像对，涵盖多种场景。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明UNIV在红外任务上表现优越(语义分割中+1.7 mIoU，目标检测中+0.7 mAP)，同时在可见RGB任务上保持了99%以上的基线性能。&lt;h4&gt;结论&lt;/h4&gt;UNIV模型成功实现了RGB可见光和红外数据的联合感知，在保持可见光任务性能的同时显著提升了红外任务性能，为自动驾驶等应用提供了多模态感知的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;对RGB可见光和红外感知的联合需求正在快速增长，特别是在各种天气条件下实现稳健性能。尽管预训练的RGB可见光和红外数据模型在其各自领域表现出色，但在多模态场景中往往表现不佳，例如配备这两种传感器的自动驾驶车辆。为应对这一挑战，我们提出了一种生物启发的统一红外和可见光模态基础模型(UNIV)，具有两个关键创新。首先，我们引入了基于补丁的跨模态对比学习(PCCL)，这是一种注意力引导的蒸馏框架，模仿视网膜水平细胞的侧向抑制，使有效的跨模态特征对齐成为可能，同时保持与任何基于Transformer架构的兼容性。其次，我们的双知识保留机制模拟视网膜双极细胞信号路由 - 结合LoRA适配器(增加2%参数)和同步蒸馏，防止灾难性遗忘，从而复制视网膜的明视觉(锥体驱动)和暗视觉(杆体驱动)功能。为支持跨模态学习，我们引入了MVIP数据集，这是迄今为止最全面的可见光-红外基准数据集。它包含98,992个精确对齐的图像对，涵盖多种场景。大量实验证明了UNIV在红外任务上的优越性能(语义分割中+1.7 mIoU，目标检测中+0.7 mAP)，同时在可见RGB任务上保持了99%以上的基线性能。我们的代码可在https://github.com/fangyuanmao/UNIV获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The demand for joint RGB-visible and infrared perception is growing rapidly,particularly to achieve robust performance under diverse weather conditions.Although pre-trained models for RGB-visible and infrared data excel in theirrespective domains, they often underperform in multimodal scenarios, such asautonomous vehicles equipped with both sensors. To address this challenge, wepropose a biologically inspired UNified foundation model for Infrared andVisible modalities (UNIV), featuring two key innovations. First, we introducePatch-wise Cross-modality Contrastive Learning (PCCL), an attention-guideddistillation framework that mimics retinal horizontal cells' lateralinhibition, which enables effective cross-modal feature alignment whileremaining compatible with any transformer-based architecture. Second, ourdual-knowledge preservation mechanism emulates the retina's bipolar cell signalrouting - combining LoRA adapters (2% added parameters) with synchronousdistillation to prevent catastrophic forgetting, thereby replicating theretina's photopic (cone-driven) and scotopic (rod-driven) functionality. Tosupport cross-modal learning, we introduce the MVIP dataset, the mostcomprehensive visible-infrared benchmark to date. It contains 98,992 preciselyaligned image pairs spanning diverse scenarios. Extensive experimentsdemonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU insemantic segmentation and +0.7 mAP in object detection) while maintaining 99%+of the baseline performance on visible RGB tasks. Our code is available athttps://github.com/fangyuanmao/UNIV.</description>
      <author>example@mail.com (Fangyuan Mao, Shuo Wang, Jilin Mei, Chen Min, Shun Lu, Fuyang Liu, Yu Hu)</author>
      <guid isPermaLink="false">2509.15642v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection</title>
      <link>http://arxiv.org/abs/2509.15570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted CVIPPR 2024 April Xiamen China&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种针对无监督异常声音检测的异常暴露方法，通过对比学习中的高频信息数据增强技术，使模型更关注代表机器正常操作模式的低频信息，从而有效提高异常检测性能。&lt;h4&gt;背景&lt;/h4&gt;异常声音检测在工业设备监控等领域具有重要意义。传统方法难以解决无监督场景下的异常检测问题，因为缺乏异常样本进行训练。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决无监督异常声音检测问题，通过使模型学习正常数据的分布空间，提高异常检测的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;基于生物感知和数据分析发现异常音频和噪声通常具有更高频率的特点，本研究提出了一种针对对比学习的高频信息数据增强方法。该方法使模型更关注代表机器正常操作模式的低频信息。&lt;h4&gt;主要发现&lt;/h4&gt;在DCASE 2020 Task 2和DCASE 2022 Task 2数据集上的评估结果表明，所提出的方法优于该数据集上使用的其他对比学习方法，具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过高频信息数据增强的对比学习方法能够有效提高无监督异常声音检测的性能，模型能够更好地学习正常数据的分布空间，从而准确识别异常声音。&lt;h4&gt;翻译&lt;/h4&gt;异常暴露方法是解决无监督异常声音检测问题的有效方法。该方法的关键焦点是如何让模型学习正常数据的分布空间。基于生物感知和数据分析，发现异常音频和噪声通常具有更高的频率。因此，我们提出了一种针对对比学习中的高频信息的数据增强方法。这使得模型能够更关注音频的低频信息，这代表了机器的正常操作模式。我们在DCASE 2020 Task 2上评估了所提出的方法。结果表明，我们的方法优于该数据集上使用的其他对比学习方法。我们还评估了该方法在DCASE 2022 Task 2数据集上的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The outlier exposure method is an effective approach to address theunsupervised anomaly sound detection problem. The key focus of this method ishow to make the model learn the distribution space of normal data. Based onbiological perception and data analysis, it is found that anomalous audio andnoise often have higher frequencies. Therefore, we propose a data augmentationmethod for high-frequency information in contrastive learning. This enables themodel to pay more attention to the low-frequency information of the audio,which represents the normal operational mode of the machine. We evaluated theproposed method on the DCASE 2020 Task 2. The results showed that our methodoutperformed other contrastive learning methods used on this dataset. We alsoevaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.</description>
      <author>example@mail.com (Xinxin Meng, Jiangtao Guo, Yunxiang Zhang, Shun Huang)</author>
      <guid isPermaLink="false">2509.15570v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Comparing Computational Pathology Foundation Models using Representational Similarity Analysis</title>
      <link>http://arxiv.org/abs/2509.15482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统分析了六个计算病理学基础模型的表示空间，发现模型训练范式与表示结构无直接关联，所有模型均表现出较高的幻灯片依赖性但较低的疾病依赖性，染色标准化可降低幻灯片依赖性，视觉-语言模型具有更紧凑的表示结构。&lt;h4&gt;背景&lt;/h4&gt;基础模型在计算病理学中日益发展，有望促进多种下游任务，但对不同模型学习到的表示结构和变异性了解有限。&lt;h4&gt;目的&lt;/h4&gt;系统分析六个计算病理学基础模型的表示空间，使用计算神经科学中流行化的技术。&lt;h4&gt;方法&lt;/h4&gt;分析六种CPath基础模型(包括视觉-语言对比学习模型CONCH、PLIP、KEEP和自蒸馏模型UNI v2、Virchow v2、Prov-GigaPath)，使用TCGA的H&amp;E图像补丁进行表示相似性分析。&lt;h4&gt;主要发现&lt;/h4&gt;UNI2和Virchow2具有最独特的表示结构，Prov-Gigapath在模型间具有最高平均相似性；相同训练范式不保证表示相似性；所有模型表示均表现出较高幻灯片依赖性和较低疾病依赖性；染色标准化将幻灯片依赖性降低5.5%-20.5%；视觉-语言模型具有相对紧凑的表示，仅视觉模型表示更为分散。&lt;h4&gt;结论&lt;/h4&gt;这些发现为提高对幻灯片特定特征的鲁棒性、模型集成策略设计以及理解训练范式如何塑造模型表示提供了见解；该框架可扩展到其他医学成像领域，有助于确保基础模型的有效开发和部署。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在计算病理学(CPath)中日益受到重视，因为它们有望促进许多下游任务。尽管最近的研究已经评估了不同模型的任务性能，但对于它们学习到的表示结构和变异性了解较少。在此，我们使用计算神经科学中流行化的技术，系统分析了六个CPath基础模型的表示空间。所分析的模型涵盖了视觉-语言对比学习(CONCH、PLIP、KEEP)和自蒸馏(UNI v2、Virchow v2、Prov-GigaPath)方法。通过对TCGA的H&amp;E图像补丁进行表示相似性分析，我们发现UNI2和Virchow2具有最独特的表示结构，而Prov-Gigapath在模型间具有最高的平均相似性。具有相同的训练范式（仅视觉vs视觉-语言）并不能保证更高的表示相似性。所有模型的表示都表现出较高的幻灯片依赖性，但相对较低的疾病依赖性。染色标准化将所有模型的幻灯片依赖性降低了5.5%(CONCH)到20.5%(PLIP)。在内在维度方面，视觉-语言模型表现出相对紧凑的表示，而仅视觉模型的表示更为分散。这些发现突显了提高对幻灯片特定特征鲁棒性的机会，为模型集成策略提供信息，并深入了解训练范式如何塑造模型表示。我们的框架可扩展到医学成像领域，探索基础模型的内部表示可以帮助确保有效开发和部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are increasingly developed in computational pathology(CPath) given their promise in facilitating many downstream tasks. While recentstudies have evaluated task performance across models, less is known about thestructure and variability of their learned representations. Here, wesystematically analyze the representational spaces of six CPath foundationmodels using techniques popularized in computational neuroscience. The modelsanalyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) andself-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Throughrepresentational similarity analysis using H&amp;E image patches from TCGA, we findthat UNI2 and Virchow2 have the most distinct representational structures,whereas Prov-Gigapath has the highest average similarity across models. Havingthe same training paradigm (vision-only vs. vision-language) did not guaranteehigher representational similarity. The representations of all models showed ahigh slide-dependence, but relatively low disease-dependence. Stainnormalization decreased slide-dependence for all models by a range of 5.5%(CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-languagemodels demonstrated relatively compact representations, compared to the moredistributed representations of vision-only models. These findings highlightopportunities to improve robustness to slide-specific features, inform modelensembling strategies, and provide insights into how training paradigms shapemodel representations. Our framework is extendable across medical imagingdomains, where probing the internal representations of foundation models canhelp ensure effective development and deployment.</description>
      <author>example@mail.com (Vaibhav Mishra, William Lotter)</author>
      <guid isPermaLink="false">2509.15482v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks</title>
      <link>http://arxiv.org/abs/2509.15272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, XAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统评估了未经额外修改的视觉Transformer（ViT）特征在图像分类和分割任务中的表现，探索了不同令牌类型、任务和预训练目标下的最佳选择，为自监督学习（SSL）在计算机视觉中的应用提供了新见解。&lt;h4&gt;背景&lt;/h4&gt;自监督学习（SSL）对视觉Transformer（ViT）显示出作为各种计算机视觉任务预训练策略的巨大潜力。对比学习和掩码图像建模是SSL技术的两大主流预训练目标。从最终transformer注意力块（键、查询和值）以及最终块的前馈层后获取的特征已成为解决下游任务的基础。然而，现有方法通常对这些预训练的ViT特征进行额外处理，如通过轻量级头部或蒸馏技术，以获得更好的任务性能。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究空白，通过系统评估未经修改的ViT特征在图像分类和分割任务（包括标准和少样本情境）中的应用，全面分析ViT特征的内在表示能力。&lt;h4&gt;方法&lt;/h4&gt;研究使用基于超平面（如逻辑回归）或余弦相似度的分类和分割规则，这些规则依赖于ViT潜在空间中可解释方向的存在。在不使用额外特征转换的情况下，对不同令牌类型、任务和预训练ViT模型进行了系统分析。&lt;h4&gt;主要发现&lt;/h4&gt;研究提供了关于令牌类型和决策规则的最佳选择的见解，这些选择基于任务、上下文和预训练目标，同时在两个广泛使用的数据集上报告了详细的研究结果。&lt;h4&gt;结论&lt;/h4&gt;研究揭示了ViT原始特征在不同任务和情境下的表现特性，为如何有效利用这些特征提供了指导，而无需额外的特征转换层。&lt;h4&gt;翻译&lt;/h4&gt;视觉Transformer（ViT）的自监督学习（SSL）最近显示出作为各种计算机视觉任务预训练策略的巨大潜力，包括图像分类和分割，在标准和少样本下游情境中都有应用。SSL技术领域主要由两种预训练目标主导：对比学习和掩码图像建模。从最终transformer注意力块（特别是键、查询和值）以及最终块的前馈层后获取的特征已成为解决下游任务的共同基础。然而，在许多现有方法中，这些预训练的ViT特征会通过额外的转换层进行处理，通常涉及轻量级头部或与蒸馏结合，以获得更好的任务性能。尽管这些方法可以改善任务结果，但据我们所知，尚未对未经修改的ViT特征的内在表示能力进行全面分析。本研究旨在通过系统评估这些未修改的特征在图像分类和分割任务（包括标准和少样本情境）中的应用来填补这一空白。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recentlydemonstrated considerable potential as a pre-training strategy for a variety ofcomputer vision tasks, including image classification and segmentation, both instandard and few-shot downstream contexts. Two pre-training objectives dominatethe landscape of SSL techniques: Contrastive Learning and Masked ImageModeling. Features (or tokens) extracted from the final transformer attentionblock -- specifically, the keys, queries, and values -- as well as featuresobtained after the final block's feed-forward layer, have become a commonfoundation for addressing downstream tasks. However, in many existingapproaches, these pre-trained ViT features are further processed throughadditional transformation layers, often involving lightweight heads or combinedwith distillation, to achieve superior task performance. Although such methodscan improve task outcomes, to the best of our knowledge, a comprehensiveanalysis of the intrinsic representation capabilities of unaltered ViT featureshas yet to be conducted. This study aims to bridge this gap by systematicallyevaluating the use of these unmodified features across image classification andsegmentation tasks, in both standard and few-shot contexts. The classificationand segmentation rules that we use are either hyperplane based (as in logisticregression) or cosine-similarity based, both of which rely on the presence ofinterpretable directions in the ViT's latent space. Based on the previous rulesand without the use of additional feature transformations, we conduct ananalysis across token types, tasks, and pre-trained ViT models. This studyprovides insights into the optimal choice for token type and decision rulebased on the task, context, and the pre-training objective, while reportingdetailed findings on two widely-used datasets.</description>
      <author>example@mail.com (Yannis Kaltampanidis, Alexandros Doumanoglou, Dimitrios Zarpalas)</author>
      <guid isPermaLink="false">2509.15272v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Are Multimodal Foundation Models All That Is Needed for Emofake Detection?</title>
      <link>http://arxiv.org/abs/2509.16193v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to APSIPA-ASC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了多模态基础模型在情感伪造检测中的应用，提出并验证了它们优于仅依赖音频的音频基础模型的假设，同时提出了SCAR融合框架实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态基础模型通过跨模态预训练从多种模态学习情感模式，而音频基础模型仅依赖音频，这可能影响它们对情感伪造检测的准确性。&lt;h4&gt;目的&lt;/h4&gt;验证多模态基础模型在情感伪造检测中会优于音频基础模型的假设，并探索基础模型融合的有效方法。&lt;h4&gt;方法&lt;/h4&gt;对最先进的多模态基础模型(如LanguageBind)和音频基础模型(如WavLM)进行综合比较分析，提出SCAR框架，该框架包含嵌套交叉注意力机制和自注意力细化模块，实现基础模型表示的顺序交互和特征增强。&lt;h4&gt;主要发现&lt;/h4&gt;多模态基础模型确实在情感伪造检测中优于音频基础模型；使用SCAR框架协同融合多模态基础模型实现了最先进的性能，超越了独立基础模型和传统融合方法以及先前的工作。&lt;h4&gt;结论&lt;/h4&gt;多模态基础模型能够更好地识别不自然的情感转变和操纵音频中的不一致性，使其在区分真实与虚假情感表达方面更有效。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们研究多模态基础模型用于情感伪造检测，并假设它们将优于音频基础模型。多模态基础模型由于其跨模态预训练，从多种模态学习情感模式，而音频基础模型仅依赖音频。因此，多模态基础模型可以更好地识别操纵音频中不自然的情感转变和不一致性，使其在区分真实与虚假情感表达方面更有效。为了验证我们的假设，我们对最先进的多模态基础模型(如LanguageBind)和音频基础模型(如WavLM)进行了全面的比较分析。我们的实验证实多模态基础模型在情感伪造检测中优于音频基础模型。除了独立基础模型的性能外，我们还探索了基础模型融合，受到相关研究领域如合成语音检测和语音情感识别的发现启发。为此，我们提出了SCAR，一种有效融合的新颖框架。SCAR引入了嵌套交叉注意力机制，其中表示从基础模型在两个阶段顺序交互以改进信息交换。此外，自注意力细化模块通过强化重要的跨基础模型线索同时抑制噪声来进一步增强特征表示。通过SCAR协同融合多模态基础模型，我们实现了最先进的性能，超越了独立基础模型、传统融合方法以及情感伪造检测的先前工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we investigate multimodal foundation models (MFMs) for EmoFakedetection (EFD) and hypothesize that they will outperform audio foundationmodels (AFMs). MFMs due to their cross-modal pre-training, learns emotionalpatterns from multiple modalities, while AFMs rely only on audio. As such, MFMscan better recognize unnatural emotional shifts and inconsistencies inmanipulated audio, making them more effective at distinguishing real from fakeemotional expressions. To validate our hypothesis, we conduct a comprehensivecomparative analysis of state-of-the-art (SOTA) MFMs (e.g. LanguageBind)alongside AFMs (e.g. WavLM). Our experiments confirm that MFMs surpass AFMs forEFD. Beyond individual foundation models (FMs) performance, we explore FMsfusion, motivated by findings in related research areas such synthetic speechdetection and speech emotion recognition. To this end, we propose SCAR, a novelframework for effective fusion. SCAR introduces a nested cross-attentionmechanism, where representations from FMs interact at two stages sequentiallyto refine information exchange. Additionally, a self-attention refinementmodule further enhances feature representations by reinforcing importantcross-FM cues while suppressing noise. Through SCAR with synergistic fusion ofMFMs, we achieve SOTA performance, surpassing both standalone FMs andconventional fusion approaches and previous works on EFD.</description>
      <author>example@mail.com (Mohd Mujtaba Akhtar, Girish, Orchid Chetia Phukan, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru)</author>
      <guid isPermaLink="false">2509.16193v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories</title>
      <link>http://arxiv.org/abs/2509.16176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ACDC系统，一个由人类导演与无人机之间自然语言通信驱动的自主无人机摄影系统，利用大型语言模型和视觉基础模型将自然语言提示直接转化为可执行的室内无人机视频导览。&lt;h4&gt;背景&lt;/h4&gt;先前无人机摄影工作流程的主要局限性是需要基于预定义的人类意图手动选择路径点和视角，这既劳动密集又导致性能不一致。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用大型语言模型和视觉基础模型将自由形式的自然语言提示直接转化为可执行的室内无人机视频导览的方法。&lt;h4&gt;方法&lt;/h4&gt;该方法包括：视觉语言检索管道用于初始路径点选择；基于偏好的贝叶斯优化框架，利用美学反馈优化姿态；以及运动规划器生成安全的四旋翼飞行轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;通过仿真和硬件在环实验验证了ACDC，证明它能够在多样化的室内场景中稳健地产生专业质量的视频画面，无需机器人或摄影专业知识。&lt;h4&gt;结论&lt;/h4&gt;这些结果突出了具身AI代理在从开放词汇对话到现实世界自主空中摄影方面闭环的潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了代理空中摄影：从对话线索到电影轨迹（ACDC），这是一个由人类导演与无人机之间自然语言通信驱动的自主无人机摄影系统。先前无人机摄影工作流程的主要局限性是需要基于预定义的人类意图手动选择路径点和视角，这既劳动密集又导致性能不一致。在本文中，我们提议采用大型语言模型和视觉基础模型将自由形式的自然语言提示直接转化为可执行的室内无人机视频导览。具体来说，我们的方法包括一个用于初始路径点选择的视觉语言检索管道，一个利用美学反馈优化姿态的基于偏好的贝叶斯优化框架，以及一个生成安全四旋翼轨迹的运动规划器。我们通过仿真和硬件在环实验验证了ACDC，证明它能够在多样化的室内场景中稳健地产生专业质量的视频画面，无需机器人或摄影专业知识。这些结果突出了具身AI代理在从开放词汇对话到现实世界自主空中摄影方面闭环的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Agentic Aerial Cinematography: From Dialogue Cues to CinematicTrajectories (ACDC), an autonomous drone cinematography system driven bynatural language communication between human directors and drones. The mainlimitation of previous drone cinematography workflows is that they requiremanual selection of waypoints and view angles based on predefined human intent,which is labor-intensive and yields inconsistent performance. In this paper, wepropose employing large language models (LLMs) and vision foundation models(VFMs) to convert free-form natural language prompts directly into executableindoor UAV video tours. Specifically, our method comprises a vision-languageretrieval pipeline for initial waypoint selection, a preference-based Bayesianoptimization framework that refines poses using aesthetic feedback, and amotion planner that generates safe quadrotor trajectories. We validate ACDCthrough both simulation and hardware-in-the-loop experiments, demonstratingthat it robustly produces professional-quality footage across diverse indoorscenes without requiring expertise in robotics or cinematography. These resultshighlight the potential of embodied AI agents to close the loop fromopen-vocabulary dialogue to real-world autonomous aerial cinematography.</description>
      <author>example@mail.com (Yifan Lin, Sophie Ziyu Liu, Ran Qi, George Z. Xue, Xinping Song, Chao Qin, Hugh H. -T. Liu)</author>
      <guid isPermaLink="false">2509.16176v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning</title>
      <link>http://arxiv.org/abs/2509.16025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Copyright 2025 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新颖的多模态基础模型方法，用于口语语言评估，能够在单次处理中进行会话级别评估，结合多目标学习和Whisper ASR模型语音先验，有效预测学习者的整体口语能力。&lt;h4&gt;背景&lt;/h4&gt;口语语言评估从自然言语中评估学习者的口语能力，随着第二语言英语使用者人数增长，对可靠SLA的需求增加，SLA是计算机辅助语言学习的关键组成部分。现有方法存在级联管道易传播错误或端到端模型在短音频窗口上操作可能丢失话语级别证据的局限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在单次处理中进行会话级别评估的新方法，克服现有方法的局限性，避免错误传播和丢失话语级别证据，创建适用于CALL应用的紧凑可部署评分器。&lt;h4&gt;方法&lt;/h4&gt;提出多模态基础模型方法，结合多目标学习与基于冻结的Whisper ASR模型的语音先验进行声学感知校准，无需手动制作特征，共同学习SLA的整体和特质级目标，一致处理L2说话者的整个回答会话。&lt;h4&gt;主要发现&lt;/h4&gt;在Speak &amp; Improve基准测试中，提出的方法优于之前最先进的级联系统，展示了强大的跨参与者泛化能力，产生了一个为CALL应用量身定制的紧凑可部署评分器。&lt;h4&gt;结论&lt;/h4&gt;提出的方法有效解决了现有SLA方法的局限性，通过一致处理整个回答会话提高了整体口语能力预测的准确性，为CALL应用提供了实用高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;口语语言评估从自然言语中估计学习者的口语能力。第二语言英语使用者人口的增加加剧了对可靠SLA的需求，SLA是计算机辅助语言学习的关键组成部分。现有工作通常依赖级联管道，容易传播错误，或端到端模型通常在短音频窗口上操作，可能错过话语级别的证据。本文引入了一种新颖的多模态基础模型方法，在单次执行中进行会话级别评估。我们的方法将多目标学习与基于冻结的Whisper ASR模型的语音先验相结合进行声学感知校准，允许共同学习SLA的整体和特质级目标，无需依赖手工制作的特征。通过一致地处理L2说话者的整个回答会话，该模型在预测整体口语能力方面表现出色。在Speak &amp; Improve基准上进行的实验表明，我们提出的方法优于之前最先进的级联系统，并表现出强大的跨参与者泛化能力，产生了一个为CALL应用量身定制的紧凑可部署评分器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spoken Language Assessment (SLA) estimates a learner's oral proficiency fromspontaneous speech. The growing population of L2 English speakers hasintensified the demand for reliable SLA, a critical component of ComputerAssisted Language Learning (CALL). Existing efforts often rely on cascadedpipelines, which are prone to error propagation, or end-to-end models thatoften operate on a short audio window, which might miss discourse-levelevidence. This paper introduces a novel multimodal foundation model approachthat performs session-level evaluation in a single pass. Our approach couplesmulti-target learning with a frozen, Whisper ASR model-based speech prior foracoustic-aware calibration, allowing for jointly learning holistic andtrait-level objectives of SLA without resorting to handcrafted features. Bycoherently processing the entire response session of an L2 speaker, the modelexcels at predicting holistic oral proficiency. Experiments conducted on theSpeak &amp; Improve benchmark demonstrate that our proposed approach outperformsthe previous state-of-the-art cascaded system and exhibits robust cross-partgeneralization, producing a compact deployable grader that is tailored for CALLapplications.</description>
      <author>example@mail.com (Hong-Yun Lin, Jhen-Ke Lin, Chung-Chun Wang, Hao-Chien Lu, Berlin Chen)</author>
      <guid isPermaLink="false">2509.16025v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>AI Methods for Permutation Circuit Synthesis Across Generic Topologies</title>
      <link>http://arxiv.org/abs/2509.16020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by First AAAI Symposium on Quantum  Information &amp; Machine Learning (QIML): Bridging Quantum Computing and  Artificial Intelligence at AAAI 2025 Fall Symposium&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了人工智能方法在通用拓扑上排列电路的综合和转译。研究使用强化学习技术实现了最多25个量子比特的排列电路的近最优综合。通过训练基础模型和采用掩码机制，使单个训练模型能够在多种拓扑上高效合成电路，并可实际集成到转译工作流中。&lt;h4&gt;背景&lt;/h4&gt;在量子计算领域，排列电路的综合和转译是一个重要问题。传统方法通常需要为每种拓扑单独训练模型，限制了方法的通用性和效率。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理通用拓扑的排列电路综合方法，无需为每种拓扑单独训练模型，实现高效且通用的电路合成。&lt;h4&gt;方法&lt;/h4&gt;研究使用强化学习技术训练一个基础模型在通用矩形格子上，并通过掩码机制在综合过程中动态选择拓扑子集。这种方法使模型能够在嵌入矩形格子的任何拓扑上合成排列电路而无需重新训练。此外，研究还展示了可以对模型进行微调以增强对特定拓扑的性能。&lt;h4&gt;主要发现&lt;/h4&gt;研究展示了5x5格子的结果，并将其与之前的AI拓扑导向模型和经典方法进行了比较。结果表明，新方法优于经典启发式方法，与之前的专用AI模型相匹配，能够对训练过程中未见过的拓扑进行综合，并且可以通过微调增强对特定拓扑的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法允许单个训练模型在多种拓扑上高效合成电路，使其能够实际集成到转译工作流中，提高了量子计算电路设计的效率和通用性。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了人工智能方法在通用拓扑上排列电路的综合和转译。我们的方法使用强化学习技术实现了最多25个量子比特的排列电路的近最优综合。我们没有为单个拓扑开发专门模型，而是在通用矩形格子上训练一个基础模型，并采用掩码机制在综合过程中动态选择拓扑子集。这使得能够在嵌入矩形格子的任何拓扑上合成排列电路，而无需重新训练模型。在本文中，我们展示了5x5格子的结果，并将其与之前的AI拓扑导向模型和经典方法进行了比较，结果表明它们优于经典启发式方法，与之前的专用AI模型相匹配，并且能够对训练过程中未见过的拓扑进行综合。我们还展示了模型可以微调以增强对所选感兴趣拓扑的性能。这种方法允许单个训练模型在多种拓扑上高效合成电路，使其能够实际集成到转译工作流中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates artificial intelligence (AI) methodologies for thesynthesis and transpilation of permutation circuits across generic topologies.Our approach uses Reinforcement Learning (RL) techniques to achievenear-optimal synthesis of permutation circuits up to 25 qubits. Rather thandeveloping specialized models for individual topologies, we train afoundational model on a generic rectangular lattice, and employ maskingmechanisms to dynamically select subsets of topologies during the synthesis.This enables the synthesis of permutation circuits on any topology that can beembedded within the rectangular lattice, without the need to re-train themodel. In this paper we show results for 5x5 lattice and compare them toprevious AI topology-oriented models and classical methods, showing that theyoutperform classical heuristics, and match previous specialized AI models, andperforms synthesis even for topologies that were not seen during training. Wefurther show that the model can be fine tuned to strengthen the performance forselected topologies of interest. This methodology allows a single trained modelto efficiently synthesize circuits across diverse topologies, allowing itspractical integration into transpilation workflows.</description>
      <author>example@mail.com (Victor Villar, Juan Cruz-Benito, Ismael Faro, David Kremer)</author>
      <guid isPermaLink="false">2509.16020v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching</title>
      <link>http://arxiv.org/abs/2509.16017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DistillMatch的多模态图像匹配方法，通过从视觉基础模型(VFM)进行知识蒸馏，构建轻量级学生模型，提取高级语义特征并注入模态类别信息，同时设计V2I-GAN进行数据增强，有效解决了不同模态间外观差异大的匹配挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态图像匹配对于跨模态感知、融合和分析至关重要，但模态间的显著外观差异使这一任务具有挑战性。由于高质量标注数据集稀缺，现有的深度学习方法表现不佳且缺乏对多样化场景的适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理不同模态间显著外观差异的多模态图像匹配方法，提高模型的泛化能力和鲁棒性，使其能够适应多样化场景。&lt;h4&gt;方法&lt;/h4&gt;提出DistillMatch方法，采用知识蒸馏技术构建轻量级学生模型，从VFM(包括DINOv2和DINOv3)提取高级语义特征辅助跨模态匹配。提取并注入模态类别信息以保留模态特定信息，设计V2I-GAN将可见图像转换为伪红外图像进行数据增强，提高模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，DistillMatch在公共数据集上的性能优于现有算法，证明了其在多模态图像匹配任务中的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;DistillMatch通过知识蒸馏和模态信息注入，能够更好地理解跨模态相关性，结合V2I-GAN数据进一步增强模型泛化能力，为多模态图像匹配提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态图像匹配寻求不同模态图像间的像素级对应关系，这对于跨模态感知、融合和分析至关重要。然而，模态之间的显著外观差异使这一任务具有挑战性。由于高质量标注数据集的稀缺，现有的深度学习方法表现不佳且缺乏对多样化场景的适应性。视觉基础模型(VFM)在大规模数据上训练，产生可泛化且鲁棒的特征表示，适应各种模态的数据和任务，包括多模态匹配。因此，我们提出了DistillMatch，一种使用VFM知识蒸馏的多模态图像匹配方法。DistillMatch采用知识蒸馏构建轻量级学生模型，从VFM(包括DINOv2和DINOv3)提取高级语义特征以辅助跨模态匹配。为保留模态特定信息，它提取并将模态类别信息注入到另一模态的特征中，增强了模型对跨模态相关性的理解。此外，我们设计了V2I-GAN，通过将可见图像转换为伪红外图像进行数据增强，提高模型的泛化能力。实验表明，DistillMatch在公共数据集上优于现有算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal image matching seeks pixel-level correspondences between images ofdifferent modalities, crucial for cross-modal perception, fusion and analysis.However, the significant appearance differences between modalities make thistask challenging. Due to the scarcity of high-quality annotated datasets,existing deep learning methods that extract modality-common features formatching perform poorly and lack adaptability to diverse scenarios. VisionFoundation Model (VFM), trained on large-scale data, yields generalizable androbust feature representations adapted to data and tasks of various modalities,including multimodal matching. Thus, we propose DistillMatch, a multimodalimage matching method using knowledge distillation from VFM. DistillMatchemploys knowledge distillation to build a lightweight student model thatextracts high-level semantic features from VFM (including DINOv2 and DINOv3) toassist matching across modalities. To retain modality-specific information, itextracts and injects modality category information into the other modality'sfeatures, which enhances the model's understanding of cross-modal correlations.Furthermore, we design V2I-GAN to boost the model's generalization bytranslating visible to pseudo-infrared images for data augmentation.Experiments show that DistillMatch outperforms existing algorithms on publicdatasets.</description>
      <author>example@mail.com (Meng Yang, Fan Fan, Zizhuo Li, Songchu Deng, Yong Ma, Jiayi Ma)</author>
      <guid isPermaLink="false">2509.16017v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Wireless Channel Foundation Model with Embedded Noise-Plus-Interference Suppression Structure</title>
      <link>http://arxiv.org/abs/2509.15993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种具有噪声加干扰抑制能力的增强型无线信道基础模型架构，解决了实际系统中完美信道信息状态不可用的问题，通过NPI估计和减法模块以及CSI完成网络获取干净版本的CSI用于特征提取，在信道预测任务上实现了比现有方法更好的性能。&lt;h4&gt;背景&lt;/h4&gt;无线信道基础模型(WCFM)是一种在大规模无线信道数据集上预训练的任务无关AI模型，可用于通信和感知相关的各种下游任务。现有WCFM研究都使用完美CSI数据进行训练，但实际系统中完美CSI不可用，只能获取降级CSI(有噪版本)，这影响了WCFM的性能。&lt;h4&gt;目的&lt;/h4&gt;解决实际系统中WCFM因无法获取完美CSI而导致的性能下降问题，提出一种能够处理噪声和干扰的增强型WCFM架构。&lt;h4&gt;方法&lt;/h4&gt;提出一种具有噪声加干扰(NPI)抑制能力的增强型无线信道基础模型架构。首先获取CSI的粗略估计，然后计算两个投影矩阵提取接收信号中的NPI项，通过NPI估计和减法模块处理，最后将结果信号通过CSI完成网络获取干净版本的CSI用于特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，与最先进的解决方案相比，具有NPI抑制结构的WCFM在信道预测任务上实现了更好的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入NPI抑制能力，增强型WCFM能够在实际系统中有效处理噪声和干扰问题，提高下游任务性能。&lt;h4&gt;翻译&lt;/h4&gt;无线信道基础模型(WCFM)是一种任务无关的AI模型，在大规模无线信道数据集上进行预训练，学习通用的信道特征表示，可用于通信和感知相关的各种下游任务。虽然现有关于WCFM的研究已展示其在波束预测、信道预测、定位等任务中的巨大潜力，但这些模型都是使用完美(即无误差且完整)的信道信息状态(CSI)数据进行训练的，这些数据是通过仿真工具生成的。然而，在实际部署WCFM的系统中，完美CSI不可用。相反，需要首先基于部分资源元素(REs)上的导频信号进行信道估计，以获取CSI的有噪版本(称为降级CSI)，这在某些噪声和干扰严重的实际环境中与完美CSI有很大差异。因此，WCFM生成的特征表示无法反映真实信道的特性，导致下游任务性能下降。为解决这一问题，本文提出了一种具有噪声加干扰(NPI)抑制能力的增强型无线信道基础模型架构。在我们的方法中，首先获取CSI的粗略估计，然后计算两个投影矩阵来提取接收信号中的NPI项，这些项进一步通过NPI估计和减法模块处理。最后，将结果信号通过CSI完成网络获取干净版本的CSI，用于特征提取。仿真结果表明，与最先进的解决方案相比，具有NPI抑制结构的WCFM在信道预测任务上实现了更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wireless channel foundation model (WCFM) is a task-agnostic AI model that ispretrained on large-scale wireless channel datasets to learn a universalchannel feature representation that can be used for a wide range of downstreamtasks related to communications and sensing. While existing works on WCFM havedemonstrated its great potentials in various tasks including beam prediction,channel prediction, localization, etc, the models are all trained using perfect(i.e., error-free and complete) channel information state (CSI) data which aregenerated with simulation tools. However, in practical systems where the WCFMis deployed, perfect CSI is not available. Instead, channel estimation needs tobe first performed based on pilot signals over a subset of the resourceelements (REs) to acquire a noisy version of the CSI (termed as degraded CSI),which significantly differs from the perfect CSI in some real-worldenvironments with severe noise and interference. As a result, the featurerepresentation generated by the WCFM is unable to reflect the characteristicsof the true channel, yielding performance degradation in downstream tasks. Toaddress this issue, in this paper we propose an enhanced wireless channelfoundation model architecture with noise-plus-interference (NPI) suppressioncapability. In our approach, coarse estimates of the CSIs are first obtained.With these information, two projection matrices are computed to extract the NPIterms in the received signals, which are further processed by a NPI estimationand subtraction module. Finally, the resultant signal is passed through a CSIcompletion network to get a clean version of the CSI, which is used for featureextraction. Simulation results demonstrated that compared to thestate-of-the-art solutions, WCFM with NPI suppression structure achievesimproved performance on channel prediction task.</description>
      <author>example@mail.com (Yuwei Wang, Li Sun, Tingting Yang)</author>
      <guid isPermaLink="false">2509.15993v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds</title>
      <link>http://arxiv.org/abs/2509.15915v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 9 figures. Accepted for presentation at the 39th Conference  on Neural Information Processing Systems (NeurIPS 2025) Workshop on Embodied  World Models for Decision Making&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了如何将基础模型整合到强化学习框架中以提高样本效率，提出了基础世界模型和基础智能体两种策略，并通过网格世界环境进行了实证评估。&lt;h4&gt;背景&lt;/h4&gt;虽然从头开始的强化学习在解决具有高效模拟器的顺序决策任务方面已显示出令人印象深刻的结果，但现实世界应用中由于交互成本高，需要更高效的样本智能体。&lt;h4&gt;目的&lt;/h4&gt;研究如何有效地将基础模型整合到强化学习框架中，以提高样本效率。&lt;h4&gt;方法&lt;/h4&gt;提出并评估两种策略：一是使用基础世界模型(FWMs)，利用基础模型的先验知识进行智能体的训练和评估；二是使用基础智能体(FAs)，利用基础模型的推理能力进行决策。在适合当前大型语言模型的网格世界环境中进行实证评估。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型的改进已经转化为更好的基础世界模型和基础智能体；基于当前大型语言模型的基础智能体可以为足够简单的环境提供优秀的策略；基础世界模型与强化学习智能体的结合在具有部分可观测性和随机性的复杂环境中非常有前景。&lt;h4&gt;结论&lt;/h4&gt;基础模型可以有效地整合到强化学习框架中，提高样本效率，为现实世界应用提供了有希望的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;虽然从头开始的强化学习在解决具有高效模拟器的顺序决策任务方面已经显示出令人印象深刻的结果，但现实世界应用中由于交互成本高，需要更高效的样本智能体。基础模型是提高样本效率的自然候选，因为它们拥有广泛的知识和推理能力，但如何将它们有效地整合到强化学习框架中尚不清楚。在本文中，我们预期并评估了两种有前景的策略。首先，我们考虑使用基础世界模型，利用基础模型的先验知识来实现智能体的训练和评估。其次，我们考虑使用基础智能体，利用基础模型的推理能力进行决策。我们在适合当前大型语言模型的一类网格世界环境中对这两种方法进行了实证评估。我们的结果表明，大型语言模型的改进已经转化为更好的基础世界模型和基础智能体；基于当前大型语言模型的基础智能体已经可以为足够简单的环境提供优秀的策略；基础世界模型与强化学习智能体的结合在具有部分可观测性和随机性的更复杂环境中非常有前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While reinforcement learning from scratch has shown impressive results insolving sequential decision-making tasks with efficient simulators, real-worldapplications with expensive interactions require more sample-efficient agents.Foundation models (FMs) are natural candidates to improve sample efficiency asthey possess broad knowledge and reasoning capabilities, but it is yet unclearhow to effectively integrate them into the reinforcement learning framework. Inthis paper, we anticipate and, most importantly, evaluate two promisingstrategies. First, we consider the use of foundation world models (FWMs) thatexploit the prior knowledge of FMs to enable training and evaluating agentswith simulated interactions. Second, we consider the use of foundation agents(FAs) that exploit the reasoning capabilities of FMs for decision-making. Weevaluate both approaches empirically in a family of grid-world environmentsthat are suitable for the current generation of large language models (LLMs).Our results suggest that improvements in LLMs already translate into betterFWMs and FAs; that FAs based on current LLMs can already provide excellentpolicies for sufficiently simple environments; and that the coupling of FWMsand reinforcement learning agents is highly promising for more complex settingswith partial observability and stochastic elements.</description>
      <author>example@mail.com (Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber)</author>
      <guid isPermaLink="false">2509.15915v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>ENSAM: an efficient foundation model for interactive segmentation of 3D medical images</title>
      <link>http://arxiv.org/abs/2509.15874v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ENSAM是一个轻量级、可提示的通用3D医学图像分割模型，具有等变性和归一化特性，结合了SegResNet编码器、提示编码器和掩码解码器，采用U-Net架构。&lt;h4&gt;背景&lt;/h4&gt;在CVPR 2025交互式3D生物医学图像分割基础模型挑战中，需要处理多模态3D医学图像分割任务。&lt;h4&gt;目的&lt;/h4&gt;设计一个在有限数据和计算预算下表现良好的3D医学图像分割模型。&lt;h4&gt;方法&lt;/h4&gt;ENSAM使用基于SegResNet的编码器与提示编码器和掩码解码器结合，采用潜在交叉注意力、相对位置编码、归一化注意力和Muon优化器进行训练。模型从零开始，使用不到5000个体积的多模态数据(CT、MRI、PET、超声、显微镜)，在单个32GB GPU上6小时内完成训练。&lt;h4&gt;主要发现&lt;/h4&gt;在隐藏测试集上，ENSAM获得了DSC AUC为2.404，NSD AUC为2.266，最终DSC为0.627，最终NSD为0.597，优于两个基线模型(VISTA3D、SAM-Med3D)，与第三个基线模型(SegVol)相当。在挑战的核心集轨道中，ENSAM在不使用预训练权重的方法中表现最佳。&lt;h4&gt;结论&lt;/h4&gt;ENSAM是一个高效的3D医学图像分割模型，在有限资源和数据条件下表现良好，不需要预训练权重就能取得优异性能。&lt;h4&gt;翻译&lt;/h4&gt;ENSAM(等变性、归一化、分割任何模型)是一个轻量级和可提示的通用3D医学图像分割模型。ENSAM将基于SegResNet的编码器与提示编码器和掩码解码器结合在U-Net风格架构中，使用潜在交叉注意力、相对位置编码、归一化注意力和Muon优化器进行训练。ENSAM旨在在有限数据和计算预算下实现良好性能，并在单个32GB GPU上使用不到5000个体积的多模态数据从零开始训练，耗时6小时。作为挑战的一部分，ENSAM在多模态3D医学图像的隐藏测试集上进行了评估，获得了优于两个先前发布的基线模型的性能，并与第三个基线模型相当。消融研究证实，相对位置编码和Muon优化器的使用显著加速了收敛并提高了分割质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present ENSAM (Equivariant, Normalized, Segment Anything Model), alightweight and promptable model for universal 3D medical image segmentation.ENSAM combines a SegResNet-based encoder with a prompt encoder and mask decoderin a U-Net-style architecture, using latent cross-attention, relativepositional encoding, normalized attention, and the Muon optimizer for training.ENSAM is designed to achieve good performance under limited data andcomputational budgets, and is trained from scratch on under 5,000 volumes frommultiple modalities (CT, MRI, PET, ultrasound, microscopy) on a single 32 GBGPU in 6 hours. As part of the CVPR 2025 Foundation Models for Interactive 3DBiomedical Image Segmentation Challenge, ENSAM was evaluated on hidden test setwith multimodal 3D medical images, obtaining a DSC AUC of 2.404, NSD AUC of2.266, final DSC of 0.627, and final NSD of 0.597, outperforming two previouslypublished baseline models (VISTA3D, SAM-Med3D) and matching the third (SegVol),surpassing its performance in final DSC but trailing behind in the other threemetrics. In the coreset track of the challenge, ENSAM ranks 5th of 10 overalland best among the approaches not utilizing pretrained weights. Ablationstudies confirm that our use of relative positional encodings and the Muonoptimizer each substantially speed up convergence and improve segmentationquality.</description>
      <author>example@mail.com (Elias Stenhede, Agnar Martin Bjørnstad, Arian Ranjbar)</author>
      <guid isPermaLink="false">2509.15874v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data</title>
      <link>http://arxiv.org/abs/2509.15859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Curated Data for Efficient Learning Workshop at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种利用视觉基础模型生成合成数据并训练简单线性分类器的新框架，用于解决长尾分类问题，在保持高性能的同时显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;不平衡分类数据集在机器学习中构成重大挑战，导致模型在少数类上表现不佳。尽管基础模型在长尾分类上有所进展，但仍无法达到平衡数据集训练的水平，且计算资源消耗大。&lt;h4&gt;目的&lt;/h4&gt;强调计算效率和简单性的重要性，提出一种利用视觉基础模型语义潜在空间生成合成数据，并结合真实数据训练简单线性分类器的方法。&lt;h4&gt;方法&lt;/h4&gt;利用视觉基础模型的丰富语义潜在空间生成合成数据，使用真实和合成数据的混合训练一个简单的线性分类器，将可训练参数减少到仅线性模型的参数数量。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-100-LT基准测试上设定了新的最先进水平，在Places-LT基准测试上展示了强大性能，证明了简单有效方法的高效性和适应性。&lt;h4&gt;结论&lt;/h4&gt;所提出的简单而有效的方法在长尾分类任务上表现优异，同时大大降低了计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;不平衡分类数据集在机器学习中构成重大挑战，通常导致模型在代表性不足的类别上表现不佳。随着基础模型的兴起，最近的研究已关注对这些模型进行全面、部分和参数高效的微调以处理长尾分类。尽管这些工作在基准数据集上表现出色，但仍无法缩小与使用平衡数据集训练的网络之间的差距，即使对于相对较小的数据集，仍然需要大量计算资源。强调计算效率和简单性的重要性，在这项工作中，我们提出了一个新框架，利用视觉基础模型的丰富语义潜在空间生成合成数据，并使用真实和合成数据的混合训练一个简单的线性分类器进行长尾分类。计算效率的提升来自于可训练参数的数量减少到仅线性模型中的参数数量。我们的方法在CIFAR-100-LT基准测试上设定了新的最先进水平，并在Places-LT基准测试上展示了强大的性能，突显了我们简单有效方法的有效性和适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imbalanced classification datasets pose significant challenges in machinelearning, often leading to biased models that perform poorly onunderrepresented classes. With the rise of foundation models, recent researchhas focused on the full, partial, and parameter-efficient fine-tuning of thesemodels to deal with long-tail classification. Despite the impressiveperformance of these works on the benchmark datasets, they still fail to closethe gap with the networks trained using the balanced datasets and still requiresubstantial computational resources, even for relatively smaller datasets.Underscoring the importance of computational efficiency and simplicity, in thiswork we propose a novel framework that leverages the rich semantic latent spaceof Vision Foundation Models to generate synthetic data and train a simplelinear classifier using a mixture of real and synthetic data for long-tailclassification. The computational efficiency gain arises from the number oftrainable parameters that are reduced to just the number of parameters in thelinear model. Our method sets a new state-of-the-art for the CIFAR-100-LTbenchmark and demonstrates strong performance on the Places-LT benchmark,highlighting the effectiveness and adaptability of our simple and effectiveapproach.</description>
      <author>example@mail.com (Nakul Sharma)</author>
      <guid isPermaLink="false">2509.15859v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation</title>
      <link>http://arxiv.org/abs/2509.15795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为TASAM的新型遥感图像分割模型，该模型是对Segment Anything Model (SAM)的改进，专门针对高分辨率遥感图像分割任务进行了优化。&lt;h4&gt;背景&lt;/h4&gt;SAM在自然图像领域展示了令人印象深刻的零样本分割能力，但难以推广到遥感数据的独特挑战，如复杂地形、多尺度目标和时间动态性。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门为高分辨率遥感图像分割设计的地形和时间感知的SAM扩展模型。&lt;h4&gt;方法&lt;/h4&gt;TASAM集成了三个轻量而有效的模块：地形感知适配器（注入高程先验）、时间提示生成器（捕捉土地覆盖随时间变化）和多尺度融合策略（增强细粒度目标 delineation），且不重新训练SAM主干网络。&lt;h4&gt;主要发现&lt;/h4&gt;在三个遥感基准测试（LoveDA、iSAID和WHU-CD）上实现了显著的性能提升，超越了零样本SAM和特定任务模型，同时保持了最小的计算开销。&lt;h4&gt;结论&lt;/h4&gt;强调了基础模型领域自适应增强的价值，为更强大的地理空间分割提供了可扩展的路径。&lt;h4&gt;翻译&lt;/h4&gt;分割任何模型（SAM）在自然图像领域展示了令人印象深刻的零样本分割能力，但它难以推广到遥感数据的独特挑战，如复杂地形、多尺度目标和时间动态性。在本文中，我们引入了TASAM，这是SAM的一个地形和时间感知的扩展，专门为高分辨率遥感图像分割而设计。TASAM集成了三个轻量而有效的模块：一个注入高程先验的地形感知适配器，一个捕捉土地覆盖随时间变化的时间提示生成器，以及一个增强细粒度目标 delineation 的多尺度融合策略。在不重新训练SAM主干网络的情况下，我们的方法在三个遥感基准测试（LoveDA、iSAID和WHU-CD）上实现了显著的性能提升，超越了零样本SAM和特定任务模型，同时计算开销最小。我们的结果突显了基础模型领域自适应增强的价值，并为更强大的地理空间分割提供了可扩展的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segment Anything Model (SAM) has demonstrated impressive zero-shotsegmentation capabilities across natural image domains, but it struggles togeneralize to the unique challenges of remote sensing data, such as complexterrain, multi-scale objects, and temporal dynamics. In this paper, weintroduce TASAM, a terrain and temporally-aware extension of SAM designedspecifically for high-resolution remote sensing image segmentation. TASAMintegrates three lightweight yet effective modules: a terrain-aware adapterthat injects elevation priors, a temporal prompt generator that capturesland-cover changes over time, and a multi-scale fusion strategy that enhancesfine-grained object delineation. Without retraining the SAM backbone, ourapproach achieves substantial performance gains across three remote sensingbenchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM andtask-specific models with minimal computational overhead. Our results highlightthe value of domain-adaptive augmentation for foundation models and offer ascalable path toward more robust geospatial segmentation.</description>
      <author>example@mail.com (Tianyang Wang, Xi Xiao, Gaofei Chen, Hanzhang Chi, Qi Zhang, Guo Cheng, Yingrui Ji)</author>
      <guid isPermaLink="false">2509.15795v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models</title>
      <link>http://arxiv.org/abs/2509.15607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PRIMT框架，通过基础模型解决偏好强化学习中的挑战，提高机器人学习复杂行为的效率&lt;h4&gt;背景&lt;/h4&gt;偏好强化学习(PbRL)是一种有前景的范式，可以教会机器人复杂行为而无需奖励工程，但其有效性常受限于依赖大量人工输入和解决查询模糊性及信用分配的困难&lt;h4&gt;目的&lt;/h4&gt;开发PRIMT框架，利用基础模型进行多模态合成反馈和轨迹合成，以克服传统PbRL方法的局限性&lt;h4&gt;方法&lt;/h4&gt;PRIMT采用分层神经符号融合策略，整合大型语言模型和视觉语言模型的优势评估机器人行为；包含前瞻性轨迹生成减少早期查询模糊性；以及后顾轨迹增强通过反事实推理改进信用分配&lt;h4&gt;主要发现&lt;/h4&gt;在2个运动和6个操作任务上的评估表明，PRIMT性能优于基于基础模型和脚本的基线方法&lt;h4&gt;结论&lt;/h4&gt;PRIMT框架有效解决了传统偏好强化学习中的关键挑战，为机器人复杂行为学习提供了更可靠的方法&lt;h4&gt;翻译&lt;/h4&gt;基于偏好的强化学习(PbRL)已成为一种有前景的范式，可以在没有奖励工程的情况下教会机器人复杂行为。然而，其有效性通常受到两个关键挑战的限制：依赖大量人工输入以及在奖励学习过程中解决查询模糊性和信用分配的固有困难。在本文中，我们介绍了PRIMT，一个PbRL框架，旨在通过利用基础模型(FMs)进行多模态合成反馈和轨迹合成来克服这些挑战。与依赖单模态FM评估的先前方法不同，PRIMT采用分层神经符号融合策略，整合大型语言模型和视觉语言模型的优势，以评估机器人行为，提供更可靠和全面的反馈。PRIMT还包含前瞻性轨迹生成，通过自举样本预热轨迹缓冲区，减少早期阶段的查询模糊性；以及后顾轨迹增强，通过因果辅助损失实现反事实推理，以改进信用分配。我们在各种基准测试的2个运动和6个操作任务上评估了PRIMT，展示了其优于基于FM和脚本的基线的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Preference-based reinforcement learning (PbRL) has emerged as a promisingparadigm for teaching robots complex behaviors without reward engineering.However, its effectiveness is often limited by two critical challenges: thereliance on extensive human input and the inherent difficulties in resolvingquery ambiguity and credit assignment during reward learning. In this paper, weintroduce PRIMT, a PbRL framework designed to overcome these challenges byleveraging foundation models (FMs) for multimodal synthetic feedback andtrajectory synthesis. Unlike prior approaches that rely on single-modality FMevaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy,integrating the complementary strengths of large language models andvision-language models in evaluating robot behaviors for more reliable andcomprehensive feedback. PRIMT also incorporates foresight trajectorygeneration, which reduces early-stage query ambiguity by warm-starting thetrajectory buffer with bootstrapped samples, and hindsight trajectoryaugmentation, which enables counterfactual reasoning with a causal auxiliaryloss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6manipulation tasks on various benchmarks, demonstrating superior performanceover FM-based and scripted baselines.</description>
      <author>example@mail.com (Ruiqi Wang, Dezhong Zhao, Ziqin Yuan, Tianyu Shao, Guohua Chen, Dominic Kao, Sungeun Hong, Byung-Cheol Min)</author>
      <guid isPermaLink="false">2509.15607v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Lynx: Towards High-Fidelity Personalized Video Generation</title>
      <link>http://arxiv.org/abs/2509.15496v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Lynx Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Lynx是一个高保真模型，用于从单张输入图像生成个性化视频。它基于开源的扩散变换器(DiT)基础模型，引入了两个轻量级适配器确保身份保真度。在基准测试中，Lynx展示了出色的面部相似性、提示遵循能力和视频质量，推进了个性化视频生成技术。&lt;h4&gt;背景&lt;/h4&gt;个性化视频生成需要从单张输入图像生成高质量且保持身份一致性的视频，这是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从单张输入图像生成高质量个性化视频的模型，同时确保身份保真度、时间一致性和视觉真实感。&lt;h4&gt;方法&lt;/h4&gt;Lynx构建在开源的扩散变换器(DiT)基础模型上，引入了两个轻量级适配器：1) ID适配器使用Perceiver Resampler将基于ArcFace的面部嵌入转换为紧凑的身份标记；2) Ref适配器集成密集VAE特征，通过交叉注意力将细粒度细节注入到所有transformer层中。&lt;h4&gt;主要发现&lt;/h4&gt;在包含40个受试者和20个无偏见提示的基准测试中(共800个测试案例)，Lynx表现出卓越的面部相似性、有竞争力的提示遵循能力和强大的视频质量。&lt;h4&gt;结论&lt;/h4&gt;Lynx通过两个轻量级适配器的协同工作，能够在保持身份保真度的同时维持时间一致性和视觉真实感，从而推进了个性化视频生成的技术水平。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Lynx，一个用于从单张输入图像进行个性化视频合成的高保真模型。Lynx构建在一个开源的扩散变换器(DiT)基础模型之上，引入了两个轻量级适配器来确保身份保真度。ID适配器采用Perceiver Resampler将基于ArcFace的面部嵌入转换为紧凑的身份标记用于条件化，而Ref适配器集成了来自冻结参考路径的密集VAE特征，通过交叉注意力将细粒度细节注入到所有transformer层中。这些模块共同实现了强大的身份保持，同时保持时间一致性和视觉真实感。通过对包含40个受试者和20个无偏见提示的精选基准进行评估(产生了800个测试案例)，Lynx展示了卓越的面部相似性、有竞争力的提示遵循能力和强大的视频质量，从而推进了个性化视频生成的技术水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Lynx, a high-fidelity model for personalized video synthesis froma single input image. Built on an open-source Diffusion Transformer (DiT)foundation model, Lynx introduces two lightweight adapters to ensure identityfidelity. The ID-adapter employs a Perceiver Resampler to convertArcFace-derived facial embeddings into compact identity tokens forconditioning, while the Ref-adapter integrates dense VAE features from a frozenreference pathway, injecting fine-grained details across all transformer layersthrough cross-attention. These modules collectively enable robust identitypreservation while maintaining temporal coherence and visual realism. Throughevaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, whichyielded 800 test cases, Lynx has demonstrated superior face resemblance,competitive prompt following, and strong video quality, thereby advancing thestate of personalized video generation.</description>
      <author>example@mail.com (Shen Sang, Tiancheng Zhi, Tianpei Gu, Jing Liu, Linjie Luo)</author>
      <guid isPermaLink="false">2509.15496v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training</title>
      <link>http://arxiv.org/abs/2509.15416v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种专门针对神经肿瘤学的基础模型，结合分布鲁棒优化技术，提高了分子标记物预测准确性和跨机构泛化能力，同时改善了生存预测性能。&lt;h4&gt;背景&lt;/h4&gt;神经肿瘤学因数据异质性和肿瘤复杂性对机器学习构成挑战，导致基础模型难以在不同队列中泛化，且在预测不常见分子标记物方面表现不佳，而这些标记物对治疗反应和风险分层至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种神经肿瘤学特定的基础模型，采用分布鲁棒损失函数，实现准确肿瘤表型估计的同时保持跨机构泛化能力。&lt;h4&gt;方法&lt;/h4&gt;在多机构脑肿瘤MRI数据上预训练自监督主干网络(BYOL, DINO, MAE, MoCo)，应用分布鲁棒优化(DRO)减轻站点和类别不平衡；在UCSF、UPenn和CUIMC机构测试常见分子标记物分类、不常见改变分类、连续标记物预测及IDH1野生型胶质母细胞瘤生存预测。&lt;h4&gt;主要发现&lt;/h4&gt;模型改进了分子预测并减少了站点特定嵌入差异；在CUIMC，平均平衡准确率从0.744提高到0.785，AUC从0.656提高到0.676，代表性不足端点提升显著；所有站点的生存预测c-index均有改善；Grad-CAM验证了模型的可解释性。&lt;h4&gt;结论&lt;/h4&gt;将基础模型与分布鲁棒优化结合可产生更多站点不变的表示，改善常见和不常见标记物预测，增强生存判别能力，强调了前瞻性验证及整合纵向和干预信号的必要性。&lt;h4&gt;翻译&lt;/h4&gt;神经肿瘤学由于其数据的异质性和肿瘤的复杂性，为机器学习带来了独特挑战，限制了基础模型在不同队列中泛化的能力。现有的FMs在预测不常见的分子标记物方面也表现不佳，而这些标记物对于治疗反应和风险分层至关重要。为了解决这些差距，我们开发了一个专门针对神经肿瘤学的FM，具有分布鲁棒损失函数，能够在保持跨机构泛化能力的同时准确估计肿瘤表型。我们在多机构脑肿瘤MRI上预训练了自监督主干网络，并应用分布鲁棒优化来减轻站点和类别不平衡问题。我们的方法改进了分子预测并减少了站点特定的嵌入差异，所有站点的生存预测性能都有所提升，验证了模型的可解释性。总体而言，将FMs与DRO结合可以产生更多站点不变的表示，改善预测能力，推进精准神经肿瘤学发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neuro-oncology poses unique challenges for machine learning due toheterogeneous data and tumor complexity, limiting the ability of foundationmodels (FMs) to generalize across cohorts. Existing FMs also perform poorly inpredicting uncommon molecular markers, which are essential for treatmentresponse and risk stratification. To address these gaps, we developed aneuro-oncology specific FM with a distributionally robust loss function,enabling accurate estimation of tumor phenotypes while maintainingcross-institution generalization. We pretrained self-supervised backbones(BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applieddistributionally robust optimization (DRO) to mitigate site and classimbalance. Downstream tasks included molecular classification of common markers(MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT),continuous markers (Ki-67, TP53), and overall survival prediction in IDH1wild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecularprediction and reduced site-specific embedding differences. At CUIMC, meanbalanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, withthe largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69).For survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647to 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoralregions, confirming interpretability. Overall, coupling FMs with DRO yieldsmore site-invariant representations, improves prediction of common and uncommonmarkers, and enhances survival discrimination, underscoring the need forprospective validation and integration of longitudinal and interventionalsignals to advance precision neuro-oncology.</description>
      <author>example@mail.com (Moinak Bhattacharya, Angelica P. Kurtz, Fabio M. Iwamoto, Prateek Prasanna, Gagandeep Singh)</author>
      <guid isPermaLink="false">2509.15416v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</title>
      <link>http://arxiv.org/abs/2509.15221v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ScaleCUA是一个大规模、开源的计算机使用代理项目，通过构建跨平台数据集和训练模型，实现了在多个操作系统和任务领域上的自主GUI操作能力。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型(VLMs)使计算机使用代理能够自主操作GUI，显示出巨大潜力，但进展受到缺乏大规模开源计算机使用数据和基础模型的限制。&lt;h4&gt;目的&lt;/h4&gt;引入ScaleCUA，朝着扩展开源计算机使用代理的方向迈进，解决数据稀缺问题。&lt;h4&gt;方法&lt;/h4&gt;ScaleCUA提供了一个跨越6个操作系统和3个任务领域的大规模数据集，通过结合自动化代理和人类专家的闭环流程构建。基于这些扩展数据训练的ScaleCUA能够跨平台无缝运行。&lt;h4&gt;主要发现&lt;/h4&gt;ScaleCUA在多个基准测试中取得了显著成果：在WebArena-Lite-v2上比基线提高26.6，在ScreenSpot-Pro上提高10.7；在MMBench-GUI L1-Hard上达到94.4%的最先进结果，在OSWorld-G上达到60.6%，在WebArena-Lite-v2上达到47.4%。&lt;h4&gt;结论&lt;/h4&gt;数据驱动的扩展对通用计算机使用代理的强大功效得到了这些发现的证明。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)已经使计算机使用代理(CUAs)能够自主操作图形用户界面(GUI)，显示出巨大潜力，但进展受到缺乏大规模开源计算机使用数据和基础模型的限制。在这项工作中，我们引入了ScaleCUA，这是朝着扩展开源计算机使用代理迈出的一步。它提供了一个跨越6个操作系统和3个任务领域的大规模数据集，通过结合自动化代理和人类专家的闭环流程构建。基于这种扩展数据训练的ScaleCUA能够跨平台无缝运行。具体而言，它在多个基准测试中对比基线取得了显著提升(+26.6在WebArena-Lite-v2上，+10.7在ScreenSpot-Pro上)，并设定了新的最先进结果(94.4%在MMBench-GUI L1-Hard上，60.6%在OSWorld-G上，47.4%在WebArena-Lite-v2上)。这些发现证明了数据驱动扩展对通用计算机使用代理的强大功效。我们将发布数据、模型和代码以促进未来研究：https://github.com/OpenGVLab/ScaleCUA。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have enabled computer use agents (CUAs) thatoperate GUIs autonomously, showing great potential, yet progress is limited bythe lack of large-scale, open-source computer use data and foundation models.In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. Itoffers a large-scale dataset spanning 6 operating systems and 3 task domains,built via a closed-loop pipeline uniting automated agents with human experts.Trained on this scaled-up data, ScaleCUA can operate seamlessly acrossplatforms. Specifically, it delivers strong gains over baselines (+26.6 onWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-artresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% onWebArena-Lite-v2). These findings underscore the power of data-driven scalingfor general-purpose computer use agents. We will release data, models, and codeto advance future research: https://github.com/OpenGVLab/ScaleCUA.</description>
      <author>example@mail.com (Zhaoyang Liu, Jingjing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Xuan Dong, Yue Yu, Chenyu Lu, YunXiang Mo, Yao Yan, Zeyue Tian, Xiao Zhang, Yuan Huang, Yiqian Liu, Weijie Su, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang)</author>
      <guid isPermaLink="false">2509.15221v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>How Good are Foundation Models in Step-by-Step Embodied Reasoning?</title>
      <link>http://arxiv.org/abs/2509.15293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了FoMER基准测试，用于评估大型多模态模型在复杂具身决策场景中的推理能力。研究涵盖了10个任务、8种具身形态和3种机器人类型，包含超过1.1k个带有详细逐步推理的样本。研究结果揭示了LMMs在具身推理方面的潜力和局限性。&lt;h4&gt;背景&lt;/h4&gt;物理世界中的具身智能体需要做出不仅有效而且安全、空间连贯且基于上下文的决策。尽管大型多模态模型在视觉理解和语言生成方面显示出有前景的能力，但它们在执行现实世界具身任务的结构化推理能力仍未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究旨在了解基础模型在具身环境中进行逐步推理的能力如何。&lt;h4&gt;方法&lt;/h4&gt;研究提出了Foundation Model Embodied Reasoning (FoMER)基准测试，用于评估LMMs在复杂具身决策场景中的推理能力。该基准测试包括：(i)大规模精选的具身推理任务套件，(ii)一种将感知基础与动作推理分离的新型评估框架，(iii)在此设置下对几个领先LMMs的经验分析。基准测试包含超过1.1k个样本，涵盖10个任务、8种具身形态和3种机器人类型。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果揭示了LMMs在具身推理方面的潜力和当前局限性，指出了机器人智能未来研究的关键挑战和机遇。&lt;h4&gt;结论&lt;/h4&gt;研究的数据和代码将公开可用，为未来机器人智能研究提供了重要资源。&lt;h4&gt;翻译&lt;/h4&gt;在物理世界中运行的具身智能体必须做出的决策不仅要有效，还要安全、空间连贯且基于上下文。尽管大型多模态模型在视觉理解和语言生成方面的最新进展显示出有前景的能力，但它们在执行现实世界具身任务的结构化推理能力仍未被充分探索。在这项工作中，我们旨在了解基础模型在具身环境中进行逐步推理的能力如何。为此，我们提出了Foundation Model Embodied Reasoning (FoMER)基准测试，旨在评估LMMs在复杂具身决策场景中的推理能力。我们的基准测试涵盖了多样化的任务集，要求智能体解释多模态观察、对物理约束和安全进行推理，并用自然语言生成有效的下一个动作。我们提出了(i)大规模精选的具身推理任务套件，(ii)一种将感知基础与动作推理分离的新型评估框架，以及(iii)在此设置下对几个领先LMMs的经验分析。我们的基准测试包含超过1.1k个样本，涵盖10个任务、8种具身形态和3种机器人类型，并包含详细的逐步推理。我们的结果突显了LMMs在具身推理方面的潜力和当前局限性，指出了机器人智能未来研究的关键挑战和机遇。我们的数据和代码将公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要想解决评估基础模型（特别是大型多模态模型）在逐步具身推理方面的能力问题。这个问题很重要，因为机器人等具身智能体需要在物理世界中做出有效、安全、空间连贯且基于上下文的决策，而当前对这些模型在真实世界具身任务中的推理能力还缺乏系统了解，特别是对它们推理过程的评估不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有具身推理基准的局限性，特别是缺乏对推理过程的详细评估。他们借鉴了Cosmos-R1等物理推理基准和VRC-Bench等对推理轨迹进行评估的方法，同时参考了现有的机器人数据集。作者设计了一个新的评估框架，不仅评估最终答案，还评估推理过程的质量，并创建了包含10种不同任务类型和8种具身形态的多样化基准测试集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是不仅要评估模型给出的最终答案是否正确，还要评估其推理过程的质量和合理性，特别是在考虑空间关系、物理约束、安全性和任务对齐等具身推理关键因素的情况下。整体流程包括：1)从多个数据集构建包含QA对和推理轨迹的基准测试；2)设计10个评估标准的评估框架；3)使用LLM-as-judge方法进行自动评估；4)对9个先进模型进行测试和比较，分析它们在不同任务和问题类型上的表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出FoMER基准测试，包含1100+样本和10种任务类型；2)设计新评估框架，同时评估答案正确性和推理质量；3)提供对9个最先进模型的全面分析。相比之前工作，FoMER明确包含推理轨迹评估，专注于具身环境中的物理推理，提供更广泛的任务和机器人类型覆盖，使用更细致的评估标准关注具身推理特有的挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了FoMER基准测试和评估框架，首次全面评估了大型多模态模型在具身推理任务中的能力，揭示了当前模型在复杂物理世界推理中的优势和局限，为未来具身智能研究提供了重要参考。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied agents operating in the physical world must make decisions that arenot only effective but also safe, spatially coherent, and grounded in context.While recent advances in large multimodal models (LMMs) have shown promisingcapabilities in visual understanding and language generation, their ability toperform structured reasoning for real-world embodied tasks remainsunderexplored. In this work, we aim to understand how well foundation modelscan perform step-by-step reasoning in embodied environments. To this end, wepropose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed toevaluate the reasoning capabilities of LMMs in complex embodied decision-makingscenarios. Our benchmark spans a diverse set of tasks that require agents tointerpret multimodal observations, reason about physical constraints andsafety, and generate valid next actions in natural language. We present (i) alarge-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluationframework that disentangles perceptual grounding from action reasoning, and(iii) empirical analysis of several leading LMMs under this setting. Ourbenchmark includes over 1.1k samples with detailed step-by-step reasoningacross 10 tasks and 8 embodiments, covering three different robot types. Ourresults highlight both the potential and current limitations of LMMs inembodied reasoning, pointing towards key challenges and opportunities forfuture research in robot intelligence. Our data and code will be made publiclyavailable.</description>
      <author>example@mail.com (Dinura Dissanayake, Ahmed Heakl, Omkar Thawakar, Noor Ahsan, Ritesh Thawkar, Ketan More, Jean Lahoud, Rao Anwer, Hisham Cholakkal, Ivan Laptev, Fahad Shahbaz Khan, Salman Khan)</author>
      <guid isPermaLink="false">2509.15293v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers</title>
      <link>http://arxiv.org/abs/2509.16058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了基于注意力模式的注意力控制(ASAC)模块，将认知科学中的注意力模式理论整合到人工神经网络中，特别是在Transformer架构中。通过使用向量量化变分自编码器作为注意力抽象器和控制器，ASAC能够精确管理注意力分配，提高系统效率。&lt;h4&gt;背景&lt;/h4&gt;注意力机制已成为AI的重要组成部分，显著提高了模型性能和可扩展性。同时，认知科学中的注意力模式理论(AST)认为，个体通过创建注意力模型来管理注意力，从而有效分配认知资源。&lt;h4&gt;目的&lt;/h4&gt;将注意力模式概念整合到人工神经网络中，创建基于注意力模式的注意力控制(ASAC)，通过明确建模注意力分配来提高系统效率。&lt;h4&gt;方法&lt;/h4&gt;引入ASAC模块，在Transformer架构中嵌入该模块，使用向量量化变分自编码器(VQVAE)作为注意力抽象器和控制器，在视觉和NLP领域进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;ASAC提高了分类准确性并加速学习过程；模型在各种数据集上具有鲁棒性和泛化能力；在多任务设置中表现更好；增强了对对抗性攻击的抵抗力；优化了注意力以提高学习效率；促进了有效的迁移学习和少样本学习。&lt;h4&gt;结论&lt;/h4&gt;这些有希望的结果建立了认知科学与机器学习之间的联系，为AI系统中注意力机制的有效利用提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;注意力机制已成为AI的重要组成部分，通过借鉴人类认知显著提高了模型性能和可扩展性。同时，认知科学中的注意力模式理论(AST)提出，个体通过创建注意力模型来管理注意力，从而有效分配认知资源。受AST启发，我们引入了基于注意力模式的注意力控制(ASAC)，将注意力模式概念整合到人工神经网络中。我们的初步实验专注于在Transformer架构中嵌入ASAC模块。该模块使用向量量化变分自编码器(VQVAE)作为注意力抽象器和控制器，实现精确的注意力管理。通过明确建模注意力分配，我们的方法旨在提高系统效率。我们在视觉和NLP领域证明了ASAC的有效性，突显了其提高分类准确性和加速学习过程的能力。我们在各种数据集上对视觉变压器的实验表明，注意力控制器不仅提高了分类准确性，还加速了学习。此外，我们还证明了模型在嘈杂和分布外数据集上的鲁棒性和泛化能力。此外，我们在多任务设置中展示了改进的性能。快速实验表明，基于注意力模式的模块增强了对对抗性攻击的抵抗力，优化了注意力以提高学习效率，并促进了有效的迁移学习和少样本学习。这些有希望的结果建立了认知科学与机器学习之间的联系，阐明了AI系统中注意力机制的有效利用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attention mechanisms have become integral in AI, significantly enhancingmodel performance and scalability by drawing inspiration from human cognition.Concurrently, the Attention Schema Theory (AST) in cognitive science positsthat individuals manage their attention by creating a model of the attentionitself, effectively allocating cognitive resources. Inspired by AST, weintroduce ASAC (Attention Schema-based Attention Control), which integrates theattention schema concept into artificial neural networks. Our initialexperiments focused on embedding the ASAC module within transformerarchitectures. This module employs a Vector-Quantized Variational AutoEncoder(VQVAE) as both an attention abstractor and controller, facilitating preciseattention management. By explicitly modeling attention allocation, our approachaims to enhance system efficiency. We demonstrate ASAC's effectiveness in boththe vision and NLP domains, highlighting its ability to improve classificationaccuracy and expedite the learning process. Our experiments with visiontransformers across various datasets illustrate that the attention controllernot only boosts classification accuracy but also accelerates learning.Furthermore, we have demonstrated the model's robustness and generalizationcapabilities across noisy and out-of-distribution datasets. In addition, wehave showcased improved performance in multi-task settings. Quick experimentsreveal that the attention schema-based module enhances resilience toadversarial attacks, optimizes attention to improve learning efficiency, andfacilitates effective transfer learning and learning from fewer examples. Thesepromising results establish a connection between cognitive science and machinelearning, shedding light on the efficient utilization of attention mechanismsin AI systems.</description>
      <author>example@mail.com (Krati Saxena, Federico Jurado Ruiz, Guido Manzi, Dianbo Liu, Alex Lamb)</author>
      <guid isPermaLink="false">2509.16058v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>What is a good matching of probability measures? A counterfactual lens on transport maps</title>
      <link>http://arxiv.org/abs/2509.16027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages; comments most welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了概率测度耦合在统计和机器学习中的核心作用，特别聚焦于不同类型的传输映射及其与因果推理的联系。&lt;h4&gt;背景&lt;/h4&gt;耦合概率测度是统计学和机器学习许多问题的核心，从领域适应到迁移学习和因果推理。即使限制在确定性传输上，这些耦合也不是可识别的，两个无原子边缘存在无限多个传输映射。最优传输的常见做法掩盖了多元单调匹配的多种不同概念共存的事实。&lt;h4&gt;目的&lt;/h4&gt;比较分析三种传输映射构建的异同，建立它们等价的必要和充分条件；将反事实推理表述为传输映射选择问题；连接统计传输与因果推理两个视角；阐明因果假设如何支持特定传输映射结构的使用。&lt;h4&gt;方法&lt;/h4&gt;对循环单调、分位数保持和三角形单调三种传输映射构建进行系统比较分析；建立等价条件的必要和充分条件；将反事实推理纳入结构因果模型框架；识别因果图和结构方程条件下反事实映射与经典统计传输一致的条件。&lt;h4&gt;主要发现&lt;/h4&gt;确定了三种传输映射等价的必要和充分条件，阐明了它们各自的结构特性；揭示了反事实推理中不可测试假设的作用；找到了因果图和结构方程使得反事实映射与经典统计传输一致的条件；界定了因果假设支持特定传输映射结构使用的情境。&lt;h4&gt;结论&lt;/h4&gt;研究结果丰富了传输映射族的理论理解，阐明了它们可能的因果解释，为统计传输和因果推理之间建立新的桥梁做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;耦合概率测度位于统计学和机器学习中许多问题的核心，从领域适应到迁移学习和因果推理。然而，即使限制在确定性传输上，这些耦合也不是可识别的：两个无原子边缘存在无限多个传输映射。最优传输的常见做法，以成本最小化和循环单调性为动机，掩盖了多种多元单调匹配概念共存的事实。在这项工作中，我们首先对三种传输映射的构建进行了比较分析：循环单调、分位数保持和三角形单调映射。我们建立了它们等价的必要和充分条件，从而阐明了它们各自的结构特性。同时，我们将结构因果模型框架内的反事实推理表述为在固定边缘之间选择传输映射的问题，这明确了不可测试假设在反事实推理中的作用。然后，我们通过识别因果图和结构方程的条件，在这些条件下反事实映射与经典统计传输一致，从而能够连接这两个视角。通过这种方式，我们界定了因果假设支持使用特定传输映射结构的情况。总之，我们的结果旨在丰富传输映射族的理论理解，并阐明它们可能的因果解释。我们希望这项工作有助于在统计传输和因果推理之间建立新的桥梁。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Coupling probability measures lies at the core of many problems in statisticsand machine learning, from domain adaptation to transfer learning and causalinference. Yet, even when restricted to deterministic transports, suchcouplings are not identifiable: two atomless marginals admit infinitely manytransport maps. The common recourse to optimal transport, motivated by costminimization and cyclical monotonicity, obscures the fact that several distinctnotions of multivariate monotone matchings coexist. In this work, we firstcarry a comparative analysis of three constructions of transport maps:cyclically monotone, quantile-preserving and triangular monotone maps. Weestablish necessary and sufficient conditions for their equivalence, therebyclarifying their respective structural properties. In parallel, we formulatecounterfactual reasoning within the framework of structural causal models as aproblem of selecting transport maps between fixed marginals, which makesexplicit the role of untestable assumptions in counterfactual reasoning. Then,we are able to connect these two perspectives by identifying conditions oncausal graphs and structural equations under which counterfactual maps coincidewith classical statistical transports. In this way, we delineate thecircumstances in which causal assumptions support the use of a specificstructure of transport map. Taken together, our results aim to enrich thetheoretical understanding of families of transport maps and to clarify theirpossible causal interpretations. We hope this work contributes to establishingnew bridges between statistical transport and causal inference.</description>
      <author>example@mail.com (Lucas De Lara, Luca Ganassali)</author>
      <guid isPermaLink="false">2509.16027v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised and probabilistic learning with Contrastive Local Learning Networks: The Restricted Kirchhoff Machine</title>
      <link>http://arxiv.org/abs/2509.15842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为受限基尔霍夫机器的自学习电阻网络，能够解决无监督学习任务，类似于受限玻尔兹曼机器算法。该系统通过两个相同网络比较不同物理状态实现对比局部学习规则，并在二值化MNIST数据集上进行了模拟训练，验证了其学习能力。&lt;h4&gt;背景&lt;/h4&gt;自主物理学习系统能够修改内部参数并解决计算任务而不依赖外部计算，相比传统计算机具有分布式和节能学习的优势。&lt;h4&gt;目的&lt;/h4&gt;引入一种自学习电阻网络（受限基尔霍夫机器），使其能够解决无监督学习任务，类似于受限玻尔兹曼机器算法，并验证其学习能力。&lt;h4&gt;方法&lt;/h4&gt;基于对比局部学习网络的现有技术构建自学习电阻网络，通过两个相同网络比较不同物理状态实现对比局部学习规则。在二值化MNIST数据集上模拟训练过程，并比较其随节点增加的时间、功率和能耗扩展行为与传统计算平台上的受限玻尔兹曼机器。&lt;h4&gt;主要发现&lt;/h4&gt;成功构建了受限基尔霍夫机器并证明了其在二值化MNIST数据集上的学习能力。该机器在时间、功率和能耗方面具有特定的扩展行为，与传统计算平台上的受限玻尔兹曼机器相比有其特点。&lt;h4&gt;结论&lt;/h4&gt;受限基尔霍夫机器作为自主物理学习系统能有效解决无监督学习任务，展示了物理系统在机器学习领域的应用潜力，基于现有技术实现，具有良好的可扩展性和能效优势。&lt;h4&gt;翻译&lt;/h4&gt;自主物理学习系统修改其内部参数并解决计算任务而不依赖外部计算。与传统计算机相比，由于其物理动力学特性，它们享有分布式和节能学习的优势。在本文中，我们介绍了一种自学习电阻网络——受限基尔霍夫机器，能够解决类似于受限玻尔兹曼机器算法的无监督学习任务。该电路依赖于基于对比局部学习网络的现有技术，其中两个相同的网络比较不同的物理状态以实现对比局部学习规则。我们在二值化MNIST数据集上模拟了该机器的训练，为其学习能力提供了概念验证。最后，我们将机器随节点增加的时间、功率和能耗扩展行为与在CPU和GPU平台上运行的受限玻尔兹曼机器进行了比较。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous physical learning systems modify their internal parameters andsolve computational tasks without relying on external computation. Compared totraditional computers, they enjoy distributed and energy-efficient learning dueto their physical dynamics. In this paper, we introduce a self-learningresistor network, the Restricted Kirchhoff Machine, capable of solvingunsupervised learning tasks akin to the Restricted Boltzmann Machine algorithm.The circuit relies on existing technology based on Contrastive Local LearningNetworks, in which two identical networks compare different physical states toimplement a contrastive local learning rule. We simulate the training of themachine on the binarized MNIST dataset, providing a proof of concept of itslearning capabilities. Finally, we compare the scaling behavior of the time,power, and energy consumed per operation as more nodes are included in themachine to their Restricted Boltzmann Machine counterpart operated on CPU andGPU platforms.</description>
      <author>example@mail.com (Marcelo Guzman, Simone Ciarella, Andrea J. Liu)</author>
      <guid isPermaLink="false">2509.15842v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Transfer learning under latent space model</title>
      <link>http://arxiv.org/abs/2509.15797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种迁移学习方法，利用与目标网络潜在变量相似的源网络信息，提高目标网络中潜在变量的估计准确性，解决了潜在空间模型中参数数量众多导致的估计挑战。&lt;h4&gt;背景&lt;/h4&gt;潜在空间模型在网络分析中起着关键作用，准确估计潜在变量对下游任务如链接预测至关重要。当潜在空间维度不是特别小时，需要估计的参数数量众多，这带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种迁移学习方法，利用与目标网络潜在变量相似的网络信息，提高目标网络中潜在变量的估计准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一个两阶段迁移学习算法，适应源网络和目标网络之间节点数量的差异。在每个阶段，推导充分的识别条件并设计定制的投影梯度下降算法进行估计。当可迁移网络未知时，引入检测算法来识别合适的源网络。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析建立了所得估计器的特性，证明了所提方法能够有效处理参数估计的挑战。&lt;h4&gt;结论&lt;/h4&gt;通过模拟研究和两个真实数据集的分析，证明了所提出方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;潜在空间模型在网络分析中起着至关重要的作用，准确估计潜在变量对链接预测等下游任务至关重要。然而，当潜在空间维度不是特别小时，需要估计的大量参数带来了挑战。在本文中，我们提出了一种迁移学习方法，利用与目标网络潜在变量相似的网络中的信息，从而提高目标网络的估计准确性。给定可迁移的源网络，我们引入了一个两阶段迁移学习算法，该算法适应了源网络和目标网络之间节点数量的差异。在每个阶段，我们推导出充分的识别条件，并设计定制的投影梯度下降算法进行估计。建立了所得估计器的理论特性。当可迁移网络未知时，引入了检测算法来识别合适的源网络。模拟研究和两个真实数据集的分析证明了所提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent space model plays a crucial role in network analysis, and accurateestimation of latent variables is essential for downstream tasks such as linkprediction. However, the large number of parameters to be estimated presents achallenge, especially when the latent space dimension is not exceptionallysmall. In this paper, we propose a transfer learning method that leveragesinformation from networks with latent variables similar to those in the targetnetwork, thereby improving the estimation accuracy for the target. Giventransferable source networks, we introduce a two-stage transfer learningalgorithm that accommodates differences in node numbers between source andtarget networks. In each stage, we derive sufficient identification conditionsand design tailored projected gradient descent algorithms for estimation.Theoretical properties of the resulting estimators are established. When thetransferable networks are unknown, a detection algorithm is introduced toidentify suitable source networks. Simulation studies and analyses of two realdatasets demonstrate the effectiveness of the proposed methods.</description>
      <author>example@mail.com (Kuangnan Fang, Ruixuan Qin, Xinyan Fan)</author>
      <guid isPermaLink="false">2509.15797v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents</title>
      <link>http://arxiv.org/abs/2509.15635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 22 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MicroRCA-Agent，一种基于大型语言模型代理的微服务根因分析创新解决方案，构建了具有多模态数据融合的智能故障根因定位系统。&lt;h4&gt;背景&lt;/h4&gt;微服务环境下的故障根因分析面临挑战，需要有效的解决方案来定位故障原因。&lt;h4&gt;目的&lt;/h4&gt;构建一个能够高效处理多模态数据、智能识别故障根因的微服务分析系统。&lt;h4&gt;方法&lt;/h4&gt;三个关键技术创新：1)结合预训练Drain日志解析算法与多级数据过滤机制；2)采用集成Isolation Forest无监督学习算法与状态码验证的双重异常检测方法；3)设计统计对称比率过滤机制与两阶段LLM分析策略；4)利用精心设计的跨模态提示实现多模态异常信息的深度整合。&lt;h4&gt;主要发现&lt;/h4&gt;全面消融研究验证了每种模态数据的互补价值和系统架构的有效性，该解决方案在复杂微服务故障场景中表现出色。&lt;h4&gt;结论&lt;/h4&gt;MicroRCA-Agent在复杂微服务故障场景中实现了最终得分50.71的优越性能，代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了MicroRCA-Agent，一种基于大型语言模型代理的微服务根因分析创新解决方案，构建了具有多模态数据融合的智能故障根因定位系统。技术创新体现在三个关键方面：首先，我们将预训练的Drain日志解析算法与多级数据过滤机制相结合，高效地将大量日志压缩为高质量的故障特征。其次，我们采用双重异常检测方法，集成Isolation Forest无监督学习算法与状态码验证，实现全面的跟踪异常识别。第三，我们设计了统计对称比率过滤机制与两阶段LLM分析策略，实现跨越节点-服务-Pod层次的全栈现象总结。多模态根因分析模块利用精心设计的跨模态提示，深度整合多模态异常信息，充分利用大型语言模型的跨模态理解和逻辑推理能力，生成包含故障组件、根因描述和推理痕迹的结构化分析结果。全面的消融研究验证了每种模态数据的互补价值和系统架构的有效性。所提出的解决方案在复杂微服务故障场景中表现出色，最终得分为50.71。代码已在https://github.com/tangpan360/MicroRCA-Agent发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents MicroRCA-Agent, an innovative solution for microserviceroot cause analysis based on large language model agents, which constructs anintelligent fault root cause localization system with multimodal data fusion.The technical innovations are embodied in three key aspects: First, we combinethe pre-trained Drain log parsing algorithm with multi-level data filteringmechanism to efficiently compress massive logs into high-quality faultfeatures. Second, we employ a dual anomaly detection approach that integratesIsolation Forest unsupervised learning algorithms with status code validationto achieve comprehensive trace anomaly identification. Third, we design astatistical symmetry ratio filtering mechanism coupled with a two-stage LLManalysis strategy to enable full-stack phenomenon summarization acrossnode-service-pod hierarchies. The multimodal root cause analysis moduleleverages carefully designed cross-modal prompts to deeply integrate multimodalanomaly information, fully exploiting the cross-modal understanding and logicalreasoning capabilities of large language models to generate structured analysisresults encompassing fault components, root cause descriptions, and reasoningtrace. Comprehensive ablation studies validate the complementary value of eachmodal data and the effectiveness of the system architecture. The proposedsolution demonstrates superior performance in complex microservice faultscenarios, achieving a final score of 50.71. The code has been released at:https://github.com/tangpan360/MicroRCA-Agent.</description>
      <author>example@mail.com (Pan Tang, Shixiang Tang, Huanqi Pu, Zhiqing Miao, Zhixing Wang)</author>
      <guid isPermaLink="false">2509.15635v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant</title>
      <link>http://arxiv.org/abs/2509.15593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SETrLUSI是一种基于统计不变式的集成学习框架，用于多源迁移学习，能够提取和整合来自源域和目标域的多样化知识，加速收敛过程，并通过随机SI选择、比例源域采样和目标域自举提高训练效率和模型稳定性。&lt;h4&gt;背景&lt;/h4&gt;在迁移学习中，源域通常包含多样化的知识，不同域通常强调不同类型的知识。&lt;h4&gt;目的&lt;/h4&gt;不同于传统迁移学习方法只处理来自所有域的单一类型知识，作者引入了一种基于统计不变式(SI)的集成学习框架，用于多源迁移学习。&lt;h4&gt;方法&lt;/h4&gt;提出了SETrLUSI(Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant)，该SI提取并整合来自源域和目标域的各种类型知识，不仅有效利用多样化知识，还加速收敛过程。此外，SETrLUSI结合了随机SI选择、比例源域采样和目标域自举，提高了训练效率并增强了模型稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SETrLUSI具有良好的收敛性，并且在较低时间成本下优于相关方法。&lt;h4&gt;结论&lt;/h4&gt;SETrLUSI是一种有效的多源迁移学习方法，能够整合多样化知识并提高训练效率。&lt;h4&gt;翻译&lt;/h4&gt;在迁移学习中，源域通常携带多样化的知识，而不同的域通常强调不同类型的知识。不同于传统迁移学习方法只处理来自所有域的单一类型知识，我们引入了一种基于统计不变式(SI)的弱收敛模式集成学习框架，用于多源迁移学习，形式化为SETrLUSI(Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant)。所提出的SI提取并整合来自源域和目标域的各种类型知识，这不仅有效利用了多样化知识，还加速了收敛过程。此外，SETrLUSI结合了随机SI选择、比例源域采样和目标域自举，在提高训练效率的同时增强了模型稳定性。实验表明，SETrLUSI具有良好的收敛性，并且以较低的时间成本优于相关方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In transfer learning, a source domain often carries diverse knowledge, anddifferent domains usually emphasize different types of knowledge. Differentfrom handling only a single type of knowledge from all domains in traditionaltransfer learning methods, we introduce an ensemble learning framework with aweak mode of convergence in the form of Statistical Invariant (SI) formulti-source transfer learning, formulated as Stochastic Ensemble Multi-SourceTransfer Learning Using Statistical Invariant (SETrLUSI). The proposed SIextracts and integrates various types of knowledge from both source and targetdomains, which not only effectively utilizes diverse knowledge but alsoaccelerates the convergence process. Further, SETrLUSI incorporates stochasticSI selection, proportional source domain sampling, and target domainbootstrapping, which improves training efficiency while enhancing modelstability. Experiments show that SETrLUSI has good convergence and outperformsrelated methods with a lower time cost.</description>
      <author>example@mail.com (Chunna Li, Yiwei Song, Yuanhai Shao)</author>
      <guid isPermaLink="false">2509.15593v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder</title>
      <link>http://arxiv.org/abs/2509.15880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 figures, 7 tables. Project page: https://evggt.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了将几何感知视觉表示整合到机器人操作中的方法，提出了高效几何感知编码器eVGGT，在保持强大3D推理能力的同时，比原始VGGT快9倍且体积小5倍。&lt;h4&gt;背景&lt;/h4&gt;现有基于RGB的模仿学习方法通常使用传统视觉编码器如ResNet或ViT，这些编码器缺乏明确的3D推理能力。而最近的几何基础视觉模型如VGGT提供了强大的空间理解能力，有潜力解决这一局限性。&lt;h4&gt;目的&lt;/h4&gt;研究将几何感知视觉表示整合到机器人操作中，解决现有视觉编码器缺乏3D推理能力的问题，并提高计算效率以适应实际机器人系统部署。&lt;h4&gt;方法&lt;/h4&gt;将几何感知视觉编码器整合到模仿学习框架中（包括ACT和DP），并提出了eVGGT，这是一种从VGGT蒸馏出的高效几何感知编码器。&lt;h4&gt;主要发现&lt;/h4&gt;整合几何感知视觉编码器后，在单手和双手操作任务上，成功率比标准视觉编码器提高了最多6.5%。eVGGT比VGGT快9倍，体积小5倍，同时保留了强大的3D推理能力。&lt;h4&gt;结论&lt;/h4&gt;几何感知视觉表示可以显著提高机器人操作的性能，而eVGGT模型在保持高性能的同时大大提高了计算效率，使其更适合实际机器人系统的部署。&lt;h4&gt;翻译&lt;/h4&gt;现有的基于RGB的模仿学习方法通常采用传统的视觉编码器，如ResNet或ViT，这些编码器缺乏明确的3D推理能力。最近的几何基础视觉模型，如VGGT，提供了强大的空间理解能力，是有望解决这一局限性的候选方案。本研究探讨了将几何感知视觉表示整合到机器人操作中。我们的结果表明，将几何感知视觉编码器整合到模仿学习框架中（包括ACT和DP），在模拟和现实世界中的单手和双手操作任务上，成功率比标准视觉编码器提高了最多6.5%。尽管有这些优势，但大多数几何基础模型需要高计算成本，限制了它们在实际机器人系统中的部署。为应对这一挑战，我们提出了eVGGT，这是一种从VGGT蒸馏出的高效几何感知编码器。eVGGT比VGGT快9倍，体积小5倍，同时保留了强大的3D推理能力。代码和预训练模型将被发布，以促进几何感知机器人技术的进一步研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人操作任务中传统视觉编码器（如ResNet或ViT）缺乏3D推理能力的问题。这个问题很重要，因为许多机器人操作任务需要精确的几何理解和空间关系判断，缺乏这种能力会限制机器人在复杂环境中的操作表现。同时，现有的几何感知模型虽然性能强大，但计算成本高，难以在实际机器人系统中实时部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统视觉编码器在机器人操作中的局限性，然后借鉴了VGGT这一具有强大几何推理能力的视觉模型。他们利用知识蒸馏技术，将VGGT的知识转移到一个小型高效的模型中，同时结合数据增强和梯度损失来提升3D重建质量。这种方法既保留了VGGT的几何推理能力，又解决了其计算效率低的问题，使其适用于实时机器人系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过知识蒸馏创建一个轻量化的几何感知视觉编码器eVGGT，并将其集成到机器人操作策略中。整体流程包括：1) 使用知识蒸馏从VGGT训练出eVGGT，减少Transformer块数量并更换小型骨干网络；2) 添加梯度损失和数据增强提升3D重建质量；3) 将预训练的eVGGT冻结并集成到ACT和DP等模仿学习框架中，替换传统视觉编码器；4) 在模拟环境和真实机器人上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出eVGGT，一种高效且具有几何感知能力的视觉编码器，比原始VGGT快9倍、小5倍；2) 设计简单有效的几何感知表示集成方法，直接替换传统视觉编码器的潜在空间；3) 改进知识蒸馏训练方案，结合数据增强和梯度损失提升模型泛化能力。相比之前的工作，eVGGT在保持竞争力的3D重建性能的同时，显著提高了计算效率，使其能够在资源受限的机器人系统中实时运行，而无需外部处理或额外传感器输入。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 作者提出了eVGGT，一种高效几何感知视觉编码器，通过知识蒸馏从VGGT中学习，显著提升了机器人操作的成功率，同时保持计算效率，使几何感知的视觉表示能够在实时机器人系统中实际应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing RGB-based imitation learning approaches typically employ traditionalvision encoders such as ResNet or ViT, which lack explicit 3D reasoningcapabilities. Recent geometry-grounded vision models, such asVGGT~\cite{wang2025vggt}, provide robust spatial understanding and arepromising candidates to address this limitation. This work investigates theintegration of geometry-aware visual representations into robotic manipulation.Our results suggest that incorporating the geometry-aware vision encoder intoimitation learning frameworks, including ACT and DP, yields up to 6.5%improvement over standard vision encoders in success rate across single- andbi-manual manipulation tasks in both simulation and real-world settings.Despite these benefits, most geometry-grounded models require highcomputational cost, limiting their deployment in practical robotic systems. Toaddress this challenge, we propose eVGGT, an efficient geometry-aware encoderdistilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller thanVGGT, while preserving strong 3D reasoning capabilities. Code and pretrainedmodels will be released to facilitate further research in geometry-awarerobotics.</description>
      <author>example@mail.com (An Dinh Vuong, Minh Nhat Vu, Ian Reid)</author>
      <guid isPermaLink="false">2509.15880v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation</title>
      <link>http://arxiv.org/abs/2509.15772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VLM3D是一种新型文本到3D生成框架，通过集成大型视觉语言模型(VLMs)到SDS管道中，解决了现有SDS方法在语义对齐和3D空间约束方面的局限性，实现了更高质量的3D生成。&lt;h4&gt;背景&lt;/h4&gt;Score Distillation Sampling (SDS)通过监督3D模型的去噪过程，使用预训练文本到图像扩散模型实现高质量文本到3D生成，但现有方法存在两个基本限制：依赖CLIP风格文本编码器导致粗略语义对齐，以及2D扩散先验缺乏明确3D空间约束。&lt;h4&gt;目的&lt;/h4&gt;解决现有SDS方法在语义对齐和3D空间约束方面的局限性，提高文本到3D生成的质量，特别是在语义保真度、几何一致性和空间正确性方面。&lt;h4&gt;方法&lt;/h4&gt;提出VLM3D框架，将大型视觉语言模型(VLMs)集成到SDS管道中作为可区分的语义和空间先验，利用VLMs的语言监督实现细粒度提示对齐，并通过其视觉语言建模能力增强空间理解。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，VLM3D在各种物体和复杂场景中显著优于之前的基于SDS的方法，在语义保真度、几何一致性和空间正确性方面表现更佳，特别是在单对象生成的3D一致性和多对象场景中的关系推理方面有显著提升。&lt;h4&gt;结论&lt;/h4&gt;VLM3D通过整合大型视觉语言模型到文本到3D生成流程中，有效解决了现有SDS方法的局限性，实现了更高质量的3D内容生成。&lt;h4&gt;翻译&lt;/h4&gt;评分蒸馏采样(SDS)通过监督多视角2D渲染的去噪过程来指导3D模型，使用预训练的文本到图像扩散模型与输入提示保持一致并确保3D一致性，从而实现高质量的文本到3D生成。然而，现有的基于SDS的方法面临两个基本限制：(1)它们对CLIP风格文本编码器的依赖导致粗略的语义对齐，难以处理细粒度提示；(2)2D扩散先验缺乏明确的3D空间约束，导致多对象场景中几何不一致和物体关系不准确。为应对这些挑战，我们提出了VLM3D，一种新型文本到3D生成框架，将大型视觉语言模型(VLMs)集成到SDS管道中作为可区分的语义和空间先验。与标准的文本到图像扩散先验不同，VLMs利用丰富的语言监督，能够实现细粒度的提示对齐。此外，它们固有的视觉语言建模提供了强大的空间理解能力，显著提高了单对象生成的3D一致性，并改善了多对象场景中的关系推理。我们基于开源的Qwen2.5-VL模型实例化了VLM3D，并在GPTeval3D基准上进行了评估。在各种物体和复杂场景的实验中表明，VLM3D在语义保真度、几何一致性和空间正确性方面明显优于之前的基于SDS的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决文本到3D生成中的两个关键问题：一是现有方法依赖CLIP文本编码器导致粗略语义对齐，难以处理细粒度提示；二是2D扩散先验缺乏明确3D空间约束，造成多视角几何不一致和对象关系不准确。这些问题在现实研究中很重要，因为高质量文本到3D生成在游戏开发、虚拟现实、数字孪生等领域有广泛应用，而现有方法生成的3D资产常与文本提示不匹配或视角不一致，限制了技术的实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了SDS方法的局限性，然后观察到视觉语言模型(VLMs)具有更强的跨模态语义理解和空间推理能力。他们借鉴了SDS的基本范式和VLMs的研究成果，特别是Qwen2.5-VL模型。设计上，他们将VLMs集成到SDS管道中作为可微分奖励信号，设计了双查询提示分别关注内容匹配和几何质量，开发了完全可微分的训练流程，并使用动态权重策略平衡VLM奖励和SDS损失。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将大型视觉语言模型(VLMs)集成到文本到3D生成流程中，作为可微分的语义和空间奖励信号，替代传统的2D扩散模型监督。实现流程包括：1)从3D模型渲染多视角图像；2)将图像和文本输入VLM进行双查询评估；3)计算VLM奖励；4)计算SDS损失；5)结合两个损失并使用动态权重策略更新3D参数；6)迭代优化直到收敛。关键设计包括双查询提示、动态权重调度和端到端可微训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将VLMs作为可微分语义和空间奖励引入文本到3D生成；2)设计双查询提示同时评估内容匹配和几何质量；3)采用动态权重调度策略平衡约束和细节；4)实现端到端可微训练流程。相比之前的工作，VLM3D不再仅依赖2D扩散模型的隐式监督，而是利用VLM的显式语义和空间约束；不依赖人类偏好或人工设计奖励，而是利用VLM内置的理解能力；从隐式分数蒸馏转向显式语义优化，强调多视角一致性而非单视角质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VLM3D通过将大型视觉语言模型作为可微分的语义和空间奖励集成到文本到3D生成流程中，显著提高了生成模型的语义对齐精度和3D一致性，解决了现有方法在处理细粒度提示和复杂空间关系时的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Score Distillation Sampling (SDS) enables high-quality text-to-3D generationby supervising 3D models through the denoising of multi-view 2D renderings,using a pretrained text-to-image diffusion model to align with the input promptand ensure 3D consistency. However, existing SDS-based methods face twofundamental limitations: (1) their reliance on CLIP-style text encoders leadsto coarse semantic alignment and struggles with fine-grained prompts; and (2)2D diffusion priors lack explicit 3D spatial constraints, resulting ingeometric inconsistencies and inaccurate object relationships in multi-objectscenes. To address these challenges, we propose VLM3D, a novel text-to-3Dgeneration framework that integrates large vision-language models (VLMs) intothe SDS pipeline as differentiable semantic and spatial priors. Unlike standardtext-to-image diffusion priors, VLMs leverage rich language-groundedsupervision that enables fine-grained prompt alignment. Moreover, theirinherent vision language modeling provides strong spatial understanding, whichsignificantly enhances 3D consistency for single-object generation and improvesrelational reasoning in multi-object scenes. We instantiate VLM3D based on theopen-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark.Experiments across diverse objects and complex scenes show that VLM3Dsignificantly outperforms prior SDS-based methods in semantic fidelity,geometric coherence, and spatial correctness.</description>
      <author>example@mail.com (Weimin Bai, Yubo Li, Weijian Luo, Wenzheng Chen, He Sun)</author>
      <guid isPermaLink="false">2509.15772v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</title>
      <link>http://arxiv.org/abs/2509.15536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages,15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAMPO是一种混合框架，结合了视觉自回归建模和因果建模，有效解决了现有自回归世界模型在视觉连贯性预测方面的问题，显著提高了视频预测质量和推理效率。&lt;h4&gt;背景&lt;/h4&gt;世界模型允许智能体在想象的环境中模拟行动后果，用于规划、控制和长时程决策。然而，现有的自回归世界模型由于空间结构破坏、解码效率不足和运动建模不充分，在视觉连贯性预测方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合框架，解决现有自回归世界模型在视觉连贯性预测方面的问题，改进视频预测和基于模型的控制性能。&lt;h4&gt;方法&lt;/h4&gt;提出了名为SAMPO（Scale-wise Autoregression with Motion Prompt）的混合框架，结合帧内生成的视觉自回归建模和下一帧生成的因果建模；集成时间因果解码与双向空间注意力，保持空间局部性并支持并行解码；设计非对称多尺度标记器，保留观察帧空间细节同时提取未来帧的动态表示；引入轨迹感知运动提示模块，注入时空线索关注动态区域。&lt;h4&gt;主要发现&lt;/h4&gt;SAMPO在条件动作视频预测和基于模型的控制方面取得了有竞争力的性能；生成质量提高，推理速度提高了4.4倍；能够泛化到未见过的任务并从更大的模型尺寸中受益。&lt;h4&gt;结论&lt;/h4&gt;SAMPO通过结合视觉自回归建模和因果建模，有效解决了现有自回归世界模型在视觉连贯性预测方面的问题，其设计显著提高了时间一致性和展开效率，同时优化了内存使用和模型性能。&lt;h4&gt;翻译&lt;/h4&gt;世界模型允许智能体在想象的环境中模拟行动后果，用于规划、控制和长时程决策。然而，现有的自回归世界模型由于空间结构破坏、解码效率不足和运动建模不充分，在视觉连贯性预测方面存在困难。为此，我们提出了SAMPO（Scale-wise Autoregression with Motion Prompt），这是一种混合框架，结合了帧内生成的视觉自回归建模和下一帧生成的因果建模。具体来说，SAMPO将时间因果解码与双向空间注意力相结合，保留了空间局部性并支持每个尺度内的并行解码。这种设计显著提高了时间一致性和展开效率。为了进一步改进动态场景理解，我们设计了一种非对称多尺度标记器，在保留观察帧空间细节的同时，为未来帧提取紧凑的动态表示，优化了内存使用和模型性能。此外，我们引入了轨迹感知运动提示模块，注入物体和机器人轨迹的时空线索，将注意力集中在动态区域，提高了时间一致性和物理真实性。大量实验表明，SAMPO在条件动作视频预测和基于模型的控制方面取得了有竞争力的性能，生成质量提高了4.4倍，推理速度提高了4.4倍。我们还评估了SAMPO的零样本泛化能力和扩展行为，证明了其能够泛化到未见过的任务并从更大的模型尺寸中受益。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有自回归世界模型在视觉预测中面临的视觉连贯性差问题，具体表现为空间结构被破坏、解码效率低以及对运动建模不足。这个问题很重要，因为世界模型能让代理人在想象环境中模拟动作后果，用于规划和决策，而视觉连贯性对于准确预测环境变化至关重要，空间结构破坏会导致不合理的视觉内容生成，效率低限制了实际应用，运动建模不足会影响动态场景的真实性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法（掩码建模、扩散模型和自回归模型）的局限性，发现自回归模型虽保留因果结构但面临结构退化、解码缓慢和运动建模不足三大挑战。基于此，作者设计了一个结合帧内双向空间注意力和跨时间因果建模的框架。借鉴了VAR的多尺度令牌预测替代光栅扫描、VQ-VAE的令牌化思想、视觉提示技术特别是运动感知提示，以及类似GPT的Transformer架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是尺度自回归，结合时间因果建模和空间粗到细生成；不对称多尺度令牌化，平衡空间细节和动态建模；轨迹感知运动提示，指导模型关注动态区域。整体流程：1)输入处理将帧转换为令牌图；2)观察帧密集令牌化，未来帧稀疏令牌化；3)提取动态轨迹作为运动提示；4)时间上自回归生成未来帧，空间上从粗到细生成每个帧的令牌图；5)使用多尺度交叉熵损失训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)混合自回归架构结合时间因果和空间粗到细生成；2)不对称多尺度令牌器优化内存和性能；3)轨迹感知运动提示增强动态交互建模。不同之处：与传统光栅扫描相比保留空间局部性；与单一尺度令牌化相比采用不对称策略；与静态提示相比引入动态时空线索；与纯时间或空间自回归不同采用时空结合架构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAMPO通过结合尺度自回归、不对称多尺度令牌化和轨迹感知运动提示，解决了现有世界模型在视觉预测中面临的视觉连贯性差、解码效率低和运动建模不足的问题，实现了高质量、高效率的动态场景预测和机器人控制。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; World models allow agents to simulate the consequences of actions in imaginedenvironments for planning, control, and long-horizon decision-making. However,existing autoregressive world models struggle with visually coherentpredictions due to disrupted spatial structure, inefficient decoding, andinadequate motion modeling. In response, we propose \textbf{S}cale-wise\textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt(\textbf{SAMPO}), a hybrid framework that combines visual autoregressivemodeling for intra-frame generation with causal modeling for next-framegeneration. Specifically, SAMPO integrates temporal causal decoding withbidirectional spatial attention, which preserves spatial locality and supportsparallel decoding within each scale. This design significantly enhances bothtemporal consistency and rollout efficiency. To further improve dynamic sceneunderstanding, we devise an asymmetric multi-scale tokenizer that preservesspatial details in observed frames and extracts compact dynamic representationsfor future frames, optimizing both memory usage and model performance.Additionally, we introduce a trajectory-aware motion prompt module that injectsspatiotemporal cues about object and robot trajectories, focusing attention ondynamic regions and improving temporal consistency and physical realism.Extensive experiments show that SAMPO achieves competitive performance inaction-conditioned video prediction and model-based control, improvinggeneration quality with 4.4$\times$ faster inference. We also evaluate SAMPO'szero-shot generalization and scaling behavior, demonstrating its ability togeneralize to unseen tasks and benefit from larger model sizes.</description>
      <author>example@mail.com (Sen Wang, Jingyi Tian, Le Wang, Zhimin Liao, Jiayi Li, Huaiyi Dong, Kun Xia, Sanping Zhou, Wei Tang, Hua Gang)</author>
      <guid isPermaLink="false">2509.15536v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters</title>
      <link>http://arxiv.org/abs/2509.15490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures, IEEE/CVF International Conference on Computer  Vision Workshops (ICCVW)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SmolRGPT，一种紧凑的视觉语言模型架构，通过整合RGB和深度线索实现区域级空间推理，仅需6亿参数就能在仓库空间推理基准测试中与更大模型相媲美。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的视觉语言模型(VLMs)虽然能实现强大的多模态推理，但通常依赖超大型模型，计算和内存需求极高，难以在资源受限的仓库、机器人或工业环境中部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且具有强大空间推理能力的视觉语言模型，使其能够在资源受限环境中部署，同时保持高性能的空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出SmolRGPT架构，通过整合RGB和深度线索实现区域级空间推理，并采用三阶段课程学习策略：逐步对齐视觉和语言特征、实现空间关系理解、适应特定任务数据集。&lt;h4&gt;主要发现&lt;/h4&gt;SmolRGPT仅使用6亿参数，就在具有挑战性的仓库空间推理基准测试中达到了与更大替代模型相匹配或超越的性能表现。&lt;h4&gt;结论&lt;/h4&gt;研究证明，可以在不牺牲核心空间推理能力的情况下，在现实场景中实现高效、可部署的多模态智能，为资源受限环境提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近的视觉语言模型(VLMs)进展已经实现了强大的多模态推理能力，但最先进的方法通常依赖于具有极高计算和内存要求的超大型模型。这使得在资源受限环境(如仓库、机器人和工业应用)中部署这些模型变得困难，而这些环境既需要高效性，也需要稳健的空间理解能力。在这项工作中，我们提出了SmolRGPT，一种紧凑的视觉语言架构，它通过整合RGB和深度线索，明确地融入了区域级别的空间推理能力。SmolRGPT采用三阶段课程学习方法，逐步对齐视觉和语言特征，实现空间关系理解，并适应特定任务的数据集。我们证明，仅使用6亿参数，SmolRGPT就在具有挑战性的仓库空间推理基准测试中取得了与更大的替代模型相匹配或更好的性能。这些发现突显了在现实场景中实现高效、可部署的多模态智能的潜力，而无需牺牲核心的空间推理能力。实验代码将在https://github.com/abtraore/SmolRGPT上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在资源受限环境（如仓库、机器人、工业应用）中部署高效空间推理模型的问题。现实中，这些环境既需要强大的空间理解能力来理解物体位置、排列和几何关系，又受限于硬件和内存资源，而当前最先进的视觉语言模型通常参数量极大（数十亿甚至上万亿），难以在这些环境中部署。这个问题的重要性在于它限制了AI技术在物流自动化、机器人导航等实际场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有大型视觉语言模型在空间推理任务上的局限性，特别是在资源受限环境中的部署问题。他们借鉴了多个现有工作：基于SmolVLM构建并集成了SpatialRGPT的区域级视觉语言建模技术；使用NanoVLM作为基础框架；采用预训练的视觉特征提取器siglip2-base-patch16-256；参考SpatialGPT的方法保持RGB和深度输入的独立路径；借鉴RegionGPT的设计使用专门的refiner模块。作者通过创建紧凑架构、结合RGB和深度信息、采用像素重排技术和三阶段课程学习等方法，设计出了这个高效的空间推理模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个仅6亿参数的紧凑视觉语言模型，通过同时处理RGB和深度信息来实现区域级空间推理，使其能够在资源受限环境中部署。整体流程包括：1)使用预训练视觉特征提取器处理RGB和深度图像；2)通过模态特定的连接器将特征映射到语言模型空间；3)使用refiner模块细化特征；4)通过掩码池化提取区域特征；5)将区域特征插入语言模型；6)采用三阶段训练策略：先在LLaVA-CC3M上训练RGB连接器，再在OSD上预热深度组件，最后在仓库数据集上微调。推理时，模型将特殊标记替换为区域特定嵌入，生成对空间查询的自然语言回答。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)仅用6亿参数就实现了强大的空间推理能力；2)同时处理RGB和深度信息，通过独立路径保持模态独特性；3)使用像素重排投影技术提供更密集的特征表示；4)采用三阶段课程学习逐步构建能力；5)高效的区域级处理通过掩码池化实现。相比之前的工作，SmolRGPT在模型规模上比SpatialRGPT(80亿参数)和LLaVA-34B(340亿参数)小得多，在架构上更简洁且明确整合了深度信息，在训练上采用渐进式策略，在应用上特别针对仓库环境优化，能够处理实际工业场景中的空间推理任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SmolRGPT通过创新的紧凑架构和三阶段训练方法，证明了仅用6亿参数的模型就能在资源受限环境中实现与大型模型相当的空间推理能力，为仓库和工业应用中的高效AI部署开辟了新途径。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision-language models (VLMs) have enabled powerfulmultimodal reasoning, but state-of-the-art approaches typically rely onextremely large models with prohibitive computational and memory requirements.This makes their deployment challenging in resource-constrained environmentssuch as warehouses, robotics, and industrial applications, where bothefficiency and robust spatial understanding are critical. In this work, wepresent SmolRGPT, a compact vision-language architecture that explicitlyincorporates region-level spatial reasoning by integrating both RGB and depthcues. SmolRGPT employs a three-stage curriculum that progressively align visualand language features, enables spatial relationship understanding, and adaptsto task-specific datasets. We demonstrate that with only 600M parameters,SmolRGPT achieves competitive results on challenging warehouse spatialreasoning benchmarks, matching or exceeding the performance of much largeralternatives. These findings highlight the potential for efficient, deployablemultimodal intelligence in real-world settings without sacrificing core spatialreasoning capabilities. The code of the experimentation will be available at:https://github.com/abtraore/SmolRGPT</description>
      <author>example@mail.com (Abdarahmane Traore, Éric Hervet, Andy Couturier)</author>
      <guid isPermaLink="false">2509.15490v1</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>SPATIALGEN: Layout-guided 3D Indoor Scene Generation</title>
      <link>http://arxiv.org/abs/2509.14981v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3D scene generation; diffusion model; Scene reconstruction and  understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SpatialGen的新型多视图多模态扩散模型，用于生成高质量且语义一致的3D室内场景，同时提供了一个包含12,328个结构化注释场景和470万张逼真2D渲染图的大型数据集。&lt;h4&gt;背景&lt;/h4&gt;创建高保真室内环境3D模型对设计、虚拟现实和机器人应用至关重要，但手动3D建模耗时且劳动密集。现有生成AI方法在平衡视觉质量、多样性和用户控制方面存在挑战，主要瓶颈是缺乏大规模高质量数据集。&lt;h4&gt;目的&lt;/h4&gt;解决缺乏大规模高质量数据集的问题，开发能够生成真实且语义一致的3D室内场景的方法。&lt;h4&gt;方法&lt;/h4&gt;引入综合合成数据集，包含12,328个结构化注释场景、57,440个房间和470万张逼真2D渲染图；提出SpatialGen模型，根据3D布局和参考图像从任意视角合成外观、几何和语义信息，同时保持跨模态空间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;SpatialGen在实验中持续产生优于先前方法的结果，能够生成高质量、多样化且语义一致的3D室内场景。&lt;h4&gt;结论&lt;/h4&gt;开源数据和模型以促进社区发展，推动室内场景理解和生成领域的进步。&lt;h4&gt;翻译&lt;/h4&gt;创建室内环境的高保真3D模型对设计、虚拟现实和机器人应用至关重要。然而，手动3D建模仍然耗时且劳动密集。虽然最近生成AI的进步实现了自动化场景合成，但现有方法在平衡视觉质量、多样性、语义一致性和用户控制方面常常面临挑战。主要瓶颈是缺乏针对此任务的大规模高质量数据集。为解决这一差距，我们引入了一个综合的合成数据集，包含12,328个结构化注释场景，57,440个房间，和470万张逼真的2D渲染图。利用此数据集，我们提出了SpatialGen，一种新颖的多视图多模态扩散模型，能够生成真实且语义一致的3D室内场景。给定3D布局和参考图像（从文本提示派生），我们的模型从任意视角合成外观（彩色图像）、几何（场景坐标图）和语义（语义分割图），同时保持跨模态的空间一致性。在实验中，SpatialGen持续产生优于先前方法的结果。我们开源了我们的数据和模型，以赋能社区并推动室内场景理解和生成领域的发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何自动生成高质量、多样化的3D室内场景模型的问题。这个问题在现实中非常重要，因为室内3D模型对设计、虚拟现实、机器人应用等领域至关重要，而手动3D建模既耗时又耗费人力。现有方法在平衡视觉质量、多样性、语义一致性和用户控制方面存在困难，缺乏大规模高质量数据集是主要瓶颈。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计解决方案：程序化建模方法缺乏多样性，3D生成方法布局和外观真实性有限，基于图像的方法面临多视图语义一致性挑战。作者借鉴了扩散模型技术、多视图扩散方法、3D语义布局先验和3D高斯溅射技术，创新性地结合这些技术，设计了一个基于3D布局先验的多视图多模态扩散模型，并创建了大规模数据集来支持模型训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是基于给定的3D语义布局，生成真实且语义一致的3D室内场景，利用多视图多模态扩散模型从任意视点合成外观、几何和语义信息，同时保持跨模态的空间一致性。整体流程包括：1)将3D语义布局转换为特定视图表示；2)设计布局引导的注意力机制，包括跨视图和跨模态注意力；3)采用迭代多视图生成策略确保完整场景覆盖；4)使用3D高斯溅射优化重建显式辐射场实现自由视点渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了一个包含12,328个场景、57,440个房间和470万张渲染的大规模数据集；2)提出SPATIALGEN框架，实现基于3D布局先验的多视图多模态图像扩散；3)开发场景坐标图VAE(SCM-VAE)提高几何重建质量；4)设计迭代密集视图生成策略确保一致性。相比之前的工作，SPATIALGEN避免了基于分数蒸馏方法的严重视觉伪影，突破了全景作为代理方法仅限于固定相机位置的局限，在语义一致性、视觉质量和场景多样性方面表现出色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPATIALGEN通过大规模数据集和创新的布局引导多视图多模态扩散模型，实现了高质量、语义一致的3D室内场景生成，显著超越了现有方法在视觉质量、多样性和一致性方面的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Creating high-fidelity 3D models of indoor environments is essential forapplications in design, virtual reality, and robotics. However, manual 3Dmodeling remains time-consuming and labor-intensive. While recent advances ingenerative AI have enabled automated scene synthesis, existing methods oftenface challenges in balancing visual quality, diversity, semantic consistency,and user control. A major bottleneck is the lack of a large-scale, high-qualitydataset tailored to this task. To address this gap, we introduce acomprehensive synthetic dataset, featuring 12,328 structured annotated sceneswith 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging thisdataset, we present SpatialGen, a novel multi-view multi-modal diffusion modelthat generates realistic and semantically consistent 3D indoor scenes. Given a3D layout and a reference image (derived from a text prompt), our modelsynthesizes appearance (color image), geometry (scene coordinate map), andsemantic (semantic segmentation map) from arbitrary viewpoints, whilepreserving spatial consistency across modalities. SpatialGen consistentlygenerates superior results to previous methods in our experiments. We areopen-sourcing our data and models to empower the community and advance thefield of indoor scene understanding and generation.</description>
      <author>example@mail.com (Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan)</author>
      <guid isPermaLink="false">2509.14981v2</guid>
      <pubDate>Mon, 22 Sep 2025 15:43:12 +0800</pubDate>
    </item>
    <item>
      <title>Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark</title>
      <link>http://arxiv.org/abs/2509.14574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了一个小型基准测试，用于评估视觉语言模型(VLMs)对城市场景的理解能力，通过蒙特利尔街道图像和人类标注数据进行比较分析。&lt;h4&gt;背景&lt;/h4&gt;理解人们如何阅读城市场景可以为城市设计和规划提供重要信息，但当前缺乏专门评估VLMs在城市感知任务上的基准测试。&lt;h4&gt;目的&lt;/h4&gt;创建一个小型基准测试，评估VLMs对城市场景的理解能力，并比较模型与人类在城市感知上的差异。&lt;h4&gt;方法&lt;/h4&gt;使用100张蒙特利尔街道图像(真实照片和合成场景各半)，12名来自7个社区组的参与者提供230份标注，涵盖30个维度(物理属性和主观印象)；评估7个VLMs在zero-shot设置下的表现；使用准确性评估单选项目，Jaccard重叠评估多标签项目；人类一致性通过Krippendorff's alpha和成对Jaccard衡量。&lt;h4&gt;主要发现&lt;/h4&gt;模型在可见、客观属性上的表现优于主观评价；最佳系统(claude-sonnet)在多标签项目上达到macro 0.31和平均Jaccard 0.48；人类一致性高的维度模型表现更好；合成图像导致模型分数略低。&lt;h4&gt;结论&lt;/h4&gt;发布的基准测试、提示和工具有助于在参与式城市分析中进行可复现、不确定性感知的评估，为城市规划提供AI辅助工具。&lt;h4&gt;翻译&lt;/h4&gt;了解人们如何阅读城市场景可以为设计和规划提供信息。我们引入了一个小型基准测试，使用100张蒙特利尔街道图像( evenly split between photographs 和 photorealistic synthetic scenes)来测试视觉语言模型(VLMs)的城市感知能力。来自7个社区组的12名参与者提供了230份注释表，涵盖30个维度，混合了物理属性和主观印象。法语响应被标准化为英语。我们在zero-shot设置下使用结构化提示和确定性解析器评估了7个VLMs。对于单选项目使用准确性，对于多标签项目使用Jaccard重叠；人类一致性使用Krippendorff's alpha和成对Jaccard。结果表明，模型在可见、客观属性上的对齐比主观评价更强。最佳系统(claude-sonnet)在多标签项目上达到macro 0.31和平均Jaccard 0.48。更高的人类一致性对应更好的模型分数。合成图像略微降低了分数。我们发布了基准测试、提示和工具，用于在参与式城市分析中进行可复现、不确定性感知的评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how people read city scenes can inform design and planning. Weintroduce a small benchmark for testing vision-language models (VLMs) on urbanperception using 100 Montreal street images, evenly split between photographsand photorealistic synthetic scenes. Twelve participants from seven communitygroups supplied 230 annotation forms across 30 dimensions mixing physicalattributes and subjective impressions. French responses were normalized toEnglish. We evaluated seven VLMs in a zero-shot setup with a structured promptand deterministic parser. We use accuracy for single-choice items and Jaccardoverlap for multi-label items; human agreement uses Krippendorff's alpha andpairwise Jaccard. Results suggest stronger model alignment on visible,objective properties than subjective appraisals. The top system (claude-sonnet)reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher humanagreement coincides with better model scores. Synthetic images slightly lowerscores. We release the benchmark, prompts, and harness for reproducible,uncertainty-aware evaluation in participatory urban analysis.</description>
      <author>example@mail.com (Rashid Mushkani)</author>
      <guid isPermaLink="false">2509.14574v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
  <item>
      <title>Biologically Plausible Online Hebbian Meta-Learning: Two-Timescale Local Rules for Spiking Neural Brain Interfaces</title>
      <link>http://arxiv.org/abs/2509.14447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, submitted to ICLR 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于局部三因子学习规则和双时间尺度资格迹的在线脉冲神经网络解码器，解决了脑机接口中神经信号不稳定和内存限制的问题。&lt;h4&gt;背景&lt;/h4&gt;脑机接口在实时植入式应用中面临神经信号不稳定和内存约束的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种避免通过时间反向传播同时保持竞争性能的在线SNN解码器，实现内存高效的神经解码。&lt;h4&gt;方法&lt;/h4&gt;结合误差调节的Hebbian更新、快速/慢速迹合并和自适应学习率控制，仅需O(1)内存，相比BPTT方法的O(T)内存大幅降低内存需求。&lt;h4&gt;主要发现&lt;/h4&gt;在两个灵长类动物数据集上实现了可比的解码准确率（Zenodo数据集Pearson R≥0.63，MC Maze数据集R≥0.81），内存减少28-35%，且比BPTT训练的SNN收敛更快。闭环模拟展示了适应神经干扰和无需离线校准即可从零开始学习的能力。&lt;h4&gt;结论&lt;/h4&gt;该研究实现了内存高效、持续自适应的神经解码，适用于资源受限的植入式BCI系统。&lt;h4&gt;翻译&lt;/h4&gt;Brain-Computer Interfaces面临来自神经信号不稳定和内存限制的挑战，用于实时植入式应用。我们引入了一种使用局部三因子学习规则和双时间尺度资格迹的在线SNN解码器，避免了通过时间反向传播，同时保持竞争性能。我们的方法结合了误差调节的Hebbian更新、快速/慢速迹合并和自适应学习率控制，仅需O(1)内存，而BPTT方法需要O(T)内存。在两个灵长类数据集上的评估实现了可比的解码准确率（Pearson R≥0.63 Zenodo，R≥0.81 MC Maze），内存减少28-35%，并且比BPTT训练的SNN收敛更快。使用合成神经群体的闭环模拟展示了适应神经干扰和无需离线校准即可从零开始学习的能力。这项工作实现了内存高效、持续自适应的神经解码，适用于资源受限的植入式BCI系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-Computer Interfaces face challenges from neural signal instability andmemory constraints for real-time implantable applications. We introduce anonline SNN decoder using local three-factor learning rules with dual-timescaleeligibility traces that avoid backpropagation through time while maintainingcompetitive performance. Our approach combines error-modulated Hebbian updates,fast/slow trace consolidation, and adaptive learning rate control, requiringonly O(1) memory versus O(T) for BPTT methods. Evaluations on two primatedatasets achieve comparable decoding accuracy (Pearson $R \geq 0.63$ Zenodo, $R\geq 0.81$ MC Maze) with 28-35% memory reduction and faster convergence thanBPTT-trained SNNs. Closed-loop simulations with synthetic neural populationsdemonstrate adaptation to neural disruptions and learning from scratch withoutoffline calibration. This work enables memory-efficient, continuously adaptiveneural decoding suitable for resource-constrained implantable BCI systems.</description>
      <author>example@mail.com (Sriram V. C. Nallani, Gautham Ramachandran, Sahil S. Shah)</author>
      <guid isPermaLink="false">2509.14447v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</title>
      <link>http://arxiv.org/abs/2509.15220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE T-PAMI 2025. Code: https://github.com/cvg/diffmvs&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于扩散模型的新型多视图立体(MVS)框架，通过条件扩散过程进行深度细化，结合轻量级网络和置信度采样策略，实现了高效且高质量的3D几何重建。&lt;h4&gt;背景&lt;/h4&gt;传统的基于学习的多视图立体方法通过多视图深度估计和深度图融合来重建3D几何形状，通常采用初始化粗糙深度图再逐步细化的策略以提高效率。扩散模型在生成任务中表现出色，通过迭代去噪过程从随机噪声中恢复样本。&lt;h4&gt;目的&lt;/h4&gt;将扩散模型引入多视图立体(MVS)框架，以提高3D几何重建的效率和性能，特别是在深度估计和细化方面。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新型MVS框架，将深度细化表述为条件扩散过程，设计了条件编码器指导扩散过程，提出了结合轻量级2D U-Net和卷积GRU的扩散网络，以及基于置信度的自适应采样策略。基于此框架开发了DiffMVS和CasDiffMVS两种方法。&lt;h4&gt;主要发现&lt;/h4&gt;DiffMVS在运行时间和GPU内存方面实现了与最先进方法相竞争的性能；CasDiffMVS在DTU、Tanks &amp; Temples和ETH3D数据集上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;将扩散模型应用于多视图立体任务可以有效提高3D几何重建的效率和性能，所提出的框架和方法为该领域提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;从校准图像重建3D几何形状时，基于学习的多视图立体(MVS)方法通常执行多视图深度估计，然后将深度图融合到网格或点云中。为了提高计算效率，许多方法初始化一个粗糙的深度图，然后在更高分辨率下逐步细化它。最近，扩散模型在生成任务中取得了巨大成功。从随机噪声开始，扩散模型通过迭代去噪过程逐渐恢复样本。在本文中，我们提出了一种新的MVS框架，在MVS中引入了扩散模型。具体来说，我们将深度细化表述为条件扩散过程。考虑到深度估计的判别性特征，我们设计了一个条件编码器来指导扩散过程。为了提高效率，我们提出了一种结合轻量级2D U-Net和卷积GRU的新型扩散网络。此外，我们提出了一种基于置信度的采样策略，根据扩散模型估计的置信度自适应地对深度假设进行采样。基于我们新的MVS框架，我们提出了两种新的MVS方法：DiffMVS和CasDiffMVS。DiffMVS在运行时间和GPU内存方面实现了与最先进方法相竞争的性能。CasDiffMVS在DTU、Tanks &amp; Temples和ETH3D上实现了最先进的性能。代码可在https://github.com/cvg/diffmvs获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多视图立体(MVS)中的3D几何重建问题，特别是在保持计算效率的同时提高重建精度的问题。这个问题在现实中非常重要，因为MVS在机器人、自动驾驶、虚拟/混合现实和'元宇宙'等场景有广泛应用。现有的方法通常面临效率和精度之间的权衡，且在光照变化、低纹理区域等挑战性条件下表现不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有MVS方法的局限性，特别是离散深度采样和局部最小值问题。他们注意到扩散模型在生成任务中的成功，并思考如何将其应用于深度估计这一判别性任务。作者借鉴了多个现有工作：扩散模型的去噪过程、PatchMatch的最近邻搜索思想、RAFT中使用GRU进行迭代细化的方法、粗到细的多阶段框架，以及置信度估计在立体匹配中的应用。基于这些借鉴，作者设计了条件编码器、轻量级扩散网络和基于置信度的采样策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将扩散模型引入多视图立体匹配，将深度细化过程表述为条件扩散过程，通过引入随机噪声避免陷入局部最小值。整体流程包括：1)深度初始化：构建代价体积并预测初始深度图；2)基于扩散的细化：添加噪声并通过条件扩散模型去噪，使用条件编码器提供指导，结合2D U-Net和卷积GRU进行迭代细化；3)基于置信度的采样：根据估计的置信度自适应生成深度假设；4)学习上采样：将深度图上采样到全分辨率；5)训练损失：使用L1损失和置信度加权损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于条件扩散模型的新型MVS框架；2)融合匹配信息、图像上下文和深度上下文的条件编码器；3)基于置信度的自适应采样策略；4)结合轻量级2D U-Net和卷积GRU的高效扩散网络；5)提出的DiffMVS和CasDiffMVS两种方法。相比之前的工作，本文将生成式扩散模型应用于判别性深度估计任务，避免了传统3D卷积和注意力机制的高计算成本，同时通过随机噪声和条件指导提高了重建质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于条件扩散模型的新型多视图立体框架，通过轻量级网络设计和基于置信度的采样策略，实现了高效且准确的3D几何重建，在保持计算效率的同时达到了最先进的重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TPAMI.2025.3597148&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To reconstruct the 3D geometry from calibrated images, learning-basedmulti-view stereo (MVS) methods typically perform multi-view depth estimationand then fuse depth maps into a mesh or point cloud. To improve thecomputational efficiency, many methods initialize a coarse depth map and thengradually refine it in higher resolutions. Recently, diffusion models achievegreat success in generation tasks. Starting from a random noise, diffusionmodels gradually recover the sample with an iterative denoising process. Inthis paper, we propose a novel MVS framework, which introduces diffusion modelsin MVS. Specifically, we formulate depth refinement as a conditional diffusionprocess. Considering the discriminative characteristic of depth estimation, wedesign a condition encoder to guide the diffusion process. To improveefficiency, we propose a novel diffusion network combining lightweight 2D U-Netand convolutional GRU. Moreover, we propose a novel confidence-based samplingstrategy to adaptively sample depth hypotheses based on the confidenceestimated by diffusion model. Based on our novel MVS framework, we propose twonovel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitiveperformance with state-of-the-art efficiency in run-time and GPU memory.CasDiffMVS achieves state-of-the-art performance on DTU, Tanks &amp; Temples andETH3D. Code is available at: https://github.com/cvg/diffmvs.</description>
      <author>example@mail.com (Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys)</author>
      <guid isPermaLink="false">2509.15220v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2509.15123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用单个RGB视频作为监督的新方法，用于动态场景中的相机参数优化，该方法包含三个关键组件，实验证明其高效且准确。&lt;h4&gt;背景&lt;/h4&gt;COLMAP是静态场景中相机参数优化的主要方法，但在动态场景中受限于运行时间长和依赖真实运动掩码的问题。许多改进方法依赖于各种先验知识，但这些在日常拍摄的RGB视频中通常不可用。&lt;h4&gt;目的&lt;/h4&gt;开发一种更准确和高效的相机参数优化方法，专门针对动态场景，且仅使用单个RGB视频作为监督。&lt;h4&gt;方法&lt;/h4&gt;提出了一种包含三个关键组件的方法：(1)基于块的跟踪滤波器，建立视频中的鲁棒稀疏铰链关系；(2)异常感知联合优化，通过自适应降低移动异常值的权重来优化相机参数，不依赖运动先验；(3)两阶段优化策略，通过在Softplus限制和凸最小值之间权衡来提高稳定性和优化速度。&lt;h4&gt;主要发现&lt;/h4&gt;在4个真实世界数据集（NeRF-DS, DAVIS, iPhone, 和 TUM-dynamics）和1个合成数据集（MPI-Sintel）上的实验表明，该方法仅使用单个RGB视频作为监督，能够更高效、更准确地估计相机参数。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为动态场景中的相机参数优化提供了一种新的解决方案，仅依靠单个RGB视频作为输入，实现了高效且准确的相机参数估计。&lt;h4&gt;翻译&lt;/h4&gt;虽然COLMAP长期以来一直是静态场景中相机参数优化的主要方法，但它在动态场景中的应用受到运行时间长和对真实运动掩码依赖的限制。许多尝试通过加入更多先验作为监督（如真实焦距、运动掩码、3D点云、相机位姿和度量深度）来改进它，然而这些信息在日常拍摄的RGB视频中通常不可用。在本文中，我们提出了一种新方法，用于在动态场景中进行更准确和高效的相机参数优化，仅由单个RGB视频监督。我们的方法包含三个关键组件：(1)基于块的跟踪滤波器，在RGB视频间建立鲁棒且最大稀疏的铰链关系；(2)异常感知联合优化，通过自适应降低移动异常值的权重进行高效的相机参数优化，不依赖运动先验；(3)两阶段优化策略，通过在Softplus限制和损失凸最小值之间权衡来提高稳定性和优化速度。我们对相机估计进行了视觉和数值评估。为了进一步验证准确性，我们将相机估计输入到4D重建方法中，评估生成的3D场景、渲染的2D RGB和深度图。我们在4个真实世界数据集（NeRF-DS, DAVIS, iPhone, 和 TUM-dynamics）和1个合成数据集（MPI-Sintel）上进行了实验，证明我们的方法仅使用单个RGB视频作为监督，能够更高效、更准确地估计相机参数。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的是在仅使用RGB视频的情况下，如何准确高效地优化动态场景中的相机参数（包括焦距、旋转和平移）。这个问题在现实中非常重要，因为日常拍摄的视频通常只有RGB信息，没有额外的传感器数据（如深度或运动掩码），而准确的相机参数估计是3D场景重建和视图合成的基础，对于AR/VR、自动驾驶和机器人导航等领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计新方法：观察到COLMAP在动态场景中运行时间长且需要真实运动掩码；发现大多数改进方法依赖于额外的真实先验信息，这些在日常RGB视频中不可用；注意到现有RGB-only监督方法计算延迟高且无法自适应排除移动异常值。作者借鉴了预训练点跟踪模型（如CoTracker）来提取轨迹，并采用3D高斯表示进行场景重建，但创新性地将不确定性建模与3D校准点关联而非2D像素。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是仅使用RGB视频作为监督，通过分块跟踪滤波器提取鲁棒的伪监督信息，然后使用异常值感知联合优化方法估计相机参数，最后通过两阶段优化策略提高稳定性和效率。整体流程：1)分块跟踪滤波：将图像分块，识别高纹理块，选择梯度最高的点，过滤不可见轨迹和重叠轨迹；2)异常值感知联合优化：联合优化校准点、焦距、旋转、平移和不确定性参数，使用ACP误差和柯西损失降低异常值影响；3)两阶段优化：第一阶段固定不确定性参数快速收敛，第二阶段使用第一阶段结果初始化并联合优化所有参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)分块跟踪滤波器：基于预训练点跟踪模型建立鲁稀疏关系，避免密集预测延迟；2)异常值感知联合优化：引入与3D校准点关联的可学习不确定性，使用柯西分布建模；3)两阶段优化策略：基于Softplus函数分析和损失凸项最小值，提高稳定性和效率。相比之前工作：仅使用RGB视频监督；将不确定性参数与3D校准点关联而非2D像素，减少参数数量；提出两阶段优化策略；在多个数据集上展示优越性能和效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种仅使用RGB视频监督的创新方法，通过分块跟踪滤波器、异常值感知联合优化和两阶段优化策略，实现了动态场景中相机参数的高效准确估计，为从日常拍摄的视频中重建高保真动态场景提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although COLMAP has long remained the predominant method for camera parameteroptimization in static scenes, it is constrained by its lengthy runtime andreliance on ground truth (GT) motion masks for application to dynamic scenes.Many efforts attempted to improve it by incorporating more priors assupervision such as GT focal length, motion masks, 3D point clouds, cameraposes, and metric depth, which, however, are typically unavailable in casuallycaptured RGB videos. In this paper, we propose a novel method for more accurateand efficient camera parameter optimization in dynamic scenes solely supervisedby a single RGB video. Our method consists of three key components: (1)Patch-wise Tracking Filters, to establish robust and maximally sparsehinge-like relations across the RGB video. (2) Outlier-aware JointOptimization, for efficient camera parameter optimization by adaptivedown-weighting of moving outliers, without reliance on motion priors. (3) ATwo-stage Optimization Strategy, to enhance stability and optimization speed bya trade-off between the Softplus limits and convex minima in losses. Wevisually and numerically evaluate our camera estimates. To further validateaccuracy, we feed the camera estimates into a 4D reconstruction method andassess the resulting 3D scenes, and rendered 2D RGB and depth maps. We performexperiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics)and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimatescamera parameters more efficiently and accurately with a single RGB video asthe only supervision.</description>
      <author>example@mail.com (Fang Li, Hao Zhang, Narendra Ahuja)</author>
      <guid isPermaLink="false">2509.15123v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Random Masking: A Dual-Stream Approach for Rotation-Invariant Point Cloud Masked Autoencoders</title>
      <link>http://arxiv.org/abs/2509.14975v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, aceppted by DICTA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双流掩码方法，结合3D空间网格掩码和渐进式语义掩码，解决了现有旋转不变点云掩码自编码器忽略几何结构和语义连贯性的问题，在各种旋转场景下表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有的旋转不变点云掩码自编码器依赖于随机掩码策略，这些策略忽略了几何结构和语义连贯性。随机掩码将块独立处理，无法捕捉跨方向保持一致的空间关系，也无法识别旋转时保持身份的语义对象部分。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉几何结构和语义一致性的掩码方法，以改进旋转不变点云表示学习，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出双流掩码方法，结合3D空间网格掩码(通过坐标排序创建结构化模式捕捉跨方向几何关系)和渐进式语义掩码(使用注意力驱动聚类发现语义有意义部分并保持连贯性)，通过课程学习和动态权重进行编排，从几何理解逐步过渡到语义发现。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40、ScanObjectNN和OmniObject3D上的综合实验表明，新方法在各种旋转场景下都有持续改进，显示出比基线旋转不变方法显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;所提出的双流掩码策略作为即插即用组件，可集成到现有旋转不变框架中无需架构更改，确保了不同方法间的广泛兼容性，有效解决了现有方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;现有的旋转不变点云掩码自编码器(MAE)依赖于随机掩码策略，这些策略忽略了几何结构和语义连贯性。随机掩码将块独立处理，无法捕捉跨方向保持一致的空间关系，也无法忽略旋转时保持身份的语义对象部分。我们提出了一种双流掩码方法，结合3D空间网格掩码和渐进式语义掩码，以解决这些基本限制。网格掩码通过坐标排序创建结构化模式，以捕捉跨不同方向保持的几何关系；而语义掩码使用注意力驱动的聚类来发现语义上有意义的部分，并在掩码过程中保持其连贯性。这些互补的流通过课程学习和动态权重进行编排，从几何理解逐步过渡到语义发现。作为即插即用组件设计，我们的策略可以集成到现有的旋转不变框架中，无需架构更改，确保了不同方法间的广泛兼容性。在ModelNet40、ScanObjectNN和OmniObject3D上的综合实验表明，在各种旋转场景下都有持续改进，显示出比基线旋转不变方法显著的性能提升。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有旋转不变性点云掩码自编码器(RI-MAE)依赖随机掩码策略而忽视几何结构和语义一致性的问题。这个问题在现实中很重要，因为物体经常以任意方向出现，模型需要具备旋转不变性才能有效识别相同形状的不同方向，而随机掩码无法捕捉跨方向保持一致的空间关系和语义对象部分，导致模型在旋转场景下性能显著下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到有效的旋转不变学习需要在掩码过程中明确保持几何结构和语义一致性，掩码策略本身必须尊重不变属性而非随机处理。他们借鉴了课程学习的动态权重机制设计，使用注意力驱动的聚类方法识别语义区域，参考了2D视觉中的结构化掩码方法如进化部分掩码(EPM)和注意力引导掩码(AttnMask)，并采用期望最大化(EM)算法进行语义聚类。这些现有工作被创新性地整合到3D点云旋转不变框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出双流掩码方法，结合3D空间网格掩码和渐进式语义掩码，通过课程学习动态平衡几何结构与语义一致性。整体流程为：输入点云通过最远点采样分解为局部区域；空间网格掩码通过坐标排序创建结构化模式；语义掩码使用注意力驱动的聚类发现语义部分；通过动态权重参数α(t)从几何主导(α≈0)逐渐过渡到语义主导(α≈1)；最终掩码概率是两种策略的加权组合；未掩码区域通过旋转不变特征提取后输入编码器，解码器重建掩码区域。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：3D空间网格掩码保持跨旋转的几何一致性；渐进式语义掩码通过注意力聚类保持语义连贯性；双流课程学习框架从几何过渡到语义；即插即用设计无需架构修改；动态权重机制平衡两个流。相比之前工作，本文不依赖随机掩码或修改网络架构，而是在掩码策略层面解决旋转不变性问题，通过课程学习实现多尺度学习，且能作为即插即用组件与多种现有框架结合，在多个数据集和旋转场景下显示出一致的性能提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种双流掩码方法，结合3D空间网格掩码和渐进式语义掩码，通过课程学习动态平衡几何结构与语义一致性，显著提升了点云掩码自编码器在任意旋转场景下的性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing rotation-invariant point cloud masked autoencoders (MAE) rely onrandom masking strategies that overlook geometric structure and semanticcoherence. Random masking treats patches independently, failing to capturespatial relationships consistent across orientations and overlooking semanticobject parts that maintain identity regardless of rotation. We propose adual-stream masking approach combining 3D Spatial Grid Masking and ProgressiveSemantic Masking to address these fundamental limitations. Grid masking createsstructured patterns through coordinate sorting to capture geometricrelationships that persist across different orientations, while semanticmasking uses attention-driven clustering to discover semantically meaningfulparts and maintain their coherence during masking. These complementary streamsare orchestrated via curriculum learning with dynamic weighting, progressingfrom geometric understanding to semantic discovery. Designed as plug-and-playcomponents, our strategies integrate into existing rotation-invariantframeworks without architectural changes, ensuring broad compatibility acrossdifferent approaches. Comprehensive experiments on ModelNet40, ScanObjectNN,and OmniObject3D demonstrate consistent improvements across various rotationscenarios, showing substantial performance gains over the baselinerotation-invariant methods.</description>
      <author>example@mail.com (Xuanhua Yin, Dingxin Zhang, Yu Feng, Shunqi Mao, Jianhui Yu, Weidong Cai)</author>
      <guid isPermaLink="false">2509.14975v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Hint: hierarchical inter-frame correlation for one-shot point cloud sequence compression</title>
      <link>http://arxiv.org/abs/2509.14859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  \c{opyright} 2026 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HINT是一种整合时空相关性的顺序点云压缩方法，显著降低了编码和解码时间，同时提高了压缩效率。&lt;h4&gt;背景&lt;/h4&gt;深度学习在点云压缩领域表现出强大能力，熵建模被广泛用于无损压缩。然而，大多数方法仅依赖父级或同级上下文和逐层自回归，导致解码延迟高达10-100秒。&lt;h4&gt;目的&lt;/h4&gt;提出HINT方法，整合时空相关性进行顺序点云压缩，以解决现有方法的高延迟问题。&lt;h4&gt;方法&lt;/h4&gt;HINT采用两阶段时间特征提取：(i)父级存在图和(ii)前一帧的子级邻域查找。这些线索通过逐元素相加与空间特征融合，并使用分组策略进行编码。&lt;h4&gt;主要发现&lt;/h4&gt;HINT实现了105毫秒的编码时间和140毫秒的解码时间，与G-PCC相比分别实现了49.6倍和21.6倍的加速，同时实现了高达43.6%的比特率降低，并一致性地优于仅使用空间信息的强基线方法RENO。&lt;h4&gt;结论&lt;/h4&gt;HINT方法在点云压缩方面既高效又有效，显著改善了编码解码速度和压缩性能。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在点云压缩方面已展现出强大的能力。在该领域中，用于无损压缩的熵建模被广泛研究。然而，大多数方法仅依赖于父级或同级上下文和逐层自回归，导致解码延迟在10到100秒的量级。我们提出了HINT，一种整合时空相关性进行顺序点云压缩的方法。具体来说，它首先使用两阶段时间特征提取：(i)父级存在图和(ii)前一帧中的子级邻域查找。这些线索通过逐元素相加与空间特征融合，并使用分组策略进行编码。实验结果表明，HINT分别实现了105毫秒和140毫秒的编码和解码时间，与G-PCC相比分别实现了49.6倍和21.6倍的加速，同时实现了高达43.6%的比特率降低。此外，HINT一致性地优于仅使用空间信息的强基线方法RENO。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云序列压缩中的效率问题，特别是解码延迟过高的问题。现有基于深度学习的点云压缩方法解码延迟高达10-100秒，这对于自动驾驶、AR/VR和远程呈现等需要实时处理点云数据的领域来说太慢了。点云数据在这些领域迅速增长，原始点云极其消耗带宽，因此高效的几何压缩对可扩展的捕获、传输和存储至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：大多数方法仅依赖父/子上下文和逐层自回归，导致高解码延迟。他们发现现有方法主要关注单帧内的空间上下文，没有充分利用帧间的时间冗余。因此，他们设计了一个既能保持严格因果性，又能引入轻量级时间线索和兄弟线索的方法。他们借鉴了RENO框架的快速占用生成器(FOG)和快速坐标生成器(FCG)模块，但扩展了RENO，使其不仅适用于稀疏LiDAR序列，还能处理高密度点云序列。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合时间和空间相关性来提高点云序列的压缩效率。方法采用两阶段时间特征提取：父级存在映射和子级邻域查找，将这些线索与空间特征通过元素级相加融合，并使用分组策略进行编码。整体流程包括：1)量化和层次结构构建；2)父级时间相关性提取，创建当前帧和前一帧的'存在映射'；3)子级时间相关性提取，查询前一帧中对应位置的邻域；4)基于兄弟相关性的熵编码，将每个父体的8个子体素分为奇数组和偶数组进行编码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段时间特征提取(父级存在映射和子级邻域查找)；2)时间特征与空间特征的元素级相加融合；3)分组编码策略，将子体素分为奇偶两组；4)高效并行化设计，显著降低延迟。相比之前工作，HINT明确建模了时间相关性，不同于仅依赖空间上下文的方法；它保持严格因果性，不同于需要显式运动估计或自回归的方法；设计更轻量，计算和内存需求低于使用注意力机制的方法；相比基线方法RENO，它解决了时间和兄弟依赖性未被充分利用的问题，并将应用扩展到高密度点云序列。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HINT通过整合两阶段时间特征提取与分组编码策略，实现了点云序列的高效压缩，在保持高压缩率的同时将编码/解码速度提高了两个数量级，解决了现有方法中时间相关性利用不足和计算效率低下的问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has demonstrated strong capability in compressing point clouds.Within this area, entropy modeling for lossless compression is widelyinvestigated. However, most methods rely solely on parent orsibling contextsand level-wise autoregression, which suffers from decoding latency on the orderof 10 to 100 seconds. We propose HINT, a method that integrates temporal andspatial correlation for sequential point cloud compression. Specifically, itfirst uses a two stage temporal feature extraction: (i) a parent-levelexistence map and (ii) a child-level neighborhood lookup in the previous frame.These cues are fused with the spatial features via elementwise addition andencoded with a group-wise strategy. Experimental results show that HINTachieves encoding and decoding time at 105 ms and 140 ms, respectively,equivalent to 49.6x and 21.6x acceleration in comparison with G-PCC, whileachieving up to bit rate reduction of 43.6%, in addition, consistentlyoutperforming over the strong spatial only baseline (RENO).</description>
      <author>example@mail.com (Yuchen Gao, Qi Zhang)</author>
      <guid isPermaLink="false">2509.14859v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>MapAnything: Mapping Urban Assets using Single Street-View Images</title>
      <link>http://arxiv.org/abs/2509.14839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MapAnything是一个能够通过单张图像自动确定物体地理坐标的模块，可减少城市数据库维护的人工工作量。&lt;h4&gt;背景&lt;/h4&gt;城市管理部门维护包含交通标志、树木等物体及其地理坐标的数据库，涂鸦和道路损坏等事件也相关。随着数字化程度提高，需要更多数据和更新的数据库，这需要大量人工工作。&lt;h4&gt;目的&lt;/h4&gt;介绍MapAnything模块，该模块能够自动使用单个图像确定物体的地理坐标，并提供自动化城市物体和事件映射的建议。&lt;h4&gt;方法&lt;/h4&gt;利用先进的度量深度估计模型，根据物体与相机的距离、几何原理和相机规格计算地理坐标。通过将估计距离与LiDAR点云进行比较来评估准确性，并分析不同距离区间和语义区域（如道路和植被）的性能。&lt;h4&gt;主要发现&lt;/h4&gt;模块在估计距离方面具有准确性，在不同距离区间和语义区域表现出良好的性能。&lt;h4&gt;结论&lt;/h4&gt;通过交通标志和道路损坏的实际用例证明了MapAnything模块的有效性，能够帮助城市管理部门更高效地维护地理数据库。&lt;h4&gt;翻译&lt;/h4&gt;为了掌握城市状况概览，城市管理部门维护着包含交通标志和树木等物体的数据库，并附有它们的地理坐标。涂鸦或道路损坏等事件也很重要。随着数字化程度提高，对更多数据和更新数据库的需求也在增加，这需要大量人工工作。本文介绍了MapAnything，一个使用单张图像自动确定物体地理坐标的模块。利用先进的度量深度估计模型，MapAnything根据物体与相机的距离、几何原理和相机规格计算地理坐标。我们详细说明并验证了该模块，提供了自动化城市物体和事件映射的建议。我们的评估将估计距离的准确性与城市环境中的LiDAR点云进行了比较，分析了不同距离区间和道路和植被等语义区域的性能。通过涉及交通标志和道路损坏的实际用例，证明了该模块的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何自动确定城市物体（如交通标志、树木等）地理坐标的问题。这个问题很重要，因为城市管理部门需要维护包含物体位置信息的数据库，但手动创建和更新这些数据库需要大量人工工作，成本高昂且耗时。随着数字化程度提高，需要更及时更新的数据，而现有解决方案要么昂贵（如使用LiDAR或航拍图像），要么不够准确（如仅使用相机坐标），要么需要物体先验知识或多个连续图像。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有城市物体映射方法的局限性，包括成本高、不够准确或需要额外信息。他们借鉴了现有的单目度量深度估计模型（Monocular Metric Depth Estimation Models），这些模型可以从单张图像估计物体与相机的距离。作者设计的方法基于三个主要步骤：1)检测/分割图像中的目标物体；2)应用MapAnything模块估计深度并计算坐标；3)去除重复识别的物体并与数据库匹配。这种方法的核心创新是使用单张图像和深度估计模型来确定物体地理坐标，不需要额外信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用单张街景图像，通过先进的单目度量深度估计模型预测物体与相机的距离，然后结合相机位置、方向和几何原理计算物体的实际地理坐标。整体流程包括：1)物体检测/分割，使用模型定位图像中的目标物体；2)深度估计，使用深度估计模型预测物体到相机的距离；3)坐标计算，基于针孔相机模型和相机参数计算物体的经纬度坐标；4)后处理，去除重复预测并与数据库条目匹配。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)通用单图像方法，不需要额外图像或物体信息；2)应用并评估了四种最新深度估计模型在城市环境中的适用性；3)全面的评估体系，测量了不同距离和语义区域的性能；4)通过两个实际用例验证了方法有效性。相比之前的工作，这篇论文的不同之处在于不依赖额外信息（如航拍图像或物体大小）、专注于单目深度估计在城市地理定位中的应用、提供实际应用建议和全面的评估体系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于单张街景图像和深度估计的自动化方法，能够准确估计城市物体的地理坐标，为城市基础设施的数字化管理和维护提供了高效、低成本的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To maintain an overview of urban conditions, city administrations managedatabases of objects like traffic signs and trees, complete with theirgeocoordinates. Incidents such as graffiti or road damage are also relevant. Asdigitization increases, so does the need for more data and up-to-datedatabases, requiring significant manual effort. This paper introducesMapAnything, a module that automatically determines the geocoordinates ofobjects using individual images. Utilizing advanced Metric Depth Estimationmodels, MapAnything calculates geocoordinates based on the object's distancefrom the camera, geometric principles, and camera specifications. We detail andvalidate the module, providing recommendations for automating urban object andincident mapping. Our evaluation measures the accuracy of estimated distancesagainst LiDAR point clouds in urban environments, analyzing performance acrossdistance intervals and semantic areas like roads and vegetation. The module'seffectiveness is demonstrated through practical use cases involving trafficsigns and road damage.</description>
      <author>example@mail.com (Miriam Louise Carnot, Jonas Kunze, Erik Fastermann, Eric Peukert, André Ludwig, Bogdan Franczyk)</author>
      <guid isPermaLink="false">2509.14839v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>A Real-Time Multi-Model Parametric Representation of Point Clouds</title>
      <link>http://arxiv.org/abs/2509.14773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多模型参数化表示方法，用于实时表面检测和拟合，能够在保持实时性能的同时提高点云处理的准确性。&lt;h4&gt;背景&lt;/h4&gt;近年来，点云的参数化表示已被广泛应用于内存高效映射和多机器人协作等任务。高度自适应的模型（如样条曲面或二次曲面）在检测或拟合时计算成本高。而实时方法（如高斯混合模型或平面）自由度低，难以用少量基元实现高精度。&lt;h4&gt;目的&lt;/h4&gt;解决现有点云参数化表示方法中，高自适应模型计算复杂度高，而实时方法精度低的问题，提出一种能够实时进行表面检测和拟合的多模型参数化表示方法。&lt;h4&gt;方法&lt;/h4&gt;首先使用高斯混合模型将点云分割成多个簇；然后选择并合并平坦的簇形成平面或曲面；平面通过基于二维体素的边界描述方法进行拟合和界定；有曲率的表面通过B样条曲面进行拟合，同样使用边界描述方法。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共数据集上的评估表明，所提出的表面检测方法比最先进的方法具有更强的鲁棒性，效率提高了3.78倍；同时，该表示方法比高斯混合模型的精度提高了2倍，在低功耗机载计算机上运行速度达到36.4 fps。&lt;h4&gt;结论&lt;/h4&gt;该方法结合了高自适应模型的精度优势和实时方法的效率优势，能够在保持实时性能的同时提高点云表面检测和拟合的准确性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，点云的参数化表示已被广泛应用于内存高效映射和多机器人协作等任务。高度自适应的模型，如样条曲面或二次曲面，在检测或拟合时计算成本高。相比之下，实时方法，如高斯混合模型或平面，自由度低，难以用少量基元实现高精度。为解决这一问题，本文提出了一种具有实时表面检测和拟合功能的多模型参数化表示方法。具体而言，首先采用高斯混合模型将点云分割成多个簇。然后，选择平坦的簇合并成平面或曲面。平面可以通过基于二维体素的边界描述方法轻松拟合和界定。有曲率的表面通过B样条曲面进行拟合，并采用相同的边界描述方法。通过对多个公共数据集的评估，所提出的表面检测方法比最先进的方法具有更强的鲁棒性，效率提高了3.78倍。同时，该表示方法比高斯混合模型的精度提高了2倍，在低功耗机载计算机上运行速度达到36.4 fps。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云参数化表示的实时性与精度之间的矛盾问题。在机器人感知、SLAM（同步定位与地图构建）和多机器人协作等应用中，需要在有限计算资源上实时处理点云数据，同时保持高精度。现有方法要么计算成本高（如样条曲面），要么精度有限（如高斯混合模型），难以满足实际需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的优缺点：高精度模型计算量大，实时方法精度有限。然后提出结合多种模型优势的思路，针对不同区域特性使用最适合的表示方法。作者借鉴了GMM点云分割、B-spline曲面拟合和体素过滤等现有技术，但通过创新的集成层次聚类和平面/曲面检测方法，将这些技术有效整合为一个统一的多模型框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是根据点云中不同区域的几何特性，灵活选择最合适的参数化模型：无结构区域用高斯分布表示，平坦区域用平面表示，弯曲区域用B-spline曲面表示。整体流程包括：1) 体素过滤预处理点云；2) 使用集成层次GMM方法分割点云；3) 提取并合并平坦区域为平面或曲面；4) 将平面和曲面点变换到局部坐标系；5) 使用2D体素方法描述边界；6) 对弯曲区域进行B-spline曲面拟合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 改进的曲面检测方法，效率比现有方法提高3.78倍；2) 多模型参数化表示框架，有效结合高斯分布、平面和B-spline曲面。相比之前的工作，不同之处在于：现有方法通常只使用单一模型，而本文方法根据区域特性灵活选择；现有曲面检测方法计算量大不适合实时应用，而本文实现了实时曲面检测；现有多模型方法无法有效处理曲面区域，而本文能够有效识别和表示曲面区域。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种实时多模型点云参数化表示方法，结合高斯分布、平面和B-spline曲面的优势，在保持36.4fps实时性能的同时，将点云表示的准确性提高了两倍。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, parametric representations of point clouds have been widelyapplied in tasks such as memory-efficient mapping and multi-robotcollaboration. Highly adaptive models, like spline surfaces or quadrics, arecomputationally expensive in detection or fitting. In contrast, real-timemethods, such as Gaussian mixture models or planes, have low degrees offreedom, making high accuracy with few primitives difficult. To tackle thisproblem, a multi-model parametric representation with real-time surfacedetection and fitting is proposed. Specifically, the Gaussian mixture model isfirst employed to segment the point cloud into multiple clusters. Then, flatclusters are selected and merged into planes or curved surfaces. Planes can beeasily fitted and delimited by a 2D voxel-based boundary description method.Surfaces with curvature are fitted by B-spline surfaces and the same boundarydescription method is employed. Through evaluations on multiple publicdatasets, the proposed surface detection exhibits greater robustness than thestate-of-the-art approach, with 3.78 times improvement in efficiency.Meanwhile, this representation achieves a 2-fold gain in accuracy over Gaussianmixture models, operating at 36.4 fps on a low-power onboard computer.</description>
      <author>example@mail.com (Yuan Gao, Wei Dong)</author>
      <guid isPermaLink="false">2509.14773v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>LLM4MG: Adapting Large Language Model for Multipath Generation via Synesthesia of Machines</title>
      <link>http://arxiv.org/abs/2509.14711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次基于机器联觉(SoM)将大型语言模型(LLM)适应用于多路径生成(LLM4MG)，针对第六代(6G)车对基础设施(V2I)场景，构建了多模态感知-通信数据集SynthSoM-V2I，并通过特征提取、融合网络以及低秩适应和传播感知提示工程，实现了基于多模态感知数据的高精度多路径生成。&lt;h4&gt;背景&lt;/h4&gt;基于机器联觉(Synesthesia of Machines, SoM)的方法，应用于第六代(6G)车对基础设施(V2I)通信场景，需要高精度的多路径生成技术来支持系统设计。&lt;h4&gt;目的&lt;/h4&gt;首次将大型语言模型(LLM)适应用于多路径生成(LLM4MG)，实现基于多模态感知数据的高精度多路径生成，并验证其在不同场景下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;构建包含信道多路径信息、毫米波雷达感知数据、RGB-D图像和LiDAR点云的多模态数据集SynthSoM-V2I；利用LLaMA 3.2模型通过特征提取和融合网络对齐多模态特征空间与语义空间；采用低秩适应(LoRA)参数高效微调和传播感知提示工程实现知识迁移。&lt;h4&gt;主要发现&lt;/h4&gt;LLM4MG在视距(LoS)/非视距(NLoS)分类上达到92.76%的准确率；多路径功率/延迟生成的归一化均方误差(NMSE)分别为0.099/0.032；具有跨车辆交通密度、跨频段和跨场景的良好泛化能力；通过实际泛化和信道容量比较验证了其有效性和高精度多路径生成的必要性。&lt;h4&gt;结论&lt;/h4&gt;基于机器联觉的大型语言模型适应方法在6G V2I场景的多路径生成中表现出色，具有良好的泛化能力和实际应用价值，为高精度信道建模提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;基于机器联觉(SoM)，首次将大型语言模型(LLM)适应用于多路径生成(LLM4MG)。考虑到非典型的第六代(6G)车对基础设施(V2I)场景，构建了一个新的多模态感知-通信数据集，名为SynthSoM-V2I，包括信道多路径信息、毫米波(mmWave)雷达感知数据、RGB-D图像和光检测与测距(LiDAR)点云。基于SynthSoM-V2I数据集，所提出的LLM4MG利用大型语言模型Meta AI(LLaMA) 3.2通过多模态感知数据进行多路径生成。所提出的LLM4MG通过特征提取和融合网络将多模态特征空间与LLaMA语义空间对齐。为了进一步实现从预训练LLaMA到多路径生成的通用知识迁移，采用了低秩适应(LoRA)参数高效微调和传播感知提示工程。仿真结果表明，所提出的LLM4MG在视距(LoS)/非视距(NLoS)分类上的准确率为92.76%，多路径功率/延迟生成的归一化均方误差(NMSE)为0.099/0.032，并且在跨车辆交通密度(VTD)、跨频段和跨场景泛化方面优于传统深度学习方法。通过实际泛化验证了所提出LLM4MG的实用性。通过信道容量比较也证明了高精度多路径生成对系统设计的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Based on Synesthesia of Machines (SoM), a large language model (LLM) isadapted for multipath generation (LLM4MG) for the first time. Considering atypical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a newmulti-modal sensing-communication dataset is constructed, named SynthSoM-V2I,including channel multipath information, millimeter wave (mmWave) radar sensorydata, RGB-D images, and light detection and ranging (LiDAR) point clouds. Basedon the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language ModelMeta AI (LLaMA) 3.2 for multipath generation via multi-modal sensory data. Theproposed LLM4MG aligns the multi-modal feature space with the LLaMA semanticspace through feature extraction and fusion networks. To further achievegeneral knowledge transfer from the pre-trained LLaMA for multipath generationvia multi-modal sensory data, the low-rank adaptation (LoRA)parameter-efficient fine-tuning and propagation-aware prompt engineering areexploited. Simulation results demonstrate that the proposed LLM4MG outperformsconventional deep learning-based methods in terms of line-of-sight(LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipathpower/delay generation precision with normalized mean square error (NMSE) of0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, andcross-scenario generalization. The utility of the proposed LLM4MG is validatedby real-world generalization. The necessity of high-precision multipathgeneration for system design is also demonstrated by channel capacitycomparison.</description>
      <author>example@mail.com (Ziwei Huang, Shiliang Lu, Lu Bai, Xuesong Cai, Xiang Cheng)</author>
      <guid isPermaLink="false">2509.14711v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Feature-aligned Motion Transformation for Efficient Dynamic Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2509.14591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种特征对齐运动变换(FMT)框架用于动态点云压缩，通过时空对齐策略替代显式运动向量，并设计了随机访问参考策略支持双向运动引用和分层编码，实现了帧级并行压缩。实验表明该方法在压缩效率和性能上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;动态点云广泛应用于沉浸式现实、机器人和自动驾驶等领域，但其高效压缩面临挑战，因为点云的不规则结构和显著局部变化使得运动估计和补偿任务非常困难。&lt;h4&gt;目的&lt;/h4&gt;克服当前动态点云压缩方法中显式运动估计的局限性，提高压缩效率和性能。&lt;h4&gt;方法&lt;/h4&gt;提出特征对齐运动变换(FMT)框架，用时空对齐策略替代显式运动向量，在潜在空间条件编码框架中使用对齐特征作为时间上下文；设计随机访问参考策略，支持双向运动引用和分层编码，实现帧级并行压缩。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在编码和解码效率上优于D-DPCC和AdaDPCC，分别实现了20%和9.4%的BD-Rate降低。&lt;h4&gt;结论&lt;/h4&gt;FMT能够有效提高压缩效率和处理性能，证明了特征对齐运动变换在动态点云压缩中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;动态点云广泛应用于沉浸式现实、机器人和自动驾驶等应用中。高效压缩在很大程度上依赖于准确的运动估计和补偿，然而点云的不规则结构和显著的局部变化使得这一任务极具挑战性。当前方法通常依赖显式运动估计，其编码向量难以捕捉复杂的动态变化，且无法充分利用时间相关性。为克服这些局限，我们提出了一种用于动态点云压缩的特征对齐运动变换(FMT)框架。FMT用时空对齐策略替代显式运动向量，该策略隐式地建模连续的时间变化，并在潜在空间条件编码框架中使用对齐特征作为时间上下文。此外，我们设计了一种随机访问(RA)参考策略，支持双向运动引用和分层编码，从而实现帧级并行压缩。大量实验表明，我们的方法在编码和解码效率上均优于D-DPCC和AdaDPCC，同时分别实现了20%和9.4%的BD-Rate降低。这些结果突显了FMT在提高压缩效率和处理性能方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态点云压缩中的高效压缩问题。当前方法通常依赖显式的运动估计和补偿，但难以捕捉复杂的动态变化，无法充分利用时间相关性。这个问题在现实中非常重要，因为动态点云数据在沉浸式现实、机器人和自动驾驶等领域有广泛应用，但其巨大的数据量对存储和传输带来挑战。高效的压缩可以减少带宽需求，降低存储成本，使这些技术更加实用化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前动态点云压缩方法的局限性，特别是显式运动估计的问题。他们借鉴了深度学习在图像和视频压缩中的成功经验，将压缩的核心问题重新定义为去除空间和时间冗余。为了解决传统运动估计的局限性，作者将传统的运动估计和补偿重新表述为时空对齐任务，并借鉴了视频压缩中的随机访问参考策略，设计了非序列的分层编码结构。在方法设计上，作者引入了特征对齐运动变换框架，使用隐式神经表示通过MLP来建模连续时间变化，而不是使用显式运动向量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将传统的显式运动估计和补偿重新表述为时空对齐任务，使用特征对齐运动变换框架隐式地建模连续时间变化，并设计随机访问参考策略实现双向运动引用和分层编码。整体实现流程：编码端时，输入点云被逐步下采样，使用八叉树编码压缩坐标，通过双向特征对齐运动变换模块将当前帧与参考帧特征对齐生成运动感知上下文，使用条件熵模型压缩特征，并按非序列分层编码顺序处理；解码端时，从比特流恢复坐标和特征，使用相同模块对齐参考帧特征，通过上下文解码器生成时间对齐特征，逐步上采样重建完整点云，并根据编码顺序存储在参考缓冲区中。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)特征对齐运动变换框架，将运动估计重新表述为时空对齐任务，使用隐式神经表示替代显式运动向量；2)随机访问参考策略，设计非序列分层帧组结构，实现双向运动引用和并行压缩；3)条件熵模型，结合多种先验提高压缩效率。相比之前的工作，这篇论文不再依赖显式运动估计，而是采用隐式时空对齐；使用非序列分层编码结构替代传统顺序编码；支持双向引用和并行处理，在低比特率场景下表现出更好的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于特征对齐运动变换的动态点云压缩框架，通过隐式时空对齐和随机访问参考策略，显著提高了压缩效率和性能，实现了比现有方法更优的率失真性能和处理速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic point clouds are widely used in applications such as immersivereality, robotics, and autonomous driving. Efficient compression largelydepends on accurate motion estimation and compensation, yet the irregularstructure and significant local variations of point clouds make this taskhighly challenging. Current methods often rely on explicit motion estimation,whose encoded vectors struggle to capture intricate dynamics and fail to fullyexploit temporal correlations. To overcome these limitations, we introduce aFeature-aligned Motion Transformation (FMT) framework for dynamic point cloudcompression. FMT replaces explicit motion vectors with a spatiotemporalalignment strategy that implicitly models continuous temporal variations, usingaligned features as temporal context within a latent-space conditional encodingframework. Furthermore, we design a random access (RA) reference strategy thatenables bidirectional motion referencing and layered encoding, therebysupporting frame-level parallel compression. Extensive experiments demonstratethat our method surpasses D-DPCC and AdaDPCC in both encoding and decodingefficiency, while also achieving BD-Rate reductions of 20% and 9.4%,respectively. These results highlight the effectiveness of FMT in jointlyimproving compression efficiency and processing performance.</description>
      <author>example@mail.com (Xuan Deng, Xiandong Meng, Longguang Wang, Tiange Zhang, Xiaopeng Fan, Debin Zhao)</author>
      <guid isPermaLink="false">2509.14591v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model</title>
      <link>http://arxiv.org/abs/2509.14560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于分数扩散模型的自适应迭代点云去噪方法，通过估计噪声变化并确定自适应去噪计划，提高了去噪效率，实验证明该方法在保留细节和边界方面优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;点云去噪任务旨在从带有不同程度或模式噪声的扫描数据中恢复干净点云。现有最先进的方法通常训练深度神经网络来更新点位置，并经验性地重复去噪过程多次，但如何有效地安排迭代去噪过程来处理不同噪声水平或模式尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;开发一种自适应迭代点云去噪方法，能够根据噪声特征自动调整去噪过程，有效处理不同水平和模式的噪声，同时保留点云的形状边界和细节。&lt;h4&gt;方法&lt;/h4&gt;基于分数扩散模型设计自适应迭代点云去噪方法。对于给定噪声点云，首先估计噪声变化并确定自适应去噪计划和适当步长，然后按照自适应计划使用训练好的网络迭代更新点云。同时设计网络架构和两阶段采样策略，支持特征融合和梯度融合以实现迭代去噪。&lt;h4&gt;主要发现&lt;/h4&gt;与现有最先进方法相比，所提出方法能够获得更干净平滑的去噪点云，同时更好地保留形状边界和细节。在定性和定量评估中均优于其他方法，并且在处理不同噪声模式的合成数据集以及真实扫描数据集上表现出色。&lt;h4&gt;结论&lt;/h4&gt;基于分数扩散模型的自适应迭代点云去噪方法能够有效处理不同水平和模式的噪声，在保留点云细节和边界方面显著优于现有方法，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;点云去噪任务旨在从耦合有不同程度或模式噪声的扫描数据中恢复干净点云。最近的最先进方法通常训练深度神经网络来更新点位置，朝向干净点云，并经验性地重复去噪过程多次以获得去噪结果。如何有效安排迭代去噪过程以处理不同程度或模式的噪声尚不清楚。在本文中，我们提出了一种基于分数扩散模型的自适应迭代点云去噪方法。对于给定的噪声点云，我们首先估计噪声变化并确定具有适当步长的自适应去噪计划，然后按照自适应计划调用训练好的网络迭代更新点云。为促进这种自适应和迭代去噪过程，我们设计了网络架构和用于网络训练的两阶段采样策略，以支持迭代去噪的特征融合和梯度融合。与最先进的点云去噪方法相比，我们的方法获得了干净平滑的去噪点云，同时更好地保留了形状边界和细节。我们的结果不仅在定性和定量上都优于其他方法，而且在具有不同噪声模式的合成数据集以及真实扫描数据集上也表现更优。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云去噪任务中如何有效安排迭代去噪过程的问题。这个问题很重要，因为点云是机器人、自动驾驶、制造业等应用中的基本3D表示，但实际获取的点云常含有不同程度的噪声，影响后续处理和使用。有效的去噪方法能恢复干净准确的形状，为后续应用提供高质量数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到现有方法在迭代去噪过程中存在局限性，网络未针对迭代过程训练，且去噪过程缺乏针对性。作者借鉴了扩散模型的思想，这种模型基于非平衡热力学理论，原本用于数据生成。作者将其改造用于去噪任务，引入因子保持噪声点云分布的均值，设计了特征融合和梯度融合模块，并采用两阶段采样策略进行训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是基于得分的扩散模型进行自适应和迭代点云去噪，通过特征融合和梯度融合模块保留形状细节。实现流程包括：1)训练阶段：使用两阶段采样策略训练网络，包含特征提取、特征融合、梯度预测和梯度融合模块；2)推理阶段：估计噪声方差，确定自适应去噪时间表，迭代更新点位置，最后拼接恢复的块形成完整去噪点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于得分的扩散模型的自适应和迭代去噪方法；2)具有特征融合和梯度融合模块的网络架构及两阶段采样训练策略；3)针对不同噪声模式的泛化能力。相比之前工作，本文方法不是利用预训练模型，而是从头开始训练；为每个点云确定自适应去噪时间表而非使用固定时间表；通过融合模块更好地保留形状细节，避免薄结构表面坍塌。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于得分的扩散模型的自适应和迭代点云去噪方法，通过特征融合和梯度融合模块，能有效处理不同水平和模式的噪声，同时保留点云的形状边界和细节。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1111/cgf.70149&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud denoising task aims to recover the clean point cloud from thescanned data coupled with different levels or patterns of noise. The recentstate-of-the-art methods often train deep neural networks to update the pointlocations towards the clean point cloud, and empirically repeat the denoisingprocess several times in order to obtain the denoised results. It is not clearhow to efficiently arrange the iterative denoising processes to deal withdifferent levels or patterns of noise. In this paper, we propose an adaptiveand iterative point cloud denoising method based on the score-based diffusionmodel. For a given noisy point cloud, we first estimate the noise variation anddetermine an adaptive denoising schedule with appropriate step sizes, theninvoke the trained network iteratively to update point clouds following theadaptive schedule. To facilitate this adaptive and iterative denoising process,we design the network architecture and a two-stage sampling strategy for thenetwork training to enable feature fusion and gradient fusion for iterativedenoising. Compared to the state-of-the-art point cloud denoising methods, ourapproach obtains clean and smooth denoised point clouds, while preserving theshape boundary and details better. Our results not only outperform the othermethods both qualitatively and quantitatively, but also are preferable on thesynthetic dataset with different patterns of noises, as well as thereal-scanned dataset.</description>
      <author>example@mail.com (Zhaonan Wang, Manyi Li, ShiQing Xin, Changhe Tu)</author>
      <guid isPermaLink="false">2509.14560v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>A Software-Defined Radio Testbed for Distributed LiDAR Point Cloud Sharing with IEEE 802.11p in V2V Networks</title>
      <link>http://arxiv.org/abs/2509.14523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一个基于软件定义无线电(SDR)的IEEE 802.11p测试平台，用于分布式车对车(V2V)通信。&lt;h4&gt;背景&lt;/h4&gt;该平台弥合了网络仿真与实际部署之间的差距，提供了为经济高效的ADALM-Pluto SDR配置的模块化代码库。&lt;h4&gt;目的&lt;/h4&gt;创建一个可协作感知的V2V通信系统，能够共享LiDAR点云并在节点间融合成集体感知环境。&lt;h4&gt;方法&lt;/h4&gt;使用基于SDR的IEEE 802.11p测试平台；提供模块化代码库配置；使用ADALM-Pluto SDR；具备运行Docker和ROS、执行Matlab并通过USB与Pluto接口连接的设备可作为通信节点；分享LiDAR点云并在节点间融合；评估利用去中心化存储系统(IPFS和Filecoin)的理论模型；分析节点存储收敛、延迟和可扩展性等约束；提供信道质量研究。&lt;h4&gt;主要发现&lt;/h4&gt;评估了利用去中心化存储系统的理论模型，分析了节点存储收敛、延迟和可扩展性等约束条件。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提供结论。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个基于软件定义无线电(SDR)的IEEE 802.11p测试平台，用于分布式车对车(V2V)通信。该平台通过为经济高效的ADALM-Pluto SDR提供模块化代码库，弥合了网络仿真与部署之间的差距。任何能够运行带有ROS的Docker、执行Matlab并通过USB与Pluto接口连接的设备都可以作为通信节点。为了展示协作感知，我们在节点间共享LiDAR点云，并将它们融合成集体感知环境。我们评估了利用去中心化存储系统(IPFS和Filecoin)的理论模型，分析了节点存储收敛、延迟和可扩展性等约束条件。此外，我们还提供了信道质量研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a Software Defined Radio (SDR)-based IEEE 802.11p testbed fordistributed Vehicle-to-Vehicle (V2V) communication. The platform bridges thegap between network simulation and deployment by providing a modular codebaseconfigured for cost-effective ADALM-Pluto SDRs. Any device capable of running aDocker with ROS, executing Matlab and interface with a Pluto via USB can act asa communication node. To demonstrate collaborative sensing, we share LiDARpoint clouds between nodes and fuse them into a collective perceptionenvironment. We evaluated a theoretical model for leveraging decentralizedstorage systems (IPFS and Filecoin), analyzing constraints such as node storageconvergence, latency, and scalability. In addition, we provide a channelquality study.</description>
      <author>example@mail.com (Mario Hernandez, Elijah Bryce, Peter Stubberud, Ebrahim Saberinia, Brendan Morris)</author>
      <guid isPermaLink="false">2509.14523v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>MATTER: Multiscale Attention for Registration Error Regression</title>
      <link>http://arxiv.org/abs/2509.12924v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于回归的点云配准质量验证方法，替代传统的分类方法，实现了更细粒度的质量量化，并通过多尺度提取和注意力聚合提高了特征表示能力。&lt;h4&gt;背景&lt;/h4&gt;点云配准(PCR)对于SLAM和目标跟踪等下游任务至关重要，因此检测和量化配准不齐（PCR质量验证）是一个重要任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种更细粒度的点云配准质量验证方法，替代传统的分类方法，提高配准质量评估的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用回归方法进行PCR验证，通过多尺度提取和基于注意力的聚合扩展了不齐相关特征，实现了更精确的配准误差估计。&lt;h4&gt;主要发现&lt;/h4&gt;所提方法在多样化数据集上提供了准确和鲁棒的配准误差估计，特别是在处理具有异构空间密度的点云时表现优异。&lt;h4&gt;结论&lt;/h4&gt;当用于指导地图绘制任务时，与基于分类的最先进方法相比，本文方法在给定数量的重新配准帧下显著提高了地图绘制质量。&lt;h4&gt;翻译&lt;/h4&gt;点云配准(PCR)对于许多下游任务（如同步定位与地图构建SLAM和目标跟踪）至关重要。这使得检测和量化配准不齐，即PCR质量验证，成为一个重要任务。所有现有方法都将验证视为分类任务，旨在将PCR质量分配到几个类别中。在本工作中，我们使用回归方法进行PCR验证，允许对配准质量进行更细粒度的量化。我们还通过多尺度提取和基于注意力的聚合扩展了先前使用的不齐相关特征。这导致在多样化数据集上准确和鲁棒的配准误差估计，特别是对于具有异构空间密度的点云。此外，当用于指导地图绘制下游任务时，与最先进的基于分类的方法相比，我们的方法在给定数量的重新配准帧下显著提高了地图绘制质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准质量评估的问题。现有方法将配准质量验证视为分类任务，只能粗略地将质量分为几个类别。而这个问题在现实中非常重要，因为点云配准是SLAM、3D重建和机器人导航等下游任务的基础，配准错误会传播到后续应用中，导致地图失真和导航失败。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有分类方法的局限性，然后借鉴了FACT等工作的基本架构，但将其从分类改为回归任务。他们还借鉴了使用微分熵和Sinkhorn散度作为特征的方法。核心创新是设计了多尺度注意力机制，通过在不同几何尺度提取特征并使用注意力自适应融合，解决了单一尺度特征难以适应不同配准场景的问题。整体实现包括特征提取、多尺度注意力融合和误差预测三个主要步骤。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用回归而非分类来评估点云配准质量，并通过多尺度特征提取和注意力机制融合不同几何尺度的特征，提高对点云配准误差估计的鲁棒性。整体流程：1)将源点云和参考点云转换到同一坐标系并选择锚点；2)提取局部特征(微分熵、Sinkhorn散度、覆盖率)和全局特征(共视性分数、距离、源标志)；3)通过多尺度注意力机制融合不同尺度的特征；4)使用PointTransformer和MLP网络预测最终的配准误差。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次将点云配准质量评估从分类转变为回归，实现更细粒度的误差量化；2)提出多尺度注意力机制，自适应融合不同几何尺度的特征；3)显著提高了在异构空间密度点云和挑战性场景下的鲁棒性；4)在下游应用中能更有效地指导重新配准。相比之前工作，本文方法提供连续的误差预测而非离散分类，使用多尺度特征而非单一尺度，并且在多个数据集上均表现出更好的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MATTER通过多尺度注意力机制将点云配准质量评估从分类转变为回归，实现了更精确、更鲁棒的配准误差估计，显著提升了下游任务的质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration (PCR) is crucial for many downstream tasks, such assimultaneous localization and mapping (SLAM) and object tracking. This makesdetecting and quantifying registration misalignment, i.e., PCR qualityvalidation, an important task. All existing methods treat validation as aclassification task, aiming to assign the PCR quality to a few classes. In thiswork, we instead use regression for PCR validation, allowing for a morefine-grained quantification of the registration quality. We also extendpreviously used misalignment-related features by using multiscale extractionand attention-based aggregation. This leads to accurate and robust registrationerror estimation on diverse datasets, especially for point clouds withheterogeneous spatial densities. Furthermore, when used to guide a mappingdownstream task, our method significantly improves the mapping quality for agiven amount of re-registered frames, compared to the state-of-the-artclassification-based method.</description>
      <author>example@mail.com (Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik Forssén)</author>
      <guid isPermaLink="false">2509.12924v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>A-TDOM: Active TDOM via On-the-Fly 3DGS</title>
      <link>http://arxiv.org/abs/2509.12759v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a short white paper for a coming Journal Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;A-TDOM是一种基于On-the-Fly 3DGS优化的近实时真正射影像图生成方法，能够在几秒内优化每个新图像的3DGS，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;背景&lt;/h4&gt;真正射影像图(TDOM)是城市管理、城市规划、土地测量等领域的重要地理空间产品。传统TDOM生成方法依赖复杂的离线摄影测量流程，导致延迟，阻碍了实时应用。此外，由于相机姿态不准确、数字表面模型(DSM)问题和场景遮挡等挑战，TDOM质量可能会下降。&lt;h4&gt;目的&lt;/h4&gt;解决传统TDOM生成方法的延迟问题，并改善因相机姿态不准确、DSM问题和场景遮挡等因素导致的TDOM质量下降问题。&lt;h4&gt;方法&lt;/h4&gt;A-TDOM是一种基于On-the-Fly 3DGS优化的近实时TDOM生成方法。每获取一张图像，通过On-the-Fly SfM计算其姿态和稀疏点云。然后将新的高斯函数整合并优化到先前未见或粗略重建的区域。通过与正交splatting集成，A-TDOM可以在每次更新新的3DGS场后立即渲染。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的初步实验表明，A-TDOM能够近实时主动渲染TDOM，每个新图像的3DGS优化只需几秒钟，同时保持了可接受的渲染质量和TDOM几何精度。&lt;h4&gt;结论&lt;/h4&gt;A-TDOM是一种有效的近实时TDOM生成方法，解决了传统方法的延迟问题，在保持质量的同时实现了近实时性能。&lt;h4&gt;翻译&lt;/h4&gt;真正射影像图(TDOM)作为城市管理、城市规划、土地测量等各领域的重要地理空间产品发挥着关键作用。然而，传统的TDOM生成方法通常依赖复杂的离线摄影测量流程，导致延迟阻碍了实时应用。此外，由于相机姿态不准确、数字表面模型(DSM)和场景遮挡等各种挑战，TDOM的质量可能会下降。为解决这些挑战，本文提出了A-TDOM，一种基于On-the-Fly 3DGS优化的近实时TDOM生成方法。每获取一张图像，通过On-the-Fly SfM计算其姿态和稀疏点云。然后将新的高斯函数整合并优化到先前未见或粗略重建的区域。通过与正交splatting集成，A-TDOM可以在每次更新新的3DGS场后立即渲染。在多个基准测试上的初步实验表明，所提出的A-TDOM能够近实时主动渲染TDOM，每个新图像的3DGS优化仅需几秒钟，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统TDOM（真实数字正射影像图）生成方法依赖复杂离线流程、处理延迟大、无法满足实时应用需求的问题。这个问题很重要，因为TDOM是城市管理、城市规划、土地测量等领域的关键地理空间产品，实时生成能力对于应急响应、动态监测等场景至关重要，而传统方法不仅耗时，还依赖DSM（数字表面模型）进行遮挡检测，计算成本高且质量易受影响。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有3D高斯溅射（3DGS）方法能生成高质量TDOM但多为离线处理，因此思考如何实现实时生成。他们借鉴了On-the-Fly SfM技术用于即时姿态估计和稀疏点云更新，参考了Tortho-Gaussian和Ortho-3DGS等3DGS方法，但针对实时需求进行了改进。特别是解决了原始3DGS densification策略不可控的问题，创新性地提出了高斯采样和集成方法，并基于投影矩阵修改实现了正交溅射，这些都是对现有工作的有效借鉴和提升。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过即时3DGS优化实现近实时的TDOM生成，使用高斯采样和集成方法替代原始3DGS的densification策略，实现可控的在线训练，并利用正交溅射技术消除建筑立面影响。整体流程是：1)训练初始3DGS场；2)每收到新图像时，使用On-the-Fly SfM更新相机姿态和稀疏点云；3)通过Delaunay三角剖分确定需要优化的关键区域；4)使用高斯采样和集成方法添加新高斯；5)优化3DGS场；6)通过正交溅射生成更新后的TDOM。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三：1)A-TDOM工作流，实现无需DSM和遮挡检测的近实时TDOM生成；2)基于高斯采样和集成方法的近实时3DGS优化技术，能智能更新3DGS场；3)基于投影矩阵修改的正交溅射方法。相比之前工作，不同之处在于：传统方法依赖复杂离线流程，而A-TDOM支持实时处理；现有3DGS方法多为离线，而A-TDOM实现了在线优化；原始3DGS的densification不可控，而A-TDOM的高斯采样和集成方法解决了这一问题；A-TDOM使用Delaunay三角剖分确定关键区域，优化了训练过程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; A-TDOM通过创新的在线3DGS优化方法，实现了无需DSM和遮挡检测的近实时真实数字正射影像图生成，显著提高了地理空间产品生成的效率和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product invarious fields such as urban management, city planning, land surveying, etc.However, traditional TDOM generation methods generally rely on a complexoffline photogrammetric pipeline, resulting in delays that hinder real-timeapplications. Moreover, the quality of TDOM may degrade due to variouschallenges, such as inaccurate camera poses or Digital Surface Model (DSM) andscene occlusions. To address these challenges, this work introduces A-TDOM, anear real-time TDOM generation method based on On-the-Fly 3DGS optimization. Aseach image is acquired, its pose and sparse point cloud are computed viaOn-the-Fly SfM. Then new Gaussians are integrated and optimized into previouslyunseen or coarsely reconstructed regions. By integrating with orthogonalsplatting, A-TDOM can render just after each update of a new 3DGS field.Initial experiments on multiple benchmarks show that the proposed A-TDOM iscapable of actively rendering TDOM in near real-time, with 3DGS optimizationfor each new image in seconds while maintaining acceptable rendering qualityand TDOM geometric accuracy.</description>
      <author>example@mail.com (Yiwei Xu, Xiang Wang, Yifei Yu, Wentian Gan, Luca Morelli, Giulio Perda, Xiongwu Xiao, Zongqian Zhan, Xin Wang, Fabio Remondino)</author>
      <guid isPermaLink="false">2509.12759v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning</title>
      <link>http://arxiv.org/abs/2509.15097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合分层分解、FPGA直接方程求解和增量学习的混合框架，用于降低深度学习的计算和能源消耗，同时保持模型性能。&lt;h4&gt;背景&lt;/h4&gt;深度学习，特别是基础模型和大语言模型的大规模架构，带来了巨大的计算和能源需求，对可持续性构成显著挑战。&lt;h4&gt;目的&lt;/h4&gt;解决传统基于梯度训练方法的低效性问题，减少计算成本和能源消耗，同时保持模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出混合框架，将神经网络分为两个功能层：底层通过FPGA上的单步方程求解进行高效特征提取；高层采用自适应增量学习支持持续更新。在此基础上引入Compound LLM框架，在两个层次上部署LLM模块，底层处理可重用表示学习，高层进行自适应决策。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著降低计算成本，同时保持高性能模型，特别适合边缘部署和能源受限环境中的实时适应。&lt;h4&gt;结论&lt;/h4&gt;这种集成设计提高了可扩展性，减少了冗余计算，符合可持续AI的原则，为能源受限环境中的深度学习应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习不断增长的计算和能源需求，特别是在基础模型和大语言模型等大规模架构中，对可持续性构成了重大挑战。传统的基于梯度的训练方法效率低下，需要多次迭代更新和高能耗。为解决这些限制，我们提出了一种结合分层分解、基于FPGA的直接方程求解和增量学习的混合框架。我们的方法将神经网络分为两个功能层：底层通过FPGA上的单步方程求解进行优化，实现高效和可并行化的特征提取；而高层采用自适应增量学习，支持持续更新而无需完全重新训练。在此基础上，我们引入了Compound LLM框架，在两个层次上明确部署LLM模块。底层LLM以最低的能源开销处理可重用的表示学习，而顶层LLM通过节能感知的更新执行自适应决策。这种集成设计提高了可扩展性，减少了冗余计算，符合可持续AI的原则。理论分析和架构洞察表明，我们的方法显著降低了计算成本，同时保持了高性能模型，非常适合边缘部署和能源受限环境中的实时适应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rising computational and energy demands of deep learning, particularly inlarge-scale architectures such as foundation models and large language models(LLMs), pose significant challenges to sustainability. Traditionalgradient-based training methods are inefficient, requiring numerous iterativeupdates and high power consumption. To address these limitations, we propose ahybrid framework that combines hierarchical decomposition with FPGA-baseddirect equation solving and incremental learning. Our method divides the neuralnetwork into two functional tiers: lower layers are optimized via single-stepequation solving on FPGAs for efficient and parallelizable feature extraction,while higher layers employ adaptive incremental learning to support continualupdates without full retraining. Building upon this foundation, we introducethe Compound LLM framework, which explicitly deploys LLM modules across bothhierarchy levels. The lower-level LLM handles reusable representation learningwith minimal energy overhead, while the upper-level LLM performs adaptivedecision-making through energy-aware updates. This integrated design enhancesscalability, reduces redundant computation, and aligns with the principles ofsustainable AI. Theoretical analysis and architectural insights demonstratethat our method reduces computational costs significantly while preserving highmodel performance, making it well-suited for edge deployment and real-timeadaptation in energy-constrained environments.</description>
      <author>example@mail.com (Mohammad Saleh Vahdatpour, Huaiyuan Chu, Yanqing Zhang)</author>
      <guid isPermaLink="false">2509.15097v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.15096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为OmniSegmentor的新型多模态学习框架，用于语义分割任务。该框架包含两个关键创新：一个名为ImageNeXt的大规模多模态预训练数据集，以及一种高效的预训练方法，使模型能够编码不同模态的信息。&lt;h4&gt;背景&lt;/h4&gt;最近的研究已经证明多模态线索在鲁棒的语义分割中的优势。然而，针对多种视觉模态的灵活的预训练和微调管道尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;作者旨在开发一个通用的多模态预训练框架，无论涉及的模态如何任意组合，都能在各种场景下一致地增强模型的感知能力。&lt;h4&gt;方法&lt;/h4&gt;基于ImageNet构建了一个名为ImageNeXt的大规模数据集，包含五种流行的视觉模态；提供了一种高效的预训练方式，使模型能够编码ImageNeXt中不同模态的信息；开发了OmniSegmentor框架，这是一个通用的多模态预训练框架。&lt;h4&gt;主要发现&lt;/h4&gt;OmniSegmentor在多种多模态语义分割数据集上取得了新的最先进记录，包括NYU Depthv2、EventScape、MFNet、DeLiVER、SUNRGBD和KITTI-360。&lt;h4&gt;结论&lt;/h4&gt;OmniSegmentor框架成功地解决了多模态视觉学习的挑战，提供了一个灵活且高效的预训练和微调管道，适用于各种视觉模态的组合。&lt;h4&gt;翻译&lt;/h4&gt;最近的表示学习研究已经证明多模态线索对鲁棒的语义分割的益处。然而，针对多种视觉模态的灵活的预训练和微调管道仍未被探索。在本文中，我们提出了一种新型的多模态学习框架，称为OmniSegmentor。它有两个关键创新：1）基于ImageNet，我们构建了一个用于多模态预训练的大规模数据集，称为ImageNeXt，它包含五种流行的视觉模态。2）我们提供了一种高效的预训练方式，使模型能够在ImageNeXt中编码不同模态的信息。我们首次引入了一个通用的多模态预训练框架，无论涉及模态的任意组合如何，都能在各种场景下一致地增强模型的感知能力。值得注意的是，我们的OmniSegmentor在多种多模态语义分割数据集上取得了新的最先进记录，包括NYU Depthv2、EventScape、MFNet、DeLiVER、SUNRGBD和KITTI-360。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research on representation learning has proved the merits ofmulti-modal clues for robust semantic segmentation. Nevertheless, a flexiblepretrain-and-finetune pipeline for multiple visual modalities remainsunexplored. In this paper, we propose a novel multi-modal learning framework,termed OmniSegmentor. It has two key innovations: 1) Based on ImageNet, weassemble a large-scale dataset for multi-modal pretraining, called ImageNeXt,which contains five popular visual modalities. 2) We provide an efficientpretraining manner to endow the model with the capacity to encode differentmodality information in the ImageNeXt. For the first time, we introduce auniversal multi-modal pretraining framework that consistently amplifies themodel's perceptual capabilities across various scenarios, regardless of thearbitrary combination of the involved modalities. Remarkably, our OmniSegmentorachieves new state-of-the-art records on a wide range of multi-modal semanticsegmentation datasets, including NYU Depthv2, EventScape, MFNet, DeLiVER,SUNRGBD, and KITTI-360.</description>
      <author>example@mail.com (Bo-Wen Yin, Jiao-Long Cao, Xuying Zhang, Yuming Chen, Ming-Ming Cheng, Qibin Hou)</author>
      <guid isPermaLink="false">2509.15096v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Representation Learning of Phenotype Trajectories for pCR Prediction in Breast Cancer</title>
      <link>http://arxiv.org/abs/2509.14872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于成像数据的学习方法，用于预测乳腺癌患者对新辅助化疗的病理完全缓解反应。&lt;h4&gt;背景&lt;/h4&gt;有效的治疗决策需要能够预测个体治疗反应的模型，但疾病进展和治疗反应在不同患者间存在显著差异，这使得预测变得具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;从成像数据中学习治疗反应的早期动态表示，以预测接受新辅助化疗(NACT)的乳腺癌患者的病理完全缓解(pCR)。&lt;h4&gt;方法&lt;/h4&gt;利用乳腺磁共振成像(MRI)数据的纵向变化，在潜在空间中形成轨迹，作为成功反应预测的基础。采用多任务模型来表示外观，促进时间连续性，并考虑非反应者队列中较高的异质性。&lt;h4&gt;主要发现&lt;/h4&gt;在公开的ISPY-2数据集上，仅使用治疗前数据(T0)时，潜在轨迹空间中的线性分类器达到0.761的平衡准确率；使用早期反应数据(T0 + T1)时，准确率提高到0.811；使用四个成像时间点(T0 -&gt; T3)时，准确率进一步提高到0.861。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效预测乳腺癌患者对新辅助化疗的反应，且随着时间点的增加，预测准确率显著提高。&lt;h4&gt;翻译&lt;/h4&gt;有效的治疗决策需要能够预测个体治疗反应的模型。这具有挑战性，因为疾病进展和治疗反应在不同患者间存在显著差异。在这里，我们提出从成像数据中学习治疗反应的早期动态表示，以预测接受新辅助化疗(NACT)的乳腺癌患者的病理完全缓解(pCR)。乳腺磁共振成像(MRI)数据的纵向变化在潜在空间中形成轨迹，作为成功反应预测的基础。多任务模型表示外观，促进时间连续性，并考虑非反应者队列中较高的异质性。在公开的ISPY-2数据集上的实验中，潜在轨迹空间中的线性分类器仅使用治疗前数据(T0)时达到0.761的平衡准确率，使用早期反应数据(T0 + T1)时达到0.811，使用四个成像时间点(T0 -&gt; T3)时达到0.861。代码将在论文接受后公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective therapy decisions require models that predict the individualresponse to treatment. This is challenging since the progression of disease andresponse to treatment vary substantially across patients. Here, we propose tolearn a representation of the early dynamics of treatment response from imagingdata to predict pathological complete response (pCR) in breast cancer patientsundergoing neoadjuvant chemotherapy (NACT). The longitudinal change in magneticresonance imaging (MRI) data of the breast forms trajectories in the latentspace, serving as basis for prediction of successful response. The multi-taskmodel represents appearance, fosters temporal continuity and accounts for thecomparably high heterogeneity in the non-responder cohort.In experiments on thepublicly available ISPY-2 dataset, a linear classifier in the latent trajectoryspace achieves a balanced accuracy of 0.761 using only pre-treatment data (T0),0.811 using early response (T0 + T1), and 0.861 using four imaging time points(T0 -&gt; T3). The code will be made available upon paper acceptance.</description>
      <author>example@mail.com (Ivana Janíčková, Yen Y. Tan, Thomas H. Helbich, Konstantin Miloserdov, Zsuzsanna Bago-Horvath, Ulrike Heber, Georg Langs)</author>
      <guid isPermaLink="false">2509.14872v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study</title>
      <link>http://arxiv.org/abs/2509.14863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;G2LFormer是一种新型的全局到局部注意力方案的图变换器，通过浅层捕获全局信息和深层学习局部结构信息，有效解决了传统方法中的信息损失问题，在保持线性复杂度的同时表现出色。&lt;h4&gt;背景&lt;/h4&gt;Graph Transformers (GTs)在图表示学习中显示出巨大潜力，其架构通常将图神经网络(GNNs)与全局注意力机制结合，形成局部与全局或局部到全局的注意力方案。&lt;h4&gt;目的&lt;/h4&gt;提出G2LFormer，解决现有GTs整合方案中可能存在的信息损失问题，防止节点忽略其直接邻居。&lt;h4&gt;方法&lt;/h4&gt;G2LFormer采用全局到局部的注意力方案，浅层网络使用注意力机制捕获全局信息，深层网络使用GNN模块学习局部结构信息；同时引入有效的跨层信息融合策略，使局部层保留全局层的有益信息并减轻信息损失，在可接受的扩展性权衡下进行操作。&lt;h4&gt;主要发现&lt;/h4&gt;G2LFormer在节点级和图级任务上与最先进的线性GTs和GNNs比较，结果表明其在保持线性复杂度的同时表现出色。&lt;h4&gt;结论&lt;/h4&gt;G2LFormer通过全局到局部的注意力方案有效解决了信息损失问题，并在性能和复杂度之间取得了良好的平衡。&lt;h4&gt;翻译&lt;/h4&gt;图变换器(GTs)在图表示学习中显示出巨大的潜力。GTs的架构通常将图神经网络(GNNs)与全局注意力机制结合，形成并行关系或作为注意力机制的先导，从而产生局部与全局或局部到全局的注意力方案。然而，由于全局注意力机制主要捕获节点间的长程依赖关系，这些整合方案可能遭受信息损失，其中GNN学习的局部邻域信息可能被注意力机制稀释。因此，我们提出了G2LFormer，它具有一种新的全局到局部注意力方案，其中浅层网络层使用注意力机制捕获全局信息，而深层层采用GNN模块学习局部结构信息，从而防止节点忽略其直接邻居。引入了一种有效的跨层信息融合策略，使局部层能够保留来自全局层的有益信息并减轻信息损失，同时在可接受的扩展性权衡下进行操作。为了验证全局到局部注意力方案的可行性，我们在节点级和图级任务上将G2LFormer与最先进的线性GTs和GNNs进行了比较。结果表明，G2LFormer在保持线性复杂度的同时表现出优异的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) show considerable potential in graph representationlearning. The architecture of GTs typically integrates Graph Neural Networks(GNNs) with global attention mechanisms either in parallel or as a precursor toattention mechanisms, yielding a local-and-global or local-to-global attentionscheme. However, as the global attention mechanism primarily captureslong-range dependencies between nodes, these integration schemes may sufferfrom information loss, where the local neighborhood information learned by GNNcould be diluted by the attention mechanism. Therefore, we propose G2LFormer,featuring a novel global-to-local attention scheme where the shallow networklayers use attention mechanisms to capture global information, while the deeperlayers employ GNN modules to learn local structural information, therebypreventing nodes from ignoring their immediate neighbors. An effectivecross-layer information fusion strategy is introduced to allow local layers toretain beneficial information from global layers and alleviate informationloss, with acceptable trade-offs in scalability. To validate the feasibility ofthe global-to-local attention scheme, we compare G2LFormer withstate-of-the-art linear GTs and GNNs on node-level and graph-level tasks. Theresults indicate that G2LFormer exhibits excellent performance while keepinglinear complexity.</description>
      <author>example@mail.com (Zhengwei Wang, Gang Wu)</author>
      <guid isPermaLink="false">2509.14863v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation</title>
      <link>http://arxiv.org/abs/2509.14688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CoRL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种具有硬件和算法创新的触觉机器人学习系统，解决了数据稀缺性和稀疏性以及缺乏力反馈的挑战。&lt;h4&gt;背景&lt;/h4&gt;触觉感知的机器人学习面临数据收集和表示的关键挑战，主要问题包括数据稀缺性和稀疏性，以及现有系统中缺乏力反馈。&lt;h4&gt;目的&lt;/h4&gt;解决数据稀缺性和稀疏性以及缺乏力反馈的限制，引入具有硬件和算法创新的触觉机器人学习系统。&lt;h4&gt;方法&lt;/h4&gt;提出exUMI可扩展数据收集设备，增强原始UMI系统，配备AR动作捕捉和旋转编码器的鲁棒本体感受、模块化视觉-触觉传感和自动校准功能，实现100%数据可用性；基于超过100万个触觉帧的高效收集，提出触觉预测预训练(TPP)表示学习框架，通过感知感知的时间触觉预测捕捉接触动力学并缓解触觉稀疏性。&lt;h4&gt;主要发现&lt;/h4&gt;TPPP在现实世界实验中表现优于传统的触觉模仿学习方法。&lt;h4&gt;结论&lt;/h4&gt;通过协同设计的硬件和算法，弥合了人类触觉直觉与机器人学习之间的差距，提供开源资源以促进接触丰富的操作研究。&lt;h4&gt;翻译&lt;/h4&gt;触觉感知的机器人学习因数据稀缺性和稀疏性以及现有系统中缺乏力反馈而面临关键的数据收集和表示挑战。为解决这些限制，我们引入了具有硬件和算法创新的触觉机器人学习系统。我们提出了exUMI，一个可扩展的数据收集设备，通过AR动作捕捉和旋转编码器增强原始UMI的鲁棒本体感受，配备模块化视觉-触觉传感和自动校准功能，实现了100%的数据可用性。基于超过100万个触觉帧的高效收集，我们提出了触觉预测预训练(TPP)，一种通过感知感知的时间触觉预测的表示学习框架，捕捉接触动力学并缓解触觉稀疏性。现实世界的实验表明，TPPP优于传统的触觉模仿学习。我们的工作通过协同设计的硬件和算法弥合了人类触觉直觉与机器人学习之间的差距，为推进接触丰富的操作研究提供了开源资源。项目页面：https://silicx.github.io/exUMI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tactile-aware robot learning faces critical challenges in data collection andrepresentation due to data scarcity and sparsity, and the absence of forcefeedback in existing systems. To address these limitations, we introduce atactile robot learning system with both hardware and algorithm innovations. Wepresent exUMI, an extensible data collection device that enhances the vanillaUMI with robust proprioception (via AR MoCap and rotary encoder), modularvisuo-tactile sensing, and automated calibration, achieving 100% datausability. Building on an efficient collection of over 1 M tactile frames, wepropose Tactile Prediction Pretraining (TPP), a representation learningframework through action-aware temporal tactile prediction, capturing contactdynamics and mitigating tactile sparsity. Real-world experiments show that TPPoutperforms traditional tactile imitation learning. Our work bridges the gapbetween human tactile intuition and robot learning through co-designed hardwareand algorithms, offering open-source resources to advance contact-richmanipulation research. Project page: https://silicx.github.io/exUMI.</description>
      <author>example@mail.com (Yue Xu, Litao Wei, Pengyu An, Qingyu Zhang, Yong-Lu Li)</author>
      <guid isPermaLink="false">2509.14688v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training</title>
      <link>http://arxiv.org/abs/2509.14642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeCoP是一种依赖控制预训练框架，通过显式建模动态多尺度依赖关系来解决时间序列预训练中的挑战，在十个数据集上实现了最先进结果，且计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;时间序列预训练中动态时间依赖关系建模是关键挑战，这些依赖因分布漂移和多尺度模式而演变，严重损害预训练模型在下游任务上的泛化能力。现有框架无法捕捉短期和长期依赖关系的复杂交互，容易受虚假相关性影响。&lt;h4&gt;目的&lt;/h4&gt;解决现有框架在捕捉动态、多尺度依赖关系方面的局限性，提出能够明确模拟演化补丁间依赖关系的方法。&lt;h4&gt;方法&lt;/h4&gt;提出DeCoP框架，包含：1)输入层的实例级补丁归一化(IPN)减轻分布漂移并保留补丁独特特征；2)潜在层的层次化依赖控制学习(DCL)策略建模跨时间尺度的补丁间依赖关系；3)实例级对比模块(ICM)通过学习时间不变正对的实例判别性表示增强全局泛化。&lt;h4&gt;主要发现&lt;/h4&gt;DeCoP在十个数据集上实现最先进结果，与PatchTST相比，在仅使用37% FLOPs的情况下，在ETTh1上将MSE提高3%。&lt;h4&gt;结论&lt;/h4&gt;DeCoP是有效的时间序列预训练框架，能够处理动态时间依赖关系，在计算资源较少的情况下仍能取得优异性能。&lt;h4&gt;翻译&lt;/h4&gt;建模动态时间依赖关系是时间序列预训练中的关键挑战，这些依赖关系会因分布漂移和多尺度模式而演变。这种时间变化性严重损害了预训练模型在下游任务上的泛化能力。现有框架无法捕捉短期和长期依赖关系的复杂交互，容易受到虚假相关性的影响，从而降低泛化能力。为解决这些局限性，我们提出了DeCoP，一种依赖控制预训练框架，通过模拟演化的补丁间依赖关系来明确建模动态、多尺度依赖关系。在输入层面，DeCoP引入实例级补丁归一化(IPN)来减轻分布漂移，同时保留每个补丁的独特特征，为表示学习创建稳健基础。在潜在层面，层次化的依赖控制学习(DCL)策略明确建模跨多个时间尺度的补丁间依赖关系，实例级对比模块(ICM)通过从不随时间变化的正对中学习实例判别性表示来增强全局泛化。DeCoP在十个数据集上以更低的计算资源实现了最先进结果，与PatchTST相比，在仅使用37% FLOPs的情况下，在ETTh1上将MSE提高了3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling dynamic temporal dependencies is a critical challenge in time seriespre-training, which evolve due to distribution shifts and multi-scale patterns.This temporal variability severely impairs the generalization of pre-trainedmodels to downstream tasks. Existing frameworks fail to capture the complexinteractions of short- and long-term dependencies, making them susceptible tospurious correlations that degrade generalization. To address theselimitations, we propose DeCoP, a Dependency Controlled Pre-training frameworkthat explicitly models dynamic, multi-scale dependencies by simulating evolvinginter-patch dependencies. At the input level, DeCoP introduces Instance-wisePatch Normalization (IPN) to mitigate distributional shifts while preservingthe unique characteristics of each patch, creating a robust foundation forrepresentation learning. At the latent level, a hierarchical DependencyControlled Learning (DCL) strategy explicitly models inter-patch dependenciesacross multiple temporal scales, with an Instance-level Contrastive Module(ICM) enhances global generalization by learning instance-discriminativerepresentations from time-invariant positive pairs. DeCoP achievesstate-of-the-art results on ten datasets with lower computing resources,improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.</description>
      <author>example@mail.com (Yuemin Wu, Zhongze Wu, Xiu Su, Feng Yang, Hongyan Xu, Xi Lin, Wenti Huang, Shan You, Chang Xu)</author>
      <guid isPermaLink="false">2509.14642v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering</title>
      <link>http://arxiv.org/abs/2509.15024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AGCN(Attentive Graph Clustering Network)，一种将注意力机制直接嵌入图结构的新型架构，用于解决图聚类任务中GNN和Transformer的互补弱点问题。&lt;h4&gt;背景&lt;/h4&gt;注意力机制已成为现代神经网络的基石，但在图结构数据中的应用尚未充分探索，相比图神经网络表现较差，特别是在图聚类任务中。&lt;h4&gt;目的&lt;/h4&gt;解决图聚类任务中GNN过度强调邻域聚合而Transformer过度全局化的问题，探究注意力机制是否对无监督图学习本质上冗余，并提出一种新方法结合两者的优势。&lt;h4&gt;方法&lt;/h4&gt;提出Attentive Graph Clustering Network (AGCN)，直接将注意力机制嵌入图结构，同时引入KV缓存机制提高计算效率和成对边界对比损失增强注意力空间的判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和Transformer在图聚类中存在互补弱点，GNN过度强调邻域聚合导致节点表示同质化，而Transformer过度全局化牺牲了有意义的局部模式。&lt;h4&gt;结论&lt;/h4&gt;AGCN通过直接嵌入注意力机制到图结构中，能够有效提取全局信息同时保持对局部拓扑线索的敏感性，实验证明其性能优于最先进的方法。&lt;h4&gt;翻译&lt;/h4&gt;注意力机制已成为现代神经网络的基石，推动着各个领域的突破。然而，其在需要捕获拓扑连接的图结构数据中的应用仍然探索不足，相比图神经网络(GNN)表现较差，特别是在图聚类任务中。GNN倾向于过度强调邻域聚合，导致节点表示的同质化。相反，Transformer倾向于过度全局化，强调远距离节点而牺牲有意义的局部模式。这种二元对立引发了一个关键问题：注意力机制是否对无监督图学习本质上冗余？为了解决这个问题，我们进行了全面的实证分析，揭示了GNN和Transformer在图聚类中的互补弱点。受这些见解的启发，我们提出了Attentive Graph Clustering Network (AGCN)，一种重新诠释'图即注意力'概念的新架构。AGCN直接将注意力机制嵌入图结构，能够有效提取全局信息同时保持对局部拓扑线索的敏感性。我们的框架包含理论分析，对比了AGCN与GNN和Transformer的行为差异，并引入了两项创新：(1) KV缓存机制提高计算效率，(2) 成对边界对比损失增强注意力空间的判别能力。广泛的实验结果表明，AGCN优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attention mechanisms have become a cornerstone in modern neural networks,driving breakthroughs across diverse domains. However, their application tograph structured data, where capturing topological connections is essential,remains underexplored and underperforming compared to Graph Neural Networks(GNNs), particularly in the graph clustering task. GNN tends to overemphasizeneighborhood aggregation, leading to a homogenization of node representations.Conversely, Transformer tends to over globalize, highlighting distant nodes atthe expense of meaningful local patterns. This dichotomy raises a key question:Is attention inherently redundant for unsupervised graph learning? To addressthis, we conduct a comprehensive empirical analysis, uncovering thecomplementary weaknesses of GNN and Transformer in graph clustering. Motivatedby these insights, we propose the Attentive Graph Clustering Network (AGCN) anovel architecture that reinterprets the notion that graph is attention. AGCNdirectly embeds the attention mechanism into the graph structure, enablingeffective global information extraction while maintaining sensitivity to localtopological cues. Our framework incorporates theoretical analysis to contrastAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KVcache mechanism to improve computational efficiency, and (2) a pairwise margincontrastive loss to boost the discriminative capacity of the attention space.Extensive experimental results demonstrate that AGCN outperformsstate-of-the-art methods.</description>
      <author>example@mail.com (Xuanting Xie, Bingheng Li, Erlin Pan, Rui Hou, Wenyu Chen, Zhao Kang)</author>
      <guid isPermaLink="false">2509.15024v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis</title>
      <link>http://arxiv.org/abs/2509.14965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为Brain-HGCN的新型几何深度学习框架，基于双曲几何，用于处理功能性磁共振成像(fMRI)数据，以更好地建模大脑网络的层次结构。该框架在精神障碍分类任务中表现优异，显著优于多种最先进的欧几里得基线方法。&lt;h4&gt;背景&lt;/h4&gt;功能性磁共振成像(fMRI)通过生成复杂的功能网络（通常建模为图）提供了一种强大的非侵入性观察大脑功能组织的窗口。大脑网络表现出对认知处理至关重要的层次拓扑结构。然而，由于固有的空间限制，标准的欧几里得图神经网络(GNN)难以在没有高失真的情况下表示这些层次结构，限制了它们的临床性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更准确地表示大脑网络层次结构的几何深度学习框架，解决标准欧几里得GNN在处理fMRI数据时的局限性。&lt;h4&gt;方法&lt;/h4&gt;研究提出了Brain-HGCN，一种基于双曲几何的几何深度学习框架，利用负曲率空间的内在特性来高保真地建模大脑网络的层次结构。该模型基于洛伦兹模型，采用了一种新颖的双曲图注意力层，具有符号聚合机制，可以分别处理兴奋性和抑制性连接，最终通过几何上合理的弗雷谢均值进行图读出，学习鲁棒的图级表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模fMRI数据集上进行的精神障碍分类实验表明，该方法显著优于多种最先进的欧几里得基线方法。&lt;h4&gt;结论&lt;/h4&gt;这项工作为fMRI分析开创了一种新的几何深度学习范式，突显了双曲图神经计算精神病学领域的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;功能性磁共振成像(fMRI)通过生成复杂的功能网络（通常建模为图）提供了一种强大的非侵入性观察大脑功能组织的窗口。这些大脑网络表现出对认知处理至关重要的层次拓扑结构。然而，由于固有的空间限制，标准的欧几里得图神经网络(GNN)难以在没有高失真的情况下表示这些层次结构，限制了它们的临床性能。为解决这一局限性，我们提出了Brain-HGCN，一种基于双曲几何的几何深度学习框架，利用负曲率空间的内在特性来高保真地建模大脑网络的层次结构。基于洛伦兹模型，我们的模型采用了一种新颖的双曲图注意力层，具有符号聚合机制，可以分别处理兴奋性和抑制性连接，最终通过几何上合理的弗雷谢均值进行图读出，学习鲁棒的图级表示。在两个用于精神障碍分类的大规模fMRI数据集上的实验表明，我们的方法显著优于多种最先进的欧几里得基线方法。这项工作为fMRI分析开创了一种新的几何深度学习范式，突显了双曲图神经计算精神病学领域的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Functional magnetic resonance imaging (fMRI) provides a powerful non-invasivewindow into the brain's functional organization by generating complexfunctional networks, typically modeled as graphs. These brain networks exhibita hierarchical topology that is crucial for cognitive processing. However, dueto inherent spatial constraints, standard Euclidean GNNs struggle to representthese hierarchical structures without high distortion, limiting their clinicalperformance. To address this limitation, we propose Brain-HGCN, a geometricdeep learning framework based on hyperbolic geometry, which leverages theintrinsic property of negatively curved space to model the brain's networkhierarchy with high fidelity. Grounded in the Lorentz model, our model employsa novel hyperbolic graph attention layer with a signed aggregation mechanism todistinctly process excitatory and inhibitory connections, ultimately learningrobust graph-level representations via a geometrically sound Fr\'echet mean forgraph readout. Experiments on two large-scale fMRI datasets for psychiatricdisorder classification demonstrate that our approach significantly outperformsa wide range of state-of-the-art Euclidean baselines. This work pioneers a newgeometric deep learning paradigm for fMRI analysis, highlighting the immensepotential of hyperbolic GNNs in the field of computational psychiatry.</description>
      <author>example@mail.com (Junhao Jia, Yunyou Liu, Cheng Yang, Yifei Sun, Feiwei Qin, Changmiao Wang, Yong Peng)</author>
      <guid isPermaLink="false">2509.14965v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Towards Pre-trained Graph Condensation via Optimal Transport</title>
      <link>http://arxiv.org/abs/2509.14722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于最优传输的预训练图压缩方法(PreGC)，解决了传统GC方法依赖刚性GNN和任务特定监督的问题，实现了任务和架构无关的图压缩。&lt;h4&gt;背景&lt;/h4&gt;传统图压缩(GC)方法旨在将原始图压缩为小规模图以减少冗余并加速GNN训练，但这些方法严重依赖刚性GNN和任务特定监督，限制了其可重用性和跨任务/架构的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;从GNN优化一致性的角度重新审视理想GC的目标，提出一种任务和架构无关的图压缩方法。&lt;h4&gt;方法&lt;/h4&gt;提出基于最优传输的预训练图压缩(PreGC)，包括混合区间图扩散增强来提高压缩图泛化能力，建立最优图传输计划与表示传输计划之间的匹配保持语义一致性，以及可追溯的语义协调器桥接语义关联。&lt;h4&gt;主要发现&lt;/h4&gt;通过广义GC优化目标推导，传统GC方法可作为该优化范式的特例；PreGC方法具有任务无关性质并能与任意GNN无缝兼容。&lt;h4&gt;结论&lt;/h4&gt;PreGC方法超越了任务和架构依赖的GC方法的局限性，在实验中展示了优越性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;图压缩(GC)旨在将原始图压缩为小规模图，减少冗余并加速GNN训练。然而，传统GC方法严重依赖刚性GNN和任务特定监督。这种依赖严重限制了它们在各种任务和架构中的可重用性和泛化能力。在本工作中，我们从GNN优化一致性的角度重新审视了理想GC的目标，然后推导出广义GC优化目标，通过这个目标，那些传统GC方法可以很好地被视为这种优化范式的特例。基于此，提出了基于最优传输的预训练图压缩(PreGC)，以超越任务和架构依赖的GC方法的局限性。具体而言，提出了混合区间图扩散增强，通过增强节点状态的不确定性来抑制压缩图在特定架构上的弱泛化能力。同时，巧妙地建立了最优图传输计划与表示传输计划之间的匹配，以保持源图和压缩图空间间的语义一致性，从而使图压缩摆脱任务依赖。为了进一步促进压缩图适应各种下游任务，提出了从源节点到压缩节点的可追溯语义协调器，通过预训练中的优化表示传输计划桥接语义关联。大量实验验证了PreGC的优越性和通用性，展示了其任务无关性质和与任意GNN的无缝兼容性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph condensation (GC) aims to distill the original graph into a small-scalegraph, mitigating redundancy and accelerating GNN training. However,conventional GC approaches heavily rely on rigid GNNs and task-specificsupervision. Such a dependency severely restricts their reusability andgeneralization across various tasks and architectures. In this work, we revisitthe goal of ideal GC from the perspective of GNN optimization consistency, andthen a generalized GC optimization objective is derived, by which thosetraditional GC methods can be viewed nicely as special cases of thisoptimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC)via optimal transport is proposed to transcend the limitations of task- andarchitecture-dependent GC methods. Specifically, a hybrid-interval graphdiffusion augmentation is presented to suppress the weak generalization abilityof the condensed graph on particular architectures by enhancing the uncertaintyof node states. Meanwhile, the matching between optimal graph transport planand representation transport plan is tactfully established to maintain semanticconsistencies across source graph and condensed graph spaces, thereby freeinggraph condensation from task dependencies. To further facilitate the adaptationof condensed graphs to various downstream tasks, a traceable semanticharmonizer from source nodes to condensed nodes is proposed to bridge semanticassociations through the optimized representation transport plan inpre-training. Extensive experiments verify the superiority and versatility ofPreGC, demonstrating its task-independent nature and seamless compatibilitywith arbitrary GNNs.</description>
      <author>example@mail.com (Yeyu Yan, Shuai Zheng, Wenjun Hui, Xiangkai Zhu, Dong Chen, Zhenfeng Zhu, Yao Zhao, Kunlun He)</author>
      <guid isPermaLink="false">2509.14722v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control</title>
      <link>http://arxiv.org/abs/2509.14431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LEGO的局部规范化等变图神经网络框架，用于解决多智能体强化学习中的关键挑战，特别是在竞争环境和不同智能体数量下的泛化问题。&lt;h4&gt;背景&lt;/h4&gt;多智能体强化学习(MARL)已成为协调复杂决策中智能体群体的强大范式，但仍存在重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决多智能体强化学习中的三个主要挑战：竞争环境中的训练不稳定、非动能对抗措施在不利条件下的失效、以及训练策略难以泛化到不同智能体数量的环境。&lt;h4&gt;方法&lt;/h4&gt;提出LEGO框架，该框架使用图神经网络捕获排列等变性并泛化到不同数量的智能体，通过规范化强制执行E(n)-等变性，并采用异构表示编码角色特定的归纳偏置，可与流行的MARL算法如MAPPO无缝集成。&lt;h4&gt;主要发现&lt;/h4&gt;在合作和竞争性群体基准测试中，LEGO优于强大的基线方法并提高了泛化能力；在真实世界的实验中，LEGO展示了对不同团队规模和智能体故障的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;LEGO框架有效解决了多智能体强化学习中的关键挑战，特别是在竞争环境和不同智能体数量下的泛化问题，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;多智能体强化学习(MARL)已成为协调复杂决策中智能体群体的强大范式，但仍存在重大挑战。在追逐者-逃避者等竞争性场景中，同时适应可能导致训练不稳定；非动能对抗措施在不利条件下往往失效；在一种配置中训练的策略很少能泛化到具有不同智能体数量的环境。为解决这些问题，我们提出了局部规范化等变图神经网络(LEGO)框架，该框架可与流行的MARL算法(如MAPPO)无缝集成。LEGO使用图神经网络来捕获排列等变性并泛化到不同数量的智能体，通过规范化强制执行E(n)-等变性，并使用异构表示来编码角色特定的归纳偏置。在合作和竞争性群体基准测试中，LEGO优于强大的基线方法并提高了泛化能力。在真实世界的实验中，LEGO展示了对不同团队规模和智能体故障的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigmfor coordinating swarms of agents in complex decision-making, yet majorchallenges remain. In competitive settings such as pursuer-evader tasks,simultaneous adaptation can destabilize training; non-kinetic countermeasuresoften fail under adverse conditions; and policies trained in one configurationrarely generalize to environments with a different number of agents. To addressthese issues, we propose the Local-Canonicalization Equivariant Graph NeuralNetworks (LEGO) framework, which integrates seamlessly with popular MARLalgorithms such as MAPPO. LEGO employs graph neural networks to capturepermutation equivariance and generalization to different agent numbers,canonicalization to enforce E(n)-equivariance, and heterogeneousrepresentations to encode role-specific inductive biases. Experiments oncooperative and competitive swarm benchmarks show that LEGO outperforms strongbaselines and improves generalization. In real-world experiments, LEGOdemonstrates robustness to varying team sizes and agent failure.</description>
      <author>example@mail.com (Keqin Wang, Tao Zhong, David Chang, Christine Allen-Blanchette)</author>
      <guid isPermaLink="false">2509.14431v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>How Bad Is Forming Your Own Multidimensional Opinion?</title>
      <link>http://arxiv.org/abs/2509.14411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Appeared in 26th ACM Conference on Economics and Computation (EC'25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究社交网络中互联话题上观点形成的模型，特别是在多维模型下提供无政府状态价格的紧边界，并探讨更复杂的相互依赖关系和群体结构的影响。&lt;h4&gt;背景&lt;/h4&gt;社交网络中互联话题上的观点形成对理解集体行为和决策具有重要意义，现有模型基于个人观点是同伴观点与自身信念的加权平均这一假设，该过程被视为最佳响应博弈。&lt;h4&gt;目的&lt;/h4&gt;为多维观点形成模型提供无政府状态价格的紧边界，并将模型推广到更复杂的相互依赖关系，同时研究非二次惩罚下的边界情况。&lt;h4&gt;方法&lt;/h4&gt;遵循Bhawalkar、Gollapudi和Munagala的研究方法，分析多维模型下的无政府状态价格，并扩展到群体内部和外部分歧惩罚的场景。&lt;h4&gt;主要发现&lt;/h4&gt;多维模型下的无政府状态价格边界与标量模型匹配；即使增加群体内部和外部分歧惩罚的复杂性，这些边界仍然保持不变。&lt;h4&gt;结论&lt;/h4&gt;研究解决了多维模型中无政府状态价格边界未知的开放问题，发现即使考虑更复杂的相互依赖关系和群体结构，边界仍然保持不变。&lt;h4&gt;翻译&lt;/h4&gt;理解社交网络中互联话题上的观点形成具有重要意义。它为集体行为和决策提供了见解，并在图神经网络中有应用。现有模型提出个人观点是基于同龄人观点和自身信念的加权平均值形成的。这个平均过程被视为最佳响应博弈，可以看作是个体最小化与同伴的分歧，通过二次惩罚定义，导致均衡状态。Bindel、Kleinberg和Oren（FOCS 2011）提供了无政府状态价格的紧边界，定义为均衡时总体最大分歧与社会最优值的比率。Bhawalkar、Gollapudi和Munagala（STOC 2013）将惩罚函数推广到非二次惩罚，并提供了无政府状态价格的紧边界。当考虑多个话题时，个人观点可以表示为向量。Parsegov、Proskurnikov、Tempo和Friedkin（2016）提出了一个多维模型，使用加权平均过程，但话题间有恒定的相互依赖关系。然而，该模型的无政府状态价格边界问题仍然悬而未决。我们通过为多维模型提供无政府状态价格的紧边界来解决这一问题，同时将其推广到更复杂的相互依赖关系。遵循Bhawalkar、Gollapudi和Munagala的工作，我们提供了非二次惩罚下无政府状态价格的紧边界。令人惊讶的是，这些边界与标量模型匹配。我们进一步证明，即使增加另一层复杂性，涉及个人组最小化其整体内部和外部分歧惩罚（现实生活中常见的情况），这些边界仍然保持不变。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3736252.3742669&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the formation of opinions on interconnected topics withinsocial networks is of significant importance. It offers insights intocollective behavior and decision-making, with applications in Graph NeuralNetworks. Existing models propose that individuals form opinions based on aweighted average of their peers' opinions and their own beliefs. This averagingprocess, viewed as a best-response game, can be seen as an individualminimizing disagreements with peers, defined by a quadratic penalty, leading toan equilibrium. Bindel, Kleinberg, and Oren (FOCS 2011) provided tight boundson the "price of anarchy" defined as the maximum overall disagreement atequilibrium relative to a social optimum. Bhawalkar, Gollapudi, and Munagala(STOC 2013) generalized the penalty function to non-quadratic penalties andprovided tight bounds on the price of anarchy.  When considering multiple topics, an individual's opinions can be representedas a vector. Parsegov, Proskurnikov, Tempo, and Friedkin (2016) proposed amultidimensional model using the weighted averaging process, but with constantinterdependencies between topics. However, the question of the price of anarchyfor this model remained open. We address this by providing tight bounds on themultidimensional model, while also generalizing it to more complexinterdependencies. Following the work of Bhawalkar, Gollapudi, and Munagala, weprovide tight bounds on the price of anarchy under non-quadratic penalties.Surprisingly, these bounds match the scalar model. We further demonstrate thatthe bounds remain unchanged even when adding another layer of complexity,involving groups of individuals minimizing their overall internal and externaldisagreement penalty, a common occurrence in real-life scenarios.</description>
      <author>example@mail.com (Kiarash Banihashem, MohammadTaghi Hajiaghayi, Mahdi JafariRaviz, Danny Mittal, Alipasha Montaseri)</author>
      <guid isPermaLink="false">2509.14411v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Property-Isometric Variational Autoencoders for Sequence Modeling and Design</title>
      <link>http://arxiv.org/abs/2509.14287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 6 figures, preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PrIVAE的保留几何特性的变分自编码器框架，用于设计具有理想功能特性的生物序列（DNA、RNA或肽），解决了现有模型无法优化复杂高维特性的挑战。&lt;h4&gt;背景&lt;/h4&gt;生物序列设计在发现新型纳米材料、生物传感器、抗菌药物等领域有广泛应用，但优化复杂高维特性（如DNA介导的荧光纳米颗粒的发射光谱、光化学稳定性以及抗菌肽的抗菌活性）是一大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型依赖简单二元标签而非高维复杂特性的问题，开发一种能够学习保留属性空间几何特性的序列设计框架。&lt;h4&gt;方法&lt;/h4&gt;PrIVAE框架将属性空间建模为高维流形，通过最近邻图近似，并使用图神经网络编码器层和等距正则化器来指导序列潜在表示，学习一个属性组织的潜在空间，使训练的解码器能够设计具有理想属性的新序列。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在DNA序列设计（用于模板荧光金属纳米团簇）和抗菌肽设计两个任务中表现出色，保持高重建精度的同时根据属性组织潜在空间；湿实验显示，稀有属性纳米团的富集度比训练数据中高16.1倍。&lt;h4&gt;结论&lt;/h4&gt;PrIVAE框架能够有效解决生物序列设计中优化复杂高维特性的挑战，在实际应用中具有显著实用价值，可大幅提高稀有属性纳米团的富集度。&lt;h4&gt;翻译&lt;/h4&gt;生物序列设计（DNA、RNA或肽）具有理想功能特性，在发现新型纳米材料、生物传感器、抗菌药物等领域有应用。一个常见挑战是优化复杂的高维特性，如DNA介导的荧光纳米颗粒的目标发射光谱、光化学稳定性以及针对目标微生物的肽的抗菌活性。现有模型依赖简单的二元标签（如结合/非结合）而非高维复杂特性。为解决这一差距，我们提出了一个保留几何特性的变分自编码器框架PrIVAE，它学习保留属性空间几何特性的潜在序列嵌入。具体而言，我们将属性空间建模为高维流形，在适当定义的距离度量下可由最近邻图局部近似。我们使用属性图通过（1）图神经网络编码器层和（2）等距正则化器来指导序列潜在表示。PrIVAE学习一个属性组织的潜在空间，通过训练的解码器实现具有理想属性的新序列的合理设计。我们评估了该框架在两个生成任务中的效用：（1）设计DNA序列以模板荧光金属纳米团簇，和（2）设计抗菌肽。训练的模型保持高重建精度的同时根据属性组织潜在空间。除了计算机模拟实验外，我们还使用采样序列进行DNA纳米团湿实验室设计，与训练数据中的丰度相比，稀有属性纳米团富集度高达16.1倍，证明了该框架的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biological sequence design (DNA, RNA, or peptides) with desired functionalproperties has applications in discovering novel nanomaterials, biosensors,antimicrobial drugs, and beyond. One common challenge is the ability tooptimize complex high-dimensional properties such as target emission spectra ofDNA-mediated fluorescent nanoparticles, photo and chemical stability, andantimicrobial activity of peptides across target microbes. Existing models relyon simple binary labels (e.g., binding/non-binding) rather thanhigh-dimensional complex properties. To address this gap, we propose ageometry-preserving variational autoencoder framework, called PrIVAE, whichlearns latent sequence embeddings that respect the geometry of their propertyspace. Specifically, we model the property space as a high-dimensional manifoldthat can be locally approximated by a nearest neighbor graph, given anappropriately defined distance measure. We employ the property graph to guidethe sequence latent representations using (1) graph neural network encoderlayers and (2) an isometric regularizer. PrIVAE learns a property-organizedlatent space that enables rational design of new sequences with desiredproperties by employing the trained decoder. We evaluate the utility of ourframework for two generative tasks: (1) design of DNA sequences that templatefluorescent metal nanoclusters and (2) design of antimicrobial peptides. Thetrained models retain high reconstruction accuracy while organizing the latentspace according to properties. Beyond in silico experiments, we also employsampled sequences for wet lab design of DNA nanoclusters, resulting in up to16.1-fold enrichment of rare-property nanoclusters compared to their abundancein training data, demonstrating the practical utility of our framework.</description>
      <author>example@mail.com (Elham Sadeghi, Xianqi Deng, I-Hsin Lin, Stacy M. Copp, Petko Bogdanov)</author>
      <guid isPermaLink="false">2509.14287v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.15033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种改进多变量异常检测的方法，通过建模多元时间序列数据中的时变非线性时空相关性，解决了现有方法假设变量独立性而忽略现实世界交互的问题。&lt;h4&gt;背景&lt;/h4&gt;在多变量时间序列数据中，异常可能表现为相互关联的时间序列同时偏离预期集体行为，即使单个时间序列本身无明显异常。现有方法通常假设时间序列变量是(条件)独立的，这简化了现实世界的复杂交互关系。&lt;h4&gt;目的&lt;/h4&gt;改进多变量异常检测性能，通过准确建模多元时间序列数据中的时变非线性时空相关性，捕捉变量间的复杂依赖关系。&lt;h4&gt;方法&lt;/h4&gt;作者提出的方法在潜在空间中建模联合依赖关系，并解耦边际分布、时间动态和变量间依赖的建模。使用transformer编码器捕获时间模式，通过拟合多变量似然和copula建模空间(变量间)依赖。时间和空间组件在潜在空间中使用自监督对比学习目标联合训练，学习能分离正常和异常样本的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过建模时间序列间的复杂依赖关系，即使单个时间序列无明显异常，也能检测出多变量数据中的异常模式。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过同时考虑时间动态和变量间依赖关系，提高了多变量异常检测的准确性和鲁棒性，能够更有效地识别复杂场景中的异常。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们旨在通过建模多元时间序列数据中发现的时变非线性时空相关性来改进多变量异常检测。在多元时间序列数据中，异常可能表现为相互关联的时间序列同时偏离其预期的集体行为，即使单个时间序列本身没有表现出明显的异常模式。在许多现有方法中，时间序列变量被假设为(条件)独立的，这简化了现实世界的交互。我们的方法通过在潜在空间中建模联合依赖关系，并解耦边际分布、时间动态和变量间依赖的建模来解决这一问题。我们使用transformer编码器来捕获时间模式，为了建模空间依赖，我们拟合了多变量似然和copula。时间和空间组件在潜在空间中使用自监督对比学习目标联合训练，以学习有意义的特征表示来分离正常和异常样本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we aim to improve multivariate anomaly detection (AD) bymodeling the \textit{time-varying non-linear spatio-temporal correlations}found in multivariate time series data . In multivariate time series data, ananomaly may be indicated by the simultaneous deviation of interrelated timeseries from their expected collective behavior, even when no individual timeseries exhibits a clearly abnormal pattern on its own. In many existingapproaches, time series variables are assumed to be (conditionally)independent, which oversimplifies real-world interactions. Our approachaddresses this by modeling joint dependencies in the latent space anddecoupling the modeling of \textit{marginal distributions, temporal dynamics,and inter-variable dependencies}. We use a transformer encoder to capturetemporal patterns, and to model spatial (inter-variable) dependencies, we fit amulti-variate likelihood and a copula. The temporal and the spatial componentsare trained jointly in a latent space using a self-supervised contrastivelearning objective to learn meaningful feature representations to separatenormal and anomaly samples.</description>
      <author>example@mail.com (Padmaksha Roy, Almuatazbellah Boker, Lamine Mili)</author>
      <guid isPermaLink="false">2509.15033v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Temporally Heterogeneous Graph Contrastive Learning for Multimodal Acoustic event Classification</title>
      <link>http://arxiv.org/abs/2509.14893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于时序异构图的对比学习方法(THGCL)，用于解决多模态声学事件分类中的时间对齐和跨模态噪声问题，在AudioSet数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态声学事件分类在音频-视觉系统中起着关键作用。虽然结合音频和视觉信号可以提高识别性能，但仍然难以在时间上对齐它们并减少跨模态噪声的影响。现有方法通常分别处理音频和视觉流，稍后使用对比学习或互信息目标融合特征。最近的进展探索了多模态图学习，但大多数方法无法区分模态内和模态间的时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法无法区分模态内和模态间时间依赖性的问题，作者提出了基于时序异构图的对比学习方法(THGCL)。&lt;h4&gt;方法&lt;/h4&gt;THGCL框架为每个事件构建一个时间图，其中音频和视频片段形成节点，它们的时间链接形成边。引入了高斯过程用于模态内平滑，霍克斯过程用于模态间衰减，以及对比学习来捕获细粒度关系。&lt;h4&gt;主要发现&lt;/h4&gt;在AudioSet上的实验表明，THGCL实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过提出THGCL方法，作者成功解决了多模态声学事件分类中对齐和噪声问题，并取得了优于现有方法的性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态声学事件分类在音频-视觉系统中起着关键作用。虽然结合音频和视觉信号可以提高识别，但仍然难以在时间上对齐它们并减少跨模态噪声的影响。现有方法通常分别处理音频和视觉流，稍后使用对比或互信息目标融合特征。最近的进展探索了多模态图学习，但大多数方法无法区分模态内和模态间的时间依赖性。为了解决这个问题，我们提出了基于时序异构图的对比学习(THGCL)。我们的框架为每个事件构建一个时间图，其中音频和视频片段形成节点，它们的时间链接形成边。我们引入高斯过程用于模态内平滑，霍克斯过程用于模态间衰减，以及对比学习来捕获细粒度关系。AudioSet上的实验表明，THGCL实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal acoustic event classification plays a key role in audio-visualsystems. Although combining audio and visual signals improves recognition, itis still difficult to align them over time and to reduce the effect of noiseacross modalities. Existing methods often treat audio and visual streamsseparately, fusing features later with contrastive or mutual informationobjectives. Recent advances explore multimodal graph learning, but most fail todistinguish between intra- and inter-modal temporal dependencies. To addressthis, we propose Temporally Heterogeneous Graph-based Contrastive Learning(THGCL). Our framework constructs a temporal graph for each event, where audioand video segments form nodes and their temporal links form edges. We introduceGaussian processes for intra-modal smoothness, Hawkes processes for inter-modaldecay, and contrastive learning to capture fine-grained relationships.Experiments on AudioSet show that THGCL achieves state-of-the-art performance.</description>
      <author>example@mail.com (Yuanjian Chen, Yang Xiao, Jinjie Huang)</author>
      <guid isPermaLink="false">2509.14893v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery</title>
      <link>http://arxiv.org/abs/2509.14788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于序列的药物-靶点相互作用框架，整合结构先验到蛋白质表示中，同时保持高通量筛选能力。该模型在多个基准测试中表现优异，在虚拟筛选任务中超越先前方法，消融研究和嵌入可视化证实了模型各组件的关键作用。&lt;h4&gt;背景&lt;/h4&gt;药物-靶点相互作用的准确识别是计算药理学中的中心挑战，基于序列的方法提供了可扩展性。&lt;h4&gt;目的&lt;/h4&gt;引入一种基于序列的药物-靶点相互作用框架，将结构先验整合到蛋白质表示中，同时保持高通量筛选能力。&lt;h4&gt;方法&lt;/h4&gt;开发一种序列基础的药物-靶点相互作用框架，整合结构先验到蛋白质表示中。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，该模型在Human和BioSNAP数据集上实现了最先进的性能，在BindingDB上保持竞争力；在虚拟筛选任务中，它在LIT-PCBA上超越了先前的方法，在AUROC和BEDROC方面获得了显著提升；消融研究证实了学习聚合、双线注意力和对比对齐在增强预测鲁棒性方面的关键作用；嵌入可视化显示出与已知结合口袋的改进空间对应，并突出了配体-残基接触的可解释注意模式。&lt;h4&gt;结论&lt;/h4&gt;这些结果验证了该框架在可扩展和结构感知的DTI预测中的实用性。&lt;h4&gt;翻译&lt;/h4&gt;准确的药物-靶点相互作用识别仍然是计算药理学中的中心挑战，其中基于序列的方法提供了可扩展性。这项工作引入了一种基于序列的药物-靶点相互作用框架，将结构先验整合到蛋白质表示中，同时保持高通量筛选能力。在多个基准测试中评估，该模型在Human和BioSNAP数据集上实现了最先进的性能，并在BindingDB上保持竞争力。在虚拟筛选任务中，它在LIT-PCBA上超越了先前的方法，在AUROC和BEDROC方面获得了显著提升。消融研究证实了学习聚合、双线注意力和对比对齐在增强预测鲁棒性方面的关键作用。嵌入可视化显示出与已知结合口袋的改进空间对应，并突出了配体-残基接触的可解释注意模式。这些结果验证了该框架在可扩展和结构感知的DTI预测中的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate identification of drug-target interactions (DTI) remains a centralchallenge in computational pharmacology, where sequence-based methods offerscalability. This work introduces a sequence-based drug-target interactionframework that integrates structural priors into protein representations whilemaintaining high-throughput screening capability. Evaluated across multiplebenchmarks, the model achieves state-of-the-art performance on Human andBioSNAP datasets and remains competitive on BindingDB. In virtual screeningtasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains inAUROC and BEDROC. Ablation studies confirm the critical role of learnedaggregation, bilinear attention, and contrastive alignment in enhancingpredictive robustness. Embedding visualizations reveal improved spatialcorrespondence with known binding pockets and highlight interpretable attentionpatterns over ligand-residue contacts. These results validate the framework'sutility for scalable and structure-aware DTI prediction.</description>
      <author>example@mail.com (Jing Lan, Hexiao Ding, Hongzhao Chen, Yufeng Jiang, Nga-Chun Ng, Gwing Kei Yip, Gerald W. Y. Cheng, Yunlin Mao, Jing Cai, Liang-ting Lin, Jung Sun Yoo)</author>
      <guid isPermaLink="false">2509.14788v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-CLAP: Learning Spatially-Aware audio--text Embeddings for Multi-Source Conditions</title>
      <link>http://arxiv.org/abs/2509.14785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Spatial-CLAP，一种能够捕获空间信息的音频-文本嵌入框架，通过内容感知的空间编码器和空间对比学习策略解决了多源条件下的空间信息建模挑战。&lt;h4&gt;背景&lt;/h4&gt;CLAP作为音频-文本嵌入框架已取得显著成功，但现有方法仅限于单声道或单源条件，无法完全捕获空间信息。建模空间信息的主要挑战在于多源条件，需要正确对应每个声源及其位置。&lt;h4&gt;目的&lt;/h4&gt;解决多源条件下的空间信息建模问题，开发能够捕获空间信息的音频-文本嵌入框架。&lt;h4&gt;方法&lt;/h4&gt;提出Spatial-CLAP，引入内容感知的空间编码器使空间表示与音频内容耦合；提出空间对比学习(SCL)训练策略，强制学习正确的对应关系，促进多源条件下更可靠的嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明Spatial-CLAP在多源条件下能有效学习嵌入；SCL策略被证实有效；在未见三源混合物上的评估突显了传统单源训练与多源训练范式的根本区别。&lt;h4&gt;结论&lt;/h4&gt;这些发现为空间感知的音频-文本嵌入建立了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-音频预训练(CLAP)作为音频-文本嵌入框架已取得显著成功，但现有方法仅限于单声道或单源条件，无法完全捕获空间信息。建模空间信息的主要挑战在于多源条件，需要正确对应每个声源及其位置。为解决这个问题，我们提出了Spatial-CLAP，引入了内容感知的空间编码器，使空间表示与音频内容耦合。我们进一步提出了空间对比学习(SCL)，一种训练策略，明确强制学习正确的对应关系，促进多源条件下更可靠的嵌入。包括下游任务在内的实验评估表明，Spatial-CLAP即使在多源条件下也能学习有效的嵌入，并确认了SCL的有效性。此外，在未见三源混合物上的评估突显了传统单源训练与所提出的多源训练范式之间的根本区别。这些发现为空间感知的音频-文本嵌入建立了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive language--audio pretraining (CLAP) has achieved remarkablesuccess as an audio--text embedding framework, but existing approaches arelimited to monaural or single-source conditions and cannot fully capturespatial information. The central challenge in modeling spatial information liesin multi-source conditions, where the correct correspondence between each soundsource and its location is required. To tackle this problem, we proposeSpatial-CLAP, which introduces a content-aware spatial encoder that enablesspatial representations coupled with audio content. We further propose spatialcontrastive learning (SCL), a training strategy that explicitly enforces thelearning of the correct correspondence and promotes more reliable embeddingsunder multi-source conditions. Experimental evaluations, including downstreamtasks, demonstrate that Spatial-CLAP learns effective embeddings even undermulti-source conditions, and confirm the effectiveness of SCL. Moreover,evaluation on unseen three-source mixtures highlights the fundamentaldistinction between conventional single-source training and our proposedmulti-source training paradigm. These findings establish a new paradigm forspatially-aware audio--text embeddings.</description>
      <author>example@mail.com (Kentaro Seki, Yuki Okamoto, Kouei Yamaoka, Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari)</author>
      <guid isPermaLink="false">2509.14785v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration</title>
      <link>http://arxiv.org/abs/2509.14084v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出AD-DINOv3，一种新型视觉-语言多模态框架，首次将DINOv3模型适应于零样本异常检测(ZSAD)任务，通过多模态对比学习和异常感知校准模块解决了领域偏差和全局语义偏差问题，在八个工业和医学基准测试上取得优异性能。&lt;h4&gt;背景&lt;/h4&gt;零样本异常检测(ZSAD)旨在从任意新类别中识别异常，提供可扩展且注释高效的解决方案。传统ZSAD工作多基于CLIP模型，通过计算视觉和文本嵌入相似性进行异常检测。DINOv3等视觉基础模型最近展示了强大的可迁移表示能力。&lt;h4&gt;目的&lt;/h4&gt;首次将DINOv3适应于ZSAD任务，解决两个关键挑战：(1)大规模预训练数据与异常检测任务间领域偏差导致的特征不对齐；(2)预表示中对全局语义的固有倾向导致细微异常被误认为正常前景物体。&lt;h4&gt;方法&lt;/h4&gt;提出AD-DINOv3框架，将异常检测表述为多模态对比学习问题，使用DINOv3作为视觉骨干网络提取补丁令牌和CLS令牌，CLIP文本编码器提供正常和异常提示嵌入。引入轻量级适配器弥合领域差距，设计异常感知校准模块(AACM)引导CLS令牌关注异常区域而非通用前景语义。&lt;h4&gt;主要发现&lt;/h4&gt;在八个工业和医学基准测试上的广泛实验表明，AD-DINOv3一致地匹配或超越了最先进方法。代码将在https://github.com/Kaisor-Yuan/AD-DINOv3上公开。&lt;h4&gt;结论&lt;/h4&gt;AD-DINOv3结合了DINOv3的强大表示能力和CLIP的文本理解能力，解决了ZSAD任务中的关键挑战，通过多模态对比学习和异常感知校准模块的设计，提供了一种高效且可扩展的异常检测解决方案。&lt;h4&gt;翻译&lt;/h4&gt;零样本异常检测(ZSAD)旨在从任意新类别中识别异常，提供了一种可扩展且注释高效的解决方案。传统上，大多数ZSAD工作基于CLIP模型，通过计算视觉和文本嵌入之间的相似性来执行异常检测。最近，DINOv3等视觉基础模型展示了强大的可迁移表示能力。在这项工作中，我们首次将DINOv3适应于ZSAD。然而，这种适应提出了两个关键挑战：(i)大规模预训练数据与异常检测任务之间的领域偏差导致特征不对齐；(ii)预表示中对全局语义的固有倾向常常导致细微异常被误认为是正常前景物体的一部分，而非被识别为异常区域。为了克服这些挑战，我们引入了AD-DINOv3，一种专为ZSAD设计的新型视觉-语言多模态框架。具体而言，我们将异常检测表述为多模态对比学习问题，其中DINOv3被用作视觉骨干网络来提取补丁令牌和CLS令牌，CLIP文本编码器为正常和异常提示提供嵌入。为了弥合领域差距，在两种模态中都引入了轻量级适配器，使其表示能够针对异常检测任务进行重新校准。除了这种基线对齐外，我们还设计了一个异常感知校准模块(AACM)，它明确引导CLS令牌关注异常区域而非通用前景语义，从而增强判别能力。在八个工业和医学基准测试上的广泛实验表明，AD-DINOv3一致地匹配或超越了最先进的方法。代码将在https://github.com/Kaisor-Yuan/AD-DINOv3上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrarynovel categories, offering a scalable and annotation-efficient solution.Traditionally, most ZSAD works have been based on the CLIP model, whichperforms anomaly detection by calculating the similarity between visual andtext embeddings. Recently, vision foundation models such as DINOv3 havedemonstrated strong transferable representation capabilities. In this work, weare the first to adapt DINOv3 for ZSAD. However, this adaptation presents twokey challenges: (i) the domain bias between large-scale pretraining data andanomaly detection tasks leads to feature misalignment; and (ii) the inherentbias toward global semantics in pretrained representations often leads tosubtle anomalies being misinterpreted as part of the normal foreground objects,rather than being distinguished as abnormal regions. To overcome thesechallenges, we introduce AD-DINOv3, a novel vision-language multimodalframework designed for ZSAD. Specifically, we formulate anomaly detection as amultimodal contrastive learning problem, where DINOv3 is employed as the visualbackbone to extract patch tokens and a CLS token, and the CLIP text encoderprovides embeddings for both normal and abnormal prompts. To bridge the domaingap, lightweight adapters are introduced in both modalities, enabling theirrepresentations to be recalibrated for the anomaly detection task. Beyond thisbaseline alignment, we further design an Anomaly-Aware Calibration Module(AACM), which explicitly guides the CLS token to attend to anomalous regionsrather than generic foreground semantics, thereby enhancing discriminability.Extensive experiments on eight industrial and medical benchmarks demonstratethat AD-DINOv3 consistently matches or surpasses state-of-the-art methods.Thecode will be available at https://github.com/Kaisor-Yuan/AD-DINOv3.</description>
      <author>example@mail.com (Jingyi Yuan, Jianxiong Ye, Wenkang Chen, Chenqiang Gao)</author>
      <guid isPermaLink="false">2509.14084v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection</title>
      <link>http://arxiv.org/abs/2509.13853v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为单阶段监督对比学习(OS-SCL)的新训练技术，用于解决无监督异常声音检测中的频繁误报问题，并提出了TFgram时间频率特征，显著提升了检测性能。&lt;h4&gt;背景&lt;/h4&gt;尽管自监督方法有所进展，但在处理来自不同机器的同类样本时，无监督异常声音检测仍然存在频繁误报的问题。&lt;h4&gt;目的&lt;/h4&gt;解决无监督异常声音检测中处理来自不同机器的同类样本时的频繁误报问题。&lt;h4&gt;方法&lt;/h4&gt;提出单阶段监督对比学习(OS-SCL)训练技术，通过扰动嵌入空间特征并采用单阶段噪声监督对比学习方法；同时提出TFgram时间频率特征，从原始音频中提取关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64% AUC、88.42% pAUC和89.24% mAUC；使用提出的TFgram特征进一步提升了性能，达到95.71% AUC、90.23% pAUC和91.23% mAUC。&lt;h4&gt;结论&lt;/h4&gt;OS-SCL方法和TFgram特征有效解决了无监督异常声音检测中的频繁误报问题，显著提升了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常声音检测旨在仅使用正常音频数据训练模型来检测未知异常声音。尽管自监督方法有所进展，但在处理来自不同机器的同类样本时频繁误报的问题仍未解决。本文引入了一种名为单阶段监督对比学习(OS-SCL)的新训练技术，通过扰动嵌入空间特征并采用单阶段噪声监督对比学习方法，显著解决了这一问题。在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64% AUC、88.42% pAUC和89.24% mAUC。此外，还提出了一种从原始音频中提取的时间频率特征TFgram，该特征有效捕获了异常声音检测的关键信息，最终实现了95.71% AUC、90.23% pAUC和91.23% mAUC。源代码可在www.github.com/huangswt/OS-SCL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomalous sound detection aims to detect unknown anomaloussounds by training a model using only normal audio data. Despite advancementsin self-supervised methods, the issue of frequent false alarms when handlingsamples of the same type from different machines remains unresolved. This paperintroduces a novel training technique called one-stage supervised contrastivelearning (OS-SCL), which significantly addresses this problem by perturbingfeatures in the embedding space and employing a one-stage noisy supervisedcontrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.Additionally, a time-frequency feature named TFgram is proposed, which isextracted from raw audio. This feature effectively captures criticalinformation for anomalous sound detection, ultimately achieving 95.71\% AUC,90.23\% pAUC, and 91.23\% mAUC. The source code is available at:\underline{www.github.com/huangswt/OS-SCL}.</description>
      <author>example@mail.com (Shun Huang, Zhihua Fang, Liang He)</author>
      <guid isPermaLink="false">2509.13853v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>AEGIS: Automated Error Generation and Identification for Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2509.14295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AEGIS框架，用于多智能体系统的自动化错误生成和识别，解决了该领域缺乏大规模多样化错误数据集的问题。通过系统注入可控错误和探索三种学习范式，证明了该框架能显著提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;随着多智能体系统变得越来越自主和复杂，理解它们的错误模式对确保可靠性和安全性至关重要。然而，该领域研究严重缺乏大规模、多样化的数据集，这些数据集需要有精确的真实错误标签。&lt;h4&gt;目的&lt;/h4&gt;解决多智能体系统错误研究中缺乏大规模、多样化错误数据集的瓶颈问题，引入AEGIS框架用于自动化错误生成和识别。&lt;h4&gt;方法&lt;/h4&gt;通过系统性地向初始成功的轨迹中注入可控和可追踪的错误来创建丰富的现实失败数据集；使用基于上下文感知的大语言模型的自适应操作器执行复杂攻击（如提示注入和响应损坏）来诱导特定错误模式；探索三种学习范式（监督微调、强化学习和对比学习）进行错误识别。&lt;h4&gt;主要发现&lt;/h4&gt;在所有三种学习范式中，使用AEGIS数据训练的模型都取得了显著改进；多个微调模型表现出与或优于数量大一个数量级的专有系统的性能；验证了自动化数据生成框架对于开发更强大和可解释的多智能体系统是关键资源。&lt;h4&gt;结论&lt;/h4&gt;AEGIS框架为多智能体系统错误研究提供了宝贵的资源，通过自动化生成的错误数据，可以显著提高模型性能，使其能够与专有系统竞争。&lt;h4&gt;翻译&lt;/h4&gt;随着多智能体系统变得越来越自主和复杂，理解它们的错误模式对于确保其可靠性和安全性至关重要。然而，该领域的研究一直严重缺乏具有精确、真实错误标签的大规模、多样化数据集。为了解决这一瓶颈，我们引入了AEGIS，这是一个用于多智能体系统自动化错误生成和识别的新颖框架。通过系统性地向初始成功的轨迹中注入可控和可追踪的错误，我们创建了一个丰富的现实失败数据集。这是通过使用一个上下文感知的、基于大语言模型的自适应操作器实现的，该操作器执行复杂攻击，如提示注入和响应损坏，以诱导特定的预定义错误模式。我们通过探索三种不同的学习范式来证明我们数据集的价值：监督微调、强化学习和对比学习。我们全面的实验表明，在所有三种学习范式中，使用AEGIS数据训练的模型都取得了显著改进。值得注意的是，我们多个微调模型的性能与或优于数量级大一个数量级的专有系统，验证了我们的自动化数据生成框架是开发更强大和可解释的多智能体系统的关键资源。我们的项目网站可在https://kfq20.github.io/AEGIS-Website获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Multi-Agent Systems (MAS) become increasingly autonomous and complex,understanding their error modes is critical for ensuring their reliability andsafety. However, research in this area has been severely hampered by the lackof large-scale, diverse datasets with precise, ground-truth error labels. Toaddress this bottleneck, we introduce \textbf{AEGIS}, a novel framework for\textbf{A}utomated \textbf{E}rror \textbf{G}eneration and\textbf{I}dentification for Multi-Agent \textbf{S}ystems. By systematicallyinjecting controllable and traceable errors into initially successfultrajectories, we create a rich dataset of realistic failures. This is achievedusing a context-aware, LLM-based adaptive manipulator that performssophisticated attacks like prompt injection and response corruption to inducespecific, predefined error modes. We demonstrate the value of our dataset byexploring three distinct learning paradigms for the error identification task:Supervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Ourcomprehensive experiments show that models trained on AEGIS data achievesubstantial improvements across all three learning paradigms. Notably, severalof our fine-tuned models demonstrate performance competitive with or superiorto proprietary systems an order of magnitude larger, validating our automateddata generation framework as a crucial resource for developing more robust andinterpretable multi-agent systems. Our project website is available athttps://kfq20.github.io/AEGIS-Website.</description>
      <author>example@mail.com (Fanqi Kong, Ruijie Zhang, Huaxiao Yin, Guibin Zhang, Xiaofei Zhang, Ziang Chen, Zhaowei Zhang, Xiaoyuan Zhang, Song-Chun Zhu, Xue Feng)</author>
      <guid isPermaLink="false">2509.14295v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models</title>
      <link>http://arxiv.org/abs/2509.14269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SparseDoctor的新型稀疏医学大型语言模型，采用对比学习增强的LoRA-MoE架构，有效解决了传统微调策略的高成本问题，在医学基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在医疗问答和临床决策中取得成功，促进了个性化虚拟医生的普及，但传统微调策略需要更新数十亿参数，显著增加了训练成本。&lt;h4&gt;目的&lt;/h4&gt;提高当前医学大型语言模型的效率和有效性，探索大型语言模型在医学领域的表示能力边界。&lt;h4&gt;方法&lt;/h4&gt;提出SparseDoctor模型，采用对比学习增强的LoRA-MoE架构，包括自动路由机制科学分配计算资源，以及专家记忆队列机制提高框架效率并防止内存溢出。&lt;h4&gt;主要发现&lt;/h4&gt;在CMB、CMExam和CMMLU-Med三个医学基准测试上，所提出的模型持续优于HuatuoGPT系列等强大基线模型。&lt;h4&gt;结论&lt;/h4&gt;SparseDoctor模型证明了其在医学领域的有效性和优越性，为高效医学大型语言模型的发展提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在医疗问答和临床决策中取得了巨大成功，促进了个性化虚拟医生在社会中的普及和效率提升。然而，在大型语言模型上的传统微调策略需要更新数十亿参数，显著增加了训练成本，包括训练时间和实用成本。为了提高当前医学大型语言模型的效率和有效性，并探索大型语言模型在医学领域的表示能力边界，除了从数据角度的传统微调策略（即监督微调或人类反馈强化学习）外，我们设计了一个名为SparseDoctor的新型稀疏医学大型语言模型，配备对比学习增强的LoRA-MoE（低秩适配-专家混合）架构。为此，设计的自动路由机制可以在对比学习的监督下科学分配不同LoRA专家之间的计算资源。此外，我们还引入了一种新的专家记忆队列机制，进一步提高整体框架的效率并防止训练过程中的内存溢出。我们在三个典型的医学基准测试（CMB、CMExam和CMMLU-Med）上进行了综合评估。实验结果表明，所提出的大型语言模型能够持续优于HuatuoGPT系列等强大基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have achieved great success in medical questionanswering and clinical decision-making, promoting the efficiency andpopularization of the personalized virtual doctor in society. However, thetraditional fine-tuning strategies on LLM require the updates of billions ofparameters, substantially increasing the training cost, including the trainingtime and utility cost. To enhance the efficiency and effectiveness of thecurrent medical LLMs and explore the boundary of the representation capabilityof the LLMs on the medical domain, apart from the traditional fine-tuningstrategies from the data perspective (i.e., supervised fine-tuning orreinforcement learning from human feedback), we instead craft a novel sparsemedical LLM named SparseDoctor armed with contrastive learning enhancedLoRA-MoE (low rank adaptation-mixture of experts) architecture. To this end,the crafted automatic routing mechanism can scientifically allocate thecomputational resources among different LoRA experts supervised by thecontrastive learning. Additionally, we also introduce a novel expert memoryqueue mechanism to further boost the efficiency of the overall framework andprevent the memory overflow during training. We conduct comprehensiveevaluations on three typical medical benchmarks: CMB, CMExam, and CMMLU-Med.Experimental results demonstrate that the proposed LLM can consistentlyoutperform the strong baselines such as the HuatuoGPT series.</description>
      <author>example@mail.com (Zhang Jianbin, Yulin Zhu, Wai Lun Lo, Richard Tai-Chiu Hsung, Harris Sik-Ho Tsang, Kai Zhou)</author>
      <guid isPermaLink="false">2509.14269v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation</title>
      <link>http://arxiv.org/abs/2509.15224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Code: https://github.com/bartn8/depthanyevent/ Project  Page: https://bartn8.github.io/depthanyevent/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种跨模态蒸馏范式，利用视觉基础模型生成密集代理标签，解决了事件相机深度估计中缺乏大规模标注数据集的问题，并提出了适应VFMs的方法，在合成和真实世界数据集上取得了与监督方法相当的性能和最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;事件相机捕捉稀疏、高时间分辨率的视觉信息，特别适合高速运动和光照条件变化大的挑战性环境。然而，缺乏具有密集真实深度标注的大数据集，阻碍了基于学习的单目深度估计从事件数据中学习。&lt;h4&gt;目的&lt;/h4&gt;解决事件相机深度估计中缺乏大规模密集真实深度标注数据集的问题，提出一种无需昂贵深度标注的深度估计方法。&lt;h4&gt;方法&lt;/h4&gt;提出跨模态蒸馏范式，利用视觉基础模型(VFM)生成密集代理标签；需要与RGB帧空间对齐的事件流；利用大规模VFMs的鲁棒性；提出适应VFMs的方法，包括使用原始模型如Depth Anything v2或衍生新的循环架构来从单目事件相机推断深度。&lt;h4&gt;主要发现&lt;/h4&gt;跨模态范式与完全监督方法相比具有竞争性性能，不需要昂贵的深度标注；基于VFM的模型达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的跨模态蒸馏范式和VFM适应方法有效解决了事件相机深度估计中的数据标注问题，在合成和真实世界数据集上取得了优异的性能。&lt;h4&gt;翻译&lt;/h4&gt;事件相机捕捉稀疏、高时间分辨率的视觉信息，使其特别适合高速运动和光照条件变化剧烈的挑战性环境。然而，缺乏具有密集真实深度标注的大数据集阻碍了基于学习的从事件数据中进行单目深度估计。为解决这一限制，我们提出了一种跨模态蒸馏范式，利用视觉基础模型(VFM)生成密集代理标签。我们的策略需要与RGB帧空间对齐的事件流，这种简单设置甚至可以现成获取，并利用大规模VFMs的鲁棒性。此外，我们提出适应VFMs，可以是原始的如Depth Anything v2 (DAv2)，或者从中衍生出一种新的循环架构来从单目事件相机推断深度。我们在合成和真实世界数据集上评估了我们的方法，证明i)我们的跨模态范式与不需要昂贵深度标注的完全监督方法相比具有竞争性性能，以及ii)我们的基于VFM的模型达到了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决事件相机(event cameras)的单目深度估计问题。问题的重要性在于：事件相机具有高时间分辨率和强光照鲁棒性，特别适合高速运动和光照变化场景，但缺乏大规模密集深度标注数据集，限制了基于学习的方法发展。深度感知对自主导航、机器人等应用至关重要，而单目设置相比多目系统在成本、校准复杂度上有优势，但事件相机信息量少且标注困难，使深度估计极具挑战性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到事件相机深度估计的主要瓶颈是缺乏标注数据，然后借鉴视觉基础模型(VFMs)在图像深度估计中的成功经验。他们思考如何将图像领域的知识转移到事件领域，设计了跨模态蒸馏策略。借鉴了现有工作包括：Depth Anything v2等VFMs的架构、Tencode事件表示方法、循环神经网络处理时序数据的思想、以及知识蒸馏技术。在此基础上，设计了两种适应策略：直接使用VFMs和基于VFMs构建新的循环架构DepthAnyEvent-R。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型的知识弥补事件相机深度估计中标注数据的不足，通过跨模态蒸馏将图像领域的深度估计知识转移到事件领域。整体流程为：1) 收集空间对齐的事件流和RGB帧；2) 使用预训练VFM处理RGB帧生成深度代理标签；3) 用Tencode方法将事件流转换为RGB格式表示；4) 训练事件网络(学生模型)模仿VFM(教师模型)的预测；5) 设计两种模型架构：直接适应的DepthAnyEvent和整合时序信息的DepthAnyEvent-R；6) 在合成和真实数据集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出跨模态蒸馏范式，利用VFMs生成密集代理标签；2) 提出适应VFMs到事件域的策略；3) 设计循环架构DepthAnyEvent-R捕捉事件流时序特性；4) 证明蒸馏方法可媲美完全监督方法而无需昂贵标注。相比之前工作的不同：传统方法依赖大量标注数据，而本文通过蒸馏减少标注需求；简单图像模型直接应用于事件数据效果有限，本文设计了更适合事件的表示和架构；现有事件方法未充分利用大规模预训练知识，本文结合了图像和事件模态优势。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了跨模态蒸馏范式和适应视觉基础模型到事件域的方法，解决了事件相机深度估计中标注数据不足的问题，实现了与完全监督方法相媲美的性能并达到了新的技术水平。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras capture sparse, high-temporal-resolution visual information,making them particularly suitable for challenging environments with high-speedmotion and strongly varying lighting conditions. However, the lack of largedatasets with dense ground-truth depth annotations hinders learning-basedmonocular depth estimation from event data. To address this limitation, wepropose a cross-modal distillation paradigm to generate dense proxy labelsleveraging a Vision Foundation Model (VFM). Our strategy requires an eventstream spatially aligned with RGB frames, a simple setup even availableoff-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),or deriving from it a novel recurrent architecture to infer depth frommonocular event cameras. We evaluate our approach with synthetic and real-worlddatasets, demonstrating that i) our cross-modal paradigm achieves competitiveperformance compared to fully supervised methods without requiring expensivedepth annotations, and ii) our VFM-based models achieve state-of-the-artperformance.</description>
      <author>example@mail.com (Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia)</author>
      <guid isPermaLink="false">2509.15224v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</title>
      <link>http://arxiv.org/abs/2509.15221v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ScaleCUA是一个大规模开源计算机使用代理系统，通过整合6个操作系统和3个任务领域的数据集，实现了跨平台无缝操作，性能显著超越现有基线，并创造了多项新纪录。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language Models(VLMs)已使计算机使用代理(CUAs)能够自主操作GUI，显示出巨大潜力，但进展受限于缺乏大规模、开源的计算机使用数据和基础模型。&lt;h4&gt;目的&lt;/h4&gt;引入ScaleCUA，朝着扩展开源CUAs迈出一步，解决当前领域内数据和模型不足的问题。&lt;h4&gt;方法&lt;/h4&gt;构建了一个大规模数据集，涵盖6个操作系统和3个任务领域；通过闭环流水线构建，该流水线将自动代理与人类专家结合；在扩展的数据上训练ScaleCUA，使其能够跨平台无缝运行。&lt;h4&gt;主要发现&lt;/h4&gt;与基线相比，ScaleCUA在WebArena-Lite-v2上提升+26.6，在ScreenSpot-Pro上提升+10.7；在MMBench-GUI L1-Hard上达到94.4%，在OSWorld-G上达到60.6%，在WebArena-Lite-v2上达到47.4%的最先进结果；数据驱动扩展对通用计算机使用代理有显著提升作用。&lt;h4&gt;结论&lt;/h4&gt;ScaleCUA证明了数据驱动扩展的有效性，将发布数据、模型和代码以推进未来研究，项目地址为https://github.com/OpenGVLab/ScaleCUA。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)已经使计算机使用代理(CUAs)能够自主操作图形用户界面，显示出巨大潜力，但进展受限于缺乏大规模、开源的计算机使用数据和基础模型。在这项工作中，我们介绍了ScaleCUA，这是朝着扩展开源CUAs迈出的一步。它提供了一个大规模数据集，涵盖6个操作系统和3个任务领域，通过将自动代理与人类专家结合的闭环流水线构建。在此扩展数据上训练的ScaleCUA能够跨平台无缝运行。具体而言，它比基线模型带来显著提升(+26.6在WebArena-Lite-v2上，+10.7在ScreenSpot-Pro上)，并设定了新的最先进结果(94.4%在MMBench-GUI L1-Hard上，60.6%在OSWorld-G上，47.4%在WebArena-Lite-v2上)。这些发现强调了数据驱动扩展对通用计算机使用代理的力量。我们将发布数据、模型和代码以推进未来研究：https://github.com/OpenGVLab/ScaleCUA。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have enabled computer use agents (CUAs) thatoperate GUIs autonomously, showing great potential, yet progress is limited bythe lack of large-scale, open-source computer use data and foundation models.In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. Itoffers a large-scale dataset spanning 6 operating systems and 3 task domains,built via a closed-loop pipeline uniting automated agents with human experts.Trained on this scaled-up data, ScaleCUA can operate seamlessly acrossplatforms. Specifically, it delivers strong gains over baselines (+26.6 onWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-artresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% onWebArena-Lite-v2). These findings underscore the power of data-driven scalingfor general-purpose computer use agents. We will release data, models, and codeto advance future research: https://github.com/OpenGVLab/ScaleCUA.</description>
      <author>example@mail.com (Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang)</author>
      <guid isPermaLink="false">2509.15221v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Self-Improving Embodied Foundation Models</title>
      <link>http://arxiv.org/abs/2509.15155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Appearing in the Conference on Neural Information Processing Systems  (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于机器人的两阶段后训练方法，结合监督微调和自我改进，使机器人能够在最少的人工监督下自主练习下游任务，并展现出比传统方法更高的样本效率和成功率。&lt;h4&gt;背景&lt;/h4&gt;基于网络规模数据训练的基础模型已经彻底改变了机器人技术，但它们在低级控制中的应用仍然主要局限于行为克隆。&lt;h4&gt;目的&lt;/h4&gt;受大型语言模型微调中强化学习阶段成功的启发，提出一种用于机器人的两阶段后训练方法，以提高基础模型在机器人控制中的性能。&lt;h4&gt;方法&lt;/h4&gt;第一阶段：监督微调(SFT)，使用行为克隆和步数预测目标对预训练的基础模型进行微调；第二阶段：自我改进，利用步数预测提取奖励函数和成功检测器，使机器人能够在最少的人工监督下自主练习下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;SFT和自我改进的结合比扩展模仿数据收集更高效，能带来成功率更高的策略；网络规模预训练和自我改进的结合是实现样本效率的关键；该方法能实现自主练习和获取泛化能力远超训练数据的新技能。&lt;h4&gt;结论&lt;/h4&gt;结合预训练基础模型和在线自我改进具有变革性潜力，可以促进机器人中的自主技能获取。&lt;h4&gt;翻译&lt;/h4&gt;基于网络规模数据训练的基础模型已经彻底改变了机器人技术，但它们在低级控制中的应用仍然主要局限于行为克隆。受大型语言模型微调中强化学习阶段成功的启发，我们提出了一种用于机器人的两阶段后训练方法。第一阶段，监督微调(SFT)，使用行为克隆和步数预测目标对预训练的基础模型进行微调。在第二阶段，自我改进，步数预测能够提取出良好的奖励函数和强大的成功检测器，使机器人能够在最少的人工监督下自主练习下游任务。通过对现实世界和模拟机器人 embodiment 的广泛实验，我们的新型后训练方案在具身基础模型上揭示了显著的结果。首先，我们证明 SFT 和自我改进的结合比扩展监督学习的模仿数据收集更高效，并且能带来成功率更高的策略。进一步的消融研究强调，网络规模预训练和自我改进的结合是实现这种样本效率的关键。接下来，我们证明提出的组合方法独特地解锁了当前方法无法实现的能力：自主练习和获取泛化能力远超训练时使用的模仿学习数据集中观察到的行为的新技能。这些发现突显了结合预训练基础模型和在线自我改进以实现机器人中自主技能获取的变革潜力。我们的项目网站可以在 https://self-improving-efms.github.io 找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models trained on web-scale data have revolutionized robotics, buttheir application to low-level control remains largely limited to behavioralcloning. Drawing inspiration from the success of the reinforcement learningstage in fine-tuning large language models, we propose a two-stagepost-training approach for robotics. The first stage, Supervised Fine-Tuning(SFT), fine-tunes pretrained foundation models using both: a) behavioralcloning, and b) steps-to-go prediction objectives. In the second stage,Self-Improvement, steps-to-go prediction enables the extraction of awell-shaped reward function and a robust success detector, enabling a fleet ofrobots to autonomously practice downstream tasks with minimal humansupervision. Through extensive experiments on real-world and simulated robotembodiments, our novel post-training recipe unveils significant results onEmbodied Foundation Models. First, we demonstrate that the combination of SFTand Self-Improvement is significantly more sample-efficient than scalingimitation data collection for supervised learning, and that it leads topolicies with significantly higher success rates. Further ablations highlightthat the combination of web-scale pretraining and Self-Improvement is the keyto this sample-efficiency. Next, we demonstrate that our proposed combinationuniquely unlocks a capability that current methods cannot achieve: autonomouslypracticing and acquiring novel skills that generalize far beyond the behaviorsobserved in the imitation learning datasets used during training. Thesefindings highlight the transformative potential of combining pretrainedfoundation models with online Self-Improvement to enable autonomous skillacquisition in robotics. Our project website can be found athttps://self-improving-efms.github.io .</description>
      <author>example@mail.com (Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, Igor Mordatch)</author>
      <guid isPermaLink="false">2509.15155v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Asymptotic Study of In-context Learning with Random Transformers through Equivalent Models</title>
      <link>http://arxiv.org/abs/2509.15152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MLSP 2025, 6 pages 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究预训练Transformer在非线性回归中的上下文学习能力，发现随机Transformer在特定条件下等价于有限阶Hermite多项式模型。&lt;h4&gt;背景&lt;/h4&gt;探索Transformer模型在非线性回归任务中的上下文学习能力，特别是在渐近情况下各参数共同增长时的表现。&lt;h4&gt;目的&lt;/h4&gt;理解MLP层如何增强上下文学习能力，以及非线性和过参数化如何影响模型性能。&lt;h4&gt;方法&lt;/h4&gt;研究具有非线性MLP头部的随机Transformer，固定第一层并训练第二层，在上下文长度、输入维度、隐藏维度等参数共同增长的渐近情况下进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;随机Transformer在ICL误差方面等价于有限阶Hermite多项式模型，这一发现通过不同激活函数、上下文长度、隐藏层宽度和正则化设置的模拟得到验证，并揭示了双下降现象。&lt;h4&gt;结论&lt;/h4&gt;MLP层能够增强上下文学习能力，而非线性和过参数化对模型性能有重要影响。&lt;h4&gt;翻译&lt;/h4&gt;我们研究预训练Transformer在非线性回归设置中的上下文学习能力。具体而言，我们关注具有非线性MLP头部的随机Transformer，其中第一层随机初始化并固定，而第二层进行训练。此外，我们考虑一个渐近情况，其中上下文长度、输入维度、隐藏维度、训练任务数量和训练样本数量共同增长。在这种情况下，我们证明随机Transformer在ICL误差方面等价于有限阶Hermite多项式模型。这种等价性通过在不同激活函数、上下文长度、隐藏层宽度(揭示了双下降现象)和正则化设置下的模拟得到验证。我们的结果提供了理论和实证见解，说明MLP层何时以及如何增强ICL，以及非线性和过参数化如何影响模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the in-context learning (ICL) capabilities of pretrainedTransformers in the setting of nonlinear regression. Specifically, we focus ona random Transformer with a nonlinear MLP head where the first layer israndomly initialized and fixed while the second layer is trained. Furthermore,we consider an asymptotic regime where the context length, input dimension,hidden dimension, number of training tasks, and number of training samplesjointly grow. In this setting, we show that the random Transformer behavesequivalent to a finite-degree Hermite polynomial model in terms of ICL error.This equivalence is validated through simulations across varying activationfunctions, context lengths, hidden layer widths (revealing a double-descentphenomenon), and regularization settings. Our results offer theoretical andempirical insights into when and how MLP layers enhance ICL, and hownonlinearity and over-parameterization influence model performance.</description>
      <author>example@mail.com (Samet Demir, Zafer Dogan)</author>
      <guid isPermaLink="false">2509.15152v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Exploring How Audio Effects Alter Emotion with Foundation Models</title>
      <link>http://arxiv.org/abs/2509.15151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了音频效果如何影响音乐聆听时的情感反应，并利用基础模型分析这些效果与情感之间的关系。&lt;h4&gt;背景&lt;/h4&gt;先前研究已考察低级音频特征与情感感知的联系，但音频效果对情感的系统性影响尚未得到充分探索。音频效果在塑造音乐聆听时的情感反应中起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;研究如何利用基础模型分析音频效果对情感的影响，揭示音频效果与估计情感之间复杂、非线性的关系，并评估基础音频模型的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过应用各种探测方法分析深度学习模型的嵌入表示，研究不同音频效果与情感估计之间的关系。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了特定音频效果与情感之间的模式关系，并评估了基础音频模型的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这项研究旨在增进对音频制作实践感知影响的理解，对音乐认知、表演和情感计算领域具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;混响、失真、调制和动态范围处理等音频效果在塑造音乐聆听时的情感反应中起着关键作用。虽然先前的研究已经考察了低级音频特征与情感感知之间的联系，但音频效果对情感的系统性影响尚未得到充分探索。这项研究探讨了如何利用基础模型（在多模态数据上预训练的大型神经网络架构）来分析这些效果。这类模型编码了音乐结构、音色与情感意义之间的丰富关联，为探测声音设计技术的情感后果提供了强大的框架。通过应用各种探测方法来分析深度学习模型的嵌入表示，我们考察了音频效果与估计情感之间复杂、非线性的关系，揭示了与特定效果相关的模式，并评估了基础音频模型的鲁棒性。我们的研究旨在增进对音频制作实践感知影响的理解，对音乐认知、表演和情感计算领域具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio effects (FX) such as reverberation, distortion, modulation, and dynamicrange processing play a pivotal role in shaping emotional responses duringmusic listening. While prior studies have examined links between low-levelaudio features and affective perception, the systematic impact of audio FX onemotion remains underexplored. This work investigates how foundation models -large-scale neural architectures pretrained on multimodal data - can beleveraged to analyze these effects. Such models encode rich associationsbetween musical structure, timbre, and affective meaning, offering a powerfulframework for probing the emotional consequences of sound design techniques. Byapplying various probing methods to embeddings from deep learning models, weexamine the complex, nonlinear relationships between audio FX and estimatedemotion, uncovering patterns tied to specific effects and evaluating therobustness of foundation audio models. Our findings aim to advanceunderstanding of the perceptual impact of audio production practices, withimplications for music cognition, performance, and affective computing.</description>
      <author>example@mail.com (Stelios Katsis, Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Giorgos Stamou)</author>
      <guid isPermaLink="false">2509.15151v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings</title>
      <link>http://arxiv.org/abs/2509.15001v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BabyHuBERT是一个在13,000小时多语言儿童中心长篇录音上训练的自监督语音表征模型，在说话人分割任务上表现优于现有模型，特别是在代表性不足的语言上。&lt;h4&gt;背景&lt;/h4&gt;研究儿童语言发展需要以儿童为中心的长篇录音，但现有的语音模型是在干净的成人数据上训练的，由于声学和语言差异，在儿童语音上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对儿童语音的自监督语音表征模型。&lt;h4&gt;方法&lt;/h4&gt;引入BabyHuBERT，在13,000小时多语言儿童中心长篇录音上训练，涵盖40多种语言。&lt;h4&gt;主要发现&lt;/h4&gt;BabyHuBERT在六个不同的数据集上实现了52.1%到74.4%的F1分数，一致优于W2V2-LL4300和标准HuBERT。在瓦努阿图语上比HuBERT高出13.2个F1点，在所罗门群岛语上高出15.9个点。&lt;h4&gt;结论&lt;/h4&gt;BabyHuBERT作为儿童语音研究的基础模型，通过分享代码和模型，能够为各种下游任务提供微调。&lt;h4&gt;翻译&lt;/h4&gt;以儿童为中心的长篇录音对于研究早期语言发展至关重要，但现有的在干净的成人数据上训练的语音模型由于声学和语言差异表现不佳。我们介绍了BabyHuBERT，这是第一个在13,000小时多语言儿童中心长篇录音上训练的自监督语音表征模型，涵盖40多种语言。我们在说话人分割任务上评估BabyHuBERT，即识别目标儿童何时说话，与女性成人、男性成人或其他儿童说话相比——这是分析自然语言体验的基本预处理步骤。BabyHuBERT在六个不同的数据集上实现了52.1%到74.4%的F1分数，一致优于W2V2-LL4300（在英语长篇录音上训练）和标准HuBERT（在干净的成人语音上训练）。显著的改进包括在瓦努阿图语上比HuBERT高出13.2个F1点，在所罗门群岛语上高出15.9个点，证明了在代表性不足的语言上的有效性。通过分享代码和模型，BabyHuBERT作为儿童语音研究的基础模型，能够为各种下游任务提供微调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Child-centered long-form recordings are essential for studying early languagedevelopment, but existing speech models trained on clean adult data performpoorly due to acoustic and linguistic differences. We introduce BabyHuBERT, thefirst self-supervised speech representation model trained on 13,000 hours ofmultilingual child-centered long-form recordings spanning over 40 languages. Weevaluate BabyHuBERT on speaker segmentation, identifying when target childrenspeak versus female adults, male adults, or other children -- a fundamentalpreprocessing step for analyzing naturalistic language experiences. BabyHuBERTachieves F1-scores from 52.1% to 74.4% across six diverse datasets,consistently outperforming W2V2-LL4300 (trained on English long-forms) andstandard HuBERT (trained on clean adult speech). Notable improvements include13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on SolomonIslands corpora, demonstrating effectiveness on underrepresented languages. Bysharing code and models, BabyHuBERT serves as a foundation model for childspeech research, enabling fine-tuning on diverse downstream tasks.</description>
      <author>example@mail.com (Théo Charlot, Tarek Kunze, Maxime Poli, Alejandrina Cristia, Emmanuel Dupoux, Marvin Lavechin)</author>
      <guid isPermaLink="false">2509.15001v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>PRISM: Product Retrieval In Shopping Carts using Hybrid Matching</title>
      <link>http://arxiv.org/abs/2509.14985v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PRISM的新混合方法，用于零售环境中的产品检索，结合了视觉语言模型和像素级匹配的优点，实现了高准确率和实时处理能力。&lt;h4&gt;背景&lt;/h4&gt;与传统图像检索相比，零售环境中的产品检索更具挑战性，因为同类型不同品牌的产品外观高度相似，且查询图像角度可能与目录图像视角差异显著。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的混合方法，解决基础模型难以区分细微差异和像素级匹配计算量大的问题，实现高效且准确的产品检索。&lt;h4&gt;方法&lt;/h4&gt;PRISM框架包含三个阶段：1)使用SigLIP视觉语言模型检索语义最相似的35个产品缩小搜索空间；2)应用YOLO-E分割模型消除背景杂乱；3)使用LightGlue进行细粒度像素级匹配。&lt;h4&gt;主要发现&lt;/h4&gt;PRISM框架能够通过关注全局模型常忽略的细微视觉线索，更准确地区分高度相似类别的产品，在ABV数据集上实现了4.21%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;PRISM方法在保持实时处理能力的同时，显著提高了产品检索的准确率，适用于实际零售环境部署。&lt;h4&gt;翻译&lt;/h4&gt;与传统图像检索任务相比，零售环境中的产品检索更具挑战性。来自不同品牌的同类型产品可能具有高度相似的视觉外观，且查询图像的拍摄角度可能与存储的目录图像视角有显著差异。基础模型（如CLIP和SigLIP）通常难以区分这些细微但重要的局部差异。另一方面，像素级匹配方法计算量大，导致匹配时间过高。在本文中，我们提出了一种名为PRISM的新混合方法，用于零售环境中的产品检索，通过结合基于视觉语言模型和像素级匹配方法的优势。为了同时提高效率/速度和细粒度检索准确性，PRISM包含三个阶段：1)首先使用视觉语言模型（SigLIP）从固定库中检索语义上最相似的35个产品，显著缩小搜索空间；2)应用分割模型（YOLO-E）消除背景杂乱；3)使用LightGlue在过滤后的候选产品上进行细粒度像素级匹配。该框架通过关注全局模型经常忽略的细微视觉线索，能够更准确地区分高度相似类别的产品。在ABV数据集上进行的实验表明，我们提出的PRISM在顶级-1准确率上比最先进的图像检索方法高出4.21%，同时仍保持实时处理能力，适用于实际零售部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compared to traditional image retrieval tasks, product retrieval in retailsettings is even more challenging. Products of the same type from differentbrands may have highly similar visual appearances, and the query image may betaken from an angle that differs significantly from view angles of the storedcatalog images. Foundational models, such as CLIP and SigLIP, often struggle todistinguish these subtle but important local differences. Pixel-wise matchingmethods, on the other hand, are computationally expensive and incurprohibitively high matching times. In this paper, we propose a new, hybridmethod, called PRISM, for product retrieval in retail settings by leveragingthe advantages of both vision-language model-based and pixel-wise matchingapproaches. To provide both efficiency/speed and finegrained retrievalaccuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP)is employed first to retrieve the top 35 most semantically similar productsfrom a fixed gallery, thereby narrowing the search space significantly; 2) asegmentation model (YOLO-E) is applied to eliminate background clutter; 3)fine-grained pixel-level matching is performed using LightGlue across thefiltered candidates. This framework enables more accurate discriminationbetween products with high inter-class similarity by focusing on subtle visualcues often missed by global models. Experiments performed on the ABV datasetshow that our proposed PRISM outperforms the state-of-the-art image retrievalmethods by 4.21% in top-1 accuracy while still remaining within the bounds ofreal-time processing for practical retail deployments.</description>
      <author>example@mail.com (Arda Kabadayi, Senem Velipasalar, Jiajing Chen)</author>
      <guid isPermaLink="false">2509.14985v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments</title>
      <link>http://arxiv.org/abs/2509.14978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出感知感知MPPI(PA-MPPI)方法，解决了四旋翼无人机在未知环境中导航时面临的探索挑战，相比传统MPPI方法性能提升高达100%&lt;h4&gt;背景&lt;/h4&gt;四旋翼无人机在未知环境中的导航对于实际任务如搜救至关重要，需要处理自由空间非凸性、四旋翼特定动力学和探索未知区域等挑战&lt;h4&gt;目的&lt;/h4&gt;解决MPPI方法在未知环境中缺乏探索未知区域能力的问题，使其能够在大障碍物阻挡时找到替代路径&lt;h4&gt;方法&lt;/h4&gt;提出感知感知MPPI(PA-MPPI)，通过根据感知目标在线调整轨迹，当目标被遮挡时，利用感知成本偏向能够感知未知区域的轨迹&lt;h4&gt;主要发现&lt;/h4&gt;硬件实验表明，PA-MPPI以50Hz频率运行，在挑战性设置中比基线方法性能提高高达100%，而最先进的MPPI在这些设置中失败&lt;h4&gt;结论&lt;/h4&gt;PA-MPPI能有效扩展可通行映射空间，增加找到替代路径的可能性，还可作为导航基础模型的安全可靠动作策略&lt;h4&gt;翻译&lt;/h4&gt;未知环境中的四旋翼导航对于搜救等实际任务至关重要。解决这一问题需要应对三个关键挑战：由障碍物导致的自由空间非凸性、四旋翼特定的动力学和目标，以及探索未知区域以找到通往目标的路径的需求。最近，模型预测路径积分(MPPI)方法已成为解决前两个挑战的有前景的解决方案。通过利用基于采样的优化，它可以有效处理非凸自由空间，同时直接优化整个四旋翼动力学，使能够包含四旋翼特定成本如能源消耗。然而，它在未知环境中的性能有限，因为它在大障碍物阻挡时缺乏探索未知区域的能力。为解决这个问题，我们引入了感知感知MPPI(PA-MPPI)。在这里，感知感知被定义为根据感知目标在线调整轨迹。具体而言，当目标被遮挡时，PA-MPPI的感知成本偏向能够感知未知区域的轨迹。这扩展了可通行映射空间并增加了找到通往目标的替代路径的可能性。通过硬件实验，我们证明PA-MPPI以50Hz的频率运行，配合我们高效的感知和映射模块，在我们具有挑战性的设置中，其性能比基线方法提高高达100%，而最先进的MPPI在这些设置中失败。此外，我们证明PA-MPPI可以用作导航基础模型的安全可靠的动作策略，这些模型通常提供无法直接到达的目标位姿。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quadrotor navigation in unknown environments is critical for practicalmissions such as search-and-rescue. Solving it requires addressing three keychallenges: the non-convexity of free space due to obstacles,quadrotor-specific dynamics and objectives, and the need for exploration ofunknown regions to find a path to the goal. Recently, the Model Predictive PathIntegral (MPPI) method has emerged as a promising solution that solves thefirst two challenges. By leveraging sampling-based optimization, it caneffectively handle non-convex free space while directly optimizing over thefull quadrotor dynamics, enabling the inclusion of quadrotor-specific costssuch as energy consumption. However, its performance in unknown environments islimited, as it lacks the ability to explore unknown regions when blocked bylarge obstacles. To solve this issue, we introduce Perception-Aware MPPI(PA-MPPI). Here, perception-awareness is defined as adapting the trajectoryonline based on perception objectives. Specifically, when the goal is occluded,PA-MPPI's perception cost biases trajectories that can perceive unknownregions. This expands the mapped traversable space and increases the likelihoodof finding alternative paths to the goal. Through hardware experiments, wedemonstrate that PA-MPPI, running at 50 Hz with our efficient perception andmapping module, performs up to 100% better than the baseline in our challengingsettings where the state-of-the-art MPPI fails. In addition, we demonstratethat PA-MPPI can be used as a safe and robust action policy for navigationfoundation models, which often provide goal poses that are not directlyreachable.</description>
      <author>example@mail.com (Yifan Zhai, Rudolf Reiter, Davide Scaramuzza)</author>
      <guid isPermaLink="false">2509.14978v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</title>
      <link>http://arxiv.org/abs/2509.14966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出RoboEye，一种两阶段识别框架，通过结合二维语义特征与领域自适应的三维推理和轻量级适配器，解决了大型电商平台产品类别快速增长导致的仓库自动化包装物体识别困难问题。&lt;h4&gt;背景&lt;/h4&gt;大型电商平台产品类别快速增长，导致仓库自动化包装中的物体识别变得困难。随着目录增长，类内变异性和稀有或视觉相似物品的长尾增加，结合多样化的包装、杂乱的容器、频繁遮挡和大视角变化等因素，放大了查询图像和参考图像之间的差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服仅依赖二维外观特征的方法性能下降问题的物体识别框架，提高在复杂环境下的识别准确率。&lt;h4&gt;方法&lt;/h4&gt;提出RoboEye两阶段识别框架：第一阶段训练大型视觉模型提取二维特征生成候选排名，使用轻量级三维特征感知模块估计三维特征质量并预测是否需要三维重排序；第二阶段使用机器人三维检索transformer，包括产生几何感知密集特征的三维特征提取器和基于关键点的匹配器。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，RoboEye比最先进的方法（RoboLLM）将Recall@1提高了7.1%，且仅使用RGB图像运行，避免了对显式三维输入的依赖。&lt;h4&gt;结论&lt;/h4&gt;RoboEye通过动态增强二维特征与三维推理，有效解决了产品类别快速增长带来的识别挑战，同时降低了部署成本。&lt;h4&gt;翻译&lt;/h4&gt;大型电商平台产品类别的快速增长使仓库自动化包装中的物体识别变得异常困难。随着目录增长，类内变异性和稀有或视觉相似物品的长尾增加，当与多样化的包装、杂乱的容器、频繁遮挡和大视角变化相结合时，这些因素放大了查询图像和参考图像之间的差异，导致仅依赖二维外观特征的方法性能急剧下降。因此，我们提出RoboEye，一个两阶段识别框架，动态将二维语义特征与领域自适应的三维推理和轻量级适配器结合，以弥合训练与部署的差距。在第一阶段，我们训练大型视觉模型提取二维特征生成候选排名。然后，轻量级三维特征感知模块估计三维特征质量并预测是否需要三维重排序，防止性能下降并避免不必要的计算。当被调用时，第二阶段使用我们的机器人三维检索transformer，包括产生几何感知密集特征的三维特征提取器和基于关键点的匹配器，计算查询图像和参考图像之间的关键点对应置信度，而不是传统的余弦相似度评分。实验表明，RoboEye比最先进的方法（RoboLLM）将Recall@1提高了7.1%。此外，RoboEye仅使用RGB图像运行，避免了对显式三维输入的依赖，降低了部署成本。本文使用的代码可在以下公开获取：https://github.com/longkukuhi/RoboEye。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决大型电商仓库中自动化包装时的物体识别问题。随着产品类别快速增长，类内差异性和视觉相似物品增加，结合多样化包装、杂乱容器、频繁遮挡和大视角变化等因素，使得仅依赖2D外观特征的方法性能急剧下降。这个问题在现实中至关重要，因为正确的物体识别为仓库自动化的运动规划和控制提供必要信息，错误识别会导致订单履行错误和巨大财务损失（论文提到亚马逊2025年第一季度因此损失约10亿美元）。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到仅依赖2D特征的方法在仓库环境中的局限性，然后思考如何利用3D几何特征提供视角不变的特征，但又不愿增加专门的传感器成本。他们设计了两阶段框架，动态增强2D特征与选择性3D推理。该方法借鉴了多项现有工作：使用BEiT-3模型提取2D特征，借鉴VGGT的3D特征提取能力，参考RoboLLM的方法论训练2D特征提取器，并采用知识适配器概念实现领域适应。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是动态增强2D外观特征与选择性3D几何推理，仅在必要时调用3D重排序，避免不必要的计算和性能下降，同时避免对显式3D输入的依赖。整体流程分为两阶段：第一阶段使用大型视觉模型提取2D特征生成候选排序，并通过轻量级3D特征感知模块判断是否需要3D重排序；第二阶段当需要时，使用机器人3D检索transformer，通过3D特征提取器和基于关键点的匹配器计算查询和参考图像间的关键点对应置信度，进行重排序。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个动态增强2D检索与隐式3D几何重排序的框架；2) MRR驱动的3D感知训练方案，选择性激活3D重排序；3) 基于关键点的检索匹配器，提供更鲁棒的相似性度量；4) 基于适配器的训练策略实现高效领域适应；5) 仅使用RGB图像，降低部署成本。相比之前的工作，RoboEye不仅提高了识别准确率（比RoboLLM高7.1%的Recall@1），还避免了显式3D传感器的需求，并智能判断何时使用3D特征，避免无条件使用导致的性能下降。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RoboEye通过选择性3D几何推理与2D外观特征的动态融合，在无需额外3D传感器的情况下，显著提升了复杂仓库环境中的物体识别准确率和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapidly growing number of product categories in large-scale e-commercemakes accurate object identification for automated packing in warehousessubstantially more difficult. As the catalog grows, intra-class variability anda long tail of rare or visually similar items increase, and when combined withdiverse packaging, cluttered containers, frequent occlusion, and largeviewpoint changes-these factors amplify discrepancies between query andreference images, causing sharp performance drops for methods that rely solelyon 2D appearance features. Thus, we propose RoboEye, a two-stage identificationframework that dynamically augments 2D semantic features with domain-adapted 3Dreasoning and lightweight adapters to bridge training deployment gaps. In thefirst stage, we train a large vision model to extract 2D features forgenerating candidate rankings. A lightweight 3D-feature-awareness module thenestimates 3D feature quality and predicts whether 3D re-ranking is necessary,preventing performance degradation and avoiding unnecessary computation. Wheninvoked, the second stage uses our robot 3D retrieval transformer, comprising a3D feature extractor that produces geometry-aware dense features and akeypoint-based matcher that computes keypoint-correspondence confidencesbetween query and reference images instead of conventional cosine-similarityscoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the priorstate of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,avoiding reliance on explicit 3D inputs and reducing deployment costs. The codeused in this paper is publicly available at:https://github.com/longkukuhi/RoboEye.</description>
      <author>example@mail.com (Xingwu Zhang, Guanxuan Li, Zhuocheng Zhang, Zijun Long)</author>
      <guid isPermaLink="false">2509.14966v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification</title>
      <link>http://arxiv.org/abs/2509.14958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为跨模态几何校正(CMGR)的框架，用于解决3D类增量学习在极端数据稀缺情况下的几何不对齐和纹理偏差问题。通过利用CLIP的层次空间语义，该方法增强了3D几何保真度，并通过结构感知几何校正模块、纹理增强模块和基础-新颖判别器提高了模型的性能。&lt;h4&gt;背景&lt;/h4&gt;随着3D数字内容的快速增长，需要对开放世界场景进行可扩展的识别系统。然而，现有的3D类增量学习方法在极端数据稀缺情况下表现不佳，主要由于几何不对齐和纹理偏差问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D类增量学习方法在极端数据稀缺情况下的几何不对齐和纹理偏差问题，提高3D少样本类增量学习的性能。&lt;h4&gt;方法&lt;/h4&gt;提出跨模态几何校正(CMGR)框架，包括：1)结构感知几何校正模块，通过注意力驱动的几何融合将3D部分结构与CLIP的中间空间先验进行层次对齐；2)纹理增强模块，合成最小但有区分度的纹理以抑制噪声并增强跨模态一致性；3)基础-新颖判别器，隔离几何变化以稳定增量原型。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了3D少样本类增量学习，在跨域和域内设置中实现了更好的几何一致性和对纹理偏差的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过利用CLIP的层次空间语义和创新的模块设计，CMGR框架有效解决了3D类增量学习中的几何不对齐和纹理偏差问题，提高了模型的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;随着3D数字内容的快速增长，需要对开放世界场景进行可扩展的识别系统。然而，现有的3D类增量学习方法在极端数据稀缺情况下因几何不对齐和纹理偏差而表现不佳。虽然最近的方法将3D数据与2D基础模型(如CLIP)集成，但它们因纹理投影偏差和几何-纹理线索的无差别融合导致语义模糊，进而导致决策原型不稳定和灾难性遗忘。为解决这些问题，我们提出了跨模态几何校正(CMGR)框架，通过利用CLIP的层次空间语义来增强3D几何保真度。具体而言，我们引入了一个结构感知几何校正模块，通过注意力驱动的几何融合将3D部分结构与CLIP的中间空间先验进行层次对齐。此外，纹理增强模块合成最小但有区分度的纹理，以抑制噪声并增强跨模态一致性。为进一步稳定增量原型，我们采用基础-新颖判别器来隔离几何变化。大量实验证明，我们的方法显著提高了3D少样本类增量学习，在跨域和域内设置中实现了更好的几何一致性和对纹理偏差的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D少样本类增量学习(3DFSCIL)中的几何错位、纹理偏差、语义模糊和灾难性遗忘问题。随着3D数字内容快速增长，在自动驾驶、工业机器人等领域需要能够处理动态出现新物体类别的系统，而这些系统在极端数据稀缺情况下表现不佳。几何结构和空间关系对3D物体识别至关重要，而现有方法过度依赖纹理信息，忽视了几何结构的重要性，导致在开放世界场景中难以有效识别新类别。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3DFSCIL方法的局限性，指出它们过度依赖CLIP的最终层表示而忽略了中间层的丰富空间先验信息，以及无差别融合机制导致的语义模糊。作者的关键洞察是CLIP的中间层保留了与3D几何层次结构自然对齐的空间关系。基于此，作者设计了跨模态几何校正框架(CMGR)，包含三个关键组件：SAGR、TAM和BND。作者借鉴了CLIP等视觉语言模型的跨模态学习能力、点云处理和深度图渲染技术，以及增量学习中的注意力机制和判别器设计，但创新性地将它们组合用于解决3D几何表示问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将CLIP的中间层作为'2D透镜'，通过跨模态几何校正增强3D表示的几何保真度。整体流程包括：1)点云通过3D分支处理生成特征，同时渲染为深度图通过2D分支处理；2)SAGR模块通过交叉注意力机制层次化对齐CLIP中间层特征与3D结构特征；3)TAM模块从点云特征学习判别性纹理模式并合成自适应RGB值；4)BND模块作为二分类器区分基础和新类别，稳定增量学习；5)结合几何特征和CLIP的多模态理解进行最终预测。每个增量任务前独立训练BND，允许基础和新类别并行处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)CMGR框架首次利用CLIP中间层作为'2D透镜'增强3D几何保真度；2)SAGR模块通过层次化对齐3D部分结构与CLIP空间先验解决语义模糊；3)TAM模块合成判别性纹理增强跨模态一致性；4)BND模块隔离几何变化稳定增量原型。相比之前工作，本文利用CLIP中间层而非仅用最终层，通过几何校正保留结构完整性而非无差别融合几何纹理特征，BND允许并行处理基础和新类别减轻灾难性遗忘，并通过层次化对齐建立3D结构与2D表示的明确对应关系解决语义模糊。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了跨模态几何校正框架，通过利用CLIP的层次化空间语义增强3D表示的几何保真度，显著提高了3D少样本类增量学习在跨域和域内场景中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of 3D digital content necessitates expandable recognitionsystems for open-world scenarios. However, existing 3D class-incrementallearning methods struggle under extreme data scarcity due to geometricmisalignment and texture bias. While recent approaches integrate 3D data with2D foundation models (e.g., CLIP), they suffer from semantic blurring caused bytexture-biased projections and indiscriminate fusion of geometric-texturalcues, leading to unstable decision prototypes and catastrophic forgetting. Toaddress these issues, we propose Cross-Modal Geometric Rectification (CMGR), aframework that enhances 3D geometric fidelity by leveraging CLIP's hierarchicalspatial semantics. Specifically, we introduce a Structure-Aware GeometricRectification module that hierarchically aligns 3D part structures with CLIP'sintermediate spatial priors through attention-driven geometric fusion.Additionally, a Texture Amplification Module synthesizes minimal yetdiscriminative textures to suppress noise and reinforce cross-modalconsistency. To further stabilize incremental prototypes, we employ aBase-Novel Discriminator that isolates geometric variations. Extensiveexperiments demonstrate that our method significantly improves 3D few-shotclass-incremental learning, achieving superior geometric coherence androbustness to texture bias across cross-domain and within-domain settings.</description>
      <author>example@mail.com (Xiang Tuo, Xu Xuemiao, Liu Bangzhen, Li Jinyi, Li Yong, He Shengfeng)</author>
      <guid isPermaLink="false">2509.14958v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications</title>
      <link>http://arxiv.org/abs/2509.14921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the IEEE International Joint Conference on Biometrics  2025 (IJCB 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了基础模型CLIP在生物识别任务微调后面临的过度专业化问题，发现微调会导致跨领域泛化能力下降，但特定任务性能提高，且较大模型能更好地保持原始泛化能力。&lt;h4&gt;背景&lt;/h4&gt;基础模型如CLIP在各种视觉任务中表现出强大的零样本和少样本迁移能力，但当针对高度专业化的生物识别任务（人脸识别FR、morphing攻击检测MAD、呈现攻击检测PAD）进行微调时，这些模型可能会过度专业化，从而失去跨领域泛化能力。&lt;h4&gt;目的&lt;/h4&gt;系统量化专业化与泛化能力之间的权衡，通过评估三个针对FR、MAD和PAD微调的CLIP实例来研究这一问题。&lt;h4&gt;方法&lt;/h4&gt;评估每个微调模型以及原始CLIP基线模型，在14个通用视觉数据集上使用零样本和线性探测协议进行评估，同时在常见的FR、MAD和PAD基准测试上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型存在过度专业化问题，特别是针对复杂的人脸识别任务；任务复杂度和分类头设计与灾难性遗忘程度相关；使用ViT-L骨干网的FRoundation模型在IJB-C上实现58.52%的改进，但在ImageNetV2上性能显著下降；较大CLIP架构比小版本保留了更多原始泛化能力。&lt;h4&gt;结论&lt;/h4&gt;微调基础模型会导致过度专业化，特别是在复杂任务上；增加模型容量可能有助于减轻过度专业化问题。&lt;h4&gt;翻译&lt;/h4&gt;基础模型如CLIP已在各种视觉任务中展现出卓越的零样本和少样本迁移能力。然而，当针对高度专业化的生物识别任务（人脸识别FR、morphing攻击检测MAD和呈现攻击检测PAD）进行微调时，这些模型可能会过度专业化，从而失去其基础优势之一——跨领域泛化能力。本研究通过评估三个针对FR、MAD和PAD微调的CLIP实例，系统量化了这些权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models such as CLIP have demonstrated exceptional zero- andfew-shot transfer capabilities across diverse vision tasks. However, whenfine-tuned for highly specialized biometric tasks, face recognition (FR),morphing attack detection (MAD), and presentation attack detection (PAD), thesemodels may suffer from over-specialization. Thus, they may lose one of theirfoundational strengths, cross-domain generalization. In this work, wesystematically quantify these trade-offs by evaluating three instances of CLIPfine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as theoriginal CLIP baseline on 14 general vision datasets under zero-shot andlinear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Ourresults indicate that fine-tuned models suffer from over-specialization,especially when fine-tuned for complex tasks of FR. Also, our results pointedout that task complexity and classification head design, multi-class (FR) vs.binary (MAD and PAD), correlate with the degree of catastrophic forgetting. TheFRoundation model with the ViT-L backbone outperforms other approaches on thelarge-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%.However, it experiences a substantial performance drop on ImageNetV2, reachingonly 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover,the larger CLIP architecture consistently preserves more of the model'soriginal generalization ability than the smaller variant, indicating thatincreased model capacity may help mitigate over-specialization.</description>
      <author>example@mail.com (Tahar Chettaoui, Naser Damer, Fadi Boutros)</author>
      <guid isPermaLink="false">2509.14921v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction</title>
      <link>http://arxiv.org/abs/2509.14739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FMGS-Avatar的新方法，用于从单目视频中重建高保真可动画化人类头像，通过结合网格引导的2D高斯泼溅和基础模型先验知识，解决了单目观察中几何信息不足和现有方法表面细节保留困难的问题。&lt;h4&gt;背景&lt;/h4&gt;从单目视频中重建高保真可动画化人类头像具有挑战性，主要是因为单目观察中的几何信息不足。现有的3D高斯泼溅方法由于3D高斯原子的自由形式特性，在保持表面细节方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决单目视频中人类头像重建的表示限制和信息稀缺问题，提高重建质量，包括几何准确性和外观保真度，同时提供丰富的语义信息。&lt;h4&gt;方法&lt;/h4&gt;FMGS-Avatar方法包含两个关键创新：1) 网格引导的2D高斯泼溅，将2D高斯原子直接附加到模板网格面上，具有受约束的位置、旋转和运动；2) 利用基础模型（如Sapiens）补充单目视频有限的视觉线索。通过具有选择性梯度隔离的协调训练策略解决多模态先验知识提炼中的冲突优化目标问题。&lt;h4&gt;主要发现&lt;/h4&gt;结合增强表示和协调信息蒸馏的方法显著提高了3D单目人类头像重建质量。实验评估显示，与现有方法相比具有更好的重建质量，在几何准确性和外观保真度方面有显著提升，同时提供丰富的语义信息。在共享规范空间内提炼的先验知识支持在新视角和姿势下进行空间和时间一致的渲染。&lt;h4&gt;结论&lt;/h4&gt;FMGS-Avatar方法通过网格引导的2D高斯泼溅和基础模型先验知识的有效结合，成功解决了单目视频中人类头像重建的关键挑战，实现了高质量的重建结果，并支持在新视角和姿势下的一致渲染。&lt;h4&gt;翻译&lt;/h4&gt;从单目视频中重建高保真可动画化人类头像由于单目观察中几何信息不足而仍然具有挑战性。虽然最近的3D高斯泼溅方法显示出前景，但由于3D高斯原子的自由形式特性，它们在保持表面细节方面存在困难。为了解决表示限制和信息稀缺问题，我们提出了一种新方法FMGS-Avatar，它整合了两个关键创新。首先，我们引入了网格引导的2D高斯泼溅，其中2D高斯原子直接附加到模板网格面上，具有受约束的位置、旋转和运动，实现了更好的表面对齐和几何细节保留。其次，我们利用在大型数据集上训练的基础模型（如Sapiens）来补充单目视频有限的视觉线索。然而，当从基础模型中提炼多模态先验知识时，不同模态表现出不同的参数敏感性，可能导致冲突的优化目标。我们通过具有选择性梯度隔离的协调训练策略解决了这一问题，使每个损失组件能够优化其相关参数而不会相互干扰。通过这种增强表示和协调信息蒸馏的组合，我们的方法显著推进了3D单目人类头像重建。实验评估表明与现有方法相比具有更好的重建质量，在几何准确性和外观保真度方面有显著提升，同时提供丰富的语义信息。此外，在共享规范空间内提炼的先验知识自然地支持在新视角和姿势下进行空间和时间一致的渲染。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单目视频中重建高保真、可动画的3D人像avatar的问题。这个问题在现实中非常重要，因为高保真数字人像创建对娱乐、医疗、AR/VR和交互式模拟等应用至关重要，而传统动作捕捉方法需要昂贵设备或受控环境，限制了技术的普及。能够从普通单目RGB视频创建数字人像的方法将显著降低技术门槛，使更多人能够使用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了单目人像重建的两个主要挑战：几何模糊性和表示局限性。他们借鉴了3D高斯溅射(3DGS)的高效渲染优势，以及基础模型(如Sapiens)提供丰富2D先验知识的能力，还参考了将高斯原语附加到模板网格的方法(如GoMAvatar)。基于这些借鉴，作者创新性地设计了网格引导的2D高斯溅射表示和协调训练策略，以解决表面表示和多模态知识蒸馏中的优化冲突问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用表面导向的2D高斯原语替代体积导向的3D高斯，并结合基础模型的多模态先验知识来解决单目重建的几何模糊性。整体流程包括：1)规范空间表示：将2D高斯原语附加到上采样的模板网格面上；2)多场蒸馏：分别处理几何场、外观场和语义场；3)蒙皮场：通过线性混合蒙皮将规范空间转换到观察空间；4)训练与渲染：使用协调训练策略解决多模态优化冲突，并通过可微分的高光栅化进行渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)网格引导的2D高斯溅射：使用2D高斯原语直接附加到模板网格面上，解决3D高斯的体积性质在表示薄表面时的局限性；2)协调训练策略：通过选择性梯度隔离解决多模态优化冲突，使不同损失组件互不干扰；3)系统性的多模态知识蒸馏：从基础模型中综合提取深度、法线和语义等多种2D先验知识。相比之前工作，该方法使用表面导向而非体积导向的表示，系统性地而非单模态地蒸馏知识，并解决了优化冲突问题，同时实现了更快的训练速度和竞争性的推理性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FMGS-Avatar通过结合网格引导的2D高斯溅射表示和协调训练策略，实现了从单目视频中高效重建高保真、可动画的3D人像，同时系统性地蒸馏了基础模型的多模态2D先验知识，解决了几何模糊性和表示局限性两大挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing high-fidelity animatable human avatars from monocular videosremains challenging due to insufficient geometric information in single-viewobservations. While recent 3D Gaussian Splatting methods have shown promise,they struggle with surface detail preservation due to the free-form nature of3D Gaussian primitives. To address both the representation limitations andinformation scarcity, we propose a novel method, \textbf{FMGS-Avatar}, thatintegrates two key innovations. First, we introduce Mesh-Guided 2D GaussianSplatting, where 2D Gaussian primitives are attached directly to template meshfaces with constrained position, rotation, and movement, enabling superiorsurface alignment and geometric detail preservation. Second, we leveragefoundation models trained on large-scale datasets, such as Sapiens, tocomplement the limited visual cues from monocular videos. However, whendistilling multi-modal prior knowledge from foundation models, conflictingoptimization objectives can emerge as different modalities exhibit distinctparameter sensitivities. We address this through a coordinated trainingstrategy with selective gradient isolation, enabling each loss component tooptimize its relevant parameters without interference. Through this combinationof enhanced representation and coordinated information distillation, ourapproach significantly advances 3D monocular human avatar reconstruction.Experimental evaluation demonstrates superior reconstruction quality comparedto existing methods, with notable gains in geometric accuracy and appearancefidelity while providing rich semantic information. Additionally, the distilledprior knowledge within a shared canonical space naturally enables spatially andtemporally consistent rendering under novel views and poses.</description>
      <author>example@mail.com (Jinlong Fan, Bingyu Hu, Xingguang Li, Yuxiang Yang, Jing Zhang)</author>
      <guid isPermaLink="false">2509.14739v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models</title>
      <link>http://arxiv.org/abs/2509.14723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用transcoders从单细胞基础模型中提取可解释的决策电路，以提高这些模型的透明度和可解释性。&lt;h4&gt;背景&lt;/h4&gt;单细胞基础模型(scFMs)在各种任务上表现出色，如细胞类型注释和扰动响应预测，通过从大规模转录组数据中学习基因调控网络。然而，这些模型的决策过程相比传统方法(如差异基因表达分析)不太可解释。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在利用transcoders技术从单细胞基础模型中提取可解释的决策电路，以揭示模型内部的决策机制并验证其与真实生物学机制的相关性。&lt;h4&gt;方法&lt;/h4&gt;研究者在cell2sentence(C2S)模型(一种先进的单细胞基础模型)上训练了一个transcoder，并利用这个训练好的transcoder从C2S模型中提取内部决策电路。&lt;h4&gt;主要发现&lt;/h4&gt;通过transcoder提取的决策电路与现实世界的生物学机制相对应，表明这些电路确实捕捉了有意义的生物学信息。&lt;h4&gt;结论&lt;/h4&gt;transcoders技术有潜力揭示复杂单细胞模型中生物学上合理的通路，提高了这些模型的可解释性和透明度。&lt;h4&gt;翻译&lt;/h4&gt;单细胞基础模型(scFMs)已通过从大规模转录组数据中学习基因调控网络，在各种任务上展示了最先进的性能，如细胞类型注释和扰动响应预测。然而，一个重大挑战仍然存在：与传统方法如差异基因表达分析相比，这些模型的决策过程不太可解释。最近，transcoders作为一种有前景的方法出现，用于从大型语言模型(LLMs)中提取可解释的决策电路。在这项工作中，我们在cell2sentence(C2S)模型(一种最先进的scFM)上训练了一个transcoder。通过利用训练好的transcoder，我们从C2S模型中提取了内部决策电路。我们证明，发现的电路对应于现实世界的生物学机制，证实了transcoders揭示复杂单细胞模型中生物学上合理的通路的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell foundation models (scFMs) have demonstrated state-of-the-artperformance on various tasks, such as cell-type annotation and perturbationresponse prediction, by learning gene regulatory networks from large-scaletranscriptome data. However, a significant challenge remains: thedecision-making processes of these models are less interpretable compared totraditional methods like differential gene expression analysis. Recently,transcoders have emerged as a promising approach for extracting interpretabledecision circuits from large language models (LLMs). In this work, we train atranscoder on the cell2sentence (C2S) model, a state-of-the-art scFM. Byleveraging the trained transcoder, we extract internal decision-making circuitsfrom the C2S model. We demonstrate that the discovered circuits correspond toreal-world biological mechanisms, confirming the potential of transcoders touncover biologically plausible pathways within complex single-cell models.</description>
      <author>example@mail.com (Sosuke Hosokawa, Toshiharu Kawakami, Satoshi Kodera, Masamichi Ito, Norihiko Takeda)</author>
      <guid isPermaLink="false">2509.14723v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>HARNESS: Lightweight Distilled Arabic Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2509.14689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HArnESS是首个阿拉伯中心的自监督语音模型家族，通过知识蒸馏技术将大型模型压缩为轻量级版本，在资源受限环境中表现出色，仅需微调即可达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大型预训练语音模型在下游任务中表现优异，但在资源受限环境中部署不切实际。&lt;h4&gt;目的&lt;/h4&gt;开发一个轻量级但功能强大的语音模型，专门捕捉阿拉伯语音的细微差别，适用于实际应用场景。&lt;h4&gt;方法&lt;/h4&gt;使用迭代自蒸馏训练大型双语HArnESS (HL) SSL模型，并将知识压缩到学生模型(HS, HST)中，同时使用低秩近似进一步压缩模型。&lt;h4&gt;主要发现&lt;/h4&gt;在阿拉伯语音识别(ASR)、说话人情感识别(SER)和方言识别(DID)任务上，HArnESS与HuBERT和XLS-R相比表现有效，仅需微调即可达到最先进或可比较的性能。&lt;h4&gt;结论&lt;/h4&gt;HArnESS是资源受限环境下的轻量级但功能强大的替代方案，研究团队已发布模型和发现以支持低资源环境中的研究和部署。&lt;h4&gt;翻译&lt;/h4&gt;大型预训练语音模型在下游任务中表现出色，但在资源受限环境中部署不切实际。在本文中，我们介绍了HArnESS，这是第一个阿拉伯中心的自监督语音模型家族，旨在捕捉阿拉伯语音的细微差别。使用迭代自蒸馏，我们训练大型双语HArnESS (HL) SSL模型，然后将知识压缩到压缩的学生模型(HS, HST)中，保留阿拉伯特定表示。我们使用低秩近似将教师的离散监督进一步压缩到浅层、薄型模型中。我们在阿拉伯语音识别(ASR)、说话人情感识别(SER)和方言识别(DID)上评估HArnESS，证明了其相对于HuBERT和XLS-R的有效性。仅通过最小微调，HArnESS就能达到最先进或可比较的性能，使其成为实际应用的轻量级但功能强大的替代方案。我们发布蒸馏模型和发现，以支持低资源环境中的负责任的研究和部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large pre-trained speech models excel in downstream tasks but theirdeployment is impractical for resource-limited environments. In this paper, weintroduce HArnESS, the first Arabic-centric self-supervised speech modelfamily, designed to capture Arabic speech nuances. Using iterativeself-distillation, we train large bilingual HArnESS (HL) SSL models and thendistill knowledge into compressed student models (HS, HST), preservingArabic-specific representations. We use low-rank approximation to furthercompact the teacher's discrete supervision into shallow, thin models. Weevaluate HArnESS on Arabic ASR, Speaker Emotion Recognition (SER), and DialectIdentification (DID), demonstrating effectiveness against HuBERT and XLS-R.With minimal fine-tuning, HArnESS achieves SOTA or comparable performance,making it a lightweight yet powerful alternative for real-world use. We releaseour distilled models and findings to support responsible research anddeployment in low-resource settings.</description>
      <author>example@mail.com (Vrunda N. sukhadia, Shammur Absar Chowdhury)</author>
      <guid isPermaLink="false">2509.14689v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images</title>
      <link>http://arxiv.org/abs/2509.14685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DACoN框架，通过融合基础模型和CNN的特征，实现线条图自动着色，支持任意数量的参考图像，解决了遮挡、姿势变化和视角变化等挑战。&lt;h4&gt;背景&lt;/h4&gt;线条图自动着色研究广泛，旨在减少手绘动漫制作的人工成本。现有深度学习方法在准确性上有所提高，但在处理遮挡、姿势变化和视角变化方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决线条图自动着色中遮挡、姿势变化和视角变化等挑战，提高着色性能。&lt;h4&gt;方法&lt;/h4&gt;DACoN框架利用基础模型捕捉部分级语义，将基础模型的低分辨率语义特征与CNN的高分辨率空间特征融合，进行细粒度且鲁棒的特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;使用多个参考图像可以显著提高线条图自动着色的性能，DACoN框架能够支持任意数量的参考图像，而不再局限于一张或两张参考图像。&lt;h4&gt;结论&lt;/h4&gt;DACoN框架通过融合基础模型和CNN的特征，实现了优越的线条图自动着色性能，解决了现有方法在遮挡、姿势变化和视角变化方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;线条图自动着色已被广泛研究，以减少手绘动漫制作的人工成本。包括图像/视频生成和基于特征的对应在内的深度学习方法提高了准确性，但在处理遮挡、姿势变化和视角变化方面存在困难。为了解决这些挑战，我们提出了DACoN框架，它利用基础模型来捕捉部分级语义，即使在线条图中也是如此。我们的方法将基础模型的低分辨率语义特征与CNN的高分辨率空间特征融合，进行细粒度且鲁棒的特征提取。与依赖多路变换器且仅支持一张或两张参考图像的先前方法不同，DACoN移除了这一限制，允许任意数量的参考图像。定量和定性评估证明了使用多个参考图像的优越性，实现了卓越的着色性能。我们的代码和模型可在https://github.com/kzmngt/DACoN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic colorization of line drawings has been widely studied to reduce thelabor cost of hand-drawn anime production. Deep learning approaches, includingimage/video generation and feature-based correspondence, have improved accuracybut struggle with occlusions, pose variations, and viewpoint changes. Toaddress these challenges, we propose DACoN, a framework that leveragesfoundation models to capture part-level semantics, even in line drawings. Ourmethod fuses low-resolution semantic features from foundation models withhigh-resolution spatial features from CNNs for fine-grained yet robust featureextraction. In contrast to previous methods that rely on the MultiplexTransformer and support only one or two reference images, DACoN removes thisconstraint, allowing any number of references. Quantitative and qualitativeevaluations demonstrate the benefits of using multiple reference images,achieving superior colorization performance. Our code and model are availableat https://github.com/kzmngt/DACoN.</description>
      <author>example@mail.com (Kazuma Nagata, Naoshi Kaneko)</author>
      <guid isPermaLink="false">2509.14685v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model</title>
      <link>http://arxiv.org/abs/2509.14664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at ICONIP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的视觉基础模型解释生成方法，通过引入注意力网格适配器(ALA)和交替周期架构者(AEA)两种机制，解决了现有方法缺乏适应性的问题，提高了模型的解释性能。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型中生成视觉解释已有多种方法被提出，但这些方法通常缺乏适应性，无法应用于复杂模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的视觉基础模型解释生成方法，旨在生成解释并部分更新模型参数以提高可解释性。&lt;h4&gt;方法&lt;/h4&gt;引入两个新机制：注意力网格适配器(ALA)通过消除手动层选择需要来简化过程并增强适应性和可解释性；交替周期架构者(AEA)每隔一个周期更新ALA参数，有效解决注意力区域过小的问题。&lt;h4&gt;主要发现&lt;/h4&gt;在CUB-200-2011和ImageNet-S两个基准数据集上，该方法在平均交并比(IoU)、插入分数、删除分数和插入-删除分数方面均优于基线方法，最佳模型在CUB-200-2011数据集上的平均IoU比基线提高了53.2个百分点。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过两个创新机制成功解决了现有视觉基础模型解释生成方法的局限性，显著提高了模型的适应性和解释性能。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，我们考虑视觉基础模型中生成视觉解释的问题。为此目的已经提出了许多方法；然而，由于缺乏适应性，它们通常无法应用于复杂模型。为了克服这些限制，我们提出了一种新颖的视觉基础模型解释生成方法，旨在生成解释并部分更新模型参数以提高可解释性。我们的方法引入了两种新机制：注意力网格适配器(ALA)和交替周期架构者(AEA)。ALA机制通过消除手动层选择的需要来简化过程，从而增强模型的适应性和可解释性。此外，每隔一个周期更新ALA参数的AEA机制有效地解决了注意力区域过小这一常见问题。我们在两个基准数据集CUB-200-2011和ImageNet-S上评估了我们的方法。我们的结果表明，在CUB-200-2011和ImageNet-S数据集上，我们的方法在平均交并比(IoU)、插入分数、删除分数和插入-删除分数方面均优于基线方法。值得注意的是，与基线方法相比，我们的最佳模型在CUB-200-2011数据集上的平均IoU提高了53.2个百分点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we consider the problem of generating visual explanations invisual foundation models. Numerous methods have been proposed for this purpose;however, they often cannot be applied to complex models due to their lack ofadaptability. To overcome these limitations, we propose a novel explanationgeneration method in visual foundation models that is aimed at both generatingexplanations and partially updating model parameters to enhanceinterpretability. Our approach introduces two novel mechanisms: AttentionLattice Adapter (ALA) and Alternating Epoch Architect (AEA). ALA mechanismsimplifies the process by eliminating the need for manual layer selection, thusenhancing the model's adaptability and interpretability. Moreover, the AEAmechanism, which updates ALA's parameters every other epoch, effectivelyaddresses the common issue of overly small attention regions. We evaluated ourmethod on two benchmark datasets, CUB-200-2011 and ImageNet-S. Our resultsshowed that our method outperformed the baseline methods in terms of meanintersection over union (IoU), insertion score, deletion score, andinsertion-deletion score on both the CUB-200-2011 and ImageNet-S datasets.Notably, our best model achieved a 53.2-point improvement in mean IoU on theCUB-200-2011 dataset compared with the baselines.</description>
      <author>example@mail.com (Shinnosuke Hirano, Yuiga Wada, Tsumugi Iida, Komei Sugiura)</author>
      <guid isPermaLink="false">2509.14664v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics</title>
      <link>http://arxiv.org/abs/2509.14600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the ICML 2025 Workshop on Multi-modal Foundation  Models and Large Language Models for Life Sciences, Vancouver, Canada. 2025.  Copyright 2025 by the author(s). 4 Pages 5 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种改进的粗粒度机器学习模型，通过在损失函数中添加能量匹配项来增强对生物分子系统的模拟效果，尽管在准确性上没有显著提升，但揭示了模型推广自由能表面的不同趋势。&lt;h4&gt;背景&lt;/h4&gt;分子动力学模拟提供生物分子系统的原子级见解，但计算成本高；传统粗粒度机器学习模型的力匹配方法无法完整捕捉势能面，因为梯度拟合可能无法反映低能构象态间的绝对差异。&lt;h4&gt;目的&lt;/h4&gt;通过在损失函数中纳入互补的能量匹配项，改进粗粒度模型对生物分子系统的模拟效果。&lt;h4&gt;方法&lt;/h4&gt;使用CGSchNet模型在Chignolin蛋白上评估新框架，系统性地调整能量损失项的权重。&lt;h4&gt;主要发现&lt;/h4&gt;能量匹配未显著提高模型准确性，但揭示了模型推广自由能表面的不同趋势。&lt;h4&gt;结论&lt;/h4&gt;改进能量估计技术和采用多模态损失公式有望增强未来的粗粒度建模效果。&lt;h4&gt;翻译&lt;/h4&gt;分子动力学模拟为生物分子系统提供了原子级见解，但通常需要高昂的计算成本才能访问长时间尺度。粗粒度机器学习模型为加速采样提供了有希望的途径，但传统的力匹配方法往往无法捕捉完整的势能面，因为在梯度上拟合模型可能无法拟合低能构象态之间的绝对差异。在这项工作中，我们将一个互补的能量匹配项纳入损失函数。我们使用CGSchNet模型在Chignolin蛋白上评估了我们的框架，系统地改变了能量损失项的权重。虽然能量匹配没有在准确性方面产生统计学上的显著改进，但它揭示了模型如何推广自由能表面的不同趋势。我们的研究结果表明，通过改进能量估计技术和多模态损失公式，未来有机会增强粗粒度建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics (MD) simulations provide atomistic insight intobiomolecular systems but are often limited by high computational costs requiredto access long timescales. Coarse-grained machine learning models offer apromising avenue for accelerating sampling, yet conventional force matchingapproaches often fail to capture the full thermodynamic landscape as fitting amodel on the gradient may not fit the absolute differences between low-energyconformational states. In this work, we incorporate a complementary energymatching term into the loss function. We evaluate our framework on theChignolin protein using the CGSchNet model, systematically varying the weightof the energy loss term. While energy matching did not yield statisticallysignificant improvements in accuracy, it revealed distinct tendencies in howmodels generalize the free energy surface. Our results suggest futureopportunities to enhance coarse-grained modeling through improved energyestimation techniques and multi-modal loss formulations.</description>
      <author>example@mail.com (Alexander Aghili, Andy Bruce, Daniel Sabo, Razvan Marinescu)</author>
      <guid isPermaLink="false">2509.14600v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>CLAIP-Emo: Parameter-Efficient Adaptation of Language-supervised models for In-the-Wild Audiovisual Emotion Recognition</title>
      <link>http://arxiv.org/abs/2509.14527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The code and models will be available at  https://github.com/MSA-LMC/CLAIP-Emo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CLAIP-Emo 是一个模块化框架，通过参数高效适应语言监督基础模型（CLIP/CLAP）解决实际环境中音频情感识别的挑战，仅需少量训练参数即可达到最先进水平。&lt;h4&gt;背景&lt;/h4&gt;实际环境中的音频情感识别（AVER）仍受姿势变化、遮挡和背景噪声的阻碍。现有方法主要依赖大规模领域特定预训练，成本高昂且与真实情感数据不匹配。&lt;h4&gt;目的&lt;/h4&gt;提出一个模块化框架，将实际环境中的 AVER 重新定义为语言监督基础模型的参数高效适应方案。&lt;h4&gt;方法&lt;/h4&gt;保留语言监督先验（冻结主干并通过 LoRA 进行情感导向适应，更新≤4.0%参数）；非对称时间建模（视觉动态使用轻量级 Transformer，音频韵律使用平均池化）；应用简单融合头进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;在 DFEW 和 MAFW 数据集上，CLAIP-Emo (ViT-L/14) 仅使用 8M 训练参数分别实现了 80.14% 和 61.18% 的加权平均召回率，创造了新的最先进水平。&lt;h4&gt;结论&lt;/h4&gt;语言监督基础模型的参数高效适应为实际环境中的 AVER 提供了可扩展的替代方案，优于领域特定的预训练。&lt;h4&gt;翻译&lt;/h4&gt;实际环境中的音频情感识别（AVER）仍受到姿势变化、遮挡和背景噪声的阻碍。现有方法主要依赖大规模领域特定的预训练，这成本高昂且常常与真实情感数据不匹配。为解决这一问题，我们提出了 CLAIP-Emo，一个模块化框架，将实际环境中的 AVER 重新定义为语言监督基础模型（CLIP/CLAP）的参数高效适应。具体而言，它（i）通过冻结 CLIP/CLAP 主干并通过 LoRA 进行情感导向的适应来保留语言监督先验（更新总参数的≤4.0%），（ii）非对称分配时间建模，对视觉动态使用轻量级 Transformer，对音频韵律应用平均池化，以及（iii）应用简单的融合头进行预测。在 DFEW 和 MAFW 上，CLAIP-Emo (ViT-L/14) 仅使用 8M 训练参数就分别实现了 80.14% 和 61.18% 的加权平均召回率，创造了新的最先进水平。我们的研究表明，语言监督基础模型的参数高效适应为实际环境中的 AVER 提供了可扩展的替代方案，优于领域特定的预训练。代码和模型将在 https://github.com/MSA-LMC/CLAIP-Emo 上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audiovisual emotion recognition (AVER) in the wild is still hindered by posevariation, occlusion, and background noise. Prevailing methods primarily relyon large-scale domain-specific pre-training, which is costly and oftenmismatched to real-world affective data. To address this, we present CLAIP-Emo,a modular framework that reframes in-the-wild AVER as a parameter-efficientadaptation of language-supervised foundation models (CLIP/CLAP). Specifically,it (i) preserves language-supervised priors by freezing CLIP/CLAP backbones andperforming emotion-oriented adaptation via LoRA (updating \ensuremath{\le}4.0\%of the total parameters), (ii) allocates temporal modeling asymmetrically,employing a lightweight Transformer for visual dynamics while applying meanpooling for audio prosody, and (iii) applies a simple fusion head forprediction. On DFEW and MAFW, CLAIP-Emo (ViT-L/14) achieves 80.14\% and 61.18\%weighted average recall with only 8M training parameters, setting a new stateof the art. Our findings suggest that parameter-efficient adaptation oflanguage-supervised foundation models provides a scalable alternative todomain-specific pre-training for real-world AVER. The code and models will beavailable at\href{https://github.com/MSA-LMC/CLAIP-Emo}{https://github.com/MSA-LMC/CLAIP-Emo}.</description>
      <author>example@mail.com (Yin Chen, Jia Li, Jinpeng Hu, Zhenzhen Hu, Richang Hong)</author>
      <guid isPermaLink="false">2509.14527v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents</title>
      <link>http://arxiv.org/abs/2509.14480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种用于训练智能体掌握工具集成推理(TIR)的新方法，通过强化学习沙盒环境和回合级裁决强化学习(TARL)策略，显著提高了任务通过率，并成功训练了具有工具使用能力的多模态基础模型。&lt;h4&gt;背景&lt;/h4&gt;有效使用交互工具需要智能体掌握工具集成推理(TIR)，这是一个涉及多轮规划和长上下文对话管理的复杂过程。在多模态上下文中训练这种动态过程尤其具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;训练智能体在多模态上下文中掌握工具集成推理能力，开发更自然、语音驱动的交互智能体。&lt;h4&gt;方法&lt;/h4&gt;引入支持交错语音-文本展开的强化学习沙盒环境；提出回合级裁决强化学习(TARL)策略，使用大型语言模型作为裁判提供回合级评估；集成包含数学推理问题的混合任务训练课程以增强探索能力；在交错语音-文本展开上训练基础多模态大型语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;统一的方法将基于文本的τ-bench上的任务通过率提高了6%以上，相比强大的强化学习基线；框架适合微调多模态基础模型用于智能体任务；通过训练使基础多模态大型语言模型获得了工具使用能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为开发更自然、语音驱动的交互智能体铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;有效的交互工具使用需要智能体掌握工具集成推理(TIR)：一个涉及多轮规划和长上下文对话管理的复杂过程。为了在多模态上下文中训练智能体掌握这一动态过程，我们引入了一个支持交错语音-文本展开的强化学习(RL)沙盒环境。我们的核心策略——回合级裁决强化学习(TARL)——通过使用大型语言模型(LLM)作为裁判提供回合级评估，解决了长距离任务中的信用分配挑战。为了增强探索能力，我们整合了包含数学推理问题的混合任务训练课程。与强大的强化学习基线相比，这种统一的方法将基于文本的τ-bench上的任务通过率提高了6%以上。关键的是，我们证明了我们的框架适合微调多模态基础模型用于智能体任务。通过在交错语音-文本展开上训练基础多模态大型语言模型，我们为其赋予了工具使用能力，为开发更自然、语音驱动的交互智能体铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective interactive tool use requires agents to master Tool IntegratedReasoning (TIR): a complex process involving multi-turn planning andlong-context dialogue management. To train agents for this dynamic process,particularly in multi-modal contexts, we introduce a sandbox environment forreinforcement learning (RL) that supports interleaved speech-text rollouts. Ourcore strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addressesthe challenge of credit assignment in long-horizon tasks by employing a LargeLanguage Model (LLM) as a judge to provide turn-level evaluation. To enhanceexploration, we integrate a mixed-task training curriculum with mathematicalreasoning problems. This unified approach boosts the task pass rate on thetext-based $\tau$-bench by over 6% compared to strong RL baselines. Crucially,we demonstrate our framework's suitability for fine-tuning a multi-modalfoundation model for agentic tasks. By training a base multi-modal LLM oninterleaved speech-text rollouts, we equip it with tool-use abilities, pavingthe way for more natural, voice-driven interactive agents.</description>
      <author>example@mail.com (Weiting Tan, Xinghua Qu, Ming Tu, Meng Ge, Andy T. Liu, Philipp Koehn, Lu Lu)</author>
      <guid isPermaLink="false">2509.14480v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</title>
      <link>http://arxiv.org/abs/2509.14380v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CRAFT框架，利用基础模型作为多机器人协调的'教练'，自动分解长期任务为子任务序列，通过LLM生成奖励函数，并用VLM优化，成功实现了复杂协调行为的学习。&lt;h4&gt;背景&lt;/h4&gt;多智能体强化学习为多系统协调提供了强大框架，但应用于机器人仍面临高维连续动作空间、复杂奖励设计和去中心化环境中非平稳转换等挑战。相比之下，人类通过阶段性课程学习协调能力，逐步构建长期行为。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自动分解长期协调任务、生成适当奖励函数并指导多机器人系统学习复杂协调行为的框架，解决MARL在机器人应用中的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出CRAFT框架，利用大型语言模型规划能力自动分解任务为子任务序列，使用LLM生成奖励函数，通过VLM引导的奖励循环进行优化，并在多四足导航和双臂操作任务上评估，最后在实际硬件中验证导航策略。&lt;h4&gt;主要发现&lt;/h4&gt;CRAFT框架能够有效学习复杂的协调行为，在多四足导航和双臂操作任务上表现良好，且其多四足导航策略在实际硬件实验中得到了成功验证。&lt;h4&gt;结论&lt;/h4&gt;CRAFT通过利用基础模型作为'教练'，有效解决了多机器人协调学习中的挑战，特别是在处理长期协调任务方面，为实际应用提供了可行方案。&lt;h4&gt;翻译&lt;/h4&gt;多智能体强化学习为多智能体系统中的协调学习提供了强大的框架。然而，由于高维连续联合动作空间、复杂的奖励设计以及去中心化设置中固有的非平稳转换，将MARL应用于机器人仍然具有挑战性。另一方面，人类通过阶段性课程学习复杂的协调能力，其中长期行为建立在更简单的技能基础上。受此启发，我们提出了CRAFT：一种利用基础模型推理能力作为多机器人协调任务'教练'的框架。CRAFT利用大型语言模型的规划能力，自动将长期协调任务分解为子任务序列。随后，CRAFT使用大型语言模型生成的奖励函数训练每个子任务，并通过视觉语言模型引导的奖励循环进行优化。我们在多四足导航和双臂操作任务上评估了CRAFT，展示了其学习复杂协调行为的能力。此外，我们在实际硬件实验中验证了多四足导航策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-Agent Reinforcement Learning (MARL) provides a powerful framework forlearning coordination in multi-agent systems. However, applying MARL torobotics still remains challenging due to high-dimensional continuous jointaction spaces, complex reward design, and non-stationary transitions inherentto decentralized settings. On the other hand, humans learn complex coordinationthrough staged curricula, where long-horizon behaviors are progressively builtupon simpler skills. Motivated by this, we propose CRAFT: CoachingReinforcement learning Autonomously using Foundation models for multi-robotcoordination Tasks, a framework that leverages the reasoning capabilities offoundation models to act as a "coach" for multi-robot coordination. CRAFTautomatically decomposes long-horizon coordination tasks into sequences ofsubtasks using the planning capability of Large Language Models (LLMs). In whatfollows, CRAFT trains each subtask using reward functions generated by LLM, andrefines them through a Vision Language Model (VLM)-guided reward-refinementloop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulationtasks, demonstrating its capability to learn complex coordination behaviors. Inaddition, we validate the multi-quadruped navigation policy in real hardwareexperiments.</description>
      <author>example@mail.com (Seoyeon Choi, Kanghyun Ryu, Jonghoon Ock, Negar Mehr)</author>
      <guid isPermaLink="false">2509.14380v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning</title>
      <link>http://arxiv.org/abs/2509.14373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CodeLSI是一个结合低秩优化和领域特定指令调优的框架，用于解决基础模型在自动代码生成中面临的领域特异性、成本效益和安全性挑战，特别是在不依赖第三方API的情况下生成高质量代码。&lt;h4&gt;背景&lt;/h4&gt;使用基础模型进行自动代码生成为提高软件开发效率提供了有前景的解决方案。然而，在确保领域特异性、成本效益和安全性方面仍然存在挑战，特别是在依赖第三方API时。&lt;h4&gt;目的&lt;/h4&gt;开发并评估CodeLSI，这是一种新的方法，用于生成针对特定领域的高质量代码，使用在公司基础设施上微调的基础模型，不依赖外部API。&lt;h4&gt;方法&lt;/h4&gt;CodeLSI应用低秩适配技术来减少模型预训练和微调的计算成本。采用领域特定指令调优使代码生成与组织需求保持一致。研究者在内部软件项目的数据集上使用JavaScript编码任务对框架进行了实现和测试。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明，CodeLSI能够生成高质量、具有上下文感知能力的代码。在相关性、准确性和领域适应性方面，它优于基线模型。低秩优化的使用显著降低了资源需求，使在公司拥有的基础设施上进行可扩展的训练成为可能。&lt;h4&gt;结论&lt;/h4&gt;CodeLSI证明，结合低秩优化和领域特定调优可以增强基础模型在自动代码生成中的实用性和性能。这种方法为基于商业API的解决方案提供了安全、成本高效的替代方案，并支持软件开发中更快、更有针对性的创新。&lt;h4&gt;翻译&lt;/h4&gt;背景：使用基础模型进行自动代码化为提高软件开发效率提供了有前景的解决方案。然而，在确保领域特异性、成本效益和安全性方面仍然存在挑战，特别是在依赖第三方API时。本文介绍了CodeLSI，这是一个结合低秩优化和领域特定指令调优以应对这些挑战的框架。目的：本研究旨在开发并评估CodeLSI，这是一种新的方法，用于生成针对特定领域的高质量代码，使用在公司基础设施上微调的基础模型，不依赖外部API。方法：CodeLSI应用低秩适配技术来减少模型预训练和微调的计算成本。采用领域特定指令调优使代码生成与组织需求保持一致。研究者在内部软件项目的数据集上使用JavaScript编码任务对框架进行了实现和测试。结果：实验评估表明，CodeLSI能够生成高质量、具有上下文感知能力的代码。在相关性、准确性和领域适应性方面，它优于基线模型。低秩优化的使用显著降低了资源需求，使在公司拥有的基础设施上进行可扩展的训练成为可能。结论：CodeLSI证明，结合低秩优化和领域特定调优可以增强基础模型在自动代码生成中的实用性和性能。这种方法为基于商业API的解决方案提供了安全、成本高效的替代方案，并支持软件开发中更快、更有针对性的创新。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Context: Automated code generation using Foundation Models (FMs) offerspromising solutions for enhancing software development efficiency. However,challenges remain in ensuring domain specificity, cost-effectiveness, andsecurity - especially when relying on third-party APIs. This paper introducesCodeLSI, a framework that combines low-rank optimization and domain-specificinstruction tuning to address these challenges.  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novelapproach for generating high-quality code tailored to specific domains, usingFMs fine-tuned on company infrastructure without dependence on external APIs.  Methods: CodeLSI applies low-rank adaptation techniques to reduce thecomputational cost of model pre-training and fine-tuning. Domain-specificinstruction tuning is employed to align code generation with organizationalneeds. We implemented and tested the framework on real-world JavaScript codingtasks using datasets drawn from internal software projects.  Results: Experimental evaluations show that CodeLSI produces high-quality,context aware code. It outperforms baseline models in terms of relevance,accuracy, and domain fit. The use of low-rank optimization significantlyreduced resource requirements, enabling scalable training on company-ownedinfrastructure.  Conclusion: CodeLSI demonstrates that combining low-rank optimization withdomain specific tuning can enhance the practicality and performance of FMs forautomated code generation. This approach provides a secure, cost-efficientalternative to commercial API based solutions and supports faster, moretargeted innovation in software development.</description>
      <author>example@mail.com (Huy Le, Phong Nguyen, Hao Do, Tuan Nguyen, Thien Pham, Anh Nguyen-Duc, Tho Quan)</author>
      <guid isPermaLink="false">2509.14373v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Reaction dynamics of lithium-mediated electrolyte decomposition using machine learning potentials</title>
      <link>http://arxiv.org/abs/2509.14067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究揭示了碳酸乙烯酯在锂原子和锂金属表面的开环分解机制，发现了一种新的超快分解途径，其反应速率比传统路径快几个数量级。&lt;h4&gt;背景&lt;/h4&gt;研究碳酸乙烯酯在单个锂原子存在下以及在锂金属表面的开环分解过程。&lt;h4&gt;目的&lt;/h4&gt;通过精确计算获得碳酸乙烯酯分解的自由能剖面和反应速率，揭示反应机制和动力学特性。&lt;h4&gt;方法&lt;/h4&gt;结合精确的电子结构理论、增强采样技术和机器学习方法，优化MACE-MP0基础模型，应用优化后的机器学习势能面进行计算。&lt;h4&gt;主要发现&lt;/h4&gt;1) 电子结构理论水平对结果至关重要，不准确的密度泛函会高估反应速率达9个数量级；2) 谐波过渡态理论低估反应速率约1个数量级；3) 发现并表征了一种新的超快分解途径，羰基深度插入锂表面并弯曲约70度；4) 超快反应在几十皮秒内发生，生成开环中间体，是CO或CO2形成的前体；5) 另一种生成CO3^2-和乙烯的途径竞争力弱，发生在几十纳秒时间尺度。&lt;h4&gt;结论&lt;/h4&gt;碳酸乙烯酯在锂表面的分解存在超快路径，比传统路径快几个数量级，这对理解锂离子电池中电解液分解机制具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了在单个锂原子存在下以及在锂金属表面上的碳酸乙烯酯的开环分解。结合精确的电子结构理论、增强采样和机器学习，我们优化了MACE-MP0基础模型，并将得到的机器学习势能面应用于获得统计收敛的自由能剖面和反应速率。我们确认电子结构理论水平很重要，不准确的密度泛函会高估反应速率达9个数量级。我们还发现谐波过渡态理论低估反应速率约1个数量级。对于表面反应，我们发现并表征了一种新的超快分解途径，其中羰基深度插入锂表面并弯曲约70度。这个反应在几十皮秒内发生，生成开环中间体，是CO或CO2形成的前体；相比之下，另一种生成CO3^2-和乙烯的途径竞争力不强，发生在几十纳秒的时间尺度上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the ring-opening decomposition of ethylene carbonate in the presenceof a single lithium atom and on the surface of lithium metal. Combiningaccurate electronic structure theory, enhanced sampling, and machine learning,we fine-tune the MACE-MP0 foundation model and apply the resulting machinelearning potentials to obtain statistically converged free energy profiles andreaction rates. We confirm that the level of electronic structure theory isimportant, and inaccurate density functionals can overestimate the reactionrate by up to nine orders of magnitude. We also find that harmonic transitionstate theory underestimates reaction rates by about one order of magnitude. Forthe surface reaction, we find and characterize a new, ultrafast decompositionpathway wherein the carbonyl is deeply inserted into the lithium surface andbent by about 70$^\circ$. This reaction, which occurs in a few tens ofpicoseconds, generates a ring-opened intermediate that is a precursor for CO orCO$_2$ formation; by contrast, an alternative pathway that yields CO$_3^{2-}$and ethylene is found to be non-competitive, occurring on a timescale of tensof nanoseconds.</description>
      <author>example@mail.com (Sohang Kundu, Diana Chamaki, Hong-Zhou Ye, Garvit Agarwal, Timothy C. Berkelbach)</author>
      <guid isPermaLink="false">2509.14067v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>SAIL-VL2 Technical Report</title>
      <link>http://arxiv.org/abs/2509.14033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAIL-VL2是一个开放的多模态基础模型，作为SAIL-VL的后续版本，在20亿和80亿参数规模上实现了最先进性能，从细粒度感知到复杂推理都表现出强大能力。&lt;h4&gt;背景&lt;/h4&gt;SAIL-VL2是SAIL-VL的后续版本，旨在开发一个全面的多模态理解和推理基础模型。&lt;h4&gt;目的&lt;/h4&gt;创建一个能够进行综合多模态理解和推理的基础模型，并在各种图像和视频基准测试中实现最先进的性能。&lt;h4&gt;方法&lt;/h4&gt;三大核心创新：1)大规模数据整理流程，通过评分和过滤策略提高质量和分布；2)渐进式训练框架，从强大的预训练视觉编码器开始，经过多模态预训练，最终采用思考融合的SFT-RL混合范式；3)架构改进，扩展到高效的稀疏专家混合(MoE)设计。&lt;h4&gt;主要发现&lt;/h4&gt;SAIL-VL2在106个数据集上具有竞争力，在MMMU和MathVista等挑战性推理基准测试上取得最先进结果，在OpenCompass排行榜上，SAIL-VL2-2B在40亿参数规模以下的官方发布开源模型中排名第一。&lt;h4&gt;结论&lt;/h4&gt;SAIL-VL2是一个高效且可扩展的开源多模态社区基础，为开源多模态社区提供了强大的基础。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了SAIL-VL2，这是一个开放的多模态基础模型，用于全面的多模态理解和推理。作为SAIL-VL的后续版本，SAIL-VL2在20亿和80亿参数规模上跨越各种图像和视频基准测试实现了最先进的性能，展示了从细粒度感知到复杂推理的强大能力。三个核心创新推动了其有效性。首先，具有评分和过滤策略的大规模数据整理流程提高了字幕、OCR、问答和视频数据的质量和分布，提高了训练效率。其次，渐进式训练框架从强大的预训练视觉编码器(SAIL-ViT)开始，通过多模态预训练，最终采用思考融合的SFT-RL混合范式，系统性地增强模型能力。第三，架构进步从密集LLM扩展到高效的稀疏专家混合(MoE)设计。凭借这些贡献，SAIL-VL2在106个数据集上展示了具有竞争力的性能，并在MMMU和MathVista等具有挑战性的推理基准测试上取得了最先进的结果。此外，在OpenCompass排行榜上，SAIL-VL2-2B在40亿参数规模以下的官方发布开源模型中排名第一，同时作为开源多模态社区的高效且可扩展的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)for comprehensive multimodal understanding and reasoning. As the successor toSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8Bparameter scales across diverse image and video benchmarks, demonstratingstrong capabilities from fine-grained perception to complex reasoning. Threecore innovations drive its effectiveness. First, a large-scale data curationpipeline with scoring and filtering strategies enhances both quality anddistribution across captioning, OCR, QA, and video data, improving trainingefficiency. Second, a progressive training framework begins with a powerfulpre-trained vision encoder (SAIL-ViT), advances through multimodalpre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm thatsystematically strengthens model capabilities. Third, architectural advancesextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.With these contributions, SAIL-VL2 demonstrates competitive performance across106 datasets and achieves state-of-the-art results on challenging reasoningbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompassleaderboard, SAIL-VL2-2B ranks first among officially released open-sourcemodels under the 4B parameter scale, while serving as an efficient andextensible foundation for the open-source multimodal community.</description>
      <author>example@mail.com (Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng)</author>
      <guid isPermaLink="false">2509.14033v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates</title>
      <link>http://arxiv.org/abs/2509.13906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TFMAdapter，一种轻量级实例级适配器，解决了时间序列基础模型(TSFMs)无法利用协变量的问题，通过两阶段方法在不微调的情况下增强TSFMs的协变量信息处理能力，在实验中实现了24-27%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)最近在单变量预测方面取得了最先进性能，仅基于过去值的简短历史就能在新时间序列上表现良好，表明大规模跨领域预训练可获取归纳偏置。然而，大多数TSFMs无法利用协变量——许多应用中准确预测所需的关键外生变量。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法使TSFMs能够利用协变量信息，同时保持其通用性和轻量化特性，无需进行模型微调。&lt;h4&gt;方法&lt;/h4&gt;提出TFMAdapter，一种轻量级、实例级别的适配器，在单次模型调用中操作有限历史，学习非参数级联来组合协变量和TSFM预测。采用两阶段方法：(1)使用简单回归模型生成伪预测；(2)训练高斯过程回归器利用伪预测、TSFM预测和协变量优化预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的广泛实验表明，TFMAdapter始终优于基础模型和监督基线，相比基础基础模型实现了24-27%的性能提升，且数据计算开销最小。&lt;h4&gt;结论&lt;/h4&gt;轻量级适配器具有弥合通用基础模型和领域特定预测需求之间差距的潜力，为时间序列预测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)最近仅通过基于过去值的简短历史就在单变量预测方面取得了最先进性能。它们的成功表明，大规模跨领域预训练可以获取归纳偏置，从而从简短历史中的时间模式泛化。然而，由于领域特定性质和相关归纳偏置的缺乏，大多数TSFMs无法利用协变量——许多应用中准确预测所需的关键外生变量。我们提出TFMAdapter，一种轻量级、实例级别的适配器，在不进行微调的情况下增强TSFMs的协变量信息。TFMAdapter在单次模型调用中操作有限历史，学习非参数级联来组合协变量和单变量TSFM预测。然而，这种学习需要在历史中的所有步骤都有单变量预测，导致需要多次调用TSFM。为了在限制TSFM调用的同时在完整历史上下文上进行训练，TFMAdapter使用两阶段方法：(1)使用简单回归模型生成伪预测；(2)训练高斯过程回归器使用伪预测、TSFM预测和协变量来优化预测。在真实世界数据集上的广泛实验表明，TFMAdapter始终优于基础模型和监督基线，以最小的数据和计算开销相比基础基础模型实现了24-27%的改进。我们的结果突显了轻量级适配器弥合通用基础模型和领域特定预测需求之间差距的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761272&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time Series Foundation Models (TSFMs) have recently achieved state-of-the-artperformance in univariate forecasting on new time series simply by conditionedon a brief history of past values. Their success demonstrates that large-scalepretraining across diverse domains can acquire the inductive bias to generalizefrom temporal patterns in a brief history. However, most TSFMs are unable toleverage covariates -- future-available exogenous variables critical foraccurate forecasting in many applications -- due to their domain-specificnature and the lack of associated inductive bias. We propose TFMAdapter, alightweight, instance-level adapter that augments TSFMs with covariateinformation without fine-tuning. Instead of retraining, TFMAdapter operates onthe limited history provided during a single model call, learning anon-parametric cascade that combines covariates with univariate TSFM forecasts.However, such learning would require univariate forecasts at all steps in thehistory, requiring too many calls to the TSFM. To enable training on the fullhistorical context while limiting TSFM invocations, TFMAdapter uses a two-stagemethod: (1) generating pseudo-forecasts with a simple regression model, and (2)training a Gaussian Process regressor to refine predictions using both pseudo-and TSFM forecasts alongside covariates. Extensive experiments on real-worlddatasets demonstrate that TFMAdapter consistently outperforms both foundationmodels and supervised baselines, achieving a 24-27\% improvement over basefoundation models with minimal data and computational overhead. Our resultshighlight the potential of lightweight adapters to bridge the gap betweengeneric foundation models and domain-specific forecasting needs.</description>
      <author>example@mail.com (Afrin Dange, Sunita Sarawagi)</author>
      <guid isPermaLink="false">2509.13906v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.14151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE TCSVT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新的几何感知教师-学生框架BEVUDA++，用于解决BEV感知中的领域适应挑战，通过融合深度感知信息和多空间特征映射，有效减少了跨领域场景中的性能下降。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的鸟瞰图(BEV)感知在自动驾驶领域具有巨大潜力，但近期研究忽视了领域迁移问题，导致模型在跨领域场景中性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;解决BEV感知中多视图3D目标检测的领域适应(DA)挑战，减少多几何空间中的领域迁移累积问题。&lt;h4&gt;方法&lt;/h4&gt;提出BEVUDA++框架，包含可靠深度教师(RDT)和几何一致学生(GCS)模型。RDT融合目标LiDAR和深度预测生成深度感知信息；GCS将多空间特征映射到统一几何嵌入空间；引入不确定性引导的指数移动平均(UEMA)减少误差累积。&lt;h4&gt;主要发现&lt;/h4&gt;在四个跨领域场景的实验中，该方法在BEV 3D目标检测任务中取得了最先进性能，如在昼夜适应任务上NDS提升12.9%，mAP提升9.5%。&lt;h4&gt;结论&lt;/h4&gt;BEVUDA++框架有效解决了BEV感知中的领域适应挑战，显著提升了模型在跨领域场景中的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的鸟瞰图(BEV)感知在自动驾驶领域具有巨大潜力。近期研究优先考虑效率或准确性提升，但忽视了领域迁移问题，导致在迁移过程中性能显著下降。我们确定了真实世界跨领域场景中的主要领域差距，并首次尝试解决BEV感知中多视图3D目标检测的领域适应(DA)挑战。鉴于BEV感知方法的复杂性及其多组件特性，多几何空间(如2D、3D Voxel、BEV)中的领域迁移累积对BEV领域适应构成了重大挑战。本文提出了一种创新的几何感知教师-学生框架BEVUDA++，以减轻这一问题，包含可靠深度教师(RDT)和几何一致学生(GCS)模型。具体而言，RDT有效融合目标LiDAR和可靠的深度预测，基于不确定性估计生成深度感知信息，增强了对目标域理解至关重要的Voxel和BEV特征的提取。为协同减少领域迁移，GCS将多空间特征映射到统一的几何嵌入空间，从而缩小两领域间的数据分布差距。此外，我们引入了一种新颖的不确定性引导指数移动平均(UEMA)，进一步减少由领域迁移导致的误差累积，基于先前获得的不确定性指导。为证明我们提出方法的优越性，我们在四个跨领域场景中进行了全面实验，在BEV 3D目标检测任务中取得了最先进性能，例如在昼夜适应任务上NDS提升12.9%，mAP提升9.5%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域中基于鸟瞰图(BEV)感知的多视角3D目标检测存在的领域自适应(DA)挑战。具体来说，就是解决模型在跨领域场景(如不同场景、天气、昼夜变化)下因数据分布差异导致的性能大幅下降问题。这个问题在现实中非常重要，因为自动驾驶系统需要在各种真实世界条件下可靠工作，而标记目标领域的数据在实际应用中往往不切实际。现有BEV感知方法主要关注提高效率或准确性，忽视了领域自适应问题，导致在实际部署中性能不佳，解决这一问题可以显著提高自动驾驶系统在多样化环境下的鲁棒性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到BEV感知方法在跨领域场景下存在显著的性能下降，特别是在LSS(基于空间变换的)方法中，由于多几何空间(2D图像、3D体素、BEV空间)的复杂性，领域偏移误差会累积。作者借鉴了教师-学生框架(mean teacher)的思想，因为教师预测通常比标准模型具有更高的质量。他们设计了一个几何感知的教师-学生框架BEVUDA++，包括一个可靠的深度教师(RDT)和一个几何一致的学生(GCS)模型，并引入不确定性引导的指数移动平均(UEMA)策略。该方法借鉴了现有工作如BEVDepth、MC Dropout不确定性估计、STM3D的自训练策略以及MTTrans和DANN的多空间特征对齐和领域对抗训练等技术，但针对BEV感知的特点进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过处理多几何空间中的领域偏移累积问题，利用深度感知信息和几何一致性来减少不同领域间的数据分布差距。整体实现流程包括：1)可靠深度教师(RDT)融合目标LiDAR数据和可靠的深度预测，基于不确定性估计生成深度感知信息，使用MC Dropout方法过滤可靠深度预测；2)几何一致学生(GCS)将多个空间特征映射到统一几何嵌入空间，通过MLP转换特征并使用对齐损失和知识转移损失减少领域差距；3)不确定性引导的指数移动平均(UEMA)动态更新教师模型参数，根据不确定性调整更新速率；4)结合检测损失、监督损失、知识转移损失和对齐损失进行训练优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)几何感知的教师-学生框架BEVUDA++，专门针对BEV感知的领域自适应问题；2)可靠深度教师(RDT)，融合LiDAR和可靠深度预测，使用不确定性估计减少噪声；3)几何一致学生(GCS)，将多几何空间特征投影到统一嵌入空间；4)不确定性引导的指数移动平均(UEMA)，动态调整教师模型更新策略。相比之前的工作，本文专注于BEV感知而非2D或单目3D检测，同时处理多个几何空间的领域偏移，使用不确定性估计而非简单置信度评分指导深度预测，并设计了动态EMA策略而非静态更新策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种几何感知的教师-学生框架BEVUDA++，通过可靠的深度估计和几何特征对齐有效解决了BEV感知中多几何空间的领域偏移累积问题，显著提升了自动驾驶系统在跨领域场景下的3D目标检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-centric Bird's Eye View (BEV) perception holds considerable promisefor autonomous driving. Recent studies have prioritized efficiency or accuracyenhancements, yet the issue of domain shift has been overlooked, leading tosubstantial performance degradation upon transfer. We identify major domaingaps in real-world cross-domain scenarios and initiate the first effort toaddress the Domain Adaptation (DA) challenge in multi-view 3D object detectionfor BEV perception. Given the complexity of BEV perception approaches withtheir multiple components, domain shift accumulation across multi-geometricspaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domainadaptation. In this paper, we introduce an innovative geometric-awareteacher-student framework, BEVUDA++, to diminish this issue, comprising aReliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.Specifically, RDT effectively blends target LiDAR with dependable depthpredictions to generate depth-aware information based on uncertaintyestimation, enhancing the extraction of Voxel and BEV features that areessential for understanding the target domain. To collaboratively reduce thedomain shift, GCS maps features from multiple spaces into a unified geometricembedding space, thereby narrowing the gap in data distribution between the twodomains. Additionally, we introduce a novel Uncertainty-guided ExponentialMoving Average (UEMA) to further reduce error accumulation due to domain shiftsinformed by previously obtained uncertainty guidance. To demonstrate thesuperiority of our proposed method, we execute comprehensive experiments infour cross-domain scenarios, securing state-of-the-art performance in BEV 3Dobject detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Nightadaptation.</description>
      <author>example@mail.com (Rongyu Zhang, Jiaming Liu, Xiaoqi Li, Xiaowei Chi, Dan Wang, Li Du, Yuan Du, Shanghang Zhang)</author>
      <guid isPermaLink="false">2509.14151v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Paediatric Sleep Apnoea Detection Using Physiology-Guided Acoustic Models</title>
      <link>http://arxiv.org/abs/2509.15008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种迁移学习框架，将预训练在成人睡眠数据上的声学模型适应到儿科阻塞性睡眠呼吸暂停(OSA)检测中，通过整合SpO2去饱和模式增强模型训练，显著提高了儿童OSA检测的准确性。&lt;h4&gt;背景&lt;/h4&gt;儿童阻塞性睡眠呼吸暂停在临床上具有重要意义但难以诊断，因为儿童对基于传感器的多导睡眠图耐受性差。声学监测为家庭OSA筛查提供了非侵入性替代方案，但儿科数据有限，阻碍了强大的深度学习方法的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一个迁移学习框架，将预训练在成人睡眠数据上的声学模型适应到儿科OSA检测中，并通过整合基于SpO2的去饱和模式来增强模型训练。&lt;h4&gt;方法&lt;/h4&gt;使用大型成人睡眠数据集(157个夜晚)和小型儿科数据集(15个夜晚)进行系统评估，比较单任务与多任务学习、编码器冻结与完整微调，以及延迟SpO2标签的影响，以更好地与声学对齐并捕获生理上有意义的特征。&lt;h4&gt;主要发现&lt;/h4&gt;使用SpO2集成的微调相比没有适应的基线模型，持续改善了儿科OSA检测效果。&lt;h4&gt;结论&lt;/h4&gt;证明了迁移学习用于儿童家庭OSA筛查的可行性，展示了其在早期诊断中的潜在临床价值。&lt;h4&gt;翻译&lt;/h4&gt;儿科阻塞性睡眠呼吸暂停在临床上具有重要意义但难以诊断，因为儿童对基于传感器的多导睡眠图耐受性差。声学监测为家庭OSA筛查提供了非侵入性替代方案，但有限的儿科数据阻碍了强大的深度学习方法的发展。本文提出了一种迁移学习框架，将预训练在成人睡眠数据上的声学模型适应到儿科OSA检测中，整合基于SpO2的去饱和模式以增强模型训练。使用大型成人睡眠数据集(157个夜晚)和小型儿科数据集(15个夜晚)，我们系统评估了(i)单任务与多任务学习，(ii)编码器冻结与完整微调，以及(iii)延迟SpO2标签的影响，以更好地与声学对齐并捕获生理上有意义的特征。结果表明，使用SpO2集成的微调相比没有适应的基线模型，持续改善了儿科OSA检测。这些发现证明了迁移学习用于儿童家庭OSA筛查的可行性，并展示了其在早期诊断中的潜在临床价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Paediatric obstructive sleep apnoea (OSA) is clinically significant yetdifficult to diagnose, as children poorly tolerate sensor-basedpolysomnography. Acoustic monitoring provides a non-invasive alternative forhome-based OSA screening, but limited paediatric data hinders the developmentof robust deep learning approaches. This paper proposes a transfer learningframework that adapts acoustic models pretrained on adult sleep data topaediatric OSA detection, incorporating SpO2-based desaturation patterns toenhance model training. Using a large adult sleep dataset (157 nights) and asmaller paediatric dataset (15 nights), we systematically evaluate (i) single-versus multi-task learning, (ii) encoder freezing versus full fine-tuning, and(iii) the impact of delaying SpO2 labels to better align them with theacoustics and capture physiologically meaningful features. Results show thatfine-tuning with SpO2 integration consistently improves paediatric OSAdetection compared with baseline models without adaptation. These findingsdemonstrate the feasibility of transfer learning for home-based OSA screeningin children and offer its potential clinical value for early diagnosis.</description>
      <author>example@mail.com (Chaoyue Niu, Veronica Rowe, Guy J. Brown, Heather Elphick, Heather Kenyon, Lowri Thomas, Sam Johnson, Ning Ma)</author>
      <guid isPermaLink="false">2509.15008v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Learning Informed Prior Distributions with Normalizing Flows for Bayesian Analysis</title>
      <link>http://arxiv.org/abs/2509.14911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究归一化流(NF)模型作为贝叶斯推理中灵活先验的应用，结合MCMC采样，通过在先前分析的后验上训练，在后续任务中作为信息性先验，捕捉非平凡分布和相关性。&lt;h4&gt;背景&lt;/h4&gt;在贝叶斯推理中，需要有效的先验分布来表示参数的不确定性，传统的简单先验可能无法捕捉复杂的参数关系和分布特征。&lt;h4&gt;目的&lt;/h4&gt;探索归一化流模型作为贝叶斯推理中灵活先验的可行性，评估其在顺序贝叶斯分析中的效果，并比较不同的训练策略和采样算法。&lt;h4&gt;方法&lt;/h4&gt;使用归一化流模型作为先验，结合MCMC采样方法，比较不同的训练策略(如基于KL散度和无监督学习)和损失函数，并比较pocoMCMC与标准emcee采样器的性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于KL散度和无监督学习的训练策略能最准确地重现参考分布；在目标分布为单峰的情况下，基于NF的先验的MCMC能很好地重现一次性联合推理结果；但在多峰性或数据张力情况下可能会出现失真；先进的采样算法对探索后验空间至关重要。&lt;h4&gt;结论&lt;/h4&gt;基于NF的先验在高维参数空间中顺序贝叶斯推理是一种实用且高效的工具，但在多阶段贝叶斯推理中需要谨慎，特别是在处理多峰分布或数据张力时。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了归一化流(NF)模型作为贝叶斯推理中灵活先验的使用，与马尔可夫链蒙特卡洛(MCMC)采样结合。这些模型在先前分析的后验上进行训练，可以在后续推理任务中用作信息性先验，捕捉非平凡分布和相关性。我们比较了不同的训练策略和损失函数，发现基于KL散度和无监督学习的训练能最准确地重现参考分布。在顺序贝叶斯工作流中应用时，基于NF的先验的MCMC能很好地重现一次性联合推理的结果，前提是目标分布是单峰的。在明显的多峰性或数据张力情况下，可能会出现失真，这强调了在多阶段贝叶斯推理中需要谨慎。pocoMCMC采样器与标准emcee采样器的进一步比较表明，先进且稳健的算法对于探索后验空间的重要性。总体而言，我们的研究结果表明，基于NF的先验在高维参数空间中顺序贝叶斯推理是一种实用且高效的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate the use of normalizing flow (NF) models as flexible priors inBayesian inference with Markov Chain Monte Carlo (MCMC) sampling. Trained onposteriors from previous analyses, these models can be used as informativepriors, capturing non-trivial distributions and correlations, in subsequentinference tasks. We compare different training strategies and loss functions,finding that training based on Kullback-Leibler (KL) divergence andunsupervised learning consistently yield the most accurate reproductions ofreference distributions. Applied in sequential Bayesian workflows, MCMC withthe NF-based priors reproduces the results of one-shot joint inferences well,provided the target distributions are unimodal. In cases with pronouncedmulti-modality or dataset tension, distortions may arise, underscoring the needfor caution in multi-stage Bayesian inference. A comparison between the pocoMCMCMC sampler and the standard emcee sampler further demonstrates the importanceof advanced and robust algorithms for exploring the posterior space. Overall,our results establish NF-based priors as a practical and efficient tool forsequential Bayesian inference in high-dimensional parameter spaces.</description>
      <author>example@mail.com (Hendrik Roch, Chun Shen)</author>
      <guid isPermaLink="false">2509.14911v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>HAM: Hierarchical Adapter Merging for Scalable Continual Learning</title>
      <link>http://arxiv.org/abs/2509.13211v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了HAM（分层适配器合并）框架，通过动态合并不同任务的适配器来解决持续学习中的灾难性遗忘问题。HAM使用层次化的方法组织适配器，通过修剪、缩放和合并相关任务的适配器来促进迁移学习，实验证明其在多个视觉基准测试上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;持续学习是人类认知的基本能力，但对当前深度学习模型构成重大挑战。主要问题是新知识会干扰已学习信息，导致模型遗忘旧知识（灾难性遗忘）。大型预训练模型可以部分缓解遗忘，但在面对新数据分布时仍有困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效扩展到动态学习场景和长任务序列的持续学习方法，管理比基线方法更多的任务，同时提高效率。&lt;h4&gt;方法&lt;/h4&gt;引入HAM框架，在训练过程中动态合并来自不同任务的适配器。维护一组固定的组层次化整合新适配器，对每个任务训练低秩适配器和重要性标量，基于适配器相似度动态分组任务，并在每个组内修剪、缩放和合并适配器以促进迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个视觉基准测试上的广泛实验表明，HAM显著优于最先进的方法，随着任务数量的增加，HAM的优势更加明显。&lt;h4&gt;结论&lt;/h4&gt;HAM有效地解决了持续学习中的灾难性遗忘问题，能够处理比竞争基线更多的任务，同时提高效率。&lt;h4&gt;翻译&lt;/h4&gt;持续学习是人类认知的基本能力，然而它对当前的深度学习模型构成了重大挑战。主要问题是新知识可能会干扰已学习的信息，导致模型为了新知识而遗忘旧知识，这种现象被称为灾难性遗忘。尽管大型预训练模型可以通过利用其现有知识和过参数化部分缓解遗忘，但当面对新的数据分布时，它们仍然存在困难。参数高效微调（PEFT）方法（如LoRA）能够有效地适应新知识。然而，在扩展到动态学习场景和长任务序列时，它们仍然面临挑战，因为每个任务维护一个适配器会增加复杂性并增加干扰的可能性。在本文中，我们引入了分层适配器合并（HAM），一种在训练过程中动态合并来自不同任务的适配器的新框架。这种方法使HAM能够有效地扩展，允许它以更高的效率管理比竞争基线更多的任务。为此，HAM维护一组固定的组，这些组层次化地整合新的适配器。对于每个任务，HAM训练一个低秩适配器和一个重要性标量，然后基于适配器相似度动态分组任务。在每个组内，适配器被修剪、缩放和合并，促进相关任务之间的迁移学习。在三个视觉基准测试上的广泛实验表明，HAM显著优于最先进的方法，特别是在任务数量增加的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning is an essential capability of human cognition, yet itposes significant challenges for current deep learning models. The primaryissue is that new knowledge can interfere with previously learned information,causing the model to forget earlier knowledge in favor of the new, a phenomenonknown as catastrophic forgetting. Although large pre-trained models canpartially mitigate forgetting by leveraging their existing knowledge andover-parameterization, they often struggle when confronted with novel datadistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,enable efficient adaptation to new knowledge. However, they still facechallenges in scaling to dynamic learning scenarios and long sequences oftasks, as maintaining one adapter per task introduces complexity and increasesthe potential for interference. In this paper, we introduce HierarchicalAdapters Merging (HAM), a novel framework that dynamically combines adaptersfrom different tasks during training. This approach enables HAM to scaleeffectively, allowing it to manage more tasks than competing baselines withimproved efficiency. To achieve this, HAM maintains a fixed set of groups thathierarchically consolidate new adapters. For each task, HAM trains a low-rankadapter along with an importance scalar, then dynamically groups tasks based onadapter similarity. Within each group, adapters are pruned, scaled and merge,facilitating transfer learning between related tasks. Extensive experiments onthree vision benchmarks show that HAM significantly outperformsstate-of-the-art methods, particularly as the number of tasks increases.</description>
      <author>example@mail.com (Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2509.13211v3</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems</title>
      <link>http://arxiv.org/abs/2509.15213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;扩展现实(XR)应用越来越多地集成大型语言模型(LLMs)来增强用户体验、场景理解，甚至生成可执行的XR内容，这些应用通常被称为'AI眼镜'。尽管有这些潜在的好处，集成的XR-LLM管道使XR应用容易受到新型攻击。&lt;h4&gt;目的&lt;/h4&gt;分析文献和实践中的LLM集成XR系统，从系统角度沿不同维度对其进行分类，识别通用威胁模型，并展示一系列概念验证攻击。&lt;h4&gt;方法&lt;/h4&gt;对文献和实践中的LLM集成XR系统进行分析和分类，在多个XR平台上实施概念验证攻击，这些平台使用各种LLM模型（包括Meta Quest 3、Meta Ray-Ban、Android和运行Llama和GPT模型的Microsoft HoloLens 2）。&lt;h4&gt;主要发现&lt;/h4&gt;尽管这些平台以不同方式实现LLM集成，但它们共享漏洞，攻击者可以修改围绕合法LLM查询的公共上下文，导致用户收到错误的视觉或听觉反馈，这可能危及用户安全或隐私，造成混乱或其他有害影响。&lt;h4&gt;结论&lt;/h4&gt;讨论了防御策略和开发人员的最佳实践，提出了一个初始防御原型，并呼吁社区开发新的保护机制来减轻这些风险。&lt;h4&gt;翻译&lt;/h4&gt;扩展现实(XR)应用越来越多地集成大型语言模型(LLMs)来增强用户体验、场景理解，甚至生成可执行的XR内容，这些应用通常被称为'AI眼镜'。尽管有这些潜在的好处，集成的XR-LLM管道使XR应用容易受到新型攻击。在本文中，我们从系统角度分析了文献和实践中的LLM集成XR系统，并沿不同维度对其进行分类。基于这种分类，我们识别出一个通用威胁模型，并在多个采用各种LLM模型的XR平台上展示了一系列概念验证攻击（包括Meta Quest 3、Meta Ray-Ban、Android和运行Llama和GPT模型的Microsoft HoloLens 2）。尽管这些平台各自以不同方式实现LLM集成，但它们共享漏洞，攻击者可以修改合法LLM查询周围的公共上下文，导致用户收到错误的视觉或听觉反馈，从而危及他们的安全或隐私，制造混乱或其他有害影响。为了防御这些威胁，我们讨论了防御策略和开发人员的最佳实践，包括一个初始防御原型，并呼吁社区开发新的保护机制来减轻这些风险。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extended reality (XR) applications increasingly integrate Large LanguageModels (LLMs) to enhance user experience, scene understanding, and evengenerate executable XR content, and are often called "AI glasses". Despitethese potential benefits, the integrated XR-LLM pipeline makes XR applicationsvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XRsystems in the literature and in practice and categorize them along differentdimensions from a systems perspective. Building on this categorization, weidentify a common threat model and demonstrate a series of proof-of-conceptattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).Although these platforms each implement LLM integration differently, they sharevulnerabilities where an attacker can modify the public context surrounding alegitimate LLM query, resulting in erroneous visual or auditory feedback tousers, thus compromising their safety or privacy, sowing confusion, or otherharmful effects. To defend against these threats, we discuss mitigationstrategies and best practices for developers, including an initial defenseprototype, and call on the community to develop new protection mechanisms tomitigate these risks.</description>
      <author>example@mail.com (Yicheng Zhang, Zijian Huang, Sophie Chen, Erfan Shayegani, Jiasi Chen, Nael Abu-Ghazaleh)</author>
      <guid isPermaLink="false">2509.15213v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>SPATIALGEN: Layout-guided 3D Indoor Scene Generation</title>
      <link>http://arxiv.org/abs/2509.14981v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3D scene ggeneration; diffusion model; Scene reconstruction and  understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过创建大型合成数据集并提出SpatialGen模型，解决了室内3D场景生成中的关键挑战，实现了高质量、语义一致的3D室内场景生成，结果优于先前方法，并已开源数据和模型以促进社区发展。&lt;h4&gt;背景&lt;/h4&gt;创建高保真室内环境3D模型对设计、虚拟现实和机器人应用至关重要，但手动3D建模耗时且劳动密集。现有生成式AI方法在平衡视觉质量、多样性、语义一致性和用户控制方面面临挑战，主要瓶颈是缺乏针对此任务的大规模高质量数据集。&lt;h4&gt;目的&lt;/h4&gt;引入一个全面的合成数据集以解决缺乏大规模高质量数据集的问题，并提出SpatialGen模型用于生成真实且语义一致的3D室内场景。&lt;h4&gt;方法&lt;/h4&gt;创建包含12,328个结构化注释场景、57,440个房间和470万张逼真2D渲染的合成数据集；提出SpatialGen多视图多模态扩散模型，基于3D布局和参考图像从任意视点合成外观、几何和语义信息，同时保持跨模态的空间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;SpatialGen在实验中始终比以前的方法产生更好的结果，能够生成高质量、语义一致的3D室内场景。&lt;h4&gt;结论&lt;/h4&gt;SpatialGen模型有效解决了室内3D场景生成中的关键挑战，开源数据和模型将有助于推动室内场景理解和生成领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;创建高保真室内环境的3D模型对于设计、虚拟现实和机器人应用至关重要。然而，手动3D建模仍然耗时且劳动密集。尽管生成式AI的最新进展已实现自动化场景合成，但现有方法在平衡视觉质量、多样性和用户控制方面常面临挑战。主要瓶颈是缺乏针对此任务的大规模高质量数据集。为解决这一差距，我们引入了一个全面的合成数据集，包含12,328个结构化注释场景、57,440个房间和470万张逼真2D渲染。利用此数据集，我们提出了SpatialGen，一种新颖的多视图多模态扩散模型，可生成真实且语义一致的3D室内场景。给定3D布局和参考图像（从文本提示派生），我们的模型从任意视点合成外观（彩色图像）、几何（场景坐标图）和语义（语义分割图），同时保持跨模态的空间一致性。在我们的实验中，SpatialGen始终比以前的方法产生更好的结果。我们开源了数据和模型，以赋能社区并推动室内场景理解和生成领域的发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决生成高质量、高保真度的3D室内场景模型的问题，特别是在平衡视觉质量、多样性、语义一致性和用户控制方面的挑战。这个问题在现实中非常重要，因为室内3D模型对室内设计、虚拟现实和机器人应用至关重要，而手动3D建模既耗时又费力。现有生成AI方法难以同时满足这些要求，主要瓶颈是缺乏大规模、高质量的数据集。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考：分数蒸馏方法利用2D预训练模型创建3D内容但存在视觉伪影，全景作为代理的方法无法外推到新视角。作者借鉴了多视角扩散模型、3D语义布局引导、ControlNet和3D高斯溅射等现有技术，但创新性地将它们结合，并创建了一个大规模数据集来解决数据稀缺问题。设计思路是通过布局条件引导生成过程，确保跨视角和跨模态的一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D语义布局作为先验条件，通过多视角多模态扩散模型同时生成RGB图像、场景坐标图和语义分割图，确保多模态一致性。整体流程包括：1)将3D布局转换为视图特定表示；2)使用布局引导的交替注意力机制(跨视图和跨模态)；3)采用迭代密集视角生成策略；4)最后通过3D高斯溅射优化重建显式辐射场，实现自由视角渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的大规模室内场景数据集(12,328个场景，57,440个房间，470万张渲染图)；2)SPATIALGEN框架，结合布局引导的多视角多模态扩散模型；3)场景坐标图VAE(SCM-VAE)专门处理几何信息；4)迭代密集视角生成策略。相比之前工作，SPATIALGEN解决了视觉质量、多样性、语义一致性和用户控制的平衡问题，能生成任意视角的图像而非仅固定位置全景，同时确保多模态一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPATIALGEN通过引入大规模室内场景数据集和基于布局引导的多视角多模态扩散模型，实现了高质量、语义一致的3D室内场景生成，显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Creating high-fidelity 3D models of indoor environments is essential forapplications in design, virtual reality, and robotics. However, manual 3Dmodeling remains time-consuming and labor-intensive. While recent advances ingenerative AI have enabled automated scene synthesis, existing methods oftenface challenges in balancing visual quality, diversity, semantic consistency,and user control. A major bottleneck is the lack of a large-scale, high-qualitydataset tailored to this task. To address this gap, we introduce acomprehensive synthetic dataset, featuring 12,328 structured annotated sceneswith 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging thisdataset, we present SpatialGen, a novel multi-view multi-modal diffusion modelthat generates realistic and semantically consistent 3D indoor scenes. Given a3D layout and a reference image (derived from a text prompt), our modelsynthesizes appearance (color image), geometry (scene coordinate map), andsemantic (semantic segmentation map) from arbitrary viewpoints, whilepreserving spatial consistency across modalities. SpatialGen consistentlygenerates superior results to previous methods in our experiments. We areopen-sourcing our data and models to empower the community and advance thefield of indoor scene understanding and generation.</description>
      <author>example@mail.com (Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan)</author>
      <guid isPermaLink="false">2509.14981v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Human Interaction for Collaborative Semantic SLAM using Extended Reality</title>
      <link>http://arxiv.org/abs/2509.14949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HICS-SLAM是一种人机协同的语义SLAM框架，通过共享扩展现实环境实现实时协作，让人类操作员能够直接与机器人的3D场景图交互并添加高级语义概念，显著提升了地图构建的准确性、精度和语义完整性。&lt;h4&gt;背景&lt;/h4&gt;语义SLAM系统通过为机器人地图添加结构和语义信息，使机器人能够在复杂环境中更有效地工作。然而，这些系统在处理遮挡、数据不完整或几何结构模糊等真实场景时存在困难，因为它们无法充分利用人类自然应用的高级空间和语义知识。&lt;h4&gt;目的&lt;/h4&gt;引入HICS-SLAM，一种人机协同的语义SLAM框架，利用共享的扩展现实环境实现实时协作，以克服传统语义SLAM系统的局限性。&lt;h4&gt;方法&lt;/h4&gt;该系统允许人类操作员直接与机器人的3D场景图进行交互和可视化，并在地图构建过程中添加高级语义概念（如房间或结构实体）。同时，提出了一种基于图的语义融合方法，将这些人类干预与机器人感知相结合，实现可扩展的协作以增强态势感知能力。&lt;h4&gt;主要发现&lt;/h4&gt;在真实建筑工地数据集上的实验评估表明，与自动化基线相比，该方法在房间检测准确性、地图精度和语义完整性方面都有显著改进。&lt;h4&gt;结论&lt;/h4&gt;HICS-SLAM方法的有效性及其未来扩展的潜力得到了验证，为人机协作语义地图构建提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;语义SLAM（同时定位与地图构建）系统通过为机器人地图添加结构和语义信息，使机器人能够在复杂环境中更有效地工作。然而，这些系统在处理遮挡、数据不完整或几何结构模糊等真实场景时存在困难，因为它们无法充分利用人类自然应用的高级空间和语义知识。我们引入HICS-SLAM，一种人机协同的语义SLAM框架，利用共享的扩展现实环境实现实时协作。该系统允许人类操作员直接与机器人的3D场景图进行交互和可视化，并在地图构建过程中添加高级语义概念（如房间或结构实体）。我们提出了一种基于图的语义融合方法，将这些人类干预与机器人感知相结合，实现可扩展的协作以增强态势感知能力。在真实建筑工地数据集上的实验评估表明，与自动化基线相比，该方法在房间检测准确性、地图精度和语义完整性方面都有所改进，证明了该方法的有效性及其未来扩展的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决语义SLAM系统在现实场景中（如遮挡、数据不完整或几何模糊环境）表现不佳的问题，因为这些系统无法充分利用人类自然应用的高级空间和语义知识。这个问题在现实世界中非常重要，因为它影响着机器人搜索救援、工业装配和建筑工地等需要精确空间理解的应用场景，人类的空间认知能力可以弥补机器人感知系统的局限性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有Human-in-the-Loop策略的局限性，如HAC-SLAM缺乏语义集成，HSS-SLAM需要手动参数调整且对非专家不直观。他们设计了一个结合扩展现实(XR)接口与语义SLAM的框架，允许人类通过自然手势直接操作3D场景图。该方法借鉴了S-Graphs 2.0作为底层SLAM算法，参考了HAC-SLAM的轨迹修正方法和HSS-SLAM的语义表示方法，并利用了XR技术中的手势识别研究。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过共享的扩展现实环境实现人机协作的语义SLAM，让人类操作员直接与机器人的3D场景图交互并添加高级语义概念。整体流程为：机器人在物理环境中收集传感器数据；通过语义SLAM算法处理数据构建层次化场景图；人类通过混合现实设备查看交互式虚拟环境；人类使用手势添加或修正语义信息；这些干预通过图优化方法集成到SLAM后端；系统实时更新机器人的地图和定位信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 通过XR接口实现直观的人机干预和3D场景图操作；2) 在线集成人类语义知识到SLAM后端优化；3) 在真实世界建筑工地数据集上验证方法有效性。相比之前工作，HICS-SLAM与HAC-SLAM不同之处在于它将语义集成到场景图中；与HSS-SLAM不同在于它支持直观的高级语义编辑；与其他HitL方法不同在于它结合了直观的场景操作和语义意识，解决了现有方法要么在低级几何层面操作要么提供高级交互但不包含语义信息的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HICS-SLAM通过结合扩展现实接口与语义SLAM，实现了人类与机器人在实时环境映射中的协作，显著提高了地图的语义完整性和空间精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic SLAM (Simultaneous Localization and Mapping) systems enrich robotmaps with structural and semantic information, enabling robots to operate moreeffectively in complex environments. However, these systems struggle inreal-world scenarios with occlusions, incomplete data, or ambiguous geometries,as they cannot fully leverage the higher-level spatial and semantic knowledgehumans naturally apply. We introduce HICS-SLAM, a Human-in-the-Loop semanticSLAM framework that uses a shared extended reality environment for real-timecollaboration. The system allows human operators to directly interact with andvisualize the robot's 3D scene graph, and add high-level semantic concepts(e.g., rooms or structural entities) into the mapping process. We propose agraph-based semantic fusion methodology that integrates these humaninterventions with robot perception, enabling scalable collaboration forenhanced situational awareness. Experimental evaluations on real-worldconstruction site datasets demonstrate improvements in room detection accuracy,map precision, and semantic completeness compared to automated baselines,demonstrating both the effectiveness of the approach and its potential forfuture extensions.</description>
      <author>example@mail.com (Laura Ribeiro, Muhammad Shaheer, Miguel Fernandez-Cortizas, Ali Tourani, Holger Voos, Jose Luis Sanchez-Lopez)</author>
      <guid isPermaLink="false">2509.14949v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition</title>
      <link>http://arxiv.org/abs/2509.14619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为LSTC-MDA的统一框架，用于解决基于骨架的动作识别中的样本稀缺和时间依赖建模问题。&lt;h4&gt;背景&lt;/h4&gt;基于骨架的动作识别面临两个长期挑战：标记训练样本稀缺和难以建模短期和长期时间依赖关系。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一框架LSTC-MDA，同时改进时间建模和数据多样性，解决上述两个挑战。&lt;h4&gt;方法&lt;/h4&gt;1) 提出长短期时间卷积(LSTC)模块，具有并行的短期和长期分支；2) 使用学习的相似度权重对齐和融合这两个特征分支；3) 扩展联合混合数据增强(JMDA)，在输入级别添加加性Mixup；4) 将mixup操作限制在同一摄像头视图中，避免分布偏移。&lt;h4&gt;主要发现&lt;/h4&gt;消融研究确认每个组件都有贡献，LSTC-MDA达到了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;LSTC-MDA在多个数据集上取得了最先进的结果：NTU 60(X-Sub和X-View)上分别为94.1%和97.5%，NTU 120(X-Sub和X-Set)上分别为90.4%和92.0%，NW-UCLA上为97.2%。&lt;h4&gt;翻译&lt;/h4&gt;基于骨架的动作识别面临两个长期挑战：标记训练样本稀缺和难以建模短期和长期时间依赖关系。为解决这些问题，我们提出了一个统一框架LSTC-MDA，同时改进时间建模和数据多样性。我们引入了一种新颖的长短期时间卷积(LSTC)模块，具有并行的短期和长期分支，然后使用学习的相似度权重对这些特征分支进行自适应的对齐和融合，以保留传统步长为2的时间卷积所丢失的关键长期线索。我们还扩展了联合混合数据增强(JMDA)，在输入级别添加了加性Mixup，增加了训练样本的多样性，并将mixup操作限制在同一摄像头视图中，以避免分布偏移。消融研究确认每个组件都有贡献。LSTC-MDA取得了最先进的结果：在NTU 60(X-Sub和X-View)上分别为94.1%和97.5%，在NTU 120(X-Sub和X-Set)上分别为90.4%和92.0%，在NW-UCLA上为97.2%。代码：https://github.com/xiaobaoxia/LSTC-MDA。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skeleton-based action recognition faces two longstanding challenges: thescarcity of labeled training samples and difficulty modeling short- andlong-range temporal dependencies. To address these issues, we propose a unifiedframework, LSTC-MDA, which simultaneously improves temporal modeling and datadiversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC)module with parallel short- and long-term branches, these two feature branchesare then aligned and fused adaptively using learned similarity weights topreserve critical long-range cues lost by conventional stride-2 temporalconvolutions. We also extend Joint Mixing Data Augmentation (JMDA) with anAdditive Mixup at the input level, diversifying training samples andrestricting mixup operations to the same camera view to avoid distributionshifts. Ablation studies confirm each component contributes. LSTC-MDA achievesstate-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4%and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code:https://github.com/xiaobaoxia/LSTC-MDA.</description>
      <author>example@mail.com (Feng Ding, Haisheng Fu, Soroush Oraki, Jie Liang)</author>
      <guid isPermaLink="false">2509.14619v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Dense Video Understanding with Gated Residual Tokenization</title>
      <link>http://arxiv.org/abs/2509.14199v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Dense Video Understanding (DVU)方法和Gated Residual Tokenization (GRT)框架，通过减少标记时间和标记开销实现高FPS视频理解，并创建了首个针对密集时间推理的基准测试DIVE。&lt;h4&gt;背景&lt;/h4&gt;当前视频大语言模型和基准测试主要依赖低帧率采样，如均匀采样或关键帧选择，这会丢弃密集的时间信息。虽然这种折衷避免了标记每一帧的高成本，但无法处理如讲座理解等需要精确时间对齐的任务。&lt;h4&gt;目的&lt;/h4&gt;解决低帧率采样无法捕捉密集时间信息的问题，实现高效的高FPS视频理解，并创建相应的基准测试。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段框架GRT：1) 运动补偿门控标记化，使用像素级运动估计跳过静态区域；2) 语义场景内标记化合并，融合静态区域标记减少冗余。同时创建首个密集时间推理基准测试DIVE。&lt;h4&gt;主要发现&lt;/h4&gt;在DIVE基准测试上，GRT优于更大的VLLM基线模型，且性能随FPS正向扩展，证明了密集时间信息的重要性。&lt;h4&gt;结论&lt;/h4&gt;GRT框架能够实现高效、可扩展的高FPS视频理解，强调了密集时间信息对视频理解的关键作用。&lt;h4&gt;翻译&lt;/h4&gt;高时间分辨率对于捕捉视频理解中的细粒度细节至关重要。然而，当前视频大语言模型和基准测试主要依赖低帧率采样，如均匀采样或关键帧选择，这会丢弃密集的时间信息。这种折衷避免了标记每一帧的高成本，否则会导致冗余计算和随视频长度线性增长的标记数量。虽然这种折衷适用于缓慢变化的内容，但在讲座理解等任务中失败，因为这些任务中信息几乎出现在每一帧，需要精确的时间对齐。为解决这一差距，我们引入了Dense Video Understanding (DVU)，通过减少标记时间和标记开销来实现高FPS视频理解。现有基准测试也有限，因为其问答对侧重于粗粒度内容变化。因此，我们提出了DIVE (Dense Information Video Evaluation)，这是第一个为密集时间推理设计的基准测试。为了使DVU实用，我们提出了Gated Residual Tokenization (GRT)，一个两阶段框架：(1) 运动补偿门控标记化使用像素级运动估计在标记化过程中跳过静态区域，实现标记数量和计算量的次线性增长。(2) 语义场景内标记化合并融合场景内静态区域的标记，进一步减少冗余，同时保留动态语义。在DIVE上的实验表明，GRT优于更大的VLLM基线模型，且随FPS正向扩展。这些结果强调了密集时间信息的重要性，并证明了GRT能够实现高效、可扩展的高FPS视频理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High temporal resolution is essential for capturing fine-grained details invideo understanding. However, current video large language models (VLLMs) andbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling orkeyframe selection, discarding dense temporal information. This compromiseavoids the high cost of tokenizing every frame, which otherwise leads toredundant computation and linear token growth as video length increases. Whilethis trade-off works for slowly changing content, it fails for tasks likelecture comprehension, where information appears in nearly every frame andrequires precise temporal alignment. To address this gap, we introduce DenseVideo Understanding (DVU), which enables high-FPS video comprehension byreducing both tokenization time and token overhead. Existing benchmarks arealso limited, as their QA pairs focus on coarse content changes. We thereforepropose DIVE (Dense Information Video Evaluation), the first benchmark designedfor dense temporal reasoning. To make DVU practical, we present Gated ResidualTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-GatedTokenization uses pixel-level motion estimation to skip static regions duringtokenization, achieving sub-linear growth in token count and compute. (2)Semantic-Scene Intra-Tokenization Merging fuses tokens across static regionswithin a scene, further reducing redundancy while preserving dynamic semantics.Experiments on DIVE show that GRT outperforms larger VLLM baselines and scalespositively with FPS. These results highlight the importance of dense temporalinformation and demonstrate that GRT enables efficient, scalable high-FPS videounderstanding.</description>
      <author>example@mail.com (Haichao Zhang, Wenhao Chai, Shwai He, Ang Li, Yun Fu)</author>
      <guid isPermaLink="false">2509.14199v2</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>D4PM: A Dual-branch Driven Denoising Diffusion Probabilistic Model with Joint Posterior Diffusion Sampling for EEG Artifacts Removal</title>
      <link>http://arxiv.org/abs/2509.14302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为D4PM的双分支驱动的去噪扩散概率模型，用于统一处理多种伪影的脑电图信号去噪，解决了现有方法在时间建模和伪影差异处理方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;伪影去除对于准确分析和解释脑电图(EEG)信号至关重要。传统方法在伪影与EEG强相关或单通道数据时表现不佳。虽然基于扩散的生成模型最近在EEG去噪方面显示出潜力，但现有方法存在明显缺陷。&lt;h4&gt;目的&lt;/h4&gt;解决现有EEG去噪方法的两个主要限制：缺乏时间建模限制了可解释性，以及使用单伪影训练范式忽略了不同伪影之间的差异。&lt;h4&gt;方法&lt;/h4&gt;提出D4PM模型，采用双分支条件扩散架构隐式建模干净EEG和伪影的数据分布，并设计联合后验采样策略协同整合互补先验，实现高保真EEG重建。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集上的广泛实验表明，D4PM提供了优越的去噪效果，在EOG伪影去除方面取得了最新的最先进性能，优于所有公开可用的基线方法。&lt;h4&gt;结论&lt;/h4&gt;D4PM模型通过统一处理多种伪影类型并改进时间建模，显著提高了EEG信号去噪的质量和可解释性，为脑电图分析提供了更可靠的工具。&lt;h4&gt;翻译&lt;/h4&gt;伪影去除对于准确分析和解释脑电图(EEG)信号至关重要。传统方法在伪影与EEG强相关或单通道数据时表现不佳。最近基于扩散的生成模型的进展展示了EEG去噪的强大潜力，显著改善了细粒度噪声抑制并减少了过平滑。然而，现有方法面临两个主要限制：缺乏时间建模限制了可解释性，以及使用单伪影训练范式忽略了伪影间的差异。为解决这些问题，我们提出了D4PM，一种双分支驱动的去噪扩散概率模型，统一了多种伪影去除。我们引入了一种双分支条件扩散架构，隐式建模干净EEG和伪影的数据分布。还设计了一种联合后验采样策略，协同整合互补先验，实现高保真EEG重建。在两个公共数据集上的广泛实验表明，D4PM提供了优越的去噪效果。它在EOG伪影去除方面取得了最新的最先进性能，优于所有公开可用的基线方法。代码可在https://github.com/flysnow1024/D4PM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artifact removal is critical for accurate analysis and interpretation ofElectroencephalogram (EEG) signals. Traditional methods perform poorly withstrong artifact-EEG correlations or single-channel data. Recent advances indiffusion-based generative models have demonstrated strong potential for EEGdenoising, notably improving fine-grained noise suppression and reducingover-smoothing. However, existing methods face two main limitations: lack oftemporal modeling limits interpretability and the use of single-artifacttraining paradigms ignore inter-artifact differences. To address these issues,we propose D4PM, a dual-branch driven denoising diffusion probabilistic modelthat unifies multi-type artifact removal. We introduce a dual-branchconditional diffusion architecture to implicitly model the data distribution ofclean EEG and artifacts. A joint posterior sampling strategy is furtherdesigned to collaboratively integrate complementary priors for high-fidelityEEG reconstruction. Extensive experiments on two public datasets show that D4PMdelivers superior denoising. It achieves new state-of-the-art performance inEOG artifact removal, outperforming all publicly available baselines. The codeis available at https://github.com/flysnow1024/D4PM.</description>
      <author>example@mail.com (Feixue Shao, Xueyu Liu, Yongfei Wu, Jianbo Lu, Guiying Yan, Weihua Yang)</author>
      <guid isPermaLink="false">2509.14302v1</guid>
      <pubDate>Fri, 19 Sep 2025 15:14:57 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings</title>
      <link>http://arxiv.org/abs/2509.12938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种绕过可微分渲染的新方法，通过预分解的对象级高斯和多视图CLIP特征聚合来克服3D高斯溅射在场景理解方面的局限性，实现了准确的开放词汇对象检索和无缝任务适应。&lt;h4&gt;背景&lt;/h4&gt;3D高斯溅射在新型视图合成方面取得了显著进展，能够实现实时照片级真实感渲染，但其内在模糊性对3D场景理解提出了挑战，限制了在AR/VR和机器人技术中的应用。现有方法通过2D基础模型蒸馏学习语义存在根本性限制，alpha blending会使不同对象的语义平均化，导致无法实现3D级别的理解。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服3D高斯溅射场景理解限制的方法，实现准确的3D开放词汇对象提取和语义理解，扩展其在AR/VR和机器人技术中的应用范围。&lt;h4&gt;方法&lt;/h4&gt;提出范式转变的替代方案，完全绕过可微分渲染处理语义。利用预分解的对象级高斯，通过多视图CLIP特征聚合表示每个对象，创建全面的'嵌入袋'来全面描述对象。实现两种功能：1) 通过比较文本查询与对象级嵌入进行开放词汇对象检索；2) 将对象ID传播到像素用于2D分割或传播到高斯用于3D提取。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明该方法有效克服了3D开放词汇对象提取的挑战，同时在2D开放词汇分割方面与最先进性能相当，确保了最小程度的性能妥协。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过改变处理语义的方式，成功解决了3D高斯溅射在场景理解方面的局限性，为AR/VR和机器人技术等领域的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;新型视图合成在3D高斯溅射(3DGS)的推动下取得了显著进展，实现了实时照片级真实感渲染。然而，高斯溅射的内在模糊性对3D场景理解提出了挑战，限制了其在AR/VR和机器人技术中的更广泛应用。虽然最近的工作尝试通过2D基础模型蒸馏来学习语义，但它们继承了根本性限制：alpha blending使不同对象的语义平均化，导致无法实现3D级别的理解。我们提出了一种范式转变的替代方案，完全绕过可微分渲染来处理语义。我们的关键见解是利用预分解的对象级高斯，并通过多视图CLIP特征聚合来表示每个对象，创建全面的'嵌入袋'，全面描述对象。这实现了：(1) 通过将文本查询与对象级(而非高斯级)嵌入进行比较，实现准确的开放词汇对象检索，以及(2) 无缝任务适应：将对象ID传播到像素用于2D分割或传播到高斯用于3D提取。实验证明，我们的方法有效克服了3D开放词汇对象提取的挑战，同时在2D开放词汇分割方面与最先进性能相当，确保最小程度的妥协。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D高斯溅射(3DGS)在场景理解方面的局限性。虽然3DGS能实现实时照片级渲染，但其内在模糊性导致难以准确识别和分割场景中的个体对象，这限制了在AR/VR、机器人技术等需要精确场景理解的应用中的使用。这个问题很重要，因为当前3D渲染技术虽然视觉效果逼真，但缺乏对场景内容的语义理解能力，无法支持需要精确对象识别和操作的高级应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有方法(如LangSplat、LeGaussians等)的局限性出发思考，这些方法通过可微分渲染将2D基础模型语义注入3D高斯，但存在噪声和不一致问题。作者借鉴了Gaussian Grouping方法，该方法能将3D高斯分组为不同对象并分配身份编码。作者还利用了CLIP模型强大的视觉-语言关联能力。基于这些观察，作者设计了一种绕过可微分渲染的新方法，转而使用预分解的对象级高斯，并通过多视图CLIP特征聚合创建'嵌入包'来表示每个对象。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：不再通过可微分渲染将语义注入单个高斯，而是将3D高斯预先分组为不同对象，为每个对象创建'嵌入包'，在对象级别执行开放词汇检索，并将对象ID传播到下游任务。整体流程：1)使用SAM模型从多视图图像生成2D掩码并确保跨视图一致性；2)使用Gaussian Grouping将3D高斯分组为不同对象并分配唯一ID；3)为每个对象提取多视图CLIP特征并聚合形成'嵌入包'；4)根据文本查询计算对象相关性分数并识别最相关对象ID；5)将对象ID用于3D对象选择(过滤高斯)或2D分割(生成像素级掩码)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)对象级语义编码，将3D高斯场景表示为每个对象的多元CLIP嵌入'包'，避免有问题的语义可微分渲染；2)开放词汇3D搜索，通过预分组场景实现直接对象级检索，解决高斯级语义不一致问题；3)任务无关的语义传播，将对象ID无缝传播到2D分割和3D提取，无需重新训练。与之前工作的不同：1)完全绕过可微分渲染进行语义学习，避免alpha混合导致的语义稀释；2)在对象级别而非高斯级别操作，提供更清洁一致的表示；3)保留多视图嵌入细节而非简单平均，捕获特定视角的独特特征；4)同时处理2D和3D任务，提供更全面场景理解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出通过高斯分组和多视图CLIP嵌入包实现开放词汇3D场景理解，能准确进行3D对象提取和2D分割，同时克服了现有方法中的语义平均化和噪声问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Novel view synthesis has seen significant advancements with 3D GaussianSplatting (3DGS), enabling real-time photorealistic rendering. However, theinherent fuzziness of Gaussian Splatting presents challenges for 3D sceneunderstanding, restricting its broader applications in AR/VR and robotics.While recent works attempt to learn semantics via 2D foundation modeldistillation, they inherit fundamental limitations: alpha blending averagessemantics across objects, making 3D-level understanding impossible. We proposea paradigm-shifting alternative that bypasses differentiable rendering forsemantics entirely. Our key insight is to leverage predecomposed object-levelGaussians and represent each object through multiview CLIP feature aggregation,creating comprehensive "bags of embeddings" that holistically describe objects.This allows: (1) accurate open-vocabulary object retrieval by comparing textqueries to object-level (not Gaussian-level) embeddings, and (2) seamless taskadaptation: propagating object IDs to pixels for 2D segmentation or toGaussians for 3D extraction. Experiments demonstrate that our methodeffectively overcomes the challenges of 3D open-vocabulary object extractionwhile remaining comparable to state-of-the-art performance in 2Dopen-vocabulary segmentation, ensuring minimal compromise.</description>
      <author>example@mail.com (Abdalla Arafa, Didier Stricker)</author>
      <guid isPermaLink="false">2509.12938v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
  <item>
      <title>Semantic 3D Reconstructions with SLAM for Central Airway Obstruction</title>
      <link>http://arxiv.org/abs/2509.13541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的流程，通过结合DROID-SLAM与分割模型，利用单目内窥镜视频实现中央气道的实时、语义感知三维重建，为自主机器人干预提供了有希望的进展。&lt;h4&gt;背景&lt;/h4&gt;中央气道阻塞(CAO)是一种发病率不断增加的危及生命的疾病，由气道内外的肿瘤引起。传统治疗方法如支气管镜和电凝术虽可完全移除肿瘤，但并发症风险高。最近的机器人干预技术降低了风险，结合场景理解和映射还可能实现自动化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够使用单目内窥镜视频进行中央气道实时、语义感知三维重建的新颖流程，为机器人干预提供支持。&lt;h4&gt;方法&lt;/h4&gt;结合DROID-SLAM与一个经过训练用于识别阻塞组织的分割模型。SLAM模块实时重建气道的三维几何结构，分割掩码指导重建点云内阻塞区域的标注。使用离体模型评估重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;真实CT扫描与3D重建之间具有高度相似性(0.62 mm Chamfer距离)。系统通过集成分割到SLAM工作流中，能实时生成突出显示临床相关区域的标注3D地图。该流程比以往工作更快完成重建，更准确地反映手术场景。&lt;h4&gt;结论&lt;/h4&gt;据所知，这是第一个将语义分割与实时单目SLAM集成用于内窥镜CAO场景的工作。该框架是模块化的，可泛化到其他解剖结构或程序，只需少量改动，为自主机器人干预提供了有希望的进展。&lt;h4&gt;翻译&lt;/h4&gt;中央气道阻塞(CAO)是一种发病率不断增加的危及生命的疾病，由气道内外的肿瘤引起。传统治疗方法如支气管镜和电凝术可用于完全移除肿瘤；然而，这些方法存在高并发症风险。最近的进展允许风险更低的机器人干预。机器人干预与场景理解和映射的结合也为自动化开辟了可能性。我们提出了一种新颖的流程，能够使用单目内窥镜视频进行中央气道的实时、语义感知三维重建。我们的方法将DROID-SLAM与一个用于识别阻塞组织的分割模型相结合。SLAM模块实时重建气道的三维几何结构，而分割掩码指导重建点云内阻塞区域的标注。为了验证我们的流程，我们使用离体模型评估重建质量。定性和定量结果显示真实CT扫描与3D重建之间具有高度相似性(0.62 mm Chamfer距离)。通过将分割直接集成到SLAM工作流中，我们的系统能实时生成突出显示临床相关区域的标注3D地图。该流程的高速能力比以往工作能更快完成重建，更准确地反映手术场景。据我们所知，这是第一个将语义分割与实时单目SLAM集成用于内窥镜CAO场景的工作。我们的框架是模块化的，可以泛化到其他解剖结构或程序，只需少量改动，为自主机器人干预提供了有希望的进展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决中心气道阻塞（CAO）的实时3D重建和场景理解问题。CAO是一种危及生命的疾病，发病率正在增加，由气道内外的肿瘤引起。传统治疗方法风险高，而机器人干预需要结合场景理解才能实现自动化。现有的3D重建方法耗时太长，无法在手术过程中实时更新，因此需要一种能够快速重建气道3D结构并识别阻塞区域的方法，以支持更安全、更精确的自动化手术操作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到CAO治疗的挑战性和机器人干预的潜力。他们之前的工作已实现实时分割但3D重建使用耗时的SfM方法，因此转向SLAM算法以实现实时重建。他们借鉴了DROID-SLAM作为3D重建框架，采用U-Net和SAM2构建分割模型，使用类似前人的phantom制作方法模拟临床场景。整体设计思路是将实时3D重建与语义分割结合，创建带有标注的3D气道地图，以支持下游自动化任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将实时3D重建（通过SLAM）与语义分割相结合，创建带有临床相关信息标注的3D气道地图。整体流程包括：1)使用羊心和鸡胸肉制作的phantom模拟CAO并收集内窥镜视频；2)训练U-Net/SAM2分割模型识别阻塞组织；3)使用DROID-SLAM处理单目内窥镜视频流，实时创建3D点云；4)将分割掩码集成到重建流程中，识别和标注阻塞区域；5)通过配准CT扫描评估重建质量和分割精度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将语义分割与实时单目SLAM结合应用于内窥镜CAO场景；创建实时、语义丰富的3D气道重建管道；实现比之前工作更快的重建速度；开发模块化框架可扩展到其他场景。相比之前工作，新方法用SLAM替代了耗时的SfM，将分割直接集成到SLAM工作流中，产生的3D重建带有语义标注，处理速度更快（每帧0.31秒），能更准确地反映手术场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的管道，通过结合实时单目SLAM与语义分割，首次实现了中心气道阻塞场景下的实时、语义丰富的3D重建，为自动化机器人手术干预提供了关键技术支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Central airway obstruction (CAO) is a life-threatening condition withincreasing incidence, caused by tumors in and outside of the airway.Traditional treatment methods such as bronchoscopy and electrocautery can beused to remove the tumor completely; however, these methods carry a high riskof complications. Recent advances allow robotic interventions with lesser risk.The combination of robot interventions with scene understanding and mappingalso opens up the possibilities for automation. We present a novel pipelinethat enables real-time, semantically informed 3D reconstructions of the centralairway using monocular endoscopic video.  Our approach combines DROID-SLAM with a segmentation model trained toidentify obstructive tissues. The SLAM module reconstructs the 3D geometry ofthe airway in real time, while the segmentation masks guide the annotation ofobstruction regions within the reconstructed point cloud. To validate ourpipeline, we evaluate the reconstruction quality using ex vivo models.  Qualitative and quantitative results show high similarity between groundtruth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). Byintegrating segmentation directly into the SLAM workflow, our system producesannotated 3D maps that highlight clinically relevant regions in real time.High-speed capabilities of the pipeline allows quicker reconstructions comparedto previous work, reflecting the surgical scene more accurately.  To the best of our knowledge, this is the first work to integrate semanticsegmentation with real-time monocular SLAM for endoscopic CAO scenarios. Ourframework is modular and can generalize to other anatomies or procedures withminimal changes, offering a promising step toward autonomous roboticinterventions.</description>
      <author>example@mail.com (Ayberk Acar, Fangjie Li, Hao Li, Lidia Al-Zogbi, Kanyifeechukwu Jane Oguine, Susheela Sharma Stern, Jesse F. d'Almeida, Robert J. Webster III, Ipek Oguz, Jie Ying Wu)</author>
      <guid isPermaLink="false">2509.13541v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors</title>
      <link>http://arxiv.org/abs/2509.13525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ColonCrafter，一个基于扩散的深度估计模型，用于从单目结肠镜视频中生成时间上一致的深度图，解决了现有内窥镜深度估计模型在视频序列中时间一致性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;结肠镜检查中的三维场景理解面临重大挑战，需要自动化的方法进行准确的深度估计。然而，现有的内窥镜深度估计模型在视频序列中的时间一致性方面存在问题，限制了它们在三维重建中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从单目结肠镜视频中生成时间上一致的深度图的模型，以克服现有方法的局限性，并支持三维重建等临床应用。&lt;h4&gt;方法&lt;/h4&gt;作者提出了ColonCrafter，一个基于扩散的深度估计模型。该方法从合成的结肠镜序列中学习强大的几何先验，生成时间上一致的深度图。同时引入了一种风格转换技术，在保持几何结构的同时，将真实的临床视频调整为匹配合成的训练域。&lt;h4&gt;主要发现&lt;/h4&gt;ColonCrafter在C3VD数据集上实现了最先进的零样本性能，优于通用方法和专门针对内窥镜的方法。该模型能够生成时间上一致的深度图，并支持临床相关的应用，包括三维点云生成和表面覆盖率评估。&lt;h4&gt;结论&lt;/h4&gt;尽管完整的轨迹三维重建仍然是一个挑战，但ColonCrafter在临床应用方面展示了潜力，特别是在三维点云生成和表面覆盖率评估方面。&lt;h4&gt;翻译&lt;/h4&gt;结肠镜检查中的三维场景理解面临着重大挑战，需要自动化的方法进行准确的深度估计。然而，现有的内窥镜深度估计模型在视频序列中的时间一致性方面存在问题，限制了它们在三维重建中的应用。我们提出了ColonCrafter，一个基于扩散的深度估计模型，可以从单目结肠镜视频中生成时间上一致的深度图。我们的方法从合成的结肠镜序列中学习强大的几何先验，以生成时间上一致的深度图。我们还引入了一种风格转换技术，在保持几何结构的同时，将真实的临床视频调整为匹配我们的合成训练域。ColonCrafter在C3VD数据集上实现了最先进的零样本性能，优于通用方法和专门针对内窥镜的方法。尽管完整的轨迹三维重建仍然是一个挑战，但我们展示了ColonCrafter的临床相关应用，包括三维点云生成和表面覆盖率评估。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决结肠镜视频中的深度估计问题，特别是实现时间一致的深度估计以支持结肠三维重建。这个问题非常重要，因为结直肠癌是美国癌症相关死亡的主要原因，结肠镜检查是筛查金标准，但临床实践中存在结肠皱褶后可视化不良导致病变漏诊率高、难以重新定位病变、以及难以准确测量息肉大小等挑战。结肠本质上是三维结构，而医生只能获得二维视觉信息，这种不匹配限制了临床效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了结肠镜环境的特殊挑战，包括黏膜缺乏明显视觉特征、非朗伯反射、严重高光等。他们借鉴了DepthCrafter架构和EDM框架，使用变分自编码器进行深度表示，并采用低秩适应(LoRA)进行模型微调而非从头训练。作者还参考了Chung等人的艺术风格转换工作，但创新性地将其应用于医学领域。他们使用合成结肠镜序列进行训练，并开发了一种风格转换技术来弥合合成数据与真实临床视频之间的域差距，同时引入了多种数据增强技术提高模型泛化能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用扩散模型进行结肠镜视频的深度估计，并通过风格转换技术解决合成训练数据与真实临床视频之间的域差距问题。整体流程包括：1)从CT扫描构建合成结肠镜数据集；2)使用LoRA微调DepthCrafter模型；3)开发真实到合成的风格转换技术，保留几何结构同时改变外观；4)使用ColonCrafter从输入视频生成时间一致的深度图；5)将深度图应用于下游任务如3D点云生成和表面覆盖评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门针对结肠镜的扩散模型深度估计框架；2)新颖的风格转换技术，解决域差距同时保留几何结构；3)在C3VD基准上实现最先进的零样本性能。相比之前工作，通用深度估计模型在结肠镜数据上表现中等，而ColonCrafter通过特定微调将δ1准确性提高17%以上；其他内窥镜特定模型难以保持长期时间一致性；基于合成数据的方法通常在真实临床视频上表现不佳，而ColonCrafter通过风格转换成功弥合了域差距。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ColonCrafter通过结合扩散模型和风格转换技术，首次实现了从单目结肠镜视频中生成时间一致的高精度深度图，为结肠三维重建和临床应用提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional (3D) scene understanding in colonoscopy presentssignificant challenges that necessitate automated methods for accurate depthestimation. However, existing depth estimation models for endoscopy strugglewith temporal consistency across video sequences, limiting their applicabilityfor 3D reconstruction. We present ColonCrafter, a diffusion-based depthestimation model that generates temporally consistent depth maps from monocularcolonoscopy videos. Our approach learns robust geometric priors from syntheticcolonoscopy sequences to generate temporally consistent depth maps. We alsointroduce a style transfer technique that preserves geometric structure whileadapting real clinical videos to match our synthetic training domain.ColonCrafter achieves state-of-the-art zero-shot performance on the C3VDdataset, outperforming both general-purpose and endoscopy-specific approaches.Although full trajectory 3D reconstruction remains a challenge, we demonstrateclinically relevant applications of ColonCrafter, including 3D point cloudgeneration and surface coverage assessment.</description>
      <author>example@mail.com (Romain Hardy, Tyler Berzin, Pranav Rajpurkar)</author>
      <guid isPermaLink="false">2509.13525v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark</title>
      <link>http://arxiv.org/abs/2509.14227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Cinéaste基准，用于评估视觉语言模型对长篇电影的理解能力，现有模型在该基准上表现不佳，表明长程时间推理是主要瓶颈。&lt;h4&gt;背景&lt;/h4&gt;最近的视觉语言模型在视频理解方面取得了进步，但诊断它们对深层叙事理解的能力仍然是一个挑战。现有基准测试通常测试短片段识别或使用基于模板的问题，在评估对长篇叙事内容的细粒度推理方面存在关键空白。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的基准测试(Cinéaste)来评估视觉语言模型对长篇电影的细粒度推理和叙事理解能力。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含来自200部不同电影的1,805个场景的3,119个多项选择题-答案对的数据集，涵盖五个细粒度上下文推理类别。使用GPT-4o生成多样化、丰富的上下文问题，整合视觉描述、字幕、场景标题和摘要。采用两阶段过滤流程确保问题质量：上下文独立性过滤和上下文真实性过滤。&lt;h4&gt;主要发现&lt;/h4&gt;现有的多模态大语言模型在Cinéaste基准上表现不佳；分析显示长程时间推理是主要瓶颈，最好的开源模型仅达到63.15%的准确率。&lt;h4&gt;结论&lt;/h4&gt;这强调了细粒度上下文理解面临的重大挑战以及对长篇电影理解进步的需求。&lt;h4&gt;翻译&lt;/h4&gt;尽管最近视觉语言模型的进步改善了视频理解，但诊断它们对深层叙事理解的能力仍然是一个挑战。现有的基准测试通常测试短片段识别或使用基于模板的问题，在评估对长篇叙事内容的细粒度推理方面留下了关键空白。为了解决这些空白，我们引入了Cinéaste，一个用于长篇电影理解的全面基准。我们的数据集包含来自200部不同电影的1,805个场景的3,119个多项选择题-答案对，涵盖了五个新颖的细粒度上下文推理类别。我们使用GPT-4o通过整合视觉描述、字幕、场景标题和摘要来生成多样化、丰富的上下文问题，这些问题需要深层的叙事理解。为确保高质量评估，我们的流程包含两阶段过滤：上下文独立性过滤确保问题需要视频上下文，而上下文真实性过滤验证与电影内容的事实一致性，减轻幻觉。实验表明，现有的多模态大语言模型在Cinéaste上表现不佳；我们的分析显示长程时间推理是主要瓶颈，最好的开源模型仅达到63.15%的准确率。这强调了细粒度上下文理解面临的重大挑战以及对长篇电影理解进步的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advancements in vision-language models have improved videounderstanding, diagnosing their capacity for deep, narrative comprehensionremains a challenge. Existing benchmarks often test short-clip recognition oruse template-based questions, leaving a critical gap in evaluating fine-grainedreasoning over long-form narrative content. To address these gaps, we introduce$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movieunderstanding. Our dataset comprises 3,119 multiple-choice question-answerpairs derived from 1,805 scenes across 200 diverse movies, spanning five novelfine-grained contextual reasoning categories. We use GPT-4o to generatediverse, context-rich questions by integrating visual descriptions, captions,scene titles, and summaries, which require deep narrative understanding. Toensure high-quality evaluation, our pipeline incorporates a two-stage filteringprocess: Context-Independence filtering ensures questions require videocontext, while Contextual Veracity filtering validates factual consistencyagainst the movie content, mitigating hallucinations. Experiments show thatexisting MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis revealsthat long-range temporal reasoning is a primary bottleneck, with the topopen-source model achieving only 63.15\% accuracy. This underscores significantchallenges in fine-grained contextual understanding and the need foradvancements in long-form movie comprehension.</description>
      <author>example@mail.com (Nisarg A. Shah, Amir Ziai, Chaitanya Ekanadham, Vishal M. Patel)</author>
      <guid isPermaLink="false">2509.14227v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Dense Video Understanding with Gated Residual Tokenization</title>
      <link>http://arxiv.org/abs/2509.14199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Dense Video Understanding (DVU)框架，通过Gated Residual Tokenization (GRT)技术实现高帧率视频理解，并创建了DIVE基准测试评估密集时间推理能力。&lt;h4&gt;背景&lt;/h4&gt;当前视频大语言模型主要依赖低帧率采样，丢弃了密集时间信息，导致对需要精确时间对齐的任务（如讲座理解）表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发能够高效处理高FPS视频理解的方法，减少标记时间和标记开销，并创建新的基准测试评估密集时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出GRT两阶段框架：1)运动补偿间门控标记化，使用像素级运动估计跳过静态区域；2)语义场景内标记化合并，融合静态区域标记减少冗余。&lt;h4&gt;主要发现&lt;/h4&gt;在DIVE基准测试上，GRT表现优于更大规模的VLLM基线模型，并能随FPS正向扩展，证明了密集时间信息的重要性。&lt;h4&gt;结论&lt;/h4&gt;密集时间信息对视频理解至关重要，GRT方法能够实现高效、可扩展的高FPS视频理解。&lt;h4&gt;翻译&lt;/h4&gt;高时间分辨率对于捕捉视频理解中的精细细节至关重要。然而，当前视频大语言模型和基准测试主要依赖低帧率采样，如均匀采样或关键帧选择，丢弃了密集的时间信息。这种折衷避免了标记每一帧的高成本，否则会导致冗余计算和随着视频长度增加的线性标记增长。虽然这种权衡对缓慢变化的内容有效，但对于像讲座理解这样的任务则失败，其中信息几乎出现在每一帧中，需要精确的时间对齐。为解决这一差距，我们引入了Dense Video Understanding (DVU)，通过减少标记时间和标记开销来实现高FPS视频理解。现有基准测试也有局限，因为它们的问答对关注粗略的内容变化。因此，我们提出了DIVE (Dense Information Video Evaluation)，这是第一个专为密集时间推理设计的基准测试。为使DVU实用，我们提出了Gated Residual Tokenization (GRT)，一个两阶段框架：(1)运动补偿间门控标记化使用像素级运动估计在标记化过程中跳过静态区域，实现标记数和计算量的次线性增长。(2)语义场景内标记化合并融合场景内静态区域的标记，进一步减少冗余，同时保留动态语义。在DIVE上的实验表明，GRT优于更大的VLLM基线，并能随FPS正向扩展。这些结果突出了密集时间信息的重要性，并证明GRT能够实现高效、可扩展的高FPS视频理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High temporal resolution is essential for capturing fine-grained details invideo understanding. However, current video large language models (VLLMs) andbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling orkeyframe selection, discarding dense temporal information. This compromiseavoids the high cost of tokenizing every frame, which otherwise leads toredundant computation and linear token growth as video length increases. Whilethis trade-off works for slowly changing content, it fails for tasks likelecture comprehension, where information appears in nearly every frame andrequires precise temporal alignment. To address this gap, we introduce DenseVideo Understanding (DVU), which enables high-FPS video comprehension byreducing both tokenization time and token overhead. Existing benchmarks arealso limited, as their QA pairs focus on coarse content changes. We thereforepropose DIVE (Dense Information Video Evaluation), the first benchmark designedfor dense temporal reasoning. To make DVU practical, we present Gated ResidualTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-GatedTokenization uses pixel-level motion estimation to skip static regions duringtokenization, achieving sub-linear growth in token count and compute. (2)Semantic-Scene Intra-Tokenization Merging fuses tokens across static regionswithin a scene, further reducing redundancy while preserving dynamic semantics.Experiments on DIVE show that GRT outperforms larger VLLM baselines and scalespositively with FPS. These results highlight the importance of dense temporalinformation and demonstrate that GRT enables efficient, scalable high-FPS videounderstanding.</description>
      <author>example@mail.com (Haichao Zhang, Wenhao Chai, Shwai He, Ang Li, Yun Fu)</author>
      <guid isPermaLink="false">2509.14199v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling</title>
      <link>http://arxiv.org/abs/2509.13784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Variable-Rate Spatial Event Mamba的新型架构，直接处理原始事件流而无需中间表示，通过轻量级编码器和Mamba状态空间模型实现高效处理，并采用自适应控制器优化延迟平衡。&lt;h4&gt;背景&lt;/h4&gt;事件相机以微秒级时间分辨率捕捉异步像素级亮度变化，为高速视觉任务提供独特优势。现有方法常将事件流转换为帧、体素网格或点云等中间表示，需要预定义时间窗口，引入窗口延迟；点检测方法则因计算成本高而无法实现实时效率。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，开发能够直接处理原始事件流、无需中间表示的架构，同时保持实时效率，实现窗口延迟和推理延迟之间的最佳平衡。&lt;h4&gt;方法&lt;/h4&gt;提出Variable-Rate Spatial Event Mamba架构，包含：1)轻量级因果空间邻域编码器，高效捕获局部几何关系；2)基于Mamba的状态空间模型，实现线性复杂度的可扩展时间建模；3)自适应控制器，根据事件率调整处理速度优化延迟。&lt;h4&gt;主要发现&lt;/h4&gt;直接处理原始事件流可避免中间表示的延迟；轻量级编码器能有效捕获局部几何关系；Mamba状态空间模型提供线性复杂度的时间建模能力；自适应处理可实现延迟优化。&lt;h4&gt;结论&lt;/h4&gt;Variable-Rate Spatial Event Mamba架构成功克服了现有方法的局限性，能够直接处理原始事件流并保持实时效率，为高速视觉任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;事件相机以微秒级时间分辨率捕捉异步像素级亮度变化，为高速视觉任务提供了独特优势。现有方法通常将事件流转换为帧、体素网格或点云等中间表示，这不可避免地需要预定义时间窗口，从而引入窗口延迟。同时，点检测方法由于计算成本高，面临计算挑战，无法实现实时效率。为克服这些局限性，我们提出了Variable-Rate Spatial Event Mamba，一种新型架构，可直接处理原始事件流而无需中间表示。我们的方法引入了轻量级因果空间邻域编码器，以高效捕获局部几何关系，然后使用基于Mamba的状态空间模型进行具有线性复杂度的可扩展时间建模。在推理过程中，控制器根据事件率自适应调整处理速度，实现窗口延迟和推理延迟之间的最佳平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras capture asynchronous pixel-level brightness changes withmicrosecond temporal resolution, offering unique advantages for high-speedvision tasks. Existing methods often convert event streams into intermediaterepresentations such as frames, voxel grids, or point clouds, which inevitablyrequire predefined time windows and thus introduce window latency. Meanwhile,pointwise detection methods face computational challenges that preventreal-time efficiency due to their high computational cost. To overcome theselimitations, we propose the Variable-Rate Spatial Event Mamba, a novelarchitecture that directly processes raw event streams without intermediaterepresentations. Our method introduces a lightweight causal spatialneighborhood encoder to efficiently capture local geometric relations, followedby Mamba-based state space models for scalable temporal modeling with linearcomplexity. During inference, a controller adaptively adjusts the processingspeed according to the event rate, achieving an optimal balance between windowlatency and inference latency.</description>
      <author>example@mail.com (Hanfang Liang, Bing Wang, Shizhen Zhang, Wen Jiang, Yizhuo Yang, Weixiang Guo, Shenghai Yuan)</author>
      <guid isPermaLink="false">2509.13784v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training</title>
      <link>http://arxiv.org/abs/2509.10426v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为DECAMP的去耦合上下文感知预训练框架，用于解决自动驾驶中轨迹预测面临的标记数据稀缺和多智能体预测场景表现不佳的问题。该框架通过将行为模式学习与潜在特征重建去耦合，优先考虑可解释的动态，并结合上下文感知表示学习和协作空间-运动预训练任务，有效提升了多智能体运动预测的性能。&lt;h4&gt;背景&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法面临标记数据稀缺的问题，在多智能体预测场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，作者提出了一个名为DECAMP的去耦合上下文感知预训练框架，专门用于多智能体运动预测。&lt;h4&gt;方法&lt;/h4&gt;该框架将行为模式学习与潜在特征重建去耦合，优先考虑可解释的动态，从而增强下游预测的场景表示。框架结合了上下文感知表示学习和协作空间-运动预训练任务，能够同时优化结构和意图推理，同时捕捉潜在的动态意图。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse 2基准测试上的实验展示了该方法优越的性能，结果证明了其在多智能体运动预测中的有效性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是自动驾驶中首个用于多智能体运动预测的上下文自编码器框架，代码和模型将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法往往面临标记数据稀缺的问题，并在多智能体预测场景中表现不佳。为了解决这些挑战，我们引入了一个用于多智能体运动预测的去耦合上下文感知预训练框架，名为DECAMP。与现有方法将表示学习与预训练任务纠缠在一起不同，我们的框架将行为模式学习与潜在特征重建去耦合，优先考虑可解释的动态，从而增强下游预测的场景表示。此外，我们的框架结合了上下文感知表示学习和协作空间-运动预训练任务，这能够在捕捉潜在动态意图的同时，对结构和意图推理进行联合优化。我们在Argoverse 2基准测试上的实验展示了我们方法的优越性能，取得的结果强调了其在多智能体运动预测中的有效性。据我们所知，这是自动驾驶中首个用于多智能体运动预测的上下文自编码器框架。代码和模型将公开提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多智能体运动预测中的两个核心挑战：标记数据稀缺问题和多智能体预测场景中的次优性能。这个问题在自动驾驶领域至关重要，因为准确预测交通参与者的运动对确保道路安全和效率至关重要，特别是在复杂的多智能体交互场景中，不准确的预测可能导致生成与整体场景不一致的轨迹，阻碍自动驾驶车辆做出可靠决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前轨迹预测的主流方法（监督学习和自监督学习）及其局限性，特别是自监督学习方法存在的两个核心问题：编码器与预训练任务紧密耦合、难以扩展到多智能体预测。基于这些分析，作者借鉴了计算机视觉中自监督学习的成功经验，特别是掩码图像建模中的回归器设计，将其扩展到自动驾驶中的行为预测。同时，作者参考了DenseTNT和HiVT等方法的向量化表示技术，设计了'编码器-回归器-解码器'的级联范式来解耦行为模式学习与特征重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是解耦行为模式学习与潜在特征重建，优先考虑可解释的动力学，并引入协作空间-运动预训练任务来同时优化结构和推理。整体实现分为两个阶段：1)预训练阶段，模型接收历史和未来状态以及地图信息，通过编码器提取场景特征，回归器预测被遮盖令牌的表示，双解码器执行空间重建和运动识别任务；2)微调阶段，模型仅使用历史状态和地图信息，利用预训练的编码器生成K个场景一致的轨迹预测，每个场景包含所有目标代理的完整轨迹组合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)解耦的自监督学习框架，分离行为模式学习与特征重建；2)协作空间-运动预训练任务，联合优化空间线索和运动信号识别；3)'编码器-回归器-解码器'级联范式，增强场景元素间语义关系建模；4)多世界预测微调，生成场景一致的轨迹组合。相比之前工作，不同之处在于：不将表示学习与预训练任务紧密耦合，支持真正的多智能体联合预测而非单智能体预测，无需复杂后处理来确保场景一致性，并首次揭示协作预训练任务可以增强驾驶行为模式的表示学习。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DECAMP提出了一种解耦的上下文感知预训练框架，通过分离行为模式学习与特征重建并引入协作空间-运动预训练任务，显著提高了多智能体运动预测中场景一致性轨迹预测的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory prediction is a critical component of autonomous driving,essential for ensuring both safety and efficiency on the road. However,traditional approaches often struggle with the scarcity of labeled data andexhibit suboptimal performance in multi-agent prediction scenarios. To addressthese challenges, we introduce a disentangled context-aware pre-trainingframework for multi-agent motion prediction, named DECAMP. Unlike existingmethods that entangle representation learning with pretext tasks, our frameworkdecouples behavior pattern learning from latent feature reconstruction,prioritizing interpretable dynamics and thereby enhancing scene representationfor downstream prediction. Additionally, our framework incorporatescontext-aware representation learning alongside collaborative spatial-motionpretext tasks, which enables joint optimization of structural and intentionalreasoning while capturing the underlying dynamic intentions. Our experiments onthe Argoverse 2 benchmark showcase the superior performance of our method, andthe results attained underscore its effectiveness in multi-agent motionforecasting. To the best of our knowledge, this is the first contextautoencoder framework for multi-agent motion forecasting in autonomous driving.The code and models will be made publicly available.</description>
      <author>example@mail.com (Jianxin Shi, Zengqi Peng, Xiaolong Chen, Tianyu Wo, Jun Ma)</author>
      <guid isPermaLink="false">2509.10426v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction</title>
      <link>http://arxiv.org/abs/2509.13841v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This preprint is also available at ESS Open Archive:  https://essopenarchive.org/users/960205/articles/1329010&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种端到端可区分的混合框架，将图神经网络嵌入到孔隙网络模型中，实现了高精度且跨尺度泛化的多孔介质渗透率预测，同时保持物理基础并提高模型可解释性。&lt;h4&gt;背景&lt;/h4&gt;准确预测多孔介质中的渗透率对地下流动建模至关重要。纯数据驱动模型计算效率高但缺乏跨尺度泛化能力和物理约束；传统孔隙网络模型基于物理且高效，但依赖理想化几何假设，限制了在复杂结构中的准确性。&lt;h4&gt;目的&lt;/h4&gt;克服纯数据驱动模型和传统孔隙网络模型的局限性，开发一种既能避免理想化几何假设又能保持物理基础流动计算的渗透率预测方法。&lt;h4&gt;方法&lt;/h4&gt;构建一个端到端可区分的混合框架，将图神经网络嵌入到孔隙网络模型中，用GNN预测替代传统解析公式进行传导率计算，并通过反向传播梯度实现完全耦合的端到端训练，仅需单一标量渗透率作为训练目标。&lt;h4&gt;主要发现&lt;/h4&gt;所得模型具有高精度，能很好地跨不同尺度泛化，性能优于纯数据驱动和传统孔隙网络模型；基于梯度的敏感性分析揭示了物理上一致的特征影响，提高了模型可解释性。&lt;h4&gt;结论&lt;/h4&gt;该方法为复杂多孔介质中的渗透率预测提供了可扩展且物理信息丰富的框架，有效减少了模型不确定性并提高了预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;多孔介质中渗透率的准确预测对地下流动建模至关重要。虽然纯数据驱动模型提供了计算效率，但它们通常缺乏跨尺度的泛化能力，且不包含明确的物理约束。另一方面，孔隙网络模型基于物理原理且高效，但依赖于理想化的几何假设来估计孔隙尺度水力传导率，限制了其在复杂结构中的准确性。为了克服这些局限，我们提出了一种端到端可区分的混合框架，将图神经网络嵌入到孔隙网络模型中。在此框架中，用于传导率计算的解析公式被基于GNN的预测所替代，这些预测由孔隙和喉部特征推导得出。预测的传导率随后被传递给PNM求解器进行渗透率计算。这样，模型避免了PNM的理想化几何假设，同时保留了基于物理的流动计算。GNN的训练不需要标记的传导率数据（每个孔隙网络可能有数千个），而是使用单一的标量渗透率作为训练目标。这通过反向传播梯度（通过GNN的自动微分和PNM求解器的离散伴随方法）成为可能，实现了完全耦合的端到端训练。所得模型实现高精度并能很好地跨不同尺度泛化，性能优于纯数据驱动和传统PNM方法。基于梯度的敏感性分析进一步揭示了物理上一致的特征影响，增强了模型可解释性。这种方法为复杂多孔介质中的渗透率预测提供了可扩展且物理信息丰富的框架，减少了模型不确定性并提高了准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of permeability in porous media is essential for modelingsubsurface flow. While pure data-driven models offer computational efficiency,they often lack generalization across scales and do not incorporate explicitphysical constraints. Pore network models (PNMs), on the other hand, arephysics-based and efficient but rely on idealized geometric assumptions toestimate pore-scale hydraulic conductance, limiting their accuracy in complexstructures. To overcome these limitations, we present an end-to-enddifferentiable hybrid framework that embeds a graph neural network (GNN) into aPNM. In this framework, the analytical formulas used for conductancecalculations are replaced by GNN-based predictions derived from pore and throatfeatures. The predicted conductances are then passed to the PNM solver forpermeability computation. In this way, the model avoids the idealized geometricassumptions of PNM while preserving the physics-based flow calculations. TheGNN is trained without requiring labeled conductance data, which can number inthe thousands per pore network; instead, it learns conductance values by usinga single scalar permeability as the training target. This is made possible bybackpropagating gradients through both the GNN (via automatic differentiation)and the PNM solver (via a discrete adjoint method), enabling fully coupled,end-to-end training. The resulting model achieves high accuracy and generalizeswell across different scales, outperforming both pure data-driven andtraditional PNM approaches. Gradient-based sensitivity analysis further revealsphysically consistent feature influences, enhancing model interpretability.This approach offers a scalable and physically informed framework forpermeability prediction in complex porous media, reducing model uncertainty andimproving accuracy.</description>
      <author>example@mail.com (Qingqi Zhao, Heng Xiao)</author>
      <guid isPermaLink="false">2509.13841v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>State Space Models over Directed Graphs</title>
      <link>http://arxiv.org/abs/2509.13735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  currently undergoing review by IEEE Transactions on Big Data&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新方法DirEgo2Token，通过k-hop自我图将有向图序列化，并开发了DirGraphSSM架构，首次将状态空间模型系统性地扩展到有向图学习领域。实验表明该方法在多个任务上取得了最先进性能，且训练速度提升1.5至2倍。&lt;h4&gt;背景&lt;/h4&gt;有向图在众多领域普遍存在，边的方向性编码关键因果关系。现有针对有向图的图神经网络和图变换器面临两大挑战：有效捕获长程因果关系，以及在大规模图数据集上平衡准确性和训练效率。状态空间模型在因果序列任务中表现出色，但其图变体仅适用于无向图，限制了在有向图学习中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有图状态空间模型仅适用于无向图的限制，将有向图学习与状态空间模型相结合，提高有向图学习的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;提出两种方法：1) DirEgo2Token，通过k-hop自我图将有向图序列化；2) DirGraphSSM，一种新的有向图神经网络架构，通过消息传递机制在有向图上实现状态空间模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，DirGraphSSM在三个代表性的有向图学习任务上取得了最先进的性能，同时在另外两个任务上取得了具有竞争力的性能，并且比现有的最先进模型快1.5到2倍。&lt;h4&gt;结论&lt;/h4&gt;DirGraphSSM成功地将状态空间模型应用于有向图学习，解决了现有方法面临的挑战，并在多个任务上取得了优异的性能和训练效率。&lt;h4&gt;翻译&lt;/h4&gt;有向图在众多领域中普遍存在，其中边的方向性编码了关键的因果关系。然而，现有的针对有向图设计的图神经网络和图变换器面临两大挑战：(1)有效捕获来自有向边的长程因果关系；(2)在处理大规模图数据集时平衡准确性和训练效率。近年来，状态空间模型在因果序列任务中取得了实质性进展，其针对图的变体在各种图学习基准测试中展示了最先进的准确性，同时保持了高效率。然而，现有的图状态空间模型仅设计用于无向图，这限制了它们在有向图学习中的性能。为此，我们提出了一种创新方法DirEgo2Token，通过k-hop自我图将有向图序列化。这是首次将状态空间模型系统性地扩展到有向图学习领域。基于此，我们开发了DirGraphSSM，一种新的有向图神经网络架构，通过消息传递机制在有向图上实现状态空间模型。实验结果表明，DirGraphSSM在三个代表性的有向图学习任务上取得了最先进的性能，同时在另外两个任务上取得了具有竞争力的性能，并且比现有的最先进模型快1.5到2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Directed graphs are ubiquitous across numerous domains, where thedirectionality of edges encodes critical causal dependencies. However, existingGNNs and graph Transformers tailored for directed graphs face two majorchallenges: (1) effectively capturing long-range causal dependencies derivedfrom directed edges; (2) balancing accuracy and training efficiency whenprocessing large-scale graph datasets. In recent years, state space models(SSMs) have achieved substantial progress in causal sequence tasks, and theirvariants designed for graphs have demonstrated state-of-the-art accuracy whilemaintaining high efficiency across various graph learning benchmarks. However,existing graph state space models are exclusively designed for undirectedgraphs, which limits their performance in directed graph learning. To this end,we propose an innovative approach DirEgo2Token which sequentializes directedgraphs via k-hop ego graphs. This marks the first systematic extension of statespace models to the field of directed graph learning. Building upon this, wedevelop DirGraphSSM, a novel directed graph neural network architecture thatimplements state space models on directed graphs via the message-passingmechanism. Experimental results demonstrate that DirGraphSSM achievesstate-of-the-art performance on three representative directed graph learningtasks while attaining competitive performance on two additional tasks with1.5$\times $ to 2$\times $ training speed improvements compared to existingstate-of-the-art models.</description>
      <author>example@mail.com (Junzhi She, Xunkai Li, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2509.13735v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Hate Detection Using Dual-Stream Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.13515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的多模态双流图神经网络模型用于仇恨视频检测，通过构建实例图和互补权重图来突出仇恨内容，系统化建模视频中的结构化关系，实现了最先进的分类性能并具有强可解释性。&lt;h4&gt;背景&lt;/h4&gt;仇恨视频对在线安全和现实福祉构成严重风险，需要有效的检测方法。虽然多模态分类方法优于单模态方法，但存在忽视仇恨内容定义视频类别、统一处理所有内容而非强调仇恨组件，以及无法系统捕获视频结构化信息等局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态仇恨视频检测方法的局限性，开发能够突出仇恨内容、系统化建模视频结构化关系的高效检测模型。&lt;h4&gt;方法&lt;/h4&gt;提出一种多模态双流图神经网络模型，通过将视频分离为多个实例构建实例图提取实例级特征，使用互补权重图分配重要性权重突出仇恨实例，结合权重和特征生成视频标签，采用基于图的框架系统化建模模态内和跨模态的结构化关系。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上的广泛实验表明，该模型在仇恨视频分类方面达到了最先进的水平，具有强可解释性，代码已在GitHub上公开。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型有效解决了现有多模态仇恨视频检测方法的局限性，通过强调仇恨内容和系统化建模结构化关系，显著提升了分类性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;仇恨视频对在线安全和现实福祉构成严重风险，需要有效的检测方法。尽管整合多种模态信息的多模态分类方法优于单模态方法，但它们通常忽视了即使是最少量的仇恨内容也能定义视频类别。具体而言，它们通常统一处理所有内容，而非强调仇恨组件。此外，现有的多模态方法无法系统捕获视频中的结构化信息，限制了多模态融合的有效性。为解决这些局限性，我们提出了一种新颖的多模态双流图神经网络模型。它通过将给定视频分离为多个实例来构建实例图，以提取实例级特征。然后，互补权重图为这些特征分配重要性权重，突出仇恨实例。结合重要性权重和实例特征生成视频标签。我们的模型采用基于图的框架系统化建模模态内和跨模态的结构化关系。在公共数据集上的广泛实验表明，我们的模型在仇恨视频分类方面达到了最先进的水平，并具有强可解释性。代码可在以下网址获取：https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hateful videos present serious risks to online safety and real-worldwell-being, necessitating effective detection methods. Although multimodalclassification approaches integrating information from several modalitiesoutperform unimodal ones, they typically neglect that even minimal hatefulcontent defines a video's category. Specifically, they generally treat allcontent uniformly, instead of emphasizing the hateful components. Additionally,existing multimodal methods cannot systematically capture structuredinformation in videos, limiting the effectiveness of multimodal fusion. Toaddress these limitations, we propose a novel multimodal dual-stream graphneural network model. It constructs an instance graph by separating the givenvideo into several instances to extract instance-level features. Then, acomplementary weight graph assigns importance weights to these features,highlighting hateful instances. Importance weights and instance features arecombined to generate video labels. Our model employs a graph-based framework tosystematically model structured relationships within and across modalities.Extensive experiments on public datasets show that our model isstate-of-the-art in hateful video classification and has strong explainability.Code is available:https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.</description>
      <author>example@mail.com (Jiangbei Yue, Shuonan Yang, Tailin Chen, Jianbo Jiao, Zeyu Fu)</author>
      <guid isPermaLink="false">2509.13515v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Context-Aware Inputs: Physics-Inspired Improvements in Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.10684v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了物理信息策略在改进NuGraph2架构语义分割方面的应用，特别关注提升对米歇尔电子等代表性不足粒子类别的识别性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在液态氩时间投影室的事件重建任务中显示出强大潜力，但对米歇尔电子等代表性不足的粒子类别性能仍然有限。&lt;h4&gt;目的&lt;/h4&gt;探索物理信息策略以改进NuGraph2架构中的语义分割，提高对米歇尔电子等粒子类别的识别能力。&lt;h4&gt;方法&lt;/h4&gt;研究三种互补方法：(i)通过探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示；(ii)引入辅助解码器捕获类别级相关性；(iii)整合基于米歇尔电子能量分布的能量正则化项。&lt;h4&gt;主要发现&lt;/h4&gt;物理启发特征增强带来最大提升，特别显著提高了米歇尔电子的精确率和召回率；辅助解码器和能量正则化项提供的改进有限，部分原因是NuGraph2缺乏明确的粒子或事件级表示。&lt;h4&gt;结论&lt;/h4&gt;将物理上下文直接嵌入节点级输入比施加任务特定辅助损失更有效；建议未来具有明确粒子和事件级推理的分层架构(如NuGraph3)将为高级解码器和基于物理的正则化提供更自然的环境。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近在液态氩时间投影室的事件重建任务中显示出强大的前景，但对于代表性不足的粒子类别(如米歇尔电子)其性能仍然有限。在这项工作中，我们研究了物理信息策略以改进NuGraph2架构中的语义分割。我们探索了三种互补方法：(i)通过从探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示，(ii)引入辅助解码器以捕获类别级相关性，以及(iii)整合受米歇尔电子能量分布启发的基于能量的正则化项。在MicroBooNE公共数据集上的实验表明，物理启发的特征增强带来了最大的收益，特别是通过解纠缠重叠的潜在空间区域，显著提高了米歇尔电子的精确率和召回率。相比之下，辅助解码器和能量正则化项提供的改进有限，部分原因是NuGraph2的命中级别特性，缺乏明确的粒子或事件级表示。我们的发现强调，将物理上下文直接嵌入节点级输入比施加任务特定的辅助损失更有效，并表明未来的分层架构(如NuGraph3)具有明确的粒子和事件级推理，将为高级解码器和基于物理的正则化提供更自然的环境。这项工作的代码已在Github上公开可用：https://github.com/vitorgrizzi/nugraph_phys/tree/main_phys。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决图神经网络在液态氩时间投影室(LArTPC)中对代表性不足粒子类别(特别是米歇尔电子)识别性能不佳的问题。这个问题很重要，因为米歇尔电子是粒子物理研究中的关键信号，准确识别不同类型粒子对于理解中微子相互作用和粒子物理现象至关重要，提高对少数类别的识别能力能改善整体粒子分类的可靠性，对高能物理实验的数据分析和物理发现具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于物理直觉探索了三种互补策略来注入物理领域知识：丰富输入表示、引入辅助解码器和结合基于能量的正则化项。他们借鉴了现有工作：参考了Drielsma的GrapPA研究引入几何描述符，借鉴了DUNE协作的CVN网络进行多任务学习，以及参考了Sharma等人的物理信息GNN将守恒定律作为损失项。作者系统性地评估了这些策略在NuGraph2架构中的适用性和有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将物理领域知识直接嵌入到图神经网络的输入表示中，而非仅通过辅助损失函数约束模型。整体实现流程包括：1)特征扩展，添加节点度、最短边长、导线差值和时间差值四个新特征；2)添加额外解码器，实验了二进制米歇尔解码器、图级米歇尔计数器和完整类别分布解码器；3)米歇尔能量正则化，在损失函数中添加基于物理的约束项，使用波形积分作为沉积能量的代理指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)设计特定的物理启发上下文感知输入特征；2)系统性评估三种物理知识注入策略；3)针对代表性不足类别优化识别性能。相比之前工作，不同之处在于：与GrapPA相比，专注于更广泛的语义分割而非仅电磁簇射组装；与CVN相比，通过特征增强而非多任务学习提高性能；与PINNs相比，将物理知识编码到输入特征而非直接施加物理方程；与原始NuGraph2相比，扩展了输入特征空间并评估了多种物理知识注入策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过在NuGraph2中引入物理启发的上下文感知输入特征，特别是节点度、几何距离和轨迹连续性度量，显著提高对代表性不足粒子类别(如米歇尔电子)的识别性能，同时证明了将领域知识直接嵌入输入表示比通过辅助损失函数约束模型更为有效。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have recently shown strong promise for eventreconstruction tasks in Liquid Argon Time Projection Chambers, yet theirperformance remains limited for underrepresented classes of particles, such asMichel electrons. In this work, we investigate physics-informed strategies toimprove semantic segmentation within the NuGraph2 architecture. We explorethree complementary approaches: (i) enriching the input representation withcontext-aware features derived from detector geometry and track continuity,(ii) introducing auxiliary decoders to capture class-level correlations, and(iii) incorporating energy-based regularization terms motivated by Michelelectron energy distributions. Experiments on MicroBooNE public datasets showthat physics-inspired feature augmentation yields the largest gains,particularly boosting Michel electron precision and recall by disentanglingoverlapping latent space regions. In contrast, auxiliary decoders andenergy-regularization terms provided limited improvements, partly due to thehit-level nature of NuGraph2, which lacks explicit particle- or event-levelrepresentations. Our findings highlight that embedding physics context directlyinto node-level inputs is more effective than imposing task-specific auxiliarylosses, and suggest that future hierarchical architectures such as NuGraph3,with explicit particle- and event-level reasoning, will provide a more naturalsetting for advanced decoders and physics-based regularization. The code forthis work is publicly available on Github athttps://github.com/vitorgrizzi/nugraph_phys/tree/main_phys.</description>
      <author>example@mail.com (Vitor F. Grizzi, Margaret Voetberg, Giuseppe Cerati, Hadi Meidani, V Hewes)</author>
      <guid isPermaLink="false">2509.10684v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Explainability: Post-hoc Explanations for Geometric Neural Network Predictions</title>
      <link>http://arxiv.org/abs/2509.10676v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究为科学应用中的AI方法引入了后验可解释性技术，通过多种解释方法共同分析图神经网络NuGraph2的决策过程，提高了AI在科学应用中的透明度和可靠性。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能在科学应用中的普及，需要能够将结果归因于网络推理过程的能力，以保持稳健的科学泛化。这促使了对AI模型可解释性的研究需求。&lt;h4&gt;目的&lt;/h4&gt;激励和展示后验可解释性方法在科学应用中AI方法的使用，特别是在中微子标记的图神经网络NuGraph2中的应用。&lt;h4&gt;方法&lt;/h4&gt;为现有的图神经网络NuGraph2引入可解释性附加组件，包括检查网络输出（节点分类）和边连接的技术，以及使用新通用工具探测潜在空间的方法。&lt;h4&gt;主要发现&lt;/h4&gt;没有单一的解释方法足以展示网络'理解'，但多种方法结合使用可以提供对分类过程中使用的见解。&lt;h4&gt;结论&lt;/h4&gt;这些可解释性方法虽然在NuGraph2上测试，但具有广泛适用性，可以应用于各种类型的网络，不仅限于图神经网络，为科学应用中的AI模型提供了更好的透明度和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;随着人工智能在科学应用中的日益普及，将结果归因于网络推理过程的能力对于保持稳健的科学泛化至关重要。在这项工作中，我们旨在激励和展示后验可解释性方法在科学应用中AI方法的使用。为此，我们为现有的用于中微子标记的图神经网络NuGraph2引入了可解释性附加组件。解释形式包括一系列技术，检查网络的输出（节点分类）和它们之间的边连接，以及使用应用于该网络的新通用工具探测潜在空间。我们展示了没有这些方法中的任何一种足以单独展示网络'理解'，但它们共同可以提供对分类过程中使用的见解。虽然这些方法在NuGraph2应用上进行了测试，但它们可以应用于广泛的网络，不仅限于GNN。这项工作的代码已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing popularity of artificial intelligence used for scientificapplications, the ability of attribute a result to a reasoning process from thenetwork is in high demand for robust scientific generalizations to hold. Inthis work we aim to motivate the need for and demonstrate the use of post-hocexplainability methods when applied to AI methods used in scientificapplications. To this end, we introduce explainability add-ons to the existinggraph neural network (GNN) for neutrino tagging, NuGraph2. The explanationstake the form of a suite of techniques examining the output of the network(node classifications) and the edge connections between them, and probing ofthe latent space using novel general-purpose tools applied to this network. Weshow how none of these methods are singularly sufficient to show network"understanding", but together can give insights into the processes used inclassification. While these methods are tested on the NuGraph2 application,they can be applied to a broad range of networks, not limited to GNNs. The codefor this work is publicly available on GitHub athttps://github.com/voetberg/XNuGraph.</description>
      <author>example@mail.com (Margaret Voetberg, Vitor F. Grizzi, Giuseppe Cerati, Hadi Meidani, V Hewes)</author>
      <guid isPermaLink="false">2509.10676v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2509.14181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TimeAlign框架，通过表示对齐技术解决时间序列预测中历史输入与未来目标间的分布差距问题，提升预测性能，且与基础预测器架构无关，计算开销小。&lt;h4&gt;背景&lt;/h4&gt;表示学习技术如对比学习在计算机视觉和自然语言处理领域已取得成功，并在时间序列预测中也有探索。然而，最近最先进的预测器很少采用这些表示方法，因为它们显示出很小的性能优势。&lt;h4&gt;目的&lt;/h4&gt;挑战当前观点，证明显式表示对齐可以提供关键信息，弥合历史输入与未来目标之间的分布差距，从而提升时间序列预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出TimeAlign，一个轻量级即插即用框架，通过简单的重建任务学习辅助特征，并将其反馈给任何基础预测器。该方法与架构无关，计算开销小。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在八个基准测试上的广泛实验验证了TimeAlign的优越性能。2. 收益主要来自于修正历史输入和未来输出之间的频率不匹配。3. 研究提供了TimeAlign有效性的理论依据，证明它能增加学习表示与预测目标之间的互信息。&lt;h4&gt;结论&lt;/h4&gt;TimeAlign可作为现代深度学习时间序列预测系统的通用对齐模块，因为它与架构无关且计算开销小。&lt;h4&gt;翻译&lt;/h4&gt;表示学习技术如对比学习长期以来一直在时间序列预测中得到探索，反映了它们在计算机视觉和自然语言处理领域的成功。然而，最近的最先进预测器很少采用这些表示方法，因为它们显示出很小的性能优势。我们挑战这一观点，并证明显式表示对齐可以提供关键信息，弥合历史输入和未来目标之间的分布差距。为此，我们引入了TimeAlign，一个轻量级即插即用框架，通过简单的重建任务学习辅助特征，并将它们反馈给任何基础预测器。在八个基准测试上的广泛实验验证了其优越性能。进一步的研究表明，收益主要来自于修正历史输入和未来输出之间的频率不匹配。我们还提供了TimeAlign在增加学习表示与预测目标之间互信息方面的有效性的理论依据。由于它与架构无关且计算开销小，TimeAlign可作为现代深度学习时间序列预测系统的通用对齐模块。代码可在https://github.com/TROUBADOUR000/TimeAlign获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning techniques like contrastive learning have long beenexplored in time series forecasting, mirroring their success in computer visionand natural language processing. Yet recent state-of-the-art (SOTA) forecastersseldom adopt these representation approaches because they have shown littleperformance advantage. We challenge this view and demonstrate that explicitrepresentation alignment can supply critical information that bridges thedistributional gap between input histories and future targets. To this end, weintroduce TimeAlign, a lightweight, plug-and-play framework that learnsauxiliary features via a simple reconstruction task and feeds them back to anybase forecaster. Extensive experiments across eight benchmarks verify itssuperior performance. Further studies indicate that the gains arises primarilyfrom correcting frequency mismatches between historical inputs and futureoutputs. We also provide a theoretical justification for the effectiveness ofTimeAlign in increasing the mutual information between learned representationsand predicted targets. As it is architecture-agnostic and incurs negligibleoverhead, TimeAlign can serve as a general alignment module for modern deeplearning time-series forecasting systems. The code is available athttps://github.com/TROUBADOUR000/TimeAlign.</description>
      <author>example@mail.com (Yifan Hu, Jie Yang, Tian Zhou, Peiyuan Liu, Yujin Tang, Rong Jin, Liang Sun)</author>
      <guid isPermaLink="false">2509.14181v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits</title>
      <link>http://arxiv.org/abs/2509.14169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TopoSizing是一个端到端框架，能直接从原始网表进行稳健的电路理解，并将这种知识转化为优化收益，解决了模拟和混合信号电路设计中的数据质量和领域知识嵌入挑战。&lt;h4&gt;背景&lt;/h4&gt;模拟和混合信号电路设计面临高质量数据短缺和领域知识难以嵌入自动化流程的挑战。传统黑盒优化采样效率高但缺乏电路理解，基于学习的方法嵌入结构知识但特定于案例且重新训练成本高，而大语言模型方法常需要人工干预，限制了通用性和透明度。&lt;h4&gt;目的&lt;/h4&gt;提出TopoSizing框架，实现从原始网表直接进行稳健的电路理解，并将这种知识转化为优化收益，提高设计效率同时减少人工干预。&lt;h4&gt;方法&lt;/h4&gt;应用图算法将电路组织成层次化的器件-模块-阶段表示；LLM代理执行迭代假设-验证-细化循环，内置一致性检查产生明确注释；将验证过的见解集成到贝叶斯优化中，通过LLM引导的初始采样和停滞触发的信任区域更新提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;TopoSizing能够直接从原始网表理解电路并将这种理解转化为优化收益；通过LLM代理的迭代循环产生明确注释；验证过的见解可以集成到贝叶斯优化中，提高效率同时保持可行性。&lt;h4&gt;结论&lt;/h4&gt;TopoSizing是一个有前途的端到端框架，结合了电路理解和优化效率，解决了模拟和混合信号电路设计中的关键挑战，无需大量人工干预。&lt;h4&gt;翻译&lt;/h4&gt;模拟和混合信号电路设计仍然具有挑战性，这是由于高质量数据的缺乏以及将领域知识嵌入自动化流程的困难。传统的黑盒优化实现了采样效率，但缺乏电路理解，这常常导致在设计空间中的低价值区域浪费评估。相比之下，基于学习的方法嵌入结构知识，但特定于案例且重新训练成本高。最近使用大语言模型的尝试显示出潜力，但它们通常依赖人工干预，限制了通用性和透明度。我们提出了TopoSizing，一个端到端框架，直接从原始网表执行稳健的电路理解，并将这种知识转化为优化收益。我们的方法首先应用图算法将电路组织成层次化的器件-模块-阶段表示。然后，LLM代理执行迭代假设-验证-细化循环，内置一致性检查，产生明确的注释。验证过的见解通过LLM引导的初始采样和停滞触发的信任区域更新集成到贝叶斯优化中，提高了效率同时保持了可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analog and mixed-signal circuit design remains challenging due to theshortage of high-quality data and the difficulty of embedding domain knowledgeinto automated flows. Traditional black-box optimization achieves samplingefficiency but lacks circuit understanding, which often causes evaluations tobe wasted in low-value regions of the design space. In contrast, learning-basedmethods embed structural knowledge but are case-specific and costly to retrain.Recent attempts with large language models show potential, yet they often relyon manual intervention, limiting generality and transparency. We proposeTopoSizing, an end-to-end framework that performs robust circuit understandingdirectly from raw netlists and translates this knowledge into optimizationgains. Our approach first applies graph algorithms to organize circuits into ahierarchical device-module-stage representation. LLM agents then execute aniterative hypothesis-verification-refinement loop with built-in consistencychecks, producing explicit annotations. Verified insights are integrated intoBayesian optimization through LLM-guided initial sampling andstagnation-triggered trust-region updates, improving efficiency whilepreserving feasibility.</description>
      <author>example@mail.com (Ziming Wei, Zichen Kong, Yuan Wang, David Z. Pan, Xiyuan Tang)</author>
      <guid isPermaLink="false">2509.14169v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration</title>
      <link>http://arxiv.org/abs/2509.14084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为AD-DINOv3的新型视觉语言多模态框架，用于零样本异常检测(ZSAD)，通过结合DINOv3和CLIP模型的优势，解决了特征不对齐和异常区域识别困难的问题。&lt;h4&gt;背景&lt;/h4&gt;零样本异常检测(ZSAD)旨在识别任意新类别的异常，提供可扩展且标注高效的解决方案。传统方法大多基于CLIP模型，通过计算视觉和文本嵌入之间的相似性进行异常检测。最近，DINOv3等视觉基础模型展示了强大的可迁移表示能力，但将其应用于ZSAD面临领域偏差和异常识别困难等挑战。&lt;h4&gt;目的&lt;/h4&gt;将DINOv3模型适应于零样本异常检测任务，解决特征不对齐和细微异常被误认为正常前景的问题，开发一个高效且通用的异常检测框架。&lt;h4&gt;方法&lt;/h4&gt;提出AD-DINOv3框架，将异常检测形式化为多模态对比学习问题。使用DINOv3作为视觉主干提取patch tokens和CLS token，CLIP文本编码器提供正常和异常提示的嵌入。引入轻量级适配器弥合领域差距，并设计异常感知校准模块(AACM)引导CLS token关注异常区域而非通用前景语义。&lt;h4&gt;主要发现&lt;/h4&gt;在八个工业和医疗基准上的大量实验表明，AD-DINOv3一致匹配或超越最先进的方法，验证了其作为通用零样本异常检测框架的优越性。轻量级适配器有效解决了领域偏差问题，AACM模块显著提高了对细微异常的识别能力。&lt;h4&gt;结论&lt;/h4&gt;AD-DINOv3框架成功将DINOv3模型应用于零样本异常检测，通过多模态对比学习和异常感知校准有效解决了传统方法面临的挑战，为异常检测领域提供了一个高效且通用的解决方案，具有广泛的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;零样本异常检测(ZSAD)旨在识别任意新类别的异常，提供可扩展且标注高效的解决方案。传统上，大多数ZSAD工作基于CLIP模型，通过计算视觉和文本嵌入之间的相似性来执行异常检测。最近，DINOv3等视觉基础模型展示了强大的可迁移表示能力。在这项工作中，我们首次将DINOv3适应于ZSAD。然而，这种适应带来了两个关键挑战：(i)大规模预训练数据与异常检测任务之间的领域偏差导致特征不对齐；(ii)预训练表示中对全局语义的固有倾向往往导致细微异常被误认为是正常前景对象的一部分，而不是被识别为异常区域。为了克服这些挑战，我们引入了AD-DINOv3，一个专为ZSAD设计的新型视觉语言多模态框架。具体来说，我们将异常检测形式化为多模态对比学习问题，其中DINOv3用作视觉主干提取patch tokens和CLS token，CLIP文本编码器为正常和异常提示提供嵌入。为了弥合领域差距，我们在两种模态中都引入了轻量级适配器，使它们的表示能够重新校准以适应异常检测任务。除了这种基线对齐外，我们还设计了一个异常感知校准模块(AACM)，明确引导CLS token关注异常区域而非通用前景语义，从而增强可区分性。在八个工业和医疗基准上的大量实验表明，AD-DINOv3一致匹配或超越最先进的方法，验证了其作为通用零样本异常检测框架的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrarynovel categories, offering a scalable and annotation-efficient solution.Traditionally, most ZSAD works have been based on the CLIP model, whichperforms anomaly detection by calculating the similarity between visual andtext embeddings. Recently, vision foundation models such as DINOv3 havedemonstrated strong transferable representation capabilities. In this work, weare the first to adapt DINOv3 for ZSAD. However, this adaptation presents twokey challenges: (i) the domain bias between large-scale pretraining data andanomaly detection tasks leads to feature misalignment; and (ii) the inherentbias toward global semantics in pretrained representations often leads tosubtle anomalies being misinterpreted as part of the normal foreground objects,rather than being distinguished as abnormal regions. To overcome thesechallenges, we introduce AD-DINOv3, a novel vision-language multimodalframework designed for ZSAD. Specifically, we formulate anomaly detection as amultimodal contrastive learning problem, where DINOv3 is employed as the visualbackbone to extract patch tokens and a CLS token, and the CLIP text encoderprovides embeddings for both normal and abnormal prompts. To bridge the domaingap, lightweight adapters are introduced in both modalities, enabling theirrepresentations to be recalibrated for the anomaly detection task. Beyond thisbaseline alignment, we further design an Anomaly-Aware Calibration Module(AACM), which explicitly guides the CLS token to attend to anomalous regionsrather than generic foreground semantics, thereby enhancing discriminability.Extensive experiments on eight industrial and medical benchmarks demonstratethat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,verifying its superiority as a general zero-shot anomaly detection framework.</description>
      <author>example@mail.com (Jingyi Yuan, Jianxiong Ye, Wenkang Chen, Chenqiang Gao)</author>
      <guid isPermaLink="false">2509.14084v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction</title>
      <link>http://arxiv.org/abs/2509.14037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PhenoGnet的新型基于图的对比学习框架，通过整合基因功能相互作用网络与人类表型本体来预测疾病相似性，并在基准测试中表现出色，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;理解疾病相似性对于推进诊断、药物发现和个性化治疗策略至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发PhenoGnet框架，通过整合基因功能相互作用网络与人类表型本体来预测疾病相似性。&lt;h4&gt;方法&lt;/h4&gt;PhenoGnet包含两个关键组件：内视图模型使用图卷积网络和图注意力网络分别编码基因和表型图；跨视图模型作为共享权重的多层感知机，通过对比学习对齐基因和表型嵌入。模型使用已知的基因-表型关联作为正对，随机采样的无关对作为负对进行训练，疾病通过其关联基因和/或表型的平均嵌入表示，通过余弦相似度计算成对相似度。&lt;h4&gt;主要发现&lt;/h4&gt;在包含1100个相似和866个不相似疾病对的基准测试中，基于基因的嵌入实现了0.9012的AUCPR和0.8764的AUROC，优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;PhenoGnet能够捕捉超越直接重叠的潜在生物学关系，为疾病相似性预测提供了可扩展且可解释的解决方案，具有在罕见疾病研究和精准医学中应用的潜力。&lt;h4&gt;翻译&lt;/h4&gt;理解疾病相似性对于推进诊断、药物发现和个性化治疗策略至关重要。我们提出了PhenoGnet，一种新颖的基于图的对比学习框架，旨在通过整合基因功能相互作用网络与人类表型本体来预测疾病相似性。PhenoGnet包含两个关键组件：一个内视图模型，分别使用图卷积网络和图注意力网络编码基因和表型图；以及一个跨视图模型，作为共享权重的多层感知机实现，通过对比学习对齐基因和表型嵌入。模型使用已知的基因-表型关联作为正对，随机采样的无关对作为负对进行训练。疾病通过其关联基因和/或表型的平均嵌入表示，成对相似度通过余弦相似度计算。在包含1100个相似和866个不相似疾病对的精选基准上的评估展示了强劲的性能，基于基因的嵌入实现了0.9012的AUCPR和0.8764的AUROC，优于现有的最先进方法。值得注意的是，PhenoGnet能够捕捉超越直接重叠的潜在生物学关系，为疾病相似性预测提供了可扩展且可解释的解决方案。这些结果强调了其在罕见疾病研究和精准医学中下游应用的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding disease similarity is critical for advancing diagnostics, drugdiscovery, and personalized treatment strategies. We present PhenoGnet, a novelgraph-based contrastive learning framework designed to predict diseasesimilarity by integrating gene functional interaction networks with the HumanPhenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-viewmodel that separately encodes gene and phenotype graphs using GraphConvolutional Networks (GCNs) and Graph Attention Networks (GATs), and a crossview model implemented as a shared weight multilayer perceptron (MLP) thataligns gene and phenotype embeddings through contrastive learning. The model istrained using known gene phenotype associations as positive pairs and randomlysampled unrelated pairs as negatives. Diseases are represented by the meanembeddings of their associated genes and/or phenotypes, and pairwise similarityis computed via cosine similarity. Evaluation on a curated benchmark of 1,100similar and 866 dissimilar disease pairs demonstrates strong performance, withgene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764,outperforming existing state of the art methods. Notably, PhenoGnet captureslatent biological relationships beyond direct overlap, offering a scalable andinterpretable solution for disease similarity prediction. These resultsunderscore its potential for enabling downstream applications in rare diseaseresearch and precision medicine.</description>
      <author>example@mail.com (Ranga Baminiwatte, Kazi Jewel Rana, Aaron J. Masino)</author>
      <guid isPermaLink="false">2509.14037v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation</title>
      <link>http://arxiv.org/abs/2509.14036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出基于问题的手语翻译（QB-SLT）任务，探索对话上下文的高效集成，并开发了SSL-SSAF方法，利用问题文本和手语序列进行特征对齐和自适应提取，在两个新数据集上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;手语翻译旨在弥合聋人与健听人之间的沟通鸿沟，对话提供重要上下文线索。传统方法依赖手语标注（gloss annotations），但标注过程较为困难，而对话自然存在于交流中且更易标注。&lt;h4&gt;目的&lt;/h4&gt;探索对话在手语翻译中的高效集成，解决多模态特征对齐挑战，利用问题上下文提高翻译质量，开发一种利用易获取问题文本替代难度较高手语标注的方法。&lt;h4&gt;方法&lt;/h4&gt;提出跨模态自监督学习与Sigmoid自注意力加权融合方法（SSL-SSAF），使用对比学习对齐多模态特征，引入Sigmoid自注意力加权模块进行自适应特征提取，通过自监督学习利用问题文本增强表示和翻译能力。&lt;h4&gt;主要发现&lt;/h4&gt;SSL-SSAF在CSL-Daily-QA和PHOENIX-2014T-QA数据集上取得最先进性能；易获取的问题文本辅助可达到甚至超过手语标注的性能；可视化结果表明整合对话有助于提高翻译质量。&lt;h4&gt;结论&lt;/h4&gt;通过引入对话上下文特别是问题文本，可以显著改进手语翻译效果，无需依赖难度较高的手语标注，同时达到或超过传统方法性能，为手语翻译领域提供新方向。&lt;h4&gt;翻译&lt;/h4&gt;手语翻译（SLT）弥合了聋人与健听人之间的沟通差距，其中对话为翻译提供了关键的上下文线索。基于这一基础概念，本文提出了基于问题的手语翻译（QB-SLT），这是一个探索对话高效集成的新任务。与手语标注（手语转录）不同，对话自然出现在交流中，且更容易标注。关键挑战在于对齐多模态特征，同时利用问题上下文提高翻译质量。为解决这个问题，我们提出了用于手语翻译的跨模态自监督学习与Sigmoid自注意力加权融合方法（SSL-SSAF）。具体来说，我们在QB-SLT中使用对比学习对齐多模态特征，然后引入Sigmoid自注意力加权（SSAW）模块，从问题和手语序列中进行自适应特征提取。此外，我们通过自监督学习利用可用的问题文本来增强表示和翻译能力。我们在新构建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上评估了我们的方法，SSL-SSAF取得了最先进的性能。值得注意的是，易于获取的问题文本辅助可以达到甚至超过手语标注的性能。此外，可视化结果表明，整合对话有助于提高翻译质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign Language Translation (SLT) bridges the communication gap between deafpeople and hearing people, where dialogue provides crucial contextual cues toaid in translation. Building on this foundational concept, this paper proposesQuestion-based Sign Language Translation (QB-SLT), a novel task that exploresthe efficient integration of dialogue. Unlike gloss (sign languagetranscription) annotations, dialogue naturally occurs in communication and iseasier to annotate. The key challenge lies in aligning multimodality featureswhile leveraging the context of the question to improve translation. To addressthis issue, we propose a cross-modality Self-supervised Learning with SigmoidSelf-attention Weighting (SSL-SSAW) fusion method for sign languagetranslation. Specifically, we employ contrastive learning to alignmultimodality features in QB-SLT, then introduce a Sigmoid Self-attentionWeighting (SSAW) module for adaptive feature extraction from question and signlanguage sequences. Additionally, we leverage available question text throughself-supervised learning to enhance representation and translationcapabilities. We evaluated our approach on newly constructed CSL-Daily-QA andPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,easily accessible question assistance can achieve or even surpass theperformance of gloss assistance. Furthermore, visualization results demonstratethe effectiveness of incorporating dialogue in improving translation quality.</description>
      <author>example@mail.com (Zekang Liu, Wei Feng, Fanhua Shang, Lianyu Hu, Jichao Feng, Liqing Gao)</author>
      <guid isPermaLink="false">2509.14036v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection</title>
      <link>http://arxiv.org/abs/2509.13853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accept ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种单阶段监督对比学习方法（OS-SCL）和TFgram特征，有效解决了无监督异常声音检测中的误报问题，并在DCASE 2020挑战赛上取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督异常声音检测旨在仅使用正常音频数据训练模型来检测未知异常声音。尽管自监督方法有所进步，但在处理来自不同机器的相同类型样本时，仍然存在频繁误报的问题。&lt;h4&gt;目的&lt;/h4&gt;解决无监督异常声音检测中处理来自不同机器的相同类型样本时出现的频繁误报问题。&lt;h4&gt;方法&lt;/h4&gt;提出单阶段监督对比学习（OS-SCL）技术，通过扰动嵌入空间中的特征并采用单阶段噪声监督对比学习方法；同时提出一种名为TFgram的时间-频率特征，从原始音频中提取以捕获异常声音检测的关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64%的AUC、88.42%的pAUC和89.24%的mAUC；使用TFgram特征后，性能提升到95.71%的AUC、90.23%的pAUC和91.23%的mAUC。&lt;h4&gt;结论&lt;/h4&gt;所提出的OS-SCL方法和TFgram特征能有效提高无监督异常声音检测的性能，显著减少了误报情况，源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常声音检测旨在仅使用正常音频数据训练模型来检测未知异常声音。尽管自监督方法有所进步，但在处理来自不同机器的相同类型样本时，仍然存在频繁误报的问题。本文介绍了一种名为单阶段监督对比学习（OS-SCL）的新型训练技术，通过扰动嵌入空间中的特征并采用单阶段噪声监督对比学习方法，显著解决了这一问题。在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64%的AUC、88.42%的pAUC和89.24%的mAUC。此外，还提出了一种名为TFgram的时间-频率特征，从原始音频中提取。该特征能有效捕获异常声音检测的关键信息，最终达到95.71%的AUC、90.23%的pAUC和91.23%的mAUC。源代码可在www.github.com/huangswt/OS-SCL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomalous sound detection aims to detect unknown anomaloussounds by training a model using only normal audio data. Despite advancementsin self-supervised methods, the issue of frequent false alarms when handlingsamples of the same type from different machines remains unresolved. This paperintroduces a novel training technique called one-stage supervised contrastivelearning (OS-SCL), which significantly addresses this problem by perturbingfeatures in the embedding space and employing a one-stage noisy supervisedcontrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.Additionally, a time-frequency feature named TFgram is proposed, which isextracted from raw audio. This feature effectively captures criticalinformation for anomalous sound detection, ultimately achieving 95.71\% AUC,90.23\% pAUC, and 91.23\% mAUC. The source code is available at:\underline{www.github.com/huangswt/OS-SCL}.</description>
      <author>example@mail.com (Shun Huang, Zhihua Fang, Liang He)</author>
      <guid isPermaLink="false">2509.13853v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Masked Feature Modeling Enhances Adaptive Segmentation</title>
      <link>http://arxiv.org/abs/2509.13801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种掩码特征建模方法，用于无监督域适应语义分割任务，通过在特征空间进行掩码和重建，提高了分割性能且不增加计算开销。&lt;h4&gt;背景&lt;/h4&gt;无监督域适应用于语义分割旨在将有标签源域的模型迁移到无标签目标域。虽然对比学习等辅助自监督任务已提高特征区分性，但掩码建模方法在此领域探索不足，主要因架构不兼容和优化目标不一致。&lt;h4&gt;目的&lt;/h4&gt;提出掩码特征建模作为辅助任务，直接在特征空间进行掩码和重建，使其学习目标与主要分割任务对齐，确保与标准架构兼容。&lt;h4&gt;方法&lt;/h4&gt;提出掩码特征建模方法，引入轻量级辅助模块Rebuilder进行联合训练但在推理时丢弃，利用分割解码器对重建特征进行分类，将辅助目标与逐像素预测任务紧密耦合。&lt;h4&gt;主要发现&lt;/h4&gt;在各种架构和UDA基准上的大量实验表明，MFM一致提高分割性能，是一种简单、高效且可推广的策略。&lt;h4&gt;结论&lt;/h4&gt;MFM为无监督域适应语义分割提供了简单、高效且通用的解决方案，通过将辅助目标与主要任务对齐，避免了与主要任务的干扰。&lt;h4&gt;翻译&lt;/h4&gt;无监督域适应用于语义分割旨在将有标签源域的模型迁移到无标签目标域。虽然辅助自监督任务，特别是对比学习，已提高特征区分性，但掩码建模方法在此设置中探索不足，主要因架构不兼容和优化目标不一致。我们提出掩码特征建模，一种在特征空间直接执行特征掩码和重建的新型辅助任务。与现有掩码建模方法不同，MFM将其学习目标与主要分割任务对齐，确保与DeepLab和DAFormer等标准架构兼容，无需修改推理流程。为有效重建，我们引入轻量级辅助模块Rebuilder，该模块联合训练但在推理时丢弃，测试时零计算开销。关键是，MFM利用分割解码器对重建特征进行分类，将辅助目标与逐像素预测任务紧密耦合，避免干扰主要任务。在各种架构和UDA基准上的大量实验表明，MFM一致提高分割性能，为无监督域适应语义分割提供了简单、高效且可推广的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised domain adaptation (UDA) for semantic segmentation aims totransfer models from a labeled source domain to an unlabeled target domain.While auxiliary self-supervised tasks-particularly contrastive learning-haveimproved feature discriminability, masked modeling approaches remainunderexplored in this setting, largely due to architectural incompatibility andmisaligned optimization objectives. We propose Masked Feature Modeling (MFM), anovel auxiliary task that performs feature masking and reconstruction directlyin the feature space. Unlike existing masked modeling methods that reconstructlow-level inputs or perceptual features (e.g., HOG or visual tokens), MFMaligns its learning target with the main segmentation task, ensuringcompatibility with standard architectures like DeepLab and DAFormer withoutmodifying the inference pipeline. To facilitate effective reconstruction, weintroduce a lightweight auxiliary module, Rebuilder, which is trained jointlybut discarded during inference, adding zero computational overhead at testtime. Crucially, MFM leverages the segmentation decoder to classify thereconstructed features, tightly coupling the auxiliary objective with thepixel-wise prediction task to avoid interference with the primary task.Extensive experiments across various architectures and UDA benchmarksdemonstrate that MFM consistently enhances segmentation performance, offering asimple, efficient, and generalizable strategy for unsupervised domain-adaptivesemantic segmentation.</description>
      <author>example@mail.com (Wenlve Zhou, Zhiheng Zhou, Tiantao Xian, Yikui Zhai, Weibin Wu, Biyun Ma)</author>
      <guid isPermaLink="false">2509.13801v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI</title>
      <link>http://arxiv.org/abs/2509.13767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint submitted to ICASSP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VocSegMRI的多模态框架，通过交叉注意力融合整合视频、音频和音韵输入，实现了在实时磁共振成像中更准确的发音结构分割。&lt;h4&gt;背景&lt;/h4&gt;在实时磁共振成像(rtMRI)中准确分割发音结构具有挑战性，因为大多数现有方法几乎完全依赖于视觉线索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够整合声学和音韵信号的多模态框架，以提高发音结构分割的精度和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出VocSegMRI多模态框架，通过交叉注意力融合整合视频、音频和音韵输入实现动态特征对齐，并引入对比学习目标以增强跨模态表示，使模型在推理时音频模态不可用的情况下仍能保持良好性能。&lt;h4&gt;主要发现&lt;/h4&gt;在USC-75 rtMRI数据集子集上评估，VocSegMRI实现了最先进的性能，Dice得分为0.95，第95百分位Hausdorff距离为4.20毫米，优于单模态和多模态基线；消融研究证实了交叉注意力和对比学习对分割精度和鲁棒性的贡献。&lt;h4&gt;结论&lt;/h4&gt;集成多模态建模对于准确的声道分析具有重要价值，VocSegMRI框架能够有效利用多种信号源提高分割性能。&lt;h4&gt;翻译&lt;/h4&gt;在实时磁共振成像中准确分割发音结构仍然具有挑战性，因为大多数现有方法几乎完全依赖于视觉线索。然而，同步的声学和音韵信号提供了补充的上下文，可以丰富视觉信息并提高精度。在本文中，我们引入了VocSegMRI，一个通过交叉注意力融合整合视频、音频和音韵输入的多模态框架，用于动态特征对齐。为了进一步增强跨模态表示，我们纳入了对比学习目标，即使在推理时音频模态不可用，也能提高分割性能。在USC-75 rtMRI数据集的一个子集上评估，我们的方法实现了最先进的性能，Dice得分为0.95，第95百分位Hausdorff距离为4.20毫米，优于单模态和多模态基线。消融研究证实了交叉注意力和对比学习对分割精度和鲁棒性的贡献。这些结果强调了集成多模态建模对准确声道分析的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately segmenting articulatory structures in real-time magnetic resonanceimaging (rtMRI) remains challenging, as most existing methods rely almostentirely on visual cues. Yet synchronized acoustic and phonological signalsprovide complementary context that can enrich visual information and improveprecision. In this paper, we introduce VocSegMRI, a multimodal framework thatintegrates video, audio, and phonological inputs through cross-attention fusionfor dynamic feature alignment. To further enhance cross-modal representation,we incorporate a contrastive learning objective that improves segmentationperformance even when the audio modality is unavailable at inference. Evaluatedon a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-artperformance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.Ablation studies confirm the contributions of cross-attention and contrastivelearning to segmentation precision and robustness. These results highlight thevalue of integrative multimodal modeling for accurate vocal tract analysis.</description>
      <author>example@mail.com (Daiqi Liu, Tomás Arias-Vergara, Johannes Enk, Fangxu Xing, Maureen Stone, Jerry L. Prince, Jana Hutter, Andreas Maier, Jonghye Woo, Paula Andrea Pérez-Toro)</author>
      <guid isPermaLink="false">2509.13767v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization</title>
      <link>http://arxiv.org/abs/2509.13474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为语义增强跨模态地点识别（SCM-PR）的框架，通过结合RGB图像的高层语义与LiDAR地图的几何信息，提高了在无GPS环境中的机器人定位精度，特别是在复杂场景和视点变化的情况下。&lt;h4&gt;背景&lt;/h4&gt;在没有GPS能力的环境中确保机器人精确定位具有挑战性。现有基于RGB的视觉地点识别技术对光照、天气和季节性变化敏感，而现有的跨模态定位方法虽利用RGB图像和3D LiDAR地图的几何特性来减少这些敏感性问题，但在复杂场景、细粒度匹配和视点变化情况下仍表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合高层语义的框架，利用RGB图像在LiDAR地图中实现更鲁棒的定位，解决当前方法在复杂场景、细粒度或高分辨率匹配以及视点变化情况下的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了SCM-PR框架，包含：1) 使用VMamba骨干网络进行RGB图像特征提取；2) 语义感知特征融合（SAFF）模块，同时使用地点描述符和分割掩码；3) 结合语义和几何的LiDAR描述符；4) 在NetVLAD中引入跨模态语义注意力机制提高匹配性能；5) 在对比学习框架中设计了多视图语义几何匹配和语义一致性损失。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和KITTI-360数据集上的实验表明，SCM-PR与其他跨模态地点识别方法相比实现了最先进的性能，特别是在复杂场景和视点变化的情况下表现更优。&lt;h4&gt;结论&lt;/h4&gt;语义信息的整合对于在复杂环境中实现鲁棒的机器人定位至关重要。SCM-PR框架有效解决了现有方法在复杂场景、细粒度匹配和视点变化情况下的局限性，提高了无GPS环境中的定位精度。&lt;h4&gt;翻译&lt;/h4&gt;确保机器人在没有GPS能力的环境中精确定位是一项具有挑战性的任务。视觉地点识别技术有可能实现这一目标，但现有的基于RGB的方法对光照、天气和其他季节性变化敏感。现有的跨模态定位方法利用RGB图像和3D LiDAR地图的几何特性来减少上述敏感性问题。目前，最先进的方法在复杂场景、细粒度或高分辨率匹配以及视点变化的情况下表现不佳。在这项工作中，我们提出了一个名为语义增强跨模态地点识别的框架，该框架结合利用RGB图像的高层语义，在LiDAR地图中实现鲁棒的定位。我们提出的方法引入：用于RGB图像特征提取的VMamba骨干网络；用于同时使用地点描述符和分割掩码的语义感知特征融合模块；结合语义和几何的LiDAR描述符；以及在NetVLAD中引入跨模态语义注意力机制以提高匹配性能。整合语义信息对于设计多视图语义几何匹配和语义一致性损失也至关重要，两者都在对比学习框架中。我们在KITTI和KITTI-360数据集上的实验表明，与其他跨模态地点识别方法相比，SCM-PR实现了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人在没有GPS信号的环境中准确定位的挑战。传统基于RGB图像的定位方法对光照、天气和季节变化敏感，而现有跨模态方法在复杂场景、细粒度匹配和视角变化时表现不佳。这个问题在现实世界中非常重要，因为随着自动驾驶和机器人技术的发展，特别是在GPS信号不可靠或不可用的环境（如城市峡谷、室内、地下空间等），机器人需要可靠的定位能力来安全导航。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有跨模态方法主要依赖低级几何特征，在几何相似但语义不同的场景中会产生歧义。他们思考通过整合高级语义信息可以提供更丰富的上下文理解，增强定位鲁棒性。作者借鉴了多项现有工作：使用VMamba作为RGB图像特征提取的骨干网络，采用预训练3D语义分割模型处理激光雷达数据，利用NetVLAD结构生成全局描述符，以及应用对比学习框架对齐不同模态特征。这些借鉴被创新性地组合并扩展，形成了独特的语义增强跨模态定位框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过深度融合高级语义信息来增强跨模态定位的鲁棒性和准确性，使系统能理解场景中的物体和结构而不仅仅是匹配外观或形状。整体流程分为三部分：1) RGB图像分支使用VMamba骨干网络提取特征，通过SAFF模块生成位置描述符和语义掩码，并应用跨模态语义注意力机制；2) 激光雷达分支将点云转换为距离图像，应用语义分割生成语义标签，构建语义-几何混合描述符，并生成多视角描述符；3) 全局描述符生成和匹配阶段使用NetVLAD结构，结合多视角语义-几何匹配方法，在对比学习框架中使用语义一致性损失进行训练，确保相同语义类别的特征在嵌入空间中聚集。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 语义增强跨模态定位框架(SCM-PR)；2) 语义感知特征融合(SAFF)模块，同时生成位置描述符和语义掩码；3) 语义-几何混合描述符，结合几何和语义信息；4) 跨模态语义注意力机制，使RGB描述符关注激光雷达中对应语义区域；5) 多视角语义-几何匹配方法，同时考虑几何重叠和语义一致性；6) 语义一致性损失，确保相同语义类别的特征在嵌入空间中接近。相比之前工作，SCM-PR深度整合了高级语义信息而非仅依赖低级几何特征，实现了双向语义利用和细粒度语义对齐，在复杂环境和视角变化下表现更佳。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了语义增强跨模态定位框架(SCM-PR)，通过深度整合RGB图像的高级语义信息和激光雷达地图的几何信息，显著提高了在复杂环境和视角变化下的机器人定位准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring accurate localization of robots in environments without GPScapability is a challenging task. Visual Place Recognition (VPR) techniques canpotentially achieve this goal, but existing RGB-based methods are sensitive tochanges in illumination, weather, and other seasonal changes. Existingcross-modal localization methods leverage the geometric properties of RGBimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.Currently, state-of-the-art methods struggle in complex scenes, fine-grained orhigh-resolution matching, and situations where changes can occur in viewpoint.In this work, we introduce a framework we call Semantic-Enhanced Cross-ModalPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGBimages for robust localization in LiDAR maps. Our proposed method introduces: aVMamba backbone for feature extraction of RGB images; a Semantic-Aware FeatureFusion (SAFF) module for using both place descriptors and segmentation masks;LiDAR descriptors that incorporate both semantics and geometry; and across-modal semantic attention mechanism in NetVLAD to improve matching.Incorporating the semantic information also was instrumental in designing aMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both ina contrastive learning framework. Our experimental work on the KITTI andKITTI-360 datasets show that SCM-PR achieves state-of-the-art performancecompared to other cross-modal place recognition methods.</description>
      <author>example@mail.com (Yujia Lin, Nicholas Evans)</author>
      <guid isPermaLink="false">2509.13474v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Supervised and Unsupervised Deep Learning Applied to the Majority Vote Model</title>
      <link>http://arxiv.org/abs/2509.14155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用深度学习技术研究多数投票模型中的连续相变临界性质，结合主成分分析方法，通过监督学习和无监督学习准确识别临界点并估计临界指数。&lt;h4&gt;背景&lt;/h4&gt;多数投票模型的连续相变临界性质研究传统上依赖蒙特卡洛方法，本研究探索使用深度学习技术替代或补充传统方法。&lt;h4&gt;目的&lt;/h4&gt;利用深度学习技术探究多数投票模型的相变临界性质，并验证深度学习方法在相变研究中的有效性和准确性。&lt;h4&gt;方法&lt;/h4&gt;结合深度学习和主成分分析，通过监督学习在动力学蒙特卡洛生成的自旋构型数据上训练密集神经网络；使用主成分分析重现磁化；应用变分自编码器进行深度无监督学习来重构和生成自旋构型。&lt;h4&gt;主要发现&lt;/h4&gt;神经网络能够准确识别正方形和三角形晶格上的临界点；变分自编码器通过损失函数检测相变；真实数据与重构数据之间的关联函数在临界点具有普适性；变分自编码器可作为生成模型产生人工自旋构型。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法能够有效研究相变临界性质，变分自编码器不仅能检测相变，还能作为生成模型产生符合物理规律的人工自旋构型。&lt;h4&gt;翻译&lt;/h4&gt;我们采用深度学习技术研究多数投票模型中连续相变的临界性质。除了深度学习外，还利用主成分分析来分析相变。对于监督学习，我们在通过动力学蒙特卡洛方法生成的自旋构型数据上训练密集神经网络。使用独立模拟的构型数据，神经网络能够准确识别正方形和三角形晶格上的临界点。使用主成分分析的经典无监督学习重现了磁化，并能够估计通常通过蒙特卡洛重要性采样获得的临界指数。此外，使用变分自编码器进行深度无监督学习，它们重构输入自旋构型并生成人工输出。自编码器通过损失函数检测相变，量化了基本数据特征的保留。我们定义了真实数据与重构数据之间的关联函数，发现该关联函数在临界点是普适的。变分自编码器还可作为生成模型，产生人工自旋构型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We employ deep learning techniques to investigate the critical properties ofthe continuous phase transition in the majority vote model. In addition to deeplearning, principal component analysis is utilized to analyze the transition.For supervised learning, dense neural networks are trained on spinconfiguration data generated via the kinetic Monte Carlo method. Usingindependently simulated configuration data, the neural network accuratelyidentifies the critical point on both square and triangular lattices. Classicalunsupervised learning with principal component analysis reproduces themagnetization and enables estimation of critical exponents, typically obtainedvia Monte Carlo importance sampling. Furthermore, deep unsupervised learning isperformed using variational autoencoders, which reconstruct input spinconfigurations and generate artificial outputs. The autoencoders detect thephase transition through the loss function, quantifying the preservation ofessential data features. We define a correlation function between the real andreconstructed data, and find that this correlation function is universal at thecritical point. Variational autoencoders also serve as generative models,producing artificial spin configurations.</description>
      <author>example@mail.com (J. F. Silva Neto, D. S. M. Alencar, L. T. Brito, G. A. Alves, F. W. S. Lima, A. Macedo-Filho, R. S. Ferreira, T. F. A. Alves)</author>
      <guid isPermaLink="false">2509.14155v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data</title>
      <link>http://arxiv.org/abs/2509.13725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于智能手表的系统，用于测量和预测社交焦虑大学生的日常焦虑波动，实现了约60%的准确率，为实时个性化干预提供了可能。&lt;h4&gt;背景&lt;/h4&gt;社交焦虑是一种常见心理健康问题，与学业、社交和职业功能显著相关。其核心特征是在社交情境中的即时焦虑升高，但之前很少有研究测量或预测这种焦虑的日常波动。&lt;h4&gt;目的&lt;/h4&gt;捕捉社交焦虑的日内动态，为设计实时、个性化的干预措施(如JITAIs)提供依据。&lt;h4&gt;方法&lt;/h4&gt;对91名社交焦虑大学生(排除后72名)进行研究，使用定制智能手表系统，平均持续9.03天；每天进行7次生态瞬时评估(EMA)报告状态焦虑；基于外部心率数据开发基础模型，迁移表示并进行微调以生成概率预测；将这些预测与特质水平测量结合到元学习器中。&lt;h4&gt;主要发现&lt;/h4&gt;在数据集中，管道在状态焦虑检测中达到60.4%的平衡准确率；在TILES-18数据集的独立保留集上(10,095次每日EMA)，该方法达到59.1%的平衡准确率，优于先前工作至少7%。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效预测社交焦虑的日常波动，为个性化实时干预提供了可能性，有助于改善社交焦虑人群的日常功能。&lt;h4&gt;翻译&lt;/h4&gt;社交焦虑是一种常见心理健康问题，与学业、社交和职业功能方面的重大挑战相关。一个核心特征是在社交情境中即时(状态)焦虑升高，但之前很少有研究测量或预测这种焦虑在一天中的波动。捕捉这些日内动态对于设计实时、个性化的干预措施(如JITAIs)至关重要。为解决这一空白，我们对社交焦虑大学生(N=91；排除后72人)进行了研究，使用我们定制的基于智能手表的系统，平均9.03天(SD = 2.95)。参与者每天接受七次生态瞬时评估(EMA)以报告状态焦虑。我们在超过10,000天的外部心率数据上开发了基础模型，将其表示迁移到我们的数据集，并进行微调以生成概率预测。这些预测与特质水平测量在元学习器中结合。我们的管道在我们的数据集中实现了60.4%的状态焦虑检测平衡准确率。为了评估泛化能力，我们将训练方法应用于TILES-18数据集的独立保留集-与预训练相同的 dataset。在10,095次每日EMA上，我们的方法实现了59.1%的平衡准确率，优于先前工作至少7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social anxiety is a common mental health condition linked to significantchallenges in academic, social, and occupational functioning. A core feature iselevated momentary (state) anxiety in social situations, yet little prior workhas measured or predicted fluctuations in this anxiety throughout the day.Capturing these intra-day dynamics is critical for designing real-time,personalized interventions such as Just-In-Time Adaptive Interventions(JITAIs). To address this gap, we conducted a study with socially anxiouscollege students (N=91; 72 after exclusions) using our custom smartwatch-basedsystem over an average of 9.03 days (SD = 2.95). Participants received sevenecological momentary assessments (EMAs) per day to report state anxiety. Wedeveloped a base model on over 10,000 days of external heart rate data,transferred its representations to our dataset, and fine-tuned it to generateprobabilistic predictions. These were combined with trait-level measures in ameta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxietydetection in our dataset. To evaluate generalizability, we applied the trainingapproach to a separate hold-out set from the TILES-18 dataset-the same datasetused for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%balanced accuracy, outperforming prior work by at least 7%.</description>
      <author>example@mail.com (Md Sabbir Ahmed, Noah French, Mark Rucker, Zhiyuan Wang, Taylor Myers-Brower, Kaitlyn Petz, Mehdi Boukhechba, Bethany A. Teachman, Laura E. Barnes)</author>
      <guid isPermaLink="false">2509.13725v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models</title>
      <link>http://arxiv.org/abs/2509.13706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种自然语言处理工具，用于自动检测放射肿瘤学中的高严重程度事件报告，解决了人工审查耗时且需要专业知识的问题。&lt;h4&gt;背景&lt;/h4&gt;事件报告是医疗保健中安全和质量改进的重要工具，但人工审查耗时且需要专业知识。&lt;h4&gt;目的&lt;/h4&gt;展示一个自然语言处理（NLP）筛查工具，用于在两个医疗机构中检测放射肿瘤学中的高严重程度事件报告。&lt;h4&gt;方法&lt;/h4&gt;使用两个文本来训练和评估NLP模型：7,094份来自我们机构（Inst.）的报告和571份来自IAEA SAFRON（SF）的报告，所有报告都由临床内容专家标记了严重程度分数。训练和评估了两种类型的模型：基线支持向量机（SVM）和BlueBERT（一种在PubMed摘要和住院患者数据上预训练的大型语言模型）。通过两种方式评估了模型的泛化能力：使用在Inst.-train上训练的模型在SF-test上进行评估；训练了一个BlueBERT_TRANSFER模型，先在Inst-train上微调，然后在SF-train上再次微调，最后在SF-test集上测试。为了进一步分析模型性能，还检查了我们机构数据集中的59份报告子集，这些报告经过人工编辑以提高清晰度。&lt;h4&gt;主要发现&lt;/h4&gt;在Inst.测试上的分类性能，SVM达到AUROC 0.82，BlueBERT达到0.81。在没有跨机构迁移学习的情况下，在SF测试上的性能有限，SVM仅为0.42，BlueBERT为0.56。在两个数据集上均进行微调的BlueBERT_TRANSFER将SF测试的性能提高到AUROC 0.78。SVM和BlueBERT_TRANSFER模型在人工编辑的Inst.报告上的性能（AUROC 0.85和0.74）与人类性能（AUROC 0.81）相似。&lt;h4&gt;结论&lt;/h4&gt;成功地在放射肿瘤学中心的事件报告文本上开发了跨机构NLP模型。这些模型能够在经过整理的数据集上像人类一样检测高严重程度报告。&lt;h4&gt;翻译&lt;/h4&gt;目的：事件报告是医疗保健中安全和质量改进的重要工具，但人工审查耗时且需要专业知识。在此，我们展示了一个自然语言处理（NLP）筛查工具，用于在两个医疗机构中检测放射肿瘤学中的高严重程度事件报告。方法与材料：我们使用两个文本来训练和评估我们的NLP模型：7,094份来自我们机构（Inst.）的报告和571份来自IAEA SAFRON（SF）的报告，所有报告都由临床内容专家标记了严重程度分数。我们训练和评估了两种类型的模型：基线支持向量机（SVM）和BlueBERT（一种在PubMed摘要和住院患者数据上预训练的大型语言模型）。我们通过两种方式评估了模型的泛化能力。首先，我们评估了使用Inst.-train训练后在SF-test上测试的模型。其次，我们训练了一个BlueBERT_TRANSFER模型，先在Inst.-train上微调，然后在SF-train上再次微调，最后在SF-test集上测试。为了进一步分析模型性能，我们还检查了我们机构数据集中的59份报告子集，这些报告经过人工编辑以提高清晰度。结果：在Inst.测试上的分类性能，SVM达到AUROC 0.82，BlueBERT达到0.81。在没有跨机构迁移学习的情况下，在SF测试上的性能有限，SVM仅为0.42，BlueBERT为0.56。在两个数据集上均进行微调的BlueBERT_TRANSFER将SF测试的性能提高到AUROC 0.78。SVM和BlueBERT_TRANSFER模型在人工编辑的Inst.报告上的性能（AUROC 0.85和0.74）与人类性能（AUROC 0.81）相似。结论：总之，我们成功地在放射肿瘤学中心的事件报告文本上开发了跨机构NLP模型。这些模型能够在经过整理的数据集上像人类一样检测高严重程度报告。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PURPOSE: Incident reports are an important tool for safety and qualityimprovement in healthcare, but manual review is time-consuming and requiressubject matter expertise. Here we present a natural language processing (NLP)screening tool to detect high-severity incident reports in radiation oncologyacross two institutions.  METHODS AND MATERIALS: We used two text datasets to train and evaluate ourNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEASAFRON (SF), all of which had severity scores labeled by clinical contentexperts. We trained and evaluated two types of models: baseline support vectormachines (SVM) and BlueBERT which is a large language model pretrained onPubMed abstracts and hospitalized patient data. We assessed forgeneralizability of our model in two ways. First, we evaluated models trainedusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model thatwas first fine-tuned on Inst.-train then on SF-train before testing on SF-testset. To further analyze model performance, we also examined a subset of 59reports from our Inst. dataset, which were manually edited for clarity.  RESULTS Classification performance on the Inst. test achieved AUROC 0.82using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,improved the performance on SF test to AUROC 0.78. Performance of SVM, andBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and0.74) was similar to human performance (AUROC 0.81).  CONCLUSION: In summary, we successfully developed cross-institution NLPmodels on incident report text from radiation oncology centers. These modelswere able to detect high-severity reports similarly to humans on a curateddataset.</description>
      <author>example@mail.com (Peter Beidler, Mark Nguyen, Kevin Lybarger, Ola Holmberg, Eric Ford, John Kang)</author>
      <guid isPermaLink="false">2509.13706v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning</title>
      <link>http://arxiv.org/abs/2509.13624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Camera-ready version. Accepted to appear in the proceedings of the  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个分析框架，通过构建迁移学习矩阵和降维技术，研究了大型语言模型在跨任务迁移学习中的交互作用，揭示了影响模型性能的关键因素。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型被广泛应用于各种任务，但这些任务通常是模型在训练过程中未遇到的。为所有任务枚举和获取高质量训练数据是不可行的，因此需要依赖迁移学习，使用具有不同特性的数据集，并处理分布外请求。&lt;h4&gt;目的&lt;/h4&gt;提出一个分析框架，用于研究跨任务交互，理解迁移学习的效果和副作用。&lt;h4&gt;方法&lt;/h4&gt;构建迁移学习矩阵，使用降维技术，训练并分析10个模型，识别潜在能力（如推理、情感分类、自然语言理解、算术等），发现迁移学习的副作用。&lt;h4&gt;主要发现&lt;/h4&gt;性能提升通常无法基于表面层面的数据集相似性或源数据质量来解释，源数据集的隐藏统计因素（如类别分布和生成长度倾向）更有影响力，特定的语言特征也起着重要作用。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了对迁移学习复杂动态的见解，为更可预测和有效的LLM适应铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型正被越来越多地部署到各种应用中。这通常包括模型在训练过程中未遇到的任务。这意味着枚举和获取所有任务的高质量训练数据是不可行的。因此，我们通常需要依赖使用具有不同特性的数据集进行迁移学习，并预期分布外的请求。受这一实际需求的启发，我们提出了一个分析框架，通过构建迁移学习矩阵和降维技术，来剖析这些跨任务交互。我们训练并分析了10个模型，以识别潜在能力（如推理、情感分类、自然语言理解、算术），并发现了迁移学习的副作用。我们的研究揭示，性能提升通常无法基于表面层面的数据集相似性或源数据质量来解释。相反，源数据集的隐藏统计因素，如类别分布和生成长度倾向，以及特定的语言特征，实际上更有影响力。这项工作为迁移学习的复杂动态提供了见解，为更可预测和有效的LLM适应铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models are increasingly deployed across diverse applications.This often includes tasks LLMs have not encountered during training. Thisimplies that enumerating and obtaining the high-quality training data for alltasks is infeasible. Thus, we often need to rely on transfer learning usingdatasets with different characteristics, and anticipate out-of-distributionrequests. Motivated by this practical need, we propose an analysis framework,building a transfer learning matrix and dimensionality reduction, to dissectthese cross-task interactions. We train and analyze 10 models to identifylatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)and discover the side effects of the transfer learning. Our findings revealthat performance improvements often defy explanations based on surface-leveldataset similarity or source data quality. Instead, hidden statistical factorsof the source dataset, such as class distribution and generation lengthproclivities, alongside specific linguistic features, are actually moreinfluential. This work offers insights into the complex dynamics of transferlearning, paving the way for more predictable and effective LLM adaptation.</description>
      <author>example@mail.com (Shambhavi Krishna, Atharva Naik, Chaitali Agarwal, Sudharshan Govindan, Taesung Lee, Haw-Shiuan Chang)</author>
      <guid isPermaLink="false">2509.13624v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Speaker-Independent Dysarthric Speech Severity Classification with DSSCNet and Cross-Corpus Adaptation</title>
      <link>http://arxiv.org/abs/2509.13442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Speaker-independent experiments on classification of dysarthric  speech severity&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了DSSCNet，一种新型深度神经网络架构，用于构音障碍言语的严重程度分类。该模型结合了卷积、挤压激励(SE)和残差网络，能够从梅尔频谱图中提取判别性表示。研究还提出了跨语料微调框架，并在两个基准语料库上进行了评估，结果显示该方法优于现有最先进技术。&lt;h4&gt;背景&lt;/h4&gt;构音障碍言语严重程度分类对于运动性言语障碍患者的客观临床评估和进展监测至关重要。尽管先前方法已解决此任务，但在说话人独立(SID)场景中实现强大泛化能力仍具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效对构音障碍言语严重程度进行分类的方法，特别是在说话人独立场景中实现更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1) 提出DSSCNet架构，结合卷积、Squeeze-Excitation和残差网络；2) 使用SE块选择性关注重要特征；3) 提出基于检测的迁移学习方法改编的跨语料微调框架；4) 在TORGO和UA-Speech语料库上评估；5) 使用OSPS和LOSO评估协议。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在OSPS协议下，DSSCNet在TORGO和UA-Speech上分别达到56.84%和62.62%的准确率；2) 在LOSO设置下，分别达到63.47%和64.18%的准确率；3) 微调后性能显著提高：OSPS下分别达到75.80%和68.25%，LOSO下分别达到77.76%和79.44%；4) 所有结果均优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明了DSSCNet在跨不同构音障碍言语数据集进行细粒度严重程度分类方面的有效性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;构音障碍言语严重程度分类对于运动性言语障碍患者的客观临床评估和进展监测至关重要。尽管先前的方法已经解决了这一任务，但在说话人独立(SID)场景中实现强大的泛化能力仍然具有挑战性。这项工作介绍了DSSCNet，一种新型深度神经网络架构，它结合了卷积、挤压激励(SE)和残差网络，帮助它从梅尔频谱图中提取构音障碍言语的判别性表示。SE块的添加选择性关注构音障碍言语的重要特征，从而减少损失并提高整体模型性能。我们还提出了一个用于严重程度分类的跨语料微调框架，该框架基于检测的迁移学习方法改编而成。DSSCNet在两个基准构音障碍言语语料库(TORGO和UA-Speech)上进行了评估，使用说话人独立评估协议：每严重程度一位说话人(OSPS)和留一说话人法(LOSO)。在OSPS和LOSO设置下，DSSCNet在TORGO上分别达到56.84%和63.47%的准确率，在UA-Speech上分别达到62.62%和64.18%的准确率，优于现有的最先进方法。微调后，性能显著提高，在OSPS下，DSSCNet在TORGO和UA-Speech上分别达到75.80%和68.25%的准确率，在LOSO下分别达到77.76%和79.44%。这些结果证明了DSSCNet在跨不同构音障碍言语数据集进行细粒度严重程度分类方面的有效性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dysarthric speech severity classification is crucial for objective clinicalassessment and progress monitoring in individuals with motor speech disorders.Although prior methods have addressed this task, achieving robustgeneralization in speaker-independent (SID) scenarios remains challenging. Thiswork introduces DSSCNet, a novel deep neural architecture that combinesConvolutional, Squeeze-Excitation (SE), and Residual network, helping itextract discriminative representations of dysarthric speech from melspectrograms. The addition of SE block selectively focuses on the importantfeatures of the dysarthric speech, thereby minimizing loss and enhancingoverall model performance. We also propose a cross-corpus fine-tuning frameworkfor severity classification, adapted from detection-based transfer learningapproaches. DSSCNet is evaluated on two benchmark dysarthric speech corpora:TORGO and UA-Speech under speaker-independent evaluation protocols:One-Speaker-Per-Severity (OSPS) and Leave-One-Speaker-Out (LOSO) protocols.DSSCNet achieves accuracies of 56.84% and 62.62% under OSPS and 63.47% and64.18% under LOSO setting on TORGO and UA-Speech respectively outperformingexisting state-of-the-art methods. Upon fine-tuning, the performance improvessubstantially, with DSSCNet achieving up to 75.80% accuracy on TORGO and 68.25%on UA-Speech in OSPS, and up to 77.76% and 79.44%, respectively, in LOSO. Theseresults demonstrate the effectiveness and generalizability of DSSCNet forfine-grained severity classification across diverse dysarthric speech datasets.</description>
      <author>example@mail.com (Arnab Kumar Roy, Hemant Kumar Kathania, Paban Sapkota)</author>
      <guid isPermaLink="false">2509.13442v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>HAM: Hierarchical Adapter Merging for Scalable Continual Learning</title>
      <link>http://arxiv.org/abs/2509.13211v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HAM是一种新的持续学习框架，通过动态合并不同任务的适配器来解决灾难性遗忘问题，在处理大量任务时表现优异。&lt;h4&gt;背景&lt;/h4&gt;持续学习是人类认知的重要能力，但对当前深度学习模型构成挑战。主要问题是新知识会干扰已学习信息，导致灾难性遗忘。大型预训练模型部分缓解此问题，但在面对新数据分布时表现不佳。参数高效微调方法如LoRA能有效适应新知识，但在扩展到动态学习场景和长任务序列时仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效扩展到更多任务的方法，提高持续学习效率，减少每个任务使用一个适配器带来的复杂性和干扰可能性。&lt;h4&gt;方法&lt;/h4&gt;提出分层适配器合并（HAM）框架，在训练期间动态组合来自不同任务的适配器。HAM维护一组固定的组，分层次整合新适配器。对于每个任务，训练一个低秩适配器和一个重要性标量，然后基于适配器相似性动态分组任务。在每个组内，适配器被修剪、缩放和合并，促进相关任务间的迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;HAM能够有效扩展，管理比竞争基线更多的任务，同时提高效率。在三个视觉基准上的广泛实验表明，HAM显著优于最先进的方法，随着任务数量的增加，其优势尤其明显。&lt;h4&gt;结论&lt;/h4&gt;HAM框架为持续学习提供了一种有效解决方案，特别是在处理大量任务时。通过动态合并适配器，HAM能够更好地管理任务间的知识迁移，减少灾难性遗忘。&lt;h4&gt;翻译&lt;/h4&gt;持续学习是人类认知的基本能力，然而它对当前的深度学习模型提出了重大挑战。主要问题是新知识可能会干扰已学习的信息，导致模型为了新知识而遗忘早期知识，这种现象被称为灾难性遗忘。尽管大型预训练模型可以通过利用其现有知识和过参数化部分缓解遗忘问题，但当面对新的数据分布时，它们往往表现不佳。参数高效微调（PEFT）方法，如LoRA，能够有效地适应新知识。然而，在扩展到动态学习场景和长任务序列时，它们仍然面临挑战，因为为每个任务维护一个适配器会引入复杂性并增加干扰的可能性。在本文中，我们引入了分层适配器合并（HAM），这是一个新颖的框架，在训练期间动态组合来自不同任务的适配器。这种方法使HAM能够有效扩展，允许它以更高的效率管理比竞争基线更多的任务。为此，HAM维护一组固定的组，这些组分层次地整合新适配器。对于每个任务，HAM训练一个低秩适配器和一个重要性标量，然后基于适配器相似性动态分组任务。在每个组内，适配器被修剪、缩放和合并，促进相关任务之间的迁移学习。在三个视觉基准上的广泛实验表明，HAM显著优于最先进的方法，特别是随着任务数量的增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning is an essential capability of human cognition, yet itposes significant challenges for current deep learning models. The primaryissue is that new knowledge can interfere with previously learned information,causing the model to forget earlier knowledge in favor of the new, a phenomenonknown as catastrophic forgetting. Although large pre-trained models canpartially mitigate forgetting by leveraging their existing knowledge andover-parameterization, they often struggle when confronted with novel datadistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,enable efficient adaptation to new knowledge. However, they still facechallenges in scaling to dynamic learning scenarios and long sequences oftasks, as maintaining one adapter per task introduces complexity and increasesthe potential for interference. In this paper, we introduce HierarchicalAdapters Merging (HAM), a novel framework that dynamically combines adaptersfrom different tasks during training. This approach enables HAM to scaleeffectively, allowing it to manage more tasks than competing baselines withimproved efficiency. To achieve this, HAM maintains a fixed set of groups thathierarchically consolidate new adapters. For each task, HAM trains a low-rankadapter along with an importance scalar, then dynamically groups tasks based onadapter similarity. Within each group, adapters are pruned, scaled and merge,facilitating transfer learning between related tasks. Extensive experiments onthree vision benchmarks show that HAM significantly outperformsstate-of-the-art methods, particularly as the number of tasks increases.</description>
      <author>example@mail.com (Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2509.13211v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds</title>
      <link>http://arxiv.org/abs/2509.13390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to: Mechanical Systems and Signal Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于领域知识的模型选择方法，通过使用代理异常来改进汽车舱内声音异常检测模型的性能评估。&lt;h4&gt;背景&lt;/h4&gt;在汽车舱内声音异常检测中，由于标记的故障数据稀缺或完全缺失，这项任务更适合作为无监督学习问题而非监督学习问题。然而，在无监督设置中，由于缺乏标记的故障样本进行验证，以及常用指标的可靠性有限，有效的模型选择仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;为克服无监督学习环境下模型选择的挑战，提出一种基于领域知识的模型选择方法，通过使用代理异常来支持模型选择。&lt;h4&gt;方法&lt;/h4&gt;通过健康频谱图的结构化扰动构建代理异常，将其用于验证集以支持模型选择。在一个包含健康和故障舱内声音的高保真电动汽车数据集上评估该方法，该数据集涵盖了五种代表性故障类型：不平衡、调制、啸叫、风声和脉冲宽度调制。&lt;h4&gt;主要发现&lt;/h4&gt;在五种故障案例上的实验评估表明，使用代理异常选择最优模型显著优于传统的模型选择策略。&lt;h4&gt;结论&lt;/h4&gt;基于领域知识的模型选择方法，使用代理异常，能够有效解决无监督学习环境下汽车舱内声音异常检测中的模型选择挑战，显著提高了模型选择的性能。&lt;h4&gt;翻译&lt;/h4&gt;汽车舱内声音异常的检测对于确保车辆质量和维护乘客舒适度至关重要。在许多现实场景中，由于标记的故障数据稀缺或完全缺失，这项任务更适合作为无监督学习问题而非监督学习案例。在这种无监督设置中，模型仅在健康样本上训练，并将异常检测为正常行为的偏差。然而，由于缺乏标记的故障样本进行验证以及常用指标（如验证重建误差）的可靠性有限，有效的模型选择仍然是一个重大挑战。为克服这些限制，提出了一种基于领域知识的模型选择方法，其中通过健康频谱图的结构化扰动构建的代理异常用于验证集以支持模型选择。该方法在一个包含健康和故障舱内声音的高保真电动汽车数据集上进行了评估，该数据集涵盖了五种代表性故障类型：不平衡、调制、啸叫、风声和脉冲宽度调制。这个数据集使用先进的声音合成技术生成，并通过专家评审评估验证，已公开以促进进一步研究。在五种故障案例上的实验评估表明，使用代理异常选择最优模型显著优于传统的模型选择策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The detection of anomalies in automotive cabin sounds is critical forensuring vehicle quality and maintaining passenger comfort. In many real-worldsettings, this task is more appropriately framed as an unsupervised learningproblem rather than the supervised case due to the scarcity or complete absenceof labeled faulty data. In such an unsupervised setting, the model is trainedexclusively on healthy samples and detects anomalies as deviations from normalbehavior. However, in the absence of labeled faulty samples for validation andthe limited reliability of commonly used metrics, such as validationreconstruction error, effective model selection remains a significantchallenge. To overcome these limitations, a domain-knowledge-informed approachfor model selection is proposed, in which proxy-anomalies engineered throughstructured perturbations of healthy spectrograms are used in the validation setto support model selection. The proposed methodology is evaluated on ahigh-fidelity electric vehicle dataset comprising healthy and faulty cabinsounds across five representative fault types viz., Imbalance, Modulation,Whine, Wind, and Pulse Width Modulation. This dataset, generated using advancedsound synthesis techniques, and validated via expert jury assessments, has beenmade publicly available to facilitate further research. Experimentalevaluations on the five fault cases demonstrate the selection of optimal modelsusing proxy-anomalies, significantly outperform conventional model selectionstrategies.</description>
      <author>example@mail.com (Deepti Kunte, Bram Cornelis, Claudio Colangeli, Karl Janssens, Brecht Van Baelen, Konstantinos Gryllias)</author>
      <guid isPermaLink="false">2509.13390v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.13907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为White Aggregation and Restoration Module (WARM)的新型原型生成方法，用于解决少样本3D点云分割问题。该方法通过白化和彩色化变换之间的交叉注意力机制，解决了支持特征与原型令牌之间的分布不对齐问题，从而生成更具代表性的原型，显著提升了分割性能。&lt;h4&gt;背景&lt;/h4&gt;少样本3D点云分割(FS-PCS)旨在仅有少量标记样本的情况下预测未标记点云的每个点标签。现有方法使用传统算法构建原型，但初始随机性显著影响性能，且原型生成过程缺乏深入探索。&lt;h4&gt;目的&lt;/h4&gt;调查一种基于注意力机制的先进原型生成方法，以提高FS-PCS性能。&lt;h4&gt;方法&lt;/h4&gt;提出White Aggregation and Restoration Module (WARM)，通过在白化和彩色化变换之间夹入交叉注意力来解决分布不对齐问题。白化在注意力过程前将支持特征与原型令牌对齐，彩色化随后恢复原始分布到已处理令牌，使注意力更加鲁棒，捕获支持特征间的语义关系生成代表性原型。&lt;h4&gt;主要发现&lt;/h4&gt;现有简单模块在可学习的原型令牌和支持特征之间存在分布差距；WARM方法能有效解决这种分布不对齐问题，生成更具代表性的原型。&lt;h4&gt;结论&lt;/h4&gt;在多个FS-PCS基准测试上，该方法以显著优势实现了最先进的性能，通过大量实验证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;少样本3D点云分割(FS-PCS)旨在仅有少量标记样本的情况下预测未标记点云的每个点标签。为从有限的支持集中提取判别性表示，现有方法使用最远点采样等传统算法构建原型。然而，我们指出其初始随机性显著影响FS-PCS性能，且尽管原型生成过程被广泛使用，它仍然缺乏深入探索。这促使我们调查一种基于注意力机制的先进原型生成方法。尽管有潜力，我们发现简单模块在可学习的原型令牌和支持特征之间存在分布差距。为克服这一问题，我们提出了白化聚合与恢复模块(WARM)，通过在白化和彩色化变换之间夹入交叉注意力来解决不对齐问题。具体而言，白化在注意力过程前将支持特征与原型令牌对齐，随后彩色化将原始分布恢复到已处理令牌。这种简单而有效的设计使注意力更加鲁棒，从而通过捕获支持特征之间的语义关系生成代表性原型。我们的方法在多个FS-PCS基准测试上以显著优势实现了最先进的性能，通过大量实验证明了其有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决少样本3D点云语义分割(FS-PCS)中的原型生成问题。传统方法使用最远点采样(FPS)等算法构建原型，但这些方法存在初始随机性大、性能不稳定的问题。这个问题很重要，因为3D点云语义分割在自动驾驶、机器人等领域有广泛应用，而标记3D点云数据需要大量人工劳动，成本高昂。少样本学习可以减少对大量标记数据的依赖，原型生成作为FS-PCS的关键步骤，其质量直接影响分割性能，现有方法的不稳定性限制了FS-PCS的实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出传统FPS方法的局限性，特别是其初始随机性导致的性能不稳定。然后考虑使用注意力机制作为替代方案，因为注意力在其他少样本下游任务中表现出色。研究发现vanilla交叉注意力存在分布差距问题：可学习的原型令牌与支持特征之间分布不匹配。通过分析特征空间的分布差异，作者设计了白化和着色变换来解决这一问题。该方法借鉴了图像领域DETR和Mask2Former的注意力机制，以及标准的ZCA白化技术，并受到原型网络的启发，但将这些技术创新性地应用于3D点云FS-PCS场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'白化-交叉注意力-着色'三阶段流程解决原型令牌与支持特征之间的分布不匹配问题。白化阶段暂时移除支持特征的分布统计信息，使其与原型令牌对齐；交叉注意力阶段在共享空间中，原型令牌能够基于语义关系而非空间距离来聚合特征；着色阶段恢复原始分布统计信息，保留支持特征的固有特性。整体流程包括：使用主干网络提取特征；将支持特征分为前景和背景；对每类应用ZCA白化；使用白化后的特征通过交叉注意力生成临时原型；应用着色变换恢复原始分布；最后将查询点分配给最近原型进行分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出WARM模块解决FS-PCS中原型生成问题；2) 首次将白化和着色变换引入注意力机制，解决分布不匹配；3) 通过白化移除特征通道间相关性，使优化更稳定；4) 通过着色恢复原始特征特性。相比之前的工作：不同于FPS方法的随机性和不稳定性，WARM是确定性的；不同于简单注意力无法处理分布不匹配问题，WARM通过白化-着色变换解决了这一问题；不同于其他FS-PCS方法主要关注查询适应，WARM专注于改进原型生成本身；不同于3D领域通常使用非可学习令牌，WARM实现了可学习令牌的应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了WARM模块，通过白化和着色变换解决了少样本3D点云语义分割中原型生成的不稳定性和分布不匹配问题，显著提升了分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-pointlabels for an unlabeled point cloud, given only a few labeled examples. Toextract discriminative representations from the limited support set, existingmethods have constructed prototypes using conventional algorithms such asfarthest point sampling. However, we point out that its initial randomnesssignificantly affects FS-PCS performance and that the prototype generationprocess remains underexplored despite its prevalence. This motivates us toinvestigate an advanced prototype generation method based on attentionmechanism. Despite its potential, we found that vanilla module suffers from thedistributional gap between learnable prototypical tokens and support features.To overcome this, we propose White Aggregation and Restoration Module (WARM),which resolves the misalignment by sandwiching cross-attention betweenwhitening and coloring transformations. Specifically, whitening aligns thesupport features to prototypical tokens before attention process, andsubsequently coloring restores the original distribution to the attendedtokens. This simple yet effective design enables robust attention, therebygenerating representative prototypes by capturing the semantic relationshipsamong support features. Our method achieves state-of-the-art performance with asignificant margin on multiple FS-PCS benchmarks, demonstrating itseffectiveness through extensive experiments.</description>
      <author>example@mail.com (Jiyun Im, SuBeen Lee, Miso Lee, Jae-Pil Heo)</author>
      <guid isPermaLink="false">2509.13907v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Algorithm of the GLMY Homology on Digraphs</title>
      <link>http://arxiv.org/abs/2509.13862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对GLMY同调的量子算法，在拓扑数据分析领域比最佳经典算法具有显著优势，在一般情况下提供二次加速，在特定情况下提供指数级加速。&lt;h4&gt;背景&lt;/h4&gt;量子算法在拓扑数据分析中比最佳经典算法有明显优势。GLMY同调是由Alexander Grigor'yan等人引入的，不同于传统的点云上的单纯复形，它是在有向图上定义的拓扑数据分析新兴领域，最近受到越来越多的关注。&lt;h4&gt;目的&lt;/h4&gt;提出一个针对GLMY同调的量子算法，使其比最佳经典算法具有显著优势。&lt;h4&gt;方法&lt;/h4&gt;设计了一个通用编码协议，用于GLMY同调的量子态和边界算子；同时证明了GLMY同调的一个性质，为量子算法提供了理论保证。&lt;h4&gt;主要发现&lt;/h4&gt;GLMY同调的量子算法在一般情况下提供了二次加速，在输入数据以路径规范形式给出时提供了指数级的量子优势。&lt;h4&gt;结论&lt;/h4&gt;量子算法在GLMY同调计算中具有显著优势，为拓扑数据分析领域提供了新的计算方法。&lt;h4&gt;翻译&lt;/h4&gt;拓扑数据分析的量子算法比最佳经典算法具有显著优势。由Alexander Grigor'yan、Yong Lin、Yuri Muranov和Shing-Tung Yau引入的GLMY同调，与之前的点云上的单纯复形不同，它是在有向图上定义的，是拓扑数据分析中的一个新兴领域，最近吸引了越来越多的关注。我们提出了一个针对GLMY同调的量子算法，比最佳经典算法有显著优势。我们设计了GLMY同调在有向图上的量子态和边界算子的通用编码协议。并且证明了GLMY同调的一个性质，为量子算法提供了理论保证。GLMY同调的量子算法在一般情况下给出了二次加速，在输入数据以路径规范形式给出时给出了指数级的量子优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum algorithms for topological data analysis provide significantadvantage over the best classical algorithm. Different from the previoussimplical complex on points cloud, the GLMY homology introduced by AlexanderGrigor'yan, Yong Lin, Yuri Muranov and Shing-Tung Yau, is defined on digraphand is a arising realm in Topological Data Analysis (TDA), which attracts moreand more attention recently. We propose a quantum algorithm for the GLMYhomology with significant advantage over the best classical algorithm. Wedesign a universal encoding protocol for the quantum states and boundaryoperators of GLMY homology on digraphs. And a property of the GLMY homology isproved for the theoretical guarantee of the quantum algorithm. The quantumalgorithm for GLMY homology gives a quadratic speedup in general cases, and itgives an exponential quantum advantage in the case of the input data is givenas a specification of paths.</description>
      <author>example@mail.com (Yunpeng Zi, Muchun Yang, D. L. Zhou)</author>
      <guid isPermaLink="false">2509.13862v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap</title>
      <link>http://arxiv.org/abs/2509.13857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出InterKey，一种利用道路交叉路口作为独特地标的跨模态框架，实现自动驾驶车辆在GNSS受限环境中的全球定位。&lt;h4&gt;背景&lt;/h4&gt;可靠的全球定位对自动驾驶车辆至关重要，特别是在GNSS信号减弱或不可用的环境中。高清地图提供准确信息但成本高，限制了可扩展性；OpenStreetMap免费可用但抽象粗糙，与传感器数据匹配困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用道路交叉路口作为独特地标的全球定位方法，解决高清地图成本高和OSM抽象粗糙的问题。&lt;h4&gt;方法&lt;/h4&gt;提出InterKey跨模态框架，通过联合编码点云和OSM的道路和建筑物印记构建紧凑二进制描述符，并引入差异缓解、方向确定和面积均衡采样策略实现鲁棒跨模态匹配。&lt;h4&gt;主要发现&lt;/h4&gt;KITTI数据集实验表明，InterKey实现最先进准确性，大幅优于最近基线方法，框架可扩展至能生成密集结构点云的传感器。&lt;h4&gt;结论&lt;/h4&gt;InterKey为车辆定位提供了一种可扩展、经济高效的鲁棒解决方案。&lt;h4&gt;翻译&lt;/h4&gt;可靠的全球定位对自动驾驶车辆至关重要，特别是在GNSS信号减弱或不可用的环境中，如城市峡谷和隧道。虽然高清地图提供准确的先验信息，但数据收集、地图构建和维护的成本限制了其可扩展性。OpenStreetMap提供了免费且全球可用的替代方案，但其粗略的抽象表示给与传感器数据的匹配带来挑战。我们提出InterKey，一种利用道路交叉路口作为独特地标的跨模态框架。我们的方法通过联合编码来自点云和OSM的道路和建筑物印记来构建紧凑的二进制描述符。为了弥合模态差距，我们引入了差异缓解、方向确定和面积均衡采样策略，实现了鲁棒的跨模态匹配。KITTI数据集上的实验表明，InterKey实现了最先进的准确性，以较大优势优于最近的基线方法。该框架可以扩展到能够生成密集结构点云的传感器，为车辆定位提供了一种可扩展且经济高效的鲁棒解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在GNSS信号弱或不可用环境（如城市峡谷、隧道）中，自动驾驶汽车的全局定位问题。这个问题很重要，因为GNSS在这些环境中容易受到多路径效应和信号遮挡的影响导致定位不可靠，而高精度地图虽准确但成本高昂限制了可扩展性。准确的全局定位对自动驾驶汽车的初始化、故障恢复、路径规划等关键任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到GNSS在复杂环境中的局限性和高精度地图的成本问题，选择道路交叉口作为关键特征，因其具有独特几何配置且可从LiDAR数据可靠提取。他们结合道路和建筑物信息提高描述符区分度，并通过差异缓解、方向确定和面积等采样策略解决模态差距。该方法借鉴了作者之前关于交叉口检测的研究，参考了Scan Context等点云识别方法，以及OSM Context等点云到OSM的匹配方法，但创新性地专注于交叉口特征的跨模态匹配。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用道路交叉口作为点云和OSM之间的跨模态关键点，联合编码道路和建筑物印记生成紧凑二进制描述符，并通过专门策略解决模态差距。整体流程分为四步：1)从OSM提取交叉口节点并生成道路和建筑物印记；2)处理点云数据生成俯视图道路和建筑物印记并检测交叉口；3)通过差异缓解、方向确定和形状编码生成二进制描述符；4)匹配描述符检索最佳OSM交叉口并计算车辆全局姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首个专注于跨模态交叉口匹配的框架、结合道路和建筑物的描述符、模态差异缓解方案、方向确定策略和面积等采样方法。相比之前工作，不同之处在于：结合道路和建筑物信息而非仅依赖建筑物轮廓，提高区分度；采用面积等采样模式而非线性增长，能均匀捕捉周围形状；专注于交叉口这一独特特征减少搜索空间；专门设计解决点云和OSM间的模态差距，而非仅处理点云到点云匹配。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InterKey通过利用道路交叉口作为跨模态关键点，结合道路和建筑物信息生成紧凑描述符，并采用创新的采样策略，实现了在GNSS受限环境中基于免费OpenStreetMap的高效、准确的全车定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable global localization is critical for autonomous vehicles, especiallyin environments where GNSS is degraded or unavailable, such as urban canyonsand tunnels. Although high-definition (HD) maps provide accurate priors, thecost of data collection, map construction, and maintenance limits scalability.OpenStreetMap (OSM) offers a free and globally available alternative, but itscoarse abstraction poses challenges for matching with sensor data. We proposeInterKey, a cross-modal framework that leverages road intersections asdistinctive landmarks for global localization. Our method constructs compactbinary descriptors by jointly encoding road and building imprints from pointclouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,orientation determination, and area-equalized sampling strategies, enablingrobust cross-modal matching. Experiments on the KITTI dataset demonstrate thatInterKey achieves state-of-the-art accuracy, outperforming recent baselines bya large margin. The framework generalizes to sensors that can produce densestructural point clouds, offering a scalable and cost-effective solution forrobust vehicle localization.</description>
      <author>example@mail.com (Nguyen Hoang Khoi Tran, Julie Stephany Berrio, Mao Shan, Stewart Worrall)</author>
      <guid isPermaLink="false">2509.13857v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2509.13692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为HGACNet的新型框架，用于解决点云补全问题，通过分层编码3D几何特征并与RGB图像引导的先验信息融合，重建完整点云。&lt;h4&gt;背景&lt;/h4&gt;点云补全对机器人感知、物体重建及下游任务（如抓取规划、避障和操作）至关重要，但自遮挡和传感器限制导致的不完整几何结构会显著降低下游推理和交互质量。&lt;h4&gt;目的&lt;/h4&gt;解决由自遮挡和传感器限制导致的点云不完整问题，提高点云补全的准确性和适用性。&lt;h4&gt;方法&lt;/h4&gt;提出HGACNet框架，包含分层图注意力(HGA)编码器进行关键点选择和特征细化，多尺度跨模态融合(MSCF)模块实现几何特征与视觉特征的对齐，以及对比损失(C-Loss)对齐跨模态特征分布。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet-ViPC基准和YCB-Complete数据集上的实验证实HGACNet的有效性，展示了最先进的性能和强适用性。&lt;h4&gt;结论&lt;/h4&gt;HGACNet能够有效解决点云补全问题，在多个数据集上表现优异，并在实际机器人操作任务中具有很好的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;点云补全对机器人感知、物体重建和支持抓取规划、避障和操作等下游任务至关重要。然而，由自遮挡和传感器限制引起的不完整几何结构会显著降低下游推理和交互质量。为解决这些挑战，我们提出HGACNet，一种新型框架，通过分层编码3D几何特征并将其与单视图RGB图像引导的先验信息融合，重建单个物体的完整点云。我们方法的核心是分层图注意力(HGA)编码器，它通过基于图注意力的下采样自适应选择关键局部点，并逐步细化分层几何特征，以更好地捕捉结构连续性和空间关系。为加强跨模态交互，我们进一步设计了多尺度跨模态融合(MSCF)模块，在分层几何特征和结构化视觉表示之间进行基于注意力的特征对齐，实现补全的细粒度语义指导。此外，我们提出了对比损失(C-Loss)，明确对齐跨模态特征分布，提高模态差异下的补全保真度。最后，在ShapeNet-ViPC基准和YCB-Complete数据集上进行的广泛实验证实了HGACNet的有效性，展示了最先进的性能以及在现实世界机器人操作任务中的强适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云补全问题，即从部分点云和对应RGB图像重建完整的3D点云。这个问题在现实中非常重要，因为实际获取的点云数据常因传感器限制和物体自遮挡而不完整，这会严重影响机器人抓取、避障、物体识别等下游任务的可靠性。准确的点云补全对机器人感知、物体重建和交互操作至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：单模态方法难以处理遮挡和细粒度结构重建，而跨模态方法未能充分利用模态间关系。作者借鉴了图注意力机制、Transformer架构和对比学习等现有技术，但创新性地设计了分层图注意力编码器（HGA）和多尺度跨模态融合模块（MSCF），通过分层特征提取和跨模态注意力对齐，实现了更有效的点云补全。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过分层编码3D几何特征并融合图像引导的先验知识，实现高质量点云补全。整体流程为：1) 使用HGA编码器从部分点云提取全局和局部特征；2) 用Swin Transformer从RGB图像提取视觉特征；3) 通过MSCF模块融合几何和视觉特征；4) 应用对比损失(C-Loss)对齐不同模态特征；5) 将融合特征输入解码器重建完整点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 分层图注意力编码器(HGA)，自适应选择关键点并保留结构细节；2) 多尺度跨模态融合模块(MSCF)，实现几何与视觉特征的实例级对齐；3) 对比损失函数(C-Loss)，减少模态差异提高重建准确性。相比之前工作，HGACNet不是通过简单连接进行早期融合，而是通过分层注意力实现深度特征交互；在不同几何抽象层次上选择性应用注意力，平衡计算效率和重建质量；同时关注全局结构和细粒度细节，并引入对比学习明确对齐不同模态特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HGACNet通过分层图注意力编码和多尺度跨模态融合，有效结合点云几何特征与图像语义先验，实现了高质量、结构一致的三维点云补全，在机器人感知任务中取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is essential for robotic perception, objectreconstruction and supporting downstream tasks like grasp planning, obstacleavoidance, and manipulation. However, incomplete geometry caused byself-occlusion and sensor limitations can significantly degrade downstreamreasoning and interaction. To address these challenges, we propose HGACNet, anovel framework that reconstructs complete point clouds of individual objectsby hierarchically encoding 3D geometric features and fusing them withimage-guided priors from a single-view RGB image. At the core of our approach,the Hierarchical Graph Attention (HGA) encoder adaptively selects criticallocal points through graph attention-based downsampling and progressivelyrefines hierarchical geometric features to better capture structural continuityand spatial relationships. To strengthen cross-modal interaction, we furtherdesign a Multi-Scale Cross-Modal Fusion (MSCF) module that performsattention-based feature alignment between hierarchical geometric features andstructured visual representations, enabling fine-grained semantic guidance forcompletion. In addition, we proposed the contrastive loss (C-Loss) toexplicitly align the feature distributions across modalities, improvingcompletion fidelity under modality discrepancy. Finally, extensive experimentsconducted on both the ShapeNet-ViPC benchmark and the YCB-Complete datasetconfirm the effectiveness of HGACNet, demonstrating state-of-the-artperformance as well as strong applicability in real-world robotic manipulationtasks.</description>
      <author>example@mail.com (Yadan Zeng, Jiadong Zhou, Xiaohan Li, I-Ming Chen)</author>
      <guid isPermaLink="false">2509.13692v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization</title>
      <link>http://arxiv.org/abs/2509.13686v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RF-LSCM的新型无线信道建模框架，通过辐射场联合表示大尺度信号衰减和多径分量，解决了传统LSCM方法的局限性，实现了多小区、多网格和多频率信道建模，显著提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;精确的局部无线信道建模是蜂窝网络优化的基石，局部统计信道建模(LSCM)是当前最先进的信道建模框架。然而，传统LSCM方法仅限于单小区、单网格和单载波频率分析，无法捕捉复杂的跨域交互。&lt;h4&gt;目的&lt;/h4&gt;克服传统LSCM方法的局限性，开发一个能够捕捉复杂跨域交互的新型信道建模框架，实现多小区、多网格和多频率信道建模。&lt;h4&gt;方法&lt;/h4&gt;提出RF-LSCM框架，通过辐射场联合表示大尺度信号衰减和多径分量；引入多域LSCM公式和频率依赖衰减模型(FDAM)促进跨频率泛化；使用点云辅助的环境增强方法实现多小区和多网格信道建模；利用低秩张量表示和分层张量角度建模(HiTAM)算法提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;RF-LSCM在真实世界多小区数据集上显著优于最先进方法，覆盖预测的平均绝对误差减少高达30%，通过有效融合多频率数据，MAE提高了22%。&lt;h4&gt;结论&lt;/h4&gt;RF-LSCM是一种高效的信道建模框架，能够处理多小区、多网格和多频率数据，在保持细粒度准确性的同时，显著降低了GPU内存需求和训练时间。&lt;h4&gt;翻译&lt;/h4&gt;精确的局部无线信道建模是蜂窝网络优化的基石，能够在参数调整过程中可靠预测网络性能。局部统计信道建模(LSCM)是专为蜂窝网络优化定制的最先进信道建模框架。然而，传统LSCM方法从参考信号接收功率(RSRP)测量中推断信道的角度功率谱(APS)，存在关键局限性：它们通常仅限于单小区、单网格和单载波频率分析，无法捕捉复杂的跨域交互。为克服这些挑战，我们提出了RF-LSCM，一种新框架，通过辐射场联合表示大尺度信号衰减和多径分量来建模信道APS。RF-LSCM引入了多域LSCM公式，采用物理信息相关的频率依赖衰减模型(FDAM)促进跨频率泛化，以及点云辅助的环境增强方法实现多小区和多网格信道建模。此外，为解决典型神经辐射场的计算效率低下问题，RF-LSCM利用低秩张量表示，并辅以新型的分层张量角度建模(HiTAM)算法。这种高效设计显著降低了GPU内存需求和训练时间，同时保持了细粒度准确性。在真实世界多小区数据集上的广泛实验表明，RF-LSCM显著优于最先进的方法，在覆盖预测中平均绝对误差(MAE)减少高达30%，并通过有效融合多频率数据，MAE提高了22%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localized wireless channel modeling is a cornerstone of cellularnetwork optimization, enabling reliable prediction of network performanceduring parameter tuning. Localized statistical channel modeling (LSCM) is thestate-of-the-art channel modeling framework tailored for cellular networkoptimization. However, traditional LSCM methods, which infer the channel'sAngular Power Spectrum (APS) from Reference Signal Received Power (RSRP)measurements, suffer from critical limitations: they are typically confined tosingle-cell, single-grid and single-carrier frequency analysis and fail tocapture complex cross-domain interactions. To overcome these challenges, wepropose RF-LSCM, a novel framework that models the channel APS by jointlyrepresenting large-scale signal attenuation and multipath components within aradiance field. RF-LSCM introduces a multi-domain LSCM formulation with aphysics-informed frequency-dependent Attenuation Model (FDAM) to facilitate thecross frequency generalization as well as a point-cloud-aided environmentenhanced method to enable multi-cell and multi-grid channel modeling.Furthermore, to address the computational inefficiency of typical neuralradiance fields, RF-LSCM leverages a low-rank tensor representation,complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.This efficient design significantly reduces GPU memory requirements andtraining time while preserving fine-grained accuracy. Extensive experiments onreal-world multi-cell datasets demonstrate that RF-LSCM significantlyoutperforms state-of-the-art methods, achieving up to a 30% reduction in meanabsolute error (MAE) for coverage prediction and a 22% MAE improvement byeffectively fusing multi-frequency data.</description>
      <author>example@mail.com (Bingsheng Peng, Shutao Zhang, Xi Zheng, Ye Xue, Xinyu Qin, Tsung-Hui Chang)</author>
      <guid isPermaLink="false">2509.13686v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Deep Lookup Network</title>
      <link>http://arxiv.org/abs/2509.13662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的查找表操作替代卷积神经网络中的乘法操作，以提高计算效率和降低能耗，同时保持与标准卷积网络相当的性能。作者构建了可微分的查找表并提出了训练策略，在多种任务上验证了查找网络的高效性。&lt;h4&gt;背景&lt;/h4&gt;卷积神经网络由大量不同类型的操作构成，计算密集度高。其中乘法操作计算复杂度高，通常比其他操作消耗更多能量且需要更长的推理时间，阻碍了卷积神经网络在移动设备上的部署。在资源受限的边缘设备上，可通过查找表计算复杂操作以降低计算成本。&lt;h4&gt;目的&lt;/h4&gt;引入一种通用的、高效的查找操作作为构建神经网络的基本操作，用简单的查找操作替代权重与激活值之间的乘法计算，提高神经网络的效率。&lt;h4&gt;方法&lt;/h4&gt;构建可微分的查找表，提出几种训练策略促进收敛，用查找操作替代计算密集的乘法操作，开发了用于图像分类、图像超分辨率和点云分类任务的查找网络。&lt;h4&gt;主要发现&lt;/h4&gt;查找网络能够在能耗和推理速度方面实现更高的效率，同时保持与标准卷积网络相竞争的性能。在不同任务（分类和回归）和数据类型（图像和点云）上都取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过用查找操作替代乘法操作，成功开发了高效的查找网络，在保持竞争力的同时显著提高了计算效率和降低了能耗，适用于资源受限的边缘设备。&lt;h4&gt;翻译&lt;/h4&gt;卷积神经网络由大量不同类型的操作构成，计算密集度高。在这些操作中，乘法操作的计算复杂度较高，通常比其他操作消耗更多能量且需要更长的推理时间，这阻碍了卷积神经网络在移动设备上的部署。在许多资源受限的边缘设备上，可以通过查找表来计算复杂操作以降低计算成本。受此启发，在本文中，我们引入了一种通用的、高效的查找操作，可作为构建神经网络的基本操作。我们采用简单而高效的查找操作来计算权重和激活值的响应，而不是直接计算它们的乘积。为了使查找操作能够端到端优化，我们以可微分的方式构建查找表，并提出了几种训练策略来促进其收敛。通过用我们的查找操作替代计算密集的乘法操作，我们开发了用于图像分类、图像超分辨率和点云分类任务的查找网络。实验证明，我们的查找网络能够受益于查找操作，在能耗和推理速度方面实现更高的效率，同时保持与标准卷积网络相竞争的性能。大量实验表明，我们的查找网络在不同任务（分类和回归任务）和数据类型（图像和点云）上都取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Convolutional neural networks are constructed with massive operations withdifferent types and are highly computationally intensive. Among theseoperations, multiplication operation is higher in computational complexity andusually requires {more} energy consumption with longer inference time thanother operations, which hinders the deployment of convolutional neural networkson mobile devices. In many resource-limited edge devices, complicatedoperations can be calculated via lookup tables to reduce computational cost.Motivated by this, in this paper, we introduce a generic and efficient lookupoperation which can be used as a basic operation for the construction of neuralnetworks. Instead of calculating the multiplication of weights and activationvalues, simple yet efficient lookup operations are adopted to compute theirresponses. To enable end-to-end optimization of the lookup operation, weconstruct the lookup tables in a differentiable manner and propose severaltraining strategies to promote their convergence. By replacing computationallyexpensive multiplication operations with our lookup operations, we developlookup networks for the image classification, image super-resolution, and pointcloud classification tasks. It is demonstrated that our lookup networks canbenefit from the lookup operations to achieve higher efficiency in terms ofenergy consumption and inference speed while maintaining competitiveperformance to vanilla convolutional networks. Extensive experiments show thatour lookup networks produce state-of-the-art performance on different tasks(both classification and regression tasks) and different data types (bothimages and point clouds).</description>
      <author>example@mail.com (Yulan Guo, Longguang Wang, Wendong Mao, Xiaoyu Dong, Yingqian Wang, Li Liu, Wei An)</author>
      <guid isPermaLink="false">2509.13662v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Object Pose Estimation through Dexterous Touch</title>
      <link>http://arxiv.org/abs/2509.13591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;结合触觉传感和强化学习的方法，能够主动探索物体表面以识别关键位姿特征，无需先验几何知识。&lt;h4&gt;背景&lt;/h4&gt;机器人在抓取和交互任务中需要稳健的目标位姿估计，特别是在视觉数据有限或对光照、遮挡和外观敏感的场景中。触觉传感器通常提供有限和局部的接触信息，这使得从部分数据重建位姿具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，通过主动控制机器人手与物体交互，来稳健地估计物体位姿。&lt;h4&gt;方法&lt;/h4&gt;使用强化学习进行训练以探索和收集触觉数据。收集到的3D点云用于迭代优化物体的形状和位姿。在一个设置中，一只手稳定地握住物体，另一只手执行主动探索。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够主动探索物体表面以识别关键的位姿特征，无需事先了解物体的几何形状。&lt;h4&gt;结论&lt;/h4&gt;通过触觉传感和强化学习训练的主动探索，可以有效地从有限数据中估计物体位姿。&lt;h4&gt;翻译&lt;/h4&gt;稳健的目标位姿估计对于机器人中的抓取和交互任务至关重要，特别是在视觉数据有限或对光照、遮挡和外观敏感的场景中。触觉传感器通常提供有限和局部的接触信息，这使得从部分数据重建位姿具有挑战性。我们的方法使用传感运动探索来主动控制机器人手与物体交互。我们使用强化学习进行训练以探索和收集触觉数据。收集到的3D点云用于迭代优化物体的形状和位姿。在我们的设置中，一只手稳定地握住物体，而另一只手执行主动探索。我们表明，我们的方法能够主动探索物体表面以识别关键的位姿特征，无需事先了解物体的几何形状。补充材料和更多演示将在https://amirshahid.github.io/BimanualTactilePose提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是通过触觉来估计物体姿态的问题。这个问题在现实中很重要，因为视觉方法在光照变化、遮挡或物体外观等因素影响下表现不稳定，而触觉传感器提供了一种紧凑、低成本的替代方案，可以直接嵌入到机器人手指尖。准确的物体姿态估计对机器人的操作和交互任务至关重要，特别是在视觉受限的环境中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类通过触摸和交互重建物体形状的能力启发，设计了双臂协作系统：一个手稳定握住物体，另一个手进行主动探索。作者设计了丰富的状态表示（包括手指配置、手部旋转、触摸状态等）和多目标奖励函数（平衡姿态估计和探索）。该方法借鉴了强化学习、基于好奇心的奖励函数、FoundationPose姿态估计和点云重建等现有工作，但创新性地将它们整合到一个完整的触觉探索框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双臂协作，一个手稳定握住物体，另一个手使用触觉传感器主动探索物体表面，收集接触点云数据，并利用这些数据迭代重建物体形状和估计物体姿态。整体流程包括：1)初始化：双手向物体移动；2)握持调整：一只手找到稳定抓取姿态；3)探索阶段：探索手收集触觉数据；4)姿态估计：通过点云重建、深度图像渲染和FoundationPose进行姿态优化；5)强化学习训练：使用PPO算法训练探索策略，结合多目标奖励函数引导机器人收集有价值的信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双臂触态探索框架，使用简单FSR传感器实现仅触觉的姿态估计；2)新颖的状态表示和奖励函数，促进在有限动作内的探索；3)将姿态估计反馈整合到奖励函数中，引导探索关键特征；4)无需物体模板即可准确估计姿态。相比之前的工作，本文方法不依赖被动接触或预定义交互，解决了物体固定假设的限制，不仅关注表面覆盖(IoU)更关注姿态估计准确性(ADD-S)，在100步内达到87%的准确率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于强化学习的双臂触觉探索框架，通过主动收集触觉数据并利用姿态估计反馈，实现了仅使用简单触觉传感器就能高效准确地估计未知物体姿态。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust object pose estimation is essential for manipulation and interactiontasks in robotics, particularly in scenarios where visual data is limited orsensitive to lighting, occlusions, and appearances. Tactile sensors often offerlimited and local contact information, making it challenging to reconstruct thepose from partial data. Our approach uses sensorimotor exploration to activelycontrol a robot hand to interact with the object. We train with ReinforcementLearning (RL) to explore and collect tactile data. The collected 3D pointclouds are used to iteratively refine the object's shape and pose. In oursetup, one hand holds the object steady while the other performs activeexploration. We show that our method can actively explore an object's surfaceto identify critical pose features without prior knowledge of the object'sgeometry. Supplementary material and more demonstrations will be provided athttps://amirshahid.github.io/BimanualTactilePose .</description>
      <author>example@mail.com (Amir-Hossein Shahidzadeh, Jiyue Zhu, Kezhou Chen, Sha Yi, Cornelia Fermüller, Yiannis Aloimonos, Xiaolong Wang)</author>
      <guid isPermaLink="false">2509.13591v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>An Exploratory Study on Abstract Images and Visual Representations Learned from Them</title>
      <link>http://arxiv.org/abs/2509.14149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了由基本形状构成的抽象图像在传达视觉语义信息方面的能力，以及其与传统栅格图像表示之间的性能差异。&lt;h4&gt;背景&lt;/h4&gt;最近的研究表明，由基本形状构建的抽象图像确实能够向深度学习模型传达视觉语义信息，但这类图像获得的表示通常不如从传统栅格图像获得的表示。&lt;h4&gt;目的&lt;/h4&gt;研究抽象图像与传统栅格图像之间性能差距的原因，并调查在不同抽象级别能捕获多少高级语义内容。&lt;h4&gt;方法&lt;/h4&gt;引入了分层抽象图像数据集(HAID)，这是一个包含从正常栅格图像在多个抽象级别生成的抽象图像的新数据集。在各种任务(包括分类、分割和目标检测)上训练和评估传统视觉系统，提供栅格化和抽象图像表示之间的综合研究。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体的研究结果，主要介绍了研究方法和数据集的构建。&lt;h4&gt;结论&lt;/h4&gt;探讨抽象图像是否可以被视为传达视觉语义信息的潜在有效格式，以及其是否有助于视觉任务。&lt;h4&gt;翻译&lt;/h4&gt;想象一下生活在一个完全由基本形状构成的世界中，你还能认出熟悉的物体吗？最近的研究表明，由基本形状构建的抽象图像确实能够向深度学习模型传达视觉语义信息。然而，从这类图像获得的表示通常不如从传统栅格图像获得的表示。在本文中，我们研究这种性能差距背后的原因，并调查在不同抽象级别能捕获多少高级语义内容。为此，我们引入了分层抽象图像数据集(HAID)，这是一个新颖的数据收集，包含从正常栅格图像在多个抽象级别生成的抽象图像。然后我们在各种任务(包括分类、分割和目标检测)上训练和评估传统视觉系统，提供了栅格化和抽象图像表示之间的综合研究。我们还讨论了抽象图像是否可以被视为传达视觉语义信息的潜在有效格式，以及是否有助于视觉任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imagine living in a world composed solely of primitive shapes, could youstill recognise familiar objects? Recent studies have shown that abstractimages-constructed by primitive shapes-can indeed convey visual semanticinformation to deep learning models. However, representations obtained fromsuch images often fall short compared to those derived from traditional rasterimages. In this paper, we study the reasons behind this performance gap andinvestigate how much high-level semantic content can be captured at differentabstraction levels. To this end, we introduce the Hierarchical AbstractionImage Dataset (HAID), a novel data collection that comprises abstract imagesgenerated from normal raster images at multiple levels of abstraction. We thentrain and evaluate conventional vision systems on HAID across various tasksincluding classification, segmentation, and object detection, providing acomprehensive study between rasterised and abstract image representations. Wealso discuss if the abstract image can be considered as a potentially effectiveformat for conveying visual semantic information and contributing to visiontasks.</description>
      <author>example@mail.com (Haotian Li, Jianbo Jiao)</author>
      <guid isPermaLink="false">2509.14149v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts</title>
      <link>http://arxiv.org/abs/2509.14104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将Soft mixture-of-experts (MoE)机制集成到遥感基础模型中的方法，创建了CSMoE模型，并引入主题-气候描述符采样策略构建训练集。实验表明该方法在降低计算需求的同时保持或提高了表示性能，实现了现有模型两倍以上的计算效率。&lt;h4&gt;背景&lt;/h4&gt;自监督学习通过掩码自编码器在遥感基础模型发展中受到广泛关注，能够促进不同传感器和下游任务中的表示学习。然而，现有遥感基础模型通常在训练和推理过程中存在巨大的计算复杂性，或者表示能力有限，限制了它们在遥感领域的实际应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有遥感基础模型计算复杂度高和表示能力有限的问题，提高模型的效率和实用性。&lt;h4&gt;方法&lt;/h4&gt;1. 将Soft mixture-of-experts (MoE)机制集成到基础模型中；2. 将此方法应用于Cross-Sensor Masked Autoencoder (CSMAE)模型，创建了Cross-Sensor Mixture-of-Experts (CSMoE)模型；3. 引入基于主题-气候描述符的采样策略构建具有代表性和多样性的训练集。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在场景分类、语义分割和基于内容的图像检索等任务上，该方法在降低计算需求的同时保持或提高了表示性能；2. 与最先进的遥感基础模型相比，CSMoE在表示能力、准确性和计算效率之间取得了更好的平衡；3. 平均而言，CSMoE实现了现有遥感基础模型两倍以上的计算效率，同时在所有实验中保持了有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的适应方法对于创建计算高效的遥感基础模型是有效的，通过集成Soft MoE机制，实现了特定模态的专家专业化与共享的跨传感器表示学习之间的平衡。&lt;h4&gt;翻译&lt;/h4&gt;通过掩码自编码器的自监督学习在遥感基础模型发展中引起了广泛关注，促进了不同传感器和下游任务中的改进表示学习。然而，现有的遥感基础模型通常在训练和推理过程中存在巨大的计算复杂性，或者表示能力有限。这些问题限制了它们在遥感领域的实际应用。为解决这一限制，我们提出了一种适应方法，通过将Soft mixture-of-experts (MoE)机制集成到基础模型中来提高遥感基础模型的效率。将Soft MoEs集成到基础模型中，允许特定模态的专家专业化与共享的跨传感器表示学习相结合。为了证明我们适应方法的有效性，我们将其应用于Cross-Sensor Masked Autoencoder (CSMAE)模型，创建了Cross-Sensor Mixture-of-Experts (CSMoE)模型。此外，我们引入了一种基于主题-气候描述符的采样策略，用于构建具有代表性和多样性的训练集来训练我们的CSMoE模型。在场景分类、语义分割和基于内容的图像检索方面的广泛实验表明，我们的适应方法在降低计算需求的同时保持了或提高了表示性能。与最先进的遥感基础模型相比，CSMoE在表示能力、准确性和计算效率之间取得了更好的平衡。平均而言，CSMoE实现了现有遥感基础模型两倍以上的计算效率，同时在所有实验中保持了有竞争力的性能。这些结果表明，所提出的适应方法对于创建计算高效的遥感基础模型是有效的。该模型的代码、训练集创建和模型权重将在https://git.tu-berlin.de/rsim/csmoe上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning through masked autoencoders has attracted greatattention for remote sensing (RS) foundation model (FM) development, enablingimproved representation learning across diverse sensors and downstream tasks.However, existing RS FMs often either suffer from substantial computationalcomplexity during both training and inference or exhibit limitedrepresentational capacity. These issues restrict their practical applicabilityin RS. To address this limitation, we propose an adaptation for enhancing theefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanisminto the FM. The integration of Soft MoEs into the FM allows modality-specificexpert specialization alongside shared cross-sensor representation learning. Todemonstrate the effectiveness of our adaptation, we apply it on theCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-SensorMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climaticdescriptor-driven sampling strategy for the construction of a representativeand diverse training set to train our CSMoE model. Extensive experiments onscene classification, semantic segmentation, and content-based image retrievaldemonstrate that our adaptation yields a reduction in computationalrequirements while maintaining or improving representational performance.Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-offbetween representational capacity, accuracy, and computational efficiency. Onaverage, CSMoE achieves more than twice the computational efficiency ofexisting RS FMs, while maintaining competitive performance across allexperiments. These results show the effectiveness of the proposed adaptationfor creating computationally efficient RS FMs. The code for the model, thetraining set creation, and the model weights will be available athttps://git.tu-berlin.de/rsim/csmoe.</description>
      <author>example@mail.com (Leonard Hackel, Tom Burgert, Begüm Demir)</author>
      <guid isPermaLink="false">2509.14104v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection</title>
      <link>http://arxiv.org/abs/2509.13878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种混合LoRA专家方法，通过集成多个低秩适配器和路由机制，提高了音频深度伪造检测模型的泛化能力，特别是在面对新型深度伪造方法时表现更佳&lt;h4&gt;背景&lt;/h4&gt;Wav2Vec2等基础模型在语音任务（包括音频深度伪造检测）的表示学习中表现出色。然而，在固定的一组真实和欺骗性音频片段上进行微调后，它们通常无法泛化到训练中未包含的新型深度伪造方法&lt;h4&gt;目的&lt;/h4&gt;解决基础模型在音频深度伪造检测中泛化能力不足的问题，提高模型对不断演变的深度伪造攻击的适应性&lt;h4&gt;方法&lt;/h4&gt;提出了一种混合LoRA专家方法，将多个低秩适配器（LoRA）集成到模型的注意力层中，并使用路由机制选择性地激活专门的专家&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在领域内和领域外场景中都优于标准微调，相比基线模型降低了等错误率；最好的MoE-LoRA模型将平均领域外EER从8.55%降低到6.08%&lt;h4&gt;结论&lt;/h4&gt;该混合LoRA专家方法在实现可泛化的音频深度伪造检测方面有效&lt;h4&gt;翻译&lt;/h4&gt;基础模型如Wav2Vec2在语音任务（包括音频深度伪造检测）的表示学习中表现出色。然而，在固定的一组真实和欺骗性音频片段上进行微调后，它们通常无法泛化到训练中未包含的新型深度伪造方法。为此，我们提出了一种混合LoRA专家方法，将多个低秩适配器（LoRA）集成到模型的注意力层中。路由机制选择性地激活专门的专家，增强了对不断演变的深度伪造攻击的适应性。实验结果表明，我们的方法在领域内和领域外场景中都优于标准微调，降低了相对于基线模型的等错误率。值得注意的是，我们最好的MoE-LoRA模型将平均领域外EER从8.55%降低到6.08%，证明了其在实现可泛化的音频深度伪造检测方面的有效性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models such as Wav2Vec2 excel at representation learning in speechtasks, including audio deepfake detection. However, after being fine-tuned on afixed set of bonafide and spoofed audio clips, they often fail to generalize tonovel deepfake methods not represented in training. To address this, we proposea mixture-of-LoRA-experts approach that integrates multiple low-rank adapters(LoRA) into the model's attention layers. A routing mechanism selectivelyactivates specialized experts, enhancing adaptability to evolving deepfakeattacks. Experimental results show that our method outperforms standardfine-tuning in both in-domain and out-of-domain scenarios, reducing equal errorrates relative to baseline models. Notably, our best MoE-LoRA model lowers theaverage out-of-domain EER from 8.55\% to 6.08\%, demonstrating itseffectiveness in achieving generalizable audio deepfake detection.</description>
      <author>example@mail.com (Janne Laakkonen, Ivan Kukanov, Ville Hautamäki)</author>
      <guid isPermaLink="false">2509.13878v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.13846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2025: 1st Place in Transformer track and 2nd Place in  Convolution track of SSL3D-OpenMind challenge&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究挑战了表征学习中关于非相关视图足以学习有意义表征的假设，提出了一种名为'Consistent View Alignment'的自监督学习方法，通过结构化视图对齐提高下游任务性能，并在MICCAI 2025 SSL3D挑战赛中取得优异成绩。&lt;h4&gt;背景&lt;/h4&gt;近年来，表征学习方法隐含地假设数据点的非相关视图足以学习对各种下游任务有意义的表征。&lt;h4&gt;目的&lt;/h4&gt;挑战这一假设，证明潜在空间中的有意义结构不会自然出现，必须被明确诱导。&lt;h4&gt;方法&lt;/h4&gt;提出一种方法，对齐来自数据不同视图的表征，以对齐互补信息而不产生假阳性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的自监督学习方法'Consistent View Alignment'提高了下游任务的性能，突显了结构化视图对齐在学习有效表征中的关键作用。&lt;h4&gt;结论&lt;/h4&gt;结构化视图对齐对于学习有效表征至关重要。&lt;h4&gt;翻译&lt;/h4&gt;表征学习中的许多近期方法隐含地假设，数据点的非相关视图足以学习对各种下游任务有意义的表征。在本工作中，我们挑战这一假设，证明潜在空间中的有意义结构不会自然出现，而必须被明确诱导。我们提出了一种方法，对齐来自数据不同视图的表征，以对齐互补信息而不产生假阳性。我们的实验表明，我们提出的自监督学习方法'Consistent View Alignment'提高了下游任务的性能，突显了结构化视图对齐在学习有效表征中的关键作用。当使用Primus视觉变换器和ResEnc卷积神经网络时，我们的方法在MICCAI 2025 SSL3D挑战赛中分别获得第一名和第二名。代码和预训练模型权重已在https://github.com/Tenbatsu24/LatentCampus发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自监督学习中当数据视图之间相关性较弱时的'假阳性'问题。在医学图像分析中，简单的随机裁剪可能导致两个视图代表完全不同的语义信息，而现有对比学习方法仍会强制模型对这些不相关特征进行对齐，导致虚假关联并降低表示质量。这个问题很重要，因为它直接影响医学AI中下游任务（如分割、分类）的性能，进而关系到诊断的准确性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有对比学习方法（如SimCLR、SwAV）假设正样本对完美相关，但在医学图像等复杂场景中这一假设常被违反。他们注意到现有方法很少直接约束特征空间中对齐应该发生的位置。因此，作者设计了'一致视图对齐'方法，通过生成具有重叠区域的视图对，并在特征空间中对齐这些重叠区域，只在这些区域应用一致性损失。作者借鉴了对比学习中的学生-教师架构、掩码自编码器的重建目标、SwAV的对称化损失计算以及VoCo的体积特定增强方法，但创新性地将它们组合以解决假阳性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过确保只有真正语义对应的区域被对齐来避免假阳性问题，而不是简单地假设两个随机视图应该相似。整体流程包括：1)从原始3D医学图像生成两个具有重叠区域(40%-80%)的随机裁剪；2)使用学生和教师网络提取特征；3)应用掩码自编码器目标重建被遮蔽区域；4)使用ROIAlign对齐重叠区域的特征；5)计算对齐区域间的特征相似度；6)在两个视图方向上对称化计算损失；7)结合重建损失、一致性损失和可选的对比损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)通过一致视图构造缓解假阳性问题；2)通过强制局部特征一致性改善下游分割任务；3)在不同空间上下文和语义内容间强制特征一致性。相比之前工作的不同：与传统对比学习不同，CVA不假设整个视图相似，只对齐已知重叠区域；与掩码自编码器不同，CVA添加了显式特征对齐步骤；与对比掩码自编码器不同，CVA主动识别并纠正假阳性；与其他3D医学图像预训练方法不同，CVA更关注减少假阳性而非假阴性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CVA通过显式对齐不同视图中的语义对应区域而非强制对齐整个视图，有效缓解了自监督学习中的假阳性问题，显著提高了3D医学图像分割任务的表现，同时展示了局部特征一致性与全局判别特征学习之间的权衡关系。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many recent approaches in representation learning implicitly assume thatuncorrelated views of a data point are sufficient to learn meaningfulrepresentations for various downstream tasks. In this work, we challenge thisassumption and demonstrate that meaningful structure in the latent space doesnot emerge naturally. Instead, it must be explicitly induced. We propose amethod that aligns representations from different views of the data to aligncomplementary information without inducing false positives. Our experimentsshow that our proposed self-supervised learning method, Consistent ViewAlignment, improves performance for downstream tasks, highlighting the criticalrole of structured view alignment in learning effective representations. Ourmethod achieved first and second place in the MICCAI 2025 SSL3D challenge whenusing a Primus vision transformer and ResEnc convolutional neural network,respectively. The code and pretrained model weights are released athttps://github.com/Tenbatsu24/LatentCampus.</description>
      <author>example@mail.com (Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink)</author>
      <guid isPermaLink="false">2509.13846v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>SAMIR, an efficient registration framework via robust feature learning from SAM</title>
      <link>http://arxiv.org/abs/2509.13629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAMIR的高效医学图像配准框架，利用Segment Anything Model (SAM)增强特征提取，显著提升了配准性能。&lt;h4&gt;背景&lt;/h4&gt;医学图像配准是医学图像分析的基础任务，变形通常与组织的形态特征密切相关，准确的特征提取至关重要。然而，现有的弱监督方法需要分割掩码或地标等解剖先验，这些标签往往难以获取，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;利用视觉基础模型的强大表示学习能力，开发一种不依赖弱标签的高效医学图像配准方法，提高配准精度和实用性。&lt;h4&gt;方法&lt;/h4&gt;使用SAM的图像编码器提取结构感知的特征嵌入，而非直接使用原始输入图像；设计特定任务的自适应管道和轻量级3D头在嵌入空间内细化特征，适应医学图像中的局部变形；引入分层特征一致性损失来引导粗到细的特征匹配，改善解剖对齐。&lt;h4&gt;主要发现&lt;/h4&gt;SAMIR在受试者内心脏图像配准和受试者间腹部CT图像配准任务上显著优于现有方法，在ACDC数据集上实现了2.68%的性能提升，在腹部数据集上实现了6.44%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;SAMIR是一种有效的医学图像配准框架，利用SAM可以增强特征提取，提高配准性能，且代码将在论文接受后公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;图像配准是医学图像分析中的基础任务。变形通常与组织的形态特征密切相关，使得准确的特征提取至关重要。近期的弱监督方法通过整合分割掩码或地标等解剖先验来改进配准，这些先验可以作为输入或损失函数的一部分。然而，这类弱标签通常不易获取，限制了其实际应用。受视觉基础模型强大表示学习能力的启发，本文引入了SAMIR，一种利用Segment Anything Model (SAM)增强特征提取的高效医学图像配准框架。SAM在大型自然图像数据集上预训练，可以学习强大且通用的视觉表示。我们设计了一个特定任务的自适应管道，使用SAM的图像编码器提取结构感知的特征嵌入，而非使用原始输入图像，从而能够更准确地建模解剖一致性和变形模式。我们还设计了一个轻量级3D头在嵌入空间内细化特征，适应医学图像中的局部变形。此外，我们引入了分层特征一致性损失来引导粗到细的特征匹配，改善解剖对齐。大量实验表明，在受试者内心脏图像配准和受试者间腹部CT图像配准的基准数据集上，SAMIR显著优于最先进的方法，在ACDC上实现了2.68%的性能提升，在腹部数据集上实现了6.44%的性能提升。论文接受后，源代码将在GitHub上公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决医学图像配准中特征提取的准确性和鲁棒性问题。医学图像配准对诊断、手术规划和运动分析至关重要，但现有方法难以处理图像质量差异（如对比度变化和噪声），且依赖难以获取的分割标签。提高配准准确性和鲁棒性可以辅助医生进行更精准的诊疗决策，扩大方法在实际临床环境中的适用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统配准方法的局限性（耗时、对图像质量敏感）和弱监督方法的不足（标签获取困难）。受SAM模型在图像分割中强大特征提取能力的启发，作者设计了一种新的配准框架，利用SAM的结构感知特征而非原始图像。作者借鉴了金字塔策略处理大变形问题，参考了弱监督方法的思想但避免了直接依赖分割标签，同时结合了视觉基础模型在其他任务中的成功应用经验。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用SAM的预训练图像编码器提取结构感知的特征嵌入，增强医学图像配准的准确性和鲁棒性。整体流程分为三部分：1) 结构感知特征嵌入模块：将3D医学数据转为2D切片处理，通过SAM编码器提取特征，再用轻量级3D卷积模块增强特征；2) 金字塔变形场预测模块：采用粗到细策略，在不同尺度上预测变形场，逐步优化；3) 分层特征一致性损失：在多尺度空间中对齐特征，提高解剖结构一致性。整个流程避免了依赖难以获取的弱标签，同时提高了对图像质量变化的鲁棒性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 创新性地将SAM模型用于医学图像配准，设计了特定的3D适应管道；2) 提出结构感知特征嵌入模块，利用SAM的强大特征提取能力；3) 设计分层特征一致性损失，在多尺度空间中对齐特征；4) 结合金字塔策略处理大变形问题。相比之前工作，SAMIR避免了依赖难以获取的分割标签，直接利用SAM的特征提取能力而非原始图像，在ACDC和腹部CT数据集上分别实现了2.68%和6.44%的性能提升，且对图像质量变化表现出更强的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAMIR创新性地利用SAM的强大特征提取能力，通过结构感知特征嵌入和分层特征一致性损失，实现了更准确、更鲁棒的医学图像配准，显著优于现有方法且无需依赖难以获取的弱标签。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image registration is a fundamental task in medical image analysis.Deformations are often closely related to the morphological characteristics oftissues, making accurate feature extraction crucial. Recent weakly supervisedmethods improve registration by incorporating anatomical priors such assegmentation masks or landmarks, either as inputs or in the loss function.However, such weak labels are often not readily available, limiting theirpractical use. Motivated by the strong representation learning ability ofvisual foundation models, this paper introduces SAMIR, an efficient medicalimage registration framework that utilizes the Segment Anything Model (SAM) toenhance feature extraction. SAM is pretrained on large-scale natural imagedatasets and can learn robust, general-purpose visual representations. Ratherthan using raw input images, we design a task-specific adaptation pipelineusing SAM's image encoder to extract structure-aware feature embeddings,enabling more accurate modeling of anatomical consistency and deformationpatterns. We further design a lightweight 3D head to refine features within theembedding space, adapting to local deformations in medical images.Additionally, we introduce a Hierarchical Feature Consistency Loss to guidecoarse-to-fine feature matching and improve anatomical alignment. Extensiveexperiments demonstrate that SAMIR significantly outperforms state-of-the-artmethods on benchmark datasets for both intra-subject cardiac image registrationand inter-subject abdomen CT image registration, achieving performanceimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source codewill be publicly available on GitHub following the acceptance of this paper.</description>
      <author>example@mail.com (Yue He, Min Liu, Qinghao Liu, Jiazheng Wang, Yaonan Wang, Hang Zhang, Xiang Chen)</author>
      <guid isPermaLink="false">2509.13629v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>HAM: Hierarchical Adapter Merging for Scalable Continual Learning</title>
      <link>http://arxiv.org/abs/2509.13211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为分层适配器合并(HAM)的新框架，用于解决持续学习中的灾难性遗忘问题，通过动态组合不同任务的适配器，有效管理多个任务并提高效率。&lt;h4&gt;背景&lt;/h4&gt;持续学习是人类认知的重要能力，但对当前深度学习模型构成重大挑战。新知识会干扰已学习的信息，导致模型遗忘旧知识，这种现象称为灾难性遗忘。大型预训练模型可通过利用现有知识和过参数化部分缓解遗忘，但在面对新数据分布时仍存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效扩展到动态学习场景和长任务序列的方法，解决参数高效微调(PEFT)方法在管理多个任务时面临的复杂性和干扰问题。&lt;h4&gt;方法&lt;/h4&gt;提出HAM框架，在训练过程中动态组合来自不同任务的适配器。维护一组固定组，分层合并新的适配器；对每个任务训练低秩适配器和重要性标量；基于适配器相似性动态分组任务；在每个组内对适配器进行修剪、缩放和合并，促进相关任务间的迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个视觉基准测试上的大量实验表明，HAM显著优于最先进的方法，特别是在任务数量增加时优势更加明显。HAM能够有效扩展，比竞争基线方法管理更多任务同时提高效率。&lt;h4&gt;结论&lt;/h4&gt;HAM框架提供了一种有效解决持续学习中灾难性遗忘问题的方法，通过分层合并适配器，能够在处理多个任务时保持性能并提高效率。&lt;h4&gt;翻译&lt;/h4&gt;持续学习是人类认知的基本能力，然而它对当前的深度学习模型构成了重大挑战。主要问题是新知识可能会干扰已学习的信息，导致模型遗忘旧知识而倾向于新知识，这种现象被称为灾难性遗忘。尽管大型预训练模型可以通过利用其现有知识和过参数化部分缓解遗忘，但当面对新的数据分布时，它们仍然存在困难。参数高效微调(PEFT)方法（如LoRA）使模型能够高效地适应新知识。然而，在扩展到动态学习场景和长任务序列时，它们仍面临挑战，因为每个任务维护一个适配器会增加复杂性并增加干扰的可能性。在本文中，我们引入了分层适配器合并(HAM)，一个在训练过程中动态组合来自不同任务的适配器的新框架。这种方法使HAM能够有效扩展，允许它以更高的效率管理比竞争基线更多的任务。为此，HAM维护一组固定组，这些组分层合并新的适配器。对于每个任务，HAM训练一个低秩适配器和一个重要性标量，然后基于适配器相似性动态对任务进行分组。在每个组内，适配器被修剪、缩放和合并，促进相关任务之间的迁移学习。在三个视觉基准测试上的大量实验表明，HAM显著优于最先进的方法，特别是随着任务数量的增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning is an essential capability of human cognition, yet itposes significant challenges for current deep learning models. The primaryissue is that new knowledge can interfere with previously learned information,causing the model to forget earlier knowledge in favor of the new, a phenomenonknown as catastrophic forgetting. Although large pre-trained models canpartially mitigate forgetting by leveraging their existing knowledge andover-parameterization, they often struggle when confronted with novel datadistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,enable efficient adaptation to new knowledge. However, they still facechallenges in scaling to dynamic learning scenarios and long sequences oftasks, as maintaining one adapter per task introduces complexity and increasesthe potential for interference. In this paper, we introduce HierarchicalAdapters Merging (HAM), a novel framework that dynamically combines adaptersfrom different tasks during training. This approach enables HAM to scaleeffectively, allowing it to manage more tasks than competing baselines withimproved efficiency. To achieve this, HAM maintains a fixed set of groups thathierarchically consolidate new adapters. For each task, HAM trains a low-rankadapter along with an importance scalar, then dynamically groups tasks based onadapter similarity. Within each group, adapters are pruned, scaled and merge,facilitating transfer learning between related tasks. Extensive experiments onthree vision benchmarks show that HAM significantly outperformsstate-of-the-art methods, particularly as the number of tasks increases.</description>
      <author>example@mail.com (Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2509.13211v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
  <item>
      <title>Ensemble Visualization With Variational Autoencoder</title>
      <link>http://arxiv.org/abs/2509.13000v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE Workshop on Uncertainty Visualization&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提出了一种新的数据集可视化方法，通过在潜在空间中构建结构化概率表示，实现数据集的有效可视化和分析。&lt;h4&gt;背景&lt;/h4&gt;数据集的可视化是理解和分析复杂空间数据的重要挑战，特别是在处理高维数据集合时。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来可视化数据集，通过构建潜在空间中的结构化概率表示，实现数据集的特征提取和概率分布分析。&lt;h4&gt;方法&lt;/h4&gt;通过特征空间转换和变分自编码器的无监督学习，将空间数据集合的特征转换为潜在空间，使潜在空间遵循多元标准高斯分布，从而支持置信区间计算和概率分布密度估计。&lt;h4&gt;主要发现&lt;/h4&gt;在天气预报集合上的初步结果表明，所提出的方法在数据集可视化方面是有效且多功能的，能够准确表示数据集合的概率分布特性。&lt;h4&gt;结论&lt;/h4&gt;该方法为数据集可视化提供了一种新的有效途径，通过结构化概率表示在潜在空间中，能够更好地理解和分析复杂的数据集合。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种通过在潜在空间中构建结构化概率表示来可视化数据集的新方法，即空间数据特征的低维表示。我们的方法通过特征空间转换和使用变分自编码器的无监督学习，将集合的空间特征转换为潜在空间。由此产生的潜在空间遵循多元标准高斯分布，使得能够计算置信区间和生成数据集的概率分布的密度估计。在天气预报集合上的初步结果证明了我们方法的有效性和多功能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new method to visualize data ensembles by constructingstructured probabilistic representations in latent spaces, i.e.,lower-dimensional representations of spatial data features. Our approachtransforms the spatial features of an ensemble into a latent space throughfeature space conversion and unsupervised learning using a variationalautoencoder (VAE). The resulting latent spaces follow multivariate standardGaussian distributions, enabling analytical computation of confidence intervalsand density estimation of the probabilistic distribution that generates thedata ensemble. Preliminary results on a weather forecasting ensembledemonstrate the effectiveness and versatility of our method.</description>
      <author>example@mail.com (Cenyang Wu, Qinhan Yu, Liang Zhou)</author>
      <guid isPermaLink="false">2509.13000v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Rapid Adaptation of SpO2 Estimation to Wearable Devices via Transfer Learning on Low-Sampling-Rate PPG</title>
      <link>http://arxiv.org/abs/2509.12515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In the proceedings of IEEE-EMBS International Conference on Body  Sensor Networks 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于迁移学习的框架，利用低采样率双通道光电容积脉搏波实现可穿戴设备上的低功耗血氧饱和度监测，无需复杂临床校准。&lt;h4&gt;背景&lt;/h4&gt;血氧饱和度(SpO2)是医疗保健监测的重要指标，但传统估计方法依赖复杂临床校准，不适合低功耗可穿戴应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速适应于能量高效可穿戴设备的SpO2估计方法，使用低采样率(25Hz)双通道光电容积脉搏波(PPG)。&lt;h4&gt;方法&lt;/h4&gt;首先在公共临床数据集上预训练带有自注意力的双向长短期记忆(BiLSTM)模型，然后使用从可穿戴We-Be带和FDA批准的参考脉搏血氧仪收集的数据进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上平均绝对误差为2.967%，在私有数据集上为2.624%，显著优于传统方法；使用25Hz PPG相比100Hz减少40%功耗；瞬时SpO2预测误差为3.284%，能有效捕捉快速波动。&lt;h4&gt;结论&lt;/h4&gt;证明了准确、低功耗SpO2监测在可穿戴设备上的快速适应性，无需临床校准即可实现。&lt;h4&gt;翻译&lt;/h4&gt;血氧饱和度(SpO2)是医疗保健监测的重要指标。传统的SpO2估计方法通常依赖复杂的临床校准，使其不适合低功耗、可穿戴应用。在本文中，我们提出了一种基于迁移学习的框架，利用低采样率(25Hz)双通道光电容积脉搏波(PPG)实现SpO2估计向节能可穿戴设备的快速适应。我们首先在公共临床数据集上预训练一个带有自注意力的双向长短期记忆(BiLSTM)模型，然后使用从我们的可穿戴We-Be带和FDA批准的参考脉搏血氧仪收集的数据对其进行微调。实验结果表明，我们的方法在公共数据集上达到2.967%的平均绝对误差(MAE)，在私有数据集上达到2.624%的MAE，显著优于传统校准和非迁移学习基线。此外，与100Hz相比，使用25Hz PPG可减少40%的功耗(不包括基线功耗)。我们的方法在瞬时SpO2预测中也达到了3.284%的MAE，有效捕捉了快速波动。这些结果证明了无需临床校准即可在可穿戴设备上实现准确、低功耗SpO2监测的快速适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Blood oxygen saturation (SpO2) is a vital marker for healthcare monitoring.Traditional SpO2 estimation methods often rely on complex clinical calibration,making them unsuitable for low-power, wearable applications. In this paper, wepropose a transfer learning-based framework for the rapid adaptation of SpO2estimation to energy-efficient wearable devices using low-sampling-rate (25Hz)dual-channel photoplethysmography (PPG). We first pretrain a bidirectional LongShort-Term Memory (BiLSTM) model with self-attention on a public clinicaldataset, then fine-tune it using data collected from our wearable We-Be bandand an FDA-approved reference pulse oximeter. Experimental results show thatour approach achieves a mean absolute error (MAE) of 2.967% on the publicdataset and 2.624% on the private dataset, significantly outperformingtraditional calibration and non-transferred machine learning baselines.Moreover, using 25Hz PPG reduces power consumption by 40% compared to 100Hz,excluding baseline draw. Our method also attains an MAE of 3.284% ininstantaneous SpO2 prediction, effectively capturing rapid fluctuations. Theseresults demonstrate the rapid adaptation of accurate, low-power SpO2 monitoringon wearable devices without the need for clinical calibration.</description>
      <author>example@mail.com (Zequan Liang, Ruoyu Zhang, Wei Shao, krishna Karthik, Ehsan Kourkchi, Setareh Rafatirad, Houman Homayoun)</author>
      <guid isPermaLink="false">2509.12515v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture</title>
      <link>http://arxiv.org/abs/2509.12363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种联邦学习框架用于智能农业，旨在为明尼苏达州农场开发可扩展、高效且安全的作物疾病检测解决方案，同时保护农场数据隐私。&lt;h4&gt;背景&lt;/h4&gt;农业部门正经历转型，先进技术特别是数据驱动决策的整合日益重要。然而，农场对共享运营数据存在隐私顾虑，导致数据驱动的农业解释需求与农场隐私担忧之间存在矛盾。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展、高效且安全的作物疾病检测解决方案，该方案能适应明尼苏达州农场的环境和运营条件，同时保持敏感农场数据的本地性，实现高准确率的作物疾病分类而不损害数据隐私。&lt;h4&gt;方法&lt;/h4&gt;研究涉及从明尼苏达州农场收集数据，应用本地深度学习算法，使用迁移学习，以及通过中央聚合服务器进行模型优化。联邦学习框架使敏感数据保持本地，同时实现协作模型更新。&lt;h4&gt;主要发现&lt;/h4&gt;该方法有望实现疾病检测准确率的提高，在农业场景中具有良好的泛化能力，降低通信和训练成本，并在未来实现疾病的早期识别和干预。&lt;h4&gt;结论&lt;/h4&gt;这项工作弥合了先进机器学习技术与明尼苏达州及其他地区农民的实际、隐私敏感需求之间的差距，利用联邦学习的优势，为农业提供了安全高效的疾病检测方法，有望彻底改变智能农业系统并解决当地农业问题，同时确保数据机密性。&lt;h4&gt;翻译&lt;/h4&gt;农业部门正经历转型，先进技术特别是数据驱动决策的整合日益重要。本研究提出了一种用于智能农业的联邦学习框架，旨在为明尼苏达州农场开发可扩展、高效且安全的作物疾病检测解决方案，适应其环境和运营条件。通过将敏感农场数据保存在本地并实现协作模型更新，我们提出的框架旨在实现高准确率的作物疾病分类而不损害数据隐私。我们概述了一种涉及从明尼苏达州农场收集数据、应用本地深度学习算法、迁移学习以及通过中央聚合服务器进行模型优化的方法，旨在提高疾病检测的准确率，在农业场景中实现良好的泛化能力，降低通信和训练成本，并在未来实施中实现疾病的早期识别和干预。我们概述了方法和预期成果，为后续研究中的实证验证奠定了基础。这项工作出现在越来越多的农业数据驱动解释需求必须与对农场不愿共享其运营数据的隐私担忧进行权衡的背景下。这将提供一种安全高效的疾病检测方法，最终能够彻底改变智能农业系统，并以数据保密性解决当地农业问题。通过这样做，本文弥合了先进机器学习技术与明尼苏达州及其他地区农民的实际、隐私敏感需求之间的差距，利用了联邦学习的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.47852/bonviewAIA52025089&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The agricultural sector is undergoing a transformation with the integrationof advanced technologies, particularly in data-driven decision-making. Thiswork proposes a federated learning framework for smart farming, aiming todevelop a scalable, efficient, and secure solution for crop disease detectiontailored to the environmental and operational conditions of Minnesota farms. Bymaintaining sensitive farm data locally and enabling collaborative modelupdates, our proposed framework seeks to achieve high accuracy in crop diseaseclassification without compromising data privacy. We outline a methodologyinvolving data collection from Minnesota farms, application of local deeplearning algorithms, transfer learning, and a central aggregation server formodel refinement, aiming to achieve improved accuracy in disease detection,good generalization across agricultural scenarios, lower costs in communicationand training time, and earlier identification and intervention against diseasesin future implementations. We outline a methodology and anticipated outcomes,setting the stage for empirical validation in subsequent studies. This workcomes in a context where more and more demand for data-driven interpretationsin agriculture has to be weighed with concerns about privacy from farms thatare hesitant to share their operational data. This will be important to providea secure and efficient disease detection method that can finally revolutionizesmart farming systems and solve local agricultural problems with dataconfidentiality. In doing so, this paper bridges the gap between advancedmachine learning techniques and the practical, privacy-sensitive needs offarmers in Minnesota and beyond, leveraging the benefits of federated learning.</description>
      <author>example@mail.com (Ritesh Janga, Rushit Dave)</author>
      <guid isPermaLink="false">2509.12363v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation</title>
      <link>http://arxiv.org/abs/2509.13177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ROOM是一个综合模拟框架，用于生成逼真的支气管镜检查训练数据，解决连续体机器人发展中缺乏真实训练环境的问题。&lt;h4&gt;背景&lt;/h4&gt;连续体机器人正在推进支气管镜检查程序，能够进入复杂的肺部气道并实现靶向干预。然而，其发展受到缺乏真实训练和测试环境的限制。真实数据难以收集，因为受到伦理约束和患者安全问题的限制，开发自主算法需要真实的成像和物理反馈。&lt;h4&gt;目的&lt;/h4&gt;提出ROOM（医学中真实的光学观察）模拟框架，用于生成逼真的支气管镜检查训练数据。&lt;h4&gt;方法&lt;/h4&gt;利用患者CT扫描，管道渲染多模态传感器数据，包括具有真实噪声和镜面反射的RGB图像、度量深度图、表面法线、光流和点云，在医学相关尺度上。&lt;h4&gt;主要发现&lt;/h4&gt;在多视图姿态估计和单目深度估计两个医疗机器人标准任务中验证了ROOM生成的数据，展示了最先进方法必须克服的多样化挑战。证明ROOM产生的数据可用于微调现有的深度估计模型，使其他下游应用（如导航）成为可能。&lt;h4&gt;结论&lt;/h4&gt;预期ROOM将能够在多样化的患者解剖结构和临床环境中难以捕捉的程序场景中实现大规模数据生成。&lt;h4&gt;翻译&lt;/h4&gt;连续体机器人通过进入复杂的肺部气道和实现靶向干预，正在推进支气管镜检查程序。然而，其发展受到缺乏真实训练和测试环境的限制：由于伦理约束和患者安全问题，真实数据难以收集，并且开发自主算法需要真实的成像和物理反馈。我们提出了ROOM（医学中真实的光学观察），这是一个综合模拟框架，专为生成逼真的支气管镜检查训练数据而设计。通过利用患者CT扫描，我们的管道渲染多模态传感器数据，包括具有真实噪声和镜面反射的RGB图像、度量深度图、表面法线、光流和点云，在医学相关尺度上。我们在医疗机器人的两个标准任务——多视图姿态估计和单目深度估计中验证了ROOM生成的数据，展示了最先进方法必须克服的多样化挑战，才能转移到这些医疗环境中。此外，我们证明ROOM产生的数据可用于微调现有的深度估计模型以克服这些挑战，同时使其他下游应用（如导航）成为可能。我们期望ROOM将能够在多样化的患者解剖结构和临床环境中难以捕捉的程序场景中实现大规模数据生成。代码和数据：https://github.com/iamsalvatore/room。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决支气管镜检查中连续体机器人开发面临的训练数据稀缺问题。由于伦理限制、患者安全问题和临床数据收集的高成本，真实数据难以获取；同时，人体解剖结构的个体化特性要求算法必须适应不同气道几何形状并保持毫米级精度。这个问题限制了自主导航算法的发展，而合成数据生成能提供大规模、多样化的训练数据，促进支气管镜机器人的技术进步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了支气管镜检查的特殊挑战：需要解剖保真度、特定光照条件以及临床尺度校准。他们借鉴了Cosserat杆理论建模连续体机器人运动学，使用PyBullet作为物理引擎，Blender的路径追踪和BSDF着色器实现光保真渲染。同时参考了现有深度估计和姿态估计方法进行验证，并利用医学图像分割和3D重建技术处理CT数据。作者整合了这些现有技术，创建了一个专门针对支气管镜检查的统一模拟框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用患者CT扫描数据，通过物理模拟和光保真渲染，生成多样化的多模态传感器数据，解决支气管镜训练数据稀缺问题。整体流程分为四步：1)中轴提取：从CT扫描重建3D肺部模型并提取中轴轨迹；2)自动采样：沿骨骼结构采样，在关键区域增加密度；3)数据合成：生成RGB图像、深度图、表面法线等多模态数据；4)传感器噪声建模：通过频域分析添加真实噪声特征。整个过程从CT扫描开始，最终生成同步的多模态传感器数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ROOM框架：首个完全自动化将CT数据转换为合成训练数据的管道；2)光保真渲染：考虑内窥镜光照和组织表面特性；3)多模态数据生成：提供RGB、深度、法线等多种传感器数据；4)频域噪声建模：准确复制真实支气管镜图像噪声特性。相比传统模拟器，ROOM提供光保真度和多模态数据；相比结肠镜数据生成，ROOM应对支气管镜特有的几何和外观挑战；相比现有连续体机器人系统，ROOM统一了物理模拟和光保真渲染，弥合了两者间的差距。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ROOM是一个创新的物理模拟框架，通过整合患者特异性解剖重建、连续体机器人物理和光保真渲染，生成多样化的支气管镜训练数据集，解决了真实数据稀缺的关键挑战，并提高了姿态估计和深度估计模型在医疗环境中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuum robots are advancing bronchoscopy procedures by accessing complexlung airways and enabling targeted interventions. However, their development islimited by the lack of realistic training and test environments: Real data isdifficult to collect due to ethical constraints and patient safety concerns,and developing autonomy algorithms requires realistic imaging and physicalfeedback. We present ROOM (Realistic Optical Observation in Medicine), acomprehensive simulation framework designed for generating photorealisticbronchoscopy training data. By leveraging patient CT scans, our pipelinerenders multi-modal sensor data including RGB images with realistic noise andlight specularities, metric depth maps, surface normals, optical flow and pointclouds at medically relevant scales. We validate the data generated by ROOM intwo canonical tasks for medical robotics -- multi-view pose estimation andmonocular depth estimation, demonstrating diverse challenges thatstate-of-the-art methods must overcome to transfer to these medical settings.Furthermore, we show that the data produced by ROOM can be used to fine-tuneexisting depth estimation models to overcome these challenges, also enablingother downstream applications such as navigation. We expect that ROOM willenable large-scale data generation across diverse patient anatomies andprocedural scenarios that are challenging to capture in clinical settings. Codeand data: https://github.com/iamsalvatore/room.</description>
      <author>example@mail.com (Salvatore Esposito, Matías Mattamala, Daniel Rebain, Francis Xiatian Zhang, Kevin Dhaliwal, Mohsen Khadem, Subramanian Ramamoorthy)</author>
      <guid isPermaLink="false">2509.13177v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</title>
      <link>http://arxiv.org/abs/2509.13172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WHU-STree是一个跨城市、丰富标注和多模态的城市行道树数据集，包含两个不同城市收集的同步点云和高分辨率图像，涵盖21,007个标注的树实例，涉及50个物种和2个形态参数。该数据集支持超过10个与行道树清单相关的任务，并针对树种分类和单木分割两个关键任务进行了基准测试。&lt;h4&gt;背景&lt;/h4&gt;行道树对城市宜居性至关重要，提供生态和社会效益。在空间受限的城市环境中优化这些多功能资产需要建立详细、准确且动态更新的行道树清单。传统的地面调查耗时且劳动密集，而利用移动测量系统(MMS)的自动化调查提供了更高效的解决方案。然而，现有的MMS获取的树数据集受限于小规模场景、有限的标注或单一模态，限制了它们用于综合分析的能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有MMS获取的树数据集的局限性，研究引入了WHU-STree，这是一个跨城市、丰富标注和多模态的城市行道树数据集。&lt;h4&gt;方法&lt;/h4&gt;WHU-STree数据集是在两个不同城市收集的，集成了同步点云和高分辨率图像，包含21,007个标注的树实例，涵盖50个物种和2个形态参数。研究利用WHU-STree的独特特性，同时支持超过10个与行道树清单相关的任务。研究对两个关键任务(树种分类和单木分割)的代表性基线进行了基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验和深入的分析证明了多模态数据融合的显著潜力，并强调了跨域适用性是实际算法部署的关键前提。研究确定了关键挑战，并概述了未来可能的工作方向，包括多模态融合、多任务协作、跨域泛化、空间模式学习以及用于行道树资产管理的多模态大语言模型。&lt;h4&gt;结论&lt;/h4&gt;WHU-STree数据集为城市行道树研究提供了一个全面、多模态的资源，支持多种任务，并有助于推动相关算法的发展和应用。&lt;h4&gt;翻译&lt;/h4&gt;行道树对城市宜居性至关重要，提供生态和社会效益。在空间受限的城市环境中优化这些多功能资产需要建立详细、准确且动态更新的行道树清单。鉴于传统的地面调查耗时且劳动密集，利用移动测量系统(MMS)的自动化调查提供了更高效的解决方案。然而，现有的MMS获取的树数据集受限于小规模场景、有限的标注或单一模态，限制了它们用于综合分析的能力。为了解决这些局限性，我们引入了WHU-STree，这是一个跨城市、丰富标注和多模态的城市行道树数据集。WHU-STree在两个不同城市收集，集成了同步点云和高分辨率图像，包含21,007个标注的树实例，涵盖50个物种和2个形态参数。利用WHU-STree的独特特性，它可以同时支持超过10个与行道树清单相关的任务。我们针对两个关键任务(树种分类和单木分割)对代表性基线进行了基准测试。广泛的实验和深入的分析证明了多模态数据融合的显著潜力，并强调了跨域适用性是实际算法部署的关键前提。特别是，我们确定了关键挑战，并概述了未来可能的工作方向，包括多模态融合、多任务协作、跨域泛化、空间模式学习以及用于行道树资产管理的多模态大语言模型。WHU-STree数据集可通过以下网址获取：https://github.com/WHU-USI3DV/WHU-STree。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有城市街道树木数据集的三个限制：小规模场景、有限标注和单一模态。这个问题很重要，因为街道树木对城市宜居性至关重要，提供生态和社会效益，建立准确、动态更新的树木清单对优化城市环境中的这些多功能资产至关重要，而传统实地调查耗时耗力，现有数据集的限制又阻碍了自动化调查解决方案的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别现有数据集的三个主要限制（小规模场景、有限标注和单一模态）来思考问题，然后设计了一个跨越城市、丰富标注和多模态的数据集。作者在两个气候区显著不同的城市（南京和沈阳）收集数据，并对数据进行预处理和详细标注。作者借鉴了现有的树木分割和分类方法（如MinkNet、PointMLP、SegmentAnyTree等）以及多模态融合方法（如TSCMDL和LCPS）作为基准进行评估。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个跨越城市、丰富标注和多模态的城市街道树木数据集，整合点云和图像数据以支持多种树木清单相关任务。整体流程包括：1)在南京和沈阳使用移动测量系统收集数据；2)对数据进行预处理（降采样、自适应分区、异常点去除）；3)进行详细标注（树木实例、物种信息和形态参数）；4)采用两种数据分割策略（类别平衡分割和跨城市分割）；5)最终形成一个包含21,007个树木实例、50个物种和2个形态参数的大规模数据集。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)跨城市覆盖，涵盖两个气候区不同的城市；2)丰富标注，包含精确的个体树木定位、物种信息和形态参数；3)多模态数据，整合点云和全景图像；4)多任务支持，支持多种配置和任务。相比之前的工作，WHU-STree规模更大（21,007个树木实例），标注更丰富（包含物种和形态参数），是多模态的，且跨城市覆盖，而现有数据集通常是单模态、小规模、单一区域的，且标注有限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一个大规模、多模态、跨城市的街道树木基准数据集WHU-STree，通过整合精确的3D点云和高分辨率图像数据，支持多种与街道树木清单相关的任务，为城市树木自动化管理提供了宝贵资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Street trees are vital to urban livability, providing ecological and socialbenefits. Establishing a detailed, accurate, and dynamically updated streettree inventory has become essential for optimizing these multifunctional assetswithin space-constrained urban environments. Given that traditional fieldsurveys are time-consuming and labor-intensive, automated surveys utilizingMobile Mapping Systems (MMS) offer a more efficient solution. However, existingMMS-acquired tree datasets are limited by small-scale scene, limitedannotation, or single modality, restricting their utility for comprehensiveanalysis. To address these limitations, we introduce WHU-STree, a cross-city,richly annotated, and multi-modal urban street tree dataset. Collected acrosstwo distinct cities, WHU-STree integrates synchronized point clouds andhigh-resolution images, encompassing 21,007 annotated tree instances across 50species and 2 morphological parameters. Leveraging the unique characteristics,WHU-STree concurrently supports over 10 tasks related to street tree inventory.We benchmark representative baselines for two key tasks--tree speciesclassification and individual tree segmentation. Extensive experiments andin-depth analysis demonstrate the significant potential of multi-modal datafusion and underscore cross-domain applicability as a critical prerequisite forpractical algorithm deployment. In particular, we identify key challenges andoutline potential future works for fully exploiting WHU-STree, encompassingmulti-modal fusion, multi-task collaboration, cross-domain generalization,spatial pattern learning, and Multi-modal Large Language Model for street treeasset management. The WHU-STree dataset is accessible at:https://github.com/WHU-USI3DV/WHU-STree.</description>
      <author>example@mail.com (Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang)</author>
      <guid isPermaLink="false">2509.13172v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation</title>
      <link>http://arxiv.org/abs/2509.13149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MSDNet是一种多阶段蒸馏框架，用于4D雷达点云超分辨率，通过将密集LiDAR先验知识转移到雷达特征上，实现高质量重建和计算效率。&lt;h4&gt;背景&lt;/h4&gt;4D雷达超分辨率是自动驾驶感知中的基础问题，旨在将稀疏且带噪的点云重建为密集且几何一致的表示。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法训练成本高、推理延迟高和泛化能力差的问题，实现准确性和效率的平衡。&lt;h4&gt;方法&lt;/h4&gt;提出MSDNet框架，包括两个阶段：1)重建引导的特征蒸馏，通过特征重建对齐和加密学生特征；2)扩散引导的特征蒸馏，通过轻量级扩散网络优化特征。此外引入噪声适配器，自适应对齐噪声级别与扩散时间步。&lt;h4&gt;主要发现&lt;/h4&gt;在VoD和内部数据集上的实验表明，MSDNet实现了高保真重建和低延迟推理，并在下游任务中持续提高性能。&lt;h4&gt;结论&lt;/h4&gt;MSDNet成功平衡了准确性和效率，代码将在发表后公开。&lt;h4&gt;翻译&lt;/h4&gt;4D雷达超分辨率旨在将稀疏且带噪的点云重建为密集且几何一致的表示，是自动驾驶感知中的一个基础问题。然而，现有方法往往存在训练成本高或依赖复杂的基于扩散的采样，导致推理延迟高和泛化能力差，难以平衡准确性和效率。为解决这些限制，我们提出MSDNet，一个多阶段蒸馏框架，有效将密集LiDAR先验知识转移到4D雷达特征上，实现高质量重建和计算效率。第一阶段执行重建引导的特征蒸馏，通过特征重建对齐和加密学生特征。在第二阶段，我们提出扩散引导的特征蒸馏，将第一阶段蒸馏的特征视为教师表示的噪声版本，并通过轻量级扩散网络进行优化。此外，我们引入噪声适配器，自适应地将特征的噪声级别与预定义的扩散时间步对齐，实现更精确的降噪。在VoD和内部数据集上的大量实验表明，MSDNet在4D雷达点云超分辨率任务中实现了高保真重建和低延迟推理，并在下游任务中持续提高性能。代码将在发表后公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D雷达点云超分辨率问题，即如何将稀疏、有噪声的4D雷达点云重建为密集、几何一致的表示。这个问题在自动驾驶领域非常重要，因为4D雷达虽然能在恶劣天气条件下稳定工作，但其点云质量有限，严重影响了物体检测、场景理解和定位等精细感知任务的性能。当前方法要么训练成本高，要么推理延迟大且泛化能力差，难以平衡准确性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：信号级方法数据收集成本高且泛化能力有限，而基于扩散的方法（如R2LDM）虽然效果好但推理延迟高。作者借鉴了知识蒸馏的思想，将其应用于4D雷达超分辨率任务，设计了两个渐进阶段的蒸馏框架：第一阶段通过重建引导特征蒸馏（RGFD）进行初步知识转移，第二阶段通过扩散引导特征蒸馏（DGFD）进行细化。作者还引入了噪声适配器来精确对齐噪声水平，提高扩散效率。这种方法结合了知识蒸馏和扩散模型的优点，避免了各自的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多阶段知识蒸馏，将高分辨率LiDAR的几何先验高效转移到稀疏的4D雷达特征中，实现高质量且高效的点云超分辨率。整体流程包括：1) 使用VoxelNet提取LiDAR和4D雷达的BEV特征；2) 构建教师网络，通过S2D模块增强LiDAR特征并重建点云；3) 第一阶段RGFD通过特征重建网络将稀疏4D雷达特征转换为密集表示，并最小化与LiDAR特征的差异；4) 第二阶段DGFD将第一阶段结果视为噪声，通过噪声适配器和轻量级扩散网络进行去噪；5) 将去噪特征输入点云重建模块生成密集4D雷达点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次将知识蒸馏应用于4D雷达超分辨率，提出多阶段蒸馏框架；2) 设计重建引导特征蒸馏（RGFD）解决跨模态对齐挑战；3) 提出扩散引导特征蒸馏（DGFD）使用轻量级扩散网络进行特征细化；4) 引入噪声适配器自适应对齐噪声水平。相比之前工作，MSDNet避免了信号级方法的高数据成本和基于扩散方法的高推理延迟，通过两阶段蒸馏实现了高质量和效率的平衡。实验表明，MSDNet在保持高重建质量的同时，推理速度比R2LDM快89.6%，参数量减少一半。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MSDNet通过多阶段知识蒸馏框架，高效地将LiDAR的密集几何先验转移到4D雷达特征中，实现了高质量且低延迟的4D雷达点云超分辨率，显著提升了下游任务性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D radar super-resolution, which aims to reconstruct sparse and noisy pointclouds into dense and geometrically consistent representations, is afoundational problem in autonomous perception. However, existing methods oftensuffer from high training cost or rely on complex diffusion-based sampling,resulting in high inference latency and poor generalization, making itdifficult to balance accuracy and efficiency. To address these limitations, wepropose MSDNet, a multi-stage distillation framework that efficiently transfersdense LiDAR priors to 4D radar features to achieve both high reconstructionquality and computational efficiency. The first stage performsreconstruction-guided feature distillation, aligning and densifying thestudent's features through feature reconstruction. In the second stage, wepropose diffusion-guided feature distillation, which treats the stage-onedistilled features as a noisy version of the teacher's representations andrefines them via a lightweight diffusion network. Furthermore, we introduce anoise adapter that adaptively aligns the noise level of the feature with apredefined diffusion timestep, enabling a more precise denoising. Extensiveexperiments on the VoD and in-house datasets demonstrate that MSDNet achievesboth high-fidelity reconstruction and low-latency inference in the task of 4Dradar point cloud super-resolution, and consistently improves performance ondownstream tasks. The code will be publicly available upon publication.</description>
      <author>example@mail.com (Minqing Huang, Shouyi Lu, Boyuan Zheng, Ziyao Li, Xiao Tang, Guirong Zhuo)</author>
      <guid isPermaLink="false">2509.13149v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.13116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  An extension of our CVPR 2023 paper, "Weakly Supervised  Class-Agnostic Motion Prediction for Autonomous Driving," accepted for  publication in TPAMI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从激光雷达点云中进行弱自监督类别无关运动预测的新方法，通过利用场景中的前景/背景或非地面/地面线索作为监督信号，有效减少了标注需求，同时保持了良好的预测性能。&lt;h4&gt;背景&lt;/h4&gt;动态环境中的运动理解对自动驾驶至关重要，户外场景通常包含移动前景和静态背景，使运动理解与场景解析相关联。&lt;h4&gt;目的&lt;/h4&gt;开发一种弱监督和自监督的类别无关运动预测方法，减少对运动标注的依赖，同时保持或提高预测性能。&lt;h4&gt;方法&lt;/h4&gt;1) 提出用前景/背景掩码替代运动标注进行监督；2) 利用非地面/地面掩码作为前景/背景掩码的替代，进一步减少标注；3) 设计需要更少标注的弱监督方法和完全无标注的自监督方法；4) 开发鲁棒一致性感知Chamfer距离损失函数，结合多帧信息和鲁棒惩罚函数抑制异常值。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的弱监督和自监督模型优于现有的自监督对应模型，弱监督模型的性能甚至可以与一些监督模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地平衡了标注工作量和预测性能，为自动驾驶中的运动预测提供了一种实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解动态环境中的运动对自动驾驶至关重要，从而推动了类别无关运动预测的研究。在本工作中，我们研究了从激光雷达点云中进行弱自监督类别无关运动预测。户外场景通常包含移动前景和静态背景，使运动理解能够与场景解析相关联。基于这一观察，我们提出了一种新的弱监督范式，用完全或部分标注的前景/背景掩码（1%、0.1%）替代运动标注进行监督。为此，我们开发了一种利用前景/背景线索指导运动预测模型自监督学习的弱监督方法。由于前景运动通常发生在非地面区域，非地面/地面掩码可以作为前景/背景掩码的替代，进一步减少标注工作量。利用非地面/地面线索，我们提出了两种额外方法：一种需要更少（0.01%）前景/背景标注的弱监督方法，以及一种无需标注的自监督方法。此外，我们设计了一种鲁棒一致性感知Chamfer距离损失，结合多帧信息和鲁棒惩罚函数，以抑制自监督学习中的异常值。实验表明，我们的弱监督和自监督模型优于现有的自监督对应模型，我们的弱监督模型甚至可以与一些监督模型相媲美。这证明了我们的方法有效地平衡了标注工作量和性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶中运动预测对昂贵运动标注数据的依赖问题。在现实中，运动数据的获取成本高且困难，而准确理解动态环境中的运动对确保自动驾驶安全至关重要。传统方法需要大量标注数据，限制了其在开放场景和未知物体类别上的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到户外场景可分解为移动前景和静态背景，从而将运动理解与场景解析联系起来。他们借鉴了现有类别无关运动预测(如MotionNet、BE-STI)和弱监督/自监督方法(如SSMP、SelfMotion)的工作，创新性地提出用前景/背景掩码替代运动标注作为弱监督信号。此外，他们利用前景运动通常发生在非地面区域的特性，进一步用非地面/地面掩码减少标注需求，并设计了鲁棒一致性感知Chamfer Distance损失函数提高自监督学习效果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用场景解析信息(前景/背景或非地面/地面掩码)替代昂贵的运动标注作为监督信号。整体实现流程包括：1)预训练前景/背景分割网络(PreSegNet)或通过平面拟合生成非地面/地面点；2)训练运动预测网络(WeakMotionNet或SelfMotionNet)，包含运动预测头和辅助分割头；3)使用鲁棒一致性感知Chamfer Distance损失函数在前景/非地面点上进行自监督学习；4)同时用少量前景/背景掩码训练辅助分割头。最终实现从弱监督(0.01%标注)到完全无监督的高效运动预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出使用前景/背景掩码作为弱监督进行类别无关运动预测；2)创新性地用非地面/地面掩码替代前景/背景掩码，进一步减少标注需求；3)设计鲁棒一致性感知Chamfer Distance损失函数，利用多帧信息和鲁棒惩罚函数提高自监督学习的鲁棒性；4)提出三种方法：WeakMotion-FB(使用前景/背景掩码)、WeakMotion-NG(使用非地面/地面掩码，需极少标注)和SelfMotion-NG(完全无监督)。相比之前工作，本文方法显著降低了标注依赖(从1%运动标注降至0.01%前景/背景掩码)，同时性能优于或媲美全监督方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的弱监督和无监督学习范式，通过利用场景解析信息替代昂贵的运动标注，实现了高效的类别无关运动预测，显著降低了自动驾驶系统中运动预测模型的标注依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TPAMI.2025.3604036&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding motion in dynamic environments is critical for autonomousdriving, thereby motivating research on class-agnostic motion prediction. Inthis work, we investigate weakly and self-supervised class-agnostic motionprediction from LiDAR point clouds. Outdoor scenes typically consist of mobileforegrounds and static backgrounds, allowing motion understanding to beassociated with scene parsing. Based on this observation, we propose a novelweakly supervised paradigm that replaces motion annotations with fully orpartially annotated (1%, 0.1%) foreground/background masks for supervision. Tothis end, we develop a weakly supervised approach utilizingforeground/background cues to guide the self-supervised learning of motionprediction models. Since foreground motion generally occurs in non-groundregions, non-ground/ground masks can serve as an alternative toforeground/background masks, further reducing annotation effort. Leveragingnon-ground/ground cues, we propose two additional approaches: a weaklysupervised method requiring fewer (0.01%) foreground/background annotations,and a self-supervised method without annotations. Furthermore, we design aRobust Consistency-aware Chamfer Distance loss that incorporates multi-frameinformation and robust penalty functions to suppress outliers inself-supervised learning. Experiments show that our weakly and self-supervisedmodels outperform existing self-supervised counterparts, and our weaklysupervised models even rival some supervised ones. This demonstrates that ourapproaches effectively balance annotation effort and performance.</description>
      <author>example@mail.com (Ruibo Li, Hanyu Shi, Zhe Wang, Guosheng Lin)</author>
      <guid isPermaLink="false">2509.13116v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>MATTER: Multiscale Attention for Registration Error Regression</title>
      <link>http://arxiv.org/abs/2509.12924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于回归而非分类的点云配准质量验证方法，通过多尺度特征提取和注意力聚合机制，实现了对配准质量的更细粒度量化，并在多样化数据集上表现出色，特别是在处理具有异构空间密度的点云时。&lt;h4&gt;背景&lt;/h4&gt;点云配准(PCR)对于同时定位与地图构建(SLAM)和目标跟踪等许多下游任务至关重要，因此检测和量化配准错位(即PCR质量验证)是一个重要任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于回归的PCR质量验证方法，替代现有的基于分类的方法，实现更精细的配准质量量化；并通过多尺度特征提取和注意力聚合扩展特征表示，提高方法在异构密度点云上的表现。&lt;h4&gt;方法&lt;/h4&gt;使用回归而非分类进行PCR质量验证；采用多尺度提取和基于注意力的聚合来扩展与错位相关的特征；在多样化数据集上评估方法的准确性和鲁棒性；将方法应用于指导地图构建下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多样化数据集上提供了准确且稳健的配准误差估计；对于具有异构空间密度的点云尤其有效；当用于指导地图构建下游任务时，与最先进的基于分类的方法相比，可以在给定数量的重新配准帧数量下显著提高地图质量。&lt;h4&gt;结论&lt;/h4&gt;基于回归的PCR质量验证方法比现有的基于分类的方法更有效；通过多尺度特征提取和注意力聚合实现了更精细的质量量化；在实际应用中，该方法可以优化地图构建过程，提高地图质量。&lt;h4&gt;翻译&lt;/h4&gt;点云配准(PCR)对于许多下游任务至关重要，例如同时定位与地图构建(SLAM)和目标跟踪。这使得检测和量化配准错位，即PCR质量验证，成为一个重要任务。所有现有方法都将验证视为分类任务，旨在将PCR质量分配到几个类别中。在本工作中，我们使用回归进行PCR验证，允许对配准质量进行更细粒度的量化。我们还通过使用多尺度提取和基于注意力的聚合来扩展先前使用的与错位相关的特征。这导致在多样化数据集上准确且稳健的配准误差估计，特别是对于具有异构空间密度的点云。此外，当用于指导地图构建下游任务时，与最先进的基于分类的方法相比，我们的方法在给定数量的重新配准帧数量下显著提高了地图质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云配准(PCR)质量评估的问题，特别是检测和量化配准误差。这个问题在现实中非常重要，因为点云配准是SLAM、3D重建和机器人导航等下游任务的基础。配准误差会传播到下游模块，导致地图扭曲或导航失败。在挑战性场景下，如优化陷入局部最小值、运动失真或测量噪声时，配准误差仍然显著，因此需要准确评估配准质量以便采取纠正措施。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，即它们都是基于分类的，只能粗略地将错位分为几个级别。作者借鉴了FACT等工作的特征提取方法，但将其从分类改为回归任务。作者注意到单尺度特征的局限性，特别是小半径在初始对齐差时可能失败，而大半径包含非重叠区域使熵和Sinkhorn散度不可靠。因此，作者设计了多尺度注意力机制，在三个不同半径(7.5m、4.0m、2.5m)上提取特征，并通过注意力机制自适应地选择最适合的尺度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云错位评估从分类任务转变为回归任务，实现更细粒度的配准质量评估，并使用多尺度注意力机制来融合不同几何尺度的特征，提高对点云空间密度异质性的鲁棒性。整体实现流程包括：1)特征提取：为每个锚点计算局部特征(分别和联合的微分熵、Sinkhorn散度、覆盖率比)和全局特征(共视分数、距离、源标志)；2)多尺度特征提取：在三个不同半径尺度上提取特征；3)多尺度交叉注意力：使用查询学习MLP生成查询，通过多头注意力机制聚合多尺度特征；4)回归预测：使用PointTransformer和MLP预测对齐误差。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)任务转换：将点云错位分类扩展为点云错位回归，实现更细粒度的评估；2)多尺度注意力机制：提取多尺度几何特征并自适应选择最适合的尺度；3)鲁棒性增强：特别处理点云中空间密度异质性的情况。相比之前的工作，MATTER提供连续的误差预测而非离散分类，使用多尺度注意力而非固定尺度特征，实验表明在多个数据集上均优于现有方法，特别是在下游任务中能更好地指导重配准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MATTER通过引入多尺度注意力机制将点云配准质量评估从分类转变为回归任务，实现了更精确、更鲁棒的配准误差估计，特别是在处理空间密度异质的点云时表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration (PCR) is crucial for many downstream tasks, such assimultaneous localization and mapping (SLAM) and object tracking. This makesdetecting and quantifying registration misalignment, i.e.,~{\it PCR qualityvalidation}, an important task. All existing methods treat validation as aclassification task, aiming to assign the PCR quality to a few classes. In thiswork, we instead use regression for PCR validation, allowing for a morefine-grained quantification of the registration quality. We also extendpreviously used misalignment-related features by using multiscale extractionand attention-based aggregation. This leads to accurate and robust registrationerror estimation on diverse datasets, especially for point clouds withheterogeneous spatial densities. Furthermore, when used to guide a mappingdownstream task, our method significantly improves the mapping quality for agiven amount of re-registered frames, compared to the state-of-the-artclassification-based method.</description>
      <author>example@mail.com (Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik Forssén)</author>
      <guid isPermaLink="false">2509.12924v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.12878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种原型扩展网络(PENet)，用于解决少样本3D点云语义分割中的类内多样性和集合间不一致性问题。该方法利用扩散模型的条件编码器提供可泛化特征，通过双流学习器架构构建大容量原型，并通过原型同化模块和原型校准机制优化原型表示。&lt;h4&gt;背景&lt;/h4&gt;少样本3D点云语义分割旨在使用少量标注的支持样本分割新类别。现有的基于原型的方法虽然有一定效果，但面临类内多样性不足和集合间不一致性两大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于原型的方法在少样本3D点云语义分割中面临的类内多样性和集合间不一致性问题，提高对新类别的分割性能。&lt;h4&gt;方法&lt;/h4&gt;提出原型扩展网络(PENet)，采用双流学习器架构：保留传统的全监督内在学习器提取代表性特征，同时引入扩散学习器提供丰富的可泛化特征。双原型通过原型同化模块处理，采用推拉交叉引导注意力块迭代对齐原型与查询空间，并通过原型校准机制防止语义漂移。&lt;h4&gt;主要发现&lt;/h4&gt;在S3DIS和ScanNet数据集上的实验表明，PENet在各种少样本设置下显著优于最先进的方法。扩散模型提供的可泛化特征能有效扩展原型的表示能力，解决类内多样性问题；原型同化模块和校准机制能有效解决集合间不一致性问题。&lt;h4&gt;结论&lt;/h4&gt;原型扩展网络(PENet)通过结合传统监督学习和扩散模型的可泛化特征，有效解决了少样本3D点云语义分割中的关键挑战，显著提高了分割性能。&lt;h4&gt;翻译&lt;/h4&gt;少样本3D点云语义分割旨在使用少量标注的支持样本分割新类别。虽然现有的基于原型的方法显示出潜力，但它们受到两个关键挑战的限制：(1)类内多样性，原型的有限表示能力无法覆盖类的全部变化；(2)集合间不一致性，从支持集导出的原型与查询特征空间不匹配。受扩散模型强大生成能力的启发，我们重新利用其预训练条件编码器为原型提供一种新的可泛化特征来源，以扩展原型的表示范围。在此设置下，我们引入了原型扩展网络(PENet)，这是一个从两个互补特征源构建大容量原型的框架。PENet采用双流学习器架构：它保留传统的全监督内在学习器(IL)来提取代表性特征，同时引入新的扩散学习器(DL)来提供丰富的可泛化特征。然后，双原型通过原型同化模块(PAM)处理，该模块采用新颖的推拉交叉引导注意力块迭代地将原型与查询空间对齐。此外，原型校准机制(PCM)正则化最终的大容量原型以防止语义漂移。在S3DIS和ScanNet数据集上的大量实验表明，PENet在各种少样本设置下显著优于最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决少样本3D点云语义分割中的两个关键挑战：类内多样性（原型的有限表示能力无法覆盖一个类的全部变化）和集合间不一致性（支持集原型与查询特征空间不对齐）。这个问题在现实中很重要，因为3D点云语义分割在自动驾驶、机器人和增强现实等领域有广泛应用，而完全监督学习需要大量昂贵的标注数据。少样本学习方法使模型仅通过少量标注样本就能泛化到新类别，大大减少了对标注数据的依赖。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析认识到现有基于原型的方法面临相互关联的挑战：容量小的原型即使对齐也无法覆盖类的全部变化，而容量大的原型如果不对齐则无法在查询空间中有效工作。因此，他们提出通过'原型扩展'来增加原型的表示能力。他们借鉴了元学习框架，利用扩散模型的生成能力重新调整其预训练条件编码器，并采用双流架构结合传统监督学习和自监督学习。具体设计包括保留内在学习者提取代表性特征，引入扩散学习者提供可泛化特征，设计原型同化模块迭代对齐原型，以及引入原型校准机制防止语义漂移。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建'大容量原型'来解决少样本3D点云语义分割中的挑战，从两个互补特征源构建原型：一个来自传统监督学习（提供代表性特征），另一个来自扩散模型（提供可泛化特征）。整体流程包括：1)双流特征提取（内在学习者和扩散学习者）；2)从支持集特征生成初始原型；3)通过原型同化模块迭代对齐双原型与查询空间；4)将对齐后的原型融合成最终的大容量原型；5)使用原型校准机制防止语义漂移；6)用最终原型对查询点云进行语义分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)原型扩展网络框架，首次将扩散模型应用于少样本3D点云语义分割；2)双流学习架构，结合传统监督学习和扩散模型特征；3)原型同化模块，采用推-拉交叉引导注意力机制迭代对齐原型；4)原型校准机制，防止语义漂移。相比之前工作的不同在于：大多数方法依赖单一特征源，而本文从两个互补特征源构建原型；传统方法采用单步对齐，本文采用迭代推-拉机制；本文首次利用扩散模型条件编码器作为特征提取器；现有方法通过增加原型数量而非扩展单个原型容量来解决多样性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了原型扩展网络（PENet），通过融合来自传统监督学习和扩散模型的互补特征构建大容量原型，有效解决了少样本3D点云语义分割中的类内多样性和集合间不一致性问题，显著提升了模型在未见类别上的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot 3D point cloud semantic segmentation aims to segment novelcategories using a minimal number of annotated support samples. While existingprototype-based methods have shown promise, they are constrained by twocritical challenges: (1) Intra-class Diversity, where a prototype's limitedrepresentational capacity fails to cover a class's full variations, and (2)Inter-set Inconsistency, where prototypes derived from the support set aremisaligned with the query feature space. Motivated by the powerful generativecapability of diffusion model, we re-purpose its pre-trained conditionalencoder to provide a novel source of generalizable features for expanding theprototype's representational range. Under this setup, we introduce thePrototype Expansion Network (PENet), a framework that constructs big-capacityprototypes from two complementary feature sources. PENet employs a dual-streamlearner architecture: it retains a conventional fully supervised IntrinsicLearner (IL) to distill representative features, while introducing a novelDiffusion Learner (DL) to provide rich generalizable features. The resultingdual prototypes are then processed by a Prototype Assimilation Module (PAM),which adopts a novel push-pull cross-guidance attention block to iterativelyalign the prototypes with the query space. Furthermore, a Prototype CalibrationMechanism (PCM) regularizes the final big capacity prototype to preventsemantic drift. Extensive experiments on the S3DIS and ScanNet datasetsdemonstrate that PENet significantly outperforms state-of-the-art methodsacross various few-shot settings.</description>
      <author>example@mail.com (Qianguang Zhao, Dongli Wang, Yan Zhou, Jianxun Li, Richard Irampa)</author>
      <guid isPermaLink="false">2509.12878v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>A-TDOM: Active TDOM via On-the-Fly 3DGS</title>
      <link>http://arxiv.org/abs/2509.12759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种称为A-TDOM的近实时真正数字正射地图生成方法，基于即时3D高斯优化技术，能够在获取每张图像后快速计算姿态和稀疏点云，并将新的高斯函数集成和优化到先前未看到或粗略重建的区域，实现近实时渲染。&lt;h4&gt;背景&lt;/h4&gt;真正数字正射地图(TDOM)是城市管理、城市规划、土地测量等领域的关键地理空间产品。然而，传统TDOM生成方法依赖复杂的离线摄影测量流程，导致延迟阻碍了实时应用，且因摄像机姿态不准确、数字表面模型和场景遮挡等问题，质量可能下降。&lt;h4&gt;目的&lt;/h4&gt;解决传统TDOM生成方法的延迟问题，实现近实时生成，同时保持可接受的渲染质量和几何精度。&lt;h4&gt;方法&lt;/h4&gt;提出A-TDOM方法，基于即时3D高斯优化：每获取一张图像后，通过即时SfM计算姿态和稀疏点云；将新的高斯函数集成并优化到先前未看到或粗略重建的区域；结合正交渲染，在每次更新3D高斯场后立即渲染。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的实验表明，A-TDOM能够在近实时主动渲染TDOM，每张新图像的3D高斯优化仅需几秒钟，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;结论&lt;/h4&gt;A-TDOM方法有效解决了传统TDOM生成方法的延迟问题，通过即时3D高斯优化实现了近实时生成，同时保持了良好的质量和精度，为需要实时TDOM的应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;真正数字正射地图(TDOM)是城市管理、城市规划、土地测量等各个领域中的关键地理空间产品。然而，传统的TDOM生成方法通常依赖于复杂的离线摄影测量流程，导致延迟阻碍了实时应用。此外，由于摄像机姿态不准确、数字表面模型(DSM)和场景遮挡等挑战，TDOM的质量可能会下降。为了解决这些挑战，本文介绍了A-TDOM，一种基于即时3D高斯优化的近实时TDOM生成方法。每获取一张图像，其姿态和稀疏点云通过即时SfM计算。然后新的高斯函数被集成并优化到先前未看到或粗略重建的区域。通过与正交渲染结合，A-TDOM可以在每次更新3D高斯场后立即渲染。在多个基准测试上的初步实验表明，所提出的A-TDOM能够在近实时主动渲染TDOM，每张新图像的3D高斯优化仅需几秒钟，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统TDOM（真正的数字正射影像图）生成方法的实时性问题。传统方法依赖复杂的离线摄影测量流程，导致处理延迟，无法满足实时应用需求，同时还存在相机姿态不准确、数字表面模型和场景遮挡等问题影响质量。这个问题在现实中非常重要，因为TDOM是城市管理、城市规划、土地测量等领域的关键地理空间产品，实时生成TDOM可以大大提高工作效率，支持灾害监测、城市规划动态调整、土地资源调查等场景中的及时决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有TDOM生成方法的局限性，包括传统方法计算成本高和新兴方法仍采用离线处理。作者借鉴了多项现有工作：基于On-the-Fly SfM进行实时姿态估计、利用3D高斯投影技术进行场景表示、参考Tortho-Gaussian和Ortho-3DGS等3DGS-based方法，以及Gaussian On-the-Fly Splatting的优化策略。作者的创新思路是将这些技术整合到一个在线框架中，通过即时更新和优化3DGS场，实现近实时TDOM生成，并设计了高斯采样和集成方法替代原始3DGS中的密集化策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; A-TDOM方法的核心思想是通过即时更新和优化3D高斯投影场(3DGS)，实现近实时的TDOM生成，无需依赖数字表面模型(DSM)和显式遮挡检测。整体流程是：1)初始化训练一个初始3DGS场；2)对于每张新图像：a)使用On-the-Fly SfM计算姿态并更新稀疏点云；b)基于稀疏点云重投影进行Delaunay三角剖分，创建掩码确定关键优化区域；c)比较当前3DGS场渲染与输入图像的梯度图，识别需要额外高斯集成的区域；d)在选定区域内采样点，生成新高斯函数并集成；e)优化更新后的3DGS场；f)通过正交投影生成更新的TDOM。这一流程允许系统在图像采集过程中持续更新3D表示并实时生成高质量TDOM。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)近实时TDOM生成框架，首次实现近实时生成；2)高斯采样和集成方法，替代原始3DGS密集化策略；3)关键区域确定机制，通过Delaunay三角掩码确定优化区域；4)自适应优化策略，包括自适应学习率更新和局部优化；5)基于投影矩阵修改的正交投影方法。相比之前的工作，A-TDOM的主要不同在于：将离线处理转变为在线近实时处理；无需依赖DSM和显式遮挡检测；通过高斯采样和集成更有效地扩展3DGS场；采用自适应优化加速处理；实现了图像采集与TDOM生成的同步进行。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; A-TDOM通过创新的即时3D高斯投影优化和正交投影技术，首次实现了无需数字表面模型和遮挡检测的近实时真正的数字正射影像图生成，显著提高了地理空间信息获取的效率和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product invarious fields such as urban management, city planning, land surveying, etc.However, traditional TDOM generation methods generally rely on a complexoffline photogrammetric pipeline, resulting in delays that hinder real-timeapplications. Moreover, the quality of TDOM may degrade due to variouschallenges, such as inaccurate camera poses or Digital Surface Model (DSM) andscene occlusions. To address these challenges, this work introduces A-TDOM, anear real-time TDOM generation method based on On-the-Fly 3DGS optimization. Aseach image is acquired, its pose and sparse point cloud are computed viaOn-the-Fly SfM. Then new Gaussians are integrated and optimized into previouslyunseen or coarsely reconstructed regions. By integrating with orthogonalsplatting, A-TDOM can render just after each update of a new 3DGS field.Initial experiments on multiple benchmarks show that the proposed A-TDOM iscapable of actively rendering TDOM in near real-time, with 3DGS optimizationfor each new image in seconds while maintaining acceptable rendering qualityand TDOM geometric accuracy.</description>
      <author>example@mail.com (Yiwei Xu, Xiang Wang, Yifei Yu, Wentian Gan, Luca Morelli, Giulio Perda, Xiongwu Xiao, Zongqian Zhan, Xin Wang, Fabio Remondino)</author>
      <guid isPermaLink="false">2509.12759v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>DisorientLiDAR: Physical Attacks on LiDAR-based Localization</title>
      <link>http://arxiv.org/abs/2509.12595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DisorientLiDAR的新型对抗攻击框架，针对基于激光雷达的定位系统。通过逆向工程定位模型，攻击者可以识别并移除关键点，从而破坏自动驾驶汽车的定位能力。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型容易受到视觉上难以察觉的扰动的对抗攻击，这对自动驾驶汽车的定位构成严重安全挑战。然而，针对定位的攻击探索很少，因为大多数对抗攻击应用于3D感知而非定位系统。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对基于激光雷达(LiDAR)的定位系统的对抗攻击方法，以揭示自动驾驶定位系统的安全漏洞。&lt;h4&gt;方法&lt;/h4&gt;提出DisorientLiDAR攻击框架，通过逆向工程定位模型(如特征提取网络)识别关键点，策略性地移除这些关键点区域来破坏基于激光雷达的定位。实验在KITTI数据集上使用三种点云配准模型(HRegNet、D3Feat和GeoTransformer)进行评估，并在Autoware自动驾驶平台和物理世界中验证了攻击效果。&lt;h4&gt;主要发现&lt;/h4&gt;移除包含Top-K关键点的区域显著降低了点云配准模型的精度；在Autoware平台上，隐藏几个关键区域会导致明显的定位漂移；使用近红外吸收材料在物理世界中隐藏关键区域可以复制数字攻击效果。&lt;h4&gt;结论&lt;/h4&gt;所提出的DisorientLiDAR攻击框架成功展示了自动驾驶定位系统的脆弱性，通过物理世界的攻击验证了方法的真实性和通用性，为提高自动驾驶系统的安全性提供了重要参考。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型已被证明容易受到具有视觉上难以察觉的扰动的对抗攻击。即使这对自动驾驶汽车的定位构成了严重的安全挑战，但针对它的攻击探索很少，因为大多数对抗攻击已被应用于3D感知。在这项工作中，我们提出了一种名为DisorientLiDAR的新型对抗攻击框架，针对基于激光雷达的定位。通过逆向工程定位模型(例如特征提取网络)，攻击者可以识别关键点并策略性地移除它们，从而破坏基于激光雷达的定位。我们的提案首先使用KITTI数据集在三种最先进的点云配准模型(HRegNet、D3Feat和GeoTransformer)上进行了评估。实验结果表明，移除包含Top-K关键点的区域显著降低了它们的配准精度。我们进一步验证了攻击对Autoware自动驾驶平台的影响，在那里隐藏仅几个关键区域就会引起明显的定位漂移。最后，我们通过使用近红外吸收材料隐藏关键区域，将攻击扩展到物理世界，从而成功复制了在KITTI数据中观察到的攻击效果。这一步骤更接近真实的物理世界攻击，证明了我们提案的真实性和通用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文研究如何对基于LiDAR的自动驾驶汽车定位系统进行物理攻击。这个问题非常重要，因为自动驾驶汽车的安全高度依赖准确定位，而定位错误可能导致严重事故。尽管已有研究证明深度学习模型对对抗攻击很脆弱，但对定位系统的攻击探索很少，且即使有GPS和IMU支持，这种攻击仍然有效，对自动驾驶安全构成重大威胁。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有的点云注册方法，发现它们都依赖检测和匹配关键几何特征区域。基于这一观察，作者设计了一种攻击策略：通过逆向工程定位模型识别关键点，然后使用红外吸收材料物理覆盖这些区域，使它们从LiDAR扫描中'消失'。作者借鉴了现有的LiDAR对抗攻击工作，但与之前需要物理访问LiDAR传感器的方法不同，这种方法只需在道路旁放置材料即可实施，更加隐蔽且易于部署。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是识别并移除对LiDAR定位系统最关键的几何特征区域，干扰点云匹配过程导致定位错误。整体流程包括：1)使用与受害者相同的定位模型提取高置信度关键点；2)选择Top-K最显著的关键点；3)用红外吸收材料覆盖这些关键区域对应的物理位置；4)当受害车辆经过时，其LiDAR无法获取这些区域数据，导致定位错误；5)评估攻击效果。为确保攻击可行性和隐蔽性，作者还提出了高度过滤、轨迹接近检查、重叠处理和埋伏工具放置等筛选步骤。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出首个通过物理隐藏关键区域干扰定位系统的攻击框架；无需物理访问LiDAR传感器；在多种点云注册模型上验证了攻击通用性；在真实世界车辆和商业平台上进行了物理攻击验证；系统化评估了攻击参数影响。相比之前工作，本文专注于定位系统而非感知模块；通过物理遮挡而非激光注入实施攻击；在多种环境和LiDAR配置上评估；同时提出了防御策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提出并验证了一种通过物理遮挡关键区域来干扰基于LiDAR的自动驾驶定位系统的攻击方法，揭示了定位系统的重要安全漏洞，并为开发更安全的定位系统提供了方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models have been shown to be susceptible to adversarial attackswith visually imperceptible perturbations. Even this poses a serious securitychallenge for the localization of self-driving cars, there has been very littleexploration of attack on it, as most of adversarial attacks have been appliedto 3D perception. In this work, we propose a novel adversarial attack frameworkcalled DisorientLiDAR targeting LiDAR-based localization. Byreverse-engineering localization models (e.g., feature extraction networks),adversaries can identify critical keypoints and strategically remove them,thereby disrupting LiDAR-based localization. Our proposal is first evaluated onthree state-of-the-art point-cloud registration models (HRegNet, D3Feat, andGeoTransformer) using the KITTI dataset. Experimental results demonstrate thatremoving regions containing Top-K keypoints significantly degrades theirregistration accuracy. We further validate the attack's impact on the Autowareautonomous driving platform, where hiding merely a few critical regions inducesnoticeable localization drift. Finally, we extended our attacks to the physicalworld by hiding critical regions with near-infrared absorptive materials,thereby successfully replicate the attack effects observed in KITTI data. Thisstep has been closer toward the realistic physical-world attack thatdemonstrate the veracity and generality of our proposal.</description>
      <author>example@mail.com (Yizhen Lao, Yu Zhang, Ziting Wang, Chengbo Wang, Yifei Xue, Wanpeng Shao)</author>
      <guid isPermaLink="false">2509.12595v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery</title>
      <link>http://arxiv.org/abs/2509.12511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于几何感知的计算机视觉流水线，用于从RGB-D图像中估计茎直径，旨在解决传统测量方法劳动强度大、易出错的问题，为作物育种提供高通量表型分析解决方案。&lt;h4&gt;背景&lt;/h4&gt;准确的高通量表型分析是现代作物育种项目的关键组成部分，特别是对于改善机械稳定性、生物量生产和抗病性等性状。茎直径是一个关键的结构性状，但传统测量方法劳动密集、容易出错，不适合可扩展的表型分析。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确、高效测量茎直径的方法，支持作物育种和农学研究中的高通量表型分析。&lt;h4&gt;方法&lt;/h4&gt;提出一种几何感知的计算机视觉流水线，结合基于深度学习的实例分割、3D点云重建和通过主成分分析(PCA)的轴对齐切片，以执行鲁棒的直径估计。&lt;h4&gt;主要发现&lt;/h4&gt;通过减轻曲率、遮挡和图像噪声的影响，该方法提供了可扩展且可靠的解决方案，支持育种和农学研究中的高通量表型分析。&lt;h4&gt;结论&lt;/h4&gt;该几何感知的计算机视觉方法为茎直径测量提供了一种高效、准确的解决方案，克服了传统方法的局限性，为作物育种提供了重要工具。&lt;h4&gt;翻译&lt;/h4&gt;准确的高通量表型分析是现代作物育种项目的关键组成部分，特别是对于改善机械稳定性、生物量生产和抗病性等性状。茎直径是一个关键的结构性状，但传统测量方法劳动密集、容易出错，不适合可扩展的表型分析。在本文中，我们提出了一种几何感知的计算机视觉流水线，用于从RGB-D图像中估计茎直径。我们的方法结合了基于深度学习的实例分割、3D点云重建和通过主成分分析(PCA)的轴对齐切片，以执行鲁棒的直径估计。通过减轻曲率、遮挡和图像噪声的影响，这种方法为育种和农学研究中的高通量表型分析提供了可扩展且可靠的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从RGB-D图像中自动估计植物茎秆直径的问题。这个问题很重要，因为茎秆直径影响作物的抗倒伏能力、生物量分配、水力功能和疾病易感性，是作物表型分析的关键指标。传统测量方法劳动密集且不适合高通量表型分析，而随着2050年全球人口预计达到97亿，自动化测量对确保粮食安全和提高农业生产力至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统测量方法的局限性，以及RGB-D传感器在自动化测量中的潜力。他们注意到现有方法（如颜色阈值、边缘检测）假设茎秆垂直对齐，限制了可靠性；更先进的方法虽改善了检测，但缺乏稳健的轴对齐。作者借鉴了深度学习在农业环境中的应用，结合了基于深度学习的实例隔离、3D点云重建和PCA轴估计，设计出能处理茎杆曲率和任意方向的轴对齐测量方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过确定茎杆的真实主轴方向，然后垂直于此轴进行横截面切片来提高测量准确性。整体流程包括：1)使用RGB-D相机采集数据；2)用YOLOv11x-seg模型分割茎杆区域；3)构建3D点云并进行过滤；4)应用PCA确定茎杆主轴；5)垂直于主轴将点云分为100个切片；6)对每个切片应用DBSCAN过滤并使用95百分位数估计半径；7)聚合切片结果得到最终直径估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)几何感知的计算机视觉管道，结合深度学习、3D重建和PCA轴估计；2)轴对齐的横截面测量，提高准确性和鲁棒性；3)能有效处理茎杆曲率和任意方向；4)使用95百分位数圆拟合和1-标准差聚合处理噪声；5)发现初始统计异常值移除过滤器冗余。相比之前工作，不同之处在于传统方法假设垂直对齐，而先进方法缺乏稳健轴对齐；本文方法能处理各种方向，并通过统计方法有效处理传感器噪声。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文开发了一种基于几何感知的计算机视觉管道，通过结合深度学习实例分割、3D点云重建和轴对齐的横截面切片，实现了从RGB-D图像中高精度、自动化地估计植物茎杆直径，为作物表型分析和农业管理提供了强大的工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate, high-throughput phenotyping is a critical component of modern cropbreeding programs, especially for improving traits such as mechanicalstability, biomass production, and disease resistance. Stalk diameter is a keystructural trait, but traditional measurement methods are labor-intensive,error-prone, and unsuitable for scalable phenotyping. In this paper, we presenta geometry-aware computer vision pipeline for estimating stalk diameter fromRGB-D imagery. Our method integrates deep learning-based instance segmentation,3D point cloud reconstruction, and axis-aligned slicing via Principal ComponentAnalysis (PCA) to perform robust diameter estimation. By mitigating the effectsof curvature, occlusion, and image noise, this approach offers a scalable andreliable solution to support high-throughput phenotyping in breeding andagronomic research.</description>
      <author>example@mail.com (Benjamin Vail, Rahul Harsha Cheppally, Ajay Sharda, Sidharth Rai)</author>
      <guid isPermaLink="false">2509.12511v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Artist-Created Mesh Generation from Raw Observation</title>
      <link>http://arxiv.org/abs/2509.12501v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种端到端框架，能够从嘈杂或不完整的点云生成高质量艺术家风格的网格，通过将3D点云精炼重新表述为2D修复任务，利用生成模型实现。&lt;h4&gt;背景&lt;/h4&gt;艺术家创建的网格对商业图形管道至关重要，因为它们与动画和纹理工具兼容且渲染效率高。然而，现有方法通常假设输入干净完整或依赖复杂的多阶段管道，限制了它们在现实场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种端到端方法，能够精炼输入点云并直接生成高质量、艺术家风格的网格，适用于现实世界传感器获取的嘈杂或不完整点云数据。&lt;h4&gt;方法&lt;/h4&gt;提出了一种将3D点云精炼重新表述为2D修复任务的方法，从而能够利用强大的生成模型。该方法是一个端到端的框架，直接从原始点云生成高质量网格。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet数据集上的初步结果表明，该框架能够生成干净、完整的网格，证明了其在处理现实世界传感器数据方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的端到端框架为从嘈杂或不完整的点云生成高质量艺术家风格网格提供了有效解决方案，适用于商业图形管道和现实应用场景。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种端到端框架，用于从嘈杂或不完整的点云生成艺术家风格的网格，例如由现实世界传感器如LiDAR或移动RGB-D相机捕获的点云。艺术家创建的网格对商业图形管道至关重要，因为它们与动画和纹理工具兼容，并且在渲染中效率高。然而，现有方法通常假设输入干净完整或依赖复杂的多阶段管道，限制了它们在现实场景中的应用。为此，我们提出了一种端到端方法，可以精炼输入点云并直接生成高质量、艺术家风格的网格。我们方法的核心是将3D点云精炼重新表述为2D修复任务，从而能够使用强大的生成模型。在ShapeNet数据集上的初步结果证明了我们的框架在生成干净、完整网格方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从真实世界传感器（如LiDAR或移动RGB-D相机）捕获的嘈杂或不完整点云中生成高质量艺术风格网格的问题。这个问题在现实中很重要，因为艺术风格网格与商业图形管线兼容，适合动画和纹理处理，且渲染效率高，而现有方法往往需要干净完整的输入或复杂的多阶段处理流程，难以应用于真实世界场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，认识到艺术风格网格生成需要干净输入，而真实传感器数据往往嘈杂不完整。他们创新性地将3D点云精炼重新表述为2D修复问题，利用成熟的2D生成模型能力。方法设计包括将3D点云投影到球形地图集、应用扩散模型修复、再映射回3D并生成网格。作者借鉴了Stable Diffusion作为2D生成模型，MeshAnything V2作为网格生成模型，以及将3D转化为2D处理的高斯地图集方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云精炼问题转化为2D修复任务，利用强大的2D生成模型处理3D数据。整体流程分为四步：1)将3D点云通过最优传输映射到球面，再通过等距矩形投影转换为2D地图集；2)使用微调的扩散模型修复不完整的地图集；3)将修复后的地图集映射回3D，生成干净完整的点云；4)将点云输入MeshAnything V2模型生成最终艺术风格网格。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)端到端框架直接从原始点云生成艺术风格网格；2)首次将3D点云精炼重新表述为2D修复任务；3)使用球形地图集表示保留3D几何信息；4)利用扩散模型进行地图集修复。相比之前工作，不同之处在于：可直接处理嘈杂不完整输入而非假设干净数据；采用统一端到端框架而非多阶段处理；利用成熟的2D生成模型而非专门开发3D模型；直接生成适合商业管线的拓扑结构网格。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新方法，通过将3D点云精炼转化为2D修复任务，实现了从嘈杂或不完整的真实世界传感器数据直接生成高质量艺术风格网格的端到端框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an end-to-end framework for generating artist-style meshes fromnoisy or incomplete point clouds, such as those captured by real-world sensorslike LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial forcommercial graphics pipelines due to their compatibility with animation andtexturing tools and their efficiency in rendering. However, existing approachesoften assume clean, complete inputs or rely on complex multi-stage pipelines,limiting their applicability in real-world scenarios. To address this, wepropose an end-to-end method that refines the input point cloud and directlyproduces high-quality, artist-style meshes. At the core of our approach is anovel reformulation of 3D point cloud refinement as a 2D inpainting task,enabling the use of powerful generative models. Preliminary results on theShapeNet dataset demonstrate the promise of our framework in producing clean,complete meshes.</description>
      <author>example@mail.com (Yao He, Youngjoong Kwon, Wenxiao Cai, Ehsan Adeli)</author>
      <guid isPermaLink="false">2509.12501v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</title>
      <link>http://arxiv.org/abs/2509.12452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  57 Pages, 4 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对点云处理的深度学习方法和数据集进行了元综述，重点关注实际应用价值而非仅关注网络架构。&lt;h4&gt;背景&lt;/h4&gt;点云处理是大地测量学和计算机视觉的基础任务，支持从空中到地面的多种应用，包括测绘、环境监测、城市建模、自动驾驶等。深度学习的发展使基于学习的方法主导了点云处理算法，但这些方法大多尚未转化为实际应用。&lt;h4&gt;目的&lt;/h4&gt;提供点云处理关键任务的深度学习方法和数据集的元综述，关注这些方法在实际应用中的价值和挑战。&lt;h4&gt;方法&lt;/h4&gt;回顾了涵盖场景补全、配准、语义分割和建模等关键任务的深度学习方法和数据集，并通过分析这些任务支持的城市和环境应用，识别方法转化为应用时需要填补的差距。&lt;h4&gt;主要发现&lt;/h4&gt;现有调查主要集中在适应无序点云的网络架构更新上，忽略了典型点云处理应用中的实际价值，如大量数据处理、多样化场景内容、变化的点密度和数据模态等实际考量。&lt;h4&gt;结论&lt;/h4&gt;在算法和实践两方面对所调查的方法得出了结论，强调了点云处理从理论研究向实际应用转化的重要性。&lt;h4&gt;翻译&lt;/h4&gt;点云处理作为大地测量学和计算机视觉领域的基础任务，一直支持着从空中到地面不同规模的任务和应用，包括测绘、环境监测、城市/树木结构建模、自动驾驶、机器人技术、灾害响应等。由于深度学习的快速发展，点云处理算法现在几乎明确由基于学习的方法主导，但其中大多数尚未转化为实际应用实践。现有调查主要集中在不断更新的网络架构上，以适应无序点云，很大程度上忽略了它们在典型点云处理应用中的实际价值，在这些应用中需要考虑大量数据、多样化场景内容、变化的点密度和数据模态。在本文中，我们对点云处理中使用的深度学习方法和数据集进行了元综述，涵盖了场景补全、配准、语义分割和建模等关键任务。通过回顾这些任务可以支持的广泛城市和环境应用，我们确定了方法转化为应用时需要填补的差距，并对所调查方法的算法和实践两方面得出了结论。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云处理算法在实际应用中的价值和挑战问题。现实中，点云处理支持从空中到地面的多种应用如自动驾驶、导航、测绘等，但现有算法大多尚未过渡到实际应用。这个问题很重要，因为实际应用中需要考虑大量数据、多样化场景内容、不同点密度和数据模态等因素，而这些在现有综述中被忽视，导致理论与实践之间存在差距。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察现有综述主要关注网络架构而非实际应用价值，思考需要一种更全面的元综述方法。他们设计了涵盖点云处理关键任务（场景补全、配准、分割、建模）的综述框架，并关注这些任务在城市和环境应用中的支持作用。作者借鉴了现有的点云处理任务分类、深度学习方法（基于体素、多视图、基于点的方法）以及应用领域（城市建模、林业等），但将这些元素整合到一个关注实际应用的元综述框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过元综述深度学习在点云处理中的应用，从任务到实际应用，填补算法理论与实际应用之间的差距。整体流程包括：1)概述点云数据生成方式；2)详细讨论四个主要任务（场景补全、点云配准、分割、建模）和相关算法；3)回顾这些任务在城市建模、林业、农业等下游应用中的支持作用；4)讨论方法和应用，总结深度模型在点云处理中的展望。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)关注算法的实际应用价值而非仅关注网络架构；2)考虑实际应用中的额外挑战如大数据量、多样化场景内容等；3)提供从任务到应用的元综述；4)识别算法转化为应用时需要填补的差距。相比之前的工作，这篇论文不再局限于网络架构的枚举，而是建立了算法与实际应用之间的连接，并比较了不同学习模型在不同任务中的角色与传统方法的差异。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过元综述深度学习在点云处理中的应用，从任务到实际城市和环境应用，填补了算法理论与实际应用之间的差距，为点云处理技术的实际部署提供了指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud processing as a fundamental task in the field of geomatics andcomputer vision, has been supporting tasks and applications at different scalesfrom air to ground, including mapping, environmental monitoring, urban/treestructure modeling, automated driving, robotics, disaster responses etc. Due tothe rapid development of deep learning, point cloud processing algorithms havenowadays been almost explicitly dominated by learning-based approaches, most ofwhich are yet transitioned into real-world practices. Existing surveysprimarily focus on the ever-updating network architecture to accommodateunordered point clouds, largely ignoring their practical values in typicalpoint cloud processing applications, in which extra-large volume of data,diverse scene contents, varying point density, data modality need to beconsidered. In this paper, we provide a meta review on deep learning approachesand datasets that cover a selection of critical tasks of point cloud processingin use such as scene completion, registration, semantic segmentation, andmodeling. By reviewing a broad range of urban and environmental applicationsthese tasks can support, we identify gaps to be closed as these methodstransformed into applications and draw concluding remarks in both thealgorithmic and practical aspects of the surveyed methods.</description>
      <author>example@mail.com (Zhenxin Zhang, Zhihua Xu, Yuwei Cao, Ningli Xu, Shuye Wang, Shen'ao Cui, Zhen Li, Rongjun Qin)</author>
      <guid isPermaLink="false">2509.12452v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction</title>
      <link>http://arxiv.org/abs/2509.12430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了MechBench基准数据集和DYNAMO模型，用于解决从静态几何结构理解耦合机械运动的问题。MechBench包含693个合成齿轮组件，DYNAMO是一个依赖感知的神经模型，可直接从CAD点云预测零件运动轨迹，实验表明该方法优于基线，实现了准确且时间一致性的预测。&lt;h4&gt;背景&lt;/h4&gt;从静态几何结构理解铰接机械组件的运动是3D感知和设计自动化中的核心挑战。现有方法通常针对日常铰接物体（如门和笔记本电脑），假设简化的运动结构或依赖关节标注。但在机械组件（如齿轮）中，运动源于几何耦合（通过啮合齿或对齐轴），使得仅从几何结构推断关系运动变得困难。&lt;h4&gt;目的&lt;/h4&gt;解决从静态几何结构理解机械组件中耦合运动的问题，特别是那些通过几何耦合而非预定义关节产生运动的组件。为此，作者引入了MechBench数据集和DYNAMO模型，建立了一个系统性框架用于数据驱动的耦合机械运动学习。&lt;h4&gt;方法&lt;/h4&gt;作者提出了两个主要贡献：1) MechBench：一个包含693个多样化合成齿轮组件的基准数据集，具有零件级真实运动轨迹，提供研究耦合运动的结构化环境；2) DYNAMO：一个依赖感知的神经模型，可直接从分割的CAD点云预测每个零件的运动轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，DYNAMO模型在各种齿轮配置下实现了准确且时间一致性的运动轨迹预测，性能优于现有的强基线方法。这证明了从几何结构推断耦合机械运动的可行性。&lt;h4&gt;结论&lt;/h4&gt;MechBench和DYNAMO共同建立了一个新颖的系统性框架，用于CAD组件中耦合机械运动的数据驱动学习，解决了从静态几何理解机械运动的核心挑战。&lt;h4&gt;翻译&lt;/h4&gt;理解铰接机械组件从静态几何的运动是3D感知和设计自动化中的核心挑战。先前关于日常铰接物体（如门和笔记本电脑）的工作通常假设简化的运动结构或依赖关节标注。然而，在机械组件（如齿轮）中，运动源于几何耦合，通过啮合齿或对齐轴产生，这使得现有方法难以仅从几何结构推断关系运动。为解决这一差距，我们引入了MechBench，一个包含693个多样化合成齿轮组件的基准数据集，具有零件级真实运动轨迹。MechBench提供了一个研究耦合运动的结构化环境，其中零件动力学由接触和传动而非预定义关节引起。基于此，我们提出了DYNAMO，一个依赖感知的神经模型，可直接从分割的CAD点云预测每个零件的运动轨迹。实验表明，DYNAMO优于强基线方法，在各种齿轮配置下实现了准确且时间一致性的预测。MechBench和DYNAMO共同建立了一个新颖的系统性框架，用于CAD组件中耦合机械运动的数据驱动学习。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决铰接式机械装配体（特别是齿轮系统）从静态几何形状预测运动的问题。在现有方法中，运动预测通常假设简化的运动学结构或依赖关节标注，但在机械装配体中，运动源于零件间的几何耦合（如齿轮啮合或对齐轴），这使得从几何推理关系运动变得困难。这个问题在3D感知和设计自动化领域至关重要，因为现代CAD工具虽然提供详细3D几何，但很少包含可执行的运动规范，限制了它们在机器人应用（如装配自动化、运动规划和仿真）中的实用性。机器人要与、操作甚至设计这样的系统，理解其组成部分如何移动是必不可少的。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：大多数方法通过估计单个零件的机动性来建模运动学，然后应用固定方程组进行运动生成，这种方法在零件运动独立时有效，但在机械装配体中，零件运动相互依赖。作者选择基于齿轮的机制作为研究重点，因为它们的运动通过接触、啮合和传动传播。由于现有数据集很少包括耦合运动，作者创建了MechBench数据集。DYNAMO方法借鉴了PointNet++进行特征提取、图神经网络建模关系和Transformer进行时间解码等现有技术，但进行了改进以处理耦合运动，通过建模零件间的机械耦合关系来捕获运动传播。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'依赖感知'，即认识到机械装配体中零件的运动不是独立的，而是通过接触和耦合相互依赖的。DYNAMO直接从分割的CAD点云预测每个零件的SE(3)运动轨迹，而不依赖于关节标注或预定义的运动学结构。整体实现流程包括：1) 使用PointNet++提取每个零件的几何特征；2) 构建零件图并使用接触启发式方法估计耦合矩阵，然后应用耦合感知的图神经网络建模零件间关系；3) 将零件特征在时间帧上复制并与位置编码结合，使用Transformer编码器处理时间依赖关系，并通过MLP预测6D扭转向量；4) 使用多项损失函数（平移L2损失、旋转测地损失和时间一致性损失）训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) MechBench数据集，包含693个合成齿轮装配体，专门用于评估耦合机械运动预测；2) DYNAMO依赖感知神经模型，能联合推理零件间关系；3) 通过图神经网络建模零件间的机械耦合关系；4) 实现从静态CAD几何端到端学习耦合刚体运动。相比之前工作，DYNAMO明确处理零件间的耦合关系而非假设独立运动；专注于机械装配体而非日常铰接物体；使用6D Lie代数向量确保预测的运动有效且连续；不依赖预定义关节信息而是从几何中学习；提供了专门针对耦合机械运动预测的数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DYNAMO和MechBench共同建立了一个新颖的系统性框架，用于从CAD装配体的静态几何形状中学习耦合机械运动，通过依赖感知的神经网络准确预测相互连接零件的运动轨迹。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the motion of articulated mechanical assemblies from staticgeometry remains a core challenge in 3D perception and design automation. Priorwork on everyday articulated objects such as doors and laptops typicallyassumes simplified kinematic structures or relies on joint annotations.However, in mechanical assemblies like gears, motion arises from geometriccoupling, through meshing teeth or aligned axes, making it difficult forexisting methods to reason about relational motion from geometry alone. Toaddress this gap, we introduce MechBench, a benchmark dataset of 693 diversesynthetic gear assemblies with part-wise ground-truth motion trajectories.MechBench provides a structured setting to study coupled motion, where partdynamics are induced by contact and transmission rather than predefined joints.Building on this, we propose DYNAMO, a dependency-aware neural model thatpredicts per-part SE(3) motion trajectories directly from segmented CAD pointclouds. Experiments show that DYNAMO outperforms strong baselines, achievingaccurate and temporally consistent predictions across varied gearconfigurations. Together, MechBench and DYNAMO establish a novel systematicframework for data-driven learning of coupled mechanical motion in CADassemblies.</description>
      <author>example@mail.com (Mayank Patel, Rahul Jain, Asim Unmesh, Karthik Ramani)</author>
      <guid isPermaLink="false">2509.12430v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning</title>
      <link>http://arxiv.org/abs/2509.11594v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper needs major revision&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GBPP是一种基于快速学习的评分器，可以从单个RGB-D快照中选择机器人抓取的基础姿态。它使用两阶段课程学习方法：第一阶段使用简单的距离-可见性规则低成本自动标记大型数据集；第二阶段使用较小的高保真模拟试验集优化模型以匹配真实抓取结果。PointNet++风格的点云编码器与多层感知机一起对候选姿态进行评分，实现快速在线选择，无需完整的任务和运动优化。&lt;h4&gt;背景&lt;/h4&gt;机器人抓取任务中需要选择合适的基础姿态，传统方法可能需要复杂的计算或大量数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速学习评分器(GBPP)，能够从单个RGB-D图像中选择机器人抓取的基础姿态。&lt;h4&gt;方法&lt;/h4&gt;两阶段课程学习：第一阶段使用简单的距离-可见性规则低成本自动标记大型数据集；第二阶段使用高保真模拟试验集优化模型。使用PointNet++风格的点云编码器与多层感知机对候选姿态进行评分，实现快速在线选择，无需完整的任务和运动优化。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实移动机械臂上，GBPP优于仅依赖接近度和几何形状的基线方法；GBPP能选择更安全、更可达的姿态；在错误情况下表现良好(优雅降级)。&lt;h4&gt;结论&lt;/h4&gt;GBPP为数据高效、几何感知的基础放置提供了实用方法：先使用廉价启发式方法覆盖，然后通过有针对性的模拟进行校准。&lt;h4&gt;翻译&lt;/h4&gt;GBPP是一种基于快速学习的评分器，它从单个RGB-D快照中选择机器人抓取的基础姿态。该方法使用两阶段课程学习：(1)简单的距离-可见性规则低成本自动标记大型数据集；(2)较小的高保真模拟试验集优化模型以匹配真实抓取结果。带有MLP的PointNet++风格点云编码器对密集网格候选姿态进行评分，实现无需完整任务和运动优化的快速在线选择。在模拟和真实移动机械臂上，GBPP优于仅依赖接近度和几何的基线方法，选择更安全、更可达的姿态，并在错误情况下优雅降级。这些结果为数据高效、几何感知的基础放置提供了实用方法：使用廉价启发式方法覆盖，然后通过有针对性的模拟进行校准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GBPP is a fast learning based scorer that selects a robot base pose forgrasping from a single RGB-D snapshot. The method uses a two stage curriculum:(1) a simple distance-visibility rule auto-labels a large dataset at low cost;and (2) a smaller set of high fidelity simulation trials refines the model tomatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLPscores dense grids of candidate poses, enabling rapid online selection withoutfull task-and-motion optimization. In simulation and on a real mobilemanipulator, GBPP outperforms proximity and geometry only baselines, choosingsafer and more reachable stances and degrading gracefully when wrong. Theresults offer a practical recipe for data efficient, geometry aware baseplacement: use inexpensive heuristics for coverage, then calibrate withtargeted simulation.</description>
      <author>example@mail.com (Jizhuo Chen, Diwen Liu, Jiaming Wang, Harold Soh)</author>
      <guid isPermaLink="false">2509.11594v2</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>3D Aware Region Prompted Vision Language Model</title>
      <link>http://arxiv.org/abs/2509.13317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Website: https://www.anjiecheng.me/sr3d&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SR-3D是一种视觉语言模型，通过共享视觉令牌空间连接2D图像和3D数据，支持灵活的区域标注，无需繁琐的多帧标注。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉模型在处理2D和3D数据时存在局限性，特别是在跨帧空间推理方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够统一2D和3D表示空间的模型，提高场景理解的准确性。&lt;h4&gt;方法&lt;/h4&gt;通过将3D位置嵌入增强2D视觉特征，使3D模型能够利用2D先验知识进行跨帧空间推理。&lt;h4&gt;主要发现&lt;/h4&gt;SR-3D在2D视觉语言和3D空间基准测试中达到了最先进的性能，即使在野外视频中也表现出色。&lt;h4&gt;结论&lt;/h4&gt;SR-3D有效地统一了2D和3D表示空间，提高了场景理解的准确性和适用性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了空间区域3D(SR-3D)感知视觉语言模型，通过共享视觉令牌空间连接单视图2D图像和多视图3D数据。SR-3D支持灵活的区域提示，允许用户在任何帧上使用边界框、分割掩码标注区域，或直接在3D中标注，无需进行繁琐的多帧标注。我们通过将3D位置嵌入增强2D视觉特征来实现这一点，使3D模型能够利用强大的2D先验知识进行更准确的跨帧空间推理，即使感兴趣的对象不出现在同一视图中。在通用2D视觉语言和专业3D空间基准上的大量实验表明，SR-3D达到了最先进的性能，证明了其在统一2D和3D表示空间以实现场景理解方面的有效性。此外，我们观察到在没有3D输入或真实3D标注的野外视频中，SR-3D也能准确推断空间关系和度量测量，显示出其适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉语言模型在3D空间推理方面的局限性。现有2D模型缺乏理解复杂3D结构的能力，而3D模型又难以利用2D模型的强大先验知识。这个问题很重要，因为准确的空间推理对机器人导航、自动驾驶、增强现实等应用至关重要，能让AI系统更好地理解和物理世界互动。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考过程是：首先认识到2D视觉语言模型有强大先验但缺乏3D能力；然后发现多视图图像可作为3D表示与2D模型对齐；接着意识到区域提示在单视图有效但扩展到多视图有挑战；最后设计统一架构连接2D和3D表示。他们借鉴了DepthAnythingV2进行深度估计、动态瓦片机制处理高分辨率图像，以及RegionGPT等工作的区域提示概念，并整合了点云估计器处理视频输入。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建统一的3D感知视觉语言模型，通过共享视觉标记空间连接单视图2D图像和多视图3D数据，并在2D特征中加入3D位置嵌入，使模型能利用2D先验进行跨帧空间推理。实现流程包括：1)单视图表示：预训练基础模型，估计深度，计算3D位置，编码并融合位置嵌入；2)多视图表示：采样视频帧，对齐并规范化点图；3)动态瓦片区域提取：处理高分辨率图像和区域特征；4)训练：先单视图预训练，再多视图微调；5)推理：支持灵活的区域标注方式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一架构实现单视图和多视图任务的3D感知；2)动态瓦片区域提取器处理高分辨率图像；3)统一的嵌入空间使2D训练的区域能推广到多视图；4)灵活的区域标注支持；5)规范化的3D位置空间。相比之前工作，SR-3D不使用单独路径处理不同视图数据，而是直接在基础模型中整合位置嵌入，支持更灵活的区域标注，且在单视图训练后就能在多视图场景中表现出强大的零样本空间推理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SR-3D通过统一的3D感知视觉语言模型架构，实现了单视图2D图像和多视图3D数据的有效连接，支持灵活的区域提示，并在各种空间推理任务上达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Spatial Region 3D (SR-3D) aware vision-language model thatconnects single-view 2D images and multi-view 3D data through a shared visualtoken space. SR-3D supports flexible region prompting, allowing users toannotate regions with bounding boxes, segmentation masks on any frame, ordirectly in 3D, without the need for exhaustive multi-frame labeling. Weachieve this by enriching 2D visual features with 3D positional embeddings,which allows the 3D model to draw upon strong 2D priors for more accuratespatial reasoning across frames, even when objects of interest do not co-occurwithin the same view. Extensive experiments on both general 2D vision languageand specialized 3D spatial benchmarks demonstrate that SR-3D achievesstate-of-the-art performance, underscoring its effectiveness for unifying 2Dand 3D representation space on scene understanding. Moreover, we observeapplicability to in-the-wild videos without sensory 3D inputs or ground-truth3D annotations, where SR-3D accurately infers spatial relationships and metricmeasurements.</description>
      <author>example@mail.com (An-Chieh Cheng, Yang Fu, Yukang Chen, Zhijian Liu, Xiaolong Li, Subhashree Radhakrishnan, Song Han, Yao Lu, Jan Kautz, Pavlo Molchanov, Hongxu Yin, Xiaolong Wang, Sifei Liu)</author>
      <guid isPermaLink="false">2509.13317v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization</title>
      <link>http://arxiv.org/abs/2509.12893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MLLM-Engaged Joint Optimization (MEJO)的框架，用于解决外科手术三元组识别中的跨任务和任务内优化冲突问题。该框架通过Shared-Specific-Disentangled学习方案和协调梯度学习策略，有效处理了长尾数据分布和类别不平衡带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;外科手术三元组识别涉及识别手术器械、动词、目标及其组合，是一个复杂的外科场景理解挑战，面临长尾数据分布问题。&lt;h4&gt;目的&lt;/h4&gt;克服外科手术三元组识别中的两个关键挑战：跨任务优化冲突（由任务通用和任务特定表示纠缠引起）和任务内优化冲突（由类别不平衡的训练数据引起）。&lt;h4&gt;方法&lt;/h4&gt;提出MEJO框架，包括：1) 对于跨任务优化，引入Shared-Specific-Disentangled学习方案，构建多模态大语言模型驱动的概率提示池；2) 对于任务内优化，开发协调梯度学习策略，解析和重新平衡来自头部和尾部类别的正负梯度。&lt;h4&gt;主要发现&lt;/h4&gt;在CholecT45和CholecT50数据集上的广泛实验表明，所提出的MEJO框架在处理外科手术三元组识别的优化冲突方面具有优越性，能有效解决跨任务和任务内优化冲突问题。&lt;h4&gt;结论&lt;/h4&gt;MEJO框架通过结合多模态大语言模型和精心设计的优化策略，能够有效解决外科手术三元组识别中的优化冲突问题，提高了识别性能。&lt;h4&gt;翻译&lt;/h4&gt;外科手术三元组识别涉及识别器械、动词、目标及其组合，是一个复杂的外科场景理解挑战，受长尾数据分布困扰。受益于跨任务协作促进的主流多任务学习范式在识别三元组方面显示出有前景的性能，但两个关键挑战仍然存在：1) 由任务通用和任务特定表示纠缠导致的跨任务优化冲突；2) 由于类别不平衡的训练数据导致的任务内优化冲突。为了克服这些困难，我们提出了MLLM-Engaged Joint Optimization (MEJO)框架，它为外科手术三元组识别赋能了跨任务和任务内优化。对于跨任务优化，我们引入了Shared-Specific-Disentangled学习方案，将表示分解为任务共享和任务特定组件。为了增强任务共享表示，我们构建了一个多模态大语言模型驱动的概率提示池，以专家级语义线索动态增强视觉特征。此外，通过覆盖时空维度的不同任务提示，全面建模了任务特定线索，有效减轻了跨任务模糊性。为了解决任务内优化冲突，我们开发了协调梯度学习策略，它解析和重新平衡来自头部和尾部类别的正负梯度，以实现更协调的学习行为。在CholecT45和CholecT50数据集上的广泛实验证明了我们提出框架的优越性，验证了其在处理优化冲突方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical triplet recognition, which involves identifying instrument, verb,target, and their combinations, is a complex surgical scene understandingchallenge plagued by long-tailed data distribution. The mainstream multi-tasklearning paradigm benefiting from cross-task collaborative promotion has shownpromising performance in identifying triples, but two key challenges remain: 1)inter-task optimization conflicts caused by entangling task-generic andtask-specific representations; 2) intra-task optimization conflicts due toclass-imbalanced training data. To overcome these difficulties, we propose theMLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- andintra-task optimization for surgical triplet recognition. For inter-taskoptimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learningscheme that decomposes representations into task-shared and task-specificcomponents. To enhance task-shared representations, we construct a MultimodalLarge Language Model (MLLM) powered probabilistic prompt pool to dynamicallyaugment visual features with expert-level semantic cues. Additionally,comprehensive task-specific cues are modeled via distinct task prompts coveringthe temporal-spatial dimensions, effectively mitigating inter-task ambiguities.To tackle intra-task optimization conflicts, we develop a Coordinated GradientLearning (CGL) strategy, which dissects and rebalances the positive-negativegradients originating from head and tail classes for more coordinated learningbehaviors. Extensive experiments on the CholecT45 and CholecT50 datasetsdemonstrate the superiority of our proposed framework, validating itseffectiveness in handling optimization conflicts.</description>
      <author>example@mail.com (Yiyi Zhang, Yuchen Yuan, Ying Zheng, Jialun Pei, Jinpeng Li, Zheng Li, Pheng-Ann Heng)</author>
      <guid isPermaLink="false">2509.12893v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer</title>
      <link>http://arxiv.org/abs/2509.12718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ongoing Work, 29 pages, 3 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了两个动态空间基准测试，评估模型在动态环境下的空间理解和自适应规划能力，揭示了主流模型在动态空间推理和长期记忆方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有空间推理基准测试主要关注静态或全局可观察环境，无法捕捉部分可观察性和动态变化下的长期推理和内存利用挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍两个动态空间基准测试，系统评估模型在局部感知、环境反馈和全局目标紧密耦合情况下的空间理解和自适应规划能力。&lt;h4&gt;方法&lt;/h4&gt;提出局部可观察迷宫导航和match-2消除游戏两个基准测试，每个动作触发环境结构变化，需要持续更新认知和策略。同时提出基于主观体验的跨任务经验转移和验证的记忆机制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这些基准测试揭示了主流模型在动态空间推理和长期记忆方面的关键局限性。&lt;h4&gt;结论&lt;/h4&gt;提供了一个全面的平台，用于未来的方法论改进。&lt;h4&gt;翻译&lt;/h4&gt;大多数现有的空间推理基准测试关注静态或全局可观察环境，无法捕捉部分可观察性和动态变化下的长期推理和内存利用挑战。我们引入了两个动态空间基准测试——局部可观察迷宫导航和match-2消除游戏，系统评估模型在局部感知、环境反馈和全局目标紧密耦合情况下的空间理解和自适应规划能力。每个动作都会触发环境结构变化，需要持续更新认知和策略。我们进一步提出了基于主观体验的跨任务经验转移和验证的记忆机制。实验表明，我们的基准测试揭示了主流模型在动态空间推理和长期记忆方面的关键局限性，为未来方法论改进提供了全面平台。我们的代码和数据可在https://anonymous.4open.science/r/EvoEmpirBench-143C/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing spatial reasoning benchmarks focus on static or globallyobservable environments, failing to capture the challenges of long-horizonreasoning and memory utilization under partial observability and dynamicchanges. We introduce two dynamic spatial benchmarks, locally observable mazenavigation and match-2 elimination that systematically evaluate models'abilities in spatial understanding and adaptive planning when local perception,environment feedback, and global objectives are tightly coupled. Each actiontriggers structural changes in the environment, requiring continuous update ofcognition and strategy. We further propose a subjective experience-based memorymechanism for cross-task experience transfer and validation. Experiments showthat our benchmarks reveal key limitations of mainstream models in dynamicspatial reasoning and long-term memory, providing a comprehensive platform forfuture methodological advances. Our code and data are available athttps://anonymous.4open.science/r/EvoEmpirBench-143C/.</description>
      <author>example@mail.com (Pukun Zhao, Longxiang Wang, Miaowei Wang, Chen Chen, Fanqing Zhou, Haojian Huang)</author>
      <guid isPermaLink="false">2509.12718v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>RailSafeNet: Visual Scene Understanding for Tram Safety</title>
      <link>http://arxiv.org/abs/2509.12125v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, EPIA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为RailSafeNet的实时框架，利用数字图像处理、深度学习和人工智能技术提高有轨电车与人类交互的安全性，通过语义分割、目标检测和距离评估来识别轨道入侵风险。&lt;h4&gt;背景&lt;/h4&gt;有轨电车经常在人口密集地区运行，与人类（行人、驾驶员、骑自行车的人、宠物等）的交互安全是一个重要挑战，碰撞可能导致从轻微伤害到致命后果的各种事故。&lt;h4&gt;目的&lt;/h4&gt;设计一个利用数字图像处理、深度学习和人工智能的解决方案，提高行人、驾驶员、骑自行车的人、宠物和有轨电车乘客的安全。&lt;h4&gt;方法&lt;/h4&gt;提出RailSafeNet实时框架，融合语义分割、目标检测和基于规则的距离评估器，使用单目视频识别轨道，定位附近物体，并通过将投影距离与标准1435毫米轨距比较来分类风险。&lt;h4&gt;主要发现&lt;/h4&gt;在RailSem19数据集上的实验显示，经过类别过滤的SegFormer B3模型实现了65%的交并比，微调后的YOLOv8在交并比阈值为0.50的情况下达到了75.6%的平均精度均值。&lt;h4&gt;结论&lt;/h4&gt;RailSafeNet提供准确的、标注较少的场景理解，可以在危险情况升级之前警告驾驶员，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;有轨电车与人类交互安全是一个重要挑战，因为有轨电车经常在人口密集地区运行，碰撞可能导致轻微伤害甚至致命后果。本文从设计解决方案的角度解决这一问题，利用数字图像处理、深度学习和人工智能来提高行人、驾驶员、骑自行车的人、宠物和有轨电车乘客的安全。我们提出了RailSafeNet，一个实时框架，融合了语义分割、目标检测和基于规则的距离评估器，以突出显示轨道入侵。仅使用单目视频，系统识别轨道，定位附近物体，并通过将投影距离与标准1435毫米轨距进行比较来分类风险。在多样化的RailSem19数据集上的实验表明，经过类别过滤的SegFormer B3模型实现了65%的交并比，而微调后的YOLOv8在交并比阈值为0.50的情况下达到了75.6%的平均精度均值。因此，RailSafeNet提供了准确的、标注较少的场景理解，可以在危险情况升级之前警告驾驶员。代码可在https://github.com/oValach/RailSafeNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tram-human interaction safety is an important challenge, given that tramsfrequently operate in densely populated areas, where collisions can range fromminor injuries to fatal outcomes. This paper addresses the issue from theperspective of designing a solution leveraging digital image processing, deeplearning, and artificial intelligence to improve the safety of pedestrians,drivers, cyclists, pets, and tram passengers. We present RailSafeNet, areal-time framework that fuses semantic segmentation, object detection and arule-based Distance Assessor to highlight track intrusions. Using onlymonocular video, the system identifies rails, localises nearby objects andclassifies their risk by comparing projected distances with the standard 1435mmrail gauge. Experiments on the diverse RailSem19 dataset show that aclass-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculatedat an intersection over union (IoU) threshold of 0.50. RailSafeNet thereforedelivers accurate, annotation-light scene understanding that can warn driversbefore dangerous situations escalate. Code available athttps://github.com/oValach/RailSafeNet.</description>
      <author>example@mail.com (Ondřej Valach, Ivan Gruber)</author>
      <guid isPermaLink="false">2509.12125v2</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Geometric Priors for Unaligned Scene Change Detection</title>
      <link>http://arxiv.org/abs/2509.11292v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入几何先验解决非对齐场景变化检测的核心挑战，提出无需训练的框架，结合视觉基础模型表征，实现视角不对齐情况下的可靠变化检测。&lt;h4&gt;背景&lt;/h4&gt;非对齐场景变化检测旨在检测不同时间拍摄且无视角对齐的图像对间的场景变化。当前方法仅依赖2D视觉线索建立对应关系，但大视角变化会导致基于外观的匹配失败。小规模数据集的2D变化掩码监督限制了多视图知识学习，缺乏显式几何推理是关键但被忽视的局限性。&lt;h4&gt;目的&lt;/h4&gt;引入几何先验解决非对齐SCD的核心挑战，实现可靠的视觉重叠识别、稳健的对应关系建立和显式的遮挡检测。&lt;h4&gt;方法&lt;/h4&gt;提出一种无需训练的框架，将几何先验与视觉基础模型的强大表征相结合，使模型能够在视角不对齐的情况下实现可靠的变化检测。&lt;h4&gt;主要发现&lt;/h4&gt;在PSCD、ChangeSim和PASLCD数据集上的广泛评估表明，该方法实现了优越且稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入几何先验，解决了非对齐SCD中的核心挑战，提高了变化检测的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;非对齐场景变化检测旨在检测在不同时间拍摄且没有假设视角对齐的图像对之间的场景变化。为处理视角变化，当前方法仅依赖2D视觉线索建立跨图像对应关系以辅助变化检测。然而，大视角变化会改变视觉观测，导致基于外观的匹配漂移或失败。此外，仅限于小规模SCD数据集的2D变化掩码监督限制了可泛化的多视图知识学习，使得难以可靠识别视觉重叠和处理遮挡。这种缺乏显式几何推理代表了关键但被忽视的局限性。在这项工作中，我们首次引入几何先验来解决非对齐SCD的核心挑战，实现可靠的视觉重叠识别、稳健的对应关系建立和显式的遮挡检测。基于这些先验，我们提出了一种无需训练的框架，将其与视觉基础模型的强大表征相结合，使模型能够在视角不对齐的情况下实现可靠的变化检测。通过在PSCD、ChangeSim和PASLCD数据集上的广泛评估，我们证明了我们的方法实现了优越且稳健的性能。我们的代码将在https://github.com/ZilingLiu/GeoSCD上发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决不对齐场景变化检测问题，即在相机视角不同的情况下检测场景变化。这个问题在现实中非常重要，因为自动驾驶、无人机和移动机器人等应用中经常遇到不同时间拍摄的图像存在视角差异的情况。传统方法假设图像视角对齐，限制了它们在真实世界场景中的实用性。缺乏显式几何推理导致这些方法在大视角变化下表现不佳，难以可靠识别视觉重叠和处理遮挡问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到当前方法仅依赖2D视觉线索建立图像对应关系，在大视角变化下容易失败。他们发现2D监督限制了多视图知识的泛化学习，难以处理遮挡问题。受人类能轻松从不同视角建立关联和感知遮挡的启发，作者认为3D信息可以解决这些挑战。他们借鉴了几何基础模型(GFMs)可以从多视角图像中恢复3D几何的能力，并利用视觉基础模型SAM的强大表示能力，设计了一个两阶段框架：先利用GFM建立几何理解，再指导变化掩码预测。这种方法还借鉴了零样本思想来实现跨数据集泛化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入几何先验知识来解决不对齐场景变化检测的挑战，包括识别视觉重叠、建立稳健对应关系和检测遮挡。整体流程分为两个主要模块：1)几何先验生成模块：使用GFM重建深度图和相机参数，建立像素级对应关系，识别视觉重叠区域，并检测遮挡区域；2)几何引导变化掩码预测模块：将几何线索与SAM模型集成，在重叠区域内通过特征相似性生成初始变化提案，用遮挡掩码精炼提案，再与SAM的分割掩码匹配融合产生最终结果。此外，还包含光照变化处理步骤，使用Retinex或Color Transfer技术减少光照差异对GFM重建的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次引入几何先验知识来识别视觉重叠、建立稳健对应关系和检测遮挡；2)提出无需训练的框架，将几何先验与视觉基础模型集成，减少对标注数据的依赖；3)在各种视角变化和场景类型上实现领先且稳健的性能。相比之前工作，本文超越了仅依赖2D视觉线索的传统方法，解决了遮挡问题，无需大规模训练数据，具有更强的跨数据集泛化能力，并能处理更复杂的场景和更广泛的视角变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次将几何先验知识引入不对齐场景变化检测，通过结合几何基础模型与视觉基础模型，实现了无需训练、视角变化鲁棒且跨数据集泛化的场景变化检测方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unaligned Scene Change Detection aims to detect scene changes between imagepairs captured at different times without assuming viewpoint alignment. Tohandle viewpoint variations, current methods rely solely on 2D visual cues toestablish cross-image correspondence to assist change detection. However, largeviewpoint changes can alter visual observations, causing appearance-basedmatching to drift or fail. Additionally, supervision limited to 2D change masksfrom small-scale SCD datasets restricts the learning of generalizablemulti-view knowledge, making it difficult to reliably identify visual overlapsand handle occlusions. This lack of explicit geometric reasoning represents acritical yet overlooked limitation. In this work, we introduce geometric priorsfor the first time to address the core challenges of unaligned SCD, forreliable identification of visual overlaps, robust correspondenceestablishment, and explicit occlusion detection. Building on these priors, wepropose a training-free framework that integrates them with the powerfulrepresentations of a visual foundation model to enable reliable changedetection under viewpoint misalignment. Through extensive evaluation on thePSCD, ChangeSim, and PASLCD datasets, we demonstrate that our approach achievessuperior and robust performance. Our code will be released athttps://github.com/ZilingLiu/GeoSCD.</description>
      <author>example@mail.com (Ziling Liu, Ziwei Chen, Mingqi Gao, Jinyu Yang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.11292v2</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.13229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CMTSSL的课程多任务自监督学习框架，专为高光谱成像分析设计的轻量级架构，有效结合了掩模图像建模与空间光谱拼图求解。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像能捕获每个像素的数百个连续波段的光谱特征，对土地覆盖分类、变化检测和环境监测等遥感应用至关重要。但由于数据高维度和卫星系统传输速率慢，需要高效模型支持星上处理。&lt;h4&gt;目的&lt;/h4&gt;开发轻量级模型支持星上处理，最小化冗余或低价值数据(如云覆盖区域)的传输，同时保持HSI分析的准确性。&lt;h4&gt;方法&lt;/h4&gt;CMTSSL框架结合掩模图像建模与解耦的空间和光谱拼图求解，采用课程学习策略逐步增加数据复杂度，使编码器能同时捕获光谱连续性、空间结构和全局语义特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上验证，CMTSSL在下游分割任务中表现一致提升，使用的架构比某些最先进模型轻16000多倍。&lt;h4&gt;结论&lt;/h4&gt;CMTSSL在轻量级架构上的可推广表示学习具有潜力，适用于真实世界的HSI应用，代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)捕获每个像素的数百个连续波段的详细光谱特征，对于土地覆盖分类、变化检测和环境监测等遥感应用不可或缺。由于HSI数据的高维性和卫星系统数据传输速率慢，需要紧凑高效的模型支持星上处理并最小化冗余或低价值数据(如云覆盖区域)的传输。为此，我们引入了一种专为HSI分析轻量架构设计的课程多任务自监督学习(CMTSSL)框架。CMTSSL将掩模图像建模与解耦的空间和光谱拼图求解相结合，通过课程学习策略在自监督过程中逐步增加数据复杂度，使编码器能够同时捕获细粒度光谱连续性、空间结构和全局语义特征。与先前的双任务SSL方法不同，CMTSSL在统一且计算高效的设计中同时解决空间和光谱推理，特别适合用于轻量级模型的星上卫星部署。我们在四个公共基准数据集上验证了我们的方法，在使用比某些最先进模型轻16000多倍的架构时，在下游分割任务中表现出一致的改进。这些结果突显了CMTSSL在轻量级架构上可推广表示学习的潜力，适用于真实世界的HSI应用。我们的代码已在https://github.com/hugocarlesso/CMTSSL公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) captures detailed spectral signatures acrosshundreds of contiguous bands per pixel, being indispensable for remote sensingapplications such as land-cover classification, change detection, andenvironmental monitoring. Due to the high dimensionality of HSI data and theslow rate of data transfer in satellite-based systems, compact and efficientmodels are required to support onboard processing and minimize the transmissionof redundant or low-value data, e.g. cloud-covered areas. To this end, weintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)framework designed for lightweight architectures for HSI analysis. CMTSSLintegrates masked image modeling with decoupled spatial and spectral jigsawpuzzle solving, guided by a curriculum learning strategy that progressivelyincreases data complexity during self-supervision. This enables the encoder tojointly capture fine-grained spectral continuity, spatial structure, and globalsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneouslyaddresses spatial and spectral reasoning within a unified and computationallyefficient design, being particularly suitable for training lightweight modelsfor onboard satellite deployment. We validate our approach on four publicbenchmark datasets, demonstrating consistent gains in downstream segmentationtasks, using architectures that are over 16,000x lighter than somestate-of-the-art models. These results highlight the potential of CMTSSL ingeneralizable representation learning with lightweight architectures forreal-world HSI applications. Our code is publicly available athttps://github.com/hugocarlesso/CMTSSL.</description>
      <author>example@mail.com (Hugo Carlesso, Josiane Mothe, Radu Tudor Ionescu)</author>
      <guid isPermaLink="false">2509.13229v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Representation Learning for Robust Sim-to-Real Transfer of Adaptive Humanoid Locomotion</title>
      <link>http://arxiv.org/abs/2509.12858v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新范式，通过对比学习框架使纯本体感觉策略获得感知的前瞻性能力，解决了人形机器人运动中反应性控制和感知驱动系统之间的权衡问题，实现了在不增加部署成本的情况下主动适应复杂地形的能力。&lt;h4&gt;背景&lt;/h4&gt;强化学习在人形机器人运动方面取得了显著进展，但在实际部署中存在一个基本困境：策略必须在反应性本体感觉控制的鲁棒性和复杂的、脆弱的感知驱动系统的主动性之间做出选择。&lt;h4&gt;目的&lt;/h4&gt;解决这一困境，引入一种范式，使纯粹的本体感觉策略具有主动性，获得感知的前瞻性能力，同时避免其部署时的成本。&lt;h4&gt;方法&lt;/h4&gt;核心贡献是一个对比学习框架，它强制执行者的潜在状态从仿真中编码特权的环境信息。这种'蒸馏意识'使自适应节拍时钟能够根据对地形的推断理解主动调整节奏。&lt;h4&gt;主要发现&lt;/h4&gt;这种协同作用解决了刚性时钟步态和不稳定的无时钟策略之间的经典权衡。通过零样本仿真到真实世界的转移，验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法使全尺寸人形机器人在具有挑战性的地形上（包括30厘米高的台阶和26.5度的斜坡）实现了高度鲁棒的运动。&lt;h4&gt;翻译&lt;/h4&gt;强化学习在人形机器人运动方面已经取得了显著进展，但实际部署中仍然存在一个基本困境：策略必须在反应性本体感觉控制的鲁棒性和复杂的、脆弱的感知驱动系统的主动性之间做出选择。本文通过引入一种范式解决了这一困境，该范式赋予纯本体感觉策略以主动能力，实现了感知的前瞻性，同时避免了其部署时的成本。我们的核心贡献是一个对比学习框架，它强制执行者的潜在状态从仿真中编码特权的环境信息。重要的是，这种'蒸馏意识'使自适应节拍时钟能够基于对地形的推断理解主动调整节奏。这种协同作用解决了刚性时钟步态和不稳定的无时钟策略之间的经典权衡。我们通过零样本仿真到真实世界的转移验证了我们的方法，证明全尺寸人形机器人在具有挑战性的地形上（包括30厘米高的台阶和26.5度的斜坡）实现了高度鲁棒的运动。网站：https://lu-yidan.github.io/cra-loco。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning has produced remarkable advances in humanoidlocomotion, yet a fundamental dilemma persists for real-world deployment:policies must choose between the robustness of reactive proprioceptive controlor the proactivity of complex, fragile perception-driven systems. This paperresolves this dilemma by introducing a paradigm that imbues a purelyproprioceptive policy with proactive capabilities, achieving the foresight ofperception without its deployment-time costs. Our core contribution is acontrastive learning framework that compels the actor's latent state to encodeprivileged environmental information from simulation. Crucially, this``distilled awareness" empowers an adaptive gait clock, allowing the policy toproactively adjust its rhythm based on an inferred understanding of theterrain. This synergy resolves the classic trade-off between rigid, clockedgaits and unstable clock-free policies. We validate our approach with zero-shotsim-to-real transfer to a full-sized humanoid, demonstrating highly robustlocomotion over challenging terrains, including 30 cm high steps and 26.5{\deg}slopes, proving the effectiveness of our method. Website:https://lu-yidan.github.io/cra-loco.</description>
      <author>example@mail.com (Yidan Lu, Rurui Yang, Qiran Kou, Mengting Chen, Tao Fan, Peter Cui, Yinzhao Dong, Peng Lu)</author>
      <guid isPermaLink="false">2509.12858v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Modeling the Multivariate Relationship with Contextualized Representations for Effective Human-Object Interaction Detection</title>
      <link>http://arxiv.org/abs/2509.12784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种上下文化表征学习网络，通过结合功能引导推理和上下文提示，改进了人机交互(HOI)检测方法，能够更好地捕捉复杂交互关系。&lt;h4&gt;背景&lt;/h4&gt;人机交互(HOI)检测旨在同时定位人-物体对并识别它们的交互。最近的两阶段方法虽然取得了显著进展，但由于上下文建模不完整，仍然面临挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍一种上下文表征学习网络，结合功能引导推理和上下文提示，以更好地捕捉复杂交互。&lt;h4&gt;方法&lt;/h4&gt;通过三元结构&lt;人，工具，物体&gt;明确建模辅助对象的功能角色；将可学习提示与实例类别丰富化，并使用注意力机制与上下文视觉特征集成；在全局和区域级别将语言与图像内容对齐。&lt;h4&gt;主要发现&lt;/h4&gt;提出的上下文化表征为模型提供了丰富的关系线索，使其能够对复杂、上下文依赖的交互进行更可靠的推理。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在HICO-Det和V-COCO数据集的大多数场景中表现出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;人机交互(HOI)检测旨在同时定位人-物体对并识别它们的交互。虽然最近的两阶段方法已经取得了显著进展，但由于上下文建模不完整，它们仍然面临挑战。在这项工作中，我们引入了一种上下文化表征学习网络，结合功能引导推理和上下文提示以及视觉线索，以更好地捕捉复杂交互。我们通过将常规HOI检测框架扩展到简单的物体对之外，包括涉及工具等辅助实体的多元关系，来增强它。具体来说，我们通过三元结构&lt;人，工具，物体&gt;明确建模这些辅助对象的功能角色(功能)。这使得我们的模型能够识别依赖于工具的交互，如'填充'。此外，可学习提示被实例类别丰富化，随后使用注意力机制与上下文视觉特征集成。这个过程在全局和区域级别上将语言与图像内容对齐。这些上下文化表征为模型提供了丰富的关系线索，以便对复杂、上下文依赖的交互进行更可靠的推理。我们提出的方法在HICO-Det和V-COCO数据集的大多数场景中表现出优越的性能。代码将在接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human-Object Interaction (HOI) detection aims to simultaneously localizehuman-object pairs and recognize their interactions. While recent two-stageapproaches have made significant progress, they still face challenges due toincomplete context modeling. In this work, we introduce a ContextualizedRepresentation Learning Network that integrates both affordance-guidedreasoning and contextual prompts with visual cues to better capture complexinteractions. We enhance the conventional HOI detection framework by expandingit beyond simple human-object pairs to include multivariate relationshipsinvolving auxiliary entities like tools. Specifically, we explicitly model thefunctional role (affordance) of these auxiliary objects through tripletstructures &lt;human, tool, object&gt;. This enables our model to identifytool-dependent interactions such as 'filling'. Furthermore, the learnableprompt is enriched with instance categories and subsequently integrated withcontextual visual features using an attention mechanism. This process alignslanguage with image content at both global and regional levels. Thesecontextualized representations equip the model with enriched relational cuesfor more reliable reasoning over complex, context-dependent interactions. Ourproposed method demonstrates superior performance on both the HICO-Det andV-COCO datasets in most scenarios. Codes will be released upon acceptance.</description>
      <author>example@mail.com (Zhehao Li, Yucheng Qian, Chong Wang, Yinghao Lu, Zhihao Yang, Jiafei Wu)</author>
      <guid isPermaLink="false">2509.12784v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>NORA: A Nephrology-Oriented Representation Learning Approach Towards Chronic Kidney Disease Classification</title>
      <link>http://arxiv.org/abs/2509.12704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures, accepted to the International Conference on  Machine Learning and Applications (ICMLA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为NORA的肾脏病导向表示学习方法，利用常规收集的非肾脏临床变量进行慢性肾脏病分类，在缺乏专门肾脏生物标志物的环境中表现出良好的性能。&lt;h4&gt;背景&lt;/h4&gt;慢性肾脏病影响全球数百万人，但其早期检测仍具有挑战性，特别是在门诊环境中，基于实验室的肾脏生物标志物通常不可用。&lt;h4&gt;目的&lt;/h4&gt;研究常规收集的非肾脏临床变量（包括社会人口学因素、合并症和尿检结果）在慢性肾脏病分类中的预测潜力。&lt;h4&gt;方法&lt;/h4&gt;引入Nephrology-Oriented Representation leArning (NORA)方法，结合监督对比学习和非线性随机森林分类器，从表格型电子健康记录数据中推导区分性患者表示，用于下游CKD分类。在Riverside肾脏病学医师的基于诊所的EHR数据集上评估该方法，并在UCI CKD数据集上测试其可推广性。&lt;h4&gt;主要发现&lt;/h4&gt;NORA提高了类间可分性和整体分类性能，特别增强了早期阶段CKD的F1分数；在不同患者队列中证明了NORA进行CKD风险分层的有效性。&lt;h4&gt;结论&lt;/h4&gt;NORA方法能够有效利用常规临床数据进行CKD检测和风险分层，特别是在缺乏专门肾脏生物标志物的环境中具有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;慢性肾脏病(CKD)影响全球数百万人，但其早期检测仍然具有挑战性，特别是在门诊环境中，基于实验室的肾脏生物标志物通常不可用。在本研究中，我们调查了常规收集的非肾脏临床变量（包括社会人口学因素、合并症和尿检结果）在CKD分类中的预测潜力。我们引入了肾脏病导向表示学习(NORA)方法，结合监督对比学习和非线性随机森林分类器。NORA首先从表格型电子健康记录数据中推导出区分性患者表示，然后用于下游CKD分类。我们在Riverside肾脏病学医师的基于诊所的EHR数据集上评估了NORA。结果表明，NORA提高了类间可分性和整体分类性能，特别增强了早期阶段CKD的F1分数。此外，我们在UCI CKD数据集上评估了NORA的可推广性，证明了其在不同患者队列中进行CKD风险分层的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chronic Kidney Disease (CKD) affects millions of people worldwide, yet itsearly detection remains challenging, especially in outpatient settings wherelaboratory-based renal biomarkers are often unavailable. In this work, weinvestigate the predictive potential of routinely collected non-renal clinicalvariables for CKD classification, including sociodemographic factors, comorbidconditions, and urinalysis findings. We introduce the Nephrology-OrientedRepresentation leArning (NORA) approach, which combines supervised contrastivelearning with a nonlinear Random Forest classifier. NORA first derivesdiscriminative patient representations from tabular EHR data, which are thenused for downstream CKD classification. We evaluated NORA on a clinic-based EHRdataset from Riverside Nephrology Physicians. Our results demonstrated thatNORA improves class separability and overall classification performance,particularly enhancing the F1-score for early-stage CKD. Additionally, weassessed the generalizability of NORA on the UCI CKD dataset, demonstrating itseffectiveness for CKD risk stratification across distinct patient cohorts.</description>
      <author>example@mail.com (Mohammad Abdul Hafeez Khan, Twisha Bhattacharyya, Omar Khan, Noorah Khan, Alina Aziz Fatima Khan, Mohammed Qutub Khan, Sujoy Ghosh Hajra)</author>
      <guid isPermaLink="false">2509.12704v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields</title>
      <link>http://arxiv.org/abs/2509.12358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为MEAGraph的无监督学习方法，用于分析原子数据集，有效去除采样偏差，优化数据集。&lt;h4&gt;背景&lt;/h4&gt;在计算化学和材料科学中，构建化学多样性的数据集并避免采样偏差对于训练高效且可推广的力场至关重要。然而，许多常见的数据集生成技术容易过度采样势能表面的某些区域，这些区域难以识别和区分，或者与人类直觉不符，使得系统性地去除数据集中的偏差具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效识别和去除数据集中采样偏差的方法，同时避免信息损失，并能正确识别势能表面的不同区域。&lt;h4&gt;方法&lt;/h4&gt;作者提出了MEAGraph模型，这是一种无监督的原子数据分析方法。该方法结合了多种线性核变换和基于注意力的消息传递，能够捕获几何敏感性，并在不依赖标签或大量训练的情况下实现有效的数据集剪枝。&lt;h4&gt;主要发现&lt;/h4&gt;在铌、钽和铁数据集上的应用表明，MEAGraph能够高效地分组相似的原子环境，使得可以使用基本的剪枝技术来去除采样偏差。&lt;h4&gt;结论&lt;/h4&gt;这种方法为表示学习和聚类提供了有效的方法，可用于数据分析、异常检测和数据集优化。&lt;h4&gt;翻译&lt;/h4&gt;在构建化学多样性数据集的同时避免采样偏差对于训练高效且可推广的力场至关重要。然而，在计算化学和材料科学中，许多常见的数据集生成技术容易过度采样势能表面的某些区域。此外，这些区域可能难以相互识别和分离，或者与人类直觉不太吻合，这使得系统性地去除数据集中的偏差具有挑战性。虽然传统的聚类和修剪方法对此有用，但由于原子描述符的高维度相关困难，它们往往会导致信息损失或无法正确识别势能表面的不同区域。在本工作中，我们引入了多核边缘注意力的图自编码器模型，这是一种用于分析原子数据集的无监督方法。MEAGraph将多种线性核变换与基于注意力的消息传递相结合，以捕获几何敏感性并实现有效的数据集修剪，而不依赖于标签或大量训练。在铌、钽和铁数据集上的应用表明，MEAGraph能够高效地分组相似的原子环境，从而可以使用基本的修剪技术来去除采样偏差。这种方法为表示学习和聚类提供了有效的方法，可用于数据分析、异常检测和数据集优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Constructing a chemically diverse dataset while avoiding sampling bias iscritical to training efficient and generalizable force fields. However, incomputational chemistry and materials science, many common dataset generationtechniques are prone to oversampling regions of the potential energy surface.Furthermore, these regions can be difficult to identify and isolate from eachother or may not align well with human intuition, making it challenging tosystematically remove bias in the dataset. While traditional clustering andpruning (down-sampling) approaches can be useful for this, they can often leadto information loss or a failure to properly identify distinct regions of thepotential energy surface due to difficulties associated with the highdimensionality of atomic descriptors. In this work, we introduce theMulti-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, anunsupervised approach for analyzing atomic datasets. MEAGraph combines multiplelinear kernel transformations with attention-based message passing to capturegeometric sensitivity and enable effective dataset pruning without relying onlabels or extensive training. Demonstrated applications on niobium, tantalum,and iron datasets show that MEAGraph efficiently groups similar atomicenvironments, allowing for the use of basic pruning techniques for removingsampling bias. This approach provides an effective method for representationlearning and clustering that can be used for data analysis, outlier detection,and dataset optimization.</description>
      <author>example@mail.com (Hong Sun, Joshua A. Vita, Amit Samanta, Vincenzo Lordi)</author>
      <guid isPermaLink="false">2509.12358v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery</title>
      <link>http://arxiv.org/abs/2509.11436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The Fourth Workshop on Applications of Medical Artificial  Intelligence, AMAI 2025, Held in Conjunction with MICCAI 2025, Daejeon,  Republic of Korea, September 23, 2025, Proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种通过机器学习主动学习域偏移的方法，通过数据潜在空间的后旋转实现生物和技术因素的解耦，提高了医学影像数据中疾病相关模式的识别能力。&lt;h4&gt;背景&lt;/h4&gt;在医学影像数据中借助机器学习识别新的疾病相关模式可以扩展可识别发现的词汇，支持诊断和预后评估。然而，图像外观的变化不仅源于生物学差异，还与供应商相关的成像技术、扫描或重建参数有关。这些导致的域偏移阻碍了数据表示学习策略和有意义的生物聚类外观的发现。&lt;h4&gt;目的&lt;/h4&gt;解决医学影像数据中的域偏移问题，使机器学习能够更好地识别疾病相关模式，提高诊断和预后评估的准确性。&lt;h4&gt;方法&lt;/h4&gt;引入一种通过数据潜在空间的后旋转来主动学习域偏移的方法，实现生物和技术因素的解耦，从而在不同采集设置下实现代表组织类型的稳定聚类。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界异构临床数据上，学习的解耦表示导致跨不同采集设置的稳定聚类；与纠缠表示相比，聚类一致性提高了+19.01%（ARI）、+16.85%（NMI）和+12.39%（Dice），优于四种最先进的调和方法；当使用聚类对特发性肺纤维化患者进行组织成分量化时，学习的特征增强了Cox生存预测。&lt;h4&gt;结论&lt;/h4&gt;所提出的无标签框架通过解耦生物和技术因素，能够有效处理医学影像数据中的域偏移问题，提高聚类一致性，并促进多中心常规成像数据中的生物标志物发现。&lt;h4&gt;翻译&lt;/h4&gt;借助机器学习在医学影像数据中识别新的疾病相关模式扩展了可识别发现的词汇，支持诊断和预后评估。然而，图像外观的变化不仅源于生物学差异，还与供应商相关的成像技术、扫描或重建参数有关。由此产生的域偏移阻碍了数据表示学习策略和有意义的生物聚类外观的发现。为应对这些挑战，我们引入了一种通过数据潜在空间的后旋转来主动学习域偏移的方法，实现生物和技术因素的解耦。真实世界异构临床数据的结果表明，学习的解耦表示导致跨不同采集设置的稳定聚类。与纠缠表示相比，聚类一致性提高了+19.01%（ARI）、+16.85%（NMI）和+12.39%（Dice），优于四种最先进的调和方法。当使用聚类对特发性肺纤维化患者进行组织成分量化时，学习的特征增强了Cox生存预测。这表明所提出的无标签框架促进了多中心常规成像数据中的生物标志物发现。代码可在GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying new disease-related patterns in medical imaging data with thehelp of machine learning enlarges the vocabulary of recognizable findings. Thissupports diagnostic and prognostic assessment. However, image appearance variesnot only due to biological differences, but also due to imaging technologylinked to vendors, scanning- or re- construction parameters. The resultingdomain shifts impedes data representation learning strategies and the discoveryof biologically meaningful cluster appearances. To address these challenges, weintroduce an approach to actively learn the domain shift via post-hoc rotationof the data latent space, enabling disentanglement of biological and technicalfactors. Results on real-world heterogeneous clinical data showcase that thelearned disentangled representation leads to stable clusters representingtissue-types across different acquisition settings. Cluster consistency isimproved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to theentangled representation, outperforming four state-of-the-art harmonizationmethods. When using the clusters to quantify tissue composition on idiopathicpulmonary fibrosis patients, the learned profiles enhance Cox survivalprediction. This indicates that the proposed label-free framework facilitatesbiomarker discovery in multi-center routine imaging data. Code is available onGitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.</description>
      <author>example@mail.com (Jeanny Pan, Philipp Seeböck, Christoph Fürböck, Svitlana Pochepnia, Jennifer Straub, Lucian Beer, Helmut Prosch, Georg Langs)</author>
      <guid isPermaLink="false">2509.11436v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE</title>
      <link>http://arxiv.org/abs/2509.12255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了GraphSAGE图神经网络框架在银行交易网络中的实际应用，证明了其在处理动态银行数据方面的优势，并展示了其在洗钱检测等实际应用中的价值。&lt;h4&gt;背景&lt;/h4&gt;金融机构日益需要可扩展的工具来分析复杂的交易网络，但传统的图嵌入方法难以处理动态、真实的银行数据。&lt;h4&gt;目的&lt;/h4&gt;展示GraphSAGE（一种归纳图神经网络框架）在银行非二分异构交易网络中的实际应用，并证明其在处理随时间演变的交易数据时的优势。&lt;h4&gt;方法&lt;/h4&gt;构建使用匿名客户和商家交易数据的交易网络，并训练GraphSAGE模型生成节点嵌入，然后将这些嵌入应用于下游分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;对嵌入的探索性工作揭示了与地理和人口统计属性一致的、可解释的聚类；在洗钱检测模型中使用这些嵌入可以改进高风险账户的优先级排序。&lt;h4&gt;结论&lt;/h4&gt;这项研究为金融机构利用图机器学习在交易生态系统中获取可操作的见解提供了蓝图，强调了该框架的归纳能力、可扩展性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;金融机构日益需要可扩展的工具来分析复杂的交易网络，然而传统的图嵌入方法难以处理动态、真实的银行数据。本文展示了GraphSAGE（一种归纳图神经网络框架）在银行非二分异构交易网络中的实际应用。与归纳方法不同，GraphSAGE能够很好地扩展到大型网络，并且可以推广到未见过的节点，这对于处理随时间演变的交易数据的机构至关重要。我们使用匿名客户和商家交易数据构建了一个交易网络，并训练了一个GraphSAGE模型来生成节点嵌入。我们对嵌入的探索性工作揭示了与地理和人口统计属性一致的、可解释的聚类。此外，我们通过将它们应用于洗钱检测模型，说明了它们在下游分类任务中的实用性，使用这些嵌入可以改进高风险账户的优先级排序。除了欺诈检测外，我们的工作强调了该框架对银行规模网络的适应性，强调了其归纳能力、可扩展性和可解释性。本研究为金融机构利用图机器学习在交易生态系统中获取可操作的见解提供了蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-94139-9_17&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial institutions increasingly require scalable tools to analyse complextransactional networks, yet traditional graph embedding methods struggle withdynamic, real-world banking data. This paper demonstrates the practicalapplication of GraphSAGE, an inductive Graph Neural Network framework, tonon-bipartite heterogeneous transaction networks within a banking context.Unlike transductive approaches, GraphSAGE scales well to large networks and cangeneralise to unseen nodes which is critical for institutions working withtemporally evolving transactional data. We construct a transaction networkusing anonymised customer and merchant transactions and train a GraphSAGE modelto generate node embeddings. Our exploratory work on the embeddings revealsinterpretable clusters aligned with geographic and demographic attributes.Additionally, we illustrate their utility in downstream classification tasks byapplying them to a money mule detection model where using these embeddingsimproves the prioritisation of high-risk accounts. Beyond fraud detection, ourwork highlights the adaptability of this framework to banking-scale networks,emphasising its inductive capability, scalability, and interpretability. Thisstudy provides a blueprint for financial organisations to harness graph machinelearning for actionable insights in transactional ecosystems.</description>
      <author>example@mail.com (Mihir Tare, Clemens Rattasits, Yiming Wu, Euan Wielewski)</author>
      <guid isPermaLink="false">2509.12255v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Why and How Auxiliary Tasks Improve JEPA Representations</title>
      <link>http://arxiv.org/abs/2509.12249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了JEPA架构的理论特性，证明了一个'无有害表征崩溃'定理，并通过实验表明联合训练JEPA模型与辅助头能生成更丰富的表征。&lt;h4&gt;背景&lt;/h4&gt;JEPA（Joint-Embedding Predictive Architecture）越来越多地用于视觉表征学习和基于模型的强化学习组件中，但其行为仍然不被充分理解。&lt;h4&gt;目的&lt;/h4&gt;提供一个简单实用的JEPA变体的理论表征，该变体有一个与潜在动力学联合训练的辅助回归头。&lt;h4&gt;方法&lt;/h4&gt;证明了一个'无有害表征崩溃'定理：在确定性MDPs中，如果训练使潜在转换一致性损失和辅助回归损失都趋近于零，那么任何非等价观测（即那些不具有相同转换动力学或辅助标签的观测）必须映射到不同的潜在表征。&lt;h4&gt;主要发现&lt;/h4&gt;辅助任务锚定了表征必须保留的区分。在计数环境中的受控消融实验证实了这一理论，并表明与辅助头联合训练JEPA模型比分别训练它们能生成更丰富的表征。&lt;h4&gt;结论&lt;/h4&gt;我们的工作指出了改进JEPA编码器的一条路径：使用一个辅助函数与它们一起训练，该辅助函数与转换动力学一起编码正确的等价关系。&lt;h4&gt;翻译&lt;/h4&gt;联合嵌入预测架构（JEPA）越来越多地用于视觉表征学习和作为基于模型的强化学习组件，但其行为仍然不被充分理解。我们提供了一个简单实用的JEPA变体的理论表征，该变体有一个与潜在动力学联合训练的辅助回归头。我们证明了一个'无有害表征崩溃'定理：在确定性MDPs中，如果训练使潜在转换一致性损失和辅助回归损失都趋近于零，那么任何非等价观测（即那些不具有相同转换动力学或辅助标签的观测）必须映射到不同的潜在表征。因此，辅助任务锚定了表征必须保留的区分。在计数环境中的受控消融实验证实了这一理论，并表明与辅助头联合训练JEPA模型比分别训练它们能生成更丰富的表征。我们的工作指出了改进JEPA编码器的一条路径：使用一个辅助函数与它们一起训练，该辅助函数与转换动力学一起编码正确的等价关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint-Embedding Predictive Architecture (JEPA) is increasingly used forvisual representation learning and as a component in model-based RL, but itsbehavior remains poorly understood. We provide a theoretical characterizationof a simple, practical JEPA variant that has an auxiliary regression headtrained jointly with latent dynamics. We prove a No Unhealthy RepresentationCollapse theorem: in deterministic MDPs, if training drives both thelatent-transition consistency loss and the auxiliary regression loss to zero,then any pair of non-equivalent observations, i.e., those that do not have thesame transition dynamics or auxiliary label, must map to distinct latentrepresentations. Thus, the auxiliary task anchors which distinctions therepresentation must preserve. Controlled ablations in a counting environmentcorroborate the theory and show that training the JEPA model jointly with theauxiliary head generates a richer representation than training them separately.Our work indicates a path to improve JEPA encoders: training them with anauxiliary function that, together with the transition dynamics, encodes theright equivalence relations.</description>
      <author>example@mail.com (Jiacan Yu, Siyi Chen, Mingrui Liu, Nono Horiuchi, Vladimir Braverman, Zicheng Xu, Dan Haramati, Randall Balestriero)</author>
      <guid isPermaLink="false">2509.12249v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Agents via Continual Pre-training</title>
      <link>http://arxiv.org/abs/2509.13310v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为智能体持续预训练(Agentic CPT)的新方法，用于构建强大的智能体基础模型，解决了后训练过程中同时学习多样智能体行为与对齐专家演示之间的优化冲突问题。基于此方法开发的AgentFounder-30B模型在10个基准测试上取得了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大语言模型已发展为能够自主使用工具和多步推理的智能体系统，用于解决复杂问题。然而，基于通用基础模型的后训练方法在智能体任务中表现不佳，特别是在开源实现中。&lt;h4&gt;目的&lt;/h4&gt;构建强大的智能体基础模型，解决后训练过程中同时学习多样智能体行为与对齐专家演示之间的优化冲突问题。&lt;h4&gt;方法&lt;/h4&gt;首次将智能体持续预训练(Agentic CPT)纳入深度研究智能体训练流程，并基于此方法开发了名为AgentFounder的深度研究智能体模型。&lt;h4&gt;主要发现&lt;/h4&gt;AgentFounder-30B在10个基准测试上取得了最先进性能，同时保持强大的工具使用能力，具体表现为：在BrowseComp-en上达到39.9%，在BrowseComp-zh上达到43.3%，在HLE上达到31.5%的Pass@1。&lt;h4&gt;结论&lt;/h4&gt;通过引入Agentic CPT方法，成功构建了强大的智能体基础模型，解决了后训练过程中的优化冲突问题，显著提升了智能体任务性能。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型已经发展为能够自主使用工具和多步推理的智能体系统，用于解决复杂问题。然而，建立在通用基础模型之上的后训练方法在智能体任务中持续表现不佳，特别是在开源实现中。我们确定了根本原因：缺乏强大的智能体基础模型迫使模型在后训练过程中同时学习多样的智能体行为，同时将这些行为与专家演示对齐，从而产生根本的优化冲突。为此，我们首次提出将智能体持续预训练(Agentic CPT)纳入深度研究智能体训练流程，以构建强大的智能体基础模型。基于这种方法，我们开发了一个名为AgentFounder的深度研究智能体模型。我们在10个基准测试上评估了AgentFounder-30B，并取得了最先进的性能，同时保持了强大的工具使用能力，特别是在BrowseComp-en上达到39.9%，在BrowseComp-zh上达到43.3%，以及在HLE上达到31.5%的Pass@1。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have evolved into agentic systems capable ofautonomous tool use and multi-step reasoning for complex problem-solving.However, post-training approaches building upon general-purpose foundationmodels consistently underperform in agentic tasks, particularly in open-sourceimplementations. We identify the root cause: the absence of robust agenticfoundation models forces models during post-training to simultaneously learndiverse agentic behaviors while aligning them to expert demonstrations, therebycreating fundamental optimization tensions. To this end, we are the first topropose incorporating Agentic Continual Pre-training (Agentic CPT) into thedeep research agents training pipeline to build powerful agentic foundationalmodels. Based on this approach, we develop a deep research agent model namedAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achievestate-of-the-art performance while retains strong tool-use ability, notably39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.</description>
      <author>example@mail.com (Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou)</author>
      <guid isPermaLink="false">2509.13310v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>ResidualViT for Efficient Temporally Dense Video Encoding</title>
      <link>http://arxiv.org/abs/2509.13255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作通过创新的架构设计和蒸馏策略，显著提高了时间密集型视频理解任务的计算效率，同时保持了准确性。&lt;h4&gt;背景&lt;/h4&gt;多个视频理解任务需要'时间密集型'推理，如自然语言视频时序定位、时序活动定位和音频描述生成。这些任务需要在高时间分辨率下对帧进行采样，而计算帧级特征对于这些任务来说计算成本很高。&lt;h4&gt;目的&lt;/h4&gt;降低时间密集型任务中特征计算的成本，提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为ResidualViT的视觉Transformer架构，利用视频中的大时间冗余来高效计算时间密集的帧级特征。该架构包含可学习的残差连接确保时间一致性，以及标记减少模块提高处理速度。同时提出了一种轻量级蒸馏策略来近似原始基础模型的帧级特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个任务和五个数据集上进行了评估，在零样本和全监督设置下都表现出显著效果。计算成本降低了高达60%，推理速度提高了高达2.5倍，同时保持了与原始基础模型相近的准确性。&lt;h4&gt;结论&lt;/h4&gt;ResidualViT架构和蒸馏策略有效地降低了时间密集型视频理解任务的计算成本，同时保持了性能，为视频理解任务提供了更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多个视频理解任务，如自然语言视频时序定位、时序活动定位和音频描述生成，需要在高时间分辨率下采样的帧上进行'时间密集型'推理。然而，考虑到时间分辨率的要求，为这些任务计算帧级特征的计算成本很高。在本文中，我们做出了三项贡献以降低时间密集型任务的特征计算成本。首先，我们引入了一种视觉Transformer架构，称为ResidualViT，它利用视频中的大时间冗余来高效计算时间密集的帧级特征。我们的架构包含(i)确保连续帧之间时间一致性的可学习残差连接，以及(ii)一个标记减少模块，通过选择性丢弃时间冗余信息同时重用预训练基础模型的权重来提高处理速度。其次，我们提出了一种轻量级蒸馏策略来近似原始基础模型的帧级特征。最后，我们在四个任务和五个数据集上评估了我们的方法，在零样本和全监督设置下，展示了计算成本的显著降低（高达60%）和推理速度的改进（高达2.5倍），同时紧密近似了原始基础模型的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Several video understanding tasks, such as natural language temporal videogrounding, temporal activity localization, and audio description generation,require "temporally dense" reasoning over frames sampled at high temporalresolution. However, computing frame-level features for these tasks iscomputationally expensive given the temporal resolution requirements. In thispaper, we make three contributions to reduce the cost of computing features fortemporally dense tasks. First, we introduce a vision transformer (ViT)architecture, dubbed ResidualViT, that leverages the large temporal redundancyin videos to efficiently compute temporally dense frame-level features. Ourarchitecture incorporates (i) learnable residual connections that ensuretemporal consistency across consecutive frames and (ii) a token reductionmodule that enhances processing speed by selectively discarding temporallyredundant information while reusing weights of a pretrained foundation model.Second, we propose a lightweight distillation strategy to approximate theframe-level features of the original foundation model. Finally, we evaluate ourapproach across four tasks and five datasets, in both zero-shot and fullysupervised settings, demonstrating significant reductions in computational cost(up to 60%) and improvements in inference speed (up to 2.5x faster), all whileclosely approximating the accuracy of the original foundation model.</description>
      <author>example@mail.com (Mattia Soldan, Fabian Caba Heilbron, Bernard Ghanem, Josef Sivic, Bryan Russell)</author>
      <guid isPermaLink="false">2509.13255v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning</title>
      <link>http://arxiv.org/abs/2509.13240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NoRA是一种创新的参数高效微调(PEFT)框架，首次直接适应预训练Transformer模型中的非线性激活函数，而非仅调整权重矩阵。&lt;h4&gt;背景&lt;/h4&gt;现有的PEFT方法主要适应权重矩阵而保持激活函数固定，限制了模型适应的灵活性。&lt;h4&gt;目的&lt;/h4&gt;引入NoRA框架，探索激活函数作为模型适应一级对象的潜力，提供一种新的参数高效微调方法。&lt;h4&gt;方法&lt;/h4&gt;NoRA用可学习的有理函数替换固定的激活函数，对分子和分母系数应用结构化低秩更新，并采用分组设计实现本地化适应和提高稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;NoRA在视觉Transformer上仅更新0.4%参数即可匹配或超越完全微调；与LoRA结合的NoRA++在多种任务上优于其他方法；NoRA能将适应限制在低维函数子空间，实现隐式正则化。&lt;h4&gt;结论&lt;/h4&gt;激活空间调优是基于权重的PEFT的互补且高度参数高效的替代方案，激活函数应被视为模型适应的一级对象。&lt;h4&gt;翻译&lt;/h4&gt;现有的参数高效微调(PEFT)方法主要适应权重矩阵，同时保持激活函数固定。我们引入NoRA，这是第一个直接适应预训练基于Transformer模型的非线性激活函数的PEFT框架。NoRA用可学习的有理函数替换固定的激活函数，并对分子和分母系数应用结构化低秩更新，采用分组设计使适应本地化并提高稳定性，同时成本最小。在CIFAR-10和CIFAR-100上训练的视觉Transformer上，NoRA在仅更新0.4%参数(0.02M)的情况下匹配或完全微调，实现了+0.17%和+0.27%的准确率提升。与LoRA结合(NoRA++)时，在匹配的训练预算下，通过添加更少的可训练参数，优于LoRA和DoRA。在LLaMA3-8B指令调优上，NoRA++持续提高生成质量，平均MMLU提升+0.3%--0.8%，包括在STEM(Alpaca)上+1.6%和OpenOrca上+1.3%。我们进一步证明NoRA将适应限制在低维函数子空间，隐式正则化更新的幅度和方向。这些结果确立了激活空间调优作为基于权重的PEFT的互补且高度参数高效的替代方案，将激活函数定位为模型适应的一级对象。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing parameter-efficient fine-tuning (PEFT) methods primarily adaptweight matrices while keeping activation functions fixed. We introduce\textbf{NoRA}, the first PEFT framework that directly adapts nonlinearactivation functions in pretrained transformer-based models. NoRA replacesfixed activations with learnable rational functions and applies structuredlow-rank updates to numerator and denominator coefficients, with a group-wisedesign that localizes adaptation and improves stability at minimal cost. Onvision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceedsfull fine-tuning while updating only 0.4\% of parameters (0.02M), achievingaccuracy gains of +0.17\% and +0.27\%. When combined with LoRA(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgetsby adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++consistently improves generation quality, yielding average MMLU gains of+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. Wefurther show that NoRA constrains adaptation to a low-dimensional functionalsubspace, implicitly regularizing update magnitude and direction. These resultsestablish activation-space tuning as a complementary and highlyparameter-efficient alternative to weight-based PEFT, positioning activationfunctions as first-class objects for model adaptation.</description>
      <author>example@mail.com (Bo Yin, Xingyi Yang, Xinchao Wang)</author>
      <guid isPermaLink="false">2509.13240v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Road Obstacle Video Segmentation</title>
      <link>http://arxiv.org/abs/2509.13181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  GCPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对自动驾驶中道路障碍物分割问题，认识到该任务具有时间相关性，提出了基于视觉基础模型的新方法，在长序列视频分割中取得了最先进的效果。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶代理的广泛部署，道路障碍物的检测与分割对于确保安全导航变得至关重要。然而，现有的道路障碍物分割方法应用于单帧图像，忽略了问题的时间特性，导致连续帧之间的预测图不一致。&lt;h4&gt;目的&lt;/h4&gt;解决现有道路障碍物分割方法在处理连续帧时存在的不一致问题，提高道路障碍物视频分割的准确性，为未来研究提供有价值的见解和方向。&lt;h4&gt;方法&lt;/h4&gt;1. 构建并调整了四个用于道路障碍物视频分割的评估基准；2. 在这些基准上评估了11种最先进的图像和视频分割方法；3. 引入了两种基于视觉基础模型的新型基线方法。&lt;h4&gt;主要发现&lt;/h4&gt;道路障碍物分割任务本质上具有时间特性，因为连续帧的分割图之间存在强相关性。作者提出的方法在长序列视频分割中建立了新的技术水平。&lt;h4&gt;结论&lt;/h4&gt;通过考虑时间相关性，改进的道路障碍物视频分割方法能够提供更一致和准确的预测，这对自动驾驶系统的安全性和可靠性具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;随着自动驾驶代理的广泛部署，道路障碍物的检测与分割已成为确保安全自动驾驶的关键。然而，现有的道路障碍物分割方法应用于单个帧，忽略了问题的时间特性，导致连续帧之间的预测图不一致。在本工作中，我们证明了道路障碍物分割任务本质上具有时间性，因为连续帧的分割图之间存在强相关性。为此，我们整理并调整了四个用于道路障碍物视频分割的评估基准，并评估了11种最先进的基于图像和视频的分割方法。此外，我们引入了两种基于视觉基础模型的有效基线方法。我们的方法在长序列视频的道路障碍物视频分割中建立了新的技术水平，为未来研究提供了宝贵的见解和方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing deployment of autonomous driving agents, the detection andsegmentation of road obstacles have become critical to ensure safe autonomousnavigation. However, existing road-obstacle segmentation methods are applied onindividual frames, overlooking the temporal nature of the problem, leading toinconsistent prediction maps between consecutive frames. In this work, wedemonstrate that the road-obstacle segmentation task is inherently temporal,since the segmentation maps for consecutive frames are strongly correlated. Toaddress this, we curate and adapt four evaluation benchmarks for road-obstaclevideo segmentation and evaluate 11 state-of-the-art image- and video-basedsegmentation methods on these benchmarks. Moreover, we introduce two strongbaseline methods based on vision foundation models. Our approach establishes anew state-of-the-art in road-obstacle video segmentation for long-range videosequences, providing valuable insights and direction for future research.</description>
      <author>example@mail.com (Shyam Nandan Rai, Shyamgopal Karthik, Mariana-Iuliana Georgescu, Barbara Caputo, Carlo Masone, Zeynep Akata)</author>
      <guid isPermaLink="false">2509.13181v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection</title>
      <link>http://arxiv.org/abs/2509.12995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于现代视觉基础模型的简单线性分类器方法，用于检测AI生成的图像，相比专门设计的检测器在真实场景中表现更优。&lt;h4&gt;背景&lt;/h4&gt;专门的AI生成图像检测器在精心设计的基准测试中表现出色，但在真实场景中表现很差，表现为在'野外'基准测试中有极高的假阴性率。&lt;h4&gt;目的&lt;/h4&gt;开发一种更有效的AI生成图像检测方法，能够在真实世界场景中取得更好的性能。&lt;h4&gt;方法&lt;/h4&gt;在现代视觉基础模型(VFM)上使用简单的线性分类器，而不是设计专门的检测器，在相同数据上训练。&lt;h4&gt;主要发现&lt;/h4&gt;1) 基于VFM的方法在'野外'准确率上比专门设计的检测器提高了超过20%；2) 最近的VLM已经学会将合成图像与伪造相关概念对齐；3) 这种对齐能力可能源于模型在预训练期间接触到的数据。&lt;h4&gt;结论&lt;/h4&gt;1) 对于AI生成图像检测，更新后的VFM的原始能力比静态检测器的专门设计更有效；2) 真正的泛化评估需要测试数据独立于模型的整个训练历史，包括预训练。&lt;h4&gt;翻译&lt;/h4&gt;虽然针对AI生成图像的专业检测器在精心设计的基准测试中表现出色，但它们在真实场景中的表现灾难性地差，正如它们在'野外'基准测试中极高的假阴性率所证明的那样。我们不是为这个问题再打造一把专门的'刀'，而是带来一把'枪'：在现代视觉基础模型(VFM)上使用一个简单的线性分类器。在相同数据上训练后，这个基线方法明显优于专门设计的检测器，在'野外'准确率上提高了超过20%。我们的分析确定了VFM'火力'的来源：首先，通过探测文本-图像相似性，我们发现最近的VLM(如Perception Encoder、Meta CLIP2)已经学会将合成图像与伪造相关概念(如'AI生成')对齐，而之前的版本则没有。其次，我们推测这是由于数据暴露，因为在一个在VFM预训练截止日期后抓取的新数据集上，这种对齐和整体准确率都大幅下降，确保了在预训练期间未见。我们的研究得出两个关键结论：1) 对于AI生成图像检测的真实'枪战'，更新后的VFM的原始'火力'比静态检测器的'工艺'更有效。2) 真正的泛化评估需要测试数据独立于模型的整个训练历史，包括预训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While specialized detectors for AI-generated images excel on curatedbenchmarks, they fail catastrophically in real-world scenarios, as evidenced bytheir critically high false-negative rates on `in-the-wild' benchmarks. Insteadof crafting another specialized `knife' for this problem, we bring a `gun' tothe fight: a simple linear classifier on a modern Vision Foundation Model(VFM). Trained on identical data, this baseline decisively `outguns' bespokedetectors, boosting in-the-wild accuracy by a striking margin of over 20\%.  Our analysis pinpoints the source of the VFM's `firepower': First, by probingtext-image similarities, we find that recent VLMs (e.g., Perception Encoder,Meta CLIP2) have learned to align synthetic images with forgery-relatedconcepts (e.g., `AI-generated'), unlike previous versions. Second, we speculatethat this is due to data exposure, as both this alignment and overall accuracyplummet on a novel dataset scraped after the VFM's pre-training cut-off date,ensuring it was unseen during pre-training. Our findings yield two criticalconclusions: 1) For the real-world `gunfight' of AI-generated image detection,the raw `firepower' of an updated VFM is far more effective than the`craftsmanship' of a static detector. 2) True generalization evaluationrequires test data to be independent of the model's entire training history,including pre-training.</description>
      <author>example@mail.com (Yue Zhou, Xinan He, Kaiqing Lin, Bing Fan, Feng Ding, Jinhua Zeng, Bin Li)</author>
      <guid isPermaLink="false">2509.12995v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder</title>
      <link>http://arxiv.org/abs/2509.12991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A simple yet effective strategy for ECG foundation models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种简单而有效的后训练方法，用于增强ECG基础模型，显著提升了其在临床应用中的性能，特别是在有限数据情况下表现优异。&lt;h4&gt;背景&lt;/h4&gt;ECG基础模型因其跨任务适应性而日益流行，但与特定任务模型相比存在性能差距，即使在大型数据集预训练和目标数据微调后仍然存在，这可能是由于缺乏有效的后训练策略。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单而有效的后训练方法来增强ECG Founder模型（一种在超过700万个ECG记录上预训练的最先进基础模型），提高其临床适用性。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单而有效的后训练方法应用于ECG Founder模型，并在PTB-XL基准测试上进行评估，同时进行了消融研究以确定关键性能贡献组件。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在宏观AUROC上比基线微调策略提高1.2%-3.3%，在宏观AUPRC上提高5.3%-20.9%；优于多种先进方法；仅使用10%训练数据时，宏观AUROC提高9.1%，宏观AUPRC提高34.9%；随机深度和预览线性探测是关键性能提升组件。&lt;h4&gt;结论&lt;/h4&gt;后训练策略在改进ECG基础模型方面具有显著潜力，这项工作有望促进ECG领域基础模型的持续发展。&lt;h4&gt;翻译&lt;/h4&gt;ECG基础模型因其跨任务的适应性而日益流行。然而，与特定任务模型相比，它们的临床适用性通常受到性能差距的限制，即使在大型ECG数据集上进行预训练并在目标数据上进行微调后也是如此。这种局限性可能是由于缺乏有效的后训练策略。在本文中，我们提出了一种简单而有效的后训练方法来增强ECG Founder模型，这是一种在超过700万个ECG记录上预训练的最先进ECG基础模型。在PTB-XL基准测试上的实验表明，我们的方法在宏观AUROC上比基线微调策略提高了1.2%-3.3%，在宏观AUPRC上提高了5.3%-20.9%。此外，我们的方法优于几种最近的先进方法，包括特定任务模型和高级架构。进一步评估显示，与基线相比，我们的方法更稳定且样本效率更高，仅使用10%的训练数据即可实现9.1%的宏观AUROC和34.9%的宏观AUPRC改进。消融研究确定了随机深度和预览线性探测等关键组件，这些组件有助于提高性能。这些发现强调了后训练策略在改进ECG基础模型方面的潜力，我们希望这项工作将促进ECG领域基础模型的持续发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; ECG foundation models are increasingly popular due to their adaptabilityacross various tasks. However, their clinical applicability is often limited byperformance gaps compared to task-specific models, even after pre-training onlarge ECG datasets and fine-tuning on target data. This limitation is likelydue to the lack of an effective post-training strategy. In this paper, wepropose a simple yet effective post-training approach to enhance ECGFounder, astate-of-the-art ECG foundation model pre-trained on over 7 million ECGrecordings. Experiments on the PTB-XL benchmark show that our approach improvesthe baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% inmacro AUPRC. Additionally, our method outperforms several recentstate-of-the-art approaches, including task-specific and advancedarchitectures. Further evaluation reveals that our method is more stable andsample-efficient compared to the baseline, achieving a 9.1% improvement inmacro AUROC and a 34.9% improvement in macro AUPRC using just 10% of thetraining data. Ablation studies identify key components, such as stochasticdepth and preview linear probing, that contribute to the enhanced performance.These findings underscore the potential of post-training strategies to improveECG foundation models, and we hope this work will contribute to the continueddevelopment of foundation models in the ECG domain.</description>
      <author>example@mail.com (Ya Zhou, Yujie Yang, Xiaohan Fan, Wei Zhao)</author>
      <guid isPermaLink="false">2509.12991v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Data Scaling Laws for Radiology Foundation Models</title>
      <link>http://arxiv.org/abs/2509.12818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性地研究了两种医学影像视觉编码器MI2和RAD-DINO在大规模胸部X光片数据上的持续预训练效果，评估了模型在分类、分割和放射报告生成任务上的表现，发现不同模型在不同任务上有各自优势，并证明了特定中心持续预训练的实用价值。&lt;h4&gt;背景&lt;/h4&gt;基础视觉编码器如CLIP和DINOv2经过网络规模数据训练后展现出强大的跨任务性能，但医学影像基础模型受限于较小的数据集，限制了我们对数据规模和预训练范式如何影响性能的理解。&lt;h4&gt;目的&lt;/h4&gt;系统研究两种代表CLIP和DINOv2范式的视觉编码器MI2和RAD-DINO在350万张胸部X光片上的持续预训练效果，同时保持计算和评估协议不变。&lt;h4&gt;方法&lt;/h4&gt;在350万张胸部X光片上持续预训练两种视觉编码器；评估模型在分类、分割和放射报告生成任务上的表现；包含线条和导管任务以平衡偏倚；使用UniCL结合报告和结构化标签预训练MI2；测试仅需少量领域内样本即可超越开放权重基础模型的场景。&lt;h4&gt;主要发现&lt;/h4&gt;MI2在发现相关任务上扩展更有效，RAD-DINO在导管相关任务上更强；使用报告和结构化标签通过UniCL持续预训练MI2可提升性能，突显大规模结构化监督的价值；对于某些任务，仅需30k领域内样本就足以超越开放权重基础模型。&lt;h4&gt;结论&lt;/h4&gt;特定中心持续预训练具有实用价值，医疗机构可利用领域内数据获得显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;基础视觉编码器如CLIP和DINOv2经过网络规模数据训练后，在任务和数据集间展现出强大的转移性能。然而，医学影像基础模型仍受限于较小的数据集，限制了我们对数据规模和预训练范式在此环境中如何影响性能的理解。在本工作中，我们系统地研究了两种视觉编码器（代表CLIP和DINOv2两种主要编码器范式的MedImageInsight (MI2)和RAD-DINO）在多达350万张来自单一机构的胸部X光片上的持续预训练，同时保持计算和评估协议不变。我们在分类（放射学发现、线条和导管）、分割（线条和导管）和放射报告生成任务上进行了评估。尽管先前工作主要关注与放射学发现相关的任务，但我们包含了线条和导管任务，以平衡这种偏倚，并评估模型提取沿 elongated 结构保持连续性的特征的能力。我们的实验表明，MI2在发现相关任务上扩展更有效，而RAD-DINO在导管相关任务上更强。令人惊讶的是，使用报告和结构化标签通过UniCL持续预训练MI2可以提高性能，突显了大规模结构化监督的价值。我们进一步表明，对于某些任务，仅需30k领域内样本就足以超越开放权重基础模型。这些结果突显了特定中心持续预训练的实用性，使医疗机构能够利用领域内数据获得显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation vision encoders such as CLIP and DINOv2, trained on web-scaledata, exhibit strong transfer performance across tasks and datasets. However,medical imaging foundation models remain constrained by smaller datasets,limiting our understanding of how data scale and pretraining paradigms affectperformance in this setting. In this work, we systematically study continualpretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINOrepresenting the two major encoder paradigms CLIP and DINOv2, on up to 3.5Mchest x-rays from a single institution, holding compute and evaluationprotocols constant. We evaluate on classification (radiology findings, linesand tubes), segmentation (lines and tubes), and radiology report generation.While prior work has primarily focused on tasks related to radiology findings,we include lines and tubes tasks to counterbalance this bias and evaluate amodel's ability to extract features that preserve continuity along elongatedstructures. Our experiments show that MI2 scales more effectively forfinding-related tasks, while RAD-DINO is stronger on tube-related tasks.Surprisingly, continually pretraining MI2 with both reports and structuredlabels using UniCL improves performance, underscoring the value of structuredsupervision at scale. We further show that for some tasks, as few as 30kin-domain samples are sufficient to surpass open-weights foundation models.These results highlight the utility of center-specific continual pretraining,enabling medical institutions to derive significant performance gains byutilizing in-domain data.</description>
      <author>example@mail.com (Maximilian Ilse, Harshita Sharma, Anton Schwaighofer, Sam Bond-Taylor, Fernando Pérez-García, Olesya Melnichenko, Anne-Marie G. Sykes, Kelly K. Horst, Ashish Khandelwal, Maxwell Reynolds, Maria T. Wetscherek, Noel C. F. Codella, Javier Alvarez-Valle, Korfiatis Panagiotis, Valentina Salvatelli)</author>
      <guid isPermaLink="false">2509.12818v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach</title>
      <link>http://arxiv.org/abs/2509.12697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种双层个性化框架，用于联邦基础模型的微调，以解决在有限数据情况下个性化与联邦之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;联邦基础模型代表了一种新的范式，可以在客户端之间联合微调预训练的基础模型。然而，为少数新用户或专门场景微调基础模型仍然是一个挑战，因为这些场景涉及的数据规模通常远小于预训练时使用的大规模数据。&lt;h4&gt;目的&lt;/h4&gt;解决在有限数据情况下个性化与联邦之间的权衡问题，特别是在为少数新用户或专门场景微调基础模型时。&lt;h4&gt;方法&lt;/h4&gt;提出了一种双层个性化框架：在客户端层面，使用私有数据进行个性化微调；在服务器层面，使用基于客户端特定任务向量测量的相似用户进行个性化聚合。&lt;h4&gt;主要发现&lt;/h4&gt;通过客户端微调获得的个性化信息，服务器层面的个性化聚合可以获得群体级别的个性化信息，同时减轻非独立同分布数据中不相关或利益冲突客户端的干扰。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法在基准数据集上的广泛实验分析中证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;联邦基础模型代表了一种新的范式，可以在客户端之间联合微调预训练的基础模型。为少数新用户或专门场景微调基础模型仍然是一个挑战，这些场景涉及的数据规模通常远小于预训练时使用的大规模数据。在这种情况下，个性化与联邦之间的权衡变得更加敏感。为了解决这些问题，我们提出了一种用于联邦基础模型微调的双层个性化框架。具体来说，我们在客户端层面使用私有数据进行个性化微调，然后在服务器层面使用基于客户端特定任务向量测量的相似用户进行个性化聚合。鉴于从客户端微调中获得的个性化信息，服务器层面的个性化聚合可以获得群体级别的个性化信息，同时减轻非独立同分布数据中不相关或利益冲突客户端的干扰。所提出的算法在基准数据集上的广泛实验分析中证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated foundation models represent a new paradigm to jointly fine-tunepre-trained foundation models across clients. It is still a challenge tofine-tune foundation models for a small group of new users or specializedscenarios, which typically involve limited data compared to the large-scaledata used in pre-training. In this context, the trade-off betweenpersonalization and federation becomes more sensitive. To tackle these, weproposed a bi-level personalization framework for federated fine-tuning onfoundation models. Specifically, we conduct personalized fine-tuning on theclient-level using its private data, and then conduct a personalizedaggregation on the server-level using similar users measured by client-specifictask vectors. Given the personalization information gained from client-levelfine-tuning, the server-level personalized aggregation can gain group-wisepersonalization information while mitigating the disturbance of irrelevant orinterest-conflict clients with non-IID data. The effectiveness of the proposedalgorithm has been demonstrated by extensive experimental analysis in benchmarkdatasets.</description>
      <author>example@mail.com (Yiyuan Yang, Guodong Long, Qinghua Lu, Liming Zhu, Jing Jiang)</author>
      <guid isPermaLink="false">2509.12697v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.12650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages,8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TimeRep是一种利用时间序列基础模型中间层表示进行异常检测的新方法，通过计算表示之间的距离作为异常分数，并采用核心集策略和适应机制处理概念漂移。&lt;h4&gt;背景&lt;/h4&gt;时间序列数据中的异常检测对许多现实系统的可靠运行至关重要，时间序列基础模型已成为异常检测的有力工具。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖于TSFMs最终层表示，而是利用中间层表示进行异常检测的新方法。&lt;h4&gt;方法&lt;/h4&gt;TimeRep选择最具信息量的中间层和patch-token位置，从训练数据形成中间表示参考集合，应用核心集策略减小集合大小，在推理时通过测量距离计算异常分数，并集成适应机制处理概念漂移。&lt;h4&gt;主要发现&lt;/h4&gt;在包含250个单变量时间序列的UCR异常档案上进行的实验表明，TimeRep始终优于各种最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;TimeRep是一种有效的时间序列异常检测方法，通过利用中间层表示而非最终层表示，结合核心集策略和适应机制，在各种基线方法上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;检测时间序列数据中的异常对于许多现实世界系统的可靠运行至关重要。最近，时间序列基础模型(TSFMs)已成为异常检测的有力工具。然而，现有方法通常依赖于TSFMs的最终层表示，通过特定任务头来计算异常分数作为重建或预测误差。相反，我们提出了TimeRep，一种新颖的异常检测方法，利用TSFMs的中间层表示，将这些表示之间的距离计算为异常分数。给定预训练的TSFM，TimeRep选择产生最具信息量表示的中间层和patch-token位置。TimeRep从训练数据形成中间表示的参考集合，并应用core-set策略来减小其大小，同时保持分布覆盖。在推理期间，TimeRep通过测量其中间表示与集合中中间表示之间的距离来计算传入数据的异常分数。为了处理概念漂移，TimeRep集成了一个适应机制，在推理时，仅用来自传入数据的非冗余中间表示来增强集合。我们在包含250个单变量时间序列的UCR异常档案上进行了广泛的实验。TimeRep始终优于各种最先进的基线方法，包括非深度学习、深度学习和基于基础模型的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting anomalies in time series data is essential for the reliableoperation of many real-world systems. Recently, time series foundation models(TSFMs) have emerged as a powerful tool for anomaly detection. However,existing methods typically rely on the final layer's representations of TSFMs,computing the anomaly score as a reconstruction or forecasting error via atask-specific head. Instead, we propose TimeRep, a novel anomaly detectionapproach that leverages the intermediate layer's representations of TSFMs,computing the anomaly score as the distance between these representations.Given a pre-trained TSFM, TimeRep selects the intermediate layer andpatch-token position that yield the most informative representation. TimeRepforms a reference collection of intermediate representations from the trainingdata and applies a core-set strategy to reduce its size while maintainingdistributional coverage. During inference, TimeRep computes the anomaly scorefor incoming data by measuring the distance between its intermediaterepresentations and those of the collection. To address concept drift, TimeRepintegrates an adaptation mechanism that, at inference time, augments thecollection exclusively with non-redundant intermediate representations fromincoming data. We conducted extensive experiments on the UCR Anomaly Archive,which contains 250 univariate time series. TimeRep consistently outperforms abroad spectrum of state-of-the-art baselines, including non-DL, DL, andfoundation model-based methods.</description>
      <author>example@mail.com (Chan Sik Han, Keon Myung Lee)</author>
      <guid isPermaLink="false">2509.12650v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction</title>
      <link>http://arxiv.org/abs/2509.12600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MICE多模态基础模型，通过整合病理图像、临床报告和基因组数据，实现了精确的泛癌预后预测，显著提升了模型的泛化性和数据效率。&lt;h4&gt;背景&lt;/h4&gt;多模态数据为全面理解肿瘤微环境提供了异构信息，但现有AI模型往往难以有效利用多模态数据中的丰富信息，提取的泛化性较差。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效整合病理图像、临床报告和基因组数据的多模态基础模型，用于精确的泛癌预后预测。&lt;h4&gt;方法&lt;/h4&gt;提出了MICE（Multimodal data Integration via Collaborative Experts）多模态基础模型，采用多种功能不同的专家全面捕捉跨癌种和癌种特定洞察，利用来自30种癌症类型、11,799名患者的数据，通过对比学习和监督学习相结合增强MICE的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;MICE优于单模态和最先进的基于多专家的多模态模型，在内部队列中C-index提高了3.8%至11.2%，在独立队列中提高了5.8%至8.8%，同时在多样化的临床场景中表现出卓越的数据效率。&lt;h4&gt;结论&lt;/h4&gt;MICE具有增强的泛化性和数据效率，为泛癌预后预测提供了有效且可扩展的基础，有潜力个性化定制治疗并改善治疗效果。&lt;h4&gt;翻译&lt;/h4&gt;多模态数据为全面理解肿瘤微环境提供了异构信息。然而，现有的人工智能模型往往难以利用多模态数据中的丰富信息，提取的泛化性较差。在此，我们提出了MICE（通过协作专家进行多模态数据集成），这是一个多模态基础模型，能够有效整合病理图像、临床报告和基因组数据，用于精确的泛癌预后预测。MICE采用多种功能不同的专家，全面捕捉跨癌种和癌种特定的洞察，而非传统的多专家模块。利用来自30种癌症类型、11,799名患者的数据，我们通过结合对比学习和监督学习增强了MICE的泛化能力。MICE优于单模态和最先进的基于多专家的多模态模型，在内部队列中C-index显著提高了3.8%至11.2%，在独立队列中提高了5.8%至8.8%。此外，它在多样化的临床场景中表现出卓越的数据效率。凭借其增强的泛化性和数据效率，MICE为泛癌预后预测建立了有效且可扩展的基础，具有为个性化定制治疗和改善治疗效果提供强大潜力的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal data provides heterogeneous information for a holisticunderstanding of the tumor microenvironment. However, existing AI models oftenstruggle to harness the rich information within multimodal data and extractpoorly generalizable representations. Here we present MICE (Multimodal dataIntegration via Collaborative Experts), a multimodal foundation model thateffectively integrates pathology images, clinical reports, and genomics datafor precise pan-cancer prognosis prediction. Instead of conventionalmulti-expert modules, MICE employs multiple functionally diverse experts tocomprehensively capture both cross-cancer and cancer-specific insights.Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE'sgeneralizability by coupling contrastive and supervised learning. MICEoutperformed both unimodal and state-of-the-art multi-expert-based multimodalmodels, demonstrating substantial improvements in C-index ranging from 3.8% to11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,respectively. Moreover, it exhibited remarkable data efficiency across diverseclinical scenarios. With its enhanced generalizability and data efficiency,MICE establishes an effective and scalable foundation for pan-cancer prognosisprediction, holding strong potential to personalize tailored therapies andimprove treatment outcomes.</description>
      <author>example@mail.com (Huajun Zhou, Fengtao Zhou, Jiabo Ma, Yingxue Xu, Xi Wang, Xiuming Zhang, Li Liang, Zhenhui Li, Hao Chen)</author>
      <guid isPermaLink="false">2509.12600v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification</title>
      <link>http://arxiv.org/abs/2509.12512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACCEPTED at the ICCV 2025 Workshop on Anomaly Detection with  Foundation Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于注意力的全局聚合框架，用于3D医学图像异常分类，利用预训练的DINOv2模型和软注意机制处理脑部MRI切片，并通过复合损失函数解决数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;医学影像中的异常检测和分类对早期诊断至关重要，但由于标注数据有限、类别不平衡和专家标注的高成本，这仍然是一个挑战。新兴的视觉基础模型如DINOv2在大量无标注数据上预训练，可提供通用表示来缓解这些限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门用于3D医学图像异常分类的基于注意力的全局聚合框架，利用自监督DINOv2模型作为预训练特征提取器。&lt;h4&gt;方法&lt;/h4&gt;处理脑部MRI的2D轴向切片，通过软注意机制分配自适应的切片级重要性权重，并采用复合损失函数结合监督对比学习和类别方差正则化来增强类间分离性和类内一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在ADNI数据集和机构多类别头痛队列上的验证表明，尽管数据有限且类别不平衡显著，该方法仍表现出强大的异常分类性能。&lt;h4&gt;结论&lt;/h4&gt;利用预训练的2D基础模型结合基于注意力的切片聚合，对于实现医学影像中鲁棒的体积异常检测是有效的，该方法已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;医学影像中的异常检测和分类对早期诊断至关重要，但由于标注数据有限、类别不平衡和专家标注的高成本，这仍然具有挑战性。新兴的视觉基础模型（如DINOv2）在大量无标注数据上预训练，可提供通用表示， potentially缓解这些限制。在本研究中，我们提出了一种专门用于3D医学图像异常分类的基于注意力的全局聚合框架。利用自监督DINOv2模型作为预训练特征提取器，我们的方法处理脑部MRI的2D轴向切片，通过软注意机制分配自适应的切片级重要性权重。为进一步解决数据稀缺问题，我们采用结合监督对比学习和类别方差正则化的复合损失函数，增强类间分离性和类内一致性。我们在ADNI数据集和机构多类别头痛队列上验证了我们的框架，尽管数据有限且类别不平衡显著，但仍表现出强大的异常分类性能。我们的结果突显了利用预训练的2D基础模型结合基于注意力的切片聚合进行医学影像中鲁棒体积异常检测的有效性。我们的实现已在https://github.com/Rafsani/DinoAtten3D.git上公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学影像（特别是3D脑部MRI）中异常检测和分类的挑战，包括标注数据有限、类别不平衡和专家标注成本高的问题。这个问题在现实中非常重要，因为早期疾病诊断对治疗和预后至关重要，而医学数据获取和标注成本高昂限制了监督学习方法的应用，类别不平衡问题也影响模型泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统监督学习方法需要大量标注数据且易过拟合，无监督方法（如基于GAN或扩散模型）仅依赖健康数据重建，可能因健康数据有限而泛化能力不足。作者注意到基础模型（如DINOv2）在大规模无标签数据上预训练的潜力，但识别到这些模型本质上是2D的，无法直接处理3D医学影像。作者借鉴了注意力池化在高维医学图像分析中的有效性、监督对比学习在处理不平衡分类问题上的进展（如SC-MIL）以及DINOv2作为特征提取器的泛化能力，设计了结合注意力机制和基础模型的新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练的2D基础模型（DINOv2）处理3D医学影像的各个2D切片，通过软注意力机制自适应地聚合不同切片的特征，为诊断相关的切片分配更高权重，并结合监督对比学习和类方差正则化处理数据稀缺和类别不平衡问题。整体流程包括：1)将3D MRI分解为2D切片，用DINOv2提取每个切片的特征嵌入；2)使用两层MLN学习注意力分数，通过softmax转换为权重，计算加权聚合特征；3)将聚合特征通过MLP映射后进行分类；4)结合交叉熵损失、监督对比损失和类内方差损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全局注意力驱动的3D医学影像聚合框架，自适应融合切片级嵌入；2)针对数据稀缺和类别不平衡的组合损失函数；3)在两个真实临床队列（ADNI和头痛队列）上验证。相比之前的工作，不同之处在于：不同于传统MIL方法可能忽略跨切片病理特征，DinoAtten3D使用全局注意力聚合切片级特征；不同于直接应用2D基础模型到3D影像，通过注意力机制桥接2D能力和体积特性；不同于单一监督对比学习，结合类方差正则化专门针对医学影像数据稀缺和类别不平衡问题设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DinoAtten3D通过结合DINOv2基础模型的特征提取能力和切片级软注意力机制，为3D脑部MRI图像异常检测提供了一个高效、数据友好的解决方案，在数据有限和类别不平衡的情况下仍能保持强大的分类性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection and classification in medical imaging are critical forearly diagnosis but remain challenging due to limited annotated data, classimbalance, and the high cost of expert labeling. Emerging vision foundationmodels such as DINOv2, pretrained on extensive, unlabeled datasets, offergeneralized representations that can potentially alleviate these limitations.In this study, we propose an attention-based global aggregation frameworktailored specifically for 3D medical image anomaly classification. Leveragingthe self-supervised DINOv2 model as a pretrained feature extractor, our methodprocesses individual 2D axial slices of brain MRIs, assigning adaptiveslice-level importance weights through a soft attention mechanism. To furtheraddress data scarcity, we employ a composite loss function combining supervisedcontrastive learning with class-variance regularization, enhancing inter-classseparability and intra-class consistency. We validate our framework on the ADNIdataset and an institutional multi-class headache cohort, demonstrating stronganomaly classification performance despite limited data availability andsignificant class imbalance. Our results highlight the efficacy of utilizingpretrained 2D foundation models combined with attention-based slice aggregationfor robust volumetric anomaly detection in medical imaging. Our implementationis publicly available at https://github.com/Rafsani/DinoAtten3D.git.</description>
      <author>example@mail.com (Fazle Rafsani, Jay Shah, Catherine D. Chong, Todd J. Schwedt, Teresa Wu)</author>
      <guid isPermaLink="false">2509.12512v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model</title>
      <link>http://arxiv.org/abs/2509.12497v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作比较了基础模型与传统方法在神经科学中的时间序列预测和因果发现能力，发现基础模型在零样本设置下表现良好，能够更精确地检测因果相互作用。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测和因果发现在神经科学中处于核心地位，因为预测大脑活动并识别神经群体和回路之间的因果关系可以阐明认知和疾病背后的机制。随着基础模型的兴起，一个开放性问题是如何它们与传统方法在脑信号预测和因果分析方面的比较，以及它们是否可以在零样本设置中应用。&lt;h4&gt;目的&lt;/h4&gt;评估一个基础模型与传统方法在推断人类功能性磁共振成像（fMRI）测量的自发性大脑活动中的方向性相互作用方面的比较。&lt;h4&gt;方法&lt;/h4&gt;研究人员测试了基础模型在零样本和微调设置中的预测能力，通过将模型中的类Granger估计与标准Granger因果性进行比较来评估因果性，并使用从真实因果模型生成的合成时间序列（包括逻辑映射耦合和Ornstein-Uhlenbeck过程）验证了该方法。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在零样本预测fMRI时间序列方面具有竞争力（对照组平均绝对百分误差为0.55，患者组为0.27）；尽管标准Granger因果性没有显示出模型之间的明显定量差异，但基础模型提供了更精确的因果相互作用检测。&lt;h4&gt;结论&lt;/h4&gt;总的来说，这些发现表明基础模型提供了多功能性、强大的零样本性能，以及在时间序列数据中进行预测和因果发现的潜在效用。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测和因果发现在神经科学中处于核心地位，因为预测大脑活动并识别神经群体和回路之间的因果关系可以阐明认知和疾病背后的机制。随着基础模型的兴起，一个开放性问题是如何它们与传统方法在脑信号预测和因果分析方面的比较，以及它们是否可以在零样本设置中应用。在这项工作中，我们评估了一个基础模型与传统方法在推断人类功能性磁共振成像（fMRI）测量的自发性大脑活动中的方向性相互作用方面的比较。传统方法通常依赖于Wiener-Granger因果性。我们测试了基础模型在零样本和微调设置中的预测能力，并通过将模型中的类Granger估计与标准Granger因果性比较来评估因果性。我们使用从真实因果模型生成的合成时间序列（包括逻辑映射耦合和Ornstein-Uhlenbeck过程）验证了该方法。基础模型在零样本预测fMRI时间序列方面具有竞争力（对照组平均绝对百分误差为0.55，患者组为0.27）。尽管标准Granger因果性没有显示出模型之间的明显定量差异，但基础模型提供了更精确的因果相互作用检测。总的来说，这些发现表明基础模型提供了多功能性、强大的零样本性能，以及在时间序列数据中进行预测和因果发现的潜在效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-series forecasting and causal discovery are central in neuroscience, aspredicting brain activity and identifying causal relationships between neuralpopulations and circuits can shed light on the mechanisms underlying cognitionand disease. With the rise of foundation models, an open question is how theycompare to traditional methods for brain signal forecasting and causalityanalysis, and whether they can be applied in a zero-shot setting. In this work,we evaluate a foundation model against classical methods for inferringdirectional interactions from spontaneous brain activity measured withfunctional magnetic resonance imaging (fMRI) in humans. Traditional approachesoften rely on Wiener-Granger causality. We tested the forecasting ability ofthe foundation model in both zero-shot and fine-tuned settings, and assessedcausality by comparing Granger-like estimates from the model with standardGranger causality. We validated the approach using synthetic time seriesgenerated from ground-truth causal models, including logistic map coupling andOrnstein-Uhlenbeck processes. The foundation model achieved competitivezero-shot forecasting fMRI time series (mean absolute percentage error of 0.55in controls and 0.27 in patients). Although standard Granger causality did notshow clear quantitative differences between models, the foundation modelprovided a more precise detection of causal interactions.  Overall, these findings suggest that foundation models offer versatility,strong zero-shot performance, and potential utility for forecasting and causaldiscovery in time-series data.</description>
      <author>example@mail.com (Alessandro Crimi, Andrea Brovelli)</author>
      <guid isPermaLink="false">2509.12497v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Towards Foundational Models for Single-Chip Radar</title>
      <link>http://arxiv.org/abs/2509.12482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文收集了最大的原始雷达数据集(100万个样本，29小时)，训练了通用雷达变换器(GRT)基础模型，用于4D单芯片雷达的3D占用预测和语义分割任务，实现了通常需要更高分辨率传感器才能达到的质量水平。&lt;h4&gt;背景&lt;/h4&gt;毫米波雷达是紧凑、廉价、耐用的传感器，对遮挡和环境条件(如天气和黑暗)具有鲁棒性，但角度分辨率较差，特别是廉价的单芯片雷达。虽然已有基于学习的方法来缓解这一弱点，但毫米波雷达缺乏标准的基础模型或大型数据集，实践者通常使用较小的数据集从头训练特定任务模型。&lt;h4&gt;目的&lt;/h4&gt;收集大型原始雷达数据集，训练适用于4D单芯片雷达的基础模型，实现高质量的3D占用预测和语义分割。&lt;h4&gt;方法&lt;/h4&gt;收集100万个样本(29小时)的原始雷达数据集，开发Generalizable Radar Transformer (GRT)模型，进行广泛的消融实验，评估原始雷达数据与有损表示的性能差异。&lt;h4&gt;主要发现&lt;/h4&gt;GRT能在不同环境中泛化，可针对不同任务微调，数据呈对数缩放比例(每增加10倍数据，性能提高20%)；使用原始雷达数据显著优于有损表示，相当于增加10倍训练数据的效果；估计需要约1亿个样本(3000小时)的数据才能完全发挥GRT潜力。&lt;h4&gt;结论&lt;/h4&gt;通过大型数据集和GRT模型解决了毫米波雷达角度分辨率差的问题，原始数据比有损表示更有效，GRT具有良好的泛化能力和可微调性，未来需要更多数据进一步发挥模型潜力。&lt;h4&gt;翻译&lt;/h4&gt;毫米波雷达是紧凑、廉价且耐用的传感器，对遮挡具有鲁棒性，并且在天气和黑暗等环境条件下仍能正常工作。然而，这以较差的角度分辨率为代价，特别是对于通常用于汽车和室内感应应用的廉价单芯片雷达。尽管许多人提出了基于学习的方法来缓解这一弱点，但毫米波雷达尚未出现标准的基础模型或大型数据集，实践者主要使用相对较小的数据集从头开始训练特定任务模型。在本文中，我们收集了(据我们所知)最大的可用原始雷达数据集，包含100万个样本(29小时)，并训练了一个用于4D单芯片雷达的基础模型，该模型可以预测3D占用和进行语义分割，其质量通常只有在使用更高分辨率传感器时才能实现。我们证明了我们的通用雷达变换器(GRT)能够在不同环境中泛化，可以针对不同任务进行微调，并显示出每增加10倍数据性能提升20%的对数数据缩放比例。我们还对常见设计决策进行了广泛的消融实验，发现使用原始雷达数据显著优于广泛使用的有损表示，相当于增加10倍训练数据的效果。最后，我们大致估计需要约1亿个样本(3000小时)的数据才能完全发挥GRT的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单芯片毫米波雷达缺乏基础模型和大规模数据集的问题。毫米波雷达虽然体积小、成本低、耐用且不受环境条件影响，但角度分辨率差，限制了其在自动驾驶、室内感知等领域的应用。目前实践者大多使用小数据集从头训练特定任务模型，效率低下。缺乏基础模型阻碍了雷达感知技术的发展，因此这个问题对推动雷达技术在自动驾驶和机器人领域的应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到毫米波雷达处理流程中过度依赖有损点云表示和缺乏大规模数据集的瓶颈。他们设计了一个便携式的多模态数据收集系统'red-rover'，在不同场景收集原始雷达数据。在模型设计上，借鉴了计算机视觉中的Transformer架构和Perceiver I/O的查询机制，将其适配到4D雷达数据处理。作者参考了雷达处理领域的4D FFT技术，以及现有雷达感知任务定义，但创新性地将其应用于基础模型训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模原始雷达数据训练通用Transformer模型，直接从4D雷达数据立方体中学习特征，而非依赖传统的有损点云表示。整体流程包括：1)使用便携式系统收集多模态数据(雷达、激光雷达、相机)；2)将原始I/Q数据通过4D FFT转换为4D数据立方体；3)将数据分割成补丁并添加位置编码；4)使用Transformer编码器-解码器架构处理数据；5)训练基础模型(3D占用分类)并扩展到其他任务；6)评估模型在不同场景的泛化能力和微调性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了目前最大的原始毫米波雷达数据集I/Q-1M(29小时/100万帧)；2)证明使用原始4D数据比传统点云表示效果更好，相当于10倍以上训练数据量；3)设计了通用雷达Transformer(GRT)架构，能生成高质量3D占用图和语义分割；4)系统研究了雷达Transformer的扩展规律。相比之前工作，本研究数据规模大得多(比最大公开数据集大8倍)，直接使用原始数据而非点云，首次系统将Transformer应用于雷达基础模型，并实现了3D而非仅2D的复杂任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建大规模原始毫米波雷达数据集并训练通用雷达Transformer，证明了数据规模对提升单芯片雷达感知能力的关键作用，为雷达领域的基础模型发展奠定了重要基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; mmWave radars are compact, inexpensive, and durable sensors that are robustto occlusions and work regardless of environmental conditions, such as weatherand darkness. However, this comes at the cost of poor angular resolution,especially for inexpensive single-chip radars, which are typically used inautomotive and indoor sensing applications. Although many have proposedlearning-based methods to mitigate this weakness, no standardized foundationalmodels or large datasets for the mmWave radar have emerged, and practitionershave largely trained task-specific models from scratch using relatively smalldatasets.  In this paper, we collect (to our knowledge) the largest available raw radardataset with 1M samples (29 hours) and train a foundational model for 4Dsingle-chip radar, which can predict 3D occupancy and semantic segmentationwith quality that is typically only possible with much higher resolutionsensors. We demonstrate that our Generalizable Radar Transformer (GRT)generalizes across diverse settings, can be fine-tuned for different tasks, andshows logarithmic data scaling of 20\% per $10\times$ data. We also runextensive ablations on common design decisions, and find that using raw radardata significantly outperforms widely-used lossy representations, equivalent toa $10\times$ increase in training data. Finally, we roughly estimate that$\approx$100M samples (3000 hours) of data are required to fully exploit thepotential of GRT.</description>
      <author>example@mail.com (Tianshu Huang, Akarsh Prabhakara, Chuhan Chen, Jay Karhade, Deva Ramanan, Matthew O'Toole, Anthony Rowe)</author>
      <guid isPermaLink="false">2509.12482v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Prompt Management in GitHub Repositories: A Call for Best Practices</title>
      <link>http://arxiv.org/abs/2509.12421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究对开源提示进行了实证分析，发现了管理挑战并提供了改进建议。&lt;h4&gt;背景&lt;/h4&gt;基础模型（如大型语言模型）的快速采用引发了promptware（使用自然语言提示构建的软件）的发展，有效管理提示变得至关重要但具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;调查提示管理实践和质量属性，以识别关键挑战并提供改进建议。&lt;h4&gt;方法&lt;/h4&gt;对来自92个GitHub仓库的24,800个开源提示进行实证分析。&lt;h4&gt;主要发现&lt;/h4&gt;提示格式化存在相当大的不一致性、内部和外部提示大量重复，以及频繁出现的可读性和拼写问题。&lt;h4&gt;结论&lt;/h4&gt;开发者需要采取措施提高开源提示的可用性和可维护性，以应对promptware生态系统中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;基础模型（如大型语言模型）的快速采用引发了promptware的发展，即使用自然语言提示构建的软件。有效管理提示，如组织和质量保证，虽然至关重要但具有挑战性。在本研究中，我们对来自92个GitHub仓库的24,800个开源提示进行了实证分析，以调查提示管理实践和质量属性。我们的发现揭示了关键挑战，如提示格式化存在相当大的不一致性、内部和外部提示大量重复，以及频繁出现的可读性和拼写问题。基于这些发现，我们为开发者提供了可操作的建议，以在不断发展的promptware生态系统中提高开源提示的可用性和可维护性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid adoption of foundation models (e.g., large language models) hasgiven rise to promptware, i.e., software built using natural language prompts.Effective management of prompts, such as organization and quality assurance, isessential yet challenging. In this study, we perform an empirical analysis of24,800 open-source prompts from 92 GitHub repositories to investigate promptmanagement practices and quality attributes. Our findings reveal criticalchallenges such as considerable inconsistencies in prompt formatting,substantial internal and external prompt duplication, and frequent readabilityand spelling issues. Based on these findings, we provide actionablerecommendations for developers to enhance the usability and maintainability ofopen-source prompts within the rapidly evolving promptware ecosystem.</description>
      <author>example@mail.com (Hao Li, Hicham Masri, Filipe R. Cogo, Abdul Ali Bangash, Bram Adams, Ahmed E. Hassan)</author>
      <guid isPermaLink="false">2509.12421v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking</title>
      <link>http://arxiv.org/abs/2509.12913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为T-SiamTPN的新型空中目标跟踪方法，通过引入时间感知的Siamese框架解决了现有跟踪器在处理尺度变化、动态背景、杂乱和频繁遮挡等问题时的局限性。该方法通过时间特征融合和基于注意力的交互显著提高了跟踪性能，同时保持计算效率，适合在资源受限的嵌入式设备上实时运行。&lt;h4&gt;背景&lt;/h4&gt;空中目标跟踪具有挑战性，因为存在尺度变化、动态背景、杂乱和频繁遮挡等问题。现有跟踪器大多强调空间线索，但忽略了时间依赖性，导致长期跟踪和遮挡情况下的鲁棒性有限。此外，基于相关的Siamese跟踪器受限于相关操作的线性本质，对复杂、非线性的外观变化效果不佳。&lt;h4&gt;目的&lt;/h4&gt;解决现有跟踪器的局限性，特别是时间依赖性建模不足的问题；提高跟踪器在长期跟踪和遮挡情况下的鲁棒性；处理复杂、非线性的外观变化。&lt;h4&gt;方法&lt;/h4&gt;提出T-SiamTPN，一个具有时间感知的Siamese跟踪框架；通过显式的时间建模扩展了SiamTPN架构；结合时间特征融合和基于注意力的交互，增强时间一致性并实现更丰富的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;T-SiamTPN相比基线有显著改进，性能与最先进的跟踪器相当；尽管增加了时间模块，T-SiamTPN仍保持计算效率；在资源受限的Jetson Nano上，跟踪器实时运行，达到7.1 FPS，适合嵌入式应用；与基线相比，T-SiamTPN将成功率提高了13.7%，精度提高了14.7%。&lt;h4&gt;结论&lt;/h4&gt;时间建模在Siamese跟踪框架中很重要；T-SiamTPN是空中目标跟踪的一个强大且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;空中目标跟踪由于尺度变化、动态背景、杂乱和频繁遮挡而仍然是一项具有挑战性的任务。虽然大多数现有跟踪器强调空间线索，但它们经常忽略时间依赖性，导致在长期跟踪和遮挡情况下鲁棒性有限。此外，基于相关的Siamese跟踪器本质上受限于相关操作的线性特性，使其对复杂、非线性的外观变化无效。为解决这些局限性，我们引入了T-SiamTPN，这是一个具有时间感知的Siamese跟踪框架，通过显式的时间建模扩展了SiamTPN架构。我们的方法结合了时间特征融合和基于注意力的交互，增强了时间一致性并实现了更丰富的特征表示。这些增强相比基线带来了显著改进，并实现了与最先进跟踪器相当的性能。关键的是，尽管增加了时间模块，T-SiamTPN仍保持了计算效率。在资源受限的Jetson Nano上部署时，跟踪器以7.1 FPS的帧率实时运行，展示了其适合实际嵌入式应用的适用性，而没有明显的运行时开销。实验结果突出了显著的增益：与基线相比，T-SiamTPN将成功率提高了13.7%，精度提高了14.7%。这些发现强调了时间建模在Siamese跟踪框架中的重要性，并将T-SiamTPN确立为空中目标跟踪的一个强大且高效的解决方案。代码可在以下网址获取：https://github.com/to/be/released&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerial object tracking remains a challenging task due to scale variations,dynamic backgrounds, clutter, and frequent occlusions. While most existingtrackers emphasize spatial cues, they often overlook temporal dependencies,resulting in limited robustness in long-term tracking and under occlusion.Furthermore, correlation-based Siamese trackers are inherently constrained bythe linear nature of correlation operations, making them ineffective againstcomplex, non-linear appearance changes. To address these limitations, weintroduce T-SiamTPN, a temporal-aware Siamese tracking framework that extendsthe SiamTPN architecture with explicit temporal modeling. Our approachincorporates temporal feature fusion and attention-based interactions,strengthening temporal consistency and enabling richer feature representations.These enhancements yield significant improvements over the baseline and achieveperformance competitive with state-of-the-art trackers. Crucially, despite theadded temporal modules, T-SiamTPN preserves computational efficiency. Deployedon the resource-constrained Jetson Nano, the tracker runs in real time at 7.1FPS, demonstrating its suitability for real-world embedded applications withoutnotable runtime overhead. Experimental results highlight substantial gains:compared to the baseline, T-SiamTPN improves success rate by 13.7% andprecision by 14.7%. These findings underscore the importance of temporalmodeling in Siamese tracking frameworks and establish T-SiamTPN as a strong andefficient solution for aerial object tracking. Code is available at:https://github.com/to/be/released</description>
      <author>example@mail.com (Hojat Ardi, Amir Jahanshahi, Ali Diba)</author>
      <guid isPermaLink="false">2509.12913v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>HistoryBankQA: Multilingual Temporal Question Answering on Historical Events</title>
      <link>http://arxiv.org/abs/2509.12720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HistoryBank，一个包含1000多万个历史事件的多语言数据库，以及一个全面的问答基准，用于评估大型语言模型在历史事件时间推理方面的能力。&lt;h4&gt;背景&lt;/h4&gt;时间推理对于自然语言处理任务至关重要，但现有的大型语言模型在时间推理方面的基准测试有限。现有的时间推理数据集规模有限，缺乏多语言覆盖，并且更关注当代事件而非历史事件。&lt;h4&gt;目的&lt;/h4&gt;解决现有时间推理数据集的局限性，提供一个大规模、多语言的历史事件数据库，并构建一个全面的问答基准来评估语言模型在时间推理任务上的表现。&lt;h4&gt;方法&lt;/h4&gt;从维基百科的时间线页面和文章信息框中提取了1000多万个历史事件，构建了一个包含10种语言的数据库。此外，还构建了一个包含6种时间问答推理任务的全面基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;评估了多种流行的语言模型（LLaMA-3-8B、Mistral-7B、Gemma-2-9b、Qwen3-8B、GPT4o）在这些任务上的性能。GPT4o在所有答案类型和语言中表现最佳；Gemma-2-9b优于其他小型语言模型。&lt;h4&gt;结论&lt;/h4&gt;这项工作旨在为推进对历史事件的多语言和时间感知自然语言理解提供全面资源。为了促进进一步研究，将在论文接受后公开代码和数据集。&lt;h4&gt;翻译&lt;/h4&gt;关于历史事件的时间推理是自然语言处理任务的关键技能，如事件提取、历史实体链接、时间问答、时间线总结、时间事件聚类和时间自然语言推理。然而，对大型语言模型时间推理能力的基准测试工作相当有限。现有的时间推理数据集在规模上有限，缺乏多语言覆盖，并且更关注当代事件。为了解决这些局限性，我们提出了HistoryBank，一个从维基百科时间线页面和文章信息框中提取的1000多万个历史事件的多语言数据库。我们的数据库在历史深度和语言广度上提供了前所未有的覆盖，包含10种语言。此外，我们构建了一个全面的跨所有语言的问答基准，用于时间推理。该基准涵盖了多样化的6种时间问答推理任务集，我们评估了一系列流行的语言模型（LLaMA-3-8B、Mistral-7B、Gemma-2-9b、Qwen3-8B、GPT4o）以评估它们在这些任务上的性能。正如预期的那样，GPT4o在所有答案类型和语言中表现最佳；Gemma-2优于其他小型语言模型。我们的工作旨在为推进对历史事件的多语言和时间感知自然语言理解提供全面资源。为了促进进一步研究，我们将在论文接受后公开我们的代码和数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning about historical events is a critical skill for NLP taskslike event extraction, historical entity linking, temporal question answering,timeline summarization, temporal event clustering and temporal natural languageinference. Yet efforts on benchmarking temporal reasoning capabilities of largelanguage models (LLMs) are rather limited. Existing temporal reasoning datasetsare limited in scale, lack multilingual coverage and focus more on contemporaryevents. To address these limitations, we present HistoryBank, a multilingualdatabase of 10M+ historical events extracted from Wikipedia timeline pages andarticle infoboxes. Our database provides unprecedented coverage in bothhistorical depth and linguistic breadth with 10 languages. Additionally, weconstruct a comprehensive question answering benchmark for temporal reasoningacross all languages. This benchmark covers a diverse set of 6 temporal QAreasoning tasks, and we evaluate a suite of popular language models(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess theirperformance on these tasks. As expected GPT4o performs best across all answertypes and languages; Gemma-2 outperforms the other small language models. Ourwork aims to provide a comprehensive resource for advancing multilingual andtemporally-aware natural language understanding of historical events. Tofacilitate further research, we will make our code and datasets publiclyavailable upon acceptance of this paper.</description>
      <author>example@mail.com (Biswadip Mandal, Anant Khandelwal, Manish Gupta)</author>
      <guid isPermaLink="false">2509.12720v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.12269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MT-DQN模型，结合了Transformer、时间图神经网络和深度Q网络，用于解决短视频环境中的用户行为预测和推荐策略优化问题。实验证明该模型在多个评估指标上均优于传统模型。&lt;h4&gt;背景&lt;/h4&gt;短视频环境中存在用户行为预测困难和推荐策略优化挑战的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效预测用户行为并优化推荐策略的模型，以应对短视频环境的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出MT-DQN模型，该模型集成了Transformer、时间图神经网络(TGNN)和深度Q网络(DQN)。&lt;h4&gt;主要发现&lt;/h4&gt;1) MT-DQN比传统连接模型(如Concat-Modal)表现更好，平均F1分数提高10.97%，平均NDCG@5提高8.3%；2) 与经典强化学习模型Vanilla-DQN相比，MT-DQN的MSE降低34.8%，MAE降低26.5%。&lt;h4&gt;结论&lt;/h4&gt;MT-DQN模型在短视频推荐领域具有显著优势，但面临计算成本高和在线推理延迟敏感等现实部署挑战，这些挑战将通过未来的架构优化来解决。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了MT-DQN模型，该模型集成了Transformer、时间图神经网络(TGNN)和深度Q网络(DQN)，以解决短视频环境中预测用户行为和优化推荐策略的挑战。实验证明，MT-DQN持续优于传统的连接模型，如Concat-Modal，平均F1分数提高了10.97%，平均NDCG@5提高了8.3%。与经典的强化学习模型Vanilla-DQN相比，MT-DQN的MSE降低了34.8%，MAE降低了26.5%。尽管如此，我们也认识到在现实场景中部署MT-DQN面临的挑战，如计算成本和在线推理时的延迟敏感性，这些问题将通过未来的架构优化来解决。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes the MT-DQN model, which integrates a Transformer,Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address thechallenges of predicting user behavior and optimizing recommendation strategiesin short-video environments. Experiments demonstrated that MT-DQN consistentlyoutperforms traditional concatenated models, such as Concat-Modal, achieving anaverage F1-score improvement of 10.97% and an average NDCG@5 improvement of8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQNreduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognizechallenges in deploying MT-DQN in real-world scenarios, such as itscomputational cost and latency sensitivity during online inference, which willbe addressed through future architectural optimization.</description>
      <author>example@mail.com (Jinmeiyang Wang, Jing Dong, Li Zhou)</author>
      <guid isPermaLink="false">2509.12269v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks</title>
      <link>http://arxiv.org/abs/2509.13266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为JANUS的双重约束隐蔽节点注入框架，通过局部和全局两个层面的优化，显著提高了图神经网络对抗攻击的隐蔽性和有效性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在各种应用中表现出色，但它们容易受到复杂的对抗攻击，特别是节点注入攻击。这些攻击的成功很大程度上依赖于它们的隐蔽性，即能够融入原始图并逃避检测的能力。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，这些方法往往依赖间接代理指标、忽略注入内容的基本特征或仅关注模仿局部结构，导致局部近视问题。&lt;h4&gt;方法&lt;/h4&gt;提出名为'Joint Alignment of Nodal and Universal Structures'(JANUS)的框架。在局部层面，引入局部特征流形对齐策略实现特征空间中的几何一致性；在全局层面，结合结构化潜在变量并最大化与生成结构的互信息，确保注入结构与原始图的语义模式一致。将注入攻击建模为顺序决策过程，通过强化学习代理进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在多个标准数据集上的实验表明，JANUS框架在攻击有效性和隐蔽性方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;JANUS框架通过同时考虑局部和全局结构，解决了现有节点注入攻击方法中的局部近视问题，实现了更隐蔽、更有效的攻击。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已在各种应用中展现出卓越的性能，但它们容易受到复杂的对抗攻击，特别是节点注入攻击。这类攻击的成功很大程度上依赖于其隐蔽性，即能够融入原始图并逃避检测的能力。然而，现有方法通常通过依赖间接代理指标来实现隐蔽性，缺乏对注入内容基本特征的考虑，或仅专注于模仿局部结构，这导致了局部近视问题。为了克服这些限制，我们提出了一种双重约束的隐蔽节点注入框架，称为'节点与通用结构联合对齐'(JANUS)。在局部层面，我们引入了局部特征流形对齐策略，以实现特征空间中的几何一致性。在全局层面，我们结合了结构化潜在变量，并最大化与生成结构的互信息，确保注入的结构与原始图的语义模式一致。我们将注入攻击建模为顺序决策过程，并通过强化学习代理进行优化。在多个标准数据集上的实验表明，JANUS框架在攻击有效性和隐蔽性方面显著优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable performance acrossvarious applications, yet they are vulnerable to sophisticated adversarialattacks, particularly node injection attacks. The success of such attacksheavily relies on their stealthiness, the ability to blend in with the originalgraph and evade detection. However, existing methods often achieve stealthinessby relying on indirect proxy metrics, lacking consideration for the fundamentalcharacteristics of the injected content, or focusing only on imitating localstructures, which leads to the problem of local myopia. To overcome theselimitations, we propose a dual-constraint stealthy node injection framework,called Joint Alignment of Nodal and Universal Structures (JANUS). At the locallevel, we introduce a local feature manifold alignment strategy to achievegeometric consistency in the feature space. At the global level, we incorporatestructured latent variables and maximize the mutual information with thegenerated structures, ensuring the injected structures are consistent with thesemantic patterns of the original graph. We model the injection attack as asequential decision process, which is optimized by a reinforcement learningagent. Experiments on multiple standard datasets demonstrate that the JANUSframework significantly outperforms existing methods in terms of both attackeffectiveness and stealthiness.</description>
      <author>example@mail.com (Jiahao Zhang, Xiaobing Pei, Zhaokun Zhong, Wenqiang Hao, Zhenghao Tang)</author>
      <guid isPermaLink="false">2509.13266v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges</title>
      <link>http://arxiv.org/abs/2509.13139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图异质性对消息传递图神经网络性能的影响，特别是图卷积网络等低通滤波器在异构图上的表现。通过添加自环和平行边修改图结构，观察图拉普拉斯矩阵特征值变化及GCN性能变化，建立了图频谱与低通滤波器性能之间的联系，提出了一种无需昂贵特征值分解即可评估图特性的方法。&lt;h4&gt;背景&lt;/h4&gt;图异质性对消息传递图神经网络性能构成了重大挑战。像图卷积网络这样的常见低通滤波器面临性能下降，这可以归因于来自不相似邻居节点的消息混合。低通滤波器在异构图上的性能仍需要深入分析。&lt;h4&gt;目的&lt;/h4&gt;研究图异质性对低通滤波器性能的影响，特别是GCN在异构图上的表现，并探索通过修改图结构（添加自环和平行边）来改善性能的可能性。&lt;h4&gt;方法&lt;/h4&gt;通过添加自环和平行边来更新异构图。观察图拉普拉斯矩阵的特征值如何随着自环和平行边数量的变化而变化。在添加自环或平行边的情况下，在各种基准异构网络上进行关于GCN性能的研究。&lt;h4&gt;主要发现&lt;/h4&gt;1) 随着自环数量的增加，图拉普拉斯矩阵的特征值减小；2) 随着平行边数量的增加，图拉普拉斯矩阵的特征值增大；3) GCN在添加自环和平行边后表现出性能增加或减少的趋势；4) 建立了图频谱与低通滤波器在异构图上性能趋势之间的联系；5) 图频谱描述了输入图的基本内在特性，如连通分量的存在、稀疏性、平均度、聚类结构等；6) 可以通过观察低通滤波器的性能趋势来评估图频谱和特性，而无需进行昂贵的特征值分解。&lt;h4&gt;结论&lt;/h4&gt;本研究工作能够通过观察低通滤波器的性能趋势来评估图频谱和特性，而不需要进行昂贵的特征值分解。理论基础也验证了添加自环和平行边对图频谱的影响。&lt;h4&gt;翻译&lt;/h4&gt;图异质性对消息传递图神经网络性能构成了严峻挑战。像图卷积网络这样的常见低通滤波器面临性能下降，这可以归因于来自不相似邻居节点的消息混合。低通滤波器在异构图上的性能仍需要深入分析。在此背景下，我们通过添加自环和平行边来更新异构图。我们观察到，随着自环数量的增加，图拉普拉斯矩阵的特征值减小；随着平行边数量的增加，图拉普拉斯矩阵的特征值增大。我们通过添加自环或平行边，在各种基准异构网络上进行了关于GCN性能的几项研究。研究表明，GCN在添加自环和平行边后表现出性能增加或减少的趋势。基于这些研究，我们建立了图频谱与低通滤波器在异构图上性能趋势之间的联系。图频谱描述了输入图的基本内在特性，如连通分量的存在、稀疏性、平均度、聚类结构等。我们的工作能够通过观察低通滤波器的性能趋势来无缝评估图频谱和特性，而无需进行昂贵的特征值分解。还讨论了理论基础，以验证添加自环和平行边对图频谱的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph heterophily poses a formidable challenge to the performance ofMessage-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filterslike Graph Convolutional Networks (GCNs) face performance degradation, whichcan be attributed to the blending of the messages from dissimilar neighboringnodes. The performance of the low-pass filters on heterophilic graphs stillrequires an in-depth analysis. In this context, we update the heterophilicgraphs by adding a number of self-loops and parallel edges. We observe thateigenvalues of the graph Laplacian decrease and increase respectively byincreasing the number of self-loops and parallel edges. We conduct severalstudies regarding the performance of GCN on various benchmark heterophilicnetworks by adding either self-loops or parallel edges. The studies reveal thatthe GCN exhibited either increasing or decreasing performance trends on addingself-loops and parallel edges. In light of the studies, we establishedconnections between the graph spectra and the performance trends of thelow-pass filters on the heterophilic graphs. The graph spectra characterize theessential intrinsic properties of the input graph like the presence ofconnected components, sparsity, average degree, cluster structures, etc. Ourwork is adept at seamlessly evaluating graph spectrum and properties byobserving the performance trends of the low-pass filters without pursuing thecostly eigenvalue decomposition. The theoretical foundations are also discussedto validate the impact of adding self-loops and parallel edges on the graphspectrum.</description>
      <author>example@mail.com (Kushal Bose, Swagatam Das)</author>
      <guid isPermaLink="false">2509.13139v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Curriculum Learning for Mesh-based simulations</title>
      <link>http://arxiv.org/abs/2509.13138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了一种从粗到细的课程学习方法，加速图神经网络在高分辨率非结构化网格上的训练过程，同时保持相当的准确性并减少训练时间。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为基于网格的计算流体动力学的强大替代模型，但在高分辨率非结构化网格上训练这些网络（包含数十万个节点）仍然极其昂贵。&lt;h4&gt;目的&lt;/h4&gt;研究一种从粗到细的课程学习方法，以加速图神经网络在高分辨率非结构化网格上的训练过程。&lt;h4&gt;方法&lt;/h4&gt;首先在非常粗糙的网格上训练，然后逐步引入中等和高分辨率（最高可达30万个节点）的网格。与多尺度GNN架构不同，模型本身保持不变，只有训练数据的保真度随时间变化。&lt;h4&gt;主要发现&lt;/h4&gt;使用这种方法可以实现相当的泛化准确性，同时将总墙上时钟时间减少高达50%；在模型缺乏学习底层物理能力的情况下，课程学习可以帮助模型突破性能瓶颈。&lt;h4&gt;结论&lt;/h4&gt;从粗到细的课程学习可以显著加速图神经网络在高分辨率非结构化网格上的训练，同时保持相当的准确性，并且在某些情况下还能帮助模型突破性能瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为基于网格的计算流体动力学的强大替代模型，但在高分辨率非结构化网格上训练它们（包含数十万个节点）仍然极其昂贵。我们研究了一种从粗到细的课程学习，通过首先在非常粗糙的网格上训练，然后逐步引入中等和高分辨率（最高达30万个节点）来加速收敛。与多尺度GNN架构不同，模型本身保持不变；只有训练数据的保真度随时间变化。我们在减少高达50%的总墙上时钟时间的同时，实现了相当的泛化准确性。此外，在我们的模型缺乏学习底层物理能力的数据集上，使用课程学习能够使其突破性能瓶颈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as powerful surrogates formesh-based computational fluid dynamics (CFD), but training them onhigh-resolution unstructured meshes with hundreds of thousands of nodes remainsprohibitively expensive. We study a \emph{coarse-to-fine curriculum} thataccelerates convergence by first training on very coarse meshes and thenprogressively introducing medium and high resolutions (up to \(3\times10^5\)nodes). Unlike multiscale GNN architectures, the model itself is unchanged;only the fidelity of the training data varies over time. We achieve comparablegeneralization accuracy while reducing total wall-clock time by up to 50\%.Furthermore, on datasets where our model lacks the capacity to learn theunderlying physics, using curriculum learning enables it to break throughplateaus.</description>
      <author>example@mail.com (Paul Garnier, Vincent Lannelongue, Elie Hachem)</author>
      <guid isPermaLink="false">2509.13138v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories</title>
      <link>http://arxiv.org/abs/2509.12953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种结合神经常微分方程、图神经网络和神经过程的概率框架，用于从稀疏观测中建模心脏运动的时空动力学特性。&lt;h4&gt;背景&lt;/h4&gt;心脏运动的建模需要考虑时空结构、稀疏观测和不确定性，传统方法可能难以捕捉这些复杂特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从稀疏观测中重建和预测心脏运动的统一模型，同时捕捉不确定性、时间连续性和解剖结构。&lt;h4&gt;方法&lt;/h4&gt;将动态系统表示为时空多路复用图，使用GNN参数化的向量场建模潜在轨迹，并通过神经过程推断初始状态和控制变量的分布。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够在合成动态系统和真实心脏成像数据集上准确重建轨迹并外推未来周期，在ACDC分类任务上达到99%准确率，在UK Biobank中检测心房颤动达到67%准确率。&lt;h4&gt;结论&lt;/h4&gt;该框架为心脏运动分析提供了灵活的方法，并为结构化生物医学时空时间序列数据中的基于图的学习奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于从稀疏观测中建模结构化时空动力学的概率框架，专注于心脏运动。我们的方法将神经常微分方程、图神经网络和神经过程整合到一个统一模型中，该模型捕捉不确定性、时间连续性和解剖结构。我们将动态系统表示为时空多路复用图，并使用GNN参数化的向量场建模其潜在轨迹。给定节点和边缘级别的稀疏上下文观测，模型推断潜在初始状态和控制变量的分布，从而实现轨迹的内插和外推。我们在三个合成动态系统和两个真实世界的心脏成像数据集上验证了该方法，展示了精确重建、外推和疾病分类能力。模型能从单个观测周期准确重建轨迹并外推未来心脏周期。在ACDC分类任务上达到最先进的结果，并在UK Biobank受试者中检测心房颤动，具有竞争力的性能。这项工作为分析心脏运动提供了灵活的方法，并为结构化生物医学时空时间序列数据中的基于图的学习奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a probabilistic framework for modeling structured spatiotemporaldynamics from sparse observations, focusing on cardiac motion. Our approachintegrates neural ordinary differential equations (NODEs), graph neuralnetworks (GNNs), and neural processes into a unified model that capturesuncertainty, temporal continuity, and anatomical structure. We representdynamic systems as spatiotemporal multiplex graphs and model their latenttrajectories using a GNN-parameterized vector field. Given the sparse contextobservations at node and edge levels, the model infers a distribution overlatent initial states and control variables, enabling both interpolation andextrapolation of trajectories. We validate the method on three syntheticdynamical systems (coupled pendulum, Lorenz attractor, and Kuramotooscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UKBiobank (N=526) - demonstrating accurate reconstruction, extrapolation, anddisease classification capabilities. The model accurately reconstructstrajectories and extrapolates future cardiac cycles from a single observedcycle. It achieves state-of-the-art results on the ACDC classification task (upto 99% accuracy), and detects atrial fibrillation in UK Biobank subjects withcompetitive performance (up to 67% accuracy). This work introduces a flexibleapproach for analyzing cardiac motion and offers a foundation for graph-basedlearning in structured biomedical spatiotemporal time-series data.</description>
      <author>example@mail.com (Jaume Banus, Augustin C. Ogier, Roger Hullin, Philippe Meyer, Ruud B. van Heeswijk, Jonas Richiardi)</author>
      <guid isPermaLink="false">2509.12953v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Explicit Multimodal Graph Modeling for Human-Object Interaction Detection</title>
      <link>http://arxiv.org/abs/2509.12554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MGNM方法，利用图神经网络的关系结构增强人类-物体交互检测性能&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的方法已成为人类-物体交互检测的主流方法，但该架构没有明确建模HOI检测中固有的关系结构，阻碍了交互识别&lt;h4&gt;目的&lt;/h4&gt;提出MGNM方法，利用基于GNN的关系结构增强HOI检测&lt;h4&gt;方法&lt;/h4&gt;设计多模态图网络框架，在四阶段图结构中明确建模HOI任务；引入多级特征交互机制，利用多级视觉和语言特征增强人类-物体对之间的信息传播&lt;h4&gt;主要发现&lt;/h4&gt;在HICO-DET和V-COCO基准测试上取得最先进性能；与先进物体检测器集成时展示显著性能提升，并在稀有和非稀有类别间保持平衡&lt;h4&gt;结论&lt;/h4&gt;MGNM方法能有效提升人类-物体交互检测的性能&lt;h4&gt;翻译&lt;/h4&gt;基于Transformer的方法最近已成为人类-物体交互检测的主流方法。然而，Transformer架构没有明确建模HOI检测中固有的关系结构，这阻碍了交互的识别。相比之下，图神经网络(GNNs)本质上更适合这项任务，因为它们明确建模了人类-物体对之间的关系。因此，在本文中，我们提出了MGNM(多模态图网络建模)，利用基于GNN的关系结构来增强HOI检测。具体来说，我们设计了一个多模态图网络框架，在四阶段图结构中明确建模HOI任务。此外，我们在图网络中引入了多级特征交互机制。该机制利用多级视觉和语言特征来增强人类-物体对之间的信息传播。因此，我们提出的MGNM在两个广泛使用的基准测试上取得了最先进的性能：HICO-DET和V-COCO。此外，当与更先进的物体检测器集成时，我们的方法展示了显著的性能提升，并在稀有和非稀有类别之间保持了有效的平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformer-based methods have recently become the prevailing approach forHuman-Object Interaction (HOI) detection. However, the Transformer architecturedoes not explicitly model the relational structures inherent in HOI detection,which impedes the recognition of interactions. In contrast, Graph NeuralNetworks (GNNs) are inherently better suited for this task, as they explicitlymodel the relationships between human-object pairs. Therefore, in this paper,we propose \textbf{M}ultimodal \textbf{G}raph \textbf{N}etwork\textbf{M}odeling (MGNM) that leverages GNN-based relational structures toenhance HOI detection. Specifically, we design a multimodal graph networkframework that explicitly models the HOI task in a four-stage graph structure.Furthermore, we introduce a multi-level feature interaction mechanism withinour graph network. This mechanism leverages multi-level vision and languagefeatures to enhance information propagation across human-object pairs.Consequently, our proposed MGNM achieves state-of-the-art performance on twowidely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with amore advanced object detector, our method demonstrates a significantperformance gain and maintains an effective balance between rare and non-rareclasses.</description>
      <author>example@mail.com (Wenxuan Ji, Haichao Shi, Xiao-Yu zhang)</author>
      <guid isPermaLink="false">2509.12554v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Graph Homophily Booster: Rethinking the Role of Discrete Features on Heterophilic Graphs</title>
      <link>http://arxiv.org/abs/2509.12530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GRAPHITE的新框架，通过图转换直接增加图的同质性来解决图神经网络在异质性图上的表现不佳问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是建模图结构数据的强大工具，但现有GNN在处理异质性图(连接节点具有不同特征或标签)时表现不佳。现有方法主要集中在架构设计上，没有直接针对异质性问题的根本原因，甚至在某些异质性数据集上表现不如简单的多层感知机。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新方法来解决图异质性问题，超越架构设计的限制，直接通过图转换增加图的同质性。&lt;h4&gt;方法&lt;/h4&gt;提出名为GRAPHITE的框架，基于同质性的精确定义，创建特征节点来促进具有相似特征的节点之间的同质性消息传递，直接将异质性图转换为更具同质性的图。&lt;h4&gt;主要发现&lt;/h4&gt;理论和实证表明GRAPHITE显著增加了原始异质性图的同质性，同时仅略微增加图的大小；在具有挑战性的数据集上，GRAPHITE在异质性图上显著优于最先进方法，在同质性图上实现了与最先进方法相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;GRAPHITE是首个明确转换图以直接提高图同质性的方法，为解决图异质性问题提供了新范式。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为建模图结构数据的强大工具。然而，现有GNN通常难以处理异质性图，其中连接的节点往往具有不同的特征或标签。虽然已提出多种方法来解决这一挑战，但它们主要关注架构设计，而没有直接针对异质性问题的根本原因。这些方法在具有挑战性的异质性数据集上的表现甚至比最简单的多层感知机还差。例如，我们的实验显示，21种最新的GNN在Actor数据集上仍然落后于MLP。这一关键挑战需要一种超越架构设计的创新方法来解决图异质性。为了弥合这一差距，我们提出并研究了一种新的、未被探索的范式：通过精心设计的图转换直接增加图的同质性。在这项工作中，我们提出了一个简单而有效的框架GRAPHITE来解决图异质性问题。据我们所知，这项工作是首个明确转换图以直接提高图同质性的方法。源于同质性的精确定义，我们提出的GRAPHITE创建特征节点，以促进具有相似特征的节点之间的同质性消息传递。此外，我们从理论和实证上都证明，我们提出的GRAPHITE显著增加了原始异质性图的同质性，同时仅略微增加了图的大小。在具有挑战性的数据集上的大量实验表明，我们提出的GRAPHITE在异质性图上显著优于最先进的方法，同时在同质性图上实现了与最先进方法相当的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as a powerful tool for modelinggraph-structured data. However, existing GNNs often struggle with heterophilicgraphs, where connected nodes tend to have dissimilar features or labels. Whilenumerous methods have been proposed to address this challenge, they primarilyfocus on architectural designs without directly targeting the root cause of theheterophily problem. These approaches still perform even worse than thesimplest MLPs on challenging heterophilic datasets. For instance, ourexperiments show that 21 latest GNNs still fall behind the MLP on the Actordataset. This critical challenge calls for an innovative approach to addressinggraph heterophily beyond architectural designs. To bridge this gap, we proposeand study a new and unexplored paradigm: directly increasing the graphhomophily via a carefully designed graph transformation. In this work, wepresent a simple yet effective framework called GRAPHITE to address graphheterophily. To the best of our knowledge, this work is the first method thatexplicitly transforms the graph to directly improve the graph homophily.Stemmed from the exact definition of homophily, our proposed GRAPHITE createsfeature nodes to facilitate homophilic message passing between nodes that sharesimilar features. Furthermore, we both theoretically and empirically show thatour proposed GRAPHITE significantly increases the homophily of originallyheterophilic graphs, with only a slight increase in the graph size. Extensiveexperiments on challenging datasets demonstrate that our proposed GRAPHITEsignificantly outperforms state-of-the-art methods on heterophilic graphs whileachieving comparable accuracy with state-of-the-art methods on homophilicgraphs.</description>
      <author>example@mail.com (Ruizhong Qiu, Ting-Wei Li, Gaotang Li, Hanghang Tong)</author>
      <guid isPermaLink="false">2509.12530v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions</title>
      <link>http://arxiv.org/abs/2509.12277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GraphDerm是一种融合图像、毫米级校准和患者元数据的群体图框架，用于多类皮肤镜分类，显著提高了黑色素瘤分诊的准确性。&lt;h4&gt;背景&lt;/h4&gt;皮肤镜辅助黑色素瘤分诊，但仅基于图像的AI通常忽略患者元数据（年龄、性别、部位）和进行几何分析所需的物理尺度信息。&lt;h4&gt;目的&lt;/h4&gt;开发GraphDerm，一个融合图像、毫米级校准和元数据的多类皮肤镜分类的群体图框架，首次将图神经网络(GNNs)应用于ISIC规模的皮肤镜分析。&lt;h4&gt;方法&lt;/h4&gt;整合ISIC 2018/2019数据集，合成带有精确掩膜的标尺嵌入图像，训练U-Nets进行病变和标尺分割，通过1D-CNN回归每毫米像素数，计算病变的真实尺度描述符，使用EfficientNet-B3作为节点特征，编码元数据/几何相似性作为边，采用谱GNN进行半监督分类。&lt;h4&gt;主要发现&lt;/h4&gt;标尺和病变分割达到Dice 0.904和0.908，尺度回归达到MAE 1.5像素，图分类达到AUC 0.9812，使用约25%边的阈值变体保留AUC 0.9788，远高于图像基线的0.9440，每类AUC通常在0.97-0.99范围内。&lt;h4&gt;结论&lt;/h4&gt;将校准尺度、病变几何形状和元数据统一到群体图中，比仅基于图像的流程有显著提升；更稀疏的图保留了接近最优的准确性，表明高效部署的可能性；具有尺度感知、基于图的AI是皮肤镜决策支持的 promising 方向。&lt;h4&gt;翻译&lt;/h4&gt;引言。皮肤镜辅助黑色素瘤分诊，但仅基于图像的AI通常忽略患者元数据（年龄、性别、部位）和进行几何分析所需的物理尺度。我们提出了GraphDerm，一个融合图像、毫米级校准和元数据的多类皮肤镜分类的群体图框架，据我们所知，这是首次将图神经网络应用于ISIC规模的皮肤镜。方法。我们整理了ISIC 2018/2019，合成了带有精确掩膜的标尺嵌入图像，并训练U-Nets(SE-ResNet-18)进行病变和标尺分割。通过轻量级1D-CNN从标尺掩膜两点相关性回归每毫米像素数。从病变掩膜我们计算真实尺度描述符（面积、周长、回转半径）。节点特征使用EfficientNet-B3；边编码元数据/几何相似性（全权重或阈值化）。谱GNN执行半监督节点分类；图像-only ANN作为基线。结果。标尺和病变分割达到Dice 0.904和0.908；尺度回归达到MAE 1.5像素（RMSE 6.6）。图达到AUC 0.9812，阈值变体使用约25%的边保留AUC 0.9788（对比图像基线的0.9440）；每类AUC通常在0.97-0.99范围内。结论。在ISIC-2019上，将校准尺度、病变几何形状和元数据统一到群体图中，比仅基于图像的流程有显著提升。更稀疏的图保留了接近最优的准确性，表明高效部署的可能性。具有尺度感知、基于图的AI是皮肤镜决策支持的 promising 方向；未来工作将改进学习的边语义并在更广泛的精选基准上评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Introduction. Dermoscopy aids melanoma triage, yet image-only AI oftenignores patient metadata (age, sex, site) and the physical scale needed forgeometric analysis. We present GraphDerm, a population-graph framework thatfuses imaging, millimeter-scale calibration, and metadata for multiclassdermoscopic classification, to the best of our knowledge the first ISIC-scaleapplication of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019,synthesize ruler-embedded images with exact masks, and train U-Nets(SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter areregressed from the ruler-mask two-point correlation via a lightweight 1D-CNN.From lesion masks we compute real-scale descriptors (area, perimeter, radius ofgyration). Node features use EfficientNet-B3; edges encode metadata/geometrysimilarity (fully weighted or thresholded). A spectral GNN performssemi-supervised node classification; an image-only ANN is the baseline.Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scaleregression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with athresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata ina population graph yields substantial gains over image-only pipelines onISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficientdeployment. Scale-aware, graph-based AI is a promising direction fordermoscopic decision support; future work will refine learned edge semanticsand evaluate on broader curated benchmarks.</description>
      <author>example@mail.com (Mehdi Yousefzadeh, Parsa Esfahanian, Sara Rashidifar, Hossein Salahshoor Gavalan, Negar Sadat Rafiee Tabatabaee, Saeid Gorgin, Dara Rahmati, Maryam Daneshpazhooh)</author>
      <guid isPermaLink="false">2509.12277v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Graph Tokenization for Behavior-Aware Generative Next POI Recommendation</title>
      <link>http://arxiv.org/abs/2509.12350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了KGTB方法，通过知识图谱标记化和多行为学习来解决现有POI推荐方法中信息损失和对用户移动理解不足的问题。&lt;h4&gt;背景&lt;/h4&gt;生成范式，特别是由大型语言模型(LLMs)驱动的，已成为解决下一个兴趣点(POI)推荐的新方案。现有研究通常采用两阶段流程，首先使用标记化器将POI转换为可由LLM处理的离散标识符，然后进行POI行为预测任务来指令微调LLM以实现下一个POI推荐。&lt;h4&gt;目的&lt;/h4&gt;解决现有POI推荐方法的两个局限性：(1)现有标记化器难以编码推荐数据中的异构信号，存在信息损失问题；(2)之前的指令微调任务只关注用户的POI访问行为，而忽略其他行为类型，导致对移动性的理解不足。&lt;h4&gt;方法&lt;/h4&gt;提出KGTB方法，具体包括：(1)将推荐数据组织为知识图谱(KG)格式，保留异构信息；(2)开发基于KG的标记化器，将每个节点量化为独立的结构ID，由KG结构监督，减少异构信息损失；(3)提出多行为学习，引入多种特定行为的预测任务（如POI、类别和区域访问行为）来微调LLM。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实城市数据集上的实验表明，KGTB具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;KGTB通过知识图谱标记化和多行为学习有效解决了现有POI推荐方法中的信息损失和对用户移动理解不足的问题，提升了推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;生成范式，特别是由大型语言模型(LLMs)驱动的，已成为解决下一个兴趣点(POI)推荐的新方案。开创性研究通常采用两阶段流程，首先使用标记化器将POI转换为可由LLM处理的离散标识符，然后进行POI行为预测任务来指令微调LLM以实现下一个POI推荐。尽管取得了显著进展，它们仍面临两个局限性：(1)现有标记化器难以编码推荐数据中的异构信号，存在信息损失问题；(2)之前的指令微调任务只关注用户的POI访问行为而忽略其他行为类型，导致对移动性的理解不足。为解决这些局限性，我们提出了KGTB（用于行为感知生成式下一个POI推荐的知识图谱标记化）。具体而言，KGTB将推荐数据组织为知识图谱(KG)格式，其结构可以无缝保留异构信息。然后，开发了一个基于KG的标记化器，将每个节点量化为独立的结构ID。此过程由KG结构监督，从而减少异构信息的损失。使用生成的ID，KGTB提出了多行为学习，为LLM微调引入多种特定行为的预测任务，例如POI、类别和区域访问行为。在这些行为任务上的学习为LLMs提供了对目标POI访问行为的全面洞察。在四个真实城市数据集上的实验证明了KGTB的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative paradigm, especially powered by Large Language Models (LLMs), hasemerged as a new solution to the next point-of-interest (POI) recommendation.Pioneering studies usually adopt a two-stage pipeline, starting with atokenizer converting POIs into discrete identifiers that can be processed byLLMs, followed by POI behavior prediction tasks to instruction-tune LLM fornext POI recommendation. Despite of remarkable progress, they still face twolimitations: (1) existing tokenizers struggle to encode heterogeneous signalsin the recommendation data, suffering from information loss issue, and (2)previous instruction-tuning tasks only focus on users' POI visit behavior whileignore other behavior types, resulting in insufficient understanding ofmobility. To address these limitations, we propose KGTB (Knowledge GraphTokenization for Behavior-aware generative next POI recommendation).Specifically, KGTB organizes the recommendation data in a knowledge graph (KG)format, of which the structure can seamlessly preserve the heterogeneousinformation. Then, a KG-based tokenizer is developed to quantize each node intoan individual structural ID. This process is supervised by the KG's structure,thus reducing the loss of heterogeneous information. Using generated IDs, KGTBproposes multi-behavior learning that introduces multiple behavior-specificprediction tasks for LLM fine-tuning, e.g., POI, category, and region visitbehaviors. Learning on these behavior tasks provides LLMs with comprehensiveinsights on the target POI visit behavior. Experiments on four real-world citydatasets demonstrate the superior performance of KGTB.</description>
      <author>example@mail.com (Ke Sun, Mayi Xu)</author>
      <guid isPermaLink="false">2509.12350v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive timbre representations for musical instrument and synthesizer retrieval</title>
      <link>http://arxiv.org/abs/2509.13285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于乐器检索的对比学习框架，能够使用单一模型直接查询乐器数据库，适用于单乐器和多乐器声音。&lt;h4&gt;背景&lt;/h4&gt;从音频混合物中高效检索特定乐器音色在数字音乐制作中仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理单乐器和多乐器声音的统一检索模型，解决现有音频数据增强方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出技术来为虚拟乐器（如采样器和合成器）生成逼真的正/负声音对，构建对比学习框架。&lt;h4&gt;主要发现&lt;/h4&gt;在单乐器检索实验中，对比学习方法与基于分类预训练的先前工作具有竞争力；在多乐器检索实验中，所提出的框架表现更优，对三种乐器混合物实现了81.7%的top-1准确率和95.7%的top-5准确率。&lt;h4&gt;结论&lt;/h4&gt;该对比学习框架在乐器检索任务中表现出色，特别是在多乐器检索方面优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;从音频混合物中高效检索特定乐器音色在数字音乐制作中仍然是一个挑战。本文介绍了一种用于乐器检索的对比学习框架，能够使用单一模型直接查询乐器数据库，适用于单乐器和多乐器声音。我们提出了为虚拟乐器（如采样器和合成器）生成逼真的正/负声音对的技术，解决了常见音频数据增强方法的局限性。第一个实验专注于从3,884种乐器的数据集中检索乐器，使用单乐器音频作为输入。对比方法与基于分类预训练的先前工作具有竞争力。第二个实验考虑使用乐器混合作为音频输入进行多乐器检索。在这种情况下，所提出的对比框架优于相关工作，对三种乐器混合物实现了81.7%的top-1准确率和95.7%的top-5准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficiently retrieving specific instrument timbres from audio mixturesremains a challenge in digital music production. This paper introduces acontrastive learning framework for musical instrument retrieval, enablingdirect querying of instrument databases using a single model for both single-and multi-instrument sounds. We propose techniques to generate realisticpositive/negative pairs of sounds for virtual musical instruments, such assamplers and synthesizers, addressing limitations in common audio dataaugmentation methods.  The first experiment focuses on instrument retrieval from a dataset of 3,884instruments, using single-instrument audio as input. Contrastive approaches arecompetitive with previous works based on classification pre-training. Thesecond experiment considers multi-instrument retrieval with a mixture ofinstruments as audio input. In this case, the proposed contrastive frameworkoutperforms related works, achieving 81.7\% top-1 and 95.7\% top-5 accuraciesfor three-instrument mixtures.</description>
      <author>example@mail.com (Gwendal Le Vaillant, Yannick Molle)</author>
      <guid isPermaLink="false">2509.13285v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Flow-Based Fragment Identification via Binding Site-Specific Latent Representations</title>
      <link>http://arxiv.org/abs/2509.13216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种蛋白质片段编码器，采用对比学习方法将分子片段和蛋白质表面映射到共享潜在空间，并提出LatentFrag方法用于虚拟筛选和生成设计，有效提高了基于片段的药物设计效率。&lt;h4&gt;背景&lt;/h4&gt;基于片段的药物设计是一种有前景的策略，利用小化学分子的结合来有效指导药物发现。然而，片段识别的初始步骤具有挑战性，因为片段通常结合较弱且非特异性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效识别和设计蛋白质-片段相互作用的方法，克服片段结合弱和非特异性的挑战，提高药物发现的效率。&lt;h4&gt;方法&lt;/h4&gt;研究人员开发了一个蛋白质片段编码器，采用对比学习方法将分子片段和蛋白质表面映射到共享的潜在空间。该方法能够捕获相互作用相关特征，并允许进行虚拟筛选以及生成设计。提出的LatentFrag方法根据蛋白质表面生成片段嵌入和位置，并在构造上保持化学真实性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 表达性片段和蛋白质表示能够以高灵敏度定位蛋白质-片段相互作用位点；2. 从学习的潜在片段嵌入分布中采样时，观察到最先进的片段回收率；3. 生成方法在计算成本仅为常用方法的一小部分的情况下，表现优于这些方法；4. 该方法为片段命中发现提供了有价值的起点。&lt;h4&gt;结论&lt;/h4&gt;这些方法共同推进了片段识别的发展，并为基于片段的药物发现提供了有价值的工具。研究人员进一步展示了LatentFrag的实际效用，并将工作流程扩展到完整的配体设计任务。&lt;h4&gt;翻译&lt;/h4&gt;基于片段的药物设计是一种有前景的策略，利用小化学分子的结合来有效指导药物发现。片段识别的初始步骤仍然具有挑战性，因为片段通常结合较弱且非特异性。我们开发了一个蛋白质片段编码器，它依赖于对比学习方法，将分子片段和蛋白质表面映射到共享的潜在空间。该编码器捕获相互作用相关特征，并允许使用我们的新方法LatentFrag进行虚拟筛选和生成设计。在LatentFrag中，根据蛋白质表面生成片段嵌入和位置，并在构造上保持化学真实性。我们表达性的片段和蛋白质表示能够以高灵敏度定位蛋白质-片段相互作用位点，并且当我们从学习的潜在片段嵌入分布中采样时，观察到最先进的片段回收率。我们的生成方法在计算成本仅为常用方法（如虚拟筛选）的一小部分的情况下，表现优于这些方法，为片段命中发现提供了有价值的起点。我们进一步展示了LatentFrag的实际效用，并将工作流程扩展到完整的配体设计任务。这些方法共同推进了片段识别的发展，并为基于片段的药物发现提供了有价值的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fragment-based drug design is a promising strategy leveraging the binding ofsmall chemical moieties that can efficiently guide drug discovery. The initialstep of fragment identification remains challenging, as fragments often bindweakly and non-specifically. We developed a protein-fragment encoder thatrelies on a contrastive learning approach to map both molecular fragments andprotein surfaces in a shared latent space. The encoder capturesinteraction-relevant features and allows to perform virtual screening as wellas generative design with our new method LatentFrag. In LatentFrag, fragmentembeddings and positions are generated conditioned on the protein surface whilebeing chemically realistic by construction. Our expressive fragment and proteinrepresentations allow location of protein-fragment interaction sites with highsensitivity and we observe state-of-the-art fragment recovery rates whensampling from the learned distribution of latent fragment embeddings. Ourgenerative method outperforms common methods such as virtual screening at afraction of its computational cost providing a valuable starting point forfragment hit discovery. We further show the practical utility of LatentFrag andextend the workflow to full ligand design tasks. Together, these approachescontribute to advancing fragment identification and provide valuable tools forfragment-based drug discovery.</description>
      <author>example@mail.com (Rebecca Manuela Neeser, Ilia Igashov, Arne Schneuing, Michael Bronstein, Philippe Schwaller, Bruno Correia)</author>
      <guid isPermaLink="false">2509.13216v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling</title>
      <link>http://arxiv.org/abs/2509.13084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpeted in Knowledge-Based Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于双网络架构的新型半监督3D医学图像分割框架，通过交叉一致性增强模块和动态加权策略解决伪标签噪声问题，并利用自监督对比学习减少预测不确定性。实验表明该方法在多个数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;监督医学图像分割模型虽然性能优异，但在实际应用中依赖大量标注数据不切实际。半监督学习方法通过伪标签生成利用未标注数据来缓解这一挑战，但现有方法仍受伪标签噪声和特征空间监督不足的影响。&lt;h4&gt;目的&lt;/h4&gt;解决现有半监督分割方法中存在的伪标签噪声和特征空间监督不足的问题，提出一种新颖的基于双网络架构的半监督3D医学图像分割框架。&lt;h4&gt;方法&lt;/h4&gt;提出交叉一致性增强模块使用交叉伪标签和熵过滤监督减少噪声伪标签；设计动态加权策略通过不确定性感知机制调整伪标签贡献；使用自监督对比学习机制区分可信和不确定预测，将不确定体素特征与可靠类别原型对齐以减少预测不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在Left Atrial、NIH Pancreas和BraTS-2019三个3D分割数据集上进行实验，结果表明所提方法在各种设置下均优于最先进方法（如在Left Atrial数据集上使用10%标注数据时达到89.95%的Dice分数），消融实验验证了各模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的新型半监督3D医学图像分割框架有效解决了伪标签噪声和特征空间监督不足的问题，实验结果证实了其在多种数据集和设置下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;尽管监督医学图像分割模型表现出色，但在实际情况中依赖大量标注数据是不切实际的。半监督学习方法旨在通过伪标签生成利用未标注数据来缓解这一挑战。然而，现有的半监督分割方法仍然受到噪声伪标签和特征空间内监督不足的影响。为了解决这些挑战，本文提出了一种基于双网络架构的新型半监督3D医学图像分割框架。具体来说，我们研究了一个交叉一致性增强模块，使用交叉伪标签和熵过滤监督来减少噪声伪标签，同时我们设计了一种动态加权策略，使用不确定性感知机制（即Kullback-Leibler散度）来调整伪标签的贡献。此外，我们使用自监督对比学习机制，通过有效区分可信和不确定的预测，将不确定的体素特征与可靠的类别原型对齐，从而减少预测不确定性。在三个3D分割数据集Left Atrial、NIH Pancreas和BraTS-2019上进行了大量实验。与最先进的方法相比，所提出的方法在各种设置下都表现出优越的性能（例如，在Left Atrial数据集上使用10%标注数据时达到89.95%的Dice分数）。此外，通过消融实验进一步验证了所提出模块的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决半监督医学图像分割中的两个关键挑战：噪声伪标签问题和特征空间监督不足。在现实中，医学图像分割需要大量像素级标注数据，而这些数据需要专家临床知识，获取成本高昂且耗时。半监督学习可以利用大量未标注数据缓解这一问题，但噪声伪标签会降低分割性能，而特征空间监督不足则限制了模型学习判别性表示的能力，这些问题限制了半监督方法在医学图像分割中的实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有半监督医学图像分割方法的局限性进行设计：发现交叉伪监督方法计算开销大，不确定性估计方法成本高，基于原型的方法往往单独应用效果有限。因此，作者借鉴了知识蒸馏框架但创新性地使用两个并行学生网络而非传统教师-学生架构。他们结合了交叉伪监督(CPS)和熵过滤监督(EFS)形成CCE模块，并引入不确定性感知机制和原型引导的对比学习，从而有效解决了现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用双网络架构相互验证和改进伪标签质量，通过熵过滤减少噪声影响，利用不确定性机制动态调整伪标签权重，并通过对比学习增强特征空间判别性。整体流程包括：1)构建两个并行的3D编码器-解码器子网络；2)对标记数据使用标准损失函数并加入一致性正则化；3)对未标记数据实施交叉伪监督、熵过滤、不确定性加权调整和原型引导的对比学习；4)整合监督损失、对比损失和半监督损失进行模型优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出交叉一致性增强(CCE)模块协同整合交叉伪监督和熵过滤监督；2)设计不确定性感知的加权机制动态调整伪标签贡献；3)整合基于原型的对比学习策略对齐不确定特征与可靠类原型。相比之前工作，该方法使用双并行学生网络而非传统教师-学生架构，将不确定性估计与原型引导相结合实现高效噪声识别，通过CCE模块有机结合多种策略，并使用自适应熵阈值而非固定阈值，在不同数据集上表现出更强的适应性和性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于双网络架构和不确定性引导伪标签的半监督医学图像分割方法，通过交叉一致性增强模块和原型引导的对比学习有效解决了噪声伪标签和特征空间监督不足的问题，在有限标注数据条件下实现了最先进的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.knosys.2025.114454&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the remarkable performance of supervised medical image segmentationmodels, relying on a large amount of labeled data is impractical in real-worldsituations. Semi-supervised learning approaches aim to alleviate this challengeusing unlabeled data through pseudo-label generation. Yet, existingsemi-supervised segmentation methods still suffer from noisy pseudo-labels andinsufficient supervision within the feature space. To solve these challenges,this paper proposes a novel semi-supervised 3D medical image segmentationframework based on a dual-network architecture. Specifically, we investigate aCross Consistency Enhancement module using both cross pseudo andentropy-filtered supervision to reduce the noisy pseudo-labels, while we designa dynamic weighting strategy to adjust the contributions of pseudo-labels usingan uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). Inaddition, we use a self-supervised contrastive learning mechanism to alignuncertain voxel features with reliable class prototypes by effectivelydifferentiating between trustworthy and uncertain predictions, thus reducingprediction uncertainty. Extensive experiments are conducted on three 3Dsegmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposedapproach consistently exhibits superior performance across various settings(e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared tothe state-of-the-art methods. Furthermore, the usefulness of the proposedmodules is further validated via ablation experiments.</description>
      <author>example@mail.com (Yunyao Lu, Yihang Wu, Ahmad Chaddad, Tareef Daqqaq, Reem Kateb)</author>
      <guid isPermaLink="false">2509.13084v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning</title>
      <link>http://arxiv.org/abs/2509.12875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LTA-Thinker是一种新型训练框架，通过增加潜在思想分布的方差和引入分布方向性优化，解决了大型语言模型中复杂推理的瓶颈问题，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型中的复杂推理可以通过测试时扩展(TTS)进行动态优化，以减轻过度思考问题。然而，在连续潜在空间推理中，高质量潜在思想的生成和利用仍然是核心瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出一个潜在思想增强训练框架——LTA-Thinker，以提高分布方差并从两个方面增强推理性能。&lt;h4&gt;方法&lt;/h4&gt;构建基于可学习先验的潜在思想生成架构，以增加生成潜在思想向量的方差分布，简化整体结构并提高性能上限；引入基于分布的方向性优化范式，联合约束分布局部性和分布规模，通过多目标共同训练策略结合标准监督微调(SFT)损失与两种新损失：语义对齐损失和推理焦点损失。&lt;h4&gt;主要发现&lt;/h4&gt;LTA-Thinker在各种基线中取得了最先进的(SOTA)性能，表现出更高的性能上限和更好的扩展效果。&lt;h4&gt;结论&lt;/h4&gt;LTA-Thinker通过提高分布方差和优化潜在思想的生成与利用，有效提升了大型语言模型的复杂推理能力。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型中的复杂推理可以通过测试时扩展(TTS)进行动态优化以减轻过度思考。诸如Coconut、SoftCoT及其变体等方法在连续潜在空间推理中有效，但核心瓶颈仍在于高质量潜在思想的生成和利用。借鉴SoftCoT++理论——生成的潜在思想分布的更大方差更接近黄金真实分布，我们提出了一个潜在思想增强训练框架——LTA-Thinker，它从两个方面提高分布方差并增强推理性能。首先，LTA-Thinker构建了一个基于可学习先验的潜在思想生成架构，该架构旨在增加生成潜在思想向量的方差分布，以简化整体结构并提高性能上限。其次，LTA-Thinker引入了一种基于分布的方向性优化范式，联合约束分布局部性和分布规模。这种机制通过多目标共同训练策略提高信息效率和计算成本，该策略结合了标准监督微调(SFT)损失与两种新损失：语义对齐损失，利用KL散度确保潜在思想与问题语义高度相关；推理焦点损失，利用对比学习机制引导模型关注最关键的推理步骤。实验表明，LTA-Thinker在各种基线中实现了最先进的(SOTA)性能，表现出更高的性能上限和更好的扩展效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Complex Reasoning in Large Language Models can be dynamically optimized usingTest-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,SoftCoT and its variant are effective in continuous latent space inference, thecore bottleneck still lies in the efficient generation and utilization ofhigh-quality Latent Thought. Drawing from the theory of SoftCoT++ that a largervariance in the generated Latent Thought distribution more closely approximatesthe golden truth distribution, we propose a Latent Thought-Augmented TrainingFramework--LTA-Thinker, which improves distributional variance and enhancesreasoning performance from two perspectives. First, LTA-Thinker constructs aLatent Thought generation architecture based on a learnable prior. Thisarchitecture aims to increase the variance distribution of generated LatentThought Vectors in order to simplify the overall structure and raise theperformance ceiling. Second, LTA-Thinker introduces a distribution-baseddirectional optimization paradigm that jointly constrains both distributionlocality and distribution scale. This mechanism improves information efficiencyand computational cost through a multi-objective co-training strategy, whichcombines standard Supervised Fine-Tuning (SFT) loss with two novel losses:Semantic Alignment Loss, which utilizes KL divergence to ensure that the LatentThought is highly relevant to the semantics of the question; Reasoning FocusLoss, which utilizes a contrastive learning mechanism to guide the model tofocus on the most critical reasoning steps. Experiments show that LTA-thinkerachieves state-of-the-art (SOTA) performance among various baselines anddemonstrates a higher performance ceiling and better scaling effects.</description>
      <author>example@mail.com (Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang, Yuantao Han, Cangyi Yang, jiaxu Zhang, Feiliang Ren)</author>
      <guid isPermaLink="false">2509.12875v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision</title>
      <link>http://arxiv.org/abs/2509.12771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉语言模型(VLMs)的概念抽象能力，提出了一种新的训练方法使模型能够识别图像作为一般概念的实例，而不仅仅是识别对象及其关系。&lt;h4&gt;背景&lt;/h4&gt;人类能够将图像识别为一般概念的实例，而不仅仅是识别其中的对象及其关系。研究者调查VLMs是否具有这种概念抽象能力。&lt;h4&gt;目的&lt;/h4&gt;1) 调查VLMs在多大程度上具有概念抽象能力；2) 研究策略使VLM模型能够更大程度地具备这种能力。&lt;h4&gt;方法&lt;/h4&gt;引入分组图像-标题数据集(MAGIC)，使用新颖的对比损失技术编码每组图像(标题)的共有信息，提出基于文本-图像对比组的分组对比损失函数(外部对比损失)和衡量组内图像-标题实例距离的内部损失，训练模型创建语义表示使其接近高层概念的语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种训练方法产生的CLEAR GLASS模型在抽象概念识别方面比最先进(SOTA)模型有所改进。&lt;h4&gt;结论&lt;/h4&gt;CLEAR GLASS模型通过这种训练方法获得了概念抽象能力，能够更好地识别抽象概念，尽管训练过程中模型并未直接接触高层概念标签。&lt;h4&gt;翻译&lt;/h4&gt;人类能够将图像识别为一般概念的实例，而不仅仅是识别其中的对象及其关系。在本文中，我们研究了1) VLMs在多大程度上具有这种概念抽象能力，以及2)如何在图像中编码那种高层概念信息，使产生的VLM模型(CLEAR GLASS模型)更大程度地具备这种能力的策略。为此，我们引入了一个分组图像-标题数据集(MAGIC)，它包含多组图像标题、相关图像和高层概念标签。我们使用新颖的对比损失技术，使模型编码每组图像(标题)中所有成员共有的信息。我们的主要贡献是基于文本-图像对比组的分组对比损失函数(外部对比损失)以及衡量组内图像-标题实例之间距离的内部损失。我们的训练方法使CLEAR GLASS模型获得了概念抽象能力，因为模型没有暴露给与每组相关的高层概念。相反，训练迫使模型为每组图像-标题创建一个语义表示，使其在潜在语义空间中更接近高层概念的语义表示。我们的实验表明，这种训练方法产生的模型在抽象概念识别方面比SOTA模型有所改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can recognize an image as an instance of a general concept, beyondsimply identifying its objects and their relationships. In this paper, weinvestigate 1. The extent to which VLMs have this concept abstraction capacity,and 2. Strategies for encoding the sort of higher-concept information in imagesthat would enable the resulting VLM model (CLEAR GLASS model) to have thiscapability to a greater degree. To this end, we introduce a groupedimage-caption dataset (MAGIC), which consists of several groups of imagecaptions and for each group a set of associated images and higher-levelconceptual labels. We use a novel contrastive loss technique to induce themodel to encode in the representation of each image (caption) in a group theinformation that is common to all members of the image-caption group. Our maincontribution is a grouped contrastive loss function based on text-imagecontrastive groups (outer contrastive loss) as well as an inner loss whichmeasures the distances between image-caption instances in the group. Ourtraining methodology results in the CLEAR GLASS model having the conceptabstraction capacity as an emergent capacity because the model is not exposedto the higher-level concepts associated with each group. Instead, the trainingforces the model to create for each image-caption group a semanticrepresentation that brings it closer to the semantic representation of thehigher-level concepts in the latent semantic space. Our experiments show thatthis training methodology results in a model which shows improvement inabstract concept recognition compared to SOTA models.</description>
      <author>example@mail.com (Omri Suissa, Muhiim Ali, Shengmai Chen, Yinuo Cai, Shekhar Pradhan)</author>
      <guid isPermaLink="false">2509.12771v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>ScaleDoc: Scaling LLM-based Predicates over Large Document Collections</title>
      <link>http://arxiv.org/abs/2509.12610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ScaleDoc是一种新型系统，通过解耦谓词执行为离线表示阶段和在线过滤阶段，结合大型语言模型和轻量级代理模型，显著提高了大规模语义分析的效率。&lt;h4&gt;背景&lt;/h4&gt;谓词是数据分析系统的基础组件，但现代工作负载越来越多地涉及需要语义理解的非结构化文档，传统基于值的谓词已无法满足需求。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在处理海量文档和临时查询时的高推理成本问题，使大规模语义分析变得实用和高效。&lt;h4&gt;方法&lt;/h4&gt;ScaleDoc系统将谓词执行分为离线表示阶段和在线过滤阶段。离线阶段利用LLM为文档生成语义表示；在线阶段训练轻量级代理模型过滤文档，仅将模糊案例转发给LLM。系统还包含两个核心创新：基于对比学习的框架训练代理模型生成可靠决策分数，以及自适应级联机制确定有效过滤策略。&lt;h4&gt;主要发现&lt;/h4&gt;ScaleDoc在三个数据集上实现了超过2倍的端到端加速，并将昂贵的LLM调用减少了高达85%。&lt;h4&gt;结论&lt;/h4&gt;ScaleDoc使大规模语义分析变得实用和高效，通过减少对大型语言模型的依赖，显著提高了处理性能。&lt;h4&gt;翻译&lt;/h4&gt;谓词是数据分析系统的基础组件。然而，现代工作负载越来越多地涉及非结构化文档，这需要超越传统基于值的谓词的语义理解。面对海量文档和临时查询，虽然大型语言模型展示了强大的零样本能力，但其高推理成本导致不可接受的额外开销。因此，我们提出了ScaleDoc，一种新型系统，通过将谓词执行解耦为离线表示阶段和优化后的在线过滤阶段来解决这一问题。在离线阶段，ScaleDoc利用LLM为每个文档生成语义表示。在线，对于每个查询，它在这些表示上训练一个轻量级代理模型来过滤大部分文档，仅将模糊案例转发给LLM进行最终决策。此外，ScaleDoc提出了两个核心创新以实现显著效率：(1)基于对比学习的框架，训练代理模型生成可靠的预测决策分数；(2)自适应级联机制，在满足特定准确度目标的同时确定有效的过滤策略。我们在三个数据集上的评估表明，ScaleDoc实现了超过2倍的端到端加速，并将昂贵的LLM调用减少了高达85%，使大规模语义分析变得实用和高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicates are foundational components in data analysis systems. However,modern workloads increasingly involve unstructured documents, which demandssemantic understanding, beyond traditional value-based predicates. Givenenormous documents and ad-hoc queries, while Large Language Models (LLMs)demonstrate powerful zero-shot capabilities, their high inference cost leads tounacceptable overhead. Therefore, we introduce \textsc{ScaleDoc}, a novelsystem that addresses this by decoupling predicate execution into an offlinerepresentation phase and an optimized online filtering phase. In the offlinephase, \textsc{ScaleDoc} leverages a LLM to generate semantic representationsfor each document. Online, for each query, it trains a lightweight proxy modelon these representations to filter the majority of documents, forwarding onlythe ambiguous cases to the LLM for final decision. Furthermore,\textsc{ScaleDoc} proposes two core innovations to achieve significantefficiency: (1) a contrastive-learning-based framework that trains the proxymodel to generate reliable predicating decision scores; (2) an adaptive cascademechanism that determines the effective filtering policy while meeting specificaccuracy targets. Our evaluations across three datasets demonstrate that\textsc{ScaleDoc} achieves over a 2$\times$ end-to-end speedup and reducesexpensive LLM invocations by up to 85\%, making large-scale semantic analysispractical and efficient.</description>
      <author>example@mail.com (Hengrui Zhang, Yulong Hui, Yihao Liu, Huanchen Zhang)</author>
      <guid isPermaLink="false">2509.12610v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>LayerLock: Non-collapsing Representation Learning with Progressive Freezing</title>
      <link>http://arxiv.org/abs/2509.10156v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LayerLock是一种简单而有效的自监督视觉表征学习方法，通过渐进式层冻结技术，从像素预测逐步过渡到潜在空间预测。&lt;h4&gt;背景&lt;/h4&gt;在视频掩码自编码(MAE)模型训练过程中，ViT层按照其深度顺序收敛：浅层先收敛，深层后收敛。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法加速标准MAE训练，并实现简单且可扩展的潜在空间预测，避免'表征崩溃'问题。&lt;h4&gt;方法&lt;/h4&gt;LayerLock利用ViT层按深度顺序收敛的观察，在整个训练过程中根据显式调度逐步冻结模型，既可用于加速标准MAE，也可用于潜在空间预测。&lt;h4&gt;主要发现&lt;/h4&gt;通过逐步冻结模型可以加速标准MAE训练；使用相同调度可以实现简单且可扩展的潜在空间预测，避免'表征崩溃'问题。&lt;h4&gt;结论&lt;/h4&gt;LayerLock方法成功应用于高达40亿参数的大模型，在4DS感知套件上的结果超越了非掩码潜在预测方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了LayerLock，一种简单而有效的自监督视觉表征学习方法，它通过渐进式层冻结技术，从像素预测逐步过渡到潜在空间预测。首先，我们观察到在视频掩码自编码(MAE)模型训练过程中，ViT层按照其深度顺序收敛：浅层先收敛，深层后收敛。然后，我们证明这一观察可以用来加速标准MAE，通过在整个训练过程中根据显式调度逐步冻结模型。此外，相同的调度可用于一种简单且可扩展的潜在空间预测方法，不会遭受'表征崩溃'问题。我们将提出的LayerLock方法应用于高达40亿参数的大模型，在4DS感知套件上的结果超越了非掩码潜在预测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LayerLock, a simple yet effective approach for self-supervisedvisual representation learning, that gradually transitions from pixel to latentprediction through progressive layer freezing. First, we make the observationthat during training of video masked-autoencoding (MAE) models, ViT layersconverge in the order of their depth: shallower layers converge early, deeperlayers converge late. We then show that this observation can be exploited toaccelerate standard MAE by progressively freezing the model according to anexplicit schedule, throughout training. Furthermore, this same schedule can beused in a simple and scalable approach to latent prediction that does notsuffer from "representation collapse". We apply our proposed approach,LayerLock, to large models of up to 4B parameters with results surpassing thoseof non-latent masked prediction on the 4DS perception suite.</description>
      <author>example@mail.com (Goker Erdogan, Nikhil Parthasarathy, Catalin Ionescu, Drew Hudson, Alexander Lerchner, Andrew Zisserman, Mehdi Sajjadi, Joao Carreira)</author>
      <guid isPermaLink="false">2509.10156v2</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
  <item>
      <title>Open-ended Hierarchical Streaming Video Understanding with Vision Language Models</title>
      <link>http://arxiv.org/abs/2509.12145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了分层流式视频理解任务，结合了在线时序动作定位和自由形式描述生成。作者提出使用大语言模型丰富现有数据集的方法，并开发了名为OpenHOUSE的系统，该系统具有专门的流式模块，能够准确检测相邻动作边界，性能显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;当前缺乏具有分层和细粒度时间标注的视频数据集，限制了视频理解技术的发展。同时，流式动作感知目前主要局限于动作分类，无法满足更复杂的视频理解需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够进行分层流式视频理解的方法，扩展流式动作感知的能力，使其超越简单的动作分类，能够进行更精细的动作边界检测和事件描述生成。&lt;h4&gt;方法&lt;/h4&gt;1. 利用大语言模型将原子动作分组为更高级别的事件，以丰富现有数据集；2. 提出OpenHOUSE系统，包含专门的流式模块用于检测紧密相邻动作之间的边界；3. 扩展流式动作感知，使其不仅限于动作分类，还包括自由形式的描述生成。&lt;h4&gt;主要发现&lt;/h4&gt;1. 大语言模型能够有效地将原子动作分组为更高级别的事件，从而丰富现有数据集；2. OpenHOUSE的专门流式模块能够准确检测紧密相邻动作之间的边界；3. OpenHOUSE的性能几乎是现有方法直接扩展的两倍。&lt;h4&gt;结论&lt;/h4&gt;流式动作感知的未来发展方向是集成强大的生成模型，OpenHOUSE代表了朝着这一方向迈出的关键一步，为更高级的视频理解任务提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了分层流式视频理解，这是一个结合在线时序动作定位与自由形式描述生成的任务。鉴于具有分层和细粒度时间标注的数据集稀缺，我们证明了大型语言模型能够有效地将原子动作分组为更高级别的事件，从而丰富现有数据集。随后，我们提出了OpenHOUSE(面向事件的开分层式在线理解系统)，它将流式动作感知扩展超越了动作分类的范围。OpenHOUSE具有专门的流式模块，能够准确检测紧密相邻动作之间的边界，性能几乎是现有方法直接扩展的两倍。我们展望流式动作感知的未来在于集成强大的生成模型，而OpenHOUSE代表了朝着这一方向迈出的关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Hierarchical Streaming Video Understanding, a task that combinesonline temporal action localization with free-form description generation.Given the scarcity of datasets with hierarchical and fine-grained temporalannotations, we demonstrate that LLMs can effectively group atomic actions intohigher-level events, enriching existing datasets. We then propose OpenHOUSE(Open-ended Hierarchical Online Understanding System for Events), which extendsstreaming action perception beyond action classification. OpenHOUSE features aspecialized streaming module that accurately detects boundaries between closelyadjacent actions, nearly doubling the performance of direct extensions ofexisting methods. We envision the future of streaming action perception in theintegration of powerful generative models, with OpenHOUSE representing a keystep in that direction.</description>
      <author>example@mail.com (Hyolim Kang, Yunsu Park, Youngbeom Yoo, Yeeun Choi, Seon Joo Kim)</author>
      <guid isPermaLink="false">2509.12145v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset</title>
      <link>http://arxiv.org/abs/2509.12047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 figures, Submitted to Computers and Electronics in Agriculture&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于计算机视觉的模块化流程，用于自动分析群体饲养环境中的动物行为，显著提高了分析准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;动物行为分析对理解农业环境中的动物福利、健康和生产率至关重要，但传统人工观察方法耗时、主观且可扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个利用开源最先进计算机视觉技术的模块化流程，实现群体饲养环境中动物行为分析的自动化。&lt;h4&gt;方法&lt;/h4&gt;结合零样本目标检测模型、感知运动跟踪和分割技术，以及使用视觉转换器进行高级特征提取，解决动物遮挡和群体饲养场景等挑战，并在室内猪监测中进行了演示。&lt;h4&gt;主要发现&lt;/h4&gt;在爱丁堡猪行为视频数据集上验证，时间模型总体准确率达94.2%，比现有方法提高21.2个百分点；跟踪能力身份保持得分为93.3%，目标检测精度达89.3%。&lt;h4&gt;结论&lt;/h4&gt;模块化设计有潜力适应其他环境，但需对不同物种进一步验证；开源实现为行为监测提供了可扩展解决方案，通过自动化、客观和持续的分析促进精准养猪和福利评估。&lt;h4&gt;翻译&lt;/h4&gt;动物行为分析在理解农业环境中的动物福利、健康状况和生产率方面起着至关重要的作用。然而，传统的人工观察方法耗时、主观且可扩展性有限。我们提出了一种模块化流程，利用开源的最先进计算机视觉技术来自动化群体饲养环境中的动物行为分析。我们的方法结合了用于零样本目标检测的最先进模型、感知运动跟踪和分割技术，以及使用视觉转换器进行高级特征提取，以实现稳健的行为识别。该流程解决了包括动物遮挡和群体饲养场景在内的挑战，如在室内猪监测中所展示的那样。我们在爱丁堡猪行为视频数据集上对多个行为任务验证了我们的系统。我们的时间模型实现了94.2%的总体准确率，比现有方法提高了21.2个百分点。该流程显示出强大的跟踪能力，身份保持得分为93.3%，目标检测精度为89.3%。模块化设计表明有潜力适应其他环境，但需要对不同物种进行进一步验证。开源实现为行为监测提供了可扩展的解决方案，通过自动化、客观和持续的分析为精准养猪和福利评估做出贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Animal behavior analysis plays a crucial role in understanding animalwelfare, health status, and productivity in agricultural settings. However,traditional manual observation methods are time-consuming, subjective, andlimited in scalability. We present a modular pipeline that leveragesopen-sourced state-of-the-art computer vision techniques to automate animalbehavior analysis in a group housing environment. Our approach combinesstate-of-the-art models for zero-shot object detection, motion-aware trackingand segmentation, and advanced feature extraction using vision transformers forrobust behavior recognition. The pipeline addresses challenges including animalocclusions and group housing scenarios as demonstrated in indoor pigmonitoring. We validated our system on the Edinburgh Pig Behavior Video Datasetfor multiple behavioral tasks. Our temporal model achieved 94.2% overallaccuracy, representing a 21.2 percentage point improvement over existingmethods. The pipeline demonstrated robust tracking capabilities with 93.3%identity preservation score and 89.3% object detection precision. The modulardesign suggests potential for adaptation to other contexts, though furthervalidation across species would be required. The open-source implementationprovides a scalable solution for behavior monitoring, contributing to precisionpig farming and welfare assessment through automated, objective, and continuousanalysis.</description>
      <author>example@mail.com (Haiyu Yang, Enhong Liu, Jennifer Sun, Sumit Sharma, Meike van Leerdam, Sebastien Franceschini, Puchun Niu, Miel Hostens)</author>
      <guid isPermaLink="false">2509.12047v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare</title>
      <link>http://arxiv.org/abs/2509.11944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于时间图的多模态医疗推理模型，通过有向图建模，能够适应动态变化，完善推理内容，并考虑时间因素跟踪患者健康变化。多代理时间推理框架进一步提高了推理准确性，实验验证了该方法的新颖性和实用性。&lt;h4&gt;背景&lt;/h4&gt;医疗和医学是多模态学科，需要处理多模态数据进行推理和诊断多种疾病。尽管已经出现了一些用于科学领域复杂任务的多模态推理模型，但它们在医疗领域的应用仍然有限，并且在诊断推理方面表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决多模态医疗推理在正确诊断方面的挑战，协助医疗专业人员。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于时间图的推理过程模型，通过有向图建模。该模型能够通过回溯来适应原因的动态变化，完善推理内容，创建新原因或删除现有原因，以达到最佳推荐或答案。考虑不同时间点的多模态数据，能够跟踪和分析患者的健康状况和疾病进展。提出了多代理时间推理框架，提供任务分配和交叉验证机制，进一步提高推理输出的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;基本实验和分析结果证明了所提出的初步方法的新颖性和实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的时间图推理框架和多代理时间推理框架能够有效解决医疗领域多模态推理的挑战，提高诊断准确性。&lt;h4&gt;翻译&lt;/h4&gt;医疗和医学是处理多模态数据以推理和诊断多种疾病的多模态学科。尽管已经出现了一些用于科学领域复杂任务的多模态推理模型，但它们在医疗领域的应用仍然有限，并且在诊断推理方面表现不佳。为解决多模态医疗推理在正确诊断方面的挑战并协助医疗专业人员，本文提出了一种新颖的基于时间图的推理过程，通过有向图建模。该模型能够通过回溯来适应原因的动态变化，完善推理内容，创建新原因或删除现有原因，以达到最佳推荐或答案。此外，考虑不同时间点的多模态数据能够跟踪和分析患者的健康状况和疾病进展。此外，所提出的多代理时间推理框架提供任务分配和交叉验证机制，进一步提高推理输出的准确性。一些基本实验和分析结果证明了所提出的初步方法的新颖性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Healthcare and medicine are multimodal disciplines that deal with multimodaldata for reasoning and diagnosing multiple diseases. Although some multimodalreasoning models have emerged for reasoning complex tasks in scientificdomains, their applications in the healthcare domain remain limited and fallshort in correct reasoning for diagnosis. To address the challenges ofmultimodal medical reasoning for correct diagnosis and assist the healthcareprofessionals, a novel temporal graph-based reasoning process modelled througha directed graph has been proposed in the current work. It helps inaccommodating dynamic changes in reasons through backtracking, refining thereasoning content, and creating new or deleting existing reasons to reach thebest recommendation or answer. Again, consideration of multimodal data atdifferent time points can enable tracking and analysis of patient health anddisease progression. Moreover, the proposed multi-agent temporal reasoningframework provides task distributions and a cross-validation mechanism tofurther enhance the accuracy of reasoning outputs. A few basic experiments andanalysis results justify the novelty and practical utility of the proposedpreliminary approach.</description>
      <author>example@mail.com (Susanta Mitra)</author>
      <guid isPermaLink="false">2509.11944v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</title>
      <link>http://arxiv.org/abs/2509.11866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了Dr.V分层框架，用于诊断大型视频模型中的幻觉问题，包括基准数据集Dr.V-Bench和卫星视频代理Dr.V-Agent，通过细粒度时空定位提高视频理解的可靠性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;大型视频模型(LVMs)的最新进展显著提升了视频理解能力，但这些模型仍然存在幻觉问题，即产生与输入视频内容相冲突的内容。&lt;h4&gt;目的&lt;/h4&gt;解决大型视频模型中的幻觉问题，建立一个能够诊断视频幻觉的框架。&lt;h4&gt;方法&lt;/h4&gt;提出Dr.V分层框架，包括：1) Dr.V-Bench基准数据集：包含来自4,974个视频的10k实例，涵盖多样化任务，每个实例都有详细的时空标注；2) Dr.V-Agent卫星视频代理：通过在感知和时空层面系统应用细粒度时空定位，然后进行认知层面推理，来检测LVMs中的幻觉。&lt;h4&gt;主要发现&lt;/h4&gt;Dr.V-Agent能够有效诊断幻觉，同时提高模型的可解释性和可靠性。&lt;h4&gt;结论&lt;/h4&gt;Dr.V为现实世界场景中的鲁棒视频理解提供了实用的蓝图，所有数据和代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;大型视频模型(LVMs)的最新进展显著提升了视频理解能力。然而，这些模型仍然存在幻觉问题，产生与输入视频内容相冲突的内容。为解决这一问题，我们提出了Dr.V，一个覆盖感知、时空和认知三个层面的分层框架，通过细粒度的时空定位来诊断视频幻觉。Dr.V包含两个关键组件：基准数据集Dr.V-Bench和卫星视频代理Dr.V-Agent。Dr.V-Bench包含来自4,974个视频的10k实例，涵盖多样化任务，每个实例都配有详细的时空标注。Dr.V-Agent通过在感知和时空层面系统应用细粒度时空定位，然后进行认知层面推理，来检测LVMs中的幻觉。这一逐步流程模拟了类人视频理解过程，能有效识别幻觉。大量实验证明，Dr.V-Agent在诊断幻觉方面有效，同时提高了可解释性和可靠性，为现实世界场景中的鲁棒视频理解提供了实用蓝图。我们所有的数据和代码可在https://github.com/Eurekaleo/Dr.V获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large video models (LVMs) have significantly enhancevideo understanding. However, these models continue to suffer fromhallucinations, producing content that conflicts with input videos. To addressthis issue, we propose Dr.V, a hierarchical framework covering perceptive,temporal, and cognitive levels to diagnose video hallucination by fine-grainedspatial-temporal grounding. Dr.V comprises of two key components: a benchmarkdataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes10k instances drawn from 4,974 videos spanning diverse tasks, each enrichedwith detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations inLVMs by systematically applying fine-grained spatial-temporal grounding at theperceptive and temporal levels, followed by cognitive level reasoning. Thisstep-by-step pipeline mirrors human-like video comprehension and effectivelyidentifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent iseffective in diagnosing hallucination while enhancing interpretability andreliability, offering a practical blueprint for robust video understanding inreal-world scenarios. All our data and code are available athttps://github.com/Eurekaleo/Dr.V.</description>
      <author>example@mail.com (Meng Luo, Shengqiong Wu, Liqiang Jing, Tianjie Ju, Li Zheng, Jinxiang Lai, Tianlong Wu, Xinya Du, Jian Li, Siyuan Yan, Jiebo Luo, William Yang Wang, Hao Fei, Mong-Li Lee, Wynne Hsu)</author>
      <guid isPermaLink="false">2509.11866v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Vision Language Models and Symbolic Grounding for Video Question Answering</title>
      <link>http://arxiv.org/abs/2509.11862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合符号场景图与视觉语言模型的新框架SG-VLM，用于提升视频问答中的因果和时间推理能力，尽管相对于强大的VLMs改进有限。&lt;h4&gt;背景&lt;/h4&gt;视频问答(VQA)需要模型对视频中的空间、时间和因果线索进行推理。近期的视觉语言模型虽取得良好结果，但常依赖浅层关联，导致时间定位能力弱且可解释性有限。&lt;h4&gt;目的&lt;/h4&gt;研究符号场景图(SGs)作为视频问答的中间定位信号，探索如何将SGs与VLMs结合以提升模型表现。&lt;h4&gt;方法&lt;/h4&gt;提出SG-VLM模块化框架，通过提示和视觉定位将冻结的VLMs与场景图定位相结合，利用SGs提供的结构化对象-关系表示来补充VLMs的整体推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准测试(NExT-QA, iVQA, ActivityNet-QA)和多个VLMs(QwenVL, InternVL)上测试，SG-VLM提高了因果和时间推理能力，超越了先前的基线方法，但相对于强大的VLMs的改进有限。&lt;h4&gt;结论&lt;/h4&gt;符号定位在视频问答中具有前景但也存在当前局限性，为未来混合VLM-符号方法在视频理解中的应用提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;视频问答(VQA)需要模型对视频中的空间、时间和因果线索进行推理。近期的视觉语言模型(VLMs)取得了很好的结果，但通常依赖于浅层关联，导致时间定位能力弱且可解释性有限。我们研究符号场景图(SGs)作为VQA的中间定位信号。SGs提供结构化的对象-关系表示，补充VLMs的整体推理能力。我们提出SG-VLM，一个通过提示和视觉定位将冻结的VLMs与场景图定位相结合的模块化框架。在三个基准测试(NExT-QA, iVQA, ActivityNet-QA)和多个VLMs(QwenVL, InternVL)上，SG-VLM提高了因果和时间推理能力，并超越了先前的基线方法，但相对于强大的VLMs的改进有限。这些发现突显了符号定位的前景和当前局限性，为未来混合VLM-符号方法在视频理解中的应用提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VQA) requires models to reason over spatial,temporal, and causal cues in videos. Recent vision language models (VLMs)achieve strong results but often rely on shallow correlations, leading to weaktemporal grounding and limited interpretability. We study symbolic scene graphs(SGs) as intermediate grounding signals for VQA. SGs provide structuredobject-relation representations that complement VLMs holistic reasoning. Weintroduce SG-VLM, a modular framework that integrates frozen VLMs with scenegraph grounding via prompting and visual localization. Across three benchmarks(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLMimproves causal and temporal reasoning and outperforms prior baselines, thoughgains over strong VLMs are limited. These findings highlight both the promiseand current limitations of symbolic grounding, and offer guidance for futurehybrid VLM-symbolic approaches in video understanding.</description>
      <author>example@mail.com (Haodi Ma, Vyom Pathak, Daisy Zhe Wang)</author>
      <guid isPermaLink="false">2509.11862v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning</title>
      <link>http://arxiv.org/abs/2509.11796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出FineQuest，一个基于双模式推理的无需训练框架，专门用于解决体育视频问答中的复杂理解问题。&lt;h4&gt;背景&lt;/h4&gt;基于大型语言模型(LLMs)的视频问答在一般视频理解方面显示出潜力，但在应用于体育视频这一固有复杂领域时面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决通用模型与特定体育领域理解之间的知识差距，提高体育视频问答的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出FineQuest框架，采用双模式推理（反应式推理用于简单问题，深思熟虑式推理用于复杂问题）；集成SSGraph，一个跨越九种运动的多模态体育知识场景图；引入两个新的体育VideoQA基准：Gym-QA和Diving-QA。&lt;h4&gt;主要发现&lt;/h4&gt;FineQuest在新的基准测试以及现有的SPORTU数据集上取得了最先进的性能，同时保持了强大的通用VideoQA能力。&lt;h4&gt;结论&lt;/h4&gt;FineQuest通过双模式推理和领域特定知识整合，有效解决了体育视频问答的挑战。&lt;h4&gt;翻译&lt;/h4&gt;基于大型语言模型(LLMs)的视频问答在一般视频理解方面显示出潜力，但在应用于体育视频这一固有复杂领域时面临重大挑战。在这项工作中，我们提出了FineQuest，这是第一个利用认知科学启发的双模式推理的无训练框架：i)用于简单体育查询的反应式推理；ii)用于更复杂查询的深思熟虑式推理。为了弥合通用模型与特定体育领域理解之间的知识差距，FineQuest集成了SSGraph，一个跨越九种运动的多模态体育知识场景图，它编码视觉实例和领域特定术语以提高推理准确性。此外，我们引入了两个新的体育VideoQA基准，Gym-QA和Diving-QA，它们源自FineGym和FineDiving数据集，实现了多样化和全面的评估。FineQuest在这些基准以及现有的SPORTU数据集上取得了最先进的性能，同时保持了强大的通用VideoQA能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VideoQA) based on Large Language Models (LLMs) hasshown potential in general video understanding but faces significant challengeswhen applied to the inherently complex domain of sports videos. In this work,we propose FineQuest, the first training-free framework that leveragesdual-mode reasoning inspired by cognitive science: i) Reactive Reasoning forstraightforward sports queries and ii) Deliberative Reasoning for more complexones. To bridge the knowledge gap between general-purpose models anddomain-specific sports understanding, FineQuest incorporates SSGraph, amultimodal sports knowledge scene graph spanning nine sports, which encodesboth visual instances and domain-specific terminology to enhance reasoningaccuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QAand Diving-QA, derived from the FineGym and FineDiving datasets, enablingdiverse and comprehensive evaluation. FineQuest achieves state-of-the-artperformance on these benchmarks as well as the existing SPORTU dataset, whilemaintains strong general VideoQA capabilities.</description>
      <author>example@mail.com (Haodong Chen, Haojian Huang, XinXiang Yin, Dian Shao)</author>
      <guid isPermaLink="false">2509.11796v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration</title>
      <link>http://arxiv.org/abs/2509.11360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了GLaVE-Cap框架，通过整合视觉专家和双流结构解决视频详细描述中的上下文一致性和细节性问题，同时构建了GLaVE-Bench基准测试和GLaVE-1.2M数据集，在多个基准上实现了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;视频详细描述旨在生成全面视频描述以促进视频理解。目前大多数研究采用从局部到全局的范式，先生成视频片段的局部描述，再将其汇总为全局描述。&lt;h4&gt;目的&lt;/h4&gt;解决现有从局部到全局的视频描述范式导致的描述不够详细且上下文不一致的问题，提高视频描述的质量和连贯性。&lt;h4&gt;方法&lt;/h4&gt;提出GLaVE-Cap框架，包含两个核心模块：1) TrackFusion模块利用视觉专家获取跨帧视觉提示，结合双流结构实现全面的局部描述生成；2) CaptionBridge模块使用全局上下文指导局部描述，并将局部描述自适应地汇总为连贯的全局描述。同时构建GLaVE-Bench基准测试和GLaVE-1.2M训练数据集。&lt;h4&gt;主要发现&lt;/h4&gt;现有范式导致描述不够详细且上下文不一致的原因是：(1)没有确保细粒度描述的机制，(2)局部和全局描述之间的交互较弱。GLaVE-Cap通过整合视觉专家和双流结构有效解决了这些问题。&lt;h4&gt;结论&lt;/h4&gt;在四个基准测试上的大量实验表明，GLaVE-Cap达到了最先进的性能。消融研究和学生模型分析进一步验证了所提出模块的有效性以及GLaVE-1.2M对视频理解社区的贡献。源代码、模型权重、基准和数据集将开源。&lt;h4&gt;翻译&lt;/h4&gt;视频详细描述旨在生成全面的视频描述以促进视频理解。最近，视频详细描述领域的大多数努力都集中在从局部到全局的范式上，该范式首先从视频片段生成局部描述，然后将其汇总为全局描述。然而，我们发现这种范式导致描述不够详细且上下文不一致，这可以归因于：(1)没有确保细粒度描述的机制，以及(2)局部和全局描述之间的交互较弱。为了解决上述两个问题，我们提出了GLaVE-Cap，这是一种具有视觉专家集成的全局-局部对齐框架，包含两个核心模块：TrackFusion通过利用视觉专家获取跨帧视觉提示，结合双流结构实现全面的局部描述生成；而CaptionBridge通过使用全局上下文指导局部描述，并将局部描述自适应地汇总为连贯的全局描述来建立局部-全局交互。此外，我们构建了GLaVE-Bench，这是一个全面的视频描述基准测试，每个视频包含比现有基准多5倍的查询，覆盖多样化的视觉维度，以促进可靠的评估。我们还提供了包含16K高质量细粒度视频描述和1.2M相关问答对的训练数据集GLaVE-1.2M。在四个基准上的大量实验表明，我们的GLaVE-Cap实现了最先进的性能。此外，消融研究和学生模型分析进一步验证了所提出模块的有效性以及GLaVE-1.2M对视频理解社区的贡献。源代码、模型权重、基准和数据集将开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video detailed captioning aims to generate comprehensive video descriptionsto facilitate video understanding. Recently, most efforts in the video detailedcaptioning community have been made towards a local-to-global paradigm, whichfirst generates local captions from video clips and then summarizes them into aglobal caption. However, we find this paradigm leads to less detailed andcontextual-inconsistent captions, which can be attributed to (1) no mechanismto ensure fine-grained captions, and (2) weak interaction between local andglobal captions. To remedy the above two issues, we propose GLaVE-Cap, aGlobal-Local aligned framework with Vision Expert integration for Captioning,which consists of two core modules: TrackFusion enables comprehensive localcaption generation, by leveraging vision experts to acquire cross-frame visualprompts, coupled with a dual-stream structure; while CaptionBridge establishesa local-global interaction, by using global context to guide local captioning,and adaptively summarizing local captions into a coherent global caption.Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmarkfeaturing 5X more queries per video than existing benchmarks, covering diversevisual dimensions to facilitate reliable evaluation. We further provide atraining dataset GLaVE-1.2M containing 16K high-quality fine-grained videocaptions and 1.2M related question-answer pairs. Extensive experiments on fourbenchmarks show that our GLaVE-Cap achieves state-of-the-art performance.Besides, the ablation studies and student model analyses further validate theeffectiveness of the proposed modules and the contribution of GLaVE-1.2M to thevideo understanding community. The source code, model weights, benchmark, anddataset will be open-sourced.</description>
      <author>example@mail.com (Wan Xu, Feng Zhu, Yihan Zeng, Yuanfan Guo, Ming Liu, Hang Xu, Wangmeng Zuo)</author>
      <guid isPermaLink="false">2509.11360v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Equalize: Data-Driven Frequency-Domain Signal Recovery in Molecular Communications</title>
      <link>http://arxiv.org/abs/2509.11327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于长短期记忆(LSTM)神经网络的频域均衡(FDE)技术，用于分子通信系统中的符号间干扰和噪声抑制，无需预先知道信道模型信息，同时保持高计算效率。&lt;h4&gt;背景&lt;/h4&gt;在分子通信中，符号间干扰(ISI)和噪声是影响通信可靠性的关键因素。时域均衡可有效减轻这些影响但计算复杂度高，而频域均衡计算效率更高却通常需要预先了解信道模型。&lt;h4&gt;目的&lt;/h4&gt;解决传统FDE方法对先验信道信息的依赖问题，提出基于LSTM神经网络的FDE技术，能够在分子通信信道中建模时间相关性，提高ISI和噪声抑制能力。&lt;h4&gt;方法&lt;/h4&gt;采用基于长短期记忆(LSTM)神经网络的频域均衡技术，通过监督训练策略实现信道自适应均衡，消除对信道先验信息的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，所提出的LSTM-FDE相比传统FDE和基于前馈神经网络的均衡器显著降低了误码率，这种性能提升归因于LSTM的时间建模能力增强了噪声抑制并加速了模型收敛。&lt;h4&gt;结论&lt;/h4&gt;基于LSTM神经网络的频域均衡技术能有效解决分子通信中的符号间干扰和噪声问题，无需预先知道信道模型信息，同时保持高计算效率，是一种有前景的分子通信均衡方案。&lt;h4&gt;翻译&lt;/h4&gt;在分子通信(MC)中，符号间干扰(ISI)和噪声是降低通信可靠性的关键因素。虽然时域均衡可以有效减轻这些影响，但它通常涉及与信道内存相关的高计算复杂度。相比之下，频域均衡(FDE)提供了更高的计算效率，但通常需要预先了解信道模型。为了解决这一限制，本文提出了一种基于长短期记忆(LSTM)神经网络的FDE技术，能够在分子通信信道中建模时间相关性，以提高ISI和噪声抑制能力。为了消除传统FDE方法对信道先验信息的依赖，采用监督训练策略进行信道自适应均衡。仿真结果表明，与传统的FDE和基于前馈神经网络的均衡器相比，所提出的LSTM-FDE显著降低了误码率。这一性能提升归因于LSTM的时间建模能力，它增强了噪声抑制并加速了模型收敛，同时保持了相当的计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In molecular communications (MC), inter-symbol interference (ISI) and noiseare key factors that degrade communication reliability. Although time-domainequalization can effectively mitigate these effects, it often entails highcomputational complexity concerning the channel memory. In contrast,frequency-domain equalization (FDE) offers greater computational efficiency buttypically requires prior knowledge of the channel model. To address thislimitation, this letter proposes FDE techniques based on long short-term memory(LSTM) neural networks, enabling temporal correlation modeling in MC channelsto improve ISI and noise suppression. To eliminate the reliance on priorchannel information in conventional FDE methods, a supervised training strategyis employed for channel-adaptive equalization. Simulation results demonstratethat the proposed LSTM-FDE significantly reduces the bit error rate compared totraditional FDE and feedforward neural network-based equalizers. Thisperformance gain is attributed to the LSTM's temporal modeling capabilities,which enhance noise suppression and accelerate model convergence, whilemaintaining comparable computational efficiency.</description>
      <author>example@mail.com (Cheng Xiang, Yu Huang, Miaowen Wen, Weiqiang Tan, Chan-Byoung Chae)</author>
      <guid isPermaLink="false">2509.11327v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic</title>
      <link>http://arxiv.org/abs/2509.11165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Traffic-MLLM是一个基于Qwen2.5-VL的多模态大语言模型，通过LoRA微调和创新的知识提示模块，实现了在交通视频理解任务上的最先进性能，并具备优秀的零样本推理和跨场景泛化能力。&lt;h4&gt;背景&lt;/h4&gt;随着智能交通系统的发展，交通视频理解在全面场景感知和因果分析中扮演着越来越重要的角色。然而，现有方法在准确建模时空因果关系和集成领域特定知识方面面临显著挑战，限制了它们在复杂场景中的有效性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，作者提出了Traffic-MLLM，一个专门用于细粒度交通分析的多模态大语言模型。&lt;h4&gt;方法&lt;/h4&gt;基于Qwen2.5-VL主干构建，利用高质量交通特定多模态数据集，使用低秩自适应(LoRA)进行轻量级微调，增强建模视频序列中连续时空特征的能力。同时引入创新的知识提示模块，将思维链(CoT)推理与检索增强生成(RAG)融合，使详细的交通规则和领域知识能够精确注入推理过程。&lt;h4&gt;主要发现&lt;/h4&gt;在TrafficQA和DriveQA基准测试上的实验结果表明，Traffic-MLLM取得了最先进的性能，验证了其处理多模态交通数据的卓越能力。它还表现出显著的零样本推理和跨场景泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Traffic-MLLM模型能够有效解决交通视频理解中的时空因果关系建模和领域知识集成问题，在复杂场景中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;随着智能交通系统的发展，交通视频理解在全面场景感知和因果分析中扮演着越来越重要的角色。然而，现有方法在准确建模时空因果关系和集成领域特定知识方面面临显著挑战，限制了它们在复杂场景中的有效性。为了解决这些局限性，我们提出了Traffic-MLLM，一个专门用于细粒度交通分析的多模态大语言模型。基于Qwen2.5-VL主干构建，我们的模型利用高质量交通特定多模态数据集，并使用低秩自适应(LoRA)进行轻量级微调，显著增强了其建模视频序列中连续时空特征的能力。此外，我们引入了一个创新的知识提示模块，将思维链(CoT)推理与检索增强生成(RAG)融合，使详细的交通规则和领域知识能够精确注入推理过程。这种设计显著提升了模型的逻辑推理和知识适应能力。在TrafficQA和DriveQA基准测试上的实验结果表明，Traffic-MLLM取得了最先进的性能，验证了其处理多模态交通数据的卓越能力。它还表现出显著的零样本推理和跨场景泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As intelligent transportation systems advance, traffic video understandingplays an increasingly pivotal role in comprehensive scene perception and causalanalysis. Yet, existing approaches face notable challenges in accuratelymodeling spatiotemporal causality and integrating domain-specific knowledge,limiting their effectiveness in complex scenarios. To address theselimitations, we propose Traffic-MLLM, a multimodal large language modeltailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,our model leverages high-quality traffic-specific multimodal datasets and usesLow-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancingits capacity to model continuous spatiotemporal features in video sequences.Furthermore, we introduce an innovative knowledge prompting module fusingChain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),enabling precise injection of detailed traffic regulations and domain knowledgeinto the inference process. This design markedly boosts the model's logicalreasoning and knowledge adaptation capabilities. Experimental results onTrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-artperformance, validating its superior ability to process multimodal trafficdata. It also exhibits remarkable zero-shot reasoning and cross-scenariogeneralization capabilities.</description>
      <author>example@mail.com (Waikit Xiu, Qiang Lu, Xiying Li, Chen Hu, Shengbo Sun)</author>
      <guid isPermaLink="false">2509.11165v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning</title>
      <link>http://arxiv.org/abs/2509.11880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将监督对比学习(SupCon)应用于模仿学习(IL)的新方法，专注于为视频游戏环境中的智能体学习更有效的状态表示。&lt;h4&gt;背景&lt;/h4&gt;模仿学习在视频游戏环境中面临状态表示学习的挑战，需要捕捉观察结果与行动之间的因果关系。&lt;h4&gt;目的&lt;/h4&gt;获取观察结果的潜在表示，更好地捕捉与行动相关的因素，从而更好地建模观察结果映射到演示者执行动作的因果关系。&lt;h4&gt;方法&lt;/h4&gt;提出一种将SupCon损失与连续输出空间集成的方案，使SupCon能够在不限制环境动作类型的情况下运行。&lt;h4&gt;主要发现&lt;/h4&gt;在3D游戏Astro Bot和Returnal以及多个2D Atari游戏上的实验表明，与仅使用监督动作预测损失函数训练的基线模型相比，改进了表示质量，加快了学习收敛速度，并提高了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;将SupCon应用于IL可以有效地改善智能体在视频游戏环境中的学习性能，使其能够更好地捕捉观察结果与行动之间的因果关系。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了监督对比学习(SupCon)在模仿学习(IL)中的新应用，专注于为视频游戏环境中的智能体学习更有效的状态表示。目标是获取观察结果的潜在表示，更好地捕捉与行动相关的因素，从而更好地建模观察结果映射到演示者执行动作的因果关系，例如当前方出现障碍物时玩家会跳跃。我们提出了一种将SupCon损失与连续输出空间集成的方案，使SupCon能够在不限制环境动作类型的情况下运行。在3D游戏Astro Bot和Returnal以及多个2D Atari游戏上的实验表明，与仅使用监督动作预测损失函数训练的基线模型相比，改进了表示质量，加快了学习收敛速度，并提高了泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的是在视频游戏环境中，如何让智能体通过高维视觉输入学习更有效的状态表示问题。这个问题在现实中很重要，因为当测试已发布游戏时，往往无法访问游戏的内部状态数据，只能通过视觉输入端到端训练智能体。这种方法需要更大的数据集和更长的训练时间，增加了过拟合风险，因此学习能识别与动作相关关键因素并泛化到新状态的有效表示至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到在视频游戏中，精确的空间配置对决策至关重要，而传统状态表示学习方法虽引入有用归纳偏置，但未能完全实现由智能体动作明确塑造表示的目标。他们借鉴了监督对比学习(SupCon)的思想，但发现其依赖的数据增强会扭曲游戏中的关键空间信息。因此，作者设计出使用动作标签而非数据增强来确定正负样本对的方法，并针对视频游戏中常见的连续和离散混合动作空间提出了具体处理策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在潜在嵌入空间中，根据动作对观察结果进行分组：导致相同动作的观察结果应具有相似表示，而与不同动作相关的观察结果应被很好分离。整体流程包括：1)构建包含特征提取器和策略网络的结构；2)组合预测任务损失和监督对比损失；3)将连续动作空间离散化为多个区间；4)使用混合基数位置编码将多维动作转换为单一分类标签；5)计算SupCon损失，同时优化两种损失函数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将监督对比学习应用于模仿学习领域；2)提出不需要人工视图或正样本对增强的对比学习方法；3)针对连续和离散混合动作空间提出有效处理策略；4)解决小批量中可能不存在正样本对的问题。相比之前工作，这种方法不依赖数据增强避免空间信息扭曲，明确由智能体动作塑造表示而非仅受时间或物理动力学影响，专门针对模仿学习任务和视频游戏环境进行了改进，通过添加SupCon损失作为正则化项改善了表示质量、学习收敛速度和泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过将监督对比学习引入模仿学习领域并针对视频游戏环境特点进行改进，提出了一种能学习更有效状态表示的方法，显著提高了智能体的学习效率、收敛速度和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CoG64752.2025.11114174&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel application of Supervised Contrastive Learning(SupCon) to Imitation Learning (IL), with a focus on learning more effectivestate representations for agents in video game environments. The goal is toobtain latent representations of the observations that capture better theaction-relevant factors, thereby modeling better the cause-effect relationshipfrom the observations that are mapped to the actions performed by thedemonstrator, for example, the player jumps whenever an obstacle appears ahead.We propose an approach to integrate the SupCon loss with continuous outputspaces, enabling SupCon to operate without constraints regarding the type ofactions of the environment. Experiments on the 3D games Astro Bot and Returnal,and multiple 2D Atari games show improved representation quality, fasterlearning convergence, and better generalization compared to baseline modelstrained only with supervised action prediction loss functions.</description>
      <author>example@mail.com (Carlos Celemin, Joseph Brennan, Pierluigi Vito Amadori, Tim Bradley)</author>
      <guid isPermaLink="false">2509.11880v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification</title>
      <link>http://arxiv.org/abs/2509.11587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层身份学习(HIL)框架，用于无监督可见光-红外行人重识别任务，通过多中心对比学习和双向反向选择传输机制解决了现有方法忽略细粒度差异的问题，在SYSU-MM01和RegDB数据集上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督可见光-红外行人重识别(USVI-ReID)旨在从无标注的跨模态行人数据集中学习模态不变图像特征，同时减少模态差距并最小化对昂贵人工标注的依赖。现有方法通常使用基于聚类的对比学习，将一个人表示为单一聚类中心，主要关注每个聚类内图像的共同性，而忽略了它们之间的细粒度差异。&lt;h4&gt;目的&lt;/h4&gt;解决现有无监督可见光-红外行人重识别方法中只关注聚类内图像共同性而忽略细粒度差异的问题，提高跨模态匹配质量。&lt;h4&gt;方法&lt;/h4&gt;提出分层身份学习(HIL)框架，包括：(1)通过二次聚类为每个现有的粗粒度聚类生成多个记忆，反映图像之间的细粒度变化；(2)提出多中心对比学习(MCCL)来优化表示，增强模内聚类并最小化跨模态差异；(3)设计双向反向选择传输(BRST)机制，通过执行伪标签的双向匹配建立可靠的跨模态对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;在SYSU-MM01和RegDB数据集上的大量实验表明，提出的方法优于现有方法，证明了HIL框架在无监督可见光-红外行人重识别任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;分层身份学习框架通过考虑细粒度差异和多中心表示，有效解决了现有无监督可见光-红外行人重识别方法的局限性，提高了跨模态匹配质量，为该领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;无监督可见光-红外行人重识别(USVI-ReID)旨在通过减少模态差距同时最小化对昂贵人工标注的依赖，从未标记的跨模态行人数据集中学习模态不变的图像特征。现有方法通常使用基于聚类的对比学习来解决USVI-ReID，将一个人表示为单一聚类中心。然而，它们主要关注每个聚类内图像的共同性，而忽略了它们之间的细粒度差异。为了解决这一局限性，我们提出了分层身份学习(HIL)框架。由于每个聚类可能包含几个反映图像间细粒度变化的小子聚类，我们通过二次聚类为每个现有的粗粒度聚类生成多个记忆。此外，我们提出了多中心对比学习(MCCL)来优化表示，以增强模内聚类并最小化跨模态差异。为了进一步提高跨模态匹配质量，我们设计了一种双向反向选择传输(BRST)机制，通过执行伪标签的双向匹配建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上进行的大量实验表明，所提出的方法优于现有方法。源代码可在以下网址获取：https://github.com/haonanshi0125/HIL。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised visible-infrared person re-identification (USVI-ReID) aims tolearn modality-invariant image features from unlabeled cross-modal persondatasets by reducing the modality gap while minimizing reliance on costlymanual annotations. Existing methods typically address USVI-ReID usingcluster-based contrastive learning, which represents a person by a singlecluster center. However, they primarily focus on the commonality of imageswithin each cluster while neglecting the finer-grained differences among them.To address the limitation, we propose a Hierarchical Identity Learning (HIL)framework. Since each cluster may contain several smaller sub-clusters thatreflect fine-grained variations among images, we generate multiple memories foreach existing coarse-grained cluster via a secondary clustering. Additionally,we propose Multi-Center Contrastive Learning (MCCL) to refine representationsfor enhancing intra-modal clustering and minimizing cross-modal discrepancies.To further improve cross-modal matching quality, we design a BidirectionalReverse Selection Transmission (BRST) mechanism, which establishes reliablecross-modal correspondences by performing bidirectional matching ofpseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDBdatasets demonstrate that the proposed method outperforms existing approaches.The source code is available at: https://github.com/haonanshi0125/HIL.</description>
      <author>example@mail.com (Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao)</author>
      <guid isPermaLink="false">2509.11587v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness</title>
      <link>http://arxiv.org/abs/2509.11355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出两种互补的正则化策略，通过减少CNN对高频纹理的依赖和促进形状感知表示，提高模型对图像损坏的鲁棒性，同时保持原始分类性能。&lt;h4&gt;背景&lt;/h4&gt;卷积神经网络在图像分类方面表现出色，但对人类容易处理的常见损坏仍然脆弱。这种脆弱性的一个关键原因是CNN依赖局部纹理线索而非全局物体形状，这与人类感知形成鲜明对比。&lt;h4&gt;目的&lt;/h4&gt;提出两种互补的正则化策略，旨在促进形状偏向的表示并增强CNN对图像损坏的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;第一种方法引入辅助损失，强制原始输入和低频滤波输入之间的特征一致性，减少对高频纹理的依赖；第二种方法结合监督对比学习，围绕类一致、形状相关的表示来构建特征空间。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10-C基准测试上评估，两种方法都提高了对图像损坏的鲁棒性，同时没有降低干净数据的准确性。&lt;h4&gt;结论&lt;/h4&gt;损失级别的正则化可以有效引导CNN朝向更形状感知、更有韧性的表示。&lt;h4&gt;翻译&lt;/h4&gt;卷积神经网络在图像分类方面表现出色，但对人类容易处理的常见损坏仍然脆弱。这种脆弱性的一个关键原因是它们依赖局部纹理线索而非全局物体形状——这与人类感知形成鲜明对比。为此，我们提出两种互补的正则化策略，旨在促进形状偏向的表示并增强鲁棒性。第一种引入辅助损失，强制原始输入和低频滤波输入之间的特征一致性，减少对高频纹理的依赖。第二种结合监督对比学习，围绕类一致、形状相关的表示来构建特征空间。在CIFAR-10-C基准测试上评估，两种方法都提高了对损坏的鲁棒性，同时没有降低干净数据的准确性。我们的结果表明，损失级别的正则化可以有效引导CNN朝向更形状感知、更有韧性的表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Convolutional Neural Networks (CNNs) excel at image classification but remainvulnerable to common corruptions that humans handle with ease. A key reason forthis fragility is their reliance on local texture cues rather than globalobject shapes -- a stark contrast to human perception. To address this, wepropose two complementary regularization strategies designed to encourageshape-biased representations and enhance robustness. The first introduces anauxiliary loss that enforces feature consistency between original andlow-frequency filtered inputs, discouraging dependence on high-frequencytextures. The second incorporates supervised contrastive learning to structurethe feature space around class-consistent, shape-relevant representations.Evaluated on the CIFAR-10-C benchmark, both methods improve corruptionrobustness without degrading clean accuracy. Our results suggest thatloss-level regularization can effectively steer CNNs toward more shape-aware,resilient representations.</description>
      <author>example@mail.com (Robin Narsingh Ranabhat, Longwei Wang, Amit Kumar Patel, KC santosh)</author>
      <guid isPermaLink="false">2509.11355v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Network Representation Learning</title>
      <link>http://arxiv.org/abs/2509.11316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自适应对比边表示学习(ACERL)的新方法，用于处理脑连接数据分析中的挑战，包括个体特异性、高维性和稀疏性网络。该方法基于对比学习和自适应随机掩码机制，在边表示学习中达到最小最优收敛率，并在多种下游任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;网络表示学习旨在将网络嵌入低维空间同时保留结构和语义属性，但脑连接数据分析面临个体特异性、高维性和稀疏性网络的挑战，且缺乏节点或边协变量。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于对比学习的统计方法用于网络边嵌入，解决脑连接数据分析中的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出自适应对比边表示学习(ACERL)，包含两个关键组件：增强网络对的对比学习和数据驱动的自适应随机掩码机制，并建立非渐近误差边界。&lt;h4&gt;主要发现&lt;/h4&gt;ACERL在边表示学习中达到最小最优收敛率，学习到的表示可用于网络分类、重要边检测和社区检测等下游任务，且对这些任务有理论保证。通过合成数据和真实脑连接研究验证了该方法，与稀疏主成分分析相比具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;ACERL方法能有效处理脑连接数据分析中的挑战，在多种任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;网络表示学习寻求将网络嵌入到低维空间，同时保留结构和语义属性，从而促进下游任务，如分类、特征预测、边识别和社区检测。受脑连接数据分析挑战的启发，这些数据具有个体特异性、高维性和稀疏性网络的特点，且缺乏节点或边协变量，我们提出了一种新颖的基于对比学习的统计方法用于网络边嵌入，命名为自适应对比边表示学习(ACERL)。它基于两个关键组件：增强网络对的对比学习和数据驱动的自适应随机掩码机制。我们建立了非渐近误差边界，并证明我们的方法在边表示学习中达到了最小最优收敛率。我们进一步展示了学习到的表示在多种下游任务中的适用性，包括网络分类、重要边检测和社区检测，并建立了相应的理论保证。我们通过合成数据和真实脑连接研究验证了我们的方法，并显示出与稀疏主成分分析基线方法相比具有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network representation learning seeks to embed networks into alow-dimensional space while preserving the structural and semantic properties,thereby facilitating downstream tasks such as classification, trait prediction,edge identification, and community detection. Motivated by challenges in brainconnectivity data analysis that is characterized by subject-specific,high-dimensional, and sparse networks that lack node or edge covariates, wepropose a novel contrastive learning-based statistical approach for networkedge embedding, which we name as Adaptive Contrastive Edge RepresentationLearning (ACERL). It builds on two key components: contrastive learning ofaugmented network pairs, and a data-driven adaptive random masking mechanism.We establish the non-asymptotic error bounds, and show that our method achievesthe minimax optimal convergence rate for edge representation learning. Wefurther demonstrate the applicability of the learned representation in multipledownstream tasks, including network classification, important edge detection,and community detection, and establish the corresponding theoreticalguarantees. We validate our method through both synthetic data and real brainconnectivities studies, and show its competitive performance compared to thebaseline method of sparse principal components analysis.</description>
      <author>example@mail.com (Zihan Dong, Xin Zhou, Ryumei Nakada, Lexin Li, Linjun Zhang)</author>
      <guid isPermaLink="false">2509.11316v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment</title>
      <link>http://arxiv.org/abs/2509.11135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AlignKT的新方法，用于知识追踪(KT)，采用前端到后端架构明确建模稳定知识状态，通过对比学习增强对齐鲁棒性，实验证明在三个数据集上优于七种基线方法。&lt;h4&gt;背景&lt;/h4&gt;知识追踪(KT)是智能教学系统(ITS)的基本组件，通过建模学习者知识状态监控学习进度。然而现有KT模型主要关注拟合交互序列，忽视知识状态本身，导致可解释性降低和教学支持不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有KT模型可解释性不足和教学支持不够的问题，通过明确建模稳定知识状态提高KT模型性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;AlignKT采用前端到后端架构，定义基于教学理论的理想知识状态作为对齐标准，使用五个编码器实现，并加入对比学习模块增强对齐过程的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验，AlignKT在三个真实数据集上优于七种KT基线方法，在两个数据集上达到最先进结果，在第三个数据集上也表现具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;AlignKT通过明确建模知识状态并引入对齐标准，成功提高了KT模型的性能和可解释性，为智能教学系统提供了更好的教学支持。&lt;h4&gt;翻译&lt;/h4&gt;知识追踪(KT)是智能教学系统(ITS)的基本组成部分，通过建模学习者的知识状态，使这些系统能够监控和理解学习者的进步。然而，许多现有的KT模型主要关注拟合学习者交互序列，而常常忽视知识状态本身。这一限制降低了可解释性，并为ITS提供了不足的教学支持。为了应对这一挑战，我们提出了AlignKT，它采用前端到后端的架构来明确建模稳定的知识状态。在此方法中，初步知识状态与额外的标准对齐。具体来说，我们基于教学理论定义了一个理想知识状态作为对齐标准，为可解释性提供了基础。我们使用五个编码器来实现此设置，并采用对比学习模块来增强对齐过程的鲁棒性。通过大量实验，AlignKT表现出优越的性能，在三个真实数据集上优于七种KT基线方法。它在其中两个数据集上取得了最先进的结果，在第三个数据集上也表现出具有竞争力的性能。本工作的代码可在https://github.com/SCNU203/AlignKT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Tracing (KT) serves as a fundamental component of IntelligentTutoring Systems (ITS), enabling these systems to monitor and understandlearners' progress by modeling their knowledge state. However, many existing KTmodels primarily focus on fitting the sequences of learners' interactions, andoften overlook the knowledge state itself. This limitation leads to reducedinterpretability and insufficient instructional support from the ITS. Toaddress this challenge, we propose AlignKT, which employs a frontend-to-backendarchitecture to explicitly model a stable knowledge state. In this approach,the preliminary knowledge state is aligned with an additional criterion.Specifically, we define an ideal knowledge state based on pedagogical theoriesas the alignment criterion, providing a foundation for interpretability. Weutilize five encoders to implement this set-up, and incorporate a contrastivelearning module to enhance the robustness of the alignment process. Throughextensive experiments, AlignKT demonstrates superior performance, outperformingseven KT baselines on three real-world datasets. It achieves state-of-the-artresults on two of these datasets and exhibits competitive performance on thethird. The code of this work is available athttps://github.com/SCNU203/AlignKT.</description>
      <author>example@mail.com (Jing Xiao, Chang You, Zhiyu Chen)</author>
      <guid isPermaLink="false">2509.11135v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SPARK: Adaptive Low-Rank Knowledge Graph Modeling in Hybrid Geometric Spaces for Recommendation</title>
      <link>http://arxiv.org/abs/2509.11094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM' 25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SPARK，一个多阶段框架，用于解决知识图谱增强推荐系统中的噪声、稀疏性和几何表示问题，特别关注长尾项目的推荐效果。&lt;h4&gt;背景&lt;/h4&gt;知识图谱增强推荐系统面临固有噪声、数据稀疏性和欧几里得几何不适合复杂关系结构的挑战，这些问题损害了表示学习，特别是对于长尾实体。现有方法通常缺乏针对项目流行度的自适应多源信号融合。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效处理知识图谱噪声和稀疏性，同时为长尾项目提供更好表示的推荐系统框架，特别关注主流和长尾项目的精确建模。&lt;h4&gt;方法&lt;/h4&gt;SPARK框架采用多阶段方法：首先使用Tucker低秩分解去噪知识图谱并生成鲁棒实体表示；然后采用SVD初始化的混合几何图神经网络同时在欧几里得和双曲空间中学习表示，利用双曲空间建模层次结构的能力；引入项目流行度感知的自适应融合策略，动态加权来自协作过滤、精炼知识图谱嵌入和不同几何空间的信号；最后使用对比学习对齐多源表示。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，SPARK在最新的方法上表现出显著的优越性，特别是在改善长尾项目推荐方面，为知识增强推荐提供了一种稳健且原则性的方法。&lt;h4&gt;结论&lt;/h4&gt;SPARK框架通过系统性地解决知识图谱增强推荐系统中的关键挑战，特别是长尾项目的表示问题，提供了一个创新且有效的解决方案，实现了主流和长尾项目的精确建模。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱增强推荐系统，但面临固有噪声、稀疏性和欧几里得几何不适合复杂关系结构的挑战，严重损害了表示学习，特别是对于长尾实体。现有方法通常也缺乏针对项目流行度的自适应多源信号融合。本文介绍了SPARK，一个新颖的多阶段框架，系统性地解决这些问题。SPARK首先采用Tucker低秩分解对知识图谱去噪并生成鲁棒实体表示。随后，一个SVD初始化的混合几何图神经网络同时在欧几里得和双曲空间中学习表示；后者被战略性地利用，因为它擅长建模层次结构，有效捕获稀疏长尾项目的语义特征。核心贡献是一个项目流行度感知的自适应融合策略，动态加权来自协作过滤、精炼知识图谱嵌入和不同几何空间的信号，精确建模主流和长尾项目。最后，对比学习对齐这些多源表示。大量实验证明了SPARK在最新方法上的显著优越性，特别是在改善长尾项目推荐方面，为知识增强推荐提供了一种稳健且原则性的方法。实现代码可在https://github.com/Applied-Machine-Learning-Lab/SPARK获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Graphs (KGs) enhance recommender systems but face challenges frominherent noise, sparsity, and Euclidean geometry's inadequacy for complexrelational structures, critically impairing representation learning, especiallyfor long-tail entities. Existing methods also often lack adaptive multi-sourcesignal fusion tailored to item popularity. This paper introduces SPARK, a novelmulti-stage framework systematically tackling these issues. SPARK first employsTucker low-rank decomposition to denoise KGs and generate robust entityrepresentations. Subsequently, an SVD-initialized hybrid geometric GNNconcurrently learns representations in Euclidean and Hyperbolic spaces; thelatter is strategically leveraged for its aptitude in modeling hierarchicalstructures, effectively capturing semantic features of sparse, long-tail items.A core contribution is an item popularity-aware adaptive fusion strategy thatdynamically weights signals from collaborative filtering, refined KGembeddings, and diverse geometric spaces for precise modeling of bothmainstream and long-tail items. Finally, contrastive learning aligns thesemulti-source representations. Extensive experiments demonstrate SPARK'ssignificant superiority over state-of-the-art methods, particularly inimproving long-tail item recommendation, offering a robust, principled approachto knowledge-enhanced recommendation. Implementation code is available athttps://github.com/Applied-Machine-Learning-Lab/SPARK.</description>
      <author>example@mail.com (Binhao Wang, Yutian Xiao, Maolin Wang, Zhiqi Li, Tianshuo Wei, Ruocheng Guo, Xiangyu Zhao)</author>
      <guid isPermaLink="false">2509.11094v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data</title>
      <link>http://arxiv.org/abs/2509.11053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对有限数据条件下轴承故障诊断的先进数据增强和对比傅里叶卷积框架(DAC-FCF)，通过结合新型生成对抗网络、对比学习和傅里叶卷积神经网络，有效解决了数据稀缺、特征提取不充分和样本关系建模不足等问题。&lt;h4&gt;背景&lt;/h4&gt;在轴承故障诊断领域，深度学习方法被广泛应用，但由于高成本或隐私问题，实际场景中高质量的标记数据稀缺。虽然少样本学习在解决数据稀缺问题上显示出潜力，但现有方法仍面临显著限制。&lt;h4&gt;目的&lt;/h4&gt;解决传统数据增强技术生成低质量样本、传统卷积神经网络难以提取全局特征以及现有方法无法建模有限训练样本间复杂关系等问题，为有限数据条件下的轴承故障诊断提供有效解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出DAC-FCF框架，包含三个核心组件：1) 条件一致潜在表示和重建生成对抗网络(CCLR-GAN)用于生成更多样化的数据；2) 基于对比学习的联合优化机制用于建模训练数据间的关系；3) 一维傅里叶卷积神经网络(1D-FCNN)用于实现输入数据的全局感知。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明DAC-FCF取得了显著改进，在CWRU数据集上比基线方法提高最多32%，在自收集测试台上提高10%。大量的消融实验证明了所提出组件的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的DAC-FCF为有限数据条件下的轴承故障诊断提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在轴承故障诊断领域，深度学习方法最近已被广泛应用。然而，由于高成本或隐私问题，实际场景中高质量的标记数据稀缺。虽然少样本学习在解决数据稀缺问题上显示出潜力，但现有方法在此领域仍面临显著限制。传统数据增强技术常遭受模式崩溃，生成无法捕捉轴承故障模式多样性的低质量样本。此外，具有局部感受野的传统卷积神经网络不足以从复杂振动信号中提取全局特征。另外，现有方法无法建模有限训练样本之间的复杂关系。为解决这些问题，我们提出了一种针对有限数据条件下轴承故障诊断的先进数据增强和对比傅里叶卷积框架(DAC-FCF)。首先，提出了一种新型的条件一致潜在表示和重建生成对抗网络(CCLR-GAN)来生成更多样化的数据。其次，利用基于对比学习的联合优化机制来更好地建模可用训练数据之间的关系。最后，我们提出了一维傅里叶卷积神经网络(1D-FCNN)来实现对输入数据的全局感知。实验证明DAC-FCF取得了显著改进，在凯斯西储大学(CWRU)数据集上比基线方法提高最多32%，在自收集测试台上提高10%。大量的消融实验证明了所提出组件的有效性。因此，提出的DAC-FCF为有限数据条件下的轴承故障诊断提供了有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the area of bearing fault diagnosis, deep learning (DL) methods have beenwidely used recently. However, due to the high cost or privacy concerns,high-quality labeled data are scarce in real world scenarios. While few-shotlearning has shown promise in addressing data scarcity, existing methods stillface significant limitations in this domain. Traditional data augmentationtechniques often suffer from mode collapse and generate low-quality samplesthat fail to capture the diversity of bearing fault patterns. Moreover,conventional convolutional neural networks (CNNs) with local receptive fieldsmakes them inadequate for extracting global features from complex vibrationsignals. Additionally, existing methods fail to model the intricaterelationships between limited training samples. To solve these problems, wepropose an advanced data augmentation and contrastive fourier convolutionframework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, anovel conditional consistent latent representation and reconstructiongenerative adversarial network (CCLR-GAN) is proposed to generate more diversedata. Secondly, a contrastive learning based joint optimization mechanism isutilized to better model the relations between the available training data.Finally, we propose a 1D fourier convolution neural network (1D-FCNN) toachieve a global-aware of the input data. Experiments demonstrate that DAC-FCFachieves significant improvements, outperforming baselines by up to 32\% oncase western reserve university (CWRU) dataset and 10\% on a self-collectedtest bench. Extensive ablation experiments prove the effectiveness of theproposed components. Thus, the proposed DAC-FCF offers a promising solution forbearing fault diagnosis under limited data.</description>
      <author>example@mail.com (Shengke Sun, Shuzhen Han, Ziqian Luan, Xinghao Qin, Jiao Yin, Zhanshan Zhao, Jinli Cao, Hua Wang)</author>
      <guid isPermaLink="false">2509.11053v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds</title>
      <link>http://arxiv.org/abs/2509.10842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpenUrban3D是首个不依赖对齐多视图图像、预训练点云分割网络或手动标注的3D开放词汇语义分割框架，可直接从原始点云生成语义特征，实现任意文本查询的零样本分割。&lt;h4&gt;背景&lt;/h4&gt;开放词汇语义分割能识别和分割来自任意自然语言描述的对象，对大规模城市点云应用至关重要，但该领域研究不足，主要受限于高质量多视图图像的缺乏和现有3D分割方法的泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;解决大规模城市点云数据集中缺乏高质量多视图图像和现有3D分割方法泛化能力差的问题，开发一个不依赖对齐多视图图像、预训练网络或手动标注的3D开放词汇语义分割框架。&lt;h4&gt;方法&lt;/h4&gt;通过多视图多粒度渲染、掩码级视觉-语言特征提取和样本平衡融合，直接从原始点云生成语义特征，然后蒸馏到3D骨干模型，实现零样本分割并保留语义和几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;在SensatUrban和SUM等大规模城市基准测试上，OpenUrban3D在分割精度和跨场景泛化能力上显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;OpenUrban3D为大规模城市场景理解提供了灵活且可扩展的解决方案，无需依赖对齐多视图图像或预训练模型。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇语义分割使模型能够识别和分割来自任意自然语言描述的对象，提供了处理固定标签集之外的新的、细粒度的或功能定义的类别的灵活性。虽然这种能力对于支持数字孪生、智能城市管理和城市分析的大规模城市点云至关重要，但在这个领域仍 largely unexplored。主要障碍是大规模城市点云数据集中经常缺乏高质量、对齐良好的多视图图像，以及现有的三维分割管道在几何、尺度和外观差异大的多样化城市环境中泛化能力差。为了解决这些挑战，我们提出了OpenUrban3D，这是第一个大规模城市场景的3D开放词汇语义分割框架，它可以在没有对齐多视图图像、预训练点云分割网络或手动标注的情况下运行。我们的方法通过多视图、多粒度渲染、掩码级视觉-语言特征提取和样本平衡融合，直接从原始点云生成强大的语义特征，然后将其蒸馏到3D骨干模型中。这种设计能够实现任意文本查询的零样本分割，同时捕获语义丰富性和几何先验。在SensatUrban和SUM等大规模城市基准测试上的广泛实验表明，OpenUrban3D在分割精度和跨场景泛化方面都显著优于现有方法，证明了其作为3D城市场景理解的灵活且可扩展解决方案的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决大规模城市点云的开集词汇语义分割问题，具体包括两个关键挑战：1) 大规模城市点云数据集常缺乏高质量对齐的多视图图像；2) 现有3D分割管道在不同城市环境中的泛化能力差。这个问题在现实中非常重要，因为城市点云支持数字孪生、智能城市管理等应用，而传统方法依赖预定义封闭类别标签，导致标注成本高昂且无法处理未见过的类别或功能定义区域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：室内场景的开集词汇方法依赖对齐的2D图像，而城市点云数据缺乏这种图像；'先分割后识别'策略在城市场景中表现不佳。因此设计了无需对齐图像、预训练网络或手动标注的框架。方法借鉴了OpenScene的2D到3D知识蒸馏策略，利用2D视觉-语言模型(如CLIP)进行特征提取，并参考了'先分割后识别'的范式，但针对城市场景进行了改进，设计了专门的多视图多粒度投影方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接从原始点云生成鲁棒语义特征，不依赖对齐图像或预训练3D分割网络，通过多视图渲染捕获不同尺度对象，利用掩码级视觉-语言特征提取和样本平衡融合，然后将知识蒸馏到3D模型。整体流程包括：1)多视图多粒度投影生成虚拟图像；2)提取2D掩码特征并反投影到点云；3)样本平衡特征融合构建2D特征库；4)2D到3D知识蒸馏训练3D主干；5)推理时融合2D-3D特征，通过余弦相似度实现任意文本查询的分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个仅需要原始点云输入的大规模城市3D开集词汇语义分割框架；2)多视图多粒度投影、掩码级特征提取和2D-3D知识蒸馏的定制化技术流程；3)样本平衡特征融合(SBFF)解决数据不平衡问题。相比之前工作，OpenUrban3D不依赖对齐RGB图像或预训练3D分割网络，能处理大规模物体尺度变化，在跨场景泛化方面表现更好，并通过2D-3D特征融合结合了语义识别能力和几何先验。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenUrban3D首次实现了无需对齐图像、预训练分割网络或手动标注的大规模城市点云开集词汇语义分割，通过多视图多粒度渲染、掩码级特征提取和2D-3D知识蒸馏，显著提升了分割精度和跨场景泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary semantic segmentation enables models to recognize and segmentobjects from arbitrary natural language descriptions, offering the flexibilityto handle novel, fine-grained, or functionally defined categories beyond fixedlabel sets. While this capability is crucial for large-scale urban point cloudsthat support applications such as digital twins, smart city management, andurban analytics, it remains largely unexplored in this domain. The mainobstacles are the frequent absence of high-quality, well-aligned multi-viewimagery in large-scale urban point cloud datasets and the poor generalizationof existing three-dimensional (3D) segmentation pipelines across diverse urbanenvironments with substantial variation in geometry, scale, and appearance. Toaddress these challenges, we present OpenUrban3D, the first 3D open-vocabularysemantic segmentation framework for large-scale urban scenes that operateswithout aligned multi-view images, pre-trained point cloud segmentationnetworks, or manual annotations. Our approach generates robust semanticfeatures directly from raw point clouds through multi-view, multi-granularityrendering, mask-level vision-language feature extraction, and sample-balancedfusion, followed by distillation into a 3D backbone model. This design enableszero-shot segmentation for arbitrary text queries while capturing both semanticrichness and geometric priors. Extensive experiments on large-scale urbanbenchmarks, including SensatUrban and SUM, show that OpenUrban3D achievessignificant improvements in both segmentation accuracy and cross-scenegeneralization over existing methods, demonstrating its potential as a flexibleand scalable solution for 3D urban scene understanding.</description>
      <author>example@mail.com (Chongyu Wang, Kunlei Jing, Jihua Zhu, Di Wang)</author>
      <guid isPermaLink="false">2509.10842v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks</title>
      <link>http://arxiv.org/abs/2509.12151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可学习的物理模拟器，能够准确预测机器人在接触丰富操作中末端执行器的运动和力矩。该模型扩展了现有的GNN-based模拟器，实现了条件动作预测，并在模拟和真实世界实验中表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有的物理模拟器在处理接触丰富的操作任务时存在局限性，特别是在预测机器人末端执行器的运动和力矩方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测机器人末端执行器在接触丰富操作中的运动和力矩的可学习物理模拟器，提高预测精度和性能。&lt;h4&gt;方法&lt;/h4&gt;扩展了基于图神经网络(GNN)的最先进模拟器FIGNET，引入了新的节点和边类型，实现了对控制任务和状态估计任务的条件动作预测。在模拟中使用MPC代理进行测试，并在真实世界实验中验证性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在模拟中，使用该模型的MPC代理在孔插入任务中与使用真实动力学模型的控制器性能相当。2. 在真实世界实验中，该模型相比基线物理模拟器，运动预测准确率提高了50%，力矩预测精度提高了3倍。3. 源代码和数据已公开可用。&lt;h4&gt;结论&lt;/h4&gt;所提出的可学习物理模拟器在机器人末端执行器的运动和力矩预测方面表现出色，能够有效支持控制任务和状态估计任务，且源代码和数据已公开，便于进一步研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种可学习的物理模拟器，能够准确预测机器人在接触丰富操作中末端执行器的运动和力矩。所提出的模型通过引入新的节点和边类型扩展了最先进的基于图神经网络的模拟器(FIGNet)，实现了对控制任务和状态估计任务的条件动作预测。在模拟中，使用我们模型的MPC代理在具有挑战性的孔插入任务中与使用真实动力学模型的相同控制器性能相匹配；而在真实世界实验中，我们的模型相比基线物理模拟器，运动预测准确率提高了50%，力矩预测精度提高了3倍。源代码和数据已公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a learnable physics simulator that provides accurate motion andforce-torque prediction of robot end effectors in contact-rich manipulation.The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)with novel node and edge types, enabling action-conditional predictions forcontrol and state estimation tasks. In simulation, the MPC agent using ourmodel matches the performance of the same controller with the ground truthdynamics model in a challenging peg-in-hole task, while in the real-worldexperiment, our model achieves a 50% improvement in motion prediction accuracyand 3$\times$ increase in force-torque prediction precision over the baselinephysics simulator. Source code and data are publicly available.</description>
      <author>example@mail.com (Zongyao Yi, Joachim Hertzberg, Martin Atzmueller)</author>
      <guid isPermaLink="false">2509.12151v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Visual Autonomous Parking via Control-Aided Attention</title>
      <link>http://arxiv.org/abs/2509.11090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CAA-Policy的端到端模仿学习系统，通过控制辅助注意力机制(CAA)解决了精确停车中感知和控制协同不足的问题，实现了更准确、鲁棒且可解释的停车决策。&lt;h4&gt;背景&lt;/h4&gt;精确停车需要一个端到端系统，感知系统需自适应提供与策略相关的细节，特别是在需要精细控制决策的关键区域。现有的端到端学习方法在感知和控制之间缺乏有效的协同作用，且transformer-based自注意力机制单独使用时往往产生不稳定和不一致的空间注意力。&lt;h4&gt;目的&lt;/h4&gt;解决现有端到端学习方法中感知和控制协同不足的问题，提高停车系统的准确性、鲁棒性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出CAA-Policy系统，引入新颖的Control-Aided Attention (CAA)机制，允许控制信号指导视觉注意力的学习；首次以自监督方式训练注意力模块，使用来自控制输出的反向传播梯度而非训练损失；集成短时程航点预测作为辅助任务；引入单独训练的运动预测模块以稳健跟踪目标位置。&lt;h4&gt;主要发现&lt;/h4&gt;仅基于transformer的自注意力机制会产生不稳定和时间上不一致的空间注意力，削弱下游策略决策的可靠性；使用控制输出的梯度训练注意力可促使注意力集中在导致动作输出高方差的视觉特征上，而非仅最小化训练损失；这种转变带来了更稳健和可推广的策略。&lt;h4&gt;结论&lt;/h4&gt;在CARLA模拟器中的大量实验表明，CAA-Policy持续优于端到端学习基线和模块化的BEV分割+混合A*流水线，实现了更高的准确性、鲁棒性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;精确停车需要一个端到端系统，其中感知系统自适应地提供与策略相关的细节-特别是在需要精细控制决策的关键区域。端到端学习通过直接将传感器输入映射到控制动作提供了一个统一的框架，但现有方法在感知和控制之间缺乏有效的协同作用。我们发现，单独使用基于transformer的自注意力机制往往会产生不稳定且时间上不一致的空间注意力，这随时间削弱了下游策略决策的可靠性。相反，我们提出了CAA-Policy，一个端到端的模仿学习系统，通过新颖的控制辅助注意力(CAA)机制允许控制信号指导视觉注意力的学习。我们首次以自监督方式训练这样的注意力模块，使用从控制输出反向传播的梯度而非来自训练损失的梯度。这种策略鼓励注意力集中在导致动作输出高方差的视觉特征上，而不仅仅是最小化训练损失-我们证明这种转变带来了更稳健和可推广的策略。为进一步增强稳定性，CAA-Policy集成了短时程航点预测作为辅助任务，并引入了单独训练的运动预测模块以随时间稳健地跟踪目标位置。在CARLA模拟器中的大量实验表明，CAA-Policy持续优于端到端学习基线和模块化的BEV分割+混合A*流水线，实现了更高的准确性、鲁棒性和可解释性。代码已在https://github.com/Joechencc/CAAPolicy发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Precise parking requires an end-to-end system where perception adaptivelyprovides policy-relevant details-especially in critical areas where finecontrol decisions are essential. End-to-end learning offers a unified frameworkby directly mapping sensor inputs to control actions, but existing approacheslack effective synergy between perception and control. We find thattransformer-based self-attention, when used alone, tends to produce unstableand temporally inconsistent spatial attention, which undermines the reliabilityof downstream policy decisions over time. Instead, we propose CAA-Policy, anend-to-end imitation learning system that allows control signal to guide thelearning of visual attention via a novel Control-Aided Attention (CAA)mechanism. For the first time, we train such an attention module in aself-supervised manner, using backpropagated gradients from the control outputsinstead of from the training loss. This strategy encourages the attention tofocus on visual features that induce high variance in action outputs, ratherthan merely minimizing the training loss-a shift we demonstrate leads to a morerobust and generalizable policy. To further enhance stability, CAA-Policyintegrates short-horizon waypoint prediction as an auxiliary task, andintroduces a separately trained motion prediction module to robustly track thetarget spot over time. Extensive experiments in the CARLA simulator show that\titlevariable~consistently surpasses both the end-to-end learning baseline andthe modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,robustness, and interpretability. Code is released athttps://github.com/Joechencc/CAAPolicy.</description>
      <author>example@mail.com (Chao Chen, Shunyu Yao, Yuanwu He, Tao Feng, Ruojing Song, Yuliang Guo, Xinyu Huang, Chenxu Wu, Ren Liu, Chen Feng)</author>
      <guid isPermaLink="false">2509.11090v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</title>
      <link>http://arxiv.org/abs/2509.12197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对从野外LiDAR点云进行3D人体姿态估计和人体网格恢复的研究进行了全面综述，提出了结构化分类方法，并分析了各种方法的优缺点。&lt;h4&gt;背景&lt;/h4&gt;从野外LiDAR点云进行3D人体姿态估计和人体网格恢复是计算机视觉领域的重要研究方向。&lt;h4&gt;目的&lt;/h4&gt;对现有方法进行系统比较，提出分类框架，分析方法的优缺点，并建立评估基准以促进该领域的发展。&lt;h4&gt;方法&lt;/h4&gt;对三个常用数据集进行定量比较，编写评估指标的统一定义，在数据集上建立两个任务的基准表格，维护一个配套网页持续更新研究。&lt;h4&gt;主要发现&lt;/h4&gt;提出了结构化分类方法，分析了各种方法的优缺点和设计选择，建立了评估基准，明确了开放挑战和研究方向。&lt;h4&gt;结论&lt;/h4&gt;基于LiDAR的3D人体理解仍面临开放挑战，需要进一步研究，作者提供了持续更新的资源平台。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们对从野外LiDAR点云进行3D人体姿态估计和人体网格恢复的研究进行了全面综述。我们在多个关键维度上比较了现有方法，并提出了一个结构化的分类方法来对这些方法进行分类。按照这一分类方法，我们分析了每种方法的优点、局限性和设计选择。此外，(i)我们对三个最常用的数据集进行了定量比较，详细描述了它们的特征；(ii)我们编写了所有评估指标的统一定义；(iii)我们在这些数据集上为两个任务建立了基准表格，以实现公平比较并促进该领域的进步。我们还概述了推动基于LiDAR的3D人体理解的关键开放挑战和研究方向。此外，我们维护了一个配套网页，根据我们的分类组织论文并持续更新新研究：https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决对从LiDAR点云进行3D人体姿态估计和人体网格恢复的全面综述问题。这个问题在现实中非常重要，因为这些技术在自动驾驶、虚拟现实、人机交互、医疗保健等领域有广泛应用，能帮助系统准确理解和预测人类行为，提高安全性和交互体验。LiDAR传感器提供精确的3D几何信息，不受光照影响，且保护隐私，是这些应用的关键技术。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性分析现有文献，提出一个结构化的分类法来组织各种方法，并分析它们的优缺点。作者借鉴了先前关于3D人体姿态估计和人体网格恢复的综述工作，但发现现有综述主要关注图像或视频方法，缺乏对LiDAR传感器的专门关注。因此，作者填补了这一空白，专注于从LiDAR点云进行3D人体理解的方法，并整合了多传感器融合、弱监督学习等先进技术思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 作为综述论文，其核心思想是提供一个全面的框架来理解和分类从LiDAR点云进行3D人体姿态估计和人体网格恢复的方法。整体流程包括：介绍问题背景和重要性；分析传感器技术特别是LiDAR的特点；提出分类法组织现有方法；按监督类型（监督、弱监督、无监督）分析3D人体姿态估计方法；讨论人体网格恢复方法；评估常用数据集；统一评估指标定义；建立基准测试；指出未来挑战和方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门针对LiDAR点云的3D人体理解进行全面综述；提出结构化分类法；对三个主要数据集进行定量比较；统一评估指标定义；建立基准测试表；维护持续更新的配套网页。相比之前工作，这篇综述专注于LiDAR而非图像/视频；针对户外'野外'场景而非受控环境；包含最新研究成果（至2025年）；提供更全面的方法分类和实际应用基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性的综述、分类和基准测试，显著推进了从LiDAR点云进行3D人体姿态估计和人体网格恢复领域的研究与应用，为该领域提供了清晰的发展路线图。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a comprehensive review of 3D human pose estimationand human mesh recovery from in-the-wild LiDAR point clouds. We compareexisting approaches across several key dimensions, and propose a structuredtaxonomy to classify these methods. Following this taxonomy, we analyze eachmethod's strengths, limitations, and design choices. In addition, (i) weperform a quantitative comparison of the three most widely used datasets,detailing their characteristics; (ii) we compile unified definitions of allevaluation metrics; and (iii) we establish benchmark tables for both tasks onthese datasets to enable fair comparisons and promote progress in the field. Wealso outline open challenges and research directions critical for advancingLiDAR-based 3D human understanding. Moreover, we maintain an accompanyingwebpage that organizes papers according to our taxonomy and continuously updateit with new studies:https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</description>
      <author>example@mail.com (Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet)</author>
      <guid isPermaLink="false">2509.12197v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2509.11853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SDI-GS的新方法，通过区域分割识别和保留结构上重要的区域，减少3D高斯点数量，同时保持场景保真度，提高稀疏视图合成的效率和实用性。&lt;h4&gt;背景&lt;/h4&gt;稀疏视图合成具有挑战性，因为从有限观测中恢复准确几何和外观困难。现有3D高斯散射方法通常依赖运动结构(SfM)进行相机姿态估计，这在真正稀疏视图设置中表现不佳。无SfM方法虽用多视图立体视觉替代，但通过反向投影所有像素生成大量3D高斯点，导致高内存成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，在稀疏视图条件下高效生成高质量3D场景表示，同时减少内存使用和提高训练速度。&lt;h4&gt;方法&lt;/h4&gt;提出分割驱动的Gaussian Splatting初始化(SDI-GS)，利用基于区域的分割识别和保留结构上重要的区域，实现对密集点云的选择性下采样，保持场景保真度的同时显著减少高斯点数量。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SDI-GS可减少高达50%的高斯点数量，在PSNR和SSIM指标上实现相当或更优的渲染质量，LPIPS指标仅有轻微下降。该方法还实现了更快的训练速度和更低的内存占用。&lt;h4&gt;结论&lt;/h4&gt;SDI-GS推进了3D高斯散射在受限视图场景中的实用性，通过减少内存需求和提高训练效率，使3DGS更适合实际应用。&lt;h4&gt;翻译&lt;/h4&gt;稀疏视图合成仍然是一个具有挑战性的问题，因为从有限的观测中恢复准确的几何和外观很困难。虽然3D高斯散射(3DGS)的最新进展实现了具有竞争力的质量的实时渲染，但现有流程通常依赖于运动结构(SfM)进行相机姿态估计，这种方法在真正稀疏视图设置中表现不佳。此外，一些无SfM的方法用多视图立体视觉(MVS)模型替代SfM，但通过将每个像素反向投影到3D空间来生成大量3D高斯点，导致高内存成本。我们提出了分割驱动的Gaussian Splatting初始化(SDI-GS)，一种通过利用基于区域的分割来识别和保留结构上重要的区域，从而缓解低效率问题的方法。这实现了对密集点云的选择性下采样，在保持场景保真度的同时显著减少高斯点数量。跨不同基准的实验表明，SDI-GS可减少高达50%的高斯点数量，并在PSNR和SSIM上实现相当或更优的渲染质量，同时LPIPS指标仅有轻微下降。它还实现了更快的训练速度和更低的内存占用，推进了3DGS在受限视图场景中的实用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决稀疏视角3D高斯溅射（3DGS）中的初始化效率问题。传统方法依赖运动恢复结构（SfM）在稀疏视角下不稳定，而无SfM方法则生成过多高斯分布导致高内存成本。这个问题在机器人、增强现实和医疗成像等实际应用中至关重要，因为在这些场景中获取密集视图不切实际，限制了3DGS在资源受限场景中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有SfM方法在稀疏视角下不可靠，而无SfM方法效率低下。他们借鉴了自己在2D高斯回归方面的先前工作，证明基于区域的分割可有效减少冗余同时保留重要结构。他们将这种分割驱动范式从2D扩展到3D高斯溅射领域，利用跨视图区域一致性指导选择性下采样。具体借鉴了MASt3R进行姿态估计和点云生成，以及使用修改的DBSCAN算法（MDBSCAN）进行高效区域分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用基于区域的分割识别并保留结构上重要的区域，对密集点云进行选择性下采样，在保持场景保真度的同时减少高斯数量。整体流程包括：1)使用MASt3R估计相机姿态和生成密集点云；2)对每个视图执行基于区域的分割；3)构建感知3D标记向量来识别跨视图一致区域；4)在每个结构聚类内进行分层采样；5)用下采样点初始化3D高斯并联合优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：分割驱动的初始化策略减少高斯数量达50%；高效的表示学习保持高质量渲染；更快的训练速度和更低内存占用；利用跨视图一致性识别有意义区域。相比SfM方法，避免了稀疏视角下的不稳定性；相比其他SfM-free方法，使用智能分割指导下采样而非简单置信度过滤；相比训练期使用分割的方法，在初始化阶段使用轻量级区域分割，不依赖语义标签或逐步密集化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种分割驱动的初始化方法，通过基于区域的分割和智能下采样，显著减少了稀疏视角3D高斯溅射中的高斯数量和内存需求，同时保持了高质量的渲染效果和快速的训练速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse-view synthesis remains a challenging problem due to the difficulty ofrecovering accurate geometry and appearance from limited observations. Whilerecent advances in 3D Gaussian Splatting (3DGS) have enabled real-timerendering with competitive quality, existing pipelines often rely onStructure-from-Motion (SfM) for camera pose estimation, an approach thatstruggles in genuinely sparse-view settings. Moreover, several SfM-free methodsreplace SfM with multi-view stereo (MVS) models, but generate massive numbersof 3D Gaussians by back-projecting every pixel into 3D space, leading to highmemory costs. We propose Segmentation-Driven Initialization for GaussianSplatting (SDI-GS), a method that mitigates inefficiency by leveragingregion-based segmentation to identify and retain only structurally significantregions. This enables selective downsampling of the dense point cloud,preserving scene fidelity while substantially reducing Gaussian count.Experiments across diverse benchmarks show that SDI-GS reduces Gaussian countby up to 50% and achieves comparable or superior rendering quality in PSNR andSSIM, with only marginal degradation in LPIPS. It further enables fastertraining and lower memory footprint, advancing the practicality of 3DGS forconstrained-view scenarios.</description>
      <author>example@mail.com (Yi-Hsin Li, Thomas Sikora, Sebastian Knorr, Måarten Sjöström)</author>
      <guid isPermaLink="false">2509.11853v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations</title>
      <link>http://arxiv.org/abs/2509.11783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 8th International Conference on Robotics, Control  and Automation Engineering (RCAE 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种增强现实(AR)增强的机器人远程操作系统，通过结合AR控制和空间点云渲染，实现了直观、无接触的机器人演示收集。该系统已在ABB机器人平台上验证，研究表明增强感知显著提高了任务性能和用户体验。&lt;h4&gt;背景&lt;/h4&gt;传统工业机器人编程复杂且耗时，需要专家程序员花费数周甚至数月时间。虽然通过演示编程(PbD)提供了一种更易选择的替代方案，但机器人控制和演示收集的直观界面仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种增强现实(AR)增强的机器人远程操作系统，结合基于AR的控制和空间点云渲染，实现直观、无接触的演示，使操作员能够远程控制机器人而无需进入工作空间或使用传统工具。&lt;h4&gt;方法&lt;/h4&gt;提出并实现了一个结合AR控制和空间点云渲染的系统，使操作员能够远程控制机器人。该系统在ABB机器人平台上进行了演示，特别是在IRB 1200工业机器人和GoFa 5协作机器人上进行了验证。通过用户研究评估了实时环境感知（有点云渲染和无点云渲染）对任务完成的影响。&lt;h4&gt;主要发现&lt;/h4&gt;增强感知显著提高了28%的任务性能，并改善了用户体验，系统可用性量表(SUS)得分提高了12%。实时环境感知对任务完成准确性、效率和用户信心有积极影响。&lt;h4&gt;结论&lt;/h4&gt;这项工作有助于推进工业环境中用于演示收集的直观机器人远程操作、AR界面设计、环境感知和远程操作安全机制。收集的演示可作为机器学习应用的有价值训练数据。&lt;h4&gt;翻译&lt;/h4&gt;传统工业机器人编程通常复杂且耗时，通常需要专家程序员花费数周甚至数月的努力。虽然通过演示编程(PbD)提供了更易选择的替代方案，但机器人控制和演示收集的直观界面仍然具有挑战性。为此，我们提出了一种增强现实(AR)增强的机器人远程操作系统，将基于AR的控制与空间点云渲染相结合，实现直观、无接触的演示。这种方法允许操作员远程控制机器人，无需进入工作空间或使用示教器等传统工具。所提出的系统具有通用性，已在ABB机器人平台上进行了演示，特别是在IRB 1200工业机器人和GoFa 5协作机器人上进行了验证。用户研究评估了实时环境感知的影响，具体是有点云渲染和无点云渲染两种情况，对任务完成准确性、效率和用户信心的影响。结果表明，增强感知显著提高了28%的任务性能，并通过系统可用性量表(SUS)得分提高12%改善了用户体验。这项工作有助于推进工业环境中用于演示收集的直观机器人远程操作、AR界面设计、环境感知和远程操作安全机制。收集的演示可作为机器学习应用的有价值训练数据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决工业机器人编程复杂、耗时的问题，以及通过演示编程(PbD)时缺乏直观界面的问题。这个问题很重要，因为传统机器人编程需要专家花费数周甚至数月，而现有方法要么局限于昂贵的协作机器人，要么存在安全风险，限制了工业机器人的广泛应用和人机协作的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统编程和现有PbD方法的局限性，然后探索了AR/VR技术在机器人控制中的应用潜力。他们借鉴了多项现有研究，如Rivera-Pinto的全息球体轨迹定义、Thormann的手势控制、Smith和Van Haastregt的数字末端执行器代理控制、Pizzagalli的数字孪生环境路径规划等，但针对这些方法的不足进行了改进，设计了一个模块化的AR遥操作系统，结合实时点云渲染增强环境感知。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用增强现实技术创建直观界面，使用户能远程控制机器人而不需进入工作空间或使用传统工具。系统由四个模块组成：机器人实时控制模块(EGM接口)、Unity AR系统模块(可视化与交互)、空间点云渲染模块(深度传感器)和机器人监控模块(RWS协议)。工作流程是用户通过HMD在AR界面演示动作，Unity处理数据并传输给物理机器人，同时点云渲染提供环境感知，机器人状态实时反馈给用户。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)适用于工业机器人和协作机器人的通用AR遥操作系统；2)集成AR控制与实时点云渲染增强视觉反馈；3)用户研究评估环境感知影响。相比之前工作，不同之处在于：确保轨迹可行性(不同于Rivera-Pinto的不验证可达性)、支持更广泛运动(不同于Thormann的手势限制)、减轻距离和物体大小限制(不同于Smith和Van Haastregt的直接视觉依赖)、无需预先建模适应动态环境(不同于Pizzagalli的数字孪生)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文开发并验证了一个基于增强现实的机器人遥操作系统，通过集成实时点云渲染，使用户能够直观、安全地远程控制工业机器人进行任务演示，显著提高了任务效率和用户体验。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional industrial robot programming is often complex and time-consuming,typically requiring weeks or even months of effort from expert programmers.Although Programming by Demonstration (PbD) offers a more accessiblealternative, intuitive interfaces for robot control and demonstrationcollection remain challenging. To address this, we propose an Augmented Reality(AR)-enhanced robot teleoperation system that integrates AR-based control withspatial point cloud rendering, enabling intuitive, contact-free demonstrations.This approach allows operators to control robots remotely without entering theworkspace or using conventional tools like the teach pendant. The proposedsystem is generally applicable and has been demonstrated on ABB robotplatforms, specifically validated with the IRB 1200 industrial robot and theGoFa 5 collaborative robot. A user study evaluates the impact of real-timeenvironmental perception, specifically with and without point cloud rendering,on task completion accuracy, efficiency, and user confidence. Results indicatethat enhanced perception significantly improves task performance by 28% andenhances user experience, as reflected by a 12% increase in the SystemUsability Scale (SUS) score. This work contributes to the advancement ofintuitive robot teleoperation, AR interface design, environmental perception,and teleoperation safety mechanisms in industrial settings for demonstrationcollection. The collected demonstrations may serve as valuable training datafor machine learning applications.</description>
      <author>example@mail.com (Shiqi Gong, Sebastian Zudaire, Chi Zhang, Zhen Li)</author>
      <guid isPermaLink="false">2509.11783v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning</title>
      <link>http://arxiv.org/abs/2509.11594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Jizhuo Chen and Diwen Liu contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GBPP是一种基于快速学习的评分器，用于从单个RGB-D快照中选择机器人抓取的基础姿态。该方法采用两阶段课程学习，结合简单启发式方法和高保真模拟优化，实现快速、安全的基础姿态选择。&lt;h4&gt;背景&lt;/h4&gt;机器人抓取任务中，从单个RGB-D图像选择合适的基础姿态具有挑战性。传统方法可能需要复杂的任务和运动优化，或仅依赖简单几何信息，效果有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、高效的方法，从单个RGB-D图像选择机器人抓取基础姿态，使其能够安全、可靠到达目标位置，并在出错时优雅降级。&lt;h4&gt;方法&lt;/h4&gt;1. 两阶段课程学习：第一阶段使用距离-可见性规则低成本自动标记大型数据集；第二阶段使用高保真模拟试验优化模型。2. 采用PointNet++风格点云编码器与多层感知机对候选姿态密集网格评分。3. 实现快速在线选择，无需完整任务和运动优化。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在模拟和真实移动机械臂上，GBPP优于仅依赖接近度和几何的基线方法。2. GBPP选择的基础姿态更安全、更易到达。3. 出错时能优雅降级而非完全失败。4. 实现了数据高效、几何感知的基础定位。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了一种实用的基础定位方法：使用廉价启发式方法获得广泛覆盖，然后通过有针对性的模拟进行校准，结合了简单高效和精确优化的优点。&lt;h4&gt;翻译&lt;/h4&gt;GBPP是一种基于快速学习的评分器，用于从单个RGB-D快照中选择机器人抓取的基础姿态。该方法采用两阶段课程：(1)使用简单的距离-可见性规则低成本自动标记大型数据集；(2)使用更小规模的高保真模拟试验来优化模型以匹配真实的抓取结果。PointNet++风格的点云编码器与多层感知机一起对候选姿态的密集网格进行评分，实现了快速在线选择，无需完整的任务和运动优化。在模拟和真实的移动机械臂上，GBPP优于仅依赖接近度和几何的基线方法，选择更安全、更易到达的姿态，并且在出错时能优雅降级。研究结果为数据高效、几何感知的基础定位提供了实用的方法：使用廉价启发式方法覆盖，然后通过有针对性的模拟进行校准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决移动机器人在杂乱环境中如何适当放置基座以成功抓取目标物体的问题。这个问题在现实中很重要，因为当前模块化系统往往忽略基座放置与机械臂可达性之间的协调，导致抓取失败；而几何方法计算成本高，任务和运动规划难以实时部署，限制了机器人在家庭、仓库等实际环境中的抓取效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：模块化系统忽略基座放置挑战，几何方法计算成本高，TAMP难以实时部署。然后他们将问题重新定义为候选姿态的二分类问题，考虑到直接使用大规模仿真训练成本极高，设计了结合廉价启发式数据和高保真仿真的两阶段方法。他们借鉴了PointNet++作为点云编码器，使用ProcTHOR和ManiSkill等现有仿真环境，但通过创新的两阶段课程学习解决了数据稀缺问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合廉价启发式数据的大规模覆盖和高保真仿真数据的高精度，通过两阶段学习实现数据高效的几何感知基座放置。整体流程包括：1)任务定义，输入RGB-D观测和机器人参数，输出最优基座位置；2)第一阶段使用距离-可见性启发式自动标记180k训练样本；3)第二阶段使用约12k高保真仿真样本微调模型；4)推理时评估所有候选位置，选择最佳位置执行；5)在仿真和真实环境中评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段课程学习框架，结合启发式和仿真数据优势；2)启发式自标记方法，大幅降低数据收集成本；3)数据高效的学习方法，在更少高质量数据下实现更好性能；4)实时性能，0.3秒内评估600个候选姿态。相比之前工作，不同于模块化系统的分离设计，直接从RGB-D预测基座位置；不同于几何方法的高计算成本，学习处理复杂环境；不同于TAMP的实时部署困难，支持快速在线规划；不同于纯学习的高数据需求，通过两阶段解决数据稀缺问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种两阶段课程学习方法，结合廉价启发式数据的大规模覆盖和高保真仿真数据的高精度，实现了数据高效的几何感知基座放置，显著提高了机器人在复杂环境中的抓取成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GBPP is a fast learning based scorer that selects a robot base pose forgrasping from a single RGB-D snapshot. The method uses a two stage curriculum:(1) a simple distance-visibility rule auto-labels a large dataset at low cost;and (2) a smaller set of high fidelity simulation trials refines the model tomatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLPscores dense grids of candidate poses, enabling rapid online selection withoutfull task-and-motion optimization. In simulation and on a real mobilemanipulator, GBPP outperforms proximity and geometry only baselines, choosingsafer and more reachable stances and degrading gracefully when wrong. Theresults offer a practical recipe for data efficient, geometry aware baseplacement: use inexpensive heuristics for coverage, then calibrate withtargeted simulation.</description>
      <author>example@mail.com (Jizhuo Chen, Diwen Liu, Jiaming Wang, Harold Soh)</author>
      <guid isPermaLink="false">2509.11594v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</title>
      <link>http://arxiv.org/abs/2509.11453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TrajTrack的新型基于轨迹的LiDAR 3D单目标跟踪框架，通过从历史边界框轨迹中隐式学习运动连续性，在保持高效率的同时显著提高了跟踪精度。&lt;h4&gt;背景&lt;/h4&gt;LiDAR-based 3D单目标跟踪是机器人和自主系统中的关键任务。现有方法中，两帧方法效率高但缺乏长期时间上下文，在稀疏或遮挡场景中容易失效；而基于序列的方法处理多个点云，计算成本高但鲁棒性强。&lt;h4&gt;目的&lt;/h4&gt;解决两帧方法和序列方法之间的权衡问题，提出一种新的基于轨迹的范式，在保持高效率的同时增强跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;TrajTrack是一个轻量级框架，通过从历史边界框轨迹中隐式学习运动连续性来增强基础两帧跟踪器，无需额外点云输入。它首先生成快速显式的运动建议，然后使用隐式运动建模模块预测未来轨迹，完善和纠正初始建议。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes基准测试上，TrajTrack实现了新的最先进性能，相比强大基线提高跟踪精度4.48%，同时保持56 FPS的运行速度，并展示了在不同基础跟踪器上的强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;TrajTrack成功解决了两帧方法和序列方法之间的权衡问题，在保持高效率的同时显著提高了跟踪精度，为LiDAR 3D单目标跟踪提供了新范式。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的3D单目标跟踪是机器人和自主系统中的关键任务。现有方法通常遵循逐帧运动估计或基于序列的范式。然而，两帧方法效率高但缺乏长期时间上下文，在稀疏或遮挡场景中容易失效，而处理多个点云的序列方法则以显著的计算成本获得鲁棒性。为解决这一困境，我们提出了一种新的基于轨迹的范式及其实现TrajTrack。TrajTrack是一个轻量级框架，通过仅从历史边界框轨迹中隐式学习运动连续性来增强基础两帧跟踪器，不需要额外昂贵的点云输入。它首先生成快速、显式的运动建议，然后使用隐式运动建模模块预测未来轨迹，进而完善和纠正初始建议。在大型NuScenes基准上的大量实验表明，TrajTrack实现了新的最先进性能，相比强大基线显著提高4.48%的跟踪精度，同时以56 FPS的速度运行。此外，我们还展示了TrajTrack在不同基础跟踪器上的强大泛化能力。视频可在https://www.bilibili.com/video/BV1ahYgzmEWP观看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是点云跟踪中效率与鲁棒性之间的权衡问题。现有的两帧方法效率高但缺乏长期时间上下文，在稀疏或遮挡场景中表现不佳；而序列方法虽能增强鲁棒性但计算成本高，不适合实时应用。这个问题在自动驾驶和机器人系统中至关重要，因为LiDAR点云跟踪是这些系统的核心任务，需要在保持实时性的同时应对各种复杂环境挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后从轨迹预测领域获得灵感，意识到可以从物体历史轨迹中学习运动连续性而不需要处理高带宽点云数据。他们设计了一个结合短期显式运动和长期隐式连续性的框架。方法借鉴了现有工作：显式运动部分参考了P2P等两帧跟踪方法；隐式轨迹预测部分借鉴了VectorNet、AgentFormer等使用Transformer建模序列数据的方法；整体框架受到SeqTrack3D等序列方法的启发，但通过只使用历史边界框而非完整点云序列来降低计算成本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过利用历史边界框轨迹学习物体长期运动连续性，在不增加大量计算成本的情况下提高跟踪鲁棒性。整体实现采用两阶段'提出-预测-细化'流程：1)显式运动提案：使用两帧模型处理点云，生成初始跟踪提案；2)隐式轨迹预测：创新性地使用仅历史边界框序列的隐式运动建模(IMM)模块，用TrajFormer架构预测未来轨迹；3)轨迹引导的提案细化：根据两个提案间的IoU动态选择，智能校正初始提案，输出最终跟踪结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于轨迹的范式，利用历史边界框融入长期运动连续性；2)隐式运动建模(IMM)模块，仅用轻量级历史边界框序列学习长期运动模式；3)两阶段处理流程，结合短期显式和长期隐式运动线索。相比之前工作：与两帧方法不同，它考虑长期连续性，在稀疏场景表现更好；与序列方法不同，它不处理多帧点云，计算成本低且能实时运行(56 FPS)；与其他轨迹预测方法不同，它专为3D跟踪设计，并提出了轨迹引导的提案细化机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrajTrack通过引入基于轨迹的范式和隐式运动建模模块，有效结合了短期显式运动和长期隐式连续性的优势，在保持实时性能的同时显著提升了点云跟踪的鲁棒性，实现了前所未有的性能和效率平衡。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D single object tracking (3D SOT) is a critical task in roboticsand autonomous systems. Existing methods typically follow frame-wise motionestimation or a sequence-based paradigm. However, the two-frame methods areefficient but lack long-term temporal context, making them vulnerable in sparseor occluded scenes, while sequence-based methods that process multiple pointclouds gain robustness at a significant computational cost. To resolve thisdilemma, we propose a novel trajectory-based paradigm and its instantiation,TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frametracker by implicitly learning motion continuity from historical bounding boxtrajectories alone-without requiring additional, costly point cloud inputs. Itfirst generates a fast, explicit motion proposal and then uses an implicitmotion modeling module to predict the future trajectory, which in turn refinesand corrects the initial proposal. Extensive experiments on the large-scaleNuScenes benchmark show that TrajTrack achieves new state-of-the-artperformance, dramatically improving tracking precision by 4.48% over a strongbaseline while running at 56 FPS. Besides, we also demonstrate the stronggeneralizability of TrajTrack across different base trackers. Video isavailable at https://www.bilibili.com/video/BV1ahYgzmEWP.</description>
      <author>example@mail.com (BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang)</author>
      <guid isPermaLink="false">2509.11453v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3D Gaussian Modeling and Ray Marching of OpenVDB datasets for Scientific Visualization</title>
      <link>http://arxiv.org/abs/2509.11377v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了3D高斯模型在科学可视化领域的应用，提出使用OpenVDB格式作为3D高斯粒子的建模框架，实现高效体积数据压缩，并开发基于光线积分的渲染算法进行光学深度累积计算。&lt;h4&gt;背景&lt;/h4&gt;3D高斯模型正被广泛研究用于场景建模和压缩。在科学可视化领域，大多数流行格式是密集网格数据结构，存储每个网格单元而忽略其贡献。OpenVDB库和数据格式专为稀疏体积数据设计，通过屏蔽空单元格避免存储它们，最初用于视觉效果如云、火和流体等。&lt;h4&gt;目的&lt;/h4&gt;探索在科学可视化中使用OpenVDB作为建模框架，转换为3D高斯粒子以实现进一步压缩，为不同科学体积类型提供统一建模方法，利用OpenVDB的压缩优势作为3D高斯模型的起点。&lt;h4&gt;方法&lt;/h4&gt;在OptiX 8.1中实现基于光线积分的渲染算法，计算3D高斯沿光线的贡献用于光学深度累积；实现SciVis风格的主光线仅NanoVDB HDDA基于光线步进器用于OpenVDB体素网格，以比较渲染结果；探索将此高斯模型应用于非规则网格体积格式如AMR体积和点云，使用OpenVDB网格类类型的内部表示用于数据层次结构。&lt;h4&gt;主要发现&lt;/h4&gt;使用OpenVDB作为3D高斯模型的起点提供了非平凡的压缩优势；成功实现了基于光线积分的渲染算法，能够计算3D高斯沿光线的贡献；开发了SciVis风格的主光线仅NanoVDB HDDA基于光线步进器用于性能比较。&lt;h4&gt;结论&lt;/h4&gt;OpenVDB在科学可视化场景中的应用具有可行性；3D高斯模型为体积数据压缩和渲染提供了新途径；高斯模型可扩展应用于非规则网格体积格式。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯模型目前因其场景建模和压缩能力而被广泛研究。在3D体积中，其使用正被探索用于以尽可能稀疏的方式表示密集体积。然而，这些方法大多开始于内存效率低下的数据格式。特别是在科学可视化(SciVis)领域，大多数流行格式是密集网格数据结构，存储每个网格单元，无论其贡献如何。OpenVDB库和数据格式被引入用于专门表示稀疏体积数据，用于视觉效果用例如云、火、流体等。它通过在存储过程中屏蔽空单元格来避免存储它们。这为在SciVis中使用提供了机会，特别是作为转换为3D高斯粒子的建模框架以实现进一步压缩，以及为不同科学体积类型提供统一建模方法。这种压缩起点是非平凡的，本文希望提出一种基于光线积分的渲染算法，在OptiX 8.1中实现，用于计算沿光线的3D高斯贡献以进行光学深度累积。为了比较我们光线步进高斯渲染器的渲染结果，我们还实现了SciVis风格的主光线仅NanoVDB HDDA基于光线步进器，用于OpenVDB体素网格。最后，本文还探索了将此高斯模型应用于非规则网格的体积格式，如AMR体积和点云，使用OpenVDB网格类类型的内部表示用于数据层次结构和细分结构。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将OpenVDB科学可视化数据集高效地转换为3D高斯模型并进行渲染的问题。这个问题很重要，因为科学可视化中常用的密集网格数据结构存储效率低下，而OpenVDB虽然能高效存储稀疏体积数据，但传统渲染方法仍有局限。3D高斯模型提供了一种潜在的压缩和统一表示方法，能够支持不同类型的科学体积数据，提高渲染效率同时保持视觉质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了科学可视化中体积数据处理的挑战，特别是稀疏数据的存储和渲染效率问题。他们注意到OpenVDB在处理稀疏体积数据方面的优势，以及3D高斯模型在场景表示和压缩方面的潜力。作者设计方法时借鉴了多项现有工作，包括OpenVDB库本身、3D高斯建模研究(如3DGS)、光线行进技术、线积分方法以及OptiX 8.1框架。作者的创新在于将这些技术结合起来，专门针对科学可视化的需求，提出了从OpenVDB到3D高斯的转换和渲染方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将OpenVDB中的稀疏体积数据转换为3D高斯表示，每个高斯代表一个体素或一组体素，然后使用光线行进技术穿过这些高斯，通过线积分计算光学深度，最后应用传递函数将累积的光学深度转换为最终的颜色和透明度。整体流程包括：1)加载OpenVDB数据集；2)遍历OpenVDB叶节点，根据密集或稀疏特性使用不同策略生成3D高斯；3)为每个高斯创建AABB并构建BVH加速结构；4)使用OptiX框架进行光线追踪，计算光学深度贡献；5)累积光学深度并应用传递函数获得最终颜色。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出将OpenVDB科学可视化数据格式转换为3D高斯模型；2)传递函数无关的建模方法，允许运行时动态调整传递函数；3)提供多层次细节控制，平衡渲染质量和性能；4)支持多种科学数据格式(规则网格、AMR体积、点云)的统一高斯表示；5)使用线积分方法计算3D高斯沿光线的贡献。相比之前工作，本文不依赖训练循环或图像监督，直接从原始体积数据生成高斯；使用3D高斯而非体素作为基本渲染单元；提供完整的体积覆盖而非仅表示表面；为多种科学数据格式提供统一的渲染框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种将OpenVDB科学可视化数据转换为3D高斯模型的方法，通过光线行进和线积分实现高效渲染，支持多种数据格式和动态传递函数调整，为科学可视化提供了统一的体积数据表示和渲染框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussians are currently being heavily investigated for their scenemodeling and compression abilities. In 3D volumes, their use is being exploredfor representing dense volumes as sparsely as possible. However, most of thesemethods begin with a memory inefficient data format. Specially in ScientificVisualization(SciVis), where most popular formats are dense-grid datastructures that store every grid cell, irrespective of its contribution.OpenVDB library and data format were introduced for representing sparsevolumetric data specifically for visual effects use cases such as clouds, fire,fluids etc. It avoids storing empty cells by masking them during storage. Itpresents an opportunity for use in SciVis, specifically as a modeling frameworkfor conversion to 3D Gaussian particles for further compression and for aunified modeling approach for different scientific volume types. Thiscompression head-start is non-trivial and this paper would like to present thiswith a rendering algorithm based on line integration implemented in OptiX8.1for calculating 3D Gaussians contribution along a ray for optical-depthaccumulation. For comparing the rendering results of our ray marching Gaussiansrenderer, we also implement a SciVis style primary-ray only NanoVDB HDDA basedray marcher for OpenVDB voxel grids. Finally, this paper also exploresapplication of this Gaussian model to formats of volumes other than regulargrids, such as AMR volumes and point clouds, using internal representation ofOpenVDB grid class types for data hierarchy and subdivision structure.</description>
      <author>example@mail.com (Isha Sharma, Dieter Schmalstieg)</author>
      <guid isPermaLink="false">2509.11377v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Limit Theorems for Verbose Persistence Diagrams</title>
      <link>http://arxiv.org/abs/2509.11256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作启动了对随机冗长图的研究，建立了随机冗长图的大数定律，证明了极限冗长图的存在性，刻画了其支撑并计算总质量，同时扩展了持久贝蒂数的概念并揭示了其与冗长图的关系。&lt;h4&gt;背景&lt;/h4&gt;持久图是持久同调研究的中心对象，也在随机拓扑背景下被研究。冗长图是对持久图的精化，通过沿对角线添加额外点来包含瞬时持久特征。&lt;h4&gt;目的&lt;/h4&gt;研究随机冗长图，建立其大数定律，证明极限冗长图的存在性，刻画其支撑并计算总质量，扩展持久贝蒂数概念并研究其与冗长图的关系。&lt;h4&gt;方法&lt;/h4&gt;通过分析随机点云的增长，研究冗长图的极限行为，将持久贝蒂数概念扩展，并研究其渐进行为。&lt;h4&gt;主要发现&lt;/h4&gt;随着随机点云规模扩大，存在极限冗长图，可视为对上半平面(对角线及其上方)上的一个测度；刻画了极限冗长图的支撑并计算了其总质量；揭示了扩展持久贝蒂数概念与冗长图之间的关系；建立了关于扩展持久贝蒂数渐进行为的结果。&lt;h4&gt;结论&lt;/h4&gt;这项工作成功地将Hiraoka、Shirai和Trinh及其后续Shirai和Suzaki的主要结果扩展到了冗长图的设定中，为随机冗长图研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;持久图是持久同调研究中的中心对象，也在随机拓扑的背景下被研究。新近提出的冗长图(也称为冗长条形码)是对持久图的精化，通过沿对角线添加额外点来包含瞬时持久特征。在这项工作中，我们启动了对随机冗长图的研究。我们建立了随机冗长图的大数定律，证明随着随机点云规模的扩大，存在一个极限冗长图，可以看作是对上半平面(对角线及其上方)上的一个测度。同时，我们刻画了其支撑并计算了其总质量。在此过程中，我们扩展了持久贝蒂数的概念，揭示了这种扩展概念与冗长图之间的关系，并建立了关于扩展持久贝蒂数渐进行为的结果。这项工作将Hiraoka、Shirai和Trinh及其后续Shirai和Suzaki的主要结果扩展到了冗长图的设定中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The persistence diagram is a central object in the study of persistenthomology and has also been investigated in the context of random topology. Themore recent notion of the verbose diagram (a.k.a. verbose barcode) is arefinement of the persistence diagram that is obtained by incorporatingephemeral persistence features as extra points along the diagonal. In thiswork, we initiate the study of random verbose diagrams. We establish a stronglaw of large numbers for verbose diagrams as a random point cloud grows in size-- that is, we prove the existence of a limiting verbose diagram, viewed as ameasure on the half-plane on and above the diagonal. Also, we characterize itssupport and compute its total mass. Along the way, we extend the notion of thepersistent Betti number, reveal the relation between this extended notion andthe verbose diagram (which is an extension of the fundamental lemma ofpersistent homology), and establish results on the asymptotic behavior of theextended persistent Betti numbers.  This work extends the main results of the work by Hiraoka, Shirai, and Trinhand its sequel by Shirai and Suzaki to the setting of verbose diagrams.</description>
      <author>example@mail.com (Jeong-hwi Joe, Woojin Kim, Cheolwoo Park)</author>
      <guid isPermaLink="false">2509.11256v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images</title>
      <link>http://arxiv.org/abs/2509.11164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新颖的轻量级可扩展学习框架，通过分析珊瑚的2D多视图RGB图像来预测其3D体积和表面积，用于珊瑚礁监测。&lt;h4&gt;背景&lt;/h4&gt;珊瑚礁监测需要量化珊瑚生长，这需要准确估算珊瑚的体积和表面积。然而，由于珊瑚形态复杂，这是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从二维多视图RGB图像预测珊瑚状物体三维体积和表面积的轻量级可扩展学习框架。&lt;h4&gt;方法&lt;/h4&gt;使用预训练模块VGGT从每个视图中提取密集点图，合并为统一点云并添加置信度分数，通过两个并行的DGCNN解码器头部输出体积和表面积及其置信度估计，并引入基于高斯负对数似然的复合损失函数以增强预测稳定性和提供不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;该方法实现了具有竞争力的准确性，并对未见过的珊瑚形态具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该框架为直接从稀疏图像集合中高效可扩展的珊瑚几何估算铺平了道路，在珊瑚生长分析和珊瑚礁监测方面具有潜在应用。&lt;h4&gt;翻译&lt;/h4&gt;有效的珊瑚礁监测需要通过精确的体积和表面积估算来量化珊瑚生长，但由于珊瑚形态复杂，这是一项具有挑战性的任务。我们提出了一种新颖的轻量级可扩展学习框架，通过从二维多视图RGB图像预测珊瑚状物体的三维体积和表面积来解决这一挑战。我们的方法使用预训练模块从每个视图中提取密集点图；这些点图被合并为统一的点云，并添加了每个视图的置信度分数。将结果点云输入到两个并行的DGCNN解码器头部，它们共同输出珊瑚的体积和表面积，以及它们相应的置信度估计。为了增强预测稳定性并提供不确定性估计，我们在实数和对数域中引入了基于高斯负对数似然的复合损失函数。我们的方法实现了具有竞争力的准确性，并对未见过的形态具有良好的泛化能力。该框架为直接从稀疏图像集合中高效可扩展的珊瑚几何估算铺平了道路，在珊瑚生长分析和珊瑚礁监测方面具有潜在应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从稀疏的多视角2D RGB图像中准确估算珊瑚体积和表面积的问题。这个问题很重要，因为珊瑚礁是海洋生态系统健康的重要指标，跟踪珊瑚体积和表面积对理解生长趋势、检测退化及指导保护工作至关重要。传统方法需要昂贵设备、大量人工处理且不可扩展，而现有方法在稀疏视角或遮挡情况下会产生不准确结果，严重影响监测可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了珊瑚形态复杂性和传统3D重建方法的局限性，然后借鉴了多视角技术在无需完整3D监督下估计几何形状的成功经验。他们利用VGGT视觉几何基础Transformer框架提取密集点图，并采用DGCNN动态图卷积网络作为解码器。方法设计考虑了珊瑚形态的极端变异性，通过混合损失函数处理不确定性，并针对稀疏视角情况进行了优化，体现了对现有计算机视觉和深度学习技术的创造性应用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接从稀疏多视角2D图像预测珊瑚体积和表面积，无需传统3D网格重建。整体流程分为三部分：1) 珊瑚数据集生成：合成watertight珊瑚网格并渲染多视角图像；2) 特征提取：使用VGGT从掩码图像提取密集点图并转换为带置信度的3D点云；3) 解码器处理：两个并行的DGCNN解码器分别预测体积和表面积及其置信度。训练时采用混合损失函数，结合高斯负对数似然和确定性误差指标，提高预测稳定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 轻量级可扩展的自动化管道，无需3D网格监督；2) 利用VGGT提取特征和DGCNN解码器预测几何属性；3) 设计混合损失函数处理不同珊瑚尺度的不确定性；4) 解决传统网格重建的伪影问题。相比Trellis方法，论文在体积估计上MAPE从60.81%降至10.27%，表面积从55.41%降至7.56%。与现有珊瑚监测方法不同，论文直接针对体积和表面积估计，而非仅关注覆盖率或地形指标。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种轻量级、可扩展的深度学习框架，能够从稀疏的多视角2D RGB图像中直接、准确地估计珊瑚的体积和表面积，克服了传统3D重建方法的局限性，为大尺度珊瑚监测和保护提供了高效工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective reef monitoring requires the quantification of coral growth viaaccurate volumetric and surface area estimates, which is a challenging task dueto the complex morphology of corals. We propose a novel, lightweight, andscalable learning framework that addresses this challenge by predicting the 3Dvolume and surface area of coral-like objects from 2D multi-view RGB images.Our approach utilizes a pre-trained module (VGGT) to extract dense point mapsfrom each view; these maps are merged into a unified point cloud and enrichedwith per-view confidence scores. The resulting cloud is fed to two parallelDGCNN decoder heads, which jointly output the volume and the surface area ofthe coral, as well as their corresponding confidence estimate. To enhanceprediction stability and provide uncertainty estimates, we introduce acomposite loss function based on Gaussian negative log-likelihood in both realand log domains. Our method achieves competitive accuracy and generalizes wellto unseen morphologies. This framework paves the way for efficient and scalablecoral geometry estimation directly from a sparse set of images, with potentialapplications in coral growth analysis and reef monitoring.</description>
      <author>example@mail.com (Diego Eustachio Farchione, Ramzi Idoughi, Peter Wonka)</author>
      <guid isPermaLink="false">2509.11164v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Maximum diversity, weighting and invariants of time series</title>
      <link>http://arxiv.org/abs/2509.11146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了magnitude（量级）的连续性及其在数据分析中的应用，特别是提出了一种用于周期时间序列分析的新不变量，并通过机器学习实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;Magnitude作为富范畴欧拉特征的特殊情况，代表了度量空间的大小感，与基数、维度和体积等经典概念相关。虽然已有研究从多角度解释了magnitude的意义，但连续性为理解magnitude提供了新视角。&lt;h4&gt;目的&lt;/h4&gt;基于magnitude和最大多样性的连续性已有结果，本文重点关注加权的连续性及其与最大多样性的关系，并将magnitude理论应用于时间序列分析。&lt;h4&gt;方法&lt;/h4&gt;应用magnitude理论到表示数据或模型参数的点云上，利用连续性结果推导周期时间序列的新不变量，并进行机器学习实验验证其性能。&lt;h4&gt;主要发现&lt;/h4&gt;加权具有连续性，其变化与最大多样性相关；magnitude理论与数据分析有密切联系；提出的周期时间序列不变量直接基于连续性结果，并在实验中改善了性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入基于magnitude连续性的新不变量，有效提升了时间序列分析的性能，展示了magnitude理论在数据科学中的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;Magnitude作为富范畴欧拉特征的特例，代表了度量空间的大小感，并与基数、维度和体积等经典概念相关。虽然研究已从多角度解释了magnitude的意义，但连续性也为magnitude提供了有价值的视角。基于关于magnitude和最大多样性的连续性已有结果，本文重点关注加权的连续性，其总和为magnitude，及其与最大多样性的变化。同时，近期研究通过将magnitude理论应用于表示数据或模型参数的点云，阐明了magnitude与数据分析之间的联系。本文还通过引入周期时间序列的新不变量为时间序列分析提供应用，其中不变性直接来自于连续性结果。作为案例，使用真实数据进行了简单的机器学习实验，结果表明建议的不变量提高了性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnitude, obtained as a special case of Euler characteristic of enrichedcategory, represents a sense of the size of metric spaces and is related toclassical notions such as cardinality, dimension, and volume. While the studieshave explained the meaning of magnitude from various perspectives, continuityalso gives a valuable view of magnitude. Based on established results aboutcontinuity of magnitude and maximum diversity, this article focuses oncontinuity of weighting, a distribution whose totality is magnitude, and itsvariation corresponding to maximum diversity. Meanwhile, recent studies alsoilluminated the connection between magnitude and data analysis by applyingmagnitude theory to point clouds representing the data or the set of modelparameters. This article will also provide an application for time seriesanalysis by introducing a new kind of invariants of periodic time series, wherethe invariance follows directly from the continuity results. As a use-case, asimple machine learning experiment is conducted with real-world data, in whichthe suggested invariants improved the performance.</description>
      <author>example@mail.com (Byungchang So)</author>
      <guid isPermaLink="false">2509.11146v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations</title>
      <link>http://arxiv.org/abs/2509.11125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为ManiVID-3D的新型3D强化学习架构，专门用于机器人操作，通过自监督解耦特征学习实现视角不变的表示。该框架包含ViewNet模块和高效的GPU加速批量渲染模块，能够在不依赖精确相机校准的情况下处理任意视角的点云观测，并实现了前所未有的训练速度。实验表明，该方法在视角变化下的表现优于现有技术，且参数效率更高。&lt;h4&gt;背景&lt;/h4&gt;在真实世界的机器人操作中部署视觉强化学习策略常常受到摄像头视角变化的阻碍。从固定前置摄像头训练的策略在摄像头位置改变时可能会失效，这是真实环境中难以避免的情况，因为传感器位置的适当管理很困难。现有方法通常依赖精确的相机校准或在处理大的透视变化时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决视觉强化学习在真实世界机器人操作中因摄像头视角变化导致的问题，开发一种不依赖精确相机校准且能有效处理大视角变化的3D强化学习架构。&lt;h4&gt;方法&lt;/h4&gt;提出ManiVID-3D，一种用于机器人操作的新型3D强化学习架构，通过自监督解耦特征学习来学习视角不变的表示。框架包含ViewNet模块，这是一个轻量级但有效的模块，能够自动将任意视角的点云观测对齐到统一的空间坐标系，无需外部校准。此外，还开发了一种高效的GPU加速批量渲染模块，每秒可处理超过5000帧，支持前所未有的3D视觉强化学习大规模训练。&lt;h4&gt;主要发现&lt;/h4&gt;在10个模拟任务和5个现实世界任务上的广泛评估表明，该方法在视角变化下的成功率比最先进的方法高44.7%，同时参数减少了80%。系统对严重透视变化的鲁棒性和强大的模拟到现实性能证明了学习几何一致表示对非结构化环境中可扩展机器人操作的有效性。&lt;h4&gt;结论&lt;/h4&gt;ManiVID-3D通过自监督解耦特征学习和ViewNet模块，有效解决了视觉强化学习在机器人操作中因视角变化导致的问题，无需精确的相机校准即可实现高性能。该方法的参数效率高、训练速度快，且在模拟到现实迁移中表现出色，为非结构化环境中的可扩展机器人操作提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在真实世界的机器人操作中部署视觉强化学习策略常常受到摄像头视角变化的阻碍。从固定前置摄像头训练的策略在摄像头位置改变时可能会失效——这是真实环境中难以避免的情况，因为传感器位置的适当管理很困难。现有方法通常依赖精确的相机校准或在处理大的透视变化时表现不佳。为了解决这些局限性，我们提出了ManiVID-3D，一种用于机器人操作的新型3D强化学习架构，通过自监督解耦特征学习来学习视角不变的表示。该框架包含ViewNet模块，这是一个轻量级但有效的模块，能够自动将任意视角的点云观测对齐到统一的空间坐标系，无需外部校准。此外，我们还开发了一种高效的GPU加速批量渲染模块，每秒可处理超过5000帧，支持前所未有的3D视觉强化学习大规模训练。在10个模拟任务和5个现实世界任务上的广泛评估表明，该方法在视角变化下的成功率比最先进的方法高44.7%，同时参数减少了80%。系统对严重透视变化的鲁棒性和强大的模拟到现实性能证明了学习几何一致表示对非结构化环境中可扩展机器人操作的有效性。我们的项目网站可以在https://zheng-joe-lee.github.io/manivid3d/找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉强化学习策略在摄像头视角变化时泛化能力不足的问题。当机器人从一个固定视角训练后，如果摄像头位置发生变化（真实世界中不可避免），训练好的策略可能会失效。这个问题在现实中非常重要，因为真实环境中传感器位置很难保持固定，特别是杂乱的家庭或动态工业环境中；视角变化会导致视觉观察中剧烈的几何畸变，使策略泛化变得特别困难；现有方法要么依赖精确相机校准，要么在大视角变化下表现不佳，限制了视觉强化学习在真实世界中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D视觉输入方法难以捕捉3D结构先验知识；深度感知方法依赖易受遮挡的深度图；3D点云方法需要精确相机校准。基于这些分析，作者设计了结合监督视图对齐和自监督特征解耦的方法。他们借鉴了DP3的轻量级MLP架构，对比学习表示解耦的思想，以及PointNet++用于点云变换。作者的创新在于将这些技术有机结合，特别是消除了对相机校准的依赖，实现了视角不变的强化学习策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦的3D表示学习视角不变的视觉强化学习策略，同时学习视角不变特征（包含任务关键信息）和视角相关特征（不影响任务执行）。整体流程包括：1)训练阶段：使用ViewNet对齐不同视角点云；处理点云数据；通过双头编码器提取视角不变和视角相关特征；使用对比学习目标实现特征解耦；2)部署阶段：处理真实世界点云；使用预训练ViewNet和策略网络执行动作；3)高效批量模拟与渲染：GPU加速系统实现大规模训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ManiVID-3D架构：使用解耦的视角不变表示实现大视角变化下的鲁棒操作；2)ViewNet模块：无需校准的多视图对齐，统一不同视角点云坐标；3)高效批量渲染系统：GPU加速实现大规模3D视觉RL训练；4)强大的零样本模拟到现实迁移能力。相比之前工作，不同之处在于：不依赖相机校准；使用真正的3D点云表示而非2D图像；参数减少80%同时提高性能；端到端训练而非两阶段预训练；在±75°极端视角变化下仍保持稳定性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ManiVID-3D通过解耦的3D表示和无需校准的多视图对齐，实现了在极端视角变化下具有强大泛化能力的机器人操作策略，同时显著提高了训练效率和模拟到现实的迁移能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying visual reinforcement learning (RL) policies in real-worldmanipulation is often hindered by camera viewpoint changes. A policy trainedfrom a fixed front-facing camera may fail when the camera is shifted--anunavoidable situation in real-world settings where sensor placement is hard tomanage appropriately. Existing methods often rely on precise camera calibrationor struggle with large perspective changes. To address these limitations, wepropose ManiVID-3D, a novel 3D RL architecture designed for roboticmanipulation, which learns view-invariant representations throughself-supervised disentangled feature learning. The framework incorporatesViewNet, a lightweight yet effective module that automatically aligns pointcloud observations from arbitrary viewpoints into a unified spatial coordinatesystem without the need for extrinsic calibration. Additionally, we develop anefficient GPU-accelerated batch rendering module capable of processing over5000 frames per second, enabling large-scale training for 3D visual RL atunprecedented speeds. Extensive evaluation across 10 simulated and 5 real-worldtasks demonstrates that our approach achieves a 44.7% higher success rate thanstate-of-the-art methods under viewpoint variations while using 80% fewerparameters. The system's robustness to severe perspective changes and strongsim-to-real performance highlight the effectiveness of learning geometricallyconsistent representations for scalable robotic manipulation in unstructuredenvironments. Our project website can be found inhttps://zheng-joe-lee.github.io/manivid3d/.</description>
      <author>example@mail.com (Zheng Li, Pei Qu, Yufei Jia, Shihui Zhou, Haizhou Ge, Jiahang Cao, Jinni Zhou, Guyue Zhou, Jun Ma)</author>
      <guid isPermaLink="false">2509.11125v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment</title>
      <link>http://arxiv.org/abs/2509.11097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了3DAeroRelief，首个专门用于灾后评估的三维基准数据集，使用低成本无人机收集飓风破坏区域的密集三维点云，并通过语义标注提供细粒度结构损伤信息。&lt;h4&gt;背景&lt;/h4&gt;及时评估结构损伤对灾害响应和恢复至关重要，但现有自然灾害分析主要依赖二维图像，存在缺乏深度、遮挡和空间上下文有限等问题；虽然三维语义分割提供了更丰富的替代方案，但现有三维基准数据集主要关注城市或室内场景，很少关注灾害影响区域。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集在灾害场景中的不足，创建首个专门针对灾后评估的三维基准数据集，为灾害响应中的三维场景理解提供资源。&lt;h4&gt;方法&lt;/h4&gt;使用低成本无人机在飓风破坏区域收集数据，通过运动结构(Motion Structure)和多视图立体技术(Multi-View Stereo)重建密集三维点云，并通过手动二维标注并将结果投影到三维空间来生成语义标注。&lt;h4&gt;主要发现&lt;/h4&gt;3DAeroRelief捕捉了具有细粒度结构损伤的大规模户外环境，处于真实世界灾害背景中；无人机能够在危险区域实现经济、灵活和安全的数据收集，特别适合紧急情况。&lt;h4&gt;结论&lt;/h4&gt;通过评估多个最先进的三维分割模型，展示了3DAeroRelief的效用，突出了灾害响应中三维场景理解的挑战和机会；该数据集作为推进灾后场景中稳健三维视觉系统在真实世界应用中有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;及时评估结构损伤对灾害响应和恢复至关重要。然而，之前大多数自然灾害分析工作依赖于二维图像，这些图像缺乏深度、存在遮挡问题，且提供有限的空间上下文。三维语义分割提供了更丰富的替代方案，但现有的三维基准数据集主要关注城市或室内场景，很少关注灾害影响区域。为解决这一差距，我们提出了3DAeroRelief——首个专门用于灾后评估的三维基准数据集。该数据集使用低成本无人机在飓风破坏区域收集，通过运动结构(Motion Structure)和多视图立体技术(Multi-View Stereo)重建密集三维点云。语义标注通过手动二维标注并投影到三维空间生成。与现有数据集不同，3DAeroRelief在真实世界灾害背景下捕捉了具有细粒度结构损伤的大规模户外环境。无人机能够在危险区域实现经济、灵活和安全的数据收集，使其特别适合紧急情况。为展示3DAeroRelief的效用，我们评估了数据集上的几个最先进的三维分割模型，以突显灾害响应中三维场景理解的挑战和机会。我们的数据集作为推进灾后场景中稳健三维视觉系统在真实世界应用中有价值的资源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决灾后评估中缺乏高质量3D数据集的问题。目前大多数灾害分析依赖2D图像，这些图像缺乏深度信息、容易受遮挡且空间上下文有限。这个问题在现实中非常重要，因为及时准确评估灾后结构损坏对灾害响应、资源分配和恢复规划至关重要，直接影响救援效率和灾后重建质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有3D数据集主要集中在城市或室内场景，缺乏灾后特定场景的问题。他们借鉴了先前2D灾害评估数据集(如FloodNet和RescueNet)的思路，但将其扩展到3D领域。技术上，他们采用标准的运动恢复结构(SfM)和多视图立体(MVS)技术进行3D重建，参考了现有的语义标注方法，并使用CloudCompare等工具进行3D编辑，形成了一套完整的从数据采集到标注的流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用无人机收集灾后区域的航拍影像，重建高分辨率3D点云，并进行精细语义标注，创建专门的灾后评估3D基准数据集。整体流程包括：1)数据收集与预处理(无人机拍摄、视频清理和帧提取)；2)3D重建(SfM建立稀疏结构，MVS生成密集点云)；3)后处理(绝对重缩放和区域清理)；4)标注(2D图像标注、标签投影到3D空间和3D精细化调整)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门用于灾后评估的3D基准数据集；2)使用低成本无人机收集数据，提高经济性和安全性；3)提供高分辨率3D点云覆盖大规模灾后区域；4)包含精细的结构损坏标注。相比之前工作，室内数据集(如S3DIS)只覆盖小规模环境，室外数据集(如SemanticKITTI)使用LiDAR生成稀疏点云且主要针对自动驾驶场景，而3DAeroRelief填补了大规模高分辨率灾后3D数据的空白。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3DAeroRelief是首个专门用于灾后评估的3D基准数据集，通过无人机收集高分辨率航拍影像并重建为密集3D点云，为开发更准确、全面的灾后评估3D视觉系统提供了宝贵的资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely assessment of structural damage is critical for disaster response andrecovery. However, most prior work in natural disaster analysis relies on 2Dimagery, which lacks depth, suffers from occlusions, and provides limitedspatial context. 3D semantic segmentation offers a richer alternative, butexisting 3D benchmarks focus mainly on urban or indoor scenes, with littleattention to disaster-affected areas. To address this gap, we present3DAeroRelief--the first 3D benchmark dataset specifically designed forpost-disaster assessment. Collected using low-cost unmanned aerial vehicles(UAVs) over hurricane-damaged regions, the dataset features dense 3D pointclouds reconstructed via Structure-from-Motion and Multi-View Stereotechniques. Semantic annotations were produced through manual 2D labeling andprojected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3Dlarge-scale outdoor environments with fine-grained structural damage inreal-world disaster contexts. UAVs enable affordable, flexible, and safe datacollection in hazardous areas, making them particularly well-suited foremergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluateseveral state-of-the-art 3D segmentation models on the dataset to highlightboth the challenges and opportunities of 3D scene understanding in disasterresponse. Our dataset serves as a valuable resource for advancing robust 3Dvision systems in real-world applications for post-disaster scenarios.</description>
      <author>example@mail.com (Nhut Le, Ehsan Karimi, Maryam Rahnemoonfar)</author>
      <guid isPermaLink="false">2509.11097v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios</title>
      <link>http://arxiv.org/abs/2509.10841v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Computer Vision and Image Understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的LiDAR点云语义分割方法，通过点平面投影从2D表示中学习特征，并引入几何感知数据增强技术，在仅依赖LiDAR数据的情况下提高了性能，特别是在数据有限场景中表现优异。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云语义分割对于自动驾驶和机器人等应用中解释3D环境至关重要。现有方法虽然性能强大，但通常计算复杂度高，需要大量训练数据，限制了它们在数据稀少场景中的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;改进基于点的方法在数据有限场景中的性能，通过有效学习2D表示的特征来提取互补信息，同时仅依赖LiDAR数据。&lt;h4&gt;方法&lt;/h4&gt;通过点平面投影从2D表示中有效学习特征，引入符合LiDAR传感器特性的几何感知数据增强技术来缓解类别不平衡问题，并将点平面投影应用于点云的多个信息丰富的2D表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种方法在数据有限的情况下取得了显著改进，同时在两个公开可用的标准数据集（SemanticKITTI和PandaSet）上也取得了具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在仅依赖LiDAR数据的情况下，通过点平面投影和几何感知数据增强技术，有效提高了点云语义分割的性能，特别是在数据有限场景中表现优异，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;激光雷达点云语义分割对于自动驾驶和机器人等应用中解释三维环境至关重要。最近的方法通过利用不同的点云表示或整合来自其他传感器（如摄像头）或外部数据集的数据来实现强大的性能。然而，这些方法通常计算复杂度高，需要大量训练数据，限制了它们在数据稀少场景中的泛化能力。本文通过点平面投影从二维表示中有效学习特征，改进了基于点的方法的性能，能够在仅依赖激光雷达数据的同时提取互补信息。此外，我们引入了一种符合激光雷达传感器特性的几何感知数据增强技术，可以缓解类别不平衡问题。我们实现了并评估了将点平面投影应用于点云的多个信息丰富的二维表示的方法。实验表明，这种方法在数据有限的情况下取得了显著改进，同时在两个公开可用的标准数据集（如SemanticKITTI和PandaSet）上也取得了具有竞争力的结果。我们方法的代码可在https://github.com/SiMoM0/3PNet获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云语义分割在数据稀缺场景下的性能提升问题。这个问题在现实世界中非常重要，因为自动驾驶和机器人等应用需要高效的3D环境理解，但获取大量标注数据成本高昂且耗时。许多现有方法依赖大量训练数据或多传感器融合，限制了它们在资源受限环境或特殊场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的优缺点，发现点云直接处理方法在数据稀缺场景下表现更好但仍有提升空间，而投影方法虽然高效但存在信息损失。他们设计了一种混合方法，保留点云直接处理的优点同时借鉴投影方法的高效性。该方法基于之前的工作(Fusaro et al., 2024)进行扩展，该工作又受Puy et al. (2023)启发。作者采用了Instance CutMix技术来缓解类别不平衡，并引入了两种新的2D点云表示和层跳跃连接机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点平面投影技术将3D点云投影到多个2D平面上，从不同视角提取互补的几何和空间信息，利用高效的2D卷积处理这些投影，同时结合简单的点级操作保持点云原始结构。整体流程包括：1)点云嵌入模块提取点级和邻域特征；2)主干网络使用SpatialMix模块将点云投影到5种2D平面并处理，ChannelMix模块在3D空间细化特征；3)分割头结合初始特征生成预测；4)几何感知的Instance CutMix数据增强；5)交叉熵和Lovasz-Softmax损失的加权和作为最终损失函数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多点平面投影架构，使用5种2D点云表示并引入极坐标网格和距离图像两种新表示；2)几何感知的Instance CutMix数据增强，根据LiDAR束配置调整点密度；3)层跳跃连接保留重要特征；4)轻量级高效设计，使用2D卷积替代3D卷积。相比之前工作，该方法仅使用LiDAR数据不需要多传感器融合，比纯投影方法保留了更多原始点云信息，比纯点云方法计算效率更高，并在小数据场景下表现显著优于作者之前的工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于点平面投影的轻量级LiDAR语义分割方法，通过多视角2D表示和几何感知数据增强，显著提升了数据稀缺场景下的分割性能，同时保持了计算效率和标准数据集上的竞争力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point cloud semantic segmentation is essential for interpreting 3Denvironments in applications such as autonomous driving and robotics. Recentmethods achieve strong performance by exploiting different point cloudrepresentations or incorporating data from other sensors, such as cameras orexternal datasets. However, these approaches often suffer from highcomputational complexity and require large amounts of training data, limitingtheir generalization in data-scarce scenarios. In this paper, we improve theperformance of point-based methods by effectively learning features from 2Drepresentations through point-plane projections, enabling the extraction ofcomplementary information while relying solely on LiDAR data. Additionally, weintroduce a geometry-aware technique for data augmentation that aligns withLiDAR sensor properties and mitigates class imbalance. We implemented andevaluated our method that applies point-plane projections onto multipleinformative 2D representations of the point cloud. Experiments demonstrate thatthis approach leads to significant improvements in limited-data scenarios,while also achieving competitive results on two publicly available standarddatasets, as SemanticKITTI and PandaSet. The code of our method is available athttps://github.com/SiMoM0/3PNet</description>
      <author>example@mail.com (Simone Mosco, Daniel Fusaro, Wanmeng Li, Emanuele Menegatti, Alberto Pretto)</author>
      <guid isPermaLink="false">2509.10841v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors</title>
      <link>http://arxiv.org/abs/2509.12081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为欺骗风险最小化(DRM)的新方法，通过学习使训练数据看起来独立同分布的数据表示来实现分布外泛化，从而识别稳定特征并消除虚假相关性。&lt;h4&gt;背景&lt;/h4&gt;传统的领域适应或不变表示学习方法需要访问测试数据或将训练数据划分为有限数量的数据生成域，限制了它们在实际应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要测试数据访问或训练数据预划分的分布外泛化方法。&lt;h4&gt;方法&lt;/h4&gt;提出欺骗风险最小化(DRM)原则，通过一个可微分目标函数实现，该函数同时学习消除分布偏移的特征并最小化任务特定损失。&lt;h4&gt;主要发现&lt;/h4&gt;DRM在概念转换的数值实验和具有协变量偏移的模拟模仿学习环境中表现有效，特别是在机器人部署的环境中。&lt;h4&gt;结论&lt;/h4&gt;DRM提供了一种新的分布外泛化机制，通过学习使训练数据看起来独立同分布的表示，可以推广到未见过的领域，且不需要传统方法所需的测试数据访问或数据预划分。&lt;h4&gt;翻译&lt;/h4&gt;本文提出将欺骗作为分布外(OOD)泛化的机制：通过学习使训练数据对观察者看起来独立同分布(iid)的数据表示，我们可以识别出消除虚假相关性的稳定特征，并推广到未见过的领域。我们将这一原则称为欺骗风险最小化(DRM)，并通过一个实用的可微分目标函数实例化，该函数同时学习从基于保鞅的检测器角度看消除分布偏移的特征，同时最小化特定任务的损失。与领域适应或先前的不变表示学习方法相比，DRM不需要访问测试数据或将训练数据划分为有限数量的数据生成域。我们在具有概念转换的数值实验和机器人部署环境中具有协变量偏移的模拟模仿学习设置上证明了DRM的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes deception as a mechanism for out-of-distribution (OOD)generalization: by learning data representations that make training data appearindependent and identically distributed (iid) to an observer, we can identifystable features that eliminate spurious correlations and generalize to unseendomains. We refer to this principle as deceptive risk minimization (DRM) andinstantiate it with a practical differentiable objective that simultaneouslylearns features that eliminate distribution shifts from the perspective of adetector based on conformal martingales while minimizing a task-specific loss.In contrast to domain adaptation or prior invariant representation learningmethods, DRM does not require access to test data or a partitioning of trainingdata into a finite number of data-generating domains. We demonstrate theefficacy of DRM on numerical experiments with concept shift and a simulatedimitation learning setting with covariate shift in environments that a robot isdeployed in.</description>
      <author>example@mail.com (Anirudha Majumdar)</author>
      <guid isPermaLink="false">2509.12081v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</title>
      <link>http://arxiv.org/abs/2509.12039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 22 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了RAM++（Robust Representation Learning via Adaptive Mask），一个用于全面图像恢复的两阶段框架，结合高级语义理解和低级纹理生成，实现面向内容的鲁棒恢复。&lt;h4&gt;背景&lt;/h4&gt;现有基于退化导向的方法在极端场景（如与图像结构强耦合的退化）中存在局限性，同时面临跨任务性能不均衡、对已见退化过拟合以及对未见过退化泛化能力弱等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理各种退化场景（包括已见、未见、极端和混合退化）的鲁棒图像恢复框架，解决现有方法在极端场景下的局限性，并提高跨任务性能的均衡性。&lt;h4&gt;方法&lt;/h4&gt;RAM++包含三个关键设计：1) 自适应语义感知掩码（AdaSAM）：对语义丰富和纹理区域应用像素级掩码的预训练策略；2) 掩码属性电导（MAC）：调整贡献度更高的层的选择性微调策略；3) 鲁棒特征正则化（RFR）：利用DINOv2的语义一致且退化不变的表示，结合高效特征融合的策略。&lt;h4&gt;主要发现&lt;/h4&gt;RAM++通过三个关键设计，在已见、未见、极端和混合退化场景下实现了鲁棒、均衡和最先进的性能，有效解决了现有方法在极端场景下的局限性。&lt;h4&gt;结论&lt;/h4&gt;RAM++是一个有效的两阶段框架，通过结合高级语义理解和低级纹理生成，实现了面向内容的鲁棒图像恢复，在各种退化场景下表现出色。&lt;h4&gt;翻译&lt;/h4&gt;这项工作提出了通过自适应掩码进行鲁棒表示学习（RAM++），这是一个用于全面图像恢复的两阶段框架。RAM++将高级语义理解与低级纹理生成相结合，以实现面向内容的鲁棒恢复。它解决了现有基于退化导向的方法在极端场景（例如与图像结构强耦合的退化）中的局限性。RAM++还通过三个关键设计缓解了常见挑战，如跨任务性能不均衡、对已见退化的过拟合以及对未见过退化的弱泛化能力：1）自适应语义感知掩码（AdaSAM）：一种预训练策略，对语义丰富和纹理区域应用像素级掩码。这种设计使网络能够从各种退化中学习生成先验和图像内容先验。2）掩码属性电导（MAC）：一种选择性微调策略，调整贡献度更高的层，以弥合掩码预训练和完整图像微调之间的完整性差距，同时保留已学习的先验。3）鲁棒特征正则化（RFR）：一种策略，利用DINOv2的语义一致且退化不变的表示，结合高效特征融合，实现忠实且语义连贯的恢复。通过这些设计，RAM++在已见、未见、极端和混合退化场景下实现了鲁棒、均衡和最先进的性能。我们的代码和模型将在https://github.com/DragonisCV/RAM发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents Robust Representation Learning via Adaptive Mask (RAM++),a two-stage framework for all-in-one image restoration. RAM++ integrateshigh-level semantic understanding with low-level texture generation to achievecontent-oriented robust restoration. It addresses the limitations of existingdegradation-oriented methods in extreme scenarios (e.g., degradations stronglycoupled with image structures). RAM++ also mitigates common challenges such asunbalanced performance across tasks, overfitting to seen degradations, and weakgeneralization to unseen ones through three key designs: 1) AdaptiveSemantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-levelmasks to semantically rich and textured regions. This design enables thenetwork to learn both generative priors and image content priors from variousdegradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuningstrategy that adjusts the layers with higher contributions to bridge theintegrity gap between masked pretraining and full-image fine-tuning whileretaining learned priors. 3) Robust Feature Regularization (RFR): a strategythat leverages DINOv2's semantically consistent and degradation-invariantrepresentations, together with efficient feature fusion, to achieve faithfuland semantically coherent restoration. With these designs, RAM++ achievesrobust, well-balanced, and state-of-the-art performance across seen, unseen,extreme, and mixed degradations. Our code and model will be released athttps://github.com/DragonisCV/RAM</description>
      <author>example@mail.com (Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, Chongyi Li)</author>
      <guid isPermaLink="false">2509.12039v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation</title>
      <link>http://arxiv.org/abs/2509.11840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 CDEL Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;生成式视觉语言模型在高层图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐。本研究通过将图像与VLMs生成的合成描述进行密集对齐，结合了生成式VLMs和视觉语言对齐表示学习两个研究方向。合成字幕成本低、可扩展且易于生成，为密集对齐方法提供了高级语义理解的优质来源。实验表明，该方法在标准零样本开放词汇分割基准上优于先前工作，且数据效率更高。&lt;h4&gt;背景&lt;/h4&gt;生成式视觉语言模型在高层图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐。另一研究方向专注于视觉语言对齐的表示学习，针对的是像分割这样的密集任务的零样本推理。&lt;h4&gt;目的&lt;/h4&gt;结合生成式VLMs和视觉语言对齐表示学习两个研究方向，通过将图像与VLMs生成的合成描述进行密集对齐。&lt;h4&gt;方法&lt;/h4&gt;使用VLMs生成合成字幕，这些字幕成本低、可扩展且易于生成，作为密集对齐方法的高级语义理解来源。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在标准的零样本开放词汇分割基准/数据集上优于先前的工作，同时具有更高的数据效率。&lt;h4&gt;结论&lt;/h4&gt;通过VLMs生成的合成描述与图像进行密集对齐是一种有效的方法，可以在零样本开放词汇分割任务中取得更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;生成式视觉语言模型在高层图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐，正如我们的研究结果所示。与生成式VLMs的进展相辅相成，另一研究方向专注于视觉语言对齐的表示学习，针对的是像分割这样的密集任务的零样本推理。在这项工作中，我们通过将图像与VLMs生成的合成描述进行密集对齐，结合了这两个方向。合成字幕成本低、可扩展且易于生成，使它们成为密集对齐方法的高级语义理解的优质来源。实验上，我们的方法在标准的零样本开放词汇分割基准/数据集上优于先前的工作，同时具有更高的数据效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative vision-language models (VLMs) exhibit strong high-level imageunderstanding but lack spatially dense alignment between vision and languagemodalities, as our findings indicate. Orthogonal to advancements in generativeVLMs, another line of research has focused on representation learning forvision-language alignment, targeting zero-shot inference for dense tasks likesegmentation. In this work, we bridge these two directions by densely aligningimages with synthetic descriptions generated by VLMs. Synthetic captions areinexpensive, scalable, and easy to generate, making them an excellent source ofhigh-level semantic understanding for dense alignment methods. Empirically, ourapproach outperforms prior work on standard zero-shot open-vocabularysegmentation benchmarks/datasets, while also being more data-efficient.</description>
      <author>example@mail.com (Tim Lebailly, Vijay Veerabadran, Satwik Kottur, Karl Ridgeway, Michael Louis Iuzzolino)</author>
      <guid isPermaLink="false">2509.11840v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Anomaly Detection in Industrial Control Systems Based on Cross-Domain Representation Learning</title>
      <link>http://arxiv.org/abs/2509.11786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于跨域表示学习的工业控制系统异常检测方法，能够学习多领域行为的联合特征并在不同领域内检测异常，实验结果表明该方法性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;工业控制系统广泛应用于工业领域，其安全性和稳定性至关重要。ICS通过网络通信远程监控和管理物理设备，而现有异常检测方法主要关注网络流量或传感器数据的安全分析。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于跨域表示学习的ICS异常检测方法，学习多领域行为的联合特征，在不同领域内检测异常，克服仅分析单一领域难以全面识别异常的问题。&lt;h4&gt;方法&lt;/h4&gt;构建能够表示ICS多领域行为的跨域图，利用图神经网络学习这些行为的联合特征，采用多任务学习方法在不同领域分别识别异常并进行联合训练。&lt;h4&gt;主要发现&lt;/h4&gt;由于异常在不同领域表现出不同行为，通过多任务学习分别识别不同领域的异常并进行联合训练，能够提高异常检测的准确性。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，所提出的基于跨域表示学习的异常检测方法在识别ICS异常方面的性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;工业控制系统(ICS)广泛应用于工业领域，其安全性和稳定性非常重要。一旦ICS受到攻击，可能会造成严重损害。因此，检测ICS中的异常非常重要。ICS可以使用通信网络远程监控和管理物理设备。现有的异常检测方法主要关注分析网络流量或传感器数据的安全性。然而，ICS不同领域（如网络流量和传感器物理状态）的行为是相互关联的，因此仅通过分析单一领域难以全面识别异常。本文提出了一种基于ICS跨域表示学习的异常检测方法，可以学习多领域行为的联合特征，并在不同领域内检测异常。在构建能够表示ICS多领域行为的跨域图后，我们的方法可以利用图神经网络学习它们的联合特征。由于异常在不同领域表现出不同的行为，我们利用多任务学习方法分别识别不同领域的异常并进行联合训练。实验结果表明，我们的方法在识别ICS异常方面的性能优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Industrial control systems (ICSs) are widely used in industry, and theirsecurity and stability are very important. Once the ICS is attacked, it maycause serious damage. Therefore, it is very important to detect anomalies inICSs. ICS can monitor and manage physical devices remotely using communicationnetworks. The existing anomaly detection approaches mainly focus on analyzingthe security of network traffic or sensor data. However, the behaviors ofdifferent domains (e.g., network traffic and sensor physical status) of ICSsare correlated, so it is difficult to comprehensively identify anomalies byanalyzing only a single domain. In this paper, an anomaly detection approachbased on cross-domain representation learning in ICSs is proposed, which canlearn the joint features of multi-domain behaviors and detect anomalies withindifferent domains. After constructing a cross-domain graph that can representthe behaviors of multiple domains in ICSs, our approach can learn the jointfeatures of them by leveraging graph neural networks. Since anomalies behavedifferently in different domains, we leverage a multi-task learning approach toidentify anomalies in different domains separately and perform joint training.The experimental results show that the performance of our approach is betterthan existing approaches for identifying anomalies in ICSs.</description>
      <author>example@mail.com (Dongyang Zhan, Wenqi Zhang, Lin Ye, Xiangzhan Yu, Hongli Zhang, Zheng He)</author>
      <guid isPermaLink="false">2509.11786v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</title>
      <link>http://arxiv.org/abs/2509.11425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FuseCodec通过强大的跨模态对齐和全局监督信息统一了声学、语义和上下文表示，在语音标记化任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的神经编解码器捕获低级声学特征，忽略了人类语音中固有的语义和上下文线索。虽然最近的工作引入了语义表示或上下文表示，但在对齐和统一这些表示方面仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;引入FuseCodec，通过强大的跨模态对齐和全局监督信息来统一声学、语义和上下文表示。&lt;h4&gt;方法&lt;/h4&gt;FuseCodec采用三种互补技术：(i)潜在表示融合：将语义和上下文特征直接集成到编码器潜在空间中；(ii)全局语义-上下文监督：使用全局池化和广播表示监督离散令牌；(iii)时间对齐的上下文监督：在局部窗口内动态匹配上下文和语音令牌。同时介绍了FuseCodec-TTS，展示其在零样本语音合成中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;FuseCodec在LibriSpeech上取得了最先进的性能，在转录准确性、感知质量、可懂度和说话人相似性方面超越了EnCodec、SpeechTokenizer和DAC。&lt;h4&gt;结论&lt;/h4&gt;上下文和语义引导的标记化对语音标记化和下游任务有效。代码和预训练模型已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;语音标记化能够实现离散表示并促进语音语言建模。然而，现有的神经编解码器捕获低级声学特征，忽略了人类语音中固有的语义和上下文线索。虽然最近的工作引入了来自自监督语音模型的语义表示或集成了来自预训练语言模型的上下文表示，但在对齐和统一这些语义和上下文表示方面仍存在挑战。我们引入了FuseCodec，它通过强大的跨模态对齐和全局监督信息来统一声学、语义和上下文表示。我们提出了三种互补技术：(i)潜在表示融合，将语义和上下文特征直接集成到编码器潜在空间中，以实现强大和统一的表示学习；(ii)全局语义-上下文监督，使用全局池化和广播表示监督离散令牌，以增强时间一致性和跨模态对齐；(iii)时间对齐的上下文监督，通过在局部窗口内动态匹配上下文和语音令牌来加强对齐，实现细粒度的令牌级监督。我们进一步介绍了FuseCodec-TTS，展示了我们的方法在零样本语音合成中的应用。实验表明，FuseCodec在LibriSpeech上取得了最先进的性能，在转录准确性、感知质量、可懂度和说话人相似性方面超越了EnCodec、SpeechTokenizer和DAC。结果强调了上下文和语义引导的标记化对语音标记化和下游任务的有效性。代码和预训练模型可在https://github.com/mubtasimahasan/FuseCodec获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech tokenization enables discrete representation and facilitates speechlanguage modeling. However, existing neural codecs capture low-level acousticfeatures, overlooking the semantic and contextual cues inherent to humanspeech. While recent efforts introduced semantic representations fromself-supervised speech models or incorporated contextual representations frompre-trained language models, challenges remain in aligning and unifying thesemantic and contextual representations. We introduce FuseCodec, which unifiesacoustic, semantic, and contextual representations through strong cross-modalalignment and globally informed supervision. We propose three complementarytechniques: (i) Latent Representation Fusion, integrating semantic andcontextual features directly into the encoder latent space for robust andunified representation learning; (ii) Global Semantic-Contextual Supervision,supervising discrete tokens with globally pooled and broadcastedrepresentations to enhance temporal consistency and cross-modal alignment; and(iii) Temporally Aligned Contextual Supervision, strengthening alignment bydynamically matching contextual and speech tokens within a local window forfine-grained token-level supervision. We further introduce FuseCodec-TTS,demonstrating our methodology's applicability to zero-shot speech synthesis.Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,perceptual quality, intelligibility, and speaker similarity. Results highlightthe effectiveness of contextually and semantically guided tokenization forspeech tokenization and downstream tasks. Code and pretrained models areavailable at https://github.com/mubtasimahasan/FuseCodec.</description>
      <author>example@mail.com (Md Mubtasim Ahasan, Rafat Hasan Khan, Tasnim Mohiuddin, Aman Chadha, Tariq Iqbal, M Ashraful Amin, Amin Ahsan Ali, Md Mofijul Islam, A K M Mahbubur Rahman)</author>
      <guid isPermaLink="false">2509.11425v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits</title>
      <link>http://arxiv.org/abs/2509.11362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了PersonaX，一个多模态数据集集合，用于全面分析公共人物的行为特征，包含CelebPersona（9444名公众人物）和AthlePersona（4181名运动员）两个子数据集，每个数据集都包含行为特征评估、面部图像和传记信息。&lt;h4&gt;背景&lt;/h4&gt;理解人类行为特征在人机交互、计算社会科学和个性化AI系统中至关重要，需要整合多种模态捕捉细微模式，但现有资源很少提供结合行为描述符与面部属性、传记信息等互补模态的数据集。&lt;h4&gt;目的&lt;/h4&gt;解决现有资源不足，提供一个结合行为描述符与互补模态的数据集，支持跨模态的公共特征全面分析。&lt;h4&gt;方法&lt;/h4&gt;创建PersonaX数据集集合，并在两个层次上分析：1)从文本描述提取高级特征分数，应用五种统计独立性检验检查与其他模态的关系；2)引入专为多模态和多测量数据设计的因果表示学习框架，提供理论可识别性保证。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实数据上的实验证明了该方法的有效性，PersonaX为研究LLM推断的行为特征与视觉和传记属性的结合奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;PersonaX通过整合结构化和非结构化分析，建立了研究LLM推断行为特征与视觉和传记属性相结合的基础，促进了多模态特征分析和因果推理的发展。&lt;h4&gt;翻译&lt;/h4&gt;理解人类行为特征是人机交互、计算社会科学和个性化AI系统应用的核心。这种理解通常需要整合多种模态来捕捉细微的模式和关系。然而，现有资源很少提供将行为描述符与面部属性和传记信息等互补模态结合的数据集。为了解决这一差距，我们提出了PersonaX，这是一个精心策划的多模态数据集集合，旨在实现跨模态的公共特征全面分析。PersonaX包括(1)CelebPersona，包含来自不同职业的9444名公众人物，和(2)AthlePersona，涵盖7个主要体育联赛的4181名专业运动员。每个数据集都包含由三个高性能大型语言模型推断的行为特征评估，以及面部图像和结构化的传记特征。我们在两个互补层次上分析PersonaX。首先，我们从文本描述中抽象出高级特征分数，并应用五种统计独立性检验来检查它们与其他模态的关系。其次，我们引入了一种新的因果表示学习框架，专为多模态和多测量数据设计，提供理论可识别性保证。在合成和真实数据上的实验证明了我们方法的有效性。通过统一结构化和非结构化分析，PersonaX为研究LLM推断的行为特征与视觉和传记属性相结合奠定了基础，推进了多模态特征分析和因果推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding human behavior traits is central to applications inhuman-computer interaction, computational social science, and personalized AIsystems. Such understanding often requires integrating multiple modalities tocapture nuanced patterns and relationships. However, existing resources rarelyprovide datasets that combine behavioral descriptors with complementarymodalities such as facial attributes and biographical information. To addressthis gap, we present PersonaX, a curated collection of multimodal datasetsdesigned to enable comprehensive analysis of public traits across modalities.PersonaX consists of (1) CelebPersona, featuring 9444 public figures fromdiverse occupations, and (2) AthlePersona, covering 4181 professional athletesacross 7 major sports leagues. Each dataset includes behavioral traitassessments inferred by three high-performing large language models, alongsidefacial imagery and structured biographical features. We analyze PersonaX at twocomplementary levels. First, we abstract high-level trait scores from textdescriptions and apply five statistical independence tests to examine theirrelationships with other modalities. Second, we introduce a novel causalrepresentation learning (CRL) framework tailored to multimodal andmulti-measurement data, providing theoretical identifiability guarantees.Experiments on both synthetic and real-world data demonstrate the effectivenessof our approach. By unifying structured and unstructured analysis, PersonaXestablishes a foundation for studying LLM-inferred behavioral traits inconjunction with visual and biographical attributes, advancing multimodal traitanalysis and causal reasoning.</description>
      <author>example@mail.com (Loka Li, Wong Yu Kang, Minghao Fu, Guangyi Chen, Zhenhao Chen, Gongxu Luo, Yuewen Sun, Salman Khan, Peter Spirtes, Kun Zhang)</author>
      <guid isPermaLink="false">2509.11362v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Towards Automated Error Discovery: A Study in Conversational AI</title>
      <link>http://arxiv.org/abs/2509.10833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMNLP 2025 main conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基于大语言模型的对话代理虽然流畅连贯，但仍会产生难以预防的不良行为。本研究提出了一种自动错误发现框架和SEEED方法，用于检测对话AI中的错误，特别是指令中未明确指定的错误。&lt;h4&gt;背景&lt;/h4&gt;基于大语言模型的对话代理表现出强大的流畅性和连贯性，但仍会产生不良行为（错误），这些错误很难在部署过程中阻止其到达用户。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够检测和定义对话AI中错误的框架，特别是那些由于响应生成模型更新或用户行为变化而产生的新错误。&lt;h4&gt;方法&lt;/h4&gt;引入'自动错误发现'框架，提出SEEED（Soft Clustering Extended Encoder-Based Error Detection）作为基于编码器的实现方法。增强软最近邻损失中负样本的距离加权，并引入基于标签的样本排序来选择高度对比的示例，以实现更好的表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;SEEED在多个错误标注的对话数据集上优于适应的基线方法（包括GPT-4o和Phi-4），将检测未知错误的准确性提高了最多8个百分点，并且在对未知意图检测方面表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SEEED方法能有效检测对话AI中的未知错误，提高错误检测的准确性，并在未知意图检测方面表现出良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;尽管基于大语言模型的对话代理表现出强大的流畅性和连贯性，但它们仍然会产生不良行为（错误），这些错误很难在部署过程中阻止其到达用户。最近的研究利用大语言模型来检测错误并指导响应生成模型的改进。然而，当前的LLM难以识别指令中未明确指定的错误，例如由于响应生成模型更新或用户行为变化而产生的错误。在这项工作中，我们引入了'自动错误发现'，这是一个用于检测和定义对话AI中错误的框架，并提出了SEEED（Soft Clustering Extended Encoder-Based Error Detection）作为其基于编码器的实现方法。我们通过增强负样本的距离加权来改进软最近邻损失，并引入基于标签的样本排序来选择高度对比的示例，以实现更好的表示学习。SEEED在多个错误标注的对话数据集上优于适应的基线方法（包括GPT-4o和Phi-4），将检测未知错误的准确性提高了最多8个百分点，并表现出对未知意图检测的强大泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although LLM-based conversational agents demonstrate strong fluency andcoherence, they still produce undesirable behaviors (errors) that arechallenging to prevent from reaching users during deployment. Recent researchleverages large language models (LLMs) to detect errors and guideresponse-generation models toward improvement. However, current LLMs struggleto identify errors not explicitly specified in their instructions, such asthose arising from updates to the response-generation model or shifts in userbehavior. In this work, we introduce Automated Error Discovery, a framework fordetecting and defining errors in conversational AI, and propose SEEED (SoftClustering Extended Encoder-Based Error Detection), as an encoder-basedapproach to its implementation. We enhance the Soft Nearest Neighbor Loss byamplifying distance weighting for negative samples and introduce Label-BasedSample Ranking to select highly contrastive examples for better representationlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --across multiple error-annotated dialogue datasets, improving the accuracy fordetecting unknown errors by up to 8 points and demonstrating stronggeneralization to unknown intent detection.</description>
      <author>example@mail.com (Dominic Petrak, Thy Thy Tran, Iryna Gurevych)</author>
      <guid isPermaLink="false">2509.10833v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Socially-Informed Content Analysis of Online Human Behavior</title>
      <link>http://arxiv.org/abs/2509.10807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Doctoral dissertation, University of Southern California, 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文采用计算社会科学技术研究社交媒体带来的挑战，如政治极化、虚假信息、仇恨言论和回音室，并提出数据驱动的解决方案促进更健康的数字互动。&lt;h4&gt;背景&lt;/h4&gt;社交媒体的爆炸性增长不仅改变了通信方式，还带来了政治极化、虚假信息、仇恨言论和回音室等挑战。&lt;h4&gt;目的&lt;/h4&gt;研究社交媒体带来的问题，理解推动负面在线行为的社会动态，并提出数据驱动的解决方案以促进更健康的数字互动。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种可扩展的社会网络表示学习方法，将用户生成的内容与社会连接相结合，创建统一的用户嵌入，从而准确预测和可视化用户属性、社区和行为倾向。&lt;h4&gt;主要发现&lt;/h4&gt;1) Twitter上关于COVID-19的讨论揭示了政治极化和不对称政治回音室；2) 在线仇恨言论表明追求社会认可推动了有毒行为；3) COVID-19讨论的道德基础揭示了道德同质性和回音室的模式，同时表明道德多样性和多元性可以提高跨意识形态分歧的信息传播和接受度。&lt;h4&gt;结论&lt;/h4&gt;这些发现有助于计算社会科学的发展，并为通过社会互动和网络同质性视角理解人类行为提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体的爆炸性增长不仅彻底改变了通信方式，也带来了政治极化、虚假信息、仇恨言论和回音室等挑战。本文采用计算社会科学技术研究这些问题，理解推动负面在线行为的社会动态，并提出数据驱动的解决方案以促进更健康的数字互动。首先，我介绍了一种可扩展的社会网络表示学习方法，将用户生成的内容与社会连接相结合，创建统一的用户嵌入，从而能够准确预测和可视化用户属性、社区和行为倾向。使用这一工具，我探索了三个相互关联的问题：1) Twitter上关于COVID-19的讨论，揭示了政治极化和不对称政治回音室；2) 在线仇恨言论，表明追求社会认可推动了有毒行为；3) COVID-19讨论的道德基础，揭示了道德同质性和回音室的模式，同时表明道德多样性和多元性可以提高跨意识形态分歧的信息传播和接受度。这些发现有助于计算社会科学的发展，并为通过社会互动和网络同质性视角理解人类行为提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The explosive growth of social media has not only revolutionizedcommunication but also brought challenges such as political polarization,misinformation, hate speech, and echo chambers. This dissertation employscomputational social science techniques to investigate these issues, understandthe social dynamics driving negative online behaviors, and propose data-drivensolutions for healthier digital interactions. I begin by introducing a scalablesocial network representation learning method that integrates user-generatedcontent with social connections to create unified user embeddings, enablingaccurate prediction and visualization of user attributes, communities, andbehavioral propensities. Using this tool, I explore three interrelatedproblems: 1) COVID-19 discourse on Twitter, revealing polarization andasymmetric political echo chambers; 2) online hate speech, suggesting thepursuit of social approval motivates toxic behavior; and 3) moral underpinningsof COVID-19 discussions, uncovering patterns of moral homophily and echochambers, while also indicating moral diversity and plurality can improvemessage reach and acceptance across ideological divides. These findingscontribute to the advancement of computational social science and provide afoundation for understanding human behavior through the lens of socialinteractions and network homophily.</description>
      <author>example@mail.com (Julie Jiang)</author>
      <guid isPermaLink="false">2509.10807v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Outlier-Resistant Heterogeneous Treatment Effect Estimation in HDLSS Settings via GAT--CVAE Framework</title>
      <link>http://arxiv.org/abs/2509.10787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对高维小样本设置的异质性治疗效应估计的稳健框架，结合图注意力网络和条件变分自编码器，通过扩展样本空间和聚类分析，实现稳定且可推广的因果效应估计。&lt;h4&gt;背景&lt;/h4&gt;在高维小样本环境下进行异质性治疗效应估计面临挑战，现有方法可能无法有效处理结构依赖性和异常值问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健的HTE估计框架，能够在高维小样本设置中准确捕捉混杂因素间的结构依赖性，并有效整合异常值数据，从而获得稳定且可推广的因果效应估计结果。&lt;h4&gt;方法&lt;/h4&gt;结合图注意力网络捕捉混杂因素间的结构依赖性，以及条件变分自编码器进行潜在表示学习，扩展样本空间并进行聚类分析，将异常值集合整合为连贯的子组，然后使用双重稳健抗异常值估计器估计子组因果效应。&lt;h4&gt;主要发现&lt;/h4&gt;模拟和实际应用表明，该方法相比现有HTE方法具有优越性能，能够有效处理高维小样本数据中的复杂结构和异常值问题。&lt;h4&gt;结论&lt;/h4&gt;该框架为精准医疗和政策评估等领域提供了有效的异质性治疗效应估计工具，具有实际应用价值和推广潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种针对高维小样本设置的异质性治疗效应估计的稳健框架。通过结合图注意力网络来捕捉混杂因素之间的结构依赖性，以及使用条件变分自编码器进行潜在表示学习，我们的方法扩展了样本空间并执行聚类，将异常值集合整合为连贯的子组。然后使用双重稳健抗异常值估计器估计子组因果效应，产生稳定且可推广的结果。模拟和实际应用证实了与现有HTE方法相比的优越性能，突显了该框架在精准医疗和政策评估方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a robust framework for heterogeneous treatment effect (HTE)estimation tailored to high-dimensional low sample size (HDLSS) settings. Bycombining Graph Attention Networks (GAT) to capture structural dependenciesamong confounders with a Conditional Variational Autoencoder (CVAE) for latentrepresentation learning, our method expands the sample space and performsclustering that integrates even outlier sets into coherent subgroups.Clusterwise causal effects are then estimated using a doubly robustoutlier-resistant estimator, yielding stable and generalizable results.Simulations and real-world applications confirm superior performance comparedwith existing HTE methods, highlighting the framework's potential for precisionmedicine and policy evaluation.</description>
      <author>example@mail.com (Byeonghee Lee, Joonsung Kang)</author>
      <guid isPermaLink="false">2509.10787v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning</title>
      <link>http://arxiv.org/abs/2509.10555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SurgLaVi，迄今为止最大且多样化的手术视觉语言数据集，包含近24万剪辑-字幕对，涵盖200多种手术程序，并具有层次化结构。研究还开发了SurgCLIP模型，在多项任务上超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;视觉语言预训练(VLP)在手术领域具有独特优势，能够将语言与手术视频对齐，实现工作流程理解和跨任务迁移，无需依赖专家标注数据集。然而，现有手术VLP数据集受限于规模小、程序多样性不足、语义质量不高和层次结构不完善等问题。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模、多样化、语义质量高且具有层次结构的手术视觉语言数据集，以促进手术基础模型的发展。&lt;h4&gt;方法&lt;/h4&gt;开发了一个全自动管道系统，系统生成手术视频的精细转录并将其分割为连贯的程序单元；应用双模态过滤去除不相关和噪声样本，确保高质量注释；推出开源版本SurgLaVi-eta，包含11.3万剪辑-字幕对，完全基于公共数据构建；引入SurgCLIP模型，一种具有双编码器的CLIP风格视频文本对比框架。&lt;h4&gt;主要发现&lt;/h4&gt;SurgCLIP在阶段、步骤、动作和工具识别任务上实现了持续改进，显著超越了先前最先进的方法；大规模、语义丰富和层次结构化的数据集直接转化为更强和更可泛化的表示能力。&lt;h4&gt;结论&lt;/h4&gt;SurgLaVi作为开发手术基础模型的关键资源，证明了高质量、大规模数据集对提升手术AI模型性能的重要性，为手术领域的基础模型研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言预训练(VLP)通过将语言与手术视频对齐，为手术领域提供了独特优势，使工作流程理解和跨任务迁移成为可能，无需依赖专家标注的数据集。然而，现有手术VLP的进展受限于现有数据集的规模有限、程序多样性不足、语义质量和层次结构等问题。在本工作中，我们提出了SurgLaVi，迄今为止最大且最多样化的手术视觉语言数据集，包含来自200多种程序的近24万剪辑-字幕对，并在阶段、步骤和任务级别具有层次结构。SurgLaVi的核心是一个全自动管道，系统生成手术视频的精细转录并将其分割为连贯的程序单元。为确保高质量注释，它应用双模态过滤去除不相关和噪声样本。在此框架内，生成的字幕通过上下文细节进行丰富，产生语义丰富且易于解释的注释。为确保可访问性，我们发布了SurgLaVi-eta，这是一个完全基于公共数据构建的开源衍生版本，包含11.3万剪辑-字幕对，比现有手术VLP数据集大四倍以上。为证明SurgLaVi数据集的价值，我们引入了SurgCLIP，一种具有双编码器的CLIP风格视频文本对比框架，作为代表性基础模型。SurgCLIP在阶段、步骤、动作和工具识别方面取得了一致的改进，显著超越了先前最先进的方法。这些结果验证了大规模、语义丰富和层次结构化的数据集直接转化为更强和更可泛化的表示能力，确立了SurgLaVi作为开发手术基础模型的关键资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language pre-training (VLP) offers unique advantages for surgery byaligning language with surgical videos, enabling workflow understanding andtransfer across tasks without relying on expert-labeled datasets. However,progress in surgical VLP remains constrained by the limited scale, proceduraldiversity, semantic quality, and hierarchical structure of existing datasets.In this work, we present SurgLaVi, the largest and most diverse surgicalvision-language dataset to date, comprising nearly 240k clip-caption pairs frommore than 200 procedures, and comprising hierarchical levels at phase-, step-,and task-level. At the core of SurgLaVi lies a fully automated pipeline thatsystematically generates fine-grained transcriptions of surgical videos andsegments them into coherent procedural units. To ensure high-qualityannotations, it applies dual-modality filtering to remove irrelevant and noisysamples. Within this framework, the resulting captions are enriched withcontextual detail, producing annotations that are both semantically rich andeasy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, anopen-source derivative of 113k clip-caption pairs constructed entirely frompublic data, which is over four times larger than existing surgical VLPdatasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,a CLIP-style video-text contrastive framework with dual encoders, as arepresentative base model. SurgCLIP achieves consistent improvements acrossphase, step, action, and tool recognition, surpassing prior state-of-the-artmethods, often by large margins. These results validate that large-scale,semantically rich, and hierarchically structured datasets directly translateinto stronger and more generalizable representations, establishing SurgLaVi asa key resource for developing surgical foundation models.</description>
      <author>example@mail.com (Alejandra Perez, Chinedu Nwoye, Ramtin Raji Kermani, Omid Mohareri, Muhammad Abdullah Jamal)</author>
      <guid isPermaLink="false">2509.10555v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Deriving accurate galaxy cluster masses using X-ray thermodynamic profiles and graph neural networks</title>
      <link>http://arxiv.org/abs/2509.12199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 15 figures, 6 tables, resubmitted to A&amp;A after revision,  comments welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用图神经网络(GNNs)从X射线观测数据估计星系团质量，实现了比传统方法更精确的结果，并发现了SZ方法中存在的质量依赖性偏差。&lt;h4&gt;背景&lt;/h4&gt;精确测定星系团质量对于建立星系团宇宙学中可靠的质量-观测标度关系至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来精确估计星系团质量，并验证其准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNNs)处理X射线观测推断的星系团内介质(ICM)径向采样剖面，将每个ICM剖面表示为图，以处理可变长度和分辨率的输入。使用The Three Hundred Project的星系团流体动力学模拟来训练和测试模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 与真实质量相比没有系统偏差；2) 质量估计散射约为6%，比传统方法小6倍；3) 算法对数据质量和星系团形态具有鲁棒性；4) 发现SZ推导的质量存在质量依赖性偏差，质量越高的星系团偏差越大；5) 中位偏差为(1-b)=0.85_{-14}^{+34}。&lt;h4&gt;结论&lt;/h4&gt;通过整合X射线、SZ和光学数据集使用深度学习技术，在建立无偏观测质量标度关系方面迈出了重要一步，增强了星系团在精密宇宙学中的作用。&lt;h4&gt;翻译&lt;/h4&gt;精确测定星系团质量对于建立星系团宇宙学中可靠的质量-观测标度关系至关重要。我们采用图神经网络(GNNs)从X射线观测推断的星系团内介质(ICM)径向采样剖面来估计星系团质量。GNNs通过将每个ICM剖面表示为图，自然处理可变长度和分辨率的输入，能够准确灵活地建模各种观测条件。我们使用The Three Hundred Project的最先进星系团流体动力学模拟来训练和测试GNN模型。与模拟中的真实星系团质量相比，我们方法的质量估计没有系统偏差。此外，我们实现恢复质量与真实质量约为6%的散射，比标准流体静力学平衡方法获得的散射小6倍。我们的算法对数据质量和星系团形态具有鲁棒性，能够同时结合模型不确定性和观测不确定性。最后，我们将该技术应用于XMM-Newton观测的星系团样本，并将GNN推导的质量估计与通过Y_SZ-M_500标度关系获得的质量估计进行比较。我们的结果提供了强有力的证据(5σ水平)表明SZ推导的质量存在质量依赖性偏差，质量更高的星系团表现出更大的偏差程度。此外，我们发现中位偏差为(1-b)=0.85_{-14}^{+34}，但由于其质量依赖性而具有显著的弥散。这项工作通过整合X射线、SZ和光学数据集使用深度学习技术，在建立无偏观测质量标度关系方面迈出了重要一步，从而增强了星系团在精密宇宙学中的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Precise determination of galaxy cluster masses is crucial for establishingreliable mass-observable scaling relations in cluster cosmology. We employgraph neural networks (GNNs) to estimate galaxy cluster masses from radiallysampled profiles of the intra-cluster medium (ICM) inferred from X-rayobservations. GNNs naturally handle inputs of variable length and resolution byrepresenting each ICM profile as a graph, enabling accurate and flexiblemodeling across diverse observational conditions. We trained and tested GNNmodel using state-of-the-art hydrodynamical simulations of galaxy clusters fromThe Three Hundred Project. The mass estimates using our method exhibit nosystematic bias compared to the true cluster masses in the simulations.Additionally, we achieve a scatter in recovered mass versus true mass of about6\%, which is a factor of six smaller than obtained from a standard hydrostaticequilibrium approach. Our algorithm is robust to both data quality and clustermorphology and it is capable of incorporating model uncertainties alongsideobservational uncertainties. Finally, we apply our technique to XMM-Newtonobserved galaxy cluster samples and compare the GNN derived mass estimates withthose obtained with $Y_{\rm SZ}$-M$_{500}$ scaling relations. Our resultsprovide strong evidence, at 5$\sigma$ level, for a mass-dependent bias in SZderived masses, with higher mass clusters exhibiting a greater degree ofdeviation. Furthermore, we find the median bias to be $(1-b)=0.85_{-14}^{+34}$,albeit with significant dispersion due to its mass dependence. This work takesa significant step towards establishing unbiased observable mass scalingrelations by integrating X-ray, SZ and optical datasets using deep learningtechniques, thereby enhancing the role of galaxy clusters in precisioncosmology.</description>
      <author>example@mail.com (Asif Iqbal, Subhabrata Majumdar, Elena Rasia, Gabriel W. Pratt, Daniel de Andres, Jean-Baptiste Melin, Weiguang Cui)</author>
      <guid isPermaLink="false">2509.12199v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data</title>
      <link>http://arxiv.org/abs/2509.12143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 1 figure, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种结合视觉Transformer和图神经网络的统一方法，用于通过结构磁共振成像自动检测重度抑郁症，有效提高了诊断准确性。&lt;h4&gt;背景&lt;/h4&gt;重度抑郁症是一种常见的精神健康问题，影响个人幸福感和全球公共卫生。使用结构磁共振成像和深度学习方法自动检测MDD有望提高诊断准确性和实现早期干预。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉复杂脑部模式的方法，超越现有的仅使用体素级特征或手工制作区域表征的方法。&lt;h4&gt;方法&lt;/h4&gt;开发了统一流程，使用视觉Transformer从sMRI数据中提取3D区域嵌入，使用图神经网络进行分类。探索了两种定义区域的方法：基于图谱的方法使用预定义的脑图谱，基于立方体的方法直接从3D补丁中识别区域。生成余弦相似度图建模区域间关系，指导GNN分类。使用REST-meta-MDD数据集进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;最佳模型在分层10折交叉验证中获得了78.98%的准确率、76.54%的敏感性、81.58%的特异性、81.58%的精确度和78.98%的F1分数。基于图谱的模型始终优于基于立方体的方法。&lt;h4&gt;结论&lt;/h4&gt;使用特定领域的解剖先验对于MDD检测至关重要，基于预定义脑图谱的方法表现更好。&lt;h4&gt;翻译&lt;/h4&gt;重度抑郁症是一种常见的精神健康问题，对个人幸福感和全球公共卫生产生负面影响。使用结构磁共振成像和深度学习方法自动检测MDD有望提高诊断准确性和实现早期干预。大多数现有方法使用体素级特征或基于预定义脑图谱构建的手工制作区域表征，限制了它们捕捉复杂脑部模式的能力。本文开发了一个统一流程，利用视觉Transformer从sMRI数据中提取3D区域嵌入，并使用图神经网络进行分类。我们探索了两种定义区域的方法：(1)使用预定义的结构性和功能性脑图谱的基于图谱方法，(2)基于立方体的方法，ViTs直接训练以从均匀提取的3D补丁中识别区域。此外，生成余弦相似度图来建模区域间关系，并指导基于GNN的分类。使用REST-meta-MDD数据集进行了广泛的实验以证明我们模型的有效性。通过分层10折交叉验证，最佳模型获得了78.98%的准确率、76.54%的敏感性、81.58%的特异性、81.58%的精确度和78.98%的F1分数。此外，基于图谱的模型始终优于基于立方体的方法，突显了使用特定领域的解剖先验进行MDD检测的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Major depressive disorder (MDD) is a prevalent mental health condition thatnegatively impacts both individual well-being and global public health.Automated detection of MDD using structural magnetic resonance imaging (sMRI)and deep learning (DL) methods holds increasing promise for improvingdiagnostic accuracy and enabling early intervention. Most existing methodsemploy either voxel-level features or handcrafted regional representationsbuilt from predefined brain atlases, limiting their ability to capture complexbrain patterns. This paper develops a unified pipeline that utilizes VisionTransformers (ViTs) for extracting 3D region embeddings from sMRI data andGraph Neural Network (GNN) for classification. We explore two strategies fordefining regions: (1) an atlas-based approach using predefined structural andfunctional brain atlases, and (2) an cube-based method by which ViTs aretrained directly to identify regions from uniformly extracted 3D patches.Further, cosine similarity graphs are generated to model interregionalrelationships, and guide GNN-based classification. Extensive experiments wereconducted using the REST-meta-MDD dataset to demonstrate the effectiveness ofour model. With stratified 10-fold cross-validation, the best model obtained78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and78.98% F1-score. Further, atlas-based models consistently outperformed thecube-based approach, highlighting the importance of using domain-specificanatomical priors for MDD detection.</description>
      <author>example@mail.com (Nojod M. Alotaibi, Areej M. Alhothali, Manar S. Ali)</author>
      <guid isPermaLink="false">2509.12143v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework</title>
      <link>http://arxiv.org/abs/2509.12043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript has been accepted as a REGULAR PAPER in the  Transactions on Intelligent Transportation Systems 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的交通流预测框架，通过自适应邻接矩阵和天气因素调整来处理交通随机性，并使用自适应一致性预测进行不确定性量化，实验证明该方法具有更好的预测精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;交通流预测对管理拥堵、提高安全和优化交通系统至关重要，但由于城市交通的随机性和环境因素，这仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发能够适应由多个动态和复杂相互依赖因素影响的交通变异性的模型，以实现更好的预测。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图神经网络框架，使用对数正态分布和变异系数值的自适应邻接矩阵反映真实世界的行程时间变异性；通过温度、风速和降水等天气因素调整边权重，使GNN能够捕获交通站点之间不断变化的时空依赖关系；使用自适应一致性预测框架提供可靠的不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与基线方法相比，所提出的模型显示出更好的预测精度和不确定性边界；在SUMO中构建交通场景并应用蒙特卡洛模拟推导测试车辆的行程时间分布，模拟的平均行程时间落在INRIX历史数据定义的区间内。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型能够有效适应交通随机性和变化的环境条件，验证了模型的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;交通流预测对于管理拥堵、提高安全和优化各种交通系统至关重要。然而，由于城市交通的随机性和环境因素，这仍然是一个普遍存在的挑战。更好的预测需要能够适应由多个动态和复杂相互依赖因素影响的交通变异性的模型。在这项工作中，我们提出了一种图神经网络框架，通过使用对数正态分布和变异系数值的自适应邻接矩阵来处理随机性，以反映真实世界的行程时间变异性。此外，温度、风速和降水等天气因素调整边权重，使GNN能够捕获交通站点之间不断变化的时空依赖关系。这种对静态邻接矩阵的增强使模型能够有效适应交通随机性和变化的环境条件。此外，我们使用自适应一致性预测框架提供可靠的不确定性量化，在实现目标覆盖率的同时保持可接受的预测区间。实验结果表明，与基线方法相比，所提出的模型显示出更好的预测精度和不确定性边界。然后，我们通过在SUMO中构建交通场景并应用蒙特卡洛模拟来推导测试车辆的行程时间分布，以反映真实世界的变异性，从而验证了该方法。测试车辆的模拟平均行程时间落在INRIX历史数据定义的区间内，验证了模型的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic flow forecasting is essential for managing congestion, improvingsafety, and optimizing various transportation systems. However, it remains aprevailing challenge due to the stochastic nature of urban traffic andenvironmental factors. Better predictions require models capable ofaccommodating the traffic variability influenced by multiple dynamic andcomplex interdependent factors. In this work, we propose a Graph Neural Network(GNN) framework to address the stochasticity by leveraging adaptive adjacencymatrices using log-normal distributions and Coefficient of Variation (CV)values to reflect real-world travel time variability. Additionally, weatherfactors such as temperature, wind speed, and precipitation adjust edge weightsand enable GNN to capture evolving spatio-temporal dependencies across trafficstations. This enhancement over the static adjacency matrix allows the model toadapt effectively to traffic stochasticity and changing environmentalconditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP)framework to provide reliable uncertainty quantification, achieving targetcoverage while maintaining acceptable prediction intervals. Experimentalresults demonstrate that the proposed model, in comparison with baselinemethods, showed better prediction accuracy and uncertainty bounds. We, then,validate this method by constructing traffic scenarios in SUMO and applyingMonte-Carlo simulation to derive a travel time distribution for a Vehicle UnderTest (VUT) to reflect real-world variability. The simulated mean travel time ofthe VUT falls within the intervals defined by INRIX historical data, verifyingthe model's robustness.</description>
      <author>example@mail.com (Mayur Patil, Qadeer Ahmed, Shawn Midlam-Mohler)</author>
      <guid isPermaLink="false">2509.12043v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Prior Observations for Incremental 3D Scene Graph Prediction</title>
      <link>http://arxiv.org/abs/2509.11895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 24th International Conference on Machine Learning and  Applications (ICMLA'25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于增量3D语义场景图预测的新型异构图模型，该模型通过消息传递过程整合多模态信息，无需完整场景重建即可灵活融合全局和局部场景表示。&lt;h4&gt;背景&lt;/h4&gt;3D语义场景图(3DSSG)通过显式建模对象、属性和关系为环境提供紧凑结构化表示，但现有方法主要依赖传感器数据，未充分整合语义丰富环境中的信息，且大多假设可访问完整场景重建，限制了在现实世界增量设置中的适用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型异构图模型，用于增量3DSSG预测，整合多模态信息到消息传递过程中，使其能在不依赖完整场景重建的情况下工作。&lt;h4&gt;方法&lt;/h4&gt;提出使用多层异构图模型，通过消息传递过程整合多模态信息如语义嵌入和先验观测，灵活融合全局和局部场景表示，无需专门模块或完整场景重建。&lt;h4&gt;主要发现&lt;/h4&gt;在3DSSG数据集上评估表明，通过多模态信息增强的图神经网络为复杂现实世界环境提供了可扩展且可推广的解决方案。&lt;h4&gt;结论&lt;/h4&gt;所提出的架构通过整合多模态信息，为增量3D语义场景图预测提供了有效方法，适用于现实世界的增量设置。&lt;h4&gt;翻译&lt;/h4&gt;3D语义场景图(3DSSG)通过显式建模对象、属性和关系，为环境提供紧凑的结构化表示。尽管3DSSG在机器人和具身AI中显示出潜力，但现有方法主要依赖传感器数据，没有进一步整合语义丰富环境中的信息。此外，大多数方法假设可以访问完整的场景重建，限制了它们在现实世界增量设置中的适用性。本文介绍了一种用于增量3DSSG预测的新型异构图模型，该模型通过消息传递过程直接整合了额外的多模态信息，如先验观测。利用多层，该模型灵活地融合全局和局部场景表示，而无需专门模块或完整场景重建。我们在3DSSG数据集上评估了我们的方法，表明通过多模态信息（如语义嵌入（例如CLIP）和先验观测）增强的图神经网络为复杂现实世界环境提供了一种可扩展且可推广的解决方案。所提出架构的完整源代码将在https://github.com/m4renz/incremental-scene-graph-prediction上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D语义场景图(3DSSG)预测中两个关键问题：一是现有方法主要依赖传感器数据，没有充分利用环境中丰富的语义信息；二是大多数方法假设可以访问完整的场景重建，这在现实世界中是不切实际的。这个问题很重要，因为3DSSG在机器人和具身AI领域潜力巨大，能提供环境的紧凑结构化表示，而实际应用中场景通常是从传感器数据流逐步捕获的，需要模型能够利用先前观察的信息来预测新输入。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，发现它们缺乏通用的信息整合机制且不适用于增量场景生成。作者借鉴了SceneGraphFusion的增量生成思想和Feng等人的历史预测整合方法，但改进了这些方法。作者设计了一个多层异构图模型，融合全局和局部信息，将先前观察直接整合到消息传递过程中，而不是像现有方法那样只使用更新的几何信息或显式编码全局信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个两层异构图架构：全局场景图积累先前帧的观察和关系，局部场景图处理当前帧数据，并通过连接匹配的对象实例使信息在两层间流动。整体流程是：1)预处理RGB-D帧数据；2)构建图模型，包括对象分割、点云转换和边连接；3)提取节点特征，包括点云采样和几何描述符计算；4)使用异构图神经网络进行消息传递；5)通过多层感知机进行节点和边分类预测；6)将预测结果合并到全局图中供后续使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新型异构图模型整合多模态信息用于增量预测；2)多层架构灵活整合全局和局部场景表示；3)将先前观察直接嵌入消息传递过程；4)展示对错误预测的鲁棒性；5)能无缝整合额外信息而无需修改核心架构。相比之前工作，不同之处在于：与SceneGraphFusion相比，直接整合先前的预测；与Feng等人的方法相比，不显式编码全局信息；与其他多模态方法相比，首次应用于增量3DSSG生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的异构图模型，通过整合先前观察的多模态信息，实现了高效的增量3D场景图预测，显著提升了机器人在复杂环境中的感知和理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D semantic scene graphs (3DSSG) provide compact structured representationsof environments by explicitly modeling objects, attributes, and relationships.While 3DSSGs have shown promise in robotics and embodied AI, many existingmethods rely mainly on sensor data, not integrating further information fromsemantically rich environments. Additionally, most methods assume access tocomplete scene reconstructions, limiting their applicability in real-world,incremental settings. This paper introduces a novel heterogeneous graph modelfor incremental 3DSSG prediction that integrates additional, multi-modalinformation, such as prior observations, directly into the message-passingprocess. Utilizing multiple layers, the model flexibly incorporates global andlocal scene representations without requiring specialized modules or full scenereconstructions. We evaluate our approach on the 3DSSG dataset, showing thatGNNs enriched with multi-modal information such as semantic embeddings (e.g.,CLIP) and prior observations offer a scalable and generalizable solution forcomplex, real-world environments. The full source code of the presentedarchitecture will be made available athttps://github.com/m4renz/incremental-scene-graph-prediction.</description>
      <author>example@mail.com (Marian Renz, Felix Igelbrink, Martin Atzmueller)</author>
      <guid isPermaLink="false">2509.11895v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Descriptor and Graph-based Molecular Representations in Prediction of Copolymer Properties Using Machine Learning</title>
      <link>http://arxiv.org/abs/2509.11874v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究使用机器学习方法预测共聚物的物理性质，通过两种不同的分子表示方法建立了预测模型，并发现不同表示方法对不同类型的物理性质预测效果不同。&lt;h4&gt;背景&lt;/h4&gt;共聚物是高度通用的材料，具有广泛的化学成分可能性。通过计算方法预测性质可以加速共聚物设计，优先选择具有有利性质的候选材料。&lt;h4&gt;目的&lt;/h4&gt;利用两种不同的分子集合表示方法，通过机器学习预测共聚物的七种不同物理性质，并比较不同表示方法的预测效果。&lt;h4&gt;方法&lt;/h4&gt;1. 使用随机森林模型从分子描述符预测聚合物性质；2. 使用图神经网络从2D聚合物图预测相同性质，包括单任务和多任务设置；3. 构建了一个包含140种具有不同单体组成和构型的二元共聚物的分子动力学模拟数据集；4. 训练和评估模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基于描述符的随机森林模型在预测密度和定压/定容比热容方面表现出色，因为这些性质与分子描述符捕获的特定分子特征密切相关；2. 图表示方法更好地预测膨胀系数和体积模量，因为这些性质更依赖于图模型更好地捕获的复杂结构相互作用。&lt;h4&gt;结论&lt;/h4&gt;这项研究强调了选择适当表示方法预测分子性质的重要性。研究结果展示了机器学习模型如何通过可学习的结构-性质关系加速共聚物发现，简化聚合物设计，促进高性能材料在多种应用中的开发。&lt;h4&gt;翻译&lt;/h4&gt;共聚物是高度通用的材料，具有广泛的化学成分可能性。通过使用计算方法进行性质预测，可以加速共聚物设计，优先选择具有有利性质的候选材料。在本研究中，我们利用分子集合的两种不同表示方法，通过机器学习预测共聚物的七种不同物理性质：我们使用随机森林模型从分子描述符预测聚合物性质，使用图神经网络从2D聚合物图预测相同性质，包括单任务和多任务设置。为了训练和评估模型，我们构建了一个数据集，包含来自分子动力学模拟的140种具有不同单体组成和构型的二元共聚物。我们的结果表明，基于描述符的随机森林在预测密度和定压/定容比热容方面表现出色，因为这些性质与分子描述符捕获的特定分子特征密切相关。相比之下，图表示方法更好地预测膨胀系数和体积模量，因为这些性质更依赖于图模型更好地捕获的复杂结构相互作用。这项研究强调了选择适当表示方法预测分子性质的重要性。我们的研究展示了机器学习模型如何通过可学习的结构-性质关系加速共聚物发现，简化聚合物设计，促进高性能材料在多种应用中的开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Copolymers are highly versatile materials with a vast range of possiblechemical compositions. By using computational methods for property prediction,the design of copolymers can be accelerated, allowing for the prioritization ofcandidates with favorable properties. In this study, we utilized two distinctrepresentations of molecular ensembles to predict the seven different physicalpolymer properties copolymers using machine learning: we used a random forest(RF) model to predict polymer properties from molecular descriptors, and agraph neural network (GNN) to predict the same properties from 2D polymergraphs under both a single- and multi-task setting. To train and evaluate themodels, we constructed a data set from molecular dynamic simulations for 140binary copolymers with varying monomer compositions and configurations. Ourresults demonstrate that descriptors-based RFs excel at predicting density andspecific heat capacities at constant pressure (Cp) and volume (Cv) becausethese properties are strongly tied to specific molecular features captured bymolecular descriptors. In contrast, graph representations better predictexpansion coefficients ({\gamma}, {\alpha}) and bulk modulus (K), which dependmore on complex structural interactions better captured by graph-based models.This study underscores the importance of choosing appropriate representationsfor predicting molecular properties. Our findings demonstrate how machinelearning models can expedite copolymer discovery with learnablestructure-property relationships, streamlining polymer design and advancing thedevelopment of high-performance materials for diverse applications.</description>
      <author>example@mail.com (Elaheh Kazemi-Khasragh, Rocío Mercado, Carlos Gonzalez, Maciej Haranczyk)</author>
      <guid isPermaLink="false">2509.11874v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Visualization and Analysis of the Loss Landscape in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.11792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过引入可学习的降维方法来可视化GNN损失景观，并分析了多种因素对GNN优化的影响，为开发更高效的GNN架构和训练策略提供了见解。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是处理图结构数据的强大模型，有广泛应用，但其参数优化、表达能力和泛化能力之间的相互作用仍理解不充分。&lt;h4&gt;目的&lt;/h4&gt;引入一种高效的、可学习的降维方法来可视化GNN损失景观，并分析过平滑、量化、稀疏化和预调节器对GNN优化的影响。&lt;h4&gt;方法&lt;/h4&gt;提出一种可学习的投影方法，优于基于PCA的最先进方法，能够以更低的内存使用量准确重建高维参数。&lt;h4&gt;主要发现&lt;/h4&gt;架构、稀疏化和优化器的预调节显著影响GNN优化景观，进而影响训练过程和最终预测性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现有助于开发更高效的GNN架构设计和训练策略。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络是处理图结构数据的强大模型，有广泛应用。然而，GNN参数优化、表达能力和泛化能力之间的相互作用仍理解不充分。我们通过引入一种高效的、可学习的降维方法来可视化GNN损失景观，并分析了过平滑、跳跃知识、量化、稀疏化和预调节器对GNN优化的影响。我们的可学习投影方法优于基于PCA的最先进方法，能够以更低的内存使用量准确重建高维参数。我们进一步表明，架构、稀疏化和优化器的预调节显著影响GNN优化景观及其训练过程和最终预测性能。这些见解有助于开发更高效的GNN架构设计和训练策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-04552-2_9&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are powerful models for graph-structured data,with broad applications. However, the interplay between GNN parameteroptimization, expressivity, and generalization remains poorly understood. Weaddress this by introducing an efficient learnable dimensionality reductionmethod for visualizing GNN loss landscapes, and by analyzing the effects ofover-smoothing, jumping knowledge, quantization, sparsification, andpreconditioner on GNN optimization. Our learnable projection method surpassesthe state-of-the-art PCA-based approach, enabling accurate reconstruction ofhigh-dimensional parameters with lower memory usage. We further show thatarchitecture, sparsification, and optimizer's preconditioning significantlyimpact the GNN optimization landscape and their training process and finalprediction performance. These insights contribute to developing more efficientdesigns of GNN architectures and training strategies.</description>
      <author>example@mail.com (Samir Moustafa, Lorenz Kummer, Simon Fetzel, Nils M. Kriege, Wilfried N. Gansterer)</author>
      <guid isPermaLink="false">2509.11792v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Regression for Enzyme Turnover Rates Prediction</title>
      <link>http://arxiv.org/abs/2509.11782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures. This paper was withdrawn from the IJCAI 2025  proceedings due to the lack of participation in the conference and  presentation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多模态框架，用于预测酶转换率，整合了酶序列、底物结构和环境因素，实现了可解释且准确的预测。&lt;h4&gt;背景&lt;/h4&gt;酶转换率是酶动力学中的基本参数，反映了酶的催化效率。然而，由于实验测量的高成本和复杂性，大多数生物体中的酶转换率数据仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;解决酶转换率数据稀缺的问题，开发一种能够准确预测酶转换率的工具，为酶动力学研究和相关应用提供支持。&lt;h4&gt;方法&lt;/h4&gt;研究提出的多模态框架结合了预训练语言模型和卷积神经网络提取蛋白质序列特征，使用图神经网络捕获底物分子表示，并加入注意力机制增强酶和底物间的相互作用。同时利用Kolmogorov-Arnold网络的符号回归学习控制酶转换率的数学公式。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该框架优于传统的和最先进的深度学习方法，为研究酶动力学提供了强大的工具。&lt;h4&gt;结论&lt;/h4&gt;这项工作为研究酶动力学提供了强大工具，在酶工程、生物技术和工业生物催化应用方面具有广阔前景。&lt;h4&gt;翻译&lt;/h4&gt;酶转换率是酶动力学中的一个基本参数，反映了酶的催化效率。然而，由于实验测量的高成本和复杂性，大多数生物体中的酶转换率数据仍然稀缺。为解决这一差距，我们提出了一种多模态框架来预测酶转换率，该框架整合了酶序列、底物结构和环境因素。我们的模型结合了预训练语言模型和卷积神经网络来提取蛋白质序列特征，同时图神经网络捕获底物分子的信息表示。加入了注意力机制以增强酶和底物表示之间的相互作用。此外，我们利用Kolmogorov-Arnold网络的符号回归来明确学习控制酶转换率的数学公式，实现了可解释且准确的预测。大量实验表明，我们的框架优于传统的和最先进的深度学习方法。这项工作为研究酶动力学提供了强大的工具，并在酶工程、生物技术和工业生物催化应用方面具有广阔前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The enzyme turnover rate is a fundamental parameter in enzyme kinetics,reflecting the catalytic efficiency of enzymes. However, enzyme turnover ratesremain scarce across most organisms due to the high cost and complexity ofexperimental measurements. To address this gap, we propose a multimodalframework for predicting the enzyme turnover rate by integrating enzymesequences, substrate structures, and environmental factors. Our model combinesa pre-trained language model and a convolutional neural network to extractfeatures from protein sequences, while a graph neural network capturesinformative representations from substrate molecules. An attention mechanism isincorporated to enhance interactions between enzyme and substraterepresentations. Furthermore, we leverage symbolic regression viaKolmogorov-Arnold Networks to explicitly learn mathematical formulas thatgovern the enzyme turnover rate, enabling interpretable and accuratepredictions. Extensive experiments demonstrate that our framework outperformsboth traditional and state-of-the-art deep learning approaches. This workprovides a robust tool for studying enzyme kinetics and holds promise forapplications in enzyme engineering, biotechnology, and industrial biocatalysis.</description>
      <author>example@mail.com (Bozhen Hu, Cheng Tan, Siyuan Li, Jiangbin Zheng, Sizhe Qiu, Jun Xia, Stan Z. Li)</author>
      <guid isPermaLink="false">2509.11782v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SpaPool: Soft Partition Assignment Pooling for__Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.11675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SpaPool的新型池化方法，结合了密集和稀疏技术的优点，用于图神经网络处理。&lt;h4&gt;背景&lt;/h4&gt;图神经网络处理需要有效的池化方法来减少图的大小同时保持结构完整性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效减少图大小同时保持结构完整性的池化方法。&lt;h4&gt;方法&lt;/h4&gt;SpaPool通过将顶点分组为自适应数量的簇，结合密集和稀疏技术的优势来实现图的池化。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的实验表明，SpaPool与现有池化技术相比具有竞争力，特别是在小规模图上表现优异。&lt;h4&gt;结论&lt;/h4&gt;SpaPool是一种有前途的方法，适用于需要高效有效图处理的应用。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了SpaPool，一种新颖的池化方法，它结合了密集和稀疏技术的优点，用于图神经网络。SpaPool将顶点分组为自适应数量的簇，利用密集和稀疏方法的优势。它旨在在保持图结构完整性的同时高效减少其大小。在多个数据集上的实验结果表明，与现有的池化技术相比，SpaPool具有竞争力的性能，并特别在小规模图上表现出色。这使得SpaPool成为需要高效有效图处理的应用的一种有前途的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SpaPool, a novel pooling method that combines thestrengths of both dense and sparse techniques for a graph neural network.SpaPool groups vertices into an adaptive number of clusters, leveraging thebenefits of both dense and sparse approaches. It aims to maintain thestructural integrity of the graph while reducing its size efficiently.Experimental results on several datasets demonstrate that SpaPool achievescompetitive performance compared to existing pooling techniques and excelsparticularly on small-scale graphs. This makes SpaPool a promising method forapplications requiring efficient and effective graph processing.</description>
      <author>example@mail.com (Rodrigue Govan, Romane Scherrer, Philippe Fournier-Viger, Nazha Selmaoui-Folcher)</author>
      <guid isPermaLink="false">2509.11675v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Drug Repurposing Using Deep Embedded Clustering and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.11493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 2025 International Conference on Machine Learning and  Applications (ICMLA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合无监督深度嵌入聚类和监督图神经网络的机器学习流程，用于从多组学数据中识别新的药物-疾病链接。&lt;h4&gt;背景&lt;/h4&gt;药物再利用在历史上是一个经济上不可行的过程，用于识别废弃药物的新用途。现代机器学习能够识别候选药物中的复杂生化细节，但许多研究依赖于具有已知药物-疾病相似性的简化数据集。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习流程，使用无监督深度嵌入聚类与监督图神经网络链接预测相结合，从多组学数据中识别新的药物-疾病链接。&lt;h4&gt;方法&lt;/h4&gt;使用无监督自编码器和聚类训练将组学数据的维度压缩为潜在嵌入。将9022种独特药物划分为35个簇，并使用图神经网络进行链接预测。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络实现了强大的统计性能，预测准确率为0.901，ROC曲线下面积为0.960，F1得分为0.901。生成了一个包含477个每个簇链接概率超过99%的排序列表。&lt;h4&gt;结论&lt;/h4&gt;这项研究可能在不同疾病领域提供新的药物-疾病链接前景，同时推进对药物再利用研究中机器学习的理解。&lt;h4&gt;翻译&lt;/h4&gt;药物再利用在历史上一直是识别废弃药物新用途的经济上不可行的过程。现代机器学习使得识别候选药物中的复杂生化细节成为可能；然而，许多研究依赖于具有已知药物-疾病相似性的简化数据集。我们提出了一种机器学习流程，使用无监督深度嵌入聚类，结合监督图神经网络链接预测，从多组学数据中识别新的药物-疾病链接。无监督自编码器和聚类训练将组学数据的维度压缩为压缩的潜在嵌入。总共9022种独特药物被划分为35个簇，平均轮廓得分为0.8550。图神经网络实现了强大的统计性能，预测准确率为0.901，受试者工作特征曲线下面积为0.960，F1得分为0.901。生成了一个包含477个每个簇链接概率超过99%的排序列表。这项研究可能在不同疾病领域提供新的药物-疾病链接前景，同时推进对药物再利用研究中机器学习的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug repurposing has historically been an economically infeasible process foridentifying novel uses for abandoned drugs. Modern machine learning has enabledthe identification of complex biochemical intricacies in candidate drugs;however, many studies rely on simplified datasets with known drug-diseasesimilarities. We propose a machine learning pipeline that uses unsuperviseddeep embedded clustering, combined with supervised graph neural network linkprediction to identify new drug-disease links from multi-omic data.Unsupervised autoencoder and cluster training reduced the dimensionality ofomic data into a compressed latent embedding. A total of 9,022 unique drugswere partitioned into 35 clusters with a mean silhouette score of 0.8550. Graphneural networks achieved strong statistical performance, with a predictionaccuracy of 0.901, receiver operating characteristic area under the curve of0.960, and F1-Score of 0.901. A ranked list comprised of 477 per-cluster linkprobabilities exceeding 99 percent was generated. This study could provide newdrug-disease link prospects across unrelated disease domains, while advancingthe understanding of machine learning in drug repurposing studies.</description>
      <author>example@mail.com (Luke Delzer, Robert Kroleski, Ali K. AlShami, Jugal Kalita)</author>
      <guid isPermaLink="false">2509.11493v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Graph Attention Networks: Trainable Quantum Encoders for Inductive Graph Learning</title>
      <link>http://arxiv.org/abs/2509.11390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了量子图注意力网络（QGATs）作为图上进行归纳学习的可训练量子编码器，扩展了量子图神经网络框架。QGATs通过量子注意力机制动态调节邻居节点的贡献，实现了具有局部感知能力的量子表示。&lt;h4&gt;背景&lt;/h4&gt;量子图神经网络（QGNN）框架为图数据提供了量子编码方法，但在处理复杂图结构时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效编码节点特征和邻域结构的量子神经网络，提高在图数据上的归纳学习能力，特别是在化学性质预测任务中。&lt;h4&gt;方法&lt;/h4&gt;利用参数化量子电路编码节点特征和邻域结构，通过动态学习的酉算子实现量子注意力机制，调节每个邻居节点的贡献。在QM9数据集上评估该方法，预测各种化学性质，并与经典和量子图神经网络进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;注意力机制在经典和量子图神经网络中都能提高性能；量子注意力随着图大小的增长而带来更大的益处；在较大分子图上，QGATs显著优于无注意力的量子对应模型；对于较小图，QGATs实现了与经典GAT模型相当的预测精度。&lt;h4&gt;结论&lt;/h4&gt;量子注意力机制有潜力增强量子图神经网络在化学及其他领域的归纳能力，QGATs作为表达性量子编码器具有可行性和优势。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了量子图注意力网络（QGATs）作为图上进行归纳学习的可训练量子编码器，扩展了量子图神经网络（QGNN）框架。QGATs利用参数化量子电路编码节点特征和邻域结构，通过量子注意力机制经由动态学习的酉算子调节每个邻居的贡献。这使得能够表达具有局部感知能力的量子表示，并能推广到未见过的图实例。我们在QM9数据集上评估了我们的方法，目标是预测各种化学性质。我们的实验比较了经典和量子图神经网络（有和没有注意力层），表明注意力在两种范式中都能提高性能。值得注意的是，我们观察到量子注意力随着图大小的增长而带来更大的益处，在较大的分子图上，QGATs显著优于无注意力的量子对应模型。此外，对于较小的图，QGATs实现了与经典GAT模型相当的预测精度，突显了它们作为表达性量子编码器的可行性。这些结果表明量子注意力机制有潜力增强QGNN在化学及其他领域的归纳能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Quantum Graph Attention Networks (QGATs) as trainable quantumencoders for inductive learning on graphs, extending the Quantum Graph NeuralNetworks (QGNN) framework. QGATs leverage parameterized quantum circuits toencode node features and neighborhood structures, with quantum attentionmechanisms modulating the contribution of each neighbor via dynamically learnedunitaries. This allows for expressive, locality-aware quantum representationsthat can generalize across unseen graph instances. We evaluate our approach onthe QM9 dataset, targeting the prediction of various chemical properties. Ourexperiments compare classical and quantum graph neural networks-with andwithout attention layers-demonstrating that attention consistently improvesperformance in both paradigms. Notably, we observe that quantum attentionyields increasing benefits as graph size grows, with QGATs significantlyoutperforming their non-attentive quantum counterparts on larger moleculargraphs. Furthermore, for smaller graphs, QGATs achieve predictive accuracycomparable to classical GAT models, highlighting their viability as expressivequantum encoders. These results show the potential of quantum attentionmechanisms to enhance the inductive capacity of QGNN in chemistry and beyond.</description>
      <author>example@mail.com (Arthur M. Faria, Mehdi Djellabi, Igor O. Sokolov, Savvas Varsamopoulos)</author>
      <guid isPermaLink="false">2509.11390v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>BIGNet: Pretrained Graph Neural Network for Embedding Semantic, Spatial, and Topological Data in BIM Models</title>
      <link>http://arxiv.org/abs/2509.11104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了首个大规模图神经网络BIGNet，用于学习和重用BIM模型中的多维设计特征，解决了现有模型忽略BIM中语义、空间和拓扑特征的问题。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型在土木工程中显示出显著优势，但主要关注文本和视觉数据，忽略了BIM模型中丰富的语义、空间和拓扑特征。&lt;h4&gt;目的&lt;/h4&gt;开发第一个大规模图神经网络（GNN）BIGNet，以学习和重用BIM模型中嵌入的多维设计特征。&lt;h4&gt;方法&lt;/h4&gt;引入可扩展的图表示编码BIM组件的'语义-空间-拓扑'特征，创建包含近100万节点和350万条边的数据集；通过向GraphMAE2引入新消息传递机制提出BIGNet，并使用节点掩蔽策略进行预训练；在基于BIM的设计检查任务中评估BIGNet。&lt;h4&gt;主要发现&lt;/h4&gt;1) 同构图在学习设计特征方面优于异构图；2) 考虑30厘米半径内的局部空间关系可以提高性能；3) 基于GAT的特征提取的BIGNet实现了最佳迁移学习结果。&lt;h4&gt;结论&lt;/h4&gt;BIGNet使平均F1分数比未预训练模型提高72.7%，证明了其在学习和转移BIM设计特征方面的有效性，促进了这些特征在设计和生命周期管理中的自动化应用。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型在土木工程中显示出显著优势，但它们主要关注文本和视觉数据，忽略了BIM（建筑信息建模）模型中丰富的语义、空间和拓扑特征。因此，本研究开发了第一个大规模图神经网络（GNN）BIGNet，以学习和重用BIM模型中嵌入的多维设计特征。首先，引入了一种可扩展的图表示来编码BIM组件的'语义-空间-拓扑'特征，并创建了一个包含近100万个节点和350万条边的数据集。随后，通过向GraphMAE2引入新的消息传递机制提出了BIGNet，并使用节点掩蔽策略进行了进一步预训练。最后，在基于BIM的设计检查的各种迁移学习任务中评估了BIGNet。结果表明：1）同构图在学习设计特征方面优于异构图；2）考虑30厘米半径内的局部空间关系可以提高性能；3）基于GAT（图注意力网络）的特征提取的BIGNet实现了最佳的迁移学习结果。这一创新使平均F1分数比未预训练的模型提高了72.7%，证明了它在学习和转移BIM设计特征方面的有效性，并促进了它们在未来的设计和生命周期管理中的自动化应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有大型基础模型忽略BIM模型中丰富的语义、空间和拓扑特征的问题，以及BIM数据表示方法缺乏通用性和标注数据稀缺的问题。这个问题在现实中非常重要，因为BIM是建筑行业数字化的重要工具，包含大量设计知识和经验；有效提取和重用这些知识可以提高设计质量，减少施工错误，降低成本，并提升建筑全生命周期管理效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了BIM模型特点和现有方法局限性，指出BIM包含复杂几何数据、非几何信息和关系数据，而深度学习需要结构化输入。作者借鉴了图神经网络处理非欧几里得数据的能力，基于GraphMAE2架构进行改进，引入新的消息传递机制。同时利用预训练和迁移学习技术解决标注数据稀缺问题，并扩展了统一的基于网络的表示方法，以支持更全面的特征编码。整个设计过程体现了对现有工作的批判性继承和创新性发展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络表示和处理BIM模型中的语义、空间和拓扑特征，通过预训练学习隐式设计知识，并利用迁移学习应用于多种下游任务。整体流程分为三步：1) 开发BIM特定的可扩展图表示方法，将BIM模型转换为图结构，提取并编码组件的多维特征；2) 基于改进的GraphMAE2架构，使用节点掩码策略(50%掩码率)进行自监督预训练，引入新的消息传递机制；3) 在三种BIM设计检查任务(语义冲突、数据范围错误和拓扑错误)中评估模型，使用混淆矩阵和F1分数等指标衡量性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个大规模预训练图神经网络(BIGNet)用于学习BIM中的多维特征；2) BIM特定的可扩展图表示方法，统一编码语义、空间和拓扑特征；3) 改进的预训练策略，引入新的消息传递机制；4) 多任务设计检查框架。相比之前的工作，不同之处在于：现有方法主要针对特定任务设计特征提取，缺乏通用性；需要大量标注数据，而BIGNet通过预训练减少标注需求；通常只处理单一错误类型，而BIGNet支持多任务检查；图结构设计更灵活，可根据任务需求调整。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BIGNet通过创新的BIM图表示方法和预训练图神经网络，有效提取和重用了建筑信息模型中的语义、空间和拓扑设计知识，实现了多任务自动化设计检查，显著提高了建筑设计的质量和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Foundation Models (LFMs) have demonstrated significant advantages incivil engineering, but they primarily focus on textual and visual data,overlooking the rich semantic, spatial, and topological features in BIM(Building Information Modelling) models. Therefore, this study develops thefirst large-scale graph neural network (GNN), BIGNet, to learn, and reusemultidimensional design features embedded in BIM models. Firstly, a scalablegraph representation is introduced to encode the "semantic-spatial-topological"features of BIM components, and a dataset with nearly 1 million nodes and 3.5million edges is created. Subsequently, BIGNet is proposed by introducing a newmessage-passing mechanism to GraphMAE2 and further pretrained with a nodemasking strategy. Finally, BIGNet is evaluated in various transfer learningtasks for BIM-based design checking. Results show that: 1) homogeneous graphrepresentation outperforms heterogeneous graph in learning design features, 2)considering local spatial relationships in a 30 cm radius enhances performance,and 3) BIGNet with GAT (Graph Attention Network)-based feature extractionachieves the best transfer learning results. This innovation leads to a 72.7%improvement in Average F1-score over non-pretrained models, demonstrating itseffectiveness in learning and transferring BIM design features and facilitatingtheir automated application in future design and lifecycle management.</description>
      <author>example@mail.com (Jin Han, Xin-Zheng Lu, Jia-Rui Lin)</author>
      <guid isPermaLink="false">2509.11104v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>CogGNN: Cognitive Graph Neural Networks in Generative Connectomics</title>
      <link>http://arxiv.org/abs/2509.10864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CogGNN，第一个认知生成模型，使图神经网络具有认知能力，能够生成保留认知特征的脑网络，并在连接性脑模板学习任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;生成式学习已推进网络神经科学，支持图超分辨率、时间图预测和多模态脑图融合等任务，但当前基于图神经网络的方法仅关注结构和拓扑特性，忽略了认知特征。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够整合认知特征的生成模型，特别是视觉输入，以生成在认知和结构上都有意义的脑网络和连接性脑模板。&lt;h4&gt;方法&lt;/h4&gt;提出CogGNN，一种认知感知的生成模型，具有基于视觉记忆的损失函数，以及一种连接性脑模板学习框架，采用共同优化策略产生居中良好、可区分且认知增强的模板。&lt;h4&gt;主要发现&lt;/h4&gt;CogGNN生成的连接性脑模板在认知和结构上都有意义，且大量实验表明CogGNN优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;CogGNN为认知基础的脑网络建模奠定了坚实基础，是第一个将认知能力整合到图神经网络中的生成模型。&lt;h4&gt;翻译&lt;/h4&gt;生成式学习已推进网络神经科学，使图超分辨率、时间图预测和多模态脑图融合等任务成为可能。然而，当前主要基于图神经网络(GNNs)的方法仅关注结构和拓扑特性，忽略了认知特征。为此，我们引入了第一个认知生成模型CogGNN，它赋予GNN认知能力（如视觉记忆），以生成保留认知特征的脑网络。虽然具有广泛适用性，我们提出了CogGNN，这是一种特殊变体，旨在整合视觉输入，这是大脑功能（如图案识别和记忆回忆）的关键因素。作为概念验证，我们使用该模型学习连接性脑模板(CBTs)，这是来自多视图脑网络的群体级指纹。与之前忽略认知属性的工作不同，CogGNN生成的CBT在认知和结构上都有意义。我们的贡献是：(i) 一种新颖的认知感知生成模型，具有基于视觉记忆的损失函数；(ii) 一种CBT学习框架，采用共同优化策略，产生居中良好、可区分且认知增强的模板。大量实验表明，CogGNN优于最先进的方法，为认知基础的脑网络建模奠定了坚实基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative learning has advanced network neuroscience, enabling tasks likegraph super-resolution, temporal graph prediction, and multimodal brain graphfusion. However, current methods, mainly based on graph neural networks (GNNs),focus solely on structural and topological properties, neglecting cognitivetraits. To address this, we introduce the first cognified generative model,CogGNN, which endows GNNs with cognitive capabilities (e.g., visual memory) togenerate brain networks that preserve cognitive features. While broadlyapplicable, we present CogGNN, a specific variant designed to integrate visualinput, a key factor in brain functions like pattern recognition and memoryrecall. As a proof of concept, we use our model to learn connectional braintemplates (CBTs), population-level fingerprints from multi-view brain networks.Unlike prior work that overlooks cognitive properties, CogGNN generates CBTsthat are both cognitively and structurally meaningful. Our contributions are:(i) a novel cognition-aware generative model with a visual-memory-based loss;(ii) a CBT-learning framework with a co-optimization strategy to yieldwell-centered, discriminative, cognitively enhanced templates. Extensiveexperiments show that CogGNN outperforms state-of-the-art methods, establishinga strong foundation for cognitively grounded brain network modeling.</description>
      <author>example@mail.com (Mayssa Soussia, Yijun Lin, Mohamed Ali Mahjoub, Islem Rekik)</author>
      <guid isPermaLink="false">2509.10864v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Context--Aware Inputs: Physics--Inspired Improvements in Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.10684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了物理信息策略以提高图神经网络在液氩时间投影室事件重建任务中的性能，特别是针对代表性不足的粒子类别如米歇尔电子。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在液氩时间投影室的事件重建任务中显示出强大潜力，但对于米歇尔电子等代表性不足的粒子类别性能仍然有限。&lt;h4&gt;目的&lt;/h4&gt;研究基于物理信息的策略，以改进NuGraph2架构中的语义分割性能。&lt;h4&gt;方法&lt;/h4&gt;探索三种互补方法：(i)通过探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示；(ii)引入辅助解码器捕获类级别相关性；(iii)整合基于能量的正则化项，受米歇尔电子能量分布启发。&lt;h4&gt;主要发现&lt;/h4&gt;物理启发的特征增强带来最大收益，特别是通过解纠缠重叠潜在空间区域提高米歇尔电子精确率和召回率；辅助解码器和能量正则化项改进有限，部分原因是NuGraph2缺乏明确的粒子或事件级别表示。&lt;h4&gt;结论&lt;/h4&gt;将物理上下文直接嵌入到节点级输入中比施加特定任务的辅助损失更有效；具有明确粒子级和事件级推理的分层架构如NuGraph3将为高级解码器和基于物理的正则化提供更自然的环境。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近在液氩时间投影室的事件重建任务中显示出强大的前景，然而对于代表性不足的粒子类别，如米歇尔电子，其性能仍然有限。在这项工作中，我们研究基于物理信息的策略以改进NuGraph2架构中的语义分割。我们探索了三种互补方法：(i)通过探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示；(ii)引入辅助解码器以捕获类级别相关性；(iii)整合受米歇尔电子能量分布启发的基于能量的正则化项。在MicroBooNE公共数据集上的实验表明，物理启发的特征增强带来最大收益，特别是在通过解纠缠重叠的潜在空间区域显著提高米歇尔电子的精确率和召回率。相比之下，辅助解码器和能量正则化项提供的改进有限，部分原因是NuGraph2的命中级别性质，缺乏明确的粒子或事件级别表示。我们的发现强调将物理上下文直接嵌入到节点级输入中比施加特定任务的辅助损失更有效，并表明未来的分层架构如NuGraph3，具有明确的粒子级和事件级推理，将为高级解码器和基于物理的正则化提供更自然的环境。本工作的代码已在Github上公开：https://github.com/vitorgrizzi/nugraph_phys/tree/main_phys。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决图神经网络在液态氩时间投影室(LArTPC)粒子事件重建中对代表性不足的粒子类别(如Michel电子)分类性能不佳的问题。这个问题很重要，因为Michel电子等稀有粒子的准确识别对粒子物理实验至关重要，它们提供了关于粒子行为和相互作用的关键信息，而当前模型对这些稀有粒子的识别能力有限，影响了科学家对基本粒子行为的理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了图神经网络在粒子物理任务中的局限性，特别是对稀有类别识别不足的问题。他们借鉴了三种主要策略：通过输入特征注入物理知识、使用辅助解码器捕获类别间相关性、在损失函数中加入基于物理的约束项。作者参考了多项现有工作，如Drielsma等人的GrapPA方法引入几何描述符，Kiesler等人结合CNN与物理特征，以及DUNE协作的CVN多任务学习框架，这些工作启发了作者将物理学知识深度融入神经网络的不同方式。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在图神经网络的输入表示中直接嵌入物理上下文来增强模型表达能力，特别是改善对稀有粒子的识别，而不是仅依赖辅助损失函数来强制执行物理约束。整体实现包括三个主要步骤：1)特征扩展：在原始四个特征基础上添加节点度、最近邻距离和双差分特征，编码结构和关系上下文；2)添加额外解码器：引入预测事件中粒子类别分布的解码器，利用类别间相关性；3)Michel能量正则化：在损失函数中加入基于物理的约束项，惩罚偏离预期能量分布的分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)设计特定的物理启发特征，直接编码节点结构和关系上下文；2)使用全局注意力机制实现有效的图级表示；3)基于Michel电子能量分布特性设计特定正则化项。相比之前的工作，本研究强调在输入表示中嵌入物理上下文的重要性，而非仅通过损失函数强制执行物理约束；专注于语义分割而非粒子聚类；针对稀有类别识别而非多任务学习；将物理知识注入图神经网络的特定组件而非直接强制执行物理方程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过在图神经网络的输入表示中融入物理学启发的上下文感知特征，显著提高了对稀有粒子类别(如Michel电子)的语义分割性能，证明了直接在节点级嵌入物理知识比通过辅助损失函数强制执行物理约束更有效。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have recently shown strong promise for eventreconstruction tasks in Liquid Argon Time Projection Chambers, yet theirperformance remains limited for underrepresented classes of particles, such asMichel electrons. In this work, we investigate physics-informed strategies toimprove semantic segmentation within the NuGraph2 architecture. We explorethree complementary approaches: (i) enriching the input representation withcontext-aware features derived from detector geometry and track continuity,(ii) introducing auxiliary decoders to capture class-level correlations, and(iii) incorporating energy-based regularization terms motivated by Michelelectron energy distributions. Experiments on MicroBooNE public datasets showthat physics-inspired feature augmentation yields the largest gains,particularly boosting Michel electron precision and recall by disentanglingoverlapping latent space regions. In contrast, auxiliary decoders andenergy-regularization terms provided limited improvements, partly due to thehit-level nature of NuGraph2, which lacks explicit particle- or event-levelrepresentations. Our findings highlight that embedding physics context directlyinto node-level inputs is more effective than imposing task-specific auxiliarylosses, and suggest that future hierarchical architectures such as NuGraph3,with explicit particle- and event-level reasoning, will provide a more naturalsetting for advanced decoders and physics-based regularization. The code forthis work is publicly available on Github athttps://github.com/vitorgrizzi/nugraph_phys/tree/main_phys.</description>
      <author>example@mail.com (Vitor F. Grizzi, Margaret Voetberg, Giuseppe Cerati, Hadi Meidani)</author>
      <guid isPermaLink="false">2509.10684v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Explainability: Post-hoc Explanations for Geometric Neural Network Predictions</title>
      <link>http://arxiv.org/abs/2509.10676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了人工智能在科学应用中的可解释性问题，为现有的中微子标记图神经网络NuGraph2引入了解释性附加组件，展示了多种解释性技术如何结合使用以揭示AI模型的推理过程。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能在科学应用中的日益普及，将结果归因于网络的推理过程变得至关重要，这对于支持稳健的科学泛化具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;强调并展示事后解释性方法在科学应用中AI方法上的需求，并通过具体案例演示其应用价值。&lt;h4&gt;方法&lt;/h4&gt;为NuGraph2图神经网络引入一系列解释性技术，包括检查网络输出（节点分类）、分析节点间的边连接关系，以及使用新开发的通用工具探测潜在空间。这些方法不仅限于图神经网络，可应用于广泛的网络类型。&lt;h4&gt;主要发现&lt;/h4&gt;单独使用任何一种解释方法都不足以展示网络的'理解'能力，但将这些方法结合使用可以提供对分类过程中所使用方法的深入见解。&lt;h4&gt;结论&lt;/h4&gt;虽然这些解释性方法在NuGraph2应用上进行了测试，但它们具有广泛的适用性，可应用于各类神经网络。相关代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;随着人工智能在科学应用中日益普及，将结果归因于网络推理过程对于支持稳健的科学泛化来说需求很高。在本工作中，我们旨在强调并展示事后解释性方法在科学应用中AI方法上的应用需求。为此，我们为现有的用于中微子标记的图神经网络NuGraph2引入了解释性附加组件。这些解释采用一系列技术形式，包括检查网络输出（节点分类）和它们之间的边连接关系，以及使用应用于该网络的新开发通用工具探测潜在空间。我们展示了这些方法中的任何单独一种都不足以展示网络的'理解'能力，但结合起来可以提供对分类过程中所使用方法的见解。虽然这些方法在NuGraph2应用上进行了测试，但它们可以应用于广泛的网络，不仅限于图神经网络。本工作的代码已在GitHub上公开，网址为https://github.com/voetberg/XNuGraph。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing popularity of artificial intelligence used for scientificapplications, the ability of attribute a result to a reasoning process from thenetwork is in high demand for robust scientific generalizations to hold. Inthis work we aim to motivate the need for and demonstrate the use of post-hocexplainability methods when applied to AI methods used in scientificapplications. To this end, we introduce explainability add-ons to the existinggraph neural network (GNN) for neutrino tagging, NuGraph2. The explanationstake the form of a suite of techniques examining the output of the network(node classifications) and the edge connections between them, and probing ofthe latent space using novel general-purpose tools applied to this network. Weshow how none of these methods are singularly sufficient to show network"understanding", but together can give insights into the processes used inclassification. While these methods are tested on the NuGraph2 application,they can be applied to a broad range of networks, not limited to GNNs. The codefor this work is publicly available on GitHub athttps://github.com/voetberg/XNuGraph.</description>
      <author>example@mail.com (Margaret Voetberg, Vitor F. Grizzi, Giuseppe Cerati, Hadi Meidani)</author>
      <guid isPermaLink="false">2509.10676v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations</title>
      <link>http://arxiv.org/abs/2509.10659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and published in Transactions on Machine Learning Research  (TMLR), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M4GN的三层段中心分层网络，用于解决基于网格的图神经网络在PDE模拟中面临的高成本、过度平滑以及构建粗图和保持细粒度精度的问题。&lt;h4&gt;背景&lt;/h4&gt;基于网格的图神经网络已成为PDE模拟的有效替代方法，但深度消息传递在大规模、长距离网格上成本高且存在过度平滑问题；分层GNN虽缩短了传播路径，但仍面临构建尊重网格拓扑、几何和物理不连续性的粗图，以及保持细粒度精度而不牺牲粗化带来的速度优势两大障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够平衡精度和效率的图神经网络架构，解决现有方法在处理大规模网格时的高计算成本和过度平滑问题，同时保持细粒度精度。&lt;h4&gt;方法&lt;/h4&gt;提出M4GN，采用混合分割策略结合快速图分割器和超像素式细化，产生动态一致节点的连续段；使用排列不变聚合器编码这些段，避免顺序敏感性和二次成本；通过微观级GNN捕获局部动力学，宏观级transformer跨段高效推理，实现精度与效率的平衡。&lt;h4&gt;主要发现&lt;/h4&gt;在多个代表性基准数据集上评估，M4GN将预测准确性提高了高达56%，同时比最先进的基线快达22%。&lt;h4&gt;结论&lt;/h4&gt;M4GN通过创新的分层架构和混合分割策略，有效解决了现有图神经网络在PDE模拟中的关键挑战，实现了精度和效率的显著提升。&lt;h4&gt;翻译&lt;/h4&gt;基于网格的图神经网络已成为PDE模拟的有效替代方法，但其深度消息传递在大规模、长距离网格上成本高且过度平滑；分层GNN缩短了传播路径但仍面临两个关键障碍：(i)构建尊重网格拓扑、几何和物理不连续性的粗图，以及(ii)保持细粒度精度而不牺牲粗化带来的速度优势。我们通过M4GN应对这些挑战，这是一种三层、段中心的分层网络。M4GN采用混合分割策略，将快速图分割器与由模态分解特征引导的超像素式细化配对，产生动态一致节点的连续段。这些段通过排列不变聚合器进行编码，避免了先前工作中使用的聚合方法的顺序敏感性和二次成本。得到的信息连接了捕获局部动力学的微观级GNN和跨段高效推理的宏观级transformer，实现了精度和效率之间的原则性平衡。在多个代表性基准数据集上评估，M4GN将预测准确性提高了高达56%，同时比最先进的基线快达22%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mesh-based graph neural networks (GNNs) have become effective surrogates forPDE simulations, yet their deep message passing incurs high cost andover-smoothing on large, long-range meshes; hierarchical GNNs shortenpropagation paths but still face two key obstacles: (i) building coarse graphsthat respect mesh topology, geometry, and physical discontinuities, and (ii)maintaining fine-scale accuracy without sacrificing the speed gained fromcoarsening. We tackle these challenges with M4GN, a three-tier, segment-centrichierarchical network. M4GN begins with a hybrid segmentation strategy thatpairs a fast graph partitioner with a superpixel-style refinement guided bymodal-decomposition features, producing contiguous segments of dynamicallyconsistent nodes. These segments are encoded by a permutation-invariantaggregator, avoiding the order sensitivity and quadratic cost of aggregationapproaches used in prior works. The resulting information bridges a micro-levelGNN, which captures local dynamics, and a macro-level transformer that reasonsefficiently across segments, achieving a principled balance between accuracyand efficiency. Evaluated on multiple representative benchmark datasets, M4GNimproves prediction accuracy by up to 56% while achieving up to 22% fasterinference than state-of-the-art baselines.</description>
      <author>example@mail.com (Bo Lei, Victor M. Castillo, Yeping Hu)</author>
      <guid isPermaLink="false">2509.10659v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Hetero-EUCLID: Interpretable model discovery for heterogeneous hyperelastic materials using stress-unsupervised learning</title>
      <link>http://arxiv.org/abs/2509.11784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Hetero-EUCLID的计算框架，用于异质材料的分割和参数识别，以表征所有组分的完整超弹性行为。&lt;h4&gt;背景&lt;/h4&gt;异质材料的完整力学行为表征是一个挑战性问题，需要能够识别不同材料组分及其力学特性的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够分割异质材料域并识别各组分材料本构参数的计算框架，以实现完整的超弹性行为表征。&lt;h4&gt;方法&lt;/h4&gt;基于Bayesian-EUCLID框架，使用稀疏促进先验和马尔可夫链蒙特卡罗采样解决异构化公式。框架包括两个主要步骤：基于残余力的分割和本构参数识别。使用有限元模拟生成的三维表面位移和边界平均力数据作为输入。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够成功分割各种类型的薄方形异质域，并表征不同组分的材料特性。即使在存在位移噪声和非原生网格离散化的情况下，框架也能保持有效。基于单次实验数据即可完成分割和材料表征。&lt;h4&gt;结论&lt;/h4&gt;Hetero-EUCLID框架在基于数字图像/体积相关的实验场景中具有适用性，为航空航天和国防复合材料等领域的快速、可解释的模型发现提供了工具，同时也适用于医疗领域如纤维动脉粥样硬化、动脉粥样硬化或癌症中的选择性组织硬化表征。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种计算框架Hetero-EUCLID，用于分割和参数识别，以表征异质材料所有组分的完整超弹性行为。在这项工作中，我们利用Bayesian-EUCLID（高效无监督本构定律识别和发现）框架，通过使用稀疏促进先验和马尔可夫链蒙特卡罗采样进行简约模型选择，有效地解决了异构化公式。我们使用了从异质试样非等双轴拉伸试验的有限元模拟中生成的实验可观测的三维表面位移和边界平均力数据。该框架 broadly包括两个步骤——基于残余力的分割和本构参数识别。我们验证并展示了所提出框架分割域和表征组分材料在各种类型薄方形异质域上的能力。我们验证了该框架在不同级别位移噪声和非原生网格离散化情况下的分割和表征材料的能力，即使用不同的网格进行前向FE模拟和逆EUCLID问题。这证明了Hetero-EUCLID框架在基于数字图像/体积相关的实验场景中的适用性。此外，所提出的框架基于单次实验数据成功进行了分割和材料表征，使其在航空航天和国防复合材料等领域的快速、可解释的模型发现，以及在纤维动脉粥样硬化、动脉粥样硬化或癌症等医疗状况中选择性组织硬化表征方面具有可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a computational framework, Hetero-EUCLID, for segmentation andparameter identification to characterize the full hyperelastic behavior of allconstituents of a heterogeneous material. In this work, we leverage theBayesian-EUCLID (Efficient Unsupervised Constitutive Law Identification andDiscovery) framework to efficiently solve the heterogenized formulation throughparsimonious model selection using sparsity-promoting priors and Monte CarloMarkov Chain sampling. We utilize experimentally observable 3D surfacedisplacement and boundary-averaged force data generated from Finite Elementsimulations of non-equi-biaxial tension tests on heterogeneous specimens. Theframework broadly consists of two steps -- residual force-based segmentation,and constitutive parameter identification. We validate and demonstrate theability of the proposed framework to segment the domain, and characterize theconstituent materials on various types of thin square heterogeneous domains. Wevalidate of the framework's ability to segment and characterize materials withvarious levels of displacement noises and non-native mesh discretizations, i.e,using different meshes for the forward FE simulations and the inverse EUCLIDproblem. This demonstrates Hetero-EUCLID framework's applicability in DigitalImage/Volume Correlation-based experimental scenarios. Furthermore, theproposed framework performs successful segmentation and materialcharacterizations based on data from a single experiment, thereby making itviable for rapid, interpretable model discovery in domains such as aerospaceand defense composites and for characterization of selective tissue stiffeningin medical conditions such as fibroatheroma, atherosclerosis, or cancer.</description>
      <author>example@mail.com (Kanhaiya Lal Chaurasiya, Saurav Dutta, Siddhant Kumar, Akshay Joshi)</author>
      <guid isPermaLink="false">2509.11784v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>The Quest for Universal Master Key Filters in DS-CNNs</title>
      <link>http://arxiv.org/abs/2509.11711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文扩展了卷积神经网络的'主键过滤器假设'，发现深度可分离卷积网络固有收敛到一组仅8个通用过滤器，这些过滤器在图像处理中具有基础性作用。&lt;h4&gt;背景&lt;/h4&gt;最近的研究提出了卷积神经网络过滤器的'主键过滤器假设'，本文进一步扩展了这一假设。&lt;h4&gt;目的&lt;/h4&gt;将假设范围严格限制到一组仅8个通用过滤器，探究深度可分离卷积网络(DS-CNN)的固有收敛特性。&lt;h4&gt;方法&lt;/h4&gt;通过系统性的无监督搜索，从不同架构和数据集中提取这些基本模式。&lt;h4&gt;主要发现&lt;/h4&gt;传统DS-CNN使用的数千个过滤器主要是这8个通用集合的线性移位；用这8个冻结过滤器初始化的网络在ImageNet上达到超过80%的准确率；这些过滤器与高斯差、高斯及其导数相似，与哺乳动物视觉系统感受野相似；深度卷积层自然倾向于这组基本空间算子。&lt;h4&gt;结论&lt;/h4&gt;深度卷积层自然地倾向于这组基本的空间算子，无论任务或架构如何。这些主键过滤器为理解泛化和迁移学习提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究提出了卷积神经网络过滤器的'主键过滤器假设'。本文通过将假设范围严格限制到一组仅8个通用过滤器来扩展这一假设，这些是深度可分离卷积网络固有收敛到的。虽然传统DS-CNN使用数千个不同的训练过滤器，但我们的分析显示这些过滤器主要是我们发现的通用集合的线性移位。通过系统性的无监督搜索，我们从不同架构和数据集中提取了这些基本模式。值得注意的是，用这8个独特的冻结过滤器初始化的网络在ImageNet上获得超过80%的准确率，甚至在应用于较小数据集时优于具有数千个可训练参数的模型。识别出的主键过滤器与高斯差、高斯及其导数非常匹配，这些结构不仅是经典图像处理的基础，而且与哺乳动物视觉系统中的感受野惊人地相似。我们的发现提供了有力证据，表明深度卷积层自然地倾向于这组基本的空间算子，无论任务或架构如何。这项工作通过这些主键过滤器的通用语言，为理解泛化和迁移学习提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A recent study has proposed the "Master Key Filters Hypothesis" forconvolutional neural network filters. This paper extends this hypothesis byradically constraining its scope to a single set of just 8 universal filtersthat depthwise separable convolutional networks inherently converge to. Whileconventional DS-CNNs employ thousands of distinct trained filters, our analysisreveals these filters are predominantly linear shifts (ax+b) of our discovereduniversal set. Through systematic unsupervised search, we extracted thesefundamental patterns across different architectures and datasets. Remarkably,networks initialized with these 8 unique frozen filters achieve over 80%ImageNet accuracy, and even outperform models with thousands of trainableparameters when applied to smaller datasets. The identified master key filtersclosely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,structures that are not only fundamental to classical image processing but alsostrikingly similar to receptive fields in mammalian visual systems. Ourfindings provide compelling evidence that depthwise convolutional layersnaturally gravitate toward this fundamental set of spatial operators regardlessof task or architecture. This work offers new insights for understandinggeneralization and transfer learning through the universal language of thesemaster key filters.</description>
      <author>example@mail.com (Zahra Babaiee, Peyman M. Kiassari, Daniela Rus, Radu Grosu)</author>
      <guid isPermaLink="false">2509.11711v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>An Unsupervised Learning Approach For A Reliable Profiling Of Cyber Threat Actors Reported Globally Based On Complete Contextual Information Of Cyber Attacks</title>
      <link>http://arxiv.org/abs/2509.11683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于无监督的高效凝聚层次聚类技术，用于对网络犯罪团伙进行全面画像，解决了现有监督机器学习方法在攻击者画像方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;随着技术进步，网络攻击迅速增加，而信息保护不足。现有监督机器学习方法只考虑了文本威胁报告中的少量特征，且依赖结构化数据集，需要先建立数据集再进行分析，效率低下。&lt;h4&gt;目的&lt;/h4&gt;识别网络威胁行为者之间的共同特征关系，对其进行聚合，并对网络犯罪团伙进行画像，以创建更有效的防御机制。&lt;h4&gt;方法&lt;/h4&gt;采用无监督的高效凝聚层次聚类技术，基于全面的上下文威胁信息对网络犯罪团伙进行画像，避免了对结构化数据集的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;通过基于网络威胁行为者的共同特征识别关系并聚合，可以更有效地创建防御机制，提高威胁分析的效率。&lt;h4&gt;结论&lt;/h4&gt;基于全面上下文信息的无监督方法可以更有效地识别网络威胁行为者之间的关系，为提前创建防御机制提供支持。&lt;h4&gt;翻译&lt;/h4&gt;网络攻击随着技术进步而迅速增加，我们的信息没有保护。为防止未来的网络攻击，及时识别网络攻击并建立强大的防御机制至关重要。为立即应对网络安全威胁，必须检查攻击者的技能、知识和行为，目的是评估他们对系统的影响并理解与这些攻击相关的特征。基于网络威胁行为者的特征或行为模式创建画像，可以帮助提前创建有效的防御机制。在现有文献中，多种基于监督机器学习的方法只考虑了文本网络威胁事件报告中报告的少量特征用于攻击者画像，尽管这些画像是基于安全专家自身的认知开发的，但我们不能依赖它们。监督机器学习方法严格依赖于结构化数据集。这通常导致一个两步过程，即我们首先必须建立结构化数据集，然后才能分析它并用于构建防御机制，这需要时间。在本文中，提出了一种无监督的高效凝聚层次聚类技术，用于基于全面的上下文威胁信息对网络犯罪团伙进行画像，以解决上述问题。本报告的主要目标是识别基于共同特征的网络威胁行为者之间的关系，对其进行聚合，并对网络犯罪团伙进行画像。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cyber attacks are rapidly increasing with the advancement of technology andthere is no protection for our information. To prevent future cyberattacks itis critical to promptly recognize cyberattacks and establish strong defensemechanisms against them. To respond to cybersecurity threats immediately, it isessential to examine the attackers skills, knowledge, and behaviors with thegoal of evaluating their impact on the system and comprehending the traitsassociated with these attacks. Creating a profile of cyber threat actors basedon their traits or patterns of behavior can help to create effective defensesagainst cyberattacks in advance. In the current literature, multiple supervisedmachine learning based approaches considered a smaller number of features forattacker profiling that are reported in textual cyber threat incident documentsalthough these profiles have been developed based on the security experts ownperception, we cannot rely on them. Supervised machine learning approachesstrictly depend upon the structure data set. This usually leads to a two stepprocess where we first have to establish a structured data set before we cananalyze it and then employ it to construct defense mechanisms, which takestime. In this paper, an unsupervised efficient agglomerative hierarchalclustering technique is proposed for profiling cybercriminal groups based ontheir comprehensive contextual threat information in order to address theaforementioned issues. The main objective of this report is to identify therelationship between cyber threat actors based on their common features,aggregate them, and also profile cyber criminal groups.</description>
      <author>example@mail.com (Sawera Shahid, Umara Noor, Zahid Rashid)</author>
      <guid isPermaLink="false">2509.11683v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Stacked Intelligent Metasurface for End-to-End OFDM System</title>
      <link>http://arxiv.org/abs/2509.11551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于SIM（DPSIM）辅助的端到端OFDM系统，将传统通信任务在电磁前向传播中同时完成，并引入电磁神经网络（EMNN）进行系统控制，通过迁移学习进行模型训练，实现了在复杂信道条件下的稳健比特流传输。&lt;h4&gt;背景&lt;/h4&gt;堆叠智能超表面（SIM）和双极化SIM（DPSIM）的波域信号处理是卸载基带数字处理和简化收发器设计的有前途的研究方向，但现有架构仅限于将SIM（DPSIM）用于单一通信功能。&lt;h4&gt;目的&lt;/h4&gt;提高SIM（DPSIM）辅助系统的整体性能，实现从发送比特流到接收比特流的端到端（E2E）联合优化。&lt;h4&gt;方法&lt;/h4&gt;提出SIM（DPSIM）辅助的端到端OFDM系统，在电磁前向传播过程中同时完成调制、预编码、合并和解调任务；受神经网络启发提出电磁神经网络（EMNN）控制系统；引入迁移学习进行模型训练；设计EMNN的训练和部署框架。&lt;h4&gt;主要发现&lt;/h4&gt;SIM辅助和DPSIM辅助的端到端OFDM系统都能在复杂信道条件下实现稳健的比特流传输。&lt;h4&gt;结论&lt;/h4&gt;EMNN和SIM（DPSIM）辅助的端到端OFDM系统在下一代收发器设计中具有应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;堆叠智能超表面（SIM）和双极化SIM（DPSIM）的波域信号处理已成为卸载基带数字处理任务和高效简化收发器设计的研究方向。然而，现有架构仅限于将SIM（DPSIM）用于单一通信功能，如预编码或合并。为了进一步提高SIM（DPSIM）辅助系统的整体性能，并实现从发送比特流到接收比特流的端到端（E2E）联合优化，我们提出了一种SIM（DPSIM）辅助的端到正交频分复用（OFDM）系统，其中传统的通信任务如调制、预编码、合并和解调在电磁（EM）前向传播过程中同时完成。此外，受将真实超表面抽象为神经网络隐藏层的启发，我们提出了电磁神经网络（EMNN）来控制端到端OFDM通信系统。另外，将迁移学习引入模型训练，并设计了EMNN的训练和部署框架。仿真结果表明，SIM辅助的端到端OFDM系统和DPSIM辅助的端到端OFDM系统都能在复杂信道条件下实现稳健的比特流传输。我们的研究突显了EMNN和SIM（DPSIM）辅助的端到端OFDM系统在下一代收发器设计中的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stacked intelligent metasurface (SIM) and dual-polarized SIM (DPSIM) enabledwave-domain signal processing have emerged as promising research directions foroffloading baseband digital processing tasks and efficiently simplifyingtransceiver design. However, existing architectures are limited to employingSIM (DPSIM) for a single communication function, such as precoding orcombining. To further enhance the overall performance of SIM (DPSIM)-assistedsystems and achieve end-to-end (E2E) joint optimization from the transmittedbitstream to the received bitstream, we propose an SIM (DPSIM)- assisted E2Eorthogonal frequency-division multiplexing (OFDM) system, where traditionalcommunication tasks such as modulation, precoding, combining, and demodulationare performed simultaneously during electromagnetic (EM) forward propagation.Furthermore, inspired by the idea of abstracting real metasurfaces as hiddenlayers of a neural network, we propose the electromagnetic neural network(EMNN) to enable the control of the E2E OFDM communication system. In addition,transfer learning is introduced into the model training, and a training anddeployment framework for the EMNN is designed. Simulation results demonstratethat both SIM-assisted E2E OFDM systems and DPSIM-assisted E2E OFDM systems canachieve robust bitstream transmission under complex channel conditions. Ourstudy highlights the application potential of EMNN and SIM (DPSIM)-assisted E2EOFDM systems in the design of next-generation transceivers.</description>
      <author>example@mail.com (Yida Zhang, Qiuyan Liu, Hongtao Luo, Yuqi Xia, Qiang Wang)</author>
      <guid isPermaLink="false">2509.11551v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>MAUI: Reconstructing Private Client Data in Federated Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.11451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MAUI，一种隐蔽的数据重建攻击方法，在联邦学习中仅利用分类头梯度即可重建原始输入数据，显著提高了重建质量。&lt;h4&gt;背景&lt;/h4&gt;联邦学习中，服务器首先在公共数据集上预训练全局模型，然后在客户端微调模型的最后几个线性层（分类头）。现有数据重建攻击存在两个弱点：初始模型层的梯度信息不共享，以及服务器对模型的手工操作易被检测。&lt;h4&gt;目的&lt;/h4&gt;开发一种隐蔽的数据重建攻击方法，不需要对模型架构或权重进行明显操作，仅依靠分类头梯度实现数据重建。&lt;h4&gt;方法&lt;/h4&gt;MAUI从分类头的梯度中提取输入批次的'鲁棒'特征表示，然后将这些特征表示反转回原始输入数据。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR10和ImageNet数据集上实现了高度准确的重建，适用于多种模型架构（CNN、VGG11、ResNets、ShuffleNet-V2和ViT B-32），无论批量大小如何都能工作，重建质量比先前方法高40-120%的PSNR分数。&lt;h4&gt;结论&lt;/h4&gt;MAUI是一种隐蔽高效的数据重建攻击方法，仅利用分类头梯度即可实现高质量的数据重建，显著优于现有攻击方法。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)的最新研究表明，利用迁移学习可以平衡联邦学习和集中式学习的好处。在这种设置中，联邦训练在通过传统训练达到稳定点后进行。全局模型权重首先由服务器在公共数据集上进行集中预训练，之后只有模型的最后几个线性层（分类头）在客户端间进行微调。在这种情况下，现有的联邦学习数据重建攻击(DRAs)表现出两个关键弱点：首先，与输入强相关的初始模型层梯度信息从不共享，显著降低了重建准确性；其次，服务器对模型结构或参数进行高度特定、手工操作的攻击方法（如全零权重层、恒等映射和具有相同权重模式的行）容易被活跃客户端检测。针对这些弱点，我们提出了MAUI，一种隐蔽的数据重建攻击，不需要对模型架构或权重进行任何明显操作，仅依赖于分类头的梯度。MAUI首先从分类头的梯度中提取输入批次的'鲁棒'特征表示，然后将这些表示反转回原始输入。我们在CIFAR10和ImageNet数据集上报告了高度准确的重建，适用于多种模型架构，包括卷积网络(CNN, VGG11)、ResNets(18, 50)、ShuffleNet-V2和Vision Transformer(ViT B-32)，无论批量大小如何。MAUI在重建质量上显著优于先前的DRAs，实现了40-120%更高的PSNR分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent works in federated learning (FL) have shown the utility of leveragingtransfer learning for balancing the benefits of FL and centralized learning. Inthis setting, federated training happens after a stable point has been reachedthrough conventional training. Global model weights are first centrallypretrained by the server on a public dataset following which only the last fewlinear layers (the classification head) of the model are finetuned acrossclients. In this scenario, existing data reconstruction attacks (DRAs) in FLshow two key weaknesses. First, strongly input-correlated gradient informationfrom the initial model layers is never shared, significantly degradingreconstruction accuracy. Second, DRAs in which the server makes highlyspecific, handcrafted manipulations to the model structure or parameters (fore.g., layers with all zero weights, identity mappings and rows with identicalweight patterns) are easily detectable by an active client.  Improving on these, we propose MAUI, a stealthy DRA that does not require anyovert manipulations to the model architecture or weights, and relies solely onthe gradients of the classification head. MAUI first extracts "robust" featurerepresentations of the input batch from the gradients of the classificationhead and subsequently inverts these representations to the original inputs. Wereport highly accurate reconstructions on the CIFAR10 and ImageNet datasets ona variety of model architectures including convolution networks (CNN, VGG11),ResNets (18, 50), ShuffleNet-V2 and Vision Transformer (ViT B-32), regardlessof the batch size. MAUI significantly outperforms prior DRAs in reconstructionquality, achieving 40-120% higher PSNR scores.</description>
      <author>example@mail.com (Ahaan Dabholkar, Atul Sharma, Z. Berkay Celik, Saurabh Bagchi)</author>
      <guid isPermaLink="false">2509.11451v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Dataset Evaluation Based on Generalized Cross Validation</title>
      <link>http://arxiv.org/abs/2509.11273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IST 2025. Official IEEE Xplore entry will  be available once published&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合广义交叉验证和领域迁移学习原则的合成数据集质量评估框架，通过构建交叉性能矩阵和GCV矩阵来量化领域可迁移性，并引入两个关键指标分别评估模拟质量和迁移质量，实验证明该框架能有效评估合成数据保真度。&lt;h4&gt;背景&lt;/h4&gt;随着合成数据集生成技术的快速发展，评估合成数据质量已成为关键研究焦点。当前评估研究有限，缺乏普遍接受的标准框架。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的评估框架，对合成数据集质量进行可推广和可比较的评估，指导合成数据集优化。&lt;h4&gt;方法&lt;/h4&gt;在合成数据集和真实世界基准数据集上训练特定任务模型，形成交叉性能矩阵；构建广义交叉验证矩阵量化领域可迁移性；引入两个关键指标：一个衡量合成数据与真实数据的相似性，一个评估合成数据在不同真实世界场景中的多样性和覆盖范围。&lt;h4&gt;主要发现&lt;/h4&gt;在Virtual KITTI上的实验验证了所提框架和指标在评估合成数据保真度方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;该可扩展和可量化的评估解决方案克服了传统局限性，为人工智能研究中合成数据集的优化提供了有原则的方法。&lt;h4&gt;翻译&lt;/h4&gt;随着合成数据集生成技术的快速发展，评估合成数据的质量已成为关键研究焦点。稳健的评估不仅推动数据生成方法的创新，还能指导研究人员优化这些合成资源的利用。然而，当前对合成数据集的评估研究仍然有限，缺乏普遍接受的标准框架。为解决这一问题，本文提出了一种结合广义交叉验证实验和领域迁移学习原则的新评估框架，能够对合成数据集质量进行可推广和可比较的评估。该框架涉及在合成数据集和多个真实世界基准（如KITTI、BDD100K）上训练特定任务模型（如YOLOv5s），形成一个交叉性能矩阵。经过归一化后，构建广义交叉验证（GCV）矩阵来量化领域可迁移性。该框架引入两个关键指标：一个通过量化合成数据与真实数据集之间的相似性来衡量模拟质量，另一个通过评估合成数据在各种真实世界场景中的多样性和覆盖范围来评估迁移质量。在Virtual KITTI上的实验验证了所提框架和指标在评估合成数据保真度方面的有效性。这种可扩展和可量化的评估解决方案克服了传统局限性，为人工智能研究中合成数据集的优化提供了有原则的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of synthetic dataset generation techniques,evaluating the quality of synthetic data has become a critical research focus.Robust evaluation not only drives innovations in data generation methods butalso guides researchers in optimizing the utilization of these syntheticresources. However, current evaluation studies for synthetic datasets remainlimited, lacking a universally accepted standard framework. To address this,this paper proposes a novel evaluation framework integrating generalizedcross-validation experiments and domain transfer learning principles, enablinggeneralizable and comparable assessments of synthetic dataset quality. Theframework involves training task-specific models (e.g., YOLOv5s) on bothsynthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K),forming a cross-performance matrix. Following normalization, a GeneralizedCross-Validation (GCV) Matrix is constructed to quantify domaintransferability. The framework introduces two key metrics. One measures thesimulation quality by quantifying the similarity between synthetic data andreal-world datasets, while another evaluates the transfer quality by assessingthe diversity and coverage of synthetic data across various real-worldscenarios. Experimental validation on Virtual KITTI demonstrates theeffectiveness of our proposed framework and metrics in assessing synthetic datafidelity. This scalable and quantifiable evaluation solution overcomestraditional limitations, providing a principled approach to guide syntheticdataset optimization in artificial intelligence research.</description>
      <author>example@mail.com (Zhihang Song, Dingyi Yao, Ruibo Ming, Lihui Peng, Danya Yao, Yi Zhang)</author>
      <guid isPermaLink="false">2509.11273v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches</title>
      <link>http://arxiv.org/abs/2509.11241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了两种深度学习模型在卡纳提克音乐节拍跟踪任务上的性能，并探索了通过迁移学习和音乐信息参数调整来提高模型适应性的方法。&lt;h4&gt;背景&lt;/h4&gt;节拍和强拍跟踪是音乐信息检索的基础任务。深度学习模型在西方音乐流派中表现优异，但在代表性不足的音乐传统上表现不佳。卡纳提克音乐以其节奏复杂性和独特节拍结构著称，先前研究主要采用概率动态贝叶斯网络(DBN)方法。&lt;h4&gt;目的&lt;/h4&gt;评估两种深度学习模型在卡纳提克音乐节拍跟踪任务上的性能，研究适应策略，包括数据微调和音乐信息参数调整，探索最先进深度学习模型在非西方音乐传统中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;在卡纳提克音乐节奏(CMR_f)数据集上评估时间卷积网络(TCN)和基于Transformer的Beat This!模型，复制DBN基线实验设置，并通过迁移学习和音乐信息参数调整来优化模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;现成的深度学习模型并不总是优于DBN基线，但通过迁移学习，模型性能显著提高，能够匹配或超过基线性能，表明深度学习模型可以有效地适应卡纳提克音乐等非西方音乐传统。&lt;h4&gt;结论&lt;/h4&gt;最先进的深度学习模型可以有效地适应代表性不足的音乐传统，为开发更具包容性和广泛适用性的节拍跟踪系统开辟了道路。&lt;h4&gt;翻译&lt;/h4&gt;节拍和强拍跟踪，统称为节拍跟踪，是音乐信息检索(MIR)中的基础任务。深度学习模型在这一领域已经远远超越了传统的信号处理和经典机器学习方法，特别是在西方(欧洲起源)音乐流派中，因为有大量标注数据集可用。然而，这些系统在代表性不足的音乐传统上表现不太可靠。卡纳提克音乐是来自印度次大陆的丰富传统，以其节奏复杂性和独特的节拍结构(talas)而闻名。在此背景下，关于节拍跟踪的最著名先前工作采用了概率动态贝叶斯网络(DBNs)。然而，最先进的深度学习模型在卡纳提克音乐上的表现仍然 largely未被探索。在本研究中，我们评估了两种用于卡纳提克音乐节拍跟踪的模型：时间卷积网络(TCN)，这是一种已成功适应拉丁节奏的轻量级架构，以及Beat This!，一种不需要后处理的基于Transformer的模型，旨在广泛风格覆盖。我们在卡纳提克音乐节奏(CMR_f)数据集上复制DBN基线的实验设置，在直接可比的环境中系统地评估这些模型的性能。我们进一步研究了适应策略，包括在卡纳提克数据上微调模型和使用音乐信息参数。结果表明，虽然现成的模型并不总是优于DBN，但通过迁移学习，它们的性能显著提高，匹配或超过了基线。这些发现表明，最先进的深度学习模型可以有效地适应代表性不足的传统，为更具包容性和更广泛适用的节拍跟踪系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Beat and downbeat tracking, jointly referred to as Meter Tracking, is afundamental task in Music Information Retrieval (MIR). Deep learning modelshave far surpassed traditional signal processing and classical machine learningapproaches in this domain, particularly for Western (Eurogenetic) genres, wherelarge annotated datasets are widely available. These systems, however, performless reliably on underrepresented musical traditions. Carnatic music, a richtradition from the Indian subcontinent, is renowned for its rhythmic intricacyand unique metrical structures (t\=alas). The most notable prior work on metertracking in this context employed probabilistic Dynamic Bayesian Networks(DBNs). The performance of state-of-the-art (SOTA) deep learning models onCarnatic music, however, remains largely unexplored.  In this study, we evaluate two models for meter tracking in Carnatic music:the Temporal Convolutional Network (TCN), a lightweight architecture that hasbeen successfully adapted for Latin rhythms, and Beat This!, atransformer-based model designed for broad stylistic coverage without the needfor post-processing. Replicating the experimental setup of the DBN baseline onthe Carnatic Music Rhythm (CMR$_f$) dataset, we systematically assess theperformance of these models in a directly comparable setting. We furtherinvestigate adaptation strategies, including fine-tuning the models on Carnaticdata and the use of musically informed parameters. Results show that whileoff-the-shelf models do not always outperform the DBN, their performanceimproves substantially with transfer learning, matching or surpassing thebaseline. These findings indicate that SOTA deep learning models can beeffectively adapted to underrepresented traditions, paving the way for moreinclusive and broadly applicable meter tracking systems.</description>
      <author>example@mail.com (Satyajeet Prabhu)</author>
      <guid isPermaLink="false">2509.11241v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Feature Space Topology Control via Hopkins Loss</title>
      <link>http://arxiv.org/abs/2509.11154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Proc. IEEE ICTAI 2025, Athens, Greece&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为'Hopkins loss'的新型损失函数，它利用Hopkins统计量强制实现所需的特征空间拓扑结构，与现有方法不同，现有方法旨在保留输入特征拓扑。研究者在语音、文本和图像数据上评估了该方法在分类和降维场景下的有效性，结果表明该方法对分类性能影响较小，同时能修改特征拓扑。&lt;h4&gt;背景&lt;/h4&gt;特征空间拓扑指的是特征空间中样本的组织方式。修改这种拓扑在机器学习应用中是有益的，包括降维、生成建模、迁移学习和对抗攻击的鲁棒性。现有的拓扑相关方法主要关注保留输入特征拓扑。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的损失函数(Hopkins loss)，能够强制实现所需的特征空间拓扑，从而在保持分类性能的同时修改特征拓扑结构。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为'Hopkins loss'的新型损失函数，它利用Hopkins统计量来强制实现所需的特征空间拓扑。研究者在语音、文本和图像数据上评估了该方法，在两个场景中进行测试：分类和非线性瓶颈自编码器的降维。&lt;h4&gt;主要发现&lt;/h4&gt;将Hopkins loss集成到分类或降维任务中，对分类性能只有很小的影响，同时能够提供修改特征拓扑的好处。&lt;h4&gt;结论&lt;/h4&gt;Hopkins loss是一种有效的工具，可以在保持模型性能的同时，修改特征空间的拓扑结构，这有助于提高机器学习应用中的多种任务表现。&lt;h4&gt;翻译&lt;/h4&gt;特征空间拓扑指的是特征空间中样本的组织方式。修改这种拓扑在机器学习应用中是有益的，包括降维、生成建模、迁移学习和对抗攻击的鲁棒性。本文介绍了一种新型的损失函数Hopkins loss，它利用Hopkins统计量来强制实现所需的特征空间拓扑，这与现有的拓扑相关方法形成对比，后者旨在保留输入特征拓扑。我们在语音、文本和图像数据上评估了Hopkins loss在两种场景下的有效性：使用非线性瓶颈自编码器的分类和降维。我们的实验表明，将Hopkins loss集成到分类或降维中，对分类性能只有很小的影响，同时能够提供修改特征拓扑的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature space topology refers to the organization of samples within thefeature space. Modifying this topology can be beneficial in machine learningapplications, including dimensionality reduction, generative modeling, transferlearning, and robustness to adversarial attacks. This paper introduces a novelloss function, Hopkins loss, which leverages the Hopkins statistic to enforce adesired feature space topology, which is in contrast to existingtopology-related methods that aim to preserve input feature topology. Weevaluate the effectiveness of Hopkins loss on speech, text, and image data intwo scenarios: classification and dimensionality reduction using nonlinearbottleneck autoencoders. Our experiments show that integrating Hopkins lossinto classification or dimensionality reduction has only a small impact onclassification performance while providing the benefit of modifying featuretopology.</description>
      <author>example@mail.com (Einari Vaaras, Manu Airaksinen)</author>
      <guid isPermaLink="false">2509.11154v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring</title>
      <link>http://arxiv.org/abs/2509.10995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, 3 algorithms, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于强化学习的迁移学习框架，使用上置信界算法自动选择最适合动物检测任务的预训练模型，实现了更高的检测率并减少了计算时间。&lt;h4&gt;背景&lt;/h4&gt;动物健康监测和种群管理是野生动物保护和畜牧业管理的关键方面，这些领域越来越依赖自动化检测和跟踪系统。基于无人机和计算机视觉的系统提供了有前景的解决方案，但标记训练数据的有限性是开发有效深度学习模型的主要障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于强化学习的迁移学习框架，使用上置信界算法自动选择最适合动物检测任务的预训练模型，简化模型选择过程。&lt;h4&gt;方法&lt;/h4&gt;采用强化学习方法进行迁移学习，使用上置信界算法系统评估和排序候选模型，基于性能选择最优模型。&lt;h4&gt;主要发现&lt;/h4&gt;该框架实现了比传统方法更高的检测率，同时需要显著更少的计算时间。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了预训练神经网络架构众多导致的模型选择难题，特别对领域新手研究者有帮助，能够简化模型选择过程。&lt;h4&gt;翻译&lt;/h4&gt;动物健康监测和种群管理是野生动物保护和畜牧业管理的关键方面，这些方面越来越依赖于自动化检测和跟踪系统。虽然基于无人机(UAV)的系统结合计算机视觉为具有挑战性地形上的非侵入式动物监测提供了有前景的解决方案，但标记训练数据的有限性仍然是开发这些应用的有效深度学习(DL)模型的障碍。迁移学习已成为一种潜在的解决方案，允许在大型数据集上训练的模型适应资源有限的情况，例如数据有限的情况。然而，预训练神经网络架构的广阔格局使得选择最佳模型具有挑战性，特别是对领域新手研究人员。在本文中，我们提出了一种基于强化学习(RL)的迁移学习框架，该框架采用上置信界(UCB)算法自动选择最适合动物检测任务的预训练模型。我们的方法基于性能系统评估和排名候选模型，简化了模型选择过程。实验结果表明，与传统方法相比，我们的框架实现了更高的检测率，同时需要显著更少的计算时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Animal health monitoring and population management are critical aspects ofwildlife conservation and livestock management that increasingly rely onautomated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)based systems combined with computer vision offer promising solutions fornon-invasive animal monitoring across challenging terrains, limitedavailability of labeled training data remains an obstacle in developingeffective deep learning (DL) models for these applications. Transfer learninghas emerged as a potential solution, allowing models trained on large datasetsto be adapted for resource-limited scenarios such as those with limited data.However, the vast landscape of pre-trained neural network architectures makesit challenging to select optimal models, particularly for researchers new tothe field. In this paper, we propose a reinforcement learning (RL)-basedtransfer learning framework that employs an upper confidence bound (UCB)algorithm to automatically select the most suitable pre-trained model foranimal detection tasks. Our approach systematically evaluates and rankscandidate models based on their performance, streamlining the model selectionprocess. Experimental results demonstrate that our framework achieves a higherdetection rate while requiring significantly less computational time comparedto traditional methods.</description>
      <author>example@mail.com (Nisha Pillai, Aditi Virupakshaiah, Harrison W. Smith, Amanda J. Ashworth, Prasanna Gowda, Phillip R. Owens, Adam R. Rivers, Bindu Nanduri, Mahalingam Ramkumar)</author>
      <guid isPermaLink="false">2509.10995v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A novel IR-SRGAN assisted super-resolution evaluation of photothermal coherence tomography for impact damage in toughened thermoplastic CFRP laminates under room temperature and low temperature</title>
      <link>http://arxiv.org/abs/2509.10894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种新型红外超分辨率生成对抗网络（IR-SRGAN），用于提高复合材料在低温条件下冲击损伤检测的准确性，解决了红外热成像技术存在的空间分辨率限制问题。&lt;h4&gt;背景&lt;/h4&gt;评估复合材料在温度变化条件下的冲击损伤对航空航天、极地等极端环境应用中的结构完整性和可靠性能至关重要。&lt;h4&gt;目的&lt;/h4&gt;精确检测和量化复合材料亚表面损伤特征，如分层面积、裂纹形态和界面分离，以进行后续机械表征和寿命预测。&lt;h4&gt;方法&lt;/h4&gt;结合红外热成像（IRT）与新开发的频分复用光热相关层析成像（FM-PCT）技术，并应用基于迁移学习的红外超分辨率生成对抗网络（IR-SRGAN）来增强成像质量。&lt;h4&gt;主要发现&lt;/h4&gt;低温条件下，基体脆性增加，导致损伤机制发生变化，常温下轻微的冲击可能引发严重的基体开裂、纤维/基体脱粘或界面失效；IRT技术存在帧率受限和横向热扩散等固有局限性，影响损伤尺寸测量的准确性。&lt;h4&gt;结论&lt;/h4&gt;通过开发的IR-SRGAN技术，可以在有限的热成像数据基础上，提高横向和深度分辨成像的保真度，从而更准确地评估复合材料在极端环境下的损伤情况。&lt;h4&gt;翻译&lt;/h4&gt;评估复合材料在变化温度条件下的冲击损伤对于确保航空航天、极地和其他极端环境应用中的结构完整性和可靠性能至关重要。随着低温下基体脆性的增加，损伤机制发生变化：在常温条件下只产生轻微分层的冲击事件在严重冷载荷下可能引发广泛的基体开裂、纤维/基体脱粘或界面失效，从而降低剩余强度和疲劳寿命。精确检测和量化亚表面损伤特征（如分层面积、裂纹形态、界面分离）对后续机械表征和寿命预测至关重要。在本研究中，采用红外热成像（IRT）与新开发的频分复用光热相关层析成像（FM-PCT）相结合，捕捉三维亚表面损伤特征，其深度分辨率接近X射线显微CT的分辨率。然而，IRT的固有局限性，包括帧率受限和横向热扩散，降低了空间分辨率，从而影响损伤尺寸测量的准确性。为解决此问题，我们开发了一种基于迁移学习的红外超分辨率生成对抗网络（IR-SRGAN），基于有限的热成像数据集增强横向和深度分辨成像保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating impact-induced damage in composite materials under varyingtemperature conditions is essential for ensuring structural integrity andreliable performance in aerospace, polar, and other extreme-environmentapplications. As matrix brittleness increases at low temperatures, damagemechanisms shift: impact events that produce only minor delaminations atambient conditions can trigger extensive matrix cracking, fiber/matrixdebonding, or interfacial failure under severe cold loads, thereby degradingresidual strength and fatigue life. Precision detection and quantification ofsubsurface damage features (e.g., delamination area, crack morphology,interface separation) are critical for subsequent mechanical characterizationand life prediction. In this study, infrared thermography (IRT) coupled with anewly developed frequency multiplexed photothermal correlation tomography(FM-PCT) is employed to capture three-dimensional subsurface damage signatureswith depth resolution approaching that of X-ray micro-computed tomography.However, the inherent limitations of IRT, including restricted frame rate andlateral thermal diffusion, reduce spatial resolution and thus the accuracy ofdamage size measurement. To address this, we develop a new transferlearning-based infrared super-resolution generative adversarial network(IR-SRGAN) that enhances both lateral and depth-resolved imaging fidelity basedon limited thermographic datasets.</description>
      <author>example@mail.com (Pengfei Zhu, Hai Zhang, Stefano Sfarra, Fabrizio Sarasini, Zijing Ding, Clemente Ibarra-Castanedo, Xavier Maldague)</author>
      <guid isPermaLink="false">2509.10894v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Neurosymbolic AI Transfer Learning Improves Network Intrusion Detection</title>
      <link>http://arxiv.org/abs/2509.10850v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种创新的神经符号AI框架，用于网络入侵检测系统，展示了迁移学习在网络安全领域的潜力。&lt;h4&gt;背景&lt;/h4&gt;迁移学习在计算机视觉、自然语言处理和医学影像等领域被广泛应用，但在网络安全领域的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;设计一个创新的神经符号AI框架应用于网络入侵检测系统，以应对网络安全中的恶意活动。&lt;h4&gt;方法&lt;/h4&gt;利用迁移学习和不确定性量化的神经符号AI框架进行网络入侵检测。&lt;h4&gt;主要发现&lt;/h4&gt;在大型和结构良好的数据集上训练的迁移学习模型比依赖较小数据集的基于神经的模型表现更好。&lt;h4&gt;结论&lt;/h4&gt;迁移学习模型在网络安全领域的应用为网络安全解决方案的新时代铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习因其解决子任务和处理不同数据集的出色能力，常被应用于计算机视觉、自然语言处理和医学影像等多个领域。然而，其在网络安全领域的应用尚未得到充分探索。在本文中，我们提出了一种创新的神经符号AI框架，专门用于网络入侵检测系统，该系统在应对网络安全中的恶意活动方面起着关键作用。我们的框架利用了迁移学习和不确定性量化。研究结果表明，在大型和结构良好的数据集上训练的迁移学习模型，比依赖较小数据集的基于神经的模型表现更优，为网络安全解决方案的新时代铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning is commonly utilized in various fields such as computervision, natural language processing, and medical imaging due to its impressivecapability to address subtasks and work with different datasets. However, itsapplication in cybersecurity has not been thoroughly explored. In this paper,we present an innovative neurosymbolic AI framework designed for networkintrusion detection systems, which play a crucial role in combating maliciousactivities in cybersecurity. Our framework leverages transfer learning anduncertainty quantification. The findings indicate that transfer learningmodels, trained on large and well-structured datasets, outperform neural-basedmodels that rely on smaller datasets, paving the way for a new era incybersecurity solutions.</description>
      <author>example@mail.com (Huynh T. T. Tran, Jacob Sander, Achraf Cohen, Brian Jalaian, Nathaniel D. Bastian)</author>
      <guid isPermaLink="false">2509.10850v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering</title>
      <link>http://arxiv.org/abs/2509.07766v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures, International Quantum Engineering conference and  exhibition (QUEST-IS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图的联盟结构生成算法(GCS-Q)，用于金融资产的相关性聚类，该方法能够直接处理带符号的加权图，无需传统转换方法，并通过量子退火技术高效探索解空间。实验证明该方法在聚类质量和动态确定聚类数量方面优于现有经典算法。&lt;h4&gt;背景&lt;/h4&gt;基于收益率相关性的金融资产聚类是投资组合优化和统计套利的基础任务，但传统聚类方法在处理带符号的相关性结构时表现不佳，通常需要损失性转换和固定聚类数量等启发式假设。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接处理带符号、加权图的聚类方法，避免传统方法的局限性，并利用量子计算技术提高聚类效率和质量。&lt;h4&gt;方法&lt;/h4&gt;应用基于图的联盟结构生成算法(GCS-Q)，将每个分区步骤表述为QUBO问题，利用量子退火技术高效探索解空间。在合成和真实金融数据上验证该方法，并与SPONGE和k-Medoids等经典算法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;GCS-Q在调整兰德指数和结构平衡惩罚等指标上始终实现更高的聚类质量，同时能够动态确定聚类数量，无需预先指定聚类数。&lt;h4&gt;结论&lt;/h4&gt;近期量子计算在金融应用的基于图的无监督学习中具有实用价值，为处理复杂金融数据提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;基于收益率相关性对金融资产进行聚类是投资组合优化和统计套利中的基础任务。然而，传统的聚类方法在处理带符号的相关性结构时往往表现不足，通常需要损失性转换和启发式假设，如固定数量的聚类。在本工作中，我们应用基于图的联盟结构生成算法(GCS-Q)直接聚类带符号的加权图，而不依赖这些转换。GCS-Q将每个分区步骤表述为QUBO问题，使其能够利用量子退火高效探索指数级大的解空间。我们在合成和真实金融数据上验证了我们的方法，并与SPONGE和k-Medoids等最先进的经典算法进行基准测试。我们的实验表明，GCS-Q在调整兰德指数和结构平衡惩罚等指标上始终实现更高的聚类质量，同时动态确定聚类数量。这些结果突显了近期量子计算在金融应用的基于图的无监督学习中的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering financial assets based on return correlations is a fundamentaltask in portfolio optimization and statistical arbitrage. However, classicalclustering methods often fall short when dealing with signed correlationstructures, typically requiring lossy transformations and heuristic assumptionssuch as a fixed number of clusters. In this work, we apply the Graph-basedCoalition Structure Generation algorithm (GCS-Q) to directly cluster signed,weighted graphs without relying on such transformations. GCS-Q formulates eachpartitioning step as a QUBO problem, enabling it to leverage quantum annealingfor efficient exploration of exponentially large solution spaces. We validateour approach on both synthetic and real-world financial data, benchmarkingagainst state-of-the-art classical algorithms such as SPONGE and k-Medoids. Ourexperiments demonstrate that GCS-Q consistently achieves higher clusteringquality, as measured by Adjusted Rand Index and structural balance penalties,while dynamically determining the number of clusters. These results highlightthe practical utility of near-term quantum computing for graph-basedunsupervised learning in financial applications.</description>
      <author>example@mail.com (Shivam Sharma, Supreeth Mysore Venkatesh, Pushkin Kachroo)</author>
      <guid isPermaLink="false">2509.07766v2</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury</title>
      <link>http://arxiv.org/abs/2509.12155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了使用低秩适应（LoRA）技术微调大型视觉模型（DinoV2和SwinV2）以诊断立体定向体放射治疗（SBRT）后X射线CT扫描中的放射性肺损伤（RILI）的有效性。&lt;h4&gt;背景&lt;/h4&gt;需要开发有效的方法来诊断放射治疗后的放射性肺损伤，这可能对患者监测和治疗调整至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估LoRA方法在微调大型视觉模型以诊断RILI方面的稳健性和效率，并与传统的完全微调和仅推理方法进行比较。&lt;h4&gt;方法&lt;/h4&gt;使用两种尺寸（50 mm³和75 mm³）的裁剪图像（以治疗等中心为中心），以及不同的适应技术将2D大型视觉模型适应为3D数据处理，评估模型对空间上下文的敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;LoRA实现了与传统微调相当或更好的性能，同时显著降低了计算成本和训练时间，因为需要更少的可训练参数。&lt;h4&gt;结论&lt;/h4&gt;LoRA是一种有效的方法，可用于微调大型视觉模型以诊断放射性肺损伤，同时保持高性能并减少计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;本研究调查了低秩适应（LoRA）技术用于微调大型视觉模型（DinoV2和SwinV2）以诊断立体定向体放射治疗（SBRT）后X射线CT扫描中放射性肺损伤（RILI）的有效性。为了评估这种方法的稳健性和效率，我们将LoRA与传统完全微调和仅推理（不微调）方法进行比较。除了使用以治疗等为中心的两种尺寸（50 mm³和75 mm³）的裁剪图像外，我们还使用了不同的适应技术将2D大型视觉模型适应为3D数据处理，以确定模型对空间上下文的敏感性。实验结果表明，LoRA实现了与传统微调相当或更好的性能，同时通过需要更少的可训练参数显著降低了计算成本和训练时间。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何使用人工智能技术自动诊断肺癌患者接受立体定向体部放射治疗（SBRT）后可能出现的辐射诱导肺损伤（RILI）。这个问题很重要，因为RILI是肺癌放疗后的常见并发症（发生率5-25%），早期诊断困难但至关重要，能帮助医生及时干预，改善患者治疗效果。传统诊断方法面临症状与其他肺部疾病重叠、影像特征随时间变化等挑战，而现有AI方法（如CNN）性能有限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到CNN在RILI诊断中的局限性，转而探索视觉变换器（ViT）等先进模型，因为它们能更好地捕捉全局图像信息。他们借鉴了LoRA（低秩自适应）技术，这是一种高效微调大型模型的方法。作者还参考了自己之前使用CNN诊断RILI的工作，以及其他研究使用放射组学预测RILI的方法。设计上，他们比较了三种微调策略（不微调、完全微调、LoRA微调），并测试了不同输入方式（2D切片和3D正交信息）和图像大小对模型性能的影响。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用LoRA技术微调大型视觉模型（DinoV2和SwinV2），只更新少量参数就能有效适应医学影像诊断任务，同时保留模型从自然图像中学到的丰富特征。整体流程包括：1）收集并预处理CT扫描数据，包括标准化分辨率、对齐、裁剪治疗区域等；2）设计两种输入方式（2D轴向切片和3D正交切片）；3）应用三种微调策略（不微调、完全微调、LoRA微调）；4）使用五折交叉验证训练和评估模型；5）在独立测试集上评估性能，使用ROC-AUC、F1分数等指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将大型视觉模型应用于SBRT后RILI分类；引入LoRA微调技术大幅降低计算成本；评估不同输入方式和图像大小对性能的影响；在具有挑战性的子集（如治疗后早期小病灶）上验证模型。相比之前工作，本研究使用更先进的视觉变换器代替CNN，直接处理原始影像而非手工特征，使用更大数据集，并通过LoRA实现了参数高效的微调，在保持性能的同时显著减少了训练时间和计算资源需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文展示了使用LoRA微调的大型视觉模型能够高效准确地诊断SBRT后的辐射诱导肺损伤，显著减少了计算成本和训练时间，同时保持了与完全微调相当或更好的性能，为临床决策提供了AI支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the efficacy of Low-Rank Adaptation (LoRA) forfine-tuning large Vision Models, DinoV2 and SwinV2, to diagnoseRadiation-Induced Lung Injury (RILI) from X-ray CT scans following StereotacticBody Radiation Therapy (SBRT). To evaluate the robustness and efficiency ofthis approach, we compare LoRA with traditional full fine-tuning andinference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3and 75 mm3), centered at the treatment isocenter, in addition to differentadaptation techniques for adapting the 2D LVMs for 3D data were used todetermine the sensitivity of the models to spatial context. Experimentalresults show that LoRA achieves comparable or superior performance totraditional fine-tuning while significantly reducing computational costs andtraining times by requiring fewer trainable parameters.</description>
      <author>example@mail.com (M. Bolhassani, B. Veasey, E. Daugherty, S. Keltner, N. Kumar, N. Dunlap, A. Amini)</author>
      <guid isPermaLink="false">2509.12155v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Multi Anatomy X-Ray Foundation Model</title>
      <link>http://arxiv.org/abs/2509.12146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了XR-0，这是一个多解剖部位的X射线基础模型，通过在大规模私有数据集上进行自监督学习训练，在多种临床任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;X射线影像在放射学中无处不在，但现有的AI基础模型大多局限于胸部解剖结构，无法在更广泛的临床任务中泛化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨多种解剖区域和临床任务泛化的X射线基础模型。&lt;h4&gt;方法&lt;/h4&gt;介绍XR-0模型，使用自监督学习方法，在包含115万张图像的大规模私有数据集上进行训练，这些图像涵盖不同的解剖区域。该模型在12个数据集和20个下游任务上进行了评估，包括分类、检索、分割、定位、视觉定位和报告生成。&lt;h4&gt;主要发现&lt;/h4&gt;XR-0在大多数多解剖部位任务上达到了最先进的性能，并且在胸部特定基准测试中保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;解剖多样性和监督对于构建健壮的通用医疗视觉模型至关重要，为放射学中可扩展和适应性强的AI系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;X射线影像在放射学中无处不在，然而大多数现有的AI基础模型仅限于胸部解剖结构，无法在更广泛的临床任务中泛化。在这项工作中，我们介绍了XR-0，这是一个多解剖部位的X射线基础模型，它通过在包含115万张图像的大规模私有数据集上进行自监督学习训练，这些图像涵盖多种解剖区域，并在12个数据集和20个下游任务上进行了评估，包括分类、检索、分割、定位、视觉定位和报告生成。XR-0在大多数多解剖部位任务上达到了最先进的性能，并在胸部特定基准测试中保持竞争力。我们的结果表明，解剖多样性和监督对于构建健壮的通用医疗视觉模型至关重要，为放射学中可扩展和适应性强的AI系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; X-ray imaging is a ubiquitous in radiology, yet most existing AI foundationmodels are limited to chest anatomy and fail to generalize across broaderclinical tasks. In this work, we introduce XR-0, the multi-anatomy X-rayfoundation model using self-supervised learning on a large, private dataset of1.15 million images spanning diverse anatomical regions and evaluated across 12datasets and 20 downstream tasks, including classification, retrieval,segmentation, localization, visual grounding, and report generation. XR-0achieves state-of-the-art performance on most multi-anatomy tasks and remainscompetitive on chest-specific benchmarks. Our results demonstrate thatanatomical diversity and supervision are critical for building robust,general-purpose medical vision models, paving the way for scalable andadaptable AI systems in radiology.</description>
      <author>example@mail.com (Nishank Singla, Krisztian Koos, Farzin Haddadpour, Amin Honarmandi Shandiz, Lovish Chum, Xiaojian Xu, Qing Jin, Erhan Bas)</author>
      <guid isPermaLink="false">2509.12146v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Navigation Foundation Model</title>
      <link>http://arxiv.org/abs/2509.12129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://pku-epic.github.io/NavFoM-Web/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个跨具身和跨任务的导航基础模型(NavFoM)，该模型在多种具身(四足动物、无人机、轮式机器人和车辆)和多种导航任务(视觉语言导航、目标搜索、目标跟踪和自动驾驶)上表现出强大的泛化能力，无需任务特定的微调即可达到最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管大型视觉-语言模型(VLMs)在通用视觉-语言任务上表现出显著的零样本性能，但它们在具身导航中的泛化能力仍然主要局限于狭窄的任务设置和特定的具身架构。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨不同具身架构和多种导航任务泛化的导航基础模型，解决当前具身AI导航系统泛化能力有限的问题。&lt;h4&gt;方法&lt;/h4&gt;NavFoM采用统一架构处理多模态导航输入，集成了标识符令牌来嵌入具身的摄像头视图信息和任务的时间上下文，并在有限的令牌长度预算下使用动态调整的采样策略控制所有观测令牌。该模型在八百万个导航样本上进行了训练，涵盖了四足动物、无人机、轮式机器人和车辆等多种具身。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准上的广泛评估表明，NavFoM在多个导航任务和具身上实现了最先进或极具竞争力的性能，无需任务特定的微调。额外的真实世界实验进一步证实了该模型具有强大的泛化能力和实际适用性。&lt;h4&gt;结论&lt;/h4&gt;NavFoM代表了具身AI导航领域的重要进展，通过统一的架构和创新的令牌处理策略，实现了跨具身和跨任务的强大泛化能力，为实际应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;导航是具身AI中的基本能力，代表遵循语言指令在物理环境中感知和互动所需的智能。尽管大型视觉-语言模型(VLMs)在通用视觉-语言任务上表现出显著的零样本性能，但它们在具身导航中的泛化能力仍然主要局限于狭窄的任务设置和特定的具身架构。在这项工作中，我们引入了一个跨具身和跨任务的导航基础模型(NavFoM)，该模型在八百万个导航样本上进行了训练，这些样本涵盖了四足动物、无人机、轮式机器人和车辆，并跨越了视觉语言导航、目标搜索、目标跟踪和自动驾驶等多种任务。NavFoM采用统一架构，处理来自不同摄像头配置和导航范围的多模态导航输入。为了适应不同的摄像头设置和时间范围，NavFoM集成了标识符令牌，这些令牌嵌入具身的摄像头视图信息和任务的时间上下文。此外，为了满足实际部署的需求，NavFoM在有限的令牌长度预算下，使用动态调整的采样策略控制所有观测令牌。在公共基准上的广泛评估表明，我们的模型在多个导航任务和具身上实现了最先进或极具竞争力的性能，而无需任务特定的微调。额外的真实世界实验进一步证实了我们方法的强大泛化能力和实际适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决具身导航模型在跨具身形态和跨任务上的泛化能力不足问题。现有导航模型要么局限于特定具身形态，要么只能处理特定任务，缺乏通用性。这个问题很重要，因为导航是具身AI的基础能力，一个通用的导航模型可以适应各种机器人和应用场景，减少定制化需求，降低开发和部署成本，并更好地适应真实世界的复杂性和多样性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类主要通过视觉完成导航任务的启发，以及最近仅视觉导航方法的成功，将通用导航任务表述为处理第一视角视频和语言指令并预测轨迹。他们借鉴了视觉语言模型的基本架构、Transformer-based在大规模跨具身数据上的训练方法、视觉特征缓存机制和位置编码等技术。创新性地设计了时间-视角指示器(TVI)tokens来标识摄像头视角和时序信息，以及基于预算的时序采样(BATS)策略来处理大量视频帧，平衡性能和推理速度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的导航基础模型，能够处理不同具身形态和不同任务的导航问题。整体流程包括：1)使用预训练视觉编码器提取视觉特征并通过网格池化生成紧凑表示；2)使用TVI tokens编码摄像头视角和时序信息；3)采用BATS策略动态采样历史帧；4)组织视觉和语言令牌并通过LLM和规划模型预测轨迹；5)在大规模数据(800万导航样本和476万开放世界知识样本)上训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)跨具身和跨任务的统一导航框架；2)时间-视角指示器(TVI)tokens显式编码摄像头视角和时序信息；3)基于预算的时序采样(BATS)策略平衡性能和推理速度；4)大规模多样化数据集训练。相比之前工作，NavFoM不仅能跨不同具身形态泛化，还能处理多种导航任务；显式编码了多视角信息，解决了输入歧义；优化了长序列处理；直接预测轨迹而非生成文本描述；处理范围更广，包括自动驾驶和UAV导航等复杂场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; NavFoM通过引入时间-视角指示器tokens和基于预算的时序采样策略，创建了一个能够跨具身形态和跨任务泛化的统一导航基础模型，在多种导航任务和具身形态上实现了最先进或极具竞争力的性能，无需任务特定的微调。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigation is a fundamental capability in embodied AI, representing theintelligence required to perceive and interact within physical environmentsfollowing language instructions. Despite significant progress in largeVision-Language Models (VLMs), which exhibit remarkable zero-shot performanceon general vision-language tasks, their generalization ability in embodiednavigation remains largely confined to narrow task settings andembodiment-specific architectures. In this work, we introduce across-embodiment and cross-task Navigation Foundation Model (NavFoM), trainedon eight million navigation samples that encompass quadrupeds, drones, wheeledrobots, and vehicles, and spanning diverse tasks such as vision-and-languagenavigation, object searching, target tracking, and autonomous driving. NavFoMemploys a unified architecture that processes multimodal navigation inputs fromvarying camera configurations and navigation horizons. To accommodate diversecamera setups and temporal horizons, NavFoM incorporates identifier tokens thatembed camera view information of embodiments and the temporal context of tasks.Furthermore, to meet the demands of real-world deployment, NavFoM controls allobservation tokens using a dynamically adjusted sampling strategy under alimited token length budget. Extensive evaluations on public benchmarksdemonstrate that our model achieves state-of-the-art or highly competitiveperformance across multiple navigation tasks and embodiments without requiringtask-specific fine-tuning. Additional real-world experiments further confirmthe strong generalization capability and practical applicability of ourapproach.</description>
      <author>example@mail.com (Jiazhao Zhang, Anqi Li, Yunpeng Qi, Minghan Li, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu, Zhizheng Zhang, He Wang)</author>
      <guid isPermaLink="false">2509.12129v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</title>
      <link>http://arxiv.org/abs/2509.12105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICIAP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于SAM2的少样本分割方法(FS-SAM2)，该方法利用SAM2的视频能力直接用于少样本任务，并通过低秩适应(LoRA)处理标准数据集中的多样化图像，只需少量参数进行元训练，在多个数据集上取得了显著结果且推理效率高。&lt;h4&gt;背景&lt;/h4&gt;少样本语义分割最近受到广泛关注，目标是用少量标注样本分割未见类别。现有方法通常需要从头训练额外模块并在大型数据集上大量训练才能达到最佳性能。SAM2是一个用于零样本图像和视频分割的基础模型，采用模块化设计。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于SAM2的少样本分割方法，将SAM2的视频能力直接应用于少样本任务，通过LoRA处理标准数据集中的多样化图像，仅用少量参数进行元训练，支持任何K-shot配置。&lt;h4&gt;方法&lt;/h4&gt;提出FS-SAM2方法，直接利用SAM2的视频能力处理少样本任务，应用低秩适应(LoRA)到原始模块以处理标准数据集中的多样化图像，只对少量参数进行元训练以适应SAM2。&lt;h4&gt;主要发现&lt;/h4&gt;在PASCAL-5^i、COCO-20^i和FSS-1000数据集上评估FS-SAM2取得了显著结果，在推理过程中表现出优秀的计算效率，代码已开源。&lt;h4&gt;结论&lt;/h4&gt;FS-SAM2是一种有效的少样本语义分割方法，能够有效利用SAM2的基础模型能力，在多个数据集上取得了良好性能且计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;少样本语义分割最近受到了广泛关注。其目标是开发一个仅使用少量标注样本就能分割未见类别的模型。大多数现有方法通过从头训练一个额外模块来调整预训练模型。这些方法需要在大型数据集上进行大量训练才能达到最佳性能。Segment Anything Model 2 (SAM2) 是一个用于零样本图像和视频分割的基础模型，采用模块化设计。在本文中，我们提出了一种基于SAM2的少样本分割方法(FS-SAM2)，其中SAM2的视频能力被直接用于少样本任务。此外，我们对原始模块应用了低秩适应(Low-Rank Adaptation, LoRA)，以处理标准数据集中常见的多样化图像，这与SAM2预训练中使用的时间连接帧不同。通过这种方法，只有少量参数进行元训练，从而有效适应SAM2，同时受益于其出色的分割性能。我们的方法支持任何K-shot配置。我们在PASCAL-5^i、COCO-20^i和FSS-1000数据集上评估了FS-SAM2，取得了显著结果，并在推理过程中表现出优秀的计算效率。代码可在https://github.com/fornib/FS-SAM2获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot semantic segmentation has recently attracted great attention. Thegoal is to develop a model capable of segmenting unseen classes using only afew annotated samples. Most existing approaches adapt a pre-trained model bytraining from scratch an additional module. Achieving optimal performance withthese approaches requires extensive training on large-scale datasets. TheSegment Anything Model 2 (SAM2) is a foundational model for zero-shot image andvideo segmentation with a modular design. In this paper, we propose a Few-Shotsegmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilitiesare directly repurposed for the few-shot task. Moreover, we apply a Low-RankAdaptation (LoRA) to the original modules in order to handle the diverse imagestypically found in standard datasets, unlike the temporally connected framesused in SAM2's pre-training. With this approach, only a small number ofparameters is meta-trained, which effectively adapts SAM2 while benefiting fromits impressive segmentation performance. Our method supports any K-shotconfiguration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ andFSS-1000 datasets, achieving remarkable results and demonstrating excellentcomputational efficiency during inference. Code is available athttps://github.com/fornib/FS-SAM2</description>
      <author>example@mail.com (Bernardo Forni, Gabriele Lombardi, Federico Pozzi, Mirco Planamente)</author>
      <guid isPermaLink="false">2509.12105v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Time-Series Foundation Model by Universal Delay Embedding</title>
      <link>http://arxiv.org/abs/2509.12080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为'通用延迟嵌入'(UDE)的预训练基础模型，通过整合延迟嵌入表示和Koopman算子预测来革新时间序列预测，在各种基准和真实世界数据集上表现出色，具有可扩展性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在科学和工业领域具有重要应用价值，但传统方法在处理非线性时间序列时面临挑战，需要更有效的方法来捕捉动力系统的动态和拓扑特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测非线性时间序列的框架，同时保持良好的可解释性和泛化能力，适用于广泛的科学和工业应用。&lt;h4&gt;方法&lt;/h4&gt;利用Takens嵌入定理，将观测数据构造为动力系统表示，从Hankel矩阵构建二维子空间补丁，将这些补丁视为图像并通过先进深度学习技术处理，使用自注意力编码器学习这些补丁，在潜在空间中以线性方式学习有限维Koopman算子进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;在各类基准和气候数据集上评估显示，UDE比最先进的基础模型平均降低20%以上的均方误差，在微调场景中表现出更好的泛化能力；学习到的动力表示和Koopman算子预测具有卓越的可解释性，能够一致识别拓扑信息丰富的子空间并稳健编码领域不变动力。&lt;h4&gt;结论&lt;/h4&gt;UDE确立了一种可扩展、可解释的通用时间序列建模和预测框架，具有广泛的科学和工业应用价值，为时间序列分析提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;本研究引入了通用延迟嵌入(UDE)，这是一种预训练基础模型，旨在通过有原则地整合延迟嵌入表示和Koopman算子预测来革新时间序列预测。利用Takens嵌入定理，UDE作为观测数据的动力表示，从Hankel矩阵构建二维子空间补丁，理论上保留了底层动力系统的动力和拓扑特性。这些补丁被视为图像，可以通过利用先进的深度学习技术高效处理。计算上，这些补丁进一步作为令牌用于学习自注意力编码器，从而在潜在空间中以线性方式通过有限维Koopman算子实现非线性时间序列的准确预测。在各种基准和真实世界气候数据集上的广泛评估表明，与最先进的基础模型相比，平均均方误差降低了20%以上，同时在微调场景中表现出更好的泛化能力。特别是，学习到的动力表示和从补丁中形成的Koopman算子预测表现出卓越的可解释性，能够一致识别拓扑信息丰富的子空间并稳健编码领域不变动力，确立了UDE作为可扩展、可解释的通用时间序列建模和预测框架，具有广泛的科学和工业应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces Universal Delay Embedding (UDE), a pretrainedfoundation model designed to revolutionize time-series forecasting throughprincipled integration of delay embedding representation and Koopman operatorprediction. Leveraging Takens' embedding theorem, UDE as a dynamicalrepresentation of observed data constructs two-dimensional subspace patchesfrom Hankel matrices, theoretically preserving dynamical and topologicalproperties of underlying dynamical systems. Such patches are viewed as images,which can be efficiently processed by exploiting advanced deep learningtechnologies. Computationally, these patches further serve as tokens forlearning a self-attention encoder, thus enabling accurate prediction ofnonlinear time-series by a finite-dimensional Koopman operator in a linearmanner in a latent space. Extensive evaluations across various benchmarks andreal-world climate datasets demonstrate over 20% average reduction in meansquared error versus state-of-the-art foundation models, alongside superiorgeneralization in fine-tuning scenarios. In particular, the learned dynamicalrepresentations and Koopman operator prediction forms from the patches exhibitexceptional interpretability, with consistent identification of topologicallyinformative subspaces and robust encoding of domain-invariant dynamics,establishing UDE as a scalable, interpretable framework for universaltime-series modeling and forecasting with broad scientific and industrialapplicability.</description>
      <author>example@mail.com (Zijian Wang, Peng Tao, Jifan Shi, Rui Bao, Rui Liu, Luonan Chen)</author>
      <guid isPermaLink="false">2509.12080v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications</title>
      <link>http://arxiv.org/abs/2509.12053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first two authors have equal contributions; Published as a  conference paper in HPCA 2025; 13 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LEGO框架是一种针对张量应用的新方法，能够自动生成空间架构设计和可综合的RTL代码，无需手工RTL设计模板，解决了现有框架在设计灵活性和RTL生成生产力之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;现代张量应用，特别是基础模型和生成式AI应用需要多种输入模态（视觉和语言），这增加对灵活加速器架构的需求。现有框架要么仅限于少数手工编写的模板，要么无法自动生成RTL。&lt;h4&gt;目的&lt;/h4&gt;提出LEGO框架，针对张量应用，自动生成空间架构设计并输出可综合的RTL代码，无需手工RTL设计模板，解决设计灵活性和RTL生成生产力之间的权衡问题。&lt;h4&gt;方法&lt;/h4&gt;利用基于仿射变换的架构表示，LEGO前端找到功能单元之间的互连，合成存储系统，并根据数据重用分析融合不同的空间数据流设计；LEGO后端在原始级别图上翻译硬件以执行低级优化，应用线性规划算法最优插入流水线寄存器，减少切换空间数据流时的未使用逻辑开销。&lt;h4&gt;主要发现&lt;/h4&gt;LEGO相比之前的工作Gemmini可以实现3.2倍的速度提升和2.4倍的能效提升，能够为生成式AI应用中的各种现代基础模型生成一个架构。&lt;h4&gt;结论&lt;/h4&gt;LEGO框架有效地解决了设计灵活性和RTL生成生产力之间的权衡问题，能够高效地为现代张量应用生成优化的硬件架构。&lt;h4&gt;翻译&lt;/h4&gt;现代张量应用，特别是基础模型和生成式AI应用需要多种输入模态（视觉和语言），这增加对灵活加速器架构的需求。现有框架在设计灵活性和RTL生成生产力之间存在权衡：要么仅限于少数手工编写的模板，要么无法自动生成RTL。为解决这一挑战，我们提出了LEGO框架，该框架针对张量应用，自动生成空间架构设计并输出可综合的RTL代码，无需手工RTL设计模板。利用基于仿射变换的架构表示，LEGO前端找到功能单元之间的互连，合成存储系统，并根据数据重用分析融合不同的空间数据流设计。LEGO后端然后在原始级别图上翻译硬件以执行低级优化，并应用一组线性规划算法以最优方式插入流水线寄存器，减少切换空间数据流时未使用逻辑的开销。我们的评估表明，与之前的工作Gemmini相比，LEGO可以实现3.2倍的速度提升和2.4倍的能效提升，并且可以为生成式AI应用中的各种现代基础模型生成一个架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/HPCA61900.2025.00101&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern tensor applications, especially foundation models and generative AIapplications require multiple input modalities (both vision and language),which increases the demand for flexible accelerator architecture. Existingframeworks suffer from the trade-off between design flexibility andproductivity of RTL generation: either limited to very few hand-writtentemplates or cannot automatically generate the RTL. To address this challenge,we propose the LEGO framework, which targets tensor applications andautomatically generates spatial architecture design and outputs synthesizableRTL code without handwritten RTL design templates. Leveraging theaffine-transformation-based architecture representation, LEGO front end findsinterconnections between function units, synthesizes the memory system, andfuses different spatial dataflow designs based on data reuse analysis. LEGOback end then translates the hardware in a primitive-level graph to performlower-level optimizations, and applies a set of linear-programming algorithmsto optimally insert pipeline registers and reduce the overhead of unused logicwhen switching spatial dataflows. Our evaluation demonstrates that LEGO canachieve 3.2x speedup and 2.4x energy efficiency compared to previous workGemmini, and can generate one architecture for diverse modern foundation modelsin generative AI applications.</description>
      <author>example@mail.com (Yujun Lin, Zhekai Zhang, Song Han)</author>
      <guid isPermaLink="false">2509.12053v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Radio Galaxy Zoo: Morphological classification by Fanaroff-Riley designation using self-supervised pre-training</title>
      <link>http://arxiv.org/abs/2509.11988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to MNRAS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用RGZ项目中的14,000多个射电星系，对约5,900个FRI型和8,100个FRII型射电星系进行了分类。研究分析了使用预训练并微调的射电星系基础模型预测的形态，发现了FRI和FRII光度-大小分布的重叠区域，模型在这些区域的置信度较低。研究还探讨了预训练和微调数据选择对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;Radio Galaxy Zoo (RGZ)项目提供了大量射电星系数据，射电星系的形态分类（特别是Fanaroff-Riley分类）是天文学研究中的重要课题。随着自动化分类方法的发展，了解模型训练数据选择对结果的影响变得尤为重要。&lt;h4&gt;目的&lt;/h4&gt;分析RGZ目录中预测的射电星系形态，评估预训练并微调的射电星系基础模型在FR形态分类任务上的表现，并探讨训练数据选择对模型输出的影响。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的射电星系基础模型，并针对预测Fanaroff-Riley形态的任务进行了微调。对RGZ项目中的14,000多个射电星系进行分类，并对约5,900个FRI型和8,100个FRII型射电星系进行分析。研究还考察了预训练和微调数据选择对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1. FRI和FRII形态分类的光度-大小分布存在重叠；2. 模型在重叠区域的预测置信度最低，表明源形态在这些区域更为模糊；3. 发现了低光度FRII源，其比例与之前的研究一致；4. 本研究发现的低光度FRII源与之前研究识别的源存在差异，可能受分类方法影响；5. 不同的预训练数据选择会影响模型置信度，但不会引起系统性的泛化偏差；6. 微调数据的选择可能对模型有不同影响。&lt;h4&gt;结论&lt;/h4&gt;随着天体源识别和分类的自动化方法日益普及，训练数据的选择会显著影响模型输出并可能传播到下游分析中，因此需要谨慎选择训练数据。&lt;h4&gt;翻译&lt;/h4&gt;在本研究中，我们检查了从Radio Galaxy Zoo (RGZ)项目中精心挑选的14,000多个射电星系，并为约5,900个FRI型和8,100个FRII型射电星系提供了分类。我们展示了使用经过微调以预测Fanaroff-Riley (FR)形态的预训练射电星系基础模型对RGZ目录中预测的射电星系形态的分析。如先前研究所示，我们的结果表明形态分类的FRI和FRII光度-大小分布存在重叠，我们发现模型对其预测的置信度在这一重叠区域最低，表明源形态更加模糊。我们确定了低光度FRII源的存在，其相对于FRII总数的比例与先前研究一致。然而，将本研究发现的低光度FRII源与先前研究确定的源进行比较，揭示了差异，这可能表明它们的选择受到分类方法选择的影响。我们研究了预训练和微调数据选择对下游分类任务模型性能的影响，表明虽然不同的预训练数据选择会影响模型置信度，但它们似乎不会引起所考虑的物理和观测特性范围内的系统性泛化偏差；然而，我们注意到对于微调情况可能并非如此。随着天体源识别和分类的自动化方法日益普及，我们强调了可能影响模型输出并传播到下游分析的训练数据选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we examine over 14,000 radio galaxies finely selected fromRadio Galaxy Zoo (RGZ) project and provide classifications for approximately5,900 FRIs and 8,100 FRIIs. We present an analysis of these predicted radiogalaxy morphologies for the RGZ catalogue, classified using a pre-trained radiogalaxy foundation model that has been fine-tuned to predict Fanaroff-Riley (FR)morphology. As seen in previous studies, our results show overlap betweenmorphologically classified FRI and FRII luminosity-size distributions and wefind that the model's confidence in its predictions is lowest in this overlapregion, suggesting that source morphologies are more ambiguous. We identify thepresence of low-luminosity FRII sources, the proportion of which, with respectto the total number of FRIIs, is consistent with previous studies. However, acomparison of the low-luminosity FRII sources found in this work with thoseidentified by previous studies reveals differences that may indicate theirselection is influenced by the choice of classification methodology. Weinvestigate the impacts of both pre-training and fine-tuning data selection onmodel performance for the downstream classification task, and show that whiledifferent pre-training data choices affect model confidence they do not appearto cause systematic generalisation biases for the range of physical andobservational characteristics considered in this work; however, we note thatthe same is not necessarily true for fine-tuning. As automated approaches toastronomical source identification and classification become increasinglyprevalent, we highlight training data choices that can affect the model outputsand propagate into downstream analyses.</description>
      <author>example@mail.com (Nutthawara Buatthaisong, Inigo Val Slijepcevic, Anna M. M. Scaife, Micah Bowles, Andrew Hopkins, Devina Mohan, Stanislav S Shabala, O. Ivy Wong)</author>
      <guid isPermaLink="false">2509.11988v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Low-rank Orthogonalization for Large-scale Matrix Optimization with Applications to Foundation Model Training</title>
      <link>http://arxiv.org/abs/2509.11983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了低秩正交化方法，用于改进神经网络训练中的矩阵优化问题，并通过实验和理论分析证明了其优越性。&lt;h4&gt;背景&lt;/h4&gt;神经网络训练本质上是一个大规模矩阵优化问题，但长期以来神经网络参数的矩阵结构被忽视了。Muon优化器因其显式利用这种结构而在基础模型训练中表现出色。&lt;h4&gt;目的&lt;/h4&gt;提出低秩正交化方法，利用神经网络训练过程中梯度的低秩特性，改进Muon优化器的性能。&lt;h4&gt;方法&lt;/h4&gt;提出低秩正交化，基于此提出低秩矩阵符号梯度下降和Muon的低秩变体。&lt;h4&gt;主要发现&lt;/h4&gt;低秩正交化具有优越的性能，低秩Muon在GPT-2和LLaMA预训练中超过了基础Muon的性能。理论上，建立了低秩矩阵符号梯度下降和低秩Muon的迭代复杂性。&lt;h4&gt;结论&lt;/h4&gt;低秩正交化是一种有效的神经网络训练方法，能够显著提升优化性能。&lt;h4&gt;翻译&lt;/h4&gt;神经网络训练本质上是一个大规模矩阵优化问题，但长期以来神经网络参数的矩阵结构被忽视了。最近，名为Muon的优化器因其显式利用这种结构而在基础模型训练中表现出色，引起了广泛关注。Muon成功的一个关键组成部分是矩阵正交化。在这篇论文中，作者提出了'低秩正交化'，它明确利用了神经网络训练过程中梯度的低秩特性。基于此，他们提出了低秩矩阵符号梯度下降和Muon的低秩变体。数值实验表明，低秩正交化具有优越的性能，低秩Muon在GPT-2和LLaMA预训练中取得了有希望的结果，超过了精心调整的基础Muon的性能。理论上，他们建立了低秩矩阵符号梯度下降寻找近似平稳解的迭代复杂性，以及低秩Muon在重尾噪声下寻找近似随机平稳解的迭代复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural network (NN) training is inherently a large-scale matrix optimizationproblem, yet the matrix structure of NN parameters has long been overlooked.Recently, the optimizer Muon \cite{jordanmuon}, which explicitly exploits thisstructure, has gained significant attention for its strong performance infoundation model training. A key component contributing to Muon's success ismatrix orthogonalization. In this paper, we propose {\it low-rankorthogonalization}, which explicitly leverages the low-rank nature of gradientsduring NN training. Building on this, we propose low-rank matrix-signedgradient descent and a low-rank variant of Muon. Our numerical experimentsdemonstrate the superior performance of low-rank orthogonalization, with thelow-rank Muon achieving promising results in GPT-2 and LLaMA pretraining --surpassing the performance of the carefully tuned vanilla Muon. Theoretically,we establish the iteration complexity of the low-rank matrix-signed gradientdescent for finding an approximate stationary solution, as well as that oflow-rank Muon for finding an approximate stochastic stationary solution underheavy-tailed noise.</description>
      <author>example@mail.com (Chuan He, Zhanwang Deng, Zhaosong Lu)</author>
      <guid isPermaLink="false">2509.11983v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>MusicSwarm: Biologically Inspired Intelligence for Music Composition</title>
      <link>http://arxiv.org/abs/2509.11973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了一种去中心化的音乐创作系统，由相同的冻结基础模型组成，通过基于信息素的点对点信号协调，无需权重更新即可生成连贯的长篇音乐作品。&lt;h4&gt;背景&lt;/h4&gt;传统音乐创作系统通常采用集中式架构，而本研究探索了去中心化群体在音乐创作中的应用潜力。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需权重更新的去中心化音乐创作系统，通过信息素协调机制生成连贯的长篇音乐作品，并探索其在创意生成中的优势。&lt;h4&gt;方法&lt;/h4&gt;使用相同的冻结基础模型组成群体，通过基于信息素的点对点信号协调；比较集中式多智能体系统与完全去中心化群体；进行符号、音频和图论分析评估结果。&lt;h4&gt;主要发现&lt;/h4&gt;去中心化群体系统产生了更高质量、更大多样性和结构变化的作品；系统动态趋于稳定的互补角色配置；自相似性网络揭示了小世界架构；局部新颖性能够整合为全球音乐形式。&lt;h4&gt;结论&lt;/h4&gt;MusicSwarm通过将专业化从参数更新转向交互规则、共享记忆和动态共识，为长时程创造性结构提供了计算和数据高效的途径，可直接应用于音乐之外的协作写作、设计和科学发现。&lt;h4&gt;翻译&lt;/h4&gt;我们展示了一种去中心化的群体，由相同的冻结基础模型组成，通过基于信息素的点对点信号协调，无需任何权重更新即可产生连贯的长篇音乐创作。我们将带有全局评论者的集中式多智能体系统与完全去中心化的群体进行比较，在后者中，小节级智能体感知并存储和声、节奏和结构线索，适应短期记忆，并达成共识。通过符号、音频和图论分析，群体系统产生了更高质量的作品，同时提供了更大的多样性和结构变化，并在创意指标上表现更优。系统动态趋于稳定的互补角色配置，自相似性网络揭示了具有高效远程连接和专门桥接模体的小世界架构，阐明了局部新颖性如何整合为全球音乐形式。通过将专业化从参数更新转向交互规则、共享记忆和动态共识，MusicSwarm为长时程创造性结构提供了一种计算和数据高效的途径，可直接应用于音乐之外的协作写作、设计和科学发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We show that coherent, long-form musical composition can emerge from adecentralized swarm of identical, frozen foundation models that coordinate viastigmergic, peer-to-peer signals, without any weight updates. We compare acentralized multi-agent system with a global critic to a fully decentralizedswarm in which bar-wise agents sense and deposit harmonic, rhythmic, andstructural cues, adapt short-term memory, and reach consensus. Across symbolic,audio, and graph-theoretic analyses, the swarm yields superior quality whiledelivering greater diversity and structural variety and leads across creativitymetrics. The dynamics contract toward a stable configuration of complementaryroles, and self-similarity networks reveal a small-world architecture withefficient long-range connectivity and specialized bridging motifs, clarifyinghow local novelties consolidate into global musical form. By shiftingspecialization from parameter updates to interaction rules, shared memory, anddynamic consensus, MusicSwarm provides a compute- and data-efficient route tolong-horizon creative structure that is immediately transferable beyond musicto collaborative writing, design, and scientific discovery.</description>
      <author>example@mail.com (Markus J. Buehler)</author>
      <guid isPermaLink="false">2509.11973v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation</title>
      <link>http://arxiv.org/abs/2509.11885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper has been accepted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Brea-Depth的新框架，用于支气管镜检查中的单目深度估计，通过整合气道特定的几何先验，提高了实时导航精度和手术安全性。&lt;h4&gt;背景&lt;/h4&gt;单目深度估计在支气管镜检查中可以显著提高复杂分支气道中的实时导航精度和手术安全性。然而，现有的深度基础模型在支气管镜场景中缺乏解剖学意识，容易过度拟合局部纹理而非捕获全局气道结构，特别是在深度线索模糊和光照条件差的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确估计支气管镜检查中深度的方法，特别是提高模型对气道结构的理解和处理能力，从而产生更准确、鲁棒的3D气道重建。&lt;h4&gt;方法&lt;/h4&gt;提出Brea-Depth框架，整合气道特定几何先验到基础模型适应中。方法包括：1) 引入深度感知的CycleGAN，优化真实支气管镜图像与解剖数据中气道几何之间的转换；2) 引入气道结构感知损失，强制气道腔内的深度一致性，同时保持平滑过渡和结构完整性。&lt;h4&gt;主要发现&lt;/h4&gt;Brea-Depth通过整合解剖先验，增强了模型泛化能力，产生了更鲁棒、准确的3D气道重建。在收集的离体人肺数据集和公开的支气管镜数据集上，该方法在解剖深度保存方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;Brea-Depth通过整合气道特定的几何先验，有效解决了现有深度基础模型在支气管镜场景中缺乏解剖学意识的问题，提高了深度估计的准确性和鲁棒性，有望改善支气管镜检查中的实时导航精度和手术安全性。&lt;h4&gt;翻译&lt;/h4&gt;支气管镜检查中的单目深度估计可以显著提高复杂分支气道中的实时导航精度和手术安全性。最近的深度基础模型进展在内镜场景中显示出前景，但这些模型通常缺乏支气管镜中的解剖学意识，过度拟合局部纹理而非捕获全局气道结构，特别是在深度线索模糊和光照条件差的情况下。为解决这一问题，我们提出了Brea-Depth，一种新颖的框架，将气道特定的几何先验整合到基础模型适应中，用于支气管镜深度估计。我们的方法引入了深度感知的CycleGAN，优化真实支气管镜图像与解剖数据中气道几何之间的转换，有效弥合了域差距。此外，我们引入了气道结构感知损失，强制气道腔内的深度一致性，同时保持平滑过渡和结构完整性。通过整合解剖先验，Brea-Depth增强了模型泛化能力，产生了更鲁棒、准确的3D气道重建。为评估解剖学真实性，我们引入了气道深度结构评估，一种新的结构一致性指标。我们在收集的离体人肺数据集和公开的支气管镜数据集上验证了Brea-Depth，它在解剖深度保存方面优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决支气管镜检查中的单目深度估计问题。现有方法缺乏解剖学意识，过度拟合局部纹理而非捕获全局气道结构，尤其在模糊深度线索和不良光照条件下表现不佳。这个问题很重要，因为准确的深度估计能显著提高实时导航精度，增强复杂分支气道中干预措施的安全性，对靶向活检或局部药物输送等需要结构精确深度的任务至关重要，同时能改进3D气道重建质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有深度基础模型在支气管镜场景中的局限性，认识到需要针对特定解剖结构进行优化。他们借鉴了CycleGAN进行图像转换、U-Net架构进行编码解码、以及基础模型提供伪深度监督等现有技术。在此基础上，他们创新性地开发了深度感知的CycleGAN专门针对支气管镜场景优化，并引入气道结构感知损失函数确保深度预测符合解剖学先验。这种设计既利用了现有方法的优势，又针对性地解决了支气管镜特有的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将气道特定的几何先验知识整合到基础深度估计模型中，创建专门针对支气管镜场景的深度估计框架，确保生成的深度图不仅在像素级准确，而且在解剖学上一致。整体流程包括：1)创建几何准确的3D气道模型；2)实现深度感知CycleGAN，包含合成到真实和真实到合成两个转换分支；3)引入气道结构感知损失函数强制气道管腔内深度一致性；4)使用合成和真实数据混合训练，最终实现60FPS的实时深度估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)BREA-Depth框架，首个将气道几何整合到基础深度估计中的专门框架；2)深度感知CycleGAN，直接将深度整合到域转换中；3)气道结构感知损失，强制气道管腔内深度一致性；4)气道深度结构评估指标，关注解剖一致性而非仅像素级准确性。相比之前工作，不同之处在于：不依赖CT生成的简化数据；具有解剖学意识关注全局结构；在模糊条件下表现更好；保留气道几何结构；引入专门评估指标更全面评估实用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BREA-Depth通过整合气道特定的几何先验知识到深度估计基础模型中，首次实现了在支气管镜场景中既保持像素级准确性又确保解剖学一致性的深度预测，显著提升了复杂气道导航和3D重建的可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular depth estimation in bronchoscopy can significantly improvereal-time navigation accuracy and enhance the safety of interventions incomplex, branching airways. Recent advances in depth foundation models haveshown promise for endoscopic scenarios, yet these models often lack anatomicalawareness in bronchoscopy, overfitting to local textures rather than capturingthe global airway structure, particularly under ambiguous depth cues and poorlighting. To address this, we propose Brea-Depth, a novel framework thatintegrates airway-specific geometric priors into foundation model adaptationfor bronchoscopic depth estimation. Our method introduces a depth-awareCycleGAN, refining the translation between real bronchoscopic images and airwaygeometries from anatomical data, effectively bridging the domain gap. Inaddition, we introduce an airway structure awareness loss to enforce depthconsistency within the airway lumen while preserving smooth transitions andstructural integrity. By incorporating anatomical priors, Brea-Depth enhancesmodel generalization and yields more robust, accurate 3D airwayreconstructions. To assess anatomical realism, we introduce Airway DepthStructure Evaluation, a new metric for structural consistency. We validateBREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopicdataset, where it outperforms existing methods in anatomical depthpreservation.</description>
      <author>example@mail.com (Francis Xiatian Zhang, Emile Mackute, Mohammadreza Kasaei, Kevin Dhaliwal, Robert Thomson, Mohsen Khadem)</author>
      <guid isPermaLink="false">2509.11885v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</title>
      <link>http://arxiv.org/abs/2509.11772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Seg2Track-SAM2框架，结合预训练目标检测器、SAM2和新型Seg2Track模块，实现了无需微调且与检测器无关的多目标跟踪与分割(MOTS)系统，在基准测试中达到最先进性能并显著降低内存使用。&lt;h4&gt;背景&lt;/h4&gt;自主系统需要在动态环境中可靠运行，这要求具备强大的多目标跟踪(MOT)能力。虽然基础模型如SAM2在视频分割方面表现出强大的零样本泛化能力，但其直接应用于MOTS仍受身份管理和内存效率不足的限制。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够解决MOTS问题的框架，结合强大的零样本跟踪能力、增强的身份保持和高效的内存利用，同时不需要微调且与检测器无关。&lt;h4&gt;方法&lt;/h4&gt;Seg2Track-SAM2框架整合了预训练的目标检测器、SAM2和一个新的Seg2Track模块，用于处理轨道初始化、轨道管理和强化。该方法采用滑动窗口内存策略，可将内存使用减少高达75%，且性能下降可忽略不计。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI MOT和KITTI MOTS基准测试上，Seg2Track-SAM2实现了最先进(SOTA)的性能，在KITTI MOTS上汽车和行人类别总体排名第四，并在关联准确率(AssA)方面建立了新基准。滑动窗口内存策略可在资源受限条件下支持部署。&lt;h4&gt;结论&lt;/h4&gt;Seg2Track-SAM2通过结合强大的零样本跟踪、增强的身份保持和高效的内存利用，推动了MOTS的发展。&lt;h4&gt;翻译&lt;/h4&gt;自主系统需要在动态环境中可靠运行，这要求具备强大的多目标跟踪(MOT)能力。MOT确保了对象身份分配的一致性和空间 delineation 的精确性。基础模型(如SAM2)的最新进展已在视频分割方面表现出强大的零样本泛化能力，但它们直接应用于MOTS(多目标跟踪+分割)仍受身份管理和内存效率不足的限制。本研究介绍了Seg2Track-SAM2，一个结合了预训练目标检测器、SAM2和新型Seg2Track模块的框架，用于解决轨道初始化、轨道管理和强化问题。所提出的方法无需微调，且与检测器无关。在KITTI MOT和KITTI MOTS基准测试上的实验结果表明，Seg2Track-SAM2实现了最先进(SOTA)的性能，在KITTI MOTS上汽车和行人类别总体排名第四，同时在关联准确率(AssA)方面建立了新基准。此外，滑动窗口内存策略可将内存使用减少高达75%，且性能下降可忽略不计，支持在资源受限条件下的部署。这些结果证实，Seg2Track-SAM2通过结合强大的零样本跟踪、增强的身份保持和高效的内存利用，推动了MOTS的发展。代码可在https://github.com/hcmr-lab/Seg2Track-SAM2获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous systems require robust Multi-Object Tracking (MOT) capabilities tooperate reliably in dynamic environments. MOT ensures consistent objectidentity assignment and precise spatial delineation. Recent advances infoundation models, such as SAM2, have demonstrated strong zero-shotgeneralization for video segmentation, but their direct application to MOTS(MOT+Segmentation) remains limited by insufficient identity management andmemory efficiency. This work introduces Seg2Track-SAM2, a framework thatintegrates pre-trained object detectors with SAM2 and a novel Seg2Track moduleto address track initialization, track management, and reinforcement. Theproposed approach requires no fine-tuning and remains detector-agnostic.Experimental results on KITTI MOT and KITTI MOTS benchmarks show thatSeg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourthoverall in both car and pedestrian classes on KITTI MOTS, while establishing anew benchmark in association accuracy (AssA). Furthermore, a sliding-windowmemory strategy reduces memory usage by up to 75% with negligible performancedegradation, supporting deployment under resource constraints. These resultsconfirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shottracking, enhanced identity preservation, and efficient memory utilization. Thecode is available at https://github.com/hcmr-lab/Seg2Track-SAM2</description>
      <author>example@mail.com (Diogo Mendonça, Tiago Barros, Cristiano Premebida, Urbano J. Nunes)</author>
      <guid isPermaLink="false">2509.11772v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Igniting VLMs toward the Embodied Space</title>
      <link>http://arxiv.org/abs/2509.11766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WALL-OSS是一个端到端的具身基础模型，通过大规模多模态预训练实现了具身感知的视觉语言理解、强大的语言-动作关联和稳健的操作能力，解决了现有视觉语言模型在空间和具身理解方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;基础模型在语言和视觉方面取得了显著进展，但现有视觉语言模型在空间和具身理解方面仍然有限。将视觉语言模型转移到具身领域揭示了模态、预训练分布和训练目标之间的根本不匹配，使得动作理解和生成成为通往通用人工智能道路上的主要瓶颈。&lt;h4&gt;目的&lt;/h4&gt;介绍WALL-OSS模型，利用大规模多模态预训练实现具身感知的视觉语言理解、强大的语言-动作关联和稳健的操作能力，解决视觉语言模型到具身领域的转移问题。&lt;h4&gt;方法&lt;/h4&gt;采用紧密耦合的架构和多策略训练课程，实现统一跨层级思维链，在单一可微分框架内无缝统一指令推理、子目标分解和细粒度动作合成。&lt;h4&gt;主要发现&lt;/h4&gt;WALL-OSS在复杂长期操作上取得高成功率，展示强大的指令遵循能力和复杂理解推理能力，性能优于强大的基线模型。&lt;h4&gt;结论&lt;/h4&gt;WALL-OSS为从视觉语言模型到具身基础模型提供了一条可靠且可扩展的路径，解决了模态间不匹配和训练目标不一致的问题。&lt;h4&gt;翻译&lt;/h4&gt;虽然基础模型在语言和视觉方面显示出显著进展，但现有的视觉语言模型在空间和具身理解方面仍然有限。将视觉语言模型转移到具身领域揭示了模态、预训练分布和训练目标之间的根本不匹配，使得动作理解和生成成为通往通用人工智能道路上的主要瓶颈。我们引入了WALL-OSS，一个端到端的具身基础模型，它利用大规模多模态预训练实现(1)具身感知的视觉语言理解，(2)强大的语言-动作关联，和(3)稳健的操作能力。我们的方法采用紧密耦合的架构和多策略训练课程，实现了统一跨层级思维链——在单一可微分框架内无缝统一指令推理、子目标分解和细粒度动作合成。我们的结果表明，WALL-OSS在复杂的长期操作上取得高成功率，展示了强大的指令遵循能力、复杂理解和推理能力，并优于强大的基线模型，从而为从视觉语言模型到具身基础模型提供了一条可靠且可扩展的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models show remarkable progress in language and vision,existing vision-language models (VLMs) still have limited spatial andembodiment understanding. Transferring VLMs to embodied domains revealsfundamental mismatches between modalities, pretraining distributions, andtraining objectives, leaving action comprehension and generation as a centralbottleneck on the path to AGI.  We introduce WALL-OSS, an end-to-end embodied foundation model that leverageslarge-scale multimodal pretraining to achieve (1) embodiment-awarevision-language understanding, (2) strong language-action association, and (3)robust manipulation capability.  Our approach employs a tightly coupled architecture and multi-strategiestraining curriculum that enables Unified Cross-Level CoT-seamlessly unifyinginstruction reasoning, subgoal decomposition, and fine-grained action synthesiswithin a single differentiable framework.  Our results show that WALL-OSS attains high success on complex long-horizonmanipulations, demonstrates strong instruction-following capabilities, complexunderstanding and reasoning, and outperforms strong baselines, therebyproviding a reliable and scalable path from VLMs to embodied foundation models.</description>
      <author>example@mail.com (Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, Lucy Liang, Make Wang, Qian Wang, Roy Gan, Ryan Yu, Shalfun Li, Starrick Liu, Sylas Chen, Vincent Chen, Zach Xu)</author>
      <guid isPermaLink="false">2509.11766v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Uniqueness Theorem for Distributed Computation under Physical Constraint</title>
      <link>http://arxiv.org/abs/2509.11754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文在网络内计算等极端环境中，针对通信效率、有限内存和健壮可扩展性之间的三难困境，提出了一种基于逻辑必然性的解决方案。作者建立了严格的公理系统，证明了对于具有幂等合并操作符的计算，存在一个最优范式——自描述并行流（SDPF），并证明了该范式的收敛性、图灵完备性和最小性。这项工作类似于CAP定理，但提供了分布式计算流中不可避免性的唯一性定理。&lt;h4&gt;背景&lt;/h4&gt;传统的计算模型通常抽象了物理硬件限制，但在网络内计算等极端环境中，这些限制变得不可违反，导致在通信效率、有限内存和健壮可扩展性之间形成尖锐的三难困境。现有的分布式范式虽然在其预期领域很强大，但并非为这种严格的体系设计，因此面临根本性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决网络内计算等极端环境中面临的三难困境，即通信效率、有限内存和健壮可扩展性之间的冲突，寻找最优的分布式计算范式。&lt;h4&gt;方法&lt;/h4&gt;建立了一个严格的公理系统来形式化物理约束，并通过数学证明推导出最优范式。作者证明了对于具有幂等合并操作符的广泛计算类别，存在一个独特的最优范式，并分析了该范式的性质。&lt;h4&gt;主要发现&lt;/h4&gt;1) 对于具有幂等合并操作符的计算，存在一个独特的最优范式；2) 任何满足所建立公理的系统都必须收敛到自描述并行流（SDPF）这一单一标准形式；3) SDPF范式是收敛的、图灵完备的且是最小的；4) 类似于CAP定理，本文提供了一个唯一性定理，揭示了在物理定律下分布式计算流中什么是不可避免的。&lt;h4&gt;结论&lt;/h4&gt;解决极端环境中的计算三难困境需要转变视角，从寻求工程权衡到从逻辑必然性推导解决方案。自描述并行流（SDPF）是一个基于物理约束推导出的最优范式，它为分布式计算流提供了不可避免的设计原则。&lt;h4&gt;翻译&lt;/h4&gt;计算的基础模型通常抽象了物理硬件限制。然而，在网络内计算等极端环境中，这些限制变得不可违反的定律，在通信效率、有限内存和健壮可扩展性之间形成尖锐的三难困境。现有的分布式范式虽然在其预期领域很强大，但并非为这种严格的体系设计，因此面临根本性挑战。本文表明，解决这一三难困境需要转变视角——从寻求工程权衡到从逻辑必然性推导解决方案。我们建立了一个严格的公理系统，形式化了这些物理约束，并证明对于具有幂等合并操作符的广泛计算类别，存在一个独特的最优范式。任何满足这些公理的系统都必须收敛到一个单一的标准形式：自描述并行流（SDPF），这是一种纯数据中心的模型，其中无状态执行器处理携带自身控制逻辑的流。我们进一步证明这种独特范式是收敛的、图灵完备的且是最小的。就像CAP定理为分布式状态管理中不可能实现的事情建立了边界一样，我们的工作提供了一个建设性的对偶：一个唯一性定理，揭示了在物理定律下分布式计算流中什么是不可避免的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models of computation often abstract away physical hardwarelimitations. However, in extreme environments like In-Network Computing (INC),these limitations become inviolable laws, creating an acute trilemma amongcommunication efficiency, bounded memory, and robust scalability. Prevailingdistributed paradigms, while powerful in their intended domains, were notdesigned for this stringent regime and thus face fundamental challenges. Thispaper demonstrates that resolving this trilemma requires a shift in perspective- from seeking engineering trade-offs to deriving solutions from logicalnecessity. We establish a rigorous axiomatic system that formalizes thesephysical constraints and prove that for the broad class of computationsadmitting an idempotent merge operator, there exists a unique, optimalparadigm. Any system satisfying these axioms must converge to a single normalform: Self-Describing Parallel Flows (SDPF), a purely data-centric model wherestateless executors process flows that carry their own control logic. Wefurther prove this unique paradigm is convergent, Turing-complete, and minimal.In the same way that the CAP theorem established a boundary for what isimpossible in distributed state management, our work provides a constructivedual: a uniqueness theorem that reveals what is \textit{inevitable} fordistributed computation flows under physical law.</description>
      <author>example@mail.com (Zhiyuan Ren, Mingxuan Lu, Wenchi Cheng)</author>
      <guid isPermaLink="false">2509.11754v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications</title>
      <link>http://arxiv.org/abs/2509.11752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EchoCare是一个新型的超声基础模型，通过自监督学习在大型、多样化数据集EchoCareData上开发，能够有效整合多源数据学习超声表征，并在多种超声应用中表现出色。&lt;h4&gt;背景&lt;/h4&gt;真实临床环境中缺乏大型标记数据集，特定任务模型的泛化能力有限，这些因素阻碍了超声应用中可泛化临床AI模型的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的超声基础模型，能够有效整合多源数据学习超声表征，提高临床护理水平。&lt;h4&gt;方法&lt;/h4&gt;创建了EchoCareData数据集，包含450万张超声图像，来自5大洲23个国家，使用多种不同成像设备获取；开发了EchoCare模型，采用分层分类器，能够同时学习像素级和表征级特征；通过自监督学习方法训练模型；在10个代表性超声基准测试上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;EchoCare在10个不同诊断难度的代表性超声基准测试中优于最先进的比较模型，这些基准测试涵盖了疾病诊断、病灶分割、器官检测、标志点预测、定量回归、图像增强和报告生成；EchoCare仅需少量训练就能表现优异。&lt;h4&gt;结论&lt;/h4&gt;EchoCare提供了一个完全开放且可泛化的基础模型，可以促进各种临床超声应用中AI技术的发展，代码和预训练模型已公开发布，支持微调和本地适应，可扩展到更多应用。&lt;h4&gt;翻译&lt;/h4&gt;能够通过整合多源数据有效学习超声表征的人工智能在推进临床护理方面具有巨大潜力。然而，真实临床环境中大型标记数据集的稀缺以及特定任务模型有限的泛化能力，阻碍了超声应用中可泛化临床AI模型的发展。本研究提出了EchoCare，一个用于通用临床的新型超声基础模型，通过在策划的、公开可用的、大规模数据集EchoCareData上进行自监督学习开发。EchoCareData包含450万张超声图像，源自5大洲23个国家，通过多种不同的成像设备获取，因此包含了多中心、多设备和多种族的全队列人群。与采用现成视觉基础模型架构的先前研究不同，我们在EchoCare中引入了分层分类器，以实现像素级和表征级特征的联合学习，同时捕捉全局解剖上下文和局部超声特征。经过最少训练，EchoCare在10个具有不同诊断难度的代表性超声基准测试中优于最先进的比较模型，涵盖了疾病诊断、病灶分割、器官检测、标志点预测、定量回归、图像增强和报告生成。代码和预训练模型已公开发布，使EchoCare可用于微调和本地适应，支持扩展到更多应用。EchoCare提供了一个完全开放且可泛化的基础模型，以促进各种临床超声应用中AI技术的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI) that can effectively learn ultrasoundrepresentations by integrating multi-source data holds significant promise foradvancing clinical care. However, the scarcity of large labeled datasets inreal-world clinical environments and the limited generalizability oftask-specific models have hindered the development of generalizable clinical AImodels for ultrasound applications. In this study, we present EchoCare, a novelultrasound foundation model for generalist clinical use, developed viaself-supervised learning on our curated, publicly available, large-scaledataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,sourced from over 23 countries across 5 continents and acquired via a diverserange of distinct imaging devices, thus encompassing global cohorts that aremulti-center, multi-device, and multi-ethnic. Unlike prior studies that adoptoff-the-shelf vision foundation model architectures, we introduce ahierarchical classifier into EchoCare to enable joint learning of pixel-leveland representation-level features, capturing both global anatomical contextsand local ultrasound characteristics. With minimal training, EchoCareoutperforms state-of-the-art comparison models across 10 representativeultrasound benchmarks of varying diagnostic difficulties, spanning diseasediagnosis, lesion segmentation, organ detection, landmark prediction,quantitative regression, imaging enhancement and report generation. The codeand pretrained model are publicly released, rendering EchoCare accessible forfine-tuning and local adaptation, supporting extensibility to additionalapplications. EchoCare provides a fully open and generalizable foundation modelto boost the development of AI technologies for diverse clinical ultrasoundapplications.</description>
      <author>example@mail.com (Hongyuan Zhang, Yuheng Wu, Mingyang Zhao, Zhiwei Chen, Rebecca Li, Fei Zhu, Haohan Zhao, Xiaohua Yuan, Meng Yang, Chunli Qiu, Xiang Cong, Haiyan Chen, Lina Luan, Randolph H. L. Wong, Huai Liao, Colin A Graham, Shi Chang, Guowei Tao, Dong Yi, Zhen Lei, Nassir Navab, Sebastien Ourselin, Jiebo Luo, Hongbin Liu, Gaofeng Meng)</author>
      <guid isPermaLink="false">2509.11752v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>DRAG: Data Reconstruction Attack using Guided Diffusion</title>
      <link>http://arxiv.org/abs/2509.11724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于引导扩散的新型数据重建攻击方法，用于评估大型基础模型在分割推理环境下的隐私风险。&lt;h4&gt;背景&lt;/h4&gt;随着大型基础模型的兴起，分割推理已成为一种流行的计算范式，用于在轻量级边缘设备和云服务器之间部署模型，以解决数据隐私和计算成本问题。然而，大多数现有的数据重建攻击都集中在较小的CNN分类模型上，而基础模型在SI环境下的隐私风险在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一研究空白，论文旨在提出一种新型的数据重建攻击方法，以评估大型基础模型在SI环境下的隐私风险。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一种基于引导扩散的数据重建攻击方法，该方法利用在大型数据集上预训练的潜在扩散模型(LDM)中嵌入的丰富先验知识。该方法在LDM学习到的图像先验上进行迭代重建，能够从中间表示(IR)有效地生成高保真度图像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在从视觉基础模型的深层中间表示重建数据方面，该方法在定性和定量上都显著优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了在SI场景中需要为大型模型开发更强大的隐私保护机制。&lt;h4&gt;翻译&lt;/h4&gt;随着大型基础模型的兴起，分割推理已成为一种流行的计算范式，用于在轻量级边缘设备和云服务器之间部署模型，以解决数据隐私和计算成本问题。然而，大多数现有的数据重建攻击都集中在较小的CNN分类模型上，而基础模型在SI环境下的隐私风险在很大程度上尚未被探索。为了解决这一研究空白，我们提出了一种基于引导扩散的新型数据重建攻击方法，该方法利用在大型数据集上预训练的潜在扩散模型(LDM)中嵌入的丰富先验知识。我们的方法在LDM学习到的图像先验上进行迭代重建，能够有效地从中间表示(IR)生成高保真度图像，这些图像类似于原始数据。大量实验表明，在从视觉基础模型的深层中间表示重建数据方面，我们的方法在定性和定量上都显著优于最先进的方法。研究结果强调了在SI场景中需要为大型模型开发更强大的隐私保护机制。代码可在以下网址获取：https://github.com/ntuaislab/DRAG。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of large foundation models, split inference (SI) has emerged asa popular computational paradigm for deploying models across lightweight edgedevices and cloud servers, addressing data privacy and computational costconcerns. However, most existing data reconstruction attacks have focused onsmaller CNN classification models, leaving the privacy risks of foundationmodels in SI settings largely unexplored. To address this gap, we propose anovel data reconstruction attack based on guided diffusion, which leverages therich prior knowledge embedded in a latent diffusion model (LDM) pre-trained ona large-scale dataset. Our method performs iterative reconstruction on theLDM's learned image prior, effectively generating high-fidelity imagesresembling the original data from their intermediate representations (IR).Extensive experiments demonstrate that our approach significantly outperformsstate-of-the-art methods, both qualitatively and quantitatively, inreconstructing data from deep-layer IRs of the vision foundation model. Theresults highlight the urgent need for more robust privacy protection mechanismsfor large models in SI scenarios. Code is available at:https://github.com/ntuaislab/DRAG.</description>
      <author>example@mail.com (Wa-Kin Lei, Jun-Cheng Chen, Shang-Tse Chen)</author>
      <guid isPermaLink="false">2509.11724v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity with Pretrained Transformers (TabPFN) and Mamba-Based Models</title>
      <link>http://arxiv.org/abs/2509.11449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the author's preprint version of a paper accepted for  presentation at the 24th International Conference on Machine Learning and  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final  published version will appear in the official IEEE proceedings. Conference  site: https://www.icmla-conference.org/icmla25/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种深度表格学习框架，使用德州2017-2023年的真实碰撞数据预测电动汽车碰撞的严重程度。&lt;h4&gt;背景&lt;/h4&gt;研究使用德州(2017-2023)的真实碰撞数据，筛选后分析了23,301起电动汽车碰撞记录。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度表格学习框架来预测电动汽车碰撞的严重程度，并评估不同模型的性能。&lt;h4&gt;方法&lt;/h4&gt;使用XGBoost和Random Forest进行特征重要性分析，识别关键预测因素；应用SMOTEENN重新采样技术解决类别不平衡问题；比较TabPFN、MambaNet和MambaAttention三种深度表格模型。&lt;h4&gt;主要发现&lt;/h4&gt;TabPFN表现出强大的泛化能力，而MambaAttention在严重伤害案例分类中表现更优，归因于其基于注意力的特征重加权机制。&lt;h4&gt;结论&lt;/h4&gt;深度表格架构在改善碰撞严重程度预测方面具有潜力，能够在电动汽车碰撞背景下实现数据驱动的安全干预。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种深度表格学习框架，使用德州(2017-2023)的真实碰撞数据预测电动汽车碰撞的严重程度。在筛选出纯电动汽车后，分析了23,301起电动汽车碰撞记录。使用XGBoost和随机森林的特征重要性技术确定了交叉路口关系、首次有害事件、人员年龄、碰撞限速和星期几为主要预测因素，以及自动紧急制动等先进安全功能。为解决类别不平衡问题，应用了合成少数过采样技术和编辑最近邻(SMOTEENN)重新采样。对三种最先进的深度表格模型TabPFN、MambaNet和MambaAttention进行了严重程度预测的基准测试。虽然TabPFN表现出强大的泛化能力，但MambaAttention由于其基于注意力的特征重加权，在严重伤害案例分类中取得了优越的性能。这些发现强调了深度表格架构在改善碰撞严重程度预测方面的潜力，并能够在电动汽车碰撞背景下实现数据驱动的安全干预。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a deep tabular learning framework for predicting crashseverity in electric vehicle (EV) collisions using real-world crash data fromTexas (2017-2023). After filtering for electric-only vehicles, 23,301EV-involved crash records were analyzed. Feature importance techniques usingXGBoost and Random Forest identified intersection relation, first harmfulevent, person age, crash speed limit, and day of week as the top predictors,along with advanced safety features like automatic emergency braking. Toaddress class imbalance, Synthetic Minority Over-sampling Technique and EditedNearest Neighbors (SMOTEENN) resampling was applied. Three state-of-the-artdeep tabular models, TabPFN, MambaNet, and MambaAttention, were benchmarked forseverity prediction. While TabPFN demonstrated strong generalization,MambaAttention achieved superior performance in classifying severe injury casesdue to its attention-based feature reweighting. The findings highlight thepotential of deep tabular architectures for improving crash severity predictionand enabling data-driven safety interventions in EV crash contexts.</description>
      <author>example@mail.com (Shriyank Somvanshi, Pavan Hebli, Gaurab Chhetri, Subasish Das)</author>
      <guid isPermaLink="false">2509.11449v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Geometric Priors for Unaligned Scene Change Detection</title>
      <link>http://arxiv.org/abs/2509.11292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于几何先验的未对齐场景变化检测方法，解决了当前方法在大视角变化下匹配漂移或失败的问题。&lt;h4&gt;背景&lt;/h4&gt;未对齐场景变化检测旨在检测不同时间拍摄且没有假设视点对齐的图像对之间的场景变化。当前方法仅依赖2D视觉线索建立跨图像对应关系来辅助变化检测，但大视角变化会改变视觉观测，导致基于外观的匹配漂移或失败。&lt;h4&gt;目的&lt;/h4&gt;利用几何基础模型的几何先验来解决未对齐场景变化检测的核心挑战，包括可靠识别视觉重叠、稳健建立对应关系和明确遮挡检测。&lt;h4&gt;方法&lt;/h4&gt;提出一个无需训练的框架，将几何先验与视觉基础模型的强大表示相结合，实现视点不对齐情况下的可靠变化检测。&lt;h4&gt;主要发现&lt;/h4&gt;在PSCD、ChangeSim和PASLCD数据集上的广泛评估表明，该方法实现了优越且稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入几何先验，解决了未对齐场景变化检测中缺乏显式几何推理的关键且被忽视的限制。&lt;h4&gt;翻译&lt;/h4&gt;未对齐场景变化检测旨在检测在不同时间拍摄且不假设视点对齐的图像对之间的场景变化。为处理视角变化，当前方法仅依赖2D视觉线索建立跨图像对应关系以辅助变化检测。然而，大视角变化会改变视觉观测，导致基于外观的匹配漂移或失败。此外，监督仅限于来自小规模SCD数据集的2D变化掩码，限制了可泛化的多视图知识的学习，使得难以可靠识别视觉重叠和处理遮挡。这种缺乏显式几何推理代表了关键但被忽视的限制。在这项工作中，我们首次利用几何基础模型的几何先验来解决未对齐SCD的核心挑战，包括可靠识别视觉重叠、稳健建立对应关系和明确遮挡检测。基于这些先验，我们提出一个无需训练的框架，将其与视觉基础模型的强大表示相结合，实现视点不对齐情况下的可靠变化检测。通过在PSCD、ChangeSim和PASLCD数据集上的广泛评估，我们证明了我们的方法实现了优越且稳健的性能。我们的代码将在https://github.com/ZilingLiu/GeoSCD发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决'未对齐场景变化检测'问题，即在不同时间、不同视点拍摄的图像对之间检测场景变化。这个问题在现实中很重要，因为自动驾驶、无人机和移动机器人等应用中，图像往往因传感器位置、运动或环境因素而存在视点差异。现有方法大多假设视点对齐，限制了实际应用；同时，大视点变化下仅依赖2D视觉线索的方法难以建立可靠对应关系，导致性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析现有方法的局限性：它们仅依赖2D视觉线索，在大视点变化下易出现匹配漂移或失败；且受限于小规模数据集，难以学习多视图可泛化知识。作者借鉴了几何基础模型(GFM)从多视图图像恢复3D几何的能力，以及视觉基础模型SAM的零样本特性。基于这些，作者设计了两阶段框架：先用GFM建立几何理解，再引导变化掩码预测，无需训练即可实现跨数据集泛化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用几何基础模型提供的3D几何先验来处理视点未对齐问题。整体流程分为两阶段：1)几何先验生成模块：用GFM恢复深度图和相机参数，建立像素级对应关系，识别视觉重叠区域并检测遮挡；2)几何引导变化掩码预测模块：将图像输入SAM提取特征，基于几何对应关系生成初始变化提案，用遮挡掩码精炼，与SAM分割掩码匹配后融合得到最终结果。此外，还包含预处理步骤减轻光照变化影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次引入GFM几何先验解决未对齐SCD的核心挑战(识别重叠、建立对应、检测遮挡)；2)提出无需训练的框架，集成几何先验与视觉基础模型，消除对大规模标注数据的需求；3)在多个数据集上验证了方法的优越性和鲁棒性。相比之前工作：不同于传统视点对齐方法和仅依赖2D线索的未对齐方法，本文利用3D几何信息建立更可靠的对应关系；不同于其他未对齐方法(如光流、特征相关)，本文方法能明确检测遮挡且无需训练；相比零样本方法，本文能有效处理大视点变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次利用几何基础模型的几何先验，提出了一种无需训练的框架，有效解决了视点未对齐场景变化检测中的核心挑战，实现了在各种视点变化和场景类型下的鲁棒变化检测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unaligned Scene Change Detection aims to detect scene changes between imagepairs captured at different times without assuming viewpoint alignment. Tohandle viewpoint variations, current methods rely solely on 2D visual cues toestablish cross-image correspondence to assist change detection. However, largeviewpoint changes can alter visual observations, causing appearance-basedmatching to drift or fail. Additionally, supervision limited to 2D change masksfrom small-scale SCD datasets restricts the learning of generalizablemulti-view knowledge, making it difficult to reliably identify visual overlapsand handle occlusions. This lack of explicit geometric reasoning represents acritical yet overlooked limitation. In this work, we are the first to leveragegeometric priors from a Geometric Foundation Model to address the corechallenges of unaligned SCD, including reliable identification of visualoverlaps, robust correspondence establishment, and explicit occlusiondetection. Building on these priors, we propose a training-free framework thatintegrates them with the powerful representations of a visual foundation modelto enable reliable change detection under viewpoint misalignment. Throughextensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, wedemonstrate that our approach achieves superior and robust performance. Ourcode will be released at https://github.com/ZilingLiu/GeoSCD.</description>
      <author>example@mail.com (Ziling Liu, Ziwei Chen, Mingqi Gao, Jinyu Yang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.11292v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation</title>
      <link>http://arxiv.org/abs/2509.11197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DreamNav是一种新的零样本视觉语言导航方法，通过改进感知、规划和预测能力，解决了现有方法依赖昂贵感知和被动理解的问题，实现了更高效、更语义一致的导航。&lt;h4&gt;背景&lt;/h4&gt;Vision-and-Language Navigation in Continuous Environments (VLN-CE)是具身机器人的核心能力，将语言指令与现实世界的感知和控制联系起来。大规模预训练基础模型被用作感知、推理和动作的共享先验，实现了无需任务特定训练的零样本VLN。&lt;h4&gt;目的&lt;/h4&gt;解决现有零样本VLN方法依赖昂贵感知和被动场景理解、控制简化为点级选择、部署成本高、动作语义不一致、规划缺乏前瞻性的问题。&lt;h4&gt;方法&lt;/h4&gt;DreamNav专注于三个方面：1) EgoView Corrector减少感知成本，对齐视角并稳定以自我为中心的感知；2) Trajectory Predictor替代点级动作，倾向于全局轨迹级规划，更好地与指令语义对齐；3) Imagination Predictor实现前瞻性和长期规划，赋予代理主动思考能力。&lt;h4&gt;主要发现&lt;/h4&gt;在VLN-CE和真实世界测试中，DreamNav建立了新的零样本最先进水平，在SR和SPL指标上分别比最强的以自我为中心的基线高出7.49%和18.15%。&lt;h4&gt;结论&lt;/h4&gt;DreamNav是第一个统一轨迹级规划和主动想象并仅使用以自我为中心输入的零样本VLN方法，显著提升了性能并降低了部署成本。&lt;h4&gt;翻译&lt;/h4&gt;Vision-and-Language Navigation in Continuous Environments (VLN-CE)将语言指令与现实世界中的感知和控制联系起来，是具身机器人的核心能力。最近，大规模预训练基础模型被用作感知、推理和动作的共享先验，实现了无需任务特定训练的零样本VLN。然而，现有零样本VLN方法依赖于昂贵的感知和被动场景理解，导致控制简化为点级选择。因此，这些方法部署成本高，动作语义不一致，且规划缺乏前瞻性。为解决这些问题，我们提出了DreamNav，专注于以下三个方面：(1)为减少感知成本，我们的EgoView Corrector对齐视角并稳定以自我为中心的感知；(2)替代点级动作，我们的Trajectory Predictor倾向于全局轨迹级规划，更好地与指令语义对齐；(3)为实现前瞻性和长期规划，我们提出了Imagination Predictor，赋予代理主动思考能力。在VLN-CE和真实世界测试中，DreamNav建立了新的零样本最先进水平，在SR和SPL指标上分别比最强的以自我为中心的基线高出7.49%和18.15%。据我们所知，这是第一个统一轨迹级规划和主动想象并仅使用以自我为中心输入的零样本VLN方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是零样本视觉语言导航(VLN-CE)中的三个核心问题：高成本感知、短视规划和语义不匹配。这些问题很重要，因为VLN-CE是具身机器人的关键能力，能让机器人根据语言指令在真实环境中导航。高成本感知限制了实际应用，短视规划导致机器人缺乏长远视野，语义不匹配则使导航决策与人类意图不符，这些都阻碍了机器人在复杂现实环境中的有效导航。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类导航行为中获得灵感，人类会构建连贯轨迹并主动想象未来场景。他们将问题分解为感知成本、规划前瞻性和语义对齐三个方面，针对性地设计解决方案。作者借鉴了扩散策略框架在机器人操作中的应用，利用预训练多模态模型作为认知核心，采用可控世界模型进行视觉预测，并借鉴FastSAM等技术用于可行走区域检测。整体设计是一个分层框架，包含视角校正、轨迹预测、想象预测和导航管理四个主要组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过轨迹级规划和主动想象模仿人类导航行为，构建连贯轨迹并预见未来场景，而非被动理解和反应。整体流程：1)接收第一人称RGB-D观察和语言指令；2)EgoView Corrector校正视角方向；3)Trajectory Predictor生成候选轨迹并过滤；4)Imagination Predictor将候选轨迹转换为未来场景的语义描述；5)Navigation Manager选择最佳轨迹并监控执行；6)系统持续调整和优化导航策略，形成闭环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)EgoView Corrector解决第一人称视角的方向错误，仅用低成本输入；2)Trajectory Predictor采用轨迹级而非点级决策；3)Imagination Predictor将被动理解转为主动想象；4)统一框架整合轨迹规划和主动想象。相比之前工作，DreamNav摒弃了高成本的全景感知，避免了点级决策的短视性，通过文本而非像素级输出降低成本，并在真实测试中实现了SR和SPL指标分别提升7.49%和18.15%的性能突破。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DreamNav首次统一了轨迹级规划和主动想象，仅使用第一人称输入就在零样本视觉语言导航中实现了最先进性能，解决了现有方法的高成本、短视和语义不匹配问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-Language Navigation in Continuous Environments (VLN-CE), whichlinks language instructions to perception and control in the real world, is acore capability of embodied robots. Recently, large-scale pretrained foundationmodels have been leveraged as shared priors for perception, reasoning, andaction, enabling zero-shot VLN without task-specific training. However,existing zero-shot VLN methods depend on costly perception and passive sceneunderstanding, collapsing control to point-level choices. As a result, they areexpensive to deploy, misaligned in action semantics, and short-sighted inplanning. To address these issues, we present DreamNav that focuses on thefollowing three aspects: (1) for reducing sensory cost, our EgoView Correctoraligns viewpoints and stabilizes egocentric perception; (2) instead ofpoint-level actions, our Trajectory Predictor favors global trajectory-levelplanning to better align with instruction semantics; and (3) to enableanticipatory and long-horizon planning, we propose an Imagination Predictor toendow the agent with proactive thinking capability. On VLN-CE and real-worldtests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming thestrongest egocentric baseline with extra information by up to 7.49\% and18.15\% in terms of SR and SPL metrics. To our knowledge, this is the firstzero-shot VLN method to unify trajectory-level planning and active imaginationwhile using only egocentric inputs.</description>
      <author>example@mail.com (Yunheng Wang, Yuetong Fang, Taowen Wang, Yixiao Feng, Yawen Tan, Shuning Zhang, Peiran Liu, Yiding Ji, Renjing Xu)</author>
      <guid isPermaLink="false">2509.11197v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos</title>
      <link>http://arxiv.org/abs/2509.11063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了Organoid Tracker平台，基于SAM2模型实现肾类器官时空显微镜视频的自动分析和定量评估，无需编程专业知识即可获取详细的定量指标，为肾脏疾病研究和药物发现提供强大工具。&lt;h4&gt;背景&lt;/h4&gt;类器官模型的最新进展改变了人类肾脏疾病机制研究和药物发现的方式，使研究可扩展、经济高效且无需牺牲动物。&lt;h4&gt;目的&lt;/h4&gt;开发一种优化的肾脏类器官平台，用于多囊肾病(PKD)的高效筛选，并解决现有手动分析方法仅限于粗略分类的问题。&lt;h4&gt;方法&lt;/h4&gt;开发了Organoid Tracker，一个基于图形用户界面(GUI)的平台，采用模块化插件架构，基于Segment Anything Model 2 (SAM2)实现零样本分割和时空显微镜视频的自动分析。&lt;h4&gt;主要发现&lt;/h4&gt;Organoid Tracker可以量化关键指标，如囊肿形成率、生长速度和形态变化，同时生成综合报告，提供有价值的像素级和纵向信息。&lt;h4&gt;结论&lt;/h4&gt;Organoid Tracker提供了一个可扩展的开源框架，为改善和加速肾脏发育、PKD建模和治疗发现研究提供了强大解决方案，平台已作为开源软件公开可用。&lt;h4&gt;翻译&lt;/h4&gt;类器官模型的最新进展通过实现可扩展、经济高效且无需牺牲动物的研究，彻底改变了人类肾脏疾病机制研究和药物发现。我们在此介绍了一种为多囊肾病(PKD)高效筛选而优化的肾脏类器官平台。虽然这些系统生成丰富的时空显微镜视频数据集，但当前的手工分析方法仍仅限于粗略分类（如命中与非命中），常常遗漏有价值的像素级和纵向信息。为帮助克服这一瓶颈，我们开发了Organoid Tracker，一个采用模块化插件架构设计的图形用户界面(GUI)平台，使研究人员无需编程专业知识即可提取详细的定量指标。基于前沿视觉基础模型Segment Anything Model 2 (SAM2)构建，Organoid Tracker实现了时空显微镜视频的零样本分割和自动分析。它量化了囊肿形成率、生长速度和形态变化等关键指标，同时生成综合报告。通过提供可扩展的开源框架，Organoid Tracker为改善和加速肾脏发育、PKD建模和治疗发现研究提供了强大解决方案。该平台作为开源软件可在https://github.com/hrlblab/OrganoidTracker公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in organoid models have revolutionized the study of humankidney disease mechanisms and drug discovery by enabling scalable,cost-effective research without the need for animal sacrifice. Here, we presenta kidney organoid platform optimized for efficient screening in polycystickidney disease (PKD). While these systems generate rich spatial-temporalmicroscopy video datasets, current manual approaches to analysis remain limitedto coarse classifications (e.g., hit vs. non-hit), often missing valuablepixel-level and longitudinal information. To help overcome this bottleneck, wedeveloped Organoid Tracker, a graphical user interface (GUI) platform designedwith a modular plugin architecture, which empowers researchers to extractdetailed, quantitative metrics without programming expertise. Built on thecutting-edge vision foundation model Segment Anything Model 2 (SAM2), OrganoidTracker enables zero-shot segmentation and automated analysis ofspatial-temporal microscopy videos. It quantifies key metrics such as cystformation rate, growth velocity, and morphological changes, while generatingcomprehensive reports. By providing an extensible, open-source framework,Organoid Tracker offers a powerful solution for improving and acceleratingresearch in kidney development, PKD modeling, and therapeutic discovery. Theplatform is publicly available as open-source software athttps://github.com/hrlblab/OrganoidTracker.</description>
      <author>example@mail.com (Xiaoyu Huang, Lauren M Maxson, Trang Nguyen, Cheng Jack Song, Yuankai Huo)</author>
      <guid isPermaLink="false">2509.11063v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Deep Reinforcement Learning-Assisted Component Auto-Configuration of Differential Evolution Algorithm for Constrained Optimization: A Foundation Model</title>
      <link>http://arxiv.org/abs/2509.11016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SuperDE的新型框架，利用深度强化学习自动配置差分进化算法的组件，以解决约束优化问题。SuperDE作为一个基础模型，能够根据实时进化动态调整算法配置，并通过元学习实现零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;尽管人们努力设计高性能的进化算法，但由于现实问题的动态性和不断演变的特性，它们的适应性仍然有限。'无免费午餐'定理表明没有单一算法能在所有问题上都表现最佳。现有的在线适应方法通常存在效率低下、收敛性弱以及在约束优化问题上的泛化能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，作者引入了一个新的框架，用于在差分进化算法中自动化组件配置，以处理约束优化问题，该框架由深度强化学习提供支持。&lt;h4&gt;方法&lt;/h4&gt;作者提出了SuperDE，这是一个基础模型，可以根据实时进化动态配置差分进化算法的进化组件。通过元学习在各种约束优化问题上进行离线训练，SuperDE能够以零样本的方式为未见问题推荐每代的最优配置。利用双深度Q网络，SuperDE能够根据优化过程中不断变化的种群状态调整其配置策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SuperDE在基准测试套件上显著优于现有的最先进算法，实现了卓越的泛化和优化性能。&lt;h4&gt;结论&lt;/h4&gt;SuperDE通过结合深度强化学习和差分进化算法，为解决约束优化问题提供了一种有效的方法，能够动态调整算法参数以适应不同的问题特性。&lt;h4&gt;翻译&lt;/h4&gt;尽管人们付出了巨大努力来手动设计高性能的进化算法，但由于现实问题的动态和不断演变的特性，它们的适应性仍然有限。'无免费午餐'定理强调，没有单一算法能在所有问题上都表现最优。虽然已经提出了在线适应方法，但它们通常效率低下、收敛性弱，并且在约束优化问题上泛化能力有限。为了解决这些挑战，我们引入了一个新的框架，用于在差分进化算法中自动化组件配置以解决约束优化问题，该框架由深度强化驱动。具体来说，我们提出了SuperDE，这是一个基础模型，能够根据实时进化动态配置差分进化的组件。通过在各种约束优化问题上进行元学习离线训练，SuperDE能够以零样本的方式为未见问题推荐每代的最优配置。利用双深度Q网络，SuperDE能够根据优化过程中不断变化的种群状态调整其配置策略。实验结果表明，SuperDE在基准测试套件上显著优于现有的最先进算法，实现了卓越的泛化和优化性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant efforts to manually design high-performance evolutionaryalgorithms, their adaptability remains limited due to the dynamic andever-evolving nature of real-world problems. The "no free lunch" theoremhighlights that no single algorithm performs optimally across all problems.While online adaptation methods have been proposed, they often suffer frominefficiency, weak convergence, and limited generalization on constrainedoptimization problems (COPs).  To address these challenges, we introduce a novel framework for automatedcomponent configuration in Differential Evolution (DE) algorithm to addressCOPs, powered by Deep Reinforcement Learning (DRL). Specifically, we proposeSuperDE, a foundation model that dynamically configures DE's evolutionarycomponents based on real-time evolution. Trained offline through meta-learningacross a wide variety of COPs, SuperDE is capable of recommending optimalper-generation configurations for unseen problems in a zero-shot manner.Utilizing a Double Deep Q-Network (DDQN), SuperDE adapts its configurationstrategies in response to the evolving population states during optimization.Experimental results demonstrate that SuperDE significantly outperformsexisting state-of-the-art algorithms on benchmark test suites, achievingsuperior generalization and optimization performance.</description>
      <author>example@mail.com (Xu Yang, Rui Wang, Kaiwen Li, Wenhua Li, Ling Wang)</author>
      <guid isPermaLink="false">2509.11016v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation</title>
      <link>http://arxiv.org/abs/2509.10919v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种紧凑的元数据感知专家混合掩码自编码器（MoE-MAE），该模型仅具有250万参数，但性能可与更大规模模型相媲美，证明了元数据感知预训练在地球观测领域的有效性。&lt;h4&gt;背景&lt;/h4&gt;地球观测领域最近的发展集中在大型基础模型上，但这些模型计算成本高，限制了其在下游任务中的可访问性和重用性。&lt;h4&gt;目的&lt;/h4&gt;研究紧凑架构作为实现小型通用地球观测模型的实用途径。&lt;h4&gt;方法&lt;/h4&gt;提出了一个只有250万参数的元数据感知专家混合掩码自编码器（MoE-MAE）。该模型结合了稀疏专家路由与地理时间条件，同时结合了图像数据以及纬度/经度和季节/日循环编码。研究者在BigEarthNet-Landsat数据集上预训练MoE-MAE，并使用线性探针评估其冻结编码器的嵌入结果。&lt;h4&gt;主要发现&lt;/h4&gt;尽管模型规模小，但它能够与更大的架构竞争，这表明元数据感知预训可以提高迁移效率和标签效率。在缺乏显式元数据的EuroSAT-Landsat数据集上的评估显示，与拥有数亿参数的模型相比，仍然观察到具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;紧凑的、元数据感知的MoE-MAE是未来地球观测基础模型的高效且可扩展的一步。&lt;h4&gt;翻译&lt;/h4&gt;最近的地球观测进展主要集中在大型基础模型上。然而，这些模型计算成本高昂，限制了它们在下游任务中的可访问性和重用。在这项工作中，我们研究紧凑架构作为迈向小型通用地球观测模型的实用途径。我们提出了一个只有250万参数的元数据感知专家混合掩码自编码器（MoE-MAE）。该模型结合了稀疏专家路由与地理时间条件，整合了图像数据以及纬度/经度和季节/日循环编码。我们在BigEarthNet-Landsat数据集上预训练MoE-MAE，并使用线性探针评估其冻结编码器的嵌入结果。尽管规模小，该模型能与更大的架构竞争，这表明元数据感知预训练提高了迁移效率和标签效率。为了进一步评估泛化能力，我们在缺乏显式元数据的EuroSAT-Landsat数据集上进行了评估，与拥有数亿参数的模型相比，仍然观察到具有竞争力的性能。这些结果表明，紧凑的、元数据感知的MoE-MAE是未来地球观测基础模型的高效且可扩展的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Earth Observation have focused on large-scale foundationmodels. However, these models are computationally expensive, limiting theiraccessibility and reuse for downstream tasks. In this work, we investigatecompact architectures as a practical pathway toward smaller general-purpose EOmodels. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routingwith geo-temporal conditioning, incorporating imagery alongsidelatitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAEon the BigEarthNet-Landsat dataset and evaluate embeddings from its frozenencoder using linear probes. Despite its small size, the model competes withmuch larger architectures, demonstrating that metadata-aware pretrainingimproves transfer and label efficiency. To further assess generalization, weevaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, andstill observe competitive performance compared to models with hundreds ofmillions of parameters. These results suggest that compact, metadata-awareMoE-MAEs are an efficient and scalable step toward future EO foundation models.</description>
      <author>example@mail.com (Mohanad Albughdadi)</author>
      <guid isPermaLink="false">2509.10919v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Nav-R1: Reasoning and Navigation in Embodied Scenes</title>
      <link>http://arxiv.org/abs/2509.10884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Nav-R1，一个用于具身导航的基础模型，通过统一推理框架解决了现有方法在推理轨迹不连贯、不稳定以及难以平衡长时程语义推理与低延迟控制方面的问题。&lt;h4&gt;背景&lt;/h4&gt;具身导航需要智能体在复杂3D环境中整合感知、推理和行动，但现有方法常因推理轨迹不连贯和不稳定而影响泛化能力，且难以平衡长时程语义推理与低延迟控制以实现实时导航。&lt;h4&gt;目的&lt;/h4&gt;解决具身导航中推理不连贯、不稳定以及语义推理与控制平衡困难的问题，提高智能体在不同环境中的导航性能。&lt;h4&gt;方法&lt;/h4&gt;1) 构建Nav-CoT-110K大规模分步思维链数据集实现冷启动初始化；2) 设计基于GRPO的强化学习框架，包含格式、理解和导航三种互补奖励；3) 引入'Fast-in-Slow'推理范式，将 deliberative 语义推理与低延迟反应控制解耦。&lt;h4&gt;主要发现&lt;/h4&gt;Nav-R1在具身AI基准测试中显著优于基线模型，推理和导航性能平均提升超过8%；在移动机器人上的实际部署验证了其在有限机载资源下的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Nav-R1通过统一推理框架和创新的训练方法，有效解决了具身导航中的关键挑战，实现了高效且连贯的导航性能。&lt;h4&gt;翻译&lt;/h4&gt;具身导航需要智能体在复杂3D环境中整合感知、推理和行动以实现稳健的交互。现有方法常因推理轨迹不连贯和不稳定而阻碍了跨不同环境的泛化能力，并且难以平衡长时程语义推理与低延迟控制以实现实时导航。为应对这些挑战，我们提出Nav-R1，一个统一具身环境中推理的基础模型。我们首先构建Nav-CoT-110K，一个用于具身任务的大规模分步思维链数据集，实现了具有结构化推理的冷启动初始化。在此基础上，我们设计了一个基于GRPO的强化学习框架，包含格式、理解和导航三种互补奖励，以提高结构遵循性、语义基础和路径保真度。此外，我们引入了'Fast-in-Slow'推理范式，将 deliberative 语义推理与低延迟反应控制解耦，实现高效且连贯的导航。在具身AI基准上的广泛评估表明，Nav-R1始终优于强大的基线模型，推理和导航性能平均提升超过8%。在移动机器人上的实际部署进一步验证了其在有限机载资源下的鲁棒性。代码：https://github.com/AIGeeksGroup/Nav-R1。网站：https://aigeeksgroup.github.io/Nav-R1。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决具身导航中的两个关键问题：一是现有方法存在推理轨迹不连贯和不稳定的问题，影响模型在不同环境中的泛化能力；二是难以平衡长距离语义推理与低延迟控制以实现实时导航。这些问题在现实中非常重要，因为具身导航是服务机器人、增强现实助手等智能系统在复杂3D环境中执行物体搜索、指令跟随等任务的核心能力，直接关系到这些系统在现实世界中的实用性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后设计了一个分阶段的解决方案：首先构建Nav-CoT-110K数据集进行冷启动初始化，然后使用基于GRPO的强化学习框架进行优化，最后提出Fast-in-Slow双系统推理范式。该方法借鉴了人类认知科学中的双系统理论（快速直觉系统1和慢速理性系统2），以及大型语言模型中常用的Chain-of-Thought推理技术和GRPO强化学习方法，同时整合了多个现有3D视觉语言数据集来构建新的高质量数据集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将语义推理与实时导航控制解耦，通过双系统设计实现高效且连贯的导航，同时使用结构化的思维链数据确保推理质量。整体实现流程分为四个阶段：1) 使用CoT数据引擎构建Nav-CoT-110K数据集，通过视觉语言模型生成结构化推理轨迹并过滤；2) 冷启动阶段，使用该数据集进行监督微调，初始化模型生成结构化推理-行动序列；3) 强化学习阶段，应用三种互补奖励函数（格式、理解和导航奖励）进行GRPO优化；4) 实现Fast-in-Slow双系统，慢系统处理长期语义推理，快系统执行短期反应控制，两者异步协调工作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) Nav-CoT-110K大型数据集，提供结构化思维链轨迹；2) 三种互补奖励机制，平衡结构一致性、语义理解和导航准确性；3) Fast-in-Slow双系统推理范式，解耦长期推理与短期控制；4) 结合模拟训练与真实世界部署。相比之前工作，Nav-R1不将语义推理和导航视为分离问题，而是在统一框架中解决；双系统设计比单一系统更好地平衡了语义连贯性和实时响应；多奖励机制提供了更全面的训练信号；结合真实世界数据提高了模型泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Nav-R1通过结构化思维链数据、多奖励强化学习和双系统推理范式，显著提升了具身导航中的推理连贯性和实时导航性能，实现了语义理解与行动执行的有效平衡。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied navigation requires agents to integrate perception, reasoning, andaction for robust interaction in complex 3D environments. Existing approachesoften suffer from incoherent and unstable reasoning traces that hindergeneralization across diverse environments, and difficulty balancinglong-horizon semantic reasoning with low-latency control for real-timenavigation. To address these challenges, we propose Nav-R1, an embodiedfoundation model that unifies reasoning in embodied environments. We firstconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought(CoT) for embodied tasks, which enables cold-start initialization withstructured reasoning. Building on this foundation, we design a GRPO-basedreinforcement learning framework with three complementary rewards: format,understanding, and navigation, to improve structural adherence, semanticgrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slowreasoning paradigm, decoupling deliberate semantic reasoning from low-latencyreactive control for efficient yet coherent navigation. Extensive evaluationson embodied AI benchmarks demonstrate that Nav-R1 consistently outperformsstrong baselines, with over 8% average improvement in reasoning and navigationperformance. Real-world deployment on a mobile robot further validates itsrobustness under limited onboard resources. Code:https://github.com/AIGeeksGroup/Nav-R1. Website:https://aigeeksgroup.github.io/Nav-R1.</description>
      <author>example@mail.com (Qingxiang Liu, Ting Huang, Zeyu Zhang, Hao Tang)</author>
      <guid isPermaLink="false">2509.10884v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research</title>
      <link>http://arxiv.org/abs/2509.10790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Transformer已成为多种先进模型的基础，但其故障条件下的鲁棒性研究不足。GoldenTransformer是一个模块化且可扩展的故障注入框架，用于评估大型语言模型对硬件故障的恢复能力。&lt;h4&gt;背景&lt;/h4&gt;Transformers在自然语言处理、计算机视觉和机器学习领域广泛应用，但这些模型在故障条件下的鲁棒性研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;开发GoldenTransformer框架，用于评估大型语言模型对诱导硬件故障的恢复能力，并提供一个统一的平台来测试不同类型的故障影响。&lt;h4&gt;方法&lt;/h4&gt;GoldenTransformer是一个基于Python的统一平台，可以向预训练的transformer模型注入多种类型的故障，如权重损坏、激活注入和注意力级别干扰。该框架基于PyTorch和HuggingFace Transformers构建，支持实验可重复性、指标记录和可视化功能。&lt;h4&gt;主要发现&lt;/h4&gt;通过在transformer的多个逻辑和结构点进行受控故障注入，GoldenTransformer能够帮助研究人员和从业者分析模型鲁棒性，指导现实世界LLM应用中的可靠系统设计。&lt;h4&gt;结论&lt;/h4&gt;GoldenTransformer解决了大型transformer架构的独特挑战，包括结构复杂性、潜在依赖性和非均匀层定义等问题，为模型鲁棒性分析提供了重要工具。&lt;h4&gt;翻译&lt;/h4&gt;Transformers已成为自然语言处理、计算机视觉和其他机器学习领域中各种先进模型的基础。尽管它们被广泛部署，但这些模型在故障条件下的鲁棒性仍然研究不足。我们提出了GoldenTransformer，这是一个模块化和可扩展的故障注入框架，旨在评估大型语言模型对诱导硬件故障的恢复能力。GoldenTransformer提供了一个基于Python的统一平台，可以向预训练的基于transformer的模型注入多种类型的故障，如权重损坏、激活注入和注意力级别干扰。受DNN的GoldenEye模拟器启发，我们的框架专注于处理大型transformer架构的独特挑战，包括结构复杂性、潜在依赖性和非均匀层定义等考虑因素。GoldenTransformer基于PyTorch和HuggingFace Transformers构建，开箱即用地支持实验可重复性、指标记录和可视化。我们详细介绍了GoldenTransformer的技术设计和使用方法，并通过分类和生成任务上的几个示例实验进行了演示。通过能够在transformer的多个逻辑和结构点进行受控故障注入，GoldenTransformer为研究人员和从业者提供了有价值的工具，用于模型鲁棒性分析和指导现实世界LLM应用中的可靠系统设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have become the foundation for a wide range ofstate--of--the--art models across natural language processing, computer vision,and other machine learning domains. Despite their widespread deployment, therobustness of these models under fault conditions remains underexplored. Wepresent GoldenTransformer, a modular and extensible fault injection frameworkdesigned to evaluate the resiliency of Large Language Models to inducedhardware faults. GoldenTransformer offers a unified Python-based platform forinjecting diverse classes of faults--such as weight corruption, activationinjections, and attention--level disruptions--into pretrainedtransformer--based models. Inspired by the GoldenEye simulator for DNNs, ourframework focuses on the unique challenges of working with large transformerarchitectures, including considerations such as structural complexity, latentdependencies, and nonuniform layer definitions. GoldenTransformer is built atopPyTorch and HuggingFace Transformers, and it supports experimentreproducibility, metric logging, and visualization out of the box. We detailthe technical design and use of GoldenTransformer and demonstrate throughseveral example experiments on classification and generation tasks. By enablingcontrolled injection of faults at multiple logical and structural points in atransformer, GoldenTransformer offers researchers and practitioners a valuabletool for model robustness analysis and for guiding dependable system design inreal-world LLM applications.</description>
      <author>example@mail.com (Luke Howard)</author>
      <guid isPermaLink="false">2509.10790v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Medical Vision Foundation Models for Volumetric Medical Image Segmentation via Active Learning and Selective Semi-supervised Fine-tuning</title>
      <link>http://arxiv.org/abs/2509.10784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 5 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种主动无源域适应(ASFDA)方法，用于高效地将医学视觉基础模型(Med-VFMs)适应到目标域，用于体积医学图像分割任务。该方法通过主动学习选择最有信息量的样本进行微调，无需访问源预训练样本，从而以最小的选择预算最大化模型性能。&lt;h4&gt;背景&lt;/h4&gt;医学视觉基础模型(Med-VFMs)通过大量未标注图像的自监督预训练学习，具有解释医学图像的优越能力。目前，为了提高它们在适应性下游评估（特别是分割任务）中的性能，通常从目标域随机选择少量样本进行微调。然而，缺乏探索如何高效适应Med-VFMs以达到目标域最佳性能的研究。&lt;h4&gt;目的&lt;/h4&gt;设计一种高效的微调方法，通过选择信息量最大的样本来最大化Med-VFMs在目标域上的适应性能。&lt;h4&gt;方法&lt;/h4&gt;提出主动无源域适应(ASFDA)方法，采用新颖的主动学习策略，通过两个查询指标选择最有信息量的样本：多样化的知识差异(DKD)测量源-目标知识差距和域内多样性，利用预训练知识指导查询源不相似和语义多样的样本；解剖分割难度(ASD)通过自适应测量前景区域的预测熵来评估解剖结构分割的难度。此外，采用选择性半监督微调，通过从未查询的样本中识别高可靠性样本来提高微调的性能和效率。&lt;h4&gt;主要发现&lt;/h4&gt;DKD和ASD两个查询指标能够有效选择最有价值的样本进行微调，提高模型在目标域上的性能，无需访问源预训练样本即可实现最优适应。&lt;h4&gt;结论&lt;/h4&gt;ASFDA方法能够高效地将Med-VFMs适应到目标域，用于体积医学图像分割，通过主动学习选择最有信息量的样本，实现最优性能。&lt;h4&gt;翻译&lt;/h4&gt;医学视觉基础模型(Med-VFMs)由于通过大量未标注图像进行自监督预训练所获得的知识，具有解释医学图像的优越能力。为了提高它们在适应性下游评估中的性能，特别是分割任务，通常从目标域随机选择少量样本进行微调。然而，缺乏探索如何高效适应Med-VFMs以达到目标域最佳性能的研究。因此，迫切需要设计一种高效的微调方法，通过选择信息量最大的样本来最大化Med-VFMs在目标域上的适应性能。为此，我们提出了一种主动无源域适应(ASFDA)方法，用于高效地将Med-VFMs适应到目标域，用于体积医学图像分割。该ASFDA采用一种新颖的主动学习(AL)方法，从目标域中选择信息量最大的样本进行微调，无需访问源预训练样本，从而以最小的选择预算最大化其性能。在该AL方法中，我们设计了一种主动测试时间样本查询策略，通过两个查询指标（包括多样化的知识差异DKD和解剖分割难度ASD）从目标域中选择样本。DKD旨在测量源-目标知识差距和域内多样性，它利用预训练知识指导从目标域中查询源不相似和语义多样的样本。ASD旨在通过自适应测量前景区域的预测熵来评估解剖结构分割的难度。此外，我们的ASFDA方法采用选择性半监督微调，通过从未查询的样本中识别高可靠性样本来提高微调的性能和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical Vision Foundation Models (Med-VFMs) have superior capabilities ofinterpreting medical images due to the knowledge learned from self-supervisedpre-training with extensive unannotated images. To improve their performance onadaptive downstream evaluations, especially segmentation, a few samples fromtarget domains are selected randomly for fine-tuning them. However, there lacksworks to explore the way of adapting Med-VFMs to achieve the optimalperformance on target domains efficiently. Thus, it is highly demanded todesign an efficient way of fine-tuning Med-VFMs by selecting informativesamples to maximize their adaptation performance on target domains. To achievethis, we propose an Active Source-Free Domain Adaptation (ASFDA) method toefficiently adapt Med-VFMs to target domains for volumetric medical imagesegmentation. This ASFDA employs a novel Active Learning (AL) method to selectthe most informative samples from target domains for fine-tuning Med-VFMswithout the access to source pre-training samples, thus maximizing theirperformance with the minimal selection budget. In this AL method, we design anActive Test Time Sample Query strategy to select samples from the targetdomains via two query metrics, including Diversified Knowledge Divergence (DKD)and Anatomical Segmentation Difficulty (ASD). DKD is designed to measure thesource-target knowledge gap and intra-domain diversity. It utilizes theknowledge of pre-training to guide the querying of source-dissimilar andsemantic-diverse samples from the target domains. ASD is designed to evaluatethe difficulty in segmentation of anatomical structures by measuring predictiveentropy from foreground regions adaptively. Additionally, our ASFDA methodemploys a Selective Semi-supervised Fine-tuning to improve the performance andefficiency of fine-tuning by identifying samples with high reliability fromunqueried ones.</description>
      <author>example@mail.com (Jin Yang, Daniel S. Marcus, Aristeidis Sotiras)</author>
      <guid isPermaLink="false">2509.10784v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation</title>
      <link>http://arxiv.org/abs/2509.10748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种语音引导的协同感知(SCOPE)框架，将大型语言模型与开放式视觉基础模型相结合，支持在手术视频中对手术器械和解剖结构进行即时分割、标记和跟踪，实现人机协作的手术场景理解。&lt;h4&gt;背景&lt;/h4&gt;手术场景中准确分割和跟踪相关元素对提供术中辅助和决策支持至关重要。当前解决方案依赖于特定领域的监督模型，需要标记数据，且难以适应新场景和预定义标签类别外的元素。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在术中环境中实现开放式、零样本分割和跟踪的框架，无需依赖手动视觉或文本提示，支持人机协作的手术场景理解。&lt;h4&gt;方法&lt;/h4&gt;提出语音引导的协同感知(SCOPE)框架，整合大型语言模型的推理能力与开放式视觉基础模型的感知能力。框架包含协同感知代理组件，生成分割候选并整合临床医生的语音反馈，实现自然的人机协作。手术器械本身作为交互式指针标记其他手术场景元素。&lt;h4&gt;主要发现&lt;/h4&gt;在Cataract1k数据集子集和自体离体颅底数据集上的评估表明，该框架能够生成手术场景的即时分割和跟踪。实时离体模拟实验进一步验证了其动态能力。&lt;h4&gt;结论&lt;/h4&gt;这种人-AI协作范式展示了开发适应性、免手持、以外科医生为中心的工具用于动态手术室环境的潜力。&lt;h4&gt;翻译&lt;/h4&gt;准确的手术场景相关元素分割和跟踪对于实现情境感知的术中辅助和决策制定至关重要。当前解决方案仍然依赖于特定领域的监督模型，这些模型依赖标记数据，并且需要特定领域的数据来适应新的手术场景和预定义标签类别之外的元素。提示驱动视觉基础模型(VFM)的最新进展使得在异构医学图像上进行开放式、零样本分割成为可能。然而，这些模型对手动视觉或文本提示的依赖限制了其在术中手术环境中的部署。我们引入了一个语音引导的协同感知(SCOPE)框架，该框架将大型语言模型(LLM)的推理能力与开放式VFM的感知能力相结合，支持在术中视频流中对手术器械和解剖结构进行即时分割、标记和跟踪。该框架的一个关键组件是一个协同感知代理，它生成VFM生成的分割候选，并整合临床医生直观的语音反馈，以自然的人机协作方式指导手术器械的分割。之后，手术器械本身作为交互式指针来标记手术场景的其他元素。我们在公开可用的Cataract1k数据集的子集和一个自体离体颅底数据集上评估了我们提出的框架，以证明其在生成手术场景即时分割和跟踪方面的潜力。此外，我们通过一个实时的离体模拟实验展示了其动态能力。这种人-AI协作范式展示了开发适应性、免手持、以外科医生为中心的工具用于动态手术室环境的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决手术场景中实时、准确分割和跟踪手术器械及解剖结构的问题。传统方法依赖标记数据和特定领域模型，难以适应新场景；现有视觉模型虽强大但需手动操作，不适合无菌手术环境。此问题对实现术中情境感知辅助和决策制定至关重要，能提高手术安全性和效率，增强医生在动态手术环境中的应变能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有监督学习难以泛化、手动交互不适合手术环境的问题。借鉴了视觉基础模型(SAM, GSAM)的分割能力和大型语言模型(GPT-4)的理解能力，参考了Visual ChatGPT的多模态交互范式但针对手术环境优化。设计了模块化交互流程，结合语音交互、协作感知代理和虚拟指针技术，实现免提操作和自然人机协作，无需针对特定手术数据进行微调。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合大型语言模型的自然语言理解能力和视觉基础模型的感知能力，通过语音引导实现人机协作，实时分割和跟踪手术场景元素。流程包括：1)语音输入处理；2)查询扩展与掩码生成；3)医生语音选择掩码并分配标签；4)使用视频分割模型跟踪器械；5)通过器械尖端作为虚拟指针分割解剖结构；6)持续交互优化结果，形成完整闭环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)语音引导的免提交互方式；2)人机协作感知框架；3)器械尖端作为虚拟指针的技术；4)模块化交互设计；5)零样本泛化能力。相比之前工作，SCOPE摒弃了手动交互方式，专为动态手术视频设计，无需领域特定数据微调，强调人机协作而非自动化，更注重实际术中应用而非数据集标注。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCOPE通过整合语言模型推理能力和视觉模型感知能力，创造了一种语音引导的人机协作框架，使外科医生能通过自然语音命令实时分割和跟踪手术场景元素，从而提高手术情境感知和决策能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation and tracking of relevant elements of the surgical sceneis crucial to enable context-aware intraoperative assistance and decisionmaking. Current solutions remain tethered to domain-specific, supervised modelsthat rely on labeled data and required domain-specific data to adapt to newsurgical scenarios and beyond predefined label categories. Recent advances inprompt-driven vision foundation models (VFM) have enabled open-set, zero-shotsegmentation across heterogeneous medical images. However, dependence of thesemodels on manual visual or textual cues restricts their deployment inintroperative surgical settings. We introduce a speech-guided collaborativeperception (SCOPE) framework that integrates reasoning capabilities of largelanguage model (LLM) with perception capabilities of open-set VFMs to supporton-the-fly segmentation, labeling and tracking of surgical instruments andanatomy in intraoperative video streams. A key component of this framework is acollaborative perception agent, which generates top candidates of VFM-generatedsegmentation and incorporates intuitive speech feedback from clinicians toguide the segmentation of surgical instruments in a natural human-machinecollaboration paradigm. Afterwards, instruments themselves serve as interactivepointers to label additional elements of the surgical scene. We evaluated ourproposed framework on a subset of publicly available Cataract1k dataset and anin-house ex-vivo skull-base dataset to demonstrate its potential to generateon-the-fly segmentation and tracking of surgical scene. Furthermore, wedemonstrate its dynamic capabilities through a live mock ex-vivo experiment.This human-AI collaboration paradigm showcase the potential of developingadaptable, hands-free, surgeon-centric tools for dynamic operating-roomenvironments.</description>
      <author>example@mail.com (Jecia Z. Y. Mao, Francis X Creighton, Russell H Taylor, Manish Sahu)</author>
      <guid isPermaLink="false">2509.10748v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction</title>
      <link>http://arxiv.org/abs/2509.10698v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CrunchLLM，一个专门用于创业公司成功预测的领域自适应大型语言模型框架，结合结构化数据和非结构化文本，通过参数高效微调和提示优化提高预测准确率，同时提供可解释的推理。&lt;h4&gt;背景&lt;/h4&gt;预测创业公司成功（通过收购或IPO退出）是创业研究的关键问题；Crunchbase等数据集提供结构化信息和非结构化文本，但传统机器学习方法仅依赖结构化特征且准确率一般，而大型语言模型虽具推理能力但难以直接适应商业领域数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于创业公司成功预测的领域自适应大型语言模型框架，整合结构化公司属性与非结构化文本叙述，应用参数高效微调策略和提示优化，使基础模型专门用于创业数据。&lt;h4&gt;方法&lt;/h4&gt;提出CrunchLLM框架，整合结构化公司属性与非结构化文本叙述，应用参数高效微调策略和提示优化，使基础模型专门适应创业领域数据。&lt;h4&gt;主要发现&lt;/h4&gt;CrunchLLM在Crunchbase创业公司成功预测上实现超过80%的准确率，显著优于传统分类器和基线LLMs；提供可解释的推理轨迹，增强金融和政策决策者的透明度和可信度。&lt;h4&gt;结论&lt;/h4&gt;通过领域感知微调和结构化-非结构化数据融合改进LLMs可推进创业成果预测建模；CrunchLLM为风险资本和创新政策中的数据驱动决策提供了方法论框架和实用工具。&lt;h4&gt;翻译&lt;/h4&gt;预测创业公司的成功，定义为通过收购或首次公开募股实现退出，是创业和创新研究中的关键问题。Crunchbase等数据集提供了结构化信息（如融资轮次、行业、投资者网络）和非结构化文本（如公司描述），但有效利用这种异构数据进行预测仍然具有挑战性。传统机器学习方法通常只依赖结构化特征并取得中等准确率，而大型语言模型虽提供丰富的推理能力却难以直接适应特定领域的商业数据。我们提出了CrunchLLM，一个用于创业公司成功预测的领域自适应LLM框架。CrunchLLM整合了结构化公司属性和非结构化文本叙述，并应用参数高效微调策略和提示优化，使基础模型专门用于创业数据。我们的方法在Crunchbase创业公司成功预测上实现了超过80%的准确率，显著优于传统分类器和基线LLMs。除了预测性能外，CrunchLLM提供可解释的推理轨迹，为其预测提供合理性解释，增强了金融和政策决策者的透明度和可信度。这项工作展示了如何通过领域感知微调和结构化-非结构化数据融合来改进LLMs，以推进创业成果的预测建模。CrunchLLM为风险资本和创新政策中的数据驱动决策提供了方法论框架和实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting the success of start-up companies, defined as achieving an exitthrough acquisition or IPO, is a critical problem in entrepreneurship andinnovation research. Datasets such as Crunchbase provide both structuredinformation (e.g., funding rounds, industries, investor networks) andunstructured text (e.g., company descriptions), but effectively leveraging thisheterogeneous data for prediction remains challenging. Traditional machinelearning approaches often rely only on structured features and achieve moderateaccuracy, while large language models (LLMs) offer rich reasoning abilities butstruggle to adapt directly to domain-specific business data. We present\textbf{CrunchLLM}, a domain-adapted LLM framework for startup successprediction. CrunchLLM integrates structured company attributes withunstructured textual narratives and applies parameter-efficient fine-tuningstrategies alongside prompt optimization to specialize foundation models forentrepreneurship data. Our approach achieves accuracy exceeding 80\% onCrunchbase startup success prediction, significantly outperforming traditionalclassifiers and baseline LLMs. Beyond predictive performance, CrunchLLMprovides interpretable reasoning traces that justify its predictions, enhancingtransparency and trustworthiness for financial and policy decision makers. Thiswork demonstrates how adapting LLMs with domain-aware fine-tuning andstructured--unstructured data fusion can advance predictive modeling ofentrepreneurial outcomes. CrunchLLM contributes a methodological framework anda practical tool for data-driven decision making in venture capital andinnovation policy.</description>
      <author>example@mail.com (Rabeya Tus Sadia, Qiang Cheng)</author>
      <guid isPermaLink="false">2509.10698v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses</title>
      <link>http://arxiv.org/abs/2509.10620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025 Workshop CVAMD&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一个基于SimCLR的自监督学习基础模型，用于3D脑结构MRI分析，该模型在多种神经系统疾病的预测任务中表现出色，即使使用少量标记数据也能保持高性能。&lt;h4&gt;背景&lt;/h4&gt;3D结构MRI常用于监测多种神经系统疾病，但现有深度学习模型针对特定任务设计，泛化能力有限；自监督学习在2D医学成像中已取得成功，但3D脑MRI基础模型在分辨率、范围或可访问性方面仍有不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用、高分辨率的自监督学习基础模型，用于3D脑结构MRI分析，提高模型在不同任务和人群中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;基于SimCLR架构构建自监督学习模型，使用来自11个公开数据集的18,759名患者（44,958次扫描）进行预训练，涵盖多种神经系统疾病；与掩码自编码器及两个监督基线模型在四种下游预测任务上进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的SimCLR模型在所有下游任务上均优于其他模型；即使仅使用20%的标记训练样本预测阿尔茨海默病，模型仍能保持卓越性能。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一个广泛适用且可访问的基础模型，显著提升了3D脑MRI分析的性能，特别适用于标记数据有限的情况。&lt;h4&gt;翻译&lt;/h4&gt;3D结构磁共振成像（MRI）脑扫描通常在临床环境中获取，用于监测广泛的神经系统疾病，包括神经退行性疾病和中风。虽然深度学习模型在分析3D MRI的多个脑成像任务中显示出有希望的结果，但大多数模型都是针对特定任务高度定制化的，标记数据有限，且无法跨任务和/或人群泛化。自监督学习（SSL）的发展使得能够利用从健康到疾病多样化的未标记数据集创建大型医学基础模型，在2D医学成像应用中显示出显著成功。然而，即使是少数已开发的3D脑MRI基础模型，在分辨率、范围或可访问性方面仍然有限。在这项工作中，我们提出了一个通用的高分辨率基于SimCLR的SSL基础模型，用于3D脑结构MRI，在11个公开可用的数据集上进行了预训练，这些数据集涵盖了18,759名患者（44,958次扫描），涉及多种神经系统疾病。我们在分布内和分布外设置中的四个不同下游预测任务上，将我们的模型与掩码自编码器（MAE）以及两个监督基线模型进行了比较。我们的微调SimCLR模型在所有任务上都优于所有其他模型。值得注意的是，即使在使用仅20%的标记训练样本来预测阿尔茨海默病时，我们的模型仍能实现卓越的性能。我们使用公开可用的代码和数据，并在https://github.com/emilykaczmarek/3D-Neuro-SimCLR上发布了我们的训练模型，为临床脑MRI分析贡献了一个广泛适用且可访问的基础模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D脑部MRI扫描分析中深度学习模型难以泛化的问题。现有模型通常针对特定任务定制，依赖有限标记数据，无法在不同任务和人群中泛化。这个问题很重要，因为脑部疾病诊断常使用3D MRI，但医学影像标记数据获取成本高、耗时长，且模型在遇到分布变化时可能失效。不同人群的脑部MRI具有高度视觉和解剖相似性，为利用多样化数据集训练通用模型提供了机会。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了当前3D脑部MRI基础模型的三个主要局限：数据集范围有限、分辨率低、难以复制。他们借鉴了现有的自监督学习方法，特别是基于对比学习的SimCLR和基于掩码的MAE架构。选择SimCLR是因为其强大的归纳偏差，适合相对较小的医学影像数据集。作者还实现了MAE作为互补基线，并设计了快速可复现的预处理流程，使用TurboPrep包处理高分辨率3D数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用自监督学习从大量未标记的多样化3D脑部MRI数据中学习通用表示，使模型能泛化到各种下游任务。整体流程包括：1)收集11个数据集的18,759名患者(44,958次扫描)的3D脑部MRI；2)使用TurboPrep进行预处理，包括偏置场校正、颅骨剥离、配准等，保持1×1×1mm³高分辨率；3)采用SimCLR或MAE进行自监督预训练，学习对变换不变的语义特征；4)在四个下游任务(中风量表回归、阿尔茨海默病分类、性别分类、年龄回归)上评估模型性能，包括分布内和分布外测试；5)研究在有限标记数据下的模型表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用大规模多样化数据集(11个数据集，近4.5万次扫描)，涵盖多种神经系统疾病；2)保持高分辨率(1×1×1mm³)处理能力，保留临床相关小特征；3)完全公开代码和训练模型，提高可访问性和可复现性；4)在所有下游任务上表现优于现有模型，即使仅使用20%标记数据；5)使用快速预处理流程(TurboPrep)，每个扫描约需一分钟。相比之前工作，本文模型数据规模更大、更多样化、分辨率更高、更易获取，且在更广泛的任务和数据分布上进行了全面评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于SimCLR的高分辨率、可泛化的3D脑部MRI自监督基础模型，通过整合多源数据实现了在各种神经系统疾病诊断任务上的卓越性能，并在有限标记数据条件下保持高准确率，同时公开了代码和模型以促进医学影像分析领域的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonlyacquired in clinical settings to monitor a wide range of neurologicalconditions, including neurodegenerative disorders and stroke. While deeplearning models have shown promising results analyzing 3D MRI across a numberof brain imaging tasks, most are highly tailored for specific tasks withlimited labeled data, and are not able to generalize across tasks and/orpopulations. The development of self-supervised learning (SSL) has enabled thecreation of large medical foundation models that leverage diverse, unlabeleddatasets ranging from healthy to diseased data, showing significant success in2D medical imaging applications. However, even the very few foundation modelsfor 3D brain MRI that have been developed remain limited in resolution, scope,or accessibility. In this work, we present a general, high-resolutionSimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on18,759 patients (44,958 scans) from 11 publicly available datasets spanningdiverse neurological diseases. We compare our model to Masked Autoencoders(MAE), as well as two supervised baselines, on four diverse downstreamprediction tasks in both in-distribution and out-of-distribution settings. Ourfine-tuned SimCLR model outperforms all other models across all tasks. Notably,our model still achieves superior performance when fine-tuned using only 20% oflabeled training samples for predicting Alzheimer's disease. We use publiclyavailable code and data, and release our trained model athttps://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadlyapplicable and accessible foundation model for clinical brain MRI analysis.</description>
      <author>example@mail.com (Emily Kaczmarek, Justin Szeto, Brennan Nichyporuk, Tal Arbel)</author>
      <guid isPermaLink="false">2509.10620v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>RailSafeNet: Visual Scene Understanding for Tram Safety</title>
      <link>http://arxiv.org/abs/2509.12125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, EPIA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为RailSafeNet的实时框架，利用数字图像处理、深度学习和人工智能技术来提高有轨电车与人交互的安全性。&lt;h4&gt;背景&lt;/h4&gt;有轨电车经常在人口密集区域运行，与人交互的安全是一个重要挑战，碰撞可能导致从轻微伤害到致命结果。&lt;h4&gt;目的&lt;/h4&gt;设计一个解决方案，利用数字图像处理、深度学习和人工智能来提高行人、司机、骑行者、宠物和有轨电车乘客的安全。&lt;h4&gt;方法&lt;/h4&gt;RailSafeNet是一个实时框架，融合了语义分割、目标检测和基于规则的距离评估器来突出显示轨道入侵。系统仅使用单目视频，可以识别轨道、定位附近物体，并通过将投影距离与标准1435毫米轨道轨距比较来分类风险。&lt;h4&gt;主要发现&lt;/h4&gt;在RailSem19数据集上的实验显示，经过类别过滤的SegFormer B3模型实现了65%的交并比，经过微调的YOLOv8在交并比阈值为0.50时达到了75.6%的平均精度均值。&lt;h4&gt;结论&lt;/h4&gt;RailSafeNet提供了准确、标注轻量的场景理解能力，可以在危险情况升级前警告司机。&lt;h4&gt;翻译&lt;/h4&gt;有轨电车与人交互安全是一个重要挑战，因为有轨电车经常在人口密集区域运行，碰撞可能导致从轻微伤害到致命结果。本文从利用数字图像处理、深度学习和人工智能设计解决方案的角度出发，以提高行人、司机、骑行者、宠物和有轨电车乘客的安全性。我们提出了RailSafeNet，一个实时框架，融合了语义分割、目标检测和基于规则的距离评估器来突出显示轨道入侵。仅使用单目视频，系统可以识别轨道，定位附近物体，并通过将投影距离与标准1435毫米轨道轨距比较来分类风险。在多样化的RailSem19数据集上的实验显示，经过类别过滤的SegFormer B3模型实现了65%的交并比，而经过微调的YOLOv8在交并比阈值为0.50时达到了75.6%的平均精度均值。因此，RailSafeNet提供了准确、标注轻量的场景理解，可以在危险情况升级前警告司机。代码可在https://github.com/oValach/RailSafeNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tram-human interaction safety is an important challenge, given that tramsfrequently operate in densely populated areas, where collisions can range fromminor injuries to fatal outcomes. This paper addresses the issue from theperspective of designing a solution leveraging digital image processing, deeplearning, and artificial intelligence to improve the safety of pedestrians,drivers, cyclists, pets, and tram passengers. We present RailSafeNet, areal-time framework that fuses semantic segmentation, object detection and arule-based Distance Assessor to highlight track intrusions. Using onlymonocular video, the system identifies rails, localises nearby objects andclassifies their risk by comparing projected distances with the standard 1435mmrail gauge. Experiments on the diverse RailSem19 dataset show that aclass-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculatedat an intersection over union (IoU) threshold of 0.50. RailSafeNet thereforedelivers accurate, annotation-light scene understanding that can warn driversbefore dangerous situations escalate. Code available athttps://github.com/oValach/RailSafeNet.</description>
      <author>example@mail.com (Ing. Ondrej Valach, Ing. Ivan Gruber)</author>
      <guid isPermaLink="false">2509.12125v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Microsurgical Instrument Segmentation for Robot-Assisted Surgery</title>
      <link>http://arxiv.org/abs/2509.11727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MISRA框架，通过增强RGB输入、集成跳跃注意力和迭代反馈模块来解决显微手术中薄结构分割的挑战，并引入了一个专门的显微手术数据集。&lt;h4&gt;背景&lt;/h4&gt;准确的薄结构分割对于显微手术场景理解至关重要，但由于分辨率损失、低对比度和类别不平衡，这仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个分割框架来解决显微手术中薄结构分割的挑战，提高分割的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;提出了MISRA分割框架，通过增加亮度通道增强RGB输入，集成跳跃注意力保留细长特征，采用迭代反馈模块在多次传递中恢复连续性，并引入了一个专门的显微手术数据集。&lt;h4&gt;主要发现&lt;/h4&gt;MISRA实现了有竞争力的性能，与竞争方法相比，平均类别IoU提高了5.37%，同时在器械接触和重叠处提供了更稳定的预测。&lt;h4&gt;结论&lt;/h4&gt;MISRA是迈向计算机辅助和机器人显微手术可靠场景解析的有希望的一步。&lt;h4&gt;翻译&lt;/h4&gt;准确的薄结构分割对于显微手术场景理解至关重要，但由于分辨率损失、低对比度和类别不平衡，这仍然具有挑战性。我们提出了MISRA（机器人辅助显微手术器械分割），一个分割框架，通过增加亮度通道来增强RGB输入，集成跳跃注意力以保留细长特征，并采用迭代反馈模块在多次传递中恢复连续性。此外，我们引入了一个专门的显微手术数据集，包含精细标注的外科器械包括薄物体，为稳健评估提供了基准。数据集可在https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg获取。实验证明MISRA实现了有竞争力的性能，与竞争方法相比，平均类别IoU提高了5.37%，同时在器械接触和重叠处提供了更稳定的预测。这些结果表明MISRA是迈向计算机辅助和机器人显微手术可靠场景解析的有希望的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of thin structures is critical for microsurgical sceneunderstanding but remains challenging due to resolution loss, low contrast, andclass imbalance. We propose Microsurgery Instrument Segmentation for RoboticAssistance(MISRA), a segmentation framework that augments RGB input withluminance channels, integrates skip attention to preserve elongated features,and employs an Iterative Feedback Module(IFM) for continuity restoration acrossmultiple passes. In addition, we introduce a dedicated microsurgical datasetwith fine-grained annotations of surgical instruments including thin objects,providing a benchmark for robust evaluation Dataset available athttps://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstratethat MISRA achieves competitive performance, improving the mean class IoU by5.37% over competing methods, while delivering more stable predictions atinstrument contacts and overlaps. These results position MISRA as a promisingstep toward reliable scene parsing for computer-assisted and roboticmicrosurgery.</description>
      <author>example@mail.com (Tae Kyeong Jeong, Garam Kim, Juyoun Park)</author>
      <guid isPermaLink="false">2509.11727v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>See What I Mean? Mobile Eye-Perspective Rendering for Optical See-through Head-mounted Displays</title>
      <link>http://arxiv.org/abs/2509.11653v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对光学透视头戴显示器中的视角错位问题，提出并评估了三种基于软件的眼睛视角渲染技术，通过用户研究证明了凝视代理方法的优越性，并开源了相关框架。&lt;h4&gt;背景&lt;/h4&gt;基于图像的场景理解技术可在未准备的真实世界环境中为增强现实系统提供上下文视觉指导，但在光学透视头戴显示器上存在世界-facing相机与用户眼睛视角之间的错位问题，影响效果。&lt;h4&gt;目的&lt;/h4&gt;开发并评估软件眼睛视角渲染技术，以近似用户的真实视角，解决光学透视头戴显示器中的错位问题。&lt;h4&gt;方法&lt;/h4&gt;在Microsoft HoloLens 2上实现并评估了三种眼睛视角渲染技术：平面代理EPR（投影到固定距离平面）、网格代理EPR（使用SLAM重建）和凝视代理EPR（基于眼动追踪的对齐方法）。&lt;h4&gt;主要发现&lt;/h4&gt;真实世界任务的用户研究强调了准确眼睛视角渲染的重要性，并发现凝视代理方法是一种轻量级的有效替代方案，优于基于几何的方法。&lt;h4&gt;结论&lt;/h4&gt;软件眼睛视角渲染技术可有效解决光学透视头戴显示器中的视角错位问题，凝视代理方法因其轻量级特性而特别有应用价值。&lt;h4&gt;翻译&lt;/h4&gt;基于图像的场景理解使增强系统能够在未准备的真实世界环境中提供上下文视觉指导。虽然在视频透视头戴显示器上有效，但这类方法在光学透视头戴显示器上存在问题，因为前置相机与用户眼睛视角之间存在错位。为了近似用户的真实视角，我们在商业化的无绳光学透视头戴显示器（Microsoft HoloLens 2）上实现了并评估了三种基于软件的眼睛视角渲染技术：（1）平面代理EPR，投影到固定距离的平面上；（2）网格代理EPR，使用基于SLAM的重建进行投影；（3）凝视代理EPR，一种新颖的基于眼动追踪的方法，将投影与用户的凝视深度对齐。真实世界任务的用户研究强调了准确眼睛视角渲染的重要性，并证明了凝视代理作为基于几何方法的轻量级替代方案。我们以开源形式发布了眼睛视角渲染框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决光学穿透式头戴显示器中世界相机视角与用户眼睛视角不匹配导致的视觉错位问题。这个问题很重要，因为当系统分析世界相机图像并叠加信息到用户视野时，视角差异会导致虚拟内容无法准确对齐真实物体，影响用户体验，在低视力辅助、颜色视觉缺陷补偿和任务指导等应用中尤其明显，还可能引起恶心等生理不适。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有工作，发现有两种主要方法：硬件修改方法（如使用半透镜）和软件方法（如固定平面投影）。硬件方法无法应用于现有商业设备，软件方法则存在明显错位问题。作者借鉴了现有的纹理投影技术，设计了三种不同的代理几何体方法：固定平面投影、网格投影和一种新的基于凝视的动态平面投影方法，以解决视角不匹配问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过纹理投影技术，从用户视角重新渲染世界相机捕获的视图，使用不同的代理几何体确保虚拟内容与真实世界场景准确对齐。流程包括：1)使用HoloLens 2获取传感器数据；2)在Unity中进行渲染；3)处理世界相机图像；4)根据选择的EPR方法将图像投影到不同代理几何体上；5)从用户眼睛位置重新渲染生成EPR视图；6)在显示器上显示结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Gaze-Proxy EPR方法，使用眼动跟踪动态调整投影平面；2)在移动式商业OST HMD上实现和比较三种EPR方法；3)进行真实场景下的用户研究；4)开源所有实现。相比之前工作，本研究关注移动、无连接线场景，提出的Gaze-Proxy方法在保持高准确性的同时降低了计算复杂度，解决了之前方法要么计算复杂要么准确性差的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这项研究提出并评估了一种基于眼动跟踪的新型轻量级EPR方法，为光学穿透式头戴显示器提供了准确的图像分析结果对齐解决方案，并通过开源框架促进了移动式增强现实应用的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image-based scene understanding allows Augmented Reality systems to providecontextual visual guidance in unprepared, real-world environments. Whileeffective on video see-through (VST) head-mounted displays (HMDs), such methodssuffer on optical see-through (OST) HMDs due to misregistration between theworld-facing camera and the user's eye perspective. To approximate the user'strue eye view, we implement and evaluate three software-based eye-perspectiverendering (EPR) techniques on a commercially available, untethered OST HMD(Microsoft HoloLens 2): (1) Plane-Proxy EPR, projecting onto a fixed-distanceplane; (2) Mesh-Proxy EPR, using SLAM-based reconstruction for projection; and(3) Gaze-Proxy EPR, a novel eye-tracking-based method that aligns theprojection with the user's gaze depth. A user study on real-world tasksunderscores the importance of accurate EPR and demonstrates gaze-proxy as alightweight alternative to geometry-based methods. We release our EPR frameworkas open source.</description>
      <author>example@mail.com (Gerlinde Emsenhuber, Tobias Langlotz, Denis Kalkofen, Markus Tatzgern)</author>
      <guid isPermaLink="false">2509.11653v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>How Auxiliary Reasoning Unleashes GUI Grounding in VLMs</title>
      <link>http://arxiv.org/abs/2509.11548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对图形用户界面(GUI)基础任务提出三种零样本辅助推理方法，通过提供明确的空间线索使视觉语言模型能够表达其隐含的空间理解能力，显著提高了GUI基础性能。&lt;h4&gt;背景&lt;/h4&gt;图形用户界面(GUI)基础是构建GUI代理的基本任务，但通用的视觉语言模型(VLMs)在这个任务上表现不佳，因为缺乏特定优化。&lt;h4&gt;目的&lt;/h4&gt;解决VLMs在Pointing Game测量中显示出潜在定位能力但在输出明确坐标任务上表现不佳的问题，同时避免当前微调方法的高数据和高标注成本。&lt;h4&gt;方法&lt;/h4&gt;提出三种零样本辅助推理方法，通过在输入图像中提供明确的空间线索(如坐标轴、网格和标记的交叉点)，使VLMs能够表达其隐含的空间理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;在四个GUI基础基准上对七个开源和专有VLMs的评估结果表明，提出的方法显著提高了GUI基础性能。&lt;h4&gt;结论&lt;/h4&gt;通过提供空间线索的零样本辅助推理方法可以有效提升VLMs在GUI基础任务上的表现，避免了传统微调方法的高成本问题。&lt;h4&gt;翻译&lt;/h4&gt;图形用户界面(GUI)基础是构建GUI代理的基本任务。然而，通用的视觉语言模型(VLMs)由于缺乏特定优化而难以完成此任务。本文确定了一个关键差距：虽然VLMs在通过Pointing Game测量的性能中显示出显著的潜在定位能力，但在输出明确坐标的任务上表现不佳。为解决这种差异，并绕过当前微调方法的高数据和标注成本，我们提出了三种零样本辅助推理方法。通过在输入图像中提供明确的空间线索，如坐标轴、网格和标记的交叉点，这些方法使VLMs能够表达其隐含的空间理解能力。我们在四个GUI基础基准上对七个开源和专有VLMs评估了这些方法。评估结果表明，提出的方法显著提高了GUI基础的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphical user interface (GUI) grounding is a fundamental task for buildingGUI agents. However, general vision-language models (VLMs) struggle with thistask due to a lack of specific optimization. We identify a key gap in thispaper: while VLMs exhibit significant latent grounding potential, asdemonstrated by their performance measured by Pointing Game, they underperformwhen tasked with outputting explicit coordinates. To address this discrepancy,and bypass the high data and annotation costs of current fine-tuningapproaches, we propose three zero-shot auxiliary reasoning methods. Byproviding explicit spatial cues such as axes, grids and labeled intersectionsas part of the input image, these methods enable VLMs to articulate theirimplicit spatial understanding capabilities. We evaluate these methods on fourGUI grounding benchmarks across seven open-source and proprietary VLMs. Theevaluation results demonstrate that the proposed methods substantially improvethe performance of GUI grounding.</description>
      <author>example@mail.com (Weiming Li, Yan Shao, Jing Yang, Yujing Lu, Ling Zhong, Yuhan Wang, Manni Duan)</author>
      <guid isPermaLink="false">2509.11548v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision</title>
      <link>http://arxiv.org/abs/2509.11476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 6th International Conference on Computer Vision and  Data Mining (ICCVDM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FusionNet的新型端到端红外和可见图像融合框架，通过模态感知注意力机制和像素级alpha混合模块实现细粒度融合，并利用目标感知损失函数保留重要对象区域的语义一致性。&lt;h4&gt;背景&lt;/h4&gt;红外和可见图像融合(IVIF)是多模态感知中的基本任务，旨在整合来自不同光谱域的互补结构和纹理线索。&lt;h4&gt;目的&lt;/h4&gt;提出一个新型端到端融合框架，明确建模模态间交互并增强任务关键区域。&lt;h4&gt;方法&lt;/h4&gt;引入模态感知注意力机制动态调整红外和可见特征贡献；集成像素级alpha混合模块自适应学习空间变化的融合权重；提出目标感知损失函数利用弱ROI监督保留重要对象区域的语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在M3FD数据集上的实验表明，FusionNet生成的融合图像具有增强的语义保留性、高感知质量和清晰的可解释性。&lt;h4&gt;结论&lt;/h4&gt;该框架为语义感知的多模态图像融合提供了通用且可扩展的解决方案，有利于下游任务如目标检测和场景理解。&lt;h4&gt;翻译&lt;/h4&gt;红外和可见图像融合(IVIF)是多模态感知中的一个基本任务，旨在整合来自不同光谱域的互补结构和纹理线索。在本文中，我们提出FusionNet，一种新型端到端融合框架，明确建模模态间交互并增强任务关键区域。FusionNet引入了一种模态感知注意力机制，根据红外和可见特征的判别能力动态调整其贡献。为实现细粒度、可解释的融合，我们进一步集成了像素级alpha混合模块，该模块以自适应和内容感知的方式学习空间变化的融合权重。此外，我们制定了一种目标感知损失函数，利用弱ROI监督来保留包含重要对象（如行人、车辆）区域的语义一致性。在公共M3FD数据集上的实验表明，FusionNet生成的融合图像具有增强的语义保留性、高感知质量和清晰的可解释性。我们的框架为语义感知的多模态图像融合提供了通用且可扩展的解决方案，有利于目标检测和场景理解等下游任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infrared and visible image fusion (IVIF) is a fundamental task in multi-modalperception that aims to integrate complementary structural and textural cuesfrom different spectral domains. In this paper, we propose FusionNet, a novelend-to-end fusion framework that explicitly models inter-modality interactionand enhances task-critical regions. FusionNet introduces a modality-awareattention mechanism that dynamically adjusts the contribution of infrared andvisible features based on their discriminative capacity. To achievefine-grained, interpretable fusion, we further incorporate a pixel-wise alphablending module, which learns spatially-varying fusion weights in an adaptiveand content-aware manner. Moreover, we formulate a target-aware loss thatleverages weak ROI supervision to preserve semantic consistency in regionscontaining important objects (e.g., pedestrians, vehicles). Experiments on thepublic M3FD dataset demonstrate that FusionNet generates fused images withenhanced semantic preservation, high perceptual quality, and clearinterpretability. Our framework provides a general and extensible solution forsemantic-aware multi-modal image fusion, with benefits for downstream taskssuch as object detection and scene understanding.</description>
      <author>example@mail.com (Tianyao Sun, Dawei Xiang, Tianqi Ding, Xiang Fang, Yijiashun Qi, Zunduo Zhao)</author>
      <guid isPermaLink="false">2509.11476v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI</title>
      <link>http://arxiv.org/abs/2509.10683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了大型语言模型(LLMs)和传统卷积神经网络(CNNs)在医学影像任务（胶质瘤分类和分割）上的性能，结果表明CNN在准确性和空间理解方面优于LLMs，LLMs在当前形式下不太适合基于图像的医疗任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)在基于文本的医疗任务中表现出色，但它们在基于图像的应用中的效用尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;研究LLMs在医学影像任务（特别是胶质瘤分类和分割）中的有效性，并将其性能与传统卷积神经网络(CNNs)进行比较。&lt;h4&gt;方法&lt;/h4&gt;使用BraTS 2020多模态脑部MRI数据集，评估通用视觉-语言LLM（LLaMA 3.2 Instruct）在微调前后的性能，并将其与自定义3D CNN进行基准测试。对于分类任务，比较低级别与高级别胶质瘤的分类效果；对于分割任务，实现中心点、边界框和多边形提取三种方法。&lt;h4&gt;主要发现&lt;/h4&gt;分类任务：CNN达到80%准确率，平衡了精确率和召回率；通用LLM达到76%准确率但特异性仅18%，经常错误分类低级别肿瘤；微调后特异性提高到55%但准确率降至72%。分割任务：CNNs能准确定位胶质瘤但有时遗漏小肿瘤；LLMs将预测聚集在图像中心，无法区分胶质瘤大小、位置；微调改善了输出格式但未提高空间准确性；边界多边形方法产生随机无结构输出。&lt;h4&gt;结论&lt;/h4&gt;CNNs在两项任务中都优于LLMs。LLMs表现出有限的空间理解能力，微调带来的改进很小，表明在当前形式下它们不适合基于图像的任务。需要更严格的微调或替代训练策略，才能使LLMs在医疗领域实现更好的性能、鲁棒性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在基于文本的医疗任务中表现出色。然而，它们在基于图像的应用中的效用尚未被探索。我们研究了LLMs在医学影像任务（特别是胶质瘤分类和分割）中的有效性，并将其性能与传统卷积神经网络(CNNs)进行了比较。使用BraTS 2020多模态脑部MRI数据集，我们评估了通用视觉-语言LLM（LLaMA 3.2 Instruct）在微调前后的性能，并将其性能与自定义3D CNN进行了基准测试。对于胶质瘤分类（低级别vs高级别），CNN实现了80%的准确率，并平衡了精确率和召回率。通用LLM达到76%的准确率，但特异性仅为18%，经常错误分类低级别肿瘤。微调将特异性提高到55%，但整体性能下降（例如，准确率降至72%）。对于分割任务，实现了三种方法：中心点、边界框和多边形提取。CNNs能够准确定位胶质瘤，但有时会遗漏小肿瘤。相比之下，LLMs将预测一致地聚集在图像中心，无法区分胶质瘤的大小、位置或放置。微调改善了输出格式，但未能有意义地提高空间准确性。边界多边形方法产生随机、无结构的输出。总体而言，CNNs在两项任务中都优于LLMs。LLMs表现出有限的空间理解能力，微调带来的改进也很小，表明在当前形式下，它们不适合基于图像的任务。可能需要更严格的微调或替代训练策略，才能使LLMs在医疗领域实现更好的性能、鲁棒性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have shown strong performance in text-basedhealthcare tasks. However, their utility in image-based applications remainsunexplored. We investigate the effectiveness of LLMs for medical imaging tasks,specifically glioma classification and segmentation, and compare theirperformance to that of traditional convolutional neural networks (CNNs). Usingthe BraTS 2020 dataset of multi-modal brain MRIs, we evaluated ageneral-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and afterfine-tuning, and benchmarked its performance against custom 3D CNNs. For gliomaclassification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy andbalanced precision and recall. The general LLM reached 76% accuracy butsuffered from a specificity of only 18%, often misclassifying Low-Grade tumors.Fine-tuning improved specificity to 55%, but overall performance declined(e.g., accuracy dropped to 72%). For segmentation, three methods - centerpoint, bounding box, and polygon extraction, were implemented. CNNs accuratelylocalized gliomas, though small tumors were sometimes missed. In contrast, LLMsconsistently clustered predictions near the image center, with no distinctionof glioma size, location, or placement. Fine-tuning improved output formattingbut failed to meaningfully enhance spatial accuracy. The bounding polygonmethod yielded random, unstructured outputs. Overall, CNNs outperformed LLMs inboth tasks. LLMs showed limited spatial understanding and minimal improvementfrom fine-tuning, indicating that, in their current form, they are notwell-suited for image-based tasks. More rigorous fine-tuning or alternativetraining strategies may be needed for LLMs to achieve better performance,robustness, and utility in the medical space.</description>
      <author>example@mail.com (Felicia Liu, Jay J. Yoo, Farzad Khalvati)</author>
      <guid isPermaLink="false">2509.10683v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal SAM-adapter for Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.10408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MM SAM-adapter的新型多模态语义分割框架，通过适配器网络将融合的多模态特征注入到SAM的RGB特征中，实现了在具有挑战性条件下的高效场景理解。&lt;h4&gt;背景&lt;/h4&gt;语义分割作为计算机视觉的关键任务，在自动驾驶、医学成像和机器人技术等领域有广泛应用。尽管深度学习推动了该领域的显著进步，但现有方法在光照不足、遮挡和恶劣天气等挑战条件下仍表现脆弱。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，作者提出MM SAM-adapter框架，扩展Segment Anything Model（SAM）的能力以实现多模态语义分割，实现多模态信息的平衡和高效使用。&lt;h4&gt;方法&lt;/h4&gt;所提出的方法采用适配器网络，将融合的多模态特征（如LiDAR、红外数据）注入到SAM丰富的RGB特征中。这种设计使模型能够保留RGB特征的强泛化能力，同时仅在辅助模态提供额外有用信息时选择性地融入它们。&lt;h4&gt;主要发现&lt;/h4&gt;在三个具有挑战性的基准测试（DeLiVER、FMB和MUSES）上评估，MM SAM-adapter实现了最先进的性能。通过将数据集划分为RGB-easy和RGB-hard子集的分析表明，该框架在有利和不利条件下均优于现有方法，证明了多模态适应对鲁棒场景理解的有效性。&lt;h4&gt;结论&lt;/h4&gt;MM SAM-adapter实现了多模态信息的平衡和高效使用，显著提升了在挑战性条件下的语义分割性能，为多模态场景理解提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;语义分割是计算机视觉中的一个关键任务，在自动驾驶、医学成像和机器人技术中有广泛应用，随着深度学习的发展取得了显著进步。然而，当前方法在光照不足、遮挡和恶劣天气等具有挑战性的条件下仍然脆弱。为了解决这些局限性，最近出现了整合辅助传感器数据（如LiDAR、红外）的多模态方法，提供互补信息以增强鲁棒性。在这项工作中，我们提出了MM SAM-adapter，一种扩展Segment Anything Model（SAM）能力以实现多模态语义分割的新框架。所提出的方法采用适配器网络，将融合的多模态特征注入到SAM丰富的RGB特征中。这种设计使模型能够保留RGB特征的强泛化能力，同时仅在辅助模态提供额外线索时选择性地融入它们。因此，MM SAM-adapter实现了多模态信息的平衡和高效使用。我们在三个具有挑战性的基准测试（DeLiVER、FMB和MUSES）上评估了我们的方法，MM SAM-adapter在这些测试中实现了最先进的性能。为了进一步分析模态贡献，我们将DeLiVER和FMB划分为RGB-easy和RGB-hard子集。结果一致表明，我们的框架在有利和不利条件下都优于竞争方法，突显了多模态适应对鲁棒场景理解的有效性。代码可在以下链接获取：https://github.com/iacopo97/Multimodal-SAM-Adapter。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation, a key task in computer vision with broad applicationsin autonomous driving, medical imaging, and robotics, has advancedsubstantially with deep learning. Nevertheless, current approaches remainvulnerable to challenging conditions such as poor lighting, occlusions, andadverse weather. To address these limitations, multimodal methods thatintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,providing complementary information that enhances robustness. In this work, wepresent MM SAM-adapter, a novel framework that extends the capabilities of theSegment Anything Model (SAM) for multimodal semantic segmentation. The proposedmethod employs an adapter network that injects fused multimodal features intoSAM's rich RGB features. This design enables the model to retain the stronggeneralization ability of RGB features while selectively incorporatingauxiliary modalities only when they contribute additional cues. As a result, MMSAM-adapter achieves a balanced and efficient use of multimodal information. Weevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,where MM SAM-adapter delivers state-of-the-art performance. To further analyzemodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hardsubsets. Results consistently demonstrate that our framework outperformscompeting methods in both favorable and adverse conditions, highlighting theeffectiveness of multimodal adaptation for robust scene understanding. The codeis available at the following link:https://github.com/iacopo97/Multimodal-SAM-Adapter.</description>
      <author>example@mail.com (Iacopo Curti, Pierluigi Zama Ramirez, Alioscia Petrelli, Luigi Di Stefano)</author>
      <guid isPermaLink="false">2509.10408v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
  <item>
      <title>LayerLock: Non-collapsing Representation Learning with Progressive Freezing</title>
      <link>http://arxiv.org/abs/2509.10156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LayerLock是一种简单有效的自监督视觉表征学习方法，通过渐进式层冻结实现从像素到潜在预测的过渡。&lt;h4&gt;背景&lt;/h4&gt;在视频掩码自编码(MAE)模型训练中，ViT层按深度顺序收敛：浅层先收敛，深层后收敛。&lt;h4&gt;目的&lt;/h4&gt;加速标准MAE训练，并提供一种简单可扩展的潜在预测方法，避免'表征崩溃'问题。&lt;h4&gt;方法&lt;/h4&gt;LayerLock通过显式计划逐步冻结模型，从像素预测过渡到潜在预测。&lt;h4&gt;主要发现&lt;/h4&gt;ViT层收敛顺序与其深度相关，浅层先收敛，深层后收敛；这种观察可用于加速MAE训练。&lt;h4&gt;结论&lt;/h4&gt;LayerLock在大模型上表现优异，在4DS感知套件上的结果超越了非掩码潜在预测方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了LayerLock，一种简单而有效的自监督视觉表征学习方法，通过渐进式层冻结从像素预测逐步过渡到潜在预测。首先，我们观察到在视频掩码自编码(MAE)模型训练过程中，ViT层按照其深度顺序收敛：浅层收敛早，深层收敛晚。然后我们证明，这一观察可以通过在整个训练过程中按照显式计划逐步冻结模型来加速标准MAE。此外，同样的计划可用于一种简单且可扩展的潜在预测方法，不会遭受'表征崩溃'。我们将提出的LayerLock方法应用于高达40亿参数的大模型，在4DS感知套件上的结果超过了非掩码潜在预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LayerLock, a simple yet effective approach for self-supervisedvisual representation learning, that gradually transitions from pixel to latentprediction through progressive layer freezing. First, we make the observationthat during training of video masked-autoencoding (MAE) models, ViT layersconverge in the order of their depth: shallower layers converge early, deeperlayers converge late. We then show that this observation can be exploited toaccelerate standard MAE by progressively freezing the model according to anexplicit schedule, throughout training. Furthermore, this same schedule can beused in a simple and scalable approach to latent prediction that does notsuffer from "representation collapse". We apply our proposed approach,LayerLock, to large models of up to 4B parameters with results surpassing thoseof non-latent masked prediction on the 4DS perception suite.</description>
      <author>example@mail.com (Goker Erdogan, Nikhil Parthasarathy, Catalin Ionescu, Drew Hudson, Alexander Lerchner, Andrew Zisserman, Mehdi Sajjadi, Joao Carreira)</author>
      <guid isPermaLink="false">2509.10156v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Merging Physics-Based Synthetic Data and Machine Learning for Thermal Monitoring of Lithium-ion Batteries: The Role of Data Fidelity</title>
      <link>http://arxiv.org/abs/2509.10380v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合物理建模与机器学习的新型框架，用于开发资源高效且可扩展的内部温度估计算法，解决了传统方法中数据收集、模型参数化和估计器设计的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;内部温度比表面温度更难获取，需要开发准确且实时的估计算法以实现更好的热管理和安全。&lt;h4&gt;目的&lt;/h4&gt;开发一种资源高效且可扩展的框架，用于构建准确、鲁棒和自适应的内部温度估计算法。&lt;h4&gt;方法&lt;/h4&gt;结合物理建模和机器学习：利用物理模型生成包含不同操作场景的仿真数据；使用这些数据预训练机器学习算法；应用迁移学习和无监督领域适应弥合仿真到现实的差距；使用有限的目标电池运行数据微调预训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;框架在多种圆柱形电池和不同对流空气冷却条件下得到验证；仅依赖电池热特性的先验知识时，均方根误差为0.5摄氏度；使用接近真实值的热参数时，误差小于0.1摄氏度；全面研究了仿真数据质量在框架中的作用。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够有效解决传统方法中数据收集、模型参数化和估计器设计方面的关键挑战，实现准确、鲁棒和自适应的内部温度估计。&lt;h4&gt;翻译&lt;/h4&gt;由于内部温度比表面温度更难获取，迫切需要开发准确且实时的估计算法以实现更好的热管理和安全。这项工作通过结合物理建模与机器学习，提出了一种新型框架，用于资源高效且可扩展地开发准确、鲁棒和自适应的内部温度估计算法，以解决传统方法中数据收集、模型参数化和估计器设计方面的关键挑战。在该框架中，利用物理模型生成包含不同操作场景的仿真数据，通过扫描模型参数和输入曲线。这种廉价的仿真数据集可用于预训练机器学习算法以捕获底层映射关系。为了弥合由不完美建模导致的仿真到现实的差距，应用了带有无监督领域适应的迁移学习，使用来自目标电池的有限运行数据（无内部温度值）来微调预训练的机器学习模型。该框架在不同操作条件和多个采用对流空气冷却的圆柱形电池上得到验证，仅依赖电池热特性的先验知识时，均方根误差为0.5摄氏度，而使用接近真实值的热参数时，误差小于0.1摄氏度。此外，全面研究了仿真数据质量在所提框架中的作用，以识别有前途的合成数据生成方法，保证机器学习模型的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since the internal temperature is less accessible than surface temperature,there is an urgent need to develop accurate and real-time estimation algorithmsfor better thermal management and safety. This work presents a novel frameworkfor resource-efficient and scalable development of accurate, robust, andadaptive internal temperature estimation algorithms by blending physics-basedmodeling with machine learning, in order to address the key challenges in datacollection, model parameterization, and estimator design that traditionallyhinder both approaches. In this framework, a physics-based model is leveragedto generate simulation data that includes different operating scenarios bysweeping the model parameters and input profiles. Such a cheap simulationdataset can be used to pre-train the machine learning algorithm to capture theunderlying mapping relationship. To bridge the simulation-to-reality gapresulting from imperfect modeling, transfer learning with unsupervised domainadaptation is applied to fine-tune the pre-trained machine learning model, byusing limited operational data (without internal temperature values) fromtarget batteries. The proposed framework is validated under different operatingconditions and across multiple cylindrical batteries with convective aircooling, achieving a root mean square error of 0.5 {\deg}C when relying solelyon prior knowledge of battery thermal properties, and less than 0.1 {\deg}Cwhen using thermal parameters close to the ground truth. Furthermore, the roleof the simulation data quality in the proposed framework has beencomprehensively investigated to identify promising ways of synthetic datageneration to guarantee the performance of the machine learning model.</description>
      <author>example@mail.com (Yusheng Zheng, Wenxue Liu, Yunhong Che, Ferdinand Grimm, Jingyuan Zhao, Xiaosong Hu, Simona Onori, Remus Teodorescu, Gregory J. Offer)</author>
      <guid isPermaLink="false">2509.10380v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning</title>
      <link>http://arxiv.org/abs/2509.10273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种数据驱动的迁移学习框架，利用神经推荐系统在稀疏实验数据集上实现离子液体性质的可靠预测。&lt;h4&gt;背景&lt;/h4&gt;离子液体因其物理化学性质可精确调节而成为传统溶剂的多功能替代品，但准确预测其关键热物理性质具有挑战性，因为化学设计空间大且实验数据有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用神经推荐系统的数据驱动迁移学习框架，能够在稀疏实验数据情况下可靠预测离子液体的热物理性质。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段过程：首先在固定温度压力下使用COSMO-RS模拟数据预训练NRS模型学习阳离子和阴离子的结构嵌入；然后使用这些嵌入和不同温度压力下的实验数据微调前馈神经网络。研究考虑了密度、粘度、表面张力、热容和熔点五种性质，并支持同性质和跨性质知识转移。&lt;h4&gt;主要发现&lt;/h4&gt;使用密度、粘度和热容的预训练模型进行微调后，五种目标性质中有四种的性能显著提高；模型对未见过的离子液体具有强大的外推能力；最终训练的模型可预测超过700,000种离子液体组合的性质。&lt;h4&gt;结论&lt;/h4&gt;结合模拟数据和迁移学习是克服实验数据稀疏性的有效方法，为离子液体筛选和工艺设计提供了可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;离子液体（ILs）因其物理化学性质可精确调节以适应各种应用而成为传统溶剂的多功能替代品。然而，由于巨大的化学设计空间和实验数据的有限性，准确预测关键热物理性质仍然具有挑战性。在本研究中，我们提出了一个数据驱动的迁移学习框架，利用神经推荐系统（NRS）使用稀疏实验数据集实现离子液体性质的可靠预测。该方法涉及一个两阶段过程：首先在固定温度和压力下使用基于COSMO-RS的模拟数据预训练NRS模型，学习阳离子和阴离子的特定性质结构嵌入；其次使用这些嵌入和不同温度压力下的实验数据微调简单的前馈神经网络。在本工作中，考虑了五种基本的离子液体性质：密度、粘度、表面张力、热容和熔点。该框架支持同性质和跨性质知识转移。值得注意的是，密度、粘度和热容的预训练模型被用来微调所有五种目标性质的模型，其中四种性质的性能显著提高。该模型对未见过的离子液体表现出强大的外推能力。此外，最终训练的模型能够预测超过700,000种离子液体组合的性质，为工艺设计中的离子液体筛选提供了可扩展的解决方案。这项工作强调了结合模拟数据和迁移学习克服实验数据稀疏性的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ionic liquids (ILs) have emerged as versatile replacements for traditionalsolvents because their physicochemical properties can be precisely tailored tovarious applications. However, accurately predicting key thermophysicalproperties remains challenging due to the vast chemical design space and thelimited availability of experimental data. In this study, we present adata-driven transfer learning framework that leverages a neural recommendersystem (NRS) to enable reliable property prediction for ILs using sparseexperimental datasets. The approach involves a two-stage process: first,pre-training NRS models on COSMO-RS-based simulated data at fixed temperatureand pressure to learn property-specific structural embeddings for cations andanions; and second, fine-tuning simple feedforward neural networks using theseembeddings with experimental data at varying temperatures and pressures. Inthis work, five essential IL properties are considered: density, viscosity,surface tension, heat capacity, and melting point. The framework supports bothwithin-property and cross-property knowledge transfer. Notably, pre-trainedmodels for density, viscosity, and heat capacity are used to fine-tune modelsfor all five target properties, achieving improved performance by a substantialmargin for four of them. The model exhibits robust extrapolation to previouslyunseen ILs. Moreover, the final trained models enable property prediction forover 700,000 IL combinations, offering a scalable solution for IL screening inprocess design. This work highlights the effectiveness of combining simulateddata and transfer learning to overcome sparsity in the experimental data.</description>
      <author>example@mail.com (Sahil Sethi, Kai Sundmacher, Caroline Ganzer)</author>
      <guid isPermaLink="false">2509.10273v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Supervised and unsupervised learning with numerical computation for the Wolfram cellular automata</title>
      <link>http://arxiv.org/abs/2509.10209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了Wolfram细胞自动机的渐近密度和动态演化机制，通过结合数值模拟与机器学习方法分析了不同规则下的系统行为。&lt;h4&gt;背景&lt;/h4&gt;Wolfram细胞自动机使用八位二进制编码的一维三细胞邻域确定性更新规则，被广泛应用于研究自组织现象和复杂系统动力学。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探究Wolfram自动机的渐近密度和动态演化机制，识别不同规则下的配置特征，并探索初始条件对系统行为的影响。&lt;h4&gt;方法&lt;/h4&gt;研究采用数值模拟和计算方法，结合监督学习和无监督学习方法（如主成分分析和自编码器）来分析不同Wolfram规则的配置特征和系统行为。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了所选规则的渐近密度与初始密度之间的关系；某些Wolfram规则即使在从单个活动位点开始的情况下也能随时间生成相似的分形图案；监督学习方法有效识别各种Wolfram规则的配置，而无监督方法可以将不同规则的配置聚类到不同组中。&lt;h4&gt;结论&lt;/h4&gt;通过结合数值模拟与机器学习方法，研究成功揭示了Wolfram自动机的动态特性，为理解复杂系统行为提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;Wolfram细胞自动机具有一维三细胞邻域的局部规则由八位二进制表示，这些二进制编码确定性更新规则。这些自动机被广泛用于研究自组织现象和复杂系统的动力学。在这项工作中，我们采用数值模拟和计算方法来研究Wolfram自动机的渐近密度和动态演化机制。我们应用监督和无监督学习方法来识别与不同Wolfram规则相关的配置。此外，我们探索了替代初始条件，在这些条件下，某些Wolfram规则即使从单个活动位点开始也能随时间生成相似的分形图案。我们的结果揭示了所选规则的渐近密度与初始密度之间的关系。监督学习方法有效识别了各种Wolfram规则的配置，而无监督方法如主成分分析和自编码器可以将不同Wolfram规则的配置近似聚类到不同的组中，产生与模拟密度输出一致的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The local rules of Wolfram cellular automata with one-dimensional three-cellneighborhoods are represented by eight-bit binary that encode deterministicupdate rules. These automata are widely utilized to investigateself-organization phenomena and the dynamics of complex systems. In this work,we employ numerical simulations and computational methods to investigate theasymptotic density and dynamical evolution mechanisms in Wolfram automata. Weapply both supervised and unsupervised learning methods to identify theconfigurations associated with different Wolfram rules. Furthermore, we explorealternative initial conditions under which certain Wolfram rules generatesimilar fractal patterns over time, even when starting from a single activesite. Our results reveal the relationship between the asymptotic density andthe initial density of selected rules. The supervised learning methodseffectively identify the configurations of various Wolfram rules, whileunsupervised methods like principal component analysis and autoencoders canapproximately cluster configurations of different Wolfram rules into distinctgroups, yielding results that align well with simulated density outputs.</description>
      <author>example@mail.com (Kui Tuo, Shengfeng Deng, Yuxiang Yang, Yanyang Wang, Qiuping A. Wang, Wei Li, Wenjun Zhang)</author>
      <guid isPermaLink="false">2509.10209v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification</title>
      <link>http://arxiv.org/abs/2509.10082v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 4 tables, 5 figures, submitted to IEEE Journal of  Biomedical and Health Informatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FetalSleepNet是一个创新的深度学习方法，首次用于从羊胎儿脑电图(EEG)分类睡眠状态。通过迁移学习和频谱均衡域适应策略，实现了高准确率的睡眠阶段分类，准确率达86.6%，宏F1分数为62.5%，优于基线模型。该方法有潜力应用于临床胎儿监测，为早期发现妊娠并发症相关的脑发育异常提供工具。&lt;h4&gt;背景&lt;/h4&gt;羊胎儿脑电图(EEG)的获取复杂，解释起来困难且繁琐。然而，精确的睡眠阶段分类可能有助于早期发现与妊娠并发症（如缺氧或宫内生长受限）相关的异常脑发育。&lt;h4&gt;目的&lt;/h4&gt;开发一个深度学习方法来分类羊胎儿的睡眠状态，并应用于妊娠并发症的早期检测。&lt;h4&gt;方法&lt;/h4&gt;研究人员将EEG电极固定在24只晚期妊娠胎羊的顶叶皮层硬脑膜上。使用一种为成人EEG睡眠阶段分类开发的轻量级深度神经网络，通过从成人EEG进行迁移学习来训练该网络处理羊EEG，并采用基于频谱均衡的域适应策略来减少跨域不匹配。&lt;h4&gt;主要发现&lt;/h4&gt;直接迁移表现不佳，但完全微调结合频谱均衡取得了最佳整体性能（准确率：86.6%，宏F1分数：62.5%），优于基线模型。&lt;h4&gt;结论&lt;/h4&gt;FetalSleepNet是第一个专门为从胎儿EEG自动睡眠阶段分类开发的深度学习框架。它可以作为标记引擎，支持大规模弱/半监督标记和蒸馏，促进在临床中获取的侵入性较小信号（如多普勒超声或心电图数据）的训练。其轻量级设计使其非常适合部署在低功耗、实时和可穿戴的胎儿监测系统中。&lt;h4&gt;翻译&lt;/h4&gt;引言：本研究介绍了FetalSleepNet，这是首个已发表的使用深度学习方法从羊胎儿脑电图(EEG)分类睡眠状态的方法。胎儿EEG的获取复杂，解释起来困难且繁琐。然而，精确的睡眠阶段分类可能有助于早期发现与妊娠并发症相关的异常脑发育（如缺氧或宫内生长受限）。方法：将EEG电极固定在24只晚期妊娠胎羊的顶叶皮层硬脑膜上。使用一种为成人EEG睡眠阶段分类开发的轻量级深度神经网络，通过从成人EEG进行迁移学习来训练该网络处理羊EEG。采用基于频谱均衡的域适应策略来减少跨域不匹配。结果：我们证明了虽然直接迁移表现不佳，但完全微调结合频谱均衡取得了最佳整体性能（准确率：86.6%，宏F1分数：62.5%），优于基线模型。结论：据我们所知，FetalSleepNet是首个专门为从胎儿EEG自动睡眠阶段分类开发的深度学习框架。在实验室之外，基于EEG的睡眠阶段分类器可以作为标记引擎，支持大规模弱/半监督标记和蒸馏，促进在临床中获取的侵入性较小信号（如多普勒超声或心电图数据）的训练。FetalSleepNet的轻量级设计使其非常适合部署在低功耗、实时和可穿戴的胎儿监测系统中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Introduction: This study presents FetalSleepNet, the first published deeplearning approach to classifying sleep states from the ovineelectroencephalogram (EEG). Fetal EEG is complex to acquire and difficult andlaborious to interpret consistently. However, accurate sleep stageclassification may aid in the early detection of abnormal brain maturationassociated with pregnancy complications (e.g. hypoxia or intrauterine growthrestriction).  Methods: EEG electrodes were secured onto the ovine dura over the parietalcortices of 24 late gestation fetal sheep. A lightweight deep neural networkoriginally developed for adult EEG sleep staging was trained on the ovine EEGusing transfer learning from adult EEG. A spectral equalisation-based domainadaptation strategy was used to reduce cross-domain mismatch.  Results: We demonstrated that while direct transfer performed poorly, fullfine tuning combined with spectral equalisation achieved the best overallperformance (accuracy: 86.6 percent, macro F1-score: 62.5), outperformingbaseline models.  Conclusions: To the best of our knowledge, FetalSleepNet is the first deeplearning framework specifically developed for automated sleep staging from thefetal EEG. Beyond the laboratory, the EEG-based sleep stage classifierfunctions as a label engine, enabling large scale weak/semi supervised labelingand distillation to facilitate training on less invasive signals that can beacquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.FetalSleepNet's lightweight design makes it well suited for deployment in lowpower, real time, and wearable fetal monitoring systems.</description>
      <author>example@mail.com (Weitao Tang, Johann Vargas-Calixto, Nasim Katebi, Nhi Tran, Sharmony B. Kelly, Gari D. Clifford, Robert Galinsky, Faezeh Marzbanrad)</author>
      <guid isPermaLink="false">2509.10082v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference</title>
      <link>http://arxiv.org/abs/2509.09747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了D-CAT（解耦跨注意力迁移）框架，用于改进多模态分类模型，特别是在人机协作中的人类活动识别，解决了资源受限环境中部署的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的跨模态迁移学习方法需要在训练和推理阶段都需要成对的传感器数据，限制了在资源受限环境中的应用，因为在这些环境中完整传感器套件在经济和技术上都不实用。&lt;h4&gt;目的&lt;/h4&gt;提出一个不需要在推理阶段联合传感器模态的框架，解决资源受限环境中的多模态分类问题，同时保持准确性并减少硬件冗余。&lt;h4&gt;方法&lt;/h4&gt;D-CAT框架结合了一个用于特征提取的自注意力模块和一个新颖的跨注意力对齐损失，强制对齐传感器特征空间，而不需要两个模态的分类管道耦合。&lt;h4&gt;主要发现&lt;/h4&gt;在分布内场景中，从高性能模态迁移可获得高达10%的F1分数提升；在分布外场景中，即使较弱的源模态也能改善目标性能，只要目标模型没有在训练数据上过拟合；D-CAT通过跨模态知识实现单传感器推理，减少了感知系统的硬件冗余。&lt;h4&gt;结论&lt;/h4&gt;D-CAT框架能够在保持准确性的同时减少感知系统的硬件冗余，这对于成本敏感或适应性部署（如传感器可用性变化的家庭辅助机器人）至关重要。&lt;h4&gt;翻译&lt;/h4&gt;跨模态迁移学习用于改进多模态分类模型（例如人机协作中的人类活动识别）。然而，现有方法在训练和推理阶段都需要成对的传感器数据，限制了在资源受限环境中的部署，在这些环境中完整传感器套件在经济和技术上都不实用。为解决这一问题，我们提出了D-CAT（解耦跨注意力迁移）框架，该框架对齐模态特定表示，不需要在推理阶段联合传感器模态。我们的方法将自注意力模块用于特征提取，结合了一个新颖的跨注意力对齐损失，强制对齐传感器特征空间，而不需要两个模态分类管道的耦合。我们在三个多模态人类活动数据集（IMU、视频和音频）上评估了D-CAT，包括分布内和分布外场景，并与单模态模型进行比较。结果表明，在分布内场景中，从高性能模态（如视频到IMU）迁移可获得高达10%的F1分数提升。在分布外场景中，即使较弱的源模态（如IMU到视频）也能改善目标性能，只要目标模型没有在训练数据上过拟合。通过实现单传感器推理与跨模态知识，D-CAT减少了感知系统的硬件冗余，同时保持准确性，这对于成本敏感或适应性部署（如传感器可用性变化的家庭辅助机器人）至关重要。代码可在https://github.com/Schindler-EPFL-Lab/D-CAT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-modal transfer learning is used to improve multi-modal classificationmodels (e.g., for human activity recognition in human-robot collaboration).However, existing methods require paired sensor data at both training andinference, limiting deployment in resource-constrained environments where fullsensor suites are not economically and technically usable. To address this, wepropose Decoupled Cross-Attention Transfer (D-CAT), a framework that alignsmodality-specific representations without requiring joint sensor modalityduring inference. Our approach combines a self-attention module for featureextraction with a novel cross-attention alignment loss, which enforces thealignment of sensors' feature spaces without requiring the coupling of theclassification pipelines of both modalities. We evaluate D-CAT on threemulti-modal human activity datasets (IMU, video, and audio) under bothin-distribution and out-of-distribution scenarios, comparing against uni-modalmodels. Results show that in in-distribution scenarios, transferring fromhigh-performing modalities (e.g., video to IMU) yields up to 10% F1-score gainsover uni-modal training. In out-of-distribution scenarios, even weaker sourcemodalities (e.g., IMU to video) improve target performance, as long as thetarget model isn't overfitted on the training data. By enabling single-sensorinference with cross-modal knowledge, D-CAT reduces hardware redundancy forperception systems while maintaining accuracy, which is critical forcost-sensitive or adaptive deployments (e.g., assistive robots in homes withvariable sensor availability). Code is available athttps://github.com/Schindler-EPFL-Lab/D-CAT.</description>
      <author>example@mail.com (Leen Daher, Zhaobo Wang, Malcolm Mielle)</author>
      <guid isPermaLink="false">2509.09747v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Pipeline for Aortic Segmentation and Shape Analysis</title>
      <link>http://arxiv.org/abs/2509.09718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  STACOM 2025 with MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种稳健、全自动的主动脉形状分析流程，结合深度学习和统计技术，从心脏MRI中提取主动脉形状信息。&lt;h4&gt;背景&lt;/h4&gt;主动脉形状分析在心血管诊断、治疗规划和理解疾病进展中起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健、全自动的主动脉形状分析流程，提高分析精度和效率。&lt;h4&gt;方法&lt;/h4&gt;结合深度学习和统计技术，包括分割、3D表面重建和网格配准；对比nnUNet、TotalSegmentator和MedSAM2等分割模型；重建高质量3D网格；引入基于深度学习的网格配准方法，直接优化顶点位移。&lt;h4&gt;主要发现&lt;/h4&gt;新方法在几何精度和解剖一致性上显著优于传统方法；主成分分析揭示了主动脉形状变化的主导模式，捕获了全局形态和局部结构差异。&lt;h4&gt;结论&lt;/h4&gt;整合传统几何处理与基于学习模型的优势，为解剖精确且可扩展的主动脉分析提供基础，支持心血管医学个性化诊断的发展。&lt;h4&gt;翻译&lt;/h4&gt;主动脉形状分析在心血管诊断、治疗规划和理解疾病进展中起着关键作用。我们提出了一种稳健的、全自动的主动脉形状分析流程，从心脏MRI中结合深度学习和统计技术，包括分割、3D表面重建和网格配准。我们基准测试了领先的分割模型，包括nnUNet、TotalSegmentator和MedSAM2，强调了在特定数据集上进行领域特定训练和迁移学习的有效性。分割后，我们重建高质量的3D网格，并引入一种基于深度学习的网格配准方法，直接优化顶点位移。这种方法在几何精度和解剖一致性上显著优于传统的刚性和非刚性方法。使用配准后的网格，我们对599名健康受试者进行统计形状分析。主成分分析揭示了主动脉形状变化的主导模式，捕获了刚性和相似变换下的全局形态和局部结构差异。我们的研究结果证明了整合传统几何处理与基于学习模型的优势，用于解剖精确且可扩展的主动脉分析。这项工作为未来研究病理形状偏差和心血管医学个性化诊断的发展奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从心脏MRI图像中进行主动脉分割和形状分析的自动化流程问题。由于MRI图像中主动脉与周围组织对比度低、分辨率低，导致分割困难，且传统表面重建和配准方法存在诸多挑战。这个问题在现实中很重要，因为准确的主动脉形状分析对心血管疾病诊断、治疗规划和疾病进展监测至关重要，能帮助建立健康参考模型以检测病理性偏差，支持个性化医疗发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为三个主要步骤：分割、3D表面重建和网格配准，针对每个步骤设计解决方案。他们系统评估了现有技术，包括nn-UNet、TotalSegmentator等分割模型，以及传统表面重建和配准方法。在此基础上，作者借鉴了现有的深度学习分割模型和经典几何处理算法，并在网格配准方面提出了创新，设计了一个结合统计方法和深度学习技术的集成管道。这种设计思路体现了对现有技术的批判性评估和有针对性的改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个全自动化的管道，结合深度学习和统计技术，通过领域特定训练和迁移学习提高分割性能，使用基于深度学习的网格配准方法直接优化顶点位移以实现更高精度，并利用统计形状分析揭示主动脉形状变化的主导模式。整体流程包括：数据准备与预处理；多种分割模型评估与选择；3D表面重建（等值面提取、顶点标准化、法线处理、网格重建）；模板选择与对齐；网格配准（比较传统方法并提出深度学习方法）；最后对599名健康受试者进行主成分分析以识别形状变化模式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了完整的端到端自动化管道；2) 系统评估并优化了分割方法，发现nn-UNet表现最佳；3) 创新性地提出基于深度学习的网格配准方法，直接优化顶点位移；4) 在大型健康队列上进行统计分析，识别主动脉形状变化模式。相比之前工作，本文提供了更全面的解决方案，在配准精度上显著优于传统方法，使用了更大的数据集进行统计分析，并成功整合了传统几何处理与基于学习的方法，实现了既解剖精确又可扩展的主动脉分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合深度学习和统计技术的全自动管道，实现了从心脏MRI中精确分割、重建和配准主动脉，并揭示了健康人群主动脉形状变化的主要模式，为心血管疾病的个性化诊断提供了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aortic shape analysis plays a key role in cardiovascular diagnostics,treatment planning, and understanding disease progression. We present a robust,fully automated pipeline for aortic shape analysis from cardiac MRI, combiningdeep learning and statistical techniques across segmentation, 3D surfacereconstruction, and mesh registration. We benchmark leading segmentation modelsincluding nnUNet, TotalSegmentator, and MedSAM2 highlighting the effectivenessof domain specific training and transfer learning on a curated dataset.Following segmentation, we reconstruct high quality 3D meshes and introduce aDL based mesh registration method that directly optimises vertex displacements.This approach significantly outperforms classical rigid and nonrigid methods ingeometric accuracy and anatomical consistency. Using the registered meshes, weperform statistical shape analysis on a cohort of 599 healthy subjects.Principal Component Analysis reveals dominant modes of aortic shape variation,capturing both global morphology and local structural differences under rigidand similarity transformations. Our findings demonstrate the advantages ofintegrating traditional geometry processing with learning based models foranatomically precise and scalable aortic analysis. This work lays thegroundwork for future studies into pathological shape deviations and supportsthe development of personalised diagnostics in cardiovascular medicine.</description>
      <author>example@mail.com (Nairouz Shehata, Amr Elsawy, Mohamed Nagy, Muhammad ElMahdy, Mariam Ali, Soha Romeih, Heba Aguib, Magdi Yacoub, Ben Glocker)</author>
      <guid isPermaLink="false">2509.09718v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets</title>
      <link>http://arxiv.org/abs/2509.10453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过采用三种先进的时序自监督学习方法处理3D脑部MRI数据，解决了阿尔茨海默病预测中标记数据缺乏、跨数据集泛化能力差以及输入灵活性不足的问题。模型在四个数据集的3161名患者上进行了预训练，并在多种预测任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病是一种导致记忆丧失和认知能力下降的进行性神经退行性疾病。现有的深度学习模型应用于阿尔茨海默病预测时受限于标记数据缺乏、跨数据集泛化能力差，以及无法处理不同数量的输入扫描和扫描间时间间隔。&lt;h4&gt;目的&lt;/h4&gt;适应三种先进的时序自监督学习方法用于3D脑部MRI分析，并添加新型扩展以处理可变长度输入和鲁棒空间特征学习。&lt;h4&gt;方法&lt;/h4&gt;聚合四个公开数据集，包含3161名患者进行预训练，评估模型在阿尔茨海默病多种预测任务中的性能，包括诊断分类、转化检测和未来转化预测。&lt;h4&gt;主要发现&lt;/h4&gt;采用时序顺序预测和对比学习的自监督学习模型在七个下游任务中的六个表现优于监督学习。该模型展示了跨任务和不同数量输入图像（具有不同时间间隔）的适应性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该自监督学习模型在临床应用中表现出强大的适应性和鲁棒性能，作者已公开代码和模型供进一步研究使用。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病是一种进行性神经退行性疾病，会导致记忆丧失和认知能力下降。尽管已有大量研究将深度学习模型应用于阿尔茨海默病预测任务，但这些模型仍受限于可用标记数据的缺乏、跨数据集的泛化能力差，以及无法处理不同数量的输入扫描和扫描间时间间隔。在本研究中，我们调整了三种最先进的时序自学习方法用于3D脑部MRI分析，并添加了新型扩展以处理可变长度输入和鲁棒空间特征学习。我们聚合了四个包含3161名患者的公开数据集进行预训练，展示了我们的模型在多种阿尔茨海默病预测任务中的性能，包括诊断分类、转化检测和未来转化预测。重要的是，我们采用时序顺序预测和对比学习的自监督模型在七个下游任务中的六个上优于监督学习。它展示了跨任务和不同数量输入图像（具有不同时间间隔）的适应性和泛化能力，突显了其在临床应用中保持鲁棒性能的能力。我们在https://github.com/emilykaczmarek/SSL-AD公开了我们的代码和模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决阿尔茨海默病预测中深度学习模型的三大局限性：缺乏足够标记数据、模型在不同数据集间泛化能力差、无法灵活处理不同数量的输入扫描图像和时间间隔。这个问题在现实中非常重要，因为阿尔茨海默病是一种进行性疾病，早期准确预测对治疗干预至关重要，而现有模型受限于数据不足和临床场景的多样性，难以充分利用患者多次就诊的完整信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到阿尔茨海默病作为随时间发展的疾病，其预测需要模型能捕捉时间变化特征。他们发现自监督学习可解决标记数据不足问题，但现有SSL方法在阿尔茨海默病应用中存在三大局限。因此，他们借鉴了计算机视觉领域的时间顺序验证和预测方法，结合对比学习技术，设计了三种模型：时间顺序验证(SSL-TOV)、时间顺序预测(SSL-TOP)和结合对比学习的SSL-TOPC。他们创新性地扩展了这些方法以处理医学影像的特殊性，如可变长度输入和3D脑MRI分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自监督学习从大量未标记脑MRI数据中学习通用表示，利用时间顺序预测任务使模型关注疾病进展的微妙变化，并结合对比学习增强空间特征鲁棒性。整体流程包括：1)聚合四个公开数据集共3,161名患者进行预训练；2)对数据进行标准化预处理；3)设计三种模型分别进行时间顺序验证、预测和结合对比学习；4)在七个下游任务(如分类、转换检测、未来转换预测)上评估模型性能。模型能处理1-4个输入图像，适应不同时间间隔，并在大多数任务上优于监督学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)整合四个大规模数据集进行预训练；2)创新性地将时间顺序预测应用于3D脑MRI分析；3)设计专门方法处理可变长度输入，无需填充；4)结合时间顺序预测和对比学习增强时空特征；5)在七个不同任务上全面评估模型性能。相比之前工作，本研究首次提出完全可适应、可泛化的自监督框架，解决了现有SSL模型缺乏大型数据集、忽略时间学习和无法处理可变输入的问题，特别是SSL-TOPC模型在六个 out of seven 的下游任务上超越了监督学习基线。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的时空自监督学习框架，通过整合大规模脑MRI数据和时间顺序预测任务，显著提高了阿尔茨海默病预测模型的泛化能力和适应性，特别是在标记数据有限和临床应用场景多变的情况下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alzheimer's disease is a progressive, neurodegenerative disorder that causesmemory loss and cognitive decline. While there has been extensive research inapplying deep learning models to Alzheimer's prediction tasks, these modelsremain limited by lack of available labeled data, poor generalization acrossdatasets, and inflexibility to varying numbers of input scans and timeintervals between scans. In this study, we adapt three state-of-the-arttemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,and add novel extensions designed to handle variable-length inputs and learnrobust spatial features. We aggregate four publicly available datasetscomprising 3,161 patients for pre-training, and show the performance of ourmodel across multiple Alzheimer's prediction tasks including diagnosisclassification, conversion detection, and future conversion prediction.Importantly, our SSL model implemented with temporal order prediction andcontrastive learning outperforms supervised learning on six out of sevendownstream tasks. It demonstrates adaptability and generalizability acrosstasks and number of input images with varying time intervals, highlighting itscapacity for robust performance across clinical applications. We release ourcode and model publicly at https://github.com/emilykaczmarek/SSL-AD.</description>
      <author>example@mail.com (Emily Kaczmarek, Justin Szeto, Brennan Nichyporuk, Tal Arbel)</author>
      <guid isPermaLink="false">2509.10453v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms</title>
      <link>http://arxiv.org/abs/2509.10369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review at npj Digital Medicine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;对比学习对队列组成的依赖性是一个重要但未被充分探索的问题。CAPE基础模型和IDB策略解决了多中心、多样化队列预训练中的挑战，队列组成对模型性能有显著影响，需要考虑临床公平性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;对比学习是一种广泛采用的自我监督预训练策略，但其对队列组成的依赖性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;系统评估队列人口统计、健康状态和人口多样性如何影响下游预测性能，开发一种临床公平且可推广的基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出了基于患者增强心电图(CAPE)的基础模型，在四个队列(n = 5,203,352)上进行预训练，这些队列来自三个大洲(北美、南美、亚洲)的多样化人群；评估了两个来自欧洲的额外队列的预测任务；提出了In-Distribution Batch (IDB)策略，在预训练期间保留队列内部一致性并增强OOD鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;下游性能取决于预训练队列的分布特性，包括人口统计和健康状态；使用多中心、人口统计多样化的队列进行预训练可以提高分布内准确率，但通过编码队列特定的人工制品，降低了对比方法的分布外(OOD)泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作为开发临床公平且可推广的基础模型提供了重要见解。&lt;h4&gt;翻译&lt;/h4&gt;对比学习是一种广泛采用的自我监督预训练策略，但其对队列组成的依赖性仍未得到充分探索。我们提出了基于患者增强心电图(CAPE)的基础模型，并在来自三个大洲(北美、南美、亚洲)的多样化人群中，在四个队列(n = 5,203,352)上进行预训练。我们系统评估了队列人口统计、健康状态和人口多样性如何影响预测任务的下游性能，这些任务还包括来自另一个大陆(欧洲)的两个额外队列。我们发现下游性能取决于预训练队列的分布特性，包括人口统计和健康状态。此外，虽然使用多中心、人口统计多样化的队列进行预训练可以提高分布内准确率，但它通过编码队列特定的人工制品，降低了我们对比方法的分布外(OOD)泛化能力。为解决这一问题，我们提出了In-Distribution Batch (IDB)策略，该策略在预训练期间保留队列内部一致性并增强OOD鲁棒性。这项工作为开发临床公平且可推广的基础模型提供了重要见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning is a widely adopted self-supervised pretrainingstrategy, yet its dependence on cohort composition remains underexplored. Wepresent Contrasting by Patient Augmented Electrocardiograms (CAPE) foundationmodel and pretrain on four cohorts (n = 5,203,352), from diverse populationsacross three continents (North America, South America, Asia). We systematicallyassess how cohort demographics, health status, and population diversityinfluence the downstream performance for prediction tasks also including twoadditional cohorts from another continent (Europe). We find that downstreamperformance depends on the distributional properties of the pretraining cohort,including demographics and health status. Moreover, while pretraining with amulti-centre, demographically diverse cohort improves in-distribution accuracy,it reduces out-of-distribution (OOD) generalisation of our contrastive approachby encoding cohort-specific artifacts. To address this, we propose theIn-Distribution Batch (IDB) strategy, which preserves intra-cohort consistencyduring pretraining and enhances OOD robustness. This work provides importantinsights for developing clinically fair and generalisable foundation models.</description>
      <author>example@mail.com (Gul Rukh Khattak, Konstantinos Patlatzoglou, Joseph Barker, Libor Pastika, Boroumand Zeidaabadi, Ahmed El-Medany, Hesham Aggour, Yixiu Liang, Antonio H. Ribeiro, Jeffrey Annis, Antonio Luiz Pinho Ribeiro, Junbo Ge, Daniel B. Kramer, Jonathan W. Waks, Evan Brittain, Nicholas Peters, Fu Siong Ng, Arunashis Sau)</author>
      <guid isPermaLink="false">2509.10369v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography</title>
      <link>http://arxiv.org/abs/2509.10344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLAM的新型深度学习方法，用于改进乳腺X光筛查中的视觉语言模型预训练，通过利用多视图成像过程的先验知识，学习局部跨视图对齐和细粒度特征，在多个数据集上超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;乳腺X光筛查是早期发现乳腺癌的重要工具，深度学习可提高其解释速度和准确性。然而，视觉语言模型的发展受限于数据有限和自然图像与医学图像间的领域差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效建模乳腺X光多视图关系的视觉语言模型，解决现有方法忽略领域特定特征和多视图关系的问题。&lt;h4&gt;方法&lt;/h4&gt;提出GLAM模型：全局和局部对齐的多视图乳腺X光模型，利用乳腺X光多视图成像过程的先验知识，通过联合全局和局部、视觉-视觉以及视觉语言对比学习来学习局部跨视图对齐和细粒度局部特征，并在EMBED数据集上预训练。&lt;h4&gt;主要发现&lt;/h4&gt;GLAM模型在不同设置下的多个数据集上均优于现有基线方法，表明其能有效捕捉乳腺X光的多视图关系和领域特定特征。&lt;h4&gt;结论&lt;/h4&gt;通过利用乳腺X光成像过程的先验知识和多视图关系，GLAM模型改进了视觉语言模型在乳腺X光筛查中的应用效果，为乳腺癌早期检测提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;乳腺X光筛查是早期发现乳腺癌的重要工具。深度学习方法有望提高乳腺X光解释的速度和准确性。然而，基础视觉语言模型(VLM)的发展受到数据有限和自然图像与医学图像之间领域差异的阻碍。现有的乳腺X光VLM通常从自然图像改编而来，忽略了多视图等特定领域特征。与放射科医生不同，当前方法将两个视图视为独立图像或未正确建模多视图对应关系学习，导致失去关键几何上下文和次优预测。我们提出GLAM：利用几何引导进行VLM预训练的全局和局部对齐多视图乳腺X光模型。通过利用乳腺X光多视图成像过程的先验知识，我们的模型通过联合全局和局部、视觉-视觉以及视觉语言对比学习，学习局部跨视图对齐和细粒度局部特征。在最大的开放乳腺X光数据集之一EMBED上预训练后，我们的模型在不同设置下的多个数据集上都优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mammography screening is an essential tool for early detection of breastcancer. The speed and accuracy of mammography interpretation have the potentialto be improved with deep learning methods. However, the development of afoundation visual language model (VLM) is hindered by limited data and domaindifferences between natural and medical images. Existing mammography VLMs,adapted from natural images, often ignore domain-specific characteristics, suchas multi-view relationships in mammography. Unlike radiologists who analyzeboth views together to process ipsilateral correspondence, current methodstreat them as independent images or do not properly model the multi-viewcorrespondence learning, losing critical geometric context and resulting insuboptimal prediction. We propose GLAM: Global and Local Alignment forMulti-view mammography for VLM pretraining using geometry guidance. Byleveraging the prior knowledge about the multi-view imaging process ofmammograms, our model learns local cross-view alignments and fine-grained localfeatures through joint global and local, visual-visual, and visual-languagecontrastive learning. Pretrained on EMBED [14], one of the largest openmammography datasets, our model outperforms baselines across multiple datasetsunder different settings.</description>
      <author>example@mail.com (Yuexi Du, Lihui Chen, Nicha C. Dvornek)</author>
      <guid isPermaLink="false">2509.10344v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion</title>
      <link>http://arxiv.org/abs/2509.10266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SignClip是一种新的手语翻译框架，通过融合手动和非手动线索（空间手势和嘴唇运动特征），并采用层次对比学习框架，显著提高了手语翻译的准确性。&lt;h4&gt;背景&lt;/h4&gt;手语翻译旨在从手语视频中翻译自然语言，作为包容性交流的重要桥梁。最近的进展利用了强大的视觉骨干网络和大语言模型，但大多数方法主要关注手动信号（手部动作）而往往忽略非手动线索如口型。&lt;h4&gt;目的&lt;/h4&gt;提出SignClip框架，提高手语翻译的准确性。&lt;h4&gt;方法&lt;/h4&gt;SignClip融合手动和非手动线索，特别是空间手势和嘴唇运动特征。此外，SignClip引入了具有多级对齐目标的层次对比学习框架，确保手语-嘴唇和视觉-文本模态之间的语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在PHOENIX14T和How2Sign两个基准数据集上的大量实验证明了该方法的优势。例如，在PHOENIX14T上，Gloss-free设置中，SignClip超越了之前的最先进模型SpaMo，将BLEU-4从24.32提高到24.71，ROUGE从46.57提高到48.38。&lt;h4&gt;结论&lt;/h4&gt;SignClip通过融合手势和口型特征，并采用层次对比学习框架，显著提高了手语翻译的准确性。&lt;h4&gt;翻译&lt;/h4&gt;手语翻译旨在从手语视频中翻译自然语言，作为包容性交流的重要桥梁。尽管最近的进展利用了强大的视觉骨干网络和大语言模型，但大多数方法主要关注手动信号（手部动作）而往往忽略非手动线索如口型。事实上，口型在手语中传达重要的语言信息，并在区分视觉相似的手势中起关键作用。本文提出SignClip，一种新的手语翻译框架，通过融合手动和非手动线索（特别是空间手势和嘴唇运动特征）来提高手语翻译的准确性。此外，SignClip引入了具有多级对齐目标的层次对比学习框架，确保手语-嘴唇和视觉-文本模态之间的语义一致性。在PHOENIX14T和How2Sign两个基准数据集上的大量实验证明了该方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language translation (SLT) aims to translate natural language from signlanguage videos, serving as a vital bridge for inclusive communication. Whilerecent advances leverage powerful visual backbones and large language models,most approaches mainly focus on manual signals (hand gestures) and tend tooverlook non-manual cues like mouthing. In fact, mouthing conveys essentiallinguistic information in sign languages and plays a crucial role indisambiguating visually similar signs. In this paper, we propose SignClip, anovel framework to improve the accuracy of sign language translation. It fusesmanual and non-manual cues, specifically spatial gesture and lip movementfeatures. Besides, SignClip introduces a hierarchical contrastive learningframework with multi-level alignment objectives, ensuring semantic consistencyacross sign-lip and visual-text modalities. Extensive experiments on twobenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of ourapproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClipsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from24.32 to 24.71, and ROUGE from 46.57 to 48.38.</description>
      <author>example@mail.com (Wenfang Wu, Tingting Yuan, Yupeng Li, Daling Wang, Xiaoming Fu)</author>
      <guid isPermaLink="false">2509.10266v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning</title>
      <link>http://arxiv.org/abs/2509.10208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SI FACT的自我改进框架，用于解决大型语言模型在知识密集型任务中生成不忠实响应的问题。该框架通过自我指导机制生成对比学习数据，并应用对比学习训练模型，显著提高了模型的上下文忠实性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在知识密集型任务中经常生成不忠实的响应，这是因为知识冲突，即模型倾向于依赖内部参数知识而非提供的上下文。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在知识密集型任务中由于知识冲突而产生的不忠实响应问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Self Improving Faithfulness Aware Contrastive Tuning(SI FACT)的自我改进框架，该框架使用自我指导机制让基础LLM自动生成高质量的、结构化的对比学习数据，包括锚样本、语义等价正样本和模拟不忠实场景的负样本，然后应用对比学习训练模型，使其在表示空间中拉近忠实响应，推远不忠实响应。&lt;h4&gt;主要发现&lt;/h4&gt;在知识冲突评估基准ECARE KRE和COSE KRE上的实验表明，基于Llama3 8B Instruct的SI FACT模型比最佳基线方法提高了6.2%的上下文召回率，同时显著减少了对内部记忆的依赖。&lt;h4&gt;结论&lt;/h4&gt;SI FACT在增强LLM的上下文忠实性方面提供了有效性和高数据效率，为构建更加主动和可信赖的语言模型提供了实用途径。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在知识密集型任务中经常因知识冲突而产生不忠实响应，即倾向于依赖内部参数知识而非提供的上下文。为解决这一问题，我们提出了一种新颖的自我改进框架——自我提高忠实性感知对比调优(SI FACT)。该框架使用自我指导机制，使基础LLM能够自动生成高质量、结构化的对比学习数据，包括锚样本、语义等价正样本和模拟不忠实场景的负样本。这种方法显著降低了手动标注成本。随后，应用对比学习训练模型，使其在表示空间中拉近忠实响应，推远不忠实响应。在知识冲突评估基准ECARE KRE和COSE KRE上的实验表明，基于Llama3 8B Instruct的SI FACT模型比最佳基线方法提高了6.2%的上下文召回率，同时显著减少了对内部记忆的依赖。结果表明，SI FACT在增强LLM的上下文忠实性方面提供了强大的有效性和高数据效率，为构建更加主动和可信赖的语言模型提供了实用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models often generate unfaithful responses in knowledgeintensive tasks due to knowledge conflict,that is,a preference for relying oninternal parametric knowledge rather than the provided context.To address thisissue,we propose a novel self improving framework,Self Improving FaithfulnessAware Contrastive Tuning.The framework uses a self instruct mechanism thatallows the base LLM to automatically generate high quality,structuredcontrastive learning data,including anchor samples,semantically equivalentpositive samples,and negative samples simulating unfaithful scenarios.Thisapproach significantly reduces the cost of manualannotation.Subsequently,contrastive learning is applied to train themodel,enabling it to pull faithful responses closer and push unfaithfulresponses farther apart in the representation space.Experiments on knowledgeconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACTmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%over the best baseline method,while significantly reducing dependence oninternal memory.The results indicate that SI FACT provides strong effectivenessand high data efficiency in enhancing the contextual faithfulness ofLLMs,offering a practical pathway toward building more proactive andtrustworthy language models.</description>
      <author>example@mail.com (Shengqiang Fu)</author>
      <guid isPermaLink="false">2509.10208v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment</title>
      <link>http://arxiv.org/abs/2509.10134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Grad-CL的新型无源域适应框架，用于解决视盘和视杯分割在不同成像条件下的适应性问题。该框架结合了梯度引导的伪标签细化和基于余弦相似度的对比学习策略，无需访问原始源数据即可实现稳健的分割性能。&lt;h4&gt;背景&lt;/h4&gt;视盘和视杯的准确分割对于眼科疾病（如青光眼）的早期诊断和管理至关重要。然而，在一个数据集上训练的分割模型在应用于不同成像协议或条件下获取的目标数据时，往往会经历显著的性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决分割模型在不同成像条件下性能下降的问题，提出一种无需访问原始源数据的无源域适应框架，实现视盘和视杯分割的稳健适应。&lt;h4&gt;方法&lt;/h4&gt;Grad-CL框架结合两个主要阶段：第一阶段通过基于梯度的机制提取显著的类别特定特征，实现更准确的不确定性量化和稳健的原型估计，用于细化有噪声的伪标签；第二阶段采用基于余弦相似度的对比损失，明确强制执行视杯和视盘的梯度引导特征之间的类间可分离性。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的跨域眼底成像数据集上进行的大量实验表明，Grad-CL优于最先进的无监督和无源域适应方法，实现了 superior 的分割精度和改进的边界描绘。&lt;h4&gt;结论&lt;/h4&gt;Grad-CL是一种有效的无源域适应方法，可以解决视盘和视杯分割在不同成像条件下的适应性问题，无需原始源数据。&lt;h4&gt;翻译&lt;/h4&gt;准确的视盘和视杯分割对于眼科疾病（如青光眼）的早期诊断和管理至关重要。然而，在一个数据集上训练的分割模型在应用于不同成像协议或条件下获取的目标数据时，往往会经历显著的性能下降。为应对这一挑战，我们提出Grad-CL，一种新颖的无源域适应框架，它利用预训练的源模型和无标签的目标数据，无需访问原始源数据即可稳健地适应分割性能。Grad-CL结合了基于梯度引导的伪标签细化模块和基于余弦相似度的对比学习策略。在第一阶段，通过基于梯度的机制提取显著的类别特定特征，实现更准确的不确定性量化和稳健的原型估计，用于细化有噪声的伪标签。在第二阶段，采用基于余弦相似度的对比损失，明确强制执行视杯和视盘的梯度引导特征之间的类间可分离性。在具有挑战性的跨域眼底成像数据集上进行的大量实验表明，Grad-CL优于最先进的无监督和无源域适应方法，实现了 superior 的分割精度和改进的边界描绘。项目和代码可在https://visdomlab.github.io/GCL/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of the optic disc and cup is critical for the earlydiagnosis and management of ocular diseases such as glaucoma. However,segmentation models trained on one dataset often suffer significant performancedegradation when applied to target data acquired under different imagingprotocols or conditions. To address this challenge, we propose\textbf{Grad-CL}, a novel source-free domain adaptation framework thatleverages a pre-trained source model and unlabeled target data to robustlyadapt segmentation performance without requiring access to the original sourcedata. Grad-CL combines a gradient-guided pseudolabel refinement module with acosine similarity-based contrastive learning strategy. In the first stage,salient class-specific features are extracted via a gradient-based mechanism,enabling more accurate uncertainty quantification and robust prototypeestimation for refining noisy pseudolabels. In the second stage, a contrastiveloss based on cosine similarity is employed to explicitly enforce inter-classseparability between the gradient-informed features of the optic cup and disc.Extensive experiments on challenging cross-domain fundus imaging datasetsdemonstrate that Grad-CL outperforms state-of-the-art unsupervised andsource-free domain adaptation methods, achieving superior segmentation accuracyand improved boundary delineation. Project and code are available athttps://visdomlab.github.io/GCL/.</description>
      <author>example@mail.com (Rini Smita Thakur, Rajeev Ranjan Dwivedi, Vinod K Kurmi)</author>
      <guid isPermaLink="false">2509.10134v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Prototypical Contrastive Learning For Improved Few-Shot Audio Classification</title>
      <link>http://arxiv.org/abs/2509.10074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and Presented at IEEE International Workshop on Machine  Learning for Signal Processing, Aug.\ 31-- Sep.\ 3, 2025, Istanbul, Turkey ,  6 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探索了在音频分类中将监督对比损失整合到原型小样本训练中的方法，通过结合SpecAugment和自注意力机制，实现了在5路5 shot设置下的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;小样本学习已成为处理标记数据有限场景的有效方法，但在图像领域已有大量研究，而音频分类中的小样本学习仍然相对未被充分探索。大规模数据标注在某些场景下不切实际，因此需要开发小样本学习方法。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探索监督对比损失在音频分类小样本学习中的应用，并比较不同对比损失（标准对比损失与角度损失）的性能，同时开发一种结合SpecAugment和自注意力机制的新方法。&lt;h4&gt;方法&lt;/h4&gt;研究将监督对比损失整合到原型小样本训练框架中，特别采用了角度损失而非标准对比损失。方法利用SpecAugment进行数据增强，然后通过自注意力机制将增强输入版本的多样化信息封装到一个统一的嵌入中。实验在MetaAudio基准数据集上进行，该数据集包含五个预定义分割的标准化数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，与标准对比损失相比，角度损失能进一步提高性能。提出的结合SpecAugment和自注意力机制的方法在5路5 shot设置下实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;将监督对比损失（特别是角度损失）整合到原型小样本训练中，结合SpecAugment和自注意力机制，是音频分类小样本学习的有效方法，能够达到当前最先进的性能水平。&lt;h4&gt;翻译&lt;/h4&gt;小样本学习已成为一种强大的范式，用于训练标记数据有限的模型，解决了大规模标注不切实际场景中的挑战。尽管图像领域已有大量研究，但音频分类中的小样本学习仍然相对未被探索。在这项工作中，我们研究了将监督对比损失整合到原型小样本训练中对音频分类的影响。具体而言，我们证明与标准对比损失相比，角度损失能进一步提高性能。我们的方法利用SpecAugment，然后通过自注意力机制，将增强输入版本的多样化信息封装到一个统一的嵌入中。我们在MetaAudio上评估了我们的方法，这是一个包含五个数据集的基准，具有预定义的分割、标准化的预处理和一套全面的用于比较的小样本学习模型。在5路、5 shot设置下，我们提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot learning has emerged as a powerful paradigm for training models withlimited labeled data, addressing challenges in scenarios where large-scaleannotation is impractical. While extensive research has been conducted in theimage domain, few-shot learning in audio classification remains relativelyunderexplored. In this work, we investigate the effect of integratingsupervised contrastive loss into prototypical few shot training for audioclassification. In detail, we demonstrate that angular loss further improvesthe performance compared to the standard contrastive loss. Our method leveragesSpecAugment followed by a self-attention mechanism to encapsulate diverseinformation of augmented input versions into one unified embedding. We evaluateour approach on MetaAudio, a benchmark including five datasets with predefinedsplits, standardized preprocessing, and a comprehensive set of few-shotlearning models for comparison. The proposed approach achieves state-of-the-artperformance in a 5-way, 5-shot setting.</description>
      <author>example@mail.com (Christos Sgouropoulos, Christos Nikou, Stefanos Vlachos, Vasileios Theiou, Christos Foukanelis, Theodoros Giannakopoulos)</author>
      <guid isPermaLink="false">2509.10074v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>On Syntactical Simplification of Temporal Operators in Negation-free MTL</title>
      <link>http://arxiv.org/abs/2509.10146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了无否定MTL时态逻辑框架的表达能力，发现可以仅使用'until'和'since'算子构建强大的时态逻辑片段，挑战了否定对于表达普遍时态约束的必要性假设。&lt;h4&gt;背景&lt;/h4&gt;在动态、数据密集型环境中进行时态推理需要表达性强且可处理的逻辑框架。传统方法依赖否定表达缺失或矛盾，但在开放分布式系统如物联网网络或语义网中，由于数据不完整和异步性，否定失败语义变得不可靠。&lt;h4&gt;目的&lt;/h4&gt;研究无否定MTL（时态逻辑框架）的表达能力，该框架用于基于规则的时态推理。&lt;h4&gt;方法&lt;/h4&gt;展示如何使用'once'、'since'和'until'算子消除MTL中的'always'算子，并进一步展示如何移除'once'算子，仅基于'until'和'since'构建片段。&lt;h4&gt;主要发现&lt;/h4&gt;'always'算子和'once'算子都可以被消除，可以仅使用'until'和'since'算子构建一个能够捕获存在性和不变时态模式的强大片段。&lt;h4&gt;结论&lt;/h4&gt;挑战了否定对于表达普遍时态约束的必要性假设，揭示了能够捕获存在性和不变时态模式的强大片段，结果导致MTL语法的简化，为理论研究和实现工作带来好处。&lt;h4&gt;翻译&lt;/h4&gt;在动态、数据密集型环境中的时态推理越来越需要表达性强且可处理的逻辑框架。传统方法通常依赖否定来表达缺失或矛盾。在这种情况下，否定失败常用于从缺乏积极证据中推断负面信息。然而，在物联网网络或语义网等开放和分布式系统中，由于数据不完整和异步性，否定失败语义变得不可靠。这导致了对无否定时态规则系统片段的日益关注，这些片段保持单调性并支持可扩展的推理。本文研究了无否定MTL的表达能力，这是一种为时间上的基于规则推理而设计的时态逻辑框架。我们展示了MTL的'always'算子通常被视为其他时态结构的语法糖，可以使用'once'、'since'和'until'算子消除。值得注意的是，即使'once'算子也可以被移除，产生一个仅基于'until'和'since'的片段。这些结果挑战了否定对于表达普遍时态约束的必要性假设，并揭示了一个能够捕获存在性和不变时态模式的强大片段。此外，这些结果导致了MTL语法的简化，这可以为理论研究以及实现工作带来好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning in dynamic, data-intensive environments increasinglydemands expressive yet tractable logical frameworks. Traditional approachesoften rely on negation to express absence or contradiction. In such contexts,Negation-as-Failure is commonly used to infer negative information from thelack of positive evidence. However, open and distributed systems such as IoTnetworks or the Semantic Web Negation-as-Failure semantics become unreliabledue to incomplete and asynchronous data. This has led to a growing interest innegation-free fragments of temporal rule-based systems, which preservemonotonicity and enable scalable reasoning.  This paper investigates the expressive power of negation-free MTL, a temporallogic framework designed for rule-based reasoning over time. We show that the"always" operators of MTL, often treated as syntactic sugar for combinations ofother temporal constructs, can be eliminated using "once", "since" and "until"operators. Remarkably, even the "once" operators can be removed, yielding afragment based solely on "until" and "since". These results challenge theassumption that negation is necessary for expressing universal temporalconstraints, and reveal a robust fragment capable of capturing both existentialand invariant temporal patterns. Furthermore, the results induce a reduction inthe syntax of MTL, which in turn can provide benefits for both theoreticalstudy as well as implementation efforts.</description>
      <author>example@mail.com (Mathijs van Noort, Femke Ongenae, Pieter Bonte)</author>
      <guid isPermaLink="false">2509.10146v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>MAESTRO: Multi-modal Adaptive Estimation for Temporal Respiratory Disease Outbreak</title>
      <link>http://arxiv.org/abs/2509.08578v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MAESTRO框架，一种用于流感发病率预测的新型统一方法，通过整合多模态数据和先进的谱时建模实现了高准确率的预测。&lt;h4&gt;背景&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测流感发病率的方法，为公共卫生决策提供支持。&lt;h4&gt;方法&lt;/h4&gt;提出MAESTRO框架，整合先进的谱时建模与多模态数据融合（包括监测数据、网络搜索趋势和气象数据），通过自适应加权异构数据源和分解复杂时间序列模式实现预测。&lt;h4&gt;主要发现&lt;/h4&gt;在香港11年以上的流感数据评估中，MAESTRO达到了0.956的R平方值，展示了最先进的预测性能；消融实验证实了多模态和谱时成分的重要贡献。&lt;h4&gt;结论&lt;/h4&gt;MAESTRO是一种模块化、可复现的预测工具，已公开可用，可部署到其他地区和病原体，为流行病学预测提供了强大工具。&lt;h4&gt;翻译&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。本文提出了MAESTRO（多模态时间呼吸道疾病爆发自适应估计），一种新颖的统一框架，该框架协同整合了先进的谱时建模与多模态数据融合，包括监测数据、网络搜索趋势和气象数据。通过自适应加权异构数据源和分解复杂时间序列模式，该模型实现了稳健且准确的预测。在香港11年以上的流感数据评估中（排除COVID-19期间），MAESTRO展示了最先进的性能，实现了0.956的R平方值的优越模型拟合。大量的消融实验证实了其多模态和谱时成分的重要贡献。模块化和可复现的管道已公开可用，便于部署和扩展到其他地区和病原体，为流行病学预测提供了强大工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely and robust influenza incidence forecasting is critical for publichealth decision-making. This paper presents MAESTRO (Multi-modal AdaptiveEstimation for Temporal Respiratory Disease Outbreak), a novel, unifiedframework that synergistically integrates advanced spectro-temporal modelingwith multi-modal data fusion, including surveillance, web search trends, andmeteorological data. By adaptively weighting heterogeneous data sources anddecomposing complex time series patterns, the model achieves robust andaccurate forecasts. Evaluated on over 11 years of Hong Kong influenza data(excluding the COVID-19 period), MAESTRO demonstrates state-of-the-artperformance, achieving a superior model fit with an R-square of 0.956.Extensive ablations confirm the significant contributions of its multi-modaland spectro-temporal components. The modular and reproducible pipeline is madepublicly available to facilitate deployment and extension to other regions andpathogens, presenting a powerful tool for epidemiological forecasting.</description>
      <author>example@mail.com (Hong Liu, Kerui Cen, Yanxing Chen, Zige Liu, Dong Chen, Zifeng Yang, Chitin Hon)</author>
      <guid isPermaLink="false">2509.08578v2</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Multiscaling in Wasserstein Spaces</title>
      <link>http://arxiv.org/abs/2509.10415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的多尺度框架，用于分析欧几里得域上Wasserstein空间中的概率测度序列。该框架利用最优传输的内在几何结构，构建了适用于绝对连续和离散测度的多尺度变换，并引入了最优性数量来量化序列与Wasserstein测地线的偏差，从而检测不规则动态和异常。&lt;h4&gt;背景&lt;/h4&gt;在欧几里得域上的Wasserstein空间中分析概率测度序列是当前研究的一个挑战领域，特别是需要同时处理绝对连续和离散测度的情况。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效分析概率测度序列的多尺度框架，检测不规则动态和异常，并提供稳健且可解释的多尺度表示。&lt;h4&gt;方法&lt;/h4&gt;研究基于最优传输的内在几何结构构建多尺度变换，核心是基于McCann插值的细化算子，该算子保留了测度流的测地线结构并作为上采样机制。此外，引入了最优性数量这一标量，用于量化序列与Wasserstein测地线的偏差。&lt;h4&gt;主要发现&lt;/h4&gt;研究建立了关键的理论保证，包括变换的稳定性和系数的几何衰减，确保了多尺度表示的稳健性和可解释性。通过数值实验展示了该方法在高斯流去噪和异常检测、向量场下点云动态分析以及神经网络学习轨迹多尺度表征方面的多功能性。&lt;h4&gt;结论&lt;/h4&gt;该多尺度框架为分析概率测度序列提供了一种强大而灵活的工具，能够有效检测不规则动态和异常，并在多种应用场景中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的多尺度框架，用于分析欧几里得域上Wasserstein空间中的概率测度序列。利用最优传输的内在几何结构，我们构建了适用于绝对连续和离散测度的多尺度变换。我们方法的核心是基于McCann插值的细化算子，它保留了测度流的测地线结构并作为上采样机制。在此基础上，我们引入了最优性数量，这是一个标量，用于量化序列跨尺度偏离Wasserstein测地线的程度，从而能够检测不规则动态和异常。我们建立了关键的理论保证，包括变换的稳定性和系数的几何衰减，确保了多尺度表示的稳健性和可解释性。最后，我们通过数值实验展示了我们方法的多功能性：高斯流中的去噪和异常检测、向量场下点云动态的分析，以及神经网络学习轨迹的多尺度表征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel multiscale framework for analyzing sequences ofprobability measures in Wasserstein spaces over Euclidean domains. Exploitingthe intrinsic geometry of optimal transport, we construct a multiscaletransform applicable to both absolutely continuous and discrete measures.Central to our approach is a refinement operator based on McCann'sinterpolants, which preserves the geodesic structure of measure flows andserves as an upsampling mechanism. Building on this, we introduce theoptimality number, a scalar that quantifies deviations of a sequence fromWasserstein geodesicity across scales, enabling the detection of irregulardynamics and anomalies. We establish key theoretical guarantees, includingstability of the transform and geometric decay of coefficients, ensuringrobustness and interpretability of the multiscale representation. Finally, wedemonstrate the versatility of our methodology through numerical experiments:denoising and anomaly detection in Gaussian flows, analysis of point clouddynamics under vector fields, and the multiscale characterization of neuralnetwork learning trajectories.</description>
      <author>example@mail.com (Wael Mattar, Nir Sharon)</author>
      <guid isPermaLink="false">2509.10415v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System</title>
      <link>http://arxiv.org/abs/2509.10349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Acetrans系统，一种自主的、基于走廊的、高效的无人机悬吊运输系统，通过统一的感知、规划和控制框架解决现有系统在感知、规划和控制方面的关键限制。&lt;h4&gt;背景&lt;/h4&gt;无人机悬吊载荷在复杂和杂乱环境中的空中运输具有显著优势，但现有系统面临感知不可靠、大规模环境中规划效率低、无法保证在电缆弯曲和外部干扰下的全身安全等关键限制。&lt;h4&gt;目的&lt;/h4&gt;提出Acetrans系统，解决现有无人机悬吊运输系统在感知、规划和控制方面的关键限制，实现更安全、高效的空中货物运输。&lt;h4&gt;方法&lt;/h4&gt;通过统一的感知、规划和控制框架解决问题：1)提出LiDAR-IMU融合模块，在张紧和弯曲模式下估计载荷姿态和电缆形状；2)引入MACIRI算法，考虑不同无人机和载荷几何形状，生成安全飞行走廊；3)开发时空受限的轨迹优化方案；4)使用带有电缆弯曲约束的非线性模型预测控制器确保执行过程中的全身安全。&lt;h4&gt;主要发现&lt;/h4&gt;通过仿真和实验验证了Acetrans的有效性，与最先进的方法相比，在感知准确性、规划效率和控制安全性方面有显著改进。&lt;h4&gt;结论&lt;/h4&gt;Acetrans系统能够有效解决现有无人机悬吊运输系统的关键限制，实现更安全、高效的空中货物运输，在复杂和杂乱环境中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;带有悬吊载荷的无人机在复杂和杂乱环境中的空中运输具有显著优势。然而，现有系统面临关键限制，包括对电缆-载荷动力学的感知不可靠、大规模环境中的规划效率低下，以及在电缆弯曲和外部干扰下无法保证全身安全。本文提出了Acetrans，一种自主的、基于走廊的、高效的无人机悬吊运输系统，通过统一的感知、规划和控制框架解决这些挑战。提出了LiDAR-IMU融合模块，在张紧和弯曲模式下联合估计载荷姿态和电缆形状，实现鲁棒的全身状态估计和电缆点云的实时滤波。为了提高规划的可扩展性，我们引入了多尺寸感知配置空间迭代区域膨胀算法，该算法在考虑不同无人机和载荷几何形状的同时生成安全飞行走廊。然后，开发了一种时空受限的轨迹优化方案，以确保动态可行且无碰撞的轨迹。最后，通过增加电缆弯曲约束的非线性模型预测控制器在执行期间提供鲁棒的全身安全。仿真和实验结果验证了Acetrans的有效性，表明与最先进的方法相比，在感知准确性、规划效率和控制安全性方面有显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无人机悬挂运输系统中的三个关键问题：对电缆-载荷动力学的感知不可靠、在大规模环境中规划效率低下、以及无法保证电缆弯曲和外部干扰下的全身安全。这个问题在现实中非常重要，因为无人机悬挂运输系统在物流、农业和救灾等领域具有巨大潜力，特别是在复杂环境（如森林、城市峡谷或室内）中能够提供传统固定翼无人机和带机械臂的多旋翼无人机无法比拟的灵活性和适应性。解决这些问题将使无人机能够在这些复杂环境中实现自主、安全、高效的货物运输，拓展其应用范围。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计Acetrans系统。他们首先识别了悬挂运输系统面临的三个主要挑战：感知不可靠、规划效率低下和安全保障不足。在感知方面，作者借鉴了LiDAR-IMU融合技术，但进行了创新以适应电缆弯曲情况；在规划方面，作者基于现有的迭代区域膨胀算法（如IRIS、RILS、FIRI和CIRI）思想，扩展了它们以适应悬挂载荷系统的复杂几何形状；在控制方面，作者借鉴了非线性模型预测控制技术，但增加了电缆弯曲约束。总的来说，作者不是从零开始设计，而是基于现有工作的基础上进行创新和扩展，解决了悬挂运输系统特有的挑战，并统一了感知、规划和控制三个模块。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; Acetrans方法的核心思想是通过一个统一的框架同时解决感知、规划和控制三个关键问题，实现无人机在复杂环境中的安全、高效悬挂运输。核心思想包括：1)全身感知：使用LiDAR-IMU融合技术同时估计无人机、载荷和电缆的状态；2)多尺寸感知的走廊生成：提出MACIRI算法根据系统中不同部分的尺寸动态调整障碍物表示；3)走廊约束的轨迹优化：在生成的安全走廊内进行轨迹优化；4)电缆弯曲安全的控制：使用增强的非线性模型预测控制器确保全身安全。整体流程为：首先通过双LiDAR和LiDAR-IMU融合进行全身感知和状态估计；然后使用动力学A*算法和MACIRI算法生成安全飞行走廊；接着在走廊内进行时空轨迹优化；最后使用带有电缆弯曲约束的NMPC控制器跟踪轨迹，形成一个完整的自主运输系统。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1)全身感知框架：能够处理电缆弯曲情况，独立于光照条件，而现有视觉方法在低光条件下表现不佳且大多假设电缆总是张紧的；2)MACIRI算法：根据系统中不同部分的尺寸动态调整障碍物表示，扩大了可用走廊体积，而现有算法只考虑单一机器人尺寸；3)走廊约束的轨迹优化：专门针对悬挂载荷系统设计，优化速度比现有基线快1-3个数量级；4)电缆弯曲安全的控制：是第一个实现电缆弯曲时障碍避免的框架，而现有方法大多忽略电缆或未保证其安全。Acetrans的创新之处在于它首次将感知、规划和控制统一到一个完整的自主框架中，解决了悬挂载荷系统特有的挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Acetrans首次提出了一个统一的自主悬挂运输系统框架，通过创新的全身感知、多尺寸感知的走廊生成和电缆弯曲安全的控制方法，实现了无人机在复杂环境中安全、高效的悬挂载荷运输。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmanned aerial vehicles (UAVs) with suspended payloads offer significantadvantages for aerial transportation in complex and cluttered environments.However, existing systems face critical limitations, including unreliableperception of the cable-payload dynamics, inefficient planning in large-scaleenvironments, and the inability to guarantee whole-body safety under cablebending and external disturbances. This paper presents Acetrans, an Autonomous,Corridor-based, and Efficient UAV suspended transport system that addressesthese challenges through a unified perception, planning, and control framework.A LiDAR-IMU fusion module is proposed to jointly estimate both payload pose andcable shape under taut and bent modes, enabling robust whole-body stateestimation and real-time filtering of cable point clouds. To enhance planningscalability, we introduce the Multi-size-Aware Configuration-space IterativeRegional Inflation (MACIRI) algorithm, which generates safe flight corridorswhile accounting for varying UAV and payload geometries. A spatio-temporal,corridor-constrained trajectory optimization scheme is then developed to ensuredynamically feasible and collision-free trajectories. Finally, a nonlinearmodel predictive controller (NMPC) augmented with cable-bending constraintsprovides robust whole-body safety during execution. Simulation and experimentalresults validate the effectiveness of Acetrans, demonstrating substantialimprovements in perception accuracy, planning efficiency, and control safetycompared to state-of-the-art methods.</description>
      <author>example@mail.com (Weiyan Lu, Huizhe Li, Yuhao Fang, Zhexuan Zhou, Junda Wu, Yude Li, Youmin Gong, Jie Mei)</author>
      <guid isPermaLink="false">2509.10349v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.10282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Page 14, 5 pictures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了MCL-AD，一个新颖的多模态协作学习框架，用于零样本3D异常检测。该框架结合点云、RGB图像和文本语义信息，通过多模态提示学习机制和协作调制机制，实现了最先进的检测性能。&lt;h4&gt;背景&lt;/h4&gt;Zero-shot 3D异常检测旨在无需标记训练数据的情况下识别3D物体缺陷，在数据稀缺、隐私限制或标注成本高的场景中特别有价值。然而，现有方法主要专注于点云，忽视了来自RGB图像和文本先验等互补模态的丰富语义线索。&lt;h4&gt;目的&lt;/h4&gt;引入MCL-AD框架，利用点云、RGB图像和文本语义之间的多模态协作学习，实现更优的零样本3D异常检测。&lt;h4&gt;方法&lt;/h4&gt;提出多模态提示学习机制（MPLM），引入与对象无关的解耦文本提示和多模态对比损失，增强模内表示能力和模间协作学习；同时提出协作调制机制（CMM），通过联合调制RGB图像引导和点云引导的分支，充分利用点云和RGB图像的互补表示。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，提出的MCL-AD框架在零样本3D异常检测任务中达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;MCL-AD框架通过多模态协作学习有效提升了零样本3D异常检测的性能，为该领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;零样本3D异常检测旨在无需依赖标记的训练数据来识别3D物体中的缺陷，在数据稀缺、隐私限制或标注成本高的场景中特别有价值。然而，大多数现有方法仅专注于点云，忽视了来自RGB图像和文本先验等互补模态的丰富语义线索。本文介绍了MCL-AD，一个新颖的框架，它利用点云、RGB图像和文本语义之间的多模态协作学习来实现卓越的零样本3D异常检测。具体而言，我们提出了多模态提示学习机制（MPLM），通过引入与对象无关的解耦文本提示和多模态对比损失，增强模内表示能力和模间协作学习。此外，还提出了协作调制机制（CMM），通过联合调制RGB图像引导和点云引导的分支，充分利用点云和RGB图像的互补表示。大量实验证明，所提出的MCL-AD框架在零样本3D异常检测中达到了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决零样本3D异常检测问题，即在没有针对特定对象标记的训练数据的情况下检测3D物体中的缺陷。这个问题在现实中很重要，因为它能解决数据稀缺、隐私限制和高标注成本等挑战，使异常检测系统能够快速适应新对象类别，在工业检测、质量控制等领域具有广泛应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法仅关注点云数据而忽略RGB图像和文本提示等互补模态的局限性，提出多模态协作学习思路。设计过程中借鉴了CLIP模型作为基础编码器、多视图渲染技术处理点云、提示学习和对比学习等现有技术，但进行了创新改进，设计了多模态提示学习机制和协作调制机制来有效融合不同模态信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态协作学习，融合点云、RGB图像和文本语义信息，增强模型对异常的感知能力，使其能在无特定类别训练数据的情况下准确检测3D异常。整体流程包括：1)特征提取阶段，将点云转换为多视图深度图像并提取特征；2)训练阶段使用多模态提示学习机制构建解耦文本提示并计算多模态对比损失；3)测试阶段通过协作调制机制动态调整RGB和点云分支的输出权重，融合生成最终结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多模态协作学习框架MCL-AD，首次融合点云、RGB图像和文本语义三种模态；2)多模态提示学习机制MPLM，包含对象无关的解耦文本提示和多模态对比损失；3)协作调制机制CMM，动态调整不同模态贡献权重。相比之前工作，MCL-AD不仅利用了更多模态信息，还通过解耦设计和动态融合解决了模态不平衡问题，显著提升了检测性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MCL-AD通过多模态协作学习框架，有效融合点云、RGB图像和文本语义信息，显著提升了在没有特定对象类别训练数据的情况下3D异常检测的准确性和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objectswithout relying on labeled training data, making it especially valuable inscenarios constrained by data scarcity, privacy, or high annotation cost.However, most existing methods focus exclusively on point clouds, neglectingthe rich semantic cues available from complementary modalities such as RGBimages and texts priors. This paper introduces MCL-AD, a novel framework thatleverages multimodal collaboration learning across point clouds, RGB images,and texts semantics to achieve superior zero-shot 3D anomaly detection.Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) thatenhances the intra-modal representation capability and inter-modalcollaborative learning by introducing an object-agnostic decoupled text promptand a multimodal contrastive loss. In addition, a collaborative modulationmechanism (CMM) is proposed to fully leverage the complementary representationsof point clouds and RGB images by jointly modulating the RGB image-guided andpoint cloud-guided branches. Extensive experiments demonstrate that theproposed MCL-AD framework achieves state-of-the-art performance in ZS-3Danomaly detection.</description>
      <author>example@mail.com (Gang Li, Tianjiao Chen, Mingle Zhou, Min Li, Delong Han, Jin Wan)</author>
      <guid isPermaLink="false">2509.10282v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion</title>
      <link>http://arxiv.org/abs/2509.10139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CaR1的新型相机-雷达融合架构，用于BEV车辆分割，通过结合相机和雷达的互补优势，实现了与最先进方法相当的分割性能。&lt;h4&gt;背景&lt;/h4&gt;相机-雷达融合为基于激光雷达的自动驾驶系统提供了一种稳健且具有成本效益的替代方案，相机提供丰富的语义线索但深度信息不可靠，而雷达提供稀疏但可靠的位置和运动信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型相机-雷达融合架构CaR1，专门用于BEV（鸟瞰图）车辆分割任务。&lt;h4&gt;方法&lt;/h4&gt;基于BEVFusion构建，包含网格化的雷达编码，将点云离散化为结构化的BEV特征，以及自适应融合机制，动态平衡传感器的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验显示，CaR1实现了57.6的IoU分割性能，与当前最先进的方法相当。&lt;h4&gt;结论&lt;/h4&gt;CaR1架构通过有效融合相机和雷达数据，在BEV车辆分割任务上取得了优异性能，相关代码已在GitHub公开可用。&lt;h4&gt;翻译&lt;/h4&gt;相机-雷达融合通过结合互补的感知能力，为基于激光雷达的自动驾驶系统提供了一种稳健且具有成本效益的替代方案：相机提供丰富的语义线索但深度信息不可靠，而雷达提供稀疏但可靠的位置和运动信息。我们介绍了CaR1，一种用于BEV车辆分割的新型相机-雷达融合架构。基于BEVFusion构建，我们的方法包含一种网格化的雷达编码，将点云离散化为结构化的BEV特征，以及一种自适应融合机制，能够动态平衡传感器的贡献。在nuScenes上的实验表明，该方法具有竞争力的分割性能（57.6 IoU），与最先进的方法相当。代码已在GitHub上公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何有效融合相机和雷达两种传感器的数据，实现鸟瞰视角下的车辆分割问题。这个问题很重要，因为自动驾驶系统需要准确感知周围环境，而单一传感器有局限性：相机提供丰富语义信息但深度估计不可靠，雷达提供可靠位置和运动信息且在恶劣天气下稳定但数据稀疏。相机-雷达融合可以提供一种成本更低、更鲁棒的替代方案，比基于LiDAR的系统更适合大规模部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了相机和雷达的互补特性及融合挑战，然后基于BEVFusion架构进行改进。借鉴了现有工作中的BEV表示空间、EfficientViT-L2图像编码器、Point Transformer V3点云编码网络和Attention U-Net解码器。作者的创新设计包括网格化雷达特征编码方法将稀疏点云转换为结构化特征，以及自适应融合机制动态平衡各传感器贡献。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用相机和雷达的互补优势，通过鸟瞰视角表示空间统一两种传感器数据，采用网格化方法处理雷达点云，并设计自适应融合机制动态调整传感器贡献。整体流程包括：1)图像特征提取：使用EfficientViT-L2处理多视角图像并投影到BEV空间；2)雷达特征提取：使用PTv3处理点云并通过金字塔聚合网络增强特征；3)自适应融合：通过注意力机制动态调整传感器贡献；4)BEV解码：使用Attention U-Net refine特征；5)分割头：生成车辆分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)网格化雷达特征编码方法，将稀疏点云转换为结构化BEV特征；2)自适应融合机制，动态平衡各传感器贡献；3)整体架构设计，使用轻量级编码器和注意力解码器。相比之前工作，不同之处在于：雷达处理方式更先进，结合了点网络优势和BEV框架；融合策略更灵活，能动态调整传感器贡献；性能表现更好，在nuScenes上实现57.6% IoU，比纯相机方法提升10.2%，与最先进方法相当。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CaR1通过创新的网格化雷达特征编码和自适应融合机制，实现了相机和雷达数据在鸟瞰视角下的有效融合，为自动驾驶车辆分割提供了一种成本更低、更鲁棒的感知方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-radar fusion offers a robust and cost-effective alternative toLiDAR-based autonomous driving systems by combining complementary sensingcapabilities: cameras provide rich semantic cues but unreliable depth, whileradar delivers sparse yet reliable position and motion information. Weintroduce CaR1, a novel camera-radar fusion architecture for BEV vehiclesegmentation. Built upon BEVFusion, our approach incorporates a grid-wise radarencoding that discretizes point clouds into structured BEV features and anadaptive fusion mechanism that dynamically balances sensor contributions.Experiments on nuScenes demonstrate competitive segmentation performance (57.6IoU), on par with state-of-the-art methods. Code is publicly available\href{https://www.github.com/santimontiel/car1}{online}.</description>
      <author>example@mail.com (Santiago Montiel-Marín, Angel Llamazares, Miguel Antunes-García, Fabio Sánchez-García, Luis M. Bergasa)</author>
      <guid isPermaLink="false">2509.10139v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Soft Tissue Simulation and Force Estimation from Heterogeneous Structures using Equivariant Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.10125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络(GNN)的软组织变形模拟方法，能够从稀疏点云预测组织表面变形和施加的力，相比传统有限元法更高效且具有较好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;软组织变形模拟对手术训练、术前规划和实时触觉反馈系统至关重要。基于物理的模型如有限元法能提供高保真结果，但计算成本高且需要大量预处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的数据驱动方法，能够准确模拟软组织变形，适用于实时应用，并具有良好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图神经网络架构，通过每个点下方的二值组织轮廓整合内部解剖信息，并利用E(n)-等变消息传递提高鲁棒性。实验数据包括真实硅胶和类骨 phantom，以及使用FEM生成的合成模拟数据。&lt;h4&gt;主要发现&lt;/h4&gt;模型在标准测试案例中与基线GNN性能相当，在旋转和跨分辨率场景中显著优于基线，显示出对未见方向和点密度的强泛化能力。模型也实现了显著的速度提升，为实时应用提供了解决方案。在实验数据上微调后，模型在有限样本量和测量噪声下仍保持亚毫米级变形精度。&lt;h4&gt;结论&lt;/h4&gt;该方法为传统模拟提供了高效的数据驱动替代方案，能够泛化到不同的解剖结构配置，并支持交互式手术环境。&lt;h4&gt;翻译&lt;/h4&gt;准确模拟软组织变形对手术培训、术前规划和实时触觉反馈系统至关重要。虽然基于物理的模型如有限元法(FEM)能提供高保真结果，但它们通常计算量大且需要大量预处理。我们提出了一种图神经网络(GNN)架构，可以从稀疏点云预测组织表面变形和施加的力。该模型通过每个点下方的二值组织轮廓整合内部解剖信息，并利用E(n)-等变消息传递提高鲁棒性。我们收集了包含真实硅胶和类骨 phantom 的实验数据，并辅以使用FEM生成的合成模拟数据。我们的模型在标准测试案例中与基线GNN性能相当，在旋转和跨分辨率场景中显著优于基线，显示出对未见方向和点密度的强泛化能力。它还实现了显著的速度提升，为实时应用提供了解决方案。在实验数据上微调后，尽管样本量有限且存在测量噪声，模型仍保持亚毫米级变形精度。结果表明，我们的方法为传统模拟提供了高效的数据驱动替代方案，能够泛化到不同的解剖结构配置，并支持交互式手术环境。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决软组织变形模拟的计算效率问题。传统物理模拟方法(如有限元法)虽然准确但计算量大，难以满足实时应用需求。这一问题在现实中至关重要，因为准确的软组织模拟对手术培训、术前规划和实时手术引导系统都至关重要，能够提高外科医生技能、预见手术并发症、改善临床决策精度，并支持医疗机器人的精确控制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：物理模拟计算昂贵，而现有学习方法无法提供触觉反馈所需的力估计或仅能处理均匀组织。他们借鉴了多种现有工作，包括多层感知机(MLP)、U-Mesh架构、图神经网络(GNN)如MagNet框架、条件图神经网络以及E(n)-等变图神经网络概念。基于这些工作，作者设计了一种新方法，通过引入异构组织建模、条件等变图卷积层(cEGCL)和多任务预测(变形和力)来解决现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过图神经网络处理点云数据，同时预测软组织变形和施加力，并利用组织内部结构信息提高预测准确性。整体流程包括：1)收集实验数据(硅胶和骨状phantom)和FEM模拟数据；2)为每个表面点提取128维二元组织特征向量，表示垂直材料分布；3)设计条件等变图卷积层(cEGCL)更新节点坐标和特征；4)构建包含位移预测和力预测两个分支的模型架构；5)使用加权损失函数进行训练，并在实验数据上微调；6)评估模型在旋转、点密度变化和迁移学习场景下的性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)异构对象建模，引入内部解剖结构信息；2)条件等变图卷积层(cEGCL)，提高对旋转的鲁棒性；3)多任务预测，同时输出变形和力；4)数据融合策略，结合实验和FEM模拟数据；5)强大的泛化能力，在旋转和不同点密度场景下表现优异。相比之前的工作，本方法不仅能处理异质组织结构，还提供力估计支持触觉反馈，计算速度更快，且对旋转和点密度变化具有更强的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合组织内部结构信息和等变图神经网络的创新方法，实现了高效、准确的软组织变形和力估计，显著提高了在异质结构、旋转和不同点密度场景下的泛化能力，为实时手术模拟和触觉反馈提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately simulating soft tissue deformation is crucial for surgicaltraining, pre-operative planning, and real-time haptic feedback systems. Whilephysics-based models such as the finite element method (FEM) providehigh-fidelity results, they are often computationally expensive and requireextensive preprocessing. We propose a graph neural network (GNN) architecturethat predicts both tissue surface deformation and applied force from sparsepoint clouds. The model incorporates internal anatomical information throughbinary tissue profiles beneath each point and leverages E(n)-equivariantmessage passing to improve robustness. We collected experimental data thatcomprises a real silicone and bone-like phantom, and complemented it withsynthetic simulations generated using FEM. Our model achieves a comparableperformance to a baseline GNN on standard test cases and significantlyoutperforms it in rotated and cross-resolution scenarios, showing a stronggeneralization to unseen orientations and point densities. It also achieves asignificant speed improvement, offering a solution for real-time applications.When fine-tuned on experimental data, the model maintains sub-millimeterdeformation accuracy despite limited sample size and measurement noise. Theresults demonstrate that our approach offers an efficient, data-drivenalternative to traditional simulations, capable of generalizing acrossanatomical configurations and supporting interactive surgical environments.</description>
      <author>example@mail.com (Madina Kojanazarova, Sidady El Hadramy, Jack Wilkie, Georg Rauter, Philippe C. Cattin)</author>
      <guid isPermaLink="false">2509.10125v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping</title>
      <link>http://arxiv.org/abs/2509.10032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Pages, 9 figures, International Workshop 3D-AdViCE in conjunction  with 12th ECMR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了两种互补的球形测绘系统，评估了它们在资源受限硬件上的激光雷达-惯性里程计性能，并发现球形运动的高动态特性导致传统算法性能下降。&lt;h4&gt;背景&lt;/h4&gt;球形机器人因其保护壳和全向移动能力，在危险或受限环境的测绘应用中具有独特优势。&lt;h4&gt;目的&lt;/h4&gt;开发两种互补的球形测绘系统（轻量级无驱动设计和内部摆锤驱动变体），并评估其测绘精度。&lt;h4&gt;方法&lt;/h4&gt;两种系统均配备Livox Mid-360固态激光雷达传感器，在资源受限硬件上运行激光雷达-惯性里程计算法，通过与真实地图比较点云来评估精度。&lt;h4&gt;主要发现&lt;/h4&gt;球形运动引入的高动态运动导致最先进的LIO算法性能下降，产生全局不一致的地图和有时不可恢复的漂移。&lt;h4&gt;结论&lt;/h4&gt;球形机器人的特殊运动特性对传统LIO算法构成挑战，需要改进算法以适应这种高动态运动环境。&lt;h4&gt;翻译&lt;/h4&gt;球形机器人因其保护壳和全向移动能力，在危险或受限环境的测绘应用中具有独特优势。这项工作提出了两种互补的球形测绘系统：一种轻量级无驱动设计和一种内部摆锤驱动运动的变体。两种系统都配备了Livox Mid-360固态激光雷达传感器，并在资源受限的硬件上运行激光雷达-惯性里程计算法。我们通过将LIO算法生成的3D点云与真实地图进行比较来评估这些系统的测绘精度。结果表明，由于球形运动引入的高动态运动，最先进的LIO算法性能下降，导致全局不一致的地图和有时不可恢复的漂移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spherical robots offer unique advantages for mapping applications inhazardous or confined environments, thanks to their protective shells andomnidirectional mobility. This work presents two complementary sphericalmapping systems: a lightweight, non-actuated design and an actuated variantfeaturing internal pendulum-driven locomotion. Both systems are equipped with aLivox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)algorithms on resource-constrained hardware. We assess the mapping accuracy ofthese systems by comparing the resulting 3D point-clouds from the LIOalgorithms to a ground truth map. The results indicate that the performance ofstate-of-the-art LIO algorithms deteriorates due to the high dynamic movementintroduced by the spherical locomotion, leading to globally inconsistent mapsand sometimes unrecoverable drift.</description>
      <author>example@mail.com (Marawan Khalil, Fabian Arzberger, Andreas Nüchter)</author>
      <guid isPermaLink="false">2509.10032v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation</title>
      <link>http://arxiv.org/abs/2509.09946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCVW 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种将现有2D多摄像头跟踪系统扩展到3D空间的方法，通过利用深度信息重建目标点云并恢复3D边界框，同时引入增强的数据关联机制，在AI City Challenge的3D MTMC数据集上获得第三名。&lt;h4&gt;背景&lt;/h4&gt;多目标多摄像头跟踪(MTMC)是自动化大规模监控的重要计算机视觉任务。通过摄像头校准和深度信息，可以将场景中的目标投影到3D空间，提供对3D环境的自动感知。然而，在3D空间中进行跟踪需要从头替换所有2D跟踪组件，这对现有的MTMC系统可能不可行。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在开发一种方法，能够将任何现有的在线2D多摄像头跟踪系统扩展到3D空间，而无需完全重新设计系统架构。&lt;h4&gt;方法&lt;/h4&gt;研究提出的方法包括：1)利用深度信息重建目标的点云空间；2)通过聚类和偏航角细化恢复目标的3D边界框；3)引入增强的在线数据关联机制，利用目标的局部ID一致性来跨帧分配全局ID。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在2025年AI City Challenge的3D MTMC数据集上进行了评估，并在排行榜上获得第三名的成绩，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;本研究提供了一种实用的方法，可以将现有的2D多摄像头跟踪系统扩展到3D空间，而无需完全重新设计系统，为大规模3D监控应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多目标多摄像头跟踪(MTMC)是自动化大规模监控的重要计算机视觉任务。通过摄像头校准和深度信息，场景中的目标可以被投影到3D空间，为3D环境提供前所未有的自动感知水平。然而，在3D空间中进行跟踪需要从头替换所有2D跟踪组件，这对于现有的MTMC系统可能不可行。在本文中，我们提出了一种方法，通过利用深度信息将任何在线2D多摄像头跟踪系统扩展到3D空间，重建目标在点云空间中的表示，并在跟踪后通过聚类和偏航角细化恢复其3D边界框。我们还引入了一种增强的在线数据关联机制，利用目标的局部ID一致性来跨帧分配全局ID。所提出的框架在2025年AI City Challenge的3D MTMC数据集上进行了评估，在排行榜上获得第三名。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将现有的在线2D多摄像头跟踪系统扩展到3D空间的问题。这个问题在现实中非常重要，因为大规模监控系统（如智慧城市、智能交通、安防等）需要处理来自多个摄像头的视频数据，而3D空间中的目标跟踪能提供更丰富的场景信息，如目标的高度、位置和朝向等。直接替换2D系统的所有组件到3D空间成本高昂且复杂，而本文提供了一种更高效、经济的解决方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有的3D多摄像头跟踪方法需要在跟踪前融合多视图数据，这需要替换所有2D组件，对大规模监控系统不切实际。因此，他们提出了'后期聚合'的思路，即在完成2D跟踪后，再利用深度信息将2D结果扩展到3D空间。作者借鉴了多项现有工作，包括使用Co-DETR进行目标检测、CLIP-ReID提取外观特征、RTMPose进行姿态估计、Deep OC-SORT进行单摄像头跟踪、SAM2进行实例分割、DBSCAN进行点云聚类以及分层聚类进行空间数据关联。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'后期聚合'的方式，在完成2D多摄像头跟踪后，再利用深度信息将2D结果扩展到3D空间，同时利用目标局部ID的一致性来改进跨摄像头跟踪的ID分配机制。整体流程分为两个阶段：第一阶段是2D多摄像头跟踪，包括单摄像头跟踪、空间数据关联和时间数据关联；第二阶段是后期3D边界框聚合，包括深度到点云转换、点云到3D边界框转换、3D边界框融合和偏航角优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 后期聚合框架，可将2D系统扩展为3D系统而无需修改现有组件；2) 增强的在线关联机制，利用局部ID一致性改进跨摄像头跟踪；3) 3D边界框恢复机制，通过分割和点云聚类将2D边界框扩展到3D空间；4) 偏航角优化，通过运动轨迹分析提高定位精度。相比之前的工作，本文方法不需要在跟踪前融合多视图数据，而是利用现有的2D跟踪结果，降低了系统复杂度，同时通过局部ID一致性改进了跨摄像头跟踪性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的后期聚合框架，能够将现有的在线2D多摄像头跟踪系统无缝扩展为3D跟踪系统，同时引入了基于局部ID一致性的增强关联机制和基于深度信息的3D边界框恢复方法，在2025年AI City Challenge的3D MTMC任务中取得了第三名的好成绩。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-Target Multi-Camera Tracking (MTMC) is an essential computer visiontask for automating large-scale surveillance. With camera calibration and depthinformation, the targets in the scene can be projected into 3D space, offeringunparalleled levels of automatic perception of a 3D environment. However,tracking in the 3D space requires replacing all 2D tracking components from theground up, which may be infeasible for existing MTMC systems. In this paper, wepresent an approach for extending any online 2D multi-camera tracking systeminto 3D space by utilizing depth information to reconstruct a target inpoint-cloud space, and recovering its 3D box through clustering and yawrefinement following tracking. We also introduced an enhanced online dataassociation mechanism that leverages the target's local ID consistency toassign global IDs across frames. The proposed framework is evaluated on the2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on theleaderboard.</description>
      <author>example@mail.com (Vu-Minh Le, Thao-Anh Tran, Duc Huy Do, Xuan Canh Do, Huong Ninh, Hai Tran)</author>
      <guid isPermaLink="false">2509.09946v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging</title>
      <link>http://arxiv.org/abs/2509.09785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Token Purging (PG)的新型测试时适应方法，用于解决3D点云分类中的分布偏移问题。该方法通过在令牌到达注意力层之前移除受域偏移高度影响的令牌，实现了无需反向传播的稳健适应。实验表明，该方法在准确率和效率方面均优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;测试时适应(TTA)对于缓解3D点云分类中分布偏移引起的性能下降至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的TTA方法，能够在不进行反向传播的情况下有效适应分布偏移，同时提高计算效率和内存利用率。&lt;h4&gt;方法&lt;/h4&gt;引入Token Purging (PG)，一种新颖的无反向传播方法，在令牌到达注意力层之前移除受域偏移高度影响的令牌。提出了两种变体：PG-SP（利用源统计信息）和PG-SF（完全无源版本，依赖CLS令牌驱动的适应）。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40-C、ShapeNet-C和ScanObjectNN-C上的广泛评估表明，PG-SP比最先进的无反向传播方法平均高出10.3%的准确率，而PG-SF为无源适应设定了新基准。此外，PG比基线快12.4倍，内存效率高5.5倍，适合实际部署。&lt;h4&gt;结论&lt;/h4&gt;Token Purging是一种有效的TTA方法，在性能和效率方面都有显著优势，适合实际部署。&lt;h4&gt;翻译&lt;/h4&gt;测试时适应(TTA)对于缓解3D点云分类中分布偏移引起的性能下降至关重要。在这项工作中，我们引入了Token Purging (PG)，一种新颖的无反向传播方法，在令牌到达注意力层之前移除受域偏移高度影响的令牌。与现有的TTA方法不同，PG在令牌级别操作，确保无需迭代更新的稳健适应。我们提出了两种变体：PG-SP，它利用源统计信息，以及PG-SF，一种完全无源的版本，依赖于CLS令牌驱动的适应。在ModelNet40-C、ShapeNet-C和ScanObjectNN-C上的广泛评估表明，PG-SP比最先进的无反向传播方法平均高出10.3%的准确率，而PG-SF为无源适应设定了新基准。此外，PG比我们的基线快12.4倍，内存效率高5.5倍，使其适合实际部署。代码可在https://github.com/MosyMosy/Purge-Gate获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云分类任务中的测试时适应问题，即在测试数据分布与训练数据分布不同（分布偏移）的情况下，如何保持模型性能。这个问题在现实中非常重要，因为3D点云应用广泛（如自动驾驶、机器人、AR/VR等），这些应用中数据分布经常变化，而预训练一个模型应对所有场景是不现实的。现有方法要么计算成本高，要么需要额外源数据，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云数据中分布偏移的特殊性，指出噪声可能表现为新增点或不均匀分布的扰动。他们观察到噪声会破坏transformer架构中的注意力机制，影响特征聚合。受Token Pruning（用于提高transformer效率）的启发，作者转而设计删除受分布偏移影响最大的token的方法。同时，他们利用了transformer中CLS token的特性，发现CLS token在预训练过程中会吸收领域信息，可以作为原型用于测试时适应。整体设计思路是创建一个轻量级的、无反向传播的门控机制，在注意力层输入前过滤掉噪声token。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在测试时识别并移除受分布偏移影响最大的token，防止这些噪声token到达注意力层，从而保护模型的注意力机制不被破坏。整体流程包括：1) 将点云样本表示为token集合；2) 计算每个token的偏离程度（PG-SP使用源数据统计信息计算马氏距离，PG-SF使用CLS token作为原型计算余弦距离）；3) 移除偏离最大的Lpg个token；4) 将净化后的token输入transformer网络进行分类；5) 使用熵最小化策略动态选择最优的Lpg值。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次在token级别进行测试时适应；2) 提出无反向传播的轻量级方法；3) 设计两种变体（PG-SP利用源数据统计，PG-SF完全无源）；4) 提供分布偏移如何影响transformer注意力机制的理论分析。相比之前工作，不同之处在于：相比反向传播方法（如TENT），不需要梯度计算和参数更新，效率更高；相比无反向传播方法（如BFTT3D），不依赖源数据类别原型；相比测试时训练方法（如MATE），不需要辅助训练任务，可应用于任意预训练模型；相比Token Pruning，专注于解决分布偏移而非提高计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Purge-Gate通过在测试时动态移除受分布偏移影响最大的token，提出了一种高效的无反向传播的点云分类测试时适应方法，显著提高了模型在分布偏移场景下的性能，同时大幅降低了计算和内存需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Test-time adaptation (TTA) is crucial for mitigating performance degradationcaused by distribution shifts in 3D point cloud classification. In this work,we introduce Token Purging (PG), a novel backpropagation-free approach thatremoves tokens highly affected by domain shifts before they reach attentionlayers. Unlike existing TTA methods, PG operates at the token level, ensuringrobust adaptation without iterative updates. We propose two variants: PG-SP,which leverages source statistics, and PG-SF, a fully source-free versionrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is12.4 times faster and 5.5 times more memory efficient than our baseline, makingit suitable for real-world deployment. Code is available at\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}</description>
      <author>example@mail.com (Moslem Yazdanpanah, Ali Bahri, Mehrdad Noori, Sahar Dastani, Gustavo Adolfo Vargas Hakim, David Osowiechi, Ismail Ben Ayed, Christian Desrosiers)</author>
      <guid isPermaLink="false">2509.09785v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training</title>
      <link>http://arxiv.org/abs/2509.10426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为DECAMP的解耦上下文感知预训练框架，用于解决自动驾驶中多智能体运动预测的挑战，特别是标记数据稀缺和多智能体场景预测效果不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法往往面临标记数据稀缺的问题，并且在多智能体预测场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法面临的标记数据稀缺和多智能体预测场景中表现不佳的挑战，通过引入一个解耦的上下文感知预训练框架来提高多智能体运动预测的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为DECAMP的解耦上下文感知预训练框架，将行为模式学习与潜在特征重建解耦，优先考虑可解释的动力学，从而增强下游预测的场景表示。同时，框架集成了上下文感知表示学习和协作空间-运动预训练任务，能够同时优化结构推理和意图推理，同时捕获潜在的动态意图。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse 2基准测试上的实验展示了该方法优越的性能，结果证明了其在多智能体运动预测中的有效性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是自动驾驶领域中首个用于多智能体运动预测的上下文自编码器框架，代码和模型将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法往往面临标记数据稀缺的问题，并且在多智能体预测场景中表现不佳。为了解决这些挑战，我们引入了一个用于多智能体运动预测的解耦上下文感知预训练框架，名为DECAMP。与现有将表示学习与预训练任务纠缠在一起的方法不同，我们的框架将行为模式学习与潜在特征重建解耦，优先考虑可解释的动力学，从而增强下游预测的场景表示。此外，我们的框架集成了上下文感知表示学习和协作空间-运动预训练任务，这能够在捕获潜在动态意图的同时，优化结构推理和意图推理。我们在Argoverse 2基准测试上的实验展示了我们方法的优越性能，取得的结果强调了其在多智能体运动预测中的有效性。据我们所知，这是自动驾驶领域中首个用于多智能体运动预测的上下文自编码器框架。代码和模型将公开提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多智能体运动预测中的两个核心挑战：标记数据稀缺问题和场景一致性不足。在自动驾驶领域，准确预测多个交通参与者的互动行为对确保道路安全和效率至关重要，而场景不一致的预测会导致不合理的轨迹组合，影响自动驾驶汽车的可靠决策。解决这些问题不仅能提高预测准确性，还能降低系统开发和部署成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有监督学习和自监督方法的局限性，特别是编码器与预训练任务纠缠的问题以及单智能体方法扩展到多智能体场景的困难。受计算机视觉中掩码图像建模的启发，作者设计了'编码器-回归器-解码器'级联范式，引入回归器来减少编码器与特定任务的耦合。同时借鉴了Transformer架构、FPN和PointNet等现有技术，并创新性地设计了协作的空间-运动预训练任务，使模型能够同时捕获空间结构和动态意图。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦行为模式学习与潜在特征重建，使编码器专注于学习驾驶行为表示，而解码器专注于完成预训练任务。整体流程分为两个阶段：1)预训练阶段，输入历史、未来状态和地图，通过掩码策略和双解码器（空间解码器和运动解码器）分别重建空间线索和识别运动信号；2)微调阶段，仅使用历史状态和地图，通过预训练编码器生成K个场景一致的联合轨迹集合，确保预测结果在场景级别保持一致。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)解耦的'编码器-回归器-解码器'预训练框架，减少编码器与特定任务的纠缠；2)协作的空间-运动预训练任务，同时优化空间重建和运动识别；3)直接生成场景一致的多智能体联合轨迹，避免复杂的后处理；4)上下文感知表示学习，增强场景理解能力。相比之前工作，DECAMP解决了现有自监督方法中编码器与预训练任务耦合的问题，从单智能体预测扩展到多智能体联合预测，并通过解耦学习更鲁棒的行为先验，显著提高了预测准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DECAMP通过解耦行为模式学习与特征重建，并引入协作的空间-运动预训练任务，显著提高了多智能体运动预测的场景一致性和准确性，同时减少了对大量标记数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory prediction is a critical component of autonomous driving,essential for ensuring both safety and efficiency on the road. However,traditional approaches often struggle with the scarcity of labeled data andexhibit suboptimal performance in multi-agent prediction scenarios. To addressthese challenges, we introduce a disentangled context-aware pre-trainingframework for multi-agent motion prediction, named DECAMP. Unlike existingmethods that entangle representation learning with pretext tasks, our frameworkdecouples behavior pattern learning from latent feature reconstruction,prioritizing interpretable dynamics and thereby enhancing scene representationfor downstream prediction. Additionally, our framework incorporatescontext-aware representation learning alongside collaborative spatial-motionpretext tasks, which enables joint optimization of structural and intentionalreasoning while capturing the underlying dynamic intentions. Our experiments onthe Argoverse 2 benchmark showcase the superior performance of our method, andthe results attained underscore its effectiveness in multi-agent motionforecasting. To the best of our knowledge, this is the first contextautoencoder framework for multi-agent motion forecasting in autonomous driving.The code and models will be made publicly available.</description>
      <author>example@mail.com (Jianxin Shi, Zengqi Peng, Xiaolong Chen, Tianyu Wo, Jun Ma)</author>
      <guid isPermaLink="false">2509.10426v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning</title>
      <link>http://arxiv.org/abs/2509.10305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GundamQ的多尺度时空Q网络，用于解决动态环境中机器人路径规划的挑战。该框架包含时空感知模块和自适应策略优化模块，能够有效处理多尺度时间依赖性和平衡探索-利用关系。实验结果表明，该方法在成功率和路径质量方面显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;在动态和不确定的环境中，机器人路径规划需要准确的空间时间环境理解以及在部分可观测情况下的鲁棒决策能力。&lt;h4&gt;目的&lt;/h4&gt;解决当前基于深度强化学习的路径规划方法在多尺度时间依赖性建模不足和探索-利用平衡效率低下方面的局限，提高机器人路径规划的成功率和质量。&lt;h4&gt;方法&lt;/h4&gt;提出GundamQ框架，包含两个关键模块：(1)时空感知模块，分层提取多粒度空间特征和多尺度时间依赖性；(2)自适应策略优化模块，在训练中平衡探索和利用，并通过约束策略更新优化路径平滑度和碰撞概率。&lt;h4&gt;主要发现&lt;/h4&gt;在动态环境中的实验表明，GundamQ成功率达到15.3%的提升，整体路径质量提高21.7%，显著优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;GundamQ通过有效建模多尺度时空特征和优化探索-利用平衡，显著提高了动态环境中机器人路径规划的适应性和路径质量。&lt;h4&gt;翻译&lt;/h4&gt;在动态和不确定的环境中，机器人路径规划需要准确的空间时间环境理解以及在部分可观测情况下的鲁棒决策能力。然而，当前基于深度强化学习的路径规划方法面临两个基本局限：(1)对多尺度时间依赖性建模不足，导致在动态场景中适应性次优；(2)探索-利用平衡效率低下，导致路径质量下降。为解决这些挑战，我们提出GundamQ：一种用于机器人路径规划的多尺度时空Q网络。该框架包含两个关键模块：(i)时空感知模块，分层提取从瞬时到延长时间范围的多粒度空间特征和多尺度时间依赖性，从而提高动态环境中的感知准确性；(ii)自适应策略优化模块，在训练过程中平衡探索和利用，同时通过约束策略更新优化平滑度和碰撞概率。动态环境中的实验表明，GundamQ成功率达到15.3%的提升，整体路径质量提高21.7%，显著优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In dynamic and uncertain environments, robotic path planning demands accuratespatiotemporal environment understanding combined with robust decision-makingunder partial observability. However, current deep reinforcement learning-basedpath planning methods face two fundamental limitations: (1) insufficientmodeling of multi-scale temporal dependencies, resulting in suboptimaladaptability in dynamic scenarios, and (2) inefficient exploration-exploitationbalance, leading to degraded path quality. To address these challenges, wepropose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic PathPlanning. The framework comprises two key modules: (i) the SpatiotemporalPerception module, which hierarchically extracts multi-granularity spatialfeatures and multi-scale temporal dependencies ranging from instantaneous toextended time horizons, thereby improving perception accuracy in dynamicenvironments; and (ii) the Adaptive Policy Optimization module, which balancesexploration and exploitation during training while optimizing for smoothnessand collision probability through constrained policy updates. Experiments indynamic environments demonstrate that GundamQ achieves a 15.3\% improvement insuccess rate and a 21.7\% increase in overall path quality, significantlyoutperforming existing state-of-the-art methods.</description>
      <author>example@mail.com (Yutong Shen, Ruizhe Xia, Bokai Yan, Shunqi zhang, Pengrui Xiang, Sicheng He, Yixin Xu)</author>
      <guid isPermaLink="false">2509.10305v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>BenchECG and xECG: a benchmark and baseline for ECG foundation models</title>
      <link>http://arxiv.org/abs/2509.10151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 4 figures, 22 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了BenchECG作为心电图(ECG)基础模型的标准化基准，并提出了xECG模型，该模型在所有评估任务中表现最佳，为ECG表示学习设定了新的基线。&lt;h4&gt;背景&lt;/h4&gt;心电图(ECG)是一种低成本、广泛使用且适合深度学习的医疗数据。最近，人们对开发能够在各种下游任务中泛化的ECG基础模型的兴趣日益增长。然而，先前的研究缺乏一致的评估方法，常使用狭窄的任务选择和不一致的数据集，妨碍了公平比较。&lt;h4&gt;目的&lt;/h4&gt;引入BenchECG作为标准化的基准，包含全面的公开可用ECG数据集和多样化任务；同时提出并评估xECG模型，这是一种基于xLSTM的循环模型。&lt;h4&gt;方法&lt;/h4&gt;使用SimDINOv2自监督学习训练基于xLSTM的循环模型(xECG)，并在BenchECG基准上与公开的最先进模型进行比较评估。&lt;h4&gt;主要发现&lt;/h4&gt;与公开的最先进模型相比，xECG在BenchECG评分上取得了最佳成绩。特别地，xECG是唯一在所有数据集和任务上都表现出色的公开可用模型。&lt;h4&gt;结论&lt;/h4&gt;通过标准化评估，BenchECG能够进行严格的模型比较，并旨在加速ECG表示学习的进展。xECG比早期方法表现更优，为未来的ECG基础模型设定了新的基线。&lt;h4&gt;翻译&lt;/h4&gt;心电图(ECG)是一种低成本、广泛使用且适合深度学习的工具。最近，人们对开发能够跨多样化下游任务泛化的ECG基础模型的兴趣日益增长。然而，一直缺乏一致的评估：先前的工作通常使用狭窄的任务选择和不一致的数据集，妨碍了公平比较。在此，我们引入了BenchECG，一个包含全面公开可用ECG数据集和多样化任务的标准基准。我们还提出了xECG，这是一种基于xLSTM的循环模型，使用SimDINOv2自监督学习进行训练，与公开的最先进模型相比，它在BenchECG评分上取得了最佳成绩。特别是，xECG是唯一在所有数据集和任务上都表现强劲的公开可用模型。通过标准化评估，BenchECG能够进行严格的比较，并旨在加速ECG表示学习的进展。xECG比早期方法表现更优，为未来的ECG基础模型设定了新的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited todeep learning. Recently, interest has grown in developing foundation models forECGs - models that generalise across diverse downstream tasks. However,consistent evaluation has been lacking: prior work often uses narrow taskselections and inconsistent datasets, hindering fair comparison. Here, weintroduce BenchECG, a standardised benchmark comprising a comprehensive suiteof publicly available ECG datasets and versatile tasks. We also propose xECG,an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,which achieves the best BenchECG score compared to publicly availablestate-of-the-art models. In particular, xECG is the only publicly availablemodel to perform strongly on all datasets and tasks. By standardisingevaluation, BenchECG enables rigorous comparison and aims to accelerateprogress in ECG representation learning. xECG achieves superior performanceover earlier approaches, defining a new baseline for future ECG foundationmodels.</description>
      <author>example@mail.com (Riccardo Lunelli, Angus Nicolson, Samuel Martin Pröll, Sebastian Johannes Reinstadler, Axel Bauer, Clemens Dlaska)</author>
      <guid isPermaLink="false">2509.10151v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data</title>
      <link>http://arxiv.org/abs/2509.10048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究评估了将变分贝叶斯最后一层（VBLL）集成到表格先验数据拟合网络（TabPFN）中后对不确定性校准性能的影响，并在三个医疗数据集上进行了比较。&lt;h4&gt;背景&lt;/h4&gt;预测模型正被广泛应用于包括医疗诊断和刑事司法等安全关键领域，在这些领域中可靠的不确定性估计至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估VBLL集成到TabPFN中后对不确定性校准性能的影响。&lt;h4&gt;方法&lt;/h4&gt;在三个基准医疗表格数据集上比较原始TabPFN和VBLL集成版本的性能。&lt;h4&gt;主要发现&lt;/h4&gt;与预期相反，原始TabPFN在所有数据集上的不确定性校准性能均优于VBLL集成的TabPFN。&lt;h4&gt;结论&lt;/h4&gt;尽管VBLL在其他应用中能有效提高不确定性估计，但在与TabPFN集成时，原始TabPFN的不确定性校准表现更好。&lt;h4&gt;翻译&lt;/h4&gt;预测模型正被广泛应用于各种领域，包括医疗诊断和刑事司法等安全关键应用。在这样的场景中，可靠的不确定性估计是一项关键任务。表格先验数据拟合网络（TabPFN）是最近提出的一种用于表格数据集的机器学习基础模型，它采用生成式Transformer架构。变分贝叶斯最后一层（VBLL）是最先进的轻量级变分公式，能够以最小的计算开销有效提高不确定性估计。在本研究中，我们旨在评估VBLL与最近提出的TabPFN集成后在不确定性校准方面的性能。我们在三个基准医疗表格数据集上进行了实验，比较了原始TabPFN和VBLL集成版本的性能。与预期相反，我们在所有数据集上都观察到原始TabPFN的不确定性校准性能始终优于VBLL集成的TabPFN。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive models are being increasingly used across a wide range of domains,including safety-critical applications such as medical diagnosis and criminaljustice. Reliable uncertainty estimation is a crucial task in such settings.Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machinelearning foundation model for tabular dataset, which uses a generativetransformer architecture. Variational Bayesian Last Layers (VBLL) is astate-of-the-art lightweight variational formulation that effectively improvesuncertainty estimation with minimal computational overhead. In this work we aimto evaluate the performance of VBLL integrated with the recently proposedTabPFN in uncertainty calibration. Our experiments, conducted on threebenchmark medical tabular datasets, compare the performance of the originalTabPFN and the VBLL-integrated version. Contrary to expectations, we observedthat original TabPFN consistently outperforms VBLL integrated TabPFN inuncertainty calibration across all datasets.</description>
      <author>example@mail.com (Madhushan Ramalingam)</author>
      <guid isPermaLink="false">2509.10048v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images</title>
      <link>http://arxiv.org/abs/2509.09952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to SIGGRAPH Asia 2025. Project page:  https://ubisoft-laforge.github.io/world/chord&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的两阶段生成和估计框架用于PBR材质生成，解决了传统方法耗时耗力以及现有方法在质量、灵活性和用户控制方面的不足。&lt;h4&gt;背景&lt;/h4&gt;材质创建和重建对外观建模至关重要，但传统方法需要艺术家投入大量时间和专业知识。近期利用视觉基础模型合成PBR材质的方法在质量、灵活性和用户控制方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效、高质量且能提供灵活用户控制的PBR材质生成方法。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段框架：1)生成阶段：使用微调的扩散模型合成符合用户输入的、可平铺的纹理图像；2)估计阶段：引入链式分解方案，通过将先前提取的表示作为输入传递到单步图像条件扩散模型，顺序预测SVBRDF通道。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在材质生成和估计方面表现出优越性能，材质估计方法在生成的纹理和野外照片上都显示出强大的鲁棒性，且框架具有广泛的适用性。&lt;h4&gt;结论&lt;/h4&gt;该方法高效、高质量且能提供灵活的用户控制，在多种应用场景中表现优异，包括文本到材质、图像到材质、结构引导的生成和材质编辑。&lt;h4&gt;翻译&lt;/h4&gt;材质创建和重建对外观建模至关重要，但传统上需要艺术家投入大量时间和专业知识。虽然最近的方法利用视觉基础模型从用户提供的输入合成PBR材质，但它们在质量、灵活性和用户控制方面常常不尽如人意。我们提出了一种新颖的两阶段生成和估计框架用于PBR材质生成。在生成阶段，微调的扩散模型合成符合用户输入的、可平铺的纹理图像。在估计阶段，我们引入了一种链式分解方案，通过将先前提取的表示作为输入传递到单步图像条件扩散模型，顺序预测SVBRDF通道。我们的方法高效、高质量且能提供灵活的用户控制。我们将我们的方法与现有的材质生成和估计方法进行了比较，展示了优越的性能。我们的材质估计方法在生成的纹理和野外照片上都显示出强大的鲁棒性。此外，我们展示了我们的框架在多种应用中的灵活性，包括文本到材质、图像到材质、结构引导的生成和材质编辑。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决PBR材质生成和估计的问题。传统方法需要专业艺术家花费大量时间，而现有自动生成方法在质量、灵活性和用户控制方面表现不佳。这个问题在现实中很重要，因为材质建模是游戏、电影、虚拟现实等领域的核心需求，影响着数字内容的真实感和视觉效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：多通道压缩限制了灵活性，逆向渲染问题具有不确定性。因此设计了两阶段框架：先生成纹理图像，再分解为材质通道。借鉴了扩散模型（如SDXL）、图像条件扩散模型（如RGB→X）、内在分解方法和材质生成统一框架（如MatFusion）的思想，但进行了创新改进以解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两阶段框架和链式分解（Chord）处理材质生成和估计。第一阶段使用微调的扩散模型生成可平铺的纹理RGB图像；第二阶段按特定顺序（基础颜色→法线→粗糙度和金属度）预测SVBRDF通道，利用LEGO-conditioning为不同模态提供特定权重。整体流程包括纹理生成、材质估计两个主要阶段，训练过程包含预训练和单步两个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）两阶段生成和估计框架；2）链式分解方案（Chord）；3）LEGO-conditioning机制；4）单步训练方法。相比之前的工作，该方法先生成纹理再分解而非直接预测通道，更好地处理了模态间关系，利用扩散模型先验知识提高了质量和效率，并更明确地建模了SVBRDF模态间的内在关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种两阶段生成和估计框架，通过链式分解和LEGO-conditioning实现了高质量、高效率且用户可控的PBR材质生成和估计，显著提升了材质生成质量和用户控制能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Material creation and reconstruction are crucial for appearance modeling buttraditionally require significant time and expertise from artists. While recentmethods leverage visual foundation models to synthesize PBR materials fromuser-provided inputs, they often fall short in quality, flexibility, and usercontrol. We propose a novel two-stage generate-and-estimate framework for PBRmaterial generation. In the generation stage, a fine-tuned diffusion modelsynthesizes shaded, tileable texture images aligned with user input. In theestimation stage, we introduce a chained decomposition scheme that sequentiallypredicts SVBRDF channels by passing previously extracted representation asinput into a single-step image-conditional diffusion model. Our method isefficient, high quality, and enables flexible user control. We evaluate ourapproach against existing material generation and estimation methods,demonstrating superior performance. Our material estimation method shows strongrobustness on both generated textures and in-the-wild photographs. Furthermore,we highlight the flexibility of our framework across diverse applications,including text-to-material, image-to-material, structure-guided generation, andmaterial editing.</description>
      <author>example@mail.com (Zhi Ying, Boxiang Rong, Jingyu Wang, Maoyuan Xu)</author>
      <guid isPermaLink="false">2509.09952v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Segment Anything for Cell Tracking</title>
      <link>http://arxiv.org/abs/2509.09943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于SAM2的零样本细胞追踪框架，解决了现有方法对标记数据集依赖性强且泛化能力有限的问题，实现了在2D和3D显微镜视频中的高精度细胞追踪，无需数据集特定的微调。&lt;h4&gt;背景&lt;/h4&gt;在时间推移显微镜图像序列中追踪细胞和检测有丝分裂事件是生物医学研究中的关键任务，但由于分裂物体、低信噪比、不明确的边界、密集簇和单个细胞的视觉相似性，这仍然极具挑战性。现有的基于深度学习的方法依赖于手动标记的数据集进行训练，这既昂贵又耗时，且由于显微镜数据的巨大多样性，它们对未见过的数据集的泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;克服现有细胞追踪方法对标记数据集的依赖性和泛化能力有限的限制，开发一种无需微调即可跨多样化显微镜数据集泛化的细胞追踪方法。&lt;h4&gt;方法&lt;/h4&gt;通过将Segment Anything 2（SAM2）这种为通用图像和视频分割设计的大型基础模型整合到细胞追踪流程中，构建了一种完全无监督的零样本细胞追踪框架。该方法不依赖于任何特定的训练数据集，也不继承任何特定训练数据集的偏差。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在二维和大规模三维时间推移显微镜视频中都取得了具有竞争力的准确性，同时消除了对数据集特定适应的需求。&lt;h4&gt;结论&lt;/h4&gt;通过整合SAM2大型基础模型，作者提出了一种完全无监督的零样本细胞追踪方法，能够在多样化显微镜数据集上泛化，无需数据集特定的微调，同时保持高准确性。&lt;h4&gt;翻译&lt;/h4&gt;在时间推移显微镜图像序列中追踪细胞和检测有丝分裂事件是生物医学研究中的关键任务。然而，由于分裂物体、低信噪比、不明确的边界、密集簇以及单个细胞的视觉相似性，这仍然极具挑战性。现有的基于深度学习的方法依赖于手动标记的数据集进行训练，这既昂贵又耗时。此外，由于显微镜数据的巨大多样性，它们对未见过的数据集的泛化能力仍然有限。为了克服这些限制，我们通过将Segment Anything 2（SAM2）——一种为通用图像和视频分割设计的大型基础模型——整合到追踪流程中，提出了一种零样本细胞追踪框架。作为一种完全无监督的方法，我们的方法不依赖于或继承任何特定训练数据集的偏差，使其能够在无需微调的情况下跨多样化显微镜数据集进行泛化。我们的方法在二维和大规模三维时间推移显微镜视频中都取得了具有竞争力的准确性，同时消除了对数据集特定适应的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tracking cells and detecting mitotic events in time-lapse microscopy imagesequences is a crucial task in biomedical research. However, it remains highlychallenging due to dividing objects, low signal-tonoise ratios, indistinctboundaries, dense clusters, and the visually similar appearance of individualcells. Existing deep learning-based methods rely on manually labeled datasetsfor training, which is both costly and time-consuming. Moreover, theirgeneralizability to unseen datasets remains limited due to the vast diversityof microscopy data. To overcome these limitations, we propose a zero-shot celltracking framework by integrating Segment Anything 2 (SAM2), a large foundationmodel designed for general image and video segmentation, into the trackingpipeline. As a fully-unsupervised approach, our method does not depend on orinherit biases from any specific training dataset, allowing it to generalizeacross diverse microscopy datasets without finetuning. Our approach achievescompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videoswhile eliminating the need for dataset-specific adaptation.</description>
      <author>example@mail.com (Zhu Chen, Mert Edgü, Er Jin, Johannes Stegmaier)</author>
      <guid isPermaLink="false">2509.09943v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios</title>
      <link>http://arxiv.org/abs/2509.09926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为LoFT的长尾半监督学习框架，通过基础模型微调解决传统方法中的过度自信和低质量伪标签问题，并进一步提出LoFT-OW处理开放世界条件下的半监督学习。实验表明，即使只使用以往方法1%的未标记数据，新方法也能取得优越性能。&lt;h4&gt;背景&lt;/h4&gt;长尾学习在现实场景中应用广泛。长尾半监督学习(LTSSL)通过整合大量未标记数据到不平衡标记数据集中成为有效解决方案，但现有方法多从零开始训练，导致过度自信和低质量伪标签问题。&lt;h4&gt;目的&lt;/h4&gt;扩展LTSSL到基础模型微调范式，解决从头训练导致的问题；探索更实际的开放世界条件下的半监督学习场景。&lt;h4&gt;方法&lt;/h4&gt;提出LoFT(Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning)框架，利用微调基础模型生成更可靠伪标签；提出LoFT-OW处理开放世界条件下的半监督学习问题，提高模型判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;微调的基础模型能生成更可靠的伪标签；在多个基准测试上，新方法性能优于以前方法；即使只使用以往方法1%的未标记数据，也能获得更好性能。&lt;h4&gt;结论&lt;/h4&gt;LoFT框架有效解决了LTSSL中的过度自信和低质量伪标签问题；LoFT-OW能处理开放世界条件下的半监督学习；基础模型微调是解决长尾半监督学习问题的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;长尾学习由于其现实场景中的广泛适用性而获得越来越多的关注。在现有方法中，长尾半监督学习(LTSSL)通过将大量未标记数据整合到不平衡标记数据集中，已成为一种有效的解决方案。然而，大多数先前的LTSSL方法设计为从头开始训练模型，这常常导致过度自信和低质量伪标签等问题。为了解决这些挑战，我们将LTSSL扩展到基础模型微调范式，并提出了一种新框架：LoFT(通过参数高效微调的长尾半监督学习)。我们证明微调的基础模型可以生成更可靠的伪标签，从而有利于不平衡学习。此外，我们通过研究开放世界条件下的半监督学习来探索更实际的设置，其中未标记数据可能包括分布外(OOD)样本。为了解决这个问题，我们提出LoFT-OW(开放世界场景下的LoFT)来提高判别能力。在多个基准测试上的实验结果表明，与以前的方法相比，我们的方法取得了优越的性能，即使与以前的工作相比只使用1%的未标记数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-tailed learning has garnered increasing attention due to its wideapplicability in real-world scenarios. Among existing approaches, Long-TailedSemi-Supervised Learning (LTSSL) has emerged as an effective solution byincorporating a large amount of unlabeled data into the imbalanced labeleddataset. However, most prior LTSSL methods are designed to train models fromscratch, which often leads to issues such as overconfidence and low-qualitypseudo-labels. To address these challenges, we extend LTSSL into the foundationmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailedsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstratethat fine-tuned foundation models can generate more reliable pseudolabels,thereby benefiting imbalanced learning. Furthermore, we explore a morepractical setting by investigating semi-supervised learning under open-worldconditions, where the unlabeled data may include out-of-distribution (OOD)samples. To handle this problem, we propose LoFT-OW (LoFT under Open-Worldscenarios) to improve the discriminative ability. Experimental results onmultiple benchmarks demonstrate that our method achieves superior performancecompared to previous approaches, even when utilizing only 1\% of the unlabeleddata compared with previous works.</description>
      <author>example@mail.com (Jiahao Chen, Zhiyuan Huang, Yurou Liu, Bing Su)</author>
      <guid isPermaLink="false">2509.09926v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>How well can LLMs provide planning feedback in grounded environments?</title>
      <link>http://arxiv.org/abs/2509.09790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了预训练基础模型（如大型语言模型和视觉语言模型）在具身环境规划中提供反馈的能力，评估了不同类型反馈和推理方法的效果，发现基础模型能提供高质量反馈，且更大规模的模型表现更好。&lt;h4&gt;背景&lt;/h4&gt;在具身环境中进行规划学习通常需要精心设计的奖励函数或高质量标注的演示数据，这增加了学习成本和难度。&lt;h4&gt;目的&lt;/h4&gt;评估基础模型在不同类型环境中提供反馈的能力，研究不同反馈类型和推理方法对规划性能的影响，探索基础模型减少对奖励设计和演示数据依赖的可能性。&lt;h4&gt;方法&lt;/h4&gt;研究者在符号、语言和连续控制环境中评估了LLMs和VLMs的反馈能力，考虑了二元反馈、偏好反馈、行动建议、目标建议和增量动作反馈等反馈类型，以及上下文学习、思维链和访问环境动态信息等推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型能够在不同领域提供多样化的高质量反馈；更大规模的推理模型提供更准确的反馈，表现出更少的偏见，并且从增强的推理方法中获益更多；反馈质量在具有复杂动态或连续状态空间和动作空间的环境中会下降。&lt;h4&gt;结论&lt;/h4&gt;基础模型可以作为规划学习的有效反馈来源，减少对奖励设计和演示数据的依赖，但在复杂动态环境中可能需要额外的改进或辅助方法。&lt;h4&gt;翻译&lt;/h4&gt;在具身环境中学习规划通常需要精心设计的奖励函数或高质量的标注演示。最近的研究表明，预训练的基础模型（如大型语言模型和视觉语言模型）捕捉了对规划有用的背景知识，这减少了策略学习所需的奖励设计和演示数据的数量。我们评估了LLMs和VLMs在符号、语言和连续控制环境中提供反馈的能力。我们考虑了规划的主要反馈类型，包括二元反馈、偏好反馈、行动建议、目标建议和增量动作反馈。我们还研究了影响反馈性能的推理方法，包括上下文学习、思维链和访问环境动态信息。我们发现基础模型能够在不同领域提供多样化的高质量反馈。此外，更大规模的推理模型始终提供更准确的反馈，表现出更少的偏见，并且从增强的推理方法中获益更多。最后，对于具有复杂动态或连续状态空间和动作空间的环境，反馈质量会下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning to plan in grounded environments typically requires carefullydesigned reward functions or high-quality annotated demonstrations. Recentworks show that pretrained foundation models, such as large language models(LLMs) and vision language models (VLMs), capture background knowledge helpfulfor planning, which reduces the amount of reward design and demonstrationsneeded for policy learning. We evaluate how well LLMs and VLMs provide feedbackacross symbolic, language, and continuous control environments. We considerprominent types of feedback for planning including binary feedback, preferencefeedback, action advising, goal advising, and delta action feedback. We alsoconsider inference methods that impact feedback performance, includingin-context learning, chain-of-thought, and access to environment dynamics. Wefind that foundation models can provide diverse high-quality feedback acrossdomains. Moreover, larger and reasoning models consistently provide moreaccurate feedback, exhibit less bias, and benefit more from enhanced inferencemethods. Finally, feedback quality degrades for environments with complexdynamics or continuous state spaces and action spaces.</description>
      <author>example@mail.com (Yuxuan Li, Victor Zhong)</author>
      <guid isPermaLink="false">2509.09790v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>AI-enabled tuberculosis screening in a high-burden setting using cough sound analysis and speech foundation models</title>
      <link>http://arxiv.org/abs/2509.09746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to The Lancet Digital Health&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用基于语音基础模型的深度学习技术分析咳嗽声音，以检测结核病。研究在赞比亚招募了500名参与者，包括TB患者、其他呼吸道疾病患者和健康人。仅使用音频的分类器表现良好，而结合人口统计和临床特征的多模态模型性能更佳，达到WHO目标产品特征基准。&lt;h4&gt;背景&lt;/h4&gt;人工智能可用于检测与疾病相关的咳嗽声音模式，为高负担、低资源环境中的结核病筛查提供可扩展方法。以往研究存在数据集小、症状性非TB患者代表性不足、依赖简单模型、在理想条件下收集录音等局限性。&lt;h4&gt;目的&lt;/h4&gt;开发并评估基于深度学习的咳嗽声音分析模型，用于结核病的筛查和分诊，特别是在资源有限的环境中。&lt;h4&gt;方法&lt;/h4&gt;在赞比亚两家医院招募512名参与者，分为TB患者组、其他呼吸道疾病患者组和健康对照组。从500名参与者获取咳嗽录音及人口统计和临床数据。使用基于语音基础模型的深度学习分类器训练模型，并在3秒片段上表现最佳的模型进一步评估人口统计和临床特征的影响。&lt;h4&gt;主要发现&lt;/h4&gt;仅音频分类器区分TB+与其他人的AUROC为85.2%，区分TB+与其他呼吸道疾病的AUROC为80.1%。加入人口统计和临床特征后，性能提升至92.1%和84.2%。在0.38阈值下，多模态模型对TB+/Rest达到90.3%敏感性和73.1%特异性，对TB+/OR达到80.6%和73.1%特异性。&lt;h4&gt;结论&lt;/h4&gt;基于语音基础模型的咳嗽分析，特别是结合人口统计和临床数据时，作为结核病分诊工具显示出强大潜力，达到WHO目标产品特征基准。该模型对背景噪声、录音时间和设备变异性等混杂因素具有鲁棒性，表明检测到真实的疾病相关声学模式。临床应用前需在不同地区和病例定义中进一步验证。&lt;h4&gt;翻译&lt;/h4&gt;背景：人工智能可检测与疾病相关的咳嗽声音模式，为高负担、低资源环境中的结核病筛查提供可扩展方法。以往研究受限于小数据集、症状性非结核病患者代表性不足、依赖简单模型及在理想条件下收集录音。方法：我们在赞比亚两家医院招募512名参与者，分为细菌学确认的结核病患者、有其他呼吸道疾病的症状患者和健康对照组。从500名参与者获取可用的咳嗽录音及人口统计和临床数据。基于语音基础模型的深度学习分类器在咳嗽录音上进行训练。表现最佳的模型在3秒片段上训练，并进一步使用人口统计和临床特征进行评估。结果：仅音频的最佳分类器区分结核病患者与其他人的AUROC为85.2%，区分结核病患者与其他呼吸道疾病患者的AUROC为80.1%。加入人口统计和临床特征后，性能提升至92.1%和84.2%。在0.38阈值下，多模态模型对结核病患者与其他人达到90.3%敏感性和73.1%特异性，对结核病患者与其他呼吸道疾病患者达到80.6%和73.1%特异性。结论：使用语音基础模型进行咳嗽分析，特别是结合人口统计和临床数据时，显示出作为结核病分诊工具的强大潜力，达到WHO目标产品特征基准。该模型对背景噪声、录音时间和设备变异性等混杂因素具有鲁棒性，表明检测到真实的疾病相关声学模式。临床应用前需在不同地区和病例定义中进一步验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background  Artificial intelligence (AI) can detect disease-related acoustic patterns incough sounds, offering a scalable approach to tuberculosis (TB) screening inhigh-burden, low-resource settings. Previous studies have been limited by smalldatasets, under-representation of symptomatic non-TB patients, reliance onsimple models, and recordings collected under idealised conditions.  Methods  We enrolled 512 participants at two hospitals in Zambia, grouped asbacteriologically confirmed TB (TB+), symptomatic patients with otherrespiratory diseases (OR), and healthy controls (HC). Usable cough recordingsplus demographic and clinical data were obtained from 500 participants. Deeplearning classifiers based on speech foundation models were trained on coughrecordings. The best-performing model, trained on 3-second segments, wasfurther evaluated with demographic and clinical features.  Findings  The best audio-only classifier achieved an AUROC of 85.2% for distinguishingTB+ from all others (TB+/Rest) and 80.1% for TB+ versus OR. Adding demographicand clinical features improved performance to 92.1% (TB+/Rest) and 84.2%(TB+/OR). At a threshold of 0.38, the multimodal model reached 90.3%sensitivity and 73.1% specificity for TB+/Rest, and 80.6% and 73.1% for TB+/OR.  Interpretation  Cough analysis using speech foundation models, especially when combined withdemographic and clinical data, showed strong potential as a TB triage tool,meeting WHO target product profile benchmarks. The model was robust toconfounding factors including background noise, recording time, and devicevariability, indicating detection of genuine disease-related acoustic patterns.Further validation across diverse regions and case definitions, includingsubclinical TB, is required before clinical use.</description>
      <author>example@mail.com (Ning Ma, Bahman Mirheidari, Guy J. Brown, Minyoi M. Maimbolwa, Nsala Sanjase, Solomon Chifwamba, Seke Muzazu, Monde Muyoyeta, Mary Kagujje)</author>
      <guid isPermaLink="false">2509.09746v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>MatSKRAFT: A framework for large-scale materials knowledge extraction from scientific tables</title>
      <link>http://arxiv.org/abs/2509.10448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MatSKRAFT是一个计算框架，能够前所未有地大规模自动提取和整合材料科学知识，从表格数据中构建包含超过535,000个条目的综合数据库，显著优于现有模型，并促进材料发现。&lt;h4&gt;背景&lt;/h4&gt;科学进步越来越需要综合大量文献中的知识，但大多数实验数据仍然被困在半结构化格式中，难以进行系统提取和分析。&lt;h4&gt;目的&lt;/h4&gt;开发一个计算框架，能够前所未有地大规模自动提取和整合材料科学知识。&lt;h4&gt;方法&lt;/h4&gt;MatSKRAFT将表格转换为基于图的表示，通过约束驱动的图神经网络处理，并将科学原理直接编码到模型架构中。&lt;h4&gt;主要发现&lt;/h4&gt;MatSKRAFT在属性提取方面达到88.68的F1分数，在成分提取方面达到71.35的F1分数，处理速度比其他模型快19-496倍，硬件要求适中；应用于47,000多篇研究论文的近69,000个表格，构建了包含535,000多个条目和104,000种成分的综合数据库，扩展了现有数据库覆盖范围，揭示了具有独特属性组合的材料，并实现了成分-属性关系的发现。&lt;h4&gt;结论&lt;/h4&gt;这种系统化的方法为材料和科学发现奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;科学进步越来越依赖于综合大量文献中的知识，但大多数实验数据仍然被困在半结构化格式中，难以进行系统提取和分析。在此，我们提出了MatSKRAFT，一个计算框架，能够前所未有地大规模自动提取和整合材料科学知识。我们的方法将表格转换为基于图的表示，通过约束驱动的图神经网络处理，将科学原理直接编码到模型架构中。MatSKRAFT显著优于最先进的大型语言模型，在属性提取方面达到88.68的F1分数，在成分提取方面达到71.35的F1分数，同时处理速度比这些模型快19-496倍（与最慢和最快模型相比），且硬件要求适中。应用于来自47,000多篇研究论文的近69,000个表格，我们构建了一个包含超过535,000个条目的综合数据库，其中包括104,000种成分，扩展了现有数据库的覆盖范围，有待人工验证。这种系统化的方法揭示了以前被忽视的具有独特属性组合的材料，并实现了构成材料和科学发现基石的成分-属性关系的发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific progress increasingly depends on synthesizing knowledge acrossvast literature, yet most experimental data remains trapped in semi-structuredformats that resist systematic extraction and analysis. Here, we presentMatSKRAFT, a computational framework that automatically extracts and integratesmaterials science knowledge from tabular data at unprecedented scale. Ourapproach transforms tables into graph-based representations processed byconstraint-driven GNNs that encode scientific principles directly into modelarchitecture. MatSKRAFT significantly outperforms state-of-the-art largelanguage models, achieving F1 scores of 88.68 for property extraction and 71.35for composition extraction, while processing data $19$-$496\times$ faster thanthem (compared to the slowest and the fastest models, respectively) with modesthardware requirements. Applied to nearly 69,000 tables from more than 47,000research publications, we construct a comprehensive database containing over535,000 entries, including 104,000 compositions that expand coverage beyondmajor existing databases, pending manual validation. This systematic approachreveals previously overlooked materials with distinct property combinations andenables data-driven discovery of composition-property relationships forming thecornerstone of materials and scientific discovery.</description>
      <author>example@mail.com (Kausik Hira, Mohd Zaki, Mausam, N. M. Anoop Krishnan)</author>
      <guid isPermaLink="false">2509.10448v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Modeling and Risk Zoning of Global Extreme Precipitation via Graph Neural Networks and r-Pareto Processes</title>
      <link>http://arxiv.org/abs/2509.10362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合图神经网络与r-Pareto过程(GNN-rP)的混合框架，用于空间极端降水建模和风险分区，能够学习非线性、非平稳依赖结构，并应用于高风险区域识别和气候变化下的热点检测。&lt;h4&gt;背景&lt;/h4&gt;极端降水事件对人类社会构成重大威胁，可引发大范围复合洪水、山体滑坡和基础设施故障，传统统计空间极值模型存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合框架用于空间极端降水建模和风险分区，能够学习非线性、非平稳依赖结构，并识别气候变化下的高风险区域。&lt;h4&gt;方法&lt;/h4&gt;提出结合图神经网络与r-Pareto过程(GNN-rP)的混合框架，从降水衍生的空间图中学习依赖结构，应用数据驱动的尾部函数建模低维嵌入空间中的联合超越概率，使用NASA的IMERG观测数据和CMIP6 SSP5-8.5投影进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;框架成功描绘了连贯的高风险区域，量化了其时序持续性，检测到气候变化下的新兴热点；与基线方法相比显著提高了高风险网格单元的检测能力；结果显示热带带特别是季风和对流区存在持续高风险区域，并揭示十年尺度持续性在高排放情景下被偶发性重组中断。&lt;h4&gt;结论&lt;/h4&gt;通过结合机器学习与极值理论，GNN-rP提供了一种可扩展、可解释的自适应气候风险分区工具，可直接应用于基础设施规划、防灾准备和气候韧性政策设计。&lt;h4&gt;翻译&lt;/h4&gt;发生在广大空间区域的极端降水事件对社会构成重大威胁，因为它们可能在大范围内引发复合洪水、山体滑坡和基础设施故障。本文提出了一种用于空间极端降水建模和风险分区的混合框架，该框架结合了图神经网络与r-Pareto过程(GNN-rP)。与传统统计空间极值模型不同，这种方法从降水衍生的空间图中学习非线性、非平稳的依赖结构，并应用数据驱动的尾部函数来建模低维嵌入空间中的联合超越概率。利用NASA的IMERG观测数据(2000-2021)和CMIP6 SSP5-8.5投影，该框架描绘了连贯的高风险区域，量化了它们的时序持续性，并检测了气候变化下的新兴热点。与两种基线方法相比，GNN-rP管道显著提高对高风险网格单元的逐点检测能力，同时保持了相当的聚类稳定性。结果突显了热带带特别是季风和对流区存在持续高风险区域，并揭示了在高排放情景下被偶发性重组所中断的十年尺度持续性。通过将机器学习与极值理论相结合，GNN-rP为自适应气候风险分区提供了一种可扩展、可解释的工具，可直接应用于基础设施规划、防灾准备和气候韧性政策设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extreme precipitation events occurring over large spatial domains posesubstantial threats to societies because they can trigger compound flooding,landslides, and infrastructure failures across wide areas. A hybrid frameworkfor spatial extreme precipitation modeling and risk zoning is proposed thatintegrates graph neural networks with r-Pareto processes (GNN-rP). Unliketraditional statistical spatial extremes models, this approach learnsnonlinear, nonstationary dependence structures from precipitation-derivedspatial graphs and applies a data-driven tail functional to model jointexceedances in a low-dimensional embedding space. Using NASA's IMERGobservations (2000-2021) and CMIP6 SSP5-8.5 projections, the frameworkdelineates coherent high-risk zones, quantifies their temporal persistence, anddetects emerging hotspots under climate change. Compared with two baselineapproaches, the GNN-rP pipeline substantially improves pointwise detection ofhigh-risk grid cells while yielding comparable clustering stability. Resultshighlight persistent high-risk regions in the tropical belt, especially monsoonand convective zones, and reveal decadal-scale persistence that is punctuatedby episodic reconfigurations under high-emission scenarios. By coupling machinelearning with extreme value theory, GNN-rP offers a scalable, interpretabletool for adaptive climate risk zoning, with direct applications ininfrastructure planning, disaster preparedness, and climate-resilient policydesign.</description>
      <author>example@mail.com (Zimu Wang, Yifan Wu, Daning Bi)</author>
      <guid isPermaLink="false">2509.10362v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Why does your graph neural network fail on some graphs? Insights from exact generalisation error</title>
      <link>http://arxiv.org/abs/2509.10337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过信号处理的视角，推导了图神经网络(GNNs)在转导固定设计设置下的确切泛化误差，解释了GNNs成功或失败的原因，并为模型选择提供了实践指导。&lt;h4&gt;背景&lt;/h4&gt;图神经网络被广泛用于图结构数据学习，但对它们成功或失败的原因缺乏原则性理解。先前研究关注架构限制如过平滑和过挤压，但不能解释GNNs如何提取有意义表示或为什么相似架构间性能差异大。这些问题与泛化能力相关，而现有GNN泛化误差界限通常宽松、仅限于单一架构，且对实践中泛化机制的洞察有限。&lt;h4&gt;目的&lt;/h4&gt;通过信号处理方法推导GNNs的确切泛化误差，解释GNNs何时以及为何能有效利用结构和特征信息，为模型选择提供实践指导。&lt;h4&gt;方法&lt;/h4&gt;采用信号处理视角，将GNNs解释为通过图结构作用于节点特征的图滤波器算子。专注于线性GNNs同时允许图滤波器中的非线性，推导出包括卷积、基于PageRank和基于注意力模型在内的广泛GNNs的第一个确切泛化误差。&lt;h4&gt;主要发现&lt;/h4&gt;泛化误差的确切表征揭示出只有节点特征与图结构之间对齐的信息才对泛化有贡献；量化了同质性(homophily)对泛化的影响。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了一个框架，解释了GNNs何时以及为何能有效利用结构和特征信息，为模型选择提供了实践指导。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)被广泛应用于图结构数据的学习，然而对它们成功或失败的原因仍缺乏原则性理解。虽然先前研究已经考察了架构限制如过平滑和过挤压，但这些并不能解释GNNs如何提取有意义表示或为什么相似架构间性能差异巨大。这些问题与泛化能力相关：模型对未标记数据做出准确预测的能力。尽管已有研究推导了GNNs的泛化误差界限，但这些界限通常宽松、仅限于单一架构，且对实践中控制泛化的因素提供有限洞察。在本工作中，我们通过信号处理的视角，采用不同方法推导了GNNs在转导固定设计设置下的确切泛化误差。从这一观点看，GNNs可被解释为通过图结构作用于节点特征的图滤波器算子。在专注于线性GNNs的同时允许图滤波器中的非线性，我们推导出广泛GNNs（包括卷积、基于PageRank和基于注意力的模型）的第一个确切泛化误差。泛化误差的确切表征揭示出只有节点特征与图结构之间对齐的信息才对泛化有贡献。此外，我们量化了同质性对泛化的影响。我们的工作提供了一个框架，解释了GNNs何时以及为何能有效利用结构和特征信息，为模型选择提供了实践指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are widely used in learning on graph-structureddata, yet a principled understanding of why they succeed or fail remainselusive. While prior works have examined architectural limitations such asover-smoothing and over-squashing, these do not explain what enables GNNs toextract meaningful representations or why performance varies drasticallybetween similar architectures. These questions are related to the role ofgeneralisation: the ability of a model to make accurate predictions onunlabelled data. Although several works have derived generalisation errorbounds for GNNs, these are typically loose, restricted to a singlearchitecture, and offer limited insight into what governs generalisation inpractice. In this work, we take a different approach by deriving the exactgeneralisation error for GNNs in a transductive fixed-design setting throughthe lens of signal processing. From this viewpoint, GNNs can be interpreted asgraph filter operators that act on node features via the graph structure. Byfocusing on linear GNNs while allowing non-linearity in the graph filters, wederive the first exact generalisation error for a broad range of GNNs,including convolutional, PageRank-based, and attention-based models. The exactcharacterisation of the generalisation error reveals that only the alignedinformation between node features and graph structure contributes togeneralisation. Furthermore, we quantify the effect of homophily ongeneralisation. Our work provides a framework that explains when and why GNNscan effectively leverage structural and feature information, offering practicalguidance for model selection.</description>
      <author>example@mail.com (Nil Ayday, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar)</author>
      <guid isPermaLink="false">2509.10337v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Approximate Graph Propagation Revisited: Dynamic Parameterized Queries, Tighter Bounds and Dynamic Updates</title>
      <link>http://arxiv.org/abs/2509.10036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文重新研究了近似图传播(AGP)框架，针对动态图和动态参数化查询场景提出了改进算法AGP-Static++和AGP-Dynamic，显著提高了查询效率和更新性能。&lt;h4&gt;背景&lt;/h4&gt;AGP是一个统一框架，可以捕获各种图传播任务，如PageRank、图神经网络中的特征传播和基于图的检索增强生成(RAG)。在动态图和动态参数化查询场景下，底层图随时间演变，查询参数根据应用需求即时指定。&lt;h4&gt;目的&lt;/h4&gt;解决现有AGP-Static算法在动态参数化查询和动态图场景下的局限性，包括查询时间复杂度高和动态图更新效率低的问题。&lt;h4&gt;方法&lt;/h4&gt;提出两种新算法：1) AGP-Static++：简化算法设计，将查询复杂度降低约对数平方n因子，同时保留近似保证；2) AGP-Dynamic：实现每次更新常数时间的摊销时间，显著提高动态图更新效率，同时保持查询复杂度和近似保证。&lt;h4&gt;主要发现&lt;/h4&gt;1) AGP-Static可适应支持动态参数化查询，但存在查询复杂度高和动态图更新效率低的问题；2) AGP-Static++能显著降低查询复杂度；3) AGP-Dynamic能大幅提高动态图更新效率，实现常数时间摊销每次更新。&lt;h4&gt;结论&lt;/h4&gt;提出的AGP-Static++和AGP-Dynamic算法在理论和实验上都显著优于现有方法，与基线相比，在更新时间上实现了高达177倍的加速，在查询效率上实现了10倍的加速。&lt;h4&gt;翻译&lt;/h4&gt;我们重新研究了近似图传播(AGP)，这是一个统一框架，可以捕获各种图传播任务，如PageRank、图神经网络(GNN)中的特征传播和基于图的检索增强生成(RAG)。我们的工作重点是动态图和动态参数化查询的场景，其中底层图随时间演变（通过边插入或删除更新），输入查询参数根据应用需求即时指定。我们的第一个贡献是一个有趣的观察：SOTA解决方案AGP-Static可以适应支持动态参数化查询；然而，仍有几个挑战未解决。首先，AGP-Static的查询时间复杂度基于一个假设：在其查询算法中使用子集采样的最优算法。不幸的是，当时不存在这样的算法；没有这种最优算法，查询复杂度需要额外的对数平方n因子，其中n是图中的顶点数。其次，AGP-Static在动态图上表现不佳，每次更新需要n乘以对数n的时间。为了解决这些挑战，我们提出了一种新算法AGP-Static++，它更简单，同时将查询复杂度大致降低了对数平方n因子，同时保留了AGP-Static的近似保证。然而，AGP-Static++仍然需要n时间来处理每个更新。为了更好地支持动态图，我们进一步提出了AGP-Dynamic，它实现了每次更新常数时间的摊销时间，显著改善了前面提到的每次更新n的界限，同时仍然保留了查询复杂度和近似保证。最后，我们的全面实验验证了理论改进：与基线相比，我们的算法在更新时间上实现了高达177倍的加速，在查询效率上实现了10倍的加速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We revisit Approximate Graph Propagation (AGP), a unified framework whichcaptures various graph propagation tasks, such as PageRank, feature propagationin Graph Neural Networks (GNNs), and graph-based Retrieval-Augmented Generation(RAG). Our work focuses on the settings of dynamic graphs and dynamicparameterized queries, where the underlying graphs evolve over time (updated byedge insertions or deletions) and the input query parameters are specified onthe fly to fit application needs. Our first contribution is an interestingobservation that the SOTA solution, AGP-Static, can be adapted to supportdynamic parameterized queries; however several challenges remain unresolved.Firstly, the query time complexity of AGP-Static is based on an assumption ofusing an optimal algorithm for subset sampling in its query algorithm.Unfortunately, back to that time, such an algorithm did not exist; without suchan optimal algorithm, an extra $O(\log^2 n)$ factor is required in the querycomplexity, where $n$ is the number of vertices in the graphs. Secondly,AGP-Static performs poorly on dynamic graphs, taking $O(n\log n)$ time toprocess each update. To address these challenges, we propose a new algorithm,AGP-Static++, which is simpler yet reduces roughly a factor of $O(\log^2 n)$ inthe query complexity while preserving the approximation guarantees ofAGP-Static. However, AGP-Static++ still requires $O(n)$ time to process eachupdate. To better support dynamic graphs, we further propose AGP-Dynamic, whichachieves $O(1)$ amortized time per update, significantly improving theaforementioned $O(n)$ per-update bound, while still preserving the querycomplexity and approximation guarantees. Last, our comprehensive experimentsvalidate the theoretical improvements: compared to the baselines, our algorithmachieves speedups of up to $177\times$ on update time and $10\times$ on queryefficiency.</description>
      <author>example@mail.com (Zhuowei Zhao, Zhuo Zhang, Hanzhi Wang, Junhao Gan, Zhifeng Bao, Jianzhong Qi)</author>
      <guid isPermaLink="false">2509.10036v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>HGEN: Heterogeneous Graph Ensemble Networks</title>
      <link>http://arxiv.org/abs/2509.09843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper is in proceedings of the 34th IJCAI Conference, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HGEN是一种创新的异构图集成学习方法，通过元路径和转换优化流程集成多个学习器，提高分类准确性。&lt;h4&gt;背景&lt;/h4&gt;异构图中的节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来挑战，特别是在适应多样化的图学习器方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理异构图的集成学习框架，提高分类准确性，同时确保学习器之间的多样性。&lt;h4&gt;方法&lt;/h4&gt;HGEN使用元路径结合随机丢弃创建等位图神经网络(Allele GNNs)，训练基础图学习器并进行对齐。框架包含两个关键组件：残差注意力机制校准不同元路径的GNNs，使节点嵌入更关注信息量大的图；相关性正则化项扩大不同元路径生成的嵌入矩阵差异，增强学习器多样性。&lt;h4&gt;主要发现&lt;/h4&gt;HGEN通过残差注意力机制和相关正则化项有效提高了基础学习器的准确性和多样性。实验表明HGEN的正则化程度高于简单投票，在五个异构网络上的表现显著优于最先进竞争对手。&lt;h4&gt;结论&lt;/h4&gt;HGEN为异构图的集成学习提供了有效解决方案，通过创新的元路径和优化流程，显著提升了分类性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了HGEN，这是一种开创性的异构图集成学习方法。我们认为，节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来了重大挑战，特别是在适应多样化的图学习器方面。我们的HGEN框架通过基于元路径和转换的优化流程集成多个学习器，以提高分类准确性。具体来说，HGEN使用元路径结合随机丢弃创建等位图神经网络(GNNs)，基础图学习器在这些网络上进行训练和对齐以便后续集成。为确保有效的集成学习，HGEN提出了两个关键组件：1)残差注意力机制用于校准不同元路径的等位GNNs，使节点嵌入更专注于信息量更大的图，提高基础学习器准确性；2)相关性正则化项扩大不同元路径生成的嵌入矩阵之间的差异，从而丰富基础学习器多样性。我们分析了HGEN的收敛性，并证明其正则化程度高于简单投票。在五个异构网络上的实验验证了HGEN以显著优势持续优于最先进的竞争对手。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents HGEN that pioneers ensemble learning for heterogeneousgraphs. We argue that the heterogeneity in node types, nodal features, andlocal neighborhood topology poses significant challenges for ensemble learning,particularly in accommodating diverse graph learners. Our HGEN frameworkensembles multiple learners through a meta-path and transformation-basedoptimization pipeline to uplift classification accuracy. Specifically, HGENuses meta-path combined with random dropping to create Allele Graph NeuralNetworks (GNNs), whereby the base graph learners are trained and aligned forlater ensembling. To ensure effective ensemble learning, HGEN presents two keycomponents: 1) a residual-attention mechanism to calibrate allele GNNs ofdifferent meta-paths, thereby enforcing node embeddings to focus on moreinformative graphs to improve base learner accuracy, and 2) acorrelation-regularization term to enlarge the disparity among embeddingmatrices generated from different meta-paths, thereby enriching base learnerdiversity. We analyze the convergence of HGEN and attest its higherregularization magnitude over simple voting. Experiments on five heterogeneousnetworks validate that HGEN consistently outperforms its state-of-the-artcompetitors by substantial margin.</description>
      <author>example@mail.com (Jiajun Shen, Yufei Jin, Yi He, Xingquan Zhu)</author>
      <guid isPermaLink="false">2509.09843v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Commissioning and Testing of IceAct Telescopes at the IceCube Neutrino Observatory</title>
      <link>http://arxiv.org/abs/2509.09778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IceAct是一个位于IceCube中微子天文台上方的成像切伦科夫望远镜阵列，专为南极恶劣环境设计。该系统由七个望远镜组成'苍蝇眼'配置和一个额外的望远镜用于立体观测。研究团队进行了严格的测试和校准程序，包括现场几何对齐和图神经网络重建初级粒子方向。&lt;h4&gt;背景&lt;/h4&gt;IceAct是一个部署在南极冰面上的成像切伦科夫望远镜阵列，位于著名的IceCube中微子天文台上方。每个望远镜配备硅光电倍增器相机和菲涅尔透镜，具有12度视场。&lt;h4&gt;目的&lt;/h4&gt;研究旨在介绍IceAct望远镜的测试程序、现场对齐校准方法，以及使用图神经网络重建初级粒子方向的技术，并验证其在实际数据中的应用。&lt;h4&gt;方法&lt;/h4&gt;研究团队通过比较IceCube测量的μ子方向重建与IceAct重建的初级粒子方向，推导出每个IceAct望远镜的几何对齐。此外，还采用了图神经网络技术重建初级粒子方向，并通过蒙特卡洛模拟进行验证，最后应用于调试数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究展示了IceAct望远镜的测试程序，包括夜空观测和低温测试；开发了现场对齐校准方法；实现了基于图神经网络的初级粒子方向重建；验证了该方法在蒙特卡洛模拟中的有效性；并将该方法成功应用于调试数据集。&lt;h4&gt;结论&lt;/h4&gt;IceAct望远镜阵列已成功部署并进行了全面测试和校准，图神经网络重建方法表现良好，为未来中微子和宇宙射线观测提供了可靠的工具。&lt;h4&gt;翻译&lt;/h4&gt;IceAct是一个位于IceCube中微子天文台冰面上方的成像切伦科夫望远镜阵列。每个望远镜配备基于硅光电倍增器的61像素相机和菲涅尔透镜作为成像光学元件，产生12度的视场。设计针对恶劣环境进行了优化，特别是在南极。该装置将由七个望远镜组成在所谓的'苍蝇眼'配置中，将视场扩大到36度，另外还有一个相距200米的望远镜用于立体观测。在部署前进行了严格的测试程序，确保在这些条件下能够运行，例如夜空观测和低温测试。此外，现场校准用于验证安装的准确性和可靠性。我们通过比较使用IceCube测量的μ子的方向重建与IceAct对应初级粒子方向重建，推导出每个IceAct望远镜的几何对齐。本贡献展示了这些测试程序。此外，我们展示了现场对齐校准，包括IceAct中初级粒子方向的图神经网络重建、蒙特卡洛模拟验证以及应用于调试数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; IceAct is an array of imaging air Cherenkov telescopes located at the icesurface above the IceCube Neutrino Observatory. Each telescope features asilicon photomultiplier based 61 pixel camera and a Fresnel-lens as imagingoptic, resulting in a 12-degree field of view. The design is optimized to beoperated in harsh environments, particularly at the South Pole. The setup willconsist of seven telescopes in a so-called fly's eye configuration, increasingthe field of view to 36^\circ, and an additional telescope 200m apart forstereoscopic observations. Rigorous testing procedures have been performedbefore deployment to ensure that operation under these conditions is possible,e.g. night sky observations and cold temperature tests. Furthermore, on-sitecalibrations are used to verify the accuracy and reliability of theinstallation. We derive the geometric alignment of each IceAct telescope bycomparing the directional reconstruction of muons measured with IceCube to thecorresponding primary particle direction reconstruction from IceAct. Thiscontribution presents these testing procedures. Additionally, we present theon-site alignment calibration, including a Graph Neural Network reconstructionfor the primary particle direction in IceAct, verification on Monte Carlosimulation, and the application to a commissioning dataset.</description>
      <author>example@mail.com (Arun Vaidyanathan, Lars Heuermann)</author>
      <guid isPermaLink="false">2509.09778v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario</title>
      <link>http://arxiv.org/abs/2509.10096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to RA-L 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于辅助机器人的交互感知运动预测方法，通过创建新数据集和开发基于Transformer的条件去噪扩散模型，有效捕捉了护工与护理接受者之间的耦合动力学，显著提升了机器人在物理交互场景中的辅助能力。&lt;h4&gt;背景&lt;/h4&gt;劳动力短缺和人口老龄化问题日益严重，需要辅助机器人来支持人类护理接受者。在物理交互场景中，机器人需要准确预测人类动作以确保安全和响应性帮助，然而这仍然是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;解决辅助环境中可变性和物理交互中耦合动力学复杂性的挑战，实现安全、响应性辅助。&lt;h4&gt;方法&lt;/h4&gt;创建HHI-Assist数据集，包含辅助任务中人类-人类交互的运动捕捉片段；开发基于Transformer的条件去噪扩散模型，用于预测交互代理的姿态。&lt;h4&gt;主要发现&lt;/h4&gt;模型能有效捕捉护工和护理接受者之间的耦合动力学，相比基线方法有所改进，对未见过的场景有很强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过推进交互感知的运动预测和引入新数据集，显著增强机器人辅助策略。数据集和代码已在https://sites.google.com/view/hhi-assist/home公开可用。&lt;h4&gt;翻译&lt;/h4&gt;日益严重的劳动力短缺和人口老龄化凸显了对辅助机器人的需求，以支持人类护理接受者。为了实现安全且响应迅速的辅助，机器人在物理交互场景中需要准确预测人类动作。然而，由于辅助环境的多变性和物理交互中耦合动力学的复杂性，这仍然是一项具有挑战性的任务。在本工作中，我们通过两个关键贡献解决了这些挑战：(1) HHI-Assist，一个包含辅助任务中人类-人类交互动作捕捉片段的数据集；(2) 一种基于Transformer的条件去噪扩散模型，用于预测交互代理的姿态。我们的模型有效地捕捉了护工和护理接受者之间的耦合动力学，展示了相比基线方法的改进，并对未见场景表现出强大的泛化能力。通过推进交互感知的运动预测和引入新数据集，我们的工作有潜力显著增强机器人辅助策略。数据集和代码可在以下网址获取：https://sites.google.com/view/hhi-assist/home&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3586011&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing labor shortage and aging population underline the need forassistive robots to support human care recipients. To enable safe andresponsive assistance, robots require accurate human motion prediction inphysical interaction scenarios. However, this remains a challenging task due tothe variability of assistive settings and the complexity of coupled dynamics inphysical interactions. In this work, we address these challenges through twokey contributions: (1) HHI-Assist, a dataset comprising motion capture clips ofhuman-human interactions in assistive tasks; and (2) a conditionalTransformer-based denoising diffusion model for predicting the poses ofinteracting agents. Our model effectively captures the coupled dynamics betweencaregivers and care receivers, demonstrating improvements over baselines andstrong generalization to unseen scenarios. By advancing interaction-awaremotion prediction and introducing a new dataset, our work has the potential tosignificantly enhance robotic assistance policies. The dataset and code areavailable at: https://sites.google.com/view/hhi-assist/home</description>
      <author>example@mail.com (Saeed Saadatnejad, Reyhaneh Hosseininejad, Jose Barreiros, Katherine M. Tsui, Alexandre Alahi)</author>
      <guid isPermaLink="false">2509.10096v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
      <link>http://arxiv.org/abs/2509.06426v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了首个果蝇腿部的三维数据驱动肌肉骨骼模型，填补了现有研究空白。该模型连接了神经活动和运动表现，为理解运动控制提供了新工具。通过结合实验数据和仿真，模型预测了肌肉协同作用，并测试了关节特性对学习的影响。该模型不仅有助于基础科学研究，还可应用于开发更自然的机器人运动控制。&lt;h4&gt;背景&lt;/h4&gt;计算模型对于理解神经、生物力学和物理系统如何相互作用以协调动物行为至关重要。尽管已有果蝇中枢神经系统、肌肉和骨骼的完整重建，但基于解剖学和物理学的果蝇腿肌模型仍然缺失。这些模型是运动神经元活动和关节运动之间不可或缺的桥梁。&lt;h4&gt;目的&lt;/h4&gt;引入第一个果蝇腿部的三维、数据驱动的肌肉骨骼模型，并在OpenSim和MuJoCo仿真环境中实现该模型。&lt;h4&gt;方法&lt;/h4&gt;基于来自多个固定标本的高分辨率X射线扫描，采用Hill型肌肉表示。提出了一个使用形态学成像数据构建肌肉模型的流程，并优化了果蝇特有的未知肌肉参数。将肌肉骨骼模型与来自行为果蝇的详细3D姿态估计数据相结合，在OpenSim中实现肌肉驱动的行为重放。在MuJoCo中训练模仿学习策略，测试不同被动关节特性对学习速度的影响。&lt;h4&gt;主要发现&lt;/h4&gt;模拟多种行走和梳理行为中的肌肉活动预测了协调的肌肉协同作用，这些协同作用可以通过实验进行测试。阻尼和刚度有助于学习。&lt;h4&gt;结论&lt;/h4&gt;该模型使得在实验上易于处理的模式生物中研究运动控制成为可能，深入了解生物力学如何促进复杂肢体运动的产生。该模型还可用于控制具身人工智能体，在模拟环境中生成自然且柔顺的运动。&lt;h4&gt;翻译&lt;/h4&gt;计算模型对于推进我们理解神经、生物力学和物理系统如何相互作用以协调动物行为至关重要。尽管已有果蝇(Drosophila melanogaster)中枢神经系统、肌肉和骨骼的近乎完整重建，但基于解剖学和物理学的果蝇腿肌模型仍然缺失。这些模型是运动神经元活动和关节运动之间不可或缺的桥梁。在此，我们介绍了首个果蝇腿部的三维、数据驱动的肌肉骨骼模型，在OpenSim和MuJoCo仿真环境中实现。我们的模型纳入了基于来自多个固定标本的高分辨率X射线扫描的Hill型肌肉表示。我们提出了一个使用形态学成像数据构建肌肉模型的流程，并优化了果蝇特有的未知肌肉参数。然后，我们将肌肉骨骼模型与来自行为果蝇的详细3D姿态估计数据相结合，在OpenSim中实现肌肉驱动的行为重放。对多种行走和梳理行为中肌肉活动的模拟预测了可通过实验测试的协调肌肉协同作用。此外，通过在MuJoCo中训练模仿学习策略，我们测试了不同被动关节特性对学习速度的影响，发现阻尼和刚度有助于学习。总体而言，我们的模型使得在实验上易于处理的模式生物中研究运动控制成为可能，深入了解生物力学如何促进复杂肢体运动的产生。此外，我们的模型可用于控制具身人工智能体，在模拟环境中生成自然且柔顺的运动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational models are critical to advance our understanding of how neural,biomechanical, and physical systems interact to orchestrate animal behaviors.Despite the availability of near-complete reconstructions of the Drosophilamelanogaster central nervous system, musculature, and exoskeleton, anatomicallyand physically grounded models of fly leg muscles are still missing. Thesemodels provide an indispensable bridge between motor neuron activity and jointmovements. Here, we introduce the first 3D, data-driven musculoskeletal modelof Drosophila legs, implemented in both OpenSim and MuJoCo simulationenvironments. Our model incorporates a Hill-type muscle representation based onhigh-resolution X-ray scans from multiple fixed specimens. We present apipeline for constructing muscle models using morphological imaging data andfor optimizing unknown muscle parameters specific to the fly. We then combineour musculoskeletal models with detailed 3D pose estimation data from behavingflies to achieve muscle-actuated behavioral replay in OpenSim. Simulations ofmuscle activity across diverse walking and grooming behaviors predictcoordinated muscle synergies that can be tested experimentally. Furthermore, bytraining imitation learning policies in MuJoCo, we test the effect of differentpassive joint properties on learning speed and find that damping and stiffnessfacilitate learning. Overall, our model enables the investigation of motorcontrol in an experimentally tractable model organism, providing insights intohow biomechanics contribute to generation of complex limb movements. Moreover,our model can be used to control embodied artificial agents to generatenaturalistic and compliant locomotion in simulated environments.</description>
      <author>example@mail.com (Pembe Gizem Özdil, Chuanfang Ning, Jasper S. Phelps, Sibo Wang-Chen, Guy Elisha, Alexander Blanke, Auke Ijspeert, Pavan Ramdya)</author>
      <guid isPermaLink="false">2509.06426v2</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Resource-Efficient Glioma Segmentation on Sub-Saharan MRI</title>
      <link>http://arxiv.org/abs/2509.09469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种针对资源受限环境的深度学习框架，用于撒哈拉以南非洲地区的胶质瘤MRI分割，克服了数据稀缺的挑战，取得了良好的分割效果，模型紧凑且高效。&lt;h4&gt;背景&lt;/h4&gt;胶质瘤是最常见的原发性脑肿瘤类型，从MRI图像中准确分割它们对诊断、治疗计划和纵向监测至关重要。然而，撒哈拉以南非洲地区高质量标注影像数据的稀缺是部署高级分割模型在临床工作流程中的重大挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍一种为资源受限环境量身定制的稳健且计算效率高的深度学习框架，以支持低资源环境中的临床决策。&lt;h4&gt;方法&lt;/h4&gt;使用3D注意力UNet架构，增加残差块，并通过从BraTS 2021数据集上的预训练权重进行迁移学习来增强模型。在95个来自BraTS-Africa数据集的MRI病例上评估模型。&lt;h4&gt;主要发现&lt;/h4&gt;尽管数据质量和数量有限，模型仍取得了良好的分割效果：增强肿瘤的Dice分数为0.76，坏死和非增强肿瘤核心的Dice分数为0.80，周围非功能半球的Dice分数为0.85。模型具有约90MB的紧凑架构，在消费级硬件上的每卷推理时间不到一分钟。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明了所提出模型的泛化能力及其在资源有限环境中支持临床决策的潜力。这项工作通过为服务不足的地区提供高性能且可及的医学影像解决方案，为缩小全球健康领域公平AI的差距做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;胶质瘤是最常见的原发性脑肿瘤类型，从MRI图像中准确分割它们对诊断、治疗计划和纵向监测至关重要。然而，撒哈拉以南非洲地区高质量标注影像数据的稀缺是部署高级分割模型在临床工作流程中的重大挑战。本研究介绍了一种为资源受限环境量身定制的稳健且计算效率高的深度学习框架。我们利用了3D注意力UNet架构，增加了残差块，并通过从BraTS 2021数据集上的预训练权重进行迁移学习来增强模型。我们的模型在来自BraTS-Africa数据集的95个MRI病例上进行了评估，该数据集是撒哈拉以南非洲MRI数据胶质瘤分割的基准。尽管数据质量和数量有限，我们的方法仍取得了良好的效果：增强肿瘤的Dice分数为0.76，坏死和非增强肿瘤核心的Dice分数为0.80，周围非功能半球的Dice分数为0.85。这些结果表明了所提出模型的泛化能力及其在资源有限环境中支持临床决策的潜力。模型约90MB的紧凑架构以及在消费级硬件上每卷不到一分钟的推理时间进一步证明了其在撒哈拉以南非洲卫生系统中部署的实用性。这项工作通过为服务不足的地区提供高性能且可及的医学影像解决方案，为缩小全球健康领域公平AI的差距做出了贡献。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在撒哈拉以南非洲地区资源有限的情况下，如何高效地对胶质瘤MRI图像进行分割的问题。这个问题很重要，因为胶质瘤是最常见的原发性脑肿瘤，准确分割对诊断和治疗至关重要；而该地区放射科医生稀缺，现有深度学习方法又需要大量计算资源，无法在资源有限的环境中部署；此外，该地区胶质瘤患者预后很差，近80%患者在诊断后两年内死亡，亟需更好的诊断工具。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法在撒哈拉以南非洲地区的局限性，包括计算资源需求高、数据质量差异大等。他们借鉴了3D U-Net架构、残差块和注意力机制等现有技术，同时针对资源受限环境进行了优化。作者采用两阶段训练策略：先在大型BraTS 2021数据集上预训练，再在BraTS-Africa数据集上微调，以提高模型在特定地区数据上的表现。此外，他们还设计了轻量级模型和优化预处理流程，以适应低质量MRI图像和有限计算资源的环境。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个轻量级的3D注意力U-Net架构，结合残差块，并通过迁移学习策略提高在撒哈拉以南非洲特定数据上的分割性能。整体流程包括：1)使用BraTS-Africa数据集，包含95名胶质瘤患者的多参数MRI扫描；2)预处理阶段进行图像裁剪、模态堆叠和强度归一化；3)应用数据增强提高模型泛化能力；4)构建3D注意力U-Net与残差块的模型架构；5)采用两阶段训练策略进行预训练和微调；6)使用5折交叉验证和多种指标评估模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)针对资源受限环境优化的轻量级模型(约91.4MB)，在消费级硬件上实现快速推理；2)专门针对撒哈拉以南非洲MRI图像的低分辨率和模态差异进行优化；3)两阶段训练策略，结合大规模预训练和特定领域微调；4)结合残差块和注意力机制提高分割精度。相比之前工作，该方法更轻量、更适合资源有限环境，且专门针对撒哈拉以南非洲地区的特定挑战进行了优化，而大多数现有方法没有考虑这些因素。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种轻量级的3D注意力U-Net模型，通过迁移学习优化了在撒哈拉以南非洲资源受限环境下对胶质瘤MRI图像的高效分割，为医疗资源不足地区提供了可及的医学影像解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gliomas are the most prevalent type of primary brain tumors, and theiraccurate segmentation from MRI is critical for diagnosis, treatment planning,and longitudinal monitoring. However, the scarcity of high-quality annotatedimaging data in Sub-Saharan Africa (SSA) poses a significant challenge fordeploying advanced segmentation models in clinical workflows. This studyintroduces a robust and computationally efficient deep learning frameworktailored for resource-constrained settings. We leveraged a 3D Attention UNetarchitecture augmented with residual blocks and enhanced through transferlearning from pre-trained weights on the BraTS 2021 dataset. Our model wasevaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for gliomasegmentation in SSA MRI data. Despite the limited data quality and quantity,our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for SurroundingNon-Functional Hemisphere (SNFH). These results demonstrate thegeneralizability of the proposed model and its potential to support clinicaldecision making in low-resource settings. The compact architecture,approximately 90 MB, and sub-minute per-volume inference time on consumer-gradehardware further underscore its practicality for deployment in SSA healthsystems. This work contributes toward closing the gap in equitable AI forglobal health by empowering underserved regions with high-performing andaccessible medical imaging solutions.</description>
      <author>example@mail.com (Freedmore Sidume, Oumayma Soula, Joseph Muthui Wacira, YunFei Zhu, Abbas Rabiu Muhammad, Abderrazek Zeraii, Oluwaseun Kalejaye, Hajer Ibrahim, Olfa Gaddour, Brain Halubanza, Dong Zhang, Udunna C Anazodo, Confidence Raymond)</author>
      <guid isPermaLink="false">2509.09469v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
  <item>
      <title>From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models</title>
      <link>http://arxiv.org/abs/2509.09303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于弱监督的专利-SDG分类方法，利用专利引用到SDG标记的科学出版物作为初始信号，结合大型语言模型提取结构化概念，并通过跨域相似度计算和复合标记函数进行校准，最终生成高质量的专利-SDG映射数据集。&lt;h4&gt;背景&lt;/h4&gt;将专利按照联合国可持续发展目标(SDGs)分类对于追踪创新如何应对全球挑战至关重要。然而，缺乏大型标记数据集限制了监督学习的应用。现有方法如关键词搜索、迁移学习和基于引用的启发式方法缺乏可扩展性和通用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效将专利映射到SDGs的方法，克服数据稀缺问题，同时保证分类的准确性和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;将专利-SDG分类视为弱监督问题，使用专利引用到SDG标记科学出版物(NPL引用)作为初始信号；开发复合标记函数(LF)，利用大型语言模型从专利和SDG论文中提取功能、解决方案和应用等结构化概念；基于专利本体计算跨域相似度分数；通过仅正损失函数校准标记函数；生成银标准软多标签数据集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在内部验证中优于多个基线模型(包括基于transformer的模型和零样本LLM)；在外部验证中，与传统技术分类相比，生成的标签在专利引用、共同发明人和共同申请人网络中显示出更大的主题、认知和组织一致性。&lt;h4&gt;结论&lt;/h4&gt;弱监督和语义对齐能够增强大规模SDG分类的效果，为追踪创新如何应对全球挑战提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;根据联合国可持续发展目标(SDGs)对专利进行分类对于追踪创新如何应对全球挑战至关重要。然而，缺乏大型标记数据集限制了监督学习的使用。现有方法如关键词搜索、迁移学习和基于引用的启发式方法缺乏可扩展性和通用性。本文将专利-SDG分类视为弱监督问题，使用专利引用到SDG标记的科学出版物(NPL引用)作为初始信号。为了解决其稀疏性和噪声问题，我们开发了一个复合标记函数(LF)，利用大型语言模型(LLMs)基于专利本体从专利和SDG论文中提取结构化概念，即功能、解决方案和应用。使用基于检索的方法计算和组合跨域相似度分数。通过定制的仅正损失函数对LF进行校准，与已知的NPL-SDG链接保持一致，同时不惩罚新SDG关联的发现。结果是一个银标准、软多标签数据集，将专利映射到SDGs，能够训练有效的多标签回归模型。我们通过两种互补策略验证该方法：(1)针对保留的NPL标签进行内部验证，我们的方法优于多个基线模型，包括基于transformer的模型和零样本LLM；(2)使用专利引用、共同发明人和共同申请人图中的网络模块度进行外部验证，我们的标签显示出比传统技术分类更大的主题、认知和组织一致性。这些结果表明，弱监督和语义对齐能够增强大规模SDG分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classifying patents by their relevance to the UN Sustainable DevelopmentGoals (SDGs) is crucial for tracking how innovation addresses globalchallenges. However, the absence of a large, labeled dataset limits the use ofsupervised learning. Existing methods, such as keyword searches, transferlearning, and citation-based heuristics, lack scalability and generalizability.This paper frames patent-to-SDG classification as a weak supervision problem,using citations from patents to SDG-tagged scientific publications (NPLcitations) as a noisy initial signal. To address its sparsity and noise, wedevelop a composite labeling function (LF) that uses large language models(LLMs) to extract structured concepts, namely functions, solutions, andapplications, from patents and SDG papers based on a patent ontology.Cross-domain similarity scores are computed and combined using a rank-basedretrieval approach. The LF is calibrated via a custom positive-only loss thataligns with known NPL-SDG links without penalizing discovery of new SDGassociations. The result is a silver-standard, soft multi-label dataset mappingpatents to SDGs, enabling the training of effective multi-label regressionmodels. We validate our approach through two complementary strategies: (1)internal validation against held-out NPL-based labels, where our methodoutperforms several baselines including transformer-based models, and zero-shotLLM; and (2) external validation using network modularity in patent citation,co-inventor, and co-applicant graphs, where our labels reveal greater thematic,cognitive, and organizational coherence than traditional technologicalclassifications. These results show that weak supervision and semanticalignment can enhance SDG classification at scale.</description>
      <author>example@mail.com (Grazia Sveva Ascione, Nicolò Tamagnone)</author>
      <guid isPermaLink="false">2509.09303v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Data Utilization for Multilingual Dense Retrieval</title>
      <link>http://arxiv.org/abs/2509.09459v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP 2025 (main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种提高多语言密集检索中数据利用率的方法，通过获取高质量的难负样本和有效的小批量数据来提升检索性能&lt;h4&gt;背景&lt;/h4&gt;多语言密集检索旨在基于统一的检索器模型在不同语言中检索相关文档，其挑战在于将不同语言的表示在共享向量空间中对齐&lt;h4&gt;目的&lt;/h4&gt;提高多语言密集检索中的数据利用率，解决对比学习中负样本质量和小批量数据效率的问题&lt;h4&gt;方法&lt;/h4&gt;不同于专注于开发复杂模型架构的现有研究，提出通过获取高质量的难负样本和有效的小批量数据来提升多语言密集检索性能的方法&lt;h4&gt;主要发现&lt;/h4&gt;在包含16种语言的多语言检索基准测试MIRACL上的广泛实验结果表明，该方法有效且性能优于多个现有的强基线模型&lt;h4&gt;结论&lt;/h4&gt;通过提高数据利用率，特别是获取高质量的难负样本和有效的小批量数据，可以有效提升多语言密集检索的性能&lt;h4&gt;翻译&lt;/h4&gt;多语言密集检索旨在基于统一的检索器模型在不同语言中检索相关文档。挑战在于将不同语言的表示在共享向量空间中对齐。常见做法是通过对比学习微调密集检索器，其有效性高度依赖于负样本的质量和小批量数据的效率。与专注于开发复杂模型架构的现有研究不同，我们提出了一种通过获取高质量的难负样本和有效的小批量数据来提高多语言密集检索数据利用率的方法。在包含16种语言的多语言检索基准MIRACL上的广泛实验结果证明了我们方法的有效性，性能优于几个现有的强基线模型&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multilingual dense retrieval aims to retrieve relevant documents acrossdifferent languages based on a unified retriever model. The challenge lies inaligning representations of different languages in a shared vector space. Thecommon practice is to fine-tune the dense retriever via contrastive learning,whose effectiveness highly relies on the quality of the negative sample and theefficacy of mini-batch data. Different from the existing studies that focus ondeveloping sophisticated model architecture, we propose a method to boost datautilization for multilingual dense retrieval by obtaining high-quality hardnegative samples and effective mini-batch data. The extensive experimentalresults on a multilingual retrieval benchmark, MIRACL, with 16 languagesdemonstrate the effectiveness of our method by outperforming several existingstrong baselines.</description>
      <author>example@mail.com (Chao Huang, Fengran Mo, Yufeng Chen, Changhao Guan, Zhenrui Yue, Xinyu Wang, Jinan Xu, Kaiyu Huang)</author>
      <guid isPermaLink="false">2509.09459v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Listening for "You": Enhancing Speech Image Retrieval via Target Speaker Extraction</title>
      <link>http://arxiv.org/abs/2509.09306v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于目标说话人语音的图像检索方法，通过对比学习整合音频和视觉模型，在多说话人场景中实现了显著优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;使用语音提示进行图像检索是多模态感知领域的一个有前景的方向，但在多说话人场景中有效利用语音信号仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的'目标说话人语音-图像检索'任务和框架，学习在有目标说话人存在的情况下图像与多说话人语音信号之间的关系。&lt;h4&gt;方法&lt;/h4&gt;将预训练的自监督音频编码器与视觉模型通过目标说话人感知对比学习进行整合，基于目标说话人提取和检索模块，使系统能够从目标说话人处提取语音命令并将其与相应图像对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在SpokenCOCO2Mix和SpokenCOCO3Mix上的实验表明，TSRE方法在2人和3人场景中分别实现了36.3%和29.9%的Recall@1，显著优于单说话人基线和最先进模型。&lt;h4&gt;结论&lt;/h4&gt;该方法在辅助机器人和多模态交互系统的实际部署中显示出巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;使用语音提示进行图像检索已成为多模态感知领域的一个有前景的方向，但在多说话人场景中利用语音仍然具有挑战性。我们提出了一种新的目标说话人语音-图像检索任务和框架，学习在有目标说话人存在的情况下图像与多说话人语音信号之间的关系。我们的方法通过目标说话人感知对比学习，将预训练的自监督音频编码器与视觉模型整合，基于目标说话人提取和检索模块。这使系统能够从目标说话人处提取语音命令并将其与相应图像对齐。在SpokenCOCO2Mix和SpokenCOCO3Mix上的实验表明，TSRE显著优于现有方法，在2人和3人场景中分别实现了36.3%和29.9%的Recall@1，比单说话人基线和最先进模型有显著提升。我们的方法在辅助机器人和多模态交互系统的实际部署中显示出潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image retrieval using spoken language cues has emerged as a promisingdirection in multimodal perception, yet leveraging speech in multi-speakerscenarios remains challenging. We propose a novel Target Speaker Speech-ImageRetrieval task and a framework that learns the relationship between images andmulti-speaker speech signals in the presence of a target speaker. Our methodintegrates pre-trained self-supervised audio encoders with vision models viatarget speaker-aware contrastive learning, conditioned on a Target SpeakerExtraction and Retrieval module. This enables the system to extract spokencommands from the target speaker and align them with corresponding images.Experiments on SpokenCOCO2Mix and SpokenCOCO3Mix show that TSRE significantlyoutperforms existing methods, achieving 36.3% and 29.9% Recall@1 in 2 and 3speaker scenarios, respectively - substantial improvements over single speakerbaselines and state-of-the-art models. Our approach demonstrates potential forreal-world deployment in assistive robotics and multimodal interaction systems.</description>
      <author>example@mail.com (Wenhao Yang, Jianguo Wei, Wenhuan Lu, Xinyue Song, Xianghu Yue)</author>
      <guid isPermaLink="false">2509.09306v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Over-the-Air Adversarial Attack Detection: from Datasets to Defenses</title>
      <link>http://arxiv.org/abs/2509.09296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了AdvSV 2.0数据集，提出了基于神经回放模拟器的对抗攻击方法和CODA-OCC防御方法，有效提高了自动说话人验证系统的安全性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;自动说话人验证(ASV)系统可用于语音应用的身份验证，但容易受到线上(OTL)和空中(OTA)对抗性攻击的威胁。现有的检测方法由于缺乏全面的数据集而未得到充分测试。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的数据集来解决对抗性攻击检测方法缺乏充分测试的问题，并提高ASV系统对对抗攻击的防御能力。&lt;h4&gt;方法&lt;/h4&gt;开发了AdvSV 2.0数据集，包含628k个样本和800小时音频数据，涵盖经典对抗攻击算法和ASV系统，包括OTL和OTA场景。引入了基于神经回放模拟器(NRS)的新型对抗攻击方法，并提出了CODA-OCC防御方法，这是一种在一类分类框架中的对比学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;CODA-OCC在AdvSV 2.0数据集上实现了11.2%的等错误率(EER)和0.95的AUC，优于几种最先进的检测方法。基于神经回放模拟器的攻击方法增强了OTA攻击的效力，对ASV系统构成更大威胁。&lt;h4&gt;结论&lt;/h4&gt;AdvSV 2.0数据集为对抗性攻击研究提供了重要资源，CODA-OCC方法能有效防御ASV系统面临的威胁，提高了系统的安全性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;自动说话人验证(ASV)系统可用于语音应用的身份验证。然而，最近的研究揭示了这些系统容易受到线上(OTL)和空中(OTA)对抗性攻击的漏洞。尽管已经提出了各种检测方法来应对这些威胁，但由于缺乏全面的数据集，这些方法没有得到充分测试。为了解决这一差距，我们开发了AdvSV 2.0数据集，包含628k个样本，总时长800小时。该数据集包含经典对抗攻击算法、ASV系统，并涵盖OTL和OTA场景。此外，我们引入了一种基于神经回放模拟器(NRS)的新型对抗攻击方法，增强了OTA对抗攻击的效力，从而对ASV系统构成更大威胁。为了防御这些攻击，我们提出了CODA-OCC，一种在一类分类框架中的对比学习方法。实验结果表明，在AdvSV 2.0数据集上，CODA-OCC的EER为11.2%，AUC为0.95，优于几种最先进的检测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic Speaker Verification (ASV) systems can be used for voice-enabledapplications for identity verification. However, recent studies have exposedthese systems' vulnerabilities to both over-the-line (OTL) and over-the-air(OTA) adversarial attacks. Although various detection methods have beenproposed to counter these threats, they have not been thoroughly tested due tothe lack of a comprehensive data set. To address this gap, we developed theAdvSV 2.0 dataset, which contains 628k samples with a total duration of 800hours. This dataset incorporates classical adversarial attack algorithms, ASVsystems, and encompasses both OTL and OTA scenarios. Furthermore, we introducea novel adversarial attack method based on a Neural Replay Simulator (NRS),which enhances the potency of adversarial OTA attacks, thereby presenting agreater threat to ASV systems. To defend against these attacks, we proposeCODA-OCC, a contrastive learning approach within the one-class classificationframework. Experimental results show that CODA-OCC achieves an EER of 11.2% andan AUC of 0.95 on the AdvSV 2.0 dataset, outperforming several state-of-the-artdetection methods.</description>
      <author>example@mail.com (Li Wang, Xiaoyan Lei, Haorui He, Lei Wang, Jie Shi, Zhizheng Wu)</author>
      <guid isPermaLink="false">2509.09296v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis</title>
      <link>http://arxiv.org/abs/2509.09251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多注意力元Transformer方法（MMT-FD）用于少样本无监督旋转机械故障诊断，从未标记数据中提取潜在故障表示，具有强大泛化能力，适用于各种类型机械设备的故障诊断。&lt;h4&gt;背景&lt;/h4&gt;旋转机械设备的智能故障诊断通常需要大量标记样本数据，但在实际工业应用中获取足够数据在时间和成本方面具有挑战性。此外，不同类型的旋转机械设备具有独特机械特性，需要为每种情况单独训练诊断模型。&lt;h4&gt;目的&lt;/h4&gt;解决有限故障样本和预测模型在实际工程应用中缺乏泛化能力的问题，提出适用于少样本无监督旋转机械故障诊断的方法。&lt;h4&gt;方法&lt;/h4&gt;MMT-FD框架集成了时频域编码器和元学习泛化模型。时频域编码器预测通过时频域随机增强生成的状态表示，这些增强数据被输入到元学习网络中进行分类和泛化训练，然后使用少量标记数据进行微调。模型通过少量对比学习迭代进行优化，实现高效率。&lt;h4&gt;主要发现&lt;/h4&gt;在轴承故障数据集和转子试验台数据上的实验表明，MMT-FD模型仅使用1%的标记样本数据就能实现99%的故障诊断准确率，表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MMT-FD框架能够有效解决有限故障样本和模型泛化能力的问题，适用于各种类型机械设备的故障诊断。&lt;h4&gt;翻译&lt;/h4&gt;旋转机械设备的智能故障诊断通常需要大量标记样本数据。然而，在实际工业应用中，获取足够的数据在时间和成本方面都具有挑战性和高成本。此外，不同类型的旋转机械设备具有不同的独特机械特性，需要为每种情况单独训练诊断模型。为了解决有限故障样本和预测模型在实际工程应用中缺乏泛化能力的问题，我们提出了一种用于少样本无监督旋转机械故障诊断的多注意力元Transformer方法（MMT-FD）。该框架从未标记数据中提取潜在故障表示，并表现出强大的泛化能力，使其适用于诊断各种类型机械设备的故障。MMT-FD框架集成了一个时频域编码器和一个元学习泛化模型。时频域编码器预测通过时频域随机增强生成的状态表示。这些增强数据随后被输入到元学习网络中进行分类和泛化训练，然后使用少量标记数据进行微调。模型通过少量对比学习迭代进行优化，从而实现高效率。为了验证该框架，我们在轴承故障数据集和转子试验台数据上进行了实验。结果表明，MMT-FD模型仅使用1%的标记样本数据就能实现99%的故障诊断准确率，表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The intelligent fault diagnosis of rotating mechanical equipment usuallyrequires a large amount of labeled sample data. However, in practicalindustrial applications, acquiring enough data is both challenging andexpensive in terms of time and cost. Moreover, different types of rotatingmechanical equipment with different unique mechanical properties, requireseparate training of diagnostic models for each case. To address the challengesof limited fault samples and the lack of generalizability in prediction modelsfor practical engineering applications, we propose a Multi-Attention MetaTransformer method for few-shot unsupervised rotating machinery fault diagnosis(MMT-FD). This framework extracts potential fault representations fromunlabeled data and demonstrates strong generalization capabilities, making itsuitable for diagnosing faults across various types of mechanical equipment.The MMT-FD framework integrates a time-frequency domain encoder and ameta-learning generalization model. The time-frequency domain encoder predictsstatus representations generated through random augmentations in thetime-frequency domain. These enhanced data are then fed into a meta-learningnetwork for classification and generalization training, followed by fine-tuningusing a limited amount of labeled data. The model is iteratively optimizedusing a small number of contrastive learning iterations, resulting in highefficiency. To validate the framework, we conducted experiments on a bearingfault dataset and rotor test bench data. The results demonstrate that theMMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeledsample data, exhibiting robust generalization capabilities.</description>
      <author>example@mail.com (Hanyang Wang, Yuxuan Yang, Hongjun Wang, Lihui Wang)</author>
      <guid isPermaLink="false">2509.09251v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing</title>
      <link>http://arxiv.org/abs/2509.09160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE International Conference on Multimedia and Expo  (ICME 2025). \copyright\ 2025 IEEE. Personal use of this material is  permitted. Permission from IEEE must be obtained for all other uses&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种反事实增强去偏框架，用于减少目标导向多模态情感分类中的虚假相关性，提高分类准确性。&lt;h4&gt;背景&lt;/h4&gt;目标导向多模态情感分类旨在预测图像-文本对中特定目标的情感极性。现有方法虽取得有竞争力性能，但过度依赖文本内容，忽视数据集偏差特别是词级上下文偏差，导致文本特征与输出标签间存在虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;减少文本特征与输出标签间的虚假相关性，提高目标导向多模态情感分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;引入反事实增强去偏框架，包含：1)反事实数据增强策略，最小程度改变情感相关因果特征，生成细节匹配的图像-文本来引导模型关注情感相关内容；2)自适应去偏对比学习机制，从反事实数据中学习鲁棒特征并提示模型决策，有效减轻偏差词影响。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验结果表明，提出的方法优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;通过反事实增强去偏框架可以有效减少虚假相关性，提高目标导向多模态情感分类的准确性。&lt;h4&gt;翻译&lt;/h4&gt;目标导向多模态情感分类旨在预测图像-文本对中特定目标的情感极性。虽然现有工作取得了有竞争力的性能，但它们往往过度依赖文本内容，而未能考虑数据集偏差，特别是词级上下文偏差。这导致文本特征与输出标签之间存在虚假相关性，影响了分类准确性。在本文中，我们引入了一种新颖的反事实增强去偏框架来减少此类虚假相关性。我们的框架结合了一种反事实数据增强策略，该策略最小程度地改变情感相关的因果特征，生成细节匹配的图像-文本来引导模型关注与情感相关的内容。此外，为了从反事实数据中学习鲁棒特征并提示模型决策，我们引入了一种自适应去偏对比学习机制，有效减轻了偏差词的影响。在多个基准数据集上的实验结果表明，我们提出的方法优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Target-oriented multimodal sentiment classification seeks to predictsentiment polarity for specific targets from image-text pairs. While existingworks achieve competitive performance, they often over-rely on textual contentand fail to consider dataset biases, in particular word-level contextualbiases. This leads to spurious correlations between text features and outputlabels, impairing classification accuracy. In this paper, we introduce a novelcounterfactual-enhanced debiasing framework to reduce such spuriouscorrelations. Our framework incorporates a counterfactual data augmentationstrategy that minimally alters sentiment-related causal features, generatingdetail-matched image-text samples to guide the model's attention toward contenttied to sentiment. Furthermore, for learning robust features fromcounterfactual data and prompting model decisions, we introduce an adaptivedebiasing contrastive learning mechanism, which effectively mitigates theinfluence of biased words. Experimental results on several benchmark datasetsshow that our proposed method outperforms state-of-the-art baselines.</description>
      <author>example@mail.com (Zhiyue Liu, Fanrong Ma, Xin Ling)</author>
      <guid isPermaLink="false">2509.09160v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval</title>
      <link>http://arxiv.org/abs/2509.09118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP2025 Main&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种改进的CLIP模型（GA-DMS）用于人物表征学习，通过构建大规模高质量数据集和改进模型架构解决了CLIP在人物表征中的两个关键挑战。&lt;h4&gt;背景&lt;/h4&gt;CLIP在多种视觉任务中表现出色，但在人物表征学习方面面临两个关键挑战：缺乏大规模标注的人物中心化视觉-语言数据；全局对比学习的固有局限性难以保持细粒度匹配所需的判别性局部特征，同时容易受到噪声文本标记的影响。&lt;h4&gt;目的&lt;/h4&gt;通过数据整理和模型架构的协同改进，推进CLIP在人物表征学习中的应用。&lt;h4&gt;方法&lt;/h4&gt;1) 开发抗噪声数据构建流程，利用MLLMs自动过滤和标注网络图像，创建WebPerson数据集（500万高质量人物中心化图像-文本对）；2) 提出GA-DMS框架，通过梯度-注意力相似度分数自适应掩码噪声文本标记；3) 结合掩码标记预测目标增强细粒度语义表征学习。&lt;h4&gt;主要发现&lt;/h4&gt;GA-DMS在多个基准测试中取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过数据构建和模型架构的改进，成功解决了CLIP在人物表征学习中的两个关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;尽管对比语言-图像预训练（CLIP）在多种视觉任务中表现出色，但其应用于人物表征学习面临两个关键挑战：（i）缺乏大规模标注的人物中心化视觉-语言数据，以及（ii）全局对比学习的固有局限性，难以保持细粒度匹配所需的判别性局部特征，同时容易受到噪声文本标记的影响。本研究通过数据整理和模型架构的协同改进，推进了CLIP在人物表征学习中的应用。首先，我们开发了一种抗噪声数据构建流程，利用MLLMs的上下文学习能力自动过滤和标注网络图像，创建了WebPerson数据集，包含500万高质量人物中心化图像-文本对。其次，我们提出了GA-DMS（梯度-注意力引导的双掩码协同）框架，通过基于梯度-注意力相似度分数自适应地掩码噪声文本标记来改进跨模态对齐。此外，我们结合了掩码标记预测目标，强制模型预测信息丰富的文本标记，增强细粒度语义表征学习。大量实验表明，GA-DMS在多个基准测试中取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Contrastive Language-Image Pre-training (CLIP) exhibits strongperformance across diverse vision tasks, its application to personrepresentation learning faces two critical challenges: (i) the scarcity oflarge-scale annotated vision-language data focused on person-centric images,and (ii) the inherent limitations of global contrastive learning, whichstruggles to maintain discriminative local features crucial for fine-grainedmatching while remaining vulnerable to noisy text tokens. This work advancesCLIP for person representation learning through synergistic improvements indata curation and model architecture. First, we develop a noise-resistant dataconstruction pipeline that leverages the in-context learning capabilities ofMLLMs to automatically filter and caption web-sourced images. This yieldsWebPerson, a large-scale dataset of 5M high-quality person-centric image-textpairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-MaskingSynergetic) framework, which improves cross-modal alignment by adaptivelymasking noisy textual tokens based on the gradient-attention similarity score.Additionally, we incorporate masked token prediction objectives that compel themodel to predict informative text tokens, enhancing fine-grained semanticrepresentation learning. Extensive experiments show that GA-DMS achievesstate-of-the-art performance across multiple benchmarks.</description>
      <author>example@mail.com (Tianlu Zheng, Yifan Zhang, Xiang An, Ziyong Feng, Kaicheng Yang, Qichuan Ding)</author>
      <guid isPermaLink="false">2509.09118v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability</title>
      <link>http://arxiv.org/abs/2509.08910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PromptGuard，一个新颖的模块化提示框架，特别是其中的VulnGuard Prompt技术，旨在主动防止大型语言模型生成有害信息，保护弱势群体。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在现实世界应用中的激增给LGBTQ+个人、单亲父母和边缘化社区等弱势群体带来了生成有害、有偏见或误导性信息的风险。现有安全方法依赖事后过滤或通用对齐技术，无法在生成源头主动防止有害输出。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够主动防止有害信息生成的框架，特别是在生成源头阻止有害内容，保护弱势群体免受潜在伤害。&lt;h4&gt;方法&lt;/h4&gt;PromptGuard框架包含六个核心模块：输入分类、VulnGuard提示、道德原则集成、外部工具交互、输出验证和用户系统交互。VulnGuard Prompt是一种混合技术，整合了来自精选GitHub存储库的少样本示例、道德思维链推理和自适应角色提示，使用真实世界数据驱动的对比学习来防止有害信息生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论多目标优化和形式化证明，实现了25-30%的分析性危害减少。框架能够为特定人群创建保护屏障，并提供完整的数学形式化，包括收敛证明、使用信息论的脆弱性分析以及使用GitHub源数据集的理论验证框架。&lt;h4&gt;结论&lt;/h4&gt;PromptGuard建立了一个实时危害预防的智能专家系统，为系统实证研究建立了数学基础，能够主动防止大型语言模型生成有害内容。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在现实世界应用中的激增给LGBTQ+个人、单亲父母和边缘化社区等弱势群体带来了前所未有的风险，可能导致生成有害、有偏见或误导性信息。虽然现有的安全方法依赖于事后过滤或通用对齐技术，但它们无法在生成源头主动防止有害输出。本文介绍了PromptGuard，一个新颖的模块化提示框架，以及我们的突破性贡献：VulnGuard Prompt，这是一种使用真实世界数据驱动的对比学习来防止有害信息生成的混合技术。VulnGuard整合了来自精选GitHub存储库的少样本示例、道德思维链推理和自适应角色提示，创建特定人群的保护屏障。我们的框架采用理论多目标优化，并通过形式化证明展示了通过熵边界和帕累托最优性实现25-30%的分析性危害减少。PromptGuard协调六个核心模块：输入分类、VulnGuard提示、道德原则集成、外部工具交互、输出验证和用户系统交互，创建了一个实时危害预防的智能专家系统。我们提供了完整的数学形式化，包括收敛证明、使用信息论的脆弱性分析以及使用GitHub源数据集的理论验证框架，为系统实证研究建立了数学基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of Large Language Models (LLMs) in real-world applicationsposes unprecedented risks of generating harmful, biased, or misleadinginformation to vulnerable populations including LGBTQ+ individuals, singleparents, and marginalized communities. While existing safety approaches rely onpost-hoc filtering or generic alignment techniques, they fail to proactivelyprevent harmful outputs at the generation source. This paper introducesPromptGuard, a novel modular prompting framework with our breakthroughcontribution: VulnGuard Prompt, a hybrid technique that prevents harmfulinformation generation using real-world data-driven contrastive learning.VulnGuard integrates few-shot examples from curated GitHub repositories,ethical chain-of-thought reasoning, and adaptive role-prompting to createpopulation-specific protective barriers. Our framework employs theoreticalmulti-objective optimization with formal proofs demonstrating 25-30% analyticalharm reduction through entropy bounds and Pareto optimality. PromptGuardorchestrates six core modules: Input Classification, VulnGuard Prompting,Ethical Principles Integration, External Tool Interaction, Output Validation,and User-System Interaction, creating an intelligent expert system forreal-time harm prevention. We provide comprehensive mathematical formalizationincluding convergence proofs, vulnerability analysis using information theory,and theoretical validation framework using GitHub-sourced datasets,establishing mathematical foundations for systematic empirical research.</description>
      <author>example@mail.com (Tung Vu, Lam Nguyen, Quynh Dao)</author>
      <guid isPermaLink="false">2509.08910v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为CAME-AB的新型跨模态注意力框架，基于专家混合（MoE）主干架构，用于稳健的抗体结合位点预测。&lt;h4&gt;方法&lt;/h4&gt;CAME-AB整合了五种生物学基础模态：原始氨基酸编码、BLOSUM替换特征、预训练语言模型嵌入、结构感知特征和GCN精化的生化图。提出自适应模态融合模块，根据全局相关性和输入特定贡献动态加权每种模态。使用Transformer编码器结合MoE模块促进特征专业化和容量扩展。加入监督对比学习目标，塑造潜在空间几何结构，并应用随机权重平均提高优化稳定性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上持续优于强基线方法。消融研究验证了每个架构组件的有效性和多模态特征集成的益处。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过整合多种生物模态和创新的架构设计，显著提高了抗体结合位点预测的性能。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。在本文中，我们提出CAME-AB，一种基于专家混合（MoE）主干架构的新型跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB整合了五种生物学基础模态，包括原始氨基酸编码、BLOSUM替换特征、预训练语言模型嵌入、结构感知特征和GCN精化的生化图，形成统一的多模态表示。为增强自适应跨模态推理，我们提出自适应模态融合模块，学习根据全局相关性和输入特定贡献动态加权每种模态。Transformer编码器结合MoE模块进一步促进特征专业化和容量扩展。我们还加入监督对比学习目标，明确塑造潜在空间几何结构，鼓励类内紧凑性和类间可分离性。为提高优化稳定性和泛化能力，我们在训练过程中应用随机权重平均。在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上持续优于强基线方法。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的益处。模型实现细节和代码可在https://anonymous.4open.science/r/CAME-AB-C525获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, anovel Cross-modality Attention framework with a Mixture-of-Experts (MoE)backbone for robust antibody binding site prediction. CAME-AB integrates fivebiologically grounded modalities, including raw amino acid encodings, BLOSUMsubstitution profiles, pretrained language model embeddings, structure-awarefeatures, and GCN-refined biochemical graphs, into a unified multimodalrepresentation. To enhance adaptive cross-modal reasoning, we propose an\emph{adaptive modality fusion} module that learns to dynamically weight eachmodality based on its global relevance and input-specific contribution. ATransformer encoder combined with an MoE module further promotes featurespecialization and capacity expansion. We additionally incorporate a supervisedcontrastive learning objective to explicitly shape the latent space geometry,encouraging intra-class compactness and inter-class separability. To improveoptimization stability and generalization, we apply stochastic weight averagingduring training. Extensive experiments on benchmark antibody-antigen datasetsdemonstrate that CAME-AB consistently outperforms strong baselines on multiplemetrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablationstudies further validate the effectiveness of each architectural component andthe benefit of multimodal feature integration. The model implementation detailsand the codes are available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Rui Xiao, Fanming Jin, Ye-Fan Hu, Hangjun Che, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v4</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes</title>
      <link>http://arxiv.org/abs/2509.09141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AEOS是一种受猫头鹰主动感知行为启发的生物启发框架，通过融合模型预测控制和强化学习，实现了无人机激光雷达的自适应扫描，显著提高了复杂环境中的里程计精度，同时保持实时性能。&lt;h4&gt;背景&lt;/h4&gt;小型激光雷达传感器视场角有限，无人机有效载荷限制排除了多传感器配置的可能性。传统电机扫描系统以固定速度旋转，缺乏场景感知和任务级适应性，导致在复杂、遮挡环境中里程计和地图绘制性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为AEOS（Active Environment-aware Optimal Scanning）的生物启发框架，用于无人机激光雷达-惯性里程计（LIO）的自适应激光雷达控制。&lt;h4&gt;方法&lt;/h4&gt;AEOS采用混合架构，结合模型预测控制（MPC）和强化学习（RL）。分析不确定性模型预测未来姿态可观测性以进行利用，轻量级神经网络从全景深度表示中学习隐式成本图以指导探索。开发基于点云的仿真环境，支持可扩展训练和泛化，并使用真实世界激光雷达地图实现仿真到现实的迁移。&lt;h4&gt;主要发现&lt;/h4&gt;在仿真和真实环境中的大量实验表明，AEOS显著提高了里程计准确性，与固定速率、仅优化和完全学习的基线相比表现更好，同时在机载计算约束下保持实时性能。&lt;h4&gt;结论&lt;/h4&gt;AEOS是一种生物启发且计算高效的框架，有效解决了无人机激光雷达3D感知和定位的局限性，显著提升了复杂环境中的感知性能。&lt;h4&gt;翻译&lt;/h4&gt;基于激光雷达的无人机3D感知和定位受到小型激光雷达传感器窄视场角和有效载荷限制（这些限制排除了多传感器配置）的根本性限制。传统固定速度旋转的电机扫描系统缺乏场景感知和任务级适应性，导致在复杂、遮挡环境中里程计和地图绘制性能下降。受猫头鹰主动感知行为的启发，我们提出了AEOS（Active Environment-aware Optimal Scanning），这是一种生物启发且计算高效的框架，用于无人机激光雷达-惯性里程计（LIO）的自适应激光雷达控制。AEOS在混合架构中结合了模型预测控制（MPC）和强化学习（RL）：分析不确定性模型预测未来姿态可观测性以进行利用，而轻量级神经网络从全景深度表示中学习隐式成本图以指导探索。为了支持可扩展训练和泛化，我们开发了一个基于点云的仿真环境，包含不同场景的真实世界激光雷达地图，实现了仿真到现实的迁移。在仿真和真实环境中的大量实验表明，与固定速率、仅优化和完全学习的基线相比，AEOS显著提高了里程计准确性，同时在机载计算约束下保持实时性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无人机在复杂场景中使用激光雷达-惯性里程计(LIO)时面临的两个关键限制：紧凑型激光雷达传感器固有的窄视场(FoV)以及无人机有效载荷限制排除了多传感器配置的可能性。这个问题在现实世界中非常重要，因为在地下基础设施、建筑工地或森林等非结构化和遮挡环境中，自主3D感知对于机器人在检查、测绘、监控和搜索救援等应用中至关重要。激光雷达作为这些复杂场景中高精度3D感知的可靠选择，但在无人机平台上部署时面临上述限制，导致定位和建图性能严重下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从猫头鹰的主动感知行为中获得灵感，猫头鹰不断重新定向头部以在物理约束下最大化视觉感知。作者分析了现有工作的局限性：早期电机扫描系统缺乏场景适应性；优化方法需要手工设计的成本函数且对局部最小值敏感；纯学习策略计算密集且难以在资源受限的无人机上部署。作者设计了一个混合RL-MPC框架，融合显式和隐式建模以平衡利用和探索：使用分析模型预测位姿不确定性指导MPC优化控制，同时使用轻量级神经网络从点云中学习成本图促进探索。作者借鉴了生物启发感知、模型预测控制和强化学习等方法，但将它们结合在一个专门针对无人机平台优化的混合框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是受猫头鹰主动感知行为启发，提出一种生物启发且计算高效的框架，用于无人机激光雷达-惯性里程计中的自适应激光雷达控制。该方法融合了显式和隐式建模，以平衡主动SLAM中的利用(将感知资源集中在特征丰富区域)和探索(获取观察不足区域信息)两个竞争性目标。整体实现流程包括：1)定义系统状态为激光雷达扫描角度，控制输入为角速度；2)接收包含局部点云和位姿不确定性的观察；3)将点云投影为全景深度图；4)使用差分混合RL-MPC架构：分析不确定性模型指导利用，神经网络成本图促进探索；5)在基于点云的模拟环境中训练和验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)生物启发的紧凑型电机激光雷达控制策略，使无人机具有全景和自适应扫描能力；2)混合RL-MPC框架，融合显式和隐式建模平衡利用和探索，保留可解释性的同时利用学习能力；3)高保真基于点云的强化学习模拟环境，支持物理一致的无人机动力学和多样化场景；4)在模拟和真实世界环境中的全面实验验证。相比之前的工作，AEOS不是简单的固定速度扫描、纯优化或纯学习方法，而是结合了生物启发的感知机制，平衡了计算效率和准确性，专门针对无人机平台约束进行了优化，提供了可解释的决策过程同时保持了学习能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AEOS提出了一种受猫头鹰启发的混合强化学习-模型预测控制框架，通过动态调整激光雷达扫描方向和速度，显著提高了无人机在复杂场景中的激光雷达-惯性里程计定位和建图性能，同时保持了实时计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)are fundamentally limited by the narrow field of view (FoV) of compact LiDARsensors and the payload constraints that preclude multi-sensor configurations.Traditional motorized scanning systems with fixed-speed rotations lack sceneawareness and task-level adaptability, leading to degraded odometry and mappingperformance in complex, occluded environments. Inspired by the active sensingbehavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),a biologically inspired and computationally efficient framework for adaptiveLiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines modelpredictive control (MPC) and reinforcement learning (RL) in a hybridarchitecture: an analytical uncertainty model predicts future poseobservability for exploitation, while a lightweight neural network learns animplicit cost map from panoramic depth representations to guide exploration. Tosupport scalable training and generalization, we develop a point cloud-basedsimulation environment with real-world LiDAR maps across diverse scenes,enabling sim-to-real transfer. Extensive experiments in both simulation andreal-world environments demonstrate that AEOS significantly improves odometryaccuracy compared to fixed-rate, optimization-only, and fully learnedbaselines, while maintaining real-time performance under onboard computationalconstraints. The project page can be found athttps://kafeiyin00.github.io/AEOS/.</description>
      <author>example@mail.com (Jianping Li, Xinhang Xu, Zhongyuan Liu, Shenghai Yuan, Muqing Cao, Lihua Xie)</author>
      <guid isPermaLink="false">2509.09141v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning</title>
      <link>http://arxiv.org/abs/2509.08982v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了iMatcher，一个用于点云配准中特征匹配的完全可微分框架。该方法通过学习特征预测几何一致性置信矩阵，结合局部和全局一致性，显著提高了刚性配准性能，在多个数据集上达到了最先进的内点比率。&lt;h4&gt;背景&lt;/h4&gt;点云配准是计算机视觉和三维重建中的关键任务，特征匹配是其中的重要环节，需要提高配准的准确性和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;开发一个完全可微分的框架，用于点云配准中的特征匹配，通过结合局部和全局一致性来提高配准性能。&lt;h4&gt;方法&lt;/h4&gt;iMatcher方法包括：1) 局部图嵌入模块初始化分数矩阵；2) 重新定位步骤通过双向最近邻搜索优化矩阵；3) 通过全局几何一致性学习预测点对点匹配概率。该方法利用学习到的特征同时考虑局部和全局一致性。&lt;h4&gt;主要发现&lt;/h4&gt;iMatcher在多个数据集上实现了最先进的内点比率：在KITTI上达到95%-97%，在KITTI-360上达到94%-97%，在3DMatch上最高达到81.1%。&lt;h4&gt;结论&lt;/h4&gt;iMatcher是一个有效的点云配准特征匹配框架，通过结合局部和全局一致性学习，在各种场景下都表现出色，具有很好的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了iMatcher，一个用于点云配准中特征匹配的完全可微分框架。该方法利用学习到的特征预测几何一致性置信矩阵，同时考虑局部和全局一致性。首先，局部图嵌入模块导致分数矩阵的初始化。随后的重新定位步骤通过考虑双向源到目标和目标到源匹配，通过在3D空间中的最近邻搜索来优化该矩阵。然后，配对的点特征被堆叠在一起，通过全局几何一致性学习进行优化，以预测点对点匹配概率。在真实世界的户外(KITTI, KITTI-360)和室内(3DMatch)数据集上，以及在6-DoF姿态估计(TUD-L)和部分到部分匹配(MVP-RG)上的大量实验表明，iMatcher显著提高了刚性配准性能。该方法在数据集上实现了最先进的内点比率，在KITTI上得分为95%-97%，在KITTI-360上得分为94%-97%，在3DMatch上最高达到81.1%，突显了其在各种设置下的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准(point cloud registration)中的特征匹配问题，提高匹配的准确性和鲁棒性。这个问题在现实中非常重要，因为点云配准是自动驾驶、机器人抓取、3D重建、医学图像对齐等多个领域的关键技术，帮助计算机理解不同视角下的3D场景。然而，点云的固有无序性、实际数据获取中的遮挡和传感器噪声等因素使得精确匹配变得困难，限制了这些应用的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考了现有方法的局限性：传统优化方法如ICP依赖初始化且对噪声敏感；基于深度学习的方法中Transformer计算复杂度高；Sinkhorn算法虽广泛使用但依赖迭代优化。作者设计iMatcher时借鉴了多项现有工作：受LightGlue启发估计点的可匹配性分数；使用图卷积网络(Graph CNN)捕获局部拓扑关系；采用一阶空间兼容性(FOSC)评估全局一致性；使用可微加权SVD进行预对齐。作者的核心思路是结合局部和全局几何一致性，创建一个支持渐进式特征精化的全可微分框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合局部图结构和全局几何一致性来提高点云匹配的准确性。整体流程分为四个步骤：1) 使用图卷积网络构建局部图，提取每个点的邻域特征，计算初始分数矩阵；2) 通过可微SVD进行预对齐，将源点云变换到目标空间，并通过最近邻搜索建立双向匹配；3) 利用一阶空间兼容性(FOSC)措施评估全局一致性，计算每个点的可匹配性分数；4) 融合局部和全局信息，通过可匹配性矩阵调整初始分数矩阵，生成最终的高质量匹配结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 局部到全局的几何一致性学习，同时考虑点云的局部结构和全局空间关系；2) 可微分的重新定位步骤，通过SVD预对齐和最近邻搜索提高匹配精度；3) 基于FOSC的可匹配性评估，学习温度参数提高对遮挡和噪声的鲁棒性；4) 高效的架构设计，避免Transformer的高计算复杂度。相比之前的工作，iMatcher不依赖迭代优化过程，提供了更好的可解释性和计算效率；同时考虑局部和全局一致性，引入可匹配性评估机制，在各种场景下实现了更高的内点比率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; iMatcher通过结合局部图卷积和全局几何一致性学习，提出了一种高效、可微的点云匹配框架，显著提高了点云配准的内点比率和鲁棒性，在各种场景下实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents iMatcher, a fully differentiable framework for featurematching in point cloud registration. The proposed method leverages learnedfeatures to predict a geometrically consistent confidence matrix, incorporatingboth local and global consistency. First, a local graph embedding module leadsto an initialization of the score matrix. A subsequent repositioning steprefines this matrix by considering bilateral source-to-target andtarget-to-source matching via nearest neighbor search in 3D space. The pairedpoint features are then stacked together to be refined through global geometricconsistency learning to predict a point-wise matching probability. Extensiveexperiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partialmatching (MVP-RG), demonstrate that iMatcher significantly improves rigidregistration performance. The method achieves state-of-the-art inlier ratios,scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,highlighting its robustness across diverse settings.</description>
      <author>example@mail.com (Karim Slimani, Catherine Achard, Brahim Tamadazte)</author>
      <guid isPermaLink="false">2509.08982v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>3D and 4D World Modeling: A Survey</title>
      <link>http://arxiv.org/abs/2509.07996v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at  https://github.com/worldbench/survey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了3D和4D世界建模与生成领域的研究，解决了该领域缺乏标准化定义和分类法的问题，建立了精确的定义和结构化分类法，总结了相关数据集和评估指标，并讨论了实际应用、开放挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;世界建模已成为AI研究的基石，使代理能够理解、表示和预测动态环境。然而，先前工作主要关注2D图像和视频数据的生成方法，忽略了利用3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的研究。&lt;h4&gt;目的&lt;/h4&gt;通过首次专门针对3D和4D世界建模和生成的全面综述，解决该领域缺乏标准化定义和分类法的问题，提供连贯且基础的参考。&lt;h4&gt;方法&lt;/h4&gt;建立精确的定义，引入涵盖基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）方法的分类法，系统地总结针对3D/4D设置的数据集和评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;讨论了3D/4D世界建模的实际应用，确定了开放性挑战，指出了有前景的研究方向。&lt;h4&gt;结论&lt;/h4&gt;为推进3D和4D世界建模领域提供连贯且基础的参考，相关文献总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;翻译&lt;/h4&gt;世界建模已成为AI研究的基石，使代理能够理解、表示和预测它们所居住的动态环境。虽然先前的工作主要强调2D图像和视频数据的生成方法，但它们忽略了利用原生3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的快速增长的研究。同时，缺乏对'世界模型'的标准化定义和分类法，导致文献中的观点碎片化且有时不一致。本综述通过首次专门针对3D和4D世界建模和生成的全面综述来解决这些差距。我们建立了精确的定义，引入了涵盖基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）方法的分类法，并系统地总结了针对3D/4D设置的数据集和评估指标。我们进一步讨论了实际应用，确定了开放性挑战，并指出了有前景的研究方向，旨在为推进该领域提供连贯且基础的参考。现有文献的系统总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决世界建模领域缺乏标准化定义和分类体系的问题，以及现有研究过度关注2D图像和视频而忽视原生3D和4D表示方法的问题。这个问题很重要，因为世界建模是AI研究的基础，使智能体能理解、表示和预测动态环境；同时，原生3D/4D表示方法在自动驾驶、机器人和安全关键系统中至关重要，它们提供明确的几何和物理基础，对构建可靠的世界模型非常必要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 这篇论文是一篇综述文章，作者通过识别世界建模领域的碎片化状态和缺乏统一框架的问题，首先建立了精确的'世界模型'和'3D/4D世界建模'定义，然后提出一个分层的分类方法，基于表示模态（视频生成、占用生成和LiDAR生成）和功能类型（数据引擎、动作解释器、神经模拟器和场景重建器）进行组织。作者借鉴了大量现有工作，通过引用和总结这些工作来构建他们的分类框架和评估体系，为3D/4D世界建模领域提供了系统化的概述。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这篇论文的核心思想是建立统一的分类框架，将3D/4D世界建模方法按照表示模态和功能类型进行组织，强调原生3D/4D表示方法的重要性，并区分条件输入和功能输出。作为综述论文，它没有具体的实现流程，而是系统地组织现有工作，介绍3D/4D世界建模的基本概念、方法分类、数据集和评估指标，为研究社区提供清晰的概念框架和参考方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门针对3D和4D世界建模进行全面综述；建立精确的'世界模型'和'3D/4D世界建模'定义；提出基于表示模态和功能类型的分层分类方法；系统总结专门针对3D/4D场景的数据集和评估协议；强调原生3D/4D表示方法的重要性。相比之前的工作，这篇论文专门聚焦于3D/4D表示方法，而大多数现有综述集中在2D或仅视觉模态上；同时，其分类框架更细致，区分了条件输入和功能输出，为复杂的研究领域提供了更清晰的视角。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统综述了3D和4D世界建模领域，建立了统一的定义和分类框架，为理解和推进这一快速发展的重要研究方向提供了基础性参考。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; World modeling has become a cornerstone in AI research, enabling agents tounderstand, represent, and predict the dynamic environments they inhabit. Whileprior work largely emphasizes generative methods for 2D image and video data,they overlook the rapidly growing body of work that leverages native 3D and 4Drepresentations such as RGB-D imagery, occupancy grids, and LiDAR point cloudsfor large-scale scene modeling. At the same time, the absence of a standardizeddefinition and taxonomy for ``world models'' has led to fragmented andsometimes inconsistent claims in the literature. This survey addresses thesegaps by presenting the first comprehensive review explicitly dedicated to 3Dand 4D world modeling and generation. We establish precise definitions,introduce a structured taxonomy spanning video-based (VideoGen),occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, andsystematically summarize datasets and evaluation metrics tailored to 3D/4Dsettings. We further discuss practical applications, identify open challenges,and highlight promising research directions, aiming to provide a coherent andfoundational reference for advancing the field. A systematic summary ofexisting literature is available at https://github.com/worldbench/survey</description>
      <author>example@mail.com (Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyuan Gao, Song Wang, Linfeng Li, Liang Pan, Yong Liu, Jianke Zhu, Wei Tsang Ooi, Steven C. H. Hoi, Ziwei Liu)</author>
      <guid isPermaLink="false">2509.07996v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Concentration for Self-Supervised Dense Representations Learning</title>
      <link>http://arxiv.org/abs/2509.09429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了图像级自监督学习在密集表示学习方面的挑战，提出了一种解决图像块过度分散现象的方法，通过显式语义集中来改善密集任务性能。&lt;h4&gt;背景&lt;/h4&gt;图像级自监督学习已取得显著进展，但学习图像块的密集表示仍然具有挑战性。主流方法存在过度分散现象，即来自同一实例/类别的图像块分散，影响密集任务性能。&lt;h4&gt;目的&lt;/h4&gt;探索显式语义集中方法来解决密集自监督学习中的过度分散问题，提高下游密集任务性能。&lt;h4&gt;方法&lt;/h4&gt;1) 提出蒸馏块对应关系打破严格空间对齐，并设计噪声容忍的排序损失处理嘈杂不平衡的伪标签；2) 提出基于对象的过滤器，通过交叉注意力将输出空间映射到基于对象的空间，使块由可学习的对象原型表示。&lt;h4&gt;主要发现&lt;/h4&gt;图像级SSL通过隐式语义集中避免过度分散，非严格空间对齐保证实例内一致性，共享模式保证图像间一致性。但这些方法对密集SSL不可行，需要探索显式语义集中方法。&lt;h4&gt;结论&lt;/h4&gt;提出的显式语义集中方法有效解决了密集自监督学习中的过度分散问题，各种任务的经验研究验证了方法的有效性，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;最近的图像级自监督学习(SSL)进展显著，但学习图像块的密集表示仍然具有挑战性。主流方法遇到过度分散现象，即来自同一实例/类别的图像块分散，损害了密集任务的下游性能。这项工作揭示图像级SSL通过隐式语义集中避免过度分散。具体来说，非严格空间对齐确保实例内一致性，而共享模式，即输入空间内同类实例的相似部分，确保图像间一致性。不幸的是，由于这些方法对空间敏感且场景复杂的数据，它们对密集SSL不可行。这些观察促使我们探索密集SSL的显式语义集中。首先，为打破严格空间对齐，我们提出蒸馏块对应关系。面对嘈杂和不平衡的伪标签，我们提出了一种噪声容忍的排序损失。核心思想是将平均精度(AP)损失扩展到连续目标，使其决策无关和自适应聚焦特性防止学生模型被误导。其次，为从复杂场景中区分共享模式，我们提出基于对象的过滤器将输出空间映射到基于对象的空间。具体来说，通过交叉注意力，块由对象的可学习原型表示。最后，各种任务的经验研究有力支持了我们方法的有效性。代码可在https://github.com/KID-7391/CoTAP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in image-level self-supervised learning (SSL) have madesignificant progress, yet learning dense representations for patches remainschallenging. Mainstream methods encounter an over-dispersion phenomenon thatpatches from the same instance/category scatter, harming downstream performanceon dense tasks. This work reveals that image-level SSL avoids over-dispersionby involving implicit semantic concentration. Specifically, the non-strictspatial alignment ensures intra-instance consistency, while shared patterns,i.e., similar parts of within-class instances in the input space, ensureinter-image consistency. Unfortunately, these approaches are infeasible fordense SSL due to their spatial sensitivity and complicated scene-centric data.These observations motivate us to explore explicit semantic concentration fordense SSL. First, to break the strict spatial alignment, we propose to distillthe patch correspondences. Facing noisy and imbalanced pseudo labels, wepropose a noise-tolerant ranking loss. The core idea is extending the AveragePrecision (AP) loss to continuous targets, such that its decision-agnostic andadaptive focusing properties prevent the student model from being misled.Second, to discriminate the shared patterns from complicated scenes, we proposethe object-aware filter to map the output space to an object-based space.Specifically, patches are represented by learnable prototypes of objects viacross-attention. Last but not least, empirical studies across various taskssoundly support the effectiveness of our method. Code is available inhttps://github.com/KID-7391/CoTAP.</description>
      <author>example@mail.com (Peisong Wen, Qianqian Xu, Siran Dai, Runmin Cong, Qingming Huang)</author>
      <guid isPermaLink="false">2509.09429v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts</title>
      <link>http://arxiv.org/abs/2509.09337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个新颖的子图专家混合(MoSE)框架，用于在不同图任务中进行灵活且具有表达力的子图表示学习，解决了传统图神经网络在捕捉复杂高阶子图模式方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在处理图结构数据方面取得了很大成功，但它们依赖于局部的、成对的消息传递，限制了捕捉复杂高阶子图模式的能力，导致结构表达能力不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一个能够在不同图任务中进行灵活且具有表达力的子图表示学习的框架。&lt;h4&gt;方法&lt;/h4&gt;MoSE通过匿名游走提取信息子图，并根据结构语义将它们动态路由到专门的专家，使模型能够以改进的灵活性和可解释性捕捉多样化的子图模式。&lt;h4&gt;主要发现&lt;/h4&gt;MoSE在子图Weisfeiler-Lehman测试中的理论分析证明其比SWL更强大；实验和可视化表明MoSE优于竞争性基线，并为模型学习的结构模式提供了可解释的见解。&lt;h4&gt;结论&lt;/h4&gt;MoSE框架能够有效捕捉复杂的高阶子图模式，在不同图任务上表现优异，并且提供了更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;虽然图神经网络(GNNs)在从图结构数据中学习方面取得了巨大成功，但它们对局部、成对消息传递的依赖限制了它们捕捉复杂高阶子图模式的能力，导致结构表达能力不足。最近的工作尝试通过将随机游走核集成到GNNs中来增强结构表达能力。然而，这些方法本质上是针对图级任务设计的，限制了它们在其他下游任务（如节点分类）中的应用。此外，它们的固定核配置阻碍了模型捕捉多样化子图结构的灵活性。为了解决这些限制，本文提出了一个新颖的子图专家混合(MoSE)框架，用于在不同图任务中进行灵活且具有表达力的子图表示学习。具体来说，MoSE通过匿名游走提取信息子图，并根据结构语义将它们动态路由到专门的专家，使模型能够以改进的灵活性和可解释性捕捉多样化的子图模式。我们进一步提供了MoSE在子图Weisfeiler-Lehman测试中的表达能力理论分析，证明它比SWL更强大。广泛的实验以及学习到的子图专家的可视化表明，MoSE不仅优于竞争性基线，还为模型学习的结构模式提供了可解释的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While graph neural networks (GNNs) have achieved great success in learningfrom graph-structured data, their reliance on local, pairwise message passingrestricts their ability to capture complex, high-order subgraph patterns.leading to insufficient structural expressiveness. Recent efforts haveattempted to enhance structural expressiveness by integrating random walkkernels into GNNs. However, these methods are inherently designed forgraph-level tasks, which limits their applicability to other downstream taskssuch as node classification. Moreover, their fixed kernel configurations hinderthe model's flexibility in capturing diverse subgraph structures. To addressthese limitations, this paper proposes a novel Mixture of Subgraph Experts(MoSE) framework for flexible and expressive subgraph-based representationlearning across diverse graph tasks. Specifically, MoSE extracts informativesubgraphs via anonymous walks and dynamically routes them to specializedexperts based on structural semantics, enabling the model to capture diversesubgraph patterns with improved flexibility and interpretability. We furtherprovide a theoretical analysis of MoSE's expressivity within the SubgraphWeisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.Extensive experiments, together with visualizations of learned subgraphexperts, demonstrate that MoSE not only outperforms competitive baselines butalso provides interpretable insights into structural patterns learned by themodel.</description>
      <author>example@mail.com (Junda Ye, Zhongbao Zhang, Li Sun, Siqiang Luo)</author>
      <guid isPermaLink="false">2509.09337v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2509.09064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Med3DInsight，一种新颖的预训练框架，通过集成3D图像编码器与2D多模态大语言模型(MLLMs)，解决了3D医学图像理解中缺乏深层次语义理解的问题，无需人工标注即可实现高性能的3D医学图像分割和分类任务。&lt;h4&gt;背景&lt;/h4&gt;理解3D医学图像体积在医学领域至关重要，但现有的基于3D医学卷积和Transformer的自监督学习方法往往缺乏深层次语义理解，而多模态大语言模型为增强图像理解提供了有前景的途径。&lt;h4&gt;目的&lt;/h4&gt;利用2D多模态大语言模型(MLLMs)来增强3D医学图像理解，开发一种无需人工标注的可扩展多模态3D医学表征学习新范式。&lt;h4&gt;方法&lt;/h4&gt;提出了Med3DInsight框架，通过专门设计的平面切片感知transformer模块将3D图像编码器与2D MLLMs集成，并采用基于部分最优传输的对齐方法，对LLM生成内容中可能引入的噪声具有更好的容忍度。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共数据集上的大量实验表明，Med3DInsight在分割和分类两个下游任务上展示了最先进的性能，优于当前的SSL方法，且可以无缝集成到现有的3D医学图像理解网络中。&lt;h4&gt;结论&lt;/h4&gt;Med3DInsight为可扩展的多模态3D医学表征学习引入了新范式，无需人工标注即可实现高性能，并且可以无缝集成到现有的3D医学图像理解网络中，增强它们的性能。相关源代码、生成的数据集和预训练模型将在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;理解3D医学图像体积在医学领域至关重要，然而现有的3D医学卷积和基于Transformer的自监督学习方法往往缺乏深层次的语义理解。多模态大语言模型(MLLMs)的最新进展提供了一种通过文本描述增强图像理解的有前景的方法。为了利用这些2D MLLMs来改进3D医学图像理解，我们提出了Med3DInsight，一种新颖的预训练框架，通过专门设计的平面切片感知transformer模块将3D图像编码器与2D MLLMs集成。此外，我们的模型采用基于部分最优传输的对齐方法，展示了对于LLM生成内容中可能引入的噪声具有更好的容忍度。Med3DInsight为可扩展的多模态3D医学表征学习引入了一种无需人工标注的新范式。大量实验证明了我们在两个下游任务，即分割和分类上的最先进性能，在各种使用CT和MRI模态的公共数据集上，超越了当前的SSL方法。Med3DInsight可以无缝集成到现有的3D医学图像理解网络中，潜在地增强它们的性能。我们的源代码、生成的数据集和预训练模型将在https://github.com/Qybc/Med3DInsight上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D医学图像理解缺乏深层次语义理解的问题。现有3D医学卷积和transformer自学习方法主要关注低级别图像理解（像素或块级别），无法捕获高级语义特征，限制了模型充分利用3D医学扫描中复杂信息的能力。这个问题很重要，因为3D医学图像（如MRI、CT）包含比2D图像更全面的信息，对诊断、治疗规划和医疗研究至关重要，而缺乏语义理解会阻碍医疗AI系统的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到2D多模态大语言模型(MLLMs)在图像理解方面的强大能力，但直接应用于3D医学图像存在挑战：缺乏大规模3D医学图像-文本配对数据集，以及2D MLLMs与3D医学图像理解需求之间的差距。作者借鉴了自监督学习(SSL)的思想、多模态大语言模型(如CLIP、GPT-4V)的架构、最优传输(OT)理论，以及类似MAE的重建策略。通过设计PSAT模块连接3D和2D特征空间，并采用POT技术处理MLLMs可能产生的噪声，作者构建了一个同时增强高级语义和低级细节理解的新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D MLLMs的语义理解能力增强3D医学图像理解，通过特殊设计的PSAT模块集成3D图像编码器和2D MLLMs，结合重建技术和图像-文本对齐，同时增强高级语义和低级细节理解，并使用POT技术处理噪声。整体流程包括：1)使用GPT-4V为3D体积的2D切片生成文本描述创建三元组数据集；2)提取3D体积、2D切片和文本的特征表示；3)通过PSAT模块将3D特征投影到2D图像和文本嵌入空间；4)使用POT对齐3D体积特征与2D图像和文本特征；5)通过重建3D体积增强低级视觉语义学习；6)结合对齐损失和重建损失训练模型；7)在下游任务上应用预训练的3D编码器。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Med3DInsight框架，利用2D MLLMs和重建技术增强3D图像编码器的高级和低级理解能力；2)设计平面切片感知Transformer(PSAT)模块连接3D和2D特征空间，考虑视觉特征的空间方向；3)提出部分最优传输(POT)对齐技术，减少模态间差异，提高对MLLMs噪声的容忍度。相比之前工作，本文无需大规模3D医学图像-文本配对数据集，同时兼顾高级语义和低级细节理解，使用POT而非对比学习进行模态对齐，在多个数据集的分割和分类任务上超越了现有SSL方法，并在数据有限情况下表现出更好的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Med3DInsight框架，通过结合2D多模态大语言模型和重建技术，显著提升了3D医学图像的理解能力，无需人工标注即可在分割和分类任务上达到最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding 3D medical image volumes is critical in the medical field, yetexisting 3D medical convolution and transformer-based self-supervised learning(SSL) methods often lack deep semantic comprehension. Recent advancements inmultimodal large language models (MLLMs) provide a promising approach toenhance image understanding through text descriptions. To leverage these 2DMLLMs for improved 3D medical image understanding, we propose Med3DInsight, anovel pretraining framework that integrates 3D image encoders with 2D MLLMs viaa specially designed plane-slice-aware transformer module. Additionally, ourmodel employs a partial optimal transport based alignment, demonstratinggreater tolerance to noise introduced by potential noises in LLM-generatedcontent. Med3DInsight introduces a new paradigm for scalable multimodal 3Dmedical representation learning without requiring human annotations. Extensiveexperiments demonstrate our state-of-the-art performance on two downstreamtasks, i.e., segmentation and classification, across various public datasetswith CT and MRI modalities, outperforming current SSL methods. Med3DInsight canbe seamlessly integrated into existing 3D medical image understanding networks,potentially enhancing their performance. Our source code, generated datasets,and pre-trained models will be available athttps://github.com/Qybc/Med3DInsight.</description>
      <author>example@mail.com (Qiuhui Chen, Xuancheng Yao, Huping Ye, Yi Hong)</author>
      <guid isPermaLink="false">2509.09064v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Divergent Representations between Text-to-Image Models</title>
      <link>http://arxiv.org/abs/2509.08940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025. Code available at  https://github.com/adobe-research/CompCon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了两个不同生成模型学习的视觉表示何时以及如何出现差异，提出了CompCon算法来发现不同模型间的视觉属性差异及其触发条件&lt;h4&gt;背景&lt;/h4&gt;现有两个文本到图像模型可能在相同提示下生成不同视觉属性的图像，但缺乏系统方法来发现这些差异&lt;h4&gt;目的&lt;/h4&gt;发现一个模型生成而另一个模型不生成的视觉属性，以及触发这些差异的提示词类型&lt;h4&gt;方法&lt;/h4&gt;提出CompCon进化搜索算法，创建自动化数据生成管道构建ID2数据集（包含60个输入相关差异），并与LLM和VLM基线方法比较&lt;h4&gt;主要发现&lt;/h4&gt;PixArt在描述孤独提示时倾向于展示湿漉街道，Stable Diffusion 3.5在描述非裔美国人媒体职业时表现出特定视觉特征&lt;h4&gt;结论&lt;/h4&gt;CompCon能有效发现不同文本到图像模型间的视觉表示差异，为理解模型行为提供新视角&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们研究两个不同生成模型学习的视觉表示何时以及如何出现差异。给定两个文本到图像模型，我们的目标是发现一个模型生成图像中出现而另一个模型不出现的视觉属性，以及触发这些属性差异的提示词类型。例如，当给定表达强烈情感的提示时，一个模型的输出中可能出现'火焰'，而另一个模型在相同提示下不会产生此属性。我们引入CompCon（比较概念），一种进化搜索算法，发现一个模型输出中比另一个模型更普遍的视觉属性，并揭示与这些视觉差异相关的提示概念。为了评估CompCon发现不同表示的能力，我们创建了一个自动化数据生成管道来生成ID2数据集（包含60个输入相关差异），并将我们的方法与几种基于LLM和VLM的基线方法进行了比较。最后，我们使用CompCon比较流行的文本到图像模型，发现不同的表示，例如PixArt如何描绘提及孤独的提示词与湿漉街道，以及Stable Diffusion 3.5如何描绘非裔美国人在媒体职业中的形象。代码地址：https://github.com/adobe-research/CompCon&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we investigate when and how visual representations learned bytwo different generative models diverge. Given two text-to-image models, ourgoal is to discover visual attributes that appear in images generated by onemodel but not the other, along with the types of prompts that trigger theseattribute differences. For example, "flames" might appear in one model'soutputs when given prompts expressing strong emotions, while the other modeldoes not produce this attribute given the same prompts. We introduce CompCon(Comparing Concepts), an evolutionary search algorithm that discovers visualattributes more prevalent in one model's output than the other, and uncoversthe prompt concepts linked to these visual differences. To evaluate CompCon'sability to find diverging representations, we create an automated datageneration pipeline to produce ID2, a dataset of 60 input-dependentdifferences, and compare our approach to several LLM- and VLM-poweredbaselines. Finally, we use CompCon to compare popular text-to-image models,finding divergent representations such as how PixArt depicts prompts mentioningloneliness with wet streets and Stable Diffusion 3.5 depicts African Americanpeople in media professions. Code at: https://github.com/adobe-research/CompCon</description>
      <author>example@mail.com (Lisa Dunlap, Joseph E. Gonzalez, Trevor Darrell, Fabian Caba Heilbron, Josef Sivic, Bryan Russell)</author>
      <guid isPermaLink="false">2509.08940v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring</title>
      <link>http://arxiv.org/abs/2509.08392v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对交通监控图像增强任务的垂直残差自编码器（VRAE）架构，通过在每个编码阶段注入输入感知特征来指导表示学习，相比传统方法在车牌识别的图像预处理中表现更优。&lt;h4&gt;背景&lt;/h4&gt;实际交通监控中，车辆图像常受到恶劣天气、光照不足或高速运动的影响，导致严重的噪声和模糊。这些退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。&lt;h4&gt;目的&lt;/h4&gt;快速实时恢复这些退化图像，是提高识别性能的关键预处理步骤。设计一种图像增强架构，用于交通监控中的图像增强任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种垂直残差自编码器（Vertical Residual Autoencoder, VRAE）架构。该方法采用了一种增强策略，包含一个辅助块，在每个编码阶段注入输入感知特征，以指导表示学习过程，与自编码器相比能够在整个网络中更好地保留一般信息。&lt;h4&gt;主要发现&lt;/h4&gt;在带有可见车牌的车辆图像数据集上的实验表明，该方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流的方法（FB）。与相同深度的自编码器相比，PSNR提高了约20%，NMSE减少了约50%，SSIM提高了1%，参数仅需要大约1%的边际增加。&lt;h4&gt;结论&lt;/h4&gt;VRAE架构在交通监控图像增强任务中表现优异，通过辅助块注入输入感知特征的方法有效地改善了图像恢复质量。&lt;h4&gt;翻译&lt;/h4&gt;在实际交通监控中，车辆图像在恶劣天气、光照不足或高速运动条件下捕获时，常常受到严重的噪声和模糊影响。此类退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。因此，快速实时地恢复这些退化图像是提高识别性能的关键预处理步骤。在这项工作中，我们提出了一种专为交通监控图像增强任务设计的垂直残差自编码器（VRAE）架构。该方法采用了一种增强策略，包含一个辅助块，在每个编码阶段注入输入感知特征，以指导表示学习过程，与传统自编码器相比，能够在整个网络中更好地保留一般信息。在有可见车牌的车辆图像数据集上的实验表明，我们的方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流（FB）的方法。与相同深度的自编码器相比，它将PSNR提高了约20%，NMSE减少了约50%，SSIM提高了1%，而参数仅需要大约1%的边际增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world traffic surveillance, vehicle images captured under adverseweather, poor lighting, or high-speed motion often suffer from severe noise andblur. Such degradations significantly reduce the accuracy of license platerecognition systems, especially when the plate occupies only a small regionwithin the full vehicle image. Restoring these degraded images a fast realtimemanner is thus a crucial pre-processing step to enhance recognitionperformance. In this work, we propose a Vertical Residual Autoencoder (VRAE)architecture designed for the image enhancement task in traffic surveillance.The method incorporates an enhancement strategy that employs an auxiliaryblock, which injects input-aware features at each encoding stage to guide therepresentation learning process, enabling better general informationpreservation throughout the network compared to conventional autoencoders.Experiments on a vehicle image dataset with visible license plates demonstratethat our method consistently outperforms Autoencoder (AE), GenerativeAdversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE atthe same depth, it improves PSNR by about 20%, reduces NMSE by around 50%, andenhances SSIM by 1%, while requiring only a marginal increase of roughly 1% inparameters.</description>
      <author>example@mail.com (Cuong Nguyen, Dung T. Tran, Hong Nguyen, Xuan-Vu Phan, Nam-Phong Nguyen)</author>
      <guid isPermaLink="false">2509.08392v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection</title>
      <link>http://arxiv.org/abs/2509.09572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PeftCD框架，一种基于视觉基础模型(VFMs)和参数高效微调(PEFT)的变化检测方法，解决了多时相多源遥感影像中的伪变化、标记样本稀缺和跨域泛化困难问题。&lt;h4&gt;背景&lt;/h4&gt;多时相多源遥感影像中存在伪变化普遍、标记样本稀缺、跨域泛化困难等挑战，影响了变化检测的准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理伪变化、减少对标记数据依赖并提高跨域泛化能力的变化检测框架。&lt;h4&gt;方法&lt;/h4&gt;构建了PeftCD框架，核心是使用源自VFM的权重共享Siamese编码器，无缝集成LoRA和Adapter模块，只需训练少量额外参数即可实现高效任务适应。研究了SAM2和DINOv3两种骨干网络，并配备轻量级解码器以保留骨干网络的强大特征表示能力。&lt;h4&gt;主要发现&lt;/h4&gt;PeftCD在多个公共数据集上实现了最先进性能：SYSU-CD (IoU 73.81%)、WHUCD (92.05%)、MSRSCD (64.07%)、MLCD (76.89%)、CDD (97.01%)、S2Looking (52.25%)和LEVIR-CD (85.62%)，具有精确的边界描绘能力和对伪变化的强抑制能力。&lt;h4&gt;结论&lt;/h4&gt;PeftCD在准确性、效率和泛化能力方面实现了最佳平衡，为适应大规模VFMs到真实世界遥感变化检测应用提供了强大且可扩展的范式。&lt;h4&gt;翻译&lt;/h4&gt;为了解决多时相和多源遥感影像中伪变化的普遍性、标记样本的稀缺性以及跨域泛化的困难，我们提出了PeftCD，这是一种基于视觉基础模型(VFMs)并采用参数高效微调(PEFT)的变化检测框架。其核心是使用源自VFM的权重共享Siamese编码器，无缝集成了LoRA和Adapter模块。这种设计通过仅训练少量额外参数，实现了高度高效的任务适应。为了充分释放VFMs的潜力，我们研究了两种主流骨干网络：以强大分割先验而闻名的Segment Anything Model v2 (SAM2)，以及最先进的自监督表征学习器DINOv3。该框架配备了精心设计的轻量级解码器，确保重点保持在骨干网络的强大特征表示上。大量实验表明，PeftCD在多个公共数据集上实现了最先进的性能，包括SYSU-CD (IoU 73.81%)、WHUCD (92.05%)、MSRSCD (64.07%)、MLCD (76.89%)、CDD (97.01%)、S2Looking (52.25%)和LEVIR-CD (85.62%)，具有精确的边界描绘能力和对伪变化的强抑制能力。总之，PeftCD在准确性、效率和泛化能力方面实现了最佳平衡。它为将大规模VFMs适应到真实世界遥感变化检测应用中提供了强大且可扩展的范式。代码和预训练模型将在https://github.com/dyzy41/PeftCD发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To tackle the prevalence of pseudo changes, the scarcity of labeled samples,and the difficulty of cross-domain generalization in multi-temporal andmulti-source remote sensing imagery, we propose PeftCD, a change detectionframework built upon Vision Foundation Models (VFMs) with Parameter-EfficientFine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siameseencoder derived from a VFM, into which LoRA and Adapter modules are seamlesslyintegrated. This design enables highly efficient task adaptation by trainingonly a minimal set of additional parameters. To fully unlock the potential ofVFMs, we investigate two leading backbones: the Segment Anything Model v2(SAM2), renowned for its strong segmentation priors, and DINOv3, astate-of-the-art self-supervised representation learner. The framework iscomplemented by a deliberately lightweight decoder, ensuring the focus remainson the powerful feature representations from the backbones. Extensiveexperiments demonstrate that PeftCD achieves state-of-the-art performanceacross multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) andLEVIR-CD (85.62%), with notably precise boundary delineation and strongsuppression of pseudo-changes. In summary, PeftCD presents an optimal balanceof accuracy, efficiency, and generalization. It offers a powerful and scalableparadigm for adapting large-scale VFMs to real-world remote sensing changedetection applications. The code and pretrained models will be released athttps://github.com/dyzy41/PeftCD.</description>
      <author>example@mail.com (Sijun Dong, Yuxuan Hu, LiBo Wang, Geng Chen, Xiaoliang Meng)</author>
      <guid isPermaLink="false">2509.09572v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>DualTrack: Sensorless 3D Ultrasound needs Local and Global Context</title>
      <link>http://arxiv.org/abs/2509.09530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为DualTrack的新型双编码器架构，用于无传感器的三维超声成像，通过分离处理局部和全局特征，实现了高精度和全局一致的三维重建。&lt;h4&gt;背景&lt;/h4&gt;三维超声相比传统二维成像具有临床优势，但传统3D系统的高成本和复杂性限制了其广泛应用。无传感器3D US是一种有前景的替代方案，使用深度学习从2D超声图像序列中估计3D探头轨迹。局部特征帮助预测帧间运动，全局特征帮助扫描定位，但先前的 approaches 无法有效分离处理这两类特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够稳健地建模局部和全局特征的方法，以提高无传感器3D US的准确性，实现高精度和全局一致的三维重建。&lt;h4&gt;方法&lt;/h4&gt;提出DualTrack双编码器架构，利用分离的局部和全局编码器分别处理不同尺度的特征。局部编码器使用密集时空卷积捕获细粒度特征，全局编码器利用图像主干和时序注意力层嵌入高级解剖特征和长程依赖关系，通过轻量级融合模块结合这些特征来估计轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;在大型公共基准上的实验结果表明，DualTrack达到了最先进的准确性，实现了全局一致的三维重建，优于先前的方法，平均重建误差低于5毫米。&lt;h4&gt;结论&lt;/h4&gt;DualTrack通过分离处理局部和全局特征，有效解决了无传感器3D US中的关键挑战，在三维超声成像领域具有重要的临床应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;三维超声(US)相比传统二维成像具有许多临床优势，但其广泛应用受到传统3D系统成本和复杂性的限制。无传感器3D US使用深度学习从2D US图像序列中估计3D探头轨迹，是一种有前景的替代方案。局部特征（如斑点模式）可以帮助预测帧间运动，而全局特征（如粗略形状和解剖结构）可以帮助将扫描相对于解剖定位并预测其一般形状。在先前的 approaches 中，全局特征要么被忽略，要么与局部特征提取紧密耦合，限制了稳健地建模这两个互补方面的能力。我们提出DualTrack，一种新颖的双编码器架构，利用分离的局部和全局编码器，专门用于各自尺度的特征提取。局部编码器使用密集时空卷积来捕获细粒度特征，而全局编码器利用图像主干（如2D CNN或基础模型）和时序注意力层来嵌入高级解剖特征和长程依赖关系。然后，轻量级融合模块结合这些特征来估计轨迹。在大型公共基准上的实验结果表明，DualTrack达到了最先进的准确性，并实现了全局一致的三维重建，优于先前的方法，平均重建误差低于5毫米。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无传感器3D超声成像中的轨迹估计问题，即如何仅从2D超声图像序列中估计探头的3D运动轨迹，从而重建出3D超声图像。这个问题很重要，因为传统3D超声系统需要昂贵的矩阵探头或外部追踪设备，限制了3D超声的广泛应用。而无传感器3D超声可以降低成本、简化操作，使3D超声在临床实践中更易于推广，同时3D超声相比2D成像具有增强可视化、改善解剖导航和精确体积测量的优势。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到局部特征（如斑点模式）能帮助预测帧间运动，全局特征（如解剖结构）能帮助定位扫描位置。先前方法要么忽略全局特征，要么将局部和全局特征紧密耦合，限制了建模能力。作者借鉴了双编码器范式，设计了两个专门编码器：一个处理局部特征，一个处理全局特征。他们还借鉴了CNN用于局部特征提取、序列模型用于长期依赖建模，以及医学图像处理中的双编码器应用。通过分离处理，作者能针对不同特征类型使用专门的设计和训练策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用双编码器架构分别处理局部和全局特征，然后融合它们以估计探头轨迹，从而提高轨迹估计的准确性和全局一致性。整体流程：1) 局部编码器使用3D CNN捕获细粒度特征，通过短连续子序列和全分辨率输入保持局部性；2) 全局编码器使用2D CNN和时序注意力捕获高级解剖特征，通过非连续子序列和下采样强调全局性；3) 融合模块使用transformer架构结合两种特征，通过交叉注意力让局部特征受益于全局上下文；4) 训练时先独立编码器预训练，再训练融合模块。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 首个用于无传感器3D超声的双编码器网络架构；2) 专门的局部和全局编码器设计，针对不同特征类型优化；3) 探索医学基础模型作为全局编码器的潜力；4) 专门的融合机制结合两种特征。相比之前工作，不同之处在于：DualTrack明确分离局部和全局建模，而先前方法要么忽略全局特征，要么使用相同编码器处理两种特征；局部编码器专注于斑点模式和短时运动，全局编码器专注于解剖结构和长期依赖；实验证明DualTrack显著优于先前方法，平均重建误差低于5毫米。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DualTrack通过创新的双编码器架构分离和融合局部与全局特征，显著提高了无传感器3D超声重建的准确性和一致性，实现了低于5毫米的平均重建误差。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional ultrasound (US) offers many clinical advantages overconventional 2D imaging, yet its widespread adoption is limited by the cost andcomplexity of traditional 3D systems. Sensorless 3D US, which uses deeplearning to estimate a 3D probe trajectory from a sequence of 2D US images, isa promising alternative. Local features, such as speckle patterns, can helppredict frame-to-frame motion, while global features, such as coarse shapes andanatomical structures, can situate the scan relative to anatomy and helppredict its general shape. In prior approaches, global features are eitherignored or tightly coupled with local feature extraction, restricting theability to robustly model these two complementary aspects. We proposeDualTrack, a novel dual-encoder architecture that leverages decoupled local andglobal encoders specialized for their respective scales of feature extraction.The local encoder uses dense spatiotemporal convolutions to capturefine-grained features, while the global encoder utilizes an image backbone(e.g., a 2D CNN or foundation model) and temporal attention layers to embedhigh-level anatomical features and long-range dependencies. A lightweightfusion module then combines these features to estimate the trajectory.Experimental results on a large public benchmark show that DualTrack achievesstate-of-the-art accuracy and globally consistent 3D reconstructions,outperforming previous methods and yielding an average reconstruction errorbelow 5 mm.</description>
      <author>example@mail.com (Paul F. R. Wilson, Matteo Ronchetti, Rüdiger Göbl, Viktoria Markova, Sebastian Rosenzweig, Raphael Prevost, Parvin Mousavi, Oliver Zettinig)</author>
      <guid isPermaLink="false">2509.09530v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Video Understanding by Design: How Datasets Shape Architectures and Insights</title>
      <link>http://arxiv.org/abs/2509.09151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Research report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述首次采用数据驱动的视角，分析数据集特性如何引导模型架构发展，将运动复杂性、时间跨度、层次结构和多模态丰富性视为施加归纳偏差的关键因素，并为视频理解领域提供统一框架和指导路线图。&lt;h4&gt;背景&lt;/h4&gt;视频理解因日益复杂的数据集和强大的架构而快速发展，但现有调查多按任务或家族分类模型，忽视了数据集引导架构演化的结构性压力。&lt;h4&gt;目的&lt;/h4&gt;采用数据驱动视角揭示数据集特性与模型架构间的关联，重新解释视频理解领域的发展里程碑，并提供将模型设计与数据集特性对齐的实用指导。&lt;h4&gt;方法&lt;/h4&gt;通过统一数据集、归纳偏差和架构到一个连贯框架中，重新解释从双流网络和3D CNN到顺序模型、Transformer和多模态基础模型的发展历程。&lt;h4&gt;主要发现&lt;/h4&gt;运动复杂性、时间跨度、层次结构和多模态丰富性对模型施加归纳偏差；模型发展里程碑是对这些数据驱动压力的具体响应；将模型设计与数据集不变性对齐同时平衡可扩展性和任务需求是有效的实践方法。&lt;h4&gt;结论&lt;/h4&gt;通过将数据集、归纳偏差和架构统一为一个连贯框架，该综述为推进通用视频理解提供了全面的回顾和指导性路线图。&lt;h4&gt;翻译&lt;/h4&gt;视频理解因日益复杂的数据集和强大的架构而迅速发展。然而，现有的调查大多按任务或家族对模型进行分类，忽视了数据集引导架构演化的结构性压力。这篇综述首次采用数据驱动的视角，展示了运动复杂性、时间跨度、层次结构和多模态丰富性如何施加模型应编码的归纳偏差。我们将从双流网络和3D CNN到顺序模型、Transformer和多模态基础模型的里程碑重新解释为对这些数据驱动压力的具体响应。基于这一综合，我们提供了将模型设计与数据集不变性对齐的实用指导，同时平衡可扩展性和任务需求。通过将数据集、归纳偏差和架构统一为一个连贯的框架，本综述为推进通用视频理解提供了全面的回顾和指导性路线图。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是：现有视频理解研究大多按任务或模型家族分类，忽视了数据集如何通过其结构特性引导架构演化的作用。这个问题在研究中很重要，因为缺乏这种理解会导致研究缺乏概念地图来解释架构为何如此演变，没有集成框架来背景化先前进展和预测未来趋势，也无法指导更有效模型的设计、训练策略的选择和更好的数据集构建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用数据驱动的视角，将数据集视为'结构透镜'而非静态基准。他们通过分析过去20年中最流行的数据集，识别出四个主要类别的数据集特性：运动复杂性、时间跨度、层次结构和多模态丰富性。然后分析这些特性如何引导架构演变。作者借鉴了现有工作，但提供了一个新视角，将数据集、架构和范式统一在一个框架下，填补了现有调查缺乏对数据集如何影响架构设计理解的空白。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：数据集通过其结构特性（运动复杂性、时间跨度、层次结构和多模态丰富性）施加'结构压力'，这些压力引导归纳偏差，最终影响架构设计。作者提出了一个'数据集-偏差-架构'框架。整体流程包括：收集分析过去20年流行数据集；根据结构特性对数据集分类；分析这些特性如何引导架构演变；重新解释从双流CNN到transformer等里程碑作为对数据集驱动压力的响应；基于此提供任务导向路线图，用于设计在时间、关系和多模态推理与可扩展性和部署约束间取得平衡的模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次采用数据驱动视角审视视频理解；提出'数据集-偏差-架构'框架统一数据集、架构和归纳偏差；系统分析数据集特性如何引导架构演变；提供任务导向路线图。相比之前工作，这篇论文的不同之处在于：之前的调查大多按任务或模型家族分类，忽视数据集的结构引导作用；缺乏能背景化先前进展和预测未来趋势的概念框架；没有将数据集、架构和范式统一在一个框架下。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过建立数据集特性与架构设计之间的明确联系，提供了一个理解视频理解领域演进的统一框架，并指导未来研究如何根据数据集的结构压力来设计更有效的模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding has advanced rapidly, fueled by increasingly complexdatasets and powerful architectures. Yet existing surveys largely classifymodels by task or family, overlooking the structural pressures through whichdatasets guide architectural evolution. This survey is the first to adopt adataset-driven perspective, showing how motion complexity, temporal span,hierarchical composition, and multimodal richness impose inductive biases thatmodels should encode. We reinterpret milestones, from two-stream and 3D CNNs tosequential, transformer, and multimodal foundation models, as concreteresponses to these dataset-driven pressures. Building on this synthesis, weoffer practical guidance for aligning model design with dataset invarianceswhile balancing scalability and task demands. By unifying datasets, inductivebiases, and architectures into a coherent framework, this survey provides botha comprehensive retrospective and a prescriptive roadmap for advancinggeneral-purpose video understanding.</description>
      <author>example@mail.com (Lei Wang, Piotr Koniusz, Yongsheng Gao)</author>
      <guid isPermaLink="false">2509.09151v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain</title>
      <link>http://arxiv.org/abs/2509.09130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ALL-PET是一种创新的低资源、少样本PET基础模型，通过在投影域直接操作，克服了数据稀缺和计算资源限制问题，仅需500个样本即可实现高质量正弦图生成，性能媲美大规模数据集训练的模型。&lt;h4&gt;背景&lt;/h4&gt;构建大规模PET成像基础模型受到标记数据获取有限和计算资源不足的阻碍，限制了PET成像领域的发展。&lt;h4&gt;目的&lt;/h4&gt;克服数据稀缺和效率限制，提出一种在投影域直接运行的低资源、少样本PET基础模型。&lt;h4&gt;方法&lt;/h4&gt;ALL-PET利用潜在扩散模型(LDM)并包含三个关键创新：1) Radon掩码增强策略(RMAS)，通过投影随机图像域掩码生成20多万个结构多样训练样本；2) 动态多掩码(DMM)机制，改变掩码数量和分布增强数据多样性；3) 正/负掩码约束嵌入几何一致性；4) 透明医学注意力(TMA)增强病变相关区域，支持临床ROI调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明ALL-PET仅使用500个样本即可实现高质量正弦图生成，性能可与更大数据集训练的模型相媲美；该模型在低剂量重建、衰减校正、延迟帧预测和示踪剂分离等多种任务中表现出良好的泛化能力；内存使用低于24GB，运行效率高。&lt;h4&gt;结论&lt;/h4&gt;ALL-PET是一种有效的解决方案，能够在数据有限的情况下实现高质量PET成像，为资源受限环境下的PET成像应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;构建用于PET成像的大规模基础模型受到标记数据获取有限和计算资源不足的阻碍。为了克服数据稀缺和效率限制，我们提出了ALL-PET，一种在投影域直接运行的低资源、少样本PET基础模型。ALL-PET利用潜在扩散模型(LDM)并具有三个关键创新。首先，我们设计了一种Radon掩码增强策略(RMAS)，通过将随机图像域掩码投影到正弦图空间，生成超过20万个结构多样的训练样本，以最小数据量显著提高泛化能力。通过动态多掩码(DMM)机制扩展该方法，改变掩码数量和分布，增加数据多样性而不增加模型复杂度。其次，我们实现了正/负掩码约束，嵌入严格的几何一致性，减少参数负担同时保持生成质量。第三，我们引入透明医学注意力(TMA)，一种无参数、几何驱动的机制，增强原始投影数据中的病变相关区域。病变焦点注意力图从粗略分割导出，覆盖高代谢和低代谢区域，并投影到正弦图空间以保持物理一致性引导。系统支持临床医生定义的ROI调整，确保灵活、可解释且任务自适应的强调，与PET采集物理原理一致。实验结果表明，ALL-PET仅使用500个样本即可实现高质量正弦图生成，性能可与在更大数据集上训练的模型相媲美。ALL-PET在低剂量重建、衰减校正、延迟帧预测和示踪剂分离等任务中具有泛化能力，内存使用低于24GB，运行效率高。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决PET成像基础模型构建中的两个核心挑战：数据资源稀缺和计算资源限制。在临床环境中，获取大量标记的PET数据困难且成本高，同时现有基础模型通常需要数亿参数和大量计算资源，难以在资源有限的医院或便携式设备中部署。这个问题很重要，因为PET成像在肿瘤学、神经科学等领域至关重要，解决这些问题可以促进先进AI模型在资源受限医疗环境中的实际应用，提高诊断效率和可及性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了PET基础模型面临的数据稀缺和计算资源限制问题，指出大多数现有医学基础模型在图像域操作，忽略了投影域中包含的重要物理先验信息。他们选择在投影域操作，采用潜在扩散模型(LDM)作为基础框架，并针对数据稀缺问题设计创新的数据增强策略。作者借鉴了扩散模型在医学图像生成中的应用、Radon变换在PET成像中的理论基础、注意力机制在医学图像分析中的应用，以及低资源学习方法。他们的创新性设计包括RMAS策略、DMM机制、TMA机制和正负掩码约束，这些都是对现有方法的改进和扩展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; ALL-PET的核心思想是通过智能的数据设计和物理引导的学习，在投影域构建一个低资源、低剂量的PET基础模型。整体实现流程包括：1)收集有限数量的PET正弦图数据；2)使用RMAS策略将图像域掩码投影到正弦图空间生成多样化训练样本；3)应用DMM机制动态变化掩码增加数据多样性；4)基于潜在扩散模型构建框架，应用正负掩码约束嵌入几何一致性；5)使用TMA机制增强病变相关区域；6)将训练好的模型应用于各种下游任务如低剂量重建、衰减校正、延迟帧预测等；7)使用扩散采样方法进行推理和生成，支持文本提示和参数控制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Radon Mask增强策略(RMAS)，通过投影生成多样化训练样本；2)动态多掩码(DMM)机制，增加数据多样性而不增加模型复杂度；3)正负掩码约束，作为零成本正则化器嵌入几何一致性；4)透明医疗注意力(TMA)，无参数的几何驱动注意力机制。相比之前工作，ALL-PET直接在投影域操作而非图像域，仅使用500个样本而非数万至数十万样本，可在单个GPU而非多节点集群上运行，通过物理约束确保物理一致性，并通过TMA机制提供临床灵活性，允许医生注入领域知识无需重新训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALL-PET通过创新的投影域数据增强、几何约束和可解释注意力机制，首次实现了在资源受限条件下仅使用500个样本即可生成高质量PET投影数据的基础模型，为临床环境中的低资源PET成像应用提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building large-scale foundation model for PET imaging is hindered by limitedaccess to labeled data and insufficient computational resources. To overcomedata scarcity and efficiency limitations, we propose ALL-PET, a low-resource,low-shot PET foundation model operating directly in the projection domain.ALL-PET leverages a latent diffusion model (LDM) with three key innovations.First, we design a Radon mask augmentation strategy (RMAS) that generates over200,000 structurally diverse training samples by projecting randomizedimage-domain masks into sinogram space, significantly improving generalizationwith minimal data. This is extended by a dynamic multi-mask (DMM) mechanismthat varies mask quantity and distribution, enhancing data diversity withoutadded model complexity. Second, we implement positive/negative mask constraintsto embed strict geometric consistency, reducing parameter burden whilepreserving generation quality. Third, we introduce transparent medicalattention (TMA), a parameter-free, geometry-driven mechanism that enhanceslesion-related regions in raw projection data. Lesion-focused attention mapsare derived from coarse segmentation, covering both hypermetabolic andhypometabolic areas, and projected into sinogram space for physicallyconsistent guidance. The system supports clinician-defined ROI adjustments,ensuring flexible, interpretable, and task-adaptive emphasis aligned with PETacquisition physics. Experimental results show ALL-PET achieves high-qualitysinogram generation using only 500 samples, with performance comparable tomodels trained on larger datasets. ALL-PET generalizes across tasks includinglow-dose reconstruction, attenuation correction, delayed-frame prediction, andtracer separation, operating efficiently with memory use under 24GB.</description>
      <author>example@mail.com (Bin Huang, Kang Chen, Bingxuan Li, Huafeng Liu, Qiegen Liu)</author>
      <guid isPermaLink="false">2509.09130v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis</title>
      <link>http://arxiv.org/abs/2509.08961v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FoundationalECGNet的基础框架，用于自动心电图分类，解决了当前ECG分析中面临的噪声、类别不平衡和数据集异质性等挑战。&lt;h4&gt;背景&lt;/h4&gt;心血管疾病是全球主要的死亡原因，心电图分析是检测心脏异常的关键技术，但当前方法面临多种技术挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种准确且可扩展的心电图自动诊断系统，提高心脏异常检测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;FoundationalECGNet模型集成双阶段去噪技术（Morlet和Daubechies小波变换）、卷积块注意力模块、图注意力网络和时间序列变换器，共同捕获多通道ECG信号中的空间和时间依赖关系，采用两阶段分类策略：先区分正常与异常ECG信号，再将异常信号细分为五种心脏疾病。&lt;h4&gt;主要发现&lt;/h4&gt;模型在多个数据集上表现优异，正常与异常分类达到99%的F1分数；在多类疾病检测中达到最先进性能，包括传导障碍和肥厚症99%的F1分数，心律失常98.9%的F1分数；同时提供风险水平估计以辅助临床决策。&lt;h4&gt;结论&lt;/h4&gt;FoundationalECGNet是一种可扩展、可解释和通用的自动ECG分析解决方案，有潜力提高医疗环境中的诊断精度和患者结局。&lt;h4&gt;翻译&lt;/h4&gt;心血管疾病(CVDs)仍然是全球主要的死亡原因，这凸显了准确且可扩展的诊断系统的重要性。心电图(ECG)分析是检测心脏异常的核心，然而噪声、类别不平衡和数据集异质性等挑战限制了当前方法。为解决这些问题，我们提出了FoundationalECGNet，一种用于自动ECG分类的基础框架。该模型集成了Morlet和Daubechies小波变换的双阶段去噪、卷积块注意力模块(CBAM)、图注意力网络(GAT)和时间序列变换器(TST)，共同捕获多通道ECG信号中的空间和时间依赖关系。FoundationalECGNet首先区分正常和异常ECG信号，然后将异常信号分类为五种心脏疾病之一：心律失常、传导障碍、心肌梗死、QT异常或肥厚症。在多个数据集上，模型在正常与异常分类中达到99%的F1分数，并在多类疾病检测中显示出最先进的性能，包括传导障碍和肥厚症99%的F1分数，以及心律失常98.9%的F1分数。此外，该模型提供风险水平估计以促进临床决策。总之，FoundationalECGNet代表了自动ECG分析的一种可扩展、可解释和通用的解决方案，有可能提高医疗环境中的诊断精度和患者结局。我们将在接受后分享代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,underscoring the importance of accurate and scalable diagnostic systems.Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,yet challenges such as noise, class imbalance, and dataset heterogeneity limitcurrent methods. To address these issues, we propose FoundationalECGNet, afoundational framework for automated ECG classification. The model integrates adual-stage denoising by Morlet and Daubechies wavelets transformation,Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),and Time Series Transformers (TST) to jointly capture spatial and temporaldependencies in multi-channel ECG signals. FoundationalECGNet firstdistinguishes between Normal and Abnormal ECG signals, and then classifies theAbnormal signals into one of five cardiac conditions: Arrhythmias, ConductionDisorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Acrossmultiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormalclassification and shows state-of-the-art performance in multi-class diseasedetection, including a 99% F1-score for Conduction Disorders and Hypertrophy,as well as a 98.9% F1-score for Arrhythmias. Additionally, the model providesrisk level estimations to facilitate clinical decision-making. In conclusion,FoundationalECGNet represents a scalable, interpretable, and generalizablesolution for automated ECG analysis, with the potential to improve diagnosticprecision and patient outcomes in healthcare settings. We'll share the codeafter acceptance.</description>
      <author>example@mail.com (Md. Sajeebul Islam Sk., Md Jobayer, Md Mehedi Hasan Shawon, Md. Golam Raibul Alam)</author>
      <guid isPermaLink="false">2509.08961v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Live(r) Die: Predicting Survival in Colorectal Liver Metastasis</title>
      <link>http://arxiv.org/abs/2509.08935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Thesis at Erasmus Mundus Joint Master's Degree in Medical Imaging and  Applications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一个全自动框架，利用术前MRI图像预测结直肠癌肝转移(CRLM)手术患者的生存结果，结合自动分割算法和基于放射组学的生存分析，显著提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;结直肠癌经常转移到肝脏，显著降低长期生存率。虽然手术切除是唯一可能治愈CRLM的治疗方法，但患者结果因肿瘤特征以及临床和基因组因素而异。当前的预后模型基于有限的临床或分子特征，预测能力不足，特别是在多发性CRLM病例中。&lt;h4&gt;目的&lt;/h4&gt;开发一个全自动框架，利用术前获取的对比增强前和对比增强后MRI图像来预测CRLM手术患者的预后结果，提高预测准确性并减少对标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;研究提出了包含分割流程和放射组学流程的框架。分割流程利用可提示的基础模型从未标注数据中学习分割肝脏、肿瘤和脾脏。研究者还提出了SAMONAI，一种零样本3D提示传播算法，利用分割任何模型从单点提示分割3D感兴趣区域。分割结果随后输入放射组学流程，提取每个肿瘤的特征并使用SurvAMINN(一种基于自编码器的多实例神经网络)预测生存。&lt;h4&gt;主要发现&lt;/h4&gt;在包含227名患者的机构数据集上进行的大量评估表明，该框架超越了现有的临床和生物标志物，C-index提高了超过10%。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明了整合自动分割算法和基于放射组学的生存分析能够为CRLM提供准确、标注效率高且可解释的结果预测。&lt;h4&gt;翻译&lt;/h4&gt;结直肠癌经常转移到肝脏，显著降低长期生存率。虽然手术切除是唯一可能治愈结直肠癌肝转移(CRLM)的治疗方法，但患者结果因肿瘤特征以及临床和基因组因素而异。当前的预后模型通常基于有限的临床或分子特征，预测能力不足，特别是在多发性CRLM病例中。我们提出了一个全自动框架，用于从术前获取的对比增强前和对比增强后MRI预测手术结果。我们的框架包含一个分割流程和一个放射组学流程。分割流程利用可提示的基础模型从未标注数据中学习分割肝脏、肿瘤和脾脏，完成缺失的标签。此外，我们提出了SAMONAI，一种新颖的零样本3D提示传播算法，利用分割任何模型从单点提示分割3D感兴趣区域，显著提高了我们分割流程的准确性和效率。然后将预测的对比增强前和对比增强后的分割结果输入我们的放射组学流程，该流程从每个肿瘤中提取特征并使用SurvAMINN(一种基于自编码器的多实例神经网络用于生存分析)预测生存。SurvAMINN从右删失生存数据中联合学习降维和风险预测，专注于最具侵袭性的肿瘤。在包含227名患者的机构数据集上进行的大量评估表明，我们的框架超越了现有的临床和基因组生物标志物，C-index提高了超过10%。我们的结果表明，整合自动分割算法和基于放射组学的生存分析能够为CRLM提供准确、标注效率高且可解释的结果预测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决结直肠癌肝转移(CRLM)患者手术预后预测不准确的问题。结直肠癌是全球第三大常见癌症和第二大癌症相关死亡原因，肝脏是其最常见的转移部位。手术切除是唯一可能治愈CRLM的方法，但并非所有患者都适合，且患者预后差异很大。准确预测手术结果对于避免无益手术、允许个性化治疗并最终提高患者生存结果至关重要。现有的预后模型（基于有限的临床或分子特征）预测能力不足，特别是在多发性CRLM病例中，因此这一问题在临床实践中具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有预后方法的局限性（忽略多发性肿瘤、仅依赖后对比图像、依赖手动分割、需要完全标注数据）来设计方法。作者借鉴了现有工作：分割管道借鉴了UNet、nn-UNet和基于Transformer的架构，以及自监督学习和基础模型（特别是Segment Anything Model, SAM）；放射组学管道借鉴了AMINN（基于自编码器的多实例网络）和多实例学习(MIL)概念。作者创新性地将SAM扩展到3D医学图像分割（提出SAMONAI），并改进了生存预测网络（SurvAMINN）以更好地处理删失数据和多发性肿瘤，实现了全自动、多病灶、标注高效的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用医学影像（MRI）的全自动分析来预测CRLM患者的生存情况，结合对比前后的MRI图像提高准确性，使用基础模型和少量学习技术解决标注数据稀缺问题，并聚焦最具侵袭性的肿瘤进行预后预测。整体流程分为两个主要管道：1）分割管道：使用SAMONAI（3D提示传播算法）从部分标注数据中学习肝脏、肿瘤和脾脏的分割；2）放射组学管道：从每个肿瘤中提取放射组学特征，使用SurvAMINN（自编码器多实例神经网络）进行生存预测，通过LogSumExp池化操作关注最具侵袭性的肿瘤。评估使用Dice系数（分割质量）、F1分数（肿瘤检测）和C-index（预后预测）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）SAMONAI - 一种3D提示传播算法，从单个点提示分割整个3D对象；2）SurvAMINN - 用于生存分析的自编码器多实例神经网络，能从删失数据中学习并关注最具侵袭性的肿瘤；3）全自动、多病灶、标注高效的框架，能从部分标注数据中学习并整合对比前后的MRI图像。相比之前的工作，不同之处在于：不再仅依赖临床或分子特征，而是整合医学影像信息；考虑多发性肿瘤而不仅是最大肿瘤；使用全自动分割而非手动分割；结合对比前后的图像信息；能够处理删失数据；联合学习降维和预测避免信息损失。这些创新使预测准确性显著提高（C-index提高超过10%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种全自动框架，通过创新的3D图像分割技术和基于放射组学的生存预测方法，显著提高了结直肠癌肝转移患者的手术预后预测准确性，同时减少了对大量标注数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Colorectal cancer frequently metastasizes to the liver, significantlyreducing long-term survival. While surgical resection is the only potentiallycurative treatment for colorectal liver metastasis (CRLM), patient outcomesvary widely depending on tumor characteristics along with clinical and genomicfactors. Current prognostic models, often based on limited clinical ormolecular features, lack sufficient predictive power, especially in multifocalCRLM cases. We present a fully automated framework for surgical outcomeprediction from pre- and post-contrast MRI acquired before surgery. Ourframework consists of a segmentation pipeline and a radiomics pipeline. Thesegmentation pipeline learns to segment the liver, tumors, and spleen frompartially annotated data by leveraging promptable foundation models to completemissing labels. Also, we propose SAMONAI, a novel zero-shot 3D promptpropagation algorithm that leverages the Segment Anything Model to segment 3Dregions of interest from a single point prompt, significantly improving oursegmentation pipeline's accuracy and efficiency. The predicted pre- andpost-contrast segmentations are then fed into our radiomics pipeline, whichextracts features from each tumor and predicts survival using SurvAMINN, anovel autoencoder-based multiple instance neural network for survival analysis.SurvAMINN jointly learns dimensionality reduction and hazard prediction fromright-censored survival data, focusing on the most aggressive tumors. Extensiveevaluation on an institutional dataset comprising 227 patients demonstratesthat our framework surpasses existing clinical and genomic biomarkers,delivering a C-index improvement exceeding 10%. Our results demonstrate thepotential of integrating automated segmentation algorithms and radiomics-basedsurvival analysis to deliver accurate, annotation-efficient, and interpretableoutcome prediction in CRLM.</description>
      <author>example@mail.com (Muhammad Alberb, Helen Cheung, Anne Martel)</author>
      <guid isPermaLink="false">2509.08935v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and 3D Metric-Scaled Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2509.08813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Calib3R的新方法，能够同时完成相机到机器人的校准和度量缩放的3D重建，无需传统校准所需的图案，只需少量RGB图像即可实现高精度校准。&lt;h4&gt;背景&lt;/h4&gt;机器人通常依赖RGB图像完成操作和导航任务，但可靠的交互需要度量缩放并与机器人参考框架对齐的3D场景表示。传统方法中，相机校准和3D重建被视为独立任务，多摄像头设置增加了数据整合的复杂性。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需图案的方法，通过统一优化同时执行相机到机器人的校准和度量缩放的3D重建，适用于单摄像头和多摄像头设置。&lt;h4&gt;方法&lt;/h4&gt;提出Calib3R方法，基于3D基础模型MASt3R从RGB图像中提取点图，结合机器人姿态信息，重建与机器人参考框架对齐的缩放3D场景，可应用于机器人手臂或移动机器人。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，Calib3R在各种数据集上使用少于10张图像即可实现精确校准，性能优于现有的无目标和基于标记的方法。&lt;h4&gt;结论&lt;/h4&gt;Calib3R提供了一种高效解决方案，能够同时解决相机校准和3D重建问题，简化了机器人系统的3D场景获取流程。&lt;h4&gt;翻译&lt;/h4&gt;机器人通常依赖RGB图像完成操作和导航等任务。然而，可靠的交互通常需要3D场景表示，该表示应该是度量缩放的并与机器人参考框架对齐。这依赖于精确的相机到机器人的校准和密集的3D重建，这些任务通常被分别处理，尽管两者都依赖于RGB数据中的几何对应关系。传统校准需要图案，而基于RGB的重建在任意框架中产生未知尺度的几何形状。多摄像头设置增加了额外的复杂性，因为数据必须在共享参考框架中表示。我们提出了Calib3R，一种无需图案的方法，通过统一优化同时执行相机到机器人的校准和度量缩放的3D重建。Calib3R处理机器人手臂或移动机器人上的单摄像头和多摄像头设置。它基于3D基础模型MASt3R从RGB图像中提取点图，这些点图与机器人姿态结合，重建与机器人对齐的缩放3D场景。在各种数据集上的实验表明，Calib3R使用少于10张图像即可实现精确校准，性能优于无目标和基于标记的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人系统中相机到机器人的标定和3D场景重建的统一问题。传统方法通常将这两个任务分开处理，且需要专门的校准模式或标记物。这个问题在现实中很重要，因为机器人需要准确理解周围环境的3D结构才能安全有效地与环境交互，而现有的标定方法往往需要专用设备、手动操作或特定环境条件，限制了机器人在复杂、动态环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括传统标定需要模式、多相机标定只关注相机间相对姿态而忽略与机器人参考帧的关系、以及标定和重建作为独立任务处理导致效率低下。作者注意到3D基础模型MASt3R能直接从RGB图像估计局部3D几何，但缺乏度量比例和与机器人参考帧的对齐。方法设计借鉴了MASt3R-SfM框架和手眼标定的经典公式AX=XB，但进行了改进以处理比例问题。作者设计了一个统一的优化过程，将相机到机器人标定和3D重建结合在一起，利用机器人姿态信息同时估计相机到机器人的变换和比例因子。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将相机到机器人标定和3D场景重建视为一个统一的优化问题，而不是两个独立任务。方法利用MASt3R模型从RGB图像中提取局部点图，结合机器人姿态信息，通过联合优化实现标定和重建，并引入比例因子λ解决重建的尺度模糊问题。在多相机情况下，添加跨相机一致性约束保持相机间相对姿态。整体流程：1)输入RGB图像和机器人姿态；2)用MASt3R生成局部点图；3)构建共可见性图；4)计算规范点图；5)定义包含场景几何、标定和跨相机一致性的联合损失函数；6)通过梯度下降优化；7)输出相机到机器人变换和对齐的3D重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一框架，联合执行标定和重建；2)无需校准模式；3)单一优化过程同时处理两个任务；4)引入比例因子解决尺度模糊；5)多相机情况下的跨相机一致性约束；6)适用于多种机器人平台和相机配置。相比之前的工作，不同之处在于：不需要传统校准设备；直接与机器人参考帧对齐；避免了多阶段处理和误差累积；将重建和标定集成在同一个优化循环中；能估计完整的6自由度变换，不受平面运动限制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Calib3R是一种创新的统一方法，通过结合3D基础模型和机器人姿态信息，实现了无需校准模式的相机到机器人标定和与机器人参考帧对齐的度量比例3D场景重建，适用于各种机器人平台和相机配置。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots often rely on RGB images for tasks like manipulation and navigation.However, reliable interaction typically requires a 3D scene representation thatis metric-scaled and aligned with the robot reference frame. This depends onaccurate camera-to-robot calibration and dense 3D reconstruction, tasks usuallytreated separately, despite both relying on geometric correspondences from RGBdata. Traditional calibration needs patterns, while RGB-based reconstructionyields geometry with an unknown scale in an arbitrary frame. Multi-camerasetups add further complexity, as data must be expressed in a shared referenceframe. We present Calib3R, a patternless method that jointly performscamera-to-robot calibration and metric-scaled 3D reconstruction via unifiedoptimization. Calib3R handles single- and multi-camera setups on robot arms ormobile robots. It builds on the 3D foundation model MASt3R to extract pointmapsfrom RGB images, which are combined with robot poses to reconstruct a scaled 3Dscene aligned with the robot. Experiments on diverse datasets show that Calib3Rachieves accurate calibration with less than 10 images, outperformingtarget-less and marker-based methods.</description>
      <author>example@mail.com (Davide Allegro, Matteo Terreran, Stefano Ghidoni)</author>
      <guid isPermaLink="false">2509.08813v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals</title>
      <link>http://arxiv.org/abs/2509.08809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的智能体标注范式和CAI比率指标，用于在动态、无监督环境中评估大型语言模型的标注质量，解决了oracle反馈稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型与基于提示的任务结合显著降低了数据标注成本和对人工标注员的依赖。然而，在动态、无监督环境中评估这些模型标注质量具有挑战性，因为oracle反馈稀缺且传统方法失效。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖oracle反馈的新方法，用于评估和改进大型语言模型在动态、无监督环境中的标注质量。&lt;h4&gt;方法&lt;/h4&gt;提出了一种智能体标注范式，其中学生模型与嘈杂的教师模型(即LLM)协作，采用基于用户偏好的多数投票策略评估LLM输出的一致性。同时引入了一致性与不一致性(CAI)比率作为无监督评估指标，系统测量LLM生成标注的可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;CAI比率不仅能在有限用户偏好下量化嘈杂教师的标注质量，还在模型选择中发挥关键作用。在四个LLM和十个开放域NLP数据集上的应用表明，CAI比率与LLM准确性呈强正相关。&lt;h4&gt;结论&lt;/h4&gt;CAI比率是真实世界环境中无监督评估和模型选择的重要工具，能够帮助识别动态、无监督环境中的稳健大型语言模型。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型与基于提示的任务结合时，显著降低了数据标注成本和对人工标注员的依赖。然而，在oracle反馈稀缺且传统方法失效的动态、无监督环境中，评估其标注质量仍然具有挑战性。为应对这一挑战，我们提出了一种新颖的智能体标注范式，其中学生模型与嘈杂的教师模型(即LLM)协作，评估和改进标注质量而不依赖oracle反馈。作为无监督反馈机制的学生模型，采用基于用户偏好的多数投票策略来评估LLM输出的一致性。为了系统测量LLM生成标注的可靠性，我们引入了一致性与不一致性(CAI)比率这一新颖的无监督评估指标。CAI比率不仅能在有限用户偏好下量化嘈杂教师的标注质量，还在模型选择中发挥关键作用，使能够在动态、无监督环境中识别稳健的LLM。在四个LLM和十个开放域NLP数据集上的应用表明，CAI比率与LLM准确性呈强正相关，使其成为真实世界环境中无监督评估和模型选择的重要工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs), when paired with prompt-based tasks, havesignificantly reduced data annotation costs and reliance on human annotators.However, evaluating the quality of their annotations remains challenging indynamic, unsupervised environments where oracle feedback is scarce andconventional methods fail. To address this challenge, we propose a novelagentic annotation paradigm, where a student model collaborates with a noisyteacher (the LLM) to assess and refine annotation quality without relying onoracle feedback. The student model, acting as an unsupervised feedbackmechanism, employs a user preference-based majority voting strategy to evaluatethe consistency of the LLM outputs. To systematically measure the reliabilityof LLM-generated annotations, we introduce the Consistent and Inconsistent(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not onlyquantifies the annotation quality of the noisy teacher under limited userpreferences but also plays a critical role in model selection, enabling theidentification of robust LLMs in dynamic, unsupervised environments. Applied toten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates astrong positive correlation with LLM accuracy, establishing it as an essentialtool for unsupervised evaluation and model selection in real-world settings.</description>
      <author>example@mail.com (Cheng Chen, Haiyan Yin, Ivor Tsang)</author>
      <guid isPermaLink="false">2509.08809v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</title>
      <link>http://arxiv.org/abs/2509.08699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于RGB视觉输入、对象级别的拓扑导航方法，无需3D地图或预训练控制器即可实现零样本、长距离机器人导航。该方法结合全局拓扑路径规划和局部度量轨迹控制，使用单目深度和可 traversability 估计持续预测局部轨迹，并包含自动切换机制。研究使用基础模型确保开放集适用性，在模拟和真实环境中证明了其有效性和优越性。&lt;h4&gt;背景&lt;/h4&gt;传统的机器人视觉导航方法依赖于全局一致的3D地图或学习控制器，这些方法计算成本高且难以在不同环境中泛化。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的RGB-only、对象级别的拓扑导航管道，实现零样本、长距离机器人导航，无需3D地图或预训练控制器，同时确保开放集适用性和领域无关性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合全局拓扑路径规划和局部度量轨迹控制的导航方法。使用单目深度和可 traversability 估计持续预测局部轨迹，并包含自动切换机制，在必要时回退到基础控制器。系统使用基础模型确保开放集适用性，无需领域特定微调。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在模拟环境和真实世界测试中表现出色，证明了其鲁棒性和可部署性。与现有最先进方法相比，该方法在开放环境中的视觉导航提供了更适应和有效的解决方案。&lt;h4&gt;结论&lt;/h4&gt;提出的RGB-only、对象级别拓扑导航方法克服了传统方法的局限性，实现了无需3D地图或预训练控制器的零样本、长距离导航，为机器人视觉导航提供了一种更适应和有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;机器人视觉导航传统上依赖于全局一致的3D地图或学习控制器，这些方法计算成本高且难以在不同环境中泛化。在这项工作中，我们提出了一种新颖的仅使用RGB、基于对象级别的拓扑导航管道，能够在无需3D地图或预训练控制器的情况下实现零样本、长距离机器人导航。我们的方法将全局拓扑路径规划与局部度量轨迹控制相结合，使机器人能够导航至对象级别的子目标，同时避开障碍物。我们通过使用单目深度和可 traversability 估计持续预测局部轨迹，并纳入自动切换机制（必要时回退到基础控制器），解决了先前方法的关键局限性。该系统使用基础模型运行，确保开放集适用性，无需领域特定的微调。我们在模拟环境和真实世界测试中证明了该方法的有效性，突显了其鲁棒性和可部署性。我们的方法优于现有的最先进方法，为开放环境中的视觉导航提供了更适应和有效的解决方案。源代码已公开：https://github.com/podgorki/TANGO。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICRA55743.2025.11127998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual navigation in robotics traditionally relies on globally-consistent 3Dmaps or learned controllers, which can be computationally expensive anddifficult to generalize across diverse environments. In this work, we present anovel RGB-only, object-level topometric navigation pipeline that enableszero-shot, long-horizon robot navigation without requiring 3D maps orpre-trained controllers. Our approach integrates global topological pathplanning with local metric trajectory control, allowing the robot to navigatetowards object-level sub-goals while avoiding obstacles. We address keylimitations of previous methods by continuously predicting local trajectoryusing monocular depth and traversability estimation, and incorporating anauto-switching mechanism that falls back to a baseline controller whennecessary. The system operates using foundational models, ensuring open-setapplicability without the need for domain-specific fine-tuning. We demonstratethe effectiveness of our method in both simulated environments and real-worldtests, highlighting its robustness and deployability. Our approach outperformsexisting state-of-the-art methods, offering a more adaptable and effectivesolution for visual navigation in open-set environments. The source code ismade publicly available: https://github.com/podgorki/TANGO.</description>
      <author>example@mail.com (Stefan Podgorski, Sourav Garg, Mehdi Hosseinzadeh, Lachlan Mares, Feras Dayoub, Ian Reid)</author>
      <guid isPermaLink="false">2509.08699v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts</title>
      <link>http://arxiv.org/abs/2509.08640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 + 8 pages, 4 + 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RoentMod是一种创新的反事实图像编辑框架，能够生成具有特定病理特征的解剖学上真实的胸部X光片，同时保留原始图像的其他解剖特征。研究表明，这种方法可以有效探测和纠正深度学习模型中的捷径学习问题，提高模型的性能和可靠性。&lt;h4&gt;背景&lt;/h4&gt;胸部X光片是医学中最常见的检查之一。自动图像解释可以减少放射科医生的工作量并扩大诊断专业知识的获取渠道。然而，深度学习多任务和基础模型在胸部X光片解释方面表现出色的同时，容易受到'捷径学习'的影响，即模型依赖虚假和非目标相关性而非临床相关特征来做决策。&lt;h4&gt;目的&lt;/h4&gt;引入RoentMod框架，生成具有用户指定的合成病理的解剖学上真实的胸部X光片，同时保留原始扫描中不相关的解剖特征，用于探测和纠正医学AI中的捷径学习问题。&lt;h4&gt;方法&lt;/h4&gt;RoentMod结合了开源医学图像生成器RoentGen和图像到图像的修改模型，无需重新训练。通过获得认证的放射科医生和放射科住院医师的读者研究评估其性能，并使用RoentMod生成的反事实图像进行训练以缓解模型对捷径学习的脆弱性。&lt;h4&gt;主要发现&lt;/h4&gt;RoentMod生成的图像在93%的情况下看起来很真实，在89-99%的情况下正确包含了指定的发现，并保留了与真实随访胸部X光片相当的原始解剖结构。研究显示，最先进的多任务和基础模型经常利用非目标病理作为捷径，限制了它们的特异性。在训练期间纳入RoentMod生成的反事实图像后，在内部验证中使多种病理的模型鉴别能力提高了3-19%的AUC，在外部测试中使6种测试病理中的5种提高了1-11%。&lt;h4&gt;结论&lt;/h4&gt;RoentMod是一个广泛适用的工具，用于探测和纠正医学AI中的捷径学习。通过实现可控的反事实干预，RoentMod增强了胸部X光片解释模型的鲁棒性和可解释性，为改进医学成像中的基础模型提供了可推广的策略。&lt;h4&gt;翻译&lt;/h4&gt;胸部X光片是医学中最常见的检查之一。自动图像解释可以减少放射科医生的工作量并扩大诊断专业知识的获取渠道。深度学习多任务和基础模型在胸部X光片解释方面表现出色，但容易受到捷径学习的影响，即模型依赖虚假和非目标相关性而非临床相关特征来做决策。我们引入了RoentMod，这是一个反事实图像编辑框架，能够生成具有用户指定的合成病理的解剖学上真实的胸部X光片，同时保留原始扫描中不相关的解剖特征。RoentMod结合了一个开源医学图像生成器（RoentGen）和一个图像到图像的修改模型，不需要重新训练。在获得认证的放射科医生和放射科住院医师的读者研究中，RoentMod生成的图像在93%的情况下看起来很真实，在89-99%的情况下正确包含了指定的发现，并保留了与真实随访胸部X光片相当的原始解剖结构。使用RoentMod，我们证明了最先进的多任务和基础模型经常利用非目标病理作为捷径，限制了它们的特异性。在训练期间纳入RoentMod生成的反事实图像可以缓解这种脆弱性，在内部验证中使多种病理的模型鉴别能力提高了3-19%的AUC，在外部测试中使6种测试病理中的5种提高了1-11%。这些研究结果表明RoentMod是一个广泛适用的工具，用于探测和纠正医学AI中的捷径学习。通过实现可控的反事实干预，RoentMod增强了胸部X光片解释模型的鲁棒性和可解释性，为改进医学成像中的基础模型提供了可推广的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chest radiographs (CXRs) are among the most common tests in medicine.Automated image interpretation may reduce radiologists\' workload and expandaccess to diagnostic expertise. Deep learning multi-task and foundation modelshave shown strong performance for CXR interpretation but are vulnerable toshortcut learning, where models rely on spurious and off-target correlationsrather than clinically relevant features to make decisions. We introduceRoentMod, a counterfactual image editing framework that generates anatomicallyrealistic CXRs with user-specified, synthetic pathology while preservingunrelated anatomical features of the original scan. RoentMod combines anopen-source medical image generator (RoentGen) with an image-to-imagemodification model without requiring retraining. In reader studies withboard-certified radiologists and radiology residents, RoentMod-produced imagesappeared realistic in 93\% of cases, correctly incorporated the specifiedfinding in 89-99\% of cases, and preserved native anatomy comparable to realfollow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-taskand foundation models frequently exploit off-target pathology as shortcuts,limiting their specificity. Incorporating RoentMod-generated counterfactualimages during training mitigated this vulnerability, improving modeldiscrimination across multiple pathologies by 3-19\% AUC in internal validationand by 1-11\% for 5 out of 6 tested pathologies in external testing. Thesefindings establish RoentMod as a broadly applicable tool for probing andcorrecting shortcut learning in medical AI. By enabling controlledcounterfactual interventions, RoentMod enhances the robustness andinterpretability of CXR interpretation models and provides a generalizablestrategy for improving foundation models in medical imaging.</description>
      <author>example@mail.com (Lauren H. Cooke, Matthias Jung, Jan M. Brendel, Nora M. Kerkovits, Borek Foldyna, Michael T. Lu, Vineet K. Raghu)</author>
      <guid isPermaLink="false">2509.08640v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>CLAPS: A CLIP-Unified Auto-Prompt Segmentation for Multi-Modal Retinal Imaging</title>
      <link>http://arxiv.org/abs/2509.08618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BIBM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CLIP-unified Auto-Prompt Segmentation (CLAPS)方法，解决了视网膜图像分割中的模态模糊性、手动提示依赖和缺乏统一框架等挑战，实现了跨任务和模态的自动化精确分割。&lt;h4&gt;背景&lt;/h4&gt;基础模型如SAM在医学图像分割特别是视网膜图像分割中取得显著进展，精确分割对诊断至关重要。&lt;h4&gt;目的&lt;/h4&gt;克服当前视网膜图像分割方法面临的三大挑战：模态模糊性、手动提示依赖和缺乏统一框架，实现跨任务和模态的自动化分割。&lt;h4&gt;方法&lt;/h4&gt;CLAPS方法包括：1)在大型多模态视网膜数据集上预训练基于CLIP的图像编码器；2)利用GroundingDINO自动生成空间边界框提示；3)使用带有独特'模态签名'的文本提示统一任务；4)通过自动化提示引导SAM执行精确分割。&lt;h4&gt;主要发现&lt;/h4&gt;在12个不同数据集的11个关键分割类别上进行的实验表明，CLAPS性能与专业专家模型相当，并在大多数指标上超越现有基准，展示了广泛的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CLAPS作为一个统一框架，成功解决了视网膜图像分割中的关键挑战，实现了自动化和跨模态、跨任务的精确分割，作为基础模型具有强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;最近基础模型的进展，如Segment Anything Model(SAM)，显著影响了医学图像分割，特别是在视网膜成像中，精确分割对诊断至关重要。尽管如此，当前方法面临关键挑战：1)文本疾病描述中的模态模糊性，2)继续依赖SAM工作流程的手动提示，3)缺乏统一框架，大多数方法都是模态和任务特定的。为克服这些障碍，我们提出了CLIP-unified Auto-Prompt Segmentation (CLAPS)，一种用于视网膜成像中跨多样化和模态的统一分割的新方法。我们的方法首先在大型多模态视网膜数据集上预训练基于CLIP的图像编码器，以处理数据稀缺和分布不平衡。然后我们利用GroundingDINO通过检测局部病变自动生成空间边界框提示。为了统一任务并解决模糊性，我们使用带有独特'模态签名'的文本提示。最终，这些自动化的文本和空间提示指导SAM执行精确分割，创建完全自动化的统一流程。在11个关键分割类别的12个不同数据集上进行的大量实验表明，CLAPS的性能与专业专家模型相当，并且在大多数指标上超越现有基准，展示了其作为基础模型的广泛泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models, such as the Segment Anything Model(SAM), have significantly impacted medical image segmentation, especially inretinal imaging, where precise segmentation is vital for diagnosis. Despitethis progress, current methods face critical challenges: 1) modality ambiguityin textual disease descriptions, 2) a continued reliance on manual promptingfor SAM-based workflows, and 3) a lack of a unified framework, with mostmethods being modality- and task-specific. To overcome these hurdles, wepropose CLIP-unified Auto-Prompt Segmentation (\CLAPS), a novel method forunified segmentation across diverse tasks and modalities in retinal imaging.Our approach begins by pre-training a CLIP-based image encoder on a large,multi-modal retinal dataset to handle data scarcity and distribution imbalance.We then leverage GroundingDINO to automatically generate spatial bounding boxprompts by detecting local lesions. To unify tasks and resolve ambiguity, weuse text prompts enhanced with a unique "modality signature" for each imagingmodality. Ultimately, these automated textual and spatial prompts guide SAM toexecute precise segmentation, creating a fully automated and unified pipeline.Extensive experiments on 12 diverse datasets across 11 critical segmentationcategories show that CLAPS achieves performance on par with specialized expertmodels while surpassing existing benchmarks across most metrics, demonstratingits broad generalizability as a foundation model.</description>
      <author>example@mail.com (Zhihao Zhao, Yinzheng Zhao, Junjie Yang, Xiangtong Yao, Quanmin Liang, Shahrooz Faghihroohi, Kai Huang, Nassir Navab, M. Ali Nasseri)</author>
      <guid isPermaLink="false">2509.08618v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.08570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages and 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进多模态模型在医疗图像分割领域性能的方法，通过解决文本提示与医学视觉特征之间的语义鸿沟和特征分散问题，显著提升了模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多模态模型在自然图像分割方面表现优异，但在医疗领域应用时性能不佳。研究发现这种性能差距主要源于多模态融合的挑战，特别是抽象文本提示与细粒度医学视觉特征之间的显著语义鸿沟，以及由此导致的特征分散问题。&lt;h4&gt;目的&lt;/h4&gt;从语义聚合的角度重新审视医疗图像分割问题，设计有效机制弥合语义鸿沟并缓解特征分散，提高多模态模型在医疗领域的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出两种关键机制：1) 期望最大化（EM）聚合机制，通过动态将特征聚类为紧凑的语义中心来缓解特征分散，增强跨模态对应；2) 文本引导的像素解码器，利用领域不变的文本知识弥合语义鸿沟，有效指导深度视觉表示。这两种机制的协同作用显著提升了模型的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在公共心脏和眼底数据集上的大量实验表明，所提出的方法在多个领域泛化基准上持续优于现有的最先进方法，证明了其在医疗图像分割领域的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过EM聚合机制和文本引导的像素解码器的协同作用，成功解决了医疗图像分割中的多模态融合挑战，显著提高了模型的性能和泛化能力，为医疗图像分割领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态模型在自然图像分割方面取得了显著成功，但应用于医疗领域时往往表现不佳。通过广泛研究，我们将这种性能差距归因于多模态融合的挑战，主要是抽象文本提示与细粒度医学视觉特征之间的显著语义鸿沟，以及由此导致的特征分散问题。为解决这些问题，我们从语义聚合的角度重新审视问题。具体而言，我们提出了期望最大化（EM）聚合机制和文本引导的像素解码器。前者通过动态将特征聚类为紧凑的语义中心来缓解特征分散，增强跨模态对应。后者旨在利用领域不变的文本知识弥合语义鸿沟，有效指导深度视觉表示。这两种机制之间的协同作用显著提高了模型的泛化能力。在公共心脏和眼底数据集上的大量实验表明，我们的方法在多个领域泛化基准上持续优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal models have achieved remarkable success in natural imagesegmentation, yet they often underperform when applied to the medical domain.Through extensive study, we attribute this performance gap to the challenges ofmultimodal fusion, primarily the significant semantic gap between abstracttextual prompts and fine-grained medical visual features, as well as theresulting feature dispersion. To address these issues, we revisit the problemfrom the perspective of semantic aggregation. Specifically, we propose anExpectation-Maximization (EM) Aggregation mechanism and a Text-Guided PixelDecoder. The former mitigates feature dispersion by dynamically clusteringfeatures into compact semantic centers to enhance cross-modal correspondence.The latter is designed to bridge the semantic gap by leveragingdomain-invariant textual knowledge to effectively guide deep visualrepresentations. The synergy between these two mechanisms significantlyimproves the model's generalization ability. Extensive experiments on publiccardiac and fundus datasets demonstrate that our method consistentlyoutperforms existing SOTA approaches across multiple domain generalizationbenchmarks.</description>
      <author>example@mail.com (Wenjun Yu, Yinchen Zhou, Jia-Xuan Jiang, Shubin Zeng, Yuee Li, Zhong Wang)</author>
      <guid isPermaLink="false">2509.08570v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning</title>
      <link>http://arxiv.org/abs/2509.08519v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了HuMo，一个用于协作多模态控制的人为中心视频生成统一框架，解决了现有方法在协调多模态输入方面的挑战，包括数据稀缺和子任务协作困难。&lt;h4&gt;背景&lt;/h4&gt;现有的人为中心视频生成方法在协调文本、图像和音频等多模态输入时面临两大挑战：缺乏具有配对三元条件的训练数据，以及难以协调主体保持和音视频同步这两个子任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的HCVG框架，有效解决多模态输入的协调问题，实现高质量的人视频合成。&lt;h4&gt;方法&lt;/h4&gt;1. 构建高质量数据集，包含多样且配对的文本、参考图像和音频；2. 提出两阶段渐进式多模态训练范式；3. 主体保持任务采用最小侵入性图像注入策略；4. 音视频同步任务结合音频交叉注意力层和预测焦点策略；5. 推理阶段使用时间自适应的无分类器引导策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，HuMo在子任务中超越了专门的最先进方法，证明了其作为协作多模态条件HCVG统一框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;HuMo成功解决了HCVG领域中的多模态协调挑战，为高质量人视频生成提供了一个统一的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;以人为中心的视频生成方法旨在从多模态输入（包括文本、图像和音频）合成人类视频。由于两个挑战，现有方法难以有效协调这些异构模态：缺乏具有配对三元条件的训练数据，以及难以协调多模态输入下的主体保持和音视频同步子任务。在这项工作中，我们提出了HuMo，一个用于协作多模态控制的统一HCVG框架。针对第一个挑战，我们构建了一个高质量的数据集，包含多样且配对的文本、参考图像和音频。针对第二个挑战，我们提出了一种两阶段渐进式多模态训练范式，带有任务特定策略。对于主体保持任务，为了保持基础模型的提示跟随和视觉生成能力，我们采用最小侵入性图像注入策略。对于音视频同步任务，除了常用的音频交叉注意力层外，我们还提出了一种预测焦点策略，隐式引导模型将音频与面部区域关联。为了实现跨多模态输入的可控性联合学习，在已获取能力的基础上，我们逐步融入音视频同步任务。在推理阶段，为了灵活和细粒度的多模态控制，我们设计了一种时间自适应的无分类器引导策略，在去噪步骤中动态调整引导权重。大量实验结果表明，HuMo在子任务中超越了专门的最先进方法，建立了协作多模态条件HCVG的统一框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human-Centric Video Generation (HCVG) methods seek to synthesize human videosfrom multimodal inputs, including text, image, and audio. Existing methodsstruggle to effectively coordinate these heterogeneous modalities due to twochallenges: the scarcity of training data with paired triplet conditions andthe difficulty of collaborating the sub-tasks of subject preservation andaudio-visual sync with multimodal inputs. In this work, we present HuMo, aunified HCVG framework for collaborative multimodal control. For the firstchallenge, we construct a high-quality dataset with diverse and paired text,reference images, and audio. For the second challenge, we propose a two-stageprogressive multimodal training paradigm with task-specific strategies. For thesubject preservation task, to maintain the prompt following and visualgeneration abilities of the foundation model, we adopt the minimal-invasiveimage injection strategy. For the audio-visual sync task, besides the commonlyadopted audio cross-attention layer, we propose a focus-by-predicting strategythat implicitly guides the model to associate audio with facial regions. Forjoint learning of controllabilities across multimodal inputs, building onpreviously acquired capabilities, we progressively incorporate the audio-visualsync task. During inference, for flexible and fine-grained multimodal control,we design a time-adaptive Classifier-Free Guidance strategy that dynamicallyadjusts guidance weights across denoising steps. Extensive experimental resultsdemonstrate that HuMo surpasses specialized state-of-the-art methods insub-tasks, establishing a unified framework for collaborativemultimodal-conditioned HCVG. Project Page:https://phantom-video.github.io/HuMo.</description>
      <author>example@mail.com (Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, Zhiyong Wu)</author>
      <guid isPermaLink="false">2509.08519v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2509.08401v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对图基础模型中的两个关键问题（模型退化和表示崩溃）提出MoT解决方案，通过信息修补和正则化修补显著提高了模型在多领域任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;图基础模型受大型语言模型成功启发，旨在从多领域图(TAGs)中学习最优嵌入以实现跨任务泛化。图VQ-MAE因其能将多领域拓扑和文本属性编码到具有清晰语义边界的离散嵌入空间而表现突出。&lt;h4&gt;目的&lt;/h4&gt;解决图基础模型中由领域泛化冲突导致的模型退化和表示崩溃问题，提高模型在预训练过程中的优化效果和下游任务性能。&lt;h4&gt;方法&lt;/h4&gt;提出MoT(Mixture-of-Tinkers)解决方案，包含：(1)信息修补：利用边缘语义融合策略和域感知路由的混合代码本提高信息容量；(2)正则化修补：使用两个额外正则化改进梯度监督。MoT作为灵活架构遵循图基础模型扩展规律，提供可控制模型规模。&lt;h4&gt;主要发现&lt;/h4&gt;图基础模型存在两个相互关联的问题：模型退化（编码器和代码本无法捕获输入多样性）和表示崩溃（隐藏嵌入和代码本向量无法保持语义可分性），这些问题共同导致解码器生成低质量重建监督，引发预训练优化困境。这些问题归因于信息瓶颈和正则化不足。&lt;h4&gt;结论&lt;/h4&gt;MoT在6个领域的22个数据集上实验表明，其在监督、少样本和零样本场景中显著优于现有最先进方法，为图基础模型优化提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型，受大型语言模型成功的启发，旨在从多领域图(TAGs)中学习最优嵌入，以实现下游跨任务泛化能力。在我们的研究中，图VQ-MAE在日益多样化的图基础模型架构中脱颖而出。这归功于其能够将多领域的拓扑和文本属性联合编码到具有清晰语义边界的离散嵌入空间的能力。尽管其潜力巨大，领域泛化冲突导致了难以察觉的陷阱。在本文中，我们实例化了其中的两个陷阱，它们就像同一枚图基础模型优化硬币的两面 - 第一面：模型退化：编码器和代码本无法捕获输入的多样性；第二面：表示崩溃：由于来自窄表示子空间的约束，隐藏嵌入和代码本向量无法保持语义可分性。这两个陷阱（面）共同损害了解码器并生成低质量的重建监督，导致预训练过程中的图基础模型优化困境（硬币）。通过实证研究，我们将上述挑战归因于信息瓶颈和正则化不足。为解决这些问题，我们提出了MoT(Mixture-of-Tinkers)-(1)针对两个问题的信息修补，利用边缘语义融合策略和具有域感知路由的混合代码本来提高信息容量。(2)针对优化硬币的正则化修补，使用两个额外的正则化来进一步改进我们提出的信息修补中的梯度监督。值得注意的是，作为一种灵活的架构，MoT遵循图基础模型的扩展规律，提供可控制的模型规模。与现有最先进基线相比，在6个领域的22个数据集上的实验表明，MoT在监督、少样本和零样本场景中实现了显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph foundation models, inspired by the success of LLMs, are designed tolearn the optimal embedding from multi-domain TAGs for the downstreamcross-task generalization capability. During our investigation, graph VQ-MAEstands out among the increasingly diverse landscape of GFM architectures. Thisis attributed to its ability to jointly encode topology and textual attributesfrom multiple domains into discrete embedding spaces with clear semanticboundaries. Despite its potential, domain generalization conflicts causeimperceptible pitfalls. In this paper, we instantiate two of them, and they arejust like two sides of the same GFM optimization coin - Side 1 ModelDegradation: The encoder and codebook fail to capture the diversity of inputs;Side 2 Representation Collapse: The hidden embedding and codebook vector failto preserve semantic separability due to constraints from narrow representationsubspaces. These two pitfalls (sides) collectively impair the decoder andgenerate the low-quality reconstructed supervision, causing the GFMoptimization dilemma during pre-training (coin). Through empiricalinvestigation, we attribute the above challenges to Information Bottleneck andRegularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semanticfusion strategy and a mixture-of-codebooks with domain-aware routing to improveinformation capacity. (2) Regularization Tinker for Optimization Coin, whichutilizes two additional regularizations to further improve gradient supervisionin our proposed Information Tinker. Notably, as a flexible architecture, MoTadheres to the scaling laws of GFM, offering a controllable model scale.Compared to SOTA baselines, experiments on 22 datasets across 6 domainsdemonstrate that MoT achieves significant improvements in supervised, few-shot,and zero-shot scenarios.</description>
      <author>example@mail.com (Xunkai Li, Daohan Su, Sicheng Liu, Ru Zhang, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2509.08401v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2509.08372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE ICIP 2025 Satellite Workshop 1: Edge  Intelligence: Smart, Efficient, and Scalable Solutions for IoT, Wearables,  and Embedded Devices (SEEDS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了类不平衡联邦无源域适应(CI-FFREEDA)问题，提出使用冻结的视觉基础模型(VFM)替换FFREEDA主干网络，以提高整体准确性并减少计算和通信成本。&lt;h4&gt;背景&lt;/h4&gt;联邦学习(FL)是一种在保护数据隐私的同时协作训练模型的框架。最近的研究集中在联邦无源域适应(FFREEDA)上，其中客户端持有的目标域数据保持未标记状态，服务器只能在预训练期间访问源域数据。&lt;h4&gt;目的&lt;/h4&gt;将FFREEDA框架扩展到更复杂的类不平衡联邦无源域适应(CI-FFREEDA)场景，考虑源域和目标域中的类不平衡问题，以及源域和目标域之间以及目标客户端之间的标签偏移。&lt;h4&gt;方法&lt;/h4&gt;提出用冻结的视觉基础模型(VFM)替换FFREEDA主干网络，改进网络内部的特征提取器，而不是增强聚合和域适应方法，从而在不进行大量参数调整的情况下提高整体准确性，并减少联邦学习中的计算和通信成本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，VFM有效缓解了域差距、类不平衡甚至目标客户端之间的非IID性问题。&lt;h4&gt;结论&lt;/h4&gt;强大的特征提取器，而不是复杂的适应或FL方法，是现实世界中联邦学习成功的关键。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)提供了一种在保护每个客户端数据隐私的同时协作训练模型的框架。最近，研究集中在联邦无源域适应(FFREEDA)上，这是一个更现实的场景，其中客户端持有的目标域数据保持未标记状态，服务器只能在预训练期间访问源域数据。我们将这个框架扩展到一个更复杂和现实的设置：类不平衡联邦无源域适应(CI-FFREEDA)，它考虑了源域和目标域中的类不平衡问题，以及源域和目标域之间以及目标客户端之间的标签偏移。在我们的实验设置中复制现有方法使我们重新思考重点，从增强聚合和域适应方法转向改进网络本身的特征提取器。我们提出用冻结的视觉基础模型(VFM)替换FFREEDA主干网络，从而在不进行大量参数调整的情况下提高整体准确性，并减少联邦学习中的计算和通信成本。我们的实验结果表明，VFM有效缓解了域差距、类不平衡甚至目标客户端之间的非IID性问题，这表明强大的特征提取器，而不是复杂的适应或FL方法，是现实世界中FL成功的关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) offers a framework for training modelscollaboratively while preserving data privacy of each client. Recently,research has focused on Federated Source-Free Domain Adaptation (FFREEDA), amore realistic scenario wherein client-held target domain data remainsunlabeled, and the server can access source domain data only duringpre-training. We extend this framework to a more complex and realistic setting:Class Imbalanced FFREEDA (CI-FFREEDA), which takes into account classimbalances in both the source and target domains, as well as label shiftsbetween source and target and among target clients. The replication of existingmethods in our experimental setup lead us to rethink the focus from enhancingaggregation and domain adaptation methods to improving the feature extractorswithin the network itself. We propose replacing the FFREEDA backbone with afrozen vision foundation model (VFM), thereby improving overall accuracywithout extensive parameter tuning and reducing computational and communicationcosts in federated learning. Our experimental results demonstrate that VFMseffectively mitigate the effects of domain gaps, class imbalances, and evennon-IID-ness among target clients, suggesting that strong feature extractors,not complex adaptation or FL methods, are key to success in the real-world FL.</description>
      <author>example@mail.com (Kosuke Kihara, Junki Mori, Taiki Miyagawa, Akinori F. Ebihara)</author>
      <guid isPermaLink="false">2509.08372v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia</title>
      <link>http://arxiv.org/abs/2509.08303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究创建了一个印度尼西亚油棕种植园和相关土地覆盖类型的开放获取地理空间数据集，通过专家对2020-2024年高分辨率卫星图像进行标注，支持可持续发展和森林砍伐监测。&lt;h4&gt;背景&lt;/h4&gt;油棕种植是印度尼西亚森林砍伐的主要原因之一，需要详细可靠的地图来跟踪和解决这一问题。&lt;h4&gt;目的&lt;/h4&gt;创建一个详细的油棕种植地图，支持可持续发展努力和新兴监管框架，提高土地覆盖类型绘图的准确性。&lt;h4&gt;方法&lt;/h4&gt;通过专家对2020年至2024年高分辨率卫星图像进行标注，创建基于多边形的全面标注数据集，使用多解释者共识和实地验证确保质量，采用大网格全面数字化方法，适合训练传统卷积神经网络和新型地理空间基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集填补了遥感训练数据的关键空白，提高了土地覆盖类型绘图的准确性，支持油棕扩张的透明监测。&lt;h4&gt;结论&lt;/h4&gt;这一资源通过支持油棕扩张的透明监测，有助于实现全球减少森林砍伐的目标，并遵循FAIR数据原则。&lt;h4&gt;翻译&lt;/h4&gt;油棕种植仍然是印度尼西亚森林砍伐的主要原因之一。为了更好地跟踪和解决这一挑战，需要详细可靠的地图来支持可持续发展努力和新兴监管框架。我们提出了一个印度尼西亚油棕种植园和相关土地覆盖类型的开放获取地理空间数据集，通过专家对2020年至2024年高分辨率卫星图像进行标注制作而成。该数据集提供了基于多边形的全面标注，涵盖各种农业生态区，并包含分层分类法，区分油棕种植阶段以及类似的多年生作物。通过多解释者共识和实地验证确保质量。该数据集使用大网格的全面数字化创建，适合训练和测试传统卷积神经网络以及新型地理空间基础模型。在CC-BY许可下发布，它填补了遥感训练数据的关键空白，旨在提高土地覆盖类型绘图的准确性。通过支持油棕扩张的透明监测，这一资源有助于实现全球减少森林砍伐的目标，并遵循FAIR数据原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Oil palm cultivation remains one of the leading causes of deforestation inIndonesia. To better track and address this challenge, detailed and reliablemapping is needed to support sustainability efforts and emerging regulatoryframeworks. We present an open-access geospatial dataset of oil palmplantations and related land cover types in Indonesia, produced through expertlabeling of high-resolution satellite imagery from 2020 to 2024. The datasetprovides polygon-based, wall-to-wall annotations across a range ofagro-ecological zones and includes a hierarchical typology that distinguishesoil palm planting stages as well as similar perennial crops. Quality wasensured through multi-interpreter consensus and field validation. The datasetwas created using wall-to-wall digitization over large grids, making itsuitable for training and benchmarking both conventional convolutional neuralnetworks and newer geospatial foundation models. Released under a CC-BYlicense, it fills a key gap in training data for remote sensing and aims toimprove the accuracy of land cover types mapping. By supporting transparentmonitoring of oil palm expansion, the resource contributes to globaldeforestation reduction goals and follows FAIR data principles.</description>
      <author>example@mail.com (M. Warizmi Wafiq, Peter Cutter, Ate Poortinga, Daniel Marc G. dela Torre, Karis Tenneson, Vanna Teck, Enikoe Bihari, Chanarun Saisaward, Weraphong Suaruang, Andrea McMahon, Andi Vika Faradiba Muin, Karno B. Batiran, Chairil A, Nurul Qomar, Arya Arismaya Metananda, David Ganz, David Saah)</author>
      <guid isPermaLink="false">2509.08303v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery</title>
      <link>http://arxiv.org/abs/2509.08032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SciGPT（科学领域专用基础模型）和ScienceBench（科学LLMs评估基准），解决了科学文献增长带来的知识综合挑战，以及通用LLMs在科学领域的局限性。&lt;h4&gt;背景&lt;/h4&gt;科学文献呈指数级增长，研究人员难以高效综合知识。通用大型语言模型在文本处理方面有潜力，但往往无法捕捉科学领域的特定细节（如技术术语、方法严谨性），难以处理复杂的科学任务，限制了其在跨学科研究中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对科学文献理解的基础模型和相应的评估基准，以解决通用LLMs在科学领域的不足，促进AI增强的科学发现。&lt;h4&gt;方法&lt;/h4&gt;SciGPT基于Qwen3架构构建，包含三个关键创新：(1)通过两阶段流程进行低成本领域蒸馏，平衡性能和效率；(2)稀疏专家混合注意力机制，可将32,000个长文档推理的内存消耗减少55%；(3)知识感知适应，整合领域本体论以弥合跨学科知识差距。&lt;h4&gt;主要发现&lt;/h4&gt;在ScienceBench上的实验结果表明，SciGPT在序列标注、生成和推理等核心科学任务上优于GPT-4o。它在未见过的科学任务中也表现出强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;SciGPT具有促进AI增强科学发现的潜力，能够有效处理科学文献并支持跨学科研究。&lt;h4&gt;翻译&lt;/h4&gt;科学文献呈指数级增长，为研究人员高效综合知识造成了关键瓶颈。虽然通用大型语言模型在文本处理方面显示出潜力，但它们往往无法捕捉科学领域的特定细微差别（如技术术语、方法严谨性），并且在处理复杂的科学任务时遇到困难，限制了它们在跨学科研究中的实用性。为解决这些不足，本文提出了SciGPT（一个针对科学文献理解的基础模型）和ScienceBench（一个专门用于评估科学LLMs的开源基准）。SciGPT基于Qwen3架构构建，包含三个关键创新：(1)通过两阶段流程进行低成本领域蒸馏，平衡性能和效率；(2)稀疏专家混合注意力机制，可将32,000个长文档推理的内存消耗减少55%；(3)知识感知适应，整合领域本体论以弥合跨学科知识差距。在ScienceBench上的实验结果表明，SciGPT在序列标注、生成和推理等核心科学任务上优于GPT-4o。它在未见过的科学任务中也表现出强大的鲁棒性，验证了其促进AI增强科学发现的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific literature is growing exponentially, creating a criticalbottleneck for researchers to efficiently synthesize knowledge. Whilegeneral-purpose Large Language Models (LLMs) show potential in text processing,they often fail to capture scientific domain-specific nuances (e.g., technicaljargon, methodological rigor) and struggle with complex scientific tasks,limiting their utility for interdisciplinary research. To address these gaps,this paper presents SciGPT, a domain-adapted foundation model for scientificliterature understanding and ScienceBench, an open source benchmark tailored toevaluate scientific LLMs.  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:(1) low-cost domain distillation via a two-stage pipeline to balanceperformance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attentionmechanism that cuts memory consumption by 55\% for 32,000-token long-documentreasoning; and (3) knowledge-aware adaptation integrating domain ontologies tobridge interdisciplinary knowledge gaps.  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o incore scientific tasks including sequence labeling, generation, and inference.It also exhibits strong robustness in unseen scientific tasks, validating itspotential to facilitate AI-augmented scientific discovery.</description>
      <author>example@mail.com (Fengyu She, Nan Wang, Hongfei Wu, Ziyi Wan, Jingmian Wang, Chang Wang)</author>
      <guid isPermaLink="false">2509.08032v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery</title>
      <link>http://arxiv.org/abs/2509.08027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个名为MCTED的新数据集，专门用于火星数字高程模型预测任务，包含80,898个处理后的样本，并开源了相关代码。研究团队还比较了专门训练的小型U-Net模型与通用深度估计基础模型在该任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;火星勘测轨道器使用CTX仪器收集了高分辨率的火星正射影像和数字高程模型数据，但原始数据中存在伪影和缺失数据点问题，影响了大规模DEM处理的质量。&lt;h4&gt;目的&lt;/h4&gt;创建一个适用于机器学习应用的火星数字高程模型预测数据集，解决原始数据中的质量问题，并提供一个基准来评估专门训练模型与通用深度估计模型的性能差异。&lt;h4&gt;方法&lt;/h4&gt;1. 使用全面管道处理高分辨率火星正射影像和DEM对；2. 开发工具解决原始数据中的伪影和缺失数据点问题；3. 将处理后的样本分为不重叠的训练集和验证集；4. 每个样本包含光学图像块、DEM块和两个掩码块；5. 提供数据集的统计分析；6. 训练小型U-Net架构并与DepthAnythingV2模型比较&lt;h4&gt;主要发现&lt;/h4&gt;专门在MCTED数据集上训练的小型U-Net模型在火星高程预测任务上的性能优于通用深度估计基础模型DepthAnythingV2的零样本性能。&lt;h4&gt;结论&lt;/h4&gt;MCTED数据集为火星数字高程模型预测任务提供了高质量、多样化的训练资源，专门训练的模型即使在规模较小的情况下也能超越通用深度估计模型的表现。数据集和代码已完全开源，供研究社区使用。&lt;h4&gt;翻译&lt;/h4&gt;这项工作为火星数字高程模型预测任务提出了一个名为MCTED的新数据集，适用于机器学习应用。该数据集是使用一个全面的管道生成的，用于处理来自Day等人提供的高分辨率火星正射影像和DEM对，最终得到包含80,898个数据样本的数据集。源图像是由火星勘测轨道器使用CTX仪器收集的数据，提供了火星表面非常多样和全面的覆盖。鉴于大规模DEM处理管道的复杂性，原始数据中经常存在伪影和缺失数据点，为此开发了工具来解决或减轻其影响。我们将处理后的样本分为训练集和验证集，确保两个集合中的样本不覆盖相互区域，避免数据泄露。数据集中的每个样本由光学图像块、DEM块和两个掩码块表示，指示原始缺失或被修改的值。我们提供了生成数据集的统计见解，包括样本的空间分布、高程值、坡度等的分布。最后，我们在MCTED数据集上训练了一个小型U-Net架构，并将其在高程预测任务上的性能与单目深度估计基础模型DepthAnythingV2进行了比较。我们发现即使是一个专门在此数据集上训练的非常小的架构，也能击败像DepthAnythingV2这样的深度估计基础模型的零样本性能。我们将数据集及其生成代码完全开源在公共仓库中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决火星高分辨率数字高程模型(DEM)生成缺乏专用机器学习数据集的问题。这个问题很重要，因为DEM对火星科学研究和探索任务至关重要，包括着陆点映射、古代水文过程建模等。传统DEM生成方法资源密集，需要立体图像对，且延迟时间长，而能从单张图像生成DEM的方法可以解决这些缺点，但目前缺乏适合此任务的公开数据集。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有火星高程图数据集的局限性，发现它们存在碎片化、空值多、噪声大等问题。他们注意到CTX仪器获取的图像具有全球覆盖和高分辨率特性，适合单目深度估计。作者借鉴了NASA Ames立体管道生成的DEM数据，但针对其质量问题开发了专门的处理管道。他们还参考了现有深度估计数据集，但指出这些数据集主要包含地面视角，而MCTED提供了纯粹空中视角的数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个机器学习就绪的数据集，使研究人员能够训练模型从单张火星图像生成DEM。整体流程包括：1)从Day等人的仓库获取原始数据；2)数据质量控制(样本选择、初步地图填充、垂直化)；3)处理缺失值和异常值；4)数据分块和选择；5)确保训练和验证集之间无数据泄漏；6)提供数据集统计分析；7)训练基线模型并与现有模型比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建MCTED数据集，包含80,8个清理策划的图像-DEM对；2)开发全面的数据处理管道解决原始数据质量问题；3)提供数据集的详细统计分析；4)展示小型U-Net架构能超越最先进的MDE基础模型；5)完全开源数据集和代码。相比之前工作，MCTED专注于单图像DEM生成，解决了数据集碎片化问题，提供纯空中视角数据，并证明专门训练的小模型能优于通用的大模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MCTED数据集填补了深度估计和遥感数据集之间的空白，通过创建高质量的策划数据和专门的处理管道，使研究人员能够从单张火星图像生成更准确的数字高程模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents a new dataset for the Martian digital elevation modelprediction task, ready for machine learning applications called MCTED. Thedataset has been generated using a comprehensive pipeline designed to processhigh-resolution Mars orthoimage and DEM pairs from Day et al., yielding adataset consisting of 80,898 data samples. The source images are data gatheredby the Mars Reconnaissance Orbiter using the CTX instrument, providing a verydiverse and comprehensive coverage of the Martian surface. Given the complexityof the processing pipelines used in large-scale DEMs, there are often artefactsand missing data points in the original data, for which we developed tools tosolve or mitigate their impact. We divide the processed samples into trainingand validation splits, ensuring samples in both splits cover no mutual areas toavoid data leakage. Every sample in the dataset is represented by the opticalimage patch, DEM patch, and two mask patches, indicating values that wereoriginally missing or were altered by us. This allows future users of thedataset to handle altered elevation regions as they please. We providestatistical insights of the generated dataset, including the spatialdistribution of samples, the distributions of elevation values, slopes andmore. Finally, we train a small U-Net architecture on the MCTED dataset andcompare its performance to a monocular depth estimation foundation model,DepthAnythingV2, on the task of elevation prediction. We find that even a verysmall architecture trained on this dataset specifically, beats a zero-shotperformance of a depth estimation foundation model like DepthAnythingV2. Wemake the dataset and code used for its generation completely open source inpublic repositories.</description>
      <author>example@mail.com (Rafał Osadnik, Pablo Gómez, Eleni Bohacek, Rickbir Bahia)</author>
      <guid isPermaLink="false">2509.08027v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Towards Post-mortem Data Management Principles for Generative AI</title>
      <link>http://arxiv.org/abs/2509.07375v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了逝者数据所有权这一较少被关注的维度，分析了当前身后数据管理和隐私权利的现状，提出了三条保护逝者数据权利的管理原则，并为政策制定者和隐私从业者提供了实践建议。&lt;h4&gt;背景&lt;/h4&gt;基础模型、大型语言模型和AI代理系统严重依赖大量用户数据，这引发了关于所有权、版权和潜在损害的持续担忧。&lt;h4&gt;目的&lt;/h4&gt;探索逝者数据所有权这一相关但较少被研究的维度，并提出保护逝者数据权利的原则。&lt;h4&gt;方法&lt;/h4&gt;检查当前身后数据管理和隐私权利的现状，分析主要科技公司的隐私政策和欧盟AI法案等法规。&lt;h4&gt;主要发现&lt;/h4&gt;提出了三条身后数据管理原则，以指导保护逝者数据权利。&lt;h4&gt;结论&lt;/h4&gt;讨论了未来工作方向，并为政策制定者和隐私从业者提供了关于如何部署这些原则，以及如何通过技术和解决方案在实践中落实和审计它们的建议。&lt;h4&gt;翻译&lt;/h4&gt;基础模型、大型语言模型和AI代理系统严重依赖大量用户数据。使用此类数据进行训练引发了关于所有权、版权和潜在损害的持续担忧。在本工作中，我们探索了一个相关但较少被研究的维度：逝者数据的所有权权利。我们检查了当前身后数据管理和隐私权利的现状，这由主要科技公司的隐私政策和欧盟AI法案等法规定义。基于此分析，我们提出了三条身后数据管理原则，以指导保护逝者数据权利。最后，我们讨论了未来工作方向，并为政策制定者和隐私从业者提供了建议，关于如何部署这些原则以及如何通过技术和解决方案在实践中落实和审计它们。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, large language models (LLMs), and agentic AI systems relyheavily on vast corpora of user data. The use of such data for training hasraised persistent concerns around ownership, copyright, and potential harms. Inthis work, we explore a related but less examined dimension: the ownershiprights of data belonging to deceased individuals. We examine the currentlandscape of post-mortem data management and privacy rights as defined by theprivacy policies of major technology companies and regulations such as the EUAI Act. Based on this analysis, we propose three post-mortem data managementprinciples to guide the protection of deceased individuals data rights.Finally, we discuss directions for future work and offer recommendations forpolicymakers and privacy practitioners on deploying these principles alongsidetechnological solutions to operationalize and audit them in practice.</description>
      <author>example@mail.com (Elina Van Kempen, Ismat Jarin, Chloe Georgiou)</author>
      <guid isPermaLink="false">2509.07375v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication</title>
      <link>http://arxiv.org/abs/2509.09597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图对齐框架，解决了现有无监督方法中的两个关键局限性：节点区分度下降和潜在空间不对齐问题。该方法通过双通道编码器和几何感知功能映射模块，显著提升了图对齐的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;图对齐是识别多个图中对应节点的基础问题，对许多应用至关重要。大多数现有无监督方法将节点特征嵌入到潜在表示中，以实现无需真实对应关系的跨图比较。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图对齐框架，同时增强节点区分度并强制执行潜在空间之间的几何一致性，解决现有方法的两个关键局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 双通道编码器：结合低通和高通谱滤波器生成既具有结构感知又高度区分性的嵌入；2) 几何感知功能映射模块：学习图嵌入之间的双射和等距变换，确保不同表示之间一致的几何关系。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在图基准测试上始终优于现有的无监督对齐基线；2) 对结构不一致性和具有挑战性的对齐场景表现出更强的鲁棒性；3) 在视觉-语言表示的无监督对齐中有效泛化到图域之外。&lt;h4&gt;结论&lt;/h4&gt;所提出的图对齐框架通过增强节点区分度和强制执行潜在空间之间的几何一致性，有效解决了现有无监督方法的局限性，在各种应用场景中表现出优越的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;图对齐-识别多个图中对应节点的问题-是许多应用的基础问题。大多数现有的无监督方法将节点特征嵌入到潜在表示中，以实现无需真实对应关系的跨图比较。然而，这些方法存在两个关键局限性：由于基于GNN的嵌入中的过平滑导致的节点区分度下降，以及由于结构噪声、特征异质性和训练不稳定性导致的潜在空间不对齐，最终导致不可靠的节点对应关系。我们提出了一种新的图对齐框架，同时增强节点区分度并强制执行潜在空间之间的几何一致性。我们的方法引入了一种双通道编码器，结合低通和高通谱滤波器来生成既具有结构感知又高度区分性的嵌入。为了解决潜在空间不对齐问题，我们纳入了一个感知几何的功能映射模块，该模块学习图嵌入之间的双射和等距变换，确保不同表示之间一致的几何关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph alignment-the problem of identifying corresponding nodes acrossmultiple graphs-is fundamental to numerous applications. Most existingunsupervised methods embed node features into latent representations to enablecross-graph comparison without ground-truth correspondences. However, thesemethods suffer from two critical limitations: the degradation of nodedistinctiveness due to oversmoothing in GNN-based embeddings, and themisalignment of latent spaces across graphs caused by structural noise, featureheterogeneity, and training instability, ultimately leading to unreliable nodecorrespondences. We propose a novel graph alignment framework thatsimultaneously enhances node distinctiveness and enforces geometric consistencyacross latent spaces. Our approach introduces a dual-pass encoder that combineslow-pass and high-pass spectral filters to generate embeddings that are bothstructure-aware and highly discriminative. To address latent spacemisalignment, we incorporate a geometry-aware functional map module that learnsbijective and isometric transformations between graph embeddings, ensuringconsistent geometric relationships across different representations. Extensiveexperiments on graph benchmarks demonstrate that our method consistentlyoutperforms existing unsupervised alignment baselines, exhibiting superiorrobustness to structural inconsistencies and challenging alignment scenarios.Additionally, comprehensive evaluation on vision-language benchmarks usingdiverse pretrained models shows that our framework effectively generalizesbeyond graph domains, enabling unsupervised alignment of vision and languagerepresentations.</description>
      <author>example@mail.com (Maysam Behmanesh, Erkan Turan, Maks Ovsjanikov)</author>
      <guid isPermaLink="false">2509.09597v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2509.09522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了语义文本相关性在职位标题匹配中的应用，提出了一种结合密集句子嵌入和领域特定知识图谱的自监督混合架构，通过分层评估方法发现知识图谱增强的模型在高语义相关性区域表现显著提升。&lt;h4&gt;背景&lt;/h4&gt;语义文本相关性(STR)能够捕捉文本间超越表面词汇相似性的细微关系。在简历推荐系统中，职位标题匹配是一个关键挑战，其中重叠术语往往有限或具有误导性。&lt;h4&gt;目的&lt;/h4&gt;研究STR在职位标题匹配中的应用，提高语义对齐和可解释性，并通过分层评估方法更细致地分析模型在不同语义相关性区域的表现。&lt;h4&gt;方法&lt;/h4&gt;引入一种自监督混合架构，结合密集句子嵌入与领域特定知识图谱(KGs)；将STR分数连续体划分为低、中、高三个语义相关性区域进行分层评估；评估多种嵌入模型，包括和不通过图神经网络整合KGs的模型。&lt;h4&gt;主要发现&lt;/h4&gt;使用知识图谱增强的微调SBERT模型在高STR区域表现显著提升，RMSE比强基线模型降低25%；结合KGs与文本嵌入的方法有益；区域性能分析能揭示全局指标隐藏的优势和弱点，支持更针对性的模型选择。&lt;h4&gt;结论&lt;/h4&gt;结合知识图谱与文本嵌入的方法能有效提升语义文本相关性任务的表现，特别是在高语义相关性区域；区域性能分析对于理解模型行为和选择适合特定应用场景的模型至关重要。&lt;h4&gt;翻译&lt;/h4&gt;语义文本相关性(STR)捕捉了超越表面词汇相似性的文本间细微关系。在本研究中，我们在职位标题匹配的背景下探讨STR——这是简历推荐系统中的一个关键挑战，其中重叠术语通常有限或具有误导性。我们引入了一种自监督混合架构，结合密集句子嵌入和领域特定的知识图谱(KGs)，以提高语义对齐和可解释性。与之前在整体性能上评估模型的研究不同，我们的方法通过将STR分数连续体划分为不同的区域：低、中、高语义相关性，强调数据分层。这种分层评估能够在语义上有意义的子空间中对模型性能进行细粒度分析。我们评估了多种嵌入模型，包括通过图神经网络整合和不整合KGs的模型。结果显示，使用KGs增强的微调SBERT模型在高STR区域产生了一致的改进，RMSE比强基线模型降低了25%。我们的发现不仅突出了结合KGs与文本嵌入的好处，还强调了区域性能分析在理解模型行为中的重要性。这种细粒度方法揭示了全局指标隐藏的优势和弱点，并支持在人力资源(HR)系统和应用中进行更有针对性的模型选择，这些应用中公平性、可解释性和上下文匹配至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic Textual Relatedness (STR) captures nuanced relationships betweentexts that extend beyond superficial lexical similarity. In this study, weinvestigate STR in the context of job title matching - a key challenge inresume recommendation systems, where overlapping terms are often limited ormisleading. We introduce a self-supervised hybrid architecture that combinesdense sentence embeddings with domain-specific Knowledge Graphs (KGs) toimprove both semantic alignment and explainability. Unlike previous work thatevaluated models on aggregate performance, our approach emphasizes datastratification by partitioning the STR score continuum into distinct regions:low, medium, and high semantic relatedness. This stratified evaluation enablesa fine-grained analysis of model performance across semantically meaningfulsubspaces. We evaluate several embedding models, both with and without KGintegration via graph neural networks. The results show that fine-tuned SBERTmodels augmented with KGs produce consistent improvements in the high-STRregion, where the RMSE is reduced by 25% over strong baselines. Our findingshighlight not only the benefits of combining KGs with text embeddings, but alsothe importance of regional performance analysis in understanding modelbehavior. This granular approach reveals strengths and weaknesses hidden byglobal metrics, and supports more targeted model selection for use in HumanResources (HR) systems and applications where fairness, explainability, andcontextual matching are essential.</description>
      <author>example@mail.com (Vadim Zadykian, Bruno Andrade, Haithem Afli)</author>
      <guid isPermaLink="false">2509.09522v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Database Views as Explanations for Relational Deep Learning</title>
      <link>http://arxiv.org/abs/2509.09482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的框架，用于解释关系数据库上的机器学习模型，特别是基于异构图神经网络(hetero-GNNs)的架构。该框架通过视图定义生成解释，突出显示对模型预测贡献最大的数据库部分。&lt;h4&gt;背景&lt;/h4&gt;近年来，在关系数据库上开发深度学习模型取得了显著进展，包括基于异构图神经网络和异构Transformer的架构。这些架构将数据库记录和链接转化为包含大量可学习参数的复杂数值表达式，导致模型难以解释。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够以人类可理解的方式解释关系数据库上机器学习模型如何做出预测的框架，特别关注异构GNNs模型。&lt;h4&gt;方法&lt;/h4&gt;通过适应Nash、Segoufin和Vianu提出的确定性概念建立全局abduction解释；开发避免穷举搜索的启发式算法；提出模型无关技术和针对异构GNNs的可学习掩码技术；允许调整确定性与简洁性之间的权衡并控制解释粒度。&lt;h4&gt;主要发现&lt;/h4&gt;在RelBench集合上的广泛实证研究表明，所提出的解释是有用的，且它们的生成过程是高效的。该框架能够突出显示对模型预测贡献最大的数据库部分，包括整个列、表间外键和相关元组组等。&lt;h4&gt;结论&lt;/h4&gt;该框架有效地解决了关系数据库上深度学习模型的可解释性问题，特别是对于异构GNNs模型，提供了既可解释又高效的解释方法。&lt;h4&gt;翻译&lt;/h4&gt;近年来，在关系数据库上开发深度学习模型取得了显著进展，包括基于异构图神经网络和异构Transformer的架构。实际上，这些架构描述了数据库记录和链接（如外键引用）如何转化为包含大量可学习参数的大型复杂数值表达式。这种复杂性使得很难用人类可理解的方式解释模型如何利用可用数据得出特定预测。我们提出了一种新颖的框架，用于解释关系数据库上的机器学习模型，其中解释是视图定义，突出显示对模型预测贡献最大的数据库部分。我们通过适应Nash、Segoufin和Vianu（2010）提出的确定性经典概念来建立这种全局abduction解释。除了调整确定性与简洁性之间的权衡外，该框架还允许通过采用不同的视图定义片段（如突出显示整个列、表之间的外键、相关元组组等）来控制粒度级别。我们研究了该框架在异构GNNs情况下的实现。我们开发了避免对所有数据库空间进行穷举搜索的启发式算法。我们提出了模型无关的技术，以及通过可学习掩码概念专门针对异构GNNs定制的技术。我们通过在RelBench集合上进行广泛的实证研究来评估该方法，涵盖各种领域和不同的记录级任务。结果表明所提出的解释是有用的，并且它们的生成是高效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been significant progress in the development ofdeep learning models over relational databases, including architectures basedon heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graphtransformers. In effect, such architectures state how the database records andlinks (e.g., foreign-key references) translate into a large, complex numericalexpression, involving numerous learnable parameters. This complexity makes ithard to explain, in human-understandable terms, how a model uses the availabledata to arrive at a given prediction. We present a novel framework forexplaining machine-learning models over relational databases, whereexplanations are view definitions that highlight focused parts of the databasethat mostly contribute to the model's prediction. We establish such globalabductive explanations by adapting the classic notion of determinacy by Nash,Segoufin, and Vianu (2010). In addition to tuning the tradeoff betweendeterminacy and conciseness, the framework allows controlling the level ofgranularity by adopting different fragments of view definitions, such as oneshighlighting whole columns, foreign keys between tables, relevant groups oftuples, and so on. We investigate the realization of the framework in the caseof hetero-GNNs. We develop heuristic algorithms that avoid the exhaustivesearch over the space of all databases. We propose techniques that aremodel-agnostic, and others that are tailored to hetero-GNNs via the notion oflearnable masking. Our approach is evaluated through an extensive empiricalstudy on the RelBench collection, covering a variety of domains and differentrecord-level tasks. The results demonstrate the usefulness of the proposedexplanations, as well as the efficiency of their generation.</description>
      <author>example@mail.com (Agapi Rissaki, Ilias Fountalis, Wolfgang Gatterbauer, Benny Kimelfeld)</author>
      <guid isPermaLink="false">2509.09482v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement</title>
      <link>http://arxiv.org/abs/2509.09219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍并评估了Vejde框架，该框架结合数据抽象、图神经网络和强化学习，用于为具有丰富结构化状态的决策问题生成归纳策略函数。&lt;h4&gt;背景&lt;/h4&gt;在处理具有丰富结构化状态（如对象类别和关系）的决策问题时，需要有效的方法来表示和处理这些复杂结构。MDP状态被表示为关于实体的事实数据库，需要能够泛化到不同大小和结构问题的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理具有丰富结构化状态的决策问题的框架，生成能够在不同大小和结构问题上泛化的归纳策略函数。&lt;h4&gt;方法&lt;/h4&gt;Vejde框架结合三种技术：1)数据抽象将MDP状态表示为事实数据库；2)图神经网络将状态转换为二部图并通过神经消息传递映射到潜在状态；3)强化学习训练策略函数。状态和动作的因子化表示使代理能够处理不同大小和结构的问题。&lt;h4&gt;主要发现&lt;/h4&gt;在八个RDDL问题域（每个域十个实例）上测试显示，Vejde策略平均能够泛化到未见实例而不会显著降低分数，其表现接近于特定于实例的MLP代理。&lt;h4&gt;结论&lt;/h4&gt;Vejde框架成功结合了数据抽象、图神经网络和强化学习，为具有丰富结构化状态的决策问题提供了有效的归纳策略函数，能够在未见实例上表现良好。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍并评估了Vejde；一个结合数据抽象、图神经网络和强化学习的框架，用于为具有丰富结构化状态（如对象类别和关系）的决策问题生成归纳策略函数。MDP状态被表示为关于实体的事实数据库，Vejde将每个状态转换为二部图，并通过神经消息传递映射到潜在状态。状态和动作的因子化表示使Vejde代理能够处理不同大小和结构的问题。我们在RDDL定义的八个问题域上测试了Vejde代理，每个域有十个问题实例，其中策略使用监督学习和强化学习进行训练。为测试策略泛化，我们将问题实例分为两个集合，一个用于训练，另一个仅用于测试。Vejde代理在未见实例上的测试结果与在每个问题实例上训练的MLP代理以及在线规划算法Prost进行了比较。我们的结果表明，Vejde策略平均能够泛化到测试实例而不会显著降低分数。此外，归纳代理在未见测试实例上的平均分数接近于特定于实例的MLP代理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present and evaluate Vejde; a framework which combines data abstraction,graph neural networks and reinforcement learning to produce inductive policyfunctions for decision problems with richly structured states, such as objectclasses and relations. MDP states are represented as data bases of facts aboutentities, and Vejde converts each state to a bipartite graph, which is mappedto latent states through neural message passing. The factored representation ofboth states and actions allows Vejde agents to handle problems of varying sizeand structure. We tested Vejde agents on eight problem domains defined in RDDL,with ten problem instances each, where policies were trained using bothsupervised and reinforcement learning. To test policy generalization, weseparate problem instances in two sets, one for training and the other solelyfor testing. Test results on unseen instances for the Vejde agents werecompared to MLP agents trained on each problem instance, as well as the onlineplanning algorithm Prost. Our results show that Vejde policies in averagegeneralize to the test instances without a significant loss in score.Additionally, the inductive agents received scores on unseen test instancesthat on average were close to the instance-specific MLP agents.</description>
      <author>example@mail.com (Jakob Nyberg, Pontus Johnson)</author>
      <guid isPermaLink="false">2509.09219v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>CryptGNN: Enabling Secure Inference for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.09107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CryptGNN是一种安全有效的云端第三方图神经网络模型推理解决方案，通过分布式安全多方计算技术保护数据隐私和模型安全。&lt;h4&gt;背景&lt;/h4&gt;第三方图神经网络模型通过机器学习即服务(MLaaS)在云端被客户使用，存在数据隐私和模型安全风险。&lt;h4&gt;目的&lt;/h4&gt;开发一种保护客户输入数据、图结构和模型参数安全的云端GNN推理解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用分布式安全多方计算(SMPC)技术实现安全消息传递和特征转换层，支持任意数量的SMPC方协作，无需可信服务器。&lt;h4&gt;主要发现&lt;/h4&gt;即使云中P个方中有P-1方合谋，CryptGNN也能保证安全性，理论分析和实验验证了其安全性和效率。&lt;h4&gt;结论&lt;/h4&gt;CryptGNN为云端第三方GNN模型推理提供了安全有效的解决方案，解决了数据隐私和模型安全问题。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了CryptGNN，一种用于云端第三方图神经网络(GNN)模型的安全有效推理解决方案，客户通过机器学习即服务(MLaaS)访问这些模型。CryptGNN的主要创新点是使用分布式安全多方计算(SMPC)技术实现安全消息传递和特征转换层。CryptGNN保护客户的输入数据和图结构，防止云服务提供商和第三方模型所有者获取；同时保护模型参数，防止云服务提供商和客户获取。CryptGNN可与任意数量的SMPC方协作，不需要可信服务器，并且即使云中P个方中有P-1方合谋，也能证明其安全性。理论分析和实证实验证明了CryptGNN的安全性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present CryptGNN, a secure and effective inference solution forthird-party graph neural network (GNN) models in the cloud, which are accessedby clients as ML as a service (MLaaS). The main novelty of CryptGNN is itssecure message passing and feature transformation layers using distributedsecure multi-party computation (SMPC) techniques. CryptGNN protects theclient's input data and graph structure from the cloud provider and thethird-party model owner, and it protects the model parameters from the cloudprovider and the clients. CryptGNN works with any number of SMPC parties, doesnot require a trusted server, and is provably secure even if P-1 out of Pparties in the cloud collude. Theoretical analysis and empirical experimentsdemonstrate the security and efficiency of CryptGNN.</description>
      <author>example@mail.com (Pritam Sen, Yao Ma, Cristian Borcea)</author>
      <guid isPermaLink="false">2509.09107v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent and Independent Data under Preferential Sampling</title>
      <link>http://arxiv.org/abs/2509.09336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一个新的时空模型，用于整合渔业独立数据(FID)和渔业依赖数据(FDD)，解决了生态数据中常见的零膨胀和优先抽样问题，并在欧洲沙丁鱼分布研究中得到验证，为渔业管理提供了实用见解。&lt;h4&gt;背景&lt;/h4&gt;海洋生态系统的可持续管理对维持健康渔业资源至关重要，需要准确评估物种分布模式。渔业科学中主要使用两种数据源：通过系统调查收集的渔业独立数据(FID)和从商业捕捞获得的渔业依赖数据(FDD)。虽然两者互补，但不同的抽样方案（系统抽样vs优先抽样）带来了整合挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的时空模型，有效整合FID和FDD数据，解决生态数据中常见的零膨胀和优先抽样(PS)问题，从而准确评估物种分布模式，为渔业管理提供科学依据。&lt;h4&gt;方法&lt;/h4&gt;研究采用六层结构的时空模型，区分存在-不存在数据和生物量观测，处理优先抽样偏差问题。通过模拟测试模型在不同PS场景下的参数估计准确性和优先信号检测能力，并将模型应用于葡萄牙大陆架南部欧洲沙丁鱼种群分布研究，整合了多种数据源及环境与船舶特定协变量。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果表明模型能在不同优先抽样场景下准确估计参数并检测优先信号。在沙丁鱼研究中，模型成功整合了不同数据源，揭示了沙丁鱼存在和生物量的时空变异性，为渔业管理提供了可操作的见解。&lt;h4&gt;结论&lt;/h4&gt;该时空模型为整合渔业独立数据和渔业依赖数据提供了有效框架，能够处理生态数据中的零膨胀和优先抽样问题，不仅在生态学研究中具有应用价值，还可广泛应用于其他学科的数据整合挑战。&lt;h4&gt;翻译&lt;/h4&gt;海洋生态系统的可持续管理对于维持健康的渔业资源至关重要，并受益于先进的科学工具来准确评估物种分布模式。在渔业科学中，使用两种主要数据源：通过系统调查收集的渔业独立数据(FID)，以及从商业捕捞活动中获得的渔业依赖数据(FDD)。虽然这些数据源提供互补信息，但它们不同的抽样方案——FID的系统抽样和FDD的优先抽样——带来了重大的整合挑战。本研究引入了一种新的时空模型，整合FID和FDD，解决了生态数据中常见的与零膨胀和优先抽样(PS)相关的挑战。该模型采用六层结构来区分存在-不存在数据和生物量观测，为受优先抽样偏差影响的生态学研究提供了稳健的框架。模拟结果证明了该模型在不同优先抽样场景下参数估计的准确性及其检测优先信号的能力。应用于研究葡萄牙大陆架南部欧洲沙丁鱼种群分布模式的案例，说明了该模型在整合不同数据源和纳入环境及船舶特定协变量方面的有效性。该模型揭示了沙丁鱼存在和生物量的时空变异性，为渔业管理提供了可操作的见解。除了生态学应用外，该框架在解决其他学科的数据整合挑战方面具有广泛的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sustainable management of marine ecosystems is vital for maintaining healthyfishery resources, and benefits from advanced scientific tools to accuratelyassess species distribution patterns. In fisheries science, two primary datasources are used: fishery-independent data (FID), collected through systematicsurveys, and fishery-dependent data (FDD), obtained from commercial fishingactivities. While these sources provide complementary information, theirdistinct sampling schemes - systematic for FID and preferential for FDD - posesignificant integration challenges. This study introduces a novelspatio-temporal model that integrates FID and FDD, addressing challengesassociated with zero-inflation and preferential sampling (PS) common inecological data. The model employs a six-layer structure to differentiatebetween presence-absence and biomass observations, offering a robust frameworkfor ecological studies affected by PS biases. Simulation results demonstratethe model's accuracy in parameter estimation across diverse PS scenarios andits ability to detect preferential signals. Application to the study of thedistribution patterns of the European sardine populations along the southernPortuguese continental shelf illustrates the model's effectiveness inintegrating diverse data sources and incorporating environmental andvessel-specific covariates. The model reveals spatio-temporal variability insardine presence and biomass, providing actionable insights for fisheriesmanagement. Beyond ecology, this framework offers broad applicability to dataintegration challenges in other disciplines.</description>
      <author>example@mail.com (Daniela Silva, Raquel Menezes, Gonçalo Araújo, Ana Machado, Renato Rosa, Ana Moreno, Alexandra Silva, Susana Garrido)</author>
      <guid isPermaLink="false">2509.09336v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>DATE: Dynamic Absolute Time Enhancement for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2509.09263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了动态绝对时间增强(DATE)方法，通过时间戳注入机制(TIM)和语义引导的时间感知相似度采样(TASS)策略，有效提高了多模态大语言模型对长时间视频的理解能力，特别是在绝对时间理解和关键事件定位方面取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;长视频理解对多模态大语言模型(MLLMs)仍然是一个基本挑战，特别是在需要精确时间推理和事件定位的任务中。现有方法通常采用统一帧采样和隐式位置编码来建模时间顺序，但这些方法难以处理长距离依赖关系，导致关键信息丢失和时间理解能力下降。&lt;h4&gt;目的&lt;/h4&gt;提高MLLMs的时间感知能力，改善对长时间视频的理解和事件定位性能，解决现有方法在处理长视频时的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出动态绝对时间增强(DATE)方法，包括：1)时间戳注入机制(TIM)，将视频帧嵌入与文本时间戳令交错，构建连续时间参考系统；2)语义引导的时间感知相似度采样(TASS)策略，将视频采样问题重新表述为视觉-语言检索任务，并引入两阶段算法确保语义相关性和时间覆盖。&lt;h4&gt;主要发现&lt;/h4&gt;在绝对时间理解和关键事件定位方面取得了显著改进，在7B和72B模型的小时级视频基准测试中达到了最先进的性能。特别值得注意的是，7B模型在某些基准测试中甚至超过了许多72B模型的表现。&lt;h4&gt;结论&lt;/h4&gt;DATE方法有效解决了长视频理解中的时间感知问题，为提高多模态大语言模型对长时间视频的理解能力提供了新的有效解决方案，在保持模型规模较小的情况下也能获得优异性能。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解对多模态大语言模型(MLLMs)来说仍然是一个基本挑战，特别是在需要精确时间推理和事件定位的任务中。现有方法通常采用统一帧采样和依赖隐式位置编码来建模时间顺序。然而，这些方法难以处理长距离依赖关系，导致关键信息丢失和时间理解能力下降。在本文中，我们提出了动态绝对时间增强(DATE)，通过时间戳注入机制(TIM)和语义引导的时间感知相似度采样(TASS)策略来增强MLLMs的时间感知能力。具体而言，我们将视频帧嵌入与文本时间戳令交错，构建连续时间参考系统。我们进一步将视频采样问题重新表述为视觉-语言检索任务，并引入两阶段算法确保语义相关性和时间覆盖：将每个查询丰富为描述性字幕以更好地与视觉特征对齐，并采用相似度驱动的时间正则化贪婪策略采样关键事件。我们的方法在绝对时间理解和关键事件定位方面取得了显著改进，在7B和72B模型的小时级视频基准测试中达到了最先进的性能。特别是，我们的7B模型在某些基准测试中甚至超过了许多72B模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long video understanding remains a fundamental challenge for multimodal largelanguage models (MLLMs), particularly in tasks requiring precise temporalreasoning and event localization. Existing approaches typically adopt uniformframe sampling and rely on implicit position encodings to model temporal order.However, these methods struggle with long-range dependencies, leading tocritical information loss and degraded temporal comprehension. In this paper,we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporalawareness in MLLMs through the Timestamp Injection Mechanism (TIM) and asemantically guided Temporal-Aware Similarity Sampling (TASS) strategy.Specifically, we interleave video frame embeddings with textual timestamptokens to construct a continuous temporal reference system. We furtherreformulate the video sampling problem as a vision-language retrieval task andintroduce a two-stage algorithm to ensure both semantic relevance and temporalcoverage: enriching each query into a descriptive caption to better align withthe vision feature, and sampling key event with a similarity-driven temporallyregularized greedy strategy. Our method achieves remarkable improvements w.r.t.absolute time understanding and key event localization, resulting instate-of-the-art performance among 7B and 72B models on hour-long videobenchmarks. Particularly, our 7B model even exceeds many 72B models on somebenchmarks.</description>
      <author>example@mail.com (Chao Yuan, Yang Yang, Yehui Yang, Zach Cheng)</author>
      <guid isPermaLink="false">2509.09263v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models</title>
      <link>http://arxiv.org/abs/2509.08538v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了MESH，一个专门用于系统评估大型视频模型(LVMs)中幻觉问题的新基准。研究指出，尽管LVMs在理解视频内容方面有所进步，但它们容易产生不准确或无关的描述。MESH基准采用问答框架，从基础到复杂评估LVMs的能力，并与人类理解视频的方式保持一致。&lt;h4&gt;背景&lt;/h4&gt;大型视频模型(LVMs)建立在大型语言模型(LLMs)和视觉模块的语义能力基础上，通过整合时间信息来更好地理解动态视频内容。然而，现有的视频幻觉评估基准严重依赖对视频内容的手动分类，忽略了人类自然解释视频时的感知过程。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一个系统化的基准来评估LVMs中的幻觉问题，该基准能够反映人类理解视频的自然过程，并全面检测模型在视频描述中的不准确或无关内容。&lt;h4&gt;方法&lt;/h4&gt;MESH基准采用问答框架，包含二进制和多选格式，并融入目标实例和陷阱实例。它采用自下而上的方法，评估基础物体、从粗到细的主体特征以及主体-动作对，与人类对视频的理解过程保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，虽然LVMs在识别基础物体和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，特别是在较长的视频中，它们产生幻觉的倾向显著增加。&lt;h4&gt;结论&lt;/h4&gt;MESH为识别视频中的幻觉问题提供了一种有效且全面的方法，能够系统化地评估LVMs在不同复杂度视频内容上的表现，并揭示其在处理更复杂视频场景时的局限性。&lt;h4&gt;翻译&lt;/h4&gt;大型视频模型(LVMs)通过整合时间信息，建立在大型语言模型(LLMs)和视觉模块的语义能力基础上，以更好地理解动态视频内容。尽管取得了进展，LVMs仍然容易产生幻觉——产生不准确或无关的描述。当前的视频幻觉评估基准严重依赖对视频内容的手动分类，忽略了人类自然解释视频时的感知过程。我们引入了MESH，一个旨在系统评估LVMs中幻觉问题的基准。MESH采用问答框架，包含二进制和多选格式，并融入目标实例和陷阱实例。它遵循自下而上的方法，评估基础物体、从粗到细的主体特征以及主体-动作对，与人类对视频的理解保持一致。我们证明MESH为识别视频中的幻觉问题提供了一种有效且全面的方法。我们的评估显示，尽管LVMs在识别基础物体和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，特别是在较长的视频中，它们产生幻觉的倾向显著增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Video Models (LVMs) build on the semantic capabilities of LargeLanguage Models (LLMs) and vision modules by integrating temporal informationto better understand dynamic video content. Despite their progress, LVMs areprone to hallucinations-producing inaccurate or irrelevant descriptions.Current benchmarks for video hallucination depend heavily on manualcategorization of video content, neglecting the perception-based processesthrough which humans naturally interpret videos. We introduce MESH, a benchmarkdesigned to evaluate hallucinations in LVMs systematically. MESH uses aQuestion-Answering framework with binary and multi-choice formats incorporatingtarget and trap instances. It follows a bottom-up approach, evaluating basicobjects, coarse-to-fine subject features, and subject-action pairs, aligningwith human video understanding. We demonstrate that MESH offers an effectiveand comprehensive approach for identifying hallucinations in videos. Ourevaluations show that while LVMs excel at recognizing basic objects andfeatures, their susceptibility to hallucinations increases markedly whenhandling fine details or aligning multiple actions involving various subjectsin longer videos.</description>
      <author>example@mail.com (Garry Yang, Zizhe Chen, Man Hon Wong, Haoyu Lei, Yongqiang Chen, Zhenguo Li, Kaiwen Zhou, James Cheng)</author>
      <guid isPermaLink="false">2509.08538v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting</title>
      <link>http://arxiv.org/abs/2509.09210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为ProgD的渐进式多尺度解码策略，结合动态异构图场景建模，用于解决自动驾驶车辆中多代理运动预测的挑战，特别关注处理代理间交互的演变性质，有效降低了预测不确定性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆的安全规划依赖于对周围代理的准确运动预测。最近的进展已将预测技术从单个代理扩展到多个交互代理的联合预测，但现有方法忽略了这些交互的演变性质。&lt;h4&gt;目的&lt;/h4&gt;解决现有多代理运动预测方法忽略交互演变性质的问题，提出一种能够捕捉动态交互并降低预测不确定性的新方法。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种名为ProgD的渐进式多尺度解码策略，结合动态异构图场景建模。具体包括：1)使用动态异构图对场景进行渐进建模，捕捉未来场景中演变的社会交互；2)设计分解架构处理时空依赖关系，逐步消除多代理未来运动的不确定性；3)集成多尺度解码过程，改进场景建模和运动预测的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的ProgD在INTERACTION多代理预测基准测试中取得了最先进的性能，排名第一，并在Argoverse 2多世界预测基准测试中表现优异。&lt;h4&gt;结论&lt;/h4&gt;通过结合动态异构图场景建模和多尺度解码过程，ProgD有效解决了多代理运动预测中交互演变性质被忽略的问题，显著提高了预测准确性，为自动驾驶车辆的安全规划提供了更可靠的决策依据。&lt;h4&gt;翻译&lt;/h4&gt;准确的周围代理运动预测对自动驾驶车辆的安全规划至关重要。最近的进展已经将预测技术从单个代理扩展到多个交互代理的联合预测，采用了各种策略来处理代理未来运动中的复杂交互。然而，这些方法忽略了这些交互的演变性质。为了解决这一局限性，我们提出了一种新颖的渐进式多尺度解码策略，称为ProgD，借助动态异构图场景建模。特别是，为了明确和全面地捕捉未来场景中演变的社会交互（考虑到其固有的不确定性），我们设计了使用动态异构图的场景渐进建模。随着这些动态异构图的展开，设计了一个分解架构来处理未来场景中的时空依赖关系，并逐步消除多个代理未来运动的不确定性。此外，还集成了多尺度解码过程，以改进未来场景建模和代理未来运动的一致性预测。所提出的ProgD在INTERACTION多代理预测基准测试中取得了最先进的性能，排名第一，并在Argoverse 2多世界预测基准测试中表现优异。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多代理联合运动预测问题，特别是在自动驾驶场景中准确预测多个交互代理（如车辆、行人）的未来运动轨迹。这个问题在现实中至关重要，因为不准确的预测可能导致代理之间的碰撞或冲突轨迹，威胁自动驾驶系统的安全性和可靠性，影响实际交通效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，即它们主要关注观察到的交互而忽略了未来交互的动态演化。基于这一洞察，作者设计了ProgD方法，利用动态异构图来显式建模未来场景的演化。该方法借鉴了图神经网络和Transformer架构处理时空依赖，异构图表示不同类型交互，以及编码器-解码器框架等现有技术，但创新性地将这些技术与动态图建模和多尺度解码相结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用动态异构图显式建模未来交互的演化，并通过多尺度解码策略逐步消除不确定性。整体流程包括：1) 场景编码：使用静态异构图编码观察到的代理历史状态和道路网络；2) 联合运动预测：通过时间模块捕获时间依赖，动态构建异构图表示未来交互，采用多尺度解码（粗略预测→快照更新→联合预测）逐步细化预测；3) 多模态预测：生成多个可能的联合预测，每个预测内部保持一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 渐进式多尺度解码策略，通过动态异构图显式建模未来交互的演化；2) 动态异构图建模，图结构随时间动态更新反映交互变化；3) 多模态一致性预测，确保多个代理预测之间的一致性。相比之前的工作，ProgD不再使用静态图而是动态图来处理未来交互，不再直接预测完整轨迹而是采用多尺度解码，并且更注重预测的一致性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ProgD通过动态异构图和多尺度解码策略实现了对多个交互代理未来运动的准确、一致预测，在INTERACTION和Argoverse 2等基准测试中达到了最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate motion prediction of surrounding agents is crucial for the safeplanning of autonomous vehicles. Recent advancements have extended predictiontechniques from individual agents to joint predictions of multiple interactingagents, with various strategies to address complex interactions within futuremotions of agents. However, these methods overlook the evolving nature of theseinteractions. To address this limitation, we propose a novel progressivemulti-scale decoding strategy, termed ProgD, with the help of dynamicheterogeneous graph-based scenario modeling. In particular, to explicitly andcomprehensively capture the evolving social interactions in future scenarios,given their inherent uncertainty, we design a progressive modeling of scenarioswith dynamic heterogeneous graphs. With the unfolding of such dynamicheterogeneous graphs, a factorized architecture is designed to process thespatio-temporal dependencies within future scenarios and progressivelyeliminate uncertainty in future motions of multiple agents. Furthermore, amulti-scale decoding procedure is incorporated to improve on the futurescenario modeling and consistent prediction of agents' future motion. Theproposed ProgD achieves state-of-the-art performance on the INTERACTIONmulti-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2multi-world forecasting benchmark.</description>
      <author>example@mail.com (Xing Gao, Zherui Huang, Weiyao Lin, Xiao Sun)</author>
      <guid isPermaLink="false">2509.09210v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>ObjectReact: Learning Object-Relative Control for Visual Navigation</title>
      <link>http://arxiv.org/abs/2509.09594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CoRL 2025; 23 pages including appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了一种基于物体的视觉导航新范式，通过使用'物体相对'控制而非传统的'图像相对'控制，解决了现有方法在姿态依赖和泛化能力方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;视觉导航通常需要额外的传感器和3D地图。目前主流方法是'图像相对'的方法，通过当前观察图像和子目标图像对来估计控制。然而，图像表示存在局限性，因为它们严格依赖于机器人的姿态和形态。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的'物体相对'控制学习范式，解决图像级表示的局限性，实现跨形态部署的高不变性，并使控制预测问题与图像匹配问题解耦。&lt;h4&gt;方法&lt;/h4&gt;提出一种'相对'3D场景图形式的拓扑地图表示，用于获取更有信息量的物体级全局路径规划成本。训练一个名为'ObjectReact'的局部控制器，直接基于高级'WayObject Costmap'表示进行条件化，不需要显式的RGB输入。&lt;h4&gt;主要发现&lt;/h4&gt;'物体相对'控制相比'图像相对'控制在传感器高度变化和多个导航任务中表现更好；能够处理反向导航等挑战空间理解能力的任务；仅在模拟环境中训练的策略能够很好地泛化到真实世界的室内环境。&lt;h4&gt;结论&lt;/h4&gt;物体作为地图的属性，提供了与形态和轨迹无关的世界表示；新方法能够实现新路径的遍历，无需严格模仿先验经验；在跨形态部署中具有高不变性。&lt;h4&gt;翻译&lt;/h4&gt;仅使用单个相机和拓扑地图的视觉导航最近已成为需要额外传感器和3D地图方法的吸引人的替代方案。这通常通过一种'图像相对'的方法来实现，从给定的当前观察图像和子目标图像对中估计控制。然而，世界的图像级表示存在局限性，因为图像严格绑定到机器人的姿态和形态。相比之下，物体作为地图的属性，提供了一种与形态和轨迹无关的世界表示。在这项工作中，我们提出了学习'物体相对'控制的新范式，展现出几个理想特性：a) 可以遍历新路径而无需严格模仿先验经验，b) 控制预测问题可以与图像匹配问题解耦，c) 在训练-测试和映射-执行环境变化的跨形态部署中可以实现高不变性。我们提出了一种'相对'3D场景图形式的拓扑地图表示，用于获取更有信息量的物体级全局路径规划成本。我们训练了一个名为'ObjectReact'的局部控制器，直接基于高级'WayObject Costmap'表示进行条件化，消除了对显式RGB输入的需求。我们展示了在传感器高度变化和多个导航任务中，学习物体相对控制相比其图像相对对手的优势，这些任务挑战了底层空间理解能力，例如在地图轨迹中反向导航。我们进一步表明，我们的仅模拟策略能够很好地泛化到真实世界的室内环境。代码和补充材料可通过项目页面访问：https://object-react.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉导航中基于图像的方法的局限性，特别是它们对机器人姿态和形态的依赖问题。这个问题很重要，因为视觉导航仅使用单个相机和拓扑图已成为一种有吸引力的替代方案，不需要额外的传感器和3D地图，但传统图像级表示与机器人姿态紧密绑定，限制了其在不同条件下的鲁棒性和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有图像相对方法的局限性，提出使用物体作为地图属性提供与形态无关的世界表示。他们借鉴了RoboHop的对象级拓扑映射，但改进了其2D连通性，使用更信息丰富的相对3D连通性；同时借鉴了SAM等基础模型进行物体分割，以及SuperPoint和LightGlue进行特征匹配；在控制器设计上参考了GNM架构但进行了修改以适应物体相对控制范式。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用物体级别的连接性而非图像级别来表示世界，提出物体相对控制范式，直接基于当前图像中可见的物体子目标进行条件化。整体流程分为三阶段：1)映射阶段：构建相对3D场景图，提取物体节点并建立图像内外的连接；2)执行阶段：进行物体定位和全局路径规划，生成WayObject Costmap；3)训练阶段：训练ObjectReact控制器，基于WayObject Costmap预测轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：物体相对控制新范式、基于相对3D场景图的地图表示、WayObject Costmap表示方法、ObjectReact控制器设计。相比之前的工作，不同之处在于：不依赖机器人姿态和形态；使用3D而非2D连通性；将控制学习与图像匹配问题解耦；在跨形态部署方面表现出色；能够处理传统图像相对方法难以解决的导航任务如反向导航和捷径寻找。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种物体相对控制的视觉导航新范式，通过基于物体级别的世界表示和WayObject Costmap控制器，实现了对机器人姿态和环境变化的鲁棒性，并能解决传统图像相对方法难以处理的复杂导航任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual navigation using only a single camera and a topological map hasrecently become an appealing alternative to methods that require additionalsensors and 3D maps. This is typically achieved through an "image-relative"approach to estimating control from a given pair of current observation andsubgoal image. However, image-level representations of the world havelimitations because images are strictly tied to the agent's pose andembodiment. In contrast, objects, being a property of the map, offer anembodiment- and trajectory-invariant world representation. In this work, wepresent a new paradigm of learning "object-relative" control that exhibitsseveral desirable characteristics: a) new routes can be traversed withoutstrictly requiring to imitate prior experience, b) the control predictionproblem can be decoupled from solving the image matching problem, and c) highinvariance can be achieved in cross-embodiment deployment for variations acrossboth training-testing and mapping-execution settings. We propose a topometricmap representation in the form of a "relative" 3D scene graph, which is used toobtain more informative object-level global path planning costs. We train alocal controller, dubbed "ObjectReact", conditioned directly on a high-level"WayObject Costmap" representation that eliminates the need for an explicit RGBinput. We demonstrate the advantages of learning object-relative control overits image-relative counterpart across sensor height variations and multiplenavigation tasks that challenge the underlying spatial understandingcapability, e.g., navigating a map trajectory in the reverse direction. Wefurther show that our sim-only policy is able to generalize well to real-worldindoor environments. Code and supplementary material are accessible via projectpage: https://object-react.github.io/</description>
      <author>example@mail.com (Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid)</author>
      <guid isPermaLink="false">2509.09594v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Robix: A Unified Model for Robot Interaction, Reasoning and Planning</title>
      <link>http://arxiv.org/abs/2509.01106v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech report. Project page: https://robix-seed.github.io/robix/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Robix是一个统一的模型，集成了机器人推理、任务规划和自然语言交互，采用视觉-语言架构。作为分层机器人系统的高级认知层，Robix动态生成低级控制器的原子命令和人类交互的口头响应，使机器人能够遵循复杂指令、规划长期任务并在端到端框架内与人类自然交互。&lt;h4&gt;背景&lt;/h4&gt;机器人系统需要能够理解人类指令、规划任务并进行自然交互的能力。现有的系统可能缺乏统一架构来整合这些能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的模型，整合机器人推理、任务规划和自然语言交互，使机器人能够遵循复杂指令、规划长期任务并与人类自然交互。&lt;h4&gt;方法&lt;/h4&gt;采用链式思维推理和三阶段训练策略：(1)持续预训练以增强基础具身推理能力，包括3D空间理解、视觉定位和任务中心推理；(2)监督微调将人机交互和任务建模为统一的推理-动作序列；(3)强化学习提高推理-动作一致性和长期任务连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;Robix在交互式任务执行中优于开源和商业基线（如GPT-4o和Gemini 2.5 Pro），在各种指令类型（开放式、多阶段、受约束的、无效的和中断的）上表现出强大的泛化能力，并在各种用户参与的任务（如餐桌清理、杂货购物和饮食过滤）上表现良好。&lt;h4&gt;结论&lt;/h4&gt;Robix是一个统一的模型，能够有效整合机器人推理、任务规划和自然语言交互，在各种任务和指令类型上表现出强大的泛化能力和性能。&lt;h4&gt;翻译&lt;/h4&gt;我们引入Robix，一个统一的模型，在单一视觉-语言架构中集成了机器人推理、任务规划和自然语言交互。作为分层机器人系统中的高级认知层，Robix动态生成低级控制器的原子命令和人类交互的口头响应，使机器人能够在端到端框架内遵循复杂指令、规划长期任务并与人类自然交互。Robix进一步引入了新颖功能，如主动对话、实时中断处理和任务执行中的上下文常识推理。其核心是，Robix利用链式思维推理并采用三阶段训练策略：(1)持续预训练以增强基础具身推理能力，包括3D空间理解、视觉定位和任务中心推理；(2)监督微调将人机交互和任务规划建模为统一的推理-动作序列；(3)强化学习提高推理-动作一致性和长期任务连贯性。大量实验表明，Robix在交互式任务执行中优于开源和商业基线（如GPT-4o和Gemini 2.5 Pro），展示了在各种指令类型（如开放式、多阶段、受约束的、无效的和中断的）和各种用户参与任务（如餐桌清理、杂货购物和饮食过滤）上的强大泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让机器人理解复杂指令、规划长期任务并与人类自然交互的问题。这个问题在现实中非常重要，因为机器人需要在开放、动态环境中协助人类完成多样化日常任务，这不仅是执行孤立命令，还需要自然交互和复杂推理能力。现有方法要么只关注任务分解而忽视交互和推理，要么采用模块化框架但缺乏灵活性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有分层方法的局限性，即大型语言模型主要关注任务分解，忽视了人类交互和具身推理；模块化系统虽然易于开发但缺乏灵活性。基于这些观察，作者借鉴了链式思维推理方法，设计了统一模型Robix。作者使用了三阶段训练策略，并从多个现有数据集（如AgiBot、BridgeData V2等）获取数据构建大规模训练集，同时参考了UI-TARS等工作的思想来合成高质量推理轨迹。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个统一的视觉-语言模型，将机器人推理、任务规划和人类交互整合在单一架构中，使机器人能以端到端方式处理复杂任务。整体流程分为三阶段：1)持续预训练：使用2000亿token数据集增强3D空间理解、视觉定位等基础能力；2)监督微调：通过数据合成将交互任务建模为统一推理-行动序列；3)强化学习：提高推理-行动一致性，特别是长期任务中的连贯性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一模型架构而非模块化设计；2)支持主动对话、实时中断处理和上下文常识推理的灵活交互能力；3)三阶段训练策略；4)大规模多样化数据集。相比之前工作，Robix不同之处在于：采用端到端架构而非模块化；同时关注任务规划和交互；引入链式思维推理；提供实时中断处理等新能力，使机器人能更好适应动态环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Robix提出了一种统一的视觉-语言模型，通过三阶段训练策略将机器人推理、任务规划和自然语言交互整合在一个端到端框架中，显著提升了机器人在复杂环境中执行长期任务的能力并实现了与人类自然灵活的交互。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Robix, a unified model that integrates robot reasoning, taskplanning, and natural language interaction within a single vision-languagearchitecture. Acting as the high-level cognitive layer in a hierarchical robotsystem, Robix dynamically generates atomic commands for the low-levelcontroller and verbal responses for human interaction, enabling robots tofollow complex instructions, plan long-horizon tasks, and interact naturallywith human within an end-to-end framework. Robix further introduces novelcapabilities such as proactive dialogue, real-time interruption handling, andcontext-aware commonsense reasoning during task execution. At its core, Robixleverages chain-of-thought reasoning and adopts a three-stage trainingstrategy: (1) continued pretraining to enhance foundational embodied reasoningabilities including 3D spatial understanding, visual grounding, andtask-centric reasoning; (2) supervised finetuning to model human-robotinteraction and task planning as a unified reasoning-action sequence; and (3)reinforcement learning to improve reasoning-action consistency and long-horizontask coherence. Extensive experiments demonstrate that Robix outperforms bothopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) ininteractive task execution, demonstrating strong generalization across diverseinstruction types (e.g., open-ended, multi-stage, constrained, invalid, andinterrupted) and various user-involved tasks such as table bussing, groceryshopping, and dietary filtering.</description>
      <author>example@mail.com (Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li)</author>
      <guid isPermaLink="false">2509.01106v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation</title>
      <link>http://arxiv.org/abs/2509.08757v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning (CoRL) 2025 Project site:  https://larg.github.io/socialnav-sub&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SocialNav-SUB基准，用于评估视觉语言模型(VLMs)在社会机器人导航场景中的场景理解能力，研究发现当前VLMs在社会场景理解方面仍存在关键差距。&lt;h4&gt;背景&lt;/h4&gt;机器人导航在动态、以人为中心的环境中需要基于鲁棒场景理解的社会合规决策。视觉语言模型(VLMs)显示出物体识别、常识推理和上下文理解等有希望的能力，但这些能力是否能准确理解复杂的社会导航场景(如推断代理间的时空关系和人类意图)尚不明确。&lt;h4&gt;目的&lt;/h4&gt;引入SocialNav-SUB(Social Navigation Scene Understanding Benchmark)，一个视觉问答(VQA)数据集和基准，旨在评估VLMs在真实世界社会机器人导航场景中的场景理解能力，并提供统一框架比较VLMs与人类和基于规则的基线。&lt;h4&gt;方法&lt;/h4&gt;设计并创建了SocialNav-SUB基准，包含需要空间、时空和社会推理的VQA任务，通过实验评估了最先进的VLMs，并将其表现与人类和基于规则的基线方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;尽管表现最好的VLMs与人类答案达成一致的几率令人鼓舞，但它们仍然表现不如简单的基于规则的方法和人类共识基线，表明当前VLMs在社会场景理解方面存在关键差距。&lt;h4&gt;结论&lt;/h4&gt;该基准为社交机器人导航的基础模型研究奠定了基础，提供了一个探索如何调整VLMs以满足真实世界社交机器人导航需求的框架。&lt;h4&gt;翻译&lt;/h4&gt;在动态的、以人为中心的环境中，机器人导航需要基于鲁棒场景理解的社会合规决策。最近的视觉语言模型(VLMs)展现出有希望的能力，如物体识别、常识推理和上下文理解——这些能力与社会机器人导航的微妙需求相符合。然而，目前尚不清楚VLMs是否能准确理解复杂的社会导航场景(例如推断代理之间的时空关系和人类意图)，这对于安全和社会合规的机器人导航至关重要。虽然最近的一些工作已经探索了VLMs在社会机器人导航中的应用，但没有现有工作系统性地评估它们满足这些必要条件的能力。在本文中，我们引入了SocialNav-SUB(社会导航场景理解基准)，这是一个视觉问答(VQA)数据集和基准，旨在评估VLMs在真实世界社会机器人导航场景中的场景理解能力。SocialNav-SUB提供了一个统一框架，用于评估VLMs在需要空间、时空和社会推理的社会机器人导航VQA任务中与人类和基于规则的基线的对比。通过对最先进的VLMs进行实验，我们发现尽管表现最好的VLMs与人类答案达成一致的几率令人鼓舞，但它仍然表现不如简单的基于规则的方法和人类共识基线，这表明当前VLMs在社会场景理解方面存在关键差距。我们的基准为社交机器人导航的基础模型研究奠定了基础，提供了一个探索如何调整VLMs以满足真实世界社交机器人导航需求的框架。本文概述以及代码和数据可在https://larg.github.io/socialnav-sub找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot navigation in dynamic, human-centered environments requiressocially-compliant decisions grounded in robust scene understanding. RecentVision-Language Models (VLMs) exhibit promising capabilities such as objectrecognition, common-sense reasoning, and contextual understanding-capabilitiesthat align with the nuanced requirements of social robot navigation. However,it remains unclear whether VLMs can accurately understand complex socialnavigation scenes (e.g., inferring the spatial-temporal relations among agentsand human intentions), which is essential for safe and socially compliant robotnavigation. While some recent works have explored the use of VLMs in socialrobot navigation, no existing work systematically evaluates their ability tomeet these necessary conditions. In this paper, we introduce the SocialNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual QuestionAnswering (VQA) dataset and benchmark designed to evaluate VLMs for sceneunderstanding in real-world social robot navigation scenarios. SocialNav-SUBprovides a unified framework for evaluating VLMs against human and rule-basedbaselines across VQA tasks requiring spatial, spatiotemporal, and socialreasoning in social robot navigation. Through experiments with state-of-the-artVLMs, we find that while the best-performing VLM achieves an encouragingprobability of agreeing with human answers, it still underperforms simplerrule-based approach and human consensus baselines, indicating critical gaps insocial scene understanding of current VLMs. Our benchmark sets the stage forfurther research on foundation models for social robot navigation, offering aframework to explore how VLMs can be tailored to meet real-world social robotnavigation needs. An overview of this paper along with the code and data can befound at https://larg.github.io/socialnav-sub .</description>
      <author>example@mail.com (Michael J. Munje, Chen Tang, Shuijing Liu, Zichao Hu, Yifeng Zhu, Jiaxun Cui, Garrett Warnell, Joydeep Biswas, Peter Stone)</author>
      <guid isPermaLink="false">2509.08757v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
  <item>
      <title>Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities</title>
      <link>http://arxiv.org/abs/2509.08302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 14 figures, accepted at IEEE Open Journal of Vehicular  Technology (OJVT)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇调查论文探讨了基础模型如何革新自动驾驶感知领域，从特定任务模型转向通用架构，解决泛化、可扩展性和鲁棒性等挑战。&lt;h4&gt;背景&lt;/h4&gt;基础模型正在改变自动驾驶感知领域，从狭隘的、特定任务的深度学习模型转向多功能、通用架构，这些架构在庞大且多样化的数据集上进行训练。&lt;h4&gt;目的&lt;/h4&gt;探讨这些模型如何解决自动驾驶感知中的关键挑战，包括泛化能力、可扩展性和对分布变化的鲁棒性限制。&lt;h4&gt;方法&lt;/h4&gt;提出一个围绕四种基本能力的分类法：泛化知识、空间理解、多传感器鲁棒性和时序推理，并对每种能力进行全面的前沿方法审查。&lt;h4&gt;主要发现&lt;/h4&gt;与传统以方法为中心的调查不同，他们的框架优先考虑概念设计原则，为模型开发提供能力驱动的指导，并对基础方面提供更清晰的见解。&lt;h4&gt;结论&lt;/h4&gt;讨论了将这些能力集成到实时系统中的挑战，以及与计算需求和模型可靠性相关的部署挑战，并提出了未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;基础模型正在革新自动驾驶感知，将领域从狭隘的、特定任务的深度学习模型转变为在庞大、多样化数据集上训练的多功能、通用架构。本调查探讨了这些模型如何解决自动驾驶感知中的关键挑战，包括泛化、可扩展性和对分布变化的鲁棒性限制。调查提出了一个围绕四种基本能力构建的新颖分类法，以在动态驾驶环境中实现稳健性能：泛化知识、空间理解、多传感器鲁棒性和时序推理。对于每种能力，调查阐明了其意义并全面审查了最前沿的方法。与传统以方法为中心的调查不同，我们的独特框架优先考虑概念设计原则，为模型开发提供了能力驱动的指导，并对基础方面提供了更清晰的见解。我们最后讨论了关键挑战，特别是将这些能力集成到实时、可扩展系统中的挑战，以及与计算需求和确保模型可靠性（如幻觉问题和分布外故障）相关的更广泛的部署挑战。调查还概述了关键的未来研究方向，以促进基础模型在自动驾驶系统中的安全有效部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/OJVT.2025.3604823 10.1109/OJVT.2025.3604823  10.1109/OJVT.2025.3604823&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are revolutionizing autonomous driving perception,transitioning the field from narrow, task-specific deep learning models toversatile, general-purpose architectures trained on vast, diverse datasets.This survey examines how these models address critical challenges in autonomousperception, including limitations in generalization, scalability, androbustness to distributional shifts. The survey introduces a novel taxonomystructured around four essential capabilities for robust performance in dynamicdriving environments: generalized knowledge, spatial understanding,multi-sensor robustness, and temporal reasoning. For each capability, thesurvey elucidates its significance and comprehensively reviews cutting-edgeapproaches. Diverging from traditional method-centric surveys, our uniqueframework prioritizes conceptual design principles, providing acapability-driven guide for model development and clearer insights intofoundational aspects. We conclude by discussing key challenges, particularlythose associated with the integration of these capabilities into real-time,scalable systems, and broader deployment challenges related to computationaldemands and ensuring model reliability against issues like hallucinations andout-of-distribution failures. The survey also outlines crucial future researchdirections to enable the safe and effective deployment of foundation models inautonomous driving systems.</description>
      <author>example@mail.com (Rajendramayavan Sathyam, Yueqi Li)</author>
      <guid isPermaLink="false">2509.08302v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2509.08126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid  Robots&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为OGRG的框架，能够通过自然语言理解抓取目标物体，即使在有重复物体的情况下也能进行空间推理和抓取预测。&lt;h4&gt;背景&lt;/h4&gt;让机器人通过自然语言抓取物体对有效人机交互至关重要，但现有方法难以处理开放形式语言表达，通常假设无重复物体，且依赖密集像素级标注。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够解释开放形式语言表达并进行空间推理的框架，以定位目标物体并预测平面抓取姿态，即使在包含重复物体实例的场景中也能工作。&lt;h4&gt;方法&lt;/h4&gt;提出基于属性的目标定位和机器人抓取(OGRG)框架，在两种设置下测试：像素级完全监督的指称抓取合成(RGS)和使用单像素标注的弱监督指称抓取可能性(RGA)。关键贡献包括双向视觉-语言融合模块和深度信息集成以增强几何推理。&lt;h4&gt;主要发现&lt;/h4&gt;OGRG在具有多样化空间语言指令的桌面场景中优于基线方法，在RGS设置下以17.59 FPS运行，提供更好的定位和抓取预测准确性；在弱监督RGA设置下，模拟和真实机器人试验中均优于基线抓取成功率。&lt;h4&gt;结论&lt;/h4&gt;OGRG成功解决了通过自然语言抓取物体的问题，特别是在处理开放形式语言表达和重复物体实例方面，在完全监督和弱监督设置下都表现出色，具有实际应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;使机器人能够通过自然语言抓取指定物体对于有效的人机交互至关重要，但仍然是一个重大挑战。现有方法通常难以处理开放形式的语言表达，通常假设没有重复的目标物体。此外，它们通常依赖于密集的像素级标注来进行物体定位和抓取配置。我们提出了基于属性的目标定位和机器人抓取(OGRG)，这是一个新框架，能够解释开放形式的语言表达并进行空间推理，以定位目标物体并预测平面抓取姿态，即使在包含重复物体实例的场景中也是如此。我们在两种设置下研究了OGRG：(1)在像素级完全监督下的指称抓取合成(RGS)，以及(2)使用仅单像素抓取标注的弱监督学习进行指称抓取可能性(RGA)。关键贡献包括双向视觉-语言融合模块和深度信息的集成，以增强几何推理，提高定位和抓取性能。实验结果表明，在具有多样化空间语言指令的桌面场景中，OGRG优于强大的基线方法。在RGS中，它在单个NVIDIA RTX 2080 Ti GPU上以17.59 FPS的速度运行，能够在闭环或多物体顺序抓取中使用，并提供比所有考虑的基线更好的定位和抓取预测准确性。在弱监督的RGA设置下，OGRG在模拟和真实机器人试验中都优于基线的抓取成功率，突显了其空间推理设计的有效性。项目页面：https://z.umn.edu/ogrg&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让机器人通过自然语言描述来抓取指定物体的问题。具体来说，现有方法难以处理开放形式的语言表达、无法处理场景中重复出现的物体，且依赖密集的像素级标注。这个问题在现实中很重要，因为它能使机器人更好地理解人类指令，实现更自然的人机交互，同时减少对昂贵标注数据的依赖，使技术更容易在实际场景中应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括处理开放形式语言能力不足、无法处理重复物体、依赖密集标注等。他们注意到虽然多模态大语言模型性能强大，但计算需求大，难以在资源受限的机器人平台上部署。因此，作者设计了一个紧凑且计算高效的融合模块作为替代。他们借鉴了ETRG的CLIP模型和下采样-上采样策略，以及LAVT的单向融合模块，但在此基础上提出了改进的双向融合模块。同时，他们使用了Swin Transformer作为视觉主干网络和BERT作为语言特征提取器，并整合了深度信息来增强几何推理能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个名为OGRG的框架，能够解释开放形式的语言表达并进行空间推理，以定位目标物体并预测平面抓取姿态。整体流程包括：1)接受RGB图像、深度图像和语言描述作为输入；2)使用Swin Transformer提取视觉特征，BERT提取语言特征，ResNet-18提取深度特征；3)通过四阶段多模态融合过程，使用双向对齐器进行视觉、语言和深度特征的交互；4)对于RGS任务，使用FCN头生成物体掩码和抓取相关图；对于RGA任务，先生成物体掩码，再使用掩码条件抓取网络预测抓取能力图；5)输出物体定位结果和抓取姿态参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双向视觉-语言融合模块(Bi-Aligner)，同时进行视觉-语言和语言-视觉的交叉注意力计算；2)深度信息集成，增强几何推理能力；3)支持两种抓取检测设置：完全监督的RGS和弱监督的RGA；4)掩码条件抓取网络(MGN)用于RGA任务。相比之前工作，本文的双向融合比ETRG的双向适配器和LAVT的单向融合更有效地对齐特征；深度融合方式比ETRG更有效，避免信息损失；计算效率比多模态大语言模型更高，适合资源受限的机器人平台；支持弱监督学习，减少标注需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于双向视觉-语言融合和深度信息集成的OGRG框架，使机器人能够通过开放形式的自然语言描述准确识别并抓取目标物体，即使在有重复物体实例的场景中也能高效工作，同时支持完全监督和弱监督两种训练方式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling robots to grasp objects specified through natural language isessential for effective human-robot interaction, yet it remains a significantchallenge. Existing approaches often struggle with open-form languageexpressions and typically assume unambiguous target objects without duplicates.Moreover, they frequently rely on costly, dense pixel-wise annotations for bothobject grounding and grasp configuration. We present Attribute-based ObjectGrounding and Robotic Grasping (OGRG), a novel framework that interpretsopen-form language expressions and performs spatial reasoning to ground targetobjects and predict planar grasp poses, even in scenes containing duplicatedobject instances. We investigate OGRG in two settings: (1) Referring GraspSynthesis (RGS) under pixel-wise full supervision, and (2) Referring GraspAffordance (RGA) using weakly supervised learning with only single-pixel graspannotations. Key contributions include a bi-directional vision-language fusionmodule and the integration of depth information to enhance geometric reasoning,improving both grounding and grasping performance. Experiment results show thatOGRG outperforms strong baselines in tabletop scenes with diverse spatiallanguage instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX2080 Ti GPU, enabling potential use in closed-loop or multi-object sequentialgrasping, while delivering superior grounding and grasp prediction accuracycompared to all the baselines considered. Under the weakly supervised RGAsetting, OGRG also surpasses baseline grasp-success rates in both simulationand real-robot trials, underscoring the effectiveness of its spatial reasoningdesign. Project page: https://z.umn.edu/ogrg</description>
      <author>example@mail.com (Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi)</author>
      <guid isPermaLink="false">2509.08126v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Tokenizing Loops of Antibodies</title>
      <link>http://arxiv.org/abs/2509.08707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 7 figures, 10 tables, code available at  https://github.com/prescient-design/igloo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Igloo的多模态抗体环标记器，能够编码主链二面角和序列信息，通过对比学习训练，高效检索匹配的环结构，解决传统方法覆盖有限的问题，并提高蛋白质基础模型在抗体设计中的应用。&lt;h4&gt;背景&lt;/h4&gt;抗体互补决定区是环状结构，对与抗原的相互作用至关重要。自1980年代以来，将CDR结构分类为规范簇有助于识别关键结构基序，但现有方法覆盖范围有限且难以整合到蛋白质基础模型中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够编码抗体环结构信息的多模态标记器，解决现有方法覆盖有限的问题，并提高蛋白质基础模型在抗体设计中的应用。&lt;h4&gt;方法&lt;/h4&gt;引入ImmunoGlobulin LOOp Tokenizer（Igloo），一种多模态抗体环标记器，编码主链二面角和序列信息。使用对比学习目标训练，将相似主链二面角的环在潜在空间中映射得更近。开发了IglooLM和IglooALM，将Igloo标记整合到蛋白质语言模型中。&lt;h4&gt;主要发现&lt;/h4&gt;1. Igloo在识别相似H3环方面比现有方法高5.9%；2. 为所有环分配标记，解决规范簇覆盖有限问题；3. IglooLM在8/10抗体-抗原靶点上优于基础模型；4. IglooLM性能与参数量多7倍的模型相当；5. IglooALM采样的抗体环序列更多样化，结构更一致。&lt;h4&gt;结论&lt;/h4&gt;Igloo证明了为抗体环引入多模态标记的益处，能够编码抗体环的多样化景观，改进蛋白质基础模型，并用于抗体CDR设计。&lt;h4&gt;翻译&lt;/h4&gt;抗体的互补决定区是与抗原相互作用的关键环状结构，对设计新型生物制剂非常重要。自1980年代以来，将CDR结构的多样性分类为规范簇有助于识别抗体的关键结构基序。然而，现有方法覆盖范围有限，且难以直接整合到蛋白质基础模型中。我们在此介绍ImmunoGlobulin LOOp Tokenizer（Igloo），这是一种多模态抗体环标记器，编码主链二面角和序列信息。Igloo使用对比学习目标进行训练，将具有相似主链二面角的环在潜在空间中映射得更近。Igloo能够从结构抗体数据库中高效检索最匹配的环结构，在识别相似的H3环方面比现有方法高出5.9%。Igloo为所有环分配标记，解决了规范簇覆盖有限的问题，同时保留了恢复规范环构象的能力。为了展示Igloo标记的多功能性，我们展示了它们可以整合到蛋白质语言模型中，形成IglooLM和IglooALM。在预测重链变体的结合亲和力方面，IglooLM在10个抗体-抗原靶点中有8个表现优于基础蛋白质语言模型。此外，它与现有的最先进的序列和 multimodal 蛋白质语言模型相当，性能与参数量多7倍的模型相当。IglooALM采样的抗体环在序列上更多样化，结构上比最先进的抗体逆向折叠模型更一致。Igloo证明了为抗体环引入多模态标记的益处，能够编码抗体环的多样化景观，改进蛋白质基础模型，并用于抗体CDR设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The complementarity-determining regions of antibodies are loop structuresthat are key to their interactions with antigens, and of high importance to thedesign of novel biologics. Since the 1980s, categorizing the diversity of CDRstructures into canonical clusters has enabled the identification of keystructural motifs of antibodies. However, existing approaches have limitedcoverage and cannot be readily incorporated into protein foundation models.Here we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibodyloop tokenizer that encodes backbone dihedral angles and sequence. Igloo istrained using a contrastive learning objective to map loops with similarbackbone dihedral angles closer together in latent space. Igloo can efficientlyretrieve the closest matching loop structures from a structural antibodydatabase, outperforming existing methods on identifying similar H3 loops by5.9\%. Igloo assigns tokens to all loops, addressing the limited coverage issueof canonical clusters, while retaining the ability to recover canonical loopconformations. To demonstrate the versatility of Igloo tokens, we show thatthey can be incorporated into protein language models with IglooLM andIglooALM. On predicting binding affinity of heavy chain variants, IglooLMoutperforms the base protein language model on 8 out of 10 antibody-antigentargets. Additionally, it is on par with existing state-of-the-artsequence-based and multimodal protein language models, performing comparably tomodels with $7\times$ more parameters. IglooALM samples antibody loops whichare diverse in sequence and more consistent in structure than state-of-the-artantibody inverse folding models. Igloo demonstrates the benefit of introducingmultimodal tokens for antibody loops for encoding the diverse landscape ofantibody loops, improving protein foundation models, and for antibody CDRdesign.</description>
      <author>example@mail.com (Ada Fang, Robert G. Alberstein, Simon Kelow, Frédéric A. Dreyer)</author>
      <guid isPermaLink="false">2509.08707v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation</title>
      <link>http://arxiv.org/abs/2509.08624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BIBM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为UOPSL的新型无配对多模态框架，利用OCT衍生的空间先验知识增强基于眼底图像的疾病识别，解决了多模态眼科图像获取成本高和模态不平衡的问题。&lt;h4&gt;背景&lt;/h4&gt;AI驱动的多模态医学图像诊断在眼科疾病识别方面取得了显著进展，但获取配对的多模态眼科图像成本过高。眼底摄影简单且经济高效，但OCT数据有限且存在模态不平衡问题。传统仅依赖眼底或文本特征的方法无法捕捉细粒度空间信息，因为每种成像模态对病变偏好部位提供不同的线索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用OCT衍生的空间先验知识来增强眼底图像疾病识别的无配对多模态框架，解决配对数据获取困难和模态不平衡的问题。&lt;h4&gt;方法&lt;/h4&gt;提出UOPSL框架，通过扩展疾病文本描述连接无配对的眼底图像和OCT。首先，在大量无配对的OCT和眼底图像上使用对比学习，同时在OCT潜在空间中学习偏好部位矩阵。该矩阵通过大量优化捕捉OCT特征空间内的病变定位模式。在仅基于眼底图像的下游分类任务中，消除OCT输入，利用偏好部位矩阵辅助眼底图像分类学习。&lt;h4&gt;主要发现&lt;/h4&gt;在9个不同数据集、28个关键类别上进行的广泛实验表明，该框架优于现有基准方法。&lt;h4&gt;结论&lt;/h4&gt;UOPSL框架成功利用OCT衍生的空间先验知识增强基于眼底图像的疾病识别，无需配对数据，有效解决了多模态眼科图像获取成本高和模态不平衡的问题。&lt;h4&gt;翻译&lt;/h4&gt;近年来，AI驱动的多模态医学图像诊断的显著进展极大地促进了眼科疾病的识别。然而，获取配对的多模态眼科图像仍然成本过高。虽然眼底摄影简单且经济高效，但OCT数据的有限性和固有的模态不平衡阻碍了进一步发展。传统仅依赖眼底或文本特征的方法往往无法捕捉细粒度空间信息，因为每种成像模态对病变偏好部位提供不同的线索。在本研究中，我们提出了一种名为UOPSL的新型无配对多模态框架，利用大量的OCT衍生空间先验知识动态识别偏好部位，增强基于眼底图像的疾病识别。我们的方法通过扩展疾病文本描述连接无配对的眼底图像和OCT。最初，我们在大量无配对的OCT和眼底图像上使用对比学习，同时在OCT潜在空间中学习偏好部位矩阵。通过大量优化，该矩阵捕捉了OCT特征空间内的病变定位模式。在仅基于眼底图像的下游分类任务的微调或推理阶段，当无法获取配对的OCT数据时，我们消除OCT输入，利用偏好部位矩阵辅助眼底图像分类学习。在9个不同数据集、28个关键类别上进行的广泛实验表明，我们的框架优于现有基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant advancements in AI-driven multimodal medical image diagnosis haveled to substantial improvements in ophthalmic disease identification in recentyears. However, acquiring paired multimodal ophthalmic images remainsprohibitively expensive. While fundus photography is simple and cost-effective,the limited availability of OCT data and inherent modality imbalance hinderfurther progress. Conventional approaches that rely solely on fundus or textualfeatures often fail to capture fine-grained spatial information, as eachimaging modality provides distinct cues about lesion predilection sites. Inthis study, we propose a novel unpaired multimodal framework \UOPSL thatutilizes extensive OCT-derived spatial priors to dynamically identifypredilection sites, enhancing fundus image-based disease recognition. Ourapproach bridges unpaired fundus and OCTs via extended disease textdescriptions. Initially, we employ contrastive learning on a large corpus ofunpaired OCT and fundus images while simultaneously learning the predilectionsites matrix in the OCT latent space. Through extensive optimization, thismatrix captures lesion localization patterns within the OCT feature space.During the fine-tuning or inference phase of the downstream classification taskbased solely on fundus images, where paired OCT data is unavailable, weeliminate OCT input and utilize the predilection sites matrix to assist infundus image classification learning. Extensive experiments conducted on 9diverse datasets across 28 critical categories demonstrate that our frameworkoutperforms existing benchmarks.</description>
      <author>example@mail.com (Zhihao Zhao, Yinzheng Zhao, Junjie Yang, Xiangtong Yao, Quanmin Liang, Daniel Zapp, Kai Huang, Nassir Navab, M. Ali Nasseri)</author>
      <guid isPermaLink="false">2509.08624v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography</title>
      <link>http://arxiv.org/abs/2509.08116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为PhysioCLR的心电图分析框架，它是一种生理感知的对比学习框架，通过整合领域特定先验知识增强心电图心律失常分类的可泛化性和临床相关性，解决了标记数据有限的问题。&lt;h4&gt;背景&lt;/h4&gt;心电图在诊断心脏疾病中起着关键作用，但基于人工智能的心电图分析往往受限于标记数据的可用性。自监督学习可以通过利用大规模无标签数据来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种名为PhysioCLR的生理感知对比学习框架，整合领域特定的先验知识，以增强心电图心律失常分类的可泛化性和临床相关性。&lt;h4&gt;方法&lt;/h4&gt;在预训练过程中，PhysioCLR学习将具有相似临床相关特征的样本嵌入拉近，同时将不相似的推开。该方法整合心电图生理相似性线索到对比学习中，引入特定于心电图的增强方法保持类别不变，并提出混合损失函数优化学习表示质量。&lt;h4&gt;主要发现&lt;/h4&gt;在Chapman、Georgia和私人ICU数据集上，PhysioCLR比最强基线模型平均提升了12%的AUROC，展示了强大的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过将生理知识嵌入对比学习，PhysioCLR使模型能够学习临床上有意义且可迁移的心电图特征。&lt;h4&gt;翻译&lt;/h4&gt;目标：心电图在诊断心脏疾病中起着关键作用；然而，基于人工智能的心电图分析的有效性常常受限于标记数据的可用性。自监督学习可以通过利用大规模无标签数据来解决这个问题。我们引入了PhysioCLR（心电图生理感知对比学习表示），这是一种生理感知的对比学习框架，整合了领域特定的先验知识，以增强基于心电图的心律失常分类的可泛化性和临床相关性。方法：在预训练过程中，PhysioCLR学习将具有相似临床相关特征的样本嵌入拉近，同时将不相似的推开。与现有方法不同，我们的方法将心电图生理相似性线索整合到对比学习中，促进学习临床上有意义的表示。此外，我们引入了特定于心电图的增强方法，这些方法在增强后保持心电图类别不变，并提出了一种混合损失函数来进一步优化学习到的表示质量。结果：我们在两个公开的心电图数据集（Chapman和Georgia）上对PhysioCLR进行多标签心电图诊断评估，以及一个标记为二元分类的私人ICU数据集。在Chapman、Georgia和私人队列中，PhysioCLR比最强基线模型平均提升了12%的AUROC，强调了其强大的跨数据集泛化能力。结论：通过将生理知识嵌入对比学习，PhysioCLR使模型能够学习临床上有意义且可迁移的心电图特征。意义：PhysioCLR展示了生理信息感知的自监督学习在提供更有效且标记高效的心电图诊断方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heartconditions; however, the effectiveness of artificial intelligence (AI)-basedECG analysis is often hindered by the limited availability of labeled data.Self-supervised learning (SSL) can address this by leveraging large-scaleunlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive LearningRepresentation for ECG), a physiology-aware contrastive learning framework thatincorporates domain-specific priors to enhance the generalizability andclinical relevance of ECG-based arrhythmia classification. Methods: Duringpretraining, PhysioCLR learns to bring together embeddings of samples thatshare similar clinically relevant features while pushing apart those that aredissimilar. Unlike existing methods, our method integrates ECG physiologicalsimilarity cues into contrastive learning, promoting the learning of clinicallymeaningful representations. Additionally, we introduce ECG- specificaugmentations that preserve the ECG category post augmentation and propose ahybrid loss function to further refine the quality of learned representations.Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,for multilabel ECG diagnoses, as well as a private ICU dataset labeled forbinary classification. Across the Chapman, Georgia, and private cohorts,PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,underscoring its robust cross-dataset generalization. Conclusion: By embeddingphysiological knowledge into contrastive learning, PhysioCLR enables the modelto learn clinically meaningful and transferable ECG eatures. Significance:PhysioCLR demonstrates the potential of physiology-informed SSL to offer apromising path toward more effective and label-efficient ECG diagnostics.</description>
      <author>example@mail.com (Nooshin Maghsoodi, Sarah Nassar, Paul F R Wilson, Minh Nguyen Nhat To, Sophia Mannina, Shamel Addas, Stephanie Sibley, David Maslove, Purang Abolmaesumi, Parvin Mousavi)</author>
      <guid isPermaLink="false">2509.08116v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CAME-AB是一种创新的跨模态注意力框架，结合专家混合主干，用于抗体结合位点预测。通过整合五种生物学模态并采用自适应融合策略，有效解决了传统方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跨模态注意力框架CAME-AB，用于稳健的抗体结合位点预测，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;CAME-AB集成了五种生物学基础的模态：原始氨基酸编码、BLOSUM置换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图。提出了自适应模态融合模块，使用Transformer编码器结合MoE模块促进特征专业化和容量扩展，并集成了监督对比学习目标以提高优化稳定性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的大量实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上始终优于强大的基线方法。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过多模态特征融合和自适应跨模态推理实现了抗体结合位点预测的性能提升，为计算免疫学和治疗性抗体设计提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。在本文中，我们提出了CAME-AB，一种具有专家混合主干的新型跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB将五种生物学基础的模态整合为统一的多模态表示，包括原始氨基酸编码、BLOSUM置换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图。为了增强自适应跨模态推理，我们提出了自适应模态融合模块，学习根据全局相关性和输入特定贡献动态加权每个模态。Transformer编码器结合MoE模块进一步促进特征专业化和容量扩展。我们还集成了监督对比学习目标，明确塑造潜在空间几何结构，鼓励类内紧凑性和类间可分离性。为了提高优化稳定性和泛化能力，我们在训练期间应用随机权重平均。在基准抗体-抗原数据集上的大量实验表明，CAME-AB在多个指标上始终优于强大的基线方法。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。模型实现细节和代码可在https://anonymous.4open.science/r/CAME-AB-C525获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, anovel Cross-modality Attention framework with a Mixture-of-Experts (MoE)backbone for robust antibody binding site prediction. CAME-AB integrates fivebiologically grounded modalities, including raw amino acid encodings, BLOSUMsubstitution profiles, pretrained language model embeddings, structure-awarefeatures, and GCN-refined biochemical graphs, into a unified multimodalrepresentation. To enhance adaptive cross-modal reasoning, we propose an\emph{adaptive modality fusion} module that learns to dynamically weight eachmodality based on its global relevance and input-specific contribution. ATransformer encoder combined with an MoE module further promotes featurespecialization and capacity expansion. We additionally incorporate a supervisedcontrastive learning objective to explicitly shape the latent space geometry,encouraging intra-class compactness and inter-class separability. To improveoptimization stability and generalization, we apply stochastic weight averagingduring training. Extensive experiments on benchmark antibody-antigen datasetsdemonstrate that CAME-AB consistently outperforms strong baselines on multiplemetrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablationstudies further validate the effectiveness of each architectural component andthe benefit of multimodal feature integration. The model implementation detailsand the codes are available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Rui Xiao, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v3</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models</title>
      <link>http://arxiv.org/abs/2509.05230v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Conference on Empirical Methods in Natural Language  Processing (EMNLP 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURE是一个新颖的轻量级框架，通过分离和抑制概念性捷径同时保留内容信息，有效提高了预训练语言模型的鲁棒性和公平性，在多个数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;预训练语言模型在各种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性影响，这损害了模型的鲁棒性和公平性。&lt;h4&gt;目的&lt;/h4&gt;引入CURE框架，系统性地分离和抑制概念性捷径，同时保留必要的内容信息，以提高模型的鲁棒性和公平性。&lt;h4&gt;方法&lt;/h4&gt;首先通过一个由反转网络强化的专门内容提取器提取概念无关的表示，确保任务相关信息的最小损失；随后采用对比学习进行可控去偏，微调剩余概念线索的影响，使模型能根据目标任务适当减少有害偏见或利用有益相关性。&lt;h4&gt;主要发现&lt;/h4&gt;在IMDB和Yelp数据集上使用三种预训练架构评估，CURE在IMDB上的F1分数绝对提高了+10分，在Yelp上提高了+2分，同时引入了最小的计算开销。&lt;h4&gt;结论&lt;/h4&gt;CURE方法为对抗概念偏见提供了一个灵活的无监督蓝图，为更可靠和公平的语言理解系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;预训练语言模型在各种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性影响，这损害了模型的鲁棒性和公平性。在这项工作中，我们引入了CURE，一个新颖且轻量级的框架，可以系统性地分离和抑制概念性捷径，同时保留必要的内容信息。我们的方法首先通过一个由反转网络强化的专门内容提取器提取概念无关的表示，确保任务相关信息的最小损失。随后的可控去偏模块采用对比学习来微调剩余概念线索的影响，使模型能够根据目标任务适当减少有害偏见或利用有益相关性。在IMDB和Yelp数据集上使用三种预训练架构评估，CURE在IMDB上的F1分数绝对提高了+10分，在Yelp上提高了+2分，同时引入了最小的计算开销。我们的方法为对抗概念偏见提供了一个灵活的无监督蓝图，为更可靠和公平的语言理解系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained language models have achieved remarkable success across diverseapplications but remain susceptible to spurious, concept-driven correlationsthat impair robustness and fairness. In this work, we introduce CURE, a noveland lightweight framework that systematically disentangles and suppressesconceptual shortcuts while preserving essential content information. Our methodfirst extracts concept-irrelevant representations via a dedicated contentextractor reinforced by a reversal network, ensuring minimal loss oftask-relevant information. A subsequent controllable debiasing module employscontrastive learning to finely adjust the influence of residual conceptualcues, enabling the model to either diminish harmful biases or harnessbeneficial correlations as appropriate for the target task. Evaluated on theIMDB and Yelp datasets using three pre-trained architectures, CURE achieves anabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,while introducing minimal computational overhead. Our approach establishes aflexible, unsupervised blueprint for combating conceptual biases, paving theway for more reliable and fair language understanding systems.</description>
      <author>example@mail.com (Aysenur Kocak, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci)</author>
      <guid isPermaLink="false">2509.05230v2</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions</title>
      <link>http://arxiv.org/abs/2509.08654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于特征的量子网络路由部分可观察马尔可夫决策过程(POMDP)框架，结合了信念状态规划和图神经网络(GNNs)，以解决动态量子系统中的部分可观察性、退相干和可扩展性挑战。&lt;h4&gt;背景&lt;/h4&gt;在动态量子系统中，部分可观察性、退相干和可扩展性是量子网络路由面临的主要挑战，需要新的方法来处理这些复杂问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理量子网络中部分可观察性、退相干和可扩展性挑战的路由框架，提高路由保真度和纠缠传输率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种混合GNN-POMDP架构，将复杂的量子网络动态（包括纠缠退相干和时间变化的信道噪声）编码到低维特征空间，并使用噪声自适应机制融合POMDP信念更新和GNN输出进行决策。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟量子网络实验中，该方法在多达100个节点的网络上显著提高了路由保真度和纠缠传输率，特别是在高退相干和非平稳条件下优于现有最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;该框架为量子网络路由提供了有效的解决方案，通过结合图神经网络和POMDP框架，能够处理复杂的量子网络动态和噪声环境，实现了更好的路由性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于特征的量子网络路由部分可观察马尔可夫决策过程(POMDP)框架，结合了信念状态规划和图神经网络(GNNs)，以解决动态量子系统中的部分可观察性、退相干和可扩展性挑战。我们的方法将复杂的量子网络动态（包括纠缠退相干和时间变化的信道噪声）编码到低维特征空间，从而实现高效的信念更新和可扩展的策略学习。我们框架的核心是一个混合GNN-POMDP架构，它处理纠缠链的图结构表示以学习路由策略，并结合一个噪声自适应机制，该机制融合POMDP信念更新和GNN输出以实现鲁棒的决策制定。我们提供了理论分析，确立了信念收敛、策略改进和对噪声鲁棒性的保证。在多达100个节点的模拟量子网络上的实验表明，与最先进的基线相比，路由保真度和纠缠传输率有显著提高，特别是在高退相干和非平稳条件下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a feature-based Partially Observable Markov DecisionProcess (POMDP) framework for quantum network routing, combining belief-stateplanning with Graph Neural Networks (GNNs) to address partial observability,decoherence, and scalability challenges in dynamic quantum systems. Ourapproach encodes complex quantum network dynamics, including entanglementdegradation and time-varying channel noise, into a low-dimensional featurespace, enabling efficient belief updates and scalable policy learning. The coreof our framework is a hybrid GNN-POMDP architecture that processesgraph-structured representations of entangled links to learn routing policies,coupled with a noise-adaptive mechanism that fuses POMDP belief updates withGNN outputs for robust decision making. We provide a theoretical analysisestablishing guarantees for belief convergence, policy improvement, androbustness to noise. Experiments on simulated quantum networks with up to 100nodes demonstrate significant improvements in routing fidelity and entanglementdelivery rates compared to state-of-the-art baselines, particularly under highdecoherence and nonstationary conditions.</description>
      <author>example@mail.com (Amirhossein Taherpour, Abbas Taherpour, Tamer Khattab)</author>
      <guid isPermaLink="false">2509.08654v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Facet: highly efficient E(3)-equivariant networks for interatomic potentials</title>
      <link>http://arxiv.org/abs/2509.08418v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Facet，一种用于高效机器学习势能面的图神经网络架构，通过系统分析可导向GNNs开发，解决了计算材料发现中的计算瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;计算材料发现受限于第一性原理计算的高成本。现有的机器学习势能面方法虽然前景广阔，但面临计算瓶颈。可导向图神经网络(GNNs)利用球谐函数编码几何结构，尊重原子对称性，但维持等变性困难且计算复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的机器学习势能面的GNN架构，通过系统分析可导向GNNs来创建性能更优、计算效率更高的模型。&lt;h4&gt;方法&lt;/h4&gt;用样条替换用于原子间距离的多层感知器(MLPs)，减少计算和内存需求；引入一种通用的等变层，通过球面网格投影混合节点信息，然后使用标准MLPs，这种方法比张量积更快且比线性或门控层更具表现力。&lt;h4&gt;主要发现&lt;/h4&gt;在MPTrj数据集上，Facet与领先模型匹配，但参数少得多，训练计算量不到10%；在晶体弛豫任务中，它比MACE模型运行速度快两倍；SevenNet-0的参数可减少25%以上而不会损失准确性；这些技术使大型基础模型的训练速度提高10倍以上。&lt;h4&gt;结论&lt;/h4&gt;这些技术使大型基础模型的训练速度提高10倍以上，有可能改变计算材料发现的格局。&lt;h4&gt;翻译&lt;/h4&gt;计算材料发现受限于第一性原理计算的高成本。预测晶体结构能量的机器学习(ML)势能面很有前景，但现有方法面临计算瓶颈。可导向图神经网络(GNNs)利用球谐函数编码几何结构，尊重原子对称性——排列、旋转和平移——进行物理上真实的预测。然而，维持等变性很困难：激活函数必须修改，每层必须处理不同谐波阶数的多种数据类型。我们提出了Facet，一种用于高效ML势能面的GNN架构，通过系统分析可导向GNNs开发。我们的创新包括用样条替换用于原子间距离的昂贵多层感知器(MLPs)，在匹配性能的同时减少计算和内存需求。我们还引入了一种通用等变层，通过球面网格投影混合节点信息，然后使用标准MLPs——比张量积更快且比线性或门控层更具表现力。在MPTrj数据集上，Facet与领先模型匹配，但参数少得多，训练计算量不到10%。在晶体弛豫任务中，它比MACE模型运行速度快两倍。我们进一步表明SevenNet-0的参数可以减少25%以上而不会损失准确性。这些技术使ML势能面的大型基础模型的训练速度提高10倍以上，可能改变计算材料发现的格局。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器学习势能函数(MLIPs)训练过程中的计算效率低下问题。当前基于第一性原理的计算方法(如密度泛函理论)虽然准确但计算成本极高，而现有的机器学习势能函数训练大型基础模型需要大量计算资源和时间(如MACE-MP-0模型训练310天，SevenNet-0训练90天)。这种高计算成本阻碍了研究人员快速更新模型、探索不同训练数据和调整参数，限制了材料科学领域的研究进展和创新。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者系统分析了现有的可导向GNN设计面临的挑战，包括保持E(3)等变性的困难和标准网络组件需要修改的问题。他们识别出计算瓶颈在于等变性卷积中的消息过滤器，并考虑了网络中节点信息混合部分的计算复杂性和表达力之间的权衡。作者借鉴了SevenNet的架构基础，但采用了EquiformerV2中的等变层归一化方法，并受到计算机视觉中MLP-Mixer架构的启发，将其应用于球面处理。整体设计思路是简化计算密集组件，同时保持物理对称性和模型表达能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用高效的样条函数替代计算密集的多层感知器处理原子间距离，并提出S2-MLP-Mixer层通过球面网格投影混合节点信息。整体流程包括：1)将晶体编码为周期轨道图，原子为节点，最近邻为边；2)交互块中分解边向量为方向和距离分量，使用球谐函数和贝塞尔基编码，通过Clebsch-Gordan张量积组合特征；3)节点自交互中应用S2-MLP-Mixer处理信息；4)每层后使用残差连接和等变层归一化；5)最后通过读取层预测节点能量并求和得到总能量。整个流程保持E(3)等变性，同时优化计算效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)用简单线性层和样条函数替代计算密集的多层感知器处理距离信息，减少参数和计算需求；2)提出S2-MLP-Mixer层，通过球面网格投影混合节点信息，在速度和表达力上优于现有方法；3)灵活的消息归一化策略，根据估计的平均邻居数归一化；4)从零开始学习元素嵌入，不依赖预训练。相比MACE，Facet避免了其计算极其昂贵的对称张量积层；相比SevenNet/GNoMe/NequIP，Facet的非线性门控函数能混合非标量信息，而不仅是缩放它们。整体上，Facet在保持相当性能的同时实现了更高的训练和推理效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Facet通过系统优化E(3)-等变图神经网络架构，用高效样条替代计算密集的多层感知器，并创新的S2-MLP-Mixer层，实现了在保持高性能的同时将训练计算需求减少90%以上的机器学习势能函数。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational materials discovery is limited by the high cost offirst-principles calculations. Machine learning (ML) potentials that predictenergies from crystal structures are promising, but existing methods facecomputational bottlenecks. Steerable graph neural networks (GNNs) encodegeometry with spherical harmonics, respecting atomic symmetries -- permutation,rotation, and translation -- for physically realistic predictions. Yetmaintaining equivariance is difficult: activation functions must be modified,and each layer must handle multiple data types for different harmonic orders.We present Facet, a GNN architecture for efficient ML potentials, developedthrough systematic analysis of steerable GNNs. Our innovations includereplacing expensive multi-layer perceptrons (MLPs) for interatomic distanceswith splines, which match performance while cutting computational and memorydemands. We also introduce a general-purpose equivariant layer that mixes nodeinformation via spherical grid projection followed by standard MLPs -- fasterthan tensor products and more expressive than linear or gate layers. On theMPTrj dataset, Facet matches leading models with far fewer parameters and under10% of their training compute. On a crystal relaxation task, it runs twice asfast as MACE models. We further show SevenNet-0's parameters can be reduced byover 25% with no accuracy loss. These techniques enable more than 10x fastertraining of large-scale foundation models for ML potentials, potentiallyreshaping computational materials discovery.</description>
      <author>example@mail.com (Nicholas Miklaucic, Lai Wei, Rongzhi Dong, Nihang Fu, Sadman Sadeed Omee, Qingyang Li, Sourin Dey, Victor Fung, Jianjun Hu)</author>
      <guid isPermaLink="false">2509.08418v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>PEHRT: A Common Pipeline for Harmonizing Electronic Health Record data for Translational Research</title>
      <link>http://arxiv.org/abs/2509.08553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PEHRT，一个标准化的电子健康记录(EHR)数据整合流程，通过多机构数据整合提高转化研究的可靠性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多机构EHR数据的整合分析可以借助更大更多样化的患者群体和多种数据模态来提高转化研究的可靠性和泛化能力，但跨机构整合EHR数据面临数据异质性、语义差异和隐私问题等主要挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个标准化的流程来解决跨机构EHR数据整合的挑战，实现高效的数据整合而不需要个体级数据共享。&lt;h4&gt;方法&lt;/h4&gt;PEHRT是一个包含两个核心模块的标准化流程：(1)数据预处理和(2)表示学习。该流程将EHR数据映射到标准编码系统，使用先进的机器学习技术生成研究就绪的数据集，且与数据模型无关，可在各机构间简化执行。&lt;h4&gt;主要发现&lt;/h4&gt;PEHRT能够在不共享个体级数据的情况下有效整合多机构EHR数据，生成研究就绪的数据集，并在各种任务中展示了其效用。&lt;h4&gt;结论&lt;/h4&gt;PEHRT是一个有效的EHR数据整合解决方案，研究人员提供了完整的开源软件套件和用户友好的教程，使其能够被广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;多机构电子健康记录(EHR)数据的整合分析通过利用更大、更多样化的患者群体和整合多种数据模态，提高了转化研究的可靠性和泛化能力。然而，由于数据异质性、语义差异和隐私问题，跨机构整合EHR数据面临重大挑战。为解决这些挑战，我们引入了PEHRT，这是一个用于高效EHR数据整合的标准化流程，包含两个核心模块：(1)数据预处理和(2)表示学习。PEHRT将EHR数据映射到标准编码系统，并使用先进的机器学习技术生成研究就绪的数据集，无需个体级数据共享。我们的流程也与数据模型无关，基于我们丰富的实际经验设计，可在各机构间简化执行。我们提供了完整的开源软件套件，配有用户友好的教程，并使用来自不同医疗系统的数据在各种任务中证明了PEHRT的效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrative analysis of multi-institutional Electronic Health Record (EHR)data enhances the reliability and generalizability of translational research byleveraging larger, more diverse patient cohorts and incorporating multiple datamodalities. However, harmonizing EHR data across institutions poses majorchallenges due to data heterogeneity, semantic differences, and privacyconcerns. To address these challenges, we introduce $\textit{PEHRT}$, astandardized pipeline for efficient EHR data harmonization consisting of twocore modules: (1) data pre-processing and (2) representation learning. PEHRTmaps EHR data to standard coding systems and uses advanced machine learning togenerate research-ready datasets without requiring individual-level datasharing. Our pipeline is also data model agnostic and designed for streamlinedexecution across institutions based on our extensive real-world experience. Weprovide a complete suite of open source software, accompanied by auser-friendly tutorial, and demonstrate the utility of PEHRT in a variety oftasks using data from diverse healthcare systems.</description>
      <author>example@mail.com (Jessica Gronsbell, Vidul Ayakulangara Panickan, Chris Lin, Thomas Charlon, Chuan Hong, Doudou Zhou, Linshanshan Wang, Jianhui Gao, Shirley Zhou, Yuan Tian, Yaqi Shi, Ziming Gan, Tianxi Cai)</author>
      <guid isPermaLink="false">2509.08553v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening</title>
      <link>http://arxiv.org/abs/2509.08502v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了手性动作识别这一新任务，并提出了一种自监督方法来构建时间敏感的紧凑视频表示，该方法在多个数据集上表现优异。&lt;h4&gt;背景&lt;/h4&gt;现有视频嵌入模型在表示随时间变化的简单视觉变化方面表现不佳，而日常生活中存在大量需要理解时间变化的手性动作对。&lt;h4&gt;目的&lt;/h4&gt;开发能够敏感感知视觉随时间变化的紧凑视频表示，并构建时间感知的视频表示，使手性动作对具有线性可分性。&lt;h4&gt;方法&lt;/h4&gt;提出一种自监督适应方案，将时间敏感性注入到冻结图像特征序列中；基于自编码器构建模型，并引入受感知拉直启发的潜在空间归纳偏置。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在三个数据集上提供了紧凑但时间敏感的视频表示；优于在大型视频数据集上预训练的更大的视频模型；与现有模型结合时，在标准基准测试中提高了分类性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功构建了时间敏感的紧凑视频表示，在手性动作识别任务上表现优异，并能提升现有模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;我们的目标是开发对随时间变化的视觉变化敏感的紧凑视频表示。为了衡量这种时间敏感性，我们引入了一个新任务：手性动作识别，其中需要区分一对时间上相反的动作，如'开门vs关门'、'接近vs远离某物'、'折叠vs展开纸张'等。这类动作(i)在日常生活中频繁出现，(ii)需要理解随时间变化的简单视觉变化(物体状态、大小、空间位置、计数等)，(iii)已知许多视频嵌入表示效果不佳。我们的目标是构建时间感知的视频表示，使这些手性动作对具有线性可分性。为此，我们提出了一种自监督适应方案，将时间敏感性注入到冻结图像特征序列中。我们的模型基于自编码器，其潜在空间具有受感知拉直启发的归纳偏置。我们证明，在三个数据集上，这为所提出的任务提供了紧凑但时间敏感的视频表示。我们的方法(i)优于在大型视频数据集上预训练的更大的视频模型，(ii)与这些现有模型结合时，在标准基准测试中提高了分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Our objective is to develop compact video representations that are sensitiveto visual change over time. To measure such time-sensitivity, we introduce anew task: chiral action recognition, where one needs to distinguish between apair of temporally opposite actions, such as "opening vs. closing a door","approaching vs. moving away from something", "folding vs. unfolding paper",etc. Such actions (i) occur frequently in everyday life, (ii) requireunderstanding of simple visual change over time (in object state, size, spatialposition, count . . . ), and (iii) are known to be poorly represented by manyvideo embeddings. Our goal is to build time aware video representations whichoffer linear separability between these chiral pairs. To that end, we propose aself-supervised adaptation recipe to inject time-sensitivity into a sequence offrozen image features. Our model is based on an auto-encoder with a latentspace with inductive bias inspired by perceptual straightening. We show thatthis results in a compact but time-sensitive video representation for theproposed task across three datasets: Something-Something, EPIC-Kitchens, andCharade. Our method (i) outperforms much larger video models pre-trained onlarge-scale video datasets, and (ii) leads to an improvement in classificationperformance on standard benchmarks when combined with these existing models.</description>
      <author>example@mail.com (Piyush Bagad, Andrew Zisserman)</author>
      <guid isPermaLink="false">2509.08502v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition</title>
      <link>http://arxiv.org/abs/2509.08470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了Sparse MERIT框架，通过多任务学习同时优化语音情感识别和语音增强任务，解决了传统方法在噪声条件下的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;语音情感识别(SER)在构建情感感知语音系统中起关键作用，但在噪声条件下性能显著下降。语音增强(SE)可以提高鲁棒性，但会引入模糊情感线索的伪影，并增加计算开销。传统多任务学习模型在SE和SER任务中存在梯度干扰和表示冲突问题。&lt;h4&gt;目的&lt;/h4&gt;解决传统多任务学习在语音增强和情感识别任务中存在的梯度干扰和表示冲突问题，提高在噪声条件下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT)，一种灵活的多任务学习框架，在自监督语音表示上应用帧级专家路由。该框架包含任务特定的门控网络，动态地从共享专家池中为每帧选择专家，实现参数高效和任务自适应的表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;在MSP-Podcast语料库上的实验表明，Sparse MERIT在SER和SE任务上都优于基线模型。在-5 dB信噪比条件下，相比SE预处理基线，SER F1-macro平均提高12.0%；相比简单MTL基线提高3.4%，并且在未见噪声条件下具有统计显著性。对于SE任务，分段信噪比(SSNR)比SE预处理基线提高28.2%，比简单MTL基线提高20.0%。&lt;h4&gt;结论&lt;/h4&gt;Sparse MERIT在嘈杂环境中为情感识别和增强任务提供了稳健且可泛化的性能。&lt;h4&gt;翻译&lt;/h4&gt;语音情感识别(SER)在构建情感感知语音系统中起着关键作用，但在噪声条件下其性能会显著下降。尽管语音增强(SE)可以提高鲁棒性，但它常常会引入模糊情感线索的伪影，并为管道增加计算开销。多任务学习(MTL)通过联合优化SE和SER任务提供了一种替代方案。然而，传统的共享主干模型经常在任务间遭受梯度干扰和表示冲突。为解决这些挑战，我们提出了稀疏专家表示集成技术(Sparse MERIT)，一种灵活的MTL框架，该框架在自监督语音表示上应用帧级专家路由。Sparse MERIT集成了任务特定的门控网络，动态地从共享专家池中为每帧选择专家，实现了参数高效和任务自适应的表示学习。在MSP-Podcast语料库上的实验表明，Sparse MERIT在SER和SE任务上都一致优于基线模型。在最具挑战性的-5 dB信噪比条件下，Sparse MERIT比依赖SE预处理策略的基线平均提高SER F1-macro 12.0%，比简单MTL基线提高3.4%，在未见噪声条件下具有统计显著性。对于SE任务，Sparse MERIT将分段信噪比(SSNR)比SE预处理基线提高28.2%，比简单MTL基线提高20.0%。这些结果表明，Sparse MERIT在嘈杂环境中为情感识别和增强任务提供了稳健且可泛化的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech emotion recognition (SER) plays a critical role in buildingemotion-aware speech systems, but its performance degrades significantly undernoisy conditions. Although speech enhancement (SE) can improve robustness, itoften introduces artifacts that obscure emotional cues and adds computationaloverhead to the pipeline. Multi-task learning (MTL) offers an alternative byjointly optimizing SE and SER tasks. However, conventional shared-backbonemodels frequently suffer from gradient interference and representationalconflicts between tasks. To address these challenges, we propose the SparseMixture-of-Experts Representation Integration Technique (Sparse MERIT), aflexible MTL framework that applies frame-wise expert routing overself-supervised speech representations. Sparse MERIT incorporates task-specificgating networks that dynamically select from a shared pool of experts for eachframe, enabling parameter-efficient and task-adaptive representation learning.Experiments on the MSP-Podcast corpus show that Sparse MERIT consistentlyoutperforms baseline models on both SER and SE tasks. Under the mostchallenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERITimproves SER F1-macro by an average of 12.0% over a baseline relying on a SEpre-processing strategy, and by 3.4% over a naive MTL baseline, withstatistical significance on unseen noise conditions. For SE, Sparse MERITimproves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline andby 20.0% over the naive MTL baseline. These results demonstrate that SparseMERIT provides robust and generalizable performance for both emotionrecognition and enhancement tasks in noisy environments.</description>
      <author>example@mail.com (Jing-Tong Tzeng, Carlos Busso, Chi-Chun Lee)</author>
      <guid isPermaLink="false">2509.08470v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring</title>
      <link>http://arxiv.org/abs/2509.08392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于交通监控图像增强任务的垂直残差自编码器（VRAE）架构，通过在每个编码阶段注入感知输入的特征，有效提升了车牌识别系统在恶劣条件下的性能。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的交通监控中，车辆图像在恶劣天气、光线不足或高速运动的情况下常常受到严重的噪声和模糊影响，这些退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。&lt;h4&gt;目的&lt;/h4&gt;快速实时地恢复这些退化图像是增强识别性能的关键预处理步骤。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种垂直残差自编码器（VRAE）架构，采用了一种增强策略，使用辅助块在每个编码阶段注入感知输入的特征，以引导表示学习过程，与传统的自编码器相比，能够在整个网络中更好地保留一般信息。&lt;h4&gt;主要发现&lt;/h4&gt;在带有可见车牌的车辆图像数据集上的实验表明，VRAE方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流的方法（FB）。与相同深度的AE相比，它将PSNR提高了约20%，将NMSE降低了约50%，将SSIM提高了1%，而参数仅增加了约1%。&lt;h4&gt;结论&lt;/h4&gt;VRAE架构在交通监控图像增强任务中表现优异，能够在保持参数量增加很小的情况下显著提高图像质量指标。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的交通监控中，车辆图像在恶劣天气、光线不足或高速运动的情况下常常受到严重的噪声和模糊影响。这些退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。快速实时地恢复这些退化图像是增强识别性能的关键预处理步骤。在这项工作中，我们提出了一种用于交通监控图像增强任务的垂直残差自编码器（VRAE）架构。该方法采用了一种增强策略，该策略使用一个辅助块，在每个编码阶段注入感知输入的特征，以引导表示学习过程，与传统的自编码器相比，能够在整个网络中更好地保留一般信息。在带有可见车牌的车辆图像数据集上的实验表明，我们的方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流的方法（FB）。与相同深度的AE相比，它将PSNR提高了约20%，将NMSE降低了约50%，将SSIM提高了1%，而参数仅增加了约1%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world traffic surveillance, vehicle images captured under adverseweather, poor lighting, or high-speed motion often suffer from severe noise andblur. Such degradations significantly reduce the accuracy of license platerecognition systems, especially when the plate occupies only a small regionwithin the full vehicle image. Restoring these degraded images a fast realtimemanner is thus a crucial pre-processing step to enhance recognitionperformance. In this work, we propose a Vertical Residual Autoencoder (VRAE)architecture designed for the image enhancement task in traffic surveillance.The method incorporates an enhancement strategy that employs an auxiliaryblock, which injects input-aware features at each encoding stage to guide therepresentation learning process, enabling better general informationpreservation throughout the network compared to conventional autoencoders.Experiments on a vehicle image dataset with visible license plates demonstratethat our method consistently outperforms Autoencoder (AE), GenerativeAdversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE atthe same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%,and enhances SSIM by 1\%, while requiring only a marginal increase of roughly1\% in parameters.</description>
      <author>example@mail.com (Cuong Nguyen, Dung T. Tran, Hong Nguyen, Xuan-Vu Phan, Nam-Phong Nguyen)</author>
      <guid isPermaLink="false">2509.08392v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video</title>
      <link>http://arxiv.org/abs/2509.08376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的通用框架，用于将视频数据分解为动态运动和静态内容两个组成部分。该方法采用自监督流水线，假设更少，归纳偏差更小，能够有效学习视频的解缠结表示。&lt;h4&gt;背景&lt;/h4&gt;视频数据包含动态运动和静态内容两个重要组成部分，如何有效解缠结这两个成分是视频理解和生成领域的挑战，现有方法通常有较多假设和归纳偏差，限制了通用性。&lt;h4&gt;目的&lt;/h4&gt;提出一个新颖且通用的框架，将视频数据分解为动态运动和静态内容两个组成部分，同时减少假设和归纳偏差，实现更通用的视频表示学习。&lt;h4&gt;方法&lt;/h4&gt;使用基于Transformer的架构联合生成帧级运动和片段级内容的隐式特征；引入低比特率矢量量化作为信息瓶颈促进解缠结；将比特率控制的潜在运动和内容作为条件输入用于去噪扩散模型；通过自监督方式进行表示学习；在多种视频类型上进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在说话头视频上有效进行运动迁移和自回归运动生成；能够推广到2D卡通角色像素精灵等其他视频类型；通过低比特率矢量量化成功促进运动和内容解缠结；基于Transformer的架构能有效生成灵活的隐式特征表示。&lt;h4&gt;结论&lt;/h4&gt;该工作为自监督学习解缠结视频表示提供了新视角，通过减少假设和归纳偏差，实现了更通用的视频表示学习，有助于视频分析和生成领域的更广泛发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖且通用的框架，将视频数据分解为其动态运动和静态内容组成部分。我们提出的方法是一个自监督流水线，比之前的工作假设更少，归纳偏差更小：它利用基于Transformer的架构联合生成帧级运动和片段级内容的灵活隐式特征，并引入低比特率矢量量化作为信息瓶颈，促进解缠结并形成有意义的离散运动空间。比特率控制的潜在运动和内容被用作去噪扩散模型的条件输入，以促进自监督表示学习。我们在真实世界的说话头视频上验证了我们的解缠结表示学习框架，并进行运动迁移和自回归运动生成任务。此外，我们还展示了我们的方法可以推广到其他类型的视频数据，如2D卡通角色的像素精灵。我们的工作为自监督学习解缠结视频表示提供了新视角，有助于视频分析和生成领域的更广泛发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel and general framework to disentangle video data into itsdynamic motion and static content components. Our proposed method is aself-supervised pipeline with less assumptions and inductive biases thanprevious works: it utilizes a transformer-based architecture to jointlygenerate flexible implicit features for frame-wise motion and clip-wisecontent, and incorporates a low-bitrate vector quantization as an informationbottleneck to promote disentanglement and form a meaningful discrete motionspace. The bitrate-controlled latent motion and content are used as conditionalinputs to a denoising diffusion model to facilitate self-supervisedrepresentation learning. We validate our disentangled representation learningframework on real-world talking head videos with motion transfer andauto-regressive motion generation tasks. Furthermore, we also show that ourmethod can generalize to other types of video data, such as pixel sprites of 2Dcartoon characters. Our work presents a new perspective on self-supervisedlearning of disentangled video representations, contributing to the broaderfield of video analysis and generation.</description>
      <author>example@mail.com (Xiao Li, Qi Chen, Xiulian Peng, Kai Yu, Xie Chen, Yan Lu)</author>
      <guid isPermaLink="false">2509.08376v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training</title>
      <link>http://arxiv.org/abs/2509.08311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种胸部CT上的相似性驱动跨粒度预训练（SimCroP）框架，结合相似性驱动对齐和跨粒度融合技术，有效解决了CT影像中病变分布的空间稀疏性问题，提高了放射影像解释能力，在多尺度下游任务中表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;医学视觉语言预训练在从大量配对的放射影像和报告中学习代表性特征方面显示出巨大潜力。然而，在CT扫描中，包含复杂结构的病变分布具有空间稀疏性，且报告中不同病理描述与其在放射影像中对应区域之间的复杂关系带来了额外挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个胸部CT上的相似性驱动跨粒度预训练框架，结合相似性驱动对齐和跨粒度融合，以提高放射影像解释能力，更好地捕捉稀疏放射影像中的关键病理结构。&lt;h4&gt;方法&lt;/h4&gt;利用多模态掩码建模优化编码器以理解放射影像的低级语义；设计相似性驱动对齐机制使编码器自适应选择并与报告句子对齐；通过跨粒度融合模块整合实例级别和单词-块级别的多模态信息；在大型配对CT-报告数据集上预训练，并在五个公共数据集上的图像分类和分割任务中验证。&lt;h4&gt;主要发现&lt;/h4&gt;SimCroP框架在实验中超越了最先进的医学自监督学习方法和医学视觉语言预训练方法，有效解决了CT影像中病变分布的空间稀疏性问题，提高了多尺度下游任务的性能。&lt;h4&gt;结论&lt;/h4&gt;SimCroP通过相似性驱动对齐和跨粒度融合技术，能够更好地捕捉稀疏放射影像中的关键病理结构，在多尺度下游任务中表现出改进的性能，为医学影像分析提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学视觉语言预训练在从大量配对的放射影像和报告中学习代表性特征方面显示出巨大潜力。然而，在计算机断层扫描（CT）扫描中，包含复杂结构的病变分布具有空间稀疏性。此外，报告中每个句子内不同病理描述与其在放射影像中对应子区域之间的复杂且隐含的关系带来了额外挑战。在本文中，我们在胸部CT上提出了一个相似性驱动跨粒度预训练（SimCroP）框架，它结合相似性驱动对齐和跨粒度融合来提高放射影像解释能力。我们首先利用多模态掩码建模来优化编码器，以便从放射影像中理解精确的低级语义。然后设计相似性驱动对齐来预训练编码器，以自适应选择并与报告中每个句子相对应的正确块进行对齐。跨粒度融合模块整合实例级别和单词-块级别的多模态信息，这有助于模型更好地捕捉稀疏放射影像中的关键病理结构，从而提高多尺度下游任务的性能。SimCroP在大型配对CT-报告数据集上进行预训练，并在五个公共数据集上的图像分类和分割任务中进行了验证。实验结果表明，SimCroP优于最先进的医学自监督学习方法和医学视觉语言预训练方法。代码和模型可在 https://github.com/ToniChopp/SimCroP 获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决医学影像（特别是CT扫描）中两个关键问题：一是病变的空间分布稀疏性问题，病变包含复杂结构但分布稀疏，导致特征提取困难；二是医学影像报告中不同病理描述与影像中相应子区域之间的复杂隐含关系难以建立对应。这些问题在现实中非常重要，因为医学影像标注需要专业医生投入大量时间和精力，资源密集且负担重，这限制了深度学习在医学影像中的应用；同时，医学影像报告包含丰富的语义信息，能有效辅助影像理解，但如何有效利用这些信息一直是挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有医学视觉语言预训练方法在处理3D胸部CT时的局限性，特别是对空间稀疏性和报告结构复杂性的处理不足。作者借鉴了多模态掩码自编码器架构，使用Vision Transformer作为视觉编码器，BERT作为文本编码器。针对CT的空间稀疏性，设计了相似性驱动对齐机制，使模型能自动选择和与报告句子对齐正确的影像块；针对报告结构复杂性，引入跨粒度融合模块，整合不同粒度的多模态信息。作者还借鉴了对比学习思想，但将其应用于细粒度的句子-影像块对齐，以及掩码建模方法用于图像和文本重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; SimCroP的核心思想是通过相似性驱动对齐和跨粒度融合来改进医学影像表征学习，让模型自动学习将医学报告中的描述性句子与影像中对应的子区域对齐，同时整合全局和局部信息以更好地理解和稀疏的医学影像。整体流程包括：1)多模态掩码建模：将CT影像分割并掩码75%的块，对报告文本分词并掩码部分词，通过编码器处理未掩码内容，解码器重建被掩码内容；2)相似性驱动对齐：计算句子与影像块相似度，为每个句子选择最相似的K个块，通过对比学习损失对齐特征；3)跨粒度融合：通过全局平均池化获取实例级别特征，通过交叉注意力计算词块级别特征，融合后重建被掩码文本；4)整体优化：结合影像重建、相似性对齐和文本重建损失优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)相似性驱动对齐机制：无需人工标注自动对齐句子与影像子区域，不同于之前方法依赖全局对比学习；2)跨粒度融合：同时整合实例级别和词块级别多模态信息，优于仅使用交叉注意力的方法；3)针对医学影像特殊性的设计：专门解决CT空间稀疏性和报告结构复杂性；4)多目标学习框架：结合掩码图像建模、句子-子区域对齐和跨粒度掩码报告建模。相比之前工作，SimCroP能更好地处理医学影像的特殊性，自动建立精确的文本-影像对应关系，整合多粒度信息，从而获得更全面的表征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SimCroP通过相似性驱动对齐和跨粒度融合的创新方法，有效解决了医学影像中的空间稀疏性问题，显著提升了医学影像表征学习能力，在多个下游任务上超越了现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical vision-language pre-training shows great potential in learningrepresentative features from massive paired radiographs and reports. However,in computed tomography (CT) scans, the distribution of lesions which containintricate structures is characterized by spatial sparsity. Besides, the complexand implicit relationships between different pathological descriptions in eachsentence of the report and their corresponding sub-regions in radiographs poseadditional challenges. In this paper, we propose a Similarity-DrivenCross-Granularity Pre-training (SimCroP) framework on chest CTs, which combinessimilarity-driven alignment and cross-granularity fusion to improve radiographinterpretation. We first leverage multi-modal masked modeling to optimize theencoder for understanding precise low-level semantics from radiographs. Then,similarity-driven alignment is designed to pre-train the encoder to adaptivelyselect and align the correct patches corresponding to each sentence in reports.The cross-granularity fusion module integrates multimodal information acrossinstance level and word-patch level, which helps the model better capture keypathology structures in sparse radiographs, resulting in improved performancefor multi-scale downstream tasks. SimCroP is pre-trained on a large-scalepaired CT-reports dataset and validated on image classification andsegmentation tasks across five public datasets. Experimental resultsdemonstrate that SimCroP outperforms both cutting-edge medical self-supervisedlearning methods and medical vision-language pre-training methods. Codes andmodels are available at https://github.com/ToniChopp/SimCroP.</description>
      <author>example@mail.com (Rongsheng Wang, Fenghe Tang, Qingsong Yao, Rui Yan, Xu Zhang, Zhen Huang, Haoran Lai, Zhiyang He, Xiaodong Tao, Zihang Jiang, Shaohua Kevin Zhou)</author>
      <guid isPermaLink="false">2509.08311v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>How Far Are We from True Unlearnability?</title>
      <link>http://arxiv.org/abs/2509.08058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了现有不可学习方法在多任务场景下的局限性，从模型优化角度分析了不可学习性的本质，并提出了新的评估指标。&lt;h4&gt;背景&lt;/h4&gt;高质量数据在大模型时代至关重要，但使用未经授权的数据进行模型训练损害数据所有者利益。为解决此问题，提出了不可学习方法(UEs)，通过破坏数据训练可用性生成不可学习的示例。&lt;h4&gt;目的&lt;/h4&gt;探究如何实现真正跨任务的不可学习示例，并评估现有不可学习方法的能力边界。&lt;h4&gt;方法&lt;/h4&gt;使用简单模型架构观察干净与中毒模型的收敛差异；从损失景观分析关键参数优化路径；提出锐度感知可学习性(SAL)量化参数不可学习性；提出不可学习距离(UD)衡量数据不可学习性；使用UD对主流不可学习方法进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;在Taskonomy多任务数据集上，现有UEs在语义分割等任务上仍表现良好，未能实现真正的跨任务不可学习性；损失景观与不可学习性有密切关系。&lt;h4&gt;结论&lt;/h4&gt;现有不可学习方法在多任务场景下存在局限性，提出的UD指标有助于促进社区对不可学习方法能力边界的认识。&lt;h4&gt;翻译&lt;/h4&gt;高质量数据在大模型时代扮演着不可或缺的角色，但使用未经授权的数据进行模型训练严重损害了数据所有者的利益。为克服这一威胁，已提出几种不可学习方法，它们通过破坏数据的训练可用性来生成不可学习的示例(UEs)。显然，由于训练目的未知和现有模型的强大表征学习能力，这些数据预计对多任务模型都不可学习，即不会帮助提高模型性能。然而，出乎意料的是，我们在Taskonomy多任务数据集上发现，UEs在语义分割等任务上仍然表现良好，未能表现出跨任务的不可学习性。这种现象让我们质疑：我们距离实现真正不可学习的示例还有多远？我们尝试从模型优化的角度回答这个问题。为此，我们使用简单模型架构观察了干净模型和中毒模型的收敛过程差异。随后，从损失景观中我们发现只有部分关键参数优化路径显示出显著差异，这表明损失景观与不可学习性密切相关。因此，我们利用损失景观解释了UEs的潜在原因，并提出了锐度感知可学习性(SAL)来基于此解释量化参数的不可学习性。此外，我们提出了不可学习距离(UD)来基于干净和中毒模型中参数的SAL分布来衡量数据的不可学习性。最后，我们使用提出的UD对主流不可学习方法进行了基准测试，旨在促进社区对现有不可学习方法能力边界的认识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality data plays an indispensable role in the era of large models, butthe use of unauthorized data for model training greatly damages the interestsof data owners. To overcome this threat, several unlearnable methods have beenproposed, which generate unlearnable examples (UEs) by compromising thetraining availability of data. Clearly, due to unknown training purposes andthe powerful representation learning capabilities of existing models, thesedata are expected to be unlearnable for models across multiple tasks, i.e.,they will not help improve the model's performance. However, unexpectedly, wefind that on the multi-task dataset Taskonomy, UEs still perform well in taskssuch as semantic segmentation, failing to exhibit cross-task unlearnability.This phenomenon leads us to question: How far are we from attaining trulyunlearnable examples? We attempt to answer this question from the perspectiveof model optimization. To this end, we observe the difference in theconvergence process between clean and poisoned models using a simple modelarchitecture. Subsequently, from the loss landscape we find that only a part ofthe critical parameter optimization paths show significant differences,implying a close relationship between the loss landscape and unlearnability.Consequently, we employ the loss landscape to explain the underlying reasonsfor UEs and propose Sharpness-Aware Learnability (SAL) to quantify theunlearnability of parameters based on this explanation. Furthermore, we proposean Unlearnable Distance (UD) to measure the unlearnability of data based on theSAL distribution of parameters in clean and poisoned models. Finally, weconduct benchmark tests on mainstream unlearnable methods using the proposedUD, aiming to promote community awareness of the capability boundaries ofexisting unlearnable methods.</description>
      <author>example@mail.com (Kai Ye, Liangcai Su, Chenxiong Qian)</author>
      <guid isPermaLink="false">2509.08058v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Unidimensional semi-discrete partial optimal transport</title>
      <link>http://arxiv.org/abs/2509.08799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种解决一维部分最优传输问题的半离散形式，通过正则化方法克服了对偶函数正则性不足的困难，并验证了二次收敛率，同时展示了该方法在稳定性和计算效率方面的优势。&lt;h4&gt;背景&lt;/h4&gt;一维部分最优传输问题在风险管理、人群运动建模和点云配准等领域有广泛应用。与高维情况不同，一维情况下的对偶函数正则性较差，这给问题的求解带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;研究一维二次成本部分最优传输的半离散形式，解决对偶函数正则性不足的问题，并设计有效的数值方法。&lt;h4&gt;方法&lt;/h4&gt;引入基于沿辅助维度加厚密度的正则化过程，证明正则化对偶问题最大值以二次速率收敛到原始对偶问题最大值，并开发利用正则化函数的数值方案。&lt;h4&gt;主要发现&lt;/h4&gt;正则化对偶问题的最大值以二次速率收敛到原始对偶问题的最大值；所提出的数值方案能够有效解决一维部分传输问题；与全离散设置相比，该方法具有更好的稳定性和计算效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的正则化方法有效解决了一维部分最优传输问题中的正则性不足挑战，提供了具有二次收敛率的数值方案，并在实际应用中表现出良好的稳定性和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了一维二次成本部分最优传输的半离散形式，其中概率密度被部分传输到总质量更小的有限狄拉克质量点之和。这个问题在风险管理、人群运动建模以及用于点云配准的切片部分传输算法中自然出现。与高维设置不同，一维情况下的对偶函数正则性较差。为了克服这一困难，我们引入了一种基于沿辅助维度加厚密度的正则化过程。我们证明了正则化对偶问题的最大值收敛到原始对偶问题的最大值，收敛速度与引入的厚度成二次方关系。我们进一步提供了一个利用正则化函数的数值方案，并通过模拟验证了我们的分析，确认了二次收敛率。最后，我们比较了半离散和全离散设置，证明我们的方法在一维部分传输问题上既提高了稳定性又提高了计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the semi-discrete formulation of one-dimensional partial optimaltransport with quadratic cost, where a probability density is partiallytransported to a finite sum of Dirac masses of smaller total mass. This problemarises naturally in applications such as risk management, the modeling of crowdmotion, and sliced partial transport algorithms for point cloud registration.Unlike higher-dimensional settings, the dual functional in the unidimensionalcase exhibits reduced regularity. To overcome this difficulty, we introduce aregularization procedure based on thickening the density along an auxiliarydimension. We prove that the maximizers of the regularized dual problemconverge to those of the original dual problem, with quadratic rate in theintroduced thickness. We further provide a numerical scheme that leverages theregularized functional, and we validate our analysis with simulations thatconfirm the quadratic convergence rate. Finally, we compare the semi-discreteand fully discrete settings, demonstrating that our approach offers bothimproved stability and computational efficiency for unidimensional partialtransport problems.</description>
      <author>example@mail.com (Adrien Cances, Hugo Leclerc)</author>
      <guid isPermaLink="false">2509.08799v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Deep Unrolling of Sparsity-Induced RDO for 3D Point Cloud Attribute Coding</title>
      <link>http://arxiv.org/abs/2509.08685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于多分辨率B样条投影框架的3D点云有损属性压缩方法，通过端到端可微分的投影操作和数据驱动的优化技术实现了高效的点云属性压缩。&lt;h4&gt;背景&lt;/h4&gt;研究基于已编码的3D点云几何信息，在解码器端进行属性压缩处理。&lt;h4&gt;目的&lt;/h4&gt;解决在多分辨率B样条投影框架下的有损属性压缩问题，提高压缩效率和质量。&lt;h4&gt;方法&lt;/h4&gt;将目标连续3D属性函数投影到嵌套的B样条函数子空间中，通过变复杂度展开的率失真优化算法计算低通系数，使用ℓ1范数作为率项，并实现端到端可微分；同时采用从粗到细的预测器，以数据驱动方式优化系数调整。&lt;h4&gt;主要发现&lt;/h4&gt;通过率失真优化算法的前馈网络展开和ℓ1范数的使用，投影操作可以实现端到端可微分；从低分辨率到高分辨率的预测调整也可以通过数据驱动方式优化，提高压缩效果。&lt;h4&gt;结论&lt;/h4&gt;所提出的多分辨率B样条投影框架结合数据驱动的优化方法，能够有效实现3D点云属性的高效压缩。&lt;h4&gt;翻译&lt;/h4&gt;在解码器端可用的已编码3D点云几何信息基础上，我们研究了多分辨率B样条投影框架下的有损属性压缩问题。首先将目标连续3D属性函数投影到一系列嵌套的子空间中，其中每个子空间由选定尺度的B样条基函数及其整数平移张成。通过将率失真优化算法展开为变复杂度的前馈网络来计算投影的低通系数，其中率项是促进稀疏的ℓ1范数。因此，投影操作是端到端可微分的。对于选定的从粗到细的预测器，然后调整系数以考虑从低分辨率到高分辨率的预测，这种预测也是以数据驱动方式优化的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given encoded 3D point cloud geometry available at the decoder, we study theproblem of lossy attribute compression in a multi-resolution B-splineprojection framework. A target continuous 3D attribute function is firstprojected onto a sequence of nested subspaces $\mathcal{F}^{(p)}_{l_0}\subseteq \cdots \subseteq \mathcal{F}^{(p)}_{L}$, where$\mathcal{F}^{(p)}_{l}$ is a family of functions spanned by a B-spline basisfunction of order $p$ at a chosen scale and its integer shifts. The projectedlow-pass coefficients $F_l^*$ are computed by variable-complexity unrolling ofa rate-distortion (RD) optimization algorithm into a feed-forward network,where the rate term is the sparsity-promoting $\ell_1$-norm. Thus, theprojection operation is end-to-end differentiable. For a chosen coarse-to-finepredictor, the coefficients are then adjusted to account for the predictionfrom a lower-resolution to a higher-resolution, which is also optimized in adata-driven manner.</description>
      <author>example@mail.com (Tam Thuc Do, Philip A. Chou, Gene Cheung)</author>
      <guid isPermaLink="false">2509.08685v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration</title>
      <link>http://arxiv.org/abs/2509.08280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 12 figures, AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为E3DPC-GZSL的新方法，用于解决3D点云广义零样本语义分割中的过度自信预测问题，通过基于证据的不确定性估计器和改进的训练策略实现了对已见和未见类别的有效分类。&lt;h4&gt;背景&lt;/h4&gt;3D点云的广义零样本语义分割旨在将每个点分类为已见和未见类别，但这些模型存在显著挑战：它们倾向于做出有偏见的预测，偏向于训练过程中遇到的类别。在3D应用中，这一问题更为突出，因为训练数据规模通常小于基于图像的任务。&lt;h4&gt;目的&lt;/h4&gt;解决模型对已见类别的过度自信预测问题，且不依赖单独的已见和未见数据分类器。&lt;h4&gt;方法&lt;/h4&gt;提出E3DPC-GZSL方法，将基于证据的不确定性估计器集成到分类器中，使用动态校准堆叠因子调整预测概率，并引入新的训练策略，通过合并可学习参数和文本派生特征来改进语义空间，提高不确定性估计和模型对未见数据的优化。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在ScanNet v2和S3DIS等广义零样本语义分割数据集上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;E3DPC-GZSL方法有效解决了3D点云广义零样本语义分割中的过度自信问题，通过基于证据的不确定性估计和改进的训练策略，显著提高了对未见类别的分割性能。&lt;h4&gt;翻译&lt;/h4&gt;3D点云的广义零样本语义分割旨在将每个点分类为已见和未见类别。这些模型的一个显著挑战是它们倾向于做出有偏见的预测，通常偏向于训练过程中遇到的类别。在3D应用中，这一问题更为突出，因为训练数据的规模通常小于基于图像的任务。为解决这个问题，我们提出了一种名为E3DPC-GZSL的新方法，它减少了对已见类别的过度自信预测，而不依赖单独的已见和未见数据分类器。E3DPC-GZSL通过将基于证据的不确定性估计器集成到分类器中来解决过度自信问题。然后使用考虑逐点预测不确定性的动态校准堆叠因子来调整预测概率。此外，E3DPC-GZSL引入了一种新的训练策略，通过合并可学习参数和文本派生特征来改进语义空间，从而提高对未见数据的不确定性估计和模型优化。大量实验证明，所提出的方法在ScanNet v2和S3DIS等广义零样本语义分割数据集上达到了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决三维点云分割中的广义零样本学习(GZSL)问题，特别是模型对已见类别(训练时见过的类别)的过度自信预测偏差问题。这个问题在现实中很重要，因为自动驾驶、医疗成像等高风险应用需要精确的分割来确保安全和可靠性；现实世界场景可能包含训练时未遇到的类别，模型需要能泛化到这些新类别；三维点云数据标注成本高，难以覆盖所有可能的类别；现有方法过度依赖超参数且难以一致地应用于所有输入数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有GZSL方法的局限性，特别是对已见类别的过度自信预测问题，设计了基于证据理论的方法。他们评估了二元分类和校准堆叠两种主流方法，发现它们都严重依赖超参数且难以一致应用。作者借鉴了证据理论和主观逻辑中的不确定性估计方法，改进了校准堆叠的基本思想，使其能够动态调整而非使用固定超参数。此外，他们借鉴了语义空间对齐的思路，但引入了场景语义调优来增强特征表示。整体设计思路是利用不确定性估计实现动态校准，并通过语义调优解决数据稀缺问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用证据理论估计预测的不确定性，并基于此动态调整已见类别的预测概率；同时通过场景语义调优改善语义空间表示，解决三维零样本学习中的数据稀缺问题。整体实现流程分为三个阶段：1)训练编码器：从头开始训练编码器E提取特征向量；2)训练解码器：训练解码器D生成合成特征，通过融合可学习的场景语义向量与文本特征增强特征表示；3)训练基于证据的分类器：训练分类器C进行分割，同时训练不确定性估计器U评估预测置信度，使用三种损失函数优化。推理阶段使用编码器提取特征，分类器预测概率，不确定性估计器计算动态校准因子，应用校准调整已见类别的预测概率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)证据驱动的动态校准：基于证据理论实现动态校准因子，而非使用预定义的固定超参数；2)语义空间调优：将可学习的场景语义向量与文本特征融合，增强语义表示；3)多阶段训练策略：分阶段训练编码器、解码器和分类器；4)不确定性感知的损失函数：设计三种互补的损失函数，同时优化分割性能和不确定性估计；5)统一的处理框架：无需显式区分已见和未见类别，通过不确定性估计自动处理。相比之前工作，本文方法解决了超参数依赖问题，提高了模型在未见类别上的性能，并增强了特征表示能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于证据理论的三维点云广义零样本分割方法，通过动态校准预测概率和语义空间调优，有效解决了模型对已见类别的过度自信问题，显著提升了在未见类别上的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v39i4.32446&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized zero-shot semantic segmentation of 3D point clouds aims toclassify each point into both seen and unseen classes. A significant challengewith these models is their tendency to make biased predictions, often favoringthe classes encountered during training. This problem is more pronounced in 3Dapplications, where the scale of the training data is typically smaller than inimage-based tasks. To address this problem, we propose a novel method calledE3DPC-GZSL, which reduces overconfident predictions towards seen classeswithout relying on separate classifiers for seen and unseen data. E3DPC-GZSLtackles the overconfidence problem by integrating an evidence-based uncertaintyestimator into a classifier. This estimator is then used to adjust predictionprobabilities using a dynamic calibrated stacking factor that accounts forpointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a noveltraining strategy that improves uncertainty estimation by refining the semanticspace. This is achieved by merging learnable parameters with text-derivedfeatures, thereby improving model optimization for unseen data. Extensiveexperiments demonstrate that the proposed approach achieves state-of-the-artperformance on generalized zero-shot semantic segmentation datasets, includingScanNet v2 and S3DIS.</description>
      <author>example@mail.com (Hyeonseok Kim, Byeongkeun Kang, Yeejin Lee)</author>
      <guid isPermaLink="false">2509.08280v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</title>
      <link>http://arxiv.org/abs/2509.08104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 6 figures, conference, 7 tables, 15 formulas&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自适应概率匹配损失(APML)的新方法，用于点云预测任务中的点集比较，解决了传统损失函数的局限性，同时保持了计算效率。&lt;h4&gt;背景&lt;/h4&gt;在点云预测任务中，常用的损失函数如Chamfer距离(CD)、HyperCD和InfoCD依赖于最近邻分配，导致多对一对应关系，引起密集区域点拥塞和稀疏区域覆盖不足，且涉及非可微分操作。虽然地球移动距离(EMD)能更好地捕获结构相似性，但其三次计算复杂度限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的损失函数，避免最近邻分配问题，解决点拥塞和覆盖不足，避免非可微分操作，保持计算效率，且不需要手动调整超参数。&lt;h4&gt;方法&lt;/h4&gt;提出自适应概率匹配损失(APML)，这是一种一对一匹配的完全可微分近似方法，利用基于成对距离导出的温度缩放相似性矩阵上的Sinkhorn迭代。通过分析计算温度来保证最小分配概率，消除手动调整的需要。&lt;h4&gt;主要发现&lt;/h4&gt;APML集成到PoinTr、PCN、FoldingNet等架构中，在ShapeNet基准测试和从WiFi CSI测量生成3D人体点云的CSI2PC模型上表现优异；实现了更快的收敛速度，特别是在低密度区域有优越的空间分布，定量性能有所提升或相当，无需额外超参数搜索。&lt;h4&gt;结论&lt;/h4&gt;APML是一种有效的点云预测任务损失函数，解决了传统方法的局限性，同时保持了计算效率，在各种架构和任务上都表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;用于点云预测任务（如形状补全和生成）的深度学习模型的训练，严重依赖于衡量预测点集与真实点集之间差异的损失函数。常用的函数如Chamfer距离(CD)、HyperCD和InfoCD依赖于最近邻分配，这通常导致多对一对应关系，引起密集区域的点拥塞和稀疏区域的覆盖不足。这些损失还涉及由于索引选择导致的非可微分操作，可能影响基于梯度的优化。地球移动距离(EMD)强制一对一对应并能更有效地捕获结构相似性，但其三次计算复杂度限制了其实际应用。我们提出了自适应概率匹配损失（APML），这是一种一对一匹配的完全可微分近似方法，它利用基于成对距离导出的温度缩放相似性矩阵上的Sinkhorn迭代。我们通过分析计算温度来保证最小分配概率，消除了手动调整的需要。APML实现了接近二次的运行时间，与基于Chamfer的损失相当，并避免了非可微分操作。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云重建任务中损失函数的设计问题。现有的常用损失函数如Chamfer Distance存在点聚集、稀疏区域覆盖不佳和非可微等问题，而Earth Mover's Distance虽然几何保真度高但计算复杂度太高。这个问题很重要，因为点云是三维数据的主要表示形式，广泛应用于自动驾驶、机器人、AR/VR等领域，高质量的点云重建对这些应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有损失函数的局限性：Chamfer Distance效率高但几何细节捕捉不足，EMD几何保真度高但计算成本太高。他们借鉴了最优传输理论，特别是熵正则化的Sinkhorn算法，该算法提供了可微的软对应关系。关键创新是解决了Sinkhorn算法需要手动调整正则化参数的问题，通过分析计算自适应温度来保证最小分配概率，从而消除了手动调整的需要。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是应用最优传输原理建立点集间的软概率对应关系，通过自适应温度机制控制分配锐度，无需手动调整。整体流程包括：1)计算预测点集与真实点集间的欧氏距离矩阵；2)应用自适应Softmax生成初始概率分配；3)双向匹配并对称化得到初始软分配矩阵；4)通过Sinkhorn归一化使矩阵近似双随机；5)计算期望匹配成本作为最终损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自适应概率匹配机制，完全可微且近似一对一匹配；2)自适应温度选择机制，通过闭式调度自动调整分配锐度；3)平衡计算效率与几何保真度，以近二次复杂度实现EMD级别的质量。相比之前的工作，APML避免了Chamfer Distance的点聚集问题，解决了EMD的高计算复杂度问题，并且不需要像其他CD变体那样手动调整多个参数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; APML通过结合最优传输理论和自适应温度机制，提供了一种高效、可微的损失函数，显著提升了3D点云重建任务中的几何保真度，同时保持了与Chamfer Distance相当的计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training deep learning models for point cloud prediction tasks such as shapecompletion and generation depends critically on loss functions that measurediscrepancies between predicted and ground-truth point sets. Commonly usedfunctions such as Chamfer Distance (CD), HyperCD, and InfoCD rely onnearest-neighbor assignments, which often induce many-to-one correspondences,leading to point congestion in dense regions and poor coverage in sparseregions. These losses also involve non-differentiable operations due to indexselection, which may affect gradient-based optimization. Earth Mover Distance(EMD) enforces one-to-one correspondences and captures structural similaritymore effectively, but its cubic computational complexity limits its practicaluse. We propose the Adaptive Probabilistic Matching Loss (APML), a fullydifferentiable approximation of one-to-one matching that leverages Sinkhorniterations on a temperature-scaled similarity matrix derived from pairwisedistances. We analytically compute the temperature to guarantee a minimumassignment probability, eliminating manual tuning. APML achieves near-quadraticruntime, comparable to Chamfer-based losses, and avoids non-differentiableoperations. When integrated into state-of-the-art architectures (PoinTr, PCN,FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)that generates 3D human point clouds from WiFi CSI measurements, APM lossyields faster convergence, superior spatial distribution, especially inlow-density regions, and improved or on-par quantitative performance withoutadditional hyperparameter search. The code is available at:https://github.com/apm-loss/apml.</description>
      <author>example@mail.com (Sasan Sharifipour, Constantino Álvarez Casado, Mohammad Sabokrou, Miguel Bordallo López)</author>
      <guid isPermaLink="false">2509.08104v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions</title>
      <link>http://arxiv.org/abs/2509.07209v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ASME IDETC/CIE 2025 (DETC2025-168977). Dataset  availability: BlendedNet dataset is openly available at Harvard Dataverse  (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VJT9EP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BlendedNet是一个包含999个混合翼身(BWB)几何形状的公开空气动力学数据集，每个几何形状在约9种飞行条件下进行模拟，生成8830个RANS案例。研究团队还提出了一个端到端代理框架用于点对点空气动力学预测，实验显示在不同BWB上的表面预测误差较低。&lt;h4&gt;背景&lt;/h4&gt;非常规空气动力学配置面临数据稀缺问题，传统方法难以有效处理。&lt;h4&gt;目的&lt;/h4&gt;创建一个大型公开数据集解决非常规配置的数据稀缺问题，并开发一个数据驱动的代理模型框架用于空气动力学设计。&lt;h4&gt;方法&lt;/h4&gt;通过采样几何设计参数和飞行条件生成数据集；使用排列不变的PointNet回归器从表面点云预测几何参数，然后在预测参数和飞行条件下调节特征线性调制(FiLM)网络，预测点对点系数Cp、Cfx和Cfz。&lt;h4&gt;主要发现&lt;/h4&gt;BlendedNet数据集包含丰富的表面量信息，可用于研究升力和阻力；提出的端到端代理框架在不同BWB上的表面预测表现出低误差。&lt;h4&gt;结论&lt;/h4&gt;BlendedNet解决了非常规空气动力学配置的数据稀缺问题，并促进了数据驱动代理模型在空气动力学设计研究中的应用。&lt;h4&gt;翻译&lt;/h4&gt;BlendedNet是一个公开可用的空气动力学数据集，包含999个混合翼身(BWB)几何形状。每个几何形状在约九种飞行条件下进行模拟，产生了8830个使用Spalart-Allmaras模型的收敛RANS案例，每个案例有900万至1400万个单元。该数据集通过采样几何设计参数和飞行条件生成，包括研究升力和阻力所需的详细点对点表面量。我们还介绍了一个用于点对点空气动力学预测的端到端代理框架。该流程首先使用排列不变的PointNet回归器从采样表面点云预测几何参数，然后在预测的参数和飞行条件下调节特征线性调制(FiLM)网络，以预测点对点系数Cp、Cfx和Cfz。实验表明，在不同BWB上的表面预测误差较低。BlendedNet解决了非常规配置的数据稀缺问题，并促进了空气动力学设计的数据驱动代理模型研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; BlendedNet is a publicly available aerodynamic dataset of 999 blended wingbody (BWB) geometries. Each geometry is simulated across about nine flightconditions, yielding 8830 converged RANS cases with the Spalart-Allmaras modeland 9 to 14 million cells per case. The dataset is generated by samplinggeometric design parameters and flight conditions, and includes detailedpointwise surface quantities needed to study lift and drag. We also introducean end-to-end surrogate framework for pointwise aerodynamic prediction. Thepipeline first uses a permutation-invariant PointNet regressor to predictgeometric parameters from sampled surface point clouds, then conditions aFeature-wise Linear Modulation (FiLM) network on the predicted parameters andflight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.Experiments show low errors in surface predictions across diverse BWBs.BlendedNet addresses data scarcity for unconventional configurations andenables research on data-driven surrogate modeling for aerodynamic design.</description>
      <author>example@mail.com (Nicholas Sung, Steven Spreizer, Mohamed Elrefaie, Kaira Samuel, Matthew C. Jones, Faez Ahmed)</author>
      <guid isPermaLink="false">2509.07209v2</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>3D and 4D World Modeling: A Survey</title>
      <link>http://arxiv.org/abs/2509.07996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at  https://github.com/worldbench/survey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是首个专门针对3D和4D世界建模与生成的全面综述，建立了精确的定义和结构化分类法，总结了相关数据集和评估指标，并讨论了应用、挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;世界建模已成为AI研究的基石，但先前研究主要关注2D图像和视频数据的生成方法，忽视了利用原生3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的工作。同时，'世界模型'缺乏标准化定义和分类法，导致文献中碎片化且有时不一致的主张。&lt;h4&gt;目的&lt;/h4&gt;提出首个专门针对3D和4D世界建模与生成的全面综述，建立精确的定义，引入结构化分类法，系统总结适用于3D/4D设置的数据集和评估指标，讨论实际应用，确定开放挑战，突出有希望的研究方向，为推进该领域提供连贯且基础的参考。&lt;h4&gt;方法&lt;/h4&gt;提出基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）的方法的分类法，并系统总结现有文献，提供可访问的文献汇编资源。&lt;h4&gt;主要发现&lt;/h4&gt;需要更多关注3D和4D表示方法；建立了世界模型的精确定义和分类框架；总结了适用于3D/4D设置的数据集和评估指标；识别了实际应用、开放挑战和有希望的研究方向。&lt;h4&gt;结论&lt;/h4&gt;该综述为3D和4D世界建模领域提供了连贯且基础的参考，有助于推进该领域的发展，相关文献系统总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;翻译&lt;/h4&gt;世界建模已成为AI研究的基石，使智能体能够理解、表示和预测他们所处的动态环境。虽然先前的工作主要强调2D图像和视频数据的生成方法，但他们忽视了利用原生3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的快速增长的研究。同时，'世界模型'缺乏标准化定义和分类法，导致文献中碎片化且有时不一致的主张。本综述通过提出首个专门针对3D和4D世界建模与生成的全面综述来解决这些差距。我们建立了精确的定义，引入了涵盖基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）方法的分类法，并系统总结了适用于3D/4D设置的数据集和评估指标。我们进一步讨论了实际应用，确定了开放挑战，并突出了有希望的研究方向，旨在为推进该领域提供连贯且基础的参考。现有文献的系统总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决世界建模领域缺乏标准化定义和分类体系的问题，以及现有工作过度关注2D数据而忽视原生3D/4D表示的局限性。这个问题很重要，因为世界建模是AI研究的基石，能帮助智能体理解、表示和预测动态环境；原生3D/4D表示（如RGB-D、占用网格、LiDAR点云）提供显式几何和物理基础，对自动驾驶、机器人等安全关键系统至关重要；缺乏统一框架导致文献碎片化，阻碍领域发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作为综述文章，作者首先明确'世界模型'和'3D/4D世界建模'的精确定义，然后提出分层分类法，根据表示模态（VideoGen、OccGen和LiDARGen）对现有方法进行分类，并系统总结专门针对3D/4D场景的数据集和评估指标。作者借鉴了多种生成模型（VAEs、GANs、扩散模型、自回归模型）作为算法基础，整合了计算机视觉、机器人和人工智能领域的相关工作，但专注于3D/4D表示而非2D数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是建立精确的'世界模型'定义，提出结构化分类法涵盖VideoGen、OccGen和LiDARGen方法，系统总结3D/4D场景的数据集和评估指标。整体流程是：第2节介绍基本概念和定义；第3节详细介绍分层分类法；第4节总结数据集和评估指标；第5节回顾实际应用；第6节讨论挑战和未来方向。这种结构化组织方式帮助读者系统理解3D/4D世界建模的全貌。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门针对3D/4D世界建模进行全面综述；建立精确的'世界模型'定义；提出涵盖VideoGen、OccGen和LiDARGen的结构化分类法；提供3D/4D场景数据集和评估指标的全面覆盖。相比之前工作，这篇综述明确聚焦原生3D/4D表示而非2D数据，提供更系统化的分类和更全面的文献覆盖，填补了现有文献的知识空白。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过建立精确的定义、提出结构化分类法、系统总结数据集和评估指标，为3D和4D世界建模领域提供了首个全面且基础的参考框架，填补了现有文献的空白并指明了未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; World modeling has become a cornerstone in AI research, enabling agents tounderstand, represent, and predict the dynamic environments they inhabit. Whileprior work largely emphasizes generative methods for 2D image and video data,they overlook the rapidly growing body of work that leverages native 3D and 4Drepresentations such as RGB-D imagery, occupancy grids, and LiDAR point cloudsfor large-scale scene modeling. At the same time, the absence of a standardizeddefinition and taxonomy for ``world models'' has led to fragmented andsometimes inconsistent claims in the literature. This survey addresses thesegaps by presenting the first comprehensive review explicitly dedicated to 3Dand 4D world modeling and generation. We establish precise definitions,introduce a structured taxonomy spanning video-based (VideoGen),occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, andsystematically summarize datasets and evaluation metrics tailored to 3D/4Dsettings. We further discuss practical applications, identify open challenges,and highlight promising research directions, aiming to provide a coherent andfoundational reference for advancing the field. A systematic summary ofexisting literature is available at https://github.com/worldbench/survey</description>
      <author>example@mail.com (Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyuan Gao, Song Wang, Linfeng Li, Liang Pan, Yong Liu, Jianke Zhu, Wei Tsang Ooi, Steven C. H. Hoi, Ziwei Liu)</author>
      <guid isPermaLink="false">2509.07996v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference</title>
      <link>http://arxiv.org/abs/2509.08542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ASP-DAC 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BitROM是一种基于CiROM的新型加速器，通过与BitNet的1.58位量化模型协同设计，解决了传统CiROM加速器在扩展到大型语言模型时面临的硅面积限制问题，实现了边缘设备上的高效LLM推理。&lt;h4&gt;背景&lt;/h4&gt;CiROM加速器通过消除运行时权重更新为CNN提供卓越能效，但在扩展到大型语言模型时受到参数量巨大的根本限制。即使是最小的LLaMA-7B模型在先进CMOS节点中也需要超过1,000 cm²的硅面积。&lt;h4&gt;目的&lt;/h4&gt;开发首个基于CiROM的加速器BitROM，通过协同设计克服LLM部署的面积限制，实现边缘设备上实用高效的LLM推理。&lt;h4&gt;方法&lt;/h4&gt;BitROM引入三种创新：1)双向ROM阵列，每个晶体管存储两个三进制权重；2)针对三进制权重计算优化的三模式局部累加器；3)集成解码刷新(DR) eDRAM支持片上KV缓存管理。此外还集成了LoRA适配器实现高效迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在65nm CMOS中评估，BitROM实现20.8 TOPS/W能效和4,967 kB/mm²比特密度，比先前的数字CiROM设计在面积效率上提高10倍。DR eDRAM减少43.6%的外部DRAM访问。&lt;h4&gt;结论&lt;/h4&gt;BitROM成功解决了CiROM加速器扩展到LLMs的硅面积限制问题，通过创新设计实现了边缘设备上的高效LLM推理，为边缘计算中的LLM部署提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;只读存储器计算(CiROM)加速器通过消除运行时权重更新为CNN提供了卓越的能效。然而，它们扩展到大型语言模型(LLMs)受到参数量巨大的根本限制。值得注意的是，即使是在先进的CMOS节点中，LLaMA系列中最小的模型LLaMA-7B也需要超过1,000 cm²的硅面积。本文提出了BitROM，这是首个基于CiROM的加速器，通过与BitNet的1.58位量化模型协同设计克服了这一限制，实现了边缘设备上实用高效的LLM推理。BitROM引入了三个关键创新：1)每个晶体管存储两个三进制权重的双向ROM阵列；2)针对三进制权重计算优化的三模式局部累加器；3)支持片上KV缓存管理的集成解码刷新(DR) eDRAM，显著减少了解码期间的外部内存访问。此外，BitROM集成了基于LoRA的适配器，实现各种下游任务的高效迁移学习。在65nm CMOS中评估，BitROM实现了20.8 TOPS/W和4,967 kB/mm²的比特密度，比先前的数字CiROM设计在面积效率上提高了10倍。此外，DR eDRAM减少了43.6%的外部DRAM访问，进一步增强了边缘应用中LLM的部署效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energyefficiency for CNNs by eliminating runtime weight updates. However, theirscalability to Large Language Models (LLMs) is fundamentally constrained bytheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMAseries - demands more than 1,000 cm2 of silicon area even in advanced CMOSnodes. This paper presents BitROM, the first CiROM-based accelerator thatovercomes this limitation through co-design with BitNet's 1.58-bit quantizationmodel, enabling practical and efficient LLM inference at the edge. BitROMintroduces three key innovations: 1) a novel Bidirectional ROM Array thatstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulatoroptimized for ternary-weight computations; and 3) an integrated Decode-Refresh(DR) eDRAM that supports on-die KV-cache management, significantly reducingexternal memory access during decoding. In addition, BitROM integratesLoRA-based adapters to enable efficient transfer learning across variousdownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bitdensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency overprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%reduction in external DRAM access, further enhancing deployment efficiency forLLMs in edge applications.</description>
      <author>example@mail.com (Wenlun Zhang, Xinyu Li, Shimpei Ando, Kentaro Yoshioka)</author>
      <guid isPermaLink="false">2509.08542v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Label Transfer Learning in Non-Stationary Data Streams</title>
      <link>http://arxiv.org/abs/2509.08181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE International Conference on Data Mining (ICDM) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对多标签数据流中的概念漂移问题，提出了两种新的迁移学习方法，通过标签间知识转移提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;在非平稳环境中，多标签数据流中的标签概念可能会发生漂移，这种漂移可能是独立发生的，也可能是与其他标签相关的。尽管在相关标签之间转移知识可以加速适应，但针对数据流的多标签迁移学习研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出两种新的迁移学习方法，解决多标签数据流中的概念漂移问题，通过标签间知识转移提高预测性能。&lt;h4&gt;方法&lt;/h4&gt;BR-MARLENE：利用源数据流和目标数据流中不同标签的知识进行多标签分类；BRPW-MARLENE：在此基础上，通过显式建模和转移成对的标签依赖关系来增强学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;两种方法在非平稳环境中都优于最先进的多标签流方法，标签间知识转移对于提高预测性能是有效的。&lt;h4&gt;结论&lt;/h4&gt;提出的方法通过标签间知识转移有效解决了多标签数据流中的概念漂移问题，在非平稳环境中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;在非平稳环境中，多标签数据流中的标签概念往往会发生漂移，无论是独立发生还是与其他标签相关。在相关标签之间转移知识可以加速适应，然而针对数据流的多标签迁移学习研究仍然有限。为此，我们提出了两种新颖的迁移学习方法：BR-MARLENE利用源数据流和目标数据流中不同标签的知识进行多标签分类；BRPW-MARLENE在此基础上通过显式建模和转移成对的标签依赖关系来增强学习性能。全面的实验表明，两种方法在非平稳环境中都优于最先进的多标签流方法，证明了标签间知识转移对于提高预测性能的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Label concepts in multi-label data streams often experience drift innon-stationary environments, either independently or in relation to otherlabels. Transferring knowledge between related labels can accelerateadaptation, yet research on multi-label transfer learning for data streamsremains limited. To address this, we propose two novel transfer learningmethods: BR-MARLENE leverages knowledge from different labels in both sourceand target streams for multi-label classification; BRPW-MARLENE builds on thisby explicitly modelling and transferring pairwise label dependencies to enhancelearning performance. Comprehensive experiments show that both methodsoutperform state-of-the-art multi-label stream approaches in non-stationaryenvironments, demonstrating the effectiveness of inter-label knowledge transferfor improved predictive performance.</description>
      <author>example@mail.com (Honghui Du, Leandro Minku, Aonghus Lawlor, Huiyu Zhou)</author>
      <guid isPermaLink="false">2509.08181v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments</title>
      <link>http://arxiv.org/abs/2509.08176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the 2020 IEEE International Conference on Data Mining  (ICDM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MARLINE的新方法，用于处理非平稳环境中的概念漂移问题，即使在源概念和目标概念不匹配的情况下也能从多个数据源中获益。&lt;h4&gt;背景&lt;/h4&gt;概念漂移是影响数据流挖掘系统预测性能的主要问题。现有方法假设至少有一个源模型与目标概念相似，这在许多现实场景中不成立。&lt;h4&gt;目的&lt;/h4&gt;开发一种即使源概念和目标概念不匹配也能从多个数据源知识中受益的方法。&lt;h4&gt;方法&lt;/h4&gt;提出MARLINE（Multi-source mApping with tRansfer LearnIng for Non-stationary Environments）方法，通过将目标概念投影到每个源概念空间，使多个源子分类器作为集成的一部分为目标概念预测做出贡献。&lt;h4&gt;主要发现&lt;/h4&gt;在多个合成和真实世界数据集上的实验表明，MARLINE比几种最先进的数据流学习方法更准确。&lt;h4&gt;结论&lt;/h4&gt;MARLINE能够有效处理非平稳环境中的概念漂移问题，即使源概念和目标概念不匹配也能提高预测性能。&lt;h4&gt;翻译&lt;/h4&gt;概念漂移是在线学习中的一个主要问题，因为它影响数据流挖掘系统的预测性能。最近的研究开始探索来自不同数据源的数据流作为处理目标领域中概念漂移的策略。这些方法假设至少有一个源模型代表与目标概念相似的概念，这在许多现实场景中可能不成立。在本文中，我们提出了一种名为MARLINE（Multi-source mApping with tRansfer LearnIng for Non-stationary Environments）的新方法。MARLINE即使在源概念和目标概念不匹配的情况下，也能从非平稳环境中的多个数据源知识中受益。这是通过将目标概念投影到每个源概念的空间实现的，使多个源子分类器能够作为集成的一部分为目标概念的预测做出贡献。在几个合成和真实世界数据集上的实验表明，MARLINE比几种最先进的数据流学习方法更准确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICDM50108.2020.00021&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concept drift is a major problem in online learning due to its impact on thepredictive performance of data stream mining systems. Recent studies havestarted exploring data streams from different sources as a strategy to tackleconcept drift in a given target domain. These approaches make the assumptionthat at least one of the source models represents a concept similar to thetarget concept, which may not hold in many real-world scenarios. In this paper,we propose a novel approach called Multi-source mApping with tRansfer LearnIngfor Non-stationary Environments (MARLINE). MARLINE can benefit from knowledgefrom multiple data sources in non-stationary environments even when source andtarget concepts do not match. This is achieved by projecting the target conceptto the space of each source concept, enabling multiple source sub-classifiersto contribute towards the prediction of the target concept as part of anensemble. Experiments on several synthetic and real-world datasets show thatMARLINE was more accurate than several state-of-the-art data stream learningapproaches.</description>
      <author>example@mail.com (Honghui Du, Leandro Minku, Huiyu Zhou)</author>
      <guid isPermaLink="false">2509.08176v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles</title>
      <link>http://arxiv.org/abs/2509.08026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为SI-EDTL的两阶段群体智能集成深度迁移学习模型，用于无人机图像中的多车辆检测。该模型结合了多种预训练特征提取器和分类器，并通过加权平均进行聚合，在AU-AIR无人机数据集上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;随着无人机技术的发展，无人机图像中的车辆检测变得越来越重要。然而，在无人机图像中准确检测多种类型的车辆仍然是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效准确的车辆检测模型，能够在无人机图像中识别多种类型的车辆（汽车、面包车、卡车、公共汽车等）。&lt;h4&gt;方法&lt;/h4&gt;1. 采用两阶段群体智能集成深度迁移学习框架；2. 结合三个预训练的Faster R-CNN特征提取器模型（InceptionV3、ResNet50、GoogLeNet）；3. 集成五个迁移分类器（KNN、SVM、MLP、C4.5、朴素贝叶斯），形成15个基础学习器；4. 使用加权平均方法聚合这些学习器的结果；5. 应用鲸鱼优化算法对超参数进行优化，以平衡准确性、精确度和召回率；6. 在MATLAB R2020b中实现，并采用并行处理技术。&lt;h4&gt;主要发现&lt;/h4&gt;SI-EDTL模型在AU-AIR无人机数据集上表现优于现有的车辆检测方法，能够准确识别多种类型的车辆。&lt;h4&gt;结论&lt;/h4&gt;SI-EDTL模型通过结合多种深度学习技术和集成学习方法，有效地提高了无人机图像中多车辆检测的准确性和效率。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了SI-EDTL，一种用于检测无人机图像中多辆车辆的两阶段群体智能集成深度迁移学习模型。它将三个预训练的Faster R-CNN特征提取器模型（InceptionV3、ResNet50、GoogLeNet）与五个迁移分类器（KNN、SVM、MLP、C4.5、朴素贝叶斯）相结合，形成15个不同的基础学习器。这些学习器通过加权平均进行聚合，将区域分类为汽车、面包车、卡车、公共汽车或背景。使用鲸鱼优化算法对超参数进行优化，以平衡准确性、精确度和召回率。该模型在MATLAB R2020b中实现并使用并行处理，在AU-AIR无人机数据集上优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/cpe.6726&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deeptransfer learning model for detecting multiple vehicles in UAV images. Itcombines three pre-trained Faster R-CNN feature extractor models (InceptionV3,ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5,Na\"ive Bayes), resulting in 15 different base learners. These are aggregatedvia weighted averaging to classify regions as Car, Van, Truck, Bus, orbackground. Hyperparameters are optimized with the whale optimization algorithmto balance accuracy, precision, and recall. Implemented in MATLAB R2020b withparallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAVdataset.</description>
      <author>example@mail.com (Zeinab Ghasemi Darehnaei, Mohammad Shokouhifar, Hossein Yazdanjouei, S. M. J. Rastegar Fatemi)</author>
      <guid isPermaLink="false">2509.08026v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
      <link>http://arxiv.org/abs/2509.07627v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为LSMTCR的新型框架，用于从头生成全长、表位特异性的T细胞受体（TCR）αβ链。该框架通过分离特异性和约束学习，实现了基于表位条件的配对全长TCR生成。&lt;h4&gt;背景&lt;/h4&gt;设计全长、表位特异性的TCR αβ链具有挑战性，原因包括巨大的序列空间、数据偏差以及免疫遗传约束的不完整建模。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展的多架构框架，能够从头生成基于表位条件的配对全长TCR。&lt;h4&gt;方法&lt;/h4&gt;LSMTCR采用三个主要组件：1) 扩散增强的BERT编码器学习时间条件的表位表示；2) 条件GPT解码器在CDR3β上预训练并迁移到CDR3α，在跨模态条件下生成链特异性CDR3，并通过温度控制多样性；3) 基因感知的Transformer通过预测V/J使用来组装完整的αβ序列，确保免疫遗传保真度。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上，LSMTCR实现了比基线更高的预测结合能力，更忠实地恢复了位置和长度语法，提供了优越且可通过温度调节的多样性。迁移学习显著改善了α链生成的预测结合、长度真实性和多样性。从已知或从头生成的CDR3进行全长组装保留了k-mer谱，与参考序列的编辑距离低。在配对αβ共建模中获得了更高的pTM/ipTM值。&lt;h4&gt;结论&lt;/h4&gt;LSMTCR仅从表位输入就能输出多样化、基因背景化的全长TCR设计，支持高通量筛选和迭代优化。&lt;h4&gt;翻译&lt;/h4&gt;设计全长、表位特异性的TCR αβ链仍然具有挑战性，这是由于巨大的序列空间、数据偏差以及免疫遗传约束的不完整建模。我们提出了LSMTCR，一种可扩展的多架构框架，该框架分离了特异性和约束学习，从而能够从头、基于表位条件地生成配对的全长TCR。扩散增强的BERT编码器学习时间条件的表位表示；条件GPT解码器在CDR3β上预训练并迁移到CDR3α，在跨模态条件下生成链特异性的CDR3，并通过温度控制的多样性；基因感知的Transformer通过预测V/J使用来组装完整的αβ序列，确保免疫遗传保真度。在GLIPH、TEP、MIRA、McPAS和我们整理的数据集上，LSMTCR在大多数数据集上实现了比基线更高的预测结合，更忠实地恢复了位置和长度语法，并提供了优越的、可通过温度调节的多样性。对于α链生成，迁移学习在预测结合、长度真实性和多样性方面优于代表性方法。从已知或从头生成的CDR3进行全长组装保留了k-mer谱，产生了与参考序列的低编辑距离，并且在与表位的配对αβ共建模中，比单链设置获得了更高的pTM/ipTM。LSMTCR仅从表位输入就能输出多样化、基因背景化的全长TCR设计， enabling高通量筛选和迭代优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing full-length, epitope-specific TCR $\alpha\beta$ remains challengingdue to vast sequence space, data biases and incomplete modeling ofimmunogenetic constraints. We present LSMTCR, a scalable multi-architectureframework that separates specificity from constraint learning to enable denovo, epitope-conditioned generation of paired, full-length TCRs. Adiffusion-enhanced BERT encoder learns time-conditioned epitoperepresentations; conditional GPT decoders, pretrained on CDR3$\beta$ andtransferred to CDR3$\alpha$, generate chain-specific CDR3s under cross-modalconditioning with temperature-controlled diversity; and a gene-awareTransformer assembles complete $\alpha\beta$ sequences by predicting V/J usageto ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and ourcurated dataset, LSMTCR achieves higher predicted binding than baselines onmost datasets, more faithfully recovers positional and length grammars, anddelivers superior, temperature-tunable diversity. For $\alpha$-chaingeneration, transfer learning improves predicted binding, length realism anddiversity over representative methods. Full-length assembly from known or denovo CDR3s preserves k-mer spectra, yields low edit distances to references,and, in paired $\alpha\beta$ co-modelling with epitope, attains higher pTM/ipTMthan single-chain settings. LSMTCR outputs diverse, gene-contextualized,full-length TCR designs from epitope input alone, enabling high-throughputscreening and iterative optimization.</description>
      <author>example@mail.com (Ruihao Zhang, Xiao Liu)</author>
      <guid isPermaLink="false">2509.07627v2</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Privacy Preservation and Reducing Analysis Time with Federated Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis</title>
      <link>http://arxiv.org/abs/2509.08018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为联邦迁移学习（FTL）的新方法，结合数字孪生技术和联邦学习应用于CT扫描分析。FTL利用预训练模型和节点间知识传输，解决了数据隐私、计算资源有限和数据异构性问题，在非独立同分布数据环境下表现优于传统联邦学习方法。&lt;h4&gt;背景&lt;/h4&gt;数字孪生（DT）技术和联邦学习（FL）在生物医学图像分析领域具有巨大潜力，特别是在CT扫描方面。然而，该领域面临数据隐私保护、计算资源有限以及数据异质性等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于数字孪生的CT扫描分析范式，即联邦迁移学习（FTL），以解决数据隐私、计算资源限制和数据异构性问题，同时实现云服务器与数字孪生CT扫描仪之间的实时协作。&lt;h4&gt;方法&lt;/h4&gt;联邦迁移学习（FTL）方法利用预训练模型和节点间的知识传输。研究团队将FTL应用于异构CT扫描数据集，并使用收敛时间、模型准确率、精确率、召回率、F1分数和混淆矩阵来评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;FTL方法在非独立同分布（non-IID）数据环境下表现优于传统联邦学习（FL）和聚类联邦学习（CFL）方法，具有更高的精确度、准确率、召回率和F1分数。&lt;h4&gt;结论&lt;/h4&gt;FTL能够改善基于数字孪生的CT扫描分析中的决策制定，为安全高效的医疗图像分析提供新可能，促进隐私保护，并为精准医疗和智能医疗系统的应用开辟新途径。&lt;h4&gt;翻译&lt;/h4&gt;数字孪生（DT）技术和联邦学习（FL）的应用对改变生物医学图像分析领域具有巨大潜力，特别是在计算机断层扫描（CT）扫描方面。本文提出了联邦迁移学习（FTL）作为一种新的基于数字孪生的CT扫描分析范式。FTL利用预训练模型和节点间的知识传输来解决数据隐私、有限计算资源和数据异构性问题。所提出的框架允许云服务器和数字孪生CT扫描仪之间的实时协作，同时保护患者身份。我们将FTL方法应用于异构CT扫描数据集，并使用收敛时间、模型准确率、精确率、召回率、F1分数和混淆矩阵评估模型性能。实验表明，与传统的联邦学习和聚类联邦学习（CFL）方法相比，FTL具有更好的精确度、准确率、召回率和F1分数。该技术在数据非独立同分布（non-IID）的环境中特别有益，为医疗诊断提供了可靠、高效和安全的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of Digital Twin (DT) technology and Federated Learning (FL)has great potential to change the field of biomedical image analysis,particularly for Computed Tomography (CT) scans. This paper presents FederatedTransfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm.FTL uses pre-trained models and knowledge transfer between peer nodes to solveproblems such as data privacy, limited computing resources, and dataheterogeneity. The proposed framework allows real-time collaboration betweencloud servers and Digital Twin-enabled CT scanners while protecting patientidentity. We apply the FTL method to a heterogeneous CT scan dataset and assessmodel performance using convergence time, model accuracy, precision, recall, F1score, and confusion matrix. It has been shown to perform better thanconventional FL and Clustered Federated Learning (CFL) methods with betterprecision, accuracy, recall, and F1-score. The technique is beneficial insettings where the data is not independently and identically distributed(non-IID), and it offers reliable, efficient, and secure solutions for medicaldiagnosis. These findings highlight the possibility of using FTL to improvedecision-making in digital twin-based CT scan analysis, secure and efficientmedical image analysis, promote privacy, and open new possibilities forapplying precision medicine and smart healthcare systems.</description>
      <author>example@mail.com (Avais Jan, Qasim Zia, Murray Patterson)</author>
      <guid isPermaLink="false">2509.08018v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>AdsQA: Towards Advertisement Video Understanding</title>
      <link>http://arxiv.org/abs/2509.08621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV-2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出使用广告视频作为测试平台来评估大型语言模型对视觉内容之外的理解能力，创建了AdsQA基准数据集，提出了ReAd-R模型，并在测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在通用人工智能方面取得重大进展，领域特定问题促使这些模型不断学习更深层次的专业知识。然而，收集具有高质量和意外性的数据具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;扩展知识型大型语言模型的多样化专业应用，通过广告视频这一具有挑战性的测试平台，探测模型对视觉领域客观物理内容之外的理解能力。&lt;h4&gt;方法&lt;/h4&gt;使用广告视频作为测试平台，提出ReAd-R模型(一种Deepseek-R1风格的强化学习模型)，该模型通过反思问题并通过奖励驱动的优化生成答案。创建了AdsQA基准数据集，包含1,544个广告视频、10,962个片段，总计22.7小时，提供5个挑战性任务。&lt;h4&gt;主要发现&lt;/h4&gt;在AdsQA基准上对14个顶级大型语言模型进行测试，ReAd-R模型以明显优势超越具有长链推理能力的强大竞争对手，达到最先进水平。&lt;h4&gt;结论&lt;/h4&gt;使用广告视频作为测试平台能够有效评估大型语言模型对视觉内容之外的理解能力，ReAd-R模型在这一任务上表现出色，为未来模型在专业应用方面的发展提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在通用人工智能方面取得了重大进展。同时，越来越多的领域特定问题，如数学和编程，促使这些通用模型通过学习更深层次的专业知识不断进化。因此，现在是时候进一步扩展知识型大型语言模型的多样化专业应用，尽管收集具有意外性和信息量高的高质量数据具有挑战性。在本文中，我们提出使用广告视频作为具有挑战性的测试平台，来探测大型语言模型对视觉领域客观物理内容之外的理解能力。我们的动机是充分利用广告视频线索丰富和信息密集的特点，例如营销逻辑、说服策略和受众参与度。我们的贡献有三方面：(1)据我们所知，这是首次使用广告视频设计任务来评估大型语言模型。我们贡献了AdsQA，这是一个具有挑战性的广告视频问答基准，源自1,544个广告视频和10,962个片段，总计22.7小时，提供5个挑战性任务。(2)我们提出了ReAd-R，一种Deepseek-R1风格的强化学习模型，该模型反思问题并通过奖励驱动的优化生成答案。(3)我们在AdsQA上对14个顶级大型语言模型进行了基准测试，我们的ReAd-R以明显优势超越了配备长链推理能力的强大竞争对手，达到了最先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have taken a great step towards AGI. Meanwhile,an increasing number of domain-specific problems such as math and programmingboost these general-purpose models to continuously evolve via learning deeperexpertise. Now is thus the time further to extend the diversity of specializedapplications for knowledgeable LLMs, though collecting high quality data withunexpected and informative tasks is challenging. In this paper, we propose touse advertisement (ad) videos as a challenging test-bed to probe the ability ofLLMs in perceiving beyond the objective physical content of common visualdomain. Our motivation is to take full advantage of the clue-rich andinformation-dense ad videos' traits, e.g., marketing logic, persuasivestrategies, and audience engagement. Our contribution is three-fold: (1) To ourknowledge, this is the first attempt to use ad videos with well-designed tasksto evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmarkderived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model thatreflects on questions, and generates answers via reward-driven optimization.(3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achievesthe state-of-the-art outperforming strong competitors equipped with long-chainreasoning capabilities by a clear margin.</description>
      <author>example@mail.com (Xinwei Long, Kai Tian, Peng Xu, Guoli Jia, Jingxuan Li, Sa Yang, Yihua Shao, Kaiyan Zhang, Che Jiang, Hao Xu, Yang Liu, Jiaheng Ma, Bowen Zhou)</author>
      <guid isPermaLink="false">2509.08621v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization</title>
      <link>http://arxiv.org/abs/2509.08578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了MAESTRO模型，一种多模态自适应集成方法，用于流感发病率的稳健预测，通过融合监测数据、网络搜索趋势和气象数据，结合先进的频谱-时间架构，在流感预测任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要，但传统方法在处理多源数据和时间序列复杂性方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够融合多模态数据并利用频谱-时间建模技术的流感预测模型，提高预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出MAESTRO模型，包含：1)多模态数据自适应融合；2)时间序列分解为季节性和趋势成分；3)混合特征增强管道(Transformer编码器、Mamba状态空间模型、多尺度时间卷积、频域分析模块)；4)跨通道注意力机制；5)时间投影头进行序列预测；6)可选预测不确定性量化器。&lt;h4&gt;主要发现&lt;/h4&gt;在11年香港流感数据上评估，MAESTRO实现了0.956的最先进R平方值，展示了优越的模型拟合和准确性，消融实验证实了多模态融合和频谱-时间组件的关键贡献。&lt;h4&gt;结论&lt;/h4&gt;MAESTRO提供了一个模块化、可重现的统一框架，展示了先进的频谱-时间建模与多模态数据融合对稳健流行病学预测的重要协同作用，已公开可用以促进部署和扩展。&lt;h4&gt;翻译&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。为此，我们提出了MAESTRO，一种用于频谱-时间稳健优化的多模态自适应集成。MAESTRO通过自适应融合多模态输入（包括监测数据、网络搜索趋势和气象数据）并利用全面的频谱-时间架构来实现稳健性。该模型首先将时间序列分解为季节性和趋势成分。然后，这些成分通过混合特征增强管道进行处理，该管道结合了基于Transformer的编码器、用于长程依赖关系的Mamba状态空间模型、多尺度时间卷积和频域分析模块。跨通道注意力机制进一步整合了不同数据模态的信息。最后，时间投影头执行序列到序列的预测，并带有可选估计器来量化预测不确定性。在超过11年的香港流感数据（排除COVID-19期间）上评估，MAESTRO显示出强大的竞争性能，展示了优越的模型拟合和相对准确性，实现了最先进的0.956的R平方值。大量消融实验证实了多模态融合和频谱-时间成分的重要贡献。我们的模块化和可重现的管道已公开可用，以促进部署和扩展到其他地区和病原体。我们公开的管道提供了一个强大、统一的框架，展示了先进的频谱-时间建模和多模态数据融合对稳健流行病学预测的关键协同作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely and robust influenza incidence forecasting is critical for publichealth decision-making. To address this, we present MAESTRO, a Multi-modalAdaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achievesrobustness by adaptively fusing multi-modal inputs-including surveillance, websearch trends, and meteorological data-and leveraging a comprehensivespectro-temporal architecture. The model first decomposes time series intoseasonal and trend components. These are then processed through a hybridfeature enhancement pipeline combining Transformer-based encoders, a Mambastate-space model for long-range dependencies, multi-scale temporalconvolutions, and a frequency-domain analysis module. A cross-channel attentionmechanism further integrates information across the different data modalities.Finally, a temporal projection head performs sequence-to-sequence forecasting,with an optional estimator to quantify prediction uncertainty. Evaluated onover 11 years of Hong Kong influenza data (excluding the COVID-19 period),MAESTRO shows strong competitive performance, demonstrating a superior modelfit and relative accuracy, achieving a state-of-the-art R-square of 0.956.Extensive ablations confirm the significant contributions of both multi-modalfusion and the spectro-temporal components. Our modular and reproduciblepipeline is made publicly available to facilitate deployment and extension toother regions and pathogens.Our publicly available pipeline presents apowerful, unified framework, demonstrating the critical synergy of advancedspectro-temporal modeling and multi-modal data fusion for robustepidemiological forecasting.</description>
      <author>example@mail.com (Hong Liu)</author>
      <guid isPermaLink="false">2509.08578v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models</title>
      <link>http://arxiv.org/abs/2509.08538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MESH是一个新基准，用于系统评估视频大型模型(LVMs)中的幻觉问题。它采用问答框架和自下而上的评估方法，与人类视频理解过程一致。研究发现LVMs在基本识别方面表现良好，但在处理复杂场景时容易产生幻觉。&lt;h4&gt;背景&lt;/h4&gt;大型视频模型(LVMs)建立在大型语言模型和视觉模块的语义能力基础上，通过整合时间信息来更好地理解动态视频内容。尽管取得了进展，但LVMs容易出现幻觉问题，产生不准确或无关的描述。&lt;h4&gt;目的&lt;/h4&gt;当前视频幻觉评估基准严重依赖视频内容的手动分类，忽略了人类自然解释视频时所依赖的感知过程。需要开发一个系统评估LVMs中幻觉的基准。&lt;h4&gt;方法&lt;/h4&gt;引入MESH基准，使用问答框架，包含二进制和多选格式，并包含目标和陷阱实例。采用自下而上的方法，评估基本对象、从粗到细的主体特征以及主体-动作对，这种方法与人类视频理解过程保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;MESH为识别视频中的幻觉提供了有效且全面的方法。评估显示，LVMs在识别基本对象和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，在较长视频中，LVMs的幻觉易感性显著增加。&lt;h4&gt;结论&lt;/h4&gt;MESH基准提供了一个系统评估LVMs幻觉的方法，表明LVMs在处理复杂视频内容时仍有改进空间。&lt;h4&gt;翻译&lt;/h4&gt;大型视频模型(LVMs)建立在大型语言模型的语义能力和视觉模块的基础上，通过整合时间信息来更好地理解动态视频内容。尽管取得了进展，LVMs仍然容易出现幻觉，产生不准确或无关的描述。当前用于视频幻觉评估的基准严重依赖视频内容的手动分类，忽略了人类自然解释视频时所依赖的感知过程。我们引入了MESH，这是一个旨在系统评估LVMs中幻觉的基准。MESH使用问答框架，包含二进制和多选格式，并融入目标和陷阱实例。它采用自下而上的方法，评估基本对象、从粗到细的主体特征以及主体-动作对，与人类视频理解过程保持一致。我们证明MESH为识别视频中的幻觉提供了有效且全面的方法。我们的评估显示，虽然LVMs在识别基本对象和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，在较长视频中，它们对幻觉的易感性显著增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Video Models (LVMs) build on the semantic capabilities of LargeLanguage Models (LLMs) and vision modules by integrating temporal informationto better understand dynamic video content. Despite their progress, LVMs areprone to hallucinations-producing inaccurate or irrelevant descriptions.Current benchmarks for video hallucination depend heavily on manualcategorization of video content, neglecting the perception-based processesthrough which humans naturally interpret videos. We introduce MESH, a benchmarkdesigned to evaluate hallucinations in LVMs systematically. MESH uses aQuestion-Answering framework with binary and multi-choice formats incorporatingtarget and trap instances. It follows a bottom-up approach, evaluating basicobjects, coarse-to-fine subject features, and subject-action pairs, aligningwith human video understanding. We demonstrate that MESH offers an effectiveand comprehensive approach for identifying hallucinations in videos. Ourevaluations show that while LVMs excel at recognizing basic objects andfeatures, their susceptibility to hallucinations increases markedly whenhandling fine details or aligning multiple actions involving various subjectsin longer videos.</description>
      <author>example@mail.com (Garry Yang, Zizhe Chen, Man Hon Wong, Haoyu Lei, Yongqiang Chen, Zhenguo Li, Kaiwen Zhou, James Cheng)</author>
      <guid isPermaLink="false">2509.08538v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Mamba for Hyperspectral Object Tracking</title>
      <link>http://arxiv.org/abs/2509.08265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HyMamba的新型高光谱目标跟踪网络，通过状态空间模块统一光谱、跨深度和时间建模，实现了高光谱目标跟踪的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;高光谱目标跟踪因高光谱图像丰富的光谱信息和细粒度材料区分能力而具有很大潜力，尤其在具有挑战性的场景中。现有方法通过将高光谱数据转换为伪彩色图像或结合模态融合策略取得进展，但往往无法捕捉内在的光谱信息、时间依赖性和跨深度交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有高光谱跟踪器的局限性，提出一种能够有效捕捉光谱信息、时间依赖性和跨深度交互的新型高光谱目标跟踪网络。&lt;h4&gt;方法&lt;/h4&gt;通过状态空间模块(SSMs)统一光谱、跨深度和时间建模；核心是光谱状态集成(SSI)模块，实现光谱特征的渐进式细化和传播；在每个SSI中嵌入高光谱Mamba(HSM)模块，通过三个方向扫描的SSM同步学习和空间信息；基于SSI和HSM构建联合特征，并通过与原始光谱特征交互来增强这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;在七个基准数据集上的广泛实验表明，HyMamba实现了最先进的性能，在HOTC2020数据集上达到73.0%的AUC分数和96.3%的DP@20分数。&lt;h4&gt;结论&lt;/h4&gt;HyMamba是一种有效的高光谱目标跟踪方法，能够有效捕捉光谱信息、时间依赖性和跨深度交互，代码将在https://github.com/lgao001/HyMamba发布。&lt;h4&gt;翻译&lt;/h4&gt;高光谱目标跟踪由于高光谱图像中丰富的光谱信息和细粒度的材料区分能力而具有巨大潜力，在具有挑战性的场景中特别有益。虽然现有的高光谱跟踪器通过将高光谱数据转换为伪彩色图像或结合模态融合策略已经取得进展，但它们往往无法捕捉内在的光谱信息、时间依赖性和跨深度交互。为解决这些局限性，本文提出了一种配备Mamba的新型高光谱目标跟踪网络(HyMamba)。它通过状态空间模块统一了光谱、跨深度和时间建模。HyMamba的核心在于光谱状态集成(SSI)模块，该模块能够通过跨深度和时间光谱信息实现光谱特征的渐进式细化和传播。嵌入在每个SSI中的是高光谱Mamba(HSM)模块，它通过三个方向扫描的SSM同步学习空间和光谱信息。基于SSI和HSM，HyMamba从伪彩色和高光谱输入构建联合特征，并通过与从原始高光谱图像提取的光谱特征交互来增强它们。在七个基准数据集上进行的广泛实验表明，HyMamba实现了最先进的性能。例如，它在HOTC2020数据集上达到了73.0%的AUC分数和96.3%的DP@20分数。代码将在https://github.com/lgao001/HyMamba发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral object tracking holds great promise due to the rich spectralinformation and fine-grained material distinctions in hyperspectral images,which are beneficial in challenging scenarios. While existing hyperspectraltrackers have made progress by either transforming hyperspectral data intofalse-color images or incorporating modality fusion strategies, they often failto capture the intrinsic spectral information, temporal dependencies, andcross-depth interactions. To address these limitations, a new hyperspectralobject tracking network equipped with Mamba (HyMamba), is proposed. It unifiesspectral, cross-depth, and temporal modeling through state space modules(SSMs). The core of HyMamba lies in the Spectral State Integration (SSI)module, which enables progressive refinement and propagation of spectralfeatures with cross-depth and temporal spectral information. Embedded withineach SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatialand spectral information synchronously via three directional scanning SSMs.Based on SSI and HSM, HyMamba constructs joint features from false-color andhyperspectral inputs, and enhances them through interaction with originalspectral features extracted from raw hyperspectral images. Extensiveexperiments conducted on seven benchmark datasets demonstrate that HyMambaachieves state-of-the-art performance. For instance, it achieves 73.0\% of theAUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code willbe released at https://github.com/lgao001/HyMamba.</description>
      <author>example@mail.com (Long Gao, Yunhe Zhang, Yan Jiang, Weiying Xie, Yunsong Li)</author>
      <guid isPermaLink="false">2509.08265v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs</title>
      <link>http://arxiv.org/abs/2509.08016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/hyungjin-chung/VPS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VPS是一种创新的推理时方法，通过并行处理视频帧的不同子集并聚合结果，解决了VideoLLMs在处理长视频序列时面临的计算成本和性能下降问题，无需额外训练即可提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;Video Large Language Models (VideoLLMs)面临一个关键瓶颈：增加输入帧数以捕捉细粒度时间细节会导致不可接受的计算成本和长上下文长度引起的性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为Video Parallel Scaling (VPS)的推理时方法，旨在扩展模型的感知带宽而不增加其上下文窗口。&lt;h4&gt;方法&lt;/h4&gt;VPS通过运行多个并行推理流来实现，每个流处理视频帧的独特、不重叠子集。通过聚合这些互补流的输出概率，VPS集成了比单次处理更丰富的视觉信息集合。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，这种方法通过利用不相关的视觉证据，有效地收缩了Chinchilla缩放定律，从而在不增加额外训练的情况下提高了性能。在各种模型架构和规模(2B-32B)上，以及在Video-MME和EventHallusion等基准测试中的大量实验表明，VPS持续且显著地提高了性能。&lt;h4&gt;结论&lt;/h4&gt;VPS比其他并行替代方案(如Self-consistency)扩展性更好，并且与其他解码策略互补，为增强VideoLLMs的时间推理能力提供了一种内存高效且强大的框架。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(VideoLLMs)面临一个关键瓶颈：增加输入帧数以捕捉细粒度时间细节会导致不可接受的计算成本和长上下文长度引起的性能下降。我们引入了视频并行缩放(VPS)，一种推理时方法，可以在不增加模型上下文窗口的情况下扩展其感知带宽。VPS通过运行多个并行推理流来实现，每个流处理视频帧的独特、不重叠子集。通过聚合这些互补流的输出概率，VPS集成了比单次处理更丰富的视觉信息集合。我们从理论上证明，这种方法通过利用不相关的视觉证据，有效地收缩了Chinchilla缩放定律，从而在不增加额外训练的情况下提高了性能。在各种模型架构和规模(2B-32B)以及在Video-MME和EventHallusion等基准测试上的大量实验表明，VPS持续且显著地提高了性能。它的扩展性比其他并行替代方案(如Self-consistency)更好，并且与其他解码策略互补，为增强VideoLLMs的时间推理能力提供了一种内存高效且强大的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VideoLLMs) face a critical bottleneck:increasing the number of input frames to capture fine-grained temporal detailleads to prohibitive computational costs and performance degradation from longcontext lengths. We introduce Video Parallel Scaling (VPS), an inference-timemethod that expands a model's perceptual bandwidth without increasing itscontext window. VPS operates by running multiple parallel inference streams,each processing a unique, disjoint subset of the video's frames. By aggregatingthe output probabilities from these complementary streams, VPS integrates aricher set of visual information than is possible with a single pass. Wetheoretically show that this approach effectively contracts the Chinchillascaling law by leveraging uncorrelated visual evidence, thereby improvingperformance without additional training. Extensive experiments across variousmodel architectures and scales (2B-32B) on benchmarks such as Video-MME andEventHallusion demonstrate that VPS consistently and significantly improvesperformance. It scales more favorably than other parallel alternatives (e.g.Self-consistency) and is complementary to other decoding strategies, offering amemory-efficient and robust framework for enhancing the temporal reasoningcapabilities of VideoLLMs.</description>
      <author>example@mail.com (Hyungjin Chung, Hyelin Nam, Jiyeon Kim, Hyojun Go, Byeongjun Park, Junho Kim, Joonseok Lee, Seongsu Ha, Byung-Hoon Kim)</author>
      <guid isPermaLink="false">2509.08016v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Learning Turbulent Flows with Generative Models: Super-resolution, Forecasting, and Sparse Flow Reconstruction</title>
      <link>http://arxiv.org/abs/2509.08752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究结合神经算子与生成模型，解决了传统神经算子在处理湍流结构时过度平滑的问题，实现了在三个湍流挑战任务中的高性能：时空超分辨率、预测和稀疏流重构。&lt;h4&gt;背景&lt;/h4&gt;神经算子作为动力系统的替代品很有前景，但当使用标准L2损失训练时，往往会过度平滑小尺度的湍流结构。&lt;h4&gt;目的&lt;/h4&gt;结合算子学习和生成建模来克服神经算子过度平滑湍流结构的限制，解决三个传统神经算子失败的湍流挑战：时空超分辨率、预测和稀疏流重构。&lt;h4&gt;方法&lt;/h4&gt;使用对抗训练的神经算子(adv-NO)处理Schlieren射流超分辨率和3D均匀各向同性湍流预测；使用条件生成模型从稀疏粒子跟踪测速样输入重构圆柱尾流。&lt;h4&gt;主要发现&lt;/h4&gt;对于Schlieren射流超分辨率，adv-NO将能量谱误差降低15倍，同时保留尖锐梯度；对于3D均匀各向同性湍流，adv-NO仅用160个时间步训练能准确预测五个涡旋翻转时间，推理速度比基线方法快114倍；条件生成模型能从高度稀疏输入推断出相位对齐和统计正确的完整3D速度和压力场。&lt;h4&gt;结论&lt;/h4&gt;这些进展实现了低成本下的准确重构和预测，使实验和计算流体力学中的近实时分析和控制成为可能。&lt;h4&gt;翻译&lt;/h4&gt;神经算子作为动力系统的替代品很有前景，但当使用标准L2损失训练时，往往会过度平滑小尺度的湍流结构。在这里，我们表明将算子学习与生成建模相结合可以克服这一限制。我们考虑了三个传统神经算子失败的湍流流体的实际挑战：时空超分辨率、预测和稀疏流重构。对于Schlieren射流超分辨率，对抗训练的神经算子(adv-NO)将能量谱误差降低了15倍，同时保留了尖锐梯度，且以神经算子级的推理成本。对于3D均匀各向同性湍流，adv-NO仅使用单个轨迹的160个时间步进行训练，能够准确预测五个涡旋翻转时间，比基线基于扩散的预测器快114倍，实现近实时滚动。对于从高度稀疏的粒子跟踪测速样输入重构圆柱尾流，条件生成模型推断出相位对齐和统计正确的完整3D速度和压力场。这些进展实现了低成本下的准确重构和预测，使实验和计算流体力学中的近实时分析和控制成为可能。请访问我们的项目页面：https://vivekoommen.github.io/Gen4Turb/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决神经算子在处理湍流时的频谱偏差问题，即传统神经算子倾向于过度平滑精细尺度的湍流结构。这个问题在湍流研究中非常重要，因为湍流具有多尺度特性，精细结构对理解湍流动力学至关重要。高精度湍流模拟计算成本极高，而实验中通常只能获得稀疏测量数据，因此需要高效的方法来准确模拟、预测和重建湍流场，这对实验和计算流体力学中的实时分析和控制具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有神经算子的局限性，指出它们存在频谱偏差问题，即使能表示高频的架构也可能无法学习到这些内容。作者借鉴了生成模型的工作，如生成对抗网络、扩散模型和流匹配技术，这些方法通过学习数据的完整概率结构，自然减轻了确定性算子的低频偏差。基于这些观察，作者设计了对抗性训练的神经算子（adv-NO），结合了算子学习和生成建模的优点，以克服传统方法在处理湍流时的频谱偏差问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将生成建模与神经算子学习相结合，以克服传统神经算子在处理湍流时的频谱偏差问题。整体实现流程包括：1) 使用时间条件UNet作为基础架构，结合L1损失、感知损失和对抗性损失进行训练；2) 对于超分辨率任务，从低分辨率低帧率输入映射到高分辨率高帧率输出；3) 对于预测任务，使用历史窗口预测未来状态，然后自回归地预测更远的未来；4) 对于重建任务，使用条件生成模型从稀疏观测重建完整流场，确保相位对齐和统计匹配。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 对抗性训练的神经算子（adv-NO），在保持高效计算的同时提高湍流精细结构的保真度；2) 条件生成模型用于零样本流场重建，从稀疏PTV样式的输入重建完整的3D速度和压力场；3) 在非常有限的数据下有效工作（预测仅用160个快照，重建仅用150个快照）。相比之前的工作，本文的不同之处在于：1) 提供了对神经算子频谱偏差的理论解释；2) 在计算效率与保真度之间取得了更好的平衡；3) 提出了统一的框架解决湍流中的三个不同挑战；4) 通过傅里叶分析显示adv-NO自然发展出高通滤波行为，减轻了频谱偏差。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将生成建模与神经算子学习相结合，开发出对抗性训练的神经算子，在保持高效计算的同时显著提高了湍流模拟、预测和重建中精细尺度结构的保真度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural operators are promising surrogates for dynamical systems but whentrained with standard L2 losses they tend to oversmooth fine-scale turbulentstructures. Here, we show that combining operator learning with generativemodeling overcomes this limitation. We consider three practical turbulent-flowchallenges where conventional neural operators fail: spatio-temporalsuper-resolution, forecasting, and sparse flow reconstruction. For Schlierenjet super-resolution, an adversarially trained neural operator (adv-NO) reducesthe energy-spectrum error by 15x while preserving sharp gradients at neuraloperator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NOtrained on only 160 timesteps from a single trajectory forecasts accurately forfive eddy-turnover times and offers 114x wall-clock speed-up at inference thanthe baseline diffusion-based forecasters, enabling near-real-time rollouts. Forreconstructing cylinder wake flows from highly sparse Particle TrackingVelocimetry-like inputs, a conditional generative model infers full 3D velocityand pressure fields with correct phase alignment and statistics. These advancesenable accurate reconstruction and forecasting at low compute cost, bringingnear-real-time analysis and control within reach in experimental andcomputational fluid mechanics. See our project page:https://vivekoommen.github.io/Gen4Turb/</description>
      <author>example@mail.com (Vivek Oommen, Siavash Khodakarami, Aniruddha Bora, Zhicheng Wang, George Em Karniadakis)</author>
      <guid isPermaLink="false">2509.08752v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.08374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了InsFusion方法，用于解决多视角相机和激光雷达三维物体检测中的误差累积问题，通过从原始和融合特征中提取提案并查询原始特征，结合注意力机制，有效减轻了误差累积的影响，并在nuScenes数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;三维物体检测是自动驾驶和智能交通的关键组成部分，但在基本特征提取、透视变换和特征融合过程中，噪声和误差会逐渐累积。&lt;h4&gt;目的&lt;/h4&gt;解决三维物体检测过程中噪声和误差逐渐累积的问题，提高检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出InsFusion方法，从原始和融合特征中提取提案，利用这些提案查询原始特征减轻累积误差影响，并结合注意力机制应用于原始特征进一步减轻误差累积。&lt;h4&gt;主要发现&lt;/h4&gt;InsFusion与各种先进的基线方法兼容，在nuScenes数据集上实现了三维物体检测的新最先进性能。&lt;h4&gt;结论&lt;/h4&gt;InsFusion通过有效的特征提取和查询机制，成功减轻了三维物体检测中的误差累积问题，提升了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;多视角相机和激光雷达的三维物体检测是自动驾驶和智能交通的关键组成部分。然而，在基本特征提取、透视变换和特征融合过程中，噪声和误差会逐渐累积。为解决这一问题，我们提出了InsFusion，它可以从原始特征和融合特征中提取提案，并利用这些提案查询原始特征，从而减轻累积误差的影响。此外，通过将注意力机制应用于原始特征，进一步减轻了累积误差的影响。在nuScenes数据集上的实验表明，InsFusion与各种先进的基线方法兼容，并在三维物体检测方面取得了新的最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D目标检测中多传感器（LiDAR和相机）融合过程中噪声和误差累积的问题。这个问题在自动驾驶和智能交通系统中至关重要，因为误差累积会导致检测精度下降，影响系统的可靠性和安全性。在特征提取、透视变换和融合的每个步骤都可能引入误差，如深度估计错误、坐标变换不准确等，这些误差会累积并最终影响整体性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有BEV（鸟瞰图）LiDAR-相机融合模型的局限性，发现误差在处理管道中逐渐累积。他们借鉴了BEVFormer系列的可变形注意力机制、IS-Fusion的实例引导融合方法以及FocalFormer3D的多阶段热图技术。但不同于这些方法专注于改进单个组件，作者设计了一种新范式，同时利用原始特征和融合特征，通过从两者中提取提案并查询原始特征来减轻误差累积，而不是试图优化管道中的特定环节。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过从原始特征和融合特征中提取提案，并利用这些提案查询原始特征，从而减轻多传感器融合过程中累积的误差。实现流程分为三步：1) 从相机特征、LiDAR特征和融合特征中提取查询；2) 通过模态特定的线性变换器将三组查询对齐到共享空间；3) 使用可变形变压器解码器，将三个核心特征源（原始相机、原始LiDAR和融合特征）作为键值对，通过注意力机制精炼实例特征。这一过程重复2次（实验确定的最佳层数），确保充分利用多源信息同时避免过度拟合局部噪声。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出通用的实例级融合范式InsFusion；2) 同时从原始特征和融合特征中提取提案；3) 使用多源查询设计而非单一特征源；4) 应用于原始特征的注意力机制；5) 与现有方法兼容且只需微调。相比之前工作，不同之处在于：传统方法主要关注改进管道中的单个组件（如减少深度估计误差），而InsFusion利用原始特征减轻噪声；传统方法依赖单一特征源，而InsFusion同时使用原始和融合特征；InsFusion具有更好的通用性，可集成到各种现有模型中，且计算开销小（FPS仅下降6.6%-9.3%）而性能提升显著（mAP提高1.0%-1.1%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InsFusion提出了一种创新的实例级LiDAR-相机融合方法，通过同时利用原始特征和融合特征中的提案来查询原始特征，有效解决了多传感器融合过程中的误差累积问题，显著提升了3D目标检测性能，同时保持了与现有方法的兼容性和较低的计算开销。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional Object Detection from multi-view cameras and LiDAR is acrucial component for autonomous driving and smart transportation. However, inthe process of basic feature extraction, perspective transformation, andfeature fusion, noise and error will gradually accumulate. To address thisissue, we propose InsFusion, which can extract proposals from both raw andfused features and utilizes these proposals to query the raw features, therebymitigating the impact of accumulated errors. Additionally, by incorporatingattention mechanisms applied to the raw features, it thereby mitigates theimpact of accumulated errors. Experiments on the nuScenes dataset demonstratethat InsFusion is compatible with various advanced baseline methods anddelivers new state-of-the-art performance for 3D object detection.</description>
      <author>example@mail.com (Zhongyu Xia, Hansong Yang, Yongtao Wang)</author>
      <guid isPermaLink="false">2509.08374v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models</title>
      <link>http://arxiv.org/abs/2509.06609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICKG 2025. 8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对图异常检测(GAD)中的泛化能力进行了全面综述，追溯了GAD中泛化能力的演变，建立了系统性分类，并对现有方法进行了评述，最后指出了开放性挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;图异常检测近年来在社交媒体、电子商务等基于图的应用中受到广泛关注，但大多数方法假设训练和测试分布相同且针对特定任务定制，导致在真实场景中适应性有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有GAD方法在数据分布变化和新应用中训练样本稀少等场景下的局限性，提高GAD模型的泛化能力，并提供对GAD中泛化能力的系统性理解。&lt;h4&gt;方法&lt;/h4&gt;通过迁移学习利用相关领域知识增强检测性能，或开发'一专多能'的GAD基础模型实现跨应用泛化；追溯GAD中泛化演变，形式化问题设置，建立系统性分类，并对现有广义GAD方法进行全面回顾。&lt;h4&gt;主要发现&lt;/h4&gt;目前对GAD中泛化的系统性理解仍然缺乏；现有方法主要通过迁移学习和基础模型来提高泛化能力，以应对真实世界场景中的挑战。&lt;h4&gt;结论&lt;/h4&gt;识别了当前GAD泛化领域的开放性挑战，提出了未来研究方向，以启发这一新兴领域的后续研究。&lt;h4&gt;翻译&lt;/h4&gt;图异常检测(GAD)近年来在识别社交媒体和电子商务等多种基于图的应用中的恶意样本方面引起了越来越多的关注。然而，大多数GAD方法假设训练和测试分布相同，并且针对特定任务定制，导致在真实场景中的适应性有限，如数据分布变化和新应用中训练样本稀少。为了解决这些局限性，最近的工作专注于通过利用相关领域的知识来增强检测性能的迁移学习来提高GAD模型的泛化能力，或者开发能够跨多个应用泛化的'一专多能'GAD基础模型。由于对GAD中泛化的系统性理解仍然缺乏，在本文中，我们提供了对GAD中泛化的全面回顾。我们首先追溯了GAD中泛化的演变，并形式化了问题设置，这进一步导致了我们的系统性分类。基于这种精细分类法，我们对现有的广义GAD方法进行了最新和全面的回顾。最后，我们确定了当前的开放性挑战，并提出了未来方向，以启发这一新兴领域的未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection (GAD) has attracted increasing attention in recentyears for identifying malicious samples in a wide range of graph-basedapplications, such as social media and e-commerce. However, most GAD methodsassume identical training and testing distributions and are tailored tospecific tasks, resulting in limited adaptability to real-world scenarios suchas shifting data distributions and scarce training samples in new applications.To address the limitations, recent work has focused on improving thegeneralization capability of GAD models through transfer learning thatleverages knowledge from related domains to enhance detection performance, ordeveloping "one-for-all" GAD foundation models that generalize across multipleapplications. Since a systematic understanding of generalization in GAD isstill lacking, in this paper, we provide a comprehensive review ofgeneralization in GAD. We first trace the evolution of generalization in GADand formalize the problem settings, which further leads to our systematictaxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensivereview is conducted for the existing generalized GAD methods. Finally, weidentify current open challenges and suggest future directions to inspirefuture research in this emerging field.</description>
      <author>example@mail.com (Junjun Pan, Yu Zheng, Yue Tan, Yixin Liu)</author>
      <guid isPermaLink="false">2509.06609v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
  <item>
      <title>Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study</title>
      <link>http://arxiv.org/abs/2509.05553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型语言模型是否真正理解概念或仅识别模式的问题，提出双向推理作为测试真正理解的标准，并开发了对比微调方法来解决认知专业化问题。&lt;h4&gt;背景&lt;/h4&gt;人工智能领域的基本问题：大型语言模型是否真正理解概念，还是仅仅识别模式。&lt;h4&gt;目的&lt;/h4&gt;提出双向推理作为测试真正理解能力的标准，真正的理解应该允许自然可逆性。&lt;h4&gt;方法&lt;/h4&gt;开发对比微调方法，使用三种类型的示例进行训练：保持语义意义的正面示例、具有不同语义的负面示例、前向方向混淆示例。&lt;h4&gt;主要发现&lt;/h4&gt;当前语言模型存在'认知专业化'现象：在前向任务上微调会提高任务表现但降低双向推理能力；对比微调方法成功实现了双向推理，同时保持前向任务能力。&lt;h4&gt;结论&lt;/h4&gt;双向推理既是评估真正理解的理论框架，也是开发更强大AI系统的实用训练方法。&lt;h4&gt;翻译&lt;/h4&gt;本研究解决了人工智能中的一个基本问题：大型语言模型是否真正理解概念或仅仅识别模式。作者提出双向推理（能够在没有明确反向训练的情况下双向应用转换的能力）作为真正理解的测试。他们认为真正的理解应该自然允许可逆性。例如，能够将变量名如userIndex更改为i的模型也应该能够推断出i代表用户索引，而无需反向训练。研究人员测试了当前的语言模型，并发现了他们所称的认知专业化：当模型在前向任务上微调时，这些任务的表现有所提高，但它们的双向推理能力显著下降。为解决此问题，他们开发了对比微调方法，使用三种类型的示例训练模型：保持语义意义的正面示例、具有不同语义的负面示例和前向方向混淆示例。这种方法旨在培养更深层次的理解，而非表面模式识别，并允许反向能力在没有明确反向训练的情况下自然发展。他们的实验证明对比微调成功实现了双向推理，能够在保持前向任务能力的同时实现强大的反向性能。作者得出结论，双向推理既是评估真正理解的理论框架，也是开发更强大AI系统的实用训练方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research addresses a fundamental question in AI: whether large languagemodels truly understand concepts or simply recognize patterns. The authorspropose bidirectional reasoning,the ability to apply transformations in bothdirections without being explicitly trained on the reverse direction, as a testfor genuine understanding. They argue that true comprehension should naturallyallow reversibility. For example, a model that can change a variable name likeuserIndex to i should also be able to infer that i represents a user indexwithout reverse training. The researchers tested current language models anddiscovered what they term cognitive specialization: when models are fine-tunedon forward tasks, their performance on those tasks improves, but their abilityto reason bidirectionally becomes significantly worse. To address this issue,they developed Contrastive Fine-Tuning (CFT), which trains models using threetypes of examples: positive examples that maintain semantic meaning, negativeexamples with different semantics, and forward-direction obfuscation examples.This approach aims to develop deeper understanding rather than surface-levelpattern recognition and allows reverse capabilities to develop naturallywithout explicit reverse training. Their experiments demonstrated that CFTsuccessfully achieved bidirectional reasoning, enabling strong reverseperformance while maintaining forward task capabilities. The authors concludethat bidirectional reasoning serves both as a theoretical framework forassessing genuine understanding and as a practical training approach fordeveloping more capable AI systems.</description>
      <author>example@mail.com (Serge Lionel Nikiema, Jordan Samhi, Micheline Bénédicte Moumoula, Albérick Euraste Djiré, Abdoul Kader Kaboré, Jacques Klein, Tegawendé F. Bissyandé)</author>
      <guid isPermaLink="false">2509.05553v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model</title>
      <link>http://arxiv.org/abs/2509.07825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Point Linguist Model (PLM)，解决了大型语言模型与3D点云表示不一致的问题，通过引入Object-centric Discriminative Representation和Geometric Reactivation Decoder显著提升了3D目标分割性能。&lt;h4&gt;背景&lt;/h4&gt;3D目标分割与大型语言模型的结合已成为主流范式，具有广泛的语义、任务灵活性和强大的泛化能力，但存在LLMs处理高级语义标记而3D点云仅传达密集几何结构的表示不一致问题。&lt;h4&gt;目的&lt;/h4&gt;解决LLMs与3D点云之间的表示不一致问题，搭建两者之间的表示差距桥梁，无需3D-文本或3D-图像之间的大规模预对齐。&lt;h4&gt;方法&lt;/h4&gt;提出Point Linguist Model通用框架，引入Object-centric Discriminative Representation学习以目标为中心的标记，捕获目标语义和场景关系；引入Geometric Reactivation Decoder结合OcDR标记和密集特征预测掩码，保留全面的密集特征。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNetv2上实现+7.3 mIoU改进，在Multi3DRefer的3D引用分割上实现+6.0 mIoU改进，在跨越4个不同任务的7个基准测试中保持一致的性能提升。&lt;h4&gt;结论&lt;/h4&gt;全面目标级推理对鲁棒的3D理解是有效的，PLM框架成功解决了LLMs和3D点云之间的表示不一致问题。&lt;h4&gt;翻译&lt;/h4&gt;使用大型语言模型进行3D目标分割已成为一种主流范式，因为它具有广泛的语义、任务灵活性和强大的泛化能力。然而，这种范式受到表示不一致的阻碍：大型语言模型处理高级语义标记，而3D点云仅传达密集的几何结构。在先前的方法中，这种不一致限制了输入和输出。在输入阶段，密集点块需要大量预对齐，弱化了目标级语义并混淆了相似的干扰项。在输出阶段，预测仅依赖于密集特征而没有明确的几何线索，导致精细粒度准确性的损失。为解决这些局限性，我们提出了Point Linguist Model，一个通用框架，它搭建了大型语言模型和密集3D点云之间的表示差距，而不需要3D-文本或3D-图像之间的大规模预对齐。具体而言，我们引入了Object-centric Discriminative Representation，它在硬负感知训练目标下学习以目标为中心的标记，捕获目标语义和场景关系。这减轻了大型语言模型标记和3D点之间的不一致，增强了对干扰项的鲁棒性，并促进了大型语言模型内的语义级推理。为了准确分割，我们引入了Geometric Reactivation Decoder，它通过结合携带大型语言模型推断几何的OcDR标记和相应的密集特征来预测掩码，在整个流程中保留全面的密集特征。大量实验表明，PLM在ScanNetv2的3D引用分割上实现了+7.3 mIoU的显著改进，在Multi3DRefer上实现了+6.0 mIoU的改进，在跨越4个不同任务的7个基准测试中保持一致的性能提升，证明了全面目标级推理对鲁棒3D理解的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云分割中的表示不匹配问题：大型语言模型处理高级语义标记，而3D点云只包含密集几何结构。这种不匹配限制了输入和输出效果，削弱了对象级语义识别能力，并导致细粒度分割精度下降。这个问题在3D场景理解、机器人导航和人机交互等领域至关重要，因为解决它可以显著提升AI系统对复杂3D环境的理解和分割能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析现有方法的局限性，然后借鉴了2D多模态语言模型(如LISA)的成功经验，结合3D点云特性进行创新。他们使用了预训练的Mask3D生成对象提案，采用LLaMA2作为基础语言模型，并应用LoRA进行高效微调。设计思路是创建一个桥梁连接LLM和3D点云，不依赖大规模预对齐数据，通过对象中心化表示和干扰物感知训练解决表示不匹配问题，最终形成PLM框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两个关键组件桥接LLM和3D点云：1)对象中心化表示(OcDR)使用对象标记作为LLM输入，捕获目标语义和场景关系；2)几何重新激活解码器(GRD)结合OcDR标记和密集特征预测掩码。流程包括：1)输入处理-生成OcDR表示；2)多模态推理-LLM处理视觉和语言输入；3)解码输出-GRD重新激活几何特征生成最终分割掩码；4)训练-使用干扰物感知机制增强对象区分能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)OcDR使用对象中心化标记替代传统点块，自然保持对象边界和语义；2)干扰物感知机制通过语义相似的硬负样本增强对象区分；3)GRD在整个流程中保留密集特征，提高细粒度精度。不同之处：传统方法依赖密集点块和大量预对齐，PLM使用对象中心化表示无需预对齐；传统方法输出仅依赖密集特征，PLM结合几何线索；传统方法任务受限，PLM支持多种3D分割任务；PLM训练效率更高，数据需求更少。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Point Linguist Model通过对象中心化表示和几何重新激活解码器，有效桥接了大型语言模型与3D点云之间的表示差距，显著提升了多种3D分割任务的性能，实现了开放词汇和语言引导的高效3D对象分割。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object segmentation with Large Language Models (LLMs) has become aprevailing paradigm due to its broad semantics, task flexibility, and stronggeneralization. However, this paradigm is hindered by representationmisalignment: LLMs process high-level semantic tokens, whereas 3D point cloudsconvey only dense geometric structures. In prior methods, misalignment limitsboth input and output. At the input stage, dense point patches require heavypre-alignment, weakening object-level semantics and confusing similardistractors. At the output stage, predictions depend only on dense featureswithout explicit geometric cues, leading to a loss of fine-grained accuracy. Toaddress these limitations, we present the Point Linguist Model (PLM), a generalframework that bridges the representation gap between LLMs and dense 3D pointclouds without requiring large-scale pre-alignment between 3D-text or3D-images. Specifically, we introduce Object-centric DiscriminativeRepresentation (OcDR), which learns object-centric tokens that capture targetsemantics and scene relations under a hard negative-aware training objective.This mitigates the misalignment between LLM tokens and 3D points, enhancesresilience to distractors, and facilitates semantic-level reasoning withinLLMs. For accurate segmentation, we introduce the Geometric ReactivationDecoder (GRD), which predicts masks by combining OcDR tokens carryingLLM-inferred geometry with corresponding dense features, preservingcomprehensive dense features throughout the pipeline. Extensive experimentsshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gainsacross 7 benchmarks spanning 4 different tasks, demonstrating the effectivenessof comprehensive object-centric reasoning for robust 3D understanding.</description>
      <author>example@mail.com (Zhuoxu Huang, Mingqi Gao, Jungong Han)</author>
      <guid isPermaLink="false">2509.07825v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Physics-informed low-rank neural operators with application to parametric elliptic PDEs</title>
      <link>http://arxiv.org/abs/2509.07687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PILNO是一种用于在点云数据上高效逼近偏微分方程解算子的神经算子框架，结合低秩核逼近与编码器-解码器架构，能够快速、连续地进行一次性预测，同时保持对特定离散化的独立性。&lt;h4&gt;背景&lt;/h4&gt;偏微分方程的求解在科学计算和工程应用中非常重要，但传统方法可能计算成本高且难以处理复杂几何形状或高维参数空间。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的神经算子框架，能够在点云数据上逼近偏微分方程的解算子，同时保持计算效率和对特定离散化的独立性。&lt;h4&gt;方法&lt;/h4&gt;提出PILNO框架，结合低秩核逼近与编码器-解码器架构，使用物理信息惩罚框架进行训练，确保PDE约束和边界条件得到满足。&lt;h4&gt;主要发现&lt;/h4&gt;PILNO在函数拟合、泊松方程、具有可变系数的屏蔽泊松方程和参数化达西流动等多种问题上表现出色；低秩结构在高维参数空间中提供了计算效率。&lt;h4&gt;结论&lt;/h4&gt;PILNO是一种可扩展且灵活的偏微分方程代理建模工具，能够高效处理各种偏微分方程问题。&lt;h4&gt;翻译&lt;/h4&gt;我们提出物理信息低秩神经算子，这是一种神经算子框架，用于在点云数据上高效逼近偏微分方程的解算子。PILNO将低秩核逼近与编码器-解码器架构相结合，能够快速、连续地进行一次性预测，同时保持对特定离散化的独立性。该模型使用物理信息惩罚框架进行训练，确保在监督和非监督设置下都满足PDE约束和边界条件。我们在各种问题上证明了其有效性，包括函数拟合、泊松方程、具有可变系数的屏蔽泊松方程和参数化达西流动。低秩结构在高维参数空间中提供了计算效率，使PILNO成为偏微分方程的可扩展且灵活的代理建模工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the Physics-Informed Low-Rank Neural Operator (PILNO), a neuraloperator framework for efficiently approximating solution operators of partialdifferential equations (PDEs) on point cloud data. PILNO combines low-rankkernel approximations with an encoder--decoder architecture, enabling fast,continuous one-shot predictions while remaining independent of specificdiscretizations. The model is trained using a physics-informed penaltyframework, ensuring that PDE constraints and boundary conditions are satisfiedin both supervised and unsupervised settings. We demonstrate its effectivenesson diverse problems, including function fitting, the Poisson equation, thescreened Poisson equation with variable coefficients, and parameterized Darcyflow. The low-rank structure provides computational efficiency inhigh-dimensional parameter spaces, establishing PILNO as a scalable andflexible surrogate modeling tool for PDEs.</description>
      <author>example@mail.com (Sebastian Schaffer, Lukas Exl)</author>
      <guid isPermaLink="false">2509.07687v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.07507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at WACV 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MVAT框架，利用时间多视图信息解决3D目标检测中的弱监督标注问题，通过教师-学生蒸馏范式和多视图2D投影损失，实现了接近完全监督方法的性能。&lt;h4&gt;背景&lt;/h4&gt;3D数据标注是3D目标检测中的昂贵瓶颈，促使研究人员开发依赖更易获取的2D边界框标注的弱监督标注方法。&lt;h4&gt;目的&lt;/h4&gt;解决仅依赖2D边界框标注时产生的投影歧义问题，以及单一视角下部分物体可见性导致的3D边界框估计困难问题。&lt;h4&gt;方法&lt;/h4&gt;提出MVAT框架，通过跨时间聚合物体中心点云构建3D物体表示，采用教师-学生蒸馏范式让教师网络从单一视角学习并生成高质量伪标签，学生网络从单一视角预测这些伪标签，同时使用多视图2D投影损失确保3D预测与2D标注的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Waymo Open数据集上的实验表明，MVAT在弱监督3D目标检测方面取得了最先进的性能，显著缩小了与完全监督方法的差距。&lt;h4&gt;结论&lt;/h4&gt;MVAT框架无需任何3D边界框标注即可实现接近完全监督方法的性能，为3D目标检测提供了一种有效的弱监督解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为MVAT的新颖框架，该框架利用序列数据中存在的时间多视图信息来解决这些挑战。我们的方法跨时间聚合以物体为中心的点云，以构建尽可能密集和完整的3D物体表示。采用教师-学生蒸馏范式：教师网络从单一视角学习，但目标是从时间聚合的静态物体中推导出来的。然后，教师生成高质量的伪标签，学生从单一视角学习预测这些伪标签，用于静态和移动物体。整个框架集成了多视图2D投影损失，以强制预测的3D边界框与所有可用的2D标注之间的一致性。在nuScenes和Waymo Open数据集上的实验表明，MVAT在弱监督3D目标检测方面取得了最先进的性能，显著缩小了与完全监督方法的差距，而无需任何3D边界框标注。我们的代码可在公共仓库中获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决弱监督3D目标检测中的投影歧义问题，即仅使用2D框标注来推断3D物体位置和大小时的不确定性。这个问题在现实中非常重要，因为3D数据标注成本高昂（平均114秒），而2D标注只需7-35秒，有3-16倍的效率提升。3D检测是自动驾驶和机器人的基础感知任务，降低标注成本能促进3D检测技术的广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考到现有弱监督方法仅依赖单帧数据，忽略了连续数据采集中自然存在的多视角信息。他们设计了一个教师-学生知识蒸馏框架，教师网络利用多帧信息学习3D几何，学生网络学习从单帧预测3D信息。方法借鉴了SAM 2图像分割、DBSCAN点云聚类、PCA 3D框估计等现有技术，以及全监督领域的时间融合方法，但创新性地将这些技术组合应用于弱监督3D检测场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用时间多视角信息解决投影歧义问题，通过聚合时序点云构建更完整的3D物体表示，采用教师-学生知识蒸馏框架。流程包括：1)使用SAM 2提取对象中心点云；2)聚合静态物体的多帧点云；3)使用PCA估计粗略3D框；4)教师网络用多帧信息训练；5)教师生成高质量伪标签；6)学生网络从单帧学习预测这些伪标签；7)使用多视角2D投影损失确保一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次有效利用时间多视角数据解决投影歧义；2)提出通过聚合点云生成高质量3D表示和伪标签的方法；3)采用多视角2D投影损失强制几何一致性；4)实现弱监督3D检测的最先进性能。不同之处：不依赖类别特定先验，而是直接解决歧义；不需要任何3D框标注或弱3D标注；首次将时间融合引入弱监督3D检测；教师用多帧训练，学生用单帧学习，实现有效知识迁移。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MVAT通过利用时间多视角一致性和教师-学生知识蒸馏框架，首次有效解决了弱监督3D目标检测中的投影歧义问题，显著缩小了与全监督方法的性能差距，同时仅需2D框标注。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Annotating 3D data remains a costly bottleneck for 3D object detection,motivating the development of weakly supervised annotation methods that rely onmore accessible 2D box annotations. However, relying solely on 2D boxesintroduces projection ambiguities since a single 2D box can correspond tomultiple valid 3D poses. Furthermore, partial object visibility under a singleviewpoint setting makes accurate 3D box estimation difficult. We propose MVAT,a novel framework that leverages temporal multi-view present in sequential datato address these challenges. Our approach aggregates object-centric pointclouds across time to build 3D object representations as dense and complete aspossible. A Teacher-Student distillation paradigm is employed: The Teachernetwork learns from single viewpoints but targets are derived from temporallyaggregated static objects. Then the Teacher generates high qualitypseudo-labels that the Student learns to predict from a single viewpoint forboth static and moving objects. The whole framework incorporates a multi-view2D projection loss to enforce consistency between predicted 3D boxes and allavailable 2D annotations. Experiments on the nuScenes and Waymo Open datasetsdemonstrate that MVAT achieves state-of-the-art performance for weaklysupervised 3D object detection, significantly narrowing the gap with fullysupervised methods without requiring any 3D box annotations. % \footnote{Codeavailable upon acceptance} Our code is available in our public repository(\href{https://github.com/CEA-LIST/MVAT}{code}).</description>
      <author>example@mail.com (Saad Lahlali, Alexandre Fournier Montgieux, Nicolas Granger, Hervé Le Borgne, Quoc Cuong Pham)</author>
      <guid isPermaLink="false">2509.07507v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis</title>
      <link>http://arxiv.org/abs/2509.07463v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DepthVision是一个多模态场景理解框架，通过结合激光雷达点云生成的RGB图像与真实RGB数据，解决了视觉输入退化或不充分时机器人可靠操作的问题，特别是在低光条件下表现优异。&lt;h4&gt;背景&lt;/h4&gt;当视觉输入退化或不充分时，确保机器人可靠运行仍然是一个核心挑战。现有的视觉语言模型(VLMs)仅依赖基于相机的视觉输入和语言，在环境条件差(如黑暗或运动模糊)时性能受限。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架，解决视觉输入退化情况下机器人操作可靠性问题，特别是在低光条件下的表现，同时保持与现有冻结VLMs的兼容性。&lt;h4&gt;方法&lt;/h4&gt;DepthVision使用条件生成对抗网络(GAN)结合集成精炼网络，从稀疏激光雷达点云合成RGB图像。然后，使用亮度感知模态适应(LAMA)将这些合成视图与真实RGB数据动态结合，基于环境光照条件混合两种类型的数据。这种方法无需对下游视觉语言模型进行任何微调即可补偿传感器退化。&lt;h4&gt;主要发现&lt;/h4&gt;在真实和模拟数据集上对各种模型和任务(特别是安全关键任务)的评估表明，该方法在低光条件下提高了性能，比仅使用RGB的基线方法获得显著提升，同时保持与冻结VLMs的兼容性。&lt;h4&gt;结论&lt;/h4&gt;激光雷达引导的RGB合成在实现真实环境中机器人操作的鲁棒性方面具有巨大潜力，为解决视觉输入退化问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;当视觉输入退化或不充分时，确保机器人可靠运行仍然是机器人技术中的一个核心挑战。本文介绍了DepthVision，这是一个为解决此问题而设计的多模态场景理解框架。与仅使用基于相机的视觉输入和语言的现有视觉语言模型(VLMs)不同，DepthVision使用带有集成精炼网络的条件生成对抗网络(GAN)，从稀疏激光雷达点云合成RGB图像。然后，使用亮度感知模态适应(LAMA)将这些合成视图与真实RGB数据结合，根据环境光照条件动态混合这两种类型的数据。这种方法补偿了传感器退化，如黑暗或运动模糊，而无需对下游视觉语言模型进行任何微调。我们在真实和模拟数据集上对各种模型和任务评估了DepthVision，特别关注安全关键任务。结果表明，我们的方法在低光条件下提高了性能，比仅使用RGB的基线方法获得显著提升，同时保持与冻结VLMs的兼容性。这项工作强调了激光雷达引导的RGB合成在实现真实环境中机器人操作鲁棒性方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人在视觉输入退化或不足时（如低光、运动模糊等条件）如何保持可靠运行的问题。这个问题在现实中非常重要，因为机器人任务依赖对环境的准确感知，而现有的视觉语言模型主要依赖相机图像，在低光等条件下表现不佳。虽然LiDAR等传感器能在这些条件下提供更好的空间感知，但LiDAR数据稀缺且难以大规模获取，限制了三维空间推理能力，这对自动驾驶等安全关键领域尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到视觉语言模型在低光条件下的局限性，以及LiDAR数据在提供空间感知方面的优势但存在数据稀缺问题。他们借鉴了生成对抗网络(GAN)特别是pix2pix框架来从LiDAR数据生成RGB图像，并采用U-Net架构保留空间细节。此外，作者还参考了refiner网络概念提高生成图像质量，并利用现有的视觉语言模型架构如Vision Transformer。关键创新在于设计了光照感知的模态适应机制，根据环境光照条件动态调整不同模态的权重，无需修改下游模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用LiDAR点云数据生成RGB图像，以补充或替代相机在低光条件下的输入，并根据环境光照条件动态调整真实RGB图像和生成RGB图像的融合权重。整体实现流程包括：1) LiDAR预处理：将3D点云投影到2D图像平面并进行插值；2) GAN和Refiner设置：使用基于pix2pix的GAN从单通道LiDAR投影生成RGB图像，并通过refiner迭代提高质量；3) 光照感知模态适应(LAMA)：计算图像亮度并动态调整融合权重；4) VLM集成：将融合后的图像送入冻结的视觉语言模型进行推理，无需微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 基于GAN的LiDAR到RGB合成架构，集成refiner网络提高生成质量；2) 光照感知模态适应(LAMA)机制，根据环境光照动态调整不同模态权重；3) 无需对下游视觉语言模型进行微调，保持与现有模型的兼容性。相比之前工作的不同：不同于现有模型仅使用相机图像，DepthVision结合LiDAR数据；不同于传统传感器融合直接处理原始数据，DepthVision生成RGB样式的表示；不同于需要大量LiDAR数据训练的方法，DepthVision通过GAN从稀疏点云生成图像；不同于静态融合策略，DepthVision根据光照条件动态调整；不同于需要修改下游模型的方法，保持视觉语言模型冻结状态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DepthVision通过基于GAN的LiDAR到RGB合成和光照感知模态适应，显著提高了机器人在低光条件下的视觉语言理解能力，同时无需修改下游模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring reliable robot operation when visual input is degraded orinsufficient remains a central challenge in robotics. This letter introducesDepthVision, a framework for multimodal scene understanding designed to addressthis problem. Unlike existing Vision-Language Models (VLMs), which use onlycamera-based visual input alongside language, DepthVision synthesizes RGBimages from sparse LiDAR point clouds using a conditional generativeadversarial network (GAN) with an integrated refiner network. These syntheticviews are then combined with real RGB data using a Luminance-Aware ModalityAdaptation (LAMA), which blends the two types of data dynamically based onambient lighting conditions. This approach compensates for sensor degradation,such as darkness or motion blur, without requiring any fine-tuning ofdownstream vision-language models. We evaluate DepthVision on real andsimulated datasets across various models and tasks, with particular attentionto safety-critical tasks. The results demonstrate that our approach improvesperformance in low-light conditions, achieving substantial gains over RGB-onlybaselines while preserving compatibility with frozen VLMs. This work highlightsthe potential of LiDAR-guided RGB synthesis for achieving robust robotoperation in real-world environments.</description>
      <author>example@mail.com (Sven Kirchner, Nils Purschke, Ross Greer, Alois C. Knoll)</author>
      <guid isPermaLink="false">2509.07463v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and Benchmark</title>
      <link>http://arxiv.org/abs/2509.07362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对密集城市环境中的视觉定位问题，提出了一种利用机载激光扫描(ALS)数据作为先验地图的新方法，并克服了现有ALS-based定位的三个主要限制。&lt;h4&gt;背景&lt;/h4&gt;在密集城市环境中进行准确的视觉定位是摄影测量、地理信息科学和机器人学中的基础任务。图像作为一种低成本且广泛可用的传感方式，其有效性常受限于无纹理表面、视角变化大和长期漂移问题。ALS数据的公开可用性为可扩展且精确的视觉定位开辟了新途径。&lt;h4&gt;目的&lt;/h4&gt;克服ALS-based定位面临的三个主要限制：缺乏平台多样化的数据集、缺乏适用于大规模城市环境的可靠真实值生成方法、以及现有I2P算法在空中-地面跨平台设置下的验证有限。&lt;h4&gt;方法&lt;/h4&gt;引入了一个新的数据集，该数据集集成了来自移动测绘系统的地面级图像与在中国武汉、香港和旧金山收集的ALS点云。&lt;h4&gt;主要发现&lt;/h4&gt;基于ALS的定位潜力尚未得到充分探索，主要受限于数据集多样性、真实值生成方法和算法验证的不足。&lt;h4&gt;结论&lt;/h4&gt;通过整合地面图像和ALS点云的新数据集为解决城市环境中的视觉定位问题提供了新的可能性，有助于推动ALS-based定位技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;在密集城市环境中进行准确的视觉定位是摄影测量、地理信息科学和机器人学中的基础任务。虽然图像是一种低成本且广泛可用的传感方式，但其在视觉里程计中的有效性常受限于无纹理表面、严重的视角变化和长期漂移。机载激光扫描(ALS)数据的日益公开可用性为可扩展且精确的视觉定位开辟了新途径，利用ALS作为先验地图。然而，由于三个关键限制，ALS-based定位的潜力尚未得到充分探索：(1)缺乏平台多样化的数据集，(2)缺乏适用于大规模城市环境的可靠真实值生成方法，(3)现有图像到点云(I2P)算法在空中-地面跨平台设置下的验证有限。为克服这些挑战，我们引入了一个新的大规模数据集，该数据集集成了来自移动测绘系统的地面级图像与在中国武汉、香港和旧金山收集的ALS点云。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决空中-地面跨模态定位问题，即如何利用地面图像和机载激光扫描(ALS)点云数据进行准确定位。这个问题在现实中非常重要，因为在高密度城市环境中，卫星导航信号常被遮挡或失效，而纯视觉定位又受限于无纹理表面、视角变化和长期漂移等问题。结合ALS点云作为先验地图可以提供更精确、更稳定的定位解决方案，对自动驾驶、机器人导航和地理测绘等领域具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有工作的三大局限性：缺乏平台多样化的数据集、缺乏适用于大规模城市环境的可靠地面真实值生成方法、以及现有图像到点云(I2P)算法在跨平台场景下的验证有限。基于这些认识，作者设计了结合地面图像和ALS点云的新数据集，并借鉴了现有的点云配准、姿态图优化和特征提取技术，但进行了改进以适应跨模态场景。在地面真实值生成方面，作者创新地采用间接方法，通过将移动激光扫描(MLS)数据与ALS点云对齐，然后将优化后的轨迹转移到图像流中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用ALS点云作为先验地图，结合地面图像实现跨模态定位，并通过间接方法生成高精度的地面真实值轨迹。整体流程分为三步：1)数据收集与预处理，收集三个城市的地面图像和ALS点云，并进行下采样和分割；2)地面真实值生成，通过地面分割和立面重建将MLS数据与ALS点云对齐，利用多传感器姿态图优化获得精确的图像姿态；3)定位算法评估，评估多种I2P算法在全局和精细定位任务上的性能，并采用SfM和ICP作为替代方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了首个整合地面图像和ALS点云的大规模跨平台数据集，覆盖三个不同特点的城市；2)提出创新的间接地面真实值生成方法，通过MLS-ALS对齐和姿态图优化，避免了跨模态直接配准的困难；3)建立了统一的基准测试套件，系统评估了现有方法在跨模态场景下的性能。相比之前的工作，这个数据集提供了更全面的跨模态场景，地面真实值生成方法不依赖重访区域或高端设备，且评估工作首次系统性地分析了I2P算法在空中-地面跨平台设置下的表现。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建大规模跨模态数据集、创新的地面真实值生成方法和统一的基准测试，为空中-地面视觉定位研究提供了重要资源和评估标准，推动了城市环境中高精度定位技术的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate visual localization in dense urban environments poses a fundamentaltask in photogrammetry, geospatial information science, and robotics. Whileimagery is a low-cost and widely accessible sensing modality, its effectivenesson visual odometry is often limited by textureless surfaces, severe viewpointchanges, and long-term drift. The growing public availability of airborne laserscanning (ALS) data opens new avenues for scalable and precise visuallocalization by leveraging ALS as a prior map. However, the potential ofALS-based localization remains underexplored due to three key limitations: (1)the lack of platform-diverse datasets, (2) the absence of reliable ground-truthgeneration methods applicable to large-scale urban environments, and (3)limited validation of existing Image-to-Point Cloud (I2P) algorithms underaerial-ground cross-platform settings. To overcome these challenges, weintroduce a new large-scale dataset that integrates ground-level imagery frommobile mapping systems with ALS point clouds collected in Wuhan, Hong Kong, andSan Francisco.</description>
      <author>example@mail.com (Yandi Yang, Jianping Li, Youqi Liao, Yuhao Li, Yizhe Zhang, Zhen Dong, Bisheng Yang, Naser El-Sheimy)</author>
      <guid isPermaLink="false">2509.07362v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Performance Characterization of a Point-Cloud-Based Path Planner in Off-Road Terrain</title>
      <link>http://arxiv.org/abs/2509.07321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been published in the Journal of Field Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究对名为MUONS的点云导航系统进行了全面评估，用于非结构化道路的自主导航。研究通过模拟和实地测试进行了30,000次规划和导航试验，分析了七个路径规划参数的二十种组合在不同地形上的表现。&lt;h4&gt;背景&lt;/h4&gt;非结构化道路环境下的自主导航是一个具有挑战性的问题，需要高效可靠的导航系统来处理复杂地形。&lt;h4&gt;目的&lt;/h4&gt;评估MUONS导航系统在非结构化道路环境中的性能，并确定影响性能的关键参数。&lt;h4&gt;方法&lt;/h4&gt;研究者在模拟环境中测试了三个具有运动学挑战的地形图，并进行了实地验证。总共进行了30,000次规划和导航试验，通过统计和相关性分析评估不同参数组合对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1. 配备MUONS的自动导引车在模拟中取得了0.98的成功率；2. 在实地测试中未出现故障；3. 初始规划阶段使用的双扩展半径与规划时间和路径长度性能最相关；4. 调整参数引起的变化比例与实地测试性能高度相关。&lt;h4&gt;结论&lt;/h4&gt;蒙特卡洛模拟活动可有效用于导航系统的性能评估和参数调整，模拟结果与实际表现具有良好的一致性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个基于点云的导航堆栈MUONS的全面评估，用于非结构化道路的自主导航。通过分析模拟中30,000次规划和导航试验的结果，并通过实地测试验证，来表征其性能。我们的模拟活动考虑了三个具有运动学挑战的地形图和七种路径规划参数的二十种组合。在模拟中，配备MUONS的自动导引车取得了0.98的成功率，并且在实地测试中没有出现故障。通过统计和相关性分析，我们确定在初始规划阶段使用的双扩展半径与规划时间和路径长度性能最相关。最后，我们观察到调整参数引起的变化比例与实地测试性能高度相关。这一发现支持使用蒙特卡洛模拟活动进行性能评估和参数调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/rob.70059&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a comprehensive evaluation of a point-cloud-based navigationstack, MUONS, for autonomous off-road navigation. Performance is characterizedby analyzing the results of 30,000 planning and navigation trials in simulationand validated through field testing. Our simulation campaign considers threekinematically challenging terrain maps and twenty combinations of sevenpath-planning parameters. In simulation, our MUONS-equipped AGV achieved a 0.98success rate and experienced no failures in the field. By statistical andcorrelation analysis we determined that the Bi-RRT expansion radius used in theinitial planning stages is most correlated with performance in terms ofplanning time and traversed path length. Finally, we observed that theproportional variation due to changes in the tuning parameters is remarkablywell correlated to performance in field testing. This finding supports the useof Monte-Carlo simulation campaigns for performance assessment and parametertuning.</description>
      <author>example@mail.com (Casey D. Majhor, Jeremy P. Bos)</author>
      <guid isPermaLink="false">2509.07321v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions</title>
      <link>http://arxiv.org/abs/2509.07209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ASME IDETC/CIE 2025 (DETC2025-168977). Dataset  availability: BlendedNet dataset is openly available at Harvard Dataverse  (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VJT9EP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BlendedNet是一个包含999个混合翼身几何形状的公开气动数据集，在约9种飞行条件下模拟产生8830个RANS案例，并引入了一个端到端的代理框架用于点状气动预测。&lt;h4&gt;背景&lt;/h4&gt;非常规配置（如混合翼身BWB）的气动设计面临数据稀缺问题，限制了数据驱动代理模型的研究。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模的混合翼身气动数据集，并提供一个端到端的代理框架用于点状气动预测，以解决数据稀缺问题并促进气动设计研究。&lt;h4&gt;方法&lt;/h4&gt;1. 采样几何设计参数和飞行条件生成数据集；2. 使用Spalart-Allmaras模型进行RANS模拟；3. 开发端到端代理框架，包括使用PointNet回归器从表面点云预测几何参数，以及使用特征线性调制(FiLM)网络预测点状系数Cp、Cfx和Cfz。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在多样化的混合翼身上，表面预测具有较低的误差，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;BlendedNet数据集解决了非常规配置的数据稀缺问题，并使基于数据驱动的代理模型研究成为可能，有助于气动设计的发展。&lt;h4&gt;翻译&lt;/h4&gt;BlendedNet是一个公开的气动数据集，包含999个混合翼身(BWB)几何形状。每个几何形状在约9种飞行条件下进行模拟，产生了8830个使用Spalart-Allmaras模型的收敛RANS案例，每个案例有900万至1400万个单元。该数据集通过采样几何设计参数和飞行条件生成，包括研究升力和阻力所需的详细点状表面量。我们还引入了一个用于点状气动预测的端到端代理框架。该流程首先使用排列不变的PointNet回归器从采样的表面点云预测几何参数，然后在预测的参数和飞行条件下调节特征线性调制(FiLM)网络，以预测点状系数Cp、Cfx和Cfz。实验显示，在多样化的BWB上表面预测误差较低。BlendedNet解决了非常规配置的数据稀缺问题，并支持基于数据驱动的代理模型研究，用于气动设计。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决混合翼身飞机(BWB)气动分析中的数据稀缺问题。这个问题很重要，因为BWB飞机具有更高的升阻比、更轻的结构重量和更低的燃油消耗(比传统飞机减少30%)，但其复杂几何形状导致流场分析困难，现有方法计算成本高、设计迭代慢，且缺乏详细的表面级气动数据，限制了数据驱动设计方法的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到BWB设计中的数据稀缺和计算效率问题，然后借鉴了多项现有工作：使用Zhang等人的几何参数化方法定义BWB形状；采用PointNet架构处理点云数据并预测几何参数；受Catalani等人启发使用Feature-wise Linear Modulation (FiLM)网络；利用Latin Hypercube Sampling系统性地采样几何和飞行条件参数。作者将这些方法整合成一个两阶段深度学习框架，从点云直接预测气动特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个高质量的BWB气动数据集，并开发一个端到端的深度学习框架，从点云数据直接预测气动表面特性。整体流程分为三部分：1)数据生成：使用OpenVSP生成999种几何形状，Pointwise生成网格，FUN3D进行高精度RANS模拟，处理得到8830个成功案例；2)代理模型：第一阶段用PointNet从点云预测9个几何参数，第二阶段用FiLM网络结合几何参数和飞行条件预测表面气动系数(Cp, Cfx, Cfz)；3)训练评估：按几何分组分割数据，使用Adam优化器训练，在独立测试集上评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个公开的BWB高分辨率表面气动数据集，包含999种几何和8830种飞行条件；2)端到点云到气动预测的完整框架；3)结合PointNet和FiLM的两阶段模型；4)提供点级压力和摩擦系数而非仅整体系数。相比之前工作，BlendedNet提供了更丰富的几何变体和更详细的表面数据，预测方法从整体系数扩展到表面分布，实现了从点云输入到气动预测的端到端流程，并作为开源数据集发布，促进了领域研究。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一个高质量的混合翼身飞机气动数据集和一个创新的端到端深度学习框架，能够从点云数据直接预测表面气动特性，解决了BWB设计中的数据稀缺和计算效率问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; BlendedNet is a publicly available aerodynamic dataset of 999 blended wingbody (BWB) geometries. Each geometry is simulated across about nine flightconditions, yielding 8830 converged RANS cases with the Spalart-Allmaras modeland 9 to 14 million cells per case. The dataset is generated by samplinggeometric design parameters and flight conditions, and includes detailedpointwise surface quantities needed to study lift and drag. We also introducean end-to-end surrogate framework for pointwise aerodynamic prediction. Thepipeline first uses a permutation-invariant PointNet regressor to predictgeometric parameters from sampled surface point clouds, then conditions aFeature-wise Linear Modulation (FiLM) network on the predicted parameters andflight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.Experiments show low errors in surface predictions across diverse BWBs.BlendedNet addresses data scarcity for unconventional configurations andenables research on data-driven surrogate modeling for aerodynamic design.</description>
      <author>example@mail.com (Nicholas Sung, Steven Spreizer, Mohamed Elrefaie, Kaira Samuel, Matthew C. Jones, Faez Ahmed)</author>
      <guid isPermaLink="false">2509.07209v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Visual Representation Alignment for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2509.07979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://cvlab-kaist.github.io/VIRAL/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出VIRAL方法，通过将多模态大语言模型的内部视觉表示与预训练视觉基础模型对齐，增强模型在视觉中心任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型通过视觉指令调优在各种任务上表现强大，但在以视觉为中心的任务（如物体计数或空间推理）中仍然有限。这种差距归因于当前仅文本的监督范式，它仅为视觉通路提供间接指导，导致模型在训练过程中丢弃细粒度的视觉细节。&lt;h4&gt;目的&lt;/h4&gt;提出VIRAL方法，通过显式对齐多模态大语言模型与预训练视觉基础模型的内部表示，使模型能够保留关键视觉细节并补充额外视觉知识，增强对复杂视觉输入的推理能力。&lt;h4&gt;方法&lt;/h4&gt;VIRAL是一种简单而有效的正则化策略，它将多模态大语言模型的内部视觉表示与预训练视觉基础模型(VFMs)的表示进行对齐。通过显式强制执行这种对齐，使模型能够保留输入视觉编码器中的关键视觉细节，同时补充VFMs的额外视觉知识。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，在广泛采用的多模态基准测试的所有任务中，VIRAL都带来了一致的性能提升。全面的消融研究验证了框架背后的关键设计选择。&lt;h4&gt;结论&lt;/h4&gt;这种简单的发现为在多模态大语言模型训练中有效整合视觉信息开辟了重要方向。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)通过视觉指令调优训练后已在各种任务上取得强大性能，但在以视觉为中心的任务（如物体计数或空间推理）中仍然存在局限。我们将这一差距归因于当前盛行的仅文本监督范式，该范式仅为视觉通路提供间接指导，并常导致MLLMs在训练过程中丢弃细粒度的视觉细节。在本文中，我们提出了VIRAL（Visual Representation Alignment，视觉表示对齐），这是一种简单而有效的正则化策略，它将MLLMs的内部视觉表示与预训练视觉基础模型(VFMs)的表示进行对齐。通过显式强制执行这种对齐，VIRAL使模型不仅能够保留来自输入视觉编码器的关键视觉细节，还能补充VFMs的额外视觉知识，从而增强其对复杂视觉输入的推理能力。我们的实验在广泛采用的多模态基准测试的所有任务中展示了一致的性能提升。此外，我们进行了全面的消融研究，以验证我们框架背后的关键设计选择。我们相信这一简单发现为在MLLMs训练中有效整合视觉信息开辟了重要方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) trained with visual instructiontuning have achieved strong performance across diverse tasks, yet they remainlimited in vision-centric tasks such as object counting or spatial reasoning.We attribute this gap to the prevailing text-only supervision paradigm, whichprovides only indirect guidance for the visual pathway and often leads MLLMs todiscard fine-grained visual details during training. In this paper, we presentVIsual Representation ALignment (VIRAL), a simple yet effective regularizationstrategy that aligns the internal visual representations of MLLMs with those ofpre-trained vision foundation models (VFMs). By explicitly enforcing thisalignment, VIRAL enables the model not only to retain critical visual detailsfrom the input vision encoder but also to complement additional visualknowledge from VFMs, thereby enhancing its ability to reason over complexvisual inputs. Our experiments demonstrate consistent improvements across alltasks on widely adopted multimodal benchmarks. Furthermore, we conductcomprehensive ablation studies to validate the key design choices underlyingour framework. We believe this simple finding opens up an important directionfor the effective integration of visual information in training MLLMs.</description>
      <author>example@mail.com (Heeji Yoon, Jaewoo Jung, Junwan Kim, Hyungyu Choi, Heeseong Shin, Sangbeom Lim, Honggyu An, Chaehyun Kim, Jisang Han, Donghyun Kim, Chanho Eom, Sunghwan Hong, Seungryong Kim)</author>
      <guid isPermaLink="false">2509.07979v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges</title>
      <link>http://arxiv.org/abs/2509.07946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为M3T FedFMs（多模态多任务联邦基础模型）的新范式，将联邦学习与多模态多任务基础模型相结合，用于教育领域，实现在保护隐私前提下的跨机构协作训练。&lt;h4&gt;背景&lt;/h4&gt;多模态多任务基础模型在人工智能领域展现出变革性潜力，特别是在教育应用方面。然而，这些模型在实际教育环境中的部署受到隐私法规、数据孤岛和领域特定数据有限性的阻碍。&lt;h4&gt;目的&lt;/h4&gt;向教育界揭示M3T FedFMs作为一种有前景但尚未被充分探索的方法，探索其潜力，并揭示相关的未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;提出M3T FedFMs框架，整合联邦学习与多模态多任务基础模型，使分散机构能够进行协作且隐私保护的训练，同时适应多样化的模态和任务。&lt;h4&gt;主要发现&lt;/h4&gt;M3T FedFMs能够推进下一代智能教育系统的三个关键支柱：(1)隐私保护，通过将敏感数据保留在本地；(2)个性化，通过模块化架构定制模型；(3)公平性和包容性，促进资源有限实体的参与。&lt;h4&gt;结论&lt;/h4&gt;M3T FedFMs面临多个开放研究挑战，包括机构间异构隐私法规研究、数据模态特性不均匀性、遗忘方法、持续学习框架以及模型可解释性，这些挑战需集体解决才能实现实际部署。&lt;h4&gt;翻译&lt;/h4&gt;多模态多任务基础模型最近在人工智能领域显示出变革性潜力，在教育领域有新兴应用。然而，其在现实教育环境中的部署受到隐私法规、数据孤岛和领域特定数据可用性有限的阻碍。我们为教育领域引入了多模态多任务联邦基础模型：一种将联邦学习与多模态多任务基础模型相结合的范式，使分散机构能够进行协作、隐私保护的训练，同时适应多样的模态和任务。随后，这篇立场论文旨在向教育界揭示M3T FedFMs作为一种有前景但尚未被充分探索的方法，探索其潜力，并揭示相关的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal multi-task (M3T) foundation models (FMs) have recently showntransformative potential in artificial intelligence, with emerging applicationsin education. However, their deployment in real-world educational settings ishindered by privacy regulations, data silos, and limited domain-specific dataavailability. We introduce M3T Federated Foundation Models (FedFMs) foreducation: a paradigm that integrates federated learning (FL) with M3T FMs toenable collaborative, privacy-preserving training across decentralizedinstitutions while accommodating diverse modalities and tasks. Subsequently,this position paper aims to unveil M3T FedFMs as a promising yet underexploredapproach to the education community, explore its potentials, and reveal itsrelated future research directions. We outline how M3T FedFMs can advance threecritical pillars of next-generation intelligent education systems: (i) privacypreservation, by keeping sensitive multi-modal student and institutional datalocal; (ii) personalization, through modular architectures enabling tailoredmodels for students, instructors, and institutions; and (iii) equity andinclusivity, by facilitating participation from underrepresented andresource-constrained entities. We finally identify various open researchchallenges, including studying of (i) inter-institution heterogeneous privacyregulations, (ii) the non-uniformity of data modalities' characteristics, (iii)the unlearning approaches for M3T FedFMs, (iv) the continual learningframeworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which mustbe collectively addressed for practical deployment.</description>
      <author>example@mail.com (Kasra Borazjani, Naji Khosravan, Rajeev Sahay, Bita Akram, Seyyedali Hosseinalipour)</author>
      <guid isPermaLink="false">2509.07946v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Long-Document Retrieval in the PLM and LLM Era</title>
      <link>http://arxiv.org/abs/2509.07759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述提供了长文档检索(LDR)的首次全面处理，系统化了从经典词汇和早期神经模型到现代预训练和大型语言模型的演变，涵盖了关键范式如段落聚合、层次编码和高效注意力技术。&lt;h4&gt;背景&lt;/h4&gt;长文档的激增对信息检索(IR)提出了根本性挑战，因为长文档的长度、分散证据和复杂结构需要超越标准段落级技术的专门方法。&lt;h4&gt;目的&lt;/h4&gt;提供长文档检索的综合参考和前瞻性议程，整合三个主要时代的方法、挑战和应用，推动基础模型时代的长文档检索发展。&lt;h4&gt;方法&lt;/h4&gt;系统化长文档检索模型的演变过程，包括经典词汇模型、早期神经模型、现代预训练模型(PLM)和大型语言模型(LLMs)，涵盖段落聚合、层次编码、高效注意力和LLM驱动的重新排序和检索技术。&lt;h4&gt;主要发现&lt;/h4&gt;长文档检索需要专门方法；领域存在从早期模型到现代预训练和大型语言模型的演变；存在领域特定应用和专门的评估资源。&lt;h4&gt;结论&lt;/h4&gt;长文档检索是一个重要且快速发展的领域，但仍面临效率权衡、多模态对齐和忠实性等关键开放挑战。&lt;h4&gt;翻译&lt;/h4&gt;长文档的激增对信息检索(IR)提出了根本性挑战，因为它们的长度、分散的证据和复杂的结构需要超越标准段落级技术的专门方法。本综述首次全面处理了长文档检索(LDR)，整合了三个主要时代的方法、挑战和应用。我们将从经典词汇和早期神经模型到现代预训练(PLM)和大型语言模型(LLMs)的演变系统化，涵盖了段落聚合、层次编码、高效注意力和最新的LLM驱动的重新排序和检索技术等关键范式。除了模型，我们还回顾了领域特定应用、专门的评估资源，并概述了关键的开放挑战，如效率权衡、多模态对齐和忠实性。本综述旨在为推进基础模型时代的长文档检索提供综合参考和前瞻性议程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of long-form documents presents a fundamental challenge toinformation retrieval (IR), as their length, dispersed evidence, and complexstructures demand specialized methods beyond standard passage-level techniques.This survey provides the first comprehensive treatment of long-documentretrieval (LDR), consolidating methods, challenges, and applications acrossthree major eras. We systematize the evolution from classical lexical and earlyneural models to modern pre-trained (PLM) and large language models (LLMs),covering key paradigms like passage aggregation, hierarchical encoding,efficient attention, and the latest LLM-driven re-ranking and retrievaltechniques. Beyond the models, we review domain-specific applications,specialized evaluation resources, and outline critical open challenges such asefficiency trade-offs, multimodal alignment, and faithfulness. This survey aimsto provide both a consolidated reference and a forward-looking agenda foradvancing long-document retrieval in the era of foundation models.</description>
      <author>example@mail.com (Minghan Li, Miyang Luo, Tianrui Lv, Yishuai Zhang, Siqi Zhao, Ercong Nie, Guodong Zhou)</author>
      <guid isPermaLink="false">2509.07759v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation</title>
      <link>http://arxiv.org/abs/2509.07596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究视觉语言基础模型中的性别偏见评估问题，发现现有基准中的虚假特征会显著影响偏见评估结果&lt;h4&gt;背景&lt;/h4&gt;视觉语言基础模型(VLMs)存在性别偏见问题，通常使用带有性别标注的真实图像基准进行评估，但这些基准中常存在性别特征与非性别特征之间的虚假相关性&lt;h4&gt;目的&lt;/h4&gt;探究非性别特征的虚假相关性是否会影响性别偏见的评估结果&lt;h4&gt;方法&lt;/h4&gt;在四个广泛使用的基准(COCO-gender, FACET, MIAP, 和 PHASE)和各种VLMs中系统地改变非性别特征，量化它们对偏见评估的影响&lt;h4&gt;主要发现&lt;/h4&gt;即使是最小的扰动（如仅遮蔽10%的物体或弱模糊背景）也会显著改变偏见分数，在生成式VLMs中使指标变化高达175%，在CLIP变体中变化43%&lt;h4&gt;结论&lt;/h4&gt;当前偏见评估往往反映的是模型对虚假特征的响应而非真正的性别偏见，建议同时报告偏见指标和特征敏感度测量以提高评估可靠性&lt;h4&gt;翻译&lt;/h4&gt;视觉语言基础模型中的性别偏见引发了对其安全部署的担忧，通常使用带有真实图像性别标注的基准进行评估。然而，由于这些基准中常存在性别特征与非性别特征（如物体和背景）之间的虚假相关性，我们识别出性别偏见评估中的一个关键疏忽：这些虚假特征是否会扭曲性别偏见评估？为解决这一问题，我们在四个广泛使用的基准和各种VLMs中系统地改变非性别特征，以量化它们对偏见评估的影响。我们的发现表明，即使是最小的扰动，如仅遮蔽10%的物体或弱模糊背景，也会显著改变偏见分数，在生成式VLMs中使指标变化高达175%，在CLIP变体中变化43%。这表明当前的偏见评估往往反映的是模型对虚假特征的响应，而非性别偏见，这降低了它们的可靠性。由于创建无虚假特征的基准具有根本性挑战，我们建议在报告偏见指标的同时报告特征敏感度测量，以实现更可靠的偏见评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gender bias in vision-language foundation models (VLMs) raises concerns abouttheir safe deployment and is typically evaluated using benchmarks with genderannotations on real-world images. However, as these benchmarks often containspurious correlations between gender and non-gender features, such as objectsand backgrounds, we identify a critical oversight in gender bias evaluation: Dospurious features distort gender bias evaluation? To address this question, wesystematically perturb non-gender features across four widely used benchmarks(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impacton bias evaluation. Our findings reveal that even minimal perturbations, suchas masking just 10% of objects or weakly blurring backgrounds, can dramaticallyalter bias scores, shifting metrics by up to 175% in generative VLMs and 43% inCLIP variants. This suggests that current bias evaluations often reflect modelresponses to spurious features rather than gender bias, undermining theirreliability. Since creating spurious feature-free benchmarks is fundamentallychallenging, we recommend reporting bias metrics alongside feature-sensitivitymeasurements to enable a more reliable bias assessment.</description>
      <author>example@mail.com (Yusuke Hirota, Ryo Hachiuma, Boyi Li, Ximing Lu, Michael Ross Boone, Boris Ivanovic, Yejin Choi, Marco Pavone, Yu-Chiang Frank Wang, Noa Garcia, Yuta Nakashima, Chao-Han Huck Yang)</author>
      <guid isPermaLink="false">2509.07596v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Towards Postmortem Data Management Principles for Generative AI</title>
      <link>http://arxiv.org/abs/2509.07375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了AI系统使用用户数据的问题，特别关注已故个人数据的所有权，分析了当前数据管理现状，提出了三条死后数据管理原则，并给出了政策建议。&lt;h4&gt;背景&lt;/h4&gt;基础模型、大型语言模型和智能AI系统严重依赖大量用户数据进行训练，这引发了关于所有权、版权和潜在危害的持续关注。然而，已故个人数据的所有权问题是一个相关但较少被研究的方面。&lt;h4&gt;目的&lt;/h4&gt;探索已故个人数据的所有权权利，分析当前死后数据管理和隐私权状况，提出死后数据管理原则，并为政策制定者和隐私从业者提供建议。&lt;h4&gt;方法&lt;/h4&gt;分析主要科技公司的隐私政策和欧盟AI法案等法规对死后数据管理和隐私权的定义，基于此分析提出死后数据管理原则。&lt;h4&gt;主要发现&lt;/h4&gt;当前在死后数据管理和隐私权方面存在空白，需要明确的原则来保护已故个人的数据权利。&lt;h4&gt;结论&lt;/h4&gt;提出了三条死后数据管理原则来指导保护已故个人数据权利，并建议政策制定者和隐私从业者与技术解决方案一起部署这些原则，使其在实践中可操作和可审计。&lt;h4&gt;翻译&lt;/h4&gt;基础模型、大型语言模型（LLMs）和智能AI系统严重依赖大量用户数据。使用此类数据进行训练引发了关于所有权、版权和潜在危害的持续关注。在这项工作中，我们探讨了一个相关但较少被研究的方面：已故个人数据的所有权权利。我们检查了主要科技公司的隐私政策和欧盟AI法案等法规所定义的死后数据管理和隐私权现状。基于此分析，我们提出了三条死后数据管理原则，以指导保护已故个人数据权利。最后，我们讨论了未来工作的方向，并为政策制定者和隐私从业者提供了关于如何与技术解决方案一起部署这些原则，使其在实践中可操作和可审计的建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, large language models (LLMs), and agentic AI systems relyheavily on vast corpora of user data. The use of such data for training hasraised persistent concerns around ownership, copyright, and potential harms. Inthis work, we explore a related but less examined dimension: the ownershiprights of data belonging to deceased individuals. We examine the currentlandscape of post-mortem data management and privacy rights as defined by theprivacy policies of major technology companies and regulations such as the EUAI Act. Based on this analysis, we propose three post-mortem data managementprinciples to guide the protection of deceased individuals data rights.Finally, we discuss directions for future work and offer recommendations forpolicymakers and privacy practitioners on deploying these principles alongsidetechnological solutions to operationalize and audit them in practice.</description>
      <author>example@mail.com (Ismat Jarin, Elina Van Kempen, Chloe Georgiou)</author>
      <guid isPermaLink="false">2509.07375v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases</title>
      <link>http://arxiv.org/abs/2509.07330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种通用人口统计预训练(GDP)模型，作为针对年龄和性别的基础表示框架，通过探索排序策略和编码方法的组合，将表格化人口统计输入转换为潜在嵌入，提高了模型的区分度、校准度和信息增益，增强了人口统计属性在预测模型中的重要性。&lt;h4&gt;背景&lt;/h4&gt;人口统计属性普遍存在于电子健康记录中，是临床风险分层和治疗决策的重要预测因素，但在模型设计中通常只起辅助作用，很少关注它们的表示学习。&lt;h4&gt;目的&lt;/h4&gt;开发一种基础表示框架，专门用于学习人口统计属性(尤其是年龄和性别)的有效表示，以提高医疗预测模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了通用人口统计预训练(GDP)模型，探索不同排序策略和编码方法的组合，将表格化人口统计输入转换为潜在嵌入，并使用来自不同地理区域的多样化疾病和人口组成的数据集进行预训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;顺序排序显著提高了模型的区分度、校准度和每个决策树分割的信息增益，特别是在年龄和性别对风险分层有显著贡献的疾病中；即使在人口统计属性预测价值相对较低的数据集中，GDP也能增强表示的重要性，增加它们在下游梯度提升模型中的影响。&lt;h4&gt;结论&lt;/h4&gt;表格化人口统计属性的基础模型可以跨任务和人群泛化，为提高医疗保健应用的预测性能提供了有希望的方向。&lt;h4&gt;翻译&lt;/h4&gt;人口统计属性普遍存在于电子健康记录中，并在临床风险分层和治疗决策中作为重要的预测因素。尽管它们具有重要意义，但这些属性在模型设计中通常只被赋予辅助角色，很少给予关注来学习它们的表示。本研究提出了一个通用人口统计预训练(GDP)模型作为基础表示框架，专门针对年龄和性别。该模型使用来自不同地理区域的多样化疾病和人口组成的数据集进行预训练和评估。GDP架构探索了排序策略和编码方法的组合，将表格化人口统计输入转换为潜在嵌入。实验结果表明，顺序排序显著提高了模型在区分度、校准度以及每个决策树分割的相应信息增益方面的性能，特别是在年龄和性别对风险分层有显著贡献的疾病中。即使在人口统计属性具有相对较低预测价值的数据集中，GDP也能增强表示的重要性，增加它们在下游梯度提升模型中的影响。研究结果表明，表格化人口统计属性的基础模型可以跨任务和人群泛化，为提高医疗保健应用的预测性能提供了有希望的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Demographic attributes are universally present in electronic health recordsand serve as vital predictors in clinical risk stratification and treatmentdecisions. Despite their significance, these attributes are often relegated toauxiliary roles in model design, with limited attention has been given tolearning their representations. This study proposes a General DemographicPre-trained (GDP) model as a foundational representation framework tailored toage and gender. The model is pre-trained and evaluated using datasets withdiverse diseases and population compositions from different geographic regions.The GDP architecture explores combinations of ordering strategies and encodingmethods to transform tabular demographic inputs into latent embeddings.Experimental results demonstrate that sequential ordering substantiallyimproves model performance in discrimination, calibration, and thecorresponding information gain at each decision tree split, particularly indiseases where age and gender contribute significantly to risk stratification.Even in datasets where demographic attributes hold relatively low predictivevalue, GDP enhances the representational importance, increasing their influencein downstream gradient boosting models. The findings suggest that foundationalmodels for tabular demographic attributes can generalize across tasks andpopulations, offering a promising direction for improving predictiveperformance in healthcare applications.</description>
      <author>example@mail.com (Li-Chin Chen, Ji-Tian Sheu, Yuh-Jue Chuang)</author>
      <guid isPermaLink="false">2509.07330v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2509.07143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TabGFM的图基础模型框架，通过将图数据转换为表格格式，利用表格基础模型进行节点分类，在28个真实世界数据集上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;图基础模型(GFMs)虽然在各种图数据上展现出泛化能力，但通常在不能很好代表真实世界图的数据集上训练，限制了其泛化性能。相比之下，表格基础模型(TFMs)不仅在表格预测任务上表现出色，还在时间序列、自然语言处理和计算机视觉等领域显示出强大适用性。&lt;h4&gt;目的&lt;/h4&gt;受表格基础模型的启发，作者重新审视图基础模型的标准视角，将节点分类问题重新表述为表格问题，使TFMs能够通过上下文学习直接执行零样本节点分类。&lt;h4&gt;方法&lt;/h4&gt;TabGFM框架首先通过特征和结构编码器将图转换为表格，然后应用多个TFMs到多样本采样的表格上，最后通过集成选择聚合它们的输出。&lt;h4&gt;主要发现&lt;/h4&gt;在28个真实世界数据集上的实验表明，TabGFM在特定任务的图神经网络(GNNs)和最先进的图基础模型上取得了一致的改进。&lt;h4&gt;结论&lt;/h4&gt;表格重新表述方法为可扩展和可泛化的图学习提供了新的可能性，展示了将图数据转换为表格格式以利用表格基础模型优势的潜力。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型(GFMs)最近出现，作为一种很有前景的范式，可以在各种图数据上实现广泛的泛化。然而，现有的GFMs通常在不能很好地表示真实世界图的数据集上进行训练，这限制了它们的泛化性能。相比之下，表格基础模型(TFMs)不仅在经典的表格预测任务上表现出色，还在其他领域如时间序列预测、自然语言处理和计算机视觉中显示出强大的适用性。受此启发，我们对GFMs的标准观点采取了另一种视角，并将节点分类重新表述为表格问题。每个节点可以表示为行，特征、结构和标签信息作为列，使TFMs能够通过上下文学习直接执行零样本节点分类。在这项工作中，我们介绍了TabGFM，这是一种图基础模型框架，它首先通过特征和结构编码器将图转换为表格，应用多个TFMs到多样本采样的表格上，然后通过集成选择聚合它们的输出。在28个真实世界数据集上的实验中，TabGFM在特定任务的GNNs和最先进的GFMs上取得了一致的改进，突显了表格重新表述对于可扩展和可泛化的图学习的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph foundation models (GFMs) have recently emerged as a promising paradigmfor achieving broad generalization across various graph data. However, existingGFMs are often trained on datasets that were shown to poorly representreal-world graphs, limiting their generalization performance. In contrast,tabular foundation models (TFMs) not only excel at classical tabular predictiontasks but have also shown strong applicability in other domains such as timeseries forecasting, natural language processing, and computer vision. Motivatedby this, we take an alternative view to the standard perspective of GFMs andreformulate node classification as a tabular problem. Each node can berepresented as a row with feature, structure, and label information as columns,enabling TFMs to directly perform zero-shot node classification via in-contextlearning. In this work, we introduce TabGFM, a graph foundation model frameworkthat first converts a graph into a table via feature and structural encoders,applies multiple TFMs to diversely subsampled tables, and then aggregates theiroutputs through ensemble selection. Through experiments on 28 real-worlddatasets, TabGFM achieves consistent improvements over task-specific GNNs andstate-of-the-art GFMs, highlighting the potential of tabular reformulation forscalable and generalizable graph learning.</description>
      <author>example@mail.com (Adrian Hayler, Xingyue Huang, İsmail İlkan Ceylan, Michael Bronstein, Ben Finkelshtein)</author>
      <guid isPermaLink="false">2509.07143v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Interleaving Reasoning for Better Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2509.06945v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为交错推理生成(IRG)的新框架，通过在文本思考和图像合成之间交替进行来改进文本到图像(T2I)生成能力。研究还提出了交错推理生成学习(IRGL)方法和IRGL-300K数据集，通过两阶段训练实现了在多个评估指标上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;统一的多模态理解和生成模型在图像生成能力方面取得了显著进步，但在遵循指令和细节保留方面与紧密耦合理解与生成的系统(如GPT-4o)相比仍有较大差距。受最近交错推理进展的启发，研究者探索了这种推理是否能进一步改进文本到图像的生成。&lt;h4&gt;目的&lt;/h4&gt;探索交错推理是否能改进文本到图像(T2I)生成，以缩小统一多模态模型与紧密耦合理解与生成的系统之间的差距，特别是在遵循指令和细节保留方面。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了交错推理生成(IRG)框架，模型交替进行文本思考和图像合成；2. 提出了交错推理生成学习(IRGL)方法，包含两个子目标：强化初始思考和生成阶段，以及实现高质量的文本反思和后续图像中的忠实实现；3. 构建了IRGL-300K数据集，包含六种分解的学习模式；4. 采用两阶段训练：首先构建强大的思考和反思能力，然后在完整的思考-图像轨迹数据中高效调整IRG管道。&lt;h4&gt;主要发现&lt;/h4&gt;在多个评估指标(GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN)上取得了5-10个百分点的绝对提升，在视觉质量和细粒度保真度方面有显著改进，实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;交错推理框架能够有效提升文本到图像生成能力，特别是在遵循指令和细节保留方面，缩小了统一多模态模型与紧密耦合系统之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;统一多模态理解和生成模型最近在图像生成能力方面取得了显著改进，然而与紧密耦合理解与生成的系统(如GPT-4o)相比，在遵循指令和细节保留方面仍存在较大差距。受最近交错推理进展的启发，我们探索了这种推理是否能进一步改进文本到图像(T2I)生成。我们引入了交错推理生成(IRG)，这是一个在文本思考和图像合成之间交替的框架：模型首先产生基于文本的思考来引导初始图像，然后反思结果以细化细粒度细节、视觉质量和美学，同时保持语义。为了有效训练IRG，我们提出了交错推理生成学习(IRGL)，它针对两个子目标：(1)强化初始思考和生成阶段以建立核心内容和基础质量，(2)在后续图像中实现高质量的文本反思和对这些调整的忠实执行。我们整理了IRGL-300K，这是一个组织成六种分解学习模式的数据集，共同涵盖了基于文本的思考和完整的思考-图像轨迹学习。从一个原生输出交错文本-图像输出的统一基础模型开始，我们的两阶段训练首先构建强大的思考和反思能力，然后在完整的思考-图像轨迹数据中高效调整IRG管道。大量实验显示了最先进的性能，在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN上获得了5-10个百分点的绝对提升，同时在视觉质量和细粒度保真度方面也有显著改进。代码、模型权重和数据集将在以下地址发布：https://github.com/Osilly/Interleaving-Reasoning-Generation。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified multimodal understanding and generation models recently have achievesignificant improvement in image generation capability, yet a large gap remainsin instruction following and detail preservation compared to systems thattightly couple comprehension with generation such as GPT-4o. Motivated byrecent advances in interleaving reasoning, we explore whether such reasoningcan further improve Text-to-Image (T2I) generation. We introduce InterleavingReasoning Generation (IRG), a framework that alternates between text-basedthinking and image synthesis: the model first produces a text-based thinking toguide an initial image, then reflects on the result to refine fine-graineddetails, visual quality, and aesthetics while preserving semantics. To trainIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),which targets two sub-goals: (1) strengthening the initial think-and-generatestage to establish core content and base quality, and (2) enabling high-qualitytextual reflection and faithful implementation of those refinements in asubsequent image. We curate IRGL-300K, a dataset organized into six decomposedlearning modes that jointly cover learning text-based thinking, and fullthinking-image trajectories. Starting from a unified foundation model thatnatively emits interleaved text-image outputs, our two-stage training firstbuilds robust thinking and reflection, then efficiently tunes the IRG pipelinein the full thinking-image trajectory data. Extensive experiments show SoTAperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual qualityand fine-grained fidelity. The code, model weights and datasets will bereleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .</description>
      <author>example@mail.com (Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin)</author>
      <guid isPermaLink="false">2509.06945v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards</title>
      <link>http://arxiv.org/abs/2509.07047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于奖励函数的优化方法，用于微调基础模型（以SAM为例），解决了基础模型参数不透明且需要大量手动优化的问题，提高了模型在实时流数据分割中的性能。&lt;h4&gt;背景&lt;/h4&gt;图像分割是显微分析中的关键任务，可通过自定义模型、迁移学习或基础模型实现。然而，基础模型通常包含大量不透明的调优参数，需要广泛的手动优化，限制了其在实时流数据分析中的应用。&lt;h4&gt;目的&lt;/h4&gt;引入基于奖励函数的优化方法来微调基础模型，以Meta的SAM框架为例，提高其在显微图像分割中的适应性和性能，特别是实现实时流数据分割。&lt;h4&gt;方法&lt;/h4&gt;构建代表成像系统物理特性的奖励函数，包括粒子尺寸分布、几何形状和其他标准，并将奖励驱动的优化框架整合到SAM模型中，创建优化变体SAM*。&lt;h4&gt;主要发现&lt;/h4&gt;通过奖励函数优化，开发了SAM*模型，该模型更好地满足多样化分割任务的需求，并特别支持实时流数据分割，在显微成像中表现出色。&lt;h4&gt;结论&lt;/h4&gt;基于奖励函数的优化方法有效提升了基础模型在显微图像分割中的性能，精确分割对分析细胞结构、材料界面和纳米级特征至关重要。&lt;h4&gt;翻译&lt;/h4&gt;图像分割是显微学中的关键任务，对于准确分析和解释复杂的视觉数据至关重要。这项任务可以使用在特定领域数据集上训练的自定义模型、从预训练模型迁移学习，或提供广泛适用性的基础模型来完成。然而，基础模型通常呈现大量不透明的调优参数，需要广泛的手动优化，限制了它们对实时流数据分析的可用性。在此，我们引入了基于奖励函数的优化方法来微调基础模型，并通过Meta的SAM（Segment Anything Model）框架说明这种方法。奖励函数可以被构建来表示成像系统的物理特性，包括粒子尺寸分布、几何形状和其他标准。通过整合奖励驱动的优化框架，我们提高了SAM的适应性和性能，从而产生了一个优化变体SAM*，它更好地满足了多样化分割任务的需求，并特别允许实时流数据分割。我们在显微成像中证明了这种方法的有效性，其中精确分割对于分析细胞结构、材料界面和纳米级特征至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image segmentation is a critical task in microscopy, essential for accuratelyanalyzing and interpreting complex visual data. This task can be performedusing custom models trained on domain-specific datasets, transfer learning frompre-trained models, or foundational models that offer broad applicability.However, foundational models often present a considerable number ofnon-transparent tuning parameters that require extensive manual optimization,limiting their usability for real-time streaming data analysis. Here, weintroduce a reward function-based optimization to fine-tune foundational modelsand illustrate this approach for SAM (Segment Anything Model) framework byMeta. The reward functions can be constructed to represent the physics of theimaged system, including particle size distributions, geometries, and othercriteria. By integrating a reward-driven optimization framework, we enhanceSAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$,that better aligns with the requirements of diverse segmentation tasks andparticularly allows for real-time streaming data segmentation. We demonstratethe effectiveness of this approach in microscopy imaging, where precisesegmentation is crucial for analyzing cellular structures, material interfaces,and nanoscale features.</description>
      <author>example@mail.com (Kamyar Barakati, Utkarsh Pratiush, Sheryl L. Sanchez, Aditya Raghavan, Delia J. Milliron, Mahshid Ahmadi, Philip D. Rack, Sergei V. Kalinin)</author>
      <guid isPermaLink="false">2509.07047v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes</title>
      <link>http://arxiv.org/abs/2509.06685v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Withdrawn due to an error in the author list &amp; incomplete  experimental results&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架，通过结合视觉惯性运动恢复结构的精确稀疏深度和大型基础模型的密集粗糙深度，实现了高质量的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;传统高斯溅射技术需要精确的深度信息来初始化高斯椭球体，但RGB-D/立体相机的深度传感范围有限，难以在大场景中应用。单目图像缺乏深度信息指导，导致新视角合成结果不佳。虽然大型基础模型可用于单目深度估计，但存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索的歧义等问题。&lt;h4&gt;目的&lt;/h4&gt;从单目RGB输入生成密集、精确的深度图像，用于高清晰度高斯溅射渲染。&lt;h4&gt;方法&lt;/h4&gt;利用视觉惯性运动恢复结构(SfM)提供的精确但稀疏的深度信息来优化大型基础模型(LFMs)提供的密集但粗糙的深度信息。提出对象分割深度传播算法渲染结构化对象像素的深度，并开发动态深度优化模块处理动态对象的损坏SfM深度，优化粗糙的LFM深度。&lt;h4&gt;主要发现&lt;/h4&gt;结合精确稀疏深度和密集粗糙深度的方法能够有效解决大场景中新视角合成的问题，提高渲染质量。&lt;h4&gt;结论&lt;/h4&gt;VIM-GS框架在大场景新视角合成中表现出优越的渲染质量，证明了所提方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架。高斯溅射通常需要精确的深度来使用RGB-D/立体相机初始化高斯椭球体。它们有限的深度传感范围使得高斯溅射难以在大场景中工作。然而，单目图像缺乏深度来指导学习，导致新视角合成结果不佳。尽管有可用于单目深度估计的大型基础模型，但它们存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索的歧义等问题。本文旨在从单目RGB输入生成密集、精确的深度图像，用于高清晰度高斯溅射渲染。关键思路是利用视觉惯性运动恢复结构(SfM)的精确但稀疏的深度来优化大型基础模型(LFMs)的密集但粗糙的深度。为了连接稀疏输入和密集输出，我们提出了一种对象分割深度传播算法，该算法渲染结构化对象像素的深度。然后我们开发了一个动态深度优化模块来处理动态对象的损坏SfM深度，并优化粗糙的LFM深度。使用公共和定制数据集的实验证明了VIM-GS在大场景中的优越渲染质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; VIM-GS is a Gaussian Splatting (GS) framework using monocular images fornovel-view synthesis (NVS) in large scenes. GS typically requires accuratedepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limiteddepth sensing range makes it difficult for GS to work in large scenes.Monocular images, however, lack depth to guide the learning and lead toinferior NVS results. Although large foundation models (LFMs) for monoculardepth estimation are available, they suffer from cross-frame inconsistency,inaccuracy for distant scenes, and ambiguity in deceptive texture cues. Thispaper aims to generate dense, accurate depth images from monocular RGB inputsfor high-definite GS rendering. The key idea is to leverage the accurate butsparse depth from visual-inertial Structure-from-Motion (SfM) to refine thedense but coarse depth from LFMs. To bridge the sparse input and dense output,we propose an object-segmented depth propagation algorithm that renders thedepth of pixels of structured objects. Then we develop a dynamic depthrefinement module to handle the crippled SfM depth of dynamic objects andrefine the coarse LFM depth. Experiments using public and customized datasetsdemonstrate the superior rendering quality of VIM-GS in large scenes.</description>
      <author>example@mail.com (Shengkai Zhang, Yuhe Liu, Guanjun Wu, Jianhua He, Xinggang Wang, Mozi Chen, Kezhong Liu)</author>
      <guid isPermaLink="false">2509.06685v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>HU-based Foreground Masking for 3D Medical Masked Image Modeling</title>
      <link>http://arxiv.org/abs/2509.07534v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI AMAI Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于HU的前景掩码方法，改进了掩码图像建模在3D医学图像计算中的应用，通过关注内脏器官的强度分布而非随机掩码，显著提高了医学图像分割的性能。&lt;h4&gt;背景&lt;/h4&gt;虽然掩码图像建模(MIM)已经彻底改变了计算机视觉领域，但在3D医学图像计算中的应用一直受到随机掩码使用的限制，这种方法忽略了解剖物体的密度。&lt;h4&gt;目的&lt;/h4&gt;通过一个简单而有效的掩码策略来增强预训练任务，解决随机掩码在医学图像处理中的局限性。&lt;h4&gt;方法&lt;/h4&gt;利用HU(亨氏单位)测量，实现基于HU的前景掩码，专注于内脏器官的强度分布，排除缺乏诊断意义特征的区域，如空气和流体等非组织区域。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公共3D医学影像数据集上的广泛实验表明，提出的掩码策略在分割质量和Dice分数方面持续提高了性能(BTCV:~84.64%, Flare22:~92.43%, MM-WHS:~90.67%, Amos22:~88.64%, BraTS:~78.55%)。&lt;h4&gt;结论&lt;/h4&gt;这些结果强调了以领域为中心的MIM的重要性，并为医学图像分割的表示学习指出了一个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;虽然掩码图像建模(MIM)已经彻底改变了计算机视觉领域，但在3D医学图像计算中的应用一直受到随机掩码使用的限制，这种方法忽略了解剖物体的密度。为了解决这一限制，我们通过一个简单而有效的掩码策略来增强预训练任务。利用HU(亨氏单位)测量，我们实现了基于HU的前景掩码，它专注于内脏器官的强度分布，排除了缺乏诊断意义特征的区域，如空气和流体等非组织区域。在五个公共3D医学影像数据集上的广泛实验表明，我们的掩码策略在分割质量和Dice分数方面持续提高了性能(BTCV:~84.64%, Flare22:~92.43%, MM-WHS:~90.67%, Amos22:~88.64%, BraTS:~78.55%)。这些结果强调了以领域为中心的MIM的重要性，并为医学图像分割的表示学习指出了一个有前景的方向。实现可在github.com/AISeedHub/SubFore/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决将掩码图像建模方法有效应用于3D医学图像的问题。传统方法使用随机掩码策略，忽略了医学图像中解剖结构的密度特点。这个问题很重要，因为医学图像（如CT扫描）包含大量缺乏诊断意义的背景区域（如空气和流体），随机掩码会导致模型在预训练时关注不相关信息，而不是包含诊断价值的解剖结构，影响最终分割效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析医学图像与自然图像的差异，发现CT扫描的HU值分布特点：低HU值（&lt;0）通常对应背景区域，而高HU值对应有诊断意义的解剖结构。他们比较了前景和背景区域的信息量，发现前景区域包含更多诊断相关信息。作者借鉴了现有的掩码图像建模框架（如MAE和SimMIM）和3D体积处理方法（如MAE3D），但没有直接照搬，而是根据医学图像特点进行了创新性改进，设计了基于HU值的前景掩码策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用医学图像中HU值的分布特点，只掩码包含诊断信息的区域（HU值在[0.1,1]范围内），排除低HU值背景区域（如空气和流体），使模型在预训练时专注于有意义的解剖结构。整体实现流程包括：1)子体积划分：将原始3D医学图像体积划分为多个16×16×16的小子体积；2)前景掩码：计算每个子体积的平均HU值，只掩码平均HU值≥0.1的子体积；3)预训练任务：使用掩码后的体积作为输入，训练模型重建原始体积；4)下游任务：将预训练好的模型应用于医学图像分割任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将HU值信息整合到掩码图像建模框架中；2)提出简单而有效的前景掩码方法，专注于解剖结构而非随机区域；3)通过大量实验验证了方法在多个医学图像数据集上的有效性。相比之前的工作，不同之处在于：传统MIM方法使用随机掩码，而本文基于HU值进行语义感知的掩码；现有医学图像MIM方法通常直接从自然图像领域迁移，没有考虑医学图像的特殊性；本文方法通过分析医学图像的强度分布，有选择地掩码信息丰富的区域，在多个数据集上表现优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于HU值的前景掩码策略，通过专注于医学图像中有诊断意义的解剖结构而非随机掩码，显著提高了3D医学图像分割任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Masked Image Modeling (MIM) has revolutionized fields of computervision, its adoption in 3D medical image computing has been limited by the useof random masking, which overlooks the density of anatomical objects. Toaddress this limitation, we enhance the pretext task with a simple yeteffective masking strategy. Leveraging Hounsfield Unit (HU) measurements, weimplement an HU-based Foreground Masking, which focuses on the intensitydistribution of visceral organs and excludes non-tissue regions, such as airand fluid, that lack diagnostically meaningful features. Extensive experimentson five public 3D medical imaging datasets demonstrate that our maskingconsistently improves performance, both in quality of segmentation and Dicescore (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%,BraTS:~78.55\%). These results underscore the importance of domain-centric MIMand suggest a promising direction for representation learning in medical imagesegmentation. Implementation is available at github.com/AISeedHub/SubFore/.</description>
      <author>example@mail.com (Jin Lee, Vu Dang, Gwang-Hyun Yu, Anh Le, Zahid Rahman, Jin-Ho Jang, Heonzoo Lee, Kun-Yung Kim, Jin-Sul Kim, Jin-Young Kim)</author>
      <guid isPermaLink="false">2509.07534v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents</title>
      <link>http://arxiv.org/abs/2509.07531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;科学文档表示学习当前面临三个主要挑战：基于引用结构的对比训练方法未能充分利用引用信息；细粒度表示学习方法需要昂贵的集成且缺乏领域泛化；任务感知学习方法依赖于手动预定义的任务分类且需要额外训练数据。为解决这些问题，作者提出了FLeW方法，统一了三种方法，通过引入三元组采样方法增强引用结构信号，利用引用意图进行细粒度表示学习，并通过简单权重搜索自适应集成嵌入。实验表明FLeW在多个科学任务和领域中具有适用性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;当前科学文档表示学习方法面临三个主要挑战：1)基于引用结构的对比训练方法未能充分利用引用信息，仍然生成单一向量表示；2)细粒度表示学习方法在句子或方面级别生成多个向量，需要昂贵的集成且缺乏领域泛化能力；3)任务感知学习方法依赖于手动预定义的任务分类，忽略了细微的任务区别，并为任务特定模块需要额外的训练数据。&lt;h4&gt;目的&lt;/h4&gt;解决科学文档表示学习中的三个挑战，提出一种统一三种方法的新方法FLeW，以获得更好的文档表示。&lt;h4&gt;方法&lt;/h4&gt;FLeW方法引入了一种新的三元组采样方法，利用引用意图和频率增强引用结构信号进行训练；利用引用意图(背景、方法、结果)与科学写作结构相一致的特点，促进细粒度表示学习的领域通用方面划分；采用简单的权重搜索来自适应地将三个方面级别的嵌入集成到任务特定的文档嵌入中，无需任务感知的微调。&lt;h4&gt;主要发现&lt;/h4&gt;FLeW方法在多个科学任务和领域中表现出适用性和鲁棒性，优于先前的模型。&lt;h4&gt;结论&lt;/h4&gt;FLeW方法通过统一三种表示学习方法的优点，解决了现有方法的局限性，为科学文档表示学习提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;科学文档表示学习为各种任务提供了强大的嵌入，而当前方法在三种方法中面临挑战。1)基于引用结构的对比训练未能充分利用引用信息，仍然生成单一向量表示。2)细粒度表示学习在句子或方面级别生成多个向量，需要昂贵的集成且缺乏领域泛化能力。3)任务感知学习依赖于手动预定义的任务分类，忽略了细微的任务区别，并为任务特定模块需要额外的训练数据。为解决这些问题，我们提出了一种统一三种方法以获得更好表示的新方法，即FLeW。具体来说，我们引入了一种新的三元组采样方法，利用引用意图和频率来增强用于训练的引用结构信号。引用意图(背景、方法、结果)与科学写作的一般结构相一致，促进细粒度表示学习的领域通用方面划分。然后，我们采用简单的权重搜索来自适应地将三个方面级别的嵌入集成到任务特定的文档嵌入中，无需任务感知的微调。实验表明，与先前的模型相比，FLeW在多个科学任务和领域中具有适用性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific document representation learning provides powerful embeddings forvarious tasks, while current methods face challenges across three approaches.1) Contrastive training with citation-structural signals underutilizes citationinformation and still generates single-vector representations. 2) Fine-grainedrepresentation learning, which generates multiple vectors at the sentence oraspect level, requires costly integration and lacks domain generalization. 3)Task-aware learning depends on manually predefined task categorization,overlooking nuanced task distinctions and requiring extra training data fortask-specific modules. To address these problems, we propose a new method thatunifies the three approaches for better representations, namely FLeW.Specifically, we introduce a novel triplet sampling method that leveragescitation intent and frequency to enhance citation-structural signals fortraining. Citation intents (background, method, result), aligned with thegeneral structure of scientific writing, facilitate a domain-generalized facetpartition for fine-grained representation learning. Then, we adopt a simpleweight search to adaptively integrate three facet-level embeddings into atask-specific document embedding without task-aware fine-tuning. Experimentsshow the applicability and robustness of FLeW across multiple scientific tasksand fields, compared to prior models.</description>
      <author>example@mail.com (Zheng Dou, Deqing Wang, Fuzhen Zhuang, Jian Ren, Yanlin Hu)</author>
      <guid isPermaLink="false">2509.07531v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Kernel VICReg for Self-Supervised Learning in Reproducing Kernel Hilbert Space</title>
      <link>http://arxiv.org/abs/2509.07289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出Kernel VICReg，一种将VICReg目标提升到再生核希尔伯特空间的自监督学习框架，通过核化损失函数实现非线性特征学习，在多个数据集上展示了一致的性能提升。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为强大的表征学习范式，通过优化几何目标而无需标签，但现有方法主要在欧几里得空间中操作，限制了捕捉非线性依赖和几何结构的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在非线性空间中操作的自监督学习框架，以捕捉数据中的非线性依赖和几何结构，提高在复杂数据或小规模数据上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出Kernel VICReg框架，将VICReg目标提升到再生核希尔伯特空间，通过核化损失函数的各个项（方差、不变性和协方差），实现在双中心核矩阵和希尔伯特-施密特范数上操作，进行非线性特征学习而无需显式映射。&lt;h4&gt;主要发现&lt;/h4&gt;Kernel VICReg避免了表示崩溃；在复杂或小规模数据任务上提高了性能；在MNIST、CIFAR-10、STL-10、TinyImageNet和ImageNetNet上展示了一致的性能提升；在非线性结构突出的数据集上改进尤为显著；UMAP可视化证实基于核的嵌入表现出更好的等距性和类分离。&lt;h4&gt;结论&lt;/h4&gt;将自监督学习目标核化是将经典核方法与现代表征学习相结合的有前途的方向。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为一种强大的表征学习范式，通过优化几何目标（如对增强的不变性、方差保持和特征解相关）而无需标签。然而，大多数现有方法在欧几里得空间中操作，限制了它们捕捉非线性依赖和几何结构的能力。在这项工作中，我们提出了Kernel VICReg，一种新颖的自监督学习框架，将VICReg目标提升到再生核希尔伯特空间中。通过核化损失函数的各个项（方差、不变性和协方差），我们获得了一种在双中心核矩阵和希尔伯特-施密特范数上操作的一般公式，能够进行非线性特征学习而无需显式映射。我们证明，Kernel VICReg不仅避免了表示崩溃，还在具有复杂或小规模数据的任务上提高了性能。在多个数据集上的经验评估显示，相比欧几里得VICReg有一致的性能提升，特别是在非线性结构突出的数据集上改进尤为显著。UMAP可视化进一步证实，基于核的嵌入表现出更好的等距性和类分离。我们的结果表明，将自监督学习目标核化是将经典核方法与现代表征学习相结合的有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful paradigm forrepresentation learning by optimizing geometric objectives--such as invarianceto augmentations, variance preservation, and feature decorrelation--withoutrequiring labels. However, most existing methods operate in Euclidean space,limiting their ability to capture nonlinear dependencies and geometricstructures. In this work, we propose Kernel VICReg, a novel self-supervisedlearning framework that lifts the VICReg objective into a Reproducing KernelHilbert Space (RKHS). By kernelizing each term of the loss-variance,invariance, and covariance--we obtain a general formulation that operates ondouble-centered kernel matrices and Hilbert-Schmidt norms, enabling nonlinearfeature learning without explicit mappings.  We demonstrate that Kernel VICReg not only avoids representational collapsebut also improves performance on tasks with complex or small-scale data.Empirical evaluations across MNIST, CIFAR-10, STL-10, TinyImageNet, andImageNet100 show consistent gains over Euclidean VICReg, with particularlystrong improvements on datasets where nonlinear structures are prominent. UMAPvisualizations further confirm that kernel-based embeddings exhibit betterisometry and class separation. Our results suggest that kernelizing SSLobjectives is a promising direction for bridging classical kernel methods withmodern representation learning.</description>
      <author>example@mail.com (M. Hadi Sepanj, Benyamin Ghojogh, Paul Fieguth)</author>
      <guid isPermaLink="false">2509.07289v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data</title>
      <link>http://arxiv.org/abs/2509.07198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Fed-REACT是一种针对异构且演变客户端数据的联邦学习框架，通过结合表示学习和进化聚类的两阶段过程，提高了模型的准确性和鲁棒性&lt;h4&gt;背景&lt;/h4&gt;集中式机器学习存在高资源成本和隐私问题，联邦学习(FL)作为替代方案允许客户端协作训练全局模型同时保持数据本地化。然而，实际部署中客户端数据分布随时间演变且在不同客户端间差异显著，这种异质性降低了标准FL算法的性能&lt;h4&gt;目的&lt;/h4&gt;提出一种针对异构且演变的客户端数据的联邦学习框架&lt;h4&gt;方法&lt;/h4&gt;引入Fed-REACT框架，结合表示学习和进化聚类的两阶段过程：(1)第一阶段：每个客户端学习局部模型从其数据中提取特征表示；(2)第二阶段：服务器基于这些表示动态地将客户端分组为集群，并协调集群特定任务的模型训练&lt;h4&gt;主要发现&lt;/h4&gt;对表示学习阶段提供了理论分析，实验证明Fed-REACT在真实数据集上实现了更高的准确性和鲁棒性&lt;h4&gt;结论&lt;/h4&gt;Fed-REACT是处理异构且演变客户端数据的有效联邦学习框架&lt;h4&gt;翻译&lt;/h4&gt;受集中式机器学习相关的高资源成本和隐私问题启发，联邦学习(FL)已成为一种高效的替代方案，使客户端能够在保持数据本地化的同时协作训练全局模型。然而，在实际部署中，客户端数据分布通常随时间演变且在不同客户端间存在显著差异，引入了异质性，降低了标准FL算法的性能。在这项工作中，我们引入了Fed-REACT，一种为异构且演变的客户端数据设计的联邦学习框架。Fed-REACT将表示学习与进化聚类结合在一个两阶段过程中：(1)在第一阶段，每个客户端学习一个局部模型，从其数据中提取特征表示；(2)在第二阶段，服务器基于这些表示动态地将客户端分组为集群，并协调针对下游目标（如分类或回归）的集群特定任务模型训练。我们对表示学习阶段提供了理论分析，并通过实证证明Fed-REACT在真实数据集上实现了更高的准确性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motivated by the high resource costs and privacy concerns associated withcentralized machine learning, federated learning (FL) has emerged as anefficient alternative that enables clients to collaboratively train a globalmodel while keeping their data local. However, in real-world deployments,client data distributions often evolve over time and differ significantlyacross clients, introducing heterogeneity that degrades the performance ofstandard FL algorithms. In this work, we introduce Fed-REACT, a federatedlearning framework designed for heterogeneous and evolving client data.Fed-REACT combines representation learning with evolutionary clustering in atwo-stage process: (1) in the first stage, each client learns a local model toextracts feature representations from its data; (2) in the second stage, theserver dynamically groups clients into clusters based on these representationsand coordinates cluster-wise training of task-specific models for downstreamobjectives such as classification or regression. We provide a theoreticalanalysis of the representation learning stage, and empirically demonstrate thatFed-REACT achieves superior accuracy and robustness on real-world datasets.</description>
      <author>example@mail.com (Yiyue Chen, Usman Akram, Chianing Wang, Haris Vikalo)</author>
      <guid isPermaLink="false">2509.07198v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition</title>
      <link>http://arxiv.org/abs/2509.06570v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, 2025 IEEE/CVF International Conference on  Computer Vision Workshops&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为保留性角度表示学习（RARL）的方法用于增量开放集识别（IOSR），解决了现有方法在动态场景中难以维持决策边界区分能力的问题。通过在角度空间中对齐未知表示并采用虚拟内在交互训练策略和分层校正策略，该方法有效减轻了表示漂移和特征空间扭曲，在CIFAR100和TinyImageNet数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的开放集识别方法通常为静态场景设计，模型分类已知类别并识别固定范围内的未知类别，这与从连续数据流中逐步识别新出现的未知类别的期望不符。在动态场景中，由于无法访问之前的训练数据，决策边界的区分能力难以维持，导致严重的类别间混淆。&lt;h4&gt;目的&lt;/h4&gt;解决增量开放集识别中由于无法访问历史训练数据导致的决策边界区分能力下降和类别间混淆问题，提出一种能够有效处理连续数据流中新出现未知类别的方法。&lt;h4&gt;方法&lt;/h4&gt;提出保留性角度表示学习（RARL）方法，包括：1) 在等角紧框架构建的角度空间内，让未知表示围绕非活跃原型对齐，减轻表示漂移；2) 采用虚拟内在交互（VII）训练策略，通过边界接近的虚拟类强制清晰的类别间边界；3) 设计分层校正策略优化决策边界，减轻样本不平衡导致的表示偏差和特征空间扭曲。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR100和TinyImageNet数据集上的实验结果表明，所提出的RARL方法在各种任务设置下都达到了最先进的性能，为增量开放集识别建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;RARL方法通过有效的表示学习和边界优化策略，成功解决了增量开放集识别中的关键挑战，为处理连续数据流中的未知类别识别提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现有的开放集识别方法通常为静态场景设计，模型旨在分类已知类别并识别固定范围内的未知类别。这与模型应该从连续数据流中逐步识别新出现的未知类别并获取相应知识的期望不符。在这样不断变化的场景中，由于无法访问之前的训练数据，开放集识别决策边界的区分能力难以维持，导致严重的类别间混淆。为解决此问题，我们提出了用于增量开放集识别的保留性角度表示学习（RARL）。在RARL中，未知表示被鼓励在等角紧框架构建的角度空间内围绕非活跃原型对齐，从而减轻知识更新过程中的表示漂移。具体来说，我们采用虚拟内在交互（VII）训练策略，通过边界接近的虚拟类强制清晰的类别间边界，从而压缩已知表示。此外，还设计了一种分层校正策略来优化决策边界，减轻由于新旧类别和正负类别样本不平衡导致的表示偏差和特征空间扭曲。我们在CIFAR100和TinyImageNet数据集上进行了全面评估，为IOSR建立了新的基准。各种任务设置下的实验结果表明，所提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing open set recognition (OSR) methods are typically designed for staticscenarios, where models aim to classify known classes and identify unknown oneswithin fixed scopes. This deviates from the expectation that the model shouldincrementally identify newly emerging unknown classes from continuous datastreams and acquire corresponding knowledge. In such evolving scenarios, thediscriminability of OSR decision boundaries is hard to maintain due torestricted access to former training data, causing severe inter-classconfusion. To solve this problem, we propose retentive angular representationlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknownrepresentations are encouraged to align around inactive prototypes within anangular space constructed under the equiangular tight frame, thereby mitigatingexcessive representation drift during knowledge updates. Specifically, we adopta virtual-intrinsic interactive (VII) training strategy, which compacts knownrepresentations by enforcing clear inter-class margins throughboundary-proximal virtual classes. Furthermore, a stratified rectificationstrategy is designed to refine decision boundaries, mitigating representationbias and feature space distortion caused by imbalances between old/new andpositive/negative class samples. We conduct thorough evaluations on CIFAR100and TinyImageNet datasets and establish a new benchmark for IOSR. Experimentalresults across various task setups demonstrate that the proposed methodachieves state-of-the-art performance.</description>
      <author>example@mail.com (Runqing Yang, Yimin Fu, Changyuan Wu, Zhunga Liu)</author>
      <guid isPermaLink="false">2509.06570v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Guided Diffusion Transformer with Spherical Harmonic Posterior Sampling for High-Fidelity Angular Super-Resolution in Diffusion MRI</title>
      <link>http://arxiv.org/abs/2509.07020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Physics-Guided Diffusion Transformer (PGDiT)方法，用于从低角分辨率dMRI数据重建高角分辨率信号，通过整合物理先验知识提高重建质量。&lt;h4&gt;背景&lt;/h4&gt;现有dMRI角超分辨率方法在恢复细粒度角细节和保持高保真度方面存在局限，主要由于q空间几何建模不足和物理约束整合不够充分。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够在训练和推理阶段探索物理先验的模型，实现高质量的高角分辨率dMRI重建。&lt;h4&gt;方法&lt;/h4&gt;训练阶段采用Q-space Geometry-Aware Module (QGAM)结合b向量调制和随机角掩码促进方向感知学习；推理阶段使用两阶段Spherical Harmonics-Guided Posterior Sampling (SHPS)进行从粗到细的细化，确保物理合理重建。&lt;h4&gt;主要发现&lt;/h4&gt;在ASR任务、DTI和NODDI应用上的实验表明，PGDiT在细节恢复和数据保真度方面优于现有深度学习模型。&lt;h4&gt;结论&lt;/h4&gt;PGDiT提供了一种新的生成式ASR框架，能实现高保真度的高角分辨率dMRI重建，在神经科学和临床研究中有应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;扩散磁共振成像(dMRI)角超分辨率(ASR)旨在从有限的低角分辨率(LAR)数据中重建高角分辨率(HAR)信号而不延长扫描时间。然而，现有方法在恢复细粒度角细节或保持高保真度方面存在局限，这是由于对q空间几何建模不足以及物理约束整合不够充分。本文引入了一个Physics-Guided Diffusion Transformer (PGDiT)，旨在探索训练和推理阶段的物理先验知识。在训练阶段，Q-space Geometry-Aware Module (QGAM)结合b向量调制和随机角掩码促进方向感知表示学习，使网络能够从稀疏和嘈杂数据中生成方向一致且具有精细角细节的重建。在推理阶段，两阶段Spherical Harmonics-Guided Posterior Sampling (SHPS)强制与获取数据对齐，然后基于热扩散的SH正则化确保物理合理的重建。这种从粗到细的细化策略减轻了纯数据驱动或生成模型中常见的过度平滑和伪影问题。在ASR任务、DTI和NODDI应用上的大量实验表明，PGDiT在细节恢复和数据保真度方面优于现有深度学习模型。我们的方法提出了一个新的生成式ASR框架，能够提供高保真度的高角分辨率dMRI重建，在神经科学和临床研究中有潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion MRI (dMRI) angular super-resolution (ASR) aims to reconstructhigh-angular-resolution (HAR) signals from limited low-angular-resolution (LAR)data without prolonging scan time. However, existing methods are limited inrecovering fine-grained angular details or preserving high fidelity due toinadequate modeling of q-space geometry and insufficient incorporation ofphysical constraints. In this paper, we introduce a Physics-Guided DiffusionTransformer (PGDiT) designed to explore physical priors throughout bothtraining and inference stages. During training, a Q-space Geometry-Aware Module(QGAM) with b-vector modulation and random angular masking facilitatesdirection-aware representation learning, enabling the network to generatedirectionally consistent reconstructions with fine angular details from sparseand noisy data. In inference, a two-stage Spherical Harmonics-Guided PosteriorSampling (SHPS) enforces alignment with the acquired data, followed byheat-diffusion-based SH regularization to ensure physically plausiblereconstructions. This coarse-to-fine refinement strategy mitigatesoversmoothing and artifacts commonly observed in purely data-driven orgenerative models. Extensive experiments on general ASR tasks and twodownstream applications, Diffusion Tensor Imaging (DTI) and Neurite OrientationDispersion and Density Imaging (NODDI), demonstrate that PGDiT outperformsexisting deep learning models in detail recovery and data fidelity. Ourapproach presents a novel generative ASR framework that offers high-fidelityHAR dMRI reconstructions, with potential applications in neuroscience andclinical research.</description>
      <author>example@mail.com (Mu Nan, Taohui Xiao, Ruoyou Wu, Shoujun Yu, Ye Li, Hairong Zheng, Shanshan Wang)</author>
      <guid isPermaLink="false">2509.07020v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title>
      <link>http://arxiv.org/abs/2509.05685v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MSRFormer，一种新的道路网络表示学习框架，通过整合多尺度空间交互解决城市道路网络的异构性和层次性问题。该框架利用空间流卷积提取小尺度特征，识别尺度依赖的空间交互区域，并通过图Transformer捕获多尺度空间依赖关系。实验表明MSRFormer在道路网络分析任务中优于基线方法，复杂道路网络结构中性能提升可达16%。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异构性和层次性特点对准确表示学习构成了挑战。图神经网络在聚合邻接节点特征时，由于其同质性假设和对单一结构尺度的关注而面临困难。&lt;h4&gt;目的&lt;/h4&gt;为了解决图神经网络在城市道路网络表示学习中的局限性，本文提出了MSRFormer框架，旨在通过整合多尺度空间交互来处理流异质性和长距离依赖问题，从而提高道路网络表示学习的准确性。&lt;h4&gt;方法&lt;/h4&gt;MSRFormer使用空间流卷积从大型轨迹数据集中提取小尺度特征，识别尺度依赖的空间交互区域以捕获道路网络的空间结构和流异质性。采用图Transformer捕获多尺度复杂空间依赖关系，通过残差连接融合空间交互特征，并将融合后的特征输入对比学习算法以获得最终的道路网络表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。与最先进的基线方法相比，在复杂道路网络结构中性能提升可达16%。交通相关任务从整合轨迹数据中获益更多，且规模效应与空间交互流异质性之间存在明显的关联模式。&lt;h4&gt;结论&lt;/h4&gt;该研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互中规模效应与流异质性相互作用之间的不同关联模式。MSRFormer通过多尺度方法有效解决了城市道路网络的异构性和层次性挑战。&lt;h4&gt;翻译&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异构性和层次性特点对准确表示学习构成了挑战。图神经网络在聚合邻接节点特征时，由于其同质性假设和对单一结构尺度的关注而面临困难。为解决这些问题，本文提出了MSRFormer，一种新颖的道路网络表示学习框架，通过解决流异质性和长距离依赖问题整合多尺度空间交互。它使用空间流卷积从大型轨迹数据集中提取小尺度特征，并识别尺度依赖的空间交互区域以捕获道路网络的空间结构和流异质性。通过采用图Transformer，MSRFormer有效捕获了多尺度复杂空间依赖关系。空间交互特征通过残差连接融合，然后输入对比学习算法以获得最终的道路网络表示。在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，交通相关任务从整合轨迹数据中获益更多，在复杂道路网络结构中与最具竞争力的基线方法相比性能提升可达16%。该研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互中规模效应与流异质性相互作用之间的不同关联模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transforming road network data into vector representations using deeplearning has proven effective for road network analysis. However, urban roadnetworks' heterogeneous and hierarchical nature poses challenges for accuraterepresentation learning. Graph neural networks, which aggregate features fromneighboring nodes, often struggle due to their homogeneity assumption and focuson a single structural scale. To address these issues, this paper presentsMSRFormer, a novel road network representation learning framework thatintegrates multi-scale spatial interactions by addressing their flowheterogeneity and long-distance dependencies. It uses spatial flow convolutionto extract small-scale features from large trajectory datasets, and identifiesscale-dependent spatial interaction regions to capture the spatial structure ofroad networks and flow heterogeneity. By employing a graph transformer,MSRFormer effectively captures complex spatial dependencies across multiplescales. The spatial interaction features are fused using residual connections,which are fed to a contrastive learning algorithm to derive the final roadnetwork representation. Validation on two real-world datasets demonstrates thatMSRFormer outperforms baseline methods in two road network analysis tasks. Theperformance gains of MSRFormer suggest the traffic-related task benefits morefrom incorporating trajectory data, also resulting in greater improvements incomplex road network structures with up to 16% improvements compared to themost competitive baseline method. This research provides a practical frameworkfor developing task-agnostic road network representation models and highlightsdistinct association patterns of the interplay between scale effects and flowheterogeneity of spatial interactions.</description>
      <author>example@mail.com (Jian Yang, Jiahui Wu, Li Fang, Hongchao Fan, Bianying Zhang, Huijie Zhao, Guangyi Yang, Rui Xin, Xiong You)</author>
      <guid isPermaLink="false">2509.05685v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges</title>
      <link>http://arxiv.org/abs/2509.07887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图神经网络在药物发现领域得到广泛应用，能够处理药物分子模型等图结构数据，本文全面涵盖了多个研究类别并提供了未来工作指导。&lt;h4&gt;背景&lt;/h4&gt;图神经网络因其处理图结构数据的能力，在药物发现这一复杂领域受到关注。&lt;h4&gt;目的&lt;/h4&gt;为图神经网络在药物发现领域的未来工作提供指导。&lt;h4&gt;方法&lt;/h4&gt;全面回顾和总结图神经网络在药物发现各类研究中的应用，包括分子属性预测、药物-药物相互作用研究等。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络已应用于药物发现研究的多个类别，包括分子属性预测、药物-药物相互作用研究、微生物组相互作用预测、药物重定位、逆合成和新药设计等。&lt;h4&gt;结论&lt;/h4&gt;图神经网络在药物发现领域具有广泛应用前景，本文为相关研究提供了全面概述和未来方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在药物发现这一复杂领域受到关注，因为它们能够处理药物分子模型等图结构数据。这一方法已在已发表文献中产生了众多方法和模型，涵盖药物发现研究的多个类别。本文全面涵盖了这些研究类别，包括最近的论文，即分子属性预测（包括药物-靶点结合亲和力预测）、药物-药物相互作用研究、微生物组相互作用预测、药物重定位、逆合成和新药设计，并为图神经网络在药物发现领域的未来工作提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have gained traction in the complex domain ofdrug discovery because of their ability to process graph-structured data suchas drug molecule models. This approach has resulted in a myriad of methods andmodels in published literature across several categories of drug discoveryresearch. This paper covers the research categories comprehensively with recentpapers, namely molecular property prediction, including drug-target bindingaffinity prediction, drug-drug interaction study, microbiome interactionprediction, drug repositioning, retrosynthesis, and new drug design, andprovides guidance for future work on GNNs for drug discovery.</description>
      <author>example@mail.com (Katherine Berry, Liang Cheng)</author>
      <guid isPermaLink="false">2509.07887v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing</title>
      <link>http://arxiv.org/abs/2509.07725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为IBN的可解释双向建模网络，用于解决多变量时间序列预测中的缺失变量问题，结合不确定性感知插值和高斯核图卷积技术，在各种缺失率场景下实现了最先进的预测性能。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列预测面临缺失变量的挑战，这些缺失变量阻碍了传统时空图神经网络对变量间相关性的建模。现有方法GinAR虽然首次使用基于注意力的插补和自适应图学习处理变量缺失，但缺乏可解释性且无法捕获更多潜在的时间模式。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提供一个更可靠且可解释的框架来处理缺失变量的多变量时间序列预测问题。&lt;h4&gt;方法&lt;/h4&gt;提出可解释的双向建模网络（IBN），集成不确定性感知插值（UAI）和高斯核图卷积（GGCN）。IBN使用MC Dropout估计重建值的不确定性，并应用不确定性加权策略降低高风险重建。GGCN明确建模变量间的空间相关性，双向RU增强时间依赖建模。&lt;h4&gt;主要发现&lt;/h4&gt;在各种缺失率场景下，IBN实现了最先进的预测性能，为处理缺失变量的多变量时间序列预测提供了更可靠且可解释的框架。&lt;h4&gt;结论&lt;/h4&gt;IBN成功解决了多变量时间序列预测中缺失变量带来的挑战，通过结合不确定性感知插值和高斯核图卷积技术，提供了更可靠和可解释的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列预测（MTSF）通常面临来自缺失变量的挑战，这阻碍了传统的时空图神经网络对变量间相关性的建模。虽然GinAR首次使用基于注意力的插补和自适应图学习来处理变量缺失，但它缺乏可解释性，并且由于其简单的递归单元（RU）而无法捕获更多潜在的时间模式。为了克服这些局限性，我们提出了可解释的双向建模网络（IBN），集成了不确定性感知插值（UAI）和高斯核图卷积（GGCN）。IBN使用MC Dropout估计重建值的不确定性，并应用不确定性加权策略来降低高风险重建。GGCN明确建模变量间的空间相关性，而双向RU增强了时间依赖建模。大量实验表明，在各种缺失率场景下，IBN实现了最先进的预测性能，为具有缺失变量的MTSF提供了更可靠和可解释的框架。代码可在https://github.com/zhangth1211/NICLab-IBN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series forecasting (MTSF) often faces challenges frommissing variables, which hinder conventional spatial-temporal graph neuralnetworks in modeling inter-variable correlations. While GinAR addressesvariable missing using attention-based imputation and adaptive graph learningfor the first time, it lacks interpretability and fails to capture more latenttemporal patterns due to its simple recursive units (RUs). To overcome theselimitations, we propose the Interpretable Bidirectional-modeling Network (IBN),integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-basedGraph Convolution (GGCN). IBN estimates the uncertainty of reconstructed valuesusing MC Dropout and applies an uncertainty-weighted strategy to mitigatehigh-risk reconstructions. GGCN explicitly models spatial correlations amongvariables, while a bidirectional RU enhances temporal dependency modeling.Extensive experiments show that IBN achieves state-of-the-art forecastingperformance under various missing-rate scenarios, providing a more reliable andinterpretable framework for MTSF with missing variables. Code is available at:https://github.com/zhangth1211/NICLab-IBN.</description>
      <author>example@mail.com (Shusen Ma, Tianhao Zhang, Qijiu Xia, Yun-Bo Zhao)</author>
      <guid isPermaLink="false">2509.07725v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>DeepGraphLog for Layered Neurosymbolic AI</title>
      <link>http://arxiv.org/abs/2509.07665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DeepGraphLog的新型神经符号AI框架，解决了现有框架在处理图结构数据时的局限性，实现了更灵活的神经-符号集成。&lt;h4&gt;背景&lt;/h4&gt;神经符号AI旨在结合神经网络的统计优势和符号推理的可解释性，但当前框架如DeepProbLog强制符号推理必须跟随神经处理，限制了它们对复杂依赖关系的建模能力，特别是在处理图等不规则数据结构时。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理复杂依赖关系，特别是图结构数据的NeSy框架，突破现有框架的限制，实现更灵活的神经-符号集成。&lt;h4&gt;方法&lt;/h4&gt;提出DeepGraphLog，将ProbLog扩展为具有图神经谓词，支持多层神经-符号推理，允许神经和符号组件以任意顺序分层，并将符号表示视为图，通过图神经网络处理。&lt;h4&gt;主要发现&lt;/h4&gt;DeepGraphLog在规划、知识图谱补全（远程监督）和GNN表达能力等任务上展示了其能力，能够有效捕获复杂的关系依赖关系，克服了现有NeSy系统的关键局限性。&lt;h4&gt;结论&lt;/h4&gt;DeepGraphLog拓宽了神经符号AI在图结构领域的适用性，提供了一个更具表现力和灵活的神经-符号集成框架。&lt;h4&gt;翻译&lt;/h4&gt;神经符号AI（NeSy）旨在结合神经网络的统计优势与符号推理的可解释性和结构性。然而，当前NeSy框架如DeepProbLog强制符号推理必须跟随神经处理，这限制了它们对复杂依赖关系的建模能力，特别是在处理图等不规则数据结构时。本文介绍了DeepGraphLog，一种新型NeSy框架，将ProbLog扩展为具有图神经谓词。DeepGraphLog支持多层神经-符号推理，允许神经和符号组件以任意顺序分层。与无法通过神经方法处理符号推理的DeepProbLog不同，DeepGraphLog将符号表示视为图，可通过图神经网络（GNN）处理。我们在规划、知识图谱补全（远程监督）和GNN表达能力等任务上展示了DeepGraphLog的能力。结果表明，DeepGraphLog能有效捕获复杂的关系依赖关系，克服了现有NeSy系统的关键局限性。通过拓宽神经符号AI在图结构领域的适用性，DeepGraphLog为神经-符号集成提供了更具表现力和灵活的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neuralnetworks with the interpretability and structure of symbolic reasoning.However, current NeSy frameworks like DeepProbLog enforce a fixed flow wheresymbolic reasoning always follows neural processing. This restricts theirability to model complex dependencies, especially in irregular data structuressuch as graphs. In this work, we introduce DeepGraphLog, a novel NeSy frameworkthat extends ProbLog with Graph Neural Predicates. DeepGraphLog enablesmulti-layer neural-symbolic reasoning, allowing neural and symbolic componentsto be layered in arbitrary order. In contrast to DeepProbLog, which cannothandle symbolic reasoning via neural methods, DeepGraphLog treats symbolicrepresentations as graphs, enabling them to be processed by Graph NeuralNetworks (GNNs). We showcase the capabilities of DeepGraphLog on tasks inplanning, knowledge graph completion with distant supervision, and GNNexpressivity. Our results demonstrate that DeepGraphLog effectively capturescomplex relational dependencies, overcoming key limitations of existing NeSysystems. By broadening the applicability of neurosymbolic AI tograph-structured domains, DeepGraphLog offers a more expressive and flexibleframework for neural-symbolic integration.</description>
      <author>example@mail.com (Adem Kikaj, Giuseppe Marra, Floris Geerts, Robin Manhaeve, Luc De Raedt)</author>
      <guid isPermaLink="false">2509.07665v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Integrated Gradients for Explaining Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.07648v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Australasian Joint Conference on Artificial  Intelligence (AJCAI) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图的集成梯度(GB-IG)方法，将传统的集成梯度(IG)可解释性技术扩展到图结构数据上，解决了IG不适用于离散图结构的问题。&lt;h4&gt;背景&lt;/h4&gt;集成梯度(IG)是一种常见的可解释性技术，用于解决神经网络的黑盒问题，但它假设数据是连续的，而图是离散结构，这使得IG不适用于图数据。&lt;h4&gt;目的&lt;/h4&gt;将集成梯度(IG)方法扩展到图结构数据上，开发一种专门针对图数据的可解释性技术。&lt;h4&gt;方法&lt;/h4&gt;提出基于图的集成梯度(GB-IG)，作为IG在图数据上的扩展方法。&lt;h4&gt;主要发现&lt;/h4&gt;在四个合成数据集上，GB-IG能够准确识别分类任务中使用的图的关键结构组件；在三个 prevalent 现实世界图数据集上，GB-IG在突出显示节点分类任务中的重要特征方面优于传统的IG方法。&lt;h4&gt;结论&lt;/h4&gt;GB-IG是IG在图数据上的有效扩展，能够更好地处理图结构的离散特性，为图神经网络提供了更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;集成梯度(IG)是一种常见的可解释性技术，用于解决神经网络的黑盒问题。集成梯度假设数据是连续的。图是离散结构，这使得IG不适用于图。在这项工作中，我们引入了基于图的集成梯度(GB-IG)；作为IG在图上的扩展。我们在四个合成数据集上证明，GB-IG能够准确识别分类任务中使用的图的关键结构组件。我们进一步在三个 prevalent 现实世界图数据集上证明，GB-IG在突出显示节点分类任务中的重要特征方面优于IG。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated Gradients (IG) is a common explainability technique to address theblack-box problem of neural networks. Integrated gradients assumes continuousdata. Graphs are discrete structures making IG ill-suited to graphs. In thiswork, we introduce graph-based integrated gradients (GB-IG); an extension of IGto graphs. We demonstrate on four synthetic datasets that GB-IG accuratelyidentifies crucial structural components of the graph used in classificationtasks. We further demonstrate on three prevalent real-world graph datasets thatGB-IG outperforms IG in highlighting important features for node classificationtasks.</description>
      <author>example@mail.com (Lachlan Simpson, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn Chew)</author>
      <guid isPermaLink="false">2509.07648v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>NestGNN: A Graph Neural Network Framework Generalizing the Nested Logit Model for Travel Mode Choice</title>
      <link>http://arxiv.org/abs/2509.07123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的嵌套效用图神经网络(NestGNN)，用于改进离散选择分析，特别是交通方式选择问题。该方法结合了经典嵌套logit模型和深度神经网络的优势，通过引入替代方案图的概念来表示交通方式间的关系。&lt;h4&gt;背景&lt;/h4&gt;嵌套logit模型(NL)已被广泛用于离散选择分析，包括交通方式选择、汽车拥有或位置决策等多种应用。然而，传统NL模型受到其表示能力有限和手工指定效用的限制。虽然研究者引入了深度神经网络(DNNs)来解决这些挑战，但现有DNN无法明确捕捉离散选择情境中的替代方案间的相关性。&lt;h4&gt;目的&lt;/h4&gt;解决传统NL模型的表示能力有限和手工指定效用的问题，以及现有DNN无法明确捕捉离散选择情境中替代方案间相关性的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的概念——替代方案图，来表示交通方式替代方案之间的关系；使用嵌套替代方案图，设计了嵌套效用图神经网络(NestGNN)，作为神经网络家族中经典NL模型的泛化。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，NestGNN在模型表示方面泛化了经典NL模型和现有DNN，同时保留了NL模型关键的二层替代模式：巢内比例替代但巢外非比例替代。经验上，NestGNN显著优于基准模型，特别是比相应的NL模型高出9.2%。从弹性表和替代可视化可以看出，NestGNN保留了NL模型的二层替代模式，但在模型设计空间上表现出更大的灵活性。&lt;h4&gt;结论&lt;/h4&gt;NestGNN在预测、解释方面表现出强大的能力，并且能够灵活地泛化经典NL模型用于分析交通方式选择问题。&lt;h4&gt;翻译&lt;/h4&gt;嵌套logit(NL)已被广泛用于离散选择分析，包括交通方式选择、汽车拥有或位置决策等多种应用。然而，传统NL模型受到其表示能力有限和手工指定效用的限制。虽然研究者引入了深度神经网络(DNNs)来解决这些挑战，但现有DNN无法明确捕捉离散选择情境中的替代方案间的相关性。为应对这些挑战，本研究提出了一个新概念——替代方案图，来表示交通方式替代方案之间的关系。使用嵌套替代方案图，本研究进一步设计了嵌套效用图神经网络(NestGNN)，作为神经网络家族中经典NL模型的泛化。理论上，NestGNN在模型表示方面泛化了经典NL模型和现有DNN，同时保留了NL模型关键的二层替代模式：巢内比例替代但巢外非比例替代。经验上，我们发现NestGNN显著优于基准模型，特别是比相应的NL模型高出9.2%。从弹性表和替代可视化可以看出，NestGNN保留了NL模型的二层替代模式，但在模型设计空间上表现出更大的灵活性。总体而言，我们的研究展示了NestGNN在预测、解释方面的能力，以及其泛化经典NL模型分析交通方式选择的灵活性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nested logit (NL) has been commonly used for discrete choice analysis,including a wide range of applications such as travel mode choice, automobileownership, or location decisions. However, the classical NL models arerestricted by their limited representation capability and handcrafted utilityspecification. While researchers introduced deep neural networks (DNNs) totackle such challenges, the existing DNNs cannot explicitly captureinter-alternative correlations in the discrete choice context. To address thechallenges, this study proposes a novel concept - alternative graph - torepresent the relationships among travel mode alternatives. Using a nestedalternative graph, this study further designs a nested-utility graph neuralnetwork (NestGNN) as a generalization of the classical NL model in the neuralnetwork family. Theoretically, NestGNNs generalize the classical NL models andexisting DNNs in terms of model representation, while retaining the crucialtwo-layer substitution patterns of the NL models: proportional substitutionwithin a nest but non-proportional substitution beyond a nest. Empirically, wefind that the NestGNNs significantly outperform the benchmark models,particularly the corresponding NL models by 9.2\%. As shown by elasticitytables and substitution visualization, NestGNNs retain the two-layersubstitution patterns as the NL model, and yet presents more flexibility in itsmodel design space. Overall, our study demonstrates the power of NestGNN inprediction, interpretation, and its flexibility of generalizing the classicalNL model for analyzing travel mode choice.</description>
      <author>example@mail.com (Yuqi Zhou, Zhanhong Cheng, Lingqian Hu, Yuheng Bu, Shenhao Wang)</author>
      <guid isPermaLink="false">2509.07123v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.06763v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VariSAC的图神经网络增强深度强化学习框架，用于解决RIS辅助ISAC使能的V2X系统中的统一可靠建模和资源优化问题，实现了时间连续的保证连接。&lt;h4&gt;背景&lt;/h4&gt;可重构智能表面与集成感知和通信在车辆网络中的集成实现了动态空间资源管理和环境实时适应，但V2I和V2V连接要求的共存以及高度动态异构的网络拓扑对统一可靠建模和资源优化提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;解决V2I和V2V连接要求共存问题，应对高度动态异构网络拓扑，实现统一可靠建模和资源优化，提供时间连续的保证连接。&lt;h4&gt;方法&lt;/h4&gt;提出VariSAC框架，引入连续连接比率作为统一指标，使用带有残差适配器的图神经网络编码复杂高维系统状态，并通过软演员-评论家代理联合优化信道分配、功率控制和RIS配置。&lt;h4&gt;主要发现&lt;/h4&gt;在真实城市数据集上的大量实验表明，VariSAC在连续V2I ISAC连接和V2V交付可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;h4&gt;结论&lt;/h4&gt;VariSAC有效解决了RIS辅助ISAC使能的V2X系统中的统一可靠建模和资源优化问题，实现了时间连续的保证连接。&lt;h4&gt;翻译&lt;/h4&gt;可重构智能表面与集成感知和通信在车辆网络中的集成实现了动态空间资源管理和对环境变化的实时适应。然而，车辆到基础设施和车辆到车辆连接要求的共存，以及高度动态和异构的网络拓扑，给统一可靠建模和资源优化带来了显著挑战。为解决这些问题，我们提出VariSAC，一种用于RIS辅助、ISAC使能的车辆到万物系统中保证时间连续连接的图神经网络增强深度强化学习框架。具体而言，我们引入了连续连接比率作为统一指标，表征V2I连接的持续时间可靠性和V2V链路概率交付保证，从而统一了它们的连续可靠性语义。接下来，我们采用带有残差适配器的图神经网络来编码复杂高维系统状态，捕捉车辆、基站和RIS节点之间的空间依赖关系。这些表示随后由软演员-评论家代理处理，该代理联合优化信道分配、功率控制和RIS配置，以最大化CCR驱动的长期奖励。在真实城市数据集上的大量实验表明，VariSAC在连续V2I ISAC连接和V2V交付可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of Reconfigurable Intelligent Surfaces (RIS) and IntegratedSensing and Communication (ISAC) in vehicular networks enables dynamic spatialresource management and real-time adaptation to environmental changes. However,the coexistence of distinct vehicle-to-infrastructure (V2I) andvehicle-to-vehicle (V2V) connectivity requirements, together with highlydynamic and heterogeneous network topologies, presents significant challengesfor unified reliability modeling and resource optimization. To address theseissues, we propose VariSAC, a graph neural network (GNN)-augmented deepreinforcement learning framework for assured, time-continuous connectivity inRIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically,we introduce the Continuous Connectivity Ratio (CCR), a unified metric thatcharacterizes the sustained temporal reliability of V2I connections and theprobabilistic delivery guarantees of V2V links, thus unifying their continuousreliability semantics. Next, we employ a GNN with residual adapters to encodecomplex, high-dimensional system states, capturing spatial dependencies amongvehicles, base stations (BS), and RIS nodes. These representations are thenprocessed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channelallocation, power control, and RIS configurations to maximize CCR-drivenlong-term rewards. Extensive experiments on real-world urban datasetsdemonstrate that VariSAC consistently outperforms existing baselines in termsof continuous V2I ISAC connectivity and V2V delivery reliability, enablingpersistent connectivity in highly dynamic vehicular environments.</description>
      <author>example@mail.com (Huijun Tang, Wang Zeng, Ming Du, Pinlong Zhao, Pengfei Jiao, Huaming Wu, Hongjian Sun)</author>
      <guid isPermaLink="false">2509.06763v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Safeguarding Graph Neural Networks against Topology Inference Attacks</title>
      <link>http://arxiv.org/abs/2509.05429v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Acctepted by ACM CCS'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图神经网络(GNNs)中的拓扑隐私风险，提出了一种名为私有图重构(PGR)的新型防御框架，能够在保护图结构隐私的同时保持模型准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为处理图结构数据的有力模型，但其广泛应用引发了严重的隐私问题。先前研究主要关注边缘级别的隐私保护，而忽略了拓扑隐私（即图整体结构的保密性）这一关键但尚未充分探索的威胁。&lt;h4&gt;目的&lt;/h4&gt;全面探讨GNN中的拓扑隐私风险，揭示其对图级推理攻击的脆弱性，并提出有效的防御机制。&lt;h4&gt;方法&lt;/h4&gt;提出了一套拓扑推理攻击(TIAs)方法，仅通过黑盒访问GNN模型就能重建目标训练图的结构；同时引入了私有图重构(PGR)防御框架，这是一个双层优化问题，通过元梯度迭代生成合成训练图，并基于不断演化的图同时更新GNN模型。&lt;h4&gt;主要发现&lt;/h4&gt;GNN极易受到拓扑推理攻击，现有的边缘级别差分隐私机制不足以缓解风险，要么无法减轻风险，要么严重损害模型准确性。&lt;h4&gt;结论&lt;/h4&gt;PGR框架能够显著减少拓扑信息泄露，同时对模型准确性的影响最小。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的有力模型。然而，它们的广泛应用引发了严重的隐私问题。虽然先前研究主要关注边缘级别的隐私保护，但一个关键 yet 尚未充分探索的威胁在于拓扑隐私 - 图整体结构的保密性。在这项工作中，我们对GNN中的拓扑隐私风险进行了全面研究，揭示了它们对图级推理攻击的脆弱性。为此，我们提出了一套拓扑推理攻击(TIAs)，仅通过黑盒访问GNN模型就能重建目标训练图的结构。我们的发现表明，GNN极易受到这些攻击，现有的边缘级别差分隐私机制不足以缓解风险，因为它们要么无法减轻风险，要么严重损害模型准确性。为应对这一挑战，我们引入了私有图重构(PGR)，这是一种新型防御框架，旨在保护拓扑隐私同时保持模型准确性。PGR被表述为一个双层优化问题，其中使用元梯度迭代生成合成训练图，GNN模型则基于不断演化的图同时更新。大量实验证明，PGR显著减少了拓扑信息泄露，同时对模型准确性的影响最小。我们的代码可在 https://github.com/JeffffffFu/PGR 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful models for learningfrom graph-structured data. However, their widespread adoption has raisedserious privacy concerns. While prior research has primarily focused onedge-level privacy, a critical yet underexplored threat lies in topologyprivacy - the confidentiality of the graph's overall structure. In this work,we present a comprehensive study on topology privacy risks in GNNs, revealingtheir vulnerability to graph-level inference attacks. To this end, we propose asuite of Topology Inference Attacks (TIAs) that can reconstruct the structureof a target training graph using only black-box access to a GNN model. Ourfindings show that GNNs are highly susceptible to these attacks, and thatexisting edge-level differential privacy mechanisms are insufficient as theyeither fail to mitigate the risk or severely compromise model accuracy. Toaddress this challenge, we introduce Private Graph Reconstruction (PGR), anovel defense framework designed to protect topology privacy while maintainingmodel accuracy. PGR is formulated as a bi-level optimization problem, where asynthetic training graph is iteratively generated using meta-gradients, and theGNN model is concurrently updated based on the evolving graph. Extensiveexperiments demonstrate that PGR significantly reduces topology leakage withminimal impact on model accuracy. Our code is available athttps://github.com/JeffffffFu/PGR.</description>
      <author>example@mail.com (Jie Fu, Hong Yuan, Zhili Chen, Wendy Hui Wang)</author>
      <guid isPermaLink="false">2509.05429v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation</title>
      <link>http://arxiv.org/abs/2509.07923v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作介绍了一种名为ToothMCL的多模态预训练框架，用于牙齿分割，整合了CBCT和IOS两种数据模态，通过多模态对比学习提高了分割精度，并在多个独立数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;数字牙科代表了现代牙科实践的重大转变，其基础是准确获取患者牙齿的数字表示。然而，尽管数字牙科技术日益受到关注，现有的分割方法通常缺乏严格的验证，性能有限且临床应用性差。&lt;h4&gt;目的&lt;/h4&gt;开发一种多模态预训练框架，用于提高牙齿分割的准确性和临床适用性，实现精确的多类分割和准确的FDI牙齿编号识别。&lt;h4&gt;方法&lt;/h4&gt;提出ToothMCL（Tooth Multimodal Contrastive Learning）框架，整合体积型CBCT和基于表面的IOS数据模态，通过多模态对比学习捕获模态不变表示。同时构建了CBCT-IOS3.8K数据集，包含3,867名患者的配对CBCT和IOS数据，并在一系列独立数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;ToothMCL在内部和外部测试中都达到了最先进的性能，Dice相似系数在CBCT分割上提高了12%，在IOS分割上提高了8%。该方法在牙齿组分割方面持续超越现有方法，并在不同的成像条件和临床场景中表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ToothMCL通过多模态对比学习有效建模了细粒度解剖特征，显著提高了牙齿分割的准确性和临床适用性，为数字牙科实践提供了强有力的工具。&lt;h4&gt;翻译&lt;/h4&gt;数字牙科代表了现代牙科实践的一次转变性变革。这一转变的基础是准确获取患者牙齿的数字表示，这通常通过分割锥形束计算机断层扫描和口腔内扫描获得。尽管数字牙科技术日益受到关注，现有的分割方法通常缺乏严格的验证，性能有限且临床应用性差。据我们所知，这是首次引入用于牙齿分割的多模态预训练框架的工作。我们提出了ToothMCL，这是一种用于预训练的牙齿多模态对比学习，整合了体积型和基于表面的模态。通过多模态对比学习捕获模态不变表示，我们的方法有效地建模了细粒度解剖特征，实现了精确的多类分割和准确的Fédération Dentaire Internationale牙齿编号识别。除了该框架外，我们还构建了CBCT-IOS3.8K，这是迄今为止最大的配对CBCT和IOS数据集，包含3,867名患者。随后，我们在一系列独立数据集上评估了ToothMCL，这是迄今为止最大且最多样化的评估。我们的方法在内部和外部测试中都达到了最先进的性能，Dice相似系数在CBCT分割上提高了12%，在IOS分割上提高了8%。此外，ToothMCL在牙齿组方面持续超越现有方法，并在不同的成像条件和临床场景中表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital dentistry represents a transformative shift in modern dentalpractice. The foundational step in this transformation is the accurate digitalrepresentation of the patient's dentition, which is obtained from segmentedCone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite thegrowing interest in digital dental technologies, existing segmentationmethodologies frequently lack rigorous validation and demonstrate limitedperformance and clinical applicability. To the best of our knowledge, this isthe first work to introduce a multimodal pretraining framework for toothsegmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning forpretraining that integrates volumetric (CBCT) and surface-based (IOS)modalities. By capturing modality-invariant representations through multimodalcontrastive learning, our approach effectively models fine-grained anatomicalfeatures, enabling precise multi-class segmentation and accurate identificationof F\'ed\'eration Dentaire Internationale (FDI) tooth numbering. Along with theframework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset todate, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensivecollection of independent datasets, representing the largest and most diverseevaluation to date. Our method achieves state-of-the-art performance in bothinternal and external testing, with an increase of 12\% for CBCT segmentationand 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC).Furthermore, ToothMCL consistently surpasses existing approaches in toothgroups and demonstrates robust generalizability across varying imagingconditions and clinical scenarios.</description>
      <author>example@mail.com (Moo Hyun Son, Juyoung Bae, Zelin Qiu, Jiale Peng, Kai Xin Li, Yifan Lin, Hao Chen)</author>
      <guid isPermaLink="false">2509.07923v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2509.07623v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的自监督交叉编码器框架，用于解决神经退行性疾病诊断中依赖大量标记数据和缺乏可解释性的问题。&lt;h4&gt;背景&lt;/h4&gt;深度学习在从MRI数据诊断神经退行性疾病方面显示出巨大潜力，但现有方法主要依赖大量标记数据且往往产生缺乏可解释性的表示。&lt;h4&gt;目的&lt;/h4&gt;解决神经退行性疾病诊断中依赖大量标记数据和缺乏可解释性的双重挑战。&lt;h4&gt;方法&lt;/h4&gt;提出一种自监督交叉编码器框架，利用纵向MRI扫描中的时间连续性进行监督。该框架将学习到的表示解耦为静态表示（通过对比学习约束，捕获稳定的解剖特征）和动态表示（由输入梯度正则化引导，反映时间变化，可微调用于下游分类任务）。&lt;h4&gt;主要发现&lt;/h4&gt;在阿尔茨海默病神经影像学倡议(ADNI)数据集上，该方法实现了 superior 的分类准确性和改进的可解释性。学习到的表示在开放获取影像学研究系列(OASIS)数据集上表现出强大的零样本泛化能力，并在帕金森病进展标志物倡议(PPMI)数据集上表现出跨任务泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该方法在神经退行性疾病诊断中表现出色，具有良好的可解释性和泛化能力，代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在从MRI数据诊断神经退行性疾病方面显示出巨大潜力。然而，大多数现有方法严重依赖大量标记数据，并且往往产生缺乏可解释性的表示。为了解决这两个挑战，我们提出了一种新颖的自监督交叉编码器框架，利用纵向MRI扫描中的时间连续性进行监督。该框架将学习到的表示解耦为两个组成部分：静态表示，通过对比学习约束，捕获稳定的解剖特征；动态表示，由输入梯度正则化引导，反映时间变化，可有效微调用于下游分类任务。在阿尔茨海默病神经影像学倡议(ADNI)数据集上的实验结果表明，我们的方法实现了 superior 的分类准确性和改进的可解释性。此外，学习到的表示在开放获取影像学研究系列(OASIS)数据集上表现出强大的零样本泛化能力，并在帕金森病进展标志物倡议(PPMI)数据集上表现出跨任务泛化能力。所提出方法的代码将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has shown significant potential in diagnosing neurodegenerativediseases from MRI data. However, most existing methods rely heavily on largevolumes of labeled data and often yield representations that lackinterpretability. To address both challenges, we propose a novelself-supervised cross-encoder framework that leverages the temporal continuityin longitudinal MRI scans for supervision. This framework disentangles learnedrepresentations into two components: a static representation, constrained bycontrastive learning, which captures stable anatomical features; and a dynamicrepresentation, guided by input-gradient regularization, which reflectstemporal changes and can be effectively fine-tuned for downstreamclassification tasks. Experimental results on the Alzheimer's DiseaseNeuroimaging Initiative (ADNI) dataset demonstrate that our method achievessuperior classification accuracy and improved interpretability. Furthermore,the learned representations exhibit strong zero-shot generalization on the OpenAccess Series of Imaging Studies (OASIS) dataset and cross-task generalizationon the Parkinson Progression Marker Initiative (PPMI) dataset. The code for theproposed method will be made publicly available.</description>
      <author>example@mail.com (Fangqi Cheng, Yingying Zhao, Xiaochen Yang)</author>
      <guid isPermaLink="false">2509.07623v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Water Demand Forecasting of District Metered Areas through Learned Consumer Representations</title>
      <link>http://arxiv.org/abs/2509.07515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at European Conference for Signal Procesing - EUSIPCO 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新型的短期水需求预测方法，通过无监督对比学习对用户进行分类，并使用小波变换卷积网络进行预测，在真实DMA区域测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;智能水表技术的进步显著提升了对水务设施的监控和管理能力。在气候变化带来不确定性的背景下，保障水资源供应已成为具有广泛社会经济影响的全球性紧迫问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对分区计量区域(DMAs)的短期水需求预测新方法，该方法包含商业、农业和住宅消费者。&lt;h4&gt;方法&lt;/h4&gt;首先应用无监督对比学习对DMA内具有不同消费行为的终端用户进行分类；然后将这些不同的消费行为作为特征，使用结合历史数据和衍生表示的小波变换卷积网络进行后续的需求预测任务。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在真实DMA区域进行了六个月的测试，在不同DMA区域的MAPE预测性能上有所提高，最大改进为4.9%。此外，该方法还能识别出其行为受社会经济因素影响的消费者，增强了关于影响需求的确定性模式的前期认识。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过结合用户行为分类和先进的预测模型，有效提高了水需求预测的准确性，并为理解影响需求的社会经济因素提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;智能水表技术的进步显著提高监控和管理水务设施的能力。在气候变化带来不确定性的背景下，保障水资源供应已成为具有广泛社会经济影响的全球性紧迫问题。来自终端用户的每小时消费数据为具有不同消费模式区域的需求预测提供了重要见解。然而，由于气象条件等非确定性影响因素，水需求预测仍然具有挑战性。本文介绍了一种针对包含商业、农业和住宅消费者的分区计量区域(DMAs)的短期水需求预测新方法。应用无监督对比学习对DMA内存在的不同消费行为的终端用户进行分类。随后，将不同的消费行为作为特征，在后续的需求预测任务中使用结合历史数据和衍生表示的小波变换卷积网络。所提出的方法在真实DMA区域进行了六个月的测试，展示了在不同DMA区域MAEP方面的改进预测性能，最大改进为4.9%。此外，它还识别出其行为受社会经济因素影响的消费者，增强了关于影响需求的确定性模式的前期认识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in smart metering technologies have significantly improved theability to monitor and manage water utilities. In the context of increasinguncertainty due to climate change, securing water resources and supply hasemerged as an urgent global issue with extensive socioeconomic ramifications.Hourly consumption data from end-users have yielded substantial insights forprojecting demand across regions characterized by diverse consumption patterns.Nevertheless, the prediction of water demand remains challenging due toinfluencing non-deterministic factors, such as meteorological conditions. Thiswork introduces a novel method for short-term water demand forecasting forDistrict Metered Areas (DMAs) which encompass commercial, agricultural, andresidential consumers. Unsupervised contrastive learning is applied tocategorize end-users according to distinct consumption behaviors present withina DMA. Subsequently, the distinct consumption behaviors are utilized asfeatures in the ensuing demand forecasting task using wavelet-transformedconvolutional networks that incorporate a cross-attention mechanism combiningboth historical data and the derived representations. The proposed approach isevaluated on real-world DMAs over a six-month period, demonstrating improvedforecasting performance in terms of MAPE across different DMAs, with a maximumimprovement of 4.9%. Additionally, it identifies consumers whose behavior isshaped by socioeconomic factors, enhancing prior knowledge about thedeterministic patterns that influence demand.</description>
      <author>example@mail.com (Adithya Ramachandran, Thorkil Flensmark B. Neergaard, Tomás Arias-Vergara, Andreas Maier, Siming Bayer)</author>
      <guid isPermaLink="false">2509.07515v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CAME-AB是一种用于抗体结合位点预测的新型跨模态注意力框架，结合了多种生物学模态和自适应融合机制，在多个评估指标上表现出色。&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跨模态注意力框架CAME-AB，用于稳健的抗体结合位点预测，解决现有方法无法识别抗体特异性结合位点的问题。&lt;h4&gt;方法&lt;/h4&gt;CAME-AB集成了五种生物学基础模态：原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN精化的生化图；提出了自适应模态融合模块，根据全局相关性和输入特定贡献动态加权每种模态；结合了Transformer编码器和MoE模块促进特征专业化和能力扩展；引入了监督对比学习目标塑造潜在空间几何；在训练期间应用随机权重平均提高优化稳定性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上一致优于强基线；消融研究验证了每个架构组件的有效性和多模态特征集成的优势。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过结合多种生物学基础模态和自适应融合机制，有效提高了抗体结合位点预测的准确性，为计算免疫学和治疗性抗体设计提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。在本文中，我们提出了CAME-AB，一种具有专家混合（MoE）主干的新颖跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB将五种基于生物学的模态整合到统一的multimodal表示中，包括原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN精化的生化图。为了增强自适应跨模态推理，我们提出了一个自适应模态融合模块，根据其全局相关性和输入特定贡献动态学习对每种模态进行加权。结合MoE模块的Transformer编码器进一步促进了特征专业化和能力扩展。我们还结合了监督对比学习目标，明确塑造潜在空间几何，鼓励类内紧凑性和类间可分离性。为了提高优化稳定性和泛化能力，我们在训练期间应用随机权重平均。在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在多个指标上始终优于强基线，包括精确度、召回率、F1分数、AUC-ROC和MCC。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。模型实现细节和代码可在https://anonymous.4open.science/r/CAME-AB-C525获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, anovel Cross-modality Attention framework with a Mixture-of-Experts (MoE)backbone for robust antibody binding site prediction. CAME-AB integrates fivebiologically grounded modalities, including raw amino acid encodings, BLOSUMsubstitution profiles, pretrained language model embeddings, structure-awarefeatures, and GCN-refined biochemical graphs, into a unified multimodalrepresentation. To enhance adaptive cross-modal reasoning, we propose an\emph{adaptive modality fusion} module that learns to dynamically weight eachmodality based on its global relevance and input-specific contribution. ATransformer encoder combined with an MoE module further promotes featurespecialization and capacity expansion. We additionally incorporate a supervisedcontrastive learning objective to explicitly shape the latent space geometry,encouraging intra-class compactness and inter-class separability. To improveoptimization stability and generalization, we apply stochastic weight averagingduring training. Extensive experiments on benchmark antibody-antigen datasetsdemonstrate that CAME-AB consistently outperforms strong baselines on multiplemetrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablationstudies further validate the effectiveness of each architectural component andthe benefit of multimodal feature integration. The model implementation detailsand the codes are available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Rui Xiao, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>The Protocol Genome A Self Supervised Learning Framework from DICOM Headers</title>
      <link>http://arxiv.org/abs/2509.06995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Protocol Genome，一种自监督学习系统，通过学习DICOM头部信息的相关性，在完全保留的外部验证中实现了AUROC 0.901（对比基线0.847）和ECE 0.036（对比0.056）的优异性能。&lt;h4&gt;背景&lt;/h4&gt;临床影像通过PACS/DICOM系统传输，程序选择（如扫描仪型号、序列参数等）影响影像质量，这些潜在的混杂因素阻碍了仅基于图像的网络在不同站点间的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;将结构化的DICOM头部信息作为标签，学习协议感知但临床稳健的图像表示，提高模型在不同模态和供应商间的校准能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;获取去标识化DICOM头部的标记化嵌入，结合图像特征使用三种方法建模：(1)协议-图像对比学习，(2)掩蔽协议预测，(3)协议-协议翻译。研究使用了126万项临床数据（7个医疗系统，31台扫描仪，3个供应商）。&lt;h4&gt;主要发现&lt;/h4&gt;相比强大的SSL基线和ImageNet迁移，Protocol Genome在外部AUROC上显著提高（肺栓塞:+0.046，胶质瘤:+0.058，心肌肥大:+0.041），并实现25-37%的校准改进（p &lt; 0.01）。即使仅使用10-20%的标记数据，这些增益仍能保持。&lt;h4&gt;结论&lt;/h4&gt;该技术减少了协议边界处的假阳性，可直接集成到PACS系统中，并已发布包含去标识化和偏见审计的模型卡和部署指南。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种名为协议基因组（Protocol Genome）的自监督学习系统，该系统从DICOM头部信息中学习相关性，并在完全保留的外部验证中实现了AUROC 0.901（对比基线0.847）和ECE 0.036（对比0.058）。我们的方法还提高了CT、MRI、CXR等不同模态和供应商间的校准能力和鲁棒性。临床影像通过PACS/DICOM系统传输，程序选择（扫描仪型号/型号、序列、内核、kVp、TR/TE和切片厚度）会影响对比度、噪声和伪影。这些潜在的混杂因素阻碍了仅基于图像的网络在不同站点间的泛化。我们将结构化的DICOM头部信息作为标签，学习协议感知但临床稳健的图像表示。协议基因组获取去标识化头部字段的标记化嵌入，并使用以下方法与图像特征一起建模：(1)协议-图像对比学习，(2)掩蔽协议预测，(3)协议-协议翻译。基于126万项研究（7个医疗系统，31台扫描仪，3个供应商；CT、MR、CR/DR），我们在以下任务上进行了实验：(A)胸部CT肺栓塞分诊，(B)脑部MRI胶质瘤分级，(C)胸部放射心肌肥大检测。与强大的SSL基线（SimCLR、MAE）以及ImageNet迁移相比，协议基因组在外部AUROC上表现更优（肺栓塞:+0.046，胶质瘤:+0.058，心肌肥大:+0.041），并获得了25-37%的校准改进（p &lt; 0.01，DeLong测试）。虽然增益可能依赖于任务，但在仅使用10-20%的标记数据时仍能保持。从临床角度来看，该技术减少了协议边界处的假阳性，并可应用于PACS系统（DICOM C-FIND/C-MOVE，DICOMweb QIDO/WADO）。我们发布了包含去标识化和偏见审计的模型卡和部署指南。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学影像中的域偏移、扫描仪异质性和隐藏混杂因素问题。这些问题在现实中很重要，因为不同医院和设备的协议差异导致AI模型难以跨站点部署，限制了医学影像AI的临床应用；标签稀缺问题也阻碍了模型训练；而模型可能学习到与临床决策无关的协议特征，导致预测偏差。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将DICOM头文件视为'基因组'代码，借鉴了基因组学的概念，设计了一个多模态自监督学习框架。他们参考了现有的自监督学习方法（如SimCLR、MoCo、MAE）、医学影像深度学习架构（ResNet、ViT等）和域适应技术。创新点在于将协议信息融入自监督学习，通过标记化DICOM头文件、混合注意力融合和对抗性头部来分离协议身份和临床特征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将DICOM头文件从'负担'转变为'第一类自监督信号'，将域偏移从'对手'变为'教师'。实现流程包括：1)数据预处理（选择、标记化、去标识化DICOM头文件）；2)自监督预训练（协议-图像对比学习、掩码协议建模、协议-协议翻译）；3)架构设计（图像和协议编码器、混合注意力融合、对抗性头部）；4)偏见感知微调（梯度反转学习、重要性重加权、采集感知增强）；5)多站点外部评估和子群体分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将DICOM头文件作为基因组代码处理；2)多组件自监督目标（对比学习、掩码建模、协议翻译）；3)混合注意力融合机制；4)对抗性头部分离协议和临床特征；5)与PACS/DICOM工作流集成。相比之前工作，传统方法只考虑像素信息，将协议视为障碍；而本文将协议视为有价值信号，首次系统利用DICOM头文件作为自监督目标，并提供完整的模型卡和部署建议。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Protocol Genome通过将DICOM头文件转化为自监督学习信号，解决了医学影像中的域偏移和标签稀缺问题，实现了跨站点、跨厂商的鲁棒临床预测，同时提供了可审计的协议感知表示。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce the Protocol Genome, a self-supervised learningsystem that learns correlations from DICOM headers and achieves AUROC 0.901 (vs0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.Our method also improves calibration and robustness across modalities (CT, MRI,CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, whereprocedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slicethickness) have consequences for contrast, noise, and artifact. These latentconfounders impede the generalization of image-only networks across sites. Weconsider structured DICOM headers as a label and learn protocol-aware butclinically robust image representations. Protocol Genome obtains tokenizedembeddings of de-identified header fields and models them along with imagefeatures using: (1) protocol-image contrastive learning, (2) masked protocolprediction, and (3) protocol-protocol translation. With 1.26M studies (7 healthsystems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CTtriage for PE, (B) brain MRI glioma grading, and (C) chest radiographcardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as wellas ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:cardiomegaly) is associated with higher external AUROC; 25-37% calibrationimprovements are obtained (p &lt; 0.01, DeLong tests). While the gains may betask-dependent, they are preserved with 10-20% of labeled data. From a clinicalpoint of view, the technique reduces false positives at protocol borders and isapplicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish amodel card and deployment guide, complete with both de-identification and biasaudits.</description>
      <author>example@mail.com (Jimmy Joseph)</author>
      <guid isPermaLink="false">2509.06995v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering</title>
      <link>http://arxiv.org/abs/2509.07766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures, International Quantum Engineering conference and  exhibition (QUEST-IS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图的联盟结构生成算法(GCS-Q)来处理金融资产的聚类问题，特别是在处理带符号的相关性结构时。这种方法避免了传统聚类方法中的有损转换和启发式假设，如固定聚类数量，并利用量子退火技术高效探索解决方案空间。实验证明，该方法在合成和真实金融数据上均优于最先进的经典算法，能够动态确定聚类数量，并提高聚类质量。&lt;h4&gt;背景&lt;/h4&gt;基于收益相关性的金融资产聚类是投资组合优化和统计套利中的基本任务。然而，传统的聚类方法在处理带符号的相关性结构时往往效果不佳，通常需要依赖有损转换和启发式假设，如固定聚类数量。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接处理带符号、加权图的聚类方法，无需依赖有损转换和固定聚类数量的假设，并利用量子计算技术提高聚类效率和质量。&lt;h4&gt;方法&lt;/h4&gt;应用基于图的联盟结构生成算法(GCS-Q)，该方法将每个分区步骤表述为QUBO问题，从而能够利用量子退火技术高效探索指数级大的解决方案空间。在合成和真实金融数据上验证该方法，并与最先进的经典算法(如SPONGE和k-Medoids)进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;GCS-Q方法在调整兰德指数和结构平衡惩罚等指标上持续获得更高的聚类质量，同时能够动态确定聚类数量。这些结果凸显了近期量子计算在金融应用的基于图的无监督学习中的实用价值。&lt;h4&gt;结论&lt;/h4&gt;GCS-Q算法为金融资产聚类提供了一种有效的新方法，特别适用于处理带符号的相关性结构。量子计算技术在解决金融领域的复杂聚类问题时具有实用价值，能够克服传统方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;基于收益相关性的金融资产聚类是投资组合优化和统计套利中的基本任务。然而，当处理带符号的相关性结构时，传统聚类方法往往表现不佳，通常需要依赖有损转换和启发式假设，如固定聚类数量。在本工作中，我们应用基于图的联盟结构生成算法(GCS-Q)来直接聚类带符号、加权图，而不依赖这些转换。GCS-Q将每个分区步骤表述为QUBO问题，使其能够利用量子退火技术高效探索指数级大的解决方案空间。我们在合成和真实金融数据上验证了我们的方法，并与最先进的经典算法(如SPONGE和k-Medoids)进行了基准测试。我们的实验证明，GCS-Q在调整兰德指数和结构平衡惩罚等指标上持续获得更高的聚类质量，同时动态确定聚类数量。这些结果凸显了近期量子计算在金融应用的基于图的无监督学习中的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering financial assets based on return correlations is a fundamentaltask in portfolio optimization and statistical arbitrage. However, classicalclustering methods often fall short when dealing with signed correlationstructures, typically requiring lossy transformations and heuristic assumptionssuch as a fixed number of clusters. In this work, we apply the Graph-basedCoalition Structure Generation algorithm (GCS-Q) to directly cluster signed,weighted graphs without relying on such transformations. GCS-Q formulates eachpartitioning step as a QUBO problem, enabling it to leverage quantum annealingfor efficient exploration of exponentially large solution spaces. We validateour approach on both synthetic and real-world financial data, benchmarkingagainst state-of-the-art classical algorithms such as SPONGE and k-Medoids. Ourexperiments demonstrate that GCS-Q consistently achieves higher clusteringquality, as measured by Adjusted Rand Index and structural balance penalties,while dynamically determining the number of clusters. These results highlightthe practical utility of near-term quantum computing for graph-basedunsupervised learning in financial applications.</description>
      <author>example@mail.com (Shivam Sharma, Supreeth Mysore Venkatesh, Pushkin Kachroo)</author>
      <guid isPermaLink="false">2509.07766v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
      <link>http://arxiv.org/abs/2509.07627v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LSMTCR是一个可扩展的多架构框架，能够从头开始、基于表位条件生成配对的全长T细胞受体(TCR)，在多个数据集上表现出优于基线的预测结合能力，并提供了温度可调的多样性和免疫遗传保真度。&lt;h4&gt;背景&lt;/h4&gt;设计全长、表位特异性的T细胞受体(TCR)具有挑战性，主要源于序列空间庞大、数据偏差和免疫遗传约束建模不完整等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展的多架构框架LSMTCR，将特异性学习与约束学习分离，实现从头开始、基于表位条件生成配对的全长TCRs。&lt;h4&gt;方法&lt;/h4&gt;LSMTCR采用三个关键组件：1) 扩散增强的BERT编码器学习时间条件的表位表示；2) 条件GPT解码器在跨模态条件下生成链特异性CDR3，具有温度控制的多样性；3) 基因感知的Transformer通过预测V/J使用来组装完整的α/β序列，确保免疫遗传保真度。&lt;h4&gt;主要发现&lt;/h4&gt;LSMTCR在GLIPH、TEP、MIRA、McPAS和策展数据集上，在大多数数据集上的预测结合能力优于基线；更忠实地恢复了位置和长度语法；提供了优越的、温度可调的多样性；迁移学习提高了α链生成的预测结合能力、长度真实性和多样性；从已知或从头开始的CDR3进行全长组装保留了k-mer谱，与参考序列的编辑距离低；在与表位的配对α/β共建模中，比单链设置获得更高的pTM/ipTM。&lt;h4&gt;结论&lt;/h4&gt;LSMTCR仅从表位输入就能输出多样化、基因上下文化的全长TCR设计，使高通量筛选和迭代优化成为可能。&lt;h4&gt;翻译&lt;/h4&gt;设计全长、表位特异性的T细胞受体(TCR)αβ由于序列空间庞大、数据偏差和免疫遗传约束建模不完整而具有挑战性。我们提出了LSMTCR，一个可扩展的多架构框架，它将特异性学习与约束学习分离，能够从头开始、基于表位条件生成配对的全长TCR。扩散增强的BERT编码器学习时间条件的表位表示；条件GPT解码器，在CDR3β上预训练并迁移到CDR3α，在跨模态条件下生成链特异性CDR3，具有温度控制的多样性；基因感知的Transformer通过预测V/J使用来组装完整的α/β序列，确保免疫遗传保真度。在GLIPH、TEP、MIRA、McPAS和我们策展的数据集上，LSMTCR在大多数数据集上的预测结合能力优于基线，更忠实地恢复了位置和长度语法，并提供了优越的、温度可调的多样性。对于α链生成，迁移学习提高了预测结合能力、长度真实性和多样性。从已知或从头开始的CDR3进行全长组装保留了k-mer谱，与参考序列的编辑距离低，并且在与表位的配对α/β共建模中，比单链设置获得更高的pTM/ipTM。LSMTCR仅从表位输入就能输出多样化、基因上下文化的全长TCR设计，使高通量筛选和迭代优化成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing full-length, epitope-specific TCR {\alpha}\b{eta} remainschallenging due to vast sequence space, data biases and incomplete modeling ofimmunogenetic constraints. We present LSMTCR, a scalable multi-architectureframework that separates specificity from constraint learning to enable denovo, epitope-conditioned generation of paired, full-length TCRs. Adiffusion-enhanced BERT encoder learns time-conditioned epitoperepresentations; conditional GPT decoders, pretrained on CDR3\b{eta} andtransferred to CDR3{\alpha}, generate chain-specific CDR3s under cross-modalconditioning with temperature-controlled diversity; and a gene-awareTransformer assembles complete {\alpha}/\b{eta} sequences by predicting V/Jusage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and ourcurated dataset, LSMTCR achieves higher predicted binding than baselines onmost datasets, more faithfully recovers positional and length grammars, anddelivers superior, temperature-tunable diversity. For {\alpha}-chaingeneration, transfer learning improves predicted binding, length realism anddiversity over representative methods. Full-length assembly from known or denovo CDR3s preserves k-mer spectra, yields low edit distances to references,and, in paired {\alpha}/\b{eta} co-modelling with epitope, attains higherpTM/ipTM than single-chain settings. LSMTCR outputs diverse,gene-contextualized, full-length TCR designs from epitope input alone, enablinghigh-throughput screening and iterative optimization.</description>
      <author>example@mail.com (Ruihao Zhang, Xiao Liu)</author>
      <guid isPermaLink="false">2509.07627v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data</title>
      <link>http://arxiv.org/abs/2509.07202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了将大型语言模型与脑电图解码相结合以增强辅助技术的潜力，特别是对有严重运动障碍人士的独立性和沟通能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的出现使文本生成能力发生了巨大变革，但基于脑电图的文本生成仍面临挑战，因为它需要大量数据和计算能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，减少EEG文本生成所需的数据和计算资源，同时保持接近最先进方法的性能。&lt;h4&gt;方法&lt;/h4&gt;结合使用Gemma 2B LLM和分类器-LLM架构，并引入循环神经网络(RNN)编码器。&lt;h4&gt;主要发现&lt;/h4&gt;新方法显著降低了所需的数据和计算能力，性能接近最先进方法，整体性能提高10%；所提出的架构展示了EEG文本生成有效迁移学习的可能性，即使在数据有限的情况下也能保持强大和功能性。&lt;h4&gt;结论&lt;/h4&gt;通过有效利用预训练语言模型的优势，该方法推动了当前能力的极限，为脑机接口的研究和应用开辟了新途径，使基于EEG的文本生成更加易于访问和高效。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型(LLMs)的引入，文本生成能力已经发生了巨大变革。然而，基于脑电图(EEG)的文本生成仍然困难，因为它需要大量数据和计算能力。本文介绍了一种新方法，它结合使用Gemma 2B LLM和分类器-LLM架构，并引入循环神经网络(RNN)编码器。我们的方法显著降低了所需的数据和计算能力，同时实现了接近最先进方法的性能。值得注意的是，与当前方法相比，我们的方法整体性能提高了10%。所提出的架构展示了EEG文本生成有效迁移学习的可能性，即使在数据有限的情况下也能保持强大和功能性。这项工作强调了将LLMs与EEG解码相结合以增强辅助技术的潜力，提高有严重运动限制人士的独立性和沟通能力。我们的方法通过有效利用预训练语言模型的优势，推动了当前能力的极限，为脑机接口的研究和应用开辟了新途径，使基于EEG的文本生成更加易于访问和高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text generating capabilities have undergone a substantial transformation withthe introduction of large language models (LLMs). Electroencephalography(EEG)-based text production is still difficult, though, because it requires alot of data and processing power. This paper introduces a new method thatcombines the use of the Gemma 2B LLM with a classifier-LLM architecture toincorporate a Recurrent Neural Network (RNN) encoder. Our approach drasticallylowers the amount of data and compute power needed while achieving performanceclose to that of cutting-edge methods. Notably, compared to currentmethodologies, our methodology delivers an overall performance improvement of10%. The suggested architecture demonstrates the possibility of effectivetransfer learning for EEG-based text production, remaining strong andfunctional even in the face of data limits. This work highlights the potentialof integrating LLMs with EEG decoding to improve assistive technologies andimprove independence and communication for those with severe motor limitations.Our method pushes the limits of present capabilities and opens new paths forresearch and application in brain-computer interfaces by efficiently using thestrengths of pre-trained language models. This makes EEG-based text productionmore accessible and efficient.</description>
      <author>example@mail.com (Khushiyant)</author>
      <guid isPermaLink="false">2509.07202v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Comparing unsupervised learning methods for local structural identification in colloidal systems</title>
      <link>http://arxiv.org/abs/2509.07186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 20 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了三种降维技术在自组装系统局部环境分类中的表现，发现UMAP在捕捉复杂结构特征方面表现最佳，为无监督结构分类提供了有效工具。&lt;h4&gt;背景&lt;/h4&gt;量化自组装系统中的局部结构是软物质和材料科学的核心挑战。当没有相关结构的先验知识可用时，传统的序参数往往不够用。&lt;h4&gt;目的&lt;/h4&gt;系统地比较三种流行的降维技术（主成分分析PCA、自编码器AE和统一流形近似和投影UMAP）用于分类自组装系统中的局部环境，探索无监督机器学习在自主发现结构基序方面的应用。&lt;h4&gt;方法&lt;/h4&gt;应用三种降维技术分别于硬球和带电球的流体和晶体构型，以及球形约束中自组装的二十面体球排列（包括模拟和实验数据）。&lt;h4&gt;主要发现&lt;/h4&gt;UMAP在捕捉复杂结构特征方面始终优于其他方法，为无监督结构分类提供了强大的工具。&lt;h4&gt;结论&lt;/h4&gt;无监督机器学习为从粒子构型中自主发现结构基序提供了便捷的途径，UMAP是一种鲁棒的工具，可用于自组装系统的结构分类，无需监督。&lt;h4&gt;翻译&lt;/h4&gt;量化自组装系统中的局部结构是软物质和材料科学的核心挑战。当没有相关结构的先验知识可用时，传统的序参数往往不够用。无监督机器学习为从粒子构型中自主发现结构基序提供了便捷的途径。在这项工作中，我们系统地比较了三种流行的降维技术：主成分分析（PCA）、自编码器（AE）和统一流形近似和投影（UMAP），用于分类自组装系统中的局部环境。我们首先将这些方法应用于硬球和带电球的流体和晶体构型。之后，我们将它们应用于球形约束中自组装的二十面体球排列，包括模拟和实验数据。我们证明UMAP在捕捉复杂结构特征方面始终优于其他方法，为无监督的结构分类提供了强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantifying local structures in self-assembled systems is a central challengein soft matter and materials science. When no a priori knowledge of therelevant structures is available, traditional order parameters often fallshort. Unsupervised machine learning provides a convenient route toautonomously uncover structural motifs directly from particle configurations.In this work, we systematically compare three popular dimensionality reductiontechniques; Principal Component Analysis (PCA), Autoencoders (AE), and UniformManifold Approximation and Projection (UMAP), for classifying localenvironments in self-assembled systems. We first apply these methods to fluidand crystal configurations of hard and charged spheres. Thereafter, we apply itto an icosahedral arrangement of spheres that self-assembled in sphericalconfinement, both from simulations as well as from experiments. We demonstratethat UMAP consistently outperforms the other methods in capturing complexstructural features, offering a robust tool for structural classificationwithout supervision.</description>
      <author>example@mail.com (Alptuğ Ulugöl, Jessi Bückmann, Ruizhi Yang, Roy Hoitink, Alfons van Blaaderen, Frank Smallenburg, Laura Filion)</author>
      <guid isPermaLink="false">2509.07186v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Fourier Neural Operators for Time-Periodic Quantum Systems: Learning Floquet Hamiltonians, Observable Dynamics, and Operator Growth</title>
      <link>http://arxiv.org/abs/2509.07084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了傅里叶神经算子(FNO)作为非平衡量子动力学的有效替代模型，通过三种学习范式展示了其多功能性，包括重建Floquet哈密顿量、预测可观测量和学习量子信息传播，并证明了其准确性和计算效率优势。&lt;h4&gt;背景&lt;/h4&gt;时间周期性量子系统表现出丰富多样的非平衡现象，是量子工程和控制的理想平台。然而，由于希尔伯特空间维度指数增长和纠缠快速传播，传统数值方法难以模拟其动力学。&lt;h4&gt;目的&lt;/h4&gt;引入傅里叶神经算子(FNO)作为非平衡量子动力学的有效、准确和可扩展的替代模型，解决传统数值方法面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用在傅里叶空间参数化的神经算子，通过三种互补学习范式：重建有效Floquet哈密顿量、预测局部可观测量期望值和学习量子信息传播，评估FNO的性能。&lt;h4&gt;主要发现&lt;/h4&gt;FNO在每个学习任务上都取得显著准确性，同时实现与精确数值方法相比的显著加速；具有在不同时间离散化和系统驱动频率间迁移学习的能力；可在训练数据时间窗口外进行外推；计算成本随系统大小呈多项式缩放。&lt;h4&gt;结论&lt;/h4&gt;FNO是预测非平衡量子动力学的多功能和可扩展替代模型，在处理近期量子计算机数据方面具有潜在应用。&lt;h4&gt;翻译&lt;/h4&gt;时间周期性量子系统表现出丰富多样的非平衡现象，并成为量子工程和控制的理想平台。然而，由于希尔伯特空间维度的指数增长和纠缠的快速传播，使用传统数值方法模拟它们的动力学仍然具有挑战性。在这项工作中，我们引入傅里叶神经算子(FNO)作为非平衡量子动力学的有效、准确和可扩展的替代模型。在傅里叶空间中参数化，FNO自然地捕捉时间相关性，并且对时间离散化的依赖性最小。我们通过三种互补的学习范式展示了FNO的多功能性：重建有效的Floquet哈密顿量、预测局部可观测量期望值和学习量子信息传播。对于每个学习任务，与精确数值方法相比，FNO都取得了显著的准确性，同时实现了显著的加速。此外，FNO还具有在不同时间离散化和系统驱动频率之间迁移学习的显著能力。我们还表明，FNO可以在训练数据提供的时间窗口之外进行外推，使原本难以获得的可观测量和算子传播动力学变得可访问。通过采用适当的局部基，我们认为FNO的计算成本仅随系统大小呈多项式缩放。我们的研究结果表明，FNO是预测非平衡量子动力学的多功能和可扩展的替代模型，在处理近期量子计算机数据方面具有潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-periodic quantum systems exhibit a rich variety of far-from-equilibriumphenomena and serve as ideal platforms for quantum engineering and control.However, simulating their dynamics with conventional numerical methods remainschallenging due to the exponential growth of Hilbert space dimension and rapidspreading of entanglement. In this work, we introduce Fourier Neural Operators(FNO) as an efficient, accurate, and scalable surrogate for non-equilibriumquantum dynamics. Parameterized in Fourier space, FNO naturally capturestemporal correlations and remains minimally dependent on discretization oftime. We demonstrate the versatility of FNO through three complementarylearning paradigms: reconstructing effective Floquet Hamiltonians, predictingexpectation values of local observables, and learning quantum informationspreading. For each learning task, FNO achieves remarkable accuracy, whileattaining a significant speedup, compared to exact numerical methods. Moreover,FNO possesses a remarkable capacity to transfer learning across differenttemporal discretizations and system driving frequencies. We also show that FNOcan extrapolate beyond the time window provided by training data, enablingaccess to observables and operator-spreading dynamics that might otherwise bedifficult to obtain. By employing an appropriate local basis, we argue that thecomputational cost of FNOs scales only polynomially with the system size. Ourresults establish FNO as a versatile and scalable surrogate for predictingnon-equilibrium quantum dynamics, with potential applications to processingdata from near-term quantum computers.</description>
      <author>example@mail.com (Zihao Qi, Yang Peng, Christopher Earls)</author>
      <guid isPermaLink="false">2509.07084v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</title>
      <link>http://arxiv.org/abs/2509.05657v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 Main&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LM-Searcher框架，利用大语言模型进行跨领域神经架构优化，无需领域特定调优。核心是NCode表示方法和基于剪枝的子空间采样策略，使LLMs能从候选池中选择高性能架构。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在解决复杂优化问题(包括神经网络架构搜索)方面取得进展，但现有方法严重依赖提示工程和领域特定调优，限制了实用性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;提出一个无需大量领域特定适应的框架，利用LLMs进行跨领域神经架构优化。&lt;h4&gt;方法&lt;/h4&gt;开发NCode作为神经架构的通用数字字符串表示；将NAS问题重新表述为排序任务；使用基于剪枝的子空间采样策略训练LLMs；构建包含各种架构-性能对的数据集促进稳健学习。&lt;h4&gt;主要发现&lt;/h4&gt;LM-Searcher在领域内(如图像分类的CNNs)和领域外(如分割和生成的LoRA配置)任务中都能取得有竞争力的性能，建立了灵活且可推广的基于LLM的架构搜索新范式。&lt;h4&gt;结论&lt;/h4&gt;LM-Searcher为跨领域神经架构优化提供了有效且通用的方法，数据集和模型将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;近期大语言模型(LLMs)的进展为解决复杂优化问题开辟了新途径，包括神经网络架构搜索(NAS)。然而，现有的LLM驱动的NAS方法严重依赖提示工程和领域特定调优，限制了它们在不同任务上的实用性和可扩展性。在这项工作中，我们提出了LM-Searcher，一个新颖的框架，利用LLMs进行跨领域神经架构优化，无需大量领域特定的适应。我们方法的核心是NCode，一种神经架构的通用数字字符串表示，它实现了跨领域架构编码和搜索。我们还重新将NAS问题表述为排序任务，训练LLMs使用基于剪枝的子空间采样策略衍生的指令调整样本从候选池中选择高性能架构。我们策划的数据集包含各种架构-性能对，促进了稳健和可迁移的学习。全面的实验表明，LM-Searcher在领域内(如图像分类的CNNs)和领域外(如分割和生成的LoRA配置)任务中都取得了有竞争力的性能，建立了灵活且可推广的基于LLM的架构搜索新范式。数据集和模型将在https://github.com/Ashone3/LM-Searcher发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in Large Language Models (LLMs) has opened new avenues forsolving complex optimization problems, including Neural Architecture Search(NAS). However, existing LLM-driven NAS approaches rely heavily on promptengineering and domain-specific tuning, limiting their practicality andscalability across diverse tasks. In this work, we propose LM-Searcher, a novelframework that leverages LLMs for cross-domain neural architecture optimizationwithout the need for extensive domain-specific adaptation. Central to ourapproach is NCode, a universal numerical string representation for neuralarchitectures, which enables cross-domain architecture encoding and search. Wealso reformulate the NAS problem as a ranking task, training LLMs to selecthigh-performing architectures from candidate pools using instruction-tuningsamples derived from a novel pruning-based subspace sampling strategy. Ourcurated dataset, encompassing a wide range of architecture-performance pairs,encourages robust and transferable learning. Comprehensive experimentsdemonstrate that LM-Searcher achieves competitive performance in both in-domain(e.g., CNNs for image classification) and out-of-domain (e.g., LoRAconfigurations for segmentation and generation) tasks, establishing a newparadigm for flexible and generalizable LLM-based architecture search. Thedatasets and models will be released at https://github.com/Ashone3/LM-Searcher.</description>
      <author>example@mail.com (Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li)</author>
      <guid isPermaLink="false">2509.05657v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>OmniMap: A General Mapping Framework Integrating Optics, Geometry, and Semantics</title>
      <link>http://arxiv.org/abs/2509.07500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Robotics (TRO), project website:  https://omni-map.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OmniMap是一个创新的在线映射框架，首次同时捕捉光学、几何和语义场景属性，保持实时性能和模型紧凑性，在多个方面优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;机器人系统需要准确和全面的3D环境感知，要求同时捕捉逼真的外观、精确的布局形状和开放词汇的场景理解。现有方法通常只能部分满足这些要求，并存在光学模糊、几何不规则和语义模糊等问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D环境感知方法中的光学模糊、几何不规则和语义模糊问题，开发一个能够同时捕捉光学、几何和语义场景属性的框架，同时保持实时性能和模型紧凑性。&lt;h4&gt;方法&lt;/h4&gt;OmniMap采用紧密耦合的3DGS-Voxel混合表示方法，结合细粒度建模和结构稳定性。引入了几项创新：自适应相机建模（用于运动模糊和曝光补偿）、具有法线约束的混合增量表示，以及用于鲁棒实例级理解的概率融合。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与最先进的方法相比，OmniMap在渲染保真度、几何准确性和零样本语义分割方面具有优越性能。该框架的通用性通过各种下游应用得到进一步证明。&lt;h4&gt;结论&lt;/h4&gt;OmniMap代表了第一个能够同时捕捉光学、几何和语义场景属性的在线映射框架，在多个方面优于现有方法，并具有广泛的下游应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;机器人系统需要准确和全面的3D环境感知，要求同时捕捉逼真的外观（光学）、精确的布局形状（几何）和开放词汇的场景理解（语义）。现有方法通常只能部分满足这些要求，并存在光学模糊、几何不规则和语义模糊等问题。为应对这些挑战，我们提出了OmniMap。总体而言，OmniMap代表了第一个在线映射框架，能够同时捕捉光学、几何和语义场景属性，同时保持实时性能和模型紧凑性。在架构层面，OmniMap采用紧密耦合的3DGS-Voxel混合表示方法，结合细粒度建模和结构稳定性。在实现层面，OmniMap确定了不同模态的关键挑战，并引入了几项创新：用于运动模糊和曝光补偿的自适应相机建模、具有法线约束的混合增量表示，以及用于鲁棒实例级理解的概率融合。大量实验表明，与最先进的方法相比，OmniMap在渲染保真度、几何准确性和零样本语义分割方面具有优越性能。该框架的通用性通过各种下游应用得到进一步证明，包括多领域场景问答、交互式编辑、感知引导操作和地图辅助导航。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决机器人系统中3D环境感知的全面性问题，即如何同时实现高保真的光学渲染、精确的几何重建和开放词汇的语义理解。这个问题很重要，因为随着具身人工智能的发展，机器人需要高质量的多维环境表示来支持各种任务，如交互式虚拟操作（场景问答和编辑）和多粒度物理交互（桌面级操作和房间级导航），这对提升机器人在现实世界中的操作能力至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性，提出了四个关键子问题：1)如何集成多样化场景属性；2)如何处理相机运动模糊和曝光不一致；3)如何提高几何稳定性；4)如何实现开放环境中的实例识别与融合。作者借鉴了多项现有工作：使用TSDF-Fusion作为体素映射基础，利用3DGS实现高保真渲染，采用YOLO-World进行开放词汇检测，使用TAP模型进行分割和标题生成，并通过SBERT进行嵌入编码。基于这些借鉴，作者创新性地设计了混合3DGS-体素表示框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过混合的3DGS-体素表示，将光学、几何和语义三种场景属性集成到一个统一的在线映射框架中，体素作为增量映射的基本单元封装概率语义，而新分配的体素则引导支持精确光学和几何重建的高斯原语初始化。整体流程包含三个主要模块：1)2D语言嵌入提取器，通过检测、分割、标题生成和编码获取实例级语义信息；2)概率体素重建器，执行3D域中的增量开放集实例融合；3)运动鲁棒的3DGS增量重建器，从新体素初始化高斯并在多种模态下进行渲染和监督。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全面的表示框架，首次连接光学、几何和语义；2)高保真光学处理，通过四个可微分参数建模相机模糊和曝光；3)详细几何重建，使用增量3DGS策略和法线监督；4)开放语义理解，设计高效实例理解管道；5)实现最先进的性能指标。相比之前工作，OmniMap突破了传统方法的局限：不同于传统语义体积映射的离散表示，不同于表面重建方法缺乏语义，不同于NeRF/3DGS方法缺乏语义理解，不同于开放词汇映射的稀疏表示，是首个将开放词汇理解直接集成到3DGS增量映射中的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OmniMap提出了首个集成光学、几何和语义的通用在线映射框架，通过创新的混合3DGS-体素表示和概率建模方法，实现了实时、高保真、高精度的3D环境感知，支持多样化的下游应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic systems demand accurate and comprehensive 3D environment perception,requiring simultaneous capture of photo-realistic appearance (optical), preciselayout shape (geometric), and open-vocabulary scene understanding (semantic).Existing methods typically achieve only partial fulfillment of theserequirements while exhibiting optical blurring, geometric irregularities, andsemantic ambiguities. To address these challenges, we propose OmniMap. Overall,OmniMap represents the first online mapping framework that simultaneouslycaptures optical, geometric, and semantic scene attributes while maintainingreal-time performance and model compactness. At the architectural level,OmniMap employs a tightly coupled 3DGS-Voxel hybrid representation thatcombines fine-grained modeling with structural stability. At the implementationlevel, OmniMap identifies key challenges across different modalities andintroduces several innovations: adaptive camera modeling for motion blur andexposure compensation, hybrid incremental representation with normalconstraints, and probabilistic fusion for robust instance-level understanding.Extensive experiments show OmniMap's superior performance in renderingfidelity, geometric accuracy, and zero-shot semantic segmentation compared tostate-of-the-art methods across diverse scenes. The framework's versatility isfurther evidenced through a variety of downstream applications, includingmulti-domain scene Q&amp;A, interactive editing, perception-guided manipulation,and map-assisted navigation.</description>
      <author>example@mail.com (Yinan Deng, Yufeng Yue, Jianyu Dou, Jingyu Zhao, Jiahui Wang, Yujie Tang, Yi Yang, Mengyin Fu)</author>
      <guid isPermaLink="false">2509.07500v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>CAViAR: Critic-Augmented Video Agentic Reasoning</title>
      <link>http://arxiv.org/abs/2509.07680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合大型语言模型代理和视频感知模块的方法来解决复杂视频推理任务，并引入评判机制来优化推理过程。&lt;h4&gt;背景&lt;/h4&gt;视频理解近年来取得显著进展，模型在短片段感知任务上性能不断提升，但在需要复杂推理的任务中，随着查询变得更复杂、视频变长，性能有所下降。多个基准测试如LVBench、Neptune和ActivityNet-RTL证实了这一现象。&lt;h4&gt;目的&lt;/h4&gt;探索是否可以利用现有的视频感知能力来成功执行更复杂的视频推理任务。&lt;h4&gt;方法&lt;/h4&gt;开发了一个大型语言模型代理，可以访问视频模块作为子代理或工具。与之前遵循固定程序的方法不同，该代理使用每次调用模块的结果来确定后续步骤。同时引入评判机制来区分成功和不成功的推理序列。&lt;h4&gt;主要发现&lt;/h4&gt;代理和评判机制的组合在LVBench、Neptune和ActivityNet-RTL等数据集上取得了强大的性能表现。&lt;h4&gt;结论&lt;/h4&gt;通过将大型语言模型与视频感知模块结合，并引入评判机制，可以有效解决复杂的视频推理任务，突破了现有方法在复杂查询和长视频上的性能瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;视频理解近年来取得了显著进展，模型在短片段感知任务上的性能持续提升。然而，多个最近的基准测试，如LVBench、Neptune和ActivityNet-RTL表明，对于需要复杂推理的任务，随着查询变得更加复杂和视频变得更长，性能有所下降。在这项工作中，我们问：现有的感知能力是否可以被利用来成功执行更复杂的视频推理？特别是，我们开发了一个大型语言模型代理，可以访问视频模块作为子代理或工具。与之前的工作如Visual Programming、ViperGPT和MoReVQA遵循固定程序来解决查询不同，该代理使用每次调用模块的结果来确定后续步骤。受到文本推理领域工作的启发，我们引入了一个评判机制来区分代理成功和不成功的序列。我们展示了代理和评判机制的组合在上述提到的数据集上取得了强大的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding has seen significant progress in recent years, withmodels' performance on perception from short clips continuing to rise. Yet,multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, showperformance wanes for tasks requiring complex reasoning on videos as queriesgrow more complex and videos grow longer. In this work, we ask: can existingperception capabilities be leveraged to successfully perform more complex videoreasoning? In particular, we develop a large language model agent given accessto video modules as subagents or tools. Rather than following a fixed procedureto solve queries as in previous work such as Visual Programming, ViperGPT, andMoReVQA, the agent uses the results of each call to a module to determinesubsequent steps. Inspired by work in the textual reasoning domain, weintroduce a critic to distinguish between instances of successful andunsuccessful sequences from the agent. We show that the combination of ouragent and critic achieve strong performance on the previously-mentioneddatasets.</description>
      <author>example@mail.com (Sachit Menon, Ahmet Iscen, Arsha Nagrani, Tobias Weyand, Carl Vondrick, Cordelia Schmid)</author>
      <guid isPermaLink="false">2509.07680v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors</title>
      <link>http://arxiv.org/abs/2509.00969v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 8 figures, EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LangDC的语言感知动态标记压缩器，解决了大型视频语言模型处理高容量视觉标记时的效率问题，通过动态调整压缩比来适应不同视频片段的语义密度。&lt;h4&gt;背景&lt;/h4&gt;大型视频语言模型的最新进展彻底改变了视频理解任务，但其效率受到处理大量视觉标记的限制。现有的标记压缩策略采用固定压缩比，忽略了不同视频片段间语义密度的变化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够根据视频片段语义密度动态调整压缩比的方法，以优化计算资源分配并提高模型效率。&lt;h4&gt;方法&lt;/h4&gt;LangDC利用轻量级语言模型描述视频片段，将其转换为软标题标记作为视觉表示，并通过语义密度感知监督进行训练，以覆盖关键视觉线索并基于场景丰富性动态调整压缩比。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与VideoGPT+相比，LangDC减少了49%的FLOPs同时保持竞争性性能，且能根据视频片段丰富性自适应调整标记压缩比。&lt;h4&gt;结论&lt;/h4&gt;LangDC通过模仿人类描述视觉内容的方式（复杂场景用更详细语言，简单场景用简洁语言），实现了高效的视频标记压缩，为视频语言模型提供了更高效的计算框架。&lt;h4&gt;翻译&lt;/h4&gt;大型视频语言模型的最新进展彻底改变了视频理解任务。然而，它们的效率受到处理大量视觉标记的显著限制。现有的标记压缩策略采用固定压缩比，忽略了不同视频片段间语义密度的变化。因此，这导致信息丰富的片段因标记不足而表示不充分，同时对静态或内容贫乏的片段进行不必要的计算。为解决此问题，我们提出了LangDC，一种语言感知的动态标记压缩器。LangDC利用轻量级语言模型描述视频片段，将其转换为软标题标记作为视觉表示。通过我们提出的语义密度感知监督进行训练，LangDC旨在1)覆盖下游任务推理所需的关键视觉线索；2)基于场景丰富性（由描述长度反映）动态调整压缩比。我们的设计模仿了人类如何动态表达他们所看到的内容：复杂场景（看到更多）引发更详细的语言来传达细微差别（说更多），而简单的场景用更少的词描述。实验结果表明，与VideoGPT+相比，我们的方法减少了49%的FLOPs，同时保持竞争性性能。此外，定性结果表明我们的方法根据视频片段丰富性自适应地调整标记压缩比。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large video-language models have revolutionized videounderstanding tasks. However, their efficiency is significantly constrained byprocessing high volumes of visual tokens. Existing token compression strategiesapply a fixed compression ratio, ignoring the variability in semantic densityamong different video clips. Consequently, this lead to inadequaterepresentation of information-rich clips due to insufficient tokens andunnecessary computation on static or content-poor ones. To address this, wepropose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages alightweight language model to describe video clips, converting them into softcaption tokens as visual representations. Trained with our proposed semanticdensity-aware supervision, LangDC aims to 1) cover key visual cues necessaryfor downstream task reasoning and 2) dynamically adjust compression ratiosbased on scene richness, reflected by descriptions length. Our design mimicshow humans dynamically express what they see: complex scenes (seeing more)elicit more detailed language to convey nuances (saying more), whereas simplerscenes are described with fewer words. Experimental results show that ourmethod reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitiveperformance. Furthermore, qualitative results demonstrate our approachadaptively adjusts the token compression ratio based on video segment richness.</description>
      <author>example@mail.com (Xiangchen Wang, Jinrui Zhang, Teng Wang, Haigang Zhang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.00969v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Interleaving Reasoning for Better Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2509.06945v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了交错推理生成(IRG)框架，通过在文本思考和图像合成之间交替进行，提高文本到图像生成质量。作者还提出了交错推理生成学习(IRGL)方法和IRGL-300K数据集，通过两阶段训练实现了在多个评估指标上的显著提升。&lt;h4&gt;背景&lt;/h4&gt;统一多模态理解和生成模型在图像生成能力上取得了显著进步，但在遵循指令和保持细节方面与GPT-4o等系统相比仍有较大差距。&lt;h4&gt;目的&lt;/h4&gt;探索交错推理是否能进一步提高文本到图像(T2I)的生成质量，缩小与紧密耦合理解与生成的系统之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出交错推理生成(IRG)框架，模型先产生文本思考指导初始图像，再反思结果以细化细节和质量；提出交错推理生成学习(IRGL)方法和IRGL-300K数据集；采用两阶段训练：首先建立思考和反思能力，然后调整IRG管道。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验显示最先进的性能，在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN等指标上取得了5-10个百分点的绝对提升，同时在视觉质量和细粒度保真度方面也取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;交错推理框架能有效提高文本到图像生成质量，特别是在细节保持和视觉质量方面。代码、模型权重和数据集将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;最近的统一多模态理解和生成模型在图像生成能力方面取得了显著进步，但在遵循指令和保持细节方面与紧密耦合理解与生成的系统(如GPT-4o)相比仍有较大差距。受最近交错推理进展的启发，我们探索了这种推理是否能进一步提高文本到图像(T2I)生成。我们引入了交错推理生成(IRG)，一个在基于文本的思考和图像合成之间交替的框架：模型首先产生基于文本的思考来指导初始图像，然后反思结果以细化细粒度细节、视觉质量和美学，同时保持语义。为了有效训练IRG，我们提出了交错推理生成学习(IRGL)，它针对两个子目标：(1)加强初始思考和生成阶段以建立核心内容和基础质量，以及(2)实现高质量的文本反思并在后续图像中忠实地实施这些改进。我们整理了IRGL-300K数据集，组织成六个分解的学习模式，共同覆盖基于文本的思考和完整的思考-图像轨迹学习。从一个原生发出交错文本-图像输出的统一基础模型开始，我们的两阶段训练首先建立强大的思考和反思能力，然后在完整的思考-图像轨迹数据中高效地调整IRG管道。大量实验显示了最先进的性能，在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN上取得了5-10个百分点的绝对提升，同时在视觉质量和细粒度保真度方面也取得了显著改进。代码、模型权重和数据集将在以下地址发布：https://github.com/Osilly/Interleaving-Reasoning-Generation 。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified multimodal understanding and generation models recently have achievesignificant improvement in image generation capability, yet a large gap remainsin instruction following and detail preservation compared to systems thattightly couple comprehension with generation such as GPT-4o. Motivated byrecent advances in interleaving reasoning, we explore whether such reasoningcan further improve Text-to-Image (T2I) generation. We introduce InterleavingReasoning Generation (IRG), a framework that alternates between text-basedthinking and image synthesis: the model first produces a text-based thinking toguide an initial image, then reflects on the result to refine fine-graineddetails, visual quality, and aesthetics while preserving semantics. To trainIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),which targets two sub-goals: (1) strengthening the initial think-and-generatestage to establish core content and base quality, and (2) enabling high-qualitytextual reflection and faithful implementation of those refinements in asubsequent image. We curate IRGL-300K, a dataset organized into six decomposedlearning modes that jointly cover learning text-based thinking, and fullthinking-image trajectories. Starting from a unified foundation model thatnatively emits interleaved text-image outputs, our two-stage training firstbuilds robust thinking and reflection, then efficiently tunes the IRG pipelinein the full thinking-image trajectory data. Extensive experiments show SoTAperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual qualityand fine-grained fidelity. The code, model weights and datasets will bereleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .</description>
      <author>example@mail.com (Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin)</author>
      <guid isPermaLink="false">2509.06945v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
  <item>
      <title>FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</title>
      <link>http://arxiv.org/abs/2509.06907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FoMo4Wheat是一种针对小麦的视觉基础模型，使用大规模、多样化的小麦图像数据集进行自监督预训练，在各种农业视觉任务中表现优异，并具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视觉驱动的田间监测是数字农业的核心，但基于通用域预训练骨干的模型难以在不同任务中泛化，这是由于精细变化的冠层结构与田间波动条件的相互作用导致的。&lt;h4&gt;目的&lt;/h4&gt;开发一种作物特定的视觉基础模型，解决通用域预训练模型在农业视觉任务中的泛化问题，提高田间监测的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出FoMo4Wheat模型，在ImAg4Wheat数据集上进行自监督预训练。该数据集包含250万张高分辨率图像，收集自30个全球站点，跨越十年，涵盖超过2000个基因型和500种环境条件。&lt;h4&gt;主要发现&lt;/h4&gt;小麦特定的预训练产生了稳健且可迁移到其他作物和杂草的表示。在十个田间视觉任务（冠层和器官层面）上，FoMo4Wheat模型持续优于最先进的通用域预训练模型。&lt;h4&gt;结论&lt;/h4&gt;作物特定的基础模型对可靠的田间感知具有重要价值，为开发具有跨物种和跨任务能力的通用作物基础模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;视觉驱动的田间监测是数字农业的核心，然而基于通用域预训练骨干构建的模型往往无法跨任务泛化，这是由于精细变化的冠层结构与田间波动条件的相互作用所致。我们提出了FoMo4Wheat，这是首批针对作物领域的视觉基础模型之一，使用自监督方法在ImAg4Wheat上进行预训练，这是迄今为止最大且最多样化的小麦图像数据集（包含250万张高分辨率图像，在十年间从30个全球站点收集，涵盖&gt;2000个基因型和&gt;500种环境条件）。这种小麦特定的预训练产生了对小麦稳健且可迁移到其他作物和杂草的表示。在冠层和器官层面的十个田间视觉任务中，FoMo4Wheat模型持续优于在通用域数据集上预训练的最先进模型。这些结果证明了作物特定基础模型对可靠田间感知的价值，并为具有跨物种和跨任务能力的通用作物基础模型铺平了道路。FoMo4Wheat模型和ImAg4Wheat数据集已在线公开：https://github.com/PheniX-Lab/FoMo4Wheat和https://huggingface.co/PheniX-Lab/FoMo4Wheat。演示网站为：https://fomo4wheat.phenix-lab.com/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-driven field monitoring is central to digital agriculture, yet modelsbuilt on general-domain pretrained backbones often fail to generalize acrosstasks, owing to the interaction of fine, variable canopy structures withfluctuating field conditions. We present FoMo4Wheat, one of the firstcrop-domain vision foundation model pretrained with self-supervision onImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5million high-resolution images collected over a decade at 30 global sites,spanning &gt;2,000 genotypes and &gt;500 environmental conditions). Thiswheat-specific pretraining yields representations that are robust for wheat andtransferable to other crops and weeds. Across ten in-field vision tasks atcanopy and organ levels, FoMo4Wheat models consistently outperformstate-of-the-art models pretrained on general-domain dataset. These resultsdemonstrate the value of crop-specific foundation models for reliable in-fieldperception and chart a path toward a universal crop foundation model withcross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheatdataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheatand https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:https://fomo4wheat.phenix-lab.com/.</description>
      <author>example@mail.com (Bing Han, Chen Zhu, Dong Han, Rui Yu, Songliang Cao, Jianhui Wu, Scott Chapman, Zijian Wang, Bangyou Zheng, Wei Guo, Marie Weiss, Benoit de Solan, Andreas Hund, Lukas Roth, Kirchgessner Norbert, Andrea Visioni, Yufeng Ge, Wenjuan Li, Alexis Comar, Dong Jiang, Dejun Han, Fred Baret, Yanfeng Ding, Hao Lu, Shouyang Liu)</author>
      <guid isPermaLink="false">2509.06907v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis</title>
      <link>http://arxiv.org/abs/2509.06831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures; accepted at ML-CDS @ MICCAI 2025, Daejeon,  Republic of Korea&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何通过迁移学习适应通用基础模型以及整合手术室的互补模态来支持外科数据科学。使用V-JEPA作为多模态模型的基础，研究了在未标记手术视频数据上微调以及整合手术室其他时间解析数据流对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;外科数据科学需要有效利用手术数据，而通用基础模型和手术室的多种数据模态提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;研究单模态基础模型(V-JEPA)如何作为多模态模型的基础用于微创手术支持，以及迁移学习和多模态整合如何提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;使用V-JEPA作为基础模型，在内部肝脏手术视频数据集和公共HeiCo数据集上测试预测住院时间、术后并发症和手术阶段识别任务。通过在未标记视频上微调模型以及整合手术室的其他时间解析数据流来评估性能变化。&lt;h4&gt;主要发现&lt;/h4&gt;在领域特定数据上微调提高了模型性能；整合其他时间解析数据也有益于模型；预训练的视频单模态基线与最佳提交相当，领域特定微调进一步提高了准确性。&lt;h4&gt;结论&lt;/h4&gt;外科数据科学可以有效地利用公共通用基础模型；领域适应和整合手术室合适的互补数据流具有很大潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们研究如何通过迁移学习适应通用基础模型以及整合手术室(OR)的互补模态来支持外科数据科学。为此，我们使用V-JEPA作为多模态模型的基础，用于微创手术支持。我们分析了模型的下游性能如何受益于(a)在未标记的手术视频数据上进行微调，以及(b)在多模态设置中提供来自手术室的其他时间解析数据流。在内部肝脏手术视频数据集中，我们分析预测住院时间和术后并发症的任务。在公共HeiCo数据集的视频中，我们分析手术阶段识别任务。作为基线，我们将预训练的V-JEPA应用于所有任务。然后我们在未标记的保留视频上进行微调，以研究领域适应后的性能变化。遵循模块化决策支持网络的理念，我们通过训练单独的编码器整合来自手术室的其他数据流，与V-JEPA的嵌入形成共享表示空间。我们的实验表明，在领域特定数据上微调可以提高模型性能。在内部数据中，整合其他时间解析数据同样有利于模型。在HeiCo数据上，仅使用预训练视频的单模态基线设置与EndoVis2017挑战的最佳提交相当，而在领域特定数据上微调可以进一步提高准确性。我们的结果因此证明了外科数据科学如何可以利用公共通用基础模型。同样，它们表明了领域适应和整合来自手术室合适的互补数据流的潜力。为支持进一步研究，我们在https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025发布了我们的代码和模型权重。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate how both the adaptation of a generic foundation model viatransfer learning and the integration of complementary modalities from theoperating room (OR) can support surgical data science. To this end, we useV-JEPA as the single-modality foundation of a multimodal model for minimallyinvasive surgery support. We analyze how the model's downstream performance canbenefit (a) from finetuning on unlabeled surgical video data and (b) fromproviding additional time-resolved data streams from the OR in a multimodalsetup.  In an in-house dataset of liver surgery videos, we analyze the tasks ofpredicting hospital length of stay and postoperative complications. In videosof the public HeiCo dataset, we analyze the task of surgical phase recognition.As a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it onunlabeled, held-out videos to investigate its change in performance afterdomain adaptation. Following the idea of modular decision support networks, weintegrate additional data streams from the OR by training a separate encoder toform a shared representation space with V-JEPA's embeddings.  Our experiments show that finetuning on domain-specific data increases modelperformance. On the in-house data, integrating additional time-resolved datalikewise benefits the model. On the HeiCo data, accuracy of the pretrainedvideo-only, single-modality baseline setup is on par with the top-performingsubmissions of the EndoVis2017 challenge, while finetuning on domain-specificdata increases accuracy further. Our results thus demonstrate how surgical datascience can leverage public, generic foundation models. Likewise, they indicatethe potential of domain adaptation and of integrating suitable complementarydata streams from the OR. To support further research, we release our code andmodel weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.</description>
      <author>example@mail.com (Simon Pezold, Jérôme A. Kurylec, Jan S. Liechti, Beat P. Müller, Joël L. Lavanchy)</author>
      <guid isPermaLink="false">2509.06831v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Curia: A Multi-Modal Foundation Model for Radiology</title>
      <link>http://arxiv.org/abs/2509.06830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了Curia，一个在大型医院多年横断面影像数据上训练的基础模型，用于AI辅助放射学解释，该模型在多模态和低数据设置下展现出临床意义的重要涌现特性。&lt;h4&gt;背景&lt;/h4&gt;当前的AI辅助放射学解释主要基于狭窄的、单任务模型，这种方法难以覆盖广泛的成像方式、疾病和放射学发现。基础模型(FMs)有望在多种模态和低数据设置下实现广泛泛化，但这一潜力在放射学领域尚未实现。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在多种成像模态和低数据设置下有效工作的基础模型，以克服当前单任务AI模型的局限性，并实现更广泛的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;研究团队训练了一个名为Curia的基础模型，该模型在一个主要医院多年的横断面影像输出上进行训练，包含150,000次检查(130 TB)。他们在一个新创建的19项任务外部验证基准上评估了该模型。&lt;h4&gt;主要发现&lt;/h4&gt;Curia能够准确识别器官，检测脑出血和心肌梗死等疾病，在肿瘤分期中预测结果，其表现达到或超过了放射科医生和最近的基础模型，并且在跨模态和低数据设置下展现出临床意义的重要涌现特性。&lt;h4&gt;结论&lt;/h4&gt;Curia代表了放射学基础模型的重要进展，通过在大型真实世界数据集上训练，实现了在多种任务和条件下的高性能。研究团队发布了基础模型的权重，以加速该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;AI-assisted radiological interpretation：AI辅助放射学解释；Foundation models：基础模型；Curia：Curia（模型名称）；cross-sectional imaging：横断面影像；brain hemorrhages：脑出血；myocardial infarctions：心肌梗死；tumor staging：肿瘤分期&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决放射学AI领域过度依赖狭窄单任务模型的问题。现实中，这种'一个任务一个模型'的方法资源密集且难以覆盖广泛的成像方式、疾病和放射学发现，成为AI模型整合到临床工作流程的瓶颈。基础模型(FMs)有潜力实现跨模态和低数据设置下的广泛泛化，但这一潜力在放射学领域尚未实现。放射学作为医学诊断的核心，AI辅助有潜力提高诊断效率和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前放射学AI的局限性，认识到基础模型在自然图像领域的成功(如DINOv2和MAE)可迁移到医学影像。他们借鉴了Vision Transformer架构和DINOv2自监督学习算法，同时参考了现有放射学基础模型如BiomedCLIP和MedImageInsight。设计上，他们专注于使用大规模真实世界临床数据而非混合专业数据，并创建了全面的评估基准。在下游任务适配上，他们借鉴了SAM和RadSAM的提示分割框架，但进行了医学领域特定的改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模未标记医学影像数据进行自监督学习，学习通用的视觉特征表示，创建能适应多种下游任务的基础模型，实现跨模态泛化。流程包括：1)收集并预处理一家医院15万次检查的130TB真实世界数据；2)使用Vision Transformer架构和DINOv2算法在2亿图像上预训练；3)为不同下游任务设计适配方法(图像级、对象级、体积级预测等)；4)在包含19个任务的CuriaBench基准上全面评估，并与放射科医师和其他模型比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用最大规模真实世界医学影像语料库(2亿图像，130TB)；2)实现卓越的跨模态泛化能力(CT到MRI性能下降仅9.17%)；3)在少样本学习中表现优异；4)创建全面的CuriaBench基准(19个任务)；5)性能达到或超过放射科医师水平；6)开源模型权重。相比之前工作，Curia使用更大规模且来源单一的放射学数据(而非多领域混合数据)，采用DINOv2自监督方法，在跨模态任务和少样本学习中表现显著更优，评估更全面且包含与放射科医师的比较。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Curia通过在超大规模真实世界医学影像数据上的自监督学习，实现了跨模态和低数据设置下的强大泛化能力，在广泛放射学任务中达到或超过放射科医师性能，为放射学AI提供了新的基础模型范式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-assisted radiological interpretation is based on predominantly narrow,single-task models. This approach is impractical for covering the vast spectrumof imaging modalities, diseases, and radiological findings. Foundation models(FMs) hold the promise of broad generalization across modalities and inlow-data settings. However, this potential has remained largely unrealized inradiology. We introduce Curia, a foundation model trained on the entirecross-sectional imaging output of a major hospital over several years, which toour knowledge is the largest such corpus of real-world data-encompassing150,000 exams (130 TB). On a newly curated 19-task external validationbenchmark, Curia accurately identifies organs, detects conditions like brainhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.Curia meets or surpasses the performance of radiologists and recent foundationmodels, and exhibits clinically significant emergent properties incross-modality, and low-data regimes. To accelerate progress, we release ourbase model's weights at https://huggingface.co/raidium/curia.</description>
      <author>example@mail.com (Corentin Dancette, Julien Khlaut, Antoine Saporta, Helene Philippe, Elodie Ferreres, Baptiste Callard, Théo Danielou, Léo Alberge, Léo Machado, Daniel Tordjman, Julie Dupuis, Korentin Le Floch, Jean Du Terrail, Mariam Moshiri, Laurent Dercle, Tom Boeken, Jules Gregory, Maxime Ronot, François Legou, Pascal Roux, Marc Sapoval, Pierre Manceron, Paul Hérent)</author>
      <guid isPermaLink="false">2509.06830v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes</title>
      <link>http://arxiv.org/abs/2509.06685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架，通过结合视觉惯性运动恢复结构的精确稀疏深度和大型基础模型的密集粗糙深度，实现了高质量的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;传统高斯溅射技术需要精确的深度信息来初始化高斯椭球体，通常依赖RGB-D/立体相机，但这些传感器的有限范围使其难以在大场景中应用。单目图像虽然易于获取，但缺乏深度信息指导学习，导致新视角合成结果质量较差。现有的单目深度估计大型基础模型存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索模糊不清等问题。&lt;h4&gt;目的&lt;/h4&gt;从单目RGB输入生成密集、精确的深度图像，用于高确定性的高斯溅射渲染，以解决大场景中的新视角合成问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种结合视觉惯性运动恢复结构(SfM)和大型基础模型(LFMs)的深度生成方法。具体包括：1)利用SfM提供的精确但稀疏的深度信息来 refine LFM提供的密集但粗糙的深度信息；2)提出对象分割的深度传播算法，连接稀疏输入和密集输出；3)开发动态深度细化模块，处理动态对象的受损SfM深度，并细化粗糙的LFM深度。&lt;h4&gt;主要发现&lt;/h4&gt;通过公共和定制数据集的实验证明，VIM-GS在大场景的新视角合成中具有优越的渲染质量，能够有效解决传统方法在大场景中面临的挑战。&lt;h4&gt;结论&lt;/h4&gt;VIM-GS框架成功结合了视觉惯性运动恢复结构和大型基础模型的优势，通过创新的深度传播和细化算法，实现了大场景中高质量的新视角合成，为单目图像驱动的三维场景重建提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架。高斯溅射通常需要精确的深度信息来使用RGB-D/立体相机初始化高斯椭球体。它们的有限深度传感范围使得高斯溅射难以在大场景中工作。然而，单目图像缺乏引导学习的深度信息，导致新视角合成结果质量较差。尽管有可用于单目深度估计的大型基础模型，但它们存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索模糊不清等问题。本文旨在从单目RGB输入生成密集、精确的深度图像，用于高确定性的高斯溅射渲染。关键思想是利用视觉惯性运动恢复结构提供的精确但稀疏的深度信息来 refine 大型基础模型提供的密集但粗糙的深度信息。为了连接稀疏输入和密集输出，我们提出了一种对象分割的深度传播算法，用于渲染结构化对象像素的深度。然后我们开发了一个动态深度细化模块，用于处理动态对象的受损SfM深度，并细化粗糙的LFM深度。使用公共和定制数据集进行的实验证明了VIM-GS在大场景中的优越渲染质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在大场景下使用单目图像进行高质量新视角合成的问题。传统高斯溅射方法需要RGB-D或立体相机提供深度信息，但这些设备深度感知范围有限，难以应用于大场景。单目图像虽然成本低，但缺乏深度信息导致渲染质量差。这个问题很重要，因为高成本激光雷达限制了GS技术在消费级应用中的普及，而低成本单目相机实现大场景高质量渲染对自动驾驶、虚拟现实等领域有重要价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：RGB-D/立体相机深度范围有限，单目SfM只提供稀疏深度，而大规模基础模型深度估计存在跨帧不一致和远处不准确等问题。设计上借鉴了视觉惯性SLAM的准确稀疏深度和大规模基础模型的密集粗糙深度，结合分割模型进行物体识别。整体思路是优势互补，将不同方法的优点结合，弥补各自的不足，通过物体分割深度传播和动态深度细化两个模块实现深度信息的优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉惯性SfM提供的准确但稀疏的深度信息来 refine大规模基础模型提供的密集但粗糙的深度信息，生成密集准确的深度图像。整体流程：1)输入RGB和IMU数据，通过VI前端获得稀疏特征3D坐标，并用分割模型生成物体掩码；2)物体分割深度传播模块识别结构化物体，将稀疏深度传播到物体所有像素；3)动态深度细化模块处理动态物体错误深度，通过新损失函数细化LFMs深度；4)输出密集准确深度图像，为高斯溅射提供良好初始化，实现大场景高质量渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)物体分割深度传播算法，识别结构化物体并将稀疏深度传播到所有像素；2)动态深度细化模块，处理动态物体错误深度并细化LFMs深度；3)结合视觉惯性SfM和大规模基础模型优势。相比之前工作：传统GS依赖RGB-D/立体相机，深度范围有限；单目支持方法因随机或稀疏深度初始化，大场景效果差；SfM只提供稀疏深度；LFMs存在跨帧不一致和远处不准确问题。VIM-GS首次有效结合两种深度信息，通过物体分割和动态处理解决了各自局限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VIM-GS通过结合视觉惯性SfM的准确稀疏深度和大规模基础模型的密集粗糙深度，并引入物体分割和动态处理机制，实现了在大场景下使用单目相机进行高质量实时新视角合成的突破。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; VIM-GS is a Gaussian Splatting (GS) framework using monocular images fornovel-view synthesis (NVS) in large scenes. GS typically requires accuratedepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limiteddepth sensing range makes it difficult for GS to work in large scenes.Monocular images, however, lack depth to guide the learning and lead toinferior NVS results. Although large foundation models (LFMs) for monoculardepth estimation are available, they suffer from cross-frame inconsistency,inaccuracy for distant scenes, and ambiguity in deceptive texture cues. Thispaper aims to generate dense, accurate depth images from monocular RGB inputsfor high-definite GS rendering. The key idea is to leverage the accurate butsparse depth from visual-inertial Structure-from-Motion (SfM) to refine thedense but coarse depth from LFMs. To bridge the sparse input and dense output,we propose an object-segmented depth propagation algorithm that renders thedepth of pixels of structured objects. Then we develop a dynamic depthrefinement module to handle the crippled SfM depth of dynamic objects andrefine the coarse LFM depth. Experiments using public and customized datasetsdemonstrate the superior rendering quality of VIM-GS in large scenes.</description>
      <author>example@mail.com (Shengkai Zhang, Yuhe Liu, Guanjun Wu, Jianhua He, Xinggang Wang, Mozi Chen, Kezhong Liu)</author>
      <guid isPermaLink="false">2509.06685v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image Analysis</title>
      <link>http://arxiv.org/abs/2509.06617v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种名为MM-DINOv2的新框架，将预训练的视觉基础模型DINOv2适应用于多模态医学影像分析。该方法通过多模态块嵌入和全模态掩码技术解决了多模态医学影像分析中的挑战，并利用半监督学习提高预测准确性和可靠性。在胶质瘤亚型分类任务中，该方法取得了优于现有监督方法的性能。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型如DINOv2在医学影像领域展现出巨大潜力，尽管它们最初是为自然图像设计的。然而，这些模型的设计主要用于单模态图像分析，限制了它们在神经学、肿瘤学等医学领域中常见的多模态成像任务中的有效性。虽然监督模型在这种情况下表现良好，但它们无法利用未标记的数据集，并且在处理缺失模态方面存在困难，这是临床环境中常见的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理多模态医学影像数据的框架，解决现有监督模型无法利用未标记数据和处理缺失模态的问题，从而提高医学预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;研究团队引入了MM-DINOv2框架，将预训练的视觉基础模型DINOv2适应用于多模态医学影像。该方法包括：1) 使用多模态块嵌入，使视觉基础模型能够有效处理多模态影像数据；2) 采用全模态掩码技术，鼓励模型学习稳健的跨模态关系，以解决缺失模态的问题；3) 利用半监督学习来利用大型未标记数据集，提高医学预测的准确性和可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;在胶质瘤亚型分类任务中（基于多序列脑部MRI），该方法在外部测试集上获得了0.6的马修斯相关系数(MCC)，比最先进的监督方法高出11.1%。&lt;h4&gt;结论&lt;/h4&gt;该研究建立了一种可扩展且稳健的多模态医学影像任务解决方案，它利用了在自然图像上预训练的强大视觉基础模型，同时解决了缺失数据和有限标注等现实临床挑战。&lt;h4&gt;翻译&lt;/h4&gt;DINOv2等视觉基础模型尽管源自自然图像领域，但在医学影像中展现出巨大潜力。然而，其设计本质上是针对单模态图像分析优化的，限制了它们在神经学和肿瘤学等许多医学领域常见的多模态成像任务中的有效性。虽然监督模型在这种情况下表现良好，但它们无法利用未标记的数据集，并且在处理缺失模态方面存在困难，这是临床环境中常见的挑战。为了弥补这些差距，我们引入了MM-DINOv2，这是一种新颖且高效的框架，将预训练的视觉基础模型DINOv2适应用于多模态医学影像。我们的方法结合了多模态块嵌入，使视觉基础模型能够有效处理多模态影像数据。为解决缺失模态问题，我们采用全模态掩码，这鼓励模型学习稳健的跨模态关系。此外，我们利用半监督学习来利用大型未标记数据集，提高医学预测的准确性和可靠性。在应用于基于多序列脑部MRI的胶质瘤亚型分类时，我们的方法在外部测试集上获得了0.6的马修斯相关系数(MCC)，比最先进的监督方法高出11.1%。我们的工作为多模态医学影像任务建立了可扩展且稳健的解决方案，利用了在自然图像上预训练的强大视觉基础模型，同时解决了缺失数据和有限标注等现实临床挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models like DINOv2 demonstrate remarkable potential inmedical imaging despite their origin in natural image domains. However, theirdesign inherently works best for uni-modal image analysis, limiting theireffectiveness for multi-modal imaging tasks that are common in many medicalfields, such as neurology and oncology. While supervised models perform well inthis setting, they fail to leverage unlabeled datasets and struggle withmissing modalities, a frequent challenge in clinical settings. To bridge thesegaps, we introduce MM-DINOv2, a novel and efficient framework that adapts thepre-trained vision foundation model DINOv2 for multi-modal medical imaging. Ourapproach incorporates multi-modal patch embeddings, enabling vision foundationmodels to effectively process multi-modal imaging data. To address missingmodalities, we employ full-modality masking, which encourages the model tolearn robust cross-modality relationships. Furthermore, we leveragesemi-supervised learning to harness large unlabeled datasets, enhancing boththe accuracy and reliability of medical predictions. Applied to glioma subtypeclassification from multi-sequence brain MRI, our method achieves a MatthewsCorrelation Coefficient (MCC) of 0.6 on an external test set, surpassingstate-of-the-art supervised approaches by +11.1%. Our work establishes ascalable and robust solution for multi-modal medical imaging tasks, leveragingpowerful vision foundation models pre-trained on natural images whileaddressing real-world clinical challenges such as missing data and limitedannotations.</description>
      <author>example@mail.com (Daniel Scholz, Ayhan Can Erdur, Viktoria Ehm, Anke Meyer-Baese, Jan C. Peeken, Daniel Rueckert, Benedikt Wiestler)</author>
      <guid isPermaLink="false">2509.06617v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking EfficientTAM on FMO datasets</title>
      <link>http://arxiv.org/abs/2509.06536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文解决了快速和小型物体跟踪的挑战，通过引入JSON元数据文件和扩展数据集描述，并展示了EfficientTAM模型在这些数据集上的良好性能。&lt;h4&gt;背景&lt;/h4&gt;快速和小型物体跟踪在计算机视觉领域仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;作者旨在引入与四个开源的快速移动物体(FMOs)图像序列相关的JSON元数据文件，并扩展这些数据集的描述。&lt;h4&gt;方法&lt;/h4&gt;引入一个JSON元数据文件与四个开源的快速移动物体图像序列相关联；扩展FMOs数据集的描述，添加以JSON格式(称为FMOX)的额外真实信息，包括物体大小信息；使用FMOX文件测试最近提出的基础跟踪模型EfficientTAM。&lt;h4&gt;主要发现&lt;/h4&gt;EfficientTAM模型在FMOX数据集上的性能与专门为这些FMO数据集设计的管道相比表现良好，使用轨迹交并比(TIoU)分数进行评估。&lt;h4&gt;结论&lt;/h4&gt;代码和JSON文件已开源共享，使FMOX能够被其他处理FMO数据集的机器学习管道访问和使用，促进了该领域的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;快速且小型物体的跟踪在计算机视觉中仍然是一个挑战。在本文中，我们首先介绍了与四个开源的快速移动物体(FMOs)图像序列相关的JSON元数据文件。此外，我们以JSON格式(称为FMOX)扩展了FMOs数据集的描述，添加了包括物体大小信息的额外真实信息。最后，我们使用FMOX文件测试了最近提出的基础跟踪模型(称为EfficientTAM)，显示其性能与专门为这些FMO数据集设计的管道相比表现良好。我们在FMOX上使用轨迹交并比(TIoU)分数提供了这些最先进技术的比较。代码和JSON已开源共享，使FMOX能够被其他旨在处理FMO数据集的机器学习管道访问和使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fast and tiny object tracking remains a challenge in computer vision and inthis paper we first introduce a JSON metadata file associated with four opensource datasets of Fast Moving Objects (FMOs) image sequences. In addition, weextend the description of the FMOs datasets with additional ground truthinformation in JSON format (called FMOX) with object size information. Finallywe use our FMOX file to test a recently proposed foundational model fortracking (called EfficientTAM) showing that its performance compares well withthe pipelines originally taylored for these FMO datasets. Our comparison ofthese state-of-the-art techniques on FMOX is provided with TrajectoryIntersection of Union (TIoU) scores. The code and JSON is shared open sourceallowing FMOX to be accessible and usable for other machine learning pipelinesaiming to process FMO datasets.</description>
      <author>example@mail.com (Senem Aktas, Charles Markham, John McDonald, Rozenn Dahyot)</author>
      <guid isPermaLink="false">2509.06536v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients</title>
      <link>http://arxiv.org/abs/2509.06516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为QualityFM的新型多模态基础模型，用于评估和处理光电容积脉搏波(PPG)和心电图(ECG)信号的质量问题，解决了现有方法在泛化能力、数据依赖性和跨任务迁移性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;光电容积脉搏波(PPG)和心电图(ECG)信号常在重症监护室和手术室中记录，但这些信号经常出现质量差、不完整和不一致的情况，可能导致误报或诊断不准确。现有方法存在泛化能力有限、依赖大量标记数据、跨任务迁移性差等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够获取生理信号质量通用理解的新型多模态基础模型，克服现有方法的局限性，提高信号质量评估的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;1) 构建双轨架构处理不同质量的成对生理信号；2) 采用自蒸馏策略，让高质量信号编码器指导低质量信号编码器训练；3) 在基于Transformer的模型中集成窗口稀疏注意力机制处理长序列信号；4) 设计复合损失函数结合直接蒸馏损失和间接重建损失；5) 在大规模数据集上预训练三个参数数量不同的模型(9.6M至319M)。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的三个模型在三个临床任务上表现出有效性和实用价值，包括：室性心动过速检测的误报减少、心房颤动的准确识别以及从PPG和ECG信号精确估算动脉血压。&lt;h4&gt;结论&lt;/h4&gt;QualityFM模型通过大规模预训练和创新的架构设计，有效解决了生理信号质量评估的挑战，为临床监护提供了更可靠的信号质量评估工具，有助于减少误报和提高诊断准确性。&lt;h4&gt;翻译&lt;/h4&gt;光电容积脉搏波(PPG)和心电图(ECG)通常在重症监护室(ICU)和手术室(OR)中记录。然而，信号质量差、不完整和不一致的高发生率，可能导致误报或诊断不准确。迄今为止探索的方法存在泛化能力有限、依赖大量标记数据和跨任务迁移性差等问题。为了克服这些挑战，我们引入了QualityFM，这是一种用于这些生理信号的新型多模态基础模型，旨在获取对信号质量的通用理解。我们的模型在一个包含超过2100万个30秒波形和179,757小时数据的大规模数据集上进行预训练。我们的方法涉及一个双轨架构，处理不同质量的成对生理信号，利用自蒸馏策略，其中高质量信号编码器用于指导低质量信号编码器的训练。为了有效处理长序列信号并捕获基本局部准周期性模式，我们在基于Transformer的模型中集成了窗口稀疏注意力机制。此外，结合编码器输出的直接蒸馏损失和基于功率谱及相位谱的间接重建损失的复合损失函数，确保了信号频域特征的保留。我们预训练了三个参数数量不同的模型(9.6M至319M)，并通过在三个不同的临床任务上的迁移学习证明了它们的有效性和实用价值：室性心动过速检测的误报、心房颤动的识别以及从PPG和ECG信号估算动脉血压(ABP)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded inintesive care unit (ICU) and operating room (OR). However, the high incidenceof poor, incomplete, and inconsistent signal quality, can lead to false alarmsor diagnostic inaccuracies. The methods explored so far suffer from limitedgeneralizability, reliance on extensive labeled data, and poor cross-tasktransferability. To overcome these challenges, we introduce QualityFM, a novelmultimodal foundation model for these physiological signals, designed toacquire a general-purpose understanding of signal quality. Our model ispre-trained on an large-scale dataset comprising over 21 million 30-secondwaveforms and 179,757 hours of data. Our approach involves a dual-trackarchitecture that processes paired physiological signals of differing quality,leveraging a self-distillation strategy where an encoder for high-qualitysignals is used to guide the training of an encoder for low-quality signals. Toefficiently handle long sequential signals and capture essential localquasi-periodic patterns, we integrate a windowed sparse attention mechanismwithin our Transformer-based model. Furthermore, a composite loss function,which combines direct distillation loss on encoder outputs with indirectreconstruction loss based on power and phase spectra, ensures the preservationof frequency-domain characteristics of the signals. We pre-train three modelswith varying parameter counts (9.6 M to 319 M) and demonstrate their efficacyand practical value through transfer learning on three distinct clinical tasks:false alarm of ventricular tachycardia detection, the identification of atrialfibrillation and the estimation of arterial blood pressure (ABP) from PPG andECG signals.</description>
      <author>example@mail.com (Zongheng Guo, Tao Chen, Manuela Ferrario)</author>
      <guid isPermaLink="false">2509.06516v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Does DINOv3 Set a New Medical Vision Standard?</title>
      <link>http://arxiv.org/abs/2509.06467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了DINOv3视觉基础模型在医学视觉任务上的表现，发现尽管仅用自然图像训练，它仍能成为医学任务的强大基线，甚至在某些任务上优于医学特定模型，但在高度专业化的医学领域表现有限。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉基础模型在自然图像上的预训练已改变了计算机视觉领域，但这些前沿模型在专业领域（如医学影像）的效能转移仍是一个开放性问题。&lt;h4&gt;目的&lt;/h4&gt;调查DINOv3（一种先进的自监督视觉Transformer）是否可以直接作为医学视觉任务的统一编码器，无需领域特定的预训练。&lt;h4&gt;方法&lt;/h4&gt;在多种医学成像模态的2D/3D分类和分割任务上对DINOv3进行基准测试，并通过改变模型大小和输入图像分辨率来分析其可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;DINOv3在医学视觉任务上表现出色，建立了强大的新基线，在某些任务上甚至超过医学特定模型；但在需要深度领域专业化的场景（如病理图像、电子显微镜和PET扫描）中性能下降；在医学领域不始终遵循缩放定律，性能不会随模型大小或分辨率增加而可靠提高。&lt;h4&gt;结论&lt;/h4&gt;DINOv3可作为医学视觉任务的强大基线模型，其视觉特征可作为复杂医学任务的稳健先验，为未来研究（如3D重建中的多视图一致性）开辟了新方向。&lt;h4&gt;翻译&lt;/h4&gt;大规模视觉基础模型的出现，这些模型在各种自然图像上进行了预训练，标志着计算机视觉的范式转变。然而，前沿视觉基础模型的效能如何转移到专业领域（如医学影像）仍然是一个悬而未决的问题。本报告研究了DINOv3（一种在密集预测任务中具有强大能力的最先进自监督视觉Transformer）是否可以直接作为医学视觉任务的强大统一编码器，无需领域特定的预训练。为了回答这个问题，我们在常见的医学视觉任务上对DINOv3进行了基准测试，包括在多种医学成像模态上的2D/3D分类和分割。我们通过改变模型大小和输入图像分辨率，系统地分析了其可扩展性。我们的研究结果显示，DINOv3表现出令人印象深刻的性能，并建立了一个强大的新基线。值得注意的是，尽管仅在自然图像上训练，它在几个任务上甚至可以超越医学特定的基础模型，如BiomedCLIP和CT-Net。然而，我们确定了明显的局限性：在需要深度领域专业化的场景中，如全载病理图像(WSIs)、电子显微镜(EM)和正电子发射断层扫描(PET)，模型的特征会退化。此外，我们观察到DINOv3在医学领域并不始终遵循缩放定律；性能不会随着更大模型或更精细的特征分辨率而可靠地增加，在不同任务上表现出多样的缩放行为。最终，我们的工作将DINOv3确立为一个强大的基线模型，其强大的视觉特征可以作为多种复杂医学任务的稳健先验。这为未来开辟了有希望的方向，例如利用其特征在3D重建中强制多视图一致性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of large-scale vision foundation models, pre-trained on diversenatural images, has marked a paradigm shift in computer vision. However, howthe frontier vision foundation models' efficacies transfer to specializeddomains remains such as medical imaging remains an open question. This reportinvestigates whether DINOv3, a state-of-the-art self-supervised visiontransformer (ViT) that features strong capability in dense prediction tasks,can directly serve as a powerful, unified encoder for medical vision taskswithout domain-specific pre-training. To answer this, we benchmark DINOv3across common medical vision tasks, including 2D/3D classification andsegmentation on a wide range of medical imaging modalities. We systematicallyanalyze its scalability by varying model sizes and input image resolutions. Ourfindings reveal that DINOv3 shows impressive performance and establishes aformidable new baseline. Remarkably, it can even outperform medical-specificfoundation models like BiomedCLIP and CT-Net on several tasks, despite beingtrained solely on natural images. However, we identify clear limitations: Themodel's features degrade in scenarios requiring deep domain specialization,such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3does not consistently obey scaling law in the medical domain; performance doesnot reliably increase with larger models or finer feature resolutions, showingdiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3as a strong baseline, whose powerful visual features can serve as a robustprior for multiple complex medical tasks. This opens promising futuredirections, such as leveraging its features to enforce multiview consistency in3D reconstruction.</description>
      <author>example@mail.com (Che Liu, Yinda Chen, Haoyuan Shi, Jinpeng Lu, Bailiang Jian, Jiazhen Pan, Linghan Cai, Jiayi Wang, Yundi Zhang, Jun Li, Cosmin I. Bercea, Cheng Ouyang, Chen Chen, Zhiwei Xiong, Benedikt Wiestler, Christian Wachinger, Daniel Rueckert, Wenjia Bai, Rossella Arcucci)</author>
      <guid isPermaLink="false">2509.06467v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics</title>
      <link>http://arxiv.org/abs/2509.06322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了文本训练的基础模型能够在不微调或自然语言提示的情况下，从离散的偏微分方程解中准确推断时空动态，并揭示了预测质量与上下文长度和输出长度之间的可预测关系。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型已在各种任务中表现出上下文学习能力，包括零样本时间序列预测。&lt;h4&gt;目的&lt;/h4&gt;探究文本训练的基础模型是否可以在无需额外训练的情况下，从偏微分方程的离散解中准确推断时空动态。&lt;h4&gt;方法&lt;/h4&gt;分析标记级输出分布，研究大型语言模型如何内部处理偏微分方程解以实现准确预测。&lt;h4&gt;主要发现&lt;/h4&gt;预测准确性随时间上下文长度增加而提高，但在更精细的空间离散化时会下降；多步预测中误差随时间范围代数增长，类似于经典有限差分解算子的全局误差累积；这些趋势符合上下文神经缩放定律。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型通过一致的上下文学习进展处理偏微分方程解：从语法模式模仿开始，过渡到探索性高熵阶段，最终以自信的、基于数字的预测结束。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已在一系列任务中表现出上下文学习能力，包括零样本时间序列预测。我们证明，文本训练的基础模型可以在不微调或自然语言提示的情况下，从离散的偏微分方程解中准确推断时空动态。预测准确性随时间上下文长度增加而提高，但在更精细的空间离散化时会下降。在多步展开中，模型递归预测多个时间步的未来空间状态，误差随时间范围代数增长，类似于经典有限差分解算子中的全局误差累积。我们将这些趋势解释为上下文神经缩放定律，其中预测质量随上下文长度和输出长度可预测地变化。为了更好地理解大型语言模型如何能够内部处理偏微分方程解以准确展开预测，我们分析了标记级输出分布，并发现了一致的上下文学习进展：从语法模式模仿开始，过渡到探索性高熵阶段，最终以自信的、基于数字的预测结束。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated emergent in-context learning(ICL) capabilities across a range of tasks, including zero-shot time-seriesforecasting. We show that text-trained foundation models can accuratelyextrapolate spatiotemporal dynamics from discretized partial differentialequation (PDE) solutions without fine-tuning or natural language prompting.Predictive accuracy improves with longer temporal contexts but degrades atfiner spatial discretizations. In multi-step rollouts, where the modelrecursively predicts future spatial states over multiple time steps, errorsgrow algebraically with the time horizon, reminiscent of global erroraccumulation in classical finite-difference solvers. We interpret these trendsas in-context neural scaling laws, where prediction quality varies predictablywith both context length and output length. To better understand how LLMs areable to internally process PDE solutions so as to accurately roll them out, weanalyze token-level output distributions and uncover a consistent ICLprogression: beginning with syntactic pattern imitation, transitioning throughan exploratory high-entropy phase, and culminating in confident, numericallygrounded predictions.</description>
      <author>example@mail.com (Jiajun Bao, Nicolas Boullé, Toni J. B. Liu, Raphaël Sarfati, Christopher J. Earls)</author>
      <guid isPermaLink="false">2509.06322v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting</title>
      <link>http://arxiv.org/abs/2509.06311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WindFM是一个专门为概率性风力发电预测设计的轻量级生成式基础模型，采用离散化和生成框架，能够实现高质量的零样本预测性能，无需微调即可优于专业模型和更大的基础模型。&lt;h4&gt;背景&lt;/h4&gt;高质量的风力发电预测对现代电网运行至关重要，但现有的数据驱动方法存在局限性，要么无法泛化到其他地点，要么难以融入能源领域的特定领域数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于概率性风力发电预测的轻量级生成式基础模型，解决现有方法的泛化和领域适应性不足的问题。&lt;h4&gt;方法&lt;/h4&gt;采用离散化和生成框架，使用专门的时间序列标记器将连续多元观测值转换为离散的分层标记，然后通过自回归预训练学习风力发电动力学的通用表示。&lt;h4&gt;主要发现&lt;/h4&gt;810万参数的WindFM模型在确定性和概率性任务上实现了最先进的零样本性能，无需微调即可优于专业模型和更大的基础模型，并且在分布外数据上表现出强适应性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;WindFM通过大规模数据预训练学习到的表示具有良好的泛化能力和可转移性，为风力发电预测提供了一个有效的基础模型解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高质量的风力发电预测对现代电网运行至关重要。然而，现有的数据驱动范式要么训练特定地点的模型无法泛化到其他位置，要么依赖通用时间序列基础模型的微调，难以融入能源领域的特定领域数据。本文介绍了WindFM，一个专门为概率性风力发电预测设计的轻量级生成式基础模型。WindFM采用离散化和生成框架，首先使用专门的时间序列标记器将连续多元观测值转换为离散的分层标记，然后通过自回归预训练学习风力发电动力学的通用表示。使用包含约1500亿个时间步和超过126,000个地点的WIND Toolkit数据集，WindFM开发了对大气条件与发电输出之间复杂相互作用的基础理解。大量实验表明，我们紧凑的810万参数模型在确定性和概率性任务上都实现了最先进的零样本性能，优于专业模型和更大的基础模型，无需任何微调。特别是，WindFM在不同大陆的分布外数据上表现出强适应性，展示了其学习表示的鲁棒性和可转移性。我们的预训练模型已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality wind power forecasting is crucial for the operation of modernpower grids. However, prevailing data-driven paradigms either train asite-specific model which cannot generalize to other locations or rely onfine-tuning of general-purpose time series foundation models which aredifficult to incorporate domain-specific data in the energy sector. This paperintroduces WindFM, a lightweight and generative Foundation Model designedspecifically for probabilistic wind power forecasting. WindFM employs adiscretize-and-generate framework. A specialized time-series tokenizer firstconverts continuous multivariate observations into discrete, hierarchicaltokens. Subsequently, a decoder-only Transformer learns a universalrepresentation of wind generation dynamics by autoregressively pre-training onthese token sequences. Using the comprehensive WIND Toolkit dataset comprisingapproximately 150 billion time steps from more than 126,000 sites, WindFMdevelops a foundational understanding of the complex interplay betweenatmospheric conditions and power output. Extensive experiments demonstrate thatour compact 8.1M parameter model achieves state-of-the-art zero-shotperformance on both deterministic and probabilistic tasks, outperformingspecialized models and larger foundation models without any fine-tuning. Inparticular, WindFM exhibits strong adaptiveness under out-of-distribution datafrom a different continent, demonstrating the robustness and transferability ofits learned representations. Our pre-trained model is publicly available athttps://github.com/shiyu-coder/WindFM.</description>
      <author>example@mail.com (Hang Fan, Yu Shi, Zongliang Fu, Shuo Chen, Wei Wei, Wei Xu, Jian Li)</author>
      <guid isPermaLink="false">2509.06311v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2509.06233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning (CoRL) 2025. Project website:  https://o3afford.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的一次性3D物体到物体可供性学习方法(O³Afford)，用于机器人操作中的物体可供性 grounding，解决了现有研究主要关注单物体可供性而忽略物体对交互关系的问题。&lt;h4&gt;背景&lt;/h4&gt;物体可供性(grounding object affordance)对机器人操作至关重要，它建立了感知与行动之间的关键联系。然而，现有研究主要关注单物体可供性预测，忽略了现实世界中大多数交互涉及物体对之间的关系。&lt;h4&gt;目的&lt;/h4&gt;解决在有限数据约束下的物体到物体可供性 grounding 挑战，提出一种新颖的一次性3D物体到物体可供性学习方法，用于机器人操作。&lt;h4&gt;方法&lt;/h4&gt;受近期2D视觉基础模型的小样本学习进展启发，结合视觉基础模型的语义特征和点云表示以实现几何理解，构建一次性学习管道。此外，将3D可供性表示与大型语言模型(LLMs)集成，显著增强LLMs在生成特定任务约束函数时理解物体交互的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在3D物体到物体可供性 grounding 和机器人操作实验中，提出的O³Afford方法在准确性和泛化能力方面显著优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;O³Afford方法在解决物体到物体可供性 grounding 问题上表现优异，在有限数据条件下能够有效学习并推广到新场景。&lt;h4&gt;翻译&lt;/h4&gt;物体可供性 grounding 是机器人操作的基础，因为它在相互作用的物体之间建立了感知与行动的关键联系。然而，先前的工作主要集中预测单物体可供性，忽略了大多数现实世界交互涉及物体对之间关系的事实。在这项工作中，我们解决了在有限数据约束下物体到物体可供性 grounding 的挑战。受近期2D视觉基础模型小样本学习进展的启发，我们提出了一种新颖的一次性3D物体到物体可供性学习方法，用于机器人操作。视觉基础模型的语义特征结合点云表示以实现几何理解，使我们的一次性学习管道能够有效推广到新物体和类别。我们进一步将3D可供性表示与大型语言模型(LLMs)集成用于机器人操作，显著增强了LLMs在生成特定任务约束函数时理解和推理物体交互的能力。我们在3D物体到物体可供性 grounding 和机器人操作上的实验表明，我们的O³Afford在准确性和泛化能力方面显著优于现有基线。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是3D物体间可操作性的单样本学习，即如何让机器人仅通过一个示例就能理解两个物体之间的交互关系并泛化到新物体上。这个问题在现实中非常重要，因为日常生活中的许多任务（如倒水、切割、悬挂等）都涉及两个物体之间的交互，而收集大量标注数据非常困难。现有方法主要关注单个物体的可操作性，忽略了物体间的关系，且泛化能力有限，难以适应新物体和新场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现实世界交互大多涉及物体对而非单个物体的特点，认识到收集大量物体间交互数据的困难，因此提出单样本学习思路。他们借鉴了视觉基础模型（如DINOv2）在少样本学习中的成功经验，结合3D点云表示来获取几何信息，弥补2D图像的局限性。同时，他们利用大型语言模型来生成任务特定的约束函数。方法设计上，他们将语义特征投影到3D点云上，设计了双向注意力变换器解码器处理物体间交互，并将可操作性表示与LLM集成用于机器人操作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 将视觉基础模型的语义特征与3D点云的几何信息结合，同时理解物体的功能属性和空间结构；2) 通过双向注意力机制，让模型同时考虑源物体和目标物体的上下文信息；3) 利用单样本学习实现泛化到新物体的能力；4) 将可操作性表示与大型语言模型结合，使机器人能根据自然语言指令执行复杂任务。整体流程包括：1) 构建语义点云，从多视角RGB-D中提取DINOv2特征并投影到3D点云；2) 使用双向注意力变换器解码器预测可操作性图；3) 将可操作性图与LLM结合，LLM生成约束函数，通过优化算法找到最佳物体姿态实现机器人操作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次实现单样本3D物体间可操作性学习，解决数据稀缺问题；2) 融合语义特征与几何信息，提高泛化能力；3) 设计双向注意力机制，理解物体间交互关系；4) 集成LLM自动生成任务特定约束函数。相比之前工作，不同之处在于：从关注单物体转向物体对交互，从2D图像空间转向3D点云空间，从需要大量数据转向仅需单样本，从手动设计约束转向LLM自动生成约束。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出O3Afford方法，通过融合视觉基础模型语义特征与3D点云几何信息，并集成大型语言模型，实现了仅通过一个示例就能理解两个物体间的交互关系并泛化到新物体的能力，显著提升了机器人在复杂操作任务中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grounding object affordance is fundamental to robotic manipulation as itestablishes the critical link between perception and action among interactingobjects. However, prior works predominantly focus on predicting single-objectaffordance, overlooking the fact that most real-world interactions involverelationships between pairs of objects. In this work, we address the challengeof object-to-object affordance grounding under limited data contraints.Inspired by recent advances in few-shot learning with 2D vision foundationmodels, we propose a novel one-shot 3D object-to-object affordance learningapproach for robotic manipulation. Semantic features from vision foundationmodels combined with point cloud representation for geometric understandingenable our one-shot learning pipeline to generalize effectively to novelobjects and categories. We further integrate our 3D affordance representationwith large language models (LLMs) for robotics manipulation, significantlyenhancing LLMs' capability to comprehend and reason about object interactionswhen generating task-specific constraint functions. Our experiments on 3Dobject-to-object affordance grounding and robotic manipulation demonstrate thatour O$^3$Afford significantly outperforms existing baselines in terms of bothaccuracy and generalization capability.</description>
      <author>example@mail.com (Tongxuan Tian, Xuhui Kang, Yen-Ling Kuo)</author>
      <guid isPermaLink="false">2509.06233v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving</title>
      <link>http://arxiv.org/abs/2509.06142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为RetinaGuard的隐私增强框架，用于保护眼底图像中的生物特征数据，特别是视网膜年龄信息，同时保持图像质量和诊断效用。&lt;h4&gt;背景&lt;/h4&gt;AI与医学图像的结合可以从图像中提取隐性的生物标志物用于健康评估。视网膜年龄是从眼底图像预测出的生物标志物，可以预测全身疾病风险、行为模式、衰老轨迹和死亡率。然而，这种技术带来了隐私风险，未经授权使用眼底图像可能导致生物信息泄露。&lt;h4&gt;目的&lt;/h4&gt;提出与医学图像相关的生物特征隐私这一新研究问题，并开发一种隐私增强框架来保护眼底图像中的敏感生物特征数据。&lt;h4&gt;方法&lt;/h4&gt;提出RetinaGuard框架，采用特征级别生成对抗性掩蔽机制来模糊视网膜年龄，同时保持图像视觉质量和疾病诊断效用。框架还利用多对一知识蒸馏策略，结合视网膜基础模型和多样化的代理年龄编码器，以实现对黑盒年龄预测模型的通用防御。&lt;h4&gt;主要发现&lt;/h4&gt;全面评估证实，RetinaGuard成功模糊了视网膜年龄预测，同时对图像质量和病理特征表示的影响最小。&lt;h4&gt;结论&lt;/h4&gt;RetinaGuard是一种有效的隐私保护框架，可以灵活扩展到其他医学图像衍生的生物标志物。&lt;h4&gt;翻译&lt;/h4&gt;将AI与医学图像相结合可以从图像中提取隐性的图像衍生生物标志物，用于精确的健康评估。最近，从眼底图像预测出的视网膜年龄是一种已证实可以预测全身疾病风险、行为模式、衰老轨迹甚至死亡率的生物标志物。然而，推断此类敏感生物特征数据的能力带来了重大的隐私风险，未经授权使用眼底图像可能导致生物信息泄露，侵犯个人隐私。为此，我们提出了与医学图像相关的生物特征隐私这一新研究问题，并提出了RetinaGuard，一种新颖的隐私增强框架，该框架采用特征级别生成对抗性掩蔽机制来模糊视网膜年龄，同时保持图像视觉质量和疾病诊断效用。该框架进一步利用了一种新颖的多对一知识蒸馏策略，结合了视网膜基础模型和多样化的代理年龄编码器，以实现对黑盒年龄预测模型的通用防御。全面评估证实，RetinaGuard成功模糊了视网膜年龄预测，同时对图像质量和病理特征表示的影响最小。RetinaGuard还可以灵活扩展到其他医学图像衍生的生物标志物。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of AI with medical images enables the extraction of implicitimage-derived biomarkers for a precise health assessment. Recently, retinalage, a biomarker predicted from fundus images, is a proven predictor ofsystemic disease risks, behavioral patterns, aging trajectory and evenmortality. However, the capability to infer such sensitive biometric dataraises significant privacy risks, where unauthorized use of fundus images couldlead to bioinformation leakage, breaching individual privacy. In response, weformulate a new research problem of biometric privacy associated with medicalimages and propose RetinaGuard, a novel privacy-enhancing framework thatemploys a feature-level generative adversarial masking mechanism to obscureretinal age while preserving image visual quality and disease diagnosticutility. The framework further utilizes a novel multiple-to-one knowledgedistillation strategy incorporating a retinal foundation model and diversesurrogate age encoders to enable a universal defense against black-box ageprediction models. Comprehensive evaluations confirm that RetinaGuardsuccessfully obfuscates retinal age prediction with minimal impact on imagequality and pathological feature representation. RetinaGuard is also flexiblefor extension to other medical image derived biomarkers. RetinaGuard is alsoflexible for extension to other medical image biomarkers.</description>
      <author>example@mail.com (Zhengquan Luo, Chi Liu, Dongfu Xiao, Zhen Yu, Yueye Wang, Tianqing Zhu)</author>
      <guid isPermaLink="false">2509.06142v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.06096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MedSeqFT是一种顺序微调框架，通过最大数据相似性选择和基于LoRA的知识蒸馏方案，解决了基础模型在医学图像分割任务中微调的局限性，显著提升了性能和可迁移性。&lt;h4&gt;背景&lt;/h4&gt;基础模型在医学图像分析中显示出巨大潜力，特别是在分割任务方面。然而，现有的微调策略存在局限性：并行微调隔离任务无法利用共享知识，而多任务微调需要同时访问所有数据集，难以集成增量任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够逐步将预训练模型适应到新任务同时改进其表示能力的微调框架，解决现有微调策略的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出MedSeqFT，包含两个核心组件：(1)最大数据相似性选择，识别最能代表原始预训练分布的下游样本以保留通用知识；(2)知识和泛化保留微调，一种基于LoRA的知识蒸馏方案，平衡特定任务适应与预训练知识保留。&lt;h4&gt;主要发现&lt;/h4&gt;在两个多任务数据集上覆盖10个3D分割任务的实验表明，MedSeqFT始终优于最先进的微调策略，带来显著的性能提升（平均Dice提高3.0%）。在两个未见过的任务上验证了MedSeqFT提高了可迁移性，特别是肿瘤分割。损失景观和参数变化的视觉分析进一步证明了MedSeqFT的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;顺序微调是一种有效的、保留知识的范式，用于将基础模型适应不断发展的临床任务。代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已成为推进医学图像分析的一种有前景的范式，特别是在下游应用通常按顺序出现的分割任务中。然而，现有的微调策略仍然有限：并行微调隔离任务且无法利用共享知识，而多任务微调需要同时访问所有数据集且难以集成增量任务。为解决这些挑战，我们提出了MedSeqFT，一种顺序微调框架，能够逐步将预训练模型适应到新任务，同时改进其表示能力。MedSeqFT引入了两个核心组件：(1)最大数据相似性选择，识别最能代表原始预训练分布的下游样本以保留通用知识；(2)知识和泛化保留微调，一种基于LoRA的知识蒸馏方案，平衡特定任务适应与预训练知识保留。在覆盖10个3D分割任务的两个多任务数据集上的广泛实验表明，MedSeqFT始终优于最先进的微调策略，带来显著的性能提升（例如，平均Dice提高3.0%）。此外，在两个未见过的任务（COVID-19-20和肾脏）上的评估验证了MedSeqFT提高了可迁移性，特别是对于肿瘤分割。损失景观和参数变化的视觉分析进一步突显了MedSeqFT的鲁棒性。这些结果确立了顺序微调作为一种有效的、保留知识的范式，用于将基础模型适应不断发展的临床任务。代码将公开发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学图像分割中基础模型的微调策略问题。现有方法中，并行微调无法利用任务间的共享知识，而多任务微调需要同时访问所有数据集，难以灵活增量整合新任务。这个问题在医学图像分析中非常重要，因为临床实践中新任务往往随时间推移逐渐出现，而非一次性全部出现；同时医学图像分割需要专家标注，耗时耗力，基础模型通过自监督学习可减少对标注数据的依赖。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有微调策略的局限性，提出更符合临床工作流的顺序微调范式。他们识别出顺序微调中的关键挑战是避免灾难性遗忘。设计方法时借鉴了自监督学习(SSL)的预训练-微调范式、知识蒸馏(KD)技术、LoRA参数高效微调方法，以及nnU-Net等医学图像分析领域的现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出MedSeqFT框架，采用顺序微调策略逐步适应预训练模型到多个3D医学图像分割任务，并通过两个关键组件平衡任务特定适应与保留预训练知识：1)最大数据相似性(MDS)选择，保留最能代表原始预训练分布的样本；2)知识与泛化保留微调(K&amp;G RFT)，基于LoRA的知识蒸馏方案。整体流程包括：初始化第一个任务，使用MDS选择代表性样本，对后续任务顺序微调，迭代更新模型和缓冲区。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出MedSeqFT顺序微调框架；2)引入MDS策略保留泛化知识；3)设计K&amp;G RFT策略平衡适应与知识保留；4)实验证明在多任务上性能优越且增强可迁移性。相比之前工作不同：能利用任务间共享知识而非独立处理；不需要同时访问所有数据集，可灵活增量整合新任务；通过MDS和K&amp;G RFT显式减轻灾难性遗忘；整体性能优于传统FFT和PEFT方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MedSeqFT通过创新的顺序微调框架解决了医学图像分割中任务间知识共享与灵活适应新任务的矛盾，在保持模型泛化能力的同时显著提升了分割性能和可迁移性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have become a promising paradigm for advancing medicalimage analysis, particularly for segmentation tasks where downstreamapplications often emerge sequentially. Existing fine-tuning strategies,however, remain limited: parallel fine-tuning isolates tasks and fails toexploit shared knowledge, while multi-task fine-tuning requires simultaneousaccess to all datasets and struggles with incremental task integration. Toaddress these challenges, we propose MedSeqFT, a sequential fine-tuningframework that progressively adapts pre-trained models to new tasks whilerefining their representational capacity. MedSeqFT introduces two corecomponents: (1) Maximum Data Similarity (MDS) selection, which identifiesdownstream samples most representative of the original pre-trainingdistribution to preserve general knowledge, and (2) Knowledge andGeneralization Retention Fine-Tuning (K&amp;G RFT), a LoRA-based knowledgedistillation scheme that balances task-specific adaptation with the retentionof pre-trained knowledge. Extensive experiments on two multi-task datasetscovering ten 3D segmentation tasks demonstrate that MedSeqFT consistentlyoutperforms state-of-the-art fine-tuning strategies, yielding substantialperformance gains (e.g., an average Dice improvement of 3.0%). Furthermore,evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFTenhances transferability, particularly for tumor segmentation. Visual analysesof loss landscapes and parameter variations further highlight the robustness ofMedSeqFT. These results establish sequential fine-tuning as an effective,knowledge-retentive paradigm for adapting foundation models to evolvingclinical tasks. Code will be released.</description>
      <author>example@mail.com (Yiwen Ye, Yicheng Wu, Xiangde Luo, He Zhang, Ziyang Chen, Ting Dang, Yanning Zhang, Yong Xia)</author>
      <guid isPermaLink="false">2509.06096v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior</title>
      <link>http://arxiv.org/abs/2509.06025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了统一交互基础模型（UIFM），这是一种为真正行为理解而设计的基础模型，通过复合标记化原则将多属性事件视为语义连贯的单位，从而能够学习用户行为的基本'语法'，感知整个交互而不是不相关的数据点流。&lt;h4&gt;背景&lt;/h4&gt;当前基础模型是为自然语言设计的，但无法理解电信、电子商务和金融等领域中复杂的、不断发展的序列事件的整体性质。&lt;h4&gt;目的&lt;/h4&gt;构建能够理解和预测复杂、不断发展的序列事件的人工智能系统。&lt;h4&gt;方法&lt;/h4&gt;引入统一交互基础模型（UIFM），采用复合标记化原则，将每个多属性事件视为单个语义连贯的单位。&lt;h4&gt;主要发现&lt;/h4&gt;UIFM架构不仅更准确，而且代表了创建更适应性和更智能的预测系统的根本性进展。&lt;h4&gt;结论&lt;/h4&gt;UIFM能够学习用户行为的基本'语法'，感知整个交互而不是不相关的数据点流，是创建更适应性和智能预测系统的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;人工智能的一个核心目标是构建能够理解和预测复杂、不断发展的序列事件的系统。然而，当前为自然语言设计的基础模型无法理解电信、电子商务和金融等领域中结构化交互的整体性质。通过将事件序列化为文本，这些模型将它们分解为语义碎片化的部分，失去了关键上下文。在这项工作中，我们介绍了统一交互基础模型（UIFM），这是一个为真正的行为理解而设计的基础模型。其核心是复合标记化原则，其中每个多属性事件被视为单个语义连贯的单位。这使得UIFM能够学习用户行为的基本'语法'，感知整个交互而不是不相关的数据点流。我们证明，这种架构不仅更准确，而且代表了创建更适应性和更智能的预测系统的根本性进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A central goal of artificial intelligence is to build systems that canunderstand and predict complex, evolving sequences of events. However, currentfoundation models, designed for natural language, fail to grasp the holisticnature of structured interactions found in domains like telecommunications,e-commerce and finance. By serializing events into text, they disassemble theminto semantically fragmented parts, losing critical context. In this work, weintroduce the Unified Interaction Foundation Model (UIFM), a foundation modelengineered for genuine behavioral understanding. At its core is the principleof composite tokenization, where each multi-attribute event is treated as asingle, semantically coherent unit. This allows UIFM to learn the underlying"grammar" of user behavior, perceiving entire interactions rather than adisconnected stream of data points. We demonstrate that this architecture isnot just more accurate, but represents a fundamental step towards creating moreadaptable and intelligent predictive systems.</description>
      <author>example@mail.com (Vignesh Ethiraj, Subhash Talluri)</author>
      <guid isPermaLink="false">2509.06025v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance</title>
      <link>http://arxiv.org/abs/2509.05978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一种基于语言提示的框架，能够生成高分辨率3D反事实医学图像，解决了3D领域缺乏预训练基础模型的问题，首次将语言引导的原生3D扩散模型应用于神经影像数据。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在2D图像生成方面表现出色，但这依赖于大量可用的预训练基础模型。然而，3D领域缺乏类似的预训练基础模型，严重限制了该领域的进展。视觉语言模型在仅基于自然语言描述生成高分辨率3D反事实医学图像方面的潜力尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;解决3D医学影像生成的这一差距，开发一个能够根据自由形式语言提示生成高分辨率3D反事实医学图像的框架，用于临床和研究应用，如个性化反事实解释、疾病进展场景模拟和增强医疗培训。&lt;h4&gt;方法&lt;/h4&gt;采用最先进的3D扩散模型，结合Simple Diffusion的增强功能，并加入增强的条件设置以提高文本对齐和图像质量。这是首次将语言引导的原生3D扩散模型应用于神经影像数据，其中忠实的三维建模对于表示大脑的三维结构至关重要。&lt;h4&gt;主要发现&lt;/h4&gt;在两个不同的神经MRI数据集上，该框架成功模拟了多发性硬化症的不同反事实病变负荷和阿尔茨海默病的认知状态，生成了高质量图像，同时在合成的医学图像中保持了主体保真度。&lt;h4&gt;结论&lt;/h4&gt;研究结果为3D医学影像中的提示驱动疾病进展分析奠定了基础，为个性化医疗和医学研究提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在各种条件下生成2D图像方面表现出令人印象深刻的能力；然而，这些模型在2D方面的出色表现很大程度上依赖于大量可用的预训练基础模型。关键的是，3D领域没有类似的预训练基础模型，这严重限制了该领域的进展。因此，视觉语言模型仅根据自然语言描述生成高分辨率3D反事实医学图像的潜力仍未被探索。解决这一差距将 enable强大的临床和研究应用，如个性化反事实解释、疾病进展场景模拟，以及通过以真实细节可视化假设性医疗条件来增强医疗培训。我们的工作通过引入一个能够根据自由形式语言提示生成高分辨率3D反事实医学图像的框架，朝着解决这一挑战迈出了有意义的一步。我们采用了最先进的3D扩散模型，结合了Simple Diffusion的增强功能，并加入了增强的条件设置以提高文本对齐和图像质量。据我们所知，这是首次将语言引导的原生3D扩散模型专门应用于神经影像数据，其中忠实的三维建模对于表示大脑的三维结构至关重要。通过对两个不同的神经MRI数据集的实验，我们的框架成功模拟了多发性硬化症中的不同反事实病变负荷和阿尔茨海默病中的认知状态，生成了高质量图像，同时在合成的医学图像中保持了主体保真度。我们的结果为3D医学影像中的提示驱动疾病进展分析奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何通过自然语言描述生成高分辨率的3D反事实医学图像问题。这个问题在现实中很重要，因为它可以实现个性化反事实解释、模拟疾病进展场景、增强医学培训，让医生能够可视化假设的医疗状况，从而提高临床决策能力和医学教育效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了多个现有工作：采用了Simple Diffusion的三种架构增强（加深U-Net瓶颈、应用目标dropout、多尺度交叉注意力层）；整合了MAISI 3D潜在扩散框架并引入Rectified Flow噪声调度；使用BiomedCLIP获取医学语义嵌入。作者设计思路是构建一个能根据语言提示生成高分辨率3D反事实医学图像的框架，直接在体素空间操作以支持精细编辑。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用语言引导的扩散模型生成3D医学图像的反事实版本，保持患者解剖结构不变，同时根据文本提示改变临床状态。流程包括：1)创建与MRI特征对应的文本提示；2)构建语言引导的3D扩散模型，采用架构增强和交叉注意力层；3)从相同噪声源使用不同文本提示生成反事实图像，共享患者身份但反映不同临床状态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：首次实现语言引导的3D反事实医学图像生成；首次将原生3D扩散模型应用于神经成像；结合Simple Diffusion增强和BiomedCLIP语义嵌入；引入Rectified Flow噪声调度提高效率；提出使用无分类器引导平衡文本对齐和解剖保真度。不同之处：现有方法主要生成新合成扫描而非修改现有图像；受限于简单变量而非自由文本；主要关注物体表面而非内部体积结构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的语言引导3D扩散模型框架，能够根据自然语言描述生成高保真度的反事实医学图像，为疾病进展模拟、个性化临床解释和医学教育提供了新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models have demonstrated impressive capabilities ingenerating 2D images under various conditions; however the impressiveperformance of these models in 2D is largely enabled by extensive, readilyavailable pretrained foundation models. Critically, comparable pretrainedfoundation models do not exist for 3D, significantly limiting progress in thisdomain. As a result, the potential of vision-language models to producehigh-resolution 3D counterfactual medical images conditioned solely on naturallanguage descriptions remains completely unexplored. Addressing this gap wouldenable powerful clinical and research applications, such as personalizedcounterfactual explanations, simulation of disease progression scenarios, andenhanced medical training by visualizing hypothetical medical conditions inrealistic detail. Our work takes a meaningful step toward addressing thischallenge by introducing a framework capable of generating high-resolution 3Dcounterfactual medical images of synthesized patients guided by free-formlanguage prompts. We adapt state-of-the-art 3D diffusion models withenhancements from Simple Diffusion and incorporate augmented conditioning toimprove text alignment and image quality. To our knowledge, this represents thefirst demonstration of a language-guided native-3D diffusion model appliedspecifically to neurological imaging data, where faithful three-dimensionalmodeling is essential to represent the brain's three-dimensional structure.Through results on two distinct neurological MRI datasets, our frameworksuccessfully simulates varying counterfactual lesion loads in MultipleSclerosis (MS), and cognitive states in Alzheimer's disease, generatinghigh-quality images while preserving subject fidelity in syntheticallygenerated medical images. Our results lay the groundwork for prompt-drivendisease progression analysis within 3D medical imaging.</description>
      <author>example@mail.com (Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel)</author>
      <guid isPermaLink="false">2509.05978v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models</title>
      <link>http://arxiv.org/abs/2509.05925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at IEEE 35th Workshop on Machine  Learning for Signal Processing (MLSP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的语义压缩方法，利用CLIP模型的能力，专注于保留图像的语义信息而非像素级细节。通过压缩CLIP特征嵌入而非原始图像，实现了极低的比特率，同时保持跨任务的语义完整性。&lt;h4&gt;背景&lt;/h4&gt;现有基于深度学习的有损图像压缩方法通过端到端训练和先进架构实现了具有竞争力的率失真性能。然而，新兴应用越来越重视语义保持而非像素级重建，并且需要在不同数据分布和下游任务中保持稳健性能。&lt;h4&gt;目的&lt;/h4&gt;开发先进的语义压缩范式，以满足新兴应用对语义保持和跨数据分布稳健性的需求。&lt;h4&gt;方法&lt;/h4&gt;基于对比语言-图像预训练(CLIP)模型提出新型语义压缩方法。不压缩图像用于重建，而是将CLIP特征嵌入压缩到最少比特数，同时保留不同任务中的语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在基准数据集中保持语义完整性，实现了每像素约2-3×10^(-3)比特的平均比特率，不到主流图像压缩方法在可比性能下所需比特率的5%。即使在极端压缩下，该方法也在不同数据分布和下游任务中表现出零样本稳健性。&lt;h4&gt;结论&lt;/h4&gt;基于CLIP模型的语义压缩方法能够在极低比特率下有效保留图像语义信息，相比传统像素级重建方法在语义保持方面具有显著优势，且在不同数据分布中表现出强大的零样本稳健性。&lt;h4&gt;翻译&lt;/h4&gt;最近基于深度学习的有损图像压缩方法通过广泛的端到端训练和先进架构实现了具有竞争力的率失真性能。然而，新兴应用越来越优先考虑语义保持而非像素级重建，并要求在不同数据分布和下游任务中保持稳健性能。这些挑战需要先进的语义压缩范式。受多模态基础模型的零样本和表征能力启发，我们提出了一种基于对比语言-图像预训练(CLIP)模型的新型语义压缩方法。我们不是为重建而压缩图像，而是将CLIP特征嵌入压缩到最少的比特数，同时保留不同任务中的语义信息。实验表明，我们的方法在基准数据集中保持语义完整性，实现了每像素约2-3×10^(-3)比特的平均比特率。这不到主流图像压缩方法在可比性能下所需比特率的5%。值得注意的是，即使在极端压缩下，所提出的方法也在不同数据分布和下游任务中表现出零样本稳健性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent deep learning-based methods for lossy image compression achievecompetitive rate-distortion performance through extensive end-to-end trainingand advanced architectures. However, emerging applications increasinglyprioritize semantic preservation over pixel-level reconstruction and demandrobust performance across diverse data distributions and downstream tasks.These challenges call for advanced semantic compression paradigms. Motivated bythe zero-shot and representational capabilities of multimodal foundationmodels, we propose a novel semantic compression method based on the contrastivelanguage-image pretraining (CLIP) model. Rather than compressing images forreconstruction, we propose compressing the CLIP feature embeddings into minimalbits while preserving semantic information across different tasks. Experimentsshow that our method maintains semantic integrity across benchmark datasets,achieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. Thisis less than 5% of the bitrate required by mainstream image compressionapproaches for comparable performance. Remarkably, even under extremecompression, the proposed approach exhibits zero-shot robustness across diversedata distributions and downstream tasks.</description>
      <author>example@mail.com (Ruiqi Shen, Haotian Wu, Wenjing Zhang, Jiangjing Hu, Deniz Gunduz)</author>
      <guid isPermaLink="false">2509.05925v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets</title>
      <link>http://arxiv.org/abs/2509.05892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了多种先进的深度学习分割模型在心血管组织病理图像中颈动脉结构分割任务上的性能，发现模型性能对数据分割高度敏感，且性能差异更多由统计噪声而非真正的算法优势驱动。&lt;h4&gt;背景&lt;/h4&gt;准确分割心血管组织病理图像中的颈动脉结构对推进心血管疾病研究和诊断至关重要。然而，该领域深度学习模型的发展受到已标注的心血管组织病理数据稀缺性的限制。&lt;h4&gt;目的&lt;/h4&gt;系统评估最先进的深度学习分割模型在有限的心血管组织病理数据集上的表现，并探讨标准基准测试实践在低数据临床环境中的局限性。&lt;h4&gt;方法&lt;/h4&gt;研究评估了多种深度学习分割模型，包括卷积神经网络（U-Net、DeepLabV3+）、视觉Transformer（SegFormer）以及最近的基础模型（SAM、MedSAM、MedSAM+UNet）。研究采用了贝叶斯搜索的广泛超参数优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;尽管进行了超参数优化，研究发现模型性能对数据分割高度敏感，微小的性能差异更多是由统计噪声而非真正的算法优势驱动的。&lt;h4&gt;结论&lt;/h4&gt;这种不稳定性暴露了标准基准测试实践在低数据临床环境中的局限性，并挑战了性能排名反映有意义临床效用的假设。&lt;h4&gt;翻译&lt;/h4&gt;准确分割心血管组织病理图像中的颈动脉结构对推进心血管疾病研究和诊断至关重要。然而，该领域深度学习模型的发展受到已标注的心血管组织病理数据稀缺性的限制。本研究评估了最先进的深度学习分割模型，包括卷积神经网络（U-Net、DeepLabV3+）、视觉Transformer（SegFormer）以及最近的基础模型（SAM、MedSAM、MedSAM+UNet）在有限的心血管组织病理图像数据集上的表现。尽管采用了贝叶斯搜索的广泛超参数优化策略，我们的研究结果显示模型性能对数据分割高度敏感，微小的性能差异更多是由统计噪声而非真正的算法优势驱动的。这种不稳定性暴露了标准基准测试实践在低数据临床环境中的局限性，并挑战了性能排名反映有意义临床效用的假设。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of carotid artery structures in histopathologicalimages is vital for advancing cardiovascular disease research and diagnosis.However, deep learning model development in this domain is constrained by thescarcity of annotated cardiovascular histopathological data. This studyinvestigates a systematic evaluation of state-of-the-art deep learningsegmentation models, including convolutional neural networks (U-Net,DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models(SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histologyimages. Despite employing an extensive hyperparameter optimization strategywith Bayesian search, our findings reveal that model performance is highlysensitive to data splits, with minor differences driven more by statisticalnoise than by true algorithmic superiority. This instability exposes thelimitations of standard benchmarking practices in low-data clinical settingsand challenges the assumption that performance rankings reflect meaningfulclinical utility.</description>
      <author>example@mail.com (Phongsakon Mark Konrad, Andrei-Alexandru Popa, Yaser Sabzehmeidani, Liang Zhong, Elisa A. Liehn, Serkan Ayvaz)</author>
      <guid isPermaLink="false">2509.05892v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2509.05801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究表明大型时间序列Transformer模型确实内部化了语义概念，而不仅仅是拟合曲线。通过激活移植技术，可以操纵模型内部表示来模拟不同市场状况，为理解和控制模型预测提供了新途径。&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的基础模型在预测常规模式方面表现出色，但有两个关键问题：这些模型是否内部化了市场状况等语义概念，还是仅仅拟合曲线？它们的内部表示是否可以被利用来模拟罕见的高风险事件如市场崩盘？&lt;h4&gt;目的&lt;/h4&gt;研究Transformer模型是否真正理解了时间序列数据中的语义概念，以及是否可以通过模型的内部表示来模拟罕见的高风险事件。&lt;h4&gt;方法&lt;/h4&gt;引入了激活移植方法，这是一种因果干预技术，通过在前向传播过程中将一个事件（如历史崩盘）的统计强加到另一个事件（如平静期）上来操纵隐藏状态，从而确定性地引导预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;注入崩盘语义会诱导下跌预测，注入平静语义会抑制崩盘并恢复稳定性；模型编码了事件严重性的分级概念，潜在向量范数与系统性冲击的幅度直接相关；这种可引导的、语义基础的表示是大型时间序列Transformer的稳健特性。&lt;h4&gt;结论&lt;/h4&gt;研究结果支持存在一个潜在的概念空间来控制模型预测，将可解释性从事后归因转向直接因果干预，并支持战略压力测试的语义化'假设分析'。&lt;h4&gt;翻译&lt;/h4&gt;虽然基于Transformer的基础模型在预测常规模式方面表现出色，但仍有两个问题：它们是否内部化了市场状况等语义概念，还是仅仅拟合曲线？它们的内部表示是否可以被利用来模拟罕见的高风险事件如市场崩盘？为了研究这一点，我们引入了激活移植，这是一种因果干预，通过在前向传播过程中将一个事件（如历史崩盘）的统计强加到另一个事件（如平静期）上来操纵隐藏状态。这个过程确定性地引导预测：注入崩盘语义会诱导下跌预测，而注入平静语义会抑制崩盘并恢复稳定性。除了二元控制外，我们发现模型编码了事件严重性的分级概念，其中潜在向量范数与系统性冲击的幅度直接相关。在两种架构不同的时间序列Transformer模型（Toto和Chronos）上得到验证，我们的结果表明可引导的、语义基础的表示是大型时间序列Transformer的稳健特性。我们的研究为控制模型预测的潜在概念空间提供了证据，将可解释性从事后归因转向直接因果干预，并支持战略压力测试的语义化'假设分析'。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While transformer-based foundation models excel at forecasting routinepatterns, two questions remain: do they internalize semantic concepts such asmarket regimes, or merely fit curves? And can their internal representations beleveraged to simulate rare, high-stakes events such as market crashes? Toinvestigate this, we introduce activation transplantation, a causalintervention that manipulates hidden states by imposing the statistical momentsof one event (e.g., a historical crash) onto another (e.g., a calm period)during the forward pass. This procedure deterministically steers forecasts:injecting crash semantics induces downturn predictions, while injecting calmsemantics suppresses crashes and restores stability. Beyond binary control, wefind that models encode a graded notion of event severity, with the latentvector norm directly correlating with the magnitude of systemic shocks.Validated across two architecturally distinct TSFMs, Toto (decoder only) andChronos (encoder-decoder), our results demonstrate that steerable, semanticallygrounded representations are a robust property of large time seriestransformers. Our findings provide evidence for a latent concept space thatgoverns model predictions, shifting interpretability from post-hoc attributionto direct causal intervention, and enabling semantic "what-if" analysis forstrategic stress-testing.</description>
      <author>example@mail.com (Debdeep Sanyal, Aaryan Nagpal, Dhruv Kumar, Murari Mandal, Saurabh Deshpande)</author>
      <guid isPermaLink="false">2509.05801v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>TPCpp-10M: Simulated proton-proton collisions in a Time Projection Chamber for AI Foundation Models</title>
      <link>http://arxiv.org/abs/2509.05792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个包含1000万次模拟质子-质子碰撞的大型开放数据集，支持基础模型的自我监督训练，包含70,000个标记示例用于三个下游任务的评估，旨在促进核物理和粒子物理领域的基础模型发展。&lt;h4&gt;背景&lt;/h4&gt;科学基础模型在核物理和粒子物理领域有很大潜力，但进展受限于缺乏大规模开放数据集、标准化评估任务和指标。此外，处理粒子物理学数据所需的专业知识和软件阻碍了与机器学习社区的跨学科合作。&lt;h4&gt;目的&lt;/h4&gt;引入一个大型开放数据集支持基础模型的自我监督训练，提供易于使用的数据集格式，并包含标记示例以评估基础模型的适应性。&lt;h4&gt;方法&lt;/h4&gt;使用Pythia蒙特卡洛事件生成器在200 GeV中心质心能量下模拟质子-质子碰撞数据，并用Geant4处理以包含真实的探测器条件和信号模拟，数据针对sPHENIX时间投影室设计，使用sPHENIX软件栈实现完整的模拟和重建链。&lt;h4&gt;主要发现&lt;/h4&gt;创建了包含1000万次模拟质子-质子碰撞的大型开放数据集，提供NumPy格式的数据集，包含70,000个标记示例涵盖三个下游任务：轨迹寻找、粒子识别和噪声标记。&lt;h4&gt;结论&lt;/h4&gt;该数据集资源为跨学科研究建立了共同基础，使机器学习科学家和物理学家能够探索扩展行为、评估可转移性，加速核物理和高能物理领域基础模型的进展。&lt;h4&gt;翻译&lt;/h4&gt;科学基础模型在推进核物理和粒子物理方面具有巨大潜力，通过提高分析精度和加速发现。然而，这一领域的进展通常受到缺乏公开可用的大规模数据集以及标准化评估任务和指标的限制。此外，处理粒子物理数据通常需要的专业知识和软件对与更广泛的机器学习社区进行跨学科合作构成了重大障碍。这项工作引入了一个包含1000万次模拟质子-质子碰撞的大型开放数据集，旨在支持基础模型的自我监督训练。为便于使用，数据集以通用的NumPy格式提供。此外，它包含70,000个标记示例，涵盖三个明确定义的下游任务：轨迹寻找、粒子识别和噪声标记，以实现对基础模型适应性的系统评估。模拟数据使用Pythia蒙特卡洛事件生成器在质心能量为200 GeV的条件下生成，并使用Geant4进行处理，以包含在布鲁克黑文国家实验室的相对论重离子对撞机上的sPHENIX时间投影室中的真实探测条件和信号模拟。该数据集资源为跨学科研究建立了共同基础，使机器学习科学家和物理学家都能够探索扩展行为、评估可转移性，并加速核物理和高能物理领域基础模型的进展。完整的模拟和重建链可以使用sPHENIX软件栈重现。所有数据和代码位置在数据可访问性部分提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific foundation models hold great promise for advancing nuclear andparticle physics by improving analysis precision and accelerating discovery.Yet, progress in this field is often limited by the lack of openly availablelarge scale datasets, as well as standardized evaluation tasks and metrics.Furthermore, the specialized knowledge and software typically required toprocess particle physics data pose significant barriers to interdisciplinarycollaboration with the broader machine learning community.  This work introduces a large, openly accessible dataset of 10 millionsimulated proton-proton collisions, designed to support self-supervisedtraining of foundation models. To facilitate ease of use, the dataset isprovided in a common NumPy format. In addition, it includes 70,000 labeledexamples spanning three well defined downstream tasks: track finding, particleidentification, and noise tagging, to enable systematic evaluation of thefoundation model's adaptability.  The simulated data are generated using the Pythia Monte Carlo event generatorat a center of mass energy of sqrt(s) = 200 GeV and processed with Geant4 toinclude realistic detector conditions and signal emulation in the sPHENIX TimeProjection Chamber at the Relativistic Heavy Ion Collider, located atBrookhaven National Laboratory.  This dataset resource establishes a common ground for interdisciplinaryresearch, enabling machine learning scientists and physicists alike to explorescaling behaviors, assess transferability, and accelerate progress towardfoundation models in nuclear and high energy physics. The complete simulationand reconstruction chain is reproducible with the sPHENIX software stack. Alldata and code locations are provided under Data Accessibility.</description>
      <author>example@mail.com (Shuhang Li, Yi Huang, David Park, Xihaier Luo, Haiwang Yu, Yeonju Go, Christopher Pinkenburg, Yuewei Lin, Shinjae Yoo, Joseph Osborn, Christof Roland, Jin Huang, Yihui Ren)</author>
      <guid isPermaLink="false">2509.05792v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian</title>
      <link>http://arxiv.org/abs/2509.05668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Michael Hoffmann and Jophin John contributed equally to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Llama-GENBA-10B是一个三语种基础模型，旨在解决大型语言模型中的英语中心主义偏见。该模型基于Llama 3.1-8B构建并扩展至100亿参数，在平衡资源分配的同时防止英语主导，针对德语NLP社区，同时促进巴伐利亚语作为低资源语言的发展。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型普遍存在英语中心主义偏见，大多数模型主要在英语数据上进行训练，导致对其他语言的支持不足。特别是对于像巴伐利亚语这样的低资源语言，缺乏有效的模型支持。&lt;h4&gt;目的&lt;/h4&gt;开发一个平衡英语、德语和巴伐利亚语的三语种基础模型，解决英语中心主义偏见，促进低资源语言巴伐利亚语的发展，并为德语NLP社区提供有力工具。&lt;h4&gt;方法&lt;/h4&gt;基于Llama 3.1-8B构建并扩展至100亿参数；在164B个标记上进行持续预训练（820亿英语，820亿德语和800万巴伐利亚语）；解决四个主要挑战：策划多语言语料库、创建统一分词器、优化架构和语言比例超参数、建立标准化三语种评估套件；使用Cerebras CS-2进行训练展示大规模多语言预训练效率。&lt;h4&gt;主要发现&lt;/h4&gt;Llama-GENBA-10B实现了强大的跨语言性能；微调后的变体在巴伐利亚语上超越了Apertus-8B-2509和gemma-2-9b，成为该语言同类中的最佳模型；在英语上表现优于EuroLLM，在德语上与EuroLLM结果相当；在Cerebras CS-2上的训练展示了大规模多语言预训练的高效性。&lt;h4&gt;结论&lt;/h4&gt;Llama-GENBA-10B为整合低资源语言的包容性基础模型提供了蓝图，证明了在平衡多语言资源分配的同时防止英语主导的可行性，为促进语言多样性和包容性AI发展做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Llama-GENBA-10B，一个三语种基础模型，旨在解决大型语言模型中的英语中心主义偏见。该模型基于Llama 3.1-8B构建并扩展至100亿参数，在164B个标记上持续预训练（820亿英语，820亿德语和800万巴伐利亚语），平衡资源分配同时防止英语主导。针对德语NLP社区，该模型还促进巴伐利亚语作为低资源语言的发展。开发过程中解决了四个挑战：(1) 尽管巴伐利亚语资源稀缺，仍策划多语言语料库，(2) 为英语、德语和巴伐利亚语创建统一的分词器，(3) 优化架构和语言比例超参数以实现跨语言迁移，(4) 通过将德语基准测试翻译成巴伐利亚语，建立首个标准化三语种评估套件。评估显示，Llama-GENBA-10B实现了强大的跨语言性能，微调后的变体在巴伐利亚语上超越了Apertus-8B-2509和gemma-2-9b，成为该语言同类中的最佳模型，同时在英语上表现优于EuroLLM，在德语上与EuroLLM结果相当。在Cerebras CS-2上的训练展示了大规模多语言预训练的高效性，并记录了能源使用情况，为整合低资源语言的包容性基础模型提供了蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Llama-GENBA-10B, a trilingual foundation model addressingEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaledto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens(82B English, 82B German, and 80M Bavarian), balancing resources whilepreventing English dominance. Targeted at the German NLP community, the modelalso promotes Bavarian as a low-resource language. Development tackled fourchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)creating a unified tokenizer for English, German, and Bavarian, (3) optimizingarchitecture and language-ratio hyperparameters for cross-lingual transfer, and(4) establishing the first standardized trilingual evaluation suite bytranslating German benchmarks into Bavarian. Evaluations show thatLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tunedvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishingitself as the best model in its class for this language, while alsooutperforming EuroLLM in English and matching its results in German. Trainingon the Cerebras CS-2 demonstrated efficient large-scale multilingualpretraining with documented energy use, offering a blueprint for inclusivefoundation models that integrate low-resource languages.</description>
      <author>example@mail.com (Michael Hoffmann, Jophin John, Stefan Schweter, Gokul Ramakrishnan, Hoi-Fong Mak, Alice Zhang, Dmitry Gaynullin, Nicolay J. Hammer)</author>
      <guid isPermaLink="false">2509.05668v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization</title>
      <link>http://arxiv.org/abs/2509.05584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures, 5 tables, 1 algorithm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ProfilingAgent的智能体方法，利用大型语言模型和性能分析技术来自动化模型压缩过程，解决了基础模型在资源受限平台上部署的计算和内存瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型面临日益严重的计算和内存瓶颈，限制了在资源有限平台上的部署。现有的压缩技术如剪枝和量化大多依赖均匀启发式方法，忽略了架构和运行时异质性。性能分析工具虽能揭示各层性能数据，但很少集成到自动化流程中。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于性能分析、智能体引导的自动化压缩方法，利用大型语言模型实现结构化剪枝和训练后动态量化，为不同架构模型定制优化策略。&lt;h4&gt;方法&lt;/h4&gt;提出ProfilingAgent，一个模块化多智能体系统，该系统结合静态指标（MACs，参数计数）和动态信号（延迟，内存）进行推理，为特定架构设计优化策略，针对瓶颈进行逐层决策，而非使用均匀启发式方法。&lt;h4&gt;主要发现&lt;/h4&gt;在ImageNet-1K、CIFAR-10和CIFAR-100数据集上使用ResNet-101、ViT-B/16、Swin-B和DeiT-B/16的实验表明：剪枝保持了竞争性或改进的精度（ImageNet-1K上约1%下降，ViT-B/16在较小数据集上+2%增益）；量化实现了高达74%的内存节省，精度损失&lt;0.5%；量化还带来了高达1.74倍的推理加速。与GPT-4o和GPT-4-Turbo的比较研究强调了LLM推理质量对迭代剪枝的重要性。&lt;h4&gt;结论&lt;/h4&gt;研究结果确立了智能体系统作为性能分析引导模型优化的可扩展解决方案，能够有效解决模型在资源受限环境中的部署问题。&lt;h4&gt;翻译&lt;/h4&gt;基础模型面临日益增长的计算和内存瓶颈，阻碍了在资源有限平台上的部署。虽然剪枝和量化等压缩技术被广泛使用，但大多数依赖均匀启发式方法，忽略了架构和运行时异质性。性能分析工具可以揭示各层的延迟、内存和计算成本，但很少集成到自动化流程中。我们提出ProfilingAgent，这是一种基于性能分析、智能体引导的方法，使用大型语言模型通过结构化剪枝和训练后动态量化来自动化压缩。我们的模块化多智能体系统基于静态指标（MACs，参数计数）和动态信号（延迟，内存）进行推理，以设计特定架构的策略。与启发式基线不同，ProfilingAgent针对瓶颈进行逐层决策。在ImageNet-1K、CIFAR-10和CIFAR-100上使用ResNet-101、ViT-B/16、Swin-B和DeiT-B/16进行的实验表明，剪枝保持了竞争性或改进的精度（ImageNet-1上约1%的下降，ViT-B/16在较小数据集上+2%的增益），而量化实现了高达74%的内存节省，精度损失&lt;0.5%。我们的量化还带来了高达1.74倍的持续推理加速。与GPT-4o和GPT-4-Turbo的比较研究突显了LLM推理质量对迭代剪枝的重要性。这些结果确立了智能体系统作为性能分析引导模型优化的可扩展解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models face growing compute and memory bottlenecks, hinderingdeployment on resource-limited platforms. While compression techniques such aspruning and quantization are widely used, most rely on uniform heuristics thatignore architectural and runtime heterogeneity. Profiling tools exposeper-layer latency, memory, and compute cost, yet are rarely integrated intoautomated pipelines. We propose ProfilingAgent, a profiling-guided, agenticapproach that uses large language models (LLMs) to automate compression viastructured pruning and post-training dynamic quantization. Our modularmulti-agent system reasons over static metrics (MACs, parameter counts) anddynamic signals (latency, memory) to design architecture-specific strategies.Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions tobottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 withResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitiveor improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 onsmaller datasets), while quantization achieves up to 74% memory savings with&lt;0.5% accuracy loss. Our quantization also yields consistent inference speedupsof up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbohighlight the importance of LLM reasoning quality for iterative pruning. Theseresults establish agentic systems as scalable solutions for profiling-guidedmodel optimization.</description>
      <author>example@mail.com (Sadegh Jafari, Aishwarya Sarkar, Mohiuddin Bilwal, Ali Jannesari)</author>
      <guid isPermaLink="false">2509.05584v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations</title>
      <link>http://arxiv.org/abs/2509.05186v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出概率框架揭示ICON隐式执行贝叶斯推理，并扩展其到生成式设置(GenICON)，实现解预测中的不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;ICON是一类基于基础模型新架构的算子学习方法，在多样化的初始和边界条件数据集（与常微分方程和偏微分方程的解配对）上进行训练。&lt;h4&gt;目的&lt;/h4&gt;揭示ICON作为隐式贝叶斯推理方法的本质，并扩展其到生成式设置以实现不确定性量化。&lt;h4&gt;方法&lt;/h4&gt;通过随机微分方程形式提供概率框架，将ICON扩展为生成式形式(GenICON)，使其能够从解算子的后验预测分布中进行采样。&lt;h4&gt;主要发现&lt;/h4&gt;1. ICON隐式执行贝叶斯推理，计算基于上下文的解算子后验预测分布均值；2. 随机微分方程形式为描述ICON任务提供了概率框架；3. GenICON能够捕获解算子的潜在不确定性。&lt;h4&gt;结论&lt;/h4&gt;ICON的概率视角为算子学习中的解预测提供了有原则的不确定性量化基础，提高了其可靠性并扩展了应用范围。&lt;h4&gt;翻译&lt;/h4&gt;上下文算子网络(ICON)是一类基于基础模型新架构的算子学习方法。在多样化的初始和边界条件数据集（与常微分方程和偏微分方程的解配对）上进行训练，ICON学习将给定微分方程的条件-解示例对映射到其解算子的近似。在此，我们提出一个概率框架，揭示ICON隐式执行贝叶斯推理，计算基于提供上下文（即条件-解示例对）的解算子后验预测分布的均值。随机微分方程的形式为描述ICON完成的任务提供了概率框架，也为理解其他多算子学习方法提供了基础。这种概率视角为ICON扩展到生成式设置提供了基础，可以从解算子的后验预测分布中进行采样。ICON的生成式形式(GenICON)捕获了解算子的潜在不确定性，使算子学习中的解预测能够进行有原则的不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In-context operator networks (ICON) are a class of operator learning methodsbased on the novel architectures of foundation models. Trained on a diverse setof datasets of initial and boundary conditions paired with correspondingsolutions to ordinary and partial differential equations (ODEs and PDEs), ICONlearns to map example condition-solution pairs of a given differential equationto an approximation of its solution operator. Here, we present a probabilisticframework that reveals ICON as implicitly performing Bayesian inference, whereit computes the mean of the posterior predictive distribution over solutionoperators conditioned on the provided context, i.e., example condition-solutionpairs. The formalism of random differential equations provides theprobabilistic framework for describing the tasks ICON accomplishes while alsoproviding a basis for understanding other multi-operator learning methods. Thisprobabilistic perspective provides a basis for extending ICON to\emph{generative} settings, where one can sample from the posterior predictivedistribution of solution operators. The generative formulation of ICON(GenICON) captures the underlying uncertainty in the solution operator, whichenables principled uncertainty quantification in the solution predictions inoperator learning.</description>
      <author>example@mail.com (Benjamin J. Zhang, Siting Liu, Stanley J. Osher, Markos A. Katsoulakis)</author>
      <guid isPermaLink="false">2509.05186v2</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.06826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于对比学习的视频分类方法，用于MPAA评级系统的自动分类，通过结合CNN、LSTM和注意力机制，实现了88%的准确率和0.8815的F1分数。&lt;h4&gt;背景&lt;/h4&gt;各平台视觉内容消费快速增长，需要自动化视频分类来满足MPAA分级系统(G, PG, PG-13, R)等年龄适宜性标准，而传统方法存在大量标记数据需求、泛化能力差和特征学习效率低等问题。&lt;h4&gt;目的&lt;/h4&gt;解决传统视频分类方法的局限性，提高视频分类的准确性和效率，实现对MPAA等级的自动分类。&lt;h4&gt;方法&lt;/h4&gt;采用对比学习提高区分能力和适应性，探索实例判别、上下文对比学习和多视图对比学习三种框架；使用混合架构，结合LRCN(CNN+LSTM)主干和Bahdanau注意力机制；评估了NT-Xent、NT-logistic和Margin Triplet等多种对比损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;在上下文对比学习框架下实现了最先进的性能，准确率达到88%，F1分数为0.8815；结合CNN的空间特征、LSTM的时间建模和注意力机制的动态帧优先级选择，模型在细粒度边界区分方面表现出色；架构在各种对比损失函数下表现出鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;模型已部署为Web应用程序，用于实时MPAA评级分类，为流媒体平台自动化内容合规性提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;各平台视觉内容消费的快速增长需要对MPAA评级系统(如G, PG, PG-13, R等级)的年龄适宜性标准进行自动化视频分类。传统方法难以满足大量标记数据需求、泛化能力差和特征学习效率低等挑战。为解决这些问题，我们采用对比学习来提高区分能力和适应性，探索了三种框架：实例判别、上下文对比学习和多视图对比学习。我们的混合架构集成了LRCN(CNN+LSTM)主干和Bahdanau注意力机制，在上下文对比学习框架下实现了最先进的性能，准确率达88%，F1分数为0.8815。通过结合CNN的空间特征、LSTM的时间建模和注意力机制的动态帧优先级选择，模型在细粒度边界区分方面表现出色，例如区分PG-13和R级内容。我们评估了模型在各种对比损失函数(包括NT-Xent、NT-logistic和Margin Triplet)上的性能，证明了我们提出架构的鲁棒性。为确保实际应用，该模型已部署为Web应用程序，用于实时MPAA评级分类，为流媒体平台自动化内容合规性提供了高效解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of visual content consumption across platforms necessitatesautomated video classification for age-suitability standards like the MPAArating system (G, PG, PG-13, R). Traditional methods struggle with largelabeled data requirements, poor generalization, and inefficient featurelearning. To address these challenges, we employ contrastive learning forimproved discrimination and adaptability, exploring three frameworks: InstanceDiscrimination, Contextual Contrastive Learning, and Multi-View ContrastiveLearning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with aBahdanau attention mechanism, achieving state-of-the-art performance in theContextual Contrastive Learning framework, with 88% accuracy and an F1 score of0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling,and attention mechanisms for dynamic frame prioritization, the model excels infine-grained borderline distinctions, such as differentiating PG-13 and R-ratedcontent. We evaluate the model's performance across various contrastive lossfunctions, including NT-Xent, NT-logistic, and Margin Triplet, demonstratingthe robustness of our proposed architecture. To ensure practical application,the model is deployed as a web application for real-time MPAA ratingclassification, offering an efficient solution for automated content complianceacross streaming platforms.</description>
      <author>example@mail.com (Dipta Neogi, Nourash Azmine Chowdhury, Muhammad Rafsan Kabir, Mohammad Ashrafuzzaman Khan)</author>
      <guid isPermaLink="false">2509.06826v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>BEAM: Brainwave Empathy Assessment Model for Early Childhood</title>
      <link>http://arxiv.org/abs/2509.06620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为BEAM的新型深度学习框架，用于客观评估4-6岁儿童的共情能力，通过多视图EEG信号捕捉共情的认知和情感维度。&lt;h4&gt;背景&lt;/h4&gt;儿童共情能力对其社交和情感发展至关重要，但预测共情能力具有挑战性。传统方法依赖自我报告或观察者标记，易受偏见影响，无法客观捕捉共情形成过程。EEG虽提供了客观替代方案，但当前方法主要提取静态模式，忽略了时间动态性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架，即脑波共情评估模型（BEAM），用于预测4-6岁儿童的共情水平。&lt;h4&gt;方法&lt;/h4&gt;BEAM利用多视图EEG信号捕捉共情的认知和情感维度，包含三个关键组件：1) 基于LaBraM的编码器用于时空特征提取，2) 特征融合模块整合多视图信号的互补信息，3) 对比学习模块增强类间分离。&lt;h4&gt;主要发现&lt;/h4&gt;在CBCP数据集上验证，BEAM在多个指标上优于最先进的方法，展示了客观共情评估的潜力。&lt;h4&gt;结论&lt;/h4&gt;BEAM为儿童亲社会发展的早期干预提供了初步见解，有望实现客观共情评估。&lt;h4&gt;翻译&lt;/h4&gt;儿童共情能力对其社交和情感发展至关重要，但预测共情能力仍然具有挑战性。传统方法通常仅依赖自我报告或观察者标记，这些方法容易受到偏见影响，无法客观捕捉共情形成过程。EEG提供了客观替代方案；然而，当前方法主要提取静态模式，忽略了时间动态性。为克服这些局限，我们提出了一种新型深度学习框架——脑波共情评估模型（BEAM），用于预测4-6岁儿童的共情水平。BEAM利用多视图EEG信号捕捉共情的认知和情感维度。该框架包含三个关键组件：1) 基于LaBraM的编码器，用于有效的时空特征提取；2) 特征融合模块，用于整合多视图信号的互补信息；3) 对比学习模块，用于增强类间分离。在CBCP数据集上验证，BEAM在多个指标上优于最先进的方法，展示了其客观共情评估的潜力，并为儿童亲社会发展的早期干预提供了初步见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Empathy in young children is crucial for their social and emotionaldevelopment, yet predicting it remains challenging. Traditional methods oftenonly rely on self-reports or observer-based labeling, which are susceptible tobias and fail to objectively capture the process of empathy formation. EEGoffers an objective alternative; however, current approaches primarily extractstatic patterns, neglecting temporal dynamics. To overcome these limitations,we propose a novel deep learning framework, the Brainwave Empathy AssessmentModel (BEAM), to predict empathy levels in children aged 4-6 years. BEAMleverages multi-view EEG signals to capture both cognitive and emotionaldimensions of empathy. The framework comprises three key components: 1) aLaBraM-based encoder for effective spatio-temporal feature extraction, 2) afeature fusion module to integrate complementary information from multi-viewsignals, and 3) a contrastive learning module to enhance class separation.Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods acrossmultiple metrics, demonstrating its potential for objective empathy assessmentand providing a preliminary insight into early interventions in children'sprosocial development.</description>
      <author>example@mail.com (Chen Xie, Gaofeng Wu, Kaidong Wang, Zihao Zhu, Xiaoshu Luo, Yan Liang, Feiyu Quan, Ruoxi Wu, Xianghui Huang, Han Zhang)</author>
      <guid isPermaLink="false">2509.06620v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs</title>
      <link>http://arxiv.org/abs/2509.06550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in: Proceedings of IEEE Conference on Cyber Security and  Resilience (CSR), 2025. Official version:  https://doi.org/10.1109/CSR64739.2025.11129979 Code:  https://github.com/jackwilkie/CLAN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLAN的新型网络入侵检测方法，通过对比学习使用增强负样本对，在良性流量预训练后提高了分类精度和推理效率，在二元和多类分类任务中均优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;网络入侵检测是网络安全的关键挑战。监督式机器学习虽性能优异但依赖大型标记数据集，不切实际；异常检测仅用良性流量训练但误报率高；现有自监督方法通过学习良性流量表示改进性能但仍有限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型网络入侵检测方法，解决现有自监督和异常检测方法的局限性，提高检测精度并降低误报率。&lt;h4&gt;方法&lt;/h4&gt;提出CLAN（使用增强负样本对的对比学习）范式，将增强样本视为负视图（代表潜在恶意分布），其他良性样本作为正视图，通过对比学习学习良性流量的判别性表示。&lt;h4&gt;主要发现&lt;/h4&gt;在Lycos2017数据集上，CLAN在二元分类任务中优于现有自监督和异常检测技术；在有限标记数据集上微调后，多类分类性能也优于现有自监督模型。&lt;h4&gt;结论&lt;/h4&gt;CLAN方法通过创新的负样本处理方式，有效提升了网络入侵检测的性能和效率，是一种实用且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;网络入侵检测仍然是网络安全中的一个关键挑战。虽然监督式机器学习模型实现了最先进的性能，但它们对大型标记数据集的依赖使得它们在许多实际应用中不切实际。仅对良性流量进行训练以识别恶意活动的异常检测方法，误报率高，限制了其可用性。最近，自监督学习技术通过学习良性流量的判别性潜在表示，展示了改进的性能和更低的误报率。特别是，对比自监督模型通过最小化良性流量相似（正）视图之间的距离，同时最大化不同（负）视图之间的距离来实现这一点。现有方法通过数据增强生成正视图，并将其他样本视为负视图。相比之下，这项工作引入了使用增强负样本对的对比学习（CLAN），这是一种用于网络入侵检测的新范式，其中增强样本被视为负视图 - 代表潜在的恶意分布 - 而其他良性样本则作为正视图。这种方法在良性流量上进行预训练后，提高了分类精度和推理效率。在Lycos2017数据集上的实验评估表明，所提出的方法在二元分类任务中优于现有的自监督和异常检测技术。此外，当在有限的标记数据集上进行微调时，所提出的方法在多类分类任务中实现了比现有自监督模型更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CSR64739.2025.11129979&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network intrusion detection remains a critical challenge in cybersecurity.While supervised machine learning models achieve state-of-the-art performance,their reliance on large labelled datasets makes them impractical for manyreal-world applications. Anomaly detection methods, which train exclusively onbenign traffic to identify malicious activity, suffer from high false positiverates, limiting their usability. Recently, self-supervised learning techniqueshave demonstrated improved performance with lower false positive rates bylearning discriminative latent representations of benign traffic. Inparticular, contrastive self-supervised models achieve this by minimizing thedistance between similar (positive) views of benign traffic while maximizing itbetween dissimilar (negative) views. Existing approaches generate positiveviews through data augmentation and treat other samples as negative. Incontrast, this work introduces Contrastive Learning using Augmented Negativepairs (CLAN), a novel paradigm for network intrusion detection where augmentedsamples are treated as negative views - representing potentially maliciousdistributions - while other benign samples serve as positive views. Thisapproach enhances both classification accuracy and inference efficiency afterpretraining on benign traffic. Experimental evaluation on the Lycos2017 datasetdemonstrates that the proposed method surpasses existing self-supervised andanomaly detection techniques in a binary classification task. Furthermore, whenfine-tuned on a limited labelled dataset, the proposed approach achievessuperior multi-class classification performance compared to existingself-supervised models.</description>
      <author>example@mail.com (Jack Wilkie, Hanan Hindy, Christos Tachtatzis, Robert Atkinson)</author>
      <guid isPermaLink="false">2509.06550v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion</title>
      <link>http://arxiv.org/abs/2509.06531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP Findings 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了SLiNT框架，通过将知识图谱结构信息注入到冻结的大型语言模型中，解决了LLM在知识图谱链接预测中结构信号利用不足的问题，实现了在不完整或零样本设置下的稳健预测。&lt;h4&gt;背景&lt;/h4&gt;知识图谱链接预测需要整合结构信息和语义上下文来推断缺失实体。大型语言模型虽有强大生成推理能力，但对结构信号利用有限，导致结构稀疏性和语义模糊性问题，尤其在不完整或少样本场景下更为突出。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用结构信息的框架，解决大型语言模型在知识图谱链接预测中的局限性，提高在不完整或零样本设置下的预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出SLiNT框架，将知识图谱衍生的结构上下文注入冻结的LLM主干中，使用LoRA进行轻量级适应。包含三个核心组件：结构引导的邻域增强(SGNE)检索伪邻居丰富稀疏实体；动态难对比学习(DHCL)通过插值难正负样本引入细粒度监督；梯度解耦双注入(GDDI)执行标记级别结构感知干预同时保留核心LLM参数。&lt;h4&gt;主要发现&lt;/h4&gt;在WN18RR和FB15k-237数据集上的实验表明，SLiNT与基于嵌入和基于生成的基线相比实现了优越或具有竞争力的性能，验证了结构感知表示学习对可扩展知识图谱补全的有效性。&lt;h4&gt;结论&lt;/h4&gt;SLiNT框架通过结构感知的学习方法，成功解决了大型语言模型在知识图谱链接预测中的局限性，提高了在不完整或零样本设置下的预测性能。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱中的链接预测需要整合结构信息和语义上下文来推断缺失的实体。虽然大型语言模型提供了强大的生成推理能力，但它们对结构信号的有限利用通常会导致结构稀疏性和语义模糊性，特别是在不完整或少样本设置下。为解决这些挑战，我们提出了SLiNT（结构感知语言模型，具有注入和对比训练），这是一个模块化框架，它将知识图谱衍生的结构上下文注入到冻结的LLM主干中，并使用基于LoRA的轻量级适应进行稳健的链接预测。具体而言，结构引导的邻域增强(SGNE)检索伪邻居来丰富稀疏实体并缓解缺失上下文；动态难对比学习(DHCL)通过插值难正样本和难负样本引入细粒度监督，以解决实体级别的模糊性；梯度解耦双注入(GDDI)在保留核心LLM参数的同时执行标记级别的结构感知干预。在WN18RR和FB15k-237上的实验表明，与基于嵌入和基于生成的基线相比，SLiNT实现了优越或具有竞争力的性能，证明了结构感知表示学习对可扩展知识图谱补全的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction in knowledge graphs requires integrating structuralinformation and semantic context to infer missing entities. While largelanguage models offer strong generative reasoning capabilities, their limitedexploitation of structural signals often results in structural sparsity andsemantic ambiguity, especially under incomplete or zero-shot settings. Toaddress these challenges, we propose SLiNT (Structure-aware Language model withInjection and coNtrastive Training), a modular framework that injectsknowledge-graph-derived structural context into a frozen LLM backbone withlightweight LoRA-based adaptation for robust link prediction. Specifically,Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors toenrich sparse entities and mitigate missing context; Dynamic Hard ContrastiveLearning (DHCL) introduces fine-grained supervision by interpolating hardpositives and negatives to resolve entity-level ambiguity; andGradient-Decoupled Dual Injection (GDDI) performs token-level structure-awareintervention while preserving the core LLM parameters. Experiments on WN18RRand FB15k-237 show that SLiNT achieves superior or competitive performancecompared with both embedding-based and generation-based baselines,demonstrating the effectiveness of structure-aware representation learning forscalable knowledge graph completion.</description>
      <author>example@mail.com (Mengxue Yang, Chun Yang, Jiaqi Zhu, Jiafan Li, Jingqi Zhang, Yuyang Li, Ying Li)</author>
      <guid isPermaLink="false">2509.06531v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CAME-AB，一种具有专家混合主干的新型跨模态注意力框架，用于抗体结合位点预测。该方法整合了五种生物学基础模态，并通过自适应模态融合和监督对比学习提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的基于序列或结构的方法依赖于单一视图特征，无法识别抗原上的抗体特异性结合位点，这在表示和预测方面存在双重限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服现有方法局限性的新型抗体结合位点预测方法，通过多模态特征融合和自适应推理提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出CAME-AB框架，整合五种生物学基础模态（原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图），采用自适应模态融合模块动态加权每种模态，结合Transformer编码器和MoE模块促进特征专业化，并通过监督对比学习塑造潜在空间几何结构。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的实验表明，CAME-AB在精确率、召回率、F1分数、AUC-ROC和MCC等多个指标上持续优于强基线方法。消融研究验证了各架构组件的有效性和多模态特征集成的优势。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过多模态特征融合和自适应推理成功克服了现有抗体结合位点预测方法的局限性，实现了更高的预测准确性。模型实现细节和代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单一视图特征，无法识别抗原上的抗体特异性结合位点——这是表示和预测方面的双重限制。在本文中，我们提出了CAME-AB，一种具有专家混合主干的新型跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB整合了五种生物学基础模态，包括原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图，形成统一的 multimodal 表示。为了增强自适应跨模态推理，我们提出了一个自适应模态融合模块，该模块学习根据全局相关性和输入特定贡献动态加权每种模态。Transformer编码器与MoE模块相结合进一步促进了特征专业化和能力扩展。我们还结合了监督对比学习目标，明确塑造潜在空间几何结构，鼓励类内紧凑性和类间可分性。为了提高优化稳定性和泛化能力，我们在训练期间应用随机权重平均。在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在多个指标上持续优于强基线方法，包括精确率、召回率、F1分数、AUC-ROC和MCC。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。模型实现细节和代码可在 https://anonymous.4open.science/r/CAME-AB-C525 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens-a dual limitation in representation andprediction. In this paper, we propose CAME-AB, a novel Cross-modality Attentionframework with a Mixture-of-Experts (MoE) backbone for robust antibody bindingsite prediction. CAME-AB integrates five biologically grounded modalities,including raw amino acid encodings, BLOSUM substitution profiles, pretrainedlanguage model embeddings, structure-aware features, and GCN-refinedbiochemical graphs-into a unified multimodal representation. To enhanceadaptive cross-modal reasoning, we propose an adaptive modality fusion modulethat learns to dynamically weight each modality based on its global relevanceand input-specific contribution. A Transformer encoder combined with an MoEmodule further promotes feature specialization and capacity expansion. Weadditionally incorporate a supervised contrastive learning objective toexplicitly shape the latent space geometry, encouraging intra-class compactnessand inter-class separability. To improve optimization stability andgeneralization, we apply stochastic weight averaging during training. Extensiveexperiments on benchmark antibody-antigen datasets demonstrate that CAME-ABconsistently outperforms strong baselines on multiple metrics, includingPrecision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies furthervalidate the effectiveness of each architectural component and the benefit ofmultimodal feature integration. The model implementation details and the codesare available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.06306v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的Memory-guided Consistency-aware Contrastive Learning (MCCL)框架，用于解决视频领域的广义类别发现(Video-GCD)问题。该方法通过整合时空信息，显著提升了在视频中发现新类别的性能。&lt;h4&gt;背景&lt;/h4&gt;广义类别发现(GCD)是一个新兴且具有挑战性的开放世界问题。现有方法主要关注静态图像中的类别发现，但仅依靠静态视觉内容往往不足以可靠地发现新类别。&lt;h4&gt;目的&lt;/h4&gt;将GCD问题扩展到视频领域，引入Video-GCD新设置，有效整合时间上的多视角信息，以准确发现视频中的新类别。&lt;h4&gt;方法&lt;/h4&gt;提出MCCL框架，包含两个核心组件：1)一致性感知对比学习(CACL)，利用多视角时间特征估计未标记实例间的一致性分数并加权对比损失；2)记忆引导的表示增强(MGRE)，通过双重级别内存缓冲区提供全局上下文，增强类内紧凑性和类间可分离性。两者形成相互强化的反馈循环。&lt;h4&gt;主要发现&lt;/h4&gt;构建了新的Video-GCD基准数据集，包括动作识别和鸟类分类视频数据集。实验表明，该方法显著优于从基于图像设置调整的竞争性GCD方法，证明了时间信息对发现视频新类别的重要性。&lt;h4&gt;结论&lt;/h4&gt;通过整合时间信息，有效解决了视频中类别发现的挑战。研究强调了时间信息在视频新类别发现中的关键作用，并将公开代码。&lt;h4&gt;翻译&lt;/h4&gt;广义类别发现(GCD)是一个新兴的、具有挑战性的开放世界问题，近年来受到越来越多的关注。大多数现有的GCD方法专注于在静态图像中发现类别。然而，仅依靠静态视觉内容通常不足以可靠地发现新类别。为了弥补这一差距，我们将GCD问题扩展到视频领域，引入了一种新的设置，称为Video-GCD。因此，有效整合时间上的多视角信息对于准确的Video-GCD至关重要。为了应对这一挑战，我们提出了一种新颖的记忆引导的一致性感知对比学习(MCCL)框架，该框架明确捕获时空线索，并通过一致性引导的投票机制将其整合到对比学习中。MCCL包含两个核心组件：一致性感知对比学习(CACL)和记忆引导的表示增强(MGRE)。CACL利用多视角时间特征来估计未标记实例之间的一致性分数，然后相应地加权对比损失。MGRE引入了一个双重级别的内存缓冲区，保持特征级别和logit级别的表示，提供全局上下文以增强类内紧凑性和类间可分离性。这反过来又完善了CACL中的一致性估计，形成了表示学习和一致性建模之间的相互强化的反馈循环。为了促进全面评估，我们构建了一个新的、具有挑战性的Video-GCD基准，包括动作识别和鸟类分类视频数据集。大量实验表明，我们的方法显著优于从基于图像设置调整的竞争性GCD方法，强调了时间信息对于在视频中发现新类别的重要性。代码将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized Category Discovery (GCD) is an emerging and challengingopen-world problem that has garnered increasing attention in recent years. Mostexisting GCD methods focus on discovering categories in static images. However,relying solely on static visual content is often insufficient to reliablydiscover novel categories. To bridge this gap, we extend the GCD problem to thevideo domain and introduce a new setting, termed Video-GCD. Thus, effectivelyintegrating multi-perspective information across time is crucial for accurateVideo-GCD. To tackle this challenge, we propose a novel Memory-guidedConsistency-aware Contrastive Learning (MCCL) framework, which explicitlycaptures temporal-spatial cues and incorporates them into contrastive learningthrough a consistency-guided voting mechanism. MCCL consists of two corecomponents: Consistency-Aware Contrastive Learning(CACL) and Memory-GuidedRepresentation Enhancement (MGRE). CACL exploits multiperspective temporalfeatures to estimate consistency scores between unlabeled instances, which arethen used to weight the contrastive loss accordingly. MGRE introduces adual-level memory buffer that maintains both feature-level and logit-levelrepresentations, providing global context to enhance intra-class compactnessand inter-class separability. This in turn refines the consistency estimationin CACL, forming a mutually reinforcing feedback loop between representationlearning and consistency modeling. To facilitate a comprehensive evaluation, weconstruct a new and challenging Video-GCD benchmark, which includes actionrecognition and bird classification video datasets. Extensive experimentsdemonstrate that our method significantly outperforms competitive GCDapproaches adapted from image-based settings, highlighting the importance oftemporal information for discovering novel categories in videos. The code willbe publicly available.</description>
      <author>example@mail.com (Zhang Jing, Pu Nan, Xie Yu Xiang, Guo Yanming, Lu Qianqi, Zou Shiwei, Yan Jie, Chen Yan)</author>
      <guid isPermaLink="false">2509.06306v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology</title>
      <link>http://arxiv.org/abs/2509.06105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accept by EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的基准测试PathoHR-Bench和病理特定的视觉语言训练方案，解决了现有模型在病理图像分析中的局限性，提高了层次语义理解和组合推理能力。&lt;h4&gt;背景&lt;/h4&gt;病理图像的准确分析对自动化肿瘤诊断至关重要，但由于组织图像中高度的结构相似性和细微的形态学变化，这仍然具有挑战性。当前的视觉语言模型往往难以捕捉解释结构化病理报告所需的复杂推理能力。&lt;h4&gt;目的&lt;/h4&gt;提出PathoHR-Bench，一个新的基准测试，用于评估视觉语言模型在病理领域中的层次语义理解和组合推理能力。&lt;h4&gt;方法&lt;/h4&gt;引入了一种病理特定的视觉语言训练方案，该方案生成增强和扰动的样本用于多模态对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;现有的视觉语言模型无法有效建模复杂的跨模态关系，限制了它们在临床环境中的应用；该方法在PathoHR-Bench和另外六个病理数据集上取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在细粒度病理表示方面是有效的，能够提高视觉语言模型在病理图像分析中的表现。&lt;h4&gt;翻译&lt;/h4&gt;病理图像的准确分析对自动化肿瘤诊断至关重要，但由于组织图像中高度的结构相似性和细微的形态学变化，这仍然具有挑战性。当前的视觉语言模型往往难以捕捉解释结构化病理报告所需的复杂推理能力。为了解决这些局限性，我们提出了PathoHR-Bench，这是一个新的基准测试，旨在评估视觉语言模型在病理领域中的层次语义理解和组合推理能力。该基准测试的结果表明，现有的视觉语言模型无法有效建模复杂的跨模态关系，从而限制了它们在临床环境中的应用。为了克服这一点，我们进一步引入了一种病理特定的视觉语言训练方案，该方案生成增强和扰动的样本用于多模态对比学习。实验评估表明，我们的方法在PathoHR-Bench和另外六个病理数据集上取得了最先进的性能，突显了它在细粒度病理表示方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate analysis of pathological images is essential for automated tumordiagnosis but remains challenging due to high structural similarity and subtlemorphological variations in tissue images. Current vision-language (VL) modelsoften struggle to capture the complex reasoning required for interpretingstructured pathological reports. To address these limitations, we proposePathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities inhierarchical semantic understanding and compositional reasoning within thepathology domain. Results of this benchmark reveal that existing VL models failto effectively model intricate cross-modal relationships, hence limiting theirapplicability in clinical setting. To overcome this, we further introduce apathology-specific VL training scheme that generates enhanced and perturbedsamples for multimodal contrastive learning. Experimental evaluationsdemonstrate that our approach achieves state-of-the-art performance onPathoHR-Bench and six additional pathology datasets, highlighting itseffectiveness in fine-grained pathology representation.</description>
      <author>example@mail.com (Yating Huang, Ziyan Huang, Lintao Xiang, Qijun Yang, Hujun Yin)</author>
      <guid isPermaLink="false">2509.06105v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>AttriPrompt: Dynamic Prompt Composition Learning for CLIP</title>
      <link>http://arxiv.org/abs/2509.05949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AttriPrompt框架，解决了当前深度文本提示方法的两个关键限制：过度依赖对比学习目标而忽略细粒度特征优化，以及使用静态提示无法实现内容自适应。该框架通过利用CLIP视觉编码器的中间层特征增强文本语义表示，并引入属性检索模块、双流对比学习和自正则化机制，在三个基准测试上实现了高达7.37%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;提示学习方法的发展推动了更深层次的提示设计以增强模型性能，但当前深度文本提示方法存在两个关键限制：过度依赖对比学习目标优先考虑高层语义对齐而忽略细粒度特征优化；在所有输入类别中使用静态提示无法实现内容自适应。&lt;h4&gt;目的&lt;/h4&gt;解决当前深度文本提示方法的两个关键限制，提出AttriPrompt框架，通过利用CLIP视觉编码器的中间层特征来增强和 refine 文本语义表示，实现细粒度特征优化和内容自适应。&lt;h4&gt;方法&lt;/h4&gt;设计属性检索模块对每一层的视觉特征进行聚类，聚合视觉特征从提示池中检索语义相似的提示并连接到文本编码器各层输入；引入双流对比学习实现细粒度对齐；通过自正则化机制在提示文本特征和非提示文本特征间应用显式正则化约束防止过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准测试上的大量实验表明AttriPrompt优于最先进方法，在基础到新颖设置中实现了高达7.37%的改进；该方法在跨领域知识转移方面表现出色，使视觉语言预训练模型成为更可行的现实世界解决方案。&lt;h4&gt;结论&lt;/h4&gt;AttriPrompt框架有效解决了当前深度文本提示方法的两个关键限制，通过利用CLIP视觉编码器的中间层特征和设计属性检索模块实现了细粒度特征优化和内容自适应，自正则化机制有效防止了在有限训练数据上的过拟合，在多个基准测试上表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;提示学习方法的发展推动了更深层次的提示设计以增强模型性能。然而，当前深度文本提示方法存在两个关键限制：过度依赖对比学习目标，优先考虑高层语义对齐，而忽略细粒度特征优化；在所有输入类别中使用静态提示，无法实现内容自适应。为解决这些限制，我们提出了AttriPrompt-一个新颖的框架，通过利用CLIP视觉编码器的中间层特征来增强和 refine 文本语义表示。我们设计了一个属性检索模块，首先对每一层的视觉特征进行聚类。聚合的视觉特征从提示池中检索语义相似的提示，然后将这些提示连接到文本编码器每一层的输入中。利用提示文本特征中嵌入的分层视觉信息，我们引入双流对比学习来实现细粒度对齐。此外，我们通过在提示文本特征和非提示文本特征之间应用显式的正则化约束，引入了自正则化机制，以防止在有限训练数据上的过拟合。在三个基准测试上的大量实验证明了AttriPrompt优于最先进的方法，在基础到新颖设置中实现了高达7.37%的改进。我们方法在跨领域知识转移方面的优势，使视觉语言预训练模型成为更可行的现实世界解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The evolution of prompt learning methodologies has driven exploration ofdeeper prompt designs to enhance model performance. However, current deep textprompting approaches suffer from two critical limitations: Over-reliance onconstrastive learning objectives that prioritize high-level semantic alignment,neglecting fine-grained feature optimization; Static prompts across all inputcategories, preventing content-aware adaptation. To address these limitations,we propose AttriPrompt-a novel framework that enhances and refines textualsemantic representations by leveraging the intermediate-layer features ofCLIP's vision encoder. We designed an Attribute Retrieval module that firstclusters visual features from each layer. The aggregated visual featuresretrieve semantically similar prompts from a prompt pool, which are thenconcatenated to the input of every layer in the text encoder. Leveraginghierarchical visual information embedded in prompted text features, weintroduce Dual-stream Contrastive Learning to realize fine-grained alignment.Furthermore, we introduce a Self-Regularization mechanism by applying explicitregularization constraints between the prompted and non-prompted text featuresto prevent overfitting on limited training data. Extensive experiments acrossthree benchmarks demonstrate AttriPrompt's superiority over state-of-the-artmethods, achieving up to 7.37\% improvement in the base-to-novel setting. Theobserved strength of our method in cross-domain knowledge transfer positionsvision-language pre-trained models as more viable solutions for real-worldimplementation.</description>
      <author>example@mail.com (Qiqi Zhan, Shiwei Li, Qingjie Liu, Yunhong Wang)</author>
      <guid isPermaLink="false">2509.05949v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of the State-of-the-Art in Conversational Question Answering Systems</title>
      <link>http://arxiv.org/abs/2509.05716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 12 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是对话式问答系统(ConvQA)领域的综述，提供了该领域的全面分析，包括核心组件、先进机器学习技术、大型语言模型的应用、关键数据集以及未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;对话式问答系统已成为自然语言处理(NLP)的关键领域，使机器能够进行动态和具有上下文感知能力的对话。这些能力正被应用于客户支持、教育、法律和医疗保健等多个领域，在这些领域中保持连贯且相关的对话至关重要。&lt;h4&gt;目的&lt;/h4&gt;提供对话式问答系统最先进技术的综合分析，全面概述ConvQA领域，并为该领域的未来发展提供有价值的见解。&lt;h4&gt;方法&lt;/h4&gt;论文分析了ConvQA系统的核心组件（历史选择、问题理解和答案预测），调查了先进的机器学习技术（包括强化学习、对比学习和迁移学习）以提高ConvQA的准确性和效率，并探讨了大型语言模型（如RoBERTa、GPT-4、Gemini 2.0 Flash、Mistral 7B和LLaMA 3）的关键作用。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型在ConvQA领域发挥着关键作用，通过数据规模扩展和架构进步展示了其影响力。论文还分析了关键的ConvQA数据集。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了ConvQA领域的全面概述，并为指导该领域的未来发展提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;对话式问答(ConvQA)系统已成为自然语言处理(NLP)领域的关键领域，推动了机器能够进行动态和具有上下文感知能力的对话的进步。这些能力正被应用于各个领域，即客户支持、教育、法律和医疗保健，在这些领域中保持连贯且相关的对话至关重要。基于最近的进展，本综述对ConvQA的最先进技术进行了全面分析。本综述首先考察了ConvQA系统的核心组件，即历史选择、问题理解和答案预测，强调了它们在确保多轮对话中的连贯性和相关性方面的相互作用。它进一步研究了先进机器学习技术的应用，包括但不限于强化学习、对比学习和迁移学习，以提高ConvQA的准确性和效率。还探讨了大型语言模型的关键作用，即RoBERTa、GPT-4、Gemini 2.0 Flash、Mistral 7B和LLaMA 3，从而展示了它们通过数据规模扩展和架构进步所产生的影响。此外，本综述还对关键的ConvQA数据集进行了全面分析，最后概述了开放的研究方向。总体而言，这项工作提供了对ConvQA领域的全面概述，并为指导该领域的未来进展提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conversational Question Answering (ConvQA) systems have emerged as a pivotalarea within Natural Language Processing (NLP) by driving advancements thatenable machines to engage in dynamic and context-aware conversations. Thesecapabilities are increasingly being applied across various domains, i.e.,customer support, education, legal, and healthcare where maintaining a coherentand relevant conversation is essential. Building on recent advancements, thissurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.This survey begins by examining the core components of ConvQA systems, i.e.,history selection, question understanding, and answer prediction, highlightingtheir interplay in ensuring coherence and relevance in multi-turnconversations. It further investigates the use of advanced machine learningtechniques, including but not limited to, reinforcement learning, contrastivelearning, and transfer learning to improve ConvQA accuracy and efficiency. Thepivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impactthrough data scalability and architectural advancements. Additionally, thissurvey presents a comprehensive analysis of key ConvQA datasets and concludesby outlining open research directions. Overall, this work offers acomprehensive overview of the ConvQA landscape and provides valuable insightsto guide future advancements in the field.</description>
      <author>example@mail.com (Manoj Madushanka Perera, Adnan Mahmood, Kasun Eranda Wijethilake, Fahmida Islam, Maryam Tahermazandarani, Quan Z. Sheng)</author>
      <guid isPermaLink="false">2509.05716v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title>
      <link>http://arxiv.org/abs/2509.05685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MSRFormer，一个新颖的道路网络表示学习框架，通过整合多尺度空间交互来解决道路网络分析的挑战。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异质性和层次性给准确的表示学习带来了挑战。图神经网络由于同质性假设和只关注单一结构尺度，在聚合邻居节点特征时往往表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决城市道路网络异质性和层次性带来的表示学习挑战，特别是处理空间交互的流量异质性和长距离依赖问题。&lt;h4&gt;方法&lt;/h4&gt;MSRFormer框架使用空间流卷积从大型轨迹数据集中提取小尺度特征，识别尺度相关的空间交互区域来捕获道路网络的空间结构和流量异质性。通过采用图Transformer，MSRFormer有效地捕获了多尺度上的复杂空间依赖关系。空间交互特征通过残差连接融合，然后输入到对比学习算法中，以获得最终的道路网络表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，与交通相关的任务从整合轨迹数据中获益更多，在复杂的道路网络结构中，与最具竞争力的基线方法相比，实现了高达16%的改进。&lt;h4&gt;结论&lt;/h4&gt;这项研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互的尺度效应与流量异质性之间相互作用的独特关联模式。&lt;h4&gt;翻译&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异质性和层次性给准确的表示学习带来了挑战。图神经网络，其聚合来自邻居节点的特征，常常由于其同质性假设和对单一结构尺度的关注而表现不佳。为解决这些问题，本文提出了MSRFormer，一种新颖的道路网络表示学习框架，通过解决其流量异质性和长距离依赖来整合多尺度空间交互。它使用空间流卷积从大型轨迹数据集中提取小尺度特征，并识别尺度相关的空间交互区域以捕获道路网络的空间结构和流量异质性。通过采用图Transformer，MSRFormer有效地捕获了多尺度上的复杂空间依赖关系。空间交互特征使用残差连接融合，然后输入到对比学习算法中以推导最终的道路网络表示。在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，与交通相关的任务从整合轨迹数据中获益更多，在复杂的道路网络结构中与最具竞争力的基线方法相比实现了高达16%的改进。这项研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互的尺度效应与流量异质性之间相互作用的独特关联模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transforming road network data into vector representations using deeplearning has proven effective for road network analysis. However, urban roadnetworks' heterogeneous and hierarchical nature poses challenges for accuraterepresentation learning. Graph neural networks, which aggregate features fromneighboring nodes, often struggle due to their homogeneity assumption and focuson a single structural scale. To address these issues, this paper presentsMSRFormer, a novel road network representation learning framework thatintegrates multi-scale spatial interactions by addressing their flowheterogeneity and long-distance dependencies. It uses spatial flow convolutionto extract small-scale features from large trajectory datasets, and identifiesscale-dependent spatial interaction regions to capture the spatial structure ofroad networks and flow heterogeneity. By employing a graph transformer,MSRFormer effectively captures complex spatial dependencies across multiplescales. The spatial interaction features are fused using residual connections,which are fed to a contrastive learning algorithm to derive the final roadnetwork representation. Validation on two real-world datasets demonstrates thatMSRFormer outperforms baseline methods in two road network analysis tasks. Theperformance gains of MSRFormer suggest the traffic-related task benefits morefrom incorporating trajectory data, also resulting in greater improvements incomplex road network structures with up to 16% improvements compared to themost competitive baseline method. This research provides a practical frameworkfor developing task-agnostic road network representation models and highlightsdistinct association patterns of the interplay between scale effects and flowheterogeneity of spatial interactions.</description>
      <author>example@mail.com (Jian Yang, Jiahui Wu, Li Fang, Hongchao Fan, Bianying Zhang, Huijie Zhao, Guangyi Yang, Rui Xin, Xiong You)</author>
      <guid isPermaLink="false">2509.05685v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation</title>
      <link>http://arxiv.org/abs/2509.05543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 accepted paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对比表示学习框架，通过使用修剪过的骨架序列进行预训练来增强人体动作分割。该框架利用多尺度表示和跨序列变化，并提出新颖的'Shuffle and Warp'数据增强策略，引入跨排列对比和相对顺序推理两个代理任务，构建双代理对比学习网络，显著提升动作分割性能。&lt;h4&gt;背景&lt;/h4&gt;以往的动作表示学习方法都是针对动作识别设计的，基于孤立的序列级表示，没有充分利用多尺度表示和跨序列变化的信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比表示学习框架，通过预训练增强人体动作分割性能，专注于利用多尺度表示和跨序列变化。&lt;h4&gt;方法&lt;/h4&gt;提出'Shuffle and Warp'数据增强策略，利用多动作排列；引入跨排列对比学习类内相似性和相对顺序推理学习类间上下文；构建双代理对比学习网络；在修剪过的骨架数据集上预训练，在未修剪数据集上评估。&lt;h4&gt;主要发现&lt;/h4&gt;DuoCLR在多类和多标签动作分割任务中显著优于最先进的比较方法；消融研究验证了所提出方法的每个组件的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过利用多尺度表示、跨序列变化、新颖的数据增强策略和双代理学习任务，有效增强了人体动作分割性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种对比表示学习框架，通过使用修剪过的（单一动作）骨架序列进行预训练来增强人体动作分割。与以往针对动作识别设计的、基于孤立序列级表示的表示学习工作不同，所提出的框架专注于利用多尺度表示和跨序列变化。更具体地说，它提出了一种新颖的数据增强策略'Shuffle and Warp'，利用多样的多动作排列。后者有效辅助了对比学习中引入的两个代理任务：跨排列对比和相对顺序推理。在优化过程中，CPC通过对比不同排列中相同动作类别的表示来学习类内相似性，而ROR通过预测两个排列之间的相对映射来推理类间上下文。这些任务共同使双代理对比学习网络能够学习针对动作分割优化的多尺度特征表示。在实验中，DuoCLR在修剪过的骨架数据集上预训练，然后在未修剪的数据集上进行评估，在多类和多标签动作分割任务中均显示出比最先进的比较方法显著的优势。最后，进行了消融研究以评估所提出方法的每个组件的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, a contrastive representation learning framework is proposed toenhance human action segmentation via pre-training using trimmed (singleaction) skeleton sequences. Unlike previous representation learning works thatare tailored for action recognition and that build upon isolated sequence-wiserepresentations, the proposed framework focuses on exploiting multi-scalerepresentations in conjunction with cross-sequence variations. Morespecifically, it proposes a novel data augmentation strategy, 'Shuffle andWarp', which exploits diverse multi-action permutations. The latter effectivelyassists two surrogate tasks that are introduced in contrastive learning: CrossPermutation Contrasting (CPC) and Relative Order Reasoning (ROR). Inoptimization, CPC learns intra-class similarities by contrastingrepresentations of the same action class across different permutations, whileROR reasons about inter-class contexts by predicting relative mapping betweentwo permutations. Together, these tasks enable a Dual-Surrogate ContrastiveLearning (DuoCLR) network to learn multi-scale feature representationsoptimized for action segmentation. In experiments, DuoCLR is pre-trained on atrimmed skeleton dataset and evaluated on an untrimmed dataset where itdemonstrates a significant boost over state-the-art comparatives in bothmulti-class and multi-label action segmentation tasks. Lastly, ablation studiesare conducted to evaluate the effectiveness of each component of the proposedapproach.</description>
      <author>example@mail.com (Haitao Tian, Pierre Payeur)</author>
      <guid isPermaLink="false">2509.05543v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.06777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的模型无关框架，通过异步更新节点特征来解决图神经网络中的过度压缩问题，在多个数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)存在过度压缩问题，当任务需要长程交互时会出现，这是由于瓶颈限制了远距离节点间的消息传播。现有的图重连方法虽然可能表现良好，但会损害归纳偏差，导致下游任务中信息损失显著增加；而增加通道容量虽然可以克服信息瓶颈，但会增加模型的参数复杂度。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，作者提出了一种高效的模型无关框架，通过异步更新节点特征来避免传统同步消息传递GNNs的局限性。&lt;h4&gt;方法&lt;/h4&gt;该框架基于节点中心度值在每一层创建节点批次，只有属于这些批次的节点特征才会被更新。异步消息更新跨层顺序处理信息，避免了同时压缩到固定容量通道中。作者还从理论上证明了该框架比标准同步方法保持更高的特征敏感度界限。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在六个标准图数据集和两个长程数据集上应用于图分类任务，在REDDIT-BINARY和Peptides-struct上分别取得了5%和4%的显著性能提升。&lt;h4&gt;结论&lt;/h4&gt;提出的异步更新框架能够有效解决GNN中的过度压缩问题，同时保持或提高模型性能，为处理需要长程交互的图任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)存在过度压缩问题，当任务需要长程交互时会出现。这个问题源于瓶颈限制了远距离节点间的消息传播。最近，图重连方法修改边连接性，并期望在长程任务上表现良好。然而，图重连损害了归纳偏差，导致在解决下游任务时出现显著的信息损失。此外，增加通道容量虽然可以克服信息瓶颈，但会增加模型的参数复杂度。为了缓解这些缺点，我们提出了一种高效的模型无关框架，该框架异步更新节点特征，不同于传统的同步消息传递GNNs。我们的框架基于节点中心度值在每一层创建节点批次，只有属于这些批次的节点特征才会被更新。异步消息更新跨层顺序处理信息，避免了同时压缩到固定容量通道中。我们还从理论上证明，与标准同步方法相比，我们提出的框架保持了更高的特征敏感度界限。我们的框架应用于六个标准图数据集和两个长程数据集进行图分类，在REDDIT-BINARY和Peptides-struct上分别取得了5%和4%的显著性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs whentasks require long-range interactions. The problem arises from the presence ofbottlenecks that limit the propagation of messages among distant nodes.Recently, graph rewiring methods modify edge connectivity and are expected toperform well on long-range tasks. Yet, graph rewiring compromises the inductivebias, incurring significant information loss in solving the downstream task.Furthermore, increasing channel capacity may overcome information bottlenecksbut enhance the parameter complexity of the model. To alleviate theseshortcomings, we propose an efficient model-agnostic framework thatasynchronously updates node features, unlike traditional synchronous messagepassing GNNs. Our framework creates node batches in every layer based on thenode centrality values. The features of the nodes belonging to these batcheswill only get updated. Asynchronous message updates process informationsequentially across layers, avoiding simultaneous compression intofixed-capacity channels. We also theoretically establish that our proposedframework maintains higher feature sensitivity bounds compared to standardsynchronous approaches. Our framework is applied to six standard graph datasetsand two long-range datasets to perform graph classification and achievesimpressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARYand Peptides-struct, respectively.</description>
      <author>example@mail.com (Kushal Bose, Swagatam Das)</author>
      <guid isPermaLink="false">2509.06777v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.06763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为VariSAC的图神经网络增强深度强化学习框架，用于解决RIS辅助的ISAC使能V2X系统中的统一可靠性和资源优化问题。&lt;h4&gt;背景&lt;/h4&gt;在车辆网络中集成可重构智能表面(RIS)与集成感知和通信(ISAC)技术可实现动态空间资源管理和实时环境适应，但车辆到基础设施(V2I)和车辆到车辆(V2V)连接要求共存，加上高度动态和异构的网络拓扑，给统一的可靠性建模和资源优化带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架来确保在RIS辅助的、ISAC使能的车辆到万物(V2X)系统中实现时间连续的连接，解决V2I和V2V连接的统一可靠性问题。&lt;h4&gt;方法&lt;/h4&gt;引入连续连接比(CCR)作为统一指标；采用带有残差适配器的图神经网络编码复杂高维系统状态；使用软演员-评论家(SAC)代理联合优化信道分配、功率控制和RIS配置，以最大化CCR驱动的长期奖励。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界城市数据集上的实验表明，VariSAC在连续V2I ISAC连接和V2V传递可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;h4&gt;结论&lt;/h4&gt;VariSAC框架有效解决了V2X系统中统一可靠性和资源优化的问题，实现了在动态环境中的持久连接。&lt;h4&gt;翻译&lt;/h4&gt;在车辆网络中集成可重构智能表面(RIS)和集成感知与通信(ISAC)技术能够实现动态空间资源管理和实时环境适应。然而，车辆到基础设施(V2I)和车辆到车辆(V2V)连接要求共存，加上高度动态和异构的网络拓扑，给统一的可靠性建模和资源优化带来了显著挑战。为解决这些问题，我们提出了VariSAC，一种用于在RIS辅助的、ISAC使能的车辆到万物(V2X)系统中确保时间连续连接的图神经网络(GNN)增强深度强化学习框架。具体而言，我们引入了连续连接比(CCR)，这是一个统一指标，表征V2I连接的持续时间可靠性和V2V链路的概率传递保证，从而统一了它们的连续可靠性语义。接下来，我们采用带有残差适配器的GNN来编码复杂的高维系统状态，捕获车辆、基站(BS)和RIS节点之间的空间依赖关系。这些表示随后由软演员-评论家(SAC)代理处理，联合优化信道分配、功率控制和RIS配置，以最大化CCR驱动的长期奖励。在真实世界城市数据集上的大量实验表明，VariSAC在连续V2I ISAC连接和V2V传递可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of Reconfigurable Intelligent Surfaces (RIS) and IntegratedSensing and Communication (ISAC) in vehicular networks enables dynamic spatialresource management and real-time adaptation to environmental changes. However,the coexistence of distinct vehicle-to-infrastructure (V2I) andvehicle-to-vehicle (V2V) connectivity requirements, together with highlydynamic and heterogeneous network topologies, presents significant challengesfor unified reliability modeling and resource optimization. To address theseissues, we propose VariSAC, a graph neural network (GNN)-augmented deepreinforcement learning framework for assured, time-continuous connectivity inRIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically,we introduce the Continuous Connectivity Ratio (CCR), a unified metric thatcharacterizes the sustained temporal reliability of V2I connections and theprobabilistic delivery guarantees of V2V links, thus unifying their continuousreliability semantics. Next, we employ a GNN with residual adapters to encodecomplex, high-dimensional system states, capturing spatial dependencies amongvehicles, base stations (BS), and RIS nodes. These representations are thenprocessed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channelallocation, power control, and RIS configurations to maximize CCR-drivenlong-term rewards. Extensive experiments on real-world urban datasetsdemonstrate that VariSAC consistently outperforms existing baselines in termsof continuous V2I ISAC connectivity and V2V delivery reliability, enablingpersistent connectivity in highly dynamic vehicular environments.</description>
      <author>example@mail.com (Huijun Tang, Wang Zeng, Ming Du, Pinlong Zhao, Pengfei Jiao, Huaming Wu, Hongjian Sun)</author>
      <guid isPermaLink="false">2509.06763v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Long-Range Graph Wavelet Networks</title>
      <link>http://arxiv.org/abs/2509.06743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Long-Range Graph Wavelet Networks (LR-GWN)的新型图神经网络，能够有效建模图中的长程交互，解决图机器学习中的核心挑战。&lt;h4&gt;背景&lt;/h4&gt;在图机器学习中，建模长程交互（信息跨越图中遥远部分传播）是一个核心挑战。图小波受多分辨率信号处理的启发，为捕获局部和全局结构提供了原则性的方法。然而，现有的基于小波的图神经网络依赖于有限阶多项式近似，限制了感受野并阻碍了长程传播。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获图中长程交互的图神经网络，克服现有基于小波的图神经网络的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出LR-GWN，将小波滤波器分解为互补的局部和全局组件。局部聚合通过高效的低阶多项式处理，长程交互则通过灵活的频域参数化捕获，在原则性小波框架内统一短距离和长距离信息流。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，LR-GWN在长程基准测试中实现了基于小波方法的最先进性能，同时在短距离数据集上保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;LR-GWN成功解决了现有基于小波的图神经网络在长程交互建模方面的局限性，通过结合局部和全局组件，实现了更有效的信息传播，同时保持了计算效率。&lt;h4&gt;翻译&lt;/h4&gt;建模长程交互，即信息跨越图中遥远部分的传播，是图机器学习中的一个核心挑战。受多分辨率信号处理启发的图小波，为捕获局部和全局结构提供了原则性的方法。然而，现有的基于小波的图神经网络依赖于有限阶多项式近似，这限制了它们的感受野并阻碍了长程传播。我们提出了长程图小波网络(LR-GWN)，它将小波滤波器分解为互补的局部和全局组件。局部聚合通过高效的低阶多项式处理，而长程交互则通过灵活的频域参数化来捕获。这种混合设计在原则性的小波框架内统一了短距离和长距离信息流。实验表明，LR-GWN在长程基准测试中实现了基于小波方法的最先进性能，同时在短距离数据集上保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling long-range interactions, the propagation of information acrossdistant parts of a graph, is a central challenge in graph machine learning.Graph wavelets, inspired by multi-resolution signal processing, provide aprincipled way to capture both local and global structures. However, existingwavelet-based graph neural networks rely on finite-order polynomialapproximations, which limit their receptive fields and hinder long-rangepropagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), whichdecompose wavelet filters into complementary local and global components. Localaggregation is handled with efficient low-order polynomials, while long-rangeinteractions are captured through a flexible spectral domain parameterization.This hybrid design unifies short- and long-distance information flow within aprincipled wavelet framework. Experiments show that LR-GWN achievesstate-of-the-art performance among wavelet-based methods on long-rangebenchmarks, while remaining competitive on short-range datasets.</description>
      <author>example@mail.com (Filippo Guerranti, Fabrizio Forte, Simon Geisler, Stephan Günnemann)</author>
      <guid isPermaLink="false">2509.06743v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>AnalysisGNN: Unified Music Analysis with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.06654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 17th International Symposium on Computer Music  Multidisciplinary Research (CMMR) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了AnalysisGNN，一种新的图神经网络框架，用于整合异构注释的符号数据集进行音乐分析。通过数据洗牌策略、加权多任务损失和logit融合技术，结合非和弦音预测模块，实现了与现有方法相当的性能，同时提高了对域偏移和注释不一致的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;近年来计算音乐分析方法蓬勃发展，但每种方法通常针对特定的分析领域，在处理异构注释的符号数据集时存在挑战。&lt;h4&gt;目的&lt;/h4&gt;引入AnalysisGNN框架，整合异构注释的符号数据集，用于全面乐谱分析。&lt;h4&gt;方法&lt;/h4&gt;使用数据洗牌策略，采用自定义加权多任务损失，在特定任务分类器之间进行logit融合，集成非和弦音预测模块识别并排除经过音和非功能性音符，以提高标签信号的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;AnalysisGNN实现了与传统静态数据集方法相当的性能，在多个异构语料库上对域偏移和注释不一致性表现出更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;AnalysisGNN是一种有效的框架，能够整合异构注释的数据集进行综合音乐分析，在处理域偏移和注释不一致方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;近年来，计算音乐分析方法蓬勃发展，但每种方法通常针对特定的分析领域。在这项工作中，我们引入了AnalysisGNN，一种新颖的图神经网络框架，它利用数据洗牌策略和自定义加权多任务损失，以及在特定任务分类器之间的logit融合，来整合异构注释的符号数据集，用于全面的乐谱分析。我们进一步集成了一个非和弦音预测模块，该模块识别并排除所有任务中的经过音和非功能性音符，从而提高标签信号的一致性。实验评估表明，AnalysisGNN实现了与传统静态数据集方法相当的性能，同时在多个异构语料库上对域偏移和注释不一致表现出更强的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have seen a boom in computational approaches to music analysis,yet each one is typically tailored to a specific analytical domain. In thiswork, we introduce AnalysisGNN, a novel graph neural network framework thatleverages a data-shuffling strategy with a custom weighted multi-task loss andlogit fusion between task-specific classifiers to integrate heterogeneouslyannotated symbolic datasets for comprehensive score analysis. We furtherintegrate a Non-Chord-Tone prediction module, which identifies and excludespassing and non-functional notes from all tasks, thereby improving theconsistency of label signals. Experimental evaluations demonstrate thatAnalysisGNN achieves performance comparable to traditional static-datasetapproaches, while showing increased resilience to domain shifts and annotationinconsistencies across multiple heterogeneous corpora.</description>
      <author>example@mail.com (Emmanouil Karystinaios, Johannes Hentschel, Markus Neuwirth, Gerhard Widmer)</author>
      <guid isPermaLink="false">2509.06654v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification</title>
      <link>http://arxiv.org/abs/2509.06600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了图神经网络(GCNs)用于归纳节点分类的PAC-Bayesian理论分析，考虑了节点作为相关且非同分布的数据点，推导了一层和两层GCNs的泛化边界，并建立了泛化差距收敛的条件。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理图结构数据方面取得了显著成功，但真实世界的图具有动态性，新节点不断添加，现有连接可能随时间变化。&lt;h4&gt;目的&lt;/h4&gt;建立图神经网络在动态图环境中的理论分析，特别是针对归纳节点分类任务，解决以往理论研究中无法充分建模时间演化和结构动态性的问题。&lt;h4&gt;方法&lt;/h4&gt;采用PAC-Bayesian理论框架分析图卷积网络(GCNs)，将节点视为相关且非同分布的数据点，推导泛化边界并建立收敛条件。&lt;h4&gt;主要发现&lt;/h4&gt;推导了一层GCNs的新泛化边界，明确包含数据依赖性和非平稳性的影响；建立泛化差距随节点数量增加而收敛到零的充分条件；对于两层GCNs，需要更强的图拓扑假设来保证收敛。&lt;h4&gt;结论&lt;/h4&gt;本研究为理解和改进动态图环境中图神经网络的泛化能力建立了理论基础，对处理动态图数据具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在各种应用中处理图结构数据方面取得了显著成功。真实世界图的一个关键方面是其动态性，新节点不断被添加，现有连接可能随时间变化。以往的理论研究主要基于归纳学习框架，无法充分建模这种时间演化和结构动态性。本文提出了图卷积网络(GCNs)用于归纳节点分类的PAC-Bayesian理论分析，将节点视为相关且非同分布的数据点。我们推导了一层GCNs的新泛化边界，明确包含了数据依赖性和非平稳性的影响，并建立了充分条件，使得泛化差距随着节点数量的增加而收敛到零。此外，我们将分析扩展到两层GCNs，并发现需要更强的图拓扑假设来保证收敛。这项工作为理解和改进动态图环境中GNN的泛化能力建立了理论基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have achieved remarkable success in processinggraph-structured data across various applications. A critical aspect ofreal-world graphs is their dynamic nature, where new nodes are continuallyadded and existing connections may change over time. Previous theoreticalstudies, largely based on the transductive learning framework, fail toadequately model such temporal evolution and structural dynamics. In thispaper, we presents a PAC-Bayesian theoretical analysis of graph convolutionalnetworks (GCNs) for inductive node classification, treating nodes as dependentand non-identically distributed data points. We derive novel generalizationbounds for one-layer GCNs that explicitly incorporate the effects of datadependency and non-stationarity, and establish sufficient conditions underwhich the generalization gap converges to zero as the number of nodesincreases. Furthermore, we extend our analysis to two-layer GCNs, and revealthat it requires stronger assumptions on graph topology to guaranteeconvergence. This work establishes a theoretical foundation for understandingand improving GNN generalization in dynamic graph environments.</description>
      <author>example@mail.com (Huayi Tang, Yong Liu)</author>
      <guid isPermaLink="false">2509.06600v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Topological Regularization for Force Prediction in Active Particle Suspension with EGNN and Persistent Homology</title>
      <link>http://arxiv.org/abs/2509.06574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种多尺度框架，用于捕捉主动粒子在流体中的动力学行为，结合三种学习驱动工具协同工作。&lt;h4&gt;背景&lt;/h4&gt;主动粒子（小型自驱动粒子）在流体中移动时既能变形又能改变流体形态，这种动力学行为的模拟具有挑战性，因为它需要耦合精细尺度的流体动力学与大尺度的集体效应。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够准确预测主动粒子动力学行为的多尺度框架，解决流体动力学与集体效应耦合的复杂问题。&lt;h4&gt;方法&lt;/h4&gt;结合三种学习驱动工具：1)使用高分辨率格子玻尔兹曼快照作为输入；2)采用E(2)-等变图神经网络预测粒子间相互作用力；3)使用物理信息神经网络结合傅里叶特征映射和拓扑正则化更新力预测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够提供整体的高度数据驱动的完整力网络预测，同时强调物理基础和主动物质典型的多尺度结构。&lt;h4&gt;结论&lt;/h4&gt;该多尺度框架成功捕捉了主动粒子的动力学行为，解决了流体动力学与集体效应耦合的挑战性问题。&lt;h4&gt;翻译&lt;/h4&gt;捕捉主动粒子的动力学，即小型自驱动粒子在流体中既能变形又能改变流体形态，是一个艰巨的问题，因为它需要将精细尺度的流体动力学与大尺度的集体效应耦合起来。因此，我们提出了一个多尺度框架，结合了三种学习驱动工具在一个管道中协同学习。我们使用周期性盒子中流体速度和粒子应力的高分辨率格子玻尔兹曼快照作为学习管道的输入。第二步使用粒子的形态、位置和方向来预测它们之间的成对相互作用力，采用E(2)-等变图神经网络，必须满足平面对称性。然后，物理信息神经网络通过使用傅里叶特征映射和残差块，结合应力数据对这些局部估计进行进一步更新，同时使用持久同调引入的拓扑项进行正则化，以惩罚不切实际的缠结或虚假连接。这些阶段共同提供了一个整体的高度数据驱动的完整力网络预测，强调物理基础以及主动物质典型的多尺度结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Capturing the dynamics of active particles, i.e., small self-propelled agentsthat both deform and are deformed by a fluid in which they move is a formidableproblem as it requires coupling fine scale hydrodynamics with large scalecollective effects. So we present a multi-scale framework that combines thethree learning-driven tools to learn in concert within one pipeline. We usehigh-resolution Lattice Boltzmann snapshots of fluid velocity and particlestresses in a periodic box as input to the learning pipeline. the second steptakes the morphology and positions orientations of particles to predictpairwise interaction forces between them with a E(2)-equivariant graph neuralnetwork that necessarily respect flat symmetries. Then, a physics-informedneural network further updates these local estimates by summing over them witha stress data using Fourier feature mappings and residual blocks that isadditionally regularized with a topological term (introduced by persistenthomology) to penalize unrealistically tangled or spurious connections. Inconcert, these stages deliver an holistic highly-data driven full force networkprediction empathizing on the physical underpinnings together with emergingmulti-scale structure typical for active matter.</description>
      <author>example@mail.com (Sadra Saremi, Amirhossein Ahmadkhan Kordbacheh)</author>
      <guid isPermaLink="false">2509.06574v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints</title>
      <link>http://arxiv.org/abs/2509.06395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络(GNN)的算法JCPGNN-M，用于解决无线通信系统中满足最小数据速率约束的挑战，同时满足服务质量要求。&lt;h4&gt;背景&lt;/h4&gt;无线通信系统满足最小数据速率要求是一个重大挑战，尤其是随着网络复杂性增加时。传统深度学习方法通过在损失函数中引入惩罚项和经验性调整超参数来处理这些约束，但这种方法没有理论收敛保证，且在实际场景中经常无法满足服务质量要求。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展且理论上有保证的解决方案，用于满足无线通信系统中的服务质量约束，同时降低计算复杂度并提高泛化能力。&lt;h4&gt;方法&lt;/h4&gt;基于WMMSE算法结构，扩展到具有QoS约束的多信道环境，得到增强的WMMSE(eWMMSE)算法；开发基于GNN的JCPGNN-M算法支持每个用户同时进行多信道分配；提出将GNN与基于拉格朗日的原始-对偶优化方法相结合的原则性框架，在拉格朗日框架内训练GNN以确保满足QoS约束并收敛到平稳点。&lt;h4&gt;主要发现&lt;/h4&gt;JCPGNN-M算法与eWMMSE性能相匹配，同时在推理速度、对更大网络泛化和在信道状态信息不完善情况下的鲁棒性方面具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;该研究为未来无线网络中的约束资源分配提供了一种可扩展且有理论依据的解决方案，克服了传统深度学习方法的理论局限性。&lt;h4&gt;翻译&lt;/h4&gt;在无线通信系统中满足最小数据速率要求是一个重大挑战，尤其是随着网络复杂性增长时。传统深度学习方法通常通过在损失函数中引入惩罚项和经验性调整超参数来处理这些约束。然而，这种启发式处理方法没有理论收敛保证，并且在实际场景中经常无法满足服务质量要求。基于WMMSE算法结构，我们首先将其扩展到具有QoS约束的多信道环境，得到了增强的WMMSE(eWMMSE)算法，当问题可行时，该算法可收敛到局部最优解。为了进一步降低计算复杂度并提高可扩展性，我们开发了一种基于GNN的算法JCPGNN-M，能够支持每个用户同时进行多信道分配。为了克服传统深度学习方法的局限性，我们提出了一个原则性框架，将GNN与基于拉格朗日的原始-对偶优化方法相结合。通过在拉格朗日框架内训练GNN，我们确保满足QoS约束并收敛到平稳点。广泛的模拟表明，JCPGNN-M与eWMMSE性能相匹配，同时在推理速度、对更大网络泛化和在信道状态信息不完善情况下的鲁棒性方面具有显著优势。这项工作为未来无线网络中的约束资源分配提供了一种可扩展且有理论依据的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meeting minimum data rate constraints is a significant challenge in wirelesscommunication systems, particularly as network complexity grows. Traditionaldeep learning approaches often address these constraints by incorporatingpenalty terms into the loss function and tuning hyperparameters empirically.However, this heuristic treatment offers no theoretical convergence guaranteesand frequently fails to satisfy QoS requirements in practical scenarios.Building upon the structure of the WMMSE algorithm, we first extend it to amulti-channel setting with QoS constraints, resulting in the enhanced WMMSE(eWMMSE) algorithm, which is provably convergent to a locally optimal solutionwhen the problem is feasible. To further reduce computational complexity andimprove scalability, we develop a GNN-based algorithm, JCPGNN-M, capable ofsupporting simultaneous multi-channel allocation per user. To overcome thelimitations of traditional deep learning methods, we propose a principledframework that integrates GNN with a Lagrangian-based primal-dual optimizationmethod. By training the GNN within the Lagrangian framework, we ensuresatisfaction of QoS constraints and convergence to a stationary point.Extensive simulations demonstrate that JCPGNN-M matches the performance ofeWMMSE while offering significant gains in inference speed, generalization tolarger networks, and robustness under imperfect channel state information. Thiswork presents a scalable and theoretically grounded solution for constrainedresource allocation in future wireless networks.</description>
      <author>example@mail.com (Lili Chen, Changyang She, Jingge Zhu, Jamie Evans)</author>
      <guid isPermaLink="false">2509.06395v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults</title>
      <link>http://arxiv.org/abs/2509.06289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 9 figures, plan to submit to ACM TODAES&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种统一的时空图卷积网络（ST-GCN），用于快速、准确地预测大型顺序电路中的长周期故障影响概率（FIPs），支持风险评估。该方法将门级网表建模为时空图，使用专门的时空编码器高效预测多周期FIPs，在ISCAS-89基准测试中，将仿真时间减少了10倍以上，同时保持高精度。&lt;h4&gt;背景&lt;/h4&gt;静态数据错误（SDEs）源于初始缺陷和老化，会降低安全关键系统的可靠性。功能测试可以检测与SDE相关的故障，但模拟成本高昂。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、准确的方法来预测大型顺序电路中的长周期故障影响概率，以支持定量风险评估，并减少仿真时间。&lt;h4&gt;方法&lt;/h4&gt;提出了一种统一的时空图卷积网络（ST-GCN），将门级网表建模为时空图以捕获拓扑结构和信号时序，使用专门的时空编码器高效预测多周期FIPs。该方法接受可测试性指标或故障仿真的特征，允许在效率和精度之间进行权衡。&lt;h4&gt;主要发现&lt;/h4&gt;在ISCAS-89基准测试中，该方法将仿真时间减少了10倍以上，同时保持高精度（5周期预测的平均绝对误差为0.024）。通过预测的FIPs选择观测点可以改进对长周期、难以检测的故障的检测。该方法可扩展到SoC级测试策略优化，并适合下游电子设计自动化流程。&lt;h4&gt;结论&lt;/h4&gt;所提出的时空图卷积网络框架能够有效预测长周期故障影响概率，显著减少仿真时间，同时保持高精度，为安全关键系统的风险评估提供了高效工具。&lt;h4&gt;翻译&lt;/h4&gt;静态数据错误（SDEs）源于初始缺陷和老化，会降低安全关键系统的可靠性。功能测试可以检测与SDE相关的故障，但模拟成本高昂。我们提出了一种统一的时空图卷积网络（ST-GCN），用于快速、准确地预测大型顺序电路中的长周期故障影响概率（FIPs），支持定量风险评估。门级网表被建模为时空图以捕获拓扑结构和信号时序；专门的时空编码器高效地预测多周期FIPs。在ISCAS-89基准测试中，该方法将仿真时间减少了10倍以上，同时保持高精度（5周期预测的平均绝对误差为0.024）。该框架接受可测试性指标或故障仿真的特征，允许在效率和精度之间进行权衡。测试点选择研究表明，通过预测的FIPs选择观测点可以改进对长周期、难以检测的故障的检测。该方法可扩展到SoC级测试策略优化，并适合下游电子设计自动化流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Silent Data Errors (SDEs) from time-zero defects and aging degradesafety-critical systems. Functional testing detects SDE-related faults but isexpensive to simulate. We present a unified spatio-temporal graph convolutionalnetwork (ST-GCN) for fast, accurate prediction of long-cycle fault impactprobabilities (FIPs) in large sequential circuits, supporting quantitative riskassessment. Gate-level netlists are modeled as spatio-temporal graphs tocapture topology and signal timing; dedicated spatial and temporal encoderspredict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the methodreduces simulation time by more than 10x while maintaining high accuracy (meanabsolute error 0.024 for 5-cycle predictions). The framework accepts featuresfrom testability metrics or fault simulation, allowing efficiency-accuracytrade-offs. A test-point selection study shows that choosing observation pointsby predicted FIPs improves detection of long-cycle, hard-to-detect faults. Theapproach scales to SoC-level test strategy optimization and fits downstreamelectronic design automation flows.</description>
      <author>example@mail.com (Shaoqi Wei, Senling Wang, Hiroshi Kai, Yoshinobu Higami, Ruijun Ma, Tianming Ni, Xiaoqing Wen, Hiroshi Takahashi)</author>
      <guid isPermaLink="false">2509.06289v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations</title>
      <link>http://arxiv.org/abs/2509.06286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RecMind是一种增强型大语言图推荐系统，将语言模型视为偏好先验而非单一排序器，通过文本条件化和图学习两种方式生成嵌入，并通过门控机制融合，在推荐任务上取得了显著效果。&lt;h4&gt;背景&lt;/h4&gt;个性化技术在消费技术、流媒体、购物、可穿戴设备和语音等领域是核心能力，但仍面临交互稀疏、内容快速更新和异构文本信号等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出RecMind，一种增强型大语言图推荐系统，将语言模型视为偏好先验而非单一排序器，以解决个性化推荐中的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用配备轻量适配器的冻结LLM从标题、属性和评论中生成文本条件化的用户/物品嵌入；使用LightGCN主干从用户-物品图中学习协作嵌入；通过对称对比目标对齐两种视图；通过层内门控融合它们，使语言在冷启动/长尾场景中占主导，图结构在其他地方稳定排序。&lt;h4&gt;主要发现&lt;/h4&gt;在Yelp和Amazon-Electronics数据集上，RecMind在所有八个报告指标上取得了最佳结果，相对于强大的基线，相对改进最高达+4.53%（Recall@40）和+4.01%（NDCG@40）；消融研究证实了跨视图对齐的必要性以及门控相比晚期融合和仅LLM变体的优势。&lt;h4&gt;结论&lt;/h4&gt;RecMind成功地将语言模型和图神经网络结合，解决了个性化推荐中的稀疏交互、内容快速更新和异构文本信号等挑战，在多个指标上超越了现有方法。&lt;h4&gt;翻译&lt;/h4&gt;个性化是消费技术、流媒体、购物、可穿戴设备和语音等领域的核心能力，但仍受到交互稀疏、内容快速周转和异构文本信号的挑战。我们提出了RecMind，一种增强型大语言图推荐系统，将语言模型视为偏好先验而非单一排序器。配备轻量适配器的冻结LLM从标题、属性和评论中生成文本条件化的用户/物品嵌入；LightGCN主干从用户-物品图中学习协作嵌入。我们通过对称对比目标对齐两种视图，并通过层内门控融合它们，使语言在冷启动/长尾场景中占主导，图结构在其他地方稳定排序。在Yelp和Amazon-Electronics上，RecMind在所有八个报告指标上取得了最佳结果，相对于强大的基线，相对改进最高达+4.53%（Recall@40）和+4.01%（NDCG@40）。消融研究证实了跨视图对齐的必要性以及门控相比晚期融合和仅LLM变体的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalization is a core capability across consumer technologies, streaming,shopping, wearables, and voice, yet it remains challenged by sparseinteractions, fast content churn, and heterogeneous textual signals. We presentRecMind, an LLM-enhanced graph recommender that treats the language model as apreference prior rather than a monolithic ranker. A frozen LLM equipped withlightweight adapters produces text-conditioned user/item embeddings fromtitles, attributes, and reviews; a LightGCN backbone learns collaborativeembeddings from the user-item graph. We align the two views with a symmetriccontrastive objective and fuse them via intra-layer gating, allowing languageto dominate in cold/long-tail regimes and graph structure to stabilize rankingselsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results onall eight reported metrics, with relative improvements up to +4.53\%(Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm boththe necessity of cross-view alignment and the advantage of gating over latefusion and LLM-only variants.</description>
      <author>example@mail.com (Chang Xue, Youwei Lu, Chen Yang, Jinming Xing)</author>
      <guid isPermaLink="false">2509.06286v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators</title>
      <link>http://arxiv.org/abs/2509.06154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages including references. Supplementary Information provided&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种图神经模拟器(GNS)方法，结合消息传递图神经网络和显式数值时间步进方案，通过建模瞬时时间导数来学习时间依赖的偏微分方程(PDE)解，显著提高了数据效率和预测精度。&lt;h4&gt;背景&lt;/h4&gt;神经算子(NOs)可以近似无限维函数空间之间的映射，但需要大型数据集且在训练数据稀疏时表现不佳。许多神经算子公式没有明确编码物理演化的因果性和时间局部结构。自回归模型虽然通过预测下一个时间步保持因果性，但存在误差快速累积的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效学习时间依赖PDE解的方法，解决神经算子在数据稀疏情况下的局限性，同时减少自回归模型中的误差累积问题。&lt;h4&gt;方法&lt;/h4&gt;使用图神经模拟器(GNS)，这是一种消息传递图神经网络框架，结合显式数值时间步进方案，通过建模瞬时时间导数来学习PDE解。引入了PCA+KMeans轨迹选择策略以提高低数据性能。&lt;h4&gt;主要发现&lt;/h4&gt;GNS在三个典型PDE系统(2D Burgers标量方程、2D耦合Burgers矢量方程和2D Allen-Cahn方程)上表现优异。仅使用30个训练样本(可用数据的3%)，GNS在所有系统上都实现了低于1%的相对L2误差。与基线模型相比，GNS显著减少了长时间范围内的误差累积：相对于FNO AR减少82.48%的误差，相对于DON AR减少99.86%的误差。&lt;h4&gt;结论&lt;/h4&gt;将基于图的局部归纳偏差与传统时间积分器相结合，可以为时间依赖的PDEs生成准确、物理一致且可扩展的代理模型，显著提高了数据效率和预测精度。&lt;h4&gt;翻译&lt;/h4&gt;神经算子(NOs)近似无限维函数空间之间的映射，但需要大型数据集且在训练数据稀疏时表现不佳。许多神经算子公式没有明确编码物理演化的因果性和时间局部结构。虽然自回归模型通过预测下一个时间步来保持因果性，但存在误差快速累积的问题。我们采用图神经模拟器(GNS)——一种消息传递图神经网络框架——结合显式数值时间步进方案，构建精确的前向模型，通过建模瞬时时间导数来学习PDE解。我们在三个典型的PDE系统上评估我们的框架：(1) 2D Burgers标量方程，(2) 2D耦合Burgers矢量方程，(3) 2D Allen-Cahn方程。严格的评估表明，GNS显著提高了数据效率，与DeepONet和FNO等神经算子基线相比，使用更少的训练轨迹实现了更高的泛化精度。仅使用30个训练样本(可用数据的3%)，GNS在所有三个PDE系统上都实现了低于1%的相对L2误差。在长时间范围内，GNS显著减少了误差累积：平均而言，GNS相对于FNO AR减少了82.48%的误差，相对于DON AR减少了99.86%的误差。我们引入了PCA+KMeans轨迹选择策略，提高了低数据性能。结果表明，将基于图的局部归纳偏差与传统时间积分器相结合，可以为时间依赖的PDEs生成准确、物理一致且可扩展的代理模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural operators (NOs) approximate mappings between infinite-dimensionalfunction spaces but require large datasets and struggle with scarce trainingdata. Many NO formulations don't explicitly encode causal, local-in-timestructure of physical evolution. While autoregressive models preserve causalityby predicting next time-steps, they suffer from rapid error accumulation. Weemploy Graph Neural Simulators (GNS) - a message-passing graph neural networkframework - with explicit numerical time-stepping schemes to construct accurateforward models that learn PDE solutions by modeling instantaneous timederivatives. We evaluate our framework on three canonical PDE systems: (1) 2DBurgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2DAllen-Cahn equation. Rigorous evaluations demonstrate GNS significantlyimproves data efficiency, achieving higher generalization accuracy withsubstantially fewer training trajectories compared to neural operator baselineslike DeepONet and FNO. GNS consistently achieves under 1% relative L2 errorswith only 30 training samples out of 1000 (3% of available data) across allthree PDE systems. It substantially reduces error accumulation over extendedtemporal horizons: averaged across all cases, GNS reduces autoregressive errorby 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce aPCA+KMeans trajectory selection strategy enhancing low-data performance.Results indicate combining graph-based local inductive biases with conventionaltime integrators yields accurate, physically consistent, and scalable surrogatemodels for time-dependent PDEs.</description>
      <author>example@mail.com (Dibyajyoti Nayak, Somdatta Goswami)</author>
      <guid isPermaLink="false">2509.06154v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities</title>
      <link>http://arxiv.org/abs/2509.06041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型多阶段图神经网络架构，用于改进封闭空腔中浮力驱动热传递的模拟，解决了传统GNN在高分辨率图结构中捕获长程依赖关系的困难。&lt;h4&gt;背景&lt;/h4&gt;封闭空腔中的浮力驱动热传递是热设计的典型测试平台，高精度CFD建模虽能提供精确温度场解，但依赖专家设计的物理模型、精细网格和密集计算，限制了快速迭代。数据驱动建模，特别是图神经网络，为从模拟数据中学习热流体行为提供了新选择。&lt;h4&gt;目的&lt;/h4&gt;解决传统GNN难以在高分辨率图结构中捕获长程依赖关系的问题，开发一种能够跨多个空间尺度建模全局到局部相互作用的架构，提高热传递模拟的预测准确性和训练效率。&lt;h4&gt;方法&lt;/h4&gt;提出利用分层池化和上池化操作的多阶段GNN架构，在新开发的CFD数据集上评估该模型，该数据集模拟了不同宽高比矩形空腔中的自然对流，条件为底部壁面等温热，顶部壁面等温冷，两个垂直壁面绝热。&lt;h4&gt;主要发现&lt;/h4&gt;与传统最先进的GNN基线相比，所提出的模型实现了更高的预测准确性，提高了训练效率，并减少了长期误差累积。&lt;h4&gt;结论&lt;/h4&gt;多阶段GNN方法在基于网格的流体动力学模拟中模拟复杂热传递具有显著潜力，为热设计提供了更高效的计算工具。&lt;h4&gt;翻译&lt;/h4&gt;封闭空腔中的浮力驱动热传递作为热设计的典型测试平台，高精度CFD建模能提供精确的温度场解，但其对专家设计的物理模型、精细网格和密集计算的依赖限制了快速迭代。数据驱动建模的最新发展，特别是图神经网络，为从模拟数据中直接学习热流体行为提供了新选择，特别是在不规则网格结构上。然而，传统GNN往往难以在高分辨率图结构中捕获长程依赖关系。为克服这一局限，我们提出了一种新型多阶段GNN架构，利用分层池化和上池化操作逐步跨多个空间尺度建模全局到局部的相互作用。我们在新开发的CFD数据集上评估了所提出的模型，该数据集模拟了不同宽高比矩形空腔中的自然对流，其中底部壁面等温热，顶部壁面等温冷，两个垂直壁面绝热。实验结果表明，与最先进的GNN基线相比，所提出的模型实现了更高的预测准确性，提高了训练效率，并减少了长期误差累积。这些发现强调了所提出的多阶段GNN方法在基于网格的流体动力学模拟中模拟复杂热传递的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Buoyancy-driven heat transfer in closed cavities serves as a canonicaltestbed for thermal design High-fidelity CFD modelling yields accurate thermalfield solutions, yet its reliance on expert-crafted physics models, finemeshes, and intensive computation limits rapid iteration. Recent developmentsin data-driven modeling, especially Graph Neural Networks (GNNs), offer newalternatives for learning thermal-fluid behavior directly from simulation data,particularly on irregular mesh structures. However, conventional GNNs oftenstruggle to capture long-range dependencies in high-resolution graphstructures. To overcome this limitation, we propose a novel multi-stage GNNarchitecture that leverages hierarchical pooling and unpooling operations toprogressively model global-to-local interactions across multiple spatialscales. We evaluate the proposed model on our newly developed CFD datasetsimulating natural convection within a rectangular cavities with varying aspectratios where the bottom wall is isothermal hot, the top wall is isothermalcold, and the two vertical walls are adiabatic. Experimental resultsdemonstrate that the proposed model achieves higher predictive accuracy,improved training efficiency, and reduced long-term error accumulation comparedto state-of-the-art (SOTA) GNN baselines. These findings underscore thepotential of the proposed multi-stage GNN approach for modeling complex heattransfer in mesh-based fluid dynamics simulations.</description>
      <author>example@mail.com (Mohammad Ahangarkiasari, Hassan Pouraria)</author>
      <guid isPermaLink="false">2509.06041v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion</title>
      <link>http://arxiv.org/abs/2509.05980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRACE是一种创新的检索增强生成方法，通过构建多级别、多语义的代码图和混合图检索器，解决了LLMs在存储库级代码任务中面临的上下文限制和结构依赖问题，显著提升了代码检索和生成的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在局部代码补全方面表现出色，但在处理存储库级别的任务时存在困难，主要原因是上下文窗口有限，以及代码库之间复杂的语义和结构依赖关系。虽然检索增强生成通过检索相关代码片段缓解了上下文稀缺问题，但当前方法存在显著限制。&lt;h4&gt;目的&lt;/h4&gt;解决当前代码检索方法过度依赖文本相似性而忽略结构关系，以及通过简单连接代码片段丢失关键结构信息的问题，提高LLMs在存储库级代码任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;GRACE构建了一个多级别、多语义的代码图，统一了文件结构、抽象语法树、函数调用图、类层次结构和数据流图，以捕获静态和动态代码语义。对于检索，GRACE采用混合图检索器，结合基于图神经网络的结构相似性和文本检索，并通过基于图注意力网络的重排序器来优化拓扑相关的子图。GRACE还引入了结构融合机制，将检索到的子图与本地代码上下文合并，并保留函数调用和继承等关键依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;在公共存储库级基准上的大量实验表明，GRACE在所有指标上都显著优于最先进的方法。使用DeepSeek-V3作为骨干LLM，GRACE在每个数据集上都比最强的基于图的RAG基线高出8.19%的EM和7.51%的ES点。&lt;h4&gt;结论&lt;/h4&gt;GRACE通过综合考虑代码的文本内容和结构关系，有效解决了LLMs在存储库级代码任务中的局限性，显著提升了代码检索和生成的性能，为代码理解与生成领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在局部代码补全方面表现出色，但在处理存储库级任务时由于上下文窗口有限以及代码库间复杂的语义和结构依赖关系而表现不佳。虽然检索增强生成通过检索相关代码片段缓解了上下文稀缺问题，但当前方法存在显著限制。它们过度依赖文本相似性进行检索，忽略了调用链和继承层次结构等结构关系，并通过简单地将检索到的代码片段连接成文本序列作为LLM输入，丢失了关键的结构信息。为解决这些不足，GRACE构建了一个多级别、多语义的代码图，统一了文件结构、抽象语法树、函数调用图、类层次结构和数据流图，以捕获静态和动态代码语义。对于检索，GRACE采用混合图检索器，结合基于图神经网络的结构相似性和文本检索，并通过基于图注意力网络的重排序器来优先考虑拓扑相关的子图。为增强上下文，GRACE引入了结构融合机制，将检索到的子图与本地代码上下文合并，并保留函数调用和继承等关键依赖关系。在公共存储库级基准上的大量实验表明，GRACE在所有指标上都显著优于最先进的方法。使用DeepSeek-V3作为骨干LLM，GRACE在每个数据集上都比最强的基于图的RAG基线高出8.19%的EM和7.51%的ES点。代码可在https://anonymous.4open.science/r/grace_icse-C3D5获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LLMs excel in localized code completion but struggle with repository-leveltasks due to limited context windows and complex semantic and structuraldependencies across codebases. While Retrieval-Augmented Generation (RAG)mitigates context scarcity by retrieving relevant code snippets, currentapproaches face significant limitations. They overly rely on textual similarityfor retrieval, neglecting structural relationships such as call chains andinheritance hierarchies, and lose critical structural information by naivelyconcatenating retrieved snippets into text sequences for LLM input. To addressthese shortcomings, GRACE constructs a multi-level, multi-semantic code graphthat unifies file structures, abstract syntax trees, function call graphs,class hierarchies, and data flow graphs to capture both static and dynamic codesemantics. For retrieval, GRACE employs a Hybrid Graph Retriever thatintegrates graph neural network-based structural similarity with textualretrieval, refined by a graph attention network-based re-ranker to prioritizetopologically relevant subgraphs. To enhance context, GRACE introduces astructural fusion mechanism that merges retrieved subgraphs with the local codecontext and preserves essential dependencies like function calls andinheritance. Extensive experiments on public repository-level benchmarksdemonstrate that GRACE significantly outperforms state-of-the-art methodsacross all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses thestrongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on everydataset. The code is available athttps://anonymous.4open.science/r/grace_icse-C3D5.</description>
      <author>example@mail.com (Xingliang Wang, Baoyi Wang, Chen Zhi, Junxiao Han, Xinkui Zhao, Jianwei Yin, Shuiguang Deng)</author>
      <guid isPermaLink="false">2509.05980v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification</title>
      <link>http://arxiv.org/abs/2509.05943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submit to IEEE Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DRDCAE-STGNN的新型端到端深度学习框架，用于增强运动想象脑机接口的特征学习和分类，在多个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;运动想象脑机接口在辅助技术和神经康复方面有很大潜力，但由于其非平稳特性和低信噪比，精确高效解码运动想象仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效提升运动想象特征学习和分类性能的深度学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出判别性残差密集卷积自编码器与时空图神经网络(DRDCAE-STGNN)框架，其中DRDCAE模块利用残差密集连接通过联合重建和分类学习判别性潜在表示，STGNN模块通过可学习图邻接矩阵捕获动态空间依赖性，并使用双向长短期记忆建模时间动态。&lt;h4&gt;主要发现&lt;/h4&gt;在BCI竞赛IV 2a、2b和PhysioNet数据集上分别实现了95.42%、97.51%和90.15%的平均准确率，消融研究确认了各组件的贡献，可解释性分析揭示了神经生理学上有意义的连接模式，模型每样本推理时间为0.32毫秒。&lt;h4&gt;结论&lt;/h4&gt;该方法为MI-EEG解码提供了稳健、准确和可解释的解决方案，在受试者和任务间具有强大的泛化能力，满足潜在实时BCI应用的要求。&lt;h4&gt;翻译&lt;/h4&gt;运动想象脑机接口在辅助技术和神经康复方面具有巨大潜力。然而，由于其非平稳特性和低信噪比，运动想象的精确高效解码仍然具有挑战性。本文引入了一种新颖的端到端深度学习框架——判别性残差密集卷积自编码器与时空图神经网络，以增强运动想象特征学习和分类。具体而言，判别性残差密集卷积自编码器模块利用残差密集连接通过联合重建和分类学习判别性潜在表示，而时空图神经网络模块通过可学习的图邻接矩阵捕获动态空间依赖性，并使用双向长短期记忆建模时间动态。在BCI竞赛IV 2a、2b和PhysioNet数据集上的广泛评估展示了最先进的性能，平均准确率分别为95.42%、97.51%和90.15%。消融研究确认了每个组件的贡献，可解释性分析揭示了神经生理学上有意义的连接模式。此外，尽管模型复杂，但它保持了可行的参数计数和每样本0.32毫秒的推理时间。这些结果表明，我们的方法为运动想象脑电图解码提供了稳健、准确和可解释的解决方案，在受试者和任务间具有强大的泛化能力，并满足潜在实时脑机接口应用的要求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motor imagery (MI) based brain-computer interfaces (BCIs) hold significantpotential for assistive technologies and neurorehabilitation. However, theprecise and efficient decoding of MI remains challenging due to theirnon-stationary nature and low signal-to-noise ratio. This paper introduces anovel end-to-end deep learning framework of Discriminative Residual DenseConvolutional Autoencoder with Spatio-Temporal Graph Neural Network(DRDCAE-STGNN) to enhance the MI feature learning and classification.Specifically, the DRDCAE module leverages residual-dense connections to learndiscriminative latent representations through joint reconstruction andclassifica-tion, while the STGNN module captures dynamic spatial dependenciesvia a learnable graph adjacency matrix and models temporal dynamics usingbidirectional long short-term memory (LSTM). Extensive evaluations on BCICompetition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-artperformance, with average accuracies of 95.42%, 97.51%, and 90.15%,respectively. Ablation studies confirm the contribution of each component, andinterpreta-bility analysis reveals neurophysiologically meaningful connectivitypatterns. Moreover, despite its complexity, the model maintains a feasibleparameter count and an inference time of 0.32 ms per sample. These resultsindicate that our method offers a robust, accurate, and interpretable solutionfor MI-EEG decoding, with strong generalizability across subjects and tasks andmeeting the requirements for potential real-time BCI applications.</description>
      <author>example@mail.com (Yi Wang, Haodong Zhang, Hongqi Li)</author>
      <guid isPermaLink="false">2509.05943v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the critical factors in crystal structure graph representation: a comparative analysis using streamlined MLPSets frameworks</title>
      <link>http://arxiv.org/abs/2509.05712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;数据驱动的图表示方法在材料科学和化学领域表现优异，特别是结合电子结构生成模型和多任务学习的方法，能够更全面地表示材料的复杂特性并达到接近DFT计算的精度。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在材料科学和化学领域迅速发展，其性能依赖于对晶体或分子结构在五个维度上的全面表示：元素信息、几何拓扑、电子相互作用、对称性和长程相互作用。然而，现有模型在表示电子相互作用、对称性和长程信息方面仍存在局限性。&lt;h4&gt;目的&lt;/h4&gt;比较基于物理的位点特征计算器与数据驱动的图表示策略，探索更有效的材料结构表示方法。&lt;h4&gt;方法&lt;/h4&gt;比较物理方法和数据驱动方法；数据驱动方法结合电子结构生成模型（如变分自编码器VAEs，用于压缩Kohn-Sham波函数）；利用多任务学习；将CHGNet-V1/V2策略集成到DenseGNN模型中；采用预训练和微调策略。&lt;h4&gt;主要发现&lt;/h4&gt;数据驱动方法在表示完整性、收敛速度和外推能力方面表现更优越；CHGNet-V1/V2策略集成到DenseGNN模型后，在35个数据集上超越了最先进模型，预测精度接近DFT计算；预训练和微调策略显著降低了复杂无序材料带隙的预测误差。&lt;h4&gt;结论&lt;/h4&gt;数据驱动的图表示在加速材料发现方面具有优越性和潜力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在材料科学和化学领域迅速发展，其性能严重依赖于对晶体或分子结构在五个维度上的全面表示：元素信息、几何拓扑、电子相互作用、对称性和长程相互作用。现有模型在表示电子相互作用、对称性和长程信息方面仍存在局限性。本研究比较了基于物理的位点特征计算器与数据驱动的图表示策略。我们发现，后者通过结合电子结构生成模型（如压缩Kohn-Sham波函数的变分自编码器VAEs）和利用多任务学习，在表示完整性、收敛速度和外推能力方面实现了更优越的性能。值得注意的是，当CHGNet-V1/V2策略集成到DenseGNN模型中时，在Matbench和JARVIS-DFT的35个数据集上显著优于最先进模型，产生了接近DFT计算精度的预测结果。此外，采用预训练和微调策略显著降低了复杂无序材料带隙的预测误差，证明了数据驱动图表示在加速材料发现方面的优越性和潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决晶体结构图表示中的关键因素分析问题，特别是如何有效表示晶体结构的五个关键维度：元素信息、几何拓扑信息、电子相互作用信息、对称性信息和长程相互作用信息。这个问题重要是因为图神经网络已成为材料科学和化学领域预测材料特性的关键工具，但现有模型在电子相互作用表示方面存在不足，缺乏系统性比较研究，限制了材料发现和性质预测的准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到晶体结构表示需要涵盖多个维度，发现现有方法在电子相互作用表示上有不足。他们设计了两种晶体结构图表示策略进行比较：基于物理的位点特征计算器和数据驱动的结构图表示策略。借鉴了现有工作如AGNIFingerprints、OPSiteFingerprint、CrystalNNFingerprint等物理方法，以及MEGNet、M3GNet等数据驱动模型，并创新性地整合了变分自编码器(VAE)来处理电子结构信息，开发出CHGNet系列模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是数据驱动的晶体结构图表示策略比基于物理的方法更能全面表示晶体结构，特别是通过VAE压缩Kohn-Sham波函数可以有效表示电子结构。实现流程包括：1)设计两种表示策略并进行比较；2)引入简化的MLPSets框架以最小化GNN操作干扰；3)在多个基准数据集上评估性能；4)将最优的CHGNet-V1策略应用于DenseGNN模型并进行实际应用测试。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统比较物理和数据驱动的两种表示策略；2)开发出CHGNet-V1/V2数据驱动模型，整合VAE处理电子结构；3)提出简化的MLPSets框架便于公平比较；4)将CHGNet-V1应用于DenseGNN显著提升性能；5)证明数据驱动方法在外推任务上的优势。相比之前工作，本文更全面地考虑了电子相互作用信息，并通过系统比较揭示了数据驱动方法的优势，特别是在小数据集和外推场景下表现出色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过系统比较和优化晶体结构图表示策略，开发出基于数据驱动的CHGNet-V1表示方法，显著提升了材料性质预测模型的性能和泛化能力，特别是在外推任务和小数据集场景下表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks have rapidly advanced in materials science andchemistry,with their performance critically dependent on comprehensiverepresentations of crystal or molecular structures across five dimensions:elemental information, geometric topology, electronic interactions, symmetry,and long-range interactions. Existing models still exhibit limitations inrepresenting electronic interactions, symmetry, and long-range information.This study compares physics-based site feature calculators with data-drivengraph representation strategies. We find that the latter achieve superiorperformance in representation completeness, convergence speed, andextrapolation capability by incorporating electronic structure generationmodels-such as variational autoencoders (VAEs) that compress Kohn-Sham wavefunctions and leveraging multi-task learning. Notably, the CHGNet-V1/V2strategies, when integrated into the DenseGNN model,significantly outperformstate-of-the-art models across 35 datasets from Matbench and JARVIS-DFT,yielding predictions with accuracy close to that of DFT calculations.Furthermore, applying a pre-training and fine-tuning strategy substantiallyreduces the prediction error for band gaps of complex disordered materials,demonstrating the superiority and potential of data-driven graphrepresentations in accelerating materials discovery.</description>
      <author>example@mail.com (Hongwei Du, Hong Wang)</author>
      <guid isPermaLink="false">2509.05712v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR</title>
      <link>http://arxiv.org/abs/2509.05671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GraMFedDHAR的基于图的多模态联邦学习框架，用于人类活动识别任务。该框架将不同传感器数据建模为模态特定图，通过残差图卷积神经网络处理，并通过基于注意力的权重融合。实验结果表明，在差分隐私约束下，所提出的MultiModalGCN模型显著优于基线模型。&lt;h4&gt;背景&lt;/h4&gt;使用多模态传感器数据进行人类活动识别面临测量噪声或不完整、标记样本稀缺以及隐私问题等挑战。传统集中式深度学习方法受限于基础设施可用性、网络延迟和数据共享限制。联邦学习虽通过本地训练解决了隐私问题，但仍需处理异构多模态数据和差分隐私要求带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图的多模态联邦学习框架GraMFedDHAR，用于解决人类活动识别任务中的隐私保护和多模态数据融合问题。&lt;h4&gt;方法&lt;/h4&gt;将不同传感器流（如压力垫、深度相机和多个加速度计）建模为模态特定图，通过残差图卷积神经网络处理，使用基于注意力的权重而非简单连接来融合多模态数据，并在联邦聚合过程中应用差分隐私保护数据安全。&lt;h4&gt;主要发现&lt;/h4&gt;MultiModalGCN模型在非差分隐私设置下比基线模型高约2%的准确率；在差分隐私约束下，性能差距达7%到13%；基于图的建模在多模态学习中表现出鲁棒性，图神经网络更能抵抗差分隐私噪声带来的性能下降。&lt;h4&gt;结论&lt;/h4&gt;基于图的建模方法在多模态学习中表现出鲁棒性，特别是在差分隐私约束下。图神经网络比传统方法更能抵抗差分隐私噪声带来的性能下降，为隐私保护下的人类活动识别提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用多模态传感器数据进行人类活动识别(HAR)仍然具有挑战性，因为测量存在噪声或不完整，标记样本稀缺，以及存在隐私问题。传统的集中式深度学习方法通常受限于基础设施可用性、网络延迟和数据共享限制。虽然联邦学习(FL)通过本地训练模型并仅共享模型参数解决了隐私问题，但仍需处理使用异构多模态数据和差分隐私要求所带来的问题。在本文中，提出了一种基于图的多模态联邦学习框架GraMFedDHAR用于HAR任务。将压力垫、深度相机和多个加速度计等不同传感器流建模为模态特定图，通过残差图卷积神经网络(GCN)处理，并通过基于注意力的权重而非简单连接进行融合。融合后的嵌入使活动分类更加鲁棒，同时差分隐私在联邦聚合过程中保护数据。实验结果表明，所提出的MultiModalGCN模型在集中式和联邦范式的非差分隐私设置下，比基线MultiModalFFN模型高高达2%的准确率。更重要的是，在差分隐私约束下观察到显著改进：MultiModalGCN模型持续超越MultiModalFFN模型，性能差距根据隐私预算和设置不同，在7%到13%之间。这些结果突显了基于图的建模在多模态学习中的鲁棒性，其中图神经网络(GNNs)被证明更能抵抗差分隐私噪声引入的性能下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Activity Recognition (HAR) using multimodal sensor data remainschallenging due to noisy or incomplete measurements, scarcity of labeledexamples, and privacy concerns. Traditional centralized deep learningapproaches are often constrained by infrastructure availability, networklatency, and data sharing restrictions. While federated learning (FL) addressesprivacy by training models locally and sharing only model parameters, it stillhas to tackle issues arising from the use of heterogeneous multimodal data anddifferential privacy requirements. In this article, a Graph-based MultimodalFederated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diversesensor streams such as a pressure mat, depth camera, and multipleaccelerometers are modeled as modality-specific graphs, processed throughresidual Graph Convolutional Neural Networks (GCNs), and fused viaattention-based weighting rather than simple concatenation. The fusedembeddings enable robust activity classification, while differential privacysafeguards data during federated aggregation. Experimental results show thatthe proposed MultiModalGCN model outperforms the baseline MultiModalFFN, withup to 2 percent higher accuracy in non-DP settings in both centralized andfederated paradigms. More importantly, significant improvements are observedunder differential privacy constraints: MultiModalGCN consistently surpassesMultiModalFFN, with performance gaps ranging from 7 to 13 percent depending onthe privacy budget and setting. These results highlight the robustness ofgraph-based modeling in multimodal learning, where GNNs prove more resilient tothe performance degradation introduced by DP noise.</description>
      <author>example@mail.com (Labani Halder, Tanmay Sen, Sarbani Palit)</author>
      <guid isPermaLink="false">2509.05671v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation</title>
      <link>http://arxiv.org/abs/2509.05550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at: https://github.com/lizixi-0x2F/TreeGPT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了TreeGPT，一种创新的神经网络架构，结合了transformer注意力机制和全局父子聚合，用于处理抽象语法树。在视觉推理任务中实现了96%的准确率，显著优于其他模型，同时仅使用1.5M参数。&lt;h4&gt;背景&lt;/h4&gt;传统神经程序合成方法主要依赖顺序处理或图神经网络，在处理抽象语法树时存在局限性。ARC Prize 2025是一个需要抽象模式识别和规则推理的视觉推理基准。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效处理抽象语法树的新神经网络架构，提高神经程序合成任务中的性能，特别是在视觉推理任务中。&lt;h4&gt;方法&lt;/h4&gt;TreeGPT采用混合设计，结合自注意力机制捕获局部依赖，以及TreeFeed-Forward Network通过迭代消息传递建模树结构。核心是全局父子聚合机制，使节点能通过多次迭代聚合整个树的信息。还包括门控聚合、残差连接和双向传播等增强功能。&lt;h4&gt;主要发现&lt;/h4&gt;在ARC Prize 2025数据集上，TreeGPT实现了96%的准确率，显著优于transformer基线模型(1.3%)、Grok-4(15.9%)和SOAR(52%)。消融研究表明边投影是最关键组件，边投影和门控组合实现最佳性能。&lt;h4&gt;结论&lt;/h4&gt;TreeGPT通过结合transformer注意力机制和全局父子聚合，为处理抽象语法树提供了有效方法，在视觉推理任务中取得显著成果，同时保持模型高效性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了TreeGPT，一种新颖的神经网络架构，它结合了基于transformer的注意力机制和全局父子聚合，用于处理神经程序合成任务中的抽象语法树。与仅依赖顺序处理或图神经网络的传统方法不同，TreeGPT采用混合设计，利用自注意力机制捕获局部依赖关系，并通过专门的TreeFeed-Forward Network通过迭代消息传递建模层次化树结构。核心创新在于全局父子聚合机制，使每个节点能够通过多次迭代逐步聚合整个树结构的信息。我们的架构集成了门控聚合、残差连接和双向传播等增强功能。在ARC Prize 2025数据集上的评估表明，TreeGPT实现了96%的准确率，显著优于其他基准模型，同时仅使用1.5M参数。消融研究表明边投影是最关键组件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce TreeGPT, a novel neural architecture that combinestransformer-based attention mechanisms with global parent-child aggregation forprocessing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.Unlike traditional approaches that rely solely on sequential processing orgraph neural networks, TreeGPT employs a hybrid design that leverages bothself-attention for capturing local dependencies and a specialized TreeFeed-Forward Network (TreeFFN) for modeling hierarchical tree structuresthrough iterative message passing.  The core innovation lies in our Global Parent-Child Aggregation mechanism,formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \inE_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents thehidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edgesinvolving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. Thisformulation enables each node to progressively aggregate information from theentire tree structure through $T$ iterations.  Our architecture integrates optional enhancements including gated aggregationwith learnable edge weights, residual connections for gradient stability, andbidirectional propagation for capturing both bottom-up and top-downdependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challengingvisual reasoning benchmark requiring abstract pattern recognition and ruleinference. Experimental results demonstrate that TreeGPT achieves 96\%accuracy, significantly outperforming transformer baselines (1.3\%),large-scale models like Grok-4 (15.9\%), and specialized program synthesismethods like SOAR (52\%) while using only 1.5M parameters. Our comprehensiveablation study reveals that edge projection is the most critical component,with the combination of edge projection and gating achieving optimalperformance.</description>
      <author>example@mail.com (Zixi Li)</author>
      <guid isPermaLink="false">2509.05550v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)</title>
      <link>http://arxiv.org/abs/2509.05447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 18 figures, accepted to IEEE Transactions on Wireless  Communications. This is the extended journal version of the conference paper  arXiv:2203.14339 (Z. Zhao, A. Swami and S. Segarra, "Distributed Link  Sparsification for Scalable Scheduling using Graph Neural Networks," IEEE  ICASSP 2022, pp. 5308-5312, doi: 10.1109/ICASSP43922.2022.9747437 )&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络(GNN)的分布式链路稀疏化方案，用于减少无线网络中的调度开销，同时保持网络容量。&lt;h4&gt;背景&lt;/h4&gt;在具有密集连接特性的无线网络中，分布式链路调度算法产生的显著信令开销会加剧拥塞、能源消耗和无线电足迹扩展等问题。&lt;h4&gt;目的&lt;/h4&gt;缓解无线网络中的信令开销问题，提出一种分布式链路稀疏化方案，减少对延迟容忍型流量的调度开销，同时保持网络容量。&lt;h4&gt;方法&lt;/h4&gt;训练一个图神经网络模块，根据流量统计和网络拓扑为单个链路调整竞争阈值，使不太可能成功的链路退出调度竞争。通过离线约束无监督学习算法平衡最小化调度开销和确保总效用达到所需水平这两个目标。&lt;h4&gt;主要发现&lt;/h4&gt;在包含多达500条链路的模拟无线多跳网络中，链路稀疏化技术有效缓解了网络拥塞，并减少了四种不同分布式链路调度协议的无线电足迹。&lt;h4&gt;结论&lt;/h4&gt;基于GNN的链路稀疏化方案是一种有效的方法，可以在保持网络容量的同时减少无线网络中的调度开销。&lt;h4&gt;翻译&lt;/h4&gt;在具有密集连接特性的无线网络中，分布式链路调度算法产生的显著信令开销会加剧拥塞、能源消耗和无线电足迹扩展等问题。为了缓解这些挑战，我们提出了一种分布式链路稀疏化方案，该方案使用图神经网络(GNNs)来减少对延迟容忍型流量的调度开销，同时保持网络容量。一个GNN模块被训练来根据流量统计和网络拓扑为单个链路调整竞争阈值，使链路在不太可能成功的情况下退出调度竞争。我们的方法通过一种新颖的离线约束无监督学习算法实现，该算法能够平衡两个相互竞争的目标：最小化调度开销，同时确保总效用达到所需水平。在包含多达500条链路的模拟无线多跳网络中，我们的链路稀疏化技术有效缓解了网络拥塞，并减少了四种不同分布式链路调度协议的无线电足迹。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TWC.2025.3606741&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In wireless networks characterized by dense connectivity, the significantsignaling overhead generated by distributed link scheduling algorithms canexacerbate issues like congestion, energy consumption, and radio footprintexpansion. To mitigate these challenges, we propose a distributed linksparsification scheme employing graph neural networks (GNNs) to reducescheduling overhead for delay-tolerant traffic while maintaining networkcapacity. A GNN module is trained to adjust contention thresholds forindividual links based on traffic statistics and network topology, enablinglinks to withdraw from scheduling contention when they are unlikely to succeed.Our approach is facilitated by a novel offline constrained {unsupervised}learning algorithm capable of balancing two competing objectives: minimizingscheduling overhead while ensuring that total utility meets the required level.In simulated wireless multi-hop networks with up to 500 links, our linksparsification technique effectively alleviates network congestion and reducesradio footprints across four distinct distributed link scheduling protocols.</description>
      <author>example@mail.com (Zhongyuan Zhao, Gunjan Verma, Ananthram Swami, Santiago Segarra)</author>
      <guid isPermaLink="false">2509.05447v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Safeguarding Graph Neural Networks against Topology Inference Attacks</title>
      <link>http://arxiv.org/abs/2509.05429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Acctepted by ACM CCS'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNNs)中的拓扑隐私问题，提出了一种新型的拓扑推理攻击方法，并设计了私有图重构(PGR)防御框架来保护拓扑隐私同时保持模型准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为学习图结构数据的有力模型，但其广泛应用引发了严重的隐私问题。先前的研究主要集中在边级隐私上，而图的整体结构拓扑隐私这一关键但未被充分探索的威胁却很少受到关注。&lt;h4&gt;目的&lt;/h4&gt;研究GNNs中的拓扑隐私风险，揭示其对图级推理攻击的脆弱性，并提出一种能够保护拓扑隐私同时保持模型准确性的防御框架。&lt;h4&gt;方法&lt;/h4&gt;提出了一套拓扑推理攻击(TIAs)，这些攻击仅通过黑盒访问GNN模型就能重构目标训练图的结构；引入私有图重构(PGR)防御框架，将其表述为双层优化问题，使用元迭代生成合成训练图，并基于不断演化的图同时更新GNN模型。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs极易受到拓扑推理攻击，现有的边级差分隐私机制要么无法缓解风险，要么严重损害模型准确性；PGR能显著减少拓扑泄露，同时对模型准确性影响最小。&lt;h4&gt;结论&lt;/h4&gt;拓扑隐私是GNNs中一个重要但被忽视的隐私问题，需要专门的防御机制来保护，而PGR框架为此提供了一个有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的有力模型。然而，它们的广泛应用引发了严重的隐私问题。尽管先前的研究主要集中在边级隐私上，但一个关键却未被充分探索的威胁在于拓扑隐私——图整体结构的保密性。在这项工作中，我们对GNNs中的拓扑隐私风险进行了全面研究，揭示了它们对图级推理攻击的脆弱性。为此，我们提出了一套拓扑推理攻击(TIAs)，这些攻击仅通过黑盒访问GNN模型就能重构目标训练图的结构。我们的研究表明，GNNs极易受到这些攻击，现有的边级差分隐私机制不足以缓解风险，因为它们要么无法减轻风险，要么严重损害模型准确性。为了应对这一挑战，我们引入了私有图重构(PGR)，这是一个新的防御框架，旨在保护拓扑隐私同时保持模型准确性。PGR被表述为一个双层优化问题，其中使用元梯度迭代生成合成训练图，并基于不断演化的图同时更新GNN模型。大量实验证明，PGR显著减少了拓扑泄露，同时对模型准确性的影响最小。我们的代码已在 https://github.com/JeffffffFu/PGR 上匿名提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful models for learningfrom graph-structured data. However, their widespread adoption has raisedserious privacy concerns. While prior research has primarily focused onedge-level privacy, a critical yet underexplored threat lies in topologyprivacy - the confidentiality of the graph's overall structure. In this work,we present a comprehensive study on topology privacy risks in GNNs, revealingtheir vulnerability to graph-level inference attacks. To this end, we propose asuite of Topology Inference Attacks (TIAs) that can reconstruct the structureof a target training graph using only black-box access to a GNN model. Ourfindings show that GNNs are highly susceptible to these attacks, and thatexisting edge-level differential privacy mechanisms are insufficient as theyeither fail to mitigate the risk or severely compromise model accuracy. Toaddress this challenge, we introduce Private Graph Reconstruction (PGR), anovel defense framework designed to protect topology privacy while maintainingmodel accuracy. PGR is formulated as a bi-level optimization problem, where asynthetic training graph is iteratively generated using meta-gradients, and theGNN model is concurrently updated based on the evolving graph. Extensiveexperiments demonstrate that PGR significantly reduces topology leakage withminimal impact on model accuracy. Our code is anonymously available athttps://github.com/JeffffffFu/PGR.</description>
      <author>example@mail.com (Jie Fu, Hong Yuan, Zhili Chen, Wendy Hui Wang)</author>
      <guid isPermaLink="false">2509.05429v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.05397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in Science Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于强化学习的框架，用于解决多机器人在共享障碍物丰富环境中的自动化任务分配、调度和运动规划问题，实现了八个机器人执行40个抓取任务的零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现代机器人制造需要多个机器人在共享、障碍物丰富的环境中无碰撞地协调完成多项任务。虽然单个任务可能简单，但在时空约束下自动进行联合任务分配、调度和运动规划对于经典方法在现实规模下计算上难以处理。现有工业多臂系统依赖人工经验手动设计轨迹，过程劳动密集。&lt;h4&gt;目的&lt;/h4&gt;解决多机器人系统在复杂环境中的自动化任务和运动规划挑战，减少对人工干预的依赖，提高效率和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出基于强化学习的框架，使用图神经网络(GNN)策略，在程序生成的多样化环境中进行训练。该方法采用场景的图表示和图策略神经网络，通过强化学习生成多个机器人的轨迹，联合解决任务分配、调度和运动规划的子问题。&lt;h4&gt;主要发现&lt;/h4&gt;训练好的策略可以直接泛化到未见过的设置，适应不同的机器人位置、障碍物几何形状和任务姿态。解决方案的高速能力使其可用于工作单元布局优化，提高解决方案时间。&lt;h4&gt;结论&lt;/h4&gt;该规划器的速度和可扩展性为容错规划和基于在线感知的重新规划等新功能开辟了可能，特别是在需要快速适应动态任务集的场景中。&lt;h4&gt;翻译&lt;/h4&gt;现代机器人制造需要多个机器人在共享、障碍物丰富的环境中无碰撞地协调完成多项任务。虽然单个任务可能简单，但在时空约束下自动进行联合任务分配、调度和运动规划对于经典方法在现实规模下计算上难以处理。现有工业中部署的多臂系统依赖人类直觉和经验手动设计可行轨迹，这是一个劳动密集型过程。为解决这一挑战，我们提出了一种强化学习(RL)框架来实现自动化任务和运动规划，在障碍物丰富的环境中进行了测试，八个机器人在共享工作空间中执行40个抓取任务，任何机器人可以以任何顺序执行任何任务。我们的方法建立在通过强化学习在程序生成的环境中训练的图神经网络(GNN)策略基础上，这些环境具有多样化的障碍物布局、机器人配置和任务分布。它采用场景的图表示和通过强化学习训练的图策略神经网络来生成多个机器人的轨迹，联合解决任务分配、调度和运动规划的子问题。在模拟中使用大型随机生成的任务集进行训练后，我们的策略可以直接泛化到具有不同机器人位置、障碍物几何形状和任务姿态的未见设置。我们进一步证明了该解决方案的高速能力使其可用于工作单元布局优化，提高解决方案时间。该规划器的速度和可扩展性也为容错规划和基于在线感知的重新规划等新功能打开了大门，在这些场景中需要快速适应动态任务集。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在障碍物丰富的共享工作空间中，多个机器人协同完成多个任务时的自动化任务分配、调度和运动规划问题。这个问题在现实中非常重要，因为现代机器人制造需要高效协调多个机器人，而传统方法在真实世界规模上计算上不可行，现有工业系统依赖人工手动设计轨迹，耗时耗力且难以适应变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了经典方法的局限性：采样方法在复杂环境中计算复杂度呈指数增长，任务调度类似旅行商问题但更复杂，任务分配类似背包问题但成本相互依赖。然后作者提出基于深度强化学习的框架，使用图神经网络表示场景关系。他们借鉴了图神经网络处理关系信息的能力，强化学习中的TD3算法，以及后视经验回放(HER)解决稀疏奖励问题。关键创新在于将这三个子问题联合解决，而不是传统方法的分步解决。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用图神经网络表示多机器人场景，其中节点代表机器人、任务和障碍物，边代表它们之间的关系。通过强化学习训练策略网络直接控制所有机器人的关节速度，将复杂计算从在线规划转移到离线训练。整体流程包括：1)将场景表示为图；2)图神经网络接收状态输入；3)策略网络输出关节速度命令；4)模拟器执行动作并防止碰撞；5)基于任务完成和碰撞情况计算奖励；6)使用改进的TD3算法和HER训练网络；7)训练后的模型可快速生成规划并支持工作单元优化等应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)图神经网络与强化学习的结合解决多机器人协调；2)联合解决任务分配、调度和运动规划三个子问题；3)模型复杂度随问题规模线性或二次增长而非指数增长；4)在随机训练环境上训练后能零样本泛化到新环境；5)规划速度极快，最大设置下每步仅需0.3毫秒。相比之前工作，传统方法需分步解决子问题且牺牲最优性，最多只能处理5个机器人和10个任务，依赖人工设计耗时轨迹，难以适应动态环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过结合图神经网络和强化学习，RoboBallet实现了在障碍物丰富的环境中高效协调多达八个机器人完成四十个任务的自动化任务和运动规划，解决了传统方法在真实世界规模上面临的计算复杂性问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1126/scirobotics.ads1204&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern robotic manufacturing requires collision-free coordination of multiplerobots to complete numerous tasks in shared, obstacle-rich workspaces. Althoughindividual tasks may be simple in isolation, automated joint task allocation,scheduling, and motion planning under spatio-temporal constraints remaincomputationally intractable for classical methods at real-world scales.Existing multi-arm systems deployed in the industry rely on human intuition andexperience to design feasible trajectories manually in a labor-intensiveprocess. To address this challenge, we propose a reinforcement learning (RL)framework to achieve automated task and motion planning, tested in anobstacle-rich environment with eight robots performing 40 reaching tasks in ashared workspace, where any robot can perform any task in any order. Ourapproach builds on a graph neural network (GNN) policy trained via RL onprocedurally-generated environments with diverse obstacle layouts, robotconfigurations, and task distributions. It employs a graph representation ofscenes and a graph policy neural network trained through reinforcement learningto generate trajectories of multiple robots, jointly solving the sub-problemsof task allocation, scheduling, and motion planning. Trained on large randomlygenerated task sets in simulation, our policy generalizes zero-shot to unseensettings with varying robot placements, obstacle geometries, and task poses. Wefurther demonstrate that the high-speed capability of our solution enables itsuse in workcell layout optimization, improving solution times. The speed andscalability of our planner also open the door to new capabilities such asfault-tolerant planning and online perception-based re-planning, where rapidadaptation to dynamic task sets is required.</description>
      <author>example@mail.com (Matthew Lai, Keegan Go, Zhibin Li, Torsten Kroger, Stefan Schaal, Kelsey Allen, Jonathan Scholz)</author>
      <guid isPermaLink="false">2509.05397v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Pothole Detection and Recognition based on Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.06750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet模型，用于自动检测和识别道路坑洞。该模型通过预处理技术和持续优化，在测试集上实现了97.78%至98.89%的高分类准确率，性能优于随机森林、MLP、SVM和LightGBM等其他模型。&lt;h4&gt;背景&lt;/h4&gt;随着计算机视觉和机器学习的快速发展，基于图像和视频数据的坑洞检测和识别自动化方法受到广泛关注。对道路图像进行深入分析对社会发展具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;通过特征提取对道路图像进行深入分析，实现对新图像中坑穴状况的自动识别。&lt;h4&gt;方法&lt;/h4&gt;研究团队对收集的原始数据集应用了标准化、归一化和数据增强等预处理技术，基于实验结果持续改进网络模型，构建了基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet模型。采用比较评估方法，将提出的模型与随机森林、MLP、SVM和LightGBM等模型进行性能比较，基于准确率、召回率、精确率、F1分数和FPS等指标进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;提出的迁移学习模型在识别速度和准确性方面表现出高性能，超越了其他模型的性能。通过仔细的参数选择和模型优化，在90个初始测试样本上实现了97.78%的分类准确率（88/90），在扩展测试集上实现了98.89%的分类准确率（890/900）。&lt;h4&gt;结论&lt;/h4&gt;构建的ResNet50-EfficientNet-RegNet模型具有高分类精度和计算效率，在坑洞检测任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;随着计算机视觉和机器学习的迅速发展，基于图像和视频数据的坑洞检测和识别自动化方法已受到显著关注。通过特征提取对道路图像进行深入分析，从而实现对新图像中坑洞状况的自动识别，对社会发展具有重要意义。因此，这是本研究解决的主要问题。基于对收集的原始数据集应用标准化、归一化和数据增强等预处理技术，我们根据实验结果持续改进网络模型。最终，我们构建了一个基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet模型。该模型具有高分类精度和计算效率。在模型评估方面，本研究采用比较评估方法，将提出的迁移学习模型与随机森林、MLP、SVM和LightGBM等其他模型进行比较。比较分析基于准确率、召回率、精确率、F1分数和FPS等指标，以评估本文提出的迁移学习模型的分类性能。结果表明，我们的模型在识别速度和准确性方面表现出高性能，超越了其他模型的性能。通过仔细的参数选择和模型优化，我们的迁移学习模型在90个初始测试样本集上实现了97.78%（88/90）的分类准确率，在扩展测试集上实现了98.89%（890/900）的分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of computer vision and machine learning, automatedmethods for pothole detection and recognition based on image and video datahave received significant attention. It is of great significance for socialdevelopment to conduct an in-depth analysis of road images through featureextraction, thereby achieving automatic identification of the pothole conditionin new images. Consequently, this is the main issue addressed in this study.Based on preprocessing techniques such as standardization, normalization, anddata augmentation applied to the collected raw dataset, we continuouslyimproved the network model based on experimental results. Ultimately, weconstructed a deep learning feature extraction networkResNet50-EfficientNet-RegNet model based on transfer learning. This modelexhibits high classification accuracy and computational efficiency. In terms ofmodel evaluation, this study employed a comparative evaluation approach bycomparing the performance of the proposed transfer learning model with othermodels, including Random Forest, MLP, SVM, and LightGBM. The comparisonanalysis was conducted based on metrics such as Accuracy, Recall, Precision,F1-score, and FPS, to assess the classification performance of the transferlearning model proposed in this paper. The results demonstrate that our modelexhibits high performance in terms of recognition speed and accuracy,surpassing the performance of other models. Through careful parameter selectionand model optimization, our transfer learning model achieved a classificationaccuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89%(890/900) on the expanded test set.</description>
      <author>example@mail.com (Mang Hu, Qianqian Xia)</author>
      <guid isPermaLink="false">2509.06750v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination</title>
      <link>http://arxiv.org/abs/2509.06575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种鲁棒且自适应的光谱方法(RAS)，用于处理具有未知且可能大量污染比例的表示多任务学习，允许任务间存在异质性，无需预先了解污染水平或真实表示维度。&lt;h4&gt;背景&lt;/h4&gt;基于表示的多任务学习(MTL)通过学习任务间的共享结构提高效率，但实际应用常受污染、异常值或对抗任务阻碍。现有方法和理论假设清洁环境，在污染严重时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;处理具有未知且可能大量污染比例的表示多任务学习，同时允许内聚任务之间存在异质性。&lt;h4&gt;方法&lt;/h4&gt;提出鲁棒且自适应的光谱方法(RAS)，可以有效且高效地提炼共享的内聚表示，无需预先了解污染水平或真实表示维度。&lt;h4&gt;主要发现&lt;/h4&gt;为学习到的表示和每个任务的参数提供了非渐近误差界，这些界适应于内聚任务相似性和异常值结构，保证RAS至少与单任务学习一样好，防止负迁移；将框架扩展到迁移学习，为目标任务提供理论保证。&lt;h4&gt;结论&lt;/h4&gt;大量实验证实了理论，展示了RAS的鲁棒性和自适应性，以及在高达80%任务污染情况下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;基于表示的多任务学习(MTL)通过学习任务间的共享结构提高效率，但其实际应用常受到污染、异常值或对抗任务的阻碍。大多数现有方法和理论假设清洁或接近清洁的环境，在污染严重时表现不佳。本文处理具有未知且可能大量污染比例的表示多任务学习，同时允许内聚任务之间存在异质性。我们提出了一种鲁棒且自适应的光谱方法(RAS)，可以有效且高效地提炼共享的内聚表示，无需预先了解污染水平或真实表示的维度。理论上，我们为学习到的表示和每个任务的参数提供了非渐近误差界。这些界适应于内聚任务相似性和异常值结构，并保证RAS至少与单任务学习一样好，从而防止负迁移。我们将框架扩展到迁移学习，为目标任务提供了相应的理论保证。大量实验证实了我们的理论，展示了RAS的鲁棒性和自适应性，以及在高达80%任务污染情况下的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation-based multi-task learning (MTL) improves efficiency bylearning a shared structure across tasks, but its practical application isoften hindered by contamination, outliers, or adversarial tasks. Most existingmethods and theories assume a clean or near-clean setting, failing whencontamination is significant. This paper tackles representation MTL with anunknown and potentially large contamination proportion, while also allowing forheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectralmethod (RAS) that can distill the shared inlier representation effectively andefficiently, while requiring no prior knowledge of the contamination level orthe true representation dimension. Theoretically, we provide non-asymptoticerror bounds for both the learned representation and the per-task parameters.These bounds adapt to inlier task similarity and outlier structure, andguarantee that RAS performs at least as well as single-task learning, thuspreventing negative transfer. We also extend our framework to transfer learningwith corresponding theoretical guarantees for the target task. Extensiveexperiments confirm our theory, showcasing the robustness and adaptivity ofRAS, and its superior performance in regimes with up to 80\% taskcontamination.</description>
      <author>example@mail.com (Yian Huang, Yang Feng, Zhiliang Ying)</author>
      <guid isPermaLink="false">2509.06575v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Synesthesia of Machines (SoM)-Aided LiDAR Point Cloud Transmission for Collaborative Perception</title>
      <link>http://arxiv.org/abs/2509.06506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了一种名为LPC-FT的激光雷达点云特征传输系统，通过机器联觉实现高效、鲁棒且适用的点云传输，支持多智能体协作感知&lt;h4&gt;背景&lt;/h4&gt;协作感知通过学习智能体之间如何共享信息来实现更准确和全面的场景理解，激光雷达点云提供了必要的精确空间数据。由于激光雷达传感器产生的大量数据，高效的点云传输对于低延迟多智能体协作至关重要&lt;h4&gt;目的&lt;/h4&gt;提出一个高效、鲁棒且适用的激光雷达点云传输系统，通过机器联觉(SoM)实现，称为激光雷达点云特征传输(LPC-FT)，用于支持多智能体之间的协作感知&lt;h4&gt;方法&lt;/h4&gt;采用一种保持密度的深度点云压缩方法，将完整点云编码为下采样高效表示；设计基于自注意力的信道编码模块，以增强激光雷达点云特征；设计基于交叉注意力的特征融合模块，集成收发器特征；利用非线性激活层和迁移学习来改善存在数字信道噪声时的深度神经网络训练&lt;h4&gt;主要发现&lt;/h4&gt;所提出的LPC-FT比基于八叉树的传统压缩后跟信道编码更鲁棒和有效；优于最先进的基于深度学习的压缩技术和现有的语义通信方法；将Chamfer距离减少了30%，平均提高了PSNR 1.9 dB&lt;h4&gt;结论&lt;/h4&gt;由于其优越的重构性能和对信道变化的鲁棒性，LPC-FT有望支持协作感知任务&lt;h4&gt;翻译&lt;/h4&gt;协作感知通过学习智能体之间如何共享信息来实现更准确和全面的场景理解，激光雷达点云提供了必要的精确空间数据。由于激光雷达传感器产生的大量数据，高效的点云传输对于低延迟多智能体协作至关重要。在这项工作中，我们通过机器联觉(SoM)提出了一种高效、鲁棒且适用的激光雷达点云传输系统，称为激光雷达点云特征传输(LPC-FT)，以支持多智能体之间的协作感知。具体来说，我们采用一种保持密度的深度点云压缩方法，将完整点云编码为下采样高效表示。为了减轻无线信道的影响，我们设计了一个基于自注意力的信道编码模块来增强激光雷达点云特征，以及一个基于交叉注意力的特征融合模块来集成收发器的特征。此外，我们利用非线性激活层和迁移学习来改善存在数字信道噪声时的深度神经网络训练。实验结果表明，所提出的LPC-FT比传统的基于八叉树的压缩后跟信道编码更鲁棒和有效，并优于最先进的基于深度学习的压缩技术和现有的语义通信方法，平均将Chamfer距离减少了30%，将PSNR提高了1.9 dB。由于其优越的重构性能和对信道变化的鲁棒性，LPC-FT有望支持协作感知任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云在多智能体协作感知中的高效传输问题。由于LiDAR传感器产生大量数据，且无线信道噪声会影响传输质量，现有方法难以高效传输点云数据。这个问题在自动驾驶和智能机器人系统中非常重要，因为协作感知能克服单一智能体的视野限制，而LiDAR点云提供了精确的空间数据，对环境理解至关重要。传输效率直接影响实时协作感知的性能，点云重建质量又直接影响感知准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了协作感知中点云传输面临的三大挑战：高效表示非均匀点云、抵抗无线信道干扰、处理数字通信系统中的非可微操作。他们借鉴了机器联觉(SoM)概念，设计任务感知的特征提取和传输方法。具体借鉴了密度保持的点云压缩方法作为特征编码器基础，利用注意力机制设计通道编码器和特征融合模块，并采用直通估计器(STE)处理数字通信系统中的非可微操作。作者采用两阶段训练策略，先训练点云压缩网络，再训练整个传输系统，以提高训练效率和性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过语义通信方式只传输点云的关键特征而非原始数据，设计端到端的深度学习框架实现点云特征的高效提取、传输和重建，利用注意力机制增强特征表示抵抗无线信道噪声，并采用数字通信框架确保与现有通信系统的兼容性。整体流程为：1)特征编码器将原始点云压缩为紧凑特征；2)通道编码器基于自注意力机制增强特征；3)调制将特征量化为数字符号；4)无线传输通过物理信道；5)解调和通道解码；6)特征融合模块基于交叉注意力机制融合接收特征和接收端自感知特征；7)特征解码器重建完整点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出LiDAR点云特征传输系统(LPC-FT)；2)采用密度保持的点云压缩网络并添加绝对坐标位置编码；3)设计基于自注意力的通道编码器增强特征；4)提出基于交叉注意力的特征融合模块；5)采用非线性激活层处理数字通信系统中的非可微操作；6)使用迁移学习和两阶段训练策略。相比之前工作的不同：专注于LiDAR点云而非普通物体点云；采用数字通信框架而非离散时间模拟传输；结合特征增强和特征融合；考虑大规模点云的绝对坐标信息；针对无线信道噪声设计专门的增强机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于机器联觉的LiDAR点云特征传输系统，通过密度保持压缩、自注意力增强和交叉注意力融合，实现了高效、鲁棒的点云传输，显著提升了多智能体协作感知的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative perception enables more accurate and comprehensive sceneunderstanding by learning how to share information between agents, with LiDARpoint clouds providing essential precise spatial data. Due to the substantialdata volume generated by LiDAR sensors, efficient point cloud transmission isessential for low-latency multi-agent collaboration. In this work, we proposean efficient, robust and applicable LiDAR point cloud transmission system viathe Synesthesia of Machines (SoM), termed LiDAR Point Cloud FeatureTransmission (LPC-FT), to support collaborative perception among multipleagents. Specifically, we employ a density-preserving deep point cloudcompression method that encodes the complete point cloud into a downsampledefficient representation. To mitigate the effects of the wireless channel, wedesign a channel encoder module based on self-attention to enhance LiDAR pointcloud features and a feature fusion module based on cross-attention tointegrate features from transceivers. Furthermore, we utilize the nonlinearactivation layer and transfer learning to improve the training of deep neuralnetworks in the presence the digital channel noise. Experimental resultsdemonstrate that the proposed LPC-FT is more robust and effective thantraditional octree-based compression followed by channel coding, andoutperforms state-of-the-art deep learning-based compression techniques andexisting semantic communication methods, reducing the Chamfer Distance by 30%and improving the PSNR by 1.9 dB on average. Owing to its superiorreconstruction performance and robustness against channel variations, LPC-FT isexpected to support collaborative perception tasks.</description>
      <author>example@mail.com (Ensong Liu, Rongqing Zhang, Xiang Cheng, Jian Tang)</author>
      <guid isPermaLink="false">2509.06506v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Minimax optimal transfer learning for high-dimensional additive regression</title>
      <link>http://arxiv.org/abs/2509.06308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a draft version of the paper. All responsibilities are  assigned to the first author&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了迁移学习框架下的高维加性回归问题，通过结合目标样本和来自相关回归模型的辅助样本，提出了一种新的估计方法，并在理论和实证上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;研究者观察到来自目标群体的样本，以及来自不同但可能相关的回归模型的辅助样本，需要利用这些信息进行高维加性回归分析。&lt;h4&gt;目的&lt;/h4&gt;开发一种在迁移学习框架下处理高维加性回归的方法，建立通用误差边界，并实现最小最优速率。&lt;h4&gt;方法&lt;/h4&gt;首先引入基于局部线性平滑的光滑反向拟合估计器的目标估计程序；然后开发一种两阶段估计方法，在迁移学习框架内提供理论和经验水平的保证。&lt;h4&gt;主要发现&lt;/h4&gt;在次魏布噪声下建立了通用误差边界，能够处理重尾误差分布；在次指数情况下达到最小最大下界；当辅助和目标分布足够接近时，实现了最小最优速率。&lt;h4&gt;结论&lt;/h4&gt;所有理论结果都得到了模拟研究和实际数据分析的支持，证明了所提出方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了迁移学习框架下高维加性回归问题，其中研究者观察到来自目标群体的样本以及来自不同但可能相关的回归模型的辅助样本。我们首先引入了基于局部线性平滑的光滑反向拟合估计器的目标估计程序。与以往工作不同，我们在次魏布噪声下建立了通用误差边界，从而能够处理重尾误差分布。在次指数情况下，我们证明估计器在正则性条件下达到最小最大下界，这需要从现有证明策略中做出重大调整。随后，我们在迁移学习框架内开发了一种新颖的两阶段估计方法，并在总体和经验水平上提供了理论保证。在通用尾部条件下推导了每个阶段的误差边界，并且我们进一步证明当辅助和目标分布足够接近时，可以实现最小最优速率。所有理论结果都得到了模拟研究和实际数据分析的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies high-dimensional additive regression under the transferlearning framework, where one observes samples from a target populationtogether with auxiliary samples from different but potentially relatedregression models. We first introduce a target-only estimation procedure basedon the smooth backfitting estimator with local linear smoothing. In contrast toprevious work, we establish general error bounds under sub-Weibull($\alpha$)noise, thereby accommodating heavy-tailed error distributions. In thesub-exponential case ($\alpha=1$), we show that the estimator attains theminimax lower bound under regularity conditions, which requires a substantialdeparture from existing proof strategies. We then develop a novel two-stageestimation method within a transfer learning framework, and provide theoreticalguarantees at both the population and empirical levels. Error bounds arederived for each stage under general tail conditions, and we furtherdemonstrate that the minimax optimal rate is achieved when the auxiliary andtarget distributions are sufficiently close. All theoretical results aresupported by simulation studies and real data analysis.</description>
      <author>example@mail.com (Seung Hyun Moon)</author>
      <guid isPermaLink="false">2509.06308v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models</title>
      <link>http://arxiv.org/abs/2509.06228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Amna-Hassan04/Fracture-Detection-Using-X-Rays-with-CNN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于AI的骨折检测解决方案，使用自定义卷积神经网络(CNN)从X光图像中自动检测骨折，并在公开数据集上进行了测试和基准比较。&lt;h4&gt;背景&lt;/h4&gt;骨折是全球性的重大健康挑战，尤其在资源有限环境中，专家放射科服务有限。传统成像方法存在成本高、辐射暴露和依赖专业解读等问题。&lt;h4&gt;目的&lt;/h4&gt;开发基于AI的解决方案，用于从X光图像中自动检测骨折，并与迁移学习模型进行比较评估。&lt;h4&gt;方法&lt;/h4&gt;使用自定义卷积神经网络(CNN)，并与迁移学习模型(EfficientNetB0, MobileNetV2, ResNet50)进行基准测试。训练使用公开可用的FracAtlas数据集，包含4,083个匿名化的肌肉骨骼X光片。&lt;h4&gt;主要发现&lt;/h4&gt;自定义CNN在FracAtlas数据集上达到95.96%的准确率、0.94的精确度、0.88的召回率和0.91的F1分数。迁移学习模型在此特定设置中表现不佳，这些结果应考虑类别不平衡和数据集限制来解释。&lt;h4&gt;结论&lt;/h4&gt;轻量级CNN在X光骨折检测方面具有前景，强调了公平基准测试、多样化数据集和临床转化的外部验证的重要性。&lt;h4&gt;翻译&lt;/h4&gt;骨折是全球面临的主要健康挑战，常导致疼痛、活动能力下降和生产力损失，特别是在资源有限的环境中，专家放射科服务有限。传统成像方法存在成本高、辐射暴露和依赖专业解读等问题。为此，我们开发了一种基于AI的解决方案，使用自定义卷积神经网络(CNN)从X光图像中自动检测骨折，并将其与迁移学习模型(包括EfficientNetB0、MobileNetV2和ResNet50)进行了基准测试。训练在公开可用的FracAtlas数据集上进行，该数据集包含4,083个匿名化的肌肉骨骼X光片。在FracAtlas数据集上，自定义CNN达到了95.96%的准确率、0.94的精确度、0.88的召回率和0.91的F1分数。尽管迁移学习模型(EfficientNetB0、MobileNetV2、ResNet50)在此特定设置中表现不佳，但这些结果应结合类别不平衡和数据集限制来解释。这项工作突出了轻量级CNN在X光骨折检测方面的前景，并强调了公平基准测试、多样化数据集和临床转化的外部验证的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bone fractures present a major global health challenge, often resulting inpain, reduced mobility, and productivity loss, particularly in low-resourcesettings where access to expert radiology services is limited. Conventionalimaging methods suffer from high costs, radiation exposure, and dependency onspecialized interpretation. To address this, we developed an AI-based solutionfor automated fracture detection from X-ray images using a custom ConvolutionalNeural Network (CNN) and benchmarked it against transfer learning modelsincluding EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted onthe publicly available FracAtlas dataset, comprising 4,083 anonymizedmusculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94precision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.Although transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)performed poorly in this specific setup, these results should be interpreted inlight of class imbalance and data set limitations. This work highlights thepromise of lightweight CNNs for detecting fractures in X-rays and underscoresthe importance of fair benchmarking, diverse datasets, and external validationfor clinical translation</description>
      <author>example@mail.com (Amna Hassan, Ilsa Afzaal, Nouman Muneeb, Aneeqa Batool, Hamail Noor)</author>
      <guid isPermaLink="false">2509.06228v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Repeating vs. Non-Repeating FRBs: A Deep Learning Approach To Morphological Characterization</title>
      <link>http://arxiv.org/abs/2509.06208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 17 figures, submitted to ApJ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于形态学的深度学习方法来分类快速射电暴(FRB)，通过迁移学习和预训练的ConvNext架构实现了高效准确的分类，能够区分重复暴和非重复暴。&lt;h4&gt;背景&lt;/h4&gt;基于CHIME/FRB目录2中记录的动态频谱对快速射电暴进行形态学分类，快速射电暴分为重复暴和非重复暴两类。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅基于FRB形态学特征的深度学习方法，用于区分重复暴和非重复暴。&lt;h4&gt;方法&lt;/h4&gt;使用迁移学习技术，采用预训练的ConvNext架构，将去色散动态频谱视为图像，基于时间和频谱特性及子脉冲结构关系进行分类，并使用数学模型表示来解释深度学习模型。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型实现了高分类指标，显著减少了训练时间和计算需求；CHIME重复和非重复事件间的形态学差异在目录2中仍然存在，深度学习模型利用了这些差异进行分类。&lt;h4&gt;结论&lt;/h4&gt;微调后的深度学习模型可用于推理，预测FRB形态是否类似于重复暴或非重复暴，当在更大数据集上训练时，这类推断将变得更加重要。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种深度学习方法，仅基于形态学对快速射电暴(FRB)进行分类，形态学编码在CHIME/FRB目录2中记录的动态频谱上。我们实现了迁移学习，使用预训练的ConvNext架构，利用其强大的特征提取能力。ConvNext被调整用于分类FRB的去色散动态频谱（我们将其视为图像），根据各种时间和频谱特性以及子脉冲结构之间的关系，将FRB分为两个子类之一，即重复暴和非重复暴。此外，我们还使用总强度数据的数学模型表示来解释深度学习模型。在FRB频谱图上微调预训练的ConvNet后，我们能够实现高分类指标，与从头开始训练深度学习模型（随机权重和偏差，没有任何特征提取能力）相比，显著减少了训练时间和计算能力。重要的是，我们的结果表明，CHIME重复事件和非重复事件之间的形态学差异在目录2中仍然存在，深度学习模型利用了这些差异进行分类。微调后的深度学习模型可用于推理，使我们能够预测FRB的形态是否类似于重复暴或非重复暴。当在即将出现的更大数据集上训练时，此类推断可能会变得越来越重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a deep learning approach to classify fast radio bursts (FRBs)based purely on morphology as encoded on recorded dynamic spectrum fromCHIME/FRB Catalog 2. We implemented transfer learning with a pretrainedConvNext architecture, exploiting its powerful feature extraction ability.ConvNext was adapted to classify dedispersed dynamic spectra (which we treat asimages) of the FRBs into one of the two sub-classes, i.e., repeater andnon-repeater, based on their various temporal and spectral properties andrelation between the sub-pulse structures. Additionally, we also usedmathematical model representation of the total intensity data to interpret thedeep learning model. Upon fine-tuning the pretrained ConvNext on the FRBspectrograms, we were able to achieve high classification metrics whilesubstantially reducing training time and computing power as compared totraining a deep learning model from scratch with random weights and biaseswithout any feature extraction ability. Importantly, our results suggest thatthe morphological differences between CHIME repeating and non-repeating eventspersist in Catalog 2 and the deep learning model leveraged these differencesfor classification. The fine-tuned deep learning model can be used forinference, which enables us to predict whether an FRB's morphology resemblesthat of repeaters or non-repeaters. Such inferences may become increasinglysignificant when trained on larger data sets that will exist in the nearfuture.</description>
      <author>example@mail.com (Bikash Kharel, Emmanuel Fonseca, Charanjot Brar, Afrokk Khan, Lluis Mas-Ribas, Swarali Shivraj Patil, Paul Scholz, Seth Robert Siegel, David C. Stenning)</author>
      <guid isPermaLink="false">2509.06208v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features</title>
      <link>http://arxiv.org/abs/2509.05913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ViSK-GAT的新型多模态深度学习框架，用于运动员肌肉骨骼风险评估，通过结合视觉和骨骼坐标特征实现了高准确率的风险分类。&lt;h4&gt;背景&lt;/h4&gt;肌肉骨骼障碍对运动员构成重大风险，早期风险评估对预防很重要，但现有方法大多针对受控环境设计，无法在复杂环境中可靠评估风险。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在复杂环境中可靠评估运动员肌肉骨骼风险的方法，通过多模态数据融合提高风险评估的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出ViSK-GAT框架，结合残差块和轻量级Transformer块学习时空依赖关系，包含细粒度注意力模块(FGAM)和多模态几何对应模块(MGCM)；构建自定义多模态数据集，结合视觉数据和骨骼坐标，将样本标记为八个风险类别。&lt;h4&gt;主要发现&lt;/h4&gt;ViSK-GAT在验证和测试中分别达到93.55%和93.89%的准确率，精确度93.86%，F1得分93.85%，Cohen's Kappa和Matthews相关系数93%；回归结果显示预测概率分布的均方根误差为0.1205，平均绝对误差为0.0156；优于九种流行的迁移学习骨干方法。&lt;h4&gt;结论&lt;/h4&gt;ViSK-GAT模型推进了人工智能在肌肉骨骼风险分类领域的应用，能够在体育领域实现有影响力的早期干预。&lt;h4&gt;翻译&lt;/h4&gt;肌肉骨骼障碍对运动员构成重大风险，早期风险评估对预防很重要。然而，大多数现有方法是为受控环境设计的，由于依赖单一类型数据，无法在复杂环境中可靠评估风险。本研究提出了ViSK-GAT（视觉-骨骼几何注意力Transformer），一种新型多模态深度学习框架，使用视觉和基于骨骼坐标的特征来分类肌肉骨骼风险。此外，还构建了一个自定义多模态数据集，结合视觉数据和骨骼坐标进行风险评估。每个样本根据全身快速评估系统标记为八个风险类别。ViSK-GAT结合了残差块和轻量级Transformer块，共同学习时空依赖关系。它包含两个新颖模块：细粒度注意力模块(FGAM)，通过视觉和骨骼输入之间的交叉注意力实现精确的跨模态特征细化；以及多模态几何对应模块(MGCM)，通过将图像特征与基于坐标的表示对齐来增强跨模态一致性。ViSK-GAT在验证和测试中分别实现了93.55%和93.89%的强性能，精确度为93.86%，F1得分为93.85%，Cohen's Kappa和Matthews相关系数为93%。回归结果也表明预测概率分布的均方根误差较低，为0.1205，相应的平均绝对误差为0.0156。与九种流行的迁移学习骨干相比，ViSK-GAT始终优于先前的方法。ViSK-GAT模型推进了人工智能的实施和应用，改变了肌肉骨骼风险分类，并能够在体育领域实现有影响力的早期干预。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Musculoskeletal disorders pose significant risks to athletes, and assessingrisk early is important for prevention. However, most existing methods aredesigned for controlled settings and fail to reliably assess risk in complexenvironments due to their reliance on a single type of data. This researchproposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novelmultimodal deep learning framework designed to classify musculoskeletal riskusing visual and skeletal coordinate-based features. In addition, a custommultimodal dataset is constructed by combining visual data and skeletalcoordinates for risk assessment. Each sample is labeled into eight riskcategories based on the Rapid Entire Body Assessment system. ViSK-GAT combinesa Residual Block with a Lightweight Transformer Block to learn spatial andtemporal dependencies jointly. It incorporates two novel modules: theFine-Grained Attention Module (FGAM), which enables precise inter-modal featurerefinement through cross-attention between visual and skeletal inputs, and theMultimodal Geometric Correspondence Module (MGCM), which enhances cross-modalcoherence by aligning image features with coordinate-based representations.ViSK-GAT achieved strong performance with validation and test accuracies of93.55\% and 93.89\%, respectively; a precision of 93.86\%; an F1 score of93.85\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\%. Theregression results also indicated a low Root Mean Square Error of the predictedprobability distribution of 0.1205 and a corresponding Mean Absolute Error of0.0156. Compared to nine popular transfer learning backbones, ViSK-GATconsistently outperformed previous methods. The ViSK-GAT model advancesartificial intelligence implementation and application, transformingmusculoskeletal risk classification and enabling impactful early interventionsin sports.</description>
      <author>example@mail.com (Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Tamanna Shermin, Md Rafiqul Islam, Mukhtar Hussain, Sami Azam)</author>
      <guid isPermaLink="false">2509.05913v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework</title>
      <link>http://arxiv.org/abs/2509.05886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;开发代理模型预测矩形微型散热器中液态钠流动的对流传热系数&lt;h4&gt;背景&lt;/h4&gt;使用计算流体动力学(CFD)对液态金属的湍流强制对流进行高保真建模既耗时又计算成本高&lt;h4&gt;目的&lt;/h4&gt;开发一种替代工具，用于液金属冷却的微型散热器的设计和优化&lt;h4&gt;方法&lt;/h4&gt;首先应用基于核的机器学习技术和浅层神经网络处理87个努塞尔数数据集，然后使用自监督物理信息神经网络和迁移学习方法提高估计性能。自监督物理信息神经网络中增加一层确定物理在损失函数中的权重，基于不确定性平衡数据和物理。迁移学习将针对水训练的浅层神经网络调整为用于钠&lt;h4&gt;主要发现&lt;/h4&gt;自监督物理信息神经网络成功估计了钠的传热率，误差约为+8%；仅使用物理进行回归时，误差保持在5%到10%之间；其他机器学习方法预测误差主要在+8%以内&lt;h4&gt;结论&lt;/h4&gt;基于机器学习的模型为液金属冷却的微型散热器的设计和优化提供了强大的替代工具&lt;h4&gt;翻译&lt;/h4&gt;开发了一种代理模型来预测矩形微型散热器中液态钠流动的对流传热系数。最初，将基于核的机器学习技术和浅层神经网络应用于包含87个矩形微型散热器中液态钠努塞尔数的数据集。随后，使用自监督物理信息神经网络和迁移学习方法提高估计性能。在自监督物理信息神经网络中，增加了一层来确定物理在损失函数中的权重，基于不确定性平衡数据和物理以获得更好的估计。对于迁移学习，将针对水训练的浅层神经网络调整为用于钠。验证结果表明，自监督物理信息神经网络成功估计了钠的传热率，误差约为+8%。仅使用物理进行回归时，误差保持在5%到10%之间。其他机器学习方法预测误差主要在+8%以内。使用计算流体动力学(CFD)对液态金属的湍流强制对流进行高保真建模既耗时又计算成本高。因此，基于机器学习的模型为液金属冷却的微型散热器的设计和优化提供了强大的替代工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A surrogate model is developed to predict the convective heat transfercoefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.Initially, kernel-based machine learning techniques and shallow neural networkare applied to a dataset with 87 Nusselt numbers for liquid sodium inrectangular miniature heat sinks. Subsequently, a self-supervisedphysics-informed neural network and transfer learning approach are used toincrease the estimation performance. In the self-supervised physics-informedneural network, an additional layer determines the weight the of physics in theloss function to balance data and physics based on their uncertainty for abetter estimation. For transfer learning, a shallow neural network trained onwater is adapted for use with Na. Validation results show that theself-supervised physics-informed neural network successfully estimate the heattransfer rates of Na with an error margin of approximately +8%. Using onlyphysics for regression, the error remains between 5% to 10%. Other machinelearning methods specify the prediction mostly within +8%. High-fidelitymodeling of turbulent forced convection of liquid metals using computationalfluid dynamics (CFD) is both time-consuming and computationally expensive.Therefore, machine learning based models offer a powerful alternative tool forthe design and optimization of liquid-metal-cooled miniature heat sinks.</description>
      <author>example@mail.com (Reza Pirayeshshirazinezhad)</author>
      <guid isPermaLink="false">2509.05886v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Brain Tumor Detection Through Diverse CNN Architectures in IoT Healthcare Industries: Fast R-CNN, U-Net, Transfer Learning-Based CNN, and Fully Connected CNN</title>
      <link>http://arxiv.org/abs/2509.05821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用人工智能深度学习技术在物联网医疗系统中进行脑肿瘤诊断，通过多种卷积神经网络架构对MRI图像进行分析，实现了高准确率的脑肿瘤分类。&lt;h4&gt;背景&lt;/h4&gt;人工智能驱动的深度学习在物联网医疗系统中推动了脑肿瘤诊断的发展。大脑健康对人类生活至关重要，而磁共振成像(MRI)为脑肿瘤检测提供了关键数据，是人工智能驱动图像分类的主要大数据来源。&lt;h4&gt;目的&lt;/h4&gt;使用MRI图像对胶质瘤、脑膜瘤和垂体肿瘤进行分类，并评估不同人工智能模型在脑肿瘤诊断中的性能。&lt;h4&gt;方法&lt;/h4&gt;使用基于区域的卷积神经网络(R-CNN)和UNet架构进行分类，同时应用卷积神经网络(CNN)和基于CNN的迁移学习模型如Inception-V3、EfficientNetB4和VGG19。模型性能通过F-score、召回率、精确度和准确率进行评估，并进行外部队列跨数据集验证。&lt;h4&gt;主要发现&lt;/h4&gt;Fast R-CNN取得了最佳结果，准确率达到99%，F得分为98.5%，曲线下面积为99.5%，召回率为99.4%，精确度为98.5%。在外部队列跨数据集验证中，EfficientNetB2表现最强，精确率达到92.11%，召回率为92.11%，特异性为95.96%，F1得分为92.02%，准确率为92.23%。&lt;h4&gt;结论&lt;/h4&gt;结合R-CNN、UNet和迁移学习可以在物联网医疗系统中实现更早的诊断和更有效的治疗。这些发现强调了人工智能模型在处理多样化数据时的稳健性和可靠性，有潜力增强物联网医疗环境中的脑肿瘤分类和患者护理。&lt;h4&gt;翻译&lt;/h4&gt;人工智能驱动的深度学习在物联网医疗系统中推动了脑肿瘤诊断的发展，在大数据集上实现了高准确性。大脑健康对人类生活至关重要，准确诊断对有效治疗必不可少。磁共振成像为脑肿瘤检测提供了关键数据，是人工智能驱动图像分类的主要大数据来源。在本研究中，我们使用基于区域的卷积神经网络和UNet架构从MRI图像中对胶质瘤、脑膜瘤和垂体肿瘤进行分类。我们还应用了卷积神经网络和基于CNN的迁移学习模型，如Inception-V3、EfficientNetB4和VGG19。使用F-score、召回率、精确度和准确率评估模型性能。Fast R-CNN取得了最佳结果，准确率为99%，F得分为98.5%，曲线下面积为99.5%，召回率为99.4%，精确度为98.5%。结合R-CNN、UNet和迁移学习可以在物联网医疗系统中实现更早的诊断和更有效的治疗，改善患者预后。物联网设备如可穿戴监控设备和智能成像系统持续收集实时数据，人工智能算法分析这些数据以提供即时见解，实现及时干预和个性化护理。在外部队列跨数据集验证中，在微调的EfficientNet模型中，EfficientNetB2表现最强，精确度为92.11%，召回率/敏感度为92.11%，特异性为95.96%，F1得分为92.02%，准确率为92.23%。这些发现强调了人工智能模型在处理多样化数据时的稳健性和可靠性，强化了它们在增强物联网医疗环境中的脑肿瘤分类和患者护理方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI)-powered deep learning has advanced brain tumordiagnosis in Internet of Things (IoT)-healthcare systems, achieving highaccuracy with large datasets. Brain health is critical to human life, andaccurate diagnosis is essential for effective treatment. Magnetic ResonanceImaging (MRI) provides key data for brain tumor detection, serving as a majorsource of big data for AI-driven image classification. In this study, weclassified glioma, meningioma, and pituitary tumors from MRI images usingRegion-based Convolutional Neural Network (R-CNN) and UNet architectures. Wealso applied Convolutional Neural Networks (CNN) and CNN-based transferlearning models such as Inception-V3, EfficientNetB4, and VGG19. Modelperformance was assessed using F-score, recall, precision, and accuracy. TheFast R-CNN achieved the best results with 99% accuracy, 98.5% F-score, 99.5%Area Under the Curve (AUC), 99.4% recall, and 98.5% precision. Combining R-CNN,UNet, and transfer learning enables earlier diagnosis and more effectivetreatment in IoT-healthcare systems, improving patient outcomes. IoT devicessuch as wearable monitors and smart imaging systems continuously collectreal-time data, which AI algorithms analyze to provide immediate insights fortimely interventions and personalized care. For external cohort cross-datasetvalidation, EfficientNetB2 achieved the strongest performance among fine-tunedEfficientNet models, with 92.11% precision, 92.11% recall/sensitivity, 95.96%specificity, 92.02% F1-score, and 92.23% accuracy. These findings underscorethe robustness and reliability of AI models in handling diverse datasets,reinforcing their potential to enhance brain tumor classification and patientcare in IoT healthcare environments.</description>
      <author>example@mail.com (Mohsen Asghari Ilani, Yaser M. Banad)</author>
      <guid isPermaLink="false">2509.05821v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Transformer-based Topology Optimization</title>
      <link>http://arxiv.org/abs/2509.05800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Transformer的机器学习模型用于拓扑优化，通过类令牌机制将边界和加载条件直接嵌入域表示，实现了高效无迭代的拓扑生成。&lt;h4&gt;背景&lt;/h4&gt;拓扑优化可设计高效复杂结构，但传统迭代方法计算成本高且对初始条件敏感；现有机器学习方法要么仍为迭代式，要么难以达到真实性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种无迭代、高保真度的拓扑生成模型，通过类令牌机制将关键条件嵌入域表示，实现实时拓扑优化。&lt;h4&gt;方法&lt;/h4&gt;提出基于Transformer的机器学习模型，使用类令牌机制嵌入边界和加载条件，在静态和动态数据集上实现，采用迁移学习和FFT编码改进性能，引入辅助损失函数提高设计真实性和可制造性。&lt;h4&gt;主要发现&lt;/h4&gt;模型在合规性误差、体积分数误差、浮动材料百分比和负载差异误差等评估指标上表现良好，接近基于扩散模型的保真度。&lt;h4&gt;结论&lt;/h4&gt;所提模型实现了无迭代的高保真拓扑生成，为实时、高质量拓扑优化提供了重要进展。&lt;h4&gt;翻译&lt;/h4&gt;拓扑优化能够设计高效复杂的结构，但传统的迭代方法（如基于SIMP的方法）通常面临高计算成本和对初始条件敏感的问题。尽管机器学习方法最近在加速拓扑生成方面显示出潜力，但现有模型要么仍然是迭代的，要么难以匹配真实性能。在这项工作中，我们提出了一种基于Transformer的机器学习模型用于拓扑优化，通过类令牌机制将关键的边界和加载条件直接嵌入到令牌化的域表示中。我们在静态和动态数据集上实现了该模型，使用迁移学习和动态负载的FFT编码来改进在动态数据集上的性能。引入了辅助损失函数以提高生成设计的真实性和可制造性。我们对模型的性能进行了全面评估，包括合规性误差、体积分数误差、浮动材料百分比和负载差异误差，并将其与最先进的非迭代和迭代生成模型进行了基准测试。我们的结果表明，所提出的模型接近基于扩散模型的保真度，同时保持无迭代特性，为实时、高保真拓扑生成迈出了重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Topology optimization enables the design of highly efficient and complexstructures, but conventional iterative methods, such as SIMP-based approaches,often suffer from high computational costs and sensitivity to initialconditions. Although machine learning methods have recently shown promise foraccelerating topology generation, existing models either remain iterative orstruggle to match ground-truth performance. In this work, we propose atransformer-based machine learning model for topology optimization that embedscritical boundary and loading conditions directly into the tokenized domainrepresentation via a class token mechanism. We implement this model on staticand dynamic datasets, using transfer learning and FFT encoding of dynamic loadsto improve our performance on the dynamic dataset. Auxiliary loss functions areintroduced to promote the realism and manufacturability of the generateddesigns. We conduct a comprehensive evaluation of the model's performance,including compliance error, volume fraction error, floating materialpercentage, and load discrepancy error, and benchmark it againststate-of-the-art non-iterative and iterative generative models. Our resultsdemonstrate that the proposed model approaches the fidelity of diffusion-basedmodels while remaining iteration-free, offering a significant step towardreal-time, high-fidelity topology generation.</description>
      <author>example@mail.com (Aaron Lutheran, Srijan Das, Alireza Tabarraei)</author>
      <guid isPermaLink="false">2509.05800v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery</title>
      <link>http://arxiv.org/abs/2509.05775v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print for camera ready version for IEEE EMBS BHI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的框架，通过基于因果森林学习的核函数对个体进行聚类，以识别对干预措施有不同反应的亚群体，从而估计异质性治疗效果。&lt;h4&gt;背景&lt;/h4&gt;估计异质性治疗效果在个性化医疗、资源分配和政策评估等领域至关重要，但传统聚类方法与因果推断的结合有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够识别对干预措施反应不同的亚群体，从而实现更有针对性和有效决策的方法。&lt;h4&gt;方法&lt;/h4&gt;提出两步法框架：首先使用Robinson分解的正交化学习器估计去偏的条件平均治疗效果，生成编码样本间治疗反应相似性的核矩阵；然后应用核化聚类发现不同的、对治疗敏感的亚群体，并计算聚类级别的平均CATEs。&lt;h4&gt;主要发现&lt;/h4&gt;通过半合成和真实世界数据集的实验证明，该方法能有效捕捉有意义的治疗效果异质性。&lt;h4&gt;结论&lt;/h4&gt;将核化聚类作为残差-残差回归框架中的正则化形式，为异质性治疗效果估计提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;估计异质性治疗效果在个性化医疗、资源分配和政策评估等领域至关重要。核心挑战在于识别对不同干预措施有不同反应的亚群体，从而实现更有针对性和有效的决策。虽然聚类方法在无监督学习中已有深入研究，但它们与因果推断的结合仍然有限。我们提出了一种新颖的框架，该框架使用从因果森林中学习到的核函数来基于估计的治疗效果对个体进行聚类，从而揭示潜在的亚群结构。我们的方法包含两个主要步骤：首先，使用Robinson分解的正交化学习器估计去偏的条件平均治疗效果，产生一个编码样本间治疗反应相似性的核矩阵；其次，对该矩阵应用核化聚类来发现不同的、对治疗敏感的亚群体，并计算聚类级别的平均CATEs。我们将这个核化聚类步骤表述为残差-残差回归框架中的一种正则化形式。通过在半合成和真实世界数据集上的大量实验，以及消融研究和探索性分析，我们证明了我们的方法在捕捉有意义的治疗效果异质性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating heterogeneous treatment effects is critical in domains such aspersonalized medicine, resource allocation, and policy evaluation. A centralchallenge lies in identifying subpopulations that respond differently tointerventions, thereby enabling more targeted and effective decision-making.While clustering methods are well-studied in unsupervised learning, theirintegration with causal inference remains limited. We propose a novel frameworkthat clusters individuals based on estimated treatment effects using a learnedkernel derived from causal forests, revealing latent subgroup structures. Ourapproach consists of two main steps. First, we estimate debiased ConditionalAverage Treatment Effects (CATEs) using orthogonalized learners via theRobinson decomposition, yielding a kernel matrix that encodes sample-levelsimilarities in treatment responsiveness. Second, we apply kernelizedclustering to this matrix to uncover distinct, treatment-sensitivesubpopulations and compute cluster-level average CATEs. We present thiskernelized clustering step as a form of regularization within theresidual-on-residual regression framework. Through extensive experiments onsemi-synthetic and real-world datasets, supported by ablation studies andexploratory analyses, we demonstrate the effectiveness of our method incapturing meaningful treatment effect heterogeneity.</description>
      <author>example@mail.com (Zilong Wang, Turgay Ayer, Shihao Yang)</author>
      <guid isPermaLink="false">2509.05775v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</title>
      <link>http://arxiv.org/abs/2509.05657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出LM-Searcher框架，利用大语言模型进行跨领域神经架构优化，无需大量领域特定适应。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在解决复杂优化问题（包括神经架构搜索）方面取得进展，但现有方法严重依赖提示工程和领域特定调优，限制了实用性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需大量领域特定适应的框架，利用LLMs进行跨领域神经架构优化。&lt;h4&gt;方法&lt;/h4&gt;提出NCode作为神经架构的通用数值字符串表示；将NAS问题重新表述为排序任务；使用基于修剪的子空间采样策略；构建包含广泛架构-性能对的数据集促进稳健学习。&lt;h4&gt;主要发现&lt;/h4&gt;LM-Searcher在领域内（如图像分类的CNN）和领域外（如分割和生成的LoRA配置）任务中都取得有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;LM-Searcher建立了灵活且可推广的基于LLM的架构搜索新范式，数据集和模型将在https://github.com/Ashone3/LM-Searcher发布。&lt;h4&gt;翻译&lt;/h4&gt;最近大语言模型的进展为解决复杂优化问题（包括神经架构搜索NAS）开辟了新途径。然而，现有的LLM驱动的NAS方法严重依赖提示工程和领域特定调优，限制了它们在不同任务中的实用性和可扩展性。在这项工作中，我们提出了LM-Searcher，一个新框架，它利用LLMs进行跨领域神经架构优化，无需大量领域特定适应。我们方法的核心是NCode，一种神经架构的通用数值字符串表示，它支持跨领域架构编码和搜索。我们还重新将NAS问题表述为排序任务，训练LLMs使用基于修剪的子空间采样策略衍生的指令调优样本，从候选池中选择高性能架构。我们策划的数据集包含广泛的架构-性能对，促进了稳健和可转移的学习。全面的实验证明，LM-Searcher在领域内（例如，图像分类的CNN）和领域外（例如，分割和生成的LoRA配置）任务中都取得了有竞争力的性能，为灵活且可推广的基于LLM的架构搜索建立了新范式。数据集和模型将在https://github.com/Ashone3/LM-Searcher发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in Large Language Models (LLMs) has opened new avenues forsolving complex optimization problems, including Neural Architecture Search(NAS). However, existing LLM-driven NAS approaches rely heavily on promptengineering and domain-specific tuning, limiting their practicality andscalability across diverse tasks. In this work, we propose LM-Searcher, a novelframework that leverages LLMs for cross-domain neural architecture optimizationwithout the need for extensive domain-specific adaptation. Central to ourapproach is NCode, a universal numerical string representation for neuralarchitectures, which enables cross-domain architecture encoding and search. Wealso reformulate the NAS problem as a ranking task, training LLMs to selecthigh-performing architectures from candidate pools using instruction-tuningsamples derived from a novel pruning-based subspace sampling strategy. Ourcurated dataset, encompassing a wide range of architecture-performance pairs,encourages robust and transferable learning. Comprehensive experimentsdemonstrate that LM-Searcher achieves competitive performance in both in-domain(e.g., CNNs for image classification) and out-of-domain (e.g., LoRAconfigurations for segmentation and generation) tasks, establishing a newparadigm for flexible and generalizable LLM-based architecture search. Thedatasets and models will be released at https://github.com/Ashone3/LM-Searcher.</description>
      <author>example@mail.com (Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li)</author>
      <guid isPermaLink="false">2509.05657v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures</title>
      <link>http://arxiv.org/abs/2509.05490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 14 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了YOLOv8和YOLOv10架构在不同层冻结策略下的性能，发现在资源受限环境中，最优冻结策略取决于数据特性而非通用规则。适当冻结可减少高达28%的GPU内存消耗，并在某些情况下超越全微调性能。&lt;h4&gt;背景&lt;/h4&gt;YOLO架构对实时目标检测至关重要，但在资源受限环境（如无人机）部署需要高效迁移学习。层冻结是常见技术，但其对当代YOLOv8和YOLOv10架构的具体影响尚未充分探索，特别是冻结深度、数据集特性和训练动态间的相互作用。&lt;h4&gt;目的&lt;/h4&gt;填补层冻结策略对YOLOv8和YOLOv10架构影响的知识空白，提供冻结深度、数据集特性和训练动态间相互作用的详细分析，并为资源有限场景下的目标检测提供平衡迁移学习的实用、循证方法。&lt;h4&gt;方法&lt;/h4&gt;系统性地研究YOLOv8和YOLOv10变体在四个代表关键基础设施监控的挑战性数据集上的多种冻结配置，整合梯度行为分析（L2范数）和视觉解释（Grad-CAM）来提供不同冻结策略下训练动态的深入见解。&lt;h4&gt;主要发现&lt;/h4&gt;1. 没有通用最优冻结策略，最优策略取决于数据特性；2. 冻结骨干网络保留通用特征有效，较浅冻结更适合处理极端类别不平衡；3. 这些配置减少高达28%的GPU内存消耗；4. 在某些情况下实现超越全微调的mAP@50分数；5. 梯度分析显示适度冻结模型具有不同收敛模式。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了实证发现和选择冻结策略的实际指导，为资源有限场景下的目标检测提供了平衡迁移学习的实用、循证方法。&lt;h4&gt;翻译&lt;/h4&gt;YOLO架构对于实时目标检测至关重要。然而，在资源受限环境（如无人机）中部署它需要高效的迁移学习。尽管层冻结是一种常见技术，但各种冻结配置对当代YOLOv8和YOLOv10架构的具体影响仍未被探索，特别是关于冻结深度、数据集特性和训练动态之间的相互作用。本研究通过呈现层冻结策略的详细分析来填补这一知识空白。我们使用四个代表关键基础设施监控的具有挑战性的数据集，系统性地研究了YOLOv8和YOLOv10变体上的多种冻结配置。我们的方法整合了梯度行为分析（L2范数）和视觉解释（Grad-CAM），以提供不同冻结策略下训练动态的深入见解。我们的研究结果表明，没有通用的最优冻结策略，而是存在一种取决于数据特性的策略。例如，冻结骨干网络对于保留通用特征有效，而较浅的冻结更适合处理极端类别不平衡。与全微调相比，这些配置可减少高达28%的GPU内存消耗，并且在某些情况下实现了超越全微调的mAP@50分数。梯度分析证实了这些发现，显示适度冻结的模型具有不同的收敛模式。最终，这项工作提供了实证发现和选择冻结策略的实际指导。它为资源有限场景下的目标检测提供了平衡迁移学习的实用、循证方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3390/math13152539&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The You Only Look Once (YOLO) architecture is crucial for real-time objectdetection. However, deploying it in resource-constrained environments such asunmanned aerial vehicles (UAVs) requires efficient transfer learning. Althoughlayer freezing is a common technique, the specific impact of various freezingconfigurations on contemporary YOLOv8 and YOLOv10 architectures remainsunexplored, particularly with regard to the interplay between freezing depth,dataset characteristics, and training dynamics. This research addresses thisgap by presenting a detailed analysis of layer-freezing strategies. Wesystematically investigate multiple freezing configurations across YOLOv8 andYOLOv10 variants using four challenging datasets that represent criticalinfrastructure monitoring. Our methodology integrates a gradient behavioranalysis (L2 norm) and visual explanations (Grad-CAM) to provide deeperinsights into training dynamics under different freezing strategies. Ourresults reveal that there is no universal optimal freezing strategy but,rather, one that depends on the properties of the data. For example, freezingthe backbone is effective for preserving general-purpose features, while ashallower freeze is better suited to handling extreme class imbalance. Theseconfigurations reduce graphics processing unit (GPU) memory consumption by upto 28% compared to full fine-tuning and, in some cases, achieve mean averageprecision (mAP@50) scores that surpass those of full fine-tuning. Gradientanalysis corroborates these findings, showing distinct convergence patterns formoderately frozen models. Ultimately, this work provides empirical findings andpractical guidelines for selecting freezing strategies. It offers a practical,evidence-based approach to balanced transfer learning for object detection inscenarios with limited resources.</description>
      <author>example@mail.com (Andrzej D. Dobrzycki, Ana M. Bernardos, José R. Casar)</author>
      <guid isPermaLink="false">2509.05490v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</title>
      <link>http://arxiv.org/abs/2509.06953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website at \url{deep-reactive-policy.com}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Deep Reactive Policy (DRP)的视觉-运动神经运动政策，用于在动态环境中实现无碰撞运动生成。DRP直接在点云感官输入上运行，结合了基于transformer的神经运动政策IMPACT和DCP-RMP模块，实现了强大的泛化能力，在模拟和现实世界环境中都表现出色。&lt;h4&gt;背景&lt;/h4&gt;在动态、部分可观察环境中生成无碰撞运动是机械臂的基本挑战。经典运动规划器可以计算全局最优轨迹但需要完整环境知识且速度慢，神经运动政策直接在原始感官输入上运行但在复杂或动态环境中难以泛化。&lt;h4&gt;目的&lt;/h4&gt;设计一种反应式神经运动政策，能够在多种动态环境中直接从点云感官输入生成无碰撞运动。&lt;h4&gt;方法&lt;/h4&gt;DRP的核心是基于transformer的神经运动政策IMPACT，在10百万条专家轨迹上预训练，并通过迭代式学生-教师微调改进静态障碍物避免能力。此外，使用DCP-RMP模块在推理时增强动态障碍物避免能力。&lt;h4&gt;主要发现&lt;/h4&gt;DRP在具有杂乱场景、动态移动障碍物和目标阻塞的挑战性任务上实现了强大的泛化能力，在模拟和现实世界环境中，成功率都超过了先前的经典方法和神经方法。&lt;h4&gt;结论&lt;/h4&gt;DRP是一种有效的解决方案，能够在动态环境中实现无碰撞运动生成，并且具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在动态、部分可观察的环境中生成无碰撞运动是机械臂的基本挑战。经典运动规划器可以计算全局最优轨迹，但需要完整的环境知识，并且在动态场景中通常太慢。神经运动政策通过直接在原始感官输入上闭环运行提供了一种有前途的替代方案，但在复杂或动态环境中往往难以泛化。我们提出了Deep Reactive Policy (DRP)，这是一种视觉-运动神经运动政策，专为在多种动态环境中进行反应式运动生成而设计，直接在点云感官输入上运行。其核心是IMPACT，一种基于transformer的神经运动政策，在10百万条跨多种模拟场景生成的专家轨迹上进行了预训练。我们通过迭代式学生-教师微调进一步改进了IMPACT的静态障碍物避免能力。此外，我们使用DCP-RMP（一种局部反应式目标提议模块）在推理时增强了政策的动态障碍物避免能力。我们在具有杂乱场景、动态移动障碍物和目标阻塞的具有挑战性的任务上评估了DRP。DRP实现了强大的泛化能力，在模拟和现实世界环境中，成功率都超过了先前的经典方法和神经方法。视频结果和代码可在https://deep-reactive-policy.com获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在动态、部分可观测环境中为机械臂生成无碰撞运动轨迹的问题。这个问题在现实中很重要，因为机器人需要在人类自然环境中（如家庭和厨房）安全导航，而传统运动规划方法需要完整环境知识且反应太慢，神经运动策略又难以在复杂或动态环境中泛化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统规划方法需要完整环境知识且速度慢；神经策略难以泛化；混合方法需要测试时间优化，牺牲了反应性。然后设计Deep Reactive Policy (DRP)，结合了大规模预训练、迭代学生-教师微调和动态障碍避免模块。作者借鉴了cuRobo作为专家规划器、Geometric Fabrics作为教师策略、RMP设计了DCP-RMP模块，并使用PointNet++处理点云数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合全局规划和局部反应控制的优势，通过大规模预训练学习通用运动规划能力，使用微调改进局部障碍避免，并在推理时添加轻量级反应模块处理动态障碍。整体流程包括：1) 大规模运动预训练：使用cuRobo生成1000万个专家轨迹训练IMPACT；2) 迭代学生-教师微调：结合IMPACT和Geometric Fabrics改进静态障碍避免；3) 动态障碍避免：添加DCP-RMP模块处理动态障碍；4) 执行：DCP-RMP先处理动态障碍，IMPACT生成动作序列供低级控制器实时执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 大规模运动预训练：使用1000万个专家轨迹，提高了泛化能力；2) 迭代学生-教师微调：结合全局规划和局部控制，改进静态障碍避免；3) DCP-RMP模块：直接从点云操作的反应性目标提议，处理动态障碍；4) 端到端的视觉运动策略：直接从点云输入生成动作。相比之前的工作，DRP不需要完整环境知识，能处理动态场景；比纯神经方法有更好的泛化能力；比混合方法不需要测试时间优化，保持实时反应性；比纯反应方法具有全局场景意识，不易陷入局部最优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Deep Reactive Policy通过结合大规模预训练的Transformer策略、迭代学生-教师微调和动态障碍避免模块，实现了在复杂动态环境中实时生成无碰撞轨迹的能力，显著优于传统和现有的神经运动规划方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating collision-free motion in dynamic, partially observableenvironments is a fundamental challenge for robotic manipulators. Classicalmotion planners can compute globally optimal trajectories but require fullenvironment knowledge and are typically too slow for dynamic scenes. Neuralmotion policies offer a promising alternative by operating in closed-loopdirectly on raw sensory inputs but often struggle to generalize in complex ordynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neuralmotion policy designed for reactive motion generation in diverse dynamicenvironments, operating directly on point cloud sensory input. At its core isIMPACT, a transformer-based neural motion policy pretrained on 10 milliongenerated expert trajectories across diverse simulation scenarios. We furtherimprove IMPACT's static obstacle avoidance through iterative student-teacherfinetuning. We additionally enhance the policy's dynamic obstacle avoidance atinference time using DCP-RMP, a locally reactive goal-proposal module. Weevaluate DRP on challenging tasks featuring cluttered scenes, dynamic movingobstacles, and goal obstructions. DRP achieves strong generalization,outperforming prior classical and neural methods in success rate across bothsimulated and real-world settings. Video results and code available athttps://deep-reactive-policy.com</description>
      <author>example@mail.com (Jiahui Yang, Jason Jingzhou Liu, Yulong Li, Youssef Khaky, Kenneth Shaw, Deepak Pathak)</author>
      <guid isPermaLink="false">2509.06953v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention</title>
      <link>http://arxiv.org/abs/2509.06705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Cortex Synth是一个从单张2D图像联合合成3D骨架几何和拓扑的端到端可微分框架，具有三个关键创新点和四个协同模块，在多个指标上达到最先进结果，并具有广泛的应用前景。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉和3D重建领域，从2D图像生成3D骨架几何和拓扑是一个具有挑战性的任务，需要同时考虑几何形状和拓扑结构。&lt;h4&gt;目的&lt;/h4&gt;开发一个端到端可微分框架，能够从单张2D图像联合合成3D骨架的几何和拓扑结构，提高合成精度并减少拓扑错误。&lt;h4&gt;方法&lt;/h4&gt;提出Cortex Synth框架，包含三个关键创新：(1)具有多尺度骨架细化的分层图注意力机制；(2)通过拉普拉斯特征分解的可微分谱拓扑优化；(3)用于姿态结构对抗性几何一致性训练。框架集成了四个协同模块：伪3D点云生成器、增强的PointNet编码器、骨架坐标解码器和新型可微分图构建网络(DGCN)。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet数据集上，该方法实现了最先进的结果，MPJPE提高了18.7%，图编辑距离提高了27.3%，与之前的方法相比拓扑错误减少了42%。&lt;h4&gt;结论&lt;/h4&gt;Cortex Synth框架通过端到端可微分设计，实现了从2D图像到3D骨架几何和拓扑的高质量合成，为机器人操作、医学成像和自动角色绑定等领域提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出Cortex Synth，一种用于从单张2D图像联合合成3D骨架几何和拓扑的新型端到端可微分框架。我们的架构引入了三个关键创新：(1)具有多尺度骨架细化的分层图注意力机制，(2)通过拉普拉斯特征分解的可微分谱拓扑优化，(3)用于姿态结构对抗性几何一致性训练。该框架集成了四个协同模块：伪3D点云生成器、增强的PointNet编码器、骨架坐标解码器以及新型可微分图构建网络(DGCN)。我们的实验表明，在ShapeNet上实现了最先进的结果，MPJPE提高了18.7%，图编辑距离提高了27.3%，与之前的方法相比拓扑错误减少了42%。该模型的端到端可微分性使其在机器人操作、医学成像和自动角色绑定中有应用前景。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从2D图像生成3D骨架时，传统方法存在的三个关键限制：非可微分骨架化管道阻止端到端优化、固定拓扑先验限制跨类别泛化能力、几何和拓扑分离优化导致结构不一致。这个问题在现实中非常重要，因为准确的3D骨架表示对机器人操作、医学成像和计算机图形学等领域至关重要，能帮助机器人理解物体结构进行抓取、辅助医生进行手术规划、以及为动画角色提供自动绑定等。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者针对传统方法的局限性，设计了一个端到端可微分框架，整合了多个现有工作的优点：借鉴了图注意力网络(GAT)用于多尺度骨架细化，采用拉普拉斯特征分解实现可微分谱拓扑优化，使用对抗训练确保几何一致性，并基于PointNet++进行点云编码。作者将这些技术整合到四个协同模块中：伪3D点云生成器、增强的PointNet++编码器、骨架坐标解码器和可微分图构造网络(DGCN)，形成一个完整的从2D图像到3D骨架的生成流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过可微分框架联合优化3D骨架的几何和拓扑结构，使用分层图注意力机制捕捉多尺度特征，并通过谱拓扑优化保持结构一致性。整体流程是：首先通过图像处理模块将2D RGB图像转换为伪3D点云；然后使用增强的PointNet++编码器提取分层特征；接着通过骨架解码器预测初始关节位置；最后通过DGCN模块使用图注意力和光谱约束优化骨架拓扑，生成最终的3D骨架表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)分层图注意力机制实现多尺度骨架细化；(2)可微分谱拓扑优化通过拉普拉斯特征分解确保结构一致性；(3)对抗几何一致性训练使用双判别器实现姿态-结构对齐；(4)自适应骨架复杂性基于结构熵动态分配节点。相比之前工作，Cortex-Synth的主要不同在于实现了完全端到端可微分框架，不依赖固定拓扑先验，能够联合优化几何和拓扑，实验显示在ShapeNet上MPJPE提高18.7%，图编辑距离降低27.3%，拓扑错误减少42%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Cortex-Synth通过引入分层图注意力和可微分谱拓扑优化，实现了从单张2D图像到3D骨架的高质量端到端合成，显著提高了骨架几何和拓扑的准确性，为机器人操作、医学成像和计算机图形学等领域提供了更强大的3D理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Cortex Synth, a novel end-to-end differentiable framework forjoint 3D skeleton geometry and topology synthesis from single 2D images. Ourarchitecture introduces three key innovations: (1) A hierarchical graphattention mechanism with multi-scale skeletal refinement, (2) Differentiablespectral topology optimization via Laplacian eigen decomposition, and (3)Adversarial geometric consistency training for pose structure alignment. Theframework integrates four synergistic modules: a pseudo 3D point cloudgenerator, an enhanced PointNet encoder, a skeleton coordinate decoder, and anovel Differentiable Graph Construction Network (DGCN). Our experimentsdemonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and27.3 percent in Graph Edit Distance on ShapeNet, while reducing topologicalerrors by 42 percent compared to previous approaches. The model's end-to-enddifferentiability enables applications in robotic manipulation, medicalimaging, and automated character rigging.</description>
      <author>example@mail.com (Mohamed Zayaan S)</author>
      <guid isPermaLink="false">2509.06705v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>ISAC Imaging by Channel State Information using Ray Tracing for Next Generation 6G</title>
      <link>http://arxiv.org/abs/2509.06672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于信道状态信息的综合感知与通信成像框架，利用多径分量的角度和延迟信息实现三维物体重建，并通过两段反射点优化算法精确估计路径长度。&lt;h4&gt;背景&lt;/h4&gt;综合感知与通信作为第六代无线系统的基石技术，通过共享硬件、频谱和波形实现连接与环境映射的统一。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用CSI路径分量、发射器和接收器位置的ISAC成像框架，实现物体表面的精确几何重建。&lt;h4&gt;方法&lt;/h4&gt;从6.75 GHz频段的NYURay射线追踪器获取数据，提取可分辨多径分量并转换为三维反射点，采用两段反射点优化算法独立估计路径长度，最后聚合多对收发位置的反射点生成密集三维点云。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的ISAC成像框架能够准确重建物体表面、边缘和曲线特征，实现了多跳ISAC成像。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是首次使用6.75 GHz无线射线追踪实现多跳ISAC成像的演示。&lt;h4&gt;翻译&lt;/h4&gt;综合感知与通信正成为第六代无线系统的基石技术，通过共享硬件、频谱和波形统一连接和环境映射。本文提出了一种利用从校准的NYURay射线追踪器在6.75 GHz上频段获取的信道状态信息路径分量、发射器位置和接收器位置的ISAC成像框架。我们的研究展示了如何从CSI估计中提取每个可分辨的多径分量，并通过融合其角度和延迟信息将其转换为等效的三维反射点，这对于多跳反射是有用且具有挑战性的。论文的主要贡献是两段反射点优化算法，该算法独立估计从发射器位置和接收器位置到物体表面上等效反射点的路径长度，从而实现精确的几何重建。随后，我们聚合从多对发射器和接收器位置导出的等效反射点，生成表示信道中物体的密集三维点云。实验结果验证了所提出的ISAC成像框架能够准确重建物体表面、边缘和曲线特征。据我们所知，本文首次展示了使用6.75 GHz无线射线追踪进行多跳ISAC成像。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用无线信道状态信息(CSI)进行高精度3D环境重建，特别是处理多跳反射场景下的成像问题。这个问题在6G时代至关重要，因为ISAC(集成感知与通信)是6G的核心技术，需要通过共享硬件、频谱和波形来实现通信与环境感知的统一。高精度环境地图对自动驾驶、数字孪生和XR服务等6G关键应用不可或缺，而无线信号能穿透障碍物、全天候工作，弥补光学成像的不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统CSI成像方法通常假设单次反射，难以处理复杂物体中的多跳反射问题。他们提出将多跳反射路径抽象为'等效反射点'(ERPs)的概念，避免追踪每次反射的具体位置。设计了独立估计发射段和接收段路径长度的两段式优化算法。借鉴了NYURay射线追踪引擎、通信系统中的CSI估计技术、SAR成像思路以及点云处理中的几何滤波技术。通过多视角融合(MVF)整合不同发射-接收位置的数据，形成完整的3D点云。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂环境中的多跳反射路径抽象为单个等效反射点(ERP)，利用无线信道的角度和延迟信息重建物体几何形状，并通过多视角融合提高成像质量和覆盖率。实现流程：1)使用NYURay生成不同TX-RX位置的CSI数据；2)提取每条路径的六元组参数(角度、延迟、增益)；3)应用两段式反射点优化算法计算ERP；4)几何滤波剔除异常值；5)多视角融合生成密集3D点云；6)后处理优化点云质量，生成最终RF图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次实现多跳反射的ISAC成像；2)提出两段式反射点优化算法；3)开发多视角融合框架；4)在6.75GHz中频段验证方法；5)利用校准的NYURay生成高质量数据。相比之前工作：突破单次反射假设限制；在更高频段验证；仅利用通信系统CSI；能处理更复杂物体形状；通过多视角融合提供更完整3D重建，减少单视角盲区。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于信道状态信息的多跳反射等效反射点优化和多视角融合方法，实现了在6.75GHz频段上对复杂物体的高精度无线射频成像，为6G通信与感知一体化系统提供了新的环境感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated sensing and communications (ISAC) is emerging as a cornerstonetechnology for sixth generation (6G) wireless systems, unifying connectivityand environmental mapping through shared hardware, spectrum, and waveforms. Thefollowing paper presents an ISAC imaging framework utilizing channel stateinformation (CSI) per-path components, transmitter (TX) positions, and receiver(RX) positions obtained from the calibrated NYURay ray tracer at 6.75 GHz inthe upper mid-band. Our work shows how each resolvable multipath component canbe extracted from CSI estimation and cast into an equivalent three-dimensionalreflection point by fusing its angle and delay information, which is useful andchallenging for multi-bounce reflections. The primary contribution of the paperis the two-segment reflection point optimization algorithm, which independentlyestimates the path lengths from the TX position and RX position to anequivalent reflection point (ERP) on the object surface, thus enabling precisegeometric reconstruction. Subsequently, we aggregate the ERPs derived frommultiple pairs of TX and RX positions, generating dense three dimensional pointclouds representing the objects in the channel. Experimental results validatethat the proposed ISAC imaging framework accurately reconstructs objectsurfaces, edges, and curved features. To the best of our knowledge, this paperprovides the first demonstration of multi bounce ISAC imaging using wirelessray tracing at 6.75 GHz.</description>
      <author>example@mail.com (Ahmad Bazzi, Mingjun Ying, Ojas Kanhere, Theodore S. Rappaport, Marwa Chafii)</author>
      <guid isPermaLink="false">2509.06672v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods</title>
      <link>http://arxiv.org/abs/2509.06597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint of final paper that will appear in the Proceedings of the  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LiHRA，一个用于促进人类-机器人交互场景中自动化、基于学习或经典风险监控方法开发的新型数据集。&lt;h4&gt;背景&lt;/h4&gt;协作机器人在工业环境中日益普及，增加了对可靠安全系统的需求。然而，缺乏捕捉真实人类-机器人交互（包括潜在危险事件）的高质量数据集，阻碍了相关方法的开发。&lt;h4&gt;目的&lt;/h4&gt;创建LiHRA数据集，为人类-机器人交互的风险监控研究提供全面、多模态的数据支持。&lt;h4&gt;方法&lt;/h4&gt;LiHRA是一个综合的多模态数据集，结合了3D LiDAR点云、人体关键点和机器人关节状态，捕捉人类-机器人协作的完整空间和动态上下文。该数据集涵盖六种代表性HRI场景，包括协作和共存任务、物体传递和表面抛光，每种场景都有安全和危险版本，共包含4,431个标记的点云，以10Hz频率记录。&lt;h4&gt;主要发现&lt;/h4&gt;LiHAR数据集通过结合多种模态，能够精确跟踪人体运动、机器人动作和环境条件，实现协作任务中的准确风险监控。作者还展示了一种利用机器人状态和机器人动态模型来量化每个场景风险水平的方法。&lt;h4&gt;结论&lt;/h4&gt;LiHAR凭借其高分辨率LiDAR数据、精确的人体跟踪、机器人状态数据和真实的碰撞事件组合，为未来研究人类-机器人工作空间中的实时风险监控和自适应安全策略提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LiHRA，这是一个新型数据集，旨在促进人类-机器人交互场景中自动化、基于学习或经典风险监控方法的发展。工业环境中协作机器人的日益普及增加了对可靠安全系统的需求。然而，缺乏捕捉真实人类-机器人交互（包括潜在危险事件）的高质量数据集，减缓了开发进程。LiHAR通过提供结合3D LiDAR点云、人体关键点和机器人关节状态的全面多模态数据集，解决了这一挑战，捕捉了人类-机器人协作的完整空间和动态上下文。这种模态组合允许精确跟踪人体运动、机器人动作和环境条件，实现协作任务中的准确风险监控。LiHAR数据集涵盖六种代表性HRI场景，包括协作和共存任务、物体传递和表面抛光，每种场景都有安全和危险版本。该数据集共包含4,431个以10Hz频率记录的标记点云，为训练和评估经典和AI驱动的风险监控算法提供了丰富的资源。最后，为了展示LiHAR的实用性，我们介绍了一种量化每个场景随时间风险水平的方法。该方法利用上下文信息，包括机器人状态和机器人动态模型。凭借其高分辨率LiDAR数据、精确的人体跟踪、机器人状态数据和真实的碰撞事件组合，LiHAR为未来研究人类-机器人工作空间中的实时风险监控和自适应安全策略提供了重要基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决缺乏高质量数据集来捕捉真实人机交互场景（包括潜在危险事件）的问题。这个问题很重要，因为随着协作机器人在工业环境中的普及，对可靠安全系统的需求增加，而现有风险评估方法依赖于专家主观评估，难以应对人机交互的复杂性、预测人类行为的挑战以及碰撞临界值估计的困难，阻碍了自动化安全系统的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有数据集无法满足人机交互风险评估需求，然后选择3D LiDAR传感器作为核心感知方式（借鉴了自动驾驶领域的经验），结合HTC VIVE Tracker 3.0捕捉人体关键点和Franka Emika Robot记录机器人状态，构建了多模态数据集。设计过程中借鉴了ISO/TS 15066:2016安全标准，并利用现有文献中的经典方法估计外力，同时创新性地整合了多种传感器数据来捕捉人机交互的完整空间和动态上下文。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个结合3D LiDAR点云、人体关键点和机器人关节状态的多模态数据集，实现对人机协作场景的精确跟踪和风险监控。整体流程包括：1)使用专业硬件采集数据(LiDAR、运动捕捉系统和协作机器人)；2)对各传感器系统进行精确校准；3)记录六种不同场景(每种有安全/危险版本)；4)开发风险监控方法量化风险水平；5)通过外部力估计评估碰撞严重性；6)将数据导出为标准格式便于后续分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个集成LiDAR点云、机器人关节状态和人体关键点的数据集；2)包含六种代表性场景的安全和危险版本；3)提供精确标记的人机交互动态数据；4)提出基于安全标准的自动化风险监控方法。相比之前工作，LiHRA更全面(包含多种传感器数据)、更实用(包含危险场景)、更符合工业需求(使用3D LiDAR而非RGB相机)，解决了现有数据集要么缺乏机器人数据、要么缺乏危险场景、要么使用不合适传感器的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiHRA数据集通过整合多模态传感器数据和危险场景，为开发人机交互环境中的自动化风险评估和风险监控方法提供了全面且实用的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LiHRA, a novel dataset designed to facilitate the development ofautomated, learning-based, or classical risk monitoring (RM) methods forHuman-Robot Interaction (HRI) scenarios. The growing prevalence ofcollaborative robots in industrial environments has increased the need forreliable safety systems. However, the lack of high-quality datasets thatcapture realistic human-robot interactions, including potentially dangerousevents, slows development. LiHRA addresses this challenge by providing acomprehensive, multi-modal dataset combining 3D LiDAR point clouds, human bodykeypoints, and robot joint states, capturing the complete spatial and dynamiccontext of human-robot collaboration. This combination of modalities allows forprecise tracking of human movement, robot actions, and environmentalconditions, enabling accurate RM during collaborative tasks. The LiHRA datasetcovers six representative HRI scenarios involving collaborative and coexistenttasks, object handovers, and surface polishing, with safe and hazardousversions of each scenario. In total, the data set includes 4,431 labeled pointclouds recorded at 10 Hz, providing a rich resource for training andbenchmarking classical and AI-driven RM algorithms. Finally, to demonstrateLiHRA's utility, we introduce an RM method that quantifies the risk level ineach scenario over time. This method leverages contextual information,including robot states and the dynamic model of the robot. With its combinationof high-resolution LiDAR data, precise human tracking, robot state data, andrealistic collision events, LiHRA offers an essential foundation for futureresearch into real-time RM and adaptive safety strategies in human-robotworkspaces.</description>
      <author>example@mail.com (Frederik Plahl, Georgios Katranis, Ilshat Mamaev, Andrey Morozov)</author>
      <guid isPermaLink="false">2509.06597v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark</title>
      <link>http://arxiv.org/abs/2509.06456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种跨源点云配准方法，构建了大型多模态数据集并设计了基于重叠区域的配准框架，有效解决了跨源配准中的挑战，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;跨源点云配准是3D视觉中的基础任务，但与同源配准相比面临两大挑战：缺乏大规模真实世界数据集用于训练深度模型；不同传感器采集的点云存在固有差异，导致特征提取和匹配困难，影响配准精度。&lt;h4&gt;目的&lt;/h4&gt;推进跨源点云配准研究，通过构建大型数据集和创新框架解决现有挑战，实现准确且鲁棒的跨源点云配准。&lt;h4&gt;方法&lt;/h4&gt;构建了Cross3DReg数据集（目前最大真实世界多模态跨源点云配准数据集）；设计了基于重叠区域的跨源配准框架，利用未对齐图像预测点云重叠区域；提出了视觉-几何注意力引导的匹配模块，通过融合图像和几何信息建立可靠对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;所提方法实现了最先进的配准性能，相对旋转误差降低63.2%，相对平移误差降低40.2%，配准召回率提高5.4%，有效验证了跨源配准的准确性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架在实现准确的跨源配准方面是有效的，能够有效处理跨源点云配准中的挑战，显著提升配准精度和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;跨源点云配准旨在对齐来自不同传感器的点云数据，是3D视觉中的基础任务。然而，与同源点云配准相比，跨源配准面临两个核心挑战：缺乏用于训练深度配准模型的大规模真实世界公开数据集，以及多传感器捕获的点云存在固有差异。传感器引起的多样模式对鲁棒且准确的点云特征提取和匹配构成巨大挑战，进而影响配准精度。为推进该领域研究，我们构建了Cross3DReg，目前最大且真实世界的多模态跨源点云配准数据集，分别通过旋转机械激光雷达和混合半固态激光雷达采集。此外，我们设计了基于重叠区域的跨源配准框架，利用未对齐的图像预测源点云和目标点云之间的重叠区域，有效过滤掉无关区域的冗余点，并显著减轻非重叠区域噪声引起的干扰。然后，提出视觉-几何注意力引导的匹配模块，通过融合图像和几何信息增强跨源点云特征一致性，建立可靠对应关系，最终实现准确且鲁棒的配准。大量实验表明，我们的方法实现了最先进的配准性能。我们的框架将相对旋转误差和相对平移误差分别降低了63.2%和40.2%，并将配准召回率提高了5.4%，这验证了其在实现准确跨源配准方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决跨源点云配准问题，即对齐来自不同传感器（如旋转机械激光雷达和混合半固态激光雷达）的点云数据。这个问题在现实中非常重要，因为它是3D视觉的基础任务，在机器人导航、遥感测绘和自动驾驶定位等领域有广泛应用。然而，与同源点云配准相比，跨源配准面临缺乏大规模真实世界数据集和传感器差异导致的特征不一致两大挑战，这些问题限制了该领域的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到跨源点云配准的两个核心挑战：缺乏大规模真实世界数据集和传感器差异导致的特征不一致。为解决数据集问题，作者构建了Cross3DReg数据集；为解决配准精度问题，作者设计了基于重叠区域的配准框架。方法设计借鉴了多项现有工作：参考ImLoveNet预测重叠区域，使用KPConv-FPN提取点云特征，采用U-Net处理图像，并利用多模态特征融合和注意力机制增强特征一致性。作者通过融合视觉和几何信息，建立了可靠的点对应关系，实现了跨源点云的准确配准。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用未对齐的图像预测源点云和目标点云之间的重叠区域，有效过滤无关区域的冗余点，减轻非重叠区域噪声干扰；同时通过融合图像和几何信息，增强跨源点云特征一致性，建立可靠对应关系。整体流程分为四个阶段：1)特征提取，通过双分支网络提取图像和点云特征；2)重叠掩码预测(OMP)，融合图像和点云特征预测重叠区域；3)视觉-几何注意力引导的超点匹配(VGAM)，在重叠区域内建立对应关系；4)点匹配和配准，基于超点匹配获得点级对应关系并估计变换矩阵。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)构建了Cross3DReg，目前最大规模的真实世界跨源点云配准数据集，包含13,231个点云对；2)提出基于重叠区域的配准框架，利用未对齐图像预测点云重叠区域；3)设计视觉-几何注意力引导匹配模块，融合视觉和几何信息增强特征一致性。相比之前工作，不同之处在于：现有数据集规模小或主要使用合成数据，而Cross3DReg是真实世界大规模数据集；现有方法主要针对合成跨源设置设计，而作者方法专门针对真实世界跨源数据；实验显示作者方法在RRE和RTE上分别降低63.2%和40.2%，RR提高5.4%，性能显著提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了Cross3DReg大规模真实世界跨源点云配准数据集和一种基于重叠区域的配准方法，通过融合视觉和几何信息实现了跨源点云的高精度对齐。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-source point cloud registration, which aims to align point cloud datafrom different sensors, is a fundamental task in 3D vision. However, comparedto the same-source point cloud registration, cross-source registration facestwo core challenges: the lack of publicly available large-scale real-worlddatasets for training the deep registration models, and the inherentdifferences in point clouds captured by multiple sensors. The diverse patternsinduced by the sensors pose great challenges in robust and accurate point cloudfeature extraction and matching, which negatively influence the registrationaccuracy. To advance research in this field, we construct Cross3DReg, thecurrently largest and real-world multi-modal cross-source point cloudregistration dataset, which is collected by a rotating mechanical lidar and ahybrid semi-solid-state lidar, respectively. Moreover, we design anoverlap-based cross-source registration framework, which utilizes unalignedimages to predict the overlapping region between source and target pointclouds, effectively filtering out redundant points in the irrelevant regionsand significantly mitigating the interference caused by noise innon-overlapping areas. Then, a visual-geometric attention guided matchingmodule is proposed to enhance the consistency of cross-source point cloudfeatures by fusing image and geometric information to establish reliablecorrespondences and ultimately achieve accurate and robust registration.Extensive experiments show that our method achieves state-of-the-artregistration performance. Our framework reduces the relative rotation error(RRE) and relative translation error (RTE) by $63.2\%$ and $40.2\%$,respectively, and improves the registration recall (RR) by $5.4\%$, whichvalidates its effectiveness in achieving accurate cross-source registration.</description>
      <author>example@mail.com (Zongyi Xu, Zhongpeng Lang, Yilong Chen, Shanshan Zhao, Xiaoshui Huang, Yifan Zuo, Yan Zhang, Qianni Zhang, Xinbo Gao)</author>
      <guid isPermaLink="false">2509.06456v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Approximations of the mean curvature, and the Buet-Rumpf approximate mean curvature flow</title>
      <link>http://arxiv.org/abs/2509.06438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文推广了B. Buet和M. Rumpf关于变分流形的近似平均曲率向量定义及点云相关平均曲率运动的工作，提出了两种推广方式，并将结果扩展到近似第二基本形式，证明了点云在平均曲率运动中满足的额外比较原理。&lt;h4&gt;背景&lt;/h4&gt;基于B. Buet和M. Rumpf关于变分流形的近似平均曲率向量定义及点云相关平均曲率运动的研究。&lt;h4&gt;目的&lt;/h4&gt;推广变分流形的近似平均曲率向量定义及其相关平均曲率运动的工作。&lt;h4&gt;方法&lt;/h4&gt;通过线性算子和变分流形的正则性两种方式推广近似平均曲率向量定义，并将结果扩展到近似第二基本形式。&lt;h4&gt;主要发现&lt;/h4&gt;点云在平均曲率运动中满足一些额外的比较原理，包括离散和连续两种情况。&lt;h4&gt;结论&lt;/h4&gt;成功推广了近似平均曲率向量定义并扩展到近似第二基本形式，证明了点云运动的比较原理。&lt;h4&gt;翻译&lt;/h4&gt;本文的目的是推广B. Buet和M. Rumpf关于变分流形的近似平均曲率向量定义及其相关点云平均曲率运动的工作。我们提出了近似平均曲率向量定义的两种推广方式：通过线性算子和通过变分流形的正则性。然后我们将结果扩展到近似第二基本形式。最后，我们证明了点云在平均曲率运动中满足的一些额外比较原理（包括离散和连续情况）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The aim of this paper is to generalize the work of B. Buet and M. Rumpf onsome definition of the approximate mean curvature vector for varifolds, and itsassociated mean curvature motions for points clouds. We propose ageneralization of the definition of the approximate mean curvature vector intwo terms: in terms of linear operators and in terms of regularity of thevarifold. We then extend the results to the approximate second fundamentalform. Finally, we prove some additional comparison principles satisfied by themotion of points cloud by mean curvature (in the discrete and the continuouscases).</description>
      <author>example@mail.com (Abdelmouksit Sagueni)</author>
      <guid isPermaLink="false">2509.06438v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap</title>
      <link>http://arxiv.org/abs/2509.06329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文系统性地解决了植物3D分割领域的三大挑战，提供了现有数据集概述，总结了深度学习方法，开发了Plant Segmentation Studio开源框架，并通过实验验证了有效策略，为研究人员提供了实用工具和路线图。&lt;h4&gt;背景&lt;/h4&gt;植物形态学的精确表征为植物环境相互作用和遗传进化研究提供了有价值的见解。3D分割技术是从复杂点云中提取植物器官信息的关键技术，但在植物表型分析中的应用仍面临三大挑战：大规模标注数据集稀缺、难以将先进深度神经网络适应到植物点云、缺乏针对植物科学的标准基准和评估协议。&lt;h4&gt;目的&lt;/h4&gt;系统性地解决植物3D分割领域面临的三大挑战，提供现有3D植物数据集概述，总结基于深度学习的点云分割方法，介绍Plant Segmentation Studio开源框架，并评估代表性网络和模拟到现实的学习策略。&lt;h4&gt;方法&lt;/h4&gt;提供现有3D植物数据集的概述；系统总结基于深度学习的点云语义和实例分割方法；介绍Plant Segmentation Studio开源框架；进行广泛的定量实验评估代表性网络和模拟到现实的学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;稀疏卷积主干和基于Transformer的实例分割方法有效；基于建模和基于增强的合成数据生成在模拟到现实学习中具有互补作用，可以减少标注需求。&lt;h4&gt;结论&lt;/h4&gt;这项研究弥合了算法进步与实际部署之间的差距，为研究人员提供了即用工具，并为开发数据高效且可泛化的3D植物表型深度学习解决方案提供了路线图。&lt;h4&gt;翻译&lt;/h4&gt;植物形态学的精确表征为植物环境相互作用和遗传进化研究提供了有价值的见解。提取这一信息的关键技术是3D分割技术，它可以从复杂点云中勾勒出单个植物器官。尽管在一般3D计算机视觉领域取得了显著进展，但3D分割在植物表型分析中的应用仍然受到三大挑战的限制：大规模标注数据集的稀缺性；将先进的深度神经网络适应到植物点云的技术困难；缺乏针对植物科学的标准基准和评估协议。这篇论文通过以下方式系统性地解决了这些障碍：在一般3D分割领域的背景下提供现有3D植物数据集的概述；系统总结基于深度学习的点云语义和实例分割方法；介绍Plant Segmentation Studio开源框架；进行广泛的定量实验以评估代表性网络和模拟到现实的学习策略。我们的研究突出了稀疏卷积主干和基于Transformer的实例分割的有效性，同时强调基于建模和基于增强的合成数据生成在模拟到现实学习中的互补作用，可以减少标注需求。总体而言，这项研究弥合了算法进步与实际部署之间的差距，为研究人员提供了即用工具，并为开发数据高效且可泛化的3D植物表型深度学习解决方案提供了路线图。数据和代码可在https://github.com/perrydoremi/PlantSegStudio获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决植物器官级别3D分割规模化应用面临的三大挑战：大规模标注数据集稀缺、将先进深度神经网络适应植物点云的技术困难、以及缺乏针对植物科学定制的标准化基准和评估协议。这个问题在现实中非常重要，因为精确的植物形态表征对理解植物-环境相互作用和遗传进化至关重要，而器官级别的3D分割是提取这些形态信息的关键技术。解决这些问题可以促进植物表型分析的发展，支持农业可持续性和韧性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用'数据-算法-计算'三角框架系统性地解决问题：在数据层面，提供3D植物数据集概述并讨论真实数据采集和合成数据生成方法；在算法层面，总结深度学习点云分割方法并分析不同策略；在计算层面，开发Plant Segmentation Studio框架。作者确实借鉴了现有工作，特别是基于MMDetection3D框架进行定制，利用现有的深度学习架构如PointNet、3D CNN和Transformer，并结合程序建模和增强方法生成合成数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'数据-算法-计算'三角框架的系统整合，解决植物器官级别3D分割的三大挑战。整体流程包括：1)数据收集与处理，使用多种3D成像技术收集植物点云并进行标准化处理；2)算法设计与优化，设计适合植物点云特性的分割算法并优化神经网络架构；3)计算框架构建，开发PSS框架实现数据准备、算法集成和简化推理；4)评估与优化，在多个数据集上进行定量实验并优化算法参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'数据-算法-计算'三角框架系统性解决问题；2)全面分析3D植物数据集和合成数据生成方法；3)系统总结适合植物点云的深度学习方法，特别关注Transformer-based方法；4)开发首个针对3D植物表型分析的标准化框架PSS。相比之前工作，本文的不同之处在于全面性（整合三个层面而非单一层面）、针对性（专门针对植物器官级别需求）、实用性（提供可用工具框架）、定量评估（进行广泛实验）和开放性（提供开源代码）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性整合数据、算法和计算三个层面，开发了一个开源框架(PSS)来弥合植物器官级别3D分割中的数据-算法-计算差距，为研究人员提供了可扩展的植物表型分析工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The precise characterization of plant morphology provides valuable insightsinto plant environment interactions and genetic evolution. A key technology forextracting this information is 3D segmentation, which delineates individualplant organs from complex point clouds. Despite significant progress in general3D computer vision domains, the adoption of 3D segmentation for plantphenotyping remains limited by three major challenges: i) the scarcity oflarge-scale annotated datasets, ii) technical difficulties in adapting advanceddeep neural networks to plant point clouds, and iii) the lack of standardizedbenchmarks and evaluation protocols tailored to plant science. This reviewsystematically addresses these barriers by: i) providing an overview ofexisting 3D plant datasets in the context of general 3D segmentation domains,ii) systematically summarizing deep learning-based methods for point cloudsemantic and instance segmentation, iii) introducing Plant Segmentation Studio(PSS), an open-source framework for reproducible benchmarking, and iv)conducting extensive quantitative experiments to evaluate representativenetworks and sim-to-real learning strategies. Our findings highlight theefficacy of sparse convolutional backbones and transformer-based instancesegmentation, while also emphasizing the complementary role of modeling-basedand augmentation-based synthetic data generation for sim-to-real learning inreducing annotation demands. In general, this study bridges the gap betweenalgorithmic advances and practical deployment, providing immediate tools forresearchers and a roadmap for developing data-efficient and generalizable deeplearning solutions in 3D plant phenotyping. Data and code are available athttps://github.com/perrydoremi/PlantSegStudio.</description>
      <author>example@mail.com (Ruiming Du, Guangxun Zhai, Tian Qiu, Yu Jiang)</author>
      <guid isPermaLink="false">2509.06329v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration</title>
      <link>http://arxiv.org/abs/2509.06285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 19 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出DCReg框架，通过三个集成创新系统性解决LiDAR点云配准中的病态条件问题，显著提高定位精度和计算效率。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云配准是机器人感知和导航的基础，但在几何退化或狭窄环境中，配准问题会变成病态条件，导致解决方案不稳定和精度降低。现有方法未能准确检测、解释和解决这种病态条件问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够系统性解决病态配准问题的框架，通过准确检测、解释和解决病态条件，提高配准的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;DCReg框架包含三个创新：1) 使用Hessian矩阵的Schur补分解实现可靠的病态条件检测，将配准问题解耦为干净的旋转和平移子空间；2) 开发定量表征技术，建立数学特征空间与物理运动方向之间的明确映射；3) 设计新型预处理器，仅稳定识别出的病态方向，同时保留所有良好约束信息。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，DCReg在各种环境中实现了比最先进方法高20%-50%的定位精度，并且速度提高了5-100倍。&lt;h4&gt;结论&lt;/h4&gt;DCReg提供了一个有效且高效的解决方案，能够处理LiDAR点云配准中的病态条件问题，显著提高了定位精度和计算效率，代码将在https://github.com/JokerJohn/DCReg上提供。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR点云配准是机器人感知和导航的基础。然而，在几何退化或狭窄环境中，配准问题会变成病态条件，导致解决方案不稳定和精度降低。虽然现有方法试图处理这些问题，但它们未能解决核心挑战：准确检测、解释和解决这种病态条件，导致漏检或损坏的解决方案。在本研究中，我们引入了DCReg，一个通过三个集成创新系统性解决病态配准问题的原则性框架。首先，DCReg通过将Schur补分解应用于Hessian矩阵，实现可靠的病态条件检测。该技术将配准问题解耦为干净的旋转和平移子空间，消除了传统分析中掩盖退化模式的耦合效应。其次，在这些干净的子空间中，我们开发了定量表征技术，建立了数学特征空间与物理运动方向之间的明确映射，提供了关于哪些特定运动缺乏约束的可操作见解。最后，利用这个干净的子空间，我们设计了一个有针对性的缓解策略：一种新型预处理器，选择性稳定仅识别出的病态方向，同时保留可观测空间中的所有良好约束信息。这通过具有单一可物理解释参数的预共轭梯度方法实现了高效且稳健的优化。广泛的实验表明，DCReg在各种环境中实现了比最先进方法高20%-50%的定位精度，并且速度提高了5-100倍。我们的实现在https://github.com/JokerJohn/DCReg上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光雷达点云配准在几何退化或狭窄环境（如走廊、隧道）中的病态问题。在这些环境中，由于重复结构或稀疏特征，系统缺乏沿特定运动方向的充分几何约束，导致信息矩阵接近奇异，使优化问题变得病态。这个问题在现实中非常重要，因为它是现代自主导航系统（如自动驾驶车辆、机器人）可靠性的关键瓶颈，在这些环境中即使微小的传感器误差或初始估计偏差也可能导致定位完全失败，严重影响实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有方法的三个根本局限性出发：1) 检测时机问题：传统方法分析Hessian谱时，旋转和平移间的尺度差异和耦合效应掩盖了关键病态模式；2) 失败原因问题：即使检测到病态，也无法解释哪些物理运动方向是退化的；3) 缓解策略问题：现有方法（如正则化、截断）不加选择地修改优化问题。作者借鉴了Schur补分解处理块矩阵耦合效应的思想，以及预调节共轭梯度(PCG)优化框架，但创新性地将它们组合成一个统一框架，通过解耦表征来系统解决病态配准问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦表征（Decoupled Characterization）处理病态配准问题，使用Schur补分解将旋转和平移参数解耦到干净子空间中，消除耦合效应，然后进行定量表征和针对性缓解。整体流程分为三步：1) 病态检测：使用Schur补分解分析旋转和平移子空间，计算归一化特征值识别病态方向；2) 定量表征：通过内积匹配解决符号歧义，最大分量分析解决排序歧义，Gram-Schmidt正交化产生稳定正交基；3) 针对性缓解：设计基于PCG的求解器，使用特征值钳制策略只稳定病态方向，保持良好约束方向的自然收敛。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 尺度鲁棒且耦合感知的病态检测，使用Schur补分解正确处理旋转-平移耦合；2) 物理可解释的方向性病态分析，建立数学特征空间和物理运动方向的明确映射；3) 针对性病态缓解求解器，使用单一参数控制条件数。与传统方法不同：检测上，传统方法忽略耦合效应，DCReg揭示隐藏退化；表征上，传统方法错误假设直接对应，DCReg提供精确映射；缓解上，传统方法不加选择修改问题，DCReg保持原始问题完整性；整体上，传统方法将检测和缓解视为分离步骤，DCReg提供统一框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DCReg通过解耦表征方法，实现了在退化环境中激光雷达点云配准的可靠病态检测、精确物理表征和高效稳定优化，显著提升了定位精度和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point cloud registration is fundamental to robotic perception andnavigation. However, in geometrically degenerate or narrow environments,registration problems become ill-conditioned, leading to unstable solutions anddegraded accuracy. While existing approaches attempt to handle these issues,they fail to address the core challenge: accurately detection, interpret, andresolve this ill-conditioning, leading to missed detections or corruptedsolutions. In this study, we introduce DCReg, a principled framework thatsystematically addresses the ill-conditioned registration problems throughthree integrated innovations. First, DCReg achieves reliable ill-conditioningdetection by employing a Schur complement decomposition to the hessian matrix.This technique decouples the registration problem into clean rotational andtranslational subspaces, eliminating coupling effects that mask degeneracypatterns in conventional analyses. Second, within these cleanly subspaces, wedevelop quantitative characterization techniques that establish explicitmappings between mathematical eigenspaces and physical motion directions,providing actionable insights about which specific motions lack constraints.Finally, leveraging this clean subspace, we design a targeted mitigationstrategy: a novel preconditioner that selectively stabilizes only theidentified ill-conditioned directions while preserving all well-constrainedinformation in observable space. This enables efficient and robust optimizationvia the Preconditioned Conjugate Gradient method with a single physicalinterpretable parameter. Extensive experiments demonstrate DCReg achieves atleast 20% - 50% improvement in localization accuracy and 5-100 times speedupover state-of-the-art methods across diverse environments. Our implementationwill be available at https://github.com/JokerJohn/DCReg.</description>
      <author>example@mail.com (Xiangcheng Hu, Xieyuanli Chen, Mingkai Jia, Jin Wu, Ping Tan, Steven L. Waslander)</author>
      <guid isPermaLink="false">2509.06285v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud</title>
      <link>http://arxiv.org/abs/2509.05954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StripDet是一种轻量级3D目标检测框架，通过创新的条带注意力模块和分层骨干网络，有效解决了高精度3D目标检测模型在计算和内存需求方面的挑战，在保持高精度的同时大幅减少了参数量。&lt;h4&gt;背景&lt;/h4&gt;高精度3D目标检测模型的部署面临重大挑战，因为它们需要大量的计算和内存资源。&lt;h4&gt;目的&lt;/h4&gt;介绍StripDet，一个为设备端效率设计的新型轻量级框架。&lt;h4&gt;方法&lt;/h4&gt;提出了新的条带注意力模块（SAB），通过将标准2D卷积分解为非对称条带卷积，高效提取方向特征并降低计算复杂度；设计了一个硬件友好的分层骨干网络，将SAB与深度可分离卷积和简单的多尺度融合策略相结合。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI数据集上，仅使用0.65M参数，StripDet在汽车检测上达到79.97%的mAP，超过了基准PointPillars，参数减少了7倍；优于最近的轻量级和基于知识蒸馏的方法，实现了更好的精度-效率权衡。&lt;h4&gt;结论&lt;/h4&gt;StripDet成为边缘设备上实际3D检测的可行解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将高精度3D目标检测模型从点云中部署仍然是一个重大挑战，因为它们需要大量的计算和内存资源。为此，我们引入了StripDet，一种专为设备端效率设计的新型轻量级框架。首先，我们提出了新型的条带注意力模块（SAB），这是一个为捕获长程空间依赖关系而设计的高效模块。通过将标准2D卷积分解为非对称条带卷积，SAB能够高效提取方向特征，同时将计算复杂度从二次方降低到线性。其次，我们设计了一个硬件友好的分层骨干网络，将SAB与深度可分离卷积和简单的多尺度融合策略相结合，实现了端到端的效率。在KITTI数据集上的大量实验验证了StripDet的优越性。仅使用0.65M参数，我们的模型在汽车检测上达到了79.97%的mAP，超过了基准PointPillars，参数减少了7倍。此外，StripDet优于最近的轻量级和基于知识蒸馏的方法，实现了更好的精度-效率权衡，同时成为边缘设备上实际3D检测的可行解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of high-accuracy 3D object detection models from point cloudremains a significant challenge due to their substantial computational andmemory requirements. To address this, we introduce StripDet, a novellightweight framework designed for on-device efficiency. First, we propose thenovel Strip Attention Block (SAB), a highly efficient module designed tocapture long-range spatial dependencies. By decomposing standard 2Dconvolutions into asymmetric strip convolutions, SAB efficiently extractsdirectional features while reducing computational complexity from quadratic tolinear. Second, we design a hardware-friendly hierarchical backbone thatintegrates SAB with depthwise separable convolutions and a simple multiscalefusion strategy, achieving end-to-end efficiency. Extensive experiments on theKITTI dataset validate StripDet's superiority. With only 0.65M parameters, ourmodel achieves a 79.97% mAP for car detection, surpassing the baselinePointPillars with a 7x parameter reduction. Furthermore, StripDet outperformsrecent lightweight and knowledge distillation-based methods, achieving asuperior accuracy-efficiency trade-off while establishing itself as a practicalsolution for real-world 3D detection on edge devices.</description>
      <author>example@mail.com (Weichao Wang, Wendong Mao, Zhongfeng Wang)</author>
      <guid isPermaLink="false">2509.05954v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Music Autotagging with MGPHot Expert Annotations vs. Generic Tag Datasets</title>
      <link>http://arxiv.org/abs/2509.06936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了基于MGPHot数据集的新音乐自动标注基准测试框架，包含专家音乐学注释、可检索音频数据集、标准化评估分割和预计算模型表示，并比较了专家与通用标签注释的差异。&lt;h4&gt;背景&lt;/h4&gt;音乐自动标注是自动为音频分配描述性标签的任务，因其挑战性、语义描述多样性和实际应用价值，已成为评估通用音乐表示性能的常见下游任务。MGPHot数据集包含专家音乐学注释，但原始数据集缺乏音频和标准化评估设置。&lt;h4&gt;目的&lt;/h4&gt;解决MGPHot数据集缺乏音频和标准化评估设置的问题，为音乐理解研究提供更先进的基准测试框架。&lt;h4&gt;方法&lt;/h4&gt;提供一组可检索的YouTube URL音频，提出train/val/test标准化评估分割，为七种最先进模型预计算表示，并在MGPHot和标准参考标签数据集上评估这些模型。&lt;h4&gt;主要发现&lt;/h4&gt;专家音乐学注释与通用标签注释之间存在关键差异，通过新基准测试框架揭示了这些差异，为音乐理解研究提供了更深入的见解。&lt;h4&gt;结论&lt;/h4&gt;所提出的新基准测试框架为未来音乐理解研究提供了更先进的评估工具，有助于推动音乐自动标注领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;音乐自动标注旨在为音频记录自动分配描述性标签，如流派、情绪或乐器等。由于其挑战性、语义描述的多样性以及在各种应用中的实际价值，它已成为评估从音频数据学习到的通用音乐表示性能的常见下游任务。我们介绍了一个基于最近发布的MGPHot数据集的新基准测试数据集，该数据集包含专家音乐学注释，允许与在通用标签数据集上获得的结果进行额外的见解和比较。虽然MGPHot注释已被证明对计算音乐学有用，但原始数据集既不包括音频，也没有提供将其用作标准化自动标注基准的评估设置。为了解决这个问题，我们提供了一组精心挑选的YouTube URL，可检索音频，并提出了用于标准化评估的train/val/test分割，以及为七种最先进模型预先计算的表示。利用这些资源，我们在MGPHot和标准参考标签数据集上评估了这些模型，突出了专家和通用标签注释之间的关键差异。总之，我们的贡献为未来音乐理解研究提供了一个更先进的基准测试框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Music autotagging aims to automatically assign descriptive tags, such asgenre, mood, or instrumentation, to audio recordings. Due to its challenges,diversity of semantic descriptions, and practical value in variousapplications, it has become a common downstream task for evaluating theperformance of general-purpose music representations learned from audio data.We introduce a new benchmarking dataset based on the recently published MGPHotdataset, which includes expert musicological annotations, allowing foradditional insights and comparisons with results obtained on common generic tagdatasets. While MGPHot annotations have been shown to be useful forcomputational musicology, the original dataset neither includes audio norprovides evaluation setups for its use as a standardized autotagging benchmark.To address this, we provide a curated set of YouTube URLs with retrievableaudio, and propose a train/val/test split for standardized evaluation, andprecomputed representations for seven state-of-the-art models. Using theseresources, we evaluated these models in MGPHot and standard reference tagdatasets, highlighting key differences between expert and generic tagannotations. Altogether, our contributions provide a more advanced benchmarkingframework for future research in music understanding.</description>
      <author>example@mail.com (Pedro Ramoneda, Pablo Alonso-Jimenez, Sergio Oramas, Xavier Serra, Dmitry Bogdanov)</author>
      <guid isPermaLink="false">2509.06936v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition</title>
      <link>http://arxiv.org/abs/2509.06570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, 2025 IEEE/CVF International Conference on  Computer Vision Workshops&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为保留性角表示学习（RARL）的方法，用于解决增量开放集识别（IOSR）问题。该方法通过在角度空间中对齐未知表示和采用虚拟内在交互训练策略，有效减轻了知识更新过程中的表示漂移和类别间混淆问题。&lt;h4&gt;背景&lt;/h4&gt;现有的开放集识别方法通常针对静态场景，模型只能分类已知类别并识别固定范围内的未知类别，无法从连续数据流中增量识别新出现的未知类别并获取相应知识。&lt;h4&gt;目的&lt;/h4&gt;解决在动态场景中由于无法访问之前训练数据导致的开放集识别决策边界区分性难以维持、严重类别间混淆的问题。&lt;h4&gt;方法&lt;/h4&gt;提出保留性角表示学习（RARL）方法，包括：1) 在等角紧框架下构建的角度空间内，鼓励未知表示围绕非活跃原型对齐；2) 采用虚拟内在交互（VII）训练策略，通过边界接近虚拟类强制清晰的类间边界；3) 设计分层校正策略优化决策边界，减轻样本不平衡引起的表示偏差和特征空间扭曲。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR100和TinyImageNet数据集上的实验结果表明，所提出的RARL方法在各种任务设置下都达到了最先进的性能，为IOSR建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;RARL方法有效解决了增量开放集识别中的表示漂移和类别混淆问题，能够更好地适应动态场景下的开放集识别任务。&lt;h4&gt;翻译&lt;/h4&gt;现有的开放集识别方法通常设计用于静态场景，模型旨在分类已知类别并识别固定范围内的未知类别。这与模型应能从连续数据流中增量识别新出现的未知类别并获取相应知识的期望不符。在这种动态场景中，由于无法访问之前的训练数据，开放集识别决策边界的区分性难以维持，导致严重的类别间混淆。为解决这一问题，我们提出了用于增量开放集识别的保留性角表示学习（RARL）。在RARL中，未知表示被鼓励在等角紧框架下构建的角度空间内围绕非活跃原型对齐，从而减轻知识更新过程中的过度表示漂移。具体来说，我们采用虚拟内在交互（VII）训练策略，通过边界接近虚拟类强制清晰的类间边界，压缩已知表示。此外，还设计了一种分层校正策略来优化决策边界，减轻由旧/新类别和正/负类别样本不平衡引起的表示偏差和特征空间扭曲。我们在CIFAR100和TinyImageNet数据集上进行了全面评估，为IOSR建立了新的基准。各种任务设置下的实验结果表明，所提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing open set recognition (OSR) methods are typically designed for staticscenarios, where models aim to classify known classes and identify unknown oneswithin fixed scopes. This deviates from the expectation that the model shouldincrementally identify newly emerging unknown classes from continuous datastreams and acquire corresponding knowledge. In such evolving scenarios, thediscriminability of OSR decision boundaries is hard to maintain due torestricted access to former training data, causing severe inter-classconfusion. To solve this problem, we propose retentive angular representationlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknownrepresentations are encouraged to align around inactive prototypes within anangular space constructed under the equiangular tight frame, thereby mitigatingexcessive representation drift during knowledge updates. Specifically, we adopta virtual-intrinsic interactive (VII) training strategy, which compacts knownrepresentations by enforcing clear inter-class margins throughboundary-proximal virtual classes. Furthermore, a stratified rectificationstrategy is designed to refine decision boundaries, mitigating representationbias and feature space distortion caused by imbalances between old/new andpositive/negative class samples. We conduct thorough evaluations on CIFAR100and TinyImageNet datasets and establish a new benchmark for IOSR. Experimentalresults across various task setups demonstrate that the proposed methodachieves state-of-the-art performance.</description>
      <author>example@mail.com (Runqing Yang, Yimin Fu, Changyuan Wu, Zhunga Liu)</author>
      <guid isPermaLink="false">2509.06570v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix</title>
      <link>http://arxiv.org/abs/2509.06314v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个名为rho(C)的冗余指数，用于直接量化表示学习中潜在嵌入的维度间依赖关系，帮助识别冗余并提高表示效率。&lt;h4&gt;背景&lt;/h4&gt;深度网络产生的潜在空间常包含冗余信息，多个坐标编码重叠内容，降低了模型容量和泛化能力，而现有标准指标无法直接检测这种冗余。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接量化潜在表示中冗余的方法，以评估和改进学习表示的效率。&lt;h4&gt;方法&lt;/h4&gt;引入冗余指数rho(C)，通过分析潜在表示的耦合矩阵，使用能量距离比较非对角线统计量与正态分布的差异，直接测量维度间依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;低rho(C)与高分类准确率和低重建误差相关；冗余增加与性能崩溃相关；估计器可靠性随潜在维度增加；TPE方法倾向于探索低冗余区域。&lt;h4&gt;结论&lt;/h4&gt;rho(C)作为冗余的通用指标，为评估和改进学习表示的效率提供了理论视角和实用工具，可用于指导神经架构搜索和作为正则化目标。&lt;h4&gt;翻译&lt;/h4&gt;摘要：表示学习中的一个核心挑战是构建既具有表达力又高效的潜在嵌入。在实践中，深度网络常常产生冗余的潜在空间，其中多个坐标编码重叠信息，降低了有效容量并阻碍了泛化能力。标准指标如准确率或重建损失仅提供此类冗余的间接证据，无法将其隔离为一种失败模式。我们引入了一个冗余指数，表示为rho(C)，它通过分析从潜在表示推导的耦合矩阵，并通过能量距离将其非对角线统计量与正态分布进行比较，直接量化了维度间的依赖关系。结果是一个紧凑、可解释且具有统计基础的表示质量度量。我们在MNIST变体、Fashion-MNIST、CIFAR-10和CIFAR-100上的判别性和生成性设置中验证了rho(C)，涵盖了多种架构和超参数优化策略。经验表明，低rho(C)可靠地预测高分类准确率或低重建误差，而冗余增加与性能崩溃相关。估计器的可靠性随潜在维度增加而提高，为可靠分析提供了自然下限。我们还表明，树结构Parzen估计器（TPE）倾向于探索低rho区域，表明rho(C)可以指导神经架构搜索并作为冗余感知的正则化目标。通过将冗余暴露为跨模型和任务的普遍瓶颈，rho(C)为评估和改进学习表示的效率提供了理论视角和实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A central challenge in representation learning is constructing latentembeddings that are both expressive and efficient. In practice, deep networksoften produce redundant latent spaces where multiple coordinates encodeoverlapping information, reducing effective capacity and hinderinggeneralization. Standard metrics such as accuracy or reconstruction lossprovide only indirect evidence of such redundancy and cannot isolate it as afailure mode. We introduce a redundancy index, denoted rho(C), that directlyquantifies inter-dimensional dependencies by analyzing coupling matricesderived from latent representations and comparing their off-diagonal statisticsagainst a normal distribution via energy distance. The result is a compact,interpretable, and statistically grounded measure of representational quality.We validate rho(C) across discriminative and generative settings on MNISTvariants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiplearchitectures and hyperparameter optimization strategies. Empirically, lowrho(C) reliably predicts high classification accuracy or low reconstructionerror, while elevated redundancy is associated with performance collapse.Estimator reliability grows with latent dimension, yielding natural lowerbounds for reliable analysis. We further show that Tree-structured ParzenEstimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)can guide neural architecture search and serve as a redundancy-awareregularization target. By exposing redundancy as a universal bottleneck acrossmodels and tasks, rho(C) offers both a theoretical lens and a practical toolfor evaluating and improving the efficiency of learned representations.</description>
      <author>example@mail.com (Mehmet Can Yavuz, Berrin Yanikoglu)</author>
      <guid isPermaLink="false">2509.06314v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
      <link>http://arxiv.org/abs/2509.06165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UNO（统一对象中心视频场景图生成），一个单阶段统一框架，能够同时处理粗粒度框级别和细粒度全景像素级别的视频场景图生成任务，通过扩展的slot注意力机制、对象时间一致性学习和动态三元组预测模块实现高效的对象时序交互建模。&lt;h4&gt;背景&lt;/h4&gt;现有视频场景图生成研究通常只针对粗粒度框级别或细粒度全景像素级别中的一种任务，需要特定的架构和多阶段训练流程，缺乏能够同时处理这两种粒度级别的统一方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个单阶段、统一的框架，在一个端到端架构中同时处理框级别和像素级别的VidSGG任务，最小化任务特定修改，最大化参数共享，实现不同视觉粒度级别上的泛化。&lt;h4&gt;方法&lt;/h4&gt;UNO框架采用扩展的slot注意力机制将视觉特征分解为对象和关系slot；引入对象时间一致性学习强制跨帧保持一致的对象表示；使用动态三元组预测模块将关系slot链接到相应的对象对，捕捉随时间变化的交互。&lt;h4&gt;主要发现&lt;/h4&gt;UNO在标准框级别和像素级别VidSGG基准测试上取得了具有竞争力的性能，同时通过统一的对象中心设计提供了更高的效率。&lt;h4&gt;结论&lt;/h4&gt;UNO不仅能够在两种任务上实现竞争性性能，还通过统一的对象中心设计提高了效率，证明了统一框架处理不同视觉粒度级别视频场景图生成的可行性。&lt;h4&gt;翻译&lt;/h4&gt;视频场景图生成(VidSGG)旨在通过检测对象并将其时间交互建模为结构化图来表示动态视觉内容。先前的研究通常针对粗粒度框级别或细粒度全景像素级别的VidSGG，通常需要特定的架构和多阶段训练流程。在本文中，我们提出了UNO（统一对象中心VidSGG），这是一个单阶段统一框架，在一个端到端架构中共同解决这两个任务。UNO旨在最小化任务特定的修改并最大化参数共享，实现在不同视觉粒度级别上的泛化。UNO的核心是一个扩展的slot注意力机制，它将视觉特征分解为对象和关系slot。为确保鲁棒的时序建模，我们引入了对象时间一致性学习，强制跨帧保持一致的对象表示，而不依赖显式跟踪模块。此外，动态三元组预测模块将关系slot链接到相应的对象对，捕捉随时间变化的交互。我们在标准的框级别和像素级别VidSGG基准上评估了UNO。结果表明，UNO不仅在两个任务上都取得了具有竞争力的性能，还通过统一的对象中心设计提供了更高的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Scene Graph Generation (VidSGG) aims to represent dynamic visualcontent by detecting objects and modeling their temporal interactions asstructured graphs. Prior studies typically target either coarse-grainedbox-level or fine-grained panoptic pixel-level VidSGG, often requiringtask-specific architectures and multi-stage training pipelines. In this paper,we present UNO (UNified Object-centric VidSGG), a single-stage, unifiedframework that jointly addresses both tasks within an end-to-end architecture.UNO is designed to minimize task-specific modifications and maximize parametersharing, enabling generalization across different levels of visual granularity.The core of UNO is an extended slot attention mechanism that decomposes visualfeatures into object and relation slots. To ensure robust temporal modeling, weintroduce object temporal consistency learning, which enforces consistentobject representations across frames without relying on explicit trackingmodules. Additionally, a dynamic triplet prediction module links relation slotsto corresponding object pairs, capturing evolving interactions over time. Weevaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Resultsdemonstrate that UNO not only achieves competitive performance across bothtasks but also offers improved efficiency through a unified, object-centricdesign.</description>
      <author>example@mail.com (Huy Le, Nhat Chung, Tung Kieu, Jingkang Yang, Ngan Le)</author>
      <guid isPermaLink="false">2509.06165v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Micro-Expression Recognition via Fine-Grained Dynamic Perception</title>
      <link>http://arxiv.org/abs/2509.06015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新颖的细粒度动态感知(FDP)框架用于面部微表情识别，通过帧级特征排序和动态图像构建任务，有效捕捉微表情的动态信息，解决了现有方法在特征提取和数据限制方面的问题，并在多个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;面部微表情识别是一项具有挑战性的任务，因为微表情具有短暂、细微和动态的特性。现有方法大多依赖手工设计特征或深度网络，前者通常需要关键帧，后者则受限于小规模和低多样性的训练数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的微表情识别框架，能够有效捕捉微表情的动态信息，解决现有方法在特征提取和数据限制方面的问题。&lt;h4&gt;方法&lt;/h4&gt;提出细粒度动态感知(FDP)框架，包括：按时间顺序对原始帧序列的帧级特征进行排序，编码微表情出现和运动的动态信息；提出局部-全局特征感知transformer进行帧表示学习；采用排序评分器计算每个帧级特征的排序分数；在时间维度上对排序特征进行池化，捕获动态表示；将动态表示共享给微表情识别模块和动态图像构建模块。&lt;h4&gt;主要发现&lt;/h4&gt;提出的FDP方法显著超越了最先进的微表情识别方法；在CASME II、SAMM、CAS(ME)^2和CAS(ME)^3数据集上，FDP的F1分数分别比之前最好的结果提高了4.05%、2.50%、7.71%和2.11%；动态图像构建任务表现良好。&lt;h4&gt;结论&lt;/h4&gt;细粒度动态感知框架有效解决了微表情识别中的挑战，通过帧级特征排序和动态图像构建任务，能够更好地捕捉微表情的动态信息，缓解数据稀缺问题，并在多个数据集上取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;面部微表情识别是一项具有挑战性的任务，因为微表情具有短暂、细微和动态的特性。大多数现有方法依赖于手工设计特征或深度网络，其中前者通常需要关键帧，而后者则受限于小规模和低多样性的训练数据。在本文中，我们为微表情识别开发了一种新颖的细粒度动态感知(FDP)框架。我们提出按时间顺序对原始帧序列的帧级特征进行排序，其中排序过程编码了微表情出现和运动的动态信息。具体来说，提出了一种新颖的局部-全局特征感知transformer用于帧表示学习。进一步采用排序评分器计算每个帧级特征的排序分数。之后，从排序评分器中提取的排序特征在时间维度上进行池化，以捕获动态表示。最后，动态表示被微表情识别模块和动态图像构建模块共享，前者预测微表情类别，后者使用编码器-解码器结构构建动态图像。动态图像构建任务的设计有助于捕捉与微表情相关的面部细微动作，并缓解数据稀缺问题。大量实验表明，我们的方法(i)显著超越了最先进的微表情识别方法，(ii)在动态图像构建方面表现良好。特别是在CASME II、SAMM、CAS(ME)^2和CAS(ME)^3数据集上，我们的FDP在F1分数上分别比之前最好的结果提高了4.05%、2.50%、7.71%和2.11%。代码可在https://github.com/CYF-cuber/FDP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial micro-expression recognition (MER) is a challenging task, due to thetransience, subtlety, and dynamics of micro-expressions (MEs). Most existingmethods resort to hand-crafted features or deep networks, in which the formeroften additionally requires key frames, and the latter suffers from small-scaleand low-diversity training data. In this paper, we develop a novel fine-graineddynamic perception (FDP) framework for MER. We propose to rank frame-levelfeatures of a sequence of raw frames in chronological order, in which the rankprocess encodes the dynamic information of both ME appearances and motions.Specifically, a novel local-global feature-aware transformer is proposed forframe representation learning. A rank scorer is further adopted to calculaterank scores of each frame-level feature. Afterwards, the rank features fromrank scorer are pooled in temporal dimension to capture dynamic representation.Finally, the dynamic representation is shared by a MER module and a dynamicimage construction module, in which the former predicts the ME category, andthe latter uses an encoder-decoder structure to construct the dynamic image.The design of dynamic image construction task is beneficial for capturingfacial subtle actions associated with MEs and alleviating the data scarcityissue. Extensive experiments show that our method (i) significantly outperformsthe state-of-the-art MER methods, and (ii) works well for dynamic imageconstruction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11%over the previous best results in terms of F1-score on the CASME II, SAMM,CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available athttps://github.com/CYF-cuber/FDP.</description>
      <author>example@mail.com (Zhiwen Shao, Yifan Cheng, Fan Zhang, Xuehuai Shi, Canlin Li, Lizhuang Ma, Dit-yan Yeung)</author>
      <guid isPermaLink="false">2509.06015v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>From perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model</title>
      <link>http://arxiv.org/abs/2509.05849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 (Main Conference)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种通过自监督学习解决婴儿语音习得中声学到发音映射问题的计算模型，模型使用wav2vec 2.0的表示学习发音参数，产生可理解语音，研究支持感知学习指导发音发展的发育理论。&lt;h4&gt;背景&lt;/h4&gt;婴儿在语音习得方面面临巨大挑战，需要将极其可变的声学输入映射到适当的发音动作，而没有明确指导。&lt;h4&gt;目的&lt;/h4&gt;提出一个解决声学到发音映射问题的计算模型，通过自监督学习来解决这个问题。&lt;h4&gt;方法&lt;/h4&gt;研究提出一个包含三个部分的计算模型：特征提取器将语音转换为潜在表示，逆向模型将这些表示映射到发音参数，合成器生成语音输出。在单说话人和多说话人环境中进行实验，使用预训练的wav2vec 2.0模型的中间层表示，并与MFCC特征进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的wav2vec 2.0模型的中间层表示为发音学习提供了最佳表示，显著优于MFCC特征。这些表示使模型能够学习与人类模式相关的发音轨迹，区分发音部位，产生可理解的语音。对成功的发音学习至关重要的是具有音素可区分性和说话人不变性的表示，这正是自监督表示学习模型的特征。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了与发育理论一致的计算证据，该理论提出音素类别的感知学习指导发音发展，为婴儿如何获得语音产生能力提供了见解，尽管他们面临复杂的映射问题。&lt;h4&gt;翻译&lt;/h4&gt;人类婴儿在语音习得方面面临一个巨大挑战：在没有明确指导的情况下，将极其可变的声学输入映射到适当的发音动作。我们提出了一个通过自监督学习解决声学到发音映射问题的计算模型。我们的模型包含一个将语音转换为潜在表示的特征提取器，一个将这些表示映射到发音参数的逆向模型，以及一个生成语音输出的合成器。在单说话人和多说话人环境中进行的实验显示，预训练的wav2vec 2.0模型的中间层为发音学习提供了最佳表示，显著优于MFCC特征。这些表示使我们的模型能够学习与人类模式相关的发音轨迹，区分发音部位，并产生可理解的语音。对成功的发音学习至关重要的是具有音素可区分性和说话人不变性的表示——这正是自监督表示学习模型的特征。我们的研究结果提供了与发育理论一致的计算证据，该理论提出音素类别的感知学习指导发音发展，为婴儿如何获得语音产生能力提供了见解，尽管他们面临复杂的映射问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human infants face a formidable challenge in speech acquisition: mappingextremely variable acoustic inputs into appropriate articulatory movementswithout explicit instruction. We present a computational model that addressesthe acoustic-to-articulatory mapping problem through self-supervised learning.Our model comprises a feature extractor that transforms speech into latentrepresentations, an inverse model that maps these representations toarticulatory parameters, and a synthesizer that generates speech outputs.Experiments conducted in both single- and multi-speaker settings reveal thatintermediate layers of a pre-trained wav2vec 2.0 model provide optimalrepresentations for articulatory learning, significantly outperforming MFCCfeatures. These representations enable our model to learn articulatorytrajectories that correlate with human patterns, discriminate between places ofarticulation, and produce intelligible speech. Critical to successfularticulatory learning are representations that balance phoneticdiscriminability with speaker invariance -- precisely the characteristics ofself-supervised representation learning models. Our findings providecomputational evidence consistent with developmental theories proposing thatperceptual learning of phonetic categories guides articulatory development,offering insights into how infants might acquire speech production capabilitiesdespite the complex mapping problem they face.</description>
      <author>example@mail.com (Marvin Lavechin, Thomas Hueber)</author>
      <guid isPermaLink="false">2509.05849v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Large Language Models</title>
      <link>http://arxiv.org/abs/2509.05757v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了双曲几何在大型语言模型中的应用，提出了双曲大型语言模型(HypLLMs)的分类框架，并探讨了其在增强语义表示学习和多尺度推理方面的潜力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在各种任务中表现出色，但现实世界数据往往具有高度非欧几里得的层次结构，如蛋白质网络、交通网络、金融网络、大脑网络和语言结构。使用LLMs有效学习这些数据的语义蕴含和层次关系仍是一个探索不足的领域。&lt;h4&gt;目的&lt;/h4&gt;提供对利用双曲几何作为表示空间来增强语义表示学习和多尺度推理的最新进展进行全面阐述，并提出双曲大型语言模型的分类框架。&lt;h4&gt;方法&lt;/h4&gt;将双曲大型语言模型(HypLLMs)的技术分为四类：(1)通过exp/log映射的双曲LLMs；(2)双曲微调模型；(3)完全双曲LLMs；(4)双曲状态空间模型。&lt;h4&gt;主要发现&lt;/h4&gt;双曲几何作为非欧几里得空间，能有效建模树状层次结构，特别适合处理具有层次结构的数据，与LLMs结合可增强语义表示学习和多尺度推理能力。&lt;h4&gt;结论&lt;/h4&gt;双曲几何在大型语言模型中的应用前景广阔，特别是在处理层次结构数据方面。论文提供了全面的分类框架，并探讨了潜在应用和未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在各种任务中取得了显著成功，展示了卓越的性能，包括自然语言处理(NLP)、天气预报、生物蛋白质折叠、文本生成和解决数学问题。然而，许多现实世界的数据表现出高度非欧几里得的潜在层次解剖结构，如蛋白质网络、交通网络、金融网络、大脑网络以及自然语言中的语言结构或句法树。使用LLMs从这些原始、非结构化的输入数据中有效学习内在语义蕴含和层次关系仍然是一个探索不足的领域。由于其在建模树状层次结构方面的有效性，双曲几何——一种非欧几里得空间——已成为跨领域复杂数据建模的表达性潜在表示空间。论文提出了双曲大型语言模型(HypLLMs)主要技术的分类，并探讨了关键应用和未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have achieved remarkable success anddemonstrated superior performance across various tasks, including naturallanguage processing (NLP), weather forecasting, biological protein folding,text generation, and solving mathematical problems. However, many real-worlddata exhibit highly non-Euclidean latent hierarchical anatomy, such as proteinnetworks, transportation networks, financial networks, brain networks, andlinguistic structures or syntactic trees in natural languages. Effectivelylearning intrinsic semantic entailment and hierarchical relationships fromthese raw, unstructured input data using LLMs remains an underexplored area.Due to its effectiveness in modeling tree-like hierarchical structures,hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularityas an expressive latent representation space for complex data modeling acrossdomains such as graphs, images, languages, and multi-modal data. Here, weprovide a comprehensive and contextual exposition of recent advancements inLLMs that leverage hyperbolic geometry as a representation space to enhancesemantic representation learning and multi-scale reasoning. Specifically, thepaper presents a taxonomy of the principal techniques of Hyperbolic LLMs(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/logmaps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)hyperbolic state-space models. We also explore crucial potential applicationsand outline future research directions. A repository of key papers, models,datasets, and code implementations is available athttps://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.</description>
      <author>example@mail.com (Sarang Patil, Zeyong Zhang, Yiran Huang, Tengfei Ma, Mengjia Xu)</author>
      <guid isPermaLink="false">2509.05757v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities</title>
      <link>http://arxiv.org/abs/2509.05615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE TKDE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医学多模态表示学习旨在将异构临床数据整合为统一的患者表示，以支持预测建模，这是医疗数据挖掘领域的一项重要且具有挑战性的任务。然而，现实世界中的医疗数据集常常因成本、协议或患者特定限制而存在模态缺失的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的方法主要通过原始数据空间或特征空间中的可用观测数据进行学习，但通常忽略了数据采集过程本身引入的潜在偏差。现实世界中的医疗数据集经常面临模态缺失问题。&lt;h4&gt;目的&lt;/h4&gt;识别阻碍模型泛化的两种偏差：缺失偏差（由模态可用性的非随机模式导致）和分布偏差（由影响观测特征和结果的潜在混杂因素引起）。提出一个统一框架来解决这些挑战。&lt;h4&gt;方法&lt;/h4&gt;进行数据生成过程的结构因果分析，并提出一个与现有直接预测多模态学习方法兼容的统一框架。该方法包含两个关键组件：(1) 基于后门调整的缺失解混杂模块，用于近似因果干预；(2) 双分支神经网络，明确将因果特征与虚假相关性分离。&lt;h4&gt;主要发现&lt;/h4&gt;在现实世界的公共和院内数据集上评估了该方法，证明了其有效性和因果洞察力。&lt;h4&gt;结论&lt;/h4&gt;通过识别和处理缺失偏差和分布偏差，该方法能够提高医学多模态表示学习的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;医学多模态表示学习旨在将异构临床数据整合为统一的患者表示，以支持预测建模，这仍然是医疗数据挖掘领域一项基本且具有挑战性的任务。然而，现实世界中的医疗数据集常常因成本、协议或患者特定限制而面临模态缺失的问题。现有方法主要通过原始数据空间或特征空间中的可用观测数据进行学习，但通常忽略了数据采集过程本身引入的潜在偏差。在这项工作中，我们确定了阻碍模型泛化的两种偏差：缺失偏差，由模态可用性的非随机模式导致；以及分布偏差，由影响观测特征和结果的潜在混杂因素引起。为应对这些挑战，我们对数据生成过程进行了结构因果分析，并提出了一个与现有直接预测多模态学习方法兼容的统一框架。我们的方法包含两个关键组件：(1) 基于后门调整的缺失解混杂模块，用于近似因果干预；(2) 双分支神经网络，明确将因果特征与虚假相关性分离。我们在现实世界的公共和院内数据集上评估了我们的方法，证明了其有效性和因果洞察力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical multimodal representation learning aims to integrate heterogeneousclinical data into unified patient representations to support predictivemodeling, which remains an essential yet challenging task in the medical datamining community. However, real-world medical datasets often suffer frommissing modalities due to cost, protocol, or patient-specific constraints.Existing methods primarily address this issue by learning from the availableobservations in either the raw data space or feature space, but typicallyneglect the underlying bias introduced by the data acquisition process itself.In this work, we identify two types of biases that hinder model generalization:missingness bias, which results from non-random patterns in modalityavailability, and distribution bias, which arises from latent confounders thatinfluence both observed features and outcomes. To address these challenges, weperform a structural causal analysis of the data-generating process and proposea unified framework that is compatible with existing direct prediction-basedmultimodal learning methods. Our method consists of two key components: (1) amissingness deconfounding module that approximates causal intervention based onbackdoor adjustment and (2) a dual-branch neural network that explicitlydisentangles causal features from spurious correlations. We evaluated ourmethod in real-world public and in-hospital datasets, demonstrating itseffectiveness and causal insights.</description>
      <author>example@mail.com (Xiaoguang Zhu, Lianlong Sun, Yang Liu, Pengyi Jiang, Uma Srivatsa, Nipavan Chiamvimonvat, Vladimir Filkov)</author>
      <guid isPermaLink="false">2509.05615v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning</title>
      <link>http://arxiv.org/abs/2509.05606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过自监督学习构建密集表示的框架，解决了全局表示方法在密集预测任务中难以捕捉局部语义的问题。&lt;h4&gt;背景&lt;/h4&gt;密集表示对于需要空间精度和细粒度细节的视觉任务至关重要，而大多数自监督表示学习方法专注于全局表示，往往无法捕捉密集预测任务所需的局部语义信息。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架，通过额外的自监督学习建立在预训练表示的基础上，旨在将现有的语义知识转移到密集特征空间中，以克服全局表示方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一种框架，对齐教师模型和学生模型之间的密集特征分布；引入Patch级核对齐（PaKA）作为简单有效的对齐目标，捕捉统计依赖关系，匹配两个模型中密集补丁的结构关系；研究专门为密集表示学习设计的增强策略。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在各种密集视觉基准测试中取得了最先进的结果，证明了所提出方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过额外的自监督学习和特定的对齐方法，可以有效地将语义知识转移到密集特征空间，Patch级核对齐（PaKA）是一种有效的方法，可以捕捉统计依赖关系并匹配结构关系。&lt;h4&gt;翻译&lt;/h4&gt;密集表示对于需要空间精度和细粒度细节的视觉任务至关重要。虽然大多数自监督表示学习方法专注于总结整个图像的全局表示，但这类方法通常难以捕捉密集预测任务所需的局部语义。为克服这些局限，我们提出了一种框架，通过额外的自监督学习建立在预训练表示的基础上，旨在将现有的语义知识转移到密集特征空间。我们的方法对齐了教师模型和学生模型之间的密集特征分布。具体而言，我们引入了Patch级核对齐（PaKA），这是一种简单而有效的对齐目标，可以捕捉统计依赖关系，从而匹配两个模型中密集补丁的结构关系。此外，我们还研究了专门为密集表示学习设计的增强策略。我们的框架在各种密集视觉基准测试中取得了最先进的结果，证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense representations are essential for vision tasks that require spatialprecision and fine-grained detail. While most self-supervised representationlearning methods focus on global representations that summarize the image as awhole, such approaches often fall short in capturing the localized semanticsnecessary for dense prediction tasks. To overcome these limitations, we proposea framework that builds on pretrained representations through additionalself-supervised learning, aiming to transfer existing semantic knowledge intothe dense feature space. Our method aligns the distributions of dense featuresbetween a teacher and a student model. Specifically, we introduce Patch-levelKernel Alignment (PaKA), a simple yet effective alignment objective thatcaptures statistical dependencies, thereby matching the structuralrelationships of dense patches across the two models. In addition, weinvestigate augmentation strategies specifically designed for denserepresentation learning. Our framework achieves state-of-the-art results acrossa variety of dense vision benchmarks, demonstrating the effectiveness of ourapproach.</description>
      <author>example@mail.com (Juan Yeo, Ijun Jang, Taesup Kim)</author>
      <guid isPermaLink="false">2509.05606v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2509.05478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PLanTS，一个周期感知的自监督学习框架，用于处理多元时间序列数据。该框架能够显式建模不规则潜在状态及其转换，通过多粒度修补机制和对比损失保留时间分辨率上的相似性，并通过下一个转换预测任务捕获时间动态。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列在医疗保健、气候科学和工业监测等领域普遍存在，但它们的高维性、有限的标记数据和非平稳特性对传统机器学习方法构成了重大挑战。现有的自监督学习方法忽略了多元时间序列的内在周期结构，无法捕获潜在状态的动态演化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉多元时间序列周期结构并建模潜在状态动态演化的自监督学习框架，以提高在多种下游任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了PLanTS框架，包含：1)周期感知的多粒度修补机制；2)广义对比损失保留多个时间分辨率上的实例级和状态级相似性；3)下一个转换预测预训练任务，鼓励表示编码关于未来状态演化的预测信息。&lt;h4&gt;主要发现&lt;/h4&gt;PLanTS在多类和多标签分类、预测、轨迹跟踪和异常检测等多种下游任务中评估，在表示质量上持续优于现有的自监督学习方法，并且与基于DTW的方法相比展示了更优的运行时效率。&lt;h4&gt;结论&lt;/h4&gt;PLanTS是一个有效的自监督学习框架，能够处理多元时间序列数据的周期性和动态性，在各种任务中表现优异且计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;多元时间序列在医疗保健、气候科学和工业监测等领域普遍存在，但它们的高维性、有限的标记数据和非平稳特性对传统机器学习方法构成了重大挑战。尽管最近的自学习方法通过数据增强或基于时间点的对比策略缓解了标签稀缺问题，但它们忽略了多元时间序列的内在周期结构，并且无法捕获潜在状态的动态演化。我们提出了PLanTS，一个周期感知的自监督学习框架，能够显式地建模不规则潜在状态及其转换。我们首先设计了一个周期感知的多粒度修补机制和一个广义对比损失，以在多个时间分辨率上保留实例级和状态级相似性。为了进一步捕获时间动态，我们设计了一个下一个转换预测预训练任务，鼓励表示编码关于未来状态演化的预测信息。我们在广泛的下游任务中评估了PLanTS，包括多类和多标签分类、预测、轨迹跟踪和异常检测。PLanTS在表示质量上持续优于现有的自监督学习方法，并且与基于DTW的方法相比展示了更优的运行时效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) are ubiquitous in domains such as healthcare,climate science, and industrial monitoring, but their high dimensionality,limited labeled data, and non-stationary nature pose significant challenges forconventional machine learning methods. While recent self-supervised learning(SSL) approaches mitigate label scarcity by data augmentations or timepoint-based contrastive strategy, they neglect the intrinsic periodic structureof MTS and fail to capture the dynamic evolution of latent states. We proposePLanTS, a periodicity-aware self-supervised learning framework that explicitlymodels irregular latent states and their transitions. We first designed aperiod-aware multi-granularity patching mechanism and a generalized contrastiveloss to preserve both instance-level and state-level similarities acrossmultiple temporal resolutions. To further capture temporal dynamics, we designa next-transition prediction pretext task that encourages representations toencode predictive information about future state evolution. We evaluate PLanTSacross a wide range of downstream tasks-including multi-class and multi-labelclassification, forecasting, trajectory tracking and anomaly detection. PLanTSconsistently improves the representation quality over existing SSL methods anddemonstrates superior runtime efficiency compared to DTW-based methods.</description>
      <author>example@mail.com (Jia Wang, Xiao Wang, Chi Zhang)</author>
      <guid isPermaLink="false">2509.05478v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes</title>
      <link>http://arxiv.org/abs/2509.06266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个新的基准测试Ego3D-Bench和一种改进框架Ego3D-VLM，用于评估和提高视觉语言模型在3D空间关系理解方面的能力，特别是在以自我为中心的多视图户外场景中。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在理解3D空间关系方面存在明显局限。先前的工作基于单图像或室内视频创建了空间问答数据集，但现实世界中的具身AI代理(如机器人和自动驾驶汽车)通常依赖于以自我为中心的多视图观测。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的基准测试Ego3D-Bench来评估视觉语言模型在使用以自我为中心的多视图户外数据时的空间推理能力，并提出一种改进框架Ego3D-VLM来增强视觉语言模型的3D空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;创建了包含8,600多个问答对的Ego3D-Bench基准测试，由人工注释者大量参与以确保质量和多样性。提出了Ego3D-VLM后训练框架，该框架基于估计的全局3D坐标生成认知地图。&lt;h4&gt;主要发现&lt;/h4&gt;在16个最先进的视觉语言模型(包括GPT-4o、Gemini1.5-Pro、InternVL3和Qwen2.5-VL)的测试中，发现了人类水平分数与模型性能之间的显著差距，表明当前的视觉语言模型仍无法达到人类水平的空间理解。Ego3D-VLM框架在多选题问答上实现了12%的平均改进，在绝对距离估计上实现了56%的平均改进。&lt;h4&gt;结论&lt;/h4&gt;Ego3D-Bench和Ego3D-VLM共同为在现实世界多视环境中推进人类水平的空间理解提供了有价值的工具。Ego3D-VLM是模块化的，可以与任何现有的视觉语言模型集成。&lt;h4&gt;翻译&lt;/h4&gt;理解3D空间关系仍然是当前视觉语言模型的主要局限性。先前的工作通过创建基于单图像或室内视频的空间问答数据集来解决这一问题。然而，现实世界中的具身AI代理通常依赖于以自我为中心的多视图观测。为此，我们引入了Ego3D-Bench，这是一个新的基准测试，旨在使用以自我为中心的多视图户外数据评估视觉语言模型的空间推理能力。Ego3D-Bench包含8,600多个问答对，由人工注释者大量参与创建，以确保质量和多样性。我们对16个最先进的视觉语言模型进行了基准测试，结果显示人类水平分数与模型性能之间存在显著差距，突显出当前的视觉语言模型仍未达到人类水平的空间理解。为了弥合这一差距，我们提出了Ego3D-VLM，一种后训练框架，可以增强视觉语言模型的3D空间推理能力。Ego3D-VLM基于估计的全局3D坐标生成认知地图，在多选题问答上实现了12%的平均改进，在绝对距离估计上实现了56%的平均改进。Ego3D-Bench和Ego3D-VLM共同为在现实世界多视环境中推进人类水平的空间理解提供了有价值的工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前视觉语言模型（VLMs）在3D空间推理方面的能力不足问题，特别是在以自我为中心的多视角场景中。这个问题很重要，因为现实世界中的具身AI代理（如机器人和自动驾驶汽车）需要理解3D空间关系来感知环境、估计物体距离和推理运动，而现有的空间理解基准测试主要基于单图像或室内视频，无法反映这些代理的真实感知体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有工作的局限性：现有空间基准测试基于单图像或室内静态视频，与真实世界多视角动态场景不符；点云和鸟瞰图方法虽提供丰富空间信息但难以在动态环境中重建且计算成本高。作者借鉴了人类自然整合多视角视觉信息形成统一空间表示的能力，设计了一个名为Ego3D-VLM的后训练框架，生成文本认知地图而非复杂3D表示。该方法整合了现有的2D目标检测和深度估计技术，但将其统一到更符合人类感知机制的框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个以自我为中心的文本认知地图，定义以代理为中心的坐标系，并将重要物体定位在3D坐标空间中。这种方法只关注提示中提到的物体，减少输入令牌数量，实现高效推理。实现流程包括：1)使用REC模型找到指代表达式的2D位置；2)使用深度估计器估计深度值；3)将2D点转换为3D点；4)将所有视角的3D点转换为全局坐标系；5)生成文本认知地图；6)将认知地图、多视角图像和查询输入VLM得到答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Ego3D-Bench基准测试，首个针对以自我为中心多视角场景的3D空间理解基准，包含8600个问答对；2)Ego3D-VLM框架，一种即插即用的后训练方法，生成文本认知地图而非复杂3D表示；3)关系缩放技术基于常识物体大小进行比例调整。相比之前工作，现有方法主要使用点云或鸟瞰图表示，在动态环境中难以重建且计算成本高；现有基准测试基于单图像或室内视频，不符合实际应用需求；Ego3D-VLM专注于多视角场景，更符合实际应用，且可集成到任何现有VLM中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出首个针对以自我为中心多视角场景的3D空间理解基准测试（Ego3D-Bench）和一种增强VLMs空间推理能力的后训练框架（Ego3D-VLM），显著缩小了当前VLMs与人类在3D空间理解能力之间的差距。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding 3D spatial relationships remains a major limitation of currentVision-Language Models (VLMs). Prior work has addressed this issue by creatingspatial question-answering (QA) datasets based on single images or indoorvideos. However, real-world embodied AI agents such as robots and self-drivingcars typically rely on ego-centric, multi-view observations. To this end, weintroduce Ego3D-Bench, a new benchmark designed to evaluate the spatialreasoning abilities of VLMs using ego-centric, multi-view outdoor data.Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvementfrom human annotators to ensure quality and diversity. We benchmark 16 SOTAVLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our resultsreveal a notable performance gap between human level scores and VLMperformance, highlighting that current VLMs still fall short of human levelspatial understanding. To bridge this gap, we propose Ego3D-VLM, apost-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLMgenerates cognitive map based on estimated global 3D coordinates, resulting in12% average improvement on multi-choice QA and 56% average improvement onabsolute distance estimation. Ego3D-VLM is modular and can be integrated withany existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools foradvancing toward human level spatial understanding in real-world, multi-viewenvironments.</description>
      <author>example@mail.com (Mohsen Gholami, Ahmad Rezaei, Zhou Weimin, Yong Zhang, Mohammad Akbari)</author>
      <guid isPermaLink="false">2509.06266v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</title>
      <link>http://arxiv.org/abs/2509.06079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种标题辅助推理框架，有效解决了多模态推理中的挑战，在ICML 2025挑战赛中获得第一名，并在MathVerse基准测试上验证了其泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多模态推理是人工智能中的基本挑战，尽管基于文本的推理取得了实质性进展，但最先进的模型如GPT-o3在多模态场景中仍难以保持强大性能。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效桥接视觉和文本模态的推理框架，以解决多模态推理中的性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种标题辅助推理框架，用于有效连接视觉和文本模态。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在ICML 2025 AI for Math Workshop &amp; Challenge 2: SeePhys中获得第一名，证明了其有效性和鲁棒性；在MathVerse基准测试上验证了其在几何推理方面的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;标题辅助推理框架能够有效解决多模态推理挑战，具有良好的性能和泛化能力，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;多模态推理仍然是人工智能中的一个基本挑战。尽管基于文本的推理取得了实质性进展，但即使是像GPT-o3这样的最先进模型也难以在多模态场景中保持强大的性能。为了解决这一差距，我们引入了一个标题辅助推理框架，有效地桥接了视觉和文本模态。我们的方法在ICML 2025 AI for Math Workshop &amp; Challenge 2: SeePhys中获得了第一名，突显了其有效性和鲁棒性。此外，我们在MathVerse基准测试上验证了其在几何推理方面的泛化能力，展示了我们方法的通用性。我们的代码已在https://github.com/OpenDCAI/SciReasoner上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal reasoning remains a fundamental challenge in artificialintelligence. Despite substantial advances in text-based reasoning, evenstate-of-the-art models such as GPT-o3 struggle to maintain strong performancein multimodal scenarios. To address this gap, we introduce a caption-assistedreasoning framework that effectively bridges visual and textual modalities. Ourapproach achieved 1st place in the ICML 2025 AI for Math Workshop \&amp; Challenge2: SeePhys, highlighting its effectiveness and robustness. Furthermore, wevalidate its generalization on the MathVerse benchmark for geometric reasoning,demonstrating the versatility of our method. Our code is publicly available athttps://github.com/OpenDCAI/SciReasoner.</description>
      <author>example@mail.com (Hao Liang, Ruitao Wu, Bohan Zeng, Junbo Niu, Wentao Zhang, Bin Dong)</author>
      <guid isPermaLink="false">2509.06079v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation</title>
      <link>http://arxiv.org/abs/2509.05746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于空间自适应重建策略的单图像超分辨率方法，解决了传统方法中空间不变退化模型的局限性。通过引入几何场景理解和深度依赖效应，作者建立了一个严格的变分框架，并将其实现为一个具有深度条件卷积核的神经架构。该方法在多个基准数据集上取得了最先进的结果，特别是在深度变化场景中表现出显著改进。&lt;h4&gt;背景&lt;/h4&gt;传统单图像超分辨率方法假设空间不变的退化模型，然而真实世界的成像系统表现出复杂的空间依赖效应，包括大气散射、景深变化和透视失真等。这种基本局限性需要空间自适应重建策略，明确结合几何场景理解以获得最佳性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理空间依赖效应（特别是与距离相关的效应）的超分辨率方法，建立将超分辨率描述为空间变化逆问题的理论框架，并实现一个能够根据深度信息自适应调整的神经架构。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种严格的变分框架，将超分辨率描述为空间变化的逆问题，将退化算子建模为具有距离依赖谱特性的伪微分算子。神经架构通过级联残差块实现离散梯度流动力学，确保收敛到理论能量泛函的平稳点，同时包含学习到的距离自适应正则化项，根据局部几何结构动态调整平滑约束。基于大气散射理论的光谱约束防止远场区域中的带宽违规和噪声放大，而自适应核生成网络学习从深度到重建滤波器的连续映射。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准数据集上的全面评估展示了最先进的性能，在KITTI户外场景2倍和4倍放大时分别达到36.89/0.9516和30.54/0.8721的PSNR/SSIM值，分别比现有方法高出0.44dB和0.36dB。该方法在深度变化场景中显示出显著改进，同时在传统基准测试中保持了竞争性性能。&lt;h4&gt;结论&lt;/h4&gt;该工作建立了首个理论上合理的距离自适应超分辨率框架，在深度变化场景上展示了显著改进，同时在传统基准测试中保持了竞争性性能，为处理真实世界成像系统中的复杂空间依赖效应提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;单图像超分辨率传统上假设空间不变的退化模型，而真实世界的成像系统表现出复杂的空间依赖效应，包括大气散射、景深变化和透视失真。这种基本局限性需要明确结合几何场景理解的空间自适应重建策略以获得最佳性能。我们提出了一种严格的变分框架，将超分辨率描述为空间变化的逆问题，将退化算子建模为具有距离依赖谱特性的伪微分算子，从而能够跨深度范围分析重建极限的理论。我们的神经架构通过具有深度条件卷积核的级联残差块实现离散梯度流动力学，确保收敛到理论能量泛函的平稳点，同时包含学习到的距离自适应正则化项，这些项根据局部几何结构动态调整平滑约束。从大气散射理论导出的光谱约束防止远场区域中的带宽违规和噪声放大，而自适应核生成网络学习从深度到重建滤波器的连续映射。在五个基准数据集上的全面评估展示了最先进的性能，在KITTI户外场景2倍和4倍放大时分别达到36.89/0.9516和30.54/0.8721的PSNR/SSIM值，分别比现有方法高出0.44dB和0.36dB。该工作建立了首个理论上合理的距离自适应超分辨率框架，并在深度变化场景上展示了显著改进，同时在传统基准测试中保持了竞争性性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决传统单图像超分辨率方法假设空间不变退化模型的问题，而现实世界成像系统存在复杂的距离相关效应（如大气散射、景深变化、透视失真）。这一问题重要是因为户外场景（如自动驾驶、卫星成像）中图像质量随距离变化而退化，传统方法无法处理这种空间变化的退化，导致在真实场景中效果不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析发现户外场景与室内场景有不同的退化机制，因此构建了基于伪微分算子的变分框架来建模距离相关的退化。他们借鉴了大气散射理论（如Mie散射）来推导光谱约束，同时利用深度估计网络获取场景几何信息。方法设计上借鉴了残差网络架构和梯度流优化技术，但将其扩展为深度条件卷积和距离自适应正则化，实现了理论框架与神经网络的结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将超分辨率建模为考虑距离相关退化的空间变化逆问题，利用深度信息指导重建过程，并根据大气散射理论限制重建带宽防止噪声放大。整体流程包括：1)提取深度图；2)生成距离自适应卷积核；3)初始化重建图像；4)迭代优化（计算数据保真度、应用距离自适应正则化、更新重建）；5)最终上采样输出高分辨率图像。整个过程中使用梯度流块逐步优化重建结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于伪微分算子的理论变分框架；2)深度自适应正则化项，根据距离动态调整平滑约束；3)基于大气散射理论的光谱约束，防止远场噪声放大；4)深度条件卷积核和自适应核生成网络。相比之前工作，不同之处在于：传统方法假设空间不变退化，而本文处理空间变化的退化；本文有严格的理论基础（传统方法多为启发式设计）；专门针对户外场景优化（传统方法在室内场景表现好但户外效果差）；显式利用深度信息指导重建（传统方法通常不利用深度信息）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于距离自适应变分公式的深度感知超分辨率方法，通过理论建模和神经网络实现了对户外场景中距离相关退化的有效处理，显著提升了超分辨率重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single image super-resolution traditionally assumes spatially-invariantdegradation models, yet real-world imaging systems exhibit complexdistance-dependent effects including atmospheric scattering, depth-of-fieldvariations, and perspective distortions. This fundamental limitationnecessitates spatially-adaptive reconstruction strategies that explicitlyincorporate geometric scene understanding for optimal performance. We propose arigorous variational framework that characterizes super-resolution as aspatially-varying inverse problem, formulating the degradation operator as apseudodifferential operator with distance-dependent spectral characteristicsthat enable theoretical analysis of reconstruction limits across depth ranges.Our neural architecture implements discrete gradient flow dynamics throughcascaded residual blocks with depth-conditional convolution kernels, ensuringconvergence to stationary points of the theoretical energy functional whileincorporating learned distance-adaptive regularization terms that dynamicallyadjust smoothness constraints based on local geometric structure. Spectralconstraints derived from atmospheric scattering theory prevent bandwidthviolations and noise amplification in far-field regions, while adaptive kernelgeneration networks learn continuous mappings from depth to reconstructionfilters. Comprehensive evaluation across five benchmark datasets demonstratesstate-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIMat 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by0.44dB and 0.36dB respectively. This work establishes the firsttheoretically-grounded distance-adaptive super-resolution framework anddemonstrates significant improvements on depth-variant scenarios whilemaintaining competitive performance across traditional benchmarks.</description>
      <author>example@mail.com (Tianhao Guo, Bingjie Lu, Feng Wang, Zhengyang Lu)</author>
      <guid isPermaLink="false">2509.05746v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision</title>
      <link>http://arxiv.org/abs/2509.05578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为OccVLA的新框架，将3D占用表示集成到多模态推理过程中，解决了多模态大语言模型在3D空间理解方面的局限性，特别是在自动驾驶领域的应用。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)展现出强大的视觉-语言推理能力，但缺乏稳健的3D空间理解能力，这对自动驾驶至关重要。这种局限性源于两个关键挑战：(1)构建易于获取且有效的3D表示困难，需要昂贵的手动标注；(2)由于缺乏大规模3D视觉语言预训练，VLMs会丢失细粒度的空间细节。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在3D空间理解方面的局限性，特别是在自动驾驶领域的应用，通过创新方法提升模型的3D空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;OccVLA框架将密集的3D占用同时作为预测输出和监督信号，使模型能够直接从2D视觉输入中学习细粒度的空间结构。与依赖显式3D输入的先前方法不同，OccVLA将占用预测视为隐式推理过程，在推理过程中可以跳过而不会导致性能下降，因此不会增加额外的计算开销。&lt;h4&gt;主要发现&lt;/h4&gt;OccVLA在nuScenes基准测试的轨迹规划任务上取得了最先进的结果，并在3D视觉问答任务上表现出优越的性能，为自动驾驶提供了一种可扩展、可解释且完全基于视觉的解决方案。&lt;h4&gt;结论&lt;/h4&gt;OccVLA通过创新地将3D占用表示集成到多模态推理过程中，有效解决了MLLMs在3D空间理解方面的局限性，为自动驾驶领域提供了一个高效且实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已经展现出强大的视觉-语言推理能力，但仍然缺乏稳健的3D空间理解能力，这对于自动驾驶至关重要。这种局限性源于两个关键挑战：(1)在没有昂贵手动标注的情况下构建易于获取且有效的3D表示存在困难，以及(2)由于缺乏大规模3D视觉语言预训练，VLMs会丢失细粒度的空间细节。为了解决这些挑战，我们提出了OccVLA，一种新颖的框架，将3D占用表示集成到统一的多模态推理过程中。与依赖显式3D输入的先前方法不同，OccVLA将密集的3D占用同时作为预测输出和监督信号，使模型能够直接从2D视觉输入中学习细粒度的空间结构。占用预测被视为隐式推理过程，在推理过程中可以跳过而不会导致性能下降，因此不会增加额外的计算开销。OccVLA在nuScenes基准测试的轨迹规划任务上取得了最先进的结果，并在3D视觉问答任务上表现出优越的性能，为自动驾驶提供了一种可扩展、可解释且完全基于视觉的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)缺乏稳健3D空间理解能力的问题，这对自动驾驶至关重要。这个问题很重要，因为自动驾驶需要强大的3D感知能力进行定位和导航，而3D感知的保真度直接影响下游决策的安全性。现有方法要么依赖昂贵的人工3D标注数据，要么在处理3D输入时丢失细粒度空间细节，限制了自动驾驶系统的性能和可扩展性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：VLM-based方法依赖稀疏的文本描述3D标注，而整合3D输入的方法受限于缺乏大规模3D预训练。作者借鉴了自动化标注流程和Transformer模型在占用表示方面的进展，设计了OccVLA框架。该方法通过交叉注意力机制使占用令牌从视觉特征中学习，并在潜在空间中预测占用以解决空间稀疏性问题，同时允许在推理时跳过占用预测以提高效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将密集3D占用表示作为模型的预测输出和监督信号，从2D视觉输入中学习细粒度空间结构，且推理时可跳过占用预测过程。整体流程包括：1)占用预测阶段：占用令牌通过交叉注意力获取视觉特征，在潜在空间预测占用后映射回高分辨率空间；2)运动规划阶段：分解为元动作预测(速度和方向)和坐标生成；3)三阶段训练：自动驾驶场景预训练、占用-语言联合训练和规划头训练，最终实现从视觉输入到未来轨迹的端到端处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新颖的OccVLA框架，将3D占用作为预测输出和监督信号而非输入；2)占用预测可作为隐式推理过程，推理时可跳过而不增加计算开销；3)通过占用监督增强VLM的3D表示能力；4)输出可解释且可定量评估。相比之前工作，该方法不依赖昂贵3D标注或额外3D传感器，保留了VLM的泛化能力，在nuScenes上实现了最先进的轨迹规划性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OccVLA通过将3D占用预测作为视觉语言模型的隐式推理和监督信号，实现了无需额外3D传感器输入的高效3D空间理解，并在自动驾驶任务中取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have shown strong vision-languagereasoning abilities but still lack robust 3D spatial understanding, which iscritical for autonomous driving. This limitation stems from two key challenges:(1) the difficulty of constructing accessible yet effective 3D representationswithout expensive manual annotations, and (2) the loss of fine-grained spatialdetails in VLMs due to the absence of large-scale 3D vision-languagepretraining. To address these challenges, we propose OccVLA, a novel frameworkthat integrates 3D occupancy representations into a unified multimodalreasoning process. Unlike prior approaches that rely on explicit 3D inputs,OccVLA treats dense 3D occupancy as both a predictive output and a supervisorysignal, enabling the model to learn fine-grained spatial structures directlyfrom 2D visual inputs. The occupancy predictions are regarded as implicitreasoning processes and can be skipped during inference without performancedegradation, thereby adding no extra computational overhead. OccVLA achievesstate-of-the-art results on the nuScenes benchmark for trajectory planning anddemonstrates superior performance on 3D visual question-answering tasks,offering a scalable, interpretable, and fully vision-based solution forautonomous driving.</description>
      <author>example@mail.com (Ruixun Liu, Lingyu Kong, Derun Li, Hang Zhao)</author>
      <guid isPermaLink="false">2509.05578v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
      <link>http://arxiv.org/abs/2509.06426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了第一个果蝇腿部的3D数据驱动肌肉骨骼模型，在OpenSim和MuJoCo仿真环境中实现，能够模拟果蝇行为并研究肌肉协同作用和关节特性对学习的影响。&lt;h4&gt;背景&lt;/h4&gt;计算模型对于理解神经、生物力学和物理系统如何协调动物行为至关重要。尽管已有果蝇神经系统、肌肉和外骨骼的完整重建，但仍缺乏基于解剖学和物理学的果蝇腿部肌肉模型，这种模型是连接运动神经元活动和关节运动的必要桥梁。&lt;h4&gt;目的&lt;/h4&gt;开发第一个果蝇腿部的3D数据驱动肌肉骨骼模型，研究运动控制，了解生物力学如何促进复杂肢体运动的产生，并用于控制人工智能体在模拟环境中生成自然柔顺的运动。&lt;h4&gt;方法&lt;/h4&gt;创建3D数据驱动的果蝇腿部肌肉骨骼模型，使用基于高分辨率X射线扫描的Hill型肌肉表示；提出构建肌肉模型的流程和优化未知肌肉参数的方法；将模型与果蝇行为数据结合实现行为重现；在MuJoCo中训练模仿学习策略测试关节特性对学习速度的影响。&lt;h4&gt;主要发现&lt;/h4&gt;模拟行走和梳理行为中的肌肉活动，预测了可实验测试的协调肌肉协同作用；阻尼和刚度特性有助于提高学习速度。&lt;h4&gt;结论&lt;/h4&gt;该模型使得在实验上易于处理的模式生物中研究运动控制成为可能，提供了关于生物力学如何促进复杂肢体运动产生的见解；同时可用于控制具身人工智能体在模拟环境中生成自然和柔顺的运动。&lt;h4&gt;翻译&lt;/h4&gt;计算模型对于推进我们对神经、生物力学和物理系统如何相互作用以协调动物行为的理解至关重要。尽管已有果蝇中枢神经系统、肌肉和外骨骼的近乎完整的重建，但仍然缺乏基于解剖学和物理学的果蝇腿部肌肉模型。这些模型提供了运动神经元活动和关节运动之间不可或缺的桥梁。在此，我们介绍了第一个果蝇腿部的3D数据驱动肌肉骨骼模型，在OpenSim和MuJoCo仿真环境中实现。我们的模型纳入了基于来自多个固定标本的高分辨率X射线扫描的Hill型肌肉表示。我们提出了一个使用形态成像数据构建肌肉模型的流程，以及优化果蝇特定未知肌肉参数的方法。随后，我们将肌肉骨骼模型与来自行为果蝇的详细3D姿态估计数据结合，在OpenSim中实现肌肉驱动的行为重现。对行走和梳理行为中肌肉活动的模拟预测了可实验测试的协调肌肉协同作用。此外，通过在MuJoCo中训练模仿学习策略，我们测试了不同被动关节特性对学习速度的影响，发现阻尼和刚度有助于学习。总体而言，我们的模型使得在实验上易于处理的模式生物中研究运动控制成为可能，提供了关于生物力学如何促进复杂肢体运动产生的见解。此外，我们的模型可用于控制具身人工智能体，在模拟环境中生成自然和柔顺的运动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational models are critical to advance our understanding of how neural,biomechanical, and physical systems interact to orchestrate animal behaviors.Despite the availability of near-complete reconstructions of the Drosophilamelanogaster central nervous system, musculature, and exoskeleton, anatomicallyand physically grounded models of fly leg muscles are still missing. Thesemodels provide an indispensable bridge between motor neuron activity and jointmovements. Here, we introduce the first 3D, data-driven musculoskeletal modelof Drosophila legs, implemented in both OpenSim and MuJoCo simulationenvironments. Our model incorporates a Hill-type muscle representation based onhigh-resolution X-ray scans from multiple fixed specimens. We present apipeline for constructing muscle models using morphological imaging data andfor optimizing unknown muscle parameters specific to the fly. We then combineour musculoskeletal models with detailed 3D pose estimation data from behavingflies to achieve muscle-actuated behavioral replay in OpenSim. Simulations ofmuscle activity across diverse walking and grooming behaviors predictcoordinated muscle synergies that can be tested experimentally. Furthermore, bytraining imitation learning policies in MuJoCo, we test the effect of differentpassive joint properties on learning speed and find that damping and stiffnessfacilitate learning. Overall, our model enables the investigation of motorcontrol in an experimentally tractable model organism, providing insights intohow biomechanics contribute to generation of complex limb movements. Moreover,our model can be used to control embodied artificial agents to generatenaturalistic and compliant locomotion in simulated environments.</description>
      <author>example@mail.com (Pembe Gizem Özdil, Chuanfang Ning, Jasper S. Phelps, Sibo Wang-Chen, Guy Elisha, Alexander Blanke, Auke Ijspeert, Pavan Ramdya)</author>
      <guid isPermaLink="false">2509.06426v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Finetuning LLMs for Human Behavior Prediction in Social Science Experiments</title>
      <link>http://arxiv.org/abs/2509.05830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何通过微调大型语言模型来提高社会科学实验模拟的准确性。研究者构建了SocSci210数据集，包含来自210个开源社会科学实验的290万份参与者回应。通过微调，模型在未见过的研究中表现优异，且能减少偏差。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型为模拟社会科学实验结果提供了强大机会，但如何提高这些模拟的准确性是一个重要挑战。&lt;h4&gt;目的&lt;/h4&gt;通过在过去的个体层面回应数据上直接微调大型语言模型，提高跨不同社会科学领域实验模拟的准确性。&lt;h4&gt;方法&lt;/h4&gt;构建SocSci210数据集，包含210个开源社会科学实验中400,491名参与者的290万份回应，并通过微调训练模型，特别是Socrates-Qwen-14B模型。&lt;h4&gt;主要发现&lt;/h4&gt;在完全未见过的研究中，Socrates-Qwen-14B模型预测与人类回应分布的 alignment 比基础模型高26%，比GPT-4o高13%；在研究条件子集上微调，对新未见条件的泛化能力提高71%；通过微调，人口统计公平性偏差降低10.6%。&lt;h4&gt;结论&lt;/h4&gt;由于社会科学经常生成丰富、主题特定的数据集，在这些数据上进行微调可以实现更准确的实验假设筛选模拟。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型为模拟社会科学实验结果提供了强大机会。在本工作中，我们证明直接在过往实验的个体层面回应上微调大型语言模型，能显著提高跨不同社会科学领域此类模拟的准确性。我们通过自动流程构建了SocSci210，这是一个包含来自210个开源社会科学实验的400,491名参与者的290万份回应的数据集。通过微调，我们实现了多个层次的泛化。在完全未见过的研究中，我们最强大的模型Socrates-Qwen-14B产生的预测与不同条件下对多样化结果问题的人类回应分布更加一致，相对于其基础模型(Qwen2.5-14B)提高了26%，优于GPT-4o达13%。通过在研究条件的子集上进行微调，对新的未见条件的泛化特别稳健，提高了71%。由于SocSci210包含丰富的人口统计信息，我们通过微调将人口统计公平性(一种偏差度量)降低了10.6%。由于社会科学常规生成丰富、主题特定的数据集，我们的研究结果表明，在这些数据上进行微调可以 enable 更准确的实验假设筛选模拟。我们在stanfordhci.github.io/socrates发布了我们的数据、模型和微调代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) offer a powerful opportunity to simulate theresults of social science experiments. In this work, we demonstrate thatfinetuning LLMs directly on individual-level responses from past experimentsmeaningfully improves the accuracy of such simulations across diverse socialscience domains. We construct SocSci210 via an automatic pipeline, a datasetcomprising 2.9 million responses from 400,491 participants in 210 open-sourcesocial science experiments. Through finetuning, we achieve multiple levels ofgeneralization. In completely unseen studies, our strongest model,Socrates-Qwen-14B, produces predictions that are 26% more aligned withdistributions of human responses to diverse outcome questions under varyingconditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by13%. By finetuning on a subset of conditions in a study, generalization to newunseen conditions is particularly robust, improving by 71%. Since SocSci210contains rich demographic information, we reduce demographic parity, a measureof bias, by 10.6% through finetuning. Because social sciences routinelygenerate rich, topic-specific datasets, our findings indicate that finetuningon such data could enable more accurate simulations for experimental hypothesisscreening. We release our data, models and finetuning code atstanfordhci.github.io/socrates.</description>
      <author>example@mail.com (Akaash Kolluri, Shengguang Wu, Joon Sung Park, Michael S. Bernstein)</author>
      <guid isPermaLink="false">2509.05830v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory</title>
      <link>http://arxiv.org/abs/2509.05337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at IEEE RO-MAN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合动态图神经网络和长短期记忆网络的混合模型，用于预测人类跌倒。该模型将运动预测和步态分类任务解耦，能够高精度地预测跌倒并监控稳定到跌倒之间的过渡状态。&lt;h4&gt;背景&lt;/h4&gt;检测和预防人类跌倒是辅助机器人系统的关键组成部分。虽然跌倒检测已取得进展，但对跌倒发生前的预测以及稳定性与即将发生的跌倒之间过渡状态的分析尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种预期跌倒检测方法，能够高精度地预测跌倒，并分析稳定与即将跌倒之间的过渡状态。&lt;h4&gt;方法&lt;/h4&gt;提出一种混合模型，结合动态图神经网络（DGNN）和长短期记忆（LSTM）网络，将运动预测和步态分类任务解耦。使用从视频序列中提取的实时骨骼特征作为输入，DGNN作为分类器区分三种步态状态，LSTM网络预测后续时间步中的人体运动以实现早期跌倒检测。&lt;h4&gt;主要发现&lt;/h4&gt;模型使用OUMVLP-Pose和URFD数据集训练验证后，在预测误差和识别准确性方面优于仅依赖DGNN的模型和文献中的模型。解耦预测和分类比统一处理问题更有效，且能监控过渡状态，为辅助系统提供额外价值。&lt;h4&gt;结论&lt;/h4&gt;解耦预测和分类任务的方法可有效提高跌倒预测性能，且能够监控过渡状态，为增强先进辅助系统的功能提供有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;检测和预防人类跌倒是辅助机器人系统的关键组成部分。虽然跌倒检测方面已取得显著进展，但对跌倒发生前的预测以及稳定性与即将发生的跌倒之间过渡状态的分析尚未被探索。本文提出了一种预期跌倒检测方法，利用结合动态图神经网络和长短期记忆网络的混合模型，将运动预测和步态分类任务解耦，从而高精度地预测跌倒。我们的方法使用从视频序列中提取的实时骨骼特征作为模型输入。DGNN作为分类器，区分三种步态状态：稳定、过渡和跌倒。基于LSTM的网络随后预测后续时间步中的人体运动，实现跌倒的早期检测。所提模型使用OUMVLP-Pose和URFD数据集进行训练和验证，在预测误差和识别准确性方面表现出优于仅依赖DGNN的模型和文献中的模型。结果表明，与仅使用DGNN解决统一问题相比，解耦预测和分类可以提高性能。此外，我们的方法允许监控过渡状态，为增强先进辅助系统的功能提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting and preventing falls in humans is a critical component of assistiverobotic systems. While significant progress has been made in detecting falls,the prediction of falls before they happen, and analysis of the transient statebetween stability and an impending fall remain unexplored. In this paper, wepropose a anticipatory fall detection method that utilizes a hybrid modelcombining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory(LSTM) networks that decoupled the motion prediction and gait classificationtasks to anticipate falls with high accuracy. Our approach employs real-timeskeletal features extracted from video sequences as input for the proposedmodel. The DGNN acts as a classifier, distinguishing between three gait states:stable, transient, and fall. The LSTM-based network then predicts humanmovement in subsequent time steps, enabling early detection of falls. Theproposed model was trained and validated using the OUMVLP-Pose and URFDdatasets, demonstrating superior performance in terms of prediction error andrecognition accuracy compared to models relying solely on DGNN and models fromliterature. The results indicate that decoupling prediction and classificationimproves performance compared to addressing the unified problem using only theDGNN. Furthermore, our method allows for the monitoring of the transient state,offering valuable insights that could enhance the functionality of advancedassistance systems.</description>
      <author>example@mail.com (Younggeol Cho, Gokhan Solak, Olivia Nocentini, Marta Lorenzini, Andrea Fortuna, Arash Ajoudani)</author>
      <guid isPermaLink="false">2509.05337v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</title>
      <link>http://arxiv.org/abs/2509.06781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UrbanTwin数据集，这是三个公共路边激光雷达数据集的高保真实时副本：LUMPI、V2X-Real-IC和TUMTraf-I。每个数据集包含10K个带注释的帧，包括3D边界框、实例分割标签、跟踪ID和语义分割标签。&lt;h4&gt;背景&lt;/h4&gt;激光雷达感知任务需要大量高质量数据，但真实数据的获取可能受到限制，且场景多样性有限。&lt;h4&gt;目的&lt;/h4&gt;创建高质量的合成数据集，能够替代真实数据用于激光雷达感知任务，并增强现有基准数据集。&lt;h4&gt;方法&lt;/h4&gt;通过仿真激光雷达传感器在精确建模的数字孪生中合成数据，基于实际位置的周围几何形状、道路对齐和车辆运动模式建模。通过统计和结构相似性分析评估合成数据与真实数据的对齐情况，并训练3D目标检测模型进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;合成数据与真实数据高度相似，仅使用合成数据训练的模型在真实、未见数据上的检测性能优于使用真实数据训练的模型。UrbanTwin数据集可通过修改模拟设计来适应测试自定义场景。&lt;h4&gt;结论&lt;/h4&gt;UrbanTwin数据集是首批能够替代领域内真实世界数据集用于激光雷达感知任务的数字合成数据集，通过增加样本量和场景多样性有效增强了现有基准数据集。&lt;h4&gt;翻译&lt;/h4&gt;这篇文章介绍了UrbanTwin数据集 - 三个公共路边激光雷达数据集的高保真真实副本：LUMPI、V2X-Real-IC和TUMTraf-I。每个UrbanTwin数据集包含10K个注释帧，对应于一个公共数据集。注释包括六个类别的3D边界框、实例分割标签和跟踪ID，以及九个类别的语义分割标签。这些数据集是在真实数字孪生中使用仿真的激光雷达传感器合成的，基于实际位置的周围几何形状、车道级别的道路对齐以及交叉口的车道拓扑和车辆运动模式建模。由于精确的数字孪生建模，合成数据集与真实数据很好地对齐，为训练深度学习模型提供了强大的独立和增强价值。我们通过统计和结构相似性分析评估了合成数据与真实数据的对齐情况，并通过仅使用合成数据训练3D目标检测模型并在真实数据上进行测试，进一步证明了它们的效用。与使用真实数据训练的模型相比，高相似度分数和改进的检测性能表明，UrbanTwin数据集通过增加样本量和场景多样性有效地增强了现有基准数据集。此外，数字孪生可以通过修改模拟的设计和动力学来适应测试自定义场景。据我们所知，这些是首批能够替代领域内真实世界数据集用于激光雷达感知任务的数字合成数据集。UrbanTwin数据集可在https://dataverse.harvard.edu/dataverse/ucf-ut公开获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决创建高质量路边激光雷达数据集的高成本和低效率问题。当前真实世界标注的激光雷达数据集需要大量人力、时间和资金投入，而现有模拟环境与真实世界存在明显差距，缺乏专门针对路边激光雷达应用的合成数据集。这个问题在研究中很重要，因为激光雷达技术对智能交通系统的感知算法发展至关重要，高质量数据集是训练和评估3D感知算法的基础，而现有数据集的局限性阻碍了相关研究的进展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有数据集创建的高成本和现有模拟器的局限性，然后提出使用数字孪生技术来精确建模真实世界场景的物理结构和行为动态。他们借鉴了现有的激光雷达模拟技术，如Manivasagam的配对场景方法学和Haider的精确光线追踪模型，以及数据驱动的生成方法如CoLiGen框架。但作者的方法从根本上不同于这些工作，不是通过事后数据驱动适应来解决sim-to-real差距，而是通过高保真数字孪生建模从源头提高模拟真实性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用高保真数字孪生技术创建与真实世界路边激光雷达数据集高度匹配的合成数据集，同时模拟真实世界场景的物理结构和行为动态。整体流程包括：1)使用卫星图像和真实位置测量构建精确的3D环境模型；2)配置虚拟激光雷达传感器匹配真实传感器规格；3)随机生成符合交通规则的动态元素；4)使用CARLA模拟器在数字孪生环境中运行模拟；5)生成带标注的点云数据；6)通过统计和结构相似性分析验证数据质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门针对路边激光雷达应用的合成数据集；2)高保真数字孪生建模方法，整合静态几何和动态行为；3)通过精确建模实现最小化sim-to-real差距；4)证明完全在合成数据上训练的模型可匹配或超过真实数据训练的性能。相比之前工作，UrbanTwin更专注于路边激光雷达应用而非车辆-基础设施协作场景；从根本上提高模拟真实性而非事后数据驱动适应；实现了比之前合成数据集更接近真实世界的质量；不仅用于数据增强，还可作为独立训练数据集使用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanTwin通过高保真数字孪生技术创建了首个能够完全替代领域内真实世界数据集的合成激光雷达数据集，显著降低了sim-to-real差距，并证明了其在3D物体检测等任务中的卓越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article presents UrbanTwin datasets - high-fidelity, realistic replicasof three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.Each UrbanTwin dataset contains 10K annotated frames corresponding to one ofthe public datasets. Annotations include 3D bounding boxes, instancesegmentation labels, and tracking IDs for six object classes, along withsemantic segmentation labels for nine classes. These datasets are synthesizedusing emulated lidar sensors within realistic digital twins, modeled based onsurrounding geometry, road alignment at lane level, and the lane topology andvehicle movement patterns at intersections of the actual locationscorresponding to each real dataset. Due to the precise digital twin modeling,the synthetic datasets are well aligned with their real counterparts, offeringstrong standalone and augmentative value for training deep learning models ontasks such as 3D object detection, tracking, and semantic and instancesegmentation. We evaluate the alignment of the synthetic replicas throughstatistical and structural similarity analysis with real data, and furtherdemonstrate their utility by training 3D object detection models solely onsynthetic data and testing them on real, unseen data. The high similarityscores and improved detection performance, compared to the models trained onreal data, indicate that the UrbanTwin datasets effectively enhance existingbenchmark datasets by increasing sample size and scene diversity. In addition,the digital twins can be adapted to test custom scenarios by modifying thedesign and dynamics of the simulations. To our knowledge, these are the firstdigitally synthesized datasets that can replace in-domain real-world datasetsfor lidar perception tasks. UrbanTwin datasets are publicly available athttps://dataverse.harvard.edu/dataverse/ucf-ut.</description>
      <author>example@mail.com (Muhammad Shahbaz, Shaurya Agarwal)</author>
      <guid isPermaLink="false">2509.06781v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion</title>
      <link>http://arxiv.org/abs/2509.05999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages. Accepted to MMSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于预计算分割信息先验的解耦策略，用于单目3D目标检测，将分割信息直接融合到特征空间中指导检测，而不扩展模型或联合学习先验。&lt;h4&gt;背景&lt;/h4&gt;单目3D目标检测是一项具有挑战性的计算机视觉任务，因为输入是单个2D图像，缺乏深度线索，使得深度估计成为一个不适定的问题。&lt;h4&gt;目的&lt;/h4&gt;评估额外分割信息对现有检测管道的影响，而不添加额外的预测分支，并探索理解输入数据对减少对额外传感器或训练数据需求的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出一种解耦策略，基于注入预计算的分割信息先验，并将它们直接融合到特征空间中用于指导检测，不扩展检测模型或联合学习先验。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI 3D目标检测基准上的评估表明，所提出的方法对于场景中的小物体（行人和骑行者）的表现优于仅依赖RGB图像特征的等效架构。&lt;h4&gt;结论&lt;/h4&gt;理解输入数据可以平衡对额外传感器或训练数据的需求，证明了分割信息对单目3D目标检测的积极影响。&lt;h4&gt;翻译&lt;/h4&gt;单目3D目标检测是一项具有挑战性的计算机视觉任务，因为所使用的输入是单个2D图像，缺乏任何深度线索，并将深度估计问题作为一个不适定的问题。现有解决方案利用从输入中提取的信息，使用卷积神经网络或Transformer架构作为特征提取主干，然后使用特定的检测头来预测3D参数。在本文中，我们介绍了一种基于注入预计算分割信息先验的解耦策略，并将它们直接融合到特征空间中用于指导检测，而不扩展检测模型或联合学习先验。重点在于评估额外分割信息对现有检测管道的影响，而不添加额外的预测分支。所提出的方法在KITTI 3D目标检测基准上进行了评估，对于场景中的小物体（行人和骑行者）的表现优于仅依赖RGB图像特征的等效架构，证明了理解输入数据可以平衡对额外传感器或训练数据的需求。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D物体检测的挑战问题。单目3D物体检测使用单个2D图像作为输入，缺乏深度线索，导致这是一个'不适定'的问题。这个问题在自动驾驶和机器人领域非常重要，因为准确检测3D物体（如汽车、行人、骑行者）的位置、方向和尺寸对于安全导航至关重要。单目相机比多传感器系统更便宜、更轻量，但缺乏深度信息使其成为最具挑战性的任务，尤其是对小物体的检测。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了单目3D物体检测的挑战，认识到缺乏深度信息是主要问题。他们借鉴了'引导式单目3D物体检测'的概念，参考了使用深度图或分割图作为先验信息的工作。特别是受到了DetAny3D和MonoCInIS的启发，但选择使用分割图而非深度图，因为分割图能提供更丰富的语义信息。作者设计了一个'解耦'策略，使用Grounded SAM生成高质量分割先验，然后探索不同的融合策略和点，最终确定了元素级乘法融合和特征聚合后注入分割先验为最佳方案，整个过程不需要联合训练或添加额外预测分支。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过注入预计算的分割信息先验来增强单目3D物体检测性能，分割图作为一种注意力机制，能强调图像中与物体相关的区域，抑制不相关背景。整体流程：1)使用Grounded SAM生成分割图；2)RGB图像通过Transformer主干提取视觉特征；3)使用深度层聚合(DLA)多尺度特征；4)将分割图与特征进行空间对齐和标准化；5)通过元素级乘法融合将分割信息与视觉特征结合；6)最后通过卷积层将融合特征投影到64维空间用于3D检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)解耦的分割先验注入策略，无需联合训练；2)简单有效的元素级乘法融合方法；3)确定特征聚合后为最佳融合点；4)使用Grounded SAM生成高质量分割先验。相比之前工作的不同：与多模态方法相比，不需要额外传感器；与深度引导方法相比，提供更丰富的语义信息；与MonoCInIS相比，使用更强大的分割模型和探索了更多融合策略；与DetAny3D相比，更专注于分割先验且更轻量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; S-LAM3D通过解耦的分割先验注入和轻量级特征融合方法，显著提升了单目3D物体检测中小物体（如行人和骑行者）的检测性能，同时保持模型轻量化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D Object Detection represents a challenging Computer Vision taskdue to the nature of the input used, which is a single 2D image, lacking in anydepth cues and placing the depth estimation problem as an ill-posed one.Existing solutions leverage the information extracted from the input by usingConvolutional Neural Networks or Transformer architectures as featureextraction backbones, followed by specific detection heads for 3D parametersprediction. In this paper, we introduce a decoupled strategy based on injectingprecomputed segmentation information priors and fusing them directly into thefeature space for guiding the detection, without expanding the detection modelor jointly learning the priors. The focus is on evaluating the impact ofadditional segmentation information on existing detection pipelines withoutadding additional prediction branches. The proposed method is evaluated on theKITTI 3D Object Detection Benchmark, outperforming the equivalent architecturethat relies only on RGB image features for small objects in the scene:pedestrians and cyclists, and proving that understanding the input data canbalance the need for additional sensors or training data.</description>
      <author>example@mail.com (Diana-Alexandra Sas, Florin Oniga)</author>
      <guid isPermaLink="false">2509.05999v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation</title>
      <link>http://arxiv.org/abs/2509.05785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CRAB的新型相机-雷达融合3D目标检测和分割模型，通过利用雷达信息缓解深度歧义问题，在基于反向投影的视图转换中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;鸟瞰图(BEV)中的相机-雷达融合3D目标检测方法因这两种传感器的互补特性和成本效益而受到关注。先前使用前向投影的方法在生成稀疏BEV特征方面存在困难，而采用反向投影的方法则忽略了深度歧义，导致误报。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述局限性，提出一种新的相机-雷达融合3D目标检测和分割模型，使用反向投影并利用雷达来缓解深度歧义问题。&lt;h4&gt;方法&lt;/h4&gt;CRAB在视图转换过程中将透视视图图像上下文特征聚合到BEV查询中，通过结合图像中密集但不可靠的深度分布与雷达占用图中稀疏但精确的深度信息，提高沿同一射线的查询之间的深度区分度。同时引入包含雷达上下文信息的特征图的空间交叉注意力机制，增强对3D场景的理解。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes开放数据集上评估时，所提出的方法在基于反向投影的相机-雷达融合方法中取得了最先进的性能，3D目标检测达到62.4%的NDS和54.0%的mAP。&lt;h4&gt;结论&lt;/h4&gt;CRAB模型通过利用雷达信息缓解深度歧义问题，有效提高了基于反向投影的相机-雷达融合3D目标检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，基于相机-雷达融合的鸟瞰图(BEV)3D目标检测方法因这些传感器的互补特性和成本效益而受到关注。先前使用前向投影的方法在生成稀疏BEV特征方面存在困难，而采用反向投影的方法则忽略了深度歧义，导致误报。在本文中，为了解决上述局限性，我们提出了一种名为CRAB的新型相机-雷达融合3D目标检测和分割模型，使用利用雷达缓解深度歧义的反向投影。在视图转换过程中，CRAB将透视视图图像上下文特征聚合到BEV查询中。它通过将图像中密集但不可靠的深度分布与雷达占用图中稀疏但精确的深度信息相结合，提高了沿同一射线的查询之间的深度区分度。我们还引入了包含雷达上下文信息的特征图的空间交叉注意力机制，以增强对3D场景的理解。在nuScenes开放数据集上进行评估时，我们提出的方法在基于反向投影的相机-雷达融合方法中取得了最先进的性能，3D目标检测达到62.4%的NDS和54.0%的mAP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于后向投影的视角变换方法中的深度模糊问题。这个问题在自动驾驶和移动机器人领域非常重要，因为它直接影响3D目标检测的准确性。深度模糊导致沿同一射线的查询获得相同特征，无法区分物体深度，产生假阳性检测结果，影响自动驾驶系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：前向投影方法生成稀疏BEV特征，后向投影方法存在深度模糊问题。他们注意到相机提供密集但不可靠的深度信息，而雷达提供稀疏但精确的深度信息，决定结合两者优势。方法设计借鉴了BEVFormer和DFA3D的后向投影框架，CRN中利用雷达占用的思想，以及变形注意力机制。作者设计了两个核心模块：雷达占用引导的空间交叉注意力(ROSCA)和雷达上下文感知的空间交叉注意力(RCSCA)，分阶段解决深度模糊并整合雷达上下文信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合相机和雷达的互补优势，解决后向投影中的深度模糊问题。整体流程包括：1)输入处理：使用图像主干网络提取图像特征并获取深度分布，处理雷达点云并提取雷达特征；2)视角变换：首先通过ROSCA模块结合相机和雷达的占用信息解决深度模糊问题，然后通过RCSCA模块整合雷达上下文信息增强3D场景理解；3)任务头：将融合后的BEV特征输入到特定任务的头部进行3D目标检测或分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出ROSCA模块，结合相机和雷达的占用信息解决深度模糊问题；2)提出RCSCA模块，利用雷达上下文信息增强3D场景理解；3)采用锥形视图而非鸟瞰视图的雷达特征图，确保特征一致性。相比前向投影方法，CRAB生成更密集的BEV特征；相比其他后向投影方法，CRAB专门解决深度模糊问题；相比仅使用图像深度分布的方法，CRAB利用雷达精确深度信息校准相机估计；相比传统融合方法，CRAB在统一BEV空间中融合，适用于多种下游任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CRAB通过创新的相机-雷达融合方法，有效解决了基于后向投影的视角变换中的深度模糊问题，显著提高了3D目标检测和分割的准确性，特别是在恶劣天气条件下展现了优越的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, camera-radar fusion-based 3D object detection methods in bird's eyeview (BEV) have gained attention due to the complementary characteristics andcost-effectiveness of these sensors. Previous approaches using forwardprojection struggle with sparse BEV feature generation, while those employingbackward projection overlook depth ambiguity, leading to false positives. Inthis paper, to address the aforementioned limitations, we propose a novelcamera-radar fusion-based 3D object detection and segmentation model named CRAB(Camera-Radar fusion for reducing depth Ambiguity in Backward projection-basedview transformation), using a backward projection that leverages radar tomitigate depth ambiguity. During the view transformation, CRAB aggregatesperspective view image context features into BEV queries. It improves depthdistinction among queries along the same ray by combining the dense butunreliable depth distribution from images with the sparse yet precise depthinformation from radar occupancy. We further introduce spatial cross-attentionwith a feature map containing radar context information to enhance thecomprehension of the 3D scene. When evaluated on the nuScenes open dataset, ourproposed approach achieves a state-of-the-art performance among backwardprojection-based camera-radar fusion methods with 62.4\% NDS and 54.0\% mAP in3D object detection.</description>
      <author>example@mail.com (In-Jae Lee, Sihwan Hwang, Youngseok Kim, Wonjune Kim, Sanmin Kim, Dongsuk Kum)</author>
      <guid isPermaLink="false">2509.05785v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>3DPillars: Pillar-based two-stage 3D object detection</title>
      <link>http://arxiv.org/abs/2509.05780v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的两阶段3D检测框架，利用伪图像表示缩小了PointPillars与最先进方法之间的性能差距，同时保持了其效率。&lt;h4&gt;背景&lt;/h4&gt;PointPillars是最快的3D目标检测器之一，利用伪图像表示编码场景中3D对象的特征，但通常被最先进的3D检测方法超越。&lt;h4&gt;目的&lt;/h4&gt;引入第一个利用伪图像表示的两阶段3D检测框架，缩小PointPillars与最先进方法之间的性能差距，同时保持其效率。&lt;h4&gt;方法&lt;/h4&gt;提出两个新颖组件：1) 3DPillars架构，通过可分离体素特征模块从伪图像表示中高效学习基于3D体素的特征；2) 带有稀疏场景上下文特征模块的RoI头，聚合多尺度特征以获得稀疏场景特征，有效采用两阶段管道并充分利用场景上下文信息。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和Waymo Open数据集上的实验证明，该方法在速度和准确性之间取得了良好的平衡。&lt;h4&gt;结论&lt;/h4&gt;该研究成功克服了PointPillars的两个主要局限性：无法保留精确的3D结构和难以采用两阶段检测管道，同时保持了PointPillars的效率。&lt;h4&gt;翻译&lt;/h4&gt;PointPillars是最快的3D目标检测器，它利用伪图像表示来编码场景中3D对象的特征。尽管高效，PointPillars通常由于以下限制而被最先进的3D检测方法超越：1) 伪图像表示无法保留精确的3D结构，2) 它们使得难以采用使用3D对象提议的两阶段检测管道，而这种方法通常比单阶段方法表现更好。我们在本文中引入了第一个利用伪图像表示的两阶段3D检测框架，缩小了PointPillars与最先进方法之间的性能差距，同时保留了其效率。我们的框架由两个新颖的组件组成，克服了PointPillars的上述局限性：首先，我们引入了一种新的CNN架构，称为3DPillars，它能够使用2D卷积从伪图像表示中高效学习基于3D体素的特征。3DPillars背后的基本思想是，来自体素的3D特征可以被视为伪图像的堆叠。为了实现这一想法，我们提出了一个可分离的体素特征模块，它不使用3D卷积来提取基于体素的特征。其次，我们引入了一个带有稀疏场景上下文特征模块的RoI头，它聚合来自3DPillars的多尺度特征以获得稀疏场景特征。这使得能够有效采用两阶段管道，并充分利用场景的上下文信息来改进3D对象提议。在KITTI和Waymo Open数据集上的实验结果证明了我们方法的有效性和效率，在速度和准确性方面取得了良好的平衡。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决PointPillars方法在3D物体检测中的两个局限性：1)伪图像表示无法保留精确的3D结构；2)难以采用通常性能更好的两阶段检测管道。这个问题在自动驾驶和机器人领域非常重要，因为3D物体检测是理解周围环境的关键组件，精确的3D结构信息对于准确识别和定位物体至关重要，而两阶段检测方法通常比单阶段方法有更好的检测性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了PointPillars的局限性，发现它无法保留精确的3D结构且难以采用两阶段检测框架。作者受到基于体素的方法能够保留精细3D结构的启发，但注意到3D卷积计算成本高。因此提出将3D体素特征视为伪图像堆栈的想法，这样可以用2D卷积高效地提取3D特征。作者借鉴了PointPillars的支柱表示方法、Zhou &amp; Tuzel的体素特征编码(VFE)、Deng et al.的体素RoI池化方法以及Miller et al.的键值记忆模块等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D体素特征视为沿X、Y、Z轴的伪图像堆栈，从而可以用2D卷积高效提取3D特征；同时引入稀疏场景上下文特征模块，利用全局场景上下文来改进物体建议。整体流程是：1)输入点云被分配到3D体素网格；2)使用VFE层提取初始特征；3)将3D体素特征分割为伪图像堆栈；4)使用SVFM模块提取多尺度视图特定特征；5)将多尺度特征转换为2D BEV特征图；6)使用RPN生成3D物体提议；7)在RoI头中，S2CFM模块聚合多尺度特征，提取RoI特征，结合全局上下文特征生成上下文感知的RoI表示；8)使用全连接层预测3D边界框和置信度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)3DPillars架构，第一个利用伪图像表示的两阶段3D检测框架；2)可分离体素特征模块(SVFM)，通过2D卷积提取视图特定特征；3)带有稀疏场景上下文特征模块(S2CFM)的RoI头，聚合多尺度特征并利用键值记忆模块获取全局场景上下文。相比之前的工作，它保留了更精细的3D结构并支持两阶段检测，使用2D卷积而非3D卷积提高了效率，不需要对点云下采样，并引入了全局场景上下文信息，特别有利于小物体检测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了3DPillars，一种创新的两阶段3D物体检测框架，通过将3D体素特征表示为伪图像堆栈并利用稀疏场景上下文信息，在保持高效的同时显著提高了检测精度，特别是在处理小物体和远距离物体时表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.eswa.2025.128349&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PointPillars is the fastest 3D object detector that exploits pseudo imagerepresentations to encode features for 3D objects in a scene. Albeit efficient,PointPillars is typically outperformed by state-of-the-art 3D detection methodsdue to the following limitations: 1) The pseudo image representations fail topreserve precise 3D structures, and 2) they make it difficult to adopt atwo-stage detection pipeline using 3D object proposals that typically showsbetter performance than a single-stage approach. We introduce in this paper thefirst two-stage 3D detection framework exploiting pseudo image representations,narrowing the performance gaps between PointPillars and state-of-the-artmethods, while retaining its efficiency. Our framework consists of two novelcomponents that overcome the aforementioned limitations of PointPillars: First,we introduce a new CNN architecture, dubbed 3DPillars, that enables learning 3Dvoxel-based features from the pseudo image representation efficiently using 2Dconvolutions. The basic idea behind 3DPillars is that 3D features from voxelscan be viewed as a stack of pseudo images. To implement this idea, we propose aseparable voxel feature module that extracts voxel-based features without using3D convolutions. Second, we introduce an RoI head with a sparse scene contextfeature module that aggregates multi-scale features from 3DPillars to obtain asparse scene feature. This enables adopting a two-stage pipeline effectively,and fully leveraging contextual information of a scene to refine 3D objectproposals. Experimental results on the KITTI and Waymo Open datasetsdemonstrate the effectiveness and efficiency of our approach, achieving a goodcompromise in terms of speed and accuracy.</description>
      <author>example@mail.com (Jongyoun Noh, Junghyup Lee, Hyekang Park, Bumsub Ham)</author>
      <guid isPermaLink="false">2509.05780v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2509.05332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个新的开源集成模拟框架，用于生成针对自动驾驶车辆感知和通信层的对抗性攻击，通过高保真建模和统一协调系统，有效测试自动驾驶系统在对抗环境下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆依赖复杂的感知和通信系统，使其容易受到对抗性攻击，危及安全。现有模拟框架通常缺乏对多领域对抗性场景的全面支持。&lt;h4&gt;目的&lt;/h4&gt;开发一个开源集成模拟框架，能够生成针对自动驾驶车辆感知和通信层的对抗性攻击，并提供高保真度的物理环境、交通动态和V2X网络建模。&lt;h4&gt;方法&lt;/h4&gt;框架通过统一的核心协调物理环境、交通动态和V2X网络建模，基于单个配置文件同步多个模拟器，支持对LiDAR传感器数据的感知级别攻击和V2X消息操纵、GPS欺骗等通信级别威胁，并集成ROS 2确保与第三方自动驾驶软件栈的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;通过评估生成的对抗性场景对最先进3D目标检测器的影响，证明在现实条件下，自动驾驶系统的性能会显著下降，验证了框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;该模拟框架为自动驾驶系统对抗性攻击的研究提供了全面工具，有助于提高自动驾驶系统在面对复杂威胁时的安全性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶车辆(AVs)依赖复杂的感知和通信系统，使其容易受到对抗性攻击，可能危及安全。虽然模拟为鲁棒性测试提供了可扩展且安全的环境，但现有框架通常缺乏对多领域对抗性场景的全面建模支持。本文介绍了一个新的开源集成模拟框架，旨在生成针对自动驾驶车辆感知和通信层的对抗性攻击。该框架提供物理环境、交通动态和V2X网络的高保真建模，通过基于单一配置文件同步多个模拟器的统一核心协调这些组件。我们的实现支持对LiDAR传感器数据的多种感知级别攻击，以及V2X消息操纵和GPS欺骗等通信级别威胁。此外，ROS 2集成确保与第三方自动驾驶软件栈的无缝兼容。我们通过评估生成的对抗性场景对最先进3D目标检测器的影响，证明了框架的有效性，显示在现实条件下性能显著下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicles (AVs) rely on complex perception and communicationsystems, making them vulnerable to adversarial attacks that can compromisesafety. While simulation offers a scalable and safe environment for robustnesstesting, existing frameworks typically lack comprehensive supportfor modelingmulti-domain adversarial scenarios. This paper introduces a novel, open-sourceintegrated simulation framework designed to generate adversarial attackstargeting both perception and communication layers of AVs. The frameworkprovides high-fidelity modeling of physical environments, traffic dynamics, andV2X networking, orchestrating these components through a unified core thatsynchronizes multiple simulators based on a single configuration file. Ourimplementation supports diverse perception-level attacks on LiDAR sensor data,along with communication-level threats such as V2X message manipulation and GPSspoofing. Furthermore, ROS 2 integration ensures seamless compatibility withthird-party AV software stacks. We demonstrate the framework's effectiveness byevaluating the impact of generated adversarial scenarios on a state-of-the-art3D object detector, revealing significant performance degradation underrealistic conditions.</description>
      <author>example@mail.com (Christos Anagnostopoulos, Ioulia Kapsali, Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos)</author>
      <guid isPermaLink="false">2509.05332v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Harnessing Object Grounding for Time-Sensitive Video Understanding</title>
      <link>http://arxiv.org/abs/2509.06335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GO-Tokenizer的轻量级模块，通过引入基础对象(Grounded Objects)来增强视频大语言模型(Video-LLMs)的时间敏感视频理解(TSV)能力，解决了传统方法中标记长度过长和对象信息噪声敏感的问题。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型在时间敏感视频理解任务中面临挑战，现有方法虽然可以通过添加对象注释的文本描述来提高性能，但会引入额外的标记长度和对对象级别信息噪声的敏感性。&lt;h4&gt;目的&lt;/h4&gt;改进视频大语言模型的时间敏感视频理解能力，使其能够更有效地利用帧内基础对象信息，同时避免传统方法的缺点。&lt;h4&gt;方法&lt;/h4&gt;提出GO-Tokenizer，一个轻量级的Video-LLMs附加模块，利用现成的对象检测器即时编码紧凑的对象信息，而不是使用冗长的文本描述。&lt;h4&gt;主要发现&lt;/h4&gt;1) 基础对象可以提升时间敏感视频理解任务性能；2) 使用GO-Tokenizer进行预训练的性能优于普通Video-LLM；3) GO-Tokenizer的性能优于在提示中使用对象文本描述的方法；4) 这种改进在不同模型、数据集和视频理解任务中具有普遍性。&lt;h4&gt;结论&lt;/h4&gt;GO-Tokenizer是一种有效的解决方案，能够增强Video-LLMs的时间敏感视频理解能力，同时避免了传统方法的局限性，在多种场景下表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出通过基础对象(GO)来提高视频大语言模型(Video-LLMs)的时间敏感视频理解(TSV)能力。我们假设TSV任务可以从帧内的基础对象中受益，这一点在我们的初步实验中得到了验证，实验对象是LITA——一种用于推理时间定位的最先进Video-LLM。虽然在提示中加入这些对象注释的文本描述可以提高LITA的性能，但它也引入了额外的标记长度和对对象级别信息噪声的敏感性。为解决这一问题，我们提出了GO-Tokenizer，这是一个轻量级的Video-LLMs附加模块，利用现成的对象检测器即时编码紧凑的对象信息。实验结果表明，使用GO-Tokenizer进行预训练的性能优于普通Video-LLM及其在提示中利用对象文本描述的对应模型。这种增益在不同模型、数据集和视频理解任务(如推理时间定位和密集字幕)中具有普遍性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose to improve the time-sensitive video understanding (TSV) capabilityof video large language models (Video-LLMs) with grounded objects (GO). Wehypothesize that TSV tasks can benefit from GO within frames, which issupported by our preliminary experiments on LITA, a state-of-the-art Video-LLMfor reasoning temporal localization. While augmenting prompts with textualdescription of these object annotations improves the performance of LITA, italso introduces extra token length and susceptibility to the noise in objectlevel information. To address this, we propose GO-Tokenizer, a lightweightadd-on module for Video-LLMs leveraging off-the-shelf object detectors toencode compact object information on the fly. Experimental results demonstratethat pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and itscounterpart utilizing textual description of objects in the prompt. The gaingeneralizes across different models, datasets and video understanding taskssuch as reasoning temporal localization and dense captioning.</description>
      <author>example@mail.com (Tz-Ying Wu, Sharath Nittur Sridhar, Subarna Tripathi)</author>
      <guid isPermaLink="false">2509.06335v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization</title>
      <link>http://arxiv.org/abs/2509.05604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IJCV, 29 pages, 14 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VideoGraph方法，将视频建模为语言引导的时空图网络，通过整合语义关系和语言查询来提高视频摘要质量，在多个基准测试中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有视频摘要方法主要关注帧之间的全局时间关系，忽视了细粒度视觉实体（如对象）与视频内容的关联，且语言引导的视频摘要需要全面理解复杂视频内容。&lt;h4&gt;目的&lt;/h4&gt;为了考虑所有对象之间的语义关系，将视频摘要视为一个语言引导的时空图建模问题，以提高摘要质量。&lt;h4&gt;方法&lt;/h4&gt;提出递归时空图网络VideoGraph，将对象和帧分别作为空间图和时间图的节点，通过图边连接表示语义关系，并引入语言查询增强节点表示，采用递归策略优化初始图并分类关键帧。&lt;h4&gt;主要发现&lt;/h4&gt;VideoGraph在通用和查询导向的视频摘要任务上，无论有监督还是无监督方式，均取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过将视频摘要建模为语言引导的时空图问题，并利用语言查询增强节点表示，能够有效提高视频摘要的性能，生成更具代表性的摘要。&lt;h4&gt;翻译&lt;/h4&gt;视频摘要旨在选择视觉上多样化且能代表给定视频整个故事的关键帧。先前的方法已通过时间建模关注了视频中帧之间的全局互联性。然而，细粒度的视觉实体（如对象）也与视频的主要内容高度相关。此外，最近研究的语言引导视频摘要需要对复杂的现实世界视频进行全面的语言理解。为了考虑所有对象之间的语义关系，本文将视频摘要视为一个语言引导的时空图建模问题。作者提出了递归时空图网络，称为VideoGraph，它将对象和帧分别作为空间图和时间图的节点。每个图中的节点通过图边连接和聚合，表示节点间的语义关系。为了防止边仅基于视觉相似性配置，作者将视频衍生的语言查询整合到图节点表示中，使节点包含语义知识。此外，作者采用递归策略来优化初始图并正确分类每个帧节点为关键帧。在实验中，VideoGraph在多个基准测试中，无论是有监督还是无监督方式，都在通用和查询导向的视频摘要任务上取得了最先进的性能。代码可在https://github.com/park-jungin/videograph获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s11263-025-02577-2&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video summarization aims to select keyframes that are visually diverse andcan represent the whole story of a given video. Previous approaches havefocused on global interlinkability between frames in a video by temporalmodeling. However, fine-grained visual entities, such as objects, are alsohighly related to the main content of the video. Moreover, language-guidedvideo summarization, which has recently been studied, requires a comprehensivelinguistic understanding of complex real-world videos. To consider how all theobjects are semantically related to each other, this paper regards videosummarization as a language-guided spatiotemporal graph modeling problem. Wepresent recursive spatiotemporal graph networks, called VideoGraph, whichformulate the objects and frames as nodes of the spatial and temporal graphs,respectively. The nodes in each graph are connected and aggregated with graphedges, representing the semantic relationships between the nodes. To preventthe edges from being configured with visual similarity, we incorporate languagequeries derived from the video into the graph node representations, enablingthem to contain semantic knowledge. In addition, we adopt a recursive strategyto refine initial graphs and correctly classify each frame node as a keyframe.In our experiments, VideoGraph achieves state-of-the-art performance on severalbenchmarks for generic and query-focused video summarization in both supervisedand unsupervised manners. The code is available athttps://github.com/park-jungin/videograph.</description>
      <author>example@mail.com (Jungin Park, Jiyoung Lee, Kwanghoon Sohn)</author>
      <guid isPermaLink="false">2509.05604v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Kwai Keye-VL 1.5 Technical Report</title>
      <link>http://arxiv.org/abs/2509.01563v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github page: https://github.com/Kwai-Keye/Keye&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Keye-VL-1.5模型，通过三种创新方法解决了视频理解中的挑战，在视频理解任务和通用多模态基准测试中展现出显著优势。&lt;h4&gt;背景&lt;/h4&gt;近年来大语言模型已扩展到多模态任务，但视频理解仍具挑战性，因为视频具有动态和信息密集的特点。现有模型在处理视频时难以平衡空间分辨率和时间覆盖范围。&lt;h4&gt;目的&lt;/h4&gt;解决视频理解中的基本挑战，开发一种能够更有效处理视频内容的新型模型。&lt;h4&gt;方法&lt;/h4&gt;1. 引入慢速-快速视频编码策略，根据帧间相似度动态分配计算资源；2. 实现渐进式四阶段预训练方法，将模型上下文长度从8K扩展到128K tokens；3. 开发全面的训练后流程，包括5步思维链数据构建、基于GSPO的迭代强化学习和对齐训练。&lt;h4&gt;主要发现&lt;/h4&gt;Keye-VL-1.5在公共基准测试和内部人工评估中显著优于现有模型，尤其在视频理解任务上表现突出，同时在通用多模态基准测试中保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;Keye-VL-1.5通过三种关键创新成功解决了视频理解中的基本挑战，实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;近年来，大语言模型的发展取得了显著进步，其能力已通过多模态大语言模型扩展到多模态任务。然而，由于视频的动态性和信息密集特性，视频理解仍然是一个具有挑战性的领域。现有模型在处理视频内容时，难以在空间分辨率和时间覆盖范围之间取得平衡。我们提出了Keye-VL-1.5，通过三个关键创新解决了视频理解中的基本挑战。首先，我们引入了一种新颖的慢速-快速视频编码策略，根据帧间相似度动态分配计算资源，以更高分辨率处理具有显著视觉变化的关键帧（慢速路径），同时以更低分辨率但增加时间覆盖范围处理相对静态的帧（快速路径）。其次，我们实现了渐进式四阶段预训练方法，系统地将模型的上下文长度从8K扩展到128K tokens，使模型能够处理更长的视频和更复杂的视觉内容。第三，我们开发了一个全面的训练后流程，专注于推理增强和人类偏好对齐，包含5步思维链数据构建过程、基于GSPO的迭代强化学习以及对困难案例的渐进式提示提示，以及训练对齐。通过在公共基准测试上的广泛评估和严格的内部人工评估，Keye-VL-1.5显示出比现有模型的显著改进，尤其在视频理解任务上表现出色，同时在通用多模态基准测试中保持有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the development of Large Language Models (LLMs) hassignificantly advanced, extending their capabilities to multimodal tasksthrough Multimodal Large Language Models (MLLMs). However, video understandingremains a challenging area due to the dynamic and information-dense nature ofvideos. Existing models struggle with the trade-off between spatial resolutionand temporal coverage when processing video content. We present Keye-VL-1.5,which addresses fundamental challenges in video comprehension through three keyinnovations. First, we introduce a novel Slow-Fast video encoding strategy thatdynamically allocates computational resources based on inter-frame similarity,processing key frames with significant visual changes at higher resolution(Slow pathway) while handling relatively static frames with increased temporalcoverage at lower resolution (Fast pathway). Second, we implement a progressivefour-stage pre-training methodology that systematically extends the model'scontext length from 8K to 128K tokens, enabling processing of longer videos andmore complex visual content. Third, we develop a comprehensive post-trainingpipeline focusing on reasoning enhancement and human preference alignment,incorporating a 5-step chain-of-thought data construction process, iterativeGSPO-based reinforcement learning with progressive prompt hinting for difficultcases, and alignment training. Through extensive evaluation on publicbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstratessignificant improvements over existing models, particularly excelling in videounderstanding tasks while maintaining competitive performance on generalmultimodal benchmarks.</description>
      <author>example@mail.com (Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Haonan Fan, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang)</author>
      <guid isPermaLink="false">2509.01563v3</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition</title>
      <link>http://arxiv.org/abs/2509.04344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE ISPA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为MICACL的新型多实例学习框架，用于解决动态面部表情识别中的长尾类别分布和时空特征建模复杂性问题。该框架结合了时空依赖建模和长尾对比学习优化，通过特定模块增强特征提取，并采用多尺度类别感知对比学习策略平衡各类别训练。实验证明该方法在野外数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;动态面部表情识别(DFER)面临两个主要挑战：长尾类别分布和时空特征建模的复杂性。现有的基于深度学习的方法虽然提高了DFER性能，但未能有效解决这些问题，导致模型引入偏差。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，解决动态面部表情识别中的长尾类别分布问题和时空特征建模复杂性，减少模型诱导偏差，提高模型的鲁棒性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为MICACL的多实例学习框架，包括：1) 图增强实例交互模块(GEIIM)：通过自适应邻接矩阵和多尺度卷积捕获相邻实例间的复杂时空关系；2) 加权实例聚合网络(WIAN)：根据实例重要性动态分配权重，增强实例级特征聚合；3) 多尺度类别感知对比学习(MCCL)策略：平衡主要和次要类别的训练。&lt;h4&gt;主要发现&lt;/h4&gt;在野外数据集(DFEW和FERV39k)上的大量实验表明，MICACL实现了最先进的性能，具有优越的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MICACL框架有效解决了动态面部表情识别中的关键挑战，通过整合时空依赖建模和长尾对比学习优化，显著提高了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;动态面部表情识别(DFER)由于长尾类别分布和时空特征建模的复杂性而面临重大挑战。虽然现有的基于深度学习的方法已经提高了DFER性能，但它们通常无法解决这些问题，导致严重的模型诱导偏差。为了克服这些局限性，我们提出了一种名为MICACL的新型多实例学习框架，该框架整合了时空依赖建模和长尾对比学习优化。具体来说，我们设计了图增强实例交互模块(GEIIM)，通过自适应邻接矩阵和多尺度卷积来捕获相邻实例之间的复杂时空关系。为了增强实例级特征聚合，我们开发了加权实例聚合网络(WIAN)，它根据实例重要性动态分配权重。此外，我们引入了多尺度类别感知对比学习(MCCL)策略，以平衡主要和次要类别之间的训练。在野外数据集(即DFEW和FERV39k)上的大量实验表明，MICACL实现了最先进的性能，具有优越的鲁棒性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic facial expression recognition (DFER) faces significant challenges dueto long-tailed category distributions and complexity of spatio-temporal featuremodeling. While existing deep learning-based methods have improved DFERperformance, they often fail to address these issues, resulting in severe modelinduction bias. To overcome these limitations, we propose a novelmulti-instance learning framework called MICACL, which integratesspatio-temporal dependency modeling and long-tailed contrastive learningoptimization. Specifically, we design the Graph-Enhanced Instance InteractionModule (GEIIM) to capture intricate spatio-temporal between adjacent instancesrelationships through adaptive adjacency matrices and multiscale convolutions.To enhance instance-level feature aggregation, we develop the Weighted InstanceAggregation Network (WIAN), which dynamically assigns weights based on instanceimportance. Furthermore, we introduce a Multiscale Category-aware ContrastiveLearning (MCCL) strategy to balance training between major and minorcategories. Extensive experiments on in-the-wild datasets (i.e., DFEW andFERV39k) demonstrate that MICACL achieves state-of-the-art performance withsuperior robustness and generalization.</description>
      <author>example@mail.com (Feng-Qi Cui, Zhen Lin, Xinlong Rao, Anyang Tong, Shiyao Li, Fei Wang, Changlin Chen, Bin Liu)</author>
      <guid isPermaLink="false">2509.04344v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
  <item>
      <title>FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</title>
      <link>http://arxiv.org/abs/2509.05297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 - Project Page: https://flowseek25.github.io/ - Code:  https://github.com/mattpoggi/flowseek&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlowSeek是一个新颖的光流计算框架，结合了最新的光流网络设计、单图像深度基础模型和低维运动参数化，实现了在有限硬件资源下的高效训练和高精度光流估计。&lt;h4&gt;背景&lt;/h4&gt;光流计算在计算机视觉领域具有重要意义，但现有的先进方法通常需要大量计算资源进行训练，限制了其在资源受限环境中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个资源高效的光流计算框架，能够在有限的硬件条件下实现高性能的光流估计，并保持良好的跨数据集泛化能力。&lt;h4&gt;方法&lt;/h4&gt;FlowSeek结合了光流网络设计空间的最新进展、单图像深度基础模型和经典低维运动参数化，实现了紧凑而准确的架构。该方法在单个消费级GPU上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;FlowSeek在硬件预算比最新方法低约8倍的情况下，在Sintel Final和KITTI数据集上实现了比之前最先进的SEA-RAFT方法高10%和15%的相对改进，同时在Spring和LayeredFlow数据集上也表现优异。&lt;h4&gt;结论&lt;/h4&gt;FlowSeek证明了通过结合最新的网络设计、基础模型和经典参数化方法，可以在显著降低硬件需求的同时，实现更优的光流估计性能和跨数据集泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了FlowSeek，这是一个用于光流计算的新颖框架，训练时只需最少的硬件资源。FlowSeek将光流网络设计空间的最新进展与前沿的单图像深度基础模型和经典低维运动参数化相结合，实现了紧凑而准确的架构。FlowSeek在单个消费级GPU上训练，硬件预算比最新方法低约8倍，但在Sintel Final和KITTI上仍实现了比之前最先进的SEA-RAFT高10%和15%的相对改进，同时在Spring和LayeredFlow数据集上也表现优异。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决光流估计领域中的两个关键问题：一是当前最先进的光流模型需要大量高端GPU进行训练，硬件成本过高；二是现有模型在不同数据集上的泛化能力有限。这个问题很重要，因为光流估计是计算机视觉的基础任务，应用于动作识别、视频插值和4D重建等高级任务；高昂的硬件成本限制了资源有限的研究团队参与前沿研究；模型泛化能力不足限制了实际应用场景，因为真实世界场景多样性远超训练数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察当前光流研究过度依赖硬件资源的现象，借鉴了三个不同领域的知识：最新的光流网络设计空间(特别是SEA-RAFT架构)、先进的单图像深度基础模型(如Depth Anything v2)和经典计算机视觉中的低维运动参数化方法。作者认为可以通过重用已经训练好的基础模型来减少硬件依赖，并将深度模型的光学先验知识整合到光流估计中，同时利用运动基来约束光流估计空间。这种将30年光流研究、最近的深度基础模型和经典方法结合的创新思路，形成了FlowSeek的设计基础。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; FlowSeek的核心思想是通过整合深度基础模型的几何先验知识、低维运动参数化和最新光流网络架构，实现高效且准确的光流估计。整体流程包括：1)输入一对图像；2)通过特征提取器生成图像特征，同时用深度基础模型估计深度图和提取深度特征；3)融合原始特征与深度特征构建增强特征；4)基于深度图计算运动基并提取其特征；5)通过上下文网络获取上下文特征和初始隐藏状态；6)通过迭代优化过程结合相关体积、隐藏状态和上下文特征逐步 refine 光流估计；7)使用混合拉普拉斯分布的负对数似然作为损失函数，在单个GPU上完成训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个整合深度基础模型的光流模型；2)可在单个消费级GPU上训练，硬件需求比最新方法低约8倍；3)在多个数据集上实现了零样本泛化，性能比之前最先进的SEA-RAFT高出10-15%；4)三重知识整合(光流网络、深度基础模型、经典运动参数化)；5)有效利用运动基提供的先验知识。相比之前工作，FlowSeek不从头训练大型网络而是重用深度模型知识，将深度信息深度整合到光流流程而非仅作为辅助输入，在保持低计算成本的同时实现更好性能，特别是在细节恢复和跨域泛化方面表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FlowSeek通过整合深度基础模型的几何先验知识和经典运动参数化方法，在显著降低硬件资源需求的同时，实现了更准确的光流估计和更强的跨域泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present FlowSeek, a novel framework for optical flow requiring minimalhardware resources for training. FlowSeek marries the latest advances on thedesign space of optical flow networks with cutting-edge single-image depthfoundation models and classical low-dimensional motion parametrization,implementing a compact, yet accurate architecture. FlowSeek is trained on asingle consumer-grade GPU, a hardware budget about 8x lower compared to mostrecent methods, and still achieves superior cross-dataset generalization onSintel Final and KITTI, with a relative improvement of 10 and 15% over theprevious state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlowdatasets.</description>
      <author>example@mail.com (Matteo Poggi, Fabio Tosi)</author>
      <guid isPermaLink="false">2509.05297v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>BEDTime: A Unified Benchmark for Automatically Describing Time Series</title>
      <link>http://arxiv.org/abs/2509.05215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个标准化的评估框架，用于测试模型使用自然语言描述时间序列的能力，通过统一多个数据集和明确定义三个核心任务，实现了对不同模型的直接比较，并发现了当前模型在特定架构和鲁棒性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;最近许多研究提出了用于多种时间序列分析任务的通用基础模型。虽然已有一些评估这些模型的数据集，但先前的研究经常在介绍模型的同时引入新数据集，这限制了直接、独立的比较机会，模糊了不同方法相对优势的见解。此外，先前的评估通常同时涵盖众多任务，评估广泛的模型能力，而没有明确指出哪些能力对整体性能有贡献。&lt;h4&gt;目的&lt;/h4&gt;解决上述评估差距，正式评估3个测试模型使用通用自然语言描述时间系列能力的任务，并统一4个最近的数据集，使模型能够在每个任务上进行直接比较。&lt;h4&gt;方法&lt;/h4&gt;定义并评估3个任务：(1)识别（是/否问题回答），(2)区分（多项选择题回答），(3)生成（开放式自然语言描述）。统一4个最近的数据集，并评估13种最先进的语言、视觉-语言和时间序列-语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;(1)流行的纯语言方法表现不佳，表明需要时间序列特定的架构；(2)视觉-语言模型非常成功，突显了视觉模型对这些任务的价值；(3)预训练的多模态时间序列-语言模型成功优于大型语言模型，但仍有显著改进空间；(4)所有方法在一系列鲁棒性测试中都表现出明显的脆弱性。&lt;h4&gt;结论&lt;/h4&gt;该基准为时间序列推理系统提供了标准化评估，有助于更清晰地了解不同模型的相对优势和不足。&lt;h4&gt;翻译&lt;/h4&gt;最近许多研究提出了用于各种时间序列分析任务的通用基础模型。虽然已经有一些既定的数据集用于评估这些模型，但先前的工作经常在介绍其模型的同时引入新数据集，限制了直接、独立比较的机会，并模糊了对不同方法相对优势的见解。此外，先前的评估通常同时涵盖众多任务，评估广泛的模型能力，而没有明确指出哪些能力对整体性能有贡献。为了解决这些差距，我们正式评估了3个任务，测试模型使用通用自然语言描述时间序列的能力：(1)识别（是/否问题回答），(2)区分（多项选择题回答），(3)生成（开放式自然语言描述）。然后我们统一了4个最近的数据集，使模型能够在每个任务上进行直接比较。实验中，在评估13种最先进的语言、视觉-语言和时间序列-语言模型时，我们发现(1)流行的纯语言方法表现不佳，表明需要时间序列特定的架构，(2)视觉-语言模型非常成功，正如预期的那样，突显了视觉模型对这些任务的价值，(3)预训练的多模态时间序列-语言模型成功优于大型语言模型，但仍有显著的改进空间。我们还发现所有方法在一系列鲁棒性测试中都表现出明显的脆弱性。总的来说，我们的基准为时间序列推理系统提供了必要的标准化评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many recent studies have proposed general-purpose foundation models designedfor a variety of time series analysis tasks. While several established datasetsalready exist for evaluating these models, previous works frequently introducetheir models in conjunction with new datasets, limiting opportunities fordirect, independent comparisons and obscuring insights into the relativestrengths of different methods. Additionally, prior evaluations often covernumerous tasks simultaneously, assessing a broad range of model abilitieswithout clearly pinpointing which capabilities contribute to overallperformance. To address these gaps, we formalize and evaluate 3 tasks that testa model's ability to describe time series using generic natural language: (1)recognition (True/False question-answering), (2) differentiation (multiplechoice question-answering), and (3) generation (open-ended natural languagedescription). We then unify 4 recent datasets to enable head-to-head modelcomparisons on each task. Experimentally, in evaluating 13 state-of-the-artlanguage, vision--language, and time series--language models, we find that (1)popular language-only methods largely underperform, indicating a need for timeseries-specific architectures, (2) VLMs are quite successful, as expected,identifying the value of vision models for these tasks and (3) pretrainedmultimodal time series--language models successfully outperform LLMs, but stillhave significant room for improvement. We also find that all approaches exhibitclear fragility in a range of robustness tests. Overall, our benchmark providesa standardized evaluation on a task necessary for time series reasoningsystems.</description>
      <author>example@mail.com (Medhasweta Sen, Zachary Gottesman, Jiaxing Qiu, C. Bayan Bruss, Nam Nguyen, Tom Hartvigsen)</author>
      <guid isPermaLink="false">2509.05215v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations</title>
      <link>http://arxiv.org/abs/2509.05186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种概率框架，揭示上下文算子网络(ICON)实际上是在执行贝叶斯推断，并扩展ICON到生成式设置，实现算子学习中的不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;ICON是一类基于基础模型新颖架构的算子学习方法，在多样化的初始和边界条件数据集上进行训练，这些数据集与常微分方程和偏微分方程的相应解配对。&lt;h4&gt;目的&lt;/h4&gt;揭示ICON作为隐式执行贝叶斯推断的本质，并扩展ICON到生成式设置，使其能够从解算子的后验预测分布中进行采样。&lt;h4&gt;方法&lt;/h4&gt;提出概率框架将ICON描述为计算给定上下文条件下解算子的后验预测分布均值的方法，利用随机微分方程的形式论为理解ICON和其他多算子学习方法提供基础。&lt;h4&gt;主要发现&lt;/h4&gt;ICON实际上执行贝叶斯推断，计算条件后验预测分布的均值；生成式ICON(GenICON)能够捕捉解算子的潜在不确定性，实现算子学习中的原则性不确定性量化。&lt;h4&gt;结论&lt;/h4&gt;通过概率视角，ICON可扩展到生成式设置，从解算子的后验预测分布中进行采样，GenICON捕捉了解算子的潜在不确定性，使算子学习中的解决方案预测能够进行原则性不确定性量化。&lt;h4&gt;翻译&lt;/h4&gt;上下文算子网络(ICON)是一类基于基础模型新颖架构的算子学习方法。在多样化的初始和边界条件数据集上进行训练，这些数据集与常微分方程和偏微分方程的相应解配对，ICON学习将给定微分方程的示例条件-解对映射到其解算子的近似值。在这里，我们提出了一个概率框架，揭示ICON作为隐式执行贝叶斯推断，其中它计算给定上下文(即示例条件-解对)条件下解算子的后验预测分布的均值。随机微分方程的形式论为描述ICON完成的任务提供了概率框架，同时也为理解其他多算子学习方法提供了基础。这种概率视角为将ICON扩展到生成式设置提供了基础，在这种设置中，可以从解算子的后验预测分布中进行采样。ICON的生成式公式(GenICON)捕捉了解算子的潜在不确定性，这使算子学习中的解决方案预测能够进行原则性不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In-context operator networks (ICON) are a class of operator learning methodsbased on the novel architectures of foundation models. Trained on a diverse setof datasets of initial and boundary conditions paired with correspondingsolutions to ordinary and partial differential equations (ODEs and PDEs), ICONlearns to map example condition-solution pairs of a given differential equationto an approximation of its solution operator. Here, we present a probabilisticframework that reveals ICON as implicitly performing Bayesian inference, whereit computes the mean of the posterior predictive distribution over solutionoperators conditioned on the provided context, i.e., example condition-solutionpairs. The formalism of random differential equations provides theprobabilistic framework for describing the tasks ICON accomplishes while alsoproviding a basis for understanding other multi-operator learning methods. Thisprobabilistic perspective provides a basis for extending ICON to\emph{generative} settings, where one can sample from the posterior predictivedistribution of solution operators. The generative formulation of ICON(GenICON) captures the underlying uncertainty in the solution operator, whichenables principled uncertainty quantification in the solution predictions inoperator learning.</description>
      <author>example@mail.com (Benjamin J. Zhang, Siting Liu, Stanley J. Osher, Markos A. Katsoulakis)</author>
      <guid isPermaLink="false">2509.05186v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Autoregressive Vision Foundation Models for Image Compression</title>
      <link>http://arxiv.org/abs/2509.05169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究首次尝试将视觉基础模型(VFMs)重新用作图像编解码器，探索其在低速率图像压缩中的生成能力。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(VFMs)在各种下游任务中广泛应用，包括条件生成和非条件生成场景，例如物理AI应用。许多VFMs采用类似于端到端学习图像编解码器的编码器-解码器架构，并学习自回归(AR)模型进行下一个令牌预测。&lt;h4&gt;目的&lt;/h4&gt;探索视觉基础模型(VFMs)在低速率图像压缩中的生成能力，并评估其与传统编解码器的性能比较。&lt;h4&gt;方法&lt;/h4&gt;重新利用VFM中的AR模型，基于已编码的令牌对下一个令牌进行熵编码，从而实现压缩。这种方法不同于早期仅依赖条件生成来重建输入图像的语义压缩工作。&lt;h4&gt;主要发现&lt;/h4&gt;某些预训练的通用VFMs在极低比特率下表现出比专业学习图像编解码器更好的感知质量。&lt;h4&gt;结论&lt;/h4&gt;这项发现为利用VFMs进行低速率、语义丰富的图像压缩这一有前景的研究方向铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;这项工作首次尝试将视觉基础模型(VFMs)重新用作图像编解码器，旨在探索它们在低速率图像压缩中的生成能力。VFMs在各种下游任务中被广泛用于条件生成和非条件生成场景，例如物理AI应用。许多VFMs采用类似于端到端学习图像编解码器的编码器-解码器架构，并学习自回归(AR)模型来执行下一个令牌预测。为了实现压缩，我们重新利用VFM中的AR模型，基于先前编码的令牌对下一个令牌进行熵编码。这种方法不同于早期依赖条件生成重建输入图像的语义压缩工作。进行了广泛的实验和分析，将基于VFM的编解码器与当前针对失真或感知质量优化的SOTA编解码器进行比较。值得注意的是，某些预训练的通用VFMs在极低比特率下表现出比专业学习图像编解码器更好的感知质量。这一发现为利用VFMs进行低速率、语义丰富的图像压缩这一有前景的研究方向铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents the first attempt to repurpose vision foundation models(VFMs) as image codecs, aiming to explore their generation capability forlow-rate image compression. VFMs are widely employed in both conditional andunconditional generation scenarios across diverse downstream tasks, e.g.,physical AI applications. Many VFMs employ an encoder-decoder architecturesimilar to that of end-to-end learned image codecs and learn an autoregressive(AR) model to perform next-token prediction. To enable compression, werepurpose the AR model in VFM for entropy coding the next token based onpreviously coded tokens. This approach deviates from early semantic compressionefforts that rely solely on conditional generation for reconstructing inputimages. Extensive experiments and analysis are conducted to compare VFM-basedcodec to current SOTA codecs optimized for distortion or perceptual quality.Notably, certain pre-trained, general-purpose VFMs demonstrate superiorperceptual quality at extremely low bitrates compared to specialized learnedimage codecs. This finding paves the way for a promising research directionthat leverages VFMs for low-rate, semantically rich image compression.</description>
      <author>example@mail.com (Huu-Tai Phung, Yu-Hsiang Lin, Yen-Kuan Ho, Wen-Hsiao Peng)</author>
      <guid isPermaLink="false">2509.05169v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights</title>
      <link>http://arxiv.org/abs/2509.05142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了联邦学习与基础模型的交叉领域，提供了一个结构化的文献综述和分类法，涵盖了42种独特方法，特别关注医疗保健领域。&lt;h4&gt;背景&lt;/h4&gt;联邦学习能够通过不共享私有数据的方式进行协作模型训练，从而解锁孤立的数据和分布式资源。随着更复杂的基础模型得到广泛应用，扩展训练资源和整合私有数据的需求也随之增长。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索联邦学习与基础模型的交叉领域，识别、分类和描述整合这两种范式的技术方法。由于目前缺乏统一的综述，作者提出了一个遵循开发生命周期阶段的新型分类法。&lt;h4&gt;方法&lt;/h4&gt;作者检索并审查了4,200多篇文章，通过纳入标准筛选出250多篇经过彻底审查的文章，其中包含42种独特方法。这些方法被用于构建分类法，并基于复杂性、效率和可扩展性进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;论文提供了实施和演变这些方法的实践见解和指导，特别关注医疗保健领域作为案例研究，并展示了联邦学习与基础模型结合的潜在影响。研究涵盖了多个交叉主题，包括联邦学习、自监督学习、微调、蒸馏和迁移学习等。&lt;h4&gt;结论&lt;/h4&gt;作者提供了一个自包含的概述，不仅总结了该领域的现状，还提供了关于采用、演变和整合基础模型与联邦学习的实践见解。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习有潜力在不共享私有数据的情况下，通过协作模型训练来解锁孤立的数据和分布式资源。随着更复杂的基础模型得到广泛使用，扩展训练资源和整合私有数据的需求也随之增长。在本文中，我们探讨了联邦学习与基础模型的交叉领域，旨在识别、分类和描述整合这两种范式的技术方法。由于目前缺乏统一的综述，我们提出了一个遵循开发生命周期阶段的文献综述和新型分类法，并对现有方法进行了技术比较。此外，我们提供了实施和演变这些方法的实践见解和指导，特别关注医疗保健领域作为案例研究，其中联邦学习与基础模型的潜在影响被认为非常重要。我们的综述涵盖了多个交叉主题，包括但不限于联邦学习、自监督学习、微调、蒸馏和迁移学习。最初，我们检索并审查了4,200多篇文章集合。通过纳入标准，这一集合被缩减到250多篇经过彻底审查的文章，其中包含42种独特方法。这些方法被用于构建分类法，并基于复杂性、效率和可扩展性进行了比较。我们呈现这些结果作为一个自包含的概述，不仅总结了该领域的现状，还提供了关于采用、演变和整合基础模型与联邦学习的实践见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.7717/peerj-cs.2993&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning has the potential to unlock siloed data and distributedresources by enabling collaborative model training without sharing privatedata. As more complex foundational models gain widespread use, the need toexpand training resources and integrate privately owned data grows as well. Inthis article, we explore the intersection of federated learning andfoundational models, aiming to identify, categorize, and characterize technicalmethods that integrate the two paradigms. As a unified survey is currentlyunavailable, we present a literature survey structured around a novel taxonomythat follows the development life-cycle stages, along with a technicalcomparison of available methods. Additionally, we provide practical insightsand guidelines for implementing and evolving these methods, with a specificfocus on the healthcare domain as a case study, where the potential impact offederated learning and foundational models is considered significant. Oursurvey covers multiple intersecting topics, including but not limited tofederated learning, self-supervised learning, fine-tuning, distillation, andtransfer learning. Initially, we retrieved and reviewed a set of over 4,200articles. This collection was narrowed to more than 250 thoroughly reviewedarticles through inclusion criteria, featuring 42 unique methods. The methodswere used to construct the taxonomy and enabled their comparison based oncomplexity, efficiency, and scalability. We present these results as aself-contained overview that not only summarizes the state of the field butalso provides insights into the practical aspects of adopting, evolving, andintegrating foundational models with federated learning.</description>
      <author>example@mail.com (Cosmin-Andrei Hatfaludi, Alex Serban)</author>
      <guid isPermaLink="false">2509.05142v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>LEMURS dataset: Large-scale multi-detector ElectroMagnetic Universal Representation of Showers</title>
      <link>http://arxiv.org/abs/2509.05108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 24 figures + appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍LEMURS数据集，这是一个模拟量热器簇射的广泛数据集，用于支持高能物理中快速模拟方法的发展和基准测试，为基础模型的发展提供支持。&lt;h4&gt;背景&lt;/h4&gt;高能物理领域需要快速模拟方法，而基础模型的发展需要大规模、多样化的训练数据。现有的CaloChallenge数据集2存在局限性。&lt;h4&gt;目的&lt;/h4&gt;创建一个比现有数据集更稳健、规模更大、多样性更高的数据集，以支持快速模拟方法的开发和基准测试，促进基础模型在高能物理中的应用。&lt;h4&gt;方法&lt;/h4&gt;构建模拟量热器簇射数据集，包含多种探测器几何结构，以HDF5格式提供，文件结构受CaloChallenge启发但包含更多变量。&lt;h4&gt;主要发现&lt;/h4&gt;LEMURS数据集比CaloChallenge数据集2更稳健，具有更大的统计量、更广的入射角度范围，以及多种探测器几何结构（包括更现实的量热器）。&lt;h4&gt;结论&lt;/h4&gt;LEMURS数据集的规模和多样性使其特别适合基础模型的发展，已在CaloDiT-2模型中得到应用，该模型已集成到Geant4模拟工具包中。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LEMURS：一个广泛的模拟量热器簇射数据集，旨在支持高能物理中快速模拟方法的发展和基准测试，主要为基础模型的发展提供一步。这个新数据集比已建立的CaloChallenge数据集2更稳健，具有更大的统计量、更广的探测器入射角度范围，并且最关键的是包含多种探测器几何结构（包括更现实的量热器）。数据集以HDF5格式提供，文件结构受CaloChallenge簇射表示的启发，同时包含更多变量。LEMURS的规模和多样性使其特别适合基础模型的发展，并已在CaloDiT-2模型中使用，这是一个在社区标准模拟工具包Geant4（版本11.4.beta）中发布的预训练模型。所有数据和生成及分析的代码都是公开可访问的，便于社区的可重复使用和重用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LEMURS: an extensive dataset of simulated calorimeter showersdesigned to support the development and benchmarking of fast simulation methodsin high-energy physics, most notably providing a step towards the developmentof foundation models. This new dataset is more robust than the well-establishedCaloChallenge dataset 2, featuring substantially greater statistics, a widerrange of incident angles in the detector, and most crucially multiple detectorgeometries (including more realistic calorimeters). The dataset is provided inHDF5 format, with a file structure inspired by the CaloChallenge showerrepresentation while also including more variables. LEMURS scale and diversitymake it particularly suitable for development of foundation models and has beenused in the CaloDiT-2 model, a pre-trained model released in the communitystandard simulation toolkit Geant4 (version 11.4.beta). All data and code forgeneration and analysis are openly accessible, facilitating reproducibility andreuse across the community.</description>
      <author>example@mail.com (Peter McKeown, Piyush Raikwar, Anna Zaborowska)</author>
      <guid isPermaLink="false">2509.05108v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions</title>
      <link>http://arxiv.org/abs/2509.05066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 (Main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ToM-SSI，一个新的多模态心智理论基准测试，用于测试在丰富社交互动和空间动态环境中的心智理论能力，突破了现有文本限制或二元互动的局限。&lt;h4&gt;背景&lt;/h4&gt;现有的大模型心智理论基准测试主要基于Sally-Anne测试的变体，视角非常有限，忽略了人类社交互动的复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出新的基准测试ToM-SSI，专门设计用于测试在丰富社交互动和空间动态环境中的心智理论能力。&lt;h4&gt;方法&lt;/h4&gt;ToM-SSI是多模态的，包括多达四个代理的群体互动，这些代理在情境环境中进行交流和移动，允许研究混合合作-阻碍设置和并行推理多个代理的心理状态。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型的表现仍然严重受限，特别是在这些新任务中表现不佳，突显了未来研究的关键差距。&lt;h4&gt;结论&lt;/h4&gt;新的基准测试能够捕捉比现有基准更广泛的社会认知范围，为未来研究提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;大多数现有针对基础模型的心智理论(ToM)基准测试依赖于Sally-Anne测试的变体，仅提供了非常有限的心智理论视角，忽略了人类社交互动的复杂性。为解决这一差距，我们提出了ToM-SSI：一个新的基准测试，专门设计用于测试在丰富社交互动和空间动态环境中的心智理论能力。虽然当前的ToM基准测试仅限于文本或二元互动，但ToM-SSI是多模态的，包括多达四个代理的群体互动，这些代理在情境环境中进行交流和移动。这种独特的设计使我们首次能够研究混合合作-阻碍设置和并行推理多个代理的心理状态，从而捕捉比现有基准更广泛的社会认知范围。我们的评估显示，当前模型的表现仍然严重受限，特别是在这些新任务中，突显了未来研究的关键差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing Theory of Mind (ToM) benchmarks for foundation models rely onvariations of the Sally-Anne test, offering only a very limited perspective onToM and neglecting the complexity of human social interactions. To address thisgap, we propose ToM-SSI: a new benchmark specifically designed to test ToMcapabilities in environments rich with social interactions and spatialdynamics. While current ToM benchmarks are limited to text-only or dyadicinteractions, ToM-SSI is multimodal and includes group interactions of up tofour agents that communicate and move in situated environments. This uniquedesign allows us to study, for the first time, mixed cooperative-obstructivesettings and reasoning about multiple agents' mental state in parallel, thuscapturing a wider range of social cognition than existing benchmarks. Ourevaluations reveal that the current models' performance is still severelylimited, especially in these new tasks, highlighting critical gaps for futureresearch.</description>
      <author>example@mail.com (Matteo Bortoletto, Constantin Ruhdorfer, Andreas Bulling)</author>
      <guid isPermaLink="false">2509.05066v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>LUIVITON: Learned Universal Interoperable VIrtual Try-ON</title>
      <link>http://arxiv.org/abs/2509.05030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LUIVITON系统，一个端到端的完全自动化虚拟试衣系统，能够将复杂的多层服装覆盖在不同姿态的人形角色上，并支持服装尺寸的快速定制。&lt;h4&gt;背景&lt;/h4&gt;虚拟试衣面临的主要挑战是将复杂服装与各种不同姿态和形状的人体正确对齐，传统方法难以处理复杂的几何形状、非流形网格以及多样化的人形角色。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够完全自动化的虚拟试衣系统，能够处理复杂的多层服装，适应各种人形角色（包括人类、机器人、卡通角色、生物和外星人），并支持服装尺寸的快速定制，同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;使用SMPL作为代理表示，将服装到人体的覆盖问题分解为服装到SMPL对应和身体到SMPL对应两个任务；对于服装到SMPL对应，使用基于几何学习的方法进行部分到完整形状对应预测；对于身体到SMPL对应，引入基于扩散模型的方法，使用多视角一致的外观特征和预训练的2D基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;系统能够产生高质量的3D服装搭配，无需任何人工劳动；即使在没有2D服装缝制模式的情况下也能工作；系统计算效率高，适合实际应用；支持服装尺寸和材质属性的快速定制。&lt;h4&gt;结论&lt;/h4&gt;LUIVITON系统提供了一个完全自动化的虚拟试衣解决方案，能够处理复杂服装和各种人形角色，同时保持计算效率，并支持服装尺寸的快速定制，为虚拟试衣领域提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LUIVITON，一个端到端的系统，用于完全自动化的虚拟试衣，能够将复杂的多层服装覆盖在不同姿态和多样化的人形角色上。为了解决将复杂服装与任意且高度多样化的人体形状对齐的挑战，我们使用SMPL作为代理表示，并将服装到人体的覆盖问题分解为两个对应任务：1)服装到SMPL和2)身体到SMPL对应，每个任务都有其独特的挑战。虽然我们使用基于几何学习的方法来解决服装到SMPL的贴合问题，以进行部分到完整形状对应的预测，但我们引入了基于扩散模型的方法来处理身体到SMPL的对应，使用多视角一致的外观特征和预训练的2D基础模型。我们的方法能够处理复杂的几何形状、非流形网格，并有效推广到广泛的人形角色--包括人类、机器人、卡通主题、生物和外星人，同时保持计算效率以便实际应用。除了提供完全自动化的贴合解决方案外，LUIVITON还支持服装尺寸的快速定制，允许用户在服装被覆盖后调整服装尺寸和材质属性。我们证明，我们的系统可以在没有任何人工劳动的情况下产生高质量的3D服装搭配，即使2D服装缝制模式不可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决虚拟试衣（virtual try-on）的自动化问题，即如何将任意3D服装模型自动、准确地穿在各种姿势和形状的人形3D角色身上。这个问题在现实和研究中的重要性在于：游戏、电影、社交媒体和AR/VR应用中，角色服装对身份认同和视觉连贯性至关重要；而现有的服装建模和模拟方法劳动密集型，难以自动化适配到不同形状、姿势或风格的角色；当前方法在处理复杂服装、多层结构或风格化角色时存在局限，且缺乏对服装尺寸的灵活控制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将复杂的服装到身体适配问题分解为两个更易管理的对应任务：服装到SMPL的对应和身体到SMPL的对应。针对服装到SMPL对应，作者使用几何学习方法处理部分到完整形状的对应；针对身体到SMPL对应，采用基于扩散模型的方法处理大形状和姿势变化。作者借鉴了多个现有技术：使用SMPL作为人体表示代理；采用DiffusionNet进行服装对应；利用SyncMVD和DINOv2进行特征提取；使用ContourCraft作为神经布料模拟器。整个系统设计为端到端的流程，分为对应预测、注册和服装适配三个主要阶段。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用SMPL作为中间表示，将复杂的服装到身体适配问题分解为两个对应问题，并针对不同问题采用不同方法：几何学习处理服装对应，扩散模型处理身体对应。整体流程分为三阶段：1）对应预测阶段，分别计算服装到SMPL和身体到SMPL的对应关系；2）注册阶段，优化SMPL和SMPL+D参数以对齐服装和身体；3）服装适配阶段，生成平滑过渡序列并使用神经布料模拟器将服装适配到目标身体，支持默认、自动调整和定制三种尺寸调整模式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）全自动通用虚拟试-on框架，能处理复杂几何和多层服装；2）基于DiffusionNet的服装到SMPL对应预测模型，能处理非流形网格；3）结合多视角扩散特征和DINOv2先验的身体注册技术，能泛化到各种身体形状；4）快速定制功能，每次调整仅需15秒。相比之前工作，不同之处在于：不再局限于参数化身体和流形服装；使用专门设计的对应预测方法提高精度；能处理人类、机器人、卡通等多种角色；支持快速尺寸调整且无需2D缝纫模式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LUIVITON是一个全自动的通用虚拟试-on系统，能够将任意3D服装精确地适配到各种姿势和形状的人形角色上，支持快速定制且无需人工干预。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LUIVITON, an end-to-end system for fully automated virtual try-on,capable of draping complex, multi-layer clothing onto diverse and arbitrarilyposed humanoid characters. To address the challenge of aligning complexgarments with arbitrary and highly diverse body shapes, we use SMPL as a proxyrepresentation and separate the clothing-to-body draping problem into twocorrespondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,where each has its unique challenges. While we address the clothing-to-SMPLfitting problem using a geometric learning-based approach forpartial-to-complete shape correspondence prediction, we introduce a diffusionmodel-based approach for body-to-SMPL correspondence using multi-viewconsistent appearance features and a pre-trained 2D foundation model. Ourmethod can handle complex geometries, non-manifold meshes, and generalizeseffectively to a wide range of humanoid characters -- including humans, robots,cartoon subjects, creatures, and aliens, while maintaining computationalefficiency for practical adoption. In addition to offering a fully automaticfitting solution, LUIVITON supports fast customization of clothing size,allowing users to adjust clothing sizes and material properties after they havebeen draped. We show that our system can produce high-quality 3D clothingfittings without any human labor, even when 2D clothing sewing patterns are notavailable.</description>
      <author>example@mail.com (Cong Cao, Xianhang Cheng, Jingyuan Liu, Yujian Zheng, Zhenhui Lin, Meriem Chkir, Hao Li)</author>
      <guid isPermaLink="false">2509.05030v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper</title>
      <link>http://arxiv.org/abs/2509.04957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了多基础模型映射器(MFM-Mapper)，用于高效的视频到音频(V2A)生成，通过融合双视觉编码器特征和使用GPT-2改进特征对齐，仅需16%的训练量即可达到与大规模模型相当的性能。&lt;h4&gt;背景&lt;/h4&gt;近期的视频到音频生成依赖于从视频中提取语义和时间特征来调节生成模型，但从头训练这些模型资源消耗大。基础模型因其跨模态知识迁移和泛化能力而受到关注。&lt;h4&gt;目的&lt;/h4&gt;改进现有的视频到音频生成方法，提出一种更高效的映射器架构，能够在降低训练成本的同时保持或提高生成质量。&lt;h4&gt;方法&lt;/h4&gt;MFM-Mapper通过融合双视觉编码器的特征来获取更丰富的语义和时间信息，并用GPT-2替换线性映射器以改进特征对齐，将跨模态特征映射与自回归翻译任务联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;MFM-Mapper在语义和时间一致性方面表现更好，仅需先前映射器方法16%的训练量，就能与在更大规模上训练的模型实现相当的性能。&lt;h4&gt;结论&lt;/h4&gt;MFM-Mapper是一种高效的视频到音频生成方法，显著降低了训练成本，同时保持了高质量的音频生成能力。&lt;h4&gt;翻译&lt;/h4&gt;近期的视频到音频(V2A)生成依赖于从视频中提取语义和时间特征来调节生成模型。从头训练这些模型资源消耗大。因此，由于基础模型(FMs)具有跨模态知识迁移和泛化能力，利用它们已成为趋势。先前的一项工作探索了微调一个轻量级映射器网络，将预训练的视觉编码器与文本到音频生成模型连接起来用于V2A。受此启发，我们引入了多基础模型映射器(MFM-Mapper)。与之前的映射器方法相比，MFM-Mapper通过融合双视觉编码器的特征，受益于更丰富的语义和时间信息。此外，通过用GPT-2替换线性映射器，MFM-Mapper改进了特征对齐，将跨模态特征映射与自回归翻译任务相提并论。我们的MFM-Mapper表现出显著的训练效率。它在语义和时间一致性方面实现了更好的性能，训练消耗更少，仅需先前基于映射器工作的16%的训练规模，却实现了与在更大规模上训练的模型相竞争的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent Video-to-Audio (V2A) generation relies on extracting semantic andtemporal features from video to condition generative models. Training thesemodels from scratch is resource intensive. Consequently, leveraging foundationmodels (FMs) has gained traction due to their cross-modal knowledge transferand generalization capabilities. One prior work has explored fine-tuning alightweight mapper network to connect a pre-trained visual encoder with atext-to-audio generation model for V2A. Inspired by this, we introduce theMultiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapperapproach, MFM-Mapper benefits from richer semantic and temporal information byfusing features from dual visual encoders. Furthermore, by replacing a linearmapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallelsbetween cross-modal features mapping and autoregressive translation tasks. OurMFM-Mapper exhibits remarkable training efficiency. It achieves betterperformance in semantic and temporal consistency with fewer training consuming,requiring only 16\% of the training scale compared to previous mapper-basedwork, yet achieves competitive performance with models trained on a much largerscale.</description>
      <author>example@mail.com (Gehui Chen, Guan'an Wang, Xiaowen Huang, Jitao Sang)</author>
      <guid isPermaLink="false">2509.04957v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series</title>
      <link>http://arxiv.org/abs/2509.04921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Patent pending&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过生成人工混沌时间序列和应用重采样技术来模拟金融时间序列数据的方法，并进行大规模预训练。使用比特币交易数据进行零样本预测，结果表明该方法在交易策略盈利能力上显著优于自相关模型。研究还发现了类似缩放定律的现象，表明通过增加训练样本数量可以扩展混沌时间序列的预测范围。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在气象学、交通、电力、经济、金融等多个领域的决策过程中起着关键作用。预测金融工具回报是一个具有挑战性的问题。研究人员已提出适用于各种预测任务的时间序列基础模型，并基于现实世界时间序列的混沌特性开发了人工生成合成混沌时间序列的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种通过生成人工混沌时间序列并对金融时间序列数据进行建模的方法论。&lt;h4&gt;方法&lt;/h4&gt;应用重采样技术模拟金融时间序列数据作为训练样本；增加重采样间隔以扩展预测范围；使用100亿个训练样本进行大规模预训练；基于实际比特币交易数据创建多时间段测试数据集；进行零样本预测；评估基于预测的简单交易策略的盈利能力。&lt;h4&gt;主要发现&lt;/h4&gt;与自相关模型相比，预测结果表现出显著的性能改进；在预训练过程中观察到类似缩放定律的现象；通过指数增加训练样本数量，可以在混沌时间序列中扩展预测范围的同时达到一定水平的预测性能。&lt;h4&gt;结论&lt;/h4&gt;如果这种缩放定律在各种混沌模型中都成立，意味着通过投入大量计算资源有可能预测近期事件。未来研究应侧重于进一步的大规模训练，并验证此缩放定律对不同混沌模型的适用性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测在气象学、交通、电力、经济、金融等多个领域的决策过程中起着关键作用。特别是，预测金融工具回报是一个具有挑战性的问题。一些研究人员提出了适用于各种预测任务的时间序列基础模型。同时，基于现实世界时间序列表现出混沌特性的认识，已经开发了人工生成合成混沌时间序列、构建多样化数据集和训练模型的方法。在本研究中，我们提出了一种通过生成人工混沌时间序列并应用重采样技术来模拟金融时间序列数据的方法，然后将其用作训练样本。通过增加重采样间隔来扩展预测范围，我们对每个案例使用了100亿个训练样本进行大规模预训练。随后，我们使用实际的比特币交易数据为多个时间段创建了测试数据集，并在不重新训练预训练模型的情况下进行了零样本预测。基于这些预测评估的简单交易策略的盈利能力结果表明，与自相关模型相比表现出显著的性能改进。在大规模预训练过程中，我们观察到了类似缩放定律的现象，即通过指数增加训练样本数量，我们可以在混沌时间序列中扩展预测范围的同时达到一定水平的预测性能。如果这种缩放定律被证明是稳健的，并且在各种混沌模型中都成立，它表明通过投入大量计算资源有可能预测近期事件。未来研究应侧重于进一步的大规模训练，并验证此缩放定律对不同混沌模型的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting plays a critical role in decision-making processesacross diverse fields including meteorology, traffic, electricity, economics,finance, and so on. Especially, predicting returns on financial instruments isa challenging problem. Some researchers have proposed time series foundationmodels applicable to various forecasting tasks. Simultaneously, based on therecognition that real-world time series exhibit chaotic properties, methodshave been developed to artificially generate synthetic chaotic time series,construct diverse datasets and train models. In this study, we propose amethodology for modeling financial time series by generating artificial chaotictime series and applying resampling techniques to simulate financial timeseries data, which we then use as training samples. Increasing the resamplinginterval to extend predictive horizons, we conducted large-scale pre-trainingusing 10 billion training samples for each case. We subsequently created testdatasets for multiple timeframes using actual Bitcoin trade data and performedzero-shot prediction without re-training the pre-trained model. The results ofevaluating the profitability of a simple trading strategy based on thesepredictions demonstrated significant performance improvements overautocorrelation models. During the large-scale pre-training process, weobserved a scaling law-like phenomenon that we can achieve predictiveperformance at a certain level with extended predictive horizons for chaotictime series by increasing the number of training samples exponentially. If thisscaling law proves robust and holds true across various chaotic models, itsuggests the potential to predict near-future events by investing substantialcomputational resources. Future research should focus on further large-scaletraining and verifying the applicability of this scaling law to diverse chaoticmodels.</description>
      <author>example@mail.com (Yuki Takemoto)</author>
      <guid isPermaLink="false">2509.04921v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Foundation Model-Driven User Interest Modeling and Behavior Analysis on Short Video Platforms</title>
      <link>http://arxiv.org/abs/2509.04751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多模态基础模型的用户兴趣建模和行为分析框架，通过整合视频、文本和音频数据，结合行为序列分析，显著提高了推荐系统的准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;随着短视频平台用户基数的快速增长，个性化推荐系统对提升用户体验和优化内容分发至关重要。然而，传统兴趣建模方法通常依赖单模态数据（如点击日志或文本标签），无法完全捕捉复杂多模态内容环境中的用户偏好。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法在复杂多模态内容环境中无法完全捕捉用户偏好的问题，提出基于多模态基础模型框架进行用户兴趣建模和行为分析，以提高推荐系统的及时性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出多模态基础模型框架，通过跨模态对齐策略将视频帧、文本描述和背景音乐集成到统一语义空间构建细粒度用户兴趣向量；引入行为驱动的特征嵌入机制，整合观看、点赞和评论序列来建模动态兴趣演化；在实验阶段使用公共和专有短视频数据集进行广泛评估；并使用注意力权重和特征可视化纳入可解释性机制。&lt;h4&gt;主要发现&lt;/h4&gt;行为预测准确性显著提高；对冷启动用户的兴趣建模效果改善；推荐点击率提高；模型决策基础可追溯，兴趣变化可追踪。&lt;h4&gt;结论&lt;/h4&gt;多模态基础模型框架能有效提升推荐系统的及时性和准确性，增强了推荐系统的透明度和可控性。&lt;h4&gt;翻译&lt;/h4&gt;随着短视频平台用户基数的快速增长，个性化推荐系统在提升用户体验和优化内容分发方面发挥着越来越关键的作用。传统的兴趣建模方法通常依赖单模态数据，如点击日志或文本标签，这限制了它们在复杂多模态内容环境中完全捕捉用户偏好的能力。为应对这一挑战，本文提出了一种基于多模态基础模型的用户兴趣建模和行为分析框架。通过跨模态对齐策略将视频帧、文本描述和背景音乐集成到统一语义空间，该框架构建了细粒度的用户兴趣向量。此外，我们引入了一种行为驱动的特征嵌入机制，结合观看、点赞和评论序列来建模动态兴趣演化，从而提高推荐的及时性和准确性。在实验阶段，我们使用公共和专有短视频数据集进行了广泛评估，将我们的方法与多种主流推荐算法和建模技术进行了比较。结果表明，在行为预测准确性、冷启动用户的兴趣建模和推荐点击率方面均有显著改善。此外，我们使用注意力权重和特征可视化纳入了可解释性机制，以揭示模型在多模态输入下的决策基础，并追踪兴趣变化，从而增强推荐系统的透明度和可控性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid expansion of user bases on short video platforms, personalizedrecommendation systems are playing an increasingly critical role in enhancinguser experience and optimizing content distribution. Traditional interestmodeling methods often rely on unimodal data, such as click logs or textlabels, which limits their ability to fully capture user preferences in acomplex multimodal content environment. To address this challenge, this paperproposes a multimodal foundation model-based framework for user interestmodeling and behavior analysis. By integrating video frames, textualdescriptions, and background music into a unified semantic space usingcross-modal alignment strategies, the framework constructs fine-grained userinterest vectors. Additionally, we introduce a behavior-driven featureembedding mechanism that incorporates viewing, liking, and commenting sequencesto model dynamic interest evolution, thereby improving both the timeliness andaccuracy of recommendations. In the experimental phase, we conduct extensiveevaluations using both public and proprietary short video datasets, comparingour approach against multiple mainstream recommendation algorithms and modelingtechniques. Results demonstrate significant improvements in behavior predictionaccuracy, interest modeling for cold-start users, and recommendationclick-through rates. Moreover, we incorporate interpretability mechanisms usingattention weights and feature visualization to reveal the model's decisionbasis under multimodal inputs and trace interest shifts, thereby enhancing thetransparency and controllability of the recommendation system.</description>
      <author>example@mail.com (Yushang Zhao, Yike Peng, Li Zhang, Qianyi Sun, Zhihui Zhang, Yingying Zhuang)</author>
      <guid isPermaLink="false">2509.04751v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization</title>
      <link>http://arxiv.org/abs/2509.04735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过引入不确定性量化方法，提高视觉基础模型在恶劣天气条件下自动驾驶场景的图像分割性能。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(如SAM和SAM2)在通用图像分割中表现优异，但在恶劣天气等视觉模糊度高的情况下表现不佳，主要原因是缺乏不确定性量化。医学影像领域通过不确定性感知训练提高了可靠性。&lt;h4&gt;目的&lt;/h4&gt;增强自动驾驶场景下图像分割模型的鲁棒性，解决模型在恶劣天气条件下的性能问题。&lt;h4&gt;方法&lt;/h4&gt;1) 引入多步微调程序，将不确定性指标直接纳入SAM2的损失函数；2) 将医学图像分割中的不确定性感知适配器(UAT)适应到驾驶场景中。&lt;h4&gt;主要发现&lt;/h4&gt;在CamVid、BDD100K和GTA数据集上评估显示，UAT-SAM在极端天气条件下优于标准SAM，而带有不确定性感知损失的SAM2在多样化驾驶场景中表现更好。&lt;h4&gt;结论&lt;/h4&gt;明确的不确定性建模对安全关键型自动驾驶在具有挑战性环境中具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;近期视觉基础模型的进展，如Segment Anything Model (SAM)及其后续版本SAM2，在通用图像分割基准测试中取得了最先进的性能。然而，在视觉模糊度高的恶劣天气条件下，这些模型表现不佳，主要是因为它们缺乏不确定性量化。受医学影像领域进展的启发，不确定性感知训练提高了模糊情况下的可靠性，我们研究了两种增强自动驾驶分割鲁棒性的方法。首先，我们引入了SAM2的多步微调程序，将不确定性指标直接纳入损失函数，提高整体场景识别能力。其次，我们将专为医学图像分割设计的'不确定性感知适配器'(UAT)适应到驾驶上下文中。我们在CamVid、BDD100K和GTA驾驶数据集上评估了这两种方法。实验表明，UAT-SAM在极端天气条件下优于标准SAM，而具有不确定性感知损失的SAM2在多样化的驾驶场景中实现了更好的性能。这些发现强调了明确的不确定性建模对安全关键型自动驾驶在具有挑战性环境中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision foundation models, such as the Segment AnythingModel (SAM) and its successor SAM2, have achieved state-of-the-art performanceon general image segmentation benchmarks. However, these models struggle inadverse weather conditions where visual ambiguity is high, largely due to theirlack of uncertainty quantification. Inspired by progress in medical imaging,where uncertainty-aware training has improved reliability in ambiguous cases,we investigate two approaches to enhance segmentation robustness for autonomousdriving. First, we introduce a multi-step finetuning procedure for SAM2 thatincorporates uncertainty metrics directly into the loss function, improvingoverall scene recognition. Second, we adapt the Uncertainty-Aware Adapter(UAT), originally designed for medical image segmentation, to driving contexts.We evaluate both methods on CamVid, BDD100K, and GTA driving datasets.Experiments show that UAT-SAM outperforms standard SAM in extreme weather,while SAM2 with uncertainty-aware loss achieves improved performance acrossdiverse driving scenes. These findings underscore the value of explicituncertainty modeling for safety-critical autonomous driving in challengingenvironments.</description>
      <author>example@mail.com (Dharsan Ravindran, Kevin Wang, Zhuoyuan Cao, Saleh Abdelrahman, Jeffery Wu)</author>
      <guid isPermaLink="false">2509.04735v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Sample-efficient Integration of New Modalities into Large Language Models</title>
      <link>http://arxiv.org/abs/2509.04606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SEMI的样本高效的模态集成方法，可以有效地将新模态集成到大型语言模型中，显著减少了所需的数据量。&lt;h4&gt;背景&lt;/h4&gt;多模态基础模型可处理多种模态，但由于模态空间大且不断演变，从头训练包含所有模态的模型不可行。此外，将模态集成到现有基础模型需要大量成对数据，而低资源模态通常缺乏这些数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种样本高效的模态集成方法，使大型语言模型能够适应新的模态，而无需大量成对训练数据。&lt;h4&gt;方法&lt;/h4&gt;设计一个超网络，该网络可以将共享投影仪(位于模态特定编码器和LLM之间)适应任何模态。超网络在高资源模态上训练，并在推理时基于少量样本生成适配器。通过等距变换增加编码器数量以提高训练模态多样性。&lt;h4&gt;主要发现&lt;/h4&gt;SEMI在集成新模态(卫星图像、天文图像、惯性测量和分子)时显著提高了样本效率，适用于任意嵌入维度的编码器。例如，要达到与32次样本SEMI相同的准确度，从头训练投影仪需要多64倍的数据。&lt;h4&gt;结论&lt;/h4&gt;SEMI方法有望扩展基础模型的模态覆盖范围，使模型能够更高效地适应新的模态。&lt;h4&gt;翻译&lt;/h4&gt;多模态基础模型可以处理多种模态。然而，由于可能的模态空间很大且随时间演变，从头开始训练一个包含所有模态的模型是不可行的。此外，将模态集成到现有基础模型中目前需要大量成对数据，而这些数据对于低资源模态通常不可用。在本文中，我们介绍了一种将模态高效集成到大型语言模型中的方法。为此，我们设计了一个超网络，可以将位于模态特定编码器和大型语言模型之间的共享投影仪适应任何模态。该超网络在高资源模态上训练，并在推理时基于任意模态的少量样本生成合适的适配器。为了增加训练模态的多样性，我们通过等距变换人为增加了编码器的数量。我们发现，SEMI在集成新模态时，在少量样本情况下显著提高了样本效率，适用于任意嵌入维度的编码器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal foundation models can process several modalities. However, sincethe space of possible modalities is large and evolving over time, training amodel from scratch to encompass all modalities is unfeasible. Moreover,integrating a modality into a pre-existing foundation model currently requiresa significant amount of paired data, which is often not available forlow-resource modalities. In this paper, we introduce a method forsample-efficient modality integration (SEMI) into Large Language Models (LLMs).To this end, we devise a hypernetwork that can adapt a shared projector --placed between modality-specific encoders and an LLM -- to any modality. Thehypernetwork, trained on high-resource modalities (i.e., text, speech, audio,video), is conditioned on a few samples from any arbitrary modality atinference time to generate a suitable adapter. To increase the diversity oftraining modalities, we artificially multiply the number of encoders throughisometric transformations. We find that SEMI achieves a significant boost insample efficiency during few-shot integration of new modalities (i.e.,satellite images, astronomical images, inertial measurements, and molecules)with encoders of arbitrary embedding dimensionality. For instance, to reach thesame accuracy as 32-shot SEMI, training the projector from scratch needs64$\times$ more data. As a result, SEMI holds promise to extend the modalitycoverage of foundation models.</description>
      <author>example@mail.com (Osman Batur İnce, André F. T. Martins, Oisin Mac Aodha, Edoardo M. Ponti)</author>
      <guid isPermaLink="false">2509.04606v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>CEHR-XGPT: A Scalable Multi-Task Foundation Model for Electronic Health Records</title>
      <link>http://arxiv.org/abs/2509.03643v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CEHR-XGPT是一种通用基础模型，专为电子健康记录数据设计，整合了特征表示、零样本预测和合成数据生成三种能力，通过时间令牌学习框架支持临床序列的时间推理，在各项任务中表现优异，能有效泛化到外部数据集。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录提供患者健康的丰富纵向视图，具有推进临床决策支持、风险预测和数据驱动医疗研究的潜力，但大多数现有人工智能模型为单一目的设计，限制了其在现实世界中的泛化能力和实用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用基础模型，将EHR数据分析的三种基本能力（特征表示、零样本预测和合成数据生成）统一在单一架构中。&lt;h4&gt;方法&lt;/h4&gt;提出CEHR-XGPT模型，采用新颖的时间令牌学习框架，将患者的动态时间线明确编码到模型结构中，以支持对临床序列的时间推理。&lt;h4&gt;主要发现&lt;/h4&gt;CEHR-XGPT在所有三项任务中均表现出强大性能，通过词汇扩展和微调能有效泛化到外部数据集。&lt;h4&gt;结论&lt;/h4&gt;CEHR-XGPT的多功能性使研究人员无需任务特定的重新训练即可实现快速模型开发、队列发现和患者结果预测。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录（EHRs）提供了患者健康的丰富纵向视图，在推进临床决策支持、风险预测和数据驱动的医疗研究方面具有巨大潜力。然而，大多数用于EHR的人工智能模型都是为狭窄的单一目的任务设计的，限制了它们在现实世界环境中的泛化能力和实用性。在这里，我们提出了CEHR-XGPT，这是一种用于EHR数据的通用基础模型，在单一架构中统一了三种基本能力——特征表示、零样本预测和合成数据生成。为了支持对临床序列的时间推理，CEHR-XGPT采用了一种新颖的时间令牌学习框架，明确将患者的动态时间线编码到模型结构中。CEHR-XGPT在所有三项任务中都表现出强大的性能，并通过词汇扩展和微调有效地泛化到外部数据集。它的多功能性使得无需任务特定的重新训练即可实现快速模型开发、队列发现和患者结果预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHRs) provide a rich, longitudinal view of patienthealth and hold significant potential for advancing clinical decision support,risk prediction, and data-driven healthcare research. However, most artificialintelligence (AI) models for EHRs are designed for narrow, single-purposetasks, limiting their generalizability and utility in real-world settings.Here, we present CEHR-XGPT, a general-purpose foundation model for EHR datathat unifies three essential capabilities - feature representation, zero-shotprediction, and synthetic data generation - within a single architecture. Tosupport temporal reasoning over clinical sequences, CEHR-XGPT incorporates anovel time-token-based learning framework that explicitly encodes patients'dynamic timelines into the model structure. CEHR-XGPT demonstrates strongperformance across all three tasks and generalizes effectively to externaldatasets through vocabulary expansion and fine-tuning. Its versatility enablesrapid model development, cohort discovery, and patient outcome forecastingwithout the need for task-specific retraining.</description>
      <author>example@mail.com (Chao Pang, Jiheum Park, Xinzhuo Jiang, Nishanth Parameshwar Pavinkurve, Krishna S. Kalluri, Shalmali Joshi, Noémie Elhadad, Karthik Natarajan)</author>
      <guid isPermaLink="false">2509.03643v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning to accelerate distributed ADMM using graph neural networks</title>
      <link>http://arxiv.org/abs/2509.05288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review, the first two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种将分布式ADMM算法与图神经网络相结合的方法，通过学习自适应超参数来提高ADMM的收敛速度和解决方案质量，同时保持算法的收敛特性。&lt;h4&gt;背景&lt;/h4&gt;分布式优化是大规模机器学习和控制应用的基础。ADMM方法因其强收敛保证和适合分布式计算而受到欢迎，但常面临收敛速度慢和对超参数选择敏感的问题。&lt;h4&gt;目的&lt;/h4&gt;解决ADMM收敛速度慢和对超参数选择敏感的问题，提高分布式优化算法的性能。&lt;h4&gt;方法&lt;/h4&gt;将分布式ADMM迭代表示在图神经网络的消息传递框架中，通过图神经网络学习自适应步长和通信权重，基于迭代预测超参数。通过展开固定次数的ADMM迭代，端到端训练网络参数，以最小化特定问题类的最终迭代误差。&lt;h4&gt;主要发现&lt;/h4&gt;学习到的ADMM变体在收敛速度和解决方案质量方面一致优于标准ADMM，数值实验验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;将图神经网络与ADMM结合可以有效改善ADMM的性能，在保持算法收敛特性的同时，显著提高收敛速度和解决方案质量。&lt;h4&gt;翻译&lt;/h4&gt;分布式优化是大规模机器学习和控制应用的基础。在现有方法中，交替方向乘子法（ADMM）因其强收敛保证和适合分布式计算而广受欢迎。然而，ADMM常面临收敛速度慢和对超参数选择敏感的问题。在这项工作中，我们展示了分布式ADMM迭代可以在图神经网络（GNNs）的消息传递框架中自然表示。基于这一联系，我们提出通过图神经网络学习自适应步长和通信权重，该网络基于迭代预测超参数。通过展开固定次数的ADMM迭代，我们端到端训练网络参数，以最小化特定问题类的最终迭代误差，同时保持算法的收敛特性。数值实验表明，我们学习到的变体在收敛速度和解决方案质量方面一致优于标准ADMM。代码可在https://github.com/paulhausner/learning-distributed-admm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distributed optimization is fundamental in large-scale machine learning andcontrol applications. Among existing methods, the Alternating Direction Methodof Multipliers (ADMM) has gained popularity due to its strong convergenceguarantees and suitability for decentralized computation. However, ADMM oftensuffers from slow convergence and sensitivity to hyperparameter choices. Inthis work, we show that distributed ADMM iterations can be naturallyrepresented within the message-passing framework of graph neural networks(GNNs). Building on this connection, we propose to learn adaptive step sizesand communication weights by a graph neural network that predicts thehyperparameters based on the iterates. By unrolling ADMM for a fixed number ofiterations, we train the network parameters end-to-end to minimize the finaliterates error for a given problem class, while preserving the algorithm'sconvergence properties. Numerical experiments demonstrate that our learnedvariant consistently improves convergence speed and solution quality comparedto standard ADMM. The code is available athttps://github.com/paulhausner/learning-distributed-admm.</description>
      <author>example@mail.com (Henri Doerks, Paul Häusner, Daniel Hernández Escobar, Jens Sjölund)</author>
      <guid isPermaLink="false">2509.05288v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.05207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2505.10806&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RapidGNN是一种创新的分布式图神经网络训练框架，通过确定性采样调度策略实现高效缓存构建和远程特征预取，显著提升了训练性能和能源效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在探索实体间结构关系的各种任务中变得流行。然而，由于数据集的高度连接结构，在大规模图上进行GNN的分布式训练面临重大挑战。传统的基于采样的方法虽然可以减轻计算负载，但通信开销仍然是一个主要问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种分布式GNN训练框架，解决大规模图上GNN训练的通信开销问题，提高训练效率和能源效率。&lt;h4&gt;方法&lt;/h4&gt;提出RapidGNN，一种具有确定性采样调度策略的分布式GNN训练框架，能够实现高效的缓存构建和远程特征预取。&lt;h4&gt;主要发现&lt;/h4&gt;在基准图数据集上，RapidGNN将端到端训练吞吐量平均提高了2.46倍到3.00倍，同时将远程特征获取减少了9.70倍到15.39倍。RapidGNN展示了接近线性的可扩展性，随着计算单元数量的增加而高效扩展。对于CPU和GPU，RapidGNN分别比基线方法提高了44%和32%的能源效率。&lt;h4&gt;结论&lt;/h4&gt;RapidGNN是一种有效的分布式GNN训练解决方案，能够显著提高训练吞吐量，减少远程特征获取，并提高能源效率，适用于不同规模和拓扑结构的图数据集。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为探索实体间结构关系的各种任务中的热门选择。然而，由于数据集的高度连接结构，在大规模图上进行GNN的分布式训练带来了重大挑战。传统的基于采样的方法减轻了计算负载，但通信开销仍然是一个挑战。本文提出了RapidGNN，一种具有确定性采样调度的分布式GNN训练框架，能够实现高效的缓存构建和远程特征预取。在基准图数据集上的评估表明，RapidGNN在不同规模和拓扑结构上都是有效的。RapidGNN在基准数据集上将端到端训练吞吐量平均提高了2.46倍到3.00倍，同时将远程特征获取减少了9.70倍到15.39倍。RapidGNN进一步展示了接近线性的可扩展性，随着计算单元数量的增加而高效扩展。此外，对于CPU和GPU，RapidGNN分别比基线方法提高了44%和32%的能源效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become popular across a diverse set oftasks in exploring structural relationships between entities. However, due tothe highly connected structure of the datasets, distributed training of GNNs onlarge-scale graphs poses significant challenges. Traditional sampling-basedapproaches mitigate the computational loads, yet the communication overheadremains a challenge. This paper presents RapidGNN, a distributed GNN trainingframework with deterministic sampling-based scheduling to enable efficientcache construction and prefetching of remote features. Evaluation on benchmarkgraph datasets demonstrates RapidGNN's effectiveness across different scalesand topologies. RapidGNN improves end-to-end training throughput by 2.46x to3.00x on average over baseline methods across the benchmark datasets, whilecutting remote feature fetches by over 9.70x to 15.39x. RapidGNN furtherdemonstrates near-linear scalability with an increasing number of computingunits efficiently. Furthermore, it achieves increased energy efficiency overthe baseline methods for both CPU and GPU by 44% and 32%, respectively.</description>
      <author>example@mail.com (Arefin Niam, Tevfik Kosar, M S Q Zulkar Nine)</author>
      <guid isPermaLink="false">2509.05207v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Matrix Factorization Based Graph Contrastive Learning for Recommendation System</title>
      <link>http://arxiv.org/abs/2509.05115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HMFGCL的新方法，结合了低秩矩阵分解和奇异值分解两种技术，通过图对比学习解决推荐系统中的数据稀疏性问题，并在多个数据集上表现出色，尤其是在小规模数据集上。&lt;h4&gt;背景&lt;/h4&gt;近年来，结合对比学习和图神经网络的方法已出现用于解决推荐系统的挑战，在推荐领域展现出强大性能并发挥重要作用。&lt;h4&gt;目的&lt;/h4&gt;解决现有图对比学习方法中数据增强策略的局限性，更好地捕获用户-项目交互信息。&lt;h4&gt;方法&lt;/h4&gt;提出HMFGCL（基于混合矩阵分解的图对比学习）方法，整合低秩矩阵分解（MF）和奇异值分解（SVD）两种技术，互补获取全局协作信息，构建增强视图。&lt;h4&gt;主要发现&lt;/h4&gt;现有图对比学习方法主要基于扰动图结构和应用聚类两种数据增强策略，但这些策略获得的交互信息不能完全捕捉用户-项目交互；对比学习通过数据增强策略有效缓解了数据稀疏问题。&lt;h4&gt;结论&lt;/h4&gt;在多个公共数据集上的实验结果表明，HMFGCL模型优于现有基线方法，特别在小规模数据集上表现突出。&lt;h4&gt;翻译&lt;/h4&gt;近年来，结合对比学习与图神经网络的方法已出现，用于解决推荐系统的挑战，展现出强大的性能并在该领域发挥重要作用。对比学习主要通过采用数据增强策略解决数据稀疏问题，有效缓解了这一问题并显示出良好的结果。尽管现有研究已取得良好成果，但当前大多数图对比学习方法基于两种数据增强策略：第一种是扰动图结构，如随机添加或删除边；第二种是应用聚类技术。我们认为通过这两种策略获得的交互信息不能完全捕捉用户-项目交互。在本文中，我们提出了一种名为HMFGCL（基于混合矩阵分解的图对比学习）的新方法，该方法整合了两种不同的矩阵分解技术——低秩矩阵分解（MF）和奇异值分解（SVD）——互补地获取全局协作信息，从而构建增强视图。在多个公共数据集上的实验结果表明，我们的模型优于现有基线方法，特别是在小规模数据集上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, methods that combine contrastive learning with graph neuralnetworks have emerged to address the challenges of recommendation systems,demonstrating powerful performance and playing a significant role in thisdomain. Contrastive learning primarily tackles the issue of data sparsity byemploying data augmentation strategies, effectively alleviating this problemand showing promising results. Although existing research has achievedfavorable outcomes, most current graph contrastive learning methods are basedon two types of data augmentation strategies: the first involves perturbing thegraph structure, such as by randomly adding or removing edges; and the secondapplies clustering techniques. We believe that the interactive informationobtained through these two strategies does not fully capture the user-iteminteractions. In this paper, we propose a novel method called HMFGCL (HybridMatrix Factorization Based Graph Contrastive Learning), which integrates twodistinct matrix factorization techniques-low-rank matrix factorization (MF) andsingular value decomposition (SVD)-to complementarily acquire globalcollaborative information, thereby constructing enhanced views. Experimentalresults on multiple public datasets demonstrate that our model outperformsexisting baselines, particularly on small-scale datasets.</description>
      <author>example@mail.com (Hao Chen, Wenming Ma, Zihao Chu, Mingqi Li)</author>
      <guid isPermaLink="false">2509.05115v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Graph Unlearning: Efficient Node Removal in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.04785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图神经网络节点遗忘方法，旨在有效移除敏感训练节点信息并保护隐私。作者提出了三种创新方法，其中两种特别利用了图拓扑特征，在基准测试中表现出优越的性能和效率。&lt;h4&gt;背景&lt;/h4&gt;随着对隐私攻击和敏感信息泄露的担忧增加，研究人员积极探索从图神经网络模型中高效移除敏感训练数据的方法。节点遗忘作为一种有前景的技术，可以通过从GNN模型中高效移除特定训练节点信息来保护敏感节点的隐私。然而，现有的节点遗忘方法要么对GNN结构施加限制，要么没有有效利用图拓扑进行节点遗忘，甚至会损害图的拓扑结构，难以实现满意的性能-复杂度权衡。&lt;h4&gt;目的&lt;/h4&gt;解决现有节点遗忘方法的局限性，实现GNN中训练节点移除的高效遗忘，提出三种新颖的节点遗忘方法，并验证它们在保护GNN模型隐私方面的优越性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了三种新颖的节点遗忘方法：1) 基于类的标签替换(Class-based Label Replacement)；2) 基于拓扑引导的邻居平均后验概率(Topology-guided Neighbor Mean Posterior Probability)；3) 类一致的邻居节点过滤(Class-consistent Neighbor Node Filtering)。其中，后两种方法有效利用了图的拓扑特征，实现了更有效的节点遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集上进行的实验结果表明，所提出的方法在模型效用、遗忘效用和遗忘效率方面均表现出优越性，证明了它们在节点遗忘方面的实用性和效率，以及与最先进的节点遗忘方法相比的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够高效移除敏感训练节点，保护GNN中敏感节点的隐私信息。这些发现有助于提高GNN模型的隐私和安全性，并为节点遗忘领域提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;随着对隐私攻击和潜在敏感信息泄露的担忧日益增加，研究人员积极探索高效移除敏感训练数据并降低图神经网络(GNN)模型隐私风险的方法。节点遗忘已成为一种有前景的技术，通过从GNN模型中高效移除特定训练节点信息来保护敏感节点的隐私。然而，现有的节点遗忘方法要么对GNN结构施加限制，要么没有有效利用图拓扑进行节点遗忘。一些方法甚至损害了图的拓扑结构，使得难以实现令人满意的性能-复杂度权衡。为了解决这些问题并实现GNN中训练节点移除的高效遗忘，我们提出了三种新颖的节点遗忘方法：基于类的标签替换、基于拓扑引导的邻居平均后验概率和类一致的邻居节点过滤。在这些方法中，基于拓扑引导的邻居平均后验概率和类一致的邻居节点过滤有效利用了图的拓扑特征，实现了更有效的节点遗忘。为了验证我们提出的节点遗忘方法的优越性，我们在三个基准数据集上进行了实验。评估标准包括模型效用、遗忘效用和遗忘效率。实验结果证明了所提出方法的实用性和效率，并展示了它们与最先进的节点遗忘方法相比的优势。总体而言，所提出的方法能够高效移除敏感训练节点，保护GNN中敏感节点的隐私信息。这些发现有助于提高GNN模型的隐私和安全性，并为节点遗忘领域提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With increasing concerns about privacy attacks and potential sensitiveinformation leakage, researchers have actively explored methods to efficientlyremove sensitive training data and reduce privacy risks in graph neural network(GNN) models. Node unlearning has emerged as a promising technique forprotecting the privacy of sensitive nodes by efficiently removing specifictraining node information from GNN models. However, existing node unlearningmethods either impose restrictions on the GNN structure or do not effectivelyutilize the graph topology for node unlearning. Some methods even compromisethe graph's topology, making it challenging to achieve a satisfactoryperformance-complexity trade-off. To address these issues and achieve efficientunlearning for training node removal in GNNs, we propose three novel nodeunlearning methods: Class-based Label Replacement, Topology-guided NeighborMean Posterior Probability, and Class-consistent Neighbor Node Filtering. Amongthese methods, Topology-guided Neighbor Mean Posterior Probability andClass-consistent Neighbor Node Filtering effectively leverage the topologicalfeatures of the graph, resulting in more effective node unlearning. To validatethe superiority of our proposed methods in node unlearning, we conductedexperiments on three benchmark datasets. The evaluation criteria included modelutility, unlearning utility, and unlearning efficiency. The experimentalresults demonstrate the utility and efficiency of the proposed methods andillustrate their superiority compared to state-of-the-art node unlearningmethods. Overall, the proposed methods efficiently remove sensitive trainingnodes and protect the privacy information of sensitive nodes in GNNs. Thefindings contribute to enhancing the privacy and security of GNN models andprovide valuable insights into the field of node unlearning.</description>
      <author>example@mail.com (Faqian Guan, Tianqing Zhu, Zhoutian Wang, Wei Ren, Wanlei Zhou)</author>
      <guid isPermaLink="false">2509.04785v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Inferring the Graph Structure of Images for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.04677v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过寻找传统网格图和超像素方法的替代图表示来提高图神经网络在图像分类任务中的准确性，使用行相关图、列相关图和乘积图表示MNIST和Fashion-MNIST数据集中的图像，实验证明这些方法可以提高下游GNN模型的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;图像数据集如MNIST是测试图神经网络架构的关键基准。传统上，图像被表示为网格图，其中每个节点代表一个像素，边连接相邻像素(垂直和水平)，图信号是图像中每个像素的值(强度)。这些图通常用作图神经网络(如GraphCNNs、GAT、GatedGCN)的输入来对图像进行分类。&lt;h4&gt;目的&lt;/h4&gt;提高下游图神经网络任务的准确性，寻找替代传统网格图和超像素方法的图表示方法来表示数据集图像。&lt;h4&gt;方法&lt;/h4&gt;基于像素值之间的相关性，为MNIST和Fashion-MNIST中的每幅图像构建行相关图、列相关图和乘积图，延续了[5,6]中的方法，并将这些不同的图表示和特征作为输入提供给下游GNN模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，使用这些不同的图表示和特征作为下游GNN模型的输入，可以提高准确性，与使用传统网格图和超像素方法相比，这些替代图表示方法表现更好。&lt;h4&gt;结论&lt;/h4&gt;使用基于相关性的图表示可以改进图神经网络在图像分类任务中的性能，为图像数据集的图表示提供了新的视角，超越了传统的网格图和超像素方法。&lt;h4&gt;翻译&lt;/h4&gt;图像数据集如MNIST是测试图神经网络架构的关键基准。图像传统上被表示为网格图，其中每个节点代表一个像素，边连接相邻像素(垂直和水平)。图信号是图像中每个像素的值(强度)。这些图通常用作图神经网络(例如图卷积神经网络(GraphCNNs)[1,2]、图注意力网络(GAT)[3]、GatedGCN[4])的输入来对图像进行分类。在本工作中，我们通过寻找网格图和超像素方法的替代图来表示数据集图像，改进了下游图神经网络任务的准确性，遵循[5,6]中的方法。我们使用像素值之间的相关性，基于[5,6]中的方法，为MNIST和Fashion-MNIST中的每幅图像找到行相关图、列相关图和乘积图。实验表明，将这些不同的图表示和特征作为下游GNN模型的输入，比使用文献中的传统网格图和超像素方法提高了准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image datasets such as MNIST are a key benchmark for testing Graph NeuralNetwork (GNN) architectures. The images are traditionally represented as a gridgraph with each node representing a pixel and edges connecting neighboringpixels (vertically and horizontally). The graph signal is the values(intensities) of each pixel in the image. The graphs are commonly used as inputto graph neural networks (e.g., Graph Convolutional Neural Networks (GraphCNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify theimages. In this work, we improve the accuracy of downstream graph neuralnetwork tasks by finding alternative graphs to the grid graph and superpixelmethods to represent the dataset images, following the approach in [5, 6]. Wefind row correlation, column correlation, and product graphs for each image inMNIST and Fashion-MNIST using correlations between the pixel values building onthe method in [5, 6]. Experiments show that using these different graphrepresentations and features as input into downstream GNN models improves theaccuracy over using the traditional grid graph and superpixel methods in theliterature.</description>
      <author>example@mail.com (Mayur S Gowda, John Shi, Augusto Santos, José M. F. Moura)</author>
      <guid isPermaLink="false">2509.04677v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Combining feature-based approaches with graph neural networks and symbolic regression for synergistic performance and interpretability</title>
      <link>http://arxiv.org/abs/2509.03547v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MatterVial是一个创新的混合框架，用于材料科学中的基于特征的机器学习，通过整合多种预训练图神经网络的潜在表示和新型特征，显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;材料科学领域需要结合传统特征模型的透明性和深度学习模型的预测能力，以提高材料预测的准确性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一个高性能、透明的机器学习框架，能够在保持化学透明度的同时，提供与最先进端到端图神经网络相媲美的预测能力。&lt;h4&gt;方法&lt;/h4&gt;整合多种预训练图神经网络（包括基于结构的MEGNet、基于组成的ROOST和等变的ORB图网络）的潜在表示，结合计算高效的GNN近似描述符和符号回归产生的新特征，并使用代理模型和符号回归进行可解释性分析。&lt;h4&gt;主要发现&lt;/h4&gt;在Matbench任务上，该方法显著降低了基于特征模型MODNet的误差，将其性能提升至与最先进的端到端GNN相当甚至在某些情况下超越它们，多个任务的准确率提高了40%以上。&lt;h4&gt;结论&lt;/h4&gt;MatterVial统一框架通过提供高性能、透明的工具，符合可解释AI原则，推动了材料信息学的发展，为更有针对性和自主性的材料发现铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了MatterVial，一个用于材料科学中基于特征机器学习的创新混合框架。MatterVial通过整合来自多样化预训练图神经网络模型的潜在表示来扩展特征空间，包括：基于结构的（MEGNet）、基于组成的（ROOST）和等变的（ORB）图网络，以及计算高效的GNN近似描述符和符号回归的新特征。我们的方法结合了传统基于特征模型的化学透明性和深度学习架构的预测能力。在Matbench任务上增强基于特征的模型MODNet时，这种方法带来了显著的误差降低，并将其性能提升至与最先进的端到端GNN相媲美，在多个情况下甚至超越它们，多个任务的准确率提高了40%以上。集成的可解释性模块使用代理模型和符号回归，将潜在的GNN衍生描述符解码为明确、具有物理意义的公式。这个统一框架通过提供符合可解释AI原则的高性能透明工具，推动了材料信息学的发展，为更有针对性和自主性的材料发现铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces MatterVial, an innovative hybrid framework forfeature-based machine learning in materials science. MatterVial expands thefeature space by integrating latent representations from a diverse suite ofpretrained graph neural network (GNN) models including: structure-based(MEGNet), composition-based (ROOST), and equivariant (ORB) graph networks, withcomputationally efficient, GNN-approximated descriptors and novel features fromsymbolic regression. Our approach combines the chemical transparency oftraditional feature-based models with the predictive power of deep learningarchitectures. When augmenting the feature-based model MODNet on Matbenchtasks, this method yields significant error reductions and elevates itsperformance to be competitive with, and in several cases superior to,state-of-the-art end-to-end GNNs, with accuracy increases exceeding 40% formultiple tasks. An integrated interpretability module, employing surrogatemodels and symbolic regression, decodes the latent GNN-derived descriptors intoexplicit, physically meaningful formulas. This unified framework advancesmaterials informatics by providing a high-performance, transparent tool thataligns with the principles of explainable AI, paving the way for more targetedand autonomous materials discovery.</description>
      <author>example@mail.com (Rogério Almeida Gouvêa, Pierre-Paul De Breuck, Tatiane Pretto, Gian-Marco Rignanese, Marcos José Leite Santos)</author>
      <guid isPermaLink="false">2509.03547v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Crosscoding Through Time: Tracking Emergence &amp; Consolidation Of Linguistic Representations Throughout LLM Pretraining</title>
      <link>http://arxiv.org/abs/2509.05291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出使用稀疏交叉编码器和新的相对间接效应(RelIE)指标来追踪大型语言模型预训练过程中语言特征的演变，填补了传统评估方法无法揭示模型如何获取概念和能力的空白。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在预训练过程中学习到复杂的抽象能力，如检测不规则复数名词主语，但传统评估方法无法揭示这些能力如何形成。&lt;h4&gt;目的&lt;/h4&gt;为了更好地理解模型在概念层面的训练过程，发现和校准不同模型检查点之间的特征演变。&lt;h4&gt;方法&lt;/h4&gt;使用稀疏交叉编码器在具有显著性能和表示变化的开放源代码检查点之间进行训练，并引入相对间接效应(RelIE)指标来追踪特征何时对任务性能变得因果重要。&lt;h4&gt;主要发现&lt;/h4&gt;交叉编码器可以检测预训练过程中语言特征的出现、维持和消失，并通过RelIE指标能够识别特征变得对任务性能有因果相关性的具体训练阶段。&lt;h4&gt;结论&lt;/h4&gt;该方法与架构无关且可扩展，为更可解释和细粒度地分析预训练过程中的表示学习提供了有希望的途径。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在预训练过程中学习到复杂的抽象能力，比如检测不规则复数名词主语。然而，目前尚不清楚特定的语言能力何时以及如何出现，因为传统的评估方法无法揭示模型如何获取概念和能力。为了填补这一空白并更好地理解模型在概念层面的训练过程，我们使用稀疏交叉编码器来发现和校准不同模型检查点之间的特征。使用这种方法，我们跟踪了预训练过程中语言特征的演变。我们在具有显著性能和表示变化的开放源代码检查点之间训练交叉编码器，并引入了一种新的指标——相对间接效应(RelIE)，用于追踪单个特征对任务性能变得因果相关的时间点。我们表明，交叉编码器可以检测预训练过程中特征的出现、维持和消失。我们的方法与架构无关且可扩展，为更可解释和细粒度地分析预训练过程中的表示学习提供了有希望的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) learn non-trivial abstractions duringpretraining, like detecting irregular plural noun subjects. However, it is notwell understood when and how specific linguistic abilities emerge astraditional evaluation methods such as benchmarking fail to reveal how modelsacquire concepts and capabilities. To bridge this gap and better understandmodel training at the concept level, we use sparse crosscoders to discover andalign features across model checkpoints. Using this approach, we track theevolution of linguistic features during pretraining. We train crosscodersbetween open-sourced checkpoint triplets with significant performance andrepresentation shifts, and introduce a novel metric, Relative Indirect Effects(RelIE), to trace training stages at which individual features become causallyimportant for task performance. We show that crosscoders can detect featureemergence, maintenance, and discontinuation during pretraining. Our approach isarchitecture-agnostic and scalable, offering a promising path toward moreinterpretable and fine-grained analysis of representation learning throughoutpretraining.</description>
      <author>example@mail.com (Deniz Bayazit, Aaron Mueller, Antoine Bosselut)</author>
      <guid isPermaLink="false">2509.05291v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition</title>
      <link>http://arxiv.org/abs/2509.05188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对手语识别的自监督学习框架，解决了传统对比学习方法在处理手语视频时面临的忽视视频部分相关性和负对相似性问题。&lt;h4&gt;背景&lt;/h4&gt;手语识别(SLR)是一个旨在识别视频中手语的机器学习任务。由于标注数据稀缺，无监督方法如对比学习在该领域变得很有前景，它们通过拉近正对(同一实例的两个增强版本)和推开负对(不同于正对)来学习有意义的表示。&lt;h4&gt;目的&lt;/h4&gt;设计一个自监督学习框架，学习对手语识别有意义的表现，解决传统对比学习方法在SLR中的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含两个关键组件的自监督学习框架：(1)一种基于自由负对的新型自监督方法；(2)一种新的数据增强技术。&lt;h4&gt;主要发现&lt;/h4&gt;该方法与多种对比和自监督方法相比，在线性评估、半监督学习和手语间迁移性方面显示出显著的精度提升。&lt;h4&gt;结论&lt;/h4&gt;通过解决传统对比学习方法在SLR中的两个关键问题，本文提出的自监督框架能够学习更具区分性的特征，提高下游任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;手语识别(SLR)是一种机器学习任务，旨在识别视频中的手语。由于标注数据的稀缺，像对比学习这样的无监督方法已成为该领域有前景的方法。它们通过将正对(同一实例的两个增强版本)拉近并将负对(不同于正对)推开来学习有意义的表示。在SLR中，在手语视频中，只有某些部分提供对其识别真正有用的信息。将对比方法应用于SLR引发两个问题：(i)对比学习方法同等对待视频的所有部分，没有考虑某些部分的相关性；(ii)不同手势之间的共享动作使得负对高度相似，增加了手势区分的难度。这些问题导致学习到对手语识别缺乏区分性的特征，并在下游任务中表现不佳。为此，本文提出了一种专为学习SLR有意义表示而设计的自监督学习框架。该框架包含两个协同工作的关键组件：(i)一种基于自由负对的新型自监督方法；(ii)一种新的数据增强技术。与多种对比和自监督方法相比，该方法在线性评估、半监督学习和手语间的迁移性方面显示出显著的精度提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language recognition (SLR) is a machine learning task aiming to identifysigns in videos. Due to the scarcity of annotated data, unsupervised methodslike contrastive learning have become promising in this field. They learnmeaningful representations by pulling positive pairs (two augmented versions ofthe same instance) closer and pushing negative pairs (different from thepositive pairs) apart. In SLR, in a sign video, only certain parts provideinformation that is truly useful for its recognition. Applying contrastivemethods to SLR raises two issues: (i) contrastive learning methods treat allparts of a video in the same way, without taking into account the relevance ofcertain parts over others; (ii) shared movements between different signs makenegative pairs highly similar, complicating sign discrimination. These issueslead to learning non-discriminative features for sign recognition and poorresults in downstream tasks. In response, this paper proposes a self-supervisedlearning framework designed to learn meaningful representations for SLR. Thisframework consists of two key components designed to work together: (i) a newself-supervised approach with free-negative pairs; (ii) a new data augmentationtechnique. This approach shows a considerable gain in accuracy compared toseveral contrastive and self-supervised methods, across linear evaluation,semi-supervised learning, and transferability between sign languages.</description>
      <author>example@mail.com (Ariel Basso Madjoukeng, Jérôme Fink, Pierre Poitier, Edith Belise Kenmogne, Benoit Frenay)</author>
      <guid isPermaLink="false">2509.05188v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Multidimensional Urban Poverty Representation with Satellite Imagery</title>
      <link>http://arxiv.org/abs/2509.04958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的表示学习框架，通过融合卫星图像中的可及性、形态学和经济特征，实现精确的城市贫困制图，解决了仅依赖城市化特征无法准确捕捉贫困的问题。&lt;h4&gt;背景&lt;/h4&gt;深度学习的进步使得从卫星图像推断城市社会经济特征成为可能，但仅依赖城市化特征的模型与贫困指标相关性较弱，因为无序的城市增长可能掩盖经济差异和空间不平等。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的表示学习框架，从超高分辨率卫星图像中捕获多维度的剥夺相关特征，用于精确的城市贫困制图。&lt;h4&gt;方法&lt;/h4&gt;该方法整合了三种互补特征：(1)通过对比学习编码接近基本基础设施的可及性特征；(2)从建筑足迹推导反映非正规定居点住房条件的形态学特征；(3)从夜间灯光强度推断经济活动的经济特征。同时，通过后门调整机制利用形态学特征减轻训练经济模块时的虚假相关性。&lt;h4&gt;主要发现&lt;/h4&gt;通过融合这些互补特征，该框架能够捕捉与经济发展趋势不同的贫困复杂性质。在开普敦、达卡和金边三个首都城市的评估中，该模型显著优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;该框架为数据稀缺地区的贫困制图和政策支持提供了强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;深度学习的最新进展使得从卫星图像中推断城市社会经济特征成为可能。然而，仅依赖城市化特征的模型通常与贫困指标相关性较弱，因为无序的城市增长可能掩盖经济差异和空间不平等。为解决这一局限，我们引入了一种新的表示学习框架，从超高分辨率卫星图像中捕获多维度的剥夺相关特征，用于精确的城市贫困制图。我们的方法整合了三种互补特征：(1)通过对比学习编码接近基本基础设施的可及性特征；(2)从建筑足迹推导反映非正规定居点住房条件的形态学特征；(3)从夜间灯光强度推断作为经济活动代理的经济特征。为了减轻虚假相关性——例如那些不能代表贫困条件的非住宅夜间灯光源——我们在训练经济模块时纳入了一个利用形态学特征的后门调整机制。通过将这些互补特征融合到统一表示中，我们的框架捕捉了贫困的复杂性质，这些性质往往与经济发展趋势不同。在开普敦、达卡和金边三个首都城市的评估中，我们的模型显著优于现有基线，为数据稀缺地区的贫困制图和政策支持提供了强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in deep learning have enabled the inference of urbansocioeconomic characteristics from satellite imagery. However, models relyingsolely on urbanization traits often show weak correlations with povertyindicators, as unplanned urban growth can obscure economic disparities andspatial inequalities. To address this limitation, we introduce a novelrepresentation learning framework that captures multidimensionaldeprivation-related traits from very high-resolution satellite imagery forprecise urban poverty mapping. Our approach integrates three complementarytraits: (1) accessibility traits, learned via contrastive learning to encodeproximity to essential infrastructure; (2) morphological traits, derived frombuilding footprints to reflect housing conditions in informal settlements; and(3) economic traits, inferred from nightlight intensity as a proxy for economicactivity. To mitigate spurious correlations - such as those fromnon-residential nightlight sources that misrepresent poverty conditions - weincorporate a backdoor adjustment mechanism that leverages morphological traitsduring training of the economic module. By fusing these complementary featuresinto a unified representation, our framework captures the complex nature ofpoverty, which often diverges from economic development trends. Evaluationsacross three capital cities - Cape Town, Dhaka, and Phnom Penh - show that ourmodel significantly outperforms existing baselines, offering a robust tool forpoverty mapping and policy support in data-scarce regions.</description>
      <author>example@mail.com (Sungwon Park, Sumin Lee, Jihee Kim, Jae-Gil Lee, Meeyoung Cha, Jeasurk Yang, Donghyun Ahn)</author>
      <guid isPermaLink="false">2509.04958v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization</title>
      <link>http://arxiv.org/abs/2509.04745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何通过语音学归纳偏差改进手语学习表示的推广性，特别是在处理未见手语符号时的表现。&lt;h4&gt;背景&lt;/h4&gt;手语数据集在词汇方面通常不具有代表性，这突显了需要能够推广到未见过的手语符号的模型的必要性。&lt;h4&gt;目的&lt;/h4&gt;调查两种语音学归纳偏差（参数解耦和语音学半监督）以提高已知手语符号的识别和未见手语符号的重构质量。&lt;h4&gt;方法&lt;/h4&gt;使用向量量化自编码器，结合参数解耦（架构偏差）和语音学半监督（正则化技术）来改进手语识别和重构。&lt;h4&gt;主要发现&lt;/h4&gt;与受控基线相比，所提出模型学习到的表示对于一次性重建未见手语符号更有效，并且对于手语识别更具区分性。&lt;h4&gt;结论&lt;/h4&gt;明确的、语言动机的偏差可以改进手语学习表示的推广性，本研究提供了相关的定量分析。&lt;h4&gt;翻译&lt;/h4&gt;手语数据集在词汇方面通常不具有代表性，突显了需要能够推广到未见手语的模型的必要性。向量量化是学习离散类令牌表示的一种有前景的方法，但目前尚未评估学习到的单元是否会捕获阻碍词汇外性能的虚假相关性。本研究调查了两种语音学归纳偏差：参数解耦（架构偏差）和语音学半监督（正则化技术），以使用向量量化自编码器改进已知手语符号的孤立手语识别和未见手语符号的重构质量。主要发现是，与受控基线相比，所提出模型学习到的表示对于一次性重建未见手语符号更有效，并且对于手语识别更具区分性。这项工作提供了关于明确的、语言动机的偏差如何改进手语学习表示的推广性的定量分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language datasets are often not representative in terms of vocabulary,underscoring the need for models that generalize to unseen signs. Vectorquantization is a promising approach for learning discrete, token-likerepresentations, but it has not been evaluated whether the learned unitscapture spurious correlations that hinder out-of-vocabulary performance. Thiswork investigates two phonological inductive biases: Parameter Disentanglement,an architectural bias, and Phonological Semi-Supervision, a regularizationtechnique, to improve isolated sign recognition of known signs andreconstruction quality of unseen signs with a vector-quantized autoencoder. Theprimary finding is that the learned representations from the proposed model aremore effective for one-shot reconstruction of unseen signs and morediscriminative for sign identification compared to a controlled baseline. Thiswork provides a quantitative analysis of how explicit, linguistically-motivatedbiases can improve the generalization of learned representations of signlanguage.</description>
      <author>example@mail.com (Lee Kezar, Zed Sehyr, Jesse Thomason)</author>
      <guid isPermaLink="false">2509.04745v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics</title>
      <link>http://arxiv.org/abs/2509.04737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 5 figures, Accepted at CoRL2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种运动生成模型，使机器人能够根据人类指令中的修饰词调整动作，在擦拭和拾取放置任务中表现良好，能够在线响应修饰指令调整动作，优于传统的批量处理方法。&lt;h4&gt;背景&lt;/h4&gt;在机器人学习领域，通过语言指令协调机器人动作正变得越来越可行，但使机器人动作适应人类指令仍具挑战性，因为这些指令通常是定性的，需要探索满足不同条件的行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种运动生成模型，使机器人能够根据人类指令中的修饰词在任务执行期间调整动作行为。&lt;h4&gt;方法&lt;/h4&gt;通过将演示分割成短序列，分配与特定修饰类型相对应的弱监督标签，学习从修饰指令到动作的映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;在擦拭和拾取放置任务中评估显示，该方法能够在线响应修饰指令调整动作，而传统批量处理方法在执行期间无法适应。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型能够实时调整机器人动作以适应人类指令中的修饰条件，提高了机器人对语言指令的适应能力。&lt;h4&gt;翻译&lt;/h4&gt;在机器人学习领域，通过语言指令协调机器人动作正变得越来越可行。然而，使机器人动作适应人类指令仍然具有挑战性，因为这些指令通常是定性的，需要探索满足不同条件的行为。本文提出了一种运动生成模型，使机器人能够根据人类指令中的修饰词在任务执行期间调整动作行为。所提出的方法通过将演示分割成短序列，分配与特定修饰类型相对应的弱监督标签，学习从修饰指令到动作的映射。我们在擦拭和拾取放置任务中评估了该方法。结果表明，它能够在线响应修饰指令调整动作，而传统的批量处理方法在执行期间无法适应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of robot learning, coordinating robot actions through languageinstructions is becoming increasingly feasible. However, adapting actions tohuman instructions remains challenging, as such instructions are oftenqualitative and require exploring behaviors that satisfy varying conditions.This paper proposes a motion generation model that adapts robot actions inresponse to modifier directives human instructions imposing behavioralconditions during task execution. The proposed method learns a mapping frommodifier directives to actions by segmenting demonstrations into shortsequences, assigning weakly supervised labels corresponding to specificmodifier types. We evaluated our method in wiping and pick and place tasks.Results show that it can adjust motions online in response to modifierdirectives, unlike conventional batch-based methods that cannot adapt duringexecution.</description>
      <author>example@mail.com (Ryoga Oishi, Sho Sakaino, Toshiaki Tsuji)</author>
      <guid isPermaLink="false">2509.04737v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
      <link>http://arxiv.org/abs/2509.04734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Beyond I-Con框架，通过探索替代的统计散度和相似性核来系统地发现新的损失函数，解决了KL散度在表示学习中可能带来的优化挑战。&lt;h4&gt;背景&lt;/h4&gt;Information Contrastive (I-Con)框架揭示了超过23种表示学习方法隐式地最小化了数据分布和学习分布之间的KL散度，但KL散度存在不对称性和无界性等特性，可能导致优化挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够系统性发现新型损失函数的框架，通过探索替代的统计散度和相似性核来超越现有的基于KL的方法。&lt;h4&gt;方法&lt;/h4&gt;探索替代的统计散度和相似性核；修改PMI算法使用总变差(TV)距离；在监督对比学习中使用TV和基于距离的相似性核替代KL和角度核；在降维中用有界的f-散度替代KL。&lt;h4&gt;主要发现&lt;/h4&gt;(1)在DINO-ViT嵌入的无监督聚类上，使用TV距离的修改版PMI算法实现了最先进结果；(2)在监督对比学习中，使用TV和基于距离的相似性核超越了标准方法；(3)在降维任务中，用有界的f-散度替代KL获得了比SNE更好的结果和下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在表示学习优化中，散度和相似性核的选择至关重要。&lt;h4&gt;翻译&lt;/h4&gt;信息对比(I-Con)框架揭示，超过23种表示学习方法隐式地最小化了数据分布和学习分布之间的KL散度，这些分布编码了数据点之间的相似性。然而，基于KL的损失函数可能与真实目标不一致，KL散度的特性如不对称性和无界性可能会带来优化挑战。我们提出了Beyond I-Con框架，通过探索替代的统计散度和相似性核，使系统能够发现新的损失函数。主要发现：(1)在DINO-ViT嵌入的无监督聚类中，我们通过修改PMI算法使用总变差(TV)距离实现了最先进的结果；(2)在监督对比学习中，我们使用TV和基于距离的相似性核而非KL和角度核，超越了标准方法；(3)在降维方面，我们通过用有界的f-散度替代KL，获得了比SNE更好的定性结果和下游任务性能。我们的结果突显了在表示学习优化中考虑散度和相似性核选择的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Information Contrastive (I-Con) framework revealed that over 23representation learning methods implicitly minimize KL divergence between dataand learned distributions that encode similarities between data points.However, a KL-based loss may be misaligned with the true objective, andproperties of KL divergence such as asymmetry and unboundedness may createoptimization challenges. We present Beyond I-Con, a framework that enablessystematic discovery of novel loss functions by exploring alternativestatistical divergences and similarity kernels. Key findings: (1) onunsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-artresults by modifying the PMI algorithm to use total variation (TV) distance;(2) on supervised contrastive learning, we outperform the standard approach byusing TV and a distance-based similarity kernel instead of KL and an angularkernel; (3) on dimensionality reduction, we achieve superior qualitativeresults and better performance on downstream tasks than SNE by replacing KLwith a bounded f-divergence. Our results highlight the importance ofconsidering divergence and similarity kernel choices in representation learningoptimization.</description>
      <author>example@mail.com (Jasmine Shone, Shaden Alshammari, Mark Hamilton, Zhening Li, William Freeman)</author>
      <guid isPermaLink="false">2509.04734v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Unified Representation Learning for Multi-Intent Diversity and Behavioral Uncertainty in Recommender Systems</title>
      <link>http://arxiv.org/abs/2509.04694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一表示学习框架，用于解决推荐系统中用户意图多样性和行为不确定性的联合建模问题。该框架包含多意图表示模块和不确定性建模机制，通过贝叶斯分布建模捕捉行为模糊性和偏好波动，结合长期意图和短期行为信号，提高推荐准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统推荐系统在处理复杂用户行为时面临建模瓶颈，特别是在捕捉用户意图多样性和行为不确定性方面存在挑战。用户行为序列中包含多粒度兴趣结构，且存在行为模糊性和偏好波动，这些因素都需要被有效建模。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够联合建模用户意图多样性和行为不确定性的推荐系统框架，通过提取用户行为序列中的多粒度兴趣结构，捕捉行为模糊性和偏好波动，提高推荐的准确性和鲁棒性，特别是在冷启动和行为干扰场景下的表现。&lt;h4&gt;方法&lt;/h4&gt;提出统一表示学习框架，包含：多意图表示模块（引入多个潜在意图向量，通过注意力机制进行加权融合，生成长期用户偏好语义丰富表示）和不确定性建模机制（通过高斯分布学习行为表示的均值和协方差，反映用户在不同行为上下文中的置信度），以及可学习融合策略（结合长期意图和短期行为信号，生成最终用户表示）。&lt;h4&gt;主要发现&lt;/h4&gt;在标准公共数据集上的实验表明，该方法在多个指标上优于现有代表性模型；在冷启动和行为干扰场景下表现出更高的稳定性和适应性；有效缓解了传统方法处理复杂用户行为时的建模瓶颈。&lt;h4&gt;结论&lt;/h4&gt;统一建模策略在现实世界推荐任务中具有有效性和实用价值，能够同时提高推荐准确性和鲁棒性，特别是在处理复杂用户行为方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了推荐系统中联合建模用户意图多样性和行为不确定性的挑战。提出了一种统一的表示学习框架。该框架构建了多意图表示模块和不确定性建模机制。它从用户行为序列中提取多粒度兴趣结构。使用贝叶斯分布建模捕捉行为模糊性和偏好波动。在多意图建模部分，模型引入了多个潜在意图向量。这些向量通过注意力机制进行加权融合，生成长期用户偏好的语义丰富表示。在不确定性建模部分，模型通过高斯分布学习行为表示的均值和协方差。这反映了用户在不同行为上下文中的置信度。接下来，使用可学习融合策略结合长期意图和短期行为信号。这产生了最终的用户表示，提高了推荐准确性和鲁棒性。该方法在标准公共数据集上进行了评估。实验结果表明，它在多个指标上优于现有代表性模型。在冷启动和行为干扰场景下，它还表现出更高的稳定性和适应性。该方法缓解了传统方法处理复杂用户行为时面临的建模瓶颈。这些发现证实了统一建模策略在现实世界推荐任务中的有效性和实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of jointly modeling user intent diversityand behavioral uncertainty in recommender systems. A unified representationlearning framework is proposed. The framework builds a multi-intentrepresentation module and an uncertainty modeling mechanism. It extractsmulti-granularity interest structures from user behavior sequences. Behavioralambiguity and preference fluctuation are captured using Bayesian distributionmodeling. In the multi-intent modeling part, the model introduces multiplelatent intent vectors. These vectors are weighted and fused using an attentionmechanism to generate semantically rich representations of long-term userpreferences. In the uncertainty modeling part, the model learns the mean andcovariance of behavior representations through Gaussian distributions. Thisreflects the user's confidence in different behavioral contexts. Next, alearnable fusion strategy is used to combine long-term intent and short-termbehavior signals. This produces the final user representation, improving bothrecommendation accuracy and robustness. The method is evaluated on standardpublic datasets. Experimental results show that it outperforms existingrepresentative models across multiple metrics. It also demonstrates greaterstability and adaptability under cold-start and behavioral disturbancescenarios. The approach alleviates modeling bottlenecks faced by traditionalmethods when dealing with complex user behavior. These findings confirm theeffectiveness and practical value of the unified modeling strategy inreal-world recommendation tasks.</description>
      <author>example@mail.com (Wei Xu, Jiasen Zheng, Junjiang Lin, Mingxuan Han, Junliang Du)</author>
      <guid isPermaLink="false">2509.04694v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</title>
      <link>http://arxiv.org/abs/2509.05144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SGS-3D的高保真3D实例分割方法，通过'先分割后生长'框架解决2D到3D提升过程中的累积误差问题，有效融合语义和几何信息，显著提高了分割准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;准确的3D实例分割对3D视觉领域的高质量场景理解至关重要，但基于2D到3D提升方法的现有技术难以产生精确的实例级分割，主要因为在提升过程中从模糊语义引导和深度约束不足引入了累积误差。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D实例分割方法的局限性，提出一种能够提高分割精度和鲁棒性的新方法，特别是在处理模糊语义和几何信息时。&lt;h4&gt;方法&lt;/h4&gt;提出SGS-3D框架，采用'先分割后生长'策略：首先使用几何基元净化和分割模糊的提升掩模，然后将它们生长为场景中的完整实例；引入掩模过滤策略利用3D几何基元共现识别可靠语义；利用空间连续性和高级特征进行几何精炼以构建细粒度对象实例。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet200、ScanNet++和KITTI-360数据集上的实验表明，SGS-3D显著提高了分割准确性和鲁棒性，能够抵抗来自预训练模型的不准确掩模，产生高保真对象实例，并在多样化的室内和室外环境中保持强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SGS-3D作为一种无需训练的精炼方法，通过联合融合语义和几何信息，有效解决了2D到3D提升过程中的累积误差问题，为高保真3D实例分割提供了新思路，代码已在补充材料中公开。&lt;h4&gt;翻译&lt;/h4&gt;准确的3D实例分割对3D视觉领域的高质量场景理解至关重要。然而，基于2D到3D提升方法的3D实例分割难以产生精确的实例级分割，这是由于在从模糊语义引导和深度约束不足的提升过程中引入了累积误差。为解决这些挑战，我们提出了用于高保真3D实例分割的可靠语义掩模分割和生长方法（SGS-3D），一种新颖的'先分割后生长'框架，首先使用几何基元净化和分割模糊的提升掩模，然后将它们生长为场景中的完整实例。与直接依赖原始提升掩模并牺牲分割准确性的现有方法不同，SGS-3D作为一种无需训练的精炼方法，联合融合语义和几何信息，实现了两种表示级别之间的有效合作。具体而言，对于语义引导，我们引入了一种掩模过滤策略，利用3D几何基元的共现来识别和移除模糊掩模，从而确保与3D对象实例更可靠的语义一致性。对于几何精炼，我们利用空间连续性和高级特征构建细粒度对象实例，特别是在不同对象之间语义模糊的情况下。在ScanNet200、ScanNet++和KITTI-360上的实验结果表明，SGS-3D显著提高了分割准确性和对预训练模型不准确掩模的鲁棒性，产生高保真对象实例，同时在多样化的室内和室外环境中保持强大的泛化能力。代码可在补充材料中获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于2D到3D提升(2D-to-3D lifting)的3D实例分割方法中存在的精度问题，特别是在处理模糊语义引导、不足深度约束以及遮挡场景时导致的累积误差。这个问题在现实中非常重要，因为3D实例分割是自动驾驶、虚拟现实和多模态场景理解等领域的核心技术，而现有方法在开放世界环境中的泛化能力有限，且深度传感器在无纹理和高反射表面表现不佳，在野外场景中常常不可用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到基于2D预训练基础模型进行3D场景感知是一个有前景的方向，但现有方法存在局限性：基于特征的方法有训练效率和误差传播问题，基于掩码的方法则忽略了语义信息的稳健传播。作者借鉴了现有工作中的3D几何过度分割思想，使用SAM提取语义掩码，并采用HDBSCAN进行空间分割，但创新性地提出了'先分割后生长'策略，联合利用语义和几何线索来克服误差累积问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用'先分割后生长'(split-then-grow)策略，联合利用语义和几何线索来克服2D到3D提升中的误差累积。整体流程分为三个主要阶段：1)点-图像映射：建立鲁棒映射并计算可见性；2)2D掩码提案：生成初始掩码并通过同时出现过滤消除模糊掩码；3)语义引导聚合：先通过空间连续性分割生成纯语义-几何种子，再利用特征引导生长将种子扩展为完整实例，最后通过多视图渐进合并形成最终对象实例。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SGS-3D训练-free框架，实现'先分割后生长'策略；2)遮挡感知的点-图像映射，无需真实深度图；3)基于同时出现的掩码过滤机制，提高计算效率和鲁棒性；4)语义引导聚合管道，结合空间连续性和特征引导。相比之前的工作，SGS-3D不直接依赖原始提升掩码，而是先净化和分割模糊掩码；联合融合语义和几何信息；在深度约束不足场景中表现更好；处理遮挡和缺乏深度信息时更鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SGS-3D通过可靠的语义掩码分割和生长策略，解决了2D到3D提升方法中的累积误差问题，实现了高保真的3D实例分割，并在多种室内外场景中达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D instance segmentation is crucial for high-quality sceneunderstanding in the 3D vision domain. However, 3D instance segmentation basedon 2D-to-3D lifting approaches struggle to produce precise instance-levelsegmentation, due to accumulated errors introduced during the lifting processfrom ambiguous semantic guidance and insufficient depth constraints. To tacklethese challenges, we propose splitting and growing reliable semantic mask forhigh-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"framework that first purifies and splits ambiguous lifted masks using geometricprimitives, and then grows them into complete instances within the scene.Unlike existing approaches that directly rely on raw lifted masks and sacrificesegmentation accuracy, SGS-3D serves as a training-free refinement method thatjointly fuses semantic and geometric information, enabling effectivecooperation between the two levels of representation. Specifically, forsemantic guidance, we introduce a mask filtering strategy that leverages theco-occurrence of 3D geometry primitives to identify and remove ambiguous masks,thereby ensuring more reliable semantic consistency with the 3D objectinstances. For the geometric refinement, we construct fine-grained objectinstances by exploiting both spatial continuity and high-level features,particularly in the case of semantic ambiguity between distinct objects.Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate thatSGS-3D substantially improves segmentation accuracy and robustness againstinaccurate masks from pre-trained models, yielding high-fidelity objectinstances while maintaining strong generalization across diverse indoor andoutdoor environments. Code is available in the supplementary materials.</description>
      <author>example@mail.com (Chaolei Wang, Yang Luo, Jing Du, Siyu Chen, Yiping Chen, Ting Han)</author>
      <guid isPermaLink="false">2509.05144v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models</title>
      <link>http://arxiv.org/abs/2509.05230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Conference on Empirical Methods in Natural Language  Processing (EMNLP 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURE是一种新型轻量级框架，能够系统地分离和抑制概念性捷径，同时保留内容信息，在IMDB和Yelp数据集上显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;预训练语言模型在多种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性的影响，这些相关性损害了模型的鲁棒性和公平性。&lt;h4&gt;目的&lt;/h4&gt;引入CURE框架，系统地分离和抑制概念性捷径，同时保留基本的内容信息，以提高模型的鲁棒性和公平性。&lt;h4&gt;方法&lt;/h4&gt;通过专用内容提取器和反转网络提取概念无关表示，并使用可控去偏模块和对比学习来微调剩余概念线索的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在IMDB上的F1分数绝对提高+10分，在Yelp上提高+2分，同时引入最小的计算开销。&lt;h4&gt;结论&lt;/h4&gt;CURE为对抗概念偏差提供了一个灵活的无监督蓝图，有助于构建更可靠和公平的语言理解系统。&lt;h4&gt;翻译&lt;/h4&gt;预训练语言模型在多种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性的影响，这些相关性损害了模型的鲁棒性和公平性。在这项工作中，我们引入了CURE，一种新型轻量级框架，能够系统地分离和抑制概念性捷径，同时保留基本的内容信息。我们的方法首先通过一个由反转网络强化的专用内容提取器提取概念无关的表示，确保最小化任务相关信息的损失。随后的可控去偏模块采用对比学习来微调剩余概念线索的影响，使模型能够根据目标任务适当减少有害偏差或利用有益相关性。在IMDB和Yelp数据集上使用三种预训练架构进行评估，CURE在IMDB上的F1分数绝对提高了+10分，在Yelp上提高了+2分，同时引入了最小的计算开销。我们的方法为对抗概念偏差提供了一个灵活的无监督蓝图，为更可靠和公平的语言理解系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained language models have achieved remarkable success across diverseapplications but remain susceptible to spurious, concept-driven correlationsthat impair robustness and fairness. In this work, we introduce CURE, a noveland lightweight framework that systematically disentangles and suppressesconceptual shortcuts while preserving essential content information. Our methodfirst extracts concept-irrelevant representations via a dedicated contentextractor reinforced by a reversal network, ensuring minimal loss oftask-relevant information. A subsequent controllable debiasing module employscontrastive learning to finely adjust the influence of residual conceptualcues, enabling the model to either diminish harmful biases or harnessbeneficial correlations as appropriate for the target task. Evaluated on theIMDB and Yelp datasets using three pre-trained architectures, CURE achieves anabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,while introducing minimal computational overhead. Our approach establishes aflexible, unsupervised blueprint for combating conceptual biases, paving theway for more reliable and fair language understanding systems.</description>
      <author>example@mail.com (Aysenur Kocak, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci)</author>
      <guid isPermaLink="false">2509.05230v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation</title>
      <link>http://arxiv.org/abs/2509.04970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为DeGuV的强化学习框架，通过可学习的掩码网络和对比学习技术，提高强化学习代理的泛化能力和样本效率，同时保持训练稳定性。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理可以从视觉输入中学习解决复杂任务，但将学到的技能泛化到新环境仍然是强化学习应用的主要挑战，特别是在机器人领域。虽然数据增强可以提高泛化能力，但它通常会降低样本效率和训练稳定性。&lt;h4&gt;目的&lt;/h4&gt;开发一种强化学习框架，既能提高泛化能力，又能保持样本效率和训练稳定性。&lt;h4&gt;方法&lt;/h4&gt;DeGuV框架包含：1)可学习的掩码网络从深度输入生成掩码，只保留关键视觉信息；2)结合对比学习增强模型对关键特征的识别能力；3)稳定Q值估计技术在数据增强条件下保持训练稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;在RL-ViGen基准测试中使用Franka Emika机器人评估显示，DeGuV在零样本模拟到现实迁移任务中表现出色，在泛化能力和样本效率方面优于最先进方法，同时通过突出显示视觉输入中最相关区域提高了模型的可解释性。&lt;h4&gt;结论&lt;/h4&gt;DeGuV框架成功地解决了强化学习中泛化能力和样本效率之间的权衡问题，通过选择性关注关键视觉特征，实现了更好的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;强化学习代理可以从视觉输入中学习解决复杂任务，但将这些学到的技能泛化到新环境仍然是强化学习应用的主要挑战，特别是在机器人领域。虽然数据增强可以提高泛化能力，但它通常会降低样本效率和训练稳定性。本文介绍了DeGuV，一种增强泛化能力和样本效率的强化学习框架。具体而言，我们利用一个可学习的掩码网络从深度输入生成掩码，只保留关键视觉信息，同时丢弃无关像素。通过这种方式，我们确保强化学习代理关注关键特征，提高数据增强下的鲁棒性。此外，我们结合对比学习，并在增强条件下稳定Q值估计，以进一步提高样本效率和训练稳定性。我们在RL-ViGen基准上使用Franka Emika机器人评估了我们的方法，并证明了其在零样本模拟到现实迁移中的有效性。我们的结果表明，DeGuV在泛化能力和样本效率方面都优于最先进的方法，同时还通过突出显示视觉输入中最相关的区域提高了可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) agents can learn to solve complex tasks fromvisual inputs, but generalizing these learned skills to new environmentsremains a major challenge in RL application, especially robotics. While dataaugmentation can improve generalization, it often compromises sample efficiencyand training stability. This paper introduces DeGuV, an RL framework thatenhances both generalization and sample efficiency. In specific, we leverage alearnable masker network that produces a mask from the depth input, preservingonly critical visual information while discarding irrelevant pixels. Throughthis, we ensure that our RL agents focus on essential features, improvingrobustness under data augmentation. In addition, we incorporate contrastivelearning and stabilize Q-value estimation under augmentation to further enhancesample efficiency and training stability. We evaluate our proposed method onthe RL-ViGen benchmark using the Franka Emika robot and demonstrate itseffectiveness in zero-shot sim-to-real transfer. Our results show that DeGuVoutperforms state-of-the-art methods in both generalization and sampleefficiency while also improving interpretability by highlighting the mostrelevant regions in the visual input</description>
      <author>example@mail.com (Tien Pham, Xinyun Chi, Khang Nguyen, Manfred Huber, Angelo Cangelosi)</author>
      <guid isPermaLink="false">2509.04970v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</title>
      <link>http://arxiv.org/abs/2509.04833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PropVG是一种端到端的基于提议的视觉定位框架，整合了前景物体提议生成与参考物体理解，无需额外检测器。它引入了基于对比的参考评分(CRS)模块和多粒度目标区分(MTD)模块，通过对比学习增强物体理解能力，并融合物体级和语义级信息改善缺失目标的识别。实验证明其在多个基准测试上的有效性。&lt;h4&gt;背景&lt;/h4&gt;视觉定位领域最近已从基于提议的传统两阶段框架转向端到端的直接参考范式，因为前者效率低下且计算复杂度高。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉定位方法完全依赖被引用目标进行监督而忽视潜在前瞻性目标的问题，以及未能融入多粒度区分导致在复杂场景中物体识别不稳健的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出PropVG框架，包括：(1)端到端的基于提议的框架，无缝集成前景物体提议生成与参考物体理解；(2)基于对比的参考评分(CRS)模块，在句子和词级别采用对比学习；(3)多粒度目标区分(MTD)模块，融合物体级和语义级信息。&lt;h4&gt;主要发现&lt;/h4&gt;PropVG在gRefCOCO (GREC/GRES)、Ref-ZOM、R-RefCOCO和RefCOCO (REC/RES)等多个基准测试上证明了其有效性，表明所提出的模块设计和框架能够提升视觉定位性能。&lt;h4&gt;结论&lt;/h4&gt;PropVG成功解决了现有视觉定位方法的局限性，通过整合前景物体提议生成与参考物体理解，以及利用对比学习和多粒度区分，显著提升了物体识别能力，特别是在复杂场景中。&lt;h4&gt;翻译&lt;/h4&gt;最近的视觉定位进展在很大程度上已经从基于提议的传统两阶段框架转移开来，因为它们效率低下且计算复杂度高，倾向于采用端到端的直接参考范式。然而，这些方法完全依赖于被引用的目标进行监督，忽视了潜在的前瞻性目标的好处。此外，现有方法通常未能融入多粒度区分，这对于复杂场景中稳健的物体识别至关重要。为解决这些局限性，我们提出了PropVG，这是一个端到端的基于提议的框架，据我们所知，它是第一个无缝集成前景物体提议生成与参考物体理解而无需额外检测器的框架。此外，我们引入了基于对比的参考评分(CRS)模块，该模块在句子和词级别采用对比学习，以增强理解和区分被引用物体的能力。另外，我们设计了一个多粒度目标区分(MTD)模块，融合物体级和语义级信息，以改善缺失目标的识别。在gRefCOCO (GREC/GRES)、Ref-ZOM、R-RefCOCO和RefCOCO (REC/RES)基准上的大量实验证明了PropVG的有效性。代码和模型可在https://github.com/Dmmm1997/PropVG获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in visual grounding have largely shifted away fromtraditional proposal-based two-stage frameworks due to their inefficiency andhigh computational complexity, favoring end-to-end direct reference paradigms.However, these methods rely exclusively on the referred target for supervision,overlooking the potential benefits of prominent prospective targets. Moreover,existing approaches often fail to incorporate multi-granularity discrimination,which is crucial for robust object identification in complex scenarios. Toaddress these limitations, we propose PropVG, an end-to-end proposal-basedframework that, to the best of our knowledge, is the first to seamlesslyintegrate foreground object proposal generation with referential objectcomprehension without requiring additional detectors. Furthermore, we introducea Contrastive-based Refer Scoring (CRS) module, which employs contrastivelearning at both sentence and word levels to enhance the capability inunderstanding and distinguishing referred objects. Additionally, we design aMulti-granularity Target Discrimination (MTD) module that fuses object- andsemantic-level information to improve the recognition of absent targets.Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes andmodels are available at https://github.com/Dmmm1997/PropVG.</description>
      <author>example@mail.com (Ming Dai, Wenxuan Cheng, Jiedong Zhuang, Jiang-jiang Liu, Hongshen Zhao, Zhenhua Feng, Wankou Yang)</author>
      <guid isPermaLink="false">2509.04833v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework</title>
      <link>http://arxiv.org/abs/2509.01910v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种将全球地理定位与概念瓶颈相结合的新框架，通过概念感知对齐模块提高了地理定位模型的准确性和解释性，首次将解释性引入地理定位领域。&lt;h4&gt;背景&lt;/h4&gt;全球地理定位涉及确定全球范围内拍摄图像的精确地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管GeoCLIP等地理定位模型有所进步，但这些模型的解释性仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决当前基于概念的解释性方法无法有效与地理定位图像-位置对齐目标保持一致的问题，提出一个将全球地理定位与概念瓶颈相结合的新框架。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种概念感知对齐模块，将图像和位置嵌入共同投影到共享的地理概念库上（如热带气候、山脉、大教堂等），并最小化概念级损失，增强概念特定子空间中的对齐，从而实现强大的解释性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在地理定位准确性上超越了GeoCLIP，在多样化的地理空间预测任务中提高了性能，并揭示了地理决策过程更丰富的语义洞察。&lt;h4&gt;结论&lt;/h4&gt;这是首个将解释性引入地理定位的工作，通过概念感知对齐模块实现了更好的性能和解释性。&lt;h4&gt;翻译&lt;/h4&gt;全球地理定位涉及确定全球范围内拍摄图像的精确地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管像GeoCLIP这样的地理定位模型有所进步，但这些模型的解释性仍未得到充分探索。当前基于概念的解释性方法无法有效地与地理定位图像-位置对齐目标保持一致，导致次优的解释性和性能。为解决这一差距，我们提出了一种将全球地理定位与概念瓶颈相结合的新框架。我们的方法插入了一个概念感知对齐模块，将图像和位置嵌入共同投影到共享的地理概念库（如热带气候、山脉、大教堂）上，并最小化概念级损失，增强概念特定子空间中的对齐，实现强大的解释性。据我们所知，这是首个将解释性引入地理定位的工作。大量实验证明，我们的方法在地理定位准确性上超越了GeoCLIP，并在多样化的地理空间预测任务中提高了性能，揭示了地理决策过程更丰富的语义洞察。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Worldwide geo-localization involves determining the exact geographic locationof images captured globally, typically guided by geographic cues such asclimate, landmarks, and architectural styles. Despite advancements ingeo-localization models like GeoCLIP, which leverages images and locationalignment via contrastive learning for accurate predictions, theinterpretability of these models remains insufficiently explored. Currentconcept-based interpretability methods fail to align effectively withGeo-alignment image-location embedding objectives, resulting in suboptimalinterpretability and performance. To address this gap, we propose a novelframework integrating global geo-localization with concept bottlenecks. Ourmethod inserts a Concept-Aware Alignment Module that jointly projects image andlocation embeddings onto a shared bank of geographic concepts (e.g., tropicalclimate, mountain, cathedral) and minimizes a concept-level loss, enhancingalignment in a concept-specific subspace and enabling robust interpretability.To our knowledge, this is the first work to introduce interpretability intogeo-localization. Extensive experiments demonstrate that our approach surpassesGeoCLIP in geo-localization accuracy and boosts performance across diversegeospatial prediction tasks, revealing richer semantic insights into geographicdecision-making processes.</description>
      <author>example@mail.com (Furong Jia, Lanxin Liu, Ce Hou, Fan Zhang, Xinyan Liu, Yu Liu)</author>
      <guid isPermaLink="false">2509.01910v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>An Emotion Recognition Framework via Cross-modal Alignment of EEG and Eye Movement Data</title>
      <link>http://arxiv.org/abs/2509.04938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于跨模态注意力机制混合架构的情感识别框架，实现了脑电图和眼动数据的精确多模态对齐，在SEED-IV数据集上达到90.62%的准确率。&lt;h4&gt;背景&lt;/h4&gt;情感识别对于情感计算和行为预测应用至关重要，但依赖单一模态数据的传统系统往往无法捕捉情感状态的复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一个情感识别框架，解决单一模态数据的局限性，实现脑电图(EEG)和眼动数据的精确多模态对齐。&lt;h4&gt;方法&lt;/h4&gt;基于跨模态注意力机制的混合架构，实现EEG和眼动数据的多模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在SEED-IV数据集上的实验表明，该方法达到了90.62%的准确率。&lt;h4&gt;结论&lt;/h4&gt;这项工作为在情感识别中利用多模态数据提供了有前景的基础。&lt;h4&gt;翻译&lt;/h4&gt;情感识别对于情感计算和行为预测应用至关重要，但依赖单一模态数据的传统系统往往无法捕捉情感状态的复杂性。为解决这一局限，我们提出了一种情感识别框架，通过基于跨模态注意力机制的混合架构，实现了脑电图(EEG)和眼动数据的精确多模态对齐。在SEED-IV数据集上的实验表明，我们的方法达到了90.62%的准确率。这项工作为在情感识别中利用多模态数据提供了有前景的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emotion recognition is essential for applications in affective computing andbehavioral prediction, but conventional systems relying on single-modality dataoften fail to capture the complexity of affective states. To address thislimitation, we propose an emotion recognition framework that achieves accuratemultimodal alignment of Electroencephalogram (EEG) and eye movement datathrough a hybrid architecture based on cross-modal attention mechanism.Experiments on the SEED-IV dataset demonstrate that our method achieve 90.62%accuracy. This work provides a promising foundation for leveraging multimodaldata in emotion recognition</description>
      <author>example@mail.com (Jianlu Wang, Yanan Wang, Tong Liu)</author>
      <guid isPermaLink="false">2509.04938v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Domain Adaptation for Different Sensor Configurations in 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.04711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了3D物体检测中不同传感器配置之间的领域适应问题，提出了两种技术：下游微调和部分层微调，实验证明联合使用这两种技术优于简单联合训练。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶的最新进展凸显了精确3D物体检测的重要性，LiDAR因其在不同能见度条件下的鲁棒性而发挥核心作用。然而，不同车辆平台采用不同传感器配置，导致模型从一个配置应用到另一个配置时性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决3D物体检测中不同传感器配置之间的领域适应问题，以提高模型在多样化车辆平台上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出两种技术：1) 下游微调（多数据集训练后进行数据集特定的微调）；2) 部分层微调（仅更新部分层以改善跨配置泛化能力）。使用在相同地理区域收集的、具有多种传感器配置的配对数据集进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;联合使用下游微调和部分层微调的训练方法，一致性地优于每种配置的简单联合训练，显著提高了模型在不同传感器配置间的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;研究结果为适应3D物体检测模型到多样化车辆平台提供了实用且可扩展的解决方案，有效解决了不同传感器配置间的领域差距问题。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶的最新进展凸显了精确3D物体检测的重要性，LiDAR由于其在不同能见度条件下的鲁棒性而发挥核心作用。然而，不同车辆平台通常采用不同的传感器配置，导致在一个配置上训练的模型应用于另一个配置时，由于点云分布的变化，性能会下降。先前关于3D物体检测的多数据集训练和领域适应工作主要解决了单一LiDAR内的环境领域差距和密度变化；相比之下，不同传感器配置的领域差距在很大程度上尚未探索。在这项工作中，我们解决了3D物体检测中不同传感器配置之间的领域适应问题。我们提出了两种技术：下游微调（多数据集训练后进行数据集特定的微调）和部分层微调（仅更新部分层以改善跨配置泛化能力）。使用在相同地理区域收集的、具有多种传感器配置的配对数据集，我们证明联合使用下游微调和部分层微调的训练一致性地优于每种配置的简单联合训练。我们的研究结果为适应3D物体检测模型到多样化车辆平台提供了实用且可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决不同传感器配置下的3D目标检测领域适应问题。当在一个传感器配置（如RoboTaxi）上训练的模型应用到另一个配置（如RoboBus）时，由于点云分布变化导致性能显著下降。这个问题很重要，因为自动驾驶平台多样化（汽车、公交车、卡车等），每种平台使用不同传感器配置，而现有研究主要关注环境变化，忽略了传感器配置差异导致的领域差异，影响了实际部署中针对特定平台的高性能模型开发。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到不同传感器配置导致点云分布差异，进而影响3D检测性能。发现现有领域适应研究主要关注环境变化，而忽略了传感器配置差异。认识到多数据集训练虽提高泛化能力，但会导致特定配置性能下降。设计方法时借鉴了无监督领域适应(UDA)、半监督领域适应(SSDA)和多数据集训练(MDT)的思想，但针对传感器配置差异进行了调整。还受到大型语言模型微调策略的启发，提出了下游微调(Downstream Fine-tuning)和部分层微调(Partial Layer Fine-tuning)的创新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用多阶段训练策略，先通过联合训练学习跨配置的通用特征，再针对每个特定配置进行专门优化，并在微调过程中只更新对传感器配置变化敏感的层。整体流程：1)构建包含RoboTaxi和RoboBus不同配置的数据集；2)在所有配置数据上进行联合训练学习通用特征；3)针对每个配置进行下游微调；4)在微调中应用部分层微调策略，保持编码器和头部固定，更新主干和颈部层；5)在测试集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次针对传感器配置差异引起的领域差异进行研究；2)构建统一标注格式的多传感器配置数据集；3)提出下游微调策略，解决多数据集训练导致的性能下降；4)提出部分层微调策略，只更新敏感层提高效率；5)提供全面的消融研究和无监督领域适应实验。相比之前工作，不同之处在于：专注于传感器配置差异而非环境变化；解决了多数据集训练的性能下降问题；提出更高效的部分层微调；提供专门的数据集和评估基准；证明传感器配置差异比其他领域差距更适合通过微调适应。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过结合下游微调和部分层微调的训练策略，有效解决了不同LiDAR传感器配置间的领域适应问题，使3D目标检测模型能够更高效地适应多样化的自动驾驶平台。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in autonomous driving have underscored the importance ofaccurate 3D object detection, with LiDAR playing a central role due to itsrobustness under diverse visibility conditions. However, different vehicleplatforms often deploy distinct sensor configurations, causing performancedegradation when models trained on one configuration are applied to anotherbecause of shifts in the point cloud distribution. Prior work on multi-datasettraining and domain adaptation for 3D object detection has largely addressedenvironmental domain gaps and density variation within a single LiDAR; incontrast, the domain gap for different sensor configurations remains largelyunexplored. In this work, we address domain adaptation across different sensorconfigurations in 3D object detection. We propose two techniques: DownstreamFine-tuning (dataset-specific fine-tuning after multi-dataset training) andPartial Layer Fine-tuning (updating only a subset of layers to improvecross-configuration generalization). Using paired datasets collected in thesame geographic region with multiple sensor configurations, we show that jointtraining with Downstream Fine-tuning and Partial Layer Fine-tuning consistentlyoutperforms naive joint training for each configuration. Our findings provide apractical and scalable solution for adapting 3D object detection models to thediverse vehicle platforms.</description>
      <author>example@mail.com (Satoshi Tanaka, Kok Seang Tan, Isamu Yamashita)</author>
      <guid isPermaLink="false">2509.04711v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection</title>
      <link>http://arxiv.org/abs/2507.23567v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了3D-MOOD，第一个端到端的3D单目开放集目标检测器，解决了现实应用中新环境和新目标类别的挑战，在多个数据集上取得了最先进结果。&lt;h4&gt;背景&lt;/h4&gt;单目3D目标检测在机器人和AR/VR等领域具有重要价值，但现有方法局限于封闭集设置，即训练集和测试集包含相同场景和目标类别，无法应对现实世界中的新环境和新类别挑战。&lt;h4&gt;目的&lt;/h4&gt;解决开放环境下的单目3D目标检测问题，提出第一个端到端的3D单目开放集目标检测器（3D-MOOD）。&lt;h4&gt;方法&lt;/h4&gt;1) 设计3D边界框头部将开放集2D检测提升到3D空间；2) 实现2D和3D任务的端到端联合训练；3) 使用几何先验条件化目标查询，提高跨场景3D估计泛化能力；4) 设计规范图像空间实现更高效的跨数据集训练。&lt;h4&gt;主要发现&lt;/h4&gt;在封闭集设置（Omni3D）和开放集设置（从Omni3D到Argoverse 2、ScanNet）上评估3D-MOOD，均取得新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;3D-MOOD成功解决了开放环境下的单目3D目标检测问题，通过创新方法设计实现了端到端训练，并在多个数据集上取得突破性成果，代码和模型已公开。&lt;h4&gt;翻译&lt;/h4&gt;单目3D目标检测对于机器人和AR/VR等应用具有重要价值。现有方法局限于封闭集设置，其中训练集和测试集包含相同的场景和/或目标类别。然而，现实应用常常引入新环境和新的目标类别，对这些方法构成了挑战。在本文中，我们解决了开放环境下的单目3D目标检测问题，并引入了第一个端到端的3D单目开放集目标检测器（3D-MOOD）。我们通过设计的3D边界框头部将开放集2D检测提升到3D空间，使2D和3D任务能够进行端到端的联合训练，从而获得更好的整体性能。我们使用几何先验条件化目标查询，克服了跨不同场景的3D估计泛化问题。为了进一步提高性能，我们设计了规范图像空间以实现更高效的跨数据集训练。我们在封闭集设置（Omni3D）和开放集设置（从Omni3D到Argoverse 2、ScanNet）上评估了3D-MOOD，并取得了新的最先进结果。代码和模型可在royyang0714.github.io/3D-MOOD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目开放集3D物体检测问题，即让模型能够识别并定位训练时未见过的物体类别和在未见过的场景中的物体。这个问题很重要，因为现实世界的应用（如机器人、AR/VR）经常需要面对新环境和未知物体，而现有方法受限于封闭集设计，只能检测训练时见过的物体类别，无法适应真实世界的多样性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了开放集单目3D物体检测的两个主要障碍：跨模态学习困难和单目深度估计泛化能力差。他们借鉴了G-DINO作为2D开放集检测器的基础架构，利用了通用单目深度估计方法（如UniDepth）的泛化能力，并参考了Cube R-CNN的虚拟深度概念。作者通过'提升'机制将2D检测转换为3D检测，并设计了几何感知的3D查询生成和规范图像空间来解决跨场景泛化问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'提升'机制将开放集2D检测转换为3D检测，同时利用几何先验增强模型泛化能力。整体流程：1)接收单目图像和语言提示；2)提取图像和文本特征；3)进行早期视觉-语言特征融合；4)生成检测查询；5)通过跨模态解码结合多模态信息；6)进行2D检测；7)使用3D边界框头和几何感知查询生成将2D检测结果'提升'为3D检测；8)输出包含3D位置、尺寸和方向的检测结果；9)通过端到端训练优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个端到端开放集单目3D物体检测器；2)2D到3D的提升机制；3)几何感知的3D查询生成；4)规范图像空间设计；5)辅助度量深度估计。相比之前工作：不同于封闭集方法（如Cube R-CNN）只能检测已知类别，3D-MOOD能检测未知类别；不同于OVM3D-Det的伪GT方法，3D-MOOD是端到端训练的；不同于传统方法使用类别先验和仅估计偏航角，3D-MOOD直接预测尺寸并使用6D方向参数化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3D-MOOD首次实现了端到端的开放集单目3D物体检测，通过创新的2D到3D提升机制和几何感知查询生成，使模型能够识别并定位未见过的物体类别，在封闭集和开放集设置下均达到最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D object detection is valuable for various applications such asrobotics and AR/VR. Existing methods are confined to closed-set settings, wherethe training and testing sets consist of the same scenes and/or objectcategories. However, real-world applications often introduce new environmentsand novel object categories, posing a challenge to these methods. In thispaper, we address monocular 3D object detection in an open-set setting andintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).We propose to lift the open-set 2D detection into 3D space through our designed3D bounding box head, enabling end-to-end joint training for both 2D and 3Dtasks to yield better overall performance. We condition the object queries withgeometry prior and overcome the generalization for 3D estimation across diversescenes. To further improve performance, we design the canonical image space formore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-setsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), andachieve new state-of-the-art results. Code and models are available atroyyang0714.github.io/3D-MOOD.</description>
      <author>example@mail.com (Yung-Hsu Yang, Luigi Piccinelli, Mattia Segu, Siyuan Li, Rui Huang, Yuqian Fu, Marc Pollefeys, Hermann Blum, Zuria Bauer)</author>
      <guid isPermaLink="false">2507.23567v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition</title>
      <link>http://arxiv.org/abs/2509.05019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20pages, 9 figures and 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了迁移学习与移动端卷积神经网络在阿拉伯手写字符识别中的集成应用，通过评估三种迁移学习策略和四种轻量级网络模型，发现MobileNet表现最佳，完全微调策略效果最好，IFHCDB数据集上达到99%的准确率，为资源高效的阿拉伯手写字符识别提供了新思路。&lt;h4&gt;背景&lt;/h4&gt;阿拉伯手写字符识别面临计算资源需求大和数据集稀缺的挑战，需要更高效的识别方法。&lt;h4&gt;目的&lt;/h4&gt;评估迁移学习与轻量级移动端卷积神经网络的结合效果，提高阿拉伯手写字符识别的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;评估三种迁移学习策略（完全微调、部分微调和从头训练）使用四种轻量级MbNets（MobileNet、SqueezeNet、MnasNet和ShuffleNet），在三个基准数据集（AHCD、HIJJA和IFHCDB）上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;MobileNet是表现最好的模型，在准确性、鲁棒性和效率方面都表现优异；ShuffleNet在泛化能力方面表现突出；IFHCDB数据集上使用MnasNet在完全微调下达到99%的准确率；AHCD数据集上使用ShuffleNet实现97%的准确率；HIJJA数据集挑战大，达到92%的峰值准确率；完全微调在所有指标中表现最佳，平衡了准确性和收敛速度；部分微调在各项指标中表现不佳。&lt;h4&gt;结论&lt;/h4&gt;迁移学习与MbNets的结合为资源高效的阿拉伯手写字符识别提供了潜力，为进一步优化和更广泛的应用铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了迁移学习与移动端卷积神经网络的集成，以增强阿拉伯手写字符识别。针对大量计算需求和数据集稀缺等挑战，本研究评估了三种迁移学习策略——完全微调、部分微调和从头训练——使用了四种轻量级MbNets：MobileNet、SqueezeNet、MnasNet和ShuffleNet。实验在三个基准数据集上进行：AHCD、HIJJA和IFHCDB。MobileNet成为表现最佳的模型，在准确性、鲁棒性和效率方面持续取得优异表现，而ShuffleNet在泛化能力方面表现出色，特别是在完全微调的情况下。IFHCDB数据集获得了最佳结果，使用MnasNet在完全微调下达到99%的准确率，突显了其在鲁棒字符识别中的适用性。AHCD数据集使用ShuffleNet实现了97%的竞争性准确率，而HIJJA由于其变化性大带来了显著挑战，使用ShuffleNet达到92%的峰值准确率。值得注意的是，完全微调展示了最佳的整体性能，平衡了准确性和收敛速度，而部分微调在各项指标中表现不佳。这些发现强调了结合迁移学习和MbNets进行资源高效阿拉伯手写字符识别的潜力，为进一步优化和更广泛的应用铺平了道路。未来工作将探索架构修改、深入数据集特征分析、数据增强和高级敏感性分析，以提高模型的鲁棒性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study explores the integration of transfer learning (TL) withmobile-enabled convolutional neural networks (MbNets) to enhance ArabicHandwritten Character Recognition (AHCR). Addressing challenges like extensivecomputational requirements and dataset scarcity, this research evaluates threeTL strategies--full fine-tuning, partial fine-tuning, and training fromscratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, andShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistentlyachieving superior accuracy, robustness, and efficiency, with ShuffleNetexcelling in generalization, particularly under full fine-tuning. The IFHCDBdataset yielded the highest results, with 99% accuracy using MnasNet under fullfine-tuning, highlighting its suitability for robust character recognition. TheAHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJAposed significant challenges due to its variability, achieving a peak accuracyof 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overallperformance, balancing accuracy and convergence speed, while partialfine-tuning underperformed across metrics. These findings underscore thepotential of combining TL and MbNets for resource-efficient AHCR, paving theway for further optimizations and broader applications. Future work willexplore architectural modifications, in-depth dataset feature analysis, dataaugmentation, and advanced sensitivity analysis to enhance model robustness andgeneralizability.</description>
      <author>example@mail.com (Mohsine El Khayati, Ayyad Maafiri, Yassine Himeur, Hamzah Ali Alkhazaleh, Shadi Atalla, Wathiq Mansoor)</author>
      <guid isPermaLink="false">2509.05019v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study</title>
      <link>http://arxiv.org/abs/2509.05004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures and 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了多种机器学习和深度学习模型在乳腺癌超声图像分类中的应用，发现ResNet-18表现最佳，支持AI诊断工具在临床实践中的应用。&lt;h4&gt;背景&lt;/h4&gt;乳腺癌是全球女性癌症相关死亡的主要原因。超声成像因其安全性和成本效益而被广泛使用，在早期检测中起着关键作用，特别是在致密乳腺组织患者中。&lt;h4&gt;目的&lt;/h4&gt;全面研究机器学习和深度学习技术在乳腺癌超声图像分类中的应用。&lt;h4&gt;方法&lt;/h4&gt;使用BUSI、BUS-BRA和BrEaST-Lesions USG等数据集，评估经典机器学习模型（SVM、KNN）和深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet）。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet-18达到最高的准确率（99.7%）和完美的恶性肿瘤敏感性；经典机器学习模型结合深度特征提取时也能达到有竞争力的性能；Grad-CAM可视化通过突出显示诊断相关的图像区域提高了模型透明度。&lt;h4&gt;结论&lt;/h4&gt;研究结果支持将基于人工智能的诊断工具整合到临床工作流程中，证明了部署高性能、可解释的超声乳腺癌检测系统的可行性。&lt;h4&gt;翻译&lt;/h4&gt;乳腺癌仍然是全球女性癌症相关死亡的主要原因。超声成像因其安全性和成本效益而被广泛使用，在早期检测中起着关键作用，特别是在致密乳腺组织患者中。本文全面研究了机器学习和深度学习技术在乳腺癌超声图像分类中的应用。使用BUSI、BUS-BRA和BrEaST-Lesions USG等数据集，我们评估了经典机器学习模型（SVM、KNN）和深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet）。实验结果表明，ResNet-18达到了最高的准确率（99.7%）和完美的恶性肿瘤敏感性。经典机器学习模型虽然被CNN超越，但当结合深度特征提取时也能达到有竞争力的性能。Grad-CAM可视化通过突出显示诊断相关的图像区域进一步提高了模型透明度。这些研究结果支持将基于人工智能的诊断工具整合到临床工作流程中，并证明了部署高性能、可解释的超声乳腺癌检测系统的可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast cancer remains a leading cause of cancer-related mortality among womenworldwide. Ultrasound imaging, widely used due to its safety andcost-effectiveness, plays a key role in early detection, especially in patientswith dense breast tissue. This paper presents a comprehensive study on theapplication of machine learning and deep learning techniques for breast cancerclassification using ultrasound images. Using datasets such as BUSI, BUS-BRA,and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,GoogLeNet). Experimental results show that ResNet-18 achieves the highestaccuracy (99.7%) and perfect sensitivity for malignant lesions. Classical MLmodels, though outperformed by CNNs, achieve competitive performance whenenhanced with deep feature extraction. Grad-CAM visualizations further improvemodel transparency by highlighting diagnostically relevant image regions. Thesefindings support the integration of AI-based diagnostic tools into clinicalworkflows and demonstrate the feasibility of deploying high-performing,interpretable systems for ultrasound-based breast cancer detection.</description>
      <author>example@mail.com (Mohammad Abbadi, Yassine Himeur, Shadi Atalla, Wathiq Mansoor)</author>
      <guid isPermaLink="false">2509.05004v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models</title>
      <link>http://arxiv.org/abs/2509.04889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  60 pages (30 main text, 30 appendix), 20 figures (5 in main text, 15  in appendix)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了计算机视觉模型在预测蜘蛛相关图像恐惧水平方面的应用。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉的进步为临床应用开辟了新途径，特别是在计算机暴露疗法中，可以根据患者反应动态调整视觉刺激。&lt;h4&gt;目的&lt;/h4&gt;研究预训练的计算机视觉模型是否能准确预测与蜘蛛相关图像的恐惧水平。&lt;h4&gt;方法&lt;/h4&gt;使用迁移学习调整三种不同的模型，从一个包含313张图像的标准数据集中预测人类的恐惧评分（0-100分制），并通过交叉验证进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;模型平均绝对误差在10.1到11.0之间；减少数据集规模会显著损害性能，但进一步增加数据量不会带来实质性收益；模型预测基于蜘蛛相关特征；远景和人工/绘制的蜘蛛与较高误差相关。&lt;h4&gt;结论&lt;/h4&gt;可解释计算机视觉模型在预测恐惧评分方面具有潜力，模型可解释性和足够的数据集规模对于开发有效的情感感知治疗技术至关重要。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉的进步为临床应用开辟了新途径，特别是在计算机暴露疗法中，可以根据患者反应动态调整视觉刺激。作为开发此类自适应系统的重要一步，我们研究了预训练的计算机视觉模型是否能准确预测与蜘蛛相关图像的恐惧水平。我们使用迁移学习调整了三种不同的模型，从一个包含313张图像的标准数据集中预测人类的恐惧评分（0-100分制）。模型通过交叉验证进行评估，平均绝对误差在10.1到11.0之间。我们的学习曲线分析显示，减少数据集规模会显著损害性能，但进一步增加数据量不会带来实质性收益。可解释性评估表明，模型的预测是基于蜘蛛相关特征的。分类错误分析进一步确定了与较高误差相关的视觉条件（例如，远景和人工/绘制的蜘蛛）。这些发现展示了可解释计算机视觉模型在预测恐惧评分方面的潜力，强调了模型可解释性和足够数据集规模对于开发有效的情感感知治疗技术的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in computer vision have opened new avenues for clinicalapplications, particularly in computerized exposure therapy where visualstimuli can be dynamically adjusted based on patient responses. As a criticalstep toward such adaptive systems, we investigated whether pretrained computervision models can accurately predict fear levels from spider-related images. Weadapted three diverse models using transfer learning to predict human fearratings (on a 0-100 scale) from a standardized dataset of 313 images. Themodels were evaluated using cross-validation, achieving an average meanabsolute error (MAE) between 10.1 and 11.0. Our learning curve analysisrevealed that reducing the dataset size significantly harmed performance,though further increases yielded no substantial gains. Explainabilityassessments showed the models' predictions were based on spider-relatedfeatures. A category-wise error analysis further identified visual conditionsassociated with higher errors (e.g., distant views and artificial/paintedspiders). These findings demonstrate the potential of explainable computervision models in predicting fear ratings, highlighting the importance of bothmodel explainability and a sufficient dataset size for developing effectiveemotion-aware therapeutic technologies.</description>
      <author>example@mail.com (Dominik Pegler, David Steyrl, Mengfan Zhang, Alexander Karner, Jozsef Arato, Frank Scharnowski, Filip Melinscak)</author>
      <guid isPermaLink="false">2509.04889v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</title>
      <link>http://arxiv.org/abs/2509.05198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for presentation at the 7th  International Conference on Pattern Recognition and Image Analysis (IPRIA  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了ModelNet-R（改进版ModelNet40数据集）和Point-SkipNet（轻量级基于图的神经网络），用于解决3D点云分类问题，强调了高质量数据集对模型效率的重要性。&lt;h4&gt;背景&lt;/h4&gt;3D点云分类对自动驾驶、机器人和增强现实等应用至关重要，但常用的ModelNet40数据集存在标签不一致、二维数据、尺寸不匹配和类别区分不足等局限性，限制了模型性能。&lt;h4&gt;目的&lt;/h4&gt;创建更可靠的基准数据集ModelNet-R以解决ModelNet40的局限性，并提出Point-SkipNet神经网络实现高分类精度同时减少计算开销。&lt;h4&gt;方法&lt;/h4&gt;通过改进ModelNet40创建ModelNet-R数据集，并设计Point-SkipNet神经网络，该网络利用高效采样、邻域分组和跳跃连接技术。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet-R上训练的模型表现出显著性能提升；Point-SkipNet在ModelNet-R上实现最先进准确性，同时参数数量大幅减少；数据集质量对优化3D点云分类模型效率至关重要。&lt;h4&gt;结论&lt;/h4&gt;高质量数据集和精心设计的网络架构能显著提高3D点云分类的性能和效率，为相关领域提供更可靠的基准和解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D点云分类对于自动驾驶、机器人和增强现实等应用至关重要。然而，常用的ModelNet40数据集存在标签不一致、二维数据、尺寸不匹配和类别区分不足等局限性，这些限制阻碍了模型性能。本文介绍了ModelNet-R，这是ModelNet40的精心改进版本，旨在解决这些问题并作为更可靠的基准。此外，本文提出了Point-SkipNet，一种轻量级基于图的神经网络，它利用高效采样、邻域分组和跳跃连接来实现高分类精度同时减少计算开销。大量实验证明，在ModelNet-R上训练的模型表现出显著的性能提升。值得注意的是，Point-SkipNet在ModelNet-R上实现了最先进的准确性，同时与当代模型相比参数数量大幅减少。这项研究强调了数据集质量在优化3D点云分类模型效率方面的关键作用。更多详情，请参阅代码：https://github.com/m-saeid/ModeNetR_PointSkipNet。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决两个问题：一是常用的ModelNet40数据集存在标签不一致、包含2D数据、尺寸不匹配和类别区分度不足等质量问题；二是现有的点云分类模型计算量大，不适合资源受限环境。这些问题很重要，因为数据集质量直接影响模型性能和评估可靠性，而计算效率限制模型在实际应用中的部署，而3D点云分类在自动驾驶、机器人和增强现实等领域有广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出3D点云分类领域的两个主要问题：数据集质量和模型效率。针对数据集问题，作者决定改进ModelNet数据集创建ModelNet-R；针对效率问题，设计了轻量级的Point-SkipNet。作者借鉴了PointNet和PointNet++的分层特征提取策略，DGCNN的动态图构建方法，以及FPS采样和ball query邻域分组等现有技术，但进行了改进以提高效率和准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想包括：1) 创建高质量的ModelNet-R数据集，通过修正标签、移除低质量样本和改进类别定义提高数据可靠性；2) 设计Point-SkipNet轻量级图神经网络，通过高效采样、邻域分组和跳跃连接实现高精度低计算开销。整体流程：ModelNet-R创建包括修正标签、移除2D数据、解决尺寸不匹配和改进类别区分；Point-SkipNet实现包括数据增强、采样分组、特征提取、跳跃连接、全局特征聚合和分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) ModelNet-R数据集解决原始数据集的质量问题；2) Point-SkipNet轻量级图神经网络架构；3) 实现高精度与低计算开销的平衡。不同之处：不同于以往仅扩展数据集，作者专注于改进现有数据集质量；Point-SkipNet结合高效采样和跳跃连接大幅减少计算量；在保持精度的同时参数数量远少于当代模型；强调了数据集质量对模型效率的重要性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建高质量的ModelNet-R数据集和设计轻量级的Point-SkipNet网络，显著提高了3D点云分类的准确性和计算效率，为资源受限环境中的应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The classification of 3D point clouds is crucial for applications such asautonomous driving, robotics, and augmented reality. However, the commonly usedModelNet40 dataset suffers from limitations such as inconsistent labeling, 2Ddata, size mismatches, and inadequate class differentiation, which hinder modelperformance. This paper introduces ModelNet-R, a meticulously refined versionof ModelNet40 designed to address these issues and serve as a more reliablebenchmark. Additionally, this paper proposes Point-SkipNet, a lightweightgraph-based neural network that leverages efficient sampling, neighborhoodgrouping, and skip connections to achieve high classification accuracy withreduced computational overhead. Extensive experiments demonstrate that modelstrained in ModelNet-R exhibit significant performance improvements. Notably,Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with asubstantially lower parameter count compared to contemporary models. Thisresearch highlights the crucial role of dataset quality in optimizing modelefficiency for 3D point cloud classification. For more details, see the codeat: https://github.com/m-saeid/ModeNetR_PointSkipNet.</description>
      <author>example@mail.com (Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari)</author>
      <guid isPermaLink="false">2509.05198v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement</title>
      <link>http://arxiv.org/abs/2509.04645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning (CoRL) 2025  (https://planning-from-point-clouds.github.io/)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SPOT的混合学习与规划方法，用于解决机器人操作中的长期规划问题，通过在点云空间中搜索变换序列，避免了传统方法中对连续状态和动作空间的离散化需求。&lt;h4&gt;背景&lt;/h4&gt;机器人操作的长期规划是一个具有挑战性的问题，需要推理一系列动作对物理3D场景的影响。传统任务规划方法虽然有效，但需要将连续的状态和动作空间离散化为对象、对象关系和动作的符号描述。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理高维连续动作空间的规划方法，避免对状态和动作空间进行离散化，同时保持规划的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出SPOT（Search over Point cloud Object Transformations）方法，通过搜索从初始场景点云到满足目标的点云的变换序列来进行规划。SPOT从在部分观测点云上运行的学习型建议器中采样候选动作，消除了离散化动作或对象关系的需要。&lt;h4&gt;主要发现&lt;/h4&gt;在多对象重排任务上评估SPOT，实验表明SPOT能够生成成功的计划，并且优于策略学习方法。消融实验强调了基于搜索的规划的重要性。&lt;h4&gt;结论&lt;/h4&gt;SPOT作为一种混合学习与规划的方法，有效解决了长期机器人操作规划问题，在模拟和真实环境中都表现出色，证明了结合学习模型与搜索规划的有效性。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作的长期规划是一个具有挑战性的问题，需要推理一系列动作对物理3D场景的影响。虽然传统的任务规划方法被证明对长期操作有效，但它们需要将连续的状态和动作空间离散化为对象、对象关系和动作的符号描述。相反，我们提出了一种混合学习和规划的方法，利用学习到的模型作为领域特定的先验知识，在高维连续动作空间中引导搜索。我们介绍了SPOT：点云对象变换搜索，它通过搜索从初始场景点云到满足目标的点云的变换序列来进行规划。SPOT从在部分观测点云上运行的学习型建议器中采样候选动作，消除了离散化动作或对象关系的需要。我们在多对象重排任务上评估了SPOT，报告了在模拟和真实环境中的任务规划成功率和任务执行成功率。我们的实验表明，SPOT能够生成成功的计划，并且优于一种策略学习方法。我们还进行了消融实验，强调了基于搜索的规划的重要性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作中的长期规划问题，特别是在多物体重新排列任务中，机器人需要推理一系列动作对3D物理场景的影响，将物体从初始配置移动到满足目标条件的配置。这个问题在现实中很重要，因为它涉及机器人如何处理复杂的物理操作任务，如餐桌收拾、物体装箱等；在研究中也很重要，因为它突破了传统规划方法需要离散化连续状态和动作空间的限制，使机器人能够更自然地处理现实世界中的连续操作问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统符号规划方法的局限性，即需要离散化连续状态和动作空间，以及对场景的完全知识需求。然后提出了一种混合学习和规划的方法，利用学习的模型作为领域特定先验来指导搜索。作者借鉴了A*搜索算法，但将其应用于连续状态和动作空间；利用了学习的物体建议器和放置建议器来指导搜索；使用了模型偏差估计器来避免不太可能的转换；并参考了TAXPose-D方法来实现相对放置任务。整体设计考虑了直接从点云观测进行规划，而不需要符号表示或潜在空间表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接从点云观测进行规划，通过搜索物体变换序列来找到满足目标条件的点云配置，利用学习的模型作为领域特定先验来指导在高维连续动作空间中的搜索。整体流程包括：1)接收初始场景的部分观测点云、物体名称和目标函数；2)将输入点云分割为有限个物体；3)使用A*搜索算法在物体变换空间中搜索，通过学习的物体建议器和放置建议器采样候选动作；4)使用成本函数和启发式函数评估和引导搜索；5)生成物体变换序列的计划；6)机器人执行计划，使用抓取检测器确定抓取姿态；7)利用模型偏差估计器预测计划动作与实际执行之间的偏差。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出新的规划范式，不需要离散化，通过A*搜索原始3D观测的3D变换进行规划；2)学习物体建议器和放置建议器从视频演示中学习；3)结合搜索基础规划和学习，利用学习到的领域特定先验；4)集成模型偏差估计器引导搜索。相比之前的工作，不同之处在于：不需要离散化连续状态和动作空间；不需要符号场景描述；不在潜在空间中规划；不假设访问技能库或谓词库；不假设离散的物体关系集合；专注于多物体重新排列的规划而非单物体姿态重新配置。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPOT是一种混合学习和规划方法，它通过直接从点云观测进行搜索，利用学习的领域特定先验来指导连续动作空间中的多物体重新排列规划，避免了传统方法中对状态和动作空间离散化的需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-horizon planning for robot manipulation is a challenging problem thatrequires reasoning about the effects of a sequence of actions on a physical 3Dscene. While traditional task planning methods are shown to be effective forlong-horizon manipulation, they require discretizing the continuous state andaction space into symbolic descriptions of objects, object relationships, andactions. Instead, we propose a hybrid learning-and-planning approach thatleverages learned models as domain-specific priors to guide search inhigh-dimensional continuous action spaces. We introduce SPOT: Search over Pointcloud Object Transformations, which plans by searching for a sequence oftransformations from an initial scene point cloud to a goal-satisfying pointcloud. SPOT samples candidate actions from learned suggesters that operate onpartially observed point clouds, eliminating the need to discretize actions orobject relationships. We evaluate SPOT on multi-object rearrangement tasks,reporting task planning success and task execution success in both simulationand real-world environments. Our experiments show that SPOT generatessuccessful plans and outperforms a policy-learning approach. We also performablations that highlight the importance of search-based planning.</description>
      <author>example@mail.com (Kallol Saha, Amber Li, Angela Rodriguez-Izquierdo, Lifan Yu, Ben Eisner, Maxim Likhachev, David Held)</author>
      <guid isPermaLink="false">2509.04645v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    </channel>
</rss>