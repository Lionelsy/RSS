<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 25 Aug 2025 15:28:37 +0800</lastBuildDate>
    <item>
      <title>HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2508.16433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HAMSt3R是一种基于MASt3R扩展的3D重建方法，能够从稀疏未校准的多视角图像中同时重建人体和场景，解决了现有方法在以人为中心场景中的局限性。&lt;h4&gt;背景&lt;/h4&gt;从少量未校准图像恢复场景3D几何是计算机视觉中的长期挑战。现有学习方法如DUSt3R和MASt3R主要针对室外静态场景，难以处理以人为中心的场景。&lt;h4&gt;目的&lt;/h4&gt;开发HAMSt3R方法，用于从稀疏未校准的多视角图像进行人体和场景的联合3D重建，弥合3D视觉中人体理解和场景理解之间的差距。&lt;h4&gt;方法&lt;/h4&gt;利用DUNE图像编码器（通过蒸馏MASt3R和多-HMR模型获得）理解场景和人体；添加额外网络头分割人体、通过Dense估计密集对应关系、预测以人为中心环境的深度；利用不同网络头的输出生成包含人体语义信息的密集点云图；采用完全前馈方法，无需复杂优化流程。&lt;h4&gt;主要发现&lt;/h4&gt;在EgoHumans和EgoExo4D基准测试上验证了模型性能；证明了方法在传统多视角立体和多视角姿态回归任务上的泛化能力；能够有效重建人体同时保持通用3D重建任务的强大性能。&lt;h4&gt;结论&lt;/h4&gt;HAMSt3R成功实现了人体和场景的联合3D重建，填补了3D视觉中人体理解和场景理解之间的空白，且方法高效适合实际应用。&lt;h4&gt;翻译&lt;/h4&gt;从一组稀疏的未校准图像中恢复场景的3D几何是计算机视觉中的一个长期问题。虽然最近基于学习的方法如DUSt3R和MASt3R通过直接预测密集场景几何展示了令人印象深刻的结果，但它们主要针对室外静态环境进行训练，难以处理以人为中心的场景。在这项工作中，我们引入了HAMSt3R，它是MASt3R的扩展，用于从稀疏、未校准的多视角图像进行人体和场景的联合3D重建。首先，我们利用DUNE，一种通过蒸馏MASt3R编码器和最先进的人体网格恢复(HMR)模型multi-HMR的编码器等获得的高效图像编码器，以更好地理解场景几何和人体。我们的方法然后添加额外的网络头来分割人体、通过Dense估计密集对应关系，并在以人为中心的环境中预测深度，实现更全面的3D重建。通过利用我们不同网络头的输出，HAMSt3R生成了一个包含人体语义信息的密集点云图。与依赖复杂优化流程的现有方法不同，我们的方法是完全前馈且高效的，使其适合实际应用。我们在EgoHumans和EgoExo4D两个包含多样化以人为中心场景的具有挑战性的基准上评估了我们的模型。此外，我们验证了它在传统多视角立体和多视角姿态回归任务上的泛化能力。我们的结果表明，我们的方法能够在保持通用3D重建任务强大性能的同时有效重建人体，弥合了3D视觉中人体理解和场景理解之间的差距。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从稀疏未校准的多视角图像中同时重建人类和周围环境的3D场景问题。这个问题很重要，因为人类是许多场景的中心元素，完整的3D场景理解需要同时处理环境和人体，而现有方法要么专注于静态场景重建，要么使用复杂优化流程处理人体场景，限制了实际应用的可扩展性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法（如MASt3R和DUSt3R）在人体场景上的局限性，然后通过扩展MASt3R架构来解决这个问题。他们借鉴了DUNE编码器（融合了MASt3R场景理解和Multi-HMR人体理解能力），并引入了新的网络分支（实例分割头、DensePose头和二值掩码头）来专门处理人体信息。训练策略上混合了原始MASt3R数据和专门的人体中心数据集，确保模型在一般场景和人体场景上都能表现良好。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过统一的、前馈的神经网络架构在一次前向传播中同时完成3D场景重建、人体实例分割和跨视角人体重建，生成包含丰富人体语义信息的密集3D点图。整体流程：1) 输入两张图像；2) 使用共享ViT编码器提取特征；3) 通过带交叉注意力的双ViT解码器处理特征；4) 多头预测（原始重建头、实例分割头、DensePose头和二值掩码头）；5) 结合各头输出生成语义丰富的3D点图；6) 对于多视角图像，独立处理每对图像并聚合结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个完全前馈的统一架构，无需复杂优化流程；2) 引入人体感知能力，同时保持一般场景重建能力；3) 使用DUNE编码器融合场景和人体理解；4) 生成包含人体语义信息的3D点表示；5) 高效处理任意数量的图像视图。相比之前工作，不同之处在于：与MASt3R/DUSt3R相比增加了人体理解能力；与JOSH/HSfM相比避免了多阶段优化；与单目人体重建方法相比提供了场景上下文；与传统优化方法相比更加高效且对超参数不敏感。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HAMSt3R首次实现了通过单一前馈神经网络从稀疏未校准图像中同时重建人体和周围环境的3D场景，提供了包含丰富人体语义信息的密集3D点表示，并在保持一般场景重建能力的同时显著提升了人体重建的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering the 3D geometry of a scene from a sparse set of uncalibratedimages is a long-standing problem in computer vision. While recentlearning-based approaches such as DUSt3R and MASt3R have demonstratedimpressive results by directly predicting dense scene geometry, they areprimarily trained on outdoor scenes with static environments and struggle tohandle human-centric scenarios. In this work, we introduce HAMSt3R, anextension of MASt3R for joint human and scene 3D reconstruction from sparse,uncalibrated multi-view images. First, we exploit DUNE, a strong image encoderobtained by distilling, among others, the encoders from MASt3R and from astate-of-the-art Human Mesh Recovery (HMR) model, multi-HMR, for a betterunderstanding of scene geometry and human bodies. Our method then incorporatesadditional network heads to segment people, estimate dense correspondences viaDensePose, and predict depth in human-centric environments, enabling a morecomprehensive 3D reconstruction. By leveraging the outputs of our differentheads, HAMSt3R produces a dense point map enriched with human semanticinformation in 3D. Unlike existing methods that rely on complex optimizationpipelines, our approach is fully feed-forward and efficient, making it suitablefor real-world applications. We evaluate our model on EgoHumans and EgoExo4D,two challenging benchmarks con taining diverse human-centric scenarios.Additionally, we validate its generalization to traditional multi-view stereoand multi-view pose regression tasks. Our results demonstrate that our methodcan reconstruct humans effectively while preserving strong performance ingeneral 3D reconstruction tasks, bridging the gap between human and sceneunderstanding in 3D vision.</description>
      <author>example@mail.com (Sara Rojas, Matthieu Armando, Bernard Ghamen, Philippe Weinzaepfel, Vincent Leroy, Gregory Rogez)</author>
      <guid isPermaLink="false">2508.16433v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
  <item>
      <title>ASCMamba: Multimodal Time-Frequency Mamba for Acoustic Scene Classification</title>
      <link>http://arxiv.org/abs/2508.15632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ASCMamba的多模态网络，用于整合音频和文本信息实现声景分类，在APSIPA ASC 2025 Grand Challenge中取得了最佳成绩，比基线提高了6.2%。&lt;h4&gt;背景&lt;/h4&gt;声景分类(ASC)是计算听觉领域的基础问题，旨在基于独特的声学特征对环境进行分类。APSIPA ASC 2025 Grand Challenge引入了一个多模态ASC任务，与传统仅依赖音频输入的ASC系统不同，提供了额外的文本信息作为输入。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够整合音频和文本信息的多模态系统，实现细粒度的声景理解和有效的多模态ASC。&lt;h4&gt;方法&lt;/h4&gt;提出名为ASCMamba的多模态网络，采用DenseEncoder从频谱图中提取分层频谱特征，使用双路径Mamba块基于Mamba状态空间模型捕获长程时间和频率依赖关系，并引入两步伪标记机制生成更可靠的伪标记。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的ASCMamba系统在APSIPA ASC 2025 Grand Challenge中优于所有参赛团队，比基线提高了6.2%的性能。&lt;h4&gt;结论&lt;/h4&gt;多模态方法结合音频和文本信息能够有效提升声景分类性能，ASCMamba系统展示了在细粒度声景理解方面的优越性。&lt;h4&gt;翻译&lt;/h4&gt;声景分类(ASC)是计算听觉领域的基础问题，旨在根据独特的声学特征对环境进行分类。在APSIPA ASC 2025 Grand Challenge的ASC任务中，组织者引入了一个多模态ASC任务。与传统仅依赖音频输入的ASC系统不同，这一挑战提供了额外的文本信息作为输入，包括音频录制位置和录制时间。在本文中，我们提出了我们在APSIPA ASC 2025 Grand Challenge ASC任务中的系统。具体而言，我们提出了一个名为ASCMamba的多模态网络，该网络整合了音频和文本信息，用于细粒度的声景理解和有效的多模态ASC。所提出的ASCMamba采用DenseEncoder从频谱图中提取分层频谱特征，随后是双路径Mamba块，使用基于Mamba的状态空间模型捕获长程时间和频率依赖关系。此外，我们提出了一种两步伪标记机制来生成更可靠的伪标记。结果表明，所提出的系统优于所有参赛团队，比基线提高了6.2%。代码、模型和预训练检查点可在https://github.com/S-Orion/ASCMamba.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Acoustic Scene Classification (ASC) is a fundamental problem in computationalaudition, which seeks to classify environments based on the distinctiveacoustic features. In the ASC task of the APSIPA ASC 2025 Grand Challenge, theorganizers introduce a multimodal ASC task. Unlike traditional ASC systems thatrely solely on audio inputs, this challenge provides additional textualinformation as inputs, including the location where the audio is recorded andthe time of recording. In this paper, we present our proposed system for theASC task in the APSIPA ASC 2025 Grand Challenge. Specifically, we propose amultimodal network, \textbf{ASCMamba}, which integrates audio and textualinformation for fine-grained acoustic scene understanding and effectivemultimodal ASC. The proposed ASCMamba employs a DenseEncoder to extracthierarchical spectral features from spectrograms, followed by a dual-path Mambablocks that capture long-range temporal and frequency dependencies usingMamba-based state space models. In addition, we present a two-steppseudo-labeling mechanism to generate more reliable pseudo-labels. Results showthat the proposed system outperforms all the participating teams and achieves a6.2% improvement over the baseline. Code, model and pre-trained checkpoints areavailable at https://github.com/S-Orion/ASCMamba.git.</description>
      <author>example@mail.com (Bochao Sun, Dong Wang, Han Yin)</author>
      <guid isPermaLink="false">2508.15632v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Hybrelighter: Combining Deep Anisotropic Diffusion and Scene Reconstruction for On-device Real-time Relighting in Mixed Reality</title>
      <link>http://arxiv.org/abs/2508.14930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新颖的混合现实场景重新照明方法，整合图像分割、各向异性扩散光照传播和滤波技术，能够在边缘设备上实时实现视觉上吸引人且准确的重新照明效果，速度高达100fps。&lt;h4&gt;背景&lt;/h4&gt;混合现实场景重新照明允许虚拟光照条件变化与物理对象真实交互，产生逼真的照明和阴影效果。现有技术存在局限性：基于深度学习的方法超出当前MR设备的实时性能能力；场景理解方法因扫描限制导致不准确；简单2D图像滤波方法无法表示复杂几何形状和阴影。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在边缘设备上实时实现视觉上吸引人且准确的重新照明效果的新方法，解决现有技术的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出整合图像分割、通过各向异性扩散进行光照传播、基本场景理解和滤波技术计算简便性的新颖方法，纠正设备端扫描的不准确性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在边缘设备上实时提供视觉上吸引人且准确的重新照明效果，实现高达100fps的速度，与行业标准进行了直接比较，并在房地产示例中展示了实际应用效果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功解决了现有重新照明技术的局限性，实现了高质量、实时性能的重新照明效果。&lt;h4&gt;翻译&lt;/h4&gt;混合现实场景重新照明，其中虚拟光照条件的变化与物理对象真实交互，产生逼真的照明和阴影效果，可用于多种应用。房地产中的一个应用可以是可视化一天中不同时间的房间并放置虚拟灯具。现有的基于深度学习的重新照明技术通常超出当前MR设备的实时性能能力。另一方面，场景理解方法（如设备端场景重建）由于扫描限制往往产生不准确结果，进而影响重新照明质量。最后，简单的基于2D图像滤波的方法无法表示复杂几何形状和阴影。我们引入了一种新颖方法，整合图像分割、通过各向异性扩散进行光照传播、基本场景理解和滤波技术的计算简便性。我们的方法纠正了设备端扫描的不准确性，在边缘设备上实时提供视觉上吸引人且准确的重新照明效果，实现高达100fps的速度。我们展示了我们方法与行业标准的直接比较，并在上述房地产示例中展示了我们方法的实际应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在混合现实(MR)设备上实现实时、高质量场景重照明(relighting)的问题。现有技术面临三个主要挑战：深度学习方法计算量大无法实时运行；设备场景重建因扫描限制产生不准确结果；简单2D滤波无法处理复杂几何和阴影。这个问题很重要，因为它能让虚拟灯光变化与物理对象真实交互，产生逼真的照明和阴影效果，在房地产可视化、虚拟家具摆放、室内设计规划等场景有广泛应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有技术的局限性，然后设计了一种混合方法结合计算效率和语义理解。他们认识到简化网格带来的视觉质量问题，利用RGB图像中的深度学习特征提取物体边界，再应用特征引导的各向异性扩散来平滑物体内部阴影同时保持锐利边缘。为提高效率，他们引入了级联扩散策略。该方法借鉴了深度超分辨率领域的工作(特别是Metzger等人的引导深度超分辨率方法)，将其应用于重照明场景，同时结合了图像滤波技术和神经渲染方法的思想，但进行了优化以适应实时应用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合网格感知的滤波方法和引导的各向异性扩散，实现高质量实时重照明。整体流程包括：1)网格感知滤波重建场景并初步渲染；2)使用深度学习提取RGB特征引导各向异性扩散，校正网格 inaccuracies；3)采用级联扩散策略在多分辨率上高效处理；4)对阴影进行单独优化处理，保持形状准确性和柔和度；5)利用深度超分辨率训练框架训练适合边缘设备的模型。这种方法既利用了3D几何信息，又保持了计算效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合重照明方法结合深度学习和滤波技术；2)改进的各向异性扩散过程优化了迭代次数；3)级联扩散策略显著提高效率；4)阴影渲染的单独处理通道；5)从深度超分辨率模型的可转移训练。相比之前工作，这种方法避免了纯深度学习方法的计算负担和预训练需求，解决了基于场景重建方法的网格质量问题，超越了简单2D滤波方法的几何限制，实现了在边缘设备上的实时高质量重照明。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Hybrelighter通过结合深度各向异性扩散和场景重建，实现了在混合现实设备上的实时、高质量场景重照明，解决了现有技术在实时性能、几何准确性和视觉质量方面的平衡问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mixed Reality scene relighting, where virtual changes to lighting conditionsrealistically interact with physical objects, producing authentic illuminationand shadows, can be used in a variety of applications. One such application inreal estate could be visualizing a room at different times of day and placingvirtual light fixtures. Existing deep learning-based relighting techniquestypically exceed the real-time performance capabilities of current MR devices.On the other hand, scene understanding methods, such as on-device scenereconstruction, often yield inaccurate results due to scanning limitations, inturn affecting relighting quality. Finally, simpler 2D image filter-basedapproaches cannot represent complex geometry and shadows. We introduce a novelmethod to integrate image segmentation, with lighting propagation viaanisotropic diffusion on top of basic scene understanding, and thecomputational simplicity of filter-based techniques. Our approach correctson-device scanning inaccuracies, delivering visually appealing and accuraterelighting effects in real-time on edge devices, achieving speeds as high as100 fps. We show a direct comparison between our method and the industrystandard, and present a practical demonstration of our method in theaforementioned real estate example.</description>
      <author>example@mail.com (Hanwen Zhao, John Akers, Baback Elmieh, Ira Kemelmacher-Shlizerman)</author>
      <guid isPermaLink="false">2508.14930v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning via Lexical Relatedness: A Sarcasm and Hate Speech Case Study</title>
      <link>http://arxiv.org/abs/2508.16555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了将讽刺作为预训练步骤对仇恨言论检测的影响，比较了两种训练策略在不同模型上的效果，发现讽刺预训练能有效提高模型检测隐式和显式仇恨的能力。&lt;h4&gt;背景&lt;/h4&gt;检测非直接形式的仇恨言论，如反讽、讽刺和暗示，仍然是社交网络面临的持续挑战。&lt;h4&gt;目的&lt;/h4&gt;探索将讽刺作为预训练步骤是否能提高隐式仇恨言论检测，并进而提高显式仇恨言论检测。&lt;h4&gt;方法&lt;/h4&gt;整合ETHOS、Reddit讽刺和隐式仇恨语料库的样本，设计了两种训练策略：单步训练方法（在仅训练讽刺的模型上测试仇恨言论）和顺序迁移学习（对讽刺、隐式仇恨和显式仇恨进行微调），比较讽刺预训练在CNN+LSTM和BERT+BiLSTM模型上的效果。&lt;h4&gt;主要发现&lt;/h4&gt;在ETHOS上，讽刺预训练将BERT+BiLSTM的召回率提高了9.7%，AUC提高了7.8%，F1分数提高了6%；在隐式仇恨语料库上，当仅测试隐式样本时，精度提高了7.8%。&lt;h4&gt;结论&lt;/h4&gt;通过将讽刺纳入训练过程，模型可以更有效地检测隐式和显式仇恨。&lt;h4&gt;翻译&lt;/h4&gt;检测非直接形式的仇恨言论，如反讽、讽刺和暗示，仍然是社交网络面临的持续挑战。尽管讽刺和仇恨言论被视为不同的表达方式，我们的工作探索了将讽刺作为预训练步骤是否能提高隐式仇恨言论检测，并进而提高显式仇恨言论检测。通过整合ETHOS、Reddit讽刺和隐式仇恨语料库的样本，我们设计了两种训练策略来比较讽刺预训练在CNN+LSTM和BERT+BiLSTM模型上的效果。第一种策略是单步训练方法，即在仅训练讽刺的模型上测试仇恨言论。第二种策略使用顺序迁移学习，对讽刺、隐式仇恨和显式仇恨进行微调。我们的结果显示，在ETHOS上，讽刺预训练将BERT+BiLSTM的召回率提高了9.7%，AUC提高了7.8%，F1分数提高了6%。在隐式仇恨语料库上，当仅测试隐式样本时，精度提高了7.8%。通过将讽刺纳入训练过程，我们表明模型可以更有效地检测隐式和显式仇恨。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting hate speech in non-direct forms, such as irony, sarcasm, andinnuendos, remains a persistent challenge for social networks. Although sarcasmand hate speech are regarded as distinct expressions, our work explores whetherintegrating sarcasm as a pre-training step improves implicit hate speechdetection and, by extension, explicit hate speech detection. Incorporatingsamples from ETHOS, Sarcasm on Reddit, and Implicit Hate Corpus, we devised twotraining strategies to compare the effectiveness of sarcasm pre-training on aCNN+LSTM and BERT+BiLSTM model. The first strategy is a single-step trainingapproach, where a model trained only on sarcasm is then tested on hate speech.The second strategy uses sequential transfer learning to fine-tune models forsarcasm, implicit hate, and explicit hate. Our results show that sarcasmpre-training improved the BERT+BiLSTM's recall by 9.7%, AUC by 7.8%, andF1-score by 6% on ETHOS. On the Implicit Hate Corpus, precision increased by7.8% when tested only on implicit samples. By incorporating sarcasm into thetraining process, we show that models can more effectively detect both implicitand explicit hate.</description>
      <author>example@mail.com (Angelly Cabrera, Linus Lei, Antonio Ortega)</author>
      <guid isPermaLink="false">2508.16555v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Pretraining for Neural Regression</title>
      <link>http://arxiv.org/abs/2508.16355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NIAQUE是一种新的神经网络模型，专为概率回归的迁移学习而设计，通过预训练和微调策略提高了预测性能。&lt;h4&gt;背景&lt;/h4&gt;概率回归的迁移学习研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;引入NIAQUE（Neural Interpretable Any-Quantile Estimation）模型，设计用于概率回归的迁移学习。&lt;h4&gt;方法&lt;/h4&gt;通过排列不变性设计NIAQUE模型，在多样化的下游回归数据集上预训练，然后在特定目标数据集上进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;预训练和微调策略能提高单个回归任务的性能，展示了概率迁移学习的积极影响；在Kaggle竞赛中，NIAQUE与基于树的模型和最近的神经基础模型相比表现有效。&lt;h4&gt;结论&lt;/h4&gt;NIAQUE是一个稳健且可扩展的概率回归框架，利用迁移学习提高预测性能。&lt;h4&gt;翻译&lt;/h4&gt;概率回归的迁移学习研究仍然不足。这项工作通过引入NIAQUE（神经可解释任意分位数估计），一种通过排列不变性为概率回归迁移学习设计的新模型，填补了这一空白。我们证明在多样化的下游回归数据集上直接预训练NIAQUE，并在特定目标数据集上微调，可以提高单个回归任务的性能，展示了概率迁移学习的积极影响。此外，我们在Kaggle竞赛中突出了NIAQUE的有效性，与涉及基于树的模型和最近的神经基础模型TabPFN和TabDPT的强基线相比。这些发现强调了NIAQUE作为概率回归的稳健且可扩展框架的有效性，利用迁移学习来提高预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning for probabilistic regression remains underexplored. Thiswork closes this gap by introducing NIAQUE, Neural Interpretable Any-QuantileEstimation, a new model designed for transfer learning in probabilisticregression through permutation invariance. We demonstrate that pre-trainingNIAQUE directly on diverse downstream regression datasets and fine-tuning it ona specific target dataset enhances performance on individual regression tasks,showcasing the positive impact of probabilistic transfer learning. Furthermore,we highlight the effectiveness of NIAQUE in Kaggle competitions against strongbaselines involving tree-based models and recent neural foundation modelsTabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust andscalable framework for probabilistic regression, leveraging transfer learningto enhance predictive performance.</description>
      <author>example@mail.com (Boris N. Oreshkin, Shiv Tavker, Dmitry Efimov)</author>
      <guid isPermaLink="false">2508.16355v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach</title>
      <link>http://arxiv.org/abs/2508.16161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时空任务常因传感器缺失或无法访问而遇到数据不完整问题，时空克里金法对推断缺失时间信息至关重要。然而，当前模型难以确保推断时空模式的有效性和泛化能力，特别是在捕捉动态空间依赖和时间偏移方面。为解决这些问题，作者提出了STA-GANN框架，包含解耦相位模块、动态数据驱动元数据图建模和对抗性迁移学习策略。在九个数据集上的验证和理论证据均证明了其优越性能。&lt;h4&gt;背景&lt;/h4&gt;时空任务经常遇到数据不完整的问题，这源于传感器缺失或无法访问，这使得时空克里金法对于推断完全缺失的时间信息至关重要。&lt;h4&gt;目的&lt;/h4&gt;克服现有模型的局限性，提出一种新的基于图神经网络的克里金法框架，提高时空模式的有效性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出Spatio-Temporal Aware Graph Adversarial Neural Network (STA-GANN)，包含三个关键组件：(1)解耦相位模块，用于感知并调整时间戳偏移；(2)动态数据驱动元数据图建模，使用时间和数据更新空间关系；(3)对抗性迁移学习策略，确保泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;STA-GANN在捕捉动态空间依赖和时间偏移方面表现出色，能够优化未知传感器的泛化能力，确保推断时空模式的有效性和泛化性。&lt;h4&gt;结论&lt;/h4&gt;STA-GANN是一种有效的时空克里金法框架，通过整合三个关键组件，显著提高了时空模式推断的有效性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;时空任务经常遇到因缺失或无法访问的传感器而产生的不完整数据，这使得时空克里金法对于推断完全缺失的时间信息至关重要。然而，当前模型难以确保推断的时空模式的有效性和泛化能力，特别是在捕捉动态空间依赖和时间偏移，以及优化未知传感器的泛化能力方面。为克服这些局限性，我们提出了时空感知图对抗神经网络(STA-GANN)，这是一种新的基于GNN的克里金法框架，提高了时空模式的有效性和泛化能力。STA-GANN集成了(1)解耦相位模块，用于感知和调整时间戳偏移；(2)动态数据驱动的元数据图建模，使用时间和数据更新空间关系；(3)对抗性迁移学习策略，确保泛化能力。在来自四个领域的九个数据集上的广泛验证和理论证据都证明了STA-GANN的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761045&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-temporal tasks often encounter incomplete data arising from missing orinaccessible sensors, making spatio-temporal kriging crucial for inferring thecompletely missing temporal information. However, current models struggle withensuring the validity and generalizability of inferred spatio-temporalpatterns, especially in capturing dynamic spatial dependencies and temporalshifts, and optimizing the generalizability of unknown sensors. To overcomethese limitations, we propose Spatio-Temporal Aware Graph Adversarial NeuralNetwork (STA-GANN), a novel GNN-based kriging framework that improvesspatio-temporal pattern validity and generalization. STA-GANN integrates (i)Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)Dynamic Data-Driven Metadata Graph Modeling to update spatial relationshipsusing temporal data and metadata; (iii) An adversarial transfer learningstrategy to ensure generalizability. Extensive validation across nine datasetsfrom four fields and theoretical evidence both demonstrate the superiorperformance of STA-GANN.</description>
      <author>example@mail.com (Yujie Li, Zezhi Shao, Chengqing Yu, Tangwen Qian, Zhao Zhang, Yifan Du, Shaoming He, Fei Wang, Yongjun Xu)</author>
      <guid isPermaLink="false">2508.16161v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>A nonstationary spatial model of PM2.5 with localized transfer learning from numerical model output</title>
      <link>http://arxiv.org/abs/2508.15978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合监管监测网络数据和数值模型输出来推断和预测空气污染数据的方法，特别关注使用非平稳协方差函数来适应空气污染数据的空间变化特性。&lt;h4&gt;背景&lt;/h4&gt;监管监测网络通常用于支持流行病学研究与环境政策决策，但这些监测站在空间上分布稀疏，并且倾向于布置在人口密集区域，无法全面反映空气污染的空间分布情况。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够整合监管监测数据与数值模型输出的方法，以更准确地进行空气污染数据的推断和预测，特别是针对空气污染数据空间变异性随位置变化的特性。&lt;h4&gt;方法&lt;/h4&gt;采用从数值模型输出中学习到的局部协方差参数，构建全局非平稳协方差，并将其纳入完全贝叶斯模型中。通过计算效率高的方式对非平稳结构进行建模，使贝叶斯模型具有可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;非平稳协方差函数能够适应空气污染数据的空间表面变化特性，结合数值模型输出可以提高空气污染数据的推断和预测准确性。&lt;h4&gt;结论&lt;/h4&gt;通过整合监管监测网络数据与数值模型输出，并利用非平稳协方差函数，可以更有效地进行空气污染数据的推断和预测，为流行病学研究与环境政策决策提供更全面的数据支持。&lt;h4&gt;翻译&lt;/h4&gt;监管监测网络的空气污染测量数据通常被用于支持流行病学研究与环境政策决策。然而，监管监测站在空间上分布稀疏，并且倾向于布置在人口密集区域。数值空气污染模型的输出可以与监测站的测量数据结合，用于空气污染数据的推断和预测。非平稳协方差函数使模型能够适应空间表面变异性随位置变化的数据，如空气污染数据。在本文中，我们采用从数值模型输出中学习到的局部协方差参数，构建全局非平稳协方差，并将其纳入完全贝叶斯模型中。我们以计算高效的方式对非平稳结构进行建模，使贝叶斯模型具有可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ambient air pollution measurements from regulatory monitoring networks areroutinely used to support epidemiologic studies and environmental policydecision making. However, regulatory monitors are spatially sparse andpreferentially located in areas with large populations. Numerical air pollutionmodel output can be leveraged into the inference and prediction of airpollution data combining with measurements from monitors. Nonstationarycovariance functions allow the model to adapt to spatial surfaces whosevariability changes with location like air pollution data. In the paper, weemploy localized covariance parameters learned from the numerical output modelto knit together into a global nonstationary covariance, to incorporate in afully Bayesian model. We model the nonstationary structure in a computationallyefficient way to make the Bayesian model scalable.</description>
      <author>example@mail.com (Wenlong Gong, Brian J. Reich, Joseph Guinness)</author>
      <guid isPermaLink="false">2508.15978v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning</title>
      <link>http://arxiv.org/abs/2508.15868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, to appear in EMNLP25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CARFT的对比学习与标注思维链强化微调方法，用于提升大语言模型的推理性能，解决了现有方法中模型崩溃和训练不稳定的问题。&lt;h4&gt;背景&lt;/h4&gt;推理能力在大语言模型的广泛应用中起着至关重要的作用。为增强LLMs的推理性能，已有多种基于强化学习的微调方法被提出，以解决仅通过监督微调训练的LLMs泛化能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于标注思维链的对比强化微调方法(CARFT)，以增强LLMs的推理性能，同时解决现有方法中忽视标注思维链和过度强调标注思维链导致的局限性。&lt;h4&gt;方法&lt;/h4&gt;为每个思维链学习一个表示，基于此表示设计新的对比信号来指导微调过程，充分利用可用的标注思维链，并通过引入额外的无监督学习信号稳定微调过程。&lt;h4&gt;主要发现&lt;/h4&gt;通过三种基线方法、两种基础模型和两个数据集进行的综合实验表明，CARFT在鲁棒性、性能(提升高达10.15%)和效率(提升高达30.62%)方面具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;CARFT方法有效解决了现有RL方法和SFT方法的局限性，显著提升了LLMs的推理性能，同时保持了训练的稳定性。&lt;h4&gt;翻译&lt;/h4&gt;推理能力在大语言模型的广泛应用中起着至关重要的作用。为了增强LLMs的推理性能，已经提出了多种基于强化学习的微调方法，以解决仅通过监督微调训练的LLMs有限的泛化能力。尽管这些方法有效，但两个主要限制阻碍了LLMs的发展。首先，基础RL方法忽略了标注的思维链并融入不稳定的推理路径采样，通常导致模型崩溃、训练过程不稳定和次优性能。其次，现有SFT方法通常过度强调标注的思维链，可能由于未能充分利用潜在的思维链而导致性能下降。在本文中，我们提出了一种基于标注思维链的对比强化微调方法，即CARFT，以增强LLMs的推理性能并解决上述局限性。具体而言，我们为每个思维链学习一个表示。基于这个表示，我们设计了新的对比信号来指导微调过程。我们的方法不仅充分利用了可用的标注思维链，还通过引入额外的无监督学习信号稳定了微调过程。我们通过三种基线方法、两种基础模型和两个数据集进行了全面的实验和深入分析，证明了CARFT在鲁棒性、性能(高达10.15%)和效率(高达30.62%)方面的显著优势。代码可在https://github.com/WNQzhu/CARFT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning capability plays a significantly critical role in the the broadapplications of Large Language Models (LLMs). To enhance the reasoningperformance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuningapproaches have been proposed to address the limited generalization capabilityof LLMs trained solely via Supervised Fine-Tuning (SFT). Despite theireffectiveness, two major limitations hinder the advancement of LLMs. First,vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) andincorporate unstable reasoning path sampling, which typically results in modelcollapse, unstable training process, and suboptimal performance. Second,existing SFT approaches generally overemphasize the annotated CoT, potentiallyleading to performance degradation due to insufficient exploitation ofpotential CoT. In this paper, we propose a Contrastive learning with annotatedCoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance thereasoning performance of LLMs while addressing the aforementioned limitations.Specifically, we propose learning a representation for each CoT. Based on thisrepresentation, we design novel contrastive signals to guide the fine-tuningprocess. Our approach not only fully exploits the available annotated CoT butalso stabilizes the fine-tuning procedure by incorporating an additionalunsupervised learning signal. We conduct comprehensive experiments and in-depthanalysis with three baseline approaches, two foundation models, and twodatasets to demonstrate significant advantages of \TheName{} in terms ofrobustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Codeis available at https://github.com/WNQzhu/CARFT.</description>
      <author>example@mail.com (Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang)</author>
      <guid isPermaLink="false">2508.15868v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks</title>
      <link>http://arxiv.org/abs/2508.16459v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Authors Ali Emre Balc{\i} and Erhan Ege Keyvan contributed equally to  this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于高斯过程的地标表示的新型SLAM方法，通过基于物体的轮廓建模实现环境表示，并在贝叶斯框架下进行联合推断。&lt;h4&gt;背景&lt;/h4&gt;传统SLAM方法通常使用网格地图或点云配准来表示环境，但这些方法可能无法充分利用物体的语义信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供物体语义信息、支持概率测量关联并提供形状置信边界的SLAM方法。&lt;h4&gt;方法&lt;/h4&gt;使用基于高斯过程的轮廓表示来对每个物体进行建模，通过递归方案在线更新轮廓，在完全贝叶斯框架内公式化SLAM问题，实现机器人姿态和基于物体地图的联合推断。&lt;h4&gt;主要发现&lt;/h4&gt;基于GP的轮廓表示能够提供物体的语义信息(如数量和面积)，支持概率测量到物体的关联，并提供物体形状的置信边界。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在合成和真实世界实验中表现出色，能够在多种结构化环境中提供准确的定位和地图构建性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的SLAM方法，它采用基于高斯过程的地标(物体)表示。与传统的网格地图或点云配准不同，我们使用基于GP的轮廓表示对每个物体进行环境建模。这些轮廓通过递归方案在线更新，实现了高效的内存使用。SLAM问题在完全贝叶斯框架内公式化，允许对机器人姿态和基于物体的地图进行联合推断。这种表示提供了语义信息，如物体数量和它们的面积，同时支持概率测量到物体的关联。此外，基于GP的轮廓提供了物体形状的置信边界，为安全导航和探索等下游任务提供了有价值的信息。我们在合成和真实世界实验中验证了我们的方法，并表明它在多种结构化环境中提供了准确的定位和地图构建性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光SLAM中环境表示的效率与质量问题。传统SLAM方法使用网格地图或点云配准，存在内存效率低、缺乏语义信息、难以提供置信度边界等缺点。这个问题很重要，因为自主机器人需要准确的环境表示进行导航和决策，置信度边界对安全导航至关重要，而内存高效的表示对资源受限的机器人平台尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有SLAM方法的局限性，然后考虑基于物体的地图表示作为替代方案。他们回顾了相关工作，包括视觉SLAM中的物体表示和隐函数表示物体边界的方法，发现这些方法在处理局部更新和概率分布方面存在不足。基于这些分析，作者提出使用高斯过程建模物体轮廓，借鉴了高斯过程在扩展目标跟踪和星凸集表示物体形状方面的现有工作，但将其整合到一个新的SLAM框架中，并针对SLAM的特殊需求进行了改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于物体的地图表示，用高斯过程建模物体轮廓，提供形状估计和置信度边界，并将SLAM问题表述为完整的贝叶斯推断问题。整体流程包括：接收激光雷达点云和里程计输入；使用里程计预测机器人状态；计算物体-测量对的似然值进行数据关联；对未关联测量聚类初始化新物体；使用迭代扩展卡尔曼滤波联合更新机器人位姿和物体形状；输出轨迹和地图估计；将结果反馈处理下一时刻输入。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：基于高斯过程的物体轮廓表示提供置信度边界；完整的贝叶斯框架联合估计机器人位姿和物体形状；基于似然的数据关联自动拒绝离群值；内存高效的表示随环境增大内存需求增长缓慢。相比之前的工作，与网格地图方法相比提供了物体级语义信息和置信度边界；与点云配准方法相比计算复杂度更低；与其他基于物体的表示方法相比支持局部更新和空间域明确定义的概率分布。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于高斯过程的物体表示SLAM框架，能够高效地构建具有置信度边界的物体级地图，同时实现准确的机器人定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel Simultaneous Localization and Mapping (SLAM) method thatemploys Gaussian Process (GP) based landmark (object) representations. Insteadof conventional grid maps or point cloud registration, we model the environmenton a per object basis using GP based contour representations. These contoursare updated online through a recursive scheme, enabling efficient memory usage.The SLAM problem is formulated within a fully Bayesian framework, allowingjoint inference over the robot pose and object based map. This representationprovides semantic information such as the number of objects and their areas,while also supporting probabilistic measurement to object associations.Furthermore, the GP based contours yield confidence bounds on object shapes,offering valuable information for downstream tasks like safe navigation andexploration. We validate our method on synthetic and real world experiments,and show that it delivers accurate localization and mapping performance acrossdiverse structured environments.</description>
      <author>example@mail.com (Ali Emre Balcı, Erhan Ege Keyvan, Emre Özkan)</author>
      <guid isPermaLink="false">2508.16459v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.16069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submit to AAAI2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型体素扩散模块(VDM)，用于增强点云数据中的体素级表示和扩散，解决了基于Transformer和状态空间模型的点云物体检测中体素表示的空间扩散能力受限问题。&lt;h4&gt;背景&lt;/h4&gt;点云物体检测领域最近越来越多地采用基于Transformer和状态空间模型(SSMs)的方法，但这些模型中的体素表示需要输入和输出维度严格一致，限制了卷积操作的空间扩散能力，显著影响检测准确性。&lt;h4&gt;目的&lt;/h4&gt;受基于CNN的物体检测架构启发，提出一个新的体素扩散模块(VDM)来增强点云数据中的体素级表示和扩散，提高检测准确性。&lt;h4&gt;方法&lt;/h4&gt;VDM由稀疏3D卷积、子流形稀疏卷积和残差连接组成，输出特征图被下采样到原始输入分辨率的四分之一。VDM有两个主要功能：(1)通过稀疏3D卷积扩散前景体素特征，丰富空间上下文；(2)聚合细粒度空间信息，加强体素级特征表示。增强后的体素特征可无缝集成到主流的基于Transformer或SSM的检测模型中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，VDM方法在多个基准数据集上显著提高了检测准确性。VDM-SSMs在Waymo上达到74.7 mAPH (L2)，在nuScenes上达到72.9 NDS，在Argoverse 2上达到42.3 mAP，在ONCE上达到67.6 mAP，在所有数据集上都取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;VDM方法具有良好的通用性，可以无缝集成到不同的检测模型中，有效提升检测准确性。该方法的代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;最近，点云物体检测领域的进展越来越多地采用基于Transformer和状态空间模型的方法，展现出强大的性能。然而，这些模型中的基于体素的表示由于序列化处理需要输入和输出维度严格一致，这限制了卷积操作通常提供的空间扩散能力，显著影响了检测准确性。受基于CNN的物体检测架构启发，我们提出了一种新型的体素扩散模块(VDM)，以增强点云数据中的体素级表示和扩散。VDM由稀疏3D卷积、子流形稀疏卷积和残差连接组成。为确保计算效率，输出特征图被下采样到原始输入分辨率的四分之一。VDM有两个主要功能：(1)通过稀疏3D卷积扩散前景体素特征，丰富空间上下文；(2)聚合细粒度空间信息，加强体素级特征表示。VDM产生的增强体素特征可以无缝集成到主流的基于Transformer或SSM的检测模型中，用于准确的目标分类和定位，突显了我们方法的通用性。我们通过将VDM嵌入到基于Transformer和SSM的模型中，在多个基准数据集上评估了VDM。实验结果表明，我们的方法在所有基线模型上持续提高了检测准确性。具体来说，VDM-SSMs在Waymo上达到74.7 mAPH (L2)，在nuScenes上达到72.9 NDS，在Argoverse 2上达到42.3 mAP，在ONCE上达到67.6 mAP，在所有数据集上都设定了新的最先进性能。我们的代码将公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于Transformer和状态空间模型(SSM)的3D目标检测中，体素化表示方法因输入输出维度限制而缺乏空间扩散能力的问题。这个问题在自动驾驶和机器人导航等应用中至关重要，因为这些应用需要精确的3D物体检测，而空间扩散对于提升检测精度，特别是在复杂场景中检测小物体和远处物体非常关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受CNN-based目标检测架构启发，认识到卷积操作在空间扩散方面的优势。他们发现现有Transformer和SSM模型在处理体素序列时缺乏这种能力，因此提出在序列化前进行体素扩散。方法借鉴了SAFDNet中的稀疏3D卷积和残差连接，以及LION中的体素扩散概念，但创新性地将扩散操作放在序列化之前，而非中间处理过程中。VDM设计为模块化结构，可以无缝集成到现有的Transformer或SSM检测模型中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在体素序列化之前通过3D卷积操作进行体素扩散，增加有意义体素信息的密度，同时利用稀疏卷积聚合局部细粒度空间特征，为下游序列模型提供更丰富的空间上下文。整体流程为：输入点云→体素化→应用VDM模块(包含稀疏3D卷积、子流形3D卷积和残差块)→下采样至1/4分辨率→序列化→输入Transformer或SSM模型→输出检测结果。VDM通过扩散前景体素特征和聚合细粒度空间信息来增强体素表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出统一的Voxel Diffusion Module (VDM)；2) 在序列化前进行体素扩散，而非中间处理；3) 结合稀疏3D卷积和子流形稀疏卷积实现双重功能；4) 设计为通用模块，可同时兼容Transformer和SSM架构；5) 通过下采样保持计算效率。相比之前工作，VDM不同于LION在中间处理中通过选择top-k体素进行扩散，也不同于传统CNN方法或UniMamba等混合方法，它明确分离了体素扩散和细粒度聚合功能，专注于增强序列模型的前处理阶段。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通用的体素扩散模块(VDM)，通过在序列化前进行3D卷积操作增强体素特征的空间扩散和细粒度聚合，显著提升了基于Transformer和状态空间模型的3D目标检测性能，并在多个基准数据集上达到了最先进的检测结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in point cloud object detection have increasingly adoptedTransformer-based and State Space Models (SSMs), demonstrating strongperformance. However, voxelbased representations in these models require strictconsistency in input and output dimensions due to their serialized processing,which limits the spatial diffusion capability typically offered byconvolutional operations. This limitation significantly affects detectionaccuracy. Inspired by CNN-based object detection architectures, we propose anovel Voxel Diffusion Module (VDM) to enhance voxel-level representation anddiffusion in point cloud data. VDM is composed of sparse 3D convolutions,submanifold sparse convolutions, and residual connections. To ensurecomputational efficiency, the output feature maps are downsampled to one-fourthof the original input resolution. VDM serves two primary functions: (1)diffusing foreground voxel features through sparse 3D convolutions to enrichspatial context, and (2) aggregating fine-grained spatial information tostrengthen voxelwise feature representation. The enhanced voxel featuresproduced by VDM can be seamlessly integrated into mainstream Transformer- orSSM-based detection models for accurate object classification and localization,highlighting the generalizability of our method. We evaluate VDM on severalbenchmark datasets by embedding it into both Transformerbased and SSM-basedmodels. Experimental results show that our approach consistently improvesdetection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAPon ONCE, setting new stateof-the-art performance across all datasets. Our codewill be made publicly available.</description>
      <author>example@mail.com (Qifeng Liu, Dawei Zhao, Yabo Dong, Linzhi Shang, Liang Xiao, Juan Wang, Kunkong Zhao, Dongming Lu, Qi Zhu)</author>
      <guid isPermaLink="false">2508.16069v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars</title>
      <link>http://arxiv.org/abs/2508.16030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCCN 2025 (IEEE International Conference on Computer  Communications and Networks), Tokyo, Japan, August 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;汽车FMCW雷达在恶劣天气条件下可靠，但稀疏点云限制了3D物体检测。作者发布了CoVeRaP数据集并提出协同感知框架，通过多车辆雷达数据融合显著提高检测性能。&lt;h4&gt;背景&lt;/h4&gt;汽车FMCW雷达在雨和强光条件下仍然可靠，但它们的稀疏、嘈杂点云限制了3D物体检测能力。&lt;h4&gt;目的&lt;/h4&gt;发布CoVeRaP数据集并提出统一协同感知框架，解决多车辆FMCW雷达感知问题，提高3D物体检测的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;构建包含21k帧的协同数据集，对齐多辆车的雷达、摄像头和GPS流；提出具有中期和晚期融合选项的统一协同感知框架；使用多分支PointNet风格编码器增强自注意力机制，融合空间、多普勒和强度线索到公共潜在空间，由解码器转换为3D边界框和深度置信度。&lt;h4&gt;主要发现&lt;/h4&gt;中期融合与强度编码相结合，在IoU 0.9条件下将平均精度提高了高达9倍，并且一致优于单车辆基线。&lt;h4&gt;结论&lt;/h4&gt;CoVeRaP建立了首个可复现的多车辆FMCW雷达感知基准，证明了经济实惠的雷达共享显著提高了检测鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;汽车FMCW雷达在雨和强光条件下仍然可靠，但它们的稀疏、嘈杂点云限制了3D物体检测。因此，我们发布了CoVeRaP，这是一个包含21k帧的协同数据集，对齐了多辆车在不同机动过程中的雷达、摄像头和GPS流。基于这些数据，我们提出了一个具有中期和晚期融合选项的统一协同感知框架。其基线网络采用多分支PointNet风格编码器，增强自注意力机制，将空间、多普勒和强度线索融合到公共潜在空间，解码器将其转换为3D边界框和每点深度置信度。实验表明，中期融合与强度编码相结合，在IoU 0.9条件下将平均精度提高了高达9倍，并且一致优于单车辆基线。CoVeRaP因此建立了首个可复现的多车辆FMCW雷达感知基准，并证明了经济实惠的雷达共享显著提高了检测鲁棒性。数据集和代码已公开可用，以鼓励进一步研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决单个车辆FMCW雷达产生的点云数据稀疏、噪声大，以及存在盲区的问题。这个问题在自动驾驶和高级驾驶辅助系统中至关重要，因为单传感器在复杂环境下（如恶劣天气、强光）感知能力有限，而通过车辆间共享雷达数据可以克服这些限制，生成更密集的点云，提供更准确的3D目标检测，从而提高驾驶安全性和系统鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到单传感器在复杂环境下的局限性，特别是雷达数据的稀疏性和噪声问题，然后提出通过多车辆合作感知来增强环境感知能力。在设计方法时，作者考虑了不同的数据融合策略（早期、中期和晚期融合），并针对雷达数据的特殊性设计了专门处理流程。该方法借鉴了PointNet的架构处理无序点集，多分支架构处理不同特征，自注意力机制融合特征，以及V2V4Real数据集的同步方法和PointNet++的分层策略来捕获多尺度结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多车辆共享毫米波FMCW雷达数据来增强环境感知，使用特征级融合和预测级融合两种策略，并设计一个多分支、注意力增强的骨干网络来融合空间、多普勒和强度线索。整体实现流程包括：1) 使用配备FMCW雷达、GPS-RTK和RGB相机的多车辆平台收集数据；2) 对数据进行预处理，包括生成3D点云、动态选择和过滤点、开发地面真实标签；3) 实现两种融合策略（中期融合在共同坐标系中融合特征，晚期融合独立预测后合并结果）；4) 设计多模态信号编码（位置、动态、强度三个分支）、上下文特征合成和输出解码的基线模型；5) 在不同IoU阈值下评估检测性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) CoVeRaP数据集，首个大规模合作雷达感知数据集，整合多车辆雷达、相机和GPS数据；2) 统一系统架构，支持特征级和预测级融合策略；3) 基线3D边界框检测模型，利用多分支架构和注意力机制，特别强调信号强度特征的重要性；4) 创新的数据处理方法，包括动态点选择、基于强度的过滤和事件触发同步。相比之前工作，本文专注于毫米波FMCW雷达而非激光雷达，专门处理雷达数据的稀疏性和噪声问题，强调信号强度特征的重要性，并提供可复现基准和开源代码。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CoVeRaP通过首个大规模毫米波雷达合作感知数据集和融合框架，证明了车辆间共享雷达数据能显著提高3D目标检测的准确性和鲁棒性，特别是在高IoU阈值下性能提升可达9倍。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automotive FMCW radars remain reliable in rain and glare, yet their sparse,noisy point clouds constrain 3-D object detection. We therefore releaseCoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, andGPS streams from multiple vehicles across diverse manoeuvres. Built on thisdata, we propose a unified cooperative-perception framework with middle- andlate-fusion options. Its baseline network employs a multi-branch PointNet-styleencoder enhanced with self-attention to fuse spatial, Doppler, and intensitycues into a common latent space, which a decoder converts into 3-D boundingboxes and per-point depth confidence. Experiments show that middle fusion withintensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 andconsistently outperforms single-vehicle baselines. CoVeRaP thus establishes thefirst reproducible benchmark for multi-vehicle FMCW-radar perception anddemonstrates that affordable radar sharing markedly improves detectionrobustness. Dataset and code are publicly available to encourage furtherresearch.</description>
      <author>example@mail.com (Jinyue Song, Hansol Ku, Jayneel Vora, Nelson Lee, Ahmad Kamari, Prasant Mohapatra, Parth Pathak)</author>
      <guid isPermaLink="false">2508.16030v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System</title>
      <link>http://arxiv.org/abs/2508.15990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GelSLAM，一个仅依靠触觉感知的实时3D SLAM系统，用于长时间估计物体姿态并高保真重建物体形状。&lt;h4&gt;背景&lt;/h4&gt;精确感知物体的姿态和形状对于精确抓取和操作至关重要。与基于视觉的方法相比，触觉感知在跟踪和重建接触物体时具有精度高和对遮挡免疫的优势。&lt;h4&gt;目的&lt;/h4&gt;开发一个仅依靠触觉感知的实时3D SLAM系统，用于长时间估计物体姿态并高保真重建物体形状。&lt;h4&gt;方法&lt;/h4&gt;GelSLAM使用触觉导出的表面法线和曲率进行稳健的跟踪和回环闭合，而非传统的点云方法。&lt;h4&gt;主要发现&lt;/h4&gt;GelSLAM能够以低误差和最小漂移实时跟踪物体运动，并以亚毫米精度重建形状，即使是对于低纹理物体（如木制工具）也是如此。&lt;h4&gt;结论&lt;/h4&gt;GelSLAM将触觉感知扩展到局部接触之外，实现了全局、长期的空间感知，将为许多涉及手中物体的精确操作任务奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;Accurately perceiving an object's pose and shape is essential for precise grasping and manipulation. Compared to common vision-based methods, tactile sensing offers advantages in precision and immunity to occlusion when tracking and reconstructing objects in contact. This makes it particularly valuable for in-hand and other high-precision manipulation tasks. In this work, we present GelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to estimate object pose over long periods and reconstruct object shapes with high fidelity. Unlike traditional point cloud-based approaches, GelSLAM uses tactile-derived surface normals and curvatures for robust tracking and loop closure. It can track object motion in real time with low error and minimal drift, and reconstruct shapes with submillimeter accuracy, even for low-texture objects such as wooden tools. GelSLAM extends tactile sensing beyond local contact to enable global, long-horizon spatial perception, and we believe it will serve as a foundation for many precise manipulation tasks involving interaction with objects in hand. The video demo is available on our website: https://joehjhuang.github.io/gelslam.&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决仅使用触觉传感实现实时、长期目标跟踪和高保真度3D物体重建的问题。这个问题很重要，因为准确感知物体的姿态和形状对机器人精确抓取和操作至关重要，而触觉传感相比视觉方法在精度和抗遮挡方面有独特优势，特别适用于手中操作等高精度任务，还可应用于AR/VR、生物学、考古学等多个领域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了触觉传感的'盲人摸象'问题：每次触觉读数提供局部信息，难以构建全局理解。他们发现传统点云方法在触觉场景中表现不佳，因为接触产生的表面形变小，点云缺乏独特特征。关键洞察是使用微分表示（法线图和曲率图）而非点云，这能捕获丰富局部特征且与GelSight传感器原理一致。系统设计借鉴了现有工作如NormalFlow姿态估计、SLAM系统的关键帧策略、SIFT特征匹配和pose graph优化，但创新性地组合这些技术解决触觉SLAM问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用表面法线图和曲率图的微分表示而非传统点云来处理触觉数据，通过回环闭合机制克服触觉传感的局部性限制。整体流程包括：1)数据预处理：提取法线图、高度图、接触掩码和曲率图；2)跟踪模块：使用NormalFlow估计相对姿态，实现失败检测和关键帧选择；3)回环闭合模块：构建覆盖集，使用SIFT匹配和NormalFlow精炼检测回环，执行pose graph优化；4)重建模块：快速融合点云(在线)和Poisson表面重建(离线)生成最终3D模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)微分表示方法使用法线图和曲率图解决点云局限性；2)首个实现长期跟踪和高保真重建的纯触觉SLAM系统；3)鲁棒的回环检测机制结合SIFT和NormalFlow；4)失败检测机制评估对齐质量和重叠度；5)改进的关键帧选择策略。相比之前工作，GelSLAM在低纹理物体上表现更好，通过回环闭合减少46%旋转误差和17.5%平移误差，比Tac2Structure更鲁棒，且首个实现'野外'触觉only 3D重建，无需已知接触姿态或视觉辅助。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GelSLAM通过创新的微分表示方法和鲁棒回环检测，首次实现了仅使用触觉传感的长期目标跟踪和高保真度3D重建，克服了触觉传感的局部性限制，为机器人和多种应用领域提供了强大的触觉感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately perceiving an object's pose and shape is essential for precisegrasping and manipulation. Compared to common vision-based methods, tactilesensing offers advantages in precision and immunity to occlusion when trackingand reconstructing objects in contact. This makes it particularly valuable forin-hand and other high-precision manipulation tasks. In this work, we presentGelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing toestimate object pose over long periods and reconstruct object shapes with highfidelity. Unlike traditional point cloud-based approaches, GelSLAM usestactile-derived surface normals and curvatures for robust tracking and loopclosure. It can track object motion in real time with low error and minimaldrift, and reconstruct shapes with submillimeter accuracy, even for low-textureobjects such as wooden tools. GelSLAM extends tactile sensing beyond localcontact to enable global, long-horizon spatial perception, and we believe itwill serve as a foundation for many precise manipulation tasks involvinginteraction with objects in hand. The video demo is available on our website:https://joehjhuang.github.io/gelslam.</description>
      <author>example@mail.com (Hung-Jui Huang, Mohammad Amin Mirzaee, Michael Kaess, Wenzhen Yuan)</author>
      <guid isPermaLink="false">2508.15990v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds</title>
      <link>http://arxiv.org/abs/2508.14879v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MeshCoder是一种新颖的框架，可将复杂的3D物体从点云重建为可编辑的Blender Python脚本，解决了现有方法依赖有限领域语言和小数据集的问题。&lt;h4&gt;背景&lt;/h4&gt;将3D物体重建为可编辑程序对逆向工程和形状编辑等应用至关重要，但现有方法通常依赖于有限领域的特定语言(DSLs)和小规模数据集，限制了它们对复杂几何形状和结构的建模能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一种能够处理复杂3D物体并支持程序化编辑的重建框架。&lt;h4&gt;方法&lt;/h4&gt;引入MeshCoder框架，开发全面的Blender Python API集合来合成复杂几何形状，构建大规模成对物体-代码数据集，并训练多模态大语言模型将3D点云转换为可执行的Blender Python脚本。&lt;h4&gt;主要发现&lt;/h4&gt;MeshCoder在形状到代码重建任务中取得优越性能，通过代码修改实现直观的几何和拓扑编辑，且基于代码的表示增强了LLM在3D形状理解任务中的推理能力。&lt;h4&gt;结论&lt;/h4&gt;MeshCoder是一个强大而灵活的解决方案，用于程序化的3D形状重建和理解。&lt;h4&gt;翻译&lt;/h4&gt;将3D物体重建为可编辑程序对于逆向工程和形状编辑等应用至关重要。然而，现有方法通常依赖于有限领域的特定语言(DSLs)和小规模数据集，限制了它们对复杂几何形状和结构的建模能力。为应对这些挑战，我们引入了MeshCoder，一种新颖的框架，可将复杂的3D物体从点云重建为可编辑的Blender Python脚本。我们开发了一套全面的Blender Python API集合，能够合成复杂的几何形状。利用这些API，我们构建了一个大规模的成对物体-代码数据集，其中每个物体的代码被分解为不同的语义部分。随后，我们训练了一个多模态大语言模型(LLM)，将3D点云转换为可执行的Blender Python脚本。我们的方法不仅在形状到代码重建任务中取得了优越的性能，还通过方便的代码修改促进了直观的几何和拓扑编辑。此外，我们的基于代码的表示增强了LLM在3D形状理解任务中的推理能力。这些贡献共同确立了MeshCoder作为程序化3D形状重建和理解的强大而灵活的解决方案。项目主页可在https://daibingquan.github.io/MeshCoder获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将复杂3D物体重建为可编辑程序的问题。这个问题在现实和研究中非常重要，因为现有的方法只能表示简单的基本形状，无法建模复杂几何结构，同时缺乏大规模训练数据。解决这一问题对于逆向工程、形状编辑和3D结构理解等应用至关重要，能够提高设计效率和灵活性，使3D模型更易于修改和重用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统DSL只能表示简单形状，且缺乏大规模训练数据。为此，他们设计了五种基本形状类型（基本形状、平移、桥接循环、布尔和阵列）来创建更复杂的几何形状。他们借鉴了现有的形状程序和基于部分表示的方法，但进行了扩展和改进。在数据集构建方面，作者先训练部分到代码的推断模型，然后利用这个模型构建整体物体-代码数据集，解决了数据稀缺问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; MeshCoder的核心思想是将3D点云转换为可执行的Blender Python脚本，这些脚本能够重建物体的各个语义部分，实现结构化和可编辑的网格重建。整体流程包括：1)开发表达性强的Blender Python API；2)构建大规模配对物体-代码数据集；3)训练多模态大语言模型；4)使用形状标记器将点云转换为固定长度标记；5)将标记输入LLM生成Blender Python脚本，重建输入几何形状的不同语义部分。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开发全面的Blender Python API，合成复杂几何形状；2)提出新方法构建大规模配对物体-代码数据集；3)训练多模态大语言模型将点云转换为可执行脚本；4)通过代码表示实现直观的几何和拓扑编辑；5)增强LLM在3D形状理解中的推理能力。相比之前工作，MeshCoder不依赖有限DSL，使用更全面的API，构建了包含41个类别、约100万个物体的大规模数据集，能处理更复杂的几何形状，且生成的代码可直接编辑和修改3D模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MeshCoder通过将3D点云转换为可执行的Blender Python脚本，实现了复杂3D物体的结构化重建和可编辑表示，为逆向工程、形状编辑和3D结构理解提供了强大而灵活的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D objects into editable programs is pivotal for applicationslike reverse engineering and shape editing. However, existing methods oftenrely on limited domain-specific languages (DSLs) and small-scale datasets,restricting their ability to model complex geometries and structures. Toaddress these challenges, we introduce MeshCoder, a novel framework thatreconstructs complex 3D objects from point clouds into editable Blender Pythonscripts. We develop a comprehensive set of expressive Blender Python APIscapable of synthesizing intricate geometries. Leveraging these APIs, weconstruct a large-scale paired object-code dataset, where the code for eachobject is decomposed into distinct semantic parts. Subsequently, we train amultimodal large language model (LLM) that translates 3D point cloud intoexecutable Blender Python scripts. Our approach not only achieves superiorperformance in shape-to-code reconstruction tasks but also facilitatesintuitive geometric and topological editing through convenient codemodifications. Furthermore, our code-based representation enhances thereasoning capabilities of LLMs in 3D shape understanding tasks. Together, thesecontributions establish MeshCoder as a powerful and flexible solution forprogrammatic 3D shape reconstruction and understanding. The project homepage isavailable at \href{https://daibingquan.github.io/MeshCoder}{this link}.</description>
      <author>example@mail.com (Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, Zhaoyang Lyu, Jiangmiao Pang)</author>
      <guid isPermaLink="false">2508.14879v2</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer</title>
      <link>http://arxiv.org/abs/2508.16569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发并验证了RenalCLIP，一个用于肾脏肿瘤特征描述、诊断和预后的视觉语言基础模型，该模型在10项核心任务上表现出优越的性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;无创评估偶然发现的肾脏肿瘤是泌尿肿瘤学中的一个关键挑战，诊断不确定性经常导致良性或惰性肿瘤的过度治疗。&lt;h4&gt;目的&lt;/h4&gt;开发并验证RenalCLIP模型，用于肾脏肿瘤的特征描述、诊断和预后。&lt;h4&gt;方法&lt;/h4&gt;使用来自9个中国医疗中心和公共TCIA队列的27,866次CT扫描数据集（来自8,809名患者），采用两阶段预训练策略：首先使用领域特定知识增强图像和文本编码器，然后通过对比学习目标对齐它们，创建强大的表示。&lt;h4&gt;主要发现&lt;/h4&gt;RenalCLIP在10项核心任务上表现出比其他最先进的通用CT基础模型更好的性能和泛化能力；在TCIA队列中的复发-无生存率预测等复杂任务上，C-index达到0.726，比领先的基线提高了约20%；预训练赋予了显著的数据效率，只需20%的训练数据即可达到所有基线模型的峰值性能；在报告生成、图像文本检索和零样本诊断任务上也表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;RenalCLIP提供了强大的工具，有望提高诊断准确性，优化预后分层，并个性化肾脏癌患者的管理。&lt;h4&gt;翻译&lt;/h4&gt;对偶然发现的肾脏肿块进行无创评估是泌尿肿瘤学中的一个关键挑战，其中诊断不确定性经常导致良性或惰性肿瘤的过度治疗。在本研究中，我们开发和验证了RenalCLIP，这是一个使用来自9个中国医疗中心和公共TCIA队列的8,809名患者的27,866次CT扫描数据集构建的视觉语言基础模型，用于肾脏肿块的特征描述、诊断和预后。该模型通过两阶段预训练策略开发，首先使用领域特定知识增强图像和文本编码器，然后通过对比学习目标对齐它们，以创建强大的表示，实现更好的泛化和诊断精度。与最先进的通用CT基础模型相比，RenalCLIP在涵盖肾脏癌症完整临床工作流程的10项核心任务上实现了更好的性能和优越的泛化能力。特别是在TCIA队列中的复发-无生存率预测等复杂任务上，RenalCLIP的C-index达到0.726，比领先的基线提高了约20%。此外，RenalCLIP的预训练赋予了显著的数据效率；在诊断分类任务中，它只需要20%的训练数据即可达到所有基线模型的峰值性能，即使它们在100%的数据上完全微调。此外，它在报告生成、图像文本检索和零样本诊断任务上也表现出优越性能。我们的研究结果表明，RenalCLIP提供了一个强大的工具，有可能提高诊断准确性，优化预后分层，并个性化肾脏癌患者的管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The non-invasive assessment of increasingly incidentally discovered renalmasses is a critical challenge in urologic oncology, where diagnosticuncertainty frequently leads to the overtreatment of benign or indolent tumors.In this study, we developed and validated RenalCLIP using a dataset of 27,866CT scans from 8,809 patients across nine Chinese medical centers and the publicTCIA cohort, a visual-language foundation model for characterization, diagnosisand prognosis of renal mass. The model was developed via a two-stagepre-training strategy that first enhances the image and text encoders withdomain-specific knowledge before aligning them through a contrastive learningobjective, to create robust representations for superior generalization anddiagnostic precision. RenalCLIP achieved better performance and superiorgeneralizability across 10 core tasks spanning the full clinical workflow ofkidney cancer, including anatomical assessment, diagnostic classification, andsurvival prediction, compared with other state-of-the-art general-purpose CTfoundation models. Especially, for complicated task like recurrence-freesurvival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726,representing a substantial improvement of approximately 20% over the leadingbaselines. Furthermore, RenalCLIP's pre-training imparted remarkable dataefficiency; in the diagnostic classification task, it only needs 20% trainingdata to achieve the peak performance of all baseline models even after theywere fully fine-tuned on 100% of the data. Additionally, it achieved superiorperformance in report generation, image-text retrieval and zero-shot diagnosistasks. Our findings establish that RenalCLIP provides a robust tool with thepotential to enhance diagnostic accuracy, refine prognostic stratification, andpersonalize the management of patients with kidney cancer.</description>
      <author>example@mail.com (Yuhui Tao, Zhongwei Zhao, Zilong Wang, Xufang Luo, Feng Chen, Kang Wang, Chuanfu Wu, Xue Zhang, Shaoting Zhang, Jiaxi Yao, Xingwei Jin, Xinyang Jiang, Yifan Yang, Dongsheng Li, Lili Qiu, Zhiqiang Shao, Jianming Guo, Nengwang Yu, Shuo Wang, Ying Xiong)</author>
      <guid isPermaLink="false">2508.16569v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Modal Prototype Augmentation and Dual-Grained Prompt Learning for Social Media Popularity Prediction</title>
      <link>http://arxiv.org/abs/2508.16147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的多模态框架，用于社交媒体流行度预测，通过层次原型、对比学习、双粒度提示学习和跨模态注意力机制，解决了现有方法的局限性，实验证明该方法在基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;社交媒体流行度预测是一个复杂的多模态任务，需要有效整合图像、文本和结构化信息。然而，当前方法存在视觉-文本对齐不足的问题，无法捕捉社交媒体数据中固有的跨内容关联和层次模式。&lt;h4&gt;目的&lt;/h4&gt;克服当前方法的局限性，建立多类别框架，提高视觉-文本对齐，实现精确的多模态表示，通过细粒度类别建模进行改进。&lt;h4&gt;方法&lt;/h4&gt;建立多类别框架，引入层次原型进行结构增强，使用对比学习改善视觉-文本对齐，提出特征增强框架，集成双粒度提示学习和跨模态注意力机制，通过细粒度类别建模实现精确的多模态表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示在基准指标上达到了最先进的性能，为多模态社交媒体分析建立了新的参考标准。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效地解决了社交媒体流行度预测中的挑战，通过整合多模态信息和改进对齐机制，提高了预测性能。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体流行度预测是一个复杂的多模态任务，需要有效整合图像、文本和结构化信息。然而，当前方法存在视觉-文本对齐不足的问题，无法捕捉社交媒体数据中固有的跨内容关联和层次模式。为了克服这些局限性，我们建立了一个多类别框架，引入层次原型进行结构增强，并使用对比学习改善视觉-文本对齐。此外，我们提出了一个特征增强框架，集成双粒度提示学习和跨模态注意力机制，通过细粒度类别建模实现精确的多模态表示。实验结果在基准指标上展示了最先进的性能，为多模态社交媒体分析建立了新的参考标准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social Media Popularity Prediction is a complex multimodal task that requireseffective integration of images, text, and structured information. However,current approaches suffer from inadequate visual-textual alignment and fail tocapture the inherent cross-content correlations and hierarchical patterns insocial media data. To overcome these limitations, we establish a multi-classframework , introducing hierarchical prototypes for structural enhancement andcontrastive learning for improved vision-text alignment. Furthermore, wepropose a feature-enhanced framework integrating dual-grained prompt learningand cross-modal attention mechanisms, achieving precise multimodalrepresentation through fine-grained category modeling. Experimental resultsdemonstrate state-of-the-art performance on benchmark metrics, establishing newreference standards for multimodal social media analysis.</description>
      <author>example@mail.com (Ao Zhou, Mingsheng Tu, Luping Wang, Tenghao Sun, Zifeng Cheng, Yafeng Yin, Zhiwei Jiang, Qing Gu)</author>
      <guid isPermaLink="false">2508.16147v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing</title>
      <link>http://arxiv.org/abs/2508.15973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Habilitation \`a Diriger des Recherches (HDR) manuscript&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是作者关于计算机视觉和遥感领域标签高效学习的一系列精选贡献，重点研究如何从有限或部分标注的数据中有效学习，并利用大量未标注数据，特别关注解决地球观测数据的独特挑战。&lt;h4&gt;背景&lt;/h4&gt;研究背景是计算机视觉和遥感领域面临的标签数据稀缺问题，以及实际应用中需要处理多模态、空间分辨率变化和场景异构性等地球观测数据的特殊挑战。&lt;h4&gt;目的&lt;/h4&gt;开发并适应能够从有限或部分标注数据中有效学习的方法，同时能够利用大量未标注数据，以解决实际应用中的标签效率问题，特别是在地球观测领域。&lt;h4&gt;方法&lt;/h4&gt;研究围绕四个主要方法展开：(1)基于异常感知表示的弱监督学习；(2)多数据集联合训练的多任务学习；(3)多模态数据的自监督和监督对比学习；(4)结合类层次显式和隐式建模的少样本学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过多种方法在自然和遥感数据集上的实验，证明了所提出方法在目标发现和检测、目标检测和语义分割性能提升、遥感场景分类以及层次场景分类方面的有效性，反映了多个合作研究项目的成果。&lt;h4&gt;结论&lt;/h4&gt;标签高效学习在计算机视觉和遥感领域具有广阔的应用前景，未来的研究方向将聚焦于扩展和增强实际应用的标签高效学习技术。&lt;h4&gt;翻译&lt;/h4&gt;这篇手稿 presents a series of my selected contributions to the topic of label-efficient learning in computer vision and remote sensing. The central focus of this research is to develop and adapt methods that can learn effectively from limited or partially annotated data, and can leverage abundant unlabeled data in real-world applications. The contributions span both methodological developments and domain-specific adaptations, in particular addressing challenges unique to Earth observation data such as multi-modality, spatial resolution variability, and scene heterogeneity. The manuscript is organized around four main axes including (1) weakly supervised learning for object discovery and detection based on anomaly-aware representations learned from large amounts of background images; (2) multi-task learning that jointly trains on multiple datasets with disjoint annotations to improve performance on object detection and semantic segmentation; (3) self-supervised and supervised contrastive learning with multimodal data to enhance scene classification in remote sensing; and (4) few-shot learning for hierarchical scene classification using both explicit and implicit modeling of class hierarchies. These contributions are supported by extensive experimental results across natural and remote sensing datasets, reflecting the outcomes of several collaborative research projects. The manuscript concludes by outlining ongoing and future research directions focused on scaling and enhancing label-efficient learning for real-world applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This manuscript presents a series of my selected contributions to the topicof label-efficient learning in computer vision and remote sensing. The centralfocus of this research is to develop and adapt methods that can learneffectively from limited or partially annotated data, and can leverage abundantunlabeled data in real-world applications. The contributions span bothmethodological developments and domain-specific adaptations, in particularaddressing challenges unique to Earth observation data such as multi-modality,spatial resolution variability, and scene heterogeneity. The manuscript isorganized around four main axes including (1) weakly supervised learning forobject discovery and detection based on anomaly-aware representations learnedfrom large amounts of background images; (2) multi-task learning that jointlytrains on multiple datasets with disjoint annotations to improve performance onobject detection and semantic segmentation; (3) self-supervised and supervisedcontrastive learning with multimodal data to enhance scene classification inremote sensing; and (4) few-shot learning for hierarchical scene classificationusing both explicit and implicit modeling of class hierarchies. Thesecontributions are supported by extensive experimental results across naturaland remote sensing datasets, reflecting the outcomes of several collaborativeresearch projects. The manuscript concludes by outlining ongoing and futureresearch directions focused on scaling and enhancing label-efficient learningfor real-world applications.</description>
      <author>example@mail.com (Minh-Tan Pham)</author>
      <guid isPermaLink="false">2508.15973v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather</title>
      <link>http://arxiv.org/abs/2508.16408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型多传感器融合方法，专门针对恶劣天气条件下的自动驾驶应用，通过融合多种传感器提高了在挑战性环境中的物体检测和决策能力。&lt;h4&gt;背景&lt;/h4&gt;多模态传感器融合是自主机器人的基本能力，但现有方法在正常环境条件下表现良好，在恶劣天气（如浓雾、冰雪或污垢遮挡）条件下会失效。&lt;h4&gt;目的&lt;/h4&gt;开发专门针对恶劣天气条件的多传感器融合方法，提高自动驾驶车辆在挑战性天气条件下的可靠性，缩小理想条件与现实世界边缘情况之间的差距。&lt;h4&gt;方法&lt;/h4&gt;融合RGB、LiDAR、NIR门控相机和雷达等多种传感器，通过注意力和基于深度的混合方案融合数据，在鸟瞰图平面上进行学习优化，使用transformer解码器根据距离和可见性对模态进行加权预测检测结果。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在恶劣天气条件下显著提高了多传感器融合的可靠性，与次优方法相比，在远距离和挑战性雾天场景中，对弱势行人的平均精度提高了17.2 AP。&lt;h4&gt;结论&lt;/h4&gt;该方法有效缩小了理想条件与现实世界边缘情况之间的差距，显著提高了自动驾驶车辆在恶劣天气条件下的物体检测能力。&lt;h4&gt;翻译&lt;/h4&gt;多模态传感器融合是自主机器人的基本能力，能够在传感器失效或输入不确定的情况下进行物体检测和决策。尽管最近的融合方法在正常环境条件下表现出色，但这些方法在恶劣天气（如浓雾、冰雪或污垢遮挡）条件下会失效。我们引入了一种专门针对恶劣天气条件的新型多传感器融合方法。除了融合最近自动驾驶文献中使用的RGB和LiDAR传感器外，我们的传感器融合堆栈还能从NIR门控相机和雷达模态中学习，以应对低光照和恶劣天气。我们通过注意力和基于深度的混合方案融合多传感器数据，并在鸟瞰图平面上进行学习优化，以有效结合图像和范围特征。我们的检测由transformer解码器预测，该解码器根据距离和可见性对模态进行加权。我们证明，我们的方法提高了自动驾驶车辆在挑战性天气条件下多传感器融合的可靠性，缩小了理想条件与现实世界边缘情况之间的差距。与次优方法相比，我们的方法在远距离和挑战性雾天场景中对弱势行人的平均精度提高了17.2 AP。我们的项目页面可在https://light.princeton.edu/samfusion/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在恶劣天气条件下（如浓雾、大雪等）的3D物体检测问题。这个问题在自动驾驶领域至关重要，因为现有多传感器融合方法在理想天气条件下表现良好，但在恶劣天气下会显著失效，影响自动驾驶系统的安全性和可靠性。恶劣天气是自动驾驶面临的主要挑战之一，而准确检测远距离和弱势道路使用者在这些条件下对保障交通安全尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有多传感器融合方法在恶劣天气条件下的局限性，特别是它们过度依赖LiDAR生成查询和深度投影的弱点。然后，他们引入了门控摄像头和雷达这两种在恶劣天气下表现优异的传感器。方法设计上借鉴了现有的BEV表示方法和transformer架构，如BEVFusion和Carion等人的工作。同时，作者创新性地设计了深度引导的跨模态变换和早期融合机制，以及基于距离的多模态查询生成方法，避免了对单一传感器的过度依赖。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个传感器自适应的多模态融合框架，能够在恶劣天气条件下有效融合不同传感器的优势，并通过距离和天气感知的加权机制动态调整各传感器的重要性。整体流程包括：1)从RGB摄像头、门控摄像头、LiDAR和雷达提取特征；2)在多模态编码器中通过深度引导的跨模态变换和注意力机制融合不同传感器特征；3)在多模态解码器中生成基于距离加权的初始对象提议，并使用transformer解码器细化检测结果；4)通过匈牙利算法匹配预测和标签，并最小化分类、回归和IoU的加权损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)整合了RGB摄像头、门控摄像头、LiDAR和雷达四种传感器，实现传感器自适应融合；2)提出深度引导的跨模态变换和早期相机融合机制；3)引入多模态、基于距离的查询生成方法，避免过度依赖LiDAR；4)设计鸟瞰图平面上的自适应混合方案，结合图像特征和范围特征。相比之前的工作，SAMFusion不再局限于理想天气条件，而是专门针对恶劣天气条件优化；不再过度依赖LiDAR，而是通过距离和天气感知的加权机制动态调整各传感器的重要性；同时引入了门控摄像头这一在恶劣天气下表现优异的传感器，显著提高了系统在低光和恶劣天气条件下的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAMFusion通过创新的多传感器自适应融合框架，结合深度引导的跨模态变换和基于距离的加权机制，显著提高了自动驾驶系统在恶劣天气条件下的3D物体检测性能，特别是在远距离行人检测方面取得了突破性进展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal sensor fusion is an essential capability for autonomous robots,enabling object detection and decision-making in the presence of failing oruncertain inputs. While recent fusion methods excel in normal environmentalconditions, these approaches fail in adverse weather, e.g., heavy fog, snow, orobstructions due to soiling. We introduce a novel multi-sensor fusion approachtailored to adverse weather conditions. In addition to fusing RGB and LiDARsensors, which are employed in recent autonomous driving literature, our sensorfusion stack is also capable of learning from NIR gated camera and radarmodalities to tackle low light and inclement weather. We fuse multimodal sensordata through attentive, depth-based blending schemes, with learned refinementon the Bird's Eye View (BEV) plane to combine image and range featureseffectively. Our detections are predicted by a transformer decoder that weighsmodalities based on distance and visibility. We demonstrate that our methodimproves the reliability of multimodal sensor fusion in autonomous vehiclesunder challenging weather conditions, bridging the gap between ideal conditionsand real-world edge cases. Our approach improves average precision by 17.2 APcompared to the next best method for vulnerable pedestrians in long distancesand challenging foggy scenes. Our project page is available athttps://light.princeton.edu/samfusion/</description>
      <author>example@mail.com (Edoardo Palladin, Roland Dietze, Praveen Narayanan, Mario Bijelic, Felix Heide)</author>
      <guid isPermaLink="false">2508.16408v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</title>
      <link>http://arxiv.org/abs/2508.16201v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SpecVLM，一种针对视频大型语言模型的无训练推测解码框架，通过分阶段视频标记剪枝实现高效解码而不牺牲准确性。&lt;h4&gt;背景&lt;/h4&gt;视频大型语言模型（Vid-LLMs）在理解视频内容方面表现出色，但它们依赖密集的视频标记表示，导致在预填充和解码阶段存在大量内存和计算开销。&lt;h4&gt;目的&lt;/h4&gt;减少信息损失并无损加速Vid-LLMs的解码阶段，同时降低计算和内存需求。&lt;h4&gt;方法&lt;/h4&gt;SpecVLM利用草稿模型推测对视频标记剪枝低敏感性的发现，采用两阶段剪枝过程：第一阶段根据验证器的注意力信号选择高信息量标记；第二阶段以空间均匀方式剪枝剩余冗余标记，可剪枝最多90%的视频标记。&lt;h4&gt;主要发现&lt;/h4&gt;草稿模型的推测对视频标记剪枝具有低敏感性，使得大幅剪枝（最多90%）成为可能而不影响准确性。&lt;h4&gt;结论&lt;/h4&gt;在四个视频理解基准测试上，SpecVLM展示了有效性和鲁棒性，对于LLaVA-OneVision-72B模型实现2.68倍解码加速，对于Qwen2.5-VL-32B模型实现2.11倍加速。&lt;h4&gt;翻译&lt;/h4&gt;视频大型语言模型（Vid-LLMs）在理解视频内容方面展现出强大能力。然而，它们对密集视频标记表示的依赖在预填充和解码阶段引入了大量内存和计算开销。为了减轻近期视频标记减少方法的信息损失并无损加速Vid-LLMs的解码阶段，我们引入了SpecVLM，一种专为Vid-LLMs设计的无训练推测解码（SD）框架，结合分阶段视频标记剪枝。基于我们的新发现——草稿模型的推测对视频标记剪枝具有低敏感性，SpecVLM能够剪枝高达90%的视频标记，实现高效推测而不牺牲准确性。为实现这一目标，它执行两阶段剪枝过程：第一阶段根据验证器（目标模型）的注意力信号选择高信息量标记，第二阶段以空间均匀方式剪枝剩余的冗余标记。在四个视频理解基准上的广泛实验证明了SpecVLM的有效性和鲁棒性，对于LLaVA-OneVision-72B实现了高达2.68倍的解码加速，对于Qwen2.5-VL-32B实现了2.11倍的加速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video large language models (Vid-LLMs) have shown strong capabilities inunderstanding video content. However, their reliance on dense video tokenrepresentations introduces substantial memory and computational overhead inboth prefilling and decoding. To mitigate the information loss of recent videotoken reduction methods and accelerate the decoding stage of Vid-LLMslosslessly, we introduce SpecVLM, a training-free speculative decoding (SD)framework tailored for Vid-LLMs that incorporates staged video token pruning.Building on our novel finding that the draft model's speculation exhibits lowsensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,enabling efficient speculation without sacrificing accuracy. To achieve this,it performs a two-stage pruning process: Stage I selects highly informativetokens guided by attention signals from the verifier (target model), whileStage II prunes remaining redundant ones in a spatially uniform manner.Extensive experiments on four video understanding benchmarks demonstrate theeffectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup forQwen2.5-VL-32B.</description>
      <author>example@mail.com (Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li)</author>
      <guid isPermaLink="false">2508.16201v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning</title>
      <link>http://arxiv.org/abs/2508.15861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入XFinBench基准测试，评估大型语言模型在解决复杂金融问题上的能力，发现当前最佳模型仍显著落后于人类专家。&lt;h4&gt;背景&lt;/h4&gt;解决金融问题需要复杂的推理、多模态数据处理和广泛的技术理解，这对当前大型语言模型构成了独特挑战。&lt;h4&gt;目的&lt;/h4&gt;开发XFinBench基准测试，评估LLM解决复杂知识密集型金融问题的能力，涵盖多个研究生级别的金融主题和多模态上下文。&lt;h4&gt;方法&lt;/h4&gt;构建包含4,235个示例的XFinBench基准，确定LLM的五项核心能力（术语理解、时间推理、未来预测、情景规划和数值建模），对18个领先模型进行实验，并构建包含3,032个金融术语的知识库进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;o1是表现最好的纯文本模型，总体准确率为67.3%，但仍比人类专家低12.5个百分点，特别是在时间推理和情景规划能力方面差距显著；相关知识只给小型开源模型带来准确率提升；计算中的舍入误差和对图像中曲线位置和交点的盲视是模型表现不佳的主要原因。&lt;h4&gt;结论&lt;/h4&gt;当前LLM在解决复杂金融问题上仍有显著局限性，特别是在时间推理、情景规划和视觉上下文理解方面，需要进一步改进。&lt;h4&gt;翻译&lt;/h4&gt;解决金融问题需要复杂的推理、多模态数据处理和广泛的技术理解，这对当前大型语言模型构成了独特挑战。我们引入XFinBench，一个包含4,235个示例的新基准，用于评估LLM解决跨多个研究生级别金融主题的复杂知识密集型金融问题的能力，并具有多模态上下文。我们使用XFinBench确定了LLM的五项核心能力，即术语理解、时间推理、未来预测、情景规划和数值建模。基于XFinBench，我们对18个领先模型进行了广泛实验。结果显示，o1是表现最好的纯文本模型，总体准确率为67.3%，但仍比人类专家低12.5个百分点，特别是在时间推理和情景规划能力方面。我们进一步构建了一个包含3,032个金融术语的知识库用于知识增强分析，发现相关知识只给小型开源模型带来一致的准确率提升。此外，我们的错误分析显示，计算过程中的舍入误差和对图像中曲线位置和交点的盲视是导致模型在计算和视觉上下文问题中表现不佳的两个主要问题。代码和数据集可通过GitHub获取：https://github.com/Zhihan72/XFinBench。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solving financial problems demands complex reasoning, multimodal dataprocessing, and a broad technical understanding, presenting unique challengesfor current large language models (LLMs). We introduce XFinBench, a novelbenchmark with 4,235 examples designed to evaluate LLM's ability in solvingcomplex, knowledge-intensive financial problems across diverse graduate-levelfinance topics with multi-modal context. We identify five core capabilities ofLLMs using XFinBench, i.e, terminology understanding, temporal reasoning,future forecasting, scenario planning, and numerical modelling. Upon XFinBench,we conduct extensive experiments on 18 leading models. The result shows that o1is the best-performing text-only model with an overall accuracy of 67.3%, butstill lags significantly behind human experts with 12.5%, especially intemporal reasoning and scenario planning capabilities. We further construct aknowledge bank with 3,032 finance terms for knowledge augmentation analysis,and find that relevant knowledge to the question only brings consistentaccuracy improvements to small open-source model. Additionally, our erroranalysis reveals that rounding errors during calculation and blindness toposition and intersection of curves in the image are two primary issues leadingto model's poor performance in calculating and visual-context questions,respectively. Code and dataset are accessible via GitHub:https://github.com/Zhihan72/XFinBench.</description>
      <author>example@mail.com (Zhihan Zhang, Yixin Cao, Lizi Liao)</author>
      <guid isPermaLink="false">2508.15861v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation</title>
      <link>http://arxiv.org/abs/2508.16568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为FedMox的新框架，用于在资源受限的边缘设备上有效地适应基础模型，同时保护数据隐私。&lt;h4&gt;背景&lt;/h4&gt;基础模型表现出显著的泛化能力但需要适应下游任务，尤其在隐私敏感应用中。由于数据隐私法规，云基础模型无法直接访问私有边缘数据，限制了其适应性。联邦学习提供隐私感知替代方案，但现有方法忽略了边缘设备的计算资源有限和标记数据稀缺问题。&lt;h4&gt;目的&lt;/h4&gt;解决边缘设备计算资源有限和标记数据稀缺的挑战，引入实用的半监督联邦学习框架，使边缘设备持有未标记低分辨率数据，服务器拥有有限标记高分辨率数据。&lt;h4&gt;方法&lt;/h4&gt;提出联邦专家混合框架FedMox，通过稀疏专家混合架构处理计算和分辨率不匹配问题，使用空间路由器对齐不同分辨率特征，采用软混合策略稳定半监督学习过程。&lt;h4&gt;主要发现&lt;/h4&gt;以目标检测为例，在真实世界自动驾驶数据集上的实验表明，FedMox有效在PSSFL下适应基础模型，显著提高性能同时控制边缘设备的内存成本。&lt;h4&gt;结论&lt;/h4&gt;该工作为联邦场景中可扩展和隐私保护的基础模型适应性铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型表现出显著的泛化能力，但需要适应下游任务，特别是在隐私敏感的应用中。由于数据隐私法规，基于云的基础模型无法直接访问私有边缘数据，限制了它们的适应性。联邦学习提供了一种隐私感知的替代方案，但现有的联邦学习方法忽略了边缘设备施加的限制，即计算资源有限和标记数据稀缺。为解决这些挑战，我们引入实用的半监督联邦学习，其中边缘设备只持有未标记的低分辨率数据，而服务器拥有有限的标记高分辨率数据。在这种设置下，我们提出了联邦专家混合，一种增强联邦学习中基础模型适应性的新框架。联邦专家混合通过稀疏的专家混合架构处理计算和分辨率不匹配的挑战，使用空间路由器对齐不同分辨率的特征，并采用软混合策略稳定半监督学习。我们以目标检测为例，在真实世界自动驾驶数据集上的实验表明，联邦专家混合有效地在实用的半监督联邦学习下适应基础模型，显著提高了性能，同时边缘设备的内存成本受到限制。我们的工作为联邦场景中可扩展和隐私保护的基础模型适应性铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) exhibit remarkable generalization but requireadaptation to downstream tasks, particularly in privacy-sensitive applications.Due to data privacy regulations, cloud-based FMs cannot directly access privateedge data, limiting their adaptation. Federated learning (FL) provides aprivacy-aware alternative, but existing FL approaches overlook the constraintsimposed by edge devices -- namely, limited computational resources and thescarcity of labeled data. To address these challenges, we introduce PracticalSemi-Supervised Federated Learning (PSSFL), where edge devices hold onlyunlabeled, low-resolution data, while the server has limited labeled,high-resolution data. In this setting, we propose the Federated Mixture ofExperts (FedMox), a novel framework that enhances FM adaptation in FL. FedMoxtackles computational and resolution mismatch challenges via a sparseMixture-of-Experts architecture, employing a spatial router to align featuresacross resolutions and a Soft-Mixture strategy to stabilize semi-supervisedlearning. We take object detection as a case study, and experiments onreal-world autonomous driving datasets demonstrate that FedMox effectivelyadapts FMs under PSSFL, significantly improving performance with constrainedmemory costs on edge devices. Our work paves the way for scalable andprivacy-preserving FM adaptation in federated scenarios.</description>
      <author>example@mail.com (Guangyu Sun, Jingtao Li, Weiming Zhuang, Chen Chen, Chen Chen, Lingjuan Lyu)</author>
      <guid isPermaLink="false">2508.16568v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>An Investigation of Visual Foundation Models Robustness</title>
      <link>http://arxiv.org/abs/2508.16225v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视觉基础模型(VFMs)在计算机视觉领域广泛应用，支持多种任务如目标检测、图像分类等。这些模型利用深度学习的重要创新，在安全敏感领域需要强大的鲁棒性来促进技术信任。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(VFMs)已成为计算机视觉领域的核心组件，利用LeNet-5、AlexNet、ResNet等深度学习模型的创新，在目标检测、图像分类、分割、姿态估计和运动跟踪等任务中表现出色。这些模型在生物特征验证、自动驾驶车辆感知和医学图像分析等安全敏感领域尤为重要。&lt;h4&gt;目的&lt;/h4&gt;调查计算机视觉系统中关键的鲁棒性要求，研究如何使系统有效地适应由光照、天气条件和传感器特性等因素影响的动态环境。&lt;h4&gt;方法&lt;/h4&gt;检查常用的经验性防御和鲁棒性训练方法，分析这些方法如何增强视觉网络对实际挑战的鲁棒性，包括分布偏移、噪声和空间失真输入以及对抗性攻击。&lt;h4&gt;主要发现&lt;/h4&gt;分析了这些防御机制相关的挑战，包括网络属性和组件，用于指导消融研究，以及评估网络鲁棒性的基准指标。&lt;h4&gt;结论&lt;/h4&gt;在安全敏感领域中，视觉模型的鲁棒性对于促进技术与最终用户之间的信任至关重要，需要通过适当的防御机制和训练方法来确保系统在各种环境条件下的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型(VFMs)在计算机视觉领域变得越来越普遍，为各种任务提供支持系统，如目标检测、图像分类、分割、姿态估计和运动跟踪。VFMs利用了深度学习模型的重要创新，如LeNet-5、AlexNet、ResNet、VGGNet、InceptionNet、DenseNet、YOLO和ViT，在一系列关键计算机视觉应用中提供卓越性能。这些包括生物特征验证、自动驾驶车辆感知和医学图像分析等安全敏感领域，在这些领域中，鲁棒性对于促进技术与最终用户之间的信任至关重要。本文调查了计算机视觉系统中关键的鲁棒性要求，以有效适应受光照、天气条件和传感器特性等因素影响的动态环境。我们检查了常用的经验性防御和鲁棒性训练，以增强视觉网络对实际挑战的鲁棒性，如分布偏移、噪声和空间失真输入，以及对抗性攻击。随后，我们提供了对这些防御机制相关挑战的全面分析，包括网络属性和组件，以指导消融研究和评估网络鲁棒性的基准指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision,powering systems for diverse tasks such as object detection, imageclassification, segmentation, pose estimation, and motion tracking. VFMs arecapitalizing on seminal innovations in deep learning models, such as LeNet-5,AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliversuperior performance across a range of critical computer vision applications.These include security-sensitive domains like biometric verification,autonomous vehicle perception, and medical image analysis, where robustness isessential to fostering trust between technology and the end-users. This articleinvestigates network robustness requirements crucial in computer vision systemsto adapt effectively to dynamic environments influenced by factors such aslighting, weather conditions, and sensor characteristics. We examine theprevalent empirical defenses and robust training employed to enhance visionnetwork robustness against real-world challenges such as distributional shifts,noisy and spatially distorted inputs, and adversarial attacks. Subsequently, weprovide a comprehensive analysis of the challenges associated with thesedefense mechanisms, including network properties and components to guideablation studies and benchmarking metrics to evaluate network robustness.</description>
      <author>example@mail.com (Sandeep Gupta, Roberto Passerone)</author>
      <guid isPermaLink="false">2508.16225v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>\textsc{T-Mask}: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring</title>
      <link>http://arxiv.org/abs/2508.16207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by 26th IEEE International Conference on  Intelligent Transportation Systems ITSC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了摄像机视角变化对驾驶员监控的影响，提出了一种名为T-Mask的新方法，通过时间令牌屏蔽提高跨视角驾驶员监控的准确性。&lt;h4&gt;背景&lt;/h4&gt;摄像机视角变化是驾驶员监控中的常见障碍。深度学习和预训练基础模型通过最终层的轻量级适应展现出强大的泛化潜力，但它们对未见视角的鲁棒性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过单视角训练将图像基础模型适应到驾驶员监控任务，并评估这些模型在无需进一步适应的情况下处理未见视角的能力。&lt;h4&gt;方法&lt;/h4&gt;使用单训练视角将图像基础模型适应到驾驶员监控，直接在未见视角上评估模型。基准测试包括简单线性探针、高级探针策略，比较DINOv2和CLIP两种基础模型与PEFT和完全微调。引入T-Mask方法，一种利用时间令牌屏蔽并强调动态视频区域的图像到视频探针方法。&lt;h4&gt;主要发现&lt;/h4&gt;在Drive&amp;Act数据集上，T-Mask相比强探针基线提高了跨视角top-1准确率+1.23%，相比PEFT方法提高了+8.0%，且未增加任何参数。对于代表性不足的次要活动，在训练视角下提高了+5.42%的识别率，在跨视角设置下提高了+1.36%。&lt;h4&gt;结论&lt;/h4&gt;使用轻量级探针方法如T-Mask来适应基础模型在细粒度驾驶员观察中具有强大潜力，特别是在跨视角和低数据设置下。时间令牌选择在构建鲁棒驾驶员监控系统中至关重要。&lt;h4&gt;翻译&lt;/h4&gt;摄像机视角的变化是驾驶员监控中的常见障碍。虽然深度学习和预训练基础模型通过最终层的轻量级适应展现出强大的泛化潜力，但它们对未见视角的鲁棒性尚未得到充分探索。我们通过单训练视角将图像基础模型适应到驾驶员监控，并在无需进一步适应的情况下直接在未见视角上评估它们。我们对简单线性探针、高级探针策略进行基准测试，并将两种基础模型(DINOv2和CLIP)与参数高效微调(PEFT)和完全微调进行比较。基于这些见解，我们引入T-Mask——一种新的图像到视频探针方法，它利用时间令牌屏蔽并强调更多动态视频区域。在公共Drive&amp;Act数据集上的基准测试表明，T-Mask相比强探针基线提高了跨视角top-1准确率+1.23%，相比PEFT方法提高了+8.0%，且未增加任何参数。它对于代表性不足的次要活动特别有效，在训练视角下提高了+5.42%的识别率，在跨视角设置下提高了+1.36%。这项工作提供了令人鼓舞的证据，表明使用轻量级探针方法如T-Mask来适应基础模型在细粒度驾驶员观察中具有强大潜力，特别是在跨视角和低数据设置下。这些结果强调了在利用基础模型构建鲁棒驾驶员监控系统时时间令牌选择的重要性。代码和模型将在https://github.com/th-nesh/T-MASK上提供，以支持持续的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Changes of camera perspective are a common obstacle in driver monitoring.While deep learning and pretrained foundation models show strong potential forimproved generalization via lightweight adaptation of the final layers('probing'), their robustness to unseen viewpoints remains underexplored. Westudy this challenge by adapting image foundation models to driver monitoringusing a single training view, and evaluating them directly on unseenperspectives without further adaptation. We benchmark simple linear probes,advanced probing strategies, and compare two foundation models (DINOv2 andCLIP) against parameter-efficient fine-tuning (PEFT) and full fine-tuning.Building on these insights, we introduce \textsc{T-Mask} -- a newimage-to-video probing method that leverages temporal token masking andemphasizes more dynamic video regions. Benchmarked on the public Drive\&amp;Actdataset, \textsc{T-Mask} improves cross-view top-1 accuracy by $+1.23\%$ overstrong probing baselines and $+8.0\%$ over PEFT methods, without adding anyparameters. It proves particularly effective for underrepresented secondaryactivities, boosting recognition by $+5.42\%$ under the trained view and$+1.36\%$ under cross-view settings. This work provides encouraging evidencethat adapting foundation models with lightweight probing methods like\textsc{T-Mask} has strong potential in fine-grained driver observation,especially in cross-view and low-data settings. These results highlight theimportance of temporal token selection when leveraging foundation models tobuild robust driver monitoring systems. Code and models will be made availableat https://github.com/th-nesh/T-MASK to support ongoing research.</description>
      <author>example@mail.com (Thinesh Thiyakesan Ponbagavathi, Kunyu Peng, Alina Roitberg)</author>
      <guid isPermaLink="false">2508.16207v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Ensemble learning of foundation models for precision oncology</title>
      <link>http://arxiv.org/abs/2508.16085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A conceptual evaluation work; more studies are in progress; examples  are here (https://github.com/lilab-stanford/ELF)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ELF（Ensemble Learning of Foundation models）框架，通过整合五种最先进的病理学基础模型，生成统一的切片级表征。ELF在跨越20个解剖部位的53,699个全切片图像上训练，利用集成学习捕捉不同模型的互补信息，同时保持高数据效率。在多种临床应用评估中，ELF表现出优于所有组成基础模型和现有切片级模型的准确性和稳健性。&lt;h4&gt;背景&lt;/h4&gt;病理组织学对疾病诊断和治疗决策至关重要。人工智能的最新进展使得病理学基础模型能够从大规模全切片图像中学习丰富的视觉表征。然而，现有模型通常在不同数据集上使用不同策略训练，导致性能不一致和泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;引入ELF框架，整合五种最先进的病理学基础模型，生成统一的切片级表征，以提高病理学AI模型的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;ELF在跨越20个解剖部位的53,699个全切片图像上训练，利用集成学习捕捉不同模型的互补信息。与传统的块级模型不同，ELF采用切片级架构，特别适合数据有限的临床环境。研究在多种临床应用中评估了ELF，包括疾病分类、生物标志物检测以及对抗癌疗法、化疗、靶向治疗和免疫治疗的反应预测。&lt;h4&gt;主要发现&lt;/h4&gt;ELF在所有评估的临床应用中，始终优于所有组成的基础模型和现有的切片级模型，展现出更高的准确性和稳健性。&lt;h4&gt;结论&lt;/h4&gt;研究结果突出了集成学习对病理学基础模型的力量，ELF作为推进AI辅助精准肿瘤学的一种可扩展和可泛化的解决方案具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;病理组织学对疾病诊断和治疗决策至关重要。人工智能的最新进展使得病理学基础模型能够从大规模全切片图像中学习丰富的视觉表征。然而，现有模型通常在不同的数据集上使用不同的策略进行训练，导致性能不一致和泛化能力有限。在此，我们引入ELF（Ensemble Learning of Foundation models），一种新颖的框架，整合了五种最先进的病理学基础模型以生成统一的切片级表征。ELF在跨越20个解剖部位的53,699个全切片图像上训练，利用集成学习捕捉来自不同模型的互补信息，同时保持高数据效率。与传统的块级模型不同，ELF的切片级架构在数据有限的临床环境中特别有利，例如治疗反应预测。我们在多种临床应用中评估了ELF，包括疾病分类、生物标志物检测以及对主要抗癌疗法、细胞毒性化疗、靶向治疗和免疫治疗的反应预测，涵盖多种癌症类型。ELF始终优于所有组成的基础模型和现有的切片级模型，展现出更高的准确性和稳健性。我们的结果突出了集成学习对病理学基础模型的力量，并建议ELF是推进AI辅助精准肿瘤学的一种可扩展和可泛化的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Histopathology is essential for disease diagnosis and treatmentdecision-making. Recent advances in artificial intelligence (AI) have enabledthe development of pathology foundation models that learn rich visualrepresentations from large-scale whole-slide images (WSIs). However, existingmodels are often trained on disparate datasets using varying strategies,leading to inconsistent performance and limited generalizability. Here, weintroduce ELF (Ensemble Learning of Foundation models), a novel framework thatintegrates five state-of-the-art pathology foundation models to generateunified slide-level representations. Trained on 53,699 WSIs spanning 20anatomical sites, ELF leverages ensemble learning to capture complementaryinformation from diverse models while maintaining high data efficiency. Unliketraditional tile-level models, ELF's slide-level architecture is particularlyadvantageous in clinical contexts where data are limited, such as therapeuticresponse prediction. We evaluated ELF across a wide range of clinicalapplications, including disease classification, biomarker detection, andresponse prediction to major anticancer therapies, cytotoxic chemotherapy,targeted therapy, and immunotherapy, across multiple cancer types. ELFconsistently outperformed all constituent foundation models and existingslide-level models, demonstrating superior accuracy and robustness. Our resultshighlight the power of ensemble learning for pathology foundation models andsuggest ELF as a scalable and generalizable solution for advancing AI-assistedprecision oncology.</description>
      <author>example@mail.com (Xiangde Luo, Xiyue Wang, Feyisope Eweje, Xiaoming Zhang, Sen Yang, Ryan Quinton, Jinxi Xiang, Yuchen Li, Yuanfeng Ji, Zhe Li, Yijiang Chen, Colin Bergstrom, Ted Kim, Francesca Maria Olguin, Kelley Yuan, Matthew Abikenari, Andrew Heider, Sierra Willens, Sanjeeth Rajaram, Robert West, Joel Neal, Maximilian Diehn, Ruijiang Li)</author>
      <guid isPermaLink="false">2508.16085v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Training a Foundation Model for Materials on a Budget</title>
      <link>http://arxiv.org/abs/2508.16067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Nequix是一种高效的E(3)-等变势能模型，通过简化的设计和现代训练实践，在保持高准确性的同时大幅降低了计算需求，训练成本不到大多数方法四分之一，推理速度比当前顶级模型快十倍。&lt;h4&gt;背景&lt;/h4&gt;材料建模的基础模型发展迅速，但训练成本高昂，使得最先进的方法难以被许多研究团队使用。&lt;h4&gt;目的&lt;/h4&gt;开发一个成本效益更高的材料建模模型，在保持准确性的同时大幅降低计算需求。&lt;h4&gt;方法&lt;/h4&gt;介绍Nequix，一个紧凑的E(3)-等变势能模型，结合简化的NequIP设计和现代训练实践，包括等变均方根层归一化和Muon优化器，使用JAX构建，有70万个参数，在500个A100-GPU小时内完成训练。&lt;h4&gt;主要发现&lt;/h4&gt;在Matbench-Discovery和MDR Phonon基准测试中，Nequix总体排名第三，训练成本不到大多数其他方法四分之一，推理速度比当前排名第一的模型快一个数量级。&lt;h4&gt;结论&lt;/h4&gt;Nequix在保持高准确性的同时显著降低了计算需求，模型权重和完全可复现的代码已在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;材料建模的基础模型正在快速发展，但它们的训练仍然昂贵，通常使得最先进的方法难以被许多研究团队企及。我们介绍了Nequix，一个紧凑的E(3)-等变势能模型，它结合了简化的NequIP设计和现代训练实践，包括等变均方根层归一化和Muon优化器，在保持准确性的同时大幅降低了计算需求。使用JAX构建，Nequix有70万个参数，并在500个A100-GPU小时内完成训练。在Matbench-Discovery和MDR Phonon基准测试中，Nequix总体排名第三，同时需要的训练成本不到大多数其他方法四分之一，并且其推理速度比当前排名第一的模型快一个数量级。我们在https://github.com/atomicarchitects/nequix上发布了模型权重和完全可复现的代码库。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for materials modeling are advancing quickly, but theirtraining remains expensive, often placing state-of-the-art methods out of reachfor many research groups. We introduce Nequix, a compact E(3)-equivariantpotential that pairs a simplified NequIP design with modern training practices,including equivariant root-mean-square layer normalization and the Muonoptimizer, to retain accuracy while substantially reducing computerequirements. Built in JAX, Nequix has 700K parameters and was trained in 500A100-GPU hours. On the Matbench-Discovery and MDR Phonon benchmarks, Nequixranks third overall while requiring less than one quarter of the training costof most other methods, and it delivers an order-of-magnitude faster inferencespeed than the current top-ranked model. We release model weights and fullyreproducible codebase at https://github.com/atomicarchitects/nequix</description>
      <author>example@mail.com (Teddy Koker, Tess Smidt)</author>
      <guid isPermaLink="false">2508.16067v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting</title>
      <link>http://arxiv.org/abs/2508.16059v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了多层可转向嵌入融合(MSEF)框架，解决大型语言模型在时间序列预测中的浅层集成问题，使模型能够直接访问所有深度的时序模式，从而减轻深层中时序信息的逐渐丢失。&lt;h4&gt;背景&lt;/h4&gt;时间序列数据在各应用领域普遍存在，使时间序列预测成为基础任务。随着大型语言模型的进步，各种方法已被开发出来以适应LLMs用于时间序列预测。然而，现有方法在时序信息集成上存在固有局限，LLMs通常只能在浅层访问时序表示。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中时序信息在深层逐渐减弱的问题，使LLMs能够在所有深度直接访问时间序列模式，并实现文本嵌入与时序表示之间的有效适应。&lt;h4&gt;方法&lt;/h4&gt;提出多层可转向嵌入融合(MSEF)框架，利用现成的时序基础模型提取语义丰富的嵌入，并通过特定层的转向向量将这些嵌入与LLM各层的中间文本表示融合。这些转向向量旨在持续优化时序和文本模态之间的对齐，并确保特定层的适应机制，促进高效的少样本学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;在七个基准测试上的实验结果表明，MSEF相比基线方法有显著的性能提升，平均均方误差降低了31.8%。&lt;h4&gt;结论&lt;/h4&gt;MSEF框架有效解决了LLMs在时间序列预测中的浅层集成问题，通过直接访问所有深度的时序模式，显著提高了预测性能。&lt;h4&gt;翻译&lt;/h4&gt;时间序列数据在各应用领域普遍存在，使时间序列预测成为一项基础任务。随着大型语言模型的惊人进展，各种方法已被开发出来以适应LLMs用于时间序列预测。尽管解锁了LLMs在理解时序数据方面的潜力，但现有方法在时序信息的集成上存在固有局限，LLMs通常只能在浅层（主要是输入层）访问时序表示。这导致时序表示的影响在深层中逐渐减弱，最终导致文本嵌入与时序表示之间的无效适应。在本文中，我们提出了多层可转向嵌入融合(MSEF)，这是一种新框架，使LLMs能够直接访问所有深度的时序模式，从而减轻深层中时序信息的逐渐丢失。具体而言，MSEF利用现成的时序基础模型提取语义丰富的嵌入，这些嵌入通过特定层的转向向量与LLM各层的中间文本表示融合。这些转向向量旨在持续优化时序和文本模态之间的对齐，并促进特定层的适应机制，确保高效的少样本学习能力。在七个基准测试上的实验结果表明，MSEF相比基线方法有显著的性能提升，平均均方误差降低了31.8%。代码可在https://github.com/One1sAll/MSEF获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760803&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series (TS) data are ubiquitous across various application areas,rendering time series forecasting (TSF) a fundamental task. With the astoundingadvances in large language models (LLMs), a variety of methods have beendeveloped to adapt LLMs for time series forecasting. Despite unlocking thepotential of LLMs in comprehending TS data, existing methods are inherentlyconstrained by their shallow integration of TS information, wherein LLMstypically access TS representations at shallow layers, primarily at the inputlayer. This causes the influence of TS representations to progressively fade indeeper layers and eventually leads to ineffective adaptation between textualembeddings and TS representations. In this paper, we propose the Multi-layerSteerable Embedding Fusion (MSEF), a novel framework that enables LLMs todirectly access time series patterns at all depths, thereby mitigating theprogressive loss of TS information in deeper layers. Specifically, MSEFleverages off-the-shelf time series foundation models to extract semanticallyrich embeddings, which are fused with intermediate text representations acrossLLM layers via layer-specific steering vectors. These steering vectors aredesigned to continuously optimize the alignment between time series and textualmodalities and facilitate a layer-specific adaptation mechanism that ensuresefficient few-shot learning capabilities. Experimental results on sevenbenchmarks demonstrate significant performance improvements by MSEF comparedwith baselines, with an average reduction of 31.8% in terms of MSE. The code isavailable at https://github.com/One1sAll/MSEF.</description>
      <author>example@mail.com (Zhuomin Chen, Dan Li, Jiahui Zhou, Shunyu Wu, Haozheng Ye, Jian Lou, See-Kiong Ng)</author>
      <guid isPermaLink="false">2508.16059v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Generative Foundation Model for Structured and Unstructured Electronic Health Records</title>
      <link>http://arxiv.org/abs/2508.16054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了生成式深度患者(GDP)模型，一种多模态基础模型，能够同时处理结构化和非结构化电子健康记录数据，实现临床预测和临床叙述生成的高性能表现。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录(EHRs)是丰富的临床数据源，包含结构化元素（人口统计、生命体征、实验室结果、代码）和非结构化临床笔记等多种数据模式。利用这种异构性对改善患者预后至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从多种数据模式中学习并支持临床任务的基础模型，同时保留电子健康记录中的时间和定量细节。&lt;h4&gt;方法&lt;/h4&gt;引入GDP模型，通过CNN-Transformer编码器原生编码结构化的EHR时间序列，并通过跨模态注意力与未结构化的EHR融合到基于LLaMA的解码器中。GDP分为两个阶段训练：(1)生成式预训练，学习从原始患者时间线生成临床叙述，同时执行掩码特征预测和下一个时间步预测；(2)多任务微调，用于临床有意义的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在临床预测方面，GDP在MIMIC-IV上表现出色：心力衰竭AUROC为0.923，2型糖尿病AUROC为0.817，30天再入院AUROC为0.627。在叙述生成方面，GDP实现了ROUGE-L为0.135和BERTScore-F1为0.545。盲人评估中，GDP-Instruct在忠实度、流畅度和整体临床实用性方面得分最高，表明可减少医院文档工作负担而不牺牲准确性。&lt;h4&gt;结论&lt;/h4&gt;单个多模态基础模型可以同时预测临床可操作事件和生成高质量的临床叙述。GDP的灵活架构可扩展到其他模态。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录是丰富的临床数据源，但也是复杂的患者数据存储库，包含结构化元素（人口统计、生命体征、实验室结果、代码）、非结构化临床笔记和其他数据模式。利用这种异构性对于改善患者预后至关重要。大型语言模型的最新进展已经能够从多种数据模式中学习的基础模型，并支持临床任务。然而，大多数当前方法简单地将数值EHR数据序列化为文本，这可能会丢失时间和定量细节。我们引入了生成式深度患者(GDP)，一种多模态基础模型，它通过CNN-Transformer编码器原生编码结构化的EHR时间序列，并通过跨模态注意力与未结构化的EHR融合到基于LLaMA的解码器中。GDP分两个阶段训练：(1)生成式预训练，学习从原始患者时间线产生临床叙述，同时执行掩码特征预测和下一个时间步预测以捕获时间动态；(2)多任务微调用于临床有意义的预测（如心力衰竭、2型糖尿病、30天再入院）。在临床预测中，GDP在MIMIC-IV上表现出色：心力衰竭AUROC为0.923，2型糖尿病AUROC为0.817，30天再入院AUROC为0.627。对于叙述生成，GDP实现了ROUGE-L为0.135和BERTScore-F1为0.545。在盲人评估中，GDP-Instruct在忠实度、流畅度和整体临床实用性方面得分最高，表明在不牺牲准确性的情况下减少了医院文档工作负担。我们的结果表明，单个多模态基础模型可以预测临床可操作事件并生成高质量的临床叙述。此外，GDP的灵活架构可以扩展到其他模态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic health records (EHRs) are rich clinical data sources but complexrepositories of patient data, spanning structured elements (demographics,vitals, lab results, codes), unstructured clinical notes and other modalitiesof data. Harnessing this heterogeneity is critical for improving patientoutcomes. Recent advances in large language models (LLMs) have enabledfoundation models that can learn from multiple data modalities and supportclinical tasks. However, most current approaches simply serialize numeric EHRdata into text, which risks losing temporal and quantitative detail. Weintroduce Generative Deep Patient (GDP), a multimodal foundation model thatnatively encodes structured EHR time-series via a CNN-Transformer encoder andfuses it with unstructured EHRs through cross-modal attention into aLLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,where it learns to produce clinical narratives from raw patient timelines whilealso performing masked feature prediction (MFP) and next time-step prediction(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning forclinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-dayreadmission). In clinical prediction, GDP demonstrated superior performance onMIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and30-day readmission AUROC = 0.627. For narrative generation, GDP achievedROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,GDP-Instruct scored highest on faithfulness, fluency, and overall clinicalutility, suggesting reduced hospital documentation workload without sacrificingaccuracy. Our results demonstrate that a single multimodal foundation model canboth predict clinically actionable events and generate high-quality clinicalnarratives. Furthermore, GDP's flexible architecture can be extended toadditional modalities.</description>
      <author>example@mail.com (Sonish Sivarajkumar, Hang Zhang, Yuelyu Ji, Maneesh Bilalpur, Xizhi Wu, Chenyu Li, Min Gu Kwak, Shyam Visweswaran, Yanshan Wang)</author>
      <guid isPermaLink="false">2508.16054v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset</title>
      <link>http://arxiv.org/abs/2508.15986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 6 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一个基于深度学习的视网膜疾病分类系统，利用合成数据集SynFundus-1M克服了真实临床数据稀缺的问题，并通过多种模型架构和集成方法实现了高准确率和良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视网膜疾病分类的多标签深度学习模型常因患者隐私问题和成本高昂而缺乏大型专家标注的临床数据集。最近发布的SynFundus-1M高保真合成数据集（包含超过一百万张眼底图像）为克服这些障碍提供了新机会。&lt;h4&gt;目的&lt;/h4&gt;为SynFundus-1M新资源建立基础性能基准，开发端到端的深度学习流程，训练模型分类11种视网膜疾病。&lt;h4&gt;方法&lt;/h4&gt;开发端到端深度学习流程，训练六种现代架构（ConvNeXtV2、SwinV2、ViT、ResNet、EfficientNetV2和RETFound基础模型），使用5折多标签分层交叉验证策略，并通过将折叠外预测与XGBoost分类器堆叠开发元集成模型。&lt;h4&gt;主要发现&lt;/h4&gt;最终集成模型在内部验证集上实现宏平均AUC为0.9973的最佳性能；模型在三个真实临床数据集上表现出强大泛化能力：组合DR数据集AUC为0.7972，AIROGS青光眼数据集AUC为0.9126，多标签RFMiD数据集宏平均AUC为0.8800。&lt;h4&gt;结论&lt;/h4&gt;该研究为大规模合成数据集的未来研究提供了强大基线，证明完全在合成数据上训练的模型可准确分类多种病理并有效泛化到真实临床图像，为加速眼科领域全面AI系统发展提供了可行途径。&lt;h4&gt;翻译&lt;/h4&gt;视网膜疾病分类的多标签深度学习模型的发展常因患者隐私问题和成本高昂而受到缺乏大型专家标注临床数据集的限制。最近发布的SynFundus-1M高保真合成数据集（包含超过一百万张眼底图像）为克服这些障碍提供了新机会。为了为这一新资源建立基础性能基准，我们开发了一个端到端的深度学习流程，训练了六种现代架构（ConvNeXtV2、SwinV2、ViT、ResNet、EfficientNetV2和RETFound基础模型）使用5折多标签分层交叉验证策略来分类11种视网膜疾病。我们进一步通过将折叠外预测与XGBoost分类器堆叠开发了一个元集成模型。我们的最终集成模型在内部验证集上实现了最佳性能，宏平均ROC曲线下面积为0.9973。关键的是，这些模型在三个不同的真实世界临床数据集上表现出强大的泛化能力，在组合DR数据集上达到0.7972的AUC，在AIROGS青光眼数据集上达到0.9126的AUC，在多标签RFMiD数据集上达到0.8800的宏平均AUC。这项工作为未来关于大规模合成数据集的研究提供了强大的基线，并证明了完全在合成数据上训练的模型可以准确分类多种病理，并有效地泛化到真实的临床图像，为加速眼科领域全面AI系统的发展提供了可行的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of multi-label deep learning models for retinal diseaseclassification is often hindered by the scarcity of large, expertly annotatedclinical datasets due to patient privacy concerns and high costs. The recentrelease of SynFundus-1M, a high-fidelity synthetic dataset with over onemillion fundus images, presents a novel opportunity to overcome these barriers.To establish a foundational performance benchmark for this new resource, wedeveloped an end-to-end deep learning pipeline, training six modernarchitectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and theRETFound foundation model) to classify eleven retinal diseases using a 5-foldmulti-label stratified cross-validation strategy. We further developed ameta-ensemble model by stacking the out-of-fold predictions with an XGBoostclassifier. Our final ensemble model achieved the highest performance on theinternal validation set, with a macro-average Area Under the Receiver OperatingCharacteristic Curve (AUC) of 0.9973. Critically, the models demonstratedstrong generalization to three diverse, real-world clinical datasets, achievingan AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGSglaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset.This work provides a robust baseline for future research on large-scalesynthetic datasets and establishes that models trained exclusively on syntheticdata can accurately classify multiple pathologies and generalize effectively toreal clinical images, offering a viable pathway to accelerate the developmentof comprehensive AI systems in ophthalmology.</description>
      <author>example@mail.com (Jerry Cao-Xue, Tien Comlekoglu, Keyi Xue, Guanliang Wang, Jiang Li, Gordon Laurie)</author>
      <guid isPermaLink="false">2508.15986v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation</title>
      <link>http://arxiv.org/abs/2508.15972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at the Conference on Robot Learning (CoRL) 2025. For more  details please visit https://frankzhaodong.github.io/UnPose&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UnPose是一种无需物体CAD模型的零样本6D物体姿态估计和重建框架，利用预训练扩散模型的3D先验和不确定性估计，从单视图RGB-D帧开始，通过多视图扩散模型估计初始3D模型，并随着更多观察的加入逐步改进，最终在6D姿态估计准确性和3D重建质量方面显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;估计新物体的6D姿态是机器人学中的基本挑战，传统方法依赖物体CAD模型，但获取这些模型成本高且不切实际。&lt;h4&gt;目的&lt;/h4&gt;提出UnPose框架，实现无需模型、零样本的6D物体姿态估计和重建，解决现有方法需要额外训练或产生几何幻觉的问题。&lt;h4&gt;方法&lt;/h4&gt;从单视图RGB-D帧开始，使用多视图扩散模型估计初始3D模型(使用3D高斯溅射表示)和像素级认识不确定性估计，随着更多观察的加入，在扩散模型不确定性指导下增量式改进3DGS模型，并通过姿态图优化确保全局一致性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明UnPose在6D姿态估计准确性和3D重建质量方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;UnPose在实际机器人操作任务中展示了良好的实际应用性。&lt;h4&gt;翻译&lt;/h4&gt;估计新物体的6D姿态是机器人学中的一个基本但具有挑战性的问题，通常依赖于获取物体CAD模型。然而，获取此类模型可能成本高昂且不切实际。最近的方法试图通过利用基础模型的强先验从单视图或多视图图像重建物体来绕过这一要求，但通常需要额外训练或产生几何幻觉。为此，我们提出了UnPose，一种新颖的零样本、无需模型的6D物体姿态估计和重建框架，它利用预训练扩散模型的3D先验和不确定性估计。具体来说，从单视图RGB-D帧开始，UnPose使用多视图扩散模型估计初始3D模型(使用3D高斯溅射表示)，并提供像素级认识不确定性估计。随着更多观察的可用，我们在扩散模型的不确定性指导下融合新视图，从而逐步改进3DGS模型，持续提高姿态估计准确性和3D重建质量。为确保全局一致性，将扩散先验生成的视图和后续观察进一步集成到姿态图中，并联合优化为一个连贯的3DGS场。大量实验证明，UnPose在6D姿态估计准确性和3D重建质量方面显著优于现有方法。我们进一步展示了它在实际机器人操作任务中的实际应用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the 6D pose of novel objects is a fundamental yet challengingproblem in robotics, often relying on access to object CAD models. However,acquiring such models can be costly and impractical. Recent approaches aim tobypass this requirement by leveraging strong priors from foundation models toreconstruct objects from single or multi-view images, but typically requireadditional training or produce hallucinated geometry. To this end, we proposeUnPose, a novel framework for zero-shot, model-free 6D object pose estimationand reconstruction that exploits 3D priors and uncertainty estimates from apre-trained diffusion model. Specifically, starting from a single-view RGB-Dframe, UnPose uses a multi-view diffusion model to estimate an initial 3D modelusing 3D Gaussian Splatting (3DGS) representation, along with pixel-wiseepistemic uncertainty estimates. As additional observations become available,we incrementally refine the 3DGS model by fusing new views guided by thediffusion model's uncertainty, thereby continuously improving the poseestimation accuracy and 3D reconstruction quality. To ensure globalconsistency, the diffusion prior-generated views and subsequent observationsare further integrated in a pose graph and jointly optimized into a coherent3DGS field. Extensive experiments demonstrate that UnPose significantlyoutperforms existing approaches in both 6D pose estimation accuracy and 3Dreconstruction quality. We further showcase its practical applicability inreal-world robotic manipulation tasks.</description>
      <author>example@mail.com (Zhaodong Jiang, Ashish Sinha, Tongtong Cao, Yuan Ren, Bingbing Liu, Binbin Xu)</author>
      <guid isPermaLink="false">2508.15972v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification</title>
      <link>http://arxiv.org/abs/2508.15960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个名为Glo-VLMs的系统框架，用于将预训练的视觉-语言模型适应于细粒度肾小球分类任务，即使在数据有限的情况下也能取得良好效果。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型在数字病理学中显示出巨大潜力，但对于细粒度的疾病特异性分类任务（如区分肾小球亚型）效果有限。肾小球亚型之间的细微形态差异，以及视觉模式与精确临床术语对齐的困难，使得肾脏病理学中的自动诊断特别具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;探索如何有效地将大型预训练视觉-语言模型适应于细粒度肾小球分类任务，即使在只有少量标记可用样本的情况下也能有效工作。&lt;h4&gt;方法&lt;/h4&gt;引入Glo-VLMs系统框架，利用精选的病理学图像和临床文本提示来促进联合图像-文本表示学习，评估在各种视觉-语言模型架构和适应策略下的性能，在少样本学习范式下探索方法选择和标记数据量对模型性能的影响，并使用标准化的多类指标进行公平比较。&lt;h4&gt;主要发现&lt;/h4&gt;使用每类仅8个样本的微调视觉-语言模型实现了0.7416的准确率、0.9045的宏AUC和0.5277的F1分数，表明即使在高度有限的监督下，基础模型也能有效地适应细粒度医学图像分类。&lt;h4&gt;结论&lt;/h4&gt;基础模型可以在高度有限的监督下有效地适应细粒度医学图像分类，为专业临床研究应用中大型预训练模型的实际要求和潜力提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型在数字病理学中显示出巨大潜力，但其对于细粒度的疾病特异性分类任务（如区分肾小球亚型）的有效性仍然有限。这些亚型之间的细微形态差异，加上视觉模式与精确临床术语对齐的困难，使得肾脏病理学中的自动诊断特别具有挑战性。在这项工作中，我们探索了如何有效地将大型预训练视觉-语言模型适应于细粒度肾小球分类任务，即使在只有少量标记样本可用的情况下。在这项工作中，我们引入了Glo-VLMs，这是一个系统框架，旨在探索在数据受限情况下将视觉-语言模型适应于细粒度肾小球分类。我们的方法利用精选的病理学图像和临床文本提示，促进针对细微肾脏病理亚型的联合图像-文本表示学习。通过在少样本学习范式下评估各种视觉-语言模型架构和适应策略，我们探索了方法选择和标记数据量在临床相关场景中对模型性能的影响。为确保公平比较，我们使用标准化的多类指标评估所有模型，旨在阐明大型预训练模型在专业临床研究应用中的实际要求和潜力。结果表明，仅使用每类8个样本微调视觉-语言模型，就实现了0.7416的准确率、0.9045的宏AUC和0.5277的F1分数，证明即使在高度有限的监督下，基础模型也能有效地适应细粒度医学图像分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) have shown considerable potential in digitalpathology, yet their effectiveness remains limited for fine-grained,disease-specific classification tasks such as distinguishing between glomerularsubtypes. The subtle morphological variations among these subtypes, combinedwith the difficulty of aligning visual patterns with precise clinicalterminology, make automated diagnosis in renal pathology particularlychallenging. In this work, we explore how large pretrained VLMs can beeffectively adapted to perform fine-grained glomerular classification, even inscenarios where only a small number of labeled examples are available. In thiswork, we introduce Glo-VLMs, a systematic framework designed to explore theadaptation of VLMs to fine-grained glomerular classification indata-constrained settings. Our approach leverages curated pathology imagesalongside clinical text prompts to facilitate joint image-text representationlearning for nuanced renal pathology subtypes. By assessing various VLMsarchitectures and adaptation strategies under a few-shot learning paradigm, weexplore how both the choice of method and the amount of labeled data impactmodel performance in clinically relevant scenarios. To ensure a faircomparison, we evaluate all models using standardized multi-class metrics,aiming to clarify the practical requirements and potential of large pretrainedmodels for specialized clinical research applications. As a result, fine-tuningthe VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score withonly 8 shots per class, demonstrating that even with highly limitedsupervision, foundation models can be effectively adapted for fine-grainedmedical image classification.</description>
      <author>example@mail.com (Zhenhao Guo, Rachit Saluja, Tianyuan Yao, Quan Liu, Yuankai Huo, Benjamin Liechty, David J. Pisapia, Kenji Ikemura, Mert R. Sabuncu, Yihe Yang, Ruining Deng)</author>
      <guid isPermaLink="false">2508.15960v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare Cancer Subtyping</title>
      <link>http://arxiv.org/abs/2508.15904v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为PathPT的新型框架，用于罕见癌症的诊断。该框架利用视觉语言病理学基础模型，通过空间感知的视觉聚合和任务特定的提示调优，解决了现有方法在罕见癌症诊断中的局限性。&lt;h4&gt;背景&lt;/h4&gt;罕见癌症占所有恶性肿瘤的20-25%，但在儿科肿瘤学中占比超过70%。由于专家资源有限，罕见癌症面临重大诊断挑战。现有的病理视觉语言基础模型在常见癌症亚型分类中显示出有希望的零样本能力，但在罕见癌症方面的临床性能仍然有限。现有的多实例学习(MIL)方法仅依赖视觉特征，忽略了跨模态知识，影响了罕见癌症诊断的可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够充分利用视觉语言病理学基础模型潜力的新框架，解决罕见癌症诊断中的挑战，提高诊断准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出PathPT框架，通过空间感知的视觉聚合和任务特定的提示调优。与传统MIL不同，PathPT利用VL模型的零样本能力，将WSI级别的监督转换为细粒度的tile级别指导，保留癌症区域定位，并通过与组织病理学语义对齐的提示实现跨模态推理。在8种罕见癌症数据集和3种常见癌症数据集上进行了基准测试，评估了4种最先进的VL模型和4种MIL框架在3种少样本设置下的性能。&lt;h4&gt;主要发现&lt;/h4&gt;PathPT在多种设置下持续提供卓越的性能，在亚型分类准确性和癌症区域定位能力方面取得显著提升。在56种亚型和2,910个WSI上验证了方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;这项工作推进了AI辅助的罕见癌症诊断，为在专业专业知识有限的情况下提高亚型分类准确性提供了一个可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;罕见癌症占所有恶性肿瘤的20-25%，但由于专家资源有限，特别是儿科肿瘤学中罕见癌症占比超过70%，因此面临重大诊断挑战。虽然病理视觉语言(VL)基础模型在常见癌症亚型分类中显示出有希望的零样本能力，但它们在罕见癌症方面的临床性能仍然有限。现有的多实例学习(MIL)方法仅依赖视觉特征，忽略了跨模态知识，影响了罕见癌症诊断中至关重要的可解释性。为解决这一局限，我们提出了PathPT，一种通过空间感知的视觉聚合和任务特定的提示调优来充分利用视觉语言病理学基础模型潜力的新框架。与传统MIL不同，PathPT利用VL模型的零样本能力，将WSI级别的监督转换为细粒度的tile级别指导，从而保留癌症区域的定位，并通过与组织病理学语义对齐的提示实现跨模态推理。我们在八种罕见癌症数据集（四种成人，四种儿科）上对PathPT进行了基准测试，涵盖56种亚型和2,910个WSI，以及三种常见癌症数据集，评估了四种最先进的VL模型和四种MIL框架在三种少样本设置下的性能。结果表明，PathPT持续提供卓越的性能，在亚型分类准确性和癌症区域定位能力方面取得显著提升。这项工作推进了AI辅助的罕见癌症诊断，为在专业专业知识有限的情况下提高亚型分类准确性提供了一个可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rare cancers comprise 20-25% of all malignancies but face major diagnosticchallenges due to limited expert availability-especially in pediatric oncology,where they represent over 70% of cases. While pathology vision-language (VL)foundation models show promising zero-shot capabilities for common cancersubtyping, their clinical performance for rare cancers remains limited.Existing multi-instance learning (MIL) methods rely only on visual features,overlooking cross-modal knowledge and compromising interpretability criticalfor rare cancer diagnosis. To address this limitation, we propose PathPT, anovel framework that fully exploits the potential of vision-language pathologyfoundation models through spatially-aware visual aggregation and task-specificprompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervisioninto fine-grained tile-level guidance by leveraging the zero-shot capabilitiesof VL models, thereby preserving localization on cancerous regions and enablingcross-modal reasoning through prompts aligned with histopathological semantics.We benchmark PathPT on eight rare cancer datasets(four adult and fourpediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancerdatasets, evaluating four state-of-the-art VL models and four MIL frameworksunder three few-shot settings. Results show that PathPT consistently deliverssuperior performance, achieving substantial gains in subtyping accuracy andcancerous region grounding ability. This work advances AI-assisted diagnosisfor rare cancers, offering a scalable solution for improving subtyping accuracyin settings with limited access to specialized expertise.</description>
      <author>example@mail.com (Dexuan He, Xiao Zhou, Wenbin Guan, Liyuan Zhang, Xiaoman Zhang, Sinuo Xu, Ge Wang, Lifeng Wang, Xiaojun Yuan, Xin Sun, Yanfeng Wang, Kun Sun, Ya Zhang, Weidi Xie)</author>
      <guid isPermaLink="false">2508.15904v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Intern-S1: A Scientific Multimodal Foundation Model</title>
      <link>http://arxiv.org/abs/2508.15763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Intern-S1，一个专门针对科学领域的多模态专家混合模型，通过算法、数据和训练系统的创新，在科学领域表现出色，特别是在专业任务上超越了闭源最先进模型。&lt;h4&gt;背景&lt;/h4&gt;近年来，大量开源基础模型在广泛关注领域取得了接近闭源模型的性能，但在高价值且更具挑战性的科学专业领域，进展显著落后，开源模型与闭源模型之间存在巨大差距，这些领域仍然依赖专家模型。&lt;h4&gt;目的&lt;/h4&gt;弥合开源模型与闭源模型在科学领域的差距，并向通用人工智能(AGI)迈进一步。&lt;h4&gt;方法&lt;/h4&gt;提出Intern-S1，一个多模态专家混合(MoE)模型，拥有280亿激活参数和2410亿总参数，在5T tokens上持续预训练，其中包括超过2.5T tokens来自科学领域。在后训练阶段，在InternBootCamp中经历离线和在线强化学习，提出奖励混合(MoR)方法，同时在1000多个任务上协同RL训练。&lt;h4&gt;主要发现&lt;/h4&gt;在综合评估基准上，Intern-S1在开源模型中展现出具有竞争力的通用推理性能，在科学领域显著优于开源模型，在分子合成规划、反应条件预测、预测晶体热力学稳定性等专业任务上超越闭源最先进模型。&lt;h4&gt;结论&lt;/h4&gt;通过算法、数据和训练系统的综合创新，Intern-S1成功缩小了开源模型与闭源模型在科学领域的差距，并在多个专业科学任务上取得了领先性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，大量开源基础模型出现，在一些广泛关注领域取得了显著进展，性能相当接近闭源模型。然而，在高价值但更具挑战性的科学专业领域，这些领域仍然依赖专家模型，或者通用基础模型的进展显著落后于流行领域，远不足以改变科学研究，在这些科学领域中开源模型与闭源模型之间存在巨大差距。为了弥合这一差距并向通用人工智能(AGI)迈进一步，我们介绍了Intern-S1，一个具备通用理解和推理能力的专业通用模型，能够分析多种科学模态数据。Intern-S1是一个多模态专家混合(MoE)模型，拥有280亿激活参数和2410亿总参数，在5T tokens上持续预训练，其中包括超过2.5T tokens来自科学领域。在后训练阶段，Intern-S1在InternBootCamp中经历离线和在线强化学习(RL)，我们提出奖励混合(MoR)方法，同时在1000多个任务上协同RL训练。通过算法、数据和训练系统的综合创新，Intern-S1在在线RL训练中取得了顶级性能。在综合评估基准上，Intern-S1在开源模型中展现出具有竞争力的通用推理性能，在科学领域显著优于开源模型，在分子合成规划、反应条件预测、预测晶体热力学稳定性等专业任务上超越闭源最先进模型。我们的模型可在https://huggingface.co/internlm/Intern-S1获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, a plethora of open-source foundation models have emerged,achieving remarkable progress in some widely attended fields, with performancebeing quite close to that of closed-source models. However, in high-value butmore challenging scientific professional fields, either the fields still relyon expert models, or the progress of general foundation models lagssignificantly compared to those in popular areas, far from sufficient fortransforming scientific research and leaving substantial gap betweenopen-source models and closed-source models in these scientific domains. Tomitigate this gap and explore a step further toward Artificial GeneralIntelligence (AGI), we introduce Intern-S1, a specialized generalist equippedwith general understanding and reasoning capabilities with expertise to analyzemultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)model with 28 billion activated parameters and 241 billion total parameters,continually pre-trained on 5T tokens, including over 2.5T tokens fromscientific domains. In the post-training stage, Intern-S1 undergoes offline andthen online reinforcement learning (RL) in InternBootCamp, where we proposeMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 taskssimultaneously. Through integrated innovations in algorithms, data, andtraining systems, Intern-S1 achieved top-tier performance in online RLtraining.On comprehensive evaluation benchmarks, Intern-S1 demonstratescompetitive performance on general reasoning tasks among open-source models andsignificantly outperforms open-source models in scientific domains, surpassingclosed-source state-of-the-art models in professional tasks, such as molecularsynthesis planning, reaction condition prediction, predicting thermodynamicstabilities for crystals. Our models are available athttps://huggingface.co/internlm/Intern-S1.</description>
      <author>example@mail.com (Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou)</author>
      <guid isPermaLink="false">2508.15763v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Waver: Wave Your Way to Lifelike Video Generation</title>
      <link>http://arxiv.org/abs/2508.15761v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Waver，一个高性能的基础模型，用于统一的图像和视频生成，能够同时支持文本到视频、图像到视频和文本到图像生成。&lt;h4&gt;背景&lt;/h4&gt;视频生成技术不断发展，但现有模型在性能、质量和功能整合方面仍有提升空间。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够高质量生成视频和图像的基础模型，提升模态对齐和训练效率，并提供详细的训练和推理方法。&lt;h4&gt;方法&lt;/h4&gt;提出混合流DiT架构，建立全面的数据筛选流程，训练基于MLLM的视频质量模型，提供详细的训练和推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;Waver能够生成5-10秒的高质量视频，擅长捕捉复杂运动，在视频合成中实现卓越的运动幅度和时间一致性，在T2V和I2V排行榜上排名前3。&lt;h4&gt;结论&lt;/h4&gt;Waver持续优于现有的开源模型，匹配或超越最先进的商业解决方案，为视频生成技术的发展提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Waver，一个用于统一图像和视频生成的高性能基础模型。Waver可以直接生成持续5到10秒、原生分辨率为720p的视频，随后放大到1080p。该模型在单一集成框架内同时支持文本到视频(T2V)、图像到视频(I2V)和文本到图像(T2I)生成。我们引入了混合流DiT架构以增强模态对齐和加速训练收敛。为确保训练数据质量，我们建立了全面的数据筛选流程，并手动标注和训练了一个基于MLLM的视频质量模型，以筛选最高质量的样本。此外，我们提供了详细的训练和推理方法，以促进高质量视频的生成。基于这些贡献，Waver在捕捉复杂运动方面表现出色，在视频合成中实现了卓越的运动幅度和时间一致性。值得注意的是，截至2025年7月30日10:00 GMT+8，它在Artificial Analysis的T2V和I2V排行榜上均排名前3，持续优于现有的开源模型，匹配或超越最先进的商业解决方案。我们希望这份技术报告能帮助社区更高效地训练高质量视频生成模型，并加速视频生成技术的进步。官方页面：https://github.com/FoundationVision/Waver。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Waver, a high-performance foundation model for unified image andvideo generation. Waver can directly generate videos with durations rangingfrom 5 to 10 seconds at a native resolution of 720p, which are subsequentlyupscaled to 1080p. The model simultaneously supports text-to-video (T2V),image-to-video (I2V), and text-to-image (T2I) generation within a single,integrated framework. We introduce a Hybrid Stream DiT architecture to enhancemodality alignment and accelerate training convergence. To ensure training dataquality, we establish a comprehensive data curation pipeline and manuallyannotate and train an MLLM-based video quality model to filter for thehighest-quality samples. Furthermore, we provide detailed training andinference recipes to facilitate the generation of high-quality videos. Buildingon these contributions, Waver excels at capturing complex motion, achievingsuperior motion amplitude and temporal consistency in video synthesis. Notably,it ranks among the Top 3 on both the T2V and I2V leaderboards at ArtificialAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperformingexisting open-source models and matching or surpassing state-of-the-artcommercial solutions. We hope this technical report will help the communitymore efficiently train high-quality video generation models and accelerateprogress in video generation technologies. Official page:https://github.com/FoundationVision/Waver.</description>
      <author>example@mail.com (Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng)</author>
      <guid isPermaLink="false">2508.15761v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>"Does the cafe entrance look accessible? Where is the door?" Towards Geospatial AI Agents for Visual Inquiries</title>
      <link>http://arxiv.org/abs/2508.15752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the ICCV'25 Workshop "Vision Foundation Models and  Generative AI for Accessibility: Challenges and Opportunities"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了地理可视化智能体（Geo-Visual Agents）的愿景，这是一种多模态AI智能体，能够通过分析大规模地理空间图像存储库来理解和响应细微的视觉空间查询，结合传统GIS数据源，回答关于世界外观的地理可视化问题。&lt;h4&gt;背景&lt;/h4&gt;交互式数字地图虽然改变了人们旅行和了解世界的方式，但它们依赖于预先存在的结构化GIS数据（如道路网络、POI索引），限制了它们回答关于'世界看起来是什么样子'的地理可视化问题的能力。&lt;h4&gt;目的&lt;/h4&gt;提出地理可视化智能体（Geo-Visual Agents）的愿景，创建能够理解和响应细微视觉空间查询的多模态AI智能体。&lt;h4&gt;方法&lt;/h4&gt;通过分析大规模地理空间图像存储库（包括街景如Google街景、基于地点的照片如TripAdvisor和Yelp、航空影像如卫星照片）并结合传统GIS数据源来实现地理可视化智能体。&lt;h4&gt;主要发现&lt;/h4&gt;论文定义了地理可视化智能体的愿景，描述了感知和交互方法，并提供了三个具体示例。&lt;h4&gt;结论&lt;/h4&gt;论文列举了地理可视化智能体未来工作的关键挑战和机会。&lt;h4&gt;翻译&lt;/h4&gt;交互式数字地图彻底改变了人们旅行和了解世界的方式；然而，它们依赖于GIS数据库中预先存在的结构化数据（例如道路网络、POI索引），这限制了它们回答关于世界外观的地理可视化问题的能力。我们介绍了地理可视化智能体的愿景——这些是多模态AI智能体，能够通过分析大规模地理空间图像存储库（包括街景如Google街景、基于地点的照片如TripAdvisor、Yelp，以及航空影像如卫星照片）并结合传统GIS数据源，来理解和响应关于世界的细微视觉空间查询。我们定义了愿景，描述了感知和交互方法，提供了三个示例，并列举了未来工作的关键挑战和机会。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是当前地图系统无法回答关于世界外观的地理视觉问题。例如，盲人用户想知道'咖啡店的入口是否可及？门在哪里？'这类问题。这个问题很重要，因为现有地图系统依赖预存在的结构化数据，无法充分利用大量地理空间图像信息，导致残障人士（如盲人、轮椅使用者）难以获取环境可访问性信息，普通用户也难以获得丰富的视觉导航和位置信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到当前地图系统的局限性，意识到大量地理空间图像数据未被充分利用，然后提出多模态AI代理的概念来整合视觉和空间数据。他们设计了针对不同旅行阶段（旅行前、旅行中、到达目的地、室内探索）的代理功能。作者借鉴了现有工作，包括地理空间人工智能（GeoAI）如CARTO AI、'自主GIS'概念、地理空间视觉问答（GVQA）系统、街景视图技术和多模态大语言模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建能够理解和响应复杂视觉-空间查询的多模态AI代理，整合多种数据源（街道景观图像、用户贡献照片、空中图像等），结合传统GIS数据提供全面的地理空间理解，并根据用户需求和能力提供个性化响应。整体实现流程包括：1)数据收集（从多种来源获取地理空间图像）；2)数据处理（使用AI技术分析图像）；3)查询理解（理解用户的视觉-空间查询）；4)数据融合（结合视觉证据和结构化地理空间数据）；5)响应生成（根据用户需求和能力生成适当的响应）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了地理视觉代理（Geo-Visual Agents）的新概念；2)关注个人、交互式和即时需求，而非专家分析；3)整合多种数据源（街道景观、用户照片、空中图像等）；4)针对不同旅行阶段提供不同功能；5)专注于可访问性和用户体验。相比之前的工作，这种方法不同于传统GeoAI系统主要面向专家，不同于'自主GIS'更广泛的科学目标，不同于GVQA系统主要关注远程空中图像，也不同于现有地图系统仅依赖预存在的结构化数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了地理视觉代理的概念，通过整合多种地理空间图像和传统GIS数据，使AI系统能够理解和响应复杂的视觉-空间查询，从而改善导航体验，特别是对残障人士的可访问性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive digital maps have revolutionized how people travel and learnabout the world; however, they rely on pre-existing structured data in GISdatabases (e.g., road networks, POI indices), limiting their ability to addressgeo-visual questions related to what the world looks like. We introduce ourvision for Geo-Visual Agents--multimodal AI agents capable of understanding andresponding to nuanced visual-spatial inquiries about the world by analyzinglarge-scale repositories of geospatial images, including streetscapes (e.g.,Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerialimagery (e.g., satellite photos) combined with traditional GIS data sources. Wedefine our vision, describe sensing and interaction approaches, provide threeexemplars, and enumerate key challenges and opportunities for future work.</description>
      <author>example@mail.com (Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. O'Meara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane)</author>
      <guid isPermaLink="false">2508.15752v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model</title>
      <link>http://arxiv.org/abs/2508.15751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 3 figures, accepted by Journal of Medical Imaging&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分子赋能的全能SAM模型(All-in-SAM)，通过全栈方法改进计算病理学中的细胞分割和分类性能，减少标注需求并提高分析精度。&lt;h4&gt;背景&lt;/h4&gt;计算病理学的发展受到视觉基础模型特别是Segment Anything Model(SAM)的推动。SAM通过基于提示的零样本分割和特定细胞SAM模型直接分割实现细胞核分割，但在细粒度语义分割方面存在挑战，如识别特定细胞亚型或特定细胞。&lt;h4&gt;目的&lt;/h4&gt;提出分子赋能的全能SAM模型，利用视觉基础模型的能力推进计算病理学发展，解决细粒度语义分割问题。&lt;h4&gt;方法&lt;/h4&gt;采用全栈方法：(1)标注参与-通过分子赋能学习吸引标注者，减少对详细像素级标注的需求；(2)学习适应-使用SAM适配器调整模型以强调特定语义，利用其强大泛化能力；(3)改进优化-整合分子导向校正学习(MOCL)提高分割准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在内部和公共数据集上的实验结果表明，All-in-SAM模型显著提高了细胞分类性能，即使在面对不同标注质量的情况下也能保持良好表现。&lt;h4&gt;结论&lt;/h4&gt;该方法减少了标注者工作负担，将精确生物医学图像分析扩展到资源有限环境，推进医学诊断和自动化病理图像分析。&lt;h4&gt;翻译&lt;/h4&gt;目的：计算病理学的最新发展由视觉基础模型特别是Segment Anything Model(SAM)的进步推动。该模型通过两种主要方法实现细胞核分割：基于提示的零样本分割和使用特定细胞SAM模型的直接分割。这些方法能够在各种细胞核和细胞中实现有效分割。然而，通用视觉基础模型在细粒度语义分割方面常面临挑战，例如识别特定细胞亚型或特定细胞。方法：本文提出分子赋能的全能SAM模型，通过利用视觉基础模型的能力推进计算病理学。该模型采用全栈方法，专注于：(1)标注参与-通过分子赋能学习吸引标注者，减少对详细像素级标注的需求；(2)学习-调整SAM模型以强调特定语义，利用SAM适配器发挥其强大泛化能力；(3)改进-通过整合分子导向校正学习(MOCL)提高分割准确性。结果：内部和公共数据集的实验结果表明，All-in-SAM模型显著提高了细胞分类性能，即使面对不同标注质量也是如此。结论：我们的方法不仅减少了标注者的工作负担，还将精确的生物医学图像分析扩展到资源有限的环境，从而推进医学诊断和自动化病理图像分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Recent developments in computational pathology have been driven byadvances in Vision Foundation Models, particularly the Segment Anything Model(SAM). This model facilitates nuclei segmentation through two primary methods:prompt-based zero-shot segmentation and the use of cell-specific SAM models fordirect segmentation. These approaches enable effective segmentation across arange of nuclei and cells. However, general vision foundation models often facechallenges with fine-grained semantic segmentation, such as identifyingspecific nuclei subtypes or particular cells. Approach: In this paper, wepropose the molecular-empowered All-in-SAM Model to advance computationalpathology by leveraging the capabilities of vision foundation models. Thismodel incorporates a full-stack approach, focusing on: (1) annotation-engaginglay annotators through molecular-empowered learning to reduce the need fordetailed pixel-level annotations, (2) learning-adapting the SAM model toemphasize specific semantics, which utilizes its strong generalizability withSAM adapter, and (3) refinement-enhancing segmentation accuracy by integratingMolecular-Oriented Corrective Learning (MOCL). Results: Experimental resultsfrom both in-house and public datasets show that the All-in-SAM modelsignificantly improves cell classification performance, even when faced withvarying annotation quality. Conclusions: Our approach not only reduces theworkload for annotators but also extends the accessibility of precisebiomedical image analysis to resource-limited settings, thereby advancingmedical diagnostics and automating pathology image analysis.</description>
      <author>example@mail.com (Xueyuan Li, Can Cui, Ruining Deng, Yucheng Tang, Quan Liu, Tianyuan Yao, Shunxing Bao, Naweed Chowdhury, Haichun Yang, Yuankai Huo)</author>
      <guid isPermaLink="false">2508.15751v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Cross-Domain EEG Analysis Application: A Survey</title>
      <link>http://arxiv.org/abs/2508.15716v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Journals&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EEG分析中基础模型的第一个全面的模态导向分类法，系统组织了研究进展，分析了各类别的理论基础和架构创新，并强调了开放挑战，为EEG基础模型的实际应用提供了参考框架。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)分析是神经科学和人工智能研究的前沿领域，基础模型正在利用其强大的表示能力和跨模态泛化能力重塑传统EEG分析范式。&lt;h4&gt;目的&lt;/h4&gt;填补EEG分析基础模型研究领域的碎片化空白，提供第一个全面的模态导向分类法，系统组织研究进展。&lt;h4&gt;方法&lt;/h4&gt;基于输出模态对EEG基础模型进行分类，包括原生EEG解码、EEG-文本、EEG-视觉、EEG-音频和更广泛的多模态框架；严格分析每个类别的研究思路、理论基础和架构创新；强调开放挑战如模型可解释性、跨域泛化能力和实际应用性。&lt;h4&gt;主要发现&lt;/h4&gt;EEG基础模型研究存在碎片化问题，表现为模型角色多样、架构不一致和缺乏系统分类；通过模态导向分类法可以系统组织这一分散领域；EEG基础模型向可扩展、可解释和在线可操作的解决方案转化存在挑战。&lt;h4&gt;结论&lt;/h4&gt;通过统一分散的EEG基础模型研究领域，该研究为未来方法发展提供了参考框架，并促进了EEG基础模型向实际应用场景的转化。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)分析处于神经科学和人工智能研究的前沿，基础模型正在利用其强大的表示能力和跨模态泛化能力重塑传统的EEG分析范式。然而，这些技术的快速扩展导致了碎片化的研究景观，表现为多样化的模型角色、不一致的架构和缺乏系统分类。为了填补这一空白，本研究提出了EEG分析中基础模型的第一个全面的模态导向分类法，基于原生EEG解码、EEG-文本、EEG-视觉、EEG-音频以及更广泛的多模态框架的输出模态系统地组织研究进展。我们严格分析了每个类别的研究思路、理论基础和架构创新，同时强调了开放挑战，如模型可解释性、跨域泛化能力和基于EEG系统的实际应用性。通过统一这个分散的领域，我们的研究不仅为未来的方法发展提供了参考框架，还促进了EEG基础模型向可扩展、可解释和在线可操作的解决方案的转化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) analysis stands at the forefront of neuroscienceand artificial intelligence research, where foundation models are reshaping thetraditional EEG analysis paradigm by leveraging their powerful representationalcapacity and cross-modal generalization. However, the rapid proliferation ofthese techniques has led to a fragmented research landscape, characterized bydiverse model roles, inconsistent architectures, and a lack of systematiccategorization. To bridge this gap, this study presents the first comprehensivemodality-oriented taxonomy for foundation models in EEG analysis,systematically organizing research advances based on output modalities of thenative EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodalframeworks. We rigorously analyze each category's research ideas, theoreticalfoundations, and architectural innovations, while highlighting open challengessuch as model interpretability, cross-domain generalization, and real-worldapplicability in EEG-based systems. By unifying this dispersed field, our worknot only provides a reference framework for future methodology development butaccelerates the translation of EEG foundation models into scalable,interpretable, and online actionable solutions.</description>
      <author>example@mail.com (Hongqi Li, Yitong Chen, Yujuan Wang, Weihang Ni, Haodong Zhang)</author>
      <guid isPermaLink="false">2508.15716v2</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>On Evaluating the Adversarial Robustness of Foundation Models for Multimodal Entity Linking</title>
      <link>http://arxiv.org/abs/2508.15481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次系统评估了多模态实体链接(MEL)模型在视觉对抗攻击下的鲁棒性，发现现有模型普遍缺乏足够的抗干扰能力。基于此，研究者提出了一种结合大语言模型和检索增强的实体链接方法(LLM-RetLink)，显著提升了模型在对抗条件下的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态数据的爆炸性增长推动了多模态实体链接(MEL)模型的快速发展，但现有研究尚未系统研究视觉对抗攻击对MEL模型的影响。&lt;h4&gt;目的&lt;/h4&gt;首次全面评估主流MEL模型在不同对抗攻击场景下的鲁棒性，覆盖Image-to-Text (I2T)和Image+Text-to-Text (IT2T)两个核心任务。&lt;h4&gt;方法&lt;/h4&gt;提出LLM-RetLink方法，采用两阶段过程：首先使用大型视觉模型(LVMs)提取初始实体描述，然后通过基于网络的检索动态生成候选描述句子，从而提高模型的抗干扰能力。&lt;h4&gt;主要发现&lt;/h4&gt;当前MEL模型普遍缺乏对视觉扰动的足够鲁棒性；输入中的上下文语义信息可以部分减轻对抗扰动的影响；LLM-RetLink在五个数据集上提高了MEL的准确性0.4%-35.7%，尤其在对抗条件下显示出显著优势。&lt;h4&gt;结论&lt;/h4&gt;研究突显了MEL鲁棒性之前未被探索的方面，构建并发布了首个MEL对抗示例数据集，为未来加强多模态系统在对抗环境中的韧性奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;多模态数据的爆炸性增长推动了多模态实体链接(MEL)模型的快速发展。然而，现有研究尚未系统研究视觉对抗攻击对MEL模型的影响。我们首次对主流MEL模型在不同对抗攻击场景下的鲁棒性进行了全面评估，涵盖两个核心任务：Image-to-Text (I2T)和Image+Text-to-Text (IT2T)。实验结果表明，当前MEL模型普遍缺乏对视觉扰动的足够鲁棒性。有趣的是，输入中的上下文语义信息可以部分减轻对抗扰动的影响。基于这一见解，我们提出了LLM-RetLink方法，通过两阶段过程显著提高了模型的抗干扰能力：首先使用大型视觉模型(LVMs)提取初始实体描述，然后通过基于网络的检索动态生成候选描述句子。在五个数据集上的实验表明，LLM-RetLink将MEL的准确性提高了0.4%-35.7%，特别是在对抗条件下显示出显著优势。这项研究突显了MEL鲁棒性之前未被探索的方面，构建并发布了首个MEL对抗示例数据集，为未来旨在加强多模态系统在对抗环境中韧性的工作奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The explosive growth of multimodal data has driven the rapid development ofmultimodal entity linking (MEL) models. However, existing studies have notsystematically investigated the impact of visual adversarial attacks on MELmodels. We conduct the first comprehensive evaluation of the robustness ofmainstream MEL models under different adversarial attack scenarios, coveringtwo core tasks: Image-to-Text (I2T) and Image+Text-to-Text (IT2T). Experimentalresults show that current MEL models generally lack sufficient robustnessagainst visual perturbations. Interestingly, contextual semantic information ininput can partially mitigate the impact of adversarial perturbations. Based onthis insight, we propose an LLM and Retrieval-Augmented Entity Linking(LLM-RetLink), which significantly improves the model's anti-interferenceability through a two-stage process: first, extracting initial entitydescriptions using large vision models (LVMs), and then dynamically generatingcandidate descriptive sentences via web-based retrieval. Experiments on fivedatasets demonstrate that LLM-RetLink improves the accuracy of MEL by0.4%-35.7%, especially showing significant advantages under adversarialconditions. This research highlights a previously unexplored facet of MELrobustness, constructs and releases the first MEL adversarial example dataset,and sets the stage for future work aimed at strengthening the resilience ofmultimodal systems in adversarial environments.</description>
      <author>example@mail.com (Fang Wang, Yongjie Wang, Zonghao Yang, Minghao Hu, Xiaoying Bai)</author>
      <guid isPermaLink="false">2508.15481v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>AudioSet-R: A Refined AudioSet with Multi-Stage LLM Label Reannotation</title>
      <link>http://arxiv.org/abs/2508.15429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, accepted in ACM MM 2025 dataset track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种三阶段重新标注框架，利用音频语言基础模型改进AudioSet数据集的标签质量，构建了高质量的AudioSet-R数据集，并在多个音频分类模型上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;AudioSet是音频研究社区广泛使用的基准数据集，已显著推进各种音频相关任务，但标签准确性和完整性问题仍然是限制下游应用性能的关键瓶颈。&lt;h4&gt;目的&lt;/h4&gt;解决AudioSet数据集的标签质量问题，构建高质量的、结构化的重新标注版本AudioSet-R。&lt;h4&gt;方法&lt;/h4&gt;提出一个三阶段重新标注框架，利用通用音频语言基础模型提高标签质量，采用跨模态提示策略，通过顺序组合提示来执行音频理解、标签合成和语义对齐三个子任务。&lt;h4&gt;主要发现&lt;/h4&gt;在AST、PANNs、SSAST和AudioMAE等代表性音频分类模型上的实验一致显示，使用AudioSet-R能带来显著的性能提升，验证了所提出方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该三阶段重新标注框架能够有效提高标签可靠性，具有通用性和有效性，代码已在GitHub公开：https://github.com/colaudiolab/AudioSet-R。&lt;h4&gt;翻译&lt;/h4&gt;AudioSet是音频研究社区广泛使用的基准，已显著推进各种音频相关任务。然而，标签准确性和完整性的持续问题仍然是限制下游应用性能的关键瓶颈。为解决上述挑战，我们提出了一种三阶段重新标注框架，利用通用音频语言基础模型系统提高AudioSet的标签质量。该框架采用跨模态提示策略，受提示链概念启发，通过顺序组合提示来执行子任务（音频理解、标签合成和语义对齐）。利用此框架，我们构建了高质量的、结构化的AudioSet-R重新标注版本。在AST、PANNs、SSAST和AudioMAE等代表性音频分类模型上进行的大量实验一致显示出显著的性能提升，从而验证了所提出方法在提高标签可靠性方面的通用性和有效性。代码已在GitHub公开：https://github.com/colaudiolab/AudioSet-R。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758260&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AudioSet is a widely used benchmark in the audio research community and hassignificantly advanced various audio-related tasks. However, persistent issueswith label accuracy and completeness remain critical bottlenecks that limitperformance in downstream applications.To address the aforementionedchallenges, we propose a three-stage reannotation framework that harnessesgeneral-purpose audio-language foundation models to systematically improve thelabel quality of AudioSet. The framework employs a cross-modal promptingstrategy, inspired by the concept of prompt chaining, wherein prompts aresequentially composed to execute subtasks (audio comprehension, labelsynthesis, and semantic alignment). Leveraging this framework, we construct ahigh-quality, structured relabeled version of AudioSet-R. Extensive experimentsconducted on representative audio classification models--including AST, PANNs,SSAST, and AudioMAE--consistently demonstrate substantial performanceimprovements, thereby validating the generalizability and effectiveness of theproposed approach in enhancing label reliability.The code is publicly availableat: https://github.com/colaudiolab/AudioSet-R.</description>
      <author>example@mail.com (Yulin Sun, Qisheng Xu, Yi Su, Qian Zhu, Yong Dou, Xinwang Liu, Kele Xu)</author>
      <guid isPermaLink="false">2508.15429v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations</title>
      <link>http://arxiv.org/abs/2508.15404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了掩码自编码器（MAEs）超参数与下游任务性能之间的关系，揭示了MAEs如何学习图像中的空间相关性，并提供了超参数选择的实践见解。&lt;h4&gt;背景&lt;/h4&gt;掩码自编码器（MAEs）已成为视觉基础模型的有效预训练技术，但当应用于新数据集时需要大量超参数调整。虽然先前研究已从注意力和潜变量模型角度分析MAEs，但超参数与下游任务性能之间的联系尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究MAEs如何学习输入图像中的空间相关性，分析超参数如何影响模型学习到的特征，并提供超参数选择的实践指导。&lt;h4&gt;方法&lt;/h4&gt;通过数学推导分析线性MAE学习到的特征，展示掩码比例和块大小如何选择短程和长程空间相关性特征，并将分析扩展到非线性MAE，研究其如何适应数据集中的空间相关性。&lt;h4&gt;主要发现&lt;/h4&gt;掩码比例和块大小可用于选择性地捕获短程和长程空间相关性特征；MAE表示能够适应数据集中的空间相关性，超越二阶统计特性。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了关于如何在实际应用中选择MAE超参数的见解，填补了MAE超参数与下游任务性能之间关系的知识空白。&lt;h4&gt;翻译&lt;/h4&gt;掩码自编码器（MAEs）已成为视觉基础模型的一种强大预训练技术。尽管它们有效，但当应用于新数据集时，需要大量的超参数调整（掩码比例、块大小、编码器/解码器层数）。虽然之前的工作已经从注意力和分层潜变量模型的角度分析了MAEs，但MAE超参数与下游任务性能之间的联系相对未被探索。本研究调查了MAEs如何学习输入图像中的空间相关性。我们通过数学推导分析了线性MAE学习到的特征，并表明掩码比例和块大小可用于选择捕获短程和长程空间相关性的特征。我们将此分析扩展到非线性MAE，以展示MAE表示如何适应数据集中的空间相关性，超越二阶统计。最后，我们讨论了一些关于如何在实践中选择MAE超参数的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked Autoencoders (MAEs) have emerged as a powerful pretraining techniquefor vision foundation models. Despite their effectiveness, they requireextensive hyperparameter tuning (masking ratio, patch size, encoder/decoderlayers) when applied to novel datasets. While prior theoretical works haveanalyzed MAEs in terms of their attention patterns and hierarchical latentvariable models, the connection between MAE hyperparameters and performance ondownstream tasks is relatively unexplored. This work investigates how MAEslearn spatial correlations in the input image. We analytically derive thefeatures learned by a linear MAE and show that masking ratio and patch size canbe used to select for features that capture short- and long-range spatialcorrelations. We extend this analysis to non-linear MAEs to show that MAErepresentations adapt to spatial correlations in the dataset, beyondsecond-order statistics. Finally, we discuss some insights on how to select MAEhyper-parameters in practice.</description>
      <author>example@mail.com (Anthony Bisulco, Rahul Ramesh, Randall Balestriero, Pratik Chaudhari)</author>
      <guid isPermaLink="false">2508.15404v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection</title>
      <link>http://arxiv.org/abs/2508.15313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RAG-SEG的新型训练范式，用于解决伪装物体检测问题，该方法无需训练即可达到与现有先进方法相当或更好的性能，同时具有很高的计算效率。&lt;h4&gt;背景&lt;/h4&gt;伪装物体检测在计算机视觉中具有挑战性，因为物体与背景高度相似。现有方法通常依赖大量训练和计算资源。基础模型如Segment Anything Model(SAM)在未经微调和缺乏高质量提示的情况下难以处理COD任务，而手动生成此类提示成本高且效率低。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练的COD方法，减少对计算资源的依赖，同时避免手动生成高质量提示的需要。&lt;h4&gt;方法&lt;/h4&gt;提出RAG-SEG范式，将COD解耦为两个阶段：1)使用检索增强生成(RAG)通过无监督聚类构建的紧凑检索数据库生成粗略掩码作为提示；2)使用基于SAM的分割(SEG)进行细化。在推理过程中，检索到的特征生成伪标签，引导SAM2进行精确掩码生成。&lt;h4&gt;主要发现&lt;/h4&gt;RAG-SEG在无需传统训练的情况下保持了竞争性能。在基准COD数据集上的广泛实验表明，该方法的表现与或优于最先进的方法。所有实验均在个人笔记本电脑上进行，突显了其计算效率和实用性。&lt;h4&gt;结论&lt;/h4&gt;RAG-SEG是一种有效的训练范式，能够高效处理伪装物体检测任务，具有高度的计算效率和实用性，为资源受限环境下的COD应用提供了可行解决方案。&lt;h4&gt;翻译&lt;/h4&gt;伪装物体检测在计算机视觉中构成重大挑战，因为物体与其背景高度相似。现有方法通常依赖大量训练和计算资源。虽然Segment Anything Model(SAM)等基础模型提供了强大的泛化能力，但在没有微调和高质量提示的情况下，它们仍然难以处理COD任务。然而，手动生成此类提示成本高昂且效率低下。为解决这些挑战，我们提出了RAG-SEG（First RAG, Second SEG），一种无需训练的范式，将COD解耦为两个阶段：使用检索增强生成(RAG)生成粗略掩码作为提示，然后使用基于SAM的分割(SEG)进行细化。RAG-SEG通过无监督聚类构建紧凑的检索数据库，实现快速有效的特征检索。在推理过程中，检索到的特征生成伪标签，引导使用SAM2进行精确的掩码生成。我们的方法消除了对传统训练的需求，同时保持了竞争性能。在基准COD数据集上的广泛实验表明，RAG-SEG的表现与或优于最先进的方法。值得注意的是，所有实验都在个人笔记本电脑上进行，突显了我们方法的计算效率和实用性。我们在附录中提供了进一步分析，涵盖局限性、显著物体检测扩展和可能的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camouflaged object detection (COD) poses a significant challenge in computervision due to the high similarity between objects and their backgrounds.Existing approaches often rely on heavy training and large computationalresources. While foundation models such as the Segment Anything Model (SAM)offer strong generalization, they still struggle to handle COD tasks withoutfine-tuning and require high-quality prompts to yield good performance.However, generating such prompts manually is costly and inefficient. To addressthese challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, atraining-free paradigm that decouples COD into two stages: Retrieval-AugmentedGeneration (RAG) for generating coarse masks as prompts, followed by SAM-basedsegmentation (SEG) for refinement. RAG-SEG constructs a compact retrievaldatabase via unsupervised clustering, enabling fast and effective featureretrieval. During inference, the retrieved features produce pseudo-labels thatguide precise mask generation using SAM2. Our method eliminates the need forconventional training while maintaining competitive performance. Extensiveexperiments on benchmark COD datasets demonstrate that RAG-SEG performs on parwith or surpasses state-of-the-art methods. Notably, all experiments areconducted on a \textbf{personal laptop}, highlighting the computationalefficiency and practicality of our approach. We present further analysis in theAppendix, covering limitations, salient object detection extension, andpossible improvements.</description>
      <author>example@mail.com (Wutao Liu, YiDan Wang, Pan Gao)</author>
      <guid isPermaLink="false">2508.15313v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Survey of Vision-Language-Action Models for Embodied Manipulation</title>
      <link>http://arxiv.org/abs/2508.15201v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  in Chinese language&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述全面审查了用于具身操作的视觉-语言-行动模型，追踪了VLA架构的发展轨迹，从五个关键维度分析了当前研究，并总结了挑战与未来方向。&lt;h4&gt;背景&lt;/h4&gt;具身智能系统通过持续的环境交互增强智能体能力，受到学术界和工业界的广泛关注。受大型基础模型进展启发的视觉-语言-行动模型作为通用机器人控制框架，显著提升了具身智能系统中智能体-环境交互能力，扩展了具身AI机器人的应用场景。&lt;h4&gt;目的&lt;/h4&gt;全面审查用于具身操作的VLA模型，系统梳理其发展历程和研究现状。&lt;h4&gt;方法&lt;/h4&gt;追踪VLA架构的发展轨迹，从五个关键维度进行详细分析：VLA模型结构、训练数据集、预训练方法、后训练方法和模型评估，并总结关键挑战与未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;视觉-语言-行动模型作为通用机器人控制框架，显著提升了具身智能系统中智能体-环境交互能力，扩展了具身AI机器人的应用场景。&lt;h4&gt;结论&lt;/h4&gt;VLA模型在具身操作领域展现出巨大潜力，但仍面临发展和实际部署中的挑战，需要进一步研究以推动该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;具身智能系统通过持续的环境交互增强智能体能力，已引起学术界和工业界的广泛关注。受大型基础模型进展启发的视觉-语言-行动模型作为通用机器人控制框架，显著提升了具身智能系统中智能体-环境交互能力。这一扩展拓宽了具身AI机器人的应用场景。本综述全面审查了用于具身操作的VLA模型。首先，它追溯了VLA架构的发展轨迹。随后，我们从5个关键维度对当前研究进行了详细分析：VLA模型结构、训练数据集、预训练方法、后训练方法和模型评估。最后，我们总结了VLA发展和实际部署中的关键挑战，并概述了有前景的未来研究方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决具身智能系统中视觉-语言-动作(VLA)模型的系统化梳理和总结问题。这个问题在现实中非常重要，因为具身智能是AI从抽象数据向物理世界延伸的重要方向，而VLA模型作为受大模型启发的机器人通用控制模型，能显著提升机器人与环境交互的能力，扩展应用场景。传统机器人系统难以应对开放环境的复杂任务，而VLA模型通过多模态融合实现从环境理解到物理执行的闭环耦合，对推动机器人技术在工业、家庭、服务等领域的落地应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 这篇论文是一篇综述文章，作者通过系统化的文献调研，对VLA模型的相关研究进行了全面梳理。作者采用结构化的内容组织方式，从具身操作与VLA的关系入手，按照发展历程、模型架构、训练数据、预训练方法、后训练方法和模型评估的逻辑顺序展开内容。作者对VLA模型的各个方面进行了分类和归纳，如将发展历程划分为三个阶段，将预训练方法分为四类等。作者在撰写过程中大量借鉴了现有工作，包括参考了Ma et al.、Sapkota et al.等VLA相关综述文章，以及RT-1、RT-2、Octo等具体VLA模型研究，同时还借鉴了各种数据集和评估方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; VLA模型的核心思想是将视觉感知、语义推理与动作生成深度融合，使机器人能够直接从多模态输入中预测连续控制指令，实现从环境理解到物理执行的闭环耦合。其整体实现流程包括：1)观测编码：将视觉和语言信息编码到特征空间；2)特征推理：通过骨干网络(如Transformer)捕获特征间相关性；3)动作解码：将特征转换为机器人可执行动作；4)分层系统(可选)：上层负责任务理解，下层负责动作执行；5)训练流程：包括预训练(使用大规模数据)和后训练(针对特定任务微调)；6)评估流程：通过真实环境、仿真器或世界模型进行评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 这篇综述论文的关键创新点包括：1)提供全面的VLA模型综述框架，从具身智能系统的核心要素出发；2)将VLA发展历程划分为三阶段，提供清晰的时间线；3)将预训练方法分为四类，系统化不同训练策略；4)将后训练方法分为三类，借鉴大模型分类方法；5)提出模型评估的三维度视角。相比之前的工作，本文的不同之处在于：视角更全面，包含2024年后最新研究成果，提供更系统的分类框架，并进行更深入的挑战分析，为领域研究提供更全面的参考和方向指导。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过对具身操作中视觉-语言-动作模型的全面综述，系统梳理了VLA模型的发展历程、架构设计、训练方法、评估体系及未来挑战，为具身智能领域的研究提供了系统化的参考框架和方向指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence systems, which enhance agent capabilities throughcontinuous environment interactions, have garnered significant attention fromboth academia and industry. Vision-Language-Action models, inspired byadvancements in large foundation models, serve as universal robotic controlframeworks that substantially improve agent-environment interactioncapabilities in embodied intelligence systems. This expansion has broadenedapplication scenarios for embodied AI robots. This survey comprehensivelyreviews VLA models for embodied manipulation. Firstly, it chronicles thedevelopmental trajectory of VLA architectures. Subsequently, we conduct adetailed analysis of current research across 5 critical dimensions: VLA modelstructures, training datasets, pre-training methods, post-training methods, andmodel evaluation. Finally, we synthesize key challenges in VLA development andreal-world deployment, while outlining promising future research directions.</description>
      <author>example@mail.com (Haoran Li, Yuhui Chen, Wenbo Cui, Weiheng Liu, Kai Liu, Mingcai Zhou, Zhengtao Zhang, Dongbin Zhao)</author>
      <guid isPermaLink="false">2508.15201v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support</title>
      <link>http://arxiv.org/abs/2508.15192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LLM4Sweat，一个专门针对多汗症的开源大型语言模型框架，通过三阶段流程解决罕见医疗条件数据稀缺问题，提供诊断、治疗建议和心理支持。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在医疗保健领域有潜力，但应用于罕见医疗条件时受限于微调数据的稀缺和不可靠。多汗症是一种影响2-3%人口的疾病，显著影响患者身体舒适度和心理社会健康。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对多汗症的支持系统，填补当前没有针对多汗症的诊断或护理定制LLM的空白。&lt;h4&gt;方法&lt;/h4&gt;LLM4Sweat采用三阶段流程：1)数据增强阶段，利用前沿LLM从精选开源数据生成医学合理的合成案例；2)微调阶段，在数据集上微调开源基础模型；3)推理和专家评估阶段，由临床和心理学专家评估系统表现，并将验证的响应迭代丰富数据集。&lt;h4&gt;主要发现&lt;/h4&gt;LLM4Sweat性能优于基线模型，成为首个针对多汗症的开源LLM框架。&lt;h4&gt;结论&lt;/h4&gt;该框架为其他具有类似数据和可靠性挑战的罕见疾病提供了一种可推广的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;虽然大型语言模型在医疗保健领域显示出前景，但其在罕见医疗条件中的应用仍受到微调数据稀缺且不可靠的限制。多汗症是一种导致超出生理需求过度出汗的疾病，影响2-3%的人口，显著影响身体舒适度和心理社会健康。迄今为止，没有工作针对多汗症的诊断或护理定制LLM。为解决这一空白，我们提出了LLM4Sweat，一个开源的、领域特定的LLM框架，用于提供可靠且富有同理心的多汗症支持。系统遵循三阶段流程。在数据增强阶段，前沿LLM从精选开源数据生成医学上合理的合成案例，创建多样化且平衡的问答数据集。在微调阶段，开源基础模型在数据集上进行微调，提供诊断、个性化治疗建议和富有同理心的心理支持。在推理和专家评估阶段，临床和心理学专家评估准确性、适当性和同理心，验证的响应迭代丰富数据集。实验表明，LLM4Sweat优于基线模型，提供了首个针对多汗症的开源LLM框架，为其他具有类似数据和可靠性挑战的罕见疾病提供了一种可推广的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While large language models (LLMs) have shown promise in healthcare, theirapplication for rare medical conditions is still hindered by scarce andunreliable datasets for fine-tuning. Hyperhidrosis, a disorder causingexcessive sweating beyond physiological needs, is one such rare disorder,affecting 2-3% of the population and significantly impacting both physicalcomfort and psychosocial well-being. To date, no work has tailored LLMs toadvance the diagnosis or care of hyperhidrosis. To address this gap, we presentLLM4Sweat, an open-source and domain-specific LLM framework for trustworthy andempathetic hyperhidrosis support. The system follows a three-stage pipeline. Inthe data augmentation stage, a frontier LLM generates medically plausiblesynthetic vignettes from curated open-source data to create a diverse andbalanced question-answer dataset. In the fine-tuning stage, an open-sourcefoundation model is fine-tuned on the dataset to provide diagnosis,personalized treatment recommendations, and empathetic psychological support.In the inference and expert evaluation stage, clinical and psychologicalspecialists assess accuracy, appropriateness, and empathy, with validatedresponses iteratively enriching the dataset. Experiments show that LLM4Sweatoutperforms baselines and delivers the first open-source LLM framework forhyperhidrosis, offering a generalizable approach for other rare diseases withsimilar data and trustworthiness challenges.</description>
      <author>example@mail.com (Wenjie Lin, Jin Wei-Kocsis)</author>
      <guid isPermaLink="false">2508.15192v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Don't Think Twice! Over-Reasoning Impairs Confidence Calibration</title>
      <link>http://arxiv.org/abs/2508.15050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICML 2025 Workshop on Reliable and Responsible  Foundation Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了大型语言模型作为问答工具时的校准问题，发现增加推理预算会降低校准性能，而搜索增强生成方法表现更优，表明信息获取而非推理深度可能是知识密集型任务校准的关键瓶颈。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型作为问答工具需要稳健的校准以避免过度自信。研究使用了ClimateX数据集(扩展到人类和行星健康领域)来评估推理能力和预算如何影响置信度评估准确性。&lt;h4&gt;目的&lt;/h4&gt;系统评估推理能力和预算如何影响大型语言模型对置信度的评估准确性，以改进知识密集型任务的置信度校准。&lt;h4&gt;方法&lt;/h4&gt;使用ClimateX数据集(扩展到人类和行星健康领域)，系统地评估推理能力和预算对置信度评估准确性的影响。比较了纯推理方法和搜索增强生成方法的性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. '测试时扩展'范式受到挑战：尽管最近的推理LLM在评估专家置信度时达到48.7%的准确率，但增加推理预算会持续损害而非改善校准。2. 扩展推理会导致系统性过度自信，且随着思考预算的增加而恶化，适度的计算投资后收益递减甚至为负。3. 搜索增强生成方法显著优于纯推理方法，通过检索相关证据实现了89.3%的准确率。&lt;h4&gt;结论&lt;/h4&gt;对于知识密集型任务的置信度校准，信息获取能力可能比推理深度或推理预算更为关键。搜索增强生成方法在置信度评估方面表现优于纯推理方法。&lt;h4&gt;翻译&lt;/h4&gt;作为问答工具部署的大型语言模型需要稳健的校准以避免过度自信。我们系统性地评估推理能力和预算如何影响置信度评估的准确性，使用ClimateX数据集(Lacombe等人，2023)并将其扩展到人类和行星健康领域。我们的关键发现挑战了'测试时扩展'范式：虽然最近的推理LLM在评估专家置信度时实现了48.7%的准确率，但增加推理预算持续损害而非改善校准。扩展推理导致系统性过度自信，且随着思考预算的增加而恶化，在适度的计算投资后产生递减和负回报。相反，搜索增强生成显著优于纯推理，通过检索相关证据实现了89.3%的准确率。我们的结果表明，对于知识密集型任务的改进置信度校准，信息访问能力而非推理深度或推理预算可能是关键瓶颈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models deployed as question answering tools require robustcalibration to avoid overconfidence. We systematically evaluate how reasoningcapabilities and budget affect confidence assessment accuracy, using theClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetaryhealth. Our key finding challenges the "test-time scaling" paradigm: whilerecent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,increasing reasoning budgets consistently impairs rather than improvescalibration. Extended reasoning leads to systematic overconfidence that worsenswith longer thinking budgets, producing diminishing and negative returns beyondmodest computational investments. Conversely, search-augmented generationdramatically outperforms pure reasoning, achieving 89.3% accuracy by retrievingrelevant evidence. Our results suggest that information access, rather thanreasoning depth or inference budget, may be the critical bottleneck forimproved confidence calibration of knowledge-intensive tasks.</description>
      <author>example@mail.com (Romain Lacombe, Kerrie Wu, Eddie Dilworth)</author>
      <guid isPermaLink="false">2508.15050v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>NeuroKoop: Neural Koopman Fusion of Structural-Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents</title>
      <link>http://arxiv.org/abs/2508.16414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint version of the paper accepted to IEEE-EMBS International  Conference on Biomedical and Health Informatics (BHI'25), 2025. This is the  author's original manuscript (preprint). The final published version will  appear in IEEE Xplore&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了NeuroKoop，一种基于图神经网络的新框架，通过神经Koopman算子驱动的潜在空间融合整合结构脑网络和功能脑网络，用于识别产前药物暴露对青少年大脑发育的影响。&lt;h4&gt;背景&lt;/h4&gt;了解产前暴露于大麻等精神活性物质如何影响青少年大脑组织是一个关键挑战，多模态神经影像数据的复杂性和传统分析方法的局限性使这一问题更加复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法来克服现有方法的局限，更好地捕捉结构连接组和功能连接组中的互补特征，提高对产前药物暴露影响的生物学见解和预测性能。&lt;h4&gt;方法&lt;/h4&gt;引入NeuroKoop，一种基于图神经网络的框架，利用神经Koopman算子驱动的潜在空间融合来整合结构脑网络和功能脑网络。通过Koopman理论，NeuroKoop统一了基于形态学源分析(SBM)和功能网络连接(FNC)的脑图节点嵌入，实现增强的表示学习和更稳健的产前药物暴露状态分类。&lt;h4&gt;主要发现&lt;/h4&gt;在ABCD数据集的大型青少年队列中应用，NeuroKoop优于相关基线方法，并揭示了显著的结构-功能连接，推进了对产前药物暴露神经发育影响的理解。&lt;h4&gt;结论&lt;/h4&gt;NeuroKoop框架通过整合结构和功能脑网络信息，能够更准确地识别产前药物暴露对青少年大脑发育的影响，为理解神经发育影响提供了新见解。&lt;h4&gt;翻译&lt;/h4&gt;了解产前暴露于大麻等精神活性物质如何塑造青少年大脑组织仍然是一个关键挑战，这受到多模态神经影像数据复杂性和传统分析方法局限性的影响。现有方法往往无法完全捕捉结构连接组和功能连接组中嵌入的互补特征，限制了生物学见解和预测性能。为此，我们引入了NeuroKoop，这是一种基于图神经网络的新框架，利用神经Koopman算子驱动的潜在空间融合来整合结构脑网络和功能脑网络。通过利用Koopman理论，NeuroKoop统一了基于形态学源分析(SBM)和基于功能网络连接(FNC)的脑图节点嵌入，从而增强了表示学习，并能更稳健地对产前药物暴露(PDE)状态进行分类。将其应用于ABCD数据集的大型青少年队列，NeuroKoop优于相关基线方法，并揭示了显著的结构-功能连接，推进了我们对PDE神经发育影响的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how prenatal exposure to psychoactive substances such ascannabis shapes adolescent brain organization remains a critical challenge,complicated by the complexity of multimodal neuroimaging data and thelimitations of conventional analytic methods. Existing approaches often fail tofully capture the complementary features embedded within structural andfunctional connectomes, constraining both biological insight and predictiveperformance. To address this, we introduced NeuroKoop, a novel graph neuralnetwork-based framework that integrates structural and functional brainnetworks utilizing neural Koopman operator-driven latent space fusion. Byleveraging Koopman theory, NeuroKoop unifies node embeddings derived fromsource-based morphometry (SBM) and functional network connectivity (FNC) basedbrain graphs, resulting in enhanced representation learning and more robustclassification of prenatal drug exposure (PDE) status. Applied to a largeadolescent cohort from the ABCD dataset, NeuroKoop outperformed relevantbaselines and revealed salient structural-functional connections, advancing ourunderstanding of the neurodevelopmental impact of PDE.</description>
      <author>example@mail.com (Badhan Mazumder, Aline Kotoski, Vince D. Calhoun, Dong Hye Ye)</author>
      <guid isPermaLink="false">2508.16414v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation</title>
      <link>http://arxiv.org/abs/2508.16269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种深度学习模型，用于学习练习的稀疏二进制表示作为辅助知识概念，这些概念能够捕捉超越人工标注的概念结构，并与现有的知识追踪模型兼容，从而提高学生建模和练习推荐的效果。&lt;h4&gt;背景&lt;/h4&gt;个性化推荐是智能辅导系统的关键特征，通常依赖于准确的学生知识模型。知识追踪模型通过估计学生基于历史互动的掌握程度来实现这一点。然而，许多现有模型依赖的人工标注知识概念可能不完整、容易出错或过于笼统。&lt;h4&gt;目的&lt;/h4&gt;提出一种深度学习模型，学习练习的稀疏二进制表示，其中每个比特表示潜在概念的存在或不存在，这些表示被称为辅助知识概念，能够捕捉超越人工标注的概念结构。&lt;h4&gt;方法&lt;/h4&gt;开发一种深度学习模型来学习练习的稀疏二进制表示作为辅助知识概念，这些表示与经典模型(如BKT)和现代深度学习知识追踪架构兼容，并将这些辅助知识概念整合到学生建模和练习推荐系统中。&lt;h4&gt;主要发现&lt;/h4&gt;将辅助知识概念纳入系统可以改善学生建模和自适应练习推荐；对于学生建模，将辅助知识概念与BKT等经典模型结合可以提高预测性能；对于推荐，使用辅助知识概念可以强化基于强化学习的策略和基于规划的方法，在模拟学生环境中带来学生学习成果的显著提升。&lt;h4&gt;结论&lt;/h4&gt;辅助知识概念能够有效捕捉超越人工标注的概念结构，提高学生知识追踪模型的预测性能和练习推荐效果，从而改善学生的学习成果。&lt;h4&gt;翻译&lt;/h4&gt;个性化推荐是智能辅导系统的关键特征，通常依赖于准确的学生知识模型。知识追踪模型通过估计学生基于历史互动的掌握程度来实现这一点。许多知识追踪模型依赖于人工标注的知识概念，这些概念标记了每个练习所需的一个或多个技能或概念。然而，这些知识概念可能不完整、容易出错或过于笼统。在本文中，我们提出了一种深度学习模型，用于学习练习的稀疏二进制表示，其中每个比特表示潜在概念的存在或不存在。我们将这些表示称为辅助知识概念。这些表示捕捉了超越人工标注的概念结构，并且与经典模型(如BKT)和现代深度学习知识追踪架构兼容。我们证明，纳入辅助知识概念可以改善学生建模和自适应练习推荐。对于学生建模，我们展示将辅助知识概念与BKT等经典模型结合可以提高预测性能。对于推荐，我们展示使用辅助知识概念可以增强基于强化学习的策略和简单的基于规划的方法(期望最大值)，在模拟学生环境中带来学生学习成果的显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized recommendation is a key feature of intelligent tutoring systems,typically relying on accurate models of student knowledge. Knowledge Tracing(KT) models enable this by estimating a student's mastery based on theirhistorical interactions. Many KT models rely on human-annotated knowledgeconcepts (KCs), which tag each exercise with one or more skills or conceptsbelieved to be necessary for solving it. However, these KCs can be incomplete,error-prone, or overly general.  In this paper, we propose a deep learning model that learns sparse binaryrepresentations of exercises, where each bit indicates the presence or absenceof a latent concept. We refer to these representations as auxiliary KCs. Theserepresentations capture conceptual structure beyond human-defined annotationsand are compatible with both classical models (e.g., BKT) and modern deeplearning KT architectures.  We demonstrate that incorporating auxiliary KCs improves both studentmodeling and adaptive exercise recommendation. For student modeling, we showthat augmenting classical models like BKT with auxiliary KCs leads to improvedpredictive performance. For recommendation, we show that using auxiliary KCsenhances both reinforcement learning-based policies and a simple planning-basedmethod (expectimax), resulting in measurable gains in student learning outcomeswithin a simulated student environment.</description>
      <author>example@mail.com (Yahya Badran, Christine Preisach)</author>
      <guid isPermaLink="false">2508.16269v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning with Adaptive Superpixel Coding</title>
      <link>http://arxiv.org/abs/2508.15959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Transformer的自监督模型Adaptive Superpixel Coding (ASC)，通过自适应超像素层克服了传统Vision Transformer的固定大小和非自适应块划分的局限性，在标准图像下游任务中表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;深度学习视觉模型通常针对特定模态进行定制，并依赖于领域特定的假设，如几乎所有现有视觉模型使用的网格结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Transformer的自监督模型Adaptive Superpixel Coding (ASC)，以克服传统视觉模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;ASC采用自适应超像素层，能够动态调整到底层图像内容，而非传统Vision Transformer使用的固定大小和非自适应的块划分方式。&lt;h4&gt;主要发现&lt;/h4&gt;分析了该方法使其有效的关键特性，并在标准图像下游任务基准测试中发现该方法优于广泛使用的替代方法。&lt;h4&gt;结论&lt;/h4&gt;通过自适应超像素层，ASC改进了传统视觉Transformer的局限性，在标准图像下游任务中表现优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;深度学习视觉模型通常针对特定模态进行定制，并且经常依赖于领域特定的假设，例如几乎所有现有视觉模型使用的网格结构。在这项工作中，我们提出了一种基于Transformer的自监督模型，我们称之为自适应超像素编码（ASC）。我们模型的关键见解是克服传统Vision Transformer的局限性，后者依赖于固定大小和非自适应的块划分。相反，ASC采用自适应超像素层，能够动态调整到底层图像内容。我们分析了该方法使其有效的关键特性，并发现我们的方法在标准图像下游任务基准测试中优于广泛使用的替代方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning vision models are typically tailored for specific modalitiesand often rely on domain-specific assumptions, such as the grid structures usedby nearly all existing vision models. In this work, we propose aself-supervised model based on Transformers, which we call Adaptive SuperpixelCoding (ASC). The key insight of our model is to overcome the limitations oftraditional Vision Transformers, which depend on fixed-size and non-adaptivepatch partitioning. Instead, ASC employs adaptive superpixel layers thatdynamically adjust to the underlying image content. We analyze key propertiesof the approach that make it effective, and find that our method outperformswidely-used alternatives on standard image downstream task benchmarks.</description>
      <author>example@mail.com (Mahmoud Khalil, Ahmad Khalil, Alioune Ngom)</author>
      <guid isPermaLink="false">2508.15959v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem</title>
      <link>http://arxiv.org/abs/2508.15949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper has been accepted for publication in the European Journal  of Operational Research. Supplementary material will be available on the  journal website or upon request&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种将元启发式算法与图表示学习相结合的GL-GRASP方法，用于解决约束增量图绘制问题，在计算实验中表现出优于现有技术的性能，特别是在处理更密集实例时展现了良好的可扩展性。&lt;h4&gt;背景&lt;/h4&gt;近年来，机器学习技术与元启发式算法的结合引起了广泛关注。然而，监督学习和强化学习等方法在某些情况下被认为过于耗时，无法与手工设计的启发式方法竞争。针对约束增量图绘制问题，现有文献有限，而贪婪随机搜索程序启发式方法已显示出有希望的结果。&lt;h4&gt;目的&lt;/h4&gt;探索将图表示学习这种成本较低的学习策略整合到GRASP算法构建阶段的可行性，以提高解决约束增量图绘制问题的性能。&lt;h4&gt;方法&lt;/h4&gt;提出图学习GRASP（GL-GRASP）混合方法，将元启发式算法与图表示学习相结合。应用于约束增量图绘制问题，分析不同的节点嵌入技术，特别是基于深度学习的策略，并采用原始积分指标根据解决方案所需时间评估质量。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基于深度学习的节点嵌入技术表现突出；2. 最佳GL-GRASP启发式算法在问题表现上优于文献中最先进的GRASP启发式算法；3. 在固定时间限制下对更密集实例进行的可扩展性测试证实了GL-GRASP启发式算法的稳健性。&lt;h4&gt;结论&lt;/h4&gt;将图表示学习与元启发式算法相结合的方法在解决约束增量图绘制问题时表现出色，不仅优于现有技术，而且在处理更密集实例时具有良好的可扩展性，证明了该混合方法的有效性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，将机器学习技术与元启发式算法相结合引起了广泛关注。许多尝试使用监督学习或强化学习来支持启发式方法的决策过程。然而，在某些情况下，这些技术被认为过于耗时，无法与手工设计的启发式方法竞争。本文提出了一种将元启发式算法与一种成本较低的学习策略相结合的混合方法，用于提取图的潜在结构，称为图表示学习（GRL）。为此，我们研究了约束增量图绘制问题（C-IGDP），这是一个层次化图可视化问题。关于此问题的方法文献有限，而贪婪随机搜索程序（GRASP）启发式方法已显示出有希望的结果。基于此，本文研究了将GRL整合到GRASP构建阶段的好处，我们称之为图学习GRASP（GL-GRASP）。在计算实验中，我们首先分析了考虑不同节点嵌入技术所取得的结果，其中基于深度学习的策略表现突出。评估采用了原始积分指标，根据所需时间评估解决方案的质量。根据该指标，最佳GL-GRASP启发式算法在问题表现上优于文献中最先进的GRASP启发式算法。在固定时间限制下对新生成的更密集实例进行的可扩展性测试进一步证实了GL-GRASP启发式算法的稳健性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hybridizing machine learning techniques with metaheuristics has attractedsignificant attention in recent years. Many attempts employ supervised orreinforcement learning to support the decision-making of heuristic methods.However, in some cases, these techniques are deemed too time-consuming and notcompetitive with hand-crafted heuristics. This paper proposes a hybridizationbetween metaheuristics and a less expensive learning strategy to extract thelatent structure of graphs, known as Graph Representation Learning (GRL). Forsuch, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), ahierarchical graph visualization problem. There is limited literature onmethods for this problem, for which Greedy Randomized Search Procedures (GRASP)heuristics have shown promising results. In line with this, this paperinvestigates the gains of incorporating GRL into the construction phase ofGRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computationalexperiments, we first analyze the results achieved considering different nodeembedding techniques, where deep learning-based strategies stood out. Theevaluation considered the primal integral measure that assesses the quality ofthe solutions according to the required time for such. According to thismeasure, the best GL-GRASP heuristics demonstrated superior performance thanstate-of-the-art literature GRASP heuristics for the problem. A scalabilitytest on newly generated denser instances under a fixed time limit furtherconfirmed the robustness of the GL-GRASP heuristics.</description>
      <author>example@mail.com (Bruna C. B. Charytitsch, María C. V. Nascimento)</author>
      <guid isPermaLink="false">2508.15949v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Noise, Adaptation, and Strategy: Assessing LLM Fidelity in Decision-Making</title>
      <link>http://arxiv.org/abs/2508.15926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMNLP 2025 (Main Conference)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种面向过程的评估框架，通过渐进式干预检验大型语言模型在不同级别外部指导下的适应能力，发现LLM在模拟人类决策行为方面存在差距。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型越来越多地被用于社会科学模拟，但对其模拟人类决策变异性和适应能力的研究较少。&lt;h4&gt;目的&lt;/h4&gt;提出一种面向过程的评估框架，检验LLM代理在不同级别外部指导和人类衍生噪声下的适应能力。&lt;h4&gt;方法&lt;/h4&gt;在二级拍卖中的非理性和报童问题中的决策偏差两个经典经济学任务上验证该框架。&lt;h4&gt;主要发现&lt;/h4&gt;1) LLM默认收敛于稳定且保守的策略，与人类行为不同；2) 风险框架指令可预测影响LLM行为但无法复制类人多样性；3) 通过上下文学习纳入人类数据可缩小差距但无法达到人类受试者的战略变异性。&lt;h4&gt;结论&lt;/h4&gt;存在行为保真度中的持续校准差距，未来LLM评估应考虑更多过程层面真实性，该评估方法为LLM在社会科学研究合成数据应用提供指导。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型越来越多地被用于社会科学模拟。虽然它们在推理和优化任务上的表现已被广泛评估，但对其模拟人类决策的变异性和适应能力的研究较少。我们提出了一种面向过程的评估框架，通过渐进式干预(内在性、指导和模仿)来检验LLM代理在不同级别的外部指导和人类衍生噪声下的适应能力。我们在两个经典经济学任务上验证了该框架：二级拍卖中的非理性和报童问题中的决策偏差，显示出LLM与人类之间的行为差距。我们发现，默认情况下，LLM收敛于稳定且保守的策略，这与观察到的人类行为不同。风险框架的指令可预测地影响LLM行为，但无法复制类人的多样性。通过上下文学习纳入人类数据可以缩小差距，但无法达到人类受试者的战略变异性。这些结果突显了行为保真度中持续的校准差距，并建议未来的LLM评估应考虑更多过程层面的真实性。我们提出了一种面向过程的方法来评估LLM在动态决策任务中的表现，为其在社会科学研究的合成数据应用提供指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are increasingly used in social sciencesimulations. While their performance on reasoning and optimization tasks hasbeen extensively evaluated, less attention has been paid to their ability tosimulate human decision-making's variability and adaptability. We propose aprocess-oriented evaluation framework with progressive interventions(Intrinsicality, Instruction, and Imitation) to examine how LLM agents adaptunder different levels of external guidance and human-derived noise. Wevalidate the framework on two classic economics tasks, irrationality in thesecond-price auction and decision bias in the newsvendor problem, showingbehavioral gaps between LLMs and humans.  We find that LLMs, by default, converge on stable and conservative strategiesthat diverge from observed human behaviors. Risk-framed instructions impact LLMbehavior predictably but do not replicate human-like diversity. Incorporatinghuman data through in-context learning narrows the gap but fails to reach humansubjects' strategic variability. These results highlight a persistent alignmentgap in behavioral fidelity and suggest that future LLM evaluations shouldconsider more process-level realism. We present a process-oriented approach forassessing LLMs in dynamic decision-making tasks, offering guidance for theirapplication in synthetic data for social science research.</description>
      <author>example@mail.com (Yuanjun Feng, Vivek Choudhary, Yash Raj Shrestha)</author>
      <guid isPermaLink="false">2508.15926v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty Quantification and Propagation for ACORN, a geometric deep learning tracking pipeline for HEP experiments</title>
      <link>http://arxiv.org/abs/2508.16518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 53 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种用于多步管道的不确定性量化方法，并将其应用于ACORN粒子跟踪管道，使用TrackML数据集进行实验。&lt;h4&gt;背景&lt;/h4&gt;粒子跟踪管道中的不确定性量化对于确保结果可靠性至关重要，特别是在高能物理实验中。&lt;h4&gt;目的&lt;/h4&gt;开发并应用不确定性量化过程来评估ACORN粒子跟踪管道中的数据不确定性和模型不确定性，研究不确定性的传播规律，以及分析训练数据集大小、输入数据几何和物理属性对不确定性的影响。&lt;h4&gt;方法&lt;/h4&gt;使用Monte Carlo Dropout方法测量管道步骤的数据和模型不确定性，研究不确定性沿管道的传播规律，并分析训练数据集大小、输入数据的几何和物理属性对不确定性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;随着训练数据集的增长，整体不确定性逐渐由认知不确定性主导，表明有足够的数据将ACORN模型训练到其全部潜力。ACORN管道在轨道重建方面具有高置信度，并且不会遭受GNN模型的校准错误。&lt;h4&gt;结论&lt;/h4&gt;ACORN管道在粒子跟踪方面表现出色，其不确定性量化方法有效，能够准确评估和传播不确定性。&lt;h4&gt;翻译&lt;/h4&gt;我们为多步管道开发了一种不确定性量化过程，并将其应用于ACORN粒子跟踪管道。我们的所有实验都是使用TrackML开放数据集进行的。使用Monte Carlo Dropout方法，我们测量了管道步骤的数据和模型不确定性，研究了它们如何沿管道传播，以及它们如何受训练数据集大小、输入数据的几何和物理属性的影响。我们将证明，对于我们的案例研究，随着训练数据集的增长，整体不确定性逐渐由认知不确定性主导，表明我们有足够的数据将我们选择的ACORN模型训练到其全部潜力。我们表明ACORN管道在轨道重建方面具有高置信度，并且不会遭受GNN模型的校准错误。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We have developed an Uncertainty Quantification process for multisteppipelines and applied it to the ACORN particle tracking pipeline. All ourexperiments are made using the TrackML open dataset. Using the Monte CarloDropout method, we measure the data and model uncertainties of the pipelinesteps, study how they propagate down the pipeline, and how they are impacted bythe training dataset's size, the input data's geometry and physical properties.We will show that for our case study, as the training dataset grows, theoverall uncertainty becomes dominated by aleatoric uncertainty, indicating thatwe had sufficient data to train the ACORN model we chose to its full potential.We show that the ACORN pipeline yields high confidence in the trackreconstruction and does not suffer from the miscalibration of the GNN model.</description>
      <author>example@mail.com (Lukas Péron, Paolo Calafiura, Xiangyang Ju, Jay Chan)</author>
      <guid isPermaLink="false">2508.16518v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering</title>
      <link>http://arxiv.org/abs/2508.16516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GNAQ的图节点感知动态量化方法，用于解决图神经网络在资源受限设备上部署的挑战，在保持推荐质量的同时显著提高了效率和速度。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在协同过滤推荐系统中表现出色，但由于高嵌入参数需求和计算成本，在资源受限的边缘设备上部署面临挑战。直接使用常见量化方法处理节点嵌入可能会忽略其基于图的结构，导致消息传递过程中的错误累积。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图的节点感知动态量化训练方法(GNAQ)，提高GNN在Top-K推荐中效率和准确性的平衡，使其能够在资源受限设备上有效部署。&lt;h4&gt;方法&lt;/h4&gt;GNAQ引入节点感知动态量化策略，根据图交互关系调整量化尺度；基于节点级特征分布初始化量化区间，并通过GNN层中的消息传递动态细化；使用图关系感知梯度估计替代传统直通估计器，确保训练期间更准确的梯度传播。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实数据集上，GNAQ在2位量化下Recall@10平均提高27.8%，NDCG@10平均提高17.6%；能够保持全精度模型的性能，同时将模型大小减少8到12倍；与量化基线方法相比，训练时间快两倍。&lt;h4&gt;结论&lt;/h4&gt;GNAQ通过利用图结构信息，有效解决了GNN在资源受限设备上部署的挑战，在保持推荐质量的同时显著提高了模型效率和训练速度。&lt;h4&gt;翻译&lt;/h4&gt;在协同过滤推荐系统领域，图神经网络已展现出卓越的性能，但由于其高嵌入参数需求和计算成本，在资源受限的边缘设备上部署面临重大挑战。直接在节点嵌入上使用常见量化方法可能会忽略其基于图的结构，导致消息传递过程中的错误累积并降低量化嵌入的质量。为此，我们提出了基于图的节点感知动态量化训练方法(GNAQ)，这是一种新颖的量化方法，利用图结构信息增强GNN在Top-K推荐中效率与准确性的平衡。GNAQ引入了节点感知动态量化策略，通过结合图交互关系，使量化尺度适应各个节点嵌入。具体而言，它基于节点级特征分布初始化量化区间，并通过GNN层中的消息传递动态细化这些区间。这种方法减轻了固定量化尺度导致的信息损失，并捕获了用户--item交互图中的层次语义特征。此外，GNAQ采用图关系感知梯度估计替代传统的直通估计器，确保训练期间更准确的梯度传播。在四个真实数据集上的广泛实验表明，GNAQ优于最先进的量化方法，包括BiGeaR和N2UQ，在2位量化下，Recall@10平均提高27.8%，NDCG@10平均提高17.6%。特别是，GNAQ能够在保持全精度模型性能的同时，将模型大小减少8到12倍；此外，与量化基线方法相比，训练时间快一倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the realm of collaborative filtering recommendation systems, Graph NeuralNetworks (GNNs) have demonstrated remarkable performance but face significantchallenges in deployment on resource-constrained edge devices due to their highembedding parameter requirements and computational costs. Using commonquantization method directly on node embeddings may overlooks their graph basedstructure, causing error accumulation during message passing and degrading thequality of quantized embeddings.To address this, we propose Graph basedNode-Aware Dynamic Quantization training for collaborative filtering (GNAQ), anovel quantization approach that leverages graph structural information toenhance the balance between efficiency and accuracy of GNNs for Top-Krecommendation. GNAQ introduces a node-aware dynamic quantization strategy thatadapts quantization scales to individual node embeddings by incorporating graphinteraction relationships. Specifically, it initializes quantization intervalsbased on node-wise feature distributions and dynamically refines them throughmessage passing in GNN layers. This approach mitigates information loss causedby fixed quantization scales and captures hierarchical semantic features inuser-item interaction graphs. Additionally, GNAQ employs graph relation-awaregradient estimation to replace traditional straight-through estimators,ensuring more accurate gradient propagation during training. Extensiveexperiments on four real-world datasets demonstrate that GNAQ outperformsstate-of-the-art quantization methods, including BiGeaR and N2UQ, by achievingaverage improvement in 27.8\% Recall@10 and 17.6\% NDCG@10 under 2-bitquantization. In particular, GNAQ is capable of maintaining the performance offull-precision models while reducing their model sizes by 8 to 12 times; inaddition, the training time is twice as fast compared to quantization baselinemethods.</description>
      <author>example@mail.com (Lin Li, Chunyang Li, Yu Yin, Xiaohui Tao, Jianwei Zhang)</author>
      <guid isPermaLink="false">2508.16516v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Fast and Accurate RFIC Performance Prediction via Pin Level Graph Neural Networks and Probabilistic Flow</title>
      <link>http://arxiv.org/abs/2508.16403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级、数据高效且拓扑感知的图神经网络模型，用于预测多种有源射频电路的关键性能指标，实验显示高预测精度且显著减少了训练数据需求。&lt;h4&gt;背景&lt;/h4&gt;准确预测有源射频电路性能对现代无线系统至关重要，但由于高度非线性、布局敏感行为以及传统仿真工具的高计算成本，这仍然具有挑战性。现有的机器学习代理模型通常需要大型数据集才能在不同拓扑结构间泛化，或准确建模偏斜和多模态性能指标。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级、数据高效且拓扑感知的图神经网络模型，用于预测多种有源射频电路拓扑结构的关键性能指标，如低噪声放大器、混频器、压控振荡器和功率放大器。&lt;h4&gt;方法&lt;/h4&gt;在器件终端级别对电路进行建模，以捕获晶体管级对称性并保留细粒度连接细节，实现可扩展的消息传递，同时减少数据需求。集成掩码自回归流输出头，以改进对复杂目标分布的建模稳健性。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示高预测精度，对称平均绝对百分比误差和平均相对误差分别平均为2.40%和2.91%。与先前工作相比，使用2.24倍更少的训练样本，平均相对误差提高了3.14倍。&lt;h4&gt;结论&lt;/h4&gt;由于电路到图的引脚级转换以及对射频指标复杂密度建模的机器学习架构的稳健性，该方法证明了其在快速准确的射频电路设计自动化方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;准确预测有源射频电路的性能对现代无线系统至关重要，但由于高度非线性、布局敏感的行为以及传统仿真工具的高计算成本，这仍然具有挑战性。现有的机器学习代理模型通常需要大型数据集才能在各种拓扑结构间泛化，或准确建模偏斜和多模态性能指标。在这项工作中，提出了一种轻量级、数据高效且拓扑感知的图神经网络模型，用于预测多种有源射频电路拓扑结构的关键性能指标，如低噪声放大器、混频器、压控振荡器和功率放大器。为了捕获晶体管级对称性并保留细粒度连接细节，电路在器件终端级别进行建模，实现了可扩展的消息传递，同时减少了数据需求。集成了掩码自回归流输出头，以提高对复杂目标分布建模的稳健性。数据集上的实验展示了高预测精度，对称平均绝对百分比误差和平均相对误差分别平均为2.40%和2.91%。由于电路到图的引脚级转换以及机器学习架构对射频指标复杂密度建模的稳健性，与先前工作相比，该方法在使用2.24倍更少训练样本的情况下，将平均相对误差提高了3.14倍，证明了该方法在快速准确的射频电路设计自动化方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the performance of active radio frequency (RF) circuitsis essential for modern wireless systems but remains challenging due to highlynonlinear, layout-sensitive behavior and the high computational cost oftraditional simulation tools. Existing machine learning (ML) surrogates oftenrequire large datasets to generalize across various topologies or to accuratelymodel skewed and multi-modal performance metrics. In this work, a lightweight,data-efficient, and topology-aware graph neural network (GNN) model is proposedfor predicting key performance metrics of multiple topologies of active RFcircuits such as low noise amplifiers (LNAs), mixers, voltage-controlledoscillators (VCOs), and PAs. To capture transistor-level symmetry and preservefine-grained connectivity details, circuits are modeled at the device-terminallevel, enabling scalable message passing while reducing data requirements.Masked autoregressive flow (MAF) output heads are incorporated to improverobustness in modeling complex target distributions. Experiments on datasetsdemonstrate high prediction accuracy, with symmetric mean absolute percentageerror (sMAPE) and mean relative error (MRE) averaging 2.40% and 2.91%,respectively. Owing to the pin-level conversion of circuit to graph and MLarchitecture robust to modeling complex densities of RF metrics, the MRE isimproved by 3.14x while using 2.24x fewer training samples compared to priorwork, demonstrating the method's effectiveness for rapid and accurate RFcircuit design automation.</description>
      <author>example@mail.com (Anahita Asadi, Leonid Popryho, Inna Partin-Vaisband)</author>
      <guid isPermaLink="false">2508.16403v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization</title>
      <link>http://arxiv.org/abs/2508.16200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures, 4 tables, 26 references, accepted at ACM  NanoCom'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索使用Set Transformer架构改进Flow-guided Localization技术，通过将纳米设备循环时间报告视为无序集合实现排列不变性，并集成深度生成模型进行合成数据生成以提高鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;Flow-guided Localization (FGL)可识别人体内含有诊断意义事件的空间区域，通过利用在血液循环中被动移动的纳米设备实现。现有FGL解决方案依赖固定拓扑图模型或手工特征，限制了对解剖变异性的适应性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;探索使用Set Transformer架构解决现有FGL解决方案的局限性，提高在数据稀缺和类别不平衡情况下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将纳米设备的循环时间报告视为无序集合，实现排列不变性和可变长度输入处理；集成CGAN、WGAN、WGAN-GP和CVAE等深度生成模型进行合成数据生成，这些模型训练后用于增强训练数据。&lt;h4&gt;主要发现&lt;/h4&gt;Set Transformer与图神经网络基线相比实现了相当分类精度，同时在解剖变异性的泛化方面提供了设计上的改进。&lt;h4&gt;结论&lt;/h4&gt;排列不变性模型和合成增强在鲁棒和可扩展的纳米级定位方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;Flow-guided Localization (FGL)使人们能够识别人体内含有诊断意义事件的空间区域。FGL通过利用在血液循环中被动移动的、能量受限的纳米设备来实现这一功能。现有的FGL解决方案依赖于具有固定拓扑结构的图模型或手工设计的特征，这限制了它们对解剖变异性的适应性并阻碍了可扩展性。在这项工作中，我们探索使用Set Transformer架构来解决这些限制。我们的公式将纳米设备的循环时间报告视为无序集合，实现了排列不变性、可变长度输入处理，而不依赖于空间先验。为了提高在数据稀缺和类别不平衡情况下的鲁棒性，我们通过深度生成模型(包括CGAN、WGAN、WGAN-GP和CVAE)集成了合成数据生成。这些模型被训练以复制基于血管区域标签条件的真实循环时间分布，并用于增强训练数据。我们的结果表明，Set Transformer与图神经网络(GNN)基线相比实现了相当分类精度，同时同时提供了设计上改进的解剖变异性泛化能力。这些发现突显了排列不变性模型和合成增强在鲁棒和可扩展纳米级定位方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flow-guided Localization (FGL) enables the identification of spatial regionswithin the human body that contain an event of diagnostic interest. FGL doesthat by leveraging the passive movement of energy-constrained nanodevicescirculating through the bloodstream. Existing FGL solutions rely on graphmodels with fixed topologies or handcrafted features, which limit theiradaptability to anatomical variability and hinder scalability. In this work, weexplore the use of Set Transformer architectures to address these limitations.Our formulation treats nanodevices' circulation time reports as unordered sets,enabling permutation-invariant, variable-length input processing withoutrelying on spatial priors. To improve robustness under data scarcity and classimbalance, we integrate synthetic data generation via deep generative models,including CGAN, WGAN, WGAN-GP, and CVAE. These models are trained to replicaterealistic circulation time distributions conditioned on vascular region labels,and are used to augment the training data. Our results show that the SetTransformer achieves comparable classification accuracy compared to GraphNeural Networks (GNN) baselines, while simultaneously providing by-designimproved generalization to anatomical variability. The findings highlight thepotential of permutation-invariant models and synthetic augmentation for robustand scalable nanoscale localization.</description>
      <author>example@mail.com (Mika Leo Hube, Filip Lemic, Ethungshan Shitiri, Gerard Calvo Bartra, Sergi Abadal, Xavier Costa Pérez)</author>
      <guid isPermaLink="false">2508.16200v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Joint Cache Placement and Routing in Satellite-Terrestrial Edge Computing Network: A GNN-Enabled DRL Approach</title>
      <link>http://arxiv.org/abs/2508.16184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了卫星地面边缘计算网络中的联合内容缓存和路由问题，提出了一种基于图神经网络和深度强化学习的框架来提高地理分布用户的服务质量。&lt;h4&gt;背景&lt;/h4&gt;低地球轨道卫星拓扑具有动态性，同时用户内容需求呈现异构性，这给卫星地面边缘计算网络中的服务优化带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提高地理分布用户在卫星地面边缘计算网络中的缓存服务质量，优化内容分发效率。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于学习的框架，将图神经网络与深度强化学习相结合；将卫星网络表示为动态图，通过GNN捕获空间和拓扑依赖性；将问题建模为马尔可夫决策过程，并应用软演员-评论家算法优化缓存策略。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法显著提高了内容交付成功率，同时减少了通信流量成本。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络和深度强化学习的联合内容缓存与路由框架能够有效应对动态卫星拓扑和异构内容需求带来的挑战，优化卫星地面边缘计算网络的服务性能。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们研究了卫星地面边缘计算网络中的联合内容缓存和路由问题，以提高地理分布用户的缓存服务质量。为应对低地球轨道卫星动态拓扑和异构内容需求带来的挑战，我们提出了一种整合图神经网络与深度强化学习的基于学习的框架。卫星网络被表示为动态图，其中图神经网络嵌入到深度强化学习代理中，以捕获空间和拓扑依赖性并支持感知路由的决策制定。通过将问题表述为马尔可夫决策过程并应用软演员-评论家算法来优化缓存策略。仿真结果表明，我们的方法显著提高了交付成功率并减少了通信流量成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this letter, we investigate the problem of joint content caching androuting in satellite-terrestrial edge computing networks (STECNs) to improvecaching service for geographically distributed users. To handle the challengesarising from dynamic low Earth orbit (LEO) satellite topologies andheterogeneous content demands, we propose a learning-based framework thatintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).The satellite network is represented as a dynamic graph, where GNNs areembedded within the DRL agent to capture spatial and topological dependenciesand support routing-aware decision-making. The caching strategy is optimized byformulating the problem as a Markov decision process (MDP) and applying softactor-critic (SAC) algorithm. Simulation results demonstrate that our approachsignificantly improves the delivery success rate and reduces communicationtraffic cost.</description>
      <author>example@mail.com (Yuhao Zheng, Ting You, Kejia Peng, Chang Liu)</author>
      <guid isPermaLink="false">2508.16184v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>FIRE-GNN: Force-informed, Relaxed Equivariance Graph Neural Network for Rapid and Accurate Prediction of Surface Properties</title>
      <link>http://arxiv.org/abs/2508.16012v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种名为FIRE-GNN的图神经网络模型，用于准确预测表面的功函数和断裂能，解决了传统第一性原理计算计算成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;表面的功函数和断裂能是决定材料在电子发射应用、半导体器件和异质催化中可行性的关键性质。第一性原理计算虽然准确，但计算成本高，加上表面的巨大搜索空间，使得使用密度泛函理论进行全面筛选不可行。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确且快速预测功函数和断裂能的方法，以便在广阔的化学空间中进行预测，并促进具有调整后表面性质的材料的发现。&lt;h4&gt;方法&lt;/h4&gt;引入FIRE-GNN（Force-Informed, Relaxed Equivariance Graph Neural Network），结合表面法线对称性破缺和机器学习原子间势推导的力信息。对最近的不变和等变架构进行了基准测试，分析了对称性破缺的影响，并评估了分布外泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;FIRE-GNN在功函数预测方面将平均绝对误差降低了两倍（降至0.065 eV），超过了以前的最先进水平。FIRE-GNN在功函数预测方面持续优于竞争模型。&lt;h4&gt;结论&lt;/h4&gt;该模型能够在广阔的化学空间中准确快速地预测功函数和断裂能，有助于发现具有调整后表面性质的材料。&lt;h4&gt;翻译&lt;/h4&gt;表面的功函数和断裂能是决定材料在电子发射应用、半导体器件和异质催化中可行性的关键性质。虽然第一性原理计算在预测这些性质方面是准确的，但其计算成本高，加上表面的巨大搜索空间，使得使用密度泛函理论进行全面筛选的方法不可行。在此，我们引入了FIRE-GNN（Force-Informed, Relaxed Equivariance Graph Neural Network），该方法结合了表面法线对称性破缺和机器学习原子间势(MLIP)推导的力信息，在功函数预测方面将平均绝对误差降低了两倍（降至0.065 eV），超过了以前的最先进水平。我们还对最近的不变和等变架构进行了基准测试，分析了对称性破缺的影响，并评估了分布外泛化能力，证明FIRE-GNN在功函数预测方面持续优于竞争模型。该模型能够在广阔的化学空间中准确快速地预测功函数和断裂能，并促进具有调整后表面性质的材料发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The work function and cleavage energy of a surface are critical propertiesthat determine the viability of materials in electronic emission applications,semiconductor devices, and heterogeneous catalysis. While first principlescalculations are accurate in predicting these properties, their computationalexpense combined with the vast search space of surfaces make a comprehensivescreening approach with density functional theory (DFT) infeasible. Here, weintroduce FIRE-GNN (Force-Informed, Relaxed Equivariance Graph Neural Network),which integrates surface-normal symmetry breaking and machine learninginteratomic potential (MLIP)-derived force information, achieving a twofoldreduction in mean absolute error (down to 0.065 eV) over the previousstate-of-the-art for work function prediction. We additionally benchmark recentinvariant and equivariant architectures, analyze the impact of symmetrybreaking, and evaluate out-of-distribution generalization, demonstrating thatFIRE-GNN consistently outperforms competing models for work functionpredictions. This model enables accurate and rapid predictions of the workfunction and cleavage energy across a vast chemical space and facilitates thediscovery of materials with tuned surface properties</description>
      <author>example@mail.com (Circe Hsu, Claire Schlesinger, Karan Mudaliar, Jordan Leung, Robin Walters, Peter Schindler)</author>
      <guid isPermaLink="false">2508.16012v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>HePGA: A Heterogeneous Processing-in-Memory based GNN Training Accelerator</title>
      <link>http://arxiv.org/abs/2508.16011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HePGA是一种基于3D异构PIM的加速器，用于GNN训练和推理，通过结合不同PIM设备的优势，实现了更高的能效和计算效率。&lt;h4&gt;背景&lt;/h4&gt;Processing-In-Memory (PIM)架构在加速图神经网络(GNN)训练和推理方面具有潜力，但不同PIM设备(如ReRAM、FeFET、PCM、MRAM和SRAM)在功耗、延迟、面积和非理想特性方面各有权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种能高效加速GNN训练的3D异构PIM架构，结合多种PIM设备的优势，实现高能效和高性能。&lt;h4&gt;方法&lt;/h4&gt;提出HePGA加速器，利用GNN层和相关计算内核的特性，优化它们在不同PIM设备和平面层的映射，通过3D集成将多种PIM设备组合在单一平台上。&lt;h4&gt;主要发现&lt;/h4&gt;实验分析显示，HePGA在能效(TOPS/W)和计算效率(TOPS/mm²)方面分别比现有PIM架构高出3.8倍和6.8倍，同时保持GNN预测准确性不变。&lt;h4&gt;结论&lt;/h4&gt;HePGA是一种有效的GNN训练加速方案，还能用于加速新兴Transformer模型的推理。&lt;h4&gt;翻译&lt;/h4&gt;内存处理(PIM)架构为加速图神经网络(GNN)训练和推理提供了一种有前景的方法。然而，各种PIM设备(如ReRAM、FeFET、PCM、MRAM和SRAM)在功耗、延迟、面积和非理想性方面各有不同的权衡。通过3D集成实现的异构多核架构可以在单一平台上组合多种PIM设备，实现能效高、性能强的GNN训练。在这项工作中，我们提出了一种用于GNN训练的基于3D异构PIM的加速器，称为HePGA。我们利用GNN层和相关计算内核的独特特性，优化它们在不同PIM设备和平面层上的映射。我们的实验分析表明，HePGA在能效(TOPS/W)和计算效率(TOPS/mm2)方面分别比现有的PIM架构高出3.8倍和6.8倍，同时不牺牲GNN预测准确性。最后，我们展示了HePGA在加速新兴Transformer模型推理方面的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Processing-In-Memory (PIM) architectures offer a promising approach toaccelerate Graph Neural Network (GNN) training and inference. However, variousPIM devices such as ReRAM, FeFET, PCM, MRAM, and SRAM exist, with each deviceoffering unique trade-offs in terms of power, latency, area, andnon-idealities. A heterogeneous manycore architecture enabled by 3D integrationcan combine multiple PIM devices on a single platform, to enableenergy-efficient and high-performance GNN training. In this work, we propose a3D heterogeneous PIM-based accelerator for GNN training referred to as HePGA.We leverage the unique characteristics of GNN layers and associated computingkernels to optimize their mapping on to different PIM devices as well as planartiers. Our experimental analysis shows that HePGA outperforms existingPIM-based architectures by up to 3.8x and 6.8x in energy-efficiency (TOPS/W)and compute efficiency (TOPS/mm2) respectively, without sacrificing the GNNprediction accuracy. Finally, we demonstrate the applicability of HePGA toaccelerate inferencing of emerging transformer models.</description>
      <author>example@mail.com (Chukwufumnanya Ogbogu, Gaurav Narang, Biresh Kumar Joardar, Janardhan Rao Doppa, Krishnendu Chakrabarty, Partha Pratim Pande)</author>
      <guid isPermaLink="false">2508.16011v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Simulation-Based Inference for Direction Reconstruction of Ultra-High-Energy Cosmic Rays with Radio Arrays</title>
      <link>http://arxiv.org/abs/2508.15991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures. Submitted to PRD. Reproducible code and  notebooks: sbi_uhecr_radio_recon v0.1.0 - Zenodo DOI 10.5281/zenodo.16895985;  GitHub https://github.com/oscar-macias/sbi_uhecr_radio_recon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于模拟的推断方法，结合物理信息图神经网络和正规化流后验分布，用于超高能宇宙射线观测站的方向重建。该方法在真实模拟数据上训练，提供了物理可解释的重建、良好校准的不确定性和快速推断，适用于未来的宇宙射线观测实验。&lt;h4&gt;背景&lt;/h4&gt;超高能宇宙射线观测站需要无偏的方向重建来实现与稀疏纳秒级无线电脉冲的多信使天文学。传统的显式似然方法通常依赖简化模型，可能导致结果偏差和不确定性估计不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供准确方向重建、良好校准的不确定性估计和快速推断的方法，特别适用于处理高度倾斜事件的宇宙射线观测实验。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于模拟的推断流程，将物理信息图神经网络与正规化流后验分布在'学习宇宙隐式似然推断'框架中耦合。每个事件通过解析平面波前拟合初始化；GNN学习天线信号间的时空相关性来改进估计；其冻结嵌入条件一个八块自回归流，返回完整的贝叶斯后验。该方法在约8000个使用ZHAireS代码生成的现实UHECR空气簇射模拟上进行训练，后验分布经过温度校准以满足经验覆盖目标。&lt;h4&gt;主要发现&lt;/h4&gt;在测试UHECR事件上展示了亚度数的中值角分辨率；名义的68%最高后验密度区域捕获了71%±2%的真实到达方向，表明不确定性校准是轻度保守的。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了物理可解释的重建、良好校准的不确定性和快速推断，非常适合即将到来的针对高度倾斜事件的宇宙射线观测实验，如GRAND、AugerPrime Radio、IceCube-Gen2、RNO-G和BEACON。&lt;h4&gt;翻译&lt;/h4&gt;超高能宇宙射线观测站需要无偏的方向重建，以便与稀疏的纳秒级无线电脉冲实现多信使天文学。显式似然方法通常依赖简化模型，这可能使结果产生偏差并低估不确定性。我们引入了一种基于模拟的推断流程，将物理信息图神经网络与正规化流后验分布在'学习宇宙隐式似然推断'框架中耦合。每个事件通过解析平面波前拟合初始化；GNN通过学习天线信号之间的时空相关性来改进这一估计；其冻结嵌入条件一个八块自回归流，返回完整的贝叶斯后验。在约8000个使用ZHAireS代码生成的现实UHECR空气簇射模拟上进行训练，后验分布经过温度校准以满足经验覆盖目标。我们在测试UHECR事件上展示了亚度数的中值角分辨率，并发现名义的68%最高后验密度区域捕获了71%±2%的真实到达方向，表明不确定性校准是轻度保守的。这种方法提供了物理可解释的重建、良好校准的不确定性和快速推断，非常适合即将到来的针对高度倾斜事件的实验，如GRAND、AugerPrime Radio、IceCube-Gen2、RNO-G和BEACON。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ultra-high-energy cosmic-ray (UHECR) observatories require unbiased directionreconstruction to enable multi-messenger astronomy with sparse,nanosecond-scale radio pulses. Explicit likelihood methods often rely onsimplified models, which may bias results and understate uncertainties. Weintroduce a simulation-based inference pipeline that couples a physics-informedgraph neural network (GNN) to a normalizing-flow posterior within the\textit{Learning the Universe Implicit Likelihood Inference} framework. Eachevent is seeded by an analytic plane-wavefront fit; the GNN refines thisestimate by learning spatiotemporal correlations among antenna signals, and itsfrozen embedding conditions an eight-block autoregressive flow that returns thefull Bayesian posterior. Trained on about $8,000$ realistic UHECR air-showersimulations generated with the ZHAireS code, the posteriors aretemperature-calibrated to meet empirical coverage targets. We demonstrate asub-degree median angular resolution on test UHECR events, and find that thenominal 68\% highest-posterior-density contours capture $71\% \pm 2\%$ of truearrival directions, indicating a mildly conservative uncertainty calibration.This approach provides physically interpretable reconstructions,well-calibrated uncertainties, and rapid inference, making it ideally suitedfor upcoming experiments targeting highly inclined events, such as GRAND,AugerPrime Radio, IceCube-Gen2, RNO-G, and BEACON.</description>
      <author>example@mail.com (Oscar Macias, Zachary Mason, Matthew Ho, Arsène Ferrière, Aurélien Benoit-Lévy, Matías Tueros)</author>
      <guid isPermaLink="false">2508.15991v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.14707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的模型驱动方法来训练视觉基础模型(VFM)，通过联合知识转移和保留来整合多个预训练教师模型，构建了一个强大的VFM，无需大量标记数据即可继承教师专业知识，并在多个视觉任务上优于现有数据中心模型。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(VFM)主要使用数据中心方法开发，这些方法需要大量高质量标签数据进行训练，这对缺乏大规模数据和高端GPU的机构构成了瓶颈。另一方面，许多开源视觉模型已经在特定领域数据上预训练，能够提炼和表示可跨多种应用转移的核心知识，但这些模型在开发通用VFM方面尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型驱动方法来训练视觉基础模型(VFM)，通过联合知识转移和保留，整合多个预训练教师模型，构建一个强大的通用VFM，无需大量标记数据即可继承教师专业知识，并支持多种下游任务。&lt;h4&gt;方法&lt;/h4&gt;提出一种新的模型驱动方法，通过以下步骤：在共享潜在空间中统一多个预训练教师模型，缓解其分布差异导致的'不平衡转移'问题；采用知识保留策略，将通用教师作为知识库，使用适配器模块整合特定目的教师的知识；统一和聚合现有模型，构建强大的VFM。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的VFM不仅提供了可泛化的视觉特征，还内在支持多种下游任务；广泛的实验表明，该VFM在图像分类、目标检测、语义分割和实例分割四个基本视觉任务上优于现有的数据中心模型。&lt;h4&gt;结论&lt;/h4&gt;通过统一和聚合现有模型，可以构建一个强大的VFM，继承教师专业知识而无需大量标记数据训练，且在多个视觉任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型(VFMs)主要使用数据中心方法开发。这些方法需要在大量通常带有高质量标签的数据上进行训练，这对缺乏大规模数据和高端GPU的大多数机构构成了瓶颈。另一方面，许多开源视觉模型已经在特定领域数据上进行了预训练，使它们能够提炼和表示一种可在多种应用间转移的核心知识形式。尽管这些模型是非常宝贵的资产，但在支持通用VFM的开发方面，它们仍然在很大程度上未被探索。在本文中，我们提出了一种新的模型驱动方法，通过联合知识转移和保留来训练VFMs。我们的方法在共享潜在空间中统一多个预训练的教师模型，以缓解它们分布差异造成的'不平衡转移'问题。此外，我们引入了一种知识保留策略，将通用教师作为知识库，使用适配器模块整合来自其他特定目的教师的知识。通过统一和聚合现有模型，我们构建了一个强大的VFM，能够继承教师的专业知识，而无需在大量标记数据上进行训练。我们的模型不仅提供了可泛化的视觉特征，还内在支持多种下游任务。广泛的实验证明，我们的VFM在图像分类、目标检测、语义分割和实例分割四个基本视觉任务上优于现有的数据中心模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models (VFMs) are predominantly developed usingdata-centric methods. These methods require training on vast amounts of datausually with high-quality labels, which poses a bottleneck for mostinstitutions that lack both large-scale data and high-end GPUs. On the otherhand, many open-source vision models have been pretrained on domain-specificdata, enabling them to distill and represent core knowledge in a form that istransferable across diverse applications. Even though these models are highlyvaluable assets, they remain largely under-explored in empowering thedevelopment of a general-purpose VFM. In this paper, we presents a newmodel-driven approach for training VFMs through joint knowledge transfer andpreservation. Our method unifies multiple pre-trained teacher models in ashared latent space to mitigate the ``imbalanced transfer'' issue caused bytheir distributional gaps. Besides, we introduce a knowledge preservationstrategy to take a general-purpose teacher as a knowledge base for integratingknowledge from the remaining purpose-specific teachers using an adapter module.By unifying and aggregating existing models, we build a powerful VFM to inheritteachers' expertise without needing to train on a large amount of labeled data.Our model not only provides generalizable visual features, but also inherentlysupports multiple downstream tasks. Extensive experiments demonstrate that ourVFM outperforms existing data-centric models across four fundamental visiontasks, including image classification, object detection, semantic and instancesegmentation.</description>
      <author>example@mail.com (Jiabo Huang, Chen Chen, Lingjuan Lyu)</author>
      <guid isPermaLink="false">2508.14707v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
  <item>
      <title>Attention-Based Explainability for Structure-Property Relationships</title>
      <link>http://arxiv.org/abs/2508.15493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了基于注意力的神经网络在揭示材料科学中结构-性质关系和潜在物理机制方面的潜力，以PbTiO3薄膜的铁电性质为案例研究。&lt;h4&gt;背景&lt;/h4&gt;机器学习方法正在成为材料科学中构建相关结构-性质关系的通用范式，基于多模态表征，但需要开发方法来解释相关模型的物理可解释性。&lt;h4&gt;目的&lt;/h4&gt;展示基于注意力的神经网络在揭示结构-性质关系和潜在物理机制方面的潜力，并开发物理可解释性方法。&lt;h4&gt;方法&lt;/h4&gt;通过分析注意力分数解开不同畴结构对极化翻转过程的影响；将基于注意力的Transformer模型作为直接可解释性工具和解释无监督机器学习表示的替代模型；比较注意力派生的可解释性分数与经典的SHAP分析。&lt;h4&gt;主要发现&lt;/h4&gt;注意力分析能够区分不同畴结构对极化翻转过程的影响；与自然语言处理应用相比，注意力机制在材料科学中能高效突出有意义的结构特征。&lt;h4&gt;结论&lt;/h4&gt;基于注意力的神经网络能够有效揭示材料科学中的结构-性质关系和物理机制，为材料科学中的模型解释提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;机器学习方法正在成为基于多模态表征构建材料科学中相关结构-性质关系的通用范式。然而，这需要开发方法来解释相关模型的物理可解释性。在此，我们展示了基于注意力的神经网络在揭示结构-性质关系和潜在物理机制方面的潜力，以PbTiO3薄膜的铁电性质作为案例研究。通过分析注意力分数，我们解开了不同畴结构对极化翻转过程的影响。基于注意力的Transformer模型被探索作为直接可解释性工具和解释通过无监督机器学习学习到的表示的替代模型，从而能够识别基于物理的相关性。我们将注意力派生的可解释性分数与经典的SHapley Additive exPlanations（SHAP）分析进行了比较，表明与自然语言处理中的应用相比，材料科学中的注意力机制在突出有意义的结构特征方面表现出高效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning methods are emerging as a universal paradigm forconstructing correlative structure-property relationships in materials sciencebased on multimodal characterization. However, this necessitates development ofmethods for physical interpretability of the resulting correlative models.Here, we demonstrate the potential of attention-based neural networks forrevealing structure-property relationships and the underlying physicalmechanisms, using the ferroelectric properties of PbTiO3 thin films as a casestudy. Through the analysis of attention scores, we disentangle the influenceof distinct domain patterns on the polarization switching process. Theattention-based Transformer model is explored both as a direct interpretabilitytool and as a surrogate for explaining representations learned via unsupervisedmachine learning, enabling the identification of physically groundedcorrelations. We compare attention-derived interpretability scores withclassical SHapley Additive exPlanations (SHAP) analysis and show that, incontrast to applications in natural language processing, attention mechanismsin materials science exhibit high efficiency in highlighting meaningfulstructural features.</description>
      <author>example@mail.com (Boris N. Slautin, Utkarsh Pratiush, Yongtao Liu, Hiroshi Funakubo, Vladimir V. Shvartsman, Doru C. Lupascu, Sergei V. Kalinin)</author>
      <guid isPermaLink="false">2508.15493v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Learning Protein-Ligand Binding in Hyperbolic Space</title>
      <link>http://arxiv.org/abs/2508.15480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了HypSeek，一种基于双曲几何的蛋白质-配体结合预测框架，能够有效捕捉分子相互作用的层次结构和精细亲和力变化，在虚拟筛选和亲和力排序任务中均表现出色。&lt;h4&gt;背景&lt;/h4&gt;蛋白质-配体结合预测是虚拟筛选和亲和力排序的核心，这两项是药物发现中的基本任务。现有的基于检索的方法将配体和蛋白质口袋嵌入欧几里得空间进行相似性搜索，但欧几里得嵌入的几何结构往往无法捕捉分子相互作用中固有的层次结构和精细的亲和力变化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更好地表示蛋白质-配体相互作用的方法，特别是能够捕捉层次结构和精细亲和力变化，以解决现有方法在处理活性悬崖等挑战性情况时的不足。&lt;h4&gt;方法&lt;/h4&gt;提出HypSeek，一种双曲表示学习框架，将配体、蛋白质口袋和序列嵌入洛伦兹模型双曲空间中。利用双曲空间的指数几何和负曲率，实现表达性强、亲和力敏感的嵌入。引入蛋白质引导的三塔架构，统一了虚拟筛选和亲和力排序在单一框架中。&lt;h4&gt;主要发现&lt;/h4&gt;HypSeek在DUD-E数据集上的虚拟筛选早期富集率从42.63提升到51.44（提升20.7%），在JACS数据集上的亲和力排序相关性从0.5774提升到0.7239（提升25.4%）。双曲几何能够有效建模全局活性和细微功能差异，特别是在活性悬崖等具有挑战性的情况下。&lt;h4&gt;结论&lt;/h4&gt;双曲几何表示学习框架为蛋白质-配体建模提供了强大的归纳偏置，能够有效捕捉分子相互作用的层次结构和精细亲和力变化，为药物发现提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质-配体结合预测是虚拟筛选和亲和力排序的核心，这两项是药物发现中的基本任务。虽然最近的基于检索的方法将配体和蛋白质口袋嵌入欧几里得空间进行相似性搜索，但欧几里得嵌入的几何结构往往无法捕捉分子相互作用中固有的层次结构和精细的亲和力变化。在这项工作中，我们提出了HypSeek，一种双曲表示学习框架，将配体、蛋白质口袋和序列嵌入洛伦兹模型双曲空间中。通过利用双曲空间的指数几何和负曲率，HypSeek能够实现表达性强、亲和力敏感的嵌入，可以有效地建模全局活性和细微的功能差异——特别是在具有挑战性的情况下，如活性悬崖，即结构相似的配体表现出大的亲和力差距。我们的模型统一了虚拟筛选和亲和力排序在一个单一框架中，引入了蛋白质引导的三塔架构来增强表示结构。HypSeek在DUD-E上的虚拟筛选早期富集率从42.63提升到51.44（+20.7%），在JACS上的亲和力排序相关性从0.5774提升到0.7239（+25.4%），证明了双曲几何在这两项任务中的优势，并强调了其作为蛋白质-配体建模的强大归纳偏置的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein-ligand binding prediction is central to virtual screening andaffinity ranking, two fundamental tasks in drug discovery. While recentretrieval-based methods embed ligands and protein pockets into Euclidean spacefor similarity-based search, the geometry of Euclidean embeddings often failsto capture the hierarchical structure and fine-grained affinity variationsintrinsic to molecular interactions. In this work, we propose HypSeek, ahyperbolic representation learning framework that embeds ligands, proteinpockets, and sequences into Lorentz-model hyperbolic space. By leveraging theexponential geometry and negative curvature of hyperbolic space, HypSeekenables expressive, affinity-sensitive embeddings that can effectively modelboth global activity and subtle functional differences-particularly inchallenging cases such as activity cliffs, where structurally similar ligandsexhibit large affinity gaps. Our mode unifies virtual screening and affinityranking in a single framework, introducing a protein-guided three-towerarchitecture to enhance representational structure. HypSeek improves earlyenrichment in virtual screening on DUD-E from 42.63 to 51.44 (+20.7%) andaffinity ranking correlation on JACS from 0.5774 to 0.7239 (+25.4%),demonstrating the benefits of hyperbolic geometry across both tasks andhighlighting its potential as a powerful inductive bias for protein-ligandmodeling.</description>
      <author>example@mail.com (Jianhui Wang, Wenyu Zhu, Bowen Gao, Xin Hong, Ya-Qin Zhang, Wei-Ying Ma, Yanyan Lan)</author>
      <guid isPermaLink="false">2508.15480v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials</title>
      <link>http://arxiv.org/abs/2508.15392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 4 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CITE数据集，这是首个也是最大的催化材料异构文本属性引用图基准，包含超过438K节点和1.2M条边，跨越四种关系类型。&lt;h4&gt;背景&lt;/h4&gt;现实世界中存在大量带有文本属性的异构图(TAGs)，但缺乏大规模基准数据集，这阻碍了表示学习方法的发展和公平比较。&lt;h4&gt;目的&lt;/h4&gt;解决异构文本属性图(TAGs)领域缺乏大规模基准数据集的问题，促进表示学习方法的发展。&lt;h4&gt;方法&lt;/h4&gt;构建CITE数据集，建立标准化评估流程，在节点分类任务上进行基准测试，并对CITE的异构和文本属性进行消融实验；同时比较四类学习范式：同构图模型、异构图模型、以大语言模型为中心的模型和图+大语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验比较了不同学习范式在CITE数据集上的表现，包括同构图模型、异构图模型、大语言模型为中心的模型以及图+大语言模型的性能。&lt;h4&gt;结论&lt;/h4&gt;提供了CITE数据集概述、标准化评估协议以及跨不同建模范式的基线和消融实验，为异构文本属性图的研究提供了重要资源。&lt;h4&gt;翻译&lt;/h4&gt;文本属性图(TAGs)在现实世界系统中普遍存在，其中每个节点都携带其自身的文本特征。在许多情况下，这些图本质上是异构的，包含多种节点类型和多样化的边类型。尽管这类异构TAGs普遍存在，但仍然缺乏大规模基准数据集。这一短缺已成为关键瓶颈，阻碍了在异构文本属性图上表示学习方法的发展和公平比较。在本文中，我们介绍了CITE - 催化信息文本实体图，这是首个也是最大的催化材料异构文本属性引用图基准。CITE包含超过438K个节点和1.2M条边，跨越四种关系类型。此外，我们建立了标准化评估流程，并在节点分类任务上进行了广泛的基准测试，以及对CITE的异构和文本属性进行了消融实验。我们比较了四类学习范式，包括同构图模型、异构图模型、以大语言模型为中心的模型和图+大语言模型。总之，我们提供了(i) CITE数据集概述，(ii)标准化评估协议，以及(iii)跨不同建模范式的基线和消融实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-attributed graphs(TAGs) are pervasive in real-world systems,where eachnode carries its own textual features. In many cases these graphs areinherently heterogeneous, containing multiple node types and diverse edgetypes. Despite the ubiquity of such heterogeneous TAGs, there remains a lack oflarge-scale benchmark datasets. This shortage has become a critical bottleneck,hindering the development and fair comparison of representation learningmethods on heterogeneous text-attributed graphs. In this paper, we introduceCITE - Catalytic Information Textual Entities Graph, the first and largestheterogeneous text-attributed citation graph benchmark for catalytic materials.CITE comprises over 438K nodes and 1.2M edges, spanning four relation types. Inaddition, we establish standardized evaluation procedures and conduct extensivebenchmarking on the node classification task, as well as ablation experimentson the heterogeneous and textual properties of CITE. We compare four classes oflearning paradigms, including homogeneous graph models, heterogeneous graphmodels, LLM(Large Language Model)-centric models, and LLM+Graph models. In anutshell, we provide (i) an overview of the CITE dataset, (ii) standardizedevaluation protocols, and (iii) baseline and ablation experiments acrossdiverse modeling paradigms.</description>
      <author>example@mail.com (Chenghao Zhang, Qingqing Long, Ludi Wang, Wenjuan Cui, Jianjun Yu, Yi Du)</author>
      <guid isPermaLink="false">2508.15392v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction</title>
      <link>http://arxiv.org/abs/2508.15378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出EvoFormer框架，解决了动态图级嵌入中的结构访问偏差和突变演化失明问题，通过结构感知Transformer模块和演化敏感时间模块，显著提升了图相似性排序、时间异常检测和时间分段任务的性能。&lt;h4&gt;背景&lt;/h4&gt;动态图级嵌入旨在捕捉网络结构的演变，这对建模现实场景至关重要。然而，现有方法存在两个关键问题：结构访问偏差和突变演化失明。&lt;h4&gt;目的&lt;/h4&gt;提出EvoFormer框架，解决现有动态图级嵌入方法中的结构访问偏差和突变演化失明问题，提高动态图级表示学习的性能。&lt;h4&gt;方法&lt;/h4&gt;EvoFormer包含两个核心模块：1)结构感知Transformer模块，基于节点结构角色的位置编码减轻结构访问偏差；2)演化敏感时间模块，通过三步策略建模时间演化：随机游走时间戳分类、图级时间分段、分段感知时间自注意力与边演化预测任务结合。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准数据集上的广泛评估显示，EvoFormer在图相似性排序、时间异常检测和时间分段任务中实现了最先进的性能，有效验证了其在修正结构和时间偏差方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;EvoFormer通过创新性地解决结构访问偏差和突变演化失明问题，显著提升了动态图级表示学习的能力，为现实世界动态网络建模提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;动态图级嵌入旨在捕捉网络结构的演变，这对建模现实场景至关重要。然而，现有方法面临两个关键但未被充分探讨的问题：结构访问偏差，其中随机游走采样过度强调高度节点，导致冗余和有噪声的结构表示；以及突变演化失明，由于僵化或过于简单的时间建模策略，无法有效检测突然的结构变化，导致不一致的时间嵌入。为克服这些挑战，我们提出EvoFormer，一个专为动态图级表示学习设计的演化感知Transformer框架。为减轻结构访问偏差，EvoFormer引入了结构感知Transformer模块，基于节点结构角色的位置编码，使模型能够全局区分和准确表示节点结构。为克服突变演化失明，EvoFormer采用演化敏感时间模块，通过三步顺序策略明确建模时间演化：(I)随机游走时间戳分类，生成初始时间感知图级嵌入；(II)图级时间分段，将图流划分为反映结构一致性的时期；(III)分段感知时间自注意力与边演化预测任务结合，使模型能够精确捕捉分段边界并感知结构演化趋势，有效适应快速的时间变化。在五个基准数据集上的广泛评估证实，EvoFormer在图相似性排序、时间异常检测和时间分段任务中实现了最先进的性能，验证了其在修正结构和时间偏差方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761134&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic graph-level embedding aims to capture structural evolution innetworks, which is essential for modeling real-world scenarios. However,existing methods face two critical yet under-explored issues: Structural VisitBias, where random walk sampling disproportionately emphasizes high-degreenodes, leading to redundant and noisy structural representations; and AbruptEvolution Blindness, the failure to effectively detect sudden structuralchanges due to rigid or overly simplistic temporal modeling strategies,resulting in inconsistent temporal embeddings. To overcome these challenges, wepropose EvoFormer, an evolution-aware Transformer framework tailored fordynamic graph-level representation learning. To mitigate Structural Visit Bias,EvoFormer introduces a Structure-Aware Transformer Module that incorporatespositional encoding based on node structural roles, allowing the model toglobally differentiate and accurately represent node structures. To overcomeAbrupt Evolution Blindness, EvoFormer employs an Evolution-Sensitive TemporalModule, which explicitly models temporal evolution through a sequentialthree-step strategy: (I) Random Walk Timestamp Classification, generatinginitial timestamp-aware graph-level embeddings; (II) Graph-Level TemporalSegmentation, partitioning the graph stream into segments reflectingstructurally coherent periods; and (III) Segment-Aware Temporal Self-Attentioncombined with an Edge Evolution Prediction task, enabling the model toprecisely capture segment boundaries and perceive structural evolution trends,effectively adapting to rapid temporal shifts. Extensive evaluations on fivebenchmark datasets confirm that EvoFormer achieves state-of-the-art performancein graph similarity ranking, temporal anomaly detection, and temporalsegmentation tasks, validating its effectiveness in correcting structural andtemporal biases.</description>
      <author>example@mail.com (Haodi Zhong, Liuxin Zou, Di Wang, Bo Wang, Zhenxing Niu, Quan Wang)</author>
      <guid isPermaLink="false">2508.15378v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>MLLMRec: Exploring the Potential of Multimodal Large Language Models in Recommender Systems</title>
      <link>http://arxiv.org/abs/2508.15304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MLLMRec是一种基于大型多模态语言模型的多模态推荐框架，通过图像语义转换、用户偏好净化和物品图优化策略解决了现有方法中的关键问题，实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;多模态推荐结合用户行为数据和物品模态特征来揭示用户偏好，相比传统推荐表现更优，但现有方法仍存在用户多模态表示初始化不佳和物品-物品图质量问题等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态推荐方法中用户表示初始化不当和物品-物品图包含噪声边的问题，提升推荐性能。&lt;h4&gt;方法&lt;/h4&gt;提出MLLMRec框架，一方面将物品图像转换为高质量语义描述并与文本元数据融合，为用户构建行为描述列表并输入MLLM推理净化用户偏好；另一方面设计阈值控制去噪和拓扑感知增强策略优化物品-物品图。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开数据集上的实验表明，MLLMRec实现了最先进的性能，比最佳基线平均提升38.53%。&lt;h4&gt;结论&lt;/h4&gt;MLLMRec通过有效利用多模态语言模型和优化图结构，成功解决了现有多模态推荐方法中的关键问题，显著提升了推荐系统性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态推荐通常将用户行为数据与物品的模态特征相结合以揭示用户偏好，相比传统推荐表现出更优越的性能。然而，现有方法仍存在两个关键问题：(1)用户多模态表示的初始化方法要么无法感知行为，要么受到噪声污染；(2)基于KNN的物品-物品图包含相似度低的噪声边，且缺乏受众共现关系。为解决这些问题，我们提出了MLLMRec，这是一种新颖的MLLM驱动的多模态推荐框架，包含两种物品-物品图优化策略。一方面，首先使用MLLM将物品图像转换为高质量的语义描述，然后与物品的文本元数据融合。接着，我们为每个用户构建行为描述列表并输入MLLM，以推理出包含交互动机的净化用户偏好。另一方面，我们设计了阈值控制去噪和拓扑感知增强策略来优化次优物品-物品图，从而增强物品表示学习。在三个公开数据集上的大量实验表明，MLLMRec实现了最先进的性能，比最佳基线平均提升38.53%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal recommendation typically combines the user behavioral data withthe modal features of items to reveal user's preference, presenting superiorperformance compared to the conventional recommendations. However, existingmethods still suffer from two key problems: (1) the initialization methods ofuser multimodal representations are either behavior-unperceived ornoise-contaminated, and (2) the KNN-based item-item graph contains noisy edgeswith low similarities and lacks audience co-occurrence relationships. Toaddress such issues, we propose MLLMRec, a novel MLLM-driven multimodalrecommendation framework with two item-item graph refinement strategies. On theone hand, the item images are first converted into high-quality semanticdescriptions using an MLLM, which are then fused with the textual metadata ofitems. Then, we construct a behavioral description list for each user and feedit into the MLLM to reason about the purified user preference containinginteraction motivations. On the other hand, we design the threshold-controlleddenoising and topology-aware enhancement strategies to refine the suboptimalitem-item graph, thereby enhancing the item representation learning. Extensiveexperiments on three publicly available datasets demonstrate that MLLMRecachieves the state-of-the-art performance with an average improvement of 38.53%over the best baselines.</description>
      <author>example@mail.com (Yuzhuo Dang, Xin Zhang, Zhiqiang Pan, Yuxiao Duan, Wanyu Chen, Fei Cai, Honghui Chen)</author>
      <guid isPermaLink="false">2508.15304v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Learning ECG Representations via Poly-Window Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.15225v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been accepted for publication in IEEE-EMBS  International Conference on Biomedical and Health Informatics 2025. The final  published version will be available via IEEE Xplore&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多窗口对比学习框架，用于心电图分析，通过提取多个时间窗口构建正样本对，学习时不变和生理上有意义的特征，显著提高了模型性能并减少了训练时间。&lt;h4&gt;背景&lt;/h4&gt;心电图分析是心血管疾病诊断的基础，但深度学习模型的性能常受限于标注数据的有限获取。自监督对比学习已成为从无标签信号中学习鲁棒ECG表征的强大方法，但现有方法大多只生成成对的增强视图，未能充分利用ECG记录的丰富时间结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够充分利用ECG时间结构的自监督学习方法，以提升模型性能并减少训练时间。&lt;h4&gt;方法&lt;/h4&gt;从每个ECCG实例中提取多个时间窗口构建正样本对，通过统计方法最大化它们的一致性；受慢特征分析原理启发，明确鼓励模型学习跨时间持续存在的时不变和生理上有意义的特征。&lt;h4&gt;主要发现&lt;/h4&gt;多窗口对比学习在多标签超类分类中始终优于传统双视图方法，达到更高的AUROC（0.891 vs 0.888）和F1分数（0.680 vs 0.679）；需要少至四倍的预训练轮次（32 vs 128）；总预训练时间减少14.8%；尽管处理多个窗口，但仍显著减少了训练轮次和总计算时间。&lt;h4&gt;结论&lt;/h4&gt;多窗口对比学习成为自动化ECG分析的一种高效可扩展范式，并为生物医学时间序列数据的自监督表征学习提供了有前景的通用框架。&lt;h4&gt;翻译&lt;/h4&gt;心电图分析是心血管疾病诊断的基础，但深度学习模型的性能常受限于标注数据的有限获取。自监督对比学习已成为从无标签信号中学习鲁棒ECG表征的强大方法。然而，大多数现有方法只生成成对的增强视图，未能充分利用ECG记录的丰富时间结构。在这项工作中，我们提出了一个多窗口对比学习框架。我们从每个ECG实例中提取多个时间窗口来构建正样本对，并通过统计方法最大化它们的一致性。受慢特征分析原理启发，我们的方法明确鼓励模型学习跨时间持续的时不变和生理上有意义的特征。我们在PTB-XL数据集上通过大量实验和消融研究验证了我们的方法。结果表明，多窗口对比学习在多标签超类分类中始终优于传统的双视图方法，达到更高的AUROC（0.891 vs 0.888）和F1分数（0.680 vs 0.679），同时需要少至四倍的预训练轮次（32 vs 128）和14.8%的总预训练时间减少。尽管每个样本处理多个窗口，我们仍实现了训练轮次和总计算时间的显著减少，使我们的方法对于训练基础模型具有实用性。通过大量消融，我们确定了最佳设计选择，并展示了在各种超参数下的鲁棒性。这些发现确立了多窗口对比学习作为自动化ECG分析的一种高效可扩展范式，并为生物医学时间序列数据的自监督表征学习提供了有前景的通用框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrocardiogram (ECG) analysis is foundational for cardiovascular diseasediagnosis, yet the performance of deep learning models is often constrained bylimited access to annotated data. Self-supervised contrastive learning hasemerged as a powerful approach for learning robust ECG representations fromunlabeled signals. However, most existing methods generate only pairwiseaugmented views and fail to leverage the rich temporal structure of ECGrecordings. In this work, we present a poly-window contrastive learningframework. We extract multiple temporal windows from each ECG instance toconstruct positive pairs and maximize their agreement via statistics. Inspiredby the principle of slow feature analysis, our approach explicitly encouragesthe model to learn temporally invariant and physiologically meaningful featuresthat persist across time. We validate our approach through extensiveexperiments and ablation studies on the PTB-XL dataset. Our results demonstratethat poly-window contrastive learning consistently outperforms conventionaltwo-view methods in multi-label superclass classification, achieving higherAUROC (0.891 vs. 0.888) and F1 scores (0.680 vs. 0.679) while requiring up tofour times fewer pre-training epochs (32 vs. 128) and 14.8% in total wall clockpre-training time reduction. Despite processing multiple windows per sample, weachieve a significant reduction in the number of training epochs and totalcomputation time, making our method practical for training foundational models.Through extensive ablations, we identify optimal design choices and demonstraterobustness across various hyperparameters. These findings establish poly-windowcontrastive learning as a highly efficient and scalable paradigm for automatedECG analysis and provide a promising general framework for self-supervisedrepresentation learning in biomedical time-series data.</description>
      <author>example@mail.com (Yi Yuan, Joseph Van Duyn, Runze Yan, Zhuoyi Huang, Sulaiman Vesal, Sergey Plis, Xiao Hu, Gloria Hyunjung Kwak, Ran Xiao, Alex Fedorov)</author>
      <guid isPermaLink="false">2508.15225v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer</title>
      <link>http://arxiv.org/abs/2508.15215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为SleepDIFFormer的睡眠阶段分类方法，通过多变量微分变换器架构处理EEG和EOG信号，实现了跨域对齐训练，提高了在未见数据集上的泛化能力，并在五个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;睡眠阶段分类对评估睡眠质量和诊断睡眠障碍至关重要，但手动检查脑电图特征既耗时又易出错。现有的机器学习和深度学习方法面临EEG和EOG信号的非平稳性和变异性挑战，导致泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理EEG和EOG信号的睡眠阶段分类方法，提高在未见数据集上的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;提出SleepDIFFormer方法，使用多变量微分变换器架构(MDTA)处理EEG和EOG时序数据，通过跨域对齐训练，减轻空间和时间注意力噪声，学习域不变的联合EEG-EOG表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在五个不同的睡眠分期数据集上取得了最先进的性能，消融分析表明微分注意力权重与特征性睡眠脑电图模式高度相关。&lt;h4&gt;结论&lt;/h4&gt;SleepDIFFormer为推进自动化睡眠阶段分类及其在睡眠质量评估中的应用提供了新方法，源代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;睡眠阶段分类对于评估睡眠质量和诊断失眠等睡眠障碍至关重要。然而，手动检查每个阶段的脑电图特征既耗时又容易出错。尽管机器学习和深度学习方法已被积极开发，但它们仍面临脑电图(EEG)和眼电图(EOG)信号的非平稳性和变异性带来的挑战，导致在未见过的数据集上泛化能力差。本研究通过开发多变量微分变换器(SleepDIFFormer)进行联合EEG和EOG表示学习，提出了睡眠阶段分类方法。具体而言，SleepDIFFormer使用我们的多变量微分变换器架构(MDTA)处理EEG和EOG信号，通过跨域对齐进行训练。我们的方法减轻了空间和时间注意力噪声，同时通过特征分布对齐学习域不变的联合EEG-EOG表示，从而能够泛化到未见的目标数据集。经验上，我们在五个不同的睡眠分期数据集上评估了我们的方法，并与现有方法进行了比较，取得了最先进的性能。我们还对SleepDIFFormer进行了彻底的消融分析，并解释了微分注意力权重，突显了它们与特征性睡眠脑电图模式的相关性。这些发现对推进自动化睡眠阶段分类及其在睡眠质量评估中的应用具有重要意义。我们的源代码已在https://github.com/Ben1001409/SleepDIFFormer公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classification of sleep stages is essential for assessing sleep quality anddiagnosing sleep disorders such as insomnia. However, manual inspection of EEGcharacteristics for each stage is time-consuming and prone to human error.Although machine learning and deep learning methods have been activelydeveloped, they continue to face challenges from the non-stationarity andvariability of electroencephalography (EEG) and electrooculography (EOG)signals, often leading to poor generalization on unseen datasets. This researchproposed a Sleep Stage Classification method by developing MultivariateDifferential Transformer (SleepDIFFormer) for joint EEG and EOG representationlearning. Specifically, SleepDIFFormer was developed to process EEG and EOGsignals using our Multivariate Differential Transformer Architecture (MDTA) fortime series, trained with cross-domain alignment. Our method mitigated spatialand temporal attention noise while learning a domain-invariant joint EEG-EOGrepresentation through feature distribution alignment, thereby enablinggeneralization to unseen target datasets. Empirically, we evaluated our methodon five different sleep staging datasets and compared it with existingapproaches, achieving state-of-the-art performance. We also conducted thoroughablation analyses of SleepDIFFormer and interpreted the differential attentionweights, highlighting their relevance to characteristic sleep EEG patterns.These findings have implications for advancing automated sleep stageclassification and its application to sleep quality assessment. Our source codeis publicly available at https://github.com/Ben1001409/SleepDIFFormer</description>
      <author>example@mail.com (Benjamin Wei Hao Chin, Yuin Torng Yew, Haocheng Wu, Lanxin Liang, Chow Khuen Chan, Norita Mohd Zain, Siti Balqis Samdin, Sim Kuan Goh)</author>
      <guid isPermaLink="false">2508.15215v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Kernel-based Equalized Odds: A Quantification of Accuracy-Fairness Trade-off in Fair Representation Learning</title>
      <link>http://arxiv.org/abs/2508.15084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于核的等化机会(EO)准则新公式EO_k，用于监督设置下的公平表征学习，能够严格量化独立性、分离性和校准性三个核心公平目标。&lt;h4&gt;背景&lt;/h4&gt;公平表征学习(FRL)旨在减轻对敏感属性的歧视同时保持预测准确性，但现有方法难以同时满足多个公平性准则。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时满足多种公平性准则的核方法，并在理论和实证上验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;提出基于核的EO_k准则，定义其经验对应物ÊO_k，推导集中不等式提供性能保证，并在无偏和有偏条件下分析其特性。&lt;h4&gt;主要发现&lt;/h4&gt;EO_k在无偏条件下同时满足独立性和分离性，在有偏条件下唯一保持预测准确性同时下限独立性和校准性，提供了公平准则间权衡的统一分析；ÊO_k可在二次时间内计算并有线性近似，具有理论保证。&lt;h4&gt;结论&lt;/h4&gt;EO_k为公平表征学习提供了统一的理论框架，为未来原则性且可证明公平的算法设计奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种用于监督设置公平表征学习的基于核的等化机会(EO)准则新公式EO_k。FRL的核心目标是在减轻对敏感属性S的歧视的同时，保持对目标变量Y的预测准确性。我们提出的准则能够对三个核心公平目标进行严格且可解释的量化：独立性(预测Ŷ与敏感属性S独立)、分离性(在目标属性Y条件下，预测Ŷ与敏感属性S独立)和校准性(在预测Ŷ条件下，目标属性Y与敏感属性S独立)。在无偏(Y与S独立)和有偏(Y依赖于S)条件下，我们证明了EO_k在前者中同时满足独立性和分离性，在后者中唯一保持预测准确性同时下限独立性和校准性，从而提供了这些公平准则间权衡的统一分析特征。我们进一步定义了经验对应物ÊO_k，这是一个基于核的统计量，可以在二次时间内计算，也有线性时间近似可用。推导了ÊO_k的集中不等式，提供了性能保证和误差边界，这些可作为公平合规的实际证明。虽然我们的重点是理论发展，但这些结果为未来实证研究中原则性且可证明公平的算法设计奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel kernel-based formulation of the Equalized Odds(EO) criterion, denoted as $EO_k$, for fair representation learning (FRL) insupervised settings. The central goal of FRL is to mitigate discriminationregarding a sensitive attribute $S$ while preserving prediction accuracy forthe target variable $Y$. Our proposed criterion enables a rigorous andinterpretable quantification of three core fairness objectives: independence(prediction $\hat{Y}$ is independent of $S$), separation (also known asequalized odds; prediction $\hat{Y}$ is independent with $S$ conditioned ontarget attribute $Y$), and calibration ($Y$ is independent of $S$ conditionedon the prediction $\hat{Y}$). Under both unbiased ($Y$ is independent of $S$)and biased ($Y$ depends on $S$) conditions, we show that $EO_k$ satisfies bothindependence and separation in the former, and uniquely preserves predictiveaccuracy while lower bounding independence and calibration in the latter,thereby offering a unified analytical characterization of the tradeoffs amongthese fairness criteria. We further define the empirical counterpart,$\hat{EO}_k$, a kernel-based statistic that can be computed in quadratic time,with linear-time approximations also available. A concentration inequality for$\hat{EO}_k$ is derived, providing performance guarantees and error bounds,which serve as practical certificates of fairness compliance. While our focusis on theoretical development, the results lay essential groundwork forprincipled and provably fair algorithmic design in future empirical studies.</description>
      <author>example@mail.com (Yijin Ni, Xiaoming Huo)</author>
      <guid isPermaLink="false">2508.15084v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Graph Structure Learning with Temporal Graph Information Bottleneck for Inductive Representation Learning</title>
      <link>http://arxiv.org/abs/2508.14859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in the 28th European Conference on Artificial Intelligence  (ECAI), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GTGIB的框架，用于解决时序图学习中的归纳表示学习问题，通过结合图结构学习和时序图信息瓶颈技术，有效处理动态网络中的节点和边演变，以及新节点加入的挑战。&lt;h4&gt;背景&lt;/h4&gt;时序图学习对于处理动态网络中随时间演变的节点和边以及持续加入的新节点至关重要。在这样的场景下，归纳表示学习面临两大主要挑战：有效表示未见节点和减轻嘈杂或冗余的图信息。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理动态网络中节点和边演变以及新节点加入的归纳表示学习框架，解决有效表示未见节点和减轻嘈杂或冗余图信息的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出GTGIB框架，结合图结构学习（GSL）和时序图信息瓶颈（TGIB）。设计了一种新颖的两步GSL基础结构增强器来丰富和优化节点邻域，并通过理论证明和实验验证其有效性和效率。TGIB通过将信息瓶颈原理扩展到时序图，基于推导的可处理TGIB目标函数通过变分近似来规范边和特征，实现稳定高效的优化。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界数据集上的链接预测任务中，基于GTGIB的模型在归纳设置下优于现有方法，并且在传递设置下表现出显著且一致的改进。&lt;h4&gt;结论&lt;/h4&gt;GTGIB框架有效解决了时序图学习中的归纳表示学习挑战，通过结合图结构学习和时序图信息瓶颈技术，能够有效处理动态网络中的节点和边演变以及新节点加入的情况，在各种设置下都表现出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;时序图学习对于动态网络至关重要，在这些网络中节点和边随时间演变，并且新节点持续加入系统。在这种情况下，归纳表示学习面临两大主要挑战：有效表示未见节点和减轻嘈杂或冗余的图信息。我们提出了GTGIB，一个将图结构学习（GSL）与时序图信息瓶颈（TGIB）相结合的多功能框架。我们设计了一种新颖的两步基于GSL的结构增强器，以丰富和优化节点邻域，并通过理论证明和实验证明了其有效性和效率。TGIB通过将信息瓶颈原理扩展到时序图，基于我们通过变分近似推导的可处理TGIB目标函数来规范边和特征，实现稳定和高效的优化。基于GTGIB的模型在四个真实世界数据集上评估用于链接预测；在归纳设置下，它们在所有数据集上都优于现有方法，并且在传递设置下有显著且一致的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal graph learning is crucial for dynamic networks where nodes and edgesevolve over time and new nodes continuously join the system. Inductiverepresentation learning in such settings faces two major challenges:effectively representing unseen nodes and mitigating noisy or redundant graphinformation. We propose GTGIB, a versatile framework that integrates GraphStructure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). Wedesign a novel two-step GSL-based structural enhancer to enrich and optimizenode neighborhoods and demonstrate its effectiveness and efficiency throughtheoretical proofs and experiments. The TGIB refines the optimized graph byextending the information bottleneck principle to temporal graphs, regularizingboth edges and features based on our derived tractable TGIB objective functionvia variational approximation, enabling stable and efficient optimization.GTGIB-based models are evaluated to predict links on four real-world datasets;they outperform existing methods in all datasets under the inductive setting,with significant and consistent improvement in the transductive setting.</description>
      <author>example@mail.com (Jiafeng Xiong, Rizos Sakellariou)</author>
      <guid isPermaLink="false">2508.14859v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Global-Distribution Aware Scenario-Specific Variational Representation Learning Framework</title>
      <link>http://arxiv.org/abs/2508.14493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025, 6 pages, 1 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种全局分布感知的场景特定变分表示学习框架(GSVR)，用于解决多场景推荐中的数据稀疏问题，帮助学习更鲁棒的场景特定表示。&lt;h4&gt;背景&lt;/h4&gt;随着电子商务的发展，商业平台需要提供适应不同场景的推荐以满足用户多样化的购物偏好。&lt;h4&gt;目的&lt;/h4&gt;解决当前多场景推荐方法中使用共享底层表示阻碍模型捕获场景独特性的问题，以及用户和项目交互在不同场景中变化导致的数据稀疏问题。&lt;h4&gt;方法&lt;/h4&gt;提出GSVR框架，使用概率模型为每个用户和项目在每个场景中生成场景特定分布，通过变分推断估计；引入全局知识感知的多项式分布作为先验知识，调节后验分布学习，确保相似用户和项目的分布相似性，减轻稀疏场景中记录较少的用户或项目被淹没的风险。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验结果证实了GSVR在协助现有多场景推荐方法学习更鲁棒表示方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;GSVR框架可直接应用于现有的多场景方法，有效解决了数据稀疏问题，帮助学习更鲁棒的场景特定表示。&lt;h4&gt;翻译&lt;/h4&gt;随着电子商务的出现，商业平台提供的推荐必须适应多样化场景以满足用户不同的购物偏好。当前方法通常使用统一框架为不同场景提供个性化推荐。然而，它们往往使用共享的底层表示，这部分阻碍了模型捕获场景独特性的能力。理想情况下，用户和项目在不同场景中应表现出特定特征，需要学习场景特定表示来区分场景。然而，用户和项目交互在不同场景中的变化导致数据稀疏问题，阻碍了场景特定表示的获取。为了学习鲁棒的场景特定表示，我们引入了一种全局分布感知的场景特定变分表示学习框架(GSVR)，可直接应用于现有的多场景方法。具体而言，考虑到有限样本带来的不确定性，我们的方法采用概率模型为每个用户和项目在每个场景中生成场景特定分布，通过变分推断(VI)进行估计。此外，我们引入全局知识感知的多项式分布作为先验知识，调节后验用户和项目分布的学习，确保兴趣相似的用户和具有相似辅助信息的项目的分布相似性。这减轻了在稀疏场景中记录较少的用户或项目被淹没的风险。大量的实验结果证实了GSVR在协助现有多场景推荐方法学习更鲁棒表示方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760866&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the emergence of e-commerce, the recommendations provided by commercialplatforms must adapt to diverse scenarios to accommodate users' varyingshopping preferences. Current methods typically use a unified framework tooffer personalized recommendations for different scenarios. However, they oftenemploy shared bottom representations, which partially hinders the model'scapacity to capture scenario uniqueness. Ideally, users and items shouldexhibit specific characteristics in different scenarios, prompting the need tolearn scenario-specific representations to differentiate scenarios. Yet,variations in user and item interactions across scenarios lead to data sparsityissues, impeding the acquisition of scenario-specific representations. To learnrobust scenario-specific representations, we introduce a Global-DistributionAware Scenario-Specific Variational Representation Learning Framework (GSVR)that can be directly applied to existing multi-scenario methods. Specifically,considering the uncertainty stemming from limited samples, our approach employsa probabilistic model to generate scenario-specific distributions for each userand item in each scenario, estimated through variational inference (VI).Additionally, we introduce the global knowledge-aware multinomial distributionsas prior knowledge to regulate the learning of the posterior user and itemdistributions, ensuring similarities among distributions for users with akininterests and items with similar side information. This mitigates the risk ofusers or items with fewer records being overwhelmed in sparse scenarios.Extensive experimental results affirm the efficacy of GSVR in assistingexisting multi-scenario recommendation methods in learning more robustrepresentations.</description>
      <author>example@mail.com (Moyu Zhang, Yujun Jin, Jinxin Hu, Yu Zhang)</author>
      <guid isPermaLink="false">2508.14493v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Machine learning classification of black holes in the mass--spin diagram</title>
      <link>http://arxiv.org/abs/2508.14316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提出了一个质量-自旋图来分类黑洞和研究它们的形成路径，类似于赫罗图，允许黑洞作为红移的演化轨迹，结合不同黑洞种类的形成、吸积和合并历史。&lt;h4&gt;背景&lt;/h4&gt;缺乏一个类似于赫罗图的黑洞分类系统，使得研究黑洞形成路径变得困难。&lt;h4&gt;目的&lt;/h4&gt;创建一个质量-自旋图，用于分类黑洞并研究它们的形成路径，结合黑洞的形成、吸积和合并历史。&lt;h4&gt;方法&lt;/h4&gt;构建黑洞连续谱，使用二元种群合成软件比较自旋规定，应用监督和无监督机器学习聚类方法进行黑洞种群分类，并使用深度学习通过变分自编码器进行潜在空间表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了可能的黑洞主序列如宇宙吸积和分层合并树；质量-自旋图暴露了建模差异；无监督聚类可恢复典型种群边界；深度学习方法有助于重叠子类的聚类；监督随机森林可准确恢复聚类；半监督方法有发展潜力；无监督分类器性能仍具挑战。&lt;h4&gt;结论&lt;/h4&gt;质量-自旋图可连接引力波和电磁观测与理论模型，促进了机器学习在天文学中的应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了用于分类黑洞和研究其形成路径的质量-自旋图，提供了类似于赫罗图的类比。这使得黑洞的演化轨迹可作为红移的函数，结合各种黑洞种类的形成、吸积和合并历史。从初始质量和自旋函数以及近似红移演化的真实黑洞连续谱揭示了可能的黑洞主序列，如宇宙时间尺度上的持续相干吸积(即宇宙吸积)或分层合并树。在恒星质量范围内，我们使用二元种群合成软件比较了三个关于Wolf-Rayet祖先潮汐演化的自旋规定，展示了质量-自旋图如何暴露有趣的建模差异。然后，我们通过应用监督和无监督机器学习聚类方法对质量-自旋数据集进行黑洞种群分类。虽然无监督聚类几乎可以恢复典型种群边界(恒星质量、中等质量和超大质量)，但更复杂的方法利用深度学习通过变分自编码器进行潜在空间表示学习，有助于对在质量-自旋空间中高度重叠的子类进行真实数据集的聚类。我们发现监督随机森林可以根据底层数据集的复杂性从学习到的潜在空间表示中准确恢复正确的聚类，半监督方法显示出进一步开发的潜力，而无监督分类器的性能是一个重大挑战。我们的研究结果促进了未来机器学习应用，并证明质量-自旋图可用于连接引力波和电磁观测与理论模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the mass--spin diagram for classifying black holes and studyingtheir formation pathways, providing an analogue to the Hertzsprung-Russelldiagram. This allows for black hole evolutionary tracks as a function ofredshift, combining formation, accretion, and merger histories for the varietyof black hole populations. A realistic black hole continuum constructed frominitial mass and spin functions and approximate redshift evolution revealspossible black hole main sequences, such as sustained coherent accretionthrough cosmic time (i.e., Cosmic Accretion) or hierarchical merger trees. Inthe stellar-mass regime, we use a binary population synthesis software tocompare three spin prescriptions for tidal evolution of Wolf-Rayet progenitors,showing how the mass--spin diagram exposes interesting modeling differences. Wethen classify black hole populations by applying supervised and unsupervisedmachine learning clustering methods to mass--spin datasets. While bareunsupervised clustering can nearly recover canonical population boundaries(stellar-mass, intermediate-mass, and supermassive), a more sophisticatedapproach utilizing deep learning via variational autoencoders for latent spacerepresentation learning aids in clustering of realistic datasets withsubclasses that highly overlap in mass--spin space. We find that a supervisedrandom forest can accurately recover the correct clusters from the learnedlatent space representation depending on the complexity of the underlyingdataset, semi-supervised methods show potential for further development, andthe performance of unsupervised classifiers is a great challenge. Our findingsmotivate future machine learning applications and demonstrate that themass--spin diagram can be used to connect gravitational-wave andelectromagnetic observations with theoretical models.</description>
      <author>example@mail.com (Nathan Steinle, Samar Safi-Harb)</author>
      <guid isPermaLink="false">2508.14316v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Constrained Random Phase Approximation: the spectral method</title>
      <link>http://arxiv.org/abs/2508.15368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures, to be published in Physical Review B, Poster  presented at PSI-K, Lausanne (August 2025). Talk given at Workshop "The  determination of Hubbard parameters: progress, pitfalls, and prospects",  Gandia (September 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的约束随机相位近似方法(spectral cRPA, s-cRPA)，在多种测试中表现优于现有cRPA方法，能够更准确地预测材料性质，解决了数值稳定性和U值低估问题。&lt;h4&gt;背景&lt;/h4&gt;现有随机相位近似方法在处理Scandium和Copper的3d壳层填充变化时存在局限性，标准cRPA方法低估了Hubbard U相互作用值，projector cRPA方法在充满d壳层时会出现负相互作用值问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的约束RPA方法(s-cRPA)，解决标准cRPA中U值被低估的问题，提高数值稳定性，并开发计算多中心相互作用的功能和低缩放变体。&lt;h4&gt;方法&lt;/h4&gt;提出spectral cRPA方法，在Scandium和Copper上通过变化3d壳层填充进行比较，应用于CaFeO₃系统，使用电子转换提高数值稳定性，开发了计算多中心相互作用的功能和具有压缩Matsubara网格的低缩放变体。&lt;h4&gt;主要发现&lt;/h4&gt;s-cRPA始终产生更大的Hubbard U相互作用值；应用于CaFeO₃系统时，产生的相互作用参数更接近DFT+U中所需的参数，成功诱导实验观察到的绝缘状态；解决了projector cRPA方法在充满d壳层时出现负相互作用值的问题，提供了更好的数值稳定性。&lt;h4&gt;结论&lt;/h4&gt;s-cRPA比标准cRPA更稳健，有效克服了已知的U值低估问题，是一种有前途的研究工具，对研究社区有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的约束随机相位近似方法，称为谱约束随机相位近似(spectral cRPA, s-cRPA)，并通过变化3d壳层填充，将其与已建立的cRPA方法在Scandium和Copper上进行比较。s-cRPA始终产生更大的Hubbard U相互作用值。应用于真实系统CaFeO₃时，s-cRPA产生的相互作用参数显著更接近DFT+U中所需的参数，以诱导实验观察到的绝缘状态，克服了标准密度泛函预测的金属行为。我们还解决了projector cRPA方法在充满d壳层时出现负相互作用值的问题，证明s-cRPA通过电子转换提供了更好的数值稳定性。总体而言，s-cRPA更加稳健，有效克服了标准cRPA中已知的U值低估问题，使其成为研究社区的一种有前途的工具。此外，我们通过增加计算多中心相互作用的功能来增强实现，以分析空间衰减，并开发了一种具有压缩Matsubara网格的低缩放变体，以高效获得全频率依赖相互作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new constrained Random Phase Approximation (cRPA) method, termedspectral cRPA (s-cRPA), and compare it to established cRPA approaches forScandium and Copper by varying the 3d shell filling. The s-cRPA consistentlyyields larger Hubbard $U$ interaction values. Applied to the realistic systemCaFeO$_3$, s-cRPA produces interaction parameters significantly closer to thoserequired within DFT+$U$ to induce the experimentally observed insulating state,overcoming the metallic behavior predicted by standard density functionals. Wealso address the issue of negative interaction values found in the projectorcRPA method for filled d-shells, demonstrating that s-cRPA offers superiornumerical stability through electron conversion. Overall, s-cRPA is more robustand effectively overcomes the known underestimation of $U$ in standard cRPA,making it a promising tool for the community. Additionally, we have enhancedour implementation with features for computing multi-center interactions toanalyze spatial decay and developed a low-scaling variant with a compressedMatsubara grid to efficiently obtain full frequency-dependent interactions.</description>
      <author>example@mail.com (Merzuk Kaltak, Alexander Hampel, Martin Schlipf, Indukuru Ramesh Reddy, Bongae Kim, Georg Kresse)</author>
      <guid isPermaLink="false">2508.15368v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features</title>
      <link>http://arxiv.org/abs/2508.15353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Optical Memory and Neural Networks, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了RCDINO，一种基于多模态变换器的三维物体检测模型，通过融合DINOv2基础模型的语义丰富表示来增强视觉特征，在nuScenes数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;三维物体检测对自动驾驶和机器人技术至关重要，需要有效融合来自摄像头和雷达的多模态数据。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于多模态变换器的模型(RCDINO)，通过融合预训练的DINOv2基础模型提供的语义丰富表示来增强视觉骨干特征。&lt;h4&gt;方法&lt;/h4&gt;RCDINO是一种基于多模态变换器的模型，它将视觉骨干特征与DINOv2基础模型提供的语义丰富表示融合，从而丰富视觉表示，同时保持与基线架构的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，RCDINO在雷达-摄像头模型中达到了最先进的性能，NDS为56.4，mAP为48.1。&lt;h4&gt;结论&lt;/h4&gt;RCDINO通过融合DINOv2基础模型的语义丰富表示，有效提升了三维物体检测的性能，同时保持了与基线架构的兼容性。&lt;h4&gt;翻译&lt;/h4&gt;三维物体检测对自动驾驶和机器人技术至关重要，依赖于有效融合来自摄像头和雷达的多模态数据。本文提出了RCDINO，一种基于多模态变换器的模型，通过融合预训练的DINOv2基础模型提供的语义丰富表示来增强视觉骨干特征。这种方法丰富了视觉表示，提高了模型的检测性能，同时保持了与基线架构的兼容性。在nuScenes数据集上的实验表明，RCDINO在雷达-摄像头模型中达到了最先进的性能，NDS为56.4，mAP为48.1。我们的实现在https://github.com/OlgaMatykina/RCDINO可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何增强雷达-相机融合的3D目标检测性能的问题。这个问题在自动驾驶和机器人领域非常重要，因为3D目标检测是安全有效环境感知的核心任务。相机提供高分辨率图像但缺乏深度信息，而雷达提供可靠的距离和速度信息但空间分辨率低且语义细节不足。有效融合这两种互补数据是持续挑战，现有方法常因视觉特征语义丰富度有限而难以准确检测远处物体，这对可靠自主感知至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有雷达-相机融合模型中视觉特征语义丰富度有限，难以检测远处物体。他们考虑引入预训练DINOv2基础模型的语义特征来增强视觉特征，同时保持与基线架构的兼容性。设计了一个轻量级适配器模块，将DINOv2表示整合到检测流程中。该方法借鉴了RCTrans架构的查询机制和顺序解码器设计，受BEVCar使用DINOv2与适配器的启发，并采用了Futr3D的基于支柱的雷达处理方法和类似U-Net的密集编码器结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入预训练DINOv2模型的语义丰富特征来增强视觉特征，提升对远处和语义复杂物体的检测能力。整体流程包括：1)视觉编码器提取图像特征；2)DINOv2适配器处理并融合DINOv2语义特征；3)雷达稀疏编码器处理雷达点数据；4)雷达密集编码器处理雷达信号稀疏性；5)顺序Transformer解码器融合多模态数据。DINOv2适配器通过注入器将特征注入DINOv2中间层，经过推理后由提取器检索修改后的特征，最后与原始主干特征融合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引入RCDINO模型，通过轻量级适配器整合DINOv2语义特征；2)提供解码器组件独立于特征信息量丰富度的消融研究；3)在公共基准和真实数据上验证方法；4)开源实现促进研究。相比之前工作，不同之处在于：不是简单拼接或使用注意力机制融合特征，而是通过DINOv2适配器将语义丰富特征注入视觉主干；不是直接使用DINOv2作为独立编码器，而是作为视觉特征增强器；采用两阶段解码器设计，即使一个模态特征信息量较少也能提高性能；在nuScenes数据集上实现了最先进的雷达-相机模型性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RCDINO通过引入轻量级DINOv2适配器模块增强视觉特征语义表示，显著提升了雷达-相机融合的3D目标检测性能，特别是在检测远处和语义复杂物体方面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional object detection is essential for autonomous driving androbotics, relying on effective fusion of multimodal data from cameras andradar. This work proposes RCDINO, a multimodal transformer-based model thatenhances visual backbone features by fusing them with semantically richrepresentations from the pretrained DINOv2 foundation model. This approachenriches visual representations and improves the model's detection performancewhile preserving compatibility with the baseline architecture. Experiments onthe nuScenes dataset demonstrate that RCDINO achieves state-of-the-artperformance among radar-camera models, with 56.4 NDS and 48.1 mAP. Ourimplementation is available at https://github.com/OlgaMatykina/RCDINO.</description>
      <author>example@mail.com (Olga Matykina, Dmitry Yudin)</author>
      <guid isPermaLink="false">2508.15353v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Tree-like Pairwise Interaction Networks</title>
      <link>http://arxiv.org/abs/2508.15678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为树状成对交互网络(PIN)的新型神经网络架构，用于表格数据中的特征交互建模，特别是在保险定价等预测建模任务中。PIN通过模拟决策树结构的共享前馈神经网络架构明确捕捉特征间的成对交互，具有内在可解释性，并能高效计算SHAP值。实证研究表明，PIN在预测准确性上优于传统和现代神经网络基准，同时提供了特征交互和预测贡献的深入见解。&lt;h4&gt;背景&lt;/h4&gt;表格数据中的特征交互建模是预测建模中的一个关键挑战，例如在保险定价等应用中。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型神经网络架构，能够明确捕捉特征间的交互作用，并具有良好的可解释性和计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出树状成对交互网络(PIN)，这是一种通过共享前馈神经网络架构模拟决策树结构的新型神经网络，能够明确捕捉特征间的成对交互。&lt;h4&gt;主要发现&lt;/h4&gt;1) PIN设计上具有内在可解释性，可直接检查交互效应；2) 由于只涉及成对交互，可高效计算SHAP值；3) PIN与GA2Ms、梯度提升机和图神经网络等已建立的模型有联系；4) 在法国汽车保险数据集上，PIN在预测准确性上优于传统和现代神经网络基准；5) PIN提供了特征如何相互交互及贡献于预测的见解。&lt;h4&gt;结论&lt;/h4&gt;PIN在预测性能和可解释性方面都表现出色，是一种有效的特征交互建模方法。&lt;h4&gt;翻译&lt;/h4&gt;表格数据中的特征交互建模仍然是预测建模中的一个关键挑战，例如在保险定价中的应用。本文提出了一种名为树状成对交互网络(PIN)的新型神经网络架构，它通过模拟决策树结构的共享前馈神经网络架构明确捕捉特征间的成对交互。PIN设计上具有内在可解释性，允许直接检查交互效应。此外，由于只涉及成对交互，它允许高效计算SHAP值。我们强调了PIN与GA2Ms、梯度提升机和图神经网络等已建立模型之间的联系。在流行的法国汽车保险数据集上的实证结果表明，PIN在预测准确性上优于传统和现代神经网络基准，同时提供了关于特征如何相互交互以及它们如何贡献于预测的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling feature interactions in tabular data remains a key challenge inpredictive modeling, for example, as used for insurance pricing. This paperproposes the Tree-like Pairwise Interaction Network (PIN), a novel neuralnetwork architecture that explicitly captures pairwise feature interactionsthrough a shared feed-forward neural network architecture that mimics thestructure of decision trees. PIN enables intrinsic interpretability by design,allowing for direct inspection of interaction effects. Moreover, it allows forefficient SHapley's Additive exPlanation (SHAP) computations because it onlyinvolves pairwise interactions. We highlight connections between PIN andestablished models such as GA2Ms, gradient boosting machines, and graph neuralnetworks. Empirical results on the popular French motor insurance dataset showthat PIN outperforms both traditional and modern neural networks benchmarks inpredictive accuracy, while also providing insight into how features interactwith each another and how they contribute to the predictions.</description>
      <author>example@mail.com (Ronald Richman, Salvatore Scognamiglio, Mario V. Wüthrich)</author>
      <guid isPermaLink="false">2508.15678v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Predictive models for strain energy in condensed phase reactions</title>
      <link>http://arxiv.org/abs/2508.15592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种图神经网络模型，用于预测凝聚相中环化反应的应变能，从而更准确地模拟分子材料的热激活化学过程。&lt;h4&gt;背景&lt;/h4&gt;当前方法结合分子动力学模拟和随机化学反应描述，但基于几何标准选择可能反应的方法常导致显著的分子应变和不真实的结构。&lt;h4&gt;目的&lt;/h4&gt;探究反应位点周围局部分子环境对分子应变能量和反应速率的影响，并开发能预测环化反应应变能的图神经网络模型。&lt;h4&gt;方法&lt;/h4&gt;开发图神经网络模型，在从MD模拟获得的聚丙烯腈激活期间凝聚相反应的大型数据集上进行训练，用于预测环化反应的应变能。&lt;h4&gt;主要发现&lt;/h4&gt;反应位点周围的局部分子环境在决定分子应变能量和相关反应速率方面起着关键作用。&lt;h4&gt;结论&lt;/h4&gt;所开发的图神经网络模型可用于调整凝聚系统中的相对反应速率，增进对复杂材料中热激活化学过程的理解。&lt;h4&gt;翻译&lt;/h4&gt;凝聚相中热激活化学的分子建模对于理解聚合物、解聚和其他分子材料的加工步骤至关重要。当前方法通常将分子动力学模拟与对预定化学反应的随机描述相结合。可能的反应通常基于几何标准（如反应原子之间的捕获距离）进行选择。尽管这些模拟提供了有价值的见解，但用于确定可能反应的近似方法常常导致显著的分子应变和不真实的结构。我们表明，反应位点周围的局部分子环境在决定所得分子应变能量和相关反应速率方面起着关键作用。我们开发了一种图神经网络，能够根据反应位点周围的预反应、局部分子环境预测环化反应相关的应变能。该模型在从MD模拟中获得的聚丙烯腈激活期间凝聚相反应的大型数据集上进行训练，可用于调整凝聚系统中的相对反应速率，并推进我们对复杂材料中热激活化学过程的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular modeling of thermally activated chemistry in condensed phases isessential to understand polymerization, depolymerization, and other processingsteps of molecular materials. Current methods typically combine moleculardynamics (MD) simulations to describe short-time relaxation with a stochasticdescription of predetermined chemical reactions. Possible reactions are oftenselected on the basis of geometric criteria, such as a capture distance betweenreactive atoms. Although these simulations have provided valuable insight, theapproximations used to determine possible reactions often lead to significantmolecular strain and unrealistic structures. We show that the local molecularenvironment surrounding the reactive site plays a crucial role in determiningthe resulting molecular strain energy and, in turn, the associated reactionrates. We develop a graph neural network capable of predicting the strainenergy associated with a cyclization reaction from the pre-reaction, local,molecular environment surrounding the reactive site. The model is trained on alarge dataset of condensed-phase reactions during the activation ofpolyacrylonitrile (PAN) obtained from MD simulations and can be used to adjustrelative reaction rates in condensed systems and advance our understanding ofthermally activated chemical processes in complex materials</description>
      <author>example@mail.com (Baptiste Martin, Shukai Yao, Chunyu Li, Anthony Bocahut, Matthew Jackson, Alejandro Strachan)</author>
      <guid isPermaLink="false">2508.15592v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Let's Grow an Unbiased Community: Guiding the Fairness of Graphs via New Links</title>
      <link>http://arxiv.org/abs/2508.15499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FairGuide的新框架，通过引入新链接来引导图神经网络结构向公平性方向发展，解决因图结构偏见导致的公平性问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在多种应用中取得显著成功，但由于图结构中的偏见，在公平性方面面临重大挑战。原始用户图结构通常存在偏见，但通过引入新链接，可以将这些现有结构引导向无偏见的方向。&lt;h4&gt;目的&lt;/h4&gt;提出FairGuide框架，通过引入新链接来指导图结构向公平性方向发展，从而增强下游应用的公平性。&lt;h4&gt;方法&lt;/h4&gt;引入可微的社区检测任务作为伪下游任务，确保在公平性引导的图上训练的下游任务的公平性；使用来自公平性引导目标的元梯度来识别显著增强结构公平性的新链接；理论分析表明优化伪任务中的公平性可以有效增强结构公平性，促进各种下游应用的公平性泛化。&lt;h4&gt;主要发现&lt;/h4&gt;通过公平性引导的新链接可以促进无偏见的社区形成；优化伪任务中的公平性可以增强结构公平性；所提出的方法在各种基于图的公平性任务中具有有效性和可泛化性。&lt;h4&gt;结论&lt;/h4&gt;FairGuide框架通过引入新链接来引导图结构向公平性方向发展，并通过理论分析和实验验证了其有效性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）在多种应用中取得了显著成功。然而，由于图结构中的偏见，图神经网络在公平性方面面临重大挑战。尽管原始用户图结构通常存在偏见，但通过引入新链接，将这些现有结构引导向无偏见的方向是很有前景的。通过新链接的公平性引导可以促进无偏见的社区形成，从而增强下游应用的公平性。为解决这一问题，我们提出了一种名为FairGuide的新框架。具体来说，为确保在公平性引导的图上训练的下游任务的公平性，我们引入了一个可微的社区检测任务作为伪下游任务。我们的理论分析进一步证明，在伪任务中优化公平性可以有效增强结构公平性，促进各种下游应用的公平性泛化。此外，FairGuide采用了一种有效策略，利用从公平性引导目标中衍生的元梯度来识别显著增强结构公平性的新链接。大量的实验结果证明了我们提出的方法在各种基于图的公平性任务中的有效性和可泛化性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved remarkable success across diverseapplications. However, due to the biases in the graph structures, graph neuralnetworks face significant challenges in fairness. Although the original usergraph structure is generally biased, it is promising to guide these existingstructures toward unbiased ones by introducing new links. The fairness guidancevia new links could foster unbiased communities, thereby enhancing fairness indownstream applications. To address this issue, we propose a novel frameworknamed FairGuide. Specifically, to ensure fairness in downstream tasks trainedon fairness-guided graphs, we introduce a differentiable community detectiontask as a pseudo downstream task. Our theoretical analysis further demonstratesthat optimizing fairness within this pseudo task effectively enhancesstructural fairness, promoting fairness generalization across diversedownstream applications. Moreover, FairGuide employs an effective strategywhich leverages meta-gradients derived from the fairness-guidance objective toidentify new links that significantly enhance structural fairness. Extensiveexperimental results demonstrate the effectiveness and generalizability of ourproposed method across a variety of graph-based fairness tasks.</description>
      <author>example@mail.com (Jiahua Lu, Huaxiao Liu, Shuotong Bai, Junjie Xu, Renqiang Luo, Enyan Dai)</author>
      <guid isPermaLink="false">2508.15499v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs</title>
      <link>http://arxiv.org/abs/2508.15468v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为JEDI-linear的新型图神经网络架构，具有线性计算复杂性和硬件高效性，解决了在FPGA硬件触发系统上部署GNN的挑战，实现了低于60纳秒的延迟，满足了HL-LHC CMS一级触发系统的要求。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)特别是交互网络(INs)在CERN高亮度大型强子对撞机(HL-LHC)的射流标记任务中表现出色，但其计算复杂性和不规则内存访问模式在FPGA硬件触发系统上部署时面临重大挑战，因为系统有严格的延迟和资源限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种具有线性计算复杂性的新型GNN架构，能够在FPGA硬件触发系统上高效部署，满足严格的延迟和资源约束，同时保持高模型精度。&lt;h4&gt;方法&lt;/h4&gt;提出JEDI-linear架构，通过利用共享变换和全局聚合消除显式的成对交互；引入细粒度量化感知训练，具有每参数位宽优化；通过分布式算术采用无乘法器的乘法累加操作。&lt;h4&gt;主要发现&lt;/h4&gt;基于FPGA的JEDI-linear与最先进的设计相比，实现了3.7至11.5倍的更低延迟，高达150倍的更低启动间隔，以及高达6.2倍的更低LUT使用率，同时提供更高的模型精度，完全消除了对DSP块的需求（最先进解决方案消耗超过8,700个DSP）。&lt;h4&gt;结论&lt;/h4&gt;JEDI-linear是第一个基于交互的GNN实现低于60纳秒延迟，满足HL-LHC CMS一级触发系统的使用要求，通过在实时环境中实现准确、可扩展和资源高效的GNN推理，推动了下一代触发系统的发展。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)，特别是交互网络(INs)，在CERN高亮度大型强子对撞机(HL-LHC)的射流标记任务中表现出色。然而，它们的计算复杂性和不规则内存访问模式在FPGA硬件触发系统上的部署提出了重大挑战，该系统有严格的延迟和资源限制。在这项工作中，我们提出了JEDI-linear，一种具有线性计算复杂性的新型GNN架构，通过利用共享变换和全局聚合消除了显式的成对交互。为了进一步提高硬件效率，我们引入了细粒度量化感知训练，具有每参数位宽优化，并通过分布式算术采用无乘法器的乘法累加操作。评估结果表明，我们的基于FPGA的JEDI-linear与最先进的设计相比，实现了3.7至11.5倍的更低延迟，高达150倍的更低启动间隔，以及高达6.2倍的更低LUT使用率，同时提供更高的模型精度，并完全消除了对DSP块的需求的需求。相比之下，最先进的解决方案消耗超过8,700个DSP。这是第一个基于交互的GNN实现低于60纳秒延迟，目前满足HL-LHC CMS一级触发系统的使用要求。这项工作通过在实时环境中实现准确、可扩展和资源高效的GNN推理，推动了下一代触发系统的发展。我们的开源模板将进一步支持可重现性和在科学应用中的更广泛采用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs), particularly Interaction Networks (INs), haveshown exceptional performance for jet tagging at the CERN High-Luminosity LargeHadron Collider (HL-LHC). However, their computational complexity and irregularmemory access patterns pose significant challenges for deployment on FPGAs inhardware trigger systems, where strict latency and resource constraints apply.In this work, we propose JEDI-linear, a novel GNN architecture with linearcomputational complexity that eliminates explicit pairwise interactions byleveraging shared transformations and global aggregation. To further enhancehardware efficiency, we introduce fine-grained quantization-aware training withper-parameter bitwidth optimization and employ multiplier-freemultiply-accumulate operations via distributed arithmetic. Evaluation resultsshow that our FPGA-based JEDI-linear achieves 3.7 to 11.5 times lower latency,up to 150 times lower initiation interval, and up to 6.2 times lower LUT usagecompared to state-of-the-art designs while also delivering higher modelaccuracy and eliminating the need for DSP blocks entirely. In contrast,state-of-the-art solutions consume over 8,700 DSPs. This is the firstinteraction-based GNN to achieve less than 60~ns latency and currently meetsthe requirements for use in the HL-LHC CMS Level-1 trigger system. This workadvances the next-generation trigger systems by enabling accurate, scalable,and resource-efficient GNN inference in real-time environments. Ouropen-sourced templates will further support reproducibility and broaderadoption across scientific applications.</description>
      <author>example@mail.com (Zhiqiang Que, Chang Sun, Sudarshan Paramesvaran, Emyr Clement, Katerina Karakoulaki, Christopher Brown, Lauri Laatu, Arianna Cox, Alexander Tapper, Wayne Luk, Maria Spiropulu)</author>
      <guid isPermaLink="false">2508.15468v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation</title>
      <link>http://arxiv.org/abs/2508.15216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为STAGNet的模型，用于从车载摄像头视频中预测交通事故，通过结合时空特征和循环网络聚合，在三个公开数据集上实现了比先前方法更高的预测精度和碰撞时间预警效果。&lt;h4&gt;背景&lt;/h4&gt;事故预测和及时警告对提高道路安全、减少人员伤害和财产损失至关重要。现有高级驾驶辅助系统多依赖LiDAR、雷达和GPS等多种传感器，而仅使用车载摄像头视频输入更具挑战性但成本更低且更易于部署。&lt;h4&gt;目的&lt;/h4&gt;改进基于车载摄像头视频输入的事故预测方法，提高预测的准确性和及时性。&lt;h4&gt;方法&lt;/h4&gt;作者融入了更优的时空特征，并通过循环网络对这些特征进行聚合，以改进现有的图神经网络模型，提出了STAGNet模型用于事故预测。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开数据集上的实验表明，STAGNet模型在交叉验证以及跨数据集训练测试中，均实现了比先前方法更高的平均精度和平均碰撞时间值。&lt;h4&gt;结论&lt;/h4&gt;STAGNet模型在事故预测任务上表现优于现有方法，证明了仅依靠车载摄像头视频进行事故预测的可行性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;事故预测和及时警告在通过减少道路使用者受伤风险和最小化财产损失来提高道路安全方面起着关键作用。高级驾驶辅助系统(ADAS)旨在支持人类驾驶员，特别是在他们能够预见潜在事故发生时。虽然许多现有系统依赖于LiDAR、雷达和GPS等多种传感器，但仅依靠车载摄像头视频输入则是一个更具挑战性但成本更低且更易于部署的解决方案。在这项工作中，我们融入了更好的时空特征，并通过循环网络对这些特征进行聚合，以改进现有的图神经网络用于从车载摄像头视频中预测事故。使用三个公开数据集进行的实验表明，我们提出的STAGNet模型在给定数据集上的交叉验证以及在不同数据集上进行训练和测试时，都比先前方法实现了更高的平均精度和平均碰撞时间值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accident prediction and timely warnings play a key role in improving roadsafety by reducing the risk of injury to road users and minimizing propertydamage. Advanced Driver Assistance Systems (ADAS) are designed to support humandrivers and are especially useful when they can anticipate potential accidentsbefore they happen. While many existing systems depend on a range of sensorssuch as LiDAR, radar, and GPS, relying solely on dash-cam video input presentsa more challenging but a more cost-effective and easily deployable solution. Inthis work, we incorporate better spatio-temporal features and aggregate themthrough a recurrent network to improve upon state-of-the-art graph neuralnetworks for predicting accidents from dash-cam videos. Experiments using threepublicly available datasets show that our proposed STAGNet model achieveshigher average precision and mean time-to-collision values than previousmethods, both when cross-validated on a given dataset and when trained andtested on different datasets.</description>
      <author>example@mail.com (Vipooshan Vipulananthan, Kumudu Mohottala, Kavindu Chinthana, Nimsara Paramulla, Charith D Chitraranjan)</author>
      <guid isPermaLink="false">2508.15216v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Equivariant Electronic Hamiltonian Prediction with Many-Body Message Passing</title>
      <link>http://arxiv.org/abs/2508.15108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MACE-H图神经网络模型作为Kohn-Sham密度泛函理论哈密顿量的机器学习替代方案，能够高效预测材料的电子属性，如电子能带结构和态密度，在保持高精度的同时实现了计算效率。&lt;h4&gt;背景&lt;/h4&gt;机器学习替代Kohn-Sham密度泛函理论哈密顿量是预测材料电子属性的有力工具，但对于大规模应用，需要兼具高泛化能力和计算效率的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效获取所有相关O(3)不可约表示，同时捕捉完整局部化学环境特征的模型，用于材料电子属性的准确预测。&lt;h4&gt;方法&lt;/h4&gt;引入MACE-H图神经网络，结合高体阶消息传递和节点阶展开，能够处理到f轨道矩阵相互作用块，捕捉完整的局部化学环境特征。&lt;h4&gt;主要发现&lt;/h4&gt;模型在二维材料和块体金的公开基准数据集上展示了高精度和可转移性，实现了亚毫电子伏特量级的矩阵元素和特征值预测误差；高体阶消息传递与局部性的相互作用使该模型成为高通量材料筛选的有力候选。&lt;h4&gt;结论&lt;/h4&gt;MACE-H图神经网络模型在保持高精度的同时实现了计算效率，能够捕捉完整的局部化学环境特征，适合用于高通量材料筛选应用。&lt;h4&gt;翻译&lt;/h4&gt;机器学习替代Kohn-Sham密度泛函理论哈密顿量为加速材料电子属性（如电子能带结构和态密度）的预测提供了强大工具。对于大规模应用，理想模型应具有高泛化能力和计算效率。本文介绍了MACE-H图神经网络，它结合高体阶消息传递和节点阶展开，高效获取所有相关的O(3)不可约表示。该模型实现了高精度和计算效率，并捕捉了完整的局部化学环境特征，目前可处理到f轨道矩阵相互作用块。我们在几个二维材料的公开材料基准数据集和一个新的块体金数据集上验证了模型的准确性和可转移性，在所有系统中实现了亚毫电子伏特量级的矩阵元素和特征值预测误差。我们进一步分析了高体阶消息传递与局部性的相互作用，这使该模型成为高通量材料筛选的有力候选。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning surrogates of Kohn-Sham Density Functional TheoryHamiltonians offer a powerful tool to accelerate the prediction of electronicproperties of materials, such as electronic band structures anddensities-of-states. For large-scale applications, an ideal model would exhibithigh generalization ability and computational efficiency. Here, we introducethe MACE-H graph neural network, which combines high body-order message passingwith a node-order expansion to efficiently obtain all relevant $O(3)$irreducible representations. The model achieves high accuracy and computationalefficiency and captures the full local chemical environment features of,currently, up to $f$ orbital matrix interaction blocks. We demonstrate themodel's accuracy and transferability on several open materials benchmarkdatasets of two-dimensional materials and a new dataset for bulk gold,achieving sub-meV prediction errors on matrix elements and eigenvalues acrossall systems. We further analyse the interplay of high body order messagepassing and locality that makes this model a good candidate for high-throughputmaterial screening.</description>
      <author>example@mail.com (Chen Qian, Valdas Vitartas, James Kermode, Reinhard J. Maurer)</author>
      <guid isPermaLink="false">2508.15108v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Fragment-Wise Interpretability in Graph Neural Networks via Molecule Decomposition and Contribution Analysis</title>
      <link>http://arxiv.org/abs/2508.15015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SEAL是一种新的可解释图神经网络，通过将分子图分解为化学相关片段并减少片段间消息传递，实现了对分子性质预测的可靠解释，在定量指标和人类对齐的可解释性方面优于其他方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在预测分子性质方面表现出色，但其黑盒性质降低了可解释性，限制了在药物发现和材料设计等重要应用中对预测结果的信任。&lt;h4&gt;目的&lt;/h4&gt;开发一种可解释的图神经网络方法，能够可靠地量化分子中单个原子或子结构对模型预测的贡献。&lt;h4&gt;方法&lt;/h4&gt;SEAL（通过归因学习的子结构解释）将输入图分解为化学相关的片段，估计它们对输出的因果影响，并通过减少片段间消息传递实现片段贡献与模型预测的强对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在合成基准和真实世界分子数据集上的评估表明，SEAL在定量归因指标和人类对齐的可解释性方面都优于其他可解释性方法；用户研究确认SEAL为领域专家提供了更直观和可信的解释。&lt;h4&gt;结论&lt;/h4&gt;SEAL通过弥合预测性能和可解释性之间的差距，为更透明和可操作的分子建模提供了有希望的方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络通过利用分子图中编码的丰富结构信息，在预测分子性质方面取得了显著成功。然而，它们的黑盒性质降低了可解释性，这限制了在药物发现和材料设计等重要应用中对预测结果的信任。此外，由于纠缠的消息传递动态，现有的解释技术往往无法可靠地量化单个原子或子结构的贡献。我们引入了SEAL（通过归因学习的子结构解释），这是一种新的可解释图神经网络，它将模型预测归因于有意义的分子子图。SEAL将输入图分解为化学相关的片段，并估计它们对输出的因果影响。通过在我们提出的模型架构中明确减少片段间的消息传递，实现了片段贡献与模型预测之间的强对齐。在合成基准和真实世界分子数据集上的广泛评估表明，SEAL在定量归因指标和人类对齐的可解释性方面都优于其他可解释性方法。用户研究进一步确认SEAL为领域专家提供了更直观和可信的解释。通过弥合预测性能和可解释性之间的差距，SEAL为更透明和可操作的分子建模提供了有希望的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have demonstrated remarkable success in predictingmolecular properties by leveraging the rich structural information encoded inmolecular graphs. However, their black-box nature reduces interpretability,which limits trust in their predictions for important applications such as drugdiscovery and materials design. Furthermore, existing explanation techniquesoften fail to reliably quantify the contribution of individual atoms orsubstructures due to the entangled message-passing dynamics. We introduce SEAL(Substructure Explanation via Attribution Learning), a new interpretable graphneural network that attributes model predictions to meaningful molecularsubgraphs. SEAL decomposes input graphs into chemically relevant fragments andestimates their causal influence on the output. The strong alignment betweenfragment contributions and model predictions is achieved by explicitly reducinginter-fragment message passing in our proposed model architecture. Extensiveevaluations on synthetic benchmarks and real-world molecular datasetsdemonstrate that SEAL outperforms other explainability methods in bothquantitative attribution metrics and human-aligned interpretability. A userstudy further confirms that SEAL provides more intuitive and trustworthyexplanations to domain experts. By bridging the gap between predictiveperformance and interpretability, SEAL offers a promising direction for moretransparent and actionable molecular modeling.</description>
      <author>example@mail.com (Sebastian Musiał, Bartosz Zieliński, Tomasz Danel)</author>
      <guid isPermaLink="false">2508.15015v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Fast Graph Neural Network for Image Classification</title>
      <link>http://arxiv.org/abs/2508.14958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, proceeding into CanadianAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合图卷积网络(GCNs)和Voronoi图的新型图像分类方法，通过将图像表示为图结构并使用Delaunay三角剖分优化表示，显著提高了图像分类的预处理效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;图像分类的快速发展主要得益于图卷积网络(GCNs)的应用，它们为处理复杂数据结构提供了强大的框架。然而，传统的卷积神经网络(CNNs)在处理某些复杂场景和细粒度分类时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过整合GCNs与Voronoi图，开发一种更有效的图像分类方法，以提高预处理效率和分类准确性，特别是在复杂场景和细粒度类别的挑战性场景中。&lt;h4&gt;方法&lt;/h4&gt;该方法将图像表示为图结构，其中像素或区域作为顶点，然后使用相应的Delaunay三角剖分来优化图的表示。结合GCNs处理这种图结构数据，实现对图像的有效分类。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在各种基准数据集上实现了预处理效率和分类准确性的显著改进，超越了最先进的方法，特别是在涉及复杂场景和细粒度类别的挑战性场景中表现优异。&lt;h4&gt;结论&lt;/h4&gt;该研究不仅为图像分类提供了新的视角，还扩展了基于图的学习范式在计算机视觉和非结构化数据分析中的潜在应用。&lt;h4&gt;翻译&lt;/h4&gt;图像分类的快速发展在很大程度上得益于图卷积网络(GCNs)的采用，它们为处理复杂数据结构提供了强大的框架。本研究引入了一种新方法，将GCNs与Voronoi图相结合，通过利用它们有效建模关系数据的能力来增强图像分类。与传统的卷积神经网络(CNNs)不同，我们的方法将图像表示为图，其中像素或区域作为顶点。然后使用相应的Delaunay三角剖分对这些图进行优化，改进其表示。所提出的模型在各种基准数据集上实现了预处理效率和分类准确性的显著提高，超越了最先进的方法，特别是在涉及复杂场景和细粒度类别的挑战性场景中。通过交叉验证验证的实验结果，强调了结合GCNs与Voronoi图对推进图像分类的有效性。这项研究不仅为图像分类提供了新的视角，还扩展了基于图的学习范式在计算机视觉和非结构化数据分析中的潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid progress in image classification has been largely driven by theadoption of Graph Convolutional Networks (GCNs), which offer a robust frameworkfor handling complex data structures. This study introduces a novel approachthat integrates GCNs with Voronoi diagrams to enhance image classification byleveraging their ability to effectively model relational data. Unlikeconventional convolutional neural networks (CNNs), our method represents imagesas graphs, where pixels or regions function as vertices. These graphs are thenrefined using corresponding Delaunay triangulations, optimizing theirrepresentation. The proposed model achieves significant improvements in bothpreprocessing efficiency and classification accuracy across various benchmarkdatasets, surpassing state-of-the-art approaches, particularly in challengingscenarios involving intricate scenes and fine-grained categories. Experimentalresults, validated through cross-validation, underscore the effectiveness ofcombining GCNs with Voronoi diagrams for advancing image classification. Thisresearch not only presents a novel perspective on image classification but alsoexpands the potential applications of graph-based learning paradigms incomputer vision and unstructured data analysis.</description>
      <author>example@mail.com (Mustafa Mohammadi Gharasuie, Luis Rueda)</author>
      <guid isPermaLink="false">2508.14958v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Structure-Aware Temporal Modeling for Chronic Disease Progression Prediction</title>
      <link>http://arxiv.org/abs/2508.14942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一预测框架，整合结构感知和时间建模，用于帕金森病进展预测。该方法利用图神经网络建模临床症状间的结构关系，引入基于图的表示捕捉症状语义依赖，采用Transformer架构建模动态时间特征，并通过结构感知门控机制融合结构信息与时间特征。实验表明该方法在AUC、RMSE和IPW-F1指标上优于现有方法，能有效区分疾病进展阶段并捕捉个性化症状轨迹。&lt;h4&gt;背景&lt;/h4&gt;帕金森病进展预测面临症状演变复杂性和时间依赖性建模不足的挑战，需要更有效的预测方法来支持临床决策。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的预测框架，整合结构感知和时间建模，提高帕金森病进展预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;利用图神经网络建模多模态临床症状间的结构关系，引入基于图的表示捕捉症状语义依赖，采用Transformer架构建模动态时间特征，设计结构感知门控机制融合结构编码和时间特征，构建包含图构建模块、时间编码模块和预测输出层的多组件建模管道。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在AUC、RMSE和IPW-F1指标上优于现有方法，能有效区分疾病进展阶段，提高模型捕捉个性化症状轨迹的能力，框架具有强大的泛化能力和结构可扩展性。&lt;h4&gt;结论&lt;/h4&gt;整体框架为帕金森病等慢性进展性疾病的智能建模提供了可靠支持，具有临床应用价值。&lt;h4&gt;翻译&lt;/h4&gt;本研究解决了帕金森病进展预测中症状演变复杂性和时间依赖性建模不足的挑战。它提出了一种统一预测框架，整合结构感知和时间建模。该方法利用图神经网络建模多模态临床症状之间的结构关系，引入基于图的表示来捕捉症状之间的语义依赖。它还采用Transformer架构来建模疾病进展过程中的动态时间特征。为了融合结构信息和时间信息，设计了一种结构感知的门控机制，用于动态调整结构编码和时间特征之间的融合权重，增强模型识别关键进展阶段的能力。为了提高分类准确性和稳定性，该框架包含一个多组件建模管道，由图构建模块、时间编码模块和预测输出层组成。模型在真实纵向帕金森病数据上进行了评估。实验涉及与主流模型的比较、超参数的敏感性分析和图连接密度控制。结果表明，所提出的方法在AUC、RMSE和IPW-F1指标上优于现有方法。它能有效区分进展阶段，提高模型捕捉个性化症状轨迹的能力。整体框架展示了强大的泛化能力和结构可扩展性，为帕金森病等慢性进展性疾病的智能建模提供了可靠支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study addresses the challenges of symptom evolution complexity andinsufficient temporal dependency modeling in Parkinson's disease progressionprediction. It proposes a unified prediction framework that integratesstructural perception and temporal modeling. The method leverages graph neuralnetworks to model the structural relationships among multimodal clinicalsymptoms and introduces graph-based representations to capture semanticdependencies between symptoms. It also incorporates a Transformer architectureto model dynamic temporal features during disease progression. To fusestructural and temporal information, a structure-aware gating mechanism isdesigned to dynamically adjust the fusion weights between structural encodingsand temporal features, enhancing the model's ability to identify keyprogression stages. To improve classification accuracy and stability, theframework includes a multi-component modeling pipeline, consisting of a graphconstruction module, a temporal encoding module, and a prediction output layer.The model is evaluated on real-world longitudinal Parkinson's disease data. Theexperiments involve comparisons with mainstream models, sensitivity analysis ofhyperparameters, and graph connection density control. Results show that theproposed method outperforms existing approaches in AUC, RMSE, and IPW-F1metrics. It effectively distinguishes progression stages and improves themodel's ability to capture personalized symptom trajectories. The overallframework demonstrates strong generalization and structural scalability,providing reliable support for intelligent modeling of chronic progressivediseases such as Parkinson's disease.</description>
      <author>example@mail.com (Jiacheng Hu, Bo Zhang, Ting Xu, Haifeng Yang, Min Gao)</author>
      <guid isPermaLink="false">2508.14942v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</title>
      <link>http://arxiv.org/abs/2508.15717v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出StreamMem，一种用于流式视频理解的查询无关KV缓存内存机制，有效解决了多模态大语言模型处理长视频时的内存和计算开销问题。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视觉-语言推理方面取得进展，但处理长视频能力有限。长上下文MLLMs存储和处理长视觉上下文KV缓存带来大量内存和计算开销。现有视觉压缩方法要么需提前编码整个视觉上下文，要么需提前访问问题，不适用于长视频理解和多轮对话场景。&lt;h4&gt;目的&lt;/h4&gt;提出一种查询无关的KV缓存内存机制，用于流式视频理解，无需提前知道问题即可有效压缩长视频的KV缓存。&lt;h4&gt;方法&lt;/h4&gt;StreamMem以流式方式编码新视频帧，利用视觉标记和通用查询标记之间的注意力分数压缩KV缓存，同时保持固定大小的KV内存，使模型能在内存受限的长视频场景中进行高效问答。&lt;h4&gt;主要发现&lt;/h4&gt;在三个长视频理解和两个流式视频问答基准测试上，StreamMem在查询无关KV缓存压缩方面实现了最先进性能，且与查询感知压缩方法具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;StreamMem有效解决了长视频理解中KV缓存压缩问题，适用于流式视频理解和多轮对话场景，无需提前访问问题。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在视觉-语言推理方面取得了显著进展，但它们有效处理长视频的能力仍然有限。尽管长上下文MLLMs最近有所进展，但存储和处理长视觉上下文的关键值缓存会带来大量的内存和计算开销。现有的视觉压缩方法要么需要在压缩前编码整个视觉上下文，要么需要提前访问问题，这在长视频理解和多轮对话场景中是不切实际的。在这项工作中，我们提出了StreamMem，一种用于流式视频理解的查询无关KV缓存内存机制。具体来说，StreamMem以流式方式编码新的视频帧，使用视觉标记和通用查询标记之间的注意力分数来压缩KV缓存，同时保持固定大小的KV内存，以便在内存受限的长视频场景中实现高效的问题回答。在三个长视频理解和两个流式视频问答基准测试上的评估表明，StreamMem在查询无关KV缓存压缩方面实现了最先进的性能，并且与查询感知压缩方法具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have made significant progress invisual-language reasoning, but their ability to efficiently handle long videosremains limited. Despite recent advances in long-context MLLMs, storing andattending to the key-value (KV) cache for long visual contexts incurssubstantial memory and computational overhead. Existing visual compressionmethods require either encoding the entire visual context before compression orhaving access to the questions in advance, which is impractical for long videounderstanding and multi-turn conversational settings. In this work, we proposeStreamMem, a query-agnostic KV cache memory mechanism for streaming videounderstanding. Specifically, StreamMem encodes new video frames in a streamingmanner, compressing the KV cache using attention scores between visual tokensand generic query tokens, while maintaining a fixed-size KV memory to enableefficient question answering (QA) in memory-constrained, long-video scenarios.Evaluation on three long video understanding and two streaming video questionanswering benchmarks shows that StreamMem achieves state-of-the-art performancein query-agnostic KV cache compression and is competitive with query-awarecompression approaches.</description>
      <author>example@mail.com (Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren)</author>
      <guid isPermaLink="false">2508.15717v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.15641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Grounded VideoDiT，一种视频大语言模型，通过引入扩散时间潜在编码器、基于对象的表示和混合令牌方案三个创新，解决了现有视频模型在时间感知、视觉对齐和时间戳建模方面的局限，在多个视频理解基准测试上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;理解视频不仅需要回答开放性问题，还需要精确定位事件发生时间和实体随时间的交互。现有视频大语言模型在整体推理方面有进展，但在时间感知方面仍较粗糙：时间戳隐式编码，帧级特征捕捉连续性弱，语言视觉对齐常偏离感兴趣实体。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够克服现有视频大语言模型在时间感知、视觉对齐和时间戳建模方面局限的视频大语言模型。&lt;h4&gt;方法&lt;/h4&gt;提出Grounded VideoDiT，包含三个关键创新：1) 扩散时间潜在编码器增强边界敏感性和时间一致性；2) 基于对象的表示将查询实体绑定到局部视觉证据，增强对齐；3) 混合令牌方案提供显式时间戳建模，实现细粒度时间推理。&lt;h4&gt;主要发现&lt;/h4&gt;Grounded VideoDiT通过三个创新设计具备了强大的基础能力，在Charades STA、NExT GQA和多个VideoQA基准测试上取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;Grounded VideoDiT通过创新的架构设计有效解决了现有视频大语言模型在时间感知、视觉对齐和时间戳建模方面的局限，为视频理解提供了更精确和细粒度的方法。&lt;h4&gt;翻译&lt;/h4&gt;理解视频不仅需要回答开放性问题，它还需要能够精确定位事件发生的时间和实体随时间的交互方式。尽管最近的视频大语言模型在整体推理方面取得了显著进展，但它们在时间感知方面仍然比较粗糙：时间戳仅被隐式编码，帧级特征在捕捉连续性方面较弱，语言视觉对齐常常偏离感兴趣的实体。在本文中，我们提出了Grounded VideoDiT，一种视频大语言模型，通过引入三个关键创新来克服这些局限。首先，扩散时间潜在编码器增强了边界敏感性并保持时间一致性。其次，基于对象的表示将查询实体明确绑定到局部视觉证据，增强了对齐。第三，具有离散时间令牌的混合令牌方案提供显式的时间戳建模，实现细粒度的时间推理。这些设计共同使Grounded VideoDiT具备了强大的基础能力，正如在Charades STA、NExT GQA和多个VideoQA基准测试上取得的最先进结果所验证的那样。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding videos requires more than answering open ended questions, itdemands the ability to pinpoint when events occur and how entities interactacross time. While recent Video LLMs have achieved remarkable progress inholistic reasoning, they remain coarse in temporal perception: timestamps areencoded only implicitly, frame level features are weak in capturing continuity,and language vision alignment often drifts from the entities of interest. Inthis paper, we present Grounded VideoDiT, a Video LLM designed to overcomethese limitations by introducing three key innovations. First, a DiffusionTemporal Latent (DTL) encoder enhances boundary sensitivity and maintainstemporal consistency. Second, object grounded representations explicitly bindquery entities to localized visual evidence, strengthening alignment. Third, amixed token scheme with discrete temporal tokens provides explicit timestampmodeling, enabling fine grained temporal reasoning. Together, these designsequip Grounded VideoDiT with robust grounding capabilities, as validated bystate of the art results on Charades STA, NExT GQA, and multiple VideoQAbenchmarks.</description>
      <author>example@mail.com (Pengcheng Fang, Yuxia Chen, Rui Guo)</author>
      <guid isPermaLink="false">2508.15641v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>An Empirical Study on How Video-LLMs Answer Video Questions</title>
      <link>http://arxiv.org/abs/2508.15360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过系统的实证研究揭示了视频大语言模型(Video-LLMs)的内部工作机制，发现视频信息提取主要发生在早期层，形成两阶段处理过程；某些中间层对视频问答有不成比例的影响；时空建模更依赖语言引导检索而非自注意力机制。这些发现可用于减少计算成本，提高模型效率。&lt;h4&gt;背景&lt;/h4&gt;利用大规模数据和预训练语言模型的视频大语言模型(Video-LLMs)在视频问答方面表现出强大能力，但大多数现有工作专注于提高性能，对理解其内部机制关注有限。&lt;h4&gt;目的&lt;/h4&gt;通过系统的实证研究填补Video-LLMs内部机制理解的空白，解释现有Video-LLMs的工作原理。&lt;h4&gt;方法&lt;/h4&gt;采用注意力敲除作为主要分析工具，设计了三种变体：视频时间敲除、视频空间敲除和语言到视频敲除，并在不同层数上应用这些敲除，提供全局设置和细粒度设置两种研究环境。&lt;h4&gt;主要发现&lt;/h4&gt;1) 全局设置表明视频信息提取主要发生在早期层，形成两阶段过程——较低层专注于感知编码，较高层处理抽象推理；2) 细粒度设置中，某些中间层对视频问答有不成比例的影响，作为关键异常值；3) 时空建模更依赖语言引导检索而非视频标记内部的帧间和帧内自注意力。&lt;h4&gt;结论&lt;/h4&gt;这些见解可以用于减少Video-LLMs中的注意力计算，提高模型效率。这是第一个系统揭示Video-LLMs内部如何处理和理解视频内容的工作，为未来研究提供了可解释性和效率的视角。&lt;h4&gt;翻译&lt;/h4&gt;利用大规模数据和预训练语言模型，视频大语言模型(Video-LLMs)在视频问答方面表现出强大的能力。然而，大多数现有工作专注于提高性能，而对理解其内部机制关注有限。本文旨在通过系统的实证研究填补这一空白。为了解释现有的Video-LLMs，我们采用注意力敲除作为主要分析工具，并设计了三种变体：视频时间敲除、视频空间敲除和语言到视频敲除。然后，我们在不同数量的层(层窗口)上应用这三种敲除。通过仔细控制层窗口和敲除类型，我们提供了两种设置：全局设置和细粒度设置。我们的研究揭示了三个关键发现：(1) 全局设置表明视频信息提取主要发生在早期层，形成一个清晰的两阶段过程——较低层专注于感知编码，而较高层处理抽象推理；(2) 在细粒度设置中，某些中间层对视频问答有不成比例的影响，作为关键异常值，而大多数其他层贡献最小；(3) 在两种设置中，我们观察到时空建模更多地依赖于语言引导的检索，而不是视频标记内部的帧间和帧内自注意力，尽管后者的计算成本很高。最后，我们证明这些见解可以用于减少Video-LLMs中的注意力计算。据我们所知，这是第一个系统揭示Video-LLMs内部如何处理和理解视频内容的工作，为未来的研究提供了可解释性和效率的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Taking advantage of large-scale data and pretrained language models, VideoLarge Language Models (Video-LLMs) have shown strong capabilities in answeringvideo questions. However, most existing efforts focus on improving performance,with limited attention to understanding their internal mechanisms. This paperaims to bridge this gap through a systematic empirical study. To interpretexisting VideoLLMs, we adopt attention knockouts as our primary analytical tooland design three variants: Video Temporal Knockout, Video Spatial Knockout, andLanguage-to-Video Knockout. Then, we apply these three knockouts on differentnumbers of layers (window of layers). By carefully controlling the window oflayers and types of knockouts, we provide two settings: a global setting and afine-grained setting. Our study reveals three key findings: (1) Global settingindicates Video information extraction primarily occurs in early layers,forming a clear two-stage process -- lower layers focus on perceptual encoding,while higher layers handle abstract reasoning; (2) In the fine-grained setting,certain intermediate layers exert an outsized impact on video questionanswering, acting as critical outliers, whereas most other layers contributeminimally; (3) In both settings, we observe that spatial-temporal modelingrelies more on language-guided retrieval than on intra- and inter-frameself-attention among video tokens, despite the latter's high computationalcost. Finally, we demonstrate that these insights can be leveraged to reduceattention computation in Video-LLMs. To our knowledge, this is the first workto systematically uncover how Video-LLMs internally process and understandvideo content, offering interpretability and efficiency perspectives for futureresearch.</description>
      <author>example@mail.com (Chenhui Gou, Ziyu Ma, Zicheng Duan, Haoyu He, Feng Chen, Akide Liu, Bohan Zhuang, Jianfei Cai, Hamid Rezatofighi)</author>
      <guid isPermaLink="false">2508.15360v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification</title>
      <link>http://arxiv.org/abs/2508.15298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为时间提示对齐(TPA)的新方法，用于超声视频中胎儿先天性心脏病(CHD)的分类检测，结合了时间建模、提示感知对比学习和不确定性量化技术。&lt;h4&gt;背景&lt;/h4&gt;超声视频中的先天性心脏病(CHD)检测受到图像噪声和探头位置变化的影响。虽然自动化方法可以减少操作者依赖，但当前机器学习方法往往忽略时间信息，仅限于二元分类，且不考虑预测校准。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用时间信息、支持多类别分类并提供可靠不确定性量化的方法，用于胎儿心脏超声视频中的CHD检测。&lt;h4&gt;方法&lt;/h4&gt;TPA方法使用图像编码器从视频子片段的每一帧提取特征，通过可训练的时间提取器聚合特征以捕获心脏运动，并通过边际铰链对比损失将视频表示与类别特定的文本提示对齐。同时引入条件变分自编码器样式调制(CVAESM)模块学习潜在样式向量来调制嵌入并量化分类不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;TPA在CHD诊断上实现了85.40%的最先进宏F1分数，同时将期望校准误差减少了5.38%，自适应ECE减少了6.8%。在EchoNet-Dynamic的三类任务中，宏F1提升了4.73%(从53.89%提升至58.62%)。&lt;h4&gt;结论&lt;/h4&gt;时间提示对齐(TPA)是一个有效的框架，集成了时间建模、提示感知对比学习和不确定性量化，能够准确可靠地检测胎儿先天性心脏病。&lt;h4&gt;翻译&lt;/h4&gt;超声视频中的先天性心脏病(CHD)检测受到图像噪声和探头位置变化的影响。虽然自动化方法可以减少操作者依赖，但当前的机器学习方法往往忽略时间信息，仅限于二元分类，且不考虑预测校准。我们提出时间提示对齐(TPA)，一种利用基础图像-文本模型和提示感知对比学习的方法，用于对胎儿心脏超声视频进行CHD分类。TPA使用图像编码器从视频子片段的每一帧提取特征，通过可训练的时间提取器聚合它们以捕获心脏运动，并通过边际铰链对比损失将视频表示与类别特定的文本提示对齐。为了增强临床可靠性的校准，我们引入了条件变分自编码器样式调制(CVAESM)模块，该模块学习潜在样式向量来调制嵌入并量化分类不确定性。在私人CHD检测数据集和大型公共数据集EchoNet-Dynamic(用于评估收缩功能障碍)上评估，TPA在CHD诊断上实现了85.40%的最先进宏F1分数，同时将期望校准误差减少了5.38%，自适应ECE减少了6.8%。在EchoNet-Dynamic的三类任务中，宏F1提升了4.73%(从53.89%提升至58.62%)。时间提示对齐(TPA)是一个用于胎儿先天性心脏病(CHD)分类的框架，集成了时间建模、提示感知对比学习和不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Congenital heart defect (CHD) detection in ultrasound videos is hindered byimage noise and probe positioning variability. While automated methods canreduce operator dependence, current machine learning approaches often neglecttemporal information, limit themselves to binary classification, and do notaccount for prediction calibration. We propose Temporal Prompt Alignment (TPA),a method leveraging foundation image-text model and prompt-aware contrastivelearning to classify fetal CHD on cardiac ultrasound videos. TPA extractsfeatures from each frame of video subclips using an image encoder, aggregatesthem with a trainable temporal extractor to capture heart motion, and alignsthe video representation with class-specific text prompts via a margin-hingecontrastive loss. To enhance calibration for clinical reliability, we introducea Conditional Variational Autoencoder Style Modulation (CVAESM) module, whichlearns a latent style vector to modulate embeddings and quantifiesclassification uncertainty. Evaluated on a private dataset for CHD detectionand on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPAachieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, whilealso reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. OnEchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenitalheart defect (CHD) classification in ultrasound videos that integrates temporalmodeling, prompt-aware contrastive learning, and uncertainty quantification.</description>
      <author>example@mail.com (Darya Taratynova, Alya Almsouti, Beknur Kalmakhanbet, Numan Saeed, Mohammad Yaqub)</author>
      <guid isPermaLink="false">2508.15298v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment</title>
      <link>http://arxiv.org/abs/2508.09399v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于联邦学习的风险评估框架，用于解决跨机构金融风险分析中的数据隐私和协作建模挑战。该方法通过特征注意力机制和时间建模结构，在不共享原始数据的情况下实现跨机构联合建模和风险识别。&lt;h4&gt;背景&lt;/h4&gt;跨机构金融风险分析面临数据隐私和协作建模的挑战。传统方法需要共享原始数据，这可能导致敏感信息泄露，同时限制了跨机构协作的效率和范围。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于联邦学习的风险评估框架，实现在不共享原始数据的情况下，跨机构进行联合建模和风险识别。&lt;h4&gt;方法&lt;/h4&gt;采用特征注意力机制和时间建模结构，通过分布式优化策略，每个金融机构训练本地子模型，使用差分隐私和噪声注入保护模型参数后上传至中央服务器，中央服务器聚合这些参数生成全局模型，用于系统性风险识别。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模型在所有评估指标上优于传统集中式方法和现有联邦学习变体，在敏感金融环境中展示出强大的建模能力和实用价值。&lt;h4&gt;结论&lt;/h4&gt;该方法增强了风险识别的范围和效率，同时保留了数据主权，为智能金融风险分析提供了安全高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了跨机构金融风险分析中的数据隐私和协作建模挑战。它提出了一种基于联邦学习的风险评估框架。在不共享原始数据的情况下，该方法使多个机构能够进行联合建模和风险识别。这是通过结合特征注意力机制和时间建模结构实现的。具体来说，该模型采用分布式优化策略。每个金融机构训练一个本地子模型。在上传之前，使用差分隐私和噪声注入保护模型参数。然后，中央服务器聚合这些参数以生成全局模型。这个全局模型用于系统性风险识别。为了验证所提出方法的有效性，进行了多次实验。这些实验评估了通信效率、模型准确性、系统性风险检测和跨市场泛化能力。结果表明，在所有评估指标上，所提出的模型优于传统的集中式方法和现有的联邦学习变体。它在敏感的金融环境中展示了强大的建模能力和实用价值。该方法增强了风险识别的范围和效率，同时保留了数据主权。它为智能金融风险分析提供了安全高效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges of data privacy and collaborativemodeling in cross-institution financial risk analysis. It proposes a riskassessment framework based on federated learning. Without sharing raw data, themethod enables joint modeling and risk identification across multipleinstitutions. This is achieved by incorporating a feature attention mechanismand temporal modeling structure. Specifically, the model adopts a distributedoptimization strategy. Each financial institution trains a local sub-model. Themodel parameters are protected using differential privacy and noise injectionbefore being uploaded. A central server then aggregates these parameters togenerate a global model. This global model is used for systemic riskidentification. To validate the effectiveness of the proposed method, multipleexperiments are conducted. These evaluate communication efficiency, modelaccuracy, systemic risk detection, and cross-market generalization. The resultsshow that the proposed model outperforms both traditional centralized methodsand existing federated learning variants across all evaluation metrics. Itdemonstrates strong modeling capabilities and practical value in sensitivefinancial environments. The method enhances the scope and efficiency of riskidentification while preserving data sovereignty. It offers a secure andefficient solution for intelligent financial risk analysis.</description>
      <author>example@mail.com (Yue Yao, Zhen Xu, Youzhu Liu, Kunyuan Ma, Yuxiu Lin, Mohan Jiang)</author>
      <guid isPermaLink="false">2508.09399v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle</title>
      <link>http://arxiv.org/abs/2508.15680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures, Presented at IAIL 2025 - Imagining the AI  Landscape after the AI Act, 4th International Workshop on Imagining the AI  Landscape After the AI Act, The fourth International Conference on Hybrid  Human-Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出对欧盟AI法案的技术哲学解读，探讨AI系统中数据的长期动态和递归价值链，揭示监管盲点，并提出受西蒙东哲学启发的AI生命周期模型和有效监管措施。&lt;h4&gt;背景&lt;/h4&gt;AI系统中的数据生命周期从摄取到部署产生了挑战现有负责任AI框架的递归价值链，现有政策制定缺乏对AI技术运作和经济逻辑背后的'生成动态'的解释。&lt;h4&gt;目的&lt;/h4&gt;提出一种受西蒙东技术哲学启发的AI正式解读，重新诠释'个体化'概念来建模AI生命周期，引入'未来性'概念解释AI的自我强化生命周期，并提出有效监管措施。&lt;h4&gt;方法&lt;/h4&gt;引入一个概念工具来构建AI管道，涵盖数据、训练模式、架构、特征存储和迁移学习；使用跨学科方法开发技术上可靠且哲学上连贯的分析。&lt;h4&gt;主要发现&lt;/h4&gt;数据的递归生成性和非竞争性特征；科技寡头通过捕获、训练和部署的基础设施集中了价值和决策权；有效监管需要解决基础设施和时间动态问题。&lt;h4&gt;结论&lt;/h4&gt;有效监管必须解决AI系统的基础设施和时间动态问题，提出包括生命周期审计、时间可追溯性、反馈问责制、递归透明度和对递归重用的争议权等措施。&lt;h4&gt;翻译&lt;/h4&gt;本文主张对欧盟AI法案进行技术哲学解读，这为AI系统中数据的长期动态提供了见解，特别是从摄取到部署的生命周期如何产生挑战现有负责任AI框架的递归价值链。我们引入了一个概念工具来构建AI管道，涵盖数据、训练模式、架构、特征存储和迁移学习。使用跨学科方法，我们开发了一种技术上可靠且哲学上连贯的分析，揭示了监管盲点。我们的核心观点是，政策制定中缺乏对AI技术运作和经济逻辑背后的'生成动态'的解释。为此，我们提出了一种受西蒙东技术哲学启发的AI正式解读，重新诠释其'个体化'概念来建模AI生命周期，包括前个体环境、个体化和个体化的AI。为了解释这些概念，我们引入了'未来性'：AI的自我强化生命周期，其中更多数据提高性能、深化个性化并扩展应用领域。未来性突显了数据的递归生成性和非竞争性，以及支持反馈、适应和时间递归的特征存储等基础设施。我们的干预凸显了不断加剧的权力不对称，特别是科技寡头，其捕获、训练和部署的基础设施集中了价值和决策权。我们认为，有效监管必须解决这些基础设施和时间动态问题，并提出包括生命周期审计、时间可追溯性、反馈问责制、递归透明度和对递归重用的争议权等措施。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper argues that a techno-philosophical reading of the EU AI Actprovides insight into the long-term dynamics of data in AI systems,specifically, how the lifecycle from ingestion to deployment generatesrecursive value chains that challenge existing frameworks for Responsible AI.We introduce a conceptual tool to frame the AI pipeline, spanning data,training regimes, architectures, feature stores, and transfer learning. Usingcross-disciplinary methods, we develop a technically grounded andphilosophically coherent analysis of regulatory blind spots. Our central claimis that what remains absent from policymaking is an account of the dynamic ofbecoming that underpins both the technical operation and economic logic of AI.To address this, we advance a formal reading of AI inspired by Simondonianphilosophy of technology, reworking his concept of individuation to model theAI lifecycle, including the pre-individual milieu, individuation, andindividuated AI. To translate these ideas, we introduce futurity: theself-reinforcing lifecycle of AI, where more data enhances performance, deepenspersonalisation, and expands application domains. Futurity highlights therecursively generative, non-rivalrous nature of data, underpinned byinfrastructures like feature stores that enable feedback, adaptation, andtemporal recursion. Our intervention foregrounds escalating power asymmetries,particularly the tech oligarchy whose infrastructures of capture, training, anddeployment concentrate value and decision-making. We argue that effectiveregulation must address these infrastructural and temporal dynamics, andpropose measures including lifecycle audits, temporal traceability, feedbackaccountability, recursion transparency, and a right to contest recursive reuse.</description>
      <author>example@mail.com (Mark Cote, Susana Aires)</author>
      <guid isPermaLink="false">2508.15680v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder (Full Version)</title>
      <link>http://arxiv.org/abs/2508.15633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Full version of the paper accepted for publication at the European  Conference on Artificial Intelligence (ECAI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GRASPED的图自编码器，用于节点异常检测，它结合了图小波卷积编码器和维纳图反卷积解码器，能够在多个尺度捕获全局和局部图信息，有效捕获异常信息。&lt;h4&gt;背景&lt;/h4&gt;图机器学习已在多个领域广泛应用，如社区检测、交易分析和推荐系统，其中异常检测扮演着重要角色。研究表明图上的异常会引起频谱偏移，但现有监督方法受限于标记数据的稀缺性，而无监督学习方法主要依赖空间信息或仅使用低通滤波，导致无法进行多频带分析。&lt;h4&gt;目的&lt;/h4&gt;开发一种无监督学习方法，能够在多个尺度捕获图的全局和局部信息，有效进行节点异常检测，克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出GRASPED模型，包含基于图小波卷积的编码器以及结构和属性解码器。图小波卷积编码器与维纳图反卷积解码器相结合，展现出带通滤波特性，实现基于学习的节点属性重建，有效捕获异常信息。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实世界图异常检测数据集上的实验表明，GRASPED模型能够有效捕获异常信息，并优于当前最先进的模型。&lt;h4&gt;结论&lt;/h4&gt;GRASPED作为一种无监督学习方法，能够在不依赖标记数据的情况下，通过多尺度分析有效检测图中的节点异常，性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;图机器学习已在各个领域得到广泛探索，如社区检测、交易分析和推荐系统。在这些应用中，异常检测扮演着重要角色。最近的研究表明，图上的异常会引起频谱偏移。一些监督方法已经提高了对这种频域信息的利用率。然而，由于异常的本质，它们仍然受限于标记数据的稀缺性。另一方面，现有的无监督学习方法主要依赖空间信息或仅使用低通滤波，从而失去了多频带分析的能力。在本文中，我们提出了用于节点异常检测的图自编码器，具有频域编码器和频域解码器（GRASPED）。我们的无监督学习模型具有基于图小波卷积的编码器，以及结构和属性解码器。基于图小波卷积的编码器与基于维纳图反卷积的解码器相结合，展现出带通滤波特性，能够在多个尺度捕获全局和局部图信息。这种设计允许基于学习的节点属性重建，有效捕获异常信息。在多个真实世界图异常检测数据集上的广泛实验表明，GRASPED优于当前最先进的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph machine learning has been widely explored in various domains, such ascommunity detection, transaction analysis, and recommendation systems. In theseapplications, anomaly detection plays an important role. Recently, studies haveshown that anomalies on graphs induce spectral shifts. Some supervised methodshave improved the utilization of such spectral domain information. However,they remain limited by the scarcity of labeled data due to the nature ofanomalies. On the other hand, existing unsupervised learning approachespredominantly rely on spatial information or only employ low-pass filters,thereby losing the capacity for multi-band analysis. In this paper, we proposeGraph Autoencoder with Spectral Encoder and Spectral Decoder (GRASPED) for nodeanomaly detection. Our unsupervised learning model features an encoder based onGraph Wavelet Convolution, along with structural and attribute decoders. TheGraph Wavelet Convolution-based encoder, combined with a Wiener GraphDeconvolution-based decoder, exhibits bandpass filter characteristics thatcapture global and local graph information at multiple scales. This designallows for a learning-based reconstruction of node attributes, effectivelycapturing anomaly information. Extensive experiments on several real-worldgraph anomaly detection datasets demonstrate that GRASPED outperforms currentstate-of-the-art models.</description>
      <author>example@mail.com (Wei Herng Choong, Jixing Liu, Ching-Yu Kao, Philip Sperl)</author>
      <guid isPermaLink="false">2508.15633v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Clustering-based aggregate value regression</title>
      <link>http://arxiv.org/abs/2508.15567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种专门用于预测聚合值的新型回归方法，称为聚合值回归(AVR)，并通过层次聚类技术(AVR-C)解决了过参数化问题，建立了在模型假设错误情况下的新型偏差-方差权衡理论。&lt;h4&gt;背景&lt;/h4&gt;在实际应用中，预测聚合值(如特定地区的总电力需求)往往比预测个体值更重要，但专门针对聚合值的统计学习方法尚未充分发展，尤其是预测误差与聚类数量之间的关系研究不足。&lt;h4&gt;目的&lt;/h4&gt;引入一种专门针对线性回归模型中聚合值的新型预测方法，解决聚合值预测中的统计学习问题，并研究预测误差与聚类数量之间的关系。&lt;h4&gt;方法&lt;/h4&gt;提出聚合值回归(AVR)方法，将所有回归模型组合成单一模型；为解决过参数化问题，引入层次聚类技术(AVR-C)，构建回归模型聚类并在每个聚类内执行AVR；通过蒙特卡洛模拟研究训练和测试误差行为。&lt;h4&gt;主要发现&lt;/h4&gt;AVR-C在模型假设错误情况下引入了新型偏差-方差权衡理论，其中聚类数量表征了模型复杂度；通过电力需求预测分析验证了该理论。&lt;h4&gt;结论&lt;/h4&gt;AVR-C方法为聚合值预测提供了有效解决方案，通过聚类技术解决了过参数化问题，并通过偏差-方差权衡理论优化了模型复杂度。&lt;h4&gt;翻译&lt;/h4&gt;在各种实际情况下，预测聚合值而非个体值往往是我们关注的重点。例如，电力公司对预测特定地区的总电力需求感兴趣，以确保电网可靠运行和资源分配。然而，据我们所知，专门用于预测聚合值的统计学习方法尚未得到充分发展。特别是，预测误差与聚类数量之间的关系尚未得到充分研究，因为聚类通常被视为无监督学习。本研究介绍了一种专门关注线性回归模型中聚合值的新型预测方法。我们称之为聚合值回归(AVR)，它通过将所有回归模型组合成一个单一模型来构建。使用AVR时，当需要组合的回归模型数量大时，我们必须估计大量参数，导致过参数化。为解决过参数化问题，我们引入了层次聚类技术，称为AVR-C(C代表聚类)。在该方法中，构建了几个回归模型聚类，并在每个聚类内执行AVR。AVR-C在模型假设错误的情况下引入了一种新型偏差-方差权衡理论。在该框架中，聚类数量表征了模型复杂度。进行了蒙特卡洛模拟以研究我们提出的聚类技术的训练和测试误差行为。通过电力需求预测分析也验证了偏差-方差权衡理论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In various practical situations, forecasting of aggregate values rather thanindividual ones is often our main focus. For instance, electricity companiesare interested in forecasting the total electricity demand in a specific regionto ensure reliable grid operation and resource allocation. However, to ourknowledge, statistical learning specifically for forecasting aggregate valueshas not yet been well-established. In particular, the relationship betweenforecast error and the number of clusters has not been well studied, asclustering is usually treated as unsupervised learning. This study introduces anovel forecasting method specifically focused on the aggregate values in thelinear regression model. We call it the Aggregate Value Regression (AVR), andit is constructed by combining all regression models into a single model. Withthe AVR, we must estimate a huge number of parameters when the number ofregression models to be combined is large, resulting in overparameterization.To address the overparameterization issue, we introduce a hierarchicalclustering technique, referred to as AVR-C (C stands for clustering). In thisapproach, several clusters of regression models are constructed, and the AVR isperformed within each cluster. The AVR-C introduces a novel bias-variancetrade-off theory under the assumption of a misspecified model. In thisframework, the number of clusters characterizes model complexity. Monte Carlosimulation is conducted to investigate the behavior of training and test errorsof our proposed clustering technique. The bias-variance trade-off theory isalso demonstrated through the analysis of electricity demand forecasting.</description>
      <author>example@mail.com (Kei Hirose, Hidetoshi Matsui, Hiroki Masuda)</author>
      <guid isPermaLink="false">2508.15567v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>EffortNet: A Deep Learning Framework for Objective Assessment of Speech Enhancement Technologies Using EEG-Based Alpha Oscillations</title>
      <link>http://arxiv.org/abs/2508.15473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EffortNet是一种新型深度学习框架，用于从语音理解过程中的脑电图（EEG）中解码个体听力努力。&lt;h4&gt;背景&lt;/h4&gt;听力理解中的听力努力是听力研究中的一个重大挑战，特别是对老年人和听力障碍人群而言。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从EEG数据中准确解码听力努力的深度学习框架，解决个体间EEG信号变异性大的问题。&lt;h4&gt;方法&lt;/h4&gt;收集122名参与者在四种语音条件（清晰、噪声、MMSE增强和Transformer增强）下的64通道EEG数据；整合三种互补学习范式：自监督学习利用未标记数据，增量学习逐步适应个体特征，迁移学习高效将知识转移到新主体。&lt;h4&gt;主要发现&lt;/h4&gt;1) alpha振荡（8-13 Hz）在噪声语音处理中功率显著更高，可作为听力努力的客观生物标志物；2) EffortNet实现80.9%分类准确率，仅使用新主体40%训练数据；3) Transformer增强语音引发的神经反应比MMSE增强语音更接近清晰语音，与主观可懂度评级形成对比。&lt;h4&gt;结论&lt;/h4&gt;EffortNet为听力技术的个性化评估提供了实用解决方案，对设计认知感知的语音增强系统具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了EffortNet，一种新型深度学习框架，用于从语音理解过程中的脑电图（EEG）解码个体听力努力。听力努力在语音听力研究中代表着一个重大挑战，特别是对老年人和听力障碍人群。我们从122名参与者收集了64通道的EEG数据，他们在四种条件下进行语音理解：清晰、噪声、MMSE增强和Transformer增强语音。统计分析证实，与清晰或增强条件相比，噪声语音处理过程中alpha振荡（8-13 Hz）的功率显著更高，证实了它们作为听力努力客观生物标志物的有效性。为了解决EEG信号中显著的个体间变异性，EffortNet整合了三种互补的学习范式：利用未标记数据的自监督学习，逐步适应个体特征的增量学习，以及高效将知识转移到新主体的迁移学习。我们的实验结果表明，EffortNet仅使用新主体40%的训练数据就实现了80.9%的分类准确率，显著优于传统的CNN（62.3%）和STAnet（61.1%）模型。从模型得出的基于概率的指标显示，Transformer增强语音引发的神经反应比MMSE增强语音更接近清晰语音。这一发现与主观可懂度评级形成对比，但与客观指标一致。所提出的框架为听力技术的个性化评估提供了实用的解决方案，对设计认知感知的语音增强系统具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents EffortNet, a novel deep learning framework for decodingindividual listening effort from electroencephalography (EEG) during speechcomprehension. Listening effort represents a significant challenge inspeech-hearing research, particularly for aging populations and those withhearing impairment. We collected 64-channel EEG data from 122 participantsduring speech comprehension under four conditions: clean, noisy, MMSE-enhanced,and Transformer-enhanced speech. Statistical analyses confirmed that alphaoscillations (8-13 Hz) exhibited significantly higher power during noisy speechprocessing compared to clean or enhanced conditions, confirming their validityas objective biomarkers of listening effort. To address the substantialinter-individual variability in EEG signals, EffortNet integrates threecomplementary learning paradigms: self-supervised learning to leverageunlabeled data, incremental learning for progressive adaptation to individualcharacteristics, and transfer learning for efficient knowledge transfer to newsubjects. Our experimental results demonstrate that Effort- Net achieves 80.9%classification accuracy with only 40% training data from new subjects,significantly outperforming conventional CNN (62.3%) and STAnet (61.1%) models.The probability-based metric derived from our model revealed thatTransformer-enhanced speech elicited neural responses more similar to cleanspeech than MMSEenhanced speech. This finding contrasted with subjectiveintelligibility ratings but aligned with objective metrics. The proposedframework provides a practical solution for personalized assessment of hearingtechnologies, with implications for designing cognitive-aware speechenhancement systems.</description>
      <author>example@mail.com (Ching-Chih Sung, Cheng-Hung Hsin, Yu-Anne Shiah, Bo-Jyun Lin, Yi-Xuan Lai, Chia-Ying Lee, Yu-Te Wang, Borchin Su, Yu Tsao)</author>
      <guid isPermaLink="false">2508.15473v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Least Squares/Gradient Descent Methods for DeepONets</title>
      <link>http://arxiv.org/abs/2508.15394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种混合最小二乘/梯度下降方法来加速DeepONet训练，通过将大型最小二乘系统分解为两个更小的子问题来解决计算复杂性问题。&lt;h4&gt;背景&lt;/h4&gt;DeepONet是一种神经网络架构，但其训练过程面临计算效率挑战，特别是当考虑所有可能的输入组合时。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的方法来加速DeepONet的训练过程，解决传统方法中因大规模线性问题导致的计算瓶颈。&lt;h4&gt;方法&lt;/h4&gt;提出混合最小二乘/梯度下降方法，将DeepONet的输出视为与分支网络最后一层参数线性相关，使用最小二乘优化这些参数，同时用梯度下降更新其他隐藏层参数；将大型最小二乘系统分解为分支网络和主干网络两个子问题分别求解；将方法推广到包含正则化项的L2损失类型，适用于物理信息损失的监督学习场景。&lt;h4&gt;主要发现&lt;/h4&gt;将大型最小二乘系统分解为两个更小、更易管理的子问题可以有效解决计算复杂性问题，同时保持训练效率。&lt;h4&gt;结论&lt;/h4&gt;该方法能够显著加速DeepONet的训练过程，并可以推广到更广泛的损失函数类型，包括正则化物理信息损失。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种高效的混合最小二乘/梯度下降方法来加速DeepONet训练。由于DeepONet的输出可以看作是与分支网络最后一层参数线性的，因此这些参数可以通过最小二乘求解进行优化，而剩余的隐藏层参数则通过梯度下降形式更新。然而，为所有可能的分支和主干输入组合构建最小二乘系统会导致一个过大且无法直接求解的线性问题。为了解决这个问题，我们的方法将大的最小二乘系统分解为两个更小、更易管理的子问题——一个针对分支网络，一个针对主干网络——并分别求解。该方法被推广到更广泛的L2损失类型，包含对最后一层参数的正则化项，包括使用物理信息损失的监督学习情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an efficient hybrid least squares/gradient descent method toaccelerate DeepONet training. Since the output of DeepONet can be viewed aslinear with respect to the last layer parameters of the branch network, theseparameters can be optimized using a least squares (LS) solve, and the remaininghidden layer parameters are updated by means of gradient descent form. However,building the LS system for all possible combinations of branch and trunk inputsyields a prohibitively large linear problem that is infeasible to solvedirectly. To address this issue, our method decomposes the large LS system intotwo smaller, more manageable subproblems $\unicode{x2014}$ one for the branchnetwork and one for the trunk network $\unicode{x2014}$ and solves themseparately. This method is generalized to a broader type of $L^2$ loss with aregularization term for the last layer parameters, including the case ofunsupervised learning with physics-informed loss.</description>
      <author>example@mail.com (Jun Choi, Chang-Ock Lee, Minam Moon)</author>
      <guid isPermaLink="false">2508.15394v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Numerical Analysis of Unsupervised Learning Approaches for Parameter Identification in PDEs</title>
      <link>http://arxiv.org/abs/2508.15381v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  49 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文综述了用于偏微分方程参数识别的无监督学习方法，特别关注扩散系数识别问题，并提出了基于不同方法的误差界限推导框架。&lt;h4&gt;背景&lt;/h4&gt;识别偏微分方程中的参数代表了一类广泛的应用反问题。近年来，已经开发了几种使用(深度)神经网络的无监督学习方法来解决PDE参数识别问题。&lt;h4&gt;目的&lt;/h4&gt;提供对这些无监督学习技术的全面综述，从经典数值分析角度出发，并推导使用不同方法获得的离散近似的严格误差界限。&lt;h4&gt;方法&lt;/h4&gt;使用神经网络作为ansatz函数来近似参数和/或状态，提出一种通用框架用于推导使用Galerkin有限元方法、混合方法和深度神经网络获得的离散近似的严格误差界限。&lt;h4&gt;主要发现&lt;/h4&gt;这些无监督学习方法展示了令人印象深刻的经验性能，条件稳定性估计在误差分析中起着关键作用。&lt;h4&gt;结论&lt;/h4&gt;研究从经典数值分析角度对无监督学习方法进行了全面综述，并提出了推导严格误差界限的通用框架。&lt;h4&gt;翻译&lt;/h4&gt;识别偏微分方程中的参数代表了一类非常广泛的应用反问题。近年来，已经开发了几种使用(深度)神经网络的无监督学习方法来解决PDE参数识别问题。这些方法使用神经网络作为ansatz函数来近似参数和/或状态，并展示了令人印象深刻的经验性能。在本文中，我们从经典数值分析的角度，对一个模型问题(扩散系数识别)上的这些无监督学习技术进行了全面综述，并概述了一个通用框架，用于推导使用Galerkin有限元方法、混合方法和深度神经网络获得的离散近似的严格误差界限。我们始终强调条件稳定性估计在误差分析中的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying parameters in partial differential equations (PDEs) represents avery broad class of applied inverse problems. In recent years, severalunsupervised learning approaches using (deep) neural networks have beendeveloped to solve PDE parameter identifications. These approaches employneural networks as ansatz functions to approximate the parameters and / or thestates, and have demonstrated impressive empirical performance. In this paper,we provide a comprehensive survey on these unsupervised learning techniques onone model problem, diffusion coefficient identification, from the classicalnumerical analysis perspective, and outline a general framework for derivingrigorous error bounds on the discrete approximations obtained using theGalerkin finite element method, hybrid method and deep neural networks.Throughout we highlight the crucial role of conditional stability estimates inthe error analysis.</description>
      <author>example@mail.com (Siyu Cen, Bangti Jin, Qimeng Quan, Zhi Zhou)</author>
      <guid isPermaLink="false">2508.15381v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Transfer learning optimization based on evolutionary selective fine tuning</title>
      <link>http://arxiv.org/abs/2508.15367v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at the Workshop artiFicial And bio-inspIred netwoRked  intelliGence foR cOnstrained aUtoNomous Devices (FAIRGROUND). 2025  International Joint Conference on Neural Networks (IJCNN)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为BioTune的进化自适应微调技术，通过选择性微调模型层来提高迁移学习效率，减少计算成本并提升性能。&lt;h4&gt;背景&lt;/h4&gt;深度学习在图像分析方面取得了显著进展，但大型完全训练模型的计算需求仍然是一个挑战。迁移学习是适应预训练模型到新任务的有效策略，但传统微调方法通常更新所有参数，可能导致过拟合和更高的计算成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够选择性微调模型层的技术，以提高迁移学习效率，减少可训练参数数量，降低计算成本，并使迁移学习能够更好地适应不同的数据特征和分布。&lt;h4&gt;方法&lt;/h4&gt;BioTune采用进化算法来识别出一组特定的层进行微调，而非更新所有参数，从而优化模型在目标任务上的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在九个来自不同领域的图像分类数据集上的评估表明，BioTune与现有的微调方法（如AutoRGN和LoRA）相比，实现了具有竞争力或改进的准确性和效率。&lt;h4&gt;结论&lt;/h4&gt;通过将微调过程集中在相关层的子集上，BioTune减少了可训练参数的数量，降低了计算成本，并促进了在不同数据特征和分布下更高效的迁移学习。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在图像分析方面已经取得了实质性进展。然而，大型完全训练模型的计算需求仍然是一个需要考虑的因素。迁移学习为将预训练模型适应到新任务提供了一种策略。传统的微调通常涉及更新所有模型参数，这可能导致过拟合和更高的计算成本。本文介绍了BioTune，这是一种进化自适应微调技术，它选择性地微调层以提高迁移学习效率。BioTune采用进化算法来识别出一组特定的层进行微调，旨在优化模型在给定目标任务上的性能。在来自不同领域的九个图像分类数据集上的评估表明，BioTune与现有的微调方法（如AutoRGN和LoRA）相比，实现了具有竞争力或改进的准确性和效率。通过将微调过程集中在相关层的子集上，BioTune减少了可训练参数的数量，可能导致计算成本降低，并促进在不同数据特征和分布下更高效的迁移学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has shown substantial progress in image analysis. However, thecomputational demands of large, fully trained models remain a consideration.Transfer learning offers a strategy for adapting pre-trained models to newtasks. Traditional fine-tuning often involves updating all model parameters,which can potentially lead to overfitting and higher computational costs. Thispaper introduces BioTune, an evolutionary adaptive fine-tuning technique thatselectively fine-tunes layers to enhance transfer learning efficiency. BioTuneemploys an evolutionary algorithm to identify a focused set of layers forfine-tuning, aiming to optimize model performance on a given target task.Evaluation across nine image classification datasets from various domainsindicates that BioTune achieves competitive or improved accuracy and efficiencycompared to existing fine-tuning methods such as AutoRGN and LoRA. Byconcentrating the fine-tuning process on a subset of relevant layers, BioTunereduces the number of trainable parameters, potentially leading to decreasedcomputational cost and facilitating more efficient transfer learning acrossdiverse data characteristics and distributions.</description>
      <author>example@mail.com (Jacinto Colan, Ana Davila, Yasuhisa Hasegawa)</author>
      <guid isPermaLink="false">2508.15367v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Anomaly Detection in Evolving Network Environments</title>
      <link>http://arxiv.org/abs/2508.15100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NetSight是一个用于网络数据监督异常检测的框架，能够持续在线检测并适应分布漂移，无需手动干预且能防止灾难性遗忘。&lt;h4&gt;背景&lt;/h4&gt;分布漂移（数据统计特性随时间变化）对深度学习异常检测系统构成重大挑战，现有系统难以适应这些漂移。&lt;h4&gt;目的&lt;/h4&gt;介绍NetSight框架，用于网络数据的监督异常检测，能够持续在线检测并适应分布漂移。&lt;h4&gt;方法&lt;/h4&gt;NetSight通过伪标记技术消除手动干预，使用基于知识蒸馏的适应策略防止灾难性遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在三个长期网络数据集上评估，NetSight相比依赖手动标注的最先进方法具有更好的适应性能，F1-score提高了最多11.72%。&lt;h4&gt;结论&lt;/h4&gt;NetSight在随时间经历分布漂移的动态网络中表现出鲁棒性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;分布漂移是数据统计特性随时间的变化，它对深度学习异常检测系统构成了关键挑战。现有的异常检测系统通常难以适应这些漂移。具体而言，基于监督学习的系统需要昂贵的手动标注，而基于无监督学习的系统则依赖难以获得的干净数据进行漂移适应。这两种要求在实践中都难以满足。在本文中，我们介绍了NetSight，一个用于网络数据监督异常检测的框架，能够持续在线检测并适应分布漂移。NetSight通过一种新的伪标记技术消除手动干预，并使用基于知识蒸馏的适应策略来防止灾难性遗忘。在三个长期网络数据集上的评估表明，与依赖手动标注的最先进方法相比，NetSight表现出更好的适应性能，F1-score提高了最多11.72%。这证明了它在随时间经历分布漂移的动态网络中的鲁棒性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distribution shift, a change in the statistical properties of data over time,poses a critical challenge for deep learning anomaly detection systems.Existing anomaly detection systems often struggle to adapt to these shifts.Specifically, systems based on supervised learning require costly manuallabeling, while those based on unsupervised learning rely on clean data, whichis difficult to obtain, for shift adaptation. Both of these requirements arechallenging to meet in practice. In this paper, we introduce NetSight, aframework for supervised anomaly detection in network data that continuallydetects and adapts to distribution shifts in an online manner. NetSighteliminates manual intervention through a novel pseudo-labeling technique anduses a knowledge distillation-based adaptation strategy to prevent catastrophicforgetting. Evaluated on three long-term network datasets, NetSightdemonstrates superior adaptation performance compared to state-of-the-artmethods that rely on manual labeling, achieving F1-score improvements of up to11.72%. This proves its robustness and effectiveness in dynamic networks thatexperience distribution shifts over time.</description>
      <author>example@mail.com (Ehssan Mousavipour, Andrey Dimanchev, Majid Ghaderi)</author>
      <guid isPermaLink="false">2508.15100v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans</title>
      <link>http://arxiv.org/abs/2508.14151v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了深度学习架构与可解释AI技术结合用于膝关节MRI扫描中感兴趣区域自动检测的有效性，发现ResNet50表现最佳，CNN-based迁移学习是最有效的方法。&lt;h4&gt;背景&lt;/h4&gt;MRI是评估膝盖损伤的重要诊断工具，但手动解释MRI切片既耗时又容易产生观察者间差异。&lt;h4&gt;目的&lt;/h4&gt;评估各种深度学习架构与可解释AI（xAI）技术结合的方法，用于膝关节MRI扫描中的感兴趣区域（ROI）自动检测。&lt;h4&gt;方法&lt;/h4&gt;研究监督和自监督方法，包括ResNet50、InceptionV3、Vision Transformers和多种U-Net变体结合MLP分类器；集成Grad-CAM和Saliency Maps等可解释AI方法；使用AUC评估分类性能，PSNR/SSIM评估重建质量，并进行定性ROI可视化。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet50在分类和ROI识别方面表现最佳，优于基于Transformer的模型；混合U-Net + MLP方法在利用空间特征方面有潜力，但分类性能较低；Grad-CAM提供了最具临床意义的解释。&lt;h4&gt;结论&lt;/h4&gt;基于CNN的迁移学习对于该数据集是最有效的方法；未来更大规模的预训练可能更好地释放Transformer模型的潜力。&lt;h4&gt;翻译&lt;/h4&gt;磁共振成像（MRI）是评估膝盖损伤的重要诊断工具。然而，手动解释MRI切片仍然耗时且容易产生观察者间差异。本研究对各种深度学习架构结合可解释AI（xAI）技术进行了系统性评估，用于膝关节MRI扫描中的感兴趣区域（ROI）自动检测。我们研究了监督和自监督方法，包括ResNet50、InceptionV3、Vision Transformers（ViT）和多种结合多层感知器（MLP）分类器的U-Net变体。为了增强可解释性和临床相关性，我们集成了Grad-CAM和Saliency Maps等可解释AI方法。模型性能使用AUC评估分类质量，使用PSNR/SSIM评估重建质量，并进行定性ROI可视化。我们的结果表明，ResNet50在分类和ROI识别方面持续表现出色，在MRNet数据集的约束下优于基于Transformer的模型。虽然混合U-Net + MLP方法在利用重建和可解释性的空间特征方面显示出潜力，但其分类性能仍然较低。Grad-CAM在各种架构中始终提供了最具临床意义的解释。总体而言，基于CNN的迁移学习成为该数据集最有效的方法，而未来更大规模的预训练可能更好地释放Transformer模型的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决MRI扫描中感兴趣区域(ROI)自动检测的问题，特别是针对膝盖MRI中的半月板损伤检测。这个问题重要是因为目前手动解释MRI切片耗时且容易产生观察者间差异，医生需要逐个检查扫描序列来识别关键区域和异常，这不仅效率低下，还可能导致诊断不一致。自动化ROI检测可以显著提高诊断速度、可靠性，并减少医疗错误，特别是在诊断半月板撕裂、ACL损伤等常见膝盖问题时具有重要临床价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有研究的局限性来设计方法，包括模型可解释性差、过度关注分类准确性而非精确定位、标注数据有限以及域转移敏感等问题。他们借鉴了多项现有工作：使用ResNet、InceptionV3和Vision Transformer等成熟架构；应用U-Net进行图像分割；整合Grad-CAM和Saliency Maps等可解释AI技术；采用迁移学习方法；基于斯坦福MRNet数据集进行研究。作者的创新在于将这些现有元素系统性地整合并进行比较，特别是在有限数据条件下评估不同架构的性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合深度学习模型与可解释AI技术，提高MRI中ROI检测的准确性和临床相关性，特别关注半月板损伤的自动检测。整体流程包括：1)使用MRNet数据集并专注于矢状面视图；2)实现多种模型架构(监督分类的ResNet50/InceptionV3/ViT，自监督重建的U-Net，以及混合的U-Net+MLP)；3)应用数据增强和正则化技术防止过拟合；4)使用AUC/准确率评估分类性能，PSNR/SSIM评估重建质量；5)通过Grad-CAM等技术生成热图提高可解释性；6)综合定量和定性分析比较不同方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统性评估多种深度学习架构与xAI方法的组合用于MRI ROI检测；2)引入自监督U-Net与MLP分类器相结合的混合方法并应用Grad-CAM；3)在有限数据条件下全面比较CNN和Transformer架构；4)同时评估分类准确性和重建质量以及临床相关的ROI可视化。相比之前工作，本文提供了更全面的比较框架，而非单一模型评估；不仅关注分类准确性，还重视ROI检测的精确性和临床相关性；在数据有限条件下证明了CNN迁移学习的优势，这与一些认为Transformer可能更优越的研究结论不同。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统比较多种深度学习架构和可解释AI方法，证明了在膝盖MRI半月板损伤检测中，基于CNN的迁移学习方法结合Grad-CAM技术能够提供最佳的性能和临床相关性，同时为未来在更大规模预训练数据上探索Transformer模型奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnetic Resonance Imaging (MRI) is an essential diagnostic tool forassessing knee injuries. However, manual interpretation of MRI slices remainstime-consuming and prone to inter-observer variability. This study presents asystematic evaluation of various deep learning architectures combined withexplainable AI (xAI) techniques for automated region of interest (ROI)detection in knee MRI scans. We investigate both supervised and self-supervisedapproaches, including ResNet50, InceptionV3, Vision Transformers (ViT), andmultiple U-Net variants augmented with multi-layer perceptron (MLP)classifiers. To enhance interpretability and clinical relevance, we integratexAI methods such as Grad-CAM and Saliency Maps. Model performance is assessedusing AUC for classification and PSNR/SSIM for reconstruction quality, alongwith qualitative ROI visualizations. Our results demonstrate that ResNet50consistently excels in classification and ROI identification, outperformingtransformer-based models under the constraints of the MRNet dataset. Whilehybrid U-Net + MLP approaches show potential for leveraging spatial features inreconstruction and interpretability, their classification performance remainslower. Grad-CAM consistently provided the most clinically meaningfulexplanations across architectures. Overall, CNN-based transfer learning emergesas the most effective approach for this dataset, while future work withlarger-scale pretraining may better unlock the potential of transformer models.</description>
      <author>example@mail.com (Justin Yiu, Kushank Arora, Daniel Steinberg, Rohit Ghiya)</author>
      <guid isPermaLink="false">2508.14151v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings</title>
      <link>http://arxiv.org/abs/2508.13672v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 34th ACM International Conference on Information and  Knowledge Management (CIKM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于实例的迁移学习LIME框架(ITL-LIME)，用于解决LIME方法在数据受限环境下的局部性和稳定性问题。&lt;h4&gt;背景&lt;/h4&gt;可解释人工智能方法如LIME通过使用可解释的替代模型来近似黑盒机器学习模型的行为，提高了模型解释性。然而，LIME在扰动和采样中的固有随机性会导致局部性和稳定性问题，特别是在训练数据有限的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法来增强数据受限环境中模型解释的保真度和稳定性。&lt;h4&gt;方法&lt;/h4&gt;ITL-LIME框架引入实例迁移学习，利用来自相关源域的相关真实实例辅助目标域的解释过程。具体包括：使用聚类将源域划分为具有代表性原型的簇；检索与目标实例最相似的原型对应的源实例，并与目标实例的相邻实例组合；构建基于对比学习的编码器作为加权机制，根据接近程度为实例分配权重；使用加权的源实例和目标实例训练替代模型。&lt;h4&gt;主要发现&lt;/h4&gt;数据稀缺会导致LIME生成不真实的变体和偏离真实数据流形的样本，使替代模型无法准确近似原始模型的复杂决策边界。&lt;h4&gt;结论&lt;/h4&gt;ITL-LIME通过利用源域的相关实例和基于对比学习的加权机制，有效提高了数据受限环境下模型解释的保真度和稳定性。&lt;h4&gt;翻译&lt;/h4&gt;可解释人工智能(XAI)方法，如局部可解释模型不可解释性(LIME)，通过使用可解释的替代模型来近似其局部行为，已推进了黑盒机器学习模型的解释性。然而，LIME在扰动和采样中的固有随机性会导致局部性和稳定性问题，特别是在训练数据有限的情况下。在这种情况下，数据稀缺可能导致生成不真实的变体和偏离真实数据流形的样本。因此，替代模型可能无法准确近似原始模型的复杂决策边界。为了解决这些挑战，我们提出了一种新颖的基于实例的迁移学习LIME框架(ITL-LIME)，该框架在数据受限环境中提高了解释的保真度和稳定性。ITL-LIME通过利用来自相关源域的相关真实实例来辅助目标域的解释过程，将实例迁移学习引入LIME框架。具体来说，我们采用聚类将源域划分为具有代表性原型的簇。与生成随机扰动不同，我们的方法从与目标实例最相似的原型对应的源簇中检索相关的真实源实例，然后将其与目标实例的相邻真实实例组合。为了定义一个紧凑的局部性，我们进一步构建了一个基于对比学习的编码器作为加权机制，根据实例与目标实例的接近程度为组合集中的实例分配权重。最后，使用这些加权的源实例和目标实例来训练用于解释目的的替代模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Explainable Artificial Intelligence (XAI) methods, such as LocalInterpretable Model-Agnostic Explanations (LIME), have advanced theinterpretability of black-box machine learning models by approximating theirbehavior locally using interpretable surrogate models. However, LIME's inherentrandomness in perturbation and sampling can lead to locality and instabilityissues, especially in scenarios with limited training data. In such cases, datascarcity can result in the generation of unrealistic variations and samplesthat deviate from the true data manifold. Consequently, the surrogate model mayfail to accurately approximate the complex decision boundary of the originalmodel. To address these challenges, we propose a novel Instance-based TransferLearning LIME framework (ITL-LIME) that enhances explanation fidelity andstability in data-constrained environments. ITL-LIME introduces instancetransfer learning into the LIME framework by leveraging relevant real instancesfrom a related source domain to aid the explanation process in the targetdomain. Specifically, we employ clustering to partition the source domain intoclusters with representative prototypes. Instead of generating randomperturbations, our method retrieves pertinent real source instances from thesource cluster whose prototype is most similar to the target instance. Theseare then combined with the target instance's neighboring real instances. Todefine a compact locality, we further construct a contrastive learning-basedencoder as a weighting mechanism to assign weights to the instances from thecombined set based on their proximity to the target instance. Finally, theseweighted source and target instances are used to train the surrogate model forexplanation purposes.</description>
      <author>example@mail.com (Rehan Raza, Guanjin Wang, Kok Wai Wong, Hamid Laga, Marco Fisichella)</author>
      <guid isPermaLink="false">2508.13672v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Fast globally optimal Truncated Least Squares point cloud registration with fixed rotation axis</title>
      <link>http://arxiv.org/abs/2508.15613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的线性时间凸松弛方法和contractor方法，用于加速点云配准问题的分支定界求解，显著提高了求解速度，能够在不到半秒内实现100个点的3D点云的全局最优配准。&lt;h4&gt;背景&lt;/h4&gt;点云配准在给定对应关系的情况下，使用截断最小平方方法可处理高达95%的离群率，但将这一组合优化问题求解到全局最优具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速且可证明全局最优的点云配准方法，解决现有SDP松弛方法速度慢的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的线性时间凸松弛方法以及一个加速分支定界（BnB）的contractor方法。&lt;h4&gt;主要发现&lt;/h4&gt;当旋转轴已知时，该求解器可以在不到半秒的时间内将两个包含100个点的3D点云以可证明的全局最优性进行配准；虽然不能解决完整的6DoF问题，但在解决仅旋转的TLS问题时，比最先进的SDP求解器STRIDE快两个数量级。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法不仅提供了全局最优性的正式证明，还通过对抗性实例展示了局部最小值接近全局最小值的经验证据，证明了方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，在给定对应关系的情况下，使用截断最小平方（TLS）方法可以使点云配准对高达95%的离群率具有鲁棒性。然而，将这个组合优化问题求解到全局最优是具有挑战性的。使用半定规划（SDP）松弛的可证明全局最优方法处理100个点需要数百秒。在本文中，我们提出了一种新的线性时间凸松弛方法以及一个加速分支定界（BnB）的contractor方法。当旋转轴已知时，我们的求解器可以在不到半秒的时间内将两个包含100个点的3D点云以可证明的全局最优性进行配准。尽管它目前不能解决完整的6DoF问题，但在解决仅旋转的TLS问题时，比最先进的SDP求解器STRIDE快两个数量级。除了提供全局最优性的正式证明外，我们还通过具有接近全局最小值的局部最小值的对抗性实例展示了全局最优性的经验证据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准中的全局最优性问题，特别是在处理高达95%异常值情况下的鲁棒性。这个问题在现实应用中非常重要，因为点云配准是场景重建、自动驾驶定位等应用的核心技术，而安全关键系统(如自动驾驶车辆)需要全局最优的算法来确保解决方案的正确性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：启发式方法(如RANSAC、GNC)没有全局最优保证，而保证全局最优的方法(如基于SDP松弛的方法)计算速度太慢。作者选择分支定界(BnB)框架，但需要更高效的边界计算。他们借鉴了区间分析思想来计算残差范围，使用现有最小二乘求解器作为基础并进行修改，同时参考了现有的分支定界框架但改进了边界计算和搜索空间缩减方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一种称为OptiPose的分支定界求解器，结合基于区间分析的线性时间凸松弛(WLS松弛)和高效的区间收缩方法。整体流程包括：1)将搜索空间划分为子空间(节点)；2)对每个节点计算WLS凸松弛获得下界；3)使用区间收缩方法显著减少搜索空间；4)在每个节点求解WLS松弛问题；5)根据下界和上界关系决定剪枝、分支或继续搜索；6)当上界等于下界时返回全局最优解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)线性时间凸松弛方法(WLS松弛)，可在O(N)时间内计算下界；2)高效的区间收缩方法，显著减少搜索空间；3)自适应活动集求解器，用于解决带球约束的最小二乘旋转估计问题。相比之前的工作，OptiPose比STRIDE等SDP方法快两个数量级(100毫秒vs200秒)，能处理更高异常值率(超过90%)，且能保证全局最优性，而启发式方法没有这种保证。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种快速的全局最优点云配准方法OptiPose，通过基于区间分析的线性时间凸松弛和高效的搜索空间缩减技术，在保证全局最优性的同时将计算速度提高了两个数量级，使点云配准在高达95%异常值率的情况下也能实时运行。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent results showed that point cloud registration with givencorrespondences can be made robust to outlier rates of up to 95\% using thetruncated least squares (TLS) formulation. However, solving this combinatorialoptimization problem to global optimality is challenging. Provably globallyoptimal approaches using semidefinite programming (SDP) relaxations takehundreds of seconds for 100 points. In this paper, we propose a novel lineartime convex relaxation as well as a contractor method to speed up Branch andBound (BnB). Our solver can register two 3D point clouds with 100 points toprovable global optimality in less than half a second when the axis of rotationis provided. Although it currently cannot solve the full 6DoF problem, it istwo orders of magnitude faster than the state-of-the-art SDP solver STRIDE whensolving the rotation-only TLS problem. In addition to providing a formal prooffor global optimality, we present empirical evidence of global optimality usingadversarial instances with local minimas close to the global minimum.</description>
      <author>example@mail.com (Ivo Ivanov, Carsten Markgraf)</author>
      <guid isPermaLink="false">2508.15613v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance</title>
      <link>http://arxiv.org/abs/2508.15650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为CFG的新型基于迁移的黑盒攻击方法，通过计算提取特征的重要性来规范对抗性点云的搜索，优先可能被多种架构采用的临界特征的破坏，并在损失函数中明确约束生成的对抗性点云的最大偏差程度，以确保其不可感知性。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络用于3D点云已被证明容易受到对抗性样本的影响。之前的3D对抗攻击方法通常利用有关目标模型的某些信息（如模型参数或输出）来生成对抗性点云。然而，在实际场景中，在绝对安全条件下获取有关目标模型的任何信息具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;专注于基于迁移的攻击，生成对抗性点云不需要任何有关目标模型的信息。基于对不同DNN架构中用于点云分类的关键特征一致性的观察，提出一种提高对抗性点云可迁移性的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出CFG，一种新颖的基于迁移的黑盒攻击方法，通过临界特征指导来提高对抗性点云的可迁移性。具体来说，该方法通过计算提取特征的重要性来规范对抗性点云的搜索，优先可能被多种架构采用的临界特征的破坏。此外，在损失函数中明确约束生成的对抗性点云的最大偏差程度，以确保其不可感知性。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40和ScanObjectNN基准数据集上进行的广泛实验表明，所提出的CFG方法以较大优势优于最先进的攻击方法。&lt;h4&gt;结论&lt;/h4&gt;CFG方法通过临界特征指导提高了对抗性点云的可迁移性，并在多个基准数据集上展示了优越的性能，为3D点云的安全研究提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络用于3D点云已被证明容易受到对抗性样本的影响。之前的3D对抗攻击方法通常利用有关目标模型的某些信息（如模型参数或输出）来生成对抗性点云。然而，在实际场景中，在绝对安全条件下获取有关目标模型的任何信息具有挑战性。因此，我们专注于基于迁移的攻击，生成对抗性点云不需要任何有关目标模型的信息。基于对不同DNN架构中用于点云分类的关键特征一致性的观察，我们提出CFG，一种新颖的基于迁移的黑盒攻击方法，通过提出的临界特征指导来提高对抗性点云的可迁移性。具体来说，我们的方法通过计算提取特征的重要性来规范对抗性点云的搜索，优先可能被多种架构采用的临界特征的破坏。此外，在损失函数中明确约束生成的对抗性点云的最大偏差程度，以确保其不可感知性。在ModelNet40和ScanObjectNN基准数据集上进行的广泛实验表明，所提出的CFG方法以较大优势优于最先进的攻击方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云深度学习模型的对抗性攻击迁移性问题，即在无法获取目标模型信息的情况下，如何生成能有效欺骗不同目标模型的对抗性点云。这个问题在现实中非常重要，因为3D点云模型广泛应用于自动驾驶、机器人等安全关键领域，而这些系统容易受到对抗性攻击的威胁；同时，在实际场景中，攻击者通常无法获取目标模型的详细信息，现有方法要么依赖白盒条件，要么需要大量查询，都不切实际。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察发现不同3D深度神经网络模型在识别点云时依赖的关键点存在显著重叠，这些关键点对应于物体类别的典型部位。基于这一发现，作者分析了现有迁移攻击方法的局限性，即生成的对抗性点云容易过度拟合到源模型。作者借鉴了2D图像领域的特征破坏攻击(FDA)和特征重要性感知的迁移攻击方法，以及AdvPC和PF-Attack等3D点云攻击方法的思想，设计了关键特征引导(CFG)策略，通过优先破坏跨模型共享的关键特征来提高迁移性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是不同3D深度学习模型在识别点云时依赖的关键特征具有一致性，通过优先破坏这些跨模型共享的关键特征，可以减少对抗性点云对源模型的过拟合，从而提高其迁移性。整体实现流程包括：1)特征重要性评估，使用梯度回传获取特征梯度并评估重要性；2)关键特征破坏，为关键特征分配高权重并引导搜索方向；3)损失函数设计，结合分类错误损失、关键特征引导损失和Chamfer距离约束；4)优化过程，使用Adam优化器迭代搜索最优对抗性点云，并限制扰动幅度确保不可感知性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次观察到不同3D深度学习模型依赖的关键特征具有一致性；2)提出关键特征引导(CFG)策略，通过优先破坏共享关键特征提高迁移性；3)设计新的损失函数，结合多个目标和Chamfer距离约束；4)采用中间层攻击策略而非输出层攻击。相比之前的工作，本文专注于中间层的关键特征而非无差别破坏特征，通过优先破坏跨模型共享的关键特征显著提高了迁移性，并对潜在防御措施表现出更强的抵抗力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于关键特征引导的3D点云迁移黑盒攻击方法，通过优先破坏跨模型共享的关键特征，显著提高了对抗性点云在不同目标模型间的迁移性，为3D深度学习系统的安全性评估提供了更有效的工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks for 3D point clouds have been demonstrated to bevulnerable to adversarial examples. Previous 3D adversarial attack methodsoften exploit certain information about the target models, such as modelparameters or outputs, to generate adversarial point clouds. However, inrealistic scenarios, it is challenging to obtain any information about thetarget models under conditions of absolute security. Therefore, we focus ontransfer-based attacks, where generating adversarial point clouds does notrequire any information about the target models. Based on our observation thatthe critical features used for point cloud classification are consistent acrossdifferent DNN architectures, we propose CFG, a novel transfer-based black-boxattack method that improves the transferability of adversarial point clouds viathe proposed Critical Feature Guidance. Specifically, our method regularizesthe search of adversarial point clouds by computing the importance of theextracted features, prioritizing the corruption of critical features that arelikely to be adopted by diverse architectures. Further, we explicitly constrainthe maximum deviation extent of the generated adversarial point clouds in theloss function to ensure their imperceptibility. Extensive experiments conductedon the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that theproposed CFG outperforms the state-of-the-art attack methods by a large margin.</description>
      <author>example@mail.com (Shuchao Pang, Zhenghan Chen, Shen Zhang, Liming Lu, Siyuan Liang, Anan Du, Yongbin Zhou)</author>
      <guid isPermaLink="false">2508.15650v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds</title>
      <link>http://arxiv.org/abs/2508.15646v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种弱监督方法用于机载激光扫描(ALS)数据的树木实例分割，通过人类操作员的初始质量评估来训练评分模型，进而微调分割模型，提高了树木识别准确率并减少了误识别。&lt;h4&gt;背景&lt;/h4&gt;机载激光扫描(ALS)数据中的树木实例分割对森林监测非常重要，但由于传感器分辨率、植被状态和地形特征等因素造成的数据变化，这仍然是一个挑战。此外，获取精确标注数据来训练完全监督的实例分割方法成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决树木实例分割中的挑战，减少对大量精确标注数据的依赖，提高分割准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一种弱监督方法：1)人类操作员对初始分割结果（通过未微调模型或闭式算法获得）进行质量评级；2)使用这些评级训练评分模型，将分割输出分类为人类指定的类别；3)利用评分模型的反馈微调分割模型。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在正确识别的树木实例方面将原始分割模型提高了34%，同时显著减少了预测的非树木实例数量。&lt;h4&gt;结论&lt;/h4&gt;该方法有效提高了树木实例分割的准确性，但在树木稀疏区域（小树）或复杂环境（灌木、巨石等）中性能仍有下降，这些区域是未来需要改进的方向。&lt;h4&gt;翻译&lt;/h4&gt;机载激光扫描(ALS)数据的树木实例分割对森林监测至关重要，但由于传感器分辨率、获取时的植被状态、地形特征等因素造成的数据变化，这仍然具有挑战性。此外，获取足够数量的精确标注数据来训练完全监督的实例分割方法成本高昂。为应对这些挑战，我们提出了一种弱监督方法，其中人类操作员对初始分割结果（通过未微调模型或闭式算法获得）提供质量评级。质量评估过程中产生的标签用于训练评分模型，该模型的任务是将分割输出分类为与人类操作员指定的相同类别。最后，使用评分模型的反馈微调分割模型。这反过来将原始分割模型在正确识别的树木实例方面提高了34%，同时显著减少了预测的非树木实例数量。在树木稀疏区域（高度小于两米的小树）或包含灌木、巨石等复杂环境的周围区域仍然存在挑战，这些区域可能被误认为树木，导致所提出方法的性能降低。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从机载激光雷达点云中进行树木实例分割的挑战，以及获取足够精确标注数据的高昂成本问题。这个问题在现实中很重要，因为准确监测森林分布对评估木材资源、气候变化影响、斜坡稳定性和碳捕获能力至关重要，而传统方法需要大量人工标注，既耗时又昂贵。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考如何减少对精确标注数据的依赖，受强化学习从人类反馈的启发，提出了一种弱监督学习方法。他们设计了人类对分割结果进行评级而非精确标注的流程，并训练评级模型来模拟人类判断。作者借鉴了点云分类领域的多种架构（如PointNet、Point Transformer等）和现有的分割模型（如SegmentAnyTree），但创新性地将它们组合成一个迭代改进的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用弱监督学习和迭代改进机制：通过人类对初始分割结果的简单评级来训练一个评级模型，然后利用该模型提供的反馈来微调分割模型，形成良性循环。整体流程包括：1)使用预训练模型或算法获得初始分割；2)人类对样本聚类进行评级；3)训练评级模型；4)用评级模型对所有结果评分生成伪标签；5)用伪标签微调分割模型；6)迭代更新伪标签直到性能稳定。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)弱监督框架，用评级代替精确标注；2)迭代改进机制，通过模型间相互反馈提升性能；3)基于KDE的VoxNet架构，提高评级准确性；4)伪标签更新策略，有效识别新树实例。相比之前工作，本文方法大幅减少了标注成本，通过迭代过程持续改进分割性能，实验证明正确识别的树木实例数量提高了34%，同时减少了非树实例的误判。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的弱监督学习框架，通过人类对分割结果的评级来训练评级模型，进而指导分割模型的迭代改进，显著提高了树木实例分割的准确性同时大幅减少了标注数据的需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tree instance segmentation of airborne laser scanning (ALS) data is of utmostimportance for forest monitoring, but remains challenging due to variations inthe data caused by factors such as sensor resolution, vegetation state atacquisition time, terrain characteristics, etc. Moreover, obtaining asufficient amount of precisely labeled data to train fully supervised instancesegmentation methods is expensive. To address these challenges, we propose aweakly supervised approach where labels of an initial segmentation resultobtained either by a non-finetuned model or a closed form algorithm areprovided as a quality rating by a human operator. The labels produced duringthe quality assessment are then used to train a rating model, whose task is toclassify a segmentation output into the same classes as specified by the humanoperator. Finally, the segmentation model is finetuned using feedback from therating model. This in turn improves the original segmentation model by 34\% interms of correctly identified tree instances while considerably reducing thenumber of non-tree instances predicted. Challenges still remain in data oversparsely forested regions characterized by small trees (less than two meters inheight) or within complex surroundings containing shrubs, boulders, etc. whichcan be confused as trees where the performance of the proposed method isreduced.</description>
      <author>example@mail.com (Swann Emilien Céleste Destouches, Jesse Lahaye, Laurent Valentin Jospin, Jan Skaloud)</author>
      <guid isPermaLink="false">2508.15646v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework</title>
      <link>http://arxiv.org/abs/2508.15457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需运动恢复结构(SfM)的3D高斯飞溅(3DGS)方法，能够从极其稀疏的视图输入中联合估计相机位置并重建3D场景。该方法通过密集立体匹配模块替代传统SfM初始化，并引入相干视图插值模块和多尺度正则化技术来增强重建质量。实验表明，在仅使用2个训练视图的极端稀疏条件下，该方法在PSNR指标上实现了2.75dB的提升，生成的图像失真最小且保留丰富高频细节，视觉质量显著优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;3D高斯飞溅(3DGS)在新颖视图合成中展现出卓越的实时性能，但其有效性严重依赖于具有精确已知相机位置的密集多视角输入，这在现实场景中很少可用。当输入视图变得极其稀疏时，3DGS依赖的运动恢复结构(SfM)初始化方法无法准确重建场景的3D几何结构，导致渲染质量下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需SfM的3DGS方法，从极其稀疏的视图输入中联合估计相机位置并重建3D场景，解决传统方法在稀疏视图条件下的局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 提出密集立体匹配模块替代SfM，逐步估计相机位置信息并重建全局密集点云用于初始化；2) 提出相干视图插值模块，基于训练视图对插值相机位置，生成视角一致的内容作为训练的额外监督信号；3) 引入多尺度拉普拉斯一致性正则化和自适应空间感知多尺度几何正则化，增强几何结构和渲染内容的质量。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法显著优于其他最先进的3DGS方法，在极其稀疏视图条件下(仅使用2个训练视图)，PSNR指标实现了2.75dB的显著提升。该方法合成的图像失真最小，同时保留了丰富的高频细节，与现有技术相比具有优越的视觉质量。&lt;h4&gt;结论&lt;/h4&gt;所提出的无需SfM的3DGS方法能够从极稀疏的视图输入中有效重建高质量3D场景，不依赖于传统的SfM初始化过程，在视觉质量和客观指标上都优于现有技术，为稀疏视图条件下的3D重建提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯飞溅(3DGS)在新颖视图合成中表现出卓越的实时性能，但其有效性严重依赖于具有精确已知相机位置的密集多视角输入，这在现实场景中很少可用。当输入视图变得极其稀疏时，3DGS依赖的运动恢复结构(SfM)初始化方法无法准确重建场景的3D几何结构，导致渲染质量下降。在本文中，我们提出了一种新颖的无需SfM的3DGS方法，能够从极其稀疏的视图输入中联合估计相机位置并重建3D场景。具体而言，我们提出密集立体匹配模块替代SfM，逐步估计相机位置信息并重建全局密集点云用于初始化。为解决极稀疏视图设置中固有的信息稀缺问题，我们提出相干视图插值模块，基于训练视图对插值相机位置，并生成视角一致的内容作为训练的额外监督信号。此外，我们引入多尺度拉普拉斯一致性正则化和自适应空间感知多尺度几何正则化，以增强几何结构和渲染内容的质量。实验表明，我们的方法显著优于其他最先进的3DGS方法，在极其稀疏视图条件下(仅使用2个训练视图)实现了2.75dB的显著PSNR提升。我们方法合成的图像失真最小，同时保留了丰富的高频细节，与现有技术相比具有优越的视觉质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在极稀疏视图输入（如只有2个视图）下进行新颖视图合成的问题。这个问题在现实中很重要，因为真实世界场景中获取密集多视图图像和精确相机姿态往往是困难的，而极稀疏视图输入在移动设备拍照、监控视频等场景中更为常见。解决这一问题可以大大扩展3D场景重建技术的应用范围，使其更适用于虚拟现实、增强现实和3D内容创建等实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3DGS在稀疏视图下的局限性，特别是SfM在极稀疏视图下无法准确重建几何结构。他们设计了一个不依赖SfM的框架，通过密集立体模块直接估计相机姿态并重建点云。针对信息稀缺问题，他们引入视图插值生成额外监督信号。方法借鉴了3DGS作为基础渲染技术、密集立体匹配用于深度估计、视频扩散模型用于视图生成，以及多尺度正则化技术来提高质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是不依赖SfM，而是通过密集立体模块直接从稀疏视图中估计相机姿态并重建点云，同时利用视图插值生成额外监督信号。整体流程：1)使用密集立体模块估计相机姿态并生成初始化点云；2)通过一致的视图插值模块生成中间视角图像作为额外监督；3)结合多尺度拉普拉斯一致正则化和自适应空间感知多尺度几何正则化提高渲染质量和几何结构；4)整体优化3DGS模型参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)密集立体模块替代SfM初始化；2)一致的视图插值模块生成额外监督信号；3)多尺度拉普拉斯一致正则化保持细节；4)自适应空间感知多尺度几何正则化增强几何结构。相比之前工作，该方法不依赖SfM和已知相机姿态，同时处理姿态估计和场景重建，通过视图插值解决信息稀缺问题，并在只有2个视图的极端情况下实现了显著性能提升(PSNR提高2.75dB)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种不依赖运动结构的3D高斯溅射框架，通过密集立体模块、视图插值和多尺度正则化技术，实现了在极稀疏视图输入下（如仅2个视图）的高质量新颖视图合成，显著提升了真实世界场景中3D重建的实用性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has demonstrated remarkable real-timeperformance in novel view synthesis, yet its effectiveness relies heavily ondense multi-view inputs with precisely known camera poses, which are rarelyavailable in real-world scenarios. When input views become extremely sparse,the Structure-from-Motion (SfM) method that 3DGS depends on for initializationfails to accurately reconstruct the 3D geometric structures of scenes,resulting in degraded rendering quality. In this paper, we propose a novelSfM-free 3DGS-based method that jointly estimates camera poses and reconstructs3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, wepropose a dense stereo module to progressively estimates camera poseinformation and reconstructs a global dense point cloud for initialization. Toaddress the inherent problem of information scarcity in extremely sparse-viewsettings, we propose a coherent view interpolation module that interpolatescamera poses based on training view pairs and generates viewpoint-consistentcontent as additional supervision signals for training. Furthermore, weintroduce multi-scale Laplacian consistent regularization and adaptivespatial-aware multi-scale geometry regularization to enhance the quality ofgeometrical structures and rendered content. Experiments show that our methodsignificantly outperforms other state-of-the-art 3DGS-based approaches,achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-viewconditions (using only 2 training views). The images synthesized by our methodexhibit minimal distortion while preserving rich high-frequency details,resulting in superior visual quality compared to existing techniques.</description>
      <author>example@mail.com (Zongqi He, Hanmin Li, Kin-Chung Chan, Yushen Zuo, Hao Xie, Zhe Xiao, Jun Xiao, Kin-Man Lam)</author>
      <guid isPermaLink="false">2508.15457v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>A Low-Latency 3D Live Remote Visualization System for Tourist Sites Integrating Dynamic and Pre-captured Static Point Clouds</title>
      <link>http://arxiv.org/abs/2508.15398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 pages, 4 figures, submitted to IEEE ISMAR 2025 Posters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合多个激光雷达和相机的系统，用于实时捕捉动态点云并与静态点云集成，实现大区域3D可视化，解决了户外旅游景点中传感器放置受限和光照变化的问题。&lt;h4&gt;背景&lt;/h4&gt;现有基于RGB-D相机和体积捕捉的实时动态3D空间捕捉方法难以应用于户外旅游景点，因为维护和美学限制限制了传感器位置，且日光变化增加了处理复杂度。&lt;h4&gt;目的&lt;/h4&gt;开发一种系统，结合多个激光雷达和相机进行实时动态点云捕捉，并与预先捕获的静态点云集成，实现大区域3D可视化。&lt;h4&gt;方法&lt;/h4&gt;系统在大区域场景中维持30 fps的帧率，延迟低于100毫秒；通过自动调整静态点云颜色以适应当前光照条件，减轻光照不一致问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过在旅游景点进行实际部署，证明了所提系统的有效性。&lt;h4&gt;结论&lt;/h4&gt;该系统能够有效解决户外旅游景点中的实时3D捕捉和可视化挑战，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;已经提出了多种用于捕捉和传输动态3D空间的实时方法，包括基于RGB-D相机和体积捕捉的方法。然而，由于维护和美学限制限制了传感器的放置，以及日光变化使处理复杂化，将现有方法应用于户外旅游景点仍然困难。我们提出了一种结合多个激光雷达和相机进行实时动态点云捕捉的系统，并将其与预先捕获的静态点云集成，用于大区域3D可视化。该系统在大区域场景中保持30 fps的帧率，同时延迟低于100毫秒。为了减轻光照不一致性，静态点云颜色会自动调整到当前光照条件。通过在旅游景点进行实际部署，证明了我们系统的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决旅游景点低延迟3D远程可视化的问题。现有方法难以在户外环境中应用，因为维护和美学限制传感器放置，且日光变化影响处理。这个问题很重要，因为3D流媒体能提供更强的存在感、参与度和空间理解，比传统2D视频更有利于旅游推广和虚拟体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有3D捕获技术的局限性：静态方法无法实时捕捉变化，RGB-D相机范围有限，体积系统需要大量相机。作者设计时考虑使用多个LiDAR和相机组合，保护传感器外壳，并融合静态与动态点云。借鉴了颜色传输算法、CNN处理和PTP同步等技术，但针对3D点云进行了改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合实时动态点云与预捕获静态点云，通过颜色调整使静态点云适应当前光照。流程包括：1)使用LiDAR和相机同步采集数据；2)处理数据解决闪烁、遮挡和稀疏问题；3)将静态点云分簇并调整颜色；4)将处理后的数据流传输到远程站点进行3D渲染。整个系统保持30fps帧率和低于100ms的延迟。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)特殊设计的防护外壳，使用蛾眼抗反射膜减少信号丢失；2)三种轻量级实时算法解决闪烁、遮挡误着色和稀疏性问题；3)改进的颜色传输方法，结合全局和局部适应；4)系统集成动态和静态点云。相比之前工作，此系统能处理更大面积场景，对光照变化更鲁棒，延迟更低，且传感器保护更好。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的低延迟3D远程可视化系统，通过结合实时动态点云与颜色调整的静态点云，成功实现了户外旅游景点的实时远程3D展示，为虚拟旅游提供了新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Various real-time methods for capturing and transmitting dynamic 3D spaceshave been proposed, including those based on RGB-D cameras and volumetriccapture. However, applying existing methods to outdoor tourist sites remainsdifficult because maintenance and aesthetic constraints limit sensor placement,and daylight variability complicates processing. We propose a system thatcombines multiple LiDARs and cameras for live dynamic point cloud capture, andintegrates them with pre-captured static point clouds for wide-area 3Dvisualization. The system sustains 30 fps across wide-area scenes while keepinglatency below 100 ms. To mitigate lighting inconsistencies, static point-cloudcolors are automatically adjusted to current lighting. The effectiveness of oursystem is demonstrated through real-world deployment in a tourist site.</description>
      <author>example@mail.com (Takahiro Matsumoto, Masafumi Suzuki, Mariko Yamaguchi, Masakatsu Aoki, Shunsuke Konagai, Kazuhiko Murasaki)</author>
      <guid isPermaLink="false">2508.15398v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT</title>
      <link>http://arxiv.org/abs/2508.15299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to MMSports&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究构建了BasketLiDAR数据集，这是体育多目标跟踪领域首个结合激光雷达点云与同步多视角摄像机画面的多模态数据集，并提出了一种新的多目标跟踪框架，实现了提高跟踪精度和降低计算成本的双重目标。&lt;h4&gt;背景&lt;/h4&gt;实时3D轨迹球员跟踪在体育战术分析、表现评估和增强观众体验中起着至关重要的作用。传统系统依赖于多摄像机设置，但受限于视频数据的固有二维性质和复杂的3D重建处理需求，使得实时分析具有挑战性。篮球是多目标跟踪领域最困难的场景之一，因为十名球员在有限场地空间内快速复杂移动，且因激烈的身体接触导致频繁遮挡。&lt;h4&gt;目的&lt;/h4&gt;解决传统系统在篮球场景中实时3D轨迹跟踪面临的挑战，包括二维视频数据的局限性、复杂3D重建处理需求、球员快速复杂移动以及频繁遮挡问题。&lt;h4&gt;方法&lt;/h4&gt;构建BasketLiDAR数据集，结合激光雷达点云与同步多视角摄像机画面；开发一种新型多目标跟踪算法，利用激光雷达的高精度3D空间信息；提出两种跟踪流程：仅使用激光雷达的实时跟踪流程和融合激光雷达与摄像头数据的多模态跟踪流程。&lt;h4&gt;主要发现&lt;/h4&gt;BasketLiDAR数据集包含4,445帧和3,105个球员ID，三个激光雷达传感器和三个多视角摄像机之间的ID完全同步；记录了5对5和3对3专业篮球运动员的比赛数据，为每位球员提供完整的3D位置信息和ID标注；所提出的方法实现了实时操作，这是传统仅使用摄像机方法难以实现的；即使在遮挡条件下，该方法也能实现卓越的跟踪性能。&lt;h4&gt;结论&lt;/h4&gt;BasketLiDAR数据集和提出的跟踪框架有效解决了篮球场景中实时3D轨迹跟踪的挑战，提高了跟踪精度并降低了计算成本，为体育战术分析、表现评估和观众体验增强提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR(激光雷达)，MOT(多目标跟踪)，point clouds(点云)，occlusions(遮挡)，multi-camera setups(多摄像机设置)，3D reconstruction(3D重建)，synchronized(同步的)，multimodal(多模态的)&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统体育3D多目标跟踪系统的局限性问题。传统系统依赖多相机获取二维信息，需要复杂处理才能重建3D位置，计算成本高且难以实时分析。篮球场景特别具有挑战性，因为十名球员在有限场地内快速移动且频繁发生遮挡。解决这个问题对战术分析、球员表现评估和提升观赛体验至关重要，能推动数据驱动的体育分析发展，为球员、教练和观众提供价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考引入LiDAR传感器能否同时提高体育MOT精度并降低计算成本。他们分析了LiDAR可直接获取高精度3D点云的优势，消除从2D信息估计3D空间的复杂处理。作者借鉴了自动驾驶领域的多模态数据集和MOT技术，但指出体育场景与自动驾驶场景在跟踪目标、运动模式和传感器放置方面存在根本差异，因此需要专门设计。作者构建了BasketLiDAR数据集并设计了融合框架，结合LiDAR的空间信息和相机的丰富外观信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用LiDAR的高精度3D空间信息消除复杂的3D位置重建过程，降低计算负载并实现高精度3D估计；结合LiDAR和相机信息实现高精度识别；采用两阶段方法：先用LiDAR快速生成轨迹，再对遮挡部分应用相机重新识别。整体流程包括：1) LiDAR-only管道：集成多LiDAR数据、过滤区域、投影到鸟瞰图、检测和跟踪；2) LiDAR-camera融合管道：检测遮挡时段、匹配遮挡前后清晰帧、基于RGB特征重新识别链接相同球员。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 创建BasketLiDAR，首个结合LiDAR和相机的专业篮球多模态数据集；2) 提出融合框架，整合LiDAR跟踪和相机ID重新识别；3) 开源数据集和模型实现。相比之前工作：1) 现有数据集仅依赖RGB视频，而BasketLiDAR结合了LiDAR点云和多视角相机；2) 传统方法需复杂处理从2D估计3D，而本文通过LiDAR直接获取3D数据；3) 传统方法在遮挡场景表现不佳，本文方法即使在遮挡条件下也能保持优越性能；4) 传统方法计算成本高难以实时，本文方法实现了实时操作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文创建了首个结合LiDAR和相机的专业篮球多模态数据集，并提出融合框架实现了高精度、实时的3D球员跟踪，即使在频繁遮挡的篮球场景中也能保持优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time 3D trajectory player tracking in sports plays a crucial role intactical analysis, performance evaluation, and enhancing spectator experience.Traditional systems rely on multi-camera setups, but are constrained by theinherently two-dimensional nature of video data and the need for complex 3Dreconstruction processing, making real-time analysis challenging. Basketball,in particular, represents one of the most difficult scenarios in the MOT field,as ten players move rapidly and complexly within a confined court space, withfrequent occlusions caused by intense physical contact.  To address these challenges, this paper constructs BasketLiDAR, the firstmultimodal dataset in the sports MOT field that combines LiDAR point cloudswith synchronized multi-view camera footage in a professional basketballenvironment, and proposes a novel MOT framework that simultaneously achievesimproved tracking accuracy and reduced computational cost. The BasketLiDARdataset contains a total of 4,445 frames and 3,105 player IDs, with fullysynchronized IDs between three LiDAR sensors and three multi-view cameras. Werecorded 5-on-5 and 3-on-3 game data from actual professional basketballplayers, providing complete 3D positional information and ID annotations foreach player. Based on this dataset, we developed a novel MOT algorithm thatleverages LiDAR's high-precision 3D spatial information. The proposed methodconsists of a real-time tracking pipeline using LiDAR alone and a multimodaltracking pipeline that fuses LiDAR and camera data. Experimental resultsdemonstrate that our approach achieves real-time operation, which was difficultwith conventional camera-only methods, while achieving superior trackingperformance even under occlusion conditions. The dataset is available uponrequest at: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar</description>
      <author>example@mail.com (Ryunosuke Hayashi, Kohei Torimi, Rokuto Nagata, Kazuma Ikeda, Ozora Sako, Taichi Nakamura, Masaki Tani, Yoshimitsu Aoki, Kentaro Yoshioka)</author>
      <guid isPermaLink="false">2508.15299v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Multi-Modal Coding for High-Quality 3D Generation</title>
      <link>http://arxiv.org/abs/2508.15228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TriMM是一种创新的3D原生前馈生成模型，通过协作多模态编码和辅助监督机制，有效整合RGB、RGBD和点云等多模态数据，实现高质量3D资产生成。&lt;h4&gt;背景&lt;/h4&gt;3D内容具有多模态特性，不同模态在3D建模中各有优势（RGB提供纹理，点云提供几何）。现有3D生成架构主要局限于单模态或特定3D结构，忽略了多模态数据的互补优势，限制了可用训练数据范围。&lt;h4&gt;目的&lt;/h4&gt;全面利用多模态数据进行3D建模，提出首个从基础多模态（RGB、RGBD和点云）学习的3D原生前馈生成模型TriMM。&lt;h4&gt;方法&lt;/h4&gt;1) 引入协作多模态编码，整合模态特定特征同时保留其独特优势；2) 引入辅助2D和3D监督提高多模态编码鲁棒性和性能；3) 基于嵌入的多模态代码，使用三平面潜在扩散模型生成高质量3D资产，增强纹理和几何细节。&lt;h4&gt;主要发现&lt;/h4&gt;在多个知名数据集上的实验表明，TriMM有效利用多模态，仅用少量训练数据即可实现与大规模数据集训练模型相竞争的性能。在RGB-D数据集上的额外实验验证了将其他多模态数据集纳入3D生成的可行性。&lt;h4&gt;结论&lt;/h4&gt;TriMM成功利用多模态数据实现高质量3D资产生成，在有限数据集上达到与大规模数据集训练模型相竞争的性能，为3D内容生成提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;3D内容本质上包含多模态特征，可以投影到不同模态（例如RGB图像、RGBD和点云）。每种模态在3D资产建模中展现独特优势：RGB图像包含生动的3D纹理，而点云定义细粒度的3D几何形状。然而，大多数现有的3D原生生成架构主要在单模态范式下运行，从而忽略了多模态数据的互补优势，或者将自己局限于3D结构，限制了可用训练数据集的范围。为了全面利用多模态进行3D建模，我们提出了TriMM，这是第一个从基础多模态（例如RGB、RGBD和点云）学习的3D原生前馈生成模型。具体来说，1) TriMM首次引入协作多模态编码，整合模态特定特征，同时保留其独特的表示优势。2) 此外，引入辅助2D和3D监督，提高多模态编码的鲁棒性和性能。3) 基于嵌入的多模态代码，TriMM采用三平面潜在扩散模型生成高质量的3D资产，同时增强纹理和几何细节。在多个知名数据集上的大量实验表明，TriMM通过有效利用多模态，尽管只使用少量训练数据，但仍实现了在大规模数据集上训练的模型相竞争的性能。此外，我们在最近的RGB-D数据集上进行了额外实验，验证了将其他多模态数据集纳入3D生成的可行性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D内容生成中多模态信息利用不足的问题。现有方法大多只使用单一模态数据（如仅RGB图像），而忽视了RGB、RGBD和点云等不同模态数据的互补优势。这个问题很重要，因为高质量3D生成在虚拟现实、机器人模拟、工业设计和动画等领域有广泛应用，而通过有效利用多模态数据可以在有限的3D数据资源下显著提高生成质量和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了不同模态数据的优缺点：RGB提供丰富纹理但几何信息有限，点云和深度图像提供准确几何但缺乏纹理。基于此，设计了协作多模态编码框架，为每种模态设计专门编码器，并通过共享解码器映射到统一空间。方法借鉴了扩散模型（如Point-E）、三平面表示、DINOv2和PointNet等现有工作，但创新性地将它们整合到一个多模态框架中，并引入混合2D/3D监督和特定模态的重建损失来提高性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过协作多模态编码整合不同模态数据的优势，利用重建损失引导模型充分利用每种模态的优势同时避免其局限性。整体流程分两阶段：1）多模态编码阶段，使用专门的RGB、RGBD和点云编码器将输入转换为三平面表示，通过共享解码器映射到统一潜在空间，结合2D和3D监督损失进行训练；2）三平面潜在扩散模型阶段，使用VAE压缩特征，以CLIP图像嵌入为条件训练扩散模型，应用模态特定重建损失，最终生成高质量三平面并解码为3D网格。整个流程从单张RGB图像输入到生成最终3D纹理网格约需4秒。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）协作多模态编码（TriMM），首次整合RGB、RGBD和点云三种模态到统一3D生成框架；2）混合2D和3D监督，结合图像空间和几何空间损失提高鲁棒性；3）特定模态的重建损失机制，引导模型利用各模态优势；4）高效分阶段训练策略。相比之前工作，不同于单模态方法（如TripoSR）的局限性，也不同于TRELLIS等早期融合多模态方法，采用后融合策略提高可扩展性；相比优化方法（如DreamFusion）无需复杂优化，速度更快；相比重建方法（如LRM）增加了生成能力而非仅重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了TriMM，一种协作多模态编码方法，通过整合RGB、RGBD和点云数据的优势，在有限训练数据条件下实现了高质量、高效的3D内容生成。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D content inherently encompasses multi-modal characteristics and can beprojected into different modalities (e.g., RGB images, RGBD, and point clouds).Each modality exhibits distinct advantages in 3D asset modeling: RGB imagescontain vivid 3D textures, whereas point clouds define fine-grained 3Dgeometries. However, most existing 3D-native generative architectures eitheroperate predominantly within single-modality paradigms-thus overlooking thecomplementary benefits of multi-modality data-or restrict themselves to 3Dstructures, thereby limiting the scope of available training datasets. Toholistically harness multi-modalities for 3D modeling, we present TriMM, thefirst feed-forward 3D-native generative model that learns from basicmulti-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMMfirst introduces collaborative multi-modal coding, which integratesmodality-specific features while preserving their unique representationalstrengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced toraise the robustness and performance of multi-modal coding. 3) Based on theembedded multi-modal code, TriMM employs a triplane latent diffusion model togenerate 3D assets of superior quality, enhancing both the texture and thegeometric detail. Extensive experiments on multiple well-known datasetsdemonstrate that TriMM, by effectively leveraging multi-modality, achievescompetitive performance with models trained on large-scale datasets, despiteutilizing a small amount of training data. Furthermore, we conduct additionalexperiments on recent RGB-D datasets, verifying the feasibility ofincorporating other multi-modal datasets into 3D generation.</description>
      <author>example@mail.com (Ziang Cao, Zhaoxi Chen, Liang Pan, Ziwei Liu)</author>
      <guid isPermaLink="false">2508.15228v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2508.15002v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新方法来合成大规模、多样化且物理可行的抓取方案，超越简单强力抓取，包括精细操作如捏取和三指精确抓取。作者引入了可微的力封闭能量公式和优化方法MALA*，显著提高了抓取多样性和预测稳定性，并提供了包含5700个物体的大型数据集。&lt;h4&gt;背景&lt;/h4&gt;灵巧的机器人手由于其多指设计的灵活性和适应性，能实现多样化交互，但充分利用其能力需要多样化和高质量的抓取数据。现有数据集生成方法通常依赖基于采样的算法或简化的力封闭分析，往往收敛到强力抓取且多样性有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有抓取数据生成方法的局限性，提出一种能合成大规模、多样化且物理可行抓取方案的方法，包括超越简单强力抓取的精细操作。&lt;h4&gt;方法&lt;/h4&gt;引入严格可微的力封闭能量公式，通过二次规划(QP)隐式定义；提出调整的优化方法MALA*，通过基于能量值分布动态拒绝梯度步骤提高性能；评估方法在抓取多样性和稳定性方面的改进；创建包含5700个物体、五种夹持器和三种抓取类型的大规模数据集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能生成超越简单强力抓取的多样化抓取方案；在抓取多样性和最终抓取预测稳定性方面有显著改进；提供了大规模抓取数据集支持灵巧手的进一步应用。&lt;h4&gt;结论&lt;/h4&gt;通过可微的力封闭能量公式和优化的MALA*方法，成功解决了现有抓取数据生成方法的局限性，为灵巧手应用提供了更多样化、更精细且物理可行的抓取方案和数据支持。&lt;h4&gt;翻译&lt;/h4&gt;灵巧的机器人手由于其多指设计的灵活性和适应性，能够实现多样化的交互，允许在各种环境中执行特定任务的抓取配置。然而，要充分利用灵巧手的能力，获取多样化和高质量的抓取数据至关重要，无论是用于从点云开发抓取预测模型、训练操作策略，还是为高级任务规划提供更广泛的选择。现有的数据集生成方法通常依赖于基于采样的算法或简化的力封闭分析，这些方法往往收敛到强力抓取，且多样性有限。在这项工作中，我们提出了一种方法来合成大规模、多样化且物理可行的抓取方案，超越了简单的强力抓取，包括精细操作，如捏取和三指精确抓取。我们引入了一种严格可微的力封闭能量公式，通过二次规划(QP)隐式定义。此外，我们提出了一种调整的优化方法(MALA*)，通过基于所有样本能量值分布动态拒绝梯度步骤来提高性能。我们广泛评估了我们的方法，证明了在抓取多样性和最终抓取预测稳定性方面的显著改进。最后，我们提供了来自DexGraspNet的5,700个物体的大规模抓取数据集，包含五种不同的夹持器和三种不同的抓取类型。数据集和代码：https://graspqp.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous robotic hands enable versatile interactions due to the flexibilityand adaptability of multi-fingered designs, allowing for a wide range oftask-specific grasp configurations in diverse environments. However, to fullyexploit the capabilities of dexterous hands, access to diverse and high-qualitygrasp data is essential -- whether for developing grasp prediction models frompoint clouds, training manipulation policies, or supporting high-level taskplanning with broader action options. Existing approaches for datasetgeneration typically rely on sampling-based algorithms or simplifiedforce-closure analysis, which tend to converge to power grasps and oftenexhibit limited diversity. In this work, we propose a method to synthesizelarge-scale, diverse, and physically feasible grasps that extend beyond simplepower grasps to include refined manipulations, such as pinches and tri-fingerprecision grasps. We introduce a rigorous, differentiable energy formulation offorce closure, implicitly defined through a Quadratic Program (QP).Additionally, we present an adjusted optimization method (MALA*) that improvesperformance by dynamically rejecting gradient steps based on the distributionof energy values across all samples. We extensively evaluate our approach anddemonstrate significant improvements in both grasp diversity and the stabilityof final grasp predictions. Finally, we provide a new, large-scale graspdataset for 5,700 objects from DexGraspNet, comprising five different grippersand three distinct grasp types.  Dataset and Code:https://graspqp.github.io/</description>
      <author>example@mail.com (René Zurbrügg, Andrei Cramariuc, Marco Hutter)</author>
      <guid isPermaLink="false">2508.15002v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>TiP4GEN: Text to Immersive Panorama 4D Scene Generation</title>
      <link>http://arxiv.org/abs/2508.12415v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted In Proceedings of the 33rd ACM International Conference on  Multimedia (MM' 25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TiP4GEN框架，一个先进的文本到动态全景场景生成系统，能够实现细粒度内容控制并合成运动丰富、几何一致的全景4D场景，为VR/AR技术提供真正的360度沉浸式体验。&lt;h4&gt;背景&lt;/h4&gt;随着VR/AR技术的快速发展和广泛应用，对高质量、沉浸式动态场景的需求不断增长。然而，现有生成工作主要局限于静态场景或狭小视角动态场景，无法提供真正的360度沉浸式体验。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从任何视角提供真正360度沉浸式体验的动态全景场景生成框架，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;TiP4GEN整合全景视频生成和动态场景重建技术。视频生成采用双分支生成模型，包括负责全局视图的全景分支和负责局部视图的透视分支，并通过双向交叉注意力机制促进信息交换。场景重建则基于3D高斯溅射的几何对齐模型，使用度量深度图对齐时空点云，并利用估计的姿态初始化场景相机，确保几何一致性和时间连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，TiP4GEN在生成视觉上引人入胜且运动连贯的动态全景场景方面具有优越性，其设计的有效性得到了充分验证。&lt;h4&gt;结论&lt;/h4&gt;TiP4GEN成功解决了现有生成方法的局限性，能够创建真正沉浸式的360度动态虚拟环境，为VR/AR应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;随着VR/AR技术的快速发展和广泛采用，对创建高质量、沉浸式动态场景的需求日益增长。然而，现有的生成工作主要集中在静态场景或狭小视角动态场景的创建上，无法从任何视角提供真正的360度沉浸式体验。在本文中，我们介绍了TiP4GEN，一个先进的文本到动态全景场景生成框架，能够实现细粒度内容控制并合成运动丰富、几何一致的全景4D场景。TiP4GEN整合了全景视频生成和动态场景重建，以创建360度沉浸式虚拟环境。对于视频生成，我们引入了一个由全景分支和透视分支组成的双分支生成模型，分别负责全局和局部视图的生成。双向交叉注意力机制促进了分支之间的全面信息交换。对于场景重建，我们提出了一个基于3D高斯溅射的几何对齐重建模型。通过使用度量深度图对齐时空点云，并使用估计的姿态初始化场景相机，我们的方法确保了重建场景的几何一致性和时间连贯性。大量实验证明了我们提出设计的有效性和TiP4GEN在生成视觉上引人入胜且运动连贯的动态全景场景方面的优越性。我们的项目页面是https://ke-xing.github.io/TiP4GEN/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有生成模型无法创建真正360度沉浸式动态全景场景的问题。随着VR/AR技术快速发展，游戏、娱乐和教育等领域对高质量沉浸式体验需求日益增长，但现有方法要么局限于静态场景，要么只能生成狭窄视角的动态场景，无法提供从任意角度观察的沉浸式体验，限制了虚拟现实应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用两阶段方法：首先生成语义丰富的全景视频，然后重建动态4D场景。设计上借鉴了扩散模型(特别是AnimateDiff的UNet架构)、3D高斯泼溅技术(3DGS)、深度估计方法(DPT和Monst3r)和球形位置编码等现有工作。通过双分支架构结合全景分支(确保全局一致性)和视角分支(增强局部多样性)，并引入双向交叉注意力机制促进信息交换，解决了全景视频生成的数据稀缺和全局一致性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双分支架构结合全局一致性和局部多样性，并利用几何对齐确保空间和时间一致性。整体流程分为两个阶段：1)双分支全景视频生成：全景分支接收全局文本提示生成全景视频，视角分支接收多个本地文本提示生成多个视角视频，通过双向交叉注意力机制融合信息；2)几何对齐重建：利用深度估计进行空间对齐，使用Monst3r进行时间对齐，估计相机姿态初始化场景，最后通过3D高斯泼溅优化重建4D场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)TiP4GEN框架，实现文本到动态全景4D场景生成；2)双分支生成模型，结合全景和视角分支并通过双向交叉注意力机制融合；3)几何对齐重建模型，利用空间和时间对齐确保几何一致性。相比之前工作，本文不仅生成静态3D场景，还创建动态4D全景场景；不仅提供单一全局控制，还支持多局部精细控制；不仅独立处理每帧，还保持时间连贯性；适应球面投影特性而非简单调整传统视频模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TiP4GEN通过创新的双分支生成和几何对齐重建框架，实现了从文本生成具有全局一致性、局部细节多样性和时间连贯性的360度沉浸式动态4D场景，显著提升了虚拟现实环境中场景生成的质量和沉浸感。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement and widespread adoption of VR/AR technologies,there is a growing demand for the creation of high-quality, immersive dynamicscenes. However, existing generation works predominantly concentrate on thecreation of static scenes or narrow perspective-view dynamic scenes, fallingshort of delivering a truly 360-degree immersive experience from any viewpoint.In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamicpanorama scene generation framework that enables fine-grained content controland synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GENintegrates panorama video generation and dynamic scene reconstruction to create360-degree immersive virtual environments. For video generation, we introduce a\textbf{Dual-branch Generation Model} consisting of a panorama branch and aperspective branch, responsible for global and local view generation,respectively. A bidirectional cross-attention mechanism facilitatescomprehensive information exchange between the branches. For scenereconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds usingmetric depth maps and initializing scene cameras with estimated poses, ourmethod ensures geometric consistency and temporal coherence for thereconstructed scenes. Extensive experiments demonstrate the effectiveness ofour proposed designs and the superiority of TiP4GEN in generating visuallycompelling and motion-coherent dynamic panoramic scenes. Our project page is athttps://ke-xing.github.io/TiP4GEN/.</description>
      <author>example@mail.com (Ke Xing, Hanwen Liang, Dejia Xu, Yuyang Yin, Konstantinos N. Plataniotis, Yao Zhao, Yunchao Wei)</author>
      <guid isPermaLink="false">2508.12415v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning</title>
      <link>http://arxiv.org/abs/2508.15471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, BDA Conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为SLM4Offer的生成式AI模型，用于个性化优惠生成，通过对比学习方法微调预训练语言模型，实现了比监督微调基线高17%的优惠接受率。&lt;h4&gt;背景&lt;/h4&gt;个性化营销是提升客户参与度和推动业务增长的关键策略，学术界和行业主要关注推荐系统和个性化广告。研究表明，良好的个性化策略可以将收入提高高达40%。&lt;h4&gt;目的&lt;/h4&gt;开发智能的、数据驱动的个性化优惠生成方法，以提高转化率和客户满意度。&lt;h4&gt;方法&lt;/h4&gt;SLM4Offer是一个基于Google的T5-Small 60M预训练语言模型的生成式AI模型，采用对比学习方法进行微调。模型使用InfoNCE损失函数将客户画像与相关优惠在共享嵌入空间中对齐，并通过对比损失引入的自适应学习行为重塑潜在空间，增强模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟客户行为和优惠接受模式的合成数据集上，SLM4Offer实现了比监督微调基线高17%的优惠接受率，证明了对比目标在个性化营销中的有效性。&lt;h4&gt;结论&lt;/h4&gt;对比学习方法在个性化优惠生成中表现出色，SLM4Offer模型的自适应学习行为能够有效改善模型性能，为个性化营销提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;个性化营销已成为提升客户参与度和推动业务增长的关键策略。学术界和行业的努力主要集中在推荐系统和个性化广告上。然而，个性化营销在提高转化率和改善客户满意度方面具有巨大潜力。先前研究表明，良好的个性化策略可以将收入提高高达40%，这凸显了开发用于优惠生成的智能数据驱动方法的重要性。本研究介绍了SLM4Offer，一个用于个性化优惠生成的生成式AI模型，通过微调预训练的编码器-解码器语言模型（特别是Google的文本到文本转换器T5-Small 60M）开发，采用对比学习方法。SLM4Offer使用InfoNCE（信息噪声对比估计）损失将客户画像与相关优惠在共享嵌入空间中对齐。SLM4Offer的关键创新在于对比损失引入的自适应学习行为，它在训练过程中重塑潜在空间并增强模型的泛化能力。该模型在模拟客户行为和优惠接受模式的合成数据集上进行微调和评估。实验结果表明，与监督微调基线相比，优惠接受率提高了17%，突显了对比目标在推进个性化营销方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized marketing has emerged as a pivotal strategy for enhancingcustomer engagement and driving business growth. Academic and industry effortshave predominantly focused on recommendation systems and personalizedadvertisements. Nonetheless, this facet of personalization holds significantpotential for increasing conversion rates and improving customer satisfaction.Prior studies suggest that well-executed personalization strategies can boostrevenue by up to 40 percent, underscoring the strategic importance ofdeveloping intelligent, data-driven approaches for offer generation. This workintroduces SLM4Offer, a generative AI model for personalized offer generation,developed by fine-tuning a pre-trained encoder-decoder language model,specifically Google's Text-to-Text Transfer Transformer (T5-Small 60M) using acontrastive learning approach. SLM4Offer employs InfoNCE (InformationNoise-Contrastive Estimation) loss to align customer personas with relevantoffers in a shared embedding space. A key innovation in SLM4Offer lies in theadaptive learning behaviour introduced by contrastive loss, which reshapes thelatent space during training and enhances the model's generalizability. Themodel is fine-tuned and evaluated on a synthetic dataset designed to simulatecustomer behaviour and offer acceptance patterns. Experimental resultsdemonstrate a 17 percent improvement in offer acceptance rate over a supervisedfine-tuning baseline, highlighting the effectiveness of contrastive objectivesin advancing personalized marketing.</description>
      <author>example@mail.com (Vedasamhitha Challapalli, Konduru Venkat Sai, Piyush Pratap Singh, Rupesh Prasad, Arvind Maurya, Atul Singh)</author>
      <guid isPermaLink="false">2508.15471v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Long-term User Behaviors with Diffusion-driven Multi-interest Network for CTR Prediction</title>
      <link>http://arxiv.org/abs/2508.15311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DiffuMIN模型，通过扩散驱动的多兴趣网络来建模长期用户行为和探索用户兴趣空间，有效提升了CTR预测性能。&lt;h4&gt;背景&lt;/h4&gt;CTR预测对推荐系统和在线广告至关重要，长期用户行为建模已被证明有益，但大量行为和噪声干扰的复杂性构成挑战。当前双阶段模型常过滤掉重要信息，无法捕捉多样化用户兴趣并构建完整的用户兴趣潜在空间。&lt;h4&gt;目的&lt;/h4&gt;提出DiffuMIN模型来建模长期用户行为并彻底探索用户兴趣空间，解决现有方法在捕捉用户兴趣方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 提出面向目标的多兴趣提取方法，通过正交分解目标获得兴趣通道；2) 建模兴趣通道与用户行为间关系，解耦提取多个用户兴趣；3) 采用由上下文兴趣和兴趣通道引导的扩散模块，锚定用户个性化和目标导向兴趣类型；4) 生成与用户兴趣潜在空间一致的增强兴趣；5) 利用对比学习确保生成兴趣与用户真实偏好一致。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集和一个工业数据集上的离线实验验证了DiffuMIN的优越性；在线A/B测试显示，DiffuMIN使CTR提高了1.52%，CPM提高了1.10%。&lt;h4&gt;结论&lt;/h4&gt;DiffuMIN能有效建模长期用户行为和探索用户兴趣空间，显著提升了CTR预测性能，在工业应用中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为中文，无需翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3705328.3748045&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CTR (Click-Through Rate) prediction, crucial for recommender systems andonline advertising, etc., has been confirmed to benefit from modeling long-termuser behaviors. Nonetheless, the vast number of behaviors and complexity ofnoise interference pose challenges to prediction efficiency and effectiveness.Recent solutions have evolved from single-stage models to two-stage models.However, current two-stage models often filter out significant information,resulting in an inability to capture diverse user interests and build thecomplete latent space of user interests. Inspired by multi-interest andgenerative modeling, we propose DiffuMIN (Diffusion-driven Multi-InterestNetwork) to model long-term user behaviors and thoroughly explore the userinterest space. Specifically, we propose a target-oriented multi-interestextraction method that begins by orthogonally decomposing the target to obtaininterest channels. This is followed by modeling the relationships betweeninterest channels and user behaviors to disentangle and extract multiple userinterests. We then adopt a diffusion module guided by contextual interests andinterest channels, which anchor users' personalized and target-orientedinterest types, enabling the generation of augmented interests that align withthe latent spaces of user interests, thereby further exploring restrictedinterest space. Finally, we leverage contrastive learning to ensure that thegenerated augmented interests align with users' genuine preferences. Extensiveoffline experiments are conducted on two public datasets and one industrialdataset, yielding results that demonstrate the superiority of DiffuMIN.Moreover, DiffuMIN increased CTR by 1.52% and CPM by 1.10% in online A/Btesting. Our source code is available athttps://github.com/laiweijiang/DiffuMIN.</description>
      <author>example@mail.com (Weijiang Lai, Beihong Jin, Yapeng Zhang, Yiyuan Zheng, Rui Zhao, Jian Dong, Jun Lei, Xingxing Wang)</author>
      <guid isPermaLink="false">2508.15311v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding</title>
      <link>http://arxiv.org/abs/2508.15297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP 2025. 22 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DesignCLIP的统一框架，利用CLIP模型改进设计专利分析，解决了传统专利图像无法充分传达视觉上下文和语义信息的问题。&lt;h4&gt;背景&lt;/h4&gt;在设计专利分析领域，传统任务如专利分类和专利图像检索高度依赖图像数据。然而，专利图像（通常包含发明的抽象和结构元素的草图）往往无法传达全面的视觉上下文和语义信息，这可能导致在现有技术搜索评估过程中产生歧义。&lt;h4&gt;目的&lt;/h4&gt;利用最新的视觉-语言模型（如CLIP）开发更可靠、更准确的AI驱动专利分析框架，解决传统专利图像的局限性。&lt;h4&gt;方法&lt;/h4&gt;作者利用CLIP模型开发了名为DesignCLIP的统一框架，用于设计专利应用，并使用了美国设计专利的大规模数据集。为解决专利数据的独特特性，DesignCLIP纳入了类感知分类和对比学习，利用生成的专利图像详细字幕和多视图图像学习。&lt;h4&gt;主要发现&lt;/h4&gt;作者在各种下游任务（包括专利分类和专利检索）中验证了DesignCLIP的有效性。此外，他们探索了多模态专利检索，这可以通过提供更多样化的灵感来源来增强设计的创造力和创新。实验表明，DesignCLIP在所有任务上都一致地优于基线和最新模型。&lt;h4&gt;结论&lt;/h4&gt;多模态方法在推进专利分析方面具有广阔前景，能够提供更可靠、更准确的AI驱动专利分析。&lt;h4&gt;翻译&lt;/h4&gt;在设计专利分析领域，诸如专利分类和专利图像检索等传统任务高度依赖图像数据。然而，专利图像--通常包含发明的抽象和结构元素的草图--往往无法传达全面的视觉上下文和语义信息。这种不足可能导致在现有技术搜索评估过程中产生歧义。视觉-语言模型的最新进展，如CLIP，为更可靠、更准确的AI驱动专利分析提供了有希望的机会。在本工作中，我们利用CLIP模型开发了一个名为DesignCLIP的统一框架，用于设计专利应用，并使用了美国设计专利的大规模数据集。为解决专利数据的独特特性，DesignCLIP纳入了类感知分类和对比学习，利用生成的专利图像详细字幕和多视图图像学习。我们在各种下游任务中验证了DesignCLIP的有效性，包括专利分类和专利检索。此外，我们探索了多模态专利检索，这可以通过提供更多样化的灵感来源来增强设计的创造力和创新。我们的实验表明，DesignCLIP在所有任务上都一致地优于基线和最新模型。我们的研究结果强调了多模态方法在推进专利分析方面的潜力。代码库可在以下网址获取：https://anonymous.4open.science/r/PATENTCLIP-4661/README.md。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of design patent analysis, traditional tasks such as patentclassification and patent image retrieval heavily depend on the image data.However, patent images -- typically consisting of sketches with abstract andstructural elements of an invention -- often fall short in conveyingcomprehensive visual context and semantic information. This inadequacy can leadto ambiguities in evaluation during prior art searches. Recent advancements invision-language models, such as CLIP, offer promising opportunities for morereliable and accurate AI-driven patent analysis. In this work, we leverage CLIPmodels to develop a unified framework DesignCLIP for design patent applicationswith a large-scale dataset of U.S. design patents. To address the uniquecharacteristics of patent data, DesignCLIP incorporates class-awareclassification and contrastive learning, utilizing generated detailed captionsfor patent images and multi-views image learning. We validate the effectivenessof DesignCLIP across various downstream tasks, including patent classificationand patent retrieval. Additionally, we explore multimodal patent retrieval,which provides the potential to enhance creativity and innovation in design byoffering more diverse sources of inspiration. Our experiments show thatDesignCLIP consistently outperforms baseline and SOTA models in the patentdomain on all tasks. Our findings underscore the promise of multimodalapproaches in advancing patent analysis. The codebase is available here:https://anonymous.4open.science/r/PATENTCLIP-4661/README.md.</description>
      <author>example@mail.com (Zhu Wang, Homaira Huda Shomee, Sathya N. Ravi, Sourav Medya)</author>
      <guid isPermaLink="false">2508.15297v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Center-Oriented Prototype Contrastive Clustering</title>
      <link>http://arxiv.org/abs/2508.15231v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种面向中心的原型对比聚类框架，通过软原型对比模块和双一致性学习模块解决对比学习在聚类任务中的类别冲突问题，实验证明该方法有效。&lt;h4&gt;背景&lt;/h4&gt;对比学习因其判别性表示在聚类任务中被广泛使用，但类别间的冲突问题难以有效解决。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中硬原型计算与真实聚类中心之间的偏差问题，提高聚类性能。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含软原型对比模块和双一致性学习模块的中心导向原型对比聚类框架。软原型对比模块使用样本属于聚类中心的概率作为权重计算原型，避免类间冲突并减少原型漂移；双一致性学习模块对齐同一样本的不同变换和不同样本的邻域，确保特征具有变换不变的语义信息和紧凑的簇内分布。&lt;h4&gt;主要发现&lt;/h4&gt;在五个数据集上的大量实验表明，所提出的方法与当前最先进的方法相比是有效的。&lt;h4&gt;结论&lt;/h4&gt;提出的中心导向原型对比聚类框架能有效解决对比学习在聚类任务中的类别冲突问题，提高聚类性能。&lt;h4&gt;翻译&lt;/h4&gt;对比学习因其判别性表示在聚类任务中被广泛使用。然而，类别间的冲突问题难以有效解决。现有方法尝试通过原型对比解决此问题，但硬原型的计算与真实聚类中心之间存在偏差。为解决这一问题，我们提出了一种面向中心的原型对比聚类框架，包含软原型对比模块和双一致性学习模块。简而言之，软原型对比模块使用样本属于聚类中心的概率作为权重计算每个类别的原型，同时避免类间冲突并减少原型漂移。双一致性学习模块分别对齐同一样本的不同变换和不同样本的邻域，确保特征具有变换不变的语义信息和紧凑的簇内分布，同时为原型计算提供可靠保证。在五个数据集上的大量实验表明，与当前最先进的方法相比，所提出的方法是有效的。我们的代码已发布在https://github.com/LouisDong95/CPCC。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning is widely used in clustering tasks due to itsdiscriminative representation. However, the conflict problem between classes isdifficult to solve effectively. Existing methods try to solve this problemthrough prototype contrast, but there is a deviation between the calculation ofhard prototypes and the true cluster center. To address this problem, wepropose a center-oriented prototype contrastive clustering framework, whichconsists of a soft prototype contrastive module and a dual consistency learningmodule. In short, the soft prototype contrastive module uses the probabilitythat the sample belongs to the cluster center as a weight to calculate theprototype of each category, while avoiding inter-class conflicts and reducingprototype drift. The dual consistency learning module aligns differenttransformations of the same sample and the neighborhoods of different samplesrespectively, ensuring that the features have transformation-invariant semanticinformation and compact intra-cluster distribution, while providing reliableguarantees for the calculation of prototypes. Extensive experiments on fivedatasets show that the proposed method is effective compared to the SOTA. Ourcode is published on https://github.com/LouisDong95/CPCC.</description>
      <author>example@mail.com (Shihao Dong, Xiaotong Zhou, Yuhui Zheng, Huiying Xu, Xinzhong Zhu)</author>
      <guid isPermaLink="false">2508.15231v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment</title>
      <link>http://arxiv.org/abs/2508.15130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HiRQA（分层排序与质量对齐）的自监督无参考图像质量评估框架，通过结合排序和对比学习实现分层质量感知嵌入，克服了传统方法对数据集偏见和主观标签的依赖，在各种真实图像退化条件下展现出优异的泛化性能。&lt;h4&gt;背景&lt;/h4&gt;尽管无参考图像质量评估(NR-IQA)取得了显著进展，但数据集偏见和对主观标签的依赖仍然阻碍了其泛化性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种自监督、无意见感知的框架HiRQA，仅使用输入图像预测质量分数，并在合成失真训练后能有效泛化到真实退化场景。&lt;h4&gt;方法&lt;/h4&gt;HiRQA结合了高阶排序损失（通过对失真对的关系排序监督质量预测）、嵌入距离损失（强制特征距离与感知差异一致）以及训练时由结构化文本提示引导的对比对齐损失。此外，还提出了轻量级变体HiRQA-S，推理时间仅需3.5毫秒每图像。&lt;h4&gt;主要发现&lt;/h4&gt;HiRQA仅在合成失真上训练，但能有效泛化到真实退化场景，如镜头光晕、雾霾、运动模糊和低光条件等。大量实验验证了其达到最先进(SOTA)的性能、强大的泛化能力和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;HiRQA通过创新的分层排序与质量对齐技术，成功克服了传统NR-IQA方法的局限性，在保持高性能的同时实现了优秀的泛化能力和实时应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;尽管无参考图像质量评估(NR-IQA)取得了显著进展，但数据集偏见和对主观标签的依赖仍然阻碍了其泛化性能。我们提出HiRQA（分层排序与质量对齐），一个自监督的、无意见感知的框架，通过排序和对比学习的组合提供分层质量感知嵌入。与依赖原始参考或辅助模态的前期方法不同，HiRQA仅使用输入图像预测质量分数。我们引入了一种新颖的高阶排序损失，通过对失真对的关系排序监督质量预测，以及嵌入距离损失强制特征距离与感知差异之间的一致性。由结构化文本提示引导的训练时对比对齐损失，进一步增强了学习的表示。仅在合成失真上训练的HiRQA能有效泛化到真实退化，如在镜头光晕、雾霾、运动模糊和低光条件等各种失真上的评估所证明。为实时部署，我们引入了HiRQA-S，一个轻量级变体，每张图像推理时间仅为3.5毫秒。在合成和真实基准上的大量实验验证了HiRQA的最先进(SOTA)性能、强大的泛化能力和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant progress in no-reference image quality assessment(NR-IQA), dataset biases and reliance on subjective labels continue to hindertheir generalization performance. We propose HiRQA, Hierarchical Ranking andQuality Alignment), a self-supervised, opinion-unaware framework that offers ahierarchical, quality-aware embedding through a combination of ranking andcontrastive learning. Unlike prior approaches that depend on pristinereferences or auxiliary modalities at inference time, HiRQA predicts qualityscores using only the input image. We introduce a novel higher-order rankingloss that supervises quality predictions through relational ordering acrossdistortion pairs, along with an embedding distance loss that enforcesconsistency between feature distances and perceptual differences. Atraining-time contrastive alignment loss, guided by structured textual prompts,further enhances the learned representation. Trained only on syntheticdistortions, HiRQA generalizes effectively to authentic degradations, asdemonstrated through evaluation on various distortions such as lens flare,haze, motion blur, and low-light conditions. For real-time deployment, weintroduce \textbf{HiRQA-S}, a lightweight variant with an inference time ofonly 3.5 ms per image. Extensive experiments across synthetic and authenticbenchmarks validate HiRQA's state-of-the-art (SOTA) performance, stronggeneralization ability, and scalability.</description>
      <author>example@mail.com (Vaishnav Ramesh, Haining Wang, Md Jahidul Islam)</author>
      <guid isPermaLink="false">2508.15130v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2508.14278v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GALA的新框架，用于基于3D高斯溅射的开词汇表3D场景理解，通过自监督对比学习和交叉注意力模块实现高效的3D表示和查询。&lt;h4&gt;背景&lt;/h4&gt;3D场景重建和理解越来越受欢迎，但现有方法难以从2D图像中捕捉细粒度的、语言感知的3D表示。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为GALA的框架，用于基于3D高斯溅射的开词汇表3D场景理解。&lt;h4&gt;方法&lt;/h4&gt;GALA通过自监督对比学习蒸馏场景特定的3D实例特征场，并引入一个带有两个可学习码本的交叉注意力模块来编码视图无关的语义嵌入，确保实例内特征相似性并支持2D和3D开词汇表查询。&lt;h4&gt;主要发现&lt;/h4&gt;GALA的设计减少了内存消耗，通过避免每个高斯体的高维特征学习，同时在2D和3D开词汇表任务上表现出色。&lt;h4&gt;结论&lt;/h4&gt;在真实世界数据集上的大量实验证明了GALA在2D和3D开词汇表任务上的卓越性能。&lt;h4&gt;翻译&lt;/h4&gt;3D场景重建和理解日益流行，然而现有方法仍然难以从2D图像中捕捉细粒度的、语言感知的3D表示。在本文中，我们提出了GALA，一个基于3D高斯溅射的开词汇表3D场景理解的新框架。GALA通过自监督对比学习蒸馏场景特定的3D实例特征场。为了扩展到通用的语言特征场，我们引入了GALA的核心贡献——一个带有两个可学习码本的交叉注意力模块，这些码本编码视图无关的语义嵌入。这种设计不仅确保了实例内特征的相似性，还支持无缝的2D和3D开词汇表查询。它通过避免每个高斯体的高维特征学习来减少内存消耗。在真实世界数据集上的大量实验证明了GALA在2D和3D开词汇表任务上的卓越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景中的开放词汇理解和语义一致性问题。这个问题在现实中非常重要，因为它使机器人能够通过自然语言与人类交互，理解环境中的对象；帮助自动驾驶系统识别复杂环境中的各种物体；增强AR/VR应用的用户体验；并为机器人导航、物体操作等任务提供精确的语义分割。现有方法存在信息损失、实例一致性差、2D和3D性能不平衡以及计算效率低等问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先批判性分析了现有方法的局限性，如NeRF效率低、3DGS方法在语义处理上不足、压缩高维特征导致信息损失、聚类方法可能导致实例分割错误等。作者提出应关注实例级别的语义一致性，为每个实例学习共享的语言嵌入而非每个高斯独立存储特征。方法借鉴了Scaffold-GS的场景重建、SAM的实例分割、CLIP的语言对齐以及对比学习等技术，同时创新性地引入了双代码本设计和引导注意力机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过实例级别语义一致性、双代码本设计和引导注意力机制实现开放词汇的3D场景理解。整体流程分为两个阶段：阶段1使用Scaffold-GS重建场景几何结构，并通过自监督对比学习提取场景特定的实例特征场；阶段2使用引导注意力模块将场景特定特征通过双代码本映射到通用语言特征，并通过注意力加权熵损失促进特征和代码本间的一对一映射。推理时支持2D和3D的开放词汇查询，且仅通过2D监督训练即可获得3D能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 实例级别语义一致性确保同一对象在不同视角和位置上语义一致；2) 双代码本设计实现场景特定特征和通用语言特征的对齐；3) 引导注意力机制支持无缝2D和3D开放词汇查询；4) 注意力加权熵损失促进特征和代码本间的一对一映射；5) 通过2D监督实现3D能力。相比之前工作，GALA避免了信息损失和聚类错误，同时支持高质量的2D和3D语义理解，不依赖复杂的MLP或KNN初始化，实现了更好的语义一致性和计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GALA通过引入双代码本和引导注意力机制，实现了高效且一致的开放词汇3D场景理解，同时支持高质量的2D和3D语义查询。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D scene reconstruction and understanding have gained increasing popularity,yet existing methods still struggle to capture fine-grained, language-aware 3Drepresentations from 2D images. In this paper, we present GALA, a novelframework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting(3DGS). GALA distills a scene-specific 3D instance feature field viaself-supervised contrastive learning. To extend to generalized language featurefields, we introduce the core contribution of GALA, a cross-attention modulewith two learnable codebooks that encode view-independent semantic embeddings.This design not only ensures intra-instance feature similarity but alsosupports seamless 2D and 3D open-vocabulary queries. It reduces memoryconsumption by avoiding per-Gaussian high-dimensional feature learning.Extensive experiments on real-world datasets demonstrate GALA's remarkableopen-vocabulary performance on both 2D and 3D.</description>
      <author>example@mail.com (Elena Alegret, Kunyi Li, Sen Wang, Siyun Liang, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari)</author>
      <guid isPermaLink="false">2508.14278v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed ML Exploration of Structure-Transport Relationships in Hard Carbon</title>
      <link>http://arxiv.org/abs/2508.14849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种数据驱动框架，结合机器学习原子间势能和分子动力学模拟，系统研究了硬碳阳极中钠离子的扩散行为，揭示了微观结构与离子传输之间的关系，为高性能钠离子电池阳极设计提供了新见解。&lt;h4&gt;背景&lt;/h4&gt;钠离子电池是大规模储能中锂离子系统的经济有效的可持续替代方案。硬碳阳极具有高容量，但表现出复杂且难以理解的离子传输行为，特别是局部微观结构与钠流动性之间的关系尚未明确，阻碍了性能的合理优化。&lt;h4&gt;目的&lt;/h4&gt;引入一个数据驱动框架，系统研究跨越广泛碳密度和钠负载的钠扩散行为，识别控制离子传输的微观因素，建立定量结构-传输关系。&lt;h4&gt;方法&lt;/h4&gt;结合机器学习的原子间势能与分子动力学模拟，计算每个离子的结构描述符，使用无监督学习发现扩散模式，通过监督分析识别决定流动性的主要因素，并进行相关性映射将传输机制与加工变量联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;无监督学习揭示了跳跃、聚集和空位捕获等不同的扩散模式；监督分析表明曲折度和NaNa配位是流动性的主要决定因素；相关性映射将传输机制与体积密度和钠含量等加工变量联系起来；基于物理学的方法建立了定量结构-传输关系，捕捉了无序碳的异质性。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了钠离子动力学的机理见解，并为下一代电池系统中高性能硬碳阳极的工程设计提供了可行设计原则。&lt;h4&gt;翻译&lt;/h4&gt;钠离子电池是大规模储能中锂离子系统的一种经济有效的可持续替代方案。硬碳阳极由无序石墨和非晶域组成，具有高容量，但表现出复杂且难以理解的离子传输行为。特别是，局部微观结构与钠流动性之间的关系仍未解决，阻碍了性能的合理优化。在这里，我们引入了一个数据驱动框架，结合机器学习的原子间势能与分子动力学模拟，系统研究跨越广泛碳密度和钠负载的钠扩散。通过计算每个离子的结构描述符，我们识别了控制离子传输的微观因素。无监督学习发现了不同的扩散模式，包括跳跃、聚集和空位捕获，而监督分析强调了曲折度和NaNa配位作为流动性的主要决定因素。相关性映射进一步将这些传输机制与加工变量（如体积密度和钠含量）联系起来。这种基于物理学的方法建立了定量结构-传输关系，捕捉了无序碳的异质性。我们的研究结果提供了钠离子动力学的机理见解，并为下一代电池系统中高性能HC阳极的工程设计提供了可行设计原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sodium-ion batteries are a cost-effective and sustainable alternative tolithium-ion systems for large-scale energy storage. Hard carbon (HC) anodes,composed of disordered graphitic and amorphous domains, offer high capacity butexhibit complex, poorly understood ion transport behavior. In particular, therelationship between local microstructure and sodium mobility remainsunresolved, hindering rational performance optimization. Here, we introduce adata-driven framework that combines machine-learned interatomic potentials withmolecular dynamics simulations to systematically investigate sodium diffusionacross a broad range of carbon densities and sodium loadings. By computingper-ion structural descriptors, we identify the microscopic factors that governion transport. Unsupervised learning uncovers distinct diffusion modes,including hopping, clustering, and void trapping, while supervised analysishighlights tortuosity and NaNa coordination as primary determinants ofmobility. Correlation mapping further connects these transport regimes toprocessing variables such as bulk density and sodium content. Thisphysics-informed approach establishes quantitative structure-transportrelationships that capture the heterogeneity of disordered carbon. Our findingsdeliver mechanistic insights into sodium-ion dynamics and provide actionabledesign principles for engineering high-performance HC anodes in next-generationbattery systems.</description>
      <author>example@mail.com (Nikhil Rampal, Stephen E. Weitzner, Fredrick Omenya, Marissa Wood, David M. Reed, Xiaolin Li, Jonathan R. I. Lee, Liwen F. Wan)</author>
      <guid isPermaLink="false">2508.14849v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
  <item>
      <title>A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans</title>
      <link>http://arxiv.org/abs/2508.14151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了深度学习架构与可解释AI技术在膝盖MRI扫描中自动化检测感兴趣区域(ROI)的应用效果，发现ResNet50在分类和ROI识别方面表现最佳，而CNN-based迁移学习是最有效的方法。&lt;h4&gt;背景&lt;/h4&gt;MRI是评估膝盖损伤的重要诊断工具，但手动解释MRI切片既耗时又容易产生观察者间差异。&lt;h4&gt;目的&lt;/h4&gt;对各种深度学习架构与可解释AI(xAI)技术进行系统评估，用于膝盖MRI扫描中的感兴趣区域(ROI)自动化检测。&lt;h4&gt;方法&lt;/h4&gt;研究监督和自监督方法，包括ResNet50、InceptionV3、Vision Transformers (ViT)和多种U-Net变体，并增加多层感知器(MLP)分类器；整合Grad-CAM和Saliency Maps等可解释AI方法；使用AUC评估分类性能，PSNR/SSIM评估重建质量，并进行定性ROI可视化。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet50在分类和ROI识别方面表现 consistently 出色，优于基于transformer的模型；混合U-Net + MLP方法在利用重建和可解释性的空间特征方面显示出潜力，但分类性能较低；Grad-CAM在所有架构中始终提供最具临床意义的解释。&lt;h4&gt;结论&lt;/h4&gt;基于CNN的迁移学习是该数据集最有效的方法，未来更大规模的预训练可能更好地释放transformer模型的潜力。&lt;h4&gt;翻译&lt;/h4&gt;磁共振成像(MRI)是评估膝盖损伤的重要诊断工具。然而，手动解释MRI切片仍然耗时且容易产生观察者间差异。本研究对各种深度学习架构与可解释AI(xAI)技术进行了系统评估，用于膝盖MRI扫描中的感兴趣区域(ROI)自动化检测。我们研究了监督和自监督方法，包括ResNet50、InceptionV3、Vision Transformers (ViT)和多种增加多层感知器(MLP)分类器的U-Net变体。为了提高可解释性和临床相关性，我们整合了Grad-CAM和Saliency Maps等可解释AI方法。模型性能使用AUC评估分类质量，使用PSNR/SSIM评估重建质量，并结合定性ROI可视化进行评估。我们的结果表明，ResNet50在分类和ROI识别方面始终表现出色，在MRNet数据集约束下优于基于transformer的模型。虽然混合U-Net + MLP方法在利用重建和可解释性的空间特征方面显示出潜力，但其分类性能仍然较低。Grad-CAM在所有架构中始终提供最具临床意义的解释。总体而言，基于CNN的迁移学习成为该数据集最有效的方法，而未来更大规模的预训练可能更好地释放transformer模型的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnetic Resonance Imaging (MRI) is an essential diagnostic tool forassessing knee injuries. However, manual interpretation of MRI slices remainstime-consuming and prone to inter-observer variability. This study presents asystematic evaluation of various deep learning architectures combined withexplainable AI (xAI) techniques for automated region of interest (ROI)detection in knee MRI scans. We investigate both supervised and self-supervisedapproaches, including ResNet50, InceptionV3, Vision Transformers (ViT), andmultiple U-Net variants augmented with multi-layer perceptron (MLP)classifiers. To enhance interpretability and clinical relevance, we integratexAI methods such as Grad-CAM and Saliency Maps. Model performance is assessedusing AUC for classification and PSNR/SSIM for reconstruction quality, alongwith qualitative ROI visualizations. Our results demonstrate that ResNet50consistently excels in classification and ROI identification, outperformingtransformer-based models under the constraints of the MRNet dataset. Whilehybrid U-Net + MLP approaches show potential for leveraging spatial features inreconstruction and interpretability, their classification performance remainslower. Grad-CAM consistently provided the most clinically meaningfulexplanations across architectures. Overall, CNN-based transfer learning emergesas the most effective approach for this dataset, while future work withlarger-scale pretraining may better unlock the potential of transformer models.</description>
      <author>example@mail.com (Justin Yiu, Kushank Arora, Daniel Steinberg, Rohit Ghiya)</author>
      <guid isPermaLink="false">2508.14151v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Learn the Macroscopic Fundamental Diagram using Physics-Informed and meta Machine Learning techniques</title>
      <link>http://arxiv.org/abs/2508.14137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种利用元学习框架来估计宏观基本图(MFD)的方法，解决了实际应用中检测器数量有限的问题。&lt;h4&gt;背景&lt;/h4&gt;宏观基本图是描述交通动态的常用工具，但估计给定网络的MFD需要大量环形检测器，这在实践中往往不可行。&lt;h4&gt;目的&lt;/h4&gt;开发一种元学习框架，以缓解数据稀缺挑战，使模型能够在检测器数量有限的情况下准确估计MFD。&lt;h4&gt;方法&lt;/h4&gt;提出一个即时的多任务物理信息神经网络，利用多个城市的数据进行训练，并应用于具有不同检测器比例和拓扑结构的其他城市。&lt;h4&gt;主要发现&lt;/h4&gt;元学习框架在流量预测方面的平均均方误差改进在约17500到36000之间，成功跨越多样化城市环境，提高了数据有限城市的性能。&lt;h4&gt;结论&lt;/h4&gt;元学习框架在使用有限数量检测器时具有潜力，其性能优于传统迁移学习方法，并证明了其可迁移性。&lt;h4&gt;翻译&lt;/h4&gt;宏观基本图是一种流行的工具，用于描述聚合方式的交通动态，应用范围从交通控制到事件分析。然而，估计给定网络的MFD需要大量的环形检测器，这在实践中并不总是可用。本文提出了一种利用元学习的框架，元学习是机器学习的一个子类别，它训练模型能够独立理解和适应新任务，以缓解数据稀缺的挑战。所开发的模型通过利用多个城市的数据进行训练和测试，并利用它来建模具有不同检测器比例和拓扑结构的其他城市的MFD。提出的元学习框架应用于一个即时的多任务物理信息神经网络，专门设计用于估计MFD。结果显示，流量预测的平均均方误差改进在约17500到36000之间（取决于测试的环形检测器子集）。元学习框架因此成功地跨越了多样化的城市环境，并改进了数据有限城市的性能，展示了在使用有限数量检测器时使用元学习的潜力。最后，提出的框架与传统迁移学习方法进行了比较，并与文献中的非参数模型FitFun进行了测试，以证明其可迁移性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Macroscopic Fundamental Diagram is a popular tool used to describetraffic dynamics in an aggregated way, with applications ranging from trafficcontrol to incident analysis. However, estimating the MFD for a given networkrequires large numbers of loop detectors, which is not always available inpractice. This article proposes a framework harnessing meta-learning, asubcategory of machine learning that trains models to understand and adapt tonew tasks on their own, to alleviate the data scarcity challenge. The developedmodel is trained and tested by leveraging data from multiple cities andexploiting it to model the MFD of other cities with different shares ofdetectors and topological structures. The proposed meta-learning framework isapplied to an ad-hoc Multi-Task Physics-Informed Neural Network, specificallydesigned to estimate the MFD. Results show an average MSE improvement in flowprediction ranging between ~ 17500 and 36000 (depending on the subset of loopdetectors tested). The meta-learning framework thus successfully generalizesacross diverse urban settings and improves performance on cities with limiteddata, demonstrating the potential of using meta-learning when a limited numberof detectors is available. Finally, the proposed framework is validated againsttraditional transfer learning approaches and tested with FitFun, anon-parametric model from the literature, to prove its transferability.</description>
      <author>example@mail.com (Amalie Roark, Serio Agriesti, Francisco Camara Pereira, Guido Cantelmo)</author>
      <guid isPermaLink="false">2508.14137v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Is Transfer Learning Necessary for Violin Transcription?</title>
      <link>http://arxiv.org/abs/2508.13516v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了从小规模数据集训练小提琴自动音乐转录模型的可行性，发现无需依赖钢琴预训练模型也能获得良好性能。&lt;h4&gt;背景&lt;/h4&gt;钢琴等乐器的自动音乐转录已取得显著进展，主要得益于大规模高质量数据集；而小提琴AMT研究不足，因标注数据有限。常见方法是微调预训练模型，但在音色和发音差异存在时，迁移效果不明确。&lt;h4&gt;目的&lt;/h4&gt;研究从中等规模小提琴数据集从头开始训练，是否能达到微调钢琴预训练模型的性能。&lt;h4&gt;方法&lt;/h4&gt;采用未经修改的钢琴转录架构，在包含约30小时对齐小提琴录音的MOSA数据集上进行训练，并在URMP和Bach10数据集上进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;从头开始训练的模型与微调模型相比，实现了具有竞争力甚至更优的性能。&lt;h4&gt;结论&lt;/h4&gt;强大的小提琴AMT不需要依赖预训练的钢琴表示，突显了乐器特定数据收集和数据增强策略的重要性。&lt;h4&gt;翻译&lt;/h4&gt;自动音乐转录（AMT）在钢琴等乐器上取得了显著进展，这主要归功于大规模高质量数据集的可用性。相比之下，小提琴AMT由于标注数据有限而研究不足。一种常见方法是微调预训练模型用于其他下游任务，但在存在音色和发音差异的情况下，这种迁移的有效性尚不清楚。在这项工作中，我们研究从中等规模小提琴数据集从头开始训练是否能匹配微调钢琴预训练模型的性能。我们采用未经修改的钢琴转录架构，并在包含约30小时对齐小提琴录音的MOSA数据集上进行训练。我们在URMP和Bach10上的实验表明，从头开始训练的模型与微调模型相比实现了具有竞争力甚至更优的性能。这些发现表明，强大的小提琴AMT不需要依赖预训练的钢琴表示，突显了乐器特定数据收集和数据增强策略的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic music transcription (AMT) has achieved remarkable progress forinstruments such as the piano, largely due to the availability of large-scale,high-quality datasets. In contrast, violin AMT remains underexplored due tolimited annotated data. A common approach is to fine-tune pretrained models forother downstream tasks, but the effectiveness of such transfer remains unclearin the presence of timbral and articulatory differences. In this work, weinvestigate whether training from scratch on a medium-scale violin dataset canmatch the performance of fine-tuned piano-pretrained models. We adopt a pianotranscription architecture without modification and train it on the MOSAdataset, which contains about 30 hours of aligned violin recordings. Ourexperiments on URMP and Bach10 show that models trained from scratch achievedcompetitive or even superior performance compared to fine-tuned counterparts.These findings suggest that strong violin AMT is possible without relying onpretrained piano representations, highlighting the importance ofinstrument-specific data collection and augmentation strategies.</description>
      <author>example@mail.com (Yueh-Po Peng, Ting-Kang Wang, Li Su, Vincent K. M. Cheung)</author>
      <guid isPermaLink="false">2508.13516v2</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.14088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出CoBAD模型，用于检测人类移动中的集体异常行为，在AUCROC和AUCPR指标上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;检测人类移动异常对公共安全和城市规划至关重要。传统方法主要关注个人移动模式，而集体异常检测（如孩子独自在家而父母不在）仍是一个未被充分探索的挑战，需要建模个体间的时空依赖关系。&lt;h4&gt;目的&lt;/h4&gt;开发CoBAD模型来捕获人类移动中的集体行为，以解决集体异常检测这一研究空白。&lt;h4&gt;方法&lt;/h4&gt;将问题表述为在集体事件序列上的无监督学习，使用共现事件图；采用两级注意力机制建模个人移动模式和个体间交互；通过掩码事件和链接重建任务在大规模数据上预训练；可检测意外的共现异常和缺失异常两种类型。&lt;h4&gt;主要发现&lt;/h4&gt;在大型移动数据集上的实验表明，CoBAD在AUCROC上比现有方法提升13%-18%，在AUCPR上提升19%-70%。&lt;h4&gt;结论&lt;/h4&gt;CoBAD模型在集体异常检测方面表现优异，源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;检测人类移动中的异常对公共安全和城市规划等应用至关重要。虽然传统异常检测方法主要关注个人移动模式（例如，孩子晚上应该待在家里），但集体异常检测旨在识别个体间集体移动行为中的异常（例如，孩子独自在家而父母在其他地方），仍然是一个未被充分探索的挑战。与个人异常不同，集体异常需要建模个体之间的时空依赖关系，引入了额外的复杂性。为解决这一研究空白，我们提出了CoBAD，一个设计用于捕获人类移动异常检测中集体行为的新模型。我们首先将问题表述为在具有共现事件图的集体事件序列上的无监督学习，其中集体事件序列代表相关个体的事件序列。然后，CoBAD采用两级注意力机制来建模个人移动模式和多个个体之间的交互。通过掩码事件和链接重建任务在大规模集体行为数据上进行预训练，CoBAD能够检测两种类型的集体异常：意外的共现异常和缺失异常，后者在先前工作中很大程度上被忽视。在大型移动数据集上的广泛实验表明，CoBAD显著优于现有的异常检测基线，在AUCROC上实现了13%-18%的提升，在AUCPR上实现了19%-70%的提升。所有源代码可在https://github.com/wenhaomin/CoBAD获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting anomalies in human mobility is essential for applications such aspublic safety and urban planning. While traditional anomaly detection methodsprimarily focus on individual movement patterns (e.g., a child should stay athome at night), collective anomaly detection aims to identify irregularities incollective mobility behaviors across individuals (e.g., a child is at homealone while the parents are elsewhere) and remains an underexplored challenge.Unlike individual anomalies, collective anomalies require modelingspatiotemporal dependencies between individuals, introducing additionalcomplexity. To address this gap, we propose CoBAD, a novel model designed tocapture Collective Behaviors for human mobility Anomaly Detection. We firstformulate the problem as unsupervised learning over Collective Event Sequences(CES) with a co-occurrence event graph, where CES represents the eventsequences of related individuals. CoBAD then employs a two-stage attentionmechanism to model both the individual mobility patterns and the interactionsacross multiple individuals. Pre-trained on large-scale collective behaviordata through masked event and link reconstruction tasks, CoBAD is able todetect two types of collective anomalies: unexpected co-occurrence anomaliesand absence anomalies, the latter of which has been largely overlooked in priorwork. Extensive experiments on large-scale mobility datasets demonstrate thatCoBAD significantly outperforms existing anomaly detection baselines, achievingan improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code isavailable at https://github.com/wenhaomin/CoBAD.</description>
      <author>example@mail.com (Haomin Wen, Shurui Cao, Leman Akoglu)</author>
      <guid isPermaLink="false">2508.14088v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Pulled fronts are not (just) pulled</title>
      <link>http://arxiv.org/abs/2508.14864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18p&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了前沿传播进入不稳定状态的动力学行为，指出线性化预测的前沿传播速度与实际一致，但前沿后的动力学行为比传统'被拉前沿'理论所描述的更为复杂。&lt;h4&gt;背景&lt;/h4&gt;前沿传播进入不稳定状态通常由线性化决定，传播速度与在不稳定状态下线性化方程的预测一致，前沿表现为以线性扩散速度传播的高斯尾部。&lt;h4&gt;目的&lt;/h4&gt;描述一类例子，展示主要效应（高斯尾部传播）并不能完全描述前沿后的动力学行为。&lt;h4&gt;方法&lt;/h4&gt;通过分析前沿传播的动力学模型，研究线性化预测与实际入侵过程之间的差异。&lt;h4&gt;主要发现&lt;/h4&gt;前沿行为最多预测两种可能的入侵情景（与高斯尾部的正负振幅相关），但实际存在三个或更多具有不同状态的前入侵过程，留下的状态不仅仅由前沿决定。&lt;h4&gt;结论&lt;/h4&gt;入侵过程留下的状态不 solely 由前沿决定，也不只是被高斯尾部拉动，表明'被拉前沿'理论不能完全描述复杂的动力学行为。&lt;h4&gt;翻译&lt;/h4&gt;前沿传播进入不稳定状态通常由线性化决定，即传播速度与在不稳定状态下线性化方程的预测一致。前沿行为表现为以线性扩散速度传播的高斯尾部。跟随这个前沿的通常被称为'被拉前沿'，暗示它们被这个前沿的高斯尾部'拉动'。本文描述了一类例子，展示了这些主要效应并不能完全描述前沿后的动力学。实际上，前沿行为最多预测两种可能的入侵情景，与高斯尾部的正负振幅相关，但我们的例子展示了三个或更多具有不同状态的前入侵前沿。因此，入侵过程留下的状态不仅仅由前沿决定，也不只是被高斯尾部拉动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Front propagation into unstable states is often determined by thelinearization, that is, propagation speeds agree with predictions from thelinearized equation at the unstable state. The leading edge behavior is then aGaussian tail propagating with the linear spreading speed. Fronts followingthis leading edge are commonly referred to as pulled fronts, alluding to theidea that they are ``pulled'' by this leading-edge Gaussian tail. We describehere a class of examples that exhibits how these leading-order effects do notcompletely describe the dynamics in the wake of the front. In fact, leadingedge behavior predicts at most two possible invasion scenarios, associated withpositive and negative amplitudes of the Gaussian tail, but our examples exhibitthree or more invasion fronts with different states in the wake. The resultinginvasion process therefore leaves behind a state that is not solely determinedby the leading edge, and thus not just pulled by the Gaussian tail.</description>
      <author>example@mail.com (Montie Avery, Matt Holzer, Arnd Scheel)</author>
      <guid isPermaLink="false">2508.14864v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism</title>
      <link>http://arxiv.org/abs/2508.14523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Great GATsBi的自行车轨迹预测框架，结合了物理模型和社会模型，有效预测自行车运动轨迹并超越了现有技术水平。&lt;h4&gt;背景&lt;/h4&gt;准确的交通参与者运动预测对高级驾驶员辅助系统和自动驾驶等应用日益重要，对道路安全尤为关键。然而，尽管大多数交通事故死亡涉及自行车，自行车运动预测得到的关注较少，之前的研究主要集中在行人和机动车辆上。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门的自行车轨迹预测框架，解决自行车运动预测被忽视的问题，提高道路安全性。&lt;h4&gt;方法&lt;/h4&gt;提出Great GATsBi，一个基于领域知识的混合多模态轨迹预测框架，结合了基于物理的建模（受机动车辆启发）和基于社会的建模（受行人运动启发），使用图注意力网络建模社会互动，并考虑自行车邻域的历史和预期未来轨迹数据。&lt;h4&gt;主要发现&lt;/h4&gt;物理模型（短期预测效果好）和社会模型（长期预测效果好）的集成超越了最先进的性能，有效预测自行车轨迹并建模社会互动。&lt;h4&gt;结论&lt;/h4&gt;通过受控的集体骑行实验验证了该框架在预测自行车轨迹和建模与其他道路使用者社会互动方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;准确的交通参与者运动预测正被越来越多的应用需求，从高级驾驶员辅助系统到自动驾驶，对道路安全尤为重要。尽管大多数交通事故死亡涉及自行车，但它们得到的关注很少，因为之前的工作主要集中在行人和机动车辆上。在这项工作中，我们提出了Great GATsBi，一个基于领域知识的混合多模态自行车轨迹预测框架。该模型结合了基于物理的建模（受机动车辆启发）和基于社会的建模（受行人运动启发），明确考虑了自行车运动的双重特性。社会互动使用图注意力网络建模，包括自行车邻域的衰减历史轨迹数据和预期未来轨迹数据，遵循心理学和社会研究的最新见解。结果表明，提出的物理模型集成（在短期预测中表现良好）和社会模型集成（在长期预测中表现良好）超越了最先进的性能。我们还进行了一项受控的集体骑行实验，展示了该框架在预测自行车轨迹和建模与其他道路使用者的社会互动方面的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of road user movement is increasingly required by manyapplications ranging from advanced driver assistance systems to autonomousdriving, and especially crucial for road safety. Even though most trafficaccident fatalities account to bicycles, they have received little attention,as previous work focused mainly on pedestrians and motorized vehicles. In thiswork, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodaltrajectory prediction framework for bicycles. The model incorporates bothphysics-based modeling (inspired by motorized vehicles) and social-basedmodeling (inspired by pedestrian movements) to explicitly account for the dualnature of bicycle movement. The social interactions are modeled with a graphattention network, and include decayed historical, but also anticipated, futuretrajectory data of a bicycles neighborhood, following recent insights frompsychological and social studies. The results indicate that the proposedensemble of physics models -- performing well in the short-term predictions --and social models -- performing well in the long-term predictions -- exceedsstate-of-the-art performance. We also conducted a controlled mass-cyclingexperiment to demonstrate the framework's performance when forecasting bicycletrajectories and modeling social interactions with road users.</description>
      <author>example@mail.com (Kevin Riehl, Shaimaa K. El-Baklish, Anastasios Kouvelas, Michail A. Makridis)</author>
      <guid isPermaLink="false">2508.14523v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds</title>
      <link>http://arxiv.org/abs/2508.14892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://hustvl.github.io/Snap-Snap/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅从正面和背面两张图像重建3D人体的方法，能够快速生成完整的带颜色的人体点云，并转换为3D高斯以获得更好的渲染质量。&lt;h4&gt;背景&lt;/h4&gt;从稀疏视图重建3D人体是一个有吸引力的研究领域，对扩展相关应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个从仅两张图像（正面和背面视图）重建人体的任务，降低用户创建自己3D数字人体的门槛。&lt;h4&gt;方法&lt;/h4&gt;重新设计了基于基础重建模型的几何重建模型，预测一致的点云；应用增强算法补充缺失的颜色信息；将完整的带颜色的人体点云直接转换为3D高斯以获得更好的渲染质量。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在单个NVIDIA RTX 4090上，以1024x1024分辨率的两张图像重建整个人体仅需190毫秒；在THuman2.0和跨域数据集上展示了最先进的性能；即使使用低成本移动设备拍摄图像也能完成人体重建，降低数据收集要求。&lt;h4&gt;结论&lt;/h4&gt;该方法能够高效、准确地从仅两张图像重建3D人体，降低了3D数字人体创建的技术门槛。&lt;h4&gt;翻译&lt;/h4&gt;Reconstructing 3D human bodies from sparse views has been an appealing topic, which is crucial to broader the related applications. In this paper, we propose a quite challenging but valuable task to reconstruct the human body from only two images, i.e., the front and back view, which can largely lower the barrier for users to create their own 3D digital humans. The main challenges lie in the difficulty of building 3D consistency and recovering missing information from the highly sparse input. We redesign a geometry reconstruction model based on foundation reconstruction models to predict consistent point clouds even input images have scarce overlaps with extensive human data training. Furthermore, an enhancement algorithm is applied to supplement the missing color information, and then the complete human point clouds with colors can be obtained, which are directly transformed into 3D Gaussians for better rendering quality. Experiments show that our method can reconstruct the entire human in 190 ms on a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024, demonstrating state-of-the-art performance on the THuman2.0 and cross-domain datasets. Additionally, our method can complete human reconstruction even with images captured by low-cost mobile devices, reducing the requirements for data collection. Demos and code are available at https://hustvl.github.io/Snap-Snap/.&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从仅有的两张图像（正面和背面）快速重建3D人体模型的问题。这个问题在现实中很重要，因为它大大降低了用户创建自己3D数字人的门槛，不需要昂贵的专业设备或复杂的设置；在研究中很重要，因为它解决了稀疏视角下人体重建的挑战，扩展了3D重建的应用场景，如虚拟试衣、游戏角色创建和元宇宙，同时降低了数据收集成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于基础几何重建模型（如DUSt3R）重新设计了点云预测模型，将通用几何先验适应到人体领域。整个方法分为三个阶段：点云预测、侧视图增强和高斯属性回归。针对正面和背面视图缺乏重叠的问题，引入了额外的侧视图预测头来推断左右两侧的几何信息。为解决侧视图颜色缺失问题，设计了最近邻搜索算法从前视图和后视图的颜色信息中补充侧视图颜色。方法借鉴了3D高斯泼溅技术来实现高质量渲染。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是从仅有的两张图像重建完整人体3D表示，利用基础几何重建模型知识推断缺失的侧视图几何信息，通过算法补充缺失的颜色信息，并将点云转换为3D高斯表示。整体流程分为三阶段：1）点云预测：输入正背面图像，通过编码器-解码器和预测头生成四个视图的点云并拼接；2）侧视图增强：用最近邻搜索算法从前视图和后视图颜色信息中补充侧视图颜色；3）高斯属性回归：输入点云和图像，用UNet-like网络预测高斯属性并拼接成完整人体高斯表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1）仅需两张图像即可重建完整人体3D高斯表示，毫秒级完成；2）重新设计基于基础几何重建模型的点云预测模型；3）提出侧视图增强算法补充颜色信息；4）直接预测3D高斯表示，无需依赖SMPL-X等人体先验；5）对低成本移动设备拍摄的照片具有鲁棒性。相比之前工作不同：与基于SMPL-X的方法不同，无需估计参数；与基于深度估计的方法不同，不依赖深度估计模块；与单视图重建方法不同，避免生成模型导致的不可控纹理；与需要多视图输入的方法不同，降低数据要求；与传统网格重建方法不同，更好保持姿势一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Snap-Snap提出了一种仅需两张人体正面和背面图像即可在毫秒级重建高质量3D人体高斯表示的方法，通过重新设计几何重建模型和侧视图增强算法，解决了稀疏视角下人体重建的几何一致性和信息缺失问题，显著降低了3D数字人创建的门槛。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D human bodies from sparse views has been an appealing topic,which is crucial to broader the related applications. In this paper, we proposea quite challenging but valuable task to reconstruct the human body from onlytwo images, i.e., the front and back view, which can largely lower the barrierfor users to create their own 3D digital humans. The main challenges lie in thedifficulty of building 3D consistency and recovering missing information fromthe highly sparse input. We redesign a geometry reconstruction model based onfoundation reconstruction models to predict consistent point clouds even inputimages have scarce overlaps with extensive human data training. Furthermore, anenhancement algorithm is applied to supplement the missing color information,and then the complete human point clouds with colors can be obtained, which aredirectly transformed into 3D Gaussians for better rendering quality.Experiments show that our method can reconstruct the entire human in 190 ms ona single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,demonstrating state-of-the-art performance on the THuman2.0 and cross-domaindatasets. Additionally, our method can complete human reconstruction even withimages captured by low-cost mobile devices, reducing the requirements for datacollection. Demos and code are available athttps://hustvl.github.io/Snap-Snap/.</description>
      <author>example@mail.com (Jia Lu, Taoran Yi, Jiemin Fang, Chen Yang, Chuiyun Wu, Wei Shen, Wenyu Liu, Qi Tian, Xinggang Wang)</author>
      <guid isPermaLink="false">2508.14892v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds</title>
      <link>http://arxiv.org/abs/2508.14879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MeshCoder是一种新颖的框架，能够将复杂的3D点云重建为可编辑的Blender Python脚本，解决了现有方法在处理复杂几何形状和结构方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有方法通常依赖于有限的领域特定语言(DSLs)和小规模数据集，限制了它们对复杂几何形状和结构的建模能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够重建复杂3D物体为可编辑程序的框架，以支持逆向工程和形状编辑等应用。&lt;h4&gt;方法&lt;/h4&gt;MeshCoder通过开发全面的Blender Python API来合成复杂几何形状，构建大规模成对对象-代码数据集，并训练多模态大语言模型将3D点云转换为可执行的Blender Python脚本。&lt;h4&gt;主要发现&lt;/h4&gt;MeshCoder在形状到代码重建任务中表现优异，通过代码修改实现了直观的几何和拓扑编辑，并且基于代码的表示增强了LLM在3D形状理解任务中的推理能力。&lt;h4&gt;结论&lt;/h4&gt;MeshCoder为程序化3D形状重建和理解提供了强大而灵活的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将3D物体重建为可编辑的程序对于逆向工程和形状编辑等应用至关重要。然而，现有方法通常依赖于有限的领域特定语言(DSLs)和小规模数据集，限制了它们对复杂几何形状和结构的建模能力。为解决这些挑战，我们引入了MeshCoder，一种新颖的框架，能够将复杂的3D点云重建为可编辑的Blender Python脚本。我们开发了一套全面的Blender Python API，能够合成复杂的几何形状。利用这些API，我们构建了一个大规模的成对对象-代码数据集，其中每个对象的代码被分解为不同的语义部分。随后，我们训练了一个多模态大语言模型(LLM)，将3D点云转换为可执行的Blender Python脚本。我们的方法不仅在形状到代码重建任务中取得了优异的性能，还通过方便的代码修改实现了直观的几何和拓扑编辑。此外，我们的基于代码的表示增强了LLM在3D形状理解任务中的推理能力。这些贡献共同确立了MeshCoder作为程序化3D形状重建和理解的强大而灵活的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将3D点云数据转换为可编辑的程序化代码（特别是Blender Python脚本）的问题。现有方法受限于简单的领域特定语言（DSLs）和缺乏大规模数据集，无法处理复杂几何结构和真实世界对象。这个问题在逆向工程、形状编辑、3D结构理解和设计自动化等领域非常重要，因为它允许通过修改代码直观地编辑3D模型，增强大型语言模型对3D形状的理解，并支持高保真资产创建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法的两个关键局限：DSLs表达能力不足和缺乏大规模配对数据集。然后他们设计了一个三阶段解决方案：1)开发表达力强的Blender Python API；2)构建大规模对象-代码配对数据集；3)训练多模态大语言模型将点云转换为可执行代码。他们借鉴了形状程序方法（如ShapeAssembly、ShapeCoder）、基于部分的3D表示方法、程序化生成框架（如Infinigen-Indoor）以及大型语言模型技术，但将其扩展到代码生成领域，形成了创新的综合方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云转换为结构化的、可执行的Blender Python脚本，这些脚本能够重建复杂3D对象的语义部分，从而提供更高层次的编辑和理解能力。整体流程分为三阶段：1)数据准备阶段，使用API生成零件数据集，并利用Infinigen-Indoor生成对象数据集；2)模型训练阶段，先训练零件到代码的推断模型，再基于此训练对象到代码的推断模型；3)推理和应用阶段，输入点云通过形状标记化器转换为标记，LLM生成可执行代码，执行后重建3D对象，支持通过代码修改进行编辑，并增强大型语言模型对3D形状的理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开发表达力强的Blender Python API，支持复杂几何操作；2)构建大规模配对对象-代码数据集，涵盖41个类别和100万个对象；3)训练多模态大语言模型将3D点云转换为可执行Blender脚本；4)通过代码表示实现直观的几何和拓扑编辑。相比之前的工作，MeshCoder超越了传统形状程序方法的简单几何限制，解决了CAD方法只关注单个零件的问题，弥补了基于部分方法不提供代码表示的不足，并增强了大型语言模型对3D形状结构的高层次理解能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MeshCoder通过将3D点云转换为可编辑的Blender Python脚本，实现了复杂3D对象的程序化重建与编辑，同时提升了大型语言模型对3D形状结构的理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D objects into editable programs is pivotal for applicationslike reverse engineering and shape editing. However, existing methods oftenrely on limited domain-specific languages (DSLs) and small-scale datasets,restricting their ability to model complex geometries and structures. Toaddress these challenges, we introduce MeshCoder, a novel framework thatreconstructs complex 3D objects from point clouds into editable Blender Pythonscripts. We develop a comprehensive set of expressive Blender Python APIscapable of synthesizing intricate geometries. Leveraging these APIs, weconstruct a large-scale paired object-code dataset, where the code for eachobject is decomposed into distinct semantic parts. Subsequently, we train amultimodal large language model (LLM) that translates 3D point cloud intoexecutable Blender Python scripts. Our approach not only achieves superiorperformance in shape-to-code reconstruction tasks but also facilitatesintuitive geometric and topological editing through convenient codemodifications. Furthermore, our code-based representation enhances thereasoning capabilities of LLMs in 3D shape understanding tasks. Together, thesecontributions establish MeshCoder as a powerful and flexible solution forprogrammatic 3D shape reconstruction and understanding.</description>
      <author>example@mail.com (Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, Zhaoyang Lyu, Jiangmiao Pang)</author>
      <guid isPermaLink="false">2508.14879v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>GeMS: Efficient Gaussian Splatting for Extreme Motion Blur</title>
      <link>http://arxiv.org/abs/2508.14682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了GeMS，一个专门用于处理严重运动模糊图像的3D高斯飞溅框架，能够直接从模糊输入重建场景，并提出了改进版本GeMS-E，结合事件数据进一步提升了重建质量。&lt;h4&gt;背景&lt;/h4&gt;现有最先进的去模糊方法和高斯飞溅方法通常假设可以使用清晰图像进行相机姿态估计和点云生成，这不切实际。依赖COLMAP初始化的方法在严重模糊下也会失败，因为特征对应不可靠。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够直接从极度模糊图像重建场景的3D高斯飞溅框架，解决现有方法在处理严重运动模糊图像时的局限性。&lt;h4&gt;方法&lt;/h4&gt;GeMS框架包含三个核心组件：(1) VGGSfM，基于深度学习的运动恢复结构管道，直接从模糊输入估计姿态和生成点云；(2) 3DGS-MCMC，通过将高斯视为概率分布的样本实现鲁棒场景初始化；(3) 相机轨迹和高斯参数的联合优化。改进版本GeMS-E进一步整合了基于事件的积分去模糊(EDI)步骤，恢复更清晰的图像输入到GeMS中。&lt;h4&gt;主要发现&lt;/h4&gt;GeMS和GeMS-E在合成和真实数据集上都达到了最先进的性能，能够有效处理严重运动模糊的图像，实现高质量的场景重建。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，GeMS是第一个直接从严重模糊输入处理极端运动模糊的3D高斯飞溅框架，为处理极端模糊条件下的3D重建提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了GeMS，一个用于3D高斯飞溅(3DGS)的框架，专门用于处理严重运动模糊图像。针对极端模糊的最先进去模糊方法(如ExBluRF)以及高斯飞溅方法(如Deblur-GS)通常假设可以使用清晰图像进行相机姿态估计和点云生成，这是一个不切实际的假设。依赖COLMAP初始化的方法(如BAD-Gaussians)也会因严重模糊下不可靠的特征对应而失败。为解决这些挑战，我们提出了GeMS，一个直接从极度模糊图像重建场景的3DGS框架。GeMS集成了：(1) VGGSfM，一个基于深度学习的运动恢复结构管道，直接从模糊输入估计姿态和生成点云；(2) 3DGS-MCMC，通过将高斯视为概率分布的样本实现鲁棒场景初始化，消除启发式密度增加和修剪；(3) 相机轨迹和高斯参数的联合优化，实现稳定重建。虽然此流程能产生良好结果，但当所有输入都严重模糊时，可能仍存在不准确之处。为此，我们提出了GeMS-E，它使用事件进行渐进式精化：(4) 基于事件的积分去模糊(EDI)恢复更清晰的图像，然后输入GeMS，改善姿态估计、点云生成和整体重建。GeMS和GeMS-E在合成和真实数据集上都达到了最先进的性能。据我们所知，这是第一个直接从严重模糊输入处理极端运动模糊的3DGS框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从极端运动模糊的图像中直接重建出清晰的3D场景的问题。这个问题在现实中非常重要，因为在高速相机运动或低光条件下（如机器人、自动驾驶、手持摄影等场景），拍摄清晰图像往往不可能，而现有的3D重建方法（如NeRF、3DGS）都依赖于清晰图像进行相机姿态估计和点云生成，导致在严重运动模糊情况下无法工作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行设计：传统SfM方法（如COLMAP）在严重模糊下失效，而去模糊方法（如ExBluRF、BAD-Gaussians）依赖清晰图像初始化。作者借鉴了三个关键技术：1) VGGSfM方法，使用深度学习进行2D点跟踪而非特征匹配，提高对模糊的鲁棒性；2) 3DGS-MCMC方法，将高斯视为概率分布样本，使用MCMC采样自适应优化几何；3) 改进BAD-Gaussians的联合优化，用贝塞尔曲线替代线性插值更好地建模复杂运动。作者将这些技术整合为一个紧密耦合的端到端系统，各组件相互补偿局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; GeMS的核心思想是通过结合模糊鲁棒的初始化、概率场景建模和物理驱动的联合优化，直接从极端运动模糊图像中重建清晰3D场景。整体流程：1) 使用VGGSfM从模糊图像直接估计相机姿态和生成点云；2) 使用3DGS-MCMC将高斯视为概率分布样本，通过MCMC采样自适应优化几何；3) 联合优化贝塞尔曲线参数化的相机轨迹和高斯参数，确保与物理模糊形成过程一致。对于极端情况（所有视图都严重模糊），GeMS-E扩展使用事件数据通过EDI模型恢复清晰图像，再输入到GeMS框架中提高重建质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 消除对清晰图像初始化的依赖，直接从模糊图像重建场景；2) 使用VGGSfM替代传统COLMAP，通过深度学习点跟踪提高模糊鲁棒性；3) 采用3DGS-MCMC进行概率场景建模，替代启发式密集化/剪枝策略；4) 使用贝塞尔曲线参数化相机轨迹，与高斯参数进行物理驱动的联合优化；5) 提出GeMS-E扩展，利用事件数据进一步提升极端模糊场景的重建质量。相比之前工作，GeMS是完全端到端的框架，无需清晰图像初始化，且在极端模糊情况下表现更优，同时计算效率更高（训练时间从数小时降至几分钟）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeMS首次实现了直接从极端运动模糊图像中高效重建高质量3D场景，无需清晰图像初始化，并通过事件数据进一步提升了在所有视图都严重模糊情况下的重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed tohandle severely motion-blurred images. State-of-the-art deblurring methods forextreme blur, such as ExBluRF, as well as Gaussian Splatting-based approacheslike Deblur-GS, typically assume access to sharp images for camera poseestimation and point cloud generation, an unrealistic assumption. Methodsrelying on COLMAP initialization, such as BAD-Gaussians, also fail due tounreliable feature correspondences under severe blur. To address thesechallenges, we propose GeMS, a 3DGS framework that reconstructs scenes directlyfrom extremely blurred images. GeMS integrates: (1) VGGSfM, a deeplearning-based Structure-from-Motion pipeline that estimates poses andgenerates point clouds directly from blurred inputs; (2) 3DGS-MCMC, whichenables robust scene initialization by treating Gaussians as samples from aprobability distribution, eliminating heuristic densification and pruning; and(3) joint optimization of camera trajectories and Gaussian parameters forstable reconstruction. While this pipeline produces strong results,inaccuracies may remain when all inputs are severely blurred. To mitigate this,we propose GeMS-E, which integrates a progressive refinement step using events:(4) Event-based Double Integral (EDI) deblurring restores sharper images thatare then fed into GeMS, improving pose estimation, point cloud generation, andoverall reconstruction. Both GeMS and GeMS-E achieve state-of-the-artperformance on synthetic and real-world datasets. To our knowledge, this is thefirst framework to address extreme motion blur within 3DGS directly fromseverely blurred inputs.</description>
      <author>example@mail.com (Gopi Raju Matta, Trisha Reddypalli, Vemunuri Divya Madhuri, Kaushik Mitra)</author>
      <guid isPermaLink="false">2508.14682v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling</title>
      <link>http://arxiv.org/abs/2508.14604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, Accepted to ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种统一时空状态空间模型(UST-SSM)，用于处理点云视频的时空无序性问题，通过时空选择扫描、时空结构聚合和时间交互采样三种技术，有效提高了对细微和连续人类动作的识别能力。&lt;h4&gt;背景&lt;/h4&gt;点云视频能够捕捉动态3D运动并减少光照和视角变化的影响，对识别细微和连续的人类动作非常有效。选择性状态空间模型(SSMs)在序列建模中表现良好，但点云视频的时空无序性阻碍了其直接单向建模。&lt;h4&gt;目的&lt;/h4&gt;解决点云视频时空无序性对SSMs单向建模的阻碍问题，使SSMs能够有效应用于点云视频处理。&lt;h4&gt;方法&lt;/h4&gt;提出统一时空状态空间模型(UST-SSM)，包含三种技术：1)时空选择扫描(STSS)通过提示引导的聚类将无序点重新组织为语义感知序列；2)时空结构聚合(STSA)聚合时空特征并补偿缺失的4D几何和运动细节；3)时间交互采样(TIS)通过非锚帧利用和扩展感受野增强时间交互。&lt;h4&gt;主要发现&lt;/h4&gt;在MSR-Action3D、NTU RGB+D和Synthia 4D数据集上的实验结果验证了UST-SSM方法的有效性，代码已在GitHub开源。&lt;h4&gt;结论&lt;/h4&gt;UST-SSM成功将SSMs的最新进展扩展到点云视频处理，通过重新组织点云序列和增强时间交互，有效解决了点云视频时空无序性问题。&lt;h4&gt;翻译&lt;/h4&gt;点云视频捕捉动态3D运动的同时减少了光照和视角变化的影响，使其在识别细微和连续的人类动作方面非常有效。尽管选择性状态空间模型(SSMs)在序列建模中已展现出良好的性能和线性复杂度，但点云视频的时空无序性在通过时间顺序扫描将点云视频直接展开为一维序列时，阻碍了其单向建模。为解决这一挑战，我们提出了统一时空状态空间模型(UST-SSM)，将SSMs的最新进展扩展到点云视频。具体而言，我们引入了时空选择扫描(STSS)，通过提示引导的聚类将无序点重新组织为语义感知序列，从而能够在序列中有效利用空间和时间上相距较远但相似的点。对于缺失的4D几何和运动细节，时空结构聚合(STSA)聚合时空特征并进行补偿。为了提高采样序列内的时间交互，时间交互采样(TIS)通过非锚帧利用和扩展感受野增强细粒度时间依赖关系。在MSR-Action3D、NTU RGB+D和Synthia 4D数据集上的实验结果验证了我们方法的有效性。我们的代码可在https://github.com/wangzy01/UST-SSM获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云视频建模中的时空无序性问题，这一问题阻碍了选择性状态空间模型(SSMs)的单向建模能力。这个问题在现实中非常重要，因为点云视频能够捕捉动态3D运动同时减少光照和视角变化的影响，对于机器人技术和自主系统中的人体动作识别至关重要。研究上，现有方法如CNNs难以建模长期依赖关系，而Transformer-based方法在长序列处理时消耗大量内存，SSMs虽有线性复杂度优势但难以直接应用于时空无序的点云视频。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云视频建模面临的三大挑战：非统一的时空无序性、局部几何信息丢失和时间交互限制。他们借鉴了选择性状态空间模型(SSMs)在序列建模中的成功应用，参考了Mamba4D的方法但指出了其局限性，同时受到Transformer-based方法的启发但注意到其内存消耗问题。基于这些分析，作者设计了三个关键组件：时空选择扫描(STSS)解决时空无序性，时空结构聚合(STSA)补偿几何信息丢失，时间交互采样(TIS)增强时间交互，从而构建了UST-SSM方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过统一时空状态空间模型重新组织点云数据，使其更适合SSMs的单向建模特性。整体流程包括：1)时间交互采样(TIS)通过多步采样策略扩展时间感受域；2)时空选择扫描(STSS)利用提示网络聚类相似点并使用Hilbert曲线排序；3)时空结构聚合(STSA)通过4D KNN和特征传播补偿几何信息；4)最后通过时空状态空间模型(ST-SSM)处理序列，并用MLP进行分类预测。这一流程有效解决了点云视频的时空无序性问题。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)时空选择扫描(STSS)通过提示引导聚类将时空相似但距离远的点组织在一起；2)时空结构聚合(STSA)通过4D KNN和特征传播补偿序列化中丢失的几何细节；3)时间交互采样(TIS)通过多步采样扩展时间感受野。相比Mamba4D，UST-SSM克服了其仅基于前序点建模的限制；相比Transformer-based方法，UST-SSM具有线性复杂度和更低的内存消耗，在长序列建模中表现更好；相比传统扫描方法，STSS直接聚类相似点避免了长距离衰减问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UST-SSM通过时空选择扫描、时空结构聚合和时间交互采样三个关键技术，成功将选择性状态空间模型扩展到点云视频建模，解决了时空无序性问题，在保持计算效率的同时显著提升了点云视频分析的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud videos capture dynamic 3D motion while reducing the effects oflighting and viewpoint variations, making them highly effective for recognizingsubtle and continuous human actions. Although Selective State Space Models(SSMs) have shown good performance in sequence modeling with linear complexity,the spatio-temporal disorder of point cloud videos hinders their unidirectionalmodeling when directly unfolding the point cloud video into a 1D sequencethrough temporally sequential scanning. To address this challenge, we proposethe Unified Spatio-Temporal State Space Model (UST-SSM), which extends thelatest advancements in SSMs to point cloud videos. Specifically, we introduceSpatial-Temporal Selection Scanning (STSS), which reorganizes unordered pointsinto semantic-aware sequences through prompt-guided clustering, therebyenabling the effective utilization of points that are spatially and temporallydistant yet similar within the sequence. For missing 4D geometric and motiondetails, Spatio-Temporal Structure Aggregation (STSA) aggregatesspatio-temporal features and compensates. To improve temporal interactionwithin the sampled sequence, Temporal Interaction Sampling (TIS) enhancesfine-grained temporal dependencies through non-anchor frame utilization andexpanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D,and Synthia 4D datasets validate the effectiveness of our method. Our code isavailable at https://github.com/wangzy01/UST-SSM.</description>
      <author>example@mail.com (Peiming Li, Ziyi Wang, Yulin Yuan, Hong Liu, Xiangming Meng, Junsong Yuan, Mengyuan Liu)</author>
      <guid isPermaLink="false">2508.14604v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR</title>
      <link>http://arxiv.org/abs/2508.14554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 IEEE/RSJ International Conference on Intelligent  Robots and Systems (IROS 2025). This work has been submitted to the IEEE for  possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出EAROL框架，通过硬件创新和算法优化解决了无人机在开放式顶部场景中的定位漂移和感知-规划耦合问题，显著提高了搜救任务中的自主导航性能。&lt;h4&gt;背景&lt;/h4&gt;无人机在开放式顶部场景（如倒塌建筑物、无顶迷宫）中面临定位漂移和感知-规划耦合的挑战，这些特殊环境对无人机自主导航提出了严峻考验。&lt;h4&gt;目的&lt;/h4&gt;解决无人机在开放式顶部场景中的定位漂移和感知-规划耦合问题，提高其在灾难后搜救任务中的自主性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出EAROL框架，采用向下倾斜20度的LiDAR配置，集成LiDAR-惯性里程计系统，结合分层轨迹-偏航优化算法；使用迭代误差状态卡尔曼滤波实现紧耦合LIO系统；设计增强环境感知能力的规划器，平衡探索、跟踪和能耗。&lt;h4&gt;主要发现&lt;/h4&gt;物理实验表明，该方案实现了81%的跟踪误差减少，22%的感知覆盖率提升，以及在室内迷宫和60米规模户外场景中接近零的垂直漂移。&lt;h4&gt;结论&lt;/h4&gt;EAROL采用硬件-算法协同设计范式，为无人机在灾难后搜救任务中的自主性提供了稳健解决方案，并将软件和硬件作为开源包发布。&lt;h4&gt;翻译&lt;/h4&gt;为解决无人机在开放式顶部场景（如倒塌建筑物、无顶迷宫）中运行的定位漂移和感知-规划耦合挑战，本文提出EAROL，一种采用向下倾斜LiDAR配置（20度倾角）的新框架，集成了LiDAR-惯性里程计系统和分层轨迹-偏航优化算法。硬件创新通过获取密集地面点云和前向环境感知实现约束增强，用于动态障碍检测。由具有动态运动补偿的迭代误差状态卡尔曼滤波(IESKF)驱动的紧耦合LIO系统，在特征稀疏环境中实现了高精度6自由度定位。规划器增强了环境感知能力，平衡环境探索、目标跟踪精度和能源效率。物理实验表明，在室内迷宫和60米规模户外场景中，实现了81%的跟踪误差减少，22%的感知覆盖率提升，以及接近零的垂直漂移。这项工作提出了硬件-算法协同设计范式，为无人机在灾难后搜救任务中的自主性提供了稳健解决方案。我们将软件和硬件作为开源包发布给社区。视频：https://youtu.be/7av2ueLSiYw。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无人机在开放顶部场景（如倒塌建筑物、无顶迷宫）中的定位漂移和感知-规划耦合问题。这些问题在现实中非常重要，因为它们限制了无人机在灾难搜救等任务中的自主导航能力，在这些场景中，缺乏天花板特征会导致传统定位算法失效，而被动规划方法无法充分利用感知能力来优化导航效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用硬件-算法协同设计思路，创新性地设计了向下倾斜20°安装的激光雷达配置，同时获取地面约束和前方感知。在算法层面，作者借鉴了现有LiDAR SLAM框架（如LOAM、LIO-SAM）的基本结构，但引入了动态运动补偿和退化场景处理机制。在规划方面，作者从视觉感知感知规划中获取灵感，将其应用于激光雷达感知规划，并引入环境信息熵作为优化目标。整个设计体现了从问题出发，创新硬件配置，并针对性地开发算法解决方案的思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过向下倾斜安装激光雷达，同时获取地面点云（提供垂直约束）和前方环境数据（用于动态障碍物检测），并将硬件设计与算法紧密结合，形成硬件-算法协同设计范式。整体实现流程包括：1)硬件层：向下倾斜20°安装激光雷达，结合IMU等传感器；2)感知层：自适应激光雷达倾斜角补偿、基于IESKF的LIO系统、多层环境表示构建和实时动态物体跟踪；3)规划层：轨迹生成（考虑平滑性、动态可行性、避障和飞行时间）和偏航角优化（基于环境信息熵、目标跟踪误差和能量消耗的多目标优化）；4)控制层：非线性模型预测控制器和行为调节有限状态机。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)硬件创新：向下倾斜安装的激光雷达配置，提供垂直约束增强和前向感知增强；2)算法创新：基于IESKF的紧密耦合LIO系统具有动态运动补偿，退化场景感知处理，以及环境增强感知偏航角规划；3)系统集成：硬件-算法协同设计范式和分层优化框架。相比之前工作，EAROL的主要不同在于：传统方法依赖后处理优化，而EAROL采用硬件-算法协同设计；传统LiDAR规划被动对齐偏航角，EAROL主动优化偏航角以增强感知；EAROL专门针对开放顶部场景的垂直漂移问题，通过地面约束解决；传统方法使用全方位感知，EAROL采用方向性感知并优化感知-规划耦合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EAROL通过创新的向下倾斜激光雷达硬件设计和环境感知感知规划算法，有效解决了无人机在开放顶部场景中的定位漂移和感知-规划耦合问题，为灾难搜救等任务提供了鲁棒自主导航解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To address the challenges of localization drift and perception-planningcoupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios(e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novelframework with a downward-mounted tilted LiDAR configuration (20{\deg}inclination), integrating a LiDAR-Inertial Odometry (LIO) system and ahierarchical trajectory-yaw optimization algorithm. The hardware innovationenables constraint enhancement via dense ground point cloud acquisition andforward environmental awareness for dynamic obstacle detection. Atightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter(IESKF) with dynamic motion compensation, achieves high level 6-DoFlocalization accuracy in feature-sparse environments. The planner, augmented byenvironment, balancing environmental exploration, target tracking precision,and energy efficiency. Physical experiments demonstrate 81% tracking errorreduction, 22% improvement in perceptual coverage, and near-zero vertical driftacross indoor maze and 60-meter-scale outdoor scenarios. This work proposes ahardware-algorithm co-design paradigm, offering a robust solution for UAVautonomy in post-disaster search and rescue missions. We will release oursoftware and hardware as an open-source package for the community. Video:https://youtu.be/7av2ueLSiYw.</description>
      <author>example@mail.com (Xinkai Liang, Yigu Ge, Yangxi Shi, Haoyu Yang, Xu Cao, Hao Fang)</author>
      <guid isPermaLink="false">2508.14554v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network</title>
      <link>http://arxiv.org/abs/2508.14373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Transformer的粗到细点移动网络（TCFNet），用于解决正颌外科手术模拟中面部骨骼形状变换的准确性问题，克服了传统方法和现有深度学习方法的各种局限性。&lt;h4&gt;背景&lt;/h4&gt;计算机辅助手术模拟是正颌外科手术规划的关键环节，准确模拟面部骨骼形状变换至关重要。传统生物力学模拟方法计算时间长、数据处理劳动强度大且准确性低。基于深度学习的模拟方法虽有所改进，但仍存在无法处理大规模点、感受野有限导致噪声点、以及基于配准的复杂预处理和后处理操作等缺点，限制了其性能和广泛应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够学习密集面骨点云转换中补丁和点级别独特复杂对应关系的网络，提高模拟的准确性和适用性。&lt;h4&gt;方法&lt;/h4&gt;提出TCFNet端到端框架，第一阶段采用基于Transformer的网络，第二阶段采用局部信息聚合网络（LIA-Net），两者相互强化生成精确点移动路径。LIA-Net通过建模局部几何结构（边缘、方向和相对位置特征）补偿Transformer网络的邻域精度损失，并使用门控循环单元利用先验全局特征指导局部位移。受可变形医学图像配准启发，提出辅助损失利用专家知识重建关键器官。&lt;h4&gt;主要发现&lt;/h4&gt;TCFNet在收集的数据集上与现有最先进方法相比，取得了卓越的评估指标和可视化结果，代码已公开在GitHub平台。&lt;h4&gt;结论&lt;/h4&gt;TCFNet有效解决了现有方法在处理大规模点云、感受野限制和复杂预处理后处理方面的局限性，提高了计算机辅助手术模拟的性能和适用性。&lt;h4&gt;翻译&lt;/h4&gt;计算机辅助手术模拟是正颌外科手术规划的关键组成部分，准确模拟面部骨骼形状变换具有重要意义。传统生物力学模拟方法受限于计算时间长、数据处理劳动强度大和低准确性。最近，基于深度学习的模拟方法被提出，将此问题视为骨骼和面部点云之间的点对点转换。然而，这些方法无法处理大规模点，感受野有限导致噪声点，并采用基于配准的复杂预处理和后处理操作。这些缺点限制了此类方法的性能和广泛应用。因此，我们提出了一种基于Transformer的粗到细点移动网络（TCFNet），用于学习密集面骨点云转换中补丁和点级别的独特复杂对应关系。该端到端框架在第一阶段采用基于Transformer的网络，在第二阶段采用局部信息聚合网络（LIA-Net），两者相互强化以生成精确的点移动路径。LIA-Net通过建模局部几何结构（边缘、方向和相对位置特征）可以有效地补偿基于Transformer网络的邻域精度损失。使用先验全局特征通过门控循环单元指导局部位移。受可变形医学图像配准启发，我们提出了一种辅助损失，可以利用专家知识重建关键器官。与收集的数据集上的现有最先进方法相比，TCFNet取得了卓越的评估指标和可视化结果。代码可在 https://github.com/Runshi-Zhang/TCFNet 获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决面部骨骼与面部外观之间的双向转换问题，特别是在正颌外科手术规划中的应用。这个问题很重要，因为准确的手术规划需要精确模拟骨骼移动对面部外观的影响，而传统方法计算时间长、数据处理复杂、准确性低，现有深度学习方法无法有效处理大规模点云数据，限制了手术规划的准确性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统生物力学模拟方法的局限性和现有深度学习方法的问题，然后借鉴了Point Transformer V3处理大规模点云的能力，参考了P2P-Net的双向和循环结构框架，受多步框架启发采用粗到细策略，并参考了可变形医学图像配准中的辅助损失方法。基于这些工作，作者设计了TCFNet框架，包含基于Transformer的第一阶段和局部信息聚合网络的第二阶段，以解决全局特征和局部特征的平衡问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于Transformer的粗到细点移动网络学习面部和骨骼点云之间的复杂对应关系，结合全局特征和局部特征。整体流程包括：1)数据预处理（点云采样、去噪、标准化）；2)第一阶段（基于Transformer的网络处理全局特征，生成粗略位移和变形结果）；3)第二阶段（LIA-Net网络处理局部特征，结合全局特征生成精细位移）；4)损失函数计算（全局距离、正则化、局部距离和可选辅助损失）；5)后处理（法线估计、表面重建、平滑处理）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双向点云转换框架TCFNet，能处理密集点云；2)局部信息聚合网络LIA-Net，解决Transformer的邻域精度损失；3)可选辅助损失促进局部关键结构对应。相比之前工作：与P2P-Net相比，TCFNet能处理更多点云且不需要预配准；与P2P-Conv相比，具有更大感受野和更好全局对应；与PTv3相比，通过LIA-Net解决了局部结构建模问题，在点云增加时性能下降更小。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TCFNet通过结合Transformer架构和局部信息聚合网络，实现了高效准确的面部骨骼与面部外观之间的双向转换，为正颌外科手术规划提供了强大的工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.media.2025.103653&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-aided surgical simulation is a critical component of orthognathicsurgical planning, where accurately simulating face-bone shape transformationsis significant. The traditional biomechanical simulation methods are limited bytheir computational time consumption levels, labor-intensive data processingstrategies and low accuracy. Recently, deep learning-based simulation methodshave been proposed to view this problem as a point-to-point transformationbetween skeletal and facial point clouds. However, these approaches cannotprocess large-scale points, have limited receptive fields that lead to noisypoints, and employ complex preprocessing and postprocessing operations based onregistration. These shortcomings limit the performance and widespreadapplicability of such methods. Therefore, we propose a Transformer-basedcoarse-to-fine point movement network (TCFNet) to learn unique, complicatedcorrespondences at the patch and point levels for dense face-bone point cloudtransformations. This end-to-end framework adopts a Transformer-based networkand a local information aggregation network (LIA-Net) in the first and secondstages, respectively, which reinforce each other to generate precise pointmovement paths. LIA-Net can effectively compensate for the neighborhoodprecision loss of the Transformer-based network by modeling local geometricstructures (edges, orientations and relative position features). The previousglobal features are employed to guide the local displacement using a gatedrecurrent unit. Inspired by deformable medical image registration, we proposean auxiliary loss that can utilize expert knowledge for reconstructing criticalorgans.Compared with the existing state-of-the-art (SOTA) methods on gathereddatasets, TCFNet achieves outstanding evaluation metrics and visualizationresults. The code is available at https://github.com/Runshi-Zhang/TCFNet.</description>
      <author>example@mail.com (Runshi Zhang, Bimeng Jie, Yang He, Junchen Wang)</author>
      <guid isPermaLink="false">2508.14373v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation</title>
      <link>http://arxiv.org/abs/2508.14358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025 Workshop on Recovering 6D Object Pose (R6D)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了HRC-Pose，一种新颖的仅使用深度的类别级物体姿态估计框架，通过对比学习学习保留6D姿态连续性的点云表示，实现了优于现有方法的性能并能实时运行。&lt;h4&gt;背景&lt;/h4&gt;类别级物体姿态估计旨在预测给定类别物体的6D姿态和3D尺寸，现有方法仅使用6D姿态作为监督信号，没有明确捕捉姿态的内在连续性，导致预测不一致和对未见姿态的泛化能力降低。&lt;h4&gt;目的&lt;/h4&gt;解决现有类别级物体姿态估计方法中缺乏姿态连续性捕捉的问题，提高预测的一致性和对未见姿态的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出HRC-Pose框架，将物体姿态解耦为旋转和平移组件，分别编码并利用；引入基于6D姿态感知层次排序方案的对比学习策略，用于多任务、多类别场景；设计分别处理旋转感知和平移感知嵌入的姿态估计模块。&lt;h4&gt;主要发现&lt;/h4&gt;HRC-Pose成功学习了连续特征空间；在REAL275和CAMERA25基准测试上持续优于现有仅使用深度的最先进方法；能够实时运行，适合实际应用。&lt;h4&gt;结论&lt;/h4&gt;HRC-Pose通过捕捉姿态的内在连续性，显著提高了类别级物体姿态估计的性能，具有实际应用潜力，代码已开源。&lt;h4&gt;翻译&lt;/h4&gt;类别级物体姿态估计旨在预测给定类别物体的6D姿态和3D尺寸。现有方法仅依赖6D姿态作为监督信号，没有明确捕捉姿态的内在连续性，导致预测不一致和对未见姿态的泛化能力降低。为解决这一局限，我们提出了HRC-Pose，一种新颖的仅使用深度的框架，利用对比学习学习保留6D姿态连续性的点云表示。HRC-Pose将物体姿态解耦为旋转和平移组件，在网络中分别编码和利用。具体而言，我们引入了基于6D姿态感知层次排序方案的对比学习策略，用于多任务、多类别场景，该策略通过考虑旋转和平移差异以及类别信息来对比来自多个类别的点云。我们进一步设计了分别处理学习到的旋转感知和平移感知嵌入的姿态估计模块。实验证明HRC-Pose成功学习了连续特征空间。在REAL275和CAMERA25基准测试上的结果显示，我们的方法持续优于现有仅使用深度的最先进方法并能实时运行，证明了其有效性和实际应用潜力。我们的代码位于https://github.com/zhujunli1993/HRC-Pose。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决类别级6D物体姿态估计中姿态连续性捕捉不足的问题。现有方法仅将6D姿态作为监督信号，没有明确捕捉姿态的内在连续性，导致预测不一致和对未见姿态的泛化能力降低。这个问题在现实中很重要，因为准确的物体姿态估计对于机器人操作、场景理解、自动驾驶和增强现实等应用至关重要，而类别级姿态估计相比实例级姿态估计具有更广泛的适用性，不需要精确的CAD模型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：仅将姿态作为训练目标导致特征碎片化，无法捕捉姿态连续性；现有方法专注于旋转而忽略平移；Rank-N-Contrast等对比学习方法只能处理单一任务和类别。作者借鉴了对比学习思想，参考了Rank-N-Contrast方法但扩展了它以处理多任务、多类别场景，同时利用了现有的3D-GCN等特征提取器和姿态估计模块。基于这些，作者设计了HRC-Pose框架，提出6D姿态感知的层次化排序方案，将姿态解耦为旋转和平移分量分别处理，并开发了专门的特征编码和姿态估计模块。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过对比学习学习点云表示，保持6D姿态的内在连续性，并将姿态解耦为旋转和平移分量分别处理。整体流程：1)输入处理：从RGB-D图像提取点云；2)特征提取：用两个独立编码器分别提取旋转和平移相关的点级嵌入；3)全局特征聚合：通过池化获得全局嵌入；4)层次化对比学习：基于6D姿态感知的层次化排序方案构建对比学习对，包括联合负样本和旋转/平移特定负样本；5)姿态估计：分别用旋转和平移感知嵌入预测相应姿态分量；6)损失计算：结合对比损失和姿态估计损失进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)层次化排名对比学习，保持旋转和平移的连续性；2)姿态解耦与分别处理，旋转和平移分量独立编码；3)多任务、多类别的对比学习框架；4)深度-only框架，不依赖RGB信息。相比之前工作的不同：相比RGB-D方法，专注于深度信息且明确捕捉姿态连续性；相比仅处理旋转的方法，同时考虑旋转和平移；相比Rank-N-Contrast，能处理多任务和多类别；相比其他深度-only方法，在保持实时性的同时提高了准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HRC-Pose通过引入层次化排名对比学习方法，成功解决了类别级6D物体姿态估计中姿态连续性捕捉不足的问题，仅使用深度信息就在保持实时性能的同时实现了最先进的姿态估计精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Category-level object pose estimation aims to predict the 6D pose and 3D sizeof objects within given categories. Existing approaches for this task relysolely on 6D poses as supervisory signals without explicitly capturing theintrinsic continuity of poses, leading to inconsistencies in predictions andreduced generalization to unseen poses. To address this limitation, we proposeHRC-Pose, a novel depth-only framework for category-level object poseestimation, which leverages contrastive learning to learn point cloudrepresentations that preserve the continuity of 6D poses. HRC-Pose decouplesobject pose into rotation and translation components, which are separatelyencoded and leveraged throughout the network. Specifically, we introduce acontrastive learning strategy for multi-task, multi-category scenarios based onour 6D pose-aware hierarchical ranking scheme, which contrasts point cloudsfrom multiple categories by considering rotational and translationaldifferences as well as categorical information. We further design poseestimation modules that separately process the learned rotation-aware andtranslation-aware embeddings. Our experiments demonstrate that HRC-Posesuccessfully learns continuous feature spaces. Results on REAL275 and CAMERA25benchmarks show that our method consistently outperforms existing depth-onlystate-of-the-art methods and runs in real-time, demonstrating its effectivenessand potential for real-world applications. Our code is athttps://github.com/zhujunli1993/HRC-Pose.</description>
      <author>example@mail.com (Zhujun Li, Shuo Zhang, Ioannis Stamos)</author>
      <guid isPermaLink="false">2508.14358v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values</title>
      <link>http://arxiv.org/abs/2508.14083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, incomplete version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的对比自学习框架(CST)用于从低质量数据中推断兴趣点(POIs)的准确人群流动，解决了标记数据稀缺、POI间复杂时空依赖以及精确人群流动与GPS报告间多种相关性等挑战。&lt;h4&gt;背景&lt;/h4&gt;准确获取兴趣点的人群流动数据对交通管理、公共服务和城市规划至关重要，但由于城市传感技术限制，大多数来源的数据质量不足以监控每个POI的人群流动。&lt;h4&gt;目的&lt;/h4&gt;从低质量数据中推断准确的人群流动，解决标记数据稀缺、POI间复杂时空依赖以及精确人群流动与GPS报告间多种相关性等挑战。&lt;h4&gt;方法&lt;/h4&gt;将人群流动推断问题重新表述为自监督属性图表示学习任务，构建基于POIs及其距离的空间邻接图，采用对比学习技术利用大量未标记的时空数据，采用交换预测方法预测相似实例的目标子图表示，并在预训练后使用准确人群流动数据对模型进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实数据集上的实验表明，CST模型在大量噪声数据上预训练后，性能始终优于从头开始训练的模型。&lt;h4&gt;结论&lt;/h4&gt;CST框架能有效解决人群流动推断中的挑战，特别是在标记数据稀缺的情况下。&lt;h4&gt;翻译&lt;/h4&gt;准确获取兴趣点(POIs)的人群流动数据对于有效的交通管理、公共服务和城市规划至关重要。尽管如此，由于城市传感技术的局限性，大多数来源的数据质量不足以监控每个POI的人群流动。这使得从低质量数据中推断准确的人群流动成为一个关键且具有挑战性的任务。这种复杂性因三个关键因素而加剧：1)标记数据的稀缺性和稀有性，2)POI之间复杂的时空依赖关系，3)精确人群流动与GPS报告之间的多种相关性。为应对这些挑战，我们将人群流动推断问题重新表述为自监督属性图表示学习任务，并引入了一个新的用于时空数据的对比自学习框架(CST)。我们的方法首先基于POIs及其距离构建空间邻接图。然后采用对比学习技术利用大量未标记的时空数据。我们采用交换预测方法来预测相似实例的目标子图表示。在预训练阶段后，使用准确的人群流动数据对模型进行微调。我们在两个真实数据集上进行的实验表明，CST在大量噪声数据上预训练后，性能始终优于从头开始训练的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotalfor effective traffic management, public service, and urban planning. Despitethis importance, due to the limitations of urban sensing techniques, the dataquality from most sources is inadequate for monitoring crowd flow at each POI.This renders the inference of accurate crowd flow from low-quality data acritical and challenging task. The complexity is heightened by three keyfactors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{Theintricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriadcorrelations between precise crowd flow and GPS reports}.  To address these challenges, we recast the crowd flow inference problem as aself-supervised attributed graph representation learning task and introduce anovel \underline{C}ontrastive \underline{S}elf-learning framework for\underline{S}patio-\underline{T}emporal data (\model). Our approach initiateswith the construction of a spatial adjacency graph founded on the POIs andtheir respective distances. We then employ a contrastive learning technique toexploit large volumes of unlabeled spatio-temporal data. We adopt a swappedprediction approach to anticipate the representation of the target subgraphfrom similar instances. Following the pre-training phase, the model isfine-tuned with accurate crowd flow data. Our experiments, conducted on tworeal-world datasets, demonstrate that the \model pre-trained on extensive noisydata consistently outperforms models trained from scratch.</description>
      <author>example@mail.com (Songyu Ke, Chenyu Wu, Yuxuan Liang, Xiuwen Yi, Yanping Sun, Junbo Zhang, Yu Zheng)</author>
      <guid isPermaLink="false">2508.14083v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition</title>
      <link>http://arxiv.org/abs/2508.14889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为多骨架对比学习(MS-CLR)的自监督框架，通过处理多种骨架约定来提高基于骨架的动作识别性能。&lt;h4&gt;背景&lt;/h4&gt;对比学习在基于骨架的动作识别领域受到关注，但现有方法依赖单一骨架约定，限制了跨数据集的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的自监督框架，能够对齐不同骨架 convention 的姿态表示，提高模型在多样化数据集上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出MS-CLR框架，通过统一表示方案处理不同关节布局和尺度的骨架，并适配ST-GCN架构以处理多骨架数据。&lt;h4&gt;主要发现&lt;/h4&gt;在NTU RGB+D 60和120数据集上，MS-CLR相比单一骨架对比学习基线方法持续提高性能，多骨架集成进一步提升了性能，达到新的最先进水平。&lt;h4&gt;结论&lt;/h4&gt;MS-CLR能够学习结构不变性并捕获多样化解剖线索，产生更具表达力和泛化能力的特征。&lt;h4&gt;翻译&lt;/h4&gt;对比学习在基于骨架的动作识别领域获得了显著关注，因为它能够从未标记数据中学习鲁棒表示。然而，现有方法依赖于单一的骨架约定，这限制了它们在不同具有不同关节结构和解剖覆盖的数据集上的泛化能力。我们提出多骨架对比学习(MS-CLR)，一种通用的自监督框架，该框架对齐从同一序列中提取的多种骨架 convention 的姿态表示。这鼓励模型学习结构不变性并捕获多样化的解剖线索，从而产生更具表达力和泛化能力的特征。为此，我们调整了ST-GCN架构，通过统一表示方案处理具有不同关节布局和尺度的骨架。在NTU RGB+D 60和120数据集上的实验表明，MS-CLR相比强大的单一骨架对比学习基线方法能够持续提高性能。多骨架集成进一步提高了性能，在这两个数据集上都设置了新的最先进结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的问题是现有基于骨架的人体动作识别方法依赖于单一骨架格式，限制了模型在不同骨架结构和解剖覆盖度的数据集上的泛化能力。这个问题在现实中很重要，因为不同动作捕捉系统使用不同的骨架格式（如Kinectv2有25个关节点，SMPL-X有42个关节点），且随着隐私问题日益突出，仅使用骨架数据的动作识别变得越来越重要，但现有方法仍受限于特定骨架格式。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有的对比学习方法（如AimCLR、ActCLR）虽有效但都依赖单一骨架格式，意识到不同骨架格式提供了结构多样性可作为监督信号。他们借鉴了ST-GCN作为骨干网络，MoCo v2的对比学习框架，AimCLR的数据增强策略，以及MeTRAbs姿态估计器来生成多种骨架格式，设计了多骨架对比学习框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多种骨架格式作为'结构增强'，通过对比学习对齐不同骨架格式中同一动作的表示，让模型学习结构不变性。整体流程包括：1)从RGB视频提取多种骨架格式；2)通过零填充将不同骨架映射到统一空间；3)使用改进的ST-GCN提取特征；4)使用多骨架对比损失对齐表示；5)采用动量对比学习框架训练；6)推理时可单独使用共享表示或使用骨架特定分类器集成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多骨架对比学习框架，利用多种骨架格式作为结构增强；2)统一的骨架ST-GCN，能处理多种骨架格式；3)骨架特定分类器集成策略。相比之前的工作，MS-CLR不再依赖单一骨架格式，而是使用不同骨架格式作为'结构增强'而非传统数据增强，学习结构不变的表示而非特定骨架表示，从而显著提高了模型的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MS-CLR通过利用多种骨架格式的结构多样性进行对比学习，显著提高了人体动作识别模型的鲁棒性和泛化能力，并在多个基准数据集上取得了最先进的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning has gained significant attention in skeleton-basedaction recognition for its ability to learn robust representations fromunlabeled data. However, existing methods rely on a single skeleton convention,which limits their ability to generalize across datasets with diverse jointstructures and anatomical coverage. We propose Multi-Skeleton ContrastiveLearning (MS-CLR), a general self-supervised framework that aligns poserepresentations across multiple skeleton conventions extracted from the samesequence. This encourages the model to learn structural invariances and capturediverse anatomical cues, resulting in more expressive and generalizablefeatures. To support this, we adapt the ST-GCN architecture to handle skeletonswith varying joint layouts and scales through a unified representation scheme.Experiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLRconsistently improves performance over strong single-skeleton contrastivelearning baselines. A multi-skeleton ensemble further boosts performance,setting new state-of-the-art results on both datasets.</description>
      <author>example@mail.com (Mert Kiray, Alvaro Ritter, Nassir Navab, Benjamin Busam)</author>
      <guid isPermaLink="false">2508.14889v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Contrastive Link Prediction With Edge Balancing Augmentation</title>
      <link>http://arxiv.org/abs/2508.14808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的链接预测方法CoEBA，通过理论分析和图增强技术解决了现有对比学习方法在链接预测中的两个主要弱点。&lt;h4&gt;背景&lt;/h4&gt;链接预测是图挖掘中最基础的任务之一，近年来有研究使用对比学习来增强其性能，但存在缺乏理论分析和未充分考虑节点度的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有对比学习方法在链接预测中的两个主要弱点：缺乏理论分析和未充分考虑节点度。&lt;h4&gt;方法&lt;/h4&gt;提供对比学习在链接预测上的首个正式理论分析，提出Edge Balancing Augmentation (EBA)图增强方法，以及Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA)方法。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析结果可推广到基于自编码器的链接预测模型，提出的CoEBA方法在8个基准数据集上显著优于其他最先进的链接预测模型。&lt;h4&gt;结论&lt;/h4&gt;通过理论分析和创新的图增强方法，CoEBA有效提升了链接预测性能。&lt;h4&gt;翻译&lt;/h4&gt;链接预测是图挖掘中最基础的任务之一，这促使了近期利用对比学习增强性能的研究。然而，我们观察到这些研究存在两个主要弱点：i)缺乏对比学习在链接预测上的理论分析，ii)对比学习中未充分考虑节点度。为解决上述弱点，我们首次提供了对比学习在链接预测上的正式理论分析，我们的分析结果可推广到带有对比学习的自编码器基础链接预测模型。受分析结果启发，我们提出了一种新的图增强方法，边平衡增强(EBA)，它通过调整节点度进行图增强。然后，我们提出了一种新方法，名为边平衡增强对比链接预测(CoEBA)，它整合了提出的EBA和新提出的对比损失来提高模型性能。我们在8个基准数据集上进行了实验。结果表明，我们提出的CoEBA显著优于其他最先进的链接预测模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction is one of the most fundamental tasks in graph mining, whichmotivates the recent studies of leveraging contrastive learning to enhance theperformance. However, we observe two major weaknesses of these studies: i) thelack of theoretical analysis for contrastive learning on link prediction, andii) inadequate consideration of node degrees in contrastive learning. Toaddress the above weaknesses, we provide the first formal theoretical analysisfor contrastive learning on link prediction, where our analysis results cangeneralize to the autoencoder-based link prediction models with contrastivelearning. Motivated by our analysis results, we propose a new graphaugmentation approach, Edge Balancing Augmentation (EBA), which adjusts thenode degrees in the graph as the augmentation. We then propose a new approach,named Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA),that integrates the proposed EBA and the proposed new contrastive losses toimprove the model performance. We conduct experiments on 8 benchmark datasets.The results demonstrate that our proposed CoEBA significantly outperforms theother state-of-the-art link prediction models.</description>
      <author>example@mail.com (Chen-Hao Chang, Hui-Ju Hung, Chia-Hsun Lu, Chih-Ya Shen)</author>
      <guid isPermaLink="false">2508.14808v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.14574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的神经手语生成模型，通过四元数空间骨骼旋转编码和语义对比损失提高对手势变异的鲁棒性&lt;h4&gt;背景&lt;/h4&gt;神经手语生成面临的主要挑战是手势类别内的高变异性，这源于手语者的形态特征和训练数据中的风格多样性&lt;h4&gt;目的&lt;/h4&gt;提高模型对这类变异的鲁棒性，改善手语生成的准确性和清晰度&lt;h4&gt;方法&lt;/h4&gt;对标准Progressive Transformers架构提出两种增强：1)使用四元数空间的骨骼旋转编码姿势并采用测地线损失训练；2)引入对比损失通过语义相似性结构化解码器嵌入，过滤无关的解剖和风格特征&lt;h4&gt;主要发现&lt;/h4&gt;在Phoenix14T数据集上，对比损失单独使用可使关键点正确概率比PT基线提高16%；结合四元数姿势编码后，模型实现了骨骼角度平均误差减少6%&lt;h4&gt;结论&lt;/h4&gt;将骨骼结构建模和语义引导的对比目标纳入Transformer-based SLP模型的训练中，对手势姿势表示有显著益处&lt;h4&gt;翻译&lt;/h4&gt;神经手语生成的主要挑战之一在于手势类别内的高变异性，这源于手语者的形态特征和训练数据中的风格多样性。为提高对此类变异的鲁棒性，我们对标准Progressive Transformers架构提出了两种改进。首先，我们使用四元数空间的骨骼旋转来编码姿势，并通过测地线损失进行训练，以提高关节角度运动的准确性和清晰度。其次，我们引入对比损失，通过语义相似性结构化解码器嵌入，使用词汇重叠或基于SBERT的句子相似性，旨在过滤掉不传达相关语义信息的解剖和风格特征。在Phoenix14T数据集上，仅对比损失就比PT基线在关键点正确概率上提高了16%。当结合基于四元数的姿势编码时，模型实现了骨骼角度平均误差减少6%。这些结果表明，将骨骼结构建模和语义引导的对比目标纳入基于Transformer的SLP模型的训练中，对手势姿势表示有益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One of the main challenges in neural sign language production (SLP) lies inthe high intra-class variability of signs, arising from signer morphology andstylistic variety in the training data. To improve robustness to suchvariations, we propose two enhancements to the standard ProgressiveTransformers (PT) architecture (Saunders et al., 2020). First, we encode posesusing bone rotations in quaternion space and train with a geodesic loss toimprove the accuracy and clarity of angular joint movements. Second, weintroduce a contrastive loss to structure decoder embeddings by semanticsimilarity, using either gloss overlap or SBERT-based sentence similarity,aiming to filter out anatomical and stylistic features that do not conveyrelevant semantic information. On the Phoenix14T dataset, the contrastive lossalone yields a 16% improvement in Probability of Correct Keypoint over the PTbaseline. When combined with quaternion-based pose encoding, the model achievesa 6% reduction in Mean Bone Angle Error. These results point to the benefit ofincorporating skeletal structure modeling and semantically guided contrastiveobjectives on sign pose representations into the training of Transformer-basedSLP models.</description>
      <author>example@mail.com (Guilhem Fauré, Mostafa Sadeghi, Sam Bigeard, Slim Ouni)</author>
      <guid isPermaLink="false">2508.14574v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever</title>
      <link>http://arxiv.org/abs/2508.14323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种行为对齐检索器(BAR)来提高增强型大型语言模型(LLMs)的工具使用准确性，通过提供行为一致的演示样本，减少错误的函数调用，同时保持高任务性能，实现经济高效的解决方案。&lt;h4&gt;背景&lt;/h4&gt;增强型大型语言模型利用外部函数扩展能力，但函数调用不准确会导致效率低下和成本增加。现有方法通过微调LLMs或基于提示的演示来解决这个问题，但它们通常面临高训练开销的问题，并且无法处理不一致的演示样本，这些样本会误导模型的调用行为。&lt;h4&gt;目的&lt;/h4&gt;训练一个行为对齐检索器(BAR)，提供行为一致的演示，以帮助LLMs做出更准确的工具使用决策。&lt;h4&gt;方法&lt;/h4&gt;构建一个包含不同函数调用行为(调用或不调用)的语料库，使用对比学习框架训练BAR，采用定制的正/负对和双负对比损失，确保行为一致例子的稳健检索。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法显著减少了错误的函数调用，同时保持了高任务性能，为增强型LLMs提供了一种经济高效的解决方案。&lt;h4&gt;结论&lt;/h4&gt;行为对齐检索器(BAR)能够有效提高增强型LLMs的工具使用准确性，减少错误函数调用，同时保持高任务性能，是一种成本效益高的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;增强工具的大型语言模型利用外部函数扩展其能力，但不准确的函数调用会导致效率低下和成本增加。现有方法通过微调LLMs或使用基于演示的提示来解决这一挑战，但它们通常遭受高训练开销的困扰，并且无法考虑不一致的演示样本，这些样本会误导模型的调用行为。在本文中，我们训练了一个行为对齐检索器(BAR)，它提供行为一致的演示，帮助LLMs做出更准确的工具使用决策。为了训练BAR，我们构建了一个包含不同函数调用行为(即调用或不调用)的语料库。我们使用对比学习框架，通过定制的正/负对和双负对比损失来训练BAR，确保行为一致例子的稳健检索。实验证明，我们的方法显著减少了错误的函数调用，同时保持了高任务性能，为增强工具的LLMs提供了一种经济高效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tool-augmented large language models (LLMs) leverage external functions toextend their capabilities, but inaccurate function calls can lead toinefficiencies and increased costs.Existing methods address this challenge byfine-tuning LLMs or using demonstration-based prompting, yet they often sufferfrom high training overhead and fail to account for inconsistent demonstrationsamples, which misguide the model's invocation behavior. In this paper, wetrained a behavior-aligned retriever (BAR), which provides behaviorallyconsistent demonstrations to help LLMs make more accurate tool-using decisions.To train the BAR, we construct a corpus including different function-callingbehaviors, i.e., calling or non-calling.We use the contrastive learningframework to train the BAR with customized positive/negative pairs and adual-negative contrastive loss, ensuring robust retrieval of behaviorallyconsistent examples.Experiments demonstrate that our approach significantlyreduces erroneous function calls while maintaining high task performance,offering a cost-effective and efficient solution for tool-augmented LLMs.</description>
      <author>example@mail.com (Yixin Chen, Ying Xiong, Shangyu Wu, Yufei Cui, Xue Liu, Nan Guan, Chun Jason Xue)</author>
      <guid isPermaLink="false">2508.14323v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2508.14278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GALA框架，一种基于3D高斯飞溅的开词汇表3D场景理解方法，通过自监督对比学习和交叉注意模块实现了从2D图像中捕捉细粒度、语言感知的3D表示。&lt;h4&gt;背景&lt;/h4&gt;3D场景重建和理解技术日益受到关注，但现有方法难以从2D图像中有效捕捉细粒度且具有语言感知能力的3D表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖的框架，用于实现开词汇表3D场景理解，能够支持2D和3D的语义查询，同时减少内存消耗。&lt;h4&gt;方法&lt;/h4&gt;GALA通过自监督对比学习蒸馏场景特定的3D实例特征场，并引入核心贡献——一个具有两个可学习码本的交叉注意模块，用于编码视图无关的语义嵌入，确保实例内特征相似性并支持无缝的2D和3D开词汇表查询。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的大量实验证明，GALA在2D和3D开词汇表任务上展现出卓越的性能，同时避免了每个高维特征学习，有效减少了内存消耗。&lt;h4&gt;结论&lt;/h4&gt;GALA框架成功解决了现有方法在捕捉细粒度、语言感知3D表示方面的挑战，为开词汇表3D场景理解提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;三维场景重建和理解日益受到关注，但现有方法仍难以从二维图像中捕捉细粒度、语言感知的三维表示。在本文中，我们提出了GALA，一种基于三维高斯飞溅的开词汇表三维场景理解新框架。GALA通过自监督对比学习蒸馏场景特定的三维实例特征场。为了扩展到通用语言特征场，我们引入了GALA的核心贡献——一个具有两个可学习码本的交叉注意模块，用于编码视图无关的语义嵌入。这种设计不仅确保了实例内特征相似性，还支持无缝的二维和三维开词汇表查询。它通过避免每个高维特征学习来减少内存消耗。在真实世界数据集上的大量实验证明了GALA在二维和三维开词汇表任务上的卓越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景理解中的开放词汇场景理解问题，即在3D场景中实现细粒度的语言感知表示，同时支持2D和3D的开放词汇查询。这个问题在现实中非常重要，因为3D场景理解是自动驾驶、机器人技术和AR/VR等应用的核心挑战，开放词汇能力能让机器人通过自然语言与人类交互，理解复杂场景，而现有的方法要么专注于2D或3D中的一方面，要么存在信息损失和特征一致性问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行设计：首先认识到NeRF方法计算效率低，3DGS方法在语义特征存储上存在问题；然后借鉴了Scaffold-GS进行场景重建，使用SAM生成实例掩码，采用CLIP进行语言对齐，并参考了codebook设计思路。作者采用两阶段训练策略：第一阶段自监督重建场景几何和实例特征场；第二阶段引入双码本和引导交叉注意力模块，将场景特征映射到语言特征空间，确保实例内特征一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过码本存储每个实例的语义嵌入而非每个高斯的语义特征，使用引导注意力机制实现场景特征到语言特征的映射，并确保实例内特征一致性。整体流程分为两阶段：第一阶段使用Scaffold-GS重建场景，生成几何和分割特征，通过对比学习增强实例内一致性；第二阶段构建实例码本和语言码本，使用交叉注意力机制将特征映射到语言空间，并通过注意力权重和熵损失确保一对一映射。推理时支持2D渲染和3D直接查询。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双码本设计减少内存消耗并确保实例语义一致性；2)引导交叉注意力模块实现场景特征到语言特征的无缝映射；3)注意力加权熵损失确保码本与实例的一对一映射；4)自监督对比学习增强实例内特征一致性。相比之前工作：不同于LangSplat的压缩方法避免信息损失；不同于OpenGaussian的KNN聚类避免实例分割错误；不同于SuperGSeg的MLP预测使用更灵活的注意力机制；不同于GOI的离散码本索引提供更细粒度语义控制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GALA通过双码本设计和引导交叉注意力机制，实现了高效的开放词汇3D场景理解，同时保持2D和3D查询能力并显著提升实例内特征一致性，为机器人和AR/VR应用提供了更精确的语义理解工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D scene reconstruction and understanding have gained increasing popularity,yet existing methods still struggle to capture fine-grained, language-aware 3Drepresentations from 2D images. In this paper, we present GALA, a novelframework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting(3DGS). GALA distills a scene-specific 3D instance feature field viaself-supervised contrastive learning. To extend to generalized language featurefields, we introduce the core contribution of GALA, a cross-attention modulewith two learnable codebooks that encode view-independent semantic embeddings.This design not only ensures intra-instance feature similarity but alsosupports seamless 2D and 3D open-vocabulary queries. It reduces memoryconsumption by avoiding per-Gaussian high-dimensional feature learning.Extensive experiments on real-world datasets demonstrate GALA's remarkableopen-vocabulary performance on both 2D and 3D.</description>
      <author>example@mail.com (Elena Alegret Regalado, Kunyi Li, Sen Wang, Siyun Liang, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari)</author>
      <guid isPermaLink="false">2508.14278v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Fracture Detection and Localisation in Wrist and Hand Radiographs using Detection Transformer Variants</title>
      <link>http://arxiv.org/abs/2508.14129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究应用目标检测Transformer模型改进手腕和手部骨折的X光片诊断，开发了基于Co-DETR的高精度检测流程，在真实世界X光片上达到83.1%的准确率，具有临床实用价值。&lt;h4&gt;背景&lt;/h4&gt;准确诊断手腕和手部骨折对急诊护理至关重要，但手动解读X光片速度慢且易出错。基于Transformer的模型在医学图像分析中显示出潜力，但在四肢骨折方面的应用有限。&lt;h4&gt;目的&lt;/h4&gt;应用目标检测Transformer到手腕和手的X光片，填补其在骨折诊断中的应用空白。&lt;h4&gt;方法&lt;/h4&gt;使用超过26,000张带有标注的临床数据集X光片对RT-DETR和Co-DETR模型进行微调；训练ResNet-50分类器对裁剪区域进行异常分类细化；采用监督对比学习增强嵌入质量；使用AP@50、精确度和召回率指标进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;RT-DETR表现中等（AP@50 = 0.39），而Co-DETR表现更优（AP@50 = 0.615）且收敛更快；集成流程在真实世界X光片上达到83.1%的准确率、85.1%的精确度和96.4%的召回率；在13种骨折类型中显示出强大的泛化能力；视觉检查确认了准确的定位。&lt;h4&gt;结论&lt;/h4&gt;基于Co-DETR的流程在手腕和手部骨折检测中表现出高准确率和临床相关性，提供可靠的骨折定位和类型区分，可扩展、高效，适合在医院工作流程中实时部署，提高肌肉骨骼放射学中的诊断速度和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;背景：使用X光片准确诊断手腕和手部骨折在急诊护理中至关重要，但手动解读速度慢且容易出错。基于Transformer的模型在改善医学图像分析方面显示出潜力，但其在四肢骨折中的应用有限。本研究通过将目标检测Transformer应用于手腕和手部X光片解决了这一空白。方法：我们使用超过26,000张来自专有临床数据集的标注X光片对在COCO上预训练的RT-DETR和Co-DETR模型进行了微调。每张图像都标有骨折存在情况和边界框。在裁剪区域上训练了ResNet-50分类器以细化异常分类。使用监督对比学习增强嵌入质量。使用AP@50、精确度和召回率指标进行性能评估，并在真实世界X光片上进行额外测试。结果：RT-DETR显示中等结果（AP@50 = 0.39），而Co-DETR表现更好，AP@50为0.615，收敛更快。集成流程在真实世界X光片上达到83.1%的准确率、85.1%的精确度和96.4%的召回率，在13种骨折类型中显示出强大的泛化能力。视觉检查确认了准确的定位。结论：我们的基于Co-DETR的流程在手腕和手部骨折检测中表现出高准确率和临床相关性，提供可靠的定位和骨折类型区分。它可扩展、高效，适合在医院工作流程中实时部署，提高肌肉骨骼放射学中的诊断速度和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background: Accurate diagnosis of wrist and hand fractures using radiographsis essential in emergency care, but manual interpretation is slow and prone toerrors. Transformer-based models show promise in improving medical imageanalysis, but their application to extremity fractures is limited. This studyaddresses this gap by applying object detection transformers to wrist and handX-rays.  Methods: We fine-tuned the RT-DETR and Co-DETR models, pre-trained on COCO,using over 26,000 annotated X-rays from a proprietary clinical dataset. Eachimage was labeled for fracture presence with bounding boxes. A ResNet-50classifier was trained on cropped regions to refine abnormality classification.Supervised contrastive learning was used to enhance embedding quality.Performance was evaluated using AP@50, precision, and recall metrics, withadditional testing on real-world X-rays.  Results: RT-DETR showed moderate results (AP@50 = 0.39), while Co-DETRoutperformed it with an AP@50 of 0.615 and faster convergence. The integratedpipeline achieved 83.1% accuracy, 85.1% precision, and 96.4% recall onreal-world X-rays, demonstrating strong generalization across 13 fracturetypes. Visual inspection confirmed accurate localization.  Conclusion: Our Co-DETR-based pipeline demonstrated high accuracy andclinical relevance in wrist and hand fracture detection, offering reliablelocalization and differentiation of fracture types. It is scalable, efficient,and suitable for real-time deployment in hospital workflows, improvingdiagnostic speed and reliability in musculoskeletal radiology.</description>
      <author>example@mail.com (Aditya Bagri, Vasanthakumar Venugopal, Anandakumar D, Revathi Ezhumalai, Kalyan Sivasailam, Bargava Subramanian, VarshiniPriya, Meenakumari K S, Abi M, Renita S)</author>
      <guid isPermaLink="false">2508.14129v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification</title>
      <link>http://arxiv.org/abs/2508.14779v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages,6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究首次系统分析了病理学基础模型(PFMs)中的医院来源偏差问题，提出了一种轻量级对抗框架来减少模型对特定医院特征的依赖，实验证明该方法能有效降低领域可预测性同时保持或提高疾病分类性能。&lt;h4&gt;背景&lt;/h4&gt;病理学基础模型(PFMs)在全切片图像(WSI)诊断方面表现出巨大潜力，但不同医院的病理图像因扫描硬件和预处理方式的差异而有所不同，可能导致PFMs无意中学习到特定医院的特征，对临床应用构成风险。&lt;h4&gt;目的&lt;/h4&gt;对由医院来源特征引起的PFMs中的领域偏差进行首次系统性研究，构建量化领域偏差的流程，评估和比较多个模型的性能，并提出一种轻量级对抗框架移除隐含的医院特定特征。&lt;h4&gt;方法&lt;/h4&gt;提出一种轻量级对抗框架，在不修改编码器本身的情况下，通过引入可训练适配器和通过梯度反转层(GRL)连接的领域分类器，学习具有任务判别性但领域不变性的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多中心组织病理学数据集上的实验表明，该方法显著降低了领域可预测性，同时保持或提高了疾病分类性能，特别是在跨域(未见过的医院)场景中；医院检测和特征空间可视化分析证实了该方法在减轻医院偏差方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效地减轻了PFMs中的医院偏差问题，在保持或提高疾病分类性能的同时，减少了模型对特定医院特征的依赖，有望促进PFMs在临床环境中的部署和应用。&lt;h4&gt;翻译&lt;/h4&gt;病理学基础模型(PFMs)在全切片图像(WSI)诊断中展现出巨大潜力。然而，不同医院的病理图像常因扫描硬件和预处理风格的差异而有所不同，这可能导致PFMs无意中学习到特定医院的特征，对临床部署构成风险。在本工作中，我们首次对源于医院来源特征的PFMs领域偏差进行了系统性研究。具体而言，我们构建了一个量化PFMs中领域偏差的流程，评估和比较了多个模型的性能，并提出了一个轻量级对抗框架，在不修改编码器本身的情况下，从冻结表示中移除隐含的医院特定特征。通过引入一个可训练的适配器和一个通过梯度反转层(GRL)连接的领域分类器，我们的方法学习具有任务判别性但领域不变性的表示。在多中心组织病理学数据集上的实验表明，我们的方法显著降低了领域可预测性，同时保持甚至提高了疾病分类性能，特别是在跨域(未见医院)场景中。包括医院检测和特征空间可视化在内的进一步分析证实了我们的方法在减轻医院偏差方面的有效性。我们将在接受后提供代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pathology foundation models (PFMs) have demonstrated remarkable potential inwhole-slide image (WSI) diagnosis. However, pathology images from differenthospitals often vary due to differences in scanning hardware and preprocessingstyles, which may lead PFMs to inadvertently learn hospital-specific features,posing risks for clinical deployment. In this work, we present the firstsystematic study of domain bias in PFMs arising from hospital sourcecharacteristics. Specifically, we (1) construct a pipeline for quantifyingdomain bias in PFMs, (2) evaluate and compare the performance of multiplemodels, and (3) propose a lightweight adversarial framework that removes latenthospital-specific features from frozen representations without modifying theencoder itself. By introducing a trainable adapter and a domain classifierconnected through a gradient reversal layer (GRL), our method learnstask-discriminative yet domain-invariant representations. Experiments onmulti-center histopathology datasets demonstrate that our approachsubstantially reduces domain predictability while maintaining or even improvingdisease classification performance, particularly in out-of-domain (unseenhospital) scenarios. Further analyses, including hospital detection and featurespace visualization, confirm the effectiveness of our method in mitigatinghospital bias. We will provide our code based on acceptance.</description>
      <author>example@mail.com (Mengliang Zhang, Jacob M. Luber)</author>
      <guid isPermaLink="false">2508.14779v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal</title>
      <link>http://arxiv.org/abs/2508.14689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的基础模型ECHO，用于通用机器信号建模，解决了现有方法在输入长度限制和频率位置编码方面的不足，并在多个数据集上展示了优越的性能。&lt;h4&gt;背景&lt;/h4&gt;预训练基础模型在视觉和语言领域取得了显著成功，但在通用机器信号建模（包括声学、振动和其他工业传感器数据）方面的潜力尚未充分探索。现有的基于子带编码器的方法虽取得有竞争力的结果，但受限于固定输入长度和缺乏显式的频率位置编码。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基础模型，结合先进的带分裂架构和相对频率位置嵌入，实现在任意采样配置下精确的频谱定位，并支持任意长度的输入。&lt;h4&gt;方法&lt;/h4&gt;开发了一种集成先进带分裂架构和相对频率位置嵌入的基础模型，该模型支持任意长度的输入而不需要填充或分割，能够产生保留时间和频谱保真度的简洁嵌入表示。&lt;h4&gt;主要发现&lt;/h4&gt;在SIREN（统一了多个数据集的大型机器信号编码基准测试）上的实验结果表明，该方法在异常检测和故障识别方面持续达到最先进性能，证明了模型的有效性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的ECHO模型为通用机器信号建模提供了有效解决方案，已在GitHub上开源，可供进一步研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;预训练基础模型在视觉和语言领域已展现出显著的成功，然而它们在通用机器信号建模（涵盖声学、振动和其他工业传感器数据）方面的潜力仍未得到充分探索。现有的基于子带编码器的方法已取得有竞争力的结果，但受限于固定的输入长度和缺乏显式的频率位置编码。在这项工作中，我们提出了一种新的基础模型，它集成了先进的带分裂架构和相对频率位置嵌入，能够在任意采样配置下实现精确的频谱定位。该模型支持任意长度的输入而无需填充或分割，产生保留时间和频谱保真度的简洁嵌入。我们在SIREN（一个新引入的大型机器信号编码基准测试，统一了多个数据集，包括所有DCASE任务2挑战赛（2020-2025）和广泛使用的工业信号语料库）上评估了我们的方法。实验结果证明了该方法在异常检测和故障识别方面持续达到最先进性能，确认了所提出模型的有效性和泛化能力。我们在https://github.com/yucongzh/ECHO上开源了ECHO。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained foundation models have demonstrated remarkable success in visionand language, yet their potential for general machine signal modeling-coveringacoustic, vibration, and other industrial sensor data-remains under-explored.Existing approach using sub-band-based encoders has achieved competitiveresults but are limited by fixed input lengths, and the absence of explicitfrequency positional encoding. In this work, we propose a novel foundationmodel that integrates an advanced band-split architecture with relativefrequency positional embeddings, enabling precise spectral localization acrossarbitrary sampling configurations. The model supports inputs of arbitrarylength without padding or segmentation, producing a concise embedding thatretains both temporal and spectral fidelity. We evaluate our method on SIREN(https://github.com/yucongzh/SIREN), a newly introduced large-scale benchmarkfor machine signal encoding that unifies multiple datasets, including all DCASEtask 2 challenges (2020-2025) and widely-used industrial signal corpora.Experimental results demonstrate consistent state-of-the-art performance inanomaly detection and fault identification, confirming the effectiveness andgeneralization capability of the proposed model. We open-sourced ECHO onhttps://github.com/yucongzh/ECHO.</description>
      <author>example@mail.com (Yucong Zhang, Juan Liu, Ming Li)</author>
      <guid isPermaLink="false">2508.14689v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels</title>
      <link>http://arxiv.org/abs/2508.14563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GOGS的新型两阶段逆渲染框架，基于2D高斯表面元，解决了从RGB图像重建高光物体时存在的模糊性、计算成本高和多视图不一致等问题，实现了几何重建、材料分离和新照明下的照片级真实感重照明的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;从RGB图像进行高光物体的逆渲染存在根本性模糊性限制；基于NeRF的方法计算成本过高；3D高斯飞溅技术在镜面反射方面存在局限性；多视图不一致性引入高频表面噪声和结构伪影；简化的渲染方程掩盖材料属性，导致不合理的重照明结果。&lt;h4&gt;目的&lt;/h4&gt;解决逆渲染中的各种挑战，提出一种新的框架GOGS，基于2D高斯表面元，实现更高效、更准确的几何重建、材料分离和重照明。&lt;h4&gt;方法&lt;/h4&gt;提出GOGS，一个基于2D高斯表面元的新型两阶段框架：第一阶段通过基于物理的渲染和分割和近似建立稳健的表面重建，并利用基础模型的几何先验进行增强；第二阶段通过利用完整渲染方程的蒙特卡罗重要性采样进行材料分解，通过可微分的2D高斯光线追踪建模间接照明，并通过基于球面mipmap的方向编码捕获各向异性高光来细化高频镜面细节。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验证明，GOGS在几何重建、材料分离和新照明下的照片级真实感重照明方面实现了最先进的性能，优于现有的逆渲染方法。&lt;h4&gt;结论&lt;/h4&gt;GOGS框架成功解决了逆渲染中的关键挑战，特别是在处理高光物体时，在几何重建、材料分离和重照明方面表现出色，为逆渲染领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从RGB图像进行高光物体的逆渲染仍然受到固有模糊性的根本限制。尽管基于NeRF的方法通过密集光线采样实现了高保真重建，但其计算成本过高。最近的3D高斯飞溅技术实现了高效的重建，但在镜面反射方面存在局限性。多视图不一致性引入高频表面噪声和结构伪影，而简化的渲染方程掩盖了材料属性，导致不合理的重照明结果。为解决这些问题，我们提出了GOGS，一种基于2D高斯表面元的新型两阶段框架。首先，我们通过基于物理的渲染和分割和近似建立了稳健的表面重建，并通过基础模型的几何先验进行增强。其次，我们通过利用完整渲染方程的蒙特卡罗重要性采样进行材料分解，通过可微分的2D高斯光线追踪建模间接照明，并通过基于球面mipmap的方向编码捕获各向异性高光来细化高频镜面细节。广泛的实验证明了在几何重建、材料分离和新照明下的照片级真实感重照明方面，优于现有逆渲染方法的最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决高光泽物体（如金属、陶瓷等）的逆渲染问题，即从多视角RGB图像中准确重建物体的几何形状和材料属性。这个问题在现实中很重要，因为高光泽物体在日常生活中很常见，准确重建它们对于产品展示、虚拟现实、数字孪生等应用至关重要。现有方法要么计算成本过高（如NeRF-based方法），要么在高光区域表现不佳（如3D高斯飞溅方法），导致重建结果出现噪声或伪影。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是高光物体重建中的多视图一致性问题。他们提出两阶段框架：先解决几何重建，再解决材料分解。几何重建阶段利用基础模型的几何先验减轻高光干扰；材料分解阶段采用完整渲染方程评估。该方法借鉴了多项现有工作：2D高斯飞溅用于表面表示，基础模型（如Marigold）提供几何先验，蒙特卡罗重要性采样减少计算方差，球体mipmap编码处理高光细节，以及IRGS的可微分2D高斯光线追踪技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用2D高斯飞溅作为基础表示，通过两阶段框架分别解决几何重建和材料分解问题，并引入基础模型几何先验和高光补偿机制。整体流程分为两个阶段：第一阶段使用基于物理的渲染与分裂和近似，结合基础模型的几何先验监督进行鲁棒的几何重建；第二阶段在固定几何基础上，通过蒙特卡罗重要性采样评估完整渲染方程，利用可微分2D高斯光线追踪计算可视性和间接辐射，并通过球体mipmap基础的高光补偿机制细化高频细节。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三方面：1）鲁棒的几何重建方法，利用几何先验和分裂和近似减轻高光干扰；2）基于物理的材料分解，通过完整渲染方程评估和蒙特卡罗重要性采样实现；3）自适应高光补偿机制，基于球体mipmap的方向编码细化高频细节。相比之前工作，该方法避免了NeRF的高计算成本，解决了3D高斯飞溅在高光区域的噪声问题，使用完整渲染方程而非简化版本提高了材料分解准确性，并引入基础模型先验解决了多视图一致性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GOGS通过结合2D高斯飞溅表示、基础模型几何先验监督和基于物理的完整渲染方程评估，实现了高光泽物体的高保真几何重建和精确材料分解，显著提升了新照明条件下的渲染质量和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inverse rendering of glossy objects from RGB imagery remains fundamentallylimited by inherent ambiguity. Although NeRF-based methods achievehigh-fidelity reconstruction via dense-ray sampling, their computational costis prohibitive. Recent 3D Gaussian Splatting achieves high reconstructionefficiency but exhibits limitations under specular reflections. Multi-viewinconsistencies introduce high-frequency surface noise and structuralartifacts, while simplified rendering equations obscure material properties,leading to implausible relighting results. To address these issues, we proposeGOGS, a novel two-stage framework based on 2D Gaussian surfels. First, weestablish robust surface reconstruction through physics-based rendering withsplit-sum approximation, enhanced by geometric priors from foundation models.Second, we perform material decomposition by leveraging Monte Carlo importancesampling of the full rendering equation, modeling indirect illumination viadifferentiable 2D Gaussian ray tracing and refining high-frequency speculardetails through spherical mipmap-based directional encoding that capturesanisotropic highlights. Extensive experiments demonstrate state-of-the-artperformance in geometry reconstruction, material separation, and photorealisticrelighting under novel illuminations, outperforming existing inverse renderingapproaches.</description>
      <author>example@mail.com (Xingyuan Yang, Min Wei)</author>
      <guid isPermaLink="false">2508.14563v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Locality-aware Concept Bottleneck Model</title>
      <link>http://arxiv.org/abs/2508.14562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 25 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了局部感知概念瓶颈模型(LCBM)框架，利用基础模型信息和原型学习确保概念在图像中的准确空间定位，有效识别图像中存在的概念并改进定位能力，同时保持相当好的分类性能。&lt;h4&gt;背景&lt;/h4&gt;概念瓶颈模型(CBMs)本质上是可解释的模型，基于人类可理解的视觉线索(概念)进行预测。通过人工标注获取密集概念标注是困难和昂贵的，最近的方法利用基础模型确定图像中的概念，但无标签CBM通常无法在相关区域定位概念，预测时会关注视觉上不相关的区域。&lt;h4&gt;目的&lt;/h4&gt;解决无标签CBM无法在相关区域定位概念的问题，确保概念在图像中的准确空间定位。&lt;h4&gt;方法&lt;/h4&gt;提出局部感知概念瓶颈模型(LCBM)框架，为每个概念分配一个原型来代表该概念的典型图像特征，通过鼓励原型编码相似的局部区域来学习这些原型，利用基础模型确保原型与概念的相关性，并使用原型促进识别每个概念应从哪个适当局部区域进行预测的学习过程。&lt;h4&gt;主要发现&lt;/h4&gt;LCBM能有效识别图像中存在的概念，改进了概念的定位能力，同时保持了相当好的分类性能。&lt;h4&gt;结论&lt;/h4&gt;LCBM框架有效解决了无标签CBM在概念定位上的问题，同时保持了良好的分类性能，提高了模型的可解释性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;概念瓶颈模型(CBMs)本质上是可解释的模型，它们基于人类可理解的视觉线索(称为概念)进行预测。由于通过人工标注获取密集的概念标注是困难和昂贵的，最近的方法利用基础模型来确定图像中存在的概念。然而，这种无标签的CBM通常无法在相关区域定位概念，在预测概念存在时会关注视觉上不相关的区域。为此，我们提出了一个框架，称为局部感知概念瓶颈模型(LCBM)，该框架利用基础模型的丰富信息并采用原型学习来确保概念的准确空间定位。具体来说，我们为每个概念分配一个原型，该原型被推广为代表该概念的典型图像特征。这些原型是通过鼓励它们编码相似的局部区域来学习的，利用基础模型确保每个原型与其关联概念的相关性。然后，我们使用原型来促进识别每个概念应从哪个适当的局部区域进行预测的学习过程。实验结果表明，LCBM能有效识别图像中存在的概念，并改进了定位能力，同时保持了相当好的分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concept bottleneck models (CBMs) are inherently interpretable models thatmake predictions based on human-understandable visual cues, referred to asconcepts. As obtaining dense concept annotations with human labeling isdemanding and costly, recent approaches utilize foundation models to determinethe concepts existing in the images. However, such label-free CBMs often failto localize concepts in relevant regions, attending to visually unrelatedregions when predicting concept presence. To this end, we propose a framework,coined Locality-aware Concept Bottleneck Model (LCBM), which utilizes richinformation from foundation models and adopts prototype learning to ensureaccurate spatial localization of the concepts. Specifically, we assign oneprototype to each concept, promoted to represent a prototypical image featureof that concept. These prototypes are learned by encouraging them to encodesimilar local regions, leveraging foundation models to assure the relevance ofeach prototype to its associated concept. Then we use the prototypes tofacilitate the learning process of identifying the proper local region fromwhich each concept should be predicted. Experimental results demonstrate thatLCBM effectively identifies present concepts in the images and exhibitsimproved localization while maintaining comparable classification performance.</description>
      <author>example@mail.com (Sujin Jeon, Hyundo Lee, Eungseo Kim, Sanghack Lee, Byoung-Tak Zhang, Inwoo Hwang)</author>
      <guid isPermaLink="false">2508.14562v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>DeepTelecom: A Digital-Twin Deep Learning Dataset for Channel and MIMO Applications</title>
      <link>http://arxiv.org/abs/2508.14507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepTelecom是一个三维数字孪生信道数据集，解决了现有无线AI数据集生成缓慢、保真度有限和场景类型狭窄的问题。&lt;h4&gt;背景&lt;/h4&gt;现有无线AI数据集生成缓慢，建模保真度有限，仅覆盖有限场景类型。&lt;h4&gt;目的&lt;/h4&gt;创建一个三维数字孪生信道数据集DeepTelecom，解决现有数据集的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用大语言模型辅助流程构建LoD3场景，基于Sionna的射线追踪引擎模拟无线电波传播，利用GPU加速流式传输射线路径轨迹和信号强度热图，编译成高帧率视频并输出多视图图像、信道张量和多尺度衰落轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;DeepTelecom能够高效流式传输大规模、高保真度和多模态信道数据。&lt;h4&gt;结论&lt;/h4&gt;DeepTelecom为无线AI研究提供统一基准，并支持基础模型将大模型智能与未来通信系统紧密融合。&lt;h4&gt;翻译&lt;/h4&gt;特定领域数据集是释放人工智能驱动的无线创新的基础。然而，现有的无线AI语料库生成缓慢，建模保真度有限，仅覆盖狭窄的场景类型。为解决这些挑战，我们创建了DeepTelecom，一个三维数字孪生信道数据集。具体而言，大语言模型辅助的流程首先构建具有可分割参数化表面的第三级细节的室外和室内场景。然后，DeepTelecom基于Sionna的射线追踪引擎模拟完整的无线电波传播效果。利用GPU加速，DeepTelecom流式传输射线路径轨迹和实时信号强度热图，将它们编译成高帧率视频，同时输出同步的多视图图像、信道张量和多尺度衰落轨迹。通过高效流式传输大规模、高保真度和多模态信道数据，DeepTelecom不仅为无线AI研究提供了统一的基准，还提供了领域丰富的训练基础，使基础模型能够将大模型智能与未来通信系统紧密融合。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有无线AI数据集生成缓慢、保真度有限和场景类型覆盖不足的问题。这个问题在6G通信发展中至关重要，因为6G将成为智能社会的核心基础设施，需要超高数据速率、超低延迟和极高可靠性，而AI与无线系统的融合依赖于高质量的大规模训练数据集。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有无线AI数据集的局限性：依赖CPU限制的射线追踪器导致生成慢、模态有限且需要繁琐校准、数字孪生模型局限于LoD1级别。他们考察了现有的开放或商业工具链如DeepMIMO、ViWi和NVIDIA Sionna，发现这些工具虽有GPU加速但未将材料感知重建与可微分射线追踪集成。基于此，作者创新性地结合了LLM辅助场景建模和Sionna的GPU加速射线追踪技术，设计了一个更完整高效的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个高保真度、多模态、LoD3级别的GPU加速射线追踪数据集，通过LLM辅助的管线构建详细的室内外环境数字孪生，并使用Sionna引擎模拟无线电波传播。整体流程包括四个主要步骤：1) LLM辅助的高保真多源场景建模，整合多种数据源并标注材料属性；2) 参数定义和配置，包括系统参数和生成参数；3) GPU加速射线追踪模拟，高效生成信道数据；4) 信道数据提取和后处理，计算标准信道表示并过滤无效数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) LLM辅助的高保真场景建模，优化材料标注一致性；2) 高效的GPU加速射线追踪，保持自动梯度能力并实现早期射线终止；3) 多模态数据输出，同时生成视频、图像、信道张量和衰落轨迹；4) 材料感知的LoD3级重建，每个表面都有明确的电磁材料属性；5) 系统性和灵活性，可配置多种参数并扩展到各种场景。相比之前工作，DeepTelecom将大语言模型、高保真3D建模、GPU加速射线追踪和多模态输出集成到统一工作流中，创建了更全面高效的无线AI数据集生成平台。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DeepTelecom通过结合大语言模型辅助的高保真场景建模和GPU加速的射线追踪技术，创建了一个前所未有的多模态、大规模、高保真度的无线信道数据集，为下一代智能无线通信研究提供了基础性资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain-specific datasets are the foundation for unleashing artificialintelligence (AI)-driven wireless innovation. Yet existing wireless AI corporaare slow to produce, offer limited modeling fidelity, and cover only narrowscenario types. To address the challenges, we create DeepTelecom, athree-dimension (3D) digital-twin channel dataset. Specifically, a largelanguage model (LLM)-assisted pipeline first builds the third level of details(LoD3) outdoor and indoor scenes with segmentable material-parameterizablesurfaces. Then, DeepTelecom simulates full radio-wave propagation effects basedon Sionna's ray-tracing engine. Leveraging GPU acceleration, DeepTelecomstreams ray-path trajectories and real-time signal-strength heat maps, compilesthem into high-frame-rate videos, and simultaneously outputs synchronizedmulti-view images, channel tensors, and multi-scale fading traces. Byefficiently streaming large-scale, high-fidelity, and multimodal channel data,DeepTelecom not only furnishes a unified benchmark for wireless AI research butalso supplies the domain-rich training substrate that enables foundation modelsto tightly fuse large model intelligence with future communication systems.</description>
      <author>example@mail.com (Bohao Wang, Zehua Jiang, Zhenyu Yang, Chongwen Huang, Yongliang Shen, Siming Jiang, Chen Zhu, Zhaohui Yang, Richeng Jin, Zhaoyang Zhang, Sami Muhaidat, Merouane Debbah)</author>
      <guid isPermaLink="false">2508.14507v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments</title>
      <link>http://arxiv.org/abs/2508.14504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为PB-IAD（基于提示的工业异常检测）的新型框架，利用基础模型的多模态和推理能力解决工业异常检测问题，特别关注数据稀疏、敏捷适应性和领域用户中心化三大需求。&lt;h4&gt;背景&lt;/h4&gt;制造业中异常检测对确保产品质量和识别工艺偏差至关重要。传统统计和数据驱动方法虽为标准，但受限于对大量标注数据集的依赖以及在动态生产条件下的有限适应性。&lt;h4&gt;目的&lt;/h4&gt;利用基础模型的感知能力，将其适应到工业异常检测这一下游任务，解决工业异常检测中面临的数据稀疏、敏捷适应性和领域用户中心化等关键需求。&lt;h4&gt;方法&lt;/h4&gt;提出PB-IAD框架，包括专门用于迭代实施领域特定工艺知识的提示模板，以及将领域用户输入转换为有效系统提示的预处理模块。使用GPT-4.1在三种不同制造场景和两种数据模态下进行评估，并通过消融研究评估语义指令贡献。&lt;h4&gt;主要发现&lt;/h4&gt;PB-IAD在性能上优于最先进的方法（如PatchCore），特别是在数据稀疏场景和少样本设置中，仅通过语义指令就实现了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;PB-IAD框架通过用户中心化设计，允许领域专家无需数据科学专业知识就能灵活定制系统，为工业异常检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;制造业中异常检测的发现对确保产品质量和识别工艺偏差至关重要。统计和数据驱动方法仍然是工业异常检测的标准，但其适应性和可用性受限于对大量标注数据集的依赖以及在动态生产条件下的有限灵活性。基础模型感知能力的最新进展为其适应这一下游任务提供了有希望的机会。本文提出了PB-IAD（基于提示的工业异常检测），一种利用基础模型多模态和推理能力进行工业异常检测的新框架。具体而言，PB-IAD解决了动态生产环境的三个关键需求：数据稀疏、敏捷适应性和领域用户中心化。除异常检测外，该框架还包括一个专门为迭代实施领域特定工艺知识而设计的提示模板，以及一个将领域用户输入转换为有效系统提示的预处理模块。这种以用户为中心的设计允许领域专家灵活定制系统而无需数据科学专业知识。所提出的框架利用GPT-4.1在三种不同的制造场景、两种数据模态下进行了评估，并通过消融研究系统评估了语义指令的贡献。此外，PB-IAD与PatchCore等最先进的异常检测方法进行了基准测试。结果表明，特别是在数据稀疏场景和少样本设置中，仅通过语义指令就实现了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The detection of anomalies in manufacturing processes is crucial to ensureproduct quality and identify process deviations. Statistical and data-drivenapproaches remain the standard in industrial anomaly detection, yet theiradaptability and usability are constrained by the dependence on extensiveannotated datasets and limited flexibility under dynamic production conditions.Recent advances in the perception capabilities of foundation models providepromising opportunities for their adaptation to this downstream task. Thispaper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novelframework that leverages the multimodal and reasoning capabilities offoundation models for industrial anomaly detection. Specifically, PB-IADaddresses three key requirements of dynamic production environments: datasparsity, agile adaptability, and domain user centricity. In addition to theanomaly detection, the framework includes a prompt template that isspecifically designed for iteratively implementing domain-specific processknowledge, as well as a pre-processing module that translates domain userinputs into effective system prompts. This user-centric design allows domainexperts to customise the system flexibly without requiring data scienceexpertise. The proposed framework is evaluated by utilizing GPT-4.1 acrossthree distinct manufacturing scenarios, two data modalities, and an ablationstudy to systematically assess the contribution of semantic instructions.Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomalydetection such as PatchCore. The results demonstrate superior performance,particularly in data-sparse scenarios and low-shot settings, achieved solelythrough semantic instructions.</description>
      <author>example@mail.com (Bernd Hofmann, Albert Scheck, Joerg Franke, Patrick Bruendl)</author>
      <guid isPermaLink="false">2508.14504v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration</title>
      <link>http://arxiv.org/abs/2508.14483v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Vivid-VR，一种基于DiT的生成式视频修复方法，利用ControlNet控制生成过程确保内容一致性。&lt;h4&gt;背景&lt;/h4&gt;传统可控管道微调常因多模态对齐不完美而遭受分布漂移，导致纹理真实性和时间一致性受损。&lt;h4&gt;目的&lt;/h4&gt;解决分布漂移问题，保持纹理和时间质量，同时增强生成可控性。&lt;h4&gt;方法&lt;/h4&gt;提出概念蒸馏训练策略，利用预训练T2V模型合成带有文本概念的训练样本；重新设计控制架构，包含控制特征投影器和双分支设计的ControlNet连接器。&lt;h4&gt;主要发现&lt;/h4&gt;Vivid-VR在合成和真实世界基准以及AIGC视频上优于现有方法，实现了优异的纹理真实性、视觉生动性和时间一致性。&lt;h4&gt;结论&lt;/h4&gt;通过概念蒸馏训练策略和重新设计的控制架构，Vivid-VR成功解决了传统可控管道微调中的分布漂移问题，提高了视频修复质量和可控性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Vivid-VR，一种基于DiT的生成式视频修复方法，它建立在先进的T2V基础模型之上，其中利用ControlNet控制生成过程，确保内容一致性。然而，这种可控管道的传统微调经常因多模态对齐不完美而遭受分布漂移，导致纹理真实性和时间一致性受损。为了解决这一挑战，我们提出了概念蒸馏训练策略，利用预训练T2V模型合成带有嵌入文本概念的训练样本，从而蒸馏其概念理解以保持纹理和时间质量。为了增强生成可控性，我们重新设计了包含两个关键组件的控制架构：1) 控制特征投影器，用于从输入视频潜在特征中过滤退化伪影，减少其在生成管道中的传播；2) 新的ControlNet连接器，采用双分支设计，该连接器协同结合基于MLP的特征映射和交叉注意力机制进行动态控制特征检索，实现内容保持和自适应控制信号调制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Vivid-VR, a DiT-based generative video restoration method builtupon an advanced T2V foundation model, where ControlNet is leveraged to controlthe generation process, ensuring content consistency. However, conventionalfine-tuning of such controllable pipelines frequently suffers from distributiondrift due to limitations in imperfect multimodal alignment, resulting incompromised texture realism and temporal coherence. To tackle this challenge,we propose a concept distillation training strategy that utilizes thepretrained T2V model to synthesize training samples with embedded textualconcepts, thereby distilling its conceptual understanding to preserve textureand temporal quality. To enhance generation controllability, we redesign thecontrol architecture with two key components: 1) a control feature projectorthat filters degradation artifacts from input video latents to minimize theirpropagation through the generation pipeline, and 2) a new ControlNet connectoremploying a dual-branch design. This connector synergistically combinesMLP-based feature mapping with cross-attention mechanism for dynamic controlfeature retrieval, enabling both content preservation and adaptive controlsignal modulation. Extensive experiments show that Vivid-VR performs favorablyagainst existing approaches on both synthetic and real-world benchmarks, aswell as AIGC videos, achieving impressive texture realism, visual vividness,and temporal consistency. The codes and checkpoints are publicly available athttps://github.com/csbhr/Vivid-VR.</description>
      <author>example@mail.com (Haoran Bai, Xiaoxu Chen, Canqian Yang, Zongyao He, Sibin Deng, Ying Chen)</author>
      <guid isPermaLink="false">2508.14483v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Pixels to Play: A Foundation Model for 3D Gameplay</title>
      <link>http://arxiv.org/abs/2508.14295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一个名为Pixels2Play-0.1（P2P0.1）的基础模型，该模型能够学习玩各种3D视频游戏，表现出类似人类的行为。&lt;h4&gt;背景&lt;/h4&gt;消费者和开发者有新兴的用例，如AI队友、可控制的NPC、个性化直播流和辅助测试工具，这促使研究人员开发一个能够像人类一样仅依靠像素流来玩游戏并能够泛化到新游戏中的智能体。&lt;h4&gt;目的&lt;/h4&gt;创建一个能够仅使用玩家可用的像素流来玩游戏的基础模型，并且能够最小化游戏特定的工程化工作，从而泛化到新游戏中。&lt;h4&gt;方法&lt;/h4&gt;使用行为克隆进行端到端训练，结合来自人类游戏游戏的标记演示数据和未标记的公共视频（通过逆动力学模型推断动作），采用仅解码器的transformer模型处理大的动作空间，并在单个消费级GPU上保持低延迟。&lt;h4&gt;主要发现&lt;/h4&gt;在简单的Roblox和经典的MS-DOS游戏中展示了熟练的游戏能力，进行了未标记数据的消融研究，并概述了达到专家级文本控制所需的扩展和评估步骤。&lt;h4&gt;结论&lt;/h4&gt;Pixels2Play-0.1模型展示了在多种游戏中表现出类似人类行为的潜力，但仍需要进一步扩展和评估才能达到专家水平。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了Pixels2Play-0.1（P2P0.1），一个基础模型，它学习玩各种3D视频游戏，表现出可识别的类人行为。受新兴的消费者和开发者用例启发——AI队友、可控制的NPC、个性化直播流、辅助测试工具——我们认为智能体必须依赖于玩家可用的相同像素流，并以最小的游戏特定工程化泛化到新游戏中。P2P0.1通过行为克隆进行端到端训练：从人类游戏游戏中收集的标记演示数据与未标记的公共视频相结合，我们通过逆动力学模型推断动作。一个仅解码器的transformer模型，具有自回归动作输出，能够处理大的动作空间，同时在单个消费级GPU上保持低延迟。我们报告了定性结果，展示了在简单的Roblox和经典MS-DOS游戏中的熟练游戏能力，对未标记数据的消融研究，并概述了达到专家级文本控制所需的扩展和评估步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to playa wide range of 3D video games with recognizable human-like behavior. Motivatedby emerging consumer and developer use cases - AI teammates, controllable NPCs,personalized live-streamers, assistive testers - we argue that an agent mustrely on the same pixel stream available to players and generalize to new titleswith minimal game-specific engineering. P2P0.1 is trained end-to-end withbehavior cloning: labeled demonstrations collected from instrumented humangame-play are complemented by unlabeled public videos, to which we imputeactions via an inverse-dynamics model. A decoder-only transformer withauto-regressive action output handles the large action space while remaininglatency-friendly on a single consumer GPU. We report qualitative resultsshowing competent play across simple Roblox and classic MS-DOS titles,ablations on unlabeled data, and outline the scaling and evaluation stepsrequired to reach expert-level, text-conditioned control.</description>
      <author>example@mail.com (Yuguang Yue, Chris Green, Samuel Hunt, Irakli Salia, Wenzhe Shi, Jonathan J Hunt)</author>
      <guid isPermaLink="false">2508.14295v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>RynnEC: Bringing MLLMs into Embodied World</title>
      <link>http://arxiv.org/abs/2508.14160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The technical report of RynnEC, an embodied cognition MLLM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队提出了RynnEC，一个专为具身认知设计的视频多模态大语言模型。该模型通过区域编码器和掩码解码器实现灵活的区域级别视频交互，在物体属性理解、物体分割和空间推理方面取得了最先进性能。研究还提出了生成具身认知数据的流水线和评估基准RynnEC-Bench。&lt;h4&gt;背景&lt;/h4&gt;具身智能体需要细粒度的物理世界感知和精确交互能力，但现有模型在区域级别视频交互方面存在局限，且标注3D数据集稀缺。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够提供细粒度物理世界感知并实现精确交互的视频多模态大语言模型，推动具身智能体通用认知核心的发展。&lt;h4&gt;方法&lt;/h4&gt;基于通用视觉语言基础模型构建RynnEC，整合区域编码器和掩码解码器；提出基于自我中心视频的流水线生成具身认知数据；创建RynnEC-Bench评估基准。&lt;h4&gt;主要发现&lt;/h4&gt;尽管架构紧凑，RynnEC在物体属性理解、物体分割和空间推理方面实现了最先进的性能，能够提供对物理世界的细粒度感知并支持更精确的交互。&lt;h4&gt;结论&lt;/h4&gt;RynnEC为具身智能体提供了一个以区域为中心的视频范式，有望促进跨多样化具身任务的泛化能力，推动具身智能体通用认知核心的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了RynnEC，这是一个专为具身认知设计的视频多模态大语言模型。基于通用视觉语言基础模型构建，RynnEC集成了区域编码器和掩码解码器，能够实现灵活的区域级别视频交互。尽管架构紧凑，RynnEC在物体属性理解、物体分割和空间推理方面取得了最先进的性能。从概念上讲，它为具身智能体的大脑提供了以区域为中心的视频范式，能够提供对物理世界的细粒度感知，并实现更精确的交互。为了缓解标注3D数据集的稀缺问题，我们提出了一个基于自我中心视频的流水线来生成具身认知数据。此外，我们引入了RynnEC-Bench，这是一个以区域为中心的基准，用于评估具身认知能力。我们预计RynnEC将推动具身智能体通用认知核心的发展，并促进跨多样化具身任务的泛化能力。代码、模型检查点和基准可在以下网址获取：https://github.com/alibaba-damo-academy/RynnEC&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce RynnEC, a video multimodal large language model designed forembodied cognition. Built upon a general-purpose vision-language foundationmodel, RynnEC incorporates a region encoder and a mask decoder, enablingflexible region-level video interaction. Despite its compact architecture,RynnEC achieves state-of-the-art performance in object property understanding,object segmentation, and spatial reasoning. Conceptually, it offers aregion-centric video paradigm for the brain of embodied agents, providingfine-grained perception of the physical world and enabling more preciseinteractions. To mitigate the scarcity of annotated 3D datasets, we propose anegocentric video based pipeline for generating embodied cognition data.Furthermore, we introduce RynnEC-Bench, a region-centered benchmark forevaluating embodied cognitive capabilities. We anticipate that RynnEC willadvance the development of general-purpose cognitive cores for embodied agentsand facilitate generalization across diverse embodied tasks. The code, modelcheckpoints, and benchmark are available at:https://github.com/alibaba-damo-academy/RynnEC</description>
      <author>example@mail.com (Ronghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, Deli Zhao)</author>
      <guid isPermaLink="false">2508.14160v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Surya: Foundation Model for Heliophysics</title>
      <link>http://arxiv.org/abs/2508.14112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Surya是日球物理学领域首个使用全分辨率SDO数据进行时间推进作为预训练任务的基础模型，它能够学习太阳演化的潜在物理机制，并在多种太阳现象预测任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;日球物理学对于理解和预测空间天气事件和太阳活动至关重要。尽管有来自太阳动力学天文台(SDO)的高分辨率观测数据数十年，但大多数模型仍然是特定任务的，受限于稀少的标记数据，限制了它们对各种太阳现象的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;介绍Surya，一个用于日球物理学的基础模型，旨在从多仪器SDO观测中学习通用的太阳表示。&lt;h4&gt;方法&lt;/h4&gt;Surya是一个拥有3.66亿参数的基础模型，使用时空变换器架构，具有频谱门控和长-短范围注意力机制。它在高分辨率太阳图像预测任务上进行预训练，并通过自回归展开调优进一步优化。&lt;h4&gt;主要发现&lt;/h4&gt;零样本评估展示了Surya预测太阳动力学和耀斑事件的能力。使用参数高效的低秩适应(LoRA)进行下游微调后，在太阳风预测、活跃区域分割、太阳耀斑预测和EUV光谱方面表现出强大性能。&lt;h4&gt;结论&lt;/h4&gt;Surya的新架构和性能表明该模型能够学习太阳演化的潜在物理机制。&lt;h4&gt;翻译&lt;/h4&gt;日球物理学对于理解和预测空间天气事件和太阳活动至关重要。尽管有来自太阳动力学天文台(SDO)的高分辨率观测数据数十年，但大多数模型仍然是特定任务的，受限于稀少的标记数据，限制了它们对各种太阳现象的泛化能力。我们介绍了Surya，一个拥有3.66亿参数的日球物理学基础模型，旨在从多仪器SDO观测中学习通用的太阳表示，包括八个大气成像组件(AIA)通道和五个日震和磁力成像仪(HMI)产品。Surya采用具有频谱门控和长-短范围注意力的时空变换器架构，在高分辨率太阳图像预测任务上进行预训练，并通过自回归展开调优进一步优化。零样本评估展示了其预测太阳动力学和耀斑事件的能力，而使用参数高效的低秩适应(LoRA)进行下游微调则在太阳风预测、活跃区域分割、太阳耀斑预测和EUV光谱方面表现出强大性能。Surya是日球物理学中使用全分辨率SDO数据进行时间推进作为预训练任务的首个基础模型。其新颖的架构和性能表明该模型能够学习太阳演化的潜在物理机制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heliophysics is central to understanding and forecasting space weather eventsand solar activity. Despite decades of high-resolution observations from theSolar Dynamics Observatory (SDO), most models remain task-specific andconstrained by scarce labeled data, limiting their capacity to generalizeacross solar phenomena. We introduce Surya, a 366M parameter foundation modelfor heliophysics designed to learn general-purpose solar representations frommulti-instrument SDO observations, including eight Atmospheric Imaging Assembly(AIA) channels and five Helioseismic and Magnetic Imager (HMI) products. Suryaemploys a spatiotemporal transformer architecture with spectral gating andlong--short range attention, pretrained on high-resolution solar imageforecasting tasks and further optimized through autoregressive rollout tuning.Zero-shot evaluations demonstrate its ability to forecast solar dynamics andflare events, while downstream fine-tuning with parameter-efficient Low-RankAdaptation (LoRA) shows strong performance on solar wind forecasting, activeregion segmentation, solar flare forecasting, and EUV spectra. Surya is thefirst foundation model in heliophysics that uses time advancement as a pretexttask on full-resolution SDO data. Its novel architecture and performancesuggest that the model is able to learn the underlying physics behind solarevolution.</description>
      <author>example@mail.com (Sujit Roy, Johannes Schmude, Rohit Lal, Vishal Gaur, Marcus Freitag, Julian Kuehnert, Theodore van Kessel, Dinesha V. Hegde, Andrés Muñoz-Jaramillo, Johannes Jakubik, Etienne Vos, Kshitiz Mandal, Ata Akbari Asanjan, Joao Lucas de Sousa Almeida, Amy Lin, Talwinder Singh, Kang Yang, Chetraj Pandey, Jinsu Hong, Berkay Aydin, Thorsten Kurth, Ryan McGranaghan, Spiridon Kasapis, Vishal Upendran, Shah Bahauddin, Daniel da Silva, Nikolai V. Pogorelov, Campbell Watson, Manil Maskey, Madhulika Guhathakurta, Juan Bernabe-Moreno, Rahul Ramachandran)</author>
      <guid isPermaLink="false">2508.14112v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>LeanGeo: Formalizing Competitional Geometry problems in Lean</title>
      <link>http://arxiv.org/abs/2508.14644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LeanGeo是一个在Lean 4定理证明器中形式化和解决竞赛级几何问题的统一形式系统，提供了高级几何定理的全面库，支持严格的证明验证并与Mathlib无缝集成。研究团队还创建了LeanGeo-Bench基准，评估了当前大型语言模型的能力和局限性，并已开源项目。&lt;h4&gt;背景&lt;/h4&gt;几何问题是测试AI推理能力的重要领域，但现有几何求解系统无法在统一框架内表达问题，难以与其他数学领域集成。此外，由于几何证明依赖于直观图形，验证几何问题尤其具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;引入LeanGeo，一个统一的形式系统，用于在Lean 4定理证明器中形式化和解决竞赛级几何问题，解决现有系统的局限性。&lt;h4&gt;方法&lt;/h4&gt;开发LeanGeo系统，包含高级几何定理的全面库，结合Lean的基础逻辑支持严格证明验证，并与Mathlib无缝集成；创建LeanGeo-Bench基准，包含国际数学奥林匹克(IMO)和其他高级来源的问题。&lt;h4&gt;主要发现&lt;/h4&gt;评估展示了最先进的大型语言模型在LeanGeo-Bench基准上的能力和局限性，强调了在自动化几何推理方面需要进一步发展。&lt;h4&gt;结论&lt;/h4&gt;LeanGeo提供了形式化几何问题的新方法，通过开源定理库和基准，促进了自动化几何推理领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;几何问题是测试AI推理能力的重要试验场。大多数现有的几何求解系统无法在统一框架内表达问题，因此难以与其他数学领域集成。此外，由于大多数几何证明依赖于直观图形，验证几何问题尤其具有挑战性。为解决这些差距，我们引入了LeanGeo，一个在Lean 4定理证明器中形式化和解决竞赛级几何问题的统一形式系统。LeanGeo具有高级几何定理的全面库，结合Lean的基础逻辑，支持严格的证明验证，并与Mathlib无缝集成。我们还展示了LeanGeo-Bench，这是LeanGeo中的一个形式化几何基准，包含国际数学奥林匹克(IMO)和其他高级来源的问题。我们的评估展示了最先进的大型语言模型在该基准上的能力和局限性，突显了在自动化几何推理方面需要进一步发展。我们在https://github.com/project-numina/LeanGeo/tree/master开源了LeanGeo的定理库和基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometry problems are a crucial testbed for AI reasoning capabilities. Mostexisting geometry solving systems cannot express problems within a unifiedframework, thus are difficult to integrate with other mathematical fields.Besides, since most geometric proofs rely on intuitive diagrams, verifyinggeometry problems is particularly challenging. To address these gaps, weintroduce LeanGeo, a unified formal system for formalizing and solvingcompetition-level geometry problems within the Lean 4 theorem prover. LeanGeofeatures a comprehensive library of high-level geometric theorems with Lean'sfoundational logic, enabling rigorous proof verification and seamlessintegration with Mathlib. We also present LeanGeo-Bench, a formal geometrybenchmark in LeanGeo, comprising problems from the International MathematicalOlympiad (IMO) and other advanced sources. Our evaluation demonstrates thecapabilities and limitations of state-of-the-art Large Language Models on thisbenchmark, highlighting the need for further advancements in automatedgeometric reasoning. We open source the theorem library and the benchmark ofLeanGeo at https://github.com/project-numina/LeanGeo/tree/master.</description>
      <author>example@mail.com (Chendong Song, Zihan Wang, Frederick Pu, Haiming Wang, Xiaohan Lin, Junqi Liu, Jia Li, Zhengying Liu)</author>
      <guid isPermaLink="false">2508.14644v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation</title>
      <link>http://arxiv.org/abs/2508.14327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的多模态多视角视频生成方法，通过统一的扩散变换器模型实现自动驾驶场景的高质量多模态数据生成。&lt;h4&gt;背景&lt;/h4&gt;视频生成在自动驾驶城市场景合成中显示出优势，但现有方法主要关注RGB视频生成，缺乏多模态能力。多模态数据（如深度图和语义图）对自动驾驶的整体城市场景理解至关重要，而使用多个模型生成不同模态会增加部署难度且无法利用互补线索。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法无法支持多模态视频生成的问题，提出一种新颖的多模态多视角视频生成方法用于自动驾驶场景合成。&lt;h4&gt;方法&lt;/h4&gt;构建一个统一的扩散变换器模型，包含模态共享组件和模态特定组件，利用多样化的条件输入将可控的场景结构和内容线索编码到统一的扩散模型中，实现多模态多视角视频生成。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes真实世界自动驾驶数据集上的实验表明，该方法能够高保真度和可控制性地生成多模态多视角城市场景视频，性能超越了最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的统一框架能够生成多模态多视角驾驶场景视频，为自动驾驶提供了更全面的城市场景理解能力。&lt;h4&gt;翻译&lt;/h4&gt;视频生成最近在自动驾驶城市场景合成中显示出优越性。现有的自动驾驶视频生成方法主要关注RGB视频生成，缺乏支持多模态视频生成的能力。然而，多模态数据，如深度图和语义图，对自动驾驶的整体城市场景理解至关重要。虽然可以使用多个模型生成不同模态，但这增加了模型部署的难度，且没有利用多模态数据生成的互补线索。为解决这一问题，本研究提出了一种新颖的用于自动驾驶的多模态多视角视频生成方法。具体而言，我们构建了一个由模态共享组件和模态特定组件组成的统一扩散变换器模型。然后，我们利用多样化的条件输入将可控的场景结构和内容线索编码到统一的扩散模型中，用于多模态多视角视频生成。通过这种方式，我们的方法能够在统一框架中生成多模态多视角驾驶场景视频。我们在具有挑战性的真实世界自动驾驶数据集nuScenes上的实验表明，我们的方法能够高保真度和可控制性地生成多模态多视角城市场景视频，超越了最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域现有视频生成方法仅支持单模态(RGB)生成而缺乏多模态(深度图、语义图等)生成能力的问题。这个问题在现实中很重要，因为自动驾驶需要全面理解城市场景，多模态数据对实现安全高效的自动驾驶至关重要；同时，使用多个模型生成不同模态会增加部署难度且无法充分利用多模态间的互补信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到自动驾驶作为感知密集型任务本质需要多模态数据，然后发现现有方法要么只关注单模态RGB生成，要么使用多个独立模型增加部署难度。作者设计方法时借鉴了扩散模型(如SVD、CogVideoX)、多视角城市场景生成方法(如DriveDreamer、Panacea)以及多模态合成技术(如IDOL)。具体技术借鉴包括使用扩散变换器架构、3D VAE编码解码、多样化的条件输入设计，以及模态共享与特定学习的分解思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个统一的多模态多视角扩散变换器模型，包含模态共享组件和模态特定组件，利用多样化条件输入将可控场景结构和内容线索编码到模型中。整体流程：1)条件输入编码(文本、布局、参考条件)；2)多模态多视角扩散变换器处理(模态共享组件学习跨模态共同特征，模态特定组件保留各模态独特特征)；3)训练与推理(使用DDPM训练，DDIM采样和CFG引导)；4)VAE解码生成多视角场景视频。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个利用多样化条件输入和扩散变换器进行多模态多视角自动驾驶场景生成的工作；2)设计包含模态共享和模态特定组件的统一扩散变换器模型；3)多样化的条件输入设计(文本、参考、布局条件)；4)模态共享与特定学习的分解。相比之前工作的不同：支持多模态联合生成而非单模态；使用统一模型而非多个独立模型；不仅关注多视角一致性，还关注多模态一致性；专门针对自动驾驶场景优化而非通用视频生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种统一的多模态多视角扩散变换器模型，能够在单一框架内生成高质量、高可控性的多模态多视角驾驶场景视频，显著提升了自动驾驶场景理解和系统评估的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation has recently shown superiority in urban scene synthesis forautonomous driving. Existing video generation approaches to autonomous drivingprimarily focus on RGB video generation and lack the ability to supportmulti-modal video generation. However, multi-modal data, such as depth maps andsemantic maps, are crucial for holistic urban scene understanding in autonomousdriving. Although it is feasible to use multiple models to generate differentmodalities, this increases the difficulty of model deployment and does notleverage complementary cues for multi-modal data generation. To address thisproblem, in this work, we propose a novel multi-modal multi-view videogeneration approach to autonomous driving. Specifically, we construct a unifieddiffusion transformer model composed of modal-shared components andmodal-specific components. Then, we leverage diverse conditioning inputs toencode controllable scene structure and content cues into the unified diffusionmodel for multi-modal multi-view video generation. In this way, our approach iscapable of generating multi-modal multi-view driving scene videos in a unifiedframework. Our experiments on the challenging real-world autonomous drivingdataset, nuScenes, show that our approach can generate multi-modal multi-viewurban scene videos with high fidelity and controllability, surpassing thestate-of-the-art methods.</description>
      <author>example@mail.com (Guile Wu, David Huang, Dongfeng Bai, Bingbing Liu)</author>
      <guid isPermaLink="false">2508.14327v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services</title>
      <link>http://arxiv.org/abs/2508.14503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Transformer架构的异常检测方法，集成多尺度特征感知，以解决云服务环境中时间建模和尺度感知特征表示的局限性。方法通过改进的Transformer模块、多尺度特征构建路径和注意力加权融合模块，在高维监控数据上进行时间建模，捕获长程依赖和上下文语义，并动态调整各尺度特征对最终决策的贡献。实验表明该方法在精确率、召回率、AUC和F1分数等关键指标上优于主流基线模型，并在各种扰动条件下保持稳定性能。&lt;h4&gt;背景&lt;/h4&gt;云服务环境中存在时间建模和尺度感知特征表示的局限性，影响了异常检测的效果。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Transformer架构的异常检测方法，集成多尺度特征感知，解决云服务环境中时间建模和尺度感知特征表示的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用改进的Transformer模块对高维监控数据进行时间建模，利用自注意力机制捕获长程依赖；引入多尺度特征构建路径通过下采样和并行编码提取不同粒度时序特征；设计注意力加权融合模块动态调整各尺度特征贡献；构建包含CPU利用率、内存使用和任务调度状态等核心信号的标准化多维时间序列；使用位置编码增强模型时间感知能力。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在精确率、召回率、AUC和F1分数等关键指标上优于主流基线模型，在各种扰动条件下保持强大的稳定性和检测性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在复杂云环境中表现出卓越的异常检测能力，具有良好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种基于Transformer架构的集成多尺度特征感知的异常检测方法，旨在解决云服务环境中时间建模和尺度感知特征表示的局限性。该方法首先采用改进的Transformer模块对高维监控数据进行时间建模，利用自注意力机制捕获长程依赖和上下文语义。然后，引入多尺度特征构建路径，通过下采样和并行编码提取不同粒度的时序特征。设计了一个注意力加权融合模块，动态调整各尺度对最终决策的贡献，增强模型在异常模式建模中的鲁棒性。在输入建模阶段，构建了标准化的多维时间序列，涵盖CPU利用率、内存使用和任务调度状态等核心信号，同时使用位置编码增强模型的时间感知能力。设计了系统的实验设置来评估性能，包括对比实验和超参数敏感性分析，重点关注优化器、学习率、异常比例和噪声水平的影响。实验结果表明，所提出的方法在精确率、召回率、AUC和F1分数等关键指标上优于主流基线模型，并在各种扰动条件下保持强大的稳定性和检测性能，证明了其在复杂云环境中的卓越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study proposes an anomaly detection method based on the Transformerarchitecture with integrated multiscale feature perception, aiming to addressthe limitations of temporal modeling and scale-aware feature representation incloud service environments. The method first employs an improved Transformermodule to perform temporal modeling on high-dimensional monitoring data, usinga self-attention mechanism to capture long-range dependencies and contextualsemantics. Then, a multiscale feature construction path is introduced toextract temporal features at different granularities through downsampling andparallel encoding. An attention-weighted fusion module is designed todynamically adjust the contribution of each scale to the final decision,enhancing the model's robustness in anomaly pattern modeling. In the inputmodeling stage, standardized multidimensional time series are constructed,covering core signals such as CPU utilization, memory usage, and taskscheduling states, while positional encoding is used to strengthen the model'stemporal awareness. A systematic experimental setup is designed to evaluateperformance, including comparative experiments and hyperparameter sensitivityanalysis, focusing on the impact of optimizers, learning rates, anomaly ratios,and noise levels. Experimental results show that the proposed methodoutperforms mainstream baseline models in key metrics, including precision,recall, AUC, and F1-score, and maintains strong stability and detectionperformance under various perturbation conditions, demonstrating its superiorcapability in complex cloud environments.</description>
      <author>example@mail.com (Lian Lian, Yilin Li, Song Han, Renzi Meng, Sibo Wang, Ming Wang)</author>
      <guid isPermaLink="false">2508.14503v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding</title>
      <link>http://arxiv.org/abs/2508.14395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to UIST 2025. Project website:  https://zhaorunning.github.io/NoteIt/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NoteIt是一个创新的系统，能够将教学视频自动转换为可交互的笔记，保留原始视频的层次结构和多模态信息，并允许用户自定义内容和格式。&lt;h4&gt;背景&lt;/h4&gt;用户经常观看教学视频并做笔记，以便日后回顾关键知识点而不必重新观看整个长视频。现有的自动笔记生成工具可以让用户高效获取信息性笔记，但现有工具生成的笔记无法全面保留原始视频中的信息，也不能满足用户对数字笔记多样化展示格式和交互功能的期望。&lt;h4&gt;目的&lt;/h4&gt;开发一个系统，能够自动将教学视频转换为可交互的笔记，全面保留视频信息并提供用户自定义功能。&lt;h4&gt;方法&lt;/h4&gt;提出了NoteIt系统，使用新的管道自动将教学视频转换为可交互笔记。该管道忠实地从视频中提取层次结构和多模态关键信息，并提供交互界面让用户根据偏好自定义笔记内容和展示格式。&lt;h4&gt;主要发现&lt;/h4&gt;通过技术评估和比较用户研究（N=36）发现，NoteIt在客观指标上表现良好，用户反馈积极，证明了管道的有效性和系统的整体可用性。&lt;h4&gt;结论&lt;/h4&gt;NoteIt系统能够有效解决现有工具的不足，提供更全面、交互性更强的笔记，帮助用户更高效地回顾教学视频中的关键知识点。&lt;h4&gt;翻译&lt;/h4&gt;用户经常为教学视频做笔记，以便日后访问关键知识点而无需重新观看长视频。自动笔记生成工具使用户能够高效获取信息性笔记。然而，现有研究或现成工具生成的笔记无法全面保留原始视频中传达的信息，也不能满足用户在使用数字笔记时对多样化展示格式和交互功能的期望。在这项工作中，我们提出了NoteIt，一个系统，它使用新颖的管道自动将教学视频转换为可交互笔记，忠实地从视频中提取层次结构和多模态关键信息。通过NoteIt的界面，用户可以与系统交互，根据偏好进一步自定义笔记的内容和展示格式。我们进行了技术评估和比较用户研究（N=36）。客观指标上的优异性能和积极的用户反馈证明了管道的有效性和NoteIt的整体可用性。项目网站：https://zhaorunning.github.io/NoteIt/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746059.3747626&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Users often take notes for instructional videos to access key knowledge laterwithout revisiting long videos. Automated note generation tools enable users toobtain informative notes efficiently. However, notes generated by existingresearch or off-the-shelf tools fail to preserve the information conveyed inthe original videos comprehensively, nor can they satisfy users' expectationsfor diverse presentation formats and interactive features when using notesdigitally. In this work, we present NoteIt, a system, which automaticallyconverts instructional videos to interactable notes using a novel pipeline thatfaithfully extracts hierarchical structure and multimodal key information fromvideos. With NoteIt's interface, users can interact with the system to furthercustomize the content and presentation formats of the notes according to theirpreferences. We conducted both a technical evaluation and a comparison userstudy (N=36). The solid performance in objective metrics and the positive userfeedback demonstrated the effectiveness of the pipeline and the overallusability of NoteIt. Project website: https://zhaorunning.github.io/NoteIt/</description>
      <author>example@mail.com (Running Zhao, Zhihan Jiang, Xinchen Zhang, Chirui Chang, Handi Chen, Weipeng Deng, Luyao Jin, Xiaojuan Qi, Xun Qian, Edith C. H. Ngai)</author>
      <guid isPermaLink="false">2508.14395v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum</title>
      <link>http://arxiv.org/abs/2508.14684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 2024 KDD Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对异常网络中的异质性结构问题，提出了一种基于因果边分离的频谱神经网络CES2-GAD，通过分离同质性和异质性边，使用混合频谱滤波器捕获信号，有效检测异常节点。&lt;h4&gt;背景&lt;/h4&gt;现实世界中，异常实体常添加合法连接同时隐藏直接联系，形成异质性网络结构，而大多数GNN技术无法有效处理这种结构。现有研究主要在空间域解决此问题，忽视了节点结构、特征和上下文环境的复杂关系，且频谱域研究有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在异质图上进行有效异常检测的方法，解决现有方法在处理异质性结构时的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出CES2-GAD方法：1)使用因果干预将原始图分离为同质性和异质性边；2)应用多种混合频谱滤波器捕获分离图中的信号；3)连接多信号表示并输入分类器预测异常。&lt;h4&gt;主要发现&lt;/h4&gt;分析不同异质性程度节点的频谱分布发现，异常节点的异质性导致频谱能量从低频向高频转移。&lt;h4&gt;结论&lt;/h4&gt;通过大量真实数据集实验验证，CES2-GAD方法在异质图异常检测中表现出色，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界中，异常实体通常会添加更多合法连接，同时隐藏与其他异常实体的直接联系，导致异常网络中出现异质性结构，大多数基于图神经网络的技术无法解决这个问题。已有一些研究在空间域尝试解决此问题，但这些方法忽视了节点结构编码、节点特征及其上下文环境之间的复杂关系，且依赖于原则性指导，在频谱域解决异质性问题的研究仍然有限。本研究分析了具有不同异质性程度的节点的频谱分布，发现异常节点的异质性导致频谱能量从低频向高频转移。为应对上述挑战，我们提出了一种基于因果边分离的频谱神经网络CES2-GAD，用于异质图上的异常检测。首先，CES2-GAD使用因果干预将原始图分离为同质性和异质性边。随后，使用各种混合频谱滤波器捕获分离图中的信号。最后，将来自多个信号的表示连接起来并输入分类器以预测异常。使用真实数据集的大量实验证明了我们提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the real world, anomalous entities often add more legitimate connectionswhile hiding direct links with other anomalous entities, leading toheterophilic structures in anomalous networks that most GNN-based techniquesfail to address. Several works have been proposed to tackle this issue in thespatial domain. However, these methods overlook the complex relationshipsbetween node structure encoding, node features, and their contextualenvironment and rely on principled guidance, research on solving spectraldomain heterophilic problems remains limited. This study analyzes the spectraldistribution of nodes with different heterophilic degrees and discovers thatthe heterophily of anomalous nodes causes the spectral energy to shift from lowto high frequencies. To address the above challenges, we propose a spectralneural network CES2-GAD based on causal edge separation for anomaly detectionon heterophilic graphs. Firstly, CES2-GAD will separate the original graph intohomophilic and heterophilic edges using causal interventions. Subsequently,various hybrid-spectrum filters are used to capture signals from the segmentedgraphs. Finally, representations from multiple signals are concatenated andinput into a classifier to predict anomalies. Extensive experiments withreal-world datasets have proven the effectiveness of the method we proposed.</description>
      <author>example@mail.com (Zengyi Wo, Wenjun Wang, Minglai Shao, Chang Liu, Yumeng Wang, Yueheng Sun)</author>
      <guid isPermaLink="false">2508.14684v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Improving Fairness in Graph Neural Networks via Counterfactual Debiasing</title>
      <link>http://arxiv.org/abs/2508.14683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 2024 KDD Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为Fair-ICD的新方法，利用反事实数据增强来减轻图神经网络中的偏见问题，同时保持高预测性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在建模图结构数据方面取得了成功，但可能会基于种族和性别等属性表现出预测偏见，且这种偏见可能因图结构和消息传递机制而加剧。&lt;h4&gt;目的&lt;/h4&gt;解决GNN中的偏见问题，同时保持预测准确性和公平性之间的平衡。&lt;h4&gt;方法&lt;/h4&gt;提出Fair-ICD方法，通过在消息传递前使用反事实创建多样化邻域来学习无偏节点表示，并采用对抗性判别器减少传统GNN分类器预测中的偏见。&lt;h4&gt;主要发现&lt;/h4&gt;Fair-ICD在适度条件下确保了GNN的公平性，实验表明该方法显著提高了公平性指标，同时保持了高预测性能。&lt;h4&gt;结论&lt;/h4&gt;Fair-ICD是一种有效的方法，可以在保持高预测性能的同时提高GNN的公平性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在建模图结构数据方面取得了成功。然而，与其他机器学习模型类似，GNN可能基于种族和性别等属性表现出预测偏见。此外，GNN中的偏见可能因图结构和消息传递机制而加剧。最近的前沿方法提出通过从输入或表示中过滤敏感信息（如边删除或特征掩码）来减轻偏见。然而，我们认为这些策略可能会无意中消除非敏感特征，导致预测准确性和公平性之间的平衡受损。为了应对这一挑战，我们提出了一种利用反事实数据增强进行偏见减轻的新方法。该方法在消息传递之前使用反事实创建多样化的邻域，促进从增强图中学习无偏的节点表示。随后，采用对抗性判别器来减少传统GNN分类器预测中的偏见。我们提出的技术Fair-ICD确保了GNN在适度条件下的公平性。在三个标准数据集上使用三种GNN骨干网络的实验表明，Fair-ICD显著提高了公平性指标，同时保持了高预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have been successful in modelinggraph-structured data. However, similar to other machine learning models, GNNscan exhibit bias in predictions based on attributes like race and gender.Moreover, bias in GNNs can be exacerbated by the graph structure andmessage-passing mechanisms. Recent cutting-edge methods propose mitigating biasby filtering out sensitive information from input or representations, like edgedropping or feature masking. Yet, we argue that such strategies mayunintentionally eliminate non-sensitive features, leading to a compromisedbalance between predictive accuracy and fairness. To tackle this challenge, wepresent a novel approach utilizing counterfactual data augmentation for biasmitigation. This method involves creating diverse neighborhoods usingcounterfactuals before message passing, facilitating unbiased noderepresentations learning from the augmented graph. Subsequently, an adversarialdiscriminator is employed to diminish bias in predictions by conventional GNNclassifiers. Our proposed technique, Fair-ICD, ensures the fairness of GNNsunder moderate conditions. Experiments on standard datasets using three GNNbackbones demonstrate that Fair-ICD notably enhances fairness metrics whilepreserving high predictive performance.</description>
      <author>example@mail.com (Zengyi Wo, Chang Liu, Yumeng Wang, Minglai Shao, Wenjun Wang)</author>
      <guid isPermaLink="false">2508.14683v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models</title>
      <link>http://arxiv.org/abs/2508.14427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于知识图谱注入的微调算法框架，用于解决大型语言模型在处理需要结构化知识的任务时存在的推理链缺失和实体级语义理解不足的问题。该方法通过图神经网络编码实体及其关系，设计融合机制联合建模知识图谱嵌入与语言模型的上下文表示，并引入门控机制动态平衡语言语义和结构知识的贡献，有效提高了模型在实体识别、问答和语言生成等任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在处理需要结构化知识的任务时存在两个主要问题：推理链缺失和实体级语义理解不足。这限制了模型在需要复杂语义理解和结构推理的任务上的表现。&lt;h4&gt;目的&lt;/h4&gt;提出一种微调算法框架，通过知识图谱注入来增强大型语言模型在结构化知识任务中的表现，提高模型的语义理解能力和推理能力。&lt;h4&gt;方法&lt;/h4&gt;基于预训练语言模型构建，引入结构化图信息进行辅助学习；使用图神经网络编码实体及其关系，构建基于图的语义表示；设计融合机制联合建模知识图谱嵌入与语言模型的上下文表示；引入门控机制动态平衡语言语义和结构知识的贡献，提高知识整合的鲁棒性；构建联合损失函数，同时考虑任务性能和结构对齐目标；进行系统敏感性实验，评估学习率、图覆盖率和结构扰动对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的结构感知微调框架显著增强了模型表示复杂语义单位的能力；在涉及结构推理和实体提取的场景中，模型表现出更好的语义一致性和上下文逻辑建模能力；实验结果验证了该方法在实体识别、问答和语言生成等任务上的有效性和稳定性。&lt;h4&gt;结论&lt;/h4&gt;基于知识图谱注入的微调算法框架可以有效解决大型语言模型在结构化知识任务中的推理链缺失和语义理解不足问题，通过融合结构化知识提高模型在多种任务上的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了大型语言模型在处理需要结构化知识的任务时存在的推理链缺失和实体级语义理解不足的问题。它提出了一种基于知识图谱注入的微调算法框架。该方法基于预训练语言模型构建，并引入结构化图信息进行辅助学习。使用图神经网络对实体及其关系进行编码，构建基于图的语义表示。然后设计了一种融合机制，联合建模知识图谱嵌入与语言模型的上下文表示。为了增强知识整合的鲁棒性，引入了一种门控机制来动态平衡语言语义和结构知识的贡献。这有效缓解了不同表示空间之间的冲突。在训练过程中，构建了一个联合损失函数，同时考虑任务性能和结构对齐目标。这有助于提高实体预测和语义推理的准确性。研究还包括一系列系统的敏感性实验。它评估了学习率、图覆盖率和结构扰动对模型性能的影响。结果进一步验证了所提出方法在实体识别、问答和语言生成等任务上的有效性和稳定性。实验结果表明，所提出的结构感知微调框架显著增强了模型表示复杂语义单位的能力。在涉及结构推理和实体提取的场景中，它表现出更好的语义一致性和上下文逻辑建模能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the problems of missing reasoning chains andinsufficient entity-level semantic understanding in large language models whendealing with tasks that require structured knowledge. It proposes a fine-tuningalgorithm framework based on knowledge graph injection. The method builds onpretrained language models and introduces structured graph information forauxiliary learning. A graph neural network is used to encode entities and theirrelations, constructing a graph-based semantic representation. A fusionmechanism is then designed to jointly model the knowledge graph embeddings withthe contextual representations from the language model. To enhance therobustness of knowledge integration, a gating mechanism is introduced todynamically balance the contributions of linguistic semantics and structuralknowledge. This effectively mitigates conflicts between differentrepresentational spaces. During training, a joint loss function is constructedto account for both task performance and structural alignment objectives. Thishelps improve the accuracy of entity prediction and semantic reasoning. Thestudy also includes a series of systematic sensitivity experiments. Itevaluates the effects of learning rate, graph coverage, and structuralperturbations on model performance. The results further validate theeffectiveness and stability of the proposed method across tasks such as entityrecognition, question answering, and language generation. Experimental findingsshow that the proposed structure-aware fine-tuning framework significantlyenhances the model's ability to represent complex semantic units. Itdemonstrates better semantic consistency and contextual logic modeling inscenarios involving structural reasoning and entity extraction.</description>
      <author>example@mail.com (Wuyang Zhang, Yexin Tian, Xiandong Meng, Mengjie Wang, Junliang Du)</author>
      <guid isPermaLink="false">2508.14427v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.14338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络中学习算法与图结构之间的相互作用关系，通过理论分析和实证研究揭示了图结构如何影响学习算法的性能，为GNN算法设计和选择提供了新见解。&lt;h4&gt;背景&lt;/h4&gt;现有关于图神经网络学习动力学的理论研究主要关注无噪声情况下的算法收敛率，且仅将学习动力与图结构(如最大度数)进行粗略关联，缺乏对图结构与学习算法之间深入关系的理解。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在弥合理论差距，探究图神经网络在有噪声情况下学习算法的过度风险(泛化性能)，并分析图结构如何影响学习算法的表现。&lt;h4&gt;方法&lt;/h4&gt;研究将学习理论的传统框架扩展到图神经网络上下文中，通过谱图理论建立学习算法性能与图结构之间的联系，并采用比较分析方法研究不同图结构(规则图与幂律图)对算法性能的影响，同时将分析扩展到多层线性GNNs。&lt;h4&gt;主要发现&lt;/h4&gt;1. 推导了SGD和岭回归在GNNs中的过度风险曲线，并通过谱图理论与图结构建立联系；2. 不同图结构(规则图与幂律图)对算法性能有显著影响；3. 多层线性GNNs中存在非各向同性效应增加的现象，从学习算法角度提供了对过度平滑问题的新见解。&lt;h4&gt;结论&lt;/h4&gt;图结构、GNNs和学习算法之间存在耦合关系，这一发现对实际中GNN算法的设计和选择具有重要指导意义，实证结果与理论预测一致验证了这一观点。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了图神经网络中学习算法与图结构之间的相互作用关系。现有关于图神经网络学习动力学的理论研究主要关注插值 regime(无噪声)下学习算法的收敛率，并且仅提供了这些动力与实际图结构(如最大度数)之间粗略的联系。本文旨在通过研究图神经网络中学习算法在泛化 regime(有噪声)下的过度风险(泛化性能)来弥合这一差距。具体来说，我们将学习理论文献中的传统设置扩展到图神经网络的上下文中，并考察图结构如何影响随机梯度下降和岭回归等学习算法的性能。我们的研究在理解图结构与图神经网络中学习的相互作用方面做出了几个关键贡献。首先，我们推导了图神经网络中随机梯度下降和岭回归的过度风险曲线，并通过谱图理论将这些曲线与图结构联系起来。基于这一建立的框架，我们通过比较分析进一步探索了不同图结构(规则图与幂律图)如何影响这些算法的性能。此外，我们将分析扩展到多层线性图神经网络，揭示了过度风险曲线上非各向同性效应的增加，从而从学习算法的角度为图神经网络中的过度平滑问题提供了新见解。我们的实证结果与理论预测一致，共同展示了图结构、图神经网络和学习算法之间的耦合关系，并为实际中图神经网络算法的设计和选择提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies the interplay between learning algorithms and graphstructure for graph neural networks (GNNs). Existing theoretical studies on thelearning dynamics of GNNs primarily focus on the convergence rates of learningalgorithms under the interpolation regime (noise-free) and offer only a crudeconnection between these dynamics and the actual graph structure (e.g., maximumdegree). This paper aims to bridge this gap by investigating the excessive risk(generalization performance) of learning algorithms in GNNs within thegeneralization regime (with noise). Specifically, we extend the conventionalsettings from the learning theory literature to the context of GNNs and examinehow graph structure influences the performance of learning algorithms such asstochastic gradient descent (SGD) and Ridge regression. Our study makes severalkey contributions toward understanding the interplay between graph structureand learning in GNNs. First, we derive the excess risk profiles of SGD andRidge regression in GNNs and connect these profiles to the graph structurethrough spectral graph theory. With this established framework, we furtherexplore how different graph structures (regular vs. power-law) impact theperformance of these algorithms through comparative analysis. Additionally, weextend our analysis to multi-layer linear GNNs, revealing an increasingnon-isotropic effect on the excess risk profile, thereby offering new insightsinto the over-smoothing issue in GNNs from the perspective of learningalgorithms. Our empirical results align with our theoretical predictions,\emph{collectively showcasing a coupling relation among graph structure, GNNsand learning algorithms, and providing insights on GNN algorithm design andselection in practice.}</description>
      <author>example@mail.com (Junwei Su, Chuan Wu)</author>
      <guid isPermaLink="false">2508.14338v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Multi-view Graph Condensation via Tensor Decomposition</title>
      <link>http://arxiv.org/abs/2508.14330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'基于张量分解的多视图图浓缩'(GCTD)的新方法，用于解决大规模图神经网络训练中的计算挑战。该方法通过张量分解技术合成信息丰富的小型图，同时保持图神经网络的预测性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在药物发现、目标检测、社交媒体分析、推荐系统和文本分类等现实世界应用中表现出色。然而，在大规模图上训练GNNs面临显著的计算挑战，因为需要大量资源进行存储和处理。现有的图浓缩方法通常依赖于计算密集的双层优化，且无法保持合成节点与原始节点之间的映射关系，限制了模型决策的可解释性。&lt;h4&gt;目的&lt;/h4&gt;探索分解技术在图浓缩中的应用潜力，了解这些技术能在多大程度上合成信息丰富的小型图，并实现与下游任务相当的性能。具体目标是提出一种新方法，解决现有图浓缩方法中的计算效率和可解释性问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种名为'基于张量分解的多视图图浓缩'(GCTD)的新方法。该方法利用分解技术从图数据中学习线性或多线性函数，通过多视图处理和张量分解技术合成小型图，同时保持与原始图的关键信息映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;在六个真实世界数据集上的广泛实验表明，GCTD能够有效减小图的大小，同时保持GNN的性能。在六个数据集中的三个上，GCTD实现了高达4.0%的准确率提升，并且在大型图上的表现与现有方法具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;GCTD方法成功地解决了现有图浓缩方法的局限性，通过张量分解技术实现了更高效、更具可解释性的图浓缩。该方法不仅减少了计算资源需求，还保持了合成图与原始图之间的映射关系，为图神经网络在大规模图上的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在各种现实世界应用中已展现出显著成果，包括药物发现、目标检测、社交媒体分析、推荐系统和文本分类。与其巨大潜力相比，在大规模图上训练它们由于存储和处理所需的资源而带来了显著的计算挑战。图浓缩已成为一种有前景的解决方案，通过学习一个合成的小型图来减少这些需求，同时保留原始图的基本信息并维持GNN的预测性能。尽管它们有效，但当前的图浓缩方法通常依赖于计算密集的双层优化。此外，它们无法保持合成节点与原始节点之间的映射关系，限制了模型决策的可解释性。从这个意义上讲，各种分解技术已被应用于从图数据中学习线性或多线性函数，提供了一种更透明且资源消耗更少的替代方案。然而，它们在图浓缩中的应用尚未被探索。本文解决了这一差距，并提出了一种名为'基于张量分解的多视图图浓缩'(GCTD)的新方法，以研究这些技术在多大程度上能够合成信息丰富的小型图并实现相当的下游任务性能。在六个真实世界数据集上的广泛实验表明，GCTD能够有效减小图的大小，同时保持GNN的性能，在六个数据集中的三个上实现了高达4.0%的准确率提升，并且在大型图上与现有方法相比具有竞争力性能。我们的代码可在https://anonymous.4open.science/r/gctd-345A获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable results in variousreal-world applications, including drug discovery, object detection, socialmedia analysis, recommender systems, and text classification. In contrast totheir vast potential, training them on large-scale graphs presents significantcomputational challenges due to the resources required for their storage andprocessing. Graph Condensation has emerged as a promising solution to reducethese demands by learning a synthetic compact graph that preserves theessential information of the original one while maintaining the GNN'spredictive performance. Despite their efficacy, current graph condensationapproaches frequently rely on a computationally intensive bi-leveloptimization. Moreover, they fail to maintain a mapping between synthetic andoriginal nodes, limiting the interpretability of the model's decisions. In thissense, a wide range of decomposition techniques have been applied to learnlinear or multi-linear functions from graph data, offering a more transparentand less resource-intensive alternative. However, their applicability to graphcondensation remains unexplored. This paper addresses this gap and proposes anovel method called Multi-view Graph Condensation via Tensor Decomposition(GCTD) to investigate to what extent such techniques can synthesize aninformative smaller graph and achieve comparable downstream task performance.Extensive experiments on six real-world datasets demonstrate that GCTDeffectively reduces graph size while preserving GNN performance, achieving upto a 4.0\ improvement in accuracy on three out of six datasets and competitiveperformance on large graphs compared to existing approaches. Our code isavailable at https://anonymous.4open.science/r/gctd-345A.</description>
      <author>example@mail.com (Nícolas Roque dos Santos, Dawon Ahn, Diego Minatel, Alneu de Andrade Lopes, Evangelos E. Papalexakis)</author>
      <guid isPermaLink="false">2508.14330v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Hypergraph Neural Network</title>
      <link>http://arxiv.org/abs/2508.14101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICDM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;超图神经网络在捕获高阶关系方面有广泛应用，但存在长程依赖捕捉不足的问题。作者提出Implicit Hypergraph Neural Network (IHNN)框架，通过联合学习节点和超边的固定点表示，以端到端方式解决此问题，实验证明其在节点分类任务上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;超图为捕捉实体间高阶关系提供广义框架，已应用于医疗保健、社交网络和生物信息学等领域。超图神经网络通过超边上节点间的消息传递学习潜在表示，已成为这些领域预测任务的首选方法。然而，这些方法通常只执行少量消息传递轮次，导致表示仅捕获局部信息而忽略长程高阶依赖。&lt;h4&gt;目的&lt;/h4&gt;解决超图神经网络在捕获长程依赖关系时面临的性能下降问题。作者证明现有超图神经网络在聚合更多信息以捕获长程依赖时会失去预测能力，并提出IHNN框架来缓解这一问题。&lt;h4&gt;方法&lt;/h4&gt;提出Implicit Hypergraph Neural Network (IHNN)框架，通过端到端方式联合学习节点和超边的固定点表示。利用隐式微分，引入可处理的投影梯度下降方法来高效训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;现有超图神经网络在尝试通过增加消息传递轮次捕获长程依赖时性能下降。IHNN框架能有效解决此问题，在真实超图上的节点分类任务中，大多数设置下都优于最先进的先前工作，建立了超图学习的新技术水平。&lt;h4&gt;结论&lt;/h4&gt;IHNN框架通过联合学习节点和超边的固定点表示，有效解决了超图神经网络在捕获长程依赖时的性能下降问题，为超图学习领域建立了新的技术水平。&lt;h4&gt;翻译&lt;/h4&gt;超图为捕捉实体间的高阶关系提供了广义框架，已广泛应用于医疗保健、社交网络和生物信息学等多个领域。超图神经网络通过超边上节点间的消息传递学习潜在表示，已成为这些领域预测任务的首选方法。这些方法通常只执行少量消息传递轮次来学习表示，然后用于预测。这种少量消息传递轮次的代价是表示仅捕获局部信息而忽略了长程高阶依赖。然而，正如我们所展示的，盲目增加消息传递轮次以捕获长程依赖同样会降低超图神经网络的性能。最近的工作表明，隐式图神经网络在标准图中能够捕获长程依赖同时保持性能。尽管它们很流行，但先前的工作尚未研究超图神经网络中的长程依赖问题。在此，我们首先证明现有超图神经网络在聚合更多信息以捕获长程依赖时会失去预测能力。然后，我们提出了隐式超图神经网络(IHNN)，这是一个新颖的框架，通过端到端方式联合学习节点和超边的固定点表示来缓解这一问题。利用隐式微分，我们引入了一种可处理的投影梯度下降方法来高效训练模型。在真实超图上的节点分类任务的广泛实验表明，IHNN在大多数设置下都优于最接近的先前工作，在超图学习中建立了新的技术水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hypergraphs offer a generalized framework for capturing high-orderrelationships between entities and have been widely applied in various domains,including healthcare, social networks, and bioinformatics. Hypergraph neuralnetworks, which rely on message-passing between nodes over hyperedges to learnlatent representations, have emerged as the method of choice for predictivetasks in many of these domains. These approaches typically perform only a smallnumber of message-passing rounds to learn the representations, which they thenutilize for predictions. The small number of message-passing rounds comes at acost, as the representations only capture local information and foregolong-range high-order dependencies. However, as we demonstrate, blindlyincreasing the message-passing rounds to capture long-range dependency alsodegrades the performance of hyper-graph neural networks.  Recent works have demonstrated that implicit graph neural networks capturelong-range dependencies in standard graphs while maintaining performance.Despite their popularity, prior work has not studied long-range dependencyissues on hypergraph neural networks. Here, we first demonstrate that existinghypergraph neural networks lose predictive power when aggregating moreinformation to capture long-range dependency. We then propose ImplicitHypergraph Neural Network (IHNN), a novel framework that jointly learnsfixed-point representations for both nodes and hyperedges in an end-to-endmanner to alleviate this issue. Leveraging implicit differentiation, weintroduce a tractable projected gradient descent approach to train the modelefficiently. Extensive experiments on real-world hypergraphs for nodeclassification demonstrate that IHNN outperforms the closest prior works inmost settings, establishing a new state-of-the-art in hypergraph learning.</description>
      <author>example@mail.com (Akash Choudhuri, Yongjian Zhong, Bijaya Adhikari)</author>
      <guid isPermaLink="false">2508.14101v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Non-Dissipative Graph Propagation for Non-Local Community Detection</title>
      <link>http://arxiv.org/abs/2508.14097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为非对称图神经网络（uAGNN）的新型无监督社区检测方法，该方法通过非耗散动力学系统确保稳定性并有效传播长程信息，解决了异质性图中社区检测的挑战。&lt;h4&gt;背景&lt;/h4&gt;在异质性图中，相似节点和属于同一社区的节点通常距离较远，这使得传统的图神经网络方法难以有效进行社区检测，因为它们依赖于本质上局部的消息传递方案来学习节点表示。&lt;h4&gt;目的&lt;/h4&gt;解决异质性图中的社区检测问题，通过在消息传递过程中传播长程信息来提高检测效果。&lt;h4&gt;方法&lt;/h4&gt;提出非对称图神经网络（uAGNN），一种利用非耗散动力学系统确保稳定性和有效传播长程信息的无监督社区检测方法，通过使用非对称权重矩阵捕获局部和全局图结构。&lt;h4&gt;主要发现&lt;/h4&gt;在10个数据集上的广泛实验表明，uAGNN在高和中等异质性设置中表现优于传统方法，特别是在传统方法无法利用长程依赖性的情况下。&lt;h4&gt;结论&lt;/h4&gt;uAGNN作为无监督社区检测工具在不同图环境中具有强大潜力，能够有效克服异质性场景的限制。&lt;h4&gt;翻译&lt;/h4&gt;图中的社区检测旨在将节点聚类为有意义的组，这在异质性图中尤其具有挑战性，因为相似节点和属于同一社区的节点通常距离较远。当图神经网络处理这项任务时，这一点尤为明显，因为它们本质上依赖于局部的消息传递方案来学习节点表示，这些节点表示用于将节点聚类到社区中。在本工作中，我们认为在消息传递过程中传播长程信息是有效进行异质性图社区检测的关键。为此，我们引入了非对称图神经网络（uAGNN），这是一种新的无监督社区检测方法，它利用非耗散动力学系统确保稳定性并有效传播长程信息。通过采用非对称权重矩阵，uAGNN捕获了局部和全局图结构，克服了异质性场景带来的限制。在10个数据集上的广泛实验证明了uAGNN在高和中等异质性设置中的优越性能，而传统方法无法利用长程依赖性。这些结果强调了uAGNN作为不同图环境中无监督社区检测的强大工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Community detection in graphs aims to cluster nodes into meaningful groups, atask particularly challenging in heterophilic graphs, where nodes sharingsimilarities and membership to the same community are typically distantlyconnected. This is particularly evident when this task is tackled by graphneural networks, since they rely on an inherently local message passing schemeto learn the node representations that serve to cluster nodes into communities.In this work, we argue that the ability to propagate long-range informationduring message passing is key to effectively perform community detection inheterophilic graphs. To this end, we introduce the Unsupervised AntisymmetricGraph Neural Network (uAGNN), a novel unsupervised community detection approachleveraging non-dissipative dynamical systems to ensure stability and topropagate long-range information effectively. By employing antisymmetric weightmatrices, uAGNN captures both local and global graph structures, overcoming thelimitations posed by heterophilic scenarios. Extensive experiments across tendatasets demonstrate uAGNN's superior performance in high and mediumheterophilic settings, where traditional methods fail to exploit long-rangedependencies. These results highlight uAGNN's potential as a powerful tool forunsupervised community detection in diverse graph environments.</description>
      <author>example@mail.com (William Leeney, Alessio Gravina, Davide Bacciu)</author>
      <guid isPermaLink="false">2508.14097v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>X-Node: Self-Explanation is All We Need</title>
      <link>http://arxiv.org/abs/2508.10461v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;X-Node是一种自解释的图神经网络框架，使每个节点在预测过程中生成自己的解释，解决了GNN决策不透明的问题，同时保持了高分类准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在计算机视觉和医学图像分类中取得优异成果，但其决策过程不透明，限制了在高风险临床应用中的可信度，而这些应用中可解释性至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供局部、节点级解释的GNN框架，增强其可解释性和可信度，特别是在医疗等高风险领域。&lt;h4&gt;方法&lt;/h4&gt;构建结构化上下文向量编码节点的局部拓扑特征，通过轻量级推理器生成解释向量，该向量用于重建节点嵌入、生成自然语言解释并通过文本注入机制引导GNN。&lt;h4&gt;主要发现&lt;/h4&gt;X-Node在保持竞争性分类准确性的同时，能够生成忠实的、针对每个节点的解释，解决了传统GNN解释技术只能提供全局解释的局限性。&lt;h4&gt;结论&lt;/h4&gt;X-Node框架成功地将可解释性整合到GNN的预测过程中，使其更适合需要高度透明度和可解释性的高风险临床应用。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通过捕捉数据实例之间的结构依赖关系，在计算机视觉和医学图像分类任务中取得了最先进的结果。然而，它们的决策过程仍然不透明，限制了它们在高风险临床应用中的可信度，而这些应用中可解释性至关重要。现有的GNN可解释性技术通常是事后和全局的，对单个节点决策或局部推理的见解有限。我们引入了X-Node，一种自解释的GNN框架，其中每个节点在预测过程中生成自己的解释。对于每个节点，我们构建一个结构化的上下文向量，编码其局部拓扑内的可解释线索，如度、中心性、聚类、特征显著性和标签一致性。一个轻量级的推理器模块将此上下文映射到一个紧凑的解释向量，该向量有三个用途：(1)通过解码器重建节点的潜在嵌入以保持忠实性，(2)使用预训练的LLM(如Grok或Gemini)生成自然语言解释，以及(3)通过'文本注入'机制将解释反馈回消息传递管道来引导GNN本身。我们在从MedMNIST和MorphoMNIST派生的两个图数据集上评估了X-Node，并将其与GCN、GAT和GIN骨干网络集成。我们的结果表明，X-Node在保持竞争性分类准确性的同时，能够生成忠实的、针对每个节点的解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have achieved state-of-the-art results incomputer vision and medical image classification tasks by capturing structuraldependencies across data instances. However, their decision-making remainslargely opaque, limiting their trustworthiness in high-stakes clinicalapplications where interpretability is essential. Existing explainabilitytechniques for GNNs are typically post-hoc and global, offering limited insightinto individual node decisions or local reasoning. We introduce X-Node, aself-explaining GNN framework in which each node generates its own explanationas part of the prediction process. For every node, we construct a structuredcontext vector encoding interpretable cues such as degree, centrality,clustering, feature saliency, and label agreement within its local topology. Alightweight Reasoner module maps this context into a compact explanationvector, which serves three purposes: (1) reconstructing the node's latentembedding via a decoder to enforce faithfulness, (2) generating a naturallanguage explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)guiding the GNN itself via a "text-injection" mechanism that feeds explanationsback into the message-passing pipeline. We evaluate X-Node on two graphdatasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,and GIN backbones. Our results show that X-Node maintains competitiveclassification accuracy while producing faithful, per-node explanations.Repository: https://github.com/basiralab/X-Node.</description>
      <author>example@mail.com (Prajit Sengupta, Islem Rekik)</author>
      <guid isPermaLink="false">2508.10461v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
  <item>
      <title>4DNeX: Feed-Forward 4D Generative Modeling Made Easy</title>
      <link>http://arxiv.org/abs/2508.13154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://4dnex.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;4DNeX是一个前馈框架，可以从单张图像生成4D（动态3D）场景表示。它通过微调预训练的视频扩散模型，实现了高效、端到端的图像到4D生成。研究团队构建了大规模数据集4DNeX-10M，引入了统一的6D视频表示，并提出了一套有效的适应策略，使预训练的视频扩散模型能够用于4D建模。4DNeX能够生成高质量的动态点云，支持新视角视频合成。&lt;h4&gt;背景&lt;/h4&gt;现有的4D生成方法依赖于计算密集型优化或需要多帧视频输入，效率较低且应用受限。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效、端到端的图像到4D生成方法，解决现有方法计算效率低、需要多帧输入的问题。&lt;h4&gt;方法&lt;/h4&gt;1) 构建了4DNeX-10M大规模数据集，使用先进的重建方法生成高质量4D标注；2) 引入统一的6D视频表示，联合建模RGB和XYZ序列；3) 提出一套简单有效的适应策略，使预训练视频扩散模型能够用于4D建模；4) 通过微预训练的视频扩散模型实现端到端的图像到4D生成。&lt;h4&gt;主要发现&lt;/h4&gt;4DNeX能够生成高质量的动态点云，支持新视角视频合成，并且在效率和泛化性上优于现有的4D生成方法。&lt;h4&gt;结论&lt;/h4&gt;4DNeX为图像到4D建模提供了可扩展的解决方案，并为能够模拟动态场景演化的生成式4D世界模型奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了4DNeX，这是第一个从单张图像生成4D（即动态3D）场景表示的前馈框架。与依赖于计算密集型优化或需要多帧视频输入的现有方法不同，4DNeX通过微调预训练的视频扩散模型，实现了高效、端到端的图像到4D生成。具体来说，1)为缓解4D数据的稀缺性，我们构建了4DNeX-10M，这是一个使用先进重建方法生成高质量4D标注的大规模数据集；2)我们引入了统一的6D视频表示，联合建模RGB和XYZ序列，促进外观和几何的结构化学习；3)我们提出了一套简单而有效的适应策略，使预训练的视频扩散模型能够用于4D建模。4DNeX生成高质量的动态点云，支持新视角视频合成。大量实验证明，4DNeX在效率和泛化性上优于现有的4D生成方法，为图像到4D建模提供了可扩展的解决方案，并为模拟动态场景演化的生成式4D世界模型奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单张图像生成4D（动态3D）场景表示的问题。这个问题很重要，因为创建4D场景是生成式建模的核心能力，它为构建能够预测和模拟动态场景演化的4D世界模型奠定了基础，在AR/VR、电影制作和数字内容创作等领域有广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有4D场景建模方法的局限性：要么需要视频输入，要么依赖计算密集的优化过程。为了解决这些问题，他们考虑微调预训练的视频扩散模型，但面临两个挑战：4D数据稀缺和如何简单高效地适配预训练模型。作者借鉴了现有的视频扩散模型（如Wan2.1）和3D重建技术（如DUSt3R、MonST3R）来构建解决方案，通过构建4DNeX-10M数据集解决数据稀缺问题，并设计了XYZ初始化、归一化等策略来适配模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过微调预训练的视频扩散模型，实现从单张图像生成动态3D场景。作者提出统一的6D视频表示，将RGB（外观）和XYZ（几何）序列联合建模。整体流程：输入单张图像和初始化XYZ地图→VAE编码→沿宽度维度融合RGB和XYZ→结合噪声潜在和掩模输入到LoRA调优的DiT模型→解码生成RGB和XYZ视频→后优化恢复相机参数和深度图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 提出4DNeX，第一个图像到4D生成的前馈框架；2) 构建4DNeX-10M大型数据集；3) 引入6D视频表示联合建模RGB和XYZ；4) 设计简单有效的微调策略。相比之前工作，4DNeX无需计算密集优化，能端到端生成动态点云，效率更高（15分钟vs其他方法1小时以上），在动态度和泛化性方面表现更好，且能处理更多样化的场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 4DNeX通过微调预训练视频扩散模型和引入6D视频表示，实现了从单张图像高效生成高质量4D场景，为构建动态世界模型提供了可扩展的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,dynamic 3D) scene representations from a single image. In contrast to existingmethods that rely on computationally intensive optimization or requiremulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4Dgeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scaledataset with high-quality 4D annotations generated using advancedreconstruction approaches. 2) we introduce a unified 6D video representationthat jointly models RGB and XYZ sequences, facilitating structured learning ofboth appearance and geometry. 3) we propose a set of simple yet effectiveadaptation strategies to repurpose pretrained video diffusion models for 4Dmodeling. 4DNeX produces high-quality dynamic point clouds that enablenovel-view video synthesis. Extensive experiments demonstrate that 4DNeXoutperforms existing 4D generation methods in efficiency and generalizability,offering a scalable solution for image-to-4D modeling and laying the foundationfor generative 4D world models that simulate dynamic scene evolution.</description>
      <author>example@mail.com (Zhaoxi Chen, Tianqi Liu, Long Zhuo, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, Ziwei Liu)</author>
      <guid isPermaLink="false">2508.13154v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>TiP4GEN: Text to Immersive Panorama 4D Scene Generation</title>
      <link>http://arxiv.org/abs/2508.12415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了TiP4GEN，一个先进的文本到动态全景场景生成框架，能够实现细粒度内容控制并合成丰富的运动、几何一致的全景4D场景。&lt;h4&gt;背景&lt;/h4&gt;随着VR/AR技术的快速发展和广泛应用，对高质量、沉浸式动态场景的需求日益增长。然而，现有的生成工作主要集中在静态场景或狭小视角的动态场景，无法提供真正的360度全视角沉浸式体验。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成360度沉浸式虚拟环境的技术，解决现有技术在动态全景场景生成中的局限性。&lt;h4&gt;方法&lt;/h4&gt;TiP4GEN结合全景视频生成和动态场景重建。视频生成部分采用双分支生成模型，包括全景分支和透视分支，通过双向交叉注意力机制促进信息交换。场景重建部分提出基于3D高斯飞溅的几何对齐重建模型，使用度量深度图对齐时空点云，确保重建场景的几何一致性和时间连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明所提出设计的有效性，TiP4GEN在生成视觉上引人注目且运动连贯的动态全景场景方面具有优越性。&lt;h4&gt;结论&lt;/h4&gt;TiP4GEN能够有效生成高质量、沉浸式的动态全景场景，满足VR/AR技术对全视角体验的需求。&lt;h4&gt;翻译&lt;/h4&gt;随着VR/AR技术的快速发展和广泛应用，对创建高质量、沉浸式动态场景的需求日益增长。然而，现有的生成工作主要集中在静态场景或狭小视角的动态场景，无法提供真正的360度全视角沉浸式体验。在本文中，我们介绍了TiP4GEN，这是一个先进的文本到动态全景场景生成框架，能够实现细粒度内容控制并合成丰富的运动、几何一致的全景4D场景。TiP4GEN结合了全景视频生成和动态场景重建，以创建360度沉浸式虚拟环境。对于视频生成，我们引入了一个双分支生成模型，包括全景分支和透视分支，分别负责全局和局部视图生成。双向交叉注意力机制促进了分支之间的全面信息交换。对于场景重建，我们提出了一个基于3D高斯飞溅的几何对齐重建模型。通过使用度量深度图对齐时空点云，并使用估计的姿态初始化场景相机，我们的方法确保了重建场景的几何一致性和时间连贯性。大量实验证明了我们提出设计的有效性和TiP4GEN在生成视觉上引人注目且运动连贯的动态全景场景方面的优越性。我们的项目页面是https://ke-xing.github.io/TiP4GEN/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何生成高质量、沉浸式的360度动态全景场景问题。随着VR/AR技术在游戏、娱乐、教育等领域的广泛应用，对能够从任何视角观察的沉浸式动态场景需求日益增长，而现有方法无法提供真正的360度全景体验或缺乏对场景内容的精细控制，限制了虚拟现实应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性，设计了双分支架构来结合全景和透视两种视角的优势。借鉴了扩散模型（特别是视频扩散模型AnimateDiff）、3D高斯溅射技术、单目深度估计和LoRA微调等技术。作者认识到直接微调基础视频扩散模型会导致场景多样性有限，因此提出全景分支确保全局一致性，透视分支增强局部细节，并通过双向交叉注意力机制促进信息交换。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合全景和透视两种视角的优势，通过双分支架构生成高质量全景视频，并使用时空对齐技术确保重建4D场景的一致性。整体流程分为两阶段：1）双分支全景视频生成：全景分支接收全局文本提示确保整体一致性，透视分支接收多个局部文本提示增强多样性，通过双向交叉注意力机制促进信息交换；2）几何对齐重建：使用深度图进行空间对齐确保不同视角几何一致，通过时间对齐确保不同时间帧几何一致，并利用相机姿态估计减轻运动引起的不一致性，最后用3D高斯溅射优化重建场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）TiP4GEN框架实现从文本到高质量、运动丰富、几何一致的沉浸式全景4D场景生成；2）双分支生成模型结合全景和透视分支，通过双向交叉注意力实现全局一致性和局部多样性融合；3）几何对齐重建模型基于3D高斯溅射，通过时空对齐和相机姿态初始化提高一致性。相比之前工作，TiP4GEN专注于动态全景场景而非静态场景，支持多个局部提示实现精细内容控制，通过双分支架构处理全景与透视差异，并在重建阶段确保时空一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TiP4GEN通过创新的双分支架构和几何对齐重建技术，实现了从文本到高质量、运动丰富、几何一致的沉浸式全景4D场景的生成，为VR/AR应用提供了强大的内容创建工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement and widespread adoption of VR/AR technologies,there is a growing demand for the creation of high-quality, immersive dynamicscenes. However, existing generation works predominantly concentrate on thecreation of static scenes or narrow perspective-view dynamic scenes, fallingshort of delivering a truly 360-degree immersive experience from any viewpoint.In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamicpanorama scene generation framework that enables fine-grained content controland synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GENintegrates panorama video generation and dynamic scene reconstruction to create360-degree immersive virtual environments. For video generation, we introduce a\textbf{Dual-branch Generation Model} consisting of a panorama branch and aperspective branch, responsible for global and local view generation,respectively. A bidirectional cross-attention mechanism facilitatescomprehensive information exchange between the branches. For scenereconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds usingmetric depth maps and initializing scene cameras with estimated poses, ourmethod ensures geometric consistency and temporal coherence for thereconstructed scenes. Extensive experiments demonstrate the effectiveness ofour proposed designs and the superiority of TiP4GEN in generating visuallycompelling and motion-coherent dynamic panoramic scenes. Our project page is athttps://ke-xing.github.io/TiP4GEN/.</description>
      <author>example@mail.com (Ke Xing, Hanwen Liang, Dejia Xu, Yuyang Yin, Konstantinos N. Plataniotis, Yao Zhao, Yunchao Wei)</author>
      <guid isPermaLink="false">2508.12415v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection</title>
      <link>http://arxiv.org/abs/2508.12330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DoppDrive的新型多普勒驱动时间聚合方法，用于增强雷达点云密度并最小化散射，从而提高自动驾驶中的目标检测性能。&lt;h4&gt;背景&lt;/h4&gt;基于雷达的目标检测对自动驾驶至关重要，因为雷达具有长距离检测能力。然而，雷达点云的稀疏性，特别是在长距离情况下，给准确检测带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;解决雷达点云稀疏性问题，提高目标检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出DoppDrive方法，通过根据动态多普勒分量径向移动前一帧的点来消除径向散射，并根据点的多普勒和角度分配唯一的聚合持续时间来最小化切向散射。这是一种点云密度增强步骤，在检测前应用，与任何检测器兼容。&lt;h4&gt;主要发现&lt;/h4&gt;DoppDrive显著提高了各种检测器和数据集上的目标检测性能。&lt;h4&gt;结论&lt;/h4&gt;DoppDrive方法有效解决了雷达点云稀疏性问题，提高了目标检测性能。&lt;h4&gt;翻译&lt;/h4&gt;基于雷达的目标检测对自动驾驶至关重要，因为雷达具有长距离检测能力。然而，雷达点云的稀疏性，特别是在长距离情况下，给准确检测带来了挑战。现有方法通过时间聚合和自运动补偿来增加点密度，但这种方法会引入动态物体的散射，降低检测性能。我们提出了DoppDrive，一种新型的多普勒驱动时间聚合方法，增强雷达点云密度的同时最小化散射。根据动态多普勒分量径向移动前一帧的点以消除径向散射，并根据点的多普勒和角度分配唯一的聚合持续时间以最小化切向散射。DoppDrive是一种点云密度增强步骤，在检测前应用，与任何检测器兼容，我们证明了它显著提高了各种检测器和数据集上的目标检测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radar-based object detection is essential for autonomous driving due toradar's long detection range. However, the sparsity of radar point clouds,especially at long range, poses challenges for accurate detection. Existingmethods increase point density through temporal aggregation with ego-motioncompensation, but this approach introduces scatter from dynamic objects,degrading detection performance. We propose DoppDrive, a novel Doppler-Driventemporal aggregation method that enhances radar point cloud density whileminimizing scatter. Points from previous frames are shifted radially accordingto their dynamic Doppler component to eliminate radial scatter, with each pointassigned a unique aggregation duration based on its Doppler and angle tominimize tangential scatter. DoppDrive is a point cloud density enhancementstep applied before detection, compatible with any detector, and we demonstratethat it significantly improves object detection performance across variousdetectors and datasets.</description>
      <author>example@mail.com (Yuval Haitman, Oded Bialer)</author>
      <guid isPermaLink="false">2508.12330v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning For Point Cloud Denoising: A Survey</title>
      <link>http://arxiv.org/abs/2508.11932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对基于深度学习的点云去噪进行了全面综述，识别关键挑战，总结现有方法贡献，并提出针对去噪任务的分类法。&lt;h4&gt;背景&lt;/h4&gt;现实环境中的点云数据通常存在不同模态和强度的噪声，因此点云去噪作为预处理步骤对提高下游任务性能至关重要。&lt;h4&gt;目的&lt;/h4&gt;填补基于深度学习的点云去噪领域缺乏全面综述的空白，识别关键挑战，总结现有方法贡献，并提出针对去噪任务的自定义分类法。&lt;h4&gt;方法&lt;/h4&gt;将点云去噪制定为两步过程：异常值去除和表面噪声恢复，涵盖大多数点云去噪场景和需求，并比较不同方法的相似性、差异和各自优势。&lt;h4&gt;主要发现&lt;/h4&gt;基于深度学习的点云去噪模型因其强大的表示能力和灵活的架构，在去噪性能上已超越传统方法。&lt;h4&gt;结论&lt;/h4&gt;讨论了研究局限性和未来方向，为点云去噪的进一步发展提供见解。&lt;h4&gt;翻译&lt;/h4&gt;现实环境中的点云数据不可避免地存在不同模态和强度的噪声。因此，点云去噪作为预处理步骤对于提高下游任务性能至关重要。基于深度学习的点云去噪模型以其强大的表示能力和灵活的架构，在去噪性能上已超越传统方法。据我们所知，尽管最近在性能上有所进展，但还没有全面的调查系统地总结基于深度学习的点云去噪的发展。为填补这一空白，本文旨在识别基于深度学习的点云去噪的关键挑战，总结现有方法的主要贡献，并提出一个针对去噪任务的自定义分类法。为实现这一目标，我们将点云去噪制定为两步过程：异常值去除和表面噪声恢复，涵盖了大多数点云去噪的场景和需求。此外，我们从相似性、差异和各自优势方面比较了各种方法。最后，我们讨论了研究局限性和未来方向，为点云去噪的进一步发展提供见解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云去噪问题，即如何从含有噪声的点云数据中恢复出干净的表面。这个问题在现实中非常重要，因为现实环境中获取的点云数据不可避免地存在噪声，这些噪声会影响下游任务如点云分类、目标检测、语义分割和表面重建的性能。基于深度学习的点云去噪方法相比传统几何方法具有更好的去噪质量和泛化能力，能有效提高这些下游任务的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云去噪的挑战，将其视为一个两步过程：异常点移除和表面噪声恢复。作者发现现有综述只关注了点云去噪的某些方面，缺乏全面系统的总结。因此，作者提出了一种新的分类法，基于点云去噪的关键挑战对现有方法进行分类。作者借鉴了大量现有工作，包括点基方法（如PointCVaR、TripleMixer）、范围图像基方法（如WeatherNet、Slide）以及表面噪声恢复方法（如GeoDualCNN、PCDNF等），并分析了这些方法之间的内在联系和潜在关系。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云去噪视为一个两步过程：异常点移除和表面噪声恢复。整体实现流程包括：1) 异常点移除阶段，分为点基方法和范围图像基方法；2) 表面噪声恢复阶段，分为边缘感知去噪和精确表面恢复；3) 学习范式，包括监督学习和无监督学习；4) 评估阶段，使用不同的指标评估不同阶段的效果。边缘感知去噪关注保留几何边缘特征，精确表面恢复关注准确估计表面位置。学习范式可以是单独训练或联合训练，评估则根据不同阶段使用不同的指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次系统全面地综述了基于深度学习的点云去噪方法；2) 提出了一种基于点云去噪任务内在特征的新分类法；3) 分析了两个去噪阶段之间的内在联系；4) 在统一基准设置下评估了代表性方法；5) 指出了具体的未来研究方向。相比之前的工作，这篇论文的范围更全面，不仅关注表面噪声恢复，还涵盖了异常点移除；分类更系统，基于任务特征而非技术特征；分析更深入，探索了方法间的内在关系；评估更统一，提供了更全面的性能比较。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统全面地综述了基于深度学习的点云去噪方法，提出了一个涵盖异常点移除和表面噪声恢复的综合框架，并分析了方法间的内在联系，为该领域的研究提供了清晰的路线图。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world environment-derived point clouds invariably exhibit noise acrossvarying modalities and intensities. Hence, point cloud denoising (PCD) isessential as a preprocessing step to improve downstream task performance. Deeplearning (DL)-based PCD models, known for their strong representationcapabilities and flexible architectures, have surpassed traditional methods indenoising performance. To our best knowledge, despite recent advances inperformance, no comprehensive survey systematically summarizes the developmentsof DL-based PCD. To fill the gap, this paper seeks to identify key challengesin DL-based PCD, summarizes the main contributions of existing methods, andproposes a taxonomy tailored to denoising tasks. To achieve this goal, weformulate PCD as a two-step process: outlier removal and surface noiserestoration, encompassing most scenarios and requirements of PCD. Additionally,we compare methods in terms of similarities, differences, and respectiveadvantages. Finally, we discuss research limitations and future directions,offering insights for further advancements in PCD.</description>
      <author>example@mail.com (Chengwei Zhang, Xueyi Zhang, Mingrui Lao, Tao Jiang, Xinhao Xu, Wenjie Li, Fubo Zhang, Longyong Chen)</author>
      <guid isPermaLink="false">2508.11932v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Constructing Invariant and Equivariant Operations by Symmetric Tensor Network</title>
      <link>http://arxiv.org/abs/2508.12596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种系统方法用于构建有效的不变性和等变性操作，适用于几何深度学习中的对称神经网络设计。&lt;h4&gt;背景&lt;/h4&gt;在几何深度学习中，融入对称性的神经网络设计至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发不变性和等变性操作，这是该研究的核心目标。&lt;h4&gt;方法&lt;/h4&gt;提出一种系统方法构建有效的不变性和等变性操作，能处理不同阶的笛卡尔张量和不同类型的球形张量输入输出，并利用对称张量网络提供图形表示，简化相关证明和构造。&lt;h4&gt;主要发现&lt;/h4&gt;该方法成功应用于设计几何图神经网络中的等变性交互消息，以及学习材料本构定律的等变性机器学习模型。&lt;h4&gt;结论&lt;/h4&gt;该系统方法为几何深度学习中的对称性神经网络设计提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;在几何深度学习中融入对称性的神经网络设计至关重要。此工作的核心是开发不变性和等变性操作。本文提出了一种构建有效不变性和等变性操作的系统方法。该方法能够处理不同阶的笛卡尔张量形式以及不同类型的球形张量的输入和输出。此外，我们的方法利用对称张量网络提供图形表示，简化了与不变性和等变性函数相关的证明和构造。我们将这种方法应用于设计几何图神经网络中的等变性交互消息，以及学习材料本构定律的等变性机器学习模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何系统地构建不变量和等变操作的问题，以处理不同形式的输入和输出（包括不同秩的笛卡尔张量和不同类型的球谐张量）。这个问题在研究中很重要，因为物理世界中的量通常不依赖于特定坐标系，在坐标变换下表现出不变性或等变性；在机器学习中考虑对称性可以提高数据效率和模型泛化能力，特别是在处理分子、材料等具有3D几何结构的科学问题时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者的设计思路基于对称性的数学基础，借鉴了Weyl的经典不变性理论和Hilbert有限性定理；利用了量子多体系统中的对称张量网络表示方法；参考了Malgrange的从不变量导出等变函数的方法；以及球谐张量中的张量积操作。作者首先从简单的向量输入开始，逐步推广到笛卡尔张量和球谐张量，构建了'张量网络生成器'的概念，然后通过计算这些生成器的导数获得等变操作，形成了一个系统化的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用对称张量网络作为表示不变量和等变操作的基础，通过构建针对不同输入形式的'张量网络生成器'来生成所有不变多项式，然后通过计算这些生成器的导数获得对应的等变操作。整体流程包括：1)确定输入和输出形式（笛卡尔张量或球谐张量）；2)构建不变量生成器（对向量使用点积和三重积，对笛卡尔张量使用连接的张量网络与单位张量和Levi-Civita张量的收缩，对球谐张量使用投影算子）；3)通过Tup变换将不变函数转换为等变函数；4)应用于几何图神经网络和材料本构关系学习等具体任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提供统一的框架处理不同形式的输入和输出；2)将量子多体系统中的对称张量网络引入机器学习；3)系统地构建'张量网络生成器'；4)提供明确的Tup变换将不变函数转换为等变函数。相比之前的工作，本文方法不仅适用于向量特征，还能处理更高秩的笛卡尔张量和球谐张量；相比理论不变性构造方法，提供了基于张量网络的具体实现；相比基于群平均的方法，提供了更直观和系统的构造方法；相比现有的张量网络应用，提供了更全面的框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于对称张量网络的系统方法，用于构建处理各种形式输入和输出的不变量和等变操作，为几何深度学习和其他需要对称性约束的机器学习任务提供了统一的数学框架和实现工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Design of neural networks that incorporate symmetry is crucial for geometricdeep learning. Central to this effort is the development of invariant andequivariant operations. This works presents a systematic method forconstructing valid invariant and equivariant operations. It can handle inputsand outputs in the form of Cartesian tensors with different rank, as well asspherical tensors with different types. In addition, our method features agraphical representation utilizing the symmetric tensor network, whichsimplifies both the proofs and constructions related to invariant andequivariant functions. We also apply this approach to design the equivariantinteraction message for the geometry graph neural network, and equivariantmachine learning model to learn the constitutive law of materials.</description>
      <author>example@mail.com (Meng Zhang, Chao Wang, Hao Zhang, Shaojun Dong, Lixin He)</author>
      <guid isPermaLink="false">2508.12596v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos</title>
      <link>http://arxiv.org/abs/2508.14041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Project page: https://linjohnss.github.io/longsplat/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LongSplat是一个鲁棒的未定位3D高斯飞溅框架，用于解决从随意拍摄的长视频中合成新视角的挑战，通过增量联合优化、鲁棒姿态估计模块和高效八叉树锚点形成机制，实现了最先进的渲染质量、姿态准确性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;当前从具有不规则相机运动、未知相机姿态和广阔场景的随意拍摄长视频中合成新视角的方法面临姿态漂移、不准确的几何初始化和严重的内存限制等问题。&lt;h4&gt;目的&lt;/h4&gt;解决长视频新视角合成中的关键挑战，包括不规则相机运动、未知相机姿态和广阔场景。&lt;h4&gt;方法&lt;/h4&gt;LongSplat引入了三个关键组件：(1)增量联合优化，同时优化相机姿态和3D高斯；(2)利用学习3D先验的鲁棒姿态估计模块；(3)基于空间密度将密集点云转换为锚点的高效八叉树锚点形成机制。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的基准测试中，LongSplat实现了最先进的结果，显著提高了渲染质量、姿态准确性和计算效率。&lt;h4&gt;结论&lt;/h4&gt;LongSplat是一个有效的框架，能够处理随意拍摄长视频中的新视角合成问题，解决了当前方法中的多个限制。&lt;h4&gt;翻译&lt;/h4&gt;LongSplat解决了从随意拍摄的长视频中合成新视角的关键挑战，这些视频具有不规则的相机运动、未知的相机姿态和广阔的场景。当前方法常常遭受姿态漂移、不准确的几何初始化和严重的内存限制。为了解决这些问题，我们引入了LongSplat，一个鲁棒的未定位3D高斯飞溅框架，具有：(1)增量联合优化，同时优化相机姿态和3D高斯，以避免局部最小值并确保全局一致性；(2)利用学习3D先验的鲁棒姿态估计模块；以及(3)基于空间密度将密集点云转换为锚点的高效八叉树锚点形成机制。在具有挑战性的基准上的广泛实验表明，LongSplat实现了最先进的结果，相比之前的方法显著提高了渲染质量、姿态准确性和计算效率。项目页面：https://linjohnss.github.io/longsplat/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从随意拍摄的长视频中生成新视角的挑战，这类视频具有不规则相机运动、未知相机姿态和广阔场景的特点。这个问题在现实中很重要，因为随着智能手机普及，随意拍摄的长视频已成为3D内容的重要来源，而传统方法在处理这类视频时面临姿态漂移、几何初始化不准确和内存限制等问题。解决这一问题对于虚拟现实、增强现实、虚拟旅游、文化遗产保护等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前方法在处理随意拍摄长视频时的局限性，如COLMAP失败、CF-3DGS内存不足、LocalRF轨迹复杂时重建碎片化等。设计思路是创建一个统一框架联合优化相机姿态和3D高斯溅射，结合基于对应关系的姿态估计和自适应八锚树机制。作者借鉴了Scaffold-GS的锚点表示但改进了其依赖SfM的局限，使用MASt3R作为3D先验但通过联合优化修正，受LocalRF的渐进式构建启发但改进了不规则运动下的性能，并融合了PnP算法与光度对齐提升姿态估计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增量联合优化相机姿态和3D高斯溅射，解决随意拍摄长视频中的新视角合成问题，将姿态估计与场景重建紧密结合，并使用自适应八锚树机制减少内存使用。整体流程包括：1)初始化阶段用MASt3R估计初始帧并形成八锚树；2)全局优化阶段联合优化所有帧的高斯和姿态；3)帧插入阶段对新帧估计姿态并处理失败情况；4)局部优化阶段优化新帧可见高斯并利用可见性自适应窗口；5)最终全局细化阶段提高整体质量和一致性；6)使用光度、深度和重投影损失函数确保准确性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)增量联合优化同时优化相机姿态和3D高斯；2)鲁棒姿态估计模块结合PnP和光度细化；3)自适应八锚树形成机制减少内存使用。相比不同工作：1)与传统COLMAP方法不同，无需SfM预处理；2)与CF-3DGS不同，解决了内存限制问题；3)与LocalRF不同，处理复杂轨迹时不会碎片化；4)与MASt3R+Scaffold-GS不同，不依赖固定姿态估计；5)与其他锚点方法不同，使用动态调整体素大小而非固定分辨率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LongSplat通过增量联合优化相机姿态和3D高斯溅射，结合自适应八锚树机制，解决了随意拍摄长视频中未知相机姿态下的新视角合成问题，实现了高质量、内存高效的场景重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LongSplat addresses critical challenges in novel view synthesis (NVS) fromcasually captured long videos characterized by irregular camera motion, unknowncamera poses, and expansive scenes. Current methods often suffer from posedrift, inaccurate geometry initialization, and severe memory limitations. Toaddress these issues, we introduce LongSplat, a robust unposed 3D GaussianSplatting framework featuring: (1) Incremental Joint Optimization thatconcurrently optimizes camera poses and 3D Gaussians to avoid local minima andensure global consistency; (2) a robust Pose Estimation Module leveraginglearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism thatconverts dense point clouds into anchors based on spatial density. Extensiveexperiments on challenging benchmarks demonstrate that LongSplat achievesstate-of-the-art results, substantially improving rendering quality, poseaccuracy, and computational efficiency compared to prior approaches. Projectpage: https://linjohnss.github.io/longsplat/</description>
      <author>example@mail.com (Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu)</author>
      <guid isPermaLink="false">2508.14041v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing</title>
      <link>http://arxiv.org/abs/2508.13797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SIGGRAPH 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Sketch3DVE是一种基于草图的3D感知视频编辑方法，能够处理具有重大视角变化的视频，实现详细的局部3D场景编辑。&lt;h4&gt;背景&lt;/h4&gt;现有的视频编辑方法在风格迁移或外观修改方面取得了良好效果，但在处理视频中的3D场景结构内容编辑时仍面临挑战，特别是当涉及重大视角变化（如大角度相机旋转或缩放）时。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够实现对具有重大视角变化视频的详细局部操作，解决生成新视角内容、保持未编辑区域和将稀疏2D输入转换为逼真3D视频输出等挑战。&lt;h4&gt;方法&lt;/h4&gt;1) 使用图像编辑方法为第一帧生成编辑结果并传播到其余帧；2) 利用素描作为精确几何控制的交互工具；3) 通过密集立体方法估计输入视频的点云和相机参数；4) 提出使用深度图表示新编辑组件3D几何的点云编辑方法；5) 引入3D感知掩码传播策略并使用视频扩散模型生成逼真编辑视频。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明Sketch3DVE在视频编辑方面具有优越性，能够有效处理视角变化并保持编辑内容与原始视频的一致性。&lt;h4&gt;结论&lt;/h4&gt;Sketch3DVE成功解决了视频编辑中处理重大视角变化的挑战，实现了对3D场景结构的详细编辑，同时保持未编辑区域的特征不变。&lt;h4&gt;翻译&lt;/h4&gt;最近视频编辑方法在风格迁移或外观修改方面取得了吸引人的结果。然而，编辑视频中3D场景的结构内容仍然具有挑战性，特别是在处理重大视角变化时，如大角度相机旋转或缩放。主要挑战包括生成与原始视频保持一致的新视角内容、保持未编辑区域以及将稀疏2D输入转换为逼真的3D视频输出。为解决这些问题，我们提出了Sketch3DVE，一种基于草图的3D感知视频编辑方法，以实现对具有重大视角变化视频的详细局部操作。为解决稀疏输入带来的挑战，我们采用图像编辑方法为第一帧生成编辑结果，然后将其传播到视频的其余帧。我们利用素描作为精确几何控制的交互工具，同时也支持其他基于掩码的图像编辑方法。为处理视角变化，我们对视频中的3D信息进行了详细分析和操作。具体来说，我们使用密集立体方法估计输入视频的点云和相机参数。然后，我们提出了一种使用深度图表示新编辑组件3D几何的点云编辑方法，将其与原始3D场景有效对齐。为将新编辑内容与原始视频无缝合并，同时保持未编辑区域的特征，我们引入了3D感知掩码传播策略，并采用视频扩散模型生成逼真的编辑视频。大量实验证明了Sketch3DVE在视频编辑方面的优越性。主页和代码：http://geometrylearning.com/Sketch3DVE/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决对具有显著视角变化的3D场景视频进行结构编辑的问题，特别是在处理大相机旋转或缩放时如何生成与原始视角一致的新内容。这个问题很重要，因为现有视频编辑方法主要针对风格转换或外观修改，难以处理3D场景的结构内容编辑；在未编辑区域保持不变的同时，将稀疏的2D输入（如草图）转换为真实的3D视频输出是一项重大挑战；这种技术在电影制作、教育、机器人、AR/VR等领域有广泛的应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有视频编辑方法的局限性，发现它们难以处理有显著视角变化的视频；注意到图像编辑方法在灵活性方面表现良好，但直接应用于视频编辑存在挑战；观察到扩散模型在视频生成方面的进展，但发现它们主要处理时间运动信息，缺乏对3D信息的广泛分析；考虑到3D信息对于处理视角变化的重要性，提出了基于点云的3D表示方法；为了解决2D草图与视频之间的领域差距，先编辑第一帧，然后将编辑效果传播到视频的其他帧。该方法借鉴了MagicQuill用于第一帧编辑，DUSt3R用于获取点云和相机参数，CogVideoX用于视频生成，以及ControlNet用于有效编辑和保持未编辑区域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：使用草图作为交互工具进行精确几何控制；通过显式分析和操作视频中的3D信息来处理视角变化；利用深度图表示和编辑3D几何，确保编辑区域与原始3D场景对齐；提出一种3D感知的掩码传播策略，准确识别和保留未编辑区域。整体实现流程包括：1) 第一帧编辑：使用图像编辑方法根据用户输入的草图、掩码和文本提示编辑视频的第一帧；2) 3D信息提取：从输入视频中获取点云和相机参数；3) 点云编辑：利用深度图表示和编辑区域的3D几何，通过变换将编辑内容与原始场景对齐；4) 掩码传播：构建3D掩码并将其渲染到所有帧中；5) 视频生成：使用视频扩散模型结合点云渲染结果、原始视频和传播的掩码生成最终编辑视频。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了一种基于草图和3D感知的视频编辑方法，能够处理具有显著视角变化的视频；2) 提出了一种基于深度图表示的点云编辑方法，有效将2D图像编辑映射到3D空间；3) 开发了一种3D感知的掩码传播策略，能够准确识别和保留未编辑区域；4) 设计了一个精确的区域修改视频扩散模型，能够合成编辑组件的新视角结果。相比之前工作的不同：与现有视频编辑方法相比，Sketch3DVE能够处理结构编辑（如组件插入和替换），而不仅仅是外观修改；与相机可控的视频生成方法相比，Sketch3DVE能够更好地保持未编辑区域，并生成更一致的编辑结果；与基于草图的内容编辑方法相比，Sketch3DVE专门针对3D场景视频，能够处理显著视角变化；通过显式处理3D信息，Sketch3DVE能够在视角变化时保持编辑内容的几何一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Sketch3DVE通过结合草图交互、3D点云分析和视频扩散模型，实现了对具有显著视角变化的3D场景视频的高质量编辑，能够精确控制几何形状并保持未编辑区域的完整性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent video editing methods achieve attractive results in style transfer orappearance modification. However, editing the structural content of 3D scenesin videos remains challenging, particularly when dealing with significantviewpoint changes, such as large camera rotations or zooms. Key challengesinclude generating novel view content that remains consistent with the originalvideo, preserving unedited regions, and translating sparse 2D inputs intorealistic 3D video outputs. To address these issues, we propose Sketch3DVE, asketch-based 3D-aware video editing method to enable detailed localmanipulation of videos with significant viewpoint changes. To solve thechallenge posed by sparse inputs, we employ image editing methods to generateedited results for the first frame, which are then propagated to the remainingframes of the video. We utilize sketching as an interaction tool for precisegeometry control, while other mask-based image editing methods are alsosupported. To handle viewpoint changes, we perform a detailed analysis andmanipulation of the 3D information in the video. Specifically, we utilize adense stereo method to estimate a point cloud and the camera parameters of theinput video. We then propose a point cloud editing approach that uses depthmaps to represent the 3D geometry of newly edited components, aligning themeffectively with the original 3D scene. To seamlessly merge the newly editedcontent with the original video while preserving the features of uneditedregions, we introduce a 3D-aware mask propagation strategy and employ a videodiffusion model to produce realistic edited videos. Extensive experimentsdemonstrate the superiority of Sketch3DVE in video editing. Homepage and code:http://http://geometrylearning.com/Sketch3DVE/</description>
      <author>example@mail.com (Feng-Lin Liu, Shi-Yang Li, Yan-Pei Cao, Hongbo Fu, Lin Gao)</author>
      <guid isPermaLink="false">2508.13797v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot</title>
      <link>http://arxiv.org/abs/2508.13785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为'DIPPeR'的自主矿场检测机器人，用于露天矿爆破孔的内部检查，通过自动化方法解决人工检查的局限性，实现精确的孔下传感器定位和目标导航。&lt;h4&gt;背景&lt;/h4&gt;在露天矿开采中，爆破孔需要内部检查以研究孔内材料类型和性质。传统的人工检查方法缓慢、昂贵，且在揭示孔及其内容物的几何和地质特性方面存在显著局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个自主矿场检测机器人系统，实现爆破孔的自动寻找、检测和精确的孔下传感器定位，以提高检查效率并降低成本。&lt;h4&gt;方法&lt;/h4&gt;提出一个稳健的爆破孔寻找和检测框架，处理LiDAR传感器收集的点云数据，提取地面上的锥形钻废物体积，通过将3D点投影到虚拟深度图像进行2D分割，识别孔中心并抑制非最大候选值，确保精确传感器放置和避免碰撞。系统在机器人导航过程中自动调整投影参数以适应不同条件，实现连续的目标跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;导航和感知系统在高保真模拟环境和现场测试中均表现出有效性，能够准确识别和定位爆破孔，为后续孔下检查提供精确位置信息。&lt;h4&gt;结论&lt;/h4&gt;DIPPeR机器人系统能够有效解决露天矿爆破孔检查的难题，通过自动化方法提高了检查效率和准确性，有望显著降低材料处理成本。&lt;h4&gt;翻译&lt;/h4&gt;在露天矿开采中，需要在爆破场地表面钻孔并使用炸药进行爆破以促进挖掘。这些爆破孔需要内部检查以研究孔内材料类型和性质。了解这些性质可以显著降低下游过程中的材料处理成本。人工孔检查缓慢且昂贵，在揭示孔及其内容物的几何和地质特性方面存在主要局限性。这促使我们开发了自主矿场检测机器人——'DIPPeR'。本文解释了该项目的自动化方面。我们提出了一个稳健的爆破孔寻找和检测框架，实现基于目标的导航和精确的孔下传感器定位。该流程首先处理机载LiDAR传感器收集的点云数据，提取地面上的锥形钻废物体积。通过将3D锥形点投影到虚拟深度图像中，在2D域实现分割，得到图像中心的圆形孔和带领锥面。然后我们使用稳健的检测模块识别孔中心，同时抑制非最大候选值，确保精确的传感器放置以进行孔下检查，并避免与孔壁碰撞。为了实现自主孔寻找，流程在机器人导航过程中自动调整其投影参数，以适应点稀疏性和孔开口尺寸的变化，确保2D图像中孔的一致外观。这允许机器人接近目标点时连续跟踪目标孔。我们在高保真模拟环境和现场测试中展示了导航和感知系统的有效性。演示视频可在'https://www.youtube.com/watch?v=fRNbcBcaSqE'观看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决露天矿场爆破孔的内部检测问题。人工检查爆破孔既缓慢又昂贵，且无法全面揭示孔的几何和地质特性。这个问题重要是因为了解孔内材料特性可显著降低下游材料处理成本，同时矿场环境对人类操作者存在安全风险和极端天气条件，自动化检测能提高效率和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于矿场环境的特殊挑战（GPS定位不足、环境恶劣、孔的几何特性）设计了方法。选择LiDAR而非相机，因其对光照变化更具鲁棒性。借鉴了Paul等人(2016)和Vu等人(2019)的点云投影方法，以及Liu等人(2022a)的最优圆拟合算法。同时，作者利用矿场爆破孔的规则布局和GPS可靠性，设计了基于接近度的自适应导航策略，避免了昂贵的地图构建过程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用矿场结构化环境的先验知识和LiDAR感知能力，实现从粗略GPS导航到精确相对定位的平滑过渡，并通过自适应参数调整确保孔检测的一致性。整体流程包括：1)导航阶段：使用GPS导航到大致位置，检测到孔后切换到局部坐标系进行精细导航，接近时使用机器人坐标系进行视觉伺服；2)感知阶段：校正地面倾斜，检测锥形钻屑堆，将3D点云投影到2D虚拟图像，通过粗检测确定孔的大致位置，再通过精检测确定精确位置；3)执行阶段：精确对准后进行孔内检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自适应感知参数调整，根据距离自动调整虚拟相机参数；2)两阶段检测流程，从粗略到精细定位；3)非最大值抑制方法，有效消除'幽灵孔'；4)基于接近度的自适应导航策略，利用多坐标系解耦定位；5)基于经典计算机视觉的方法，避免数据驱动学习的需求。相比Valencia等人(2024)的HOG+SVM方法，本文方法不需要大量数据标注且圆拟合更准确；相比Paul等人(2016)和Vu等人(2019)，增加了自适应参数调整和两阶段流程；相比Kiran等人(2024)，专门针对深孔爆破检测设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于LiDAR的自适应爆破孔检测与导航框架，通过结合结构化环境的先验知识和两阶段检测流程，实现了矿场爆破孔的高精度自主检测，无需昂贵的地图构建和大量数据训练。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In open-pit mining, holes are drilled into the surface of the excavation siteand detonated with explosives to facilitate digging. These blast holes need tobe inspected internally for investigation of downhole material types andproperties. Knowing these properties can lead to significant savings inmaterial handling costs in downstream processes. Manual hole inspection is slowand expensive, with major limitations in revealing the geometric and geologicalproperties of the holes and their contents. This has been the motivation forthe development of our autonomous mine-site inspection robot - "DIPPeR". Inthis paper, the automation aspect of the project is explained. We present arobust blast hole seeking and detection framework that enables target-basednavigation and accurate down-hole sensor positioning. The pipeline firstprocesses point-cloud data collected by the on-board LiDAR sensors, extractingthe cone-shaped volume of drill-waste above the ground. By projecting the 3Dcone points into a virtual depth image, segmentation is achieved in the 2Ddomain, yielding a circular hole at the image centre and a collared cone face.We then identify the hole centre using a robust detection module whilesuppressing non-maximum candidates, ensuring precise sensor placement fordown-hole inspection and avoiding collisions with the cavity wall. To enableautonomous hole-seeking, the pipeline automatically adjusts its projectionparameters during robot navigation to account for variations in point sparsityand hole opening size, ensuring a consistent hole appearance in 2D images. Thisallows continuous tracking of the target hole as the robot approaches the goalpoint. We demonstrate the effectiveness of our navigation and perception systemin both high-fidelity simulation environments and on-site field tests. Ademonstration video is available at"https://www.youtube.com/watch?v=fRNbcBcaSqE".</description>
      <author>example@mail.com (Liyang Liu, Ehsan Mihankhah, Nathan Wallace, Javier Martinez, Andrew J. Hill)</author>
      <guid isPermaLink="false">2508.13785v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video</title>
      <link>http://arxiv.org/abs/2508.13756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM  International Conference on Multimedia (MM '25), October 27--31, 2025,  Dublin, Ireland&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;点云视频实时流传输面临数据量大和丢包敏感的挑战，INDS框架通过基于信息中心网络的自适应流媒体技术解决了传统协议的局限性，实现了更高效的传输和缓存。&lt;h4&gt;背景&lt;/h4&gt;点云视频实时流传输具有数据量大和对丢包敏感的特点，在动态网络条件下对沉浸式应用仍然是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;引入INDS（增量命名数据流），一个基于信息中心网络的自适应流媒体框架，重新思考分层分层媒体的传输方式。&lt;h4&gt;方法&lt;/h4&gt;INDS利用点云视频的八叉树结构和富有表现力的内容命名，支持基于消费者带宽和解码能力的渐进式部分检索增强层。通过将时间窗口与帧组（GoF）相结合，INDS的命名方案支持细粒度的网络内缓存，并促进高效的多用户数据重用。&lt;h4&gt;主要发现&lt;/h4&gt;INDS可作为叠加层部署，与基于QUIC的传输基础设施以及未来的媒体 over QUIC（MoQ）架构兼容，无需更改底层IP网络。原型实现表明，与最先进的DASH风格系统相比，INDS可降低高达80%的延迟，提高15-50%的吞吐量，并增加20-30%的缓存命中率。&lt;h4&gt;结论&lt;/h4&gt;这些结果共同确立了INDS作为在多变和有损条件下实时点云流的可扩展、缓存友好解决方案，同时它与MoQ叠加层的兼容性进一步将其定位为新兴沉浸式媒体系统的实用、向前兼容的架构。&lt;h4&gt;翻译&lt;/h4&gt;点云视频的实时流传输，以其巨大的数据量和对丢包的高敏感性，在动态网络条件下仍然是沉浸式应用的一个关键挑战。虽然像TCP这样的面向连接协议以及QUIC等更现代的替代方案缓解了一些传输层的低效率，包括头部阻塞，但它们仍然保留了粗粒度的基于段的传输模型和集中式控制循环，这限制了细粒度的适应性和有效缓存。我们引入了INDS（增量命名数据流），这是一个基于信息中心网络的自适应流媒体框架，重新思考了分层分层媒体的传输方式。INDS利用点云视频的八叉树结构和富有表现力的内容命名，支持基于消费者带宽和解码能力的渐进式部分检索增强层。通过将时间窗口与帧组（GoF）相结合，INDS的命名方案支持细粒度的网络内缓存，并促进高效的多用户数据重用。INDS可以作为叠加层部署，与基于QUIC的传输基础设施以及未来的媒体 over QUIC（MoQ）架构兼容，而无需更改底层IP网络。我们的原型实现表明，与最先进的DASH风格系统相比，INDS可降低高达80%的延迟，提高15-50%的吞吐量，并增加20-30%的缓存命中率。这些结果共同确立了INDS作为在多变和有损条件下实时点云流的可扩展、缓存友好解决方案，同时它与MoQ叠加层的兼容性进一步将其定位为新兴沉浸式媒体系统的实用、向前兼容的架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time streaming of point cloud video, characterized by massive datavolumes and high sensitivity to packet loss, remains a key challenge forimmersive applications under dynamic network conditions. Whileconnection-oriented protocols such as TCP and more modern alternatives likeQUIC alleviate some transport-layer inefficiencies, including head-of-lineblocking, they still retain a coarse-grained, segment-based delivery model anda centralized control loop that limit fine-grained adaptation and effectivecaching. We introduce INDS (Incremental Named Data Streaming), an adaptivestreaming framework based on Information-Centric Networking (ICN) that rethinksdelivery for hierarchical, layered media. INDS leverages the Octree structureof point cloud video and expressive content naming to support progressive,partial retrieval of enhancement layers based on consumer bandwidth anddecoding capability. By combining time-windows with Group-of-Frames (GoF),INDS's naming scheme supports fine-grained in-network caching and facilitatesefficient multi-user data reuse. INDS can be deployed as an overlay, remainingcompatible with QUIC-based transport infrastructure as well as futureMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IPnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%higher throughput, and 20-30% increased cache hit rates compared tostate-of-the-art DASH-style systems. Together, these results establish INDS asa scalable, cache-friendly solution for real-time point cloud streaming undervariable and lossy conditions, while its compatibility with MoQ overlaysfurther positions it as a practical, forward-compatible architecture foremerging immersive media systems.</description>
      <author>example@mail.com (Ruonan Chai, Yixiang Zhu, Xinjiao Li, Jiawei Li, Zili Meng, Dirk Kutscher)</author>
      <guid isPermaLink="false">2508.13756v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.13485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, Accepted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CORENet是一种新颖的跨模态去噪框架，用于解决4D雷达点云稀疏和噪声特性带来的物体检测挑战&lt;h4&gt;背景&lt;/h4&gt;4D雷达物体检测因其在不利天气条件下的鲁棒性和在不同驾驶场景中提供丰富空间信息的能力而受到广泛关注&lt;h4&gt;目的&lt;/h4&gt;解决4D雷达点云的稀疏和噪声特性对有效感知构成的挑战&lt;h4&gt;方法&lt;/h4&gt;提出CORENet，利用LiDAR监督来识别噪声模式并从原始4D雷达数据中提取判别性特征&lt;h4&gt;主要发现&lt;/h4&gt;在具有高噪声水平的Dual-Radar数据集上评估，证明CORENet能有效增强检测鲁棒性&lt;h4&gt;结论&lt;/h4&gt;CORENet与现有主流方法相比具有优越的性能&lt;h4&gt;翻译&lt;/h4&gt;基于4D雷达的物体检测因其在不利天气条件下的鲁棒性以及在各种驾驶场景中提供丰富空间信息的能力而受到广泛关注。然而，4D雷达点云的稀疏和噪声特性对有效感知构成了重大挑战。为解决这一限制，我们提出了CORENet，一种新颖的跨模态去噪框架，利用LiDAR监督来识别噪声模式并从原始4D雷达数据中提取判别性特征。作为即插即用架构设计，我们的解决方案能够无缝集成到基于体素的检测框架中，而无需修改现有流程。值得注意的是，所提出的方法仅在训练期间使用LiDAR数据进行跨模态监督，而在推理期间保持完全的雷达-only操作。在具有高噪声水平的具有挑战性的Dual-Radar数据集上进行的大量评估证明了我们的框架在增强检测鲁棒性方面的有效性。全面的实验验证了CORENet与现有主流方法相比具有优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D radar-based object detection has garnered great attention for itsrobustness in adverse weather conditions and capacity to deliver rich spatialinformation across diverse driving scenarios. Nevertheless, the sparse andnoisy nature of 4D radar point clouds poses substantial challenges foreffective perception. To address the limitation, we present CORENet, a novelcross-modal denoising framework that leverages LiDAR supervision to identifynoise patterns and extract discriminative features from raw 4D radar data.Designed as a plug-and-play architecture, our solution enables seamlessintegration into voxel-based detection frameworks without modifying existingpipelines. Notably, the proposed method only utilizes LiDAR data forcross-modal supervision during training while maintaining full radar-onlyoperation during inference. Extensive evaluation on the challenging Dual-Radardataset, which is characterized by elevated noise level, demonstrates theeffectiveness of our framework in enhancing detection robustness. Comprehensiveexperiments validate that CORENet achieves superior performance compared toexisting mainstream approaches.</description>
      <author>example@mail.com (Fuyang Liu, Jilin Mei, Fangyuan Mao, Chen Min, Yan Xing, Yu Hu)</author>
      <guid isPermaLink="false">2508.13485v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>UNICON: UNIfied CONtinual Learning for Medical Foundational Models</title>
      <link>http://arxiv.org/abs/2508.14024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UNICON框架，使医学基础模型能够无缝适应不同领域、任务和模态，解决了医学影像数据稀缺导致的预训练挑战。&lt;h4&gt;背景&lt;/h4&gt;基础模型通常在大型数据集上训练以捕捉领域普遍趋势，但医学影像领域数据稀缺，难以针对每个领域、模态或任务进行预训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一且可扩展的框架，使医学基础模型能够动态适应不同影像模态、解剖区域和临床目标，避免灾难性遗忘和任务干扰。&lt;h4&gt;方法&lt;/h4&gt;提出UNICON（UNIfied CONtinual Learning for Medical Foundational Models）框架，通过持续学习在不同领域或任务上顺序微调模型，而非孤立处理变化。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型可动态扩展跨越影像模态、解剖区域和临床目标；将胸部CT基础模型从分类任务适应到预后和分割任务，性能均得到提升；整合PET扫描后，Dice分数较基线提高5%。&lt;h4&gt;结论&lt;/h4&gt;医学基础模型并非固有受限于初始训练范围，而是可以持续发展，为医学影像领域的通用AI模型开辟了道路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在大型数据集上训练，以捕捉领域的普遍趋势。然而，在医学影像领域，数据稀缺使得为每个领域、模态或任务进行预训练具有挑战性。持续学习通过在不同领域或任务上顺序微调模型提供了解决方案，使其能够整合新知识，而不需要每个训练阶段都有大型数据集。在本文中，我们提出了UNICON（UNIfied CONtinual Learning for Medical Foundational Models）框架，使基础模型能够无缝适应不同的领域、任务和模态。与将变化孤立对待的传统适应方法不同，UNICON提供了一个统一且可无限扩展的框架。通过精心整合，我们证明基础模型可以动态扩展跨越影像模态、解剖区域和临床目标，而不会发生灾难性遗忘或任务干扰。通过实验，我们验证了将最初为分类任务训练的胸部CT基础模型适应到预后和分割任务的方法。我们的结果显示在两个额外任务上性能均有提升。此外，我们持续整合PET扫描，实现了与相应基线相比5%的Dice分数提升。这些发现确立了基础模型并非固有地受限于其初始训练范围，而是可以发展的，为医学影像领域的通用AI模型铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学基础模型如何持续适应不同影像模态、任务和解剖区域的问题，而不会出现灾难性遗忘。这个问题在现实中非常重要，因为医学数据稀缺且获取困难，而医学影像又具有多样性（多种模态、任务和身体部位），传统方法需要为每种情况单独训练模型，既不经济也不现实。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了医学基础模型面临的挑战，然后借鉴了持续学习、领域适应和LoRA等技术，设计了UNICON框架。作者通过两种适应机制（模型内适应WMA和模型后适应PMA）来解决参数效率和知识保留问题，并添加了分辨率适应模块来处理不同分辨率的医学影像。实验中，作者逐步将模型从单一分类任务扩展到多模态分割，验证了方法的有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过轻量级适配器扩展基础模型功能，而不修改其核心参数，实现持续学习。整体流程：1)使用预训练的医学基础模型并冻结其参数；2)根据新任务添加特定适配器（如预后预测的MLP适配器或分割任务的解码器）；3)使用LoRA技术微调部分参数；4)动态添加补丁嵌入层适应不同分辨率；5)按顺序逐步适应新任务、模态和区域，每个新任务添加新适配器而不修改已有模块。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一的持续学习框架，同时处理跨任务、跨模态和跨解剖区域的适应；2)参数高效的适应方法，使用轻量级适配器而非全模型微调；3)分辨率自适应能力，处理不同分辨率的医学影像；4)多模态融合机制，有效整合不同影像模态。相比之前工作，UNICON范围更广（多维度适应）、更灵活（动态适应新任务）、更高效（低计算资源需求）且更全面（解决分辨率问题）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UNICON提出了一种统一的持续学习框架，使医学基础模型能够高效地适应多种医学影像任务、模态和解剖区域，在医疗数据稀缺的情况下实现了模型的多功能扩展和应用，避免了灾难性遗忘。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models are trained on extensive datasets to capture the generaltrends of a domain. However, in medical imaging, the scarcity of data makespre-training for every domain, modality, or task challenging. Continuallearning offers a solution by fine-tuning a model sequentially on differentdomains or tasks, enabling it to integrate new knowledge without requiringlarge datasets for each training phase. In this paper, we propose UNIfiedCONtinual Learning for Medical Foundational Models (UNICON), a framework thatenables the seamless adaptation of foundation models to diverse domains, tasks,and modalities. Unlike conventional adaptation methods that treat these changesin isolation, UNICON provides a unified, perpetually expandable framework.Through careful integration, we show that foundation models can dynamicallyexpand across imaging modalities, anatomical regions, and clinical objectiveswithout catastrophic forgetting or task interference. Empirically, we validateour approach by adapting a chest CT foundation model initially trained forclassification to a prognosis and segmentation task. Our results show improvedperformance across both additional tasks. Furthermore, we continuallyincorporated PET scans and achieved a 5\% improvement in Dice score compared torespective baselines. These findings establish that foundation models are notinherently constrained to their initial training scope but can evolve, pavingthe way toward generalist AI models for medical imaging.</description>
      <author>example@mail.com (Mohammad Areeb Qazi, Munachiso S Nwadike, Ibrahim Almakky, Mohammad Yaqub, Numan Saeed)</author>
      <guid isPermaLink="false">2508.14024v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.13977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;深度估计是自动驾驶、机器人和增强现实领域中3D场景理解的基础任务。现有数据集如KITTI、nuScenes和DDAD虽推动了该领域发展，但在多样性和可扩展性方面存在局限。作者提出一个大规模、多样化的帧级连续深度估计数据集，包含20K视频帧，采用轻量级采集流程确保低成本下的广泛场景覆盖，同时提供稀疏但统计充分的真实数据支持鲁棒训练。&lt;h4&gt;背景&lt;/h4&gt;深度估计是自动驾驶、机器人和增强现实领域中3D场景理解的基础任务。现有数据集如KITTI、nuScenes和DDAD推动了该领域发展，但在多样性和可扩展性方面存在局限。随着这些数据集上的基准性能接近饱和，需要新一代大规模、多样化且成本高效的数据集来支持基础模型和多模态学习时代。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有深度估计数据集的多样性不足和可扩展性有限的问题，作者旨在开发一个大规模、多样化、帧级连续的深度估计数据集，用于支持基础模型和多模态学习时代。&lt;h4&gt;方法&lt;/h4&gt;作者介绍了一个大规模、多样化、帧级连续的深度估计数据集，包含20K视频帧。他们采用轻量级采集流程确保低成本下的广泛场景覆盖，同时提供稀疏但统计充分的真实数据以支持鲁棒训练。&lt;h4&gt;主要发现&lt;/h4&gt;与现有数据集相比，新数据集在驾驶场景方面具有更高的多样性，且深度密度更低，为泛化能力带来新挑战。基准实验使用标准单目深度估计模型验证了该数据集的实用性，并突显了在具有挑战性条件下的显著性能差距。&lt;h4&gt;结论&lt;/h4&gt;该数据集为推进深度估计研究建立了新平台，有助于推动该领域的进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;深度估计是自动驾驶、机器人技术和增强现实领域中3D场景理解的基本任务。现有的深度数据集，如KITTI、nuScenes和DDAD，虽然推动了该领域的发展，但在多样性和可扩展性方面存在局限性。随着这些数据集上的基准性能趋于饱和，迫切需要新一代大规模、多样化且成本高效的数据集，以支持基础模型和多模态学习时代。为应对这些挑战，我们介绍了一个用于动态户外驾驶环境中深度估计的大规模、多样化、帧级连续数据集，包含20K视频帧用于评估现有方法。我们的轻量级采集流程确保了低成本下的广泛场景覆盖，而稀疏但统计充分的真实数据则支持鲁棒训练。与现有数据集相比，我们的数据集在驾驶场景方面具有更高的多样性，且深度密度更低，为泛化能力创造了新的挑战。使用标准单目深度估计模型进行的基准实验验证了该数据集的实用性，并突显了在具有挑战性条件下的显著性能差距，为推进深度估计研究建立了新平台。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有深度估计数据集（如KITTI、nuScenes、DDAD）在多样性和可扩展性方面的局限性。这个问题很重要，因为深度估计是自动驾驶、机器人和增强现实等领域3D场景理解的基础任务，准确深度地图支持障碍物检测、运动规划等关键功能。随着深度神经网络和基础模型的发展，现有数据集性能接近饱和，需要新的大规模、多样化数据集来推动研究进步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，包括场景多样性不足、标注密度有限和成本高昂等问题。他们认识到随着模型容量增加，现有基准性能接近饱和，需要新数据源。设计上借鉴了现有数据集的多传感器采集方法、数据组织格式和评估指标，但在采集流程设计上采用轻量级方案，确保广泛场景覆盖和低成本，同时提供稀疏但统计上足够的地面真实数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多样化、帧级连续的深度估计数据集，通过轻量级采集流程实现低成本广泛场景覆盖，提供稀疏但统计上足够的地面真实数据。整体流程包括：1)多车辆采集系统，配备激光雷达、RGB相机、GNSS和IMU；2)数据以ROS 2 bag格式组织，包含同步的多模态数据；3)多传感器精确校准和同步；4)覆盖高速公路、乡村和城市场景，包括正常、夜间和雨天条件。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)更大规模和更高多样性的数据集(193,648帧)；2)轻量级低成本采集流程；3)稀疏但统计上足够的地面真实数据；4)完全可扩展的数据集设计；5)为模型带来新的泛化挑战。相比之前工作，ROVR数据集规模更大(比KITTI大2倍以上)，场景更多样(包含夜间和雨天)，成本效益更高，且深度GT密度更低，对模型泛化能力提出更高要求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ROVR数据集通过大规模、多样化的深度估计基准和轻量级采集流程，为自动驾驶领域的深度估计研究提供了新的挑战和机遇，推动了更具泛化能力的深度估计方法的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depth estimation is a fundamental task for 3D scene understanding inautonomous driving, robotics, and augmented reality. Existing depth datasets,such as KITTI, nuScenes, and DDAD, have advanced the field but suffer fromlimitations in diversity and scalability. As benchmark performance on thesedatasets approaches saturation, there is an increasing need for a newgeneration of large-scale, diverse, and cost-efficient datasets to support theera of foundation models and multi-modal learning. To address these challenges,we introduce a large-scale, diverse, frame-wise continuous dataset for depthestimation in dynamic outdoor driving environments, comprising 20K video framesto evaluate existing methods. Our lightweight acquisition pipeline ensuresbroad scene coverage at low cost, while sparse yet statistically sufficientground truth enables robust training. Compared to existing datasets, ourspresents greater diversity in driving scenarios and lower depth density,creating new challenges for generalization. Benchmark experiments with standardmonocular depth estimation models validate the dataset's utility and highlightsubstantial performance gaps in challenging conditions, establishing a newplatform for advancing depth estimation research.</description>
      <author>example@mail.com (Xianda Guo, Ruijun Zhang, Yiqun Duan, Ruilin Wang, Keyuan Zhou, Wenzhao Zheng, Wenke Huang, Gangwei Xu, Mike Horton, Yuan Si, Hao Zhao, Long Chen)</author>
      <guid isPermaLink="false">2508.13977v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>MMIS-Net for Retinal Fluid Segmentation and Detection</title>
      <link>http://arxiv.org/abs/2508.13936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MMIS-Net的多模态医学图像分割网络，通过利用多种医学数据集的协同潜力，提高了在未见数据上的分割性能。&lt;h4&gt;背景&lt;/h4&gt;深度学习方法在医学图像分割和疾病检测中表现出色，但大多数方法仅使用单一来源、模态、器官或疾病类型的数据进行训练和测试，忽略了其他可用标注数据的组合潜力。&lt;h4&gt;目的&lt;/h4&gt;利用各种模态、器官和疾病的小型标注医学图像数据集的协同潜力，提高在未见数据上的分割性能。&lt;h4&gt;方法&lt;/h4&gt;提出MMIS-Net算法，包含相似性融合块，利用监督和像素级相似性知识选择进行特征图融合；创建独热标签空间处理不同数据集间的类别定义不一致和标签矛盾；在10个包含2种模态的19个器官的数据集上训练构建单一模型。&lt;h4&gt;主要发现&lt;/h4&gt;在RETOUCH重大挑战隐藏测试集上，MMIS-Net性能优于大型基础模型和其他最先进算法；在流体分割任务中达到0.83的最佳平均Dice分数和0.035的绝对体积差异；在流体检测任务中获得完美的1曲线下面积。&lt;h4&gt;结论&lt;/h4&gt;将相似性融合块整合到网络主干中用于监督和相似性知识选择，以及使用独热标签空间处理标签类别不一致和矛盾，显著提高了模型的有效性。&lt;h4&gt;翻译&lt;/h4&gt;目的：深度学习方法在医学图像的分割和疾病检测中显示出良好的结果。然而，大多数方法仅使用单一来源、模态、器官或疾病类型的数据进行训练和测试，忽略了其他可用标注数据的组合潜力。各种模态、器官和疾病的小型标注医学图像数据集公开可用。本研究旨在利用这些数据集的协同潜力，提高在未见数据上的性能。方法：为此，我们提出了一种名为MMIS-Net（多模态医学图像分割网络）的新算法，该算法具有相似性融合块，利用监督和像素级相似性知识选择进行特征图融合。此外，为了处理类别定义不一致和标签矛盾，我们创建了独热标签空间来处理在一个数据集中缺失但在另一个数据集中标注的类别。MMIS-Net在10个包含2种模态的19个器官的数据集上训练，构建了单一模型。结果：在RETOUCH重大挑战隐藏测试集上评估算法，性能优于医学图像分割的大型基础模型和其他最先进算法。我们在流体分割任务中获得了0.83的最佳平均Dice分数和0.035的绝对体积差异，在流体检测任务中获得了完美的1曲线下面积。结论：定量结果突显了所提出模型的有效性，这是由于将相似性融合块整合到网络主干中用于监督和相似性知识选择，以及使用独热标签空间处理标签类别不一致和矛盾。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Deep learning methods have shown promising results in thesegmentation, and detection of diseases in medical images. However, mostmethods are trained and tested on data from a single source, modality, organ,or disease type, overlooking the combined potential of other availableannotated data. Numerous small annotated medical image datasets from variousmodalities, organs, and diseases are publicly available. In this work, we aimto leverage the synergistic potential of these datasets to improve performanceon unseen data. Approach: To this end, we propose a novel algorithm calledMMIS-Net (MultiModal Medical Image Segmentation Network), which featuresSimilarity Fusion blocks that utilize supervision and pixel-wise similarityknowledge selection for feature map fusion. Additionally, to addressinconsistent class definitions and label contradictions, we created a one-hotlabel space to handle classes absent in one dataset but annotated in another.MMIS-Net was trained on 10 datasets encompassing 19 organs across 2 modalitiesto build a single model. Results: The algorithm was evaluated on the RETOUCHgrand challenge hidden test set, outperforming large foundation models formedical image segmentation and other state-of-the-art algorithms. We achievedthe best mean Dice score of 0.83 and an absolute volume difference of 0.035 forthe fluids segmentation task, as well as a perfect Area Under the Curve of 1for the fluid detection task. Conclusion: The quantitative results highlightthe effectiveness of our proposed model due to the incorporation of SimilarityFusion blocks into the network's backbone for supervision and similarityknowledge selection, and the use of a one-hot label space to address labelclass inconsistencies and contradictions.</description>
      <author>example@mail.com (Nchongmaje Ndipenocha, Alina Mirona, Kezhi Wanga, Yongmin Li)</author>
      <guid isPermaLink="false">2508.13936v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>RED.AI Id-Pattern: First Results of Stone Deterioration Patterns with Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2508.13872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure, 1 table. Contribution for REEACH 2025 Symposium&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Id-Pattern系统是RED.AI项目中的一个多代理人工智能系统，用于协助识别石材劣化模式，通过模拟专家协作来提高诊断效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;传统石材劣化模式识别方法依赖专家团队的直接观察，虽然准确但时间和成本较高。&lt;h4&gt;目的&lt;/h4&gt;引入并评估一个多代理人工智能系统，模拟专家之间的协作，并从视觉证据自动诊断石材病变。&lt;h4&gt;方法&lt;/h4&gt;基于认知架构协调五个专门的AI代理：岩石学家、病理学家、环境专家、修复师和诊断协调员，使用28张涉及多种劣化模式的困难图像进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;与基础模型相比，系统在所有指标上都有显著提升。&lt;h4&gt;结论&lt;/h4&gt;多代理AI系统能够有效提高石材病变诊断的效率和准确性，减少对专家团队的依赖。&lt;h4&gt;翻译&lt;/h4&gt;RED.AI项目中的Id-Pattern系统是一个代理系统，旨在协助识别石材劣化模式。基于专家团队直接观察的传统方法虽然准确但在时间和资源方面成本高昂。这里开发的系统引入并评估了一个多代理人工智能系统，旨在模拟专家之间的协作并从视觉证据自动诊断石材病变。该方法基于一个认知架构，协调一组专门的AI代理，在这个特定案例中限制为五个：岩石学家、病理学家、环境专家、修复师和诊断协调员。为了评估系统，我们选择了28张涉及多种劣化模式的困难图像。我们的初步结果显示，与基础模型相比，我们系统在所有指标上都有巨大提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Id-Pattern system within the RED.AI project (Reabilita\c{c}\~aoEstrutural Digital atrav\'es da AI) consists of an agentic system designed toassist in the identification of stone deterioration patterns. Traditionalmethodologies, based on direct observation by expert teams, are accurate butcostly in terms of time and resources. The system developed here introduces andevaluates a multi-agent artificial intelligence (AI) system, designed tosimulate collaboration between experts and automate the diagnosis of stonepathologies from visual evidence. The approach is based on a cognitivearchitecture that orchestrates a team of specialized AI agents which, in thisspecific case, are limited to five: a lithologist, a pathologist, anenvironmental expert, a conservator-restorer, and a diagnostic coordinator. Toevaluate the system we selected 28 difficult images involving multipledeterioration patterns. Our first results showed a huge boost on all metrics ofour system compared to the foundational model.</description>
      <author>example@mail.com (Daniele Corradetti, José Delgado Rodrigues)</author>
      <guid isPermaLink="false">2508.13872v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Large-scale cooperative sulfur vacancy dynamics in two-dimensional MoS2 from machine learning interatomic potentials</title>
      <link>http://arxiv.org/abs/2508.13790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过机器学习原子间势的分子动力学模拟，揭示了MoS2单层中硫空位形成机制及其对催化活性和忆阻行为的影响。&lt;h4&gt;背景&lt;/h4&gt;MoS2单层中扩展硫空位的形成与催化活性密切相关，并且可能是其忆阻行为的基础。理解这些空位的形成机制对于材料性能优化具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;旨在通过分子动力学模拟揭示MoS2单层中空位协同输运的关键机制，特别是空位并入团簇的过程，并解释实验中观察到的辐照诱导空位模式。&lt;h4&gt;方法&lt;/h4&gt;采用纳秒级分子动力学模拟，使用机器学习原子间势(MLIPs)框架，包括两种方法：(i)使用高斯近似势的在线学习，和(ii)等变基础模型的微调。&lt;h4&gt;主要发现&lt;/h4&gt;模拟揭示了空位协同输运的关键机制，包括空位并入任意大小的团簇；为实验观察到的辐照诱导空位模式提供了连贯的原子解释，特别是跨越数十纳米的线缺陷的形成。&lt;h4&gt;结论&lt;/h4&gt;扩展硫空位在MoS2单层中的形成机制对于理解其催化活性和忆阻行为至关重要，机器学习原子间势框架能够有效模拟这些复杂过程。&lt;h4&gt;翻译&lt;/h4&gt;MoS2单层中扩展硫空位的形成与催化活性密切相关，并且可能是其忆阻行为的基础。使用机器学习原子间势(MLIPs)进行的纳秒级分子动力学模拟揭示了空位协同输运的关键机制，包括空位并入任意大小的团簇。这些模拟为实验观察到的辐照诱导空位模式提供了连贯的原子解释，特别是跨越数十纳米的线缺陷的形成。比较了两种MLIP框架的结果和性能：(i)使用高斯近似势的在线学习，和(ii)等变基础模型的微调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The formation of extended sulfur vacancies in MoS2 monolayers is closelyassociated with catalytic activity and may also be the basis for its memristivebehavior. Nanosecond-scale molecular dynamics simulations using machinelearning interatomic potentials (MLIPs) reveal key mechanisms of cooperativevacancy transport, including incorporation of vacancies into clusters ofarbitrary size. The simulations provide a coherent atomistic explanation forirradiation-induced vacancy patterns observed experimentally, especially theformation of line defects spanning tens of nanometers. Results and performanceare compared of two MLIP frameworks: (i) on-the-fly learning with Gaussianapproximation potential, and (ii) fine-tuning of an equivariant foundationmodel.</description>
      <author>example@mail.com (Aaron Flötotto, Benjamin Spetzler, Rose von Stackelberg, Martin Ziegler, Erich Runge, Christian Dreßler)</author>
      <guid isPermaLink="false">2508.13790v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>subCellSAM: Zero-Shot (Sub-)Cellular Segmentation for Hit Validation in Drug Discovery</title>
      <link>http://arxiv.org/abs/2508.13701v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at DAGM German Conference on Pattern Recognition (GCPR) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于分割基础模型的新方法，通过上下文学习策略在零样本设置下实现细胞分割，无需数据集特定的调整即可准确分割生物学相关结构。&lt;h4&gt;背景&lt;/h4&gt;高通量筛选使用自动化显微镜是生物制药药物发现的关键驱动力，能够同时评估数千种药物候选物。传统的图像分析和深度学习方法已被用于分析这些复杂的大规模数据集，其中细胞分割是提取相关结构的关键步骤。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需大量手动参数调整或领域特定模型微调的细胞分割方法。&lt;h4&gt;方法&lt;/h4&gt;采用三步过程进行细胞核、细胞和亚细胞分割，引入自提示机制，使用生长掩码和战略性放置的前景/背景点来编码形态和拓扑先验，在零样本设置下应用分割基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在标准细胞分割基准测试和行业相关的命中验证测定上表现良好，能够准确分割生物学相关结构而无需针对特定数据集进行调整。&lt;h4&gt;结论&lt;/h4&gt;基于分割基础模型和上下文学习策略的方法解决了传统细胞分割方法需要大量手动调整的问题，提供了一种更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用自动化显微镜的高通量筛选是生物制药药物发现的关键驱动力，使能够并行评估数千种用于癌症等疾病的药物候选物。传统的图像分析和深度学习方法已被用于分析这些复杂的大规模数据集，其中细胞分割是提取相关结构的关键步骤。然而，这两种策略通常都需要大量的手动参数调整或领域特定的模型微调。我们提出了一种新方法，在零样本设置（即不进行微调）下应用分割基础模型，由上下文学习策略引导。我们的方法采用三步过程进行细胞核、细胞和亚细胞分割，引入了一种自提示机制，使用生长掩码和战略性放置的前景/背景点来编码形态和拓扑先验。我们在标准的细胞分割基准测试和行业相关的命中验证测定上验证了我们的方法，证明它能够准确分割生物学相关结构而无需针对特定数据集进行调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-throughput screening using automated microscopes is a key driver inbiopharma drug discovery, enabling the parallel evaluation of thousands of drugcandidates for diseases such as cancer. Traditional image analysis and deeplearning approaches have been employed to analyze these complex, large-scaledatasets, with cell segmentation serving as a critical step for extractingrelevant structures. However, both strategies typically require extensivemanual parameter tuning or domain-specific model fine-tuning. We present anovel method that applies a segmentation foundation model in a zero-shotsetting (i.e., without fine-tuning), guided by an in-context learning strategy.Our approach employs a three-step process for nuclei, cell, and subcellularsegmentation, introducing a self-prompting mechanism that encodes morphologicaland topological priors using growing masks and strategically placedforeground/background points. We validate our method on both standard cellsegmentation benchmarks and industry-relevant hit validation assays,demonstrating that it accurately segments biologically relevant structureswithout the need for dataset-specific tuning.</description>
      <author>example@mail.com (Jacob Hanimann, Daniel Siegismund, Mario Wieser, Stephan Steigele)</author>
      <guid isPermaLink="false">2508.13701v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter</title>
      <link>http://arxiv.org/abs/2508.13530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CrafterDojo，一套基础模型和工具，使Crafter环境成为一个轻量级、原型友好且类似Minecraft的通用智能体研究测试平台。&lt;h4&gt;背景&lt;/h4&gt;开发通用智能体是AI的核心挑战。Minecraft虽然复杂且有大规模数据，但速度慢且工程开销大，不适合快速原型设计。Crafter是一个轻量级替代方案，保留了Minecraft的关键挑战，但由于缺乏基础模型，其使用仅限于狭窄任务。&lt;h4&gt;目的&lt;/h4&gt;开发一套基础模型和工具，将Crafter环境转变为一个轻量级、原型友好且类似Minecraft的通用智能体研究测试平台。&lt;h4&gt;方法&lt;/h4&gt;通过引入CrafterVPT（行为先验）、CrafterCLIP（视觉语言基础）和CrafterSteve-1（指令跟随）来解决Crafter环境的应用限制。此外，还提供了用于生成行为和标题数据集的工具包（CrafterPlay和CrafterCaption）、参考智能体实现、基准评估和完整的开源代码库。&lt;h4&gt;主要发现&lt;/h4&gt;CrafterDojo成功将Crafter环境转变为一个适合通用智能体研究的轻量级测试平台，通过多种基础模型和工具支持了智能体的行为先验、视觉语言基础和指令跟随能力。&lt;h4&gt;结论&lt;/h4&gt;CrafterDojo为通用智能体研究提供了一个轻量级、原型友好且类似Minecraft的测试环境，解决了Minecraft在快速原型设计方面的局限性，同时保留了其关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;开发通用智能体是AI的核心挑战。Minecraft提供了丰富的复杂性和互联网规模的数据，但其缓慢的速度和工程开销使其不适合快速原型设计。Crafter提供了一个轻量级替代方案，保留了来自Minecraft的关键挑战，但由于缺乏在Minecraft环境中推动进展的基础模型，其使用仅限于狭窄任务。在本文中，我们提出了CrafterDojo，一套基础模型和工具，使Crafter环境成为一个轻量级、原型友好且类似Minecraft的通用智能体研究测试平台。CrafterDojo通过引入CrafterVPT、CrafterCLIP和CrafterSteve-1分别用于行为先验、视觉语言基础和指令跟随来解决这一问题。此外，我们还提供了用于生成行为和标题数据集的工具包（CrafterPlay和CrafterCaption）、参考智能体实现、基准评估和完整的开源代码库。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing general-purpose embodied agents is a core challenge in AI.Minecraft provides rich complexity and internet-scale data, but its slow speedand engineering overhead make it unsuitable for rapid prototyping. Crafteroffers a lightweight alternative that retains key challenges from Minecraft,yet its use has remained limited to narrow tasks due to the absence offoundation models that have driven progress in the Minecraft setting. In thispaper, we present CrafterDojo, a suite of foundation models and tools thatunlock the Crafter environment as a lightweight, prototyping-friendly, andMinecraft-like testbed for general-purpose embodied agent research. CrafterDojoaddresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 forbehavior priors, vision-language grounding, and instruction following,respectively. In addition, we provide toolkits for generating behavior andcaption datasets (CrafterPlay and CrafterCaption), reference agentimplementations, benchmark evaluations, and a complete open-source codebase.</description>
      <author>example@mail.com (Junyeong Park, Hyeonseo Cho, Sungjin Ahn)</author>
      <guid isPermaLink="false">2508.13530v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation</title>
      <link>http://arxiv.org/abs/2508.13525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, 2 tables. Code:  https://github.com/HasanBGIt/Saudi-Dialect-ALLaM . Dataset and trained  weights/adapters are not released. Primary category: cs.CL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对阿拉伯语大语言模型对沙特方言支持不足的问题，通过LoRA微调沙特基础模型，实现了更准确的沙特方言生成，并提出了两种训练方法：方言标记训练和无标记训练。&lt;h4&gt;背景&lt;/h4&gt;阿拉伯语大语言模型主要由现代标准阿拉伯语(MSA)主导，对沙特方言(如Najdi和Hijazi)的支持有限，这种代表性不足阻碍了模型捕捉真实方言变异的能力。&lt;h4&gt;目的&lt;/h4&gt;提高大语言模型对沙特方言(特别是Hijazi和Najdi)的生成能力，增强模型对方言变异的捕捉，同时减少现代标准阿拉伯语的泄漏。&lt;h4&gt;方法&lt;/h4&gt;使用私有的沙特方言指令数据集(5,466个合成指令-响应对，Hijazi和Najdi各占50%)，对沙特阿拉伯开发的首个基础模型ALLaM-7B-Instruct-preview进行LoRA微调。研究了两种训练变体：方言标记训练(在指令前添加方言标记)和无标记训练(省略标记)。评估结合了外部方言分类器、文本保真度指标(chrF++和BERTScore)和多样性度量。&lt;h4&gt;主要发现&lt;/h4&gt;方言标记模型实现了最佳控制，将沙特语比例从47.97%提高到84.21%，并将MSA泄漏从32.63%降低到6.21%；保真度也有所提高(chrF++ +3.53，BERTScore +0.059)。两种LoRA变体在方言控制和保真度方面都优于多种通用指令模型，同时避免了这些基线模型经常出现的元数据标记回声问题。&lt;h4&gt;结论&lt;/h4&gt;通过LoRA微调可以显著提高大语言模型对沙特方言的生成能力。研究团队选择不发布数据集或模型权重/适配器，而是发布训练/评估/推理代码和详细数据表，以支持独立验证和未来研究。&lt;h4&gt;翻译&lt;/h4&gt;针对阿拉伯语的大语言模型仍然由现代标准阿拉伯语(MSA)主导，对沙特方言(如Najdi和Hijazi)的支持有限。这种代表性不足阻碍了它们捕捉真实方言变异的能力。使用一个私有的沙特方言指令数据集(包含Hijazi和Najdi；5,466个合成指令-响应对；50/50分割)，我们LoRA微调了ALLaM-7B-Instruct-preview，这是在沙特阿拉伯开发的首个基础模型，用于沙特方言生成。我们研究了两种变体：(i)方言标记训练，即在指令前添加明确的方言标记，和(ii)无标记训练，即在格式化时省略标记。在保留测试集上的评估结合了外部方言分类器与文本保真度指标(chrF++和BERTScore)和多样性度量。方言标记模型实现了最佳控制，将沙特语比例从47.97%提高到84.21%，并将MSA泄漏从32.63%降低到6.21%；保真度也有所提高(chrF++ +3.53，BERTScore +0.059)。两种LoRA变体在方言控制和保真度方面都优于强大的通用指令模型(Falcon-7B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat)，同时避免了这些基线模型经常出现的元数据标记回声问题。我们没有发布数据集或任何模型权重/适配器；相反，我们发布了训练/评估/推理代码和详细的数据表(模式和聚合统计)，以支持独立验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) for Arabic are still dominated by ModernStandard Arabic (MSA), with limited support for Saudi dialects such as Najdiand Hijazi. This underrepresentation hinders their ability to capture authenticdialectal variation. Using a privately curated Saudi Dialect Instructiondataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation modeldeveloped in Saudi Arabia, for Saudi dialect generation. We investigate twovariants: (i) Dialect-Token training, which prepends an explicit dialect tag tothe instruction, and (ii) No-Token training, which omits the tag at formattingtime. Evaluation on a held-out test set combines an external dialect classifierwith text fidelity metrics (chrF++ and BERTScore) and diversity measures. TheDialect-Token model achieves the best control, raising the Saudi rate from47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity alsoimproves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform stronggeneric instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control andfidelity, while avoiding metadata-tag echoing that these baselines frequentlyexhibit. We do not release the dataset or any model weights/adapters; instead,we release training/evaluation/inference code and a detailed datasheet (schemaand aggregate statistics) to support independent verification.</description>
      <author>example@mail.com (Hassan Barmandah)</author>
      <guid isPermaLink="false">2508.13525v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency</title>
      <link>http://arxiv.org/abs/2508.13518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, CVPR Oral&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种几何知识引导的分布校准框架，利用基础模型特征分布的几何形状的跨领域迁移性，解决联邦学习和长尾识别中的数据异质性和样本不平衡问题。&lt;h4&gt;背景&lt;/h4&gt;尽管深度学习发展迅速，但仍面临观测训练样本与潜在真实分布之间的差距挑战，这种差距可能由采样偏差、噪声等因素造成。&lt;h4&gt;目的&lt;/h4&gt;验证在基础模型时代，利用现成的视觉基础模型进行特征提取时，特征分布几何形状的跨领域迁移性在联邦学习和长尾识别等场景中的实用性。&lt;h4&gt;方法&lt;/h4&gt;提出几何知识引导的分布校准框架，在联邦学习中设计隐私约束下获取全局几何形状的技术并生成新样本；在长尾学习中利用从样本丰富类别迁移的几何知识恢复尾部类别的真实分布。&lt;h4&gt;主要发现&lt;/h4&gt;特征分布的几何形状在不同领域和数据集上展现出显著的迁移性，几何知识引导的分布校准能有效克服数据异质性和样本不平衡导致的信息不足。&lt;h4&gt;结论&lt;/h4&gt;所提出的几何知识引导的分布校准框架能够有效弥合观测训练样本与潜在真实分布之间的差距，提升模型在联邦学习和长尾识别等场景下的性能。&lt;h4&gt;翻译&lt;/h4&gt;尽管深度学习取得了快速进展，但一个持续的挑战是观测到的训练样本与潜在真实分布之间的差距。造成这种差距有多种原因，例如采样偏差、噪声等。在基础模型时代，我们表明，当利用现成的视觉基础模型（如CLIP、DINOv2）进行特征提取时，所得特征分布的几何形状在不同领域和数据集上展现出显著的迁移性。为了验证其实用性，我们在两个流行且具有挑战性的场景中实现了我们的几何知识引导的分布校准框架：联邦学习和长尾识别。在联邦学习场景中，我们设计了一种在隐私约束下获取全局几何形状的技术，然后利用这些知识为客户端生成新样本，旨在弥合本地和全局观测之间的差距。在长尾学习中，它利用从样本丰富的类别迁移的几何知识来恢复样本稀少的尾部类别的真实分布。全面的实验表明，我们提出的几何知识引导的分布校准有效地克服了由数据异质性和样本不平衡导致的信息不足，在多个基准测试中提升了性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the fast progress of deep learning, one standing challenge is the gapof the observed training samples and the underlying true distribution. Thereare multiple reasons for the causing of this gap e.g. sampling bias, noise etc.In the era of foundation models, we show that when leveraging the off-the-shelf(vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, thegeometric shapes of the resulting feature distributions exhibit remarkabletransferability across domains and datasets. To verify its practicalusefulness, we embody our geometric knowledge-guided distribution calibrationframework in two popular and challenging settings: federated learning andlong-tailed recognition. In the federated setting, we devise a technique ofacquiring the global geometric shape under privacy constraints, then leveragethis knowledge to generate new samples for clients, in the aim of bridging thegap between local and global observations. In long-tailed learning, it utilizesthe geometric knowledge transferred from sample-rich categories to recover thetrue distribution for sample-scarce tail classes. Comprehensive experimentsshow that our proposed geometric knowledge-guided distribution calibrationeffectively overcomes information deficits caused by data heterogeneity andsample imbalance, with boosted performance across benchmarks.</description>
      <author>example@mail.com (Yanbiao Ma, Wei Dai, Bowei Liu, Jiayi Chen, Wenke Huang, Guancheng Wan, Zhiwu Lu, Junchi Yan)</author>
      <guid isPermaLink="false">2508.13518v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>NovoMolGen: Rethinking Molecular Language Model Pretraining</title>
      <link>http://arxiv.org/abs/2508.13408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为NovoMolGen的transformer基础模型家族，用于从头分子生成，在15亿个分子上进行了预训练，在分子生成任务中取得了新的最先进结果。&lt;h4&gt;背景&lt;/h4&gt;设计具有理想特性的新分子需要探索庞大的化学空间（10^23到10^60个可能的可合成候选分子），尽管已有多种深度生成模型，但基于字符串表示的分子大语言模型（Mol-LLMs）作为可扩展方法能够探索数十亿分子。&lt;h4&gt;目的&lt;/h4&gt;系统研究标准语言建模实践（如文本表示、标记化策略、模型大小和数据集规模）对分子生成性能的影响，并开发一种高效的基础模型用于分子生成。&lt;h4&gt;方法&lt;/h4&gt;引入NovoMolGen，一类基于transformer的预训练基础模型，在15亿个分子上进行预训练，用于从头分子生成，并通过广泛的实证分析评估其性能。&lt;h4&gt;主要发现&lt;/h4&gt;预训练期间测量的性能指标与实际下游性能之间存在弱相关性，揭示了分子训练与一般自然语言处理训练动力学之间的区别；NovoMolGen在无约束和目标导向的分子生成任务中都显著优于之前的Mol-LLMs和专门的生成模型。&lt;h4&gt;结论&lt;/h4&gt;NovoMolGen为推进高效有效的分子建模策略提供了坚实基础，代表了分子生成领域的重大进展。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了一种名为NovoMolGen的transformer基础模型家族，用于从头分子生成，在15亿个分子上进行了预训练，在分子生成任务中取得了新的最先进结果。设计具有理想特性的新分子需要探索庞大的化学空间（10^23到10^60个可能的可合成候选分子），尽管已有多种深度生成模型，但基于字符串表示的分子大语言模型（Mol-LLMs）作为可扩展方法能够探索数十亿分子。本研究旨在系统研究标准语言建模实践（如文本表示、标记化策略、模型大小和数据集规模）对分子生成性能的影响，并开发一种高效的基础模型用于分子生成。研究人员引入NovoMolGen，一类基于transformer的预训练基础模型，在15亿个分子上进行预训练，用于从头分子生成，并通过广泛的实证分析评估其性能。研究发现，预训练期间测量的性能指标与实际下游性能之间存在弱相关性，揭示了分子训练与一般自然语言处理训练动力学之间的区别；NovoMolGen在无约束和目标导向的分子生成任务中都显著优于之前的Mol-LLMs和专门的生成模型。结论是，NovoMolGen为推进高效有效的分子建模策略提供了坚实基础，代表了分子生成领域的重大进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing de-novo molecules with desired property profiles requires efficientexploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$possible synthesizable candidates. While various deep generative models havebeen developed to design small molecules using diverse input representations,Molecular Large Language Models (Mol-LLMs) based on string representations haveemerged as a scalable approach capable of exploring billions of molecules.However, there remains limited understanding regarding how standard languagemodeling practices such as textual representations, tokenization strategies,model size, and dataset scale impact molecular generation performance. In thiswork, we systematically investigate these critical aspects by introducingNovoMolGen, a family of transformer-based foundation models pretrained on 1.5billion molecules for de-novo molecule generation. Through extensive empiricalanalyses, we identify a weak correlation between performance metrics measuredduring pretraining and actual downstream performance, revealing importantdistinctions between molecular and general NLP training dynamics. NovoMolGenestablishes new state-of-the-art results, substantially outperforming priorMol-LLMs and specialized generative models in both unconstrained andgoal-directed molecular generation tasks, thus providing a robust foundationfor advancing efficient and effective molecular modeling strategies.</description>
      <author>example@mail.com (Kamran Chitsaz, Roshan Balaji, Quentin Fournier, Nirav Pravinbhai Bhatt, Sarath Chandar)</author>
      <guid isPermaLink="false">2508.13408v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Driven High-Dimensional Variable Selection</title>
      <link>http://arxiv.org/abs/2508.13890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于扩散模型的重采样-聚合框架，用于高维高度相关数据的变量选择，通过生成合成数据并应用多种选择器，实现了稳定且可靠的预测变量子集选择。&lt;h4&gt;背景&lt;/h4&gt;高维、高度相关数据的变量选择一直是一个具有挑战性的问题，传统方法往往产生不稳定和不可靠的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理高维高度相关数据并产生稳定结果的变量选择方法。&lt;h4&gt;方法&lt;/h4&gt;从拟合原始数据的扩散模型生成多个伪数据集，应用现成的选择器（如lasso或SCAD），存储包含指标和系数，然后通过跨副本聚合产生具有校准稳定性分数的预测变量子集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在理论上是选择一致的；由于迁移学习，在小样本或噪声数据中表现更好；扩展到图形模型选择和统计推断；实验显示与基线方法相比有更高真正例率和更低假发现比例。&lt;h4&gt;结论&lt;/h4&gt;基于扩散的数据增强与原则性聚合相结合的方法推进了变量选择方法论，为复杂科学应用提供了可解释且统计严谨的分析工具。&lt;h4&gt;翻译&lt;/h4&gt;高维高度相关数据的变量选择一直是一个具有挑战性的问题，通常会产生不稳定且不可靠的模型。我们提出了一种重采样-聚合框架，利用扩散模型生成高保真合成数据的能力。具体而言，我们从拟合原始数据的扩散模型中抽取多个伪数据集，应用任何现成的选择器（如lasso或SCAD），并存储所得的包含指标和系数。跨副本聚合产生具有校准稳定性分数的预测变量子集用于变量选择。理论上，我们在温和假设下证明了所提出方法的选择一致性。由于生成模型从大型预训练权重中导入知识，该程序自然受益于迁移学习，在观测样本小或嘈杂时提高功效。我们还扩展了合成数据聚合框架到其他模型选择问题，包括图形模型选择和支持有效置信区间和假设检验的统计推断。广泛的模拟显示与lasso、稳定性选择和knockoff基线相比有一致的改进，特别是在预测变量强相关的情况下，实现了更高的真正例率和更低的假发现比例。通过将基于扩散的数据增强与原则性聚合相结合，我们的方法推进了变量选择方法论，并为复杂科学应用中可解释的、统计严谨的分析扩展了工具包。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Variable selection for high-dimensional, highly correlated data has long beena challenging problem, often yielding unstable and unreliable models. Wepropose a resample-aggregate framework that exploits diffusion models' abilityto generate high-fidelity synthetic data. Specifically, we draw multiplepseudo-data sets from a diffusion model fitted to the original data, apply anyoff-the-shelf selector (e.g., lasso or SCAD), and store the resulting inclusionindicators and coefficients. Aggregating across replicas produces a stablesubset of predictors with calibrated stability scores for variable selection.Theoretically, we show that the proposed method is selection consistent undermild assumptions. Because the generative model imports knowledge from largepre-trained weights, the procedure naturally benefits from transfer learning,boosting power when the observed sample is small or noisy. We also extend theframework of aggregating synthetic data to other model selection problems,including graphical model selection, and statistical inference that supportsvalid confidence intervals and hypothesis tests. Extensive simulations showconsistent gains over the lasso, stability selection, and knockoff baselines,especially when predictors are strongly correlated, achieving highertrue-positive rates and lower false-discovery proportions. By couplingdiffusion-based data augmentation with principled aggregation, our methodadvances variable selection methodology and broadens the toolkit forinterpretable, statistically rigorous analysis in complex scientificapplications.</description>
      <author>example@mail.com (Minjie Wang, Xiaotong Shen, Wei Pan)</author>
      <guid isPermaLink="false">2508.13890v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling</title>
      <link>http://arxiv.org/abs/2508.13833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了建筑信息模型(BIM)与自然语言处理(NLP)的集成，用于从非结构化的法国建筑技术规范(BTS)文档中自动提取需求。&lt;h4&gt;背景&lt;/h4&gt;在建筑行业中，需要从非结构化的法国建筑技术规范文档中提取需求信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化系统，能够从非结构化的法国建筑技术规范文档中提取需求信息。&lt;h4&gt;方法&lt;/h4&gt;使用命名实体识别(NER)和关系提取(RE)技术，利用基于transformer的模型CamemBERT和应用迁移学习，使用法语模型Fr_core_news_lg。开发了从基于规则到基于深度学习的多种方法进行基准测试。对于RE，实现了包括随机森林在内的四种监督模型，使用自定义特征向量。使用手工制作的标注数据集比较不同方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;CamemBERT和Fr_core_news_lg在NER中表现出卓越的性能，F1分数超过90%；随机森林在RE中最有效，F1分数超过80%。&lt;h4&gt;结论&lt;/h4&gt;研究结果将在未来工作中以知识图谱的形式呈现，以进一步增强自动验证系统。&lt;h4&gt;翻译&lt;/h4&gt;本研究探索了建筑信息模型(BIM)与自然语言处理(NLP)的集成，用于在建筑行业内从非结构化的法国建筑技术规范(BTS)文档中自动提取需求。采用命名实体识别(NER)和关系提取(RE)技术，研究利用了基于transformer的模型CamemBERT，并应用了迁移学习，使用在大型法语语料库上预训练的法语模型Fr_core_news_lg。为对这些模型进行基准测试，开发了从基于规则到基于深度学习的多种方法。对于RE，实现了包括随机森林在内的四种监督模型，使用自定义特征向量。使用手工制作的标注数据集来比较NER方法和RE模型的有效性。结果表明，CamemBERT和Fr_core_news_lg在NER中表现出卓越的性能，F1分数超过90%，而随机森林在RE中最有效，F1分数超过80%。结果将在未来工作中以知识图谱的形式呈现，以进一步增强自动验证系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study explores the integration of Building Information Modeling (BIM)with Natural Language Processing (NLP) to automate the extraction ofrequirements from unstructured French Building Technical Specification (BTS)documents within the construction industry. Employing Named Entity Recognition(NER) and Relation Extraction (RE) techniques, the study leverages thetransformer-based model CamemBERT and applies transfer learning with the Frenchlanguage model Fr\_core\_news\_lg, both pre-trained on a large French corpus inthe general domain. To benchmark these models, additional approaches rangingfrom rule-based to deep learning-based methods are developed. For RE, fourdifferent supervised models, including Random Forest, are implemented using acustom feature vector. A hand-crafted annotated dataset is used to compare theeffectiveness of NER approaches and RE models. Results indicate that CamemBERTand Fr\_core\_news\_lg exhibited superior performance in NER, achievingF1-scores over 90\%, while Random Forest proved most effective in RE, with anF1 score above 80\%. The outcomes are intended to be represented as a knowledgegraph in future work to further enhance automatic verification systems.</description>
      <author>example@mail.com (Insaf Nahri, Romain Pinquié, Philippe Véron, Nicolas Bus, Mathieu Thorel)</author>
      <guid isPermaLink="false">2508.13833v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings</title>
      <link>http://arxiv.org/abs/2508.13672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 34th ACM International Conference on Information and  Knowledge Management (CIKM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的基于实例迁移学习的LIME框架(ITL-LIME)，用于解决数据受限环境下LIME方法的局部性和稳定性问题。&lt;h4&gt;背景&lt;/h4&gt;可解释人工智能(XAI)方法如LIME通过使用可解释的替代模型来近似黑盒机器学习模型的行为，提高了模型的可解释性。但LIME在扰动和采样中的固有随机性可能导致局部性和稳定性问题，特别是在训练数据有限的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于实例迁移学习的LIME框架(ITL-LIME)，以增强数据受限环境下的解释保真度和稳定性。&lt;h4&gt;方法&lt;/h4&gt;ITL-LIME通过将实例迁移学习引入LIME框架，利用来自相关源域的相关真实实例来辅助目标域的解释过程。具体来说，使用聚类将源域划分为具有代表性原型的簇，从与目标实例最相似的原型对应的源簇中检索相关的真实源实例，与目标实例的相邻真实实例组合，并构建基于对比学习的编码器作为加权机制，根据实例与目标实例的接近度为组合集中的实例分配权重，最后使用这些加权的源实例和目标实例来训练用于解释的替代模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用源域的相关实例和基于对比学习的加权机制，ITL-LIME能够提高数据受限环境下的解释质量和稳定性。&lt;h4&gt;结论&lt;/h4&gt;ITL-LIME框架解决了LIME在数据稀缺环境下的局部性和稳定性问题，通过迁移学习的方法提高了解释的保真度。&lt;h4&gt;翻译&lt;/h4&gt;可解释人工智能(XAI)方法，如局部可解释模型无关解释(LIME)，通过使用可解释的替代模型近似黑盒机器学习模型的行为，提高了其可解释性。然而，LIME在扰动和采样中的固有随机性可能导致局部性和稳定性问题，特别是在训练数据有限的情况下。在这种情况下，数据稀缺可能导致生成不真实的变异和偏离真实数据流形的样本，从而使替代模型无法准确近似原始模型的复杂决策边界。为了解决这些挑战，我们提出了一种新的基于实例迁移学习的LIME框架(ITL-LIME)，该框架在数据受限环境中增强了解释保真度和稳定性。ITL-LIME通过利用来自相关源域的相关真实实例来辅助目标域的解释过程，将实例迁移学习引入LIME框架。具体来说，我们使用聚类将源域划分为具有代表性原型的簇。我们的方法不是生成随机扰动，而是从与目标实例最相似的原型对应的源簇中检索相关的真实源实例，然后将它们与目标实例的相邻真实实例组合。为了定义紧凑的局部性，我们进一步构建了一个基于对比学习的编码器作为加权机制，根据实例与目标实例的接近度为组合集中的实例分配权重。最后，使用这些加权的源实例和目标实例来训练用于解释的替代模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Explainable Artificial Intelligence (XAI) methods, such as LocalInterpretable Model-Agnostic Explanations (LIME), have advanced theinterpretability of black-box machine learning models by approximating theirbehavior locally using interpretable surrogate models. However, LIME's inherentrandomness in perturbation and sampling can lead to locality and instabilityissues, especially in scenarios with limited training data. In such cases, datascarcity can result in the generation of unrealistic variations and samplesthat deviate from the true data manifold. Consequently, the surrogate model mayfail to accurately approximate the complex decision boundary of the originalmodel. To address these challenges, we propose a novel Instance-based TransferLearning LIME framework (ITL-LIME) that enhances explanation fidelity andstability in data-constrained environments. ITL-LIME introduces instancetransfer learning into the LIME framework by leveraging relevant real instancesfrom a related source domain to aid the explanation process in the targetdomain. Specifically, we employ clustering to partition the source domain intoclusters with representative prototypes. Instead of generating randomperturbations, our method retrieves pertinent real source instances from thesource cluster whose prototype is most similar to the target instance. Theseare then combined with the target instance's neighboring real instances. Todefine a compact locality, we further construct a contrastive learning-basedencoder as a weighting mechanism to assign weights to the instances from thecombined set based on their proximity to the target instance. Finally, theseweighted source and target instances are used to train the surrogate model forexplanation purposes.</description>
      <author>example@mail.com (Rehan Raza, Guanjin Wang, Kevin Wong, Hamid Laga, Marco Fisichella)</author>
      <guid isPermaLink="false">2508.13672v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Is Transfer Learning Necessary for Violin Transcription?</title>
      <link>http://arxiv.org/abs/2508.13516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了小提琴自动音乐转录中从头训练与微调预训练钢琴模型的性能比较，发现从头训练在特定条件下可以达到或超过微调模型的性能。&lt;h4&gt;背景&lt;/h4&gt;自动音乐转录在钢琴等乐器上已取得显著进展，主要得益于大规模高质量数据集的可用性。相比之下，小提琴AMT研究不足，主要受限于有限的标注数据。&lt;h4&gt;目的&lt;/h4&gt;研究从中等规模小提琴数据集从头开始训练是否能够匹配微调预训练钢琴模型的性能，探究在音色和演奏技巧差异存在的情况下迁移学习的有效性。&lt;h4&gt;方法&lt;/h4&gt;采用未经修改的钢琴转录架构，在包含约30小时对齐小提琴录音的MOSA数据集上进行训练，并在URMP和Bach10数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;从头开始训练的模型与微调的对应模型相比，实现了具有竞争力甚至更好的性能，表明不依赖预训练的钢琴表示也能实现强大的小提琴AMT。&lt;h4&gt;结论&lt;/h4&gt;研究强调了乐器特定数据收集和增强策略对于小提琴AMT的重要性，表明针对特定乐器收集足够的数据可能比依赖其他乐器的预训练模型更有效。&lt;h4&gt;翻译&lt;/h4&gt;自动音乐转录在钢琴等乐器上取得了显著进展，这主要得益于大规模高质量数据集的可用性。相比之下，由于标注数据有限，小提琴AMT仍然研究不足。一种常见的方法是为其他下游任务微调预训练模型，但在音色和演奏技巧差异存在的情况下，这种迁移学习的有效性尚不明确。在本工作中，我们研究了从中等规模小提琴数据集从头开始训练是否能够匹配微调预训练钢琴模型的性能。我们采用未经修改的钢琴转录架构，在包含约30小时对齐小提琴录音的MOSA数据集上进行训练。我们在URMP和Bach10上的实验表明，从头开始训练的模型与微调对应模型相比实现了具有竞争力甚至更好的性能。这些发现表明，不依赖预训练的钢琴表示也能实现强大的小提琴AMT，突显了乐器特定数据收集和增强策略的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic music transcription (AMT) has achieved remarkable progress forinstruments such as the piano, largely due to the availability of large-scale,high-quality datasets. In contrast, violin AMT remains underexplored due tolimited annotated data. A common approach is to fine-tune pretrained models forother downstream tasks, but the effectiveness of such transfer remains unclearin the presence of timbral and articulatory differences. In this work, weinvestigate whether training from scratch on a medium-scale violin dataset canmatch the performance of fine-tuned piano-pretrained models. We adopt a pianotranscription architecture without modification and train it on the MOSAdataset, which contains about 30 hours of aligned violin recordings. Ourexperiments on URMP and Bach10 show that models trained from scratch achievedcompetitive or even superior performance compared to fine-tuned counterparts.These findings suggest that strong violin AMT is possible without relying onpretrained piano representations, highlighting the importance ofinstrument-specific data collection and augmentation strategies.</description>
      <author>example@mail.com (Yueh-Po Peng, Ting-Kang Wang, Li Su, Vincent K. M. Cheung)</author>
      <guid isPermaLink="false">2508.13516v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Control</title>
      <link>http://arxiv.org/abs/2508.12738v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, accepted for CDC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种层次化贝叶斯优化框架，用于在顺序决策和控制场景中高效学习不同任务的控制器参数，通过利用问题结构知识而非将闭环成本视为黑盒，实现了任务间的知识迁移和增强的数据效率。&lt;h4&gt;背景&lt;/h4&gt;许多控制问题需要在不同的闭环任务中反复调整和适应控制器，其中数据效率和适应性至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种层次化贝叶斯优化框架，针对顺序决策和控制场景中的不同任务，实现高效的控制器参数学习。&lt;h4&gt;方法&lt;/h4&gt;该方法不将闭环成本视为黑盒，而是利用底层问题的结构知识（包括动态系统、控制定律和相关的闭环成本函数）。使用高斯过程构建层次化代理模型，捕捉不同参数化下的闭环状态演化，并通过已知的闭式表达式精确计算特定任务的权重并累积到闭环成本中，实现不同闭环任务间的知识迁移和增强的数据效率。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架保留了与标准黑盒贝叶斯优化相当的次线性遗憾保证，同时支持多任务或迁移学习。&lt;h4&gt;结论&lt;/h4&gt;在模型预测控制的模拟实验中，与纯黑盒贝叶斯优化方法相比，该方法在样本效率和适应性方面都显示出显著优势。&lt;h4&gt;翻译&lt;/h4&gt;许多控制问题需要在不同的闭环任务中反复调整和适应控制器，其中数据效率和适应性至关重要。我们提出了一种层次化贝叶斯优化框架，专为顺序决策和控制场景中不同任务的高效控制器参数学习而定制。该方法不将闭环成本视为黑盒，而是利用底层问题的结构知识，包括动态系统、控制定律和相关的闭环成本函数。我们使用高斯过程构建层次化代理模型，捕捉不同参数化下的闭环状态演化，同时通过已知的闭式表达式精确计算特定任务的权重并累积到闭环成本中。这实现了不同闭环任务间的知识迁移和增强的数据效率。与标准黑盒贝叶斯优化相比，所提出的框架保留了次线性遗憾保证，同时支持多任务或迁移学习。在模型预测控制的模拟实验中，与纯黑盒贝叶斯优化方法相比，该方法在样本效率和适应性方面都显示出显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many control problems require repeated tuning and adaptation of controllersacross distinct closed-loop tasks, where data efficiency and adaptability arecritical. We propose a hierarchical Bayesian optimization (BO) framework thatis tailored to efficient controller parameter learning in sequentialdecision-making and control scenarios for distinct tasks. Instead of treatingthe closed-loop cost as a black-box, our method exploits structural knowledgeof the underlying problem, consisting of a dynamical system, a control law, andan associated closed-loop cost function. We construct a hierarchical surrogatemodel using Gaussian processes that capture the closed-loop state evolutionunder different parameterizations, while the task-specific weighting andaccumulation into the closed-loop cost are computed exactly via knownclosed-form expressions. This allows knowledge transfer and enhanced dataefficiency between different closed-loop tasks. The proposed framework retainssublinear regret guarantees on par with standard black-box BO, while enablingmulti-task or transfer learning. Simulation experiments with model predictivecontrol demonstrate substantial benefits in both sample efficiency andadaptability when compared to purely black-box BO approaches.</description>
      <author>example@mail.com (Sebastian Hirt, Lukas Theiner, Maik Pfefferkorn, Rolf Findeisen)</author>
      <guid isPermaLink="false">2508.12738v2</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features</title>
      <link>http://arxiv.org/abs/2508.13953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为ReviewGraph的新框架，用于酒店业客户评论评分预测，通过将文本评论转换为知识图谱并利用图嵌入和情感特征进行预测，相比传统方法具有更低计算成本和更好的可解释性。&lt;h4&gt;背景&lt;/h4&gt;在酒店业中，理解影响客户评论评分的因素对提高客户满意度和业务表现至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出ReviewGraph框架用于评论评分预测(RRP)，通过分析客户评论来预测评分。&lt;h4&gt;方法&lt;/h4&gt;将文本客户评论转换为知识图谱，提取(主语、谓语、宾语)三元组并关联情感分数，使用Node2Vec图嵌入和情感特征，通过机器学习分类器预测评论评分，并在HotelRec数据集上与传统NLP基线和大型语言模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;ReviewGraph与现有最佳模型性能相似但计算成本更低；与LLMs具有可比的预测性能；在基于一致性的指标上优于基线；在可解释性、可视化和RAG系统集成方面具有额外优势。&lt;h4&gt;结论&lt;/h4&gt;图表示法在增强评论分析方面具有潜力，为未来研究整合高级图神经网络和微调的基于LLM的提取方法奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;在酒店业中，理解驱动客户评论评分的因素对于提高客户满意度和业务表现至关重要。本研究提出了ReviewGraph用于评论评分预测(RRP)的新框架，通过提取(主语、谓语、宾语)三元组并关联情感分数，将文本客户评论转换为知识图谱。利用图嵌入(Node2Vec)和情感特征，该框架通过机器学习分类器预测评论评分。我们将ReviewGraph性能与传统NLP基线(如词袋、TF-IDF和Word2Vec)和大型语言模型进行比较，并在HotelRec数据集上评估它们。与最先进的文献相比，我们提出的模型与最佳性能模型相似，但计算成本更低(无需集成)。虽然ReviewGraph在预测性能上与LLMs相当，并在基于一致性的指标(如Cohen's Kappa)上优于基线，但它还提供了可解释性、可视化和潜在集成到检索增强生成(RAG)系统的额外优势。这项研究强调了图表示法在增强评论分析方面的潜力，并为未来研究整合高级图神经网络和微调的基于LLM的提取方法奠定了基础。我们将在GitHub页面上分享ReviewGraph输出和开源平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the hospitality industry, understanding the factors that drive customerreview ratings is critical for improving guest satisfaction and businessperformance. This work proposes ReviewGraph for Review Rating Prediction (RRP),a novel framework that transforms textual customer reviews into knowledgegraphs by extracting (subject, predicate, object) triples and associatingsentiment scores. Using graph embeddings (Node2Vec) and sentiment features, theframework predicts review rating scores through machine learning classifiers.We compare ReviewGraph performance with traditional NLP baselines (such as Bagof Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluatingthem in the HotelRec dataset. In comparison to the state of the art literature,our proposed model performs similar to their best performing model but withlower computational cost (without ensemble).  While ReviewGraph achieves comparable predictive performance to LLMs andoutperforms baselines on agreement-based metrics such as Cohen's Kappa, itoffers additional advantages in interpretability, visual exploration, andpotential integration into Retrieval-Augmented Generation (RAG) systems. Thiswork highlights the potential of graph-based representations for enhancingreview analytics and lays the groundwork for future research integratingadvanced graph neural networks and fine-tuned LLM-based extraction methods. Wewill share ReviewGraph output and platform open-sourced on our GitHub pagehttps://github.com/aaronlifenghan/ReviewGraph</description>
      <author>example@mail.com (A. J. W. de Vink, Natalia Amat-Lefort, Lifeng Han)</author>
      <guid isPermaLink="false">2508.13953v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems</title>
      <link>http://arxiv.org/abs/2508.13839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可移动天线(MA)辅助的无蜂窝集成感知与通信(CF-ISAC)系统，以解决实际部署中功率放大器非线性失真对通信和感知性能的负面影响。该系统通过分布式失真感知的最坏情况鲁棒优化框架和自注意力卷积图神经网络算法，有效减轻了失真并增强了系统鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;无蜂窝集成感知与通信(CF-ISAC)架构是6G的潜在使能技术，提供频谱效率和无处不在的覆盖。然而，实际部署中存在硬件缺陷，特别是功率放大器(PA)的非线性失真，这会降低通信和感知性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种可移动天线(MA)辅助的CF-ISAC系统，以减轻功率放大器非线性失真并增强系统鲁棒性，改善通信-感知权衡。&lt;h4&gt;方法&lt;/h4&gt;1. 将PA非线性建模为三阶无记忆多项式，考虑三阶失真系数(3RDCs)在不同接入点间的变化；2. 设计分布式失真感知的最坏情况鲁棒优化框架；3. 分析PA失真对Cramer-Rao下界和通信速率的最坏情况影响；4. 应用连续凸近似(SCA)估计3RDCs；5. 在发射功率和感知约束下联合优化波束成形和MA位置；6. 开发MA使能的自注意力卷积图神经网络(SACGNN)算法解决高度非凸问题。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，所提方法显著提高了失真条件下的通信-感知权衡，并在鲁棒性和容量方面优于固定位置天线基线。&lt;h4&gt;结论&lt;/h4&gt;MA辅助的CF-ISAC系统在减轻功率放大器非线性失真方面具有明显优势，能有效提升系统性能，为6G网络提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;无蜂窝集成感知与通信(CF-ISAC)架构是6G的一种有前景的使能技术，提供频谱效率和无处不在的覆盖。然而，实际部署中存在硬件缺陷，特别是功率放大器(PA)的非线性失真，这会降低通信和感知性能。为此，我们提出了一种可移动天线(MA)辅助的CF-ISAC系统，以减轻失真并增强鲁棒性。PA非线性被建模为三阶无记忆多项式，其中三阶失真系数(3RDCs)因硬件差异、老化和环境条件而在不同接入点之间变化。我们设计了一种分布式失真感知的最坏情况鲁棒优化框架，明确考虑了3RDCs的不确定性。首先，我们分析PA失真对Cramer-Rao下界(CRLB)和通信速率的最坏情况影响。然后，为解决由此产生的非凸性，我们应用连续凸近似(SCA)来估计3RDCs。在此基础上，我们在发射功率和感知约束下联合优化波束成形和MA位置。为有效解决这个高度非凸问题，我们开发了一种MA使能的自注意力卷积图神经网络(SACGNN)算法。仿真结果表明，我们的方法显著提高了失真条件下的通信-感知权衡，并在鲁棒性和容量方面优于固定位置天线基线，从而突显了MA辅助CF-ISAC系统的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The cell-free integrated sensing and communication (CF-ISAC) architecture isa promising enabler for 6G, offering spectrum efficiency and ubiquitouscoverage. However, real deployments suffer from hardware impairments,especially nonlinear distortion from power amplifiers (PAs), which degradesboth communication and sensing. To address this, we propose a movable antenna(MA)-aided CF-ISAC system that mitigates distortion and enhances robustness.The PAs nonlinearities are modeled by a third-order memoryless polynomial,where the third-order distortion coefficients (3RDCs) vary across access points(APs) due to hardware differences, aging, and environmental conditions. Wedesign a distributed distortion-aware worst-case robust optimization frameworkthat explicitly incorporates uncertainty in 3RDCs. First, we analyze theworst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB)and communication rate. Then, to address the resulting non-convexity, we applysuccessive convex approximation (SCA) for estimating the 3RDCs. With these, wejointly optimize beamforming and MA positions under transmit power and sensingconstraints. To efficiently solve this highly non-convex problem, we develop anMA-enabled self-attention convolutional graph neural network (SACGNN)algorithm. Simulations demonstrate that our method substantially enhances thecommunication-sensing trade-off under distortion and outperforms fixed-positionantenna baselines in terms of robustness and capacity, thereby highlighting theadvantages of MA-aided CF-ISAC systems.</description>
      <author>example@mail.com (Yue Xiu, Yang Zhao, Ran Yang, Zheng Dong, Wanting Lyu, Zeyuan Zhang, Dusit Niyato, Guangyi Liu, Ning Wei)</author>
      <guid isPermaLink="false">2508.13839v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation</title>
      <link>http://arxiv.org/abs/2508.13745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted as a full paper at ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为REARM的新框架，用于改进多模态推荐系统中的对比学习和同构图关系，解决了现有方法在数据稀疏环境下的两个主要局限性。&lt;h4&gt;背景&lt;/h4&gt;多模态推荐系统利用丰富的模态信息（图像和文本描述）提高推荐性能，当前方法依赖图神经网络的结构建模能力取得成功，但在真实场景中常受限于稀疏数据。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态推荐系统中的两个主要局限性：1)简单多模态特征对比产生的无效表示问题；2)用户兴趣与项目共现间同构图关系探索不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出REARM框架，使用元网络和正交约束策略补充多模态对比学习，过滤模态共享特征噪声并保留模态独特特征中与推荐相关的信息；同时整合新构建的用户兴趣图和项目共现图与现有图进行学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实数据集上的实验表明，REARM优于各种最先进的基线方法；可视化分析显示REARM在区分模态共享和模态独特特征方面有显著改进。&lt;h4&gt;结论&lt;/h4&gt;REARM框架有效解决了多模态推荐系统中的特征表示和关系挖掘问题，提高了在数据稀疏环境下的推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态推荐系统专注于利用物品丰富的模态信息（即图像和文本描述）来提高推荐性能。当前方法凭借图神经网络的强大结构建模能力取得了显著成功。然而，这些方法在现实场景中常受限于稀疏数据。尽管采用对比学习和同质图（即同构图）来解决数据稀疏挑战，现有方法仍存在两个主要局限：1)简单的多模态特征对比无法产生有效表示，导致模态共享特征中的噪声和模态独特特征中有价值信息的丢失；2)缺乏对用户兴趣和项目共现之间同构图关系的探索，导致用户-项目交互挖掘不完整。为解决上述局限，我们提出了一个新颖的框架REARM（改进多模态对比学习和同构图关系）。具体而言，我们通过采用元网络和正交约束策略来补充多模态对比学习，过滤模态共享特征中的噪声并保留模态独特特征中与推荐相关的信息。为了有效挖掘同质关系，我们将新构建的用户兴趣图和项目共现图与现有的用户共现图和项目语义图整合进行图学习。在三个真实数据集上的广泛实验证明了REARM相对于各种最先进基线的优越性。我们的可视化进一步显示了REARM在区分模态共享和模态独特特征方面的改进。代码可在https://github.com/MrShouxingMa/REARM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755779&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal recommender system focuses on utilizing rich modal information (i.e., images and textual descriptions) of items to improve recommendationperformance. The current methods have achieved remarkable success with thepowerful structure modeling capability of graph neural networks. However, thesemethods are often hindered by sparse data in real-world scenarios. Althoughcontrastive learning and homography ( i.e., homogeneous graphs) are employed toaddress the data sparsity challenge, existing methods still suffer two mainlimitations: 1) Simple multi-modal feature contrasts fail to produce effectiverepresentations, causing noisy modal-shared features and loss of valuableinformation in modal-unique features; 2) The lack of exploration of thehomograph relations between user interests and item co-occurrence results inincomplete mining of user-item interplay.  To address the above limitations, we propose a novel framework for\textbf{R}\textbf{E}fining multi-mod\textbf{A}l cont\textbf{R}astive learningand ho\textbf{M}ography relations (\textbf{REARM}). Specifically, we complementmulti-modal contrastive learning by employing meta-network and orthogonalconstraint strategies, which filter out noise in modal-shared features andretain recommendation-relevant information in modal-unique features. To minehomogeneous relationships effectively, we integrate a newly constructed userinterest graph and an item co-occurrence graph with the existing userco-occurrence and item semantic graphs for graph learning. The extensiveexperiments on three real-world datasets demonstrate the superiority of REARMto various state-of-the-art baselines. Our visualization further shows animprovement made by REARM in distinguishing between modal-shared andmodal-unique features. Code is available\href{https://github.com/MrShouxingMa/REARM}{here}.</description>
      <author>example@mail.com (Shouxing Ma, Yawen Zeng, Shiqing Wu, Guandong Xu)</author>
      <guid isPermaLink="false">2508.13745v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning</title>
      <link>http://arxiv.org/abs/2508.13716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CaPGNN框架，通过联合自适应缓存算法和资源感知图分区算法，在单服务器多GPU环境下实现了高效的全批量GNN训练，显著减少了通信开销并平衡了计算负载。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在处理图结构数据方面表现出色，但在分布式环境中，全批量GNN训练的可扩展性受到高通信开销和负载不平衡的严重限制。&lt;h4&gt;目的&lt;/h4&gt;设计一个在单服务器多GPU环境下高效并行全批量GNN训练的框架，以减少冗余的GPU间通信和平衡计算工作负载。&lt;h4&gt;方法&lt;/h4&gt;提出联合自适应缓存算法利用CPU和GPU内存减少顶点特征的重复传输；引入资源感知图分区算法根据GPU的异构计算和通信能力动态调整子图大小。&lt;h4&gt;主要发现&lt;/h4&gt;在大型基准数据集上的实验表明，CaPGNN能够将通信成本降低高达96%，并将GNN训练加速高达12.7倍，相比现有方法有显著提升。&lt;h4&gt;结论&lt;/h4&gt;自适应缓存和资源感知分区技术能够有效促进分布式计算环境中全批量GNN训练的可扩展性、高效性和实际部署。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在处理各种现实世界应用中常见的图结构数据方面表现出色。然而，在分布式环境中，全批量GNN训练的可扩展性受到高通信开销和负载不平衡的严重限制。在本文中，我们提出了CaPGNN，这是一个在单服务器多GPU环境下高效并行全批量GNN训练的新颖框架，专门设计用于减少冗余的GPU间通信和平衡计算工作负载。我们提出了一种联合自适应缓存算法，利用CPU和GPU内存来显著减少分区之间顶点特征的重复传输。此外，我们引入了一种资源感知的图分区算法，根据GPU的异构计算和通信能力动态调整子图大小。在大型基准数据集上的大量实验表明，与最先进的方法相比，CaPGNN能够将通信成本降低高达96%，并将GNN训练加速高达12.7倍。我们的结果凸显了自适应缓存和资源感知分区在促进分布式计算环境中全批量GNN训练的可扩展性、高效性和实际部署方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable capabilities in processinggraph-structured data prevalent in various real-world applications. However,the scalability of full-batch GNN training becomes severely limited by highcommunication overhead and load imbalance in distributed environments. In thispaper, we present CaPGNN, a novel framework for efficient parallel full-batchGNN training on single-server with multi-GPU, designed specifically to reduceredundant inter-GPU communication and balance computational workloads. Wepropose a joint adaptive caching algorithm that leverages both CPU and GPUmemory to significantly reduce the repetitive transmission of vertex featuresacross partitions. Additionally, we introduce a resource-aware graphpartitioning algorithm that adjusts subgraph sizes dynamically according to theheterogeneous computational and communication capacities of GPUs. Extensiveexperiments on large-scale benchmark datasets demonstrate that CaPGNNeffectively reduces communication costs by up to 96% and accelerates GNNtraining by up to 12.7 times compared to state-of-the-art approaches. Ourresults highlight the potential of adaptive caching and resource-awarepartitioning to facilitate scalable, efficient, and practical deployment offull-batch GNN training in distributed computing environments.</description>
      <author>example@mail.com (Xianfeng Song, Yi Zou, Zheng Shi)</author>
      <guid isPermaLink="false">2508.13716v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer</title>
      <link>http://arxiv.org/abs/2508.13435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SVDformer，一种结合奇异值分解和Transformer架构的新型有向图表示学习框架，有效解决了现有有向图神经网络难以同时捕获方向语义和全局结构模式的问题。&lt;h4&gt;背景&lt;/h4&gt;有向图被广泛用于模拟现实系统中的非对称关系。然而，现有的有向图神经网络由于其各向同性的聚合机制和局部化的过滤机制，往往难以同时捕获方向语义和全局结构模式。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有有向图神经网络的局限性，本文提出SVDformer框架，协同奇异值分解和Transformer架构进行有向感知的图表示学习。&lt;h4&gt;方法&lt;/h4&gt;SVDformer通过多头自注意力机制细化奇异值嵌入，增强关键频谱成分并抑制高频噪声，实现可学习的低通/高通图过滤；同时将奇异向量视为方向投影基，奇异值视为缩放因子，利用Transformer建模传入/传出边缘模式间的多尺度交互，保留边缘方向性。&lt;h4&gt;主要发现&lt;/h4&gt;在六个有向图基准上的大量实验表明，SVDformer在节点分类任务上始终优于最先进的GNN和有向感知基线方法。&lt;h4&gt;结论&lt;/h4&gt;SVDformer为在有向图上学习表示建立了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;有向图被广泛用于模拟现实系统中的非对称关系。然而，现有的有向图神经网络由于其各向同性的聚合机制和局部化的过滤机制，往往难以同时捕获方向语义和全局结构模式。为了解决这一限制，本文提出了SVDformer，这是一种新的框架，协同奇异值分解和Transformer架构进行有向感知的图表示学习。SVDformer首先通过多头自注意力机制细化奇异值嵌入，自适应地增强关键频谱成分，同时抑制高频噪声。这使得可学习的低通/高通图过滤成为可能，而无需频谱核。此外，通过将奇异向量视为方向投影基，奇异值视为缩放因子，SVDformer使用Transformer来通过注意力权重建模传入/传出边缘模式之间的多尺度交互，从而在特征传播过程中明确保留边缘方向性。在六个有向图基准上的大量实验表明，SVDformer在节点分类任务上始终优于最先进的GNN和有向感知基线方法，为在有向图上学习表示建立了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Directed graphs are widely used to model asymmetric relationships inreal-world systems. However, existing directed graph neural networks oftenstruggle to jointly capture directional semantics and global structuralpatterns due to their isotropic aggregation mechanisms and localized filteringmechanisms. To address this limitation, this paper proposes SVDformer, a novelframework that synergizes SVD and Transformer architecture for direction-awaregraph representation learning. SVDformer first refines singular valueembeddings through multi-head self-attention, adaptively enhancing criticalspectral components while suppressing high-frequency noise. This enableslearnable low-pass/high-pass graph filtering without requiring spectralkernels. Furthermore, by treating singular vectors as directional projectionbases and singular values as scaling factors, SVDformer uses the Transformer tomodel multi-scale interactions between incoming/outgoing edge patterns throughattention weights, thereby explicitly preserving edge directionality duringfeature propagation. Extensive experiments on six directed graph benchmarksdemonstrate that SVDformer consistently outperforms state-of-the-art GNNs anddirection-aware baselines on node classification tasks, establishing a newparadigm for learning representations on directed graphs.</description>
      <author>example@mail.com (Jiayu Fang, Zhiqi Shao, S T Boris Choy, Junbin Gao)</author>
      <guid isPermaLink="false">2508.13435v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems</title>
      <link>http://arxiv.org/abs/2508.13371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IAAI-26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LOOP是一种创新的神经符号规划框架，通过神经和符号组件之间的迭代对话解决复杂规划问题，在标准基准测试中表现出色，为构建可靠的自主系统提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;规划是自主系统中最重要的任务之一，小错误可能导致重大故障或经济损失。当前神经规划方法在复杂领域存在缺陷，产生不完整计划；经典规划器虽提供逻辑保证但缺乏灵活性；现有神经符号方法仅做一次性翻译，未充分利用神经与符号组件的协作潜力。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型神经符号规划框架，使神经和符号组件能够像迭代对话一样协作，而非简单翻译，从而解决复杂规划问题。&lt;h4&gt;方法&lt;/h4&gt;创建名为LOOP的神经符号规划框架，集成13个协调神经特征，包括空间关系图神经网络、多代理验证系统、分层分解任务管理和因果记忆系统。LOOP生成PDDL规范，基于符号反馈迭代完善，并从执行轨迹构建因果知识库。&lt;h4&gt;主要发现&lt;/h4&gt;在六个IPC基准领域测试中，LOOP实现85.8%的成功率，显著优于LLM+P(55.0%)、LLM-as-Planner(19.2%)和Tree-of-Thoughts(3.3%)。&lt;h4&gt;结论&lt;/h4&gt;可靠规划的关键不在于选择神经网络或符号推理器，而在于让它们在整个规划过程中真正'对话'。LOOP为构建可信任的自主系统提供了全面蓝图。&lt;h4&gt;翻译&lt;/h4&gt;规划是自主系统中最重要的任务之一，即使是小错误也可能导致重大故障或数百万美元的损失。当前最先进的神经规划方法难以处理复杂领域，产生的计划可能缺少前提条件、目标不一致或出现幻觉。虽然经典规划器能提供逻辑保证，但它们缺乏现代自主系统所需的灵活性和自然语言理解能力。现有的神经符号方法使用从自然语言到形式化计划的一次性翻译，错失了神经和符号组件共同工作和完善解决方案的机会。为解决这一差距，我们开发了LOOP——一种新型神经符号规划框架，将规划视为神经和符号组件之间的迭代对话，而非简单翻译。LOOP集成了13个协调的神经特征，包括用于空间关系的图神经网络、用于基于共识正确性的多代理验证、用于复杂任务管理的分层分解，以及能够从成功和失败中学习的因果记忆。与现有方法不同，LOOP生成PDDL规范，基于符号反馈迭代完善它们，并从执行轨迹构建因果知识库。LOOP在六个标准IPC基准领域进行了评估，实现了85.8%的成功率，而LLM+P为55.0%，LLM-as-Planner为19.2%，Tree-of-Thoughts为3.3%。这项工作表明，可靠规划的关键不在于在神经网络或符号推理器之间做出选择，而在于让它们在整个过程中真正'对话'。LOOP为构建最终可以信任用于关键现实世界应用的自主系统提供了全面的蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Planning is one of the most critical tasks in autonomous systems, where evena small error can lead to major failures or million-dollar losses. Currentstate-of-the-art neural planning approaches struggle with complex domains,producing plans with missing preconditions, inconsistent goals, andhallucinations. While classical planners provide logical guarantees, they lackthe flexibility and natural language understanding capabilities needed formodern autonomous systems. Existing neuro-symbolic approaches use one-shottranslation from natural language to formal plans, missing the opportunity forneural and symbolic components to work and refine solutions together. Toaddress this gap, we develop LOOP -- a novel neuro-symbolic planning frameworkthat treats planning as an iterative conversation between neural and symboliccomponents rather than simple translation. LOOP integrates 13 coordinatedneural features including graph neural networks for spatial relationships,multi-agent validation for consensus-based correctness, hierarchicaldecomposition for complex task management, and causal memory that learns fromboth successes and failures. Unlike existing approaches, LOOP generates PDDLspecifications, refines them iteratively based on symbolic feedback, and buildsa causal knowledge base from execution traces. LOOP was evaluated on sixstandard IPC benchmark domains, where it achieved 85.8% success rate comparedto LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). Thiswork shows that the key to reliable planning is not in choosing between neuralnetworks or symbolic reasoners but it lies in making them actually ``talk'' toeach other during the entire process. LOOP provides a thorough blueprint forbuilding autonomous systems that can finally be trusted with criticalreal-world applications.</description>
      <author>example@mail.com (Ronit Virwani, Ruchika Suryawanshi)</author>
      <guid isPermaLink="false">2508.13371v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning</title>
      <link>http://arxiv.org/abs/2508.10747v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种稀疏的、目标感知的图神经网络表示方法，用于解决大规模规划问题中的挑战，有效提高了算法在大型网格环境中的可扩展性。&lt;h4&gt;背景&lt;/h4&gt;使用深度强化学习结合图神经网络的通用规划方法在PDDL描述的符号规划领域显示出良好的结果。然而，现有方法通常将规划状态表示为完全连接的图，这导致了边信息的组合爆炸和大量稀疏性，特别是在大型网格环境中尤为明显。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在大型规划问题中的可扩展性问题，特别是完全连接图表示导致的内存需求指数级增长和学习不可行的问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种稀疏的、目标感知的图神经网络表示方法，选择性编码相关的局部关系并明确整合与目标相关的空间特征。作者还设计了基于PDDL的无人机任务场景，在网格世界中模拟真实的任务执行环境。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法能够有效扩展到之前使用密集图表示不可行的大型网格尺寸，并显著提高了策略泛化和成功率。&lt;h4&gt;结论&lt;/h4&gt;该方法为解决现实世界的大规模通用规划任务提供了实用基础。&lt;h4&gt;翻译&lt;/h4&gt;使用深度强化学习结合图神经网络的通用规划在由PDDL描述的各种符号规划领域显示出良好的结果。然而，现有方法通常将规划状态表示为完全连接的图，这导致了边信息的组合爆炸，随着问题规模的扩大而出现大量稀疏性，特别是在大型网格环境中尤为明显。这种密集表示稀释了节点级信息，指数级增加了内存需求，最终使得在更大规模问题上学习变得不可行。为解决这些挑战，我们提出了一种稀疏的、目标感知的图神经网络表示，它选择性编码相关的局部关系并明确整合与目标相关的空间特征。我们通过在网格世界中设计基于PDDL的新型无人机任务场景来验证我们的方法，有效模拟了真实的任务执行环境。我们的实验结果表明，我们的方法能够有效扩展到之前使用密集图表示不可行的大型网格尺寸，并显著提高了策略泛化和成功率。我们的发现为解决现实世界的大规模通用规划任务提供了实用基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized planning using deep reinforcement learning (RL) combined withgraph neural networks (GNNs) has shown promising results in various symbolicplanning domains described by PDDL. However, existing approaches typicallyrepresent planning states as fully connected graphs, leading to a combinatorialexplosion in edge information and substantial sparsity as problem scales grow,especially evident in large grid-based environments. This dense representationresults in diluted node-level information, exponentially increases memoryrequirements, and ultimately makes learning infeasible for larger-scaleproblems. To address these challenges, we propose a sparse, goal-aware GNNrepresentation that selectively encodes relevant local relationships andexplicitly integrates spatial features related to the goal. We validate ourapproach by designing novel drone mission scenarios based on PDDL within a gridworld, effectively simulating realistic mission execution environments. Ourexperimental results demonstrate that our method scales effectively to largergrid sizes previously infeasible with dense graph representations andsubstantially improves policy generalization and success rates. Our findingsprovide a practical foundation for addressing realistic, large-scalegeneralized planning tasks.</description>
      <author>example@mail.com (Sangwoo Jeon, Juchul Shin, Gyeong-Tae Kim, YeonJe Cho, Seongwoo Kim)</author>
      <guid isPermaLink="false">2508.10747v2</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model</title>
      <link>http://arxiv.org/abs/2508.13584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种时态条件指代视频对象分割模型，通过创新整合现有分割方法、利用文本到视频扩散模型、移除噪声预测模块和设计时间上下文掩模精炼模块，在四个公开基准测试上取得了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;指代视频对象分割(RVOS)旨在根据文本描述分割视频中的特定对象。现有方法过分强调特征提取和时间建模，而忽视了分割头设计，实际上分割头仍有很大改进空间。&lt;h4&gt;目的&lt;/h4&gt;解决现有RVOS方法在分割头设计上的不足，提高边界分割能力，同时简化模型结构并提升性能。&lt;h4&gt;方法&lt;/h4&gt;1. 提出时态条件指代视频对象分割模型，整合现有分割方法；2. 利用文本到视频扩散模型进行特征提取；3. 移除传统噪声预测模块避免噪声随机性影响；4. 设计时间上下文掩模精炼(TCMR)模块克服VAE特征提取局限。&lt;h4&gt;主要发现&lt;/h4&gt;1. 分割头设计在RVOS中有很大改进空间；2. 移除噪声预测模块可简化模型同时提高性能；3. TCMR模块能显著提高分割质量而无需复杂设计。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在四个公开的RVOS基准测试上持续取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;指代视频对象分割(RVOS)旨在根据文本描述分割视频中的特定对象。我们观察到，最近的RVOS方法往往过分强调特征提取和时间建模，而相对忽视了分割头的设计。事实上，分割头设计仍有相当大的改进空间。为此，我们提出了一种时态条件指代视频对象分割模型，创新性地整合现有分割方法以有效增强边界分割能力。此外，我们的模型利用文本到视频扩散模型进行特征提取。在此基础上，我们移除了传统的噪声预测模块，以避免噪声的随机性降低分割准确性，从而简化模型同时提高性能。最后，为了克服VAE特征提取能力的局限性，我们设计了一个时间上下文掩模精炼(TCMR)模块，它显著提高了分割质量而无需引入复杂设计。我们在四个公开的RVOS基准测试上评估了我们的方法，它持续取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring Video Object Segmentation (RVOS) aims to segment specific objectsin a video according to textual descriptions. We observe that recent RVOSapproaches often place excessive emphasis on feature extraction and temporalmodeling, while relatively neglecting the design of the segmentation head. Infact, there remains considerable room for improvement in segmentation headdesign. To address this, we propose a Temporal-Conditional Referring VideoObject Segmentation model, which innovatively integrates existing segmentationmethods to effectively enhance boundary segmentation capability. Furthermore,our model leverages a text-to-video diffusion model for feature extraction. Ontop of this, we remove the traditional noise prediction module to avoid therandomness of noise from degrading segmentation accuracy, thereby simplifyingthe model while improving performance. Finally, to overcome the limited featureextraction capability of the VAE, we design a Temporal Context Mask Refinement(TCMR) module, which significantly improves segmentation quality withoutintroducing complex designs. We evaluate our method on four public RVOSbenchmarks, where it consistently achieves state-of-the-art performance.</description>
      <author>example@mail.com (Ruixin Zhang, Jiaqing Fan, Yifan Liao, Qian Qiao, Fanzhang Li)</author>
      <guid isPermaLink="false">2508.13584v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>YOLO11-CR: a Lightweight Convolution-and-Attention Framework for Accurate Fatigue Driving Detection</title>
      <link>http://arxiv.org/abs/2508.13205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为YOLO11-CR的轻量级高效目标检测模型，用于实时驾驶员疲劳检测。该模型通过引入卷积与注意力融合模块和矩形校准模块，解决了现有视觉疲劳检测方法面临的挑战，在DSM数据集上取得了优异的性能表现。&lt;h4&gt;背景&lt;/h4&gt;驾驶员疲劳检测对智能交通系统至关重要，有助于减少道路交通事故。现有的基于生理学和车辆动力学的方法虽然准确，但通常具有侵入性、依赖硬件，且在实际环境中缺乏鲁棒性。基于视觉的方法提供了一种非侵入性和可扩展的替代方案，但仍面临检测小目标或被遮挡目标困难、多尺度特征建模有限等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉疲劳检测方法面临的挑战，提出一种轻量级、高效的目标检测模型，用于实时疲劳检测。&lt;h4&gt;方法&lt;/h4&gt;提出YOLO11-CR模型，一种专门为实时疲劳检测设计的轻量级高效目标检测模型。引入两个关键模块：1) 卷积与注意力融合模块（CAFM）：将局部CNN特征与基于Transformer的全局上下文结合，以增强特征表达能力；2) 矩形校准模块（RCM）：捕获水平和垂直上下文信息，改善空间定位，特别是对于侧面人脸和手机等小物体。&lt;h4&gt;主要发现&lt;/h4&gt;在DSM数据集上的实验结果表明，YOLO11-CR达到了87.17%的精确率、83.86%的召回率、mAP@50为88.09%、mAP@50-95为55.93%，显著优于基线模型。消融研究进一步验证了CAFM和RCM模块在提高敏感性和定位准确性方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;YOLO11-CR为车载疲劳监测提供了一种实用且高性能的解决方案，具有在实际部署中的强大潜力，并可通过时间建模、多模态数据集成和嵌入式优化进行未来改进。&lt;h4&gt;翻译&lt;/h4&gt;驾驶员疲劳检测对智能交通系统至关重要，因为它在减轻道路交通事故方面发挥着关键作用。虽然基于生理学和车辆动力学的方法提供了准确性，但它们通常是侵入性的、依赖硬件的，并且在真实环境中缺乏鲁棒性。基于视觉的技术提供了一种非侵入性和可扩展的替代方案，但仍面临诸如小目标或被遮挡目标检测能力差以及多尺度特征建模有限等挑战。为了解决这些问题，本文提出了YOLO11-CR，一种专为实时疲劳检测量身定制的轻量级高效目标检测模型。YOLO11-CR引入了两个关键模块：卷积与注意力融合模块（CAFM），它将局部CNN特征与基于Transformer的全局上下文相结合，以增强特征表达能力；以及矩形校准模块（RCM），它捕获水平和垂直上下文信息，以改善空间定位，特别是对于侧面人脸和手机等小目标。在DSM数据集上的实验表明，YOLO11-CR达到了87.17%的精确率、83.86%的召回率、88.09%的mAP@50和55.93%的mAP@50-95，显著优于基线模型。消融研究进一步验证了CAFM和RCM模块在提高敏感性和定位准确性方面的有效性。这些结果表明，YOLO11-CR为车载疲劳监测提供了一种实用且高性能的解决方案，具有强大的实际部署潜力，以及未来涉及时间建模、多模态数据集成和嵌入式优化的改进潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driver fatigue detection is of paramount importance for intelligenttransportation systems due to its critical role in mitigating road trafficaccidents. While physiological and vehicle dynamics-based methods offeraccuracy, they are often intrusive, hardware-dependent, and lack robustness inreal-world environments. Vision-based techniques provide a non-intrusive andscalable alternative, but still face challenges such as poor detection of smallor occluded objects and limited multi-scale feature modeling. To address theseissues, this paper proposes YOLO11-CR, a lightweight and efficient objectdetection model tailored for real-time fatigue detection. YOLO11-CR introducestwo key modules: the Convolution-and-Attention Fusion Module (CAFM), whichintegrates local CNN features with global Transformer-based context to enhancefeature expressiveness; and the Rectangular Calibration Module (RCM), whichcaptures horizontal and vertical contextual information to improve spatiallocalization, particularly for profile faces and small objects like mobilephones. Experiments on the DSM dataset demonstrated that YOLO11-CR achieves aprecision of 87.17%, recall of 83.86%, mAP@50 of 88.09%, and mAP@50-95 of55.93%, outperforming baseline models significantly. Ablation studies furthervalidate the effectiveness of the CAFM and RCM modules in improving bothsensitivity and localization accuracy. These results demonstrate that YOLO11-CRoffers a practical and high-performing solution for in-vehicle fatiguemonitoring, with strong potential for real-world deployment and futureenhancements involving temporal modeling, multi-modal data integration, andembedded optimization.</description>
      <author>example@mail.com (Zhebin Jin, Ligang Dong)</author>
      <guid isPermaLink="false">2508.13205v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Multi-view Clustering via Bi-level Decoupling and Consistency Learning</title>
      <link>http://arxiv.org/abs/2508.13499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为双层解耦与一致性学习框架(BDCL)的新方法，用于多视图聚类，通过探索多视图数据的有效表示来提高特征的簇间判别性和簇内紧凑性。&lt;h4&gt;背景&lt;/h4&gt;多视图聚类是分析多视图数据中潜在模式的有效方法，通过学习多视图特征之间的一致性和互补性可以改进聚类性能，但通常忽略了面向聚类的表示学习。&lt;h4&gt;目的&lt;/h4&gt;进一步探索多视图数据的有效表示，以提高多视图聚类中特征的簇间判别性和簇内紧凑性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含三个模块的框架：1)多视图实例学习模块，通过重构自编码器和对比学习对齐一致信息并保留私有特征；2)特征和簇的双层解耦，增强特征空间和簇空间的判别性；3)一致性学习模块，将样本的不同视图及其邻居视为正对，学习聚类分配的一致性并压缩簇内空间。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准数据集上的实验结果表明，所提出的BDCL方法与最先进的方法相比具有优越性。&lt;h4&gt;结论&lt;/h4&gt;BDCL框架通过双层解耦和一致性学习有效提升了多视图聚类的性能，代码已公开发布。&lt;h4&gt;翻译&lt;/h4&gt;多视图聚类已被证明是分析多视图数据中潜在模式的有效方法。通过学习多视图特征之间的一致性和互补性可以改进聚类性能，然而，面向聚类的表示学习常常被忽视。在本文中，我们提出了一种新颖的双层解耦与一致性学习框架(BDCL)，以进一步探索多视图数据的有效表示，增强多视图聚类中特征的簇间判别性和簇内紧凑性。我们的框架包含三个模块：1)多视图实例学习模块通过重构自编码器和对比学习对齐一致信息，同时保留视图间的私有特征。2)特征和簇的双层解耦增强了特征空间和簇空间的判别性。3)一致性学习模块将样本的不同视图及其邻居视为正对，学习它们聚类分配的一致性，并进一步压缩簇内空间。在五个基准数据集上的实验结果表明，与最先进的方法相比，所提出的方法具有优越性。我们的代码已发布在https://github.com/LouisDong95/BDCL。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-view clustering has shown to be an effective method for analyzingunderlying patterns in multi-view data. The performance of clustering can beimproved by learning the consistency and complementarity between multi-viewfeatures, however, cluster-oriented representation learning is oftenoverlooked. In this paper, we propose a novel Bi-level Decoupling andConsistency Learning framework (BDCL) to further explore the effectiverepresentation for multi-view data to enhance inter-cluster discriminabilityand intra-cluster compactness of features in multi-view clustering. Ourframework comprises three modules: 1) The multi-view instance learning modulealigns the consistent information while preserving the private features betweenviews through reconstruction autoencoder and contrastive learning. 2) Thebi-level decoupling of features and clusters enhances the discriminability offeature space and cluster space. 3) The consistency learning module treats thedifferent views of the sample and their neighbors as positive pairs, learns theconsistency of their clustering assignments, and further compresses theintra-cluster space. Experimental results on five benchmark datasetsdemonstrate the superiority of the proposed method compared with the SOTAmethods. Our code is published on https://github.com/LouisDong95/BDCL.</description>
      <author>example@mail.com (Shihao Dong, Yuhui Zheng, Huiying Xu, Xinzhong Zhu)</author>
      <guid isPermaLink="false">2508.13499v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2508.13433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STPFormer是一种时空模式感知Transformer，通过统一且可解释的表示学习实现了交通预测的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;时空交通预测具有挑战性，因为涉及复杂的时间模式、动态的空间结构和多样的输入格式。&lt;h4&gt;目的&lt;/h4&gt;提出STPFormer模型，解决基于Transformer的模型在时间编码僵化和时空融合不足方面的问题。&lt;h4&gt;方法&lt;/h4&gt;STPFormer整合了四个模块：时间位置聚合器(TPA)用于模式感知的时间编码，空间序列聚合器(SSA)用于顺序空间学习，时空图匹配(STGM)用于跨域对齐，以及注意力混合器用于多尺度融合。&lt;h4&gt;主要发现&lt;/h4&gt;在五个真实世界数据集上的实验表明，STPFormer持续取得了新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;消融研究和可视化证实了STPFormer模型的有效性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;时空交通预测具有挑战性，因为存在复杂的时间模式、动态的空间结构和多样的输入格式。尽管基于Transformer的模型提供了强大的全局建模能力，但它们通常面临时间编码僵化和时空融合不足的问题。我们提出了STPFormer，一种时空模式感知Transformer，通过统一且可解释的表示学习实现了最先进的性能。它整合了四个模块：用于模式感知时间编码的时间位置聚合器(TPA)，用于顺序空间学习的空间序列聚合器(SSA)，用于跨域对齐的时空图匹配(STGM)，以及用于多尺度融合的注意力混合器。在五个真实世界数据集上的实验表明，STPFormer持续取得了新的最先进结果，消融和可视化证实了其有效性和通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-temporal traffic forecasting is challenging due to complex temporalpatterns, dynamic spatial structures, and diverse input formats. AlthoughTransformer-based models offer strong global modeling, they often struggle withrigid temporal encoding and weak space-time fusion. We propose STPFormer, aSpatio-Temporal Pattern-Aware Transformer that achieves state-of-the-artperformance via unified and interpretable representation learning. Itintegrates four modules: Temporal Position Aggregator (TPA) for pattern-awaretemporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatiallearning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,and an Attention Mixer for multi-scale fusion. Experiments on five real-worlddatasets show that STPFormer consistently sets new SOTA results, with ablationand visualizations confirming its effectiveness and generalizability.</description>
      <author>example@mail.com (Jiayu Fang, Zhiqi Shao, S T Boris Choy, Junbin Gao)</author>
      <guid isPermaLink="false">2508.13433v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>NucEL: Single-Nucleotide ELECTRA-Style Genomic Pre-training for Efficient and Interpretable Representations</title>
      <link>http://arxiv.org/abs/2508.13191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NucEL是首个用于基因组基础模型的ELECTRA风格预训练框架，解决了传统MLM方法的局限性，在多种基因组下游任务上达到最先进性能，并能与规模大25倍的模型竞争。&lt;h4&gt;背景&lt;/h4&gt;在基因组序列上预训练大型语言模型是学习生物学有意义表征的强大方法，但现有的掩盖语言建模(MLM)方法如DNABERT和核苷酸变换器(NT)存在部分标记监督、预训练/微调不匹配和高计算成本等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的预训练框架NucEL，解决传统MLM方法的局限性，提供更全面、高效的基因组序列表征学习方法。&lt;h4&gt;方法&lt;/h4&gt;NucEL采用ELECTRA风格框架，使用判别器识别生成器改变的标记，提供全面的标记级监督；整合ModernBERT的混合局部-全局注意力和flash注意力；采用单核苷酸标记化而非6-mer标记化；在人类基因组上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;NucEL在调控元件识别、转录因子结合预测、开放染色质分类和组蛋白修饰谱分析等下游任务上达到最先进结果，超过类似大小的基于MLM的模型，并能与规模大25倍的模型竞争；消融研究确定了最佳标记化和屏蔽策略；注意力分析显示NucEL在捕获生物学相关基序方面优于NT。&lt;h4&gt;结论&lt;/h4&gt;ELECTRA风格的预训练是基因组表征学习的一种高效、有效的策略，对基因组研究有广泛影响。&lt;h4&gt;翻译&lt;/h4&gt;在基因组序列上预训练大型语言模型是学习生物学有意义表征的强大方法。掩盖语言建模(MLM)方法，如DNABERT和核苷酸变换器(NT)，虽然表现良好，但存在部分标记监督、预训练/微调不匹配和高计算成本等问题。我们引入了NucEL，这是首个用于基因组基础模型的ELECTRA风格预训练框架，解决了这些局限性。通过使用判别器识别生成器改变的标记，NucEL提供全面的标记级监督，改进了MLM部分监督的效率。整合ModernBERT的混合局部-全局注意力和flash注意力，NucEL为基因组建模提供了优化的BERT架构。与6-mer标记化不同，NucEL使用单核苷酸标记，实现细粒度分辨率，提高了效率和可解释性。在人类基因组上预训练后，NucEL在多种下游任务上取得了最先进的结果，包括调控元件识别(如启动子、增强子)、转录因子结合预测、开放染色质分类和组蛋白修饰谱分析，超过了类似大小的基于MLM的模型，并能与规模大25倍的模型(如NT)竞争。消融研究突出了ELECTRA风格DNA预训练的最佳标记化和屏蔽策略。注意力分析显示，与NT相比，NucEL在捕获生物学相关基序方面表现更优，为层次学习和调控元件建模提供了见解。这些研究结果表明，ELECTRA风格的预训练是基因组表征学习的一种高效、有效的策略，对基因组研究具有广泛影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training large language models on genomic sequences is a powerfulapproach for learning biologically meaningful representations. Masked languagemodeling (MLM) methods, such as DNABERT and Nucleotide Transformer (NT),achieve strong performance but suffer from partial token supervision,pre-training/fine-tuning mismatches, and high computational costs. We introduceNucEL, the first ELECTRA-style pre-training framework for genomic foundationmodels, addressing these limitations. Using a discriminator to identify tokensaltered by a generator, NucEL provides comprehensive token-level supervisionacross all sequence positions, improving efficiency over the partialsupervision of MLM. Incorporating ModernBERT's hybrid local-global attentionand flash attention, NucEL offers an optimized BERT architecture for genomicmodeling. Unlike 6-mer tokenization, NucEL uses single-nucleotide tokens forfine-grained resolution, boosting both efficiency and interpretability.Pre-trained on the human genome, NucEL achieves state-of-the-art results ondiverse downstream tasks -- regulatory element identification (e.g., promoters,enhancers), transcription factor binding prediction, open chromatinclassification, and histone modification profiling -- surpassing similarlysized MLM-based models and rivaling models 25x larger, such as NT. Ablationstudies highlight optimal tokenization and masking strategies for ELECTRA-styleDNA pre-training. Attention analysis reveals NucEL's superior capture ofbiologically relevant motifs compared to NT, providing insights intohierarchical learning and regulatory element modeling. These findingsdemonstrate ELECTRA-style pre-training as an efficient, effective strategy forgenomic representation learning with broad implications for genomic research.</description>
      <author>example@mail.com (Ke Ding, Brian Parker, Jiayu Wen)</author>
      <guid isPermaLink="false">2508.13191v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference</title>
      <link>http://arxiv.org/abs/2508.13439v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 10 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种结构化提示和知识蒸馏框架，通过协调两个大型视觉语言模型(GPT-4o和o3-mini)生成高质量交通场景注释，并成功将知识蒸馏到轻量级模型VISTA中，实现了在低分辨率视频上的场景理解和风险推断。&lt;h4&gt;背景&lt;/h4&gt;高速公路场景理解和稳健的交通风险推断对智能交通系统和自动驾驶至关重要，但传统方法在可扩展性和泛化能力方面存在局限，特别是在真实世界的复杂动态条件下。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统方法的局限性，作者引入一个新颖框架，能够自动生成高质量的交通场景注释和上下文风险评估。&lt;h4&gt;方法&lt;/h4&gt;框架通过结构化思维链策略协调两个大型视觉语言模型(GPT-4o和o3-mini)生成丰富多角度输出，这些输出作为知识丰富的伪注释用于监督微调更小的学生VLM，最终得到紧凑型3B规模模型VISTA。&lt;h4&gt;主要发现&lt;/h4&gt;VISTA模型能够理解低分辨率交通视频并生成语义忠实、风险感知的标题，尽管参数显著减少，但在BLEU-4、METEOR、ROUGE-L和CIDEr等指标上表现出色，证明轻量级模型也能捕获复杂推理能力。&lt;h4&gt;结论&lt;/h4&gt;有效的知识蒸馏和结构化多代理监督能够使轻量级VLMs具备复杂推理能力，VISTA的紧凑架构便于在边缘设备上高效部署，实现实时风险监控而无需大量基础设施升级。&lt;h4&gt;翻译&lt;/h4&gt;综合的高速公路场景理解和稳健的交通风险推断对于推进智能交通系统和自动驾驶至关重要。传统方法通常难以扩展和泛化，特别是在真实世界环境的复杂动态条件下。为应对这些挑战，我们引入了一种新颖的结构化提示和知识蒸馏框架，能够自动生成高质量的交通场景注释和上下文风险评估。我们的框架通过结构化的思维链策略协调两个大型视觉语言模型(GPT-4o和o3-mini)，生成丰富、多角度的输出。这些输出作为知识丰富的伪注释，用于监督微调一个更小的学生VLM。得到的紧凑型3B规模模型VISTA（智能场景和交通分析视觉模型）能够理解低分辨率交通视频并生成语义忠实、风险感知的标题。尽管参数显著减少，VISTA在既定的标题指标（BLEU-4、METEOR、ROUGE-L和CIDEr）上与其教师模型相比表现出色。这表明有效的知识蒸馏和结构化多代理监督能够使轻量级VLMs捕获复杂的推理能力。VISTA的紧凑架构便于在边缘设备上高效部署，无需大量基础设施升级即可实现实时风险监控。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehensive highway scene understanding and robust traffic risk inferenceare vital for advancing Intelligent Transportation Systems (ITS) and autonomousdriving. Traditional approaches often struggle with scalability andgeneralization, particularly under the complex and dynamic conditions ofreal-world environments. To address these challenges, we introduce a novelstructured prompting and knowledge distillation framework that enablesautomatic generation of high-quality traffic scene annotations and contextualrisk assessments. Our framework orchestrates two large Vision-Language Models(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategyto produce rich, multi-perspective outputs. These outputs serve asknowledge-enriched pseudo-annotations for supervised fine-tuning of a muchsmaller student VLM. The resulting compact 3B-scale model, named VISTA (Visionfor Intelligent Scene and Traffic Analysis), is capable of understandinglow-resolution traffic videos and generating semantically faithful, risk-awarecaptions. Despite its significantly reduced parameter count, VISTA achievesstrong performance across established captioning metrics (BLEU-4, METEOR,ROUGE-L, and CIDEr) when benchmarked against its teacher models. Thisdemonstrates that effective knowledge distillation and structured multi-agentsupervision can empower lightweight VLMs to capture complex reasoningcapabilities. The compact architecture of VISTA facilitates efficientdeployment on edge devices, enabling real-time risk monitoring withoutrequiring extensive infrastructure upgrades.</description>
      <author>example@mail.com (Yunxiang Yang, Ningning Xu, Jidong J. Yang)</author>
      <guid isPermaLink="false">2508.13439v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism</title>
      <link>http://arxiv.org/abs/2508.13228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2025 International Joint Conference on Neural Networks (IJCNN 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PreSem-Surf，一种基于神经辐射场(NeRF)框架的优化方法，能从RGB-D序列中快速重建高质量场景表面&lt;h4&gt;背景&lt;/h4&gt;现有NeRF框架在场景重建方面存在时间或质量问题，需要改进方法&lt;h4&gt;目的&lt;/h4&gt;开发能快速重建高质量场景表面的方法，整合RGB、深度和语义信息提高重建性能&lt;h4&gt;方法&lt;/h4&gt;结合SG-MLP采样结构与PR-MLP进行体素预渲染，采用渐进式语义建模提取不同精度语义信息，整合多源信息提升重建效果&lt;h4&gt;主要发现&lt;/h4&gt;在七个合成场景和六个评估指标上，PreSem-Surf在C-L1、F-score和IoU方面表现最佳，在NC、Accuracy和Completeness方面保持竞争力&lt;h4&gt;结论&lt;/h4&gt;PreSem-Surf方法有效且具有实际应用价值，能快速重建高质量场景表面&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了PreSem-Surf，一种基于神经辐射场(NeRF)框架的优化方法，能够从RGB-D序列中快速重建高质量的场景表面。该方法整合RGB、深度和语义信息以提高重建性能。具体而言，引入了一种结合PR-MLP(预调节多层感知器)的新型SG-MLP采样结构用于体素预渲染，使模型能够更早更好地捕获场景相关信息，并更好地区分噪声和局部细节。此外，采用渐进式语义建模以不断提高精度提取语义信息，同时减少训练时间并增强场景理解。在七个合成场景和六个评估指标上的实验表明，PreSem-Surf在C-L1、F-score和IoU方面取得了最佳性能，同时在NC、Accuracy和Completeness方面保持了具有竞争力的结果，证明了其有效性和实际适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从RGB-D序列中快速重建高质量表面的问题，特别是在NeRF框架下平衡重建的平滑度和准确性。这个问题在现实中非常重要，因为随着3D场景理解和虚拟环境可视化需求的增长，传统3D重建方法存在精度有限、对输入质量敏感和资源消耗高等问题，而NeRF虽然表现优异但仍面临对高质量数据依赖和高资源消耗的挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计中借鉴了多个现有工作，包括NeRF的基本框架、3DGS的实时渲染特性、SDF的表示方法，以及MSeg3D、SNI-SLAM和Kimera等模型中利用语义信息的策略。作者针对NeRF的局限性，通过结合RGB信息、深度信息和语义信息，设计了SG-MLP结构和渐进式语义建模策略，以解决高质量数据依赖和高资源消耗的问题，同时提高采样效率和渲染质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合RGB信息、深度信息和语义信息，在较短时间内重建高质量表面，并利用分层预渲染和渐进式语义建模来提高效率和质量。整体实现流程包括：1) SG-MLP结构使用PR MLP进行均匀预采样，然后采用分层渐进采样策略，通过动态阈值选择关键区域；2) 渐进式语义建模策略分为粗粒度渲染(快速捕捉场景整体结构)和细粒度渲染(捕捉细节)；3) 使用综合损失函数联合优化渲染质量、几何表示和语义信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出SG-MLP结构，通过分层预渲染机制为后续渲染提供关键指导；2) 设计渐进式语义建模策略，根据场景语义逐步细化建模过程；3) 整合RGB、深度和语义信息，提高重建质量。相比之前的工作，PreSem-Surf在保持高质量重建的同时有效减少了训练时间，更好地平衡了重建的平滑度与精度，并且对低质量数据有更好的适应能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PreSem-Surf通过创新的SG-MLP结构和渐进式语义建模策略，有效整合RGB、深度和语义信息，在显著减少训练时间的同时实现了高质量的3D场景表面重建，平衡了重建的平滑度与精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes PreSem-Surf, an optimized method based on the NeuralRadiance Field (NeRF) framework, capable of reconstructing high-quality scenesurfaces from RGB-D sequences in a short time. The method integrates RGB,depth, and semantic information to improve reconstruction performance.Specifically, a novel SG-MLP sampling structure combined with PR-MLP(Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering,allowing the model to capture scene-related information earlier and betterdistinguish noise from local details. Furthermore, progressive semanticmodeling is adopted to extract semantic information at increasing levels ofprecision, reducing training time while enhancing scene understanding.Experiments on seven synthetic scenes with six evaluation metrics show thatPreSem-Surf achieves the best performance in C-L1, F-score, and IoU, whilemaintaining competitive results in NC, Accuracy, and Completeness,demonstrating its effectiveness and practical applicability.</description>
      <author>example@mail.com (Yuyan Ye, Hang Xu, Yanghang Huang, Jiali Huang, Qian Weng)</author>
      <guid isPermaLink="false">2508.13228v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2508.05064v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;高斯飞溅是一种新兴的实时3D场景表示技术，结合大型语言模型后为文本条件生成和语义理解开辟了新可能性，本文对此交叉领域进行了全面综述。&lt;h4&gt;背景&lt;/h4&gt;高斯飞溅作为神经辐射场的高效替代方案，能高保真度渲染复杂场景，已在场景重建、机器人和交互式内容创作等领域取得应用，但语言引导与高斯飞溅结合的研究缺乏全面概述。&lt;h4&gt;目的&lt;/h4&gt;提供结合语言引导与3D高斯飞溅研究的结构化综述，介绍理论基础、集成策略和实际用例，指出限制和未来挑战。&lt;h4&gt;方法&lt;/h4&gt;对当前结合语言引导和3D高斯飞溅的研究工作进行结构化回顾，分析理论基础、集成策略和实际应用案例。&lt;h4&gt;主要发现&lt;/h4&gt;语言引导的高斯飞溅为文本条件生成、编辑和语义场景理解提供了新可能性；当前面临计算瓶颈、泛化能力有限和语义注释数据稀缺等挑战。&lt;h4&gt;结论&lt;/h4&gt;语言引导的3D高斯飞溅是一个新兴且有前景的研究领域，需要进一步解决计算效率、泛化能力和数据可用性问题，以推动3D场景理解的进步。&lt;h4&gt;翻译&lt;/h4&gt;高斯飞溅已成为实时3D场景表示的革命性技术，提供了比神经辐射场更高效且表现力强的替代方案。它能够高保真度渲染复杂场景，推动了场景重建、机器人和交互式内容创作等领域的进展。最近，大型语言模型和语言嵌入被整合到高斯飞溅流程中，为文本条件生成、编辑和语义场景理解开辟了新可能性。尽管有这些进展，对于这一新兴交叉领域的全面概述仍然缺乏。本综述对当前结合语言引导与3D高斯飞溅的研究工作进行了结构化回顾，详细介绍了理论基础、集成策略和实际用例。我们强调了关键限制，如计算瓶颈、泛化能力有限，以及语义注释的3D高斯数据稀缺等问题，并概述了使用高斯飞溅推进语言引导的3D场景理解的开放挑战和未来方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将语言嵌入技术与3D高斯泼溅(3DGS)有效结合，以实现更高级的3D场景理解和交互。这个问题非常重要，因为3D场景理解是机器人、自动驾驶和虚拟现实等领域的核心技术，而现有的3D表示方法缺乏与自然语言的交互能力。将语言模型与3D场景结合可以创建能理解自然语言描述、回答问题并执行语言指导操作的智能系统，对于未来需要与3D环境交互的AI系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D高斯泼溅在实时渲染方面优于NeRF但缺乏语义理解能力，然后借鉴了LLMs和VLMs在2D任务中的成功应用，探索将其扩展到3D场景。作者对现有研究进行了系统综述，将方法分为静态和动态场景两大类，并提出了语言特征量化、语义嵌入、多尺度语义推理等创新方法。作者确实借鉴了大量现有工作，包括3D高斯泼溅技术、从Word2Vec到BERT的自然语言处理技术、CLIP等对比学习模型，以及现有的多模态学习框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将自然语言理解能力与3D场景表示技术相结合，创建能够通过自然语言进行交互和理解的3D场景系统。整体流程包括：1)使用3D高斯泼溅表示场景；2)用预训练语言模型提取语言特征；3)通过各种方法将语言特征与3D高斯对齐（如特征量化、直接嵌入、自编码器压缩、网格映射）；4)通过监督学习优化语言-3D对齐关系；5)实现自然语言查询与场景交互；6)结合语言和几何信息渲染3D场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)语言嵌入与3DGS的系统整合；2)多种技术路线的分类与比较；3)语言特征压缩与优化方法；4)多尺度语义推理能力；5)训练-free方法。相比之前工作，这篇论文的不同之处在于：1)提供了全面的综述而非专注特定方法；2)强调实际应用场景；3)整合了最新的LLMs、VLMs和MLLMs技术；4)探讨了动态场景处理；5)特别关注性能与效率的平衡，支持实时交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统性地综述了将语言嵌入技术与3D高斯泼溅相结合的方法，为创建能够通过自然语言进行交互和理解的智能3D场景系统提供了全面的理论框架和实践指南，推动了3D场景理解从单纯的几何表示向语义丰富的交互式系统的转变。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gaussian Splatting has rapidly emerged as a transformative technique forreal-time 3D scene representation, offering a highly efficient and expressivealternative to Neural Radiance Fields (NeRF). Its ability to render complexscenes with high fidelity has enabled progress across domains such as scenereconstruction, robotics, and interactive content creation. More recently, theintegration of Large Language Models (LLMs) and language embeddings intoGaussian Splatting pipelines has opened new possibilities for text-conditionedgeneration, editing, and semantic scene understanding. Despite these advances,a comprehensive overview of this emerging intersection has been lacking. Thissurvey presents a structured review of current research efforts that combinelanguage guidance with 3D Gaussian Splatting, detailing theoreticalfoundations, integration strategies, and real-world use cases. We highlight keylimitations such as computational bottlenecks, generalizability, and thescarcity of semantically annotated 3D Gaussian data and outline open challengesand future directions for advancing language-guided 3D scene understandingusing Gaussian Splatting.</description>
      <author>example@mail.com (Mahmoud Chick Zaouali, Todd Charter, Yehor Karpichev, Brandon Haworth, Homayoun Najjaran)</author>
      <guid isPermaLink="false">2508.05064v2</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Backdooring Self-Supervised Contrastive Learning by Noisy Alignment</title>
      <link>http://arxiv.org/abs/2508.14015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Noisy Alignment（NA）的数据投毒对比学习（DPCL）方法，通过抑制有毒图像中的噪声成分来防御后门攻击，达到了最先进的性能同时保持干净数据的准确性。&lt;h4&gt;背景&lt;/h4&gt;自监督对比学习（CL）能有效从未标记的数据中学习可迁移表示，但这种方法容易受到数据投毒后门攻击（DPCLs）的影响，攻击者可以在预训练数据集中注入有毒图像，导致编码器在下游任务中表现出有针对性的错误行为。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的DPCL方法，明确抑制有毒图像中的噪声成分，以防御后门攻击。&lt;h4&gt;方法&lt;/h4&gt;受强大的训练可控CL攻击启发，识别并提取噪声对齐的关键目标，将其应用于数据投毒场景；通过策略性地操作对比学习的随机裁剪机制实现噪声对齐，将此过程制定为具有理论推导最优参数的图像布局优化问题。&lt;h4&gt;主要发现&lt;/h4&gt;Noisy Alignment方法简单但有效，与现有DPCLs相比达到最先进性能，同时保持干净数据的准确性，并且对常见的后门防御具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Noisy Alignment是一种有效的DPCL防御方法，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;自监督对比学习（CL）能有效从未标记的数据中学习可迁移表示，这些数据包含图像或图像文本对，但它容易受到数据投毒后门攻击（DPCLs）的影响。攻击者可以在预训练数据集中注入有毒图像，导致被破坏的CL编码器在下游任务中表现出有针对性的错误行为。然而，现有的DPCLs由于其依赖于后门和目标对象之间脆弱的隐式共现关系，以及对有毒图像中判别性特征的抑制不足，因此效果有限。我们提出了Noisy Alignment（NA），一种DPCL方法，明确抑制有毒图像中的噪声成分。受强大的训练可控CL攻击启发，我们识别并提取了噪声对齐的关键目标，并将其有效应用于数据投毒场景。我们的方法通过策略性地操作对比学习的随机裁剪机制来实现噪声对齐，将此过程制定为具有理论推导最优参数的图像布局优化问题。得到的方法简单而有效，与现有DPCLs相比达到了最先进的性能，同时保持了干净数据的准确性。此外，Noisy Alignment对常见的后门防御具有鲁棒性。代码可在https://github.com/jsrdcht/Noisy-Alignment找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised contrastive learning (CL) effectively learns transferablerepresentations from unlabeled data containing images or image-text pairs butsuffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversarycan inject poisoned images into pretraining datasets, causing compromised CLencoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs,however, achieve limited efficacy due to their dependence on fragile implicitco-occurrence between backdoor and target object and inadequate suppression ofdiscriminative features in backdoored images. We propose Noisy Alignment (NA),a DPCL method that explicitly suppresses noise components in poisoned images.Inspired by powerful training-controllable CL attacks, we identify and extractthe critical objective of noisy alignment, adapting it effectively intodata-poisoning scenarios. Our method implements noisy alignment bystrategically manipulating contrastive learning's random cropping mechanism,formulating this process as an image layout optimization problem withtheoretically derived optimal parameters. The resulting method is simple yeteffective, achieving state-of-the-art performance compared to existing DPCLs,while maintaining clean-data accuracy. Furthermore, Noisy Alignmentdemonstrates robustness against common backdoor defenses. Codes can be found athttps://github.com/jsrdcht/Noisy-Alignment.</description>
      <author>example@mail.com (Tuo Chen, Jie Gui, Minjing Dong, Ju Jia, Lanting Fang, Jian Liu)</author>
      <guid isPermaLink="false">2508.14015v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.13712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DCMamba的新型半监督医学图像分割框架，通过从数据、网络和特征三个维度增强多样性，显著提升了分割性能。&lt;h4&gt;背景&lt;/h4&gt;医学图像分割的高质量标注数据获取繁琐且昂贵，半监督分割技术可通过利用未标记数据生成伪标签来减轻这一负担，而Mamba等先进状态空间模型在处理长程依赖关系方面表现出色。&lt;h4&gt;目的&lt;/h4&gt;探索Mamba在半监督医学图像分割中的潜力，提出一个增强多样性协作Mamba框架以提升分割性能。&lt;h4&gt;方法&lt;/h4&gt;从数据角度开发基于Mamba扫描特性的补丁级别弱-强混合增强；从网络角度引入多样扫描协作模块，利用不同扫描方向的预测差异；从特征角度采用不确定性加权对比学习机制增强特征表示多样性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明DCMamba显著优于其他半监督医学图像分割方法，在Synapse数据集上使用20%标记数据时，比最新的基于SSM的方法提高了6.69%。&lt;h4&gt;结论&lt;/h4&gt;DCMamba框架通过多维度增强多样性，有效提升了半监督医学图像分割的性能，为解决标注数据稀缺问题提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;获取高质量的医学图像分割标注数据是繁琐且昂贵的。半监督分割技术通过利用未标记数据生成伪标签来减轻这一负担。最近，以Mamba为代表的先进状态空间模型已显示出有效处理长程依赖关系的能力。这促使我们探索它们在半监督医学图像分割中的潜力。在本文中，我们提出了一种新的增强多样性协作Mamba框架（即DCMamba），用于半监督医学图像分割，该框架从数据、网络和特征角度探索和利用多样性。首先，从数据角度，我们开发了基于Mamba扫描建模特性的补丁级别弱-强混合增强。此外，从网络角度，我们引入了一个多样扫描协作模块，该模块可以受益于不同扫描方向产生的预测差异。此外，从特征角度，我们采用不确定性加权对比学习机制来增强特征表示的多样性。实验证明，我们的DCMamba显著优于其他半监督医学图像分割方法，例如，在使用20%标记数据的Synapse数据集上，比最新的基于SSM的方法提高了6.69%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Acquiring high-quality annotated data for medical image segmentation istedious and costly. Semi-supervised segmentation techniques alleviate thisburden by leveraging unlabeled data to generate pseudo labels. Recently,advanced state space models, represented by Mamba, have shown efficienthandling of long-range dependencies. This drives us to explore their potentialin semi-supervised medical image segmentation. In this paper, we propose anovel Diversity-enhanced Collaborative Mamba framework (namely DCMamba) forsemi-supervised medical image segmentation, which explores and utilizes thediversity from data, network, and feature perspectives. Firstly, from the dataperspective, we develop patch-level weak-strong mixing augmentation withMamba's scanning modeling characteristics. Moreover, from the networkperspective, we introduce a diverse-scan collaboration module, which couldbenefit from the prediction discrepancies arising from different scanningdirections. Furthermore, from the feature perspective, we adopt anuncertainty-weighted contrastive learning mechanism to enhance the diversity offeature representation. Experiments demonstrate that our DCMamba significantlyoutperforms other semi-supervised medical image segmentation methods, e.g.,yielding the latest SSM-based method by 6.69% on the Synapse dataset with 20%labeled data.</description>
      <author>example@mail.com (Shumeng Li, Jian Zhang, Lei Qi, Luping Zhou, Yinghuan Shi, Yang Gao)</author>
      <guid isPermaLink="false">2508.13712v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model</title>
      <link>http://arxiv.org/abs/2508.13676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MHSNet的多层级身份验证框架，用于解决第三方简历与公司人才库简历间的重复检测问题。&lt;h4&gt;背景&lt;/h4&gt;招聘人员需要从第三方网站（如LinkedIn、Indeed）持续搜索简历来维持公司人才库，但这些获取的简历往往不完整且不准确。&lt;h4&gt;目的&lt;/h4&gt;提高第三方简历质量，丰富公司人才库，通过在获取的简历与公司现有人才库中的简历之间进行重复检测来实现。&lt;h4&gt;方法&lt;/h4&gt;提出MHSNet框架，使用对比学习微调BGE-M3模型，利用微调后的Mixture-of-Experts（MoE）生成简历的多级稀疏和密集表示，并计算相应的多级语义相似性。同时采用状态感知的MoE来处理各种不完整的简历。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果验证了MHSNet在处理简历重复检测问题上的有效性。&lt;h4&gt;结论&lt;/h4&gt;MHSNet能够有效解决简历文本的语义复杂性、结构异质性和信息不完整性带来的重复检测挑战，提高第三方简历质量。&lt;h4&gt;翻译&lt;/h4&gt;为了维持公司的人才库，招聘人员需要持续从第三方网站（如LinkedIn、Indeed）搜索简历。然而，获取的简历往往不完整且不准确。为了提高第三方简历质量并丰富公司人才库，有必要在获取的简历与公司人才库中已有的简历之间进行重复检测。由于简历文本的语义复杂性、结构异质性和信息不完整性，此类重复检测具有挑战性。为此，我们提出了MHSNet，一个多层级身份验证框架，使用对比学习微调BGE-M3。微调后的Mixture-of-Experts（MoE）为简历生成多级稀疏和密集表示，使计算相应的多级语义相似性成为可能。此外，MHSNet中采用了状态感知的Mixture-of-Experts（MoE）来处理各种不完整的简历。实验结果验证了MHSNet的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To maintain the company's talent pool, recruiters need to continuously searchfor resumes from third-party websites (e.g., LinkedIn, Indeed). However,fetched resumes are often incomplete and inaccurate. To improve the quality ofthird-party resumes and enrich the company's talent pool, it is essential toconduct duplication detection between the fetched resumes and those already inthe company's talent pool. Such duplication detection is challenging due to thesemantic complexity, structural heterogeneity, and information incompletenessof resume texts. To this end, we propose MHSNet, an multi-level identityverification framework that fine-tunes BGE-M3 using contrastive learning. Withthe fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse anddense representations for resumes, enabling the computation of correspondingmulti-level semantic similarities. Moreover, the state-aware Mixture-of-Experts(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimentalresults verify the effectiveness of MHSNet</description>
      <author>example@mail.com (Yu Li, Zulong Chen, Wenjian Xu, Hong Wen, Yipeng Yu, Man Lung Yiu, Yuyu Yin)</author>
      <guid isPermaLink="false">2508.13676v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>A Generalized Learning Framework for Self-Supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.13596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种广义学习框架（GLF），统一了现有的自监督对比学习方法，并引入了自适应分布校准（ADC）方法来提高特征空间的质量。&lt;h4&gt;背景&lt;/h4&gt;自监督对比学习（SSCL）在多个下游任务中表现出优越性，但现有方法缺乏统一的框架来理解和改进。&lt;h4&gt;目的&lt;/h4&gt;将标准SSCL方法推广到广义学习框架（GLF）中，并设计一种有效的约束部分来提高特征空间的质量。&lt;h4&gt;方法&lt;/h4&gt;分析了BYOL、Barlow Twins和SwAV三种现有SSCL方法，提出包含对齐部分和约束部分的GLF框架。通过迭代捕捉锚点与其他样本之间的动态关系，设计了自适应分布校准（ADC）方法，确保在原始输入空间中靠近或远离锚点的样本在特征空间中也相应地靠近或远离锚点。&lt;h4&gt;主要发现&lt;/h4&gt;设计GLF的约束部分时需要考虑两个洞见：类内紧凑性和类间可分性，它们衡量特征空间如何保留输入的类别信息。ADC方法能够有效提高特征空间的这些性质。&lt;h4&gt;结论&lt;/h4&gt;理论分析和实证评估都证明了ADC方法的优越性，它是一种即插即用的方法，能够有效地提高自监督对比学习的效果。&lt;h4&gt;翻译&lt;/h4&gt;自监督对比学习（SSCL）最近在多个下游任务中表现出优越性。在本文中，我们将标准的SSCL方法推广到一个包含对齐部分和约束部分的广义学习框架（GLF）中。我们分析了三种现有的SSCL方法：BYOL、Barlow Twins和SwAV，并展示了它们可以通过不同的约束部分选择在GLF下得到统一。我们进一步提出了经验分析和理论分析，为设计GLF的约束部分提供了两个洞见：类内紧凑性和类间可分性，它们衡量特征空间如何保留输入的类别信息。然而，由于SSCL无法使用标签，设计满足这些性质的约束部分具有挑战性。为了解决这个问题，我们考虑通过迭代捕捉锚点与其他样本之间的动态关系来诱导类内紧凑性和类间可分性，并提出了一种即插即用的方法称为自适应分布校准（ADC），确保在原始输入空间中靠近或远离锚点的样本在特征空间中也相应地靠近或远离锚点。理论分析和实证评估都证明了ADC的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised contrastive learning (SSCL) has recently demonstratedsuperiority in multiple downstream tasks. In this paper, we generalize thestandard SSCL methods to a Generalized Learning Framework (GLF) consisting oftwo parts: the aligning part and the constraining part. We analyze threeexisting SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can beunified under GLF with different choices of the constraining part. We furtherpropose empirical and theoretical analyses providing two insights intodesigning the constraining part of GLF: intra-class compactness and inter-classseparability, which measure how well the feature space preserves the classinformation of the inputs. However, since SSCL can not use labels, it ischallenging to design a constraining part that satisfies these properties. Toaddress this issue, we consider inducing intra-class compactness andinter-class separability by iteratively capturing the dynamic relationshipbetween anchor and other samples and propose a plug-and-play method calledAdaptive Distribution Calibration (ADC) to ensure that samples that are near orfar from the anchor point in the original input space are closer or furtheraway from the anchor point in the feature space. Both the theoretical analysisand the empirical evaluation demonstrate the superiority of ADC.</description>
      <author>example@mail.com (Lingyu Si, Jingyao Wang, Wenwen Qiang)</author>
      <guid isPermaLink="false">2508.13596v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models</title>
      <link>http://arxiv.org/abs/2508.13507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究解决了羽毛球双打分析研究不足的问题，通过将单打训练的模型扩展到双打分析，使用ViT-Pose提取关键点，ST-GCN对比学习框架嵌入，自定义多目标跟踪算法提高跟踪稳定性，以及基于Transformer的分类器确定击球发生，证明了基于姿势的击球识别扩展到双打羽毛球的可行性。&lt;h4&gt;背景&lt;/h4&gt;羽毛球是世界上速度最快的球拍运动之一，国际比赛中双打比赛比单打更普遍，但之前的研究主要关注单打，因为数据可用性和多人跟踪的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决双打分析研究不足的问题，将单打训练的模型扩展到双打分析。&lt;h4&gt;方法&lt;/h4&gt;使用ViT-Pose从ShuttleSet单场比赛数据集中提取关键点，通过基于ST-GCN的对比学习框架嵌入这些关键点，加入自定义的多目标跟踪算法解决ID切换问题，使用基于Transformer的分类器确定击球发生。&lt;h4&gt;主要发现&lt;/h4&gt;证明了基于姿势的击球识别扩展到双打羽毛球的可行性，扩展了分析能力。&lt;h4&gt;结论&lt;/h4&gt;为双打特定数据集奠定了基础，有助于增强对这种主要但研究不足的快速球拍运动形式的理解。&lt;h4&gt;翻译&lt;/h4&gt;羽毛球被誉为世界上速度最快的球拍运动之一。尽管国际比赛中双打比赛比单打更为普遍，但之前的研究主要关注单打，这是由于数据可用性和多人跟踪的挑战。为了解决这一空白，我们设计了一种将单打训练的模型转换到双打分析的方法。我们使用ViT-Pose从ShuttleSet单场比赛数据集中提取关键点，并通过基于ST-GCN的对比学习框架嵌入它们。为了提高跟踪稳定性，我们融入了一个自定义的多目标跟踪算法，解决了快速和重叠球员移动导致的ID切换问题。然后，基于Transformer的分类器根据学习的嵌入确定击球发生。我们的发现证明了将基于姿势的击球识别扩展到双打羽毛球的可行性，扩展了分析能力。这项工作为双打特定数据集奠定了基础，以增强对这种主要但研究不足的快速球拍运动形式的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Badminton is known as one of the fastest racket sports in the world. Despitedoubles matches being more prevalent in international tournaments than singles,previous research has mainly focused on singles due to the challenges in dataavailability and multi-person tracking. To address this gap, we designed anapproach that transfers singles-trained models to doubles analysis. Weextracted keypoints from the ShuttleSet single matches dataset using ViT-Poseand embedded them through a contrastive learning framework based on ST-GCN. Toimprove tracking stability, we incorporated a custom multi-object trackingalgorithm that resolves ID switching issues from fast and overlapping playermovements. A Transformer-based classifier then determines shot occurrencesbased on the learned embeddings. Our findings demonstrate the feasibility ofextending pose-based shot recognition to doubles badminton, broadeninganalytics capabilities. This work establishes a foundation for doubles-specificdatasets to enhance understanding of this predominant yet understudied formatof the fast racket sport.</description>
      <author>example@mail.com (Seungheon Baek, Jinhyuk Yun)</author>
      <guid isPermaLink="false">2508.13507v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification</title>
      <link>http://arxiv.org/abs/2508.13452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures, accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HCAL的分类器，通过集成原型对比学习和自适应任务权重机制，解决了层次多标签分类中的结构一致性和多任务学习中的损失权重平衡问题。&lt;h4&gt;背景&lt;/h4&gt;层次多标签分类（HMC）在保持结构一致性和平衡损失权重方面面临关键挑战，传统多任务学习（MTL）方法存在'一个强任务多个弱任务'的优化偏差问题。&lt;h4&gt;目的&lt;/h4&gt;解决HMC中的结构一致性和损失权重平衡问题，以及MTL中的优化偏差问题，提高分类精度并降低层次违规率。&lt;h4&gt;方法&lt;/h4&gt;提出基于MTL的HCAL分类器，集成原型对比学习和自适应任务权重机制，包括语义一致性机制（显式建模标签的原型和从子类到父类的特征聚合）、自适应损失权重机制（通过监控任务特定收敛率动态分配优化资源）、原型扰动机制（通过注入受控噪声扩展决策边界），并提出了层次违规率（HVR）作为评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的实验表明，所提出的HCAL分类器比基线模型具有更高的分类精度和更低的层次违规率。&lt;h4&gt;结论&lt;/h4&gt;HCAL分类器有效解决了HMC中的结构一致性和MTL中的优化偏差问题，通过自适应任务权重机制和原型扰动机制提高了模型的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;层次多标签分类（HMC）在保持结构一致性和平衡多任务学习（MTL）中的损失权重方面面临关键挑战。为了解决这些问题，我们提出了一种名为HCAL的分类器，该分类器基于MTL，集成了原型对比学习和自适应任务权重机制。我们分类器最重要的优势是语义一致性，包括显式建模标签的原型和从子类到父类的特征聚合。另一个重要优势是自适应损失权重机制，它通过监控任务特定的收敛率来动态分配优化资源，有效解决了传统MTL方法中固有的'一个强任务多个弱任务'的优化偏差。为了进一步增强鲁棒性，通过向原型注入受控噪声来制定原型扰动机制以扩展决策边界。此外，我们形式化了一个称为层次违规率（HVR）的定量指标，用于评估层次一致性和泛化能力。在三个数据集上的大量实验表明，与基线模型相比，所提出的分类器具有更高的分类精度和更低的层次违规率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761241&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical Multi-Label Classification (HMC) faces critical challenges inmaintaining structural consistency and balancing loss weighting in Multi-TaskLearning (MTL). In order to address these issues, we propose a classifiercalled HCAL based on MTL integrated with prototype contrastive learning andadaptive task-weighting mechanisms. The most significant advantage of ourclassifier is semantic consistency including both prototype with explicitlymodeling label and feature aggregation from child classes to parent classes.The other important advantage is an adaptive loss-weighting mechanism thatdynamically allocates optimization resources by monitoring task-specificconvergence rates. It effectively resolves the "one-strong-many-weak"optimization bias inherent in traditional MTL approaches. To further enhancerobustness, a prototype perturbation mechanism is formulated by injectingcontrolled noise into prototype to expand decision boundaries. Additionally, weformalize a quantitative metric called Hierarchical Violation Rate (HVR) as toevaluate hierarchical consistency and generalization. Extensive experimentsacross three datasets demonstrate both the higher classification accuracy andreduced hierarchical violation rate of the proposed classifier over baselinemodels.</description>
      <author>example@mail.com (Ruobing Jiang, Mengzhe Liu, Haobing Liu, Yanwei Yu)</author>
      <guid isPermaLink="false">2508.13452v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</title>
      <link>http://arxiv.org/abs/2508.06189v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了MA-CBP框架，一种基于多智能体异步协作的犯罪行为预测方法，能够有效处理实时视频数据并预测潜在犯罪活动。&lt;h4&gt;背景&lt;/h4&gt;城市化加速导致公共场所犯罪行为对社会安全构成严重威胁。传统基于特征识别的异常检测方法难以捕捉高级行为语义，而基于大型语言模型的生成方法无法满足实时性要求。&lt;h4&gt;目的&lt;/h4&gt;解决传统犯罪行为检测方法的局限性，提出一种能够捕捉高级行为语义并满足实时性要求的犯罪行为预测框架。&lt;h4&gt;方法&lt;/h4&gt;提出MA-CBP框架，将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，融合相邻图像帧进行长短期上下文联合推理，并包含事件主体、地点和原因等关键要素。同时构建了提供多尺度语言监督的高质量犯罪行为数据集。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明MA-CBP在多个数据集上取得了优越性能，为城市公共安全场景的风险预警提供了有效解决方案。&lt;h4&gt;结论&lt;/h4&gt;MA-CBP框架能够有效预测潜在的犯罪活动，实现早期预警，对提升城市公共安全具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化进程的加速，公共场所的犯罪行为对社会安全构成日益严重的威胁。基于特征识别的传统异常检测方法难以从历史信息中捕捉高级行为语义，而基于大型语言模型的生成方法通常无法满足实时性要求。为应对这些挑战，我们提出了MA-CBP，一种基于多智能体异步协作的犯罪行为预测框架。该框架将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合相邻图像帧对长短期上下文进行联合推理。生成的行为决策包括事件主体、地点和原因等关键要素，能够对潜在的犯罪活动进行早期预警。此外，我们构建了一个高质量的犯罪行为数据集，提供多尺度语言监督，包括帧级、摘要级和事件级语义标注。实验结果表明，我们的方法在多个数据集上取得了优越性能，为城市公共安全场景的风险预警提供了有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the acceleration of urbanization, criminal behavior in public scenesposes an increasingly serious threat to social security. Traditional anomalydetection methods based on feature recognition struggle to capture high-levelbehavioral semantics from historical information, while generative approachesbased on Large Language Models (LLMs) often fail to meet real-timerequirements. To address these challenges, we propose MA-CBP, a criminalbehavior prediction framework based on multi-agent asynchronous collaboration.This framework transforms real-time video streams into frame-level semanticdescriptions, constructs causally consistent historical summaries, and fusesadjacent image frames to perform joint reasoning over long- and short-termcontexts. The resulting behavioral decisions include key elements such as eventsubjects, locations, and causes, enabling early warning of potential criminalactivity. In addition, we construct a high-quality criminal behavior datasetthat provides multi-scale language supervision, including frame-level,summary-level, and event-level semantic annotations. Experimental resultsdemonstrate that our method achieves superior performance on multiple datasetsand offers a promising solution for risk warning in urban public safetyscenarios.</description>
      <author>example@mail.com (Cheng Liu, Daou Zhang, Tingxu Liu, Yuhan Wang, Jinyang Chen, Yuexuan Li, Xinying Xiao, Chenbo Xin, Ziru Wang, Weichao Wu)</author>
      <guid isPermaLink="false">2508.06189v2</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Representations for Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2508.13113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://princeton-rl.github.io/CRTR/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为CRTR的新方法，用于从同时感知和时序结构的表示中实现时序推理，解决了传统时序对比学习中因依赖伪特征而无法捕获时序结构的问题。&lt;h4&gt;背景&lt;/h4&gt;在经典AI中，感知依赖于学习基于状态的表示，而规划（可视为对动作序列的时序推理）通常通过搜索实现。&lt;h4&gt;目的&lt;/h4&gt;研究是否可以从同时捕捉感知和时序结构的表示中实现时序推理，并解决标准时序对比学习中因依赖伪特征而无法捕获时序结构的问题。&lt;h4&gt;方法&lt;/h4&gt;引入了用于时序推理的组合表示（CRTR），该方法使用负采样方案来证明性地移除这些伪特征并促进时序推理。&lt;h4&gt;主要发现&lt;/h4&gt;CRTR在具有复杂时序结构的领域（如Sokoban和魔方）取得了强有力结果。特别是对于魔方，CRTR学习到的表示可以泛化到所有初始状态，并且使用比最佳优先搜索（BestFS）更少的搜索步骤解决谜题，尽管解决方案更长。&lt;h4&gt;结论&lt;/h4&gt;据我们所知，这是第一种仅使用学习到的表示而无需依赖外部搜索算法就能高效解决任意魔方状态的方法。&lt;h4&gt;翻译&lt;/h4&gt;在经典AI中，感知依赖于学习基于状态的表示，而规划（可视为对动作序列的时序推理）通常通过搜索实现。我们研究是否可以从同时捕捉感知和时序结构的表示中实现此类推理。我们表明，尽管标准时序对比学习很受欢迎，但它常常因依赖伪特征而无法捕获时序结构。为了解决这个问题，我们引入了用于时序推理的组合表示（CRTR），该方法使用负采样方案来证明性地移除这些伪特征并促进时序推理。CRTR在具有复杂时序结构的领域（如Sokoban和魔方）取得了强有力结果。特别是对于魔方，CRTR学习到的表示可以泛化到所有初始状态，并允许它使用比最佳优先搜索（BestFS）更少的搜索步骤解决谜题，尽管解决方案更长。据我们所知，这是第一种仅使用学习到的表示而无需依赖外部搜索算法就能高效解决任意魔方状态的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In classical AI, perception relies on learning state-based representations,while planning, which can be thought of as temporal reasoning over actionsequences, is typically achieved through search. We study whether suchreasoning can instead emerge from representations that capture both perceptualand temporal structure. We show that standard temporal contrastive learning,despite its popularity, often fails to capture temporal structure due to itsreliance on spurious features. To address this, we introduce CombinatorialRepresentations for Temporal Reasoning (CRTR), a method that uses a negativesampling scheme to provably remove these spurious features and facilitatetemporal reasoning. CRTR achieves strong results on domains with complextemporal structure, such as Sokoban and Rubik's Cube. In particular, for theRubik's Cube, CRTR learns representations that generalize across all initialstates and allow it to solve the puzzle using fewer search steps than BestFS,though with longer solutions. To our knowledge, this is the first method thatefficiently solves arbitrary Cube states using only learned representations,without relying on an external search algorithm.</description>
      <author>example@mail.com (Alicja Ziarko, Michal Bortkiewicz, Michal Zawalski, Benjamin Eysenbach, Piotr Milos)</author>
      <guid isPermaLink="false">2508.13113v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
  <item>
      <title>EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding</title>
      <link>http://arxiv.org/abs/2508.12687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了EgoIllusion，这是首个用于评估多模态大语言模型在自我中心视频中产生幻觉的基准测试，包含1400个视频和8000个人类标注的问题。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在复杂多模态任务中表现出色，特别是在第三人称和自我中心视频的视觉感知与推理方面，但容易产生幻觉，生成连贯但不准确的响应。&lt;h4&gt;目的&lt;/h4&gt;开发一个基准测试来评估MLLM在自我中心视频中的幻觉问题，促进更强大的自我中心MLLM发展，减少幻觉率。&lt;h4&gt;方法&lt;/h4&gt;创建了EgoIllusion基准，包含1400个视频和8000个人类标注的开放式和封闭式问题，旨在触发自我中心视频中视觉和听觉线索的幻觉，并评估了十个MLLM模型。&lt;h4&gt;主要发现&lt;/h4&gt;十个被评估的MLLM模型在EgoIllusion基准上表现不佳，即使是GPT-4o和Gemini等强大模型也仅达到59%的准确率，表明这些模型在处理自我中心视频时存在显著挑战。&lt;h4&gt;结论&lt;/h4&gt;EgoIllusion为开发评估MLLM有效性的稳健基准奠定了基础，并促进开发幻觉率降低的更好的自我中心MLLM，该基准将被开源以确保可复现性。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在复杂的多模态任务中表现出色。虽然MLLM在第三人称和自我中心视频的视觉感知和推理方面表现出色，但它们容易产生幻觉，生成连贯但不准确的响应。我们提出了EgoIllusion，这是第一个用于评估自我中心视频中MLLM幻觉的基准。EgoIllusion包含1400个视频和8000个人类标注的开放式和封闭式问题，旨在触发自我中心视频中视觉和听觉线索的幻觉。对十个MLLM的评估揭示了重大挑战，包括GPT-4o和Gemini等强大模型，仅达到59%的准确率。EgoIllusion为开发评估MLLM有效性的稳健基准奠定了基础，并促进开发幻觉率降低的更好的自我中心MLLM。我们的基准将被开源以确保可复现性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated remarkableperformance in complex multimodal tasks. While MLLMs excel at visual perceptionand reasoning in third-person and egocentric videos, they are prone tohallucinations, generating coherent yet inaccurate responses. We presentEgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentricvideos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotatedopen and closed-ended questions designed to trigger hallucinations in bothvisual and auditory cues in egocentric videos. Evaluations across ten MLLMsreveal significant challenges, including powerful models like GPT-4o andGemini, achieving only 59% accuracy. EgoIllusion lays the foundation indeveloping robust benchmarks to evaluate the effectiveness of MLLMs and spursthe development of better egocentric MLLMs with reduced hallucination rates.Our benchmark will be open-sourced for reproducibility.</description>
      <author>example@mail.com (Ashish Seth, Utkarsh Tyagi, Ramaneswaran Selvakumar, Nishit Anand, Sonal Kumar, Sreyan Ghosh, Ramani Duraiswami, Chirag Agarwal, Dinesh Manocha)</author>
      <guid isPermaLink="false">2508.12687v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2508.12282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ChronoQA是一个大规模的中文问答基准数据集，专门用于评估检索增强生成(RAG)系统中的时间推理能力。&lt;h4&gt;背景&lt;/h4&gt;需要评估RAG系统中的时间推理能力，但缺乏专门的中文基准数据集。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模的中文问答基准数据集，专门用于评估RAG系统中的时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;从2019年至2024年间发表的30多万篇新闻文章构建数据集，包含5,176个高质量问题，涵盖绝对、聚合和相对时间类型，包含显性和隐性时间表达。数据集支持单文档和多文档场景，具有全面的结构化注释，并经历了基于规则、基于LLM和人工评估的多阶段验证。&lt;h4&gt;主要发现&lt;/h4&gt;ChronoQA数据集提供了动态、可靠和可扩展的资源，能够支持广泛的时间任务结构化评估，并作为推进时间敏感型检索增强问答系统的强大基准。&lt;h4&gt;结论&lt;/h4&gt;ChronoQA数据集能够为时间敏感型检索增强问答系统的发展提供有力支持。&lt;h4&gt;翻译&lt;/h4&gt;ChronoQA是一个大规模的中文问答基准数据集，专门用于评估检索增强生成(RAG)系统中的时间推理能力。ChronoQA是从2019年至2024年间发表的30多万篇新闻文章构建而成，包含5,176个高质量问题，涵盖绝对、聚合和相对时间类型，包含显性和隐性时间表达。该数据集支持单文档和多文档场景，反映了时间对齐和逻辑一致性的现实需求。ChronoQA具有全面的结构化注释，并经历了基于规则、基于LLM和人工评估的多阶段验证，确保数据质量。通过提供动态、可靠和可扩展的资源，ChronoQA能够在广泛的时间任务中进行结构化评估，并作为推进时间敏感型检索增强问答系统的强大基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ChronoQA, a large-scale benchmark dataset for Chinese questionanswering, specifically designed to evaluate temporal reasoning inRetrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over300,000 news articles published between 2019 and 2024, and contains 5,176high-quality questions covering absolute, aggregate, and relative temporaltypes with both explicit and implicit time expressions. The dataset supportsboth single- and multi-document scenarios, reflecting the real-worldrequirements for temporal alignment and logical consistency. ChronoQA featurescomprehensive structural annotations and has undergone multi-stage validation,including rule-based, LLM-based, and human evaluation, to ensure data quality.By providing a dynamic, reliable, and scalable resource, ChronoQA enablesstructured evaluation across a wide range of temporal tasks, and serves as arobust benchmark for advancing time-sensitive retrieval-augmented questionanswering systems.</description>
      <author>example@mail.com (Ziyang Chen, Erxue Min, Xiang Zhao, Yunxin Li, Xin Jia, Jinzhi Liao, Jichao Li, Shuaiqiang Wang, Baotian Hu, Dawei Yin)</author>
      <guid isPermaLink="false">2508.12282v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting</title>
      <link>http://arxiv.org/abs/2508.11923v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种尺度解耦时空建模框架，用于解决长期交通排放预测中的级联误差放大问题，通过多尺度特征分解与融合提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;长期交通排放预测对城市空气污染综合管理至关重要，但传统时空图建模方法在长期推理过程中容易遭受级联误差放大。&lt;h4&gt;目的&lt;/h4&gt;解决交通排放在时间和空间上的多尺度纠缠导致的级联误差放大问题，提高长期交通排放预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出尺度解耦时空建模框架，利用基于Koopman提升算子的双流特征分解策略将尺度耦合的时空动力学系统提升到无限维线性空间，使用门控小波分解勾勒可预测性边界，并构建包含双流独立性约束的融合机制来优化预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;在西安市二环路道路级交通排放数据集上的实验表明，所提出的模型达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;尺度解耦时空建模框架能有效解决长期交通排放预测中的级联误差放大问题，提高预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;长期交通排放预测对城市空气污染的综合管理至关重要。传统预测方法通常通过挖掘时空依赖关系构建时空图模型来预测排放。然而，由于交通排放在时间和空间上的多尺度纠缠特性，这些时空图建模方法在长期推理过程中容易遭受级联误差放大。为解决这一问题，我们提出了一种用于长期交通排放预测的尺度解耦时空建模框架。它利用多尺度间的可预测性差异来分解和融合不同尺度的特征，同时约束它们保持独立但互补。具体而言，该模型首先引入了基于Koopman提升算子的双流特征分解策略。它通过Koopman算子将尺度耦合的时空动力学系统提升到无限维线性空间，并使用门控小波分解勾勒可预测性边界。然后构建了一种新的融合机制，包含基于交叉项损失的双流独立性约束，以动态优化双流预测结果，抑制相互干扰，并提高长期交通排放预测的准确性。在西安市二环路道路级交通排放数据集上进行的大量实验表明，所提出的模型达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term traffic emission forecasting is crucial for the comprehensivemanagement of urban air pollution. Traditional forecasting methods typicallyconstruct spatiotemporal graph models by mining spatiotemporal dependencies topredict emissions. However, due to the multi-scale entanglement of trafficemissions across time and space, these spatiotemporal graph modeling methodtend to suffer from cascading error amplification during long-term inference.To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling(SDSTM) framework for long-term traffic emission forecasting. It leverages thepredictability differences across multiple scales to decompose and fusefeatures at different scales, while constraining them to remain independent yetcomplementary. Specifically, the model first introduces a dual-stream featuredecomposition strategy based on the Koopman lifting operator. It lifts thescale-coupled spatiotemporal dynamical system into an infinite-dimensionallinear space via Koopman operator, and delineates the predictability boundaryusing gated wavelet decomposition. Then a novel fusion mechanism isconstructed, incorporating a dual-stream independence constraint based oncross-term loss to dynamically refine the dual-stream prediction results,suppress mutual interference, and enhance the accuracy of long-term trafficemission prediction. Extensive experiments conducted on a road-level trafficemission dataset within Xi'an's Second Ring Road demonstrate that the proposedmodel achieves state-of-the-art performance.</description>
      <author>example@mail.com (Yan Wu, Lihong Pei, Yukai Han, Yang Cao, Yu Kang, Yanlong Zhao)</author>
      <guid isPermaLink="false">2508.11923v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Re:Verse -- Can Your VLM Read a Manga?</title>
      <link>http://arxiv.org/abs/2508.08508v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted (oral) at ICCV (AISTORY Workshop) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;当前视觉语言模型在处理顺序视觉叙事时存在表面识别与深层叙事推理之间的关键差距，研究表明这些模型在单个面板解释方面表现出色，但在时间因果性和跨面板连贯性方面系统性失败。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在处理顺序视觉叙事时表现出表面识别与深层叙事推理之间的关键差距，尽管大型多模态模型能够解释单个面板，但它们在连贯故事理解的核心要求上存在系统性失败。&lt;h4&gt;目的&lt;/h4&gt;通过全面研究漫画叙事理解，揭示当前模型在叙事推理方面的局限性，并引入一个新的评估框架来系统性地表征这些缺陷。&lt;h4&gt;方法&lt;/h4&gt;引入一个结合细粒度多模态标注、跨模态嵌入分析和检索增强评估的框架；包括严格的标注协议将视觉元素与叙事结构联系；在多种推理范式进行全面评估；进行跨模态相似性分析；应用框架对Re:Zero漫画的11章308个标注面板进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型缺乏真正的故事级智能，特别难以处理非线性叙事、角色一致性和跨越长序列的因果推理；在生成性叙事、上下文对话定位和时间推理三个核心评估轴上表现不佳。&lt;h4&gt;结论&lt;/h4&gt;这项工作为评估叙事智能建立了基础和实用方法，同时为多模态模型中离散视觉叙事的深度序列理解能力提供了可操作的见解，超越了基本识别。&lt;h4&gt;翻译&lt;/h4&gt;当前的视觉语言模型在处理顺序视觉叙事时，表现出表面识别与深层叙事推理之间的关键差距。通过对漫画叙事理解的全面研究，我们揭示尽管最近的大型多模态模型擅长单个面板解释，但它们在时间因果性和跨面板连贯性方面系统性失败，这些是连贯故事理解的核心要求。我们引入了一个新颖的评估框架，结合细粒度多模态标注、跨模态嵌入分析和检索增强评估，以系统性地表征这些局限性。我们的方法包括(i)一个严格的标注协议，通过对齐的轻小说文本将视觉元素与叙事结构联系起来，(ii)在多种推理范式中的全面评估，包括直接推理和检索增强生成，以及(iii)跨模态相似性分析，揭示当前VLMs联合表示中的基本错位。将此框架应用于Re:Zero漫画的11章308个标注面板，我们通过三个核心评估轴进行了VLMs中长篇叙事理解的首次系统性研究：生成性叙事、上下文对话定位和时间推理。我们的研究结果表明，当前模型缺乏真正的故事级智能，特别难以处理非线性叙事、角色一致性和跨越长序列的因果推理。这项工作为评估叙事智能建立了基础和实用方法，同时为多模态模型中离散视觉叙事的深度序列理解能力提供了可操作的见解，超越了基本识别。项目页面：https://re-verse.vercel.app&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Vision Language Models (VLMs) demonstrate a critical gap betweensurface-level recognition and deep narrative reasoning when processingsequential visual storytelling. Through a comprehensive investigation of manganarrative understanding, we reveal that while recent large multimodal modelsexcel at individual panel interpretation, they systematically fail at temporalcausality and cross-panel cohesion, core requirements for coherent storycomprehension. We introduce a novel evaluation framework that combinesfine-grained multimodal annotation, cross-modal embedding analysis, andretrieval-augmented assessment to systematically characterize theselimitations.  Our methodology includes (i) a rigorous annotation protocol linking visualelements to narrative structure through aligned light novel text, (ii)comprehensive evaluation across multiple reasoning paradigms, including directinference and retrieval-augmented generation, and (iii) cross-modal similarityanalysis revealing fundamental misalignments in current VLMs' jointrepresentations. Applying this framework to Re:Zero manga across 11 chapterswith 308 annotated panels, we conduct the first systematic study of long-formnarrative understanding in VLMs through three core evaluation axes: generativestorytelling, contextual dialogue grounding, and temporal reasoning. Ourfindings demonstrate that current models lack genuine story-level intelligence,struggling particularly with non-linear narratives, character consistency, andcausal inference across extended sequences. This work establishes both thefoundation and practical methodology for evaluating narrative intelligence,while providing actionable insights into the capability of deep sequentialunderstanding of Discrete Visual Narratives beyond basic recognition inMultimodal Models.  Project Page: https://re-verse.vercel.app</description>
      <author>example@mail.com (Aaditya Baranwal, Madhav Kataria, Naitik Agrawal, Yogesh S Rawat, Shruti Vyas)</author>
      <guid isPermaLink="false">2508.08508v3</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation</title>
      <link>http://arxiv.org/abs/2508.13068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种两阶段多模态框架，用于增强基于胸部X光片的疾病分类和区域感知的放射学报告生成，通过整合放射科医生的眼动追踪数据提高分类性能和报告可解释性。&lt;h4&gt;背景&lt;/h4&gt;胸部X光片是疾病诊断的重要工具，但传统方法在疾病分类和报告生成方面存在局限性。MIMIC-Eye数据集提供了包含放射科医生眼动追踪信号的多模态数据，为改进医疗图像分析提供了新机会。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提高疾病分类性能并生成区域感知放射学报告的框架，同时整合放射科医生的注视信息以提高模型的可解释性。&lt;h4&gt;方法&lt;/h4&gt;研究采用两阶段框架：第一阶段使用注视引导的对比学习架构进行疾病分类，整合视觉特征、临床标签、边界框和眼动追踪信号，并采用多项注视注意力损失函数；第二阶段实现模块化报告生成流水线，提取置信度加权的诊断关键词，将其映射到解剖区域，并通过结构化提示生成区域对齐的句子。&lt;h4&gt;主要发现&lt;/h4&gt;整合注视数据显著提高了分类性能，F1分数从0.597提高到0.631（提升5.70%），AUC从0.821提高到0.849（提升3.41%），同时也提高了精确率和召回率。报告生成流水线通过临床关键词召回率和ROUGE重叠度衡量的报告质量有所提高。&lt;h4&gt;结论&lt;/h4&gt;整合注视数据可以同时提高疾病分类性能和生成医疗报告的可解释性，为医学影像分析提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种两阶段多模态框架，可增强基于胸部X光片的疾病分类和区域感知的放射学报告生成，利用MIMIC-Eye数据集。在第一阶段，我们引入了一种注视引导的对比学习架构用于疾病分类。它整合了视觉特征、临床标签、边界框和放射科医生眼动追踪信号，并配备了一种新颖的多项注视注意力损失函数，结合了均方误差、KL散度、相关性和质心对齐。引入注视点使F1分数从0.597提高到0.631（提升5.70%），AUC从0.821提高到0.849（提升3.41%），同时也提高了精确率和召回率，突显了注视感知注意力监督的有效性。在第二阶段，我们提出了一个模块化的报告生成流水线，提取具有置信度加权的诊断关键词，使用基于领域特定先验构建的精选字典将它们映射到解剖区域，并通过结构化提示生成区域对齐的句子。该流水线通过临床关键词召回率和ROUGE重叠度衡量的报告质量有所提高。我们的结果表明，整合注视数据可以提高分类性能和生成医疗报告的可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a two-stage multimodal framework that enhances diseaseclassification and region-aware radiology report generation from chest X-rays,leveraging the MIMIC-Eye dataset. In the first stage, we introduce agaze-guided contrastive learning architecture for disease classification. Itintegrates visual features, clinical labels, bounding boxes, and radiologisteye-tracking signals and is equipped with a novel multi-term gaze-attentionloss combining MSE, KL divergence, correlation, and center-of-mass alignment.Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUCfrom 0.821 to 0.849 (+3.41%), while also improving precision and recall,highlighting the effectiveness of gaze-informed attention supervision. In thesecond stage, we present a modular report generation pipeline that extractsconfidence-weighted diagnostic keywords, maps them to anatomical regions usinga curated dictionary constructed from domain-specific priors, and generatesregion-aligned sentences via structured prompts. This pipeline improves reportquality as measured by clinical keyword recall and ROUGE overlap. Our resultsdemonstrate that integrating gaze data improves both classification performanceand the interpretability of generated medical reports.</description>
      <author>example@mail.com (Tanjim Islam Riju, Shuchismita Anwar, Saman Sarker Joy, Farig Sadeque, Swakkhar Shatabda)</author>
      <guid isPermaLink="false">2508.13068v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score</title>
      <link>http://arxiv.org/abs/2508.12718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为双重对比去噪评分的框架，用于解决文本到图像生成模型在编辑真实图像时面临的挑战，实现了灵活的内容修改和结构保持。&lt;h4&gt;背景&lt;/h4&gt;大规模文本到图像生成模型能够合成多样且高质量的图像，但在直接应用于编辑真实图像时存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决用户难以描述图像细节和现有模型会意外改变不需要区域的问题，实现更好的真实图像编辑。&lt;h4&gt;方法&lt;/h4&gt;提出双重对比去噪评分框架，利用文本到图像扩散模型的生成先验，引入双重对比损失，利用潜在扩散模型自注意力层中间表示的空间信息，不依赖辅助网络。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够实现输入和输出图像之间的灵活内容修改和结构保持，实现零样本图像到图像翻译，在真实图像编辑方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该方法能够直接利用预训练的文本到图像扩散模型而无需进一步训练，为真实图像编辑提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大规模文本到图像生成模型已展现出合成多样且高质量图像的卓越能力。然而，由于两个原因，直接将这些模型应用于编辑真实图像仍然具有挑战性。首先，用户难以提出完美文本提示来准确描述输入图像中的每个视觉细节。其次，尽管现有模型可以在某些区域引入期望的变化，但它们通常会显著改变输入内容并在不需要的区域引入意外的变化。为应对这些挑战，我们提出了双重对比去噪评分，这是一个简单而强大的框架，利用了文本到图像扩散模型的丰富生成先验。受无配对图像到图像翻译的对比学习方法启发，我们在提出的框架内引入了一个简单的双重对比损失。我们的方法利用了潜在扩散模型自注意力层中间表示的丰富空间信息，而不依赖辅助网络。我们的方法实现了输入和输出图像之间的灵活内容修改和结构保持，以及零样本图像到图像翻译。通过大量实验，我们表明我们的方法在真实图像编辑方面优于现有方法，同时保持直接利用预训练文本到图像扩散模型的能力而无需进一步训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale text-to-image generative models have shown remarkable ability tosynthesize diverse and high-quality images. However, it is still challenging todirectly apply these models for editing real images for two reasons. First, itis difficult for users to come up with a perfect text prompt that accuratelydescribes every visual detail in the input image. Second, while existing modelscan introduce desirable changes in certain regions, they often dramaticallyalter the input content and introduce unexpected changes in unwanted regions.To address these challenges, we present Dual Contrastive Denoising Score, asimple yet powerful framework that leverages the rich generative prior oftext-to-image diffusion models. Inspired by contrastive learning approaches forunpaired image-to-image translation, we introduce a straightforward dualcontrastive loss within the proposed framework. Our approach utilizes theextensive spatial information from the intermediate representations of theself-attention layers in latent diffusion models without depending on auxiliarynetworks. Our method achieves both flexible content modification and structurepreservation between input and output images, as well as zero-shotimage-to-image translation. Through extensive experiments, we show that ourapproach outperforms existing methods in real image editing while maintainingthe capability to directly utilize pretrained text-to-image diffusion modelswithout further training.</description>
      <author>example@mail.com (Syed Muhmmad Israr, Feng Zhao)</author>
      <guid isPermaLink="false">2508.12718v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Domain Supervised Contrastive Learning for UAV Radio-Frequency Open-Set Recognition</title>
      <link>http://arxiv.org/abs/2508.12689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多域监督对比学习的无人机射频开放式识别框架，结合深度学习特征融合和改进的OpenMax算法，通过冻结特征提取层和重新训练分类层的方法，在25种无人机类型下实现了95.12%的闭集识别率和96.08%的开集识别率。&lt;h4&gt;背景&lt;/h4&gt;5G-Advanced促进了低空一体化感知与通信网络的蓬勃发展，作为核心组件的无人机近年来快速增长，但由于传统行业监管规范的滞后，未经授权的飞行事件频繁发生，对LA-ISAC网络构成严重安全威胁。&lt;h4&gt;目的&lt;/h4&gt;监视非合作无人机，提出一种用于无人机射频开放式识别的多域监督对比学习框架。&lt;h4&gt;方法&lt;/h4&gt;融合来自ResNet和TransformerEncoder的纹理特征和时间频率位置特征，应用监督对比学习优化闭集样本特征表示，提出改进的生成式OpenMax算法，构建Open-RFNet模型，对于未知样本冻结特征提取层并仅重新训练分类层。&lt;h4&gt;主要发现&lt;/h4&gt;提出的Open-RFNet在已知和未知无人机的识别准确性方面优于现有基准方法，在25种无人机类型下，闭集识别率达95.12%，开集识别率达96.08%。&lt;h4&gt;结论&lt;/h4&gt;该模型在闭集和开集识别中均取得了优异的识别性能，分析了所提出模型的计算复杂性。&lt;h4&gt;翻译&lt;/h4&gt;5G-Advanced(5G-A)已促进低空一体化感知与通信(LA-ISAC)网络的蓬勃发展。作为这些网络的核心组件，无人机(UAVs)近年来经历了快速增长。然而，由于传统行业监管规范的滞后，未经授权的飞行事件频繁发生，对LA-ISAC网络构成严重安全威胁。为了监视非合作无人机，本文提出了一种用于无人机射频(RF)开放式识别的多域监督对比学习(MD-SupContrast)框架。具体来说，首先融合来自ResNet和TransformerEncoder的纹理特征和时间频率位置特征，然后应用监督对比学习来优化闭集样本的特征表示。接下来，为了监视现实生活中出现的入侵无人机，我们提出了一种改进的生成式OpenMax(IG-OpenMax)算法，并构建了一个开放式识别模型，即Open-RFNet。根据未知样本，我们首先冻结特征提取层，然后仅重新训练分类层，在闭集和开集识别中均取得了优异的识别性能。我们分析了所提出模型的计算复杂性。在大型无人机开放数据集上进行了实验。结果表明，所提出的Open-RFNet在已知和未知无人机之间的识别准确性方面优于现有的基准方法，因为在25种无人机类型下，它分别实现了95.12%的闭集和96.08%的开集识别率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 5G-Advanced (5G-A) has enabled the vibrant development of low altitudeintegrated sensing and communication (LA-ISAC) networks. As a core component ofthese networks, unmanned aerial vehicles (UAVs) have witnessed rapid growth inrecent years. However, due to the lag in traditional industry regulatory norms,unauthorized flight incidents occur frequently, posing a severe security threatto LA-ISAC networks. To surveil the non-cooperative UAVs, in this paper, wepropose a multi-domain supervised contrastive learning (MD-SupContrast)framework for UAV radio frequency (RF) open-set recognition. Specifically,first, the texture features and the time-frequency position features from theResNet and the TransformerEncoder are fused, and then the supervisedcontrastive learning is applied to optimize the feature representation of theclosed-set samples. Next, to surveil the invasive UAVs that appear in reallife, we propose an improved generative OpenMax (IG-OpenMax) algorithm andconstruct an open-set recognition model, namely Open-RFNet. According to theunknown samples, we first freeze the feature extraction layers and then onlyretrain the classification layer, which achieves excellent recognitionperformance both in closed-set and open-set recognitions. We analyze thecomputational complexity of the proposed model. Experiments are conducted witha large-scale UAV open dataset. The results show that the proposed Open-RFNetoutperforms the existing benchmark methods in terms of recognition accuracybetween the known and the unknown UAVs, as it achieves 95.12% in closed-set and96.08% in open-set under 25 UAV types, respectively.</description>
      <author>example@mail.com (Ning Gao, Tianrui Zeng, Bowen Chen, Donghong Cai, Shi Jin, Michail Matthaiou)</author>
      <guid isPermaLink="false">2508.12689v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection</title>
      <link>http://arxiv.org/abs/2508.12684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AdaBEV，一种通过精炼-对比范式学习自适应实例感知BEV表示的新框架，用于多无人机协作3D检测。&lt;h4&gt;背景&lt;/h4&gt;多无人机协作3D检测通过融合多视角观测实现准确且鲁棒的环境感知，具有覆盖范围大和遮挡处理能力强的优势，但给资源受限的无人机平台带来计算挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的框架，能够在资源受限的无人机平台上实现准确的多无人机协作3D检测。&lt;h4&gt;方法&lt;/h4&gt;AdaBEV框架引入了Box-Guided Refinement Module (BG-RM)和Instance-Background Contrastive Learning (IBCL)，前者使用2D监督和空间细分精炼与前景实例相关的BEV网格，后者通过BEV空间中的对比学习增强前景和背景特征的分离。&lt;h4&gt;主要发现&lt;/h4&gt;在Air-Co-Pred数据集上的实验表明，AdaBEV在各种模型规模上实现了优越的精度-计算权衡，在低分辨率下优于其他最先进方法，同时保持低分辨率BEV输入和可忽略的开销。&lt;h4&gt;结论&lt;/h4&gt;AdaBEV有效解决了多无人机协作3D检测中的计算挑战，在保持高检测性能的同时实现了高效计算。&lt;h4&gt;翻译&lt;/h4&gt;多无人机协作3D检测通过融合来自空中平台的多视角观测实现准确且鲁棒的环境感知，在覆盖范围和遮挡处理方面具有显著优势，同时也给资源受限的无人机平台上的计算带来了新挑战。本文提出了AdaBEV，一种通过精炼-对比范式学习自适应实例感知BEV表示的新框架。与将所有BEV网格同等对待的现有方法不同，AdaBEV引入了Box-Guided Refinement Module (BG-RM)和Instance-Background Contrastive Learning (IBCL)来增强语义感知和特征区分度。BG-RM使用2D监督和空间细分只精炼与前景实例相关的BEV网格，而IBCL通过BEV空间中的对比学习促进前景和背景特征之间的更强分离。在Air-Co-Pred数据集上的大量实验表明，AdaBEV在各种模型规模上实现了优越的精度-计算权衡，在低分辨率下优于其他最先进方法，同时保持低分辨率BEV输入和可忽略的开销，接近上限性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多无人机协作3D目标检测中BEV（鸟瞰图）表示的效率问题。现有方法对所有BEV网格区域进行均匀处理，在资源受限的无人机平台上造成了计算资源浪费。这个问题很重要，因为多无人机协作能提供更全面的感知，在监控、城市监测和灾害响应等应用中至关重要，但需要高精度与高效率的平衡才能满足实时性要求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有BEV方法采用均匀网格假设，这与无人机视角下信息分布不均匀的现实不符。他们借鉴了BEVFormer作为基础框架，参考了Focal-PETR等使用2D检测监督的辅助任务方法，以及SimCLR等对比学习方法。基于这些，设计了两个核心组件：利用2D检测框识别重要区域并进行细化的Box-Guided Refinement Module，以及在BEV空间中应用对比学习增强特征区分度的Instance-Background Contrastive Learning。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是自适应实例感知的BEV表示学习，根据前景和背景区域的重要性差异进行非均匀处理。整体流程：1)输入多视角图像并提取特征；2)通过透视感知监督生成2D边界框；3)初始化BEV查询并获取初始BEV特征；4)应用BG-RM模块识别前景区域并进行4×4细分处理；5)应用IBCL模块提取实例特征和背景特征并进行对比学习；6)通过检测解码器进行3D目标检测；7)计算包括基础检测损失、透视感知损失和对比学习损失在内的总损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自适应非均匀BEV建模，打破传统均匀网格假设；2)Box-Guided Refinement Module，仅对前景区域精细化处理；3)Instance-Background Contrastive Learning，在BEV空间直接应用对比学习增强特征区分度；4)实现精度-计算效率的平衡。相比之前的工作，不同在于：传统方法均匀处理所有网格，本文只精细处理前景区域；虽然使用2D检测作为辅助，但用于精确识别需要精细化的区域；首次在BEV空间直接应用实例-背景对比学习；不需要显式的无人机间信息交换即可实现高效协作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种自适应实例感知的BEV表示方法，通过仅对前景区域进行精细化处理和增强前景-背景特征区分度，实现了在资源受限无人机平台上高效准确的3D目标检测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-UAV collaborative 3D detection enables accurate and robust perceptionby fusing multi-view observations from aerial platforms, offering significantadvantages in coverage and occlusion handling, while posing new challenges forcomputation on resource-constrained UAV platforms. In this paper, we presentAdaBEV, a novel framework that learns adaptive instance-aware BEVrepresentations through a refine-and-contrast paradigm. Unlike existing methodsthat treat all BEV grids equally, AdaBEV introduces a Box-Guided RefinementModule (BG-RM) and an Instance-Background Contrastive Learning (IBCL) toenhance semantic awareness and feature discriminability. BG-RM refines only BEVgrids associated with foreground instances using 2D supervision and spatialsubdivision, while IBCL promotes stronger separation between foreground andbackground features via contrastive learning in BEV space. Extensiveexperiments on the Air-Co-Pred dataset demonstrate that AdaBEV achievessuperior accuracy-computation trade-offs across model scales, outperformingother state-of-the-art methods at low resolutions and approaching upper boundperformance while maintaining low-resolution BEV inputs and negligibleoverhead.</description>
      <author>example@mail.com (Zhongyao Li, Peirui Cheng, Liangjin Zhao, Chen Chen, Yundu Li, Zhechao Wang, Xue Yang, Xian Sun, Zhirui Wang)</author>
      <guid isPermaLink="false">2508.12684v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision</title>
      <link>http://arxiv.org/abs/2508.12278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CRoC的框架，用于解决图神经网络在图异常检测中面临的标记数据不足问题。该方法通过重构节点上下文和编码异构关系增强GNN的鲁棒性，并结合对比学习有效利用未标记数据，在七个真实世界数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;图神经网络被广泛用于各种图相关任务，但在训练鲁棒GNN时通常需要大量标记数据，这在实际应用中是一个关键瓶颈。这一限制在图异常检测(GAD)中尤为严重，因为异常样本本质上是稀有的、标记成本高，并且可能主动伪装其模式以逃避检测。&lt;h4&gt;目的&lt;/h4&gt;解决图异常检测中标记数据不足的问题，提高GNN在有限标记数据情况下的检测性能，特别是对抗伪装异常的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;CRoC框架通过以下方式改进GNN用于GAD：1)利用GAD中固有的类别不平衡来重构每个节点的上下文，通过重新组合节点属性同时保留其交互模式来构建增强图；2)分别编码异构关系并将它们整合到消息传递过程中，增强模型捕获复杂交互语义的能力；3)在训练阶段，将CRoC与对比学习范式结合，使GNN能够有效利用未标记数据，生成更丰富、更具区分性的节点嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在七个不同规模的真实世界GAD数据集上进行的广泛实验表明，CRoC相比基础GNN方法实现了高达14%的AUC提升，并且在有限标记设置下优于最先进的GAD方法。&lt;h4&gt;结论&lt;/h4&gt;CRoC框架通过重构节点上下文和编码异构关系，结合对比学习，有效解决了GAD中标记数据不足的问题，显著提高了GNN在异常检测中的性能，特别是在对抗伪装异常情况下的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)被广泛用作各种图相关任务的引擎，它们在分析图结构数据方面具有有效性。然而，训练鲁棒的GNN通常需要大量的标记数据，这在实际应用中是一个关键瓶颈。这一限制严重阻碍了图异常检测(GAD)的进展，其中异常本质上是稀有的，标记成本高，并且可能主动伪装其模式以逃避检测。为了解决这些问题，我们提出了上下文重构对比(CRoC)，一个简单而有效的框架，通过联合利用有限的标记数据和丰富的未标记数据来训练GNN进行GAD。与先前的工作不同，CRoC利用GAD中固有的类别不平衡来重构每个节点的上下文，通过重新组合节点属性同时保留其交互模式来构建增强图。此外，CRoC分别编码异构关系并将它们整合到消息传递过程中，增强模型捕获复杂交互语义的能力。这些操作保留了节点语义，同时鼓励对抗伪装的鲁棒性，使GNN能够发现复杂的异常情况。在训练阶段，CRoC进一步与对比学习范式集成。这使GNN能够在联合训练过程中有效利用未标记数据，生成更丰富、更具区分性的节点嵌入。CRoC在七个不同规模的真实世界GAD数据集上进行了评估。大量实验表明，CRoC相比基础GNN方法实现了高达14%的AUC提升，并且在有限标记设置下优于最先进的GAD方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are widely used as the engine for variousgraph-related tasks, with their effectiveness in analyzing graph-structureddata. However, training robust GNNs often demands abundant labeled data, whichis a critical bottleneck in real-world applications. This limitation severelyimpedes progress in Graph Anomaly Detection (GAD), where anomalies areinherently rare, costly to label, and may actively camouflage their patterns toevade detection. To address these problems, we propose Context RefactoringContrast (CRoC), a simple yet effective framework that trains GNNs for GAD byjointly leveraging limited labeled and abundant unlabeled data. Different fromprevious works, CRoC exploits the class imbalance inherent in GAD to refactorthe context of each node, which builds augmented graphs by recomposing theattributes of nodes while preserving their interaction patterns. Furthermore,CRoC encodes heterogeneous relations separately and integrates them into themessage-passing process, enhancing the model's capacity to capture complexinteraction semantics. These operations preserve node semantics whileencouraging robustness to adversarial camouflage, enabling GNNs to uncoverintricate anomalous cases. In the training stage, CRoC is further integratedwith the contrastive learning paradigm. This allows GNNs to effectively harnessunlabeled data during joint training, producing richer, more discriminativenode embeddings. CRoC is evaluated on seven real-world GAD datasets withvarying scales. Extensive experiments demonstrate that CRoC achieves up to 14%AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methodsunder limited-label settings.</description>
      <author>example@mail.com (Siyue Xie, Da Sun Handason Tam, Wing Cheong Lau)</author>
      <guid isPermaLink="false">2508.12278v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction</title>
      <link>http://arxiv.org/abs/2508.12247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的时空多尺度Mamba模型(STM2/STM3)来解决长期时空时间序列预测中的挑战，通过多尺度Mamba架构和自适应图因果卷积网络有效捕获复杂的多尺度时空依赖关系，并在真实世界基准测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;时空时间序列预测发展迅速，但现有的深度学习方法难以高效学习复杂的长期时空依赖关系。&lt;h4&gt;目的&lt;/h4&gt;解决长期时空依赖学习带来的两个新挑战：1)长期时间序列中包含的多尺度信息难以高效提取；2)不同节点的多尺度时间信息高度相关且难以建模。&lt;h4&gt;方法&lt;/h4&gt;提出了STM2模型，包含多尺度Mamba架构和自适应图因果卷积网络，以及层次化信息聚合保证不同尺度信息的可区分性；进一步提出了STM3增强版本，采用专家混合架构、更稳定的路由策略和因果对比学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;STM3具有更好的路由平滑性，成功保证了每个专家的模式解纠缠，能够更有效地捕获所有空间节点上的多样化时间动态。&lt;h4&gt;结论&lt;/h4&gt;在真实世界基准测试中，STM2/STM3表现出优越性能，在长期时空时间序列预测中取得了最先进的结果。&lt;h4&gt;翻译&lt;/h4&gt;最近，时空时间序列预测发展迅速，但现有的深度学习方法难以高效学习复杂的长期时空依赖关系。长期时空依赖学习带来了两个新的挑战：1)长期时间序列自然包含多尺度信息，难以高效提取；2)来自不同节点的多尺度时间信息高度相关且难以建模。为应对这些挑战，我们提出了一种高效的时空多尺度Mamba(STM2)，包含多尺度Mamba架构，能够同时高效捕获多尺度信息，以及自适应图因果卷积网络来学习复杂的多尺度时空依赖。STM2包含不同尺度信息的层次化信息聚合，保证它们的可区分性。为了更有效地捕获所有空间节点上的多样化时间动态，我们进一步提出了一个增强版本，称为时空多尺度Mamba混合(STM3)，它采用特殊的专家混合架构，包括更稳定的路由策略和因果对比学习策略来增强尺度可区分性。我们证明STM3具有更好的路由平滑性，并成功保证了每个专家的模式解纠缠。在真实世界基准上的大量实验证明了STM2/STM3的优越性能，在长期时空时间序列预测中取得了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, spatio-temporal time-series prediction has developed rapidly, yetexisting deep learning methods struggle with learning complex long-termspatio-temporal dependencies efficiently. The long-term spatio-temporaldependency learning brings two new challenges: 1) The long-term temporalsequence includes multiscale information naturally which is hard to extractefficiently; 2) The multiscale temporal information from different nodes ishighly correlated and hard to model. To address these challenges, we propose anefficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capturethe multiscale information efficiently and simultaneously, and an adaptivegraph causal convolution network to learn the complex multiscalespatio-temporal dependency. STM2 includes hierarchical information aggregationfor different-scale information that guarantees their distinguishability. Tocapture diverse temporal dynamics across all spatial nodes more efficiently, wefurther propose an enhanced version termed\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a specialMixture-of-Experts architecture, including a more stable routing strategy and acausal contrastive learning strategy to enhance the scale distinguishability.We prove that STM3 has much better routing smoothness and guarantees thepattern disentanglement for each expert successfully. Extensive experiments onreal-world benchmarks demonstrate STM2/STM3's superior performance, achievingstate-of-the-art results in long-term spatio-temporal time-series prediction.</description>
      <author>example@mail.com (Haolong Chen, Liang Zhang, Zhengyuan Xin, Guangxu Zhu)</author>
      <guid isPermaLink="false">2508.12247v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection</title>
      <link>http://arxiv.org/abs/2508.12230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TASLP. 15 pages, 7 figures;&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自监督预训练模型的机器异常声音检测方法，通过全连接低秩自适应和机器感知组适配器模块提升模型泛化能力，并设计了处理缺失标签的新目标函数。&lt;h4&gt;背景&lt;/h4&gt;机器异常声音检测(ASD)是一项有价值的技术，但其泛化性能常受数据收集困难和声学环境复杂性的限制。预训练数据集与ASD任务之间存在不一致性。&lt;h4&gt;目的&lt;/h4&gt;引入鲁棒的ASD模型利用大规模语音和音频数据集上的自监督预训练模型；缓解微调时的过拟合；使模型能捕捉不同机器间的差异；解决属性标签缺失的挑战。&lt;h4&gt;方法&lt;/h4&gt;利用大规模语音和音频数据集上的自监督预训练模型；采用全连接低秩自适应(LoRA)替代完全微调；提出机器感知组适配器模块；设计使用向量量化对未标记数据进行动态聚类并通过双重对比学习损失优化的新目标函数。&lt;h4&gt;主要发现&lt;/h4&gt;尽管预训练数据集与ASD任务存在不一致，预训练仍能为ASD提供实质性好处；所提出方法在DCASE 2020-2024五个ASD挑战等基准数据集上表现优异，实验结果显示显著改进。&lt;h4&gt;结论&lt;/h4&gt;提出的策略和方法能有效提升ASD系统性能，自监督预训练模型结合特定适配方法可解决ASD中的泛化问题。&lt;h4&gt;翻译&lt;/h4&gt;机器异常声音检测(ASD)是一项在各种应用中都有价值的技术。然而，由于数据收集困难和声学环境的复杂性，其泛化性能通常受到限制。受众多领域中大型预训练模型成功的启发，本文引入了一个鲁棒的ASD模型，该模型利用在大型语音和音频数据集上训练的自监督预训练模型。尽管预训练数据集与ASD任务存在不一致性，但我们的研究结果表明预训练仍能为ASD提供实质性好处。为了在使用有限数据进行微调时缓解过拟合并保留已学习的知识，我们探索了全连接低秩自适应(LoRA)作为完全微调的替代方案。此外，我们提出了一个机器感知组适配器模块，使模型能够在统一框架内捕捉不同机器之间的差异，从而增强ASD系统的泛化性能。为了解决属性标签缺失的挑战，我们设计了一种新的目标函数，该函数使用向量量化对未标记数据进行动态聚类，并通过双重对比学习损失进行优化。所提出的方法在所有基准数据集上进行了评估，包括DCASE 2020-2024五个ASD挑战，实验结果表明我们的新方法有显著改进，并证明了所提出策略的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine anomalous sound detection (ASD) is a valuable technique acrossvarious applications. However, its generalization performance is often limiteddue to challenges in data collection and the complexity of acousticenvironments. Inspired by the success of large pre-trained models in numerousfields, this paper introduces a robust ASD model that leverages self-supervisedpre-trained models trained on large-scale speech and audio datasets. Althoughthere are inconsistencies between the pre-training datasets and the ASD task,our findings indicate that pre-training still provides substantial benefits forASD. To mitigate overfitting and retain learned knowledge when fine-tuning withlimited data, we explore Fully-Connected Low-Rank Adaptation (LoRA) as analternative to full fine-tuning. Additionally, we propose a Machine-aware GroupAdapter module, which enables the model to capture differences between variousmachines within a unified framework, thereby enhancing the generalizationperformance of ASD systems. To address the challenge of missing attributelabels, we design a novel objective function that dynamically clustersunattributed data using vector quantization and optimizes through a dual-levelcontrastive learning loss. The proposed methods are evaluated on all benchmarkdatasets, including the DCASE 2020-2024 five ASD challenges, and theexperimental results show significant improvements of our new approach anddemonstrate the effectiveness of our proposed strategies.</description>
      <author>example@mail.com (Bing Han, Anbai Jiang, Xinhu Zheng, Wei-Qiang Zhang, Jia Liu, Pingyi Fan, Yanmin Qian)</author>
      <guid isPermaLink="false">2508.12230v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine</title>
      <link>http://arxiv.org/abs/2508.12108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VELVET-Med是一个针对医学体积数据(如3D CT扫描)的新型视觉语言预训练框架，通过创新的预训练目标和模型架构，解决了医学领域大规模数据收集的挑战，在多种下游任务上展现出卓越的性能。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在医学领域越来越受到关注，但与2D图像和文本的直接配对不同，为体积模态如CT扫描收集大规模配对数据具有挑战性且耗时，这限制了下游任务的性能。&lt;h4&gt;目的&lt;/h4&gt;提出一个专门针对有限体积数据(如3D CT和相关放射学报告)的视觉语言预训练框架VELVET-Med，不依赖大规模数据收集，而是专注于有效的预训练目标和模型架构的开发。&lt;h4&gt;方法&lt;/h4&gt;将单模态自监督学习整合到VLP框架中；提出新型语言编码器TriBERT学习多级文本语义；设计分层对比学习捕获多级视觉语言对应关系；仅使用38,875个扫描-报告对。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能揭示体积医学图像和相关临床叙述中嵌入的丰富空间和语义关系，增强学习编码器的泛化能力，编码器表现出强大的可迁移性。&lt;h4&gt;结论&lt;/h4&gt;VELVET-Med在多种下游任务上取得了最先进的性能，包括3D分割、跨模态检索、视觉问答和报告生成，证明了其在医学视觉语言处理中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在医学领域越来越受到探索，特别是在CLIP在通用领域取得成功之后。然而，与2D图像和文本的直接配对不同，在医学领域为体积模态如CT扫描收集大规模配对数据仍然是一个具有挑战性和耗时的过程。这种困难常常限制了下游任务的性能。为解决这些挑战，我们提出了一种新型的视觉语言预训练框架，称为VELVET-Med，专门针对有限的体积数据如3D CT和相关的放射学报告。我们的方法不依赖大规模数据收集，而是专注于有效的预训练目标和模型架构的开发。主要贡献包括：1)我们将单模态自监督学习整合到VLP框架中，这在现有文献中常常未被充分探索；2)我们提出了一种新型语言编码器TriBERT，用于学习多级文本语义；3)我们设计了分层对比学习来捕获多级视觉语言对应关系。仅使用38,875个扫描-报告对，我们的方法旨在揭示体积医学图像和相关临床叙述中嵌入的丰富空间和语义关系，从而增强学习编码器的泛化能力。所得编码器表现出强大的可迁移性，在3D分割、跨模态检索、视觉问答和报告生成等一系列广泛的下游任务上取得了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学领域特别是3D体积成像数据（如CT扫描）与相应放射学报告配对数据稀缺的问题。这个问题很重要，因为医学数据获取面临患者隐私保护和专业标注需求的双重挑战，而3D数据比2D图像更复杂，需要更多样本；同时医学报告比普通图像描述更复杂，包含多层次临床概念，现有方法难以充分利用这些信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了医学视觉-语言模型面临的两大挑战：全局特征对齐忽略局部信息，以及医学报告的复杂层次结构未被充分利用。他们借鉴了CLIP的成功经验、自监督学习方法和现有VLM架构，但针对医学领域特点进行了创新。设计上不依赖大规模数据收集，而是专注于开发有效的预训练目标和模型架构，通过层次对比学习对齐不同粒度的视觉和文本特征，同时利用单模态自监督学习增强数据效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过层次对比学习对齐不同粒度的视觉和文本特征，同时利用单模态自监督学习提升数据效率，充分挖掘3D医学扫描中的空间和语义关系。整体流程包括：1)采用三编码器架构（视觉、语言、多模态）；2)准备不同尺度的视觉输入和多种文本嵌入；3)设计四类学习目标（单模态视觉、单模态语言、跨模态、多模态）；4)提出TriBERT处理医学报告层次结构；5)实现层次对比学习对齐不同级别的视觉和文本特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)VELVET-Med框架专为医学体积数据设计，在有限数据条件下表现优异；2)层次对比学习对齐多级视觉和文本特征，捕捉特定区域的医学概念；3)TriBERT语言编码器学习报告/句子/词级语义，通过特殊注意力掩码防止句子间信息泄漏；4)集成单模态自监督学习增强模型泛化能力；5)创建高质量M3D-CAP-filtered数据集。相比之前工作，VELVET-Med更关注3D而非2D医学数据，更好地处理医学报告的层次结构，同时考虑局部和全局特征，且不依赖大规模数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VELVET-Med通过创新的层次对比学习和TriBERT语言编码器，在有限医学体积数据条件下实现了先进的视觉-语言对齐，显著提升了3D医学图像理解任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-language models (VLMs) have been increasingly explored in themedical domain, particularly following the success of CLIP in general domain.However, unlike the relatively straightforward pairing of 2D images and text,curating large-scale paired data in the medical field for volumetric modalitiessuch as CT scans remains a challenging and time-intensive process. Thisdifficulty often limits the performance on downstream tasks. To address thesechallenges, we propose a novel vision-language pre-training (VLP) framework,termed as \textbf{VELVET-Med}, specifically designed for limited volumetricdata such as 3D CT and associated radiology reports. Instead of relying onlarge-scale data collection, our method focuses on the development of effectivepre-training objectives and model architectures. The key contributions are: 1)We incorporate uni-modal self-supervised learning into VLP framework, which areoften underexplored in the existing literature. 2) We propose a novel languageencoder, termed as \textbf{TriBERT}, for learning multi-level textualsemantics. 3) We devise the hierarchical contrastive learning to capturemulti-level vision-language correspondence. Using only 38,875 scan-reportpairs, our approach seeks to uncover rich spatial and semantic relationshipsembedded in volumetric medical images and corresponding clinical narratives,thereby enhancing the generalization ability of the learned encoders. Theresulting encoders exhibit strong transferability, achieving state-of-the-artperformance across a wide range of downstream tasks, including 3D segmentation,cross-modal retrieval, visual question answering, and report generation.</description>
      <author>example@mail.com (Ziyang Zhang, Yang Yu, Xulei Yang, Si Yong Yeo)</author>
      <guid isPermaLink="false">2508.12108v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases</title>
      <link>http://arxiv.org/abs/2508.12031v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于指令的持续对比调优方法，用于大型语言模型在持续关系提取任务中，通过重视错误案例和利用LLM的指令跟随能力，有效减轻了灾难性遗忘问题。&lt;h4&gt;背景&lt;/h4&gt;持续关系提取(CRE)旨在持续学习新出现的关系同时避免灾难性遗忘。现有CRE方法主要使用内存回放和对比学习来减轻灾难性遗忘，但这些方法没有重视能更有效揭示模型认知偏差的错误案例。&lt;h4&gt;目的&lt;/h4&gt;解决现有CRE方法未重视错误案例的问题，提出一种基于指令的持续对比调优方法，利用大型语言模型的指令跟随能力，更有效地减轻灾难性遗忘并处理新旧关系之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于指令的持续对比调优方法，将每个任务的训练和内存数据根据初始响应的正确性分成两部分，通过双任务微调分别处理；利用LLM的指令跟随能力，提出基于指令的对比调优策略，以指令调优方式用先前数据持续指导纠正当前认知偏差，更适合LLM的方式减轻新旧关系之间的差距。&lt;h4&gt;主要发现&lt;/h4&gt;在TACRED和FewRel数据集上的实验表明，该方法取得了新的最先进CRE性能，有显著改进，证明了专门利用错误案例的重要性。&lt;h4&gt;结论&lt;/h4&gt;专门利用错误案例对于提升持续关系提取性能至关重要，所提出的基于指令的持续对比调优方法能够有效减轻灾难性遗忘并处理新旧关系之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;持续关系提取(CRE)旨在持续学习新出现的关系同时避免灾难性遗忘。现有CRE方法主要使用内存回放和对比学习来减轻灾难性遗忘。然而，这些方法没有重视能更有效揭示模型认知偏差的错误案例。为解决这一问题，我们提出了一种基于指令的持续对比调优方法，应用于CRE中的大型语言模型(LLMs)。与现有统一处理训练和内存数据的CRE方法不同，该方法根据初始响应的正确性将每个任务的训练和内存数据分成两部分，并通过双任务微调分别处理它们。此外，利用LLM的指令跟随能力优势，我们提出了一种新颖的基于指令的对比调优策略，用于LLM，以指令调优方式用先前数据持续指导纠正当前认知偏差，从而以更适合LLM的方式减轻新旧关系之间的差距。我们在TACRED和FewRel上对模型进行了实验评估，结果表明我们的模型取得了新的最先进CRE性能，有显著改进，证明了专门利用错误案例的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual Relation Extraction (CRE) aims to continually learn new emergingrelations while avoiding catastrophic forgetting. Existing CRE methods mainlyuse memory replay and contrastive learning to mitigate catastrophic forgetting.However, these methods do not attach importance to the error cases that canreveal the model's cognitive biases more effectively. To address this issue, wepropose an instruction-based continual contrastive tuning approach for LargeLanguage Models (LLMs) in CRE. Different from existing CRE methods thattypically handle the training and memory data in a unified manner, thisapproach splits the training and memory data of each task into two partsrespectively based on the correctness of the initial responses and treats themdifferently through dual-task fine-tuning. In addition, leveraging theadvantages of LLM's instruction-following ability, we propose a novelinstruction-based contrastive tuning strategy for LLM to continuously correctcurrent cognitive biases with the guidance of previous data in aninstruction-tuning manner, which mitigates the gap between old and newrelations in a more suitable way for LLMs. We experimentally evaluate our modelon TACRED and FewRel, and the results show that our model achieves newstate-of-the-art CRE performance with significant improvements, demonstratingthe importance of specializing in exploiting error cases.</description>
      <author>example@mail.com (Shaozhe Yin, Jinyu Guo, Kai Shuang, Xia Liu, Ruize Ou)</author>
      <guid isPermaLink="false">2508.12031v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach</title>
      <link>http://arxiv.org/abs/2508.11742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了首个基于攻击的基准评估方法，用于评估合成流量生成器(SynNetGens)的隐私性，并开发了TraceBleed攻击方法和TracePatch防御方法。&lt;h4&gt;背景&lt;/h4&gt;当前的合成流量生成器虽然承诺隐私保护但缺乏全面保证和实证验证，尽管其保真度不断提高。&lt;h4&gt;目的&lt;/h4&gt;引入首个直接从生成的流量中评估SynNetGens隐私性的基准，将隐私定义为流量源级别的成员推断。&lt;h4&gt;方法&lt;/h4&gt;提出TraceBleed攻击方法，利用流之间的行为指纹，采用对比学习和时间分块技术，比先前的成员推断基线效果高出172%；对基于GAN、扩散模型和GPT的SynNetGens进行了大规模研究。&lt;h4&gt;主要发现&lt;/h4&gt;SynNetGens会泄露用户级信息；差分隐私要么无法阻止攻击，要么严重降低保真度；分享更多合成数据会使泄露平均增加59%。&lt;h4&gt;结论&lt;/h4&gt;引入TracePatch，首个与SynNetGen无关的防御方法，结合对抗性机器学习和SMT约束来减轻泄露同时保持保真度。&lt;h4&gt;翻译&lt;/h4&gt;当前合成流量生成器(SynNetGens)承诺隐私但缺乏全面保证或实证验证，即使其保真度不断提高。我们引入了首个基于攻击的基准，用于直接从它们生成的流量中评估SynNetGens的隐私性。我们将隐私定义为流量源级别的成员推断——这对数据持有者来说是一个现实且可操作的威胁。为此，我们提出了TraceBleed，这是首个利用流之间行为指纹的攻击方法，使用对比学习和时间分块技术，比先前的成员推断基线高出172%。我们对基于GAN、扩散模型和GPT的SynNetGens进行的大规模研究发现了关键见解：(i) SynNetGens泄露用户级信息；(ii) 差分隐私要么无法阻止这些攻击，要么严重降低保真度；(iii) 分享更多合成数据会使泄露平均增加59%。最后，我们引入了TracePatch，这是首个与SynNetGen无关的防御方法，结合对抗性机器学习和SMT约束来减轻泄露同时保持保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current synthetic traffic generators (SynNetGens) promise privacy but lackcomprehensive guarantees or empirical validation, even as their fidelitysteadily improves. We introduce the first attack-grounded benchmark forassessing the privacy of SynNetGens directly from the traffic they produce. Weframe privacy as membership inference at the traffic-source level--a realisticand actionable threat for data holders. To this end, we present TraceBleed, thefirst attack that exploits behavioral fingerprints across flows usingcontrastive learning and temporal chunking, outperforming prior membershipinference baselines by 172%. Our large-scale study across GAN-, diffusion-, andGPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-levelinformation; (ii) differential privacy either fails to stop these attacks orseverely degrades fidelity; and (iii) sharing more synthetic data amplifiesleakage by 59% on average. Finally, we introduce TracePatch, the firstSynNetGen-agnostic defense that combines adversarial ML with SMT constraints tomitigate leakage while preserving fidelity.</description>
      <author>example@mail.com (Minhao Jin, Hongyu He, Maria Apostolaki)</author>
      <guid isPermaLink="false">2508.11742v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning</title>
      <link>http://arxiv.org/abs/2508.11328v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HS-GPPT模型解决了现有图预训练方法无法处理多样化频谱分布的问题，通过频谱对齐实现了高效的知识迁移。&lt;h4&gt;背景&lt;/h4&gt;图预训练和提示调优可以使下游任务与预训练目标保持一致，从而在有限监督下实现高效的知识迁移。然而，现有方法依赖于基于同质性的低频知识，无法处理现实世界中具有不同同质性的图的多样化频谱分布。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，确保在预训练和提示调优过程中实现频谱对齐，以解决现有方法无法处理多样化频谱分布的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了HS-GPPT模型，使用混合频谱滤波器主干和局部-全局对比学习来获取丰富的频谱知识，并设计提示图以使频谱分布与预训练任务保持一致，促进跨越同质性和异质性的频谱知识迁移。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析揭示了一个频谱特异性原则：最优知识迁移需要预训练频谱滤波器与下游图的内在频谱保持一致；在有限监督下，预训练和下游任务之间的较大频谱差距会阻碍有效适应。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验验证了HS-GPPT模型在直推学习和归纳学习设置下的有效性，代码已公开提供。&lt;h4&gt;翻译&lt;/h4&gt;图'预训练和提示调优'使下游任务与预训练目标保持一致，从而在有限监督下实现高效的知识迁移。然而，现有方法依赖于基于同质性的低频知识，无法处理现实世界中具有不同同质性的图的多样化频谱分布。我们的理论分析揭示了一个频谱特异性原则：最优知识迁移需要预训练频谱滤波器与下游图的内在频谱保持一致。在有限监督下，预训练和下游任务之间的较大频谱差距会阻碍有效适应。为了弥合这一差距，我们提出了HS-GPPT模型，这是一个新颖的框架，确保在预训练和提示调优过程中实现频谱对齐。我们使用混合频谱滤波器主干和局部-全局对比学习来获取丰富的频谱知识。然后我们设计提示图以使频谱分布与预训练任务保持一致，促进跨越同质性和异质性的频谱知识迁移。广泛的实验验证了在直推学习和归纳学习设置下的有效性。我们的代码可在https://anonymous.4open.science/r/HS-GPPT-62D2/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph ``pre-training and prompt-tuning'' aligns downstream tasks withpre-trained objectives to enable efficient knowledge transfer under limitedsupervision. However, existing methods rely on homophily-based low-frequencyknowledge, failing to handle diverse spectral distributions in real-worldgraphs with varying homophily. Our theoretical analysis reveals a spectralspecificity principle: optimal knowledge transfer requires alignment betweenpre-trained spectral filters and the intrinsic spectrum of downstream graphs.Under limited supervision, large spectral gaps between pre-training anddownstream tasks impede effective adaptation. To bridge this gap, we proposethe HS-GPPT model, a novel framework that ensures spectral alignment throughoutboth pre-training and prompt-tuning. We utilize a hybrid spectral filterbackbone and local-global contrastive learning to acquire abundant spectralknowledge. Then we design prompt graphs to align the spectral distribution withpretexts, facilitating spectral knowledge transfer across homophily andheterophily. Extensive experiments validate the effectiveness under bothtransductive and inductive learning settings. Our code is available athttps://anonymous.4open.science/r/HS-GPPT-62D2/.</description>
      <author>example@mail.com (Haitong Luo, Suhang Wang, Weiyao Zhang, Ruiqi Meng, Xuying Meng, Yujun Zhang)</author>
      <guid isPermaLink="false">2508.11328v2</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry</title>
      <link>http://arxiv.org/abs/2508.13111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为因果引导的成对Transformer（CGPT）的新型架构，解决了工业系统中多维时间序列建模中通道相关（CD）与通道独立（CI）模型之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;工业系统中多维时间序列的基础建模面临核心权衡：CD模型捕捉特定变量间动态但缺乏鲁棒性和适应性，而CI模型提供通用性但无法建模系统级预测任务中必要的显式交互。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉变量间动态并保持通用性和适应性的架构，解决CD/CI模型之间的权衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出CGPT架构，将已知因果图作为归纳偏置，采用成对建模范式将多维数据分解为对，使用通道不可知的学习层，在成对级别强制执行CD信息流，在跨对时实现类似CI的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实工业数据集上的长期和单步预测任务中，CGPT在预测准确性方面显著优于CI和CD基线模型，与端到端训练的CD模型相比具有竞争力，同时保持对问题维度的不可知性。&lt;h4&gt;结论&lt;/h4&gt;CGPT成功解决了工业系统中多维时间序列建模的CD/CI权衡问题，通过因果引导的成对Transformer架构实现了高预测精度和通用性。&lt;h4&gt;翻译&lt;/h4&gt;工业系统中多维时间序列的基础建模呈现一个核心权衡：通道相关(CD)模型捕捉特定变量间的动态，但缺乏鲁棒性和适应性，因为模型层通常受限于特定用例的数据维度；而通道独立(CI)模型提供通用性，但以建模系统级预测回归任务中必要的显式交互为代价。为此，我们提出了因果引导的成对Transformer(CGPT)，一种将已知因果图作为归纳偏置集成的新型架构。CGPT的核心围绕成对建模范式，通过将多维数据分解为对来解决CD/CI冲突。该模型使用通道不可知的学习层，所有参数维度独立于变量数量。CGPT在成对级别强制执行CD信息流，并在跨对时实现类似CI的泛化能力。这种方法解耦了复杂的系统动态，并产生高度灵活的架构，确保了可扩展性和任意变量适应性。我们在模拟常见工业复杂性的长期和单步预测任务中，在一系列合成和真实工业数据集上验证了CGPT。结果表明，CGPT在预测准确性方面显著优于CI和CD基线模型，同时保持对问题维度的不可知性，与端到端训练的CD模型相比具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational modelling of multi-dimensional time-series data in industrialsystems presents a central trade-off: channel-dependent (CD) models capturespecific cross-variable dynamics but lack robustness and adaptability as modellayers are commonly bound to the data dimensionality of the tackled use-case,while channel-independent (CI) models offer generality at the cost of modellingthe explicit interactions crucial for system-level predictive regression tasks.To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), anovel architecture that integrates a known causal graph as an inductive bias.The core of CGPT is built around a pairwise modeling paradigm, tackling theCD/CI conflict by decomposing the multidimensional data into pairs. The modeluses channel-agnostic learnable layers where all parameter dimensions areindependent of the number of variables. CGPT enforces a CD information flow atthe pair-level and CI-like generalization across pairs. This approachdisentangles complex system dynamics and results in a highly flexiblearchitecture that ensures scalability and any-variate adaptability. We validateCGPT on a suite of synthetic and real-world industrial datasets on long-termand one-step forecasting tasks designed to simulate common industrialcomplexities. Results demonstrate that CGPT significantly outperforms both CIand CD baselines in predictive accuracy and shows competitive performance withend-to-end trained CD models while remaining agnostic to the problemdimensionality.</description>
      <author>example@mail.com (Michael Mayr, Georgios C. Chasparis)</author>
      <guid isPermaLink="false">2508.13111v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields</title>
      <link>http://arxiv.org/abs/2508.12888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;aims-PAX是一个自动化的多轨迹主动学习框架，简化了机器学习力场(MLFF)的开发过程，提供了模块化、高性能的工作流程，支持灵活采样和可扩展训练，与FHI-aims和先进ML模型无缝集成，在测试案例中显著减少了参考计算需求并加速了AL周期，是学术和工业环境中下一代ML驱动原子模拟的强大平台。&lt;h4&gt;背景&lt;/h4&gt;机器学习力场(MLFF)的最新进展显著扩展了原子模拟的能力，这一进展突显了对可靠参考数据集、准确MLFF以及高效主动学习策略的迫切需求，这些对于复杂化学和材料系统的稳健建模至关重要。&lt;h4&gt;目的&lt;/h4&gt;介绍aims-PAX，一个自动化的多轨迹主动学习框架，旨在简化MLFF的开发过程，为专家和新手用户提供支持。&lt;h4&gt;方法&lt;/h4&gt;aims-PAX是一个模块化、高性能的工作流程，结合灵活的采样和可扩展的训练，支持CPU和GPU架构；基于广泛采用的从头计算代码FHI-aims构建，无缝集成最先进的ML模型，并支持使用通用模型进行预训练以便快速部署到各种系统中。&lt;h4&gt;主要发现&lt;/h4&gt;在高柔性肽和块状CsPbI₃钙钛矿两个具有挑战性的系统上，aims-PAX将所需的参考计算数量减少了高达两个数量级，通过优化资源利用实现了AL周期时间超过20倍的加速。&lt;h4&gt;结论&lt;/h4&gt;aims-PAX是一个强大且通用的平台，适用于下一代机器学习驱动的原子模拟，可在学术和工业环境中应用。&lt;h4&gt;翻译&lt;/h4&gt;机器学习力场(MLFF)的最新进展显著扩展了原子模拟的能力。这一进展突显了对可靠参考数据集、准确MLFF以及关键的高效主动学习策略的迫切需求，以实现复杂化学和材料系统的稳健建模。在此，我们介绍了aims-PAX，一个自动化的多轨迹主动学习框架，简化了MLFF的开发。aims-PAX专为专家和新手设计，提供模块化、高性能的工作流程，将灵活采样与可扩展训练相结合，支持CPU和GPU架构。基于广泛采用的从头计算代码FHI-aims构建，该框架无缝集成最先进的ML模型，并支持使用通用(或'基础')模型进行预训练，以便快速部署到各种系统中。我们在两个具有挑战性的系统上展示了aims-PAX的能力：高柔性肽和块状CsPbI₃钙钛矿。在这些案例中，aims-PAX将所需的参考计算数量减少了高达两个数量级，并通过优化资源利用实现了AL周期时间超过20倍的加速。这使aims-PAX成为学术和工业环境中下一代机器学习驱动原子模拟的强大而通用的平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in machine learning force fields (MLFF) have significantlyextended the capabilities of atomistic simulations. This progress highlightsthe critical need for reliable reference datasets, accurate MLFFs, and,crucially, efficient active learning strategies to enable robust modeling ofcomplex chemical and materials systems. Here, we introduce aims-PAX, anautomated, multi-trajectory active learning framework that streamlines thedevelopment of MLFFs. Designed for both experts and newcomers, aims-PAX offersa modular, high-performance workflow that couples flexible sampling withscalable training across CPU and GPU architectures. Built on the widely adoptedab initio code FHI-aims, the framework seamlessly integrates withstate-of-the-art ML models and supports pretraining using general-purpose (or"foundational") models for rapid deployment in diverse systems. We demonstratethe capabilities of aims-PAX on two challenging systems: a highly flexiblepeptide and bulk CsPbI$_3$ perovskite. Across these cases, aims-PAX achieves areduction of up to two orders of magnitude in the number of required referencecalculations and enables over 20x speedup in AL cycle time through optimizedresource utilization. This positions aims-PAX as a powerful and versatileplatform for next-generation ML-driven atomistic simulations in both academicand industrial settings.</description>
      <author>example@mail.com (Tobias Henkes, Shubham Sharma, Alexandre Tkatchenko, Mariana Rossi, Igor Poltavskyi)</author>
      <guid isPermaLink="false">2508.12888v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model for Skeleton-Based Human Action Understanding</title>
      <link>http://arxiv.org/abs/2508.12586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TPAMI, Code is available at:  https://github.com/wengwanjiang/FoundSkelModel&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个统一的基于骨骼的密集表示学习(USDRL)框架，作为基于骨骼的人类动作理解的基础模型，解决了现有方法在可扩展性和泛化能力方面的不足。&lt;h4&gt;背景&lt;/h4&gt;人类动作理解是智能运动感知领域的基础，基于骨骼的动作表示具有模态和设备无关性，在类人机器人控制与交互中有应用潜力，但现有方法缺乏处理多样化任务所需的可扩展性和泛化能力，且缺乏可适应广泛任务的基础模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的基于骨骼的密集表示学习框架，作为基于骨骼的人类动作理解的基础模型，提高动作理解的可扩展性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;USDRL框架包含三个模块：1)基于Transformer的密集时空编码器(DSTE)，采用两个并行流学习时间动态和空间结构特征；2)多粒度特征解相关(MG-FD)，在时间、空间和实例域上协同执行特征解相关，减少维度冗余并增强信息提取；3)多视角一致性训练(MPCT)，采用多视角和多模态自监督一致性训练，前者增强高级语义学习并减轻低级差异影响，后者促进信息丰富的多模态特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;在9个基于骨骼的动作理解任务上的25个基准测试中进行了广泛实验，结果表明该方法显著优于当前最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;这项工作有望拓宽基于骨骼的动作理解研究范围，并鼓励更多关注密集预测任务的发展。&lt;h4&gt;翻译&lt;/h4&gt;人类动作理解作为智能运动感知领域的基础支柱，在类人机器人控制和交互中具有潜在应用。然而，现有工作往往缺乏处理多样化动作理解任务所需的可扩展性和泛化能力。本文提出了统一的基于骨骼的密集表示学习(USDRL)框架，作为基于骨骼的人类动作理解的基础模型。USDRL包含基于Transformer的密集时空编码器(DSTE)、多粒度特征解相关(MG-FD)和多视角一致性训练(MPCT)三个模块。DSTE采用两个并行流学习时间动态和空间结构特征；MG-FD在时间、空间和实例域上协同执行特征解相关，减少维度冗余并增强信息提取；MPCT采用多视角和多模态自监督一致性训练。我们在9个基于骨骼的动作理解任务上的25个基准测试中进行了广泛实验，该方法显著优于当前最先进的方法。我们希望这项工作能拓宽基于骨骼的动作理解研究范围，并鼓励更多关注密集预测任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于骨架的人体动作理解缺乏可扩展性和泛化能力的问题，特别是无法有效处理多样化的密集预测任务(如动作检测、动作预测等)。这个问题很重要，因为骨架数据作为模态和设备无关的人体表示，具有计算高效、隐私保护好的优势，在机器人控制、人机交互、虚拟环境等领域有广泛应用价值。现有方法要么需要大量标注数据，要么只关注粗粒度表示而忽略了细粒度表示对密集预测任务的重要性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有自监督骨架表示学习方法的两类主要范式(掩码序列建模和对比学习)的局限性，然后提出使用特征解相关作为新的自监督学习范式。他们设计了包含三个核心组件的USDRL框架：密集时空编码器(DSTE)、多粒度特征解相关(MG-FD)和多视角一致性训练(MPCT)。该方法借鉴了Barlow Twins和VICREG的特征解相关思想，但将其应用于骨架动作理解；参考了传统的双流架构；受到对比学习和掩码序列建模的启发，但通过特征解相关简化了流程；在多模态融合方面采用了早期融合策略以提高效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多粒度特征解相关学习密集表示，同时保持样本内部一致性和样本间可区分性，使用双流编码器分别捕获时间动态和空间结构特征，并引入多视角一致性训练增强模型鲁棒性。整体流程：1)输入增强的3D骨架序列，重塑为时间域和空间域两个视图；2)通过嵌入层映射到特征空间；3)特征通过DSTE编码器生成密集表示；4)应用MaxPooling和连接操作得到压缩向量；5)通过三个领域特定投影器映射到新空间；6)应用多粒度特征解相关损失函数；7)在训练过程中使用多视角一致性训练增强表示学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出首个基于骨架的基础模型框架(USDRL)，专注于密集表示学习；2)设计多粒度特征解相关(MG-FD)方法；3)提出密集时空编码器(DSTE)；4)引入多视角一致性训练(MPCT)；5)全面适应多种下游任务。相比之前工作，不同之处在于：不需要解码器或复杂掩码策略(对比掩码序列建模)；不需要动量编码器或记忆库(对比对比学习)；同时学习粗粒度和细粒度表示；特别关注密集预测任务；使用早期融合策略处理多模态数据，计算效率更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了首个基于骨架的统一密集表示学习框架(USDRL)，通过多粒度特征解相关和多视角一致性训练，实现了对多种人体动作理解任务(包括密集预测任务)的高效泛化和优异性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human action understanding serves as a foundational pillar in the field ofintelligent motion perception. Skeletons serve as a modality- anddevice-agnostic representation for human modeling, and skeleton-based actionunderstanding has potential applications in humanoid robot control andinteraction. \RED{However, existing works often lack the scalability andgeneralization required to handle diverse action understanding tasks. There isno skeleton foundation model that can be adapted to a wide range of actionunderstanding tasks}. This paper presents a Unified Skeleton-based DenseRepresentation Learning (USDRL) framework, which serves as a foundational modelfor skeleton-based human action understanding. USDRL consists of aTransformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained FeatureDecorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). TheDSTE module adopts two parallel streams to learn temporal dynamic and spatialstructure features. The MG-FD module collaboratively performs featuredecorrelation across temporal, spatial, and instance domains to reducedimensional redundancy and enhance information extraction. The MPCT moduleemploys both multi-view and multi-modal self-supervised consistency training.The former enhances the learning of high-level semantics and mitigates theimpact of low-level discrepancies, while the latter effectively facilitates thelearning of informative multimodal features. We perform extensive experimentson 25 benchmarks across across 9 skeleton-based action understanding tasks,covering coarse prediction, dense prediction, and transferred prediction. Ourapproach significantly outperforms the current state-of-the-art methods. Wehope that this work would broaden the scope of research in skeleton-basedaction understanding and encourage more attention to dense prediction tasks.</description>
      <author>example@mail.com (Hongsong Wang, Wanjiang Weng, Junbo Wang, Fang Zhao, Guo-Sen Xie, Xin Geng, Liang Wang)</author>
      <guid isPermaLink="false">2508.12586v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>jXBW: Fast Substructure Search in Large-Scale JSONL Datasets for Foundation Model Applications</title>
      <link>http://arxiv.org/abs/2508.12536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;jXBW是一种快速的大规模JSONL数据集子结构搜索方法，显著提高了处理效率。&lt;h4&gt;背景&lt;/h4&gt;在JSON Lines数据集中进行子结构搜索对现代应用如基础模型提示工程至关重要，但现有方法因完全树遍历和子树匹配导致计算成本过高。&lt;h4&gt;目的&lt;/h4&gt;提出jXBW方法，解决大规模JSONL数据集子结构搜索的性能问题。&lt;h4&gt;方法&lt;/h4&gt;jXBW包含三项技术贡献：合并树表示保留个体身份；基于扩展伯劳斯-惠勒变换的简洁数据结构支持高效树导航；三步子结构搜索算法结合路径分解、祖先计算和自适应树标识符收集。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明jXBW优于现有方法，小数据集提速16倍，大数据集提速高达4,700倍，相比XML处理提速超过6×10^6倍，同时保持有竞争力的内存使用。&lt;h4&gt;结论&lt;/h4&gt;jXBW是一种高效的子结构搜索方法，显著提升了处理大规模JSONL数据集的性能。&lt;h4&gt;翻译&lt;/h4&gt;在JSON Lines数据集中进行子结构搜索对于现代应用（如基础模型中的提示工程）至关重要，但现有方法由于完全树遍历和子树匹配而存在计算成本过高的问题。我们提出了jXBW，一种用于大规模JSONL数据集子结构搜索的快速方法。我们的方法有三个关键技术贡献：(i)一种合并树表示，通过合并多个JSON对象的树同时保留个体身份；(ii)一种基于扩展伯劳斯-惠勒变换的简洁数据结构，支持高效的树导航和子路径搜索；(iii)一种有效的三步子结构搜索算法，结合路径分解、祖先计算和自适应树标识符收集，确保正确性同时避免完全树遍历。在真实数据集上的实验评估表明，jXBW持续优于现有方法，对于较小的数据集实现16倍的速度提升，对于较大的数据集实现高达4,700倍的速度提升（相比基于树的方法），相比基于XML的处理速度提升超过6×10^6倍，同时保持有竞争力的内存使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Substructure search in JSON Lines (JSONL) datasets is essential for modernapplications such as prompt engineering in foundation models, but existingmethods suffer from prohibitive computational costs due to exhaustive treetraversal and subtree matching. We present jXBW, a fast method for substructuresearch on large-scale JSONL datasets. Our method makes three key technicalcontributions: (i) a merged tree representation built by merging trees ofmultiple JSON objects while preserving individual identities, (ii) a succinctdata structure based on the eXtended Burrows-Wheeler Transform that enablesefficient tree navigation and subpath search, and (iii) an efficient three-stepsubstructure search algorithm that combines path decomposition, ancestorcomputation, and adaptive tree identifier collection to ensure correctnesswhile avoiding exhaustive tree traversal. Experimental evaluation on real-worlddatasets demonstrates that jXBW consistently outperforms existing methods,achieving speedups of 16$\times$ for smaller datasets and up to 4,700$\times$for larger datasets over tree-based approaches, and more than 6$\times$10$^6$over XML-based processing while maintaining competitive memory usage.</description>
      <author>example@mail.com (Yasuo Tabei)</author>
      <guid isPermaLink="false">2508.12536v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing</title>
      <link>http://arxiv.org/abs/2508.12409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;S5是首个可扩展的半监督语义分割框架，用于遥感领域，通过大规模未标记数据预训练和混合专家多数据集微调方法，显著提升了遥感基础模型在土地覆盖分割和物体检测任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;半监督语义分割通过伪标签和一致性学习利用未标记数据推动了遥感分析发展，但现有研究依赖小规模数据集和模型，限制了实际应用性。&lt;h4&gt;目的&lt;/h4&gt;提出S5框架，利用大量通常因像素级标注成本高而未被充分利用的未标记地球观测数据，提高遥感应用的实用性和性能。&lt;h4&gt;方法&lt;/h4&gt;构建RS4P-1M数据集，采用基于熵的过滤和多样性扩展策略；在大规模语料上预训练不同大小的遥感基础模型；采用基于混合专家的多数据集微调方法，使模型高效适应多个遥感基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;所得到的RSFMs在所有基准测试上实现了最先进的性能，证明了扩展半监督学习用于遥感应用的可行性。&lt;h4&gt;结论&lt;/h4&gt;S5框架证明了半监督学习在遥感领域的可扩展性，通过大规模数据集和模型实现了性能提升，所有数据集、代码和模型将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;半监督语义分割(S4)通过伪标签和一致性学习利用未标记数据推动了遥感(RS)分析的发展。然而，现有S4研究通常依赖小规模数据集和模型，限制了其实际适用性。为解决这一问题，我们提出了S5，这是首个用于遥感的可扩展半监督语义分割框架，它解锁了大量未标记地球观测数据的潜力，这些数据通常因像素级标注成本高而未被充分利用。基于现有的大规模RS数据集，S5引入了一种结合基于熵的过滤和多样性扩展的数据选择策略，从而创建了RS4P-1M数据集。使用此数据集，我们通过在这个大规模语料上预训练不同大小的遥感基础模型(RSFMs)，系统性地扩展了S4方法，显著提高了它们在土地覆盖分割和物体检测任务上的性能。此外，在微调过程中，我们集成了基于混合专家(MoE)的多数据集微调方法，使模型能够以更少的参数高效适应多个RS基准测试。这种方法提高了RSFMs在多样化RS基准测试上的泛化能力和多功能性。所得到的RSFMs在所有基准测试上实现了最先进的性能，证明了扩展半监督学习用于RS应用的可行性。所有数据集、代码和模型将在https://github.com/MiliLab/S5上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)analysis by leveraging unlabeled data through pseudo-labeling and consistencylearning. However, existing S4 studies often rely on small-scale datasets andmodels, limiting their practical applicability. To address this, we propose S5,the first scalable framework for semi-supervised semantic segmentation in RS,which unlocks the potential of vast unlabeled Earth observation data typicallyunderutilized due to costly pixel-level annotations. Built upon existinglarge-scale RS datasets, S5 introduces a data selection strategy thatintegrates entropy-based filtering and diversity expansion, resulting in theRS4P-1M dataset. Using this dataset, we systematically scales S4 methods bypre-training RS foundation models (RSFMs) of varying sizes on this extensivecorpus, significantly boosting their performance on land cover segmentation andobject detection tasks. Furthermore, during fine-tuning, we incorporate aMixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, whichenables efficient adaptation to multiple RS benchmarks with fewer parameters.This approach improves the generalization and versatility of RSFMs acrossdiverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performanceacross all benchmarks, underscoring the viability of scaling semi-supervisedlearning for RS applications. All datasets, code, and models will be releasedat https://github.com/MiliLab/S5</description>
      <author>example@mail.com (Liang Lv, Di Wang, Jing Zhang, Lefei Zhang)</author>
      <guid isPermaLink="false">2508.12409v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization</title>
      <link>http://arxiv.org/abs/2508.12292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为HuBERT-VIC的噪声鲁棒语音基础模型，通过方差、不变性和协方差正则化目标提高模型在噪声环境下的性能。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型在噪声环境下表现不佳，因为大多数模型主要在干净数据上训练，当遇到噪声语音时会出现性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决语音基础模型在噪声环境下的鲁棒性问题，提高模型对不同类型噪声的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出HuBERT-VIC模型，采用方差、不变性和协方差正则化(VICReg)目标来调整噪声语音表示的统计特性，使模型能够捕捉多样化的声学特征。&lt;h4&gt;主要发现&lt;/h4&gt;与在噪声语音上预训练的基线模型相比，HuBERT-VIC在LibriSpeech测试干净集上显示出23.3%的相对性能提升，在测试其他集上显示出13.2%的相对性能提升。&lt;h4&gt;结论&lt;/h4&gt;通过引入方差、不变性和协方差正则化目标，可以有效提高语音基础模型在噪声环境下的鲁棒性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型中的噪声鲁棒性一直是一个关键挑战，因为大多数模型主要在干净数据上训练，当模型暴露在有噪声的语音中时会出现性能下降。为了解决这个问题，我们提出了HuBERT-VIC，这是一种具有方差、不变性和协方差正则化目标的噪声鲁棒语音基础模型。这些目标调整了噪声语音表示的统计特性，使模型能够捕捉多样化的声学特征，并提高对不同类型噪声的泛化能力。当应用于HuBERT时，与在噪声语音上预训练的基线模型相比，我们的模型在LibriSpeech测试干净集上显示出23.3%的相对性能提升，在测试其他集上显示出13.2%的相对性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Noise robustness in speech foundation models (SFMs) has been a criticalchallenge, as most models are primarily trained on clean data and experienceperformance degradation when the models are exposed to noisy speech. To addressthis issue, we propose HuBERT-VIC, a noise-robust SFM with variance,in-variance, and covariance regularization (VICReg) objectives. Theseobjectives adjust the statistics of noisy speech representations, enabling themodel to capture diverse acoustic characteristics and improving thegeneralization ability across different types of noise. When applied to HuBERT,our model shows relative performance improvements of 23.3% on LibriSpeechtest-clean and 13.2% on test-other, compared to the baseline model pre-trainedon noisy speech.</description>
      <author>example@mail.com (Hyebin Ahn, Kangwook Jang, Hoirin Kim)</author>
      <guid isPermaLink="false">2508.12292v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval</title>
      <link>http://arxiv.org/abs/2508.12290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLAIR的方法，用于处理弱监督的零样本跨域图像检索问题，通过优化大型基础模型生成的噪声伪标签，设计多种对比损失函数，以及学习跨域映射函数来提升检索性能。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型（如CLIP）能够轻松为大量未标记数据生成伪标签，这使得传统的无监督零样本跨域图像检索方法变得不那么相关。&lt;h4&gt;目的&lt;/h4&gt;研究弱监督的零样本跨域图像检索（WSZS-CDIR），使用大型基础模型生成的噪声伪标签。&lt;h4&gt;方法&lt;/h4&gt;提出CLAIR方法，通过CLIP文本和图像特征之间的相似性得到的置信分数来优化噪声伪标签；设计实例间和簇间对比损失将图像编码到类感知的潜在空间；设计域间对比损失缓解域差异；使用闭式学习跨域映射函数对齐图像特征；引入可学习提示增强处理新类别的零样本泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在TUBerlin、Sketchy、Quickdraw和DomainNet零样本数据集上的实验表明，CLAIR相比现有最先进方法持续表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;CLAIR方法在弱监督的零样本跨域图像检索任务中表现优异，通过多种技术创新有效提升了检索性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，能够轻松为大量未标记数据生成伪标签的大型基础模型的增长，使得无监督的零样本跨域图像检索（UZS-CDIR）变得不那么相关。因此，在本文中，我们将注意力转向了由大型基础模型（如CLIP）生成噪声伪标签的弱监督ZS-CDIR（WSZS-CDIR）。为此，我们提出CLAIR，通过CLIP文本和图像特征之间的相似性得到的置信分数来优化噪声伪标签。此外，我们设计了实例间和簇间对比损失，将图像编码到类感知的潜在空间，以及域间对比损失来缓解域差异。我们还使用闭式学习了一种新的跨域映射函数，仅使用CLIP文本嵌入将图像特征从一个域投影到另一个域，从而进一步对齐图像特征以进行检索。最后，我们通过引入一组额外的可学习提示来增强CLAIR的零样本泛化能力，以处理新类别。我们在TUBerlin、Sketchy、Quickdraw和DomainNet零样本数据集上进行了大量实验，其中我们的CLAIR与现有最先进方法相比持续表现出优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent growth of large foundation models that can easily generatepseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-ShotCross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, wetherefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) withnoisy pseudo labels generated by large foundation models such as CLIP. To thisend, we propose CLAIR to refine the noisy pseudo-labels with a confidence scorefrom the similarity between the CLIP text and image features. Furthermore, wedesign inter-instance and inter-cluster contrastive losses to encode imagesinto a class-aware latent space, and an inter-domain contrastive loss toalleviate domain discrepancies. We also learn a novel cross-domain mappingfunction in closed-form, using only CLIP text embeddings to project imagefeatures from one domain to another, thereby further aligning the imagefeatures for retrieval. Finally, we enhance the zero-shot generalizationability of our CLAIR to handle novel categories by introducing an extra set oflearnable prompts. Extensive experiments are carried out using TUBerlin,Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIRconsistently shows superior performance compared to existing state-of-the-artmethods.</description>
      <author>example@mail.com (Chor Boon Tan, Conghui Hu, Gim Hee Lee)</author>
      <guid isPermaLink="false">2508.12290v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting</title>
      <link>http://arxiv.org/abs/2508.12260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Mantis是一种基于机制模拟训练的基础模型，能够在没有疾病特定数据和专家调优的情况下，跨疾病、地区和结果进行准确的传染病预测，即使在历史数据有限的环境中也能工作。&lt;h4&gt;背景&lt;/h4&gt;传染病预测在新型疫情爆发或资源有限的环境中受到限制，因为需要疾病特定数据、专门训练和专家调优。&lt;h4&gt;目的&lt;/h4&gt;开发Mantis模型，使其能够开箱即用地跨疾病、地区和结果进行预测，即使在历史数据有限的情况下也能提供准确预测。&lt;h4&gt;方法&lt;/h4&gt;Mantis构建在超过4亿个模拟日的疫情动态数据上，涵盖多种病原体、传播方式、干预措施和监测伪影，训练过程中不需要真实世界数据。&lt;h4&gt;主要发现&lt;/h4&gt;尽管训练时不需要真实世界数据，但Mantis在六种疾病上优于39个专家调优的模型，包括CDC的COVID-19预测中心的所有模型；Mantis能够推广到新的流行病学领域，捕捉基本传染动态；具有机制上的可解释性，能识别预测背后的潜在驱动因素；能提供8周时间范围的准确预测，使可用范围增加一倍以上。&lt;h4&gt;结论&lt;/h4&gt;Mantis具有通用性、可解释性和在传统模型失效的地方可部署的特点，为下一代疾病预测系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;在新型疫情爆发或资源有限的环境中，传染病预测一直受到需要疾病特定数据、专门训练和专家调优的限制。我们介绍了Mantis，这是一个完全基于机制模拟训练的基础模型，使其能够在没有历史数据的情况下，跨疾病、地区和结果进行开箱即用的预测。Mantis构建在超过4亿个模拟日的疫情动态数据上，涵盖多种病原体、传播方式、干预措施和监测伪影。尽管训练时不需要真实世界数据，但Mantis在六种疾病上优于我们测试的39个专家调优模型，包括CDC的COVID-19预测中心的所有模型。Mantis能够推广到新的流行病学领域，包括具有保留传播机制的疾病，证明它捕捉了基本的传染动态。重要的是，Mantis具有机制上的可解释性，使公共卫生决策者能够识别预测背后的潜在驱动因素。最后，Mantis能够提供8周时间范围的准确预测，使大多数模型的可用范围增加一倍以上，能够主动进行公共卫生规划。这些能力共同使Mantis成为下一代疾病预测系统的基础：通用、可解释，并在传统模型失效的地方可部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infectious disease forecasting in novel outbreaks or low resource settingshas been limited by the need for disease-specific data, bespoke training, andexpert tuning. We introduce Mantis, a foundation model trained entirely onmechanistic simulations, which enables out-of-the-box forecasting acrossdiseases, regions, and outcomes, even in settings with limited historical data.Mantis is built on over 400 million simulated days of outbreak dynamicsspanning diverse pathogens, transmission modes, interventions, and surveillanceartifacts. Despite requiring no real-world data during training, Mantisoutperformed 39 expert-tuned models we tested across six diseases, includingall models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novelepidemiological regimes, including diseases with held-out transmissionmechanisms, demonstrating that it captures fundamental contagion dynamics.Critically, Mantis is mechanistically interpretable, enabling public healthdecision-makers to identify the latent drivers behind its predictions. Finally,Mantis delivers accurate forecasts at 8-week horizons, more than doubling theactionable range of most models, enabling proactive public health planning.Together, these capabilities position Mantis as a foundation fornext-generation disease forecasting systems: general, interpretable, anddeployable where traditional models fail.</description>
      <author>example@mail.com (Carson Dudley, Reiden Magdaleno, Christopher Harding, Ananya Sharma, Emily Martin, Marisa Eisenberg)</author>
      <guid isPermaLink="false">2508.12260v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>What do Speech Foundation Models Learn? Analysis and Applications</title>
      <link>http://arxiv.org/abs/2508.12255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ph.D. Thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一个关于语音基础模型(SFMs)的研究，通过分析框架研究SFM中编码的知识，并在口语理解任务上评估其性能，为SFM的未来发展和应用提供指导。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型被设计为各种语音处理任务的通用表示。过去五年中，自监督和监督预训练模型取得了显著成功，但对SFMs获取的知识理解仍然滞后。&lt;h4&gt;目的&lt;/h4&gt;研究SFMs中编码的声学和语言学知识，评估其在口语理解任务上的性能，并提供工具和数据集以促进对SFMs的更好理解和应用。&lt;h4&gt;方法&lt;/h4&gt;提出轻量级分析框架，使用统计工具和无训练任务研究SFM层中编码的知识；进行跨多个SFM和统计工具的比较研究；为口语理解评估基准贡献了命名实体识别(NER)和命名实体定位(NEL)任务；开发基于SFM的NER和NEL方法；评估不同SFM和适应策略对端到端SLU模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;分析性见解对下游任务性能有具体影响；利用SFMs的端到端模型可以超越传统的级联方法；不同SFM和适应策略对SLU模型性能有显著影响。&lt;h4&gt;结论&lt;/h4&gt;论文解决了关于SFMs的先前未解答的问题，提供了工具和数据集以促进对SFMs的更好理解，使社区能够为未来模型开发和采用做出明智的设计选择。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型(SFMs)被设计为各种语音处理任务的通用表示。过去五年中，大量成功的自监督和监督预训练模型在各种下游任务上取得了令人印象深刻的性能。尽管SFMs的种类不断增长，但我们对其获取知识的理解仍然滞后。本文提出了一个轻量级分析框架，使用统计工具和无训练任务来研究SFM层中编码的声学和语言学知识。我们在多个SFM和统计工具之间进行了比较研究。我们的研究还表明，分析性见解对下游任务性能有具体影响。SFM的最终有效性取决于其在语音应用上的性能。然而，目前尚不清楚其优势是否扩展到需要比广泛研究的语音识别等任务更深层理解的口语理解(SLU)任务。对SLU的探索有限主要是由于缺乏相关数据集。为此，本文为口语理解评估基准贡献了任务，特别是口语命名实体识别(NER)和命名实体定位(NEL)。我们开发了基于SFM的NER和NEL方法，并发现利用SFMs的端到端(E2E)模型可以超越传统的级联(语音识别后接文本模型)方法。此外，我们评估了不同SFM和适应策略对端到端SLU模型在任务性能上的影响。总体而言，本文解决了关于SFMs的先前未解答的问题，提供了工具和数据集以促进我们的理解，并使社区能够为未来模型开发和采用做出明智的设计选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models (SFMs) are designed to serve as general-purposerepresentations for a wide range of speech-processing tasks. The last fiveyears have seen an influx of increasingly successful self-supervised andsupervised pre-trained models with impressive performance on various downstreamtasks.  Although the zoo of SFMs continues to grow, our understanding of theknowledge they acquire lags behind. This thesis presents a lightweight analysisframework using statistical tools and training-free tasks to investigate theacoustic and linguistic knowledge encoded in SFM layers. We conduct acomparative study across multiple SFMs and statistical tools. Our study alsoshows that the analytical insights have concrete implications for downstreamtask performance.  The effectiveness of an SFM is ultimately determined by its performance onspeech applications. Yet it remains unclear whether the benefits extend tospoken language understanding (SLU) tasks that require a deeper understandingthan widely studied ones, such as speech recognition. The limited explorationof SLU is primarily due to a lack of relevant datasets. To alleviate that, thisthesis contributes tasks, specifically spoken named entity recognition (NER)and named entity localization (NEL), to the Spoken Language UnderstandingEvaluation benchmark. We develop SFM-based approaches for NER and NEL, and findthat end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded(speech recognition followed by a text model) approaches. Further, we evaluateE2E SLU models across SFMs and adaptation strategies to assess the impact ontask performance.  Collectively, this thesis tackles previously unanswered questions about SFMs,providing tools and datasets to further our understanding and to enable thecommunity to make informed design choices for future model development andadoption.</description>
      <author>example@mail.com (Ankita Pasad)</author>
      <guid isPermaLink="false">2508.12255v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>DermINO: Hybrid Pretraining for a Versatile Dermatology Foundation Model</title>
      <link>http://arxiv.org/abs/2508.12190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了DermNIO，一个皮肤病领域的基础模型，通过混合预训练框架解决了现有AI工具在皮肤病诊断中的局限性，在各种临床任务上表现出色，甚至在诊断准确率上超过了专业皮肤科医生。&lt;h4&gt;背景&lt;/h4&gt;皮肤病对全球医疗系统造成巨大负担，影响高达70%的人口，诊断过程复杂，且资源有限地区皮肤科医生严重短缺。现有AI工具虽然显示出潜力，但通常依赖大型手动标记数据集，仅针对狭窄特定任务构建，在现实环境中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个更通用、有效的皮肤病AI诊断工具，解决现有模型的局限性，提高在真实世界环境中的诊断效果和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;DermNIO在432,776张图像组成的精选数据集上训练（来自公共存储库、网络图像和专有收藏），采用新颖的混合预训练框架，通过半监督学习和知识引导的原型初始化增强了自监督学习范式。&lt;h4&gt;主要发现&lt;/h4&gt;DermNIO在20个数据集上评估时持续优于最先进模型，在高层次临床应用（如恶性肿瘤分类、疾病严重程度分级、多类别诊断和皮肤病图像描述）和低级任务（如皮肤病变分割）中均表现优异。在隐私保护的联邦学习场景以及不同皮肤类型和性别中表现出强大鲁棒性。盲法研究中，DermNIO达到95.79%的诊断准确率（临床医生为73.66%），AI辅助使临床医生表现提高17.21%。&lt;h4&gt;结论&lt;/h4&gt;DermNIO通过创新的训练方法和大规模数据集，成功解决了现有皮肤病AI模型的局限性，在各种临床任务上展现出卓越性能和泛化能力，具有显著的临床应用价值和辅助诊断潜力。&lt;h4&gt;翻译&lt;/h4&gt;皮肤病对全球医疗系统造成巨大负担，其高发病率（影响高达70%的人口）、复杂的诊断过程以及资源有限地区皮肤科医生的严重短缺是主要驱动因素。虽然人工智能工具在皮肤病图像分析方面显示出潜力，但当前模型存在局限性——它们通常依赖大型手动标记数据集，并且是为狭窄、特定任务构建的，因此在现实环境中效果不佳。为解决这些局限性，我们提出了DermNIO，一个皮肤病领域的基础模型。在来自三个来源（公共存储库、网络图像和专有收藏）的432,776张图像组成的精选数据集上进行训练，DermNIO采用了一种新颖的混合预训练框架，通过半监督学习和知识引导的原型初始化增强了自监督学习范式。这种集成方法不仅加深了对复杂皮肤病状况的理解，还显著提高了在各种临床任务上的泛化能力。在20个数据集上的评估表明，DermNIO在广泛任务中持续优于最先进模型。它在包括恶性肿瘤分类、疾病严重程度分级、多类别诊断和皮肤病图像描述等高层次临床应用中表现出色，同时在皮肤病变分割等低级任务上也实现了最先进性能。此外，DermNIO在隐私保护的联邦学习场景以及不同皮肤类型和性别中表现出强大的鲁棒性。在23名皮肤科参与的盲法读者研究中，DermNIO达到95.79%的诊断准确率（而临床医生为73.66%），AI辅助使临床医生的表现提高了17.21%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skin diseases impose a substantial burden on global healthcare systems,driven by their high prevalence (affecting up to 70% of the population),complex diagnostic processes, and a critical shortage of dermatologists inresource-limited areas. While artificial intelligence(AI) tools havedemonstrated promise in dermatological image analysis, current models facelimitations-they often rely on large, manually labeled datasets and are builtfor narrow, specific tasks, making them less effective in real-world settings.To tackle these limitations, we present DermNIO, a versatile foundation modelfor dermatology. Trained on a curated dataset of 432,776 images from threesources (public repositories, web-sourced images, and proprietary collections),DermNIO incorporates a novel hybrid pretraining framework that augments theself-supervised learning paradigm through semi-supervised learning andknowledge-guided prototype initialization. This integrated method not onlydeepens the understanding of complex dermatological conditions, but alsosubstantially enhances the generalization capability across various clinicaltasks. Evaluated across 20 datasets, DermNIO consistently outperformsstate-of-the-art models across a wide range of tasks. It excels in high-levelclinical applications including malignancy classification, disease severitygrading, multi-category diagnosis, and dermatological image caption, while alsoachieving state-of-the-art performance in low-level tasks such as skin lesionsegmentation. Furthermore, DermNIO demonstrates strong robustness inprivacy-preserving federated learning scenarios and across diverse skin typesand sexes. In a blinded reader study with 23 dermatologists, DermNIO achieved95.79% diagnostic accuracy (versus clinicians' 73.66%), and AI assistanceimproved clinician performance by 17.21%.</description>
      <author>example@mail.com (Jingkai Xu, De Cheng, Xiangqian Zhao, Jungang Yang, Zilong Wang, Xinyang Jiang, Xufang Luo, Lili Chen, Xiaoli Ning, Chengxu Li, Xinzhu Zhou, Xuejiao Song, Ang Li, Qingyue Xia, Zhou Zhuang, Hongfei Ouyang, Ke Xue, Yujun Sheng, Rusong Meng, Feng Xu, Xi Yang, Weimin Ma, Yusheng Lee, Dongsheng Li, Xinbo Gao, Jianming Liang, Lili Qiu, Nannan Wang, Xianbo Zuo, Cui Yong)</author>
      <guid isPermaLink="false">2508.12190v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Infusing fine-grained visual knowledge to Vision-Language Models</title>
      <link>http://arxiv.org/abs/2508.12137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCVW 2025 accepted paper. Workshop name: "What is Next in Multimodal  Foundation Models?"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对视觉语言模型（VLMs）的微调方法，旨在解决细粒度开放集视觉检索任务中预训练模型表现不佳的问题，同时避免灾难性遗忘，保留模型的通用视觉和跨模态能力。&lt;h4&gt;背景&lt;/h4&gt;大规模对比预训练产生的VLMs能广泛用于视觉和多模态任务，但在细粒度开放集视觉检索任务中表现不够理想，需要使用特定领域标注样本对视觉编码器进行微调。然而，直接微调通常会导致灾难性遗忘，严重削弱模型的通用能力。&lt;h4&gt;目的&lt;/h4&gt;设计一种微调方法，在实现细粒度领域适应的同时，有效保留预训练VLM的广泛多模态知识，解决验证集设计和超参数调整中的关键问题，确保方法的可复现性和鲁棒泛化。&lt;h4&gt;方法&lt;/h4&gt;从持续学习文献中汲取灵感，系统分析标准正则化技术并提出高效有效的组合策略；解决验证集设计和超参数调整中的关键问题；在细粒度和粗粒度图像-图像和图像-文本检索基准上评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在各种检索任务中取得了一致的强有力结果，特别值得注意的是，在微调过程中不使用任何文本数据或原始文本编码器的情况下，成功保留了视觉-文本对齐能力。&lt;h4&gt;结论&lt;/h4&gt;该微调方法能够在细粒度领域适应与保留预训练VLM的广泛多模态知识之间实现最佳平衡，为细粒度开放集视觉检索任务提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大规模对比预训练产生了强大的视觉语言模型（VLMs），能够为各种视觉和多模态任务生成有效的表示（嵌入）。然而，这些预训练的嵌入对于细粒度开放集视觉检索仍然不够理想，最先进的结果需要使用特定领域的标注样本对视觉编码器进行微调。直接进行这种微调通常会导致灾难性遗忘，严重削弱模型的通用视觉和跨模态能力。在这项工作中，我们提出了一种专门设计的微调方法，旨在实现细粒度领域适应与保留预训练VLM广泛多模态知识之间的最佳平衡。从持续学习文献中汲取灵感，我们系统地分析了旨在保留知识的标准正则化技术，并提出了一种高效有效的组合策略。此外，我们解决了验证集设计和超参数调整中常被忽视但至关重要的方面，以确保在不同数据集和预训练模型上的可复现性和鲁棒泛化。我们在细粒度和粗粒度图像-图像和图像-文本检索基准上广泛评估了我们的方法。我们的方法始终取得强有力结果，值得注意的是在微调过程中不使用任何文本数据或原始文本编码器的情况下保留了视觉-文本对齐。代码和模型见：https://github.com/nikosips/infusing&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale contrastive pre-training produces powerful Vision-and-LanguageModels (VLMs) capable of generating representations (embeddings) effective fora wide variety of visual and multimodal tasks. However, these pretrainedembeddings remain suboptimal for fine-grained open-set visual retrieval, wherestate-of-the-art results require fine-tuning the vision encoder using annotateddomain-specific samples. Naively performing such fine-tuning typically leads tocatastrophic forgetting, severely diminishing the model's general-purposevisual and cross-modal capabilities.  In this work, we propose a fine-tuning method explicitly designed to achieveoptimal balance between fine-grained domain adaptation and retention of thepretrained VLM's broad multimodal knowledge. Drawing inspiration from continuallearning literature, we systematically analyze standard regularizationtechniques aimed at knowledge retention and propose an efficient and effectivecombination strategy. Additionally, we address the commonly overlooked yetcritical aspects of validation set design and hyperparameter tuning to ensurereproducibility and robust generalization across datasets and pretrainedmodels. We extensively evaluate our method on both fine-grained andcoarse-grained image-image and image-text retrieval benchmarks. Our approachconsistently achieves strong results, notably retaining the visual-textalignment without utilizing any text data or the original text encoder duringfine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .</description>
      <author>example@mail.com (Nikolaos-Antonios Ypsilantis, Kaifeng Chen, André Araujo, Ondřej Chum)</author>
      <guid isPermaLink="false">2508.12137v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Generative Medical Event Models Improve with Scale</title>
      <link>http://arxiv.org/abs/2508.12104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Cosmos Medical Event Transformer (CoMET)模型，这是一种基于大规模医疗事件数据预训练的基础模型，能够有效捕获复杂临床动态，支持临床决策和改善患者结局。&lt;h4&gt;背景&lt;/h4&gt;实现个性化医疗需要从患者纵向历程中提取见解，这些历程可视为医疗事件序列。大规模医疗事件数据预训练的基础模型为生成真实世界证据和推广到多样化下游任务提供了有前景的方向。&lt;h4&gt;目的&lt;/h4&gt;开发能够从大规模医疗事件数据中学习的基础模型，建立预训练方法学，研究计算、标记和模型大小的幂律缩放关系。&lt;h4&gt;方法&lt;/h4&gt;使用Epic Cosmos数据集（包含310个医疗系统的16.3亿次就诊记录和3亿份去标识化的纵向健康记录），引入CoMET模型家族，预训练了多达10亿参数的计算最优模型系列。基于患者真实世界历史，CoMET自回归地生成下一个医疗事件，模拟患者健康时间线，并研究了78个真实世界任务。&lt;h4&gt;主要发现&lt;/h4&gt;建立了医疗事件数据最大的缩放定律研究；预训练的CoMET模型在不需要任务特定微调或少样本示例的情况下，通常优于或匹配任务特定的监督模型；CoMET的预测能力随着模型和预训练规模的扩大而持续提高。&lt;h4&gt;结论&lt;/h4&gt;CoMET作为生成性医疗事件基础模型，能够有效捕获复杂的临床动态，提供了可扩展和可推广的框架来支持临床决策、简化医疗保健运营和改善患者结局。&lt;h4&gt;翻译&lt;/h4&gt;实现大规模个性化医疗需要能够从患者纵向历程中提取见解的方法，这些历程可视为一系列医疗事件。在大型医疗事件数据上预训练的基础模型为扩展真实世界证据生成和推广到多样化下游任务提供了有前景的方向。使用Epic Cosmos数据集（包含来自310个医疗系统的3亿份去标识化纵向健康记录中的163亿次就诊记录），我们引入了Cosmos Medical Event Transformer (CoMET)模型家族，这是一系列仅在解码器上的transformer模型，在1.18亿患者代表1150亿个离散医疗事件（1510亿个标记）的数据上进行了预训练。我们展示了医疗事件数据最大的缩放定律研究，建立了预训练方法学，并揭示了计算、标记和模型大小的幂律缩放关系。基于此，我们预训练了一系列多达10亿参数的计算最优模型。基于患者的真实世界历史，CoMET自回归地生成下一个医疗事件，模拟患者健康时间线。我们研究了78个真实世界任务，包括诊断预测、疾病预后和医疗保健运营。值得注意的是，对于一个具有通用预训练和基于模拟推理的基础模型，CoMET在这些任务上通常优于或匹配任务特定的监督模型，而无需任务特定的微调或少样本示例。CoMET的预测能力随着模型和预训练规模的扩大而持续提高。我们的结果表明，CoMET作为一种生成性医疗事件基础模型，能够有效捕获复杂的临床动态，提供了一个可扩展和可推广的框架来支持临床决策、简化医疗保健运营和改善患者结局。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Realizing personalized medicine at scale calls for methods that distillinsights from longitudinal patient journeys, which can be viewed as a sequenceof medical events. Foundation models pretrained on large-scale medical eventdata represent a promising direction for scaling real-world evidence generationand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset withmedical events from de-identified longitudinal health records for 16.3 billionencounters over 300 million unique patient records from 310 health systems, weintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family ofdecoder-only transformer models pretrained on 118 million patients representing115 billion discrete medical events (151 billion tokens). We present thelargest scaling-law study for medical event data, establishing a methodologyfor pretraining and revealing power-law scaling relationships for compute,tokens, and model size. Based on this, we pretrained a series ofcompute-optimal models with up to 1 billion parameters. Conditioned on apatient's real-world history, CoMET autoregressively generates the next medicalevent, simulating patient health timelines. We studied 78 real-world tasks,including diagnosis prediction, disease prognosis, and healthcare operations.Remarkably for a foundation model with generic pretraining and simulation-basedinference, CoMET generally outperformed or matched task-specific supervisedmodels on these tasks, without requiring task-specific fine-tuning or few-shotexamples. CoMET's predictive power consistently improves as the model andpretraining scale. Our results show that CoMET, a generative medical eventfoundation model, can effectively capture complex clinical dynamics, providingan extensible and generalizable framework to support clinical decision-making,streamline healthcare operations, and improve patient outcomes.</description>
      <author>example@mail.com (Shane Waxler, Paul Blazek, Davis White, Daniel Sneider, Kevin Chung, Mani Nagarathnam, Patrick Williams, Hank Voeller, Karen Wong, Matthew Swanhorst, Sheng Zhang, Naoto Usuyama, Cliff Wong, Tristan Naumann, Hoifung Poon, Andrew Loza, Daniella Meeker, Seth Hain, Rahul Shah)</author>
      <guid isPermaLink="false">2508.12104v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>MAPF-World: Action World Model for Multi-Agent Path Finding</title>
      <link>http://arxiv.org/abs/2508.12087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MAPF-World的新型多智能体路径规划方法，这是一种自回归动作世界模型，能够统一情况理解和动作生成，指导超越即时局部观察的决策。&lt;h4&gt;背景&lt;/h4&gt;多智能体路径规划（MAPF）是从指定起始位置到目标位置为多个智能体规划无冲突路径的问题，支撑着多机器人协调、机器人辅助物流等现实世界任务。现有去中心化可学习求解器在大规模MAPF中表现出色，但作为反应式策略模型，它们对环境时间动态和智能体间依赖性的建模有限，导致在复杂长期规划场景中性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有去中心化可学习求解器在复杂长期规划场景中的性能限制，通过提出MAPF-World模型，提高情境感知能力，实现更智能、协调和有远见的决策。&lt;h4&gt;方法&lt;/h4&gt;作者提出了MAPF-World，一种用于MAPF的自回归动作世界模型，通过未来状态和动作预测明确建模环境动态（包括空间特征和时间依赖性）。此外，引入了一个基于现实场景的自动地图生成器，用于捕获实际地图布局，以训练和评估MAPF求解器。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，MAPF-World优于最先进的可学习求解器，展现出对分布外案例的零样本泛化能力。值得注意的是，MAPF-World的模型尺寸小96.5%，数据减少92%。&lt;h4&gt;结论&lt;/h4&gt;MAPF-World通过明确建模环境动态和智能体间依赖关系，解决了现有方法在复杂长期规划场景中的局限性，实现了更智能、协调和有远见的决策，同时显著降低了模型大小和训练数据需求。&lt;h4&gt;翻译&lt;/h4&gt;多智能体路径规划（MAPF）是为多个智能体从指定的起始位置到目标位置规划无冲突路径的问题。它支撑着多种现实世界任务，包括多机器人协调、机器人辅助物流和社会导航。最近去中心化的可学习求解器在大规模MAPF中显示出巨大潜力，特别是当利用基础模型和大型数据集时。然而，这些智能体是反应式策略模型，对环境时间动态和智能体间依赖性的建模有限，导致在复杂长期规划场景中性能下降。为了解决这些限制，我们提出了MAPF-World，这是一种用于MAPF的自回归动作世界模型，它统一了情况理解和动作生成，指导超越即时局部观察的决策。它通过未来状态和动作预测明确建模环境动态（包括空间特征和时间依赖性），从而提高情境感知能力。通过整合这些预测的未来，MAPF-World能够在复杂的多智能体环境中实现更明智、协调和有远见的决策。此外，我们通过引入一个基于现实场景的自动地图生成器来增强MAPF基准，该生成器捕获实际的地图布局，用于训练和评估MAPF求解器。大量实验表明，MAPF-World优于最先进的可学习求解器，展现出对分布外案例的优越零样本泛化能力。值得注意的是，MAPF-World的模型尺寸小96.5%，数据减少92%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent path finding (MAPF) is the problem of planning conflict-freepaths from the designated start locations to goal positions for multipleagents. It underlies a variety of real-world tasks, including multi-robotcoordination, robot-assisted logistics, and social navigation. Recentdecentralized learnable solvers have shown great promise for large-scale MAPF,especially when leveraging foundation models and large datasets. However, theseagents are reactive policy models and exhibit limited modeling of environmentaltemporal dynamics and inter-agent dependencies, resulting in performancedegradation in complex, long-term planning scenarios. To address theselimitations, we propose MAPF-World, an autoregressive action world model forMAPF that unifies situation understanding and action generation, guidingdecisions beyond immediate local observations. It improves situationalawareness by explicitly modeling environmental dynamics, including spatialfeatures and temporal dependencies, through future state and actionsprediction. By incorporating these predicted futures, MAPF-World enables moreinformed, coordinated, and far-sighted decision-making, especially in complexmulti-agent settings. Furthermore, we augment MAPF benchmarks by introducing anautomatic map generator grounded in real-world scenarios, capturing practicalmap layouts for training and evaluating MAPF solvers. Extensive experimentsdemonstrate that MAPF-World outperforms state-of-the-art learnable solvers,showcasing superior zero-shot generalization to out-of-distribution cases.Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduceddata.</description>
      <author>example@mail.com (Zhanjiang Yang, Meng Li, Yang Shen, Yueming Li, Lijun Sun)</author>
      <guid isPermaLink="false">2508.12087v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Towards interpretable prediction of recurrence risk in breast cancer using pathology foundation models</title>
      <link>http://arxiv.org/abs/2508.12025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入MAKO框架，评估了基于组织病理学的基础模型预测乳腺癌复发风险的能力，发现这些模型能够有效替代转录组检测方法。&lt;h4&gt;背景&lt;/h4&gt;PAM50-based ROR-P评分用于指导非转移性、ER阳性、HER2阴性乳腺癌的复发风险分层，但这些转录组检测方法并非普遍可及。组织病理学是常规可用的，可能提供一种可扩展的替代方案。&lt;h4&gt;目的&lt;/h4&gt;开发并评估基于组织病理学的基础模型，用于从H&amp;E染色全切片图像中预测ROR-P评分，以替代转录组检测方法。&lt;h4&gt;方法&lt;/h4&gt;引入MAKO框架，评估12个病理学基础模型和2个非病理学基线，使用基于注意力的多实例学习方法预测ROR-P评分。模型在Carolina乳腺癌研究(CBCS)上进行训练和验证，并在TCGA BRCA上进行外部测试。&lt;h4&gt;主要发现&lt;/h4&gt;多个基础模型在分类、回归和生存任务上优于基线；CONCH实现了最高的ROC AUC；H-optimus-0和Virchow2与连续ROR-P评分显示出最高的相关性；所有病理学模型将CBCS参与者按复发分层的效果类似于转录组ROR-P；肿瘤区域对于高风险预测是必要且充分的；研究人员确定了复发的候选组织生物标志物。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了在精准肿瘤学中，可解释的组织学风险模型的潜力，表明组织病理学可以作为一种有效的替代方法来预测乳腺癌复发风险。&lt;h4&gt;翻译&lt;/h4&gt;转录组检测如基于PAM50的ROR-P评分可指导非转移性、ER阳性、HER2阴性乳腺癌的复发风险分层，但并非普遍可及。组织病理学是常规可用的，可能提供一种可扩展的替代方案。我们引入了MAKO框架，评估了12个病理学基础模型和2个非病理学基线，用于使用基于注意力的多实例学习方法从H&amp;E染色全切片图像预测ROR-P评分。模型在Carolina乳腺癌研究上进行训练和验证，并在TCGA BRCA上进行外部测试。多个基础模型在分类、回归和生存任务上优于基线。CONCH实现了最高的ROC AUC，而H-optimus-0和Virchow2与连续ROR-P评分显示出最高的相关性。所有病理学模型将CBCS参与者按复发分层的效果类似于转录组ROR-P。肿瘤区域对于高风险预测是必要且充分的，我们确定了复发的候选组织生物标志物。这些结果突显了可解释、基于组织学的风险模型在精准肿瘤学中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transcriptomic assays such as the PAM50-based ROR-P score guide recurrencerisk stratification in non-metastatic, ER-positive, HER2-negative breast cancerbut are not universally accessible. Histopathology is routinely available andmay offer a scalable alternative. We introduce MAKO, a benchmarking frameworkevaluating 12 pathology foundation models and two non-pathology baselines forpredicting ROR-P scores from H&amp;E-stained whole slide images usingattention-based multiple instance learning. Models were trained and validatedon the Carolina Breast Cancer Study and externally tested on TCGA BRCA. Severalfoundation models outperformed baselines across classification, regression, andsurvival tasks. CONCH achieved the highest ROC AUC, while H-optimus-0 andVirchow2 showed top correlation with continuous ROR-P scores. All pathologymodels stratified CBCS participants by recurrence similarly to transcriptomicROR-P. Tumor regions were necessary and sufficient for high-risk predictions,and we identified candidate tissue biomarkers of recurrence. These resultshighlight the promise of interpretable, histology-based risk models inprecision oncology.</description>
      <author>example@mail.com (Jakub R. Kaczmarzyk, Sarah C. Van Alsten, Alyssa J. Cozzo, Rajarsi Gupta, Peter K. Koo, Melissa A. Troester, Katherine A. Hoadley, Joel H. Saltz)</author>
      <guid isPermaLink="false">2508.12025v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Toward General Physical Intelligence for Resilient Agile Manufacturing Automation</title>
      <link>http://arxiv.org/abs/2508.11960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Advanced Engineering Informatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了基础模型，特别是视觉语言行动(VLA)模型在敏捷和人本制造中的应用。研究系统地调查了VLA模型在通用物理智能(GPI)背景下的最新进展，评估了它们在工业部署中的准备情况，并提出了将GPI整合到下一代工业生态系统中的挑战和方向。&lt;h4&gt;背景&lt;/h4&gt;敏捷和人本制造需要具有上下文推理能力和在非结构化环境中安全交互的弹性机器人解决方案。基础模型，特别是VLA模型，已经出现，能够将多模态感知、推理和物理基础行动融合为统一表示，称为通用物理智能(GPI)。然而，GPI在当代敏捷制造过程中的实际应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;为了填补GPI在敏捷制造中实际应用研究的空白，这篇实践性综述旨在系统地调查VLA模型的最新进展，比较领先实施，评估它们在工业部署中的准备情况，并确定开放的研究挑战和未来方向，以便更好地将GPI整合到与工业5.0一致的下一代工业生态系统中。&lt;h4&gt;方法&lt;/h4&gt;研究采用系统综述方法，调查了VLA模型在GPI背景下的最新进展。研究进行了全面的比较分析，评估了领先实施，并通过结构化消融研究评估了它们在工业部署中的准备情况。研究将最先进的技术组织成五个主题支柱：多感官表示学习、sim2real迁移、规划与控制、不确定性和安全措施以及基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;研究将最先进的技术组织成五个主题支柱：多感官表示学习、sim2real迁移、规划与控制、不确定性和安全措施以及基准测试。这些发现有助于理解VLA模型在GPI框架下的当前状态，并评估它们在工业环境中的适用性。&lt;h4&gt;结论&lt;/h4&gt;研究强调了将GPI整合到下一代工业生态系统中的开放研究挑战和未来方向，这些方向与工业5.0的目标一致。通过系统性地调查和比较VLA模型，研究为GPI在敏捷制造中的应用提供了重要见解，并指明了未来研究的方向。&lt;h4&gt;翻译&lt;/h4&gt;敏捷和人本制造规定了具有上下文推理能力和在非结构化环境中安全交互的弹性机器人解决方案。基础模型，特别是视觉语言行动(VLA)模型，已经出现，能够将多模态感知、推理和物理基础行动跨不同 embodiment 融合为统一表示，称为通用物理智能(GPI)。虽然GPI已在文献中有所描述，但其在当代敏捷制造过程中的实际应用和不断演变的作用尚未得到充分探索。为了填补这一空白，这篇实践性综述系统地调查了在GPI背景下VLA模型的最新进展，对领先实施进行了全面的比较分析，并通过结构化消融研究评估了它们在工业部署中的准备情况。我们的分析将最先进的技术组织成五个主题支柱，包括多感官表示学习、sim2real迁移、规划与控制、不确定性和安全措施以及基准测试。最后，我们阐述了开放的研究挑战和未来方向，以便更好地将GPI与工业5.0保持一致，整合到下一代工业生态系统中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何开发具有通用物理智能(GPI)的机器人系统，使其能够适应敏捷制造环境中的多样化任务和动态变化。这个问题很重要，因为市场需求正从大规模生产转向大规模定制，而中小企业缺乏技术基础设施来实现这种转型；现有机器人系统缺乏灵活性，难以处理不同任务、环境和形态的变化；工业机器人需要与人类协作并适应非结构化环境，而传统机器智能方法无法满足Industry 5.0愿景中的多样性需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到基础模型(特别是视觉-语言-动作VLA模型)能够融合多模态感知、推理和物理行动，然后将这些模型组织成五个主题支柱进行系统化研究。作者设计了一个分层控制架构，区分系统1(使用VLA模型和触觉反馈进行高级规划)和系统2(执行低级动作的快速控制器)。该方法大量借鉴了现有工作，包括基础模型如Gato、RT-2、PaLM-E，多模态感知技术，数据生成方法，规划框架以及安全估计方法，但针对工业应用进行了专门改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是开发通用物理智能(GPI)系统，使机器人能够通过整合多模态观测(视觉、语言、触觉、本体感觉)来执行多样化任务，并结合脑样计算能力与在现实世界中感知和物理行动的能力。整体实现流程包括：1)多感官表示学习，创建统一的潜在空间表示；2)数据生成和sim2real转移，解决数据收集瓶颈；3)规划和控制框架，处理复杂制造工作流程；4)不确定性估计和安全保障，确保可靠操作；5)基准测试和评估，系统衡量系统性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出专门针对敏捷制造的GPI框架；2)开发结合高级推理和精确运动的分层控制架构；3)对现有VLA模型(Gato、RT-2、PaLM-E等)进行工业适应性改进；4)进行全面的消融研究评估不同模型性能；5)确定五个主要研究挑战并提供具体建议。相比之前的工作，本文专注于工业制造环境而非一般机器人任务，强调触觉和本体感觉的重要性，提出更全面的评估框架，并解决实际部署中的实时性、可解释性和安全问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个通用物理智能(GPI)框架，通过整合多模态感知、分层控制和适应性学习，使机器人能够在敏捷制造环境中实现类似人类的灵活性和鲁棒性操作。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agile and human-centric manufacturing stipulates resilient robotic solutionscapable of contextual reasoning and safe interaction in unstructuredenvironments. Foundation models particularly the Vision Language Action (VLA)models have emerged to fuse multimodal perception, reasoning and physicallygrounded action across varied embodiments into unified representation, termedas General Physical Intelligence (GPI). While GPI has already been described inthe literature but its practical application and evolving role in contemporaryagile manufacturing processes have yet to be duly explored. To bridge this gap,this practical review systematically surveys recent advancements in VLA modelswithin GPI context, performs comprehensive comparative analysis of leadingimplementations and evaluates their readiness for industrial deployment throughstructured ablation study. Our analysis has organized state-of-the-art intofive thematic pillars including multisensory representation learning, sim2realtransfer, planning and control, uncertainty and safety measures andbenchmarking. Finally, we articulate open research challenges and proposedirections to better integrate GPI into next-generation industrial ecosystemsin line with Industry 5.0.</description>
      <author>example@mail.com (Sandeep Kanta, Mehrdad Tavassoli, Varun Teja Chirkuri, Venkata Akhil Kumar, Santhi Bharath Punati, Praveen Damacharla, Sunny Katyara)</author>
      <guid isPermaLink="false">2508.11960v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2508.11954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为UniCast的新型多模态框架，它扩展了时间序列基础模型以同时利用时间序列、视觉和文本模态，通过软提示调度的方法提高预测性能，同时保持基础模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测是金融、医疗保健和环境监测等领域的基础任务。尽管现有时间序列基础模型通过大规模预训练展示了强大的泛化能力，但它们主要在单模态环境中运行，忽略了现实场景中时间序列数据经常伴随的丰富多模态上下文，如视觉和文本信号。&lt;h4&gt;目的&lt;/h4&gt;开发一个参数高效的多模态框架，使时间序列基础模型能够联合利用时间序列、视觉和文本模态，以提高预测性能，同时保持基础模型的泛化能力并实现有效的跨模态交互。&lt;h4&gt;方法&lt;/h4&gt;提出UniCast框架，将预训练的视觉和文本编码器的模态特定嵌入与冻结的时间序列基础模型通过软提示调度的方法集成，实现高效适应且只需少量参数更新。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的时间序列预测基准测试中，UniCast一致且显著地优于所有现有的时间序列基础模型基线，证明了多模态上下文对提升预测性能的关键作用。&lt;h4&gt;结论&lt;/h4&gt;多模态上下文对于提升时间序列预测性能至关重要。UniCast框架通过有效整合时间序列、视觉和文本信息，为时间序列预测提供了一个更强大的方法，代表了时间序列基础模型发展的重要进步。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测是金融、医疗保健和环境监测等领域的基础任务。尽管最近时间序列基础模型(TSFMs)的进展通过大规模预训练展示了强大的泛化能力，但现有模型主要在单模态环境中运行，忽略了现实场景中时间序列数据经常伴随的丰富的多模态上下文，如视觉和文本信号。本文介绍了一种新颖的参数高效多模态框架UniCast，它扩展了TSFMs，以联合利用时间序列、视觉和文本模态来增强预测性能。我们的方法通过软提示调度的方法，将预训练的视觉和文本编码器的模态特定嵌入与冻结的TSFM集成，实现高效适应且只需少量参数更新。这种设计不仅保留了基础模型的泛化能力，还实现了有效的跨模态交互。在多样化的时间序列预测基准上的广泛实验表明，UniCast一致且显著地优于所有现有的TSFM基线。这些发现强调了多模态上下文在推进下一代通用时间序列预测器方面的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting is a foundational task across domains, such asfinance, healthcare, and environmental monitoring. While recent advances inTime Series Foundation Models (TSFMs) have demonstrated strong generalisationthrough large-scale pretraining, existing models operate predominantly in aunimodal setting, ignoring the rich multimodal context, such as visual andtextual signals, that often accompanies time series data in real-worldscenarios. This paper introduces a novel parameter-efficient multimodalframework, UniCast, that extends TSFMs to jointly leverage time series, vision,and text modalities for enhanced forecasting performance. Our method integratesmodality-specific embeddings from pretrained Vision and Text Encoders with afrozen TSFM via soft prompt tuning, enabling efficient adaptation with minimalparameter updates. This design not only preserves the generalisation strengthof the foundation model but also enables effective cross-modal interaction.Extensive experiments across diverse time-series forecasting benchmarksdemonstrate that UniCast consistently and significantly outperforms allexisting TSFM baselines. The findings highlight the critical role of multimodalcontext in advancing the next generation of general-purpose time seriesforecasters.</description>
      <author>example@mail.com (Sehyuk Park, Soyeon Caren Han, Eduard Hovy)</author>
      <guid isPermaLink="false">2508.11954v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>QuarkMed Medical Foundation Model Technical Report</title>
      <link>http://arxiv.org/abs/2508.11894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;QuarkMed是一个高性能医疗基础模型，通过专业数据处理、检索增强生成和强化学习技术，解决了医疗应用对专业知识、准确性和定制化的需求&lt;h4&gt;背景&lt;/h4&gt;大语言模型的最新进展加速了其在医疗保健应用中的采用，包括AI医疗咨询、诊断报告辅助和医疗搜索工具&lt;h4&gt;目的&lt;/h4&gt;开发一个强大可靠的基础模型，满足医疗任务对高度专业知识、专业准确性和定制能力的需求&lt;h4&gt;方法&lt;/h4&gt;利用精心策划的医疗数据处理、医疗内容检索增强生成(RAG)以及大规模可验证的强化学习管道&lt;h4&gt;主要发现&lt;/h4&gt;QuarkMed模型在中国医学执照考试中达到70%的准确率，在多种医疗基准测试中表现出强大的泛化能力&lt;h4&gt;结论&lt;/h4&gt;QuarkMed提供了一个强大且多功能的个人医疗AI解决方案，已在ai.quark.cn服务数百万用户&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型的最新进展显著加速了其在医疗保健应用中的采用，包括AI驱动的医疗咨询、诊断报告辅助和医疗搜索工具。然而，医疗任务通常需要高度专业的知识、专业准确性和定制能力，因此需要强大可靠的基础模型。QuarkMed通过精心策划的医疗数据处理、医疗内容检索增强生成(RAG)以及大规模可验证的强化学习管道来满足这些需求，开发出高性能医疗基础模型。该模型在中国医学执照考试中达到70%的准确率，显示出在多样化医疗基准测试中的强大泛化能力。QuarkMed提供了一个强大且多功能的个人医疗AI解决方案，已在ai.quark.cn服务数百万用户&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large language models have significantly acceleratedtheir adoption in healthcare applications, including AI-powered medicalconsultations, diagnostic report assistance, and medical search tools. However,medical tasks often demand highly specialized knowledge, professional accuracy,and customization capabilities, necessitating a robust and reliable foundationmodel. QuarkMed addresses these needs by leveraging curated medical dataprocessing, medical-content Retrieval-Augmented Generation (RAG), and alarge-scale, verifiable reinforcement learning pipeline to develop ahigh-performance medical foundation model. The model achieved 70% accuracy onthe Chinese Medical Licensing Examination, demonstrating strong generalizationacross diverse medical benchmarks. QuarkMed offers a powerful yet versatilepersonal medical AI solution, already serving over millions of users atai.quark.cn.</description>
      <author>example@mail.com (Ao Li, Bin Yan, Bingfeng Cai, Chenxi Li, Cunzhong Zhao, Fugen Yao, Gaoqiang Liu, Guanjun Jiang, Jian Xu, Liang Dong, Liansheng Sun, Rongshen Zhang, Xiaolei Gui, Xin Liu, Xin Shang, Yao Wu, Yu Cao, Zhenxin Ma, Zhuang Jia)</author>
      <guid isPermaLink="false">2508.11894v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Impact of Clinical Image Quality on Efficient Foundation Model Finetuning</title>
      <link>http://arxiv.org/abs/2508.11864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了特定领域视觉基础模型ProFound在前列腺多参数MRI中的应用，探讨了图像质量分布对标签效率微调的影响，发现微调和测试集中的图像质量比例匹配度对模型性能有显著影响。&lt;h4&gt;背景&lt;/h4&gt;医学影像基础模型在标签效率方面展现出前景，仅用少量标注数据即可实现高性能。ProFound是一个在大规模前列腺MRI数据上预训练的特定领域视觉基础模型。&lt;h4&gt;目的&lt;/h4&gt;研究图像质量的变化如何影响标签效率的微调，通过测量微调模型的泛化能力来评估这一影响。&lt;h4&gt;方法&lt;/h4&gt;实验系统性地改变微调和评估集中高质量与低质量图像的比例，研究不同质量分布对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;a) 微调和测试集中高质量到低质量图像比例的变化会导致下游性能显著差异；b) 微调集中存在足够的高质量图像对保持强大性能至关重要；c) 微调和测试分布的匹配度在不同下游任务(如自动放射学报告和前列腺癌检测)中的重要性不同；d) 当质量比例一致时，微调比从头开始训练需要少得多的标记数据，但标签效率取决于图像质量分布；e) 没有足够的高质量微调数据，预训练模型可能无法优于没有预训练的模型。&lt;h4&gt;结论&lt;/h4&gt;评估和微调与部署之间的质量分布一致性非常重要，特定下游任务需要制定微调数据的质量标准。使用ProFound的研究表明，量化微调和部署中的图像质量对于充分实现基础模型的数据和计算效率优势至关重要。&lt;h4&gt;翻译&lt;/h4&gt;医学影像领域的基础模型在标签效率方面显示出前景，仅用少量标注数据就能实现高性能。本文使用在大型前列腺MRI数据集上预训练的特定领域视觉基础模型ProFound，评估了其在前列腺多参数MRI中的应用。我们通过测量微调模型的泛化能力，研究了图像质量的变化如何影响标签效率的微调。实验系统性地改变了微调和评估集中高质量/低质量图像的比例。我们的研究结果表明，图像质量分布及其微调和测试不匹配显著影响模型性能。特别是：a) 微调和测试集中高质量到低质量图像比例的变化会导致下游性能显著差异；b) 微调集中存在足够的高质量图像对于保持强大性能至关重要，而微调和测试分布的匹配度在不同下游任务(如自动放射学报告和前列腺癌检测)中的重要性有所不同。当质量比例一致时，微调比从头开始训练需要少得多的标记数据，但标签效率取决于图像质量分布。如果没有足够的高质量微调数据，预训练模型可能无法优于没有预训练训练的模型。这强调了评估和微调与部署之间质量分布一致性的重要性，以及特定下游任务微调数据质量标准的必要性。使用ProFound，我们展示了量化微调和部署中图像质量的价值，以充分实现基础模型的数据和计算效率优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models in medical imaging have shown promising label efficiency,achieving high downstream performance with only a fraction of annotated data.Here, we evaluate this in prostate multiparametric MRI using ProFound, adomain-specific vision foundation model pretrained on large-scale prostate MRIdatasets. We investigate how variable image quality affects label-efficientfinetuning by measuring the generalisability of finetuned models. Experimentssystematically vary high-/low-quality image ratios in finetuning and evaluationsets. Our findings indicate that image quality distribution and itsfinetune-and-test mismatch significantly affect model performance. Inparticular: a) Varying the ratio of high- to low-quality images betweenfinetuning and test sets leads to notable differences in downstreamperformance; and b) The presence of sufficient high-quality images in thefinetuning set is critical for maintaining strong performance, whilst theimportance of matched finetuning and testing distribution varies betweendifferent downstream tasks, such as automated radiology reporting and prostatecancer detection.When quality ratios are consistent, finetuning needs far lesslabeled data than training from scratch, but label efficiency depends on imagequality distribution. Without enough high-quality finetuning data, pretrainedmodels may fail to outperform those trained without pretraining. Thishighlights the importance of assessing and aligning quality distributionsbetween finetuning and deployment, and the need for quality standards infinetuning data for specific downstream tasks. Using ProFound, we show thevalue of quantifying image quality in both finetuning and deployment to fullyrealise the data and compute efficiency benefits of foundation models.</description>
      <author>example@mail.com (Yucheng Tang, Pawel Rajwa, Alexander Ng, Yipei Wang, Wen Yan, Natasha Thorley, Aqua Asif, Clare Allen, Louise Dickinson, Francesco Giganti, Shonit Punwani, Daniel C. Alexander, Veeru Kasivisvanathan, Yipeng Hu)</author>
      <guid isPermaLink="false">2508.11864v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data</title>
      <link>http://arxiv.org/abs/2508.11794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Fed-Meta-Align的新型四阶段框架，用于在资源受限的物联网设备中进行实时故障分类，解决了联邦学习在非独立同分布数据环境下的模型发散问题。&lt;h4&gt;背景&lt;/h4&gt;在资源受限的物联网设备中进行实时故障分类对工业安全至关重要，但在异构环境中训练鲁棒模型仍然是一个重大挑战。标准的联邦学习在非独立同分布数据存在时往往会失败，导致模型发散。&lt;h4&gt;目的&lt;/h4&gt;设计一个新的框架来解决这些限制，通过复杂的初始化和训练流程来克服联邦学习在异构物联网环境中的挑战。&lt;h4&gt;方法&lt;/h4&gt;Fed-Meta-Align采用四阶段框架：1)在通用公共数据集上训练基础模型建立起点；2)串行元初始化阶段，在IoT设备数据子集上顺序训练，学习对异构性感知的初始化；3)并行FL阶段，使用双重标准聚合机制基于本地性能和余弦相似度对齐进行加权更新；4)设备上个性化阶段，将全局模型适配为每个IoT设备的专家模型。&lt;h4&gt;主要发现&lt;/h4&gt;Fed-Meta-Align在异构IoT设备上平均测试准确率达到91.27%，在电气和机械故障数据集上分别比个性化FedAvg和FedProx高出最高3.87%和3.37%。&lt;h4&gt;结论&lt;/h4&gt;这种顺序初始化和自适应聚合的多阶段方法为在各种TinyML网络上部署高性能智能提供了强大途径。&lt;h4&gt;翻译&lt;/h4&gt;在资源受限的物联网设备中进行实时故障分类对工业安全至关重要，但在这种异构环境中训练鲁棒模型仍然是一个重大挑战。标准的联邦学习在存在非独立同分布数据时往往会失败，导致模型发散。本文介绍了Fed-Meta-Align，一种新型四阶段框架，通过复杂的初始化和训练流程设计来克服这些限制。我们的过程首先在通用公共数据集上训练基础模型，建立一个有能力的起点。然后该模型经历串行元初始化阶段，在IoT设备数据的子集上顺序训练，学习对异构性感知的初始化，已经位于损失景观的有利区域。这个信息丰富的模型随后在并行FL阶段得到完善，该阶段使用双重标准聚合机制，基于本地性能和余弦相似度对齐对IoT设备更新进行加权。最后，设备上个性化阶段将收敛的全局模型适配为每个IoT设备的专家。全面的实验证明，Fed-Meta-Align在异构IoT设备上实现了91.27%的平均测试准确率，在电气和机械故障数据集上分别比个性化FedAvg和FedProx高出最高3.87%和3.37%。这种顺序初始化和自适应聚合的多阶段方法为在各种TinyML网络上部署高性能智能提供了强大途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time fault classification in resource-constrained Internet of Things(IoT) devices is critical for industrial safety, yet training robust models insuch heterogeneous environments remains a significant challenge. StandardFederated Learning (FL) often fails in the presence of non-IID data, leading tomodel divergence. This paper introduces Fed-Meta-Align, a novel four-phaseframework designed to overcome these limitations through a sophisticatedinitialization and training pipeline. Our process begins by training afoundational model on a general public dataset to establish a competentstarting point. This model then undergoes a serial meta-initialization phase,where it sequentially trains on a subset of IOT Device data to learn aheterogeneity-aware initialization that is already situated in a favorableregion of the loss landscape. This informed model is subsequently refined in aparallel FL phase, which utilizes a dual-criterion aggregation mechanism thatweights for IOT devices updates based on both local performance and cosinesimilarity alignment. Finally, an on-device personalization phase adapts theconverged global model into a specialized expert for each IOT Device.Comprehensive experiments demonstrate that Fed-Meta-Align achieves an averagetest accuracy of 91.27% across heterogeneous IOT devices, outperformingpersonalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical andmechanical fault datasets, respectively. This multi-stage approach of sequencedinitialization and adaptive aggregation provides a robust pathway for deployinghigh-performance intelligence on diverse TinyML networks.</description>
      <author>example@mail.com (Hemanth Macharla, Mayukha Pal)</author>
      <guid isPermaLink="false">2508.11794v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Geospatial Data Generation Using AlphaEarth Foundations Model</title>
      <link>http://arxiv.org/abs/2508.11739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种利用AlphaEarth Foundations（AEF）扩展地理空间标记数据集的方法，使其超出初始地理区域，并通过案例研究验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;高质量的标记地理空间数据集对于提取见解和理解地球至关重要，但这些数据集通常不覆盖全球，仅限于数据收集的特定地理区域。&lt;h4&gt;目的&lt;/h4&gt;提出并评估一种利用AEF扩展地理空间标记数据集的方法，使其超出初始地理区域。&lt;h4&gt;方法&lt;/h4&gt;利用AEF来扩展地理空间标记数据集，使用随机森林或逻辑回归等基本模型完成这一任务。研究了一个案例，将LANDFIRE的现有植被类型（EVT）数据集从美国扩展到加拿大，分为EvtPhys（13个类别）和EvtGp（80个类别）两个粒度级别。&lt;h4&gt;主要发现&lt;/h4&gt;对于EvtPhys，模型预测与地面实况一致。在美国和加拿大的EvtPhys验证集上，训练的模型分别实现了81%和73%的分类准确率。&lt;h4&gt;结论&lt;/h4&gt;尽管存在讨论的限制，但基本模型如随机森林或逻辑回归可以有效地用于扩展地理空间标记数据集。&lt;h4&gt;翻译&lt;/h4&gt;高质量的标记地理空间数据集对于提取见解和理解我们的地球至关重要。不幸的是，这些数据集通常不覆盖全球，仅限于数据收集的特定地理区域。谷歌DeepMind最近发布的AlphaEarth Foundations（AEF）提供了密集的全球地理空间表示，旨在作为各种任务的有用输入。在本文中，我们提出并评估了一种利用AEF扩展地理空间标记数据集超出其初始地理区域的方法。我们表明，即使是随机森林或逻辑回归等基本模型也可以用于完成这一任务。我们研究了一个案例，将LANDFIRE的现有植被类型（EVT）数据集从美国扩展到加拿大，分为两个粒度级别：EvtPhys（13个类别）和EvtGp（80个类别）。从定性上看，对于EvtPhys，模型预测与地面实况一致。尽管存在讨论的限制，训练的模型在美国和加拿大的EvtPhys验证集上分别实现了81%和73%的分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality labeled geospatial datasets are essential for extractinginsights and understanding our planet. Unfortunately, these datasets often donot span the entire globe and are limited to certain geographic regions wheredata was collected. Google DeepMind's recently released AlphaEarth Foundations(AEF) provides an information-dense global geospatial representation designedto serve as a useful input across a wide gamut of tasks. In this article wepropose and evaluate a methodology which leverages AEF to extend geospatiallabeled datasets beyond their initial geographic regions. We show that evenbasic models like random forests or logistic regression can be used toaccomplish this task. We investigate a case study of extending LANDFIRE'sExisting Vegetation Type (EVT) dataset beyond the USA into Canada at two levelsof granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, forEvtPhys, model predictions align with ground truth. Trained models achieve 81%and 73% classification accuracy on EvtPhys validation sets in the USA andCanada, despite discussed limitations.</description>
      <author>example@mail.com (Luc Houriez, Sebastian Pilarski, Behzad Vahedi, Ali Ahmadalipour, Teo Honda Scully, Nicholas Aflitto, David Andre, Caroline Jaffe, Martha Wedner, Rich Mazzola, Josh Jeffery, Ben Messinger, Sage McGinley-Smith, Sarah Russell)</author>
      <guid isPermaLink="false">2508.11739v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation</title>
      <link>http://arxiv.org/abs/2508.11738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性回顾了2019-2024年间发表的109项研究，探讨了人工智能技术在解决农村医疗挑战方面的潜力，包括基础设施不足、劳动力短缺和社会经济差异等问题。&lt;h4&gt;背景&lt;/h4&gt;农村医疗面临持续挑战，包括基础设施不足、劳动力短缺和社会经济差异，这些因素阻碍了基本医疗服务的获取。&lt;h4&gt;目的&lt;/h4&gt;研究人工智能在解决农村地区医疗问题方面的变革潜力，重点关注服务不足的农村地区。&lt;h4&gt;方法&lt;/h4&gt;系统性回顾了2019年至2024年间发表的109项研究，数据来源包括PubMed、Embase、Web of Science、IEEE Xplore和Scopus，使用PRISMA指南和Covidence软件筛选文章，并进行主题分析以确定农村医疗中AI实施的关键模式和见解。&lt;h4&gt;主要发现&lt;/h4&gt;AI应用在改善医疗可及性、质量和效率方面显示出巨大潜力；多模态基础模型(MFMs)和大语言模型(LLMs)具有特别显著的变革潜力；MFMs整合多种数据源支持全面决策，LLMs有助于临床文档记录、患者分诊、翻译和虚拟协助；这些技术可以通过增强人类能力、减少诊断延迟和民主化专业知识获取来彻底改变农村医疗。&lt;h4&gt;结论&lt;/h4&gt;农村医疗AI应用仍面临基础设施限制、数据质量问题和伦理考虑等障碍；解决这些挑战需要跨学科合作、数字基础设施投资和监管框架发展；本研究提供了可操作建议并指出了未来研究方向，以确保AI在农村医疗系统中的公平和可持续整合。&lt;h4&gt;翻译&lt;/h4&gt;农村医疗面临持续挑战，包括基础设施不足、劳动力短缺和社会经济差异，这些因素阻碍了基本医疗服务的获取。本研究探讨了人工智能在解决农村地区这些问题方面的变革潜力。我们系统性回顾了2019年至2024年间发表的109项研究，数据来源包括PubMed、Embase、Web of Science、IEEE Xplore和Scopus。文章使用PRISMA指南和Covidence软件进行筛选，并进行了主题分析以确定农村医疗中AI实施的关键模式和见解。研究结果显示AI应用在改善医疗可及性、质量和效率方面显示出巨大潜力。其中，多模态基础模型(MFMs)和大语言模型(LLMs)具有特别显著的变革潜力。MFMs整合多种数据源（如图像、临床记录和生物信号）以支持全面决策，而LLMs有助于临床文档记录、患者分诊、翻译和虚拟协助。这些技术可以通过增强人类能力、减少诊断延迟和民主化专业知识获取来彻底改变农村医疗。然而，仍存在障碍，包括基础设施限制、数据质量问题和伦理考虑。解决这些挑战需要跨学科合作、对数字基础设施的投资以及监管框架的发展。本综述提供了可操作建议并指出了未来研究的领域，以确保人工智能在农村医疗系统中的公平和可持续整合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rural healthcare faces persistent challenges, including inadequateinfrastructure, workforce shortages, and socioeconomic disparities that hinderaccess to essential services. This study investigates the transformativepotential of artificial intelligence (AI) in addressing these issues inunderserved rural areas. We systematically reviewed 109 studies publishedbetween 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, andScopus. Articles were screened using PRISMA guidelines and Covidence software.A thematic analysis was conducted to identify key patterns and insightsregarding AI implementation in rural healthcare delivery. The findings revealsignificant promise for AI applications, such as predictive analytics,telemedicine platforms, and automated diagnostic tools, in improving healthcareaccessibility, quality, and efficiency. Among these, advanced AI systems,including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs),offer particularly transformative potential. MFMs integrate diverse datasources, such as imaging, clinical records, and bio signals, to supportcomprehensive decision-making, while LLMs facilitate clinical documentation,patient triage, translation, and virtual assistance. Together, thesetechnologies can revolutionize rural healthcare by augmenting human capacity,reducing diagnostic delays, and democratizing access to expertise. However,barriers remain, including infrastructural limitations, data quality concerns,and ethical considerations. Addressing these challenges requiresinterdisciplinary collaboration, investment in digital infrastructure, and thedevelopment of regulatory frameworks. This review offers actionablerecommendations and highlights areas for future research to ensure equitableand sustainable integration of AI in rural healthcare systems.</description>
      <author>example@mail.com (Kiruthika Balakrishnan, Durgadevi Velusamy, Hana E. Hinkle, Zhi Li, Karthikeyan Ramasamy, Hikmat Khan, Srini Ramaswamy, Pir Masoom Shah)</author>
      <guid isPermaLink="false">2508.11738v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</title>
      <link>http://arxiv.org/abs/2508.11584v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了视觉感知引擎（VPEngine），一个模块化框架，旨在解决资源受限机器人平台上多模型部署的冗余计算、大内存占用和复杂集成挑战，实现视觉多任务的高效GPU使用。&lt;h4&gt;背景&lt;/h4&gt;在资源受限的机器人平台上为不同的感知任务部署多个机器学习模型，常常导致冗余计算、大内存占用和复杂的集成挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个模块化框架（VPEngine），使视觉多任务能够高效利用GPU，同时保持可扩展性和开发者可访问性。&lt;h4&gt;方法&lt;/h4&gt;框架架构利用共享的基础模型主干提取图像表示，这些表示可以在并行运行的多专业化任务专用模型头之间高效共享，无需不必要的GPU-CPU内存传输。这种设计消除了传统顺序模型中特征提取的计算冗余，并支持动态任务优先级排序。示例实现使用DINOv2作为基础模型，具有深度、目标检测和语义分割多个任务头。&lt;h4&gt;主要发现&lt;/h4&gt;与顺序执行相比，实现了高达3倍的加速；基于CUDA Multi-Process Service (MPS)，提供高效的GPU利用率和恒定的内存占用；允许在运行时动态调整每个任务的推理频率；在NVIDIA Jetson Orin AGX上，对TensorRT优化模型的端到端实时性能达到≥50 Hz。&lt;h4&gt;结论&lt;/h4&gt;VPEngine是一个用Python编写的开源框架，具有ROS2 C++ (Humble)绑定，便于机器人社区在各种机器人平台上使用，有效解决了多模型部署挑战，实现了高效GPU利用和实时性能。&lt;h4&gt;翻译&lt;/h4&gt;在资源受限的机器人平台上为不同的感知任务部署多个机器学习模型，常常导致冗余计算、大内存占用和复杂的集成挑战。对此，本文提出了视觉感知引擎（VPEngine），这是一个模块化框架，旨在实现视觉多任务的高效GPU使用，同时保持可扩展性和开发者可访问性。我们的框架架构利用共享的基础模型主干来提取图像表示，这些表示可以在并行运行的多专业化任务专用模型头之间高效共享，无需任何不必要的GPU-CPU内存传输。这种设计消除了部署传统顺序模型时特征提取组件中固有的计算冗余，同时能够根据应用需求实现动态任务优先级排序。我们通过使用DINOv2作为基础模型和多个任务（深度、目标检测和语义分割）头的示例实现来展示我们框架的能力，与顺序执行相比实现了高达3倍的加速。基于CUDA Multi-Process Service (MPS)，VPEngine提供高效的GPU利用率和恒定的内存占用，同时允许在运行时动态调整每个任务的推理频率。该框架用Python编写，是开源的，并具有ROS2 C++ (Humble)绑定，便于机器人社区在各种机器人平台上使用。我们的示例实现在NVIDIA Jetson Orin AGX上对TensorRT优化模型的端到端实时性能达到了≥50 Hz。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying multiple machine learning models on resource-constrained roboticplatforms for different perception tasks often results in redundantcomputations, large memory footprints, and complex integration challenges. Inresponse, this work presents Visual Perception Engine (VPEngine), a modularframework designed to enable efficient GPU usage for visual multitasking whilemaintaining extensibility and developer accessibility. Our frameworkarchitecture leverages a shared foundation model backbone that extracts imagerepresentations, which are efficiently shared, without any unnecessary GPU-CPUmemory transfers, across multiple specialized task-specific model heads runningin parallel. This design eliminates the computational redundancy inherent infeature extraction component when deploying traditional sequential models whileenabling dynamic task prioritization based on application demands. Wedemonstrate our framework's capabilities through an example implementationusing DINOv2 as the foundation model with multiple task (depth, objectdetection and semantic segmentation) heads, achieving up to 3x speedup comparedto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngineoffers efficient GPU utilization and maintains a constant memory footprintwhile allowing per-task inference frequencies to be adjusted dynamically duringruntime. The framework is written in Python and is open source with ROS2 C++(Humble) bindings for ease of use by the robotics community across diverserobotic platforms. Our example implementation demonstrates end-to-end real-timeperformance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimizedmodels.</description>
      <author>example@mail.com (Jakub Łucki, Jonathan Becktor, Georgios Georgakis, Rob Royce, Shehryar Khattak)</author>
      <guid isPermaLink="false">2508.11584v2</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction</title>
      <link>http://arxiv.org/abs/2508.11728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UniDCF是一个统一的框架，能够通过多模态融合编码重建多种口腔颅面硬组织，克服了现有单模态方法的局限性，实现了快速、自动化和高保真度的重建。&lt;h4&gt;背景&lt;/h4&gt;口腔颅面硬组织缺陷严重影响患者的生理功能、面部美观和心理健康，精确重建面临挑战。现有的深度学习模型局限于单组织和特定模态的成像输入，导致泛化性差，且在解剖保真度、计算效率和跨组织适应性之间存在权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够通过多模态融合编码重建多种口腔颅面硬组织的统一框架。&lt;h4&gt;方法&lt;/h4&gt;提出UniDCF框架，通过点云和多视图图像的多模态融合编码来重建多种口腔颅面硬组织。利用每种模态的互补优势，并引入基于评分的降噪模块来优化表面平滑度。作者整理了包含6,609名患者的口腔扫描、CBCT和CT的多模态数据集，共54,555个标注实例。&lt;h4&gt;主要发现&lt;/h4&gt;UniDCF在几何精度、结构完整性和空间准确性方面优于现有最先进的方法。临床模拟表明，UniDCF将重建设计时间减少了99%，并达到超过94%的临床医生可接受性。&lt;h4&gt;结论&lt;/h4&gt;UniDCF实现了快速、自动化和高保真度的重建，支持个性化和精确的修复治疗，简化临床工作流程，改善患者预后。&lt;h4&gt;翻译&lt;/h4&gt;口腔颅面硬组织缺陷严重影响患者的生理功能、面部美观和心理健康，对精确重建构成重大挑战。当前的深度学习模型仅限于单组织场景和特定模态的成像输入，导致泛化性差，且在解剖保真度、计算效率和跨组织适应性之间存在权衡。在此，我们引入UniDCF，这是一个统一框架，能够通过点云和多视图图像的多模态融合编码来重建多种口腔颅面硬组织。通过利用每种模态的互补优势，并纳入基于评分的降噪模块来优化表面平滑度，UniDCF克服了先前单模态方法的局限性。我们整理了最大的多模态数据集，包含6,609名患者的口腔扫描、CBCT和CT，共54,555个标注实例。评估显示，UniDCF在几何精度、结构完整性和空间准确性方面优于现有最先进的方法。临床模拟表明，UniDCF将重建设计时间减少了99%，并达到超过94%的临床医生可接受性。总体而言，UniDCF实现了快速、自动化和高保真度的重建，支持个性化和精确的修复治疗，简化临床工作流程，改善患者预后。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决牙颅面硬组织（包括颅骨、牙齿和颌骨）缺陷的精确重建问题。现实中，这类缺陷严重影响患者的生理功能、面部美观和心理健康，由创伤、肿瘤切除、先天性畸形或疾病引起。随着这些疾病的发病率和复杂性增长，临床迫切需要精确、高效和个性化的重建解决方案，而传统CAD方法劳动密集且依赖专家经验，现有AI方法又局限于单一组织场景，难以满足全面重建需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前深度学习模型的局限性，认识到缺乏大规模多模态数据集和单模态方法难以平衡解剖保真度、计算效率和跨组织适应性这两个核心挑战。他们设计了一个统一框架，通过多模态融合（点云和多视图图像）重建多种牙颅面硬组织。该方法借鉴了现有工作：基于AdaPoinTr点云完成骨干网络，使用Transformer架构进行几何推理，采用基于分数的去噪方法，并整合了多个公共数据集和临床队列数据集，构建了迄今为止最大的整合牙颅面硬组织数据集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态融合（点云和多视图图像）重建多种牙颅面硬组织，利用不同模态的互补优势，并引入基于分数的去噪模块提高几何平滑度。整体流程：1)数据预处理：将原始多模态输入转换为稀疏点云和多视图灰度图像；2)模型架构：基于AdaPoinTr骨干网络，使用几何感知的Transformer编码器-解码器处理点云，通过多头注意力机制融合图像特征，最后使用基于分数的去噪模块优化结果；3)输出：生成解剖准确的牙颅面重建结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一的框架设计UniDCF，能够重建多种牙颅面硬组织；2)构建了包含6,609名患者54,555个样本的大规模多模态数据集；3)多模态融合技术，利用点云和图像的互补优势；4)基于分数的去噪模块提高表面平滑度；5)跨组织适应性，从牙冠到颅面骨植入的端到端重建。相比之前工作，UniDCF突破了单组织领域限制，在几何精度、结构完整性和空间准确性方面表现更优，临床设计时间减少99%，医生可接受性超过94%，实现了真正统一、高效、精确的重建方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniDCF通过统一的多模态深度学习框架，实现了对牙颅面硬组织的高精度、自动化重建，显著提高了临床效率和治疗效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dentocraniofacial hard tissue defects profoundly affect patients'physiological functions, facial aesthetics, and psychological well-being,posing significant challenges for precise reconstruction. Current deep learningmodels are limited to single-tissue scenarios and modality-specific imaginginputs, resulting in poor generalizability and trade-offs between anatomicalfidelity, computational efficiency, and cross-tissue adaptability. Here weintroduce UniDCF, a unified framework capable of reconstructing multipledentocraniofacial hard tissues through multimodal fusion encoding of pointclouds and multi-view images. By leveraging the complementary strengths of eachmodality and incorporating a score-based denoising module to refine surfacesmoothness, UniDCF overcomes the limitations of prior single-modalityapproaches. We curated the largest multimodal dataset, comprising intraoralscans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotatedinstances. Evaluations demonstrate that UniDCF outperforms existingstate-of-the-art methods in terms of geometric precision, structuralcompleteness, and spatial accuracy. Clinical simulations indicate UniDCFreduces reconstruction design time by 99% and achieves clinician-ratedacceptability exceeding 94%. Overall, UniDCF enables rapid, automated, andhigh-fidelity reconstruction, supporting personalized and precise restorativetreatments, streamlining clinical workflows, and enhancing patient outcomes.</description>
      <author>example@mail.com (Chunxia Ren, Ning Zhu, Yue Lai, Gui Chen, Ruijie Wang, Yangyi Hu, Suyao Liu, Shuwen Mao, Hong Su, Yu Zhang, Li Xiao)</author>
      <guid isPermaLink="false">2508.11728v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis</title>
      <link>http://arxiv.org/abs/2508.11721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统评估了眼科基础模型的性能，提出了FusionFM评估套件和两种融合方法，发现DINORET和RetiZero表现最佳，融合策略能带来适度改进，但全身性疾病预测仍有挑战&lt;h4&gt;背景&lt;/h4&gt;基础模型在医学图像分析中展现出提高下游任务泛化能力的巨大潜力，眼科领域已出现多种基础模型，但关于最佳模型、任务适应性和模型组合等基本问题尚无明确答案&lt;h4&gt;目的&lt;/h4&gt;提出FusionFM评估套件和两种融合方法，系统评估单一和融合的眼科基础模型，回答基础模型性能和组合效果等关键问题&lt;h4&gt;方法&lt;/h4&gt;构建涵盖眼科疾病检测和全身性疾病预测的框架，使用多国标准化数据集对RETFound、VisionFM、RetiZero和DINORET四种模型进行基准测试，采用AUC和F1指标评估性能&lt;h4&gt;主要发现&lt;/h4&gt;DINORET和RetiZero在各类任务中表现最佳，RetiZero在外部数据集上泛化能力更强；基于门控的融合策略对青光眼、AMD和高血压预测有适度改进；全身性疾病预测尤其是外部队列中的高血压预测仍面临挑战&lt;h4&gt;结论&lt;/h4&gt;研究为眼科基础模型提供了基于证据的评估，证明了模型融合的益处，并指出了提高临床适用性的可能策略&lt;h4&gt;翻译&lt;/h4&gt;基础模型在医学图像分析中展现出通过提高多样化下游任务的泛化能力而展现巨大潜力。在眼科领域，最近出现了几种基础模型，但对于基本问题仍无明确答案：哪种基础模型表现最佳？它们在不同任务上是否同样出色？如果我们组合所有基础模型会怎样？据我们所知，这是第一个系统评估单一和融合眼科基础模型的研究。为解决这些问题，我们提出了FusionFM，这是一个全面的评估套件，以及两种融合方法来整合不同的眼科基础模型。我们的框架涵盖了基于视网膜成像的眼科疾病检测(青光眼、糖尿病视网膜病变和年龄相关性黄斑变性)和全身性疾病预测(糖尿病和高血压)。我们使用来自多个国家的标准化数据集对四种最先进的基础模型(RETFound、VisionFM、RetiZero和DINORET)进行了基准测试，并使用AUC和F1指标评估其性能。我们的结果表明，DINORET和RetiZero在眼科和全身性疾病任务上都表现出卓越的性能，其中RetiZero在外部数据集上显示出更强的泛化能力。关于融合策略，基于门控的方法在预测青光眼、AMD和高血压方面提供了适度的改进。尽管取得了这些进展，预测全身性疾病，特别是在外部队列中的高血压仍然具有挑战性。这些发现为眼科基础模型提供了基于证据的评估，突显了模型融合的益处，并指出了提高其临床适用性的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) have shown great promise in medical image analysis byimproving generalization across diverse downstream tasks. In ophthalmology,several FMs have recently emerged, but there is still no clear answer tofundamental questions: Which FM performs the best? Are they equally good acrossdifferent tasks? What if we combine all FMs together? To our knowledge, this isthe first study to systematically evaluate both single and fused ophthalmicFMs. To address these questions, we propose FusionFM, a comprehensiveevaluation suite, along with two fusion approaches to integrate differentophthalmic FMs. Our framework covers both ophthalmic disease detection(glaucoma, diabetic retinopathy, and age-related macular degeneration) andsystemic disease prediction (diabetes and hypertension) based on retinalimaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,RetiZero, and DINORET) using standardized datasets from multiple countries andevaluated their performance using AUC and F1 metrics. Our results show thatDINORET and RetiZero achieve superior performance in both ophthalmic andsystemic disease tasks, with RetiZero exhibiting stronger generalization onexternal datasets. Regarding fusion strategies, the Gating-based approachprovides modest improvements in predicting glaucoma, AMD, and hypertension.Despite these advances, predicting systemic diseases, especially hypertensionin external cohort remains challenging. These findings provide anevidence-based evaluation of ophthalmic FMs, highlight the benefits of modelfusion, and point to strategies for enhancing their clinical applicability.</description>
      <author>example@mail.com (Ke Zou, Jocelyn Hui Lin Goh, Yukun Zhou, Tian Lin, Samantha Min Er Yew, Sahana Srinivasan, Meng Wang, Rui Santos, Gabor M. Somfai, Huazhu Fu, Haoyu Chen, Pearse A. Keane, Ching-Yu Cheng, Yih Chung Tham)</author>
      <guid isPermaLink="false">2508.11721v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>MATPAC++: Enhanced Masked Latent Prediction for Self-Supervised Audio Representation Learning</title>
      <link>http://arxiv.org/abs/2508.12709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种掩码潜在预测自监督学习方法的改进，通过整合多选学习来处理音频内容中的不明确性，提高了表征质量和系统性能。&lt;h4&gt;背景&lt;/h4&gt;掩码潜在预测是自监督学习中的前沿范式，特别适用于音频和音乐表征学习。现有方法中，用于解决预训练任务的预测器模块的作用被忽视，无法有效处理多声源音频内容中的不明确性。&lt;h4&gt;目的&lt;/h4&gt;明确建模预测的不明确性并提高表征质量，改进现有的MATPAC系统，使其在音频表征学习中达到更优性能。&lt;h4&gt;方法&lt;/h4&gt;将多选学习（Multiple Choice Learning, MCL）整合到MATPAC系统中，创建MATPAC++方法。通过线性探测和微调方式评估，采用统一协议与最先进方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;MATPAC++在AudioSet上微调时达到最先进性能，在下游任务上整体表现最优。在仅使用音乐数据训练时，模型在提高效率的同时也达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;整合多选学习到掩码潜在预测框架中能有效处理音频内容的不明确性，显著提高表征质量和系统性能，在音频和音乐表征学习领域具有广泛应用前景。&lt;h4&gt;翻译&lt;/h4&gt;掩码潜在预测已成为自监督学习的前沿范式，特别适用于通用音频和音乐表征学习。尽管最近的方法展示了强大的性能，但在这些SSL系统中用于解决预训练任务的预测器模块的作用主要被忽视，尽管它对于解决当前预训练任务至关重要。特别是，该模块应该能够处理音频内容固有的不明确性，特别是当它由多个声源组成时。这项工作提出了一种新颖的增强：整合多选学习来明确建模预测不明确性并提高表征质量。我们在最近提出的MATPAC系统基础上，通过MCL改进其预测和无监督分类预训练任务。我们通过在多个下游任务上进行线性探测和在AudioSet上进行微调，广泛评估了我们的方法MATPAC++，采用统一协议实现与最先进SSL方法的严格公平比较。结果表明，我们的提议在AudioSet上微调时达到最先进性能，在下游任务上整体达到最先进的分数。此外，我们通过仅在音乐数据上训练来检查领域专业化，我们的模型在显著提高效率的同时实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked latent prediction has emerged as a leading paradigm in self-supervisedlearning (SSL), especially for general audio and music representation learning.While recent methods have demonstrated strong performance, the role of thepredictor module used at the output of such SSL systems remains mainlyoverlooked, despite being crucial for solving the pretext task at hand. Inparticular, this module should be able to deal with the ambiguity inherent inaudio content, especially when it is composed of multiple sound sources. Thiswork proposes a novel enhancement: integrating Multiple Choice Learning (MCL)to explicitly model prediction ambiguity and improve representation quality. Webuild on top of the recently proposed MATPAC system, improving its predictionand unsupervised classification pretext tasks with MCL. We extensively evaluateour method, MATPAC++, through both linear probing across multiple downstreamtasks and fine-tuning on AudioSet, employing a unified protocol that enablesrigorous and fair comparisons with state-of-the-art SSL approaches. Resultsshow that our proposal achieves state-of-the-art when fine-tuned on AudioSetand overall state-of-the-art scores on downstream tasks. Additionally, weexamine domain specialisation by training exclusively on music data, where ourmodel achieves state-of-the-art performance with significantly improvedefficiency.</description>
      <author>example@mail.com (Aurian Quelennec, Pierre Chouteau, Geoffroy Peeters, Slim Essid)</author>
      <guid isPermaLink="false">2508.12709v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Asymmetric Diffusion Recommendation Model</title>
      <link>http://arxiv.org/abs/2508.12706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了非对称扩散推荐模型(AsymDiffRec)，通过非对称方式处理推荐系统中的离散数据空间问题，有效保留了个性化信息并提升了推荐性能。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在推荐系统中被用于加强表征学习，但大多数模型在连续数据空间中使用对称的高斯噪声，而推荐样本实际存在于离散数据空间，高斯噪声可能破坏潜在表征中的个性化信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖有效的方法解决基于扩散的推荐模型在离散数据空间中的问题，保留个性化信息并提高推荐性能。&lt;h4&gt;方法&lt;/h4&gt;采用非对称方式学习前向和反向过程，定义广义前向过程模拟缺失特征，在非对称潜在特征空间执行反向过程，引入面向任务的优化策略保留个性化信息，将缺失特征样本视为有噪声输入生成去噪表征。&lt;h4&gt;主要发现&lt;/h4&gt;通过在线A/B测试，用户活跃天数和应用程序使用时长分别提升+0.131%和+0.166%，离线实验也显示改进效果，AsymDiffRec已在抖音音乐应用中实现。&lt;h4&gt;结论&lt;/h4&gt;AsymDiffRec有效解决了推荐系统中离散数据空间的问题，通过非对称扩散过程和面向任务的优化策略保留了个性化信息并提高了推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，受扩散模型卓越成就的启发，扩散过程已被用于加强推荐系统中的表征学习。大多数基于扩散的推荐模型通常在连续数据空间中使用标准高斯噪声进行对称的前向和反向过程。然而，从推荐系统获取的样本存在于离散数据空间，这与连续数据空间有根本区别。此外，高斯噪声可能会破坏潜在表征中的个性化信息。在这项工作中，我们提出了一种新颖且有效的方法，名为非对称扩散推荐模型(AsymDiffRec)，以非对称方式学习前向和反向过程。我们定义了一个广义前向过程，模拟真实推荐样本中缺失的特征。然后在非对称的潜在特征空间中执行反向过程。为了保留潜在表征中的个性化信息，引入了面向任务的优化策略。在服务阶段，将具有缺失特征的原始样本视为有噪声输入，为最终预测生成去噪和鲁棒的表征。通过为基础模型配备AsymDiffRec，我们进行了在线A/B测试，在用户活跃天数和应用程序使用时长方面分别实现了+0.131%和+0.166%的改进。此外，扩展的离线实验也显示出改进效果。AsymDiffRec已在抖音音乐应用中实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760833&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, motivated by the outstanding achievements of diffusion models, thediffusion process has been employed to strengthen representation learning inrecommendation systems. Most diffusion-based recommendation models typicallyutilize standard Gaussian noise in symmetric forward and reverse processes incontinuous data space. Nevertheless, the samples derived from recommendationsystems inhabit a discrete data space, which is fundamentally different fromthe continuous one. Moreover, Gaussian noise has the potential to corruptpersonalized information within latent representations. In this work, wepropose a novel and effective method, named Asymmetric Diffusion RecommendationModel (AsymDiffRec), which learns forward and reverse processes in anasymmetric manner. We define a generalized forward process that simulates themissing features in real-world recommendation samples. The reverse process isthen performed in an asymmetric latent feature space. To preserve personalizedinformation within the latent representation, a task-oriented optimizationstrategy is introduced. In the serving stage, the raw sample with missingfeatures is regarded as a noisy input to generate a denoising and robustrepresentation for the final prediction. By equipping base models withAsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and+0.166% in terms of users' active days and app usage duration respectively.Additionally, the extended offline experiments also demonstrate improvements.AsymDiffRec has been implemented in the Douyin Music App.</description>
      <author>example@mail.com (Yongchun Zhu, Guanyu Jiang, Jingwu Chen, Feng Zhang, Xiao Yang, Zuotao Liu)</author>
      <guid isPermaLink="false">2508.12706v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Factorized Disentangled Representation Learning for Interpretable Radio Frequency Fingerprin</title>
      <link>http://arxiv.org/abs/2508.12660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的解缠表示学习框架，用于解决物联网设备射频指纹识别中各种变化因素相互纠缠的问题，通过学习多个因素的显式和独立表示，提高了识别的准确性和可控性。&lt;h4&gt;背景&lt;/h4&gt;随着物联网设备快速增长和安全风险上升，射频指纹成为设备识别和认证的关键。然而，信号传输到接收过程中各种变化因素会相互纠缠，降低射频指纹识别有效性。现有方法主要依赖领域自适应技术，缺乏明确因素表示，导致鲁棒性不足和控制有限。&lt;h4&gt;目的&lt;/h4&gt;提出一种解缠表示学习框架，学习包括射频指纹在内的多个因素的显式和独立表示，提高射频指纹识别的鲁棒性和对下游任务的控制能力。&lt;h4&gt;方法&lt;/h4&gt;设计了解缠表示学习框架，引入基于显式性、模块化和紧凑性原则指导的解缠模块。包含因素分类模块和信号重构模块两个专用模块，每个模块都有专门损失函数促进有效解缠并增强对下游任务支持。框架能提取一组可解释的向量明确表示相应因素。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共基准数据集和一个自收集数据集上评估，方法在多个解缠表示学习指标上取得优异性能。框架所有模块提高分类准确性，并能对条件生成信号进行精确控制。在下游射频指纹识别任务和条件信号生成任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;所提出的解缠表示学习框架有效解决了射频指纹识别中因素纠缠问题，通过显式和独立表示多个因素，提高了识别准确性和可控性，为可解释和显式射频指纹提供了新可能。&lt;h4&gt;翻译&lt;/h4&gt;针对物联网设备的快速增长和日益增长的安全风险，射频指纹已成为设备识别和认证的关键。然而，从信号传输到接收过程中，除了射频指纹本身之外的各种变化因素可能会相互纠缠，降低了射频指纹识别的有效性。现有的射频指纹识别方法主要依赖于领域自适应技术，这些技术通常缺乏明确的因素表示，导致鲁棒性不足和对下游任务的控制有限。为了解决这个问题，我们提出了一种新颖的解缠表示学习框架，该框架学习包括射频指纹在内的多个因素的显式和独立表示。我们的框架引入了基于显式性、模块化和紧凑性原则指导的解缠模块。我们设计了两个专用模块用于因素分类和信号重构，每个模块都有专门的损失函数，以促进有效的解缠并增强对下游任务的支持。因此，该框架可以提取一组可解释的向量，这些向量明确表示相应的因素。我们在两个公共基准数据集和一个自收集数据集上评估了我们的方法。我们的方法在多个解缠表示学习指标上取得了令人印象深刻的性能。我们还分析了我们的方法在下游射频指纹识别任务和条件信号生成任务上的有效性。框架的所有模块都有助于提高分类准确性，并能够对条件生成的信号进行精确控制。这些结果突显了我们的解缠表示学习框架在可解释和显式射频指纹方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In response to the rapid growth of Internet of Things (IoT) devices andrising security risks, Radio Frequency Fingerprint (RFF) has become key fordevice identification and authentication. However, various changing factors -beyond the RFF itself - can be entangled from signal transmission to reception,reducing the effectiveness of RFF Identification (RFFI). Existing RFFI methodsmainly rely on domain adaptation techniques, which often lack explicit factorrepresentations, resulting in less robustness and limited controllability fordownstream tasks. To tackle this problem, we propose a novel DisentangledRepresentation Learning (DRL) framework that learns explicit and independentrepresentations of multiple factors, including the RFF. Our frameworkintroduces modules for disentanglement, guided by the principles ofexplicitness, modularity, and compactness. We design two dedicated modules forfactor classification and signal reconstruction, each with tailored lossfunctions that encourage effective disentanglement and enhance support fordownstream tasks. Thus, the framework can extract a set of interpretablevectors that explicitly represent corresponding factors. We evaluate ourapproach on two public benchmark datasets and a self-collected dataset. Ourmethod achieves impressive performance on multiple DRL metrics. We also analyzethe effectiveness of our method on downstream RFFI task and conditional signalgeneration task. All modules of the framework contribute to improvedclassification accuracy, and enable precise control over conditional generatedsignals. These results highlight the potential of our DRL framework forinterpretable and explicit RFFs.</description>
      <author>example@mail.com (Yezhuo Zhang, Zinan Zhou, Guangyu Li, Xuanpeng Li)</author>
      <guid isPermaLink="false">2508.12660v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion</title>
      <link>http://arxiv.org/abs/2508.12484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合CNN-Transformer和CKAN的混合模型用于皮肤癌分类，通过迁移学习和数据增强技术，在多个基准数据集上实现了高准确率和F1分数，证明了该方法的有效性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;皮肤癌分类在医学图像分析中是至关重要的任务，准确区分良性和恶性病变对早期诊断和治疗至关重要。&lt;h4&gt;目的&lt;/h4&gt;探索结合卷积神经网络(CNN)和Transformer的顺序和并行混合模型，并使用卷积Kolmogorov-Arnold网络(CKAN)，以提高皮肤癌分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;结合迁移学习和大规模数据增强，利用CNN提取局部空间特征，Transformer建模全局依赖关系，CKAN促进非线性特征融合，并在多个基准数据集(HAM10000、BCN20000和PAD-UFES)上评估模型，考虑不同的数据分布和类别不平衡。&lt;h4&gt;主要发现&lt;/h4&gt;混合CNN-Transformer架构能有效捕获空间和上下文特征；集成CKAN通过可学习的激活函数增强特征融合；在HAM10000数据集上达到92.81%的准确率和92.47%的F1分数；在PAD-UFES数据集上达到97.83%的准确率和97.83%的F1分数；在BCN20000数据集上达到91.17%的准确率和91.79%的F1分数。&lt;h4&gt;结论&lt;/h4&gt;特征表示和模型设计在推进稳健和准确的医学图像分类方面具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;皮肤癌分类是医学图像分析中的一个关键任务，其中准确区分良性和恶性病变对早期诊断和治疗至关重要。在本研究中，我们探索了结合卷积Kolmogorov-Arnold网络(CKAN)的顺序和并行混合CNN-Transformer模型。我们的方法结合了迁移学习和大规模数据增强，其中CNN提取局部空间特征，Transformer建模全局依赖关系，CKAN促进非线性特征融合以改进表示学习。为了评估泛化能力，我们在多个基准数据集(HAM10000、BCN20000和PAD-UFES)上评估了我们的模型，考虑不同的数据分布和类别不平衡。实验结果表明，混合CNN-Transformer架构能有效捕获空间和上下文特征，从而提高分类性能。此外，集成CKAN通过可学习的激活函数增强特征融合，产生更具判别力的表示。我们提出的方法在皮肤癌分类中取得了具有竞争力的性能，在HAM10000数据集上达到92.81%的准确率和92.47%的F1分数，在PAD-UFES数据集上达到97.83%的准确率和97.83%的F1分数，在BCN20000数据集上达到91.17%的准确率和91.79%的F1分数，突显了我们的模型在不同数据集上的有效性和泛化能力。这项研究强调了特征表示和模型设计在推进稳健和准确的医学图像分类方面的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skin cancer classification is a crucial task in medical image analysis, whereprecise differentiation between malignant and non-malignant lesions isessential for early diagnosis and treatment. In this study, we exploreSequential and Parallel Hybrid CNN-Transformer models with ConvolutionalKolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning andextensive data augmentation, where CNNs extract local spatial features,Transformers model global dependencies, and CKAN facilitates nonlinear featurefusion for improved representation learning. To assess generalization, weevaluate our models on multiple benchmark datasets (HAM10000,BCN20000 andPAD-UFES) under varying data distributions and class imbalances. Experimentalresults demonstrate that hybrid CNN-Transformer architectures effectivelycapture both spatial and contextual features, leading to improvedclassification performance. Additionally, the integration of CKAN enhancesfeature fusion through learnable activation functions, yielding morediscriminative representations. Our proposed approach achieves competitiveperformance in skin cancer classification, demonstrating 92.81% accuracy and92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score onthe PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000dataset highlighting the effectiveness and generalizability of our model acrossdiverse datasets. This study highlights the significance of featurerepresentation and model design in advancing robust and accurate medical imageclassification.</description>
      <author>example@mail.com (Shubhi Agarwal, Amulya Kumar Mahto)</author>
      <guid isPermaLink="false">2508.12484v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Uncovering Emergent Physics Representations Learned In-Context by Large Language Models</title>
      <link>http://arxiv.org/abs/2508.12448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了大型语言模型的上下文学习能力，特别是在物理推理方面的能力。研究通过物理系统中的动力学预测任务，使用稀疏自编码器分析模型残差流激活，揭示了LLMs在上下文学习中能够编码有意义的物理概念。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型展现出令人印象深刻的上下文学习能力，能够仅通过文本提示解决各种任务。然而，目前仍不清楚LLMs中允许成功执行上下文学习的精确机制或内部结构。物理任务为探索这一挑战提供了有希望的测试平台，因为物理系统提供基于基本原理的结构化动力学的实验可控、真实世界数据。&lt;h4&gt;目的&lt;/h4&gt;通过物理系统中的动力学预测任务，探究LLMs的上下文学习能力，特别是它们推理物理的能力，以确定LLMs是否能够在上下文中学习物理。&lt;h4&gt;方法&lt;/h4&gt;使用动力学预测任务作为代理来评估LLMs的物理推理能力。首先展示动力学预测性能随输入上下文长度提高。使用稀疏自编码器(SAEs)分析模型的残差流激活，以揭示这种能力如何在LLMs中出现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SAEs捕捉到的特征与关键物理变量(如能量)相关，表明有意义的物理概念被编码在LLMs的上下文学习中。此外，研究发现动力学预测的上下文性能随着输入上下文长度的增加而提高。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一个新颖的案例研究，扩展了我们对LLMs如何进行上下文学习的理解。研究表明，LLMs能够在上下文学习中编码有意义的物理概念，为理解LLMs的内部工作机制提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型展现出令人印象深刻的上下文学习能力，使他们能够仅通过文本提示解决广泛范围的任务。随着这些能力的进步，适用领域的范围也在显著扩大。然而，识别LLMs中允许成功执行跨不同、 distinct任务类别的上下文学习的精确机制或内部结构仍然难以捉摸。基于物理的任务为探索这一挑战提供了有希望的测试平台。与基本算术或符号方程等合成序列不同，物理系统提供基于基本原理的结构化动力学的实验可控、真实世界数据。这使它们特别适合在现实 yet 可管理的环境中研究LLMs的涌现推理行为。在这里，我们从机制上研究LLMs的上下文学习能力，特别关注它们推理物理的能力。使用物理系统中的动力学预测任务作为代理，我们评估LLMs是否能够在上下文中学习物理。我们首先展示了上下文中的动力学预测性能随输入上下文长度的增加而提高。为了揭示这种能力如何在LLMs中出现，我们使用稀疏自编码器(SAEs)分析模型的残差流激活。我们的实验揭示SAEs捕捉到的特征与关键物理变量(如能量)相关。这些发现表明有意义的物理概念被编码在LLMs的上下文学习中。总之，我们的工作提供了一个新颖的案例研究，扩展了我们对LLMs如何进行上下文学习的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) exhibit impressive in-context learning (ICL)abilities, enabling them to solve wide range of tasks via textual promptsalone. As these capabilities advance, the range of applicable domains continuesto expand significantly. However, identifying the precise mechanisms orinternal structures within LLMs that allow successful ICL across diverse,distinct classes of tasks remains elusive. Physics-based tasks offer apromising testbed for probing this challenge. Unlike synthetic sequences suchas basic arithmetic or symbolic equations, physical systems provideexperimentally controllable, real-world data based on structured dynamicsgrounded in fundamental principles. This makes them particularly suitable forstudying the emergent reasoning behaviors of LLMs in a realistic yet tractablesetting. Here, we mechanistically investigate the ICL ability of LLMs,especially focusing on their ability to reason about physics. Using a dynamicsforecasting task in physical systems as a proxy, we evaluate whether LLMs canlearn physics in context. We first show that the performance of dynamicsforecasting in context improves with longer input contexts. To uncover how suchcapability emerges in LLMs, we analyze the model's residual stream activationsusing sparse autoencoders (SAEs). Our experiments reveal that the featurescaptured by SAEs correlate with key physical variables, such as energy. Thesefindings demonstrate that meaningful physical concepts are encoded within LLMsduring in-context learning. In sum, our work provides a novel case study thatbroadens our understanding of how LLMs learn in context.</description>
      <author>example@mail.com (Yeongwoo Song, Jaeyong Bae, Dong-Kyum Kim, Hawoong Jeong)</author>
      <guid isPermaLink="false">2508.12448v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems</title>
      <link>http://arxiv.org/abs/2508.12375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种层次化知识引导的故障强度诊断框架(HKG)，通过图卷积网络捕捉类别间的依赖关系，并开发重新加权的层次化知识相关性矩阵(Re-HKCM)方案，在多个真实工业数据集上验证了其优越性。&lt;h4&gt;背景&lt;/h4&gt;故障强度诊断(FID)在复杂工业系统中机械设备的监控和维护中起关键作用，但现有方法基于思维链且未考虑目标类别间的依赖关系。&lt;h4&gt;目的&lt;/h4&gt;捕捉和探索类别之间的依赖关系，提高故障强度诊断的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出受思维树启发的层次化知识引导的故障强度诊断框架(HKG)，使用图卷积网络映射层次拓扑图，开发重新加权的层次化知识相关性矩阵(Re-HKCM)方案，将类间层次化知识嵌入统计相关性矩阵。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实工业数据集(包括SAMSON AG的三个气蚀数据集和一个公开数据集)上的实验显示，所提方法结果优越，优于最近的先进FID方法。&lt;h4&gt;结论&lt;/h4&gt;HKG框架和Re-HKCM方案能有效捕捉类别间依赖关系，提高故障强度诊断性能，避免了过平滑问题。&lt;h4&gt;翻译&lt;/h4&gt;故障强度诊断(FID)在复杂工业系统中机械设备的监控和维护中起着关键作用。由于当前FID方法基于思维链而未考虑目标类别间的依赖关系。为了捕捉和探索这些依赖关系，我们提出了一种受思维树启发的层次化知识引导的故障强度诊断框架(HKG)，该框架适用于任何表示学习方法。HKG使用图卷积网络将类别表示的层次拓扑图映射为一组相互依赖的全局层次分类器，其中每个节点由类别的词嵌入表示。这些全局层次分类器应用于通过表示学习提取的深度特征，使整个模型能够端到端学习。此外，我们通过将类间层次化知识嵌入数据驱动的统计相关性矩阵(SCM)中，开发了一种重新加权的层次化知识相关性矩阵(Re-HKCM)方案，有效指导了图卷积神经网络中节点的信息共享并避免了过平滑问题。Re-HKCM通过一系列数学变换从SCM推导得出。我们在四个来自不同工业领域的真实世界数据集(包括SAMSON AG的三个气蚀数据集和一个现有的公开数据集)上进行了大量FID实验，所有结果都显示出优越性并优于最近的先进FID方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3637528.3671610&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fault intensity diagnosis (FID) plays a pivotal role in monitoring andmaintaining mechanical devices within complex industrial systems. As currentFID methods are based on chain of thought without considering dependenciesamong target classes. To capture and explore dependencies, we propose ahierarchical knowledge guided fault intensity diagnosis framework (HKG)inspired by the tree of thought, which is amenable to any representationlearning methods. The HKG uses graph convolutional networks to map thehierarchical topological graph of class representations into a set ofinterdependent global hierarchical classifiers, where each node is denoted byword embeddings of a class. These global hierarchical classifiers are appliedto learned deep features extracted by representation learning, allowing theentire model to be end-to-end learnable. In addition, we develop a re-weightedhierarchical knowledge correlation matrix (Re-HKCM) scheme by embeddinginter-class hierarchical knowledge into a data-driven statistical correlationmatrix (SCM) which effectively guides the information sharing of nodes ingraphical convolutional neural networks and avoids over-smoothing issues. TheRe-HKCM is derived from the SCM through a series of mathematicaltransformations. Extensive experiments are performed on four real-worlddatasets from different industrial domains (three cavitation datasets fromSAMSON AG and one existing publicly) for FID, all showing superior results andoutperform recent state-of-the-art FID methods.</description>
      <author>example@mail.com (Yu Sha, Shuiping Gou, Bo Liu, Johannes Faber, Ningtao Liu, Stefan Schramm, Horst Stoecker, Thomas Steckenreiter, Domagoj Vnucec, Nadine Wetzstein, Andreas Widl, Kai Zhou)</author>
      <guid isPermaLink="false">2508.12375v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding</title>
      <link>http://arxiv.org/abs/2508.11999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为MOON的生成式多模态大语言模型(MLLM)，用于改进产品表示学习，解决了现有方法在多图像与文本多对一对齐问题上的局限性。&lt;h4&gt;背景&lt;/h4&gt;随着电子商务快速发展，探索通用表示而非特定任务表示引起研究关注。现有判别性双流架构难以建模多个产品图像和文本之间的多对一对齐关系。&lt;h4&gt;目的&lt;/h4&gt;利用生成式多模态大语言模型改进产品表示学习，解决三个关键挑战：典型LLMs缺乏多模态和方面感知建模模块；产品图像中存在背景噪声；缺乏评估标准基准。&lt;h4&gt;方法&lt;/h4&gt;1) 采用引导的专家混合(MoE)模块针对多模态和特定方面产品内容建模；2) 检测产品图像核心语义区域减轻背景噪声干扰；3) 引入专门负采样策略增加负样本难度和多样性；4) 发布大规模多模态基准MBE用于产品理解任务。&lt;h4&gt;主要发现&lt;/h4&gt;MOON模型在自建基准和公共数据集上展现具有竞争力的零样本性能，在跨模态检索、产品分类和属性预测等多种下游任务上表现良好，案例研究和可视化证实了其有效性。&lt;h4&gt;结论&lt;/h4&gt;MOON作为首个基于生成式MLLM的产品表示学习模型，通过解决多模态建模、背景噪声干扰和评估基准缺失等问题，显著提升了产品表示学习效果，并在多种任务上展现出优异性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;随着电子商务的快速发展，探索通用表示而非特定任务的表示已引起越来越多的研究关注。对于产品理解，尽管现有的判别性双流架构推动了该领域的进步，但它们本质上难以建模多个产品图像和文本之间的多对一对齐关系。因此，我们认为生成式多模态大语言模型(MLLM)在改进产品表示学习方面具有巨大潜力。然而，由于几个关键挑战，实现这一目标仍然非同寻常：典型LLMs中缺乏多模态和方面感知建模模块；产品图像中普遍存在背景噪声；以及缺乏用于评估的标准基准。为了解决这些问题，我们提出了首个基于生成式MLLM的产品表示学习模型MOON。我们的方法(1)采用引导的专家混合(MoE)模块针对多模态和特定方面的产品内容进行建模；(2)有效检测产品图像中的核心语义区域，减轻背景噪声造成的干扰；(3)引入专门的负采样策略，增加负样本的难度和多样性。此外，我们发布了一个大规模多模态基准MBE，用于各种产品理解任务。实验表明，我们的模型在我们提出的基准和公共数据集上都展示了具有竞争力的零样本性能，展示了在包括跨模态检索、产品分类和属性预测在内的各种下游任务上的强大泛化能力。此外，案例研究和可视化说明了MOON在产品理解方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of e-commerce, exploring general representationsrather than task-specific ones has attracted increasing research attention. Forproduct understanding, although existing discriminative dual-flow architecturesdrive progress in this field, they inherently struggle to model the many-to-onealignment between multiple images and texts of products. Therefore, we arguethat generative Multimodal Large Language Models (MLLMs) hold significantpotential for improving product representation learning. Nevertheless,achieving this goal still remains non-trivial due to several key challenges:the lack of multimodal and aspect-aware modeling modules in typical LLMs; thecommon presence of background noise in product images; and the absence of astandard benchmark for evaluation. To address these issues, we propose thefirst generative MLLM-based model named MOON for product representationlearning. Our method (1) employs a guided Mixture-of-Experts (MoE) module fortargeted modeling of multimodal and aspect-specific product content; (2)effectively detects core semantic regions in product images to mitigate thedistraction and interference caused by background noise; and (3) introduces thespecialized negative sampling strategy to increase the difficulty and diversityof negative samples. In addition, we release a large-scale multimodal benchmarkMBE for various product understanding tasks. Experimentally, our modeldemonstrates competitive zero-shot performance on both our benchmark and thepublic dataset, showcasing strong generalization across various downstreamtasks, including cross-modal retrieval, product classification, and attributeprediction. Furthermore, the case study and visualization illustrate theeffectiveness of MOON for product understanding.</description>
      <author>example@mail.com (Daoze Zhang, Zhanheng Nie, Jianyu Liu, Chenghan Fu, Wanxian Guan, Yuan Gao, Jun Song, Pengjie Wang, Jian Xu, Bo Zheng)</author>
      <guid isPermaLink="false">2508.11999v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations</title>
      <link>http://arxiv.org/abs/2508.11978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的双曲推荐模型，利用几何洞察提高表示学习和计算稳定性，通过重新定义双曲距离和学习更丰富的用户和项目表示，改进了传统欧几里得空间的局限性，并使用三元组损失函数更好地捕捉用户-项目交互。&lt;h4&gt;背景&lt;/h4&gt;最近的研究已经证明了双曲几何在捕捉推荐系统中交互数据的复杂模式方面的潜力，推荐系统需要处理复杂的用户-项目交互数据。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的双曲推荐模型，利用几何洞察同时提高表示学习和计算稳定性，重新定义双曲距离概念以释放更多表示能力，学习更具表现力的用户和项目表示。&lt;h4&gt;方法&lt;/h4&gt;重新定义双曲距离概念以释放比传统欧几里得空间更多的表示能力，构建一个三元组损失函数，通过几何数据驱动的成对交互项组合来建模用户及其相应偏好和非偏好选择之间的三元关系。&lt;h4&gt;主要发现&lt;/h4&gt;双曲方法不仅优于现有的欧几里得和双曲模型，还减少了流行度偏差，导致更多样化和个性化的推荐。&lt;h4&gt;结论&lt;/h4&gt;双曲几何为推荐系统中的表示学习提供了一种有效方法，通过改进的几何表示可以实现更好的推荐性能和多样性。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究已经证明了双曲几何在捕捉推荐系统中交互数据的复杂模式方面的潜力。在这项工作中，我们引入了一种新颖的双曲推荐模型，利用几何洞察来同时提高表示学习和计算稳定性。我们重新定义了双曲距离的概念，以释放比传统欧几里得空间更多的表示能力，并学习更具表现力的用户和项目表示。为了更好地捕捉用户-项目交互，我们构建了一个三元组损失函数，通过几何数据驱动的成对交互项组合来建模用户及其相应偏好和非偏好选择之间的三元关系。我们的双曲方法不仅优于现有的欧几里得和双曲模型，还减少了流行度偏差，从而带来更多样化和个性化的推荐。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have demonstrated the potential of hyperbolic geometry forcapturing complex patterns from interaction data in recommender systems. Inthis work, we introduce a novel hyperbolic recommendation model that usesgeometrical insights to improve representation learning and increasecomputational stability at the same time. We reformulate the notion ofhyperbolic distances to unlock additional representation capacity overconventional Euclidean space and learn more expressive user and itemrepresentations. To better capture user-items interactions, we construct atriplet loss that models ternary relations between users and theircorresponding preferred and nonpreferred choices through a mix of pairwiseinteraction terms driven by the geometry of data. Our hyperbolic approach notonly outperforms existing Euclidean and hyperbolic models but also reducespopularity bias, leading to more diverse and personalized recommendations.</description>
      <author>example@mail.com (Viacheslav Yusupov, Maxim Rakhuba, Evgeny Frolov)</author>
      <guid isPermaLink="false">2508.11978v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</title>
      <link>http://arxiv.org/abs/2508.13073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page:  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是关于基于大型视觉语言模型的视觉-语言-动作模型在机器人操作中的系统性综述，提供了首个分类导向的回顾。&lt;h4&gt;背景&lt;/h4&gt;机器人操作是机器人和具身AI的关键前沿，需要精确的电机控制和多模态理解。传统基于规则的方法在非结构化、新颖环境中无法扩展或泛化。近年来，基于大型视觉语言模型的视觉-语言-动作模型已成为变革性范式。&lt;h4&gt;目的&lt;/h4&gt;提供对用于机器人操作的大型VLM-based VLA模型的第一个系统性、分类导向的回顾，解决现有分类不一致性，减轻研究碎片化，填补大型VLMs与机器人操作交叉领域的研究空白。&lt;h4&gt;方法&lt;/h4&gt;明确定义大型VLM-based VLA模型，划分两种主要架构范式：单体模型（包括单系统和双系统设计）和分层模型（通过可解释中间表示解耦规划与执行）。深入探讨模型与先进领域的整合、独特特征合成以及未来发展方向。&lt;h4&gt;主要发现&lt;/h4&gt;大型VLM-based VLA模型主要分为单体模型和分层模型两大架构。这些模型与强化学习、无需训练的优化等领域相结合，展现出独特的架构特征和操作优势。未来发展方向包括记忆机制、4D感知、高效适应和多智能体合作等。&lt;h4&gt;结论&lt;/h4&gt;综述整合了最新进展，通过系统整合大型VLMs与机器人操作交叉领域研究，解决了分类不一致性和研究碎片化问题，填补了关键空白，并提供了定期更新的项目页面记录持续进展。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作是机器人和具身AI的关键前沿，需要精确的电机控制和多模态理解，然而传统的基于规则的方法在非结构化、新颖环境中无法扩展或泛化。近年来，基于在大量图像-文本数据上预训练的大型视觉语言模型的视觉-语言-动作模型已成为一种变革性范式。本综述提供了首个针对用于机器人操作的大型VLM-based VLA模型的系统性、分类导向的回顾。我们首先明确定义大型VLM-based VLA模型，并划分两种主要架构范式：(1) 单体模型，包括具有不同集成程度的单系统和双系统设计；(2) 分层模型，通过可解释的中间表示明确解耦规划与执行。基于此，我们深入探讨了大型VLM-based VLA模型：(1) 与先进领域的整合，包括强化学习、无需训练的优化、从人类视频中学习以及世界模型整合；(2) 独特特征的合成，整合架构特征、操作优势以及支持其发展的数据集和基准；(3) 有前途的方向的识别，包括记忆机制、4D感知、高效适应、多智能体合作和其他新兴能力。本综述整合了最新进展，通过系统整合大型VLMs与机器人操作交叉领域的研究，解决了现有分类中的不一致性，减轻了研究碎片化，并填补了关键空白。我们提供了一个定期更新的项目页面来记录持续进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic manipulation, a key frontier in robotics and embodied AI, requiresprecise motor control and multimodal understanding, yet traditional rule-basedmethods fail to scale or generalize in unstructured, novel environments. Inrecent years, Vision-Language-Action (VLA) models, built upon LargeVision-Language Models (VLMs) pretrained on vast image-text datasets, haveemerged as a transformative paradigm. This survey provides the firstsystematic, taxonomy-oriented review of large VLM-based VLA models for roboticmanipulation. We begin by clearly defining large VLM-based VLA models anddelineating two principal architectural paradigms: (1) monolithic models,encompassing single-system and dual-system designs with differing levels ofintegration; and (2) hierarchical models, which explicitly decouple planningfrom execution via interpretable intermediate representations. Building on thisfoundation, we present an in-depth examination of large VLM-based VLA models:(1) integration with advanced domains, including reinforcement learning,training-free optimization, learning from human videos, and world modelintegration; (2) synthesis of distinctive characteristics, consolidatingarchitectural traits, operational strengths, and the datasets and benchmarksthat support their development; (3) identification of promising directions,including memory mechanisms, 4D perception, efficient adaptation, multi-agentcooperation, and other emerging capabilities. This survey consolidates recentadvances to resolve inconsistencies in existing taxonomies, mitigate researchfragmentation, and fill a critical gap through the systematic integration ofstudies at the intersection of large VLMs and robotic manipulation. We providea regularly updated project page to document ongoing progress:https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.</description>
      <author>example@mail.com (Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie)</author>
      <guid isPermaLink="false">2508.13073v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>On the N-elliptic localized solutions to the derivative nonlinear Schrödinger equation and their asymptotic analysis</title>
      <link>http://arxiv.org/abs/2508.12882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了DNLS方程的椭圆函数解，通过四个独立参数进行参数化，并利用Darboux-Bäcklund变换生成了两种等价的N-椭圆局域解形式。这些解表示为行列式比值的形式，分析表明解之间的碰撞是弹性的，并建立了严格弹性碰撞的充分条件。&lt;h4&gt;背景&lt;/h4&gt;研究DNLS方程的椭圆函数解及其渐进行为&lt;h4&gt;目的&lt;/h4&gt;生成和分析DNLS方程的N-椭圆局域解，验证其碰撞弹性行为&lt;h4&gt;方法&lt;/h4&gt;使用四个独立参数参数化椭圆函数解，通过Darboux-Bäcklund变换生成解的形式，分析t→±∞时的渐进行为&lt;h4&gt;主要发现&lt;/h4&gt;1) 解表示为行列式比值；2) 解之间的碰撞是弹性的；3) 解在每个传播方向上趋于简单椭圆局域解；4) 在传播方向之间解渐近接近移动背景；5) 解表现出广义孤子分解猜想预测的行为&lt;h4&gt;结论&lt;/h4&gt;N-椭圆局域解在DNLS方程中表现出弹性碰撞行为，满足广义孤子分解猜想&lt;h4&gt;翻译&lt;/h4&gt;我们用四个独立参数对DNLS方程的椭圆函数解进行参数化，并通过Darboux-Bäcklund变换生成DNLS方程的两种等价的N-椭圆局域解形式。N-椭圆局域解表示为(导数形式的)行列式比值，其中行列式的条目用Weierstrass sigma函数表示。此外，分析了两种N-椭圆局域解在传播方向上和传播方向之间的渐进行为(t→±∞)，验证了椭圆解之间的碰撞是弹性的。我们证明了解在每个传播方向上趋于简单的椭圆局域解。在传播方向之间，解渐近接近一个移动的背景。此外，我们建立了严格弹性碰撞的充分条件。系统地研究了解的动态行为，并通过图形可视化展示了分析结果。这些解的渐近分析确认了它们表现出在椭圆函数背景下的广义孤子分解猜想预测的行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We parameterize the elliptic function solutions to the derivative nonlinearSchr\"odinger (DNLS) equation with four independent parameters and generate twoequivalent forms of N-elliptic localized solutions to the DNLS equation throughthe Darboux-B\"acklund transformation. The N-elliptic localized solutions areexpressed as (the derivative of) the ratios of determinants with entries interms of Weierstrass sigma functions. Moreover, the asymptotic behaviors ofboth forms of N-elliptic localized solutions are analyzed along and between thepropagation directions as $t \rightarrow \pm\infty$, which verify that thecollisions between elliptic-solutions are elastic. We prove that the solutiontends to a simple elliptic localized solution along each propagation direction.Between the propagation directions, the solution asymptotically approaches ashifted background. Furthermore, we establish sufficient conditions forstrictly elastic collisions. The dynamic behaviors of the solutions aresystematically investigated, with analytical results visualized throughgraphical illustrations. The asymptotic analysis of these solutions confirmsthat they exhibit the behavior predicted by the generalized soliton resolutionconjecture on the elliptic function background.</description>
      <author>example@mail.com (Liming Ling, Wang Tang)</author>
      <guid isPermaLink="false">2508.12882v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics</title>
      <link>http://arxiv.org/abs/2508.12456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 40 figures. Framework combining Liquid Time-Constant Neural  Networks with autonomous marine robotics for oil spill trajectory prediction  and response coordination&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一个结合多智能体群体机器人系统和液态时间常数神经网络的集成框架，用于海洋溢油的实时预测、动态跟踪和快速响应。&lt;h4&gt;背景&lt;/h4&gt;海洋溢油对环境和经济构成严重风险，威胁海洋生态系统、海岸线和相关产业。由于风、洋流和温度等物理、化学和环境因素的相互作用，预测和管理溢油轨迹极为复杂，这使得及时有效的应对具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发准确实时的溢油轨迹预测和协调减轻措施系统，以减少溢油灾难的环境和经济影响。&lt;h4&gt;方法&lt;/h4&gt;引入一个综合框架，结合基于MOOS-IvP平台的多智能体群体机器人系统和液态时间常数神经网络，将自适应机器学习与自主海洋机器人相结合。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的系统能够实现溢油运动的高精度实时预测；使用深水地平线溢油数据验证，LTC-RK4模型达到0.96的空间精度，比LSTM方法提高23%；群体智能使机器人智能体之间能够进行去中心化、可扩展和有弹性的决策，增强了集体监测和封堵能力。&lt;h4&gt;结论&lt;/h4&gt;先进的神经建模与自主协调机器人的结合在预测精度、灵活性和操作可扩展性方面显示出显著改进，为可持续、自主的溢油管理和环境保护推进了最先进的技术。&lt;h4&gt;翻译&lt;/h4&gt;海洋溢油对环境和经济构成严重风险，威胁海洋生态系统、海岸线和相关产业。由于风、洋流和温度等物理、化学和环境因素的相互作用，预测和管理溢油轨迹极为复杂，这使得及时有效的应对具有挑战性。准确实时的轨迹预测和协调减轻措施对于减少这些灾难的影响至关重要。本研究引入了一个综合框架，结合了基于MOOS-IvP平台的多智能体群体机器人系统和液态时间常数神经网络。所提出的系统将自适应机器学习与自主海洋机器人相结合，能够实时预测、动态跟踪和快速应对不断变化的溢油。通过利用LTCNs（非常适合建模复杂的时间相关过程），该框架实现了溢油运动的高精度实时预测。群体智能使机器人智能体之间能够进行去中心化、可扩展和有弹性的决策，增强了集体监测和封堵工作。我们的方法使用深水地平线溢油数据进行了验证，其中LTC-RK4模型达到了0.96的空间精度，比LSTM方法提高了23%。先进的神经建模与自主协调机器人的集成在预测精度、灵活性和操作可扩展性方面显示出显著改进。最终，这项研究通过增强轨迹预测和响应协调，为可持续、自主的溢油管理和环境保护推进了最先进的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Marine oil spills pose grave environmental and economic risks, threateningmarine ecosystems, coastlines, and dependent industries. Predicting andmanaging oil spill trajectories is highly complex, due to the interplay ofphysical, chemical, and environmental factors such as wind, currents, andtemperature, which makes timely and effective response challenging. Accuratereal-time trajectory forecasting and coordinated mitigation are vital forminimizing the impact of these disasters. This study introduces an integratedframework combining a multi-agent swarm robotics system built on the MOOS-IvPplatform with Liquid Time-Constant Neural Networks (LTCNs). The proposed systemfuses adaptive machine learning with autonomous marine robotics, enablingreal-time prediction, dynamic tracking, and rapid response to evolving oilspills. By leveraging LTCNs--well-suited for modeling complex, time-dependentprocesses--the framework achieves real-time, high-accuracy forecasts of spillmovement. Swarm intelligence enables decentralized, scalable, and resilientdecision-making among robot agents, enhancing collective monitoring andcontainment efforts. Our approach was validated using data from the DeepwaterHorizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,surpassing LSTM approaches by 23%. The integration of advanced neural modelingwith autonomous, coordinated robotics demonstrates substantial improvements inprediction precision, flexibility, and operational scalability. Ultimately,this research advances the state-of-the-art for sustainable, autonomous oilspill management and environmental protection by enhancing both trajectoryprediction and response coordination.</description>
      <author>example@mail.com (Hadas C. Kuzmenko, David Ehevich, Oren Gal)</author>
      <guid isPermaLink="false">2508.12456v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Has GPT-5 Achieved Spatial Intelligence? An Empirical Study</title>
      <link>http://arxiv.org/abs/2508.13142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态模型在空间理解和推理方面仍存在明显局限，即使是GPT-5这样的先进模型也未能达到人类水平。作者通过全面评估发现，专有模型在面对最困难问题时并未表现出决定性优势。&lt;h4&gt;背景&lt;/h4&gt;多模态模型在近年来取得了显著进展，但在空间理解和推理方面仍存在明显局限，这些能力是实现通用人工智能的基础。随着GPT-5的发布，现在是评估领先模型在空间智能道路上所处位置的好时机。&lt;h4&gt;目的&lt;/h4&gt;提出一个全面的空间任务分类法，统一现有基准并讨论确保公平评估的挑战；评估最先进的专有和开源模型在八个关键基准上的表现。&lt;h4&gt;方法&lt;/h4&gt;对最先进的专有和开源模型在八个关键基准上进行评估，总成本超过十亿个标记；同时进行定性评估，考察对人类直观但先进多模态模型仍然失败的多样化场景。&lt;h4&gt;主要发现&lt;/h4&gt;1) GPT-5在空间智能方面展现出前所未有的强度；2) 但在广泛任务范围内仍不及人类表现；3) 多模态模型面临更具挑战性的空间智能问题；4) 专有模型在面对最困难问题时并未表现出决定性优势；5) 多样化场景的定性评估显示先进模型仍有明显不足。&lt;h4&gt;结论&lt;/h4&gt;尽管多模态模型取得了显著进展，但在空间理解和推理方面仍存在明显局限，即使是GPT-5这样的先进模型也未能达到人类水平，表明在实现真正的空间智能方面仍有很长的路要走。&lt;h4&gt;翻译&lt;/h4&gt;多模态模型近年来取得了显著进展。然而，它们在空间理解和推理方面仍然表现出明显的局限性，这些是实现通用人工智能的基本能力。随着最近GPT-5的发布，据称是迄今为止最强大的AI模型，现在是审视领先模型在通向空间智能道路上的所处位置的好时机。首先，我们提出了一个统一现有基准的空间任务综合分类法，并讨论了确保公平评估的挑战。随后，我们在八个关键基准上评估了最先进的专有和开源模型，总成本超过十亿个标记。我们的实证研究表明：(1) GPT-5在空间智能方面展现出前所未有的强度，但(2)在广泛任务范围内仍不及人类表现。此外，我们(3)确定了多模态模型面临更具挑战性的空间智能问题，并且(4)专有模型在面对最困难问题时并未表现出决定性优势。此外，我们在对人类直观但在最先进的多模态模型上仍然失败的多样化场景进行了定性评估。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要想评估GPT-5是否已经实现了空间智能。这个问题很重要，因为空间理解和推理是实现通用人工智能的基本能力，没有它，AI无法完全理解和与物理世界互动。尽管多模态模型取得了很大进步，但它们在空间任务上仍然表现不佳，而这些任务对人类来说很简单。了解GPT-5在这方面的能力，对于评估AI的进步和未来发展方向至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先提出了一个全面的空间任务分类法，统一了现有基准测试，并讨论了确保公平评估的挑战。然后他们在八个关键基准上评估了最先进的模型，总成本超过10亿个标记。作者标准化了提示、评估策略和指标，确保公平比较。他们使用了三种答案匹配方法和圆形评估策略来减少偏差。作者确实借鉴了现有工作，包括采用SITE的CAA指标、OmniSpatial的zero-shot CoT方法、VLMEvalKit和LMMS-Eval的答案匹配方法，以及CoreCognition和MMBench的圆形评估策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过统一框架评估GPT-5和其他模型在空间智能方面的能力，识别它们的挑战和局限性，并比较专有模型与开源模型在面对最困难问题时的表现。整体流程是：1)提出空间智能的六种基本能力分类；2)选择八个最新的空间智能基准测试；3)标准化评估协议；4)在这些基准上评估模型；5)进行消融研究评估不同参数的影响；6)进行案例研究分析优势和局限性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出全面的空间任务分类法；在八个最新基准上进行大规模评估(超10亿标记)；发现GPT-5在空间智能方面表现出前所未有的能力但仍未达到人类水平；识别更具挑战性的空间智能问题；发现专有模型在面对最困难问题时没有决定性优势。相比之前工作，这篇论文评估范围更广、规模更大、方法更标准化、提供了更全面的分析，并发现了新的见解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过全面评估揭示了GPT-5在空间智能方面的显著进步，同时明确指出其与人类水平仍存在明显差距，并为未来空间智能研究提供了统一的框架和基准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal models have achieved remarkable progress in recent years.Nevertheless, they continue to exhibit notable limitations in spatialunderstanding and reasoning, which are fundamental capabilities to achievingartificial general intelligence. With the recent release of GPT-5, allegedlythe most powerful AI model to date, it is timely to examine where the leadingmodels stand on the path toward spatial intelligence. First, we propose acomprehensive taxonomy of spatial tasks that unifies existing benchmarks anddiscuss the challenges in ensuring fair evaluation. We then evaluatestate-of-the-art proprietary and open-source models on eight key benchmarks, ata cost exceeding one billion total tokens. Our empirical study reveals that (1)GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)still falls short of human performance across a broad spectrum of tasks.Moreover, we (3) identify the more challenging spatial intelligence problemsfor multi-modal models, and (4) proprietary models do not exhibit a decisiveadvantage when facing the most difficult problems. In addition, we conduct aqualitative evaluation across a diverse set of scenarios that are intuitive forhumans yet fail even the most advanced multi-modal models.</description>
      <author>example@mail.com (Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Jiaqi Li, Xiangyu Fan, Hanming Deng, Lewei Lu, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang)</author>
      <guid isPermaLink="false">2508.13142v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks</title>
      <link>http://arxiv.org/abs/2508.12741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个全面的基准框架，用于系统评估神经网络中的空间推理能力，特别关注形态特性如连接性和距离关系。初步实验结果表明神经网络在空间推理方面存在重大挑战。&lt;h4&gt;背景&lt;/h4&gt;当前缺乏系统评估神经网络空间推理能力的综合框架，特别是对形态特性如连接性和距离关系的评估。&lt;h4&gt;目的&lt;/h4&gt;定义一个综合基准框架，用于系统评估神经网络中的空间推理能力，特别关注形态特性。&lt;h4&gt;方法&lt;/h4&gt;使用VoxLogicA空间模型检查器生成两类合成数据集：迷宫连接问题和空间距离计算任务。通过多种分辨率评估，并采用完整的机器学习工作流程，包括数据集生成、交叉验证训练、推理执行，以及使用Dice系数和IoU指标的综合评估。&lt;h4&gt;主要发现&lt;/h4&gt;神经网络在空间推理能力方面存在重大挑战，在基本的几何和拓扑理解任务中显示出系统性失败。&lt;h4&gt;结论&lt;/h4&gt;该框架提供了可重现的实验协议，使研究人员能够确定特定限制。通过结合神经网络与符号推理方法，可改善临床应用中的空间理解，为持续研究神经网络空间推理限制和潜在解决方案奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了在定义一个全面的基准框架方面的初步结果，该框架旨在系统评估神经网络中的空间推理能力，特别关注连接性和距离关系等形态特性。该框架目前正被用于研究nnU-Net的能力，利用空间模型检查器VoxLogicA生成两类不同的合成数据集：用于拓扑分析的迷宫连接问题和用于几何理解的空间距离计算任务。每个类别都在多种分辨率下进行评估，以评估可扩展性和泛化特性。自动化流程包含完整的机器学习工作流程，包括：合成数据集生成、具有交叉验证的标准化训练、推理执行，以及使用Dice系数和IoU（交并比）指标的综合评估。初步实验结果表明神经网络在空间推理能力方面存在重大挑战，揭示了在基本几何和拓扑理解任务中的系统性失败。该框架提供了可重现的实验协议，使研究人员能够确定特定限制。这些限制可以通过结合神经网络与符号推理方法的方法来解决，以改善临床应用中的空间理解，为持续研究神经网络空间推理限制和潜在解决方案奠定基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决神经网络空间推理能力缺乏系统性评估基准框架的问题。这个问题在医疗图像分析等领域尤为重要，因为空间理解直接影响诊断准确性和治疗效果。在现实应用中，神经网络在理解基本几何和拓扑关系方面存在系统性失败，可能导致医疗环境中的临床相关错误。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别现有评估方法的局限性（如传统指标无法充分评估空间结构理解）来设计方法。他们借鉴了空间模型检查领域的工作，特别是VoxLogicA工具，该工具使用逻辑公式识别图像中满足特定拓扑特性的像素。作者还参考了先进神经网络架构（如nnU-Net）和拓扑感知损失函数等方法，但创新性地将这些技术整合到一个系统化的多分辨率评估框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合形式空间逻辑规范与受控合成数据集生成，实现数学精确的真实值定义，并通过多分辨率方法系统评估神经网络的空间推理能力。整体流程包括：1)使用VoxLogicA生成两类合成数据集（点距离和迷宫连通性）；2)在多个分辨率下训练神经网络；3)执行推理并使用Dice系数和IoU等指标评估；4)分析不同分辨率下的性能差异，确定可靠空间推理的最小分辨率要求。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多分辨率基准测试框架；2)形式空间逻辑与神经网络评估的结合；3)包含完整机器学习工作流程的自动化管道；4)两种互补的评估域（连接性分析和距离推理）；5)开源实现。相比之前工作，新框架使用形式规范生成真实值而非手动注释，系统评估不同尺度下的空间能力，并专注于传统指标无法检测的拓扑和几何理解局限性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个创新的多分辨率基准测试框架，通过结合形式空间逻辑与自动化机器学习工作流程，系统评估了神经网络的空间推理能力，揭示了传统评估方法无法检测的拓扑和几何理解局限性，为开发更可靠的混合AI系统奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents preliminary results in the definition of a comprehensivebenchmark framework designed to systematically evaluate spatial reasoningcapabilities in neural networks, with a particular focus on morphologicalproperties such as connectivity and distance relationships. The framework iscurrently being used to study the capabilities of nnU-Net, exploiting thespatial model checker VoxLogicA to generate two distinct categories ofsynthetic datasets: maze connectivity problems for topological analysis andspatial distance computation tasks for geometric understanding. Each categoryis evaluated across multiple resolutions to assess scalability andgeneralization properties. The automated pipeline encompasses a completemachine learning workflow including: synthetic dataset generation, standardizedtraining with cross-validation, inference execution, and comprehensiveevaluation using Dice coefficient and IoU (Intersection over Union) metrics.Preliminary experimental results demonstrate significant challenges in neuralnetwork spatial reasoning capabilities, revealing systematic failures in basicgeometric and topological understanding tasks. The framework provides areproducible experimental protocol, enabling researchers to identify specificlimitations. Such limitations could be addressed through hybrid approachescombining neural networks with symbolic reasoning methods for improved spatialunderstanding in clinical applications, establishing a foundation for ongoingresearch into neural network spatial reasoning limitations and potentialsolutions.</description>
      <author>example@mail.com (Manuela Imbriani, Gina Belmonte, Mieke Massink, Alessandro Tofani, Vincenzo Ciancia)</author>
      <guid isPermaLink="false">2508.12741v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.12404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LMAD是一种新型视觉语言框架，专为自动驾驶设计，通过整合全面场景理解和专门任务结构，显著提升了现有VLMs在驾驶推理任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型在场景理解和驾驶行为可解释性方面展现出有前景的能力，但现有方法在车载多视图图像和场景推理文本上微调VLMs时，往往缺乏自动驾驶所需的整体和细致的场景识别以及强大的空间感知能力，特别是在复杂情况下。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLM方法在自动驾驶场景中的不足，提出一种专门为自动驾驶设计的视觉语言框架，提高模型在复杂驾驶场景中的表现。&lt;h4&gt;方法&lt;/h4&gt;提出名为LMAD的新型视觉语言框架，模仿现代端到端驾驶范式，整合全面场景理解和VLMs的任务专门结构，引入初步场景交互和专门专家适配器在同一驾驶任务结构中，使VLMs更好地与自动驾驶场景对齐，同时保持与现有VLMs和面向规划的驾驶系统的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;在DriveLM和nuScenes-QA数据集上的大量实验表明，LMAD显著提升了现有VLMs在驾驶推理任务上的性能，为可解释自动驾驶树立了新标准。&lt;h4&gt;结论&lt;/h4&gt;LMAD框架有效解决了VLMs在自动驾驶应用中的不足，通过整合全面场景理解和专门的任务结构，提高了模型在复杂驾驶场景中的表现，为可解释自动驾驶提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉语言模型（VLMs）在场景理解、增强驾驶行为可解释性和与用户交互方面展现出有前景的能力。现有方法主要在车载多视图图像和场景推理文本上微调VLMs，但这种方法通常缺乏自动驾驶所需的整体和细致的场景识别以及强大的空间感知能力，特别是在复杂情况下。为解决这一差距，我们提出了一种专为自动驾驶设计的新型视觉语言框架，称为LMAD。我们的框架通过整合全面场景理解和带有VLMs的任务专门结构，模仿现代端到端驾驶范式。特别是，我们在同一驾驶任务结构中引入了初步场景交互和专门专家适配器，使VLMs更好地与自动驾驶场景对齐。此外，我们的设计完全兼容现有VLMs，并能与面向规划的驾驶系统无缝集成。在DriveLM和nuScenes-QA数据集上的大量实验表明，LMAD显著提升了现有VLMs在驾驶推理任务上的性能，为可解释自动驾驶树立了新标准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision-language models (VLMs) have shown promising capabilities inscene understanding, enhancing the explainability of driving behaviors andinteractivity with users. Existing methods primarily fine-tune VLMs on on-boardmulti-view images and scene reasoning text, but this approach often lacks theholistic and nuanced scene recognition and powerful spatial awareness requiredfor autonomous driving, especially in complex situations. To address this gap,we propose a novel vision-language framework tailored for autonomous driving,called LMAD. Our framework emulates modern end-to-end driving paradigms byincorporating comprehensive scene understanding and a task-specializedstructure with VLMs. In particular, we introduce preliminary scene interactionand specialized expert adapters within the same driving task structure, whichbetter align VLMs with autonomous driving scenarios. Furthermore, our approachis designed to be fully compatible with existing VLMs while seamlesslyintegrating with planning-oriented driving systems. Extensive experiments onthe DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly booststhe performance of existing VLMs on driving reasoning tasks,setting a newstandard in explainable autonomous driving.</description>
      <author>example@mail.com (Nan Song, Bozhou Zhang, Xiatian Zhu, Jiankang Deng, Li Zhang)</author>
      <guid isPermaLink="false">2508.12404v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Splat Feature Solver</title>
      <link>http://arxiv.org/abs/2508.12216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  webpage not that stable&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的特征提升方法，用于解决3D场景理解中多视图图像不一致性问题，实现了高质量的特征提升和开放词汇3D分割的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;特征提升已成为3D场景理解的关键组成部分，允许将丰富的图像特征描述符附加到基于splat的3D表示上，但面临如何将丰富通用属性最优分配给3D原语并解决多视图图像不一致性的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一且与核和特征无关的特征提升公式，高效解决特征提升问题，处理多视图观察中的不一致性和噪声，实现高质量的特征提升。&lt;h4&gt;方法&lt;/h4&gt;将特征提升问题表述为稀疏线性逆问题，可通过闭式高效解决；引入两种互补正则化策略：Tikhonov指导通过软对角占优强制数值稳定性，提升后聚合通过特征聚类过滤噪声输入。&lt;h4&gt;主要发现&lt;/h4&gt;在开放词汇3D分割基准上实现了最先进的性能，优于基于训练、基于分组和启发式前向基线，且能在几分钟内生成提升的特征。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了3D场景理解中的特征提升挑战，代码已公开在GitHub上，并提供在线演示。&lt;h4&gt;翻译&lt;/h4&gt;特征提升已成为3D场景理解的关键组成部分，使能够将丰富的图像特征描述符（如DINO、CLIP）附加到基于splat的3D表示上。核心挑战在于将丰富的通用属性最优分配给3D原语的同时，解决来自多视图图像的不一致性问题。我们提出了一个统一的、与核和特征无关的特征提升问题公式化，将其作为稀疏线性逆问题，可以闭式高效解决。我们的方法在凸损失下为提供高质量提升特征的全局最优误差提供了可证明的上界。为解决多视图观察中的不一致性和噪声，我们引入了两种互补的正则化策略来稳定解决方案并增强语义保真度。Tikhonov指导通过软对角占优强制数值稳定性，而提升后聚合通过特征聚类过滤噪声输入。大量实验证明，我们的方法在开放词汇3D分割基准上实现了最先进的性能，优于基于训练、基于分组和启发式前向基线，同时能在几分钟内生成提升的特征。代码可在GitHub上获取，我们也提供了一个在线演示。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将2D图像特征描述符（如CLIP、DINO等）有效提升到3D表示中的问题，特别是在基于splat的3D表示（如3D高斯溅射）中。这个问题在3D场景理解中非常重要，因为它能够将丰富的语义信息与3D几何结合，支持3D语义分割、物体查询等下游任务，从而推动虚拟现实、自动驾驶和机器人等领域的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有特征提升方法的三种主要类型：基于训练的优化方法、基于分组的方法和基于启发式前向传播的方法。作者发现这些方法缺乏统一的数学框架、没有理论保证、针对特定特征类型设计，且未明确处理数据噪声。基于这些观察，作者将特征提升问题建模为稀疏线性逆问题，借鉴了Tikhonov正则化的思想来提高数值稳定性，并参考了基于聚类的方法来过滤噪声输入，从而设计出这个新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将特征提升问题形式化为稀疏线性逆问题AX=B，其中每个基本元素的描述符通过系统解来恢复。整体实现流程包括：1)输入预计算的splat表示、传感器参数和密集特征观测；2)构建特征提升方程，使用行和预处理器作为初始解；3)应用Tikhonov指导增强数值稳定性；4)使用后提升聚合过滤噪声输入；5)输出提升的特征参数，支持上游任务和下游应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将特征提升形式化为统一的稀疏线性逆问题；2)提供全局最优误差的可证明上界；3)引入Tikhonov指导和后提升聚合两种互补正则化策略；4)支持多种基本核和不同密集特征的通用实现；5)在开放词汇3D语义分割上实现最先进性能。相比之前工作，本文提供了理论框架和保证，同时处理噪声和不一致性问题，方法更加通用且计算效率高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种将特征提升问题形式化为稀疏线性逆问题的统一框架，通过理论保证和正则化策略解决了多视角噪声和不一致性问题，实现了高质量、高效率的3D特征提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature lifting has emerged as a crucial component in 3D scene understanding,enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)onto splat-based 3D representations. The core challenge lies in optimallyassigning rich general attributes to 3D primitives while addressing theinconsistency issues from multi-view images. We present a unified, kernel- andfeature-agnostic formulation of the feature lifting problem as a sparse linearinverse problem, which can be solved efficiently in closed form. Our approachadmits a provable upper bound on the global optimal error under convex lossesfor delivering high quality lifted features. To address inconsistencies andnoise in multi-view observations, we introduce two complementary regularizationstrategies to stabilize the solution and enhance semantic fidelity. TikhonovGuidance enforces numerical stability through soft diagonal dominance, whilePost-Lifting Aggregation filters noisy inputs via feature clustering. Extensiveexperiments demonstrate that our approach achieves state-of-the-art performanceon open-vocabulary 3D segmentation benchmarks, outperforming training-based,grouping-based, and heuristic-forward baselines while producing the liftedfeatures in minutes. Code is available at\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. Wealso have a \href{https://splat-distiller.pages.dev/}</description>
      <author>example@mail.com (Butian Xiong, Rong Liu, Kenneth Xu, Meida Chen, Andrew Feng)</author>
      <guid isPermaLink="false">2508.12216v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Euclidean Approach to Green-Wave Theory Applied to Traffic Signal Networks</title>
      <link>http://arxiv.org/abs/2508.12146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  74 pages, 49 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种应用于信号主干道网络的绿波理论，通过几何推理和专用设备实现无间断交通流，并通过仿真验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;长主干道上的信号交叉口如果没有适当协调，交通效率会低下。随着信号数量的增加，协调变得更加困难，传统的通行方案往往会失效。&lt;h4&gt;目的&lt;/h4&gt;介绍一种绿波理论，可应用于相交主干道网络，实现任意长信号主干道上的无间断通行。&lt;h4&gt;方法&lt;/h4&gt;使用'道路到旅行者反馈设备'，基于欧几里得几何模型，定义了RGW-道路、绿箭头、实节点、虚节点、绿波速度、区块等概念，并使用几何推理推导结果，通过名为RGW-SIM的仿真模型验证效果。&lt;h4&gt;主要发现&lt;/h4&gt;绿箭头长度有最大值且被限制为离散长度；绿箭头运动定律表明可以选择现有主干道转换为RGW-道路；产生的信号时间和偏移已被证明是有效的。&lt;h4&gt;结论&lt;/h4&gt;该方法可以实现长主干道上的无间断交通流，长绿波可以节省时间和燃料，减少污染和交通事故。&lt;h4&gt;翻译&lt;/h4&gt;在信号交叉口协调不当的情况下，长主干道上的交通可能效率低下。随着信号数量的增加，协调变得更加困难，传统的通行方案往往会失效。长绿波通过提供更顺畅的交通流，可以节省旅行时间和燃料，减少污染和交通事故。本文介绍了一种绿波理论，可应用于相交主干道网络。它使用'道路到旅行者反馈设备'，实现在任意长的信号主干道上的无间断通行。该方法借鉴了欧几里得几何模型。我们定义了诸如RGW-道路（车辆以推荐速度行驶时可通过所有交通信号的道路）、绿箭头（代表车辆车队）、实节点（代表RGW-道路相交的信号交叉口）和虚节点、绿波速度、区块等概念——相当于欧几里得公理的类比。然后我们使用几何推理推导出结果：绿箭头长度有最大值，被限制为离散长度，绿箭头运动定律表明可以选择现有主干道转换为RGW-道路。使用先前开发的名为RGW-SIM的仿真模型，产生的信号时间和偏移已被证明是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Travel on long arterials with signalized intersections can be inefficient ifnot coordinated properly. As the number of signals increases, coordinationbecomes more challenging and traditional progression schemes tend to breakdown. Long progressions save travel time and fuel, reduce pollution and trafficaccidents by providing a smoother flow of traffic. This paper introduces agreen-wave theory that can be applied to a network of intersecting arterialroads. It enables uninterrupted flow on arbitrary long signalized arterialsusing a Road-to-Traveler-Feedback Device. The approach is modelled afterEuclid. We define concepts such as RGW-roads (roads where vehicles traveling atthe recommended speed make all traffic signals), green-arrows (representingvehicle platoons), real nodes (representing signalized intersections whereRGW-roads intersect) and virtual nodes, green-wave speed, blocks, etc. - theanalogue of Euclid's postulates. We then use geometric reasoning to deduceresults: green-arrow lengths have a maximum value, are restricted to discretelengths, and green-arrow laws of motion imply that select existing arterialroads can be converted to RGW-roads. The signal timings and offsets that areproduced have been shown to be effective using a simulation model developedpreviously called RGW-SIM.</description>
      <author>example@mail.com (Melvin H. Friedman, Brian L. Mark, Nathan H. Gartner)</author>
      <guid isPermaLink="false">2508.12146v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes</title>
      <link>http://arxiv.org/abs/2508.12015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了InstDrive，一个实例感知的3D高斯飞溅框架，用于动态驾驶场景的交互式重建。该方法使用SAM生成的掩码作为伪真实标签指导2D特征学习，并通过3D级别的正则化隐式编码实例身份，同时使用轻量级静态码本连接连续特征和离散身份。&lt;h4&gt;背景&lt;/h4&gt;从车载摄像头视频重建动态驾驶场景在自动驾驶和场景理解中具有重要意义。现有方法将所有背景元素统一为单一表示，阻碍了实例级理解和灵活的场景编辑。一些将2D分割提升到3D空间的方法通常依赖于预处理的实例ID或复杂流程，且主要适用于室内场景。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现动态驾驶场景交互式重建的实例感知3D高斯飞溅框架，解决现有方法在实例级理解和场景编辑方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出InstDrive框架，使用SAM生成的掩码作为伪真实标签通过对比损失和伪监督目标指导2D特征学习；在3D级别引入正则化隐式编码实例身份，通过基于体素的损失强制一致性；使用轻量级静态码本连接连续特征和离散身份，无需数据预处理或复杂优化。&lt;h4&gt;主要发现&lt;/h4&gt;定量和定性实验证明了InstDrive的有效性，据作者所知，这是第一个实现动态开放世界驾驶场景中3D实例分割的框架。&lt;h4&gt;结论&lt;/h4&gt;InstDrive成功实现了动态驾驶场景的交互式重建，解决了现有方法在实例级理解和场景编辑方面的局限性，为自动驾驶和场景理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从车载摄像头视频重建动态驾驶场景由于其对于自动驾驶和场景理解的重要性而吸引了越来越多的关注。尽管最近的进展取得了令人印象深刻的成果，但大多数方法仍然将所有背景元素统一为单一表示，这阻碍了实例级理解和灵活的场景编辑。一些尝试将2D分割提升到3D空间的方法通常依赖于预处理的实例ID或复杂流程来映射连续特征到离散身份。此外，这些方法通常为具有丰富视点的室内场景设计，使其不太适用于户外驾驶场景。在本文中，我们提出了InstDrive，一个专为动态驾驶场景交互式重建而设计的实例感知3D高斯飞溅框架。我们使用SAM生成的掩码作为伪真实标签，通过对比损失和伪监督目标指导2D特征学习。在3D级别，我们引入正则化来隐式编码实例身份，并通过基于体素的损失强制一致性。一个轻量级静态码本进一步连接连续特征和离散身份，无需数据预处理或复杂优化。定量和定性实验证明了InstDrive的有效性，据我们所知，它是第一个实现动态开放世界驾驶场景中3D实例分割的框架。更多可视化内容可在我们的项目页面查看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从车载摄像头视频中重建动态驾驶场景并实现实例级别3D分割的问题。这个问题在自动驾驶领域非常重要，因为它能支持场景理解、行为预测和交互式规划，同时为对象级编辑、可控模拟和细粒度语义理解提供基础，这些都是自动驾驶技术发展的关键需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性设计了这个方法：现有方法要么将场景统一表示，要么依赖复杂的预处理或跟踪技术。作者借鉴了SAM生成的掩码作为伪监督信号，采用对比学习促进特征聚类，并基于3D高斯溅射作为基础表示。关键创新在于提出两阶段训练策略：先学习连续特征，再通过静态二值化码本实现离散实例编码，解决了现有方法中依赖跟踪、聚类精度低或码本设计复杂的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用实例感知的3D高斯溅射框架，通过SAM生成的掩码指导学习，实现动态驾驶场景的交互式重建。整体流程分为两个阶段：首先是连续特征学习阶段，使用对比学习拉近同一掩码特征、推远不同掩码特征，并通过体素一致性损失确保3D空间连贯性；其次是量化实例特征学习阶段，采用静态二值化码本将连续特征映射为离散实例ID，并通过多数投票机制增强特征一致性。最终实现实时的3D实例分割和交互式编辑能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个端到端的3D实例分割框架，适用于开放驾驶场景；2)静态二值化码本设计，确保均匀分布的聚类中心且无需更新；3)基于多数投票的伪监督机制，增强特征一致性；4)基于体素的一致性损失，提高3D空间连贯性。相比之前工作，InstDrive无需实例跟踪或复杂预处理，提供全局一致的实例ID，支持实时交互编辑，并且在多视角驾驶场景中表现更好，而不仅仅是受控环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InstDrive首次实现了从车载摄像头视频中端到端地重建具有实例级编辑能力的动态驾驶场景，通过创新的静态码本设计和两阶段训练策略，实现了无需人工标注的实时3D实例分割和交互式编辑。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing dynamic driving scenes from dashcam videos has attractedincreasing attention due to its significance in autonomous driving and sceneunderstanding. While recent advances have made impressive progress, mostmethods still unify all background elements into a single representation,hindering both instance-level understanding and flexible scene editing. Someapproaches attempt to lift 2D segmentation into 3D space, but often rely onpre-processed instance IDs or complex pipelines to map continuous features todiscrete identities. Moreover, these methods are typically designed for indoorscenes with rich viewpoints, making them less applicable to outdoor drivingscenarios. In this paper, we present InstDrive, an instance-aware 3D GaussianSplatting framework tailored for the interactive reconstruction of dynamicdriving scene. We use masks generated by SAM as pseudo ground-truth to guide 2Dfeature learning via contrastive loss and pseudo-supervised objectives. At the3D level, we introduce regularization to implicitly encode instance identitiesand enforce consistency through a voxel-based loss. A lightweight staticcodebook further bridges continuous features and discrete identities withoutrequiring data pre-processing or complex optimization. Quantitative andqualitative experiments demonstrate the effectiveness of InstDrive, and to thebest of our knowledge, it is the first framework to achieve 3D instancesegmentation in dynamic, open-world driving scenes.More visualizations areavailable at our project page.</description>
      <author>example@mail.com (Hongyuan Liu, Haochen Yu, Jianfei Jiang, Qiankun Liu, Jiansheng Chen, Huimin Ma)</author>
      <guid isPermaLink="false">2508.12015v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding</title>
      <link>http://arxiv.org/abs/2508.11952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了UniUGG，首个用于3D模态的统一理解和生成框架。该框架利用大语言模型理解和解码句子及3D表示，核心是采用空间解码器和潜在扩散模型生成高质量3D表示，支持基于参考图像和任意视图变换的3D场景生成，同时保持对空间视觉问答任务的支持。此外，还提出了几何-语义学习策略预训练视觉编码器，共同捕获输入的语义和几何线索，增强空间理解和生成能力。&lt;h4&gt;背景&lt;/h4&gt;尽管最近的统一架构在图像理解和生成方面取得了显著进展，但3D任务的集成仍然具有挑战性且很大程度上未被探索。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的3D模态理解和生成框架，解决3D任务集成的挑战性问题。&lt;h4&gt;方法&lt;/h4&gt;提出UniUGG框架，使用大语言模型理解和解码句子及3D表示，设计空间解码器利用潜在扩散模型生成高质量3D表示，并采用几何-语义学习策略预训练视觉编码器，共同捕获输入的语义和几何线索。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证明该方法在视觉表示、空间理解和3D生成方面具有优越性；框架能够基于参考图像和任意视图变换生成和想象3D场景；同时支持空间视觉问答(VQA)任务。&lt;h4&gt;结论&lt;/h4&gt;UniUGG框架成功解决了3D任务集成的挑战，在多个方面展示了优越性能，源代码将在论文接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;尽管最近的统一架构在图像理解和生成方面取得了令人印象深刻的进展，但3D任务的集成仍然具有挑战性且很大程度上未被探索。在本文中，我们介绍了UniUGG，这是首个用于3D模态的统一理解和生成框架。我们的统一框架采用大语言模型来理解和解码句子及3D表示。其核心是，我们提出了一种空间解码器，利用潜在扩散模型生成高质量的3D表示。这使得能够基于参考图像和任意视图变换生成和想象3D场景，同时仍然支持空间视觉问答(VQA)任务。此外，我们提出了一种几何-语义学习策略来预训练视觉编码器。该设计共同捕获输入的语义和几何线索，增强空间理解和生成能力。大量的实验结果证明了我们的方法在视觉表示、空间理解和3D生成方面的优越性。源代码将在论文接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D模态的统一理解和生成问题。尽管2D图像领域已有统一架构，但3D任务仍面临挑战。这个问题很重要，因为3D场景理解对增强现实、机器人导航等应用至关重要，而统一框架能同时处理空间推理和场景生成，提高效率，且能更好地模拟人类的空间认知能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了两个瓶颈：视觉表示局限性和3D生成与LLM不兼容性。受DUSt3R多视图几何训练启发，设计了几何-语义学习策略预训练视觉编码器，结合RADIOv2.5作为教师模型和MASt3R解码器。针对3D生成问题，设计了Spatial-VAE压缩表示，利用LLM和扩散模型生成3D场景。该方法借鉴了多项现有工作，包括MASt3R的空间解码器、RADIOv2.5的语义引导和扩散模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过几何-语义统一编码、空间解码器、Spatial-VAE设计和基于LLM的统一框架，实现3D场景的理解和生成。整体流程分三阶段：1)视觉编码器预训练：在多数据集上预训练ViT编码器，结合语义和几何学习；2)Spatial-VAE训练：压缩视觉表示，与空间解码器联合微调；3)统一学习：空间生成学习使用LLM和扩散模型，空间理解学习在VQA任务上微调LLM。推理时，3D生成通过参考图像和视图变换生成目标视图，空间理解通过图像和问答实现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个统一的3D理解和生成框架；2)几何-语义视觉编码器预训练策略；3)专为3D设计的Spatial-VAE；4)统一的学习范式。相比之前工作，不同在于：专注于3D模态而非2D；同时支持理解和生成而非仅关注其中之一；仅依赖视觉输入而非额外模态；统一了几何和语义信息而非单独处理；通过Spatial-VAE解决了3D数据不规则性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniUGG首次实现了3D场景的统一理解和生成，通过创新的几何-语义编码策略和Spatial-VAE设计，使模型能够同时处理空间级视觉问答和几何一致的3D场景生成，在多个基准测试上超越了现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the impressive progress on understanding and generating images shownby the recent unified architectures, the integration of 3D tasks remainschallenging and largely unexplored. In this paper, we introduce UniUGG, thefirst unified understanding and generation framework for 3D modalities. Ourunified framework employs an LLM to comprehend and decode sentences and 3Drepresentations. At its core, we propose a spatial decoder leveraging a latentdiffusion model to generate high-quality 3D representations. This allows forthe generation and imagination of 3D scenes based on a reference image and anarbitrary view transformation, while remaining supports for spatial visualquestion answering (VQA) tasks. Additionally, we propose a geometric-semanticlearning strategy to pretrain the vision encoder. This design jointly capturesthe input's semantic and geometric cues, enhancing both spatial understandingand generation. Extensive experimental results demonstrate the superiority ofour method in visual representation, spatial understanding, and 3D generation.The source code will be released upon paper acceptance.</description>
      <author>example@mail.com (Yueming Xu, Jiahui Zhang, Ze Huang, Yurui Chen, Yanpeng Zhou, Zhenyu Chen, Yu-Jie Yuan, Pengxiang Xia, Guowei Huang, Xinyue Cai, Zhongang Qi, Xingyue Quan, Jianye Hao, Hang Xu, Li Zhang)</author>
      <guid isPermaLink="false">2508.11952v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction</title>
      <link>http://arxiv.org/abs/2508.12917v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The Paper is Accepted by TCSVT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CMF-IOU的多阶段跨模态融合3D检测框架，通过深度完成网络统一LiDAR和摄像头信息表示，设计双向跨视图增强3D主干网络，并引入迭代体素-点感知细粒度池化模块和IoU联合预测分支，有效解决了3D空间信息和2D语义信息对齐的挑战，在多个数据集上展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;多模态方法基于摄像头和LiDAR传感器在3D检测领域受到广泛关注，但许多现有工作专注于单阶段或部分阶段融合，导致特征提取不足和性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种多阶段跨模态融合3D检测框架CMF-IOU，有效解决3D空间信息和2D语义信息对齐的挑战。&lt;h4&gt;方法&lt;/h4&gt;通过深度完成网络将像素信息投影到3D空间获得伪点统一表示；设计双向跨视图增强3D主干网络，包含稀疏到远距离(S2D)分支和残差视图一致性(ResVC)分支；引入迭代体素-点感知细粒度池化模块；设计交并比(IoU)联合预测分支结合新颖提案生成技术。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI、nuScenes和Waymo数据集上的大量实验表明，该方法具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;CMF-IOU框架通过多阶段融合和精细化的处理方法，有效解决了3D空间和2D语义信息对齐的挑战，提高了3D检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;基于摄像头和LiDAR传感器的多模态方法在3D检测领域已引起广泛关注。然而，许多流行的工作专注于单阶段或部分阶段融合，导致特征提取不足和性能次优。在本文中，我们引入了一种多阶段跨模态融合3D检测框架，称为CMF-IOU，以有效解决对齐3D空间和2D语义信息的挑战。具体而言，我们首先通过深度完成网络将像素信息投影到3D空间以获得伪点，这统一了LiDAR和摄像头信息的表示。然后，设计了一个双向跨视图增强3D主干网络来编码LiDAR点和伪点。第一个稀疏到远距离(S2D)分支利用编码器-解码器结构来增强稀疏LiDAR点的表示。第二个残差视图一致性(ResVC)分支通过3D和2D卷积过程提出，以减轻不准确伪点的影响。随后，我们引入了一个迭代体素-点感知细粒度池化模块，在提案细化阶段捕获来自LiDAR点的空间信息和来自伪点的纹理信息。为了在迭代过程中实现更精确的细化，设计了一个交并比(IoU)联合预测分支，结合新颖的提案生成技术，以保留具有高IoU和分类分数的边界框。大量实验表明，我们的方法在KITTI、nuScenes和Waymo数据集上具有优越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态3D目标检测中相机和LiDAR传感器信息融合不充分的问题，以及现有方法在NMS后处理中使用分类分数而非IoU作为评估指标的不一致性问题。这个问题在自动驾驶领域至关重要，因为准确可靠的3D目标检测是自动驾驶系统的核心功能，而融合不同传感器的互补信息可以提高检测的准确性和鲁棒性，同时使用与评估指标一致的IoU分数进行后处理可以进一步提升检测性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有多模态3D检测方法的局限性，发现单阶段融合导致特征提取不充分，且NMS使用分类分数而非IoU与评估指标不一致。因此，作者设计了一个多阶段跨模态融合框架，分别处理LiDAR原始点和相机生成的伪点，以减轻深度偏差影响。该方法借鉴了深度完成网络(PENet)、基于体素的处理方法(VoxelRCNN)、点集抽象(PointNet)以及IoU预测(3D IoU-Net)等现有工作，但进行了创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在整个检测管道中实现多阶段跨模态融合，同时引入IoU联合预测来平衡分类分数和IoU分数。整体流程包括：1)使用深度完成网络将相机图像投影到3D空间生成伪点；2)设计双边跨视图增强骨干网络，分别处理原始点和伪点；3)采用迭代体素-点感知细粒度池化模块优化提案；4)引入IoU联合预测分支和提案生成技术，保留高IoU和高分类分数的边界框；5)使用综合损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多阶段跨模态融合框架，在整个管道中实现多阶段融合；2)双边跨视图增强骨干网络，分别处理原始点和伪点；3)迭代体素-点感知细粒度池化模块，结合空间和纹理信息；4)IoU联合预测分支，结合IoU和分类分数进行后处理。相比之前的工作，不同之处在于：不再局限于单阶段融合，而是全程多模态融合；分别处理原始点和伪点而非简单合并；引入IoU预测分支和均匀提案生成策略，解决了IoU分布不均和评估指标不一致的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CMF-IoU通过多阶段跨模态融合和IoU联合预测，有效解决了3D目标检测中多模态信息融合不充分和后处理评估指标不一致的问题，显著提高了自动驾驶场景下的3D目标检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal methods based on camera and LiDAR sensors have garneredsignificant attention in the field of 3D detection. However, many prevalentworks focus on single or partial stage fusion, leading to insufficient featureextraction and suboptimal performance. In this paper, we introduce amulti-stage cross-modal fusion 3D detection framework, termed CMF-IOU, toeffectively address the challenge of aligning 3D spatial and 2D semanticinformation. Specifically, we first project the pixel information into 3D spacevia a depth completion network to get the pseudo points, which unifies therepresentation of the LiDAR and camera information. Then, a bilateralcross-view enhancement 3D backbone is designed to encode LiDAR points andpseudo points. The first sparse-to-distant (S2D) branch utilizes anencoder-decoder structure to reinforce the representation of sparse LiDARpoints. The second residual view consistency (ResVC) branch is proposed tomitigate the influence of inaccurate pseudo points via both the 3D and 2Dconvolution processes. Subsequently, we introduce an iterative voxel-pointaware fine grained pooling module, which captures the spatial information fromLiDAR points and textural information from pseudo points in the proposalrefinement stage. To achieve more precise refinement during iteration, anintersection over union (IoU) joint prediction branch integrated with a novelproposals generation technique is designed to preserve the bounding boxes withboth high IoU and classification scores. Extensive experiments show thesuperior performance of our method on the KITTI, nuScenes and Waymo datasets.</description>
      <author>example@mail.com (Zhiwei Ning, Zhaojiang Liu, Xuanang Gao, Yifan Zuo, Jie Yang, Yuming Fang, Wei Liu)</author>
      <guid isPermaLink="false">2508.12917v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Neural Rendering for Sensor Adaptation in 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.12695v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE Intelligent Vehicles Symposium (IV) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了自动驾驶车辆不同相机传感器设置导致的跨传感器域差距问题，提出了CamShift数据集和基于神经渲染的传感器适应解决方案，有效减轻了3D目标检测器的性能下降。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆因车型限制导致相机传感器设置各不相同，在一种设置上训练的模型在另一种设置上评估时会产生跨传感器域差距，导致准确率下降。&lt;h4&gt;目的&lt;/h4&gt;研究跨传感器域差距对最先进3D目标检测器的影响，并提出解决方案减轻这一差距。&lt;h4&gt;方法&lt;/h4&gt;创建CamShift数据集模拟紧凑型车辆和SUV之间的域差距，提出基于神经渲染的数据驱动传感器适应流程，将数据集转换为匹配不同相机传感器设置。&lt;h4&gt;主要发现&lt;/h4&gt;显著的跨传感器性能下降；基于密集鸟瞰图表示和反向投影的模型架构(如BEVFormer)对变化的传感器配置最为鲁棒；提出的传感器适应方法改善了所有研究的3D目标检测器性能，大幅减轻了跨传感器域差距影响。&lt;h4&gt;结论&lt;/h4&gt;通过实现不同传感器设置的车辆间高效数据重用，减少了对新数据收集的需求，CamShift数据集和传感器适应基准已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶车辆通常具有不同的相机传感器设置，由于不同车型受到安装位置的限制，这种差异是不可避免的。在一种特定设置上训练感知模型并在新的不同传感器设置上评估会产生所谓的跨传感器域差距，通常导致准确率下降。本文研究了跨传感器域差距对最先进3D目标检测器的影响。为此，我们引入了CamShift数据集，该数据集受nuScenes启发，在CARLA中创建，专门用于模拟紧凑型车辆和运动型多用途车(SUV)之间的域差距。使用CamShift，我们展示了显著的跨传感器性能下降，识别了模型架构对鲁棒性的依赖关系，并提出了数据驱动的解决方案来减轻这种影响。一方面，我们表明基于密集鸟瞰图(BEV)表示和反向投影的模型架构(如BEVFormer)对变化的传感器配置最为鲁棒。另一方面，我们提出了基于神经渲染的新型数据驱动传感器适应流程，可以将整个数据集转换为匹配不同的相机传感器设置。应用这种方法改善了所有研究的3D目标检测器的性能，大幅减轻了跨传感器域差距，并通过实现不同传感器设置的车辆间高效数据重用，减少了对新数据收集的需求。CamShift数据集和传感器适应基准可在https://dmholtz.github.io/camshift/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶车辆中不同相机传感器设置之间的跨传感器域差距问题。这个问题在现实中非常重要，因为不同车型（如SUV和轿车）的相机安装位置不同，导致同一感知模型在不同车辆上的性能显著下降，限制了感知模型在整个车队的部署，并增加了数据收集成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶领域中相机感知模型面临的跨传感器域差距问题，发现缺乏专门研究这一问题的数据集。他们借鉴了现有的3D目标检测方法（如BEVDet、BEVFormer）和神经渲染技术（如NeRF、NeuRAD），设计了CamShift数据集来隔离跨传感器域差距，并提出了基于神经渲染的传感器适配管道。该方法结合了显式场景分解和时间变化物体外观建模，改进了NeuRAD架构，并添加了单独的天空分支。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用神经渲染技术将一个传感器设置的数据集转换到另一个传感器设置，解决跨传感器域差距问题。整体流程包括：1) 创建CamShift数据集，包含sim-SUV和sim-SUB两种传感器设置；2) 为每个序列的周围相机图像训练神经场景表示；3) 渲染原始视图并与原始图像比较进行质量控制；4) 达到质量要求后，为新传感器设置渲染新视图；5) 使用转换后的数据集训练3D目标检测器并评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) CamShift数据集，首个专门隔离跨传感器域差距的数据集；2) 传感器适配基准测试，系统研究不同模型架构对跨传感器域差距的鲁棒性；3) 基于神经渲染的传感器适配管道，能转换整个数据集以匹配不同相机设置；4) 发现基于密集BEV表示和反向投影的方法（如BEVFormer）对跨传感器域差距最鲁棒。相比之前工作，本文专注于传感器设置的域适应（而非跨数据集或天气条件），使用神经渲染进行整个数据集转换，而非传统数据增强技术。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建专门的CamShift数据集和基于神经渲染的传感器适配管道，有效解决了自动驾驶3D目标检测中的跨传感器域差距问题，显著提升了模型在不同相机配置下的性能并减少了数据收集需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IV64158.2025.11097434&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicles often have varying camera sensor setups, which isinevitable due to restricted placement options for different vehicle types.Training a perception model on one particular setup and evaluating it on a new,different sensor setup reveals the so-called cross-sensor domain gap, typicallyleading to a degradation in accuracy. In this paper, we investigate the impactof the cross-sensor domain gap on state-of-the-art 3D object detectors. To thisend, we introduce CamShift, a dataset inspired by nuScenes and created in CARLAto specifically simulate the domain gap between subcompact vehicles and sportutility vehicles (SUVs). Using CamShift, we demonstrate significantcross-sensor performance degradation, identify robustness dependencies on modelarchitecture, and propose a data-driven solution to mitigate the effect. On theone hand, we show that model architectures based on a dense Bird's Eye View(BEV) representation with backward projection, such as BEVFormer, are the mostrobust against varying sensor configurations. On the other hand, we propose anovel data-driven sensor adaptation pipeline based on neural rendering, whichcan transform entire datasets to match different camera sensor setups. Applyingthis approach improves performance across all investigated 3D object detectors,mitigating the cross-sensor domain gap by a large margin and reducing the needfor new data collection by enabling efficient data reusability across vehicleswith different sensor setups. The CamShift dataset and the sensor adaptationbenchmark are available at https://dmholtz.github.io/camshift/.</description>
      <author>example@mail.com (Felix Embacher, David Holtz, Jonas Uhrig, Marius Cordts, Markus Enzweiler)</author>
      <guid isPermaLink="false">2508.12695v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.11951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了点云目标检测中的多尺度特征近似和可转移特征问题。&lt;h4&gt;背景&lt;/h4&gt;多尺度特征对点云目标检测至关重要，但传统多尺度特征学习方法通常涉及多次邻域搜索和尺度感知层，这阻碍了轻量级模型的实现，且不利于计算资源有限的研究。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于知识蒸馏的单邻域点云多尺度特征近似方法，并设计可转移特征嵌入机制以补偿单邻域的结构多样性损失。&lt;h4&gt;方法&lt;/h4&gt;1) 使用知识蒸馏从单邻域近似点云多尺度特征；2) 设计可转移特征嵌入机制；3) 使用类别感知统计作为可转移特征，因其计算成本小；4) 引入中心加权交并比用于定位，缓解优化中中心偏移带来的不对齐问题。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法节省了计算成本。&lt;h4&gt;结论&lt;/h4&gt;在公共数据集上的大量实验证明了所提方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了点云目标检测中的多尺度特征近似和可转移特征。多尺度特征对点云目标检测至关重要。然而，多尺度特征学习通常涉及多次邻域搜索和尺度感知层，这阻碍了轻量级模型的实现，且不利于计算资源有限的研究。本文基于知识蒸馏从单邻域近似点云多尺度特征。为补偿单邻域的结构多样性损失，本文设计了可转移特征嵌入机制。具体而言，使用类别感知统计作为可转移特征，因其计算成本小。此外，本文引入了中心加权交并比用于定位，以缓解优化中中心偏移带来的不对齐问题。注意，本文提出的方法节省了计算成本。在公共数据集上的大量实验证明了所提方法的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云目标检测中多尺度特征学习与模型轻量化之间的平衡问题。在现实应用中，如自动驾驶和机器人视觉，3D目标检测至关重要，但传统多尺度特征方法需要多次邻域搜索，计算成本高，不利于在资源有限的设备上部署，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到多尺度特征学习的高计算成本问题，然后借鉴了知识蒸馏技术，设计了一个'教师-学生'框架：教师分支使用多尺度邻域搜索，学生分支使用单尺度邻域搜索。为了弥补单邻域的信息损失，作者引入了类感知统计作为可转移特征。此外，还借鉴了PointNet++的多尺度分组方法、CPC-3Det的类感知统计概念以及中心投票技术，但进行了创新性整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过知识蒸馏让单尺度邻域的学生模型近似多尺度邻域的教师模型性能，并利用类感知统计增强特征学习能力。整体流程包括：1)使用PointNet初始化特征库；2)通过知识蒸馏实现多尺度特征近似；3)利用中心投票进行目标级别特征聚合；4)将类感知统计嵌入分类和定位头；5)结合软损失和硬损失进行训练优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于知识蒸馏的单邻域多尺度特征近似，大幅减少计算量；2)可转移类感知统计机制，以低成本增强特征学习；3)中心加权IoU损失解决中心偏移问题。相比之前工作，本文不是简单减少参数，而是通过知识蒸馏保持特征多样性；不仅关注模型轻量化，还注重检测精度；引入类感知统计作为可转移特征，这是之前较少考虑的。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于知识蒸馏和可转移类感知统计的轻量级3D点云目标检测方法，通过单邻域多尺度特征近似和中心加权IoU损失，在保持检测精度的同时显著降低了计算复杂度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates multi-scale feature approximation and transferablefeatures for object detection from point clouds. Multi-scale features arecritical for object detection from point clouds. However, multi-scale featurelearning usually involves multiple neighborhood searches and scale-awarelayers, which can hinder efforts to achieve lightweight models and may not beconducive to research constrained by limited computational resources. Thispaper approximates point-based multi-scale features from a single neighborhoodbased on knowledge distillation. To compensate for the loss of constructivediversity in a single neighborhood, this paper designs a transferable featureembedding mechanism. Specifically, class-aware statistics are employed astransferable features given the small computational cost. In addition, thispaper introduces the central weighted intersection over union for localizationto alleviate the misalignment brought by the center offset in optimization.Note that the method presented in this paper saves computational costs.Extensive experiments on public datasets demonstrate the effectiveness of theproposed method.</description>
      <author>example@mail.com (Hao Peng, Hong Sang, Yajing Ma, Ping Qiu, Chao Ji)</author>
      <guid isPermaLink="false">2508.11951v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Point upsampling networks for single-photon sensing</title>
      <link>http://arxiv.org/abs/2508.12986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures, any comments are welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于状态空间模型的点上采样网络，用于解决单光子传感产生的稀疏且空间偏倚点云的问题，提高了点云密度并减少了空间失真。&lt;h4&gt;背景&lt;/h4&gt;单光子传感作为一种远距离和超灵敏成像技术受到广泛关注，但其产生的点云稀疏且空间分布不均，限制了其实际应用价值。&lt;h4&gt;目的&lt;/h4&gt;开发点上采样网络以增加单光子点云的点密度并减少空间失真，提高单光子传感的实用价值。&lt;h4&gt;方法&lt;/h4&gt;构建基于状态空间模型的网络，包含多路径扫描机制以丰富空间上下文、双向Mamba主干网络以捕获全局几何和局部细节、以及自适应上采样偏移模块以校正偏移引起的失真。&lt;h4&gt;主要发现&lt;/h4&gt;在常用数据集上的实验证实了该方法具有高重建精度和强鲁棒性；在真实世界数据上展示了模型能够生成视觉一致、保留细节且噪声抑制的点云。&lt;h4&gt;结论&lt;/h4&gt;这是首个为单光子传感建立的上采样框架，为单光子传感及其在实际应用中的下游任务开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;单光子传感作为一种突出的远距离和超灵敏成像技术引起了广泛关注，然而，它往往产生稀疏且空间偏倚的点云，从而限制了其实用性。在这项工作中，我们提出使用点上采样网络来增加单光子点云的点密度并减少空间失真。特别是，我们的网络基于状态空间模型构建，集成了多路径扫描机制以丰富空间上下文、双向Mamba主干网络以捕获全局几何和局部细节、以及自适应上采样偏移模块以校正偏移引起的失真。在常用数据集上进行了大量实验，证实了其高重建精度和对失真噪声的强鲁棒性，并在真实数据上展示了我们的模型能够生成视觉一致、保留细节且噪声抑制的点云。我们的工作是首个为单光子传感建立的上采样框架，因此为单光子传感及其在实际应用中的下游任务开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-photon sensing has generated great interest as a prominent techniqueof long-distance and ultra-sensitive imaging, however, it tends to yield sparseand spatially biased point clouds, thus limiting its practical utility. In thiswork, we propose using point upsampling networks to increase point density andreduce spatial distortion in single-photon point cloud. Particularly, ournetwork is built on the state space model which integrates a multi-pathscanning mechanism to enrich spatial context, a bidirectional Mamba backbone tocapture global geometry and local details, and an adaptive upsample shiftmodule to correct offset-induced distortions. Extensive experiments areimplemented on commonly-used datasets to confirm its high reconstructionaccuracy and strong robustness to the distortion noise, and also on real-worlddata to demonstrate that our model is able to generate visually consistent,detail-preserving, and noise suppressed point clouds. Our work is the first toestablish the upsampling framework for single-photon sensing, and hence opens anew avenue for single-photon sensing and its practical applications in thedownstreaming tasks.</description>
      <author>example@mail.com (Jinyi Liu, Guoyang Zhao, Lijun Liu, Yiguang Hong, Weiping Zhang, Shuming Cheng)</author>
      <guid isPermaLink="false">2508.12986v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>RoTO: Robust Topology Obfuscation Against Tomography Inference Attacks</title>
      <link>http://arxiv.org/abs/2508.12852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RoTO是一种健壮的拓扑模糊化方案，通过建模攻击者观察延迟的不确定性，消除了现有防御方法对完美探测控制和固定攻击者模型的依赖，显著提高了网络拓扑隐私保护的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;Tomography推理攻击通过分析端到端的探测延迟来重建网络拓扑。现有防御方法依赖于两个强假设：(i)探测包可以被完美检测和修改，(ii)攻击者使用已知、固定的推理算法。这些假设在实践中常常失效，导致在检测错误或自适应攻击者下防御性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出RoTO，一种健壮的拓扑模糊化方案，通过分布公式建模攻击者观察延迟的不确定性，消除现有防御方法的两个关键假设。&lt;h4&gt;方法&lt;/h4&gt;RoTO将防御目标建模为一个min-max优化问题，最大化不确定性集上的预期拓扑失真，不依赖完美的探测控制或特定的攻击者模型。利用图神经网络进行推理模拟和对抗训练，推导攻击者成功概率的上界，并通过优化这个上界来改进拓扑模糊化性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，RoTO优于现有防御方法，在结构相似性方面平均提高34%，在链路距离方面提高42.6%，同时保持强大的健壮性和隐藏能力。&lt;h4&gt;结论&lt;/h4&gt;RoTO通过消除现有防御方法的两个关键假设，显著提高了拓扑模糊化的性能和鲁棒性，为网络拓扑隐私保护提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;断层扫描推理攻击旨在通过分析端到端的探测延迟来重建网络拓扑。现有防御方法通过操纵探测延迟来误导推理，但依赖于两个强假设：(i)探测包可以被完美检测和修改，(ii)攻击者使用已知、固定的推理算法。这些假设在实践中常常失效，导致在检测错误或自适应攻击者下防御性能下降。我们提出了RoTO，一种健壮的拓扑模糊化方案，通过分布公式建模攻击者观察延迟的不确定性，消除了这两个假设。RoTO将防御目标建模为一个min-max优化问题，最大化这个不确定性集上的预期拓扑失真，不依赖完美的探测控制或特定的攻击者模型。为了近似攻击者行为，RoTO利用图神经网络进行推理模拟和对抗训练。我们还推导了攻击者成功概率的上界，并证明我们的方法通过优化这个上界来增强拓扑模糊化性能。实验结果表明，RoTO优于现有防御方法，在结构相似性方面平均提高34%，在链路距离方面提高42.6%，同时保持强大的健壮性和隐藏能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tomography inference attacks aim to reconstruct network topology by analyzingend-to-end probe delays. Existing defenses mitigate these attacks bymanipulating probe delays to mislead inference, but rely on two strongassumptions: (i) probe packets can be perfectly detected and altered, and (ii)attackers use known, fixed inference algorithms. These assumptions often breakin practice, leading to degraded defense performance under detection errors oradaptive adversaries. We present RoTO, a robust topology obfuscation schemethat eliminates both assumptions by modeling uncertainty in attacker-observeddelays through a distributional formulation. RoTO casts the defense objectiveas a min-max optimization problem that maximizes expected topologicaldistortion across this uncertainty set, without relying on perfect probecontrol or specific attacker models. To approximate attacker behavior, RoTOleverages graph neural networks for inference simulation and adversarialtraining. We also derive an upper bound on attacker success probability, anddemonstrate that our approach enhances topology obfuscation performance throughthe optimization of this upper bound. Experimental results show that RoTOoutperforms existing defense methods, achieving average improvements of 34% instructural similarity and 42.6% in link distance while maintaining strongrobustness and concealment capabilities.</description>
      <author>example@mail.com (Chengze Du, Heng Xu, Zhiwei Yu, Ying Zhou, Zili Meng, Jialong Li)</author>
      <guid isPermaLink="false">2508.12852v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics</title>
      <link>http://arxiv.org/abs/2508.12840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种利用图神经网络(GNNs)改进多智能体认知规划(MEP)可扩展性的方法，通过学习认知状态中的模式和关系结构来指导规划过程。&lt;h4&gt;背景&lt;/h4&gt;多智能体认知规划(MEP)是一个自主规划框架，用于推理物理世界和智能体信念，在信息流动和智能体意识至关重要的领域有应用。MEP的状态需要表示为克里普克结构（有向标记图），这限制了现有启发式方法的应用，导致认知求解器在探索指数级搜索空间时通常不可解。&lt;h4&gt;目的&lt;/h4&gt;解决多智能体认知规划中因状态表示导致的可扩展性问题，提高认知求解器的效率。&lt;h4&gt;方法&lt;/h4&gt;利用图神经网络(GNNs)学习认知状态中的模式和关系结构，通过从已解决的规划实例中获取的知识，推导出有意义的状态质量估计（例如，到最近目标的距离），并将这些预测启发式方法整合到认知规划流程中。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络能够自然捕捉克里普克模型的图特性，有效指导规划过程，显著提高了多智能体认知规划的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;将图神经网络学习到的预测启发式方法整合到认知规划流程中，能够有效解决传统方法面临的可扩展性问题，为多智能体认知规划提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多智能体认知规划(MEP)是一个自主规划框架，用于推理物理世界和智能体信念，在信息流动和智能体意识至关重要的领域有应用。MEP的丰富性要求状态被表示为克里普克结构，即有向标记图。这种表示限制了现有启发式方法的应用，阻碍了认知求解器的可扩展性，这些求解器必须在没有指导的情况下探索指数级搜索空间，通常导致不可解。为此，我们利用图神经网络(GNNs)学习认知状态中的模式和关系结构，以指导规划过程。GNNs能够自然捕捉克里普克模型的图特性，使我们能够通过从已解决的规划实例中获取的知识，推导出有意义的状态质量估计——例如，到最近目标的距离。我们将这些预测启发式方法整合到认知规划流程中，并与标准基线进行比较，显示出多智能体认知规划可扩展性的显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent Epistemic Planning (MEP) is an autonomous planning framework forreasoning about both the physical world and the beliefs of agents, withapplications in domains where information flow and awareness among agents arecritical. The richness of MEP requires states to be represented as Kripkestructures, i.e., directed labeled graphs. This representation limits theapplicability of existing heuristics, hindering the scalability of epistemicsolvers, which must explore an exponential search space without guidance,resulting often in intractability. To address this, we exploit Graph NeuralNetworks (GNNs) to learn patterns and relational structures within epistemicstates, to guide the planning process. GNNs, which naturally capture thegraph-like nature of Kripke models, allow us to derive meaningful estimates ofstate quality -- e.g., the distance from the nearest goal -- by generalizingknowledge obtained from previously solved planning instances. We integratethese predictive heuristics into an epistemic planning pipeline and evaluatethem against standard baselines, showing significant improvements in thescalability of multi-agent epistemic planning.</description>
      <author>example@mail.com (Giovanni Briglia, Francesco Fabiano, Stefano Mariani)</author>
      <guid isPermaLink="false">2508.12840v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>MPOCryptoML: Multi-Pattern based Off-Chain Crypto Money Laundering Detection</title>
      <link>http://arxiv.org/abs/2508.12641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MPOCryptoML的新型加密货币洗钱检测模型，能够有效识别多种洗钱模式，并在多个公开数据集上显示出显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;现有使用图神经网络的洗钱检测模型没有专门设计来检测链下加密货币的多样化洗钱模式，忽略任何洗钱模式都会导致关键检测缺口，降低模型准确性并让洗钱方案规避检测。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测加密货币交易中多种洗钱模式的模型，解决现有模型无法全面识别不同洗钱结构的问题。&lt;h4&gt;方法&lt;/h4&gt;MPOCryptoML模型包含：1)开发多源个性化PageRank算法识别随机洗钱模式；2)分析高容量金融网络中的交易时间和权重，引入两种新算法检测扇入、扇出、二分图、聚集-分散和堆叠等洗钱结构；3)使用逻辑回归模型分析这些模式间的相关性；4)设计异常评分函数整合各模块结果，按异常分数排名识别高风险账户。&lt;h4&gt;主要发现&lt;/h4&gt;在Elliptic++、以太坊欺诈检测和Wormhole交易数据集上的实验表明，MPOCryptoML在精度上提升最高达9.13%，召回率提升最高达10.16%，F1分数提升最高达7.63%，准确率提升最高达10.19%。&lt;h4&gt;结论&lt;/h4&gt;MPOCryptoML模型能够全面检测多种加密货币洗钱模式，显著提高检测性能，有效识别高风险账户，为加密货币安全交易提供更强有力的保障。&lt;h4&gt;翻译&lt;/h4&gt;近期洗钱检测进展表明，图神经网络在准确捕捉洗钱模式方面具有潜力。然而，现有模型并非专门设计用于检测链下加密货币的多样化洗钱模式。忽略任何洗钱模式都会引入关键检测缺口，因为每种模式都反映促进非法资金来源和流动混淆的独特交易结构。未能考虑这些模式可能导致对特定洗钱活动的检测不足或遗漏，降低模型准确性并使洗钱方案规避检测。为解决这一差距，我们提出了MPOCryptoML模型，以有效检测加密货币交易中的多种洗钱模式。MPOCryptoML包括开发多源个性化PageRank算法来识别随机洗钱模式。此外，我们通过分析高容量金融网络中的交易时间和权重，引入了两种新算法来检测各种洗钱结构，包括扇入、扇出、二分图、聚集-分散和堆叠模式。我们进一步使用逻辑回归模型分析这些模式之间的相关性。异常评分函数整合了每个模块的结果，按异常分数对账户进行排名，系统识别高风险账户。在包括Elliptic++、以太坊欺诈检测和Wormhole交易数据集在内的公共数据集上的广泛实验验证了MPOCryptoML的有效性和效率。结果显示性能持续提升，精度提高最高达9.13%，召回率提高最高达10.16%，F1分数提高最高达7.63%，准确率提高最高达10.19%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in money laundering detection have demonstrated thepotential of using graph neural networks to capture laundering patternsaccurately. However, existing models are not explicitly designed to detect thediverse patterns of off-chain cryptocurrency money laundering. Neglecting anylaundering pattern introduces critical detection gaps, as each pattern reflectsunique transactional structures that facilitate the obfuscation of illicit fundorigins and movements. Failure to account for these patterns may result inunder-detection or omission of specific laundering activities, diminishingmodel accuracy and allowing schemes to bypass detection. To address this gap,we propose the MPOCryptoML model to effectively detect multiple launderingpatterns in cryptocurrency transactions. MPOCryptoML includes the developmentof a multi-source Personalized PageRank algorithm to identify random launderingpatterns. Additionally, we introduce two novel algorithms by analyzing thetimestamp and weight of transactions in high-volume financial networks todetect various money laundering structures, including fan-in, fan-out,bipartite, gather-scatter, and stack patterns. We further examine correlationsbetween these patterns using a logistic regression model. An anomaly scorefunction integrates results from each module to rank accounts by anomaly score,systematically identifying high-risk accounts. Extensive experiments on publicdatasets including Elliptic++, Ethereum fraud detection, and Wormholetransaction datasets validate the efficacy and efficiency of MPOCryptoML.Results show consistent performance gains, with improvements up to 9.13% inprecision, up to 10.16% in recall, up to 7.63% in F1-score, and up to 10.19% inaccuracy.</description>
      <author>example@mail.com (Yasaman Samadi, Hai Dong, Xiaoyu Xia)</author>
      <guid isPermaLink="false">2508.12641v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation</title>
      <link>http://arxiv.org/abs/2508.12629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlowMol3是一种开源的多模态流匹配模型，能够在原子级别生成具有所需特性的真实分子，实现了接近100%的分子有效性，且比同类方法少一个数量级的可学习参数。&lt;h4&gt;背景&lt;/h4&gt;能够生成具有所需特性的真实分子的生成模型可以加速各种应用领域的化学发现。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够联合采样分子拓扑和三维结构的模型，以促进化学发现。&lt;h4&gt;方法&lt;/h4&gt;FlowMol3采用三种架构无关的技术：自条件、假原子和训练时几何扭曲，这些技术不需要改变图神经网络架构或底层流匹配公式，且计算成本可忽略。&lt;h4&gt;主要发现&lt;/h4&gt;FlowMol3在药物类分子上实现了接近100%的分子有效性，更准确地重现了训练数据的官能团组成和几何结构，比同类方法少一个数量级的可学习参数，并能检测和推理过程中的分布漂移。&lt;h4&gt;结论&lt;/h4&gt;这些简单的、可转移的策略可以改善基于扩散和流式的分子生成模型的稳定性和质量。&lt;h4&gt;翻译&lt;/h4&gt;一种能够生成具有所需特性的真实分子的生成模型可以加速各种应用领域的化学发现。为实现这一目标，大量工作集中在开发能够联合采样分子拓扑和三维结构的模型上。我们提出了FlowMol3，一个开源的多模态流匹配模型，在原子级别小分子生成方面取得了最先进的技术突破。与之前的FlowMol版本相比，FlowMol3的性能提升显著，且无需改变图神经网络架构或底层流匹配公式。相反，FlowMol3的改进来自于三种架构无关的技术，这些技术计算成本可忽略：自条件、假原子和训练时几何扭曲。FlowMol3在具有显式氢的药物类分子上实现了接近100%的分子有效性，更准确地重现了训练数据的官能团组成和几何结构，并且比同类方法少一个数量级的可学习参数。我们假设这些技术减轻了影响基于传输的生成模型的普遍病理，使模型能够检测和推理过程中的分布漂移。我们的结果强调了改善基于扩散和流式的分子生成模型的稳定性和质量的简单、可转移的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A generative model capable of sampling realistic molecules with desiredproperties could accelerate chemical discovery across a wide range ofapplications. Toward this goal, significant effort has focused on developingmodels that jointly sample molecular topology and 3D structure. We presentFlowMol3, an open-source, multi-modal flow matching model that advances thestate of the art for all-atom, small-molecule generation. Its substantialperformance gains over previous FlowMol versions are achieved without changesto the graph neural network architecture or the underlying flow matchingformulation. Instead, FlowMol3's improvements arise from threearchitecture-agnostic techniques that incur negligible computational cost:self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3achieves nearly 100% molecular validity for drug-like molecules with explicithydrogens, more accurately reproduces the functional group composition andgeometry of its training data, and does so with an order of magnitude fewerlearnable parameters than comparable methods. We hypothesize that thesetechniques mitigate a general pathology affecting transport-based generativemodels, enabling detection and correction of distribution drift duringinference. Our results highlight simple, transferable strategies for improvingthe stability and quality of diffusion- and flow-based molecular generativemodels.</description>
      <author>example@mail.com (Ian Dunn, David R. Koes)</author>
      <guid isPermaLink="false">2508.12629v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios</title>
      <link>http://arxiv.org/abs/2508.12100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 1 figure, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ReT-Eval的推理线程评估框架，通过两个阶段解决交互式问题解决中的推理效率问题：第一阶段从领域知识图中提取语义相关结构并利用大语言模型知识解决差异；第二阶段使用奖励引导策略评估和修剪推理线程，保持语义连贯性。&lt;h4&gt;背景&lt;/h4&gt;当前推理模型在交互式问题解决场景中存在三大局限：缺乏显式语义层次、用户-领域知识对齐不足、以及缺乏有效机制修剪推理线程，导致输出冗长且无法引导用户进行目标导向推理。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够生成有效推理线程的框架，引导用户完成目标导向推理步骤，提升用户理解能力。&lt;h4&gt;方法&lt;/h4&gt;ReT-Eval框架采用两阶段方法：第一阶段利用图神经网络从稀疏领域知识图中提取语义相关知识结构，并融入大语言模型知识解决知识差异；第二阶段通过奖励引导策略评估和修剪推理线程，确保语义连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;实验和专家评估表明，ReT-Eval框架能够有效增强用户理解，并优于现有最先进的推理模型。&lt;h4&gt;结论&lt;/h4&gt;ReT-Eval框架通过结构化知识重用和有效的推理线程修剪机制，成功解决了当前推理模型的局限性，为交互式问题解决提供了更有效的推理支持。&lt;h4&gt;翻译&lt;/h4&gt;在交互式问题解决场景中的推理需要模型构建反映用户理解并与结构化领域知识对齐的推理线程。然而，当前推理模型通常缺乏显式语义层次、用户-领域知识对齐，以及修剪推理线程的有效原则性机制。这些局限性导致冗长的通用输出，无法引导用户完成目标导向的推理步骤。为此，我们提出了一种受原型启发的两阶段推理线程评估框架，借鉴了强调结构化知识重用的人类推理策略。第一阶段，使用图神经网络从稀疏领域知识图中提取语义相关的知识结构，并利用大语言模型的内在知识解决知识差异。第二阶段，使用奖励引导策略评估和修剪这些线程，旨在保持语义连贯性以生成有效的推理线程。实验和专家评估表明，ReT-Eval增强了用户理解，并优于最先进的推理模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning in interactive problem solving scenarios requires models toconstruct reasoning threads that reflect user understanding and align withstructured domain knowledge. However, current reasoning models often lackexplicit semantic hierarchies, user-domain knowledge alignment, and principledmechanisms to prune reasoning threads for effectiveness. These limitationsresult in lengthy generic output that does not guide users throughgoal-oriented reasoning steps. To address this, we propose aprototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)framework, drawing inspiration from human-like reasoning strategies thatemphasize structured knowledge reuse. In the first phase, semantically relevantknowledge structures are extracted from a sparse domain knowledge graph using agraph neural network and enriched with intrinsic large language model knowledgeto resolve knowledge discrepancies. In the second phase, these threads areevaluated and pruned using a reward-guided strategy aimed at maintainingsemantic coherence to generate effective reasoning threads. Experiments andexpert evaluations show that ReT-Eval enhances user understanding andoutperforms state-of-the-art reasoning models.</description>
      <author>example@mail.com (Daniel Burkhardt, Xiangwei Cheng)</author>
      <guid isPermaLink="false">2508.12100v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Poisson Models for Supply Chain Relationship Forecasting</title>
      <link>http://arxiv.org/abs/2508.12044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种图双指数平滑（GDES）模型，用于预测供应链网络中未来供应关系的形成概率。该模型结合了图神经网络和非参数双指数平滑方法，能够捕捉供应链中复杂的互动关系。&lt;h4&gt;背景&lt;/h4&gt;在供应链网络中，公司会动态地形成或终止合作关系以适应市场波动，这给预测未来的供应链关系带来了挑战。仅依靠历史数据来预测未来强度存在局限性。&lt;h4&gt;目的&lt;/h4&gt;预测供应链网络中未来供应关系的形成概率，克服仅依靠历史数据的局限性。&lt;h4&gt;方法&lt;/h4&gt;将供应关系建模为非齐次泊松过程，提出图双指数平滑（GDES）模型，结合图神经网络和非参数双指数平滑方法。假设供应边的泊松强度函数是相关的，模型具有可解释性，能够将强度增量分解为当前边的历史数据和相邻边的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在包含87,969家公司的大规模供应链数据集上评估，该方法在动态链接预测中达到了93.84%的AUC，有效捕捉了复杂的供应链互动关系。&lt;h4&gt;结论&lt;/h4&gt;图双指数平滑模型在准确预测供应链网络中的未来关系方面是有效的，能够处理供应链中复杂的互动关系。&lt;h4&gt;翻译&lt;/h4&gt;在供应链网络中，公司动态地形成或终止合作关系以适应市场波动，这给预测未来的供应链关系带来了挑战。我们将供应关系（从公司i到公司j）建模为非齐次泊松过程，使用历史事件计数来估计直到时间t的泊松强度函数。然而，仅依靠历史数据预测未来强度存在局限性。为此，我们提出了一种新的图双指数平滑（GDES）模型，该模型结合了图神经网络和非参数双指数平滑方法，以预测未来供应关系形成的概率。认识到上游和下游公司之间的相互依存的经济动态，我们假设供应边的泊松强度函数是相关的，符合过程的非齐次性质。我们的模型具有可解释性，将强度增量分解为当前边的历史数据和供应链网络中相邻边的影响。在包含87,969家公司的大规模供应链数据集上进行评估，我们的方法在动态链接预测中达到了93.84%的AUC，证明了其在捕捉复杂供应链互动以实现准确预测方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In supply chain networks, firms dynamically form or dissolve partnerships toadapt to market fluctuations, posing a challenge for predicting future supplyrelationships. We model the occurrence of supply edges (firm i to firm j) as anon-homogeneous Poisson process (NHPP), using historical event counts toestimate the Poisson intensity function up to time t. However, forecastingfuture intensities is hindered by the limitations of historical data alone. Toovercome this, we propose a novel Graph Double Exponential Smoothing (GDES)model, which integrates graph neural networks (GNNs) with a nonparametricdouble exponential smoothing approach to predict the probability of futuresupply edge formations.Recognizing the interdependent economic dynamics betweenupstream and downstream firms, we assume that the Poisson intensity functionsof supply edges are correlated, aligning with the non-homogeneous nature of theprocess.Our model is interpretable, decomposing intensity increments intocontributions from the current edge's historical data and influences fromneighboring edges in the supply chain network. Evaluated on a large-scalesupply chain dataset with 87,969 firms, our approach achieves an AUC of 93.84 %in dynamic link prediction, demonstrating its effectiveness in capturingcomplex supply chain interactions for accurate forecasting.</description>
      <author>example@mail.com (Ling Xiang, Quan Hu, Xiang Zhang, Wei Lan, Bin Liu)</author>
      <guid isPermaLink="false">2508.12044v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>AI Models for Depressive Disorder Detection and Diagnosis: A Review</title>
      <link>http://arxiv.org/abs/2508.12022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文对抑郁症检测和诊断的最先进人工智能方法进行了全面综述，基于对55项关键研究的系统回顾，提出了一个分层分类法，并揭示了该领域的三个主要趋势。&lt;h4&gt;背景&lt;/h4&gt;抑郁症是全球主要的致残原因之一，但其诊断仍主要依赖主观的临床评估。&lt;h4&gt;目的&lt;/h4&gt;整合人工智能有望开发出客观、可扩展和及时的诊断工具，为抑郁症诊断提供新方法。&lt;h4&gt;方法&lt;/h4&gt;通过对55项关键研究进行系统回顾，提出一个分层分类法，按主要临床任务（诊断与预测）、数据模态（文本、语音、神经影像、多模态）和计算模型类别（如图神经网络、大型语言模型、混合方法）组织该领域。&lt;h4&gt;主要发现&lt;/h4&gt;深入分析揭示了三个主要趋势：图神经网络在建模大脑连接性中的主导地位、大型语言模型在语言和对话数据中的兴起，以及对多模态融合、可解释性和算法公平性的新兴关注。&lt;h4&gt;结论&lt;/h4&gt;通过综合当前进展并突出开放性挑战，这篇调查为计算精神病学未来的创新提供了全面的路线图。&lt;h4&gt;翻译&lt;/h4&gt;重度抑郁症是全球主要的致残原因之一，但其诊断仍然很大程度上依赖于主观的临床评估。整合人工智能有望开发出客观、可扩展和及时的诊断工具。在本文中，我们基于对55项关键研究的系统回顾，对用于抑郁症检测和诊断的最先进人工智能方法进行了全面综述。我们引入了一种新颖的分层分类法，通过主要临床任务（诊断与预测）、数据模态（文本、语音、神经影像、多模态）和计算模型类别（如图神经网络、大型语言模型、混合方法）对该领域进行结构化组织。我们的深入分析揭示了三个主要趋势：图神经网络在建模大脑连接性中的主导地位、大型语言模型在语言和对话数据中的兴起，以及对多模态融合、可解释性和算法公平性的新兴关注。除了方法论见解外，我们还概述了突出的公共数据集和标准评估指标，为研究人员提供实用指南。通过综合当前进展并突出开放性挑战，本调查为计算精神病学未来的创新提供了全面的路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Major Depressive Disorder is one of the leading causes of disabilityworldwide, yet its diagnosis still depends largely on subjective clinicalassessments. Integrating Artificial Intelligence (AI) holds promise fordeveloping objective, scalable, and timely diagnostic tools. In this paper, wepresent a comprehensive survey of state-of-the-art AI methods for depressiondetection and diagnosis, based on a systematic review of 55 key studies. Weintroduce a novel hierarchical taxonomy that structures the field by primaryclinical task (diagnosis vs. prediction), data modality (text, speech,neuroimaging, multimodal), and computational model class (e.g., graph neuralnetworks, large language models, hybrid approaches). Our in-depth analysisreveals three major trends: the predominance of graph neural networks formodeling brain connectivity, the rise of large language models for linguisticand conversational data, and an emerging focus on multimodal fusion,explainability, and algorithmic fairness. Alongside methodological insights, weprovide an overview of prominent public datasets and standard evaluationmetrics as a practical guide for researchers. By synthesizing current advancesand highlighting open challenges, this survey offers a comprehensive roadmapfor future innovation in computational psychiatry.</description>
      <author>example@mail.com (Dorsa Macky Aleagha, Payam Zohari, Mostafa Haghir Chehreghani)</author>
      <guid isPermaLink="false">2508.12022v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images</title>
      <link>http://arxiv.org/abs/2508.11826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究填补了图神经网络在图像到图转换和图级异常检测中的比较空白，系统评估了多种转换方法的有效性&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为基于图的机器学习任务的有力方法，先前工作将GNNs应用于图像派生的图表示用于下游任务，但尚无研究严格比较众多图像到图转换方法在GNN-based图级异常检测中的有效性&lt;h4&gt;目的&lt;/h4&gt;系统评估多种分割方案、边构建策略和基于颜色、纹理和形状描述符的节点特征集的有效性，生成适合图级异常检测的图像派生图表示&lt;h4&gt;方法&lt;/h4&gt;使用最先进的GLAD模型在皮肤镜图像上进行大量实验，检查在完全无监督、弱监督和完全监督模式下的性能和效率&lt;h4&gt;主要发现&lt;/h4&gt;颜色描述符提供最佳独立性能，结合形状和纹理特征能持续提高检测效果；最佳无监督配置使用OCGTL实现0.805的AUC-ROC；加入稀疏标签后性能提高到0.872；完全监督下达到0.914 AUC-ROC&lt;h4&gt;结论&lt;/h4&gt;颜色、形状和纹理特征的结合能提高异常检测性能；即使在无监督设置下，所提出的方法也能与基于图像的竞争方法相媲美&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为基于图的机器学习任务的有力方法。先前的工作将GNNs应用于图像派生的图表示，用于各种下游任务，如分类或异常检测。这些转换包括分割图像、从片段中提取特征、将它们映射到节点以及连接它们。然而，据我们所知，尚无研究严格比较众多可能的图像到图转换方法在基于GNN的图级异常检测中的有效性。在本研究中，我们系统评估了多种分割方案、边构建策略和基于颜色、纹理和形状描述符的节点特征集的有效性，以生成适合用于图级异常检测的图像派生图表示。我们使用最先进的GLAD模型在皮肤镜图像上进行了大量实验，检查了在完全无监督、弱监督和完全监督模式下的性能和效率。我们的研究结果表明，例如，颜色描述符提供了最佳的独立性能，而结合形状和纹理特征能持续提高检测效果。特别是，我们使用OCGTL的最佳无监督配置实现了高达0.805的竞争性AUC-ROC分数，而不依赖于预训练主干网络，与可比的基于图像的方法相当。加入稀疏标签后，性能显著提高到0.872，完全监督下达到0.914 AUC-ROC。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as a powerful approach forgraph-based machine learning tasks. Previous work applied GNNs to image-derivedgraph representations for various downstream tasks such as classification oranomaly detection. These transformations include segmenting images, extractingfeatures from segments, mapping them to nodes, and connecting them. However, tothe best of our knowledge, no study has rigorously compared the effectivenessof the numerous potential image-to-graph transformation approaches forGNN-based graph-level anomaly detection (GLAD). In this study, wesystematically evaluate the efficacy of multiple segmentation schemes, edgeconstruction strategies, and node feature sets based on color, texture, andshape descriptors to produce suitable image-derived graph representations toperform graph-level anomaly detection. We conduct extensive experiments ondermoscopic images using state-of-the-art GLAD models, examining performanceand efficiency in purely unsupervised, weakly supervised, and fully supervisedregimes. Our findings reveal, for example, that color descriptors contributethe best standalone performance, while incorporating shape and texture featuresconsistently enhances detection efficacy. In particular, our best unsupervisedconfiguration using OCGTL achieves a competitive AUC-ROC score of up to 0.805without relying on pretrained backbones like comparable image-based approaches.With the inclusion of sparse labels, the performance increases substantially to0.872 and with full supervision to 0.914 AUC-ROC.</description>
      <author>example@mail.com (Dehn Xu, Tim Katzke, Emmanuel Müller)</author>
      <guid isPermaLink="false">2508.11826v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication</title>
      <link>http://arxiv.org/abs/2508.11733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages for main content, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SafeSieve是一种渐进式自适应多智能体剪枝算法，通过双机制动态优化智能体间通信，实现了从启发式初始化到经验驱动精细化的平滑过渡，在保持高准确率的同时显著减少了token使用和部署成本。&lt;h4&gt;背景&lt;/h4&gt;基于大语言模型的多智能体系统表现出强大的协作能力，但通常存在冗余通信和过多的token开销问题。现有方法通常通过预训练的GNN或贪心算法来提高效率，但缺乏统一的策略来优化任务前和任务后的过程。&lt;h4&gt;目的&lt;/h4&gt;提出SafeSieve，一种渐进式自适应多智能体剪枝算法，通过新的双机制动态优化智能体间通信，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;SafeSieve整合了基于LLM的初始语义评估和累积的性能反馈，采用0扩展聚类来保持结构连贯的智能体组，同时消除无效连接，实现了从启发式初始化到经验驱动精细化的平滑过渡。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，SafeSieve实现了94.01%的平均准确率，同时减少了12.4%-27.8%的token使用；在提示注入攻击下表现出鲁棒性（平均准确率仅下降1.23%）；在异构环境中，在保持性能的同时减少了13.3%的部署成本。&lt;h4&gt;结论&lt;/h4&gt;SafeSieve是一种鲁棒、高效和可扩展的框架，适用于实际多智能体系统。&lt;h4&gt;翻译&lt;/h4&gt;基于大语言模型的多智能体系统表现出强大的协作能力，但常常遭受冗余通信和过多的token开销。现有方法通常通过预训练的GNN或贪心算法来提高效率，但通常将任务前和任务后的优化分开，缺乏统一的策略。为此，我们提出了SafeSieve，一种渐进式自适应多智能体剪枝算法，通过一种新的双机制动态优化智能体间通信。SafeSieve整合了基于LLM的初始语义评估和累积的性能反馈，实现了从启发式初始化到经验驱动精细化的平滑过渡。与现有的贪心Top-k剪枝方法不同，SafeSieve采用0扩展聚类来保持结构连贯的智能体组，同时消除无效连接。在多个基准测试（SVAMP、HumanEval等）中的实验表明，SafeSieve实现了94.01%的平均准确率，同时减少了12.4%-27.8%的token使用。结果进一步表明在提示注入攻击下具有鲁棒性（平均准确率下降1.23%）。在异构环境中，SafeSieve在保持性能的同时减少了13.3%的部署成本。这些结果确立了SafeSieve作为实际多智能体系统的一种鲁棒、高效和可扩展的框架。我们的代码可以在https://anonymous.4open.science/r/SafeSieve-D8F2FFUN找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LLM-based multi-agent systems exhibit strong collaborative capabilities butoften suffer from redundant communication and excessive token overhead.Existing methods typically enhance efficiency through pretrained GNNs or greedyalgorithms, but often isolate pre- and post-task optimization, lacking aunified strategy. To this end, we present SafeSieve, a progressive and adaptivemulti-agent pruning algorithm that dynamically refines the inter-agentcommunication through a novel dual-mechanism. SafeSieve integrates initialLLM-based semantic evaluation with accumulated performance feedback, enabling asmooth transition from heuristic initialization to experience-drivenrefinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs0-extension clustering to preserve structurally coherent agent groups whileeliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval,etc.) showcase that SafeSieve achieves 94.01% average accuracy while reducingtoken usage by 12.4%-27.8%. Results further demonstrate robustness under promptinjection attacks (1.23% average accuracy drop). In heterogeneous settings,SafeSieve reduces deployment costs by 13.3% while maintaining performance.These results establish SafeSieve as a robust, efficient, and scalableframework for practical multi-agent systems. Our code can be found inhttps://anonymous.4open.science/r/SafeSieve-D8F2FFUN.</description>
      <author>example@mail.com (Ruijia Zhang, Xinyan Zhao, Ruixiang Wang, Sigen Chen, Guibin Zhang, An Zhang, Kun Wang, Qingsong Wen)</author>
      <guid isPermaLink="false">2508.11733v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data</title>
      <link>http://arxiv.org/abs/2508.11723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 32 figures, submitted to Environment and Planning B: Urban  Analytics and City Science&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SPLI的场地规划布局指标系统，这是一个数据驱动的框架，整合实证知识和多源异构数据，用于生成结构化的城市空间信息，支持多模态空间数据分析。&lt;h4&gt;背景&lt;/h4&gt;城市空间布局影响土地利用效率和空间组织，但传统场地规划依赖经验判断和单一数据源，限制了多功能布局的系统量化。&lt;h4&gt;目的&lt;/h4&gt;开发SPLI系统，通过整合多源数据支持城市空间分析，并扩展传统指标，包括建筑功能层次分类、空间组织、功能多样性、基本服务可达性和土地利用强度五个维度。&lt;h4&gt;方法&lt;/h4&gt;SPLI系统结合OpenStreetMap、兴趣点、建筑形态、土地利用和卫星影像等多源数据，通过关系图神经网络和图神经网络等深度学习技术解决数据缺口问题。&lt;h4&gt;主要发现&lt;/h4&gt;SPLI系统提高了功能分类的准确性，为自动化、数据驱动的城市空间分析提供了标准化基础。&lt;h4&gt;结论&lt;/h4&gt;SPLI系统通过整合多源数据和实证知识，为城市空间分析提供了更系统、量化的方法，克服了传统场地规划的局限性。&lt;h4&gt;翻译&lt;/h4&gt;城市场地的空间布局塑造了土地利用效率和空间组织。传统的场地规划往往依赖经验判断和单一数据源，限制了多功能布局的系统量化。我们提出了一个场地规划布局指标（SPLI）系统，这是一个数据驱动的框架，整合了实证知识和多源异构数据，以生成结构化的城市空间信息。SPLI通过结合OpenStreetMap（OSM）、兴趣点（POI）、建筑形态、土地利用和卫星影像，支持多模态空间数据系统进行分析、推理和检索。它通过五个维度扩展了传统指标：（1）层次化建筑功能分类，将实证系统细化为清晰的层次结构；（2）空间组织，量化七种布局模式（如对称型、同心型、轴向型）；（3）功能多样性，使用功能比率（FR）和辛普森指数（SI）将定性评估转化为可测量指标；（4）基本服务可达性，整合设施分布和交通网络，提供全面的可达性指标；（5）土地利用强度，使用容积率（FAR）和建筑覆盖率（BCR）评估利用效率。通过深度学习（包括关系图神经网络（RGNN）和图神经网络（GNN））解决数据缺口问题。实验表明，SPLI提高了功能分类的准确性，并为自动化、数据驱动的城市空间分析提供了标准化基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The spatial layout of urban sites shapes land-use efficiency and spatialorganization. Traditional site planning often relies on experiential judgmentand single-source data, limiting systematic quantification of multifunctionallayouts. We propose a Site Planning Layout Indicator (SPLI) system, adata-driven framework integrating empirical knowledge with heterogeneousmulti-source data to produce structured urban spatial information. The SPLIsupports multimodal spatial data systems for analytics, inference, andretrieval by combining OpenStreetMap (OSM), Points of Interest (POI), buildingmorphology, land use, and satellite imagery. It extends conventional metricsthrough five dimensions: (1) Hierarchical Building Function Classification,refining empirical systems into clear hierarchies; (2) Spatial Organization,quantifying seven layout patterns (e.g., symmetrical, concentric,axial-oriented); (3) Functional Diversity, transforming qualitative assessmentsinto measurable indicators using Functional Ratio (FR) and Simpson Index (SI);(4) Accessibility to Essential Services, integrating facility distribution andtransport networks for comprehensive accessibility metrics; and (5) Land UseIntensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) toassess utilization efficiency. Data gaps are addressed through deep learning,including Relational Graph Neural Networks (RGNN) and Graph Neural Networks(GNN). Experiments show the SPLI improves functional classification accuracyand provides a standardized basis for automated, data-driven urban spatialanalytics.</description>
      <author>example@mail.com (Qian Cao, Jielin Chen, Junchao Zhao, Rudi Stouffs)</author>
      <guid isPermaLink="false">2508.11723v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis</title>
      <link>http://arxiv.org/abs/2508.13028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Speech Synthesis Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种新的讽刺性语音合成方法，通过整合双模态讽刺检测模型的反馈损失到TTS训练过程，并利用迁移学习进行两阶段微调，有效提高了合成语音的质量、自然度和讽刺表达能力。&lt;h4&gt;背景&lt;/h4&gt;讽刺性语音合成对于增强娱乐和人机交互等应用中的自然互动至关重要。然而，由于讽刺的韵律微妙性以及标注的讽刺语音数据有限，合成讽刺性语音仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;解决讽刺性语音合成中的挑战，提高模型捕捉和传达讽刺的能力，生成更自然、更具讽刺意识的语音。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一种新方法，将双模态讽刺检测模型的反馈损失整合到TTS训练过程中。同时，利用迁移学习，首先在包含各种语音风格（包括讽刺语音）的多样化数据集上对预训练的语音合成模型进行微调，然后在专注于讽刺语音的数据集上进一步优化模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过客观和主观评估，证明提出的方法提高了合成语音的质量、自然度和讽刺表达能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效解决讽刺性语音合成中的挑战，生成更高质量的讽刺语音，适用于娱乐和人机交互等应用场景。&lt;h4&gt;翻译&lt;/h4&gt;讽刺性语音合成涉及生成能有效传达讽刺的语音，对于增强娱乐和人机交互等应用中的自然互动至关重要。然而，由于讽刺特有的微妙韵律以及标注的讽刺语音数据有限，合成讽刺性语音仍然是一个挑战。为解决这些挑战，本研究引入了一种新方法，将双模态讽刺检测模型的反馈损失整合到TTS训练过程中，增强模型捕捉和传达讽刺的能力。此外，通过利用迁移学习，在朗读语音上预训练的语音合成模型经历了两阶段微调过程。首先，它在包含各种语音风格（包括讽刺语音）的多样化数据集上进行微调。在第二阶段，模型使用专注于讽刺语音的数据集进一步优化，提高其生成讽刺意识语音的能力。客观和主观评估证明，我们提出的方法提高了合成语音的质量、自然度和讽刺意识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sarcastic speech synthesis, which involves generating speech that effectivelyconveys sarcasm, is essential for enhancing natural interactions inapplications such as entertainment and human-computer interaction. However,synthesizing sarcastic speech remains a challenge due to the nuanced prosodythat characterizes sarcasm, as well as the limited availability of annotatedsarcastic speech data. To address these challenges, this study introduces anovel approach that integrates feedback loss from a bi-modal sarcasm detectionmodel into the TTS training process, enhancing the model's ability to captureand convey sarcasm. In addition, by leveraging transfer learning, a speechsynthesis model pre-trained on read speech undergoes a two-stage fine-tuningprocess. First, it is fine-tuned on a diverse dataset encompassing variousspeech styles, including sarcastic speech. In the second stage, the model isfurther refined using a dataset focused specifically on sarcastic speech,enhancing its ability to generate sarcasm-aware speech. Objective andsubjective evaluations demonstrate that our proposed methods improve thequality, naturalness, and sarcasm-awareness of synthesized speech.</description>
      <author>example@mail.com (Zhu Li, Yuqing Zhang, Xiyuan Gao, Devraj Raghuvanshi, Nagendra Kumar, Shekhar Nayak, Matt Coler)</author>
      <guid isPermaLink="false">2508.13028v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian</title>
      <link>http://arxiv.org/abs/2508.12993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究发现图的代数连通性（Fiedler值）可以作为预测图卷积网络（GCN）性能的有效指标，具有相似Fiedler值的图结构特性类似，可使用相同超参数获得相似结果，且在这些图间迁移学习更有效。&lt;h4&gt;背景&lt;/h4&gt;在图卷积网络（GCN）文献中，一个常见的观察是堆叠GCN层可能也可能不会在节点分类和边预测等任务上带来更好的性能，需要更好的指标来预测GCN性能。&lt;h4&gt;目的&lt;/h4&gt;探索图的代数连通性（Fiedler值）作为GCN性能预测指标的可行性，并研究为什么Fiedler值是一个好的预测指标。&lt;h4&gt;方法&lt;/h4&gt;通过理论和实验方式研究，在合成和真实图数据（包括Cora、CiteSeer和Polblogs数据集）上进行实验，探索多种聚合图中连通分量Fiedler值的方法，并给出Fiedler值作为良好预测指标的理论解释。&lt;h4&gt;主要发现&lt;/h4&gt;1. 图的代数连通性（Fiedler值）是预测GCN性能的良好指标；2. 具有相似Fiedler值的图具有相似的结构特性；3. 在具有相似代数连通性的图之间进行迁移学习可能更有效；4. 可以通过多种方式聚合图中连通分量的Fiedler值来获得整个图的值。&lt;h4&gt;结论&lt;/h4&gt;Fiedler值可以作为预测GCN性能的有效指标，对于理解和优化GCN在不同图上的应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;图卷积网络（GCN）文献中的一个常见观察是，堆叠GCN层可能也可能不会在节点分类和边预测等任务上带来更好的性能。我们通过经验发现，图的代数连通性（称为Fiedler值）是预测GCN性能的良好指标。直观地说，具有相似Fiedler值的图具有类似的结构特性，这表明当与GCN一起使用时，相同的过滤器和超参数可能会产生相似的结果，并且在具有相似代数连通性的图之间的迁移学习可能更有效。我们在合成和真实图数据（包括Cora、CiteSeer和Polblogs数据集）上的实验中从理论和经验上探索了这一点。我们探索了多种方法来聚合图中连通分量的Fiedler值，以获得整个图的值，并展示它可以用于预测GCN性能。我们还提出了理论论据，解释为什么Fiedler值是一个好的预测指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A common observation in the Graph Convolutional Network (GCN) literature isthat stacking GCN layers may or may not result in better performance on taskslike node classification and edge prediction. We have found empirically that agraph's algebraic connectivity, which is known as the Fiedler value, is a goodpredictor of GCN performance. Intuitively, graphs with similar Fiedler valueshave analogous structural properties, suggesting that the same filters andhyperparameters may yield similar results when used with GCNs, and thattransfer learning may be more effective between graphs with similar algebraicconnectivity. We explore this theoretically and empirically with experiments onsynthetic and real graph data, including the Cora, CiteSeer and Polblogsdatasets. We explore multiple ways of aggregating the Fiedler value forconnected components in the graphs to arrive at a value for the entire graph,and show that it can be used to predict GCN performance. We also presenttheoretical arguments as to why the Fiedler value is a good predictor.</description>
      <author>example@mail.com (Shalima Binta Manir, Tim Oates)</author>
      <guid isPermaLink="false">2508.12993v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Neutrino Scattering: Domain Adaptation with GANs</title>
      <link>http://arxiv.org/abs/2508.12987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用迁移学习技术，将在合成中微子-碳散射数据上训练的GAN模型迁移应用于中微子-氩和反中微子-碳相互作用的散射事件生成，显著优于从头开始训练的方法，即使在数据量较小的情况下也能保持良好性能。&lt;h4&gt;背景&lt;/h4&gt;研究基于在合成的荷电流(CC)中微子-碳非弹性散射数据上训练的生成对抗网络(GAN)模型，关注中微子与不同原子核相互作用的散射事件生成问题。&lt;h4&gt;目的&lt;/h4&gt;利用迁移学习将GAN模型中的物理知识迁移到新的中微子-原子核相互作用类型，评估迁移学习在数据来自不同相互作用模型时的有效性，为实验数据稀少场景提供解决方案。&lt;h4&gt;方法&lt;/h4&gt;采用迁移学习技术，将基础GAN模型适应到新的相互作用类型，比较迁移学习与从头开始训练的性能，使用10,000和100,000两种不同规模的事件数据集进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习显著优于从头开始训练生成模型；通过迁移学习获得的模型即使在较小的训练数据(10,000事件)下也能表现良好；该方法在实验数据稀少的场景中具有应用潜力。&lt;h4&gt;结论&lt;/h4&gt;所提出的迁移学习方法为构建中微子散射事件生成器提供了有希望的技术路线，特别适用于实验数据有限的粒子物理研究领域。&lt;h4&gt;翻译&lt;/h4&gt;我们利用迁移学习来推断在合成的荷电流(CC)中微子-碳非弹性散射数据上训练的生成对抗网络(GAN)模型中编码的物理知识。这个基础模型被调整为生成中微子-氩和反中微子-碳相互作用的CC非弹性散射事件(仅轻子运动学)。此外，我们评估了迁移学习在当新数据来自不同的中微子-原子核相互作用模型时重新优化自定义模型的有效性。我们的结果表明，迁移学习显著优于从头开始训练生成模型。为了研究这一点，我们考虑两个训练数据集：一个包含10,000个事件，另一个包含100,000个事件。通过迁移学习获得的模型即使在较小的训练数据下也能表现良好。所提出的方法为在实验数据稀少的场景中构建中微子散射事件生成器提供了一种有希望的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We utilize transfer learning to extrapolate the physics knowledge encoded ina Generative Adversarial Network (GAN) model trained on syntheticcharged-current (CC) neutrino-carbon inclusive scattering data. This base modelis adapted to generate CC inclusive scattering events (lepton kinematics only)for neutrino-argon and antineutrino-carbon interactions. Furthermore, we assessthe effectiveness of transfer learning in re-optimizing a custom model when newdata comes from a different neutrino-nucleus interaction model. Our resultsdemonstrate that transfer learning significantly outperforms traininggenerative models from scratch. To study this, we consider two training datasets: one with 10,000 and another with 100,000 events. The models obtained viatransfer learning perform well even with smaller training data. The proposedmethod provides a promising approach for constructing neutrino scattering eventgenerators in scenarios where experimental data is sparse.</description>
      <author>example@mail.com (Jose L. Bonilla, Krzysztof M. Graczyk, Artur M. Ankowski, Rwik Dharmapal Banerjee, Beata E. Kowal, Hemant Prasad, Jan T. Sobczyk)</author>
      <guid isPermaLink="false">2508.12987v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning</title>
      <link>http://arxiv.org/abs/2508.12877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MPS-Tuning的新型微调方法，用于预训练视觉-语言模型，通过保留和雕刻语义流形来提高模型性能同时保持数据分布的几何结构。&lt;h4&gt;背景&lt;/h4&gt;预训练的视觉-语言模型（如CLIP）在少样本图像分类中展现出巨大潜力，催生了多种迁移学习策略。这些方法利用VLM的预训练知识实现有效的领域适应，同时通过参数高效调整或基于实例的一致性约束减轻过拟合。&lt;h4&gt;目的&lt;/h4&gt;克服现有正则化方法忽略数据分布几何结构的问题，提出一种能够保留数据分布几何结构同时增强类别可分性的新型微调方法。&lt;h4&gt;方法&lt;/h4&gt;提出'流形保持与雕刻调优'（MPS-Tuning）方法，将特征空间中的数据分布视为语义流形，通过微调前后特征的Gram矩阵对齐保留原始流形的拓扑结构，并将图像和文本模态的特征配对优化以增强类别判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，MPS-Tuning显著提高了模型性能，同时有效保留了语义流形的结构。&lt;h4&gt;结论&lt;/h4&gt;MPS-Tuning是一种有效的微调方法，能够平衡模型性能和语义结构保持，代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;预训练的视觉-语言模型（如CLIP）在少样本图像分类中展现出巨大潜力，并催生了许多有效的迁移学习策略。这些方法利用VLM的预训练知识实现有效的领域适应，同时通过参数高效调整或基于实例的一致性约束来减轻过拟合。然而，这类正则化方法通常忽略了数据分布的几何结构，可能导致整体语义表示的扭曲。为克服这一局限，我们提出了一种新型微调方法——流形保持与雕刻调优（MPS-Tuning）。将特征空间中的数据分布视为语义流形，MPS-Tuning明确约束该流形的内在几何结构，同时进一步雕刻它以增强类别可分性。具体而言，MPS-Tuning通过微调前后特征的Gram矩阵对齐，保留了原始流形的宏观和微观拓扑结构。理论上，该约束被证明近似于Gromov-Wasserstein距离的上界。此外，图像和文本模态的特征被配对，并通过优化成对相似性来增强流形的类别判别能力。大量实验表明，MPS-Tuning显著提高了模型性能，同时有效保留了语义流形的结构。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained vision-language models (VLMs), such as CLIP, have shown remarkablepotential in few-shot image classification and led to numerous effectivetransfer learning strategies. These methods leverage the pretrained knowledgeof VLMs to enable effective domain adaptation while mitigating overfittingthrough parameter-efficient tuning or instance-based consistency constraints.However, such regularizations often neglect the geometric structure of datadistribution, which may lead to distortion of the overall semanticrepresentation. To overcome this limitation, we propose a novel fine-tuningmethod, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding thedata distribution in feature space as a semantic manifold, MPS-Tuningexplicitly constrains the intrinsic geometry of this manifold while furthersculpting it to enhance class separability. Specifically, MPS-Tuning preservesboth macroscopic and microscopic topological structures of the originalmanifold by aligning Gram matrices of features before and after fine-tuning.Theoretically, this constraint is shown to approximate an upper bound of theGromov-Wasserstein distance. Furthermore, features from the image and textmodalities are paired, and pairwise similarities are optimized to enhance themanifold's class discriminability. Extensive experiments demonstrate thatMPS-Tuning significantly improves model performance while effectivelypreserving the structure of the semantic manifold. The code will be released.</description>
      <author>example@mail.com (Dexia Chen, Qianjie Zhu, Weibing Li, Yue Yu, Tong Zhang, Ruixuan Wang)</author>
      <guid isPermaLink="false">2508.12877v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models</title>
      <link>http://arxiv.org/abs/2508.12861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对视觉语言模型在跨域少样本图像识别任务中的局限性，提出了一种名为一致性引导多视图协同优化的新型微调策略，并建立了相应的跨域少样本基准进行评估。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在自然图像和语言数据上预训练后，在少样本图像识别任务中展现出巨大潜力，各种迁移学习方法也取得了良好性能。然而，当面对与自然图像不同的成像域的跨域任务时，这些方法的有效性通常受到限制。&lt;h4&gt;目的&lt;/h4&gt;解决视觉语言模型在跨域少样本任务中的有效性受限问题，提高模型在非自然图像域的识别性能。&lt;h4&gt;方法&lt;/h4&gt;提出一致性引导多视图协同优化策略，该策略使用两个功能互补的专家模块提取多视图特征，并结合基于先验知识的一致性约束和基于信息几何的共识机制增强特征学习的鲁棒性。同时建立新的跨域少样本基准用于全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;在现有和新提出的基准上的大量经验评估表明，CoMuCo在少样本任务中始终优于当前方法，有效提升了视觉语言模型在跨域场景下的性能。&lt;h4&gt;结论&lt;/h4&gt;CoMuCo是一种有效的视觉语言模型微调策略，能够成功处理跨域少样本图像识别任务，相关代码和基准将公开发布以促进领域研究。&lt;h4&gt;翻译&lt;/h4&gt;在自然图像和语言数据上预训练的视觉语言模型，如CLIP，在少样本图像识别任务中展现出巨大潜力，促进了各种高效迁移学习方法的发展。这些方法利用视觉语言模型中固有的预学习知识，在标准图像数据集上取得了强大性能。然而，当面对与自然图像不同的成像域的跨域任务时，它们的有效性通常受到限制。为解决这一局限，我们提出了一致性引导多视图协同优化，一种视觉语言模型的新型微调策略。该策略采用两个功能互补的专家模块提取多视图特征，同时融入基于先验知识的一致性约束和基于信息几何的共识机制，以增强特征学习的鲁棒性。此外，还建立了一个新的跨域少样本基准，用于全面评估在不同于自然图像的成像域上的方法。在现有和新提出的基准上的大量经验评估表明，CoMuCo在少样本任务中持续优于当前方法。代码和基准将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) pre-trained on natural image and language data,such as CLIP, have exhibited significant potential in few-shot imagerecognition tasks, leading to development of various efficient transferlearning methods. These methods exploit inherent pre-learned knowledge in VLMsand have achieved strong performance on standard image datasets. However, theireffectiveness is often limited when confronted with cross-domain tasks whereimaging domains differ from natural images. To address this limitation, wepropose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), anovel fine-tuning strategy for VLMs. This strategy employs two functionallycomplementary expert modules to extract multi-view features, whileincorporating prior knowledge-based consistency constraints and informationgeometry-based consensus mechanisms to enhance the robustness of featurelearning. Additionally, a new cross-domain few-shot benchmark is established tohelp comprehensively evaluate methods on imaging domains distinct from naturalimages. Extensive empirical evaluations on both existing and newly proposedbenchmarks suggest CoMuCo consistently outperforms current methods in few-shottasks. The code and benchmark will be released.</description>
      <author>example@mail.com (Dexia Chen, Wentao Zhang, Qianjie Zhu, Ping Hu, Weibing Li, Tong Zhang, Ruixuan Wang)</author>
      <guid isPermaLink="false">2508.12861v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Contro</title>
      <link>http://arxiv.org/abs/2508.12738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, accepted for CDC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层次贝叶斯优化框架，用于在不同闭环任务中高效学习控制器参数，通过利用问题的结构知识实现知识转移和增强数据效率。&lt;h4&gt;背景&lt;/h4&gt;许多控制问题需要在不同闭环任务中重复调整和适应控制器，其中数据效率和适应性至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个分层次贝叶斯优化框架，专门针对顺序决策和控制场景中的高效控制器参数学习。&lt;h4&gt;方法&lt;/h4&gt;该方法不将闭环成本视为黑盒，而是利用底层问题的结构知识（包括动态系统、控制律和相关的闭环成本函数）。使用高斯过程构建分层代理模型，捕捉不同参数化下的闭环状态演化，并通过已知闭式表达式精确计算特定任务的权重和累积到闭环成本中。这允许在不同闭环任务之间进行知识转移和增强数据效率。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架保留了与标准黑盒贝叶斯优化相当的次线性遗憾保证，同时支持多任务或迁移学习。&lt;h4&gt;结论&lt;/h4&gt;在模型预测控制的仿真实验中，与纯黑盒贝叶斯优化方法相比，在样本效率和适应性方面都有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;许多控制问题需要在不同闭环任务中重复调整和适应控制器，其中数据效率和适应性至关重要。我们提出了一种针对顺序决策和控制场景中不同任务高效控制器参数学习的分层次贝叶斯优化框架。该方法不将闭环成本视为黑盒，而是利用底层问题的结构知识，包括动态系统、控制律和相关的闭环成本函数。我们使用高斯过程构建分层代理模型，捕捉不同参数化下的闭环状态演化，同时通过已知闭式表达式精确计算特定任务的权重并累积到闭环成本中。这允许在不同闭环任务之间进行知识转移和增强数据效率。与标准黑盒贝叶斯优化相比，所提出的框架保留了次线性遗憾保证，同时支持多任务或迁移学习。模型预测控制的仿真实验表明，与纯黑盒贝叶斯优化方法相比，在样本效率和适应性方面都有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many control problems require repeated tuning and adaptation of controllersacross distinct closed-loop tasks, where data efficiency and adaptability arecritical. We propose a hierarchical Bayesian optimization (BO) framework thatis tailored to efficient controller parameter learning in sequentialdecision-making and control scenarios for distinct tasks. Instead of treatingthe closed-loop cost as a black-box, our method exploits structural knowledgeof the underlying problem, consisting of a dynamical system, a control law, andan associated closed-loop cost function. We construct a hierarchical surrogatemodel using Gaussian processes that capture the closed-loop state evolutionunder different parameterizations, while the task-specific weighting andaccumulation into the closed-loop cost are computed exactly via knownclosed-form expressions. This allows knowledge transfer and enhanced dataefficiency between different closed-loop tasks. The proposed framework retainssublinear regret guarantees on par with standard black-box BO, while enablingmulti-task or transfer learning. Simulation experiments with model predictivecontrol demonstrate substantial benefits in both sample efficiency andadaptability when compared to purely black-box BO approaches.</description>
      <author>example@mail.com (Sebastian Hirt, Lukas Theiner, Maik Pfefferkorn, Rolf Findeisen)</author>
      <guid isPermaLink="false">2508.12738v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>BUILDA: A Thermal Building Data Generation Framework for Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.12703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings can be accessed at:  https://annsim.org/2025-annsim-proceedings/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了BuilDa，一个用于生成高质量、足够数量合成数据的建筑热数据生成框架，以支持迁移学习研究，无需深入的建筑模拟专业知识。&lt;h4&gt;背景&lt;/h4&gt;迁移学习可改善建筑热动力学数据驱动建模，但现有研究方向需要大量建筑热数据，而公共数据集和数据生成器在数据质量和数量上无法满足需求，且现有数据生成方法需要建筑模拟专业知识。&lt;h4&gt;目的&lt;/h4&gt;开发BuilDa框架，生成质量和数量足够的合成数据用于迁移学习研究，减少对建筑模拟专业知识的依赖。&lt;h4&gt;方法&lt;/h4&gt;BuilDa使用单区域Modelica模型，导出为功能mock-up unit (FMU)，在Python中模拟，无需深入的建筑模拟知识即可生成大量数据。&lt;h4&gt;主要发现&lt;/h4&gt;通过生成数据并用于预训练和微调迁移学习模型，成功演示了BuilDa框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;BuilDa框架能够生成满足迁移学习研究需求的高质量和足够数量的合成数据，降低了建筑热数据获取的门槛。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习可以改善建筑热动力学的数据驱动建模。因此，该领域出现了许多新的迁移学习研究方向，例如选择合适的迁移学习源模型。然而，这些研究方向目前需要大量的建筑热数据，而公共数据集和现有数据生成器在数据质量和数量上无法满足迁移学习研究的需要。此外，现有数据生成方法通常需要建筑模拟专业知识。我们提出了BuilDa，一个用于生成高质量和足够数量合成数据的建筑热数据生成框架，以支持迁移学习研究。该框架无需深厚的建筑模拟知识即可生成大量数据。BuilDa使用单区域Modelica模型，该模型被导出为功能mock-up unit (FMU)并在Python中模拟。我们通过生成数据并将其用于预训练和微调迁移学习模型来演示BuilDa。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning (TL) can improve data-driven modeling of building thermaldynamics. Therefore, many new TL research areas emerge in the field, such asselecting the right source model for TL. However, these research directionsrequire massive amounts of thermal building data which is lacking presently.Neither public datasets nor existing data generators meet the needs of TLresearch in terms of data quality and quantity. Moreover, existing datageneration approaches typically require expert knowledge in buildingsimulation. We present BuilDa, a thermal building data generation framework forproducing synthetic data of adequate quality and quantity for TL research. Theframework does not require profound building simulation knowledge to generatelarge volumes of data. BuilDa uses a single-zone Modelica model that isexported as a Functional Mock-up Unit (FMU) and simulated in Python. Wedemonstrate BuilDa by generating data and utilizing it for pretraining andfine-tuning TL models.</description>
      <author>example@mail.com (Thomas Krug, Fabian Raisch, Dominik Aimer, Markus Wirnsberger, Ferdinand Sigg, Benjamin Schäfer, Benjamin Tischler)</author>
      <guid isPermaLink="false">2508.12703v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification</title>
      <link>http://arxiv.org/abs/2508.12418v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 7 figures. Submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Bi-Axial Transformer(BAT)的新型模型，用于处理电子健康记录(EHRs)的分类任务，该模型同时关注临床变量和时间点轴，能够学习更丰富的数据关系并解决数据稀疏性问题。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录(EHRs)作为患者医疗历史的数字表示，是流行病学和临床研究的宝贵资源。然而，EHRs正变得越来越复杂，表现为更大的数据集、更长的时间序列和多模态集成。Transformer模型虽然因其能够建模长距离依赖和并行处理数据而适合处理这些挑战，但在EHR分类中的应用仍受限于数据表示方式，这可能降低性能或无法捕捉有意义的缺失信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理EHR数据复杂性和稀疏性的模型，提高EHR分类任务的性能，特别是在败血症预测和死亡率分类方面。&lt;h4&gt;方法&lt;/h4&gt;提出Bi-Axial Transformer(BAT)，该模型同时关注EHR数据的临床变量和时间点两个轴，学习更丰富的数据关系，并解决数据稀疏性问题。作者还重新实现了基线模型，使用PyTorch框架，使这些模型可在多个存储库中获得。&lt;h4&gt;主要发现&lt;/h4&gt;BAT在败血症预测任务上取得了最先进的性能，在死亡率分类方面与顶尖方法具有竞争力。与其他Transformer相比，BAT对数据缺失具有更强的鲁棒性，并能学习独特的传感器嵌入，可用于迁移学习。&lt;h4&gt;结论&lt;/h4&gt;BAT模型通过双轴注意力机制有效解决了EHR数据中的挑战，提高了分类性能并增强了鲁棒性。重新实现的基线模型为未来研究提供了可复现的基准。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录(EHRs)是患者医疗历史的数字表示，是流行病学和临床研究的宝贵资源。它们也正变得越来越复杂，最近的趋势表明数据集更大、时间序列更长、多模态集成增加。由于在自然语言处理和其他领域的成功，Transformer迅速普及，它们非常适合解决这些挑战，因为它们能够建模长距离依赖和并行处理数据。但它们在EHR分类中的应用仍受限于数据表示，这可能降低性能或无法捕捉有意义的缺失信息。在本文中，我们提出了双轴Transformer(BAT)，它同时关注EHR数据的临床变量和时间点轴，以学习更丰富的数据关系并解决数据稀疏性问题。BAT在败血症预测上取得了最先进的性能，并且在死亡率分类方面与顶尖方法具有竞争力。与其他Transformer相比，BAT对数据缺失表现出更强的鲁棒性，并学习独特的传感器嵌入，可用于迁移学习。之前位于多个存储库或使用已弃用库的基线模型，已使用PyTorch重新实现，可供复现和未来基准测试。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHRs), the digital representation of a patient'smedical history, are a valuable resource for epidemiological and clinicalresearch. They are also becoming increasingly complex, with recent trendsindicating larger datasets, longer time series, and multi-modal integrations.Transformers, which have rapidly gained popularity due to their success innatural language processing and other domains, are well-suited to address thesechallenges due to their ability to model long-range dependencies and processdata in parallel. But their application to EHR classification remains limitedby data representations, which can reduce performance or fail to captureinformative missingness. In this paper, we present the Bi-Axial Transformer(BAT), which attends to both the clinical variable and time point axes of EHRdata to learn richer data relationships and address the difficulties of datasparsity. BAT achieves state-of-the-art performance on sepsis prediction and iscompetitive to top methods for mortality classification. In comparison to othertransformers, BAT demonstrates increased robustness to data missingness, andlearns unique sensor embeddings which can be used in transfer learning.Baseline models, which were previously located across multiple repositories orutilized deprecated libraries, were re-implemented with PyTorch and madeavailable for reproduction and future benchmarking.</description>
      <author>example@mail.com (Rachael DeVries, Casper Christensen, Marie Lisandra Zepeda Mendoza, Ole Winther)</author>
      <guid isPermaLink="false">2508.12418v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Dual-species atomic absorption image reconstruction using deep neural networks</title>
      <link>http://arxiv.org/abs/2508.12120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于深度学习的在线图像补全协议，用于减少双原子系统光学吸收信号中的干涉条纹。&lt;h4&gt;背景&lt;/h4&gt;光学成像在理解被困中性原子行为方面发挥着重要作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来减少双原子系统中光学吸收信号的干涉条纹。&lt;h4&gt;方法&lt;/h4&gt;采用深度学习方法和迁移学习方案，逐步更新先前学习的参数，实现在线图像补全。&lt;h4&gt;主要发现&lt;/h4&gt;该方法对于两种不同的原子物种（6Li和23Na）都能有效抑制干涉条纹，显示出稳健的解决方案。&lt;h4&gt;结论&lt;/h4&gt;提出的在线图像补全方法能够有效适应漂移的实验条件，可以轻松集成到实验室环境中，迁移学习可以加速图像分析。&lt;h4&gt;翻译&lt;/h4&gt;光学成像在理解被困中性原子行为方面发挥着重要作用。在本工作中，我们描述了一种基于深度学习的在线图像补全协议，用于减少双原子系统光学吸收信号中的干涉条纹。无论对于两种不同原子物种（6Li和23Na）的任务性质如何不同，该方法都能显示出抑制条纹的稳健解决方案。为了将其融入日常操作，需要一种迁移学习方案，逐步更新先前学习的参数。我们概述了一种在线图像补全方法，能够有效适应漂移的实验条件。我们的方法可以轻松集成到实验室环境中，迁移学习可以加速图像分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optical imaging plays an instrumental role in understanding the behavior oftrapped neutral atoms. In this work, we describe a deep learning-based onlineimage completion protocol that reduces interference fringes in opticalabsorption signals for a dual-species atomic system. Regardless of the distinctnature of the task for two different atomic species, 6Li and 23Na, the methoddisplays a robust solution for suppressing fringes. To incorporate this intodaily operations, a transfer learning scheme is required that incrementallyupdates the previously learned parameters. We outline an online imagecompletion method that efficiently adapts to drifting experimental conditions.Our method can be easily integrated into lab settings, where transfer learningcan accelerate image analysis.</description>
      <author>example@mail.com (Kyuhwan Lee, Yong-il Shin)</author>
      <guid isPermaLink="false">2508.12120v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Robust Data Fusion via Subsampling</title>
      <link>http://arxiv.org/abs/2508.12048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在数据污染情况下结合子抽样策略的迁移学习方法，以提高有限目标数据集的性能，并通过理论分析和实际应用验证了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;数据融合和迁移学习是快速发展领域，可通过利用相关数据源或任务提升目标人群模型性能，但面临数据异质性和实际集成挑战。&lt;h4&gt;目的&lt;/h4&gt;研究目标数据有限而外部数据量大但受异常值污染的情况，开发结合子抽样策略的迁移学习方法，解决数据污染下的迁移学习问题。&lt;h4&gt;方法&lt;/h4&gt;研究两种子抽样策略（减少偏差和最小化方差），提出结合策略提高估计器性能，提供非渐近误差边界，并通过模拟和A380飞机硬着陆风险分析验证方法。&lt;h4&gt;主要发现&lt;/h4&gt;异常值可能因任意均值偏移偏离真实模型；样本量、信号强度、采样率等因素影响估计器性能；所提方法在模拟中表现优越；健壮迁移学习可提高稀有类型数据的估计效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的结合子抽样策略的迁移学习方法能有效处理数据污染问题，提高估计器性能，并成功应用于稀有飞机类型的风险分析。&lt;h4&gt;翻译&lt;/h4&gt;数据融合和迁移学习是快速发展的领域，通过利用其他相关数据源或任务来提高目标人群的模型性能。挑战在于目标与外部数据之间各种潜在的异质性，以及阻止简单数据集成的实际问题。我们考虑一个现实场景，其中目标数据规模有限，而外部数据量大但被异常值污染；这种数据污染以及其他计算和操作约束，需要适当选择或对迁移学习的外部数据进行子抽样。据我们所知，数据污染下的迁移学习和子抽样尚未得到充分研究。我们通过研究使用外部数据子样本的各种迁移学习方法来解决这个问题，考虑了由于任意均值偏移而偏离底层真实模型的异常值。研究了两种子抽样策略：一种旨在减少偏差，另一种旨在最小化方差。还引入了结合这些策略的方法，以提高估计器的性能。我们提供了迁移学习估计器的非渐近误差边界，阐明了样本量、信号强度、采样率、异常值大小和模型误差分布尾部行为等因素的作用。广泛的模拟显示了所提出方法的优越性能。此外，我们通过利用其他飞机类型的数据，将方法应用于分析A380飞机的硬着陆风险，证明健壮的迁移学习可以在借助其他飞机类型数据的情况下，提高相对罕见飞机类型的估计效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data fusion and transfer learning are rapidly growing fields that enhancemodel performance for a target population by leveraging other related datasources or tasks. The challenges lie in the various potential heterogeneitiesbetween the target and external data, as well as various practical concernsthat prevent a na\"ive data integration. We consider a realistic scenario wherethe target data is limited in size while the external data is large butcontaminated with outliers; such data contamination, along with othercomputational and operational constraints, necessitates proper selection orsubsampling of the external data for transfer learning. To ourknowledge,transfer learning and subsampling under data contamination have notbeen thoroughly investigated. We address this gap by studying various transferlearning methods with subsamples of the external data, accounting for outliersdeviating from the underlying true model due to arbitrary mean shifts. Twosubsampling strategies are investigated: one aimed at reducing biases and theother at minimizing variances. Approaches to combine these strategies are alsointroduced to enhance the performance of the estimators. We providenon-asymptotic error bounds for the transfer learning estimators, clarifyingthe roles of sample sizes, signal strength, sampling rates, magnitude ofoutliers, and tail behaviors of model error distributions, among other factors.Extensive simulations show the superior performance of the proposed methods.Additionally, we apply our methods to analyze the risk of hard landings in A380airplanes by utilizing data from other airplane types,demonstrating that robusttransfer learning can improve estimation efficiency for relatively rareairplane types with the help of data from other types of airplanes.</description>
      <author>example@mail.com (Jing Wang, HaiYing Wang, Kun Chen)</author>
      <guid isPermaLink="false">2508.12048v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</title>
      <link>http://arxiv.org/abs/2508.11584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 6 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Visual Perception Engine (VPEngine)框架，用于在资源受限的机器人平台上高效执行多个视觉感知任务，通过共享基础模型主干和并行处理任务特定模型头，显著提高了GPU利用率和计算效率。&lt;h4&gt;背景&lt;/h4&gt;在资源受限的机器人平台上部署多个机器学习模型进行不同感知任务时，常面临冗余计算、大内存占用和复杂集成挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一个模块化框架，实现视觉多任务的高效GPU使用，同时保持可扩展性和开发者可访问性。&lt;h4&gt;方法&lt;/h4&gt;VPEngine采用共享基础模型主干提取图像表示，实现高效共享且无需不必要的GPU-CPU内存传输，同时运行多个并行的专用任务特定模型头。示例实现使用DINOv2作为基础模型，并实现深度、目标检测和语义分割等多个任务头。&lt;h4&gt;主要发现&lt;/h4&gt;与顺序执行相比，框架实现高达3倍的加速；基于CUDA多进程服务(MPS)，提供高效GPU利用率并保持恒定内存占用；支持在运行时动态调整每个任务的推理频率；在NVIDIA Jetson Orin AGX上，对TensorRT优化模型的端到端实时性能达到大于或等于50 Hz。&lt;h4&gt;结论&lt;/h4&gt;VPEngine框架用Python编写，是开源的，具有ROS2 C++绑定，便于机器人社区在不同平台上使用；在NVIDIA Jetson Orin AGX上，对TensorRT优化模型的端到端实时性能达到大于或等于50 Hz。&lt;h4&gt;翻译&lt;/h4&gt;在资源受限的机器人平台上为不同的感知任务部署多个机器学习模型通常会导致冗余计算、大内存占用和复杂的集成挑战。作为回应，这项工作提出了视觉感知引擎(VPEngine)，一个模块化框架，旨在实现视觉多任务的高效GPU使用，同时保持可扩展性和开发者可访问性。我们的框架架构利用共享的基础模型主干来提取图像表示，这些表示可以高效共享，无需不必要的GPU-CPU内存传输，同时运行多个并行的专用任务特定模型头。这种设计消除了部署传统顺序模型时特征提取组件中固有的计算冗余，同时允许根据应用需求进行动态任务优先级排序。我们使用DINOv2作为基础模型并实现多个任务（深度、目标检测和语义分割）头的示例实现，与顺序执行相比实现了高达3倍的加速。基于CUDA多进程服务(MPS)，VPEngine提供高效的GPU利用率，同时保持恒定的内存占用，并允许在运行时动态调整每个任务的推理频率。该框架用Python编写，是开源的，并具有ROS2 C++ (Humble)绑定，便于机器人社区在不同机器人平台上使用。我们的示例实现表明，对于TensorRT优化的模型，在NVIDIA Jetson Orin AGX上端到端实时性能达到大于或等于50 Hz。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying multiple machine learning models on resource-constrained roboticplatforms for different perception tasks often results in redundantcomputations, large memory footprints, and complex integration challenges. Inresponse, this work presents Visual Perception Engine (VPEngine), a modularframework designed to enable efficient GPU usage for visual multitasking whilemaintaining extensibility and developer accessibility. Our frameworkarchitecture leverages a shared foundation model backbone that extracts imagerepresentations, which are efficiently shared, without any unnecessary GPU-CPUmemory transfers, across multiple specialized task-specific model heads runningin parallel. This design eliminates the computational redundancy inherent infeature extraction component when deploying traditional sequential models whileenabling dynamic task prioritization based on application demands. Wedemonstrate our framework's capabilities through an example implementationusing DINOv2 as the foundation model with multiple task (depth, objectdetection and semantic segmentation) heads, achieving up to 3x speedup comparedto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngineoffers efficient GPU utilization and maintains a constant memory footprintwhile allowing per-task inference frequencies to be adjusted dynamically duringruntime. The framework is written in Python and is open source with ROS2 C++(Humble) bindings for ease of use by the robotics community across diverserobotic platforms. Our example implementation demonstrates end-to-end real-timeperformance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimizedmodels.</description>
      <author>example@mail.com (Jakub Łucki, Jonathan Becktor, Georgios Georgakis, Robert Royce, Shehryar Khattak)</author>
      <guid isPermaLink="false">2508.11584v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
  <item>
      <title>Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</title>
      <link>http://arxiv.org/abs/2508.11452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Our platform is publicly accessible at  https://doraemon.alipay.com/model-ranking&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Inclusion Arena，一个基于真实应用场景中人类反馈的动态排行榜，用于评估和排序大型语言模型。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和多模态大型语言模型已展现出接近人类水平的性能，但现有评估基准和排行榜主要依赖静态数据集或众包通用领域提示，无法充分反映实际应用场景中的性能。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有评估方法的局限性，作者提出了Inclusion Arena，一个从AI驱动应用中直接收集人类反馈的实时排行榜，以更准确地反映模型在实际应用中的表现。&lt;h4&gt;方法&lt;/h4&gt;将成对模型比较集成到自然用户交互中；使用Bradley-Terry模型进行稳健的模型排序；引入Placement Matches（冷启动机制）和Proximity Sampling（智能比较策略）两项创新技术。&lt;h4&gt;主要发现&lt;/h4&gt;Inclusion Arena产生可靠且稳定的排名；与通用众包数据集相比，具有更高的数据传递性；显著降低了恶意操纵的风险。&lt;h4&gt;结论&lt;/h4&gt;Inclusion Arena通过促进基础模型和实际应用之间的开放联盟，旨在加速真正针对实际、以用户为中心的部署而优化的大型语言模型和多模态大型语言模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）和多模态大型语言模型（MLLMs）已经开启了一个AI能力的新时代，在各种场景中展现出接近人类水平的性能。虽然已经提出了许多基准（如MMLU）和排行榜（如Chatbot Arena）来帮助推动LLMs和MLLMs的发展，但大多数都依赖于静态数据集或众包通用领域提示，往往无法反映实际应用中的性能。为了弥合这一关键差距，我们提出了Inclusion Arena，一个基于从AI驱动应用中直接收集的人类反馈的实时排行榜。我们的平台将成对模型比较集成到自然用户交互中，确保评估反映实际使用场景。为了稳健的模型排序，我们采用了Bradley-Terry模型，并增加了两项关键创新：(1) Placement Matches，一种冷启动机制，用于快速为新集成的模型估计初始评分；(2) Proximity Sampling，一种智能比较策略，优先选择能力相近的模型之间进行比较，以最大化信息增益并提高评分稳定性。大量的实证分析和模拟表明，Inclusion Arena产生可靠且稳定的排名，与通用众包数据集相比具有更高的数据传递性，并显著降低了恶意操纵的风险。通过促进基础模型和实际应用之间的开放联盟，Inclusion Arena旨在加速真正针对实际、以用户为中心的部署而优化的大型语言模型和多模态大型语言模型的发展。该平台可在https://doraemon.alipay.com/model-ranking公开访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)have ushered in a new era of AI capabilities, demonstrating near-human-levelperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve thedevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourcedgeneral-domain prompts, often falling short of reflecting performance inreal-world applications. To bridge this critical gap, we present InclusionArena, a live leaderboard that ranks models based on human feedback collecteddirectly from AI-powered applications. Our platform integrates pairwise modelcomparisons into natural user interactions, ensuring evaluations reflectpractical usage scenarios. For robust model ranking, we employ theBradley-Terry model augmented with two key innovations: (1) Placement Matches,a cold-start mechanism to quickly estimate initial ratings for newly integratedmodels, and (2) Proximity Sampling, an intelligent comparison strategy thatprioritizes battles between models of similar capabilities to maximizeinformation gain and enhance rating stability. Extensive empirical analyses andsimulations demonstrate that Inclusion Arena yields reliable and stablerankings, exhibits higher data transitivity compared to general crowdsourceddatasets, and significantly mitigates the risk of malicious manipulation. Byfostering an open alliance between foundation models and real-worldapplications, Inclusion Arena aims to accelerate the development of LLMs andMLLMs truly optimized for practical, user-centric deployments. The platform ispublicly accessible at https://doraemon.alipay.com/model-ranking.</description>
      <author>example@mail.com (Kangyu Wang, Hongliang He, Lin Liu, Ruiqi Liang, Zhenzhong Lan, Jianguo Li)</author>
      <guid isPermaLink="false">2508.11452v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging the RETFound foundation model for optic disc segmentation in retinal images</title>
      <link>http://arxiv.org/abs/2508.11354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RETFound是一个知名的基础模型，首次被适应用于视盘分割任务。该系统在使用少量任务特定示例训练后，性能优于最先进的特定分割基线网络，在多个数据集上实现了约96%的Dice系数。&lt;h4&gt;背景&lt;/h4&gt;RETFound是为眼底相机和光学相干断层扫描图像开发的基础模型，在多个数据集上显示出从视网膜图像诊断眼部疾病和系统性疾病的良好性能，但据作者所知，尚未被用于其他任务。&lt;h4&gt;目的&lt;/h4&gt;首次将RETFound模型适应于视盘分割，这是视网膜图像分析中普遍存在的基础任务。&lt;h4&gt;方法&lt;/h4&gt;通过训练一个仅包含少量任务特定示例的头部，将RETFound模型适应于视盘分割任务，并在四个公开数据集（IDRID、Drishti-GS、RIM-ONE-r3和REFUGE）和一个私有数据集（GoDARTS）上进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;1) 分割系统性能优于最先进的特定分割基线网络；2) 在所有测试数据集上Dice系数约为96%；3) 在内部验证、领域泛化和领域适应方面表现优异；4) 超过了大多数最先进的基线结果。&lt;h4&gt;结论&lt;/h4&gt;研究讨论了基础模型作为任务特定架构替代品的辩论框架下的结果，代码将在论文接受后提供链接。&lt;h4&gt;翻译&lt;/h4&gt;RETFound是一个知名的基础模型(FM)，专为眼底相机和光学相干断层扫描图像而开发。它在多个数据集上显示出从视网膜图像诊断眼部疾病和系统性疾病的良好性能。然而，据我们所知，它尚未被用于其他任务。我们首次将RETFound适应于视盘分割，这是视网膜图像分析中的一个普遍存在的基础任务。所得的分割系统在使用非常少量的任务特定示例训练头部后，性能优于最先进的特定分割基线网络。我们报告并讨论了四个公共数据集（IDRID、Drishti-GS、RIM-ONE-r3和REFUGE）和一个私有数据集（GoDARTS）的结果，在所有数据集上Dice系数约为96%。总体而言，我们的方法在内部验证、领域泛化和领域适应方面获得了优异的性能，并超过了大多数最先进的基线结果。我们在基础模型作为任务特定架构替代品的辩论框架下讨论了结果。代码可在以下网址获取：[论文接受后将添加链接]&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; RETFound is a well-known foundation model (FM) developed for fundus cameraand optical coherence tomography images. It has shown promising performanceacross multiple datasets in diagnosing diseases, both eye-specific andsystemic, from retinal images. However, to our best knowledge, it has not beenused for other tasks. We present the first adaptation of RETFound for opticdisc segmentation, a ubiquitous and foundational task in retinal imageanalysis. The resulting segmentation system outperforms state-of-the-art,segmentation-specific baseline networks after training a head with only a verymodest number of task-specific examples. We report and discuss results withfour public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a privatedataset, GoDARTS, achieving about 96% Dice consistently across all datasets.Overall, our method obtains excellent performance in internal verification,domain generalization and domain adaptation, and exceeds most of thestate-of-the-art baseline results. We discuss the results in the framework ofthe debate about FMs as alternatives to task-specific architectures. The codeis available at: [link to be added after the paper is accepted]</description>
      <author>example@mail.com (Zhenyi Zhao, Muthu Rama Krishnan Mookiah, Emanuele Trucco)</author>
      <guid isPermaLink="false">2508.11354v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>LLM Compression: How Far Can We Go in Balancing Size and Performance?</title>
      <link>http://arxiv.org/abs/2508.11318v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for presentation at the RANLP 2025  conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究应用4位组缩放量化和生成式预训练变换器量化技术到三种不同规模的模型上，评估了它们在多个自然语言处理任务上的影响，并分析了模型压缩与任务性能之间的权衡。&lt;h4&gt;背景&lt;/h4&gt;量化是一种通过减少内存使用和计算成本来提高大型语言模型可访问性的重要且流行的技术，同时保持模型性能。&lt;h4&gt;目的&lt;/h4&gt;评估量化技术在不同规模语言模型上的效果，分析低比特量化在实际部署中的适用性，为用户提供基于需求做出合适决策的参考。&lt;h4&gt;方法&lt;/h4&gt;将4位组缩放量化和生成式预训练变换器量化技术应用于LLaMA 1B、Qwen 0.5B和PHI 1.5B模型，并在MS MARCO、BoolQ和GSM8K数据集上进行基准测试，评估准确性和效率。&lt;h4&gt;主要发现&lt;/h4&gt;研究测量了模型压缩与任务性能之间的权衡，分析了准确率、推理延迟和吞吐量等关键评估指标，比较了两种量化技术在不同规模模型上的优缺点。&lt;h4&gt;结论&lt;/h4&gt;研究结果可以帮助用户根据需要满足的规格做出合适的决策，同时为未来的量化实验提供了基准参考。&lt;h4&gt;翻译&lt;/h4&gt;量化是一种通过减少内存使用和计算成本来提高大型语言模型(LLMs)可访问性的重要且流行的技术，同时保持性能。在本研究中，我们将4位组缩放量化(GSQ)和生成式预训练变换器量化(GPTQ)应用于LLaMA 1B、Qwen 0.5B和PHI 1.5B模型，评估它们在多个NLP任务上的影响。我们在MS MARCO(信息检索)、BoolQ(布尔问答)和GSM8K(数学推理)数据集上对这些模型进行基准测试，评估各种任务上的准确性和效率。研究测量了模型压缩与任务性能之间的权衡，分析了关键评估指标，即准确率、推理延迟和吞吐量(每秒生成的总输出令牌数)，提供了关于低比特量化在实际部署中适用性的见解。利用这些结果，用户可以基于需要满足的规格做出合适的决策。我们讨论了GSQ和GPTQ技术在不同规模模型上的优缺点，这也为未来的实验提供了基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantization is an essential and popular technique for improving theaccessibility of large language models (LLMs) by reducing memory usage andcomputational costs while maintaining performance. In this study, we apply4-bit Group Scaling Quantization (GSQ) and Generative Pretrained TransformerQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating theirimpact across multiple NLP tasks. We benchmark these models on MS MARCO(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K(Mathematical Reasoning) datasets, assessing both accuracy and efficiencyacross various tasks. The study measures the trade-offs between modelcompression and task performance, analyzing key evaluation metrics, namelyaccuracy, inference latency, and throughput (total output tokens generated persecond), providing insights into the suitability of low-bit quantization forreal-world deployment. Using the results, users can then make suitabledecisions based on the specifications that need to be met. We discuss the prosand cons of GSQ and GPTQ techniques on models of different sizes, which alsoserve as a benchmark for future experiments.</description>
      <author>example@mail.com (Sahil Sk, Debasish Dhal, Sonal Khosla, Sk Shahid, Sambit Shekhar, Akash Dhaka, Shantipriya Parida, Dilip K. Prasad, Ondřej Bojar)</author>
      <guid isPermaLink="false">2508.11318v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception</title>
      <link>http://arxiv.org/abs/2508.11256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2505.04410&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出DeCLIP框架，通过解耦CLIP的自注意力模块获取内容和上下文特征，并分别增强这些特征，以改进开放词汇密集感知任务的性能。&lt;h4&gt;背景&lt;/h4&gt;密集视觉感知任务受限于预定义类别，难以应用于现实世界中无限视觉概念的场景。虽然CLIP等视觉语言模型在开放词汇任务有潜力，但直接应用于密集感知时表现不佳，因局部特征表示有限。&lt;h4&gt;目的&lt;/h4&gt;解决CLIP在密集感知任务中局部特征表示不足和空间一致性差的问题，建立开放词汇密集感知基础框架。&lt;h4&gt;方法&lt;/h4&gt;提出DeCLIP框架，解耦自注意力模块获取内容和上下文特征：1) 上下文特征通过联合蒸馏视觉基础模型语义相关性和扩散模型对象完整性线索增强空间一致性；2) 内容特征与图像裁剪表示对齐，并通过视觉基础模型区域相关性约束提高局部判别性。&lt;h4&gt;主要发现&lt;/h4&gt;CLIP的图像标记难以有效聚合空间或语义相关区域信息，导致特征缺乏局部判别性和空间一致性。&lt;h4&gt;结论&lt;/h4&gt;DeCLIP为开放词汇密集感知奠定基础，在2D检测和分割、3D实例分割、视频实例分割和6D物体姿态估计等多项任务中持续达到最先进性能。&lt;h4&gt;翻译&lt;/h4&gt;密集视觉感知任务一直受限于其对预定义类别的依赖，这限制了它们在现实世界场景中的应用，因为现实世界中的视觉概念是无限的。虽然像CLIP这样的视觉语言模型在开放词汇任务中显示出潜力，但它们直接应用于密集感知任务时往往表现不佳，这是由于局部特征表示的局限性。在这项工作中，我们提出了一个观察：CLIP的图像标记难以有效聚合来自空间或语义相关区域的信息，导致特征缺乏局部判别性和空间一致性。为了解决这个问题，我们提出了DeCLIP，一个通过解耦自注意力模块分别获取'内容'和'上下文'特征的新型框架。上下文特征通过联合蒸馏视觉基础模型的语义相关性和扩散模型的对象完整性线索来增强，从而提高空间一致性。同时，内容特征与图像裁剪表示对齐，并通过视觉基础模型的区域相关性进行约束，以提高局部判别性。大量实验表明，DeCLIP为开放词汇密集感知奠定了坚实基础，在2D检测和分割、3D实例分割、视频实例分割和6D物体姿态估计等一系列任务中持续取得最先进的性能。代码可在https://github.com/xiaomoguhz/DeCLIP获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决密集视觉感知任务受限于预定义类别的问题，导致它们无法处理现实世界中无限的视觉概念。这个问题很重要，因为传统方法只能识别固定类别的物体，而现实场景中的视觉概念是无限的，直接应用CLIP等视觉语言模型到密集感知任务时表现不佳，限制了它们在自动驾驶、机器人等实际应用中的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析CLIP和视觉基础模型的注意力图，发现CLIP的图像标记在深层无法有效聚合语义相关信息，出现了'代理标记'现象。他们尝试同时进行自蒸馏和VFM蒸馏但效果不佳，推断空间相关性和视觉-语言对齐存在优化冲突。设计方法借鉴了CLIP的预训练优势、VFM的语义关联能力、SD模型的边界捕捉能力，以及自蒸馏的区域对齐方法，但创新性地将这些元素通过解耦策略结合起来。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将CLIP的自注意力模块解耦为'内容'和'上下文'特征，分别优化局部判别能力和空间一致性。流程包括：1)解耦注意力机制分离两种特征；2)上下文特征通过VFM的语义亲和图和SD的自我注意力图增强；3)内容特征通过区域与对应[CLS]标记对齐；4)添加区域相关性约束防止特征退化；5)组合两种特征的损失函数进行整体优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出解耦CLIP自注意力为内容特征和上下文特征；2)结合VFM和SD模型的优势进行多模型协同蒸馏；3)提出无监督微调框架，无需额外密集标注；4)方法适用于多种开放词汇密集感知任务。相比之前工作，DeCLIP专注于增强CLIP本身而非将其集成到其他模型中，不仅适用于区域级任务还能扩展到像素级分割，且避免了优化冲突问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DeCLIP通过解耦学习策略分离CLIP的自注意力特征，并协同视觉基础模型和扩散模型进行增强，显著提升了CLIP在开放词汇密集感知任务中的性能，为多种视觉任务提供了通用基础模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense visual perception tasks have been constrained by their reliance onpredefined categories, limiting their applicability in real-world scenarioswhere visual concepts are unbounded. While Vision-Language Models (VLMs) likeCLIP have shown promise in open-vocabulary tasks, their direct application todense perception often leads to suboptimal performance due to limitations inlocal feature representation. In this work, we present our observation thatCLIP's image tokens struggle to effectively aggregate information fromspatially or semantically related regions, resulting in features that lacklocal discriminability and spatial consistency. To address this issue, wepropose DeCLIP, a novel framework that enhances CLIP by decoupling theself-attention module to obtain ``content'' and ``context'' featuresrespectively. \revise{The context features are enhanced by jointly distillingsemantic correlations from Vision Foundation Models (VFMs) and object integritycues from diffusion models, thereby enhancing spatial consistency. In parallel,the content features are aligned with image crop representations andconstrained by region correlations from VFMs to improve local discriminability.Extensive experiments demonstrate that DeCLIP establishes a solid foundationfor open-vocabulary dense perception, consistently achieving state-of-the-artperformance across a broad spectrum of tasks, including 2D detection andsegmentation, 3D instance segmentation, video instance segmentation, and 6Dobject pose estimation.} Code is available athttps://github.com/xiaomoguhz/DeCLIP</description>
      <author>example@mail.com (Junjie Wang, Keyu Chen, Yulin Li, Bin Chen, Hengshuang Zhao, Xiaojuan Qi, Zhuotao Tian)</author>
      <guid isPermaLink="false">2508.11256v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images</title>
      <link>http://arxiv.org/abs/2508.11167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Manuscript submitted to IEEE TGRS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种视觉基础引导的检测Transformer(VG-DETR)，用于解决遥感图像中的无源目标检测问题，通过集成视觉基础模型来减轻伪标签噪声并提高特征提取能力。&lt;h4&gt;背景&lt;/h4&gt;无监督域适应方法在实际遥感场景中因隐私和传输限制无法访问源域数据而应用受限。无源目标检测(SFOD)虽不依赖源数据，但经常因噪声伪标签导致训练崩溃，特别是在具有密集目标和复杂背景的遥感图像中。&lt;h4&gt;目的&lt;/h4&gt;开发一种半监督框架下的VG-DETR模型，利用少量标记的目标数据和视觉基础模型来改善无源遥感检测性能，解决伪标签噪声问题并增强特征表示的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1) 引入VFM引导的伪标签挖掘策略，利用VFM的语义先验评估伪标签可靠性；2) 通过从低置信度输出恢复可能正确的预测提高伪标签质量；3) 提出双层VFM引导对齐方法，在实例和图像级别对齐检测器特征与VFM嵌入；4) 利用细粒度原型对比学习和特征图相似性匹配增强特征表示鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;VG-DETR通过集成视觉基础模型和创新的伪标签策略，有效减轻了伪标签噪声问题，提高了特征提取能力，并在无源遥感检测任务中取得了优越性能。&lt;h4&gt;结论&lt;/h4&gt;VG-DETR为遥感图像中的无源目标检测提供了一种有效解决方案，通过视觉基础模型的引导，成功克服了传统方法在隐私受限环境下的局限性，同时提高了检测精度和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;无监督域适应方法已被广泛探索用于弥合域差距。然而，在实际遥感场景中，隐私和传输限制通常无法访问源域数据，限制了它们的实际适用性。最近，无源目标检测(SFOD)作为一种有前途的替代方案出现，旨在不依赖源数据进行跨域适应，主要通过自训练范式。尽管有潜力，SFOD经常因噪声伪标签导致训练崩溃，特别是在具有密集目标和复杂背景的遥感图像中。考虑到在实际中有限的目标域标注通常是可行的，我们提出了一种视觉基础引导的检测Transformer(VG-DETR)，建立在半监督框架上，用于遥感图像中的SFOD。VG-DETR以'免费午餐'方式将视觉基础模型(VFM)集成到训练流程中，利用少量标记的目标数据来减轻伪标签噪声，同时提高检测器的特征提取能力。具体来说，我们引入了一种VFM引导的伪标签挖掘策略，利用VFM的语义先验来进一步评估生成伪标签的可靠性。通过从低置信度输出中恢复可能正确的预测，我们的策略提高了伪标签的质量和数量。此外，提出了一种双层VFM引导的对齐方法，在实例和图像级别上将检测器特征与VFM嵌入对齐。通过细粒度原型之间的对比学习和特征图之间的相似性匹配，这种双层对齐进一步增强了对域差距的特征表示的鲁棒性。大量实验表明，VG-DETR在无源遥感检测任务中取得了优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised domain adaptation methods have been widely explored to bridgedomain gaps. However, in real-world remote-sensing scenarios, privacy andtransmission constraints often preclude access to source domain data, whichlimits their practical applicability. Recently, Source-Free Object Detection(SFOD) has emerged as a promising alternative, aiming at cross-domainadaptation without relying on source data, primarily through a self-trainingparadigm. Despite its potential, SFOD frequently suffers from training collapsecaused by noisy pseudo-labels, especially in remote sensing imagery with denseobjects and complex backgrounds. Considering that limited target domainannotations are often feasible in practice, we propose a Visionfoundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervisedframework for SFOD in remote sensing images. VG-DETR integrates a VisionFoundation Model (VFM) into the training pipeline in a "free lunch" manner,leveraging a small amount of labeled target data to mitigate pseudo-label noisewhile improving the detector's feature-extraction capability. Specifically, weintroduce a VFM-guided pseudo-label mining strategy that leverages the VFM'ssemantic priors to further assess the reliability of the generatedpseudo-labels. By recovering potentially correct predictions fromlow-confidence outputs, our strategy improves pseudo-label quality andquantity. In addition, a dual-level VFM-guided alignment method is proposed,which aligns detector features with VFM embeddings at both the instance andimage levels. Through contrastive learning among fine-grained prototypes andsimilarity matching between feature maps, this dual-level alignment furtherenhances the robustness of feature representations against domain gaps.Extensive experiments demonstrate that VG-DETR achieves superior performance insource-free remote sensing detection tasks.</description>
      <author>example@mail.com (Jianhong Han, Yupei Wang, Liang Chen)</author>
      <guid isPermaLink="false">2508.11167v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback</title>
      <link>http://arxiv.org/abs/2508.11453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EvoPSF的在线进化框架，用于提高自动驾驶系统在未知环境中的适应能力和规划性能。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统已从模块化流水线发展到端到端架构，但大多数现有方法是在离线状态下训练的，缺乏在部署过程中适应新环境的机制，导致在面对真实驾驶场景中未见过的变化时泛化能力下降。&lt;h4&gt;目的&lt;/h4&gt;打破传统的'一次训练，永远部署'模式，提出一种基于规划状态反馈的在线进化框架，提高自动驾驶系统对环境变化的适应能力。&lt;h4&gt;方法&lt;/h4&gt;将规划者不确定性视为在线进化的触发器，利用规划者的智能体-智能体注意力识别关键对象，通过比较预测路径与实际位置计算自监督损失，并将该损失反向传播以在线适应模型。&lt;h4&gt;主要发现&lt;/h4&gt;EvoPSF方法提高了模型对环境变化的鲁棒性，实现了更精确的运动预测，从而实现更准确和稳定的规划行为。&lt;h4&gt;结论&lt;/h4&gt;EvoPSF是一种有效的在线进化框架，能够在具有挑战性的条件下持续改善自动驾驶系统的规划性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，自动驾驶领域取得了显著进展，系统已从模块化流水线发展为端到端架构。然而，大多数现有方法是在离线状态下训练的，缺乏在部署过程中适应新环境的机制。因此，当面对真实驾驶场景中未见过的变化时，它们的泛化能力会下降。在本文中，我们打破了传统的'一次训练，永远部署'模式，提出了EvoPSF，一种基于规划状态反馈的自动驾驶在线进化框架。我们认为，规划失败主要由对象级运动预测不准确引起，而这种失败通常表现为规划者不确定性的增加。为此，我们将规划者不确定性视为在线进化的触发器，用作诊断信号来启动有针对性的模型更新。我们不是盲目更新，而是利用规划者的智能体-智能体注意力来识别智能车辆最关注的具体对象，这些对象主要负责规划失败。对于这些关键对象，我们通过比较预测模块的预测路径与感知模块输出中高置信度分数的实际未来位置，计算有针对性的自监督损失。然后将该损失反向传播以在线适应模型。因此，我们的方法提高了模型对环境变化的鲁棒性，实现了更精确的运动预测，从而实现更准确和稳定的规划行为。在nuScenes数据集的跨区域和损坏变体上的实验表明，EvoPSF在具有挑战性的条件下持续改善了规划性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have witnessed remarkable progress in autonomous driving, withsystems evolving from modular pipelines to end-to-end architectures. However,most existing methods are trained offline and lack mechanisms to adapt to newenvironments during deployment. As a result, their generalization abilitydiminishes when faced with unseen variations in real-world driving scenarios.In this paper, we break away from the conventional "train once, deploy forever"paradigm and propose EvoPSF, a novel online Evolution framework for autonomousdriving based on Planning-State Feedback. We argue that planning failures areprimarily caused by inaccurate object-level motion predictions, and suchfailures are often reflected in the form of increased planner uncertainty. Toaddress this, we treat planner uncertainty as a trigger for online evolution,using it as a diagnostic signal to initiate targeted model updates. Rather thanperforming blind updates, we leverage the planner's agent-agent attention toidentify the specific objects that the ego vehicle attends to most, which areprimarily responsible for the planning failures. For these critical objects, wecompute a targeted self-supervised loss by comparing their predicted waypointsfrom the prediction module with their actual future positions, selected fromthe perception module's outputs with high confidence scores. This loss is thenbackpropagated to adapt the model online. As a result, our method improves themodel's robustness to environmental changes, leads to more precise motionpredictions, and therefore enables more accurate and stable planning behaviors.Experiments on both cross-region and corrupted variants of the nuScenes datasetdemonstrate that EvoPSF consistently improves planning performance underchallenging conditions.</description>
      <author>example@mail.com (Jiayue Jin, Lang Qian, Jingyu Zhang, Chuanyu Ju, Liang Song)</author>
      <guid isPermaLink="false">2508.11453v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.11428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ImagiDrive的新型端到端自动驾驶框架，整合视觉语言模型和驾驶世界模型，形成统一的想象和规划循环，以提高自动驾驶的安全性和效率。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶需要丰富的上下文理解和精确的预测推理能力来安全导航动态复杂的环境。视觉语言模型(VLMs)和驾驶世界模型(DWMs)作为解决这一挑战的强大方法已经出现，但它们各自独立发展，缺乏有效整合。&lt;h4&gt;目的&lt;/h4&gt;整合VLMs和DWMs，利用它们在准确行为预测和真实场景生成方面的互补优势，解决将行动级决策与高保真像素级预测有效连接并保持计算效率的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出ImagiDrive框架，将基于VLM的驾驶代理与基于DWM的场景想象器整合，形成一个统一的想象和规划循环。驾驶代理基于多模态输入预测初始驾驶轨迹，引导场景想象器生成相应的未来场景，这些想象场景随后用于迭代优化驾驶代理的规划决策。同时引入提前停止机制和轨迹选择策略解决效率和预测准确性问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过在nuScenes和NAVSIM数据集上的大量实验验证，ImagiDrive在开环和闭环条件下都比之前的替代方案具有更好的鲁棒性和优越性。提前停止机制和轨迹选择策略有效解决了整合中的效率和预测准确性挑战。&lt;h4&gt;结论&lt;/h4&gt;ImagiDrive框架通过整合VLM和DWM的优势，为自动驾驶提供了一个统一、高效的解决方案，能够更好地理解和预测复杂动态环境，提高自动驾驶的安全性和效率。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶需要丰富的上下文理解和精确的预测推理能力，以便在动态复杂的环境中安全导航。视觉语言模型(VLMs)和驾驶世界模型(DWMs)已经独立出现，作为解决这一挑战的不同方面的强大方法。VLMs通过理解多模态上下文提供可解释性和稳健的动作预测，而DWMs擅长生成详细的、合理的未来驾驶场景，这对于主动规划至关重要。将VLMs与DWMs整合是一种直观、有前景但尚未被充分研究的策略，可以利用准确行为预测和真实场景生成的互补优势。然而，这种整合也带来了显著挑战，特别是在有效连接行动级决策与高保真像素级预测以及保持计算效率方面。在本文中，我们提出了ImagiDrive，一种新颖的端到端自动驾驶框架，它将基于VLM的驾驶代理与基于DWM的场景想象器整合，形成一个统一的想象和规划循环。驾驶代理基于多模态输入预测初始驾驶轨迹，引导场景想象器生成相应的未来场景。这些想象场景随后用于迭代优化驾驶代理的规划决策。为了解决这种集成中固有的效率和预测准确性挑战，我们引入了提前停止机制和轨迹选择策略。在nuScenes和NAVSIM数据集上的大量实验验证表明，ImagiDrive在开环和闭环条件下都比之前的替代方案具有鲁棒性和优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将视觉语言模型(VLMs)和驾驶世界模型(DWMs)有效集成，以提升自动驾驶系统的安全性和决策能力。这个问题很重要，因为自动驾驶需要在复杂动态环境中做出安全决策，这需要丰富的上下文理解和预测能力。VLMs擅长理解和解释场景但缺乏对未来场景的详细生成能力，而DWMs能够生成合理的未来场景但在行为预测方面存在不足。将两者结合可以互补优势，提高自动驾驶系统在复杂场景中的安全性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到VLMs和DWMs各自的优势和局限性，意识到将两者结合可以互补优势。他们设计了一个循环框架，让驾驶代理预测轨迹，场景想象器生成未来场景，然后反馈给代理进行迭代优化。为了提高效率，还引入了早期停止机制和轨迹选择策略。该方法借鉴了现有工作：基于现有的VLM架构（如LLaVA和InternVL系列），利用了现有的DWM（如Vista和Epona）作为场景想象器，并受到世界模型在预测未来场景演化方面的启发。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个'想象-规划'循环，让系统能够基于当前场景预测未来，并根据预测的未来场景迭代优化驾驶决策。整体实现流程包括：1)初始阶段：驾驶代理基于当前帧预测初始轨迹；2)想象阶段：场景想象器基于历史帧和初始轨迹生成未来场景；3)选择阶段：从生成的未来场景中选择关键帧；4)反馈阶段：将关键帧反馈给驾驶代理，进行轨迹优化；5)迭代阶段：重复2-4步，直到达到最大迭代次数或满足早期停止条件；6)最终选择阶段：使用轨迹选择策略选择最佳轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了统一的想象和规划框架，将VLM和DWM紧密结合；2)设计了想象-规划循环机制，使系统能够基于预测的未来场景迭代优化决策；3)引入了早期停止机制和轨迹选择策略，提高了效率和安全性；4)实现了灵活的多模态输入和结构化输出，支持不同类型的VLM和DWM。相比之前的工作，ImagiDrive不同于仅使用VLM或DWM的单一方法，也不同于简单串联两者的方法，而是实现了迭代优化的循环机制；它同时关注效率和安全性，实现了端到端的想象和规划框架，在复杂场景中表现出更好的安全性和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ImagiDrive通过将视觉语言模型与驾驶世界模型有机结合，在一个统一的想象-规划循环中实现了更安全、更高效的自动驾驶决策系统。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving requires rich contextual comprehension and precisepredictive reasoning to navigate dynamic and complex environments safely.Vision-Language Models (VLMs) and Driving World Models (DWMs) haveindependently emerged as powerful recipes addressing different aspects of thischallenge. VLMs provide interpretability and robust action prediction throughtheir ability to understand multi-modal context, while DWMs excel in generatingdetailed and plausible future driving scenarios essential for proactiveplanning. Integrating VLMs with DWMs is an intuitive, promising, yetunderstudied strategy to exploit the complementary strengths of accuratebehavioral prediction and realistic scene generation. Nevertheless, thisintegration presents notable challenges, particularly in effectively connectingaction-level decisions with high-fidelity pixel-level predictions andmaintaining computational efficiency. In this paper, we propose ImagiDrive, anovel end-to-end autonomous driving framework that integrates a VLM-baseddriving agent with a DWM-based scene imaginer to form a unifiedimagination-and-planning loop. The driving agent predicts initial drivingtrajectories based on multi-modal inputs, guiding the scene imaginer togenerate corresponding future scenarios. These imagined scenarios aresubsequently utilized to iteratively refine the driving agent's planningdecisions. To address efficiency and predictive accuracy challenges inherent inthis integration, we introduce an early stopping mechanism and a trajectoryselection strategy. Extensive experimental validation on the nuScenes andNAVSIM datasets demonstrates the robustness and superiority of ImagiDrive overprevious alternatives under both open-loop and closed-loop conditions.</description>
      <author>example@mail.com (Jingyu Li, Bozhou Zhang, Xin Jin, Jiankang Deng, Xiatian Zhu, Li Zhang)</author>
      <guid isPermaLink="false">2508.11428v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring</title>
      <link>http://arxiv.org/abs/2508.11482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对建筑行业视觉数据集进行了系统性综述，收集分析了2005-2024年间51个公开可用的视觉数据集，创建了OpenConstruction开源目录，并提出基于FAIR原则的未来数据基础设施路线图，以支持建筑行业数据驱动解决方案发展。&lt;h4&gt;背景&lt;/h4&gt;建筑行业越来越依赖视觉数据来支持人工智能(AI)和机器学习(ML)应用进行现场监控。高质量、领域特定的数据集（包括图像、视频和点云）捕捉现场几何和时空动态，包括对象、工人和材料的位置和交互。然而，现有资源在大小、数据模态、注释质量和真实建筑条件代表性方面差异很大。&lt;h4&gt;目的&lt;/h4&gt;解决现有建筑数据集缺乏系统性分类的问题，使社区能够完全理解数据集景观，识别关键差距，并为未来更有效、可靠和可扩展的建筑AI应用指明方向。&lt;h4&gt;方法&lt;/h4&gt;研究团队广泛搜索了学术数据库和开放数据平台，收集了2005-2024年期间的51个公开可用的视觉数据集。这些数据集使用结构化数据模式进行分类，包括：(i)数据基础（如大小和许可），(ii)数据模态（如RGB和点云），(iii)注释框架（如边界框），和(iv)下游应用领域（如进度跟踪）。&lt;h4&gt;主要发现&lt;/h4&gt;研究将这些发现综合成一个名为OpenConstruction的开源目录，支持数据驱动的方法开发。此外，研究讨论了现有建筑数据集景观中的几个关键局限性，并提出了基于可发现性、可访问性、互操作性和可重用性(FAIR)原则的未来数据基础设施路线图。&lt;h4&gt;结论&lt;/h4&gt;通过审查当前景观和概述战略优先事项，该研究支持建筑行业数据驱动解决方案的发展。&lt;h4&gt;翻译&lt;/h4&gt;建筑行业越来越依赖视觉数据来支持人工智能(AI)和机器学习(ML)应用进行现场监控。高质量、领域特定的数据集，包括图像、视频和点云，捕捉现场几何和时空动态，包括对象、工人和材料的位置和交互。然而，尽管对利用视觉数据集的兴趣日益增长，现有资源在大小、数据模态、注释质量和真实建筑条件代表性方面差异很大。缺乏对这些数据集特征和应用背景进行分类的系统性综述，限制了社区完全理解数据集景观、识别关键差距以及为未来更有效、可靠和可扩展的建筑AI应用指明方向的能力。为解决这一差距，本研究广泛搜索了学术数据库和开放数据平台，获得了2005-2024年期间的51个公开可用的视觉数据集。这些数据集使用涵盖(i)数据基础（如大小和许可），(ii)数据模态（如RGB和点云），(iii)注释框架（如边界框），和(iv)下游应用领域（如进度跟踪）的结构化数据模式进行分类。本研究将这些发现综合成一个名为OpenConstruction的开源目录，支持数据驱动的方法开发。此外，研究讨论了现有建筑数据集景观中的几个关键局限性，并提出了基于可发现性、可访问性、互操作性和可重用性(FAIR)原则的未来数据基础设施路线图。通过审查当前景观和概述战略优先事项，该研究支持建筑行业数据驱动解决方案的发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决建筑行业中开放视觉数据集碎片化、缺乏系统性分类和评估的问题。这个问题很重要，因为高质量、领域特定的视觉数据集对于AI和机器学习在建筑监控中的应用至关重要，而现有数据集在大小、模态、标注质量和代表性方面差异很大，缺乏集中索引和标准化文档，限制了研究人员有效利用这些资源，阻碍了建筑行业AI应用的可靠性和可扩展性发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统化的多步骤搜索策略设计方法，包括制定针对性的布尔搜索字符串，在学术数据库和公共数据平台进行搜索。他们设定了明确的纳入标准：开放可访问性、建筑监控相关性、AI/ML就绪标注和科学文档记录。该方法借鉴了现有数据集表征框架，并采用FAIR原则（可发现性、可访问性、互操作性、可重用性）设计未来数据基础设施路线图，体现了对建筑监控领域现有研究的继承和发展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统性综述和分类建筑监控领域的开放视觉数据集，创建一个全面的数据集景观，识别关键差距，并提供未来发展的战略指导。整体流程包括：1）识别阶段 - 在学术数据库和开放平台搜索相关数据集；2）筛选阶段 - 应用纳入标准筛选数据集；3）分析阶段 - 使用结构化数据架构对51个选定数据集进行分类；4）组织阶段 - 创建OpenConstruction开源目录；5）讨论阶段 - 分析现有局限并提出未来路线图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首次对建筑监控领域开放视觉数据集进行全面系统性综述；2）开发结构化数据架构从四个维度对数据集进行分类；3）创建OpenConstruction开源目录提供集中化数据集索引；4）明确指出现有数据集的关键局限性和差距；5）提出基于FAIR原则的未来数据发展路线图。相比之前工作，本研究提供了首个集中化的数据集目录，全面评估了现有数据集的局限性，并首次将FAIR原则系统应用于建筑监控数据集发展规划，填补了领域空白。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性综述51个开放视觉数据集并创建OpenConstruction开源目录，为建筑监控领域的数据中心AI应用提供了全面的基础设施和基于FAIR原则的未来发展指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The construction industry increasingly relies on visual data to supportArtificial Intelligence (AI) and Machine Learning (ML) applications for sitemonitoring. High-quality, domain-specific datasets, comprising images, videos,and point clouds, capture site geometry and spatiotemporal dynamics, includingthe location and interaction of objects, workers, and materials. However,despite growing interest in leveraging visual datasets, existing resources varywidely in sizes, data modalities, annotation quality, and representativeness ofreal-world construction conditions. A systematic review to categorize theirdata characteristics and application contexts is still lacking, limiting thecommunity's ability to fully understand the dataset landscape, identifycritical gaps, and guide future directions toward more effective, reliable, andscalable AI applications in construction. To address this gap, this studyconducts an extensive search of academic databases and open-data platforms,yielding 51 publicly available visual datasets that span the 2005-2024 period.These datasets are categorized using a structured data schema covering (i) datafundamentals (e.g., size and license), (ii) data modalities (e.g., RGB andpoint cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)downstream application domains (e.g., progress tracking). This studysynthesizes these findings into an open-source catalog, OpenConstruction,supporting data-driven method development. Furthermore, the study discussesseveral critical limitations in the existing construction dataset landscape andpresents a roadmap for future data infrastructure anchored in the Findability,Accessibility, Interoperability, and Reusability (FAIR) principles. Byreviewing the current landscape and outlining strategic priorities, this studysupports the advancement of data-centric solutions in the construction sector.</description>
      <author>example@mail.com (Ruoxin Xiong, Yanyu Wang, Jiannan Cai, Kaijian Liu, Yuansheng Zhu, Pingbo Tang, Nora El-Gohary)</author>
      <guid isPermaLink="false">2508.11482v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Optimizing ROS 2 Communication for Wireless Robotic Systems</title>
      <link>http://arxiv.org/abs/2508.11366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对ROS 2中大型载荷无线传输的性能瓶颈问题，进行了深入的网络层分析，并提出了一种轻量级DDS优化框架，有效解决了IP分片、重传时机和缓冲区突发等问题，在各种无线场景中表现出色。&lt;h4&gt;背景&lt;/h4&gt;ROS 2中大型载荷（如高分辨率图像和LiDAR点云）的无线传输是一个主要瓶颈，默认的DDS通信堆栈在丢无线链路上表现出显著的性能下降。&lt;h4&gt;目的&lt;/h4&gt;探究ROS 2无线通信挑战的根本原因，并提出优化解决方案。&lt;h4&gt;方法&lt;/h4&gt;首次对ROS 2的DDS堆栈在无线条件下处理大型载荷的情况进行深入的网络层分析，并基于发现的问题提出轻量级且完全兼容的DDS优化框架。&lt;h4&gt;主要发现&lt;/h4&gt;识别出三个关键问题：过度的IP分片、低效的重传时机和拥塞缓冲区突发。&lt;h4&gt;结论&lt;/h4&gt;提出的优化框架可以在现有DDS模式失败的条件下成功传输大型载荷，同时保持低端到端延迟，且无需协议修改、额外组件和集成工作。&lt;h4&gt;翻译&lt;/h4&gt;无线传输大型载荷，如高分辨率图像和LiDAR点云，是ROS 2（领先的开源机器人中间件）中的一个主要瓶颈。ROS 2中默认的数据分发服务（DDS）通信堆栈在丢无线链路上表现出显著的性能下降。尽管ROS 2被广泛使用，但这些无线通信挑战的根本原因仍未被探索。在本文中，我们首次对ROS 2的DDS堆栈在无线条件下处理大型载荷的情况进行了深入的网络层分析。我们确定了以下三个关键问题：过度的IP分片、低效的重传时机和拥塞缓冲区突发。为解决这些问题，我们提出了一个轻量级且完全兼容的DDS优化框架，根据链路和载荷特性调整通信参数。我们的解决方案可以通过基于XML的QoS配置，通过标准ROS 2应用程序接口无缝应用，无需协议修改、额外组件和几乎无需集成工作。在各种无线场景中的广泛实验表明，我们的框架在现有DDS模式失败的条件下成功传输大型载荷，同时保持低端到端延迟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wireless transmission of large payloads, such as high-resolution images andLiDAR point clouds, is a major bottleneck in ROS 2, the leading open-sourcerobotics middleware. The default Data Distribution Service (DDS) communicationstack in ROS 2 exhibits significant performance degradation over lossy wirelesslinks. Despite the widespread use of ROS 2, the underlying causes of thesewireless communication challenges remain unexplored. In this paper, we presentthe first in-depth network-layer analysis of ROS 2's DDS stack under wirelessconditions with large payloads. We identify the following three key issues:excessive IP fragmentation, inefficient retransmission timing, and congestivebuffer bursts. To address these issues, we propose a lightweight and fullycompatible DDS optimization framework that tunes communication parameters basedon link and payload characteristics. Our solution can be seamlessly appliedthrough the standard ROS 2 application interface via simple XML-based QoSconfiguration, requiring no protocol modifications, no additional components,and virtually no integration efforts. Extensive experiments across variouswireless scenarios demonstrate that our framework successfully delivers largepayloads in conditions where existing DDS modes fail, while maintaining lowend-to-end latency.</description>
      <author>example@mail.com (Sanghoon Lee, Taehun Kim, Jiyeong Chae, Kyung-Joon Park)</author>
      <guid isPermaLink="false">2508.11366v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds</title>
      <link>http://arxiv.org/abs/2508.11265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  to be published in International Conference on Computer Vision, ICCV  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对3D分割中的领域泛化挑战，提出了一种类别级别几何学习框架，通过类别级别几何嵌入和几何一致性学习来提取领域不变的几何特征，有效提高了模型在新环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;3D分割中的领域泛化是将模型部署到未见环境时的关键挑战。当前方法通过增强点云的数据分布来缓解领域差异，但现有模型学习点云的全局几何模式，忽略了类别级别的分布和对齐。&lt;h4&gt;目的&lt;/h4&gt;提出一个类别级别几何学习框架，探索领域不变的几何特征，用于领域泛化的3D语义分割。&lt;h4&gt;方法&lt;/h4&gt;提出了两个核心组件：1) 类别级别几何嵌入（CGE）来感知点云特征的细粒度几何属性，构建每个类别的几何属性，并将几何嵌入与语义学习耦合；2) 几何一致性学习（GCL）来模拟潜在的3D分布，对齐类别级别的几何嵌入，使模型专注于几何不变信息以提高泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果验证了所提方法的有效性，与最先进的领域泛化点云方法相比，具有非常有竞争力的分割精度。&lt;h4&gt;结论&lt;/h4&gt;通过关注类别级别的几何特征和对齐，所提出的类别级别几何学习框架能够有效解决3D分割中的领域泛化问题，提高模型在新环境中的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;3D分割中的领域泛化是将模型部署到未见环境时的关键挑战。当前方法通过增强点云的数据分布来缓解领域差异。然而，模型学习点云的全局几何模式，同时忽略了类别级别的分布和对齐。本文提出了一个类别级别几何学习框架，用于探索领域不变的几何特征，以实现领域泛化的3D语义分割。具体而言，提出了类别级别几何嵌入（CGE）来感知点云特征的细粒度几何属性，构建每个类别的几何属性，并将几何嵌入与语义学习耦合。其次，提出了几何一致性学习（GCL）来模拟潜在的3D分布并对齐类别级别的几何嵌入，使模型能够专注于几何不变信息以提高泛化能力。实验结果验证了所提方法的有效性，与最先进的领域泛化点云方法相比，具有非常有竞争力的分割精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云分割中的领域泛化问题，即如何让模型在未见过的环境中保持良好的分割性能。这个问题在现实中非常重要，因为点云数据广泛应用于自动驾驶、医疗分析和机器人导航等领域，而这些应用场景中，数据采集条件、传感器类型和环境条件经常变化，导致训练数据和测试数据分布不一致，严重影响模型在实际应用中的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法发现，虽然数据增强可以缓解领域差异，但模型在学习点云的全局几何模式时，忽略了类别级别的分布和对齐问题。作者借鉴了PointDR、DGUIL等数据增强方法和领域泛化技术，创新性地提出从几何角度解决领域泛化问题。他们设计了两个关键组件：类别级几何嵌入(CGE)来感知每个类别的细粒度几何属性，以及几何一致性学习(GCL)来模拟潜在3D分布并对齐几何特征，从而学习领域不变的几何表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习领域不变的几何特征来提高3D点云语义分割的泛化能力。整体流程包括：1)从源域点云中提取原始特征；2)通过类别级几何嵌入(CGE)构建每个类别的几何属性，并将几何嵌入与语义学习耦合；3)通过几何一致性学习(GCL)模拟物质积累和模糊识别等潜在3D分布，并对齐原始特征和模拟特征的几何嵌入；4)结合分割损失、几何属性学习损失和几何一致性学习损失进行模型训练，使模型专注于学习类别内的几何不变信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出类别级几何学习框架，探索3D点云中领域不变的几何特征；2)类别级几何嵌入(CGE)构建每个类别的几何属性而非全局几何模式；3)几何一致性学习(GCL)模拟潜在3D分布并对齐几何嵌入。相比之前的工作，本文方法首次从几何角度解决点云分割的领域泛化问题，关注类别级别的几何分布而非仅全局分布，特别重视几何特征中的领域不变信息，而不仅仅是数据增强。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种类别级几何学习方法，通过构建类别级几何嵌入和对齐不同领域的几何特征，显著提高了3D点云语义分割在未知领域的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain generalization in 3D segmentation is a critical challenge in deployingmodels to unseen environments. Current methods mitigate the domain shift byaugmenting the data distribution of point clouds. However, the model learnsglobal geometric patterns in point clouds while ignoring the category-leveldistribution and alignment. In this paper, a category-level geometry learningframework is proposed to explore the domain-invariant geometric features fordomain generalized 3D semantic segmentation. Specifically, Category-levelGeometry Embedding (CGE) is proposed to perceive the fine-grained geometricproperties of point cloud features, which constructs the geometric propertiesof each class and couples geometric embedding to semantic learning. Secondly,Geometric Consistent Learning (GCL) is proposed to simulate the latent 3Ddistribution and align the category-level geometric embeddings, allowing themodel to focus on the geometric invariant information to improvegeneralization. Experimental results verify the effectiveness of the proposedmethod, which has very competitive segmentation accuracy compared with thestate-of-the-art domain generalized point cloud methods.</description>
      <author>example@mail.com (Pei He, Lingling Li, Licheng Jiao, Ronghua Shang, Fang Liu, Shuang Wang, Xu Liu, Wenping Ma)</author>
      <guid isPermaLink="false">2508.11265v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies</title>
      <link>http://arxiv.org/abs/2508.11513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GraphOracle是一种新型自解释GNN框架，专门设计用于生成和评估GNN的类别级解释，解决了现有方法在类别级解释上的局限性，实现了更好的保真度、可解释性和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;增强图神经网络(GNNs)的可解释性对其安全公平部署至关重要。近期工作引入了自解释GNN，在训练过程中生成解释，提高了保真度和效率。其中一些模型如ProtGNN和PGIB学习类别特定的原型，为类别级解释提供了可能，但其评估仅关注实例级解释。&lt;h4&gt;目的&lt;/h4&gt;引入GraphOracle，一种新型自解释GNN框架，旨在为GNN生成和评估类别级解释，解决现有方法无法验证原型在同一类别不同实例间泛化能力的问题。&lt;h4&gt;方法&lt;/h4&gt;模型联合学习GNN分类器和一组具有区分性的、结构化的稀疏子图，用于每个类别。提出了一种新的集成训练方法，有效且忠实地捕捉图-子图-预测依赖关系，并通过基于掩码的评估策略进行验证。使用熵正则化的子图选择和轻量级随机游走提取，避免了计算瓶颈。&lt;h4&gt;主要发现&lt;/h4&gt;通过掩码评估策略验证，先前的方法如ProtGNN和PGIB不能提供有效的类别级解释。相比之下，GraphOracle在一系列图分类任务中实现了更好的保真度、可解释性和可扩展性，且训练速度更快、更可扩展。&lt;h4&gt;结论&lt;/h4&gt;GraphOracle为GNN中忠实的类别级自解释提供了实用且原则性的解决方案，克服了现有方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;增强图神经网络(GNNs)的可解释性对其安全公平部署至关重要。近期工作引入了自解释GNN，在训练过程中生成解释，提高了保真度和效率。其中一些模型，如ProtGNN和PGIB，学习类别特定的原型，为类别级解释提供了可能。然而，它们的评估仅关注实例级解释，留下了一个开放问题：这些原型是否真正能在同一类别的不同实例间有意义地泛化。在本文中，我们引入了GraphOracle，一种新型自解释GNN框架，旨在为GNN生成和评估类别级解释。我们的模型联合学习GNN分类器和一组具有区分性的、结构化稀疏子图，每个类别对应一个。我们提出了一种新的集成训练方法，有效且忠实地捕捉图-子图-预测依赖关系，并通过基于掩码的评估策略进行验证。该策略使我们能够回顾性评估先前的方法如ProtGNN和PGIB是否能提供有效的类别级解释。结果表明它们不能。相比之下，GraphOracle在一系列图分类任务中实现了更好的保真度、可解释性和可扩展性。我们进一步证明，GraphOracle通过使用熵正则化的子图选择和轻量级随机游走提取，避免了先前方法(如蒙特卡洛树搜索)的计算瓶颈，实现了更快、更可扩展的训练。这些研究成果将GraphOracle定位为GNN中忠实的类别级自解释的一种实用且原则性的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enhancing the interpretability of graph neural networks (GNNs) is crucial toensure their safe and fair deployment. Recent work has introducedself-explainable GNNs that generate explanations as part of training, improvingboth faithfulness and efficiency. Some of these models, such as ProtGNN andPGIB, learn class-specific prototypes, offering a potential pathway towardclass-level explanations. However, their evaluations focus solely oninstance-level explanations, leaving open the question of whether theseprototypes meaningfully generalize across instances of the same class. In thispaper, we introduce GraphOracle, a novel self-explainable GNN frameworkdesigned to generate and evaluate class-level explanations for GNNs. Our modeljointly learns a GNN classifier and a set of structured, sparse subgraphs thatare discriminative for each class. We propose a novel integrated training thatcaptures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependenciesefficiently and faithfully, validated through a masking-based evaluationstrategy. This strategy enables us to retroactively assess whether priormethods like ProtGNN and PGIB deliver effective class-level explanations. Ourresults show that they do not. In contrast, GraphOracle achieves superiorfidelity, explainability, and scalability across a range of graphclassification tasks. We further demonstrate that GraphOracle avoids thecomputational bottlenecks of previous methods$\unicode{x2014}$like Monte CarloTree Search$\unicode{x2014}$by using entropy-regularized subgraph selection andlightweight random walk extraction, enabling faster and more scalable training.These findings position GraphOracle as a practical and principled solution forfaithful class-level self-explainability in GNNs.</description>
      <author>example@mail.com (Fanzhen Liu, Xiaoxiao Ma, Jian Yang, Alsharif Abuadbba, Kristen Moore, Surya Nepal, Cecile Paris, Quan Z. Sheng, Jia Wu)</author>
      <guid isPermaLink="false">2508.11513v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity</title>
      <link>http://arxiv.org/abs/2508.11436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为mCOCO（多感官认知计算）的新框架，利用储层计算从BOLD信号中学习群体水平的功能性连接脑模板(CBT)，提高可解释性并整合认知能力。&lt;h4&gt;背景&lt;/h4&gt;连接脑模板的生成受到广泛关注，但现有方法（传统机器学习和图神经网络）存在三大局限：可解释性差、计算成本高、仅关注结构和拓扑而忽略认知能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有CBT学习方法的局限性，开发一种提高可解释性、降低计算成本并整合认知能力的新框架。&lt;h4&gt;方法&lt;/h4&gt;提出mCOCO框架，利用储层计算从BOLD信号学习CBT，分为两个阶段：1)将BOLD信号映射到储层推导个体功能连接组并聚合成群体CBT；2)通过认知储层整合多感官输入（文本、音频、视觉数据）赋予CBT认知特征。&lt;h4&gt;主要发现&lt;/h4&gt;基于mCOCO的模板在中心性、判别性、拓扑合理性和多感官记忆保留方面显著优于基于图神经网络的CBT。&lt;h4&gt;结论&lt;/h4&gt;mCOCO框架提供了一种高效、可解释的方法生成具有认知能力的连接脑模板，能更好地捕捉脑功能连接的本质。&lt;h4&gt;翻译&lt;/h4&gt;连接脑模板(CBT)的生成最近引起了广泛关注，因为它有潜力识别个体间共享的独特连接模式。然而，现有的CBT学习方法（如传统机器学习和图神经网络）受到几个限制的阻碍。这些包括：(i)由于黑盒性质导致的可解释性差，(ii)高计算成本，以及(iii)仅关注结构和拓扑，忽略了所生成CBT的认知能力。为了应对这些挑战，我们引入了mCOCO（多感官认知计算），这是一个新颖的框架，利用储层计算(RC)从BOLD（血氧水平依赖）信号中学习群体水平的功能性CBT。储层计算的动态系统特性允许跟踪状态随时间的变化，提高了可解释性并实现了类脑动力学的建模，正如先前文献所证明的那样。通过整合多感官输入（如文本、音频和视觉数据），mCOCO不仅捕捉结构和拓扑，还捕捉脑区域如何处理信息和适应认知任务（如感官处理），所有这些都以计算高效的方式实现。我们的mCOCO框架包括两个阶段：(1)将BOLD信号映射到储层中，推导出个体功能连接组，然后聚合成群体水平CBT - 据我们所知，这种方法在功能连接研究中尚未被探索过 - (2)通过认知储层整合多感官输入，赋予CBT认知特征。广泛的评估表明，我们基于mCOCO的模板在中心性、判别性、拓扑合理性和多感官记忆保留方面显著优于基于图神经网络的CBT。我们的源代码可在https://github.com/basiralab/mCOCO获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generation of connectional brain templates (CBTs) has recently garneredsignificant attention for its potential to identify unique connectivitypatterns shared across individuals. However, existing methods for CBT learningsuch as conventional machine learning and graph neural networks (GNNs) arehindered by several limitations. These include: (i) poor interpretability dueto their black-box nature, (ii) high computational cost, and (iii) an exclusivefocus on structure and topology, overlooking the cognitive capacity of thegenerated CBT. To address these challenges, we introduce mCOCO (multi-sensoryCOgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)to learn population-level functional CBT from BOLD(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allowfor tracking state changes over time, enhancing interpretability and enablingthe modeling of brain-like dynamics, as demonstrated in prior literature. Byintegrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCOcaptures not only structure and topology but also how brain regions processinformation and adapt to cognitive tasks such as sensory processing, all in acomputationally efficient manner. Our mCOCO framework consists of two phases:(1) mapping BOLD signals into the reservoir to derive individual functionalconnectomes, which are then aggregated into a group-level CBT - an approach, tothe best of our knowledge, not previously explored in functional connectivitystudies - and (2) incorporating multi-sensory inputs through a cognitivereservoir, endowing the CBT with cognitive traits. Extensive evaluations showthat our mCOCO-based template significantly outperforms GNN-based CBT in termsof centeredness, discriminativeness, topological soundness, and multi-sensorymemory retention. Our source code is available athttps://github.com/basiralab/mCOCO.</description>
      <author>example@mail.com (Mayssa Soussia, Mohamed Ali Mahjoub, Islem Rekik)</author>
      <guid isPermaLink="false">2508.11436v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting</title>
      <link>http://arxiv.org/abs/2508.11390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图神经网络在从关系数据中学习方面非常有效，但许多真实世界的系统表现出需要高阶拓扑域表示的复杂交互。本研究提出基于Forman-Ricci曲率的结构提升策略，揭示图的骨干结构，解决信息在长距离传递中的失真问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络能有效学习关系数据，利用节点和边特征同时保持图结构的对称性。然而，社交或生物网络等真实系统表现出更复杂的交互，需要更高阶的拓扑表示。&lt;h4&gt;目的&lt;/h4&gt;介绍几何和拓扑深度学习领域，通过引入利用高阶结构的方法来解决图神经网络处理复杂交互的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出基于Forman-Ricci曲率的结构提升策略，利用曲率揭示图的局部和全局特性，识别网络的骨干结构，并用超边表示社区间的连接。&lt;h4&gt;主要发现&lt;/h4&gt;曲率能够揭示图的骨干结构，这些结构形成主要社区之间的连接，最适合用超边表示，以建模网络中远距离集群之间的信息流。&lt;h4&gt;结论&lt;/h4&gt;该方法解决了信息在长距离传递和图瓶颈中的失真问题，这种现象在图学习中被称为'过度压缩'。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在从关系数据中学习方面非常有效，能够利用节点和边特征同时保持图结构的固有对称性。然而，许多真实世界的系统，如社交或生物网络，表现出更复杂的交互，这些交互更适合用高阶拓扑域表示。新兴的几何和拓扑深度学习领域通过引入利用高阶结构的方法来解决这一挑战。拓扑深度学习的核心是提升的概念，它将数据表示从基本图形式转换为更富表现力的拓扑结构，然后再应用图神经网络模型进行学习。在本工作中，我们提出了一种基于Forman-Ricci曲率的结构提升策略，该曲率基于黎曼几何定义了基于边的网络特征。曲率揭示了图的局部和全局特性，如网络的骨干结构，即保持结构的粗粒度图几何，这些几何形成主要社区之间的连接，最适合用超边表示，以建模网络中远距离集群之间的信息流。为此，我们的方法解决了信息在长距离传递和图瓶颈中的失真问题，这一现象在图学习中被称为过度压缩。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks are highly effective at learning from relational data,leveraging node and edge features while maintaining the symmetries inherent tograph structures. However, many real-world systems, such as social orbiological networks, exhibit complex interactions that are more naturallyrepresented by higher-order topological domains. The emerging field ofGeometric and Topological Deep Learning addresses this challenge by introducingmethods that utilize and benefit from higher-order structures. Central to TDLis the concept of lifting, which transforms data representations from basicgraph forms to more expressive topologies before the application of GNN modelsfor learning. In this work, we propose a structural lifting strategy usingForman-Ricci curvature, which defines an edge-based network characteristicbased on Riemannian geometry. Curvature reveals local and global properties ofa graph, such as a network's backbones, i.e. coarse, structure-preserving graphgeometries that form connections between major communities - most suitablyrepresented as hyperedges to model information flows between clusters acrosslarge distances in the network. To this end, our approach provides a remedy tothe problem of information distortion in message passing across long distancesand graph bottlenecks - a phenomenon known in graph learning as over-squashing.</description>
      <author>example@mail.com (Michael Banf, Dominik Filipiak, Max Schattauer, Liliya Imasheva)</author>
      <guid isPermaLink="false">2508.11390v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis</title>
      <link>http://arxiv.org/abs/2508.11375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AnatoMaskGAN是一种新的医学语义掩码合成框架，通过嵌入切片相关空间特征、引入多样化图像增强策略和优化深度特征学习，解决了现有GAN方法在复杂医学图像中缺乏空间一致性的问题。&lt;h4&gt;背景&lt;/h4&gt;医学语义掩码合成可以增强数据扩充和分析，但目前大多数基于GAN的方法仍生成一对一图像，在复杂扫描中缺乏空间一致性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的合成框架AnatoMaskGAN，解决现有方法在复杂医学图像处理中的局限性，提高重建精度和感知质量。&lt;h4&gt;方法&lt;/h4&gt;1) 嵌入切片相关空间特征以精确聚合切片间上下文依赖关系；2) 设计基于GNN的强相关切片特征融合模块建模切片间空间关系；3) 引入三维空间噪声注入策略增强结构多样性建模；4) 结合灰度-纹理分类器优化生成过程中的灰度分布和纹理表示。&lt;h4&gt;主要发现&lt;/h4&gt;在L2R-OASIS数据集上PSNR达到26.50 dB，比当前最先进水平高0.43 dB；在L2R-Abdomen CT上SSIM达到0.8602，比最佳模型提高0.48个百分点；消融研究证实每个组件都对性能有显著贡献。&lt;h4&gt;结论&lt;/h4&gt;AnatoMaskGAN在重建精度和感知质量方面表现出优越性，每个核心设计在增强重建质量方面都具有独立价值。&lt;h4&gt;翻译&lt;/h4&gt;医学语义掩码合成增强了数据扩充和分析，但大多数基于GAN的方法仍然生成一对一图像，并在复杂扫描中缺乏空间一致性。为解决这一问题，我们提出了AnatoMaskGAN，一种新的合成框架，它嵌入切片相关的空间特征以精确聚合切片间的上下文依赖关系，引入多样化的图像增强策略，并优化深度特征学习以提高复杂医学图像的性能。具体而言，我们设计了一个基于GNN的强相关切片特征融合模块，用于建模切片间的空间关系并集成相邻切片的上下文信息，从而更全面地捕捉解剖细节；我们引入了三维空间噪声注入策略，对空间特征进行加权并融合噪声以增强结构多样性的建模；我们结合了灰度-纹理分类器，在生成过程中优化灰度分布和纹理表示。在公开的L2R-OASIS和L2R-Abdomen CT数据集上的大量实验表明，AnatoMaskGAN将L2R-OASIS上的PSNR提高到26.50 dB（比当前最先进水平高0.43 dB），并在L2R-Abdomen CT上实现了0.8602的SSIM——比最佳模型提高了0.48个百分点，证明了其在重建精度和感知质量方面的优越性。逐步移除切片特征融合模块、空间三维噪声注入策略和灰度-纹理分类器的消融研究表明，每个组件都对PSNR、SSIM和LPIPS有显著贡献，进一步证实了每个核心设计在增强重建精度和感知质量方面的独立价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学语义掩码合成中GAN方法的一对一映射问题，导致生成的医学图像在复杂扫描中缺乏空间一致性。这个问题很重要，因为医学成像数据获取成本高、隐私保护要求严格，且专家标注劳动密集，导致数据稀缺，限制了医疗AI模型的性能和泛化能力。有效的数据扩充方法对于突破医疗AI瓶颈至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有GAN方法（如SPADE、DAGAN、OASIS等）的局限性，特别是它们在处理多切片医学数据时无法保持空间一致性的问题。基于这些分析，作者设计了三个核心组件：1) 基于GNN的切片特征融合模块，2) 三维空间噪声注入策略，3) 灰度-纹理分类器。作者借鉴了SPADE作为骨干网络，同时吸收了OASIS的3D噪声张量概念，但通过GNN和灰度-纹理分类器增强了它们在建模解剖连续性和细节方面的能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过图神经网络建模切片间的空间关系来捕获解剖结构的连续性，使用三维空间噪声增强模型对多样化解剖结构的表示能力，并通过灰度-纹理分类器优化生成图像的视觉质量。整体流程为：1) 编码器提取语义特征；2) 三维空间噪声注入模块对齐噪声与物理坐标；3) GNN切片融合模块建模切片间依赖关系；4) SPADE调制的解码器生成灰度体积；5) 多尺度判别器和灰度-纹理分类器评估生成质量并提供反馈。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三个：1) GNN切片特征融合模块，通过构建切片间空间邻接图并使用GNN融合特征，捕获解剖连续性；2) 三维空间噪声注入策略，在空间域融合加权噪声增强结构多样性；3) 灰度-纹理分类器，优化灰度分布和纹理表示。相比之前工作，AnatoMaskGAN解决了SPADE难以捕获长距离依赖的问题，超越了DAGAN对掩码几何布局的依赖，改进了OASIS在细粒度结构渲染上的不足，并通过GNN和噪声注入增强了SAFM的解剖建模能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AnatoMaskGAN通过结合图神经网络驱动的切片特征融合、三维空间噪声注入和灰度-纹理分类器，显著提高了医学语义图像合成的空间一致性、解剖准确性和纹理真实感，在有限数据条件下实现了优于现有方法的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical semantic-mask synthesis boosts data augmentation and analysis, yetmost GAN-based approaches still produce one-to-one images and lack spatialconsistency in complex scans. To address this, we propose AnatoMaskGAN, a novelsynthesis framework that embeds slice-related spatial features to preciselyaggregate inter-slice contextual dependencies, introduces diverseimage-augmentation strategies, and optimizes deep feature learning to improveperformance on complex medical images. Specifically, we design a GNN-basedstrongly correlated slice-feature fusion module to model spatial relationshipsbetween slices and integrate contextual information from neighboring slices,thereby capturing anatomical details more comprehensively; we introduce athree-dimensional spatial noise-injection strategy that weights and fusesspatial features with noise to enhance modeling of structural diversity; and weincorporate a grayscale-texture classifier to optimize grayscale distributionand texture representation during generation. Extensive experiments on thepublic L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNRon L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) andachieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain overthe best model, demonstrating its superiority in reconstruction accuracy andperceptual quality. Ablation studies that successively remove the slice-featurefusion module, spatial 3D noise-injection strategy, and grayscale-textureclassifier reveal that each component contributes significantly to PSNR, SSIM,and LPIPS, further confirming the independent value of each core design inenhancing reconstruction accuracy and perceptual quality.</description>
      <author>example@mail.com (Zonglin Wu, Yule Xue, Qianxiang Hu, Yaoyao Feng, Yuqi Ma, Shanxiong Chen)</author>
      <guid isPermaLink="false">2508.11375v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Diffusion via Generalized Opinion Dynamics</title>
      <link>http://arxiv.org/abs/2508.11249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了GODNF，一种通用的观点动力学神经框架，用于解决现有扩散图神经网络的局限性&lt;h4&gt;背景&lt;/h4&gt;扩散图神经网络(GNNs)正受到越来越多的关注，这些方法基于GNN中的消息传递机制与物理扩散过程之间的联系&lt;h4&gt;目的&lt;/h4&gt;解决现有扩散图神经网络的三个关键局限性：依赖同质扩散和静态动力学、深度受限、收敛行为理论理解有限&lt;h4&gt;方法&lt;/h4&gt;提出GODNF框架，将多种观点动力学模型统一为可训练的扩散机制，通过节点特定行为建模和动态邻域影响捕获异质扩散模式和时态动力学，确保深层高效且可解释的消息传播&lt;h4&gt;主要发现&lt;/h4&gt;理论分析证明GODNF能够建模不同的收敛配置，经验评估显示其在节点分类和影响估计任务中优于最先进的GNNs&lt;h4&gt;结论&lt;/h4&gt;GODNF是一种创新的扩散图神经网络框架，能够克服现有方法的局限性&lt;h4&gt;翻译&lt;/h4&gt;近年来，开发基于扩散的图神经网络(GNNs)引起了越来越多的关注，这些研究建立在GNN中的消息传递机制与物理扩散过程之间的联系之上。然而，现有方法存在三个关键局限性：(1)它们依赖于具有静态动力学的同质扩散，限制了适应不同图结构的能力；(2)它们的深度受到计算开销和可解释性降低的限制；(3)对其收敛行为的理论理解仍然有限。为解决这些挑战，我们提出了GODNF，一种通用的观点动力学神经框架，将多种观点动力学模型统一为一个原则性的、可训练的扩散机制。我们的框架通过节点特定行为建模和动态邻域影响来捕获异质扩散模式和时态动力学，同时确保即使在深层也能实现高效和可解释的消息传播。我们提供了严格的理论分析，证明了GODNF建模不同收敛配置的能力。在节点分类和影响估计任务上的广泛经验评估证实了GODNF优于最先进的GNNs。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There has been a growing interest in developing diffusion-based Graph NeuralNetworks (GNNs), building on the connections between message passing mechanismsin GNNs and physical diffusion processes. However, existing methods suffer fromthree critical limitations: (1) they rely on homogeneous diffusion with staticdynamics, limiting adaptability to diverse graph structures; (2) their depth isconstrained by computational overhead and diminishing interpretability; and (3)theoretical understanding of their convergence behavior remains limited. Toaddress these challenges, we propose GODNF, a Generalized Opinion DynamicsNeural Framework, which unifies multiple opinion dynamics models into aprincipled, trainable diffusion mechanism. Our framework captures heterogeneousdiffusion patterns and temporal dynamics via node-specific behavior modelingand dynamic neighborhood influence, while ensuring efficient and interpretablemessage propagation even at deep layers. We provide a rigorous theoreticalanalysis demonstrating GODNF's ability to model diverse convergenceconfigurations. Extensive empirical evaluations of node classification andinfluence estimation tasks confirm GODNF's superiority over state-of-the-artGNNs.</description>
      <author>example@mail.com (Asela Hevapathige, Asiri Wijesinghe, Ahad N. Zehmakan)</author>
      <guid isPermaLink="false">2508.11249v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network</title>
      <link>http://arxiv.org/abs/2508.11212v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从粗到细的两阶段知识蒸馏框架，用于高效准确的人体姿态估计，在保持高性能的同时显著降低了计算复杂度。&lt;h4&gt;背景&lt;/h4&gt;人体姿态估计已广泛应用于以人为中心的理解和生成领域，但现有最先进的方法通常需要大量计算资源才能获得准确预测。&lt;h4&gt;目的&lt;/h4&gt;开发一个准确、鲁棒且轻量级的人体姿态估计器，解决现有方法计算资源消耗过大的问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种从粗到细的两阶段知识蒸馏框架：第一阶段引入人体关节结构损失，挖掘关节间结构信息，传递高级语义知识；第二阶段使用图像引导渐进图卷积网络(IGP-GCN)优化初始姿态，并通过教师模型的最终输出以渐进方式监督训练。&lt;h4&gt;主要发现&lt;/h4&gt;在COCO关键点和CrowdPose基准数据集上的实验表明，该方法优于多种现有最先进的人体姿态估计方法，特别是在更复杂的CrowdPose数据集上性能提升更为显著。&lt;h4&gt;结论&lt;/h4&gt;所提出的从粗到细的两阶段知识蒸馏框架能够有效提高人体姿态估计的性能，同时保持模型轻量化，适用于实际应用场景。&lt;h4&gt;翻译&lt;/h4&gt;人体姿态估计已广泛应用于以人为中心的理解和生成，但大多数现有最先进的人体姿态估计方法需要大量计算资源才能获得准确预测。为了获得一个准确、鲁棒且轻量级的人体姿态估计器，一种可行的方法是通过知识蒸馏将强大的教师模型中的姿态知识转移到参数较少的学生模型中。然而，传统的知识蒸馏框架没有充分探索人体关节之间的上下文信息。因此，本文提出了一种新颖的从粗到细的两阶段知识蒸馏框架用于人体姿态估计。在第一阶段蒸馏中，我们引入人体关节结构损失来挖掘人体关节之间的结构信息，以便将教师模型的高级语义知识传递给学生模型。在第二阶段蒸馏中，我们利用图像引导渐进图卷积网络(IGP-GCN)来优化从第一阶段蒸馏获得的初始人体姿态，并通过教师模型的最终输出姿态以渐进方式监督IGP-GCN的训练。在基准数据集：COCO关键点和CrowdPose数据集上的大量实验表明，我们提出的方法优于许多现有最先进的人体姿态估计方法，特别是对于更复杂的CrowdPose数据集，我们模型的性能提升更为显著。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human pose estimation has been widely applied in the human-centricunderstanding and generation, but most existing state-of-the-art human poseestimation methods require heavy computational resources for accuratepredictions. In order to obtain an accurate, robust yet lightweight human poseestimator, one feasible way is to transfer pose knowledge from a powerfulteacher model to a less-parameterized student model by knowledge distillation.However, the traditional knowledge distillation framework does not fullyexplore the contextual information among human joints. Thus, in this paper, wepropose a novel coarse-to-fine two-stage knowledge distillation framework forhuman pose estimation. In the first-stage distillation, we introduce the humanjoints structure loss to mine the structural information among human joints soas to transfer high-level semantic knowledge from the teacher model to thestudent model. In the second-stage distillation, we utilize an Image-GuidedProgressive Graph Convolutional Network (IGP-GCN) to refine the initial humanpose obtained from the first-stage distillation and supervise the training ofthe IGP-GCN in the progressive way by the final output pose of teacher model.The extensive experiments on the benchmark dataset: COCO keypoint and CrowdPosedatasets, show that our proposed method performs favorably against lots of theexisting state-of-the-art human pose estimation methods, especially for themore complex CrowdPose dataset, the performance improvement of our model ismore significant.</description>
      <author>example@mail.com (Zhangjian Ji, Wenjin Zhang, Shaotong Qiao, Kai Feng, Yuhua Qian)</author>
      <guid isPermaLink="false">2508.11212v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation</title>
      <link>http://arxiv.org/abs/2508.11105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为FGAT的新型时尚推荐框架，通过结合图神经网络、图注意力机制和多模态特征，有效解决了时尚推荐系统中服装搭配兼容性与个性化推荐的双重挑战，在POG数据集上表现优于基线模型。&lt;h4&gt;背景&lt;/h4&gt;时尚行业快速扩张导致产品种类激增，用户在电子商务平台上寻找兼容商品面临挑战。现有时尚推荐系统通常将服装搭配兼容性与个性化推荐独立处理，忽视了商品与用户偏好间的复杂相互作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时考虑服装搭配兼容性和个性化推荐的时尚推荐系统，解决现有研究中的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出名为FGAT的新框架，构建用户-服装-商品三层分层图结构，整合视觉和文本特征，利用图神经网络和图注意力机制动态加权节点重要性，捕获关键交互并生成精确表示。&lt;h4&gt;主要发现&lt;/h4&gt;在POG数据集上评估，FGAT模型在精确率、命中率、召回率、NDCG和准确率等指标上均优于HFGN等基线模型，证明了多模态视觉-文本特征与分层图结构和注意力机制结合的有效性。&lt;h4&gt;结论&lt;/h4&gt;结合多模态视觉-文本特征与分层图结构和注意力机制显著提高了个性化时尚推荐系统的准确性和效率，为解决时尚推荐中的兼容性和个性化双重挑战提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;时尚行业的快速扩张和产品种类的日益增多，使得用户在电子商务平台上寻找兼容性强的商品变得具有挑战性。有效的时尚推荐系统对于过滤不相关商品和推荐合适商品至关重要。然而，同时解决服装搭配兼容性和个性化推荐仍然是一个重大挑战，因为现有研究通常将这两个方面独立处理，往往忽视了商品与用户偏好之间复杂的相互作用。本研究引入了一个名为FGAT的新框架，灵感来自HFGN模型，该框架利用图神经网络和图注意力机制来解决这一问题。所提出的框架构建了一个包含用户、服装和商品的三层分层图，整合视觉和文本特征，同时建模服装搭配兼容性和用户偏好。图注意力机制在表示传播过程中动态加权节点重要性，能够捕获关键交互并生成用户偏好和服装兼容性的精确表示。在POG数据集上评估，FGAT优于HFGN等基线模型，在精确率、命中率、召回率、NDCG和准确率方面取得了改进的结果。这些结果表明，将多模态视觉-文本特征与分层图结构和注意力机制相结合，显著提高了个性化时尚推荐系统的准确性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid expansion of the fashion industry and the growing variety ofproducts have made it challenging for users to find compatible items one-commerce platforms. Effective fashion recommendation systems are crucial forfiltering irrelevant items and suggesting suitable ones. However,simultaneously addressing outfit compatibility and personalized recommendationsremains a significant challenge, as these aspects are often treatedindependently in existing studies, often overlooking the complex interactionsbetween items and user preferences. This research introduces a new frameworknamed FGAT, inspired by the HFGN model, which leverages graph neural networksand graph attention mechanisms to tackle this issue. The proposed frameworkconstructs a three-tier hierarchical graph of users, outfits, and items,integrating visual and textual features to simultaneously model outfitcompatibility and user preferences. A graph attention mechanism dynamicallyweights node importance during representation propagation, enabling the captureof key interactions and generating precise representations for both userpreferences and outfit compatibility. Evaluated on the POG dataset, FGAToutperforms baseline models such as HFGN, achieving improved results inprecision, HR, recall, NDCG, and accuracy.These results demonstrate thatcombining multimodal visual-textual features with a hierarchical graphstructure and attention mechanisms significantly enhances the accuracy andefficiency of personalized fashion recommendation systems.</description>
      <author>example@mail.com (Sajjad Saed, Babak Teimourpour)</author>
      <guid isPermaLink="false">2508.11105v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Explainable Attention-Guided Stacked Graph Neural Networks for Malware Detection</title>
      <link>http://arxiv.org/abs/2508.09801v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图的恶意软件检测和解释的新型堆叠集成框架，通过结合多种图神经网络基础学习器和注意力机制的元学习器，提高了检测准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;现代计算环境中的恶意软件检测需要模型不仅准确，还要可解释且能抵抗逃避技术。图神经网络在基于控制流图的程序表示建模方面显示出潜力，但单一模型方法存在泛化能力有限和缺乏可解释性的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提高恶意软件检测性能并提供可解释结果的集成框架，特别适用于高风险安全应用场景。&lt;h4&gt;方法&lt;/h4&gt;从PE文件动态提取控制流图，通过两步嵌入策略编码基本块；使用具有不同消息传递机制的多样化GNN基础学习器捕获互补行为特征；采用基于注意力的多层感知器作为元学习器聚合预测结果并量化各基础模型的贡献；引入集成感知的后解释技术，利用GNN解释器生成的边级重要性分数和注意力权重生成可解释结果。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在提高分类性能的同时，能够提供对恶意软件行为的有意义解释，使检测结果更加透明和可信。&lt;h4&gt;结论&lt;/h4&gt;这种堆叠集成框架有效解决了单一模型在恶意软件检测中的局限性，通过结合多种学习方法和注意力机制，实现了更准确的检测和更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;现代计算环境中的恶意软件检测需要不仅准确而且可解释且能抵抗逃避技术的模型。图神经网络通过建模基于图结构的程序表示（如控制流图CFG）中的丰富结构依赖关系，在此领域显示出前景。然而，单一模型方法可能存在泛化能力有限和缺乏可解释性的问题，特别是在高风险的安全应用中。在本文中，我们提出了一种用于基于图的恶意软件检测和解释的新型堆叠集成框架。我们的方法通过两步嵌入策略从可移植可执行（PE）文件中动态提取CFG并编码其基本块。一组具有不同消息传递机制的多样化GNN基础学习器用于捕获互补的行为特征。它们的预测输出由一个基于注意力的多层感知器实现的元学习器聚合，该元学习器既能分类恶意软件实例又能量化每个基础模型的贡献。为了增强可解释性，我们引入了一种集成感知的后解释技术，它利用GNN解释器生成的边级重要性分数，并使用学习到的注意力权重融合它们。这产生了与最终集成决策一致的可解释的、模型不可知解释。实验结果表明，我们的框架提高了分类性能，同时提供了对恶意软件行为的有意义的解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malware detection in modern computing environments demands models that arenot only accurate but also interpretable and robust to evasive techniques.Graph neural networks (GNNs) have shown promise in this domain by modeling richstructural dependencies in graph-based program representations such as controlflow graphs (CFGs). However, single-model approaches may suffer from limitedgeneralization and lack interpretability, especially in high-stakes securityapplications. In this paper, we propose a novel stacking ensemble framework forgraph-based malware detection and explanation. Our method dynamically extractsCFGs from portable executable (PE) files and encodes their basic blocks througha two-step embedding strategy. A set of diverse GNN base learners, each with adistinct message-passing mechanism, is used to capture complementary behavioralfeatures. Their prediction outputs are aggregated by a meta-learner implementedas an attention-based multilayer perceptron, which both classifies malwareinstances and quantifies the contribution of each base model. To enhanceexplainability, we introduce an ensemble-aware post-hoc explanation techniquethat leverages edge-level importance scores generated by a GNN explainer andfuses them using the learned attention weights. This produces interpretable,model-agnostic explanations aligned with the final ensemble decision.Experimental results demonstrate that our framework improves classificationperformance while providing insightful interpretations of malware behavior.</description>
      <author>example@mail.com (Hossein Shokouhinejad, Roozbeh Razavi-Far, Griffin Higgins, Ali A Ghorbani)</author>
      <guid isPermaLink="false">2508.09801v2</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Pretrained Conformers for Audio Fingerprinting and Retrieval</title>
      <link>http://arxiv.org/abs/2508.11609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Conformers在语音处理中表现出色，本研究通过自监督对比学习框架训练的Conformer编码器能够为小段音频生成独特嵌入，在音频检索任务中表现优异，且对各种音频失真具有鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;Conformers在语音处理中表现出色，因为它们能够捕捉局部和全局交互。&lt;h4&gt;目的&lt;/h4&gt;利用自监督对比学习框架训练基于Conformer的编码器，能够为小段音频生成独特的嵌入表示，并能很好地泛化到未见过的数据。&lt;h4&gt;方法&lt;/h4&gt;使用自监督对比学习框架训练基于Conformer的编码器，为小段音频生成嵌入表示。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用3秒音频生成嵌入，在音频检索任务中取得了最先进的结果；模型几乎完全不受时间错位的影响；在噪声、混响或极端时间拉伸等其他音频失真情况下也取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;代码和模型已公开可用，由于使用不同大小的流行且免费的数据集进行训练和测试，结果易于重现。&lt;h4&gt;翻译&lt;/h4&gt;Conformers由于其能够捕捉局部和全局交互的能力，在语音处理中已显示出优异的结果。在这项工作中，我们利用自监督对比学习框架训练基于Conformer的编码器，这些编码器能够为小段音频生成独特的嵌入，并能够很好地泛化到未见过的数据。我们仅使用3秒音频生成嵌入，在音频检索任务中取得了最先进的结果。我们的模型几乎完全不受时间错位的影响，并且在其他音频失真（如噪声、混响或极端时间拉伸）的情况下也取得了最先进的结果。代码和模型已公开提供，并且结果是易于复现的，因为我们使用不同大小的流行且免费的数据集进行训练和测试。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conformers have shown great results in speech processing due to their abilityto capture both local and global interactions. In this work, we utilize aself-supervised contrastive learning framework to train conformer-basedencoders that are capable of generating unique embeddings for small segments ofaudio, generalizing well to previously unseen data. We achieve state-of-the-artresults for audio retrieval tasks while using only 3 seconds of audio togenerate embeddings. Our models are almost completely immune to temporalmisalignments and achieve state-of-the-art results in cases of other audiodistortions such as noise, reverb or extreme temporal stretching. Code andmodels are made publicly available and the results are easy to reproduce as wetrain and test using popular and freely available datasets of different sizes.</description>
      <author>example@mail.com (Kemal Altwlkany, Elmedin Selmanovic, Sead Delalic)</author>
      <guid isPermaLink="false">2508.11609v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models</title>
      <link>http://arxiv.org/abs/2508.11348v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出NeMo，一种可扩展且通用的深度神经网络模块化-同时-训练方法，解决了现有方法难以处理大规模和多样化模型的问题。&lt;h4&gt;背景&lt;/h4&gt;随着深度神经网络模型在现代软件系统中的整合增多，高昂的构建成本成为显著挑战。模型重用虽可降低训练成本，但整体重用可能导致显著的推理开销，因此DNN模块化方法应运而生，通过分解模型实现模块重用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理多样化DNN架构（特别是基于Transformer的大规模模型）的可扩展且通用的模块化-同时-训练方法。&lt;h4&gt;方法&lt;/h4&gt;NeMo在神经元级别（所有DNN的基本组成部分）操作，确保适用于Transformer和各种架构。设计了一种基于对比学习的模块化训练方法，配合有效的复合损失函数，使方法能够扩展到大规模模型。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基于Transformer的模型和四个CNN模型上的实验表明，NeMo优于现有最先进的MwT方法，模块分类准确率平均提高1.72%，模块大小减少58.10%，在CNN和大规模Transformer模型上均表现良好。&lt;h4&gt;结论&lt;/h4&gt;NeMo为可扩展和通用的DNN模块化提供了有前景的方法，实际案例研究显示其在开源项目中的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;随着深度神经网络模型在现代软件系统中的整合日益增多，高昂的构建成本已成为一个重大挑战。模型重用已被广泛应用于降低训练成本，但不加选择地重用整个模型可能会带来显著的推理开销。因此，DNN模块化引起了关注，它通过分解DNN模型实现模块重用。新兴的模块化-同时-训练范式将模块化整合到训练过程中，优于训练后模块化的方法。然而，现有的MwT方法主要关注小规模CNN模型在卷积核级别的应用，难以处理多样化的DNN和大模型，特别是基于Transformer的模型。为解决这些限制，我们提出了NeMo，一种可扩展且通用的MwT方法。NeMo在神经元级别操作，这是所有DNN的基本组成部分，确保适用于Transformer和各种架构。我们设计了一种基于对比学习的模块化训练方法，配合有效的复合损失函数，使其能够扩展到大规模模型。在两个分类数据集上的两个基于Transformer的模型和四个CNN模型的综合实验证明了NeMo优于最先进的MwT方法。结果显示模块分类准确率平均提高1.72%，模块大小减少58.10%，证明了其在CNN和大规模基于Transformer的模型上的有效性。开源项目的案例研究显示了NeMo在实际场景中的潜在益处，为可扩展和通用的DNN模块化提供了有前景的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3757740&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing incorporation of deep neural network (DNN) models intomodern software systems, the prohibitive construction costs have become asignificant challenge. Model reuse has been widely applied to reduce trainingcosts, but indiscriminately reusing entire models may incur significantinference overhead. Consequently, DNN modularization has gained attention,enabling module reuse by decomposing DNN models. The emergingmodularizing-while-training (MwT) paradigm, which incorporates modularizationinto training, outperforms modularizing-after-training approaches. However,existing MwT methods focus on small-scale CNN models at the convolutionalkernel level and struggle with diverse DNNs and large-scale models,particularly Transformer-based models. To address these limitations, we proposeNeMo, a scalable and generalizable MwT approach. NeMo operates at the neuronlevel fundamental component common to all DNNs-ensuring applicability toTransformers and various architectures. We design a contrastive learning-basedmodular training method with an effective composite loss function, enablingscalability to large-scale models. Comprehensive experiments on twoTransformer-based models and four CNN models across two classification datasetsdemonstrate NeMo's superiority over state-of-the-art MwT methods. Results showaverage gains of 1.72% in module classification accuracy and 58.10% reductionin module size, demonstrating efficacy across both CNN and large-scaleTransformer-based models. A case study on open-source projects shows NeMo'spotential benefits in practical scenarios, offering a promising approach forscalable and generalizable DNN modularization.</description>
      <author>example@mail.com (Xiaohan Bi, Binhang Qi, Hailong Sun, Xiang Gao, Yue Yu, Xiaojun Liang)</author>
      <guid isPermaLink="false">2508.11348v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning</title>
      <link>http://arxiv.org/abs/2508.11328v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HS-GPPT的新型图预训练和提示调优框架，解决了现有方法无法处理具有不同同质性的现实世界图中的多样化频谱分布问题。&lt;h4&gt;背景&lt;/h4&gt;现有图'预训练和提示调优'方法依赖于基于同质性的低频知识，无法处理具有不同同质性的现实世界图中的多样化频谱分布，导致在有限监督下知识迁移效率低下。&lt;h4&gt;目的&lt;/h4&gt;解决预训练和下游任务之间的频谱差距问题，确保在预训练和提示调优过程中都保持频谱对齐，从而促进同质性和异质性之间的频谱知识迁移。&lt;h4&gt;方法&lt;/h4&gt;提出HS-GPPT模型，使用混合频谱滤波器主干和局部-全局对比学习来获取丰富的频谱知识，并设计提示图以将频谱分布与预文本对齐，实现频谱知识迁移。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析揭示了频谱特异性原则：最佳知识迁移需要预训练频谱滤波器与下游图的内在频谱之间的对齐；在有限监督下，预训练和下游任务之间的较大频谱差距阻碍了有效的适应。&lt;h4&gt;结论&lt;/h4&gt;HS-GPPT模型通过确保频谱对齐，有效解决了现有方法的局限性，在直推学习和归纳学习设置下均表现出有效性。&lt;h4&gt;翻译&lt;/h4&gt;图'预训练和提示调优'将下游任务与预训练目标对齐，以在有限监督下实现高效知识迁移。然而，现有方法依赖于基于同质性的低频知识，无法处理具有不同同质性的现实世界图中的多样化频谱分布。我们的理论分析揭示了一个频谱特异性原则：最佳知识迁移需要预训练频谱滤波器与下游图的内在频谱之间的对齐。在有限监督下，预训练和下游任务之间的较大频谱差距阻碍了有效的适应。为了弥合这一差距，我们提出了HS-GPPT模型，一个确保在预训练和提示调优过程中都保持频谱对齐的新框架。我们使用混合频谱滤波器主干和局部-全局对比学习来获取丰富的频谱知识。然后我们设计提示图以将频谱分布与预文本对齐，促进同质性和异质性之间的频谱知识迁移。大量实验在直推学习和归纳学习设置下验证了其有效性。我们的代码可在https://anonymous.4open.science/r/HS-GPPT-62D2/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph ``pre-training and prompt-tuning'' aligns downstream tasks withpre-trained objectives to enable efficient knowledge transfer under limitedsupervision. However, existing methods rely on homophily-based low-frequencyknowledge, failing to handle diverse spectral distributions in real-worldgraphs with varying homophily. Our theoretical analysis reveals a spectralspecificity principle: optimal knowledge transfer requires alignment betweenpre-trained spectral filters and the intrinsic spectrum of downstream graphs.Under limited supervision, large spectral gaps between pre-training anddownstream tasks impede effective adaptation. To bridge this gap, we proposethe HS-GPPT model, a novel framework that ensures spectral alignment throughoutboth pre-training and prompt-tuning. We utilize a hybrid spectral filterbackbone and local-global contrastive learning to acquire abundant spectralknowledge. Then we design prompt graphs to align the spectral distribution withpretexts, facilitating spectral knowledge transfer across homophily andheterophily. Extensive experiments validate the effectiveness under bothtransductive and inductive learning settings. Our code is available athttps://anonymous.4open.science/r/HS-GPPT-62D2/.</description>
      <author>example@mail.com (Haitong Luo, Suhang Wang, Weiyao Zhang, Ruiqi Meng, Xuying Meng, Yujun Zhang)</author>
      <guid isPermaLink="false">2508.11328v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models</title>
      <link>http://arxiv.org/abs/2508.11317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对视觉语言模型(VLMs)的逻辑理解能力不足问题，提出了LogicBench基准测试和LogicCLIP训练框架，显著提升了VLMs在逻辑推理任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)如CLIP已成为多模态智能的基础，但其逻辑理解能力尚未得到充分探索，存在'逻辑盲点'，限制了它们在实际应用中的可靠性。&lt;h4&gt;目的&lt;/h4&gt;系统诊断VLMs的逻辑理解能力，并通过新的训练框架增强VLMs的逻辑敏感性，弥补逻辑理解与人类表现之间的差距。&lt;h4&gt;方法&lt;/h4&gt;创建LogicBench基准测试(包含50,000多对视觉语言数据，涵盖9个逻辑类别和4个多样化场景)；提出LogicCLIP训练框架，利用逻辑感知数据生成和结合粗粒度对齐、细粒度多目标选择及逻辑结构感知目标的对比学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;现有VLMs即使在最先进的模型中，准确率也比人类表现低40多个百分点，在因果关系和条件性等挑战性任务中表现尤其不佳，过度依赖表面语义而非关键逻辑结构；LogicCLIP在所有LogicBench领域都显著提升了逻辑理解能力，同时保持了在通用视觉语言基准测试中的竞争性能。&lt;h4&gt;结论&lt;/h4&gt;LogicBench和LogicCLIP将成为推进VLM逻辑能力的重要资源，为增强多模态模型的逻辑理解提供了有效途径。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)以CLIP为代表已成为多模态智能的基础。然而，它们的逻辑理解能力仍然远未被充分探索，导致关键的'逻辑盲点'，限制了它们在实际应用中的可靠性。为了系统诊断这一问题，我们引入了LogicBench，这是一个包含9个逻辑类别和4个多样化场景(图像、视频、异常检测和医疗诊断)的综合性基准，拥有超过50,000对视觉语言数据。我们的评估显示，现有的VLMs，即使是最先进的模型，准确率也比人类表现低40多个百分点，特别是在因果关系和条件性等挑战性任务中，突显了它们对表面语义而非关键逻辑结构的依赖。为了弥合这一差距，我们提出了LogicCLIP，这是一个新颖的训练框架，旨在通过数据生成和优化目标的改进来增强VLMs的逻辑敏感性。LogicCLIP利用逻辑感知数据生成和一种结合粗粒度对齐、细粒度多目标选择以及新颖的逻辑结构感知目标的对比学习策略。大量实验证明了LogicCLIP在所有LogicBench领域中逻辑理解的显著提升，明显优于基线模型。此外，LogicCLIP在通用视觉语言基准测试上保持了甚至超越了竞争性能，表明增强的逻辑理解不会以通用对齐为代价。我们相信LogicBench和LogicCLIP将成为推进VLM逻辑能力的重要资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs), exemplified by CLIP, have emerged asfoundational for multimodal intelligence. However, their capacity for logicalunderstanding remains significantly underexplored, resulting in critical''logical blindspots'' that limit their reliability in practical applications.To systematically diagnose this, we introduce LogicBench, a comprehensivebenchmark with over 50,000 vision-language pairs across 9 logical categoriesand 4 diverse scenarios: images, videos, anomaly detection, and medicaldiagnostics. Our evaluation reveals that existing VLMs, even thestate-of-the-art ones, fall at over 40 accuracy points below human performance,particularly in challenging tasks like Causality and Conditionality,highlighting their reliance on surface semantics over critical logicalstructures. To bridge this gap, we propose LogicCLIP, a novel trainingframework designed to boost VLMs' logical sensitivity through advancements inboth data generation and optimization objectives. LogicCLIP utilizeslogic-aware data generation and a contrastive learning strategy that combinescoarse-grained alignment, a fine-grained multiple-choice objective, and a novellogical structure-aware objective. Extensive experiments demonstrateLogicCLIP's substantial improvements in logical comprehension across allLogicBench domains, significantly outperforming baselines. Moreover, LogicCLIPretains, and often surpasses, competitive performance on generalvision-language benchmarks, demonstrating that the enhanced logicalunderstanding does not come at the expense of general alignment. We believethat LogicBench and LogicCLIP will be important resources for advancing VLMlogical capabilities.</description>
      <author>example@mail.com (Yuchen Zhou, Jiayu Tang, Shuo Yang, Xiaoyan Xiao, Yuqin Dai, Wenhao Yang, Chao Gou, Xiaobo Xia, Tat-Seng Chua)</author>
      <guid isPermaLink="false">2508.11317v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.11210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by Machine Learning for Healthcare 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为'从未来借用'(BFF)的对比多模态框架，通过从后期阶段借用信息信号来改进早期儿童风险评估的预测性能。&lt;h4&gt;背景&lt;/h4&gt;针对儿童群体的风险评估通常在多个阶段进行，如产前、出生和儿童健康检查等。虽然后期阶段的预测通常更精确，但临床实践更希望尽可能早地进行可靠的风险评估。&lt;h4&gt;目的&lt;/h4&gt;专注于改进早期阶段风险评估的预测性能，使临床医生能够更早地识别风险。&lt;h4&gt;方法&lt;/h4&gt;BFF框架将每个时间窗口视为不同的模态，模型在整个时间范围内使用所有可用数据进行训练，同时使用最新信息进行风险评估。这种对比框架允许模型从后期阶段(如儿童健康检查)借用信息，隐式监督早期阶段(如产前/出生阶段)的学习过程。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实的儿童结果预测任务上验证了BFF方法，证明了其在早期风险评估方面的一致性改进。&lt;h4&gt;结论&lt;/h4&gt;BFF方法能够有效提升早期风险评估的准确性，为临床决策提供更有价值的早期预警信息。&lt;h4&gt;翻译&lt;/h4&gt;针对儿童群体的风险评估通常在多个阶段进行。例如，临床医生可能会在产前、出生和儿童健康检查等阶段评估风险。虽然后期阶段的预测通常能达到更高的精确度，但临床实践更希望尽可能早地进行可靠的风险评估。因此，本研究专注于改进早期阶段风险评估的预测性能。我们的解决方案'从未来借用'(BFF)是一个对比多模态框架，将每个时间窗口视为不同的模态。在BFF中，模型在整个时间范围内使用所有可用数据进行训练，同时使用最新信息进行风险评估。这种对比框架允许模型'借用'后期阶段(如儿童健康检查)的有用信息信号，隐式监督早期阶段(如产前/出生阶段)的学习。我们在两个真实的儿童结果预测任务上验证了BFF，证明了其在早期风险评估方面的一致性改进。代码可在https://github.com/scotsun/bff获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Risk assessments for a pediatric population are often conducted acrossmultiple stages. For example, clinicians may evaluate risks prenatally, atbirth, and during Well-Child visits. Although predictions made at later stagestypically achieve higher precision, it is clinically desirable to make reliablerisk assessments as early as possible. Therefore, this study focuses onimproving prediction performance in early-stage risk assessments. Our solution,\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modalframework that treats each time window as a distinct modality. In BFF, a modelis trained on all available data throughout the time while performing a riskassessment using up-to-date information. This contrastive framework allows themodel to ``borrow'' informative signals from later stages (e.g., Well-Childvisits) to implicitly supervise the learning at earlier stages (e.g.,prenatal/birth stages). We validate BFF on two real-world pediatric outcomeprediction tasks, demonstrating consistent improvements in early riskassessments. The code is available at https://github.com/scotsun/bff.</description>
      <author>example@mail.com (Minghui Sun, Matthew M. Engelhard, Benjamin A. Goldstein)</author>
      <guid isPermaLink="false">2508.11210v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations</title>
      <link>http://arxiv.org/abs/2508.11141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于对比学习的跨模态谣言检测方法，通过多尺度图像与上下文相关性探索，有效解决了现有方法忽视图像内容和跨模态关系的问题&lt;h4&gt;背景&lt;/h4&gt;现有的谣言检测方法经常忽视图像中的内容以及不同视觉尺度下上下文与图像之间的固有关系，导致与谣言识别相关的关键信息丢失&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中忽视图像内容和跨模态关系的问题，提出一种新颖的基于对比学习的跨模态谣言检测方案&lt;h4&gt;方法&lt;/h4&gt;提出了多尺度图像与上下文相关性探索算法（MICC），包括：1）设计SCLIP编码器生成文本和多尺度图像块的统一语义嵌入；2）引入跨模态多尺度对齐模块识别与文本语义最相关的图像区域；3）设计尺度感知融合网络整合高度相关的多尺度图像特征与全局文本特征&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的广泛评估表明，与现有最先进的方法相比，该方法在谣言检测方面取得了显著的性能提升&lt;h4&gt;结论&lt;/h4&gt;该方法在谣言检测方面具有有效性和实际应用潜力&lt;h4&gt;翻译&lt;/h4&gt;现有的谣言检测方法经常忽视图像中的内容以及不同视觉尺度下上下文与图像之间的固有关系，从而导致与谣言识别相关的关键信息丢失。为解决这些问题，本文提出了一种新颖的基于对比学习的跨模态谣言检测方案，即多尺度图像与上下文相关性探索算法（MICC）。具体而言，我们设计了一个SCLIP编码器，通过对比预训练为文本和多尺度图像块生成统一的语义嵌入，使其相关性可以通过点积相似度进行测量。在此基础上，引入了一个跨模态多尺度对齐模块，通过互信息最大化和信息瓶颈原则，在基于文本与多尺度图像块之间构建的跨模态相关性矩阵的指导下，识别与文本语义最相关的图像区域。此外，设计了一个尺度感知融合网络，根据图像区域的语义重要性和跨模态相关性为其分配自适应权重，将高度相关的多尺度图像特征与全局文本特征进行融合。所提出的方法已在两个真实世界数据集上进行了广泛评估。实验结果表明，与现有最先进的方法相比，该方法在谣言检测方面取得了显著的性能提升，突显了其有效性和实际应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing rumor detection methods often neglect the content within images aswell as the inherent relationships between contexts and images across differentvisual scales, thereby resulting in the loss of critical information pertinentto rumor identification. To address these issues, this paper presents a novelcross-modal rumor detection scheme based on contrastive learning, namely theMulti-scale Image and Context Correlation exploration algorithm (MICC).Specifically, we design an SCLIP encoder to generate unified semanticembeddings for text and multi-scale image patches through contrastivepretraining, enabling their relevance to be measured via dot-productsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module isintroduced to identify image regions most relevant to the textual semantics,guided by mutual information maximization and the information bottleneckprinciple, through a Top-K selection strategy based on a cross-modal relevancematrix constructed between the text and multi-scale image patches. Moreover, ascale-aware fusion network is designed to integrate the highly correlatedmulti-scale image features with global text features by assigning adaptiveweights to image regions based on their semantic importance and cross-modalrelevance. The proposed methodology has been extensively evaluated on tworeal-world datasets. The experimental results demonstrate that it achieves asubstantial performance improvement over existing state-of-the-art approachesin rumor detection, highlighting its effectiveness and potential for practicalapplications.</description>
      <author>example@mail.com (Bin Ma, Yifei Zhang, Yongjin Xian, Qi Li, Linna Zhou, Gongxun Miao)</author>
      <guid isPermaLink="false">2508.11141v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title>
      <link>http://arxiv.org/abs/2508.09922v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了原型扩散模型(PDM)，一种将原型学习直接整合到扩散过程中的方法，用于高效和自适应的视觉条件生成，无需外部内存。PDM通过对比学习构建动态紧凑视觉原型集，指导去噪步骤，实现具有强语义基础的高效生成，同时保持高生成质量并减少计算和存储开销。&lt;h4&gt;背景&lt;/h4&gt;扩散模型已成为高质量图像生成的领先框架，提供稳定的训练和跨不同领域的强大性能。然而，它们在迭代去噪过程中计算密集。潜在空间模型如Stable Diffusion通过在压缩表示上操作缓解部分成本，但牺牲了细粒度细节。检索增强扩散模型(RDM)通过从大型外部存储库检索相似示例解决效率问题，但需要昂贵的存储和检索基础设施，依赖静态视觉语言模型如CLIP进行相似性匹配，且在训练期间缺乏适应性。&lt;h4&gt;目的&lt;/h4&gt;解决扩散模型计算密集的问题，同时避免现有方法(如RDM)的局限性，如需要外部存储、检索基础设施、静态视觉语言模型和缺乏训练适应性。&lt;h4&gt;方法&lt;/h4&gt;提出原型扩散模型(PDM)，一种将原型学习直接整合到扩散过程中的方法。PDM不检索参考样本，而是使用对比学习从干净图像特征构建动态紧凑视觉原型集。这些原型通过将噪声表示与语义相关的视觉模式对齐来指导去噪步骤，实现高效生成和强语义基础。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，PDM在保持高生成质量的同时，减少了计算和存储开销，为基于检索的条件扩散模型提供了可扩展的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PDM提供了一种高效且自适应的视觉条件生成方法，无需外部内存，同时保持高质量的图像生成能力，解决了现有扩散模型在计算效率和存储需求方面的挑战。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型已成为高质量图像生成的领先框架，提供稳定的训练和跨不同领域的强大性能。然而，它们在迭代去噪过程中仍然计算密集。像Stable Diffusion这样的潜在空间模型通过在压缩表示上操作来缓解部分成本，但牺牲了细粒度细节。更近期的方法如检索增强扩散模型(RDM)通过从大型外部存储库中检索相似示例来条件化去噪以解决效率问题。虽然这些方法有效，但它们引入了缺点：需要昂贵的存储和检索基础设施，依赖于静态的视觉语言模型(如CLIP)进行相似性匹配，并且在训练期间缺乏适应性。我们提出了原型扩散模型(PDM)，一种将原型学习直接整合到扩散过程中的方法，用于高效和自适应的视觉条件生成 - 无需外部内存。PDM不检索参考样本，而是使用对比学习从干净图像特征构建动态紧凑视觉原型集。这些原型通过将噪声表示与语义相关的视觉模式对齐来指导去噪步骤，实现具有强语义基础的高效生成。实验表明，PDM在保持高生成质量的同时减少了计算和存储开销，为基于检索的条件扩散模型提供了可扩展的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models have emerged as a leading framework for high-quality imagegeneration, offering stable training and strong performance across diversedomains. However, they remain computationally intensive, particularly duringthe iterative denoising process. Latent-space models like Stable Diffusionalleviate some of this cost by operating in compressed representations, thoughat the expense of fine-grained detail. More recent approaches such asRetrieval-Augmented Diffusion Models (RDM) address efficiency by conditioningdenoising on similar examples retrieved from large external memory banks. Whileeffective, these methods introduce drawbacks: they require costly storage andretrieval infrastructure, depend on static vision-language models like CLIP forsimilarity, and lack adaptability during training. We propose the PrototypeDiffusion Model (PDM), a method that integrates prototype learning directlyinto the diffusion process for efficient and adaptive visual conditioning -without external memory. Instead of retrieving reference samples, PDMconstructs a dynamic set of compact visual prototypes from clean image featuresusing contrastive learning. These prototypes guide the denoising steps byaligning noisy representations with semantically relevant visual patterns,enabling efficient generation with strong semantic grounding. Experiments showthat PDM maintains high generation quality while reducing computational andstorage overhead, offering a scalable alternative to retrieval-basedconditioning in diffusion models.</description>
      <author>example@mail.com (Bilal Faye, Hanane Azzag, Mustapha Lebbah)</author>
      <guid isPermaLink="false">2508.09922v2</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model</title>
      <link>http://arxiv.org/abs/2508.11350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HOID-R1是一种创新的HOI检测框架，结合思维链引导的监督微调和组相对策略优化，在HOI检测任务上实现了最先进的性能，并具有良好的开放世界泛化能力。&lt;h4&gt;背景&lt;/h4&gt;人类-物体交互(HOI)理解与识别是AR/VR和机器人技术中的关键应用。现有开放词汇HOI检测方法完全依赖大型语言模型提供文本提示，忽视了它们固有的3D空间理解能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法忽视3D空间理解能力的不足，开发一种能够整合推理能力和空间理解的HOI检测框架。&lt;h4&gt;方法&lt;/h4&gt;结合思维链(CoT)引导的监督微调(SFT)与组相对策略优化(GRPO)在强化学习范式中；应用SFT赋予模型推理能力并强制其阐述思考过程；集成GRPO利用多奖励信号优化策略；引入'MLLM-as-a-judge'机制监督CoT输出减少幻觉。&lt;h4&gt;主要发现&lt;/h4&gt;HOID-R1在HOI检测基准上实现了最先进的性能；在开放世界中对新场景的泛化能力优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;HOID-R1通过整合CoT引导的SFT和GRPO，有效解决了现有方法忽视3D空间理解能力的问题，在HOI检测任务上表现出色且具有良好的开放场景泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;理解和识别人类-物体交互是AR/VR和机器人技术中的一个关键应用。最近的开放词汇HOI检测方法完全依赖大型语言模型来提供更丰富的文本提示，忽视了它们固有的3D空间理解能力。为了解决这一不足，我们引入了HOID-R1，这是第一个将思维链(CoT)引导的监督微调(SFT)与组相对策略优化(GRPO)集成在强化学习(RL)范式中的HOI检测框架。具体来说，我们首先应用SFT赋予模型基本的推理能力，强制模型在输出中阐述其思考过程。随后，我们集成GRPO利用多奖励信号进行策略优化，从而增强不同模态之间的对齐。为了减轻CoT推理中的幻觉，我们引入了一个'MLLM-as-a-judge'机制来监督CoT输出，进一步提高泛化能力。大量实验表明，HOID-R1在HOI检测基准上实现了最先进的性能，并且在开放世界中对新场景的泛化能力优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and recognizing human-object interaction (HOI) is a pivotalapplication in AR/VR and robotics. Recent open-vocabulary HOI detectionapproaches depend exclusively on large language models for richer textualprompts, neglecting their inherent 3D spatial understanding capabilities. Toaddress this shortcoming, we introduce HOID-R1, the first HOI detectionframework that integrates chain-of-thought (CoT) guided supervised fine-tuning(SFT) with group relative policy optimization (GRPO) within a reinforcementlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the modelwith essential reasoning capabilities, forcing the model to articulate itsthought process in the output. Subsequently, we integrate GRPO to leveragemulti-reward signals for policy optimization, thereby enhancing alignmentacross diverse modalities. To mitigate hallucinations in the CoT reasoning, weintroduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,further improving generalization. Extensive experiments show that HOID-R1achieves state-of-the-art performance on HOI detection benchmarks andoutperforms existing methods in open-world generalization to novel scenarios.</description>
      <author>example@mail.com (Zhenhao Zhang, Hanqing Wang, Xiangyu Zeng, Ziyu Cheng, Jiaxin Liu, Haoyu Yan, Zhirui Liu, Kaiyang Ji, Tianxiang Gui, Ke Hu, Kangyi Chen, Yahao Fan, Mokai Pan)</author>
      <guid isPermaLink="false">2508.11350v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset</title>
      <link>http://arxiv.org/abs/2508.11058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepeted to ACM MM 25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MV-ScanQA和TripAlign两个新数据集以及LEGO方法，解决了3D视觉语言学习中的多视点推理和多对象上下文对齐问题，显著提升了模型在3D场景理解方面的能力。&lt;h4&gt;背景&lt;/h4&gt;现有3D视觉语言数据集存在局限性：很少需要超越单视点近距离物体的推理，注释通常将指令与单个对象关联，忽略了多个对象之间的丰富上下文对齐，限制了模型进行深度、多视点3D场景理解的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D视觉语言数据集的限制，促进能够进行深度、多视点3D场景理解模型的发展，特别是对远处物体的理解能力。&lt;h4&gt;方法&lt;/h4&gt;1) 创建MV-ScanQA数据集，其中68%的问题需要整合多视图信息；2) 提出TripAlign数据集，包含100万个&lt;2D视图，3D对象集合，文本&gt;三元组，提供多对象多模态对齐信号；3) 开发LEGO方法，将预训练的2D视觉语言模型知识转移到3D领域。&lt;h4&gt;主要发现&lt;/h4&gt;使用TripAlign预训练的LEGO模型在MV-ScanQA和现有3D密集描述和问答基准测试上都取得了最先进的性能，证明了所提方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;MV-ScanQA、TripAlign和LEGO方法有效解决了3D视觉语言学习中的关键挑战，为开发更强大的3D场景理解模型提供了重要资源。&lt;h4&gt;翻译&lt;/h4&gt;3D视觉语言(3D VL)学习的进步受到现有3D VL数据集的几个限制：它们很少需要在单视点近距离物体范围之外的推理，而且注释通常将指令与单个对象关联，忽略了多个对象之间更丰富的上下文对齐。这显著限制了模型进行深度、多视点3D场景理解的能力，特别是对远处物体的理解。为了解决这些挑战，我们引入了MV-ScanQA，这是一个新的3D问答数据集，其中68%的问题明确需要整合多视图信息（与现有数据集中的不到7%相比），从而严格测试多视图组合推理。为了促进模型在这种要求高场景的训练，我们提出了TripAlign数据集，这是一个大规模、低成本的2D-3D语言预训练语料库，包含100万个&lt;2D视图，3D对象集合，文本&gt;三元组，显式地将上下文相关的对象组与文本对齐，提供比之前单对象注释更丰富、基于视图的多对象多模态对齐信号。我们进一步开发了LEGO，一种用于MV-ScanQA中多视点推理挑战的基线方法，通过TripAlign将预训练的2D视觉语言模型(2D VLVMs)的知识转移到3D领域。实验证明，在TripAlign上预训练的LEGO不仅在提出的MV-ScanQA上取得了最先进的性能，而且在现有的3D密集描述和问答基准测试上也取得了最先进的性能。数据集和代码可在https://matthewdm0816.github.io/tripalign-mvscanqa获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D视觉语言模型中的两个关键问题：现有数据集很少需要模型整合多个视点的信息进行推理，且标注通常只关联指令与单个物体而忽略多物体间的上下文关系。这个问题很重要，因为真实场景理解往往需要综合多视点信息，现有评估无法测试模型的空间推理能力，训练数据稀疏也限制了模型学习复杂场景关系的能力，阻碍了能进行深度3D场景理解的模型发展，这对机器人导航、AR/VR等需要深入理解3D环境的系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过'可解性分析'量化了现有数据集的局限性，发现93%的任务单视点即可解决。针对评估问题，设计了MV-ScanQA数据集；针对训练数据稀疏问题，创建了TripAlign数据集；并提出了LEGO模型作为基线。作者借鉴了2D大型视觉语言模型的成功经验，利用现有3D场景数据集和2D视图，采用预训练的图像描述器和图像-文本检索模型构建高质量数据，并在现有2D LVLM基础上构建3D模型。创新在于将2D视觉语言模型能力扩展到3D场景理解，同时解决了多视点推理和多物体对齐的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D视图作为中介自然分组上下文相关的多个物体，通过多视点推理评估和训练提升3D场景理解能力，并将预训练2D大型视觉语言模型知识有效转移到3D领域。整体流程：1) MV-ScanQA构建：使用LLM引导框架合成需要多视点理解的新问题；2) TripAlign构建：从第一人称视角生成高质量描述和为现有3D数据集扩展上下文视图；3) LEGO模型实现：基于预训练2D LVLM，接受3D场景、文本和2D视图输入，使用视图依赖的多物体对齐机制处理多视点信息，将多模态特征输入语言模型进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) MV-ScanQA评估数据集(68%问题需多视点信息，现有数据集&lt;7%)；2) TripAlign预训练数据集(100万+三元组，使用2D视图自然分组多物体)；3) LEGO模型(有效将2D LVLM知识转移到3D)。不同之处：现有数据集93%任务单视点可解，MV-ScanQA迫使模型超越单视点理解；现有数据集只将指令与单物体对齐，TripAlign提供多物体到文本的密集对齐；现有方法依赖3D目标检测器或不加区分处理2D视图，LEGO建立在2D LVLM基础上，利用视图依赖的多物体对齐机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入MV-ScanQA多视点推理评估基准和TripAlign大规模预训练数据集，显著提升了3D场景理解能力，并提出了LEGO模型有效将2D视觉语言知识转移到3D领域，实现了在多视点3D场景理解任务上的最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advancement of 3D vision-language (3D VL) learning is hindered by severallimitations in existing 3D VL datasets: they rarely necessitate reasoningbeyond a close range of objects in single viewpoint, and annotations often linkinstructions to single objects, missing richer contextual alignments betweenmultiple objects. This significantly curtails the development of models capableof deep, multi-view 3D scene understanding over distant objects. To addressthese challenges, we introduce MV-ScanQA, a novel 3D question answering datasetwhere 68% of questions explicitly require integrating information from multipleviews (compared to less than 7% in existing datasets), thereby rigorouslytesting multi-view compositional reasoning. To facilitate the training ofmodels for such demanding scenarios, we present TripAlign dataset, alarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M &lt;2Dview, set of 3D objects, text&gt; triplets that explicitly aligns groups ofcontextually related objects with text, providing richer, view-groundedmulti-object multimodal alignment signals than previous single-objectannotations. We further develop LEGO, a baseline method for the multi-viewreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2DLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlignachieves state-of-the-art performance not only on the proposed MV-ScanQA, butalso on existing benchmarks for 3D dense captioning and question answering.Datasets and code are available athttps://matthewdm0816.github.io/tripalign-mvscanqa.</description>
      <author>example@mail.com (Wentao Mo, Qingchao Chen, Yuxin Peng, Siyuan Huang, Yang Liu)</author>
      <guid isPermaLink="false">2508.11058v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Causality Matters: How Temporal Information Emerges in Video Language Models</title>
      <link>http://arxiv.org/abs/2508.11576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视频语言模型中的时间理解机制，发现时间信息主要通过帧间交互而非位置编码被整合，并提出了两种提高效率的策略。&lt;h4&gt;背景&lt;/h4&gt;视频语言模型在多模态理解方面取得了显著进展，但时间理解（涉及识别事件顺序、持续时间和跨时间关系）仍然是一个核心挑战。先前的研究强调位置编码(PEs)是编码时间结构的关键机制。&lt;h4&gt;目的&lt;/h4&gt;探究视频语言模型中时间理解的内在机制，并基于发现提出提高模型效率的策略。&lt;h4&gt;方法&lt;/h4&gt;通过移除或修改位置编码以及反转帧序列等对比实验，分析时间信息在模型中的整合路径；基于分析结果提出分阶段跨模态注意力和时间退出机制两种策略。&lt;h4&gt;主要发现&lt;/h4&gt;1. 移除或修改视频输入中的位置编码对时间理解性能影响很小；2. 保留原始位置编码但反转帧序列会导致性能大幅下降；3. 时间信息通过帧间注意力逐步合成，在最后一帧中聚合，然后被整合到查询标记中；4. 时间推理是在因果注意力约束下，通过视觉标记间的交互产生的。&lt;h4&gt;结论&lt;/h4&gt;视频语言模型中的时间理解主要依赖于帧间交互而非位置编码，基于这一发现提出的两种效率策略在基准测试上被证明有效。&lt;h4&gt;翻译&lt;/h4&gt;视频语言模型(VideoLMs)在多模态理解方面已经取得了显著进展。然而，时间理解（涉及识别事件顺序、持续时间和跨时间关系）仍然是一个核心挑战。先前的研究强调位置编码(PEs)是编码时间结构的关键机制。令人惊讶的是，我们发现移除或修改视频输入中的PEs对时间理解性能的影响很小。相反，保留原始PEs但反转帧序列会导致性能大幅下降。为了解释这一行为，我们进行了大量分析实验来追踪时间信息在模型中的整合方式。我们揭示了一个因果信息通路：时间线索通过帧间注意力逐步合成，在最后一帧中聚合，然后被整合到查询标记中。这种新兴的机制表明，时间推理是在因果注意力约束下，通过视觉标记间的交互产生的，这隐式地编码了时间结构。基于这些见解，我们提出了两种面向效率的策略：分阶段跨模态注意力和时间退出机制（用于早期标记截断）。在两个基准测试上的实验验证了这两种方法的有效性。据我们所知，这是第一个系统研究VideoLMs中时间理解的工作，为未来模型改进提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video language models (VideoLMs) have made significant progress in multimodalunderstanding. However, temporal understanding, which involves identifyingevent order, duration, and relationships across time, still remains a corechallenge. Prior works emphasize positional encodings (PEs) as a key mechanismfor encoding temporal structure. Surprisingly, we find that removing ormodifying PEs in video inputs yields minimal degradation in the performance oftemporal understanding. In contrast, reversing the frame sequence whilepreserving the original PEs causes a substantial drop. To explain thisbehavior, we conduct substantial analysis experiments to trace how temporalinformation is integrated within the model. We uncover a causal informationpathway: temporal cues are progressively synthesized through inter-frameattention, aggregated in the final frame, and subsequently integrated into thequery tokens. This emergent mechanism shows that temporal reasoning emergesfrom inter-visual token interactions under the constraints of causal attention,which implicitly encodes temporal structure. Based on these insights, wepropose two efficiency-oriented strategies: staged cross-modal attention and atemporal exit mechanism for early token truncation. Experiments on twobenchmarks validate the effectiveness of both approaches. To the best of ourknowledge, this is the first work to systematically investigate video temporalunderstanding in VideoLMs, offering insights for future model improvement.</description>
      <author>example@mail.com (Yumeng Shi, Quanyu Long, Yin Wu, Wenya Wang)</author>
      <guid isPermaLink="false">2508.11576v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding</title>
      <link>http://arxiv.org/abs/2508.11357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PTSM（生理感知和任务不变的时空建模）框架，用于可解释和鲁棒的跨被试脑电图解码，通过双分支掩码机制和信息论约束实现零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;跨被试脑电图解码在脑机接口研究中是基本挑战，主要由于被试间存在显著变性和缺乏被试不变的表征。&lt;h4&gt;目的&lt;/h4&gt;开发一个新颖的、可解释且鲁棒的脑电图解码框架，能够在未见过的被试上实现零样本泛化，同时保留个体特定神经特征并提取任务相关共享特征。&lt;h4&gt;方法&lt;/h4&gt;PTSM采用双分支掩码机制独立学习个性化和共享时空模式，在时间和空间维度上分解掩码以精细调制动态脑电图模式，通过信息论约束将潜在嵌入分解为正交的任务相关和被试相关子空间，并使用整合分类、对比和解纠缠目标的多目标损失进行端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;在跨被试运动想象数据集上的实验表明，PTSM实现了强大的零样本泛化能力，无需被试特定校准即可超越最先进基线方法。&lt;h4&gt;结论&lt;/h4&gt;解缠的神经表征对于在非平稳神经生理学环境中实现个性化和可转移解码非常有效，PTSM框架成功解决了跨被试脑电图解码中的基本挑战。&lt;h4&gt;翻译&lt;/h4&gt;跨被试脑电图（EEG）解码由于被试间存在显著差异和缺乏被试不变的表征，在脑机接口（BCI）研究中仍然是一个基本挑战。本文提出了PTSM（生理感知和任务不变的时空建模），一种用于可解释和鲁棒的跨被试EEG解码的新框架。PTSM采用双分支掩码机制，独立学习个性化和共享的时空模式，使模型能够在提取任务相关的、群体共享特征的同时保留个体特定的神经特征。掩码在时间和空间维度上进行分解，允许以低计算开销精细调制动态EEG模式。为了进一步解决表征纠缠问题，PTSM强制执行信息论约束，将潜在嵌入分解为正交的任务相关和被试相关子空间。该模型通过整合分类、对比和解纠缠目标的多目标损失进行端到端训练。在跨被试运动想象数据集上的大量实验表明，PTSM实现了强大的零样本泛化能力，无需被试特定校准即可超越最先进的基线。结果强调了在非平稳神经生理学环境中实现个性化和可转移解码的解缠神经表征的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-subject electroencephalography (EEG) decoding remains a fundamentalchallenge in brain-computer interface (BCI) research due to substantialinter-subject variability and the scarcity of subject-invariantrepresentations. This paper proposed PTSM (Physiology-aware and Task-invariantSpatio-temporal Modeling), a novel framework for interpretable and robust EEGdecoding across unseen subjects. PTSM employs a dual-branch masking mechanismthat independently learns personalized and shared spatio-temporal patterns,enabling the model to preserve individual-specific neural characteristics whileextracting task-relevant, population-shared features. The masks are factorizedacross temporal and spatial dimensions, allowing fine-grained modulation ofdynamic EEG patterns with low computational overhead. To further addressrepresentational entanglement, PTSM enforces information-theoretic constraintsthat decompose latent embeddings into orthogonal task-related andsubject-related subspaces. The model is trained end-to-end via amulti-objective loss integrating classification, contrastive, anddisentanglement objectives. Extensive experiments on cross-subject motorimagery datasets demonstrate that PTSM achieves strong zero-shotgeneralization, outperforming state-of-the-art baselines withoutsubject-specific calibration. Results highlight the efficacy of disentangledneural representations for achieving both personalized and transferabledecoding in non-stationary neurophysiological settings.</description>
      <author>example@mail.com (Changhong Jing, Yan Liu, Shuqiang Wang, Bruce X. B. Yu, Gong Chen, Zhejing Hu, Zhi Zhang, Yanyan Shen)</author>
      <guid isPermaLink="false">2508.11357v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Video Temporal Grounding with Multimodal Large Language Model</title>
      <link>http://arxiv.org/abs/2508.10922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages,6 figures,survey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是对基于多模态大语言模型(MLLMs)的视频时序定位(VTG-MLLMs)技术的全面综述研究。通过三维分类法系统性地分析了当前VTG-MLLMs的研究，包括MLLMs的功能角色、训练范式和视频特征处理技术。同时讨论了基准数据集、评估协议，总结了经验发现，并指出了现有局限性和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;视频时序定位(VTG)技术的最新进展显著提升了细粒度的视频理解能力，这主要得益于多模态大语言模型(MLLMs)的发展。基于MLLMs的VTG方法凭借其卓越的多模态理解和推理能力，正逐渐超越传统的微调方法，在零样本、多任务和多领域设置中表现出强大的泛化能力。尽管已有大量关于一般视频语言理解的调查，但专门针对VTG-MLLMs的全面综述仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;填补现有文献中针对VTG-MLLMs全面综述的空白，系统性地审视当前VTG-MLLMs的研究，为该领域的研究者提供全面的技术框架和未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;采用三维分类法系统性地分析VTG-MLLMs的研究：1) MLLMs的功能角色，强调其架构重要性；2) 训练范式，分析时间推理和任务适应的策略；3) 视频特征处理技术，这些技术决定了时空表示的有效性。此外，还讨论了基准数据集、评估协议，并总结了经验发现。&lt;h4&gt;主要发现&lt;/h4&gt;1) MLLMs在VTG任务中展现出强大的多模态理解和推理能力；2) VTG-MLLMs在性能上超越了传统的微调方法；3) VTG-MLLMs在零样本、多任务和多领域设置中表现出良好的泛化能力；4) 不同的MLLM架构、训练策略和视频特征处理方法对VTG任务性能有显著影响。&lt;h4&gt;结论&lt;/h4&gt;VTG-MLLMs代表了视频理解领域的重大进步，但仍存在一些局限性。论文通过系统性的综述和三维分类法，为该领域的研究者提供了全面的技术框架，并提出了有希望的未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;视频时序定位(VTG)的最新进展显著增强了细粒度视频理解能力，这主要得益于多模态大语言模型(MLLMs)的发展。凭借卓越的多模态理解和推理能力，基于MLLMs的VTG方法(VTG-MLLMs)正逐渐超越传统的微调方法。它们不仅实现了具有竞争力的性能，还在零样本、多任务和多领域设置中表现出色。尽管已有大量关于一般视频语言理解的调查，但专门针对VTG-MLLMs的全面综述仍然稀缺。为了填补这一空白，本综述通过三维分类法系统地审视了当前VTG-MLLMs的研究：1) MLLMs的功能角色，强调其架构重要性；2) 训练范式，分析时间推理和任务适应的策略；3) 视频特征处理技术，这些技术决定了时空表示的有效性。我们进一步讨论了基准数据集、评估协议，并总结了经验发现。最后，我们确定了现有局限性并提出了有希望的研究方向。有关更多资源和详细信息，读者可访问我们的存储库：https://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent advancement in video temporal grounding (VTG) has significantlyenhanced fine-grained video understanding, primarily driven by multimodal largelanguage models (MLLMs). With superior multimodal comprehension and reasoningabilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassingtraditional fine-tuned methods. They not only achieve competitive performancebut also excel in generalization across zero-shot, multi-task, and multi-domainsettings. Despite extensive surveys on general video-language understanding,comprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fillthis gap, this survey systematically examines current research on VTG-MLLMsthrough a three-dimensional taxonomy: 1) the functional roles of MLLMs,highlighting their architectural significance; 2) training paradigms, analyzingstrategies for temporal reasoning and task adaptation; and 3) video featureprocessing techniques, which determine spatiotemporal representationeffectiveness. We further discuss benchmark datasets, evaluation protocols, andsummarize empirical findings. Finally, we identify existing limitations andpropose promising research directions. For additional resources and details,readers are encouraged to visit our repository athttps://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.</description>
      <author>example@mail.com (Jianlong Wu, Wei Liu, Ye Liu, Meng Liu, Liqiang Nie, Zhouchen Lin, Chang Wen Chen)</author>
      <guid isPermaLink="false">2508.10922v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Representing Speech Through Autoregressive Prediction of Cochlear Tokens</title>
      <link>http://arxiv.org/abs/2508.11598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AuriStream是一种受生物启发的语音编码模型，采用两阶段框架模拟人类听觉处理层次结构。&lt;h4&gt;背景&lt;/h4&gt;人类听觉系统处理声音的方式为语音编码提供了灵感。&lt;h4&gt;目的&lt;/h4&gt;推进更类人模型的发展，这些模型能有效处理各种基于语音的任务。&lt;h4&gt;方法&lt;/h4&gt;第一阶段将原始音频转换为基于人类耳蜗的时间-频率表示并提取耳蜗标记；第二阶段在耳蜗标记上应用自回归序列模型。&lt;h4&gt;主要发现&lt;/h4&gt;AuriStream能学习有意义的音素和词表示，实现最先进的词汇语义，在SUPERB语音任务上表现优异，并能生成可可视化和解码的音频连续。&lt;h4&gt;结论&lt;/h4&gt;两阶段语音表示学习框架有助于开发更类人、高效处理语音任务的模型。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了AuriStream，一种受生物启发的语音编码模型，采用受人类听觉处理层次结构启发的两阶段框架。第一阶段将原始音频转换为基于人类耳蜗的时间-频率表示，从中提取离散的耳蜗标记。第二阶段在耳蜗标记上应用自回归序列模型。AuriStream学习有意义的音素和词表示以及最先进的词汇语义。AuriStream在多样化的SUPERB语音任务上展现出竞争力。补充AuriStream强大的表示能力，它能生成音频连续，可在频谱图空间中可视化并解码回音频，为模型的预测提供见解。总之，我们提出了一个语音表示学习的两阶段框架，以推进更类人模型的发展，这些模型能有效处理各种基于语音的任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce AuriStream, a biologically inspired model for encoding speechvia a two-stage framework inspired by the human auditory processing hierarchy.The first stage transforms raw audio into a time-frequency representation basedon the human cochlea, from which we extract discrete \textbf{cochlear tokens}.The second stage applies an autoregressive sequence model over the cochleartokens. AuriStream learns meaningful phoneme and word representations, andstate-of-the-art lexical semantics. AuriStream shows competitive performance ondiverse downstream SUPERB speech tasks. Complementing AuriStream's strongrepresentational capabilities, it generates continuations of audio which can bevisualized in a spectrogram space and decoded back into audio, providinginsights into the model's predictions. In summary, we present a two-stageframework for speech representation learning to advance the development of morehuman-like models that efficiently handle a range of speech-based tasks.</description>
      <author>example@mail.com (Greta Tuckute, Klemen Kotar, Evelina Fedorenko, Daniel L. K. Yamins)</author>
      <guid isPermaLink="false">2508.11598v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity</title>
      <link>http://arxiv.org/abs/2508.11442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoDiEmb是一个统一框架，解决了信息检索(IR)和语义文本相似性(STS)联合训练中的负迁移问题，通过任务特定目标、动态采样器、基于delta的模型融合策略和高效单阶段训练流程实现了有效联合优化。&lt;h4&gt;背景&lt;/h4&gt;统一文本嵌入学习是一个重要目标，但负迁移仍然是一个持续存在的障碍。特别是当联合训练一个编码器用于信息检索(IR)和语义文本相似性(STS)这两个基本但根本不同的任务时，这种挑战尤为明显，因为简单的共同训练通常会导致性能的急剧权衡。&lt;h4&gt;目的&lt;/h4&gt;解决IR和STS联合训练中的冲突，需要在整个训练流程中系统地解耦任务特定的学习信号。&lt;h4&gt;方法&lt;/h4&gt;CoDiEmb框架整合了三个关键创新：(1)任务特定目标与动态采样器配对，形成单任务批次并平衡每个任务的更新，防止梯度干扰。对于IR，采用具有多个正样本和难负样本的对比损失；对于STS，采用直接优化相关性和排序一致性的目标。(2)基于delta的模型融合策略，通过分析参数偏差计算检查点的细粒度合并权重。(3)高效的单阶段训练流程，易于实现且稳定收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在15个标准IR和STS基准测试上对三个基础编码器进行的广泛实验验证了CoDiEmb。该框架不仅减轻了跨任务权衡，而且显著改善了嵌入空间的几何特性。&lt;h4&gt;结论&lt;/h4&gt;CoDiEmb框架成功地解决了IR和STS联合训练中的负迁移问题，并提高了嵌入质量。&lt;h4&gt;翻译&lt;/h4&gt;学习在多种下游任务中表现出色的统一文本嵌入是表征学习的中心目标，但负迁移仍然是一个持续存在的障碍。当联合训练单个编码器用于信息检索(IR)和语义文本相似性(STS)这两个基本但根本不同的任务时，这一挑战尤为突出，因为简单的共同训练通常会导致严重的性能权衡。我们认为，解决这种冲突需要在整个训练流程中系统地解耦任务特定的学习信号。为此，我们引入了CoDiEmb，一个统一框架，以协作但独特的方式协调IR和STS的不同需求。CoDiEmb整合了三个关键创新以实现有效的联合优化：(1)任务特定目标与动态采样器配对，形成单任务批次并平衡每个任务的更新，从而防止梯度干扰。对于IR，我们采用具有多个正样本和难负样本的对比损失，并通过跨设备采样增强。对于STS，我们采用直接优化相关性和排序一致性的顺序感知目标。(2)一种基于delta的模型融合策略，通过分析每个参数与预训练初始化的偏差来计算检查点的细粒度合并权重，证明比传统的模型汤更有效。(3)一种高效的单阶段训练流程，易于实现且稳定收敛。在15个标准IR和STS基准测试上对三个基础编码器进行的广泛实验验证了CoDiEmb。我们的结果和分析表明，该框架不仅减轻了跨任务权衡，而且显著改善了嵌入空间的几何特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning unified text embeddings that excel across diverse downstream tasksis a central goal in representation learning, yet negative transfer remains apersistent obstacle. This challenge is particularly pronounced when jointlytraining a single encoder for Information Retrieval (IR) and Semantic TextualSimilarity (STS), two essential but fundamentally disparate tasks for whichnaive co-training typically yields steep performance trade-offs. We argue thatresolving this conflict requires systematically decoupling task-specificlearning signals throughout the training pipeline. To this end, we introduceCoDiEmb, a unified framework that reconciles the divergent requirements of IRand STS in a collaborative yet distinct manner. CoDiEmb integrates three keyinnovations for effective joint optimization: (1) Task-specialized objectivespaired with a dynamic sampler that forms single-task batches and balancesper-task updates, thereby preventing gradient interference. For IR, we employ acontrastive loss with multiple positives and hard negatives, augmented bycross-device sampling. For STS, we adopt order-aware objectives that directlyoptimize correlation and ranking consistency. (2) A delta-guided model fusionstrategy that computes fine-grained merging weights for checkpoints byanalyzing each parameter's deviation from its pre-trained initialization,proving more effective than traditional Model Soups. (3) An efficient,single-stage training pipeline that is simple to implement and convergesstably. Extensive experiments on 15 standard IR and STS benchmarks across threebase encoders validate CoDiEmb. Our results and analysis demonstrate that theframework not only mitigates cross-task trade-offs but also measurably improvesthe geometric properties of the embedding space.</description>
      <author>example@mail.com (Bowen Zhang, Zixin Song, Chunquan Chen, Qian-Wen Zhang, Di Yin, Xing Sun)</author>
      <guid isPermaLink="false">2508.11442v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Towards the Next-generation Bayesian Network Classifiers</title>
      <link>http://arxiv.org/abs/2508.11145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型的高阶贝叶斯网络分类器范式，通过学习特征值的分布表示来捕捉高阶特征依赖关系，显著提升了贝叶斯网络分类器在复杂数据上的性能。&lt;h4&gt;背景&lt;/h4&gt;贝叶斯网络分类器是表格数据分类的可行解决方案，具有高效、内存占用小和可解释性强等优点。然而，由于参数爆炸和数据稀疏问题，传统贝叶斯网络分类器仅限于低阶特征依赖建模，难以推断复杂数据的发生概率。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在设计能够捕捉高阶特征依赖的贝叶斯网络分类器，以克服传统方法的局限性，提高在复杂数据上的分类性能。&lt;h4&gt;方法&lt;/h4&gt;研究者提出了一种新范式，通过学习特征值的分布表示（类似于词嵌入和图表示学习），将不同特征间的语义相关性通过训练数据中的共同出现模式进行编码，从而推断新测试样本的发生概率。作为具体实现，研究者将K依赖贝叶斯分类器扩展为神经版本NeuralKDB，设计了新的神经网络架构来学习特征值的分布表示并参数化特征间的条件概率，并基于随机梯度下降设计了高效训练算法。&lt;h4&gt;主要发现&lt;/h4&gt;在60个UCI数据集上的大量分类实验表明，所提出的NeuralKDB分类器在捕捉高阶特征依赖方面表现出色，显著优于传统贝叶斯网络分类器以及其他竞争分类器，包括两种不进行分布表示学习的神经网络分类器。&lt;h4&gt;结论&lt;/h4&gt;通过引入分布表示学习，新型的高阶贝叶斯网络分类器能够有效捕捉特征间的高阶依赖关系，显著提升分类性能，为表格数据分类提供了更强大的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;贝叶斯网络分类器为表格数据分类提供了可行的解决方案，具有高效、内存占用小和可解释性强等优点。然而，由于参数爆炸和数据稀疏问题，贝叶斯网络分类器仅限于低阶特征依赖建模，难以推断复杂数据的发生概率。在本文中，我们提出了一种设计高阶贝叶斯网络分类器的新范式，通过学习特征值的分布表示，类似于词嵌入和图表示学习。学习到的分布表示通过训练数据中观察到的共同出现模式编码了不同特征之间的语义相关性，从而推断新测试样本的发生概率。作为分类器设计的具体实现，我们将K依赖贝叶斯分类器扩展为神经版本NeuralKDB，设计了新的神经网络架构来学习特征值的分布表示并参数化相互依赖特征之间的条件概率。设计了一种基于随机梯度下降的高效算法来训练NeuralKDB模型。在60个UCI数据集上的大量分类实验表明，所提出的NeuralKDB分类器在捕捉高阶特征依赖方面表现出色，显著优于传统贝叶斯网络分类器以及其他竞争分类器，包括两种不进行分布表示学习的神经网络分类器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bayesian network classifiers provide a feasible solution to tabular dataclassification, with a number of merits like high time and memory efficiency,and great explainability. However, due to the parameter explosion and datasparsity issues, Bayesian network classifiers are restricted to low-orderfeature dependency modeling, making them struggle in extrapolating theoccurrence probabilities of complex real-world data. In this paper, we proposea novel paradigm to design high-order Bayesian network classifiers, by learningdistributional representations for feature values, as what has been done inword embedding and graph representation learning. The learned distributionalrepresentations are encoded with the semantic relatedness between differentfeatures through their observed co-occurrence patterns in training data, whichthen serve as a hallmark to extrapolate the occurrence probabilities of newtest samples. As a classifier design realization, we remake the K-dependenceBayesian classifier (KDB) by extending it into a neural version, i.e.,NeuralKDB, where a novel neural network architecture is designed to learndistributional representations of feature values and parameterize theconditional probabilities between interdependent features. A stochasticgradient descent based algorithm is designed to train the NeuralKDB modelefficiently. Extensive classification experiments on 60 UCI datasetsdemonstrate that the proposed NeuralKDB classifier excels in capturinghigh-order feature dependencies and significantly outperforms the conventionalBayesian network classifiers, as well as other competitive classifiers,including two neural network based classifiers without distributionalrepresentation learning.</description>
      <author>example@mail.com (Huan Zhang, Daokun Zhang, Kexin Meng, Geoffrey I. Webb)</author>
      <guid isPermaLink="false">2508.11145v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning</title>
      <link>http://arxiv.org/abs/2508.10298v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SynBrain框架，一种概率性和生物可解释的生成模型，用于模拟视觉语义到神经反应的转化，解决了视觉到神经映射中的一对多关系挑战。&lt;h4&gt;背景&lt;/h4&gt;视觉刺激转化为皮层反应是计算神经科学的基本挑战，视觉到神经映射本质上是多对一关系，相同视觉输入在不同试验、背景和受试者间会引发不同的血氧反应。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时模拟生物变异性并捕捉编码刺激信息的基础功能一致性的方法，以确定性方法难以实现的方式处理视觉到神经的映射。&lt;h4&gt;方法&lt;/h4&gt;SynBrain框架包含两个关键组件：(i) BrainVAE模型通过概率学习将神经表示为连续概率分布，同时通过视觉语义约束保持功能一致性；(ii) 语义到神经映射器作为语义传输途径，将视觉语义投影到神经响应流形中，促进高保真fMRI合成。&lt;h4&gt;主要发现&lt;/h4&gt;SynBrain在特定受试者的视觉到fMRI编码性能方面超越最先进方法；能高效适应新受试者，使用少量数据合成高质量fMRI信号，有效提高数据有限的fMRI到图像解码性能；揭示了试验和受试者间的功能一致性，合成的信号捕捉到由生物神经变异性形成的可解释模式。&lt;h4&gt;结论&lt;/h4&gt;SynBrain是一个有效的生成框架，能够模拟视觉到神经的转化，同时处理生物变异性并保持功能一致性，代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;解析视觉刺激如何转化为皮层反应是计算神经科学的一个基本挑战。这种视觉到神经的映射本质上是一对多关系，因为相同的视觉输入在试验、背景和受试者之间会可靠地引发不同的血氧反应。然而，现有的确定性方法难以同时模拟这种生物变异性，同时又能捕捉编码刺激信息的基础功能一致性。为解决这些局限性，我们提出了SynBrain，一个生成框架，以概率性和生物可解释的方式模拟视觉语义到神经反应的转化。SynBrain引入两个关键组件：(i) BrainVAE模型通过概率学习将神经表示为连续概率分布，同时通过视觉语义约束保持功能一致性；(ii) 语义到神经映射器作为语义传输途径，将视觉语义投影到神经响应流形中，促进高保真fMRI合成。实验结果表明，SynBrain在特定受试者的视觉到fMRI编码性能方面超越了最先进的方法。此外，SynBrain能够高效适应新受试者，使用少量数据，并合成高质量的fMRI信号，这些信号能有效提高数据有限的fMRI到图像解码性能。除此之外，SynBrain揭示了试验和受试者之间的功能一致性，合成的信号捕捉到由生物神经变异性形成的可解释模式。代码将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deciphering how visual stimuli are transformed into cortical responses is afundamental challenge in computational neuroscience. This visual-to-neuralmapping is inherently a one-to-many relationship, as identical visual inputsreliably evoke variable hemodynamic responses across trials, contexts, andsubjects. However, existing deterministic methods struggle to simultaneouslymodel this biological variability while capturing the underlying functionalconsistency that encodes stimulus information. To address these limitations, wepropose SynBrain, a generative framework that simulates the transformation fromvisual semantics to neural responses in a probabilistic and biologicallyinterpretable manner. SynBrain introduces two key components: (i) BrainVAEmodels neural representations as continuous probability distributions viaprobabilistic learning while maintaining functional consistency through visualsemantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantictransmission pathway, projecting visual semantics into the neural responsemanifold to facilitate high-fidelity fMRI synthesis. Experimental resultsdemonstrate that SynBrain surpasses state-of-the-art methods insubject-specific visual-to-fMRI encoding performance. Furthermore, SynBrainadapts efficiently to new subjects with few-shot data and synthesizeshigh-quality fMRI signals that are effective in improving data-limitedfMRI-to-image decoding performance. Beyond that, SynBrain reveals functionalconsistency across trials and subjects, with synthesized signals capturinginterpretable patterns shaped by biological neural variability. The code willbe made publicly available.</description>
      <author>example@mail.com (Weijian Mai, Jiamin Wu, Yu Zhu, Zhouheng Yao, Dongzhan Zhou, Andrew F. Luo, Qihao Zheng, Wanli Ouyang, Chunfeng Song)</author>
      <guid isPermaLink="false">2508.10298v2</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Mining the Social Fabric: Unveiling Communities for Fake News Detection in Short Videos</title>
      <link>http://arxiv.org/abs/2508.07992v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  in submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DugFND是一种基于双社区图的虚假新闻检测方法，通过建模上传者社区和事件驱动社区，结合异构图注意力网络和预训练技术，显著提升了短视频平台上的虚假新闻检测性能。&lt;h4&gt;背景&lt;/h4&gt;短视频平台已成为信息分享的主要媒介，但其快速内容生成和算法放大也导致虚假新闻广泛传播。检测短视频中的虚假新闻具有挑战性，因为它们是多模态的，且单个视频的上下文有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法忽视视频、上传者和事件之间隐含关系的问题，提出一种增强现有视频分类器的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出DugFND方法，建模两种关键社区模式：(1)上传者社区，具有共同兴趣或相似内容创作模式的上传者聚集；(2)事件驱动社区，与相同或语义相似公共事件相关的视频形成局部集群。构建连接上传者、视频和事件节点的异构图，设计时间感知的异构图注意力网络，并通过基于预训练的重建阶段改进节点表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验在公共数据集上显示，DugFND方法取得了显著的性能提升，证明了双社区建模对短视频虚假新闻检测的价值。&lt;h4&gt;结论&lt;/h4&gt;双社区建模方法有效提升了短视频虚假新闻检测的准确性，DugFND可应用于任何预训练的分类器。&lt;h4&gt;翻译&lt;/h4&gt;短视频平台已成为信息分享的主要媒介，但其快速的内容生成和算法放大也导致虚假新闻的广泛传播。由于短视频的多模态性质和单个视频的有限上下文，检测短视频中的虚假新闻具有挑战性。尽管最近的方法侧重于分析内容信号-视觉、文本和音频-但它们常常忽视视频、上传者和事件之间的隐含关系。为解决这一差距，我们提出了DugFND（双社区图虚假新闻检测），一种新颖的方法，通过建模两种关键的社区模式来增强现有的视频分类器：(1)上传者社区，具有共同兴趣或相似内容创作模式的上传者聚集在一起；(2)事件驱动社区，与相同或语义相似的公共事件相关的视频形成局部集群。我们构建了一个连接上传者、视频和事件节点的异构图，并设计了一个时间感知的异构图注意力网络以实现有效的消息传递。基于预训练的重建阶段进一步改进了节点表示学习。DugFND可以应用于任何预训练的分类器。公共数据集上的实验表明，我们的方法取得了显著的性能提升，证明了双社区建模对短视频虚假新闻检测的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Short video platforms have become a major medium for information sharing, buttheir rapid content generation and algorithmic amplification also enable thewidespread dissemination of fake news. Detecting misinformation in short videosis challenging due to their multi-modal nature and the limited context ofindividual videos. While recent methods focus on analyzing contentsignals-visual, textual, and audio-they often overlook implicit relationshipsamong videos, uploaders, and events. To address this gap, we propose DugFND(Dual-community graph for fake news detection), a novel method that enhancesexisting video classifiers by modeling two key community patterns: (1) uploadercommunities, where uploaders with shared interests or similar content creationpatterns group together, and (2) event-driven communities, where videos relatedto the same or semantically similar public events form localized clusters. Weconstruct a heterogeneous graph connecting uploader, video, and event nodes,and design a time-aware heterogeneous graph attention network to enableeffective message passing. A reconstruction-based pretraining phase furtherimproves node representation learning. DugFND can be applied to any pre-trainedclassifier. Experiments on public datasets show that our method achievessignificant performance gains, demonstrating the value of dual-communitymodeling for fake news detection in short videos.</description>
      <author>example@mail.com (Haisong Gong, Bolan Su, Xinrong Zhang, Jing Li, Qiang Liu, Shu Wu, Liang Wang)</author>
      <guid isPermaLink="false">2508.07992v2</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Deconfounding via Profiled Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.11622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为ProTrans的profiled transfer learning框架，用于解决回归效应估计和因果推断中未测量混杂因素导致的偏差问题。&lt;h4&gt;背景&lt;/h4&gt;未测量的混杂因素是回归效应估计和因果推断中的主要偏差来源。&lt;h4&gt;目的&lt;/h4&gt;当存在具有相似混杂结构的其他源数据集时，提出ProTrans框架来解决目标数据集中的混杂效应问题。&lt;h4&gt;方法&lt;/h4&gt;引入profiled residuals概念描述源数据集和目标数据集间共享的混杂模式，将这些profiled residuals整合到目标去偏步骤中减轻潜在混杂效应，并提出源选择策略增强ProTrans对非信息性源的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;ProTrans无需使用工具变量或代理变量等难以选择的辅助特征即可估计处理效应；理论上证明从源到目标转换的估计模型无混杂且无需对真实混杂结构做假设；在温和条件下目标参数估计达到最小最优速率。&lt;h4&gt;结论&lt;/h4&gt;模拟和真实实验验证了ProTrans的有效性并支持理论发现。&lt;h4&gt;翻译&lt;/h4&gt;未测量的混杂因素是回归效应估计和因果推断中的主要偏差来源。在本文中，我们提出了一种新的profiled transfer learning框架ProTrans，当存在具有相似混杂结构的额外源数据集时，用于解决目标数据集中的混杂效应问题。我们引入了profiled residuals的概念来表征源数据集和目标数据集之间的共享混杂模式。通过将这些profiled residuals整合到目标去偏步骤中，我们有效减轻了潜在的混杂效应。我们还提出了一种源选择策略，以增强ProTrans对非信息性源的鲁棒性。作为副产品，ProTrans也可以用于估计存在潜在混杂因素时的处理效应，而无需使用工具变量或代理变量等辅助特征，这些变量在实践中往往难以选择。理论上，我们证明从源到目标转换的估计模型没有混杂，无需对真实混杂结构做出任何假设，并且在温和条件下目标参数估计达到最小最优速率。模拟和真实实验验证了ProTrans的有效性并支持了理论发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmeasured confounders are a major source of bias in regression-based effectestimation and causal inference. In this paper, we advocate a new profiledtransfer learning framework, ProTrans, to address confounding effects in thetarget dataset, when additional source datasets that possess similarconfounding structures are available. We introduce the concept of profiledresiduals to characterize the shared confounding patterns between source andtarget datasets. By incorporating these profiled residuals into the targetdebiasing step, we effectively mitigates the latent confounding effects. Wealso propose a source selection strategy to enhance robustness of ProTransagainst noninformative sources. As a byproduct, ProTrans can also be utilizedto estimate treatment effects when potential confounders exist, without the useof auxiliary features such as instrumental or proxy variables, which are oftenchallenging to select in practice. Theoretically, we prove that the resultingestimated model shift from sources to target is confounding-free without anyassumptions imposed on the true confounding structure, and that the targetparameter estimation achieves the minimax optimal rate under mild conditions.Simulated and real-world experiments validate the effectiveness of ProTrans andsupport the theoretical findings.</description>
      <author>example@mail.com (Ziyuan Chen, Yifan Jiang, Jingyuan Liu, Fang Yao)</author>
      <guid isPermaLink="false">2508.11622v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Holistic Bioprocess Development Across Scales Using Multi-Fidelity Batch Bayesian Optimization</title>
      <link>http://arxiv.org/abs/2508.10970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种多保真度批次贝叶斯优化框架，用于加速生物过程开发并降低实验成本，通过整合高斯过程进行多保真度建模和混合变量优化，指导跨尺度和生物催化剂的实验选择，案例研究证实了该框架能够降低实验成本并提高产量。&lt;h4&gt;背景&lt;/h4&gt;生物过程在现代生物技术中处于核心地位，用于制药、特种化学品、化妆品和食品的可持续生产。然而，开发高性能过程成本高且复杂，需要从微孔板到中试反应器的迭代、多尺度实验。传统的实验设计方法难以解决过程放大以及反应条件和生物催化剂选择的联合优化问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种多保真度批次贝叶斯优化框架，以加速生物过程开发并降低实验成本。&lt;h4&gt;方法&lt;/h4&gt;该方法整合了专门用于多保真度建模和混合变量优化的高斯过程，指导跨尺度和生物催化剂的实验选择。使用中国仓鼠卵巢生物过程的定制模拟进行基准测试，该模拟捕捉了非线性和耦合的放大动力学特性，并与多个模拟的工业实验设计基线进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;多个案例研究表明，所提出的工作流程可以实现实验成本的降低和产量的提高。&lt;h4&gt;结论&lt;/h4&gt;这项工作为生物过程优化提供了数据高效策略，并强调了可持续生物技术中迁移学习和不确定性感知设计的未来机会。&lt;h4&gt;翻译&lt;/h4&gt;生物过程是现代生物技术的核心，能够在制药、特种化学品、化妆品和食品领域实现可持续生产。然而，开发高性能过程成本高昂且复杂，需要从微孔板到中试反应器的迭代、多尺度实验。传统的实验设计方法往往难以解决过程放大以及反应条件和生物催化剂选择的联合优化问题。我们提出了一种多保真度批次贝叶斯优化框架，以加速生物过程开发并降低实验成本。该方法整合了专门用于多保真度建模和混合变量优化的高斯过程，指导跨尺度和生物催化剂的实验选择。使用中国仓鼠卵巢生物过程的定制模拟进行基准测试，该模拟捕捉了非线性和耦合的放大动力学特性，并与多个模拟的工业实验设计基线进行比较。多个案例研究表明，所提出的工作流程可以实现实验成本的降低和产量的提高。这项工作为生物过程优化提供了数据高效策略，并强调了可持续生物技术中迁移学习和不确定性感知设计的未来机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bioprocesses are central to modern biotechnology, enabling sustainableproduction in pharmaceuticals, specialty chemicals, cosmetics, and food.However, developing high-performing processes is costly and complex, requiringiterative, multi-scale experimentation from microtiter plates to pilotreactors. Conventional Design of Experiments (DoE) approaches often struggle toaddress process scale-up and the joint optimization of reaction conditions andbiocatalyst selection.  We propose a multi-fidelity batch Bayesian optimization framework toaccelerate bioprocess development and reduce experimental costs. The methodintegrates Gaussian Processes tailored for multi-fidelity modeling andmixed-variable optimization, guiding experiment selection across scales andbiocatalysts. A custom simulation of a Chinese Hamster Ovary bioprocess,capturing non-linear and coupled scale-up dynamics, is used for benchmarkingagainst multiple simulated industrial DoE baselines. Multiple case studies showhow the proposed workflow can achieve a reduction in experimental costs andincreased yield.  This work provides a data-efficient strategy for bioprocess optimization andhighlights future opportunities in transfer learning and uncertainty-awaredesign for sustainable biotechnology.</description>
      <author>example@mail.com (Adrian Martens, Mathias Neufang, Alessandro Butté, Moritz von Stosch, Antonio del Rio Chanona, Laura Marie Helleckes)</author>
      <guid isPermaLink="false">2508.10970v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>Opportunities and Applications of GenAI in Smart Cities: A User-Centric Survey</title>
      <link>http://arxiv.org/abs/2505.08034v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE COINS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了生成式人工智能(GenAI)在智能城市中的应用，重点关注面向市民、运营商和规划者三种关键用户类型的对话界面应用，并分析了基于城市现有数据基础构建GenAI的方法。&lt;h4&gt;背景&lt;/h4&gt;物联网在城市中的普及与数字孪生的结合为智能城市提供了丰富的数据基础，旨在改善城市生活和运营。&lt;h4&gt;目的&lt;/h4&gt;确定并回顾针对智能城市中三种关键用户类型(市民、运营商和规划者)的GenAI模型和技术，并分析如何利用城市现有数据基础构建GenAI应用。&lt;h4&gt;方法&lt;/h4&gt;聚焦于基于对话界面的GenAI应用，分析针对不同用户类型的各种城市子系统所提出或部署的GenAI模型和技术。&lt;h4&gt;主要发现&lt;/h4&gt;GenAI能处理多模态内容并生成新的输出，其自然语言能力可以支持定制化应用和统一界面，降低用户与复杂智能系统交互的门槛；GenAI可以建立在官方城市记录、物联网数据流和城市数字孪生的现有数据基础上。&lt;h4&gt;结论&lt;/h4&gt;这项工作代表了从智能城市关键用户角度对智能城市GenAI技术的首次全面总结，为未来研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;物联网在城市中的普及，结合数字孪生技术，为旨在改善城市生活和运营的智能城市创造了丰富的数据基础。生成式人工智能(GenAI)显著增强了这一潜力，通过处理多模态内容和生成文本和模拟等新颖输出，超越了传统的AI分析和预测。利用专业或基础模型，GenAI的自然语言能力(如自然语言理解NLU和自然语言生成NLG)可以为定制化应用和统一界面提供支持，大大降低了用户与复杂智能城市系统交互的门槛。在本文中，我们关注基于对话界面的GenAI应用，针对智能城市中的三种关键用户类型 - 市民、运营商和规划者。我们确定并回顾了针对这些用户类型背景下各种城市子系统所提出或部署的GenAI模型和技术。我们还考虑了GenAI如何建立在官方城市记录、物联网数据流和城市数字孪生的现有数据基础上。我们相信这项工作代表了从智能城市关键用户角度对智能城市GenAI技术的首次全面总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of IoT in cities, combined with Digital Twins, creates arich data foundation for Smart Cities aimed at improving urban life andoperations. Generative AI (GenAI) significantly enhances this potential, movingbeyond traditional AI analytics and predictions by processing multimodalcontent and generating novel outputs like text and simulations. Usingspecialized or foundational models, GenAI's natural language abilities such asNatural Language Understanding (NLU) and Natural Language Generation (NLG) canpower tailored applications and unified interfaces, dramatically loweringbarriers for users interacting with complex smart city systems. In this paper,we focus on GenAI applications based on conversational interfaces within thecontext of three critical user archetypes in a Smart City - Citizens, Operatorsand Planners. We identify and review GenAI models and techniques that have beenproposed or deployed for various urban subsystems in the contexts of these userarchetypes. We also consider how GenAI can be built on the existing datafoundation of official city records, IoT data streams and Urban Digital Twins.We believe this work represents the first comprehensive summarization of GenAItechniques for Smart Cities from the lens of the critical users in a SmartCity.</description>
      <author>example@mail.com (Ankit Shetgaonkar, Dipen Pradhan, Lakshit Arora, Sanjay Surendranath Girija, Shashank Kapoor, Aman Raj)</author>
      <guid isPermaLink="false">2505.08034v2</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector</title>
      <link>http://arxiv.org/abs/2508.11185v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对单目3D目标检测器在未见或分布外相机高度下表现不佳的问题，提出了一种名为CHARM3D的新方法，显著提高了检测器对不同相机高度的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的单目3D目标检测器在单一相机高度的数据上有效，但在处理未见过的或分布外的相机高度时表现不佳。现有方法通常依赖于Plucker嵌入、图像变换或数据增强。&lt;h4&gt;目的&lt;/h4&gt;研究相机高度变化对最先进单目3D模型的影响，并提出一种能够提高对未见相机高度泛化能力的检测器。&lt;h4&gt;方法&lt;/h4&gt;在扩展的CARLA数据集上进行了系统性分析，该数据集包含多种相机高度；提出CHARM3D方法，通过在模型内平均深度估计来提高鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;深度估计是影响相机高度变化下性能的主要因素；回归深度模型和地面深度模型的平均深度误差在相机高度变化下分别表现出一致负相关和正相关趋势；CHARM3D提高了对未见相机高度的泛化能力超过45%。&lt;h4&gt;结论&lt;/h4&gt;CHARM3D在CARLA数据集上取得了最先进的性能，能够有效处理不同相机高度的情况，代码和模型已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;单目3D目标检测器虽然在单一相机高度的数据上有效，但在处理未见或分布外的相机高度时表现不佳。现有方法通常依赖于Plucker嵌入、图像变换或数据增强。本文通过研究相机高度变化对最先进单目3D模型的影响，朝着这一被忽视的问题迈出了一步。在扩展的CARLA数据集上进行了系统性分析，该数据集包含多种相机高度，我们观察到深度估计是影响相机高度变化下性能的主要因素。我们数学证明并经验观察到，回归深度模型和地面深度模型的平均深度误差在相机高度变化下分别表现出一致负相关和正相关趋势。为减轻这一问题，我们提出了相机高度鲁棒单目3D检测器CHARM3D，该方法在模型内平均深度估计。CHARM3D将未见相机高度的泛化能力提高了45%以上，在CARLA数据集上取得了最先进的性能。代码和模型位于https://github.com/abhi1kumar/CHARM3R&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D目标检测器在未见相机高度上性能下降的问题。这个问题在现实中非常重要，因为自动驾驶车辆在不同平台（如地面机器人、轿车、卡车）部署时相机高度差异显著，而收集每种高度的数据并重新训练模型不切实际。在研究中，这是一个相对未被充分探索的领域，现有方法在显著高度变化下效果有限，且相机高度变化会导致投影变换，现有网络难以处理。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过系统分析扩展的CARLA数据集，研究了相机高度变化对单目3D模型的影响，发现深度估计是主要因素。然后数学证明了回归深度模型和地面深度模型在高度变化下呈现相反趋势（前者低估深度，后者高估深度）。基于这一洞察，作者设计了CHARM3R方法，在模型内部平均这两种深度估计以抵消偏差。该方法借鉴了现有单目3D检测方法（如GUP Net、DEVIANT）和地面深度估计技术，但创新性地将它们组合以解决相机高度变化的泛化问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合两种具有互补泛化行为的深度估计方法（回归深度和地面深度）来提高对未见相机高度的鲁棒性。回归深度在训练高度表现好但在高度变化时低估深度，地面深度在高度变化时高估深度，平均两者可抵消偏差。实现流程：1)输入单目图像；2)用骨干网络提取特征；3)同时预测回归深度和地面深度（通过计算投影的3D底部中心并查询地面深度）；4)平均两种深度估计得到最终深度；5)基于融合深度进行3D目标检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次系统研究单目3D检测器对未见相机高度的泛化问题；2)数学证明回归深度和地面深度在高度变化下呈现相反趋势；3)提出简单有效的方法——在模型内部平均两种深度估计；4)在CARLA数据集上实现超过45%的泛化能力提升。相比之前工作的不同：区别于Plucker嵌入（用于3D姿态估计）、图像变换方法（假设固定距离参数，在显著高度变化下失败）、数据增强方法（需要复杂数据合成且在未知高度效果差）以及多高度训练方法（需要训练多个检测器），CHARM3R仅从单一高度数据训练即可泛化到多个高度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CHARM3R通过结合具有互补泛化行为的回归深度和地面深度估计，显著提高了单目3D目标检测器对未见相机高度的鲁棒性，在CARLA数据集上实现了超过45%的泛化能力提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D object detectors, while effective on data from one ego cameraheight, struggle with unseen or out-of-distribution camera heights. Existingmethods often rely on Plucker embeddings, image transformations or dataaugmentation. This paper takes a step towards this understudied problem byfirst investigating the impact of camera height variations on state-of-the-art(SoTA) Mono3D models. With a systematic analysis on the extended CARLA datasetwith multiple camera heights, we observe that depth estimation is a primaryfactor influencing performance under height variations. We mathematically proveand also empirically observe consistent negative and positive trends in meandepth error of regressed and ground-based depth models, respectively, undercamera height changes. To mitigate this, we propose Camera Height RobustMonocular 3D Detector (CHARM3R), which averages both depth estimates within themodel. CHARM3R improves generalization to unseen camera heights by more than$45\%$, achieving SoTA performance on the CARLA dataset. Codes and Models athttps://github.com/abhi1kumar/CHARM3R</description>
      <author>example@mail.com (Abhinav Kumar, Yuliang Guo, Zhihao Zhang, Xinyu Huang, Liu Ren, Xiaoming Liu)</author>
      <guid isPermaLink="false">2508.11185v1</guid>
      <pubDate>Mon, 18 Aug 2025 14:51:15 +0800</pubDate>
    </item>
    <item>
      <title>HGAurban: Heterogeneous Graph Autoencoding for Urban Spatial-Temporal Learning</title>
      <link>http://arxiv.org/abs/2410.10915v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HGAurban是一种新型异构时空图掩码自编码器，利用生成式自监督学习解决城市感知应用中时空数据的噪声和稀疏性问题，有效提升了区域表示的质量。&lt;h4&gt;背景&lt;/h4&gt;时空图表示在城市感知应用（如交通分析、人类移动行为建模和城市犯罪预测）中起着关键作用，但时空数据的噪声和稀疏性限制了现有神经网络学习有意义区域表示的能力。&lt;h4&gt;目的&lt;/h4&gt;克服时空数据噪声和稀疏性的限制，提出一种鲁棒的城市数据表示方法。&lt;h4&gt;方法&lt;/h4&gt;提出HGAurban框架，包含时空异构图编码器从多源数据提取区域依赖关系，以及掩码自编码器联合处理节点特征和图结构，自动学习跨区域的异构时空模式。&lt;h4&gt;主要发现&lt;/h4&gt;在多个时空挖掘任务上的实验表明，该框架优于最先进的方法，并能鲁棒处理空间和时间维度上的噪声和稀疏性挑战。&lt;h4&gt;结论&lt;/h4&gt;HGAurban框架有效解决了时空数据噪声和稀疏性问题，通过自监督学习显著提高了城市数据表示的质量。&lt;h4&gt;翻译&lt;/h4&gt;时空图表示在城市感知应用中起着关键作用，包括交通分析、人类移动行为建模和城市犯罪预测。然而，一个关键挑战在于时空数据的噪声和稀疏性，这限制了现有神经网络学习时空图中有意义区域表示的能力。为了克服这些限制，我们提出了HGAurban，一种新型异构时空图掩码自编码器，利用生成式自监督学习进行鲁棒的城市数据表示。我们的框架引入了一个时空异构图编码器，从多源数据中提取区域间的依赖关系，实现对多样化空间关系的全面建模。在我们的自监督学习范式中，我们实现了一个掩码自编码器，联合处理节点特征和图结构。这种方法能够自动学习跨区域的异构时空模式，显著改善动态时间相关性的表示。在多个时空挖掘任务上的综合实验表明，我们的框架优于最先进的方法，并能鲁棒处理真实世界城市数据挑战，包括空间和时间维度上的噪声和稀疏性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial-temporal graph representations play a crucial role in urban sensingapplications, including traffic analysis, human mobility behavior modeling, andcitywide crime prediction. However, a key challenge lies in the noisy andsparse nature of spatial-temporal data, which limits existing neural networks'ability to learn meaningful region representations in the spatial-temporalgraph. To overcome these limitations, we propose HGAurban, a novelheterogeneous spatial-temporal graph masked autoencoder that leveragesgenerative self-supervised learning for robust urban data representation. Ourframework introduces a spatial-temporal heterogeneous graph encoder thatextracts region-wise dependencies from multi-source data, enablingcomprehensive modeling of diverse spatial relationships. Within ourself-supervised learning paradigm, we implement a masked autoencoder thatjointly processes node features and graph structure. This approachautomatically learns heterogeneous spatial-temporal patterns across regions,significantly improving the representation of dynamic temporal correlations.Comprehensive experiments across multiple spatiotemporal mining tasksdemonstrate that our framework outperforms state-of-the-art methods androbustly handles real-world urban data challenges, including noise and sparsityin both spatial and temporal dimensions.</description>
      <author>example@mail.com (Qianru Zhang, Xinyi Gao, Haixin Wang, Dong Huang, Siu-Ming Yiu, Hongzhi Yin)</author>
      <guid isPermaLink="false">2410.10915v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
  <item>
      <title>SimAQ: Mitigating Experimental Artifacts in Soft X-Ray Tomography using Simulated Acquisitions</title>
      <link>http://arxiv.org/abs/2508.10821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种模拟管道，用于生成真实的细胞幻影并应用合成伪影，解决了软X射线断层扫描中实验伪影和数据集有限的问题。通过在合成数据上训练神经网络，实现了在真实SXT断层扫描图像上的有效迁移学习，提供了准确的分割结果，使对噪声断层扫描图像的定量分析成为可能，而不依赖于大型标记数据集或复杂的重建方法。&lt;h4&gt;背景&lt;/h4&gt;软X射线断层扫描(SXT)可以提供全细胞结构的详细洞察，但受到实验伪影(如缺失楔形)和注释数据集有限可用性的阻碍。&lt;h4&gt;目的&lt;/h4&gt;提出一种模拟管道，用于生成真实的细胞幻影并应用合成伪影，生成成对的噪声体积、正弦图和重建图像。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为\method的模拟管道，通过在合成数据上训练神经网络进行验证，并在真实的SXT断层扫描图像上展示有效的少样本和零样本迁移学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够提供准确的分割结果，使对噪声断层扫描图像的定量分析成为可能，不依赖于大型标记数据集或复杂的重建方法。&lt;h4&gt;结论&lt;/h4&gt;该方法解决了SXT中的实验伪影和数据集有限的问题，通过合成数据训练，实现了在真实数据上的有效迁移学习。&lt;h4&gt;翻译&lt;/h4&gt;软X射线断层扫描(SXT)为全细胞提供了详细的结构洞察，但受到缺失楔形等实验伪影和注释数据集有限可用性的阻碍。我们提出了\method，这是一种模拟管道，可生成真实的细胞幻影并应用合成伪影，以产生成对的噪声体积、正弦图和重建图像。我们通过主要在合成数据上训练神经网络来验证我们的方法，并证明了在真实SXT断层扫描图像上有效的少样本和零样本迁移学习能力。我们的模型提供了准确的分割，使对噪声断层扫描图像的定量分析成为可能，而不依赖于大型标记数据集或复杂的重建方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决软X射线断层扫描(SXT)中的实验伪影问题和标注数据稀缺问题。这些问题很重要，因为SXT是一种能以30-50纳米高分辨率解析完整细胞结构的技术，但'缺失楔形'和各种实验噪声会严重影响重建质量，降低定量和形态学分析的准确性，而获取大量标注数据又非常耗时困难。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考如何利用模拟数据解决真实数据稀缺的问题，设计出SimAQ模拟管道。他们借鉴了现有的SXT重建算法(如滤波反投影FBP)、细胞模拟方法(如椭球体模拟细胞器)、Martínez-Sánchez等人的生物真实合成数据生成、Yao等人的正弦图修复方法以及Liu等人的自监督学习策略，但将这些方法整合并扩展为一个更全面的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过模拟真实的细胞结构和实验条件生成训练数据，结合合成数据和少量真实数据进行混合学习，实现零样本和少样本迁移学习。整体流程包括：1)使用椭球体生成酵母细胞幻影并添加随机变形；2)添加冰裂纹、散射和基准标记等伪影；3)模拟有限角度投影和重建过程；4)使用编码器-解码器架构在混合数据上训练；5)用少量真实数据进行微调适应真实数据分布。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全面的SimAQ模拟框架，涵盖多种实验伪影；2)混合学习方法，结合合成和真实数据优势；3)实现零样本和少样本迁移学习；4)提供端到端开源解决方案。相比之前工作，SimAQ比Chen等人的纯真实数据方法需要更少标注，比Martínez-Sánchez等人更专注于伪影模拟，比Yao的UsiNet更适合低对比度生物成像，比Liu的方法更好地处理各向异性伪影。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SimAQ通过创建全面的细胞结构和实验伪影模拟管道，结合合成数据和少量真实数据训练，实现了软X射线断层扫描中伪影的有效校正和细胞分割，大幅减少了对大量标注数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soft X-ray tomography (SXT) provides detailed structural insight into wholecells but is hindered by experimental artifacts such as the missing wedge andby limited availability of annotated datasets. We present \method, a simulationpipeline that generates realistic cellular phantoms and applies syntheticartifacts to produce paired noisy volumes, sinograms, and reconstructions. Wevalidate our approach by training a neural network primarily on synthetic dataand demonstrate effective few-shot and zero-shot transfer learning on real SXTtomograms. Our model delivers accurate segmentations, enabling quantitativeanalysis of noisy tomograms without relying on large labeled datasets orcomplex reconstruction methods.</description>
      <author>example@mail.com (Jacob Egebjerg, Daniel Wüstner)</author>
      <guid isPermaLink="false">2508.10821v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SemPT: Semantic Prompt Tuning for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2508.10645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为语义提示调优(SemPT)的新型框架，通过利用跨类别的共享属性级知识来解决视觉迁移学习中未见类别的泛化挑战，在各种设置下实现了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;视觉迁移学习对于未见类别是一个活跃且有挑战性的研究领域，存在保留特定类别表示和获取可迁移知识之间的固有冲突。虽然视觉-语言模型(VLMs)提供了有前景的解决方案，但现有的提示调优方法依赖于稀疏类别标签或不同的LLM生成的描述，这分散了知识表示并阻碍了可迁移性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，通过引入语义提示调优(SemPT)框架，利用跨类别的共享属性级知识来处理视觉迁移学习中的泛化挑战。&lt;h4&gt;方法&lt;/h4&gt;SemPT采用两步提示策略，指导LLM提取共享视觉属性并生成属性级描述，捕捉超越标签的可迁移语义线索；应用视觉引导加权减少无关属性噪声并增强文本嵌入；图像嵌入与标签和属性增强文本嵌入联合对齐，平衡已见类别的判别性和对未见类别的可迁移性；根据类别可用性动态选择适当的嵌入方式。&lt;h4&gt;主要发现&lt;/h4&gt;在15个基准数据集上的广泛实验表明，SemPT在各种设置下实现了最先进性能，包括基础到新颖泛化、跨数据集迁移、跨域迁移和少样本学习。&lt;h4&gt;结论&lt;/h4&gt;SemPT有效解决了视觉迁移学习中的挑战，通过利用共享属性级知识显著提高了模型在未见类别上的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉迁移学习对于未见类别呈现一个活跃的研究课题 yet 一个具有挑战性的任务，由于保留特定类别表示和获取可迁移知识之间的固有冲突。视觉-语言模型(VLMs)在大量图像-文本对上预训练提供了一个有前景的解决方案。然而，现有的提示调优方法依赖于稀疏类别标签或不同的LLM生成的描述，这分散了知识表示并阻碍了可迁移性。为了解决这一局限性，我们引入了语义提示调优(SemPT)，一个通过利用跨类别的共享属性级知识处理泛化挑战的新型框架。具体来说，SemPT采用两步提示策略指导LLM提取共享视觉属性并生成属性级描述，捕捉超越标签的可迁移语义线索，同时确保结构连贯性。然后，对属性级描述的嵌入应用视觉引导加权，减少无关属性的噪声并增强文本嵌入。此外，图像嵌入与标签和属性增强的文本嵌入联合对齐，平衡已见类别的判别性和对未见类别的可迁移性。考虑到类别的可用性，我们的推理动态选择已见类别的标准标签嵌入和未见类别的属性增强嵌入，以确保有效适应。在15个基准数据集上的广泛实验表明，SemPT在各种设置下实现了最先进的性能，包括基础到新颖泛化、跨数据集迁移、跨域迁移和少样本学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual transfer learning for unseen categories presents an active researchtopic yet a challenging task, due to the inherent conflict between preservingcategory-specific representations and acquiring transferable knowledge.Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairsoffer a promising solution. However, existing prompt tuning methods rely onsparse category labels or disparate LLM-generated descriptions, which fragmentknowledge representation and hinder transferability. To address thislimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework thattackles the generalization challenge by leveraging shared attribute-levelknowledge across categories. Specifically, SemPT adopts a two-step promptingstrategy to guide LLM in extracting shared visual attributes and generatingattribute-level descriptions, capturing transferable semantic cues beyondlabels while ensuring coherent structure. Then, visually guided weighting isapplied to the embeddings of attribute-level descriptions to reduce noise fromirrelevant attributes and enhance the text embeddings. Additionally, imageembeddings are jointly aligned with both label and attribute-enhanced textembeddings, balancing discrimination for seen categories and transferability tounseen ones. Considering the availability of category exposure, our inferencedynamically selects between standard label embeddings for seen categories andattribute-enhanced embeddings for unseen ones to ensure effective adaptation.Extensive experiments on 15 benchmark datasets demonstrate that SemPT achievesstate-of-the-art performance across various settings, including base-to-novelgeneralization, cross-dataset transfer, cross-domain transfer, and few-shotlearning.</description>
      <author>example@mail.com (Xiao Shi, Yangjun Ou, Zhenzhong Chen)</author>
      <guid isPermaLink="false">2508.10645v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Deep Equilibrium Model Learning for Large-Scale Channel Estimation with Performance Guarantees</title>
      <link>http://arxiv.org/abs/2508.10546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需真实信道标签的无监督学习框架用于大规模信道估计(LCE)，结合广义Stein无偏风险估计(GSURE)和深度平衡(DEQ)模型，实现了与监督方法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;监督深度学习方法在大规模信道估计中显示出潜力，但它们对真实信道标签的依赖严重限制了它们在实际系统中的实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要真实信道标签的无监督学习框架用于LCE。&lt;h4&gt;方法&lt;/h4&gt;利用广义Stein无偏风险估计(GSURE)作为无监督损失函数，集成深度平衡(DEQ)模型隐式表示无限深度网络，通过直接学习参数化迭代过程的固定点实现。&lt;h4&gt;主要发现&lt;/h4&gt;DEQ架构强制执行可压缩解；DEQ诱导的可压缩性确保通过GSURE优化投影误差足以保证良好的MSE性能；提供了严格的性能保证。&lt;h4&gt;结论&lt;/h4&gt;提出的框架在真实信道不可用时显著优于各种基线方法。&lt;h4&gt;翻译&lt;/h4&gt;监督深度学习方法在大规模信道估计(LCE)中显示出前景，但它们对真实信道标签的依赖严重限制了它们在实际系统中的实用性。在本文中，我们提出了一种用于LCE的无监督学习框架，不需要真实信道标签。所提出的方法利用广义Stein无偏风险估计(GSURE)作为原则性的无监督损失函数，它提供了从压缩噪声测量中投影均方误差(PMSE)的无偏估计。为确保保证的性能，我们集成了一个深度平衡(DEQ)模型，它通过直接学习参数化迭代过程的固定点来隐式表示无限深度网络。我们理论上证明，在温和条件下，所提出的基于GSURE的无监督DEQ学习可以达到与监督方法相当的性能。特别是，我们表明DEQ架构本质上强制执行可压缩解。然后我们证明DEQ诱导的可压缩性确保通过GSURE优化投影误差足以保证良好的MSE性能，从而实现严格的性能保证。广泛的模拟验证了理论发现，并证明当真实信道不可用时，所提出的框架显著优于各种基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supervised deep learning methods have shown promise for large-scale channelestimation (LCE), but their reliance on ground-truth channel labels greatlylimits their practicality in real-world systems. In this paper, we propose anunsupervised learning framework for LCE that does not require ground-truthchannels. The proposed approach leverages Generalized Stein's Unbiased RiskEstimate (GSURE) as a principled unsupervised loss function, which provides anunbiased estimate of the projected mean-squared error (PMSE) from compressednoisy measurements. To ensure a guaranteed performance, we integrate a deepequilibrium (DEQ) model, which implicitly represents an infinite-depth networkby directly learning the fixed point of a parameterized iterative process. Wetheoretically prove that, under mild conditions, the proposed GSURE-basedunsupervised DEQ learning can achieve oracle-level supervised performance. Inparticular, we show that the DEQ architecture inherently enforces acompressible solution. We then demonstrate that DEQ-induced compressibilityensures that optimizing the projected error via GSURE suffices to guarantee agood MSE performance, enabling a rigorous performance guarantee. Extensivesimulations validate the theoretical findings and demonstrate that the proposedframework significantly outperforms various baselines when ground-truth channelis unavailable.</description>
      <author>example@mail.com (Haotian Tian, Lixiang Lian)</author>
      <guid isPermaLink="false">2508.10546v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>A dataset and model for recognition of audiologically relevant environments for hearing aids: AHEAD-DS and YAMNet+</title>
      <link>http://arxiv.org/abs/2508.10360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了AHEAD-DS数据集和YAMNet+模型，解决了听力辅助设备场景识别中的数据集局限性和边缘设备部署挑战，实现了高效准确的实时场景识别。&lt;h4&gt;背景&lt;/h4&gt;听力辅助设备中的场景识别很重要，但现有数据集缺乏公开性、完整性或听力相关标签，阻碍了机器学习模型的系统比较。同时，将这些模型部署在资源受限的边缘设备上也是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;创建一个标准化的公开数据集用于听力相关环境场景识别，并开发一个适用于边缘设备的声音识别模型，作为基于声音的场景识别的基线模型。&lt;h4&gt;方法&lt;/h4&gt;利用多个开源数据集创建AHEAD-DS数据集，并引入YAMNet+声音识别模型。通过迁移学习使用预训练的YAMNet模型进行优化。将模型部署到Android智能手机上进行实时场景识别测试。&lt;h4&gt;主要发现&lt;/h4&gt;YAMNet+在AHEAD-DS测试集的十四种听力相关环境类别中达到了0.83的平均精度和0.93的准确率。即使在配置适中的Google Pixel 3手机上，模型也能实现实时处理，加载模型的延迟约为50毫秒，每1秒音频的处理时间线性增加约30毫秒。&lt;h4&gt;结论&lt;/h4&gt;AHEAD-DS数据集和YAMNet+模型为听力辅助设备的场景识别提供了有效的解决方案，能够在边缘设备上实现实时、准确的声音场景识别，为未来研究提供了基准。&lt;h4&gt;翻译&lt;/h4&gt;听力辅助设备相关环境的场景识别对助听器很重要；然而，这具有挑战性，部分原因是现有数据集的局限性。数据集通常缺乏公开可访问性、完整性或听力相关的标签，阻碍了机器学习模型的系统比较。将这些模型部署在资源受限的边缘设备上提出了另一个挑战。我们的解决方案是双重的：我们利用几个开源数据集创建了AHEAD-DS，一个为听力相关环境场景识别而设计的数据集，并引入了YAMNet+声音识别模型。AHEAD-DS旨在提供一个标准化的、公开可用的数据集，具有与听力辅助设备相关的统一标签，促进模型比较。YAMNet+设计用于部署在连接到听力设备的智能手机等边缘设备上，如助听器和具有助听功能的无线耳机；作为基于声音的场景识别的基线模型。在AHEAD-DS测试集上，YAMNet+在十四种听力相关环境类别中达到了0.83的平均精度和0.93的准确率。我们发现，使用预训练的YAMNet模型进行迁移学习是必不可少的。我们通过将YAMNet+部署到Android智能手机上，证明了边缘设备上的实时声音场景识别能力。即使在配置适中的Google Pixel 3手机（2018年发布）上，模型处理音频的加载延迟约为50毫秒，每1秒音频的处理时间线性增加约30毫秒。我们的网站和代码https://github.com/Australian-Future-Hearing-Initiative。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene recognition of audiologically relevant environments is important forhearing aids; however, it is challenging, in part because of the limitations ofexisting datasets. Datasets often lack public accessibility, completeness, oraudiologically relevant labels, hindering systematic comparison of machinelearning models. Deploying these models on resource-constrained edge devicespresents another challenge. Our solution is two-fold: we leverage several opensource datasets to create AHEAD-DS, a dataset designed for scene recognition ofaudiologically relevant environments, and introduce YAMNet+, a soundrecognition model. AHEAD-DS aims to provide a standardised, publicly availabledataset with consistent labels relevant to hearing aids, facilitating modelcomparison. YAMNet+ is designed for deployment on edge devices like smartphonesconnected to hearing devices, such as hearing aids and wireless earphones withhearing aid functionality; serving as a baseline model for sound-based scenerecognition. YAMNet+ achieved a mean average precision of 0.83 and accuracy of0.93 on the testing set of AHEAD-DS across fourteen categories ofaudiologically relevant environments. We found that applying transfer learningfrom the pretrained YAMNet model was essential. We demonstrated real-timesound-based scene recognition capabilities on edge devices by deploying YAMNet+to an Android smartphone. Even with a Google Pixel 3 (a phone with modestspecifications, released in 2018), the model processes audio with approximately50ms of latency to load the model, and an approximate linear increase of 30msper 1 second of audio. Our website and codehttps://github.com/Australian-Future-Hearing-Initiative .</description>
      <author>example@mail.com (Henry Zhong, Jörg M. Buchholz, Julian Maclaren, Simon Carlile, Richard Lyon)</author>
      <guid isPermaLink="false">2508.10360v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Explainable AI Technique in Lung Cancer Detection Using Convolutional Neural Networks</title>
      <link>http://arxiv.org/abs/2508.10196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures, 4 tables. Undergraduate research project report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了一个深度学习框架，用于从胸部CT图像进行自动化肺癌筛查，并集成了可解释性功能。通过评估多种模型架构，发现ResNet152准确率最高，而DenseNet121在各项指标间提供了最佳平衡。使用SHAP方法增强了临床透明度，使这种方法在资源有限的环境中特别有价值。&lt;h4&gt;背景&lt;/h4&gt;肺癌的早期检测对提高生存率至关重要，研究使用胸部CT图像进行自动化肺癌筛查。&lt;h4&gt;目的&lt;/h4&gt;开发一个深度学习框架用于肺癌筛查，集成可解释性功能，提高临床决策透明度。&lt;h4&gt;方法&lt;/h4&gt;使用IQ-OTH/NCCD数据集（包含1197个正常、良性和恶性三类扫描），评估自定义卷积神经网络和三种微调的迁移学习骨干网络（DenseNet121、ResNet152和VGG19）。采用成本敏感学习缓解类别不平衡，通过准确率、精确率、召回率、F1分数和ROC-AUC评估模型，并应用SHAP方法可视化预测证据。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet152达到最高准确率(97.3%)，DenseNet121在精确率、召回率和F1分数方面提供了最佳平衡（分别为92%、90%、91%）。集成可解释性的CNN方法可以提供快速、准确且可解释的支持。&lt;h4&gt;结论&lt;/h4&gt;基于CNN的方法结合可解释性可以为肺癌筛查提供支持，尤其在资源有限的环境中特别有用。&lt;h4&gt;翻译&lt;/h4&gt;肺癌的早期检测对提高生存率至关重要。我们提出了一种深度学习框架，用于从胸部计算机断层扫描（CT）图像进行自动化肺癌筛查，并集成了可解释性功能。我们使用IQ-OTH/NCCD数据集（包含正常、良性和恶性三类共1197个扫描）评估了一个自定义卷积神经网络（CNN）和三种微调的迁移学习骨干网络：DenseNet121、ResNet152和VGG19。模型通过成本敏感学习进行训练以减轻类别不平衡，并通过准确率、精确率、召回率、F1分数和ROC-AUC进行评估。虽然ResNet152达到了最高的准确率（97.3%），但DenseNet121在精确率、召回率和F1分数方面提供了最佳平衡（分别高达92%、90%、91%）。我们进一步应用Shapley Additive Explanations（SHAP）来可视化对预测有贡献的证据，提高了临床透明度。结果表明，结合可解释性的基于CNN的方法可以为肺癌筛查提供快速、准确且可解释的支持，特别是在资源有限的设置中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early detection of lung cancer is critical to improving survival outcomes. Wepresent a deep learning framework for automated lung cancer screening fromchest computed tomography (CT) images with integrated explainability. Using theIQ-OTH/NCCD dataset (1,197 scans across Normal, Benign, and Malignant classes),we evaluate a custom convolutional neural network (CNN) and three fine-tunedtransfer learning backbones: DenseNet121, ResNet152, and VGG19. Models aretrained with cost-sensitive learning to mitigate class imbalance and evaluatedvia accuracy, precision, recall, F1-score, and ROC-AUC. While ResNet152achieved the highest accuracy (97.3%), DenseNet121 provided the best overallbalance in precision, recall, and F1 (up to 92%, 90%, 91%, respectively). Wefurther apply Shapley Additive Explanations (SHAP) to visualize evidencecontributing to predictions, improving clinical transparency. Results indicatethat CNN-based approaches augmented with explainability can provide fast,accurate, and interpretable support for lung cancer screening, particularly inresource-limited settings.</description>
      <author>example@mail.com (Nishan Rai, Sujan Khatri, Devendra Risal)</author>
      <guid isPermaLink="false">2508.10196v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model</title>
      <link>http://arxiv.org/abs/2508.10156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了生成式人工智能(GenAI)模型在农业作物病害诊断中的应用，验证了结合真实图像与合成图像训练模型的可行性及效果。&lt;h4&gt;背景&lt;/h4&gt;生成式人工智能模型的发展为生成高分辨率合成图像提供了新可能，为训练农业计算机视觉模型提供了替代传统图像采集的方法。在作物病害诊断领域，GenAI模型可创建各种疾病的合成图像，减少对资源密集型田间数据采集的依赖。&lt;h4&gt;目的&lt;/h4&gt;研究结合少量真实图像与合成图像是否能提高EfficientNetV2-L模型对西瓜病害分类的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;将训练数据集分为五种处理方式：H0（仅真实图像）、H1（仅合成图像）、H2（真实与合成图像比例为1:1）、H3（真实与合成图像比例为1:10）、H4（H3 + 随机图像以提高变异性）。所有处理都使用自定义的EfficientNetV2-L架构进行训练，应用了增强的微调和迁移学习技术。&lt;h4&gt;主要发现&lt;/h4&gt;在H2、H3和H4处理上训练的模型表现出高精确度、召回率和F1分数指标。加权F1分数从H0的0.65提高到H3-H4的1.00，表明添加少量真实图像与大量合成图像相结合提高了模型性能和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;合成图像单独无法充分替代真实图像；相反，必须以混合方式使用两者，以最大化作物病害分类的模型性能。&lt;h4&gt;翻译&lt;/h4&gt;当前生成式人工智能(GenAI)模型的进步为生成高分辨率合成图像开辟了新的可能性，从而为训练农业计算机视觉模型提供了替代传统图像采集的有前景选择。在作物病害诊断的背景下，GenAI模型正被用于创建各种疾病的合成图像，可能促进模型创建并减少对资源密集型田间数据采集的依赖。然而，关于评估将真实图像与合成图像整合以提高疾病分类性能效果的研究有限。因此，本研究旨在调查结合少量真实图像与合成图像是否能提高EfficientNetV2-L模型对西瓜(Citrullus lanatus)病害分类的预测准确性。训练数据集被分为五种处理方式：H0（仅真实图像）、H1（仅合成图像）、H2（真实与合成图像比例为1:1）、H3（真实与合成图像比例为1:10）和H4（H3 + 随机图像以提高变性和模型泛化能力）。所有处理均使用自定义的EfficientNetV2-L架构进行训练，并应用了增强的微调和迁移学习技术。在H2、H3和H4处理上训练的模型表现出高精确度、召回率和F1分数指标。此外，加权F1分数从H0的0.65提高到H3-H4的1.00，这表明添加少量真实图像与大量合成图像相结合提高了模型性能和泛化能力。总体而言，这验证了发现：合成图像单独无法充分替代真实图像；相反，必须以混合方式使用两者，以最大化作物病害分类的模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The current advancements in generative artificial intelligence (GenAI) modelshave paved the way for new possibilities for generating high-resolutionsynthetic images, thereby offering a promising alternative to traditional imageacquisition for training computer vision models in agriculture. In the contextof crop disease diagnosis, GenAI models are being used to create syntheticimages of various diseases, potentially facilitating model creation andreducing the dependency on resource-intensive in-field data collection.However, limited research has been conducted on evaluating the effectiveness ofintegrating real with synthetic images to improve disease classificationperformance. Therefore, this study aims to investigate whether combining alimited number of real images with synthetic images can enhance the predictionaccuracy of an EfficientNetV2-L model for classifying watermelon\textit{(Citrullus lanatus)} diseases. The training dataset was divided intofive treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images toimprove variability and model generalization). All treatments were trainedusing a custom EfficientNetV2-L architecture with enhanced fine-tuning andtransfer learning techniques. Models trained on H2, H3, and H4 treatmentsdemonstrated high precision, recall, and F1-score metrics. Additionally, theweighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifyingthat the addition of a small number of real images with a considerable volumeof synthetic images improved model performance and generalizability. Overall,this validates the findings that synthetic images alone cannot adequatelysubstitute for real images; instead, both must be used in a hybrid manner tomaximize model performance for crop disease classification.</description>
      <author>example@mail.com (Nitin Rai, Nathan S. Boyd, Gary E. Vallad, Arnold W. Schumann)</author>
      <guid isPermaLink="false">2508.10156v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Unifying equivalences across unsupervised learning, network science, and imaging/network neuroscience</title>
      <link>http://arxiv.org/abs/2508.10045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过描述不同科学分析方法之间的等价性，促进了科学数据的整合，解决了现代科学领域中数据整合不足导致的循环分析和冗余解释问题。&lt;h4&gt;背景&lt;/h4&gt;现代科学领域面临整合大量数据、分析和结果的挑战，忽视这种整合可能导致循环分析和冗余解释。&lt;h4&gt;目的&lt;/h4&gt;通过描述统一数据集和网络不同分析的等价性，促进科学整合，简化跨学科分析方法的理解和应用。&lt;h4&gt;方法&lt;/h4&gt;描述聚类与降维、网络中心性与动力学、影像学与网络神经科学模型间的等价性；统一无监督学习和网络科学的基础目标；融合优化算法；扩展目标以简化降维方法解释；将连接测量与网络科学中的六种通信测量等同；提供三个半分析性案例阐明分析方法。&lt;h4&gt;主要发现&lt;/h4&gt;统一了无监督学习、网络科学、影像神经科学和网络神经科学中的多种分析方法；开发了abc开放多语言工具箱实现这些分析。&lt;h4&gt;结论&lt;/h4&gt;研究成功统一了跨无监督学习、网络科学、影像神经科学和网络神经科学的多种分析方法，为科学整合提供了新框架。&lt;h4&gt;翻译&lt;/h4&gt;现代科学领域面临整合大量数据、分析和结果的挑战。我们最近表明，忽视这种整合可能导致循环分析和冗余解释。在此，我们通过描述统一数据集和网络不同分析的等价性，促进科学整合。我们描述了聚类和降维、网络中心性和动力学、影像学和网络神经科学中流行模型分析之间的等价性。首先，我们将无监督学习和网络科学中的基础目标等同（从k-means到模块化再到UMAP），融合优化这些目标经典算法，并扩展这些目标以简化流行降维方法的解释。其次，我们将连接幅度和离散度的基本测量与网络科学和网络神经科学中的六种通信、控制和多样性测量等同。第三，我们描述了三个半分析性案例，阐明和简化影像学和网络神经科学中结构和动力学分析的解释。我们在示例脑成像数据上说明了我们的结果，并提供了abc，一个实现我们分析的开源多语言工具箱。总之，我们的研究统一了跨无监督学习、网络科学、影像神经科学和网络神经科学的多种分析方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern scientific fields face the challenge of integrating a wealth of data,analyses, and results. We recently showed that a neglect of this integrationcan lead to circular analyses and redundant explanations. Here, we help advancescientific integration by describing equivalences that unify diverse analysesof datasets and networks. We describe equivalences across analyses ofclustering and dimensionality reduction, network centrality and dynamics, andpopular models in imaging and network neuroscience. First, we equatefoundational objectives across unsupervised learning and network science (fromk means to modularity to UMAP), fuse classic algorithms for optimizing theseobjectives, and extend these objectives to simplify interpretations of populardimensionality reduction methods. Second, we equate basic measures ofconnectional magnitude and dispersion with six measures of communication,control, and diversity in network science and network neuroscience. Third, wedescribe three semi-analytical vignettes that clarify and simplify theinterpretation of structural and dynamical analyses in imaging and networkneuroscience. We illustrate our results on example brain-imaging data andprovide abct, an open multi-language toolbox that implements our analyses.Together, our study unifies diverse analyses across unsupervised learning,network science, imaging neuroscience, and network neuroscience.</description>
      <author>example@mail.com (Mika Rubinov)</author>
      <guid isPermaLink="false">2508.10045v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Hypercomplex Prompt-aware Multimodal Recommendation</title>
      <link>http://arxiv.org/abs/2508.10753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HPMRec是一种超复杂提示感知多模态推荐框架，通过超复杂嵌入、非线性跨模态交互和提示感知补偿机制解决了现有多模态推荐系统中的三个关键局限性，在四个公共数据集上实现了最先进的推荐性能。&lt;h4&gt;背景&lt;/h4&gt;现代推荐系统在处理信息过载和解决多模态表征学习的固有局限性方面面临关键挑战。&lt;h4&gt;目的&lt;/h4&gt;克服现有多模态推荐方法的三个基本局限性：单一表征能力有限、忽略模态间非线性相关性、无法动态缓解GCN中的过平滑问题。&lt;h4&gt;方法&lt;/h4&gt;提出HPMRec框架，利用超复杂嵌入增强多模态特征表征多样性，采用超复杂乘法建立非线性跨模态交互，引入提示感知补偿机制缓解过平滑问题，并设计自监督学习任务增强表征多样性。&lt;h4&gt;主要发现&lt;/h4&gt;HPMRec通过超复杂嵌入、非线性交互和补偿机制有效解决了现有方法的局限性，在四个公共数据集上实现了最先进的推荐性能。&lt;h4&gt;结论&lt;/h4&gt;HPMRec框架成功解决了多模态推荐系统中的关键问题，提升了推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;现代推荐系统在处理信息过载的同时，解决多模态表征学习的固有局限性方面面临关键挑战。现有方法存在三个基本局限性：(1)通过单一表征表示丰富多模态特征的能力有限；(2)现有的线性模态融合策略忽略了模态间的深层非线性相关性；(3)静态优化方法无法动态缓解图卷积网络(GCN)中的过平滑问题。为克服这些局限性，我们提出了HPMRec，一种新颖的超复杂提示感知多模态推荐框架，它利用多组件形式的超复杂嵌入来增强多模态特征的表征多样性。HPMRec采用超复杂乘法自然地建立非线性跨模态交互，以弥合语义差距，有利于探索跨模态特征。HPMRec还引入了提示感知补偿机制，帮助组件与模态特定特征损失之间的不对齐，该机制从根本上缓解了过平滑问题。它进一步设计了自监督学习任务，以增强表征多样性并使不同模态保持一致。在四个公共数据集上的广泛实验表明，HPMRec实现了最先进的推荐性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern recommender systems face critical challenges in handling informationoverload while addressing the inherent limitations of multimodal representationlearning. Existing methods suffer from three fundamental limitations: (1)restricted ability to represent rich multimodal features through a singlerepresentation, (2) existing linear modality fusion strategies ignore the deepnonlinear correlations between modalities, and (3) static optimization methodsfailing to dynamically mitigate the over-smoothing problem in graphconvolutional network (GCN). To overcome these limitations, we propose HPMRec,a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, whichutilizes hypercomplex embeddings in the form of multi-components to enhance therepresentation diversity of multimodal features. HPMRec adopts the hypercomplexmultiplication to naturally establish nonlinear cross-modality interactions tobridge semantic gaps, which is beneficial to explore the cross-modalityfeatures. HPMRec also introduces the prompt-aware compensation mechanism to aidthe misalignment between components and modality-specific features loss, andthis mechanism fundamentally alleviates the over-smoothing problem. It furtherdesigns self-supervised learning tasks that enhance representation diversityand align different modalities. Extensive experiments on four public datasetsshow that HPMRec achieves state-of-the-art recommendation performance.</description>
      <author>example@mail.com (Zheyu Chen, Jinfeng Xu, Hewei Wang, Shuo Yang, Zitong Wan, Haibo Hu)</author>
      <guid isPermaLink="false">2508.10753v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SPHENIC: Topology-Informed Multi-View Clustering for Spatial Transcriptomics</title>
      <link>http://arxiv.org/abs/2508.10646v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SPHENIC是一种新颖的空间持续同调增强邻域集成聚类方法，通过整合不变拓扑特征和空间约束优化模块，解决了现有空间转录组学聚类方法在拓扑学习和空间邻域信息建模方面的局限性，在14个基准数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学聚类通过整合空间位置信息，可以更全面地了解细胞亚群识别。然而，现有方法存在两个主要局限性：拓扑学习易受低质量拓扑信号影响，以及对空间邻域信息的建模不足导致低质量的空间嵌入。&lt;h4&gt;目的&lt;/h4&gt;解决现有空间转录组学聚类方法在拓扑学习和空间邻域信息建模方面的局限性，提高聚类性能。&lt;h4&gt;方法&lt;/h4&gt;提出SPHENIC方法，包括：1) 将不变拓扑特征整合到聚类网络中实现稳定的表示学习；2) 设计空间约束和分布优化模块(SCDOM)，构建高质量空间嵌入，增加细胞与其空间邻域的相似性，减少与非邻域细胞的相似性。&lt;h4&gt;主要发现&lt;/h4&gt;在14个基准空间转录组切片上的实验表明，SPHENIC在空间聚类任务上实现了卓越的性能，比现有最先进的方法高出3.31%-6.54%。&lt;h4&gt;结论&lt;/h4&gt;SPHENIC方法通过整合拓扑特征和空间约束优化，有效解决了现有方法的局限性，在空间转录组学聚类任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;通过整合空间位置信息，空间转录组学聚类能够为细胞亚群识别提供更全面的见解。尽管最近有所进展，但现有方法至少有两个局限性：(i) 拓扑学习通常只考虑单个细胞或其交互图的表示；然而，空间转录组资料通常存在噪声，使得这些方法容易受到低质量拓扑信号的影响；(ii) 对空间邻域信息的建模不足导致低质量的空间嵌入。为了解决这些局限性，我们提出了SPHENIC，一种新颖的空间持续同调增强邻域集成聚类方法。具体而言，SPHENIC将不变拓扑特征整合到聚类网络中，以实现稳定的表示学习。此外，为了构建反映真实细胞分布的高质量空间嵌入，我们设计了空间约束和分布优化模块(SCDOM)。该模块增加细胞嵌入与其空间邻域嵌入之间的相似性，减少与非邻域细胞的相似性，从而产生聚类友好的空间嵌入。在14个基准空间转录组切片上的大量实验表明，SPHENIC在空间聚类任务上实现了卓越的性能，比现有最先进的方法高出3.31%-6.54%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; By incorporating spatial location information, spatial-transcriptomicsclustering yields more comprehensive insights into cell subpopulationidentification. Despite recent progress, existing methods have at least twolimitations: (i) topological learning typically considers only representationsof individual cells or their interaction graphs; however, spatialtranscriptomic profiles are often noisy, making these approaches vulnerable tolow-quality topological signals, and (ii) insufficient modeling of spatialneighborhood information leads to low-quality spatial embeddings. To addressthese limitations, we propose SPHENIC, a novel Spatial Persistent HomologyEnhanced Neighborhood Integrative Clustering method. Specifically, SPHENICincorporates invariant topological features into the clustering network toachieve stable representation learning. Additionally, to construct high-qualityspatial embeddings that reflect the true cellular distribution, we design theSpatial Constraint and Distribution Optimization Module (SCDOM). This moduleincreases the similarity between a cell's embedding and those of its spatialneighbors, decreases similarity with non-neighboring cells, and therebyproduces clustering-friendly spatial embeddings. Extensive experiments on 14benchmark spatial transcriptomic slices demonstrate that SPHENIC achievessuperior performance on the spatial clustering task, outperforming existingstate-of-the-art methods by 3.31%-6.54% over the best alternative.</description>
      <author>example@mail.com (Chenkai Guo, Yikai Zhu, Jing Yangum, Renxiang Guan, Por Lip Yee, Guangdun Peng, Dayu Hu)</author>
      <guid isPermaLink="false">2508.10646v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks</title>
      <link>http://arxiv.org/abs/2508.10639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MirGuard的稳健异常检测框架，用于增强溯源入侵检测系统(PIDS)抵抗图操纵攻击的能力。&lt;h4&gt;背景&lt;/h4&gt;基于学习的溯源入侵检测系统(PIDS)已成为主机系统异常检测的重要工具，能够捕捉丰富的上下文和结构信息，并有检测未知攻击的潜力。然而，这些系统容易受到图操纵攻击，攻击者通过操纵图结构来逃避检测，而之前的解决方案未能充分解决这一问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健的异常检测解决方案，增强PIDS抵抗图操纵攻击的能力，提高其安全性和实用性。&lt;h4&gt;方法&lt;/h4&gt;提出MirGuard框架，结合逻辑感知的多视图增强与对比表示学习。该框架引入逻辑感知噪声注入(LNI)生成语义有效的图视图，确保增强保留溯源数据的底层因果语义。这些视图在逻辑保持对比学习框架中使用，鼓励模型学习对良性变换不变但对对抗性变化敏感的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多个溯源数据集上的全面评估表明，MirGuard在抵抗各种图操纵攻击方面的稳健性显著优于最先进的检测器，同时没有牺牲检测性能和效率。&lt;h4&gt;结论&lt;/h4&gt;MirGuard代表了首个针对增强PIDS抵御图操纵攻击的针对性研究，为现代网络安全挑战提供了稳健有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于学习的溯源入侵检测系统(PIDS)已成为主机系统异常检测的重要工具，因为它们能够捕捉丰富的上下文和结构信息，并有潜力检测未知攻击。然而，最近研究表明这些系统容易受到图操纵攻击，攻击者通过操纵图结构来逃避检测。虽然之前的方法讨论过这类攻击，但没有提供稳健的检测解决方案，限制了PIDS的实际应用。为了应对这一挑战，我们提出MirGuard，一个稳健的异常检测框架，结合了逻辑感知的多视图增强与对比表示学习。MirGuard引入逻辑感知噪声注入(LNI)生成语义有效的图视图，确保所有增强都保留了溯源数据的底层因果语义。这些视图在逻辑保持对比学习框架中使用，鼓励模型学习对良性变换不变但对对抗性变化敏感的表示。在多个溯源数据集上的全面评估表明，MirGuard在抵抗各种图操纵攻击方面的稳健性显著优于最先进的检测器，同时没有牺牲检测性能和效率。我们的工作是首个针对增强PIDS抵御此类对抗威胁的针对性研究，为现代网络安全挑战提供了稳健有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning-based Provenance-based Intrusion Detection Systems (PIDSes) havebecome essential tools for anomaly detection in host systems due to theirability to capture rich contextual and structural information, as well as theirpotential to detect unknown attacks. However, recent studies have shown thatthese systems are vulnerable to graph manipulation attacks, where attackersmanipulate the graph structure to evade detection. While some previousapproaches have discussed this type of attack, none have fully addressed itwith a robust detection solution, limiting the practical applicability ofPIDSes.  To address this challenge, we propose MirGuard, a robust anomaly detectionframework that combines logic-aware multi-view augmentation with contrastiverepresentation learning. Rather than applying arbitrary structuralperturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) togenerate semantically valid graph views, ensuring that all augmentationspreserve the underlying causal semantics of the provenance data. These viewsare then used in a Logic-Preserving Contrastive Learning framework, whichencourages the model to learn representations that are invariant to benigntransformations but sensitive to adversarial inconsistencies. Comprehensiveevaluations on multiple provenance datasets demonstrate that MirGuardsignificantly outperforms state-of-the-art detectors in robustness againstvarious graph manipulation attacks without sacrificing detection performanceand efficiency. Our work represents the first targeted study to enhance PIDSagainst such adversarial threats, providing a robust and effective solution tomodern cybersecurity challenges.</description>
      <author>example@mail.com (Anyuan Sang, Lu Zhou, Li Yang, Junbo Jia, Huipeng Yang, Pengbin Feng, Jianfeng Ma)</author>
      <guid isPermaLink="false">2508.10639v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning</title>
      <link>http://arxiv.org/abs/2508.10298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SynBrain生成框架，用于模拟视觉语义到神经响应的转化，解决了现有方法难以同时建模生物变性和功能一致性的问题。SynBrain包含BrainVAE模型和语义到神经映射器两个关键组件，实验证明其在视觉到fMRI编码性能上超越现有方法，并能高效适应新主体并合成高质量fMRI信号。&lt;h4&gt;背景&lt;/h4&gt;视觉到神经映射本质上是一对多关系，相同视觉输入在不同试验、情境和主体中会引发不同血流动力学响应。现有确定性方法难以同时建模这种生物变性和编码刺激信息的基本功能一致性。&lt;h4&gt;目的&lt;/h4&gt;提出一个生成框架，能够以概率化和生物学可解释的方式模拟视觉语义到神经响应的转化，同时捕捉生物变性和功能一致性。&lt;h4&gt;方法&lt;/h4&gt;SynBrain框架包含两个关键组件：(i) BrainVAE模型通过概率学习将神经表示建模为连续概率分布，同时通过视觉语义约束保持功能一致性；(ii) 语义到神经映射器作为语义传输通路，将视觉语义投影到神经响应流形，促进高保真fMRI合成。&lt;h4&gt;主要发现&lt;/h4&gt;SynBrain在特定主体的视觉到fMRI编码性能上超越了最先进方法；能够高效适应新主体，使用少量数据；合成的信号能有效提高数据有限情况下的fMRI到图像解码性能；揭示了试验和主体之间的功能一致性，合成的信号捕捉了由生物神经变性的可解释模式。&lt;h4&gt;结论&lt;/h4&gt;SynBrain是一个有效的生成框架，能够模拟视觉语义到神经响应的转化，同时捕捉生物变性和功能一致性，为视觉神经编码研究提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;解读视觉刺激如何转化为皮层响应是计算神经科学中的一个基本挑战。这种视觉到神经映射本质上是一对多的关系，因为相同的视觉输入在不同试验、情境和主体中会可靠地引发不同的血流动力学响应。然而，现有的确定性方法难以同时建模这种生物变性和编码刺激信息的基本功能一致性。为了解决这些局限性，我们提出了SynBrain，一个生成框架，能够以概率化和生物学可解释的方式模拟视觉语义到神经响应的转化。SynBrain引入了两个关键组件：(i) BrainVAE模型通过概率学习将神经表示建模为连续概率分布，同时通过视觉语义约束保持功能一致性；(ii) 语义到神经映射器作为语义传输通路，将视觉语义投影到神经响应流形，促进高保真fMRI合成。实验结果表明，SynBrain在特定主体的视觉到fMRI编码性能上超越了最先进的方法。此外，SynBrain能够高效适应新主体，使用少量数据，并合成高质量的fMRI信号，这些信号在提高数据有限的fMRI到图像解码性能方面是有效的。除此之外，SynBrain揭示了试验和主体之间的功能一致性，合成的信号捕捉了由生物神经变性的可解释模式。代码将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deciphering how visual stimuli are transformed into cortical responses is afundamental challenge in computational neuroscience. This visual-to-neuralmapping is inherently a one-to-many relationship, as identical visual inputsreliably evoke variable hemodynamic responses across trials, contexts, andsubjects. However, existing deterministic methods struggle to simultaneouslymodel this biological variability while capturing the underlying functionalconsistency that encodes stimulus information. To address these limitations, wepropose SynBrain, a generative framework that simulates the transformation fromvisual semantics to neural responses in a probabilistic and biologicallyinterpretable manner. SynBrain introduces two key components: (i) BrainVAEmodels neural representations as continuous probability distributions viaprobabilistic learning while maintaining functional consistency through visualsemantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantictransmission pathway, projecting visual semantics into the neural responsemanifold to facilitate high-fidelity fMRI synthesis. Experimental resultsdemonstrate that SynBrain surpasses state-of-the-art methods insubject-specific visual-to-fMRI encoding performance. Furthermore, SynBrainadapts efficiently to new subjects with few-shot data and synthesizeshigh-quality fMRI signals that are effective in improving data-limitedfMRI-to-image decoding performance. Beyond that, SynBrain reveals functionalconsistency across trials and subjects, with synthesized signals capturinginterpretable patterns shaped by biological neural variability. The code willbe made publicly available.</description>
      <author>example@mail.com (Weijian Mai, Jiamin Wu, Yu Zhu, Zhouheng Yao, Dongzhan Zhou, Andrew F. Luo, Qihao Zheng, Wanli Ouyang, Chunfeng Song)</author>
      <guid isPermaLink="false">2508.10298v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation</title>
      <link>http://arxiv.org/abs/2508.10281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个新的时间动作分割(TAS)框架，专门用于识别花样滑冰跳跃动作的类型和时间，该框架结合了三维特性和跳跃动作的语义过程。&lt;h4&gt;背景&lt;/h4&gt;从视频中理解人类动作在多个领域都起着关键作用，包括体育分析。在花样滑冰中，准确识别滑冰者跳跃的类型和时间对于客观评估表现至关重要。然而，由于跳跃过程的细粒度和复杂性，这项任务通常需要专家级知识。现有的TAS方法在应用于花样滑冰时有两个主要限制：标注数据不足，且现有方法没有考虑跳跃动作固有的三维方面和程序结构。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的TAS框架，专门用于花样滑冰跳跃识别，该框架明确结合了跳跃动作的三维特性和语义过程。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的视角不变的、花样滑冰特定的姿态表示学习方法(VIFSS)，结合对比学习作为预训练和动作分类作为微调；构建了FS-Jump3D，这是第一个专门用于花样滑冰跳跃的公开可用3D姿态数据集；引入了一种细粒度标注方案，标记'入场(准备)'和'落地'阶段，使TAS模型能够学习跳跃的程序结构。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在元素级别的TAS上实现了超过92%的F1@50，需要识别跳跃类型和旋转级别；当微调数据有限时，视角不变的对比预训练特别有效，突显了该方法在实际场景中的实用性。&lt;h4&gt;结论&lt;/h4&gt;提出的TAS框架有效解决了花样滑冰跳跃识别中的挑战，通过结合三维特性和语义过程，实现了高精度的跳跃识别，特别是在数据有限的情况下表现良好。&lt;h4&gt;翻译&lt;/h4&gt;从视频中理解人类动作在各个领域都起着关键作用，包括体育分析。在花样滑冰中，准确识别滑冰者跳跃的类型和时间对于客观评估表现至关重要。然而，由于跳跃过程的细粒度和复杂性，这项任务通常需要专家级知识。虽然最近的方法试图使用时间动作分割(TAS)来自动化这项任务，但TAS在应用于花样滑冰时有两个主要限制：标注数据不足，且现有方法没有考虑跳跃动作固有的三维方面和程序结构。在这项工作中，我们提出了一个新的花样滑冰跳跃TAS框架，明确结合了跳跃动作的三维特性和语义过程。首先，我们提出了一种新的视角不变的、花样滑冰特定的姿态表示学习方法(VIFSS)，结合了对比学习作为预训练和动作分类作为微调。对于视角不变的对比预训练，我们构建了FS-Jump3D，这是第一个专门用于花样滑冰跳跃的公开可用3D姿态数据集。其次，我们引入了一种细粒度标注方案，标记'入场(准备)'和'落地'阶段，使TAS模型能够学习跳跃的程序结构。大量实验证明了我们框架的有效性。我们的方法在元素级别的TAS上实现了超过92%的F1@50，需要识别跳跃类型和旋转级别。此外，我们表明当微调数据有限时，视角不变的对比预训练特别有效，突显了我们的方法在实际场景中的实用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决花样滑冰跳跃动作的时间动作分割(TAS)问题，即自动识别视频中跳跃的类型和精确时机。这个问题在现实中很重要，因为当前花样滑冰评判依赖人工识别和记录跳跃类型和时机，需要专家知识和大量时间精力。自动化这一过程可以减轻裁判负担，提高评判效率和客观性，同时为花样滑冰训练提供更精确的反馈。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D姿势特征对视角变化敏感，而直接使用3D姿势坐标存在质量瓶颈。他们设计了两阶段学习框架：先用对比学习预训练姿势编码器获取视角不变的表示，然后在花样滑冰特定动作分类任务上微调。借鉴了Pr-VIPE和CV-MIM的对比学习思想、JointFormer作为姿势编码器，以及基于BiGRU的时序建模架构。但自主创新了FS-Jump3D数据集构建和跳跃程序感知的标注策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习既视角不变又适应花样滑冰特定动作的姿势表示，并利用跳跃的程序结构信息。整体流程分为三阶段：1)构建FS-Jump3D数据集和细粒度标注；2)姿势表示学习，包括视角不变的对比预训练和花样滑冰特定的动作分类微调；3)将学习到的VIFSS姿势特征输入TAS模型进行时间动作分割。对比预训练阶段通过随机虚拟相机生成同一姿势的不同视角，微调阶段使用SkatingVerse数据集使模型适应花样滑冰动作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)FS-Jump3D数据集，首个专门针对花样滑冰跳跃的3D姿势数据集；2)细粒度跳跃程序感知标注策略，将跳跃分为进入、跳跃和着陆三阶段；3)VIFSS姿势表示学习框架，结合对比预训练和领域特定微调。相比之前工作，不同之处在于：使用3D姿势而非2D姿势捕捉动作三维特性；考虑跳跃的完整程序结构而非仅跳跃本身；通过姿势嵌入而非直接坐标表示更高级的运动语义；在标注数据有限情况下表现优异。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了VIFSS方法，通过首个花样滑冰3D姿势数据集和跳跃程序感知的细粒度标注策略，结合视角不变的对比预训练和领域特定的动作分类微调，显著提升了花样滑冰跳跃动作时间分割任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding human actions from videos plays a critical role across variousdomains, including sports analytics. In figure skating, accurately recognizingthe type and timing of jumps a skater performs is essential for objectiveperformance evaluation. However, this task typically requires expert-levelknowledge due to the fine-grained and complex nature of jump procedures. Whilerecent approaches have attempted to automate this task using Temporal ActionSegmentation (TAS), there are two major limitations to TAS for figure skating:the annotated data is insufficient, and existing methods do not account for theinherent three-dimensional aspects and procedural structure of jump actions. Inthis work, we propose a new TAS framework for figure skating jumps thatexplicitly incorporates both the three-dimensional nature and the semanticprocedure of jump movements. First, we propose a novel View-Invariant, FigureSkating-Specific pose representation learning approach (VIFSS) that combinescontrastive learning as pre-training and action classification as fine-tuning.For view-invariant contrastive pre-training, we construct FS-Jump3D, the firstpublicly available 3D pose dataset specialized for figure skating jumps.Second, we introduce a fine-grained annotation scheme that marks the ``entry(preparation)'' and ``landing'' phases, enabling TAS models to learn theprocedural structure of jumps. Extensive experiments demonstrate theeffectiveness of our framework. Our method achieves over 92% F1@50 onelement-level TAS, which requires recognizing both jump types and rotationlevels. Furthermore, we show that view-invariant contrastive pre-training isparticularly effective when fine-tuning data is limited, highlighting thepracticality of our approach in real-world scenarios.</description>
      <author>example@mail.com (Ryota Tanaka, Tomohiro Suzuki, Keisuke Fujii)</author>
      <guid isPermaLink="false">2508.10281v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization</title>
      <link>http://arxiv.org/abs/2508.09560v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了WeatherPrompt，一种多模态学习范式，通过融合图像嵌入和文本上下文建立天气不变表示，解决无人机视觉地理定位在恶劣天气条件下的性能下降问题。方法包含无需训练的天气推理机制和由文本嵌入驱动的动态门控机制，在多种天气条件下实现了优于现有方法的召回率。&lt;h4&gt;背景&lt;/h4&gt;无人机视觉地理定位在天气扰动（如雨和雾）下面临严重性能下降问题。现有方法有两个固有局限性：1) 严重依赖于有限的天气类别，限制了泛化能力；2) 通过伪天气类别对纠缠的场景-天气特征进行解缠分是不优的。&lt;h4&gt;目的&lt;/h4&gt;提出WeatherPrompt方法，通过融合图像嵌入和文本上下文来建立天气不变表示，解决现有方法在天气变化条件下的地理定位性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;WeatherPrompt是一个多模态学习范式，包含两个关键贡献：1) 无需训练的天气推理机制，利用现成的多模态模型合成多天气文本描述，提高对复杂天气的可扩展性；2) 由文本嵌入驱动的动态门控机制的多模态框架，自适应地重新加权并跨模态融合视觉特征，通过图文对比学习和图文匹配等跨模态目标进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在多种天气条件下，WeatherPrompt方法实现了与最先进的无人机地理定位方法相竞争的召回率。特别是在夜间条件下，Recall@1提高了13.37%，在雾和雪条件下提高了18.69%。&lt;h4&gt;结论&lt;/h4&gt;WeatherPrompt方法通过多模态学习和天气推理机制，有效解决了无人机视觉地理定位在恶劣天气条件下的性能下降问题，显著提高了不同天气条件下的定位精度。&lt;h4&gt;翻译&lt;/h4&gt;无人机视觉地理定位在天气扰动（如下雨和起雾）下面临严重性能下降，现有方法存在两个固有局限：1) 严重依赖有限的天气类别，限制了泛化能力；2) 通过伪天气类别对纠缠的场景-天气特征进行解缠分是不优的。我们提出了WeatherPrompt，一种多模态学习范式，通过融合图像嵌入和文本上下文建立天气不变表示。我们的框架引入两个关键贡献：首先，一种无需训练的天气推理机制，利用现成的多模态模型合成多天气文本描述，提高对未见或复杂天气的可扩展性，并能反映不同的天气强度。其次，为了更好地解缠场景和天气特征，我们提出了一个由文本嵌入驱动的动态门控机制的多模态框架，能够自适应地重新加权并跨模态融合视觉特征。该框架通过跨模态目标（包括图文对比学习和图文匹配）进一步优化，使不同天气条件下的同一场景在表示空间中更加接近。大量实验验证了，在多种天气条件下，我们的方法与最先进的无人机地理定位方法相比具有竞争力的召回率。值得注意的是，它在夜间条件下将Recall@1提高了13.37%，在雾和雪条件下提高了18.69%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual geo-localization for drones faces critical degradation under weatherperturbations, \eg, rain and fog, where existing methods struggle with twoinherent limitations: 1) Heavy reliance on limited weather categories thatconstrain generalization, and 2) Suboptimal disentanglement of entangledscene-weather features through pseudo weather categories. We presentWeatherPrompt, a multi-modality learning paradigm that establishesweather-invariant representations through fusing the image embedding with thetext context. Our framework introduces two key contributions: First, aTraining-free Weather Reasoning mechanism that employs off-the-shelf largemulti-modality models to synthesize multi-weather textual descriptions throughhuman-like reasoning. It improves the scalability to unseen or complex weather,and could reflect different weather strength. Second, to better disentangle thescene and weather feature, we propose a multi-modality framework with thedynamic gating mechanism driven by the text embedding to adaptively reweightand fuse visual features across modalities. The framework is further optimizedby the cross-modal objectives, including image-text contrastive learning andimage-text matching, which maps the same scene with different weatherconditions closer in the respresentation space. Extensive experiments validatethat, under diverse weather conditions, our method achieves competitive recallrates compared to state-of-the-art drone geo-localization methods. Notably, itimproves Recall@1 by +13.37\% under night conditions and by 18.69\% under fogand snow conditions.</description>
      <author>example@mail.com (Jiahao Wen, Hang Yu, Zhedong Zheng)</author>
      <guid isPermaLink="false">2508.09560v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.10785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in ECAI-2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DECAF-GAD的框架，用于解决基于自编码器的图异常检测模型中的公平性问题，能够在减轻偏见的同时保持异常检测性能。&lt;h4&gt;背景&lt;/h4&gt;图异常检测（GAD）在多个领域变得越来越重要，图神经网络的发展显著提高了GAD方法的性能。然而，GAD中的公平性问题尚未得到充分探索，基于GNN的GAD模型可能会继承和放大训练数据中的偏见，导致不公平结果。现有公平性研究大多针对节点分类任务，而非异常检测任务，后者通常使用基于自编码器的结构。&lt;h4&gt;目的&lt;/h4&gt;解决基于自编码器的图异常检测模型中的公平性问题，减轻模型偏见的同时保持异常检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出DECAF-GAD（DisEntangled Counterfactual Adversarial Fair GAD）框架，引入结构因果模型（SCM）将敏感属性与学习到的表示分离，基于此因果框架设计专门的自编码器架构和公平性引导的损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;通过在合成和真实世界数据集上的大量实验，DECAF-GAD不仅实现了具有竞争力的异常检测性能，相比基准GAD方法还显著提高了公平性指标。&lt;h4&gt;结论&lt;/h4&gt;DECAF-GAD框架能够有效减轻图异常检测模型中的偏见问题，同时保持优异的异常检测性能。&lt;h4&gt;翻译&lt;/h4&gt;图异常检测（GAD）已成为各个领域中日益重要的任务。随着图神经网络（GNNs）的快速发展，GAD方法已取得了显著的性能提升。然而，GAD中的公平性考虑在很大程度上仍未被充分探索。实际上，基于GNN的GAD模型可能会继承和放大训练数据中存在的偏见，从而导致不公平的结果。虽然现有工作主要集中在开发公平的GNN上，但大多数方法针对的是节点分类任务，在这些任务中，模型通常依赖简单的层架构而非自编码器结构，而后者是异常检测应用中最广泛使用的架构。为了解决基于自编码器的GAD模型中的公平性问题，我们提出了DECAF-GAD（DisEntangled Counterfactual Adversarial Fair GAD）框架，该框架能够在减轻偏见的同时保持GAD性能。具体而言，我们引入了一个结构因果模型（SCM）来将敏感属性与学习到的表示分离。基于这个因果框架，我们制定了一个专门的自编码器架构以及一个公平性引导的损失函数。通过在合成和真实世界数据集上的大量实验，我们证明了DECAF-GAD不仅实现了具有竞争力的异常检测性能，而且与基准GAD方法相比显著提高了公平性指标。我们的代码可在https://github.com/Tlhey/decaf_code获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection (GAD) has become an increasingly important taskacross various domains. With the rapid development of graph neural networks(GNNs), GAD methods have achieved significant performance improvements.However, fairness considerations in GAD remain largely underexplored. Indeed,GNN-based GAD models can inherit and amplify biases present in training data,potentially leading to unfair outcomes. While existing efforts have focused ondeveloping fair GNNs, most approaches target node classification tasks, wheremodels often rely on simple layer architectures rather than autoencoder-basedstructures, which are the most widely used architecturs for anomaly detection.To address fairness in autoencoder-based GAD models, we propose\textbf{D}is\textbf{E}ntangled \textbf{C}ounterfactual \textbf{A}dversarial\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preservingGAD performance. Specifically, we introduce a structural causal model (SCM) todisentangle sensitive attributes from learned representations. Based on thiscausal framework, we formulate a specialized autoencoder architecture alongwith a fairness-guided loss function. Through extensive experiments on bothsynthetic and real-world datasets, we demonstrate that DECAF-GAD not onlyachieves competitive anomaly detection performance but also significantlyenhances fairness metrics compared to baseline GAD methods. Our code isavailable at https://github.com/Tlhey/decaf_code.</description>
      <author>example@mail.com (Shouju Wang, Yuchen Song, Sheng'en Li, Dongmian Zou)</author>
      <guid isPermaLink="false">2508.10785v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning</title>
      <link>http://arxiv.org/abs/2508.10747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种稀疏的、目标感知的图神经网络表示方法，用于解决广义规划中的密集图表示问题，有效提高了大规模网格环境中的规划性能。&lt;h4&gt;背景&lt;/h4&gt;使用深度强化学习结合图神经网络进行广义规划在PDDL描述的符号规划领域显示出良好结果，但现有方法通常将规划状态表示为全连接图，导致边缘信息组合爆炸和大规模问题下的稀疏性，特别是在大型网格环境中。&lt;h4&gt;目的&lt;/h4&gt;解决密集表示导致节点级信息稀释、内存需求呈指数增长以及大规模问题学习不可行的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出一种稀疏的、目标感知的GNN表示方法，选择性编码相关的局部关系，并明确整合与目标相关的空间特征；在网格世界内基于PDDL设计新的无人机任务场景，有效模拟真实的任务执行环境。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法能够有效扩展到密集图表示不可行的大型网格尺寸，并显著提高策略泛化和成功率。&lt;h4&gt;结论&lt;/h4&gt;研究结果为解决现实的大规模广义规划任务提供了实际基础。&lt;h4&gt;翻译&lt;/h4&gt;使用深度强化学习结合图神经网络的广义规划已在各种由PDDL描述的符号规划领域显示出良好的结果。然而，现有方法通常将规划状态表示为全连接图，导致边缘信息组合爆炸和问题规模增长时的显著稀疏性，特别是在大型网格环境中尤为明显。这种密集表示导致节点级信息稀释，内存需求呈指数增长，最终使得大规模问题的学习变得不可行。为应对这些挑战，我们提出了一种稀疏的、目标感知的GNN表示，选择性编码相关的局部关系并明确整合与目标相关的空间特征。我们通过在网格世界内基于PDDL设计新的无人机任务场景来验证我们的方法，有效模拟了真实的任务执行环境。实验结果表明，我们的方法能够有效扩展到密集图表示不可行的大型网格尺寸，并显著提高策略泛化和成功率。我们的发现为解决现实的大规模广义规划任务提供了实际基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized planning using deep reinforcement learning (RL) combined withgraph neural networks (GNNs) has shown promising results in various symbolicplanning domains described by PDDL. However, existing approaches typicallyrepresent planning states as fully connected graphs, leading to a combinatorialexplosion in edge information and substantial sparsity as problem scales grow,especially evident in large grid-based environments. This dense representationresults in diluted node-level information, exponentially increases memoryrequirements, and ultimately makes learning infeasible for larger-scaleproblems. To address these challenges, we propose a sparse, goal-aware GNNrepresentation that selectively encodes relevant local relationships andexplicitly integrates spatial features related to the goal. We validate ourapproach by designing novel drone mission scenarios based on PDDL within a gridworld, effectively simulating realistic mission execution environments. Ourexperimental results demonstrate that our method scales effectively to largergrid sizes previously infeasible with dense graph representations andsubstantially improves policy generalization and success rates. Our findingsprovide a practical foundation for addressing realistic, large-scalegeneralized planning tasks.</description>
      <author>example@mail.com (Sangwoo Jeon, Juchul Shin, Gyeong-Tae Kim, YeonJe Cho, Seongwoo Kim)</author>
      <guid isPermaLink="false">2508.10747v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Graph Learning via Logic-Based Weisfeiler-Leman Variants and Tabularization</title>
      <link>http://arxiv.org/abs/2508.10651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于Weisfeiler-Leman算法变体的图分类新方法，通过将图数据表格化后应用表格数据方法，在保持与最先进方法相当准确性的同时提高了效率。&lt;h4&gt;背景&lt;/h4&gt;图分类是机器学习中的重要任务，传统方法如图神经网络和图核方法在准确性和效率方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的图分类方法，能够达到与现有最先进方法相当的准确性，同时在计算效率方面有所提升。&lt;h4&gt;方法&lt;/h4&gt;通过修改底层逻辑框架获得Weisfeiler-Leman算法的变体，将图数据表格化，然后应用表格数据方法进行分类，并在十二个基准数据集上进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法能够达到与最先进的图神经网络和图核方法相当的准确性，同时在时间或内存效率方面更具优势，具体取决于数据集。&lt;h4&gt;结论&lt;/h4&gt;基于Weisfeiler-Leman变体的表格化图分类方法是一种有效且高效的图分类解决方案，同时研究还展示了直接从图数据提取可解释模态逻辑公式的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的图分类方法，通过Weisfeiler-Leman算法的变体将图数据表格化，然后应用表格数据方法。我们研究了一类通过修改底层逻辑框架获得的Weisfeiler-Leman变体，并建立了它们表达能力的精确理论表征。随后，我们在十二个涵盖不同领域的基准数据集上测试了两种选定的变体。实验表明，我们的方法能够达到最先进的图神经网络和图核方法的准确性，同时在时间或内存效率方面更具优势，具体取决于数据集。我们还简要讨论了直接从图数据集中提取可解释的模态逻辑公式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel approach for graph classification based on tabularizinggraph data via variants of the Weisfeiler-Leman algorithm and then applyingmethods for tabular data. We investigate a comprehensive class ofWeisfeiler-Leman variants obtained by modifying the underlying logicalframework and establish a precise theoretical characterization of theirexpressive power. We then test two selected variants on twelve benchmarkdatasets that span a range of different domains. The experiments demonstratethat our approach matches the accuracy of state-of-the-art graph neuralnetworks and graph kernels while being more time or memory efficient, dependingon the dataset. We also briefly discuss directly extracting interpretable modallogic formulas from graph datasets.</description>
      <author>example@mail.com (Reijo Jaakkola, Tomi Janhunen, Antti Kuusisto, Magdalena Ortiz, Matias Selin, Mantas Šimkus)</author>
      <guid isPermaLink="false">2508.10651v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>GNN-based Unified Deep Learning</title>
      <link>http://arxiv.org/abs/2508.10583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为'统一学习'的新范式，用于解决医学影像领域中深度学习模型在不同分布下的泛化能力问题。该方法通过将不同架构的模型编码为图表示，在共享图学习空间中实现统一，并通过GNN指导模型优化，实现了跨架构和分布的知识迁移，提高了模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型在医学影像领域往往难以维持泛化能力，特别是在领域断裂场景下，即当分布因成像技术、采集协议、患者群体、人口统计和设备的差异而发生变化时。实践中，每家医院可能需要训练不同参数（学习任务、宽度和深度）的本地模型，例如使用欧几里得架构（MLPs、CNNs）处理规则数据，或使用非欧几里得架构（GNNs）处理不规则数据（如脑连接组）。&lt;h4&gt;目的&lt;/h4&gt;研究目的是解决如何跨数据集一致地训练这些异构模型，同时提高每个模型泛化能力的开放问题。&lt;h4&gt;方法&lt;/h4&gt;研究提出了'统一学习'这一新范式，将每个模型编码为图表示，在共享图学习空间中实现统一。通过解耦单个模型的参数，并通过统一的GNN（uGNN）控制它们，支持在不同架构（MLPs、CNNs、GNNs）和分布之间进行参数共享和知识迁移。&lt;h4&gt;主要发现&lt;/h4&gt;在MorphoMNIST和两个MedMNIST基准（PneumoniaMNIST和BreastMNIST）上的评估表明，统一学习提高了模型在独特分布上训练并在混合分布上测试时的性能，显示出对具有大分布偏移的未见数据的强大鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;统一学习是一种有效的范式，能够提高医学影像领域深度学习模型在不同分布下的泛化能力，通过实现跨架构和分布的知识迁移，增强了模型对未见数据的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型通常难以在医学影像中保持泛化能力，特别是在领域断裂场景下，当分布因不同的成像技术、采集协议、患者群体、人口统计和设备而变化时。在实践中，每家医院可能需要训练不同的模型（学习任务、宽度和深度各不相同）以匹配本地数据。例如，一家医院可能使用欧几里得架构（如MLPs和CNNs）处理表格或网格状图像数据，而另一家医院可能需要非欧几里得架构（如图神经网络GNNs）处理不规则数据（如脑连接组）。如何跨数据集一致地训练这些异构模型，同时提高每个模型的泛化能力，仍然是一个开放问题。我们提出了统一学习，一种新范式，它将每个模型编码为图表示，在共享图学习空间中实现统一。然后，一个GNN指导这些统一模型的优化。通过解耦单个模型的参数并通过统一的GNN（uGNN）控制它们，我们的方法支持在不同架构（MLPs、CNNs、GNNs）和分布之间进行参数共享和知识迁移，提高泛化能力。在MorphoMNIST和两个MedMNIST基准（PneumoniaMNIST和BreastMNIST）上的评估表明，当模型在独特分布上训练并在混合分布上测试时，统一学习提高了性能，显示出对具有大分布偏移的未见数据的强大鲁棒性。代码和基准：https://github.com/basiralab/uGNN&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models often struggle to maintain generalizability in medicalimaging, particularly under domain-fracture scenarios where distribution shiftsarise from varying imaging techniques, acquisition protocols, patientpopulations, demographics, and equipment. In practice, each hospital may needto train distinct models - differing in learning task, width, and depth - tomatch local data. For example, one hospital may use Euclidean architecturessuch as MLPs and CNNs for tabular or grid-like image data, while another mayrequire non-Euclidean architectures such as graph neural networks (GNNs) forirregular data like brain connectomes. How to train such heterogeneous modelscoherently across datasets, while enhancing each model's generalizability,remains an open problem. We propose unified learning, a new paradigm thatencodes each model into a graph representation, enabling unification in ashared graph learning space. A GNN then guides optimization of these unifiedmodels. By decoupling parameters of individual models and controlling themthrough a unified GNN (uGNN), our method supports parameter sharing andknowledge transfer across varying architectures (MLPs, CNNs, GNNs) anddistributions, improving generalizability. Evaluations on MorphoMNIST and twoMedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unifiedlearning boosts performance when models are trained on unique distributions andtested on mixed ones, demonstrating strong robustness to unseen data with largedistribution shifts. Code and benchmarks: https://github.com/basiralab/uGNN</description>
      <author>example@mail.com (Furkan Pala, Islem Rekik)</author>
      <guid isPermaLink="false">2508.10583v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation</title>
      <link>http://arxiv.org/abs/2508.10471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GraphFedMIG，一种新的联邦图学习框架，用于解决联邦学习中的统计异质性和类别不平衡问题，特别是针对少数类节点的识别挑战。&lt;h4&gt;背景&lt;/h4&gt;联邦图学习(FGL)允许多个客户端在不共享私有图数据的情况下协作训练图神经网络。FGL面临统计异质性的挑战，即客户端间的非独立同分布数据分布会严重影响模型性能。类别不平衡是一种特别具有破坏性的形式，导致全局模型偏向多数类，无法识别少数但关键的事件。在FGL中，这一问题更加严重，因为少数类节点通常被有偏见的邻域信息包围，阻碍了表达性嵌入的学习。&lt;h4&gt;目的&lt;/h4&gt;解决联邦图学习中由统计异质性和类别不平衡导致的模型性能问题，特别是提高对少数类节点的识别能力。&lt;h4&gt;方法&lt;/h4&gt;提出GraphFedMIG框架，将问题重新构造成联邦生成数据增强任务。采用分层生成对抗网络，每个客户端训练本地生成器来合成高保真特征表示。客户端被分组到聚类中，每个聚类共享一个专用的判别器。设计了一种互信息引导机制来指导这些客户端生成器的演变，通过计算每个客户端的独特信息价值，修正本地生成器参数，确保后续生成专注于产生高价值的少数类特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实数据集上进行的广泛实验结果表明，与其它基线相比，所提出的GraphFedMIG具有优越性。&lt;h4&gt;结论&lt;/h4&gt;GraphFedMIG框架有效解决了联邦图学习中的统计异质性和类别不平衡问题，能够更好地识别少数类节点，提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;联邦图学习(FGL)使多个客户端能够在不共享其私有、分散的图数据的情况下协作训练强大的图神经网络。继承自通用联邦学习，FGL面临着统计异质性的严重挑战，即客户端间的非独立同分布数据分布会严重影响模型性能。其中特别具有破坏性的一种形式是类别不平衡，它导致全局模型偏向多数类，无法识别少数但关键的事件。在FGL中，这一问题更加严重，因为少数类节点通常被有偏见的邻域信息包围，阻碍了表达性嵌入的学习。为应对这一挑战，我们提出了GraphFedMIG，一种新颖的FGL框架，将问题重新构造成联邦生成数据增强任务。GraphFedMIG采用分层生成对抗网络，每个客户端训练本地生成器来合成高保真特征表示。为了提供定制化的监督，客户端被分组到聚类中，每个聚类共享一个专用的判别器。关键的是，该框架设计了一种互信息引导机制来指导这些客户端生成器的演变。通过计算每个客户端的独特信息价值，该机制修正本地生成器参数，确保后续互信息引导的生成专注于产生高价值的少数类特征。我们在四个真实数据集上进行了广泛的实验，结果表明与其它基线相比，所提出的GraphFedMIG具有优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated graph learning (FGL) enables multiple clients to collaborativelytrain powerful graph neural networks without sharing their private,decentralized graph data. Inherited from generic federated learning, FGL iscritically challenged by statistical heterogeneity, where non-IID datadistributions across clients can severely impair model performance. Aparticularly destructive form of this is class imbalance, which causes theglobal model to become biased towards majority classes and fail at identifyingrare but critical events. This issue is exacerbated in FGL, as nodes from aminority class are often surrounded by biased neighborhood information,hindering the learning of expressive embeddings. To grapple with thischallenge, we propose GraphFedMIG, a novel FGL framework that reframes theproblem as a federated generative data augmentation task. GraphFedMIG employs ahierarchical generative adversarial network where each client trains a localgenerator to synthesize high-fidelity feature representations. To providetailored supervision, clients are grouped into clusters, each sharing adedicated discriminator. Crucially, the framework designs a mutualinformation-guided mechanism to steer the evolution of these client generators.By calculating each client's unique informational value, this mechanismcorrects the local generator parameters, ensuring that subsequent rounds ofmutual information-guided generation are focused on producing high-value,minority-class features. We conduct extensive experiments on four real-worlddatasets, and the results demonstrate the superiority of the proposedGraphFedMIG compared with other baselines.</description>
      <author>example@mail.com (Xinrui Li, Qilin Fan, Tianfu Wang, Kaiwen Wei, Ke Yu, Xu Zhang)</author>
      <guid isPermaLink="false">2508.10471v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Influence Maximization in Multi-layer Social Networks Based on Differentiated Graph Embeddings</title>
      <link>http://arxiv.org/abs/2508.10289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Inf-MDE，一种新颖的多层影响力最大化方法，利用差异化图嵌入解决社交网络中识别影响力节点的问题&lt;h4&gt;背景&lt;/h4&gt;现有方法通常忽略本地意见领袖倾向，导致种子节点影响力范围重叠；基于普通图神经网络的方法在消息传递中难以有效聚合影响力特征；当前技术未能充分处理社交网络的多层特性和节点异质性&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在识别影响力节点时的局限性，包括忽略本地意见领袖倾向、无法有效聚合影响力特征、未能处理社交网络的多层特性和节点异质性&lt;h4&gt;方法&lt;/h4&gt;提出Inf-MDE方法，使用多层网络结构建模社交关系，提取自影响力传播子图消除节点嵌入与传播动力学间的表示偏差，并在图神经网络设计中集成自适应局部影响力聚合机制，根据局部上下文和影响力强度动态调整影响力特征聚合&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同的多层社交网络数据集上的实验表明，Inf-MDE显著优于现有的最先进方法&lt;h4&gt;结论&lt;/h4&gt;Inf-MDE通过解决现有方法的局限性，能够更有效地识别社交网络中的影响力节点，为社交网络分析提供了新的解决方案&lt;h4&gt;翻译&lt;/h4&gt;在社交网络分析中识别影响力节点至关重要。现有方法通常忽略本地意见领袖的倾向，导致种子节点的影响力范围重叠。此外，基于普通图神经网络的方法在消息传递过程中难以有效聚合影响力特征，特别是在影响力强度不同的情况下。当前技术也未能充分解决社交网络的多层特性和节点异质性。为解决这些问题，本文提出了Inf-MDE，一种新颖的利用差异化图嵌入的多层影响力最大化方法。Inf-MDE使用多层网络结构建模社交关系。该模型提取自影响力传播子图以消除节点嵌入与传播动力学之间的表示偏差。此外，Inf-MDE在其图神经网络设计中集成了自适应局部影响力聚合机制。该机制根据局部上下文和影响力强度在消息传递过程中动态调整影响力特征聚合，使其能够有效捕获层间传播异质性和层内扩散动力学。在四个不同的多层社交网络数据集上进行的大量实验表明，Inf-MDE显著优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying influential nodes is crucial in social network analysis. Existingmethods often neglect local opinion leader tendencies, resulting in overlappinginfluence ranges for seed nodes. Furthermore, approaches based on vanilla graphneural networks (GNNs) struggle to effectively aggregate influencecharacteristics during message passing, particularly with varying influenceintensities. Current techniques also fail to adequately address the multi-layernature of social networks and node heterogeneity. To address these issues, thispaper proposes Inf-MDE, a novel multi-layer influence maximization methodleveraging differentiated graph embedding. Inf-MDE models social relationshipsusing a multi-layer network structure. The model extracts a self-influencepropagation subgraph to eliminate the representation bias between nodeembeddings and propagation dynamics. Additionally, Inf-MDE incorporates anadaptive local influence aggregation mechanism within its GNN design. Thismechanism dynamically adjusts influence feature aggregation during messagepassing based on local context and influence intensity, enabling it toeffectively capture both inter-layer propagation heterogeneity and intra-layerdiffusion dynamics. Extensive experiments across four distinct multi-layersocial network datasets demonstrate that Inf-MDE significantly outperformsstate-of-the-art methods.</description>
      <author>example@mail.com (Ronghua Lin, Runbin Yao, Yijia Wang, Junjie Lin, Zhengyang Wu, Yong Tang)</author>
      <guid isPermaLink="false">2508.10289v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Less is More: Learning Graph Tasks with Just LLMs</title>
      <link>http://arxiv.org/abs/2508.10115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了大型语言模型在图推理方面的能力，发现即使小型LLMs也能通过训练解决图任务并实现泛化，无需专门的图编码器。&lt;h4&gt;背景&lt;/h4&gt;对于大型语言模型来说，对图进行推理可以帮助解决许多问题。先前的工作试图通过研究如何将图序列化为文本以及如何结合图神经网络和LLMs来改进LLM的图推理能力，但这些方法的优点尚不明确。&lt;h4&gt;目的&lt;/h4&gt;通过实证研究回答三个研究问题：(1) LLMs是否能在没有专门图编码模型的情况下学习解决基本图任务？(2) LLMs是否能将学到的解决方案推广到未见过的图结构或任务？(3) 竞争性方法在学习图任务方面的优点是什么？&lt;h4&gt;方法&lt;/h4&gt;训练小型LLMs解决图任务，使用指导性的思维链（instructive chain-of-thought）解决方案进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;即使小型LLMs也能学习解决图任务，这种训练可以推广到新的任务和图结构，而无需专门的图编码器。&lt;h4&gt;结论&lt;/h4&gt;研究表明LLMs具有解决图任务的潜力，并通过适当的训练方法实现了泛化能力，为未来在图推理领域应用LLMs提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;对于大型语言模型（LLMs）来说，对图进行推理可以帮助解决许多问题。先前的工作试图通过研究如何将图序列化为文本以及如何结合图神经网络（GNNs）和LLMs来改进LLM的图推理能力。然而，这些方法的优点尚不清楚，因此我们通过实证研究回答了以下研究问题：(1) LLMs是否能够在没有专门的图编码模型的情况下学习解决基本的图任务？(2) LLMs是否能够将学到的解决方案推广到未见过的图结构或任务上？(3) 竞争性方法在学习图任务方面的优点是什么？我们表明，即使小型LLMs也能够通过使用指导性的思维链解决方案进行训练来学习解决图任务，并且这种训练可以推广到新的任务和图结构，而无需专门的图编码器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For large language models (LLMs), reasoning over graphs could help solve manyproblems. Prior work has tried to improve LLM graph reasoning by examining howbest to serialize graphs as text and by combining GNNs and LLMs. However, themerits of such approaches remain unclear, so we empirically answer thefollowing research questions: (1) Can LLMs learn to solve fundamental graphtasks without specialized graph encoding models?, (2) Can LLMs generalizelearned solutions to unseen graph structures or tasks?, and (3) What are themerits of competing approaches to learn graph tasks? We show that even smallLLMs can learn to solve graph tasks by training them with instructivechain-of-thought solutions, and this training generalizes, without specializedgraph encoders, to new tasks and graph structures.</description>
      <author>example@mail.com (Sola Shirai, Kavitha Srinivas, Julian Dolby, Michael Katz, Horst Samulowitz, Shirin Sohrabi)</author>
      <guid isPermaLink="false">2508.10115v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Quantum Mechanics to Organic Liquid Properties via a Universal Force Field</title>
      <link>http://arxiv.org/abs/2508.08575v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究人员开发了ByteFF-Pol，一种基于图神经网络的极化力场，能够基于量子力学数据准确预测宏观性质，克服了计算成本与精度之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;分子动力学模拟是研究凝聚相系统原子级结构和动力学的必要工具，但从从头算计算准确预测宏观属性仍面临重大挑战，通常受限于计算成本和模拟精度之间的权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够基于高水平量子力学数据进行训练的力场，以准确预测宏观性质，弥合微观计算与宏观性质之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出ByteFF-Pol，一种图神经网络(GNN)参数化的可极化力场，完全基于高水平的量子力学数据进行训练，利用物理启发的力场形式和训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;ByteFF-Pol在预测各种小分子液体和电解质的热力学和输运性质方面表现出色，优于最先进的经典和机器学习力场，具有零样本预测能力。&lt;h4&gt;结论&lt;/h4&gt;这一进展在电解质设计和定制溶剂等应用方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;h4&gt;翻译&lt;/h4&gt;分子动力学(MD)模拟是揭示凝聚相系统结构和动力学原子级洞察力的必要工具。然而，从从头算计算准确预测宏观属性仍然是一个重大挑战，通常受限于计算成本和模拟精度之间的权衡。在此，我们提出了ByteFF-Pol，一种图神经网络(GNN)参数化的极化力场，完全基于高水平的量子力学(QM)数据进行训练。利用物理启发的力场形式和训练策略，ByteFF-Pol在预测各种小分子液体和电解质的热力学和输运性质方面表现出色，优于最先进的(SOTA)经典和机器学习力场。ByteFF-Pol的零样本预测能力弥合了微观QM计算与宏观液体性质之间的差距，使得探索以前难以处理的化学空间成为可能。这一进展在电解质设计和定制溶剂等应用方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics (MD) simulations are essential tools for unravelingatomistic insights into the structure and dynamics of condensed-phase systems.However, the universal and accurate prediction of macroscopic properties fromab initio calculations remains a significant challenge, often hindered by thetrade-off between computational cost and simulation accuracy. Here, we presentByteFF-Pol, a graph neural network (GNN)-parameterized polarizable force field,trained exclusively on high-level quantum mechanics (QM) data. Leveragingphysically-motivated force field forms and training strategies, ByteFF-Polexhibits exceptional performance in predicting thermodynamic and transportproperties for a wide range of small-molecule liquids and electrolytes,outperforming state-of-the-art (SOTA) classical and machine learning forcefields. The zero-shot prediction capability of ByteFF-Pol bridges the gapbetween microscopic QM calculations and macroscopic liquid properties, enablingthe exploration of previously intractable chemical spaces. This advancementholds transformative potential for applications such as electrolyte design andcustom-tailored solvent, representing a pivotal step toward data-drivenmaterials discovery.</description>
      <author>example@mail.com (Tianze Zheng, Xingyuan Xu, Zhi Wang, Zhenze Yang, Yuanheng Wang, Xu Han, Zhenliang Mu, Ziqing Zhang, Siyuan Liu, Sheng Gong, Kuang Yu, Wen Yan)</author>
      <guid isPermaLink="false">2508.08575v3</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>M3-Net: A Cost-Effective Graph-Free MLP-Based Model for Traffic Prediction</title>
      <link>http://arxiv.org/abs/2508.08543v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M3-Net的经济高效的基于无图多层感知器的交通预测模型，该模型结合了时间序列和时空嵌入以及具有专家混合机制的MLP-Mixer架构，在多个真实数据集上显示出优越的预测性能和轻量级部署能力。&lt;h4&gt;背景&lt;/h4&gt;交通预测是智能交通系统发展中的基本但关键的任务。主流方法主要依赖于时空图神经网络和时空注意力机制等。&lt;h4&gt;目的&lt;/h4&gt;解决现有深度学习方法面临的挑战，即它们要么依赖于完整的交通网络结构，要么需要复杂的模型设计来捕捉复杂的时空依赖关系，从而实现深度学习模型在大规模数据集上的高效部署和操作。&lt;h4&gt;方法&lt;/h4&gt;提出一种名为M3-Net的基于无图多层感知器的模型，该模型使用时间序列和时空嵌入进行高效特征处理，并引入了一种具有专家混合机制的新型MLP-Mixer架构。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实数据集上进行的大量实验证明了所提出模型在预测性能和轻量级部署方面的优越性。&lt;h4&gt;结论&lt;/h4&gt;M3-Net模型能够有效解决现有深度学习方法在交通预测中面临的挑战，提供了一种更经济高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;实现准确的交通预测是当前智能交通系统发展中的一个基本但关键的任务。在交通预测方面取得突破的大多数主流方法依赖于时空图神经网络、时空注意力机制等。现有深度学习方法的主要挑战是它们要么依赖于完整的交通网络结构，要么需要复杂的模型设计来捕捉复杂的时空依赖关系。这些限制对于在大规模数据集上高效部署和操作深度学习模型构成了重大挑战。为了解决这些挑战，我们提出了一种经济高效的基于无图多层感知器的模型M3-Net用于交通预测。我们提出的模型不仅使用时间序列和时空嵌入进行高效特征处理，而且首次引入了一种具有专家混合机制的新型MLP-Mixer架构。在多个真实数据集上进行的大量实验证明了所提出模型在预测性能和轻量级部署方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving accurate traffic prediction is a fundamental but crucial task inthe development of current intelligent transportation systems.Most of themainstream methods that have made breakthroughs in traffic prediction rely onspatio-temporal graph neural networks, spatio-temporal attention mechanisms,etc. The main challenges of the existing deep learning approaches are that theyeither depend on a complete traffic network structure or require intricatemodel designs to capture complex spatio-temporal dependencies. Theselimitations pose significant challenges for the efficient deployment andoperation of deep learning models on large-scale datasets. To address thesechallenges, we propose a cost-effective graph-free Multilayer Perceptron (MLP)based model M3-Net for traffic prediction. Our proposed model not only employstime series and spatio-temporal embeddings for efficient feature processing butalso first introduces a novel MLP-Mixer architecture with a mixture of experts(MoE) mechanism. Extensive experiments conducted on multiple real datasetsdemonstrate the superiority of the proposed model in terms of predictionperformance and lightweight deployment.</description>
      <author>example@mail.com (Guangyin Jin, Sicong Lai, Xiaoshuai Hao, Mingtao Zhang, Jinlei Zhang)</author>
      <guid isPermaLink="false">2508.08543v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>On Understanding of the Dynamics of Model Capacity in Continual Learning</title>
      <link>http://arxiv.org/abs/2508.08052v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了持续学习中的稳定-塑性困境，引入了持续学习的有效模型容量(CLEMC)概念，通过差分方程建模神经网络、任务数据和优化过程的相互作用，证明了有效容量本质上是非平稳的，并发现当新任务分布与先前任务不同时，神经网络表示新任务的能力会下降。&lt;h4&gt;背景&lt;/h4&gt;稳定-塑性困境与神经网络容量(表示任务的能力)密切相关，是持续学习中的一个基本挑战。&lt;h4&gt;目的&lt;/h4&gt;引入持续学习的有效模型容量(CLEMC)概念，用于描述稳定-塑性平衡点的动态行为。&lt;h4&gt;方法&lt;/h4&gt;开发差分方程来建模神经网络、任务数据和优化过程之间相互作用的演化过程，并利用CLEMC进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;无论神经网络架构或优化方法如何，当新任务分布与先前任务不同时，神经网络表示新任务的能力都会下降；有效容量和稳定-塑性平衡点本质上是非平稳的。&lt;h4&gt;结论&lt;/h4&gt;通过大量实验支持了理论发现，实验涵盖了从小型前馈网络和卷积网络到中等规模的图神经网络和基于Transformer的大型语言模型等多种架构。&lt;h4&gt;翻译&lt;/h4&gt;稳定-塑性困境与神经网络(NN)的容量(其表示任务的能力)密切相关，是持续学习(CL)中的一个基本挑战。在此背景下，我们引入了持续学习的有效模型容量(CLEMC)，用于描述稳定-塑性平衡点的动态行为。我们开发了一个差分方程来建模神经网络、任务数据和优化过程之间相互作用的演化过程。然后我们利用CLEMC证明，有效容量以及稳定-塑性平衡点本质上是非平稳的。我们表明，无论神经网络架构或优化方法如何，当传入任务分布与先前任务不同时，神经网络表示新任务的能力都会下降。我们进行了大量实验来支持我们的理论发现，涵盖了从小型前馈网络和卷积网络到中等规模的图神经网络和基于Transformer的具有数百万参数的大型语言模型等多种架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The stability-plasticity dilemma, closely related to a neural network's (NN)capacity-its ability to represent tasks-is a fundamental challenge in continuallearning (CL). Within this context, we introduce CL's effective model capacity(CLEMC) that characterizes the dynamic behavior of the stability-plasticitybalance point. We develop a difference equation to model the evolution of theinterplay between the NN, task data, and optimization procedure. We thenleverage CLEMC to demonstrate that the effective capacity-and, by extension,the stability-plasticity balance point is inherently non-stationary. We showthat regardless of the NN architecture or optimization method, a NN's abilityto represent new tasks diminishes when incoming task distributions differ fromprevious ones. We conduct extensive experiments to support our theoreticalfindings, spanning a range of architectures-from small feedforward network andconvolutional networks to medium-sized graph neural networks andtransformer-based large language models with millions of parameters.</description>
      <author>example@mail.com (Supriyo Chakraborty, Krishnan Raghavan)</author>
      <guid isPermaLink="false">2508.08052v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering</title>
      <link>http://arxiv.org/abs/2508.10729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了EgoCross基准测试，用于评估多模态大语言模型在第一人称视频问答中的跨领域泛化能力，突破了现有研究主要局限于日常活动的局限。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在第一人称视频问答领域取得了显著进展，但现有基准测试和研究主要局限于日常活动如烹饪和清洁，而真实世界部署会遇到领域偏移问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的基准测试来评估MLLMs在第一人称视频问答中的跨领域泛化能力，填补现有研究在非日常活动领域的空白。&lt;h4&gt;方法&lt;/h4&gt;构建了EgoCross基准测试，涵盖四个多样且具挑战性的领域（手术、工业、极限运动、动物视角），包含约1000个问答对跨越798个视频片段，涵盖预测、识别、定位和计数四个关键问答任务，并提供开放式和封闭式问答两种格式。&lt;h4&gt;主要发现&lt;/h4&gt;大多数现有的MLLMs，无论是通用型还是第一人称专用型，都难以推广到日常生活以外的领域，突显了当前模型的局限性。&lt;h4&gt;结论&lt;/h4&gt;EgoCross基准测试和相关分析为推进领域自适应、鲁棒的第一人称视频理解提供了基础，数据和代码将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;最近多模态大语言模型的进展显著推动了第一人称视频问答的前沿。然而，现有的基准测试和研究主要局限于烹饪和清洁等日常活动。相比之下，真实世界的部署不可避免地会遇到领域偏移问题，其中目标领域在视觉风格和语义内容上差异显著。为了弥补这一差距，我们引入了EgoCross，这是一个全面的基准测试，旨在评估MLLMs在第一人称视频问答中的跨领域泛化能力。EgoCross涵盖了四个多样且具有挑战性的领域，包括手术、工业、极限运动和动物视角，代表了现实且影响深远的应用场景。它包含跨越798个视频片段的约1000个问答对，涵盖四个关键问答任务：预测、识别、定位和计数。每个问答对提供开放式和封闭式问答两种格式，以支持细粒度评估。大量实验表明，大多数现有的MLLMs，无论是通用型还是第一人称专用型，都难以推广到日常生活以外的领域，突显了当前模型的局限性。此外，我们进行了几项初步研究，例如微调和强化学习，以探索潜在的改进方向。我们希望EgoCross和我们的相关分析能够成为推进领域自适应、鲁棒的第一人称视频理解的基础。数据和代码将在以下地址发布：https://github.com/MyUniverse0726/EgoCross&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Multimodal Large Language Models (MLLMs) havesignificantly pushed the frontier of egocentric video question answering(EgocentricQA). However, existing benchmarks and studies are mainly limited tocommon daily activities such as cooking and cleaning. In contrast, real-worlddeployment inevitably encounters domain shifts, where target domains differsubstantially in both visual style and semantic content. To bridge this gap, weintroduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate thecross-domain generalization of MLLMs in EgocentricQA. EgoCross covers fourdiverse and challenging domains, including surgery, industry, extreme sports,and animal perspective, representing realistic and high-impact applicationscenarios. It comprises approximately 1,000 QA pairs across 798 video clips,spanning four key QA tasks: prediction, recognition, localization, andcounting. Each QA pair provides both OpenQA and CloseQA formats to supportfine-grained evaluation. Extensive experiments show that most existing MLLMs,whether general-purpose or egocentric-specialized, struggle to generalize todomains beyond daily life, highlighting the limitations of current models.Furthermore, we conduct several pilot studies, \eg, fine-tuning andreinforcement learning, to explore potential improvements. We hope EgoCross andour accompanying analysis will serve as a foundation for advancingdomain-adaptive, robust egocentric video understanding. Data and codes will bereleased at:\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}</description>
      <author>example@mail.com (Yanjun Li, Yuqian Fu, Tianwen Qian, Qi'ao Xu, Silong Dai, Danda Pani Paudel, Luc Van Gool, Xiaoling Wang)</author>
      <guid isPermaLink="false">2508.10729v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation</title>
      <link>http://arxiv.org/abs/2508.10635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ChatENV是一个创新的交互式视觉语言模型，能够同时处理卫星图像对和现实世界传感器数据，用于环境变化理解和分析。&lt;h4&gt;背景&lt;/h4&gt;理解航空影像中的环境变化对气候适应力、城市规划和生态系统监测至关重要，但当前视觉语言模型存在忽略环境传感器因果信号、依赖单一来源标题易受风格偏见影响、缺乏交互式场景推理能力等问题。&lt;h4&gt;目的&lt;/h4&gt;开发ChatENV，一个能够联合推理卫星图像对和现实世界传感器数据的交互式视觉语言模型，以克服现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;创建包含177k图像的数据集，形成跨越62个土地利用类别、197个国家的152k个时间对，并具有丰富的传感器元数据；使用GPT-4o和Gemini 2.0实现风格和语义多样性注释；采用低秩自适应(LoRA)适配器对Qwen-2.5-VL进行微调以支持聊天功能。&lt;h4&gt;主要发现&lt;/h4&gt;ChatENV在时间和'what-if'推理方面表现优异(BERT-F1达到0.903)，与最先进的时间模型相比具有竞争力或表现更好，同时支持交互式场景分析。&lt;h4&gt;结论&lt;/h4&gt;ChatENV是一个强大的工具，可用于基于传感器感知的环境监测，为环境变化理解和分析提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;从航空影像理解环境变化对气候适应力、城市规划和生态系统监测至关重要。然而，当前的视觉语言模型忽略了环境传感器的因果信号，依赖单一来源的标题而易受风格偏见影响，且缺乏交互式场景推理能力。我们提出了ChatENV，这是首个能够同时处理卫星图像对和现实世界传感器数据的交互式视觉语言模型。我们的框架：(i)创建了一个包含177k图像的数据集，形成了跨越62个土地利用类别、197个国家的152k个时间对，并具有丰富的传感器元数据(如温度、PM10、CO)；(ii)使用GPT-4o和Gemini 2.0对数据进行注释，以实现风格和语义多样性；(iii)使用高效的低秩自适应(LoRA)适配器对Qwen-2.5-VL进行微调以用于聊天目的。ChatENV在时间和'what-if'推理方面取得了强劲性能(例如，BERT-F1达到0.903)，与最先进的时间模型相比具有竞争力或表现更好，同时支持交互式场景分析。这使ChatENV成为一个强大的工具，用于基于传感器感知的环境监测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding environmental changes from aerial imagery is vital for climateresilience, urban planning, and ecosystem monitoring. Yet, current visionlanguage models (VLMs) overlook causal signals from environmental sensors, relyon single-source captions prone to stylistic bias, and lack interactivescenario-based reasoning. We present ChatENV, the first interactive VLM thatjointly reasons over satellite image pairs and real-world sensor data. Ourframework: (i) creates a 177k-image dataset forming 152k temporal pairs across62 land-use classes in 197 countries with rich sensor metadata (e.g.,temperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 forstylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL usingefficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENVachieves strong performance in temporal and "what-if" reasoning (e.g., BERT-F10.903) and rivals or outperforms state-of-the-art temporal models, whilesupporting interactive scenario-based analysis. This positions ChatENV as apowerful tool for grounded, sensor-aware environmental monitoring.</description>
      <author>example@mail.com (Hosam Elgendy, Ahmed Sharshar, Ahmed Aboeitta, Mohsen Guizani)</author>
      <guid isPermaLink="false">2508.10635v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Trajectory-aware Shifted State Space Models for Online Video Super-Resolution</title>
      <link>http://arxiv.org/abs/2508.10453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TS-Mamba的在线视频超分辨率方法，结合轨迹感知和低复杂度Mamba模型实现高效空间-时间信息聚合，在三个VSR测试数据集上取得最先进性能，同时降低22.7%计算复杂度。&lt;h4&gt;背景&lt;/h4&gt;在线视频超分辨率(VSR)是许多现实世界视频处理应用的重要技术，旨在基于时间上之前的帧恢复当前高分辨率视频帧。现有方法主要只使用一个相邻前一帧实现时间对齐，限制了视频的长程时间建模。状态空间模型(SSMs)因其线性的计算复杂度和全局感受野被提出，显著提高了计算效率和性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型在线VSR方法，结合长程轨迹建模和低复杂度Mamba模型，实现高效的空间-时间信息聚合。&lt;h4&gt;方法&lt;/h4&gt;TS-Mamba首先在视频中构建轨迹，从前面的帧中选择最相似的token；然后采用由提出的移位SSMs块组成的轨迹感知移位Mamba聚合(TSMA)模块聚合选中的token；移位SSMs块基于Hilbert扫描和相应移位操作设计，用于补偿扫描损失并增强Mamba的空间连续性；同时提出轨迹感知损失函数监督轨迹生成，确保训练时token选择的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在三个广泛使用的VSR测试数据集上的大量实验表明，与六个在线VSR基准模型相比，TS-Mamba在大多数情况下实现了最先进的性能，并且计算复杂度降低了22.7%(以MACs计)。&lt;h4&gt;结论&lt;/h4&gt;TS-Mamba方法通过结合轨迹感知和移位SSMs，有效解决了在线VSR中的长程时间建模问题，同时保持较低计算复杂度，为视频超分辨率领域提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在线视频超分辨率(VSR)是许多现实世界视频处理应用中的重要技术，旨在基于时间上之前的帧来恢复当前的高分辨率视频帧。大多数现有的在线VSR方法仅使用一个相邻的前一帧来实现时间对齐，这限制了视频的长程时间建模。最近，状态空间模型(SSMs)被提出，具有线性的计算复杂度和全局感受野，显著提高了计算效率和性能。在此背景下，本文提出了一种基于轨迹感知移位SSMs(TS-Mamba)的新型在线VSR方法，利用长程轨迹建模和低复杂度Mamba来实现高效的空间-时间信息聚合。具体而言，TS-Mamba首先在视频中构建轨迹，从前面的帧中选择最相似的token。然后，采用由提出的移位SSMs块组成的轨迹感知移位Mamba聚合(TSMA)模块来聚合选中的token。这些移位SSMs块基于Hilbert扫描和相应的移位操作设计，用于补偿扫描损失并增强Mamba的空间连续性。此外，我们还提出了一个轨迹感知损失函数来监督轨迹生成，确保在训练模型时token选择的准确性。在三个广泛使用的VSR测试数据集上的大量实验表明，与六个在线VSR基准模型相比，我们的TS-Mamba在大多数情况下实现了最先进的性能，并且计算复杂度降低了22.7%(以MACs计)。TS-Mamba的源代码将在https://github.com上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online video super-resolution (VSR) is an important technique for manyreal-world video processing applications, which aims to restore the currenthigh-resolution video frame based on temporally previous frames. Most of theexisting online VSR methods solely employ one neighboring previous frame toachieve temporal alignment, which limits long-range temporal modeling ofvideos. Recently, state space models (SSMs) have been proposed with linearcomputational complexity and a global receptive field, which significantlyimprove computational efficiency and performance. In this context, this paperpresents a novel online VSR method based on Trajectory-aware Shifted SSMs(TS-Mamba), leveraging both long-term trajectory modeling and low-complexityMamba to achieve efficient spatio-temporal information aggregation.Specifically, TS-Mamba first constructs the trajectories within a video toselect the most similar tokens from the previous frames. Then, aTrajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposedshifted SSMs blocks is employed to aggregate the selected tokens. The shiftedSSMs blocks are designed based on Hilbert scannings and corresponding shiftoperations to compensate for scanning losses and strengthen the spatialcontinuity of Mamba. Additionally, we propose a trajectory-aware loss functionto supervise the trajectory generation, ensuring the accuracy of tokenselection when training our model. Extensive experiments on three widely usedVSR test datasets demonstrate that compared with six online VSR benchmarkmodels, our TS-Mamba achieves state-of-the-art performance in most cases andover 22.7\% complexity reduction (in MACs). The source code for TS-Mamba willbe available at https://github.com.</description>
      <author>example@mail.com (Qiang Zhu, Xiandong Meng, Yuxian Jiang, Fan Zhang, David Bull, Shuyuan Zhu, Bing Zeng)</author>
      <guid isPermaLink="false">2508.10453v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Learning for Generalizable Surgical Video Understanding</title>
      <link>http://arxiv.org/abs/2508.10215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究解决了手术视频分析中的关键挑战，通过开发创新的半监督方法和基准测试模型，减少了对标记数据的依赖，并创建了大型公共数据集，促进了该领域的发展和临床应用。&lt;h4&gt;背景&lt;/h4&gt;外科视频分析技术的进步正在将手术室转变为智能、数据驱动的环境。计算机辅助系统支持完整的手术工作流程，从术前规划到术中指导和术后评估。然而，开发稳健且可推广的手术视频理解模型面临三大挑战：标注稀缺、时空复杂性以及跨手术和机构的领域差异。&lt;h4&gt;目的&lt;/h4&gt;弥合基于深度学习的手术视频分析研究与实际临床部署之间的差距，开发稳健、数据高效且临床可扩展的解决方案。&lt;h4&gt;方法&lt;/h4&gt;通过对先进神经网络架构进行基准测试确定最佳设计；提出新架构和集成高级模块提高性能；开发半监督框架减少对标记数据的依赖；引入DIST、SemiVT-Surge和ENCORE等半监督框架；发布GynSurg和Cataract-1K两个多任务数据集。&lt;h4&gt;主要发现&lt;/h4&gt;新的半监督框架通过利用少量标记数据和动态伪标记增强模型训练，在具有挑战性的手术数据集上取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;这项工作为手术视频分析提供了稳健、数据高效且临床可扩展的解决方案，为可推广的AI系统奠定了基础，这些系统可以显著影响手术护理和培训。&lt;h4&gt;翻译&lt;/h4&gt;外科视频分析技术的进步正在将手术室转变为智能、数据驱动的环境。计算机辅助系统支持完整的手术工作流程，从术前规划到术中指导和术后评估。然而，由于标注稀缺、时空复杂性以及跨手术和机构的领域差异，开发稳健且可推广的手术视频理解模型仍然具有挑战性。这项博士研究旨在弥合基于深度学习的手术视频分析研究与实际临床部署之间的差距。为解决识别手术阶段、动作和事件这一核心挑战，我对最先进的神经网络架构进行了基准测试，以确定每个任务的最有效设计。我通过提出新的架构和集成高级模块进一步提高了性能。鉴于专家标注的高成本和跨手术视频源的领域差异，我专注于减少对标记数据的依赖。我们开发了半监督框架，通过利用大量未标记的手术视频来提高模型在各项任务中的性能。我们引入了新的半监督框架，包括DIST、SemiVT-Surge和ENCORE，这些框架通过利用少量标记数据和通过动态伪标记增强模型训练，在具有挑战性的手术数据集上取得了最先进的结果。为了支持可复现性和推进该领域，我们发布了两个多任务数据集：GynSurg，最大的妇科腹腔镜数据集，以及Cataract-1K，最大的白内障手术视频数据集。这项工作共同为手术视频分析提供了稳健、数据高效且临床可扩展的解决方案，为可推广的AI系统奠定了基础，这些系统可以显著影响手术护理和培训。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in surgical video analysis are transforming operating rooms intointelligent, data-driven environments. Computer-assisted systems support fullsurgical workflow, from preoperative planning to intraoperative guidance andpostoperative assessment. However, developing robust and generalizable modelsfor surgical video understanding remains challenging due to (I) annotationscarcity, (II) spatiotemporal complexity, and (III) domain gap acrossprocedures and institutions. This doctoral research aims to bridge the gapbetween deep learning-based surgical video analysis in research and itsreal-world clinical deployment. To address the core challenge of recognizingsurgical phases, actions, and events, critical for analysis, I benchmarkedstate-of-the-art neural network architectures to identify the most effectivedesigns for each task. I further improved performance by proposing novelarchitectures and integrating advanced modules. Given the high cost of expertannotations and the domain gap across surgical video sources, I focused onreducing reliance on labeled data. We developed semi-supervised frameworks thatimprove model performance across tasks by leveraging large amounts of unlabeledsurgical video. We introduced novel semi-supervised frameworks, including DIST,SemiVT-Surge, and ENCORE, that achieved state-of-the-art results on challengingsurgical datasets by leveraging minimal labeled data and enhancing modeltraining through dynamic pseudo-labeling. To support reproducibility andadvance the field, we released two multi-task datasets: GynSurg, the largestgynecologic laparoscopy dataset, and Cataract-1K, the largest cataract surgeryvideo dataset. Together, this work contributes to robust, data-efficient, andclinically scalable solutions for surgical video analysis, laying thefoundation for generalizable AI systems that can meaningfully impact surgicalcare and training.</description>
      <author>example@mail.com (Sahar Nasirihaghighi)</author>
      <guid isPermaLink="false">2508.10215v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SurgPub-Video: A Comprehensive Surgical Video Dataset for Enhanced Surgical Intelligence in Vision-Language Model</title>
      <link>http://arxiv.org/abs/2508.10054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SurgPub-Video数据集、SurgLLaVA-Video模型和外科视频级VQA基准三项贡献，解决了现有VLMs在外科场景分析中受限于帧级数据集和缺乏高质量外科视频数据的问题。实验证明SurgLLaVA-Video性能显著优于其他模型。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在外科场景分析中显示出巨大潜力，但现有模型受限于帧级数据集，缺乏带有外科程序知识的高质量视频数据。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLMs在外科场景分析中的局限性，提出三项贡献以促进外科视频理解的发展。&lt;h4&gt;方法&lt;/h4&gt;创建了包含11个专业、超过3000个外科视频和2500万标注帧的SurgPub-Video数据集；开发了基于TinyLLaVA-Video架构构建的SurgLLaVA-Video模型，支持视频级和帧级输入；建立了涵盖11个外科专业的视频级VQA基准。&lt;h4&gt;主要发现&lt;/h4&gt;在提出的基准和三个额外的外科下游任务（动作识别、技能评估和三元组识别）上进行的大量实验表明，仅有三十亿参数的SurgLLaVA-Video显著优于通用外科专用VLM。&lt;h4&gt;结论&lt;/h4&gt;数据集、模型和基准将被发布，以促进外科视频理解的进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在外科场景分析中显示出巨大潜力，但现有模型受限于帧级数据集，缺乏带有外科程序知识的高质量视频数据。为解决这些挑战，我们做出了以下贡献：(i) SurgPub-Video，一个包含11个专业、超过3000个外科视频和2500万标注帧的综合数据集，数据来源于同行评审的临床期刊；(ii) SurgLLaVA-Video，一个专门用于外科视频理解的VLM，基于TinyLLaVA-Video架构构建，支持视频级和帧级输入；(iii) 一个外科视频级视觉问答基准，涵盖11个不同的外科专业，如血管科、心脏病学和胸外科。在提出的基准和三个额外的外科下游任务（动作识别、技能评估和三元组识别）上进行的大量实验表明，仅有三十亿参数的SurgLLaVA-Video显著优于通用外科专用VLM。数据集、模型和基准将被发布，以促进外科视频理解的进一步发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have shown significant potential in surgicalscene analysis, yet existing models are limited by frame-level datasets andlack high-quality video data with procedural surgical knowledge. To addressthese challenges, we make the following contributions: (i) SurgPub-Video, acomprehensive dataset of over 3,000 surgical videos and 25 million annotatedframes across 11 specialties, sourced from peer-reviewed clinical journals,(ii) SurgLLaVA-Video, a specialized VLM for surgical video understanding, builtupon the TinyLLaVA-Video architecture that supports both video-level andframe-level inputs, and (iii) a video-level surgical Visual Question Answering(VQA) benchmark, covering diverse 11 surgical specialities, such as vascular,cardiology, and thoracic. Extensive experiments, conducted on the proposedbenchmark and three additional surgical downstream tasks (action recognition,skill assessment, and triplet recognition), show that SurgLLaVA-Videosignificantly outperforms both general-purpose and surgical-specific VLMs withonly three billion parameters. The dataset, model, and benchmark will bereleased to enable further advancements in surgical video understanding.</description>
      <author>example@mail.com (Yaoqian Li, Xikai Yang, Dunyuan Xu, Yang Yu, Litao Zhao, Xiaowei Hu, Jinpeng Li, Pheng-Ann Heng)</author>
      <guid isPermaLink="false">2508.10054v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Re:Verse -- Can Your VLM Read a Manga?</title>
      <link>http://arxiv.org/abs/2508.08508v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV (AISTORY Workshop) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究揭示了当前视觉语言模型在处理连续视觉叙事时，在表面识别与深层叙事推理之间存在关键差距。研究提出了一种新的评估框架，并应用于Re:Zero漫画，发现当前模型缺乏真正的故事级智能，特别是在处理非线性叙事、角色一致性和跨序列因果推理方面存在困难。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在处理连续视觉叙事时表现出局限性，特别是在时间因果性和跨面板连贯性方面，这些是连贯故事理解的核心要求。现有模型虽然在单幅图像解释方面表现出色，但在叙事理解方面存在系统性失败。&lt;h4&gt;目的&lt;/h4&gt;研究旨在系统地评估视觉语言模型在长篇叙事理解方面的能力，揭示其在时间因果性和跨面板连贯性方面的局限性，并提出一个新的评估框架来系统地描述这些局限性。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一个新的评估框架，结合了细粒度多模态注释、跨模态嵌入分析和检索增强评估。具体方法包括：(i)严格的注释协议，通过对齐的轻小说文本将视觉元素与叙事结构联系起来；(ii)多种推理范式的综合评估，包括直接推理和检索增强生成；(iii)跨模态相似性分析。研究应用了Re:Zero漫画的11个章节，共308个注释面板，通过三个核心评估轴：生成式叙事、上下文化对话基础和时间推理。&lt;h4&gt;主要发现&lt;/h4&gt;当前视觉语言模型缺乏真正的故事级智能，特别是在处理非线性叙事、角色一致性和跨序列因果推理方面存在困难。研究还揭示了当前VLMs联合表示中的基本错位。&lt;h4&gt;结论&lt;/h4&gt;这项工作为评估叙事智能建立了基础和实践方法，同时提供了关于多模态模型在离散视觉叙事中超越基本识别的深层序列理解能力的实用见解。&lt;h4&gt;翻译&lt;/h4&gt;当前视觉语言模型在处理连续视觉叙事时，在表面识别与深层叙事推理之间存在关键差距。通过对漫画叙事理解的全面调查，我们发现，虽然最近的大型多模态模型在单幅图像解释方面表现出色，但它们在时间因果性和跨面板连贯性方面存在系统性失败，而这些是连贯故事理解的核心要求。我们引入了一种新的评估框架，结合了细粒度多模态注释、跨模态嵌入分析和检索增强评估，以系统地描述这些局限性。我们的方法包括(i)通过通过对齐的轻小说文本将视觉元素与叙事结构联系起来的严格注释协议，(ii)多种推理范式的综合评估，包括直接推理和检索增强生成，以及(iii)跨模态相似性分析，揭示了当前VLMs联合表示中的基本错位。将此框架应用于Re:Zero漫画的11个章节，共308个注释面板，我们进行了第一个关于VLMs长篇叙事理解的系统性研究，通过三个核心评估轴：生成式叙事、上下文化对话基础和时间推理。我们的研究结果表明，当前模型缺乏真正的故事级智能，特别是在处理非线性叙事、角色一致性和跨序列因果推理方面存在困难。这项工作为评估叙事智能建立了基础和实践方法，同时提供了关于多模态模型在离散视觉叙事中超越基本识别的深层序列理解能力的实用见解。项目页面：https://re-verse.vercel.app&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Vision Language Models (VLMs) demonstrate a critical gap betweensurface-level recognition and deep narrative reasoning when processingsequential visual storytelling. Through a comprehensive investigation of manganarrative understanding, we reveal that while recent large multimodal modelsexcel at individual panel interpretation, they systematically fail at temporalcausality and cross-panel cohesion, core requirements for coherent storycomprehension. We introduce a novel evaluation framework that combinesfine-grained multimodal annotation, cross-modal embedding analysis, andretrieval-augmented assessment to systematically characterize theselimitations.  Our methodology includes (i) a rigorous annotation protocol linking visualelements to narrative structure through aligned light novel text, (ii)comprehensive evaluation across multiple reasoning paradigms, including directinference and retrieval-augmented generation, and (iii) cross-modal similarityanalysis revealing fundamental misalignments in current VLMs' jointrepresentations. Applying this framework to Re:Zero manga across 11 chapterswith 308 annotated panels, we conduct the first systematic study of long-formnarrative understanding in VLMs through three core evaluation axes: generativestorytelling, contextual dialogue grounding, and temporal reasoning. Ourfindings demonstrate that current models lack genuine story-level intelligence,struggling particularly with non-linear narratives, character consistency, andcausal inference across extended sequences. This work establishes both thefoundation and practical methodology for evaluating narrative intelligence,while providing actionable insights into the capability of deep sequentialunderstanding of Discrete Visual Narratives beyond basic recognition inMultimodal Models.  Project Page: https://re-verse.vercel.app</description>
      <author>example@mail.com (Aaditya Baranwal, Madhav Kataria, Naitik Agrawal, Yogesh S Rawat, Shruti Vyas)</author>
      <guid isPermaLink="false">2508.08508v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning</title>
      <link>http://arxiv.org/abs/2508.04549v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ACMMM2025 (Dataset track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对海洋视频理解面临的挑战，提出了一种两阶段的海洋物体导向的视频字幕处理流程，并引入了一个全面的视频理解基准。&lt;h4&gt;背景&lt;/h4&gt;海洋视频理解面临多重挑战，包括海洋物体和环境的动态变化、相机运动以及水下场景的复杂性。现有视频字幕数据集通常专注于通用或以人为中心的领域，难以推广到海洋环境的复杂性，也无法深入了解海洋生物。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频字幕数据集在海洋环境中的局限性，提高海洋视频的理解和分析能力，以及海洋视频生成能力。&lt;h4&gt;方法&lt;/h4&gt;提出一个两阶段的海洋物体导向的视频字幕处理流程，引入一个利用视频、文本和分割掩码三元组的视频理解基准，并采用视频分割技术检测场景变化中的显著物体转换。&lt;h4&gt;主要发现&lt;/h4&gt;视频分割技术对于检测场景变化中的显著物体转换非常有效，能够显著丰富字幕内容的语义。所提出的方法和基准能够改善海洋视频的理解、分析和生成。&lt;h4&gt;结论&lt;/h4&gt;通过提出的两阶段流程和视频理解基准，有效解决了海洋视频理解的挑战，提高了字幕质量和语义丰富度。相关数据集和代码已公开发布。&lt;h4&gt;翻译&lt;/h4&gt;海洋视频由于其物体和环境的动态性、相机运动以及水下场景的复杂性，给视频理解带来了重大挑战。现有的视频字幕数据集通常专注于通用或以人为中心的领域，往往难以推广到海洋环境的复杂性，也无法深入了解海洋生物。为解决这些局限性，我们提出了一个两阶段的海洋物体导向的视频字幕处理流程。我们引入了一个全面的视频理解基准，利用视频、文本和分割掩码的三元组来促进视觉定位和字幕生成，从而改善海洋视频的理解和分析，以及海洋视频生成。此外，我们强调了视频分割在检测场景变化中显著物体转换的有效性，这大大丰富了字幕内容的语义。我们的数据集和代码已在https://msc.hkustvgd.com发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758198&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Marine videos present significant challenges for video understanding due tothe dynamics of marine objects and the surrounding environment, camera motion,and the complexity of underwater scenes. Existing video captioning datasets,typically focused on generic or human-centric domains, often fail to generalizeto the complexities of the marine environment and gain insights about marinelife. To address these limitations, we propose a two-stage marineobject-oriented video captioning pipeline. We introduce a comprehensive videounderstanding benchmark that leverages the triplets of video, text, andsegmentation masks to facilitate visual grounding and captioning, leading toimproved marine video understanding and analysis, and marine video generation.Additionally, we highlight the effectiveness of video splitting in order todetect salient object transitions in scene changes, which significantly enrichthe semantics of captioning content. Our dataset and code have been released athttps://msc.hkustvgd.com.</description>
      <author>example@mail.com (Quang-Trung Truong, Yuk-Kwan Wong, Vo Hoang Kim Tuyen Dang, Rinaldi Gotama, Duc Thanh Nguyen, Sai-Kit Yeung)</author>
      <guid isPermaLink="false">2508.04549v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.10838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出BaCon-Stereo，一个简单有效的对比学习框架，用于解决自监督立体匹配中遮挡区域的问题。通过多基线输入的教师-学生范式和遮挡感知注意力图，该方法显著提升了遮挡和非遮挡区域的预测性能，并在多个基准测试上超越现有方法。&lt;h4&gt;背景&lt;/h4&gt;当前的自监督立体匹配依赖于光度一致性假设，但在遮挡区域由于对应关系不明确而失效，导致预测不准确。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在遮挡和非遮挡区域都有效工作的自监督立体网络训练框架，提高立体匹配的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;采用多基线输入的教师-学生范式，教师和学生的立体对共享相同参考视图但目标视图不同。利用几何关系，让学生从教师的预测中学习，并通过重新缩放匹配基线进行监督。引入遮挡感知注意力图指导遮挡完成学习，并合成多基线数据集BaCon-20k支持训练。&lt;h4&gt;主要发现&lt;/h4&gt;BaCon-Stereo有效改善了遮挡和非遮挡区域的预测性能，实现了强大的泛化能力和鲁棒性，在多个基准测试上表现优异。&lt;h4&gt;结论&lt;/h4&gt;BaCon-Stereo在KITTI 2015和2012基准测试上超越了最先进的自监督方法，证明了其在解决立体匹配遮挡问题上的有效性。&lt;h4&gt;翻译&lt;/h4&gt;当前的自监督立体匹配依赖于光度一致性假设，由于对应关系不明确，在遮挡区域会失效。为解决这个问题，我们提出了BaCon-Stereo，一个简单有效的对比学习框架，用于在遮挡和非遮挡区域进行自监督立体网络训练。我们采用多基线输入的教师-学生范式，其中输入教师和学生的立体对共享相同的参考视图，但在目标视图上不同。从几何上看，学生在目标视图中被遮挡的区域在教师视图中通常是可见的，使教师更容易在这些区域进行预测。教师预测被重新缩放以匹配学生的基线，然后用于监督学生。我们还引入了一个遮挡感知注意力图，以更好地指导学生学习遮挡完成。为了支持训练，我们合成了一个多基线数据集BaCon-20k。大量实验表明，BaCon-Stereo改善了遮挡和非遮挡区域的预测，实现了强大的泛化能力和鲁棒性，并且在KITTI 2015和2012基准测试上都优于最先进的自监督方法。我们的代码和数据集将在论文接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自监督立体匹配中遮挡区域的预测不准确问题。这个问题很重要，因为立体匹配在自动驾驶、增强现实和机器人等领域有广泛应用，而遮挡区域在这些场景中普遍存在。当前自监督方法依赖光度一致性假设，这在遮挡区域会失效，导致网络错误地复制相邻非遮挡区域的视差值，影响整体系统的可靠性和精度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有自监督方法在遮挡区域表现不佳的原因，认为需要额外的目标线索来获得遮挡区域的可靠伪真值。他们借鉴了对比学习中的教师-学生范式(特别是MoCo和BYOL中的动量教师思想)，并基于现有的自监督立体匹配方法(如SsSnet、OASM)进行改进。通过使用多基线配置，教师网络可以在学生被遮挡的区域提供可靠信息，因为那些区域在教师网络中可能是可见的。此外，作者还使用了CARLA模拟器来合成多基线数据，这是计算机视觉中常用的数据合成方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多基线对比学习框架，通过教师-学生范式提供遮挡区域的可靠监督。具体实现流程：1) 使用CARLA模拟器合成多基线立体图像数据集BaCon-20k；2) 设计教师和学生网络，共享参考图像但使用不同目标图像；3) 训练时，将教师的输出视差重新缩放以匹配学生的基线，作为伪真值监督学生；4) 设计遮挡感知注意力图，为不同区域分配不同监督权重；5) 计算对比损失、光度损失和平滑损失的加权和；6) 学生网络通过反向传播更新，教师网络通过指数移动平均更新参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多基线对比学习框架，使用教师-学生范式解决遮挡区域问题；2) 遮挡感知注意力图，强调对教师友好但对学生具有挑战性的区域；3) 合成多基线数据集BaCon-20k，支持多基线对比学习；4) 动量教师机制，提供更稳定的监督信号。相比之前工作，不同之处在于：现有方法如SsSnet、OASM主要依赖光度一致性假设，在遮挡区域会失效；而BaCon-Stereo通过多基线配置提供遮挡区域的可靠监督，而非简单排除；与DualNet相比，使用不同目标视图而非相同视图，提供更有效遮挡监督；与NerfStereo相比，不依赖额外第三视图，设计更简洁高效。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，我会怎么说？:&lt;/strong&gt; BaCon-Stereo通过多基线对比学习和遮挡感知监督机制，有效解决了自监督立体匹配中遮挡区域的预测问题，显著提高了在遮挡和非遮挡区域的预测精度，并在多种真实场景和天气条件下表现出强大的泛化能力和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current self-supervised stereo matching relies on the photometric consistencyassumption, which breaks down in occluded regions due to ill-posedcorrespondences. To address this issue, we propose BaCon-Stereo, a simple yeteffective contrastive learning framework for self-supervised stereo networktraining in both non-occluded and occluded regions. We adopt a teacher-studentparadigm with multi-baseline inputs, in which the stereo pairs fed into theteacher and student share the same reference view but differ in target views.Geometrically, regions occluded in the student's target view are often visiblein the teacher's, making it easier for the teacher to predict in these regions.The teacher's prediction is rescaled to match the student's baseline and thenused to supervise the student. We also introduce an occlusion-aware attentionmap to better guide the student in learning occlusion completion. To supporttraining, we synthesize a multi-baseline dataset BaCon-20k. Extensiveexperiments demonstrate that BaCon-Stereo improves prediction in both occludedand non-occluded regions, achieves strong generalization and robustness, andoutperforms state-of-the-art self-supervised methods on both KITTI 2015 and2012 benchmarks. Our code and dataset will be released upon paper acceptance.</description>
      <author>example@mail.com (Peng Xu, Zhiyu Xiang, Jingyun Fu, Tianyu Pu, Kai Wang, Chaojie Ji, Tingming Bai, Eryun Liu)</author>
      <guid isPermaLink="false">2508.10838v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive ECOC: Learning Output Codes for Adversarial Defense</title>
      <link>http://arxiv.org/abs/2508.10491v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于对比学习的自动化码本学习方法，用于改进错误纠正输出码技术，提高多类分类的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;虽然one-hot编码常用于多类分类，但它并不总是最有效的编码机制。传统错误纠正输出码方法依赖于手动设计或随机生成的码本，这些方法劳动密集且可能产生次优的、数据集无关的结果。&lt;h4&gt;目的&lt;/h4&gt;引入三种基于对比学习的自动化码本学习模型，使码本能够直接且自适应地从数据中学习，提高多类分类的性能。&lt;h4&gt;方法&lt;/h4&gt;提出三种基于对比学习的自动化码本学习模型，这些模型允许码本直接且自适应地从数据中学习，无需人工干预。&lt;h4&gt;主要发现&lt;/h4&gt;在四个数据集上，所提出的模型比两种基线方法显示出对对抗攻击更强的鲁棒性，表明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;自动化码本学习方法比传统方法更有效，能够更好地适应特定数据集，提高多类分类的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;虽然one-hot编码常用于多类分类，但它并不总是最有效的编码机制。错误纠正输出码通过将每个类映射到用作标签的唯一码字来解决多类分类问题。传统ECOC方法依赖于手动设计或随机生成的码本，这些方法劳动密集且可能产生次优的、数据集无关的结果。本文介绍了三种基于对比学习的自动化码本学习模型，使码本能够直接且自适应地从数据中学习。在四个数据集上，我们提出的模型与两种基线方法相比，表现出对对抗攻击更强的鲁棒性。源代码可在https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although one-hot encoding is commonly used for multiclass classification, itis not always the most effective encoding mechanism. Error Correcting OutputCodes (ECOC) address multiclass classification by mapping each class to aunique codeword used as a label. Traditional ECOC methods rely on manuallydesigned or randomly generated codebooks, which are labor-intensive and mayyield suboptimal, dataset-agnostic results. This paper introduces three modelsfor automated codebook learning based on contrastive learning, allowingcodebooks to be learned directly and adaptively from data. Across fourdatasets, our proposed models demonstrate superior robustness to adversarialattacks compared to two baselines. The source is available athttps://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.</description>
      <author>example@mail.com (Che-Yu Chou, Hung-Hsuan Chen)</author>
      <guid isPermaLink="false">2508.10491v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation</title>
      <link>http://arxiv.org/abs/2508.10432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CRISP（对比残差注入和语义提示）方法，用于解决持续视频实例分割中的实例级、类别级和任务级混淆问题，同时保持模型的可塑性和稳定性。&lt;h4&gt;背景&lt;/h4&gt;持续视频实例分割需要模型同时具备吸收新类别的可塑性和保留已学习类别的稳定性，同时保持帧间的时间一致性，这导致了实例级、类别级和任务级的混淆问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来解决持续视频实例分割中的混淆问题，避免灾难性遗忘，并提高分割和分类性能。&lt;h4&gt;方法&lt;/h4&gt;CRISP方法包含三个主要部分：1) 实例级学习：建模实例跟踪并构建实例相关损失；2) 类别级学习：构建自适应残差语义提示学习框架，使用可学习的语义残差提示池和调整性查询-提示匹配机制；3) 任务级学习：引入增量提示的初始化策略，确保任务间在查询空间内的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;在YouTube-VIS-2019和YouTube-VIS-2021数据集上的实验表明，CRISP显著优于现有的持续分割方法，有效避免了灾难性遗忘，并提高了分割和分类性能。&lt;h4&gt;结论&lt;/h4&gt;CRISP方法通过针对实例级、类别级和任务级问题的专门设计，成功解决了持续视频实例分割中的关键挑战，为长期视频实例分割任务提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;持续视频实例分割需要同时具备吸收新类别的可塑性和保留已学习类别的稳定性，同时保持帧间的时间一致性。在这项工作中，我们引入了对比残差注入和语义提示(CRISP)，这是一种专门用于解决持续视频实例分割中实例级、类别级和任务级混淆的早期尝试。对于实例级学习，我们建模实例跟踪并构建实例相关损失，强调与先验查询空间的相关性，同时加强当前任务查询的特异性。对于类别级学习，我们构建了一个自适应残差语义提示(ARSP)学习框架，该框架由类别文本生成可学习的语义残差提示池，并使用调整性查询-提示匹配机制建立当前任务查询与语义残差提示之间的映射关系。同时，引入基于对比学习的语义一致性损失，以在增量训练期间保持对象查询和残差提示之间的语义一致性。对于任务级学习，为确保查询空间内任务间的相关性，我们引入了一种简洁而强大的增量提示初始化策略。在YouTube-VIS-2019和YouTube-VIS-2021数据集上的大量实验表明，CRISP在长期持续视频实例分割任务中显著优于现有的持续分割方法，避免了灾难性遗忘，并有效提高了分割和分类性能。代码可在https://github.com/01upup10/CRISP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual video instance segmentation demands both the plasticity to absorbnew object categories and the stability to retain previously learned ones, allwhile preserving temporal consistency across frames. In this work, we introduceContrastive Residual Injection and Semantic Prompting (CRISP), an earlierattempt tailored to address the instance-wise, category-wise, and task-wiseconfusion in continual video instance segmentation. For instance-wise learning,we model instance tracking and construct instance correlation loss, whichemphasizes the correlation with the prior query space while strengthening thespecificity of the current task query. For category-wise learning, we build anadaptive residual semantic prompt (ARSP) learning framework, which constructs alearnable semantic residual prompt pool generated by category text and uses anadjustive query-prompt matching mechanism to build a mapping relationshipbetween the query of the current task and the semantic residual prompt.Meanwhile, a semantic consistency loss based on the contrastive learning isintroduced to maintain semantic coherence between object queries and residualprompts during incremental training. For task-wise learning, to ensure thecorrelation at the inter-task level within the query space, we introduce aconcise yet powerful initialization strategy for incremental prompts. Extensiveexperiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate thatCRISP significantly outperforms existing continual segmentation methods in thelong-term continual video instance segmentation task, avoiding catastrophicforgetting and effectively improving segmentation and classificationperformance. The code is available at https://github.com/01upup10/CRISP.</description>
      <author>example@mail.com (Baichen Liu, Qi Lyu, Xudong Wang, Jiahua Dong, Lianqing Liu, Zhi Han)</author>
      <guid isPermaLink="false">2508.10432v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance</title>
      <link>http://arxiv.org/abs/2508.10280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高保真图像生成方法，通过整合文本-图像对比约束和结构引导机制，解决了现有文本驱动图像生成方法在语义对齐准确性和结构一致性方面的性能瓶颈。&lt;h4&gt;背景&lt;/h4&gt;现有文本驱动图像生成方法存在性能瓶颈，特别是在语义对齐准确性和结构一致性方面，文本描述与生成图像之间的语义匹配不够精确，且生成图像的结构完整性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种高保真图像生成方法，提高文本驱动图像生成的语义对齐准确性和结构一致性，同时不增加计算复杂度，生成语义清晰且结构完整的图像。&lt;h4&gt;方法&lt;/h4&gt;1. 引入对比学习模块建立强跨模态对齐约束；2. 使用语义布局图或边缘草图等结构先验指导生成器进行空间级别的结构建模；3. 联合优化对比损失、结构一致性损失和语义保持损失；4. 采用多目标监督机制提高生成内容的语义一致性和可控性。&lt;h4&gt;主要发现&lt;/h4&gt;在COCO-2014数据集上的系统实验表明，所提出方法在CLIP Score、FID和SSIM等指标上表现优越；敏感性分析显示该方法对嵌入维度、文本长度和结构引导强度具有良好的适应性；该方法有效弥合了语义对齐和结构保真度之间的差距而不增加计算复杂度。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了生成语义清晰且结构完整图像的强大能力，为联合文本-图像建模和图像生成提供了可行的技术路径。&lt;h4&gt;翻译&lt;/h4&gt;本文针对现有文本驱动图像生成方法在语义对齐准确性和结构一致性方面的性能瓶颈，提出了一种高保真图像生成方法，通过整合文本-图像对比约束与结构引导机制。该方法引入了对比学习模块，建立了强跨模态对齐约束，以提高文本和图像之间的语义匹配。同时，使用语义布局图或边缘草图等结构先验来指导生成器进行空间级别的结构建模，从而增强生成图像的布局完整性和细节保真度。在整体框架中，模型联合优化对比损失、结构一致性损失和语义保持损失，采用多目标监督机制提高生成内容的语义一致性和可控性。在COCO-2014数据集上进行了系统实验，对嵌入维度、文本长度和结构引导强度进行了敏感性分析。定量指标证实了所提出方法在CLIP Score、FID和SSIM方面的优越性能。结果表明，该方法有效弥合了语义对齐和结构保真度之间的差距，同时不会增加计算复杂度。它展示了生成语义清晰且结构完整图像的强大能力，为联合文本-图像建模和图像生成提供了可行的技术路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the performance bottlenecks of existing text-drivenimage generation methods in terms of semantic alignment accuracy and structuralconsistency. A high-fidelity image generation method is proposed by integratingtext-image contrastive constraints with structural guidance mechanisms. Theapproach introduces a contrastive learning module that builds strongcross-modal alignment constraints to improve semantic matching between text andimage. At the same time, structural priors such as semantic layout maps or edgesketches are used to guide the generator in spatial-level structural modeling.This enhances the layout completeness and detail fidelity of the generatedimages. Within the overall framework, the model jointly optimizes contrastiveloss, structural consistency loss, and semantic preservation loss. Amulti-objective supervision mechanism is adopted to improve the semanticconsistency and controllability of the generated content. Systematicexperiments are conducted on the COCO-2014 dataset. Sensitivity analyses areperformed on embedding dimensions, text length, and structural guidancestrength. Quantitative metrics confirm the superior performance of the proposedmethod in terms of CLIP Score, FID, and SSIM. The results show that the methodeffectively bridges the gap between semantic alignment and structural fidelitywithout increasing computational complexity. It demonstrates a strong abilityto generate semantically clear and structurally complete images, offering aviable technical path for joint text-image modeling and image generation.</description>
      <author>example@mail.com (Danyi Gao)</author>
      <guid isPermaLink="false">2508.10280v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.10567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpaRC-AD是一种创新的端到端摄像头-雷达融合框架，通过稀疏3D特征对齐和多普勒速度估计，解决了纯视觉方法在恶劣天气、遮挡和速度估计方面的局限性。在多个自动驾驶任务上实现了显著的性能提升，包括检测、跟踪、地图构建、运动预测和轨迹规划。&lt;h4&gt;背景&lt;/h4&gt;端到端自动驾驶系统通过统一优化感知、运动预测和规划来提供更强的性能。然而，基于视觉的方法在恶劣天气条件、部分遮挡和精确速度估计方面存在基本限制，这些是在安全敏感场景中的关键挑战，准确的运动理解和长时域轨迹预测对避免碰撞至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决基于视觉方法的局限性，提出一种基于查询的端到端摄像头-雷达融合框架，用于规划导向的自动驾驶。&lt;h4&gt;方法&lt;/h4&gt;通过稀疏3D特征对齐和多普勒速度估计，实现强大的3D场景表示，用于优化智能体锚点、地图多边形和运动建模。&lt;h4&gt;主要发现&lt;/h4&gt;在多个自动驾驶任务上与最先进的仅视觉基线相比实现了显著改进：3D检测(+4.8% mAP)、多目标跟踪(+8.3% AMOTA)、在线地图(+1.8% mAP)、运动预测(-4.0% mADE)和轨迹规划(-0.1m L2和-9% TPC)。在多个具有挑战性的基准测试上实现了空间相干性和时间一致性，证明了基于雷达的融合在安全关键场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;雷达融合方法可以有效解决视觉方法在恶劣条件下的局限性，在多个自动驾驶任务上实现了性能提升，实现了空间相干性和时间一致性，强调了雷达融合在需要准确运动理解和长时域轨迹预测的安全关键场景中的价值。&lt;h4&gt;翻译&lt;/h4&gt;端到端自动驾驶系统通过统一优化感知、运动预测和规划承诺提供更强的性能。然而，基于视觉的方法在恶劣天气条件、部分遮挡和精确速度估计方面面临基本限制——这些是在安全敏感场景中的关键挑战，准确的运动理解和长时域轨迹预测对避免碰撞至关重要。为解决这些局限性，我们提出了SpaRC-AD，一种基于查询的端到端摄像头-雷达融合框架，用于规划导向的自动驾驶。通过稀疏3D特征对齐和多普勒速度估计，我们实现了强大的3D场景表示，用于优化智能体锚点、地图多边形和运动建模。我们的方法在多个自动驾驶任务上实现了对最先进的仅视觉基线的显著改进，包括3D检测(+4.8% mAP)、多目标跟踪(+8.3% AMOTA)、在线地图(+1.8% mAP)、运动预测(-4.0% mADE)和轨迹规划(-0.1m L2和-9% TPC)。我们在多个具有挑战性的基准测试上实现了空间相干性和时间一致性，包括真实世界的开环nuScenes、长时域T-nuScenes和闭环模拟器Bench2Drive。我们展示了基于雷达的融合在安全关键场景中的有效性，在这些场景中，准确的运动理解和长时域轨迹预测对避免碰撞至关重要。所有实验的源代码可在https://phi-wol.github.io/sparcad/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决纯视觉端到端自动驾驶系统在恶劣天气条件、部分遮挡和精确速度估计方面的局限性。这个问题在现实中非常重要，因为这些条件是自动驾驶面临的常见挑战，准确的运动理解和长时程轨迹预测对于避免碰撞至关重要，直接关系到自动驾驶系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了纯视觉方法的局限性，认识到雷达传感器在长距离检测、直接速度测量和天气鲁棒性方面的独特优势。他们借鉴了SparseDrive的稀疏表示范式和SpaRC的稀疏融合设计，将雷达数据特性与规划需求相结合，设计了查询方法来迭代优化地图和代理表示的运动和位置特征。他们利用反射雷达点的空间接近性作为强归纳偏置，构建了一个统一的端到端优化框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过稀疏3D特征对齐和多普勒速度估计实现强3D场景表示，用于优化代理锚点、地图多段线和运动建模。整体流程包括：1)多模态稀疏特征编码处理摄像头和雷达输入；2)统一稀疏融合利用查询方法在模态间交互；3)并行运动规划联合优化增强的空间场景表示。具体实现中，系统处理360度图像和雷达点云，设计检测和地图查询，实现范围自适应聚合和多模态透视特征对齐，最后通过概率轨迹建模生成优化后的驾驶轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个基于雷达的端到端自动驾驶基准；2)扩展的稀疏融合设计同时处理检测、跟踪和规划；3)整体雷达融合提升多个任务性能；4)多基准评估验证方法有效性。相比之前工作，不同之处在于：专注于摄像头-雷达而非摄像头-激光雷达融合；将雷达集成到端到端优化而非仅用于模块化感知；使用稀疏而非密集表示处理雷达数据；采用查询方法迭代优化表示；特别关注长期轨迹预测一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SpaRC-AD通过创新的摄像头-雷达稀疏融合方法，显著提升了端到端自动驾驶系统在恶劣天气、遮挡场景和长距离检测中的性能，特别是在运动理解和轨迹预测方面，为安全关键自动驾驶应用提供了更可靠的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end autonomous driving systems promise stronger performance throughunified optimization of perception, motion forecasting, and planning. However,vision-based approaches face fundamental limitations in adverse weatherconditions, partial occlusions, and precise velocity estimation - criticalchallenges in safety-sensitive scenarios where accurate motion understandingand long-horizon trajectory prediction are essential for collision avoidance.To address these limitations, we propose SpaRC-AD, a query-based end-to-endcamera-radar fusion framework for planning-oriented autonomous driving. Throughsparse 3D feature alignment, and doppler-based velocity estimation, we achievestrong 3D scene representations for refinement of agent anchors, map polylinesand motion modelling. Our method achieves strong improvements over thestate-of-the-art vision-only baselines across multiple autonomous drivingtasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA),online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectoryplanning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporalconsistency on multiple challenging benchmarks, including real-world open-loopnuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. Weshow the effectiveness of radar-based fusion in safety-critical scenarios whereaccurate motion understanding and long-horizon trajectory prediction areessential for collision avoidance. The source code of all experiments isavailable at https://phi-wol.github.io/sparcad/</description>
      <author>example@mail.com (Philipp Wolters, Johannes Gilg, Torben Teepe, Gerhard Rigoll)</author>
      <guid isPermaLink="false">2508.10567v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes</title>
      <link>http://arxiv.org/abs/2508.10427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://turingmotors.github.io/stride-qa/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STRIDE-QA是一个大规模视觉问答数据集，专为自动驾驶中物理基础的时空推理设计，填补了现有视觉-语言模型在处理动态交通场景方面的不足。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉-语言模型(VLMs)在自动驾驶中应用于支持复杂现实场景的决策制定，但其基于静态网页图像-文本对的训练方式限制了精确时空推理能力，难以理解和预测动态交通场景。&lt;h4&gt;目的&lt;/h4&gt;开发一个支持物理基础推理的大规模视觉问答数据集，特别关注自我中心视角，以提高VLMs在自动驾驶中的时空推理能力。&lt;h4&gt;方法&lt;/h4&gt;构建STRIDE-QA数据集，使用东京100小时的多传感器驾驶数据，包含1600万个问答对和285K帧，通过密集自动生成的注释(3D边界框、分割掩码和多对象轨迹)进行基础支持，设计三种新颖的QA任务支持对象中心和自我中心推理。&lt;h4&gt;主要发现&lt;/h4&gt;现有VLMs在预测一致性方面表现极差(接近零分)，而在STRIDE-QA上微调的VLMs表现出显著性能提升，空间定位成功率达55%，未来运动预测一致性达28%，远高于通用VLMs的接近零分。&lt;h4&gt;结论&lt;/h4&gt;STRIDE-QA为开发更可靠的视觉-语言模型用于安全关键型自主系统奠定了全面基础，显著提升了模型在自动驾驶场景中的时空推理能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)已被应用于自动驾驶，以支持复杂现实场景中的决策制定。然而，它们基于静态网页来源的图像-文本对的训练从根本上限制了精确时空推理能力，而这种能力对于理解和预测动态交通场景是必需的。我们通过STRIDE-QA解决了这一关键差距，这是一个大规模视觉问答(VQA)数据集，用于从自我中心视角进行物理基础的推理。该数据集由东京100小时的多传感器驾驶数据构建，捕捉了多样且具有挑战性的条件，是城市驾驶中时空推理的最大VQA数据集，在285K帧上提供了1600万个问答对。通过密集的自动生成的注释(包括3D边界框、分割掩码和多对象轨迹)进行基础支持，该数据集通过三种新颖的QA任务独特地支持了对象中心和自我中心推理，这些任务需要空间定位和时间预测。我们的基准测试表明，现有的VLMs表现不佳，在预测一致性方面接近零分。相比之下，在STRIDE-QA上微调的VLMs表现出显著的性能提升，在空间定位上达到55%的成功率，在未来运动预测上达到28%的一致性，而通用VLMs的得分接近零。因此，STRIDE-QA为开发更可靠的VLMs用于安全关键的自主系统建立了全面的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉语言模型(VLMs)在自动驾驶领域缺乏精确时空推理能力的问题。这个问题很重要，因为自动驾驶系统需要在复杂多变的交通环境中安全可靠地决策，而缺乏对物体位置、关系和未来运动轨迹的准确理解会导致安全隐患，同时这也是连接大规模多模态预训练与物理人工智能需求之间的关键研究差距。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLMs在自动驾驶应用中的局限性，认识到它们大多在静态网络图像上训练，缺乏动态场景理解能力。他们借鉴了多个现有工作的元素：参考nuScenes等数据集使用多传感器数据，采用BEVFusion进行3D检测，使用PubTracker进行目标跟踪，应用SAM 2.1进行语义分割，并基于模板生成QA对。然而，作者将这些元素创新性地整合到一个专门设计用于生成时空问答对的自动化管道中，形成了独特的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模视觉问答数据集，专门用于训练和评估VLMs在自动驾驶场景中的时空推理能力，不仅包含静态空间关系问题，还包含预测物体未来位置和运动的问题。整体流程包括：1)在东京收集100多小时多传感器驾驶数据；2)以1Hz采样关键帧；3)使用BEVFusion进行3D物体检测；4)通过PubTracker进行多目标跟踪；5)提取物体距离、方向和速度等属性；6)应用SAM 2.1生成语义分割掩码；7)过滤不可靠检测；8)基于模板生成QA对。最终形成包含1600万个QA对的数据集，支持三种推理任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)定义三种新颖VQA任务（物体中心空间QA、自中心空间QA和自中心时空QA）；2)创建包含1600万个QA对的大规模数据集；3)提出模块化、可扩展的自动化注释管道；4)提供物理和时空一致的高质量注释。相比之前工作，STRIDE-QA规模更大、任务更多样（同时支持物体中心和自中心推理）、注释更时空一致、数据来源更真实（自收集而非网络来源），并提出了专门的评估指标来衡量时空推理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出STRIDE-QA这一大规模时空问答数据集，填补了视觉语言模型在自动驾驶领域精确时空推理能力的关键空白，并展示了基于该数据集训练的模型在预测物体未来运动方面的显著性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have been applied to autonomous driving tosupport decision-making in complex real-world scenarios. However, theirtraining on static, web-sourced image-text pairs fundamentally limits theprecise spatiotemporal reasoning required to understand and predict dynamictraffic scenes. We address this critical gap with STRIDE-QA, a large-scalevisual question answering (VQA) dataset for physically grounded reasoning froman ego-centric perspective. Constructed from 100 hours of multi-sensor drivingdata in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is thelargest VQA dataset for spatiotemporal reasoning in urban driving, offering 16million QA pairs over 285K frames. Grounded by dense, automatically generatedannotations including 3D bounding boxes, segmentation masks, and multi-objecttracks, the dataset uniquely supports both object-centric and ego-centricreasoning through three novel QA tasks that require spatial localization andtemporal prediction. Our benchmarks demonstrate that existing VLMs strugglesignificantly, achieving near-zero scores on prediction consistency. Incontrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,achieving 55% success in spatial localization and 28% consistency in futuremotion prediction, compared to near-zero scores from general-purpose VLMs.Therefore, STRIDE-QA establishes a comprehensive foundation for developing morereliable VLMs for safety-critical autonomous systems.</description>
      <author>example@mail.com (Keishi Ishihara, Kento Sasaki, Tsubasa Takahashi, Daiki Shiono, Yu Yamaguchi)</author>
      <guid isPermaLink="false">2508.10427v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Conic Formulations of Transport Metrics for Unbalanced Measure Networks and Hypernetworks</title>
      <link>http://arxiv.org/abs/2508.10888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了锥形Gromov-Wasserstein (CGW)距离，提出了一种基于半耦合的新公式，扩展了框架以比较更一般的网络和超网络结构，建立了CGW度量的基本性质，包括尺度行为、变分收敛性和比较边界，同时推导了其鲁棒性的定量边界。&lt;h4&gt;背景&lt;/h4&gt;Gromov-Wasserstein (GW)最优传输是一种用于比较不同度量空间上定义的概率密度的方法，已成为分析具有复杂数据结构（如点云集合或网络）的重要工具。然而，传统GW方法存在限制，如只能比较等质量的测度，以及对异常值敏感。&lt;h4&gt;目的&lt;/h4&gt;研究Séjourne、Vialard和Peyré提出的锥形Gromov-Wasserstein (CGW)距离，克服传统GW方法的限制，特别是等质量比较和异常值敏感性问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于半耦合的新公式；将框架扩展到超越度量测度空间设置，以比较更一般的网络和超网络结构；建立CGW度量的基本性质，包括在膨胀下的尺度行为、体积增长约束极限下的变分收敛性；与已建立的最优传输度量进行比较边界；推导定量边界，表征CGW度量对基础测度扰动的鲁棒性；开发简单且可证明收敛的块坐标上升算法进行估计。&lt;h4&gt;主要发现&lt;/h4&gt;CGW度量在膨胀下的尺度行为；在体积增长约束极限下的变分收敛性；与其他最优传输度量的比较边界；CGW度量对基础测度扰动的鲁棒性的定量边界。&lt;h4&gt;结论&lt;/h4&gt;提出了CGW度量的新公式，可以处理更一般的网络和超网络结构；该方法在计算上可行且可扩展，在高维和结构化数据集上进行了实验验证。&lt;h4&gt;翻译&lt;/h4&gt;Gromov-Wasserstein (GW)最优传输变体设计用于比较定义在不同度量空间上的概率密度，已成为分析具有复杂结构数据（如点云集合或网络）的重要工具。为克服某些限制，如仅限于比较等质量测度和对异常值的敏感性，最近文献中引入了几种非平衡或部分传输的GW距离松弛。本文关注Séjourne、Vialard和Peyré提出的锥形Gromov-Wasserstein (CGW)距离。我们提供了基于半耦合的新公式，并将框架扩展到超越度量测度空间设置，以比较更一般的网络和超网络结构。通过这个新公式，我们建立了CGW度量的几个基本性质，包括其在膨胀下的尺度行为、体积增长约束极限下的变分收敛性，以及与已建立的最优传输度量的比较边界。我们进一步推导出定量边界，表征CGW度量对基础测度扰动的鲁棒性。CGW的超网络公式允许简单且可证明收敛的块坐标上升算法进行估计，我们通过在合成和真实世界的高维和结构化数据集上的实验证明了我们方法的计算可行性和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Gromov-Wasserstein (GW) variant of optimal transport, designed to compareprobability densities defined over distinct metric spaces, has emerged as animportant tool for the analysis of data with complex structure, such asensembles of point clouds or networks. To overcome certain limitations, such asthe restriction to comparisons of measures of equal mass and sensitivity tooutliers, several unbalanced or partial transport relaxations of the GWdistance have been introduced in the recent literature. This paper is concernedwith the Conic Gromov-Wasserstein (CGW) distance introduced byS\'{e}journ\'{e}, Vialard, and Peyr\'{e}. We provide a novel formulation interms of semi-couplings, and extend the framework beyond the metric measurespace setting, to compare more general network and hypernetwork structures.With this new formulation, we establish several fundamental properties of theCGW metric, including its scaling behavior under dilation, variationalconvergence in the limit of volume growth constraints, and comparison boundswith established optimal transport metrics. We further derive quantitativebounds that characterize the robustness of the CGW metric to perturbations inthe underlying measures. The hypernetwork formulation of CGW admits a simpleand provably convergent block coordinate ascent algorithm for its estimation,and we demonstrate the computational tractability and scalability of ourapproach through experiments on synthetic and real-world high-dimensional andstructured datasets.</description>
      <author>example@mail.com (Mary Chriselda Antony Oliver, Emmanuel Hartman, Tom Needham)</author>
      <guid isPermaLink="false">2508.10888v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets</title>
      <link>http://arxiv.org/abs/2508.10758v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过结合Erwin架构和Native Sparse Attention机制，解决了Transformer在处理大型物理系统数据时的二次方扩展问题，提高了模型效率和感受野，同时在三个物理科学数据集上实现了与原始模型相当或更好的性能。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型在处理大型物理系统数据时具有潜力，但注意力机制的二次方扩展问题限制了其应用效率。&lt;h4&gt;目的&lt;/h4&gt;探索将Erwin架构与Native Sparse Attention (NSA)机制相结合，提高Transformer模型处理大规模物理系统的效率和感受野，解决注意力机制的二次复杂度挑战。&lt;h4&gt;方法&lt;/h4&gt;将NSA机制适应为非序列数据，实现Erwin NSA模型，并在三个物理科学数据集（宇宙学模拟、分子动力学和气压建模）上评估，同时重现Erwin论文的实验结果以验证实现。&lt;h4&gt;主要发现&lt;/h4&gt;在三个物理科学数据集上，Erwin NSA模型的性能与原始Erwin模型相当或更好，成功将NSA机制应用于非序列数据。&lt;h4&gt;结论&lt;/h4&gt;通过结合Erwin架构和NSA机制，有效解决了Transformer在处理大型物理系统数据时的二次方扩展问题，提高了模型的效率和感受野，同时保持了或提高了性能。&lt;h4&gt;翻译&lt;/h4&gt;释放Transformer在大型物理系统数据集上的潜力，需要克服注意力机制的二次方扩展问题。本研究探索将Erwin架构与Native Sparse Attention (NSA)机制相结合，以提高Transformer模型处理大规模物理系统的效率和感受野，解决注意力机制二次复杂度的挑战。我们将NSA机制适应为非序列数据，实现了Erwin NSA模型，并在三个物理科学数据集（宇宙学模拟、分子动力学和气压建模）上进行了评估，其性能与原始Erwin模型相当或更好。此外，我们重现了Erwin论文的实验结果以验证其实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unlocking the potential of transformers on datasets of large physical systemsdepends on overcoming the quadratic scaling of the attention mechanism. Thiswork explores combining the Erwin architecture with the Native Sparse Attention(NSA) mechanism to improve the efficiency and receptive field of transformermodels for large-scale physical systems, addressing the challenge of quadraticattention complexity. We adapt the NSA mechanism for non-sequential data,implement the Erwin NSA model, and evaluate it on three datasets from thephysical sciences -- cosmology simulations, molecular dynamics, and airpressure modeling -- achieving performance that matches or exceeds that of theoriginal Erwin model. Additionally, we reproduce the experimental results fromthe Erwin paper to validate their implementation.</description>
      <author>example@mail.com (Nicolas Lapautre, Maria Marchenko, Carlos Miguel Patiño, Xin Zhou)</author>
      <guid isPermaLink="false">2508.10758v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition</title>
      <link>http://arxiv.org/abs/2508.10469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对基于毫米波雷达的人类动作识别系统，评估了三种主要数据处理方法（DBSCAN、匈牙利算法和卡尔曼滤波）及其组合的性能，旨在提高雷达数据的准确性和连续性，同时降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;人类动作识别在医疗保健、健身追踪和辅助生活技术中起着关键作用。传统基于视觉的HAR系统虽然有效，但存在隐私问题。毫米波雷达传感器提供了一种保护隐私的替代方案，但其点云数据稀疏且嘈杂，带来了挑战。文献中广泛使用了三种主要的数据处理方法来提高雷达数据的质量和连续性，但缺乏对这些方法及其组合的综合评估。&lt;h4&gt;目的&lt;/h4&gt;填补毫米波雷达数据处理方法综合评估的空白，通过详细分析三种方法（单独使用、两两组合以及全部组合）在MiliPoint数据集上的性能，评估识别准确性和计算成本，并提出针对性的改进方法以提高准确性。&lt;h4&gt;方法&lt;/h4&gt;使用MiliPoint数据集对三种数据处理方法（DBSCAN、匈牙利算法和卡尔曼滤波）进行详细性能分析。评估包括：单独使用每种方法、所有可能的成对组合以及三种方法的组合，同时评估识别准确性和计算成本。此外，还提出了针对单个方法的改进措施以提高准确性。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果提供了关于每种方法及其组合的优势和权衡的关键见解，这将指导未来基于毫米波雷达的HAR系统的工作。虽然摘要中没有具体说明结果，但研究应该已经确定了哪种方法或组合在准确性和计算效率方面表现最佳。&lt;h4&gt;结论&lt;/h4&gt;通过对毫米波雷达数据处理方法的全面评估，该研究为基于毫米波雷达的HAR系统提供了宝贵的指导，帮助研究人员和开发者选择最适合其应用场景的数据处理方法，平衡准确性和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;人类动作识别(HAR)在医疗保健、健身追踪和辅助生活技术中起着至关重要的作用。虽然传统的基于视觉的HAR系统有效，但它们带来了隐私问题。毫米波雷达传感器提供了一种保护隐私的替代方案，但由于其点云数据的稀疏和嘈杂特性，带来了挑战。在文献中，三种主要的数据处理方法：基于密度的噪声应用空间聚类(DBSCAN)、匈牙利算法和卡尔曼滤波已被广泛用于提高雷达数据的质量和连续性。然而，对这些方法单独使用和组合使用的综合评估仍然缺乏。本文通过使用MiliPoint数据集对这三种方法进行详细的性能分析来填补这一空白。我们单独评估每种方法、所有可能的成对组合以及三种方法的组合，评估识别准确性和计算成本。此外，我们还提出了针对单个方法的改进措施以提高准确性。我们的结果提供了关于每种方法及其组合的优势和权衡的关键见解，指导未来基于毫米波雷达的HAR系统的工作。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决毫米波雷达传感器产生的稀疏点云数据在人体动作识别中的处理挑战。这个问题很重要，因为传统基于视觉的动作识别系统虽然有效，但存在隐私问题；而毫米波雷达虽然能保护隐私，但其数据稀疏且嘈杂，难以直接用于准确识别。随着智能环境和辅助生活技术的发展，在保护隐私的同时准确识别人体动作变得越来越重要，特别是在医疗保健、健身追踪和老年人护理等敏感应用场景中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有研究中已经应用的三种主要数据处理方法：DBSCAN用于聚类和噪声过滤，匈牙利算法用于数据关联，卡尔曼滤波用于轨迹预测。然而，作者发现这些方法缺乏系统性评估，且大多独立使用而非组合。作者借鉴了mID管道等现有工作，但指出其缺乏系统调参且在稀疏输入时性能下降。基于这些观察，作者设计了一个综合框架，不仅单独评估每种方法，还评估了它们的组合（两两组合和三者组合），并针对每种方法进行了参数优化和特定增强。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过组合三种数据处理技术（DBSCAN、匈牙利算法和卡尔曼滤波）来增强毫米波雷达点云数据的质量，提高人体动作识别的准确性和效率，同时保护用户隐私。整体流程包括：1)将原始雷达帧分割成5个连续段；2)移除原点附近的零填充点；3)使用DBSCAN进行噪声减少和聚类识别；4)应用匈牙利算法进行跨段聚类关联；5)使用卡尔曼滤波预测和修正轨迹；6)比较预测轨迹与真实关键点，选择最佳人类聚类；7)输出最终处理后的数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次对三种主要雷达点云预处理技术及其组合进行了全面的性能分析；2)针对每种方法进行了详细的参数调优；3)提出了针对个体方法的特定增强，如DBSCAN中的垂直加权因子；4)评估了所有可能的二元组合和三者组合，同时考虑了识别准确性和计算成本；5)公开了源代码，促进研究可重复性。相比之前的工作，本研究不仅独立评估各种方法，还系统分析了它们的组合；不仅关注准确性，还考虑了计算效率；针对稀疏数据问题提出了特定增强方法，为实际应用提供了更全面的指导。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性评估和增强三种主流雷达点云预处理技术及其组合，为隐私保护的人体动作识别提供了一种高效且准确的数据处理方法，同时为实际应用中的方法选择提供了重要指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Action Recognition (HAR) plays a crucial role in healthcare, fitnesstracking, and ambient assisted living technologies. While traditional visionbased HAR systems are effective, they pose privacy concerns. mmWave radarsensors offer a privacy preserving alternative but present challenges due tothe sparse and noisy nature of their point cloud data. In the literature, threeprimary data processing methods: Density-Based Spatial Clustering ofApplications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filteringhave been widely used to improve the quality and continuity of radar data.However, a comprehensive evaluation of these methods, both individually and incombination, remains lacking. This paper addresses that gap by conducting adetailed performance analysis of the three methods using the MiliPoint dataset.We evaluate each method individually, all possible pairwise combinations, andthe combination of all three, assessing both recognition accuracy andcomputational cost. Furthermore, we propose targeted enhancements to theindividual methods aimed at improving accuracy. Our results provide crucialinsights into the strengths and trade-offs of each method and theirintegrations, guiding future work on mmWave based HAR systems</description>
      <author>example@mail.com (Maimunatu Tunau, Vincent Gbouna Zakka, Zhuangzhuang Dai)</author>
      <guid isPermaLink="false">2508.10469v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>GPZ: GPU-Accelerated Lossy Compressor for Particle Data</title>
      <link>http://arxiv.org/abs/2508.10305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出GPZ，一种专为GPU大规模粒子数据设计的高性能、有误差边界的有损压缩器，通过四阶段并行流水线和针对性优化，显著提升了压缩效率和吞吐量。&lt;h4&gt;背景&lt;/h4&gt;基于粒子的模拟和点云应用产生大量不规则数据集，对存储、I/O和实时分析构成挑战，传统压缩技术在处理不规则粒子分布和GPU架构限制方面存在困难，导致有限的吞吐量和次优的压缩比。&lt;h4&gt;目的&lt;/h4&gt;设计一种高性能、有误差边界的有损压缩器，专门针对现代GPU上的大规模粒子数据，解决传统压缩技术的局限性。&lt;h4&gt;方法&lt;/h4&gt;GPZ采用新颖的四阶段并行流水线，协同平衡高压缩效率与大规模并行硬件的架构需求；引入针对计算、内存访问和GPU占用的一系列针对性优化，实现接近硬件极限的吞吐量。&lt;h4&gt;主要发现&lt;/h4&gt;GPZ在三种不同GPU架构(工作站、数据中心和边缘)上，使用六个来自不同领域的大规模真实科学数据集进行评估，结果显示GPZ持续且显著优于五种最先进的GPU压缩器，提供高达8倍的更高端到端吞吐量，同时实现更好的压缩比和数据质量。&lt;h4&gt;结论&lt;/h4&gt;GPZ通过创新的架构设计和优化策略，成功解决了大规模粒子数据在GPU上的压缩挑战，为粒子模拟和点云应用提供了高效的压缩解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于粒子的模拟和点云应用产生大量不规则数据集，对存储、I/O和实时分析构成挑战。传统压缩技术在处理不规则粒子分布和GPU架构限制方面存在困难，通常导致有限的吞吐量和次优的压缩比。在本文中，我们提出GPZ，一种专为现代GPU上大规模粒子数据设计的高性能、有误差边界的有损压缩器。GPZ采用新颖的四阶段并行流水线，协同平衡高压缩效率与大规模并行硬件的架构需求。我们引入了一系列针对计算、内存访问和GPU占用的针对性优化，使GPZ能够实现接近硬件极限的吞吐量。我们在三种不同的GPU架构(工作站、数据中心和边缘)上使用来自五个不同领域的六个大规模、真实科学数据集进行了广泛评估。结果表明，GPZ持续且显著优于五种最先进的GPU压缩器，提供高达8倍的更高端到端吞吐量，同时实现更好的压缩比和数据质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Particle-based simulations and point-cloud applications generate massive,irregular datasets that challenge storage, I/O, and real-time analytics.Traditional compression techniques struggle with irregular particledistributions and GPU architectural constraints, often resulting in limitedthroughput and suboptimal compression ratios. In this paper, we present GPZ, ahigh-performance, error-bounded lossy compressor designed specifically forlarge-scale particle data on modern GPUs. GPZ employs a novel four-stageparallel pipeline that synergistically balances high compression efficiencywith the architectural demands of massively parallel hardware. We introduce asuite of targeted optimizations for computation, memory access, and GPUoccupancy that enables GPZ to achieve near-hardware-limit throughput. Weconduct an extensive evaluation on three distinct GPU architectures(workstation, data center, and edge) using six large-scale, real-worldscientific datasets from five distinct domains. The results demonstrate thatGPZ consistently and significantly outperforms five state-of-the-art GPUcompressors, delivering up to 8x higher end-to-end throughput whilesimultaneously achieving superior compression ratios and data quality.</description>
      <author>example@mail.com (Ruoyu Li, Yafan Huang, Longtao Zhang, Zhuoxun Yang, Sheng Di, Jiajun Huang, Jinyang Liu, Jiannan Tian, Xin Liang, Guanpeng Li, Hanqi Guo, Franck Cappello, Kai Zhao)</author>
      <guid isPermaLink="false">2508.10305v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</title>
      <link>http://arxiv.org/abs/2508.10881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://lg-li.github.io/project/tooncomposer&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ToonComposer是一种生成模型，统一了中间帧生成和上色过程，通过稀疏草图注入机制提供精确控制，仅需少量输入就能高效完成卡通制作，显著减少了人工工作量并提高了灵活性。&lt;h4&gt;背景&lt;/h4&gt;传统卡通和动漫制作包含关键帧绘制、中间帧生成和上色等需要大量人工劳动的环节。尽管AI技术有所进步，但现有方法通常分别处理这些阶段，导致错误累积和伪影问题。例如，中间帧生成方法难以处理大幅度动作，而上色方法需要密集的每帧草图。&lt;h4&gt;目的&lt;/h4&gt;引入ToonComposer模型，将中间帧生成和上色统一到一个后关键帧处理阶段，解决现有方法的局限性，减少人工工作量，提高创作灵活性，为艺术家提供更好的AI辅助工具。&lt;h4&gt;方法&lt;/h4&gt;ToonComposer采用稀疏草图注入机制，通过关键帧草图提供精确控制。同时，使用卡通适配方法结合空间低秩适配器，将现代视频基础模型调整为卡通领域，同时保持其时间先验特性。该模型只需一个草图和一个彩色参考帧就能处理稀疏输入，并支持在任意时间位置使用多个草图实现更精确的运动控制。&lt;h4&gt;主要发现&lt;/h4&gt;研究团队创建了PKBench基准，包含手绘草图以模拟真实用例。评估结果显示，ToonComposer在视觉质量、运动一致性和生产效率方面均优于现有方法，为AI辅助卡通制作提供了更优越的解决方案。&lt;h4&gt;结论&lt;/h4&gt;ToonComposer通过统一处理流程和灵活的输入机制，显著减少了卡通制作的人工工作量，提高了创作灵活性，能够有效赋能艺术家在真实场景中的应用，代表了AI辅助卡通制作的重要进步。&lt;h4&gt;翻译&lt;/h4&gt;传统卡通和动漫制作涉及关键帧绘制、中间帧生成和上色等阶段，这些都需要大量人工劳动。尽管最近AI技术有所进步，但现有方法通常分别处理这些阶段，导致错误累积和伪影。例如，中间帧生成方法难以处理大幅度动作，而上色方法需要密集的每帧草图。为解决这一问题，我们引入了ToonComposer，这是一个生成模型，将中间帧生成和上色统一到一个后关键帧处理阶段。ToonComposer采用稀疏草图注入机制，通过关键帧草图提供精确控制。此外，它使用卡通适配方法结合空间低秩适配器，将现代视频基础模型调整为卡通领域，同时保持其时间先验。仅需一个草图和一个彩色参考帧，ToonComposer就能高效处理稀疏输入，同时支持在任意时间位置使用多个草图实现更精确的运动控制。这种双重能力减少了人工工作量并提高了灵活性，赋能艺术家在真实场景中的应用。为评估我们的模型，我们进一步创建了PKBench基准，包含模拟真实用例的手绘草图。我们的评估表明，ToonComposer在视觉质量、运动一致性和生产效率方面优于现有方法，为AI辅助卡通制作提供了更优越、更灵活的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional cartoon and anime production involves keyframing, inbetweening,and colorization stages, which require intensive manual effort. Despite recentadvances in AI, existing methods often handle these stages separately, leadingto error accumulation and artifacts. For instance, inbetweening approachesstruggle with large motions, while colorization methods require dense per-framesketches. To address this, we introduce ToonComposer, a generative model thatunifies inbetweening and colorization into a single post-keyframing stage.ToonComposer employs a sparse sketch injection mechanism to provide precisecontrol using keyframe sketches. Additionally, it uses a cartoon adaptationmethod with the spatial low-rank adapter to tailor a modern video foundationmodel to the cartoon domain while keeping its temporal prior intact. Requiringas few as a single sketch and a colored reference frame, ToonComposer excelswith sparse inputs, while also supporting multiple sketches at any temporallocation for more precise motion control. This dual capability reduces manualworkload and improves flexibility, empowering artists in real-world scenarios.To evaluate our model, we further created PKBench, a benchmark featuringhuman-drawn sketches that simulate real-world use cases. Our evaluationdemonstrates that ToonComposer outperforms existing methods in visual quality,motion consistency, and production efficiency, offering a superior and moreflexible solution for AI-assisted cartoon production.</description>
      <author>example@mail.com (Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan)</author>
      <guid isPermaLink="false">2508.10881v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares</title>
      <link>http://arxiv.org/abs/2508.10732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分析个性化联邦学习(APFL)方法，通过双流最小二乘法解决非IID数据问题，实现集体泛化和个体个性化的平衡。&lt;h4&gt;背景&lt;/h4&gt;个性化联邦学习(PFL)旨在通过协作训练为各个客户端提供个性化模型，但现有方法容易受到非IID数据的挑战，这严重阻碍了集体泛化能力并影响个性化效果。&lt;h4&gt;目的&lt;/h4&gt;解决PFL中的非IID问题，提高个性化模型的泛化能力和个性化效果。&lt;h4&gt;方法&lt;/h4&gt;提出APFL方法，使用基础模型作为冻结骨干网络进行特征提取，并开发双流分析模型：共享主流实现所有客户端的全局泛化，专用精炼流实现每个客户端的本地个性化。&lt;h4&gt;主要发现&lt;/h4&gt;APFL具有异构不变性的理想特性，理论上无论数据在其他客户端间如何分布，每个个性化模型都保持不变；实验结果表明，该方法比最先进基线方法在准确性上提高至少1.10%-15.45%。&lt;h4&gt;结论&lt;/h4&gt;APFL方法有效解决了PFL中的非IID问题，通过双流分析模型同时实现了集体泛化和个体个性化，显著提高了个性化模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;个性化联邦学习(PFL)通过协作训练向各个客户端提供个性化模型方面提出了重大挑战。现有的PFL方法通常容易受到非IID数据的挑战，这严重阻碍了集体泛化能力，进而影响了后续的个性化工作。在本文中，为了解决PFL中的非IID问题，我们通过双流最小二乘法提出了一种分析个性化联邦学习(APFL)方法。在我们的APFL中，我们使用基础模型作为冻结骨干网络进行特征提取。在特征提取器之后，我们开发了双流分析模型，以实现集体泛化和个体个性化。具体来说，我们的APFL包含一个共享的主流，用于所有客户端的全局泛化，以及一个专用的精炼流，用于每个客户端的本地个性化。我们的APFL的分析解决方案使其具有异构不变性的理想特性，理论上意味着无论数据在其他所有客户端之间如何异构分布，每个个性化模型都保持不变。各种数据集上的经验结果也验证了我们的APFL优于最先进的基线方法，在准确性方面具有至少1.10%-15.45%的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized Federated Learning (PFL) has presented a significant challengeto deliver personalized models to individual clients through collaborativetraining. Existing PFL methods are often vulnerable to non-IID data, whichseverely hinders collective generalization and then compromises the subsequentpersonalization efforts. In this paper, to address this non-IID issue in PFL,we propose an Analytic Personalized Federated Learning (APFL) approach viadual-stream least squares. In our APFL, we use a foundation model as a frozenbackbone for feature extraction. Subsequent to the feature extractor, wedevelop dual-stream analytic models to achieve both collective generalizationand individual personalization. Specifically, our APFL incorporates a sharedprimary stream for global generalization across all clients, and a dedicatedrefinement stream for local personalization of each individual client. Theanalytical solutions of our APFL enable its ideal property of heterogeneityinvariance, theoretically meaning that each personalized model remainsidentical regardless of how heterogeneous the data are distributed across allother clients. Empirical results across various datasets also validate thesuperiority of our APFL over state-of-the-art baselines, with advantages of atleast 1.10%-15.45% in accuracy.</description>
      <author>example@mail.com (Kejia Fan, Jianheng Tang, Zhirui Yang, Feijiang Han, Jiaxu Li, Run He, Yajiang Huang, Anfeng Liu, Houbing Herbert Song, Yunhuai Liu, Huiping Zhuang)</author>
      <guid isPermaLink="false">2508.10732v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Towards Agentic AI for Multimodal-Guided Video Object Segmentation</title>
      <link>http://arxiv.org/abs/2508.10572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Multi-Modal Agent的新型智能体系统，用于解决基于引用的视频对象分割问题，该方法利用大语言模型的推理能力生成动态工作流程，与专门工具交互以识别目标对象，在两个多模态条件VOS任务上明显优于先前方法。&lt;h4&gt;背景&lt;/h4&gt;基于引用的视频对象分割是一个多模态问题，需要根据外部提示产生细粒度分割结果。传统方法需要训练专门模型，计算复杂度高且需要大量人工标注。虽然最近视觉-语言基础模型的进展为免训练方法提供了新方向，但现有方法仍依赖固定流程，缺乏适应任务动态性质的灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法缺乏适应任务动态性质灵活性的问题，提出一种更灵活、自适应的解决方案来处理基于引用的视频对象分割任务。&lt;h4&gt;方法&lt;/h4&gt;提出Multi-Modal Agent智能体系统，利用大语言模型的推理能力为每个输入生成动态工作流程，这种自适应过程与一组为不同模态低级任务设计的专门工具迭代交互，通过多模态提示识别目标对象。&lt;h4&gt;主要发现&lt;/h4&gt;在两个多模态条件VOS任务(RVOS和Ref-AVS)上，多模态智能体方法明显优于先前方法；利用通用基础模型可以实现与全监督、特定任务模型相当的性能。&lt;h4&gt;结论&lt;/h4&gt;多模态智能体方法为基于引用的视频对象分割任务提供了更灵活、自适应的解决方案，其动态工作流程设计比固定流程方法更具优势。&lt;h4&gt;翻译&lt;/h4&gt;基于引用的视频对象分割是一个需要根据外部提示产生细粒度分割结果的多模态问题。传统方法通常涉及训练专门模型，这些模型具有高计算复杂度和人工标注工作量。最近视觉-语言基础模型的进展为免训练方法提供了有希望的方向。一些研究已经探索利用这些通用模型进行细粒度分割，实现了与全监督、特定任务模型相当的性能。然而，现有方法依赖于缺乏适应任务动态性质所需灵活性的固定流程。为解决这一局限，我们提出了Multi-Modal Agent，一种新型智能体系统，旨在以更灵活和自适应的方式解决此任务。具体而言，我们的方法利用大语言模型的推理能力为每个输入生成定制化的动态工作流程。这种自适应过程与一组为不同模态低级任务设计的专门工具迭代交互，以识别由多模态提示描述的目标对象。我们的智能体方法在两个多模态条件VOS任务上明显优于先前方法：RVOS和Ref-AVS。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring-based Video Object Segmentation is a multimodal problem thatrequires producing fine-grained segmentation results guided by external cues.Traditional approaches to this task typically involve training specializedmodels, which come with high computational complexity and manual annotationeffort. Recent advances in vision-language foundation models open a promisingdirection toward training-free approaches. Several studies have exploredleveraging these general-purpose models for fine-grained segmentation,achieving performance comparable to that of fully supervised, task-specificmodels. However, existing methods rely on fixed pipelines that lack theflexibility needed to adapt to the dynamic nature of the task. To address thislimitation, we propose Multi-Modal Agent, a novel agentic system designed tosolve this task in a more flexible and adaptive manner. Specifically, ourmethod leverages the reasoning capabilities of large language models (LLMs) togenerate dynamic workflows tailored to each input. This adaptive procedureiteratively interacts with a set of specialized tools designed for low-leveltasks across different modalities to identify the target object described bythe multimodal cues. Our agentic approach demonstrates clear improvements overprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.</description>
      <author>example@mail.com (Tuyen Tran, Thao Minh Le, Truyen Tran)</author>
      <guid isPermaLink="false">2508.10572v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection</title>
      <link>http://arxiv.org/abs/2508.10568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于SAM模型的遥感变化检测方法，通过微调SAM编码器并结合空间-时间特征增强和多尺度解码器融合技术，以及一种新的交叉熵掩码损失函数，有效解决了遥感变化检测中的类别不平衡问题，在多个数据集上超越了现有最先进的方法。&lt;h4&gt;背景&lt;/h4&gt;基础模型在计算机视觉领域取得了显著成功，它们学习的是通用表示，这些表示可以轻松迁移到训练过程中未见的任务。SAM作为基础模型之一，能够准确分割图像中的物体。&lt;h4&gt;目的&lt;/h4&gt;改进遥感变化检测的性能，使其能够在多尺度上稳健地检测变化，并处理变化检测数据集中的高类别不平衡问题。&lt;h4&gt;方法&lt;/h4&gt;1. 通过微调适应SAM编码器用于遥感变化检测；2. 结合空间-时间特征增强技术；3. 应用多尺度解码器融合技术实现多尺度变化检测；4. 提出一种新的交叉熵掩码损失函数来处理类别不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;1. 该方法在四个变化检测数据集（Levir-CD、WHU-CD、CLCD和S2Looking）上超越了现有的最先进方法；2. 在大型复杂的S2Looking数据集上实现了2.5%的F1分数提升；3. 代码已公开在GitHub上。&lt;h4&gt;结论&lt;/h4&gt;通过结合SAM基础模型的强大表征学习能力与专门设计的空间-时间特征增强、多尺度解码器融合和交叉熵掩码损失函数，该方法有效提升了遥感变化检测的性能，特别是在处理复杂场景和类别不平衡问题时表现突出。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在计算机视觉的各个领域都取得了显著成功。它们学习的是通用表示，这些表示可以轻松迁移到训练过程中未见的任务。SAM就是这样一种基础模型，能够准确分割图像中的物体。我们提出通过微调来适应SAM编码器用于遥感变化检测，并结合空间-时间特征增强和多尺度解码器融合，以在多尺度上稳健地检测变化。此外，我们还提出了一种新颖的交叉熵掩码损失函数来处理变化检测数据集中的高类别不平衡问题。我们的方法在四个变化检测数据集上超越了现有的最先进方法。我们在大型复杂的S2Looking数据集上实现了2.5%的F1分数提升。代码可在以下地址获取：https://github.com/humza909/SAM-CEM-CD&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models have achieved significant success in diverse domains ofcomputer vision. They learn general representations that are easilytransferable to tasks not seen during training. One such foundational model isSegment anything model (SAM), which can accurately segment objects in images.We propose adapting the SAM encoder via fine-tuning for remote sensing changedetection (RSCD) along with spatial-temporal feature enhancement (STFE) andmulti-scale decoder fusion (MSDF) to detect changes robustly at multiplescales. Additionally, we propose a novel cross-entropy masking (CEM) loss tohandle high class imbalance in change detection datasets. Our methodoutperforms state-of-the-art (SOTA) methods on four change detection datasets,Levir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement ona large complex S2Looking dataset. The code is available at:https://github.com/humza909/SAM-CEM-CD</description>
      <author>example@mail.com (Humza Naveed, Xina Zeng, Mitch Bryson, Nagita Mehrseresht)</author>
      <guid isPermaLink="false">2508.10568v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Cross-Utterance Speech Contexts for Conformer-Transducer Speech Recognition Systems</title>
      <link>http://arxiv.org/abs/2508.10456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了四种跨语句语音上下文建模方法，应用于流式和非流式Conformer-Transformer自动语音识别系统，提出高效批处理训练方案，实验结果表明跨语句上下文能显著提高识别准确率。&lt;h4&gt;背景&lt;/h4&gt;语音识别系统需要考虑上下文信息以提高识别准确率，但如何高效整合跨语句语音上下文仍是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;评估四种跨语句语音上下文建模方法在Conformer-Transformer ASR系统中的性能，并提出高效训练方案以减少同步开销。&lt;h4&gt;方法&lt;/h4&gt;研究了四种方法：输入音频特征拼接、跨语句编码器嵌入拼接、跨语句编码器嵌入池化投影，以及首次应用于C-T模型的基于块的方法；提出使用拼接语音语句的批处理训练方案；在四个多语言基准数据集上进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;最佳上下文C-T系统在四个任务上实现了统计显著的WER/CER降低，分别为0.9%、1.1%、0.51%和0.98%绝对值，性能与先进模型相当，证明跨语句上下文的有效性。&lt;h4&gt;结论&lt;/h4&gt;跨语句语音上下文建模能有效提高ASR系统性能，基于块的方法表现突出，高效批处理方案能在保持上下文顺序的同时减少同步开销，为语音基础模型整合上下文信息提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了四种跨语句语音上下文建模方法，应用于流式和非流式Conformer-Transformer自动语音识别系统：i)输入音频特征拼接；ii)跨语句编码器嵌入拼接；iii)跨语句编码器嵌入池化投影；或iv)一种首次应用于C-T模型的基于块的新方法。提出了用于上下文C-T的高效批处理训练方案，使用拼接的语音语句在每个小批量中，以最小化同步开销，同时保留跨语句语音上下文的顺序顺序。在三个语言的四个基准语音数据集上进行了实验：用于上下文C-T模型预训练的英语GigaSpeech和中文Wenetspeech语料库；以及用于领域微调的英语DementiaBank Pitt和粤语JCCOCC MoCA老年语音数据集。最佳性能的上下文C-T系统在预训练和微调阶段始终优于不使用跨语句语音上下文的相应基线系统，在四个任务上实现了统计上显著的词错误率或字符错误率平均降低，分别为0.9%、1.1%、0.51%和0.98%绝对值。它们与先进模型的性能竞争性突显了将跨语句语音上下文整合到当前语音基础模型中的潜在益处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates four types of cross-utterance speech contextsmodeling approaches for streaming and non-streaming Conformer-Transformer (C-T)ASR systems: i) input audio feature concatenation; ii) cross-utterance Encoderembedding concatenation; iii) cross-utterance Encoder embedding poolingprojection; or iv) a novel chunk-based approach applied to C-T models for thefirst time. An efficient batch-training scheme is proposed for contextual C-Tsthat uses spliced speech utterances within each minibatch to minimize thesynchronization overhead while preserving the sequential order ofcross-utterance speech contexts. Experiments are conducted on four benchmarkspeech datasets across three languages: the English GigaSpeech and MandarinWenetspeech corpora used in contextual C-T models pre-training; and the EnglishDementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets used indomain fine-tuning. The best performing contextual C-T systems consistentlyoutperform their respective baselines using no cross-utterance speech contextsin pre-training and fine-tuning stages with statistically significant averageword error rate (WER) or character error rate (CER) reductions up to 0.9%,1.1%, 0.51%, and 0.98% absolute (6.0%, 5.4%, 2.0%, and 3.4% relative) on thefour tasks respectively. Their performance competitiveness againstWav2vec2.0-Conformer, XLSR-128, and Whisper models highlights the potentialbenefit of incorporating cross-utterance speech contexts into current speechfoundation models.</description>
      <author>example@mail.com (Mingyu Cui, Mengzhe Geng, Jiajun Deng, Chengxi Deng, Jiawen Kang, Shujie Hu, Guinan Li, Tianzi Wang, Zhaoqing Li, Xie Chen, Xunying Liu)</author>
      <guid isPermaLink="false">2508.10456v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design</title>
      <link>http://arxiv.org/abs/2508.10409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AnalogSeeker，一个面向模拟电路设计的开源基础语言模型，旨在整合领域知识并提供设计辅助。研究团队通过创新的语料库收集策略和知识蒸馏方法解决了数据稀缺和知识复杂性问题，并建立了以微调为中心的训练范式。训练后的模型在知识评估基准上达到85.04%的准确率，比原始模型提高15.67个百分点，与主流商业模型竞争，并在实际设计任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;模拟电路设计领域面临数据稀缺和知识复杂性的挑战，缺乏专门针对这一领域的开源基础语言模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个开源基础语言模型AnalogSeeker，用于模拟电路设计，整合领域知识并提供设计辅助，解决该领域数据稀缺和知识复杂性的挑战。&lt;h4&gt;方法&lt;/h4&gt;1. 采用基于模拟电路领域知识框架的语料库收集策略，整理相关教科书形成文本领域语料库；2. 引入粒度领域知识蒸馏方法，通过多智能体框架将非结构化文本中的隐式知识转化为问答数据对；3. 建立以微调为中心的训练范式，实现邻域自约束监督微调算法；4. 训练Qwen2.5-32B-Instruct模型获得AnalogSeeker。&lt;h4&gt;主要发现&lt;/h4&gt;1. AnalogSeeker在AMSBench-TQA评估基准上达到85.04%的准确率，比原始模型提高15.67个百分点；2. AnalogSeeker与主流商业模型具有竞争力；3. AnalogSeeker在下游运算放大器设计任务中显示出有效性。&lt;h4&gt;结论&lt;/h4&gt;AnalogSeeker作为专门针对模拟电路设计的开源基础语言模型，成功整合了领域知识，通过创新训练方法解决了数据稀缺和知识复杂性挑战，为模拟电路设计领域提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了AnalogSeeker，这是一个面向模拟电路设计的开源基础语言模型的尝试，旨在整合领域知识并提供设计辅助。为了克服该领域数据的稀缺性，我们采用了一种基于模拟电路领域知识框架的语料库收集策略。系统整理和清理了相关子领域的高质量、易获取的教科书，形成文本领域语料库。为了解决模拟电路知识的复杂性，我们引入了粒度领域知识蒸馏方法。将原始、未标记的领域语料库分解为典型的粒度学习节点，通过多智能体框架将非结构化文本中嵌入的隐式知识蒸馏为具有详细推理过程的问答数据对，生成用于微调的细粒度可学习数据集。为了解决训练模拟电路基础模型中未探索的挑战，我们通过理论分析和实验验证探索并分享了我们的训练方法。我们最终建立了以微调为中心的训练范式，定制和实现了邻域自约束监督微调算法。这种方法通过约束训练前后模型输出分布之间的扰动幅度来提高训练效果。在实践中，我们训练Qwen2.5-32B-Instruct模型获得AnalogSeeker，在AMSBench-TQA上达到85.04%的准确率，比原始模型提高了15.67个百分点，并与主流商业模型具有竞争力。此外，AnalogSeeker在下游运算放大器设计任务中也显示出有效性。AnalogSeeker已在https://huggingface.co/analogllm/analogseeker开源供研究使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose AnalogSeeker, an effort toward an open-sourcefoundation language model for analog circuit design, with the aim ofintegrating domain knowledge and giving design assistance. To overcome thescarcity of data in this field, we employ a corpus collection strategy based onthe domain knowledge framework of analog circuits. High-quality, accessibletextbooks across relevant subfields are systematically curated and cleaned intoa textual domain corpus. To address the complexity of knowledge of analogcircuits, we introduce a granular domain knowledge distillation method. Raw,unlabeled domain corpus is decomposed into typical, granular learning nodes,where a multi-agent framework distills implicit knowledge embedded inunstructured text into question-answer data pairs with detailed reasoningprocesses, yielding a fine-grained, learnable dataset for fine-tuning. Toaddress the unexplored challenges in training analog circuit foundation models,we explore and share our training methods through both theoretical analysis andexperimental validation. We finally establish a fine-tuning-centric trainingparadigm, customizing and implementing a neighborhood self-constrainedsupervised fine-tuning algorithm. This approach enhances training outcomes byconstraining the perturbation magnitude between the model's outputdistributions before and after training. In practice, we train theQwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04%accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark,with a 15.67% point improvement over the original model and is competitive withmainstream commercial models. Furthermore, AnalogSeeker also showseffectiveness in the downstream operational amplifier design task. AnalogSeekeris open-sourced at https://huggingface.co/analogllm/analogseeker for researchuse.</description>
      <author>example@mail.com (Zihao Chen, Ji Zhuang, Jinyi Shen, Xiaoyue Ke, Xinyi Yang, Mingjie Zhou, Zhuoyao Du, Xu Yan, Zhouyang Wu, Zhenyu Xu, Jiangli Huang, Li Shang, Xuan Zeng, Fan Yang)</author>
      <guid isPermaLink="false">2508.10409v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models</title>
      <link>http://arxiv.org/abs/2508.10349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, Submitted to INFOCOM2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FlexP-SFL的灵活个性化分割联邦学习方法，使客户端能在有限计算资源条件下进行协作学习同时保持个性化目标&lt;h4&gt;背景&lt;/h4&gt;微调基础模型对个性化下游任务性能至关重要，但客户端数据有限和数据分布异质性阻碍了有效的协作学习&lt;h4&gt;目的&lt;/h4&gt;解决客户端数据有限和数据分布异质性导致的协作问题，提出一种使客户端能够参与协作学习同时保持个性化目标的范式&lt;h4&gt;方法&lt;/h4&gt;提出FlexP-SFL方法，基于分割学习，允许客户端根据资源约束在本地训练模型部分并卸载剩余部分到服务器，同时提出对齐策略提高模型在全局数据上的性能&lt;h4&gt;主要发现&lt;/h4&gt;FlexP-SFL在个性化微调效率和最终准确性方面优于基线模型&lt;h4&gt;结论&lt;/h4&gt;FlexP-SFL是在有限和异构计算资源条件下实现个性化协作学习的有效方法&lt;h4&gt;翻译&lt;/h4&gt;微调基础模型对于在个性化下游任务上获得优异性能至关重要，与使用预训练模型相比更为有效。协作学习可以利用本地客户端的数据集进行微调，但客户端数据有限和数据分布异质性阻碍了有效协作。为应对这一挑战，我们提出了一种灵活的个性化联邦学习范式，使客户端能够在参与协作学习的同时保持个性化目标。考虑到客户端上有限且异构的计算资源，我们引入了灵活的个性化分割联邦学习（FlexP-SFL）。基于分割学习，FlexP-SFL允许每个客户端根据资源约束在本地训练模型的一部分，同时将剩余部分卸载到服务器。此外，我们还提出了一种对齐策略，以提高个性化模型在全局数据上的性能。实验结果表明，FlexP-SFL在个性化微调效率和最终准确性方面优于基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning foundation models is critical for superior performance onpersonalized downstream tasks, compared to using pre-trained models.Collaborative learning can leverage local clients' datasets for fine-tuning,but limited client data and heterogeneous data distributions hinder effectivecollaboration. To address the challenge, we propose a flexible personalizedfederated learning paradigm that enables clients to engage in collaborativelearning while maintaining personalized objectives. Given the limited andheterogeneous computational resources available on clients, we introduce\textbf{flexible personalized split federated learning (FlexP-SFL)}. Based onsplit learning, FlexP-SFL allows each client to train a portion of the modellocally while offloading the rest to a server, according to resourceconstraints. Additionally, we propose an alignment strategy to improvepersonalized model performance on global data. Experimental results show thatFlexP-SFL outperforms baseline models in personalized fine-tuning efficiencyand final accuracy.</description>
      <author>example@mail.com (Tianjun Yuan, Jiaxiang Geng, Pengchao Han, Xianhao Chen, Bing Luo)</author>
      <guid isPermaLink="false">2508.10349v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning</title>
      <link>http://arxiv.org/abs/2508.10299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了联邦知识增强初始化（FedKEI）框架，通过跨客户端和跨任务的知识转移，为医疗联邦学习中的基础模型适配器调整提供有信息的初始值，以帮助医疗机构快速适应新任务或疾病。&lt;h4&gt;背景&lt;/h4&gt;在医疗保健领域，联邦学习被广泛采用以实现隐私保护的协作。随着大型基础模型展现出强大能力，通过成本高效的适配器调整将基础模型应用于联邦学习已成为流行方法。然而，在快速变化的医疗环境中，各机构需要快速适应新疾病。&lt;h4&gt;目的&lt;/h4&gt;开发一种新框架，利用过去的知识来生成适配器调整的初始值，使医疗机构能够更有效地学习新任务或适应新疾病。&lt;h4&gt;方法&lt;/h4&gt;FedKEI首先在服务器端进行全局聚类以跨任务泛化知识，然后优化聚类间和聚类内的权重以个性化知识转移。采用双层优化方案，共同学习客户端间的全局簇内权重，并优化针对每个客户端任务目标的局部跨簇权重。&lt;h4&gt;主要发现&lt;/h4&gt;在三个不同模态的基准数据集（皮肤病学、胸部X光和视网膜OCT）上的实验表明，FedKEI在适应新疾病方面优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;FedKEI框架有效地解决了医疗联邦学习中快速适应新任务的需求，通过知识转移和权重优化提高了性能，为医疗机构的协作学习提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;在医疗保健领域，联邦学习（FL）是一种被广泛采用的框架，使医疗机构能够进行隐私保护的协作。随着大型基础模型（FMs）展现出令人印象深刻的能力，通过成本高效的适配器调整在FL中使用FMs已成为一种流行方法。鉴于快速变化的医疗环境，对于各个客户端来说，在调整适配器的同时借鉴过去的经验来快速适应新任务或疾病至关重要。在这项工作中，我们引入了联邦知识增强初始化（FedKEI），这是一个新框架，利用跨客户端和跨任务的知识转移，为学习新任务的适配器生成有信息的初始值。FedKEI首先在服务器端进行全局聚类过程，跨任务泛化知识，然后优化聚类间的权重（跨簇权重）和每个聚类内的权重（簇内权重），以个性化每个新任务的知识转移。为了促进跨簇权重和簇内权重的更有效学习，我们采用双层优化方案，共同学习客户端间的全局簇内权重，并优化针对每个客户端任务目标的局部跨簇权重。在三个不同模态的基准数据集（包括皮肤病学、胸部X光和视网膜OCT）上的广泛实验证明了FedKEI在适应新疾病方面比最先进方法具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In healthcare, federated learning (FL) is a widely adopted framework thatenables privacy-preserving collaboration among medical institutions. With largefoundation models (FMs) demonstrating impressive capabilities, using FMs in FLthrough cost-efficient adapter tuning has become a popular approach. Given therapidly evolving healthcare environment, it is crucial for individual clientsto quickly adapt to new tasks or diseases by tuning adapters while drawing uponpast experiences. In this work, we introduce Federated Knowledge-EnhancedInitialization (FedKEI), a novel framework that leverages cross-client andcross-task transfer from past knowledge to generate informed initializationsfor learning new tasks with adapters. FedKEI begins with a global clusteringprocess at the server to generalize knowledge across tasks, followed by theoptimization of aggregation weights across clusters (inter-cluster weights) andwithin each cluster (intra-cluster weights) to personalize knowledge transferfor each new task. To facilitate more effective learning of the inter- andintra-cluster weights, we adopt a bi-level optimization scheme thatcollaboratively learns the global intra-cluster weights across clients andoptimizes the local inter-cluster weights toward each client's task objective.Extensive experiments on three benchmark datasets of different modalities,including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI'sadvantage in adapting to new diseases compared to state-of-the-art methods.</description>
      <author>example@mail.com (Danni Peng, Yuan Wang, Kangning Cai, Peiyan Ning, Jiming Xu, Yong Liu, Rick Siow Mong Goh, Qingsong Wei, Huazhu Fu)</author>
      <guid isPermaLink="false">2508.10299v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</title>
      <link>http://arxiv.org/abs/2508.10256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是关于裂缝检测领域的综述研究，重点分析了深度学习在该领域的最新发展趋势，包括学习范式转变、泛化能力提升和数据集获取多样化等方面。作者还介绍了一个新的3D裂缝数据集，并进行了基准测试实验。&lt;h4&gt;背景&lt;/h4&gt;裂缝检测在土木基础设施（如路面、建筑物等）的检查中起着关键作用。近年来，深度学习显著推进了这一领域的发展。尽管该领域已有众多技术和综述论文，但新兴趋势正在重塑这一领域。&lt;h4&gt;目的&lt;/h4&gt;这篇综述旨在系统分析裂缝检测领域的最新趋势，突出代表性工作。同时，作者引入了一个使用3D激光扫描收集的新数据集3DCrack，以支持未来研究，并对常用的深度学习方法（包括最近的基础模型）进行了广泛的基准测试实验，建立了基线。&lt;h4&gt;方法&lt;/h4&gt;作者通过系统性综述方法分析了裂缝检测领域的最新趋势。他们收集并标注了一个使用3D激光扫描的新数据集3DCrack，并进行了广泛的基准测试实验，评估了常用的深度学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;作者发现了裂缝检测领域的几个关键趋势：学习范式的转变（从完全监督学习转向半监督、弱监督、无监督、少样本、领域适应和微调基础模型）、泛化能力的提升（从单数据集性能转向跨数据集评估）以及数据集获取的多样化（从RGB图像转向专业传感器数据）。&lt;h4&gt;结论&lt;/h4&gt;该研究为基于深度学习的裂缝检测领域的演变方法和未来方向提供了见解，并建立了一个新的基准数据集和实验结果，支持了未来的研究工作。&lt;h4&gt;翻译&lt;/h4&gt;裂缝检测在土木基础设施（包括路面、建筑物等的检查）中起着至关重要的作用，近年来深度学习显著推进了这一领域的发展。尽管该领域存在众多技术和综述论文，但新兴趋势正在重塑这一领域。这些转变包括学习范式的转变（从完全监督学习转向半监督、弱监督、无监督、少样本、领域适应和微调基础模型）、泛化能力的提升（从单数据集性能转向跨数据集评估）以及数据集获取的多样化（从RGB图像转向专业传感器数据）。在本综述中，我们系统分析了这些趋势并突出了代表性工作。此外，我们介绍了一个使用3D激光扫描收集的新数据集3DCrack，以支持未来研究，并对常用的深度学习方法（包括最近的基础模型）进行了广泛的基准测试实验，建立了基线。我们的发现为基于深度学习的裂缝检测领域的演变方法和未来方向提供了见解。项目页面：https://github.com/nantonzhang/Awesome-Crack-Detection&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决深度学习在裂缝检测领域中的系统性分析问题，特别是三个关键趋势：学习范式的转变、泛化能力的提升和数据集的多样化。这个问题在现实中非常重要，因为裂缝检测对土木基础设施安全至关重要，及时准确的识别可以预防性维护，降低维修成本，防止严重结构故障。传统方法存在劳动密集、耗时和主观错误等问题，而深度学习方法虽提高了性能，但仍面临标注数据需求量大和泛化能力有限等挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者作为综述论文，没有提出新方法，而是通过系统性地分析裂缝检测领域的最新趋势。作者借鉴了大量现有工作，总结了之前的综述论文，并指出它们未全面分析三个关键趋势。作者还借鉴了计算机视觉领域的其他技术，如CLIP和SAM等基础模型，以及PEFT技术。作者引入了新数据集3DCrack，并对常用深度学习方法进行了基准测试实验。整体思路是分类分析、系统综述和实验验证相结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 作为综述论文，没有单一的核心思想，但整体核心思想是对深度学习裂缝检测领域的最新趋势进行系统性的分类和分析。论文结构包括：背景介绍、学习范式和泛化能力分析、数据集综述、实验和发现、开放挑战和结论。重点分析了七种学习范式（监督、半监督、弱监督、领域适应、少样本、无监督和基础模型），讨论了它们的特点、优缺点和泛化能力，并介绍了各类数据集及其特点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 系统性分析裂缝检测领域的三个关键趋势；2) 引入新数据集3DCrack，具有更高分辨率和多样化路面条件；3) 对常用深度学习方法进行基准测试，包括基础模型；4) 总结当前挑战和未来机会。相比之前工作，不同之处在于采用层次化结构全面分析三个关键趋势，而非仅按任务类型或流水线组织内容；涵盖最新学习范式如基础模型和PEFT技术；强调跨数据集评估和泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统分析深度学习裂缝检测的最新趋势、引入新数据集并进行基准测试，为裂缝检测领域提供了全面的视角和未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crack detection plays a crucial role in civil infrastructures, includinginspection of pavements, buildings, etc., and deep learning has significantlyadvanced this field in recent years. While numerous technical and review papersexist in this domain, emerging trends are reshaping the landscape. These shiftsinclude transitions in learning paradigms (from fully supervised learning tosemi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptationand fine-tuning foundation models), improvements in generalizability (fromsingle-dataset performance to cross-dataset evaluation), and diversification indataset reacquisition (from RGB images to specialized sensor-based data). Inthis review, we systematically analyze these trends and highlightrepresentative works. Additionally, we introduce a new dataset collected with3D laser scans, 3DCrack, to support future research and conduct extensivebenchmarking experiments to establish baselines for commonly used deep learningmethodologies, including recent foundation models. Our findings provideinsights into the evolving methodologies and future directions in deeplearning-based crack detection. Project page:https://github.com/nantonzhang/Awesome-Crack-Detection</description>
      <author>example@mail.com (Xinan Zhang, Haolin Wang, Yung-An Hsieh, Zhongyu Yang, Anthony Yezzi, Yi-Chang Tsai)</author>
      <guid isPermaLink="false">2508.10256v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Metrics and Best Practices for System-Level Inference Performance Benchmarking</title>
      <link>http://arxiv.org/abs/2508.10251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了FMwork，一种针对基础模型（如大语言模型）推理性能基准测试的全面且系统化的方法，通过元指标、参数选择和战略性成本-性能评估三个关键组件，能够在不牺牲准确性的情况下显著提高实验效率。&lt;h4&gt;背景&lt;/h4&gt;基准测试基础模型（如大语言模型）的推理性能（速度）需要理解硬件和软件组件之间复杂的交互作用，但评估每一种可能的测试配置是不切实际、不可行且不必要的。&lt;h4&gt;目的&lt;/h4&gt;解决基础模型推理性能基准测试中面临的实验配置空间过大问题，提供一种高效且准确的测试方法。&lt;h4&gt;方法&lt;/h4&gt;提出FMwork框架，包含三个关键组成部分：1)元指标，考虑基准测试所花费的时间和资源以及结果的相对准确性；2)参数选择策略；3)战略性成本-性能评估方法。该框架通过元指标将完整实验空间表征为可管理的部分。&lt;h4&gt;主要发现&lt;/h4&gt;使用FMwork框架，与真实情况相比，运行实验扫描可提高24倍（加速和/或资源节省）。即使将实验输出大小从1024减少到128个标记，对于使用Llama 3.1 8B模型的评估仍可获得2.7倍的增益，同时保持96.6%的准确性。&lt;h4&gt;结论&lt;/h4&gt;FMwork提供了一种高效的基础模型推理性能基准测试方法，通过智能选择实验参数和评估策略，能够在大幅减少实验工作量的同时保持高准确性。&lt;h4&gt;翻译&lt;/h4&gt;基准测试基础模型（如大语言模型）的推理性能（速度）需要导航广泛的实验领域，以理解硬件和软件组件之间复杂的相互作用。然而，评估每一种可能的测试配置是不切实际、不可行且不必要的。为了应对这一挑战，我们引入了FMwork，这是一种创建受控测试环境的全面且系统化的方法，能够准确反映和表征性能。FMwork包含一组基准测试最佳实践，具有三个关键组成部分：1)元指标，2)参数选择，和3)战略性成本-性能评估。元指标考虑了基准测试所花费的时间和资源，以及与更大量测量结果相比的相对准确性，代表了完整的实验空间。FMwork将元指标付诸实践，并为参数选择和成本-性能分析提供高效策略。使用该框架，我们展示了与真实情况相比，运行实验扫描可提高24倍（加速和/或资源节省）。即使已经将实验子集作为参考点（使用批量大小的2的幂），将实验输出大小从1024减少到128个标记，对于使用Llama 3.1 8B模型的评估仍可获得2.7倍的增益，同时保持96.6%的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Benchmarking inference performance (speed) of Foundation Models such as LargeLanguage Models (LLM) involves navigating a vast experimental landscape tounderstand the complex interactions between hardware and software components.However, evaluating every possible test configuration is impractical,unfeasible and unnecessary. To address this challenge, we introduce FMwork, acomprehensive and methodical approach to creating a controlled testingenvironment that accurately reflects and characterizes performance. FMworkcomprises a set of benchmkaring best practices with three key components: 1)meta-metrics, 2) parameter selection, and 3) strategic cost-performanceevaluation. Meta-metrics account for time and resources spent on benchmarkingand the relative accuracy of the results compared to a larger body ofmeasurements, representing the complete experimental space. FMworkoperationalizes the meta-metrics and provides efficient strategies forparameter selection and cost-performance analysis. Using the framework, we showup to 24x improvement (speedup and/or resource savings) running sweeps ofexperiments compared to the ground truth. Even already considering a subset ofexperiments as reference point (using the power of two for batch sizes),reducing experimental output size from 1024 to 128 tokens yields another 2.7xgain while keeping 96.6% accuracy for an evaluation using Llama 3.1 8B model.</description>
      <author>example@mail.com (Shweta Salaria, Zhuoran Liu, Nelson Mimura Gonzalez)</author>
      <guid isPermaLink="false">2508.10251v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics</title>
      <link>http://arxiv.org/abs/2508.10232v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究人员开发了CellSymphony，一个多模态框架，整合了Xenium空间转录组学数据和组织学图像，实现了亚细胞分辨率的复杂肿瘤组织分析，准确识别细胞类型并发现不同癌症的微环境生态位。&lt;h4&gt;背景&lt;/h4&gt;Xenium是一种新型空间转录组学平台，可对复杂肿瘤组织进行亚细胞分辨率分析。尽管组织学图像富含形态信息，但提取稳健的细胞级特征并将其与空间转录组学数据整合仍是关键挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个灵活的多模态框架，整合Xenium转录组学数据和形态学信息，克服细胞特征提取和数据整合的挑战。&lt;h4&gt;方法&lt;/h4&gt;介绍CellSymphony，一个利用来自Xenium转录组学和组织学图像的基础模型衍生嵌入的多模态框架，在真实单细胞分辨率水平上学习融合空间基因表达和形态上下文的联合表示。&lt;h4&gt;主要发现&lt;/h4&gt;CellSymphony实现了准确的细胞类型注释，并在三种癌症类型中发现了不同的微环境生态位。&lt;h4&gt;结论&lt;/h4&gt;这项工作强调了基础模型和多模态融合在解析复杂组织生态系统中细胞的生理和表型编排方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Xenium是一种新的空间转录组学平台，能够对复杂肿瘤组织进行亚细胞分辨率的分析。尽管组织学图像中富含形态信息，但提取稳健的细胞级特征并将其与空间转录组学数据整合仍然是一个关键挑战。我们介绍了CellSymphony，一个灵活的多模态框架，利用来自Xenium转录组学和组织学图像的基础模型衍生嵌入，在真实的单细胞分辨率水平上。通过学习融合空间基因表达和形态上下文的联合表示，CellSymphony实现了准确的细胞类型注释，并在三种癌症类型中发现了不同的微环境生态位。这项工作强调了基础模型和多模态融合在解析复杂组织生态系统中细胞的生理和表型编排方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Xenium, a new spatial transcriptomics platform, enablessubcellular-resolution profiling of complex tumor tissues. Despite the richmorphological information in histology images, extracting robust cell-levelfeatures and integrating them with spatial transcriptomics data remains acritical challenge. We introduce CellSymphony, a flexible multimodal frameworkthat leverages foundation model-derived embeddings from both Xeniumtranscriptomic profiles and histology images at true single-cell resolution. Bylearning joint representations that fuse spatial gene expression withmorphological context, CellSymphony achieves accurate cell type annotation anduncovers distinct microenvironmental niches across three cancer types. Thiswork highlights the potential of foundation models and multimodal fusion fordeciphering the physiological and phenotypic orchestration of cells withincomplex tissue ecosystems.</description>
      <author>example@mail.com (Paul H. Acosta, Pingjun Chen, Simon P. Castillo, Maria Esther Salvatierra, Yinyin Yuan, Xiaoxi Pan)</author>
      <guid isPermaLink="false">2508.10232v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs</title>
      <link>http://arxiv.org/abs/2508.10180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为For-Value的数据估值框架，用于量化单个训练样本对大型语言模型和视觉语言模型的影响，该方法仅通过前向传递即可高效计算影响分数，避免了传统方法中昂贵的梯度计算。&lt;h4&gt;背景&lt;/h4&gt;量化单个训练样本的影响对于提高大型语言模型和视觉语言模型的透明度和责任感至关重要。然而，现有的数据估值方法通常依赖于Hessian信息或模型重新训练，对于十亿参数模型来说计算成本过高，难以实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展且高效的数据估值方法，能够准确量化训练样本对大型语言模型和视觉语言模型的影响，同时降低计算复杂度。&lt;h4&gt;方法&lt;/h4&gt;作者提出了For-Value，一种仅前向传递的数据估值框架。该方法利用现代基础模型的丰富表示，通过基于单个前向传递的简单闭式表达式计算影响分数，消除了对昂贵梯度计算的需求。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，For-Value通过捕捉训练和验证样本在隐藏表示和预测误差中的对齐，能够准确估计每个样本的影响。大量实验证明，For-Value在识别有影响力的微调示例和有效检测错误标记数据方面匹配或优于基于梯度的基线方法。&lt;h4&gt;结论&lt;/h4&gt;For-Value为大型语言模型和视觉语言模型提供了一种高效且准确的数据估值方法，通过仅使用前向传递即可实现影响估计，大大降低了计算复杂度，同时保持了与更复杂方法相当或更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;量化单个训练样本的影响对于提高大型语言模型和视觉语言模型的透明度和责任感至关重要。然而，现有的数据估值方法通常依赖于Hessian信息或模型重新训练，对于十亿参数模型来说计算成本过高。在这项工作中，我们引入了For-Value，一种仅前向传递的数据估值框架，能够为大型语言模型和视觉语言模型提供可扩展且高效的影响估计。通过利用现代基础模型的丰富表示，For-Value仅基于单个前向传递的简单闭式表达式计算影响分数，从而消除了昂贵梯度计算的需求。我们的理论分析表明，For-Value通过捕捉训练和验证样本在隐藏表示和预测误差中的对齐，准确估计了每个样本的影响。大量实验表明，For-Value在识别有影响力的微调示例和有效检测错误标记数据方面匹配或优于基于梯度的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantifying the influence of individual training samples is essential forenhancing the transparency and accountability of large language models (LLMs)and vision-language models (VLMs). However, existing data valuation methodsoften rely on Hessian information or model retraining, making themcomputationally prohibitive for billion-parameter models. In this work, weintroduce For-Value, a forward-only data valuation framework that enablesscalable and efficient influence estimation for both LLMs and VLMs. Byleveraging the rich representations of modern foundation models, For-Valuecomputes influence scores using a simple closed-form expression based solely ona single forward pass, thereby eliminating the need for costly gradientcomputations. Our theoretical analysis demonstrates that For-Value accuratelyestimates per-sample influence by capturing alignment in hidden representationsand prediction errors between training and validation samples. Extensiveexperiments show that For-Value matches or outperforms gradient-based baselinesin identifying impactful fine-tuning examples and effectively detectingmislabeled data.</description>
      <author>example@mail.com (Wenlong Deng, Jiaming Zhang, Qi Zeng, Christos Thrampoulidis, Boying Gong, Xiaoxiao Li)</author>
      <guid isPermaLink="false">2508.10180v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training</title>
      <link>http://arxiv.org/abs/2508.10160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于预训练大规模基础模型的神经解码方法，用于病理和生理状态估计，实现患者个体化的闭环神经调节治疗。&lt;h4&gt;背景&lt;/h4&gt;神经解码技术可以用于患者个体化的闭环神经调节治疗，但通常需要患者个体化训练。&lt;h4&gt;目的&lt;/h4&gt;探索预训练的大规模基础模型在无需患者个体化训练的情况下进行状态估计的潜力。&lt;h4&gt;方法&lt;/h4&gt;在超过24天的慢性深度脑刺激记录数据上训练基础模型，采用30分钟的扩展上下文窗口，并提出了针对神经电生理数据优化的预训练损失函数，纠正了常见掩码自编码器损失函数由于1/f幂律导致的频率偏差。&lt;h4&gt;主要发现&lt;/h4&gt;使用留一法交叉验证成功解码了帕金森病症状，无需患者个体化训练。&lt;h4&gt;结论&lt;/h4&gt;预训练的大规模基础模型可以有效用于神经解码，无需患者个体化训练，为患者个体化的闭环神经调节治疗提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;神经解码病理和生理状态可以实现对患者个体化的闭环神经调节治疗。最近，预训练大规模基础模型的进展提供了无需患者个体化训练即可进行通用状态估计的可能性。我们展示了一个在超过24天的慢性深度脑刺激记录上训练的基础模型。遵循长时间尺度的症状波动，我们突显了30分钟的扩展上下文窗口。我们提出了针对神经电生理数据优化的预训练损失函数，纠正了常见掩码自编码器损失函数由于1/f幂律导致的频率偏差。我们在一个下游任务中展示了使用留一法交叉验证解码帕金森病症状，无需患者个体化训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural decoding of pathological and physiological states can enablepatient-individualized closed-loop neuromodulation therapy. Recent advances inpre-trained large-scale foundation models offer the potential for generalizedstate estimation without patient-individual training. Here we present afoundation model trained on chronic longitudinal deep brain stimulationrecordings spanning over 24 days. Adhering to long time-scale symptomfluctuations, we highlight the extended context window of 30 minutes. Wepresent an optimized pre-training loss function for neural electrophysiologicaldata that corrects for the frequency bias of common masked auto-encoder lossfunctions due to the 1-over-f power law. We show in a downstream task thedecoding of Parkinson's disease symptoms with leave-one-subject-outcross-validation without patient-individual training.</description>
      <author>example@mail.com (Timon Merk, Saeed Salehi, Richard M. Koehler, Qiming Cui, Maria Olaru, Amelia Hahn, Nicole R. Provenza, Simon Little, Reza Abbasi-Asl, Phil A. Starr, Wolf-Julian Neumann)</author>
      <guid isPermaLink="false">2508.10160v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model</title>
      <link>http://arxiv.org/abs/2508.10110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多模态学习方法，用于人脸识别系统中的形变攻击检测，能够提供文本描述并进行零样本评估。&lt;h4&gt;背景&lt;/h4&gt;形变攻击检测已成为人脸识别系统确保可靠验证场景的重要组成部分。&lt;h4&gt;目的&lt;/h4&gt;提出一种多模态学习方法，为形变攻击检测提供文本描述。&lt;h4&gt;方法&lt;/h4&gt;使用对比语言-图像预训练模型进行零样本评估，分析十种不同文本提示（包括长短文本），在公开面部生物特征数据集开发的面部形变数据集上进行实验，评估五种不同形变生成技术（三种不同媒介）。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架通过零样本评估不仅能实现可推广的形变攻击检测，还能预测最相关的文本片段。&lt;h4&gt;结论&lt;/h4&gt;多模态学习方法在形变攻击检测中具有良好的性能，能够提供文本描述并实现零样本评估。&lt;h4&gt;翻译&lt;/h4&gt;形变攻击检测已成为人脸识别系统确保可靠验证场景的重要组成部分。在本文中，我们提出了一种多模态学习方法，可以为形变攻击检测提供文本描述。我们首先展示了使用对比语言-图像预训练模型对所提出框架进行零样本评估，不仅可以实现可推广的形变攻击检测，还可以预测最相关的文本片段。我们对十种不同的文本提示进行了广泛分析，包括长短文本提示。这些提示是根据人类可理解的文本片段设计的。在公开可用的面部生物特征数据集开发的面部形变数据集上进行了大量实验。我们评估了最先进的预训练神经网络与所提出的框架，在五种不同的形变生成技术（三种不同媒介）的零样本评估中的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-93694-4_14&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Morphing attack detection has become an essential component of facerecognition systems for ensuring a reliable verification scenario. In thispaper, we present a multimodal learning approach that can provide a textualdescription of morphing attack detection. We first show that zero-shotevaluation of the proposed framework using Contrastive Language-ImagePretraining (CLIP) can yield not only generalizable morphing attack detection,but also predict the most relevant text snippet. We present an extensiveanalysis of ten different textual prompts that include both short and longtextual prompts. These prompts are engineered by considering the humanunderstandable textual snippet. Extensive experiments were performed on a facemorphing dataset that was developed using a publicly available face biometricdataset. We present an evaluation of SOTA pre-trained neural networks togetherwith the proposed framework in the zero-shot evaluation of five differentmorphing generation techniques that are captured in three different mediums.</description>
      <author>example@mail.com (Sushrut Patwardhan, Raghavendra Ramachandra, Sushma Venkatesh)</author>
      <guid isPermaLink="false">2508.10110v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>DINOv3</title>
      <link>http://arxiv.org/abs/2508.10104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DINOv3是一个通过自监督学习实现的视觉基础模型，通过扩展数据集和模型规模、引入Gram锚定方法以及应用后处理策略，在各种视觉任务上取得了卓越性能，无需微调就超过了专业化的最先进技术。&lt;h4&gt;背景&lt;/h4&gt;自监督学习有望消除人工数据标注的需求，使模型能够轻松扩展到大规模数据集和更大的架构。这种训练范式可以从各种来源学习视觉表示，使用单一算法处理从自然图像到航空图像等多种数据类型。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的视觉基础模型，实现自监督学习的愿景，在各种视觉任务上达到最先进性能，同时保持对不同数据类型和任务的适用性。&lt;h4&gt;方法&lt;/h4&gt;通过精心准备数据、设计和优化来扩展数据集和模型规模；引入Gram锚定方法解决密集特征图在长期训练中的退化问题；应用后处理策略增强模型在分辨率、模型大小和与文本对齐方面的灵活性。&lt;h4&gt;主要发现&lt;/h4&gt;DINOv3产生了高质量的密集特征，在各种视觉任务上取得了卓越的性能，显著超过了之前的自监督和弱监督基础模型。该模型无需微调就在广泛的设置中优于专业化的最先进技术。&lt;h4&gt;结论&lt;/h4&gt;DINOv3是一个通用的视觉基础模型，代表了实现自监督学习愿景的重要里程碑。通过提供可扩展的解决方案，DINOv3视觉模型套件旨在推进各种任务和数据上的最先进技术，满足不同的资源限制和部署场景需求。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习有望消除对人工数据标注的需求，使模型能够轻松扩展到大规模数据集和更大的架构。由于不针对特定任务或领域，这种训练范式有可能从各种来源（从自然图像到航空图像）学习视觉表示——使用单一算法。本技术报告介绍了DINOv3，这是通过利用简单而有效的策略实现这一愿景的重要里程碑。首先，我们通过精心准备数据、设计和优化，利用扩展数据集和模型规模的优势。其次，我们引入了一种称为Gram锚定的新方法，它有效地解决了在长期训练过程中密集特征图退化的已知但尚未解决的问题。最后，我们应用后处理策略，进一步增强我们的模型在分辨率、模型大小和与文本对齐方面的灵活性。因此，我们提出了一个通用的视觉基础模型，在广泛的设置中优于专业化的最先进技术，无需微调。DINOv3产生高质量的密集特征，在各种视觉任务上取得了卓越的性能，显著超过了之前的自监督和弱监督基础模型。我们还分享了DINOv3视觉模型套件，旨在通过为各种资源限制和部署场景提供可扩展的解决方案，在广泛的任务和数据上推进最先进的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning holds the promise of eliminating the need for manualdata annotation, enabling models to scale effortlessly to massive datasets andlarger architectures. By not being tailored to specific tasks or domains, thistraining paradigm has the potential to learn visual representations fromdiverse sources, ranging from natural to aerial images -- using a singlealgorithm. This technical report introduces DINOv3, a major milestone towardrealizing this vision by leveraging simple yet effective strategies. First, weleverage the benefit of scaling both dataset and model size by careful datapreparation, design, and optimization. Second, we introduce a new method calledGram anchoring, which effectively addresses the known yet unsolved issue ofdense feature maps degrading during long training schedules. Finally, we applypost-hoc strategies that further enhance our models' flexibility with respectto resolution, model size, and alignment with text. As a result, we present aversatile vision foundation model that outperforms the specialized state of theart across a broad range of settings, without fine-tuning. DINOv3 produceshigh-quality dense features that achieve outstanding performance on variousvision tasks, significantly surpassing previous self- and weakly-supervisedfoundation models. We also share the DINOv3 suite of vision models, designed toadvance the state of the art on a wide spectrum of tasks and data by providingscalable solutions for diverse resource constraints and deployment scenarios.</description>
      <author>example@mail.com (Oriane Siméoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothée Darcet, Théo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Hervé Jégou, Patrick Labatut, Piotr Bojanowski)</author>
      <guid isPermaLink="false">2508.10104v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation</title>
      <link>http://arxiv.org/abs/2508.09626v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAD-Splat的新型3D航空场景语义分割方法，通过引入高斯点丢弃模块和高置信度伪标签生成流程，有效解决了传统方法难以处理的语义模糊问题，提高了分割准确性和一致性。&lt;h4&gt;背景&lt;/h4&gt;传统方法难以处理航空图像中由尺度变化和结构遮挡引起的语义模糊问题，这限制了分割的准确性和一致性。&lt;h4&gt;目的&lt;/h4&gt;解决3D航空场景语义分割中的语义模糊问题，提高分割准确性和一致性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为SAD-Splat的新型3D-AVS-SS方法，引入了一个高斯点丢弃模块，该模块结合了语义置信度估计和基于Hard Concrete分布的可学习稀疏机制，有效消除了冗余和语义模糊的高斯点；同时集成了一个高置信度伪标签生成流程，利用2D基础模型在真实标签有限时增强监督。&lt;h4&gt;主要发现&lt;/h4&gt;SAD-Splat在分割准确性和表示紧凑性之间取得了良好的平衡，为3D航空场景理解提供了一种高效且可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明SAD-Splat在分割准确性和表示紧凑性方面表现优秀，为3D航空场景理解提供了高效可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在3D航空场景语义分割任务中，传统方法难以解决航空图像中由尺度变化和结构遮挡引起的语义模糊问题。这限制了它们的分割准确性和一致性。为了应对这些挑战，我们提出了一种名为SAD-Splat的新型3D航空场景语义分割方法。我们的方法引入了一个高斯点丢弃模块，该模块将语义置信度估计与基于Hard Concrete分布的可学习稀疏机制相结合。该模块有效消除了冗余和语义模糊的高斯点，提高了分割性能和表示紧凑性。此外，SAD-Splat集成了一个高置信度伪标签生成流程，它利用2D基础模型在真实标签有限时增强监督，从而进一步提高分割准确性。为了推进该领域的研究，我们引入了一个具有挑战性的基准数据集：3D航空语义，其中包含具有稀疏注释的多样化真实世界航空场景。实验结果表明，SAD-Splat在分割准确性和表示紧凑性之间取得了良好的平衡。它为3D航空场景理解提供了一种高效且可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D航空视角场景语义分割（3D-AVS-SS）中的两个核心挑战：一是航空图像中因尺度变化和结构遮挡导致的语义歧义问题，二是3D高斯溅射框架下模型在语义模糊区域产生冗余高斯点的问题。这个问题在现实中非常重要，因为3D-AVS-SS在土地使用监测、城市规划和灾害响应等多种遥感应用中起着关键作用，而当前方法难以处理航空图像特有的挑战，限制了分割的准确性和一致性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D-AVS-SS任务中的语义歧义和高斯点冗余问题，发现2D基础模型直接应用于航空图像会产生噪声和不一致性。他们借鉴了3D高斯溅射（3DGS）框架、特征3DGS的语义嵌入方法、SAM和GeoRSCLIP用于生成置信度图，以及Hard Concrete分布实现可微分二值丢弃。基于这些现有工作，作者创新性地设计了语义感知丢弃模块结合语义置信度估计和可学习稀疏机制，以及高置信度伪标签生成管道，解决了现有方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自适应修剪冗余高斯点解决语义歧义和结构冗余问题，同时利用高置信度伪标签增强监督信号，实现更准确、更紧凑的3D航空场景语义分割。整体流程分为三阶段：1）预处理阶段使用SAM和GeoRSCLIP生成置信度图和可靠伪标签；2）训练阶段联合优化语义特征学习和高斯点丢弃，定期根据综合丢弃概率修剪高斯点；3）推理阶段使用优化后的3D高斯表示进行语义渲染和预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）语义感知的高斯点丢弃机制，结合语义置信度和可学习稀疏机制自适应修剪冗余点；2）高置信度伪标签生成管道，通过多指标过滤确保监督可靠性；3）构建了3D航空语义（3D-AS）基准数据集。相比之前工作，SAD-Splat不仅显式建模3D结构提供更好的多视图一致性，还通过自适应修剪解决了高斯点过饱和问题，使用高置信度监督减轻语义混淆，在复杂场景中表现更稳定且模型更紧凑（高斯点数量减少82%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAD-Splat通过语义感知的高斯点丢弃机制和高置信度伪标签生成方法，有效解决了3D航空视角场景语义分割中的语义歧义和冗余表示问题，实现了更准确、更紧凑的3D场景理解，并为此领域提供了新的基准数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),traditional methods struggle to address semantic ambiguity caused by scalevariations and structural occlusions in aerial images. This limits theirsegmentation accuracy and consistency. To tackle these challenges, we propose anovel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussianpoint drop module, which integrates semantic confidence estimation with alearnable sparsity mechanism based on the Hard Concrete distribution. Thismodule effectively eliminates redundant and semantically ambiguous Gaussianpoints, enhancing both segmentation performance and representation compactness.Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generationpipeline. It leverages 2D foundation models to enhance supervision whenground-truth labels are limited, thereby further improving segmentationaccuracy. To advance research in this domain, we introduce a challengingbenchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diversereal-world aerial scenes with sparse annotations. Experimental resultsdemonstrate that SAD-Splat achieves an excellent balance between segmentationaccuracy and representation compactness. It offers an efficient and scalablesolution for 3D aerial scene understanding.</description>
      <author>example@mail.com (Xu Tang, Junan Jia, Yijing Wang, Jingjing Ma, Xiangrong Zhang)</author>
      <guid isPermaLink="false">2508.09626v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models</title>
      <link>http://arxiv.org/abs/2508.10770v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了视觉语言模型在空间物理推理方面的能力，发现当前模型表现不佳，主要原因是类人先验偏见和缺乏深度推理。通过监督微调和基于规则的强化学习改进Qwen2.5-VL-7B模型，显著提升了其空间物理推理能力，但模型在新物理场景中的泛化能力仍然有限。&lt;h4&gt;背景&lt;/h4&gt;空间物理推理是理解真实物理世界的基础能力，也是构建强大世界模型的关键步骤。虽然最近的视觉语言模型在多模态数学和纯空间理解等专业领域取得了显著进展，但它们在空间物理推理方面的能力在很大程度上仍未被探索。&lt;h4&gt;目的&lt;/h4&gt;对主流视觉语言模型进行全面诊断分析，揭示它们在空间物理推理任务上的表现不足，并提出改进方法以提升模型在这一关键任务上的能力。&lt;h4&gt;方法&lt;/h4&gt;采用监督微调随后基于规则的强化学习对Qwen2.5-VL-7B模型进行改进。&lt;h4&gt;主要发现&lt;/h4&gt;当前主流视觉语言模型在空间物理推理任务上表现不佳，这种表现不足主要归因于类人先验偏见和缺乏深度推理。通过提出的改进方法，模型的空间物理推理能力得到显著提升，超越了领先的专有模型，但模型在新物理场景中的泛化能力仍然有限。&lt;h4&gt;结论&lt;/h4&gt;尽管通过监督微调和基于规则的强化学习改进了模型的空间物理推理能力，但模型在新物理场景中的泛化能力仍然有限，这突显了在空间物理推理方面需要新方法的紧迫性。&lt;h4&gt;翻译&lt;/h4&gt;空间物理推理是理解真实物理世界的基础能力，是构建强大世界模型的关键步骤。虽然最近的视觉语言模型在多模态数学和纯空间理解等专业领域取得了显著进展，但它们在空间物理推理方面的能力在很大程度上仍未被探索。本文对主流视觉语言模型进行了全面的诊断分析，揭示当前模型在这一关键任务上表现不佳。进一步详细分析表明，这种表现不足主要归因于类人先验偏见和缺乏深度推理。为应对这些挑战，我们应用监督微调随后基于规则的强化学习对Qwen2.5-VL-7B进行改进，显著提升了空间物理推理能力，超越了领先的专有模型。尽管如此，尽管取得了这一成功，模型对新物理场景的泛化能力仍然有限——这突显了在空间物理推理方面需要新方法的紧迫性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉语言模型(VLMs)在空间物理推理能力不足的问题。这个问题很重要，因为空间物理推理是构建稳健世界模型的基础能力，对实现通用人工智能(AGI)至关重要；同时，强大的物理认知能力能直接提升具身AI任务的表现，帮助AI代理在现实世界中有效运作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先对主流VLMs进行全面诊断分析，识别出模型在空间物理推理上的三种主要错误：视觉感知错误、物理推理错误和因果推理错误。同时发现模型存在类似人类的偏见(难度偏见和高度偏见)。基于这些发现，作者设计了两阶段训练方法：先进行监督微调(SFT)提供高质量思维链示例，再应用基于规则的强化学习(RL)优化模型。这种方法借鉴了多模态数学推理和空间理解领域的研究，但将其应用到空间物理推理这一新领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过监督微调提供高质量的思维链推理示例作为强化学习的初始化，再使用基于规则的强化学习进一步优化模型，使其能更好地理解和应用空间物理推理规则。整体流程包括：1)数据准备，使用ShapeStacks训练集，其中部分样本蒸馏出思维链推理响应；2)监督微调阶段，使用LoRA方法对Qwen2.5-VL-7B进行40个epoch微调；3)强化学习阶段，使用veRL框架和GRPO算法进行8个epoch全参数微调，总奖励由格式奖励(10%)和答案奖励(90%)组成；4)在ShapeStacks测试集上评估改进后的模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次对VLMs在空间物理推理任务进行全面诊断分析；揭示模型存在类似人类的偏见和缺乏深度推理的问题；提出结合监督微调和强化学习的两阶段训练方法；验证该方法能显著提升模型性能并超过领先专有模型；系统测试模型在2D/3D、动态/静态和高度变化方面的泛化能力。相比之前工作，本文专注于涉及物理规律的空间推理而非纯空间理解或数学推理；不仅评估性能还深入分析推理行为和错误来源；采用SFT+RL组合方法而非单一训练范式；对改进模型进行全面泛化能力测试。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过诊断分析揭示了视觉语言模型在空间物理推理方面的局限性，并提出结合监督微调和强化学习的有效改进方法，显著提升了模型性能，同时指出了当前模型在泛化到新物理场景方面的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-physical reasoning, a foundation capability for understanding the realphysics world, is a critical step towards building robust world models. Whilerecent vision language models (VLMs) have shown remarkable progress inspecialized domains like multimodal mathematics and pure spatial understanding,their capability for spatio-physical reasoning remains largely unexplored. Thispaper provides a comprehensive diagnostic analysis of mainstream VLMs,revealing that current models perform inadequately on this crucial task.Further detailed analysis shows that this underperformance is largelyattributable to biases caused by human-like prior and a lack of deep reasoning.To address these challenges, we apply supervised fine-tuning followed byrule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significantimprovements in spatio-physical reasoning capabilities and surpassing leadingproprietary models. Nevertheless, despite this success, the model'sgeneralization to new physics scenarios remains limited -- underscoring thepressing need for new approaches in spatio-physical reasoning.</description>
      <author>example@mail.com (Tiancheng Han, Yunfei Gao, Yong Li, Wuzhou Yu, Qiaosheng Zhang, Wenqi Shao)</author>
      <guid isPermaLink="false">2508.10770v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</title>
      <link>http://arxiv.org/abs/2508.06259v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SIFThinker，一种空间感知的'图像思考'框架，通过交错增强深度的边界框和自然语言实现注意力校正和图像区域聚焦，解决了当前多模态大语言模型在复杂视觉任务中的挑战。&lt;h4&gt;背景&lt;/h4&gt;当前多模态大语言模型在复杂视觉任务(如空间理解、细粒度感知)方面仍面临重大挑战，先前方法虽尝试整合视觉推理，但未能有效利用空间提示进行注意力校正。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够模仿人类视觉感知的框架，通过注意力校正和图像区域聚焦来提升模型在空间理解和细粒度视觉感知方面的能力。&lt;h4&gt;方法&lt;/h4&gt;提出SIFThinker框架，包含两个主要贡献：1)反向扩展-前向推理策略，生成交错图像-思维链用于过程级监督，构建SIF-50K数据集；2)GRPO-SIF强化训练范式，将深度感知的视觉定位集成到统一推理流程中。&lt;h4&gt;主要发现&lt;/h4&gt;SIFThinker在空间理解和细粒度视觉感知任务上优于最先进方法，同时保持强大的通用能力，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过空间感知的思考和注意力校正机制，SIFThinker能够有效提升多模态大语言模型在复杂视觉任务中的表现。&lt;h4&gt;翻译&lt;/h4&gt;当前多模态大语言模型在复杂视觉任务(如空间理解、细粒度感知)方面仍面临重大挑战。先前方法尝试整合视觉推理，然而它们未能利用空间提示进行注意力校正，迭代地完善其对提示相关区域的关注。在本文中，我们介绍了SIFThinker，一种空间感知的'图像思考'框架，模仿人类视觉感知。具体而言，SIFThinker通过交错增强深度的边界框和自然语言，实现注意力校正和图像区域聚焦。我们的贡献有两方面：首先，我们引入反向扩展-前向推理策略，促进交错图像-思维链的生成，用于过程级监督，进而构建SIF-50K数据集。此外，我们提出GRPO-SIF，一种强化训练范式，将深度感知的视觉定位集成到统一的推理流程中，教会模型动态校正和关注提示相关区域。大量实验表明，SIFThinker在空间理解和细粒度视觉感知方面优于最先进的方法，同时保持强大的通用能力，突显了我们方法的有效性。代码：https://github.com/zhangquanchen/SIFThinker。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)在复杂视觉任务（如空间理解、细粒度感知）中的挑战，特别是现有方法未能利用空间提示进行注意力校正来迭代地改进对提示相关区域的关注。这个问题很重要，因为视觉理解是计算机视觉的基本任务，而人类视觉感知是动态的，涉及注意力转换和底层空间感知。当前模型处理图像时往往采用全局统一方式，无法模拟人类如何逐步关注相关区域并考虑3D空间关系，这限制了模型在需要精确空间推理和细粒度感知的任务上的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察人类视觉感知特点（逐步关注区域、考虑3D空间关系、动态校正注意力）来启发方法设计。他们分析了现有方法的局限性：传统方法忽略动态注意力转换，早期分步方法切断推理链连续性，外部工具依赖方法不够内在，空间感知方法关注整个图像而非相关区域。因此，作者设计了具有3D感知的自适应图像聚焦机制，将视觉感知和空间感知整合到统一框架。该方法借鉴了视觉思维链推理、强化学习训练范式、深度估计技术和边界框定位等现有工作，但进行了创新性整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是模仿人类视觉感知过程，通过空间感知的图像聚焦机制使模型在推理过程中动态关注和校正提示相关区域。整体流程分为三个阶段：1)数据生成：构建SIF-50K数据集，使用反向扩展-前向推理策略生成图像-文本交错思维链；2)预热阶段：通过监督微调使模型生成结构化推理链；3)强化学习阶段：基于GRPO框架，使用四个奖励函数（格式奖励、渐进答案准确性奖励、定位奖励、深度一致性奖励）优化模型的空间感知能力，并引入分层交并比(HIoU)评估定位质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)空间感知的'图像思考'框架，首个结合自适应聚焦机制和3D感知的框架；2)反向扩展-前向推理策略，用于构建图像-文本交错思维链；3)GRPO-SIF强化训练范式，整合四个专门奖励函数；4)分层交并比(HIoU)评估边界框质量。相比之前工作的不同：与传统方法相比，SIFThinker模拟人类动态视觉过程；与早期分步方法相比，保持推理链连续性；与外部工具依赖方法相比，内在支持图像-文本交错推理；与空间感知方法相比，专注于提示相关区域；与分离定位和生成的方法相比，整合3D感知作为视觉理解基础。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SIFThinker通过引入空间感知的图像聚焦机制和强化训练范式，首次将3D空间感知与动态视觉定位相结合，显著提升了多模态大语言模型在空间理解和细粒度视觉感知任务中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current multimodal large language models (MLLMs) still face significantchallenges in complex visual tasks (e.g., spatial understanding, fine-grainedperception). Prior methods have tried to incorporate visual reasoning, however,they fail to leverage attention correction with spatial cues to iterativelyrefine their focus on prompt-relevant regions. In this paper, we introduceSIFThinker, a spatially-aware "think-with-images" framework that mimics humanvisual perception. Specifically, SIFThinker enables attention correcting andimage region focusing by interleaving depth-enhanced bounding boxes and naturallanguage. Our contributions are twofold: First, we introduce areverse-expansion-forward-inference strategy that facilitates the generation ofinterleaved image-text chains of thought for process-level supervision, whichin turn leads to the construction of the SIF-50K dataset. Besides, we proposeGRPO-SIF, a reinforced training paradigm that integrates depth-informed visualgrounding into a unified reasoning pipeline, teaching the model to dynamicallycorrect and focus on prompt-relevant regions. Extensive experiments demonstratethat SIFThinker outperforms state-of-the-art methods in spatial understandingand fine-grained visual perception, while maintaining strong generalcapabilities, highlighting the effectiveness of our method. Code:https://github.com/zhangquanchen/SIFThinker.</description>
      <author>example@mail.com (Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang)</author>
      <guid isPermaLink="false">2508.06259v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces</title>
      <link>http://arxiv.org/abs/2508.09950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种点云监督的本体感知运动强化学习方法，用于提升足式机器人在空间受限的爬行环境中的运动能力，无需依赖外部传感器。&lt;h4&gt;背景&lt;/h4&gt;在爬行空间中，基于外部感知的运动学习方法受限于传感器在低可见度条件下的大噪声和误差；而基于本体感知的运动方法难以穿越爬行空间，因为只能推断地面特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在爬行空间中有效导航的足式机器人运动控制方法，减少对外部传感器的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出了一种点云监督的本体感知运动强化学习方法，包括：设计状态估计网络利用历史本体感知数据估计地面和空间特征及碰撞状态；将点云表示在极坐标中并提出处理方法提取特征；设计全面的奖励函数指导机器人在碰撞后穿越爬行空间。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与现有方法相比，该方法在爬行空间中表现出更敏捷的运动能力。&lt;h4&gt;结论&lt;/h4&gt;该研究增强了足式机器人在不需要外部传感器的情况下穿越空间受限环境的能力。&lt;h4&gt;翻译&lt;/h4&gt;在空间受限结构（称为爬行空间）中的足式运动具有挑战性。在爬行空间中，当前基于外部感知的运动学习方法受限于传感器在可能的低可见度条件下的大噪声和误差，而当前基于本体感知的运动学习方法难以穿越爬行空间，因为只能推断地面特征。在本研究中，提出了一种用于爬行空间中足式机器人的点云监督本体感知运动强化学习方法。设计了一个状态估计网络，使用历史本体感知传感器数据估计机器人周围的地面和空间特征以及机器人的碰撞状态。点云在极坐标系中表示，并提出了一种点云处理方法，用于有效提取用于监督状态估计网络学习的地面和空间特征。设计了全面的奖励函数，指导机器人在碰撞后穿越爬行空间。实验表明，与现有方法相比，我们的方法在爬行空间中表现出更敏捷的运动。这项研究增强了足式机器人穿越空间受限环境的能力，而无需外部传感器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决腿式机器人在空间受限环境（如低天花板隧道和小洞穴）中的运动控制问题。在现实应用中，这个问题很重要，因为地震救援、矿难救援等场景需要机器人在这些危险且人难以进入的环境中作业。现有方法要么依赖外部传感器（在低可见度条件下性能下降），要么无法处理天花板等空间结构，限制了机器人在这些环境中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于外部感知的方法在低可见度条件下的局限性，以及基于本体感知的方法无法处理空间受限环境的问题。然后设计了点云监督的本体感受强化学习框架（PPL），借鉴了强化学习在腿式机器人中的应用、点云处理技术和课程学习等方法，但创新性地将点云表示为极坐标形式，使MLP能够高效处理点云数据，并设计了专门的状态估计网络和奖励函数来处理爬行空间的特殊挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用点云数据作为监督信息，训练本体感知网络来估计机器人的周围环境特征和碰撞状态，使机器人能够在不依赖外部传感器的情况下穿越爬行空间。整体流程包括：1）网络架构设计（Actor、Critic和PPL-Net）；2）点云预处理（转换为极坐标表示）；3）状态估计（估计速度、碰撞状态和环境特征）；4）训练过程（使用课程学习和领域随机化）；5）部署与测试（在模拟器和真实机器人上验证）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）点云监督的本体感受强化学习框架；2）极坐标表示的点云处理方法；3）全面的状态估计网络（PPL-Net）；4）智能奖励设计（包括碰撞惩罚和碰撞后速度奖励）。相比之前的工作，不同之处在于：不依赖外部传感器（在低可见度条件下仍然有效）；不仅考虑地面特征，还考虑空间特征（如天花板位置）；使用极坐标表示点云，简化了处理流程；设计了专门的奖励函数来处理爬行空间的特殊挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于点云监督的本体感受强化学习方法，使腿式机器人能够在不依赖外部传感器的情况下，高效穿越空间受限的爬行环境，包括低照明和烟雾等低可见度条件。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The legged locomotion in spatially constrained structures (called crawlspaces) is challenging. In crawl spaces, current exteroceptive locomotionlearning methods are limited by large noises and errors of the sensors inpossible low visibility conditions, and current proprioceptive locomotionlearning methods are difficult in traversing crawl spaces because only groundfeatures are inferred. In this study, a point cloud supervised proprioceptivelocomotion reinforcement learning method for legged robots in crawl spaces isproposed. A state estimation network is designed to estimate the robot'ssurrounding ground and spatial features as well as the robot's collision statesusing historical proprioceptive sensor data. The point cloud is represented inpolar coordinate frame and a point cloud processing method is proposed toefficiently extract the ground and spatial features that are used to supervisethe state estimation network learning. Comprehensive reward functions thatguide the robot to traverse through crawl spaces after collisions are designed.Experiments demonstrate that, compared to existing methods, our method exhibitsmore agile locomotion in crawl spaces. This study enhances the ability oflegged robots to traverse spatially constrained environments without requiringexteroceptive sensors.</description>
      <author>example@mail.com (Bida Ma, Nuo Xu, Chenkun Qi, Xin Liu, Yule Mo, Jinkai Wang, Chunpeng Lu)</author>
      <guid isPermaLink="false">2508.09950v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
  <item>
      <title>RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians</title>
      <link>http://arxiv.org/abs/2508.09830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 Highlight. Shenxing and Jinxi are co-first authors. Code  and data are available at: https://github.com/vLAR-group/RayletDF&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RayletDF的可泛化方法，用于从原始点云或通过3DGS从RGB图像预估计的3D高斯函数进行3D表面重建。&lt;h4&gt;背景&lt;/h4&gt;现有的基于坐标的方法在渲染显式表面时通常计算量大，限制了3D表面重建的效率。&lt;h4&gt;目的&lt;/h4&gt;开发一种新技术，能够直接从查询射线预测表面点，从而高效地重建3D表面。&lt;h4&gt;方法&lt;/h4&gt;RayletDF方法引入了射线距离场技术，包含三个关键模块：射线特征提取器、射线距离场预测器和多射线混合器，这些组件协同工作提取几何特征、预测距离并重建精确表面点。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多个公共真实世界数据集上表现出优越的表面重建性能，具有出色的泛化能力，能够通过单次前向传递成功重建未见过的数据集的3D表面。&lt;h4&gt;结论&lt;/h4&gt;RayletDF为3D表面重建提供了一种高效且泛化能力强的解决方案，适用于多种场景和数据集。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种从原始点云或通过3DGS从RGB图像预估计的3D高斯函数进行3D表面重建的可泛化方法。与现有的在渲染显式表面时通常计算量大的基于坐标的方法不同，我们提出的方法RayletDF引入了一种称为射线距离场的新技术，旨在直接从查询射线预测表面点。我们的管道包含三个关键模块：射线特征提取器、射线距离场预测器和多射线混合器。这些组件协同工作以提取细粒度的局部几何特征，预测射线距离，并聚合多个预测以重建精确的表面点。我们在多个公共真实世界数据集上广泛评估了我们的方法，展示了在从点云或3D高斯函数进行表面重建方面的优越性能。最值得注意的是，我们的方法具有出色的泛化能力，能够在测试中通过单次前向传递成功重建未见过的数据集的3D表面。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D表面重建的泛化问题。现有方法通常需要在每个场景单独训练，难以泛化到新场景，且计算密集或难以捕捉精细表面细节。这个问题在现实世界中很重要，因为增强现实、机器人导航和数字孪生等应用需要快速适应新环境而不重新训练模型，同时保持高质量的表面重建能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有3D重建方法的局限性：基于坐标的方法计算密集，3D高斯溅射在深度渲染方面表现不佳，而基于光线的方法局限于物体级表面且缺乏泛化能力。受基于光线方法启发，作者提出了'raylet'概念（光线的单位线段），专注于表面局部几何模式。方法借鉴了光线表示和稀疏卷积网络，但创新性地设计了raylet距离场和多raylet混合器，通过学习局部表面特征实现泛化重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习局部表面的raylet距离表示实现可泛化的3D表面重建，避免传统方法中的密集采样问题。整体流程包括三个模块：1) Raylet特征提取器：使用稀疏卷积网络提取场景特征，为每个查询raylet收集附近点的位置和相对信息；2) Raylet距离场预测器：通过MLP预测raylet起点到表面的有符号距离和置信度分数；3) 多raylet混合器：沿每条光线采样多个raylet，使用置信度加权混合多个预测，得到精确的表面点位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 引入raylet距离场概念，专注于表面局部几何模式；2) 设计三个协同工作的模块实现高效重建；3) 同时支持点云和3D高斯作为输入；4) 实现了跨数据集的强泛化能力。相比之前工作，RayletDF不使用基于坐标的表示避免了密集采样问题，不局限于物体级重建，不需要每个场景单独训练，通过多raylet混合提高了鲁棒性，在未见过的数据集上表现显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RayletDF通过创新性地引入raylet距离场和多raylet混合机制，实现了从点云或3D高斯表示的高效、准确且高度可泛化的3D表面重建，显著优于现有方法在跨数据集场景重建中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a generalizable method for 3D surfacereconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS fromRGB images. Unlike existing coordinate-based methods which are oftencomputationally intensive when rendering explicit surfaces, our proposedmethod, named RayletDF, introduces a new technique called raylet distancefield, which aims to directly predict surface points from query rays. Ourpipeline consists of three key modules: a raylet feature extractor, a rayletdistance field predictor, and a multi-raylet blender. These components worktogether to extract fine-grained local geometric features, predict rayletdistances, and aggregate multiple predictions to reconstruct precise surfacepoints. We extensively evaluate our method on multiple public real-worlddatasets, demonstrating superior performance in surface reconstruction frompoint clouds or 3D Gaussians. Most notably, our method achieves exceptionalgeneralization ability, successfully recovering 3D surfaces in a single-forwardpass across unseen datasets in testing.</description>
      <author>example@mail.com (Shenxing Wei, Jinxi Li, Yafei Yang, Siyuan Zhou, Bo Yang)</author>
      <guid isPermaLink="false">2508.09830v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios</title>
      <link>http://arxiv.org/abs/2508.09470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CitySeg，一个用于城市规模点云语义分割的基础模型，结合文本模态实现开放词汇分割和零样本推理。该模型通过创新的数据预处理、网络架构和训练策略，解决了现有模型因数据规模有限和域差距导致的泛化能力不足问题，在多个基准测试上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;语义分割是无人机感知系统的关键技术，能实现不依赖视觉信息的3D点分类和全面3D理解。然而，现有模型受限于3D数据规模有限和数据集间域差距，导致泛化能力下降。&lt;h4&gt;目的&lt;/h4&gt;解决城市规模点云语义分割面临的两大挑战：多领域间数据分布不均匀导致的泛化能力有限，以及不同数据集间的语义标签不一致问题。&lt;h4&gt;方法&lt;/h4&gt;1) 提出CitySeg基础模型结合文本模态；2) 自定义数据预处理规则缓解多领域数据分布不均；3) 设计局部-全局交叉注意力网络增强点网络感知能力；4) 引入分层分类策略解决语义标签差异；5) 建立分层图整合数据标签并用图编码器建模类别关系；6) 提出两阶段训练策略并采用hinge损失增加特征可分性。&lt;h4&gt;主要发现&lt;/h4&gt;CitySeg在九个闭集基准测试上实现最先进性能，显著优于现有方法。首次实现不依赖视觉信息的城市规模点云场景下的零样本泛化。&lt;h4&gt;结论&lt;/h4&gt;CitySeg通过结合文本模态和创新的网络架构与训练策略，成功解决了城市规模点云语义分割中的泛化能力问题，实现了更广泛的场景适应性和零样本学习能力。&lt;h4&gt;翻译&lt;/h4&gt;城市规模点云的语义分割是无人机感知系统的关键技术，它能够在不依赖任何视觉信息的情况下实现3D点的分类，从而获得全面的3D理解。然而，现有模型常常受限于3D数据的有限规模以及数据集之间的域差距，这导致泛化能力下降。为了应对这些挑战，我们提出了CitySeg，这是一个用于城市规模点云语义分割的基础模型，它结合了文本模态来实现开放词汇分割和零样本推理。具体而言，为了缓解多领域间数据分布不均匀的问题，我们自定义了数据预处理规则，并提出了一种局部-全局交叉注意力网络，以增强无人机场景下点网络的感知能力。为了解决不同数据集间的语义标签差异，我们引入了一种分层分类策略。根据数据标注规则建立的分层图整合了数据标签，并使用图编码器建模类别间的分层关系。此外，我们提出了一个两阶段训练策略，并采用hinge损失来增加子类别的特征可分性。实验结果表明，我们提出的CitySeg在九个闭集基准测试上实现了最先进的性能，显著优于现有方法。更重要的是，CitySeg首次实现了不依赖视觉信息的城市规模点云场景下的零样本泛化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决城市规模场景下的3D点云语义分割问题，特别是开放词汇语义分割。这个问题很重要，因为它是无人机(UAV)感知系统的关键技术，能够对3D点进行分类而不依赖视觉信息，实现全面3D理解，对无人机在交通、物流和应急救援等领域的应用至关重要。现有方法受限于3D数据规模和不同数据集间的领域差距，泛化能力有限，且缺乏零样本推理能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有工作识别出城市规模3D语义分割的两个主要挑战：多源数据分布差异和标注不一致。他们借鉴了RandLA-Net、KPConv等点云处理网络的思想，以及视觉语言模型(如CLIP)的开放词汇理念，但针对城市规模场景进行了改进。作者创新性地设计了局部-全局交叉注意力网络增强点网络感知能力，提出层级分类策略解决标注差异，并设计两阶段训练策略提高性能。整体架构结合点云处理和文本编码，通过层级图编码器实现点云与文本的语义对齐。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合文本模态实现开放词汇分割和零样本推理，解决城市规模场景下多源点云数据的分布差异和标注不一致问题。整体流程包括：1)数据预处理标准化多域数据；2)点云处理使用局部-全局交叉注意力模块融合特征；3)文本处理构建层级图并编码类别关系；4)两阶段训练先捕获粗粒度语义再细化细粒度识别；5)支持零样本推理和增量学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)CitySeg基础模型，首个针对城市规模的3D开放词汇语义分割模型；2)局部-全局交叉注意力模块解决多源数据分布差异；3)层级分类策略处理标注不一致；4)两阶段训练策略提高性能。相比之前工作，不同之处在于：直接对齐点云与文本而非依赖2D图像中介；专门解决城市规模多源数据挑战；支持零样本推理；具备更强的跨域泛化能力，避免在单个数据集上过拟合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CitySeg通过引入局部-全局交叉注意力网络和层级分类策略，解决了城市规模场景下多源点云数据的分布差异和标注不一致问题，实现了首个支持开放词汇分割和零样本推理的3D语义分割基础模型，显著提升了城市规模3D场景理解的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of city-scale point clouds is a critical technology forUnmanned Aerial Vehicle (UAV) perception systems, enabling the classificationof 3D points without relying on any visual information to achieve comprehensive3D understanding. However, existing models are frequently constrained by thelimited scale of 3D data and the domain gap between datasets, which lead toreduced generalization capability. To address these challenges, we proposeCitySeg, a foundation model for city-scale point cloud semantic segmentationthat incorporates text modality to achieve open vocabulary segmentation andzero-shot inference. Specifically, in order to mitigate the issue ofnon-uniform data distribution across multiple domains, we customize the datapreprocessing rules, and propose a local-global cross-attention network toenhance the perception capabilities of point networks in UAV scenarios. Toresolve semantic label discrepancies across datasets, we introduce ahierarchical classification strategy. A hierarchical graph establishedaccording to the data annotation rules consolidates the data labels, and thegraph encoder is used to model the hierarchical relationships betweencategories. In addition, we propose a two-stage training strategy and employhinge loss to increase the feature separability of subcategories. Experimentalresults demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)performance on nine closed-set benchmarks, significantly outperforming existingapproaches. Moreover, for the first time, CitySeg enables zero-shotgeneralization in city-scale point cloud scenarios without relying on visualinformation.</description>
      <author>example@mail.com (Jialei Xu, Zizhuang Wei, Weikang You, Linyun Li, Weijian Sun)</author>
      <guid isPermaLink="false">2508.09470v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.09404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM Multimedia 2025 (Dataset Track) Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Waymo-3DSkelMo，这是首个提供高质量、时间连贯的3D骨骼运动并包含明确交互语义的大规模数据集，用于自动驾驶中的细粒度行人交互理解。&lt;h4&gt;背景&lt;/h4&gt;现有数据集大多依赖于从单目RGB视频帧估计3D姿态，这种方法存在遮挡和缺乏时间连续性的问题，导致产生不真实和低质量的人体运动。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D运动数据集的质量问题，提供高质量、时间连贯的3D骨骼运动数据，用于自动驾驶系统中的细粒度行人交互理解。&lt;h4&gt;方法&lt;/h4&gt;利用3D人体形状和运动先验来增强从原始LiDAR点云中提取的3D姿态序列的质量，数据集基于Waymo感知数据集构建。&lt;h4&gt;主要发现&lt;/h4&gt;数据集涵盖超过800个真实驾驶场景，总时长超过14,000秒；每个场景平均有27个智能体参与交互，最大场景包含多达250个智能体；建立了不同行人密度下的3D姿态预测基准；证明了该数据集作为未来复杂城市环境中细粒度人类行为理解研究的基础资源的价值。&lt;h4&gt;结论&lt;/h4&gt;Waymo-3DSkelMo数据集解决了现有数据集的质量问题，为自动驾驶领域提供了宝贵的资源，有助于提高系统对城市环境中人类交互行为的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;大规模高质量多人交互3D运动数据集对自动驾驶中的数据驱动模型至关重要，使其能够在动态城市环境中实现细粒度行人交互理解。然而，现有数据集大多依赖于从单目RGB视频帧估计3D姿态，这些方法存在遮挡和缺乏时间连续性的问题，从而产生不真实和低质量的人体运动。在本文中，我们介绍了Waymo-3DSkelMo，这是首个提供高质量、时间连贯的3D骨骼运动并包含明确交互语义的大规模数据集，源自Waymo感知数据集。我们的关键见解是利用3D人体形状和运动先验来增强从原始LiDAR点云中提取的3D姿态序列的质量。该数据集涵盖超过800个真实驾驶场景中的14,000多秒，包括每个场景中平均27个智能体之间的丰富交互（最大场景中有多达250个智能体）。此外，我们在不同行人密度下建立了3D姿态预测基准，结果表明其作为未来复杂城市环境中细粒度人类行为理解研究基础资源的价值。数据集和代码将在https://github.com/GuangxunZhu/Waymo-3DSkelMo上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶领域缺乏高质量大规模多人3D骨骼运动数据集的问题，特别是精细的行人交互建模。这个问题很重要，因为准确理解行人和其他交通参与者之间的交互对自动驾驶车辆的安全决策至关重要，不准确或粗糙的建模可能导致对行人未来轨迹的错误预测，从而引发安全隐患。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法从RGB图像估计3D姿态的局限性（如遮挡和时间不连续），以及直接从LiDAR点云估计3D姿态的噪声问题。他们选择了Waymo开放数据集作为基础，设计了四阶段处理流程：点云提取与融合、3D人体网格恢复、时空对齐和神经运动场增强。该方法借鉴了SMPL人体模型、LiDAR-HMR网格恢复技术、Neural Motion Field (NeMF)运动增强和Frenet frame方向校正等现有工作，但创新性地将它们结合成一个完整流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D人体形状和运动先验来增强从原始LiDAR点云提取的3D姿态序列质量，生成高质量、时间连贯的3D骨骼运动数据集。整体流程包括：1)从Waymo LiDAR数据中提取并融合点云；2)使用基于SMPL的人体形状先验进行3D人体网格恢复；3)通过时空对齐提高运动连贯性，包括插值缺失帧和采用Frenet frame处理方向问题；4)利用预训练的Neural Motion Field作为运动先验增强运动质量；5)将优化后的SMPL参数转换为骨骼运动表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出结合人体形状先验和运动先验的新处理流程；2)发布首个自动驾驶领域的大规模连续抗遮挡3D骨骼运动数据集；3)提供丰富的多智能体交互场景(800+场景，平均27个智能体/场景，最多250个)；4)建立不同行人密度下的3D姿态预测基准。相比之前的工作，不同之处在于：数据规模更大(14,000秒vs最多4,000秒)，交互更丰富(平均27个智能体vs最多6.8个)，标注更密集(243万帧级标注vs约1万手动标注)，且不依赖RGB图像估计，避免了遮挡和时间连续性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Waymo-3DSkelMo通过结合人体形状先验和神经运动场，从Waymo原始LiDAR数据生成了首个大规模、高质量、时间连贯的3D骨骼运动数据集，为自动驾驶中的精细行人交互建模提供了宝贵的资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale high-quality 3D motion datasets with multi-person interactionsare crucial for data-driven models in autonomous driving to achievefine-grained pedestrian interaction understanding in dynamic urbanenvironments. However, existing datasets mostly rely on estimating 3D posesfrom monocular RGB video frames, which suffer from occlusion and lack oftemporal continuity, thus resulting in unrealistic and low-quality humanmotion. In this paper, we introduce Waymo-3DSkelMo, the first large-scaledataset providing high-quality, temporally coherent 3D skeletal motions withexplicit interaction semantics, derived from the Waymo Perception dataset. Ourkey insight is to utilize 3D human body shape and motion priors to enhance thequality of the 3D pose sequences extracted from the raw LiDRA point clouds. Thedataset covers over 14,000 seconds across more than 800 real driving scenarios,including rich interactions among an average of 27 agents per scene (with up to250 agents in the largest scene). Furthermore, we establish 3D pose forecastingbenchmarks under varying pedestrian densities, and the results demonstrate itsvalue as a foundational resource for future research on fine-grained humanbehavior understanding in complex urban environments. The dataset and code willbe available at https://github.com/GuangxunZhu/Waymo-3DSkelMo</description>
      <author>example@mail.com (Guangxun Zhu, Shiyu Fan, Hang Dai, Edmond S. L. Ho)</author>
      <guid isPermaLink="false">2508.09404v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2508.09071v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The project is visible at https://linsun449.github.io/GeoVLA/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GeoVLA是一种新型VLA框架，通过整合3D信息提升机器人操作性能，在模拟和真实世界环境中表现出色。&lt;h4&gt;背景&lt;/h4&gt;当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界中的丰富几何信息，限制了机器人的空间感知能力和适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效集成3D信息的VLA框架，提升机器人的空间感知能力和适应性。&lt;h4&gt;方法&lt;/h4&gt;GeoVLA使用视觉语言模型处理图像和语言指令提取嵌入，将深度图转换为点云并通过点嵌入网络生成3D几何嵌入，然后通过3D增强动作专家处理这些嵌入生成精确动作序列。&lt;h4&gt;主要发现&lt;/h4&gt;GeoVLA在LIBERO和ManiSkill2模拟基准测试中取得最先进结果，在需要高度适应性、尺度感知和视角不变性的真实世界任务中表现出显著鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GeoVLA通过整合3D几何信息有效提升了机器人的操作性能和鲁棒性，为机器人操作提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-行动(VLA)模型已成为一种有前景的方法，使机器人能够遵循语言指令并预测相应动作。然而，当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界中的丰富几何信息，这限制了它们的空间感知能力和适应性。在本文中，我们提出了GeoVLA，一种新型VLA框架，可有效整合3D信息以推进机器人操作。它使用视觉语言模型(VLM)处理图像和语言指令，提取融合的视觉语言嵌入。同时，它将深度图转换为点云，并采用定制的点编码器（称为点嵌入网络）独立生成3D几何嵌入。然后将这些生成的嵌入连接起来，并通过我们提出的空间感知动作专家（称为3D增强动作专家）进行处理，该专家结合来自不同传感器模态的信息以生成精确的动作序列。通过在模拟和真实世界环境中的大量实验，GeoVLA表现出卓越的性能和鲁棒性。它在LIBERO和ManiSkill2模拟基准测试中取得了最先进的结果，并在需要高度适应性、尺度感知和视角不变性的真实世界任务中表现出显著的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 当前视觉-语言-动作（VLA）模型主要依赖2D视觉输入，忽视了3D物理世界中丰富的几何信息，这限制了机器人的空间感知和适应能力。这个问题很重要，因为真实世界是三维的，机器人需要理解3D空间关系才能进行精确操作；2D视觉缺乏深度信息导致机器人在处理高度变化、尺寸感知和视角变化等任务时表现不佳；现有方法在集成3D信息时往往破坏视觉编码器与语言模型之间的对齐，需要大量额外训练数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者希望保留预训练视觉-语言模型的知识，同时有效整合3D信息，避免破坏现有对齐。他们借鉴了视觉-语言模型架构（如Prismatic-7B）、扩散模型用于动作生成、混合专家架构处理多模态信息以及点云处理技术。创新性地设计了点嵌入网络（PEN）独立处理3D几何信息，以及3D增强的动作专家（3DAE）整合视觉和几何信息，并采用静态路由策略平衡不同模态贡献。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过并行处理视觉和几何信息，再通过专门设计的模块有效融合这两种模态，增强机器人在3D空间中的感知和操作能力。整体流程：1)双路径并行处理 - 视觉-语言路径用VLM处理图像和语言指令，几何路径将深度图转换为点云并用PEN处理；2)PEN通过双路径架构处理点云，选择末端执行器对应的标记作为锚点；3)将视觉-语言特征和3D几何特征连接；4)用3DAE处理融合特征，采用扩散模型生成动作序列；5)输出精确动作序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)双路径架构设计，分别处理视觉-语言和3D几何信息；2)点嵌入网络（PEN）专门提取与末端执行器相关的几何特征；3)3D增强的动作专家（3DAE）采用混合专家架构处理多模态信息；4)端到端训练整个系统。不同之处：相比纯2D VLA模型，GeoVLA明确集成3D几何信息，增强空间感知；相比修改视觉主干网络的3D-VLA模型，GeoVLA不破坏视觉编码器与语言模型的对齐，不需要额外大规模数据；相比注入3D信息的动作专家模型，GeoVLA采用端到端训练，允许模型适应新模态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeoVLA通过创新的点嵌入网络和3D增强的动作专家，实现了视觉-语言-动作模型中2D视觉与3D几何信息的有效融合，显著提升了机器人在复杂3D环境中的空间感知能力和操作精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have emerged as a promising approach forenabling robots to follow language instructions and predict correspondingactions. However, current VLA models mainly rely on 2D visual inputs,neglecting the rich geometric information in the 3D physical world, whichlimits their spatial awareness and adaptability. In this paper, we presentGeoVLA, a novel VLA framework that effectively integrates 3D information toadvance robotic manipulation. It uses a vision-language model (VLM) to processimages and language instructions,extracting fused vision-language embeddings.In parallel, it converts depth maps into point clouds and employs a customizedpoint encoder, called Point Embedding Network, to generate 3D geometricembeddings independently. These produced embeddings are then concatenated andprocessed by our proposed spatial-aware action expert, called 3D-enhancedAction Expert, which combines information from different sensor modalities toproduce precise action sequences. Through extensive experiments in bothsimulation and real-world environments, GeoVLA demonstrates superiorperformance and robustness. It achieves state-of-the-art results in the LIBEROand ManiSkill2 simulation benchmarks and shows remarkable robustness inreal-world tasks requiring height adaptability, scale awareness and viewpointinvariance.</description>
      <author>example@mail.com (Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, Jiale Cao)</author>
      <guid isPermaLink="false">2508.09071v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title>
      <link>http://arxiv.org/abs/2508.09922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出原型扩散模型(PDM)，将原型学习直接整合到扩散过程中，实现高效且自适应的视觉条件化图像生成，无需外部内存存储，同时保持高质量输出。&lt;h4&gt;背景&lt;/h4&gt;扩散模型已成为高质量图像生成的领先框架，但计算密集。潜在空间模型如Stable Diffusion减轻了部分计算负担但牺牲了细节。检索增强扩散模型(RDM)通过外部检索提高效率，但需要昂贵基础设施、依赖静态模型且缺乏适应性。&lt;h4&gt;目的&lt;/h4&gt;解决现有扩散模型计算密集、存储需求高、依赖静态模型且缺乏适应性的问题，开发一种高效且可扩展的图像生成方法。&lt;h4&gt;方法&lt;/h4&gt;提出原型扩散模型(PDM)，通过对比学习从清洁图像特征构建动态紧凑视觉原型集合，这些原型引导去噪步骤，通过将噪声表示与语义相关视觉模式对齐实现高效生成。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明PDM在保持高生成质量的同时，显著减少了计算和存储开销，为基于检索的条件化提供了可扩展的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PDM通过整合原型学习到扩散过程中，实现了高效且自适应的视觉条件化图像生成，无需外部内存，为扩散模型的实际应用提供了更可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型已成为高质量图像生成的领先框架，提供稳定的训练和跨多个领域的强大性能。然而，它们仍然计算密集，特别是在去噪迭代过程中。像Stable Diffusion这样的潜在空间模型通过在压缩表示上操作减轻了部分成本，但牺牲了精细细节。更接近的方法如检索增强扩散模型(RDM)通过从大型外部存储库中检索相似示例来条件化去噪，从而解决效率问题。虽然有效，但这些方法引入了缺点：它们需要昂贵的存储和检索基础设施，依赖静态的视觉-语言模型（如CLIP）进行相似度匹配，并且在训练过程中缺乏适应性。我们提出了原型扩散模型(PDM)，一种将原型学习直接整合到扩散过程中的方法，用于高效和自适应的视觉条件化 - 无需外部内存。PDM不检索参考样本，而是通过对比学习从清洁图像特征构建动态紧凑视觉原型集合。这些原型通过将噪声表示与语义相关的视觉模式对齐来指导去噪步骤，实现具有强语义基础的高效生成。实验表明，PDM在保持高生成质量的同时减少了计算和存储开销，为扩散模型中的基于检索的条件化提供了可扩展的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models have emerged as a leading framework for high-quality imagegeneration, offering stable training and strong performance across diversedomains. However, they remain computationally intensive, particularly duringthe iterative denoising process. Latent-space models like Stable Diffusionalleviate some of this cost by operating in compressed representations, thoughat the expense of fine-grained detail. More recent approaches such asRetrieval-Augmented Diffusion Models (RDM) address efficiency by conditioningdenoising on similar examples retrieved from large external memory banks. Whileeffective, these methods introduce drawbacks: they require costly storage andretrieval infrastructure, depend on static vision-language models like CLIP forsimilarity, and lack adaptability during training. We propose the PrototypeDiffusion Model (PDM), a method that integrates prototype learning directlyinto the diffusion process for efficient and adaptive visual conditioning -without external memory. Instead of retrieving reference samples, PDMconstructs a dynamic set of compact visual prototypes from clean image featuresusing contrastive learning. These prototypes guide the denoising steps byaligning noisy representations with semantically relevant visual patterns,enabling efficient generation with strong semantic grounding. Experiments showthat PDM maintains high generation quality while reducing computational andstorage overhead, offering a scalable alternative to retrieval-basedconditioning in diffusion models.</description>
      <author>example@mail.com (Bilal Faye, Hanane Azzag, Mustapha Lebbah)</author>
      <guid isPermaLink="false">2508.09922v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation</title>
      <link>http://arxiv.org/abs/2508.09860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 tables, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VIPCGRL的新型深度强化学习框架，整合文本、关卡和草图三种模态，通过四元对比学习训练的共享嵌入空间和基于嵌入相似性的辅助奖励对齐策略，提高了AI系统的类人特性，使其在协作内容创作中更好地理解和满足人类设计者的意图。&lt;h4&gt;背景&lt;/h4&gt;与人类对齐的AI是共创力的关键组成部分，它使模型能够准确解释人类意图并生成符合设计目标的可控输出。这在通过强化学习进行程序内容生成（PCGRL）的领域中尤为重要，因为PCGRL旨在作为人类设计师的工具。然而，现有系统往往缺乏人类中心行为，限制了AI驱动生成工具在实际设计工作流程中的实用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型深度强化学习框架，解决现有系统在人类中心行为方面的不足，提高AI驱动生成工具在实际设计工作流程中的实用性，使其更好地理解和满足人类设计者的意图。&lt;h4&gt;方法&lt;/h4&gt;提出VIPCGRL（Vision-Instruction PCGRL）框架，整合三种模态（文本、关卡和草图）来扩展控制模态并增强类人特性。引入通过跨模态和人类-AI风格之间的四元对比学习训练的共享嵌入空间，并使用基于嵌入相似性的辅助奖励来对齐策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，VIPCGRL在类人特性方面优于现有基线，这一结论得到了定量指标和人工评估的双重验证。代码和数据集将在发表后提供。&lt;h4&gt;结论&lt;/h4&gt;VIPCGRL框架通过整合多模态输入和创新的训练方法，成功提高了AI系统的类人特性，使其在协作内容创作中更具实用性，为AI驱动的设计工具提供了新的发展方向。&lt;h4&gt;翻译&lt;/h4&gt;与人类对齐的AI是共创力的关键组成部分，它使模型能够准确解释人类意图并在协作内容创作中生成符合设计目标的可控输出。这一方向在通过强化学习进行程序内容生成（PCGRL）的领域尤为相关，因为PCGRL旨在作为人类设计师的工具。然而，现有系统往往缺乏人类中心行为，限制了AI驱动生成工具在实际设计工作流程中的实用性。在本文中，我们提出了VIPCGRL（Vision-Instruction PCGRL），一种新型深度强化学习框架，它整合了文本、关卡和草图三种模态，以扩展控制模态并增强类人特性。我们引入了一个通过跨模态和人类-AI风格之间的四元对比学习训练的共享嵌入空间，并使用基于嵌入相似性的辅助奖励来对齐策略。实验结果表明，VIPCGRL在类人特性方面优于现有基线，这一结论得到了定量指标和人工评估的双重验证。代码和数据集将在发表后提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human-aligned AI is a critical component of co-creativity, as it enablesmodels to accurately interpret human intent and generate controllable outputsthat align with design goals in collaborative content creation. This directionis especially relevant in procedural content generation via reinforcementlearning (PCGRL), which is intended to serve as a tool for human designers.However, existing systems often fall short of exhibiting human-centeredbehavior, limiting the practical utility of AI-driven generation tools inreal-world design workflows. In this paper, we propose VIPCGRL(Vision-Instruction PCGRL), a novel deep reinforcement learning framework thatincorporates three modalities-text, level, and sketches-to extend controlmodality and enhance human-likeness. We introduce a shared embedding spacetrained via quadruple contrastive learning across modalities and human-AIstyles, and align the policy using an auxiliary reward based on embeddingsimilarity. Experimental results show that VIPCGRL outperforms existingbaselines in human-likeness, as validated by both quantitative metrics andhuman evaluations. The code and dataset will be available upon publication.</description>
      <author>example@mail.com (In-Chang Baek, Seoyoung Lee, Sung-Hyun Kim, Geumhwan Hwang, KyungJoong Kim)</author>
      <guid isPermaLink="false">2508.09860v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization</title>
      <link>http://arxiv.org/abs/2508.09560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 4figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出WeatherPrompt多模态学习方法，通过融合图像嵌入和文本上下文解决无人机视觉地理定位在恶劣天气条件下的性能下降问题，无需训练的天气推理机制和动态门控机制是两大创新点。&lt;h4&gt;背景&lt;/h4&gt;无人机视觉地理定位在雨、雾等天气扰动条件下面临严重性能下降，现有方法存在两个固有局限：严重依赖有限的天气类别限制了泛化能力，以及通过伪天气类别对纠缠的场景-天气特征进行解缠时效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在各种天气条件下（包括复杂和未见过的天气）保持高性能的无人机视觉地理定位方法，解决场景和天气特征解缠问题。&lt;h4&gt;方法&lt;/h4&gt;WeatherPrompt采用多模态学习范式，包含：1) 无需训练的天气推理机制，利用大模型合成多天气文本描述；2) 由文本嵌入驱动的动态门控机制，自适应重新加权并跨模态融合视觉特征；3) 通过图像-文本对比学习和图像-文本匹配等跨模态目标优化框架。&lt;h4&gt;主要发现&lt;/h4&gt;在多种天气条件下，该方法与最先进的无人机地理定位方法相比具有竞争力的召回率。特别在夜间条件下，Recall@1提高13.37%，在雾和雪条件下提高18.69%。&lt;h4&gt;结论&lt;/h4&gt;WeatherPrompt通过多模态学习和创新的天气推理机制，有效解决了无人机视觉地理定位在恶劣天气条件下的性能下降问题，特别是在夜间和恶劣天气条件下表现出显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;无人机视觉地理定位在雨、雾等天气扰动条件下面临严重性能下降，现有方法存在两个固有局限：1) 严重依赖有限的天气类别，限制了泛化能力；2) 通过伪天气类别对纠缠的场景-天气特征进行解缠时效果不佳。我们提出了WeatherPrompt，一种通过融合图像嵌入和文本上下文来建立天气不变表示的多模态学习范式。我们的框架引入了两个关键贡献：首先，一种无需训练的天气推理机制，使用现成的多模态大模型通过类人推理合成多天气文本描述，提高了对未见或复杂天气的可扩展性，并能反映不同的天气强度。其次，为了更好地解缠场景和天气特征，我们提出了一种由文本嵌入驱动的动态门控机制的多模态框架，自适应地重新加权并跨模态融合视觉特征。该框架通过跨模态目标（包括图像-文本对比学习和图像-文本匹配）进一步优化，使不同天气条件下的同一场景在表示空间中更接近。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual geo-localization for drones faces critical degradation under weatherperturbations, \eg, rain and fog, where existing methods struggle with twoinherent limitations: 1) Heavy reliance on limited weather categories thatconstrain generalization, and 2) Suboptimal disentanglement of entangledscene-weather features through pseudo weather categories. We presentWeatherPrompt, a multi-modality learning paradigm that establishesweather-invariant representations through fusing the image embedding with thetext context. Our framework introduces two key contributions: First, aTraining-free Weather Reasoning mechanism that employs off-the-shelf largemulti-modality models to synthesize multi-weather textual descriptions throughhuman-like reasoning. It improves the scalability to unseen or complex weather,and could reflect different weather strength. Second, to better disentangle thescene and weather feature, we propose a multi-modality framework with thedynamic gating mechanism driven by the text embedding to adaptively reweightand fuse visual features across modalities. The framework is further optimizedby the cross-modal objectives, including image-text contrastive learning andimage-text matching, which maps the same scene with different weatherconditions closer in the respresentation space. Extensive experiments validatethat, under diverse weather conditions, our method achieves competitive recallrates compared to state-of-the-art drone geo-localization methods. Notably, itimproves Recall@1 by +13.37\% under night conditions and by 18.69\% under fogand snow conditions.</description>
      <author>example@mail.com (Jiahao Wen, Hang Yu, Zhedong Zheng)</author>
      <guid isPermaLink="false">2508.09560v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Contrastive-Generative Framework for Time Series Classification</title>
      <link>http://arxiv.org/abs/2508.09451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为CoGenT的新型自监督学习框架，结合了对比学习和生成方法的优点，用于多变量时间序列分析，解决了两种方法各自的局限性，并在多个数据集上显示出显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列的自监督学习主要包括两种方法：对比方法和生成方法。对比方法在实例区分方面表现出色，生成方法则擅长建模数据分布。虽然这些方法各自有效，但它们的互补潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;探索对比学习和生成方法在多变量时间序列自监督学习中的互补潜力，提出一种统一这两种范式的框架，以克服各自的方法局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了CoGenT（Contrastive Generative Time series framework），这是第一个通过联合对比-生成优化统一对比方法和生成方法的自监督学习框架。该方法通过混合目标函数，同时优化对比学习和生成学习。&lt;h4&gt;主要发现&lt;/h4&gt;在六个不同的时间序列数据集上评估CoGenT，结果显示了持续的改进，与独立的SimCLR和MAE相比，F1分数分别提高了高达59.2%和14.27%。分析表明，混合目标保留了判别能力，同时获得了生成鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;CoGenT为时间域中的混合自监督学习奠定了基础，通过统一对比学习和生成方法，克服了各自的局限性，并显著提升了性能。代码将很快发布。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列的自监督学习主要包括两种范式：在实例区分方面表现出色的对比方法和建模数据分布的生成方法。虽然这些方法各自有效，但它们的互补潜力尚未被探索。我们提出了CoGenT（对比生成时间序列框架），这是第一个通过联合对比-生成优化统一这些范式的框架。CoGenT解决了两种方法的基本局限性：它克服了对比学习对时序数据高类内相似性的敏感性，同时减少了生成方法对大型数据集的依赖。我们在六个不同的时间序列数据集上评估了CoGenT。结果显示了持续的改进，与独立的SimCLR和MAE相比，F1分数分别提高了高达59.2%和14.27%。我们的分析表明，混合目标保留了判别能力，同时获得了生成鲁棒性。这些发现为时间域中的混合自监督学习奠定了基础。我们将很快发布代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) for multivariate time series mainly includestwo paradigms: contrastive methods that excel at instance discrimination andgenerative approaches that model data distributions. While effectiveindividually, their complementary potential remains unexplored. We propose aContrastive Generative Time series framework (CoGenT), the first framework tounify these paradigms through joint contrastive-generative optimization. CoGenTaddresses fundamental limitations of both approaches: it overcomes contrastivelearning's sensitivity to high intra-class similarity in temporal data whilereducing generative methods' dependence on large datasets. We evaluate CoGenTon six diverse time series datasets. The results show consistent improvements,with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,respectively. Our analysis reveals that the hybrid objective preservesdiscriminative power while acquiring generative robustness. These findingsestablish a foundation for hybrid SSL in temporal domains. We will release thecode shortly.</description>
      <author>example@mail.com (Ziyu Liu, Azadeh Alavi, Minyi Li, Xiang Zhang)</author>
      <guid isPermaLink="false">2508.09451v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data</title>
      <link>http://arxiv.org/abs/2508.08173v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CD-TVD框架，结合对比学习和改进的扩散模型，实现从有限高分辨率数据的3D超分辨率，减少了对大规模数据集的依赖。&lt;h4&gt;背景&lt;/h4&gt;大规模科学模拟需要大量资源生成高分辨率时变数据，而现有超分辨率方法依赖大量高分辨率训练数据，限制了其在多样化模拟场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型框架，能够从有限时间步的高分辨率数据实现准确的3D超分辨率，减少对大规模训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出CD-TVD框架，结合对比学习和改进的扩散模型；在历史模拟数据上预训练，学习高低分辨率样本的退化模式和特征；使用局部注意力机制的改进扩散模型，仅通过一个高分辨率时间步进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在流体和大气模拟数据集上的实验表明，CD-TVD能够提供准确且资源高效的3D超分辨率结果。&lt;h4&gt;结论&lt;/h4&gt;CD-TVD显著减少了大规模科学模拟中对高分辨率数据集的依赖，同时保持了恢复细粒度细节的能力，代表了科学模拟数据增强领域的重要进展。&lt;h4&gt;翻译&lt;/h4&gt;大规模科学模拟需要大量资源来生成高分辨率时变数据。虽然超分辨率是一种有效的后处理策略来降低成本，但现有方法依赖于大量高分辨率训练数据，限制了它们在不同模拟场景中的适用性。为了解决这一限制，我们提出了CD-TVD，一个结合对比学习和改进的基于扩散的超分辨率模型的新型框架，从有限时间步的高分辨率数据实现准确的3D超分辨率。在历史模拟数据上的预训练期间，对比编码器和扩散超分辨率模块学习高分辨率和低分辨率样本的退化模式和详细特征。在训练阶段，使用局部注意力机制的改进扩散模型仅通过一个新生成的高分辨率时间步进行微调，利用编码器学习到的退化知识。这种设计最小化了对大规模高分辨率数据集的依赖，同时保持了恢复细粒度细节的能力。在流体和大气模拟数据集上的实验结果证实，CD-TVD提供了准确且资源高效的3D超分辨率，标志着大规模科学模拟数据增强的重要进展。代码可在https://github.com/Xin-Gao-private/CD-TVD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D超分辨率任务中高分辨率时间变化数据稀缺的问题。这个问题非常重要，因为大规模科学模拟需要巨大计算资源生成高分辨率数据，直接进行高分辨率模拟面临计算成本指数增长和数据存储需求激增的瓶颈，而传统低精度计算又无法捕捉微观结构的演化细节或关键状态中的突变特征。现有超分辨率方法依赖大量高分辨率训练数据，限制了它们在不同模拟场景中的适用性，且科学数据具有多物理场耦合、非线性时空演化和严格守恒定律等独特特性，使得简单数据增强技术难以应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有超分辨率方法在科学模拟中的局限性，然后借鉴了对比学习思想来建模高分辨率和低分辨率数据间的退化模式，同时采用扩散模型作为超分辨率基础架构，因为扩散模型能很好地处理复杂模式和细微纹理。作者还引入了局部注意力机制减少计算成本，并在对比编码模块训练中使用对抗学习。这些现有技术的创新组合形成了CD-TVD方法，特别设计了两阶段训练流程：先用历史模拟数据预训练，再用少量高分辨率样本针对新场景微调。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将高分辨率和低分辨率数据间的退化过程建模为对比学习任务，结合对比学习和改进的扩散模型，实现从有限高分辨率时间步数据的准确3D超分辨率。整体流程分为两阶段：1)预训练阶段使用历史模拟数据同时训练对比编码模块(学习退化模式)和扩散超分辨率模块(学习细节恢复)，通过对抗训练联合优化；2)微调阶段冻结对比编码模块，保留先验知识，使用少量高分辨率样本(基于熵选择最具代表性的时间步)微调扩散超分辨率模块。推理时，微调后的模型能准确重建所有低分辨率时间步的高分辨率版本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将HR-LR退化过程明确建模为对比学习任务，提取强判别性退化特征；2)设计局部注意力增强的扩散架构，减轻计算负担同时恢复细粒度细节；3)利用预训练的通用退化模式，只需一个HR时间步即可重建所有后续LR时间步。相比之前工作，CD-TVD在数据效率上显著提升(仅需少量HR数据而非大量成对数据)，架构上创新结合对比学习和扩散模型而非主要使用GAN，训练策略上采用两阶段流程而非端到端训练，且能更好地处理科学数据的独特特性，在各种模拟数据集上表现出更强的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CD-TVD通过结合对比学习和改进的扩散模型，实现了从有限高分辨率时间步数据的准确3D超分辨率，显著减少了对大规模高分辨率数据的依赖，为资源受限的科学模拟提供了高效的数据增强解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale scientific simulations require significant resources to generatehigh-resolution time-varying data (TVD). While super-resolution is an efficientpost-processing strategy to reduce costs, existing methods rely on a largeamount of HR training data, limiting their applicability to diverse simulationscenarios. To address this constraint, we proposed CD-TVD, a novel frameworkthat combines contrastive learning and an improved diffusion-basedsuper-resolution model to achieve accurate 3D super-resolution from limitedtime-step high-resolution data. During pre-training on historical simulationdata, the contrastive encoder and diffusion superresolution modules learndegradation patterns and detailed features of high-resolution andlow-resolution samples. In the training phase, the improved diffusion modelwith a local attention mechanism is fine-tuned using only one newly generatedhigh-resolution timestep, leveraging the degradation knowledge learned by theencoder. This design minimizes the reliance on large-scale high-resolutiondatasets while maintaining the capability to recover fine-grained details.Experimental results on fluid and atmospheric simulation datasets confirm thatCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking asignificant advancement in data augmentation for large-scale scientificsimulations. The code is available athttps://github.com/Xin-Gao-private/CD-TVD.</description>
      <author>example@mail.com (Chongke Bi, Xin Gao, Jiangkang Deng, Guan Li, Jun Han)</author>
      <guid isPermaLink="false">2508.08173v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering</title>
      <link>http://arxiv.org/abs/2508.09180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了scAGC，一种用于单细胞RNA测序数据聚类的新方法，通过学习自适应细胞图和对比引导来提高细胞类型注释的准确性。&lt;h4&gt;背景&lt;/h4&gt;单细胞RNA测序数据分析中，准确的细胞类型注释是关键步骤，但传统方法面临高维度和大量零元素的挑战，而现有图神经网络方法依赖静态图结构，难以处理噪声和长尾分布问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理scRNA-seq数据特性的聚类方法，解决传统方法面临的统计和计算挑战，提高细胞类型注释的准确性。&lt;h4&gt;方法&lt;/h4&gt;scAGC方法结合了拓扑自适应图自编码器、可区分的Gumbel-Softmax采样策略、零膨胀负二项式损失函数和对比学习目标，以端到端方式同时优化特征表示和细胞图结构。&lt;h4&gt;主要发现&lt;/h4&gt;scAGC在9个真实scRNA-seq数据集上的实验结果表明，它持续优于其他最先进的方法，在9个和7个数据集上分别获得了最佳NMI和ARI分数。&lt;h4&gt;结论&lt;/h4&gt;scAGC通过自适应图结构和对比学习有效解决了scRNA-seq数据聚类中的挑战，提供了更准确和稳定的细胞类型注释结果。&lt;h4&gt;翻译&lt;/h4&gt;准确的细胞类型注释是分析单细胞RNA测序数据的关键步骤，它为细胞异质性提供了有价值的见解。然而，由于scRNA-seq数据的高维度和零元素普遍存在，传统聚类方法面临着显著的统计和计算挑战。虽然一些先进的方法使用图神经网络来建模细胞间关系，但它们通常依赖于对噪声敏感的静态图结构，并且无法捕捉单细胞群体中固有的长尾分布。为解决这些局限性，我们提出了scAGC，一种学习自适应细胞图并带有对比引导的单细胞聚类方法。我们的方法以端到端方式同时优化特征表示和细胞图。具体来说，我们引入了一种拓扑自适应图自编码器，它利用可区分的Gumbel-Softmax采样策略在训练过程中动态优化图结构。这种自适应机制通过促进更平衡的邻域结构来缓解长尾度分布问题。为了建模scRNA-seq数据的离散、过度离散和零膨胀特性，我们整合了零膨胀负二项式损失以实现稳健的特征重建。此外，还加入了对比学习目标来规范图学习过程，防止图拓扑的突然变化，确保稳定性和提高收敛性。在9个真实scRNA-seq数据集上的综合实验表明，scAGC持续优于其他最先进的方法，在9个和7个数据集上分别获得了最佳NMI和ARI分数。我们的代码可在匿名Github上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate cell type annotation is a crucial step in analyzing single-cell RNAsequencing (scRNA-seq) data, which provides valuable insights into cellularheterogeneity. However, due to the high dimensionality and prevalence of zeroelements in scRNA-seq data, traditional clustering methods face significantstatistical and computational challenges. While some advanced methods use graphneural networks to model cell-cell relationships, they often depend on staticgraph structures that are sensitive to noise and fail to capture thelong-tailed distribution inherent in single-cell populations.To address theselimitations, we propose scAGC, a single-cell clustering method that learnsadaptive cell graphs with contrastive guidance. Our approach optimizes featurerepresentations and cell graphs simultaneously in an end-to-end manner.Specifically, we introduce a topology-adaptive graph autoencoder that leveragesa differentiable Gumbel-Softmax sampling strategy to dynamically refine thegraph structure during training. This adaptive mechanism mitigates the problemof a long-tailed degree distribution by promoting a more balanced neighborhoodstructure. To model the discrete, over-dispersed, and zero-inflated nature ofscRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss forrobust feature reconstruction. Furthermore, a contrastive learning objective isincorporated to regularize the graph learning process and prevent abruptchanges in the graph topology, ensuring stability and enhancing convergence.Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGCconsistently outperforms other state-of-the-art methods, yielding the best NMIand ARI scores on 9 and 7 datasets, respectively.Our code is available atAnonymous Github.</description>
      <author>example@mail.com (Huifa Li, Jie Fu, Xinlin Zhuang, Haolin Yang, Xinpeng Ling, Tong Cheng, Haochen xue, Imran Razzak, Zhili Chen)</author>
      <guid isPermaLink="false">2508.09180v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Mixture-of-Experts for Incremental Graph Learning</title>
      <link>http://arxiv.org/abs/2508.09974v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图增量学习是一种使模型能够适应不断增加的图和数据的学习范式，但常规方法会遭遇灾难性遗忘问题。本文提出动态混合专家(DyMoE)方法，添加新专家网络并设计定制正则化损失，同时采用稀疏MoE减少计算成本，在类别增量学习中实现4.92%的相对准确率提升。&lt;h4&gt;背景&lt;/h4&gt;图增量学习旨在使训练好的模型随时间适应不断增加的图和数据，无需在完整数据集上重新训练。常规图机器学习方法在增量学习环境中会遭遇灾难性遗忘问题，之前学习知识被新知识覆盖。先前方法未考虑不同时间点获取的知识对学习新任务有不同贡献。&lt;h4&gt;目的&lt;/h4&gt;解决图增量学习中的灾难性遗忘问题，区分哪些旧知识可以迁移到新任务学习，哪些可能偏离新数据分布并产生负面影响，使模型能够有效适应不断增长的数据。&lt;h4&gt;方法&lt;/h4&gt;提出动态混合专家(DyMoE)方法：1) DyMoE GNN层添加新专家网络专门建模传入数据块；2) 设计定制正则化损失利用数据序列信息，使现有专家保持解决旧任务能力同时帮助新专家学习；3) 引入稀疏MoE方法，仅让top-k最相关专家进行预测，减少计算时间。&lt;h4&gt;主要发现&lt;/h4&gt;不同时间点获取的知识对学习新任务贡献不同；一些先前模式可转移到新数据学习，其他可能偏离新数据分布并产生负面影响；稀疏MoE方法显著减少计算成本。&lt;h4&gt;结论&lt;/h4&gt;DyMoE模型在类别增量学习方面比最佳基线提高4.92%相对准确率，显示了模型的卓越能力和实用性。&lt;h4&gt;翻译&lt;/h4&gt;图增量学习是一种学习范式，旨在使训练好的模型能够随时间适应不断增加的图和数据，而无需在完整数据集上重新训练。然而，常规的图机器学习方法在增量学习环境中会遭遇灾难性遗忘问题，即之前学习的知识会被新知识覆盖。先前的方法将之前训练的模型视为不可分割的单位，使用技术在学习新知识的同时保持旧行为。但这些方法没有考虑不同时间点获取的知识对学习新任务有不同的贡献。一些先前的模式可以转移到新数据学习中，而其他可能偏离新数据分布并产生负面影响。为解决此问题，我们提出了一种动态混合专家(DyMoE)方法用于增量学习。具体来说，DyMoE GNN层添加了新的专家网络，专门用于建模传入的数据块。我们设计了定制的正则化损失，利用数据序列信息，使现有专家能够保持解决旧任务的能力，同时帮助新专家有效学习新数据。随着数据块数量随时间增长，完整混合专家模型(MoE)的计算成本增加。为解决此问题，我们引入了稀疏MoE方法，只有top-k最相关的专家进行预测，显著减少计算时间。我们的模型在类别增量学习方面比最佳基线提高了4.92%的相对准确率，显示了模型的卓越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph incremental learning is a learning paradigm that aims to adapt trainedmodels to continuously incremented graphs and data over time without the needfor retraining on the full dataset. However, regular graph machine learningmethods suffer from catastrophic forgetting when applied to incrementallearning settings, where previously learned knowledge is overridden by newknowledge. Previous approaches have tried to address this by treating thepreviously trained model as an inseparable unit and using techniques tomaintain old behaviors while learning new knowledge. These approaches, however,do not account for the fact that previously acquired knowledge at differenttimestamps contributes differently to learning new tasks. Some prior patternscan be transferred to help learn new data, while others may deviate from thenew data distribution and be detrimental. To address this, we propose a dynamicmixture-of-experts (DyMoE) approach for incremental learning. Specifically, aDyMoE GNN layer adds new expert networks specialized in modeling the incomingdata blocks. We design a customized regularization loss that utilizes datasequence information so existing experts can maintain their ability to solveold tasks while helping the new expert learn the new data effectively. As thenumber of data blocks grows over time, the computational cost of the fullmixture-of-experts (MoE) model increases. To address this, we introduce asparse MoE approach, where only the top-$k$ most relevant experts makepredictions, significantly reducing the computation time. Our model achieved4.92\% relative accuracy increase compared to the best baselines on classincremental learning, showing the model's exceptional power.</description>
      <author>example@mail.com (Lecheng Kong, Theodore Vasiloudis, Seongjun Yun, Han Xie, Xiang Song)</author>
      <guid isPermaLink="false">2508.09974v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment</title>
      <link>http://arxiv.org/abs/2508.09843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的全方向图像质量评估方法，通过建模视口间的结构关系来增强对空间失真非均匀性的感知，实验证明该方法显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;当前的全方向图像质量评估方法难以评估局部非均匀失真，原因是它们对空间质量变化建模不足，且无法有效捕捉局部细节和全局上下文特征。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于图神经网络的OIQA框架，明确建模视口之间的结构关系，增强对空间失真非均匀性的感知。&lt;h4&gt;方法&lt;/h4&gt;使用斐波那契球采样生成具有良好拓扑结构的视口，将每个视口表示为图节点，使用多阶段特征提取网络推导高维节点表示，集成图注意力网络(GAT)建模相邻视口间的细粒度局部失真变化，使用图变换器捕获远距离区域间的长程质量相互作用。&lt;h4&gt;主要发现&lt;/h4&gt;在两个具有复杂空间失真的大规模OIQA数据库上进行的大量实验表明，该方法显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该方法有效且具有强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;当前的全方向图像质量评估(OIQA)方法由于对空间质量变化建模不足且无法有效捕捉局部细节和全局上下文特征，难以评估局部非均匀失真。为此，我们提出了一种基于图神经网络的OIQA框架，通过明确建模视口之间的结构关系来增强对空间失真非均匀性的感知。我们的方法采用斐波那契球采样生成具有良好拓扑结构的视口，并将每个视口表示为图节点。然后，多阶段特征提取网络推导出高维节点表示。为了全面捕捉空间依赖关系，我们集成了图注意力网络(GAT)来建模相邻视口间的细粒度局部失真变化，以及一个图变换器来捕获远距离区域间的长程质量相互作用。在两个具有复杂空间失真的大规模OIQA数据库上进行的大量实验表明，我们的方法显著优于现有方法，证实了其有效性和强大的泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决全景图像质量评估方法难以评估局部不均匀失真的问题。这个问题在现实中很重要，因为360度全景图像在虚拟现实、增强现实和元宇宙等应用中越来越普及，这些图像在采集、拼接、压缩过程中容易产生复杂的空间失真，而现有方法无法有效捕捉这些非均匀分布的质量变化，影响了用户体验和系统性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到全景图像的复杂空间结构给质量评估带来挑战，特别是非均匀失真难以评估。他们借鉴了图神经网络的思想，设计了一个结合图注意力网络和图变换器的框架。具体来说，他们采用了斐波那契球面采样来均匀分布视口，使用Swin Transformer提取特征，并构建图结构来建模视口之间的关系。这种方法融合了计算机视觉和图神经网络领域的现有技术，但进行了创新性组合以适应全景图像质量评估的特殊需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将全景图像视为图结构，其中每个节点代表一个从球面均匀采样的视口，通过图神经网络同时建模局部和全局的空间依赖关系。整体流程包括：1)使用斐波那契球面采样提取空间分布均匀的视口；2)用Swin Transformer提取每个视口的多尺度特征；3)添加球形位置编码；4)基于Haversine距离构建视口邻接图；5)用图注意力网络建模局部质量依赖；6)用图变换器捕获长程质量依赖；7)融合特征预测整体质量分数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)采用斐波那契球面采样获得空间均匀的视口；2)构建球形视口邻接图并应用GAT建模局部交互；3)设计图变换器架构捕获跨区域的全局质量依赖；4)联合建模局部失真敏感性和全局结构一致性。相比之前工作，传统方法直接应用2D评估方法到全景图像投影，忽略了人类视觉系统的视口感知机制；基于整个图像的方法受极地区域拉伸失真影响；基于视口的方法虽然模拟人类观看行为，但对非均匀失真建模不足。本文通过图神经网络同时建模局部和全局空间依赖，有效解决了这些问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于层次图注意力网络的全景图像无参考质量评估方法，通过联合建模局部和全局的空间依赖关系，有效解决了全景图像中非均匀失真的评估问题，显著提升了预测精度和跨数据集泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Omnidirectional Image Quality Assessment (OIQA) methods struggle toevaluate locally non-uniform distortions due to inadequate modeling of spatialvariations in quality and ineffective feature representation capturing bothlocal details and global context. To address this, we propose a graph neuralnetwork-based OIQA framework that explicitly models structural relationshipsbetween viewports to enhance perception of spatial distortion non-uniformity.Our approach employs Fibonacci sphere sampling to generate viewports withwell-structured topology, representing each as a graph node. Multi-stagefeature extraction networks then derive high-dimensional node representation.To holistically capture spatial dependencies, we integrate a Graph AttentionNetwork (GAT) modeling fine-grained local distortion variations among adjacentviewports, and a graph transformer capturing long-range quality interactionsacross distant regions. Extensive experiments on two large-scale OIQA databaseswith complex spatial distortions demonstrate that our method significantlyoutperforms existing approaches, confirming its effectiveness and stronggeneralization capability.</description>
      <author>example@mail.com (Hao Yang, Xu Zhang, Jiaqi Ma, Linwei Zhu, Yun Zhang, Huan Zhang)</author>
      <guid isPermaLink="false">2508.09843v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Explainable Ensemble Learning for Graph-Based Malware Detection</title>
      <link>http://arxiv.org/abs/2508.09801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图的恶意软件检测和解释的堆叠集成框架，通过多样化GNN基础学习器和基于注意力的元学习器提高分类性能并提供可解释性。&lt;h4&gt;背景&lt;/h4&gt;现代计算环境中的恶意软件检测需要准确、可解释且能抵抗规避技术的模型。图神经网络在基于图的程序表示建模方面显示出潜力，但单一模型方法可能存在泛化能力有限和缺乏可解释性的问题，特别是在高风险安全应用中。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于图的恶意软件检测和解释框架，以提高分类性能并提供对恶意软件行为的可解释解释。&lt;h4&gt;方法&lt;/h4&gt;动态提取PE文件的CFG并使用两步嵌入策略编码基本块；使用具有不同消息传递机制的多样化GNN基础学习器捕获互补行为特征；通过基于注意力的多层感知器元学习器聚合预测输出并量化各基础模型的贡献；引入集成感知的后解释技术，利用GNN解释器生成的边级重要性分数和学习到的注意力权重产生可解释的解释。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该框架提高了恶意软件分类性能，同时提供了对恶意软件行为的有意义的解释。&lt;h4&gt;结论&lt;/h4&gt;所提出的堆叠集成框架在提高分类性能的同时，能够提供对恶意软件行为的可解释性，适用于高风险安全应用。&lt;h4&gt;翻译&lt;/h4&gt;现代计算环境中的恶意软件检测需要不仅准确而且可解释且能抵抗规避技术的模型。图神经网络通过建模基于图的程序表示（如控制流图CFG）中的丰富结构依赖关系在该领域显示出潜力。然而，单一模型方法可能存在泛化能力有限和缺乏可解释性的问题，特别是在高风险安全应用中。在本文中，我们提出了一种用于基于图的恶意软件检测和解释的新型堆叠集成框架。我们的方法从可移植可执行文件（PE）动态提取CFG，并通过两步嵌入策略对其基本块进行编码。使用一组具有不同消息传递机制的多样化GNN基础学习器来捕获互补的行为特征。它们的预测输出由一个元学习器聚合，该元学习器实现为基于注意力的多层感知器，既能分类恶意软件实例又能量化每个基础模型的贡献。为了增强可解释性，我们引入了一种集成感知的后解释技术，利用GNN解释器生成的边级重要性分数，并使用学习到的注意力权重融合它们。这产生了与最终集成决策一致的可解释的、模型不可知的解释。实验结果表明，我们的框架提高了分类性能，同时提供了对恶意软件行为的有意义的解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malware detection in modern computing environments demands models that arenot only accurate but also interpretable and robust to evasive techniques.Graph neural networks (GNNs) have shown promise in this domain by modeling richstructural dependencies in graph-based program representations such as controlflow graphs (CFGs). However, single-model approaches may suffer from limitedgeneralization and lack interpretability, especially in high-stakes securityapplications. In this paper, we propose a novel stacking ensemble framework forgraph-based malware detection and explanation. Our method dynamically extractsCFGs from portable executable (PE) files and encodes their basic blocks througha two-step embedding strategy. A set of diverse GNN base learners, each with adistinct message-passing mechanism, is used to capture complementary behavioralfeatures. Their prediction outputs are aggregated by a meta-learner implementedas an attention-based multilayer perceptron, which both classifies malwareinstances and quantifies the contribution of each base model. To enhanceexplainability, we introduce an ensemble-aware post-hoc explanation techniquethat leverages edge-level importance scores generated by a GNN explainer andfuses them using the learned attention weights. This produces interpretable,model-agnostic explanations aligned with the final ensemble decision.Experimental results demonstrate that our framework improves classificationperformance while providing insightful interpretations of malware behavior.</description>
      <author>example@mail.com (Hossein Shokouhinejad, Roozbeh Razavi-Far, Griffin Higgins, Ali A Ghorbani)</author>
      <guid isPermaLink="false">2508.09801v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring</title>
      <link>http://arxiv.org/abs/2508.09527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的、可解释的图神经网络框架，用于预测性业务流程监控(PBPM)，解决了现有模型在架构、时间和语义方面的不足。&lt;h4&gt;背景&lt;/h4&gt;预测性业务流程监控(PBPM)旨在基于历史事件日志预测正在进行案例中的未来事件。尽管图神经网络(GNN)非常适合捕捉流程数据中的结构依赖关系，但现有的基于GNN的PBPM模型仍然发展不完善。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的、可解释的GNN框架，通过三个关键方面提升PBPM技术的现状：比较局部与全局建模方法、引入时间衰减注意力机制、嵌入转换类型语义。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一个包含多级可解释性模块的GNN架构，包括：1)比较基于前缀的图卷积网络(GCNs)和完整轨迹图注意力网络(GATs)；2)引入新的时间衰减注意力机制，构建动态的、以预测为中心的窗口；3)将转换类型语义嵌入边特征，以支持对结构模糊轨迹的细粒度推理。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在五个基准测试上取得了具有竞争力的Top-k准确率和DL分数，无需针对每个数据集进行调整。通过解决架构、时间和语义方面的差距，该工作为PBPM中的下一个事件预测提供了稳健、通用且可解释的解决方案。&lt;h4&gt;结论&lt;/h4&gt;该研究通过创新的GNN框架，解决了现有PBPM模型在处理局部与全局建模、时间相关性和转换语义方面的局限性，为业务流程预测提供了一个更强大、更通用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;预测性业务流程监控(PBPM)旨在基于历史事件日志预测正在进行案例中的未来事件。尽管图神经网络(GNN)非常适合捕捉流程数据中的结构依赖关系，但现有的基于GNN的PBPM模型仍然发展不完善。大多数模型要么依赖于短前缀子图，要么使用忽略时间相关性和转换语义的全局架构。我们提出了一个统一的、可解释的GNN框架，在三个关键方面提升了最先进的技术水平。首先，我们比较了基于前缀的图卷积网络(GCNs)和完整轨迹图注意力网络(GATs)，以量化局部建模和全局建模之间的性能差距。其次，我们引入了一种新颖的时间衰减注意力机制，构建动态的、以预测为中心的窗口，强调时间相关的历史信息并抑制噪声。第三，我们将转换类型语义嵌入边特征，以便对结构模糊的轨迹进行细粒度推理。我们的架构包含多级可解释性模块，提供对注意力行为的多样化可视化。在五个基准测试上评估，所提出的模型取得了具有竞争力的Top-k准确率和DL分数，无需针对每个数据集进行调整。通过解决架构、时间和语义方面的差距，这项工作为PBPM中的下一个事件预测提供了一个稳健、通用且可解释的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive Business Process Monitoring (PBPM) aims to forecast future eventsin ongoing cases based on historical event logs. While Graph Neural Networks(GNNs) are well suited to capture structural dependencies in process data,existing GNN-based PBPM models remain underdeveloped. Most rely either on shortprefix subgraphs or global architectures that overlook temporal relevance andtransition semantics. We propose a unified, interpretable GNN framework thatadvances the state of the art along three key axes. First, we compareprefix-based Graph Convolutional Networks(GCNs) and full trace Graph AttentionNetworks(GATs) to quantify the performance gap between localized and globalmodeling. Second, we introduce a novel time decay attention mechanism thatconstructs dynamic, prediction-centered windows, emphasizing temporallyrelevant history and suppressing noise. Third, we embed transition typesemantics into edge features to enable fine grained reasoning over structurallyambiguous traces. Our architecture includes multilevel interpretabilitymodules, offering diverse visualizations of attention behavior. Evaluated onfive benchmarks, the proposed models achieve competitive Top-k accuracy and DLscores without per-dataset tuning. By addressing architectural, temporal, andsemantic gaps, this work presents a robust, generalizable, and explainablesolution for next event prediction in PBPM.</description>
      <author>example@mail.com (Fang Wang, Ernesto Damiani)</author>
      <guid isPermaLink="false">2508.09527v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery</title>
      <link>http://arxiv.org/abs/2508.09401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对分布式后端服务系统的无监督异常检测方法，通过构建动态图和应用图卷积提取结构特征，使用Transformer建模时间行为，并通过联合嵌入机制整合特征，实现端到端的异常检测。&lt;h4&gt;背景&lt;/h4&gt;分布式后端服务系统面临复杂结构依赖、多样化行为演变和缺少标记数据等实际挑战，使得异常检测变得困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标记数据的有效异常检测方法，能够捕捉系统中的异常传播路径和动态行为序列。&lt;h4&gt;方法&lt;/h4&gt;构建基于服务调用关系的动态图，应用图卷积提取高阶结构表示；使用Transformer建模节点时间行为；通过可学习的联合嵌入机制整合结构和行为特征；应用非线性映射计算异常分数，实现端到端检测。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多个关键指标上优于现有模型，在捕捉异常传播路径和建模动态行为序列方面表现出更强的表达能力和稳定性，对图深度、序列长度和数据扰动具有良好的适应性。&lt;h4&gt;结论&lt;/h4&gt;该方法具有实际部署的高潜力，能够有效解决分布式后端服务系统中的异常检测问题，无需标记数据，适用于实际场景。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种针对分布式后端服务系统的无监督异常检测方法，解决了复杂结构依赖、多样化行为演变和缺少标记数据等实际挑战。该方法基于服务调用关系构建动态图，应用图卷积从多跳拓扑结构中提取高阶结构表示。使用Transformer建模每个节点的时间行为，捕捉长期依赖和局部波动。在特征融合阶段，通过可学习的联合嵌入机制将结构表示和行为表示整合为统一的异常向量。然后应用非线性映射计算异常分数，实现无需监督的端到端检测过程。在真实云监控数据上的实验包括对不同图深度、序列长度和数据扰动的敏感性分析。结果表明，该方法在几个关键指标上优于现有模型，在捕捉异常传播路径和建模动态行为序列方面表现出更强的表达能力和稳定性，具有实际部署的高潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study proposes an unsupervised anomaly detection method for distributedbackend service systems, addressing practical challenges such as complexstructural dependencies, diverse behavioral evolution, and the absence oflabeled data. The method constructs a dynamic graph based on service invocationrelationships and applies graph convolution to extract high-order structuralrepresentations from multi-hop topologies. A Transformer is used to model thetemporal behavior of each node, capturing long-term dependencies and localfluctuations. During the feature fusion stage, a learnable joint embeddingmechanism integrates structural and behavioral representations into a unifiedanomaly vector. A nonlinear mapping is then applied to compute anomaly scores,enabling an end-to-end detection process without supervision. Experiments onreal-world cloud monitoring data include sensitivity analyses across differentgraph depths, sequence lengths, and data perturbations. Results show that theproposed method outperforms existing models on several key metrics,demonstrating stronger expressiveness and stability in capturing anomalypropagation paths and modeling dynamic behavior sequences, with high potentialfor practical deployment.</description>
      <author>example@mail.com (Yun Zi, Ming Gong, Zhihao Xue, Yujun Zou, Nia Qi, Yingnan Deng)</author>
      <guid isPermaLink="false">2508.09401v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Exact Verification of Graph Neural Networks with Incremental Constraint Solving</title>
      <link>http://arxiv.org/abs/2508.09320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GNNev的精确验证方法，用于计算图神经网络在属性和结构扰动下的鲁棒性保证，支持三种聚合函数(sum、max和mean)，并在多个数据集上验证了其有效性和优越性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络越来越多地应用于高风险领域如欺诈检测和医疗保健，但容易受到对抗性攻击。虽然已有一些技术提供对抗性鲁棒性保证，但支持消息传递GNNs中常用聚合函数的方法仍然缺乏。&lt;h4&gt;目的&lt;/h4&gt;开发一种精确的验证方法，用于计算图神经网络在属性和结构扰动(包括边的添加或删除)下的保证，这些扰动受到预算限制的约束，特别关注节点分类任务。&lt;h4&gt;方法&lt;/h4&gt;该方法使用约束求解和边界紧缩，迭代解决一系列松弛的约束满足问题，并利用求解器的增量求解能力来提高效率。实现了GNNev求解器，支持三种聚合函数(sum、max和mean)，其中后两种是首次在此类方法中考虑。&lt;h4&gt;主要发现&lt;/h4&gt;在Cora和CiteSeer标准基准数据集以及Amazon和Yelp真实世界欺诈数据集上的实验评估，证明了GNNev的可用性和有效性，以及在sum聚合节点分类任务上比现有精确验证工具具有更好的性能。&lt;h4&gt;结论&lt;/h4&gt;GNNev为消息传递神经网络提供了一个有效的验证框架，支持多种聚合函数，能够在高风险应用中提供对抗性鲁棒性保证。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)越来越多地应用于高风险应用中，如欺诈检测或医疗保健，但容易受到对抗性攻击。已经提出了一些技术来提供对抗性鲁棒性保证，但对于消息传递GNNs中常用的聚合函数的支持仍然缺乏。在本文中，我们开发了一种精确的(可靠且完整的)GNN验证方法，用于计算在属性和结构扰动下的保证，这些扰动涉及边的添加或删除，并受到预算限制的约束。专注于节点分类任务，我们的方法使用约束求解和边界紧缩，迭代解决一系列松弛的约束满足问题，同时依赖求解器的增量求解能力来提高效率。我们实现了GNNev，这是一个用于消息传递神经网络的多功能求解器，支持三种聚合函数：sum、max和mean，其中后两种是首次在此类方法中考虑。在两个标准基准(Cora和CiteSeer)和两个真实世界的欺诈数据集(Amazon和Yelp)上对GNNev进行的广泛实验评估，证明了其可用性和有效性，以及在sum聚合节点分类任务上比现有精确验证工具具有更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are increasingly employed in high-stakesapplications, such as fraud detection or healthcare, but are susceptible toadversarial attacks. A number of techniques have been proposed to provideadversarial robustness guarantees, but support for commonly used aggregationfunctions in message-passing GNNs is still lacking. In this paper, we developan exact (sound and complete) verification method for GNNs to computeguarantees against attribute and structural perturbations that involve edgeaddition or deletion, subject to budget constraints. Focusing on nodeclassification tasks, our method employs constraint solving with boundtightening, and iteratively solves a sequence of relaxed constraintsatisfaction problems while relying on incremental solving capabilities ofsolvers to improve efficiency. We implement GNNev, a versatile solver formessage-passing neural networks, which supports three aggregation functions,sum, max and mean, with the latter two considered here for the first time.Extensive experimental evaluation of GNNev on two standard benchmarks (Cora andCiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates itsusability and effectiveness, as well as superior performance compared toexisting {exact verification} tools on sum-aggregated node classificationtasks.</description>
      <author>example@mail.com (Minghao Liu, Chia-Hsuan Lu, Marta Kwiatkowska)</author>
      <guid isPermaLink="false">2508.09320v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Over-Squashing in GNNs and Causal Inference of Rewiring Strategies</title>
      <link>http://arxiv.org/abs/2508.09265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种评估图神经网络中过压缩问题的方法，并分析了重连接技术对缓解这一问题的效果，发现重连接在图分类任务中通常有益，但在节点分类任务中可能增加过压缩问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在推荐系统、材料设计和药物重定位等领域表现出最先进的性能。然而，消息传递GNNs存在过压缩问题，即来自远程节点的长程信息被指数级压缩，这限制了其表达能力。重连接技术可以缓解这一瓶颈，但由于缺乏直接的过压缩经验指标，其实际影响尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;提出一种严格的、以拓扑为中心的方法来评估节点对之间的过压缩问题，并研究重连接策略如何影响过压缩问题，以及这种影响如何转化为性能提升。&lt;h4&gt;方法&lt;/h4&gt;使用节点对之间相互敏感性的衰减率来评估过压缩问题，并将这些成对评估扩展到四个图级统计量(普遍性、强度、变异性、极端性)。将这些指标与图内因果设计相结合，量化了重连接策略如何影响不同图和节点分类基准测试中的过压缩问题。&lt;h4&gt;主要发现&lt;/h4&gt;大多数图分类数据集都受到过压缩问题的影响（但程度各异）；重连接有效地缓解了这一问题，尽管缓解程度及其转化为性能提升的程度因数据集和方法而异；在节点分类数据集中，过压缩问题不太明显，重连接往往会增加过压缩，且性能变化与过压缩变化无关；当过压缩既显著又得到适度纠正时，重连接最有益；过于激进的重连接或应用于最小过压缩图的重连接不太可能有所帮助，甚至可能损害性能。&lt;h4&gt;结论&lt;/h4&gt;论文提出的即插即用诊断工具可以让从业者在任何训练之前确定重连接是否可能有效。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已在推荐系统、材料设计和药物重定位等广泛领域展现出最先进的性能。然而，消息传递GNNs遭受过压缩问题——来自远程节点的长程信息被指数级压缩——这限制了表达能力。重连接技术可以缓解这一瓶颈；但由于缺乏直接的过压缩经验指标，其实际影响尚不清楚。我们提出了一种严格的、以拓扑为中心的方法，使用节点对之间相互敏感性的衰减率来评估它们之间的过压缩问题。然后，我们将这些成对评估扩展到四个图级统计量（普遍性、强度、变异性、极端性）。将这些指标与图内因果设计相结合，我们量化了重连接策略如何影响不同图和节点分类基准测试中的过压缩问题。我们广泛的实证分析显示，大多数图分类数据集都受到过压缩问题的影响（但程度各异），重连接有效地缓解了这一问题——尽管缓解程度及其转化为性能提升的程度因数据集和方法而异。我们还发现，在节点分类数据集中，过压缩问题不太明显，重连接往往会增加过压缩，且性能变化与过压缩变化无关。这些发现表明，当过压缩既显著又得到适度纠正时，重连接最有益；而过于激进的重连接，或应用于最小过压缩图的重连接，不太可能有所帮助，甚至可能损害性能。我们的即插即用诊断工具让从业者在任何训练之前就能确定重连接是否可能有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have exhibited state-of-the-art performanceacross wide-range of domains such as recommender systems, material design, anddrug repurposing. Yet message-passing GNNs suffer from over-squashing --exponential compression of long-range information from distant nodes -- whichlimits expressivity. Rewiring techniques can ease this bottleneck; but theirpractical impacts are unclear due to the lack of a direct empiricalover-squashing metric. We propose a rigorous, topology-focused method forassessing over-squashing between node pairs using the decay rate of theirmutual sensitivity. We then extend these pairwise assessments to fourgraph-level statistics (prevalence, intensity, variability, extremity).Coupling these metrics with a within-graph causal design, we quantify howrewiring strategies affect over-squashing on diverse graph- andnode-classification benchmarks. Our extensive empirical analyses show that mostgraph classification datasets suffer from over-squashing (but to variousextents), and rewiring effectively mitigates it -- though the degree ofmitigation, and its translation into performance gains, varies by dataset andmethod. We also found that over-squashing is less notable in nodeclassification datasets, where rewiring often increases over-squashing, andperformance variations are uncorrelated with over-squashing changes. Thesefindings suggest that rewiring is most beneficial when over-squashing is bothsubstantial and corrected with restraint -- while overly aggressive rewiring,or rewiring applied to minimally over-squashed graphs, is unlikely to help andmay even harm performance. Our plug-and-play diagnostic tool lets practitionersdecide -- before any training -- whether rewiring is likely to pay off.</description>
      <author>example@mail.com (Danial Saber, Amirali Salehi-Abari)</author>
      <guid isPermaLink="false">2508.09265v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Blockchain Network Analysis using Quantum Inspired Graph Neural Networks &amp; Ensemble Models</title>
      <link>http://arxiv.org/abs/2508.09237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合量子启发图神经网络与集成模型的新方法，用于区块链网络中的反洗钱交易检测，通过引入规范多项式分解层增强了处理复杂数据结构的能力，达到了74.8%的F2分数，展示了量子启发技术在金融安全领域的潜力。&lt;h4&gt;背景&lt;/h4&gt;金融科技领域快速发展，在区块链网络中检测非法交易仍然是一个重大挑战，需要强大且创新的解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门用于区块链网络分析以反洗钱(AML)的系统，提高欺诈交易的检测能力。&lt;h4&gt;方法&lt;/h4&gt;结合量子启发图神经网络(QI-GNN)与集成模型(使用QBoost或随机森林分类器等经典模型)，并在图神经网络框架中引入规范多项式(CP)分解层，以增强处理和分析复杂数据结构的能力。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在检测欺诈交易方面达到了74.8%的F2分数，展示了量子启发技术结合CP层结构改进的有效性。&lt;h4&gt;结论&lt;/h4&gt;量子启发技术结合CP层的结构改进，在复杂网络分析中具有匹配甚至超越传统方法的潜力，倡导在金融领域更广泛地采用和探索量子启发算法以有效打击欺诈。&lt;h4&gt;翻译&lt;/h4&gt;在快速发展的金融科技领域，在区块链网络中检测非法交易仍然是一个关键挑战，需要强大且创新的解决方案。本研究通过结合量子启发图神经网络(QI-GNN)与使用QBoost或随机森林分类器等经典模型的集成模型的灵活性，提出了一种新方法。该系统专门为反洗钱(AML)工作中的区块链网络分析而定制。我们设计该系统的方法包含一个新组件，即在图神经网络框架中的规范多项式(CP)分解层，增强了其高效处理和分析复杂数据结构的能力。我们的技术方法已经过与经典机器学习实现的严格评估，在检测欺诈交易方面达到了74.8%的F2分数。这些结果突显了量子启发技术的潜力，辅以CP层的结构改进，不仅能够在复杂网络分析中匹配，而且可能超过传统方法，用于金融安全。研究结果倡导在金融部门更广泛地采用和进一步探索量子启发算法，以有效打击欺诈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the rapidly evolving domain of financial technology, the detection ofillicit transactions within blockchain networks remains a critical challenge,necessitating robust and innovative solutions. This work proposes a novelapproach by combining Quantum Inspired Graph Neural Networks (QI-GNN) withflexibility of choice of an Ensemble Model using QBoost or a classic model suchas Random Forrest Classifier. This system is tailored specifically forblockchain network analysis in anti-money laundering (AML) efforts. Ourmethodology to design this system incorporates a novel component, a CanonicalPolyadic (CP) decomposition layer within the graph neural network framework,enhancing its capability to process and analyze complex data structuresefficiently. Our technical approach has undergone rigorous evaluation againstclassical machine learning implementations, achieving an F2 score of 74.8% indetecting fraudulent transactions. These results highlight the potential ofquantum-inspired techniques, supplemented by the structural advancements of theCP layer, to not only match but potentially exceed traditional methods incomplex network analysis for financial security. The findings advocate for abroader adoption and further exploration of quantum-inspired algorithms withinthe financial sector to effectively combat fraud.</description>
      <author>example@mail.com (Luigi D'Amico, Daniel De Rosso, Ninad Dixit, Raul Salles de Padua, Samuel Palmer, Samuel Mugel, Román Orús, Holger Eble, Ali Abedi)</author>
      <guid isPermaLink="false">2508.09237v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor Search</title>
      <link>http://arxiv.org/abs/2508.08744v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at SIGMOD 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为Tagore的GPU加速库，用于高效构建基于图的近似最近邻搜索索引，解决了传统图索引方法开销大的问题。&lt;h4&gt;背景&lt;/h4&gt;高维向量空间中的近似最近邻搜索有广泛应用，基于图的索引因其高精度和效率而受到关注，但随着数据量增长和动态调整需求增加，索引开销不断升级。&lt;h4&gt;目的&lt;/h4&gt;开发一个GPU加速的图索引构建库，能够高效处理基于细化的图索引构建，如NSG和Vamana，同时减少索引开销。&lt;h4&gt;方法&lt;/h4&gt;引入GNN-Descent算法进行k-近邻图初始化；提出CFS通用计算过程支持多种k-NN图剪枝策略；设计两个广义GPU内核并行处理邻居关系中的复杂依赖；实现异步GPU-CPU-磁盘索引框架和集群感知缓存机制。&lt;h4&gt;主要发现&lt;/h4&gt;在7个真实数据集上测试，Tagore在保持索引质量的同时实现了1.32倍至112.79倍的加速效果。&lt;h4&gt;结论&lt;/h4&gt;Tagore是一个高效的GPU加速库，能够显著提高图索引构建速度，解决了大规模数据集下图索引开销大的挑战。&lt;h4&gt;翻译&lt;/h4&gt;高维向量空间中的近似最近邻搜索(ANNS)有广泛的真实世界应用。虽然已提出多种方法来高效处理ANNS，但基于图的索引因其高精度和效率而受到广泛关注。然而，基于图的索引开销仍然很大。随着数据量的指数级增长和对动态索引调整需求的增加，这种开销不断升级，构成了关键挑战。在本文中，我们介绍了Tagore，一个由GPU加速的用于图索引的快速库，具有构建基于细化的图索引(如NSG和Vamana)的强大功能。我们首先介绍了GNN-Descent，一种针对GPU的高效k-近邻(k-NN)图初始化算法。GNN-Descent通过两阶段下降过程加速相似性比较，并实现高度并行的邻居更新。接下来，为了支持各种k-NN图剪枝策略，我们提出了一个称为CFS的通用计算过程，并设计了两个广义GPU内核，用于并行处理邻居关系中的复杂依赖。对于超过GPU内存容量的大规模数据集，我们提出了一个异步GPU-CPU-磁盘索引框架，具有集群感知缓存机制，以最小化磁盘的I/O压力。在7个真实数据集上的广泛实验表明，Tagore在保持索引质量的同时实现了1.32倍至112.79倍的加速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Approximate nearest neighbor search (ANNS) in high-dimensional vector spaceshas a wide range of real-world applications. Numerous methods have beenproposed to handle ANNS efficiently, while graph-based indexes have gainedprominence due to their high accuracy and efficiency. However, the indexingoverhead of graph-based indexes remains substantial. With exponential growth indata volume and increasing demands for dynamic index adjustments, this overheadcontinues to escalate, posing a critical challenge. In this paper, we introduceTagore, a fast library accelerated by GPUs for graph indexing, which haspowerful capabilities of constructing refinement-based graph indexes such asNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm forefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds upthe similarity comparison by a two-phase descent procedure and enables highlyparallelized neighbor updates. Next, aiming to support various k-NN graphpruning strategies, we formulate a universal computing procedure termed CFS anddevise two generalized GPU kernels for parallel processing complex dependenciesin neighbor relationships. For large-scale datasets exceeding GPU memorycapacity, we propose an asynchronous GPU-CPU-disk indexing framework with acluster-aware caching mechanism to minimize the I/O pressure on the disk.Extensive experiments on 7 real-world datasets exhibit that Tagore achieves1.32x-112.79x speedup while maintaining the index quality.</description>
      <author>example@mail.com (Zhonggen Li, Xiangyu Ke, Yifan Zhu, Bocheng Yu, Baihua Zheng, Yunjun Gao)</author>
      <guid isPermaLink="false">2508.08744v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Quantum Mechanics to Organic Liquid Properties via a Universal Force Field</title>
      <link>http://arxiv.org/abs/2508.08575v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ByteFF-Pol的新型可极化力场，基于图神经网络参数化，完全使用高级量子力学数据训练，能够准确预测小分子液体和电解质的热力学和传输性质。&lt;h4&gt;背景&lt;/h4&gt;分子动力学模拟是理解凝聚相系统结构和动力学的必要工具，但基于从头算计算预测宏观属性仍然面临挑战，通常受计算成本和模拟精度之间的权衡限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的力场方法，能够更准确、更高效地连接微观量子力学计算和宏观液体性质。&lt;h4&gt;方法&lt;/h4&gt;提出了ByteFF-Pol，一种基于图神经网络参数化的可极化力场，完全基于高级量子力学数据进行训练，利用物理启发的力场形式和训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;ByteFF-Pol在预测小分子液体和电解质的热力学和传输性质方面表现出色，优于最先进的经典和机器学习力场，具有零样本预测能力。&lt;h4&gt;结论&lt;/h4&gt;这一进展在电解质设计和定制溶剂等方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;h4&gt;翻译&lt;/h4&gt;分子动力学（MD）模拟是揭示凝聚相系统结构和动力学原子级洞察力的基本工具。然而，从头算计算普遍准确地预测宏观属性仍然是一个重大挑战，通常受计算成本和模拟精度之间的权衡阻碍。在此，我们提出了ByteFF-Pol，这是一种基于图神经网络（GNN）参数化的可极化力场，完全基于高级量子力学（QM）数据进行训练。利用物理启发的力场形式和训练策略，ByteFF-Pol在预测各种小分子液体和电解质的热力学和传输性质方面表现出色，优于最先进的（SOTA）经典和机器学习力场。ByteFF-Pol的零样本预测能力连接了微观量子力学计算和宏观液体性质，使先前难以处理的化学空间的探索成为可能。这一进展在电解质设计和定制溶剂等应用方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics (MD) simulations are essential tools for unravelingatomistic insights into the structure and dynamics of condensed-phase systems.However, the universal and accurate prediction of macroscopic properties fromab initio calculations remains a significant challenge, often hindered by thetrade-off between computational cost and simulation accuracy. Here, we presentByteFF-Pol, a graph neural network (GNN)-parameterized polarizable force field,trained exclusively on high-level quantum mechanics (QM) data. Leveragingphysically-motivated force field forms and training strategies, ByteFF-Polexhibits exceptional performance in predicting thermodynamic and transportproperties for a wide range of small-molecule liquids and electrolytes,outperforming state-of-the-art (SOTA) classical and machine learning forcefields. The zero-shot prediction capability of ByteFF-Pol bridges the gapbetween microscopic QM calculations and macroscopic liquid properties, enablingthe exploration of previously intractable chemical spaces. This advancementholds transformative potential for applications such as electrolyte design andcustom-tailored solvent, representing a pivotal step toward data-drivenmaterials discovery.</description>
      <author>example@mail.com (Tianze Zheng, Xingyuan Xu, Zhi Wang, Zhenze Yang, Yuanheng Wang, Xu Han, Zhenliang Mu, Ziqing Zhang, Siyuan Liu, Sheng Gong, Kuang Yu, Wen Yan)</author>
      <guid isPermaLink="false">2508.08575v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation</title>
      <link>http://arxiv.org/abs/2508.09626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAD-Splat的新型3D航空场景语义分割方法，通过高斯点丢弃模块和高置信度伪标签生成流程解决了传统方法在处理尺度变化和结构遮挡时的语义模糊问题，提高了分割准确性和表示紧凑性。&lt;h4&gt;背景&lt;/h4&gt;在3D航空场景语义分割任务中，传统方法难以处理航空图像中由尺度变化和结构遮挡引起的语义模糊问题，这限制了它们的分割准确性和一致性。&lt;h4&gt;目的&lt;/h4&gt;解决3D航空场景语义分割中的语义模糊问题，提高分割准确性和一致性。&lt;h4&gt;方法&lt;/h4&gt;提出SAD-Splat方法，包含：1)高斯点丢弃模块，结合语义置信度估计和基于Hard Concrete分布的可学习稀疏机制；2)高置信度伪标签生成流程，利用2D基础模型在真实标签有限时增强监督。&lt;h4&gt;主要发现&lt;/h4&gt;SAD-Splat有效消除了冗余和语义模糊的高斯点，提高了分割性能和表示紧凑性；同时引入了3D航空语义(3D-AS)基准数据集，包含具有稀疏注释的多样化真实世界航空场景。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，SAD-Splat在分割准确性和表示紧凑性之间取得了良好的平衡，为3D航空场景理解提供了高效且可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在3D航空场景语义分割(3D-AVS-SS)任务中，传统方法难以处理航空图像中由尺度变化和结构遮挡引起的语义模糊问题。这限制了它们的分割准确性和一致性。为了应对这些挑战，我们提出了一种名为SAD-Splat的新型3D-AVS-SS方法。我们的方法引入了一个高斯点丢弃模块，该模块将语义置信度估计与基于Hard Concrete分布的可学习稀疏机制相结合。该模块有效消除了冗余和语义模糊的高斯点，提高了分割性能和表示紧凑性。此外，SAD-Splat集成了一个高置信度伪标签生成流程。它利用2D基础模型在真实标签有限时增强监督，从而进一步提高分割准确性。为了推进该领域的研究，我们引入了一个具有挑战性的基准数据集：3D航空语义(3D-AS)，其中包含具有稀疏注释的多样化真实世界航空场景。实验结果表明，SAD-Splat在分割准确性和表示紧凑性之间取得了优异的平衡。它为3D航空场景理解提供了高效且可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D航空场景语义分割中的语义歧义问题。传统方法难以处理航空图像中因尺度变化和结构遮挡导致的语义模糊，这限制了分割的准确性和一致性。这个问题在现实中很重要，因为3D航空场景语义分割在土地使用监测、城市规划和灾害响应等多种遥感应用中起着关键作用，准确的语义分割可以帮助理解复杂航空场景并支持各种决策过程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法在3D航空场景语义分割中的局限性，发现了两个主要挑战：3D重建过程中的冗余高斯点问题和语义分割步骤中的噪声监督问题。针对这些挑战，作者设计了SAD-Splat方法，引入了高斯点丢弃模块和高置信度伪标签生成管道。该方法借鉴了现有的3D高斯溅射(3DGS)框架，并利用了2D基础模型(如SAM和GeoRSCLIP)的知识提取能力，同时采用了知识蒸馏技术和Hard Concrete分布来实现可学习的稀疏机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过语义感知的高斯点丢弃机制消除冗余和语义模糊的高斯点，同时利用高置信度伪标签生成管道增强监督，在保证分割性能的同时提高表示的紧凑性。整体流程包括：预处理阶段利用SAM和GeoRSCLIP生成高置信度伪标签和语义置信图；训练阶段联合重建语义特征，为每个高斯点学习语义置信度和可学习丢弃率，并定期执行丢弃操作；最后结合语义特征损失、RGB重建损失和L0正则化损失进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：语义感知的高斯点丢弃机制(结合语义置信度估计和基于Hard Concrete分布的可学习稀疏机制)、高置信度伪标签生成管道(利用2D基础模型生成可靠伪标签增强监督)、以及新的基准数据集3D-AS(包含各种真实世界航空场景)。相比之前的工作，该方法更有效地处理了航空图像特有的尺度变化和结构遮挡问题，通过丢弃冗余高斯点提高了表示的紧凑性，同时提供了更好的空间一致性和3D结构理解，在有限标注数据的情况下表现更佳。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAD-Splat通过引入语义感知的高斯点丢弃机制和高置信度伪标签生成方法，有效解决了3D航空场景语义分割中的语义歧义和结构冗余问题，同时提高了模型的表示紧凑性和分割准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),traditional methods struggle to address semantic ambiguity caused by scalevariations and structural occlusions in aerial images. This limits theirsegmentation accuracy and consistency. To tackle these challenges, we propose anovel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussianpoint drop module, which integrates semantic confidence estimation with alearnable sparsity mechanism based on the Hard Concrete distribution. Thismodule effectively eliminates redundant and semantically ambiguous Gaussianpoints, enhancing both segmentation performance and representation compactness.Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generationpipeline. It leverages 2D foundation models to enhance supervision whenground-truth labels are limited, thereby further improving segmentationaccuracy. To advance research in this domain, we introduce a challengingbenchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diversereal-world aerial scenes with sparse annotations. Experimental resultsdemonstrate that SAD-Splat achieves an excellent balance between segmentationaccuracy and representation compactness. It offers an efficient and scalablesolution for 3D aerial scene understanding.</description>
      <author>example@mail.com (Xu Tang, Junan Jia, Yijing Wang, Jingjing Ma, Xiangrong Zhang)</author>
      <guid isPermaLink="false">2508.09626v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</title>
      <link>http://arxiv.org/abs/2508.09987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了GPT-4o生成合成图像的优势，并提出了Echo-4o-Image数据集和Echo-4o模型，同时开发了两个新的评估基准，证明了合成数据在补充罕见场景和提供可控监督方面的价值。&lt;h4&gt;背景&lt;/h4&gt;GPT-4o因其强大的图像生成能力而受到广泛关注，但开源模型仍落后于它。已有研究探索从GPT-4o中蒸馏图像数据来增强开源模型，但存在一个关键问题：既然现实世界图像数据已是高质量数据来源，为何还需使用GPT-4o生成的合成数据？&lt;h4&gt;目的&lt;/h4&gt;识别合成图像的关键优势，构建基于GPT-4o的合成数据集，提出新的评估基准，并验证合成数据对图像生成模型的提升效果。&lt;h4&gt;方法&lt;/h4&gt;识别出合成图像的两个关键优势：1) 可补充现实数据集中罕见场景，如超现实主义幻想或多参考图像生成；2) 提供干净可控的监督，避免现实数据中的背景噪声和文本-图像错位。基于此，构建了18万规模的Echo-4o-Image合成数据集，微调Bagel模型获得Echo-4o，并提出两个新评估基准：GenEval++(增加指令复杂度)和Imagine-Bench(评估想象内容的理解和生成)。&lt;h4&gt;主要发现&lt;/h4&gt;Echo-4o在标准基准上表现优异；将Echo-4o-Image应用于其他基础模型(如OmniGen2、BLIP3-o)时，在多个指标上带来了一致的性能提升，证明了该数据集的强可转移性。&lt;h4&gt;结论&lt;/h4&gt;合成图像数据在补充罕见场景和提供干净可控监督方面具有独特优势，Echo-4o-Image有效解决了现实世界数据覆盖的盲点，Echo-4o模型在多个基准测试中表现良好且具有很好的可转移性。&lt;h4&gt;翻译&lt;/h4&gt;最近，GPT-4因其强大的图像生成能力而受到广泛关注，但开源模型仍然落后。几项研究已经探索从GPT-4中蒸馏图像数据来增强开源模型，取得了显著进展。然而，一个关键问题仍然存在：鉴于现实世界图像数据已经构成高质量数据的自然来源，为什么我们应该使用GPT-4生成的合成数据？在这项工作中，我们确定了合成图像的两个关键优势。首先，它们可以补充现实世界数据集中的罕见场景，如超现实主义幻想或多参考图像生成，这些场景在用户查询中经常出现。其次，它们提供干净和可控的监督。现实世界数据通常包含复杂的背景噪声和文本描述与图像内容之间的固有错位，而合成图像则提供纯背景和长尾监督信号，促进更准确的文本到图像对齐。基于这些见解，我们引入了Echo-4o-Image，这是一个由GPT-4生成的18万规模的合成数据集，利用合成图像数据的力量来解决现实世界覆盖中的盲点。使用这个数据集，我们微调了统一的多模态生成基线Bagel以获得Echo-4o。此外，我们提出了两个新的评估基准，以更准确和具有挑战性地评估图像生成能力：GenEval++通过增加指令复杂度来减轻分数饱和，而Imagine-Bench专注于评估想象内容的理解和生成。Echo-4o在标准基准上表现出强大的性能。此外，将Echo-4o-Image应用于其他基础模型(如OmniGen2、BLIP3-o)在多个指标上带来了一致的性能提升，突显了该数据集的强可转移性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, GPT-4o has garnered significant attention for its strongperformance in image generation, yet open-source models still lag behind.Several studies have explored distilling image data from GPT-4o to enhanceopen-source models, achieving notable progress. However, a key questionremains: given that real-world image datasets already constitute a naturalsource of high-quality data, why should we use GPT-4o-generated synthetic data?In this work, we identify two key advantages of synthetic images. First, theycan complement rare scenarios in real-world datasets, such as surreal fantasyor multi-reference image generation, which frequently occur in user queries.Second, they provide clean and controllable supervision. Real-world data oftencontains complex background noise and inherent misalignment between textdescriptions and image content, whereas synthetic images offer pure backgroundsand long-tailed supervision signals, facilitating more accurate text-to-imagealignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scalesynthetic dataset generated by GPT-4o, harnessing the power of synthetic imagedata to address blind spots in real-world coverage. Using this dataset, wefine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.In addition, we propose two new evaluation benchmarks for a more accurate andchallenging assessment of image generation capabilities: GenEval++, whichincreases instruction complexity to mitigate score saturation, andImagine-Bench, which focuses on evaluating both the understanding andgeneration of imaginative content. Echo-4o demonstrates strong performanceacross standard benchmarks. Moreover, applying Echo-4o-Image to otherfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gainsacross multiple metrics, highlighting the datasets strong transferability.</description>
      <author>example@mail.com (Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, Weijia Li)</author>
      <guid isPermaLink="false">2508.09987v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</title>
      <link>http://arxiv.org/abs/2508.09977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  GitHub Repo:  https://github.com/heshuting555/Awesome-3DGS-Applications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于3D高斯溅射(3DGS)应用的综述，全面介绍了其在各种下游任务中的应用进展，包括分割、编辑和生成等，并提供了常用数据集和评估方法的总结。&lt;h4&gt;背景&lt;/h4&gt;3D高斯溅射(3DGS)最近作为一种强大的神经辐射场(NeRF)替代方案出现，用于3D场景表示，提供高保真度的照片级真实感渲染和实时性能。&lt;h4&gt;目的&lt;/h4&gt;提供对3DGS应用最新进展的全面概述，支持需要几何和语义理解的各种下游应用。&lt;h4&gt;方法&lt;/h4&gt;介绍支持3DGS应用中语义理解和控制的2D基础模型；回顾基于NeRF的方法及其对3DGS对应方法的启示；将3DGS应用分为分割、编辑、生成和其他功能任务；总结代表性方法、监督策略和学习范式；强调共享设计原则和新兴趋势；总结常用数据集和评估协议；提供方法比较分析；维护资源库。&lt;h4&gt;主要发现&lt;/h4&gt;3DGS明确且紧凑的性质使其能够支持广泛的下游应用；在不同应用类别中存在共享的设计原则。&lt;h4&gt;结论&lt;/h4&gt;维护了一个持续更新的资源库，网址为https://github.com/heshuting555/Awesome-3DGS-Applications，以支持正在进行的研究和开发。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯溅射(3DGS)最近作为一种强大的神经辐射场(NeRF)替代方案出现，用于3D场景表示，提供高保真度的照片级真实感渲染和实时性能。除了新颖视图合成外，3DGS明确且紧凑的性质使其能够支持需要几何和语义理解的广泛下游应用。本调查提供了3DGS应用最新进展的全面概述。它首先介绍支持3DGS应用中语义理解和控制的2D基础模型，然后回顾基于NeRF的方法及其对3DGS对应方法的启示。我们将3DGS应用分为分割、编辑、生成和其他功能任务。对于每个类别，我们总结了代表性方法、监督策略和学习范式，突出了共享的设计原则和新兴趋势。同时还总结了常用的数据集和评估协议，以及最近方法在公共基准上的比较分析。为了支持正在进行的研究和开发，我们维护了一个不断更新的论文、代码和资源库，网址为https://github.com/heshuting555/Awesome-3DGS-Applications。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是3D高斯溅射(3DGS)在下游应用领域的系统性梳理和分类问题。虽然已有一些关于3DGS的综述，但它们主要关注全局分类、实时渲染流水线或压缩策略，而对3DGS驱动的下游应用缺乏深入分析。这个问题在现实中很重要，因为3DGS的显式和紧凑特性使其能够支持需要几何和语义理解的广泛应用，包括虚拟现实、机器人、自主导航和城市映射等领域，通过系统分析这些应用可以促进3DGS技术的发展和应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作为综述论文，作者不是设计新方法而是系统梳理现有研究。作者首先介绍3DGS基本概念和应用背景，然后讨论支持3DGS的2D基础模型(如DINO、CLIP、SAM)和相关NeRF研究，接着按任务类型(分割、编辑、生成)分类分析应用，最后总结数据集、评估协议和未来方向。作者借鉴了大量现有研究成果，特别是2D基础模型和NeRF方法，为3DGS提供概念连续性和技术基础。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这篇综述论文的核心思想是对3DGS在分割、编辑和生成等下游应用领域的最新进展进行系统性分类、比较和分析。整体流程包括：1)引言介绍背景和贡献；2)背景介绍问题表述、分类和相关技术；3)详细讨论三大类应用任务；4)总结评估协议和性能比较；5)讨论挑战和未来方向；6)结论总结关键见解。这种结构化方式帮助读者全面理解3DGS应用现状和发展趋势。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)专注性 - 首个专门针对3DGS下游应用的综述；2)系统性 - 对三大应用方向进行系统分类；3)全面性 - 总结代表性方法、监督策略和学习范式；4)实用性 - 提供数据集、评估协议和性能比较；5)资源支持 - 维护持续更新的资源库。相比之前工作，这篇论文更深入探讨3DGS在高层次视觉和图形任务中的潜力，提供更全面系统的视角，而非仅关注渲染效率或压缩技术。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统性地综述了3D高斯溅射在分割、编辑和生成等下游应用领域的最新进展，为研究人员提供了全面的技术分析和未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternativeto Neural Radiance Fields (NeRF) for 3D scene representation, offeringhigh-fidelity photorealistic rendering with real-time performance. Beyond novelview synthesis, the explicit and compact nature of 3DGS enables a wide range ofdownstream applications that require geometric and semantic understanding. Thissurvey provides a comprehensive overview of recent progress in 3DGSapplications. It first introduces 2D foundation models that support semanticunderstanding and control in 3DGS applications, followed by a review ofNeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGSapplications into segmentation, editing, generation, and other functionaltasks. For each, we summarize representative methods, supervision strategies,and learning paradigms, highlighting shared design principles and emergingtrends. Commonly used datasets and evaluation protocols are also summarized,along with comparative analyses of recent methods across public benchmarks. Tosupport ongoing research and development, a continually updated repository ofpapers, code, and resources is maintained athttps://github.com/heshuting555/Awesome-3DGS-Applications.</description>
      <author>example@mail.com (Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding)</author>
      <guid isPermaLink="false">2508.09977v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification</title>
      <link>http://arxiv.org/abs/2508.09967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Meta-Optimized Classifier (MOC)的新方法，用于提高组织病理学视觉语言基础模型在整张切片图像分类任务中的性能，特别是在少样本学习场景下。&lt;h4&gt;背景&lt;/h4&gt;组织病理学视觉语言基础模型通过零样本适应解决了整张切片图像分类的数据稀缺问题，但仍被传统多实例学习方法超越。现有少样本方法虽能提高诊断准确性，但依赖传统分类器设计，对数据稀缺存在脆弱性。&lt;h4&gt;目的&lt;/h4&gt;解决现有少样本方法对数据稀缺的脆弱性问题，提高基于视觉语言基础模型的整张切片图像分类性能，特别是在临床部署中训练数据严重有限的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出Meta-Optimized Classifier (MOC)，包含两个核心组件：(1)一个元学习器，能从候选分类器混合中自动优化分类器配置；(2)一个分类器库，包含多样化候选分类器，实现全面病理解释。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，MOC在多个少样本基准测试中优于先前方法。在TCGA-NSCLC基准测试中，MOC比最先进的少样本VLFM方法提高10.4%的AUC，在1-shot条件下增益高达26.25%。&lt;h4&gt;结论&lt;/h4&gt;MOC为临床部署提供了关键进展，特别是在诊断训练数据严重有限的情况下，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;组织病理学视觉语言基础模型的最新进展已显示出通过零样本适应解决整张切片图像分类数据稀缺问题的前景。然而，这些方法仍然被在大型数据集上训练的传统多实例学习方法所超越，促使近期努力通过少样本学习范式增强基于VLFM的WSI分类。虽然现有少样本方法通过有限的标注提高了诊断准确性，但它们对传统分类器设计的依赖引入了对数据稀缺的关键脆弱性。为解决这个问题，我们提出了一种元优化分类器，包含两个核心组件：(1)一个元学习器，能从候选分类器混合中自动优化分类器配置；(2)一个分类器库，包含多样化的候选分类器，能够实现全面的病理解释。大量实验证明，MOC在多个少样本基准测试中优于先前方法。值得注意的是，在TCGA-NSCLC基准测试中，MOC比最先进的少样本VLFM方法提高了10.4%的AUC，在1-shot条件下增益高达26.25%，为诊断训练数据严重有限的临床部署提供了关键进展。代码可在https://github.com/xmed-lab/MOC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in histopathology vision-language foundation models (VLFMs)have shown promise in addressing data scarcity for whole slide image (WSI)classification via zero-shot adaptation. However, these methods remainoutperformed by conventional multiple instance learning (MIL) approachestrained on large datasets, motivating recent efforts to enhance VLFM-based WSIclassification through fewshot learning paradigms. While existing few-shotmethods improve diagnostic accuracy with limited annotations, their reliance onconventional classifier designs introduces critical vulnerabilities to datascarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)comprising two core components: (1) a meta-learner that automatically optimizesa classifier configuration from a mixture of candidate classifiers and (2) aclassifier bank housing diverse candidate classifiers to enable a holisticpathological interpretation. Extensive experiments demonstrate that MOCoutperforms prior arts in multiple few-shot benchmarks. Notably, on theTCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-artfew-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,offering a critical advancement for clinical deployments where diagnostictraining data is severely limited. Code is available athttps://github.com/xmed-lab/MOC.</description>
      <author>example@mail.com (Tianqi Xiang, Yi Li, Qixiang Zhang, Xiaomeng Li)</author>
      <guid isPermaLink="false">2508.09967v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Multi-head committees enable direct uncertainty prediction for atomistic foundation models</title>
      <link>http://arxiv.org/abs/2508.09907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures in main article + supporting information&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用MACE模型及其多头机制实现了神经网络势能的委员会预测，通过预测的标准差来估计模型的不确定性。该方法在多个数据集上表现出良好的不确定性估计能力，并成功应用于基础模型，通过主动学习将训练集缩减到原来的5%，同时保持了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;机器学习势能已成为原子级材料建模的标准工具。尽管模型变得越来越通用，但在主动学习和稳健误差分析方面，如何高效预测不确定性仍是一个开放的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在实现一种能够有效预测不确定性的机器学习势能模型，以支持主动学习和稳健误差分析。&lt;h4&gt;方法&lt;/h4&gt;研究利用MACE及其多头机制实现了一个用于消息传递架构的委员会神经网络势能模型。委员会由多个连接到相同原子环境描述符的输出模块组成。预测的标准差作为模型不确定性的估计。研究者将这一概念应用于基础模型MACE-MP-0，仅训练新添加的输出头，同时保持模型其余部分固定。&lt;h4&gt;主要发现&lt;/h4&gt;1. 力预测的不确定性与真实误差在多个数据集上表现出良好的相关性；2. 将委员会概念应用于基础模型MACE-MP-0；3. 通过主动学习将基础模型的训练集缩减到原来的5%；4. 在缩减的训练集上训练的多头委员会模型能够提供可靠的不确定性估计，而不会显著降低预测准确性。&lt;h4&gt;结论&lt;/h4&gt;通过利用MACE的多头机制实现的委员会神经网络势能模型可以有效预测不确定性，支持主动学习流程，并能在大幅缩减训练集的情况下保持预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;机器学习势能已成为原子级材料建模的标准工具。虽然模型继续变得更加通用，但一个开放的挑战是如何为主动学习和稳健误差分析高效预测不确定性。在这项工作中，我们利用MACE及其多头机制实现了用于消息传递架构的委员会神经网络势能模型，其中委员会由多个连接到相同原子环境描述符的输出模块组成。与传统独立网络的委员会一样，预测的标准差可作为模型不确定性的估计。我们在多个自建模型数据集上表明，力预测的不确定性与真实误差有良好的相关性。随后，我们将这一概念应用于基础模型，特别是MACE-MP-0，在其中我们仅训练新添加的输出头，同时保持模型的其余部分固定。我们在主动学习工作流程中使用这种方法，将基础模型的训练集压缩到其原始大小的5%。在缩减训练集上训练的基础模型多头委员会能够提供可靠的不确定性估计，而不会显著降低预测准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning potentials have become a standard tool for atomisticmaterials modelling. While models continue to become more generalisable, anopen challenge relates to efficient uncertainty predictions for active learningand robust error analysis. In this work, we utilise MACE and its multi-headmechanism to implement a committee neural network potential for message-passingarchitectures, where the committee comprises multiple output modules attachedto the same atomic environment descriptors. As with traditional committees ofindependent networks, the standard deviation of the predictions functions as anestimate of the model's uncertainty. We show for a range of datasets incustom-build models that the uncertainty of the force predictions correlateswell with the true errors. We subsequently apply this concept to foundationmodels, specifically MACE-MP-0, where we train only the newly attached outputheads while keeping the remaining part of the model fixed. We use this approachin an active learning workflow to condense the training set of the foundationmodel to just 5\% of its original size. The foundation model multi-headcommittee trained on the condensed training set enables reliable uncertaintyestimation without any substantial decrease in prediction accuracy.</description>
      <author>example@mail.com (Hubert Beck, Pavol Simko, Lars L. Schaaf, Ondrej Marsalek, Christoph Schran)</author>
      <guid isPermaLink="false">2508.09907v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?</title>
      <link>http://arxiv.org/abs/2508.09888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了现代人工神经网络在田间尺度预测土壤建模任务中的性能，发现现代ANN架构在大多数任务上优于传统机器学习方法，其中TabPFN表现最佳且具有稳健性。&lt;h4&gt;背景&lt;/h4&gt;在土壤计量学领域，表格机器学习是预测土壤性质的主要方法，但田间尺度PSM任务通常受限于小样本量和土壤光谱中的高特征样本比率，这对传统深度学习方法构成挑战。&lt;h4&gt;目的&lt;/h4&gt;评估现代人工神经网络架构在田间尺度PSM任务上的适用性，挑战古典机器学习算法作为默认选择的观点。&lt;h4&gt;方法&lt;/h4&gt;引入全面的基准测试，评估包括多层感知器模型、注意力Transformer变体、检索增强方法和上下文学习基础模型在内的31种先进ANN架构，在31个田间和农场规模数据集上测试三种关键土壤属性。&lt;h4&gt;主要发现&lt;/h4&gt;现代ANN在大多数任务上始终优于传统方法，深度学习已足够成熟以克服古典机器学习的长期主导地位，TabPFN表现最佳且具有稳健性。&lt;h4&gt;结论&lt;/h4&gt;建议在田间尺度PSM中采用现代ANN，并将TabPFN作为每位土壤计量学家工具包中的新默认选择。&lt;h4&gt;翻译&lt;/h4&gt;在土壤计量学领域，表格机器学习是从远程和近端土壤传感数据预测土壤性质的主要方法，构成了数字土壤图的核心组成部分。在田间尺度上，这种预测土壤建模(PSM)任务通常受限于土壤光谱中的小训练样本量和高特征样本比率。传统上，这些条件已被证明对传统深度学习方法构成挑战。经典机器学习算法，特别是像随机森林这样的树模型和偏最小二乘回归等线性模型，长期以来一直是田间尺度PSM的默认选择。用于表格数据的人工神经网络(ANN)的最新进展挑战了这一观点，但它们在田间尺度PSM中的适用性尚未得到证实。我们引入了一个全面的基准测试，评估了最先进的ANN架构，包括最新的多层感知器(MLP)模型(TabM, RealMLP)、基于注意力的Transformer变体(FT-Transformer, ExcelFormer, T2G-Former, AMFormer)、检索增强方法(TabR, ModernNCA)以及上下文学习基础模型(TabPFN)。我们的评估涵盖31个田间和农场规模的数据集，包含30到460个样本，以及三种关键的土壤性质：土壤有机物或土壤有机碳、pH值和粘土含量。我们的结果显示，现代ANN在大多数任务上始终优于传统方法，证明了深度学习已经足够成熟，可以克服古典机器学习在PSM上的长期主导地位。值得注意的是，TabPFN提供了最强的整体性能，显示出在不同条件下的稳健性。因此，我们建议在田间尺度PSM中采用现代ANN，并提议TabPFN作为每位土壤计量学家工具包中的新默认选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of pedometrics, tabular machine learning is the predominantmethod for predicting soil properties from remote and proximal soil sensingdata, forming a central component of digital soil mapping. At the field-scale,this predictive soil modeling (PSM) task is typically constrained by smalltraining sample sizes and high feature-to-sample ratios in soil spectroscopy.Traditionally, these conditions have proven challenging for conventional deeplearning methods. Classical machine learning algorithms, particularlytree-based models like Random Forest and linear models such as Partial LeastSquares Regression, have long been the default choice for field-scale PSM.Recent advances in artificial neural networks (ANN) for tabular data challengethis view, yet their suitability for field-scale PSM has not been proven. Weintroduce a comprehensive benchmark that evaluates state-of-the-art ANNarchitectures, including the latest multilayer perceptron (MLP)-based models(TabM, RealMLP), attention-based transformer variants (FT-Transformer,ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,ModernNCA), and an in-context learning foundation model (TabPFN). Ourevaluation encompasses 31 field- and farm-scale datasets containing 30 to 460samples and three critical soil properties: soil organic matter or soil organiccarbon, pH, and clay content. Our results reveal that modern ANNs consistentlyoutperform classical methods on the majority of tasks, demonstrating that deeplearning has matured sufficiently to overcome the long-standing dominance ofclassical machine learning for PSM. Notably, TabPFN delivers the strongestoverall performance, showing robustness across varying conditions. We thereforerecommend the adoption of modern ANNs for field-scale PSM and propose TabPFN asthe new default choice in the toolkit of every pedometrician.</description>
      <author>example@mail.com (Viacheslav Barkov, Jonas Schmidinger, Robin Gebbers, Martin Atzmueller)</author>
      <guid isPermaLink="false">2508.09888v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</title>
      <link>http://arxiv.org/abs/2508.09834v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Survey, 82 pages, GitHub:  https://github.com/weigao266/Awesome-Efficient-Arch&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了创新的大型语言模型(LLM)架构，旨在解决传统Transformer模型的局限性并提高效率，涵盖了从语言建模到多模态应用的多种技术方法。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)在语言理解、生成和推理方面取得了显著成果，推动了多模态模型的能力边界。Transformer模型作为现代LLMs的基础，提供了具有优秀扩展性的强基线，但传统Transformer架构需要大量计算，对大规模训练和实际部署构成显著障碍。&lt;h4&gt;目的&lt;/h4&gt;系统性地检查创新的LLM架构，解决Transformer的固有局限性，提高模型效率，并为开发可扩展、资源感知的基础模型提供更广泛的视角。&lt;h4&gt;方法&lt;/h4&gt;从语言建模开始，涵盖线性序列建模方法和稀疏序列建模方法的背景和技术细节，讨论高效的全注意力变体，探索稀疏混合专家模型，研究结合上述技术的混合模型架构，以及新兴的扩散LLMs，同时讨论这些技术在其他模态中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;通过将近期研究分组到不同类别，展示了现代高效LLM架构的蓝图，为未来研究提供了方向。&lt;h4&gt;结论&lt;/h4&gt;希望这篇综述能够激励未来朝着更高效、多功能的AI系统的研究，推动资源感知的基础模型发展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在语言理解、生成和推理方面取得了令人印象深刻的结果，并推动了多模态模型的能力边界。Transformer模型作为现代LLMs的基础，提供了具有优秀扩展性的强基线。然而，传统Transformer架构需要大量计算，并对大规模训练和实际部署构成显著障碍。在本综述中，我们系统地检查了创新的LLM架构，解决了Transformer的固有局限性并提高了效率。从语言建模开始，本综述涵盖了线性序列建模方法和稀疏序列建模方法的背景和技术细节，高效的全注意力变体，稀疏混合专家模型，结合上述技术的混合模型架构，以及新兴的扩散LLMs。此外，我们讨论了这些技术在其他模态中的应用，并考虑了它们对开发可扩展、资源感知的基础模型的更广泛影响。通过将近期研究分组到上述类别，本综述展示了现代高效LLM架构的蓝图，我们希望这能够帮助激励未来朝着更高效、多功能的AI系统的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have delivered impressive results in languageunderstanding, generation, reasoning, and pushes the ability boundary ofmultimodal models. Transformer models, as the foundation of modern LLMs, offera strong baseline with excellent scaling properties. However, the traditionaltransformer architecture requires substantial computations and posessignificant obstacles for large-scale training and practical deployment. Inthis survey, we offer a systematic examination of innovative LLM architecturesthat address the inherent limitations of transformers and boost the efficiency.Starting from language modeling, this survey covers the background andtechnical details of linear and sparse sequence modeling methods, efficientfull attention variants, sparse mixture-of-experts, hybrid model architecturesincorporating the above techniques, and emerging diffusion LLMs. Additionally,we discuss applications of these techniques to other modalities and considertheir wider implications for developing scalable, resource-aware foundationmodels. By grouping recent studies into the above category, this surveypresents a blueprint of modern efficient LLM architectures, and we hope thiscould help motivate future research toward more efficient, versatile AIsystems.</description>
      <author>example@mail.com (Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng)</author>
      <guid isPermaLink="false">2508.09834v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>BeatFM: Improving Beat Tracking with Pre-trained Music Foundation Model</title>
      <link>http://arxiv.org/abs/2508.09790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by ICME2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了BeatFM，一种基于预训练音乐基础模型的节拍跟踪方法，通过多维度语义聚合模块提高了在不同音乐风格中的节拍跟踪性能，实验证明其达到了最先进水平。&lt;h4&gt;背景&lt;/h4&gt;当前节拍跟踪方法面临标注数据稀缺的挑战，这限制了它们在不同音乐风格中的泛化能力，以及准确捕捉复杂节奏结构的能力。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，研究者提出了一种新的节拍跟踪范式BeatFM，通过引入预训练的音乐基础模型，并利用其丰富的语义知识来提高节拍跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;1. 提出BeatFM，一种新的节拍跟踪范式；2. 引入预训练的音乐基础模型；3. 设计了一个即插即用的多维度语义聚合模块，包含三个并行子模块：时域语义聚合子模块、频域语义聚合子模块和通道域语义聚合子模块。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法在多个基准数据集的节拍和下拍跟踪方面取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;预训练在多样化音乐数据集上的音乐基础模型使模型能够对音乐有稳健的理解，从而有效解决了节拍跟踪中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;节拍跟踪是音乐信息检索中广泛研究的主题。然而，由于标注数据的稀缺，当前的节拍跟踪方法面临着挑战，这限制了它们在不同音乐风格中的泛化能力以及准确捕捉复杂节奏结构的能力。为了克服这些挑战，我们提出了一种新颖的节拍跟踪范式BeatFM，它引入了预训练的音乐基础模型，并利用其丰富的语义知识来提高节拍跟踪性能。在多样化音乐数据集上进行预训练使音乐基础模型能够对音乐有稳健的理解，从而有效地解决了这些挑战。为了进一步使其适应节拍跟踪，我们设计了一个即插即用的多维度语义聚合模块，它由三个并行子模块组成，分别专注于时域、频域和通道域的语义聚合。大量实验证明，我们的方法在多个基准数据集的节拍和下拍跟踪方面取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Beat tracking is a widely researched topic in music information retrieval.However, current beat tracking methods face challenges due to the scarcity oflabeled data, which limits their ability to generalize across diverse musicalstyles and accurately capture complex rhythmic structures. To overcome thesechallenges, we propose a novel beat tracking paradigm BeatFM, which introducesa pre-trained music foundation model and leverages its rich semantic knowledgeto improve beat tracking performance. Pre-training on diverse music datasetsendows music foundation models with a robust understanding of music, therebyeffectively addressing these challenges. To further adapt it for beat tracking,we design a plug-and-play multi-dimensional semantic aggregation module, whichis composed of three parallel sub-modules, each focusing on semanticaggregation in the temporal, frequency, and channel domains, respectively.Extensive experiments demonstrate that our method achieves state-of-the-artperformance in beat and downbeat tracking across multiple benchmark datasets.</description>
      <author>example@mail.com (Ganghui Ru, Jieying Wang, Jiahao Zhao, Yulun Wu, Yi Yu, Nannan Jiang, Wei Wang, Wei Li)</author>
      <guid isPermaLink="false">2508.09790v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>HingeNet: A Harmonic-Aware Fine-Tuning Approach for Beat Tracking</title>
      <link>http://arxiv.org/abs/2508.09788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by ICME2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HingeNet是一种专门为节拍跟踪任务设计的参数高效微调方法，通过轻量级铰链式网络结构和谐波感知机制，有效解决了预训练基础模型在有限标注数据下的应用问题，并在基准测试中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;微调预训练的基础模型在音乐信息检索方面取得了显著进展，但这些模型在节拍跟踪任务上的应用尚未被探索，有限的标注数据使得传统微调方法无效。&lt;h4&gt;目的&lt;/h4&gt;解决预训练基础模型在节拍跟踪任务中的应用挑战，提出一种参数高效的微调方法。&lt;h4&gt;方法&lt;/h4&gt;提出了HingeNet，一种轻量级且可分离的铰链式网络，使用预训练基础模型的中间特征表示作为输入，并在微调过程中引入谐波感知机制以更好地捕获和强调音乐信号中的谐波结构。&lt;h4&gt;主要发现&lt;/h4&gt;HingeNet具有广泛的通用性，能够有效集成各种预训练基础模型；在基准数据集上的实验表明，HingeNet在节拍和下拍跟踪方面取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;HingeNet通过创新的网络设计和谐波感知机制，解决了预训练基础模型在节拍跟踪任务中的应用问题，实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;微调预训练的基础模型在音乐信息检索方面取得了显著进展。然而，由于有限的标注数据使得传统微调方法无效，将这些模型应用于节拍跟踪任务仍然是一个未探索的领域。为了应对这一挑战，我们提出了HingeNet，一种专门为节拍跟踪任务设计的新型通用参数高效微调方法。HingeNet是一个轻量级且可分离的网络，视觉上类似铰链，通过使用预训练基础模型的中间特征表示作为输入，与预训练基础模型紧密接口。这种独特的架构赋予了HingeNet广泛的通用性，能够有效集成各种预训练基础模型。此外，考虑到谐波在节拍跟踪中的重要性，我们在微调过程中引入了谐波感知机制，以更好地捕获和强调音乐信号中的谐波结构。在基准数据集上的实验证明，HingeNet在节拍和下拍跟踪方面取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning pre-trained foundation models has made significant progress inmusic information retrieval. However, applying these models to beat trackingtasks remains unexplored as the limited annotated data renders conventionalfine-tuning methods ineffective. To address this challenge, we proposeHingeNet, a novel and general parameter-efficient fine-tuning methodspecifically designed for beat tracking tasks. HingeNet is a lightweight andseparable network, visually resembling a hinge, designed to tightly interfacewith pre-trained foundation models by using their intermediate featurerepresentations as input. This unique architecture grants HingeNet broadgeneralizability, enabling effective integration with various pre-trainedfoundation models. Furthermore, considering the significance of harmonics inbeat tracking, we introduce harmonic-aware mechanism during the fine-tuningprocess to better capture and emphasize the harmonic structures in musicalsignals. Experiments on benchmark datasets demonstrate that HingeNet achievesstate-of-the-art performance in beat and downbeat tracking</description>
      <author>example@mail.com (Ganghui Ru, Jieying Wang, Jiahao Zhao, Yulun Wu, Yi Yu, Nannan Jiang, Wei Wang, Wei Li)</author>
      <guid isPermaLink="false">2508.09788v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors</title>
      <link>http://arxiv.org/abs/2508.09667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GSFixer是一个改进从稀疏视图重建的3D高斯溅射表示质量的新框架，通过参考引导的视频恢复模型解决3D重建中的伪影问题，实验证明其性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;从稀疏视图使用3D高斯溅射重建3D场景是一个不适定问题，由于信息不足常导致明显伪影。现有方法利用生成先验补全信息，但难以生成与输入观察保持一致的内容。&lt;h4&gt;目的&lt;/h4&gt;提出GSFixer框架，提高从稀疏输入重建的3D高斯溅射表示质量，解决3D重建中的伪影问题。&lt;h4&gt;方法&lt;/h4&gt;核心是基于DiT的视频扩散模型构建的参考引导视频恢复模型，在成对伪影3D高斯溅射渲染和干净帧上训练。模型将输入稀疏视图作为参考，集成从视觉几何基础模型提取的2D语义特征和3D几何特征，增强语义一致性和3D一致性。同时提出DL3DV-Res基准用于评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明GSFixer在3D高斯溅射伪影恢复和稀疏视图3D重建方面优于当前最先进方法。&lt;h4&gt;结论&lt;/h4&gt;GSFixer有效解决了3D高斯溅射重建中的伪影问题，提高了从稀疏输入重建的3D场景质量，并能生成与输入观察保持一致的内容。&lt;h4&gt;翻译&lt;/h4&gt;从稀疏视图使用3D高斯溅射重建3D场景是一个不适定问题，由于信息不足，通常会导致明显的伪影。虽然最近的方法试图利用生成先验来补全欠约束区域的信息，但它们难以生成与输入观察保持一致的内容。为应对这一挑战，我们提出了GSFixer，一个旨在提高从稀疏输入重建的3D高斯溅射表示质量的新框架。我们方法的核心是基于参考引导的视频恢复模型，构建在基于DiT的视频扩散模型上，该模型在成对的伪影3D高斯溅射渲染和干净帧上训练，并带有基于参考的附加条件。将输入稀疏视图作为参考，我们的模型集成了从视觉几何基础模型中提取的参考视图的2D语义特征和3D几何特征，在修复伪影新视图时增强了语义一致性和3D一致性。此外，考虑到缺乏适合的3D高斯溅射伪影恢复评估基准，我们提出了DL3DV-Res，其中包含使用低质量3D高斯溅射渲染的伪影帧。大量实验证明，我们的GSFixer在3D高斯溅射伪影恢复和稀疏视图3D重建方面优于当前最先进的方法。项目页面：https://github.com/GVCLab/GSFixer。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决使用稀疏视图进行3D高斯泼溅重建时产生的明显伪影问题。这个问题在现实中非常重要，因为获取密集的多视图数据成本高、耗时长，而3D重建在虚拟现实、自动驾驶和机器人等众多领域有广泛应用价值。稀疏视图重建能力使得这些技术能够在资源有限的环境下实现高质量3D场景重建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3DGS在稀疏视图下的局限性，然后借鉴了ReconFusion等将扩散模型引入3D重建的工作思路。作者还参考了视频扩散模型（如CogVideoX）的成功应用，以及视觉几何基础模型（如VGGT和DINOv2）的特征提取能力。通过整合这些现有技术，作者创新性地提出同时利用参考视图的2D语义特征和3D几何特征来指导修复过程，解决了现有方法难以保持生成内容与输入观察一致性的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用参考视图的2D语义特征和3D几何特征来引导视频扩散模型修复伪影，同时保持语义一致性和3D一致性。整体流程包括：1)从稀疏视图构建初始3DGS；2)使用DINOv2和VGGT提取参考视图的2D语义和3D几何特征；3)采用参考引导的轨迹采样策略在参考视图间生成新视角；4)将伪影视图和参考特征输入视频扩散模型进行修复；5)将修复后的视图加入训练集，通过重建损失和生成损失迭代优化3DGS；6)输出高质量3D表示和新视图合成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)参考引导的视频修复模型，基于DiT架构并注入参考信息；2)双条件信号融合，同时使用2D语义特征和3D几何特征；3)参考引导的轨迹采样策略，平衡修复质量和视角多样性；4)提出DL3DV-Res基准数据集。相比之前的工作，GSFixer的主要不同在于：使用双条件信号而非单一条件，基于视频扩散模型而非仅图像扩散，采用创新的轨迹策略，同时适用于伪影修复和稀疏视图重建任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GSFixer通过引入参考引导的双条件视频扩散模型和创新的轨迹采样策略，显著提升了3D高斯泼溅在稀疏视图场景下的重建质量和新视图合成的一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse viewsis an ill-posed problem due to insufficient information, often resulting innoticeable artifacts. While recent approaches have sought to leveragegenerative priors to complete information for under-constrained regions, theystruggle to generate content that remains consistent with input observations.To address this challenge, we propose GSFixer, a novel framework designed toimprove the quality of 3DGS representations reconstructed from sparse inputs.The core of our approach is the reference-guided video restoration model, builtupon a DiT-based video diffusion model trained on paired artifact 3DGS rendersand clean frames with additional reference-based conditions. Considering theinput sparse views as references, our model integrates both 2D semanticfeatures and 3D geometric features of reference views extracted from the visualgeometry foundation model, enhancing the semantic coherence and 3D consistencywhen fixing artifact novel views. Furthermore, considering the lack of suitablebenchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res whichcontains artifact frames rendered using low-quality 3DGS. Extensive experimentsdemonstrate our GSFixer outperforms current state-of-the-art methods in 3DGSartifact restoration and sparse-view 3D reconstruction. Project page:https://github.com/GVCLab/GSFixer.</description>
      <author>example@mail.com (Xingyilang Yin, Qi Zhang, Jiahao Chang, Ying Feng, Qingnan Fan, Xi Yang, Chi-Man Pun, Huaqi Zhang, Xiaodong Cun)</author>
      <guid isPermaLink="false">2508.09667v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges</title>
      <link>http://arxiv.org/abs/2508.09561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages. 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇调查论文分析了世界模型如何赋能边缘代理AI系统(EGI)，探讨了其架构基础、在多种边缘场景中的应用、与基础模型和数字孪生的协同作用，并指出了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;边缘通用智能(EGI)代表了边缘计算的演进，其中分布式代理能够在多样化动态环境中自主感知、推理和行动。世界模型作为核心组件，能够主动预测、想象未来轨迹、在不确定性下推理并规划多步骤行动。尽管世界模型在机器人和游戏领域已有应用，但在无线边缘与EGI的集成仍不充分。&lt;h4&gt;目的&lt;/h4&gt;这篇调查旨在填补世界模型在无线边缘与EGI集成方面的研究空白，提供世界模型如何赋能边缘代理AI系统的全面分析，并为实现下一代智能、自主边缘系统提供概念基础和实用路线图。&lt;h4&gt;方法&lt;/h4&gt;文章首先检查世界模型的架构基础，包括潜在表示学习、动态建模和基于想象的规划。然后展示这些核心能力在车联网、无人机网络、物联网系统和网络功能虚拟化等EGI场景中的应用，探讨它们与基础模型和数字孪生的协同作用，最后分析开放挑战和未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;世界模型作为主动的内部模拟器，能够增强边缘系统在延迟、能源和隐私约束下的优化能力；它们与基础模型和数字孪生协同作用，可作为EGI的认知支柱；当前面临安全保证、高效训练和受限部署等开放挑战。&lt;h4&gt;结论&lt;/h4&gt;这篇调查为世界模型在边缘通用智能中的应用提供了全面分析，包括架构基础、应用场景、协同作用和未来挑战，为实现下一代智能、自主边缘系统提供了概念基础和实用路线图。&lt;h4&gt;翻译&lt;/h4&gt;边缘通用智能(EGI)代表了边缘计算的变革性演进，其中分布式代理能够在多样化、动态的环境中自主感知、推理和行动。这一愿景的核心是世界模型，它们作为主动的内部模拟器，不仅能预测，还能主动想象未来轨迹、在不确定性下推理、并有远见地规划多步骤行动。这种主动性使代理能够预期潜在结果并在真实世界交互前优化决策。尽管机器人和游戏领域的前期工作已经展示了世界模型的潜力，但它们在无线边缘与EGI的集成仍未得到充分探索。这篇调查通过提供世界模型如何赋能边缘代理AI系统的全面分析来填补这一空白。我们首先检查世界模型的架构基础，包括潜在表示学习、动态建模和基于想象的规划。基于这些核心能力，我们说明了它们在车联网、无人机网络、物联网系统和网络功能虚拟化等EGI场景中的主动应用，从而展示了它们如何在延迟、能源和隐私约束下增强优化。然后我们探讨了它们与基础模型和数字孪生的协同作用，将世界模型定位为EGI的认知支柱。最后，我们强调了开放挑战，如安全保证、高效训练和受限部署，并概述了未来研究方向。这篇调查为实现下一代智能、自主边缘系统提供了概念基础和实用路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Edge General Intelligence (EGI) represents a transformative evolution of edgecomputing, where distributed agents possess the capability to perceive, reason,and act autonomously across diverse, dynamic environments. Central to thisvision are world models, which act as proactive internal simulators that notonly predict but also actively imagine future trajectories, reason underuncertainty, and plan multi-step actions with foresight. This proactive natureallows agents to anticipate potential outcomes and optimize decisions ahead ofreal-world interactions. While prior works in robotics and gaming haveshowcased the potential of world models, their integration into the wirelessedge for EGI remains underexplored. This survey bridges this gap by offering acomprehensive analysis of how world models can empower agentic artificialintelligence (AI) systems at the edge. We first examine the architecturalfoundations of world models, including latent representation learning, dynamicsmodeling, and imagination-based planning. Building on these core capabilities,we illustrate their proactive applications across EGI scenarios such asvehicular networks, unmanned aerial vehicle (UAV) networks, the Internet ofThings (IoT) systems, and network functions virtualization, therebyhighlighting how they can enhance optimization under latency, energy, andprivacy constraints. We then explore their synergy with foundation models anddigital twins, positioning world models as the cognitive backbone of EGI.Finally, we highlight open challenges, such as safety guarantees, efficienttraining, and constrained deployment, and outline future research directions.This survey provides both a conceptual foundation and a practical roadmap forrealizing the next generation of intelligent, autonomous edge systems.</description>
      <author>example@mail.com (Changyuan Zhao, Guangyuan Liu, Ruichen Zhang, Yinqiu Liu, Jiacheng Wang, Jiawen Kang, Dusit Niyato, Zan Li, Xuemin, Shen, Zhu Han, Sumei Sun, Chau Yuen, Dong In Kim)</author>
      <guid isPermaLink="false">2508.09561v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks</title>
      <link>http://arxiv.org/abs/2508.09532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层联邦微调框架，用于车联网系统中资源感知和移动弹性学习，通过LoRA和UCB-DUAL算法优化了精度与效率的权衡。&lt;h4&gt;背景&lt;/h4&gt;联邦微调是使基础模型适应边缘环境中多样化下游任务的有前途方法，但在车联网系统中，由于客户端移动性、异构资源和间歇性连接，实现高效低延迟的多任务适应特别具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一个协调路边单元和车辆的分层联邦微调框架，以支持动态车联网场景下的资源感知和移动弹性学习。&lt;h4&gt;方法&lt;/h4&gt;利用低秩适应(LoRA)引入去中心化、能耗感知的秩适应机制，将其表述为约束多臂老虎机问题；开发UCB-DUAL算法使每个任务在能源预算下进行自适应探索，实现次线性遗憾。&lt;h4&gt;主要发现&lt;/h4&gt;在基于真实轨迹构建的大规模车联网模拟器上进行的实验表明，该方法在所有基线中实现了最佳精度-效率权衡，将延迟减少了24%以上，将平均精度提高了2.5%以上。&lt;h4&gt;结论&lt;/h4&gt;该分层联邦微调框架能够有效处理车联网系统中的资源约束和移动性问题，同时保持高精度和低延迟。&lt;h4&gt;翻译&lt;/h4&gt;联邦微调已成为一种有前景的方法，用于在边缘环境中使基础模型适应多样化的下游任务。在车联网系统中，由于客户端移动性、异构资源和间歇性连接，实现高效低延迟的多任务适应特别具有挑战性。本文提出了一种分层联邦微调框架，协调路边单元和车辆，以支持动态车联网场景下的资源感知和移动弹性学习。利用低秩适应(LoRA)，我们引入了一种去中心化、能耗感知的秩适应机制，将其表述为约束多臂老虎机问题。开发了一种新的UCB-DUAL算法，使每个任务在能源预算下能够进行自适应探索，实现了可证明的次线性遗憾。为了评估我们的方法，我们构建了一个基于真实轨迹的大规模车联网模拟器，捕捉动态参与、路边单元切换和通信变化性。大量实验表明，我们的方法在所有基线中实现了最佳精度-效率权衡，将延迟减少了24%以上，并将平均精度提高了2.5%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated fine-tuning has emerged as a promising approach for adaptingfoundation models (FMs) to diverse downstream tasks in edge environments. InInternet of Vehicles (IoV) systems, enabling efficient and low-latencymulti-task adaptation is particularly challenging due to client mobility,heterogeneous resources, and intermittent connectivity. This paper proposes ahierarchical federated fine-tuning framework that coordinates roadside units(RSUs) and vehicles to support resource-aware and mobility-resilient learningacross dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), weintroduce a decentralized, energy-aware rank adaptation mechanism formulated asa constrained multi-armed bandit problem. A novel UCB-DUAL algorithm isdeveloped to enable adaptive exploration under per-task energy budgets,achieving provable sublinear regret. To evaluate our method, we construct alarge-scale IoV simulator based on real-world trajectories, capturing dynamicparticipation, RSU handoffs, and communication variability. Extensiveexperiments show that our approach achieves the best accuracy-efficiencytrade-off among all baselines, reducing latency by over 24\% and improvingaverage accuracy by more than 2.5\%.</description>
      <author>example@mail.com (Bokeng Zheng, Jianqiang Zhong, Jiayi Liu, Xiaoxi Zhang)</author>
      <guid isPermaLink="false">2508.09532v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Large-Small Model Collaborative Framework for Federated Continual Learning</title>
      <link>http://arxiv.org/abs/2508.09489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种联邦持续学习框架，通过轻量级本地模型作为动态桥梁，解决基础模型在本地下游任务上表现不佳以及学习新任务时忘记先验知识的问题。&lt;h4&gt;背景&lt;/h4&gt;持续学习对于基础模型是一个重要但尚未充分探索的挑战，特别是在联邦持续学习中，客户端面临严格的数据和通信限制。基础模型虽然具有强大的泛化能力，但无法有效利用私有本地数据，且由于参数量大和模型复杂度高，难以在不忘记先验知识的情况下学习新任务。&lt;h4&gt;目的&lt;/h4&gt;弥合小型模型和基础模型之间的差距，提出一种协作框架使基础模型能够有效利用本地数据并持续学习。&lt;h4&gt;方法&lt;/h4&gt;提出FCL中的第一个协作框架，其中轻量级本地模型作为动态桥梁，不断适应新任务同时增强大型模型的效用。包含两个新组件：小模型持续微调(防止小型模型随时间遗忘)和逐一蒸馏(在服务器上执行异构本地知识的个性化融合)。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证明了该框架的优越性能，即使在使用异构小型模型的客户端情况下也能取得良好效果。&lt;h4&gt;结论&lt;/h4&gt;该协作框架有效解决了联邦持续学习中基础模型面临的挑战，使基础模型能够利用本地数据并持续学习而不忘记先验知识。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)的持续学习(CL)是一个重要但尚未充分探索的挑战，特别是在联邦持续学习(FCL)中，每个客户端在严格的数据和通信约束下，从私有、演化的任务流中学习。尽管基础模型具有强大的泛化能力，但它们在本地下游任务上通常表现不佳，因为它们无法利用私有本地数据。此外，使基础模型学习新任务而不忘记先验知识本质上是一个具有挑战性的问题，主要由于其巨大的参数量和高模型复杂度。相比之下，小型模型可以在资源受限条件下进行本地训练，并受益于更成熟的持续学习技术。为了弥合小型模型和基础模型之间的差距，我们提出了FCL中的第一个协作框架，其中轻量级本地模型作为动态桥梁，不断适应新任务同时增强大型模型的效用。还包括两个新组件：小模型持续微调用于防止小型模型随时间遗忘；逐一蒸馏在服务器上执行异构本地知识的个性化融合。实验结果证明了其优越的性能，即使客户端使用异构的小型模型也是如此。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning (CL) for Foundation Models (FMs) is an essential yetunderexplored challenge, especially in Federated Continual Learning (FCL),where each client learns from a private, evolving task stream under strict dataand communication constraints. Despite their powerful generalization abilities,FMs often exhibit suboptimal performance on local downstream tasks, as they areunable to utilize private local data. Furthermore, enabling FMs to learn newtasks without forgetting prior knowledge is inherently a challenging problem,primarily due to their immense parameter count and high model complexity. Incontrast, small models can be trained locally under resource-constrainedconditions and benefit from more mature CL techniques. To bridge the gapbetween small models and FMs, we propose the first collaborative framework inFCL, where lightweight local models act as a dynamic bridge, continuallyadapting to new tasks while enhancing the utility of the large model. Two novelcomponents are also included: Small Model Continual Fine-tuning is forpreventing small models from temporal forgetting; One-by-One Distillationperforms personalized fusion of heterogeneous local knowledge on the server.Experimental results demonstrate its superior performance, even when clientsutilize heterogeneous small models.</description>
      <author>example@mail.com (Hao Yu, Xin Yang, Boyang Fan, Xuemei Cao, Hanlin Gu, Lixin Fan, Qiang Yang)</author>
      <guid isPermaLink="false">2508.09489v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models</title>
      <link>http://arxiv.org/abs/2508.09471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EGGS-PTP是一种基于扩展图引导的结构化剪枝方法，可以有效解决大语言模型部署中的计算和内存挑战。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型(LLMs)被更广泛采用和规模扩大，部署这些大规模基础模型面临的计算和内存挑战日益严重。&lt;h4&gt;目的&lt;/h4&gt;开发更高效的模型变体，以应对大语言模型部署中的计算和内存挑战。&lt;h4&gt;方法&lt;/h4&gt;提出EGGS-PTP（基于扩展图引导的结构化剪枝方法），利用图理论指导N:M结构化剪枝的设计，有效减少模型大小和计算需求，并通过引入扩展图概念确保剪枝网络中的信息流，保留基本模型功能。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的数值实验表明，EGGS-PTP不仅通过结构化稀疏性实现了显著的加速和内存节省，而且在各种LLM上的准确性方面优于现有的结构化剪枝技术。&lt;h4&gt;结论&lt;/h4&gt;EGGS-PTP是一种有效的方法，可以在减少模型大小和计算需求的同时保持或提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型(LLMs)被更广泛采用和规模扩大，部署这些大规模基础模型所涉及的计算和内存挑战日益严峻。这凸显了开发更高效模型变体的迫切需求。面对这一挑战，本研究提出了EGGS-PTP：一种基于扩展图引导的结构化剪枝方法。该方法利用图理论指导N:M结构化剪枝的设计，有效减少模型大小和计算需求。通过引入扩展图概念，EGGS-PTP确保剪枝网络中的信息流动，保留基本模型功能。大量数值实验表明，EGGS-PTP不仅通过结构化稀疏性实现了显著的加速和内存节省，而且在各种LLM的准确性方面优于现有的结构化剪枝技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Large Language Models (LLMs) become more widely adopted and scale up insize, the computational and memory challenges involved in deploying thesemassive foundation models have grown increasingly severe. This underscores theurgent need to develop more efficient model variants. Faced with thischallenge, the present work introduces EGGS-PTP: an Expander-Graph GuidedStructured Post-training Pruning method. The proposed approach leverages graphtheory to guide the design of N:M structured pruning, effectively reducingmodel size and computational demands. By incorporating concepts from expandergraphs, EGGS-PTP ensures information flow within the pruned network, preservingessential model functionality. Extensive numerical experiments demonstrate thatEGGS-PTP not only achieves significant acceleration and memory savings due tostructured sparsity but also outperforms existing structured pruning techniquesin terms of accuracy across various LLMs.</description>
      <author>example@mail.com (Omar Bazarbachi, Zijun Sun, Yanning Shen)</author>
      <guid isPermaLink="false">2508.09471v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss</title>
      <link>http://arxiv.org/abs/2508.09453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HyperKD的新型知识蒸馏框架，用于解决基础模型在高光谱遥感领域的应用挑战，实现了不同类型光谱数据之间的逆向知识转移，显著提高了模型在高光谱图像上的表现。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大规模无标签数据集上的预训练已成为创建可适应和可重用架构的有效方法，可用于各种下游任务包括卫星观测。然而，将这些模型直接应用于高光谱遥感仍面临挑战，因为存在固有的光谱差异和可用观测数据的稀缺性。&lt;h4&gt;目的&lt;/h4&gt;提出HyperKD框架，使教师模型学习的表示能够转移到学生模型中，有效地在高光谱图像上开发基础模型，弥合光谱域差距，使预训练基础模型能够有效用于地理空间应用。&lt;h4&gt;方法&lt;/h4&gt;HyperKD基于掩码自编码器，实现了不同类型光谱数据之间的逆向知识转移，由更简单的教师模型指导。该方法通过引入基于特征的策略解决逆向域适应问题，包括基于光谱范围的通道对齐、空间特征引导的掩码，以及为高光谱图像量身定制的增强损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，HyperKD显著提高了MAE中的表示学习，提高了重建保真度，并在下游任务上表现更加稳健，包括土地覆盖分类、作物类型识别和土壤有机碳预测。&lt;h4&gt;结论&lt;/h4&gt;知识蒸馏框架在高光谱图像的遥感分析中具有巨大潜力，HyperKD为有效利用预训练基础模型解决高光谱遥感问题提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在大规模无标签数据集上的激增已成为创建可适应和可重用架构的有效方法，这些架构可以被利用于各种使用卫星观测的下游任务。然而，由于固有的光谱差异和可用观测数据的稀缺性，它们在高光谱遥感的直接应用仍然具有挑战性。在这项工作中，我们提出了HyperKD，一种新颖的知识蒸馏框架，使教师模型学习的表示能够转移到学生模型中，以便在高光谱图像上有效开发基础模型。与使用复杂教师指导简单学生的典型知识蒸馏框架不同，HyperKD实现了不同类型光谱数据之间的逆向知识转移，由更简单的教师模型指导。基于掩码自编码器，HyperKD将Prithvi基础模型的知识蒸馏到专门为EnMAP高光谱图像设计的学生模型中。HyperKD通过引入基于特征的策略解决光谱差距的逆向域适应问题，包括基于光谱范围的通道对齐、空间特征引导的掩码，以及为高光谱图像量身定制的增强损失函数。HyperKD弥合了巨大的光谱域差距，使预训练基础模型能够有效用于地理空间应用。大量实验表明，HyperKD显著提高了MAE中的表示学习，提高了重建保真度，并在土地覆盖分类、作物类型识别和土壤有机碳预测等下游任务上表现更加稳健，证明了知识蒸馏框架在高光谱图像遥感分析中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of foundation models, pretrained on large-scale unlabeleddatasets, has emerged as an effective approach in creating adaptable andreusable architectures that can be leveraged for various downstream tasks usingsatellite observations. However, their direct application to hyperspectralremote sensing remains challenging due to inherent spectral disparities and thescarcity of available observations. In this work, we present HyperKD, a novelknowledge distillation framework that enables transferring learnedrepresentations from a teacher model into a student model for effectivedevelopment of a foundation model on hyperspectral images. Unlike typicalknowledge distillation frameworks, which use a complex teacher to guide asimpler student, HyperKD enables an inverse form of knowledge transfer acrossdifferent types of spectral data, guided by a simpler teacher model. Buildingupon a Masked Autoencoder, HyperKD distills knowledge from the Prithvifoundational model into a student tailored for EnMAP hyperspectral imagery.HyperKD addresses the inverse domain adaptation problem with spectral gaps byintroducing a feature-based strategy that includes spectral range-based channelalignment, spatial feature-guided masking, and an enhanced loss functiontailored for hyperspectral images. HyperKD bridges the substantial spectraldomain gap, enabling the effective use of pretrained foundation models forgeospatial applications. Extensive experiments show that HyperKD significantlyimproves representation learning in MAEs, leading to enhanced reconstructionfidelity and more robust performance on downstream tasks such as land coverclassification, crop type identification, and soil organic carbon prediction,underpinning the potential of knowledge distillation frameworks in remotesensing analytics with hyperspectral imagery.</description>
      <author>example@mail.com (Abdul Matin, Tanjim Bin Faruk, Shrideep Pallickara, Sangmi Lee Pallickara)</author>
      <guid isPermaLink="false">2508.09453v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata</title>
      <link>http://arxiv.org/abs/2508.09415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the ICCV'25 Workshop on Vision Foundation Models and  Generative AI for Accessibility: Challenges and Opportunities&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为RampNet的两阶段管道，用于大规模路缘坡道检测数据集的构建和模型性能提升。研究团队通过自动转换政府提供的路缘坡道位置数据生成了超过21万张标注的Google街景全景图像，并基于此数据集训练了一个改进的ConvNeXt V2检测模型，达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;路缘坡道对城市无障碍设施至关重要，但在图像中稳健地检测它们仍然是一个开放问题，主要是由于缺乏大规模、高质量的数据集。先前的工作尝试通过众包或手动标记的数据来提高数据可用性，但这些努力在质量或规模上往往不尽如人意。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在构建一个大规模、高质量的路缘坡道检测数据集，并开发一个高性能的检测模型，以提高城市无障碍设施的可及性。&lt;h4&gt;方法&lt;/h4&gt;研究采用两阶段管道RampNet：第一阶段通过自动转换政府提供的路缘坡道位置数据，生成了超过21万张标注的Google街景全景图像数据集；第二阶段使用生成的数据集训练一个改进的ConvNeXt V2模型进行路缘坡道检测；评估阶段将生成的数据集和检测模型与手动标记的全景图像进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;生成的数据集达到了94.0%的精确度和92.5%的召回率；检测模型达到了0.9236的平均精度(AP)，显著超过了先前的工作；该工作贡献了首个大规模、高质量的路缘坡道检测数据集、基准和模型。&lt;h4&gt;结论&lt;/h4&gt;RampNet两阶段管道成功解决了路缘坡道检测中大规模高质量数据集缺乏的问题，通过自动转换政府数据生成了大规模标注数据集，并训练出高性能检测模型，为城市无障碍设施评估提供了新工具。&lt;h4&gt;翻译&lt;/h4&gt;路缘坡道对城市无障碍设施至关重要，但在图像中稳健地检测它们仍然是一个开放问题，主要是由于缺乏大规模、高质量的数据集。虽然先前的工作试图通过众包或手动标记的数据来提高数据可用性，但这些努力在质量或规模上往往不尽如人意。在本文中，我们介绍并评估了一个名为RampNet的两阶段管道，以扩展路缘坡道检测数据集并提高模型性能。在第一阶段，我们通过自动转换政府提供的路缘坡道位置数据到全景图像中的像素坐标，生成了一个包含超过21万张标注的Google街景(GSV)全景图像的数据集。在第二阶段，我们从生成的数据集训练一个路缘坡道检测模型(改进的ConvNeXt V2)，达到了最先进的性能。为了评估我们管道的两个阶段，我们与手动标记的全景图像进行了比较。我们生成的数据集达到了94.0%的精确度和92.5%的召回率，我们的检测模型达到了0.9236的平均精度(AP)——远远超过了先前的工作。我们的工作贡献了首个大规模、高质量的路缘坡道检测数据集、基准和模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Curb ramps are critical for urban accessibility, but robustly detecting themin images remains an open problem due to the lack of large-scale, high-qualitydatasets. While prior work has attempted to improve data availability withcrowdsourced or manually labeled data, these efforts often fall short in eitherquality or scale. In this paper, we introduce and evaluate a two-stage pipelinecalled RampNet to scale curb ramp detection datasets and improve modelperformance. In Stage 1, we generate a dataset of more than 210,000 annotatedGoogle Street View (GSV) panoramas by auto-translating government-provided curbramp location data to pixel coordinates in panoramic images. In Stage 2, wetrain a curb ramp detection model (modified ConvNeXt V2) from the generateddataset, achieving state-of-the-art performance. To evaluate both stages of ourpipeline, we compare to manually labeled panoramas. Our generated datasetachieves 94.0% precision and 92.5% recall, and our detection model reaches0.9236 AP -- far exceeding prior work. Our work contributes the firstlarge-scale, high-quality curb ramp detection dataset, benchmark, and model.</description>
      <author>example@mail.com (John S. O'Meara, Jared Hwang, Zeyu Wang, Michael Saugstad, Jon E. Froehlich)</author>
      <guid isPermaLink="false">2508.09415v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders</title>
      <link>http://arxiv.org/abs/2508.09363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了领域受限稀疏自编码器在医学文本中的应用，发现这种方法比广泛领域训练的SAEs能更好地捕捉领域特定特征，提高模型的可解释性和重构保真度。&lt;h4&gt;背景&lt;/h4&gt;传统稀疏自编码器在广泛数据分布上训练，使用固定潜在预算只捕获高频通用模式，导致重构误差中存在显著的线性'暗物质'，并产生碎片化或相互吸收的潜在特征，使解释复杂化。&lt;h4&gt;目的&lt;/h4&gt;研究将SAE训练限制在特定领域(医学文本)是否能改善模型的性能和可解释性，以及如何缓解广泛领域SAEs的关键局限性。&lt;h4&gt;方法&lt;/h4&gt;使用195k临床问答示例在Gemma-2模型的第20层激活上训练JumpReLU稀疏自编码器，并与广泛领域训练的SAEs进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;领域受限的SAEs比广泛领域的SAEs能解释多达20%的方差，实现更高的损失恢复，减少线性残差误差；学习到的特征与临床上有意义的概念对齐，而非频繁但信息量少的标记；这些SAEs捕获相关的线性结构，留下更小、更纯粹的非线性残差。&lt;h4&gt;结论&lt;/h4&gt;领域限制缓解了广泛领域SAEs的关键局限性，实现了更完整和可解释的潜在分解，暗示该领域可能需要质疑用于通用SAEs的'基础模型'扩展。&lt;h4&gt;翻译&lt;/h4&gt;稀疏自编码器将大语言模型的激活分解为揭示机制结构的潜在特征。传统稀疏自编码器在广泛的数据分布上训练，迫使固定的潜在预算只能捕获高频、通用模式。这通常导致重构误差中存在显著的线性'暗物质'，并产生碎片化或相互吸收的潜在特征，使解释复杂化。我们证明，将SAE训练限制在明确定义的领域(医学文本)可以将容量重新分配到领域特定特征，提高重构保真度和可解释性。使用195k临床问答示例在Gemma-2模型的第20层激活上训练JumpReLU SAEs，我们发现领域受限的SAEs比广泛领域的SAEs能解释多达20%的方差，实现更高的损失恢复，并减少线性残差误差。自动和人工评估确认，学习到的特征与临床上有意义的概念(如'味觉感觉'或'传染性单核细胞增多症')对齐，而不是频繁但信息量少的标记。这些领域特定的SAEs捕获相关的线性结构，留下更小、更纯粹的非线性残差。我们得出结论，领域限制缓解了广泛领域SAEs的关键局限性，实现了更完整和可解释的潜在分解，并暗示该领域可能需要质疑用于通用SAEs的'基础模型'扩展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse autoencoders (SAEs) decompose large language model (LLM) activationsinto latent features that reveal mechanistic structure. Conventional SAEs trainon broad data distributions, forcing a fixed latent budget to capture onlyhigh-frequency, generic patterns. This often results in significant linear``dark matter'' in reconstruction error and produces latents that fragment orabsorb each other, complicating interpretation. We show that restricting SAEtraining to a well-defined domain (medical text) reallocates capacity todomain-specific features, improving both reconstruction fidelity andinterpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2models using 195k clinical QA examples, we find that domain-confined SAEsexplain up to 20\% more variance, achieve higher loss recovery, and reducelinear residual error compared to broad-domain SAEs. Automated and humanevaluations confirm that learned features align with clinically meaningfulconcepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), ratherthan frequent but uninformative tokens. These domain-specific SAEs capturerelevant linear structure, leaving a smaller, more purely nonlinear residual.We conclude that domain-confinement mitigates key limitations of broad-domainSAEs, enabling more complete and interpretable latent decompositions, andsuggesting the field may need to question ``foundation-model'' scaling forgeneral-purpose SAEs.</description>
      <author>example@mail.com (Charles O'Neill, Mudith Jayasekara, Max Kirkby)</author>
      <guid isPermaLink="false">2508.09363v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Aryabhata: An exam-focused language model for JEE Math</title>
      <link>http://arxiv.org/abs/2508.08665v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Aryabhata 1.0，一个针对印度联合入学考试(JEE)优化的70亿参数数学推理模型，通过合并开源模型、监督微调和强化学习等方法，在多个基准测试上表现出色，并提供教育友好的逐步推理。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型迅速发展，但当前模型通常仍不适合教育用途。印度联合入学考试(JEE)是重要的学术考试，需要专门的数学推理能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对JEE考试优化的高效准确的数学推理模型，提供教育友好的逐步推理，并作为开源模型发布促进相关研究。&lt;h4&gt;方法&lt;/h4&gt;合并强大的开源推理模型，使用经过最佳拒绝采样验证的思维链痕迹进行监督微调和课程学习，应用可验证奖励的强化学习(A2C目标与组相对优势估计)，实施自适应组大小调整和温度缩放等探索策略。&lt;h4&gt;主要发现&lt;/h4&gt;Aryabhata 1.0在分布内(JEE Main 2025)和分布外(MATH, GSM8K)基准测试上均优于现有模型，在准确性和效率方面表现出色，并提供教育有用的逐步推理。&lt;h4&gt;结论&lt;/h4&gt;Aryabhata作为面向考试的开源小型语言模型基础模型发布，PhysicsWallahAI正在积极训练未来模型以进一步改善学生学习成果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Aryabhata 1.0，这是一个针对印度联合入学考试(JEE)优化的紧凑型70亿参数数学推理模型。尽管大型语言模型迅速发展，但当前模型通常仍不适合教育用途。Aryabhata 1.0是通过合并强大的开源权重推理模型构建的，随后使用通过最佳拒绝采样验证的思维链痕迹进行课程学习和监督微调。为了进一步提高性能，我们应用了使用A2C目标和组相对优势估计的可验证奖励强化学习，以及自适应组大小调整和温度缩放等新的探索策略。在分布内和分布外基准测试上评估，Aryabhata在准确性和效率方面均优于现有模型，同时提供教育有用的逐步推理。我们将Aryabhata作为基础模型发布，以推进面向考试的开源小型语言模型。这是我们首次开放发布供社区反馈；PhysicsWallahAI正在积极训练未来模型，以进一步改善学生的学习成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Aryabhata 1.0, a compact 7B parameter math reasoning modeloptimized for the Indian academic exam, the Joint Entrance Examination (JEE).Despite rapid progress in large language models (LLMs), current models oftenremain unsuitable for educational use. Aryabhata 1.0 is built by merging strongopen-weight reasoning models, followed by supervised fine-tuning (SFT) withcurriculum learning on verified chain-of-thought (CoT) traces curated throughbest-of-$n$ rejection sampling. To further boost performance, we applyreinforcement learning with verifiable rewards (RLVR) using A2C objective withgroup-relative advantage estimation along with novel exploration strategiessuch as Adaptive Group Resizing and Temperature Scaling. Evaluated on bothin-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K)benchmarks, Aryabhata outperforms existing models in accuracy and efficiency,while offering pedagogically useful step-by-step reasoning. We releaseAryabhata as a foundation model to advance exam-centric, open-source smalllanguage models. This marks our first open release for community feedback(https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0); PW is actively trainingfuture models to further improve learning outcomes for students.</description>
      <author>example@mail.com (Ritvik Rastogi, Sachin Dharashivkar, Sandeep Varma)</author>
      <guid isPermaLink="false">2508.08665v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Towards Scalable Training for Handwritten Mathematical Expression Recognition</title>
      <link>http://arxiv.org/abs/2508.09220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决手写数学表达式识别(HMER)领域数据稀缺问题的新方法，通过开发可扩展数据引擎生成LaTeX序列，构建了包含8000万实例的最大公式数据集Tex80M，并提出了首个大规模训练的HMER模型TexTeller，在几乎所有基准测试中达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型通过大规模数据集训练取得显著性能提升，但HMER领域因数据稀缺而发展受限，主要是由于手动标注过程困难和昂贵。&lt;h4&gt;目的&lt;/h4&gt;解决HMER领域数据稀缺问题，通过结合有限手写公式与大规模LaTeX渲染公式来提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;开发可扩展数据引擎生成复杂一致的LaTeX序列；构建包含8000万高质量训练实例的Tex80M数据集；提出TexTeller模型，通过混合训练Tex80M和小规模HME数据集进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;大规模训练数据集和改进的流程使TexTeller在几乎所有基准测试中都达到了最先进(SOTA)性能。&lt;h4&gt;结论&lt;/h4&gt;将开放发布完整模型、整个数据集和完整代码库，促进基于其贡献的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型通过大规模数据集的可扩展训练已取得显著的性能提升。然而，手写数学表达式识别(HMER)领域因数据稀缺而受到阻碍，这主要是由于手动标注过程困难和昂贵。为了弥补这一差距，我们提出了一种新方法，通过开发可扩展数据引擎将有限的手写公式与大规模LaTeX渲染的公式结合起来，生成复杂且一致的LaTeX序列。利用该引擎，我们构建了迄今为止最大的公式数据集，称为Tex80M，包含超过8000万个高质量训练实例。然后我们提出了TexTeller，这是首个大规模训练的HMER模型，通过将Tex80M与相对较小的HME数据集混合训练。广泛的训练数据集和我们改进的流程使TexTeller在几乎所有基准测试中都达到了最先进的(SOTA)性能。为了推动该领域发展，我们将开放发布完整模型、整个数据集和完整代码库，促进基于我们贡献的进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large foundation models have achieved significant performance gains throughscalable training on massive datasets. However, the field of\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarilydue to the arduous and costly process of manual annotation. To bridge this gap,we propose a novel method integrating limited handwritten formulas withlarge-scale LaTeX-rendered formulas by developing a scalable data engine togenerate complex and consistent LaTeX sequences. With this engine, we built thelargest formula dataset to date, termed \texttt{Tex80M}, comprising over 80million high-quality training instances. Then we propose \texttt{TexTeller},the first HMER model trained at scale, by mix-training \texttt{Tex80M} with arelatively small HME dataset. The expansive training dataset and our refinedpipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)performance across nearly all benchmarks. To advance the field, we will openlyrelease our complete model, entire dataset, and full codebase, enabling furtherresearch building upon our contributions.</description>
      <author>example@mail.com (Haoyang Li, Jiaqing Li, Jialun Cao, Zongyuan Yang, Yongping Xiong)</author>
      <guid isPermaLink="false">2508.09220v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>VGGSounder: Audio-Visual Evaluations for Foundation Models</title>
      <link>http://arxiv.org/abs/2508.08237v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the IEEE/CVF International Conference on Computer  Vision (ICCV) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VGGSounder，一个改进的视听评估数据集，解决了原VGGSound数据集的标注不完整、类别重叠和模态未对齐等问题，为评估视听基础模型提供了更可靠的工具。&lt;h4&gt;背景&lt;/h4&gt;视听基础模型的兴起凸显了可靠评估其多模态理解能力的重要性，VGGSound数据集常被用作视听分类的基准评估工具。&lt;h4&gt;目的&lt;/h4&gt;解决VGGSound数据集的局限性，创建一个更可靠的评估工具来准确评估视听基础模型的多模态理解能力。&lt;h4&gt;方法&lt;/h4&gt;创建VGGSounder，一个全面重新注释的多标签测试集，扩展VGGSound数据集，并引入模态混淆指标来分析模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;VGGSound数据集存在标注不完整、类别部分重叠和模态未对齐等局限性，导致视听能力评估失真；通过分析添加另一输入模态时的性能下降，揭示了模型的局限性。&lt;h4&gt;结论&lt;/h4&gt;VGGSounder通过详细模态注释和设计，能够提供更精确的模态特定性能分析，为评估视听基础模型提供了更可靠的基准。&lt;h4&gt;翻译&lt;/h4&gt;视听基础模型的出现凸显了可靠评估其多模态理解能力的重要性。VGGSound数据集通常被用作视听分类的评估基准。然而，我们的分析发现了VGGSound的几个局限性，包括标注不完整、类别部分重叠和模态未对齐。这些问题导致了对视听能力的扭曲评估。为了解决这些局限性，我们引入了VGGSounder，这是一个全面重新注释的多标签测试集，扩展了VGGSound，并专门设计用于评估视听基础模型。VGGSounder具有详细的模态注释，能够精确分析特定模态的性能。此外，我们通过使用新的模态混淆指标分析添加另一个输入模态时的性能下降，揭示了模型的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of audio-visual foundation models underscores the importance ofreliably assessing their multi-modal understanding. The VGGSound dataset iscommonly used as a benchmark for evaluation audio-visual classification.However, our analysis identifies several limitations of VGGSound, includingincomplete labelling, partially overlapping classes, and misaligned modalities.These lead to distorted evaluations of auditory and visual capabilities. Toaddress these limitations, we introduce VGGSounder, a comprehensivelyre-annotated, multi-label test set that extends VGGSound and is specificallydesigned to evaluate audio-visual foundation models. VGGSounder featuresdetailed modality annotations, enabling precise analyses of modality-specificperformance. Furthermore, we reveal model limitations by analysing performancedegradation when adding another input modality with our new modality confusionmetric.</description>
      <author>example@mail.com (Daniil Zverev, Thaddäus Wiedemer, Ameya Prabhu, Matthias Bethge, Wieland Brendel, A. Sophia Koepke)</author>
      <guid isPermaLink="false">2508.08237v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>MolmoAct: Action Reasoning Models that can Reason in Space</title>
      <link>http://arxiv.org/abs/2508.07917v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Appendix include. Code, Data and Weights:  https://allenai.org/blog/molmoact&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了行动推理模型(ARMs)，一种通过结构化三阶段管道整合感知、规划和控制的机器人基础模型。MolmoAct作为代表模型，在多个基准测试中展现出优异性能，并首次发布了包含10,000多个高质量机器人轨迹的MolmoAct数据集。&lt;h4&gt;背景&lt;/h4&gt;推理是目标行动的核心，但大多数机器人基础模型直接将感知和指令映射到控制，这限制了适应性、泛化和语义基础。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够通过结构化推理将感知转变为目标行动的机器人基础模型，提高机器人的适应性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;MolmoAct模型采用三阶段处理：1)将观察和指令编码为深度感知令牌；2)生成中级空间计划作为可编辑的轨迹痕迹；3)预测精确的低级行动，使行为可解释和可操控。&lt;h4&gt;主要发现&lt;/h4&gt;MolmoAct-7B-D在SimplerEnv视觉匹配任务上达到70.5%零样本准确率；在LIBERO上达到86.6%平均成功率；真实世界微调中比Pi-0-FAST提高10%(单臂)和22.7%(双臂)；分布外泛化上提高23.3%；使用MolmoAct数据集训练比基础模型提高5.5%性能。&lt;h4&gt;结论&lt;/h4&gt;通过发布模型权重、训练代码和数据集，MolmoAct成为最先进的机器人基础模型，并为构建通过结构化推理将感知转变为目标行动的ARMs提供了开放蓝图。&lt;h4&gt;翻译&lt;/h4&gt;推理是目标行动的核心，然而大多数机器人基础模型直接将感知和指令映射到控制，这限制了适应性、泛化和语义基础。我们引入了行动推理模型(ARMs)，一类通过结构化三阶段管道整合感知、规划和控制的机器人基础模型。我们的模型MolmoAct将观察和指令编码为深度感知令牌，生成中级空间计划作为可编辑的轨迹痕迹，并预测精确的低级行动，使行为可解释和可操控。MolmoAct-7B-D在模拟和真实世界环境中表现出色：在SimplerEnv视觉匹配任务上达到70.5%的零样本准确率，优于闭源Pi-0和GR00T N1；在LIBERO上达到86.6%的平均成功率，比ThinkAct在长距离任务上额外提高6.3%；在真实世界微调中，比Pi-0-FAST额外提高10%（单臂）和22.7%（双臂）任务进展；在分布外泛化上比基线额外提高23.3%；在开放指令跟随和轨迹操控方面获得最高的人类偏好分数。此外，我们首次发布了MolmoAct数据集——包含超过10,000个跨多样场景和任务的高质量机器人轨迹的中期训练机器人数据集。使用此数据集训练比基础模型平均提高5.5%的总体性能。我们发布了所有模型权重、训练代码、收集的数据集和行动推理数据集，将MolmoAct建立为最先进的机器人基础模型，以及构建通过结构化推理将感知转变为目标行动的ARMs的开放蓝图。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人基础模型直接将感知和指令映射到控制的问题，这限制了机器人的适应性、泛化能力和语义理解。这个问题很重要，因为机器人需要像人类一样能够推理而不仅仅是执行任务，才能在复杂环境中灵活应对各种情况。当前的视觉-语言-动作模型(VLA)虽然取得进展，但仍存在脆弱、不透明、难以跨任务泛化等局限性，无法满足实际应用需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类行为中获得启发，人类在行动前会潜意识地权衡上下文、目标和约束。他们认为机器人也需要学习推理而非简单映射。设计上借鉴了语言模型从蛮力扩展向结构化学习转变的趋势，构建中间表示来支持推理。具体借鉴了Molmo多模态模型作为视觉-语言骨干，RT-1等工作的动作离散化方法，以及链式思维(CoT)在语言模型中的应用。但作者创新性地将推理从语言空间转移到空间本身，设计了三阶段推理管道。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构化的三阶段推理管道整合感知、规划和控制，让模型'在空间中推理'而非仅通过语言推理。整体流程为：1)深度感知标记生成：将观察和指令编码成深度感知标记，重建3D环境；2)视觉推理轨迹生成：生成中级空间计划作为可编辑的轨迹痕迹；3)动作预测：基于前两步结果预测精确的低级动作。训练分为预训练、中间训练和后训练三个阶段，分别在不同数据集上进行训练和微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)结构化的三阶段推理管道；2)在空间而非仅通过语言进行推理；3)引入深度感知标记增强3D理解；4)视觉推理轨迹作为可解释中间表示；5)通过视觉轨迹实现交互式引导；6)发布MolmoAct数据集；7)完全开源模型、代码和数据。相比之前工作，不同之处在于：不直接映射感知到控制，而是通过中间推理步骤；不使用语言作为推理媒介，直接在空间推理；提供比语言引导更精确的视觉轨迹引导；使用更少数据实现更好性能；提供完整开源方案而非封闭模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MolmoAct引入了一种新型的动作推理模型，通过结构化的空间推理管道整合感知、规划和控制，实现了可解释、可引导的机器人行为，并在多个基准测试中取得了最先进的结果，同时完全开源了模型、代码和数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning is central to purposeful action, yet most robotic foundation modelsmap perception and instructions directly to control, which limits adaptability,generalization, and semantic grounding. We introduce Action Reasoning Models(ARMs), a class of robotic foundation models that integrate perception,planning, and control through a structured three-stage pipeline. Our model,MolmoAct, encodes observations and instructions into depth-aware perceptiontokens, generates mid-level spatial plans as editable trajectory traces, andpredicts precise low-level actions, enabling explainable and steerablebehavior. MolmoAct-7B-D achieves strong performance across simulation andreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matchingtasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success onLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;and in real-world fine-tuning, an additional 10% (single-arm) and an additional22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselinesby an additional 23.3% on out-of-distribution generalization and achieves tophuman-preference scores for open-ended instruction following and trajectorysteering. Furthermore, we release, for the first time, the MolmoAct Dataset --a mid-training robot dataset comprising over 10,000 high quality robottrajectories across diverse scenarios and tasks. Training with this datasetyields an average 5.5% improvement in general performance over the base model.We release all model weights, training code, our collected dataset, and ouraction reasoning dataset, establishing MolmoAct as both a state-of-the-artrobotics foundation model and an open blueprint for building ARMs thattransform perception into purposeful action through structured reasoning.Blogpost: https://allenai.org/blog/molmoact</description>
      <author>example@mail.com (Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna)</author>
      <guid isPermaLink="false">2508.07917v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Episodic Memory Representation for Long-form Video Understanding</title>
      <link>http://arxiv.org/abs/2508.09486v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Video-EM是一种受人类情景记忆启发的无需训练框架，将关键帧建模为时序有序的情景事件而非孤立实体，结合链式思维方法识别最小但信息丰富的帧子集，在减少帧数的同时提高了Video-LLMs对长视频的理解和问答能力。&lt;h4&gt;背景&lt;/h4&gt;Video-LLMs在视频理解方面表现出色，但因上下文窗口限制难以处理长视频。现有方法通过关键帧检索简化问题，但忽略了时空关系且可能产生冗余关键帧，稀释了重要线索。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，引入Video-EM框架促进稳健且上下文基础的推理，准确捕捉视频中的时空关系和叙事连续性。&lt;h4&gt;方法&lt;/h4&gt;将关键帧明确建模为时序有序的情景事件，捕捉空间关系和时序动态；利用链式思维(CoT)与LLMs迭代识别最小但信息丰富的情景子集，使Video-LLMs能高效准确回答问题。&lt;h4&gt;主要发现&lt;/h4&gt;在Video-MME、EgoSchema、HourVideo和LVBench基准测试上，Video-EM相比基线实现了4-9%的性能提升，同时使用更少的帧。&lt;h4&gt;结论&lt;/h4&gt;Video-EM有效解决了长视频理解中的关键挑战，通过模拟人类情景记忆方式更好地捕捉视频时空关系，在减少帧数的同时提高了性能。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(Vedio-LLMs)在通用视频理解方面表现出色，但由于上下文窗口限制，难以处理长视频。因此，最近的方法专注于关键帧检索，将冗长的视频压缩为一小组信息丰富的帧。尽管这些方法实用，但它们将问题简化为静态文本图像匹配，忽略了捕捉场景转换和上下文连续性至关重要的时空关系，并且可能产生信息有限且冗余的关键帧，稀释了准确视频问答所需的重要线索。为解决这些限制，我们引入了Video-EM，这是一个受人类情景记忆原理启发的无需训练的框架，旨在促进稳健且上下文基础的推理。Video-EM不将关键帧视为孤立的视觉实体，而是明确地将它们建模为时序有序的情景事件，捕捉准确重建底层叙事所需的空间关系和时序动态。此外，该框架利用LLMs的链式思维(CoT)迭代识别最小但信息丰富的情景子集，使Video-LLMs能够高效准确地回答问题。在Video-MME、EgoSchema、HourVideo和LVBench基准上的广泛评估证实了Video-EM的优越性，相比相应基线实现了4-9%的性能提升，同时使用更少的帧。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (Video-LLMs) excel at general video understandingbut struggle with long-form videos due to context window limits. Consequently,recent approaches focus on keyframe retrieval, condensing lengthy videos into asmall set of informative frames. Despite their practicality, these methodssimplify the problem to static text image matching, overlooking spatio temporalrelationships crucial for capturing scene transitions and contextualcontinuity, and may yield redundant keyframes with limited information,diluting salient cues essential for accurate video question answering. Toaddress these limitations, we introduce Video-EM, a training free frameworkinspired by the principles of human episodic memory, designed to facilitaterobust and contextually grounded reasoning. Rather than treating keyframes asisolated visual entities, Video-EM explicitly models them as temporally orderedepisodic events, capturing both spatial relationships and temporal dynamicsnecessary for accurately reconstructing the underlying narrative. Furthermore,the framework leverages chain of thought (CoT) thinking with LLMs toiteratively identify a minimal yet highly informative subset of episodicmemories, enabling efficient and accurate question answering by Video-LLMs.Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBenchbenchmarks confirm the superiority of Video-EM, which achieves highlycompetitive results with performance gains of 4-9 percent over respectivebaselines while utilizing fewer frames.</description>
      <author>example@mail.com (Yun Wang, Long Zhang, Jingren Liu, Jiaqi Yan, Zhanjie Zhang, Jiahao Zheng, Xun Yang, Dapeng Wu, Xiangyu Chen, Xuelong Li)</author>
      <guid isPermaLink="false">2508.09486v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment</title>
      <link>http://arxiv.org/abs/2508.09399v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于联邦学习的风险评估框架，解决跨机构金融风险分析中的数据隐私和协作建模挑战。通过特征注意力和时间建模结构，实现了无需共享原始数据的跨机构联合建模和风险识别。实验表明该方法在所有评估指标上均优于传统方法和现有联邦学习变体。&lt;h4&gt;背景&lt;/h4&gt;跨机构金融风险分析面临数据隐私和协作建模的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于联邦学习的风险评估框架，实现无需共享原始数据的跨机构联合建模和风险识别。&lt;h4&gt;方法&lt;/h4&gt;采用特征注意力和时间建模结构；分布式优化策略；各金融机构训练本地子模型；使用差分隐私和噪声注入保护模型参数；中央服务器聚合参数生成全局模型；全局模型用于系统性风险识别。&lt;h4&gt;主要发现&lt;/h4&gt;所提模型在通信效率、模型准确性、系统性风险检测和跨市场泛化能力等评估指标上均优于传统集中式方法和现有联邦学习变体；在敏感金融环境中展现出强大的建模能力和实用价值。&lt;h4&gt;结论&lt;/h4&gt;该方法增强了风险识别的范围和效率，同时保留了数据主权；为智能金融风险分析提供了安全高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了跨机构金融风险分析中的数据隐私和协作建模挑战。它提出了一种基于联邦学习的风险评估框架。在不共享原始数据的情况下，该方法使多个机构能够进行联合建模和风险识别。这是通过整合特征注意力和时间建模结构实现的。具体来说，该模型采用分布式优化策略。每个金融机构训练一个本地子模型。模型参数在上传前使用差分隐私和噪声注入进行保护。然后中央服务器聚合这些参数以生成全局模型。该全局模型用于系统性风险识别。为验证所提方法的有效性，进行了多项实验。这些实验评估了通信效率、模型准确性、系统性风险检测和跨市场泛化能力。结果表明，在所有评估指标上，所提出的模型均优于传统集中式方法和现有联邦学习变体。它在敏感金融环境中展现出强大的建模能力和实用价值。该方法在保留数据主权的同时增强了风险识别的范围和效率。它为智能金融风险分析提供了安全高效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges of data privacy and collaborativemodeling in cross-institution financial risk analysis. It proposes a riskassessment framework based on federated learning. Without sharing raw data, themethod enables joint modeling and risk identification across multipleinstitutions. This is achieved by incorporating a feature attention mechanismand temporal modeling structure. Specifically, the model adopts a distributedoptimization strategy. Each financial institution trains a local sub-model. Themodel parameters are protected using differential privacy and noise injectionbefore being uploaded. A central server then aggregates these parameters togenerate a global model. This global model is used for systemic riskidentification. To validate the effectiveness of the proposed method, multipleexperiments are conducted. These evaluate communication efficiency, modelaccuracy, systemic risk detection, and cross-market generalization. The resultsshow that the proposed model outperforms both traditional centralized methodsand existing federated learning variants across all evaluation metrics. Itdemonstrates strong modeling capabilities and practical value in sensitivefinancial environments. The method enhances the scope and efficiency of riskidentification while preserving data sovereignty. It offers a secure andefficient solution for intelligent financial risk analysis.</description>
      <author>example@mail.com (Yue Yao, Zhen Xu, Youzhu Liu, Kunyuan Ma, Yuxiu Lin, Mohan Jiang)</author>
      <guid isPermaLink="false">2508.09399v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Online Data Generation for MIMO-OFDM Channel Denoising: Transfer Learning vs. Meta Learning</title>
      <link>http://arxiv.org/abs/2508.09751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种标准兼容的在线训练数据生成策略，用于MIMO-OFDM系统中的自适应信道去噪，通过数据辅助信道估计和两种学习方法(迁移学习和元学习)有效减少了信道估计误差&lt;h4&gt;背景&lt;/h4&gt;信道去噪是减轻多输入多输出正交频分复用(MIMO-OFDM)系统中信道估计误差的一种实用有效技术，但将去噪技术适应变化的信道条件通常需要先验知识或产生大量训练开销&lt;h4&gt;目的&lt;/h4&gt;解决信道去噪技术适应变化信道条件时面临的挑战，提出一种标准兼容的策略，用于生成在线训练数据，实现在线自适应信道去噪&lt;h4&gt;方法&lt;/h4&gt;利用数据辅助信道估计获得的高质量信道估计作为真实信道的替代品；利用特定时频邻域内的相邻检测数据符号作为虚拟参考信号，并分析推导该邻域的最佳大小；基于此策略设计了两种信道去噪方法：基于迁移学习(微调预训练神经网络)和基于元学习(快速适应新信道环境)&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法能有效适应动态信道条件，与传统技术相比显著减少了信道估计误差&lt;h4&gt;结论&lt;/h4&gt;提出的在线训练数据生成策略和两种信道去噪方法能够有效解决信道估计问题，提高MIMO-OFDM系统性能&lt;h4&gt;翻译&lt;/h4&gt;信道去噪是一种实用有效的技术，用于减轻多输入多输出正交频分复用(MIMO-OFDM)系统中的信道估计误差。然而，将去噪技术适应变化的信道条件通常需要先验知识或产生大量训练开销。为解决这些挑战，我们提出了一种标准兼容的策略，用于生成在线训练数据，实现在线自适应信道去噪。关键思想是利用通过数据辅助信道估计获得的高质量信道估计作为不可用真实信道的实际替代品。我们的数据辅助方法利用特定时频邻域内的相邻检测数据符号作为虚拟参考信号，并分析推导了该邻域的最佳大小，以最小化所得估计的均方误差。利用所提出的策略，我们设计了两种信道去噪方法，一种是基于迁移学习，微调预训练的去噪神经网络；另一种是基于元学习，以最小的更新快速适应新的信道环境。仿真结果表明，提出的方法能有效适应动态信道条件，与传统技术相比显著减少了信道估计误差&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Channel denoising is a practical and effective technique for mitigatingchannel estimation errors in multiple-input multiple-output orthogonalfrequency-division multiplexing (MIMO-OFDM) systems. However, adaptingdenoising techniques to varying channel conditions typically requires priorknowledge or incurs significant training overhead. To address these challenges,we propose a standard-compatible strategy for generating online training datathat enables online adaptive channel denoising. The key idea is to leveragehigh-quality channel estimates obtained via data-aided channel estimation aspractical substitutes for unavailable ground-truth channels. Our data-aidedmethod exploits adjacent detected data symbols within a specific time-frequencyneighborhood as virtual reference signals, and we analytically derive theoptimal size of this neighborhood to minimize the mean squared error of theresulting estimates. By leveraging the proposed strategy, we devise two channeldenoising approaches, one based on transfer learning, which fine-tunes apre-trained denoising neural network, and the other based on meta learning,which rapidly adapts to new channel environments with minimal updates.Simulation results demonstrate that the proposed methods effectively adapt todynamic channel conditions and significantly reduce channel estimation errorscompared to conventional techniques.</description>
      <author>example@mail.com (Sungyoung Ha, Ikbeom Lee, Seunghyeon Jeon, Yo-Seb Jeon)</author>
      <guid isPermaLink="false">2508.09751v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Reinforcement learning in densely recurrent biological networks</title>
      <link>http://arxiv.org/abs/2508.09618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为ENOMAD的混合优化框架，结合进化搜索和直接搜索，用于训练连续动作空间中的循环神经网络，在线虫神经连接组上的觅食任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;在连续动作空间中训练高度循环神经网络面临技术挑战：基于梯度的方法容易遇到梯度爆炸或消失问题，而纯进化搜索在高维权重空间中收敛缓慢。&lt;h4&gt;目的&lt;/h4&gt;引入一种混合的、无导数的优化框架，通过结合全局进化探索和局部直接搜索开发来实现强化学习。&lt;h4&gt;方法&lt;/h4&gt;提出名为ENOMAD（基于网格自适应直接搜索的进化非线性优化）的方法，在线虫完全映射的神经连接组中的觅食任务套件上进行基准测试。该方法利用生物学衍生的权重先验，改进而非重建生物体的原生电路。引入了两种算法变体：一种是对多个权重进行小分布式调整，另一种是对有限数量的权重进行较大变化。&lt;h4&gt;主要发现&lt;/h4&gt;两种ENOMAD变体都显著优于未经训练的连接组性能（可解释为迁移学习例子）和现有训练策略。&lt;h4&gt;结论&lt;/h4&gt;将进化搜索与非线性优化相结合，为专门化自然循环网络执行特定任务提供了一种高效且基于生物学的方法。&lt;h4&gt;翻译&lt;/h4&gt;在连续动作空间中训练高度循环网络是一个技术挑战：基于梯度的方法容易遇到梯度爆炸或消失问题，而纯进化搜索在高维权重空间中收敛缓慢。我们引入了一种混合的、无导数的优化框架，通过结合全局进化探索和局部直接搜索开发来实现强化学习。该方法称为ENOMAD（基于网格自适应直接搜索的进化非线性优化），在线虫完全映射的神经连接组中的觅食任务套件上进行基准测试。关键是，ENOMAD利用生物学衍生的权重先验，让它改进而非重建生物体的原生电路。介绍了该方法的两种算法变体，导致对多个权重进行小分布式调整，或对有限数量的权重进行较大变化。两种变体都显著优于未经训练的连接组性能（可解释为迁移学习的例子）和现有训练策略。这些发现表明，将进化搜索与非线性优化相结合，为专门化自然循环网络执行特定任务提供了一种高效且基于生物学的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training highly recurrent networks in continuous action spaces is a technicalchallenge: gradient-based methods suffer from exploding or vanishing gradients,while purely evolutionary searches converge slowly in high-dimensional weightspaces. We introduce a hybrid, derivative-free optimization framework thatimplements reinforcement learning by coupling global evolutionary explorationwith local direct search exploitation. The method, termed ENOMAD (EvolutionaryNonlinear Optimization with Mesh Adaptive Direct search), is benchmarked on asuite of food-foraging tasks instantiated in the fully mapped neural connectomeof the nematode \emph{Caenorhabditis elegans}. Crucially, ENOMAD leveragesbiologically derived weight priors, letting it refine--rather than rebuild--theorganism's native circuitry. Two algorithmic variants of the method areintroduced, which lead to either small distributed adjustments of many weights,or larger changes on a limited number of weights. Both variants significantlyexceed the performance of the untrained connectome (in what can be interpretedas an example of transfer learning) and of existing training strategies. Thesefindings demonstrate that integrating evolutionary search with nonlinearoptimization provides an efficient, biologically grounded strategy forspecializing natural recurrent networks towards a specified set of tasks.</description>
      <author>example@mail.com (Miles Walter Churchland, Jordi Garcia-Ojalvo)</author>
      <guid isPermaLink="false">2508.09618v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.09477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于异常检测的通用AI生成图像检测器，通过无监督学习实现无需访问特定AI生成图像即可有效检测多种生成模型产生的AI生成图像。&lt;h4&gt;背景&lt;/h4&gt;随着AI生成模型的快速发展，AI生成图像(AII)的视觉质量越来越接近自然图像，引发安全担忧。传统AII检测器通常使用自然图像和特定生成模型产生的AI图像进行训练，导致对来自未见过的生成模型的AI图像检测性能有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的AI生成图像检测器，能够有效检测来自各种生成模型(包括未见过的模型)的AI生成图像，解决传统检测方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于异常检测视角的检测器，使用预训练的CLIP编码器作为特征提取器，设计类似正规流的无监督模型。使用代理图像(通过对自然图像应用光谱修改操作获得)而非AI图像进行训练，通过最小化代理图像的可能性进行训练，可选择性地结合最大化自然图像的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明该方法对各种图像生成器产生的AI生成图像有效，具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;基于异常检测的方法能够有效检测来自不同生成模型的AI生成图像，包括未见过的模型，比传统方法具有更好的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;随着AI生成模型的快速发展，AI生成图像(AII)的视觉质量越来越接近自然图像，这不可避免地引发了安全担忧。大多数AII检测器通常采用传统图像分类流程，使用自然图像和AI生成图像(由生成模型生成)，这可能导致对来自未见过的生成模型的AI图像检测性能有限。为解决这一问题，我们从异常检测的角度提出了一个通用的AI生成图像检测器。我们的判别器不需要访问任何AI图像，并通过无监督学习学习可泛化的表示。具体来说，我们使用预训练的CLIP编码器作为特征提取器，并设计了一种类似正规流的无监督模型。训练时使用代理图像(例如通过对自然图像应用光谱修改操作获得的图像)，而不是AI图像。我们的模型通过最小化代理图像的可能性进行训练，可选择性地结合最大化自然图像的可能性。大量实验证明我们的方法对各种图像生成器产生的AI图像有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of AI generative models, the visual quality ofAI-generated images (AIIs) has become increasingly close to natural images,which inevitably raises security concerns. Most AII detectors often employ theconventional image classification pipeline with natural images and AIIs(generated by a generative model), which can result in limited detectionperformance for AIIs from unseen generative models. To solve this, we proposeda universal AI-generated image detector from the perspective of anomalydetection. Our discriminator does not need to access any AIIs and learn ageneralizable representation with unsupervised learning. Specifically, we usethe pre-trained CLIP encoder as the feature extractor and design a normalizingflow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained byapplying a spectral modification operation on natural images, are used fortraining. Our models are trained by minimizing the likelihood of proxy images,optionally combined with maximizing the likelihood of natural images. Extensiveexperiments demonstrate the effectiveness of our method on AIIs produced byvarious image generators.</description>
      <author>example@mail.com (Zhipeng Yuan, Kai Wang, Weize Quan, Dong-Ming Yan, Tieru Wu)</author>
      <guid isPermaLink="false">2508.09477v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation</title>
      <link>http://arxiv.org/abs/2508.09462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为细粒度聚类和拒绝网络(FGCRN)的新型开放集故障诊断模型，解决了多模式过程中健康状态样本多聚类分布导致的决策边界构建困难问题。&lt;h4&gt;背景&lt;/h4&gt;在多模式过程中，同一种健康状态的样本通常表现出多种聚类分布，这使得为该状态构建紧凑且准确的决策边界变得困难。同时，可靠的故障诊断系统不仅要准确分类已知健康状态，还要能有效识别未知故障。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确分类已知健康状态并有效识别未知故障的开放集故障诊断模型，解决多模式过程中健康状态样本多聚类分布带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出FGCRN模型，结合多尺度深度卷积、双向门控循环单元和时序注意力机制捕获判别性特征；设计基于距离的损失函数增强类内紧凑性；通过无监督学习构建细粒度特征表示揭示健康状态内在结构；利用极值理论建模样本特征与细粒度表示间的距离以识别未知故障。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明了FGCRN方法的优越性能，表明该模型能有效解决多模式过程中健康状态样本多聚类分布带来的挑战。&lt;h4&gt;结论&lt;/h4&gt;FGCRN模型通过细粒度聚类和拒绝策略，实现了对已知健康状态的准确分类和对未知故障的有效识别，为多模式过程的故障诊断提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;一个可靠的故障诊断系统不仅要准确分类已知的健康状态，还要有效识别未知故障。在多模式过程中，属于同一健康状态的样本通常表现出多种聚类分布，这使得为该状态构建紧凑且准确的决策边界变得困难。为了应对这一挑战，提出了一种名为细粒度聚类和拒绝网络(FGCRN)的新型开放集故障诊断模型。它结合了多尺度深度卷积、双向门控循环单元和时序注意力机制来捕获判别性特征。设计了一种基于距离的损失函数来增强类内紧凑性。通过无监督学习构建细粒度特征表示，以揭示每个健康状态的内在结构。采用极值理论来建模样本特征与其相应细粒度表示之间的距离，从而有效识别未知故障。大量实验证明了所提出方法的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A reliable fault diagnosis system should not only accurately classify knownhealth states but also effectively identify unknown faults. In multimodeprocesses, samples belonging to the same health state often show multiplecluster distributions, making it difficult to construct compact and accuratedecision boundaries for that state. To address this challenge, a novel open-setfault diagnosis model named fine-grained clustering and rejection network(FGCRN) is proposed. It combines multiscale depthwise convolution,bidirectional gated recurrent unit and temporal attention mechanism to capturediscriminative features. A distance-based loss function is designed to enhancethe intra-class compactness. Fine-grained feature representations areconstructed through unsupervised learning to uncover the intrinsic structuresof each health state. Extreme value theory is employed to model the distancebetween sample features and their corresponding fine-grained representations,enabling effective identification of unknown faults. Extensive experimentsdemonstrate the superior performance of the proposed method.</description>
      <author>example@mail.com (Guangqiang Li, M. Amine Atoui, Xiangshun Li)</author>
      <guid isPermaLink="false">2508.09462v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges</title>
      <link>http://arxiv.org/abs/2508.09022v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10pages,5figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DPGNet的双路径引导网络，用于解决深度伪造检测中的标注挑战，通过利用未标记数据并弥合不同生成模型间的域差距，实现了比现有方法高6.3%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;现有的深度伪造检测方法严重依赖标记数据，但随着AI生成内容越来越逼真，人工标注也变得困难且不可靠。同时，AI生成的面部图像与真实图像分布相似，导致传统无监督学习方法性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决深度伪造检测中的两个关键挑战：弥合不同生成模型面部间的域差距，以及有效利用未标记的图像样本来缓解标注难题。&lt;h4&gt;方法&lt;/h4&gt;提出双路径引导网络(DPGNet)，包含两个核心模块：文本引导的跨域对齐(使用可学习提示统一视觉和文本嵌入到域不变特征空间)和课程驱动的伪标签生成(动态利用信息量更大的未标记样本)。此外，采用跨域知识蒸馏防止灾难性遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在11个流行数据集上的实验表明，DPGNet比现有最先进方法高出6.3%的性能，验证了其在利用未标记数据解决深度伪造标注挑战方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;DPGNet通过有效利用未标记数据，能够应对深度伪造日益真实性带来的标注挑战，为深度伪造检测提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现有的深度伪造检测方法严重依赖标记的训练数据。然而，随着AI生成内容变得越来越逼真，即使是人工标注者也难以区分深度伪造和真实图像。这使得标注过程既耗时又不可靠。具体来说，对于能够有效利用来自在线社交网络的大规模未标记数据的方法需求日益增长。与典型的无监督学习任务(类别明显不同)不同，AI生成的面部图像紧密模仿真实图像分布并具有强相似性，导致传统策略性能下降。在本文中，我们引入双路径引导网络(DPGNet)，以解决两个关键挑战：(1)弥合不同生成模型面部之间的域差距，(2)利用未标记的图像样本。该方法具有两个核心模块：文本引导的跨域对齐，使用可学习提示将视觉和文本嵌入统一到域不变特征空间；课程驱动的伪标签生成，动态利用信息量更大的未标记样本。为防止灾难性遗忘，我们还通过跨域知识蒸馏促进域之间的桥接。在11个流行数据集上的大量实验表明，DPGNet比最先进的方法高出6.3%，突显了其利用未标记数据解决深度伪造日益真实性带来的标注挑战的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing deepfake detection methods heavily depend on labeled training data.However, as AI-generated content becomes increasingly realistic, even\textbf{human annotators struggle to distinguish} between deepfakes andauthentic images. This makes the labeling process both time-consuming and lessreliable. Specifically, there is a growing demand for approaches that caneffectively utilize large-scale unlabeled data from online social networks.Unlike typical unsupervised learning tasks, where categories are distinct,AI-generated faces closely mimic real image distributions and share strongsimilarities, causing performance drop in conventional strategies. In thispaper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two keychallenges: (1) bridging the domain gap between faces from different generationmodels, and (2) utilizing unlabeled image samples. The method features two coremodules: text-guided cross-domain alignment, which uses learnable prompts tounify visual and textual embeddings into a domain-invariant feature space, andcurriculum-driven pseudo label generation, which dynamically exploit moreinformative unlabeled samples. To prevent catastrophic forgetting, we alsofacilitate bridging between domains via cross-domain knowledge distillation.Extensive experiments on \textbf{11 popular datasets}, show that DPGNetoutperforms SoTA approaches by \textbf{6.3\%}, highlighting its effectivenessin leveraging unlabeled data to address the annotation challenges posed by theincreasing realism of deepfakes.</description>
      <author>example@mail.com (Zhiqiang Yang, Renshuai Tao, Xiaolong Zheng, Guodong Yang, Chunjie Zhang)</author>
      <guid isPermaLink="false">2508.09022v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring</title>
      <link>http://arxiv.org/abs/2508.09187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于呼吸分析作为健康监测工具的综述研究，全面考察了接触式和非接触式方法，重点强调了机器学习和深度学习技术的最新应用。&lt;h4&gt;背景&lt;/h4&gt;呼吸分析已成为健康监测的关键工具，能提供呼吸功能、疾病检测和持续健康评估的见解。传统接触式方法虽然可靠，但在舒适性和实用性方面存在挑战，特别是对于长期监测。&lt;h4&gt;目的&lt;/h4&gt;全面考察接触式和非接触式呼吸分析方法，强调机器学习和深度学习技术的最新进展，分析非接触式方法的准确性，探讨各种应用场景，并总结关键挑战和新兴趋势。&lt;h4&gt;方法&lt;/h4&gt;分析包括Wi-Fi信道状态信息和声学传感在内的非接触式方法，研究从单用户呼吸率检测到多用户场景、用户识别和呼吸疾病检测的广泛应用。详细介绍了数据预处理、特征提取和分类技术，并提供不同方法的机器学习/深度学习模型比较。&lt;h4&gt;主要发现&lt;/h4&gt;非接触式方法能提供准确、非侵入性的呼吸监测；详细比较了适合不同方法的机器学习/深度学习模型；讨论了数据集稀缺、多用户干扰和数据隐私等挑战；指出可解释AI、联邦学习、迁移学习和混合建模等新兴趋势。&lt;h4&gt;结论&lt;/h4&gt;通过综合当前方法并确定开放研究方向，该综述提供了指导呼吸分析未来创新的全面框架，旨在将先进技术能力与实际医疗保健应用相结合。&lt;h4&gt;翻译&lt;/h4&gt;呼吸分析已成为健康监测的关键工具，为呼吸功能、疾病检测和持续健康评估提供了见解。虽然传统的接触式方法可靠，但它们在舒适性和实用性方面常常带来挑战，特别是对于长期监测。本综述全面考察了接触式和非接触式方法，重点强调了应用于呼吸分析的机器学习和深度学习技术的最新进展。分析了包括Wi-Fi信道状态信息和声学传感在内的非接触式方法，探讨了它们提供准确、非侵入性呼吸监测的能力。我们研究了从单用户呼吸率检测到多用户场景、用户识别和呼吸疾病检测的广泛应用。此外，本综述详细介绍了必要的数据预处理、特征提取和分类技术，提供了适合每种方法的机器学习/深度学习模型的比较见解。还讨论了数据集稀缺、多用户干扰和数据隐私等关键挑战，以及可解释AI、联邦学习、迁移学习和混合建模等新兴趋势。通过综合当前方法并确定开放研究方向，本综述提供了一个全面的框架，以指导呼吸分析的未来创新，将先进的技术能力与实际的医疗保健应用相结合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.smhl.2025.100579&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breath analysis has emerged as a critical tool in health monitoring, offeringinsights into respiratory function, disease detection, and continuous healthassessment. While traditional contact-based methods are reliable, they oftenpose challenges in comfort and practicality, particularly for long-termmonitoring. This survey comprehensively examines contact-based and contactlessapproaches, emphasizing recent advances in machine learning and deep learningtechniques applied to breath analysis. Contactless methods, including Wi-FiChannel State Information and acoustic sensing, are analyzed for their abilityto provide accurate, noninvasive respiratory monitoring. We explore a broadrange of applications, from single-user respiratory rate detection tomulti-user scenarios, user identification, and respiratory disease detection.Furthermore, this survey details essential data preprocessing, featureextraction, and classification techniques, offering comparative insights intomachine learning/deep learning models suited to each approach. Key challengeslike dataset scarcity, multi-user interference, and data privacy are alsodiscussed, along with emerging trends like Explainable AI, federated learning,transfer learning, and hybrid modeling. By synthesizing current methodologiesand identifying open research directions, this survey offers a comprehensiveframework to guide future innovations in breath analysis, bridging advancedtechnological capabilities with practical healthcare applications.</description>
      <author>example@mail.com (Almustapha A. Wakili, Babajide J. Asaju, Woosub Jung)</author>
      <guid isPermaLink="false">2508.09187v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection</title>
      <link>http://arxiv.org/abs/2508.09913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于音频-视觉语音表征学习的人脸伪造视频检测方法，通过利用音频和视觉语音元素间的协同效应，在无需伪造视频参与训练的情况下实现了跨数据集泛化和鲁棒性的显著提升。&lt;h4&gt;背景&lt;/h4&gt;人脸伪造视频检测是数字取证领域的一大挑战，特别是在对未见数据集和常见干扰的泛化能力方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决人脸伪造视频检测中的泛化问题，特别是提高对未见数据集和常见干扰的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过自监督的掩码预测任务在真实视频上学习音频-视觉语音表征，同时编码局部和全局语义信息，然后将学习到的模型直接迁移到伪造检测任务。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法在跨数据集泛化能力和鲁棒性方面优于现有最先进方法，且模型训练过程中无需使用任何伪造视频。&lt;h4&gt;结论&lt;/h4&gt;基于音频-视觉语音表征的学习方法能有效提升人脸伪造视频检测的泛化能力和鲁棒性，为该领域提供了新的解决思路。&lt;h4&gt;翻译&lt;/h4&gt;人脸伪造视频的检测仍然是数字取证领域的一项艰巨挑战，特别是在对未见数据集和常见干扰的泛化方面。本文通过利用音频和视觉语音元素之间的协同效应，采用了一种新颖的音频-视觉语音表征学习方法来解决这一问题。我们的研究动机是发现富含语音内容的音频信号能提供精确反映面部运动的信息。为此，我们首先通过自监督的掩码预测任务在真实视频上学习精确的音频-视觉语音表征，同时编码局部和全局语义信息。然后将得到的模型直接迁移到伪造检测任务。大量实验表明，在跨数据集泛化能力和鲁棒性方面，我们的方法优于最先进的方法，且模型训练过程中无需使用任何伪造视频。代码可在https://github.com/Eleven4AI/SpeechForensics获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detection of face forgery videos remains a formidable challenge in the fieldof digital forensics, especially the generalization to unseen datasets andcommon perturbations. In this paper, we tackle this issue by leveraging thesynergy between audio and visual speech elements, embarking on a novel approachthrough audio-visual speech representation learning. Our work is motivated bythe finding that audio signals, enriched with speech content, can provideprecise information effectively reflecting facial movements. To this end, wefirst learn precise audio-visual speech representations on real videos via aself-supervised masked prediction task, which encodes both local and globalsemantic information simultaneously. Then, the derived model is directlytransferred to the forgery detection task. Extensive experiments demonstratethat our method outperforms the state-of-the-art methods in terms ofcross-dataset generalization and robustness, without the participation of anyfake video in model training. Code is available athttps://github.com/Eleven4AI/SpeechForensics.</description>
      <author>example@mail.com (Yachao Liang, Min Yu, Gang Li, Jianguo Jiang, Boquan Li, Feng Yu, Ning Zhang, Xiang Meng, Weiqing Huang)</author>
      <guid isPermaLink="false">2508.09913v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training</title>
      <link>http://arxiv.org/abs/2508.09691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PaCo-FR的无监督面部表征预训练框架，结合掩码图像建模和块-像素对齐技术，解决了现有方法在捕捉面部特征、保留空间结构和利用有限标记数据方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;面部表征预训练对面部识别、表情分析和虚拟现实等任务至关重要，但现有方法面临三个关键挑战：无法捕捉独特面部特征和细粒度语义、忽略面部解剖学固有的空间结构、低效利用有限的标记数据。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的三个关键挑战，开发一种高效的面部表征预训练框架，减少对昂贵标注数据集的依赖。&lt;h4&gt;方法&lt;/h4&gt;PaCo-FR框架包含三个创新组件：(1)结构化掩码策略，保留面部区域的空间连贯性；(2)基于块的代码本，使用多个候选令牌增强特征区分能力；(3)空间一致性约束，保留面部组件间的几何关系。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用200万张未标记图像进行预训练，PaCo-FR在多个面部分析任务上取得最先进性能，尤其在姿势变化、遮挡和不同光照条件场景中表现显著提升。&lt;h4&gt;结论&lt;/h4&gt;PaCo-FR推动了面部表征学习发展，提供了可扩展、高效的解决方案，减少了对昂贵标注数据集的依赖，促进了更有效的面部分析系统。&lt;h4&gt;翻译&lt;/h4&gt;面部表征预训练对于面部识别、表情分析和虚拟现实等任务至关重要。然而，现有方法面临三个关键挑战：(1)无法捕捉独特的面部特征和细粒度语义，(2)忽略了面部解剖学固有的空间结构，(3)低效地利用有限的标记数据。为克服这些问题，我们引入了PaCo-FR，这是一种结合掩码图像建模和块-像素对齐的无监督框架。我们的方法整合了三个创新组件：(1)结构化掩码策略，通过与语义上有意义的面部区域保持一致来保留空间连贯性，(2)基于块的代码本，使用多个候选令牌增强特征区分能力，(3)空间一致性约束，保留面部组件之间的几何关系。仅使用200万张未标记图像进行预训练，PaCo-FR在多个面部分析任务上取得了最先进的性能。我们的方法在不同姿势、遮挡和光照条件的场景中显示出显著改进。我们认为这项工作推动了面部表征学习的发展，并提供了一种可扩展、高效的解决方案，减少了对昂贵标注数据集的依赖，促进了更有效的面部分析系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial representation pre-training is crucial for tasks like facialrecognition, expression analysis, and virtual reality. However, existingmethods face three key challenges: (1) failing to capture distinct facialfeatures and fine-grained semantics, (2) ignoring the spatial structureinherent to facial anatomy, and (3) inefficiently utilizing limited labeleddata. To overcome these, we introduce PaCo-FR, an unsupervised framework thatcombines masked image modeling with patch-pixel alignment. Our approachintegrates three innovative components: (1) a structured masking strategy thatpreserves spatial coherence by aligning with semantically meaningful facialregions, (2) a novel patch-based codebook that enhances feature discriminationwith multiple candidate tokens, and (3) spatial consistency constraints thatpreserve geometric relationships between facial components. PaCo-FR achievesstate-of-the-art performance across several facial analysis tasks with just 2million unlabeled images for pre-training. Our method demonstrates significantimprovements, particularly in scenarios with varying poses, occlusions, andlighting conditions. We believe this work advances facial representationlearning and offers a scalable, efficient solution that reduces reliance onexpensive annotated datasets, driving more effective facial analysis systems.</description>
      <author>example@mail.com (Yin Xie, Zhichao Chen, Xiaoze Yu, Yongle Zhao, Xiang An, Kaicheng Yang, Zimin Ran, Jia Guo, Ziyong Feng, Jiankang Deng)</author>
      <guid isPermaLink="false">2508.09691v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning</title>
      <link>http://arxiv.org/abs/2508.09281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个新颖的、可解释的框架，用于通过基于模式的KCs来自动发现知识组件。这些模式是学生代码中重复出现的结构模式，捕捉了学生必须掌握的特定编程模式和语言结构。&lt;h4&gt;背景&lt;/h4&gt;在计算机科学教育中，个性化学习依赖于准确建模学生已掌握和需要学习的内容。知识组件(KCs)为此提供了基础，但自动从学生代码中提取KCs具有挑战性，因为发现的KCs可解释性不足，编程问题的开放性，以及学生解决方案之间的结构差异显著，编程概念之间的复杂交互。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化、可扩展且可解释的框架，用于识别细粒度的代码模式和算法结构，这对学生学习至关重要。&lt;h4&gt;方法&lt;/h4&gt;训练一个变分自编码器，在基于注意力的可解释代码表示模型的指导下，从学生代码中生成重要的代表性模式。该模型能识别学生代码中重要的正确和错误模式实现。然后将这些模式聚类形成基于模式的KCs。使用学习曲线分析和深度知识追踪(DKT)两种基于认知科学的方法评估KCs。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明有意义的学习轨迹，以及在DKT预测性能上与传统KT方法相比有显著改进。&lt;h4&gt;结论&lt;/h4&gt;这项工作通过提供自动化、可扩展且可解释的框架来识别细粒度的代码模式和算法结构，推进了计算机科学教育中的知识建模，这对学生学习至关重要。&lt;h4&gt;翻译&lt;/h4&gt;计算机科学教育中有效的个性化学习依赖于准确建模学生已掌握和需要学习的内容。虽然知识组件(KCs)为此提供了基础，但由于发现的KCs可解释性不足、编程问题的开放性以及学生解决方案之间的结构差异显著和编程概念之间的复杂交互，自动从学生代码中提取KCs本质上是具有挑战性的。在这项工作中，我们提出了一种新颖的、可解释的框架，用于通过基于模式的KCs来自动发现知识组件：学生代码中重复出现的结构模式，捕捉了学生必须掌握的特定编程模式和语言结构。为此，我们训练了一个变分自编码器，在可解释的、基于注意力的代码表示模型的指导下，从学生代码中生成重要的代表性模式，该模型能识别学生代码中重要的正确和错误模式实现。然后将这些模式聚类形成基于模式的KCs。我们使用两种基于认知科学的方法评估我们的KCs：学习曲线分析和深度知识追踪(DKT)。实验结果表明有意义的学习轨迹，以及与传统KT方法相比，DKT预测性能有显著改进。这项工作通过提供自动化、可扩展且可解释的框架来识别细粒度的代码模式和算法结构（对学生学习至关重要），推进了计算机科学教育中的知识建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective personalized learning in computer science education depends onaccurately modeling what students know and what they need to learn. WhileKnowledge Components (KCs) provide a foundation for such modeling, automated KCextraction from student code is inherently challenging due to insufficientexplainability of discovered KCs and the open-endedness of programming problemswith significant structural variability across student solutions and complexinteractions among programming concepts. In this work, we propose a novel,explainable framework for automated KC discovery through pattern-based KCs:recurring structural patterns within student code that capture the specificprogramming patterns and language constructs that students must master. Towardthis, we train a Variational Autoencoder to generate important representativepatterns from student code guided by an explainable, attention-based coderepresentation model that identifies important correct and incorrect patternimplementations from student code. These patterns are then clustered to formpattern-based KCs. We evaluate our KCs using two well-established methodsinformed by Cognitive Science: learning curve analysis and Deep KnowledgeTracing (DKT). Experimental results demonstrate meaningful learningtrajectories and significant improvements in DKT predictive performance overtraditional KT methods. This work advances knowledge modeling in CS educationby providing an automated, scalable, and explainable framework for identifyinggranular code patterns and algorithmic constructs, essential for studentlearning.</description>
      <author>example@mail.com (Muntasir Hoq, Griffin Pitts, Andrew Lan, Peter Brusilovsky, Bita Akram)</author>
      <guid isPermaLink="false">2508.09281v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Chartwin: a Case Study on Channel Charting-aided Localization in Dynamic Digital Network Twins</title>
      <link>http://arxiv.org/abs/2508.09055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Chartwin，一种将面向定位的信道图表与动态数字网络孪生(DNTs)集成的案例研究，展示了半监督信道图表在构建空间一致无线电地图方面的显著性能。&lt;h4&gt;背景&lt;/h4&gt;无线通信系统可以从空间一致的无线信道表征中显著受益，以高效执行各种通信任务。&lt;h4&gt;目的&lt;/h4&gt;实现局部和全局一致的无线电地图，通过信道图表技术提高无线通信系统的性能。&lt;h4&gt;方法&lt;/h4&gt;提出Chartwin，将面向定位的信道图表与动态数字网络孪生(DNTs)进行集成。&lt;h4&gt;主要发现&lt;/h4&gt;半监督信道图表在构建扩展城市环境的空间一致图表方面具有显著性能，静态DNT的定位误差约为4.5米，动态DNT的定位误差约为6米。&lt;h4&gt;结论&lt;/h4&gt;DNT辅助的信道图表和定位技术具有良好性能，有助于提高无线通信系统的空间一致性和定位精度。&lt;h4&gt;翻译&lt;/h4&gt;无线通信系统可以从空间一致的无线信道表征中显著受益，以高效执行各种通信任务。为此，信道图表已被引入作为一种有效的无监督学习技术，以实现局部和全局一致的无线电地图。在本信函中，我们提出了Chartwin，这是一个关于将面向定位的信道图表与动态数字网络孪生(DNTs)集成的案例研究。数值结果表明，半监督信道图表在构建所考虑的扩展城市环境的空间一致图表方面具有显著性能。所考虑的方法对于静态DNT导致约4.5米的定位误差，对于动态DNT导致约6米的定位误差，促进了DNT辅助的信道图表和定位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wireless communication systems can significantly benefit from theavailability of spatially consistent representations of the wireless channel toefficiently perform a wide range of communication tasks. Towards this purpose,channel charting has been introduced as an effective unsupervised learningtechnique to achieve both locally and globally consistent radio maps. In thisletter, we propose Chartwin, a case study on the integration oflocalization-oriented channel charting with dynamic Digital Network Twins(DNTs). Numerical results showcase the significant performance ofsemi-supervised channel charting in constructing a spatially consistent chartof the considered extended urban environment. The considered method results in$\approx$ 4.5 m localization error for the static DNT and $\approx$ 6 m in thedynamic DNT, fostering DNT-aided channel charting and localization.</description>
      <author>example@mail.com (Lorenzo Cazzella, Francesco Linsalata, Mahdi Maleki, Damiano Badini, Matteo Matteucci, Umberto Spagnolini)</author>
      <guid isPermaLink="false">2508.09055v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
  <item>
      <title>When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges</title>
      <link>http://arxiv.org/abs/2508.09022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10pages,5figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为双路径引导网络(DPGNet)的新型深度伪造检测方法，解决了现有方法对标记数据的依赖问题，通过文本引导的跨域对齐和课程驱动的伪标签生成有效利用未标记数据，在多个数据集上实现了比现有方法高6.3%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;现有的深度伪造检测方法严重依赖标记的训练数据，但随着AI生成内容变得越来越逼真，即使是人工标注者也难以区分深度伪造和真实图像，这使得标注过程既耗时又不可靠。市场对能够有效利用来自在线社交网络的大规模未标记数据的方法需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;解决深度伪造检测中的两个关键挑战：(1)弥合不同生成模型面部之间的域差距，(2)利用未标记的图像样本来克服标注困难。&lt;h4&gt;方法&lt;/h4&gt;提出双路径引导网络(DPGNet)，包含两个核心模块：文本引导的跨域对齐(使用可学习的提示将视觉和文本嵌入统一到域不变特征空间)和课程驱动的伪标签生成(动态利用信息量更大的未标记样本)。同时通过跨域知识蒸馏来防止灾难性遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在11个流行数据集上的大量实验表明，DPGNet比现有最先进的方法高出6.3%的性能，突显了其在利用未标记数据解决深度伪造日益真实性带来的标注挑战方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;DPGNet能够有效利用未标记数据解决深度伪造检测中的标注挑战，通过处理不同生成模型域差距和智能选择未标记样本，显著提高了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;现有的深度伪造检测方法严重依赖标记的训练数据。然而，随着AI生成内容变得越来越真实，即使是人工标注者也难以区分深度伪造和真实图像。这使得标注过程既耗时又不可靠。具体而言，市场对能够有效利用来自在线社交网络的大规模未标记数据的方法需求日益增长。与典型的无监督学习任务不同(其中类别是明确的)，AI生成的面部图像与真实图像分布非常相似，并具有强相似性，导致传统策略性能下降。在本文中，我们引入了双路径引导网络(DPGNet)，以解决两个关键挑战：(1)弥合不同生成模型面部之间的域差距，(2)利用未标记的图像样本。该方法具有两个核心模块：文本引导的跨域对齐，使用可学习的提示将视觉和文本嵌入统一到域不变特征空间；以及课程驱动的伪标签生成，动态利用信息量更大的未标记样本。为了防止灾难性遗忘，我们还通过跨域知识蒸馏促进域之间的桥接。在11个流行数据集上的大量实验表明，DPGNet比最先进的方法高出6.3%，突显了其在利用未标记数据解决深度伪造日益真实性带来的标注挑战方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing deepfake detection methods heavily depend on labeled training data.However, as AI-generated content becomes increasingly realistic, even\textbf{human annotators struggle to distinguish} between deepfakes andauthentic images. This makes the labeling process both time-consuming and lessreliable. Specifically, there is a growing demand for approaches that caneffectively utilize large-scale unlabeled data from online social networks.Unlike typical unsupervised learning tasks, where categories are distinct,AI-generated faces closely mimic real image distributions and share strongsimilarities, causing performance drop in conventional strategies. In thispaper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two keychallenges: (1) bridging the domain gap between faces from different generationmodels, and (2) utilizing unlabeled image samples. The method features two coremodules: text-guided cross-domain alignment, which uses learnable prompts tounify visual and textual embeddings into a domain-invariant feature space, andcurriculum-driven pseudo label generation, which dynamically exploit moreinformative unlabeled samples. To prevent catastrophic forgetting, we alsofacilitate bridging between domains via cross-domain knowledge distillation.Extensive experiments on \textbf{11 popular datasets}, show that DPGNetoutperforms SoTA approaches by \textbf{6.3\%}, highlighting its effectivenessin leveraging unlabeled data to address the annotation challenges posed by theincreasing realism of deepfakes.</description>
      <author>example@mail.com (Zhiqiang Yang, Renshuai Tao, Xiaolong Zheng, Guodong Yang, Chunjie Zhang)</author>
      <guid isPermaLink="false">2508.09022v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Empirical Bayes for Data Integration</title>
      <link>http://arxiv.org/abs/2508.08336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了经验贝叶斯方法在数据整合中的应用，特别是在迁移学习的背景下。研究关注的是在只能访问不完整数据的情况下学习结构（如特征选择）的问题。&lt;h4&gt;背景&lt;/h4&gt;研究者希望学习数据结构，但只能访问先前研究的不完整数据，如摘要、估计或相关特征列表。&lt;h4&gt;目的&lt;/h4&gt;开发一种计算框架，用于经验贝叶斯方法进行数据整合，并比较其与完全贝叶斯方法在变量选择和收敛速率方面的表现。&lt;h4&gt;方法&lt;/h4&gt;讨论完全贝叶斯与经验贝叶斯的差异，为经验贝叶斯开发计算框架，并通过高维回归示例进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;经验贝叶斯在较弱条件下（稀疏性和beta最小假设）实现了一致的变量选择，具有更快的收敛速率；完全贝叶斯推断具有出色特性；经验贝叶斯数据整合在实践中提供适度的但有意义的改进。&lt;h4&gt;结论&lt;/h4&gt;经验贝叶斯方法在处理不完整数据的数据整合和迁移学习方面具有优势，能够提供实用的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;我们讨论了经验贝叶斯在数据整合中的应用，即在迁移学习的意义上。我们的主要兴趣在于那些希望学习结构（如特征选择）但只能访问来自先前研究的不完整数据的场景，如摘要、估计或相关特征列表。我们讨论了完全贝叶斯与经验贝叶斯之间的差异，并为后者开发了计算框架。我们讨论了经验贝叶斯如何在比完全贝叶斯和其他标准标准更弱的条件下（稀疏性和beta最小假设）实现一致的变量选择，以及如何实现更快的收敛速率。我们的高维回归示例表明，完全贝叶斯推断具有出色的特性，而使用经验贝叶斯进行数据整合在实践中可以提供适度的但有意义的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We discuss the use of empirical Bayes for data integration, in the sense oftransfer learning. Our main interest is in settings where one wishes to learnstructure (e.g. feature selection) and one only has access to incomplete datafrom previous studies, such as summaries, estimates or lists of relevantfeatures. We discuss differences between full Bayes and empirical Bayes, anddevelop a computational framework for the latter. We discuss how empiricalBayes attains consistent variable selection under weaker conditions (sparsityand betamin assumptions) than full Bayes and other standard criteria do, andhow it attains faster convergence rates. Our high-dimensional regressionexamples show that fully Bayesian inference enjoys excellent properties, andthat data integration with empirical Bayes can offer moderate yet meaningfulimprovements in practice.</description>
      <author>example@mail.com (Paul Rognon-Vael, David Rossell)</author>
      <guid isPermaLink="false">2508.08336v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models</title>
      <link>http://arxiv.org/abs/2508.05685v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review. Code will be released upon acceptance&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Domain-guided Fine-tuning (DogFit)方法，用于解决扩散模型在较小目标领域迁移学习中的挑战。该方法通过在训练损失中注入领域感知的引导偏置，实现了无需额外计算开销的可控图像生成，同时支持高效的保真度-多样性权衡。&lt;h4&gt;背景&lt;/h4&gt;将扩散模型迁移到较小的目标领域具有挑战性，因为简单的微调模型通常会导致泛化能力差。现有的测试时引导方法虽然可以通过在图像保真度和样本多样性之间进行权衡来改善结果，但需要高昂的计算成本，通常在采样期间需要双重前向传播。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的扩散迁移学习引导机制，能够在不增加额外计算开销的情况下保持对生成图像的可控性，同时支持高效的保真度-多样性权衡。&lt;h4&gt;方法&lt;/h4&gt;DogFit方法将领域感知的引导偏置注入到训练损失中，内化了引导行为。作者通过轻量级条件机制将引导强度值编码为额外的模型输入，以支持推理时的可控性。此外，作者还研究了训练期间引导偏置的最佳位置和时间，提出了两种简单的调度策略：late-start和cut-off。&lt;h4&gt;主要发现&lt;/h4&gt;在微调过程中，无条件源模型比目标模型提供了更强的边际估计；提出的DogFit方法在六种不同目标领域的实验中，能够在FID和FDDINOV2指标上优于先前的引导方法，同时采样所需的计算量减少多达2倍。&lt;h4&gt;结论&lt;/h4&gt;DogFit是一种有效的扩散模型迁移学习方法，它通过领域感知的引导偏置和调度策略，在不增加计算成本的情况下实现了更好的生成质量和可控性。&lt;h4&gt;翻译&lt;/h4&gt;将扩散模型迁移到较小的目标领域具有挑战性，因为简单地微调模型通常会导致泛化能力差。测试时引导方法通过在图像保真度和样本多样性之间进行权衡来缓解这一问题，但这种好处带来了高昂的计算成本，通常在采样期间需要双重前向传播。我们提出了Domain-guided Fine-tuning (DogFit)方法，这是一种用于扩散迁移学习的有效引导机制，可以在不增加额外计算开销的情况下保持可控性。DogFit将领域感知的引导偏置注入到训练损失中，有效地在微调过程中内化了引导行为。领域感知设计源于我们的观察：在微调过程中，无条件源模型比目标模型提供了更强的边际估计。为了在推理时支持高效的可控保真度-多样性权衡，我们通过轻量级条件机制将引导强度值编码为额外的模型输入。我们进一步研究了训练期间引导偏置的最佳位置和时间，并提出了两种简单的调度策略，即late-start和cut-off，这些策略提高了生成质量和训练稳定性。在DiT和SiT主干网络上的六种不同目标领域的实验表明，DogFit在迁移学习中可以优于先前的引导方法，在FID和FDDINOV2指标方面表现更好，同时采样所需的TFLOPS减少多达2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning of diffusion models to smaller target domains ischallenging, as naively fine-tuning the model often results in poorgeneralization. Test-time guidance methods help mitigate this by offeringcontrollable improvements in image fidelity through a trade-off with samplediversity. However, this benefit comes at a high computational cost, typicallyrequiring dual forward passes during sampling. We propose the Domain-guidedFine-tuning (DogFit) method, an effective guidance mechanism for diffusiontransfer learning that maintains controllability without incurring additionalcomputational overhead. DogFit injects a domain-aware guidance offset into thetraining loss, effectively internalizing the guided behavior during thefine-tuning process. The domain-aware design is motivated by our observationthat during fine-tuning, the unconditional source model offers a strongermarginal estimate than the target model. To support efficient controllablefidelity-diversity trade-offs at inference, we encode the guidance strengthvalue as an additional model input through a lightweight conditioningmechanism. We further investigate the optimal placement and timing of theguidance offset during training and propose two simple scheduling strategies,i.e., late-start and cut-off, which improve generation quality and trainingstability. Experiments on DiT and SiT backbones across six diverse targetdomains show that DogFit can outperform prior guidance methods in transferlearning in terms of FID and FDDINOV2 while requiring up to 2x fewer samplingTFLOPS.</description>
      <author>example@mail.com (Yara Bahram, Mohammadhadi Shateri, Eric Granger)</author>
      <guid isPermaLink="false">2508.05685v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>KFFocus: Highlighting Keyframes for Enhanced Video Understanding</title>
      <link>http://arxiv.org/abs/2508.08989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;KFFocus是一种新的视频压缩方法，能够有效识别关键帧并保留重要信息，同时提高计算效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和多模态LLMs在图像和视频模态中表现出色，但视频LLMs在处理长视频序列时面临巨大的计算需求。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频LLMs压缩策略中忽略关键信息不均匀分布的问题，实现高效的视频标记压缩并保留重要信息内容。&lt;h4&gt;方法&lt;/h4&gt;KFFocus方法用基于经典视频压缩原理的改进方法替代均匀采样，根据时间冗余识别关键帧；根据帧的上下文相关性分配不同压缩比例；引入时空建模模块编码视频帧间的时间关系和每帧内的空间结构。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛认可的视频理解基准测试上，特别是在长视频场景中，KFFocus显著优于现有方法，实现了显著的计算效率和准确性提升。&lt;h4&gt;结论&lt;/h4&gt;KFFocus能够有效解决视频LLMs面临的计算效率和关键信息保留问题，为视频理解任务提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;最近，随着大型语言模型的出现，多模态LLMs在图像和视频模态中表现出色。尽管视频理解能力有所提升，但长视频序列的巨大计算需求导致当前的视频LLMs(Vid-LLMs)在帧间级别(如均匀采样视频帧)和帧内级别(如将每帧的所有视觉标记压缩为有限数量)都采用压缩策略。然而，这种方法往往忽略了关键信息在帧间的不均匀时间分布，可能导致包含重要时间和语义细节的关键帧被遗漏。为解决这些挑战，我们提出了KFFocus，这是一种旨在高效压缩视频标记并强调视频帧中包含信息丰富的上下文的方法。我们用基于经典视频压缩原理的改进方法替代均匀采样，根据时间冗余识别和捕获关键帧。通过根据帧的上下文相关性分配不同的压缩比例，KFFocus在保留信息内容细节的同时有效减少了标记冗余。此外，我们引入了一个时空建模模块，编码视频帧之间的时间关系和每帧内的空间结构，从而为Vid-LLMs提供对时空动态的细致理解。在广泛认可的视频理解基准测试上的大量实验，特别是在长视频场景中，表明KFFocus显著优于现有方法，实现了显著的计算效率和准确性提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, with the emergence of large language models, multimodal LLMs havedemonstrated exceptional capabilities in image and video modalities. Despiteadvancements in video comprehension, the substantial computational demands oflong video sequences lead current video LLMs (Vid-LLMs) to employ compressionstrategies at both the inter-frame level (e.g., uniform sampling of videoframes) and intra-frame level (e.g., condensing all visual tokens of each frameinto a limited number). However, this approach often neglects the uneventemporal distribution of critical information across frames, risking theomission of keyframes that contain essential temporal and semantic details. Totackle these challenges, we propose KFFocus, a method designed to efficientlycompress video tokens and emphasize the informative context present withinvideo frames. We substitute uniform sampling with a refined approach inspiredby classic video compression principles to identify and capture keyframes basedon their temporal redundancy. By assigning varying condensation ratios toframes based on their contextual relevance, KFFocus efficiently reduces tokenredundancy while preserving informative content details. Additionally, weintroduce a spatiotemporal modeling module that encodes both the temporalrelationships between video frames and the spatial structure within each frame,thus providing Vid-LLMs with a nuanced understanding of spatial-temporaldynamics. Extensive experiments on widely recognized video understandingbenchmarks, especially long video scenarios, demonstrate that KFFocussignificantly outperforms existing methods, achieving substantial computationalefficiency and enhanced accuracy.</description>
      <author>example@mail.com (Ming Nie, Chunwei Wang, Hang Xu, Li Zhang)</author>
      <guid isPermaLink="false">2508.08989v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>UniSTFormer: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition</title>
      <link>http://arxiv.org/abs/2508.08944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级的基于骨架的动作识别方法，通过统一的空间-时间建模和多尺度池化融合，显著降低了参数和计算复杂度，同时保持了良好的识别性能。&lt;h4&gt;背景&lt;/h4&gt;现有基于骨架的动作识别方法使用Transformer架构取得了显著进展，但这些方法通常依赖复杂的模块组合和重型设计，导致参数数量增加、计算成本高和可扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的空间-时间轻量级Transformer框架，减少参数数量和计算成本，同时保持良好的识别性能。&lt;h4&gt;方法&lt;/h4&gt;提出一个统一的空间-时间轻量级Transformer框架，将空间和时间建模集成在单个注意力模块中，消除单独时间建模块的需求；引入简化的多尺度池化融合模块，结合局部和全局池化路径，增强模型捕获细粒度局部运动和全局运动模式的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的广泛实验表明，该轻量级模型在准确性和效率之间取得了优越的平衡，与最先进的基于Transformer的基线相比，参数复杂性减少了58%以上，计算成本降低了60%以上，同时保持了具有竞争力的识别性能。&lt;h4&gt;结论&lt;/h4&gt;提出的统一空间-时间轻量级Transformer框架和多尺度池化融合模块能够有效减少计算复杂度，同时保持良好的动作识别性能。&lt;h4&gt;翻译&lt;/h4&gt;基于骨架的动作识别已通过Transformer架构取得了显著进展。然而，现有方法通常依赖复杂的模块组合和重型设计，导致参数数量增加、计算成本高昂和可扩展性有限。在本文中，我们提出了一个统一的空间-时间轻量级Transformer框架，将空间和时间建模集成在单个注意力模块中，消除了单独时间建模块的需求。这种方法在保留空间建模过程中的时间感知能力的同时减少了冗余计算。此外，我们引入了一个简化的多尺度池化融合模块，结合局部和全局池化路径，以增强模型捕获细粒度局部运动和全局运动模式的能力。在基准数据集上的广泛实验表明，我们的轻量级模型在准确性和效率之间实现了优越的平衡，与最先进的基于Transformer的基线相比，参数复杂性减少了58%以上，计算成本降低了60%以上，同时保持了具有竞争力的识别性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skeleton-based action recognition (SAR) has achieved impressive progress withtransformer architectures. However, existing methods often rely on complexmodule compositions and heavy designs, leading to increased parameter counts,high computational costs, and limited scalability. In this paper, we propose aunified spatio-temporal lightweight transformer framework that integratesspatial and temporal modeling within a single attention module, eliminating theneed for separate temporal modeling blocks. This approach reduces redundantcomputations while preserving temporal awareness within the spatial modelingprocess. Furthermore, we introduce a simplified multi-scale pooling fusionmodule that combines local and global pooling pathways to enhance the model'sability to capture fine-grained local movements and overarching global motionpatterns. Extensive experiments on benchmark datasets demonstrate that ourlightweight model achieves a superior balance between accuracy and efficiency,reducing parameter complexity by over 58% and lowering computational cost byover 60% compared to state-of-the-art transformer-based baselines, whilemaintaining competitive recognition performance.</description>
      <author>example@mail.com (Wenhan Wu, Zhishuai Guo, Chen Chen, Aidong Lu)</author>
      <guid isPermaLink="false">2508.08944v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Re:Verse -- Can Your VLM Read a Manga?</title>
      <link>http://arxiv.org/abs/2508.08508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究揭示了当前视觉语言模型在处理连续视觉叙事时，表面识别与深层叙事推理之间存在关键差距，特别是在时间因果关系和跨面板连贯性方面存在系统性缺陷。&lt;h4&gt;背景&lt;/h4&gt;尽管最近的大型多模态模型在单幅图像解释方面表现出色，但在连贯故事理解所需的核心要素上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的评估框架，系统性地表征视觉语言模型在叙事理解方面的局限性，并评估它们在长篇叙事理解方面的能力。&lt;h4&gt;方法&lt;/h4&gt;结合细粒度多模态注释、跨模态嵌入分析和检索增强评估，包括严格的注释协议、多种推理范式下的全面评估以及跨模态相似性分析，并应用此框架对《Re:Zero》漫画的11章（308个注释面板）进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型缺乏真正的故事级智能，特别在处理非线性叙事、角色一致性和跨越长序列的因果推理方面存在困难。&lt;h4&gt;结论&lt;/h4&gt;这项工作为评估叙事智能奠定了基础和实践方法，同时提供了对多模态模型中深度序列理解能力的实用见解。&lt;h4&gt;翻译&lt;/h4&gt;当前视觉语言模型在处理连续视觉叙事时，表现出表面识别与深层叙事推理之间的关键差距。通过对漫画叙事理解的全面调查，我们揭示尽管最近的大型多模态模型在单幅图像解释方面表现出色，但它们在时间因果关系和跨面板连贯性方面存在系统性缺陷，而这些是连贯故事理解的核心要求。我们引入了一个新的评估框架，结合细粒度多模态注释、跨模态嵌入分析和检索增强评估，以系统性地表征这些局限性。我们的方法包括：(i)一个严格的注释协议，通过对齐的轻小说文本将视觉元素与叙事结构联系起来；(ii)在多种推理范式下的全面评估，包括直接推理和检索增强生成；(iii)跨模态相似性分析，揭示了当前VLMs联合表示中的基本错位。将此框架应用于《Re:Zero》漫画的11章（308个注释面板），我们首次对VLMs中的长篇叙事理解进行了系统性研究，通过三个核心评估轴：生成式叙事、上下文对话定位和时间推理。我们的研究结果表明，当前模型缺乏真正的故事级智能，特别是在处理非线性叙事、角色一致性和跨越长序列的因果推理方面存在困难。这项工作为评估叙事智能奠定了基础和实践方法，同时提供了对多模态模型中超越基本识别的深度序列理解能力的实用见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Vision Language Models (VLMs) demonstrate a critical gap betweensurface-level recognition and deep narrative reasoning when processingsequential visual storytelling. Through a comprehensive investigation of manganarrative understanding, we reveal that while recent large multimodal modelsexcel at individual panel interpretation, they systematically fail at temporalcausality and cross-panel cohesion, core requirements for coherent storycomprehension. We introduce a novel evaluation framework that combinesfine-grained multimodal annotation, cross-modal embedding analysis, andretrieval-augmented assessment to systematically characterize theselimitations.  Our methodology includes (i) a rigorous annotation protocol linking visualelements to narrative structure through aligned light novel text, (ii)comprehensive evaluation across multiple reasoning paradigms, including directinference and retrieval-augmented generation, and (iii) cross-modal similarityanalysis revealing fundamental misalignments in current VLMs' jointrepresentations. Applying this framework to Re:Zero manga across 11 chapterswith 308 annotated panels, we conduct the first systematic study of long-formnarrative understanding in VLMs through three core evaluation axes: generativestorytelling, contextual dialogue grounding, and temporal reasoning. Ourfindings demonstrate that current models lack genuine story-level intelligence,struggling particularly with non-linear narratives, character consistency, andcausal inference across extended sequences. This work establishes both thefoundation and practical methodology for evaluating narrative intelligence,while providing actionable insights into the capability of deep sequentialunderstanding of Discrete Visual Narratives beyond basic recognition inMultimodal Models.</description>
      <author>example@mail.com (Aaditya Baranwal, Madhav Kataria, Naitik Agrawal, Yogesh S Rawat, Shruti Vyas)</author>
      <guid isPermaLink="false">2508.08508v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Audio-Visual Speech Enhancement: Architectural Design and Deployment Strategies</title>
      <link>http://arxiv.org/abs/2508.08468v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种基于AI的视听语音增强系统，并比较了不同部署架构的性能表现。&lt;h4&gt;背景&lt;/h4&gt;随着通信技术的发展，语音增强技术在各种应用场景中变得越来越重要，特别是在嘈杂环境下的语音通信。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的视听语音增强系统，并分析不同部署架构在性能、延迟和计算开销方面的差异，为实际应用提供部署指南。&lt;h4&gt;方法&lt;/h4&gt;系统采用卷积神经网络进行频谱特征提取，使用长短期记忆网络进行时间建模，通过音频和视觉提示的多模态融合实现语音增强。研究比较了云部署、边缘辅助部署和独立设备部署三种架构，并在以太网、Wi-Fi、4G和5G等不同网络条件下进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;云部署方案虽然能实现最高的语音增强质量，但延迟较高；边缘辅助架构在语音质量和延迟之间取得了最佳平衡，能够满足5G和Wi-Fi 6条件下的实时要求。&lt;h4&gt;结论&lt;/h4&gt;研究结果为在不同应用场景中选择和优化视听语音增强系统部署架构提供了实用指导，包括辅助听力设备、远程呈现和工业通信等领域。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种新的基于人工智能的视听语音增强系统，并对不同部署架构的性能进行了比较分析。所提出的AVSE系统采用卷积神经网络进行频谱特征提取，使用长短期记忆网络进行时间建模，通过音频和视觉提示的多模态融合实现稳健的语音增强。研究调查了多种部署场景，包括基于云的、边缘辅助的和独立设备的实现。从语音质量改进、延迟和计算开销等方面评估了它们的性能。在各种网络条件下进行了实际实验，包括以太网、Wi-Fi、4G和5G，以分析处理延迟、通信延迟和感知语音质量之间的权衡。结果表明，虽然云部署实现了最高的增强质量，但边缘辅助架构在延迟和可理解性之间提供了最佳平衡，能够满足5G和Wi-Fi 6条件下的实时要求。这些发现为在不同应用中选择和优化AVSE部署架构提供了实用指导，包括辅助听力设备、远程呈现和工业通信。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a new AI-based Audio-Visual Speech Enhancement (AVSE)system and presents a comparative performance analysis of different deploymentarchitectures. The proposed AVSE system employs convolutional neural networks(CNNs) for spectral feature extraction and long short-term memory (LSTM)networks for temporal modeling, enabling robust speech enhancement throughmultimodal fusion of audio and visual cues. Multiple deployment scenarios areinvestigated, including cloud-based, edge-assisted, and standalone deviceimplementations. Their performance is evaluated in terms of speech qualityimprovement, latency, and computational overhead. Real-world experiments areconducted across various network conditions, including Ethernet, Wi-Fi, 4G, and5G, to analyze the trade-offs between processing delay, communication latency,and perceptual speech quality. The results show that while cloud deploymentachieves the highest enhancement quality, edge-assisted architectures offer thebest balance between latency and intelligibility, meeting real-timerequirements under 5G and Wi-Fi 6 conditions. These findings provide practicalguidelines for selecting and optimizing AVSE deployment architectures indiverse applications, including assistive hearing devices, telepresence, andindustrial communications.</description>
      <author>example@mail.com (Anis Hamadouche, Haifeng Luo, Mathini Sellathurai, Tharm Ratnarajah)</author>
      <guid isPermaLink="false">2508.08468v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators</title>
      <link>http://arxiv.org/abs/2508.08328v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出DQ-Bench基准和DQ-Net网络，解决了四足机器人动态抓取问题，实现了在动态环境中的高效物体抓取。&lt;h4&gt;背景&lt;/h4&gt;带机械臂的四足机器人通过全身协调控制，在不规则、动态环境中具有强大移动性和适应性，但现有研究主要关注静态物体抓取，忽视了动态目标带来的挑战，限制了在物流分拣和人机协作等动态场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决四足机器人在动态环境中抓取动态目标的问题，提高机器人在动态场景中的适用性。&lt;h4&gt;方法&lt;/h4&gt;引入DQ-Bench基准系统评估不同条件下的动态抓取能力；提出DQ-Net教师-学生框架，教师网络利用特权信息建模目标特性并提供指导，学生网络仅使用有限感知数据进行双视点时间建模实现闭环动作输出。&lt;h4&gt;主要发现&lt;/h4&gt;在DQ-Bench上的大量实验表明，DQ-Net在多种任务设置中实现了稳健的动态物体抓取，在成功率和响应速度方面显著优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;DQ-Bench和DQ-Net为四足机器人在动态环境中的抓取任务提供了有效解决方案，显著提高了机器人在动态场景中的适用性。&lt;h4&gt;翻译&lt;/h4&gt;带机械臂的四足机器人通过全身协调控制，在不规则、动态环境中提供了强大的移动性和抓取适应性。然而，现有研究主要关注静态物体抓取，忽视了动态目标带来的挑战，从而限制了在物流分拣和人机协作等动态场景中的应用。为此，我们引入了DQ-Bench，一个新的基准，系统性地评估不同物体运动、速度、高度、物体类型和地形复杂度下的动态抓取能力，并配有全面的评估指标。基于此基准，我们提出了DQ-Net，一个紧凑的教师-学生框架，设计用于从有限的感知线索中推断抓取配置。在训练过程中，教师网络利用特权信息全面建模目标的静态几何特性和动态运动特征，并集成抓取融合模块为运动规划提供稳健指导。同时，我们设计了一个轻量级的学生网络，仅使用目标掩码、深度图和本体感受状态进行双视点时间建模，实现闭环动作输出而不依赖特权数据。在DQ-Bench上的大量实验表明，DQ-Net在多种任务设置中实现了稳健的动态物体抓取，在成功率和响应速度方面显著优于基线方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决四足机器人在动态环境中抓取移动物体的问题。这个问题在现实中非常重要，因为真实世界中的物体通常是运动的，而非静止的，动态抓取是物流分拣、人机协作等应用场景的核心需求。现有研究主要集中在静态物体抓取上，限制了机器人在实际动态环境中的应用能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有研究的局限性，然后设计了DQ-Bench基准测试平台来系统评估动态抓取性能。基于此，他们提出了DQ-Net教师-学生框架：教师网络利用特权信息学习高质量策略，学生网络仅使用可感知输入模仿教师行为。作者还设计了抓取融合模块(GFM)通过注意力机制动态选择最优抓取姿势。方法借鉴了强化学习、知识蒸馏、Transformer架构和注意力机制等技术，但将其创新性地应用于四足机器人的动态抓取任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用教师-学生框架解决动态抓取问题，通过抓取融合模块动态选择最优抓取姿势，并采用双视角视觉编码增强对物体动态的建模。整体流程分为三部分：1)教师网络使用特权信息和GFM训练，生成高质量抓取策略；2)学生网络仅使用双视角视觉输入和本体感受状态，通过双流Transformer学习模仿教师行为；3)低层控制器将高层命令转换为可执行的关节角度控制信号，实现全身协调控制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DQ-Bench基准测试平台，首个专门针对四足机器人动态抓取的评估框架；2)DQ-Net教师-学生框架，结合抓取融合模块和轻量级学生网络；3)抓取融合模块(GFM)，通过注意力机制动态选择最优抓取姿势；4)双视角时序建模，使用双流Transformer捕获全局语义和细粒度动态。相比之前工作，该方法从静态扩展到动态抓取，从依赖高质量视觉输入扩展到使用有限感知输入，通过记忆库提高计算效率，并提出了专门的评估指标全面评估性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DQ-Bench和DQ-Net共同解决了四足机器人在动态环境中抓取移动物体的挑战，通过教师-学生框架和抓取融合模块实现了高效、鲁棒的全身协调控制，显著提高了动态抓取的成功率和响应速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quadrupedal robots with manipulators offer strong mobility and adaptabilityfor grasping in unstructured, dynamic environments through coordinatedwhole-body control. However, existing research has predominantly focused onstatic-object grasping, neglecting the challenges posed by dynamic targets andthus limiting applicability in dynamic scenarios such as logistics sorting andhuman-robot collaboration. To address this, we introduce DQ-Bench, a newbenchmark that systematically evaluates dynamic grasping across varying objectmotions, velocities, heights, object types, and terrain complexities, alongwith comprehensive evaluation metrics. Building upon this benchmark, we proposeDQ-Net, a compact teacher-student framework designed to infer graspconfigurations from limited perceptual cues. During training, the teachernetwork leverages privileged information to holistically model both the staticgeometric properties and dynamic motion characteristics of the target, andintegrates a grasp fusion module to deliver robust guidance for motionplanning. Concurrently, we design a lightweight student network that performsdual-viewpoint temporal modeling using only the target mask, depth map, andproprioceptive state, enabling closed-loop action outputs without reliance onprivileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Netachieves robust dynamic objects grasping across multiple task settings,substantially outperforming baseline methods in both success rate andresponsiveness.</description>
      <author>example@mail.com (Qiwei Liang, Boyang Cai, Rongyi He, Hui Li, Tao Teng, Haihan Duan, Changxin Huang, Runhao Zeng)</author>
      <guid isPermaLink="false">2508.08328v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Dynamic Scenes in Ego Centric 4D Point Clouds</title>
      <link>http://arxiv.org/abs/2508.07251v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了EgoDynamic4D，一个用于从自我中心视角理解动态4D场景的新型QA基准数据集，包含RGB-D视频、相机姿态、实例掩码和4D边界框，并配有927K个问答对和思维链推理。作者还提出了一个统一的时空推理框架，用于处理这些任务。&lt;h4&gt;背景&lt;/h4&gt;从自我中心视角理解动态4D场景（随时间变化的3D空间结构）对人机交互、自主导航和具身智能至关重要。现有自我中心数据集虽包含动态场景，但缺乏统一的4D注释和针对细粒度时空推理的任务驱动评估协议，特别是关于物体和人体运动及其相互作用的方面。&lt;h4&gt;目的&lt;/h4&gt;解决现有自我中心动态场景数据集缺乏统一4D注释和任务驱动评估协议的问题，特别是针对细粒度时空推理，包括物体和人体运动及其相互作用。&lt;h4&gt;方法&lt;/h4&gt;提出EgoDynamic4D基准数据集，包含RGB-D视频、相机姿态、全局唯一实例掩码和4D边界框，构建927K个问答对并配有思维链推理。设计12个动态QA任务，包括代理运动、人机交互等，并配有细粒度指标。提出端到端时空推理框架，统一动态和静态场景信息，使用实例感知特征编码、时间和相机编码，以及空间自适应下采样将4D场景压缩为令牌序列。&lt;h4&gt;主要发现&lt;/h4&gt;在EgoDynamic4D上的实验表明，所提出的方法持续优于基线方法，验证了多模态时间建模在自我中心动态场景理解中的有效性。&lt;h4&gt;结论&lt;/h4&gt;EgoDynamic4D基准数据集和相关方法为从自我中心视角理解动态4D场景提供了新资源和解决方案，特别是在细粒度时空推理方面，有助于推动人机交互、自主导航和具身智能领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;从自我中心视角理解动态4D场景——即随时间变化的3D空间结构的变化——对人机交互、自主导航和具身智能至关重要。虽然现有的自我中心数据集包含动态场景，但它们缺乏统一的4D注释和针对细粒度时空推理的任务驱动评估协议，特别是关于物体和人体的运动及其相互作用。为了解决这一差距，我们介绍了EgoDynamic4D，这是一个关于高度动态场景的新型QA基准，包含RGB-D视频、相机姿态、全局唯一实例掩码和4D边界框。我们构建了927K个问答对，并配有显式的思维链(CoT)，可实现可验证的、逐步的时空推理。我们设计了12个动态QA任务，包括代理运动、人机交互、轨迹预测、关系理解和时间因果推理，并配有细粒度、多维度的指标。为了解决这些任务，我们提出了一个端到端的时空推理框架，该框架统一了动态和静态场景信息，使用实例感知特征编码、时间和相机编码，以及空间自适应下采样，将大型4D场景压缩为LLMs可管理的令牌序列。在EgoDynamic4D上的实验表明，我们的方法持续优于基线方法，验证了多模态时间建模在自我中心动态场景理解中的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从第一人称视角理解动态4D场景（3D空间+时间维度）的问题。现有第一人称数据集缺乏统一的4D标注和任务驱动的评估协议，无法进行细粒度的时空推理，特别是关于物体和人类的运动及其相互作用。这个问题在现实中非常重要，因为机器人感知、增强现实和自动驾驶等应用需要高效准确的第一人称场景理解，以赋能下一代具身智能体。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，然后整合了ADT和THUD++两个数据集，创建了统一的、多模态的数据集。他们设计了12个动态QA任务，涵盖场景描述、瞬时动力学和持续动力学。方法设计上借鉴了ConceptFusion和3DLLM的视觉编码技术，使用八叉树进行空间下采样，傅里叶基进行时间编码，以及注意力机制进行相机姿态压缩。这些现有工作为作者提供了基础，但作者在此基础上进行了创新，使其更适合4D动态场景的理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是统一动态和静态场景信息，通过实例感知特征编码、时间和相机编码以及空间自适应下采样，将大型4D场景压缩为LLM可处理的令牌序列。整体流程分为三步：1)实例和时间戳增强的点级特征提取，包括视觉特征、唯一实例嵌入和时间戳；2)特征融合，使用八叉树下采样和傅里叶时间编码；3)投影为LLM令牌，结合相机嵌入和LoRA微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)EgoDynamic4D基准测试，首个专门用于高度动态4D场景理解的第一人称QA基准，包含927K个QA对和12种任务；2)端到端时空推理框架，提出全局唯一实例嵌入、体素时间戳编码和相机嵌入三个新组件；3)多阶段QA数据构建管道。相比之前工作，该研究提供了完整的4D标注而非部分或静态标注，专注于时空推理而非仅图生成，并支持直接QA任务而非仅表示构建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了EgoDynamic4D首个专门用于高度动态4D场景理解的第一人称QA基准测试，以及一个端到端的时空推理框架，实现了对第一人称视角下物体和人类运动及其相互作用的细粒度理解和推理。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding dynamic 4D scenes from an egocentric perspective-modelingchanges in 3D spatial structure over time-is crucial for human-machineinteraction, autonomous navigation, and embodied intelligence. While existingegocentric datasets contain dynamic scenes, they lack unified 4D annotationsand task-driven evaluation protocols for fine-grained spatio-temporalreasoning, especially on motion of objects and human, together with theirinteractions. To address this gap, we introduce EgoDynamic4D, a novel QAbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,globally unique instance masks, and 4D bounding boxes. We construct 927K QApairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,step-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks coveringagent motion, human-object interaction, trajectory prediction, relationunderstanding, and temporal-causal reasoning, with fine-grained,multidimensional metrics. To tackle these tasks, we propose an end-to-endspatio-temporal reasoning framework that unifies dynamic and static sceneinformation, using instance-aware feature encoding, time and camera encoding,and spatially adaptive down-sampling to compress large 4D scenes into tokensequences manageable by LLMs. Experiments on EgoDynamic4D show that our methodconsistently outperforms baselines, validating the effectiveness of multimodaltemporal modeling for egocentric dynamic scene understanding.</description>
      <author>example@mail.com (Junsheng Huang, Shengyu Hao, Bocheng Hu, Gaoang Wang)</author>
      <guid isPermaLink="false">2508.07251v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>GRAVITY: A Controversial Graph Representation Learning for Vertex Classification</title>
      <link>http://arxiv.org/abs/2508.08954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GRAVITY框架，一种基于图的表示学习方法，通过顶点相互作用拓扑实现准确的顶点分类。&lt;h4&gt;背景&lt;/h4&gt;在寻求准确的顶点分类过程中，需要开发新的方法来提高分类性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于图的表示学习方法，用于顶点分类任务，提高分类准确性。&lt;h4&gt;方法&lt;/h4&gt;GRAVITY受物理系统启发，将每个顶点建模为通过学习到的相互作用施加影响，这些相互作用由结构邻近性和属性相似性形成。这些相互作用诱导一个潜在势场，顶点在其中向能量有效的位置移动，围绕类别一致的吸引子聚集。与传统消息传递方案不同，GRAVITY自适应调制每个顶点的感受野，实现由上下文驱动的动态聚合。&lt;h4&gt;主要发现&lt;/h4&gt;GRAVITY通过场驱动的组织使类别边界更加清晰，促进潜在簇内的语义一致性，在直推和归纳顶点分类任务中均表现出色。&lt;h4&gt;结论&lt;/h4&gt;在真实世界基准测试上的实验表明，GRAVITY能够产生具有竞争力的嵌入，在直推和归纳顶点分类任务中均表现出色。&lt;h4&gt;翻译&lt;/h4&gt;在寻求准确的顶点分类过程中，我们引入了GRAVITY(基于图的表示学习通过顶点相互作用拓扑)，这是一种受物理系统启发的框架，在物理系统中，物体在吸引力作用下自组织。GRAVITY将每个顶点建模为通过学习到的相互作用施加影响，这些相互作用由结构邻近性和属性相似性形成。这些相互作用诱导一个潜在势场，顶点在其中向能量有效的位置移动，围绕类别一致的吸引子聚集，并与无关组保持距离。与传统具有静态邻域的消息传递方案不同，GRAVITY基于学习到的力函数自适应地调制每个顶点的感受野，实现由上下文驱动的动态聚合。这种场驱动的组织使类别边界更加清晰，促进潜在簇内的语义一致性。在真实世界基准测试上的实验表明，GRAVITY能够产生具有竞争力的嵌入，在直推和归纳顶点分类任务中均表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the quest of accurate vertex classification, we introduce GRAVITY(Graph-based Representation leArning via Vertices Interaction TopologY), aframework inspired by physical systems where objects self-organize underattractive forces. GRAVITY models each vertex as exerting influence throughlearned interactions shaped by structural proximity and attribute similarity.These interactions induce a latent potential field in which vertices movetoward energy efficient positions, coalescing around class-consistentattractors and distancing themselves from unrelated groups. Unlike traditionalmessage-passing schemes with static neighborhoods, GRAVITY adaptively modulatesthe receptive field of each vertex based on a learned force function, enablingdynamic aggregation driven by context. This field-driven organization sharpensclass boundaries and promotes semantic coherence within latent clusters.Experiments on real-world benchmarks show that GRAVITY yields competitiveembeddings, excelling in both transductive and inductive vertex classificationtasks.</description>
      <author>example@mail.com (Etienne Gael Tajeuna, Jean Marie Tshimula)</author>
      <guid isPermaLink="false">2508.08954v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Learning Generalizable and Efficient Image Watermarking via Hierarchical Two-Stage Optimization</title>
      <link>http://arxiv.org/abs/2508.08667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为分层水印学习（HiWL）的新方法，通过两阶段优化解决了现有深度水印技术在不可见性、鲁棒性和广泛适用性方面难以同时满足的问题。该方法在实验中表现出色，水印提取准确率比现有方法提高7.6%，同时保持极低延迟（8秒处理10万张图像）。&lt;h4&gt;背景&lt;/h4&gt;深度图像水印技术能够在载体图像中实现不可见水印嵌入和可靠提取，对图像资产版权保护有效。然而，现有方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有水印方法难以同时满足三个关键标准（不可见性、鲁棒性和广泛适用性）的问题。&lt;h4&gt;方法&lt;/h4&gt;提出分层水印学习（HiWL）两阶段优化方法：第一阶段使用分布对齐学习建立具有视觉一致性和信息不变性约束的公共潜在空间；第二阶段采用广义水印表示学习在RGB空间中建立分离水印与图像内容的解纠缠策略，并对相同消息对应的RGB水印大幅波动进行强惩罚。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明HiWL方法有效，水印提取准确率比现有方法提高7.6%，同时保持极低延迟（8秒处理10万张图像）。&lt;h4&gt;结论&lt;/h4&gt;HiWL能够有效学习可推广的潜在空间水印表示，同时保持广泛适用性，解决了现有水印技术的局限性。&lt;h4&gt;翻译&lt;/h4&gt;深度图像水印技术，即在载体图像中实现不可见水印嵌入和可靠提取，已被证明对图像资产版权保护有效。然而，现有方法在同时满足可推广水印的三个基本标准方面存在局限性：1）不可见性（水印的不可见隐藏），2）鲁棒性（在不同条件下可靠的水印恢复），以及3）广泛适用性（水印处理过程中的低延迟）。为解决这些限制，我们提出了一种分层水印学习（HiWL），这是一种两阶段优化，使水印模型能够同时实现这三个标准。在第一阶段，设计了分布对齐学习来建立具有两个约束的公共潜在空间：1）水印图像和非水印图像之间的视觉一致性，以及2）水印潜在表示之间的信息不变性。这样，包括水印消息（二进制码）和载体图像（RGB像素）在内的多模态输入可以得到良好表示，从而确保水印的不可见性和水印处理过程中的鲁棒性。第二阶段采用广义水印表示学习来建立分离RGB空间中水印与图像内容的解纠缠策略。特别是，它对相同消息对应的分离RGB水印的大幅波动进行强惩罚。因此，HiWL在保持广泛适用性的同时，有效学习了可推广的潜在空间水印表示。大量实验证明了所提出方法的有效性。特别是，它在水印提取方面比现有方法提高了7.6%的准确率，同时保持极低的延迟（8秒处理10万张图像）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep image watermarking, which refers to enable imperceptible watermarkembedding and reliable extraction in cover images, has shown to be effectivefor copyright protection of image assets. However, existing methods facelimitations in simultaneously satisfying three essential criteria forgeneralizable watermarking: 1) invisibility (imperceptible hide of watermarks),2) robustness (reliable watermark recovery under diverse conditions), and 3)broad applicability (low latency in watermarking process). To address theselimitations, we propose a Hierarchical Watermark Learning (HiWL), a two-stageoptimization that enable a watermarking model to simultaneously achieve threecriteria. In the first stage, distribution alignment learning is designed toestablish a common latent space with two constraints: 1) visual consistencybetween watermarked and non-watermarked images, and 2) information invarianceacross watermark latent representations. In this way, multi-modal inputsincluding watermark message (binary codes) and cover images (RGB pixels) can bewell represented, ensuring the invisibility of watermarks and robustness inwatermarking process thereby. The second stage employs generalized watermarkrepresentation learning to establish a disentanglement policy for separatingwatermarks from image content in RGB space. In particular, it stronglypenalizes substantial fluctuations in separated RGB watermarks corresponding toidentical messages. Consequently, HiWL effectively learns generalizablelatent-space watermark representations while maintaining broad applicability.Extensive experiments demonstrate the effectiveness of proposed method. Inparticular, it achieves 7.6\% higher accuracy in watermark extraction thanexisting methods, while maintaining extremely low latency (100K imagesprocessed in 8s).</description>
      <author>example@mail.com (Ke Liu, Xuanhan Wang, Qilong Zhang, Lianli Gao, Jingkuan Song)</author>
      <guid isPermaLink="false">2508.08667v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>$\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models</title>
      <link>http://arxiv.org/abs/2508.08657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了M²LLM多视图框架，通过整合分子结构、任务和规则三个视角，利用大型语言模型生成丰富的分子表示，实现了在分子属性预测任务上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;准确的分子属性预测在化学、材料科学和药物发现领域至关重要。现有的分子表示方法如指纹和图神经网络虽能从分子结构中提取特征并取得先进结果，但往往忽视了数十年积累的语义和上下文知识。&lt;h4&gt;目的&lt;/h4&gt;解决现有分子表示方法忽视语义和上下文知识的问题，探索大型语言模型在生成丰富分子表示方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出M²LLM多视图框架，整合三个视角：分子结构视图、分子任务视图和分子规则视图，并通过动态融合这些视图来适应不同任务需求。&lt;h4&gt;主要发现&lt;/h4&gt;M²LLM在多个基准测试的分类和回归任务上取得了最先进的性能。通过利用大型语言模型的两个核心功能（编码能力生成分子嵌入和高级推理过程整理分子特征），实现了卓越的性能表现。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型能够通过多视角推理生成丰富的分子表示，M²LLM框架有效整合了分子结构、任务和规则视图，提高了分子属性预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;准确的分子属性预测是一个关键挑战，在化学、材料科学和药物发现中有广泛应用。分子表示方法，包括指纹和图神经网络(GNNs)，通过有效从分子结构中提取特征，取得了最先进的结果。然而，这些方法往往忽视了数十年来积累的语义和上下文知识。大型语言模型(LLMs)的最新进展展示了在科学领域跨领域的显著推理能力和先验知识，这使我们假设当被引导从多个视角进行推理时，LLM可以生成丰富的分子表示。为解决这些差距，我们提出了M²LLM，一个多视图框架，整合了三个视角：分子结构视图、分子任务视图和分子规则视图。这些视图动态融合以适应任务需求，实验证明M²LLM在多个基准测试的分类和回归任务上取得了最先进的性能。此外，我们证明通过利用两个核心功能，从LLM衍生的表示实现了卓越的性能：通过编码能力生成分子嵌入，以及通过高级推理过程整理分子特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate molecular property prediction is a critical challenge withwide-ranging applications in chemistry, materials science, and drug discovery.Molecular representation methods, including fingerprints and graph neuralnetworks (GNNs), achieve state-of-the-art results by effectively derivingfeatures from molecular structures. However, these methods often overlookdecades of accumulated semantic and contextual knowledge. Recent advancementsin large language models (LLMs) demonstrate remarkable reasoning abilities andprior knowledge across scientific domains, leading us to hypothesize that LLMscan generate rich molecular representations when guided to reason in multipleperspectives. To address these gaps, we propose $\text{M}^{2}$LLM, a multi-viewframework that integrates three perspectives: the molecular structure view, themolecular task view, and the molecular rules view. These views are fuseddynamically to adapt to task requirements, and experiments demonstrate that$\text{M}^{2}$LLM achieves state-of-the-art performance on multiple benchmarksacross classification and regression tasks. Moreover, we demonstrate thatrepresentation derived from LLM achieves exceptional performance by leveragingtwo core functionalities: the generation of molecular embeddings through theirencoding capabilities and the curation of molecular features through advancedreasoning processes.</description>
      <author>example@mail.com (Jiaxin Ju, Yizhen Zheng, Huan Yee Koh, Can Wang, Shirui Pan)</author>
      <guid isPermaLink="false">2508.08657v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Think as Cardiac Sonographers: Marrying SAM with Left Ventricular Indicators Measurements According to Clinical Guidelines</title>
      <link>http://arxiv.org/abs/2508.08566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AutoSAME框架，结合分割任意模型(SAM)的强大视觉理解能力，同时进行左心室分割和关键点定位，实现与临床指南一致的LV指标测量。&lt;h4&gt;背景&lt;/h4&gt;左心室指标测量对心血管疾病诊断很重要，但现有算法因训练数据集小难以捕捉通用视觉表示。视觉基础模型虽有丰富知识，但如SAM等模型适合分割但不擅长识别关键解剖点。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时进行分割和关键点定位的框架，模仿心脏超声操作者操作，实现与临床指南一致的LV指标测量。&lt;h4&gt;方法&lt;/h4&gt;提出AutoSAME框架，引入过滤交叉分支注意力(FCBA)利用分割特征增强关键点热图回归，以及空间引导提示对齐(SGPA)根据LV空间属性自动生成提示嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在超声心动图数据集上的大量实验证明了各设计元素的有效性，AutoSAME在LV分割、关键点定位和指标测量方面表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;AutoSAME框架成功模仿了心脏超声操作者的工作方式，实现了与临床指南一致的LV指标测量，代码将在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;左心室(LV)指标测量遵循临床超声心动图指南对诊断心血管疾病至关重要。尽管现有算法已探索自动化LV量化，但由于通常训练数据集较小，它们难以捕捉通用视觉表示。因此，有必要引入具有丰富知识的视觉基础模型(VFM)。然而，以分割任意模型(SAM)为代表的VFM通常适合分割但无法识别关键解剖点，这对LV指标测量至关重要。在本文中，我们提出了一个名为AutoSAME的新框架，结合SAM的强大视觉理解能力，同时进行分割和关键点定位任务。因此，该框架模仿了心脏超声操作者的操作，实现了与临床指南一致的LV指标测量。我们在AutoSAME中进一步提出了过滤交叉分支注意力(FCBA)，利用分割中的相对全面特征从频域角度增强关键点的热图回归(HR)，优化了后者学习的视觉表示。此外，我们提出了空间引导提示对齐(SGPA)，根据LV的空间属性自动生成提示嵌入，从而通过先验空间知识提高密集预测的准确性。在超声心动图数据集上的大量实验证明了每个设计的效率和AutoSAME在LV分割、关键点定位和指标测量方面的优越性。代码将在https://github.com/QC-LIU-1997/AutoSAME上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Left ventricular (LV) indicator measurements following clinicalechocardiog-raphy guidelines are important for diagnosing cardiovasculardisease. Alt-hough existing algorithms have explored automated LVquantification, they can struggle to capture generic visual representations dueto the normally small training datasets. Therefore, it is necessary tointroduce vision founda-tional models (VFM) with abundant knowledge. However,VFMs represented by the segment anything model (SAM) are usually suitable forsegmentation but incapable of identifying key anatomical points, which arecritical in LV indicator measurements. In this paper, we propose a novelframework named AutoSAME, combining the powerful visual understanding of SAMwith seg-mentation and landmark localization tasks simultaneously.Consequently, the framework mimics the operation of cardiac sonographers,achieving LV indi-cator measurements consistent with clinical guidelines. Wefurther present fil-tered cross-branch attention (FCBA) in AutoSAME, whichleverages relatively comprehensive features in the segmentation to enhance theheatmap regression (HR) of key points from the frequency domain perspective,optimizing the vis-ual representation learned by the latter. Moreover, wepropose spatial-guided prompt alignment (SGPA) to automatically generate promptembeddings guid-ed by spatial properties of LV, thereby improving the accuracyof dense pre-dictions by prior spatial knowledge. The extensive experiments onan echocar-diography dataset demonstrate the efficiency of each design and thesuperiori-ty of our AutoSAME in LV segmentation, landmark localization, andindicator measurements. The code will be available athttps://github.com/QC-LIU-1997/AutoSAME.</description>
      <author>example@mail.com (Tuo Liu, Qinghan Yang, Yu Zhang, Rongjun Ge, Yang Chen, Guangquan Zhou)</author>
      <guid isPermaLink="false">2508.08566v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>CObL: Toward Zero-Shot Ordinal Layering without User Prompting</title>
      <link>http://arxiv.org/abs/2508.08498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025: Project page with demo, datasets, and code:  https://vision.seas.harvard.edu/cobl/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为Concurrent Object Layers (CObL)的扩散架构，能够并行生成物体层堆栈来表示场景中的物体及其空间关系，包括横向和深度方向的遮挡关系。&lt;h4&gt;背景&lt;/h4&gt;视觉系统受益于将像素分组为物体并理解它们的空间关系，而现有的模态物体分割和无监督物体中心表示学习模型存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获场景中物体遮挡关系的场景表示方法，并构建相应的架构来推断这种表示。&lt;h4&gt;方法&lt;/h4&gt;提出CObL架构，使用Stable Diffusion作为自然物体的先验，通过推理时指导确保推断的层能够复合回输入图像，并使用合成的多物体桌面场景图像进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;CObL能够零样本泛化到具有不同数量新物体的真实世界桌面照片；无需用户提示且无需预先知道物体数量即可重建多个被遮挡物体；不限于训练时的世界，能够处理未知场景。&lt;h4&gt;结论&lt;/h4&gt;CObL有效地解决了场景表示中的物体分组和空间关系理解问题，为视觉场景理解提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;视觉受益于将像素分组为物体并理解它们的空间关系，包括横向和深度方向。我们通过包含遮挡排序的'物体层'堆栈的场景表示来捕获这一点，每层包含一个被隔离和模态完成的物体。为了从图像推断这种表示，我们引入了一种名为Concurrent Object Layers (CObL)的基于扩散的架构。CObL并行生成物体层堆栈，使用Stable Diffusion作为自然物体的先验，并使用推理时指导确保推断的层能够复合回输入图像。我们使用数千个合成的多物体桌面场景图像训练CObL，发现它能够零样本泛化到具有不同数量新物体的真实世界桌面照片。与最近的模态物体分割模型相比，CObL可以在没有用户提示且不知道物体数量的情况下重建多个被遮挡物体。与之前无监督物体中心表示学习模型不同，CObL不限于其训练时的世界。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何在没有用户提示的情况下，将图像自动分解为按遮挡顺序排列的'对象层'堆叠的问题。每个对象层包含一个完整补全的对象（包括被遮挡部分），这些层按顺序组合后能重构原始图像。这个问题在计算机视觉中非常重要，因为它模拟了人类视觉系统对场景的理解能力，对于增强现实、机器人导航、图像编辑和场景理解等多种应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到缺乏带标注的训练数据是重大挑战，因此设计了一个创新的合成训练数据管道。他们借鉴了多项现有工作：使用Stable Diffusion作为基础模型；采用类似视频生成中并发生成多个帧的思路，但应用于对象层；使用交叉层注意力权重连接UNet；采用类似score distillation sampling的先验分数匹配损失；以及合成到真实的迁移策略。作者将这些技术组合创新，设计了能够并行生成多个对象层的架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用多个并行的Stable Diffusion UNet同时生成多个对象层，通过可学习的交叉层注意力连接这些UNet使各层之间通信，并使用推理时指导确保生成的层能组合回原始图像。整体流程包括：1) 创建合成训练数据，结合3D建模和文本到图像生成；2) 设计模型架构，使用N个冻结的Stable Diffusion UNet副本通过交叉层注意力连接；3) 训练模型，优化输入条件和交叉层注意力的参数；4) 推理过程，使用DDIM采样结合组合损失和先验分数匹配损失进行指导；5) 应用非微分更新操作如排列、擦除和排序。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 零样本分层排序，自动完成多个被遮挡对象的分解；2) 并行生成所有对象层而非顺序生成；3) 组合指导确保生成的层能重构原始图像；4) 创新的合成训练数据管道；5) 先验分数匹配损失确保生成内容在自然图像分布内。相比之前工作，CObL不需要用户提示或输入掩码，能同时处理多个对象，并且不局限于训练时见过的对象类别。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CObL通过创新的扩散模型架构和组合指导策略，实现了无需用户提示的零样本图像分层排序，能够自动将图像分解为按遮挡顺序排列的完整对象层堆叠。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision benefits from grouping pixels into objects and understanding theirspatial relationships, both laterally and in depth. We capture this with ascene representation comprising an occlusion-ordered stack of "object layers,"each containing an isolated and amodally-completed object. To infer thisrepresentation from an image, we introduce a diffusion-based architecture namedConcurrent Object Layers (CObL). CObL generates a stack of object layers inparallel, using Stable Diffusion as a prior for natural objects andinference-time guidance to ensure the inferred layers composite back to theinput image. We train CObL using a few thousand synthetically-generated imagesof multi-object tabletop scenes, and we find that it zero-shot generalizes tophotographs of real-world tabletops with varying numbers of novel objects. Incontrast to recent models for amodal object completion, CObL reconstructsmultiple occluded objects without user prompting and without knowing the numberof objects beforehand. Unlike previous models for unsupervised object-centricrepresentation learning, CObL is not limited to the world it was trained in.</description>
      <author>example@mail.com (Aneel Damaraju, Dean Hazineh, Todd Zickler)</author>
      <guid isPermaLink="false">2508.08498v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>SHeRL-FL: When Representation Learning Meets Split Learning in Hierarchical Federated Learning</title>
      <link>http://arxiv.org/abs/2508.08339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SHeRL-FL是一种新的联邦学习方法，通过整合分割学习和分层模型聚合，结合中间层的表示学习，显著降低了协调复杂性和通信开销，在多个图像分类和分割任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;联邦学习(FL)是一种有前景的方法，通过在不共享原始数据的情况下进行协作模型训练，解决大规模网络的可扩展性和延迟问题。然而，现有FL框架往往忽略边缘客户端的计算异构性和资源有限设备的训练负担，且面临高通信成本和复杂模型聚合问题。之前结合分割学习(SL)和分层FL(HierFL)的方法虽减少了设备端计算，但引入了跨层协调的复杂性。&lt;h4&gt;目的&lt;/h4&gt;解决现有联邦学习框架中的协调复杂性和通信开销问题，特别是在处理大规模模型和资源有限设备时。&lt;h4&gt;方法&lt;/h4&gt;提出SHeRL-FL，整合分割学习和分层模型聚合，在中间层引入表示学习。允许客户端和边缘服务器独立于云计算训练目标，从而降低协调复杂性和通信开销。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10、CIFAR-100和HAM10000数据集上使用AlexNet、ResNet-18和ResNet-50的图像分类任务，以及在ISIC-2018数据集上使用ResNet-50-based U-Net的图像分割任务实验表明，SHeRL-FL比集中式FL和HierFL减少90%以上的数据传输，比SplitFed减少50%，并改进了分层分割学习方法。&lt;h4&gt;结论&lt;/h4&gt;SHeRL-FL有效解决了联邦学习中的通信成本和协调复杂性问题，显著减少了数据传输量，同时保持了良好的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)是一种有前景的方法，通过在不共享原始数据的情况下进行协作模型训练，解决大规模网络的可扩展性和延迟问题。然而，现有的FL框架往往忽略了边缘客户端的计算异构性以及对资源有限设备的日益增长训练负担。然而，FL面临高通信成本和复杂模型聚合的问题，特别是对于大型模型而言。之前的工作结合了分割学习(SL)和分层FL(HierFL)来减少设备端计算并提高可扩展性，但这引入了跨层协调的训练复杂性。为解决这些问题，我们提出了SHeRL-FL，它集成了SL和分层模型聚合并在中间层结合了表示学习。通过允许客户端和边缘服务器独立于云计算训练目标，SHeRL-FL显著降低了协调复杂性和通信开销。为评估SHeRL-FL的有效性和效率，我们在CIFAR-10、CIFAR-100和HAM10000上使用AlexNet、ResNet-18和ResNet-50进行了图像分类任务的实验，在IID和非IID设置下进行。此外，我们使用基于ResNet-50的U-Net在ISIC-2018数据集上评估了图像分割任务。实验结果表明，与集中式FL和HierFL相比，SHeRL-FL减少了90%以上的数据传输，与作为FL和SL混合体的SplitFed相比减少了50%，并进一步改进了分层分割学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning (FL) is a promising approach for addressing scalabilityand latency issues in large-scale networks by enabling collaborative modeltraining without requiring the sharing of raw data. However, existing FLframeworks often overlook the computational heterogeneity of edge clients andthe growing training burden on resource-limited devices. However, FL suffersfrom high communication costs and complex model aggregation, especially withlarge models. Previous works combine split learning (SL) and hierarchical FL(HierFL) to reduce device-side computation and improve scalability, but thisintroduces training complexity due to coordination across tiers. To addressthese issues, we propose SHeRL-FL, which integrates SL and hierarchical modelaggregation and incorporates representation learning at intermediate layers. Byallowing clients and edge servers to compute training objectives independentlyof the cloud, SHeRL-FL significantly reduces both coordination complexity andcommunication overhead. To evaluate the effectiveness and efficiency ofSHeRL-FL, we performed experiments on image classification tasks usingCIFAR-10, CIFAR-100, and HAM10000 with AlexNet, ResNet-18, and ResNet-50 inboth IID and non-IID settings. In addition, we evaluate performance on imagesegmentation tasks using the ISIC-2018 dataset with a ResNet-50-based U-Net.Experimental results demonstrate that SHeRL-FL reduces data transmission byover 90\% compared to centralized FL and HierFL, and by 50\% compared toSplitFed, which is a hybrid of FL and SL, and further improves hierarchicalsplit learning methods.</description>
      <author>example@mail.com (Dung T. Tran, Nguyen B. Ha, Van-Dinh Nguyen, Kok-Seng Wong)</author>
      <guid isPermaLink="false">2508.08339v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>ImageDDI: Image-enhanced Molecular Motif Sequence Representation for Drug-Drug Interaction Prediction</title>
      <link>http://arxiv.org/abs/2508.08338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted By Information Fusion&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ImageDDI的图像增强分子基序序列表示框架，用于药物-药物相互作用预测，该方法同时考虑药物的全局和局部结构，通过自适应特征融合整合分子视觉信息，实验证明其性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;多药使用可能产生不良健康影响，包括意外副作用和相互作用，因此准确识别和预测药物-药物相互作用在深度学习领域被视为关键任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的药物-药物相互作用预测方法，能够同时考虑药物的全局和局部结构，克服现有方法中基于功能基序的表示学习瓶颈。&lt;h4&gt;方法&lt;/h4&gt;ImageDDI框架将分子标记化为功能基序，将药物对的基序组合成单一序列并使用基于transformer的编码器进行嵌入。通过利用药物对之间的关联，使用全局分子图像信息增强分子的空间表示。采用自适应特征融合将分子视觉信息整合到功能基序序列中，动态调整特征表示的融合过程以增强泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在常用数据集上的实验结果表明，ImageDDI优于最先进的方法。在2D和3D图像增强场景中都取得了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;ImageDDI是一种有效的药物-药物相互作用预测方法，能够同时考虑药物的全局和局部结构，并通过自适应特征融合整合分子视觉信息，提高了预测性能。&lt;h4&gt;翻译&lt;/h4&gt;为了减轻同时使用多种药物的潜在不良健康影响，包括意外副作用和相互作用，准确识别和预测药物-药物相互作用被视为深度学习领域的关键任务。尽管现有方法已显示出有希望的性能，但它们受限于基于功能基序的表示学习瓶颈，因为药物-药物相互作用本质上是由基序相互作用而非整体药物结构引起的。在本文中，我们提出了一种用于药物-药物相互作用预测的图像增强分子基序序列表示框架，称为ImageDDI，它从全局和局部结构表示药物对。具体来说，ImageDDI将分子标记化为功能基序。为了有效表示药物对，它们的基序被组合成单一序列，并使用基于transformer的编码器进行嵌入，从局部结构表示开始。通过利用药物对之间的关联，ImageDDI进一步使用全局分子图像信息(如纹理、阴影、颜色和平面空间关系)增强分子的空间表示。为了将分子视觉信息整合到功能基序序列中，ImageDDI采用自适应特征融合，通过动态调整特征表示的融合过程来增强ImageDDI的泛化能力。在常用数据集上的实验结果表明，ImageDDI优于最先进的方法。此外，大量实验表明，与其他模型相比，ImageDDI在2D和3D图像增强场景中都取得了具有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To mitigate the potential adverse health effects of simultaneous multi-druguse, including unexpected side effects and interactions, accurately identifyingand predicting drug-drug interactions (DDIs) is considered a crucial task inthe field of deep learning. Although existing methods have demonstratedpromising performance, they suffer from the bottleneck of limited functionalmotif-based representation learning, as DDIs are fundamentally caused by motifinteractions rather than the overall drug structures. In this paper, we proposean Image-enhanced molecular motif sequence representation framework for\textbf{DDI} prediction, called ImageDDI, which represents a pair of drugs fromboth global and local structures. Specifically, ImageDDI tokenizes moleculesinto functional motifs. To effectively represent a drug pair, their motifs arecombined into a single sequence and embedded using a transformer-based encoder,starting from the local structure representation. By leveraging theassociations between drug pairs, ImageDDI further enhances the spatialrepresentation of molecules using global molecular image information (e.g.texture, shadow, color, and planar spatial relationships). To integratemolecular visual information into functional motif sequence, ImageDDI employsAdaptive Feature Fusion, enhancing the generalization of ImageDDI bydynamically adapting the fusion process of feature representations.Experimental results on widely used datasets demonstrate that ImageDDIoutperforms state-of-the-art methods. Moreover, extensive experiments show thatImageDDI achieved competitive performance in both 2D and 3D image-enhancedscenarios compared to other models.</description>
      <author>example@mail.com (Yuqin He, Tengfei Ma, Chaoyi Li, Pengsen Ma, Hongxin Xiang, Jianmin Wang, Yiping Liu, Bosheng Song, Xiangxiang Zeng)</author>
      <guid isPermaLink="false">2508.08338v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>HSA-Net: Hierarchical and Structure-Aware Framework for Efficient and Scalable Molecular Language Modeling</title>
      <link>http://arxiv.org/abs/2508.08334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为HSA-Net的分层和结构感知网络框架，用于解决图神经网络在分子表示学习中的过平滑问题，通过结合交叉注意力和Mamba的优势，实现了更有效的特征投影和融合。&lt;h4&gt;背景&lt;/h4&gt;分子表示学习是分子字幕生成和分子属性预测等下游任务的基础，主要依赖图神经网络。然而，GNN存在过平滑问题，即节点特征在深层GNN层中会崩溃。现有的基于交叉注意力的特征投影方法在处理深层特征时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决GNN的过平滑问题，改进深层特征的处理能力，并解决Mamba和交叉注意力之间的全局-局部权衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出HSA-Net框架，包含两个主要模块：1)分层自适应投影器(HAP)，动态选择交叉注意力投影器处理浅层特征和结构感知的图Mamba投影器处理深层特征；2)源感知融合(SAF)模块，根据特征特点灵活选择融合专家进行多级特征的有效合并。&lt;h4&gt;主要发现&lt;/h4&gt;Mamba能够保留来自深层的全局拓扑信息，但忽略了浅层的细粒度细节，而交叉注意力则相反，两者存在全局-局部权衡关系。HSA-Net能够有效处理这种权衡，生成高质量的多级特征。&lt;h4&gt;结论&lt;/h4&gt;大量实验证明，HSA-Net框架在定量和定性上都优于当前最先进的方法，为分子表示学习提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;分子表示学习是分子字幕生成和分子属性预测等下游任务的基础，严重依赖图神经网络。然而，GNN存在过平滑问题，即节点特征在深层GNN层中会崩溃。虽然现有的基于交叉注意力的特征投影方法已被引入以缓解此问题，但它们在处理深层特征时仍然表现不佳。这促使我们探索使用Mamba作为替代投影器，因为它能够处理复杂序列。然而，我们观察到虽然Mamba擅长保留来自深层的全局拓扑信息，但它忽略了浅层的细粒度细节。Mamba和交叉注意力的能力存在全局-局部权衡。为解决这一关键的全局-局部权衡，我们提出了分层和结构感知网络(HSA-Net)，这是一个包含两个模块的新框架，可实现分层特征投影和融合。首先，引入分层自适应投影器(HAP)模块来处理来自不同图层的特征，它学会动态切换浅层的交叉注意力投影器和深层的结构感知图Mamba投影器，产生高质量的多级特征。其次，为了自适应地合并这些多级特征，我们设计了源感知融合(SAF)模块，它根据聚合特征的特点灵活选择融合专家，确保精确有效的最终表示融合。大量实验证明，我们的HSA-Net框架在定量和定性上都优于当前最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular representation learning, a cornerstone for downstream tasks likemolecular captioning and molecular property prediction, heavily relies on GraphNeural Networks (GNN). However, GNN suffers from the over-smoothing problem,where node-level features collapse in deep GNN layers. While existing featureprojection methods with cross-attention have been introduced to mitigate thisissue, they still perform poorly in deep features. This motivated ourexploration of using Mamba as an alternative projector for its ability tohandle complex sequences. However, we observe that while Mamba excels atpreserving global topological information from deep layers, it neglectsfine-grained details in shallow layers. The capabilities of Mamba andcross-attention exhibit a global-local trade-off. To resolve this criticalglobal-local trade-off, we propose Hierarchical and Structure-Aware Network(HSA-Net), a novel framework with two modules that enables a hierarchicalfeature projection and fusion. Firstly, a Hierarchical Adaptive Projector (HAP)module is introduced to process features from different graph layers. It learnsto dynamically switch between a cross-attention projector for shallow layersand a structure-aware Graph-Mamba projector for deep layers, producinghigh-quality, multi-level features. Secondly, to adaptively merge thesemulti-level features, we design a Source-Aware Fusion (SAF) module, whichflexibly selects fusion experts based on the characteristics of the aggregationfeatures, ensuring a precise and effective final representation fusion.Extensive experiments demonstrate that our HSA-Net framework quantitatively andqualitatively outperforms current state-of-the-art (SOTA) methods.</description>
      <author>example@mail.com (Zihang Shao, Wentao Lei, Lei Wang, Wencai Ye, Li Liu)</author>
      <guid isPermaLink="false">2508.08334v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment</title>
      <link>http://arxiv.org/abs/2508.08811v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025. Project page:  https://github.com/HVision-NKU/OffSeg&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种耦合的双分支偏移学习范式来解决语义分割在资源受限设备上的部署问题，通过学习特征和类别偏移来动态优化类别表示和图像特征，并在多个数据集上实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;语义分割对需要像素级场景理解的视觉系统至关重要，但在资源受限设备上部署需要高效架构。现有方法通过轻量级设计实现实时推理，但存在固有局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有语义分割方法中像素级分类范式导致的类别表示与图像特征不匹配问题，提高在资源受限设备上的分割性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种耦合的双分支偏移学习范式，明确学习特征和类别偏移来动态优化类别表示和空间图像特征，并基于此范式构建了高效语义分割网络OffSeg。&lt;h4&gt;主要发现&lt;/h4&gt;像素级分类范式导致了一个极具挑战性的假设：同一类别的图像像素特征在不同图像中不应变化。实验表明，所提出的偏移学习范式可以显著提升现有方法的性能。&lt;h4&gt;结论&lt;/h4&gt;偏移学习范式可以应用于现有方法而无需架构更改，在四个数据集上实现了一致的性能提升且仅需极少的额外参数。例如，在ADE20K数据集上将SegFormer-B0等模型的mIoU提高了约2%，仅需0.1-0.2M额外参数。&lt;h4&gt;翻译&lt;/h4&gt;语义分割对需要像素级场景理解的视觉系统至关重要，但在资源受限设备上部署需要高效架构。尽管现有方法通过轻量级设计实现实时推理，但我们揭示了它们的固有局限性：像素级分类范式导致的类别表示与图像特征不匹配。通过实验分析，我们发现这种范式导致高效场景下的一个极具挑战性的假设：同一类别的图像像素特征在不同图像中不应变化。为解决这一困境，我们提出了一种耦合的双分支偏移学习范式，明确学习特征和类别偏移以动态优化类别表示和空间图像特征。基于所提出的范式，我们构建了一个高效的语义分割网络OffSeg。值得注意的是，偏移学习范式可以应用于现有方法而无需额外的架构更改。在ADE20K、Cityscapes、COCO-Stuff-164K和Pascal Context四个数据集上的广泛实验展示了一致的改进且参数可忽略不计。例如，在ADE20K数据集上，我们提出的偏移学习范式分别将SegFormer-B0、SegNeXt-T和Mask2Former-Tiny的mIoU提高了2.7%、1.9%和2.6%，仅需0.1-0.2M额外参数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation is fundamental to vision systems requiring pixel-levelscene understanding, yet deploying it on resource-constrained devices demandsefficient architectures. Although existing methods achieve real-time inferencethrough lightweight designs, we reveal their inherent limitation: misalignmentbetween class representations and image features caused by a per-pixelclassification paradigm. With experimental analysis, we find that this paradigmresults in a highly challenging assumption for efficient scenarios: Image pixelfeatures should not vary for the same category in different images. To addressthis dilemma, we propose a coupled dual-branch offset learning paradigm thatexplicitly learns feature and class offsets to dynamically refine both classrepresentations and spatial image features. Based on the proposed paradigm, weconstruct an efficient semantic segmentation network, OffSeg. Notably, theoffset learning paradigm can be adopted to existing methods with no additionalarchitectural changes. Extensive experiments on four datasets, includingADE20K, Cityscapes, COCO-Stuff-164K, and Pascal Context, demonstrate consistentimprovements with negligible parameters. For instance, on the ADE20K dataset,our proposed offset learning paradigm improves SegFormer-B0, SegNeXt-T, andMask2Former-Tiny by 2.7%, 1.9%, and 2.6% mIoU, respectively, with only 0.1-0.2Madditional parameters required.</description>
      <author>example@mail.com (Shi-Chen Zhang, Yunheng Li, Yu-Huan Wu, Qibin Hou, Ming-Ming Cheng)</author>
      <guid isPermaLink="false">2508.08811v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>OpenCUA: Open Foundations for Computer-Use Agents</title>
      <link>http://arxiv.org/abs/2508.09123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpenCUA是一个综合的开源框架，用于扩展计算机使用代理(CUA)数据和基础模型，包含注释基础设施、大规模数据集和可扩展的演示转换管道，其端到端代理模型在基准测试中表现出色，超过了现有的开源模型和OpenAI的CUA。&lt;h4&gt;背景&lt;/h4&gt;Vision-language models已展现出作为计算机使用代理的能力，能自动化各种计算机任务。随着这些系统商业潜力增长，最强大的CUA系统细节仍封闭。这些代理将越来越多地代表人们进行数字交互和重要决策，研究社区需要开放框架来研究其能力、局限性和风险。&lt;h4&gt;目的&lt;/h4&gt;弥合研究社区对开放CUA框架的需求，提出OpenCUA，一个用于扩展CUA数据和基础模型的综合开源框架。&lt;h4&gt;方法&lt;/h4&gt;包含三个主要部分：(1)注释基础设施，无缝捕获人类计算机使用演示；(2)AgentNet，首个大规模计算机使用任务数据集，涵盖3个操作系统和200多个应用程序和网站；(3)可扩展管道，将演示转换为具有反思性长链思维推理的状态-动作对，随数据扩展保持性能提升。&lt;h4&gt;主要发现&lt;/h4&gt;端到端代理模型在CUA基准测试中表现优异。OpenCUA-32B在OSWorld-Verified上平均成功率达34.8%，在开源模型中建立新最先进水平，超过OpenAI的CUA(GPT-4o)。方法跨领域泛化能力强，从增加测试时间计算中显著受益。&lt;h4&gt;结论&lt;/h4&gt;发布注释工具、数据集、代码和模型，为CUA研究建立开放基础，促进对这类系统的能力、局限性和风险的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型已展现出作为计算机使用代理(CUAs)的能力，能够自动化各种计算机任务。随着其商业潜力的增长，最强大的CUA系统的关键细节仍然封闭。由于这些代理将越来越多地代表我们调解数字交互并执行重要决策，研究社区需要访问开放的CUA框架来研究其能力、局限性和风险。为弥合这一差距，我们提出OpenCUA，一个用于扩展CUA数据和基础模型的综合开源框架。我们的框架包括：(1)一个注释基础设施，能够无缝捕获人类计算机使用演示；(2)AgentNet，首个大规模计算机使用任务数据集，涵盖3个操作系统和200多个应用程序和网站；(3)一个可扩展的管道，将演示转换为具有反思性长链思维推理的状态-动作对，随着数据扩展保持稳健的性能提升。我们的端到端代理模型在CUA基准测试中表现出强大的性能。特别是，OpenCUA-32B在OSWorld-Verified上平均成功率达到34.8%，在开源模型中建立了新的最先进水平(SOTA)，超过了OpenAI的CUA(GPT-4o)。进一步分析证实，我们的方法在跨领域泛化能力良好，并且从增加的测试时间计算中显著受益。我们发布注释工具、数据集、代码和模型，为CUA研究建立开放基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models have demonstrated impressive capabilities ascomputer-use agents (CUAs) capable of automating diverse computer tasks. Astheir commercial potential grows, critical details of the most capable CUAsystems remain closed. As these agents will increasingly mediate digitalinteractions and execute consequential decisions on our behalf, the researchcommunity needs access to open CUA frameworks to study their capabilities,limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensiveopen-source framework for scaling CUA data and foundation models. Our frameworkconsists of: (1) an annotation infrastructure that seamlessly captures humancomputer-use demonstrations; (2) AgentNet, the first large-scale computer-usetask dataset spanning 3 operating systems and 200+ applications and websites;(3) a scalable pipeline that transforms demonstrations into state-action pairswith reflective long Chain-of-Thought reasoning that sustain robust performancegains as data scales. Our end-to-end agent models demonstrate strongperformance across CUA benchmarks. In particular, OpenCUA-32B achieves anaverage success rate of 34.8% on OSWorld-Verified, establishing a newstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA(GPT-4o). Further analysis confirms that our approach generalizes well acrossdomains and benefits significantly from increased test-time computation. Werelease our annotation tool, datasets, code, and models to build openfoundations for further CUA research.</description>
      <author>example@mail.com (Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, Tao Yu)</author>
      <guid isPermaLink="false">2508.09123v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation</title>
      <link>http://arxiv.org/abs/2508.08939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ACM Multimedia Workshops&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探索了一种纯零样本的人脸变形攻击检测方法，通过CLIP模型设计和聚合多个文本提示来提高检测性能，无需额外训练或微调。&lt;h4&gt;背景&lt;/h4&gt;人脸变形攻击检测(MAD)是人脸识别安全的关键挑战，攻击者通过将多个人的身份信息插值到一张人脸图像中，生成可被验证为属于多个身份的样本。&lt;h4&gt;目的&lt;/h4&gt;探索一种纯零样本人脸变形攻击检测方法，利用CLIP基础模型的多模态知识，无需额外训练或微调。&lt;h4&gt;方法&lt;/h4&gt;利用CLIP模型不进行额外训练或微调，专注于设计和聚合每个类别的多个文本提示，通过聚合多样化提示的嵌入来对齐模型的内部表示与MAD任务。&lt;h4&gt;主要发现&lt;/h4&gt;提示聚合显著提高了零样本检测性能，能够捕获更丰富、更多样化的真实样本或攻击样本的线索。&lt;h4&gt;结论&lt;/h4&gt;通过高效的提示工程利用基础模型内置的多模态知识是一种有效的人脸变形攻击检测方法。&lt;h4&gt;翻译&lt;/h4&gt;人脸变形攻击检测(MAD)是人脸识别安全中的一个关键挑战，攻击者可以通过将两个或多个人的身份信息插值到一张人脸图像中，生成可以被人脸识别系统验证为属于多个身份的样本。虽然多模态基础模型(如CLIP)通过联合建模图像和文本提供了强大的零样本能力，但大多数先前的工作依赖于针对特定下游任务的微调，忽略了它们直接、可部署的潜力。这项工作探索了一种纯零样本的MAD方法，利用CLIP而不进行任何额外的训练或微调，而是专注于每个类别设计和聚合多个文本提示。通过聚合多样化提示的嵌入，我们可以更好地将模型的内部表示与MAD任务对齐，捕获更丰富、更多样化的真实样本或攻击样本的线索。我们的结果表明，提示聚合显著提高了零样本检测性能，证明了通过高效的提示工程利用基础模型内置的多模态知识是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3728425.3759909&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Face Morphing Attack Detection (MAD) is a critical challenge in facerecognition security, where attackers can fool systems by interpolating theidentity information of two or more individuals into a single face image,resulting in samples that can be verified as belonging to multiple identitiesby face recognition systems. While multimodal foundation models (FMs) like CLIPoffer strong zero-shot capabilities by jointly modeling images and text, mostprior works on FMs for biometric recognition have relied on fine-tuning forspecific downstream tasks, neglecting their potential for direct, generalizabledeployment. This work explores a pure zero-shot approach to MAD by leveragingCLIP without any additional training or fine-tuning, focusing instead on thedesign and aggregation of multiple textual prompts per class. By aggregatingthe embeddings of diverse prompts, we better align the model's internalrepresentations with the MAD task, capturing richer and more varied cuesindicative of bona-fide or attack samples. Our results show that promptaggregation substantially improves zero-shot detection performance,demonstrating the effectiveness of exploiting foundation models' built-inmultimodal knowledge through efficient prompt engineering.</description>
      <author>example@mail.com (Eduarda Caldeira, Fadi Boutros, Naser Damer)</author>
      <guid isPermaLink="false">2508.08939v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance</title>
      <link>http://arxiv.org/abs/2508.08774v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个记忆增强型AR代理的概念框架，通过四个相互连接的模块来支持个性化任务辅助，解决当前AR系统在处理复杂多步骤场景时的局限性。&lt;h4&gt;背景&lt;/h4&gt;增强现实系统越来越多地集成基础模型如多模态大语言模型，以提供更上下文感知和自适应的用户体验。当前AR代理能有效支持即时任务，但在需要理解和利用用户长期经验和偏好的复杂多步骤场景中存在困难，这源于它们无法在时空上下文中捕获、保留和推理历史用户交互。&lt;h4&gt;目的&lt;/h4&gt;提出一个记忆增强型AR代理的概念框架，使其能够通过随时间学习和适应用户特定经验来提供个性化的任务辅助。&lt;h4&gt;方法&lt;/h4&gt;框架包含四个相互连接的模块：(1)感知模块用于多模态传感器处理，(2)记忆模块用于持久化的时空经验存储，(3)时空推理模块用于综合过去和现在的上下文，(4)执行器模块用于有效的AR通信。同时提供了实施路线图、未来评估策略、潜在目标应用和使用案例。&lt;h4&gt;主要发现&lt;/h4&gt;当前AR系统在处理复杂多步骤场景时存在局限性，主要源于无法有效利用用户的历史交互数据。记忆增强型框架可以解决这些局限性，提供更智能的AR体验。&lt;h4&gt;结论&lt;/h4&gt;这项工作旨在激励未来研究，开发更智能的AR系统，这些系统能够有效地将用户的交互历史与自适应的、上下文感知的任务辅助联系起来。&lt;h4&gt;翻译&lt;/h4&gt;增强现实(AR)系统越来越多地集成基础模型，如多模态大语言模型(MLLMs)，以提供更上下文感知和自适应的用户体验。这种集成促使了AR代理的发展，以在真实环境中支持智能的、目标导向的交互。虽然当前的AR代理能有效支持即时任务，但在处理需要理解和利用用户长期经验和偏好的复杂多步骤场景时存在困难。这种局限性源于它们无法在时空上下文中捕获、保留和推理历史用户交互。为解决这些挑战，我们提出了一个记忆增强型AR代理的概念框架，该框架能够通过随时间学习和适应用户特定经验来提供个性化的任务辅助。我们的框架由四个相互连接的模块组成：(1)多模态传感器处理的感知模块，(2)持久化时空经验存储的记忆模块，(3)综合过去和现在上下文的时空推理模块，以及(4)有效AR通信的执行器模块。我们还提出了实施路线图、未来评估策略、潜在目标应用和使用案例，以展示我们框架在不同领域的实际适用性。我们希望这项工作能够激励未来研究，开发更智能的AR系统，能够有效地将用户的交互历史与自适应的、上下文感知的任务辅助联系起来。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是当前AR系统无法处理需要理解和利用用户长期经验和偏好的复杂多步骤任务。这个问题很重要，因为随着AR系统越来越多地集成基础模型，它们应该能够提供真正个性化的辅助，而不是仅支持即时任务。这种局限性阻碍了AR系统在需要个性化经验的场景中的应用，如重现用户的烹饪习惯或基于用户偏好组织物品。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了当前AR代理在处理复杂任务时的局限性，然后提出了一个记忆增强型AR代理的概念框架。他们借鉴了多模态场景图生成技术，使用场景图作为统一表示；参考了记忆增强型代理的研究，特别是在具身代理和Web代理中的应用；利用了基础模型在常识推理和多模态理解方面的能力；并整合了现有AR辅助系统的研究成果。作者通过模块化设计，将框架分为感知、记忆、时空推理和执行器四个相互连接的模块。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过存储和检索用户的长期经验，使AR系统能够提供个性化的任务辅助，并利用场景图作为统一表示来整合多模态感知信息。整体流程分为两个阶段：记录阶段，用户使用AR眼镜记录日常活动，这些记录被转换为结构化记忆并存储；回忆阶段，感知模块处理多模态输入生成场景图，记忆模块组织存储的记忆，时空推理模块进行任务意图推断、进度跟踪和行动规划，最后执行器模块通过适当方式提供个性化指导。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出记忆增强型AR代理的概念框架；设计四个相互连接的模块架构；采用场景图作为统一表示；强调时空推理能力；设计记录和回忆两阶段交互模式。相比之前的工作，该框架专注于长期用户经验而非短期交互；提供基于用户工作流程的个性化辅助而非通用指导；支持复杂多步骤任务；存储程序格式的经验而非简单语言匹配；能在嘈杂环境下通过上下文对齐解决不确定性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出并论证了一个记忆增强型AR代理的概念框架，通过整合用户的长期时空经验，实现了真正个性化的任务辅助，突破了当前AR系统在复杂多步骤场景中的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Augmented Reality (AR) systems are increasingly integrating foundationmodels, such as Multimodal Large Language Models (MLLMs), to provide morecontext-aware and adaptive user experiences. This integration has led to thedevelopment of AR agents to support intelligent, goal-directed interactions inreal-world environments. While current AR agents effectively support immediatetasks, they struggle with complex multi-step scenarios that requireunderstanding and leveraging user's long-term experiences and preferences. Thislimitation stems from their inability to capture, retain, and reason overhistorical user interactions in spatiotemporal contexts. To address thesechallenges, we propose a conceptual framework for memory-augmented AR agentsthat can provide personalized task assistance by learning from and adapting touser-specific experiences over time. Our framework consists of fourinterconnected modules: (1) Perception Module for multimodal sensor processing,(2) Memory Module for persistent spatiotemporal experience storage, (3)Spatiotemporal Reasoning Module for synthesizing past and present contexts, and(4) Actuator Module for effective AR communication. We further present animplementation roadmap, a future evaluation strategy, a potential targetapplication and use cases to demonstrate the practical applicability of ourframework across diverse domains. We aim for this work to motivate futureresearch toward developing more intelligent AR systems that can effectivelybridge user's interaction history with adaptive, context-aware task assistance.</description>
      <author>example@mail.com (Dongwook Choi, Taeyoon Kwon, Dongil Yang, Hyojun Kim, Jinyoung Yeo)</author>
      <guid isPermaLink="false">2508.08774v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Aryabhata: An exam-focused language model for JEE Math</title>
      <link>http://arxiv.org/abs/2508.08665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了Aryabhata 1.0，一个针对印度JEE考试优化的7B参数数学推理模型，通过合并开源模型、监督微调和强化学习等方法，在多项基准测试中表现出色，并作为基础模型开源发布。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型发展迅速，但现有模型通常不适合教育用途，特别是在应对特定学术考试方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;创建一个专门针对印度学术考试联合入学考试(JEE)优化的数学推理模型，提供教育友好的逐步推理，并作为基础模型开源以促进相关研究。&lt;h4&gt;方法&lt;/h4&gt;通过合并强大的开源权重推理模型，然后使用课程学习进行监督微调，基于验证链式思维轨迹进行训练；进一步应用带有可验证奖励的强化学习，使用A2C目标和组相对优势估计，结合自适应组调整和温度缩放等探索策略进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;Aryabhata 1.0在JEE Main 2025(分布内)和MATH、GSM8K(分布外)等基准测试中，在准确性和效率方面均优于现有模型，并能提供教育有用的逐步推理过程。&lt;h4&gt;结论&lt;/h4&gt;Aryabhata 1.0作为以考试为中心的开源小型语言模型基础模型发布，标志着向教育领域适用的大型语言模型迈出了重要一步，研究团队将持续改进模型以提升学生学习成果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Aryabhata 1.0，这是一个紧凑的7B参数数学推理模型，专门为印度学术考试联合入学考试(JEE)优化。尽管大型语言模型进展迅速，但当前模型通常仍不适合教育用途。Aryabhata 1.0通过合并强大的开源权重推理模型构建，然后使用课程学习对通过最佳n拒绝采样筛选的验证链式思维轨迹进行监督微调(SFT)。为进一步提升性能，我们应用了带有可验证奖励的强化学习(RLVR)，使用A2C目标和组相对优势估计，以及自适应组调整和温度缩放等新的探索策略。在分布内(JEE Main 2025)和分布外(MATH, GSM8K)基准测试中评估，Aryabhata在准确性和效率上都优于现有模型，同时提供教育有用的逐步推理。我们将Aryabhata作为基础模型发布，以推进以考试为中心的开源小型语言模型。这是我们首次公开发布以获取社区反馈；PW正在积极训练未来的模型以进一步改善学生的学习成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present $\textbf{Aryabhata 1.0}$, a compact 7B parameter math reasoningmodel optimized for the Indian academic exam, the Joint Entrance Examination(JEE). Despite rapid progress in large language models (LLMs), current modelsoften remain unsuitable for educational use. Aryabhata 1.0 is built by mergingstrong open-weight reasoning models, followed by supervised fine-tuning (SFT)with curriculum learning on verified chain-of-thought (CoT) traces curatedthrough best-of-$n$ rejection sampling. To further boost performance, we applyreinforcement learning with verifiable rewards (RLVR) using A2C objective withgroup-relative advantage estimation alongwith novel exploration strategies suchas $\textit{Adaptive Group Resizing}$ and $\textit{Temperature Scaling}$.Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution(MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy andefficiency, while offering pedagogically useful step-by-step reasoning. Werelease Aryabhata as a foundation model to advance exam-centric, open-sourcesmall language models. This marks our first open release for community feedback($\href{https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0}{Aryabhata\ 1.0\on\ Hugging\ Face}$); PW is actively training future models to further improvelearning outcomes for students.</description>
      <author>example@mail.com (Ritvik Rastogi, Sachin Dharashivkar, Sandeep Varma)</author>
      <guid isPermaLink="false">2508.08665v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>DeepFleet: Multi-Agent Foundation Models for Mobile Robots</title>
      <link>http://arxiv.org/abs/2508.08574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 10 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepFleet是一套支持大规模移动机器人舰队协调和规划的基础模型套件，包含四种不同架构，基于全球亚马逊仓库中数十万台机器人的数据训练。&lt;h4&gt;背景&lt;/h4&gt;全球亚马逊仓库中有数十万台机器人，需要有效的协调和规划系统。&lt;h4&gt;目的&lt;/h4&gt;设计支持大规模移动机器人舰队协调和规划的基础模型。&lt;h4&gt;方法&lt;/h4&gt;DeepFleet包含四种架构：机器人中心模型(自回归决策变换器)、机器人地面模型(交叉注意力变换器)、图像地面模型(卷积编码多通道图像)和图形地面模型(时间注意力与图神经网络结合)。&lt;h4&gt;主要发现&lt;/h4&gt;机器人和图形地面模型使用异步状态更新和局部交互结构，显示出最大潜力；当模型规模扩大时，能有效利用更大的仓库运营数据集。&lt;h4&gt;结论&lt;/h4&gt;不同设计选择对预测任务性能有不同影响，机器人和图形地面模型表现最佳。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了DeepFleet，一套旨在支持大规模移动机器人舰队协调和规划的基础模型套件。这些模型基于全球亚马逊仓库中数十万台机器人的舰队运动数据进行训练，包括机器人位置、目标和交互。DeepFleet包含四种架构，每种体现了不同的归纳偏置，共同探索多智能体基础模型设计空间的关键点：机器人中心模型是在单个机器人邻域上运行的自回归决策变换器；机器人地面模型使用机器人和仓库地面之间的交叉注意力的变换器；图像地面模型将卷积编码应用于整个舰队的多通道图像表示；图形地面模型将时间注意力与图神经网络结合，用于空间关系。在本文中，我们描述了这些模型并展示了我们对这些设计选择对预测任务性能影响的评估。我们发现，使用异步机器人状态更新并融入机器人交互局部结构的机器人和图形地面模型显示出最大的潜力。我们还展示了实验，表明当模型规模扩大时，这两种模型能够有效地利用更大的仓库运营数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce DeepFleet, a suite of foundation models designed to supportcoordination and planning for large-scale mobile robot fleets. These models aretrained on fleet movement data, including robot positions, goals, andinteractions, from hundreds of thousands of robots in Amazon warehousesworldwide. DeepFleet consists of four architectures that each embody a distinctinductive bias and collectively explore key points in the design space formulti-agent foundation models: the robot-centric (RC) model is anautoregressive decision transformer operating on neighborhoods of individualrobots; the robot-floor (RF) model uses a transformer with cross-attentionbetween robots and the warehouse floor; the image-floor (IF) model appliesconvolutional encoding to a multi-channel image representation of the fullfleet; and the graph-floor (GF) model combines temporal attention with graphneural networks for spatial relationships. In this paper, we describe thesemodels and present our evaluation of the impact of these design choices onprediction task performance. We find that the robot-centric and graph-floormodels, which both use asynchronous robot state updates and incorporate thelocalized structure of robot interactions, show the most promise. We alsopresent experiments that show that these two models can make effective use oflarger warehouses operation datasets as the models are scaled up.</description>
      <author>example@mail.com (Ameya Agaskar, Sriram Siva, William Pickering, Kyle O'Brien, Charles Kekeh, Ang Li, Brianna Gallo Sarker, Alicia Chua, Mayur Nemade, Charun Thattai, Jiaming Di, Isaac Iyengar, Ramya Dharoor, Dino Kirouani, Jimmy Erskine, Tamir Hegazy, Scott Niekum, Usman A. Khan, Federico Pecora, Joseph W. Durham)</author>
      <guid isPermaLink="false">2508.08574v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Chi-Geometry: A Library for Benchmarking Chirality Prediction of GNNs</title>
      <link>http://arxiv.org/abs/2508.09097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages total: 9 pages main text, 4 pages references, 8 pages  appendices. 4 figures and 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Chi-Geometry是一个用于生成图数据的库，专门用于测试和基准测试图神经网络(GNN)预测手性的能力。它可以生成具有特定几何和拓扑特征的合成图样本，每个图包含一个标记为R或S的手性中心，其他节点标记为N/A。这些样本组合成数据集，用于评估GNN作为节点分类任务预测手性的能力。&lt;h4&gt;背景&lt;/h4&gt;手性预测在化学和材料科学中是一个重要问题，图神经网络被用于预测分子结构中的手性中心。然而，缺乏专门用于测试和基准测试GNN手性预测能力的工具和数据集。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成具有特定几何和拓扑特征的合成图样本的库，用于更可解释和减少混淆因素的GNN手性预测基准测试，从而指导设计具有更好预测性能的新型GNN架构。&lt;h4&gt;方法&lt;/h4&gt;Chi-Geometry库生成合成图样本，具有用户指定的几何和拓扑特征以隔离特定类型的样本，以及随机化的节点位置和种类以最小化外部相关性。每个生成的图包含一个标记为R或S的手性中心，其他节点标记为N/A。&lt;h4&gt;主要发现&lt;/h4&gt;使用Chi-Geometry生成的数据集有效基准测试了各种最先进的GNN架构。基于结果设计了两种新型GNN架构：第一种建立全连接，能准确预测所有具有挑战性的配置中的手性，但计算成本随节点数量二次增长；第二种通过引入虚拟节点避免全连接，恢复了计算成本的线性缩放，同时保持相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;Chi-Geometry库提供了更可解释和减少混淆因素的GNN手性预测基准测试，能够指导设计新型GNN架构。通过引入虚拟节点的优化方法，可以在保持准确性的同时显著降低计算成本。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了Chi-Geometry一个用于生成图数据以测试和基准测试GNN预测手性能力的库。Chi-Geometry生成具有以下特征的合成图样本：(i)用户指定的几何和拓扑特征，以隔离特定类型的样本，以及(ii)随机化的节点位置和种类，以最小化外部相关性。每个生成的图包含一个标记为R或S的手性中心，而所有其他节点标记为N/A（非手性）。然后将生成的样本组合成一个连贯的数据集，可用于评估GNN预测手性的能力，作为节点分类任务。Chi-Geometry使得对图样本中手性预测的GNN进行更可解释和减少混淆因素的基准测试，可以指导设计具有更好预测性能的新型GNN架构。我们通过使用Chi-Geometry生成合成数据集来基准测试各种最先进的GNN架构，说明了Chi-Geometry的有效性。这些基准测试的结果指导了我们设计两种新型GNN架构。第一种GNN架构在图中建立全连接，能够准确预测先前测试的最先进模型失败的所有具有挑战性的配置中的手性，但训练和推理的计算成本随图节点数量二次增长。第二种GNN架构通过在原始图结构中引入虚拟节点避免全连接，这恢复了训练和推理计算成本随图节点数量的线性缩放，同时仍确保与最先进的GNN架构相比在检测手性方面具有竞争性的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Chi-Geometry - a library that generates graph data for testingand benchmarking GNNs' ability to predict chirality. Chi-Geometry generatessynthetic graph samples with (i) user-specified geometric and topologicaltraits to isolate certain types of samples and (ii) randomized node positionsand species to minimize extraneous correlations. Each generated graph containsexactly one chiral center labeled either R or S, while all other nodes arelabeled N/A (non-chiral). The generated samples are then combined into acohesive dataset that can be used to assess a GNN's ability to predictchirality as a node classification task. Chi-Geometry allows more interpretableand less confounding benchmarking of GNNs for prediction of chirality in thegraph samples which can guide the design of new GNN architectures with improvedpredictive performance. We illustrate Chi-Geometry's efficacy by using it togenerate synthetic datasets for benchmarking various state-of-the-art (SOTA)GNN architectures. The conclusions of these benchmarking results guided ourdesign of two new GNN architectures. The first GNN architecture establishedall-to-all connections in the graph to accurately predict chirality across allchallenging configurations where previously tested SOTA models failed, but at acomputational cost (both for training and inference) that grows quadraticallywith the number of graph nodes. The second GNN architecture avoids all-to-allconnections by introducing a virtual node in the original graph structure ofthe data, which restores the linear scaling of training and inferencecomputational cost with respect to the number of nodes in the graph, whilestill ensuring competitive accuracy in detecting chirality with respect to SOTAGNN architectures.</description>
      <author>example@mail.com (Rylie Weaver, Massamiliano Lupo Pasini)</author>
      <guid isPermaLink="false">2508.09097v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Meta-learning optimizes predictions of missing links in real-world networks</title>
      <link>http://arxiv.org/abs/2508.09069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 5 tables, 7 appendices&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性地比较了多种算法在预测关系网络中缺失链接方面的性能，发现没有一种算法在所有类型网络上都表现最佳，但模型堆叠方法具有良好的可扩展性和竞争力。作者还提出了一种元学习算法，可根据网络特性选择最优算法，显著提高了预测性能。&lt;h4&gt;背景&lt;/h4&gt;关系数据在现实世界应用中无处不在，如社交网络分析和生物建模，但观测到的网络数据往往不完整。预测缺失链接是网络分析中的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;确定哪种算法最适合预测缺失链接，以及最佳算法选择是否依赖于输入网络的特性。&lt;h4&gt;方法&lt;/h4&gt;使用包含550个真实世界网络的大型多样化基准测试集，在AUC和Top-k两个准确度指标下，比较四种堆叠算法、42种拓扑链接预测器（包括两种新算法）和两种图神经网络算法。&lt;h4&gt;主要发现&lt;/h4&gt;没有一种算法在所有输入网络上都表现最佳；所有算法在大多数社交网络上表现良好，但只有少数算法在经济和生物网络上表现良好；使用随机森林的模型堆叠方法既高度可扩展，在AUC上超越图神经网络，或在Top-k准确度上与图神经网络具有竞争力；算法性能强烈依赖于网络特性如度分布、三角形密度和度同配性。&lt;h4&gt;结论&lt;/h4&gt;作者引入了一种元学习算法，通过利用网络特性间的变异性，为每个网络选择最佳算法来优化链接预测，该方法优于所有最先进算法并可扩展到大型网络。&lt;h4&gt;翻译&lt;/h4&gt;关系数据在现实世界数据应用中无处不在，例如在社交网络分析或生物建模中，但网络几乎总是不完整观测的。在没有节点属性的网络这种困难情况下，预测缺失链接的最先进技术使用模型堆叠或神经网络技术。目前尚不清楚哪种方法最佳，以及最佳算法选择是否或如何依赖于输入网络的特性。我们使用一个大型、结构多样化的550个真实世界网络基准测试集，在两个标准准确度指标（AUC和Top-k）下，系统地回答了这些问题，比较了四种堆叠算法与42种顶级拓扑链接预测器（其中两种是我们在此引入的），以及两种图神经网络算法。我们表明没有一种算法在所有输入网络上都是最好的，所有算法在大多数社交网络上表现良好，只有少数算法在经济和生物网络上表现良好。总体而言，使用随机森林的模型堆叠方法既高度可扩展，在AUC上超越图神经网络，或在Top-k准确度上与图神经网络具有竞争力。但是，算法性能强烈依赖于网络特性如度分布、三角形密度和度同配性。我们引入了一种元学习算法，利用这种变异性通过选择最佳算法来优化单个网络的链接预测，我们证明该方法优于所有最先进的算法并可扩展到大型网络。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relational data are ubiquitous in real-world data applications, e.g., insocial network analysis or biological modeling, but networks are nearly alwaysincompletely observed. The state-of-the-art for predicting missing links in thehard case of a network without node attributes uses model stacking or neuralnetwork techniques. It remains unknown which approach is best, and whether orhow the best choice of algorithm depends on the input network'scharacteristics. We answer these questions systematically using a large,structurally diverse benchmark of 550 real-world networks under two standardaccuracy measures (AUC and Top-k), comparing four stacking algorithms with 42topological link predictors, two of which we introduce here, and two graphneural network algorithms. We show that no algorithm is best across all inputnetworks, all algorithms perform well on most social networks, and few performwell on economic and biological networks. Overall, model stacking with a randomforest is both highly scalable and surpasses on AUC or is competitive withgraph neural networks on Top-k accuracy. But, algorithm performance dependsstrongly on network characteristics like the degree distribution, triangledensity, and degree assortativity. We introduce a meta-learning algorithm thatexploits this variability to optimize link predictions for individual networksby selecting the best algorithm to apply, which we show outperforms allstate-of-the-art algorithms and scales to large networks.</description>
      <author>example@mail.com (Bisman Singh, Lucy Van Kleunen, Aaron Clauset)</author>
      <guid isPermaLink="false">2508.09069v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs</title>
      <link>http://arxiv.org/abs/2508.08769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 5 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DiFac的差异化因子一致性半监督框架，用于增强图神经网络在半监督学习中的性能，通过从单一信息源派生差异化因子并强制它们保持一致性，减轻伪标签确认偏差和训练崩溃问题。&lt;h4&gt;背景&lt;/h4&gt;在半监督学习中，使用互不独立的决策因素进行交叉验证被认为是减轻伪标签确认偏差和训练崩溃的有效策略，但在实践中获取这样的因素具有挑战性，因为额外的有效信息源本质上稀缺，且无法保证与原始信息源的独立性。&lt;h4&gt;目的&lt;/h4&gt;解决半监督学习中获取互不独立决策因素的挑战，提高图神经网络在低标签情况下的鲁棒性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出DiFac框架，在预训练阶段学习提取差异化因子，在训练阶段迭代移除具有冲突因子的样本，并根据最短杆原理对伪标签排序选择顶级候选样本。同时利用大型多模态语言模型引入潜在文本知识作为辅助决策因素，并设计问责评分机制减轻辅助因素引入的错误判断。&lt;h4&gt;主要发现&lt;/h4&gt;DiFac框架在多个基准数据集上实验表明，在低标签情况下持续提高了鲁棒性和泛化能力，优于其他基线方法。&lt;h4&gt;结论&lt;/h4&gt;DiFac框架通过从单一信息源派生差异化因子并强制它们保持一致性，有效解决了半监督学习中获取互不独立决策因素的挑战，提高了图神经网络在低标签情况下的性能。&lt;h4&gt;翻译&lt;/h4&gt;在利用未标记数据增强图神经网络性能的半监督学习中，为交叉验证引入相互独立的决策因素被认为是减轻伪标签确认偏差和训练崩溃的有效策略。然而，在实践中获得这样的因素具有挑战性：额外的有效信息源本质上稀缺，即使有这样的信息源，它们也无法保证与原始信息源的独立性。为应对这一挑战，本文提出了一种差异化因子一致性半监督框架（DiFac），该框架从单一信息源派生出差异化因子并强制它们保持一致性。在预训练阶段，模型学习提取这些因子；在训练阶段，它迭代地移除具有冲突因子的样本，并根据最短杆原理对伪标签进行排序，选择顶级候选样本以减少基于置信度或集成方法中常见的过度自信。我们的框架还可以整合额外的信息源。在这项工作中，我们利用大型多模态语言模型引入潜在文本知识作为辅助决策因素，并设计了一种问责评分机制来减轻这些辅助因素引入的额外错误判断。在多个基准数据集上的实验表明，DiFac在低标签情况下持续提高了鲁棒性和泛化能力，优于其他基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In semi-supervised learning (SSL) for enhancing the performance of graphneural networks (GNNs) with unlabeled data, introducing mutually independentdecision factors for cross-validation is regarded as an effective strategy toalleviate pseudo-label confirmation bias and training collapse. However,obtaining such factors is challenging in practice: additional and validinformation sources are inherently scarce, and even when such sources areavailable, their independence from the original source cannot be guaranteed. Toaddress this challenge, In this paper we propose a Differentiated FactorConsistency Semi-supervised Framework (DiFac), which derives differentiatedfactors from a single information source and enforces their consistency. Duringpre-training, the model learns to extract these factors; in training, ititeratively removes samples with conflicting factors and ranks pseudo-labelsbased on the shortest stave principle, selecting the top candidate samples toreduce overconfidence commonly observed in confidence-based or ensemble-basedmethods. Our framework can also incorporate additional information sources. Inthis work, we leverage the large multimodal language model to introduce latenttextual knowledge as auxiliary decision factors, and we design a accountabilityscoring mechanism to mitigate additional erroneous judgments introduced bythese auxiliary factors. Experiments on multiple benchmark datasets demonstratethat DiFac consistently improves robustness and generalization in low-labelregimes, outperforming other baseline methods.</description>
      <author>example@mail.com (Long Wang, Kai Liu)</author>
      <guid isPermaLink="false">2508.08769v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor Search</title>
      <link>http://arxiv.org/abs/2508.08744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at SIGMOD 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了Tagore，一个GPU加速的图索引库，用于高效处理高维向量空间中的近似最近邻搜索问题。该库能够构建基于细化的图索引，并通过多种技术创新实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;高维向量空间中的近似最近邻搜索(ANNS)在现实世界中有广泛应用。图索引方法因其高精度和效率而受到关注，但其索引开销仍然很大。随着数据量的指数级增长和动态索引调整需求的增加，这一问题变得更加严重。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的GPU加速图索引库，以解决图索引方法的开销问题，特别是在处理大规模数据集时。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了GNN-Descent算法，一种GPU特定的算法，用于高效的k-近邻(k-NN)图初始化；2. 提出了CFS通用计算过程，支持各种k-NN图剪枝策略；3. 设计了两个广义GPU内核，用于并行处理邻居关系中的复杂依赖；4. 提出了异步GPU-CPU-磁盘索引框架，具有集群感知的缓存机制，以最小化磁盘I/O压力。&lt;h4&gt;主要发现&lt;/h4&gt;在7个真实世界数据集上的实验表明，Tagore在保持索引质量的同时实现了1.32x-112.79x的加速比。&lt;h4&gt;结论&lt;/h4&gt;Tagore通过GPU加速和多种技术创新，有效解决了图索引方法的开销问题，特别是在处理大规模数据集时，为高维向量空间中的近似最近邻搜索提供了高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高维向量空间中的近似最近邻搜索(ANNS)在现实世界中有广泛应用。虽然已有许多方法被提出以有效处理ANNS，但基于图的索引因其高精度和效率而日益突出。然而，基于图的索引的开销仍然很大。随着数据量的指数级增长和动态索引调整需求的增加，这一开销持续攀升，构成了一个关键挑战。在本文中，我们介绍了Tagore，一个由GPU加速的快速图索引库，它具有构建基于细化的图索引（如NSG和Vamana）的强大功能。我们首先介绍了GNN-Descent，一种用于高效k-近邻(k-NN)图初始化的GPU特定算法。GNN-Descent通过两阶段下降过程加速相似性比较，并实现高度并行的邻居更新。接下来，为了支持各种k-NN图剪枝策略，我们提出了一个称为CFS的通用计算过程，并设计了两个广义GPU内核，用于并行处理邻居关系中的复杂依赖。对于超过GPU内存容量的大规模数据集，我们提出了一个具有集群感知缓存机制的异步GPU-CPU-磁盘索引框架，以最小化磁盘的I/O压力。在7个真实世界数据集上的广泛实验表明，Tagore在保持索引质量的同时实现了1.32x-112.79x的加速比。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Approximate nearest neighbor search (ANNS) in high-dimensional vector spaceshas a wide range of real-world applications. Numerous methods have beenproposed to handle ANNS efficiently, while graph-based indexes have gainedprominence due to their high accuracy and efficiency. However, the indexingoverhead of graph-based indexes remains substantial. With exponential growth indata volume and increasing demands for dynamic index adjustments, this overheadcontinues to escalate, posing a critical challenge. In this paper, we introduceTagore, a fast library accelerated by GPUs for graph indexing, which haspowerful capabilities of constructing refinement-based graph indexes such asNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm forefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds upthe similarity comparison by a two-phase descent procedure and enables highlyparallelized neighbor updates. Next, aiming to support various k-NN graphpruning strategies, we formulate a universal computing procedure termed CFS anddevise two generalized GPU kernels for parallel processing complex dependenciesin neighbor relationships. For large-scale datasets exceeding GPU memorycapacity, we propose an asynchronous GPU-CPU-disk indexing framework with acluster-aware caching mechanism to minimize the I/O pressure on the disk.Extensive experiments on 7 real-world datasets exhibit that Tagore achieves1.32x-112.79x speedup while maintaining the index quality.</description>
      <author>example@mail.com (Zhonggen Li, Xiangyu Ke, Yifan Zhu, Bocheng Yu, Baihua Zheng, Yunjun Gao)</author>
      <guid isPermaLink="false">2508.08744v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Node-Destroyer Model with Large Neighborhood Search for Solving the Capacitated Vehicle Routing Problem</title>
      <link>http://arxiv.org/abs/2508.08659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种迭代学习混合优化求解器，用于增强元启发式算法在解决带容量车辆路径问题中的性能，通过结合机器学习模型和图神经网络来指导搜索过程，显著提高了算法效率和解决方案质量。&lt;h4&gt;背景&lt;/h4&gt;元启发式算法在解决带容量车辆路径问题时面临性能挑战，需要更有效的优化方法来处理大规模实例。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合优化求解器，通过迭代学习机制和机器学习方法增强元启发式算法在CVRP问题上的性能，同时保持算法的可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种迭代混合机制，集成了节点销毁模型(Node-Destroyer Model)，该模型利用图神经网络(GNNs)识别和选择客户节点，以指导大型邻域搜索(LNS)算子在元启发式框架中的操作，利用问题解决方案的图结构属性来指导节点移除的战略选择。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的混合机制能够改进基线元启发式算法的性能，不仅提高了标准CVRP基准的解决方案质量，而且在处理多达30,000个客户节点的超大实例上也证明了其可扩展性，无需针对不同大小的问题实例重新训练模型。&lt;h4&gt;结论&lt;/h4&gt;该混合优化求解器为解决带容量车辆路径问题提供了一种有效的方法，通过结合机器学习和元启发式算法的优势，实现了更高质量的解决方案和更好的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，我们提出了一种迭代学习混合优化求解器，旨在增强元启发式算法在解决带容量车辆路径问题(CVRP)中的性能。迭代混合机制集成了所提出的节点销毁模型(Node-Destroyer Model)，这是一个利用图神经网络(GNNs)的机器学习混合模型，用于识别和选择客户节点，以指导元启发式优化框架中的大型邻域搜索(LNS)算子。该模型利用了可以表示为图的问题和解决方案的结构属性，来指导关于节点移除的战略选择。所提出的方法降低了操作复杂性，并缩小了优化过程中涉及的搜索空间。该混合方法专门应用于CVRP，不需要针对不同大小的不同问题实例重新训练。所提出的混合机制能够改进基线元启发式算法的性能。我们的方法不仅提高了标准CVRP基准的解决方案质量，而且在处理多达30,000个客户节点的非常大规模的实例上也证明了其可扩展性。在基准数据集上的实验评估表明，所提出的混合机制能够改进不同的基线算法，在相似设置下获得更高质量的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this research, we propose an iterative learning hybrid optimization solverdeveloped to strengthen the performance of metaheuristic algorithms in solvingthe Capacitated Vehicle Routing Problem (CVRP). The iterative hybrid mechanismintegrates the proposed Node-Destroyer Model, a machine learning hybrid modelthat utilized Graph Neural Networks (GNNs) such identifies and selects customernodes to guide the Large Neighborhood Search (LNS) operator within themetaheuristic optimization frameworks. This model leverages the structuralproperties of the problem and solution that can be represented as a graph, toguide strategic selections concerning node removal. The proposed approachreduces operational complexity and scales down the search space involved in theoptimization process. The hybrid approach is applied specifically to the CVRPand does not require retraining across problem instances of different sizes.The proposed hybrid mechanism is able to improve the performance of baselinemetaheuristic algorithms. Our approach not only enhances the solution qualityfor standard CVRP benchmarks but also proves scalability on very large-scaleinstances with up to 30,000 customer nodes. Experimental evaluations onbenchmark datasets show that the proposed hybrid mechanism is capable ofimproving different baseline algorithms, achieving better quality of solutionsunder similar settings.</description>
      <author>example@mail.com (Bachtiar Herdianto, Romain Billot, Flavien Lucas, Marc Sevaux, Daniele Vigo)</author>
      <guid isPermaLink="false">2508.08659v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence: A Survey</title>
      <link>http://arxiv.org/abs/2508.08620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了图神经网络(GNNs)在无线通信和网络中的应用，并提出采用智能体人工智能(AI)来组织和整合GNNs，实现面向边缘通用智能的场景和任务感知实现。&lt;h4&gt;背景&lt;/h4&gt;通信技术的快速发展推动通信网络向高维资源利用和多功能集成方向发展，这种复杂性为设计满足移动应用服务质量(QoS)和时间敏感性需求的通信网络带来挑战。&lt;h4&gt;目的&lt;/h4&gt;全面了解GNNs的能力，通过智能体AI组织和整合GNNs，实现场景和任务感知的实现；回顾GNNs在无线通信和网络中的最新应用，特别关注图表示与网络拓扑之间以及神经网络架构与无线任务之间的对齐。&lt;h4&gt;方法&lt;/h4&gt;基于突出的神经网络架构概述GNNs，介绍智能体GNNs概念；总结和比较GNN在传统系统和新兴技术(物理层、MAC层、网络层设计、ISAC、RIS、无小区网络架构)中的应用；提出基于大型语言模型(LLM)的智能问答代理框架，利用调查作为本地知识库。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs已成为复杂通信网络的基本深度学习模型，增强网络拓扑特征提取，提高可扩展性并促进分布式计算；但大多数现有GNNs遵循传统被动学习框架，可能无法满足多样化无线系统需求；智能体AI组织和整合GNNs可实现场景和任务感知实现。&lt;h4&gt;结论&lt;/h4&gt;通过全面回顾GNNs在无线通信和网络中的应用，提出基于大型语言模型的智能问答代理框架，利用调查作为知识库，实现针对无线通信研究定制的GNN相关响应。&lt;h4&gt;翻译&lt;/h4&gt;通信技术的快速发展推动通信网络向高维资源利用和多功能集成方向发展。这种日益增长的复杂性为设计通信网络以满足移动应用在动态环境中对服务质量(QoS)和时间敏感性的不断增长需求带来了重大挑战。图神经网络(GNNs)已成为复杂通信网络的基本深度学习(DL)模型。GNNs不仅增强了网络拓扑特征提取，还提高了可扩展性并促进分布式计算。然而，大多数现有的GNNs遵循传统的被动学习框架，可能无法满足日益多样化的无线系统的需求。该调查提出采用智能体人工智能(AI)来组织和整合GNNs，实现面向边缘通用智能的场景和任务感知实现。为了全面了解GNNs的能力，作者全面回顾了GNNs在无线通信和网络中的最新应用。特别是，作者关注图表示与网络拓扑之间以及神经网络架构与无线任务之间的对齐。首先，作者基于突出的神经网络架构概述了GNNs，然后介绍了智能体GNNs的概念。接着，总结了和比较了GNN在传统系统和新兴技术中的应用，包括物理层、MAC层和网络层设计、集成传感和通信(ISAC)、可重构智能表面(RIS)和无小区网络架构。此外，作者还提出了一个大型语言模型(LLM)框架作为智能问答代理，利用本调查作为本地知识库，实现针对无线通信研究定制的GNN相关响应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of communication technologies has driven the evolutionof communication networks towards both high-dimensional resource utilizationand multifunctional integration. This evolving complexity poses significantchallenges in designing communication networks to satisfy the growingquality-of-service and time sensitivity of mobile applications in dynamicenvironments. Graph neural networks (GNNs) have emerged as fundamental deeplearning (DL) models for complex communication networks. GNNs not only augmentthe extraction of features over network topologies but also enhance scalabilityand facilitate distributed computation. However, most existing GNNs follow atraditional passive learning framework, which may fail to meet the needs ofincreasingly diverse wireless systems. This survey proposes the employment ofagentic artificial intelligence (AI) to organize and integrate GNNs, enablingscenario- and task-aware implementation towards edge general intelligence. Tocomprehend the full capability of GNNs, we holistically review recentapplications of GNNs in wireless communications and networking. Specifically,we focus on the alignment between graph representations and network topologies,and between neural architectures and wireless tasks. We first provide anoverview of GNNs based on prominent neural architectures, followed by theconcept of agentic GNNs. Then, we summarize and compare GNN applications forconventional systems and emerging technologies, including physical, MAC, andnetwork layer designs, integrated sensing and communication (ISAC),reconfigurable intelligent surface (RIS) and cell-free network architecture. Wefurther propose a large language model (LLM) framework as an intelligentquestion-answering agent, leveraging this survey as a local knowledge base toenable GNN-related responses tailored to wireless communication research.</description>
      <author>example@mail.com (Yang Lu, Shengli Zhang, Chang Liu, Ruichen Zhang, Bo Ai, Dusit Niyato, Wei Ni, Xianbin Wang, Abbas Jamalipour)</author>
      <guid isPermaLink="false">2508.08620v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Quantum Mechanics to Organic Liquid Properties via a Universal Force Field</title>
      <link>http://arxiv.org/abs/2508.08575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ByteFF-Pol，一种基于图神经网络参数化的可极化力场，能够准确预测宏观性质，解决了分子动力学模拟中计算成本与准确性之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;分子动力学模拟是理解凝聚相系统结构和动力学的关键工具，但从量子力学计算准确预测宏观属性仍面临挑战，常受计算成本与模拟精度之间的权衡限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够基于高阶量子力学数据训练的力场，实现对宏观性质的准确预测，同时保持合理的计算成本。&lt;h4&gt;方法&lt;/h4&gt;提出ByteFF-Pol，一种基于图神经网络(GNN)参数化的可极化力场，完全基于高阶量子力学(QM)数据进行训练，采用物理动机的力场形式和训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;ByteFF-Pol在预测多种小分子液体和电解质的宏观热力学和输运性质方面表现出色，性能优于最先进的经典和机器学习力场。其零样本预测能力成功弥合了微观量子力学计算与宏观液体性质之间的差距。&lt;h4&gt;结论&lt;/h4&gt;ByteFF-Pol的进步在电解质设计和定制溶剂等应用领域具有变革性潜力，代表了数据驱动材料发现的关键一步，使探索以前难以处理的化学空间成为可能。&lt;h4&gt;翻译&lt;/h4&gt;分子动力学(MD)模拟是揭示凝聚相系统结构和动力学的原子级见解的必要工具。然而，从头计算(ab initio)普遍准确地预测宏观属性仍然是一个重大挑战，通常受到计算成本与模拟精度之间权衡的阻碍。在此，我们提出ByteFF-Pol，一种基于图神经网络(GNN)参数化的可极化力场，完全基于高阶量子力学(QM)数据进行训练。利用物理动机的力场形式和训练策略，ByteFF-Pol在预测多种小分子液体和电解质的热力学和输运性质方面表现出色，性能优于最先进的(SOTA)经典和机器学习力场。ByteFF-Pol的零样本预测能力弥合了微观QM计算与宏观液体性质之间的差距，使探索以前难以处理的化学空间成为可能。这一进展在电解质设计和定制溶剂等应用方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics (MD) simulations are essential tools for unravelingatomistic insights into the structure and dynamics of condensed-phase systems.However, the universal and accurate prediction of macroscopic properties fromab initio calculations remains a significant challenge, often hindered by thetrade-off between computational cost and simulation accuracy. Here, we presentByteFF-Pol, a graph neural network (GNN)-parameterized polarizable force field,trained exclusively on high-level quantum mechanics (QM) data. Leveragingphysically-motivated force field forms and training strategies, ByteFF-Polexhibits exceptional performance in predicting thermodynamic and transportproperties for a wide range of small-molecule liquids and electrolytes,outperforming state-of-the-art (SOTA) classical and machine learning forcefields. The zero-shot prediction capability of ByteFF-Pol bridges the gapbetween microscopic QM calculations and macroscopic liquid properties, enablingthe exploration of previously intractable chemical spaces. This advancementholds transformative potential for applications such as electrolyte design andcustom-tailored solvent, representing a pivotal step toward data-drivenmaterials discovery.</description>
      <author>example@mail.com (Tianze Zheng, Xingyuan Xu, Zhi Wang, Xu Han, Zhenliang Mu, Ziqing Zhang, Sheng Gong, Kuang Yu, Wen Yan)</author>
      <guid isPermaLink="false">2508.08575v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>UQGNN: Uncertainty Quantification of Graph Neural Networks for Multivariate Spatiotemporal Prediction</title>
      <link>http://arxiv.org/abs/2508.08551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures, SIGSPATIAL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为UQGNN的新型图神经网络，用于多变量时空预测，能够同时预测期望均值和相关的不确定性，并在多个真实数据集上表现出优于现有基线的性能。&lt;h4&gt;背景&lt;/h4&gt;时空预测在城市规划、交通优化、灾害响应和疫情控制等众多实际应用中扮演着关键角色。现有大多数模型是确定性的，仅预测期望均值而不量化不确定性，导致结果可能不可靠。虽然已有概率模型引入不确定性量化，但通常只关注单一现象，忽略了异构城市现象间的内在相关性。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有研究的不足，作者提出了UQGNN模型，用于多变量时空预测，旨在同时捕获复杂时空交互模式并量化预测的不确定性。&lt;h4&gt;方法&lt;/h4&gt;UQGNN引入两个关键创新：(i) 交互感知时空嵌入模块，整合多变量扩散图卷积网络和交互感知时间卷积网络，以捕获复杂时空交互模式；(ii) 多变量概率预测模块，用于估计期望均值和相关的不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在深圳、纽约市和芝加哥四个真实世界多变量时空数据集上的实验表明，UQGNN在预测准确性和不确定性量化方面均优于最先进的基线。例如，在深圳数据集上，UQGNN在预测准确性和不确定性量化方面均实现了5%的提升。&lt;h4&gt;结论&lt;/h4&gt;UQGNN通过结合交互感知的时空嵌入和多变量概率预测，有效解决了多变量时空预测中的不确定性量化问题，并在多个真实数据集上证明了其优越性。&lt;h4&gt;翻译&lt;/h4&gt;时空预测在众多实际应用中扮演着关键角色，如城市规划、交通优化、灾害响应和疫情控制。近年来，研究人员通过开发先进的深度学习模型在时空预测方面取得了显著进展。然而，大多数现有模型是确定性的，即仅预测期望均值而不量化不确定性，可能导致不可靠和不准确的结果。虽然最近的研究已经引入概率模型来量化不确定性，但它们通常只关注单一现象（如出租车、自行车、犯罪或交通事故），从而忽略了异构城市现象之间的内在相关性。为了解决这一研究空白，我们提出了一种名为UQGNN的新型图神经网络，用于多变量时空预测的不确定性量化。UQGNN引入了两个关键创新：(i) 交互感知时空嵌入模块，它整合了多变量扩散图卷积网络和交互感知时间卷积网络，以有效捕获复杂的时空交互模式；(ii) 多变量概率预测模块，用于估计期望均值和相关的不确定性。在深圳、纽约市和芝加哥四个真实世界多变量时空数据集上的大量实验表明，UQGNN在预测准确性和不确定性量化方面都始终优于最先进的基线。例如，在深圳数据集上，UQGNN在预测准确性和不确定性量化方面都实现了5%的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatiotemporal prediction plays a critical role in numerous real-worldapplications such as urban planning, transportation optimization, disasterresponse, and pandemic control. In recent years, researchers have madesignificant progress by developing advanced deep learning models forspatiotemporal prediction. However, most existing models are deterministic,i.e., predicting only the expected mean values without quantifying uncertainty,leading to potentially unreliable and inaccurate outcomes. While recent studieshave introduced probabilistic models to quantify uncertainty, they typicallyfocus on a single phenomenon (e.g., taxi, bike, crime, or traffic crashes),thereby neglecting the inherent correlations among heterogeneous urbanphenomena. To address the research gap, we propose a novel Graph Neural Networkwith Uncertainty Quantification, termed UQGNN for multivariate spatiotemporalprediction. UQGNN introduces two key innovations: (i) an Interaction-awareSpatiotemporal Embedding Module that integrates a multivariate diffusion graphconvolutional network and an interaction-aware temporal convolutional networkto effectively capture complex spatial and temporal interaction patterns, and(ii) a multivariate probabilistic prediction module designed to estimate bothexpected mean values and associated uncertainties. Extensive experiments onfour real-world multivariate spatiotemporal datasets from Shenzhen, New YorkCity, and Chicago demonstrate that UQGNN consistently outperformsstate-of-the-art baselines in both prediction accuracy and uncertaintyquantification. For example, on the Shenzhen dataset, UQGNN achieves a 5%improvement in both prediction accuracy and uncertainty quantification.</description>
      <author>example@mail.com (Dahai Yu, Dingyi Zhuang, Lin Jiang, Rongchao Xu, Xinyue Ye, Yuheng Bu, Shenhao Wang, Guang Wang)</author>
      <guid isPermaLink="false">2508.08551v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>M3-Net: A Cost-Effective Graph-Free MLP-Based Model for Traffic Prediction</title>
      <link>http://arxiv.org/abs/2508.08543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M3-Net的经济高效的无图多层感知器模型，用于交通预测任务，该模型在预测性能和轻量级部署方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;准确交通预测是智能交通系统开发的基础任务，主流方法依赖时空图神经网络和注意力机制，但这些方法要么需要完整交通网络结构，要么需要复杂模型设计来捕捉时空依赖关系，限制了在大规模数据集上的高效部署。&lt;h4&gt;目的&lt;/h4&gt;解决现有深度学习方法在交通预测中的局限性，提出一种经济高效且无需图结构的模型。&lt;h4&gt;方法&lt;/h4&gt;提出M3-Net模型，使用时间序列和时空嵌入进行高效特征处理，并首次引入具有专家混合机制的新型MLP-Mixer架构。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实数据集上的广泛实验表明，M3-Net在预测性能和轻量级部署方面具有优越性。&lt;h4&gt;结论&lt;/h4&gt;M3-Net模型有效解决了现有深度学习方法在交通预测中的局限性，提供了更好的预测性能和部署效率。&lt;h4&gt;翻译&lt;/h4&gt;实现准确的交通预测是当前智能交通系统开发中的一项基础但关键的任务。在交通预测方面取得突破的大多数主流方法依赖于时空图神经网络、时空注意力机制等。现有深度学习方法的主要挑战在于它们要么依赖于完整的交通网络结构，要么需要复杂的模型设计来捕捉复杂的时空依赖关系。这些限制对深度学习模型在大规模数据集上的高效部署和运营提出了重大挑战。为应对这些挑战，我们提出了一种经济高效的无图多层感知器模型M3-Net用于交通预测。我们提出的模型不仅使用时间序列和时空嵌入进行高效特征处理，而且首次引入了一种具有专家混合机制的新型MLP-Mixer架构。在多个真实数据集上进行的广泛实验证明了所提出模型在预测性能和轻量级部署方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving accurate traffic prediction is a fundamental but crucial task inthe development of current intelligent transportation systems.Most of themainstream methods that have made breakthroughs in traffic prediction rely onspatio-temporal graph neural networks, spatio-temporal attention mechanisms,etc. The main challenges of the existing deep learning approaches are that theyeither depend on a complete traffic network structure or require intricatemodel designs to capture complex spatio-temporal dependencies. Theselimitations pose significant challenges for the efficient deployment andoperation of deep learning models on large-scale datasets. To address thesechallenges, we propose a cost-effective graph-free Multilayer Perceptron (MLP)based model M3-Net for traffic prediction. Our proposed model not only employstime series and spatio-temporal embeddings for efficient feature processing butalso first introduces a novel MLP-Mixer architecture with a mixture of experts(MoE) mechanism. Extensive experiments conducted on multiple real datasetsdemonstrate the superiority of the proposed model in terms of predictionperformance and lightweight deployment.</description>
      <author>example@mail.com (Guangyin Jin, Sicong Lai, Xiaoshuai Hao, Mingtao Zhang, Jinlei Zhang)</author>
      <guid isPermaLink="false">2508.08543v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Discrete Diffusion-Based Model-Level Explanation of Heterogeneous GNNs with Node Features</title>
      <link>http://arxiv.org/abs/2508.08458v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DiGNNExplainer的模型级别解释方法，用于解释异构图神经网络(HGNNs)的预测结果，通过离散去噪扩散技术生成具有真实节点特征的异构图，从而提供更真实和可信的解释。&lt;h4&gt;背景&lt;/h4&gt;许多现实世界的数据集（如引用网络、社交网络和分子结构）自然地表示为异构图，其中节点属于不同类型并具有附加特征。在这些图上的节点分类任务对假新闻检测、企业风险评估和分子性质预测等应用很有用。&lt;h4&gt;目的&lt;/h4&gt;解决现有异构图神经网络解释方法缺乏对实际节点特征支持、无法生成真实可信解释的问题，提供一种能够生成真实节点特征并忠实反映模型决策过程的解释方法。&lt;h4&gt;方法&lt;/h4&gt;提出DiGNNExplainer，一种模型级别的解释方法，通过离散去噪扩散技术合成具有真实节点特征的异构图。在离散空间中使用扩散模型生成真实的离散特征（如词袋特征），突破了之前方法仅限于连续空间的限制。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的评估表明，DiGNNExplainer生成的解释既真实又忠实于模型的决策过程，性能优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;DiGNNExplainer能够有效解决异构图神经网络解释中存在的问题，为理解模型决策提供了更可靠和真实的解释工具。&lt;h4&gt;翻译&lt;/h4&gt;许多现实世界的数据集，如引用网络、社交网络和分子结构，自然地表示为异构图，其中节点属于不同类型并具有附加特征。例如，在引用网络中，代表'论文'或'作者'的节点可能包含关键词或隶属关系等属性。在这些图上的一个关键机器学习任务是节点分类，这对假新闻检测、企业风险评估和分子性质预测等应用很有用。尽管异构图神经网络(HGNNs)在这些情境中表现良好，但它们的预测仍然不透明。现有的事后解释方法除了节点类型的一hot编码外，缺乏对实际节点特征的支持，并且往往无法生成真实、可信的解释。为了解决这些差距，我们提出了DiGNNExplainer，这是一种模型级别的解释方法，通过离散去噪扩散合成具有真实节点特征的异构图。具体来说，我们在离散空间中使用扩散模型生成真实的离散特征（例如，词袋特征），而之前的方法仅限于连续空间。我们在多个数据集上评估了我们的方法，并表明DiGNNExplainer生成的解释既真实又忠实于模型的决策过程，优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many real-world datasets, such as citation networks, social networks, andmolecular structures, are naturally represented as heterogeneous graphs, wherenodes belong to different types and have additional features. For example, in acitation network, nodes representing "Paper" or "Author" may include attributeslike keywords or affiliations. A critical machine learning task on these graphsis node classification, which is useful for applications such as fake newsdetection, corporate risk assessment, and molecular property prediction.Although Heterogeneous Graph Neural Networks (HGNNs) perform well in thesecontexts, their predictions remain opaque. Existing post-hoc explanationmethods lack support for actual node features beyond one-hot encoding of nodetype and often fail to generate realistic, faithful explanations. To addressthese gaps, we propose DiGNNExplainer, a model-level explanation approach thatsynthesizes heterogeneous graphs with realistic node features via discretedenoising diffusion. In particular, we generate realistic discrete features(e.g., bag-of-words features) using diffusion models within a discrete space,whereas previous approaches are limited to continuous spaces. We evaluate ourapproach on multiple datasets and show that DiGNNExplainer producesexplanations that are realistic and faithful to the model's decision-making,outperforming state-of-the-art methods.</description>
      <author>example@mail.com (Pallabee Das, Stefan Heindorf)</author>
      <guid isPermaLink="false">2508.08458v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts</title>
      <link>http://arxiv.org/abs/2508.07094v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ScamDetect框架，这是一个未来2.5年内发展的智能合约恶意软件检测系统，旨在解决现有检测方法面临的混淆技术和平台异质性问题。&lt;h4&gt;背景&lt;/h4&gt;智能合约通过可编程、无需信任的交易改变了去中心化金融，但其广泛采用也吸引了持续且复杂的威胁，如网络钓鱼活动和合约级别漏洞利用。传统基于交易的威胁检测方法会暴露敏感用户数据，引发隐私和安全问题。&lt;h4&gt;目的&lt;/h4&gt;开发ScamDetect，一个强大、模块化且与平台无关的智能合约恶意软件检测框架，为去中心化生态系统提供主动、可扩展的安全性。&lt;h4&gt;方法&lt;/h4&gt;静态字节码分析作为主动缓解策略，在恶意合约执行前识别威胁。PhishingHook作为首个基于机器学习的框架，通过静态字节码和操作码分析实现约90%的检测准确率。ScamDetect将分两个阶段发展：首先通过控制流图的图神经网络分析处理混淆的EVM字节码；其次将检测能力扩展到WASM等新兴运行时。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法面临两个紧迫挑战：日益复杂的字节码混淆技术旨在逃避静态分析，以及区块链环境的异质性需要与平台无关的解决方案。&lt;h4&gt;结论&lt;/h4&gt;ScamDetect旨在解决当前智能合约安全检测的关键挑战，特别是混淆代码检测和多平台兼容性问题，为未来去中心化生态系统提供更强大的安全保障。&lt;h4&gt;翻译&lt;/h4&gt;智能合约通过可编程、无需信任的交易改变了去中心化金融。然而，它们的广泛采用和日益增长的经济重要性吸引了持续且复杂的威胁，如网络钓鱼活动和合约级别漏洞利用。传统的基于交易的威胁检测方法通常会暴露敏感的用户数据和交互，引发隐私和安全问题。作为回应，静态字节码分析已成为一种主动缓解策略，可在恶意合约执行有害操作之前识别它们。基于这种方法，我们引入了PhishingHook，这是第一个通过静态字节码和操作码分析检测智能合约中网络钓鱼活动的基于机器学习的框架，实现了约90%的检测准确率。然而，仍有两个紧迫的挑战：(1)日益复杂的字节码混淆技术的使用旨在逃避静态分析，以及(2)区块链环境的异质性需要与平台无关的解决方案。本文提出了ScamDetect（智能合约无关恶意软件检测器）的愿景，这是一个强大、模块化且与平台无关的智能合约恶意软件检测框架。在接下来的2.5年里，ScamDetect将分两个阶段发展：首先，通过控制流图的图神经网络分析处理混淆的以太坊虚拟机字节码，利用GNN捕获操作码序列之外的复杂结构模式的能力；其次，将检测能力扩展到WASM等新兴运行时。ScamDetect旨在为未来去中心化生态系统实现主动、可扩展的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/DSN-S65789.2025.00068&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Smart contracts have transformed decentralized finance by enablingprogrammable, trustless transactions. However, their widespread adoption andgrowing financial significance have attracted persistent and sophisticatedthreats, such as phishing campaigns and contract-level exploits. Traditionaltransaction-based threat detection methods often expose sensitive user data andinteractions, raising privacy and security concerns. In response, staticbytecode analysis has emerged as a proactive mitigation strategy, identifyingmalicious contracts before they execute harmful actions. Building on thisapproach, we introduced PhishingHook, the first machine-learning-basedframework for detecting phishing activities in smart contracts via staticbytecode and opcode analysis, achieving approximately 90% detection accuracy.Nevertheless, two pressing challenges remain: (1) the increasing use ofsophisticated bytecode obfuscation techniques designed to evade staticanalysis, and (2) the heterogeneity of blockchain environments requiringplatform-agnostic solutions. This paper presents a vision for ScamDetect (SmartContract Agnostic Malware Detector), a robust, modular, and platform-agnosticframework for smart contract malware detection. Over the next 2.5 years,ScamDetect will evolve in two stages: first, by tackling obfuscated EthereumVirtual Machine (EVM) bytecode through graph neural network (GNN) analysis ofcontrol flow graphs (CFGs), leveraging GNNs' ability to capture complexstructural patterns beyond opcode sequences; and second, by generalizingdetection capabilities to emerging runtimes such as WASM. ScamDetect aims toenable proactive, scalable security for the future of decentralized ecosystems.</description>
      <author>example@mail.com (Pasquale De Rosa, Pascal Felber, Valerio Schiavoni)</author>
      <guid isPermaLink="false">2508.07094v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring</title>
      <link>http://arxiv.org/abs/2508.07552v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于特征图收敛分数的独立评估方法，通过双粒度动态加权评分系统和CLIP-based特征图质量评估网络，有效解决了自动驾驶端到端模型中中间功能模块缺乏明确监督信号的问题，提高了特征图质量和模型性能。&lt;h4&gt;背景&lt;/h4&gt;端到端模型正成为自动驾驶感知和规划的主流方法，但缺乏对中间功能模块的明确监督信号，导致操作机制不透明且可解释性有限，使传统方法难以独立评估和训练这些模块。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于特征图收敛分数的独立评估方法，构建双粒度动态加权评分系统，并开发CLIP-based特征图质量评估网络，以实现对功能模块生成特征图质量的全面评估。&lt;h4&gt;方法&lt;/h4&gt;基于特征图-真实表示相似性的评估框架，构建双粒度动态加权评分系统形成统一的特征图质量分数指标，开发结合特征-真实编码器和质量分数预测头的CLIP-FMQE-Net，实现对功能模块生成特征图的实时质量分析。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes数据集上的实验表明，将评估模块整合到训练中可提高3D目标检测性能，NDS指标提升3.89%，验证了该方法在增强特征表示质量和整体模型性能方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能有效评估自动驾驶端到端模型中功能模块生成的特征图质量，通过整合评估模块可显著提高模型的整体性能。&lt;h4&gt;翻译&lt;/h4&gt;端到端模型正在成为自动驾驶感知和规划的主流。然而，由于缺乏对中间功能模块的明确监督信号，导致操作机制不透明且可解释性有限，使传统方法难以独立评估和训练这些模块。本研究开创性地解决了这一问题，基于特征图-真实表示相似性评估框架，提出了基于特征图收敛分数的独立评估方法。构建了双粒度动态加权评分系统，制定了统一的定量指标——特征图质量分数，以实现对功能模块生成的特征图质量的全面评估。进一步开发了基于CLIP的特征图质量评估网络，结合特征-真实编码器和质量分数预测头，能够对功能模块生成的特征图进行实时质量分析。在NuScenes数据集上的实验结果表明，将评估模块整合到训练中提高了3D目标检测性能，NDS指标提升了3.89%。这些结果验证了我们的方法在增强特征表示质量和整体模型性能方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶端到端模型中中间功能模块缺乏显式监督信号的问题，导致操作机制不透明且难以独立评估。这个问题在现实中很重要，因为自动驾驶系统需要高可靠性和可解释性，而传统评估方法无法有效优化功能模块，影响模型开发效率和系统安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到多模块学习(MML)将感知算法划分为专门功能模块的优势，但发现缺乏统一的评估标准。他们借鉴了特征图-真实表示相似性评估框架[19]、对比语言-图像预训练模型(CLIP)[13]、多模块学习[4]和BEVFormer[9]等现有工作，在此基础上设计了基于特征图质量评分(FMQS)的动态评估机制和双粒度动态加权评分系统(DG-DWSS)，并开发了CLIP-FMQE-Net实现闭环评估。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过特征图质量评分实现功能模块的独立评估，结合特征图语义一致性和任务性能反馈建立多维评估模型。整体流程包括：1)特征图质量评估系统，包含特征图编码器、真实值文本编码器和FMQS预测头；2)双粒度成熟度评分机制，包括宏观粒度指标(NDS归一化)和微观粒度指标(CS-CosSim相似性)；3)CLIP-FMQE-Net实现特征-真实值对齐和FMQS预测；4)将评估模块集成到训练过程中，将FMQS作为辅助损失实现自适应优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出特征图质量评分(FMQS)和双粒度动态加权评分系统；2)设计基于CLIP的特征图质量评估网络(CLI-FMQE-Net)；3)将评估模块集成到训练过程中。相比之前工作，不同之处在于：1)相比FMCE-Net只能评估单个功能模块，本文能独立评估多个级联模块；2)相比基于特征图-真实表示相似性的评估方法，本文将评估指标集成到训练中可直接指导优化；3)采用双粒度评估方法，同时从全局和局部角度评估模块成熟度；4)利用CLIP跨模态对齐能力提高评分准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于特征图质量评分的自动驾驶模型解耦功能评估方法，通过双粒度评分系统和CLIP-based评估网络实现了对功能模块训练成熟度的独立评估和优化，显著提高了3D目标检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end models are emerging as the mainstream in autonomous drivingperception and planning. However, the lack of explicit supervision signals forintermediate functional modules leads to opaque operational mechanisms andlimited interpretability, making it challenging for traditional methods toindependently evaluate and train these modules. Pioneering in the issue, thisstudy builds upon the feature map-truth representation similarity-basedevaluation framework and proposes an independent evaluation method based onFeature Map Convergence Score (FMCS). A Dual-Granularity Dynamic WeightedScoring System (DG-DWSS) is constructed, formulating a unified quantitativemetric - Feature Map Quality Score - to enable comprehensive evaluation of thequality of feature maps generated by functional modules. A CLIP-based FeatureMap Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combiningfeature-truth encoders and quality score prediction heads to enable real-timequality analysis of feature maps generated by functional modules. Experimentalresults on the NuScenes dataset demonstrate that integrating our evaluationmodule into the training improves 3D object detection performance, achieving a3.89 percent gain in NDS. These results verify the effectiveness of our methodin enhancing feature representation quality and overall model performance.</description>
      <author>example@mail.com (Ludan Zhang, Sihan Wang, Yuqi Dai, Shuofei Qiao, Qinyue Luo, Lei He)</author>
      <guid isPermaLink="false">2508.07552v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision</title>
      <link>http://arxiv.org/abs/2508.09087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3rd Workshop in Data Engineering in Medical Imaging (DEMI),  MICCAI-2025 Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于重加权对比学习框架的属性不可知去偏方法，用于自动化青光眼筛查，通过自适应地关注困难示例来减少模型在不同人口统计子组间的偏见。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型(VLMs)在图像文本检索和零样本分类等多模态任务上取得了显著成功，但即使训练过程中没有明确的受保护属性，也可能表现出人口统计偏见。青光眼是不可逆失明的主要原因，且不成比例地影响服务不足的人群，因此自动化青光眼筛查具有重要的临床意义。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化青光眼筛查方法，从视网膜眼底图像中检测青光眼，同时解决VLMs在医疗应用中可能存在的人口统计偏见问题，提高模型在不同人口统计子组间的公平性表现。&lt;h4&gt;方法&lt;/h4&gt;提出一种属性不可知的去偏方法，包含三个步骤：(i)通过无监督聚类图像-图像嵌入来推断代理子组；(ii)计算CLIP风格的多模态损失和SimCLR风格的图像对对比损失之间的梯度相似性权重；(iii)在联合的、top-k加权目标中应用这些权重，以提高表现较差的聚类权重。这种无标签方法自适应地针对最困难的示例，从而减少子组差异。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在哈佛FairVLMed青光眼子集上进行了评估，通过均衡赔率距离(EOD)、均衡子组AUC(ESAUC)和组内AUC等指标，证明了模型在推断的人口统计子组间具有公平的性能表现。&lt;h4&gt;结论&lt;/h4&gt;这种无标签的属性不可知方法能够有效减少模型在不同子组间的偏见，通过自适应地关注困难示例，提高了自动化青光眼筛查任务中模型在人口统计子组间的公平性。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)在图像文本检索和零样本分类等多模态任务上取得了显著成功，但即使在没有明确受保护属性的训练过程中，它们也可能表现出人口统计偏见。在这项工作中，我们专注于从视网膜眼底图像进行自动化青光眼筛查，这是一个关键应用，因为青光眼是不可逆失明的主要原因，且不成比例地影响服务不足的人群。基于一种基于重加权的对比学习框架，我们引入了一种属性不可知的去偏方法，该方法(i)通过无监督聚类图像-图像嵌入来推断代理子组，(ii)计算CLIP风格的多模态损失和SimCLR风格的图像对对比损失之间的梯度相似性权重，以及(iii)在联合的、top-k加权目标中应用这些权重，以提高表现较差的聚类权重。这种无标签方法自适应地针对最困难的示例，从而减少子组差异。我们在哈佛FairVLMed青光学子集上评估了我们的方法，通过报告均衡赔率距离(EOD)、均衡子组AUC(ESAUC)和组内AUC，证明了在推断的人口统计子组间的公平性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have achieved remarkable success on multimodaltasks such as image-text retrieval and zero-shot classification, yet they canexhibit demographic biases even when explicit protected attributes are absentduring training. In this work, we focus on automated glaucoma screening fromretinal fundus images, a critical application given that glaucoma is a leadingcause of irreversible blindness and disproportionately affects underservedpopulations. Building on a reweighting-based contrastive learning framework, weintroduce an attribute-agnostic debiasing method that (i) infers proxysubgroups via unsupervised clustering of image-image embeddings, (ii) computesgradient-similarity weights between the CLIP-style multimodal loss and aSimCLR-style image-pair contrastive loss, and (iii) applies these weights in ajoint, top-$k$ weighted objective to upweight underperforming clusters. Thislabel-free approach adaptively targets the hardest examples, thereby reducingsubgroup disparities. We evaluate our method on the Harvard FairVLMed glaucomasubset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ESAUC), and Groupwise AUC to demonstrate equitable performance across inferreddemographic subgroups.</description>
      <author>example@mail.com (Ahsan Habib Akash, Greg Murray, Annahita Amireskandari, Joel Palko, Carol Laxson, Binod Bhattarai, Prashnna Gyawali)</author>
      <guid isPermaLink="false">2508.09087v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Masked Clustering Prediction for Unsupervised Point Cloud Pre-training</title>
      <link>http://arxiv.org/abs/2508.08910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3D point cloud pretraining method. 8 pages in the main manuscript&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MaskClu是一种创新的ViTs预训练方法，通过结合掩码点建模和聚类学习，以及全局对比学习机制，有效提升了3D点云理解的性能。&lt;h4&gt;背景&lt;/h4&gt;Vision transformers (ViTs) 已被广泛应用于3D点云理解，掩码自编码是主要的预训练范式。然而，通过标准ViTs从点云学习密集且信息丰富的语义特征仍然是一个未被充分探索的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出MaskClu，一种用于3D点云上ViTs的新型无监督预训练方法，以解决从点云学习密集语义特征的挑战。&lt;h4&gt;方法&lt;/h4&gt;MaskClu结合了基于掩码的点建模和基于聚类的学习，设计为从掩码点云中重建聚类分配和聚类中心，鼓励模型捕获密集语义信息。同时引入全局对比学习机制，通过对同一点云的不同掩码视图进行对比，增强实例级别的特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过联合优化密集语义重建和实例级对比学习这两个互补目标，MaskClu使ViTs能够从3D点云中学习更丰富和语义上有意义的表示。&lt;h4&gt;结论&lt;/h4&gt;通过部分分割、语义分割、目标检测和分类等多个3D任务验证了方法的有效性，MaskClu取得了新的竞争性结果。代码和模型将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;视觉transformers (ViTs) 最近已被广泛应用于3D点云理解，掩码自编码作为主要的预训练范式。然而，通过标准ViTs从点云学习密集且信息丰富的语义特征的挑战仍然探索不足。我们提出了MaskClu，这是一种用于3D点云上ViTs的新型无监督预训练方法，它将基于掩码的点建模与基于聚类的学习相结合。MaskClu设计为从掩码点云中重建聚类分配和聚类中心，从而鼓励模型捕获密集的语义信息。此外，我们引入了一种全局对比学习机制，通过对同一点云的不同掩码视图进行对比，增强实例级别的特征学习。通过联合优化这些互补目标，即密集语义重建和实例级对比学习，MaskClu使ViTs能够从3D点云中学习更丰富和语义上有意义的表示。我们通过多个3D任务验证了我们方法的有效性，包括部分分割、语义分割、目标检测和分类，其中MaskClu取得了新的竞争性结果。代码和模型将在以下地址发布：https://github.com/Amazingren/maskclu。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从3D点云中学习密集且信息丰富的语义特征的问题。这一问题在现实中很重要，因为点云理解广泛应用于自动驾驶、机器人、虚拟现实等领域，而无监督预训练可以减少对大量标注数据的依赖，学习密集语义特征对于部分分割、语义分割、目标检测等下游任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法存在局限性：对比学习方法难以探索对象间相似性，聚类方法存在模糊组分配问题，而掩码自编码方法强调空间关系而非语义信息。因此，作者思考将掩码建模和聚类学习的优势结合，同时考虑局部和全局形状信息的重要性。他们借鉴了掩码自编码的掩码重建思想、聚类学习的伪标签指导、对比学习的视图对比机制以及图卷积网络处理点云空间关系的思路，设计了MaskClu方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将掩码自编码与基于聚类的表示学习相结合，通过重建聚类分配和聚类中心来学习密集语义特征，同时加入全局对比学习机制增强实例级特征学习。整体流程包括：1)点云分割为点块并嵌入；2)生成两个随机掩码视图；3)使用共享编码器处理视图；4)解码器重建完整特征；5)构建几何-语义图并进行MinCut聚类；6)重建聚类中心和分配；7)通过对比不同掩码视图进行全局对比学习；8)联合优化分配损失、聚类中心损失和对比损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的预训练框架结合掩码建模和聚类学习；2)聚类重建目标预测聚类分配和中心；3)几何-语义图构建同时利用几何和语义线索；4)实例级对比策略增强表示判别性；5)联合优化互补目标。相比之前工作，不同于Point-MAE的点级重建，它专注于更高层次语义结构；区别于PointClustering仅使用坐标，它利用几何和特征线索；解决了传统多任务学习中姿势感知与不变特征的冲突问题，能同时捕获局部和全局形状信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MaskClu通过创新性地结合掩码点建模与聚类预测，实现了从无标注3D点云中学习密集语义特征，显著提升了多种下游3D视觉任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision transformers (ViTs) have recently been widely applied to 3D pointcloud understanding, with masked autoencoding as the predominant pre-trainingparadigm. However, the challenge of learning dense and informative semanticfeatures from point clouds via standard ViTs remains underexplored. We proposeMaskClu, a novel unsupervised pre-training method for ViTs on 3D point cloudsthat integrates masked point modeling with clustering-based learning. MaskCluis designed to reconstruct both cluster assignments and cluster centers frommasked point clouds, thus encouraging the model to capture dense semanticinformation. Additionally, we introduce a global contrastive learning mechanismthat enhances instance-level feature learning by contrasting different maskedviews of the same point cloud. By jointly optimizing these complementaryobjectives, i.e., dense semantic reconstruction, and instance-level contrastivelearning. MaskClu enables ViTs to learn richer and more semantically meaningfulrepresentations from 3D point clouds. We validate the effectiveness of ourmethod via multiple 3D tasks, including part segmentation, semanticsegmentation, object detection, and classification, where MaskClu sets newcompetitive results. The code and models will be releasedat:https://github.com/Amazingren/maskclu.</description>
      <author>example@mail.com (Bin Ren, Xiaoshui Huang, Mengyuan Liu, Hong Liu, Fabio Poiesi, Nicu Sebe, Guofeng Mei)</author>
      <guid isPermaLink="false">2508.08910v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2508.09071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The project is visible at https://linsun449.github.io/GeoVLA/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了GeoVLA，一个新的Vision-Language-Action框架，通过整合3D信息来提升机器人的操作能力。该框架结合了视觉语言模型和3D几何信息处理，在模拟和真实环境实验中表现出优越的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action (VLA)模型已成为使机器人能够遵循语言指令并预测相应操作的有前景的方法。然而，当前的VLA模型主要依赖2D视觉输入，忽略了3D物理世界中丰富的几何信息，这限制了它们的空间感知能力和适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效整合3D信息以推进机器人操作的VLA框架，解决当前模型在空间感知和适应性方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;GeoVLA框架结合了视觉语言模型(VLM)处理图像和语言指令，提取融合的视觉语言嵌入；同时将深度图转换为点云，并使用定制的点编码器(点嵌入网络)独立生成3D几何嵌入。然后将这些嵌入连接起来，由提出的空间感知动作专家(3D增强动作专家)处理，该专家结合来自不同传感器模态的信息以生成精确的动作序列。&lt;h4&gt;主要发现&lt;/h4&gt;GeoVLA在模拟和真实环境实验中表现出优越的性能和鲁棒性。在LIBERO和ManiSkill2模拟基准测试中取得了最先进的结果，在需要高度适应性、尺度感知和视角不变性的真实世界任务中显示出显著的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过整合3D几何信息，GeoVLA框架显著提升了机器人的空间感知能力和适应性，为机器人操作提供了一个更强大、更鲁棒的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作(VLA)模型已成为一种有前景的方法，使机器人能够遵循语言指令并预测相应的动作。然而，当前的VLA模型主要依赖2D视觉输入，忽略了3D物理世界中丰富的几何信息，这限制了它们的空间感知能力和适应性。在本文中，我们提出了GeoVLA，一个新颖的VLA框架，能够有效整合3D信息以推进机器人操作。它使用视觉语言模型(VLM)处理图像和语言指令，提取融合的视觉语言嵌入。同时，它将深度图转换为点云，并采用一个定制的点编码器，称为点嵌入网络，独立生成3D几何嵌入。然后将这些生成的嵌入连接起来，由我们提出的空间感知动作专家处理，称为3D增强动作专家，它结合来自不同传感器模态的信息以产生精确的动作序列。通过在模拟和真实环境中的大量实验，GeoVLA展示了优越的性能和鲁棒性。它在LIBERO和ManiSkill2模拟基准测试中取得了最先进的结果，并在需要高度适应性、尺度感知和视角不变性的真实世界任务中显示出显著的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前视觉-语言-行动(VLA)模型过度依赖2D视觉输入而忽视3D几何信息的问题。这个问题很重要，因为3D几何信息能提供准确的深度线索、增强空间理解和视点变化鲁棒性，对机器人在现实世界中进行精确物理操作至关重要，特别是在需要处理不同高度、尺度和视点的任务时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLA模型在2D输入上的局限性，考察了3D感知在VLA中的应用尝试，发现要么破坏视觉编码器和LLM的对齐，要么无法适应新模态。作者设计了双路径架构分别处理2D和3D信息，借鉴了VLM、扩散模型和MoE架构的思想，但创新性地设计了PEN和3DAE模块来解决3D信息整合问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是双模态并行处理，同时利用2D视觉语言信息和3D几何信息。整体流程：1)视觉语言路径用VLM处理图像和语言提取特征；2)几何路径将深度图转为点云，用PEN提取3D特征；3)将两种特征连接后输入3DAE；4)3DAE通过MoE架构和扩散模型生成精确动作序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)双路径架构保留预训练知识同时增强空间感知；2)PEN双路径设计捕捉末端执行器周围的3D结构；3)3DAE的MoE架构和静态路由策略有效融合多模态信息。不同之处：不同于纯2D模型、直接修改视觉编码器的方法、冻结行动专家注入3D特征的方法以及动态路由MoE，GeoVLA是端到端训练的，能更好适应点云模态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeoVLA通过专门设计的点嵌入网络和3D增强行动专家，成功将3D几何信息整合到视觉-语言-行动模型中，显著提升了机器人在空间感知和适应性方面的性能，实现了模拟和现实世界任务中的最先进表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have emerged as a promising approach forenabling robots to follow language instructions and predict correspondingactions.However, current VLA models mainly rely on 2D visual inputs, neglectingthe rich geometric information in the 3D physical world, which limits theirspatial awareness and adaptability. In this paper, we present GeoVLA, a novelVLA framework that effectively integrates 3D information to advance roboticmanipulation. It uses a vision-language model (VLM) to process images andlanguage instructions,extracting fused vision-language embeddings. In parallel,it converts depth maps into point clouds and employs a customized pointencoder, called Point Embedding Network, to generate 3D geometric embeddingsindependently. These produced embeddings are then concatenated and processed byour proposed spatial-aware action expert, called 3D-enhanced Action Expert,which combines information from different sensor modalities to produce preciseaction sequences. Through extensive experiments in both simulation andreal-world environments, GeoVLA demonstrates superior performance androbustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2simulation benchmarks and shows remarkable robustness in real-world tasksrequiring height adaptability, scale awareness and viewpoint invariance.</description>
      <author>example@mail.com (Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, Jiale Cao)</author>
      <guid isPermaLink="false">2508.09071v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Long and Short Range Flows for Point Cloud Filtering</title>
      <link>http://arxiv.org/abs/2508.08542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HybridPF的点云去噪方法，结合短程和长程过滤轨迹，通过两个并行模块分别处理短程分数和长程流，并设计联合损失函数进行端到端训练。同时提出了动态图卷积解码器改进推理过程，实验表明该方法达到最先进结果且推理速度更快。&lt;h4&gt;背景&lt;/h4&gt;点云采集过程容易出错并引入噪声伪影，需要过滤/去噪。然而，现有的过滤方法通常存在点聚类或保留噪声的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的点云去噪方法HybridPF，结合短程和长程过滤轨迹，解决现有方法的点聚类和保留噪声问题。&lt;h4&gt;方法&lt;/h4&gt;1. 提出HybridPF方法，同时考虑短程和长程过滤轨迹；2. 设计两个并行模块：ShortModule和LongModule，每个模块包含编码器-解码器对，分别处理短程分数和长程流；3. 将噪声点视为高噪声变体块和干净块之间的中间状态；4. 设计联合损失函数，以端到端方式同时训练两个模块；5. 提出动态图卷积解码器改进推理过程。&lt;h4&gt;主要发现&lt;/h4&gt;1. 长程信息可以引导短程分数更紧密地与干净点对齐；2. 由长程特征引导的短程分数能产生具有良好点分布且收敛到干净表面的过滤点云；3. 分数模型通常能更快收敛到干净表面；4. 当前基于位移的方法存在解码器架构的局限性。&lt;h4&gt;结论&lt;/h4&gt;HybridPF方法通过结合短程和长程过滤轨迹，解决了现有方法的点聚类和保留噪声问题，达到了最先进的结果，同时实现了更快的推理速度。&lt;h4&gt;翻译&lt;/h4&gt;点云采集过程容易出错并引入噪声伪影，需要进行过滤/去噪。最近的过滤方法通常存在点聚类或保留噪声的问题。在本文中，我们提出了混合点云过滤方法，在去除噪声时同时考虑短程和长程过滤轨迹。众所周知，短程分数可以提供必要的位移，将噪声点移动到底层干净表面。相比之下，长程速度流近似恒定位移，从高噪声变体块指向相应的干净表面。在这里，噪声块被视为高噪声变体和干净块之间的中间状态。我们的直觉是，来自速度流模型的长程信息可以引导短程分数更紧密地与干净点对齐。反过来，分数模型通常能更快收敛到干净表面。具体来说，我们设计了两个并行模块：ShortModule和LongModule，每个模块都包含一个编码器-解码器对，分别处理短程分数和长程流。我们发现，由长程特征引导的短程分数可以产生具有良好点分布且收敛到干净表面的过滤点云。我们设计了一个联合损失函数，以端到端方式同时训练两个模块。最后，我们确定了当前基于位移方法的一个关键弱点，即解码器架构的局限性，并提出了一种动态图卷积解码器来改进推理过程。全面的实验表明，我们的HybridPF实现了最先进的结果，同时实现了更快的推理速度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云滤波/去噪问题。点云捕获过程容易引入噪声和伪影，而现有滤波方法通常存在点云聚类或噪声保留的问题。这个问题在现实中非常重要，因为点云是3D视觉、建模和图形任务中广泛使用的数据形式，滤波/去噪是点云预处理的基本任务，直接影响后续的网格重建、配准、3D建模和场景理解等任务的质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法的局限性：基于分数匹配的方法需要大量迭代且点会聚类，基于直线流动的方法可能过冲或欠冲干净表面，且解码器架构存在局限。作者的关键洞察是长程流动信息可以指导短程随机流动。方法设计上借鉴了分数匹配的短程流和直线流动的长程流思想，同时创新性地将两者结合，并使用动态图卷积改进了解码器架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是同时考虑短程和长程信息来去除点云噪声，其中长程流动信息指导短程随机流动。整体实现流程包括：1) 设计两个并行模块(ShortModule和LongModule)，每个模块采用编码器-解码器结构；2) LongModule训练推断长程常量流，只在训练期间使用；3) ShortModule推断短程分数，条件基于LongModule编码器的特征；4) 使用联合损失函数同时训练两个模块；5) 推理时应用迭代滤波过程，使用ShortModule处理噪声点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 混合点云滤波方法(HybridPF)，同时考虑短程和长程信息；2) 联合训练方案，简化了模型训练；3) 动态图卷积解码器，考虑了高维特征空间中的拓扑信息。相比之前的工作，混合方法解决了分数匹配方法的点云聚类问题和直线流动方法的过冲/欠冲问题，同时动态图卷积解码器提供了比全连接层更好的滤波结果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种混合长程和短程流动的点云滤波方法，结合了分数匹配和直线流动的优势，并通过动态图卷积解码器改进了位移估计，实现了更高质量的点云去噪效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud capture processes are error-prone and introduce noisy artifactsthat necessitate filtering/denoising. Recent filtering methods often sufferfrom point clustering or noise retaining issues. In this paper, we proposeHybrid Point Cloud Filtering ($\textbf{HybridPF}$) that considers bothshort-range and long-range filtering trajectories when removing noise. It iswell established that short range scores, given by $\nabla_{x}\log p(x_t)$, mayprovide the necessary displacements to move noisy points to the underlyingclean surface. By contrast, long range velocity flows approximate constantdisplacements directed from a high noise variant patch $x_0$ towards thecorresponding clean surface $x_1$. Here, noisy patches $x_t$ are viewed asintermediate states between the high noise variant and the clean patches. Ourintuition is that long range information from velocity flow models can guidethe short range scores to align more closely with the clean points. In turn,score models generally provide a quicker convergence to the clean surface.Specifically, we devise two parallel modules, the ShortModule and LongModule,each consisting of an Encoder-Decoder pair to respectively account forshort-range scores and long-range flows. We find that short-range scores,guided by long-range features, yield filtered point clouds with good pointdistributions and convergence near the clean surface. We design a joint lossfunction to simultaneously train the ShortModule and LongModule, in anend-to-end manner. Finally, we identify a key weakness in current displacementbased methods, limitations on the decoder architecture, and propose a dynamicgraph convolutional decoder to improve the inference process. Comprehensiveexperiments demonstrate that our HybridPF achieves state-of-the-art resultswhile enabling faster inference speed.</description>
      <author>example@mail.com (Dasith de Silva Edirimuni, Xuequan Lu, Ajmal Saeed Mian, Lei Wei, Gang Li, Scott Schaefer, Ying He)</author>
      <guid isPermaLink="false">2508.08542v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>3D Human Mesh Estimation from Single View RGBD</title>
      <link>http://arxiv.org/abs/2508.08178v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M$^3$（Masked Mesh Modeling）的方法，用于从单个RGBD视图进行准确的3D人体网格估计。该方法通过利用现有的动作捕捉数据集克服了数据稀缺问题，使用掩码自编码器完成部分网格，有效地恢复了3D人体网格模型中不可见部分，实现了比现有方法更准确的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管从RGB图像和RGBD相机进行3D人体网格估计已取得显著进展，但RGBD相机提供的额外深度数据仍未被充分利用。现有的完全监督方法需要RGBD图像和3D网格标签对的数据集，但收集这样的数据集成本高且具有挑战性，导致现有数据集小，且姿势和形状多样性有限。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，用于从单个RGBD视图进行准确的3D人体网格估计，利用RGBD相机的经济性和广泛性进行实际应用。&lt;h4&gt;方法&lt;/h4&gt;利用现有的动作捕捉(MoCap)数据集，首先从MoCap数据集中的人体模型获取完整的3D网格，然后通过投影到虚拟相机创建它们的部分、单视图版本，模拟RGBD相机从单一视点提供的深度数据。训练一个掩码自编码器来完成部分、单视图网格。在推理过程中，将传感器传来的深度值与模板人体网格的顶点匹配，创建部分、单视图网格，从而恢复完整的3D人体网格。&lt;h4&gt;主要发现&lt;/h4&gt;在SURREAL和CAPE数据集上分别实现了16.8毫米和22.0毫米的每顶点误差(PVE)，优于使用全身点云作为输入的现有方法。在BEHAVE数据集上获得了具有竞争力的70.9 PVE，比最近发表的基于RGB的方法高出18.4毫米，突显了深度数据的有用性。&lt;h4&gt;结论&lt;/h4&gt;M$^3$方法有效地从单个RGBD视图恢复了完整的3D人体网格，证明了深度数据在3D人体网格估计中的价值。代码将被发布。&lt;h4&gt;翻译&lt;/h4&gt;尽管从RGB图像和RGBD相机进行3D人体网格估计已取得显著进展；提供额外深度数据的RGBD相机仍未被充分利用。在本文中，我们提出了一种从单个RGBD视图进行准确3D人体网格估计的方法，利用RGBD相机的经济性和广泛采用性进行实际应用。这个问题的完全监督方法需要包含RGBD图像和3D网格标签对的数据集。然而，收集这样的数据集成本高昂且具有挑战性，因此现有数据集小，且姿势和形状多样性有限。为了克服这种数据稀缺问题，我们利用现有的动作捕捉(MoCap)数据集。我们首先从MoCap数据集中的人体模型获取完整的3D网格，并通过投影到虚拟相机创建它们的部分、单视图版本。这模拟了从单一视点由RGBD相机提供的深度数据。然后，我们训练一个掩码自编码器来完成部分、单视图网格。在推理过程中，我们的方法（我们将其命名为M$^3$，代表'Masked Mesh Modeling'）将传感器传来的深度值与模板人体网格的顶点匹配，从而创建部分、单视图网格。我们有效地恢复了3D人体网格模型中不可见部分，得到完整的人体网格。M$^3$在SURREAL和CAPE数据集上分别实现了16.8毫米和22.0毫米的每顶点误差(PVE)，优于使用全身点云作为输入的现有方法。我们在BEHAVE数据集上获得了具有竞争力的70.9 PVE，比最近发表的基于RGB的方法高出18.4毫米，突显了深度数据的有用性。代码将被发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单视角RGBD图像估计3D人体网格模型的问题。这个问题重要是因为RGBD相机越来越便宜普及，而准确估计3D人体网格在计算机图形学、医疗保健、体育和AR/VR等领域有广泛应用。相比仅使用RGB图像的方法，RGBD提供额外深度信息有助于更准确重建；相比需要完整3D扫描的方法，单视角RGBD更易于获取且成本更低。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先确定了两个主要挑战：从部分可见数据完成人体网格和缺乏成对的RGBD-3D网格训练数据。针对第一个挑战，借鉴了掩码图像建模(Masked Image Modeling)的思想，使用基于Transformer的掩码自编码器完成部分网格；针对第二个挑战，利用现有的动作捕捉(MoCap)数据集，通过虚拟相机投影模拟RGBD深度数据。整体设计结合了DensePose(用于UV映射提取)和ViT-MAE(掩码自编码器架构)等现有工作，但进行了创新整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用掩码自编码器来完成部分人体网格，利用MoCap数据解决训练数据稀缺问题，并通过UV映射建立RGB像素与3D网格顶点间的对应关系。整体流程：1)输入RGBD图像；2)用Densepose提取UV映射；3)结合深度数据和UV映射生成3D点云；4)将3D点与模板网格顶点匹配生成部分网格；5)用掩码自编码器完成部分网格；6)输出完整3D人体网格。训练采用两阶段：先在MoCap数据上训练，再在真实RGBD数据上微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次系统研究单视角RGBD人体网格估计；2)提出无需成对RGBD-3D网格数据的训练方法；3)设计基于Transformer的掩码自编码器(M3)；4)通过UV映射结合RGB和深度数据；5)在多个数据集上实现最先进性能。相比之前工作的不同：与仅用RGB的方法相比，利用额外深度信息提高准确性；与需要完整3D扫描的方法相比，只需单视角数据；与需要成对数据的方法相比，利用现有MoCap数据；与优化循环方法相比，效率更高；与使用序列数据的方法相比，只需单帧数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的基于掩码自编码器的单视角RGBD人体网格估计方法，通过利用动作捕捉数据集解决了训练数据稀缺问题，并在多个数据集上实现了最先进的性能，为现实世界应用提供了高效准确的3D人体重建解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant progress in 3D human mesh estimation from RGB images;RGBD cameras, offering additional depth data, remain underutilized. In thispaper, we present a method for accurate 3D human mesh estimation from a singleRGBD view, leveraging the affordability and widespread adoption of RGBD camerasfor real-world applications. A fully supervised approach for this problem,requires a dataset with RGBD image and 3D mesh label pairs. However, collectingsuch a dataset is costly and challenging, hence, existing datasets are small,and limited in pose and shape diversity. To overcome this data scarcity, weleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3Dmeshes from the body models found in MoCap datasets, and create partial,single-view versions of them by projection to a virtual camera. This simulatesthe depth data provided by an RGBD camera from a single viewpoint. Then, wetrain a masked autoencoder to complete the partial, single-view mesh. Duringinference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',matches the depth values coming from the sensor to vertices of a template humanmesh, which creates a partial, single-view mesh. We effectively recover partsof the 3D human body mesh model that are not visible, resulting in a full bodymesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREALand CAPE datasets, respectively; outperforming existing methods that usefull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVEdataset, outperforming a recently published RGB based method by 18.4 mm,highlighting the usefulness of depth data. Code will be released.</description>
      <author>example@mail.com (Ozhan Suat, Bedirhan Uguz, Batuhan Karagoz, Muhammed Can Keles, Emre Akbas)</author>
      <guid isPermaLink="false">2508.08178v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Open Scene Graphs for Open-World Object-Goal Navigation</title>
      <link>http://arxiv.org/abs/2508.04678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In IJRR Special Issue: Foundation Models and Neuro-symbolic AI for  Robotics. Journal extension to arXiv:2407.02473&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OSG Navigator是一种用于开放世界目标物体导航的模块化系统，结合基础模型和开放场景图表示，实现了零样本适应新环境和泛化到多样化目标、环境和机器人形态的能力。&lt;h4&gt;背景&lt;/h4&gt;如何构建用于开放世界语义导航的通用机器人系统，例如在陌生环境中搜索自然语言指定的目标物体。基础模型虽然能提供丰富的语义知识，但在大规模组织和维护空间信息方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够零样本适应新环境类型、在目标物体导航任务上取得最先进性能并能够泛化到多样化目标、环境和机器人形态的机器人导航系统。&lt;h4&gt;方法&lt;/h4&gt;OSG Navigator是一个模块化系统，由基础模型组成。其核心是开放场景图表示，作为系统的空间记忆。系统使用OSG模式(模板)来分层组织空间信息，这些模式可以从简单语义标签自动生成，如'家'或'超市'。&lt;h4&gt;主要发现&lt;/h4&gt;在Fetch和Spot机器人的模拟和真实世界实验中，OSG Navigator在ObjectNav基准测试上实现了最先进性能，并能零样本泛化到多样化的目标、环境和机器人形态。&lt;h4&gt;结论&lt;/h4&gt;OSG Navigator通过结合基础模型和开放场景图表示，有效解决了开放世界语义导航中的挑战，为构建通用机器人导航系统提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;我们如何构建用于开放世界语义导航的通用机器人系统，例如在陌生环境中搜索自然语言指定的目标物体？为应对这一挑战，我们引入了OSG Navigator，这是一个由基础模型组成的模块化系统，用于开放世界目标物体导航(ObjectNav)。基础模型提供了关于世界的丰富语义知识，但在大规模组织和维护空间信息方面存在困难。OSG Navigator的关键是开放场景图表示，它作为OSG Navigator的空间记忆。它使用OSG模式(模板)分层组织空间信息，每个模式描述一类环境的常见结构。OSG模式可以从给定环境的简单语义标签自动生成，例如'家'或'超市'。它们使OSG Navigator能够零样本适应新环境类型。我们在模拟和真实世界中使用Fetch和Spot机器人进行了实验，表明OSG Navigator在ObjectNav基准测试上实现了最先进性能，并能零样本泛化到多样化的目标、环境和机器人形态。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何构建通用机器人系统用于开放世界的语义导航问题，特别是在未知环境中根据自然语言描述寻找特定目标物体。这个问题在现实中非常重要，因为它能让机器人在陌生环境中执行实用任务，如在药店找药、超市购物或家中寻找物品，而不需要预先了解环境地图或接受特定训练。研究上，它解决了传统导航系统难以泛化到新环境、新目标和不同机器人平台的局限性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到基础模型（如LLMs和VFMs）拥有丰富的语义知识但难以有效组织空间信息，因此提出开放场景图（OSG）作为结构化空间记忆。他们借鉴了现有工作中的基础模型（GPT-3.5、GroundingDINO、ViNT等）、场景图表示、语义导航任务和概率拓扑映射框架，但创新性地将这些元素组合成一个能够处理开放世界导航挑战的系统。设计思路是分层组织空间信息，通过OSG schemas实现环境适应，并将高级语义推理与低级导航控制分离。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用开放场景图（OSG）作为结构化的环境表示，结合基础模型的语义知识实现零样本泛化。整体流程分为三部分：1）映射阶段：从图像中提取语义信息，估计机器人状态，并构建/更新OSG；2）推理与控制阶段：基于OSG提出搜索子目标，规划路径，并执行导航命令；3）可选的OSG Schema生成：使用LLMs从简单环境标签自动生成环境结构模板。系统通过分层抽象（Objects、Places、Connectors、Region Abstractions）组织空间信息，实现从粗粒度到细粒度的环境理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）开放场景图（OSG）表示，支持多种抽象层次和开放世界环境；2）零样本泛化能力，能适应新环境类型、开放词汇目标和不同机器人平台；3）模块化基础模型架构，完全由基础模型组成；4）自动OSG Schema生成，只需简单环境标签。相比之前工作，传统系统需要特定环境的手动工程和训练，而OSG Navigator能处理三个维度的泛化（目标、环境、机器人），且使用纯语义信息（不依赖精确几何）实现强大性能，同时支持新环境类型的零样本适应。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OSG Navigator通过结合基础模型和开放场景图表示，实现了在开放世界中零样本泛化的物体目标导航，能够适应多样化的目标、环境和机器人平台，无需特定环境的手动工程或预先训练。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1177/02783649251369549&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How can we build general-purpose robot systems for open-world semanticnavigation, e.g., searching a novel environment for a target object specifiedin natural language? To tackle this challenge, we introduce OSG Navigator, amodular system composed of foundation models, for open-world Object-GoalNavigation (ObjectNav). Foundation models provide enormous semantic knowledgeabout the world, but struggle to organise and maintain spatial informationeffectively at scale. Key to OSG Navigator is the Open Scene Graphrepresentation, which acts as spatial memory for OSG Navigator. It organisesspatial information hierarchically using OSG schemas, which are templates, eachdescribing the common structure of a class of environments. OSG schemas can beautomatically generated from simple semantic labels of a given environment,e.g., "home" or "supermarket". They enable OSG Navigator to adapt zero-shot tonew environment types. We conducted experiments using both Fetch and Spotrobots in simulation and in the real world, showing that OSG Navigator achievesstate-of-the-art performance on ObjectNav benchmarks and generalises zero-shotover diverse goals, environments, and robot embodiments.</description>
      <author>example@mail.com (Joel Loo, Zhanxin Wu, David Hsu)</author>
      <guid isPermaLink="false">2508.04678v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
  <item>
      <title>3D Human Mesh Estimation from Single View RGBD</title>
      <link>http://arxiv.org/abs/2508.08178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从单RGBD视图进行准确3D人体网格估计的方法，称为M$^3$（Masked Mesh Modeling）。该方法利用动作捕捉数据集克服了RGBD-3D网格配对数据集稀缺的问题，通过掩码自编码器完成部分人体网格，有效恢复不可见的身体部分。实验证明该方法在多个数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;尽管从RGB图像进行3D人体网格估计已取得显著进展，但提供额外深度数据的RGBD相机仍未得到充分利用。现有的完全监督方法需要包含RGBD图像和3D网格标签对的数据集，但收集这样的数据集成本高且具有挑战性，导致现有数据集小且多样性有限。&lt;h4&gt;目的&lt;/h4&gt;提出一种从单RGBD视图进行准确3D人体网格估计的方法，利用RGBD相机的经济性和广泛应用性进行实际应用。&lt;h4&gt;方法&lt;/h4&gt;利用现有的动作捕捉(MoCap)数据集获取完整3D网格，通过投影到虚拟相机创建部分单视图版本模拟RGBD深度数据。训练掩码自编码器完成部分单视图网格。在推理过程中，M$^3$方法将传感器深度值与模板人体网格顶点匹配创建部分网格，有效恢复不可见的身体部分获得完整网格。&lt;h4&gt;主要发现&lt;/h4&gt;M$^3$在SURREAL和CAPE数据集上分别实现了16.8毫米和22.0毫米的每顶点误差(PVE)，优于使用全身点云作为输入的现有方法。在BEHAVE数据集上获得70.9的竞争性PVE，比最近发布的基于RGB的方法好18.4毫米，突显了深度数据的有用性。&lt;h4&gt;结论&lt;/h4&gt;该方法有效利用RGBD深度数据进行3D人体网格估计，在多个数据集上优于现有方法，代码将被发布。&lt;h4&gt;翻译&lt;/h4&gt;尽管从RGB图像进行3D人体网格估计已取得显著进展；提供额外深度数据的RGBD相机仍未得到充分利用。在本文中，我们提出了一种从单RGBD视图进行准确3D人体网格估计的方法，利用RGBD相机的经济性和广泛应用性进行实际应用。这个问题的完全监督方法需要包含RGBD图像和3D网格标签对的数据集。然而，收集这样的数据集成本高昂且具有挑战性，因此现有数据集小，且在姿势和形状多样性方面有限。为克服这种数据稀缺问题，我们利用现有的动作捕捉(MoCap)数据集。我们首先从MoCap数据集中的身体模型获取完整的3D网格，并通过投影到虚拟相机创建它们的单视图部分版本。这模拟了RGBD相机从单一视点提供的深度数据。然后，我们训练一个掩码自编码器来完成部分、单视图网格。在推理过程中，我们的方法（我们称之为M$^3$，代表'Masked Mesh Modeling'）将传感器传来的深度值与模板人体网格的顶点匹配，创建部分、单视图网格。我们有效地恢复了3D人体身体网格模型中不可见的部分，得到完整身体网格。M$^3$在SURREAL和CAPE数据集上分别实现了16.8毫米和22.0毫米的每顶点误差(PVE)，优于使用全身点云作为输入的现有方法。我们在BEHAVE数据集上获得了70.9的竞争性PVE，比最近发布的基于RGB的方法好18.4毫米，突显了深度数据的有用性。代码将被发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从单视角RGBD图像中估计完整3D人体网格模型的问题。这个问题很重要，因为RGBD相机越来越便宜和普及，而3D人体网格估计在计算机图形学、医疗保健、体育和AR/VR等领域有广泛应用。现有方法要么缺乏深度信息导致3D重建不准确，要么需要昂贵的设备设置或耗时的优化过程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先确定了两个主要挑战：从部分可见数据完成人体网格，以及缺乏成对的RGBD-网格训练数据。针对这些挑战，作者借鉴了掩码图像建模的思想，设计了基于transformer的掩码自编码器来恢复完整网格；同时利用现有的MoCap数据集，通过投影网格到虚拟相机来模拟单视角深度数据。作者借鉴了DensePose进行UV映射提取，以及ViT-MAE的架构思想，但针对3D网格进行了专门设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用掩码自编码器学习从部分可见的人体网格恢复完整网格的能力，并利用MoCap数据集避免对成对RGBD-网格数据的依赖。整体流程：1)使用DensePose提取RGB图像的UV映射；2)结合深度图和UV映射生成带UV值的3D点云；3)通过UV对应关系匹配点云与模板网格顶点，生成部分网格；4)将部分网格输入掩码自编码器，完成不可见部分并去噪可见部分，输出完整3D人体网格。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出不依赖成对RGBD-网格数据的训练方法，利用MoCap数据集创建虚拟单视角深度数据；2)设计了专门的掩码网格建模(M3)方法，基于transformer的掩码自编码器；3)使用UV映射建立2D图像、深度数据和3D网格之间的对应关系。相比之前工作，不同之处在于：利用深度信息提高了3D重建准确性；不需要昂贵设备或耗时的优化过程；采用单次前向传播而非迭代优化；在多个数据集上表现更好；不依赖特定硬件，适用性更广。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于掩码自编码器的单视角RGBD人体网格估计方法，通过利用MoCap数据集和UV映射技术，实现了从部分可见数据中准确恢复完整3D人体网格，为实际应用提供了高效且准确的人体重建解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant progress in 3D human mesh estimation from RGB images;RGBD cameras, offering additional depth data, remain underutilized. In thispaper, we present a method for accurate 3D human mesh estimation from a singleRGBD view, leveraging the affordability and widespread adoption of RGBD camerasfor real-world applications. A fully supervised approach for this problem,requires a dataset with RGBD image and 3D mesh label pairs. However, collectingsuch a dataset is costly and challenging, hence, existing datasets are small,and limited in pose and shape diversity. To overcome this data scarcity, weleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3Dmeshes from the body models found in MoCap datasets, and create partial,single-view versions of them by projection to a virtual camera. This simulatesthe depth data provided by an RGBD camera from a single viewpoint. Then, wetrain a masked autoencoder to complete the partial, single-view mesh. Duringinference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',matches the depth values coming from the sensor to vertices of a template humanmesh, which creates a partial, single-view mesh. We effectively recover partsof the 3D human body mesh model that are not visible, resulting in a full bodymesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREALand CAPE datasets, respectively; outperforming existing methods that usefull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVEdataset, outperforming a recently published RGB based method by 18.4 mm,highlighting the usefulness of depth data. Code will be released.</description>
      <author>example@mail.com (Ozhan Suat, Bedirhan Uguz, Batuhan Karagoz, Muhammed Can Keles, Emre Akbas)</author>
      <guid isPermaLink="false">2508.08178v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking</title>
      <link>http://arxiv.org/abs/2508.08117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GRASPTrack，一种结合单目深度估计和实例分割的新型深度感知多目标跟踪框架，用于解决遮挡和深度模糊问题。&lt;h4&gt;背景&lt;/h4&gt;单目视频中的多目标跟踪面临遮挡和深度模糊的根本性挑战，传统的基于检测的跟踪方法因缺乏几何感知能力而难以解决这些问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理遮挡和深度模糊问题的多目标跟踪框架，提高在复杂场景中的跟踪鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将单目深度估计和实例分割集成到标准基于检测的跟踪流程中，生成高保真三维点云实现几何推理；将点云体素化实现精确的空间关联；引入深度自适应噪声补偿动态调整卡尔曼滤波器；提出深度增强的观察中心动量将运动方向一致性从图像平面扩展到三维空间。&lt;h4&gt;主要发现&lt;/h4&gt;在MOT17、MOT20和DanceTrack基准测试上的实验表明，该方法取得了具有竞争力的性能，显著提高了在频繁遮挡和复杂运动模式的复杂场景中的跟踪鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GRASPTrack通过整合深度感知和三维几何推理，有效解决了单目视频多目标跟踪中的遮挡和深度模糊问题，在复杂场景中表现出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;单目视频中的多目标跟踪在根本上受到遮挡和深度模糊的挑战，这些问题由于缺乏几何感知能力，传统的基于检测的跟踪方法难以解决。为了克服这些局限性，我们引入了GRASPTrack，一种新型深度感知多目标跟踪框架，它将单目深度估计和实例分割集成到标准基于检测的跟踪流程中，从二维检测生成高保真三维点云，从而实现显式的三维几何推理。然后，这些三维点云被体素化，以实现精确和鲁棒的基于体素的3D交并比用于空间关联。为了进一步增强跟踪鲁棒性，我们的方法结合了深度自适应噪声补偿，它根据遮挡严重程度动态调整卡尔曼滤波器过程噪声，以获得更可靠的状态估计。此外，我们提出了深度增强的观察中心动量，它将运动方向一致性从图像平面扩展到三维空间，以改善基于运动的关联线索，特别是对于具有复杂轨迹的物体。在MOT17、MOT20和DanceTrack基准上的大量实验表明，我们的方法取得了具有竞争力的性能，显著提高了在频繁遮挡和复杂运动模式的复杂场景中的跟踪鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目视频中的多目标跟踪问题，特别是遮挡和深度模糊带来的挑战。这个问题在现实中非常重要，因为在自动驾驶、机器人导航和体育分析等应用中，准确跟踪多个物体是至关重要的。遮挡和深度模糊是导致跟踪失败的主要原因，解决这些问题可以提高跟踪系统在复杂场景中的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统基于检测的跟踪方法(TBD)的局限性，特别是在处理遮挡和深度模糊方面的不足。他们借鉴了现有的单目深度估计和实例分割技术，将它们集成到跟踪流程中。作者还参考了OC-SORT等现有跟踪方法，并在此基础上进行了改进，如扩展了状态向量以包含深度信息，并改进了运动一致性建模。此外，作者还借鉴了3D点云处理和体素化技术，以实现更精确的空间关联。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D几何推理集成到多目标跟踪流程中，利用单目深度估计和实例分割生成高保真的3D点云表示，从而更好地处理遮挡和深度模糊问题。整体实现流程包括：1)使用单目深度估计模型生成深度图；2)使用实例分割模型生成物体掩码；3)使用掩码引导投影将2D检测转换为3D点云；4)将3D点云转换为体素表示；5)计算基于体素的3D IoU用于物体关联；6)使用扩展的卡尔曼滤波器进行状态预测，并根据遮挡程度动态调整噪声参数；7)在3D空间中建模运动一致性以改进关联。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)深度感知的MOT框架：集成单目深度估计和实例分割生成高保真3D点云；2)基于体素的3D IoU：用于更精确的空间关联，特别是在遮挡场景中；3)DANC：根据遮挡严重程度动态调整卡尔曼滤波过程噪声；4)DOCM：将运动方向一致性从2D扩展到3D空间。相比之前的工作，不同之处在于：不再依赖2D边界框，而是使用精确的3D点云表示；不再使用固定的过程噪声，而是根据遮挡情况动态调整；不再仅考虑2D运动，而是在3D空间中建模运动一致性；使用掩码引导投影，减少背景和遮挡物体的噪声影响。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GRASPTrack通过集成3D几何推理、自适应噪声补偿和深度增强的运动建模，显著提高了单目视频多目标跟踪在遮挡和深度模糊场景中的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-object tracking (MOT) in monocular videos is fundamentally challengedby occlusions and depth ambiguity, issues that conventionaltracking-by-detection (TBD) methods struggle to resolve owing to a lack ofgeometric awareness. To address these limitations, we introduce GRASPTrack, anovel depth-aware MOT framework that integrates monocular depth estimation andinstance segmentation into a standard TBD pipeline to generate high-fidelity 3Dpoint clouds from 2D detections, thereby enabling explicit 3D geometricreasoning. These 3D point clouds are then voxelized to enable a precise androbust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. Tofurther enhance tracking robustness, our approach incorporates Depth-awareAdaptive Noise Compensation, which dynamically adjusts the Kalman filterprocess noise based on occlusion severity for more reliable state estimation.Additionally, we propose a Depth-enhanced Observation-Centric Momentum, whichextends the motion direction consistency from the image plane into 3D space toimprove motion-based association cues, particularly for objects with complextrajectories. Extensive experiments on the MOT17, MOT20, and DanceTrackbenchmarks demonstrate that our method achieves competitive performance,significantly improving tracking robustness in complex scenes with frequentocclusions and intricate motion patterns.</description>
      <author>example@mail.com (Xudong Han, Pengcheng Fang, Yueying Tian, Jianhui Yu, Xiaohao Cai, Daniel Roggen, Philip Birch)</author>
      <guid isPermaLink="false">2508.08117v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Biases in Surgical Operating Rooms with Geometry</title>
      <link>http://arxiv.org/abs/2508.08028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended Abstract, presented at the MICCAI'25 workshop on  Collaborative Intelligence and Autonomy in Image-guided Surgery&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;深度神经网络在手术室人员识别任务中容易受到虚假相关性的影响，如手术衣标准化导致的视觉特征偏差。研究表明，几何表示方法比基于RGB的方法在现实临床环境中表现更稳定，准确率高12%。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络倾向于学习数据集中的虚假相关性，而非有意义的预测特征。在手术室环境中，手术衣和罩袍的标准化掩盖了稳健的识别特征，为手术室人员建模任务引入了模型偏差。&lt;h4&gt;目的&lt;/h4&gt;开发能够准确识别手术室人员个性化工作流程特征（如手术技能水平或团队协调能力）的智能辅助系统，避免因视觉特征标准化导致的识别偏差。&lt;h4&gt;方法&lt;/h4&gt;使用基于梯度的显著性分析方法研究两个公开手术室数据集，揭示CNN模型如何受到视觉捷径的影响。通过将人员编码为3D点云序列，将身份相关的形状和运动模式与基于外观的混淆因素分离。&lt;h4&gt;主要发现&lt;/h4&gt;RGB和几何方法在具有明显模拟伪影的数据集上性能相当，但在视觉多样性降低的现实临床环境中，RGB模型的准确性下降了12%，表明几何表示能够捕获更有意义的生物特征。&lt;h4&gt;结论&lt;/h4&gt;几何表示方法为开发手术室中稳健的人员建模提供了有效途径，能够更好地捕捉有意义的生物特征，减少因视觉标准化导致的识别偏差。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络容易学习虚假相关性，利用数据集特有的特征而非有意义的特征进行预测。在手术室中，这表现为手术衣和罩袍的标准化掩盖了稳健的识别特征，为建模手术室人员的任务引入了模型偏差。通过对两个公开手术室数据集进行基于梯度的显著性分析，我们揭示CNN模型会受到此类捷径的影响，专注于偶然的视觉线索，如手术服下的鞋子、独特的眼镜或其他角色特定标识。避免此类偏差对于下一代手术室智能辅助系统至关重要，这些系统应能准确识别个性化的工作流程特征，如手术技能水平或与其他工作人员的协调能力。我们通过将人员编码为3D点云序列来解决此问题，将身份相关的形状和运动模式与基于外观的混淆因素分离。我们的实验表明，虽然在具有明显模拟伪影的数据集上RGB和几何方法性能相当，但在视觉多样性降低的现实临床环境中，RGB模型的准确性下降了12%。这一性能差距证实了几何表示能够捕获更有意义的生物特征，为开发手术室中建模人类的稳健方法提供了途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决手术操作室中深度神经网络模型因医护人员穿着标准化手术服而导致的偏见问题。当模型无法依靠可靠的身份识别特征时，会错误地依赖偶然的视觉线索（如鞋子或眼镜）进行人员识别。这个问题在现实中非常重要，因为手术室智能辅助系统需要准确识别个人化的工作流程特征（如手术技能水平或团队协调能力），而这些系统若依赖数据集特定的伪相关而非有意义的特征，在新临床环境中的表现会显著下降，影响医疗安全和手术质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过梯度显著度分析（使用GradCAM）揭示了CNN模型如何依赖于偶然的视觉线索，而非真正的身份特征。他们提出转向几何表示学习，将人员编码为3D点云序列，从而捕获与身份相关的形状和运动模式。作者借鉴了现有工作，包括使用LiDARGait框架处理点云表示、采用弱监督方法分割个体、使用三元组损失和在线困难负样本挖掘进行训练，并基于已有观点：几何特征在外观变化时保持不变。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D点云序列而非RGB图像来表示手术室人员，捕获不受标准化手术服影响的几何特征（如身高、步态和运动模式）。整体流程包括：1）收集模拟和真实临床数据集；2）使用弱监督方法从3D场景中分割个体；3）将个体表示为3D点云序列；4）使用修改后的LiDARGait框架，采用三元组损失进行训练；5）通过人员重新识别指标（如mAP、CMC@3）进行评估，并进行四折交叉验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）识别并解决了手术室环境中的特定偏见问题；2）提出使用3D点云序列表示手术室人员；3）通过对比RGB和几何表示，证明几何表示在真实临床环境中的优越性；4）进行详细显著度分析揭示模型偏见来源。相比之前工作，本文专注于解决标准化环境下的偏见问题，强调几何特征而非外观特征的重要性，专门针对手术室环境进行定制，并通过详细分析揭示问题本质而非仅提供解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过使用3D点云序列表示手术室人员，有效解决了标准化手术服导致的模型偏见问题，显著提高了在真实临床环境中的人员识别准确性和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks are prone to learning spurious correlations, exploitingdataset-specific artifacts rather than meaningful features for prediction. Insurgical operating rooms (OR), these manifest through the standardization ofsmocks and gowns that obscure robust identifying landmarks, introducing modelbias for tasks related to modeling OR personnel. Through gradient-basedsaliency analysis on two public OR datasets, we reveal that CNN models succumbto such shortcuts, fixating on incidental visual cues such as footwear beneathsurgical gowns, distinctive eyewear, or other role-specific identifiers.Avoiding such biases is essential for the next generation of intelligentassistance systems in the OR, which should accurately recognize personalizedworkflow traits, such as surgical skill level or coordination with other staffmembers. We address this problem by encoding personnel as 3D point cloudsequences, disentangling identity-relevant shape and motion patterns fromappearance-based confounders. Our experiments demonstrate that while RGB andgeometric methods achieve comparable performance on datasets with apparentsimulation artifacts, RGB models suffer a 12% accuracy drop in realisticclinical settings with decreased visual diversity due to standardizations. Thisperformance gap confirms that geometric representations capture more meaningfulbiometric features, providing an avenue to developing robust methods ofmodeling humans in the OR.</description>
      <author>example@mail.com (Tony Danjun Wang, Tobias Czempiel, Nassir Navab, Lennart Bastian)</author>
      <guid isPermaLink="false">2508.08028v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</title>
      <link>http://arxiv.org/abs/2508.07611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种端到端的人形机器人导航策略，可直接从原始激光雷达点云生成电机命令，实现了复杂动态环境中的安全导航，并通过模拟到现实的转移验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;人形机器人在非结构化、以人为中心的环境中部署需要超越简单运动的导航能力，包括强大感知、可证明安全性和社会感知行为。当前强化学习方法受限于缺乏环境意识的控制器或无法感知复杂3D障碍物的视觉系统。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在复杂动态场景中进行鲁棒导航的人形机器人系统，具备环境感知能力、安全性保证和社会感知行为。&lt;h4&gt;方法&lt;/h4&gt;提出端到端运动策略直接映射激光雷达点云到电机命令；将控制问题表述为约束马尔可夫决策过程；将控制屏障函数原理转化为成本函数；引入基于人机交互研究的舒适导向奖励机制。&lt;h4&gt;主要发现&lt;/h4&gt;所提框架成功从模拟转移到物理人形机器人；机器人能够展示敏捷且安全的导航，有效避开静态和动态3D障碍物。&lt;h4&gt;结论&lt;/h4&gt;结合端到端策略、安全保证和舒适性奖励，实现了人形机器人在复杂环境中的安全、社会感知导航。&lt;h4&gt;翻译&lt;/h4&gt;人形机器人在非结构化、以人为中心的环境中的部署需要超越简单运动的导航能力，包括强大的感知能力、可证明的安全性和社会感知行为。当前的强化学习方法通常受到缺乏环境意识的盲目控制器或无法感知复杂3D障碍物的视觉系统的限制。在这项工作中，我们提出了一个端到端的运动策略，直接将原始时空激光雷达点云映射到电机命令，实现复杂动态场景中的鲁棒导航。我们将控制问题表述为约束马尔可夫决策过程，以正式分离安全与任务目标。我们的关键贡献是一种新颖的方法，将控制屏障函数的原理转化为CMDP中的成本，允许无模型的惩罚近端策略优化在训练过程中强制执行安全约束。此外，我们引入了一系列基于人机交互研究的舒适导向奖励，促进平滑、可预测且干扰性较小的运动。我们通过成功将框架从模拟转移到物理人形机器人上，证明了我们框架的有效性，该机器人展示了围绕静态和动态3D障碍物的敏捷且安全的导航。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决人形机器人在复杂、以人为中心的环境中安全、舒适移动的问题。当前方法要么是盲目控制器缺乏环境感知能力，要么是基于视觉的系统无法感知复杂的3D障碍物。这个问题非常重要，因为人形机器人的最终目标是能够在人类日常环境中无缝共存和协作，这要求它们能够安全、高效地导航复杂空间，并且其行为要符合人类社会的舒适度和接受度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前方法的局限性：盲目控制器无法在有障碍物的环境中导航，而基于深度相机的方法对光照敏感且会丢失3D信息。作者选择LiDAR传感器作为感知方案，因为它具有光照不变性且能提供直接的3D信息。在安全方面，作者指出通过碰撞惩罚来设计奖励函数的方法往往是脆弱且难以调整的。作者借鉴了现有工作，包括使用强化学习开发控制器、将控制问题表述为约束马尔可夫决策过程(CMDP)，以及受控制屏障函数(CBF)理论的启发，但创新性地将其转化为模型自由RL算法的成本函数，最终设计了一个综合的端到端框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接处理原始时空LiDAR点云到电机命令的端到端映射，将控制问题表述为约束马尔可夫决策过程(CMDP)以分离安全与任务目标，将控制屏障函数(CBF)原则转化为CMDP中的成本，以及引入基于人机交互研究的舒适导向奖励。整体流程包括：1)使用LiDAR获取环境3D点云；2)提取64维特征向量；3)结合本体感受信息和命令历史；4)通过GRU和MLP处理数据并输出动作；5)使用LDCBF定义安全边界；6)设计任务导向和舒适导向奖励及安全成本函数；7)使用P3O算法训练策略；8)部署到实际机器人。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)LiDAR驱动的端到端策略，直接处理3D点云克服了盲目和2D视觉方法的局限；2)基于约束强化学习的原则性安全框架，将CBF原理转化为模型自由RL算法的成本；3)舒适导向的奖励结构，基于人机交互研究产生社会感知的动作；4)成功在物理人形机器人上的部署。相比之前工作，不同之处在于：能处理非地面级别障碍物，不受光照影响；提供更可靠的安全保证而非简单的奖励塑造；同时考虑安全性和舒适性，使机器人行为更符合人类期望。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于LiDAR感知和约束强化学习的端到端框架，使人形机器人能够在复杂3D环境中实现安全、舒适的导航，并成功在实际机器人上验证了该方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of humanoid robots in unstructured, human-centric environmentsrequires navigation capabilities that extend beyond simple locomotion toinclude robust perception, provable safety, and socially aware behavior.Current reinforcement learning approaches are often limited by blindcontrollers that lack environmental awareness or by vision-based systems thatfail to perceive complex 3D obstacles. In this work, we present an end-to-endlocomotion policy that directly maps raw, spatio-temporal LiDAR point clouds tomotor commands, enabling robust navigation in cluttered dynamic scenes. Weformulate the control problem as a Constrained Markov Decision Process (CMDP)to formally separate safety from task objectives. Our key contribution is anovel methodology that translates the principles of Control Barrier Functions(CBFs) into costs within the CMDP, allowing a model-free Penalized ProximalPolicy Optimization (P3O) to enforce safety constraints during training.Furthermore, we introduce a set of comfort-oriented rewards, grounded inhuman-robot interaction research, to promote motions that are smooth,predictable, and less intrusive. We demonstrate the efficacy of our frameworkthrough a successful sim-to-real transfer to a physical humanoid robot, whichexhibits agile and safe navigation around both static and dynamic 3D obstacles.</description>
      <author>example@mail.com (Zifan Wang, Xun Yang, Jianzhuang Zhao, Jiaming Zhou, Teli Ma, Ziyao Gao, Arash Ajoudani, Junwei Liang)</author>
      <guid isPermaLink="false">2508.07611v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Dynamic Scenes in Ego Centric 4D Point Clouds</title>
      <link>http://arxiv.org/abs/2508.07251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为EgoDynamic4D的新型QA基准，用于理解和推理以自我为中心的动态4D场景，包含丰富的标注数据、多种任务设计和一个有效的端到端推理框架，实验证明所提方法在动态场景理解任务上优于基线方法。&lt;h4&gt;背景&lt;/h4&gt;理解以自我为中心视角的动态4D场景（随时间变化的3D空间结构）对人机交互、自主导航和具身智能至关重要，但现有的以自我为中心数据集缺乏统一的4D标注和针对细粒度时空推理的任务驱动评估协议，特别是关于物体和人类的运动及其相互作用。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集的不足，引入EgoDynamic4D，一个针对高度动态场景的新型QA基准，包含RGB-D视频、相机姿态、全局唯一实例掩码和4D边界框，以支持细粒度的时空推理。&lt;h4&gt;方法&lt;/h4&gt;构建927K个配有思维链的QA对，设计12个动态QA任务包括智能体运动、人机交互、轨迹预测等，并提出一个端到端的时空推理框架，使用实例感知特征编码、时间和相机编码以及空间自适应下采样，将大型4D场景压缩为语言模型可处理的标记序列。&lt;h4&gt;主要发现&lt;/h4&gt;在EgoDynamic4D上的实验表明，所提出的方法始终优于基线方法，验证了多模态时间建模对以自我为中心的动态场景理解的有效性。&lt;h4&gt;结论&lt;/h4&gt;EgoDynamic4D基准为理解和推理动态4D场景提供了新的资源和方法，多模态时间建模在以自我为中心的动态场景理解中具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;理解从自我中心视角出发的动态四维场景即建模三维空间结构随时间的变化，对于人机交互、自主导航和具身智能至关重要。虽然现有的自我中心数据集包含动态场景，但它们缺乏统一的四维标注和针对细粒度时空推理的任务驱动评估协议，特别是关于物体和人类的运动及其相互作用。为解决这一差距，我们引入了EgoDynamic4D，一个针对高度动态场景的新型QA基准，包含RGB-D视频、相机姿态、全局唯一实例掩码和四维边界框。我们构建了927K个QA对，并配有明确的思维链，可实现可验证的、逐步的时空推理。我们设计了12个动态QA任务，包括智能体运动、人机交互、轨迹预测、关系理解和时序因果推理，具有细粒度、多维度的指标。为解决这些任务，我们提出了一个端到端的时空推理框架，统一处理动态和静态场景信息，使用实例感知特征编码、时间和相机编码以及空间自适应下采样，将大型四维场景压缩为语言模型可管理的标记序列。在EgoDynamic4D上的实验表明，我们的方法始终优于基线方法，验证了多模态时间建模对自我中心动态场景理解的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从第一人称视角理解动态4D场景（3D空间随时间变化）的问题。这个问题在现实中很重要，因为人机交互、自主导航和具身智能等领域需要准确理解周围环境的变化。现有的第一人称数据集缺乏统一的4D标注和任务驱动的评估协议，特别是对物体和人类运动及其相互作用的细粒度时空推理，限制了相关技术的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，包括不完整的4D标注、有限的时序推理评估和不完整的多模态评估。然后，他们创建了EgoDynamic4D基准，整合了ADT（真实世界）和THUD++（合成场景）两个数据集，并设计了端到端的时空推理框架。他们借鉴了现有工作如CLIP视觉编码器、LLaVA-3D等3D LLM，以及Video-CoT和SpatialCoT的思维链方法，但将这些技术扩展到4D动态场景理解中，并进行了创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将动态4D点云转换为LLM可处理的令牌序列，通过实例感知特征编码区分不同对象，使用时间和相机编码捕捉动态变化和视角变化，并通过空间自适应下采样压缩大型4D场景数据。整体流程包括：1)实例和时间增强的点级特征提取；2)特征融合，包括动态下采样、时间编码和特征集成；3)相机嵌入，压缩相机姿态序列；4)将融合特征投影到LLM嵌入空间并进行推理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)EgoDynamic4D基准，包含927K个QA对和思维链，涵盖12种任务类型；2)统一的多模态数据集，提供完整的4D标注；3)端到端时空推理框架，使用实例感知特征编码、时间和相机编码；4)思维链监督增强推理能力。相比之前工作，本文提供了完整的4D标注而非稀疏标注，专门针对动态场景而非静态场景，集成了时间和相机信息，并提供了细粒度的多维评估指标和思维链评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了EgoDynamic4D基准和一个端到端的时空推理框架，通过统一的4D标注和思维链监督，显著提升了从第一人称视角理解动态场景的能力，为具身AI和机器人感知等领域提供了新的技术基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding dynamic 4D scenes from an egocentric perspective-modelingchanges in 3D spatial structure over time-is crucial for human-machineinteraction, autonomous navigation, and embodied intelligence. While existingegocentric datasets contain dynamic scenes, they lack unified 4D annotationsand task-driven evaluation protocols for fine-grained spatio-temporalreasoning, especially on motion of objects and human, together with theirinteractions. To address this gap, we introduce EgoDynamic4D, a novel QAbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,globally unique instance masks, and 4D bounding boxes. We construct 927K QApairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,step-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks coveringagent motion, human-object interaction, trajectory prediction, relationunderstanding, and temporal-causal reasoning, with fine-grained,multidimensional metrics. To tackle these tasks, we propose an end-to-endspatio-temporal reasoning framework that unifies dynamic and static sceneinformation, using instance-aware feature encoding, time and camera encoding,and spatially adaptive down-sampling to compress large 4D scenes into tokensequences manageable by LLMs. Experiments on EgoDynamic4D show that our methodconsistently outperforms baselines, validating the effectiveness of multimodaltemporal modeling for egocentric dynamic scene understanding.</description>
      <author>example@mail.com (Junsheng Huang, Shengyu Hao, Bocheng Hu, Gaoang Wang)</author>
      <guid isPermaLink="false">2508.07251v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree</title>
      <link>http://arxiv.org/abs/2508.07083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TeSO是一种创新的3D表示方法，通过结合Surfel和纹理贴图在八叉树结构中，解决了现有3D表示方法的局限性，实现了高质量渲染和高效压缩的平衡&lt;h4&gt;背景&lt;/h4&gt;3D视觉内容流技术是新兴的3D远程呈现和AR/VR应用的关键技术。该技术的基础是一种通用的3D表示方法，能够同时产生高质量渲染和高效压缩。现有的3D表示方法（点云、网格和3D高斯）在渲染质量、表面定义和可压缩性方面各有局限&lt;h4&gt;目的&lt;/h4&gt;提出一种新的3D表示方法，解决现有方法的局限性，创建一种能够同时提供高质量渲染和高效压缩的3D表示方法&lt;h4&gt;方法&lt;/h4&gt;提出了纹理化Surfel八叉树（TeSO），该方法基于点云构建，将3D场景表示为在八叉树上组织的立方体边界Surfel，每个Surfel关联一个纹理贴图。通过在八叉树的较粗层级用大Surfel近似平滑表面，减少表示3D场景所需的基元数量，同时保留高频纹理细节。还提出了一种压缩方案，利用八叉树结构高效编码几何和纹理信息&lt;h4&gt;主要发现&lt;/h4&gt;提出的纹理化Surfel八叉树结合压缩方案，与多个基于点云和3D高斯的基线相比，能够在较低的比特率下实现更高的渲染质量&lt;h4&gt;结论&lt;/h4&gt;TeSO是一种有前景的3D表示方法，能够平衡渲染质量和压缩效率&lt;h4&gt;翻译&lt;/h4&gt;3D视觉内容流是新兴的3D远程呈现和AR/VR应用的关键技术。该技术的基础是一种通用的3D表示方法，能够同时产生高质量渲染并可以高效压缩。现有的3D表示方法如点云、网格和3D高斯在渲染质量、表面定义和可压缩性方面各有局限。在本文中，我们提出了纹理化Surfel八叉树（TeSO），一种基于点云构建但解决了上述局限性的新型3D表示方法。它将3D场景表示为在八叉树上组织的立方体边界Surfel，其中每个Surfel进一步关联一个纹理贴图。通过在八叉树的较粗层级用大Surfel近似平滑表面，减少了表示3D场景所需的基元数量，同时通过附加到每个Surfel的纹理图保留了高频纹理细节。我们进一步提出了一种压缩方案，利用八叉树结构高效编码几何和纹理信息。与多个基于点云和3D高斯的基线相比，所提出的纹理化Surfel八叉树结合压缩方案在较低的比特率下实现了更高的渲染质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D视觉内容流媒体中的表示和压缩问题，特别是针对新兴的3D远程呈现和AR/VR应用需要同时提供高质量渲染和高效压缩的需求。这个问题很重要，因为它直接影响3D远程通信技术的实用性和用户体验，如远程AR教育、3D视频会议和沉浸式游戏等应用需要实时传输大量3D数据，而现有方法（点云、网格、3D高斯）在渲染质量、表面定义和压缩性方面各有局限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D表示方法的优缺点：点云灵活但缺乏显式表面表示；网格有表面定义但不够紧凑；3D高斯渲染质量好但无法表示高频纹理。作者观察到3D场景中几何和纹理复杂性可能不同，平滑区域可用较少基元表示，但仍需保留纹理细节。基于这些观察，作者设计了TeSO方法，借鉴了八叉树结构、点云渲染中的splatting概念、网格纹理映射、学习熵模型和3D高斯溅射的混合机制等现有工作，并将它们创新地结合在一起。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D场景表示为由八叉树组织的立方体边界纹理表面元素（surfel），每个surfel包含几何属性（位置、法线、半径）和一个纹理贴图，允许在平滑区域使用较大surfel减少基元数量，同时通过纹理贴图保留高频细节。整体流程包括：1)从点云构建surfel八叉树；2)为每个surfel生成纹理贴图；3)使用光线追踪和软区域混合进行高质量渲染；4)使用学习熵模型压缩几何，用标准视频编解码器或点云方法压缩纹理；5)解压缩并从任意视角渲染场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Textured Surfel Octree新表示方法，结合点云和网格优点；2)设计GPU加速的构建算法，100万点可在0.3秒内完成；3)提出专门的压缩方案，使用学习卷积熵模型；4)设计高质量渲染器，支持6自由度实时渲染。相比之前工作：与点云方法相比，TeSO提供显式表面表示，无渲染空洞，不需解码端重建；与网格方法相比，无需顶点连接信息，更紧凑灵活；与3D高斯方法相比，能减少平滑区域基元，更好表示高频纹理；与现有渲染方法相比，不需重型神经网络，对相机设置更鲁棒，支持实时渲染。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TeSO提出了一种创新的3D场景表示方法，通过结合纹理表面元素和八叉树结构，实现了高质量的实时渲染和高效的压缩，为3D远程呈现和AR/VR应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D visual content streaming is a key technology for emerging 3D telepresenceand AR/VR applications. One fundamental element underlying the technology is aversatile 3D representation that is capable of producing high-quality rendersand can be efficiently compressed at the same time. Existing 3D representationslike point clouds, meshes and 3D Gaussians each have limitations in terms ofrendering quality, surface definition, and compressibility. In this paper, wepresent the Textured Surfel Octree (TeSO), a novel 3D representation that isbuilt from point clouds but addresses the aforementioned limitations. Itrepresents a 3D scene as cube-bounded surfels organized on an octree, whereeach surfel is further associated with a texture patch. By approximating asmooth surface with a large surfel at a coarser level of the octree, it reducesthe number of primitives required to represent the 3D scene, and yet retainsthe high-frequency texture details through the texture map attached to eachsurfel. We further propose a compression scheme to encode the geometry andtexture efficiently, leveraging the octree structure. The proposed texturedsurfel octree combined with the compression scheme achieves higher renderingquality at lower bit-rates compared to multiple point cloud and 3DGaussian-based baselines.</description>
      <author>example@mail.com (Yueyu Hu, Ran Gong, Tingyu Fan, Yao Wang)</author>
      <guid isPermaLink="false">2508.07083v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View</title>
      <link>http://arxiv.org/abs/2508.06968v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究首次评估了鱼眼镜头的3D高斯散射方法在真实广角图像上的性能，提出了基于深度的初始化策略解决强失真问题，并证明了这些方法从稀疏失真图像进行广角3D重建的可行性。&lt;h4&gt;背景&lt;/h4&gt;鱼眼镜头能提供超过180度的广阔视野但导致严重图像失真，现有3D高斯散射方法通常针对普通相机设计，在处理鱼眼图像的极端失真时面临挑战，且基于SfM的初始化在强失真条件下常常失败。&lt;h4&gt;目的&lt;/h4&gt;评估鱼眼基础的3D高斯散射方法在真实广角图像上的性能，分析处理极端失真的能力，研究不同视野角度下的性能权衡，提出新的初始化策略，展示鱼眼3DGS方法的实际应用价值。&lt;h4&gt;方法&lt;/h4&gt;使用200度鱼眼相机拍摄室内外场景，评估Fisheye-GS和3DGUT两种方法，在不同视野角度(200度、160度和120度)下测试性能，提出基于深度的初始化策略利用仅2-3张鱼眼图像通过UniK3D预测生成密集点云进行重建。&lt;h4&gt;主要发现&lt;/h4&gt;Fisheye-GS在160度视野时表现更好，3DGUT在所有设置下保持稳定并在200度视野下维持高质量；基于深度的UniK3D策略能生成与SfM相当的密集点云，即使在雾、眩光或天空等困难场景中也能有效工作。&lt;h4&gt;结论&lt;/h4&gt;鱼眼基础的3D高斯散射方法在处理广角图像和极端失真方面具有实际可行性，通过适当调整视野角度和使用基于深度的初始化策略，即使在具有挑战性的条件下也能实现高质量的3D重建。&lt;h4&gt;翻译&lt;/h4&gt;我们首次对基于鱼眼的3D高斯散射方法(Fisheye-GS和3DGUT)进行了评估，研究对象是视野超过180度的真实图像。我们的研究涵盖了使用200度鱼眼相机拍摄的室内和室外场景，并分析了每种方法如何处理现实世界中的极端失真。我们在不同视野(200度、160度和120度)下评估性能，以研究周边失真与空间覆盖之间的权衡。Fisheye-GS从视野减少中受益，特别是在160度时，而3DGUT在所有设置下保持稳定，并在完整的200度视野下保持高感知质量。为了解决SfM初始化的局限性(在强失真条件下常常失败)，我们还提出了一种基于深度的策略，使用仅每场景2-3张鱼眼图像的UniK3D预测。尽管UniK3D未在真实鱼眼数据上训练，但它能生成密集点云，即使在有雾、眩光或天空的困难场景中，也能实现与SfM相当的重建质量。我们的结果突显了鱼眼基础3DGS方法从稀疏和高度失真的图像输入中进行广角3D重建的实际可行性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要评估鱼眼相机上的3D高斯溅射方法在视野超过180度的真实图像上的表现。这个问题很重要，因为鱼眼相机能用更少图像覆盖更大场景，对自动驾驶、机器人感知和虚拟现实等领域至关重要，但其非线性投影和强畸变给3D重建带来挑战，而现有方法大多针对窄视野透视相机设计，缺乏对鱼眼图像的系统评估。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D高斯溅射在鱼眼相机上的应用潜力但缺乏系统评估，于是借鉴了现有的Fisheye-GS和3DGUT两种鱼眼适配方法。针对传统SfM初始化在强畸变下不可靠的问题，作者探索了使用单目深度估计作为替代方案，特别是评估了UniK3D预测的有效性。实验设计包括使用四个真实场景，评估不同视野角度下的性能，并提出基于深度估计的初始化策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是评估两种鱼眼适配的3D高斯溅射方法在超宽视野真实图像上的表现，并提出基于深度估计的初始化策略替代传统SfM。整体流程包括：1)数据采集和相机校准；2)评估Fisheye-GS和3DGUT两种方法；3)研究不同视野角度(200°、160°、120°)对重建质量的影响；4)使用UniK3D从少量鱼眼图像生成深度图并构建密集点云；5)比较不同初始化策略和视野设置下的重建质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次在超宽视野真实图像上评估鱼眼适配的3D高斯溅射方法；首次实证分析UniK3D在真实鱼眼数据上的表现；展示仅用2-3张图像的单目深度可产生与SfM相当的重建质量；分析视野角度与重建质量的关系；将单目点云与COLMAP坐标系对齐。相比之前工作，本文专注于鱼眼相机这种超宽视野设备，在真实超宽视野图像而非合成数据上评估，并提出了替代SfM的初始化策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统评估了鱼眼适配的3D高斯溅射方法在超宽视野真实图像上的表现，并提出了一种基于深度估计的初始化策略，使得仅用少量鱼眼图像就能实现高质量的3D重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the first evaluation of fisheye-based 3D Gaussian Splattingmethods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180degree. Our study covers both indoor and outdoor scenes captured with 200degree fisheye cameras and analyzes how each method handles extreme distortionin real world settings. We evaluate performance under varying fields of view(200 degree, 160 degree, and 120 degree) to study the tradeoff betweenperipheral distortion and spatial coverage. Fisheye-GS benefits from field ofview (FoV) reduction, particularly at 160 degree, while 3DGUT remains stableacross all settings and maintains high perceptual quality at the full 200degree view. To address the limitations of SfM-based initialization, whichoften fails under strong distortion, we also propose a depth-based strategyusing UniK3D predictions from only 2-3 fisheye images per scene. AlthoughUniK3D is not trained on real fisheye data, it produces dense point clouds thatenable reconstruction quality on par with SfM, even in difficult scenes withfog, glare, or sky. Our results highlight the practical viability offisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse anddistortion-heavy image inputs.</description>
      <author>example@mail.com (Ulas Gunes, Matias Turkulainen, Juho Kannala, Esa Rahtu)</author>
      <guid isPermaLink="false">2508.06968v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery</title>
      <link>http://arxiv.org/abs/2508.06744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种处理高维感官反馈安全关键控制的新方法，特别是在机器人手术等应用领域。通过引入次高斯噪声模型和开发新的不确定性传播技术以及模型预测控制框架，能够在复杂图像引导机器人手术任务中提供安全保证。&lt;h4&gt;背景&lt;/h4&gt;在高维感官反馈（如图像、点云）进行安全关键控制面临重大挑战，特别是在自动驾驶和机器人手术等领域。控制依赖于从高维数据估计的低维状态，但估计误差通常遵循复杂且未知的分布，标准概率模型无法捕捉这些分布，使得形式化安全保证变得困难。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的方法来表征一般的估计误差，开发不确定性传播技术，以及在提出的噪声假设下为线性系统提供闭环安全保证的模型预测控制框架。&lt;h4&gt;方法&lt;/h4&gt;1) 引入使用有界均值的次高斯噪声表征估计误差；2) 开发结合鲁棒集合方法和次高斯方差代理传播的不确定性传播技术；3) 开发模型预测控制框架为线性系统提供安全保证；4) 将方法应用于超声图像引导的机器人脊柱手术流程；5) 开发整合真实人体解剖结构、机器人动力学、超声仿真以及呼吸运动和钻孔力数据的仿真环境。&lt;h4&gt;主要发现&lt;/h4&gt;1) 次高斯噪声模型可有效表征估计误差；2) 结合鲁棒集合方法和次高斯方差代理的不确定性传播技术是有效的；3) 模型预测控制框架能在噪声假设下为线性系统提供安全保证；4) 仿真评估结果表明该方法能解决复杂图像引导机器人手术任务同时确保安全。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在解决复杂图像引导机器人手术任务同时确保安全方面显示出潜力。&lt;h4&gt;翻译&lt;/h4&gt;在高维感官反馈（如图像、点云等光学数据）中进行安全关键控制在自动驾驶和机器人手术等领域带来重大挑战。控制可以依赖于从高维数据估计出的低维状态，但估计误差通常遵循复杂且未知的分布，标准概率模型无法捕捉这些分布，使得形式化安全保证变得困难。在这项工作中，我们引入了一种使用有界均值的次高斯噪声来表征这些一般估计误差的新方法。我们开发了一种在线性系统中传播所提出噪声表征的不确定性新技术，结合了鲁棒的集合方法和次高斯方差代理的传播。我们进一步开发了一种模型预测控制框架，为线性系统在提出的噪声假设下提供闭环安全保证。我们将这种方法应用于基于超声图像引导的机器人脊柱手术流程，该流程包含基于深度学习的语义分割、基于图像的配准、高层基于优化的规划和低层机器人控制。为了验证该流程，我们开发了一个真实的仿真环境，整合了真实人体解剖结构、机器人动力学、高效的超声仿真以及呼吸运动和钻孔力的体内数据。仿真中的评估结果表明了我们的方法在解决复杂图像引导机器人手术任务同时确保安全方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety-critical control using high-dimensional sensory feedback from opticaldata (e.g., images, point clouds) poses significant challenges in domains likeautonomous driving and robotic surgery. Control can rely on low-dimensionalstates estimated from high-dimensional data. However, the estimation errorsoften follow complex, unknown distributions that standard probabilistic modelsfail to capture, making formal safety guarantees challenging. In this work, weintroduce a novel characterization of these general estimation errors usingsub-Gaussian noise with bounded mean. We develop a new technique foruncertainty propagation of proposed noise characterization in linear systems,which combines robust set-based methods with the propagation of sub-Gaussianvariance proxies. We further develop a Model Predictive Control (MPC) frameworkthat provides closed-loop safety guarantees for linear systems under theproposed noise assumption. We apply this MPC approach in anultrasound-image-guided robotic spinal surgery pipeline, which containsdeep-learning-based semantic segmentation, image-based registration, high-leveloptimization-based planning, and low-level robotic control. To validate thepipeline, we developed a realistic simulation environment integrating realhuman anatomy, robot dynamics, efficient ultrasound simulation, as well asin-vivo data of breathing motion and drilling force. Evaluation results insimulation demonstrate the potential of our approach for solving compleximage-guided robotic surgery task while ensuring safety.</description>
      <author>example@mail.com (Yunke Ao, Manish Prajapat, Yarden As, Yassine Taoudi-Benchekroun, Fabio Carrillo, Hooman Esfandiari, Benjamin F. Grewe, Andreas Krause, Philipp Fürnstahl)</author>
      <guid isPermaLink="false">2508.06744v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography</title>
      <link>http://arxiv.org/abs/2508.06703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种高效快速的流水线框架，用于计算机生成全息术(CGH)的合成，通过结合初始点云和MRI数据，利用非凸傅里叶光学优化算法生成全息图，并通过中值滤波提高图像质量。&lt;h4&gt;背景&lt;/h4&gt;计算机生成全息术(CGH)是一种通过数字全息图调制用户定义波形的有前景的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效快速的流水线框架，用于从初始点云和MRI数据合成CGH。&lt;h4&gt;方法&lt;/h4&gt;将输入数据重建为体积对象，然后使用交替投影、随机梯度下降和拟牛顿方法进行非凸傅里叶光学优化，生成仅相位全息图(POH)和复全息图(CH)，并通过二维中值滤波去除伪影和散斑噪声。&lt;h4&gt;主要发现&lt;/h4&gt;使用二维中值滤波可以去除优化过程中的伪影和散斑噪声，从而提高MSE、RMSE和PSNR等性能指标。&lt;h4&gt;结论&lt;/h4&gt;提出的框架和方法能够有效地生成计算机生成全息图，且通过后处理技术可以进一步提高图像质量。&lt;h4&gt;翻译&lt;/h4&gt;计算机生成全息术(CGH)是一种通过数字全息图调制用户定义波形的有前景的方法。该研究提出了一种高效快速的流水线框架，用于使用初始点云和MRI数据合成CGH。这些输入数据被重建为体积对象，然后输入到非凸傅里叶光学优化算法中，使用交替投影、随机梯度下降和拟牛顿方法生成仅相位全息图(POH)和复全息图(CH)。分析了这些算法在MSE、RMSE和PSNR指标下的重建性能，并与HoloNet深度学习CGH进行了比较。研究表明，使用二维中值滤波去除优化过程中的伪影和散斑噪声可以改善性能指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-generated holography (CGH) is a promising method that modulatesuser-defined waveforms with digital holograms. An efficient and fast pipelineframework is proposed to synthesize CGH using initial point cloud and MRI data.This input data is reconstructed into volumetric objects that are then inputinto non-convex Fourier optics optimization algorithms for phase-only hologram(POH) and complex-hologram (CH) generation using alternating projection, SGD,and quasi-Netwton methods. Comparison of reconstruction performance of thesealgorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNetdeep learning CGH. Performance metrics are shown to be improved by using 2Dmedian filtering to remove artifacts and speckled noise during optimization.</description>
      <author>example@mail.com (Justin London)</author>
      <guid isPermaLink="false">2508.06703v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data</title>
      <link>http://arxiv.org/abs/2508.08173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Time-varying data visualization, deep learning, super-resolution,  diffusion model&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CD-TVD的新框架，结合对比学习和改进的扩散模型，实现从有限高分辨率数据中进行准确的3D超分辨率，减少了对大规模数据集的依赖。&lt;h4&gt;背景&lt;/h4&gt;大规模科学模拟需要大量资源生成高分辨率时变数据，现有超分辨率方法依赖大量高分辨率训练数据，限制了其在多样模拟场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从有限时间步长高分辨率数据实现准确3D超分辨率的方法，减少对大规模数据集的依赖。&lt;h4&gt;方法&lt;/h4&gt;CD-TVD框架结合对比学习和改进的扩散模型，通过历史数据预训练学习退化模式和特征，然后利用局部注意力机制和仅一个高分辨率时间步长进行微调，保持细节恢复能力。&lt;h4&gt;主要发现&lt;/h4&gt;在流体和大气模拟数据集上的实验证实，CD-TVD提供了准确且资源高效的3D超分辨率，是大规模科学模拟数据增强的重要进展。&lt;h4&gt;结论&lt;/h4&gt;CD-TVD框架显著减少了对大规模高分辨率数据集的依赖，同时保持了恢复细粒度细节的能力，为科学模拟提供了高效的数据增强解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大规模科学模拟需要大量资源来生成高分辨率时变数据。虽然超分辨率是一种有效的后处理策略来降低成本，但现有方法依赖于大量高分辨率训练数据，限制了它们在不同模拟场景中的适用性。为了解决这一限制，我们提出了CD-TVD，一种结合对比学习和改进的基于扩散的超分辨率模型的新框架，从有限时间步长的高分辨率数据实现准确的3D超分辨率。在历史模拟数据上的预训练期间，对比编码器和扩散超分辨率模块学习高分辨率和低分辨率样本的退化模式和详细特征。在训练阶段，使用仅一个新生成的高分辨率时间步长来微调具有局部注意力机制的改进扩散模型，利用编码器学到的退化知识。这种设计最小化了对大规模高分辨率数据集的依赖，同时保持恢复细粒度细节的能力。在流体和大气模拟数据集上的实验结果证实，CD-TVD提供了准确且资源高效的3D超分辨率，标志着大规模科学模拟数据增强的重大进展。代码可在https://github.com/Xin-Gao-private/CD-TVD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决科学模拟中高分辨率时间变化数据获取成本高昂的问题。在现实研究中，大规模科学模拟需要大量资源生成高分辨率数据，直接进行高分辨率模拟面临计算成本指数增长和数据存储需求激增的瓶颈。虽然低精度计算提高了效率，但往往无法捕捉微观结构的演化细节或临界状态的突变特征，导致预测精度显著下降。超分辨率技术本可通过建立从低精度数据到高分辨率空间的映射关系，用有限计算资源恢复关键物理场中的精细结构，但现有方法依赖大量高分辨率训练数据，而获取这些数据成本极高，例如一个高分辨率案例在CFD模拟中通常需要几周的GPU集群计算，严重限制了超分辨率技术在科学模拟中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到科学模拟中高分辨率数据获取困难和成本高昂的问题，意识到现有超分辨率方法因依赖大量训练数据而应用受限。他们借鉴了对比学习和扩散模型两种技术：对比学习用于学习高分辨率和低分辨率数据之间的退化模式，扩散模型用于捕获精细细节。作者设计了一个两阶段流程：预训练阶段使用历史模拟数据训练对比编码模块和扩散超分辨率模块，共同学习退化模式和特征；微调阶段冻结对比编码模块，只使用少量高分辨率样本微调扩散超分辨率模块。此外，作者还借鉴了局部注意力机制来减少传统扩散模型在高维3D数据上的计算负担，并使用对抗训练提高模型性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将高分辨率和低分辨率数据之间的退化过程建模为对比学习任务，从历史数据中提取判别性退化特征，同时使用扩散模型进行超分辨率处理，通过局部注意力机制减少计算成本，并利用预训练学习到的通用退化模式，使模型只需少量高分辨率时间步就能重建所有后续低分辨率时间步。整体实现流程分为两个阶段：1)预训练阶段：使用历史模拟数据同时训练对比编码模块和扩散超分辨率模块，前者通过对比高分辨率、低分辨率和超分辨率数据学习退化模式，后者使用对抗训练捕获精细细节并通过局部注意力机制减少计算成本；2)微调阶段：冻结对比编码模块保留已学习的先验知识，使用少量高分辨率样本微调扩散超分辨率模块，弥补新数据集中缺失的高频细节，实现最小高分辨率数据下的精确超分辨率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将高分辨率和低分辨率数据之间的退化过程显式建模为对比学习任务，从历史数据中提取判别性退化特征，实现跨场景的3D超分辨率泛化；2)将局部注意力机制集成到扩散模型中，减轻传统扩散方法的计算和内存负担，同时能恢复高分辨率结构；3)利用预训练学习的通用退化模式，只需新数据集中的少量高分辨率时间步就能重建所有后续低分辨率时间步。相比之前的工作，CD-TVD显著减少了对高分辨率数据的需求，只需少量高分辨率样本而非大量数据；使用扩散模型避免了GAN方法的训练不稳定问题；相比纯CNN方法能更好地捕获复杂退化模式和精细细节；与其他需要多个高分辨率快照的方法不同，CD-TVD只需一个高分辨率时间步；结合了对比学习和扩散模型的优势，效果优于单独使用其中一种方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CD-TVD通过结合对比学习和扩散模型，实现了在只有少量高分辨率时间数据的情况下，高效准确地重建3D科学模拟中的高分辨率时间变化数据，显著降低了科学模拟对计算资源的需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale scientific simulations require significant resources to generatehigh-resolution time-varying data (TVD). While super-resolution is an efficientpost-processing strategy to reduce costs, existing methods rely on a largeamount of HR training data, limiting their applicability to diverse simulationscenarios. To address this constraint, we proposed CD-TVD, a novel frameworkthat combines contrastive learning and an improved diffusion-basedsuper-resolution model to achieve accurate 3D super-resolution from limitedtime-step high-resolution data. During pre-training on historical simulationdata, the contrastive encoder and diffusion superresolution modules learndegradation patterns and detailed features of high-resolution andlow-resolution samples. In the training phase, the improved diffusion modelwith a local attention mechanism is fine-tuned using only one newly generatedhigh-resolution timestep, leveraging the degradation knowledge learned by theencoder. This design minimizes the reliance on large-scale high-resolutiondatasets while maintaining the capability to recover fine-grained details.Experimental results on fluid and atmospheric simulation datasets confirm thatCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking asignificant advancement in data augmentation for large-scale scientificsimulations. The code is available athttps://github.com/Xin-Gao-private/CD-TVD.</description>
      <author>example@mail.com (Chongke Bi, Xin Gao, Jiangkang Deng, Guan)</author>
      <guid isPermaLink="false">2508.08173v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</title>
      <link>http://arxiv.org/abs/2508.08127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了BlindGuard，一种用于保护基于大语言模型的多智能体系统(MAS)的无监督防御方法，能够有效检测恶意智能体而不需要攻击特定标签或先验知识。&lt;h4&gt;背景&lt;/h4&gt;基于大语言模型的多智能体系统面临传播漏洞的安全威胁，恶意智能体可通过智能体间消息交互扭曲集体决策。现有监督防御方法在实际应用中可能不实用，因为它们严重依赖标记的恶意智能体进行训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种实用且可泛化的MAS防御方法，不需要任何攻击特定标签或恶意行为先验知识的无监督防御方法。&lt;h4&gt;方法&lt;/h4&gt;提出BlindGuard，建立分层智能体编码器捕获每个智能体的个体、邻域和全局交互模式；设计基于损坏的检测器，包含定向噪声注入和对比学习，仅使用正常智能体行为进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;BlindGuard能有效检测多种攻击类型（提示注入、记忆中毒和工具攻击），在各种通信模式的MAS中有效工作，且与监督基线相比具有更好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;BlindGuard是一种有效的无监督防御方法，不需要攻击特定标签或恶意行为的先验知识，能够检测多种攻击类型并在不同通信模式的MAS中保持良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;基于大语言模型的多智能体系统(MAS)的安全性受到传播漏洞的严重威胁，恶意智能体可以通过智能体间消息交互扭曲集体决策。虽然现有的监督防御方法显示出有希望的性能，但由于它们严重依赖标记的恶意智能体来训练监督恶意检测模型，可能在现实场景中不实用。为了实现实用且可泛化的MAS防御，在本文中，我们提出了BlindGuard，一种无监督防御方法，无需任何攻击特定标签或恶意行为先验知识即可学习。为此，我们建立了分层智能体编码器来捕获每个智能体的个体、邻域和全局交互模式，为恶意智能体检测提供全面理解。同时，我们设计了基于损坏的检测器，包含定向噪声注入和对比学习，允许仅使用正常智能体行为进行有效的检测模型训练。大量实验表明，BlindGuard能够有效检测具有各种通信模式的MAS中的不同攻击类型（即提示注入、记忆中毒和工具攻击），同时保持比监督基线更好的泛化能力。代码可在以下网址获取：https://github.com/MR9812/BlindGuard。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The security of LLM-based multi-agent systems (MAS) is critically threatenedby propagation vulnerability, where malicious agents can distort collectivedecision-making through inter-agent message interactions. While existingsupervised defense methods demonstrate promising performance, they may beimpractical in real-world scenarios due to their heavy reliance on labeledmalicious agents to train a supervised malicious detection model. To enablepractical and generalizable MAS defenses, in this paper, we propose BlindGuard,an unsupervised defense method that learns without requiring anyattack-specific labels or prior knowledge of malicious behaviors. To this end,we establish a hierarchical agent encoder to capture individual, neighborhood,and global interaction patterns of each agent, providing a comprehensiveunderstanding for malicious agent detection. Meanwhile, we design acorruption-guided detector that consists of directional noise injection andcontrastive learning, allowing effective detection model training solely onnormal agent behaviors. Extensive experiments show that BlindGuard effectivelydetects diverse attack types (i.e., prompt injection, memory poisoning, andtool attack) across MAS with various communication patterns while maintainingsuperior generalizability compared to supervised baselines. The code isavailable at: https://github.com/MR9812/BlindGuard.</description>
      <author>example@mail.com (Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang)</author>
      <guid isPermaLink="false">2508.08127v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Selective Contrastive Learning for Weakly Supervised Affordance Grounding</title>
      <link>http://arxiv.org/abs/2508.07877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进的弱监督功能定位方法，通过选择性原型和像素对比目标解决现有模型过度依赖分类而非功能相关部分的问题，实现了在部分和对象级别自适应学习功能相关线索。&lt;h4&gt;背景&lt;/h4&gt;弱监督功能定位(WSAG)试图模仿人类从第三方演示中学习的方式，无需像素级标注即可理解功能部分。现有方法通常使用跨图像共享分类器和部分发现过程的蒸馏策略，但由于功能相关部分不易区分，模型往往关注与功能无关的常见类别特定模式。&lt;h4&gt;目的&lt;/h4&gt;解决现有WSAG方法过度依赖分类而非功能相关部分的问题，通过引入新型学习目标在不同粒度信息下自适应地学习功能相关线索。&lt;h4&gt;方法&lt;/h4&gt;首先利用CLIP在第一人称和第三人称图像中找到与动作相关的对象；然后通过交叉参考互补视角中发现的物体，挖掘每个视角中的精确部分级功能线索；最后通过持续学习区分功能相关区域与功能无关的背景上下文，使激活从无关区域转向有意义的功能线索。&lt;h4&gt;主要发现&lt;/h4&gt;现有WSAG方法主要依赖分类，关注与功能无关的常见类别特定模式；通过选择性原型和像素对比目标，可以在部分和对象级别自适应地学习功能相关线索；通过交叉参考互补视角，可以挖掘精确的部分级功能线索。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明提出的方法是有效的，代码已在github.com/hynnsk/SelectiveCL上公开。&lt;h4&gt;翻译&lt;/h4&gt;促进实体与物体的交互需要准确识别能够支持特定动作的部分。弱监督功能定位(WASG)试图模仿人类从第三方演示中学习的方式，人类无需像素级标注就能直观理解功能部分。为此，通常使用跨图像的共享分类器和结合部分发现过程的蒸馏策略来进行定位学习。然而，由于功能相关部分并不总是容易区分，模型主要依赖分类，往往关注与功能无关的常见类别特定模式。为解决这一局限，我们通过引入选择性原型和像素对比目标，超越了孤立的部分级学习，这些目标能够根据可用信息的粒度，在部分和对象级别自适应地学习功能相关线索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facilitating an entity's interaction with objects requires accuratelyidentifying parts that afford specific actions. Weakly supervised affordancegrounding (WSAG) seeks to imitate human learning from third-persondemonstrations, where humans intuitively grasp functional parts without needingpixel-level annotations. To achieve this, grounding is typically learned usinga shared classifier across images from different perspectives, along withdistillation strategies incorporating part discovery process. However, sinceaffordance-relevant parts are not always easily distinguishable, modelsprimarily rely on classification, often focusing on common class-specificpatterns that are unrelated to affordance. To address this limitation, we movebeyond isolated part-level learning by introducing selective prototypical andpixel contrastive objectives that adaptively learn affordance-relevant cues atboth the part and object levels, depending on the granularity of the availableinformation. Initially, we find the action-associated objects in bothegocentric (object-focused) and exocentric (third-person example) images byleveraging CLIP. Then, by cross-referencing the discovered objects ofcomplementary views, we excavate the precise part-level affordance clues ineach perspective. By consistently learning to distinguish affordance-relevantregions from affordance-irrelevant background context, our approach effectivelyshifts activation from irrelevant areas toward meaningful affordance cues.Experimental results demonstrate the effectiveness of our method. Codes areavailable at github.com/hynnsk/SelectiveCL.</description>
      <author>example@mail.com (WonJun Moon, Hyun Seok Seong, Jae-Pil Heo)</author>
      <guid isPermaLink="false">2508.07877v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.07788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ALDEN是一种创新的解剖学感知低剂量CT去噪方法，通过结合预训练视觉模型的语义特征、对抗学习和对比学习，有效解决了现有方法忽略解剖学语义的问题，在去噪的同时保留解剖结构，减少过度平滑问题。&lt;h4&gt;背景&lt;/h4&gt;低剂量CT(LDCT)旨在减少辐射暴露并提高诊断效果，已有许多基于深度学习的去噪方法被开发出来，但大多数现有方法忽略了人体组织的解剖学语义，可能导致次优的去噪效果。&lt;h4&gt;目的&lt;/h4&gt;解决现有LDCT去噪方法忽略解剖学语义的问题，提出一种能够保留解剖学语义的LDCT去噪方法。&lt;h4&gt;方法&lt;/h4&gt;提出ALDEN方法，集成了预训练视觉模型(PVMs)的语义特征，并结合对抗学习和对比学习；引入解剖学感知的判别器，通过交叉注意力机制从参考正常剂量CT动态融合分层语义特征；提出语义引导的对比学习模块，通过对比来自LDCT、去噪CT和NDCT的PVM衍生特征来强制解剖一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在两个LDCT去噪数据集上的实验表明，ALDEN达到了最先进的性能，提供了解剖学保存的优越性，显著减少了先前工作中的过度平滑问题；在包含117个解剖结构的下游多器官分割任务上的验证证实了模型保持解剖意识的能力。&lt;h4&gt;结论&lt;/h4&gt;ALDEN是一种有效的LDCT去噪方法，能够在去噪的同时保留解剖学语义，在多个任务上表现优越，证明了其在临床应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;为了减少辐射暴露并提高低剂量计算机断层扫描的诊断效果，已经开发了许多基于深度学习的去噪方法来减轻噪声和伪影。然而，这些方法中的大多数忽略了人体组织的解剖学语义，这可能导致次优的去噪效果。为了解决这个问题，我们提出了ALDEN，一种具有解剖学意识的LDCT去噪方法，它将预训练视觉模型的语义特征与对抗学习和对比学习相结合。具体来说，我们引入了一个具有解剖学意识的判别器，通过交叉注意力机制从参考正常剂量CT动态融合分层语义特征，使判别器能够进行组织特定的真实性评估。此外，我们提出了一个语义引导的对比学习模块，通过对比来自LDCT、去噪CT和NDCT的PVM衍生特征来强制解剖一致性，通过正对保留组织特定模式，并通过双重负对抑制伪影。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决低剂量CT（LDCT）图像去噪的问题。低剂量CT虽然能减少患者辐射暴露，但会产生更多噪声和伪影，降低图像质量，影响医生诊断。传统去噪方法往往忽略人体组织的解剖语义信息，导致过度平滑，掩盖小组织和细微病变特征。解决这个问题对提高诊断准确性、保护患者健康（特别是需要频繁CT检查的患者）以及改善下游任务（如器官分割）的表现都具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有LDCT去噪方法的局限性，特别是它们忽略了解剖语义信息。他们意识到需要从像素级约束转向细粒度的解剖感知去噪。考虑到传统分割网络需要大量精确的解剖标注，他们发现预训练视觉模型（PVMs）具有强大的语义理解能力，可以在不需要明确监督的情况下生成语义特征。该方法借鉴了GAN框架、对抗学习、对比学习以及注意力机制等现有技术，但创新性地将它们与PVMs结合，专门针对医学图像去噪任务进行了优化设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将预训练视觉模型（PVMs）与对抗学习和对比学习相结合，实现解剖感知的低剂量CT去噪。整体流程包括：1) 使用生成器网络（ESAU-Net）将低剂量CT作为输入，输出去噪后的CT图像；2) 通过解剖感知判别器（AAD）融合参考正常剂量CT的语义特征，实现细粒度的纹理恢复；3) 利用语义引导对比学习（SCL）模块，通过正样本对对齐去噪图像与正常剂量CT的解剖特征，同时使用双负样本对抑制噪声和纠正解剖错位；4) 结合像素保真度损失、对抗损失和对比学习损失进行模型优化。最终输出高质量的去噪CT图像，在减少噪声的同时保留重要解剖结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次将预训练视觉模型（PVMs）整合到LDCT去噪中；2) 提出解剖感知判别器（AAD），通过交叉注意力机制动态融合层次化语义特征；3) 设计语义引导对比学习（SCL）模块，采用双负样本策略同时处理噪声和解剖错位问题。相比之前的工作，ALDEN最大的不同是明确利用解剖语义信息指导去噪过程，而传统方法通常不考虑这一点。此外，ALDEN能够平衡保真度指标和感知质量，避免过度平滑问题，并在下游任务中表现出更好的解剖感知能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALDEN通过整合预训练视觉模型与对抗学习和对比学习，实现了对低剂量CT图像的解剖感知去噪，显著提高了图像质量并保留了重要的解剖结构细节。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To reduce radiation exposure and improve the diagnostic efficacy of low-dosecomputed tomography (LDCT), numerous deep learning-based denoising methods havebeen developed to mitigate noise and artifacts. However, most of theseapproaches ignore the anatomical semantics of human tissues, which maypotentially result in suboptimal denoising outcomes. To address this problem,we propose ALDEN, an anatomy-aware LDCT denoising method that integratessemantic features of pretrained vision models (PVMs) with adversarial andcontrastive learning. Specifically, we introduce an anatomy-aware discriminatorthat dynamically fuses hierarchical semantic features from referencenormal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specificrealism evaluation in the discriminator. In addition, we propose asemantic-guided contrastive learning module that enforces anatomicalconsistency by contrasting PVM-derived features from LDCT, denoised CT andNDCT, preserving tissue-specific patterns through positive pairs andsuppressing artifacts via dual negative pairs. Extensive experiments conductedon two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-artperformance, offering superior anatomy preservation and substantially reducingover-smoothing issue of previous work. Further validation on a downstreammulti-organ segmentation task (encompassing 117 anatomical structures) affirmsthe model's ability to maintain anatomical awareness.</description>
      <author>example@mail.com (Runze Wang, Zeli Chen, Zhiyun Song, Wei Fang, Jiajin Zhang, Danyang Tu, Yuxing Tang, Minfeng Xu, Xianghua Ye, Le Lu, Dakai Jin)</author>
      <guid isPermaLink="false">2508.07788v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion</title>
      <link>http://arxiv.org/abs/2508.07755v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025 workshop (AI4CC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为对比反转(Contrastive Inversion)的新方法，通过比较输入图像而不依赖额外信息来识别共同概念，实现了在概念表示和编辑方面的平衡高性能。&lt;h4&gt;背景&lt;/h4&gt;当前对定制化图像生成的需求增加，需要从小图像集中有效提取共同概念的技术。现有方法通常依赖文本提示或空间掩码等额外指导，但手动提供的指导可能导致辅助特征分离不完整，降低生成质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖额外信息的方法来识别图像集中的共同概念，通过比较输入图像来提取共同目标概念，提高概念表示和编辑的性能。&lt;h4&gt;方法&lt;/h4&gt;提出对比反转方法，通过对比学习训练目标标记和图像特定的辅助文本标记，提取良好解耦的目标真实语义；然后应用解耦的交叉注意力微调来提高概念保真度，避免过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果和分析表明，该方法在概念表示和编辑方面实现了平衡的高性能，优于现有技术。&lt;h4&gt;结论&lt;/h4&gt;对比反转方法能够有效提取图像集中的共同概念，不需要额外的指导信息，在概念表示和编辑方面表现优异。&lt;h4&gt;翻译&lt;/h4&gt;最近对定制化图像生成的需求增加，需要从小图像集中有效提取共同概念的技术。现有方法通常依赖额外的指导，如文本提示或空间掩码，来捕获共同目标概念。不幸的是，依赖手动提供的指导可能导致辅助特征分离不完整，从而降低生成质量。在本文中，我们提出了对比反转，一种新颖的方法，通过比较输入图像而不依赖额外信息来识别共同概念。我们通过对比学习训练目标标记以及图像特定的辅助文本标记，提取良好解耦的目标真实语义。然后我们应用解耦的交叉注意力微调来提高概念保真度而不过拟合。实验结果和分析证明，我们的方法在概念表示和编辑方面实现了平衡的高性能，优于现有技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent demand for customized image generation raises a need fortechniques that effectively extract the common concept from small sets ofimages. Existing methods typically rely on additional guidance, such as textprompts or spatial masks, to capture the common target concept. Unfortunately,relying on manually provided guidance can lead to incomplete separation ofauxiliary features, which degrades generation quality.In this paper, we proposeContrastive Inversion, a novel approach that identifies the common concept bycomparing the input images without relying on additional information. We trainthe target token along with the image-wise auxiliary text tokens viacontrastive learning, which extracts the well-disentangled true semantics ofthe target. Then we apply disentangled cross-attention fine-tuning to improveconcept fidelity without overfitting. Experimental results and analysisdemonstrate that our method achieves a balanced, high-level performance in bothconcept representation and editing, outperforming existing techniques.</description>
      <author>example@mail.com (Minseo Kim, Minchan Kwon, Dongyeun Lee, Yunho Jeon, Junmo Kim)</author>
      <guid isPermaLink="false">2508.07755v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Symmetry-Aware Transformer Training for Automated Planning</title>
      <link>http://arxiv.org/abs/2508.07743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对称性感知的transformer训练方法，解决了transformer在自动化规划领域应用中的外推问题，特别是在处理从简单到困难规划任务时的局限性。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型在许多领域表现出色，但在自动化规划领域的应用有限。先前的工作如PlanGPT（一种最先进的仅解码器transformer）在从简单到困难的规划问题外推方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比学习目标，使transformer具有对称性感知能力，从而弥补其归纳偏置的不足；结合架构改进，展示transformer可以针对规划生成或启发式预测进行高效训练。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的对比学习目标，使transformer能够识别和处理规划任务中的对称性问题；结合架构改进，对transformer进行对称性感知训练。&lt;h4&gt;主要发现&lt;/h4&gt;在多个规划领域的结果表明，对称性感知训练有效且高效地解决了PlanGPT的局限性，使transformer能够更好地处理规划任务。&lt;h4&gt;结论&lt;/h4&gt;对称性感知训练使transformer能够有效应对规划任务中的对称性问题，解决了从简单到困难规划任务的外推挑战。&lt;h4&gt;翻译&lt;/h4&gt;尽管Transformer在许多场景中表现出色，它们在自动化规划领域的应用却有限。像PlanGPT这样的先前工作（一种最先进的仅解码器Transformer）在从简单到困难的规划问题外推方面存在困难。这一问题源于问题的对称性：规划任务可以用任意变量名表示，这些变量名除了作为标识符外没有其他含义。这导致了等价表示的组合爆炸，纯Transformer无法从中高效学习。我们提出了一种新颖的对比学习目标，使Transformer具有对称性感知能力，从而弥补其归纳偏置的不足。结合架构改进，我们展示Transformer可以针对规划生成或启发式预测进行高效训练。我们在多个规划领域的结果表明，我们的对称性感知训练有效且高效地解决了PlanGPT的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While transformers excel in many settings, their application in the field ofautomated planning is limited. Prior work like PlanGPT, a state-of-the-artdecoder-only transformer, struggles with extrapolation from easy to hardplanning problems. This in turn stems from problem symmetries: planning taskscan be represented with arbitrary variable names that carry no meaning beyondbeing identifiers. This causes a combinatorial explosion of equivalentrepresentations that pure transformers cannot efficiently learn from. Wepropose a novel contrastive learning objective to make transformerssymmetry-aware and thereby compensate for their lack of inductive bias.Combining this with architectural improvements, we show that transformers canbe efficiently trained for either plan-generation or heuristic-prediction. Ourresults across multiple planning domains demonstrate that our symmetry-awaretraining effectively and efficiently addresses the limitations of PlanGPT.</description>
      <author>example@mail.com (Markus Fritzsche, Elliot Gestrin, Jendrik Seipp)</author>
      <guid isPermaLink="false">2508.07743v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning</title>
      <link>http://arxiv.org/abs/2508.07607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/OPPO-Mente-Lab/X2Edit&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了X2Edit数据集和基于FLUX.1的任务感知MoE-LoRA训练方法，用于改进任意指令图像编辑性能。&lt;h4&gt;背景&lt;/h4&gt;现有的开源数据集在任意指令图像编辑方面表现不佳，同时缺乏与社区主流生成模型兼容的即插即用编辑模块。&lt;h4&gt;目的&lt;/h4&gt;引入覆盖14种不同编辑任务的X2Edit数据集，并设计能够与社区图像生成模型无缝集成的任务感知MoE-LoRA训练方法。&lt;h4&gt;方法&lt;/h4&gt;使用行业领先的综合图像生成模型和专家模型构建数据，利用VLM设计编辑指令并实施评分机制过滤数据；基于FLUX.1设计任务感知的MoE-LoRA训练（仅使用全模型8%参数），并利用扩散模型内部表示引入对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;构建了370万高质量且类别均衡的数据集；模型编辑性能在众多优秀模型中具有竞争力；构建的数据集比现有开源数据集具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;X2Edit项目包括开源代码、检查点和数据集，已在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;现有的任意指令图像编辑开源数据集仍不理想，而与社区主流生成模型兼容的即插即用编辑模块明显缺失。本文首先介绍了X2Edit数据集，这是一个涵盖14种不同编辑任务的综合性数据集，包括主体驱动的生成。我们使用行业领先的综合图像生成模型和专家模型来构建数据。同时，我们利用VLM设计合理的编辑指令，并实施各种评分机制来过滤数据。因此，我们构建了370万高质量且类别均衡的数据。其次，为了更好地与社区图像生成模型无缝集成，我们基于FLUX.1设计了任务感知的MoE-LoRA训练，仅使用全模型8%的参数。为了进一步提高最终性能，我们利用扩散模型的内部表示，基于图像编辑类型定义正/负样本来引入对比学习。大量实验表明，该模型的编辑性能在众多优秀模型中具有竞争力。此外，构建的数据集比现有的开源数据集具有显著优势。X2Edit的开源代码、检查点和数据集可在以下链接找到：https://github.com/OPPO-Mente-Lab/X2Edit。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing open-source datasets for arbitrary-instruction image editing remainsuboptimal, while a plug-and-play editing module compatible withcommunity-prevalent generative models is notably absent. In this paper, wefirst introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverseediting tasks, including subject-driven generation. We utilize theindustry-leading unified image generation models and expert models to constructthe data. Meanwhile, we design reasonable editing instructions with the VLM andimplement various scoring mechanisms to filter the data. As a result, weconstruct 3.7 million high-quality data with balanced categories. Second, tobetter integrate seamlessly with community image generation models, we designtask-aware MoE-LoRA training based on FLUX.1, with only 8\% of the parametersof the full model. To further improve the final performance, we utilize theinternal representations of the diffusion model and define positive/negativesamples based on image editing types to introduce contrastive learning.Extensive experiments demonstrate that the model's editing performance iscompetitive among many excellent models. Additionally, the constructed datasetexhibits substantial advantages over existing open-source datasets. Theopen-source code, checkpoints, and datasets for X2Edit can be found at thefollowing link: https://github.com/OPPO-Mente-Lab/X2Edit.</description>
      <author>example@mail.com (Jian Ma, Xujie Zhu, Zihao Pan, Qirong Peng, Xu Guo, Chen Chen, Haonan Lu)</author>
      <guid isPermaLink="false">2508.07607v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.07539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种处理病理图像中域偏移的方法，专注于整张幻灯片图像(WSIs)内部的偏移，如患者特征和组织厚度，而非医院间的偏移。&lt;h4&gt;背景&lt;/h4&gt;传统方法依赖多医院数据进行域适应，但数据收集的挑战使这种方法不切实际。&lt;h4&gt;目的&lt;/h4&gt;开发一种域泛化方法，能够捕获和利用医院内部的域偏移，解决病理图像分析中的域适应问题。&lt;h4&gt;方法&lt;/h4&gt;通过聚类来自非肿瘤区域的WSI级别特征并将这些聚类视为域，应用对比学习来减少不同聚类WSI对之间的特征差距。提出两阶段对比学习方法，包括WSI级别和补丁级别对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过捕获WSI内部的域偏移特征并应用对比学习，可以有效减少不同域之间的特征差距，提高病理图像分析的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法不需要多医院数据，仅通过医院内部的域偏移信息就能实现有效的域泛化，解决了传统方法中数据收集的挑战。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们通过关注整张幻灯片图像(WSIs)内部的偏移（如患者特征和组织厚度）而非医院之间的偏移，来解决病理图像中的域偏移问题。传统方法依赖多医院数据，但数据收集的挑战常常使这种方法不切实际。因此，所提出的域泛化方法通过聚类来自非肿瘤区域的WSI级别特征并将这些聚类视为域，来捕获和利用医院内部的域偏移。为了减轻域偏移，我们应用对比学习来减少不同聚类WSI对之间的特征差距。所提出的方法引入了两阶段对比学习方法，包括WSI级别和补丁级别对比学习，以有效减少这些差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we address domain shifts in pathological images by focusing onshifts within whole slide images~(WSIs), such as patient characteristics andtissue thickness, rather than shifts between hospitals. Traditional approachesrely on multi-hospital data, but data collection challenges often make thisimpractical. Therefore, the proposed domain generalization method captures andleverages intra-hospital domain shifts by clustering WSI-level features fromnon-tumor regions and treating these clusters as domains. To mitigate domainshift, we apply contrastive learning to reduce feature gaps between WSI pairsfrom different clusters. The proposed method introduces a two-stage contrastivelearning approach WSI-level and patch-level contrastive learning to minimizethese gaps effectively.</description>
      <author>example@mail.com (Yuki Shigeyasu, Shota Harada, Akihiko Yoshizawa, Kazuhiro Terada, Naoki Nakazima, Mariyo Kurata, Hiroyuki Abe, Tetsuo Ushiku, Ryoma Bise)</author>
      <guid isPermaLink="false">2508.07539v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.07205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by COLING2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对谣言检测中的数据稀缺和不平衡问题，提出了基于图监督对比学习的异常检测框架AD-GSCL，并通过大规模数据集验证了其在各种条件下的优越性。&lt;h4&gt;背景&lt;/h4&gt;当前基于传播结构学习的谣言检测方法主要将谣言检测视为有限标记数据上的平衡分类任务，但真实社交媒体数据呈现出不平衡分布，谣言只是海量常规帖子中的少数。&lt;h4&gt;目的&lt;/h4&gt;解决谣言检测中的数据稀缺和不平衡问题，提高真实不平衡数据分布下的谣言检测效果。&lt;h4&gt;方法&lt;/h4&gt;构建了两个来自微博和Twitter的大规模对话数据集，分析了领域分布差异，提出了AD-GSCL框架，启发式地将未标记数据视为非谣言，并适应图对比学习用于谣言检测。&lt;h4&gt;主要发现&lt;/h4&gt;谣言和非谣言分布有明显差异，非谣言多集中在娱乐领域，而谣言则集中在新闻领域，表明谣言检测符合异常检测范式。&lt;h4&gt;结论&lt;/h4&gt;AD-GSCL框架在平衡、不平衡和少样本条件下均表现出优越性，为具有不平衡数据分布的真实世界谣言检测提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;当前基于传播结构学习的谣言检测方法主要将谣言检测视为有限标记数据上的平衡分类任务。然而，真实社交媒体数据呈现出不平衡分布，谣言只是海量常规帖子中的少数。为解决数据稀缺和不平衡问题，我们从微博和Twitter构建了两个大规模对话数据集并分析了领域分布。我们发现谣言和非谣言分布有明显差异，非谣言多集中在娱乐领域而谣言集中在新闻领域，表明谣言检测符合异常检测范式。相应地，我们提出了带图监督对比学习的异常检测框架AD-GSCL。它启发式地将未标记数据视为非谣言并适应图对比学习用于谣言检测。大量实验证明了AD-GSCL在平衡、不平衡和少样本条件下的优越性。我们的发现为具有不平衡数据分布的真实世界谣言检测提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current rumor detection methods based on propagation structure learningpredominately treat rumor detection as a class-balanced classification task onlimited labeled data. However, real-world social media data exhibits animbalanced distribution with a minority of rumors among massive regular posts.To address the data scarcity and imbalance issues, we construct two large-scaleconversation datasets from Weibo and Twitter and analyze the domaindistributions. We find obvious differences between rumor and non-rumordistributions, with non-rumors mostly in entertainment domains while rumorsconcentrate in news, indicating the conformity of rumor detection to an anomalydetection paradigm. Correspondingly, we propose the Anomaly Detection frameworkwith Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treatsunlabeled data as non-rumors and adapts graph contrastive learning for rumordetection. Extensive experiments demonstrate AD-GSCL's superiority underclass-balanced, imbalanced, and few-shot conditions. Our findings providevaluable insights for real-world rumor detection featuring imbalanced datadistributions.</description>
      <author>example@mail.com (Chaoqun Cui, Caiyan Jia)</author>
      <guid isPermaLink="false">2508.07205v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection</title>
      <link>http://arxiv.org/abs/2508.07201v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by AAAI2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为谣言自适应图对比学习(RAGCL)的方法，用于社交媒体谣言检测。通过分析真实数据集，作者发现谣言传播树(RPTs)实际上呈现宽结构而非深结构，大多数节点是浅层1级回复。基于这一发现，RAGCL方法采用基于节点中心性的自适应视图增强，遵循三条原则：免除根节点、保留深层回复节点和保留深层部分的低级节点。通过节点丢弃、属性掩码和边丢弃技术生成视图，利用图对比目标学习鲁棒的谣言表示。实验证明该方法在四个基准数据集上优于现有最先进方法。&lt;h4&gt;背景&lt;/h4&gt;社交媒体上的谣言检测变得越来越重要。大多数现有的基于图的模型假设谣言传播树(RPTs)具有深层结构，并沿着分支学习序列立场特征。&lt;h4&gt;目的&lt;/h4&gt;为了专注于密集子结构的学习，提出谣言自适应图对比学习(RAGCL)方法，通过基于节点中心性的自适应视图增强来提高谣言检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出RAGCL方法，包含三条RPT增强原则：1)免除根节点，2)保留深层回复节点，3)保留深层部分的低级节点。使用基于中心性重要性分数的概率进行节点丢弃、属性掩码和边丢弃来生成视图，并通过图对比目标学习鲁棒的谣言表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实世界数据集的统计分析发现，谣言传播树(RPTs)实际上呈现宽结构，大多数节点是浅层1级回复，而非传统模型假设的深层结构。&lt;h4&gt;结论&lt;/h4&gt;作者的工作揭示了RPTs的宽结构特性，并通过原则性的自适应增强为谣言检测贡献了一种有效的图对比学习方法。所提出的原则和增强技术可能有益于涉及树状图的其他应用。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体上的谣言检测变得越来越重要。大多数现有的基于图的模型假设谣言传播树(RPTs)具有深层结构，并沿着分支学习序列立场特征。然而，通过对真实世界数据集的统计分析，我们发现RPTs呈现宽结构，大多数节点是浅层1级回复。为了专注于密集子结构的学习，我们提出了谣言自适应图对比学习(RAGCL)方法，该方法由基于节点中心性的自适应视图增强引导。我们总结了RPT增强的三条原则：1)免除根节点，2)保留深层回复节点，3)保留深层部分的低级节点。我们使用基于中心性重要性分数的概率进行节点丢弃、属性掩码和边丢弃来生成视图。然后，图对比目标学习鲁棒的谣言表示。在四个基准数据集上的广泛实验表明，RAGCL优于最先进的方法。我们的工作揭示了RPTs的宽结构特性，并通过原则性的自适应增强为谣言检测贡献了一种有效的图对比学习方法。所提出的原则和增强技术可能有益于涉及树状图的其他应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v38i1.27757&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rumor detection on social media has become increasingly important. Mostexisting graph-based models presume rumor propagation trees (RPTs) have deepstructures and learn sequential stance features along branches. However,through statistical analysis on real-world datasets, we find RPTs exhibit widestructures, with most nodes being shallow 1-level replies. To focus learning onintensive substructures, we propose Rumor Adaptive Graph Contrastive Learning(RAGCL) method with adaptive view augmentation guided by node centralities. Wesummarize three principles for RPT augmentation: 1) exempt root nodes, 2)retain deep reply nodes, 3) preserve lower-level nodes in deep sections. Weemploy node dropping, attribute masking and edge dropping with probabilitiesfrom centrality-based importance scores to generate views. A graph contrastiveobjective then learns robust rumor representations. Extensive experiments onfour benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods.Our work reveals the wide-structure nature of RPTs and contributes an effectivegraph contrastive learning approach tailored for rumor detection throughprincipled adaptive augmentation. The proposed principles and augmentationtechniques can potentially benefit other applications involving tree-structuredgraphs.</description>
      <author>example@mail.com (Chaoqun Cui, Caiyan Jia)</author>
      <guid isPermaLink="false">2508.07201v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations</title>
      <link>http://arxiv.org/abs/2508.07016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于时间滞后交叉相关性的序列预测框架（TLCCSP），通过整合时间滞后交叉相关序列提高预测准确性。该框架使用序列移动动态时间规整（SSDTW）算法捕捉滞后相关性，并采用基于对比学习的编码器（CLE）近似SSDTW距离。实验证明该方法在多个数据集上显著降低了预测误差，同时大幅减少了计算时间。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在天气、金融和房地产等领域至关重要，准确的预测支持决策制定和风险缓解。虽然最近的深度学习模型提高了预测能力，但它们常常忽略了相关序列之间的时间滞后交叉相关性，这对于捕捉复杂的时间关系至关重要。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型忽略时间滞后交叉相关性的问题，作者提出了TLCCSP框架，通过有效整合时间滞后交叉相关序列来提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;TLCCSP框架采用序列移动动态时间规整（SSDTW）算法来捕捉滞后相关性，并使用基于对比学习的编码器（CLE）来有效近似SSDTW距离。&lt;h4&gt;主要发现&lt;/h4&gt;在天气数据集上，SSDTW降低MSE 16.01%，CLE进一步降低17.88%；在股票数据集上，SSDTW降低MSE 9.95%，CLE降低6.13%；在房地产数据集上，SSDTW降低MSE 21.29%，CLE降低8.62%。此外，对比学习方法将SSDTW计算时间减少约99%，提高了框架的扩展性和实时适用性。&lt;h4&gt;结论&lt;/h4&gt;TLCCSP框架在天气、金融和房地产时间序列数据集上的实验结果证明了其有效性，能够显著提高预测准确性并减少计算时间。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测在天气、金融和房地产预测等各个领域至关重要，因为准确的预测支持明智的决策制定和风险缓解。虽然最近的深度学习模型提高了预测能力，但它们常常忽略了相关序列之间时间滞后交叉相关性，这对于捕捉复杂的时间关系至关重要。为此，我们提出了基于时间滞后交叉相关性的序列预测框架（TLCCSP），通过有效整合时间滞后交叉相关序列来提高预测准确性。TLCCSP采用序列移动动态时间规整（SSDTW）算法来捕捉滞后相关性，并使用基于对比学习的编码器来有效近似SSDTW距离。在天气、金融和房地产时间序列数据集上的实验结果证明了我们框架的有效性。在天气数据集上，SSDTW与单序列方法相比均方误差降低了16.01%，而对比学习编码器进一步将均方误差降低了17.88%。在股票数据集上，SSDTW实现了9.95%的均方误差降低，对比学习编码器将其降低了6.13%。对于房地产数据集，SSDTW和对比学习编码器分别将均方误差降低了21.29%和8.62%。此外，对比学习方法将SSDTW计算时间减少了约99%，确保了跨多个时间序列预测任务的扩展性和实时适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting is critical across various domains, such as weather,finance and real estate forecasting, as accurate forecasts support informeddecision-making and risk mitigation. While recent deep learning models haveimproved predictive capabilities, they often overlook time-laggedcross-correlations between related sequences, which are crucial for capturingcomplex temporal relationships. To address this, we propose the Time-LaggedCross-Correlations-based Sequence Prediction framework (TLCCSP), which enhancesforecasting accuracy by effectively integrating time-lagged cross-correlatedsequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)algorithm to capture lagged correlations and a contrastive learning-basedencoder to efficiently approximate SSDTW distances.  Experimental results on weather, finance and real estate time series datasetsdemonstrate the effectiveness of our framework. On the weather dataset, SSDTWreduces mean squared error (MSE) by 16.01% compared with single-sequencemethods, while the contrastive learning encoder (CLE) further decreases MSE by17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLEreduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by21.29% and 8.62%, respectively. Additionally, the contrastive learning approachdecreases SSDTW computational time by approximately 99%, ensuring scalabilityand real-time applicability across multiple time series forecasting tasks.</description>
      <author>example@mail.com (Jianfei Wu, Wenmian Yang, Bingning Liu, Weijia Jia)</author>
      <guid isPermaLink="false">2508.07016v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation</title>
      <link>http://arxiv.org/abs/2508.06781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 5 figures, accepted at COLM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BiXSE的简单有效的点式训练方法，用于优化基于大型语言模型生成的分级相关性分数的二进制交叉熵，实现了在密集检索任务上的优异性能。&lt;h4&gt;背景&lt;/h4&gt;神经句子嵌入模型通常使用二元相关性标签，将查询-文档对简单分类为相关或不相关。然而，现实世界中的相关性实际上是一个连续谱系，而非简单的二元分类。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够利用细粒度分级相关性标签的训练方法，以提高密集检索模型的性能，同时减少标注和计算成本。&lt;h4&gt;方法&lt;/h4&gt;BiXSE方法将大型语言模型生成的分级相关性分数解释为概率目标，使得每个查询的单个标记查询-文档对就能提供细粒度的监督。通过利用批次内负样本，BiXSE实现了性能提升并降低了计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试(MMTEB, BEIR, TREC-DL)上，BiXSE始终优于基于softmax的对比学习方法(InfoNCE)，并且在LLM监督数据上训练时，能够匹配或超过强成的对排序基线性能。&lt;h4&gt;结论&lt;/h4&gt;BiXSE为训练密集检索模型提供了一种强大、可扩展的替代方案，随着分级相关性监督变得越来越容易获取，这种方法将变得更加实用。&lt;h4&gt;翻译&lt;/h4&gt;用于密集检索的神经句子嵌入模型通常依赖于二元相关性标签，将查询-文档对视为相关或不相关。然而，现实世界中的相关性往往是一个连续谱系，大型语言模型(LLMs)的最新进展使得生成细粒度分级相关性标签的扩展成为可能。在这项工作中，我们提出了BiXSE，这是一种简单有效的点式训练方法，可优化基于LLM生成的分级相关性分数的二进制交叉熵(BCE)。BiXSE将这些分数解释为概率目标，使得每个查询的单个标记查询-文档对能够提供细粒度的监督。与需要每个查询多个注释比较的成对或列表损失不同，BiXSE通过利用批次内负样本，在减少注释和计算成本的同时实现了强大的性能。在句子嵌入(MMTEB)和检索基准(BEIR, TREC-DL)上的大量实验表明，BiXSE始终优于基于softmax的对比学习(InfoNCE)，并且在LLM监督数据上训练时，匹配或超过了强成的对排序基线。随着分级相关性监督变得越来越容易获取，BiXSE为训练密集检索模型提供了一种强大、可扩展的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural sentence embedding models for dense retrieval typically rely on binaryrelevance labels, treating query-document pairs as either relevant orirrelevant. However, real-world relevance often exists on a continuum, andrecent advances in large language models (LLMs) have made it feasible to scalethe generation of fine-grained graded relevance labels. In this work, wepropose BiXSE, a simple and effective pointwise training method that optimizesbinary cross-entropy (BCE) over LLM-generated graded relevance scores. BiXSEinterprets these scores as probabilistic targets, enabling granular supervisionfrom a single labeled query-document pair per query. Unlike pairwise orlistwise losses that require multiple annotated comparisons per query, BiXSEachieves strong performance with reduced annotation and compute costs byleveraging in-batch negatives. Extensive experiments across sentence embedding(MMTEB) and retrieval benchmarks (BEIR, TREC-DL) show that BiXSE consistentlyoutperforms softmax-based contrastive learning (InfoNCE), and matches orexceeds strong pairwise ranking baselines when trained on LLM-supervised data.BiXSE offers a robust, scalable alternative for training dense retrieval modelsas graded relevance supervision becomes increasingly accessible.</description>
      <author>example@mail.com (Christos Tsirigotis, Vaibhav Adlakha, Joao Monteiro, Aaron Courville, Perouz Taslakian)</author>
      <guid isPermaLink="false">2508.06781v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations</title>
      <link>http://arxiv.org/abs/2508.05097v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为'MultiCheck'的细粒度多模态事实验证统一框架，用于应对多模态虚假信息的挑战&lt;h4&gt;背景&lt;/h4&gt;多模态虚假信息（文本和图像结合的虚假信息）的增长率很高，这对主要依赖文本证据的事实核查系统构成了重大挑战&lt;h4&gt;目的&lt;/h4&gt;设计一个能够对结构化文本和视觉信号进行推理的事实验证框架&lt;h4&gt;方法&lt;/h4&gt;架构结合文本和图像的专用编码器，以及使用元素级交互捕获跨模态关系的融合模块，分类头预测主张真实性，并通过对比学习目标在共享潜在空间中实现主张-证据对的语义对齐&lt;h4&gt;主要发现&lt;/h4&gt;在Factify 2数据集上评估，获得0.84的加权F1分数，显著优于基线&lt;h4&gt;结论&lt;/h4&gt;显式多模态推理有效，该方法在复杂现实场景中具有可扩展和可解释的事实核查潜力&lt;h4&gt;翻译&lt;/h4&gt;多模态虚假信息（主张由文本和图像共同支持）的增长率日益提高，这对主要依赖文本证据的事实核查系统构成了重大挑战。在这项工作中，我们提出了一个名为'MultiCheck'的细粒度多模态事实验证统一框架，旨在对结构化的文本和视觉信号进行推理。我们的架构结合了文本和图像的专用编码器，以及一个使用元素级交互捕获跨模态关系的融合模块。然后，分类头预测主张的真实性，并通过对比学习目标在共享潜在空间中鼓励主张-证据对的语义对齐。我们在Factify 2数据集上评估了我们的方法，获得了0.84的加权F1分数，显著优于基线。这些结果突显了显式多模态推理的有效性，并展示了我们的方法在复杂现实场景中可扩展和可解释的事实核查潜力&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing rate of multimodal misinformation, where claims are supported byboth text and images, poses significant challenges to fact-checking systemsthat rely primarily on textual evidence. In this work, we have proposed aunified framework for fine-grained multimodal fact verification called"MultiCheck", designed to reason over structured textual and visual signals.Our architecture combines dedicated encoders for text and images with a fusionmodule that captures cross-modal relationships using element-wise interactions.A classification head then predicts the veracity of a claim, supported by acontrastive learning objective that encourages semantic alignment betweenclaim-evidence pairs in a shared latent space. We evaluate our approach on theFactify 2 dataset, achieving a weighted F1 score of 0.84, substantiallyoutperforming the baseline. These results highlight the effectiveness ofexplicit multimodal reasoning and demonstrate the potential of our approach forscalable and interpretable fact-checking in complex, real-world scenarios.</description>
      <author>example@mail.com (Aditya Kishore, Gaurav Kumar, Jasabanta Patro)</author>
      <guid isPermaLink="false">2508.05097v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos</title>
      <link>http://arxiv.org/abs/2508.06570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in ACL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个新的视频数据集ImpliHateVid和一个两阶段对比学习框架，用于检测视频中的隐式仇恨言论。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要关注基于文本和图像的仇恨言论检测，而基于视频的方法仍然探索不足。&lt;h4&gt;目的&lt;/h4&gt;创建一个专门用于检测视频中隐式仇恨言论的数据集，并提出一个有效的多模态对比学习框架来检测视频中的仇恨内容。&lt;h4&gt;方法&lt;/h4&gt;1. 创建了包含2009个视频的数据集ImpliHateVid，包括509个隐式仇恨视频、500个显式仇恨视频和1000个非仇恨视频。2. 提出一个两阶段对比学习框架：第一阶段使用对比损失训练音频、文本和图像的模态特定编码器；第二阶段使用对比学习训练交叉编码器来优化多模态表示。3. 整合情感、情绪和基于标题的特征来增强隐式仇恨检测。&lt;h4&gt;主要发现&lt;/h4&gt;提出的多模态对比学习方法在两个数据集（ImpliHateVid和HateMM）上证明了其在检测视频中的仇恨内容方面的有效性，并且他们的数据集具有重要意义。&lt;h4&gt;结论&lt;/h4&gt;论文提出的新数据集和两阶段对比学习框架对视频中的隐式仇恨言论检测有重要贡献。&lt;h4&gt;翻译&lt;/h4&gt;现有研究主要关注基于文本和图像的仇恨言论检测，而基于视频的方法仍然探索不足。在这项工作中，我们引入了一个新的数据集ImpliHateVid，专门为视频中隐式仇恨言论检测而策划。ImpliHateVid包含2009个视频，包括509个隐式仇恨视频、500个显式仇恨视频和1000个非仇恨视频，使其成为首批专门用于隐式仇恨检测的大规模视频数据集之一。我们还提出了一个新颖的两阶段对比学习框架，用于检测视频中的仇恨言论。在第一阶段，我们通过连接三个编码器的特征，使用对比损失训练音频、文本和图像的模态特定编码器。在第二阶段，我们使用对比学习训练交叉编码器，以优化多模态表示。此外，我们整合了情感、情绪和基于标题的特征，以增强隐式仇恨检测。我们在两个数据集上评估了我们的方法：用于隐式仇恨言论检测的ImpliHateVid和用于视频中一般仇恨言论检测的另一个数据集HateMM，证明了提出的多模态对比学习在检测视频中仇恨内容方面的有效性以及我们数据集的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.18653/v1/2025.acl-long.842&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The existing research has primarily focused on text and image-based hatespeech detection, video-based approaches remain underexplored. In this work, weintroduce a novel dataset, ImpliHateVid, specifically curated for implicit hatespeech detection in videos. ImpliHateVid consists of 2,009 videos comprising509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,making it one of the first large-scale video datasets dedicated to implicithate detection. We also propose a novel two-stage contrastive learningframework for hate speech detection in videos. In the first stage, we trainmodality-specific encoders for audio, text, and image using contrastive loss byconcatenating features from the three encoders. In the second stage, we traincross-encoders using contrastive learning to refine multimodal representations.Additionally, we incorporate sentiment, emotion, and caption-based features toenhance implicit hate detection. We evaluate our method on two datasets,ImpliHateVid for implicit hate speech detection and another dataset for generalhate speech detection in videos, HateMM dataset, demonstrating theeffectiveness of the proposed multimodal contrastive learning for hatefulcontent detection in videos and the significance of our dataset.</description>
      <author>example@mail.com (Mohammad Zia Ur Rehman, Anukriti Bhatnagar, Omkar Kabde, Shubhi Bansal, Nagendra Kumar)</author>
      <guid isPermaLink="false">2508.06570v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>SAGOnline: Segment Any Gaussians Online</title>
      <link>http://arxiv.org/abs/2508.08219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了SAGOnline框架，一种轻量级的零样本方法，用于高斯场景的实时3D分割。通过结合视频基础模型和GPU加速的3D掩码生成算法，解决了当前3D分割方法计算成本高、3D空间推理有限以及无法同时跟踪多个对象的问题。&lt;h4&gt;背景&lt;/h4&gt;3D高斯溅射已成为显式3D场景表示的强大范式，但实现高效且一致的3D分割仍然具有挑战性。当前方法存在 prohibitive 计算成本、有限的3D空间推理能力以及无法同时跟踪多个对象的局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个轻量级且零样本的框架，用于高斯场景的实时3D分割，解决现有方法的计算成本高、3D空间推理有限以及无法同时跟踪多个对象的问题。&lt;h4&gt;方法&lt;/h4&gt;SAGOnline框架包含两个关键创新：(1)解耦策略，集成了视频基础模型(如SAM2)用于在合成视图间进行视图一致的2D掩码传播；(2)GPU加速的3D掩码生成和高斯级实例标记算法，为3D基元分配唯一标识符，实现跨视图的无损多目标跟踪和分割。&lt;h4&gt;主要发现&lt;/h4&gt;SAGOnline在NVOS(92.7% mIoU)和Spin-NeRF(95.2% mIoU)基准测试上实现了最先进的性能，推理速度比Feature3DGS、OmniSeg3D-gs和SA3D快15-1500倍(每帧27毫秒)。定性结果表明在复杂场景中具有强大的多目标分割和跟踪能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作实现了实时渲染和3D场景理解，为实际AR/VR和机器人应用铺平了道路。贡献包括：(i)高斯场景中3D分割的轻量级零样本框架；(ii)高斯基元的显式标记，实现同时分割和跟踪；(iii)2D视频基础模型向3D领域的有效适应。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯溅射已成为显式3D场景表示的强大范式，但实现高效且一致的3D分割仍然具有挑战性。当前方法存在 prohibitive 计算成本、有限的3D空间推理能力以及无法同时跟踪多个对象的局限性。我们提出了'在线分割任意高斯'(SAGOnline)，这是一个轻量级的零样本框架，用于高斯场景的实时3D分割，通过两个关键创新解决了这些限制：(1)解耦策略，集成了视频基础模型(如SAM2)用于在合成视图间进行视图一致的2D掩码传播；(2)GPU加速的3D掩码生成和高斯级实例标记算法，为3D基元分配唯一标识符，实现跨视图的无损多目标跟踪和分割。SAGOnline在NVOS(92.7% mIoU)和Spin-NeRF(95.2% mIoU)基准测试上实现了最先进的性能，推理速度比Feature3DGS、OmniSeg3D-gs和SA3D快15-1500倍(每帧27毫秒)。定性结果表明在复杂场景中具有强大的多目标分割和跟踪能力。我们的贡献包括：(i)高斯场景中3D分割的轻量级零样本框架；(ii)高斯基元的显式标记，实现同时分割和跟踪；(iii)2D视频基础模型向3D领域的有效适应。这项工作实现了实时渲染和3D场景理解，为实际AR/VR和机器人应用铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D高斯场景中的高效一致3D分割问题。当前方法面临三个关键挑战：计算成本过高、难以获取全局3D分割结果、无法同时进行多目标分割和跟踪。这个问题在AR/VR应用、机器人操作和自主系统中至关重要，因为这些应用需要实时3D场景解析，而现有方法要么计算代价太大，要么无法处理多目标场景，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前3D分割方法的局限性，包括基于蒸馏、对比学习和直接操作的方法的不足。他们提出将3D分割任务解耦为一致的2D分割和3D高斯级分割两个阶段，利用视频基础模型进行跨视角一致的2D掩码传播，并设计GPU加速的3D掩码生成算法。该方法借鉴了3D高斯溅射的显式表示方法、SAM 2作为视频基础模型、逆光栅化技术以及类似U-Net的掩码细化网络架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D分割解耦为两个互补阶段：利用视频基础模型进行跨视角一致的2D掩码传播，以及通过GPU加速的逆投影投票算法将2D掩码转换为3D高斯标签。整体流程分为两个阶段：第一阶段(预热初始化)使用SAM 2生成跨视角一致的2D掩码，通过逆投影投票为高斯基元分配实例标签；第二阶段(加速分割)利用分割后的3D高斯表示实时渲染新视图，并通过轻量级后处理网络优化分割结果，保持实时性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：轻量级零样本框架实现无需训练的3D分割；显式的高斯基元标记支持同时分割和跟踪；有效将2D视频基础模型适应到3D领域。相比之前工作，SAGOnline计算效率更高(比SA3D快15-1500倍，27ms/帧)，支持多目标处理(为每个高斯基元分配唯一标识符)，能生成全局3D掩码而非仅2D掩码，并实现实时性能(37 FPS)，同时零训练时间而其他方法需要20-40分钟额外训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAGOnline提出了一种轻量级零样本框架，通过解耦2D视频基础模型与3D高斯分割，实现了实时、多视角一致的3D场景分割与多目标跟踪，显著提高了计算效率并支持实际AR/VR和机器人应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit3D scene representation, yet achieving efficient and consistent 3D segmentationremains challenging. Current methods suffer from prohibitive computationalcosts, limited 3D spatial reasoning, and an inability to track multiple objectssimultaneously. We present Segment Any Gaussians Online (SAGOnline), alightweight and zero-shot framework for real-time 3D segmentation in Gaussianscenes that addresses these limitations through two key innovations: (1) adecoupled strategy that integrates video foundation models (e.g., SAM2) forview-consistent 2D mask propagation across synthesized views; and (2) aGPU-accelerated 3D mask generation and Gaussian-level instance labelingalgorithm that assigns unique identifiers to 3D primitives, enabling losslessmulti-object tracking and segmentation across views. SAGOnline achievesstate-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 timesin inference speed (27 ms/frame). Qualitative results demonstrate robustmulti-object segmentation and tracking in complex scenes. Our contributionsinclude: (i) a lightweight and zero-shot framework for 3D segmentation inGaussian scenes, (ii) explicit labeling of Gaussian primitives enablingsimultaneous segmentation and tracking, and (iii) the effective adaptation of2D video foundation models to the 3D domain. This work allows real-timerendering and 3D scene understanding, paving the way for practical AR/VR androbotic applications.</description>
      <author>example@mail.com (Wentao Sun, Quanyun Wu, Hanqing Xu, Kyle Gao, Zhengsen Xu, Yiping Chen, Dedong Zhang, Lingfei Ma, John S. Zelek, Jonathan Li)</author>
      <guid isPermaLink="false">2508.08219v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking</title>
      <link>http://arxiv.org/abs/2508.07968v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Full Research Paper, presented at MICCAI'25 Workshop on Collaborative  Intelligence and Autonomy in Image-guided Surgery&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TrackOR框架，用于手术室中长期多人跟踪和重新识别，利用3D几何特征实现高性能跟踪，支持离线恢复过程，为手术团队提供个性化智能支持。&lt;h4&gt;背景&lt;/h4&gt;为手术团队提供智能支持是自动化手术场景理解的关键前沿，长期目标是改善患者预后。为所有工作人员开发个性化智能需要维持长时间手术中每个人的位置状态，这仍然存在许多计算挑战。&lt;h4&gt;目的&lt;/h4&gt;开发TrackOR框架，以解决手术室中长期多人跟踪和重新识别的问题，从而实现为手术团队提供个性化智能支持。&lt;h4&gt;方法&lt;/h4&gt;TrackOR使用3D几何特征来实现最先进的在线跟踪性能，同时支持有效的离线恢复过程来创建可用于分析的轨迹数据。&lt;h4&gt;主要发现&lt;/h4&gt;通过利用3D几何信息，持久的身份跟踪成为可能，这为手术室中所需的更细致的以人员为中心的分析提供了关键转变，从而能够实现个性化智能系统。&lt;h4&gt;结论&lt;/h4&gt;这项新功能开辟了各种应用，包括提出的时序路径印记，可将原始跟踪数据转化为可操作的见解，以提高团队效率和安全性，最终提供个性化支持。&lt;h4&gt;翻译&lt;/h4&gt;为手术团队提供智能支持是自动化手术场景理解的关键前沿，长期目标是改善患者预后。为所有工作人员开发个性化智能需要维持长时间手术中每个人的位置状态，这仍然存在许多计算挑战。我们提出了TrackOR，一个用于解决手术室中长期多人跟踪和重新识别问题的框架。TrackOR使用3D几何特征实现了最先进的在线跟踪性能（比最强的基线高11%的关联准确率），同时支持有效的离线恢复过程来创建可用于分析的轨迹数据。我们的工作表明，通过利用3D几何信息，持久的身份跟踪成为可能，这为手术室中所需的更细致的以人员为中心的分析提供了关键转变，从而能够实现个性化智能系统。这项新功能开辟了各种应用，包括我们提出的时序路径印记，可将原始跟踪数据转化为可操作的见解，以提高团队效率和安全性，最终提供个性化支持。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决手术室中的长期多人跟踪和再识别问题。这个问题很重要，因为手术是高风险的团队协作过程，而现有方法无法处理医护人员频繁离开和重新进入场景的情况，导致无法进行长期分析。实现个性化智能手术系统需要能够持续跟踪每个医护人员，了解他们的独特习惯和工作模式，从而提高手术安全性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到手术室环境对多目标跟踪的挑战，包括严重遮挡、拥挤环境和医护人员穿着难以区分的制服。他们发现现有方法基本上都是身份不可知的，只能进行短期分析。作者借鉴了现有的多目标跟踪基础方法、3D人体姿态估计技术和点云处理方法，但创新性地提出了使用3D几何签名而非视觉特征来实现身份跟踪，解决了基于外观的方法在手术室环境中失效的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D几何签名来实现手术室中的长期多人跟踪和再识别，不依赖于容易混淆的视觉特征。整体流程包括：1)从多视图RGB数据检测3D人体姿态；2)从分割的3D点云中提取ReID特征；3)通过计算成本矩阵并使用匈牙利算法进行数据关联；4)执行离线全局轨迹恢复来纠正碎片化；5)生成时间路径印记将轨迹转化为可操作的见解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了TrackOR框架，利用3D几何签名实现长期身份跟踪；2)结合3D姿态和ReID进行在线跟踪，单独使用ReID进行离线恢复；3)提出时间路径印记用于长期工作流程分析。相比之前的工作，TrackOR不依赖视觉特征，在医护人员离开和重新进入场景时仍能保持身份一致性，而传统2D跟踪器受限于视野，标准3D跟踪器缺乏长期再识别能力，现有OR方法则基本是身份不可知的。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrackOR通过利用3D几何签名实现了手术室中的长期多人跟踪和再识别，为个性化智能手术系统的发展铺平了道路，使医护人员工作流程的细粒度分析和个性化支持成为可能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Providing intelligent support to surgical teams is a key frontier inautomated surgical scene understanding, with the long-term goal of improvingpatient outcomes. Developing personalized intelligence for all staff membersrequires maintaining a consistent state of who is located where for longsurgical procedures, which still poses numerous computational challenges. Wepropose TrackOR, a framework for tackling long-term multi-person tracking andre-identification in the operating room. TrackOR uses 3D geometric signaturesto achieve state-of-the-art online tracking performance (+11% AssociationAccuracy over the strongest baseline), while also enabling an effective offlinerecovery process to create analysis-ready trajectories. Our work shows that byleveraging 3D geometric information, persistent identity tracking becomesattainable, enabling a critical shift towards the more granular, staff-centricanalyses required for personalized intelligent systems in the operating room.This new capability opens up various applications, including our proposedtemporal pathway imprints that translate raw tracking data into actionableinsights for improving team efficiency and safety and ultimately providingpersonalized support.</description>
      <author>example@mail.com (Tony Danjun Wang, Christian Heiliger, Nassir Navab, Lennart Bastian)</author>
      <guid isPermaLink="false">2508.07968v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts</title>
      <link>http://arxiv.org/abs/2508.07842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages,8 figures. Submitted to AAAI'26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DETACH是一种通过生物启发的双流解耦进行跨领域学习的长时程任务框架，包含环境学习模块和技能学习模块，能够有效解决人类-场景交互中长时程任务的跨领域学习挑战。&lt;h4&gt;背景&lt;/h4&gt;人类-场景交互中的长时程任务是复杂的多步骤任务，需要连续规划、顺序决策和跨领域的扩展执行才能实现最终目标。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法无法推广到新环境和技能组合的问题，实现跨领域的长时程任务学习与执行。&lt;h4&gt;方法&lt;/h4&gt;DETACH受大脑的'位置-内容'双通路机制启发，包含两个核心模块：环境学习模块用于空间理解，捕捉物体功能、空间关系和场景语义，通过完全的环境-自我解耦实现跨领域迁移；技能学习模块用于任务执行，处理包括自由度和运动模式在内的自我状态信息，通过独立的运动模式编码实现跨技能迁移。&lt;h4&gt;主要发现&lt;/h4&gt;在HSI场景中的各种长时程任务上进行了大量实验，与现有方法相比，DETACH可以实现平均子任务成功率提高23%，平均执行效率提高29%。&lt;h4&gt;结论&lt;/h4&gt;DETACH通过双流解耦的方法有效解决了长时程任务在跨领域学习中的挑战，显著提高了任务成功率和执行效率。&lt;h4&gt;翻译&lt;/h4&gt;人类-场景交互(HSI)中的长时程(LH)任务是复杂的多步骤任务，需要连续规划、顺序决策和跨领域的扩展执行才能实现最终目标。然而，现有方法严重依赖于通过连接预训练的子任务来进行技能链式操作，环境观察和自我状态紧密耦合，缺乏推广到新环境和技能组合的能力，无法完成跨领域的各种长时程任务。为了解决这个问题，本文提出了DETACH，一种通过生物启发的双流解耦进行跨领域学习的长时程任务框架。受大脑的'位置-内容'双通路机制启发，DETACH包含两个核心模块：i)环境学习模块用于空间理解，捕捉物体功能、空间关系和场景语义，通过完全的环境-自我解耦实现跨领域迁移；ii)技能学习模块用于任务执行，处理包括自由度和运动模式在内的自我状态信息，通过独立的运动模式编码实现跨技能迁移。我们在HSI场景中的各种长时程任务上进行了大量实验。与现有方法相比，DETACH可以实现平均子任务成功率提高23%，平均执行效率提高29%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决人场景交互(HSI)中长时程(LH)任务在跨领域环境下的泛化问题。现有方法在环境变化时无法有效分离环境与自我状态的影响，遇到新技能时即使运动模式相似也需重新训练整个网络。这个问题在现实中非常重要，因为机器人、医疗干预和智能家居系统等应用需要在不同环境中灵活适应，现有方法的性能差距严重阻碍了这些系统在现实世界中的部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受神经科学中'背侧-腹侧流'假说启发，该假说认为大脑有专门处理物体识别的'what'通路和处理空间运动控制的'where-how'通路。作者借鉴了这一双通路机制，但将其应用于功能解纠缠而非视觉模态分离。设计上采用了双流架构：环境编码器处理空间信息，自我编码器处理身体状态信息，并设计了多策略自适应融合机制和渐进式训练协议。作者也参考了现有的解纠缠学习方法，但这些方法主要关注静态因子分离，不适合动态交互和生成式适应的需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦环境感知和自我状态表示，实现长时程任务在跨领域环境下的有效泛化。整体流程包括：1)观察空间重构模型，将统一观察空间分离为环境感知和自我状态；2)解纠缠双编码器，环境编码器用多尺度CNN处理空间信息，自我编码器用双向LSTM处理身体状态；3)多策略自适应融合机制，结合交叉注意力、门控融合和专家混合三种策略；4)渐进式训练协议，包括独立预训练、融合层优化和端到端联合优化三个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DETACH解纠缠架构，第一个基于生物认知原则的HSI具身AI控制框架；2)专门的双流编码器，分别增强环境迁移能力和技能重用能力；3)多策略自适应融合机制，根据任务特性动态选择最优融合策略；4)渐进式训练协议，确保各模块专门特性。相比之前工作，DETACH实现了功能解纠缠而非模态分离，同时解决了环境迁移和技能迁移双重挑战，使用多策略自适应融合而非固定融合策略，并通过渐进式训练避免灾难性遗忘。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DETACH提出了一种受生物启发的双流解纠缠框架，通过分离环境感知和自我状态表示，实现了长时程任务在跨领域环境下的高效泛化和技能重用，显著提升了人场景交互系统的适应性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complexmulti-step tasks that require continuous planning, sequential decision-making,and extended execution across domains to achieve the final goal. However,existing methods heavily rely on skill chaining by concatenating pre-trainedsubtasks, with environment observations and self-state tightly coupled, lackingthe ability to generalize to new combinations of environments and skills,failing to complete various LH tasks across domains. To solve this problem,this paper presents DETACH, a cross-domain learning framework for LH tasks viabiologically inspired dual-stream disentanglement. Inspired by the brain's"where-what" dual pathway mechanism, DETACH comprises two core modules: i) anenvironment learning module for spatial understanding, which captures objectfunctions, spatial relationships, and scene semantics, achieving cross-domaintransfer through complete environment-self disentanglement; ii) a skilllearning module for task execution, which processes self-state informationincluding joint degrees of freedom and motor patterns, enabling cross-skilltransfer through independent motor pattern encoding. We conducted extensiveexperiments on various LH tasks in HSI scenes. Compared with existing methods,DETACH can achieve an average subtasks success rate improvement of 23% andaverage execution efficiency improvement of 29%.</description>
      <author>example@mail.com (Yutong Shen, Hangxu Liu, Penghui Liu, Ruizhe Xia, Tianyi Yao, Yitong Sun, Tongtong Feng)</author>
      <guid isPermaLink="false">2508.07842v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models</title>
      <link>http://arxiv.org/abs/2508.07714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种半自动化流程，结合先进的目标检测器和大型语言模型，以最小化人工努力构建多类门检测数据集，解决了相关公开数据集稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;在建筑平面图中准确检测和分类不同类型的门对于建筑合规检查和室内场景理解等多个应用至关重要。然而，专门用于细粒度多类门检测的公开数据集仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;提出一个半自动化流程，利用最先进的对象检测器和大型语言模型以最少的人工努力构建多类门检测数据集。&lt;h4&gt;方法&lt;/h4&gt;首先使用深度目标检测模型将门作为统一类别进行检测；接着，大型语言模型基于每个检测到的实例的视觉和上下文特征对其进行分类；最后，人机交互阶段确保高质量的标签和边界框。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著降低了标注成本，同时产生适合用于分析平面图的神经网络模型基准测试的数据集。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了结合深度学习和多模态推理在复杂现实领域高效构建数据集的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在建筑平面图中准确检测和分类不同类型的门对于多个应用（如建筑合规检查和室内场景理解）至关重要。尽管如此，专门为细粒度多类门检测设计的公开可用数据集仍然稀少。在这项工作中，我们提出了一个半自动化流程，利用最先进的对象检测器和大型语言模型，以最小化人工努力构建多类门检测数据集。门首先使用深度目标检测模型作为统一类别被检测出来。接着，大型语言模型根据每个检测到的实例的视觉和上下文特征对其进行分类。最后，人机交互阶段确保高质量的标签和边界框。我们的方法显著降低了标注成本，同时产生了一个适合用于平面图分析中神经网络模型基准测试的数据集。这项工作展示了结合深度学习和多模态推理在复杂现实领域高效构建数据集的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate detection and classification of diverse door types in floor plansdrawings is critical for multiple applications, such as building compliancechecking, and indoor scene understanding. Despite their importance, publiclyavailable datasets specifically designed for fine-grained multi-class doordetection remain scarce. In this work, we present a semi-automated pipelinethat leverages a state-of-the-art object detector and a large language model(LLM) to construct a multi-class door detection dataset with minimal manualeffort. Doors are first detected as a unified category using a deep objectdetection model. Next, an LLM classifies each detected instance based on itsvisual and contextual features. Finally, a human-in-the-loop stage ensureshigh-quality labels and bounding boxes. Our method significantly reducesannotation cost while producing a dataset suitable for benchmarking neuralmodels in floor plan analysis. This work demonstrates the potential ofcombining deep learning and multimodal reasoning for efficient datasetconstruction in complex real-world domains.</description>
      <author>example@mail.com (Licheng Zhang, Bach Le, Naveed Akhtar, Tuan Ngo)</author>
      <guid isPermaLink="false">2508.07714v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images</title>
      <link>http://arxiv.org/abs/2508.06546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted in ICCV 25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用多视图RGB图像进行3D语义场景图估计的新方法，通过改进特征聚合和利用相邻节点信息，克服了从预测深度图重建的伪点几何噪声问题，并减少了多视图图像特征中的背景噪声。&lt;h4&gt;背景&lt;/h4&gt;现代3D语义场景图估计方法通常依赖于真实的3D标注数据来准确预测目标对象、谓词和关系。然而，在没有给定3D真实表示的情况下，仅使用多视图RGB图像进行场景图估计是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅使用多视图RGB图像的3D语义场景图估计方法，克服从预测深度图重建的伪点几何噪声问题，减少多视图图像特征中的背景噪声，提高场景图估计的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 获取语义掩码来指导特征聚合，过滤背景特征；2. 设计一种新颖的方法来合并相邻节点信息，增强场景图估计的鲁棒性；3. 利用从训练统计数据中计算出的显式统计先验，基于节点的单跳邻域来改进节点和边预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法优于当前仅使用多视图图像作为初始输入的现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合语义掩码引导的特征聚合和相邻节点信息，以及利用统计先验进行预测优化，可以有效地在没有3D真实标注的情况下进行准确的3D语义场景图估计。&lt;h4&gt;翻译&lt;/h4&gt;现代3D语义场景图估计方法利用真实的3D标注来准确预测目标对象、谓词和关系。在没有给定3D真实表示的情况下，我们探索仅利用多视图RGB图像来解决这一任务。为了获得准确的场景图估计所需的鲁棒特征，我们必须克服从预测深度图重建的噪声伪点几何，并减少多视图图像特征中存在的背景噪声。关键是通过相邻关系来丰富节点和边特征，加入准确的语义和空间信息。我们获取语义掩码来指导特征聚合以过滤背景特征，并设计了一种新颖的方法来合并相邻节点信息，以增强我们场景图估计的鲁棒性。此外，我们利用从训练统计数据计算出的显式统计先验，基于它们的单跳邻域来改进节点和边预测。我们的实验表明，我们的方法优于当前仅使用多视图图像作为初始输入的方法。我们的项目页面可在 https://qixun1.github.io/projects/SCRSSG 获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在没有真实3D标注的情况下，仅使用多视角RGB图像生成3D语义场景图的问题。这个问题很重要，因为3D场景图是增强高级场景理解的关键表示，在图像描述、检索、编辑和医疗应用中扮演重要角色。现有方法大多依赖昂贵的3D LiDAR点云，而仅使用RGB图像的方法面临特征噪声和背景干扰的挑战，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法依赖边界框提案会导致背景噪声和部分对象特征提取不完整的问题。他们借鉴了Segment Anything Model(SAM)用于生成精确语义掩码，参考了JointSSG的可学习几何门设计，受ResNet残差连接启发引入邻近特征融合机制，并借鉴了2D场景图方法KERNS和Schemata的统计先验思想，但将这些技术整合并创新应用于3D场景图生成任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过掩码特征初始化减少背景干扰，设计残差空间邻居图神经网络增强节点特征，并利用统计先验知识进行置信度重评分。整体流程：1)输入多视角RGB图像；2)使用RGB-D SLAM和深度估计器重建3D点云；3)通过SAM生成语义掩码并初始化节点特征；4)使用RSN-GNN进行消息传递和特征增强；5)通过CR模块基于统计先验细化预测；6)输出最终3D场景图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)掩码特征初始化(MFI)使用语义掩码而非边界框聚合特征，减少背景噪声；2)残差空间邻居图神经网络(RSN-GNN)将高度激活的邻近特征集成到目标节点；3)置信度重评分(CR)模块利用统计先验知识细化预测。相比之前工作，不依赖实体检测器的边界框，能提取完整对象特征；结合视觉和几何线索处理低置信度预测；在处理类别不平衡特别是低尾部类别上表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于统计置信度重评分的创新框架，通过掩码特征初始化、残差空间邻居图神经网络和置信度重评分三个模块，显著提高了仅使用多视角RGB图像进行3D语义场景图生成的鲁棒性和准确性，特别是在处理低置信度和类别不平衡场景时表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern 3D semantic scene graph estimation methods utilize ground truth 3Dannotations to accurately predict target objects, predicates, andrelationships. In the absence of given 3D ground truth representations, weexplore leveraging only multi-view RGB images to tackle this task. To attainrobust features for accurate scene graph estimation, we must overcome the noisyreconstructed pseudo point-based geometry from predicted depth maps and reducethe amount of background noise present in multi-view image features. The key isto enrich node and edge features with accurate semantic and spatial informationand through neighboring relations. We obtain semantic masks to guide featureaggregation to filter background features and design a novel method toincorporate neighboring node information to aid robustness of our scene graphestimates. Furthermore, we leverage on explicit statistical priors calculatedfrom the training summary statistics to refine node and edge predictions basedon their one-hop neighborhood. Our experiments show that our method outperformscurrent methods purely using multi-view images as the initial input. Ourproject page is available at https://qixun1.github.io/projects/SCRSSG.</description>
      <author>example@mail.com (Qi Xun Yeo, Yanyan Li, Gim Hee Lee)</author>
      <guid isPermaLink="false">2508.06546v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.01713v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at MICCAI AMAI 2025 workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种增强的TOPICS+方法，专门针对机器人手术场景的鲁棒分割问题，通过引入Dice损失处理类别不平衡、分层伪标记和定制标签分类法，解决了静态分割模型在动态手术环境中的局限性。&lt;h4&gt;背景&lt;/h4&gt;机器人辅助手术依赖于准确和实时的场景理解来安全引导手术器械，然而在静态数据集上训练的分割模型在部署到动态和不断演变的手术环境时面临关键限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够持续适应新类别且避免灾难性遗忘的分割模型，以应对手术环境中的动态变化。&lt;h4&gt;方法&lt;/h4&gt;基于Taxonomy-Oriented Poincaré-regularized Incremental Class Segmentation (TOPICS)方法提出增强版本TOPICS+，在分层损失公式中纳入Dice损失处理强类别不平衡，引入分层伪标记，设计针对机器人手术环境的定制标签分类法，并提出了六个新的CISS基准。&lt;h4&gt;主要发现&lt;/h4&gt;TOPICS+方法在机器人手术场景的分割任务中表现良好，所提出的六个CISS基准能够模拟手术环境中的真实类别增量设置，在Syn-Mediverse合成数据集上包含144多个类别的标签集。&lt;h4&gt;结论&lt;/h4&gt;TOPICS+方法能够有效处理手术环境中的分割挑战，代码和训练模型已公开提供。&lt;h4&gt;翻译&lt;/h4&gt;机器人辅助手术依赖于准确和实时的场景理解来安全引导手术器械。然而，在静态数据集上训练的分割模型在部署到这些动态和不断演变的手术环境时面临关键限制。增量类语义分割(CISS)允许模型持续适应新类别，同时避免对先前知识的灾难性遗忘，无需在先前数据上训练。在这项工作中，我们建立在最近引入的面向分类学的庞加莱正则化增量类分割(TOPICS)方法的基础上，提出了一个增强版本，称为TOPICS+，专门针对手术场景的鲁棒分割。具体而言，我们将Dice损失纳入分层损失公式中以处理强类别不平衡，引入分层伪标记，并设计针对机器人手术环境的定制标签分类法。我们还提出了六个专为机器人手术环境设计的新型CISS基准，包括多个增量步骤和几个语义类别，以模拟手术环境中的真实类别增量设置。此外，我们在Syn-Mediverse合成数据集上引入了一个包含144多个类别的 refined 标签集，在线托管作为评估基准。我们在http://topics.cs.uni-freiburg.de公开提供代码和训练模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人辅助手术中的动态场景理解问题，特别是类增量语义分割(CISS)在手术环境中的应用。这个问题非常重要，因为机器人辅助手术正在快速增长并扩展到多个医疗领域，能够显著减少失血、输血率、住院时间和并发症。手术环境的复杂性和多变性需要系统能够不断适应新的器械、组织和注释协议，而传统方法存在'灾难性遗忘'问题，无法在不重放历史数据的情况下持续学习新知识，这在受隐私法规限制的医疗环境中尤其关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于现有的TOPICS方法进行改进，该方法利用超几何空间表示层次化类别关系来缓解灾难性遗忘。作者借鉴了持续学习原则、超几何空间中的树状结构建模以及知识蒸馏在医学图像分类中的应用。针对手术领域的特殊挑战，作者整合了Dice损失处理类别不平衡，引入层次化伪标记应对多样化背景，并设计了专门的标签分类法。作者还创建了六个针对机器人手术环境的新型CISS基准，扩展了Syn-Mediverse数据集到144多个类别，以更真实地模拟手术环境中的增量学习场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将类别组织成树状结构并在超几何空间中编码层次关系，通过层次化Dice损失处理类别不平衡，使用层次化伪标记保持准确性，并设计定制标签分类法防止遗忘。整体流程包括：1)使用三个手术数据集并创建CISS设置；2)基于DeepLabV3和ResNet-101构建模型；3)在超几何空间中显式编码类层次结构；4)结合层次化Dice损失、稀缺性和关系正则化损失；5)在增量步骤中使用层次化伪标记；6)在六个不同CISS设置中评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出TOPICS+专门针对机器人辅助手术的无重放CISS方法；2)整合Dice损失处理手术场景中的严重类别不平衡；3)引入层次化伪标记应对多样化背景；4)设计定制标签分类法防止遗忘；5)创建六个针对机器人手术环境的新型CISS基准；6)扩展Syn-Mediverse数据集到144多个类别。相比之前工作，本文专门针对手术场景优化，使用更精细的层次化损失和伪标记技术，在更大规模数据集上评估，并创建了更真实的手术环境基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了TOPICS+，一种基于层次化超几何空间表示的无重放类增量语义分割方法，通过集成Dice损失、层次化伪标记和定制标签分类法，有效解决了机器人辅助手术场景中的动态环境理解问题，并在六个新提出的基准测试上展示了卓越的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot-assisted surgeries rely on accurate and real-time scene understandingto safely guide surgical instruments. However, segmentation models trained onstatic datasets face key limitations when deployed in these dynamic andevolving surgical environments. Class-incremental semantic segmentation (CISS)allows models to continually adapt to new classes while avoiding catastrophicforgetting of prior knowledge, without training on previous data. In this work,we build upon the recently introduced Taxonomy-Oriented Poincar\'e-regularizedIncremental Class Segmentation (TOPICS) approach and propose an enhancedvariant, termed TOPICS+, specifically tailored for robust segmentation ofsurgical scenes. Concretely, we incorporate the Dice loss into the hierarchicalloss formulation to handle strong class imbalances, introduce hierarchicalpseudo-labeling, and design tailored label taxonomies for robotic surgeryenvironments. We also propose six novel CISS benchmarks designed for roboticsurgery environments including multiple incremental steps and several semanticcategories to emulate realistic class-incremental settings in surgicalenvironments. In addition, we introduce a refined set of labels with more than144 classes on the Syn-Mediverse synthetic dataset, hosted online as anevaluation benchmark. We make the code and trained models publicly available athttp://topics.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Julia Hindel, Ema Mekic, Enamundram Naga Karthik, Rohit Mohan, Daniele Cattaneo, Maria Kalweit, Abhinav Valada)</author>
      <guid isPermaLink="false">2508.01713v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility</title>
      <link>http://arxiv.org/abs/2508.07989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures, 2 tables. Accepted at CV4A11y, ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了多模态大语言模型作为视障人士辅助技术的潜力及其面临的挑战，特别指出了模型在感知运动方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在为视障和视力受损(BVI)人群提供辅助技术方面展现出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;识别并分析多模态大语言模型在现实应用中存在的一个关键失败模式，提高对其局限性的认识并推动改进。&lt;h4&gt;方法&lt;/h4&gt;作为立场性论文，通过提出'自动扶梯问题'作为典型案例，分析模型在感知连续运动方面的缺陷，而非开发新模型。&lt;h4&gt;主要发现&lt;/h4&gt;发现最先进的模型存在'隐式运动失明'问题，无法感知自动扶梯的行进方向；这源于视频理解中的帧采样范式将视频视为静态图像序列，难以处理连续的低信号运动。&lt;h4&gt;结论&lt;/h4&gt;需要从纯语义识别向稳健的物理感知转变，并开发新的以人为本的基准测试，优先考虑安全性、可靠性和用户在动态环境中的真实需求。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)作为视障和视力受损(BVI)人群的辅助技术具有巨大潜力。然而，我们识别出一种关键的失败模式，它削弱了模型在现实应用中的可信度。我们引入了自动扶梯问题——最先进模型无法感知自动扶梯行进方向——作为我们称为隐式运动失明的更深层次局限性的典型例子。这种失明源于视频理解中的主流帧采样范式，该范式将视频视为静态图像的离散序列，从根本上难以感知连续的低信号运动。作为立场性论文，我们的贡献不是新模型，而是：(I)正式阐述这一盲点，(II)分析其对用户信任的影响，以及(III)发出行动呼吁。我们倡导从纯语义识别向稳健的物理感知转变，并敦促开发新的以人为本的基准测试，优先考虑安全性、可靠性和用户在动态环境中的真实需求。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决AI系统中的'隐式运动盲区'问题，特别是多模态大语言模型无法感知自动扶梯运动方向的现象。这个问题对盲人和视障人士(BVI)社区至关重要，因为运动方向判断是安全导航的关键信息。当前AI系统连这种基本运动都无法判断，不仅影响功能性，更严重损害用户信任，阻碍视障人士获得真正的独立导航能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察先进AI模型在自动扶梯方向判断任务上的失败，将其视为典型案例来揭示更广泛的运动盲区问题。他们分析了人类视觉系统感知连续光流与AI系统基于帧采样的根本差异。作者借鉴了经典计算机视觉中的光流概念，参考了现有视频理解基准测试的局限性，并考察了当前AI辅助技术的应用情况。作为立场论文，作者没有提出完整解决方案，而是识别问题、分析影响并提出未来研究方向。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是当前AI系统的运动盲区源于其视频处理的帧采样范式，它将视频视为离散静态图像序列，无法捕捉连续运动。解决方案需要从'语义识别'转向'物理感知'。整体实现流程包括：1)开发混合架构，结合传统光流技术与现代MLLM；2)探索事件相机等新型传感器；3)研究物理信息学习，将物理定律纳入模型训练；4)创建以人为本的基准测试，优先考虑安全性、可靠性和用户信任。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次正式识别并命名'隐式运动盲区'问题；2)深入分析人类视觉与AI系统的根本差异；3)将技术问题与用户体验和信任危机联系起来；4)呼吁研究范式从'语义识别'转向'物理感知'。相比之前工作，这篇论文不是提出新模型或算法，而是识别被忽视的根本问题；将技术问题与用户体验全面关联；呼吁研究范式转变而非简单技术改进；强调与最终用户合作的重要性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文识别并正式化了AI辅助技术中的'隐式运动盲区'问题，揭示了当前视频理解范式的根本局限性，并呼吁研究社区从语义识别转向物理感知，以开发真正可靠、值得信赖的辅助技术。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) hold immense promise as assistivetechnologies for the blind and visually impaired (BVI) community. However, weidentify a critical failure mode that undermines their trustworthiness inreal-world applications. We introduce the Escalator Problem -- the inability ofstate-of-the-art models to perceive an escalator's direction of travel -- as acanonical example of a deeper limitation we term Implicit Motion Blindness.This blindness stems from the dominant frame-sampling paradigm in videounderstanding, which, by treating videos as discrete sequences of staticimages, fundamentally struggles to perceive continuous, low-signal motion. As aposition paper, our contribution is not a new model but rather to: (I) formallyarticulate this blind spot, (II) analyze its implications for user trust, and(III) issue a call to action. We advocate for a paradigm shift from purelysemantic recognition towards robust physical perception and urge thedevelopment of new, human-centered benchmarks that prioritize safety,reliability, and the genuine needs of users in dynamic environments.</description>
      <author>example@mail.com (Xiantao Zhang)</author>
      <guid isPermaLink="false">2508.07989v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding</title>
      <link>http://arxiv.org/abs/2508.07683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了TAR-TVG框架，通过在推理过程中引入时间戳锚点来强制执行显式监督，解决了现有强化学习方法无法明确约束推理过程的问题。同时，他们开发了一个三阶段的自蒸馏训练策略，使模型能够生成稳健的锚点，同时保持推理质量。实验表明，该模型在实现最先进性能的同时，产生了可解释、可验证的推理链，并逐步完善时间估计。&lt;h4&gt;背景&lt;/h4&gt;时序视频定位(TVG)旨在精确定位与自然语言查询相对应的视频片段，这对长篇视频理解至关重要。尽管现有的强化学习方法鼓励模型在预测前生成推理链，但它们未能明确约束推理过程，无法确保最终时间预测的质量。&lt;h4&gt;目的&lt;/h4&gt;解决现有强化学习方法在时序视频定位中无法明确约束推理过程的问题，确保推理质量，并提高最终时间预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了TAR-TVG框架，在推理过程中引入时间戳锚点作为中间验证点，并要求每个推理步骤产生越来越精确的时间估计。同时，开发了一个三阶段的自蒸馏训练策略：(1)初始GRPO训练收集包含多个时间戳锚点的3万条高质量推理轨迹；(2)在蒸馏数据上进行监督微调；(3)在SFT增强的模型上进行最终的GRPO优化。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了最先进的性能；产生了可解释、可验证的推理链；推理过程中包含逐步完善的时间估计；三阶段训练策略使模型能够生成稳健的锚点，同时保持推理质量。&lt;h4&gt;结论&lt;/h4&gt;TAR-TVG框架通过引入时间戳锚点和三阶段训练策略，有效解决了现有方法在时序视频定位中推理质量控制的问题，实现了更准确的视频片段定位，并提供了可解释的推理过程。&lt;h4&gt;翻译&lt;/h4&gt;时序视频定位旨在精确定位与自然语言查询相对应的视频片段，这对长篇视频理解至关重要。尽管现有的强化学习方法鼓励模型在预测前生成推理链，但它们未能明确约束推理过程，无法确保最终时间预测的质量。为解决这一局限，我们提出了时间戳锚点约束的时序视频推理(TAR-TVG)，一种新颖的框架，它在推理过程中引入时间戳锚点，强制对思考内容进行显式监督。这些锚点作为中间验证点。更重要的是，我们要求每个推理步骤产生越来越精确的时间估计，从而确保推理过程对最终预测有实质性贡献。为解决模型中低概率锚点生成的挑战，我们开发了一种高效的自蒸馏训练策略：(1)初始GRPO训练收集3万条包含多个时间戳锚点的高质量推理轨迹；(2)在蒸馏数据上进行监督微调；(3)在SFT增强的模型上进行最终GRPO优化。这一三阶段训练策略使模型能够生成稳健的锚点，同时保持推理质量。实验表明，我们的模型在实现最先进性能的同时，产生了可解释、可验证的推理链，并逐步完善时间估计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Video Grounding (TVG) aims to precisely localize video segmentscorresponding to natural language queries, which is a critical capability forlong-form video understanding. Although existing reinforcement learningapproaches encourage models to generate reasoning chains before predictions,they fail to explicitly constrain the reasoning process to ensure the qualityof the final temporal predictions. To address this limitation, we proposeTimestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),a novel framework that introduces timestamp anchors within the reasoningprocess to enforce explicit supervision to the thought content. These anchorsserve as intermediate verification points. More importantly, we require eachreasoning step to produce increasingly accurate temporal estimations, therebyensuring that the reasoning process contributes meaningfully to the finalprediction. To address the challenge of low-probability anchor generation inmodels (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillationtraining strategy: (1) initial GRPO training to collect 30K high-qualityreasoning traces containing multiple timestamp anchors, (2) supervisedfine-tuning (SFT) on distilled data, and (3) final GRPO optimization on theSFT-enhanced model. This three-stage training strategy enables robust anchorgeneration while maintaining reasoning quality. Experiments show that our modelachieves state-of-the-art performance while producing interpretable, verifiablereasoning chains with progressively refined temporal estimations.</description>
      <author>example@mail.com (Chaohong Guo, Xun Mo, Yongwei Nie, Xuemiao Xu, Chao Xu, Fei Yu, Chengjiang Long)</author>
      <guid isPermaLink="false">2508.07683v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</title>
      <link>http://arxiv.org/abs/2508.07642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 5 Figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SkillNav，一个模块化框架，将结构化、基于技能的推理引入到基于Transformer的VLN智能体中。该方法将导航分解为可解释的原子技能，由专门智能体处理，并通过新颖的零样本VLM路由器动态选择最合适的智能体。SkillNav在R2R基准测试上实现了最先进性能，并在GSA-R2R基准测试上表现出强大的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视觉与语言导航(VLN)在让智能体解释自然语言指令并在复杂3D环境中导航方面提出了重大挑战。尽管最近的进展是由大规模预训练和数据增强推动的，但当前方法仍然难以推广到未见过的场景，特别是当需要复杂的时空推理时。&lt;h4&gt;目的&lt;/h4&gt;提出SkillNav框架，将结构化、基于技能的推理引入到基于Transformer的VLN智能体中，以提高泛化能力，特别是在复杂时空推理场景中。&lt;h4&gt;方法&lt;/h4&gt;将导航分解为一组可解释的原子技能（例如，垂直移动、区域和区域识别、停止和暂停），每个技能由专门的智能体处理。然后，引入了一种新颖的零样本视觉语言模型(VLM)路由器，它通过对齐子目标和视觉观察以及历史动作，在每个时间步动态选择最合适的智能体。&lt;h4&gt;主要发现&lt;/h4&gt;SkillNav在R2R基准测试上实现了新的最先进性能，并且对包含新指令风格和未见环境的GSA-R2R基准测试表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SkillNav通过模块化设计和基于技能的推理，有效解决了VLN中的泛化问题，特别是在复杂时空推理方面，为未来研究提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;视觉与语言导航(VLN)在让智能体解释自然语言指令并在复杂3D环境中导航方面提出了重大挑战。尽管最近的进展是由大规模预训练和数据增强推动的，但当前方法仍然难以推广到未见过的场景，特别是当需要复杂的时空推理时。在这项工作中，我们提出了SkillNav，一个模块化框架，将结构化、基于技能的推理引入到基于Transformer的VLN智能体中。我们的方法将导航分解为一组可解释的原子技能（例如，垂直移动、区域和区域识别、停止和暂停），每个技能由专门的智能体处理。然后，我们引入了一种新颖的零样本视觉语言模型(VLM)路由器，它通过对齐子目标和视觉观察以及历史动作，在每个时间步动态选择最合适的智能体。SkillNav在R2R基准测试上实现了新的最先进性能，并表现出对包含新指令风格和未见环境的GSA-R2R基准测试的强大泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Vision-and-Language Navigation (VLN)任务中的泛化能力不足问题，特别是当代理需要处理复杂时空推理、未见过的环境和新型指令时的挑战。这个问题在现实中非常重要，因为它关系到AI代理能否真正理解和执行人类在复杂环境中的导航指令，对于机器人技术、虚拟助手、自动驾驶等应用领域具有重要意义。在研究中，解决这一问题将推动具身AI的发展，使代理能够更好地适应多样化的现实世界场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到导航任务具有内在的组成性，可以分解为一系列可解释的原子技能。他们借鉴了Transformer-based VLN代理架构(如DUET)和技能专家混合系统的思想，但创新性地将其应用于VLN领域。作者参考了NavNuances数据集中的技能分类，并扩展了'停止和暂停'和'时间顺序规划'两个新技能。在路由器设计中，他们受到了LLM-based规划系统的启发，并利用现有的VLM模型(如GPT-4o和Qwen2.5-VL)来实现零样本推理。整体设计思路是将复杂的导航任务分解为专门的技能代理，并通过智能路由器动态选择最合适的代理来执行特定任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂的导航任务分解为一系列可解释的原子技能(如垂直移动、区域识别、停止和暂停等)，每个技能由专门的代理处理，并使用基于VLM的路由器动态选择最合适的代理。整体实现流程包括：1)技能分类和数据合成，为每个技能创建专门的合成数据集；2)代理训练，每个技能代理经过两阶段训练；3)导航流程，首先使用时间重排序模块将指令分解为结构化的子目标，然后通过子目标定位确定当前需要执行的子目标，接着通过技能路由选择最合适的代理，最后由选定的代理执行相应的导航动作，这个过程循环进行直到导航完成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SkillNav模块化框架，将导航分解为原子可重用技能；2)VLM-based路由器，动态选择最合适的技能代理；3)扩展技能分类，增加'停止和暂停'和'时间顺序规划'技能；4)技能特定数据合成方法；5)时间重排序模块。与之前工作的不同在于：SkillNav采用模块化而非端到端设计，强调技能的灵活重组而非简单的专家选择，具有更好的泛化能力和可解释性，特别是在处理新型指令和未知环境时表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SkillNav通过将导航任务分解为可重用的原子技能并使用基于VLM的路由器动态选择最合适的技能代理，显著提升了视觉语言导航代理在未知环境和新型指令上的泛化能力和可解释性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-Language Navigation (VLN) poses significant challenges in enablingagents to interpret natural language instructions and navigate complex 3Denvironments. While recent progress has been driven by large-scale pre-trainingand data augmentation, current methods still struggle to generalize to unseenscenarios, particularly when complex spatial and temporal reasoning isrequired. In this work, we propose SkillNav, a modular framework thatintroduces structured, skill-based reasoning into Transformer-based VLN agents.Our method decomposes navigation into a set of interpretable atomic skills(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), eachhandled by a specialized agent. We then introduce a novel zero-shotVision-Language Model (VLM)-based router, which dynamically selects the mostsuitable agent at each time step by aligning sub-goals with visual observationsand historical actions. SkillNav achieves a new state-of-the-art performance onthe R2R benchmark and demonstrates strong generalization to the GSA-R2Rbenchmark that includes novel instruction styles and unseen environments.</description>
      <author>example@mail.com (Tianyi Ma, Yue Zhang, Zehao Wang, Parisa Kordjamshidi)</author>
      <guid isPermaLink="false">2508.07642v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>FineBadminton: A Multi-Level Dataset for Fine-Grained Badminton Video Understanding</title>
      <link>http://arxiv.org/abs/2508.07554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FineBadminton数据集和FBBench基准，用于解决复杂高速运动（如羽毛球）的细粒度分析挑战，并提出了优化的基线方法。&lt;h4&gt;背景&lt;/h4&gt;复杂高速运动（如羽毛球）的细粒度分析对多模态大语言模型(MLLMs)构成重大挑战，尽管MLLMs在通用视频理解方面取得了显著进展。这种困难主要源于缺乏足够丰富且领域特定的注释数据集。&lt;h4&gt;目的&lt;/h4&gt;为了弥补数据集的不足，引入FineBadminton数据集和FBBench基准，促进细粒度视频理解研究并推动MLLMs在体育智能领域的发展。&lt;h4&gt;方法&lt;/h4&gt;1. 构建FineBadminton数据集，采用多级语义注释层次结构（基础动作、战术语义和决策评估）；2. 开创新的注释流程，协同结合MLLM生成的提案和人工细化；3. 提出FBBench基准用于评估；4. 设计优化的基线方法，包括以击球为中心的关键帧选择和坐标引导的视觉信息浓缩。&lt;h4&gt;主要发现&lt;/h4&gt;FBBench上的结果表明，尽管当前的MLLMs在深度体育视频分析方面仍面临重大挑战，但提出的策略仍然取得了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;FineBadminton和FBBench共同提供了一个关键的生态系统，以促进细粒度视频理解研究并推动MLLMs在体育智能领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;复杂高速运动（如羽毛球）的细粒度分析对多模态大语言模型(MLLMs)提出了重大挑战，尽管它们在通用视频理解方面取得了显著进展。这种困难主要源于缺乏足够丰富且领域特定的注释数据集。为了弥补这一差距，我们引入了FineBadminton，这是一个新颖的大规模数据集，具有独特的多级语义注释层次结构（基础动作、战术语义和决策评估），用于全面的羽毛球理解。FineBadminton的构建由创新的注释流程驱动，该流程协同结合了MLLM生成的提案和人工细化。我们还提出了FBBench，这是一个从FineBadminton衍生的具有挑战性的基准，用于严格评估MLLMs在细微时空推理和战术理解方面的能力。FineBadminton和FBBench共同提供了一个关键的生态系统，以促进细粒度视频理解研究并推动MLLMs在体育智能领域的发展。此外，我们提出了一种优化的基线方法，结合了以击球为中心的关键帧选择，专注于关键时刻，以及坐标引导的浓缩，用于提炼显著视觉信息。FBBench上的结果表明，尽管当前的MLLMs在深度体育视频分析方面仍面临重大挑战，但我们提出的策略仍然取得了显著的性能提升。项目主页可在提供的网址访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758218&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-grained analysis of complex and high-speed sports like badmintonpresents a significant challenge for Multimodal Large Language Models (MLLMs),despite their notable advancements in general video understanding. Thisdifficulty arises primarily from the scarcity of datasets with sufficientlyrich and domain-specific annotations. To bridge this gap, we introduceFineBadminton, a novel and large-scale dataset featuring a unique multi-levelsemantic annotation hierarchy (Foundational Actions, Tactical Semantics, andDecision Evaluation) for comprehensive badminton understanding. Theconstruction of FineBadminton is powered by an innovative annotation pipelinethat synergistically combines MLLM-generated proposals with human refinement.We also present FBBench, a challenging benchmark derived from FineBadminton, torigorously evaluate MLLMs on nuanced spatio-temporal reasoning and tacticalcomprehension. Together, FineBadminton and FBBench provide a crucial ecosystemto catalyze research in fine-grained video understanding and advance thedevelopment of MLLMs in sports intelligence. Furthermore, we propose anoptimized baseline approach incorporating Hit-Centric Keyframe Selection tofocus on pivotal moments and Coordinate-Guided Condensation to distill salientvisual information. The results on FBBench reveal that while current MLLMsstill face significant challenges in deep sports video analysis, our proposedstrategies nonetheless achieve substantial performance gains. The projecthomepage is available at https://finebadminton.github.io/FineBadminton/.</description>
      <author>example@mail.com (Xusheng He, Wei Liu, Shanshan Ma, Qian Liu, Chenghao Ma, Jianlong Wu)</author>
      <guid isPermaLink="false">2508.07554v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.06869v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages,3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为视觉-字幕集成(VSI)的多模态关键帧搜索方法，解决了长视频理解中多模态大语言模型面临的数据规模挑战，通过整合字幕、时间戳和场景边界信息，显著提高了关键帧检索的准确率。&lt;h4&gt;背景&lt;/h4&gt;长视频理解对多模态大语言模型(MLLMs)是一个重大挑战，主要原因在于数据规模巨大。目前广泛采用的关键帧检索策略受到文本查询与视觉内容之间弱多模态对齐的限制，无法捕获精确推理所需的复杂时间语义信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合视频视觉信息与文本信息的多模态关键帧搜索方法，提高长视频理解中的关键帧检索准确率。&lt;h4&gt;方法&lt;/h4&gt;提出视觉-字幕集成(VSI)方法，通过双流搜索机制分别捕获视频帧的视觉信息和互补的文本信息，包括视频搜索流和字幕匹配流，并通过两个搜索流的交互提高关键帧搜索准确性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，VSI在LongVideoBench的文本相关子集上实现了40.00%的关键帧定位准确率，在下游长视频问答任务上实现了68.48%的准确率，分别比竞争基线高出20.35%和15.79%。此外，VSI在LongVideoBench的中长视频问答任务上达到了最先进水平(SOTA)。&lt;h4&gt;结论&lt;/h4&gt;VSI方法通过整合视觉和文本信息，有效解决了长视频理解中的关键帧检索问题，展示了其鲁棒性和通用性，为多模态长视频理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解对多模态大语言模型(MLLMs)提出了重大挑战，主要原因是数据规模巨大。使这一计算上可处理的广泛采用的关键策略是关键帧检索，旨在识别与给定文本查询最相关的稀疏视频帧集。然而，这种方法的有效性受到文本查询与视觉内容之间弱多模态对齐的限制，无法捕获精确推理所需的复杂时间语义信息。为此，我们提出了视觉-字幕集成(VSI)，一种多模态关键帧搜索方法，将字幕、时间戳和场景边界整合到统一的多模态搜索过程中。所提出的方法通过视频搜索流和字幕匹配流的双流搜索机制分别捕获视频帧的视觉信息和互补的文本信息，并通过两个搜索流的交互提高关键帧搜索准确性。实验结果表明，VSI在LongVideoBench的文本相关子集上实现了40.00%的关键帧定位准确率，在下游长视频问答任务上实现了68.48%的准确率，分别超过了竞争基线20.35%和15.79%。此外，在LongVideoBench上，VSI在中长视频问答任务中达到了最先进水平(SOTA)，证明了所提出多模态搜索策略的鲁棒性和通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long video understanding presents a significant challenge to multimodal largelanguage models (MLLMs) primarily due to the immense data scale. A critical andwidely adopted strategy for making this task computationally tractable iskeyframe retrieval, which seeks to identify a sparse set of video frames thatare most salient to a given textual query. However, the efficacy of thisapproach is hindered by weak multimodal alignment between textual queries andvisual content and fails to capture the complex temporal semantic informationrequired for precise reasoning. To address this, we propose Visual-SubtitleIntegeration(VSI), a multimodal keyframe search method that integratessubtitles, timestamps, and scene boundaries into a unified multimodal searchprocess. The proposed method captures the visual information of video frames aswell as the complementary textual information through a dual-stream searchmechanism by Video Search Stream as well as Subtitle Match Stream,respectively, and improves the keyframe search accuracy through the interactionof the two search streams. Experimental results show that VSI achieve 40.00%key frame localization accuracy on the text-relevant subset of LongVideoBenchand 68.48% accuracy on downstream long Video-QA tasks, surpassing competitivebaselines by 20.35% and 15.79%, respectively. Furthermore, on theLongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QAtasks, demonstrating the robustness and generalizability of the proposedmultimodal search strategy.</description>
      <author>example@mail.com (Jianxiang He, Shaoguang Wang, Weiyu Guo, Meisheng Hong, Jungang Li, Yijie Xu, Ziyang Chen, Hui Xiong)</author>
      <guid isPermaLink="false">2508.06869v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding</title>
      <link>http://arxiv.org/abs/2508.04369v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为时间采样策略优化(TSPO)的方法，通过强化学习解决了多模态大语言模型处理长视频输入的挑战，在多个长视频理解基准测试中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视觉-语言任务中已取得显著进展，但在处理长视频输入时仍面临挑战。这些挑战源于模型的上下文限制和训练成本，需要在将视频输入模型前进行稀疏帧采样。然而，由于稀疏帧采样在视频多模态大语言模型中的无监督和非可微分性质，构建可训练的采样方法仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型处理长视频输入的挑战，提出时间采样策略优化方法，通过强化学习提升模型对长视频的理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种可训练的事件感知时间代理，捕捉事件-查询相关性以执行概率性关键帧选择；提出了TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程；提出了双风格长视频训练数据构建管道，平衡全面的时间理解和关键段定位；集成了基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。&lt;h4&gt;主要发现&lt;/h4&gt;TSPO在多个长视频理解基准测试中实现了最先进的性能，并且在不同前沿视频多模态大语言模型上显示了可转移能力。&lt;h4&gt;结论&lt;/h4&gt;TSPO有效地解决了多模态大语言模型处理长视频输入的挑战，通过强化学习优化了时间采样策略，提升了模型对长视频的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在视觉-语言任务中已取得显著进展，但在处理长视频输入时仍面临挑战。这种限制源于模型的上下文限制和训练成本，需要在将视频输入模型前进行稀疏帧采样。然而，由于稀疏帧采样在视频多模态大语言模型中的无监督和非可微分性质，构建可训练的采样方法仍然具有挑战性。为了解决这些问题，我们提出了时间采样策略优化，通过强化学习推进模型对长视频的理解。具体来说，我们首先提出了一种可训练的事件感知时间代理，捕捉事件-查询相关性以执行概率性关键帧选择。然后，我们提出了TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程，实现时间采样策略的端到端分组相对优化。此外，我们提出了双风格长视频训练数据构建管道，平衡全面的时间理解和关键段定位。最后，我们整合了基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。全面的实验表明，我们的TSPO在多个长视频理解基准测试中实现了最先进的性能，并显示出在不同前沿视频多模态大语言模型上的可转移能力。我们的代码可在https://github.com/Hui-design/TSPO获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated significantprogress in vision-language tasks, yet they still face challenges whenprocessing long-duration video inputs. The limitation arises from MLLMs'context limit and training costs, necessitating sparse frame sampling beforefeeding videos into MLLMs. However, building a trainable sampling methodremains challenging due to the unsupervised and non-differentiable nature ofsparse frame sampling in Video-MLLMs. To address these problems, we proposeTemporal Sampling Policy Optimization (TSPO), advancing MLLMs' long-formvideo-language understanding via reinforcement learning. Specifically, we firstpropose a trainable event-aware temporal agent, which captures event-querycorrelation for performing probabilistic keyframe selection. Then, we proposethe TSPO reinforcement learning paradigm, which models keyframe selection andlanguage generation as a joint decision-making process, enabling end-to-endgroup relative optimization for the temporal sampling policy. Furthermore, wepropose a dual-style long video training data construction pipeline, balancingcomprehensive temporal understanding and key segment localization. Finally, weincorporate rule-based answering accuracy and temporal locating rewardmechanisms to optimize the temporal sampling policy. Comprehensive experimentsshow that our TSPO achieves state-of-the-art performance across multiple longvideo understanding benchmarks, and shows transferable ability across differentcutting-edge Video-MLLMs. Our code is available athttps://github.com/Hui-design/TSPO</description>
      <author>example@mail.com (Canhui Tang, Zifan Han, Hongbo Sun, Sanping Zhou, Xuchong Zhang, Xin Wei, Ye Yuan, Huayu Zhang, Jinglin Xu, Hao Sun)</author>
      <guid isPermaLink="false">2508.04369v3</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning</title>
      <link>http://arxiv.org/abs/2508.08186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to IEEE Transactions on Pattern Analysis and Machine  Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为KARMA的高效语义分割框架，用于土木基础设施结构缺陷检测。该框架通过组合一维函数而非传统卷积来建模复杂缺陷模式，具有参数效率高、性能优越的特点，适合实时检测系统。&lt;h4&gt;背景&lt;/h4&gt;土木基础设施的结构缺陷语义分割面临三大挑战：缺陷外观变化大、成像条件恶劣、类别严重不平衡。现有深度学习方法虽有效，但通常需要数百万参数，使其在实时检测系统中不实用。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的语义分割框架，能够在保持竞争力的性能的同时显著减少参数量，使其适合实时基础设施检测系统。&lt;h4&gt;方法&lt;/h4&gt;KARMA框架包含三个技术创新：1) 参数高效的Tiny Kolmogorov-Arnold Network（TiKAN）模块，利用低阶分解进行基于KAN的特征变换；2) 优化的特征金字塔结构，使用可分离卷积进行多尺度缺陷分析；3) 静态-动态原型机制，增强不平衡类别的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;在基准基础设施检测数据集上的实验表明，KARMA与最先进方法相比具有竞争性或更好的平均IoU性能，同时使用显著更少的参数（0.959M vs. 31.04M，减少了97%）。KARMA以0.264 GFLOPS的速度运行，保持适合实时部署的推理速度。&lt;h4&gt;结论&lt;/h4&gt;KARMA框架通过创新的架构设计，成功解决了传统深度学习模型在实时基础设施检测中的参数效率问题，为自动化基础设施检测系统提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;土木基础设施中结构缺陷的语义分割由于缺陷外观变化大、成像条件恶劣和严重的类别不平衡而仍然具有挑战性。当前的深度学习方法尽管有效，但通常需要数百万参数，使它们在实时检测系统中不切实际。我们引入了KARMA（Kolmogorov-Arnold表示映射架构），这是一种高效的语义分割框架，通过组合一维函数而非传统卷积来建模复杂的缺陷模式。KARMA具有三个技术创新：（1）参数高效的Tiny Kolmogorov-Arnold Network（TiKAN）模块，利用低阶分解进行基于KAN的特征变换；（2）优化的特征金字塔结构，使用可分离卷积进行多尺度缺陷分析；（3）静态-动态原型机制，增强不平衡类别的特征表示。在基准基础设施检测数据集上的大量实验表明，与最先进的方法相比，KARMA实现了竞争性或更好的平均IoU性能，同时使用显著更少的参数（0.959M vs. 31.04M，减少了97%）。KARMA以0.264 GFLOPS的速度运行，保持适合实时部署的推理速度，使实用的自动化基础设施检测系统成为可能，同时不妥协准确性。源代码可通过以下URL访问：https://github.com/faeyelab/karma。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of structural defects in civil infrastructure remainschallenging due to variable defect appearances, harsh imaging conditions, andsignificant class imbalance. Current deep learning methods, despite theireffectiveness, typically require millions of parameters, rendering themimpractical for real-time inspection systems. We introduce KARMA(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficientsemantic segmentation framework that models complex defect patterns throughcompositions of one-dimensional functions rather than conventionalconvolutions. KARMA features three technical innovations: (1) aparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraginglow-rank factorization for KAN-based feature transformation; (2) an optimizedfeature pyramid structure with separable convolutions for multi-scale defectanalysis; and (3) a static-dynamic prototype mechanism that enhances featurerepresentation for imbalanced classes. Extensive experiments on benchmarkinfrastructure inspection datasets demonstrate that KARMA achieves competitiveor superior mean IoU performance compared to state-of-the-art approaches, whileusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable forreal-time deployment, enabling practical automated infrastructure inspectionsystems without compromising accuracy. The source code can be accessed at thefollowing URL: https://github.com/faeyelab/karma.</description>
      <author>example@mail.com (Md Meftahul Ferdaus, Mahdi Abdelguerfi, Elias Ioup, Steven Sloan, Kendall N. Niles, Ken Pathak)</author>
      <guid isPermaLink="false">2508.08186v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0</title>
      <link>http://arxiv.org/abs/2508.08110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了自监督语音表征学习模型架构对表征中学习到的语言信息的影响，特别比较了HuBERT和wav2vec 2.0两种模型在训练目标和迭代伪标签精细化方面的差异。&lt;h4&gt;背景&lt;/h4&gt;自监督语音表征学习模型因其多功能性和在下游任务上的性能而被广泛使用，但模型架构对表征中学习到的语言信息的影响尚未得到充分研究。&lt;h4&gt;目的&lt;/h4&gt;研究HuBERT和wav2vec 2.0两种模型，并比较它们架构上的两个差异：训练目标和通过多次训练迭代进行的迭代伪标签精细化。&lt;h4&gt;方法&lt;/h4&gt;分析两种自监督语音表征学习模型的隐藏表征与词身份、音素身份和说话人身份的典型相关性差异。&lt;h4&gt;主要发现&lt;/h4&gt;隐藏表征与词身份、音素身份和说话人身份的典型相关性差异是由训练迭代解释的，而不是训练目标。&lt;h4&gt;结论&lt;/h4&gt;建议未来的研究调查迭代精细化在自监督语音表征中编码语言信息有效性的原因。&lt;h4&gt;翻译&lt;/h4&gt;自监督语音表征学习模型现在因其多功能性和在下游任务上的性能而被广泛使用，但模型架构对其表征中学习的语言信息的影响仍未得到充分研究。本研究调查了两种这样的模型，HuBERT和wav2vec 2.0，并最小程度地比较了它们架构上的两个差异：训练目标和通过多次训练迭代的迭代伪标签精细化。我们发现，隐藏表征与词身份、音素身份和说话人身份的典型相关性差异是由训练迭代解释的，而不是训练目标。我们建议未来的研究调查迭代精细化在自监督语音表征中编码语言信息有效性的原因。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised models for speech representation learning now see widespreaduse for their versatility and performance on downstream tasks, but the effectof model architecture on the linguistic information learned in theirrepresentations remains under-studied. This study investigates two such models,HuBERT and wav2vec 2.0, and minimally compares two of their architecturaldifferences: training objective and iterative pseudo-label refinement throughmultiple training iterations. We find that differences in canonical correlationof hidden representations to word identity, phoneme identity, and speakeridentity are explained by training iteration, not training objective. Wesuggest that future work investigate the reason for the effectiveness ofiterative refinement in encoding linguistic information in self-supervisedspeech representations.</description>
      <author>example@mail.com (Robin Huo, Ewan Dunbar)</author>
      <guid isPermaLink="false">2508.08110v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning</title>
      <link>http://arxiv.org/abs/2508.08031v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对联邦自监督学习的不可感知且有效的后门攻击方法IPBA，解决了现有方法在隐蔽性和实用性方面的局限性&lt;h4&gt;背景&lt;/h4&gt;联邦自监督学习结合了分布式建模和无标签表示学习的优势，具有可扩展性和隐私保护潜力，但容易受到后门攻击&lt;h4&gt;目的&lt;/h4&gt;开发一种不可感知且有效的后门攻击方法，满足实际部署中的隐蔽性和实用性要求&lt;h4&gt;方法&lt;/h4&gt;IPBA通过解耦后门和增强样本的特征分布，引入Sliced-Wasserstein距离减轻后门样本的分布外特性，优化触发器生成过程&lt;h4&gt;主要发现&lt;/h4&gt;现有不可感知触发器在FSSL中面临有限的迁移性、与增强样本的特征纠缠以及分布外特性等挑战；IPBA在多个FSSL场景和数据集上显著优于现有方法&lt;h4&gt;结论&lt;/h4&gt;IPBA作为一种针对FSSL的后门攻击方法，在性能和隐蔽性上表现出色，且在各种防御机制下具有强大的鲁棒性&lt;h4&gt;翻译&lt;/h4&gt;联邦自监督学习结合了分布式建模和无标签表示学习的优势，是一种具有可扩展性和隐私保护潜力的前沿范式。尽管FSSL受到越来越多的关注，但研究表明它容易受到后门攻击。现有方法通常依赖于视觉上明显的触发器，难以满足实际部署中的隐蔽性和实用性要求。本文提出了一种针对FSSL的不可感知且有效的后门攻击方法IPBA。我们的实证研究表明，现有不可感知触发器在FSSL中面临一系列挑战，特别是有限的迁移性、与增强样本的特征纠缠以及分布外特性。这些问题共同削弱了传统后门攻击在FSSL中的有效性和隐蔽性。为了克服这些挑战，IPBA解耦了后门和增强样本的特征分布，并引入Sliced-Wasserstein距离来减轻后门样本的分布外特性，从而优化触发器生成过程。我们在多个FSSL场景和数据集上的实验结果表明，IPBA在性能上显著优于现有的后门攻击方法，并且在各种防御机制下表现出强大的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated self-supervised learning (FSSL) combines the advantages ofdecentralized modeling and unlabeled representation learning, serving as acutting-edge paradigm with strong potential for scalability and privacypreservation. Although FSSL has garnered increasing attention, researchindicates that it remains vulnerable to backdoor attacks. Existing methodsgenerally rely on visually obvious triggers, which makes it difficult to meetthe requirements for stealth and practicality in real-world deployment. In thispaper, we propose an imperceptible and effective backdoor attack method againstFSSL, called IPBA. Our empirical study reveals that existing imperceptibletriggers face a series of challenges in FSSL, particularly limitedtransferability, feature entanglement with augmented samples, andout-of-distribution properties. These issues collectively undermine theeffectiveness and stealthiness of traditional backdoor attacks in FSSL. Toovercome these challenges, IPBA decouples the feature distributions of backdoorand augmented samples, and introduces Sliced-Wasserstein distance to mitigatethe out-of-distribution properties of backdoor samples, thereby optimizing thetrigger generation process. Our experimental results on several FSSL scenariosand datasets show that IPBA significantly outperforms existing backdoor attackmethods in performance and exhibits strong robustness under various defensemechanisms.</description>
      <author>example@mail.com (Jiayao Wang, Yang Song, Zhendong Zhao, Jiale Zhang, Qilin Wu, Junwu Zhu, Dongfang Zhao)</author>
      <guid isPermaLink="false">2508.08031v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Mining the Social Fabric: Unveiling Communities for Fake News Detection in Short Videos</title>
      <link>http://arxiv.org/abs/2508.07992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DugFND的双社区图方法，用于检测短视频中的虚假新闻。该方法通过建模上传者社区和事件驱动社区，结合异构图注意力网络和基于预训练的节点表示学习，显著提高了虚假新闻检测的性能。&lt;h4&gt;背景&lt;/h4&gt;短视频平台已成为信息分享的主要媒介，但其快速的内容生成和算法放大也导致虚假新闻广泛传播。检测短视频中的虚假新闻具有挑战性，因为它们是多模态的，且单个视频的上下文有限。现有方法主要分析内容信号（视觉、文本和音频），但常常忽略视频、上传者和事件之间的隐含关系。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法忽略视频、上传者和事件之间隐含关系的问题，提出一种新的方法来增强现有的视频分类器，提高短视频虚假新闻检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出DugFND（用于虚假新闻检测的双社区图），通过建模两种关键社区模式来增强现有的视频分类器：1) 上传者社区：具有共同兴趣或相似内容创作模式的上传者聚集在一起；2) 事件驱动社区：与相同或语义相似公共事件相关的视频形成局部集群。构建一个连接上传者、视频和事件节点的异构图，设计了一个时间感知的异构图注意力网络来实现有效的消息传递。通过基于预训练的进一步改进节点表示学习阶段，DugFND可以应用于任何预训练的分类器。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上的实验表明，DugFND方法取得了显著的性能提升，证明了双社区建模对短视频虚假新闻检测的价值。&lt;h4&gt;结论&lt;/h4&gt;双社区建模对于短视频虚假新闻检测是有价值的，DugFND方法能有效提高虚假新闻检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;短视频平台已成为信息分享的主要媒介，但其快速的内容生成和算法放大也使得虚假新闻广泛传播。由于短视频的多模态性质和单个视频的有限上下文，检测短视频中的虚假新闻具有挑战性。虽然最近的方法专注于分析内容信号-视觉、文本和音频-但它们常常忽略了视频、上传者和事件之间的隐含关系。为了解决这一差距，我们提出了DugFND（用于虚假新闻检测的双社区图），一种通过建模两种关键社区模式来增强现有视频分类器的新方法：(1) 上传者社区，具有共同兴趣或相似内容创作模式的上传者聚集在一起；(2) 事件驱动社区，与相同或语义相似公共事件相关的视频形成局部集群。我们构建了一个连接上传者、视频和事件节点的异构图，并设计了一个时间感知的异构图注意力网络以实现有效的消息传递。基于预训练的进一步改进了节点表示学习。DugFND可以应用于任何预训练的分类器。公共数据集上的实验表明，我们的方法取得了显著的性能提升，证明了双社区建模对短视频虚假新闻检测的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Short video platforms have become a major medium for information sharing, buttheir rapid content generation and algorithmic amplification also enable thewidespread dissemination of fake news. Detecting misinformation in short videosis challenging due to their multi-modal nature and the limited context ofindividual videos. While recent methods focus on analyzing contentsignals-visual, textual, and audio-they often overlook implicit relationshipsamong videos, uploaders, and events. To address this gap, we propose DugFND(Dual-community graph for fake news detection), a novel method that enhancesexisting video classifiers by modeling two key community patterns: (1) uploadercommunities, where uploaders with shared interests or similar content creationpatterns group together, and (2) event-driven communities, where videos relatedto the same or semantically similar public events form localized clusters. Weconstruct a heterogeneous graph connecting uploader, video, and event nodes,and design a time-aware heterogeneous graph attention network to enableeffective message passing. A reconstruction-based pretraining phase furtherimproves node representation learning. DugFND can be applied to any pre-trainedclassifier. Experiments on public datasets show that our method achievessignificant performance gains, demonstrating the value of dual-communitymodeling for fake news detection in short videos.</description>
      <author>example@mail.com (Haisong Gong, Bolan Su, Xinrong Zhang, Jing Li, Qiang Liu, Shu Wu, Liang Wang)</author>
      <guid isPermaLink="false">2508.07992v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images</title>
      <link>http://arxiv.org/abs/2508.07847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Deep Space Weather Model (Deep SWM)的新方法，用于太阳耀斑预测。该方法基于多个深度状态空间模型处理多通道太阳图像和长程时空依赖关系，并包含稀疏掩码自编码器这一创新预训练策略。同时建立了FlareBench基准数据集，验证了该方法在性能和可靠性上超越基线方法和人类专家。&lt;h4&gt;背景&lt;/h4&gt;准确的太阳耀斑预测对减轻关键基础设施潜在破坏至关重要，但太阳耀斑预测仍面临重大挑战。现有基于启发式物理特征的方法缺乏对太阳图像的表示学习能力，而端到端学习方法难以建模太阳图像中的长程时间依赖关系。&lt;h4&gt;目的&lt;/h4&gt;开发能够处理多通道太阳图像和长程时空依赖关系的太阳耀斑预测方法，并建立新的基准数据集验证该方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出Deep Space Weather Model (Deep SWM)，基于多个深度状态空间模型处理十通道太阳图像和长程时空依赖关系。采用稀疏掩码自编码器作为预训练策略，使用两阶段掩码方法保留太阳黑斑等关键区域同时压缩空间信息。建立FlareBench基准数据集，覆盖完整的11年太阳活动周期。&lt;h4&gt;主要发现&lt;/h4&gt;Deep SWM在标准指标上的性能和可靠性方面优于基线方法，甚至超过了人类专家的表现。&lt;h4&gt;结论&lt;/h4&gt;Deep SWM有效结合了深度状态空间模型和稀疏掩码自编码器的优势，成功处理了太阳图像中的长程时空依赖关系，在太阳耀斑预测任务上取得优异性能。FlareBench基准为该领域研究提供了新的评估标准。&lt;h4&gt;翻译&lt;/h4&gt;准确的太阳耀斑预测对于减轻对关键基础设施的潜在破坏至关重要，而预测太阳耀斑仍然是一个重大挑战。现有的基于启发式物理特征的方法通常缺乏对太阳图像的表示学习能力。另一方面，端到端学习方法难以建模太阳图像中的长程时间依赖关系。在本研究中，我们提出了Deep Space Weather Model (Deep SWM)，它基于多个深度状态空间模型，用于处理十通道太阳图像和长程时空依赖关系。Deep SWM还具有稀疏掩码自编码器，这是一种新的预训练策略，采用两阶段掩码方法来保留太阳黑斑等关键区域，同时压缩空间信息。此外，我们建立了FlareBench，一个新的太阳耀斑预测公共基准，覆盖完整的11年太阳活动周期，以验证我们的方法。我们的方法在性能和可靠性方面的标准指标上超越了基线方法，甚至超过了人类专家的表现。项目页面可以在https://keio-smilab25.github.io/DeepSWM找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate, reliable solar flare prediction is crucial for mitigating potentialdisruptions to critical infrastructure, while predicting solar flares remains asignificant challenge. Existing methods based on heuristic physical featuresoften lack representation learning from solar images. On the other hand,end-to-end learning approaches struggle to model long-range temporaldependencies in solar images. In this study, we propose Deep Space WeatherModel (Deep SWM), which is based on multiple deep state space models forhandling both ten-channel solar images and long-range spatio-temporaldependencies. Deep SWM also features a sparse masked autoencoder, a novelpretraining strategy that employs a two-phase masking approach to preservecrucial regions such as sunspots while compressing spatial information.Furthermore, we built FlareBench, a new public benchmark for solar flareprediction covering a full 11-year solar activity cycle, to validate ourmethod. Our method outperformed baseline methods and even human expertperformance on standard metrics in terms of performance and reliability. Theproject page can be found at https://keio-smilab25.github.io/DeepSWM.</description>
      <author>example@mail.com (Shunya Nagashima, Komei Sugiura)</author>
      <guid isPermaLink="false">2508.07847v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Topological Feature Compression for Molecular Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.07807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种新的图神经网络架构，结合压缩的高阶拓扑信号和标准分子特征，在保持计算效率和可解释性的同时实现了优异的预测性能。&lt;h4&gt;背景&lt;/h4&gt;分子表征学习虽有进展，但在平衡预测准确性、可解释性和计算效率的同时提取通用化学见解仍是重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能捕获全局几何信息同时保持计算效率和人类可解释结构的图神经网络架构。&lt;h4&gt;方法&lt;/h4&gt;引入一种结合压缩高阶拓扑信号与标准分子特征的新型图神经网络架构，在保持计算可处理性和可解释结构的同时捕获全局几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在从小分子到复杂数据材料集的一系列基准测试中表现出优越性能，使用参数高效架构，在几乎所有基准测试中准确性和鲁棒性均达到最佳结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的图神经网络架构成功地将高阶拓扑信息与标准分子特征结合，实现了多数据集上的高性能，同时保持了计算效率和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;分子表征学习的最新进展为许多化学信息学和生物信息学任务产生了高效的分子编码。然而，在平衡预测准确性、可解释性和计算效率的同时提取通用化学见解仍然是一个重大挑战。在这项工作中，我们引入了一种新颖的图神经网络(GNN)架构，该架构结合了压缩的高阶拓扑信号和标准分子特征。我们的方法在保持计算效率和人类可解释结构的同时捕获了全局几何信息。我们在从小分子数据集到复杂数据材料集的一系列基准测试中评估了我们的模型，并展示了使用参数高效架构的优越性能。我们在几乎所有基准测试中实现了最佳的性能和鲁棒性结果。我们将所有代码开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in molecular representation learning have produced highlyeffective encodings of molecules for numerous cheminformatics andbioinformatics tasks. However, extracting general chemical insight whilebalancing predictive accuracy, interpretability, and computational efficiencyremains a major challenge. In this work, we introduce a novel Graph NeuralNetwork (GNN) architecture that combines compressed higher-order topologicalsignals with standard molecular features. Our approach captures globalgeometric information while preserving computational tractability andhuman-interpretable structure. We evaluate our model across a range ofbenchmarks, from small-molecule datasets to complex material datasets, anddemonstrate superior performance using a parameter-efficient architecture. Weachieve the best performing results in both accuracy and robustness acrossalmost all benchmarks. We open source all code \footnote{All code and resultscan be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.</description>
      <author>example@mail.com (Rahul Khorana)</author>
      <guid isPermaLink="false">2508.07807v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation</title>
      <link>http://arxiv.org/abs/2508.07649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为DiMuST的社会增强兴趣点(POI)推荐模型，基于多空间-时间转换图的解耦表示学习，解决了现有方法中空间和时间转换单独建模导致的表示不一致问题。&lt;h4&gt;背景&lt;/h4&gt;下一个兴趣点(POI)推荐是商业智能领域的研究热点，用户的空间-时间转换和社会关系在其中起着关键作用。然而，大多数现有工作将空间和时间转换分开建模，导致相同空间-时间关键节点的表示不一致。&lt;h4&gt;目的&lt;/h4&gt;解决现有POI推荐方法中空间和时间转换单独建模导致的表示不一致问题，减少融合过程中的冗余信息，降低模型不确定性，提高模型可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出DiMuST模型，采用创新的解耦变分多层图自编码器(DAE)，首先使用多层空间-时间图策略解耦共享和私有分布，然后通过专家乘积(PoE)机制融合共享特征，通过对比约束去噪私有特征，有效捕获POI的空间-时间转换表示，同时保持其空间-时间关系的内在相关性。&lt;h4&gt;主要发现&lt;/h4&gt;在两个具有挑战性的数据集上进行的实验表明，DiMuST在多个评估指标上显著优于现有的推荐方法，证明了其有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;DiMuST通过解耦表示学习和多空间-时间图策略，成功解决了POI推荐中空间和时间转换建模不一致的问题，提高了推荐性能和模型可解释性，为POI推荐领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;下一个兴趣点(POI)推荐是商业智能领域的研究热点，用户的空间-时间转换和社会关系在其中起着关键作用。然而，大多数现有工作将空间和时间转换分开建模，导致相同空间-时间关键节点的表示不一致。这种不一致在融合过程中引入冗余信息，增加模型不确定性，降低可解释性。为解决这个问题，我们提出了DiMuST，一种基于多空间-时间转换图解耦表示学习的社会增强POI推荐模型。该模型采用一种新的解耦变分多层图自编码器(DAE)，首先使用多层空间-时间图策略解耦共享和私有分布，然后通过专家乘积(PoE)机制融合共享特征，通过对比约束去噪私有特征。该模型有效捕获了POI的空间-时间转换表示，同时保留了其空间-时间关系的内在相关性。在两个具有挑战性的数据集上的实验表明，我们的DiMuST在多个指标上显著优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next Point-of-Interest (POI) recommendation is a research hotspot in businessintelligence, where users' spatial-temporal transitions and socialrelationships play key roles. However, most existing works model spatial andtemporal transitions separately, leading to misaligned representations of thesame spatial-temporal key nodes. This misalignment introduces redundantinformation during fusion, increasing model uncertainty and reducinginterpretability. To address this issue, we propose DiMuST, a socially enhancedPOI recommendation model based on disentangled representation learning overmultiplex spatial-temporal transition graphs. The model employs a novelDisentangled variational multiplex graph Auto-Encoder (DAE), which firstdisentangles shared and private distributions using a multiplexspatial-temporal graph strategy. It then fuses the shared features via aProduct of Experts (PoE) mechanism and denoises the private features throughcontrastive constraints. The model effectively captures the spatial-temporaltransition representations of POIs while preserving the intrinsic correlationof their spatial-temporal relationships. Experiments on two challengingdatasets demonstrate that our DiMuST significantly outperforms existing methodsacross multiple metrics.</description>
      <author>example@mail.com (Jie Li, Haoye Dong, Zhengyang Wu, Zetao Zheng, Mingrong Lin)</author>
      <guid isPermaLink="false">2508.07649v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>UniFlow: Unifying Speech Front-End Tasks via Continuous Generative Modeling</title>
      <link>http://arxiv.org/abs/2508.07558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  extended version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为UniFlow的统一框架，该框架使用连续生成模型在共享潜在空间中处理多样化的语音前端任务，展示了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;生成模型在图像、视频和音频领域取得了显著成功，但语音前端任务（如语音增强、目标说话人提取、声学回声消除和语言查询源分离）仍然使用分散的、特定于任务的解决方案，导致冗余工程努力、不一致性能和有限扩展性。&lt;h4&gt;目的&lt;/h4&gt;引入UniFlow统一框架，采用连续生成模型在共享潜在空间中处理多样化语音前端任务，解决现有方法的碎片化问题。&lt;h4&gt;方法&lt;/h4&gt;UniFlow使用波形变分自编码器学习原始音频的紧凑潜在表示，结合扩散Transformer预测潜在更新；使用任务ID索引的可学习条件嵌入实现最大参数共享同时保持任务特定适应性；研究和比较了三种生成目标：去噪扩散、流匹配和均值流。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共基准上验证UniFlow，结果显示其性能优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;UniFlow的统一潜在公式和条件设计使其易于扩展到新任务，为构建和扩展生成式语音处理管道提供了集成基础，作者将开源代码库以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;生成建模最近在图像、视频和音频领域取得了显著成功，展示了统一表示学习的强大能力。然而，语音前端任务，如语音增强、目标说话人提取、声学回声消除和语言查询源分离，仍然主要由分散的、特定于任务的解决方案处理。这种碎片化导致了冗余的工程努力、不一致的性能和有限的扩展性。为了解决这一差距，我们引入了UniFlow，一个统一框架，它采用连续生成模型在共享潜在空间中处理多样化的语音前端任务。具体来说，UniFlow使用波形变分自编码器学习原始音频的紧凑潜在表示，并结合一个预测潜在更新的扩散Transformer。为了在训练期间区分语音处理任务，使用了由任务ID索引的可学习条件嵌入，以实现最大的参数共享同时保持任务特定的适应性。为了平衡模型性能和计算效率，我们在潜在域内研究和比较了三种生成目标：去噪扩散、流匹配和均值流。我们在多个公共基准上验证了UniFlow，展示了优于最先进基线的一致性提升。UniFlow的统一潜在公式和条件设计使其易于扩展到新任务，为构建和扩展生成式语音处理管道提供了集成基础。为了促进未来研究，我们将开源我们的代码库。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has recently achieved remarkable success across image,video, and audio domains, demonstrating powerful capabilities for unifiedrepresentation learning. Yet speech front-end tasks such as speech enhancement(SE), target speaker extraction (TSE), acoustic echo cancellation (AEC), andlanguage-queried source separation (LASS) remain largely tackled by disparate,task-specific solutions. This fragmentation leads to redundant engineeringeffort, inconsistent performance, and limited extensibility. To address thisgap, we introduce UniFlow, a unified framework that employs continuousgenerative modeling to tackle diverse speech front-end tasks in a shared latentspace. Specifically, UniFlow utilizes a waveform variational autoencoder (VAE)to learn a compact latent representation of raw audio, coupled with a DiffusionTransformer (DiT) that predicts latent updates. To differentiate the speechprocessing task during the training, learnable condition embeddings indexed bya task ID are employed to enable maximal parameter sharing while preservingtask-specific adaptability. To balance model performance and computationalefficiency, we investigate and compare three generative objectives: denoisingdiffusion, flow matching, and mean flow within the latent domain. We validateUniFlow on multiple public benchmarks, demonstrating consistent gains overstate-of-the-art baselines. UniFlow's unified latent formulation andconditional design make it readily extensible to new tasks, providing anintegrated foundation for building and scaling generative speech processingpipelines. To foster future research, we will open-source our codebase.</description>
      <author>example@mail.com (Ziqian Wang, Zikai Liu, Yike Zhu, Xingchen Li, Boyi Kang, Jixun Yao, Xianjun Xia, Chuanzeng Huang, Lei Xie)</author>
      <guid isPermaLink="false">2508.07558v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction</title>
      <link>http://arxiv.org/abs/2508.07518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a Research Paper (short) at ACM SIGSPATIAL 2025. This  arXiv version is the full version of the paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于解耦表示学习的新框架FairDRL-ST，用于解决时空预测中的公平性问题，特别是在移动性需求预测方面，通过无监督方式实现公平性并最小化性能损失。&lt;h4&gt;背景&lt;/h4&gt;深度时空神经网络在城市计算中的应用日益广泛，直接影响公共交通、紧急服务和交通管理系统等关键城市基础设施的用户。虽然许多方法关注提高准确性，但公平性近期受到重视，因为时空应用中的有偏见预测可能不成比例地损害特定人口统计或地理群体，加剧社会经济不平等，影响AI在公共服务中的道德部署。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为FairDRL-ST的新框架，解决时空预测中的公平性问题，特别关注移动性需求预测。&lt;h4&gt;方法&lt;/h4&gt;基于解耦表示学习的框架，利用对抗学习和解耦表示学习技术，学习分离包含敏感信息的属性。与现有通过监督学习强制实现公平性的方法不同，该框架采用无监督方式实现公平性，避免了过度补偿和性能下降。&lt;h4&gt;主要发现&lt;/h4&gt;将框架应用于真实世界城市移动性数据集，结果表明该方法能够缩小公平差距，同时与最先进的公平感知方法相比提供具有竞争力的预测性能。&lt;h4&gt;结论&lt;/h4&gt;FairDRL-ST框架能够在保持预测性能的同时有效实现公平性，无监督方法比监督方法在处理公平性问题时更为有效。&lt;h4&gt;翻译&lt;/h4&gt;随着深度时空神经网络在城市计算环境中日益广泛应用，此类方法的部署可以直接影响关键城市基础设施用户，如公共交通、紧急服务和交通管理系统。虽然许多时空方法专注于提高准确性，但公平性最近受到关注，因为越来越多证据表明时空应用中的有偏见预测可能会不成比例地损害某些人口统计或地理群体，从而加剧现有的社会经济不平等，并削弱人工智能在公共服务中的道德部署。在本文中，我们提出了一种名为FairDRL-ST的新框架，基于解耦表示学习，解决时空预测中的公平性问题，特别关注移动性需求预测。通过利用对抗学习和解耦表示学习，我们的框架学习分离包含敏感信息的属性。与现有通过监督学习强制实现公平性可能导致过度补偿和性能下降的方法不同，我们的框架以无监督方式实现公平性，且性能损失最小。我们将框架应用于真实世界城市移动性数据集，证明了其缩小公平差距的能力，同时与最先进的公平感知方法相比提供具有竞争力的预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As deep spatio-temporal neural networks are increasingly utilised in urbancomputing contexts, the deployment of such methods can have a direct impact onusers of critical urban infrastructure, such as public transport, emergencyservices, and traffic management systems. While many spatio-temporal methodsfocus on improving accuracy, fairness has recently gained attention due togrowing evidence that biased predictions in spatio-temporal applications candisproportionately disadvantage certain demographic or geographic groups,thereby reinforcing existing socioeconomic inequalities and undermining theethical deployment of AI in public services. In this paper, we propose a novelframework, FairDRL-ST, based on disentangled representation learning, toaddress fairness concerns in spatio-temporal prediction, with a particularfocus on mobility demand forecasting. By leveraging adversarial learning anddisentangled representation learning, our framework learns to separateattributes that contain sensitive information. Unlike existing methods thatenforce fairness through supervised learning, which may lead toovercompensation and degraded performance, our framework achieves fairness inan unsupervised manner with minimal performance loss. We apply our framework toreal-world urban mobility datasets and demonstrate its ability to closefairness gaps while delivering competitive predictive performance compared tostate-of-the-art fairness-aware methods.</description>
      <author>example@mail.com (Sichen Zhao, Wei Shao, Jeffrey Chan, Ziqi Xu, Flora Salim)</author>
      <guid isPermaLink="false">2508.07518v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification</title>
      <link>http://arxiv.org/abs/2508.07465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为MOTGNN的新型多组学整合框架，用于二元疾病分类，通过结合XGBoost、图神经网络和深度前馈网络，实现了高准确率和可解释性。&lt;h4&gt;背景&lt;/h4&gt;整合多组学数据（如DNA甲基化、mRNA表达和microRNA表达）可以全面了解疾病的生物学机制，但高维度和组学层之间的复杂交互给预测建模带来挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖且可解释的框架，用于二元疾病分类，解决多组学数据整合中的高维度和复杂交互问题。&lt;h4&gt;方法&lt;/h4&gt;MOTGNN框架使用XGBoost进行组学特定的监督图构建，采用模态特定的图神经网络进行分层表示学习，并通过深度前馈网络实现跨组学整合。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界疾病数据集上，MOTGNN在准确率、ROC-AUC和F1分数方面比最先进的基线方法高出5-10%；在严重类别不平衡情况下保持稳健（F1分数87.2% vs 33.4%）；通过稀疏图保持计算效率；提供内置可解释性，揭示顶级生物标志物和各组学模态的相对贡献。&lt;h4&gt;结论&lt;/h4&gt;MOTGNN在多组学疾病建模中显著提高了预测准确性和可解释性，具有很大的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;整合多组学数据，如DNA甲基化、mRNA表达和microRNA（miRNA）表达，可以全面了解疾病背后的生物学机制。然而，组学数据的高维度和组学层之间的复杂交互给预测建模带来了主要挑战。我们提出了多组学整合与树生成图神经网络（MOTGNN），这是一种新颖且可解释的二元疾病分类框架。MOTGNN采用eXtreme Gradient Boosting（XGBoost）进行组学特定的监督图构建，然后使用模态特定的图神经网络进行分层表示学习，最后使用深度前馈网络进行跨组学整合。在三个真实世界疾病数据集上，MOTGNN在准确率、ROC-AUC和F1分数方面比最先进的基线方法高出5-10%，并且在严重类别不平衡的情况下保持稳健（例如在不平衡数据上F1分数为87.2% vs 33.4%）。模型通过稀疏图（每个节点2.1-2.8条边）保持计算效率，并提供内置可解释性，揭示顶级生物标志物和每个组学模态的相对贡献。这些结果突显了MOTGNN在提高多组学疾病建模预测准确性和可解释性方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating multi-omics data, such as DNA methylation, mRNA expression, andmicroRNA (miRNA) expression, offers a comprehensive view of the biologicalmechanisms underlying disease. However, the high dimensionality and complexinteractions among omics layers present major challenges for predictivemodeling. We propose Multi-Omics integration with Tree-generated Graph NeuralNetwork (MOTGNN), a novel and interpretable framework for binary diseaseclassification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to performomics-specific supervised graph construction, followed by modality-specificGraph Neural Networks (GNNs) for hierarchical representation learning, and adeep feedforward network for cross-omics integration. On three real-worlddisease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% inaccuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintainscomputational efficiency through sparse graphs (2.1-2.8 edges per node) andprovides built-in interpretability, revealing both top-ranked biomarkers andthe relative contributions of each omics modality. These results highlightMOTGNN's potential to improve both predictive accuracy and interpretability inmulti-omics disease modeling.</description>
      <author>example@mail.com (Tiantian Yang, Zhiqian Chen)</author>
      <guid isPermaLink="false">2508.07465v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2508.07452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCORER的新框架，通过将感知和控制网络之间的交互建模为Stackelberg博弈，改进了深度强化学习中的表示学习和策略学习，提高了样本效率和最终性能，而无需复杂的辅助目标或架构。&lt;h4&gt;背景&lt;/h4&gt;深度强化学习中的端到端学习是基础，但面对稀疏奖励信号时，学习有效特征具有挑战性。最近研究趋势是添加复杂辅助目标或完全解耦表示学习和策略学习过程，但这增加了设计复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种替代方案，既不是完全解耦也不是简单端到端学习，而是通过原则性的博弈论动态结构来组织和控制感知与控制网络之间的交互，从而提高性能。&lt;h4&gt;方法&lt;/h4&gt;引入Stackelberg耦合表示和强化学习(SCORER)框架，将感知网络(领导者)和控制网络(跟随者)之间的交互建模为Stackelberg博弈。感知网络战略性地学习特征以有利于控制网络，控制网络的目标是最小化其Bellman误差。使用双时间尺度算法近似博弈均衡。&lt;h4&gt;主要发现&lt;/h4&gt;在标准DQN变体和基准任务上应用SCORER，提高了样本效率和最终性能。结果表明，可以通过对感知-控制动态的原则性算法设计实现性能提升，无需复杂辅助目标或架构。&lt;h4&gt;结论&lt;/h4&gt;通过原则性地设计感知-控制动态的算法，可以实现显著的性能提升，而无需依赖复杂的辅助目标或架构。&lt;h4&gt;翻译&lt;/h4&gt;端到端的表示学习和策略学习的集成仍然是深度强化学习(RL)的基石。然而，为了应对从稀疏奖励信号中学习有效特征的挑战，最近的趋势已转向添加复杂的辅助目标或完全解耦这两个过程，但这通常以增加设计复杂性为代价。本文提出了一种替代完全解耦和简单端到端学习的方法，认为通过使用原则性的博弈论动态结构来组织和控制不同感知和控制网络之间的交互，可以显著提高性能。我们通过引入Stackelberg耦合表示和强化学习(SCORER)框架来形式化这种动态，该框架将感知和控制之间的交互建模为Stackelberg博弈。感知网络(领导者)战略性地学习特征以有利于控制网络(跟随者)，而控制网络的目标是最小化其Bellman误差。我们使用实用的双时间尺度算法来近似博弈的均衡。在基准任务上的标准DQN变体应用中，SCORER提高了样本效率和最终性能。我们的结果表明，可以通过对感知-控制动态的原则性算法设计实现性能提升，而无需复杂的辅助目标或架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated, end-to-end learning of representations and policies remains acornerstone of deep reinforcement learning (RL). However, to address thechallenge of learning effective features from a sparse reward signal, recenttrends have shifted towards adding complex auxiliary objectives or fullydecoupling the two processes, often at the cost of increased design complexity.This work proposes an alternative to both decoupling and naive end-to-endlearning, arguing that performance can be significantly improved by structuringthe interaction between distinct perception and control networks with aprincipled, game-theoretic dynamic. We formalize this dynamic by introducingthe Stackelberg Coupled Representation and Reinforcement Learning (SCORER)framework, which models the interaction between perception and control as aStackelberg game. The perception network (leader) strategically learns featuresto benefit the control network (follower), whose own objective is to minimizeits Bellman error. We approximate the game's equilibrium with a practicaltwo-timescale algorithm. Applied to standard DQN variants on benchmark tasks,SCORER improves sample efficiency and final performance. Our results show thatperformance gains can be achieved through principled algorithmic design of theperception-control dynamic, without requiring complex auxiliary objectives orarchitectures.</description>
      <author>example@mail.com (Fernando Martinez, Tao Li, Yingdong Lu, Juntao Chen)</author>
      <guid isPermaLink="false">2508.07452v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Statistical Inference for Autoencoder-based Anomaly Detection after Representation Learning-based Domain Adaptation</title>
      <link>http://arxiv.org/abs/2508.07049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了STAND-DA框架，用于在领域适应后进行统计严谨的自编码器异常检测，通过选择性推断计算有效p值并控制假阳性率。&lt;h4&gt;背景&lt;/h4&gt;异常检测在多个领域至关重要，但应用于数据有限的目标域时性能会下降。领域适应可通过从数据丰富的源域转移知识来解决这个问题，但适应过程会引入不确定性，使难以得出统计有效的结论。&lt;h4&gt;目的&lt;/h4&gt;开发一个统计严谨的异常检测框架，能够在领域适应后提供有效的统计推断，控制假阳性率。&lt;h4&gt;方法&lt;/h4&gt;STAND-DA建立在选择性推断框架上，计算检测到的异常的有效p值，将假阳性率控制在预定义水平α以下。开发GPU加速的SI实现以提高可扩展性和运行时性能，使SI适用于现代大规模深度架构。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实世界数据集上的广泛实验验证了STAND-DA方法的计算效率和理论结果，证明其在领域适应后能提供统计严谨的异常检测。&lt;h4&gt;结论&lt;/h4&gt;STAND-DA成功解决了深度学习模型应用选择性推断的计算挑战，为领域适应后的异常检测提供了统计严谨的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;异常检测在广泛领域中发挥着重要作用，但当应用于数据有限的目标域时，其性能可能会下降。领域适应通过从数据丰富的相关源域转移知识提供了解决方案。然而，这种适应过程会引入额外的不确定性，使得难以从异常检测结果中得出统计上有效的结论。在本文中，我们提出了STAND-DA——一个用于在基于表示学习的领域适应之后进行统计严谨的自编码器异常检测的新框架。建立在选择性推断框架之上，STAND-DA计算检测到的异常的有效p值，并将假阳性率严格控制在预定义的水平以下。为解决将选择性推断应用于深度学习模型的计算挑战，我们开发了GPU加速的实现，显著提高了可扩展性和运行时性能。这一进展使选择性推断在现代大规模深度架构中实际可行。在合成和真实世界数据集上的广泛实验验证了所提出的STAND-DA方法的计算效率和理论结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection (AD) plays a vital role across a wide range of domains, butits performance might deteriorate when applied to target domains with limiteddata. Domain Adaptation (DA) offers a solution by transferring knowledge from arelated source domain with abundant data. However, this adaptation process canintroduce additional uncertainty, making it difficult to draw statisticallyvalid conclusions from AD results. In this paper, we propose STAND-DA -- anovel framework for statistically rigorous Autoencoder-based AD afterRepresentation Learning-based DA. Built on the Selective Inference (SI)framework, STAND-DA computes valid $p$-values for detected anomalies andrigorously controls the false positive rate below a pre-specified level$\alpha$ (e.g., 0.05). To address the computational challenges of applying SIto deep learning models, we develop the GPU-accelerated SI implementation,significantly enhancing both scalability and runtime performance. Thisadvancement makes SI practically feasible for modern, large-scale deeparchitectures. Extensive experiments on synthetic and real-world datasetsvalidate the theoretical results and computational efficiency of the proposedSTAND-DA method.</description>
      <author>example@mail.com (Tran Tuan Kiet, Nguyen Thang Loi, Vo Nguyen Le Duy)</author>
      <guid isPermaLink="false">2508.07049v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting</title>
      <link>http://arxiv.org/abs/2508.06915v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了QuiZSF框架，将检索增强生成(RAG)与时间序列预训练模型(TSPMs)结合，以增强零样本时间序列预测能力，在数据稀缺场景下表现优异。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测对支持流数据应用至关重要，而零样本预测在数据稀缺场景(如领域转移或极端条件)下极具价值但传统模型难以处理。虽然TSPMs在ZSF中表现良好，但缺乏动态整合外部知识的机制。&lt;h4&gt;目的&lt;/h4&gt;将RAG引入TSPMs以增强零样本时间序列预测，开发一个轻量级且模块化的框架，耦合高效检索、表示学习和模型适应。&lt;h4&gt;方法&lt;/h4&gt;提出QuiZSF框架，包含分层树结构的ChronoRAG Base(CRB)用于时间序列存储和检索，多粒度序列交互学习器(MSIL)提取关系特征，以及双分支模型协作器(MCC)对齐检索知识与两类TSPMs。&lt;h4&gt;主要发现&lt;/h4&gt;与当代基线相比，QuiZSF以非LLM型和LLM型TSPMs为基础时，分别在75%和87.5%的预测设置中排名第一，同时保持内存和推理时间的高效率。&lt;h4&gt;结论&lt;/h4&gt;QuiZSF成功结合RAG和TSPMs优势，显著提升零样本时间序列预测性能，在保持高效的同时在大多数预测设置中超越现有方法。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测已成为支持各种流数据应用的重要技术。零样本时间序列预测(ZSF)在数据稀缺场景(如领域转移或极端条件预测)下特别有价值，但传统模型难以处理这一问题。虽然时间序列预训练模型(TSPMs)在ZSF中表现出色，但它们通常缺乏动态整合外部知识的机制。幸运的是，新兴的检索增强生成(RAG)为按需注入此类知识提供了有前景的路径，但它们很少与TSPMs集成。为了结合两者的优势，我们将RAG引入TSPMs以增强零样本时间序列预测。在本文中，我们提出了QuiZSF(快速零样本时间序列预测器)，这是一个轻量级且模块化的框架，将高效检索与表示学习和模型适应相结合用于ZSF。具体而言，我们构建了分层树结构的ChronoRAG Base(CRB)用于可扩展的时间序列存储和领域感知检索，引入了多粒度序列交互学习器(MSIL)来提取细粒度和粗粒度关系特征，并开发了双分支模型协作器(MCC)，将检索到的知识与两种TSPMs对齐：非LLM型和LLM型。与当代基线相比，QuiZSF分别以非LLM型和LLM型TSPMs作为基础模型，在75%和87.5%的预测设置中排名第一，同时在内存和推理时间方面保持高效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting has become increasingly important to empower diverseapplications with streaming data. Zero-shot time-series forecasting (ZSF),particularly valuable in data-scarce scenarios, such as domain transfer orforecasting under extreme conditions, is difficult for traditional models todeal with. While time series pre-trained models (TSPMs) have demonstratedstrong performance in ZSF, they often lack mechanisms to dynamicallyincorporate external knowledge. Fortunately, emerging retrieval-augmentedgeneration (RAG) offers a promising path for injecting such knowledge ondemand, yet they are rarely integrated with TSPMs. To leverage the strengths ofboth worlds, we introduce RAG into TSPMs to enhance zero-shot time seriesforecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time SeriesForecaster), a lightweight and modular framework that couples efficientretrieval with representation learning and model adaptation for ZSF.Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)for scalable time-series storage and domain-aware retrieval, introduce aMulti-grained Series Interaction Learner (MSIL) to extract fine- andcoarse-grained relational features, and develop a dual-branch Model CooperationCoherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLMbased and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLMbased and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and87.5% of prediction settings, while maintaining high efficiency in memory andinference time.</description>
      <author>example@mail.com (Shichao Ma, Zhengyang Zhou, Qihe Huang, Binwu Wang, Kuo Yang, Huan Li, Yang Wang)</author>
      <guid isPermaLink="false">2508.06915v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos</title>
      <link>http://arxiv.org/abs/2508.06902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对短视频情绪分析(VEA)的挑战，提出了大规模数据集eMotions和新型网络架构AV-CANet。研究解决了短视频多模态复杂性带来的语义差距和音频-视觉表达不一致等问题，并通过实验验证了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;短视频已成为在线信息获取和分享的重要部分，其多模态复杂性给视频分析带来新挑战。现有研究主要关注有明显情绪线索的视频，而短视频内容多样性大、语义差距明显，且音频-视觉共同表达普遍存在，导致情绪分析困难。&lt;h4&gt;目的&lt;/h4&gt;创建大规模短视频情绪数据集eMotions，并开发有效的短视频情绪分析方法AV-CANet，解决短视频情绪分析中的语义差距和音频-视觉表达不一致等问题。&lt;h4&gt;方法&lt;/h4&gt;构建包含27,996个视频的eMotions数据集，采用多阶段标注流程确保质量；提出AV-CANet网络架构，利用视频transformer捕获语义表示，引入局部-全局融合模块处理音频-视觉特征关联，并设计EP-CE损失函数进行全局优化。&lt;h4&gt;主要发现&lt;/h4&gt;短视频内容多样性导致更明显的语义差距，音频-视觉共同表达的普遍性造成局部偏差和集体信息差距；在三个eMotions相关数据集和四个公共VEA数据集上的实验验证了AV-CANet的有效性。&lt;h4&gt;结论&lt;/h4&gt;AV-CANet能有效处理短视频情绪分析中的挑战，为未来研究提供新见解；消融研究确认了方法关键组件的重要性；数据集和代码将在Github上公开共享。&lt;h4&gt;翻译&lt;/h4&gt;短视频已成为我们日常在线获取和分享信息的重要部分。其多模态复杂性给视频分析带来了新挑战，突显了社区对视频情绪分析(VEA)的需求。鉴于短视频情绪数据的有限可用性，我们引入了eMotions，这是一个包含27,996个具有完整标注视频的大规模数据集。为确保质量和减少主观偏见，我们强调更好的人员配置并提出了多阶段标注流程。此外，通过有针对性的采样，我们提供了类别平衡和测试导向的变体以满足不同需求。虽然已有大量研究关注有明显情绪线索的视频(如面部表情)，但分析短视频中的情绪仍然是一项具有挑战性的任务。挑战源于内容多样性更广，这引入了更明显的语义差距，并使情绪相关特征的表示学习复杂化。此外，短视频中音频-视觉共同表达的普遍性导致了由情绪表达不一致引起的局部偏差和集体信息差距。为解决这一问题，我们提出了AV-CANet，这是一种利用视频transformer捕获语义相关表示的端到端音频-视觉融合网络。我们进一步引入了局部-全局融合模块，旨在逐步捕获音频-视觉特征的关联。此外，构建了EP-CE损失函数，使用三极惩罚进行全局优化引导。在三个eMotions相关数据集和四个公共VEA数据集上的广泛实验证明了我们提出的AV-CANet的有效性，同时为未来研究提供了广泛的见解。此外，我们进行了消融研究以检查我们方法的关键组件。数据集和代码将在Github上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Short-form videos (SVs) have become a vital part of our online routine foracquiring and sharing information. Their multimodal complexity poses newchallenges for video analysis, highlighting the need for video emotion analysis(VEA) within the community. Given the limited availability of SVs emotion data,we introduce eMotions, a large-scale dataset consisting of 27,996 videos withfull-scale annotations. To ensure quality and reduce subjective bias, weemphasize better personnel allocation and propose a multi-stage annotationprocedure. Additionally, we provide the category-balanced and test-orientedvariants through targeted sampling to meet diverse needs. While there have beensignificant studies on videos with clear emotional cues (e.g., facialexpressions), analyzing emotions in SVs remains a challenging task. Thechallenge arises from the broader content diversity, which introduces moredistinct semantic gaps and complicates the representations learning ofemotion-related features. Furthermore, the prevalence of audio-visualco-expressions in SVs leads to the local biases and collective information gapscaused by the inconsistencies in emotional expressions. To tackle this, wepropose AV-CANet, an end-to-end audio-visual fusion network that leveragesvideo transformer to capture semantically relevant representations. We furtherintroduce the Local-Global Fusion Module designed to progressively capture thecorrelations of audio-visual features. Besides, EP-CE Loss is constructed toglobally steer optimizations with tripolar penalties. Extensive experimentsacross three eMotions-related datasets and four public VEA datasets demonstratethe effectiveness of our proposed AV-CANet, while providing broad insights forfuture research. Moreover, we conduct ablation studies to examine the criticalcomponents of our method. Dataset and code will be made available at Github.</description>
      <author>example@mail.com (Xuecheng Wu, Dingkang Yang, Danlei Huang, Xinyi Yin, Yifan Wang, Jia Zhang, Jiayu Nie, Liangyu Fu, Yang Liu, Junxiao Xue, Hadi Amirpour, Wei Zhou)</author>
      <guid isPermaLink="false">2508.06902v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>A Joint Sparse Self-Representation Learning Method for Multiview Clustering</title>
      <link>http://arxiv.org/abs/2508.06857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的联合稀疏自表示学习模型用于多视图聚类，通过引入基数约束而非图拉普拉斯正则化来提取视图特定的局部信息，并开发了具有全局收敛性的交替二次惩罚方法来解决算法收敛问题。实验结果表明，与八种最先进算法相比，所提出的方法在六个标准数据集上表现更优越。&lt;h4&gt;背景&lt;/h4&gt;多视图聚类旨在利用不同视图间的一致性和互补信息对样本进行分组。作为多视图聚类的基本技术，子空间聚类已受到广泛关注。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的联合稀疏自表示学习模型用于多视图聚类，解决基于增广拉格朗日法的交替最小化算法在非凸、非光滑模型中不能保证收敛的问题，提高方法的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出一种联合稀疏自表示学习模型，通过引入基数约束而非图拉普拉斯正则化来提取视图特定的局部信息。在每个视图下，基数约束限制自表示阶段使用的样本，以提取可靠的局部和全局结构信息，同时低秩约束有助于在合并过程中揭示共识亲和矩阵中的全局相干结构。为解决算法收敛问题，开发了具有全局收敛性的交替二次惩罚方法，通过闭式解迭代求解两个子问题。&lt;h4&gt;主要发现&lt;/h4&gt;在六个标准数据集上的经验结果表明，与八种最先进算法相比，所提出的模型和方法具有优越性。这表明基于基数约束的局部信息提取和交替二次惩罚方法能有效提高多视图聚类的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的联合稀疏自表示学习模型和交替二次惩罚方法能够有效处理多视图聚类问题，通过提取视图特定的局部信息和保证算法的全局收敛性，显著提高了聚类性能，为多视图聚类研究提供了新的思路和方法。&lt;h4&gt;翻译&lt;/h4&gt;多视图聚类旨在利用各种视图间的一致性和互补信息对样本进行分组。作为多视图聚类的基本技术，子空间聚类已引起广泛关注。在本文中，我们提出了一种用于多视图聚类的新型联合稀疏自表示学习模型，其特点是引入基数约束而非图拉普拉斯正则化来提取视图特定的局部信息。具体而言，在每个视图下，基数约束直接限制自表示阶段使用的样本，以提取可靠的局部和全局结构信息，同时低秩约束有助于在合并过程中揭示共识亲和矩阵中的全局相干结构。伴随的挑战是，基于增广拉格朗日法的交替最小化算法不能保证直接应用于非凸、非光滑模型时的收敛性，从而导致泛化能力差。为解决这一问题，我们开发了一种具有全局收敛性的交替二次惩罚方法，通过闭式解迭代求解两个子问题。在六个标准数据集上的经验结果表明，与八种最先进算法相比，我们的模型和方法具有优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiview clustering (MC) aims to group samples using consistent andcomplementary information across various views. The subspace clustering, as afundamental technique of MC, has attracted significant attention. In thispaper, we propose a novel joint sparse self-representation learning model forMC, where a featured difference is the extraction of view-specific localinformation by introducing cardinality (i.e., $\ell_0$-norm) constraintsinstead of Graph-Laplacian regularization. Specifically, under each view,cardinality constraints directly restrict the samples used in theself-representation stage to extract reliable local and global structureinformation, while the low-rank constraint aids in revealing a global coherentstructure in the consensus affinity matrix during merging. The attendantchallenge is that Augmented Lagrange Method (ALM)-based alternatingminimization algorithms cannot guarantee convergence when applied directly toour nonconvex, nonsmooth model, thus resulting in poor generalization ability.To address it, we develop an alternating quadratic penalty (AQP) method withglobal convergence, where two subproblems are iteratively solved by closed-formsolutions. Empirical results on six standard datasets demonstrate thesuperiority of our model and AQP method, compared to eight state-of-the-artalgorithms.</description>
      <author>example@mail.com (Mengxue Jia, Zhihua Allen-Zhao, You Zhao, Sanyang Liu)</author>
      <guid isPermaLink="false">2508.06857v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-Aware Spiking Graph Neural Network</title>
      <link>http://arxiv.org/abs/2508.06793v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种几何感知的脉冲图神经网络(GSG)，结合了脉冲神经网络的能效性和图神经网络在复杂图结构建模方面的优势，通过在黎曼流形上进行自适应表示学习，解决了现有方法在欧几里得空间中无法有效建模层次结构和循环等复杂图结构的问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在建模图结构数据方面表现出色，而脉冲神经网络(SNNs)通过稀疏、事件驱动的计算提供高能效。然而，现有的脉冲图神经网络主要在欧几里得空间运行，依赖固定的几何假设，限制了它们对复杂图结构如层次结构和循环的建模能力。&lt;h4&gt;目的&lt;/h4&gt;克服现有脉冲图神经网络的局限性，提出一种能够统一脉冲神经动力学与黎曼流形上自适应表示学习的几何感知脉冲图神经网络。&lt;h4&gt;方法&lt;/h4&gt;提出GSG，包含三个关键组件：1)黎曼嵌入层，将节点特征投影到常曲率流形池中，捕获非欧几里得结构；2)流形脉冲层，通过几何一致的邻域聚合和基于曲率的注意，在弯曲空间中建模膜电位演化和脉冲行为；3)流形学习目标，通过在测地距离上联合优化的分类和链接预测损失，实现实例级别的几何适应。所有模块使用黎曼SGD训练，无需通过时间反向传播。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上，GSG相比欧几里得SNN和基于流形的GNN实现了更高的准确性、鲁棒性和能效。&lt;h4&gt;结论&lt;/h4&gt;GSG建立了用于曲率感知、能效图学习的新范式，为处理复杂图结构提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在建模图结构数据方面表现出色，而脉冲神经网络(SNNs)通过稀疏、事件驱动的计算提供高能效。然而，现有的脉冲图神经网络主要在欧几里得空间运行，依赖固定的几何假设，限制了它们对层次结构和循环等复杂图结构的建模能力。为克服这些局限性，我们提出了GSG，一种新颖的几何感知脉冲图神经网络，统一了基于脉冲的神经动力学与黎曼流形上的自适应表示学习。GSG具有三个关键组件：一个将节点特征投影到常曲率流形池中的黎曼嵌入层，捕获非欧几里得结构；一个通过几何一致的邻域聚合和基于曲率的注意，在弯曲空间中建模膜电位演化和脉冲行为的流形脉冲层；以及一个通过在测地距离上联合优化的分类和链接预测损失实现实例级别几何适应的流形学习目标。所有模块都使用黎曼SGD进行训练，无需通过时间反向传播。在多个基准上的广泛实验表明，GSG相比欧几里得SNN和基于流形的GNN实现了更高的准确性、鲁棒性和能效，为曲率感知、能效的图学习建立了新范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated impressive capabilities inmodeling graph-structured data, while Spiking Neural Networks (SNNs) offer highenergy efficiency through sparse, event-driven computation. However, existingspiking GNNs predominantly operate in Euclidean space and rely on fixedgeometric assumptions, limiting their capacity to model complex graphstructures such as hierarchies and cycles. To overcome these limitations, wepropose \method{}, a novel Geometry-Aware Spiking Graph Neural Network thatunifies spike-based neural dynamics with adaptive representation learning onRiemannian manifolds. \method{} features three key components: a RiemannianEmbedding Layer that projects node features into a pool of constant-curvaturemanifolds, capturing non-Euclidean structures; a Manifold Spiking Layer thatmodels membrane potential evolution and spiking behavior in curved spaces viageometry-consistent neighbor aggregation and curvature-based attention; and aManifold Learning Objective that enables instance-wise geometry adaptationthrough jointly optimized classification and link prediction losses definedover geodesic distances. All modules are trained using Riemannian SGD,eliminating the need for backpropagation through time. Extensive experiments onmultiple benchmarks show that GSG achieves superior accuracy, robustness, andenergy efficiency compared to both Euclidean SNNs and manifold-based GNNs,establishing a new paradigm for curvature-aware, energy-efficient graphlearning.</description>
      <author>example@mail.com (Bowen Zhang, Genan Dai, Hu Huang, Long Lan)</author>
      <guid isPermaLink="false">2508.06793v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>In-Context Reinforcement Learning via Communicative World Models</title>
      <link>http://arxiv.org/abs/2508.06659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为CORAL的框架，通过通信表示来增强强化学习代理的泛化能力，使其能够在不更新参数的情况下适应新任务。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理通常难以在不更新参数的情况下泛化到新任务和环境中，主要原因是它们学到的表示策略过度拟合了训练环境的特定特征。&lt;h4&gt;目的&lt;/h4&gt;提高代理的上下文强化学习能力(ICRL)，将其表述为两个代理的涌现通信问题。&lt;h4&gt;方法&lt;/h4&gt;引入CORAL框架，通过解耦潜在表示学习和控制来学习可迁移的通信上下文；信息代理(IA)作为世界模型在各种任务上预训练，不追求最大化任务奖励，而是构建世界模型并将理解提炼为简洁消息；通过因果影响损失形成涌现通信协议；部署时，预训练的IA作为新控制代理(CA)的固定上下文化器，CA通过解释通信上下文学习解决任务。&lt;h4&gt;主要发现&lt;/h4&gt;该方法使CA能够实现显著的样本效率提升，在完全未见过的稀疏奖励环境中，借助预训练的IA成功实现零样本适应。&lt;h4&gt;结论&lt;/h4&gt;验证了学习可迁移通信表示的有效性，为强化学习代理的泛化能力提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;强化学习(RL)代理通常难以在不更新其参数的情况下泛化到新任务和环境中，主要是因为它们学到的表示和策略过度拟合了其训练环境的特定特征。为了提高代理的上下文强化学习(ICRL)能力，这项工作将ICRL表述为一个双代理的涌现通信问题，并引入了CORAL（用于自适应RL的通信表示）框架，该框架通过解耦潜在表示学习和控制来学习可迁移的通信上下文。在CORAL中，信息代理(IA)在多样化的任务分布上预训练为世界模型。其目标不是最大化任务奖励，而是构建世界模型并将其理解提炼为简洁消息。涌现通信协议由一种新颖的因果影响损失(Causal Influence Loss)塑造，该损失衡量消息对下一个动作的影响。在部署期间，先前训练的IA作为新控制代理(CA)的固定上下文化器，CA通过解释提供的通信上下文来学习解决任务。我们的实验证明，这种方法使CA能够在样本效率方面取得显著提升，并借助预训练的IA在完全未见过的稀疏奖励环境中成功执行零样本适应，从而验证了学习可迁移通信表示的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) agents often struggle to generalize to new tasksand contexts without updating their parameters, mainly because their learnedrepresentations and policies are overfit to the specifics of their trainingenvironments. To boost agents' in-context RL (ICRL) ability, this workformulates ICRL as a two-agent emergent communication problem and introducesCORAL (Communicative Representation for Adaptive RL), a framework that learns atransferable communicative context by decoupling latent representation learningfrom control. In CORAL, an Information Agent (IA) is pre-trained as a worldmodel on a diverse distribution of tasks. Its objective is not to maximize taskreward, but to build a world model and distill its understanding into concisemessages. The emergent communication protocol is shaped by a novel CausalInfluence Loss, which measures the effect that the message has on the nextaction. During deployment, the previously trained IA serves as a fixedcontextualizer for a new Control Agent (CA), which learns to solve tasks byinterpreting the provided communicative context. Our experiments demonstratethat this approach enables the CA to achieve significant gains in sampleefficiency and successfully perform zero-shot adaptation with the help ofpre-trained IA in entirely unseen sparse-reward environments, validating theefficacy of learning a transferable communicative representation.</description>
      <author>example@mail.com (Fernando Martinez-Lopez, Tao Li, Yingdong Lu, Juntao Chen)</author>
      <guid isPermaLink="false">2508.06659v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2508.06588v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对图数据向量量化中的代码本崩溃问题，提出了RGVQ框架，通过整合图拓扑和特征相似性作为正则化信号，有效提高了代码本利用率和标记多样性，显著提升了图标记表示的表达能力和可转移性。&lt;h4&gt;背景&lt;/h4&gt;向量量化(VQ)已成为学习图结构数据离散表示的有前景方法，但代码本崩溃这一基本挑战在图领域尚未得到充分探索，限制了图标记的表达能力和泛化能力。即使在视觉或语言领域提出的缓解策略，在图数据上应用VQ时仍然会出现代码本崩溃问题。&lt;h4&gt;目的&lt;/h4&gt;首次实证研究图数据上VQ的代码本崩溃问题，理解图VQ特别容易崩溃的原因，并提出解决方案来增强代码本利用率和促进标记多样性，最终提高图标记表示的表达能力和可转移性。&lt;h4&gt;方法&lt;/h4&gt;提出RGVQ框架，整合图拓扑和特征相似性作为显式正则化信号；通过Gumbel-Softmax重参数化引入软分配，确保所有码字接收梯度更新；结合结构感知对比正则化来惩罚相似节点对之间的标记共分配。&lt;h4&gt;主要发现&lt;/h4&gt;代码本崩溃在图数据上应用VQ时持续发生；图VQ容易崩溃的两个关键因素是：图特征和结构模式中的冗余导致的早期分配不平衡，以及确定性VQ中的自我强化优化循环；RGVQ显著改善了代码本利用，并一致提升了多个下游任务中最先进图VQ骨干模型的性能。&lt;h4&gt;结论&lt;/h4&gt;RGVQ框架通过解决代码本崩溃问题，实现了更具表达能力和可转移性的图标记表示，为图数据上的向量量化提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;向量量化(VQ)最近已成为学习图结构数据离散表示的一种有前景的方法。然而，一个基本挑战，即代码本崩溃，在图领域仍未得到充分探索，这显著限制了图标记的表达能力和泛化能力。在本文中，我们进行了首次实证研究，表明即使在视觉或语言领域提出的缓解策略，当将VQ应用于图数据时，代码本崩溃仍然持续发生。为了理解为什么图VQ特别容易崩溃，我们提供了理论分析并确定了两个关键因素：图特征和结构模式中的冗余导致的早期分配不平衡，以及确定性VQ中的自我强化优化循环。为了解决这些问题，我们提出了RGVQ，一个新颖的框架，它整合图拓扑和特征相似性作为显式正则化信号，以增强代码本利用率和促进标记多样性。RGVQ通过Gumbel-Softmax重参数化引入软分配，确保所有码字接收梯度更新。此外，RGVQ集成了结构感知对比正则化来惩罚相似节点对之间的标记共分配。大量实验表明，RGVQ显著提高了代码本利用率，并一致提升了多个下游任务中最先进图VQ骨干模型的性能，实现了更具表达能力和可转移性的图标记表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vector Quantization (VQ) has recently emerged as a promising approach forlearning discrete representations of graph-structured data. However, afundamental challenge, i.e., codebook collapse, remains underexplored in thegraph domain, significantly limiting the expressiveness and generalization ofgraph tokens.In this paper, we present the first empirical study showing thatcodebook collapse consistently occurs when applying VQ to graph data, even withmitigation strategies proposed in vision or language domains. To understand whygraph VQ is particularly vulnerable to collapse, we provide a theoreticalanalysis and identify two key factors: early assignment imbalances caused byredundancy in graph features and structural patterns, and self-reinforcingoptimization loops in deterministic VQ. To address these issues, we proposeRGVQ, a novel framework that integrates graph topology and feature similarityas explicit regularization signals to enhance codebook utilization and promotetoken diversity. RGVQ introduces soft assignments via Gumbel-Softmaxreparameterization, ensuring that all codewords receive gradient updates. Inaddition, RGVQ incorporates a structure-aware contrastive regularization topenalize the token co-assignments among similar node pairs. Extensiveexperiments demonstrate that RGVQ substantially improves codebook utilizationand consistently boosts the performance of state-of-the-art graph VQ backbonesacross multiple downstream tasks, enabling more expressive and transferablegraph token representations.</description>
      <author>example@mail.com (Zian Zhai, Fan Li, Xingyu Tan, Xiaoyang Wang, Wenjie Zhang)</author>
      <guid isPermaLink="false">2508.06588v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution</title>
      <link>http://arxiv.org/abs/2508.06584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Omni模型，一种具有全几何编码器的地理空间实体解析模型，能够处理多种几何形状，并在测试中表现出色&lt;h4&gt;背景&lt;/h4&gt;地理空间数据库的开发、集成和维护依赖于高效准确的实体解析匹配程序，但具有多样几何形状的实体解析被忽视，现有方法将复杂几何形状简化为点导致信息丢失&lt;h4&gt;目的&lt;/h4&gt;开发能够无缝嵌入异构几何形状到神经网络框架的统一技术，解决现有方法无法保留复杂几何形状空间信息的问题&lt;h4&gt;方法&lt;/h4&gt;提出Omni模型，包含能够嵌入点、线、折线、多边形和多边形几何形状的全几何编码器，并通过属性亲和力机制利用transformer预训练语言模型处理文本属性&lt;h4&gt;主要发现&lt;/h4&gt;Omni相比现有方法实现了高达12%的F1值提升；大型语言模型在地理空间实体解析任务中展现出有竞争力的性能&lt;h4&gt;结论&lt;/h4&gt;Omni模型能有效处理复杂几何形状的地理空间实体解析，大型语言模型在这一领域具有应用潜力&lt;h4&gt;翻译&lt;/h4&gt;地理空间数据库的开发、集成和维护在很大程度上依赖于地理空间实体解析的高效准确的匹配程序。虽然兴趣点的解析已被广泛研究，但具有多样几何形状的实体解析在很大程度上被忽视。这部分是由于缺乏将异构几何形状无缝嵌入到神经网络框架中的统一技术。现有的神经方法将复杂几何形状简化为单个点，导致大量空间信息丢失。为解决这一限制，我们提出了Omni，一种具有全几何编码器的地理空间ER模型。该编码器能够嵌入点、线、折线、多边形和多边形几何形状，使模型能够捕获被比较场所的复杂地理空间细微差别。此外，Omni通过属性亲和力机制利用基于transformer的预训练语言模型处理场所记录的各个文本属性。该模型在现有的仅有点数据集和一个新的多样几何形状地理空间ER数据集上得到了严格测试。Omni相比现有方法实现了高达12%(F1)的改进。此外，我们测试了大型语言模型进行地理空间解析的潜力，实验了提示策略和学习场景，比较了基于预训练语言模型的方法与LLMs的结果。结果表明LLMs具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development, integration, and maintenance of geospatial databases relyheavily on efficient and accurate matching procedures of Geospatial EntityResolution (ER). While resolution of points-of-interest (POIs) has been widelyaddressed, resolution of entities with diverse geometries has been largelyoverlooked. This is partly due to the lack of a uniform technique for embeddingheterogeneous geometries seamlessly into a neural network framework. Existingneural approaches simplify complex geometries to a single point, resulting insignificant loss of spatial information. To address this limitation, we proposeOmni, a geospatial ER model featuring an omni-geometry encoder. This encoder iscapable of embedding point, line, polyline, polygon, and multi-polygongeometries, enabling the model to capture the complex geospatial intricacies ofthe places being compared. Furthermore, Omni leverages transformer-basedpre-trained language models over individual textual attributes of place recordsin an Attribute Affinity mechanism. The model is rigorously tested on existingpoint-only datasets and a new diverse-geometry geospatial ER dataset. Omniproduces up to 12% (F1) improvement over existing methods.  Furthermore, we test the potential of Large Language Models (LLMs) to conductgeospatial ER, experimenting with prompting strategies and learning scenarios,comparing the results of pre-trained language model-based methods with LLMs.Results indicate that LLMs show competitive results.</description>
      <author>example@mail.com (Kalana Wijegunarathna, Kristin Stock, Christopher B. Jones)</author>
      <guid isPermaLink="false">2508.06584v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>VGGSounder: Audio-Visual Evaluations for Foundation Models</title>
      <link>http://arxiv.org/abs/2508.08237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the IEEE/CVF International Conference on Computer  Vision (ICCV) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对视听基础模型评估中的数据集局限性问题，提出了一个全面重新注释的多标签测试集VGGSounder，解决了原数据集标签不完整、类别重叠和模态未对齐等问题，能够更准确地评估模型的多模态理解能力。&lt;h4&gt;背景&lt;/h4&gt;随着视听基础模型的兴起，可靠评估其多模态理解能力变得尤为重要。VGGSounder数据集常被用作视听分类评估的基准。&lt;h4&gt;目的&lt;/h4&gt;解决VGGSounder数据集的局限性，包括标签不完整、类别部分重叠和模态未对齐等问题，提供更准确的视听基础模型评估方法。&lt;h4&gt;方法&lt;/h4&gt;创建了一个全面重新注释的多标签测试集VGGSounder，扩展了VGGSound，专门用于评估视听基础模型。该数据集包含详细的模态注释，能够进行精确的模态特定性能分析。同时，通过新的模态混淆指标分析添加另一个输入模态时的性能下降，揭示模型局限性。&lt;h4&gt;主要发现&lt;/h4&gt;原VGGSounder数据集存在标签不完整、类别部分重叠和模态未对齐等局限性，导致视听能力的扭曲评估。新的VGGSounder数据集通过详细模态注释能够精确分析模态特定性能，并通过模态混淆指标揭示了模型的局限性。&lt;h4&gt;结论&lt;/h4&gt;新的VGGSounder数据集解决了原数据集的局限性，能够更准确地评估视听基础模型的多模态理解能力，为视听模型评估提供了更可靠的基准。&lt;h4&gt;翻译&lt;/h4&gt;视听基础模型的兴起凸显了可靠评估其多模态理解能力的重要性。VGGSounder数据集常被用作视听分类评估的基准。然而，我们的分析发现了VGGSounder的几个局限性，包括标签不完整、类别部分重叠和模态未对齐。这些导致了视听能力的扭曲评估。为解决这些局限性，我们引入了VGGSounder，这是一个全面重新注释的多标签测试集，扩展了VGGSound，并专门设计用于评估视听基础模型。VGGSounder具有详细的模态注释，能够进行精确的模态特定性能分析。此外，我们通过使用新的模态混淆指标分析添加另一个输入模态时的性能下降，揭示了模型的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of audio-visual foundation models underscores the importance ofreliably assessing their multi-modal understanding. The VGGSounder dataset iscommonly used as a benchmark for evaluation audio-visual classification.However, our analysis identifies several limitations of VGGSounder, includingincomplete labelling, partially overlapping classes, and misaligned modalities.These lead to distorted evaluations of auditory and visual capabilities. Toaddress these limitations, we introduce VGGSounder, a comprehensivelyre-annotated, multi-label test set that extends VGGSound and is specificallydesigned to evaluate audio-visual foundation models. VGGSounder featuresdetailed modality annotations, enabling precise analyses of modality-specificperformance. Furthermore, we reveal model limitations by analysing performancedegradation when adding another input modality with our new modality confusionmetric.</description>
      <author>example@mail.com (Daniil Zverev, Thaddäus Wiedemer, Ameya Prabhu, Matthias Bethge, Wieland Brendel, A. Sophia Koepke)</author>
      <guid isPermaLink="false">2508.08237v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>RedDino: A foundation model for red blood cell analysis</title>
      <link>http://arxiv.org/abs/2508.08180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RedDino是一个专为红细胞图像分析设计的自监督基础模型，在红细胞形状分类方面表现优于现有最先进模型，具有强大的特征表示能力和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;红细胞对人类健康至关重要，其精确形态分析对诊断血液疾病很重要，但针对红细胞分析的全面AI解决方案仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于红细胞图像分析的基础模型，以解决计算血液学中的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;RedDino使用DINOv2自学习框架的红细胞特定适配，在一个包含125万张来自不同获取方式和来源的红细胞图像的精心策划数据集上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;广泛评估显示RedDino在红细胞形状分类方面优于现有最先进模型；通过线性探测和最近邻分类评估，确认了其强大的特征表示能力和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;RedDino通过捕捉细微的形态特征，解决了计算血液学中的关键挑战，推进了可靠诊断工具的发展。源代码和预训练模型可在GitHub和Hugging Face上获取。&lt;h4&gt;翻译&lt;/h4&gt;红细胞(RBCs)对人类健康至关重要，其精确形态分析对诊断血液疾病很重要。尽管基础模型在医学诊断中有前景，但针对红细胞分析的全面AI解决方案仍然稀缺。我们提出了RedDino，一个专为红细胞图像分析设计的自监督基础模型。RedDino使用了DINOv2自学习框架的红细胞特定适配，并在一个包含125万张来自不同获取方式和来源的红细胞图像的精心策划数据集上进行训练。广泛评估显示RedDino在红细胞形状分类方面优于现有最先进模型。通过线性探测和最近邻分类评估，我们确认了其强大的特征表示能力和泛化能力。我们的主要贡献是：(1)一个专为红细胞分析定制的基础模型，(2)探索DINOv2配置用于红细胞建模的消融研究，以及(3)对泛化性能的详细评估。RedDino通过捕捉细微的形态特征，解决了计算血液学中的关键挑战，推进了可靠诊断工具的发展。RedDino的源代码和预训练模型可在https://github.com/Snarci/RedDino获取，预训练模型可从我们的Hugging Face集合https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc下载。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Red blood cells (RBCs) are essential to human health, and their precisemorphological analysis is important for diagnosing hematological disorders.Despite the promise of foundation models in medical diagnostics, comprehensiveAI solutions for RBC analysis remain scarce. We present RedDino, aself-supervised foundation model designed for RBC image analysis. RedDino usesan RBC-specific adaptation of the DINOv2 self-supervised learning framework andis trained on a curated dataset of 1.25 million RBC images from diverseacquisition modalities and sources. Extensive evaluations show that RedDinooutperforms existing state-of-the-art models on RBC shape classification.Through assessments including linear probing and nearest neighborclassification, we confirm its strong feature representations andgeneralization ability. Our main contributions are: (1) a foundation modeltailored for RBC analysis, (2) ablation studies exploring DINOv2 configurationsfor RBC modeling, and (3) a detailed evaluation of generalization performance.RedDino addresses key challenges in computational hematology by capturingnuanced morphological features, advancing the development of reliablediagnostic tools. The source code and pretrained models for RedDino areavailable at https://github.com/Snarci/RedDino, and the pretrained models canbe downloaded from our Hugging Face collection athttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc</description>
      <author>example@mail.com (Luca Zedda, Andrea Loddo, Cecilia Di Ruberto, Carsten Marr)</author>
      <guid isPermaLink="false">2508.08180v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Imaging</title>
      <link>http://arxiv.org/abs/2508.08107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述全面介绍了高光谱成像技术，包括其物理原理、传感器架构、数据处理方法、应用领域以及面临的挑战和未来发展方向&lt;h4&gt;背景&lt;/h4&gt;高光谱成像是一种先进的传感模式，能够同时捕获空间和光谱信息，实现对材料、化学和生物特性的非侵入式、无标记分析&lt;h4&gt;目的&lt;/h4&gt;提供高光谱成像技术的全面概述，从基础原理到实际应用，并讨论面临的挑战和未来发展方向&lt;h4&gt;方法&lt;/h4&gt;介绍高光谱成像的数据获取、校准和校正技术，总结常见数据结构和分析方法，包括降维、分类、光谱解混和人工智能驱动的技术，以及计算成像、物理信息建模、跨模态融合和自监督学习等新兴解决方案&lt;h4&gt;主要发现&lt;/h4&gt;高光谱成像在地球观测、精准农业、生物医学、工业检测、文化遗产和安全等领域有广泛应用，能够揭示亚视觉特征用于高级监测和决策；面临硬件权衡、获取变异性和高维数据复杂性等挑战&lt;h4&gt;结论&lt;/h4&gt;高光谱成像正朝着可扩展、实时和嵌入式系统发展，由传感器小型化、自监督学习和基础模型驱动，有望成为跨学科平台，在科学、技术和社会领域实现变革性应用&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)是一种先进的传感模式，能够同时捕获空间和光谱信息，实现对材料、化学和生物特性的非侵入式、无标记分析。本综述全面介绍了HSI，从基本物理原理和传感器架构到数据获取、校准和校正的关键步骤。我们总结了常见数据结构，并强调了经典和现代分析方法，包括降维、分类、光谱解混以及人工智能驱动的技术（如深度学习）。我们还讨论了在地球观测、精准农业、生物医学、工业检测、文化遗产和安全等领域的代表性应用，强调了HSI揭示亚视觉特征以实现高级监测、诊断和决策制定的能力。我们审视了持久的挑战，如硬件权衡、获取变异性以及高维数据的复杂性，同时探讨了新兴解决方案，包括计算成像、物理信息建模、跨模态融合和自监督学习。我们进一步强调了数据集共享、可重复性和元数据文档记录的最佳实践，以支持透明度和重用。展望未来，我们探索了未来朝着可扩展、实时和嵌入式HSI系统发展的方向，这一趋势由传感器小型化、自监督学习和基础模型驱动。随着HSI演变为通用、跨学科平台，它在科学、技术和社会的变革性应用方面具有巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) is an advanced sensing modality thatsimultaneously captures spatial and spectral information, enablingnon-invasive, label-free analysis of material, chemical, and biologicalproperties. This Primer presents a comprehensive overview of HSI, from theunderlying physical principles and sensor architectures to key steps in dataacquisition, calibration, and correction. We summarize common data structuresand highlight classical and modern analysis methods, including dimensionalityreduction, classification, spectral unmixing, and AI-driven techniques such asdeep learning. Representative applications across Earth observation, precisionagriculture, biomedicine, industrial inspection, cultural heritage, andsecurity are also discussed, emphasizing HSI's ability to uncover sub-visualfeatures for advanced monitoring, diagnostics, and decision-making. Persistentchallenges, such as hardware trade-offs, acquisition variability, and thecomplexity of high-dimensional data, are examined alongside emerging solutions,including computational imaging, physics-informed modeling, cross-modal fusion,and self-supervised learning. Best practices for dataset sharing,reproducibility, and metadata documentation are further highlighted to supporttransparency and reuse. Looking ahead, we explore future directions towardscalable, real-time, and embedded HSI systems, driven by sensorminiaturization, self-supervised learning, and foundation models. As HSIevolves into a general-purpose, cross-disciplinary platform, it holds promisefor transformative applications in science, technology, and society.</description>
      <author>example@mail.com (Danfeng Hong, Chenyu Li, Naoto Yokoya, Bing Zhang, Xiuping Jia, Antonio Plaza, Paolo Gamba, Jon Atli Benediktsson, Jocelyn Chanussot)</author>
      <guid isPermaLink="false">2508.08107v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.07996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ProGraD方法，通过可学习群体提示和轻量级GroupContext Transformer，有效利用视觉基础模型进行群体活动检测，在多群体场景中表现尤为突出。&lt;h4&gt;背景&lt;/h4&gt;群体活动检测(GAD)涉及在视频中识别社会群体及其集体行为。视觉基础模型(VFMs)如DinoV2虽然提供了优秀特征，但主要在以物体为中心的数据上预训练，在建模群体动力学方面探索不足。简单替换CNN主干网络为VFMs收益有限，表明需要结构化的群体感知推理。&lt;h4&gt;目的&lt;/h4&gt;弥合视觉基础模型与群体活动检测之间的差距，开发一种方法使VFMs能够更好地捕捉群体动态和社会配置。&lt;h4&gt;方法&lt;/h4&gt;提出提示驱动的群体活动检测(ProGraD)，包括：1)可学习的群体提示，引导VFM注意力朝向社会配置；2)轻量级的两层GroupContext Transformer，用于推断参与者-群体关联和集体行为。&lt;h4&gt;主要发现&lt;/h4&gt;在Cafe和Social-CAD两个基准测试上超越了最先进方法；在复杂多群体场景中特别有效，仅使用1000万可训练参数，获得了6.5%(Group mAP@1.0)和8.2%(Group mAP@0.5)的提升；ProGraD产生可解释的注意力图，为参与者-群体推理提供见解。&lt;h4&gt;结论&lt;/h4&gt;ProGraD是一种有效的群体活动检测方法，能够充分利用视觉基础模型的优势，特别是在复杂的多群体场景中表现突出。代码和模型将被发布。&lt;h4&gt;翻译&lt;/h4&gt;群体活动检测(GAD)涉及在视频中识别社会群体及其集体行为。视觉基础模型(VFMs)，如DinoV2，提供了优秀的特征，但主要在以物体为中心的数据上进行预训练，在建模群体动力学方面探索不足。虽然它们是高度特定于GAD架构的有前途的替代方案，但这些架构需要完全微调，我们初步调查发现，简单地将这些方法中使用的CNN主干网络替换为VFMs带来的收益很小，这强调了在VFM之上进行结构化的、群体感知推理的必要性。我们提出了提示驱动的群体活动检测(ProGraD)——一种通过以下方式弥合这一差距的方法：1)可学习的群体提示，引导VFM注意力朝向社会配置；2)轻量级的两层GroupContext Transformer，用于推断参与者-群体关联和集体行为。我们在两个最近的GAD基准测试上评估了我们的方法：Cafe(具有多个并发社会群体)和Social-CAD(专注于单群体互动)。虽然我们在两种设置中都超越了最先进的方法，但我们的方法在复杂的多群体场景中特别有效，仅使用1000万可训练参数，我们获得了6.5%(Group mAP@1.0)和8.2%(Group mAP@0.5)的提升。此外，我们的实验表明ProGraD产生可解释的注意力图，为参与者-群体推理提供了见解。代码和模型将被发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Group Activity Detection (GAD) involves recognizing social groups and theircollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,offer excellent features, but are pretrained primarily on object-centric dataand remain underexplored for modeling group dynamics. While they are apromising alternative to highly task-specific GAD architectures that requirefull fine-tuning, our initial investigation reveals that simply swapping CNNbackbones used in these methods with VFMs brings little gain, underscoring theneed for structured, group-aware reasoning on top.  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a methodthat bridges this gap through 1) learnable group prompts to guide the VFMattention toward social configurations, and 2) a lightweight two-layerGroupContext Transformer that infers actor-group associations and collectivebehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, whichfeatures multiple concurrent social groups, and Social-CAD, which focuses onsingle-group interactions. While we surpass state-of-the-art in both settings,our method is especially effective in complex multi-group scenarios, where weyield a gain of 6.5\% (Group mAP\@1.0) and 8.2\% (Group mAP\@0.5) using only10M trainable parameters. Furthermore, our experiments reveal that ProGraDproduces interpretable attention maps, offering insights into actor-groupreasoning. Code and models will be released.</description>
      <author>example@mail.com (Thinesh Thiyakesan Ponbagavathi, Chengzheng Yang, Alina Roitberg)</author>
      <guid isPermaLink="false">2508.07996v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>MolmoAct: Action Reasoning Models that can Reason in Space</title>
      <link>http://arxiv.org/abs/2508.07917v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Appendix on Blogpost: https://allenai.org/blog/molmoact&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了行动推理模型(ARMs)，特别是MolmoAct模型，通过结构化的三阶段管道整合感知、规划和控制，在多种任务中表现出色，并发布了首个机器人行动推理数据集。&lt;h4&gt;背景&lt;/h4&gt;推理是目的性行动的核心，但大多数机器人基础模型直接将感知和指令映射到控制，这限制了适应性、泛化和语义基础。&lt;h4&gt;目的&lt;/h4&gt;开发一类整合感知、规划和控制的视觉-语言-动作模型，通过结构化推理提高机器人的适应性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;MolmoAct模型通过三阶段管道工作：将观察和指令编码为深度感知标记，生成中级空间计划作为可编辑轨迹轨迹，预测精确低级行动，使行为可解释和可引导。&lt;h4&gt;主要发现&lt;/h4&gt;MolmoAct-7B-D在SimplerEnv视觉匹配任务上达到70.5%的零样本准确率；在LIBERO上平均86.6%的成功率；真实世界微调中比基线提高10%-22.7%；分布外泛化提高23.3%；使用MolmoAct数据集训练性能提高5.5%。&lt;h4&gt;结论&lt;/h4&gt;MolmoAct作为尖端的机器人基础模型和构建ARMs的开蓝图，通过结构化推理将感知转化为目的性行动，所有模型权重、代码和数据集已公开。&lt;h4&gt;翻译&lt;/h4&gt;推理是目的性行动的核心，然而大多数机器人基础模型直接将感知和指令映射到控制，这限制了适应性、泛化和语义基础。我们引入了行动推理模型(ARMs)，这是一类通过结构化三阶段管道整合感知、规划和控制的视觉-语言-动作模型。我们的模型MolmoAct将观察和指令编码为深度感知标记，生成中级空间计划作为可编辑轨迹轨迹，并预测精确的低级行动，使行为可解释和可引导。MolmoAct-7B-D在模拟和真实世界环境中表现出色：在SimplerEnv视觉匹配任务上达到70.5%的零样本准确率，优于闭源Pi-0和GR00T N1；在LIBERO上平均86.6%的成功率，比ThinkAct在长距离任务上额外提高6.3%；在真实世界微调中，比Pi-0-FAST额外提高10%（单臂）和22.7%（双臂）任务进展；在分布外泛化上比基线额外提高23.3%；在开放式指令跟随和轨迹引导方面获得最高的人类偏好分数。此外，我们首次发布了MolmoAct数据集，包含10,000多条跨多样化场景和任务的高质量机器人轨迹。使用此数据集训练比基础模型平均提高5.5%的性能。我们发布所有模型权重、训练代码、收集的数据集和行动推理数据集，使MolmoAct成为尖端的机器人基础模型和构建通过结构化推理将感知转化为目的性行动的ARMs的开蓝图。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是机器人基础模型直接将感知和指令映射到控制的问题，这限制了机器人的适应性、泛化能力和语义基础。这个问题在现实中很重要，因为机器人需要像人类一样学会推理，而不仅仅是机械地执行指令；在研究中很重要，因为当前的VLA模型仍然脆弱且不透明，难以跨任务、场景或形态迁移，且缺乏对决策过程的解释性。机器人需要精细的、具身化的交互数据，而这些数据成本高昂、模糊且难以规模化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为机器人必须学会推理，就像人类在行动前会潜意识地权衡上下文、目标和约束。他们注意到语言模型已开始从蛮力扩展转向结构化学习，构建支持推理、抽象和控制的中间表示。作者借鉴了Molmo多模态开放语言模型的视觉-语言主干，采用了链式思维(CoT)推理方法但扩展到空间推理，利用VQVAE进行深度感知表示学习，并受到多模态链式思维(MCoT)的启发。作者改进了动作令牌的初始化方式，使其更好地保持动作空间的几何特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'在空间中推理'，教导模型明确推理深度和具体的、与图像对齐的运动草图，然后再发出低级动作。整体实现流程包括三阶段推理管道：1)深度感知令牌生成：使用深度估计器和VQVAE将深度图量化为令牌序列；2)视觉推理轨迹生成：提取机器人夹爪坐标并构建包含当前点、最终点和中间点的轨迹；3)动作令牌生成：基于前两个阶段的输出预测离散化的动作。训练流程分为预训练、中间训练和后训练三个阶段，分别在混合数据、MolmoAct数据集和特定任务数据上进行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)行动推理模型(ARMs)架构，整合感知、规划和控制；2)深度感知令牌，显式整合深度信息；3)视觉推理轨迹，提供精确的运动规划；4)可引导的行为，通过编辑轨迹线实现直接动作引导；5)完全开放模型和数据。相比之前的工作，MolmoAct引入了显式的中间推理阶段而非直接映射；专注于空间推理而非纯语言推理；用更少数据实现了更好性能；提供了决策过程的可解释视图；支持双模态控制（语言和视觉轨迹）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MolmoAct引入了行动推理模型(ARMs)，通过结构化的三阶段推理管道实现了机器人在空间中的推理能力，使机器人能够产生可解释和可引导的行为，同时在各种模拟和现实世界任务中实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning is central to purposeful action, yet most robotic foundation modelsmap perception and instructions directly to control, which limits adaptability,generalization, and semantic grounding. We introduce Action Reasoning Models(ARMs), a class of vision-language-action models that integrate perception,planning, and control through a structured three-stage pipeline. Our model,MolmoAct, encodes observations and instructions into depth-aware perceptiontokens, generates mid-level spatial plans as editable trajectory traces, andpredicts precise low-level actions, enabling explainable and steerablebehavior. MolmoAct-7B-D achieves strong performance across simulation andreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matchingtasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success onLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;and in real-world fine-tuning, an additional 10% (single-arm) and an additional22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselinesby an additional 23.3% on out-of-distribution generalization and achieves tophuman-preference scores for open-ended instruction following and trajectorysteering. Furthermore, we release, for the first time, the MolmoAct Dataset --a mid-training robot dataset comprising over 10,000 high quality robottrajectories across diverse scenarios and tasks. Training with this datasetyields an average 5.5% improvement in general performance over the base model.We release all model weights, training code, our collected dataset, and ouraction reasoning dataset, establishing MolmoAct as both a state-of-the-artrobotics foundation model and an open blueprint for building ARMs thattransform perception into purposeful action through structured reasoning.Blogpost: https://allenai.org/blog/molmoact</description>
      <author>example@mail.com (Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna)</author>
      <guid isPermaLink="false">2508.07917v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Multi-agent systems for chemical engineering: A review and perspective</title>
      <link>http://arxiv.org/abs/2508.07880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述调查了大型语言模型驱动的多智能体系统在化学工程领域的最新进展，尽管早期研究显示出有希望的结果，但仍面临多个科学挑战，包括架构设计、数据集成、模型开发和安全性等方面。作为一个新兴但快速发展的领域，多智能体系统为重新思考化学工程工作流程提供了机遇。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型驱动的多智能体系统是一项新兴但快速发展的技术，具有通过将复杂工作流程分解为具有专业知识和工具的协作智能体团队来变革化学工程的潜力。&lt;h4&gt;目的&lt;/h4&gt;这篇综述调查了化学工程中多智能体系统的最先进技术。&lt;h4&gt;方法&lt;/h4&gt;通过将复杂工作流程分解为具有专业知识和工具的协作智能体团队来构建和评估多智能体系统。&lt;h4&gt;主要发现&lt;/h4&gt;早期研究显示出有希望的结果，但仍然存在多个科学挑战，包括定制架构的设计、异构数据模态的集成、具有领域特定模态的基础模型的开发，以及确保透明度、安全性和环境影响的策略。&lt;h4&gt;结论&lt;/h4&gt;作为一个年轻但快速发展的领域，多智能体系统为重新思考化学工程工作流程提供了令人兴奋的机会。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型驱动的多智能体系统是一项新兴但快速发展的技术，具有通过将复杂工作流程分解为具有专业知识和工具的协作智能体团队来变革化学工程的潜力。这篇综述调查了化学工程中多智能体系统的最先进技术。虽然早期研究显示出有希望的结果，但仍然存在科学挑战，包括定制架构的设计、异构数据模态的集成、具有领域特定模态的基础模型的开发，以及确保透明度、安全性和环境影响的策略。作为一个年轻但快速发展的领域，多智能体系统为重新思考化学工程工作流程提供了令人兴奋的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language model (LLM)-based multi-agent systems (MASs) are a recent butrapidly evolving technology with the potential to transform chemicalengineering by decomposing complex workflows into teams of collaborative agentswith specialized knowledge and tools. This review surveys the state-of-the-artof MAS within chemical engineering. While early studies demonstrate promisingresults, scientific challenges remain, including the design of tailoredarchitectures, integration of heterogeneous data modalities, development offoundation models with domain-specific modalities, and strategies for ensuringtransparency, safety, and environmental impact. As a young but fast-movingfield, MASs offer exciting opportunities to rethink chemical engineeringworkflows.</description>
      <author>example@mail.com (Sophia Rupprecht, Qinghe Gao, Tanuj Karia, Artur M. Schweidtmann)</author>
      <guid isPermaLink="false">2508.07880v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</title>
      <link>http://arxiv.org/abs/2508.07819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 1 reference, 3 figures, icassp 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种架构协同设计框架，解决了预训练视觉-语言模型在零样本异常检测中面临的适应差距问题，通过整合卷积低秩适应适配器和动态融合网关，显著提升了模型在密集感知任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;预训练的视觉-语言模型在应用于零样本异常检测时存在显著的适应差距，主要源于模型缺乏密集预测所需的局部归纳偏置，以及对不灵活的特征融合范式的依赖。&lt;h4&gt;目的&lt;/h4&gt;通过架构协同设计框架解决预训练视觉-语言模型在零样本异常检测中的适应性问题，共同优化特征表示和跨模态融合。&lt;h4&gt;方法&lt;/h4&gt;整合参数高效的卷积低秩适应(Conv-LoRA)适配器为细粒度表示注入局部归纳偏置，并引入动态融合网关(DFG)，利用视觉上下文自适应调制文本提示，实现强大的双向融合。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的工业和医学基准测试中进行了广泛的实验，结果表明该方法具有优越的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这种协同的架构设计对于将基础模型稳健地适应密集感知任务是至关重要的。&lt;h4&gt;翻译&lt;/h4&gt;预训练的视觉-语言模型在应用于零样本异常检测时面临显著的适应差距，这源于它们缺乏密集预测的局部归纳偏置以及对不灵活特征融合范式的依赖。我们通过架构协同设计框架解决了这些局限性，该框架共同优化了特征表示和跨模态融合。我们的方法整合了参数高效的卷积低秩适应(Conv-LoRA)适配器，为细粒度表示注入局部归纳偏置，并引入了动态融合网关(DFG)，该网关利用视觉上下文自适应调制文本提示，实现强大的双向融合。在多样化的工业和医学基准测试中的大量实验证明了其优越的准确性和鲁棒性，验证了这种协同的协同设计对于将基础模型稳健地适应密集感知任务是至关重要的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained Vision-Language Models (VLMs) face a significant adaptation gapwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack oflocal inductive biases for dense prediction and their reliance on inflexiblefeature fusion paradigms. We address these limitations through an ArchitecturalCo-Design framework that jointly refines feature representation and cross-modalfusion. Our method integrates a parameter-efficient Convolutional Low-RankAdaptation (Conv-LoRA) adapter to inject local inductive biases forfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) thatleverages visual context to adaptively modulate text prompts, enabling apowerful bidirectional fusion. Extensive experiments on diverse industrial andmedical benchmarks demonstrate superior accuracy and robustness, validatingthat this synergistic co-design is critical for robustly adapting foundationmodels to dense perception tasks.</description>
      <author>example@mail.com (Ke Ma, Jun Long, Hongxiao Fei, Liujie Hua, Yueyi Luo)</author>
      <guid isPermaLink="false">2508.07819v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>An Experimental Reservoir-Augmented Foundation Model: 6G O-RAN Case Study</title>
      <link>http://arxiv.org/abs/2508.07778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RA-MAT的时间序列基础模型，用于解决6G O-RAN中超高维、非平稳时间序列数据的分析挑战，实现了高效、低延迟的KPI预测。&lt;h4&gt;背景&lt;/h4&gt;下一代开放无线接入网络(O-RAN)持续传输大量关键性能指标(KPIs)和原始同相/正交(IQ)样本，产生超高维、非平稳的时间序列数据，这些数据超出了传统Transformer架构的处理能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种满足6G O-RAN测试严格延迟、能效和可扩展性要求的时间序列基础模型，实现实时基础级分析。&lt;h4&gt;方法&lt;/h4&gt;引入基于水库增强的掩码自编码Transformer(RA-MAT)，结合回声状态网络(ESN)计算与掩码自编码技术，通过固定随机初始化的ESN将时间块投影为动态嵌入，将二次自注意力转换为线性操作，使用30%随机掩码块重建进行自监督学习，最后通过浅层任务头微调实现低足迹适应。&lt;h4&gt;主要发现&lt;/h4&gt;在O-RAN KPI案例研究中，RA-MAT在多个连续和离散KPI上实现了低于0.06的均方误差(MSE)，展示了优异的性能。&lt;h4&gt;结论&lt;/h4&gt;RA-MAT为未来6G网络实现实时基础级分析提供了实用途径，满足了高维时间序列分析的效率和可扩展性需求。&lt;h4&gt;翻译&lt;/h4&gt;下一代开放无线接入网络(O-RAN)持续传输大量关键性能指标(KPIs)和原始同相/正交(IQ)样本，产生超高维、非平稳的时间序列数据，这些数据超出了传统Transformer架构的处理能力。我们引入了一种基于水库增强的掩码自编码Transformer(RA-MAT)。这种时间序列基础模型使用回声状态网络(ESN)计算结合掩码自编码，以满足6G O-RAN测试对延迟、能效和可扩展性的严格要求。一个固定随机初始化的ESN无需通过时间反向传播，即可将每个时间块快速投影到丰富的动态嵌入中，将二次自注意力瓶颈转换为轻量级线性操作。这些嵌入驱动一个块级掩码自编码器，该自编码器重建30%随机掩码的块，强制编码器从未标记数据中捕获局部动力学和长程结构。在自监督预训练后，RA-MAT使用浅层任务头进行微调，同时保持水库和大部分Transformer层冻结，实现对各种下游任务(如O-RAN KPI预测)的低足迹适应。在全面的O-RAN KPI案例研究中，RA-MAT在几个连续和离散KPI上实现了低于0.06的均方误差(MSE)。这项工作将RA-MAT定位为未来6G网络实现实时基础级分析的实际途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation open radio access networks (O-RAN) continuously stream tensof key performance indicators (KPIs) together with raw in-phase/quadrature (IQ)samples, yielding ultra-high-dimensional, non-stationary time series thatoverwhelm conventional transformer architectures. We introduce areservoir-augmented masked autoencoding transformer (RA-MAT). This time seriesfoundation model employs echo state network (ESN) computing with maskedautoencoding to satisfy the stringent latency, energy efficiency, andscalability requirements of 6G O-RAN testing. A fixed, randomly initialized ESNrapidly projects each temporal patch into a rich dynamical embedding withoutbackpropagation through time, converting the quadratic self-attentionbottleneck into a lightweight linear operation. These embeddings drive apatch-wise masked autoencoder that reconstructs 30% randomly masked patches,compelling the encoder to capture both local dynamics and long-range structurefrom unlabeled data. After self-supervised pre-training, RA-MAT is fine-tunedwith a shallow task head while keeping the reservoir and most transformerlayers frozen, enabling low-footprint adaptation to diverse downstream taskssuch as O-RAN KPI forecasting. In a comprehensive O-RAN KPI case study, RA-MATachieved sub-0.06 mean squared error (MSE) on several continuous and discreteKPIs. This work positions RA-MAT as a practical pathway toward real-time,foundation-level analytics in future 6G networks.</description>
      <author>example@mail.com (Farhad Rezazadeh, Raymond Zhao, Jiongyu Dai, Amir Ashtari Gargari, Hatim Chergui, Lingjia Liu)</author>
      <guid isPermaLink="false">2508.07778v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild</title>
      <link>http://arxiv.org/abs/2508.07759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为CAV-SAM的新方法，通过将参考-目标图像对的对应关系表示为伪视频，利用SAM2的交互式视频对象分割能力，轻量级地将视觉模型适应到下游任务，避免了传统元学习方法的高昂成本。&lt;h4&gt;背景&lt;/h4&gt;大型视觉模型如SAM在应用于野外下游任务时存在显著局限性。现有的参考分割方法主要依赖元学习，但元学习需要大量元训练过程，带来巨大的数据和计算成本。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级方法来适应视觉模型到下游任务，避免元学习方法带来的高成本问题。&lt;h4&gt;方法&lt;/h4&gt;将参考-目标图像对之间的固有对应关系表示为伪视频，利用具有交互式视频对象分割能力的SAM2。CAV-SAM包含两个关键模块：基于扩散的语义转换(DBST)模块使用扩散模型构建语义转换序列；测试时几何对齐(TTGA)模块通过测试时微调对齐序列中的几何变化。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛使用的数据集上评估CAV-SAM，其分割性能比最先进方法提高了超过5%。&lt;h4&gt;结论&lt;/h4&gt;CAV-SAM是一种轻量级方法，能够有效适应视觉模型到下游任务，实现已在补充材料中提供。&lt;h4&gt;翻译&lt;/h4&gt;像Segment Anything Model(SAM)这样的大型视觉模型在应用于野外下游任务时表现出显著的局限性。因此，参考分割(reference segmentation)作为一种有前景的新方向出现，它利用参考图像及其对应的掩码为模型注入新知识。然而，现有的参考分割方法主要依赖元学习，这仍然需要大量的元训练过程并带来巨大的数据和计算成本。在本研究中，我们提出了一种新颖的方法，将参考-目标图像对之间的固有对应关系表示为伪视频。这一视角使得具有交互式视频对象分割(iVOS)能力的SAM2最新版本能够以轻量级方式适应下游任务。我们将这种方法命名为Correspondence As Video for SAM (CAV-SAM)。CAV-SAM包含两个关键模块：基于扩散的语义转换(DBST)模块使用扩散模型构建语义转换序列，而测试时几何对齐(TTGA)模块通过测试时微调对齐此序列中的几何变化。我们在广泛使用的数据集上评估了CAV-SAM，其分割性能比最先进方法提高了超过5%。实现已在补充材料中提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision models like the Segment Anything Model (SAM) exhibit significantlimitations when applied to downstream tasks in the wild. Consequently,reference segmentation, which leverages reference images and theircorresponding masks to impart novel knowledge to the model, emerges as apromising new direction for adapting vision models. However, existing referencesegmentation approaches predominantly rely on meta-learning, which stillnecessitates an extensive meta-training process and brings massive data andcomputational cost. In this study, we propose a novel approach by representingthe inherent correspondence between reference-target image pairs as a pseudovideo. This perspective allows the latest version of SAM, known as SAM2, whichis equipped with interactive video object segmentation (iVOS) capabilities, tobe adapted to downstream tasks in a lightweight manner. We term this approachCorrespondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:the Diffusion-Based Semantic Transition (DBST) module employs a diffusion modelto construct a semantic transformation sequence, while the Test-Time GeometricAlignment (TTGA) module aligns the geometric changes within this sequencethrough test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,achieving segmentation performance improvements exceeding 5% over SOTA methods.Implementation is provided in the supplementary materials.</description>
      <author>example@mail.com (Haoran Wang, Zekun Li, Jian Zhang, Lei Qi, Yinghuan Shi)</author>
      <guid isPermaLink="false">2508.07759v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification</title>
      <link>http://arxiv.org/abs/2508.07577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了LayerNorm在Vision Transformers中的微调动力学，特别是在数据稀缺和领域迁移情况下。研究发现LayerNorm参数在微调后的变化可以反映源域和目标域之间的过渡，并提出了基于微调移位比率(FSR)的重缩放机制和循环框架来增强LayerNorm微调效果。&lt;h4&gt;背景&lt;/h4&gt;LayerNorm在Vision Transformers (ViTs)中至关重要，但在数据稀缺和领域迁移情况下的微调动力学尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究LayerNorm参数在微调后的变化如何反映源域和目标域之间的过渡，并提高LayerNorm微调的效果。&lt;h4&gt;方法&lt;/h4&gt;提出了微调移位比率(FSR)来量化目标训练样本对目标域的准确代表程度；设计了一个使用与FSR负相关的标量λ的重缩放机制来对齐学习的LayerNorm shifts与理想shifts；结合循环框架进一步增强LayerNorm微调。&lt;h4&gt;主要发现&lt;/h4&gt;OOD任务比ID任务产生更低的FSR和更高的λ，特别是在数据稀缺时，表明目标训练样本代表性不足；在病理数据上微调的ViTs行为更像ID设置，倾向于保守的LayerNorm更新。&lt;h4&gt;结论&lt;/h4&gt;研究结果揭示了迁移学习中LayerNorm未被充分探索的动力学，并为LayerNorm微调提供了实用策略。&lt;h4&gt;翻译&lt;/h4&gt;LayerNorm在Vision Transformers (ViTs)中至关重要，然而在数据稀缺和领域迁移情况下的微调动力学仍未被充分探索。本文表明，微调后LayerNorm参数的变化反映了源域和目标域之间的过渡；其有效性取决于目标训练样本对目标域的准确代表程度，由我们提出的微调移位比率(FSR)量化。基于此，我们提出了一种简单而有效的重缩放机制，使用与FSR负相关的标量λ，将学习的LayerNorm shifts与在完全代表性数据下实现的理想shifts对齐，并结合一个循环框架进一步增强LayerNorm微调。在自然图像和病理图像、ID和OOD设置以及各种目标训练样本条件下的广泛实验验证了我们的框架。值得注意的是，与ID情况相比，OOD任务往往产生更低的FSR和更高的λ，特别是在数据稀缺的情况下，表明目标训练样本代表性不足。此外，在病理数据上微调的ViTs行为更接近ID设置，倾向于保守的LayerNorm更新。我们的研究结果揭示了迁移学习中LayerNorm未被充分探索的动力学，并为LayerNorm微调提供了实用策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuningdynamics under data scarcity and domain shifts remain underexplored. This papershows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)are indicative of the transitions between source and target domains; itsefficacy is contingent upon the degree to which the target training samplesaccurately represent the target domain, as quantified by our proposedFine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yeteffective rescaling mechanism using a scalar $\lambda$ that is negativelycorrelated to $FSR$ to align learned LayerNorm shifts with those ideal shiftsachieved under fully representative data, combined with a cyclic framework thatfurther enhances the LayerNorm fine-tuning. Extensive experiments acrossnatural and pathological images, in both in-distribution (ID) andout-of-distribution (OOD) settings, and various target training sample regimesvalidate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher$\lambda$ in comparison to ID cases, especially with scarce data, indicatingunder-represented target training samples. Moreover, ViTFs fine-tuned onpathological data behave more like ID settings, favoring conservative LayerNormupdates. Our findings illuminate the underexplored dynamics of LayerNorm intransfer learning and provide practical strategies for LayerNorm fine-tuning.</description>
      <author>example@mail.com (Zhaorui Tan, Tan Pan, Kaizhu Huang, Weimiao Yu, Kai Yao, Chen Jiang, Qiufeng Wang, Anh Nguyen, Xin Guo, Yuan Cheng, Xi Yang)</author>
      <guid isPermaLink="false">2508.07577v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Mega-Satellite Networks with Generative Semantic Communication: A Networking Perspective</title>
      <link>http://arxiv.org/abs/2508.07573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted paper to be published in IEEE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了将生成式语义通信(GSC)集成到巨型卫星星座中的网络视角，提出GSC赋能的卫星网络架构和关键使能技术，构建离散时间图模型，开发模型部署和路由方案，并评估性能。&lt;h4&gt;背景&lt;/h4&gt;直接卫星到设备通信的进步使巨型卫星星座成为6G无线通信的基石，实现全球无缝连接，但频谱稀缺和容量限制仍是支持多媒体应用大量数据需求的挑战。&lt;h4&gt;目的&lt;/h4&gt;从网络视角研究将生成式语义通信(GSC)集成到巨型卫星星座中，提出GSC赋能的卫星网络架构并确定关键使能技术。&lt;h4&gt;方法&lt;/h4&gt;构建离散时间图模型来建模语义编码器/解码器、知识库和资源变化；开发语义编码器和解码器的模型部署以及GSC兼容的路由方案；进行性能评估。&lt;h4&gt;主要发现&lt;/h4&gt;GSC由AI生成基础模型支持，实现了从传输原始数据到交换语义意义的范式转变，可减少带宽消耗并增强多媒体内容的关键语义特征，为克服传统卫星通信系统局限提供解决方案。&lt;h4&gt;结论&lt;/h4&gt;提出GSC赋能的卫星网络架构和关键使能技术，构建离散时间图模型，开发模型部署和路由方案，并提出未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;直接卫星到设备通信的进步使巨型卫星星座成为6G无线通信的基石，即使在偏远和服务不足的地区也能实现无缝全球连接。然而，由香农经典信息理论造成的频谱稀缺和容量限制，仍然是支持多媒体丰富无线应用大量数据需求的重大挑战。由基于人工智能的生成基础模型支持的生成式语义通信(GSC)，代表了一种从传输原始数据到交换语义意义的范式转变。GSC不仅可以减少带宽消耗，还可以增强多媒体内容中的关键语义特征，从而为克服传统卫星通信系统的局限性提供了有希望的解决方案。本文从网络视角研究了将GSC集成到巨型卫星星座中。我们提出了GSC赋能的卫星网络架构，并确定了关键的使能技术，重点关注GSC赋能的网络建模和GSC感知的网络策略。我们构建了一个离散时间图，用于建模巨型卫星网络中的语义编码器和解码器、不同的知识库和资源变化。基于此框架，我们开发了语义编码器和解码器的模型部署和GSC兼容的路由方案，然后进行了性能评估。最后，我们概述了推进GSC赋能卫星网络的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advance of direct satellite-to-device communication has positionedmega-satellite constellations as a cornerstone of 6G wireless communication,enabling seamless global connectivity even in remote and underserved areas.However, spectrum scarcity and capacity constraints imposed by the Shannon'sclassical information theory remain significant challenges for supporting themassive data demands of multimedia-rich wireless applications. GenerativeSemantic Communication (GSC), powered by artificial intelligence-basedgenerative foundation models, represents a paradigm shift from transmitting rawdata to exchanging semantic meaning. GSC can not only reduce bandwidthconsumption, but also enhance key semantic features in multimedia content,thereby offering a promising solution to overcome the limitations oftraditional satellite communication systems. This article investigates theintegration of GSC into mega-satellite constellations from a networkingperspective. We propose a GSC-empowered satellite networking architecture andidentify key enabling technologies, focusing on GSC-empowered network modelingand GSC-aware networking strategies. We construct a discrete temporal graph tomodel semantic encoders and decoders, distinct knowledge bases, and resourcevariations in mega-satellite networks. Based on this framework, we developmodel deployment for semantic encoders and decoders and GSC-compatible routingschemes, and then present performance evaluations. Finally, we outline futureresearch directions for advancing GSC-empowered satellite networks.</description>
      <author>example@mail.com (Binquan Guo, Wanting Yang, Zehui Xiong, Zhou Zhang, Baosheng Li, Zhu Han, Rahim Tafazolli, Tony Q. S. Quek)</author>
      <guid isPermaLink="false">2508.07573v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</title>
      <link>http://arxiv.org/abs/2508.07407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对自进化AI代理系统技术进行了全面综述，提出统一概念框架，分析各种自进化技术，探讨特定领域应用策略，并讨论评估、安全与伦理问题。&lt;h4&gt;背景&lt;/h4&gt;大语言模型的进步引发了人们对解决复杂现实世界任务的AI代理的兴趣，但现有系统大多依赖手动配置且部署后保持静态，难以适应动态变化的环境。&lt;h4&gt;目的&lt;/h4&gt;探索基于交互数据和环境反馈自动增强代理系统的进化技术，为自进化AI代理奠定基础，连接基础模型的静态能力和终身代理系统所需的持续适应性。&lt;h4&gt;方法&lt;/h4&gt;引入统一概念框架抽象自进化代理系统设计背后的反馈循环，突出四个关键组件(系统输入、代理系统、环境和优化器)；基于该框架系统审查各种自进化技术；调查生物医学、编程和金融等特定领域的进化策略；讨论评估、安全性和伦理考虑。&lt;h4&gt;主要发现&lt;/h4&gt;自进化代理系统可通过反馈循环实现持续适应；统一框架有助于理解和比较不同策略；特定领域需要结合领域约束的优化目标；评估、安全和伦理对确保系统有效性和可靠性至关重要。&lt;h4&gt;结论&lt;/h4&gt;为研究人员和实践者提供对自进化AI代理的系统理解，为开发更适应性、自主性和终身代理系统奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;近期大型语言模型的进步引发了人们对能够解决复杂现实世界任务的AI代理日益增长的关注。然而，大多数现有代理系统依赖于手动配置的方案，且部署后保持静态，限制了它们适应动态和不断变化环境的能力。为此，近期研究探索了代理进化技术，旨在基于交互数据和环境反馈自动增强代理系统。这一新兴方向为自进化AI代理奠定了基础，连接了基础模型的静态能力和终身代理系统所需的持续适应性。在本综述中，我们对自进化代理系统的现有技术进行了全面回顾。具体而言，我们首先介绍了一个统一的概念框架，该框架抽象了自进化代理系统设计背后的反馈循环。该框架突出了四个关键组件：系统输入、代理系统、环境和优化器，作为理解和比较不同策略的基础。基于此框架，我们系统地审查了针对代理系统不同组件的各种自进化技术。我们还调查了针对生物医学、编程和金融等专门领域开发的特定领域进化策略，其中优化目标与领域约束紧密耦合。此外，我们专门讨论了自进化代理系统的评估、安全性和伦理考虑，这对于确保其有效性和可靠性至关重要。本综述旨在为研究人员和实践者提供对自进化AI代理的系统理解，为开发更适应性、自主性和终身代理系统奠定基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in large language models have sparked growing interest in AIagents capable of solving complex, real-world tasks. However, most existingagent systems rely on manually crafted configurations that remain static afterdeployment, limiting their ability to adapt to dynamic and evolvingenvironments. To this end, recent research has explored agent evolutiontechniques that aim to automatically enhance agent systems based on interactiondata and environmental feedback. This emerging direction lays the foundationfor self-evolving AI agents, which bridge the static capabilities of foundationmodels with the continuous adaptability required by lifelong agentic systems.In this survey, we provide a comprehensive review of existing techniques forself-evolving agentic systems. Specifically, we first introduce a unifiedconceptual framework that abstracts the feedback loop underlying the design ofself-evolving agentic systems. The framework highlights four key components:System Inputs, Agent System, Environment, and Optimisers, serving as afoundation for understanding and comparing different strategies. Based on thisframework, we systematically review a wide range of self-evolving techniquesthat target different components of the agent system. We also investigatedomain-specific evolution strategies developed for specialised fields such asbiomedicine, programming, and finance, where optimisation objectives aretightly coupled with domain constraints. In addition, we provide a dedicateddiscussion on the evaluation, safety, and ethical considerations forself-evolving agentic systems, which are critical to ensuring theireffectiveness and reliability. This survey aims to provide researchers andpractitioners with a systematic understanding of self-evolving AI agents,laying the foundation for the development of more adaptive, autonomous, andlifelong agentic systems.</description>
      <author>example@mail.com (Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, Zhaochun Ren, Nikos Aletras, Xi Wang, Han Zhou, Zaiqiao Meng)</author>
      <guid isPermaLink="false">2508.07407v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack</title>
      <link>http://arxiv.org/abs/2508.07402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对参数高效微调(PEFT)方法在图像伪造检测与定位(IFDL)任务中易受对抗攻击的问题，提出了ForensicsSAM框架，通过注入伪造专家和对抗专家，以及设计轻量级对抗检测器，显著提高了模型对对抗攻击的鲁棒性，同时保持了先进的IFDL性能。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)已成为适应大型视觉基础模型(如SAM和LLaVA)用于下游任务(如图像伪造检测和定位)的流行策略。然而，现有的PEFT方法容易受到对抗攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;解决PEFT方法在IFDL任务中的对抗攻击脆弱性问题，提高模型对对抗攻击的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出ForensicsSAM框架，包含三个关键设计：(1)向transformer块注入伪造专家，增强捕获伪造伪影的能力；(2)设计轻量级对抗检测器，学习捕获RGB域中的结构化、任务特定伪影；(3)向全局注意力层和MLP模块注入对抗专家，纠正对抗噪声引起的特征偏移。&lt;h4&gt;主要发现&lt;/h4&gt;可以通过上游模型生成高度可转移的对抗图像，无需访问下游模型或训练数据，这会显著降低IFDL性能。ForensicsSAM能够有效抵抗各种对抗攻击方法，同时保持先进的IFDL性能。&lt;h4&gt;结论&lt;/h4&gt;ForensicsSAM通过集成对抗鲁棒性设计，解决了PEFT方法在IFDL任务中的对抗攻击脆弱性问题，为视觉基础模型的安全应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)已成为适应大型视觉基础模型(如Segment Anything Model和LLaVA)用于下游任务(如图像伪造检测和定位)的流行策略。然而，现有的基于PEFT的方法忽视了它们对对抗攻击的脆弱性。在本文中，我们展示了可以通过上游模型专门生成高度可转移的对抗图像，无需访问下游模型或训练数据，这会显著降低IFDL性能。为解决此问题，我们提出了ForensicsSAM，一个具有内置对抗鲁棒性的统一IFDL框架。我们的设计由三个关键思想指导：(1)为弥补冻结图像编码器中缺乏伪造相关知识的问题，我们向每个transformer块注入伪造专家，增强其捕获伪造伪影的能力。这些伪造专家始终处于激活状态，并在任何输入图像间共享。(2)为检测对抗图像，我们设计了一个轻量级对抗检测器，学习捕获RGB域中的结构化、任务特定伪影，实现对各种攻击方法的可靠区分。(3)为抵抗对抗攻击，我们向全局注意力层和MLP模块注入对抗专家，逐步纠正由对抗噪声引起的特征偏移。这些对抗专家由对抗检测器自适应激活，从而避免对干净图像造成不必要干扰。在多个基准上的广泛实验表明，ForensicsSAM对各种对抗攻击方法具有更强的抵抗力，同时在图像级伪造检测和像素级伪造定位方面取得了最先进的性能。资源可在https://github.com/siriusPRX/ForensicsSAM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy foradapting large vision foundation models, such as the Segment Anything Model(SAM) and LLaVA, to downstream tasks like image forgery detection andlocalization (IFDL). However, existing PEFT-based approaches overlook theirvulnerability to adversarial attacks. In this paper, we show that highlytransferable adversarial images can be crafted solely via the upstream model,without accessing the downstream model or training data, significantlydegrading the IFDL performance. To address this, we propose ForensicsSAM, aunified IFDL framework with built-in adversarial robustness. Our design isguided by three key ideas: (1) To compensate for the lack of forgery-relevantknowledge in the frozen image encoder, we inject forgery experts into eachtransformer block to enhance its ability to capture forgery artifacts. Theseforgery experts are always activated and shared across any input images. (2) Todetect adversarial images, we design an light-weight adversary detector thatlearns to capture structured, task-specific artifact in RGB domain, enablingreliable discrimination across various attack methods. (3) To resistadversarial attacks, we inject adversary experts into the global attentionlayers and MLP modules to progressively correct feature shifts induced byadversarial noise. These adversary experts are adaptively activated by theadversary detector, thereby avoiding unnecessary interference with cleanimages. Extensive experiments across multiple benchmarks demonstrate thatForensicsSAM achieves superior resistance to various adversarial attackmethods, while also delivering state-of-the-art performance in image-levelforgery detection and pixel-level forgery localization. The resource isavailable at https://github.com/siriusPRX/ForensicsSAM.</description>
      <author>example@mail.com (Rongxuan Peng, Shunquan Tan, Chenqi Kong, Anwei Luo, Alex C. Kot, Jiwu Huang)</author>
      <guid isPermaLink="false">2508.07402v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control</title>
      <link>http://arxiv.org/abs/2508.07387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用RGB相机在未知环境中进行导航的新方法，通过将估计的深度作为上下文输入到学习的碰撞模型中，而非直接用于碰撞检测，从而提高了在杂乱环境中的导航成功率。&lt;h4&gt;背景&lt;/h4&gt;仅使用RGB相机在未知环境中导航具有挑战性，因为缺乏深度信息无法进行可靠的碰撞检测。现有使用视觉基础模型估计深度的方法在杂乱环境中过于嘈杂，不适合零样本导航。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖嘈杂估计深度直接进行碰撞检测的替代方法，提高在高度杂乱环境中的导航成功率。&lt;h4&gt;方法&lt;/h4&gt;提出将估计的深度作为上下文输入到学习的碰撞模型中，该模型预测给定控制序列下机器人可以预期的最小障碍物清除距离分布。使用感知风险的MPC规划器最小化估计的碰撞风险。通过联合学习管道同时训练碰撞模型和风险度量，使用安全和unsafe轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;联合训练确保了碰撞模型的最佳方差，提高了在高度杂乱环境中的导航能力。真实世界实验显示，与NoMaD和ROS堆栈相比，成功率分别提高了9倍和7倍。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过创新地使用估计深度和联合学习策略，显著提高了仅使用RGB相机在未知杂乱环境中的导航性能。&lt;h4&gt;翻译&lt;/h4&gt;使用单个RGB相机在未知环境中导航具有挑战性，因为缺乏深度信息会阻碍可靠的碰撞检测。虽然一些方法使用估计的深度来构建碰撞地图，但我们发现视觉基础模型估计的深度对于在杂乱环境中的零样本导航来说过于嘈杂。我们提出了一种替代方法：不使用嘈杂的估计深度进行直接碰撞检测，而是将其作为丰富的上下文输入到学习的碰撞模型中。该模型预测机器人对于给定控制序列可以预期的最小障碍物清除距离分布。在推理时，这些预测会通知一个感知风险的MPC规划器，该规划器最小化估计的碰撞风险。我们的联合学习管道使用安全和unsafe轨迹共同训练碰撞模型和风险度量。关键是，我们的联合训练确保了碰撞模型的最佳方差，从而提高了在高度杂乱环境中的导航能力。因此，真实世界的实验表明，与NoMaD和ROS堆栈相比，成功率分别提高了9倍和7倍。消融研究进一步验证了他们设计选择的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决单目RGB相机在未知环境中导航的挑战，特别是缺乏深度信息导致的可靠碰撞检测问题。这个问题很重要，因为单目相机对轻型机器人平台（如空中或紧凑地面机器人）非常适用，这些平台受限于尺寸、重量和功耗难以使用LiDAR。仅视觉设置降低了硬件复杂性和能源使用，支持更长时间的任务和成本敏感环境中的广泛部署，但缺乏深度感知使得安全碰撞检测变得困难。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，发现直接使用估计深度构建碰撞地图的方法在杂乱环境中表现不佳。他们提出将嘈杂的估计深度作为上下文输入到学习的碰撞模型中，而非直接用于碰撞检测。作者借鉴了视觉基础模型（如DepthAnything）进行深度预测，借鉴了MPC框架进行规划，借鉴了PointNet++进行点云特征提取，借鉴了基于采样的优化方法解决轨迹优化问题，并借鉴了MMD作为风险度量概念。他们的创新在于重新解释了估计深度的使用方式，设计了联合学习管道，并通过下游任务监督改进了方差正则化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：将单目RGB图像和候选控制序列作为输入，学习一个概率碰撞模型来预测沿轨迹的最小障碍物清除分布，使用这些预测估计碰撞风险，并通过风险感知的MPC规划器最小化风险。整体流程包括：1)使用预训练深度估计器从RGB图像生成深度图并转换为点云；2)使用PointNet++提取点云特征；3)将特征与初始状态连接并通过MLP预测障碍物清除的均值和方差；4)生成样本并计算约束违反和MMD风险；5)使用采样优化器解决轨迹优化问题生成控制序列；6)以递归方式应用MPC反馈循环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)重新解释估计深度作为信息丰富的输入而非真实深度的代理；2)联合学习管道共同训练碰撞模型和风险度量；3)通过下游任务监督实现任务感知方差正则化；4)风险感知MPC框架利用预测分布估计碰撞风险；5)学习最优MMD核参数。相比之前工作的不同：与端到端方法不同，作者的方法显式结合了安全约束；与世界模型方法不同，作者的方法建模了预测不确定性和碰撞风险；与使用估计深度构建地图的方法不同，作者的方法不直接依赖嘈杂深度估计；大多数单目导航方法只在简单环境中测试，而作者的方法在复杂杂乱环境中展示了可靠导航。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种新颖的单目视觉导航方法，通过学习概率碰撞模型和风险感知模型预测控制，有效解决了嘈杂深度估计下的可靠碰撞检测问题，显著提高了杂乱环境中的导航成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigating unknown environments with a single RGB camera is challenging, asthe lack of depth information prevents reliable collision-checking. While somemethods use estimated depth to build collision maps, we found that depthestimates from vision foundation models are too noisy for zero-shot navigationin cluttered environments.  We propose an alternative approach: instead of using noisy estimated depthfor direct collision-checking, we use it as a rich context input to a learnedcollision model. This model predicts the distribution of minimum obstacleclearance that the robot can expect for a given control sequence. At inference,these predictions inform a risk-aware MPC planner that minimizes estimatedcollision risk. Our joint learning pipeline co-trains the collision model andrisk metric using both safe and unsafe trajectories. Crucially, ourjoint-training ensures optimal variance in our collision model that improvesnavigation in highly cluttered environments. Consequently, real-worldexperiments show 9x and 7x improvements in success rates over NoMaD and the ROSstack, respectively. Ablation studies further validate the effectiveness of ourdesign choices.</description>
      <author>example@mail.com (Basant Sharma, Prajyot Jadhav, Pranjal Paul, K. Madhava Krishna, Arun Kumar Singh)</author>
      <guid isPermaLink="false">2508.07387v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance</title>
      <link>http://arxiv.org/abs/2508.07375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TurnGuide的新方法，用于解决全双工语音语言模型在对话能力方面面临的挑战。该方法通过模拟人类的对话规划过程，动态将助手语音分割为对话回合，并在语音输出前生成回合级别的文本指导，从而显著提高了端到端FD-SLMs的对话能力。&lt;h4&gt;背景&lt;/h4&gt;全双工语音语言模型(FD-SLMs)是专门设计的基础模型，旨在通过建模复杂的对话动态（如打断、反馈和重叠语音）来实现自然、实时的口语交互。端到端FD-SLMs利用真实世界的双通道对话数据来捕捉精细的双人对话模式，以实现类人交互。然而，由于长时间的语音序列和有限的高质量口语对话数据，这些模型的对话能力往往比纯文本对话差。&lt;h4&gt;目的&lt;/h4&gt;解决全双工语音语言模型在对话能力方面面临的挑战，特别是由于长时间语音序列和有限高质量口语对话数据导致的对话能力下降问题，以及文本引导语音生成在整合到双通道音频流时面临的时机和长度问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为TurnGuide的、受规划启发的新方法。该方法通过模拟人类的对话规划过程，动态地将助手语音分割为对话回合，并在语音输出前生成回合级别的文本指导，从而有效解决了插入时机和长度的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;TurnGuide方法显著提高了端到端FD-SLMs的对话能力，使模型能够生成语义上有意义且连贯的语音，同时保持了自然的对话流程。&lt;h4&gt;结论&lt;/h4&gt;TurnGuide方法有效地解决了全双工语音语言模型面临的挑战，通过模拟人类的对话规划过程，提高了模型的对话能力，使其能够生成更自然、更有意义的语音交互。&lt;h4&gt;翻译&lt;/h4&gt;全双工语音语言模型(FD-SLMs)是专门设计的基础模型，通过建模复杂的对话动态（如打断、反馈和重叠语音）来实现自然、实时的口语交互。端到端(e2e)FD-SLMs利用真实世界的双通道对话数据来捕捉精细的双人对话模式，以实现类人交互。然而，由于长时间的语音序列和有限的高质量口语对话数据，这些模型的对话能力往往比纯文本对话差。虽然文本引导的语音生成可以缓解这些问题，但在将文本引导整合到双通道音频流时，它面临着时机和长度问题，破坏了自然交互所需的时间精确对齐。为解决这些挑战，我们提出了TurnGuide，一种受规划启发的新方法，它通过动态将助手语音分割为对话回合并在语音输出前生成回合级别的文本指导，模拟人类的对话规划过程，从而有效解决了插入时机和长度的挑战。大量实验证明我们的方法显著提高了端到端FD-SLMs的对话能力，使它们能够生成语义上有意义且连贯的语音，同时保持自然的对话流程。演示可在https://dreamtheater123.github.io/TurnGuide-Demo/查看，代码将在https://github.com/dreamtheater123/TurnGuide提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Full-Duplex Speech Language Models (FD-SLMs) are specialized foundationmodels designed to enable natural, real-time spoken interactions by modelingcomplex conversational dynamics such as interruptions, backchannels, andoverlapping speech, and End-to-end (e2e) FD-SLMs leverage real-worlddouble-channel conversational data to capture nuanced two-speaker dialoguepatterns for human-like interactions. However, they face a critical challenge-- their conversational abilities often degrade compared to pure-textconversation due to prolonged speech sequences and limited high-quality spokendialogue data. While text-guided speech generation could mitigate these issues,it suffers from timing and length issues when integrating textual guidance intodouble-channel audio streams, disrupting the precise time alignment essentialfor natural interactions. To address these challenges, we propose TurnGuide, anovel planning-inspired approach that mimics human conversational planning bydynamically segmenting assistant speech into dialogue turns and generatingturn-level text guidance before speech output, which effectively resolves bothinsertion timing and length challenges. Extensive experiments demonstrate ourapproach significantly improves e2e FD-SLMs' conversational abilities, enablingthem to generate semantically meaningful and coherent speech while maintainingnatural conversational flow. Demos are available athttps://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available athttps://github.com/dreamtheater123/TurnGuide.</description>
      <author>example@mail.com (Wenqian Cui, Lei Zhu, Xiaohui Li, Zhihan Guo, Haoli Bai, Lu Hou, Irwin King)</author>
      <guid isPermaLink="false">2508.07375v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Gradient Surgery for Safe LLM Fine-Tuning</title>
      <link>http://arxiv.org/abs/2508.07172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SafeGrad是一种新型防御方法，通过梯度手术解决微调即服务中的安全对齐问题，能够在保持任务性能的同时增强模型安全性。&lt;h4&gt;背景&lt;/h4&gt;微调即服务(Fine-tuning-as-a-Service)引入了一个关键漏洞，用户微调数据集中的少量恶意示例可能损害大型语言模型(LLMs)的安全对齐。&lt;h4&gt;目的&lt;/h4&gt;解决现有安全微调解决方案对有害比例敏感的问题，提高在高有害比例情况下的防御效果。&lt;h4&gt;方法&lt;/h4&gt;SafeGrad采用梯度手术技术，当检测到冲突时，通过将用户任务梯度投影到对齐梯度的正交平面上来消除有害成分；同时使用KL散度对齐损失学习基础模型的分布式安全配置文件。&lt;h4&gt;主要发现&lt;/h4&gt;现有安全微调解决方案的性能会随着有害比例的增加而急剧下降，这是因为用户任务更新与安全目标之间存在冲突的梯度。&lt;h4&gt;结论&lt;/h4&gt;SafeGrad在各种LLMs和数据集上提供了最先进的防御，即使在有害比例较高的情况下也能保持强大的安全性，而不损害任务保真度。&lt;h4&gt;翻译&lt;/h4&gt;微调即服务引入了一个关键漏洞，即用户微调数据集中的少量恶意示例可能损害大型语言模型的安全对齐。虽然公认的安全微调范式将其视为一个多目标优化问题，平衡用户任务性能与安全对齐，但我们发现现有解决方案对有害比例极其敏感，随着有害比例的增加，防御效果会急剧下降。我们诊断出这种失败源于冲突的梯度，其中用户任务更新直接损害了安全目标。为此，我们提出了SafeGrad，一种采用梯度手术的新方法。当检测到冲突时，SafeGrad通过将用户任务梯度投影到对齐梯度的正交平面上来消除其有害成分，使模型能够学习用户的任务而不牺牲安全性。为了进一步增强鲁棒性和数据效率，我们采用了KL散度对齐损失，学习良好对齐基础模型的丰富、分布式安全配置文件。大量实验表明，SafeGrad在各种LLMs和数据集上提供了最先进的防御，即使在有害比例较高的情况下也能保持强大的安全性而不损害任务保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning-as-a-Service introduces a critical vulnerability where a fewmalicious examples mixed into the user's fine-tuning dataset can compromise thesafety alignment of Large Language Models (LLMs). While a recognized paradigmframes safe fine-tuning as a multi-objective optimization problem balancinguser task performance with safety alignment, we find existing solutions arecritically sensitive to the harmful ratio, with defenses degrading sharply asharmful ratio increases. We diagnose that this failure stems from conflictinggradients, where the user-task update directly undermines the safety objective.To resolve this, we propose SafeGrad, a novel method that employs gradientsurgery. When a conflict is detected, SafeGrad nullifies the harmful componentof the user-task gradient by projecting it onto the orthogonal plane of thealignment gradient, allowing the model to learn the user's task withoutsacrificing safety. To further enhance robustness and data efficiency, weemploy a KL-divergence alignment loss that learns the rich, distributionalsafety profile of the well-aligned foundation model. Extensive experiments showthat SafeGrad provides state-of-the-art defense across various LLMs anddatasets, maintaining robust safety even at high harmful ratios withoutcompromising task fidelity.</description>
      <author>example@mail.com (Biao Yi, Jiahao Li, Baolei Zhang, Lihai Nie, Tong Li, Tiansheng Huang, Zheli Liu)</author>
      <guid isPermaLink="false">2508.07172v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications</title>
      <link>http://arxiv.org/abs/2508.07165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了PRISM，一种基于大规模多序列MRI预训练的基础模型，解决了MRI序列异质性导致的深度学习模型泛化能力差的问题，在44个下游任务中39项排名第一，显著提升了AI在放射学中的应用潜力。&lt;h4&gt;背景&lt;/h4&gt;多序列磁共振成像(MRI)具有显著多样性，能可视化不同组织类型，但MRI序列间的内在异质性对深度学习模型的泛化能力构成重大挑战，影响了模型在不同采集参数下的表现，限制了其临床应用。&lt;h4&gt;目的&lt;/h4&gt;开发PRISM基础模型，通过大规模多序列MRI预训练，解决MRI序列异质性导致的模型泛化能力差的问题，提升模型在临床应用中的表现。&lt;h4&gt;方法&lt;/h4&gt;收集64个数据集(336,476个体积MRI扫描)构建最大多器官多序列MRI预训练语料库；提出新预训练范式，解耦解剖不变特征与序列特定变化；建立包含44个下游任务的基准测试，在32个公共数据集和5个私人队列上评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;PRISM在非预训练模型和现有基础模型上表现更优，在44个下游基准测试中39项排名第一，具有统计学显著改进；能够学习在多样化MRI协议下采集的未见数据上的鲁棒且可泛化的表示。&lt;h4&gt;结论&lt;/h4&gt;PRISM为多序列MRI分析提供了可扩展框架，增强了AI在放射学中的转化潜力，在多样化成像协议上提供一致性能，强化了临床适用性。&lt;h4&gt;翻译&lt;/h4&gt;多序列磁共振成像(MRI)提供了显著的多样性，能够清晰可视化不同组织类型。然而，MRI序列之间的内在异质性对深度学习模型的泛化能力构成了重大挑战。这些挑战在面对不同采集参数时损害了模型性能，从而严重限制了它们的临床应用。在本研究中，我们提出了PRISM，一种基于大规模多序列MRI预训练的基础模型。我们从公共和私人来源收集了总共64个数据集，涵盖广泛的全身解剖结构，扫描跨越多种MRI序列。其中，336,476个体积MRI扫描来自34个数据集(8个公共和26个私人)，被精心筛选构建迄今为止最大的多器官多序列MRI预训练语料库。我们提出了一种新的预训练范式，将MRI中解剖不变特征与序列特定变化解耦，同时保留高级语义表示。我们建立了一个包含44个下游任务的基准测试，包括疾病诊断、图像分割、配准、进展预测和报告生成。这些任务在32个公共数据集和5个私人队列上进行了评估。PRISM始终优于非预训练模型和现有基础模型，在44个下游基准测试中的39项中取得排名第一的结果，具有统计学显著改进。这些结果强调了PRISM在多样化MRI协议下采集的未见数据上学习鲁棒且可泛化表示的能力。PRISM为多序列MRI分析提供了可扩展框架，从而增强了AI在放射学中的转化潜力。它在多样化的成像协议上提供一致的性能，强化了其临床适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多序列磁共振成像（MRI）中的内在异质性对深度学习模型泛化能力的挑战。这个问题在现实中非常重要，因为MRI是临床诊断的基础工具，能够无创、无辐射地可视化软组织并提供复杂的解剖和病理细节。然而，不同MRI序列之间的差异导致AI模型难以在不同采集参数下保持稳定性能，严重限制了AI在放射学领域的临床应用和实用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有医学基础模型在MRI应用中的局限性，包括解剖覆盖有限、多序列信息利用不足和下游任务验证范围窄等问题。他们借鉴了自然图像和语言领域的基础模型思想，以及现有的医学基础模型如Med3D和BrainSegFounder，设计了PRISM模型。核心思路是创建一个能够学习通用表示的基础模型，通过解耦解剖不变特征与序列特定变化，使模型能够适应各种临床应用。作者采用了Swin Transformer作为骨干网络，并创新性地结合了多种自监督学习任务，包括掩码图像重建、跨序列翻译、元数据预测和解剖不变对比学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在大规模多序列MRI数据上预训练一个基础模型（PRISM），解耦解剖不变特征和序列特定特征，使模型能够捕捉跨序列的共享解剖模式，同时保留序列特定的对比度变化，从而提高模型对不同扫描仪协议和采集参数的鲁棒性。整体实现流程包括：1）收集并预处理64个数据集，构建包含336,476个体积MRI扫描的预训练语料库；2）采用Swin Transformer骨干网络和双分支解缠模块进行特征提取；3）通过掩码图像重建、跨序列翻译、元数据预测和解剖不变对比学习四种自监督任务进行预训练；4）在44个下游任务上评估模型性能，包括疾病诊断、图像分割、跨序列配准等。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）构建了迄今为止最大的多器官多序列MRI预训练语料库；2）提出新的预训练范式，解耦解剖不变特征和序列特定变化；3）结合像素级掩码图像重建和图像到图像翻译，保持不同对比度条件下的结构保真度；4）将元数据预测与对比学习相结合，增强语义表示学习；5）在44个下游任务上进行全面验证。相比之前的工作，PRISM的不同之处在于：它覆盖了更广泛的解剖区域，充分利用了多序列信息，对真实世界的异质性具有更强鲁棒性，适用于更广泛的临床应用，并且训练数据规模远大于现有MRI基础模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PRISM通过在大规模多序列MRI数据上进行预训练并采用创新的解耦表示学习方法，实现了在多种临床应用中具有卓越泛化能力和鲁棒性的MRI基础模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkableversatility, enabling the distinct visualization of different tissue types.Nevertheless, the inherent heterogeneity among MRI sequences poses significantchallenges to the generalization capability of deep learning models. Thesechallenges undermine model performance when faced with varying acquisitionparameters, thereby severely restricting their clinical utility. In this study,we present PRISM, a foundation model PRe-trained with large-scalemultI-Sequence MRI. We collected a total of 64 datasets from both public andprivate sources, encompassing a wide range of whole-body anatomical structures,with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRIscans from 34 datasets (8 public and 26 private) were curated to construct thelargest multi-organ multi-sequence MRI pretraining corpus to date. We propose anovel pretraining paradigm that disentangles anatomically invariant featuresfrom sequence-specific variations in MRI, while preserving high-level semanticrepresentations. We established a benchmark comprising 44 downstream tasks,including disease diagnosis, image segmentation, registration, progressionprediction, and report generation. These tasks were evaluated on 32 publicdatasets and 5 private cohorts. PRISM consistently outperformed bothnon-pretrained models and existing foundation models, achieving first-rankresults in 39 out of 44 downstream benchmarks with statistical significanceimprovements. These results underscore its ability to learn robust andgeneralizable representations across unseen data acquired under diverse MRIprotocols. PRISM provides a scalable framework for multi-sequence MRI analysis,thereby enhancing the translational potential of AI in radiology. It deliversconsistent performance across diverse imaging protocols, reinforcing itsclinical applicability.</description>
      <author>example@mail.com (Zelin Qiu, Xi Wang, Zhuoyao Xie, Juan Zhou, Yu Wang, Lingjie Yang, Xinrui Jiang, Juyoung Bae, Moo Hyun Son, Qiang Ye, Dexuan Chen, Rui Zhang, Tao Li, Neeraj Ramesh Mahboobani, Varut Vardhanabhuti, Xiaohui Duan, Yinghua Zhao, Hao Chen)</author>
      <guid isPermaLink="false">2508.07165v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>UniMove: A Unified Model for Multi-city Human Mobility Prediction</title>
      <link>http://arxiv.org/abs/2508.06986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by SIGSPATIAL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出UniMove，一个统一的多城市人类移动性预测模型，通过通用空间表示和异构移动模式建模，解决了现有方法需为每个城市单独训练的问题，实现10.2%以上的预测准确率提升。&lt;h4&gt;背景&lt;/h4&gt;人类移动性预测对城市规划、交通优化和个性化服务至关重要，但人类移动性的内在随机性、非均匀时间间隔和复杂模式，以及不同城市结构、基础设施和人口密度带来的异质性，给建模带来重大挑战。现有解决方案通常需要为每个城市单独训练模型。&lt;h4&gt;目的&lt;/h4&gt;提出UniMove统一模型，解决两个挑战：(1)构建通用空间表示以实现城市间有效标记共享；(2)建模来自不同城市特征的异构移动模式。&lt;h4&gt;方法&lt;/h4&gt;采用轨迹-位置双塔架构，包括位置塔用于通用空间编码和轨迹塔用于顺序移动性建模，并设计MoE Transformer块自适应选择专家处理不同移动模式，支持多城市数据联合训练和相互数据增强。&lt;h4&gt;主要发现&lt;/h4&gt;在多个不同城市数据集上的实验表明，UniMove通过多城市数据联合训练和相互数据增强，显著提高了移动性预测准确性，提升超过10.2%。&lt;h4&gt;结论&lt;/h4&gt;UniMove代表了朝着实现具有统一架构的人类移动性真正基础模型迈出的关键一步，相关实现已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;人类移动性预测对城市规划、交通优化和个性化服务至关重要。然而，人类移动性的内在随机性、非均匀时间间隔和复杂模式，加上不同城市结构、基础设施和人口密度引入的异质性，给建模带来了重大挑战。现有解决方案通常需要为每个城市训练单独的模型，因为它们具有不同的空间表示和地理覆盖范围。在本文中，我们提出了UniMove，一个用于多城市人类移动性预测的统一模型，解决了两个挑战：(1)构建通用的空间表示，以便在城市间有效共享标记；(2)建模来自不同城市特征的异构移动模式。我们提出了一个轨迹-位置双塔架构，包括一个用于通用空间编码的位置塔和一个用于顺序移动性建模的轨迹塔。我们还设计了MoE Transformer块，以自适应地选择专家来处理不同的移动模式。在多个不同城市数据集上的大量实验表明，UniMove真正体现了统一模型的本质。通过多城市数据的联合训练和相互数据增强，它显著提高了移动性预测的准确性，提高了10.2%以上。UniMove代表了朝着实现具有统一架构的人类移动性真正基础模型迈出的关键一步。我们在https://github.com/tsinghua-fib-lab/UniMove/上发布了该模型的实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human mobility prediction is vital for urban planning, transportationoptimization, and personalized services. However, the inherent randomness,non-uniform time intervals, and complex patterns of human mobility, compoundedby the heterogeneity introduced by varying city structures, infrastructure, andpopulation densities, present significant challenges in modeling. Existingsolutions often require training separate models for each city due to distinctspatial representations and geographic coverage. In this paper, we proposeUniMove, a unified model for multi-city human mobility prediction, addressingtwo challenges: (1) constructing universal spatial representations foreffective token sharing across cities, and (2) modeling heterogeneous mobilitypatterns from varying city characteristics. We propose a trajectory-locationdual-tower architecture, with a location tower for universal spatial encodingand a trajectory tower for sequential mobility modeling. We also design MoETransformer blocks to adaptively select experts to handle diverse movementpatterns. Extensive experiments across multiple datasets from diverse citiesdemonstrate that UniMove truly embodies the essence of a unified model. Byenabling joint training on multi-city data with mutual data enhancement, itsignificantly improves mobility prediction accuracy by over 10.2\%. UniMoverepresents a key advancement toward realizing a true foundational model with aunified architecture for human mobility. We release the implementation athttps://github.com/tsinghua-fib-lab/UniMove/.</description>
      <author>example@mail.com (Chonghua Han, Yuan Yuan, Yukun Liu, Jingtao Ding, Jie Feng, Yong Li)</author>
      <guid isPermaLink="false">2508.06986v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing</title>
      <link>http://arxiv.org/abs/2508.06937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: vaynexie.github.io/CannyEdit/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CannyEdit是一种创新的无需训练框架，通过选择性Canny控制和双提示引导解决了现有文本到图像编辑方法在平衡文本遵循度、上下文保真度和编辑无缝性方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;文本到图像(T2I)模型的发展使得基于基础模型生成先验的无需训练区域图像编辑成为可能，但现有方法难以平衡编辑区域的文本遵循度、未编辑区域的上下文保真度以及编辑的无缝集成。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的无需训练框架，实现更精确的图像编辑，同时保持文本遵循度、上下文保真度和编辑无缝性的平衡。&lt;h4&gt;方法&lt;/h4&gt;CannyEdit框架包含两个关键创新：(1)选择性Canny控制：在用户指定的可编辑区域屏蔽Canny ControlNet的结构指导，同时保留未编辑区域的源图像细节；(2)双提示引导：结合局部提示进行特定对象编辑和全局目标提示以保持场景交互一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在实际图像编辑任务中，CannyEdit优于先前方法如KV-Edit，在文本遵循度和上下文保真度的平衡上实现了2.93%到10.49%的改进。用户研究表明，只有49.2%的普通用户和42.0%的AIGC专家能将CannyEdit的结果识别为AI编辑，而竞争对手方法这一比例为76.08%到89.09%。&lt;h4&gt;结论&lt;/h4&gt;CannyEdit通过其创新方法成功解决了现有文本到图像编辑面临的挑战，在保持文本遵循度、上下文保真度和编辑无缝性方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;最近的文本到图像(T2I)模型进展使得通过利用基础模型的生成先验实现无需训练的区域图像编辑成为可能。然而，现有方法难以平衡编辑区域的文本遵循度、未编辑区域的上下文保真度以及编辑的无缝集成。我们引入了CannyEdit，一种新颖的无需训练框架，通过两个关键创新解决这些挑战：(1)选择性Canny控制，在用户指定的可编辑区域屏蔽Canny ControlNet的结构指导，同时通过反转阶段ControlNet信息保留严格保留未编辑区域的源图像细节。这能够在不损害上下文完整性的情况下实现精确的文本驱动编辑。(2)双提示引导，结合用于特定对象编辑的局部提示和用于保持场景交互一致性的全局目标提示。在实际图像编辑任务（添加、替换、删除）中，CannyEdit优于先前的方法如KV-Edit，在文本遵循度和上下文保真度的平衡上实现了2.93%到10.49%的改进。在编辑无缝性方面，用户研究表明，当与未编辑的真实图像配对时，只有49.2%的普通用户和42.0%的AIGC专家能够识别CannyEdit的结果为AI编辑，而竞争对手方法这一比例为76.08%到89.09%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in text-to-image (T2I) models have enabled training-freeregional image editing by leveraging the generative priors of foundationmodels. However, existing methods struggle to balance text adherence in editedregions, context fidelity in unedited areas, and seamless integration of edits.We introduce CannyEdit, a novel training-free framework that addresses thesechallenges through two key innovations: (1) Selective Canny Control, whichmasks the structural guidance of Canny ControlNet in user-specified editableregions while strictly preserving details of the source images in uneditedareas via inversion-phase ControlNet information retention. This enablesprecise, text-driven edits without compromising contextual integrity. (2)Dual-Prompt Guidance, which combines local prompts for object-specific editswith a global target prompt to maintain coherent scene interactions. Onreal-world image editing tasks (addition, replacement, removal), CannyEditoutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percentimprovement in the balance of text adherence and context fidelity. In terms ofediting seamlessness, user studies reveal only 49.2 percent of general usersand 42.0 percent of AIGC experts identified CannyEdit's results as AI-editedwhen paired with real images without edits, versus 76.08 to 89.09 percent forcompetitor methods.</description>
      <author>example@mail.com (Weiyan Xie, Han Gao, Didan Deng, Kaican Li, April Hua Liu, Yongxiang Huang, Nevin L. Zhang)</author>
      <guid isPermaLink="false">2508.06937v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification</title>
      <link>http://arxiv.org/abs/2508.06908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了MMReID-Bench，这是专为行人重识别设计的首个多任务多模态基准。研究表明多模态大语言模型在行人ReID方面具有显著能力，但在处理某些模态（特别是热成像和红外数据）时存在局限性。&lt;h4&gt;背景&lt;/h4&gt;传统行人ReID模型受限于单模态能力，在处理多模态数据（如RGB、热成像、红外、草图图像、文本描述等）时泛化能力较差。多模态大语言模型(MLLMs)的出现为解决这一问题提供了新途径，但现有方法未能充分利用MLLMs的推理、指令遵循和跨模态理解能力。&lt;h4&gt;目的&lt;/h4&gt;引入MMReID-Bench，填补现有方法的不足，促进开发更强大且可泛化的多模态基础模型用于行人ReID。&lt;h4&gt;方法&lt;/h4&gt;提出MMReID-Bench，包含20,710个多模态查询和图库图像，涵盖10种不同的行人ReID任务。通过全面实验评估MLLMs在行人ReID任务中的能力。&lt;h4&gt;主要发现&lt;/h4&gt;多模态大语言模型在提供有效且多样化的行人ReID方面具有显著能力，但在处理热成像和红外数据等某些模态时存在局限性。&lt;h4&gt;结论&lt;/h4&gt;MMReID-Bench作为首个专为行人ReID设计的多任务多模态基准，有望促进社区开发更强大且可泛化的多模态基础模型。&lt;h4&gt;翻译&lt;/h4&gt;行人重识别(ReID)旨在在图库图像中检索感兴趣人员的图像，在医疗康复、异常行为检测和公共安全等领域有广泛应用。然而，传统行人ReID模型受限于单模态能力，在处理多模态数据（如RGB、热成像、红外、草图图像、文本描述等）时泛化能力较差。最近，多模态大语言模型(MLLMs)的出现为解决这个问题提供了有希望的途径。尽管如此，现有方法仅将MLLMs视为特征提取器或字幕生成器，未能充分利用其推理、指令遵循和跨模态理解能力。为了弥补这一差距，我们引入了MMReID-Bench，这是第一个专为行人ReID设计的多任务多模态基准。MMReID-Bench包含20,710个多模态查询和图库图像，涵盖10种不同的行人ReID任务。全面实验表明MLLMs在提供有效且多样化的行人ReID方面具有显著能力。然而，它们在处理某些模态（特别是热成像和红外数据）时也存在局限性。我们希望MMReID-Bench能够促进社区开发更强大且可泛化的多模态基础模型用于行人ReID。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Person re-identification (ReID) aims to retrieve the images of an interestedperson in the gallery images, with wide applications in medical rehabilitation,abnormal behavior detection, and public security. However, traditional personReID models suffer from uni-modal capability, leading to poor generalizationability in multi-modal data, such as RGB, thermal, infrared, sketch images,textual descriptions, etc. Recently, the emergence of multi-modal largelanguage models (MLLMs) shows a promising avenue for addressing this problem.Despite this potential, existing methods merely regard MLLMs as featureextractors or caption generators, which do not fully unleash their reasoning,instruction-following, and cross-modal understanding capabilities. To bridgethis gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmarkspecifically designed for person ReID. The MMReID-Bench includes 20,710multi-modal queries and gallery images covering 10 different person ReID tasks.Comprehensive experiments demonstrate the remarkable capabilities of MLLMs indelivering effective and versatile person ReID. Nevertheless, they also havelimitations in handling a few modalities, particularly thermal and infrareddata. We hope MMReID-Bench can facilitate the community to develop more robustand generalizable multimodal foundation models for person ReID.</description>
      <author>example@mail.com (Jinhao Li, Zijian Chen, Lirong Deng, Changbo Wang, Guangtao Zhai)</author>
      <guid isPermaLink="false">2508.06908v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Large Model Driven Solar Activity AI Forecaster: A Scalable Dual Data-Model Framework</title>
      <link>http://arxiv.org/abs/2508.06892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了'太阳活动AI预测器'，一个基于基础模型的可扩展双数据-模型驱动框架，通过整合专家知识和多模态太阳数据，在OODA范式中实现了自主太阳耀斑预测，其性能在多源数据泛化、预测准确性和操作效率方面优于或匹配人类预报员。&lt;h4&gt;背景&lt;/h4&gt;太阳活动驱动空间天气，影响地球磁层和技术基础设施，使准确的太阳耀斑预测至关重要。当前空间天气模型未充分利用多模态太阳数据，缺乏专家知识的迭代增强，且过度依赖人类预报员。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自主复制人类预测任务并提供可量化输出的系统，以提高太阳耀斑预测的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出'太阳活动AI预测器'，一个基于基础模型构建的可扩展双数据-模型驱动框架，集成专家知识，在OODA范式中实现。包含三个模块：情境感知模块（整合多模态观测生成太阳态势感知图）、深度分析工具（表征关键太阳特征）和耀斑预测模块（预测强耀斑）。&lt;h4&gt;主要发现&lt;/h4&gt;模型可在几分钟内执行，在多源数据的泛化能力、预测准确性和操作效率方面，优于或匹配人类预报员。&lt;h4&gt;结论&lt;/h4&gt;建立了基于AI的空间天气预测新范式，证明了AI提高预测准确性和效率的潜力，为自主运行预测系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;太阳活动驱动空间天气，影响地球磁层和技术基础设施，这使得准确的太阳耀斑预测至关重要。当前空间天气模型未充分利用多模态太阳数据，缺乏专家知识的迭代增强，并在观察-定向-决策-行动（OODA）范式下过度依赖人类预报员。我们在此提出'太阳活动AI预测器'，一个基于基础模型构建的可扩展双数据-模型驱动框架，集成专家知识，自主复制人类预测任务并提供可量化输出。它在OODA范式中实现，包含三个模块：通过整合多模态观测生成每日太阳态势感知图的情境感知模块；表征关键太阳特征（活动区、冕洞、日珥）的深度分析工具；以及预测整个太阳圆盘和活动区强耀斑的耀斑预测模块。模型在几分钟内执行，在多源数据泛化、预测准确性和操作效率方面优于或匹配人类预报员。这项工作建立了基于AI的空间天气预测新范式，证明了AI提高预测准确性和效率的潜力，为自主运行预测系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solar activity drives space weather, affecting Earth's magnetosphere andtechnological infrastructure, which makes accurate solar flare forecastingcritical. Current space weather models under-utilize multi-modal solar data,lack iterative enhancement via expert knowledge, and rely heavily on humanforecasters under the Observation-Orientation-Decision-Action (OODA) paradigm.Here we present the "Solar Activity AI Forecaster", a scalable dual data-modeldriven framework built on foundational models, integrating expert knowledge toautonomously replicate human forecasting tasks with quantifiable outputs. It isimplemented in the OODA paradigm and comprises three modules: a SituationalPerception Module that generates daily solar situation awareness maps byintegrating multi-modal observations; In-Depth Analysis Tools that characterizekey solar features (active regions, coronal holes, filaments); and a FlarePrediction Module that forecasts strong flares for the full solar disk andactive regions. Executed within a few minutes, the model outperforms or matcheshuman forecasters in generalization across multi-source data, forecastaccuracy, and operational efficiency. This work establishes a new paradigm forAI-based space weather forecasting, demonstrating AI's potential to enhanceforecast accuracy and efficiency, and paving the way for autonomous operationalforecasting systems.</description>
      <author>example@mail.com (Jingjing Wang, Pengyu Liang, Tingyu Wang, Ming Li, Yanmei Cui, Siwei Liu, Xin Huang, Xiang Li, Minghui Zhang, Yunshi Zeng, Zhu Cao, Jiekang Feng, Qinghua Hu, Bingxian Luo, Bing Cao)</author>
      <guid isPermaLink="false">2508.06892v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Analysis of Solow-Swan model with nonlocal fractional derivative operator</title>
      <link>http://arxiv.org/abs/2508.06883v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过引入分数微积分扩展了传统的Solow-Swan经济增长模型，以包含记忆效应，使模型能够反映现实经济系统中过去状态对当前资本变化率的影响，并对两种模型下的资本动态进行了比较分析。&lt;h4&gt;背景&lt;/h4&gt;Solow-Swan方程是现代经济增长理论发展的基础模型，对理解资本积累和产出的长期行为提供了关键见解，但传统模型依赖整数阶导数，可能无法完全捕捉现实经济系统中常见的记忆和遗传特性。&lt;h4&gt;目的&lt;/h4&gt;扩展经典Solow-Swan框架，通过引入记忆效应使模型能够反映过去经济状态对当前资本变化率的影响，并比较分析经典与分数阶形式下的资本动态。&lt;h4&gt;方法&lt;/h4&gt;使用分数微积分将记忆效应融入传统Solow-Swan模型，创建分数阶模型，并对经典和分数阶形式下的资本动态进行比较分析。&lt;h4&gt;主要发现&lt;/h4&gt;分数阶模型能够捕捉到传统模型无法反映的记忆效应，即过去经济状态对当前资本变化率的影响。&lt;h4&gt;结论&lt;/h4&gt;通过分数微积分扩展的Solow-Swan模型能够更准确地描述现实经济系统中的动态行为，特别是那些表现出记忆和遗传特性的系统。&lt;h4&gt;翻译&lt;/h4&gt;Solow-Swan方程是现代经济增长理论发展中的基础模型。它对资本积累和产出的长期行为提供了关键见解。自其创立以来，该模型已成为理解宏观经济动态的基石，并激发了大量的后续研究。然而，传统Solow-Swan模型的表述依赖于整数阶导数，这可能无法完全捕捉现实经济系统中常见的记忆和遗传特性。在本文中，我们通过使用分数微积分将记忆效应融入，扩展了经典的Solow-Swan框架。分数阶模型考虑了过去状态对当前资本变化率的影响，这是标准模型未涵盖的特征。我们提出了对经典和分数阶形式的Solow-Swan方程下资本动态的比较分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Solow-Swan equation is a foundational model in the evolution of moderneconomic growth theory. It offers key insights into the long-term behaviour ofcapital accumulation and output. Since its inception, the model has served as acornerstone for understanding macroeconomic dynamics and has inspired a vastbody of subsequent research. However, traditional formulations of theSolow-Swan model rely on integer-order derivatives, which may not fully capturethe memory and hereditary properties often observed in real-world economicsystems. In this paper, we extend the classical Solow-Swan framework byincorporating memory effects through the use of fractional calculus. Thefractional model accounts for the influence of past states on the present rateof capital change, a feature not accommodated in the standard model. We presenta comparative analysis of the capital dynamics under both the classical andfractional-order formulations of the Solow-Swan equation.</description>
      <author>example@mail.com (MO Aibinu, KJ Duffy, S Moyo)</author>
      <guid isPermaLink="false">2508.06883v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>On Understanding of the Dynamics of Model Capacity in Continual Learning</title>
      <link>http://arxiv.org/abs/2508.08052v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了持续学习中的稳定性-塑性困境问题，引入了持续学习的有效模型容量(CLEMC)概念，揭示了神经网络有效容量的非平稳性特征。&lt;h4&gt;背景&lt;/h4&gt;稳定性-塑性困境是与神经网络能力相关的持续学习中的一个基本挑战，该能力指神经网络表示任务的能力。&lt;h4&gt;目的&lt;/h4&gt;引入持续学习的有效模型容量(CLEMC)，用于描述稳定性-塑性平衡点的动态行为。&lt;h4&gt;方法&lt;/h4&gt;开发差分方程建模神经网络、任务数据和优化过程的相互作用演变；利用CLEMC分析有效容量的特性；在多种架构网络(前馈网络、卷积网络、图神经网络、大型语言模型)上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;无论神经网络架构或优化方法如何，当传入任务分布与之前不同时，神经网络表示新任务的能力会减弱；有效容量及稳定性-塑性平衡点本质上是非平稳的。&lt;h4&gt;结论&lt;/h4&gt;理论发现得到广泛实验支持，这些实验覆盖了从小型到大型多种神经网络架构。&lt;h4&gt;翻译&lt;/h4&gt;稳定性-塑性困境与神经网络(NN)能力-其表示任务的能力-密切相关，是持续学习(CL)中的一个基本挑战。在此背景下，我们引入了持续学习的有效模型容量(CLEMC)，用于描述稳定性-塑性平衡点的动态行为。我们开发了一个差分方程来建模神经网络、任务数据和优化过程之间相互作用的演变。然后我们利用CLEMC证明，有效容量以及稳定性-塑性平衡点本质上是非平稳的。我们表明，无论神经网络架构或优化方法如何，当传入的任务分布与先前不同时，神经网络表示新任务的能力会减弱。我们进行了广泛的实验来支持我们的理论发现，涵盖了从小的前馈网络和卷积网络到中等规模的图神经网络和基于Transformer的具有数百万参数的大型语言模型等多种架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The stability-plasticity dilemma, closely related to a neural network's (NN)capacity-its ability to represent tasks-is a fundamental challenge in continuallearning (CL). Within this context, we introduce CL's effective model capacity(CLEMC) that characterizes the dynamic behavior of the stability-plasticitybalance point. We develop a difference equation to model the evolution of theinterplay between the NN, task data, and optimization procedure. We thenleverage CLEMC to demonstrate that the effective capacity-and, by extension,the stability-plasticity balance point is inherently non-stationary. We showthat regardless of the NN architecture or optimization method, a NN's abilityto represent new tasks diminishes when incoming task distributions differ fromprevious ones. We conduct extensive experiments to support our theoreticalfindings, spanning a range of architectures-from small feedforward network andconvolutional networks to medium-sized graph neural networks andtransformer-based large language models with millions of parameters.</description>
      <author>example@mail.com (Supriyo Chakraborty, Krishnan Raghavan)</author>
      <guid isPermaLink="false">2508.08052v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</title>
      <link>http://arxiv.org/abs/2508.08005v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个基于学习的框架，结合传统机器学习和图神经网络来解决最大团问题(MCP)的算法选择问题，通过实验证明双通道架构在算法选择中的有效性。&lt;h4&gt;背景&lt;/h4&gt;现有研究表明，没有单一的最大团算法能在所有实例上表现最佳，这突显了根据实例特征选择合适算法的重要性。然而，针对最大团问题的算法选择研究较少。&lt;h4&gt;目的&lt;/h4&gt;填补最大团问题算法选择研究的空白，开发一种能够根据图实例特征选择最合适算法的方法。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于学习的框架，结合传统机器学习和图神经网络。通过在多样化的图实例上运行四种精确MCP算法构建标记数据集，并提取每个图的结构和全局统计特征。评估了四种传统分类器(SVM、随机森林、决策树和KNN)，并基于这些发现开发了名为GAT-MLP的双通道模型，该模型结合了用于局部结构编码的图注意力网络(GAT)和用于全局特征建模的多层感知机(MLP)。&lt;h4&gt;主要发现&lt;/h4&gt;随机森林(RF)在各种数据集变体和指标上表现一致且强劲，是可靠的基线；连通性和拓扑结构是算法性能的强预测指标；GAT-MLP模型在所有指标上表现出强劲且一致的性能。&lt;h4&gt;结论&lt;/h4&gt;双通道架构在组合算法选择中有效，图神经网络在该领域具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;广泛的实验和先前的研究表明，没有任何单一的最大团算法能在所有实例上始终表现最佳，这突显了根据实例特征选择合适算法的重要性。通过对相关研究的广泛分析，发现针对最大团问题(MCP)的算法选择研究工作较少。在本工作中，我们提出了一个基于学习的框架，结合传统机器学习和图神经网络来解决这一空白。我们通过在多样化的图实例上运行四种精确的MCP算法构建了一个标记数据集，并提取了每个图的结构和全局统计特征。我们首先评估了四种传统分类器：支持向量机(SVM)、随机森林(RF)、决策树(DT)和K近邻(KNN)，在多个数据集变体上的表现。实验结果表明，RF在指标和数据集变体中始终表现出强劲性能，使其成为一个可靠的基线。此外，特征重要性分析表明，连通性和拓扑结构是算法性能的强预测指标。基于这些发现，我们开发了一个名为GAT-MLP的双通道模型，该模型结合了用于局部结构编码的图注意力网络(GAT)和用于全局特征建模的多层感知机(MLP)。GAT-MLP模型在所有指标上表现出强劲且一致的性能。我们的结果突显了双通道架构的有效性以及图神经网络在组合算法选择中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extensive experiments and prior studies show that no single maximum cliquealgorithm consistently performs best across all instances, highlighting theimportance of selecting suitable algorithms based on instance features. Throughan extensive analysis of relevant studies, it is found that there is a lack ofresearch work concerning algorithm selection oriented toward the Maximum CliqueProblem (MCP). In this work, we propose a learning-based framework thatintegrates both traditional machine learning and graph neural networks toaddress this gap. We construct a labeled dataset by running four exact MCPalgorithms on a diverse collection of graph instances, accompanied bystructural and global statistical features extracted from each graph. We firstevaluate four conventional classifiers: Support Vector Machine (SVM), RandomForest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multipledataset variants. Experimental results show that RF consistently shows strongperformance across metrics and dataset variants, making it a reliable baseline.In addition, feature importance analysis indicates that connectivity andtopological structure are strong predictors of algorithm performance. Buildingon these findings, we develop a dual-channel model named GAT-MLP, whichcombines a Graph Attention Network (GAT) for local structural encoding with aMultilayer Perceptron (MLP) for global feature modeling. The GAT-MLP modelshows strong and consistent performance across all metrics. Our resultshighlight the effectiveness of dual-channel architectures and the promise ofgraph neural networks in combinatorial algorithm selection.</description>
      <author>example@mail.com (Xiang Li, Shanshan Wang, Chenglong Xiao)</author>
      <guid isPermaLink="false">2508.08005v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TLV-HGNN: Thinking Like a Vertex for Memory-efficient HGNN Inference</title>
      <link>http://arxiv.org/abs/2508.07796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures, accepted by ICCD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TVL-HGNN的可重构硬件加速器，通过创新的语义完整执行范式和顶点分组技术解决了异构图神经网络推理中邻居聚合阶段的内存低效问题，实现了显著的性能提升和能耗降低。&lt;h4&gt;背景&lt;/h4&gt;异构图神经网络(HGNNs)在处理异构图数据方面表现出色，广泛应用于关键领域。在HGNN推理中，邻居聚合阶段是性能的主要决定因素，但存在两个主要的内存低效问题：按语义执行范式导致内存扩展，以及聚合过程中的冗余内存访问。&lt;h4&gt;目的&lt;/h4&gt;消除按语义的中间存储和冗余目标顶点访问，设计优化的硬件加速器，减少对共享邻居的冗余访问，从而提高HGNN推理的性能和能效。&lt;h4&gt;方法&lt;/h4&gt;提出从顶点视角出发的语义完整执行范式，设计TVL-HGNN硬件加速器，以及实现基于跨语义邻域重叠的顶点分组技术并进行硬件实现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TVL-HGNN比NVIDIA A100 GPU平均加速7.85倍，比最先进的HGNN加速器HiHGNN平均加速1.41倍，同时能耗降低98.79%(相比A100 GPU)和32.61%(相比HiHGNN)。&lt;h4&gt;结论&lt;/h4&gt;TVL-HGNN通过创新的执行范式和硬件设计显著提高了HGNN推理的性能和能效，解决了传统方法中的内存低效问题。&lt;h4&gt;翻译&lt;/h4&gt;异构图神经网络(HGNNs)在处理异构图数据方面表现出色，并广泛应用于关键领域。在HGNN推理中，邻居聚合阶段是性能的主要决定因素，但该阶段存在两个主要的内存低效问题。首先，常用的按语义执行范式在语义融合前为每个语义存储中间聚合结果，导致大量内存扩展。其次，聚合过程会产生大量冗余内存访问，包括跨语义重复加载目标顶点特征和由于跨语义邻域重叠导致的重复访问共享邻居。这些低效性严重限制了可扩展性并降低了HGNN推理性能。在本工作中，我们首先提出一种从顶点视角出发的语义完整执行范式，消除了按语义的中间存储和冗余目标顶点访问。基于该范式，我们设计了TVL-HGNN，这是一个为高效聚合而优化的可重构硬件加速器。此外，我们引入了一种基于跨语义邻域重叠的顶点分组技术，并进行了硬件实现，以减少对共享邻居的冗余访问。实验结果表明，TVL-HGNN比NVIDIA A100 GPU和最先进的HGNN加速器HiHGNN分别实现了平均7.85倍和1.41倍的加速，同时能耗降低了98.79%和32.61%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous graph neural networks (HGNNs) excel at processing heterogeneousgraph data and are widely applied in critical domains. In HGNN inference, theneighbor aggregation stage is the primary performance determinant, yet itsuffers from two major sources of memory inefficiency. First, the commonlyadopted per-semantic execution paradigm stores intermediate aggregation resultsfor each semantic prior to semantic fusion, causing substantial memoryexpansion. Second, the aggregation process incurs extensive redundant memoryaccesses, including repeated loading of target vertex features across semanticsand repeated accesses to shared neighbors due to cross-semantic neighborhoodoverlap. These inefficiencies severely limit scalability and reduce HGNNinference performance.  In this work, we first propose a semantics-complete execution paradigm from avertex perspective that eliminates per-semantic intermediate storage andredundant target vertex accesses. Building on this paradigm, we designTVL-HGNN, a reconfigurable hardware accelerator optimized for efficientaggregation. In addition, we introduce a vertex grouping technique based oncross-semantic neighborhood overlap, with hardware implementation, to reduceredundant accesses to shared neighbors. Experimental results demonstrate thatTVL-HGNN achieves average speedups of 7.85x and 1.41x over the NVIDIA A100 GPUand the state-of-the-art HGNN accelerator HiHGNN, respectively, while reducingenergy consumption by 98.79% and 32.61%.</description>
      <author>example@mail.com (Dengke Han, Duo Wang, Mingyu Yan, Xiaochun Ye, Dongrui Fan)</author>
      <guid isPermaLink="false">2508.07796v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning</title>
      <link>http://arxiv.org/abs/2508.07659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于时空图神经网络的方法，通过自适应边采样解决大气状态估计中的空间相关性问题，提高了天气预报准确性。&lt;h4&gt;背景&lt;/h4&gt;数值天气预报(NWP)系统是天气预报的基础，通过分析前期大气状态和新获取的地球观测数据来预测未来大气状态。然而，周围气象背景和观测位置的变化使得大气状态和观测之间的空间相关性随时间动态变化，增加了预测难度。&lt;h4&gt;目的&lt;/h4&gt;发现地球观测和大气状态之间的空间相关性，以提高全球大气状态估计的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;采用具有结构学习的时空图神经网络(STGNNs)处理动态变化的空间相关性，并通过自适应确定节点度数并考虑NWP网格点和观测之间的空间距离来调节边采样，解决结构学习中的结构信息损失和过平滑问题。&lt;h4&gt;主要发现&lt;/h4&gt;使用东亚地区的实际数据验证，即使在大气变化程度高的区域，所提方法也优于现有有结构学习和无结构学习的STGNN模型。&lt;h4&gt;结论&lt;/h4&gt;自适应边采样的时空图神经网络能有效处理动态变化的空间相关性，提高大气状态估计和天气预报的准确性。&lt;h4&gt;翻译&lt;/h4&gt;本研究旨在发现地球观测和大气状态之间的空间相关性，以提高全球大气状态估计的预测准确性，这些估计通常使用传统的数值天气预报(NWP)系统进行，是天气预报的开始。NWP系统通过分析前期大气状态和新获取的地球观测数据来预测固定位置(称为NWP网格点)的未来大气状态。因此，周围气象背景和观测位置的变化使得大气状态和观测之间的空间相关性随时间变化。为了处理这种动态变化的空间相关性，我们采用了具有结构学习的时空图神经网络(STGNNs)。然而，结构学习存在一个固有局限性，即通过生成过多边可能导致结构信息损失和过平滑问题。为了解决这个问题，我们通过自适应确定节点度数并考虑NWP网格点和观测之间的空间距离来调节边采样。我们使用东亚地区的实际大气状态和观测数据验证了所提方法的有效性。即使在大气变化程度高的区域，所提方法也优于现有有结构学习和无结构学习的STGNN模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study aims to discover spatial correlations between Earth observationsand atmospheric states to improve the forecasting accuracy of globalatmospheric state estimation, which are usually conducted using conventionalnumerical weather prediction (NWP) systems and is the beginning of weatherforecasting. NWP systems predict future atmospheric states at fixed locations,which are called NWP grid points, by analyzing previous atmospheric states andnewly acquired Earth observations without fixed locations. Thus, surroundingmeteorological context and the changing locations of the observations makespatial correlations between atmospheric states and observations over time. Tohandle complicated spatial correlations, which change dynamically, we employspatiotemporal graph neural networks (STGNNs) with structure learning. However,structure learning has an inherent limitation that this can cause structuralinformation loss and over-smoothing problem by generating excessive edges. Tosolve this problem, we regulate edge sampling by adaptively determining nodedegrees and considering the spatial distances between NWP grid points andobservations. We validated the effectiveness of the proposed method by usingreal-world atmospheric state and observation data from East Asia. Even in areaswith high atmospheric variability, the proposed method outperformed existingSTGNN models with and without structure learning.</description>
      <author>example@mail.com (Hyeon-Ju Jeon, Jeon-Ho Kang, In-Hyuk Kwon, O-Joun Lee)</author>
      <guid isPermaLink="false">2508.07659v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction</title>
      <link>http://arxiv.org/abs/2508.07624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图的后处理流程，利用物体间的空间关系来修正检测异常，显著提高了物体检测系统的性能和可靠性。&lt;h4&gt;背景&lt;/h4&gt;在许多现实世界的静态环境应用中，物体的空间布局在不同实例中保持一致。然而，最先进的物体检测模型往往无法利用这种空间先验知识，导致预测不一致、漏检或误分类，特别是在杂乱或遮挡场景中。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图的后处理流程，明确建模物体间的空间关系，以修正第一人称视角帧中的检测异常。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNN)在手动标注数据上训练，模型能够识别无效的物体类别标签，并根据邻域上下文预测修正后的类别标签。该方法既可以作为独立的异常检测和修正框架，也可以作为标准物体检测器(如YOLOv7和RT-DETR)的后处理模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，整合这种空间推理显著提高了检测性能，mAP@50提升了最高达4%。&lt;h4&gt;结论&lt;/h4&gt;这种方法强调了利用环境空间结构提高物体检测系统可靠性的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在许多涉及静态环境的现实世界应用中，物体的空间布局在不同实例中保持一致。然而，最先进的物体检测模型往往无法利用这种空间先验知识，导致预测不一致、漏检或误分类，特别是在杂乱或遮挡场景中。在这项工作中，我们提出了一种基于图的后处理流程，明确建模物体间的空间关系，以修正第一人称视角帧中的检测异常。使用在手动标注数据上训练的图神经网络(GNN)，我们的模型能够识别无效的物体类别标签，并根据其邻域上下文预测修正后的类别标签。我们将我们的方法评估为独立的异常检测和修正框架，以及标准物体检测器(如YOLOv7和RT-DETR)的后处理模块。实验证明，整合这种空间推理显著提高了检测性能，mAP_50提升了最高达4%。这种方法强调了利用环境空间结构提高物体检测系统可靠性的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In many real-world applications involving static environments, the spatiallayout of objects remains consistent across instances. However,state-of-the-art object detection models often fail to leverage this spatialprior, resulting in inconsistent predictions, missed detections, ormisclassifications, particularly in cluttered or occluded scenes. In this work,we propose a graph-based post-processing pipeline that explicitly models thespatial relationships between objects to correct detection anomalies inegocentric frames. Using a graph neural network (GNN) trained on manuallyannotated data, our model identifies invalid object class labels and predictscorrected class labels based on their neighbourhood context. We evaluate ourapproach both as a standalone anomaly detection and correction framework and asa post-processing module for standard object detectors such as YOLOv7 andRT-DETR. Experiments demonstrate that incorporating this spatial reasoningsignificantly improves detection performance, with mAP@50 gains of up to 4%.This method highlights the potential of leveraging the environment's spatialstructure to improve reliability in object detection systems.</description>
      <author>example@mail.com (Vishakha Lall, Yisi Liu)</author>
      <guid isPermaLink="false">2508.07624v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Neuro-Symbolic Acceleration of MILP Motion Planning with Temporal Logic and Chance Constraints</title>
      <link>http://arxiv.org/abs/2508.07515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种神经符号方法来解决自主系统中的复杂运动规划问题，通过机器学习技术引导MILP求解器的符号搜索，显著提高了计算效率和解决方案质量，比现有方法平均提升约20%。&lt;h4&gt;背景&lt;/h4&gt;自主系统需要解决日益复杂、时效性强且具有不确定性的任务中的运动规划问题。这些问题通常涉及高层任务规范，如时序逻辑或机会约束，需要解决大规模混合整数线性规划(MILP)问题。然而，现有的基于MILP的规划方法计算成本高且可扩展性有限，阻碍了它们的实时应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种神经符号方法来加速基于MILP的运动规划，利用机器学习技术引导求解器的符号搜索。&lt;h4&gt;方法&lt;/h4&gt;专注于两类代表性的规划问题：具有信号时序逻辑(STL)规范的问题和通过形式化预测编程(CPP)表述机会约束的问题。展示了如何基于图神经网络的学习方法引导传统符号MILP求解器解决具有挑战性的规划问题，包括分支变量选择和求解器参数配置。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验，表明神经符号搜索技术带来了可扩展性的提升。该方法取得了显著的改进，在关键指标上（包括运行时间和解决方案质量）比最先进的求解器平均提高了约20%的性能。&lt;h4&gt;结论&lt;/h4&gt;神经符号方法可以有效解决自主系统中的复杂运动规划问题，通过机器学习引导MILP求解器可以显著提高计算效率和解决方案质量。&lt;h4&gt;翻译&lt;/h4&gt;自主系统必须解决日益复杂、时效性强且具有不确定性的任务中的运动规划问题。这些问题通常涉及高层任务规范，如时序逻辑或机会约束，需要解决大规模混合整数线性规划(MILP)问题。然而，现有的基于MILP的规划方法计算成本高且可扩展性有限，阻碍了它们的实时应用。我们提出使用神经符号方法来加速基于MILP的运动规划，利用机器学习技术引导求解器的符号搜索。专注于两类代表性的规划问题，即具有信号时序逻辑(STL)规范的问题和通过形式化预测编程(CPP)表述机会约束的问题。我们展示了如何基于图神经网络的学习方法引导传统符号MILP求解器解决具有挑战性的规划问题，包括分支变量选择和求解器参数配置。通过大量实验，我们表明神经符号搜索技术带来了可扩展性的提升。我们的方法取得了显著的改进，在关键指标上（包括运行时间和解决方案质量）比最先进的求解器平均提高了约20%的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous systems must solve motion planning problems subject toincreasingly complex, time-sensitive, and uncertain missions. These problemsoften involve high-level task specifications, such as temporal logic or chanceconstraints, which require solving large-scale Mixed-Integer Linear Programs(MILPs). However, existing MILP-based planning methods suffer from highcomputational cost and limited scalability, hindering their real-timeapplicability. We propose to use a neuro-symbolic approach to accelerateMILP-based motion planning by leveraging machine learning techniques to guidethe solver's symbolic search. Focusing on two representative classes ofplanning problems, namely, those with Signal Temporal Logic (STL)specifications and those with chance constraints formulated via ConformalPredictive Programming (CPP). We demonstrate how graph neural network-basedlearning methods can guide traditional symbolic MILP solvers in solvingchallenging planning problems, including branching variable selection andsolver parameter configuration. Through extensive experiments, we show thatneuro-symbolic search techniques yield scalability gains. Our approach yieldssubstantial improvements, achieving an average performance gain of about 20%over state-of-the-art solver across key metrics, including runtime and solutionquality.</description>
      <author>example@mail.com (Junyang Cai, Weimin Huang, Jyotirmoy V. Deshmukh, Lars Lindemann, Bistra Dilkina)</author>
      <guid isPermaLink="false">2508.07515v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering</title>
      <link>http://arxiv.org/abs/2508.07486v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mo2oM的框架，用于将单体软件系统转换为重叠的微服务架构，允许组件以概率方式属于多个微服务，从而提高系统的模块化、可维护性和部署灵活性。&lt;h4&gt;背景&lt;/h4&gt;现代软件系统正从单体架构转向微服务架构以提高可扩展性、可维护性和部署灵活性。然而，现有微服务提取方法通常依赖硬聚类，将每个组件分配到单一微服务，这往往增加服务间耦合并降低服务内聚。&lt;h4&gt;目的&lt;/h4&gt;提出Mo2oM框架，将微服务提取制定为软聚类问题，允许组件以概率方式属于多个微服务，以减少服务间耦合并提高服务内聚。&lt;h4&gt;方法&lt;/h4&gt;Mo2oM结合深度语义嵌入和从方法调用图中提取的结构依赖关系，以捕获功能性和架构性关系，然后使用基于图神经网络的软聚类算法生成最终的微服务集合。&lt;h4&gt;主要发现&lt;/h4&gt;在四个开源单体基准测试上评估显示，Mo2oM在结构模块化方面提高40.97%，在服务间调用百分比（通信开销）方面提高58%，在接口数量方面提高26.16%，在非极端分布（服务大小平衡）方面提高38.96%。&lt;h4&gt;结论&lt;/h4&gt;Mo2oM框架通过允许组件以概率方式属于多个微服务，有效提高了微服务架构的模块化、通信效率、接口数量平衡和服务大小平衡，显著优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;现代软件系统正越来越多地从单体架构转向微服务架构，以提高可扩展性、可维护性和部署灵活性。现有的微服务提取方法通常依赖硬聚类，将每个软件组件分配到单个微服务。这种方法通常会增加服务间耦合并降低服务内聚。我们提出了Mo2oM（单体到重叠微服务）框架，将微服务提取制定为软聚类问题，允许组件以概率方式属于多个微服务。这种方法受到专家驱动的分解启发，实践者有意地在服务间复制某些软件组件以减少通信开销。Mo2oM结合了深度语义嵌入和从方法调用图中提取的结构依赖关系，以捕获功能性和架构性关系。然后使用基于图神经网络的软聚类算法生成最终的微服务集合。我们在四个开源单体基准测试上评估了Mo2oM，并与八个最先进的基线方法进行比较。我们的结果表明，Mo2oM在所有基准测试中，结构模块化（平衡内聚和耦合）方面提高了40.97%，服务间调用百分比（通信开销）方面提高了58%，接口数量（模块化和解耦）方面提高了26.16%，非极端分布（服务大小平衡）方面提高了38.96%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern software systems are increasingly shifting from monolithicarchitectures to microservices to enhance scalability, maintainability, anddeployment flexibility. Existing microservice extraction methods typically relyon hard clustering, assigning each software component to a single microservice.This approach often increases inter-service coupling and reduces intra-servicecohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), aframework that formulates microservice extraction as a soft clustering problem,allowing components to belong probabilistically to multiple microservices. Thisapproach is inspired by expert-driven decompositions, where practitionersintentionally replicate certain software components across services to reducecommunication overhead. Mo2oM combines deep semantic embeddings with structuraldependencies extracted from methodcall graphs to capture both functional andarchitectural relationships. A graph neural network-based soft clusteringalgorithm then generates the final set of microservices. We evaluate Mo2oM onfour open-source monolithic benchmarks and compare it against eightstate-of-the-art baselines. Our results demonstrate that Mo2oM achievesimprovements of up to 40.97% in structural modularity (balancing cohesion andcoupling), 58% in inter-service call percentage (communication overhead),26.16% in interface number (modularity and decoupling), and 38.96% innon-extreme distribution (service size balance) across all benchmarks.</description>
      <author>example@mail.com (Morteza Ziabakhsh, Kiyan Rezaee, Sadegh Eskandari, Seyed Amir Hossein Tabatabaei, Mohammad M. Ghassemi)</author>
      <guid isPermaLink="false">2508.07486v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures</title>
      <link>http://arxiv.org/abs/2508.07423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis, Chapters 8 and 9 include results from work performed in  collaboration with Anthony Correia&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了为CERN的LHCb实验开发的基于图神经网络的管道，用于带电粒子轨迹重建，该管道完全在GPU上实现并嵌入到第一级触发器中，同时也在FPGA架构上进行了加速，并与经典算法进行了性能比较。&lt;h4&gt;背景&lt;/h4&gt;粒子物理需要更高精度测试亚原子模型，对撞机实验探测器升级导致更多碰撞和复杂相互作用，数据量大幅增加，CERN产生的数据量巨大，需要实时过滤和选择。&lt;h4&gt;目的&lt;/h4&gt;理解如何在高频率数据处理环境中高效部署机器学习模型，最大化吞吐量并最小化能耗，应对严格的高频率数据速率挑战。&lt;h4&gt;方法&lt;/h4&gt;开发基于图神经网络的管道用于LHCb实验中的带电粒子轨迹重建，完全在GPU上实现端到端嵌入到第一级触发器中，与经典跟踪算法比较性能，并在FPGA架构上加速，比较GPU和FPGA实现的功耗和处理速度。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体主要发现。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及具体结论。&lt;h4&gt;翻译&lt;/h4&gt;随着粒子物理界需要越来越高的精度来测试我们对亚原子世界的当前模型，更大的数据集是必要的。随着世界各地对撞机实验探测器的升级计划，特别是CERN的大型强子对撞机，预计会有更多的碰撞和更复杂的相互作用。这直接意味着产生的数据量增加，以及处理这些数据所需的计算资源相应增加。在CERN，产生的数据量是巨大的。这就是为什么数据必须在永久存储前进行实时过滤和选择。然后，这些数据可用于进行物理分析，以扩展我们对宇宙的当前理解并改进物理的标准模型。这种实时过滤，称为触发，涉及复杂的处理，频率高达40 MHz。本论文有助于理解如何在这样的环境中高效部署机器学习模型，以最大化吞吐量并最小化能耗。为了应对严格的高频率数据速率带来的挑战，不可避免地需要用于此类任务的现代硬件和当代算法。在本工作中，我展示了我们为CERN的LHCb实验中带电粒子轨迹重建而开发的基于图神经网络的管道。该管道完全在GPU上实现，端到端地嵌入到LHCb的第一级触发器中。其性能与LHCb当前使用的经典跟踪算法进行了比较。该管道还在FPGA架构上进行了加速，并在功耗和处理速度方面将其性能与GPU实现进行了比较。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the particle physics community needs higher and higher precisions in orderto test our current model of the subatomic world, larger and larger datasetsare necessary. With upgrades scheduled for the detectors of colliding-beamexperiments around the world, and specifically at the Large Hadron Collider atCERN, more collisions and more complex interactions are expected. This directlyimplies an increase in data produced and consequently in the computationalresources needed to process them. At CERN, the amount of data produced isgargantuan. This is why the data have to be heavily filtered and selected inreal time before being permanently stored. This data can then be used toperform physics analyses, in order to expand our current understanding of theuniverse and improve the Standard Model of physics. This real-time filtering,known as triggering, involves complex processing happening often at frequenciesas high as 40 MHz. This thesis contributes to understanding how machinelearning models can be efficiently deployed in such environments, in order tomaximize throughput and minimize energy consumption. Inevitably, modernhardware designed for such tasks and contemporary algorithms are needed inorder to meet the challenges posed by the stringent, high-frequency data rates.In this work, I present our graph neural network-based pipeline, developed forcharged particle track reconstruction at the LHCb experiment at CERN. Thepipeline was implemented end-to-end inside LHCb's first-level trigger, entirelyon GPUs. Its performance was compared against the classical tracking algorithmscurrently in production at LHCb. The pipeline was also accelerated on the FPGAarchitecture, and its performance in terms of power consumption and processingspeed was compared against the GPU implementation.</description>
      <author>example@mail.com (Fotis I. Giasemis)</author>
      <guid isPermaLink="false">2508.07423v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging GNN to Enhance MEF Method in Predicting ENSO</title>
      <link>http://arxiv.org/abs/2508.07410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于图分析的新框架来改进ENSO预测，通过构建集合成员之间的相似性图并使用社区检测方法选择最优子集，提高了预测技能，提供了对集合行为的新见解，并适用于其他预测模型。&lt;h4&gt;背景&lt;/h4&gt;ENSO的长期可靠预测一直是气候科学中的长期挑战。之前开发的MEF模型使用了80个集合预测，通过两个独立的深度学习模块：3D卷积神经网络和时间序列模块，但未对单个集合成员进行单独加权或测试，这可能限制了模型对高性能但分散预测的优化使用。&lt;h4&gt;目的&lt;/h4&gt;改进现有的预测框架，优化使用高性能但分散的预测，提高ENSO长期预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于图分析的新框架，构建一个无向图，顶点是集合输出，边权重通过RMSE和相关性测量相似性，使用社区检测方法识别并聚类结构相似且准确的预测，从中获得20个成员的优化子集，通过平均这个优化子集获得最终预测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法通过去除噪声和强调集合一致性提高了预测技能；基于图的选择显示顶级预测者具有稳健的统计特征，提供了新的集合行为见解；GNN方法在复合长期情况下产生更稳定和一致的输出；该方法是模型不可知的，可以应用于其他具有巨大集合输出的预测模型。&lt;h4&gt;结论&lt;/h4&gt;基于图的集合选择方法改进了ENSO预测的准确性，并可以推广到其他预测模型。&lt;h4&gt;翻译&lt;/h4&gt;厄尔尼诺-南方振荡(ENSO)的可靠长期预测一直是气候科学中的一个长期挑战。先前开发的多模态ENSO预测(MEF)模型使用两个独立深度学习模块的80个集合预测：一个3D卷积神经网络(3D-CNN)和一个时间序列模块。在他们的方法中，两个模块的输出使用一种加权策略组合，其中一个根据全球性能优先于另一个。然而，没有对单个集合成员进行单独加权或测试，这可能限制了模型优化使用高性能但分散的预测。在本研究中，我们提出了一个更好的框架，使用基于图的分析直接建模所有80个集合成员之间的相似性。通过构建一个无向图，其顶点是集合输出，边上的权重通过RMSE和相关性测量相似性，我们识别并聚类结构相似且准确的预测。从中我们使用社区检测方法获得了一个由20个成员组成的优化子集。然后通过平均这个优化子集获得最终预测。这种方法通过去除噪声和强调集合一致性提高了预测技能。有趣的是，我们的基于图的选择显示顶级预测者具有稳健的统计特征，提供了新的集合行为见解。此外，我们观察到，虽然基于GNN的方法在每种情况下并不总是优于基线MEF，但它产生更稳定和一致的输出，特别是在复合长期情况下。该方法也是模型不可知的，表明它可以直接应用于其他具有巨大集合输出的预测模型，如统计、物理或混合模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable long-lead forecasting of the El Nino Southern Oscillation (ENSO)remains a long-standing challenge in climate science. The previously developedMultimodal ENSO Forecast (MEF) model uses 80 ensemble predictions by twoindependent deep learning modules: a 3D Convolutional Neural Network (3D-CNN)and a time-series module. In their approach, outputs of the two modules arecombined using a weighting strategy wherein one is prioritized over the otheras a function of global performance. Separate weighting or testing ofindividual ensemble members did not occur, however, which may have limited themodel to optimize the use of high-performing but spread-out forecasts. In thisstudy, we propose a better framework that employs graph-based analysis todirectly model similarity between all 80 members of the ensemble. Byconstructing an undirected graph whose vertices are ensemble outputs and whoseweights on edges measure similarity (via RMSE and correlation), we identify andcluster structurally similar and accurate predictions. From which we obtain anoptimized subset of 20 members using community detection methods. The finalprediction is then obtained by averaging this optimized subset. This methodimproves the forecast skill through noise removal and emphasis on ensemblecoherence. Interestingly, our graph-based selection shows robust statisticalcharacteristics among top performers, offering new ensemble behavior insights.In addition, we observe that while the GNN-based approach does not alwaysoutperform the baseline MEF under every scenario, it produces more stable andconsistent outputs, particularly in compound long-lead situations. The approachis model-agnostic too, suggesting that it can be applied directly to otherforecasting models with gargantuan ensemble outputs, such as statistical,physical, or hybrid models.</description>
      <author>example@mail.com (Saghar Ganji, Mohammad Naisipour)</author>
      <guid isPermaLink="false">2508.07410v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.07122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于时空图神经网络的性能预测算法，用于解决具有多层服务调用结构的分布式后端系统中性能波动的预测挑战。&lt;h4&gt;背景&lt;/h4&gt;分布式后端系统面临性能波动预测的挑战，这些系统具有多层服务调用结构，使得性能预测变得复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测分布式后端系统性能波动的算法，帮助系统管理者更好地进行性能优化和资源分配。&lt;h4&gt;方法&lt;/h4&gt;将不同时间片的系统状态抽象为图结构序列，整合服务节点的运行时特征与服务间的调用关系构建统一时空建模框架；使用图卷积网络提取服务拓扑的高阶依赖信息；采用门控循环网络捕获性能指标的动态演化；引入时间编码机制增强对非平稳时间序列的表示能力；通过端到端方式训练架构实现高精度回归。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模公共集群数据集上的实验表明，该方法在MAE、RMSE和R2等关键指标上优于现有代表性方法；在不同负载强度和结构复杂度下保持强鲁棒性；设计的多维度实验（包括时间窗口和并发负载水平变化）全面验证了模型的预测性能和稳定性。&lt;h4&gt;结论&lt;/h4&gt;该模型在后端服务性能管理任务中具有实际应用潜力，能够有效支持分布式系统的性能优化决策。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于时空图神经网络的性能预测算法，以应对具有多层服务调用结构的分布式后端系统中性能波动的预测挑战。该方法将不同时间片的系统状态抽象为图结构序列，整合服务节点的运行时特征与服务间的调用关系，构建统一的时空建模框架。模型首先应用图卷积网络从服务拓扑中提取高阶依赖信息，然后使用门控循环网络捕获性能指标随时间的动态演化。同时引入时间编码机制增强模型对非平稳时间序列的表示能力。该架构以端到端方式训练，优化多层嵌套结构以实现未来服务性能指标的高精度回归。为验证所提方法的有效性，使用了大规模公共集群数据集，并设计了一系列多维度实验，包括时间窗口和并发负载水平的变化。这些实验全面评估了模型的预测性能和稳定性。实验结果表明，所提模型在MAE、RMSE和R2等关键指标上优于现有代表性方法，并在不同负载强度和结构复杂度下保持强鲁棒性。这些结果证明了该模型在后端服务性能管理任务中的实际应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a spatiotemporal graph neural network-based performanceprediction algorithm to address the challenge of forecasting performancefluctuations in distributed backend systems with multi-level service callstructures. The method abstracts system states at different time slices into asequence of graph structures. It integrates the runtime features of servicenodes with the invocation relationships among services to construct a unifiedspatiotemporal modeling framework. The model first applies a graphconvolutional network to extract high-order dependency information from theservice topology. Then it uses a gated recurrent network to capture the dynamicevolution of performance metrics over time. A time encoding mechanism is alsointroduced to enhance the model's ability to represent non-stationary temporalsequences. The architecture is trained in an end-to-end manner, optimizing themulti-layer nested structure to achieve high-precision regression of futureservice performance metrics. To validate the effectiveness of the proposedmethod, a large-scale public cluster dataset is used. A series ofmulti-dimensional experiments are designed, including variations in timewindows and concurrent load levels. These experiments comprehensively evaluatethe model's predictive performance and stability. The experimental results showthat the proposed model outperforms existing representative methods across keymetrics such as MAE, RMSE, and R2. It maintains strong robustness under varyingload intensities and structural complexities. These results demonstrate themodel's practical potential for backend service performance management tasks.</description>
      <author>example@mail.com (Zhihao Xue, Yun Zi, Nia Qi, Ming Gong, Yujun Zou)</author>
      <guid isPermaLink="false">2508.07122v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context</title>
      <link>http://arxiv.org/abs/2508.07117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 3 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LOGIC，一种用于图神经网络(GNN)预测解释的轻量级事后框架，利用大型语言模型生成忠实且可解释的解释，通过将GNN节点嵌入投影到LLM嵌入空间并构建混合提示，使LLM能够推理GNN内部表示并生成自然语言解释和简洁解释子图。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为处理结构化数据(包括文本属性图)的强大工具，常见于引文网络、社交平台和知识图谱等领域。然而，GNN本身不具有内在可解释性，现有解释方法在生成可解释的细粒度理由方面存在困难，特别是当节点属性包含丰富的自然语言时。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够为GNN预测生成忠实且可解释的解释的方法，提高解释质量使其更符合人类推理方式，并在保真度和稀疏性之间取得良好平衡。&lt;h4&gt;方法&lt;/h4&gt;提出LOGIC框架，将GNN节点嵌入投影到大型语言模型嵌入空间，构建混合提示交错软提示与图结构文本输入，使LLM能够推理GNN内部表示并生成自然语言解释和简洁解释子图。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实文本属性图数据集上的实验表明，LOGIC在保真度和稀疏性之间取得有利平衡，同时显著提高了以人为中心的指标如洞察力。&lt;h4&gt;结论&lt;/h4&gt;LOGIC为图神经网络提供了一种新的解释方法，通过利用大型语言模型能力生成更符合人类推理方式的解释，为基于LLM的图学习可解释性开辟了新方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为处理结构化数据的强大工具，包括文本属性图，这些图在引文网络、社交平台和知识图谱等领域很常见。GNN本身不具有内在可解释性，因此提出了许多解释方法。然而，现有的解释方法在生成可解释的细粒度理由方面往往存在困难，特别是当节点属性包含丰富的自然语言时。在这项工作中，我们介绍了LOGIC，一种轻量级的事后框架，使用大型语言模型(LLM)为GNN预测生成忠实且可解释的解释。LOGIC将GNN节点嵌入投影到LLM嵌入空间，并构建混合提示，将软提示与来自图结构的文本输入交错。这使得LLM能够推理GNN的内部表示，并生成自然语言解释以及简洁的解释子图。我们在四个真实的TAG数据集上的实验表明，LOGIC在保真度和稀疏性之间取得了有利的平衡，同时显著提高了以人为中心的指标，如洞察力。LOGIC通过将GNN内部与人类推理对齐，为基于LLM的图学习可解释性设定了新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful tools for learning overstructured data, including text-attributed graphs, which are common in domainssuch as citation networks, social platforms, and knowledge graphs. GNNs are notinherently interpretable and thus, many explanation methods have been proposed.However, existing explanation methods often struggle to generate interpretable,fine-grained rationales, especially when node attributes include rich naturallanguage. In this work, we introduce LOGIC, a lightweight, post-hoc frameworkthat uses large language models (LLMs) to generate faithful and interpretableexplanations for GNN predictions. LOGIC projects GNN node embeddings into theLLM embedding space and constructs hybrid prompts that interleave soft promptswith textual inputs from the graph structure. This enables the LLM to reasonabout GNN internal representations and produce natural language explanationsalong with concise explanation subgraphs. Our experiments across fourreal-world TAG datasets demonstrate that LOGIC achieves a favorable trade-offbetween fidelity and sparsity, while significantly improving human-centricmetrics such as insightfulness. LOGIC sets a new direction for LLM-basedexplainability in graph learning by aligning GNN internals with humanreasoning.</description>
      <author>example@mail.com (Peyman Baghershahi, Gregoire Fournier, Pranav Nyati, Sourav Medya)</author>
      <guid isPermaLink="false">2508.07117v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation</title>
      <link>http://arxiv.org/abs/2508.07106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为BrainATCL的无监督、非参数化自适应时间序列脑连接学习框架，用于捕捉功能性磁共振成像(fMRI)数据中的长程时间依赖性。该方法在功能链接预测和年龄估计任务中表现出优越的性能和强大的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;功能性磁共振成像(fMRI)是一种广泛用于研究人类大脑活动的成像技术。即使在个体休息时，大脑各区域的fMRI信号也会以高度结构化的方式短暂同步和去同步，形成功能连接动力学。这些动力学可能与行为和神经精神疾病相关，但传统的图神经网络(GNN)难以捕捉动态fMRI数据中的长程时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉动态fMRI数据中长程时间依赖性的方法，用于功能链接预测和年龄估计，从而更好地理解大脑功能连接动力学及其与行为和疾病的关系。&lt;h4&gt;方法&lt;/h4&gt;作者提出了BrainATCL框架，该方法根据新增边的速率动态调整每个快照的回溯窗口，使用GINE-Mamba2主干编码图序列来学习来自1000名参与者静息态fMRI数据的动态功能连接的空间-时间表示。此外，还融入了脑结构和功能信息提示的边属性（左右半球身份和子网络成员身份），以增强模型捕捉生物学上有意义的拓扑模式的能力。&lt;h4&gt;主要发现&lt;/h4&gt;BrainATCL在功能链接预测和年龄估计两个任务上均表现出优越的性能和强大的泛化能力，包括在跨会话预测场景中。该方法能够有效捕捉动态fMRI数据中的长程时间依赖性，并识别出生物学上有意义的脑区连接模式。&lt;h4&gt;结论&lt;/h4&gt;BrainATCL框架为研究大脑功能连接动力学提供了一种有效工具，能够捕捉传统方法难以处理的复杂时间依赖关系。该方法的优越性能和泛化能力表明其在脑功能研究和临床应用中的潜力，特别是在理解神经精神疾病和大脑老化过程方面。&lt;h4&gt;翻译&lt;/h4&gt;功能性磁共振成像(fMRI)是一种广泛用于研究人类大脑活动的成像技术。即使在个体休息时，大脑各区域的fMRI信号也会以高度结构化的方式短暂同步和去同步。这些功能连接动力学可能与行为和神经精神疾病有关。为了建模这些动力学，时间序列脑连接表示是必要的，因为它们反映了脑区之间不断变化的相互作用，并提供了关于瞬时神经状态和网络重构的见解。然而，传统的图神经网络(GNN)往往难以捕捉动态fMRI数据中的长程时间依赖性。为了应对这一挑战，我们提出了BrainATCL，这是一种用于自适应时间序列脑连接学习的无监督、非参数化框架，能够实现功能链接预测和年龄估计。我们的方法根据新增边的速率动态调整每个快照的回溯窗口。随后使用GINE-Mamba2主干对图序列进行编码，学习来自人类连接组计划中1000名参与者的静息态fMRI数据的动态功能连接的空间-时间表示。为了进一步提高空间建模，我们融入了脑结构和功能信息提示的边属性，即脑区的左右半球身份和子网络成员身份，使模型能够捕捉生物学上有意义的拓扑模式。我们在两个任务上评估了BrainATCL：功能链接预测和年龄估计。实验结果展示了优越的性能和强大的泛化能力，包括在跨会话预测场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widelyused to study human brain activity. fMRI signals in areas across the braintransiently synchronise and desynchronise their activity in a highly structuredmanner, even when an individual is at rest. These functional connectivitydynamics may be related to behaviour and neuropsychiatric disease. To modelthese dynamics, temporal brain connectivity representations are essential, asthey reflect evolving interactions between brain regions and provide insightinto transient neural states and network reconfigurations. However,conventional graph neural networks (GNNs) often struggle to capture long-rangetemporal dependencies in dynamic fMRI data. To address this challenge, wepropose BrainATCL, an unsupervised, nonparametric framework for adaptivetemporal brain connectivity learning, enabling functional link prediction andage estimation. Our method dynamically adjusts the lookback window for eachsnapshot based on the rate of newly added edges. Graph sequences aresubsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporalrepresentations of dynamic functional connectivity in resting-state fMRI dataof 1,000 participants from the Human Connectome Project. To further improvespatial modeling, we incorporate brain structure and function-informed edgeattributes, i.e., the left/right hemispheric identity and subnetwork membershipof brain regions, enabling the model to capture biologically meaningfultopological patterns. We evaluate our BrainATCL on two tasks: functional linkprediction and age estimation. The experimental results demonstrate superiorperformance and strong generalization, including in cross-session predictionscenarios.</description>
      <author>example@mail.com (Yiran Huang, Amirhossein Nouranizadeh, Christine Ahrends, Mengjia Xu)</author>
      <guid isPermaLink="false">2508.07106v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts</title>
      <link>http://arxiv.org/abs/2508.07094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ScamDetec框架，一个用于智能合约恶意软件检测的稳健、模块化且平台无关的解决方案，旨在应对日益复杂的字节码混淆技术和区块链环境异构性带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;智能合约通过可编程、无需信任的交易改变了去中心化金融，但其广泛采用和增长的经济重要性吸引了持续而复杂的威胁，如网络钓鱼活动和合约级别漏洞利用。传统基于交易的威胁检测方法会暴露敏感用户数据，引发隐私和安全问题。&lt;h4&gt;目的&lt;/h4&gt;开发ScamDetec框架，解决两个紧迫挑战：日益复杂的字节码混淆技术用于规避静态分析，以及区块链环境异构性需要平台无关的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出分阶段发展策略：第一阶段使用图神经网络分析控制流图来处理混淆的以太坊虚拟机字节码；第二阶段将检测能力泛化到新兴运行时如WASM。前期工作PhishingHook通过静态字节码和操作码分析实现了约90%的检测准确率。&lt;h4&gt;主要发现&lt;/h4&gt;静态字节码分析可作为主动缓解策略，在恶意合约执行有害操作前识别它们；PhishingHook框架实现了约90%的智能合约网络钓鱼活动检测准确率。&lt;h4&gt;结论&lt;/h4&gt;ScamDetec旨在为去中心化生态系统的未来提供主动、可扩展的安全保障。&lt;h4&gt;翻译&lt;/h4&gt;智能合约通过可编程、无需信任的交易改变了去中心化金融。然而，其广泛采用和日益增长的经济重要性吸引了持续而复杂的威胁，如网络钓鱼活动和合约级别漏洞利用。传统基于交易的威胁检测方法通常会暴露敏感的用户数据和交互，引发隐私和安全问题。作为回应，静态字节码分析已成为一种主动缓解策略，在恶意合约执行有害操作之前识别它们。基于这种方法，我们引入了PhishingHook，第一个基于机器学习的框架，通过静态字节码和操作码分析检测智能合约中的网络钓鱼活动，实现了约90%的检测准确率。然而，两个紧迫挑战仍然存在：(1)日益复杂的字节码混淆技术用于规避静态分析，(2)区块链环境的异构性需要平台无关的解决方案。本文提出了ScamDetec（智能合约通用恶意软件检测器）的愿景，这是一个稳健、模块化且平台无关的智能合约恶意软件检测框架。在未来2.5年内，ScamDetec将分两个阶段发展：首先，通过图神经网络分析控制流图来处理混淆的以太坊虚拟机字节码，利用GNN捕获操作码序列之外的复杂结构模式的能力；其次，将检测能力泛化到新兴运行时如WASM。ScamDetec旨在为去中心化生态系统的未来提供主动、可扩展的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/DSN-S65789.2025.00068&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Smart contracts have transformed decentralized finance by enablingprogrammable, trustless transactions. However, their widespread adoption andgrowing financial significance have attracted persistent and sophisticatedthreats, such as phishing campaigns and contract-level exploits. Traditionaltransaction-based threat detection methods often expose sensitive user data andinteractions, raising privacy and security concerns. In response, staticbytecode analysis has emerged as a proactive mitigation strategy, identifyingmalicious contracts before they execute harmful actions.Building on thisapproach, we introduced PhishingHook, the first machine-learning-basedframework for detecting phishing activities in smart contracts via staticbytecode and opcode analysis, achieving approximately 90% detection accuracy.Nevertheless, two pressing challenges remain: (1) the increasing use ofsophisticated bytecode obfuscation techniques designed to evade staticanalysis, and (2) the heterogeneity of blockchain environments requiringplatform-agnostic solutions.This paper presents a vision for ScamDetect (SmartContract Agnostic Malware Detector), a robust, modular, and platform-agnosticframework for smart contract malware detection. Over the next 2.5 years,ScamDetect will evolve in two stages: first, by tackling obfuscated EthereumVirtual Machine (EVM) bytecode through graph neural network (GNN) analysis ofcontrol flow graphs (CFGs), leveraging GNNs' ability to capture complexstructural patterns beyond opcode sequences; and second, by generalizingdetection capabilities to emerging runtimes such as WASM. ScamDetect aims toenable proactive, scalable security for the future of decentralized ecosystems.</description>
      <author>example@mail.com (Pasquale De Rosa, Pascal Felber, Valerio Schiavoni)</author>
      <guid isPermaLink="false">2508.07094v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.07028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Manuscript under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FOCUS-Med是一种创新的息肉分割方法，通过结合图卷积网络、自注意力机制和多尺度融合策略，有效解决了内镜图像中息肉分割的挑战，在五个关键指标上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;准确的内镜图像息肉分割对早期结直肠癌检测至关重要，但这一任务具有挑战性，因为息肉与周围黏膜对比度低、存在镜面高光、边界不清晰。&lt;h4&gt;目的&lt;/h4&gt;解决内镜图像中息肉分割的挑战，提高分割准确性，从而辅助早期结直肠癌检测。&lt;h4&gt;方法&lt;/h4&gt;提出FOCUS-Med方法，整合双图卷积网络（Dual-GCN）模块捕获空间和拓扑结构依赖，采用位置融合的独立自注意力机制加强全局上下文集成，引入可训练的加权快速归一化融合策略处理编码器-解码器层间的语义差距，并首次使用大型语言模型提供分割质量的定性评估。&lt;h4&gt;主要发现&lt;/h4&gt;FOCUS-Med能够更好地保留边界并描绘息肉的复杂形状，在五个关键指标上实现了最先进的性能，证明了其在AI辅助结肠镜检查中的有效性和临床潜力。&lt;h4&gt;结论&lt;/h4&gt;FOCUS-Med是一种有效的息肉分割方法，具有临床应用潜力，可以辅助结肠镜检查中的早期结直肠癌检测。&lt;h4&gt;翻译&lt;/h4&gt;准确的内镜图像息肉分割对早期结直肠癌检测至关重要。然而，由于与周围黏膜对比度低、镜面高光和边界不清晰，这一任务仍然具有挑战性。为应对这些挑战，我们提出了FOCUS-Med，即内镜医学成像中具有注意力上下文感知的息肉分割的空间和结构图融合。FOCUS-Med集成了双图卷积网络（Dual-GCN）模块来捕获上下文空间和拓扑结构依赖关系。这种基于图的表示使模型能够利用拓扑线索和空间连接来更好地区分息肉和背景组织，而这些信息在原始图像强度中通常被掩盖。它增强了模型保留边界和描绘息肉典型复杂形状的能力。此外，采用位置融合的独立自注意力机制来加强全局上下文集成。为了弥合编码器-解码器层之间的语义差距，我们纳入了可训练的加权快速归一化融合策略以实现有效的多尺度聚合。值得注意的是，我们首次引入使用大型语言模型（LLM）来提供分割质量的详细定性评估。在公共基准上的大量实验表明，FOCUS-Med在五个关键指标上实现了最先进的性能，强调了其在AI辅助结肠镜检查中的有效性和临床潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate endoscopic image segmentation on the polyps is critical for earlycolorectal cancer detection. However, this task remains challenging due to lowcontrast with surrounding mucosa, specular highlights, and indistinctboundaries. To address these challenges, we propose FOCUS-Med, which stands forFusion of spatial and structural graph with attentional context-aware polypsegmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual GraphConvolutional Network (Dual-GCN) module to capture contextual spatial andtopological structural dependencies. This graph-based representation enablesthe model to better distinguish polyps from background tissues by leveragingtopological cues and spatial connectivity, which are often obscured in rawimage intensities. It enhances the model's ability to preserve boundaries anddelineate complex shapes typical of polyps. In addition, a location-fusedstand-alone self-attention is employed to strengthen global contextintegration. To bridge the semantic gap between encoder-decoder layers, weincorporate a trainable weighted fast normalized fusion strategy for efficientmulti-scale aggregation. Notably, we are the first to introduce the use of aLarge Language Model (LLM) to provide detailed qualitative evaluations ofsegmentation quality. Extensive experiments on public benchmarks demonstratethat FOCUS-Med achieves state-of-the-art performance across five key metrics,underscoring its effectiveness and clinical potential for AI-assistedcolonoscopy.</description>
      <author>example@mail.com (Juntong Fan, Shuyi Fan, Debesh Jha, Changsheng Fang, Tieyong Zeng, Hengyong Yu, Dayang Wang)</author>
      <guid isPermaLink="false">2508.07028v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Blending Sequential Embeddings, Graphs, and Engineered Features: 4th Place Solution in RecSys Challenge 2025</title>
      <link>http://arxiv.org/abs/2508.06970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文描述了'ambitious'团队在RecSysChallenge 2025比赛中获得的第四名解决方案，该方案专注于通用行为建模，通过整合多种技术生成对多种下游任务有效的用户嵌入。&lt;h4&gt;背景&lt;/h4&gt;RecSysChallenge 2025由Synerise和ACM RecSys组织，专注于通用行为建模领域的研究与竞赛。&lt;h4&gt;目的&lt;/h4&gt;比赛目标是生成能够在六个不同下游任务中有效的用户嵌入，而本文旨在提出一种能够实现这一目标的解决方案。&lt;h4&gt;方法&lt;/h4&gt;解决方案整合了四个主要组件：序列编码器捕捉用户兴趣的时序演变；图神经网络增强模型泛化能力；深度交叉网络建模高阶特征交互；以及针对性能关键的特征工程方法。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及主要研究发现。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及结论。&lt;h4&gt;翻译&lt;/h4&gt;本文描述了'ambitious'团队在由Synerise和ACM RecSys组织的RecSysChallenge 2025比赛中获得的第四名解决方案，该比赛专注于通用行为建模。比赛目标是为六个不同的下游任务生成有效的用户嵌入。我们的解决方案整合了以下技术：(1)序列编码器，用于捕捉用户兴趣的时序演变；(2)图神经网络，用于增强泛化能力；(3)深度交叉网络，用于建模高阶特征交互；(4)性能关键特征工程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3758126.3758131&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper describes the 4th-place solution by team ambitious for the RecSysChallenge 2025, organized by Synerise and ACM RecSys, which focused onuniversal behavioral modeling. The challenge objective was to generate userembeddings effective across six diverse downstream tasks. Our solutionintegrates (1) a sequential encoder to capture the temporal evolution of userinterests, (2) a graph neural network to enhance generalization, (3) a deepcross network to model high-order feature interactions, and (4)performance-critical feature engineering.</description>
      <author>example@mail.com (Sergei Makeev, Alexandr Andreev, Vladimir Baikalov, Vladislav Tytskiy, Aleksei Krasilnikov, Kirill Khrylchenko)</author>
      <guid isPermaLink="false">2508.06970v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings</title>
      <link>http://arxiv.org/abs/2508.06734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures, 7 tables, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种语义增强框架，通过添加上下文特征来改进基于图的恶意软件分类器，解决了现有方法在处理未见过的恶意软件变种时的泛化能力不足问题。&lt;h4&gt;背景&lt;/h4&gt;基于图的恶意软件分类器在标准Android数据集上可达到94%以上准确率，但在评估同一家族中未见过的恶意软件变种时，准确率会下降高达45%，表明现有方法无法捕捉更深层次的语义模式。&lt;h4&gt;目的&lt;/h4&gt;开发一个强大的语义增强框架，提高恶意软件分类器在分布变化情况下的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出语义增强框架，通过添加函数级元数据和来自大型语言模型的代码嵌入等上下文特征来增强函数调用图，设计用于处理特征可用性不一致的现实约束，支持语义信号的灵活集成。&lt;h4&gt;主要发现&lt;/h4&gt;引入了MalNet-Tiny-Common和MalNet-Tiny-Distinct两个新基准测试数据集；实验表明该方法在分布变化情况下将分类性能提高高达8%；与基于适应的方法结合使用时能持续增强鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为在不断变化的威胁环境中构建弹性恶意软件检测系统提供了实用途径。&lt;h4&gt;翻译&lt;/h4&gt;基于图的恶意软件分类器可以在标准Android数据集上实现超过94%的准确率，然而我们发现，当在来自同一家族但之前未见过的恶意软件变种上进行评估时，它们的准确率会下降高达45%——这种情况通常应该表现出强大的泛化能力。这突显了现有方法的一个关键局限性：模型架构及其仅基于结构的表示往往无法捕捉更深层次的语义模式。在这项工作中，我们提出了一个强大的语义增强框架，通过添加上下文特征（包括函数级元数据和可用的来自大型语言模型的代码嵌入）来增强函数调用图。该框架设计用于在特征可用性不一致的现实约束下运行，并支持语义信号的灵活集成。为了评估在真实领域和时间变化下的泛化能力，我们引入了两个新的基准测试：MalNet-Tiny-Common和MalNet-Tiny-Distinct，它们使用恶意软件家族分区构建，以模拟跨家族泛化和不断变化的威胁行为。在多个图神经网络骨干网络上的实验表明，我们的方法在分布变化的情况下将分类性能提高了高达8%，并且与基于适应的方法集成时能持续增强鲁棒性。这些结果为在不断变化的威胁环境中构建弹性恶意软件检测系统提供了实用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based malware classifiers can achieve over 94% accuracy on standardAndroid datasets, yet we find they suffer accuracy drops of up to 45% whenevaluated on previously unseen malware variants from the same family - ascenario where strong generalization would typically be expected. Thishighlights a key limitation in existing approaches: both the modelarchitectures and their structure-only representations often fail to capturedeeper semantic patterns. In this work, we propose a robust semantic enrichmentframework that enhances function call graphs with contextual features,including function-level metadata and, when available, code embeddings derivedfrom large language models. The framework is designed to operate underreal-world constraints where feature availability is inconsistent, and supportsflexible integration of semantic signals. To evaluate generalization underrealistic domain and temporal shifts, we introduce two new benchmarks:MalNet-Tiny-Common and MalNet-Tiny-Distinct, constructed using malware familypartitioning to simulate cross-family generalization and evolving threatbehavior. Experiments across multiple graph neural network backbones show thatour method improves classification performance by up to 8% under distributionshift and consistently enhances robustness when integrated withadaptation-based methods. These results offer a practical path toward buildingresilient malware detection systems in evolving threat environments.</description>
      <author>example@mail.com (Ngoc N. Tran, Anwar Said, Waseem Abbas, Tyler Derr, Xenofon D. Koutsoukos)</author>
      <guid isPermaLink="false">2508.06734v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks</title>
      <link>http://arxiv.org/abs/2508.06663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究将Kolmogorov-Arnold Networks (KANs)集成到图神经网络中，提高了节点分类准确率，并通过知识融合方法显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在图结构数据上表现出色，但依赖图连接性限制了其可扩展性和效率。KANs是一种具有可学习单变量函数的架构，提供强大的非线性表达能力和高效的推理能力。&lt;h4&gt;目的&lt;/h4&gt;将KANs集成到GNN架构中以提高表达能力和推理效率，并探索通过知识融合进一步提升模型性能的可能性。&lt;h4&gt;方法&lt;/h4&gt;将KANs集成到三种流行的GNN架构（GAT、SGC和APPNP）中，分别得到KGAT、KSGC和KAPPNP三种新模型。采用多教师知识融合框架，将多个基于KAN的GNN知识蒸馏到一个与图无关的KAN学生模型中。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模型提高了节点分类准确率，知识融合方法显著提升了学生模型的性能。&lt;h4&gt;结论&lt;/h4&gt;KANs在增强GNN表达能力和实现高效、与图无关的推理方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在图结构数据上表现出色，但它们对图连接性的依赖往往限制了其可扩展性和效率。Kolmogorov-Arnold Networks（KANs）是一种最近的架构，具有可学习的单变量函数，提供强大的非线性表达能力和高效的推理能力。在这项工作中，我们将KANs集成到三种流行的GNN架构中——GAT、SGC和APPNP，从而得到三种新模型：KGAT、KSGC和KAPPNP。我们进一步采用了一个多教师知识融合框架，其中多个基于KAN的GNN知识被蒸馏到一个与图无关的KAN学生模型中。在基准数据集上的实验表明，提出的模型提高了节点分类准确率，知识融合方法显著提升了学生模型的性能。我们的研究结果强调了KANs在增强GNN表达能力和实现高效、与图无关的推理方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown strong performance ongraph-structured data, but their reliance on graph connectivity often limitsscalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recentarchitecture with learnable univariate functions, offer strong nonlinearexpressiveness and efficient inference. In this work, we integrate KANs intothree popular GNN architectures-GAT, SGC, and APPNP-resulting in three newmodels: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledgeamalgamation framework, where knowledge from multiple KAN-based GNNs isdistilled into a graph-independent KAN student model. Experiments on benchmarkdatasets show that the proposed models improve node classification accuracy,and the knowledge amalgamation approach significantly boosts student modelperformance. Our findings highlight the potential of KANs for enhancing GNNexpressiveness and for enabling efficient, graph-free inference.</description>
      <author>example@mail.com (Yuan-Hung Chao, Chia-Hsun Lu, Chih-Ya Shen)</author>
      <guid isPermaLink="false">2508.06663v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Hypergraph Neural Network with State Space Models for Node Classification</title>
      <link>http://arxiv.org/abs/2508.06587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的超图神经网络与状态空间模型相结合的模型（HGMN），有效解决了传统图神经网络忽略基于角色的特征的问题，通过超图构建技术和可学习机制整合了基于角色和邻接的表示，在节点分类任务上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;图神经网络近年来在图结构数据节点分类任务中备受关注，但传统GNN主要关注节点间邻接关系，忽略了基于角色的关键特征；现有捕获基于角色特征的方法多为无监督，在下游任务中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效整合基于角色的表示到GNN和状态空间模型中的方法，解决传统GNN的局限性，提升节点分类性能。&lt;h4&gt;方法&lt;/h4&gt;提出HGMN模型，利用超图构建技术建模高阶关系，通过可学习的mamba transformer机制结合基于角色和邻接的表示，采用基于节点度和邻域级别的超图构建方法加强相似角色节点间连接，包含超图卷积层捕获复杂依赖，并集成残差网络缓解过平滑问题。&lt;h4&gt;主要发现&lt;/h4&gt;在一个新数据集和四个基准数据集上的实验证明HGMN优于最先进GNN方法，在节点分类任务上实现了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;HGMN能有效嵌入基于角色的特征和邻接信息，提供丰富的节点表示，成为各种基于图的学习应用的通用且强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;近年来，图神经网络在图结构数据的节点分类任务中获得了显著关注。然而，传统GNN主要关注节点之间的邻接关系，往往忽略了对于学习更丰富的节点表示至关重要的基于角色的丰富特征。现有的捕获基于角色的特征的方法大多是无监督的，并且在下游任务中无法实现最佳性能。为解决这些限制，我们提出了一种新颖的超图神经网络与状态空间模型相结合的模型，有效地将基于角色的表示整合到GNN和状态空间模型中。该模型利用超图构建技术来建模高阶关系，并通过可学习的mamba transformer机制结合基于角色和基于邻接的表示。通过利用两种不同的基于节点度和邻域级别的超图构建方法，它加强了具有相似角色的节点之间的连接，增强了模型的表示能力。此外，超图卷积层的包含使模型能够捕获超图结构内的复杂依赖关系。为了缓解深度GNN中固有的过平滑问题，我们集成了残差网络，确保了更好的层间稳定性和特征传播。在一个新引入的数据集和四个基准数据集上进行的广泛实验证明了该模型的优越性。与最先进的GNN方法相比，该模型在节点分类任务上取得了显著的性能提升。这些结果突显了该模型通过有效嵌入基于角色的特征和邻接信息来提供丰富节点表示的能力，使其成为各种基于图的学习应用的通用且强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, graph neural networks (GNNs) have gained significantattention for node classification tasks on graph-structured data. However,traditional GNNs primarily focus on adjacency relationships between nodes,often overlooking the rich role-based characteristics that are crucial forlearning more expressive node representations. Existing methods for capturingrole-based features are largely unsupervised and fail to achieve optimalperformance in downstream tasks. To address these limitations, we propose anovel hypergraph neural network with state space model (HGMN) that effectivelyintegrates role-aware representations into GNNs and the state space model. HGMNutilizes hypergraph construction techniques to model higher-order relationshipsand combines role-based and adjacency-based representations through a learnablemamba transformer mechanism. By leveraging two distinct hypergraph constructionmethods-based on node degree and neighborhood levels, it strengthens theconnections among nodes with similar roles, enhancing the model'srepresentational power. Additionally, the inclusion of hypergraph convolutionlayers enables the model to capture complex dependencies within hypergraphstructures. To mitigate the over-smoothing problem inherent in deep GNNs, weincorporate a residual network, ensuring improved stability and better featurepropagation across layers. Extensive experiments conducted on one newlyintroduced dataset and four benchmark datasets demonstrate the superiority ofHGMN. The model achieves significant performance improvements on nodeclassification tasks compared to state-of-the-art GNN methods. These resultshighlight HGMN's ability to provide enriched node representations byeffectively embedding role-based features alongside adjacency information,making it a versatile and powerful tool for a variety of graph-based learningapplications.</description>
      <author>example@mail.com (A. Quadir, M. Tanveer)</author>
      <guid isPermaLink="false">2508.06587v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion</title>
      <link>http://arxiv.org/abs/2508.08216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'Individual Tangent Space Alignment (ITSA)'的新型预对齐策略，用于增强跨主体泛化能力，以支持基于音乐的个性化运动康复干预。&lt;h4&gt;背景&lt;/h4&gt;个性化音乐干预可以通过动态调整听觉刺激来支持运动康复，提供外部节拍线索，调节情感状态，稳定步态模式。通用脑机接口(BCIs)有望使这些干预措施能够适应不同个体。然而，脑电图(EEG)信号的主体间变异性，加上运动引起的伪影和运动计划差异，阻碍了BCIs的泛化能力，导致校准过程冗长。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的预对齐策略(ITSA)，以提高跨主体泛化能力，解决BCIs在个体间差异导致的泛化困难和校准时间长的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了'Individual Tangent Space Alignment (ITSA)'，一种新颖的预对齐策略，包含主体特定的重新中心化、分布匹配和监督旋转对齐。使用混合架构，融合了正则化公共空间模式(RCSP)和黎曼几何，采用并行和顺序配置。使用留一主体交叉验证评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;ITSA在主体和条件下显示出显著的性能改进。并行融合方法相比顺序融合方法显示出最大的改进。系统在不同数据条件和电极配置下保持了稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;ITSA策略有效地增强了跨主体泛化能力，特别是在并行融合配置下，能够适应不同的数据条件和电极配置，为基于音乐的个性化运动康复干预提供了更好的支持。&lt;h4&gt;翻译&lt;/h4&gt;个性化音乐干预提供了一种强大的支持运动康复的手段，通过动态调整听觉刺激来提供外部节拍线索，调节情感状态，并稳定步态模式。因此，通用脑机接口(BCIs)有望使这些干预措施能够适应不同个体。然而，脑电图信号的主体间变异性，再加上运动引起的伪影和运动计划差异，阻碍了BCIs的泛化能力，并导致冗长的校准过程。我们提出了'Individual Tangent Space Alignment (ITSA)'，一种新颖的预对齐策略，包含主体特定的重新中心化、分布匹配和监督旋转对齐，以增强跨主体泛化。我们的混合架构并行和顺序地融合了正则化公共空间模式(RCSP)和黎曼几何，提高了类可分性，同时保持了协方差矩阵的几何结构，以实现稳健的统计计算。使用留一主体交叉验证，'ITSA'在主体和条件下显示出显著的性能改进。并行融合方法相比其顺序对应方法显示出最大的改进，并且在变化的数据条件和电极配置下保持了稳健的性能。代码将在发表时公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalised music-based interventions offer a powerful means of supportingmotor rehabilitation by dynamically tailoring auditory stimuli to provideexternal timekeeping cues, modulate affective states, and stabilise gaitpatterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise foradapting these interventions across individuals. However, inter-subjectvariability in EEG signals, further compounded by movement-induced artefactsand motor planning differences, hinders the generalisability of BCIs andresults in lengthy calibration processes. We propose Individual Tangent SpaceAlignment (ITSA), a novel pre-alignment strategy incorporating subject-specificrecentering, distribution matching, and supervised rotational alignment toenhance cross-subject generalisation. Our hybrid architecture fuses RegularisedCommon Spatial Patterns (RCSP) with Riemannian geometry in parallel andsequential configurations, improving class separability while maintaining thegeometric structure of covariance matrices for robust statistical computation.Using leave-one-subject-out cross-validation, `ITSA' demonstrates significantperformance improvements across subjects and conditions. The parallel fusionapproach shows the greatest enhancement over its sequential counterpart, withrobust performance maintained across varying data conditions and electrodeconfigurations. The code will be made publicly available at the time ofpublication.</description>
      <author>example@mail.com (Nicole Lai-Tan, Xiao Gu, Marios G. Philiastides, Fani Deligianni)</author>
      <guid isPermaLink="false">2508.08216v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks</title>
      <link>http://arxiv.org/abs/2508.08125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published In Proceedings of the 2024 Joint International Conference  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING  2024). Official version: https://aclanthology.org/2024.lrec-main.374/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个新的捷克语数据集，用于基于方面的情感分析(ABSA)，包含3100条手动标注的餐厅评论，支持更复杂的任务，并遵循SemEval-2016数据集格式。&lt;h4&gt;背景&lt;/h4&gt;之前有一个捷克语数据集，但只包含基本ABSA任务的独立标签，如方面术语提取或方面极性检测，缺乏支持更复杂任务的统一标注格式。&lt;h4&gt;目的&lt;/h4&gt;创建一个新的捷克语数据集，支持更复杂的ABSA任务，如目标-方面类别检测，并采用统一的标注格式将情感元素链接在一起。&lt;h4&gt;方法&lt;/h4&gt;数据集构建于之前的捷克语数据集，包含3100条手动标注的餐厅评论，两名训练有素的标注员完成标注，标注者间一致性约为90%，还提供2400万条未标注评论适用于无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;使用各种基于Transformer的模型实现了强大的单语基线结果，并提供了有价值的错误分析。&lt;h4&gt;结论&lt;/h4&gt;代码和数据集可供非商业研究目的免费使用，促进了跨语言比较。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们介绍了一个用于基于方面的情感分析(ABSA)的新型捷克语数据集，该数据集包含3100条来自餐厅领域的手动标注评论。该数据集建立在较早的捷克语数据集之上，该数据集仅包含基本ABSA任务（如方面术语提取或方面极性检测）的独立标签。与前身不同，我们的新数据集专门设计用于更复杂的任务，例如目标-方面类别检测。这些高级任务需要统一的标注格式，将情感元素（标签）无缝连接在一起。我们的数据集遵循著名的SemEval-2016数据集的格式。这一设计选择使得在跨语言场景中能够轻松应用和评估，最终促进与其他语言中对应数据集的跨语言比较。标注过程涉及两名训练有素的标注员，获得了约90%的惊人标注者间一致性。此外，我们还提供了2400万条未标注的评论，适用于无监督学习。我们展示了使用各种基于Transformer的模型实现的强大单语基线结果，并提供有价值的错误分析作为补充贡献。我们的代码和数据集可供非商业研究目的免费使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce a novel Czech dataset for aspect-based sentimentanalysis (ABSA), which consists of 3.1K manually annotated reviews from therestaurant domain. The dataset is built upon the older Czech dataset, whichcontained only separate labels for the basic ABSA tasks such as aspect termextraction or aspect polarity detection. Unlike its predecessor, our newdataset is specifically designed for more complex tasks, e.g.target-aspect-category detection. These advanced tasks require a unifiedannotation format, seamlessly linking sentiment elements (labels) together. Ourdataset follows the format of the well-known SemEval-2016 datasets. This designchoice allows effortless application and evaluation in cross-lingual scenarios,ultimately fostering cross-language comparisons with equivalent counterpartdatasets in other languages. The annotation process engaged two trainedannotators, yielding an impressive inter-annotator agreement rate ofapproximately 90%. Additionally, we provide 24M reviews without annotationssuitable for unsupervised learning. We present robust monolingual baselineresults achieved with various Transformer-based models and insightful erroranalysis to supplement our contributions. Our code and dataset are freelyavailable for non-commercial research purposes.</description>
      <author>example@mail.com (Jakub Šmíd, Pavel Přibáň, Ondřej Pražák, Pavel Král)</author>
      <guid isPermaLink="false">2508.08125v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations</title>
      <link>http://arxiv.org/abs/2508.08061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于迁移学习的预测性过程监控技术，使缺乏足够事件数据的组织也能实施PPM进行有效决策支持。该技术在两个实际用例中进行了验证，实验表明业务流程知识可以在组织内部和组织间转移，实现目标环境中的有效预测监控。&lt;h4&gt;背景&lt;/h4&gt;事件日志反映了组织信息系统中映射的业务流程行为。预测性过程监控(PPM)通过创建过程相关预测将数据转化为价值，为过程运行时的主动干预提供洞察。然而，现有PPM技术需要大量事件数据或其他相关资源，这些资源可能不易获得，限制了部分组织应用PPM的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于迁移学习的PPM技术，使没有合适事件数据或其他相关资源的组织也能实施PPM，从而实现有效的决策支持。&lt;h4&gt;方法&lt;/h4&gt;该技术在两个实际用例中得到实例化，并基于这些用例，使用IT服务管理流程的事件日志进行了数值实验，包括组织内部和组织间的设置。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，一个业务流程的知识可以转移到同一组织或不同组织中的相似业务流程，以实现目标环境中的有效PPM。通过所提出的技术，组织可以在组织内部和组织间环境中从迁移学习中受益，其中预训练模型等资源可以在组织内部和组织边界之间转移。&lt;h4&gt;结论&lt;/h4&gt;基于迁移学习的PPM技术使缺乏足够数据资源的组织也能实现有效的过程监控和决策支持，知识可以在相似业务流程和组织间转移应用，扩大了PPM的适用范围。&lt;h4&gt;翻译&lt;/h4&gt;事件日志反映了在组织信息系统中映射的业务流程行为。预测性过程监控(PPM)通过创建与过程相关的预测，将数据转化为价值，为过程运行时的主动干预提供洞察。现有的PPM技术需要足够的事件数据或其他相关资源，这些资源可能不容易获得，阻止一些组织利用PPM。本文提出的基于迁移学习的PPM技术使没有合适事件数据或其他相关资源的组织能够实施PPM以实现有效的决策支持。该技术在两个实际用例中得到实例化，基于这些用例，使用IT服务管理流程的事件日志在组织内部和组织间设置中进行了数值实验。实验结果表明，一个业务流程的知识可以转移到同一组织或不同组织中的相似业务流程，以实现目标环境中的有效PPM。通过所提出的技术，组织可以在组织内部和组织间环境中从迁移学习中受益，其中预训练模型等资源可以在组织内部和组织边界之间转移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event logs reflect the behavior of business processes that are mapped inorganizational information systems. Predictive process monitoring (PPM)transforms these data into value by creating process-related predictions thatprovide the insights required for proactive interventions at process runtime.Existing PPM techniques require sufficient amounts of event data or otherrelevant resources that might not be readily available, preventing someorganizations from utilizing PPM. The transfer learning-based PPM techniquepresented in this paper allows organizations without suitable event data orother relevant resources to implement PPM for effective decision support. Thetechnique is instantiated in two real-life use cases, based on which numericalexperiments are performed using event logs for IT service management processesin an intra- and inter-organizational setting. The results of the experimentssuggest that knowledge of one business process can be transferred to a similarbusiness process in the same or a different organization to enable effectivePPM in the target context. With the proposed technique, organizations canbenefit from transfer learning in an intra- and inter-organizational setting,where resources like pre-trained models are transferred within and acrossorganizational boundaries.</description>
      <author>example@mail.com (Sven Weinzierl, Sandra Zilker, Annina Liessmann, Martin Käppel, Weixin Wang, Martin Matzner)</author>
      <guid isPermaLink="false">2508.08061v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.07536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种物理信息多模态卷积神经网络，用于轴承故障分类，整合了振动和电机电流信号，并采用后期融合架构。模型包含基于物理特性的损失函数，以惩罚不合理的预测。实验表明该方法优于传统基线，并评估了三种迁移学习策略以提高在未见工况下的性能。&lt;h4&gt;背景&lt;/h4&gt;准确且可解释的轴承故障分类对确保旋转机械的可靠性至关重要，特别是在可变工况下，域偏移会显著降低模型性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种物理信息多模态神经网络，提高轴承故障分类的准确性、鲁棒性和可解释性，特别是在可变工况下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种物理信息多模态卷积神经网络，采用后期融合架构，整合振动和电机电流信号，并配有专门的基于物理的特征提取分支。模型包含基于轴承几何形状和轴速推导的特征故障频率（外圈通过频率和内圈通过频率）的物理信息损失函数。评估了三种迁移学习策略（目标特定微调、逐层适应策略和混合特征重用）以解决在未见工况下的性能下降问题。&lt;h4&gt;主要发现&lt;/h4&gt;物理信息方法持续优于非物理信息基线，实现更高准确率、减少错误分类并提高鲁棒性；逐层适应策略提供最佳泛化能力；物理信息建模与逐层适应策略结合可进一步提升性能；在KAIST数据集上验证了框架的跨数据集适用性，准确率高达98%；统计假设检验验证了分类性能的显著改善。&lt;h4&gt;结论&lt;/h4&gt;将领域知识与数据驱动学习相结合的框架能够实现鲁棒、可解释和可泛化的故障诊断，适用于实际工业应用，在轴承故障分类任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;准确且可解释的轴承故障分类对于确保旋转机械的可靠性至关重要，特别是在可变工况下，域偏移会显著降低模型性能。本研究提出了一种物理信息多模态卷积神经网络，采用后期融合架构，整合了振动和电机电流信号，并配有专门的基于物理的特征提取分支。该模型包含一种新颖的物理信息损失函数，根据轴承几何形状和轴速推导出的特征轴承故障频率对物理上不可能的预测进行惩罚。在帕德博恩大学数据集上的综合实验表明，所提出的物理信息方法持续优于非物理信息基线，在多个数据分割中实现了更高的准确性、减少了错误分类并提高了鲁棒性。为解决在未见工况下的性能下降问题，评估了三种迁移学习策略。结果显示，逐层适应策略提供了最佳泛化能力，与物理信息建模结合时还有额外的性能提升。在KAIST轴承数据集上的验证确认了该框架的跨数据集适用性，准确率高达98%。统计假设检验进一步验证了分类性能的显著改善。所提出的框架展示了将领域知识与数据驱动学习相结合以实现鲁棒、可解释和可泛化的故障诊断的潜力，适用于实际工业应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and interpretable bearing fault classification is critical forensuring the reliability of rotating machinery, particularly under variableoperating conditions where domain shifts can significantly degrade modelperformance. This study proposes a physics-informed multimodal convolutionalneural network (CNN) with a late fusion architecture, integrating vibration andmotor current signals alongside a dedicated physics-based feature extractionbranch. The model incorporates a novel physics-informed loss function thatpenalizes physically implausible predictions based on characteristic bearingfault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass FrequencyInner (BPFI) - derived from bearing geometry and shaft speed. Comprehensiveexperiments on the Paderborn University dataset demonstrate that the proposedphysics-informed approach consistently outperforms a non-physics-informedbaseline, achieving higher accuracy, reduced false classifications, andimproved robustness across multiple data splits. To address performancedegradation under unseen operating conditions, three transfer learning (TL)strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LASyields the best generalization, with additional performance gains when combinedwith physics-informed modeling. Validation on the KAIST bearing datasetconfirms the framework's cross-dataset applicability, achieving up to 98percent accuracy. Statistical hypothesis testing further verifies significantimprovements (p &lt; 0.01) in classification performance. The proposed frameworkdemonstrates the potential of integrating domain knowledge with data-drivenlearning to achieve robust, interpretable, and generalizable fault diagnosisfor real-world industrial applications.</description>
      <author>example@mail.com (Tasfiq E. Alam, Md Manjurul Ahsan, Shivakumar Raman)</author>
      <guid isPermaLink="false">2508.07536v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation</title>
      <link>http://arxiv.org/abs/2508.07295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个名为CCFQA的新基准，用于评估多模态大型语言模型在跨语言和跨模态事实准确性方面的能力，特别是在语音处理方面。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在多语言世界中日益普及，确保无幻觉的事实准确性变得至关重要。然而，现有的多模态大型语言模型评估基准主要关注文本或视觉模态，且以英语为主，在处理多语言输入特别是语音时存在评估差距。&lt;h4&gt;目的&lt;/h4&gt;填补多语言语音输入评估的空白，提出一个能够系统评估多模态大型语言模型跨语言和跨模态事实准确性能力的新型基准。&lt;h4&gt;方法&lt;/h4&gt;开发了CCFQA基准，包含8种语言的并行语音文本事实问题；提出少样本迁移学习策略，将LLMs在英语问答能力迁移到多语言口语问答任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明当前MLLMs在CCFQA基准上仍面临重大挑战；使用仅5次训练的少样本迁移学习策略，可以与GPT-4o-mini-Audio实现具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;发布CCFQA作为基础研究资源，促进具有更强大可靠语音理解能力的MLLMs发展，相关代码和数据集已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型在多语言世界中日益普及，确保无幻觉的事实准确性变得至关重要。然而，现有的用于评估多模态大型语言模型可靠性的基准主要关注文本或视觉模态，并以英语为主，这在处理多语言输入时（特别是语音）造成了评估差距。为了弥补这一差距，我们提出了一个新颖的跨语言和跨模态事实准确性基准CCFQA。具体来说，CCFQA基准包含8种语言的并行语音文本事实问题，旨在系统评估MLLMs的跨语言和跨模态事实准确性能力。我们的实验结果表明，当前的MLLMs在CCFQA基准上仍面临重大挑战。此外，我们提出了一种少样本迁移学习策略，有效地将LLMs在英语中的问答能力迁移到多语言口语问答任务，仅使用5次训练就实现了与GPT-4o-mini-Audio相竞争的性能。我们发布CCFQA作为基础研究资源，以促进具有更强大可靠语音理解能力的MLLMs的发展。我们的代码和数据集可在https://github.com/yxduir/ccfqa获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Large Language Models (LLMs) are increasingly popularized in themultilingual world, ensuring hallucination-free factuality becomes markedlycrucial. However, existing benchmarks for evaluating the reliability ofMultimodal Large Language Models (MLLMs) predominantly focus on textual orvisual modalities with a primary emphasis on English, which creates a gap inevaluation when processing multilingual input, especially in speech. To bridgethis gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQAbenchmark contains parallel speech-text factual questions across 8 languages,designed to systematically evaluate MLLMs' cross-lingual and cross-modalfactuality capabilities. Our experimental results demonstrate that currentMLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, wepropose a few-shot transfer learning strategy that effectively transfers theQuestion Answering (QA) capabilities of LLMs in English to multilingual SpokenQuestion Answering (SQA) tasks, achieving competitive performance withGPT-4o-mini-Audio using just 5-shot training. We release CCFQA as afoundational research resource to promote the development of MLLMs with morerobust and reliable speech understanding capabilities. Our code and dataset areavailable at https://github.com/yxduir/ccfqa.</description>
      <author>example@mail.com (Yexing Du, Kaiyuan Liu, Youcheng Pan, Zheng Chu, Bo Yang, Xiaocheng Feng, Yang Xiang, Ming Liu)</author>
      <guid isPermaLink="false">2508.07295v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Reconstruction of Solar EUV Irradiance Using CaII K Images and SOHO/SEM Data with Bayesian Deep Learning and Uncertainty Quantification</title>
      <link>http://arxiv.org/abs/2508.07065v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SEMNet的贝叶斯深度学习模型，用于重建太阳极紫外(EUV)辐照度的长期数据，填补了1995年之前EUV连续测量数据的空白。通过使用CaII K图像作为代理数据，研究人员成功重建了1950-1960年和1998-2014年期间的EUV通量数据，为理解太阳对地球气候的长期影响提供了重要依据。&lt;h4&gt;背景&lt;/h4&gt;太阳极紫外(EUV)辐照度对地球电离层、热层和中层的加热起着关键作用，影响不同时间尺度的大气动力学。虽然已有大量研究关注太阳瞬态事件导致的短期EUV变化，但对多个太阳周期内EUV通量长期演变的研究较少。此外，连续的EUV通量测量数据自1995年起才有，早期数据存在显著空白。&lt;h4&gt;目的&lt;/h4&gt;填补太阳EUV辐照度长期数据的空白，特别是1995年之前的数据，以便更好地理解太阳对地球气候在较长时间尺度上的影响。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为SEMNet的贝叶斯深度学习模型。首先使用精密太阳光度望远镜的CaII K图像构建1998年至2014年期间的SOHO/SEM EUV通量测量来验证该方法。然后通过迁移学习扩展SEMNet，使用科代卡纳尔太阳天文台的CaII K图像重建1950年至1960年期间的太阳EUV辐照度。&lt;h4&gt;主要发现&lt;/h4&gt;SEMNet模型能够提供可靠的EUV通量预测和不确定性边界，证明了CaII K图像可以作为长期EUV通量的稳健代理数据。&lt;h4&gt;结论&lt;/h4&gt;通过使用SEMNet模型和CaII K图像作为代理数据，成功重建了长期太阳EUV辐照度数据，为研究太阳对地球气候的长期影响提供了重要数据支持。&lt;h4&gt;翻译&lt;/h4&gt;太阳极紫外(EUV)辐照度在加热地球电离层、热层和中层方面起着关键作用，影响不同时间尺度的大气动力学。虽然已经投入大量精力研究太阳瞬态事件导致的短期EUV变化，但很少有工作探索多个太阳周期内EUV通量的长期演变。连续的EUV通量测量数据自1995年起才有，早期数据存在显著空白。在本研究中，我们提出了一种名为SEMNet的贝叶斯深度学习模型来填补这些空白。我们通过将SEMNet应用于使用精密太阳光度望远镜的CaII K图像构建1998年至2014年期间的SOHO/SEM EUV通量测量来验证我们的方法。然后我们通过迁移学习扩展SEMNet，使用科代卡纳尔太阳天文台的CaII K图像重建1950年至1960年期间的太阳EUV辐照度。实验结果表明，SEMNet提供了可靠的预测和不确定性边界，证明了CaII K图像作为长期EUV通量的稳健代理的可行性。这些发现有助于更好地理解太阳对地球气候在较长时间尺度上的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solar extreme ultraviolet (EUV) irradiance plays a crucial role in heatingthe Earth's ionosphere, thermosphere, and mesosphere, affecting atmosphericdynamics over varying time scales. Although significant effort has been spentstudying short-term EUV variations from solar transient events, there is littlework to explore the long-term evolution of the EUV flux over multiple solarcycles. Continuous EUV flux measurements have only been available since 1995,leaving significant gaps in earlier data. In this study, we propose a Bayesiandeep learning model, named SEMNet, to fill the gaps. We validate our approachby applying SEMNet to construct SOHO/SEM EUV flux measurements in the periodbetween 1998 and 2014 using CaII K images from the Precision Solar PhotometricTelescope. We then extend SEMNet through transfer learning to reconstruct solarEUV irradiance in the period between 1950 and 1960 using CaII K images from theKodaikanal Solar Observatory. Experimental results show that SEMNet providesreliable predictions along with uncertainty bounds, demonstrating thefeasibility of CaII K images as a robust proxy for long-term EUV fluxes. Thesefindings contribute to a better understanding of solar influences on Earth'sclimate over extended periods.</description>
      <author>example@mail.com (Haodi Jiang, Qin Li, Jason T. L. Wang, Haimin Wang, Serena Criscuoli)</author>
      <guid isPermaLink="false">2508.07065v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2508.06784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了模式感知非线性Tucker自编码器(MA-NTAE)，用于解决高阶张量在自监督学习中的挑战，有效克服了传统方法和现有张量网络的局限性。&lt;h4&gt;背景&lt;/h4&gt;高维数据，特别是高阶张量，在自监督学习中构成重大挑战。基于MLP的自编码器依赖扁平化操作，加剧维度灾难，导致模型规模过大、计算开销高且难以优化深层特征捕获。现有张量网络虽通过张量分解减轻计算负担，但在学习非线性关系方面能力有限。&lt;h4&gt;目的&lt;/h4&gt;克服现有自编码器和张量网络方法的局限性，开发一种能够有效处理高阶张量的自监督学习方法。&lt;h4&gt;方法&lt;/h4&gt;引入模式感知非线性Tucker自编码器(MA-NTAE)，将经典Tucker分解推广到非线性框架，采用'选择-展开'策略，通过递归的展开-编码-折叠操作实现高阶张量的灵活按模式编码，有效整合张量结构先验。&lt;h4&gt;主要发现&lt;/h4&gt;MA-NTAE的计算复杂度随张量阶数呈线性增长，随模式维度呈比例增长。实验表明，MA-NTAE在压缩和聚类任务上优于标准自编码器和当前张量网络，且对更高阶、更高维的张量，这种优势更加明显。&lt;h4&gt;结论&lt;/h4&gt;MA-NTAE是处理高阶张量数据的有效方法，特别是在自监督学习任务中，能够平衡计算效率与模型性能。&lt;h4&gt;翻译&lt;/h4&gt;高维数据，特别是高阶张量形式，在自监督学习中构成了重大挑战。虽然基于MLP的自编码器(AE)被广泛使用，但它们对扁平化操作的依赖加剧了维度灾难，导致模型规模过大、计算开销高，且难以优化深层特征捕获。虽然现有的张量网络通过张量分解技术减轻了计算负担，但大多数在学习非线性关系方面能力有限。为了克服这些局限性，我们引入了模式感知非线性Tucker自编码器(MA-NTAE)。MA-NTAE将经典Tucker分解推广到非线性框架，并采用'选择-展开'策略，通过递归的展开-编码-折叠操作实现高阶张量的灵活按模式编码，有效整合了张量结构先验。值得注意的是，MA-NTAE的计算复杂度随张量阶数呈线性增长，随模式维度呈比例增长。大量实验表明，MA-NTAE在压缩和聚类任务上优于标准自编码器和当前张量网络，对于更高阶、更高维的张量，这种优势变得更加明显。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional data, particularly in the form of high-order tensors,presents a major challenge in self-supervised learning. While MLP-basedautoencoders (AE) are commonly employed, their dependence on flatteningoperations exacerbates the curse of dimensionality, leading to excessivelylarge model sizes, high computational overhead, and challenging optimizationfor deep structural feature capture. Although existing tensor networksalleviate computational burdens through tensor decomposition techniques, mostexhibit limited capability in learning non-linear relationships. To overcomethese limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linearframework and employs a Pick-and-Unfold strategy, facilitating flexibleper-mode encoding of high-order tensors via recursive unfold-encode-foldoperations, effectively integrating tensor structural priors. Notably, MA-NTAEexhibits linear growth in computational complexity with tensor order andproportional growth with mode dimensions. Extensive experiments demonstrateMA-NTAE's performance advantages over standard AE and current tensor networksin compression and clustering tasks, which become increasingly pronounced forhigher-order, higher-dimensional tensors.</description>
      <author>example@mail.com (Junjing Zheng, Chengliang Song, Weidong Jiang, Xinyu Zhang)</author>
      <guid isPermaLink="false">2508.06784v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised learning for inverse problems in computed tomography</title>
      <link>http://arxiv.org/abs/2508.05321v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种无监督深度学习方法用于CT图像重建，通过在深度学习框架中整合前向和后向投影层，实现了在不依赖真实图像的情况下从投影数据重建图像。该方法在2DeteCT数据集上表现出优于传统FBP和ML重建技术的性能，同时显著减少了重建时间。&lt;h4&gt;背景&lt;/h4&gt;研究利用深度神经网络训练与传统迭代重建方法之间的内在相似性，开发了一种创新的CT图像重建方法。&lt;h4&gt;目的&lt;/h4&gt;实现在不依赖真实图像的情况下从投影数据重建图像，并开发一种适用于实时医学成像的替代方法。&lt;h4&gt;方法&lt;/h4&gt;在深度学习框架中整合前向和后向投影层，并在二维2DeteCT数据集上评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的滤波反投影(FBP)和最大似然(ML)重建技术相比，该方法在均方误差(MSE)和结构相似性指数(SSIM)方面表现更优，同时显著减少了重建时间。&lt;h4&gt;结论&lt;/h4&gt;该方法是一种有前景的实时医学成像替代方案，未来工作将重点扩展到三维重建并提高投影几何的适应性。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种用于计算机断层扫描(CT)图像重建的无监督深度学习方法，利用深度神经网络训练与传统迭代重建方法之间的内在相似性。通过在深度学习框架中整合前向和后向投影层，我们证明了在不依赖真实图像的情况下从投影数据重建图像的可行性。我们的方法在二维2DeteCT数据集上进行了评估，与传统的滤波反投影(FBP)和最大似然(ML)重建技术相比，在均方误差(MSE)和结构相似性指数(SSIM)方面表现出优越的性能。此外，我们的方法显著减少了重建时间，使其成为实时医学成像应用的一种有前途的替代方案。未来的工作将重点扩展该方法到三维重建并提高投影几何的适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents an unsupervised deep learning approach for computedtomography (CT) image reconstruction, leveraging the inherent similaritiesbetween deep neural network training and conventional iterative reconstructionmethods. By incorporating forward and backward projection layers within thedeep learning framework, we demonstrate the feasibility of reconstructingimages from projection data without relying on ground-truth images. Our methodis evaluated on the two-dimensional 2DeteCT dataset, showcasing superiorperformance in terms of mean squared error (MSE) and structural similarityindex (SSIM) compared to traditional filtered backprojection (FBP) and maximumlikelihood (ML) reconstruction techniques. Additionally, our approachsignificantly reduces reconstruction time, making it a promising alternativefor real-time medical imaging applications. Future work will focus on extendingthis methodology to three-dimensional reconstructions and enhancing theadaptability of the projection geometry.</description>
      <author>example@mail.com (Laura Hellwege, Johann Christopher Engster, Moritz Schaar, Thorsten M. Buzug, Maik Stille)</author>
      <guid isPermaLink="false">2508.05321v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images</title>
      <link>http://arxiv.org/abs/2508.03996v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了大规模预训练数据集对仅使用2D图像进行营养成分估算的深度学习模型性能的影响，发现数据集的特性（规模、领域相关性和质量）对迁移学习效果至关重要。&lt;h4&gt;背景&lt;/h4&gt;从食物图像中估算营养成分对健康和饮食监测具有重要意义，但仅依靠2D图像具有挑战性，因为食物呈现方式、光照条件的变化以及缺乏深度信息导致难以推断体积和质量。此外，最先进的方法依赖于专有数据集进行大规模预训练，这影响了该领域的可重复性。&lt;h4&gt;目的&lt;/h4&gt;研究大规模预训练数据集对仅使用2D图像进行营养成分估算的深度学习模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;作者对在ImageNet和COYO两个大型公共数据集上预训练的Vision Transformer模型进行微调和评估，并将其性能与在私有JFT-300M数据集上预训练的最先进方法以及基线CNN模型（InceptionV2和ResNet-50）进行比较。实验在Nutrition5k数据集上进行，这是一个包含高精度营养注释的真实世界食物盘的大规模集合。&lt;h4&gt;主要发现&lt;/h4&gt;在JFT-300M上预训练的模型显著优于在公共数据集上预训练的模型；在大型COYO数据集上预训练的模型对于这个特定的回归任务表现比在ImageNet上预训练的模型更差，这与最初的假设相反；数据集的特性（规模、领域相关性和质量）对有效迁移学习至关重要。&lt;h4&gt;结论&lt;/h4&gt;分析提供了定量证据，突显了预训练数据集特性（包括规模、领域相关性和质量）在2D营养成分估算的有效迁移学习中的关键作用。&lt;h4&gt;翻译&lt;/h4&gt;从食物图像中估算营养成分是一项具有重大健康和饮食监测意义的任务。仅依靠2D图像时，这具有挑战性，因为食物呈现方式的多样性、光照条件的变化以及缺乏深度信息导致难以推断体积和质量。此外，该领域的可重复性受到最先进方法对大规模预训练专有数据集依赖的阻碍。在本文中，我们研究了大规模预训练数据集对仅使用2D图像进行营养成分估算的深度学习模型性能的影响。我们对在两个大型公共数据集ImageNet和COYO上预训练的Vision Transformer模型进行微调和评估，将其性能与在私有JFT-300M数据集上预训练的最先进方法以及基线CNN模型（InceptionV2和ResNet-50）进行比较。我们在Nutrition5k数据集上进行了广泛实验，这是一个包含高精度营养注释的真实世界食物盘的大规模集合。我们使用平均绝对误差和平均绝对百分比误差进行的评估显示，在JFT-300M上预训练的模型显著优于在公共数据集上预训练的模型。出乎意料的是，对于这个特定的回归任务，在大型COYO数据集上预训练的模型表现比在ImageNet上预训练的模型更差，这与我们最初的假设相矛盾。我们的分析提供了定量证据，突显了预训练数据集特性（包括规模、领域相关性和质量）在2D营养成分估算的有效迁移学习中的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the nutritional content of food from images is a critical taskwith significant implications for health and dietary monitoring. This ischallenging, especially when relying solely on 2D images, due to thevariability in food presentation, lighting, and the inherent difficulty ininferring volume and mass without depth information. Furthermore,reproducibility in this domain is hampered by the reliance of state-of-the-artmethods on proprietary datasets for large-scale pre-training. In this paper, weinvestigate the impact of large-scale pre-training datasets on the performanceof deep learning models for nutritional estimation using only 2D images. Wefine-tune and evaluate Vision Transformer (ViT) models pre-trained on two largepublic datasets, ImageNet and COYO, comparing their performance againstbaseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art methodpre-trained on the proprietary JFT-300M dataset. We conduct extensiveexperiments on the Nutrition5k dataset, a large-scale collection of real-worldfood plates with high-precision nutritional annotations. Our evaluation usingMean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) revealsthat models pre-trained on JFT-300M significantly outperform those pre-trainedon public datasets. Unexpectedly, the model pre-trained on the massive COYOdataset performs worse than the model pre-trained on ImageNet for this specificregression task, refuting our initial hypothesis. Our analysis providesquantitative evidence highlighting the critical role of pre-training datasetcharacteristics, including scale, domain relevance, and curation quality, foreffective transfer learning in 2D nutritional estimation.</description>
      <author>example@mail.com (Michele Andrade, Guilherme A. L. Silva, Valéria Santos, Gladston Moreira, Eduardo Luz)</author>
      <guid isPermaLink="false">2508.03996v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.07838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CBDES MoE的分层解耦专家混合架构，用于解决自动驾驶中的鸟瞰图感知系统问题。该系统通过整合多个结构异构的专家网络和轻量级自注意力路由器门控机制，实现了动态专家路径选择和稀疏、输入感知的高效推理。实验表明，该方法在3D目标检测中优于固定单专家基线模型。&lt;h4&gt;背景&lt;/h4&gt;基于多传感器特征融合的鸟瞰图(BEV)感知系统已成为端到端自动驾驶的基础。然而，现有的多模态BEV方法通常存在输入适应性有限、建模能力受限和泛化能力欠佳等问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态BEV方法中存在的输入适应性有限、建模能力受限和泛化能力欠佳等挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为CBDES MoE的分层解耦专家混合架构。该架构在功能模块级别整合了多个结构异构的专家网络，并采用轻量级自注意力路由器(SAR)门控机制，实现动态专家路径选择和稀疏、输入感知的高效推理。这是首个在自动驾驶领域功能模块粒度构建的模块化专家混合框架。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界的nuScenes数据集上进行的大量评估表明，CBDES MoE在3D目标检测中始终优于固定的单专家基线模型。与最强的单专家模型相比，CBDES MoE在mAP上提高了1.6个百分点，在NDS上提升了4.1个百分点。&lt;h4&gt;结论&lt;/h4&gt;CBDES MoE方法的有效性和实际优势得到了验证，表明所提出的架构能够解决现有多模态BEV方法的局限性，提高自动驾驶系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;基于多传感器特征融合的鸟瞰图(BEV)感知系统已成为端到端自动驾驶的基本基石。然而，现有的多模态BEV方法普遍存在输入适应性有限、建模能力受限和泛化能力欠佳等问题。为解决这些挑战，我们在功能模块级别提出了一种分层解耦的专家混合架构，称为计算脑发育系统专家混合(CBDES MoE)。CBDES MoE整合了多个结构异构的专家网络和轻量级自注意力路由器(SAR)门控机制，实现了动态专家路径选择和稀疏、输入感知的高效推理。据我们所知，这是自动驾驶领域首个在功能模块粒度构建的模块化专家混合框架。在真实世界nuScenes数据集上的大量评估表明，CBDES MoE在3D目标检测中始终优于固定的单专家基线模型。与最强的单专家模型相比，CBDES MoE在mAP上实现了1.6个百分点的增长，在NDS上提升了4.1个百分点，证明了所提出方法的有效性和实际优势。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有多模态BEV感知方法存在的输入适应性有限、建模能力受限和泛化能力不足的问题。在现实中，自动驾驶系统需要在各种复杂动态环境下（如不同光照、天气条件、摄像头视角）可靠工作，而单一固定骨干网络难以捕捉多样化的场景信息，导致性能下降，这对安全关键的自动驾驶系统尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后借鉴了Mixture-of-Experts(MoE)范式在自然语言处理中的成功应用，将其引入到自动驾驶BEV感知领域。作者设计了异构专家集合（Swin Transformer、ResNet、ConvNeXt和PVT）和轻量级自注意力路由器(SAR)，并引入负载平衡正则化。该方法借鉴了BEVFusion等多模态融合框架的基本架构，以及动态卷积和可变形注意力的设计思想，但将其扩展到了更粗粒度的架构层面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多个结构异构的专家网络和动态路由机制，根据输入条件自动选择最适合的专家进行处理，从而增强模型对不同环境的适应性。整体流程：1)接收多摄像头图像和LiDAR输入；2)四个异构专家网络分别提取图像特征；3)自注意力路由器分析输入并生成专家路由概率；4)根据路由概率选择和融合专家特征；5)将特征投影到BEV空间；6)与其他模态特征融合；7)传递给下游任务处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个在自动驾驶领域构建在功能模块粒度的模块化MoE框架；2)整合四种结构异构专家网络的设计；3)轻量级自注意力路由机制；4)负载平衡正则化防止专家坍塌。相比之前工作，CBDES MoE不采用固定单骨干网络，而是实现动态专家选择；不仅限于细粒度模块调整，而是实现粗粒度架构多样性；专门针对自动驾驶多模态BEV感知任务设计，解决了异构专家组合和跨模态一致性挑战；专注于感知系统而非仅规划决策层的MoE应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CBDES MoE通过引入异构专家网络和自注意力路由机制，实现了自动驾驶BEV感知系统中动态、高效的任务特定专家选择，显著提升了3D目标检测性能和环境适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bird's Eye View (BEV) perception systems based on multi-sensor feature fusionhave become a fundamental cornerstone for end-to-end autonomous driving.However, existing multi-modal BEV methods commonly suffer from limited inputadaptability, constrained modeling capacity, and suboptimal generalization. Toaddress these challenges, we propose a hierarchically decoupledMixture-of-Experts architecture at the functional module level, termedComputing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoEintegrates multiple structurally heterogeneous expert networks with alightweight Self-Attention Router (SAR) gating mechanism, enabling dynamicexpert path selection and sparse, input-aware efficient inference. To the bestof our knowledge, this is the first modular Mixture-of-Experts frameworkconstructed at the functional module granularity within the autonomous drivingdomain. Extensive evaluations on the real-world nuScenes dataset demonstratethat CBDES MoE consistently outperforms fixed single-expert baselines in 3Dobject detection. Compared to the strongest single-expert model, CBDES MoEachieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,demonstrating the effectiveness and practical advantages of the proposedapproach.</description>
      <author>example@mail.com (Qi Xiang, Kunsong Shi, Zhigui Lin, Lei He)</author>
      <guid isPermaLink="false">2508.07838v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring</title>
      <link>http://arxiv.org/abs/2508.07552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于特征图收敛分数的独立评估方法，通过构建双粒度动态加权评分系统和CLIP-based特征图质量评估网络，实现了对自动驾驶端到端模型中功能模块生成的特征图质量的全面评估，并在NuScenes数据集上验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;端到端模型正在成为自动驾驶感知和规划的主流方法，但缺乏对中间功能模块的明确监督信号，导致操作机制不透明，可解释性有限，使传统方法难以独立评估和训练这些模块。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于特征图收敛分数的独立评估方法，构建双粒度动态加权评分系统，形成统一的定量指标-特征图质量分数，并开发CLIP-based特征图质量评估网络，以实现对功能模块生成的特征图质量的实时质量分析。&lt;h4&gt;方法&lt;/h4&gt;基于特征图-真实表示相似性的评估框架，构建双粒度动态加权评分系统，形成特征图质量分数这一统一定量指标，开发结合特征-真实编码器和质量分数预测头的CLIP-based特征图质量评估网络。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes数据集上的实验表明，将评估模块整合到训练中可以提高3D目标检测性能，NDS指标提升了3.89%，验证了该方法在提高特征表示质量和整体模型性能方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效评估功能模块生成的特征图质量，这种评估方法可以提升整体模型性能并增强特征表示质量。&lt;h4&gt;翻译&lt;/h4&gt;端到端模型正在成为自动驾驶感知和规划的主流。然而，缺乏对中间功能模块的明确监督信号导致操作机制不透明和可解释性有限，使传统方法难以独立评估和训练这些模块。在该问题上，本研究基于特征图-真实表示相似性评估框架，提出了基于特征图收敛分数的独立评估方法。构建了双粒度动态加权评分系统，形成了统一的定量指标-特征图质量分数，以实现对功能模块生成的特征图质量的全面评估。进一步开发了基于CLIP的特征图质量评估网络，结合特征-真实编码器和质量分数预测头，实现对功能模块生成的特征图质量的实时分析。在NuScenes数据集上的实验结果表明，将我们的评估模块整合到训练中提高了3D目标检测性能，NDS指标提升了3.89%。这些结果验证了我们的方法在提高特征表示质量和整体模型性能方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶端到端模型中中间功能模块缺乏独立评估和优化的问题。由于端到端模型的'黑盒'特性，内部特征缺乏显式监督信号，使得传统方法难以独立评估和训练这些模块。这个问题在现实中至关重要，因为自动驾驶系统的安全性和可靠性直接依赖于其感知和规划模块的性能。无法有效评估这些模块就难以提高系统整体性能和安全性，也限制了模型调试效率和可解释性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是端到端模型中中间特征缺乏显式监督的问题。他们借鉴了多模块学习(MML)的思想，将感知算法划分为多个专门功能模块，并参考了特征图-真实表示相似性评估框架[19]，但认识到这种方法尚未集成到模型训练中。基于这些观察，作者设计了基于特征图质量评分(FMQS)的独立评估方法，构建了双粒度动态加权评分系统，并开发了基于CLIP的特征图质量评估网络，实现了对特征图质量的实时分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过特征图质量评分(FMQS)实现对自动驾驶模型中功能模块的独立评估和优化，认为特征图质量与模型最终性能之间存在密切关联。整体流程：1)构建双粒度动态加权评分系统，从宏观(模型性能)和微观(特征图相似性)两个维度评估特征图质量；2)开发基于CLIP的特征图质量评估网络，包括特征图编码器、真实文本编码器和FMQS预测头；3)将评估网络集成到模型训练过程中，将预测的FMQS作为辅助损失项，与原始任务损失结合共同优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出特征图质量评分(FMQS)和双粒度动态加权评分系统；2)设计基于CLIP的特征图质量评估网络；3)将评估模块有效集成到训练过程中。相比之前工作：1)与FMCE-Net[18]相比，能评估多个级联功能模块；2)与[19]相比，将评估指标集成到训练中可直接贡献于优化；3)结合特征图语义一致性和任务性能反馈，建立多维度评估模型；4)通过双粒度评估机制同时考虑全局模型水平和局部特征图水平。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于特征图质量评分的解耦功能评估方法，通过双粒度动态加权评分系统和CLIP-based评估网络，实现了对自动驾驶模型中间功能模块的独立评估和优化，显著提升了模型性能和训练效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end models are emerging as the mainstream in autonomous drivingperception and planning. However, the lack of explicit supervision signals forintermediate functional modules leads to opaque operational mechanisms andlimited interpretability, making it challenging for traditional methods toindependently evaluate and train these modules. Pioneering in the issue, thisstudy builds upon the feature map-truth representation similarity-basedevaluation framework and proposes an independent evaluation method based onFeature Map Convergence Score (FMCS). A Dual-Granularity Dynamic WeightedScoring System (DG-DWSS) is constructed, formulating a unified quantitativemetric - Feature Map Quality Score - to enable comprehensive evaluation of thequality of feature maps generated by functional modules. A CLIP-based FeatureMap Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combiningfeature-truth encoders and quality score prediction heads to enable real-timequality analysis of feature maps generated by functional modules. Experimentalresults on the NuScenes dataset demonstrate that integrating our evaluationmodule into the training improves 3D object detection performance, achieving a3.89 percent gain in NDS. These results verify the effectiveness of our methodin enhancing feature representation quality and overall model performance.</description>
      <author>example@mail.com (Ludan Zhang, Sihan Wang, Yuqi Dai, Shuofei Qiao, Lei He)</author>
      <guid isPermaLink="false">2508.07552v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Toward Patient-specific Partial Point Cloud to Surface Completion for Pre- to Intra-operative Registration in Image-guided Liver Interventions</title>
      <link>http://arxiv.org/abs/2505.19518v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种患者特定的点云补全方法，利用VN-OccNet从不完整的术中点云生成完整肝脏表面，以解决图像引导手术中术中数据缺乏表面下信息导致的配准挑战。&lt;h4&gt;背景&lt;/h4&gt;在图像引导手术中捕获的术中数据缺乏表面下的信息，而关键感兴趣区域（如血管和肿瘤）位于表面下。图像到物理配准能够融合术前信息和术中数据，但由于术中点云的部分可见性，配准过程面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种患者特定的点云补全方法来辅助配准过程，解决术中点云部分可见性导致的配准困难问题。&lt;h4&gt;方法&lt;/h4&gt;利用VN-OccNet从不完整的术中点云生成完整的肝脏表面。网络以患者特定的方式进行训练，使用来自术前模型的模拟形变来训练模型。首先分析VN-OccNet的旋转等变性特性，然后将补全的术中表面集成到Go-ICP配准算法中。&lt;h4&gt;主要发现&lt;/h4&gt;VN-OccNet具有有效的旋转等变性和从不完整的术中表面恢复完整表面的能力。将补全的术中表面集成到配准算法中能够改善初始刚性配准结果。&lt;h4&gt;结论&lt;/h4&gt;患者特定的点云补全方法在缓解术中部分可见性带来的挑战方面具有前景。VN-OccNet的旋转等变性和表面生成能力在开发针对术中点云变化的稳健配准框架方面具有强大前景。&lt;h4&gt;翻译&lt;/h4&gt;在图像引导手术期间捕获的术中数据缺乏表面下信息，而关键感兴趣区域（如血管和肿瘤）位于表面下。图像到物理配准能够融合术前信息和术中数据，通常表示为点云。然而，由于术中点云的部分可见性，这种配准过程面临挑战。在本研究中，我们提出了一种患者特定的点云补全方法来辅助配准过程。具体而言，我们利用VN-OccNet从不完整的术中点云生成完整的肝脏表面。该网络以患者特定的方式进行训练，其中使用来自术前模型的模拟形变来训练模型。首先，我们对VN-OccNet的旋转等变性特性及其从不完整的术中表面恢复完整表面的有效性进行了深入分析。接下来，我们将补全的术中表面集成到Go-ICP配准算法中，以展示其在改善初始刚性配准结果方面的效用。我们的结果突显了这种患者特定的补全方法在缓解术中部分可见性带来的挑战方面的前景。VN-OccNet的旋转等变性和表面生成能力在开发针对术中点云变化的稳健配准框架方面具有强大前景。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决图像引导肝脏手术中术中数据部分可见性的问题。由于相机视角限制和遮挡，术中获取的点云数据不完整，导致术前CT/MRI图像与术中数据的配准精度不足。这个问题很重要，因为肝脏中的血管和肿瘤等关键区域位于器官表面下方，准确的配准对帮助外科医生精确定位这些区域、提高手术安全性和成功率至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法（如Jia和Foti等人的工作）的局限性，指出它们需要刚性初始化或手动识别对应点。作者选择使用VN-OccNet作为基础，因为它具有旋转等变性和生成水密网格的能力，适合处理不同方向的术中数据。作者借鉴了点云补全、表面重建和配准领域的现有技术，但创新性地采用了患者特定训练策略，通过模拟患者肝脏变形来生成训练数据，使网络能够学习患者特定的几何特征和变形模式。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过患者特定的点云补全来提高术前到术中配准的准确性。整体流程包括：1) 使用患者特定的术前肝脏模型模拟变形生成训练数据；2) 训练VN-OccNet网络，输入部分术中点云，输出占用概率；3) 使用多分辨率等值面提取和Marching Cubes算法从预测的占用点生成完整表面网格；4) 从生成的网格中提取顶点作为完整目标点云；5) 使用Go-ICP算法将完整目标点云与术前源点云进行配准。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 患者特定训练策略，使网络专注于特定患者的几何和变形特征；2) 利用VN-OccNet的旋转等变性，处理不同方向的术中数据；3) 生成水密网格而非点云，提供均匀表面采样；4) 直接将补全表面用于配准，显著提高精度。相比之前工作，此方法不需要刚性配准初始化（如Jia的方法），也不需要手动识别对应点（如Foti的方法），且专门针对肝脏手术中的配准挑战进行了优化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种患者特定的点云补全方法，利用VN-OccNet的旋转等变性和网格生成能力，显著提高了图像引导肝脏手术中术前到术中配准的准确性，有效解决了术中部分可见性带来的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intra-operative data captured during image-guided surgery lacks sub-surfaceinformation, where key regions of interest, such as vessels and tumors, reside.Image-to-physical registration enables the fusion of pre-operative informationand intra-operative data, typically represented as a point cloud. However, thisregistration process struggles due to partial visibility of the intra-operativepoint cloud. In this research, we propose a patient-specific point cloudcompletion approach to assist with the registration process. Specifically, weleverage VN-OccNet to generate a complete liver surface from a partialintra-operative point cloud. The network is trained in a patient-specificmanner, where simulated deformations from the pre-operative model are used totrain the model. First, we conduct an in-depth analysis of VN-OccNet'srotation-equivariant property and its effectiveness in recovering completesurfaces from partial intra-operative surfaces. Next, we integrate thecompleted intra-operative surface into the Go-ICP registration algorithm todemonstrate its utility in improving initial rigid registration outcomes. Ourresults highlight the promise of this patient-specific completion approach inmitigating the challenges posed by partial intra-operative visibility. Therotation equivariant and surface generation capabilities of VN-OccNet holdstrong promise for developing robust registration frameworks for variations ofthe intra-operative point cloud.</description>
      <author>example@mail.com (Nakul Poudel, Zixin Yang, Kelly Merrell, Richard Simon, Cristian A. Linte)</author>
      <guid isPermaLink="false">2505.19518v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Learning an Implicit Physics Model for Image-based Fluid Simulation</title>
      <link>http://arxiv.org/abs/2508.08254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种从单张静态图像生成具有物理一致性的4D场景（包含运动和3D几何）的新方法，特别针对自然流体图像。通过物理信息神经网络和基于物理原理的损失函数，该方法能够生成更符合物理规律的动画效果。&lt;h4&gt;背景&lt;/h4&gt;人类具有从单张静态图像想象4D场景（包括运动和3D几何）的非凡能力，这基于我们对类似场景的积累观察和对物理的直观理解。然而，现有方法通常使用简化的2D运动估计器来使图像动起来，导致运动预测常常违背物理原理，产生不真实的动画效果。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在复制人类从单张静态图像想象4D场景的能力，特别是在自然流体图像方面，解决现有方法生成的动画不符合物理原理的问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种新颖的方法，使用物理信息神经网络为每个表面点预测运动，并通过基于基本物理原理（包括纳维-斯托克斯方程）导出的损失项进行指导。为了捕捉外观，作者从输入图像及其估计的深度预测基于特征的3D高斯分布，然后使用预测的运动对这些高斯分布进行动画处理，并从任何期望的摄像机视角进行渲染。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果突显了该方法在产生物理合理动画方面的有效性，显示出与现有方法相比显著的性能改进。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发了一种从单张静态图像生成物理一致4D场景的新方法，特别是在自然流体图像方面，通过物理信息神经网络和基于物理原理的损失函数，解决了现有方法生成的动画不符合物理原理的问题，实验结果表明该方法具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;人类拥有从单张静态图像想象包含运动和3D几何的4D场景的非凡能力。这种能力源于我们对类似场景的积累观察和对物理的直观理解。在本文中，我们旨在在神经网络中复制这种能力，特别关注自然流体图像。现有方法通常使用简化的2D运动估计器来使图像动起来，导致运动预测常常违背物理原理，产生不真实的动画。我们的方法引入了一种从单张图像生成具有物理一致动画的4D场景的新方法。我们提出使用物理信息神经网络为每个表面点预测运动，由基本物理原理（包括纳维-斯托克斯方程）导出的损失项进行指导。为了捕捉外观，我们从输入图像及其估计的深度预测基于特征的3D高斯分布，然后使用预测的运动对这些高斯分布进行动画处理，并从任何期望的摄像机视角进行渲染。实验结果突显了我们的方法在产生物理合理动画方面的有效性，显示出与现有方法相比显著的性能改进。我们的项目页面是https://physfluid.github.io/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从单张静态流体图像生成符合物理规律的动画视频的问题。这个问题很重要，因为人类能从静止图像想象出动态场景，但现有计算机方法生成的动画往往不符合物理原理，导致结果不真实。解决这个问题可以应用于电影特效、游戏、虚拟现实等领域，生成更真实的流体效果，同时为计算机视觉和图形学提供新研究方向。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了人类从静止图像想象动态场景的能力，指出现有方法使用简单2D运动估计器导致动画不物理真实。他们设计结合数据驱动和物理原理的方法，使用物理信息神经网络预测流体运动。作者借鉴了3D高斯表示法来捕捉场景外观，参考了物理信息神经网络概念和Navier-Stokes方程等流体力学原理，并使用单目深度图帮助将2D图像转换为3D表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合数据驱动和物理原理，通过物理信息神经网络预测流体的3D速度场，然后使用3D高斯表示法生成符合物理规律的动画视频。整体流程：1)输入单张流体图像和相机轨迹；2)将图像转换为3D高斯表示（通过分层深度图像和特征提取）；3)使用物理信息神经网络预测3D速度场（结合图像特征和物理损失函数）；4)动画模块（用预测速度移动3D高斯中心并渲染视频帧，处理可能出现的空洞问题）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出物理信息神经网络动力学，结合数据驱动和物理原理；2)使用3D高斯表示法捕捉和动画化流体外观；3)直接从单张图像估计3D速度场而非仅2D运动；4)设计物理损失函数确保动画符合流体力学原理。不同之处：不同于纯数据驱动方法，我们结合物理约束；不同于传统流体模拟，我们可从单张图像推断；优于其他基于学习的方法，我们考虑物理规律使动画更真实；能处理图像编辑任务并生成符合物理规律的结果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合物理信息神经网络和3D高斯表示的新方法，能够从单张静态流体图像生成符合物理规律的动画视频，并在质量和真实性上超越了现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess an exceptional ability to imagine 4D scenes, encompassing bothmotion and 3D geometry, from a single still image. This ability is rooted inour accumulated observations of similar scenes and an intuitive understandingof physics. In this paper, we aim to replicate this capacity in neuralnetworks, specifically focusing on natural fluid imagery. Existing methods forthis task typically employ simplistic 2D motion estimators to animate theimage, leading to motion predictions that often defy physical principles,resulting in unrealistic animations. Our approach introduces a novel method forgenerating 4D scenes with physics-consistent animation from a single image. Wepropose the use of a physics-informed neural network that predicts motion foreach surface point, guided by a loss term derived from fundamental physicalprinciples, including the Navier-Stokes equations. To capture appearance, wepredict feature-based 3D Gaussians from the input image and its estimateddepth, which are then animated using the predicted motions and rendered fromany desired camera perspective. Experimental results highlight theeffectiveness of our method in producing physically plausible animations,showcasing significant performance improvements over existing methods. Ourproject page is https://physfluid.github.io/ .</description>
      <author>example@mail.com (Emily Yue-Ting Jia, Jiageng Mao, Zhiyuan Gao, Yajie Zhao, Yue Wang)</author>
      <guid isPermaLink="false">2508.08254v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Model Probabilistic Framework for Seismic Risk Assessment and Retrofit Planning of Electric Power Networks</title>
      <link>http://arxiv.org/abs/2508.07376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种用于电力系统地震风险评估和加固规划的多模型概率框架，通过整合地震危险性表征、组件级损伤分析、系统级级联影响评估和启发式优化方法，有效提高了电力网络的地震弹性。&lt;h4&gt;背景&lt;/h4&gt;电力网络是关键的生命线基础设施，地震期间的中断可能导致严重的级联故障，并严重阻碍灾后恢复。&lt;h4&gt;目的&lt;/h4&gt;提高电力网络的地震弹性，以经济有效且系统感知的方式识别和加强脆弱组件。&lt;h4&gt;方法&lt;/h4&gt;提出了一种多模型概率框架，包括：(1)区域地震危险性表征；(2)组件级损伤分析；(3)系统级级联影响评估；(4)启发式优化加固规划。使用蒙特卡洛模拟传播不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够捕获级联故障、识别关键组件并生成有效的加固策略，证明了其作为可扩展数据驱动决策支持工具的潜力。&lt;h4&gt;结论&lt;/h4&gt;该框架有望成为一种可扩展的、数据驱动的决策支持工具，用于提高电力基础设施的地震弹性。&lt;h4&gt;翻译&lt;/h4&gt;电力网络是关键的生命线，地震期间的中断可能导致严重的级联故障并显著阻碍灾后恢复。提高其地震弹性需要以经济有效且系统感知的方式识别和加强脆弱组件。然而，现有研究往往忽视电力网络在地震荷载下的系统性行为。常见局限包括忽视网络范围相互依赖关系的孤立组件分析、假设二元状态或损伤独立性的过度简化损伤模型，以及排除电气运行约束。这些简化可能导致不准确的风险估计和低效的加固决策。本研究提出了一种电力系统地震风险评估和加固规划的多模型概率框架。该方法整合：(1)区域地震危险性表征，结合地面运动预测和空间相关模型；(2)使用易损性函数和多状态损伤-功能映射的组件级损伤分析；(3)基于图的孤岛检测和约束最优潮流分析的系统级级联影响评估；(4)通过启发式优化进行加固规划，在预算约束下最小化预期年度功能损失。使用蒙特卡洛模拟在整个框架中传播不确定性。该方法在IEEE 24节点可靠性测试系统上得到展示，展示了其捕获级联故障、识别关键组件和生成有效加固策略的能力。结果强调了该框架作为提高电力基础设施地震弹性的可扩展数据驱动决策支持工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electric power networks are critical lifelines, and their disruption duringearthquakes can lead to severe cascading failures and significantly hinderpost-disaster recovery. Enhancing their seismic resilience requires identifyingand strengthening vulnerable components in a cost-effective and system-awaremanner. However, existing studies often overlook the systemic behavior of powernetworks under seismic loading. Common limitations include isolated componentanalyses that neglect network-wide interdependencies, oversimplified damagemodels assuming binary states or damage independence, and the exclusion ofelectrical operational constraints. These simplifications can result ininaccurate risk estimates and inefficient retrofit decisions. This studyproposes a multi-model probabilistic framework for seismic risk assessment andretrofit planning of electric power systems. The approach integrates: (1)regional seismic hazard characterization with ground motion prediction andspatial correlation models; (2) component-level damage analysis using fragilityfunctions and multi-state damage-functionality mappings; (3) system-levelcascading impact evaluation through graph-based island detection andconstrained optimal power flow analysis; and (4) retrofit planning viaheuristic optimization to minimize expected annual functionality loss (EAFL)under budget constraints. Uncertainty is propagated throughout the frameworkusing Monte Carlo simulation. The methodology is demonstrated on the IEEE24-bus Reliability Test System, showcasing its ability to capture cascadingfailures, identify critical components, and generate effective retrofitstrategies. Results underscore the potential of the framework as a scalable,data-informed decision-support tool for enhancing the seismic resilience ofpower infrastructure.</description>
      <author>example@mail.com (Huangbin Liang, Beatriz Moya, Francisco Chinesta, Eleni Chatzi)</author>
      <guid isPermaLink="false">2508.07376v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Conical Intersections Shed Light on Hot Carrier Cooling in Quantum Dots</title>
      <link>http://arxiv.org/abs/2508.07322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过实验观测胶体半导体纳米晶体中电子激发态的振动相干性，揭示了热载流子超快动力学机制，并证明了锥形交叉级联框架在不同配体钝化量子点中的普遍适用性。&lt;h4&gt;背景&lt;/h4&gt;先前研究表明，胺钝化量子点中的振动相干性是在通过电子激发态之间锥形交叉级联弛豫过程中产生的，这种观测为研究热载流子超快动力学提供了窗口。&lt;h4&gt;目的&lt;/h4&gt;将锥形交叉级联框架应用于表面结合羧酸酯配体的量子点，证明该框架的普遍性，并研究配体对弛豫动力学的影响机制。&lt;h4&gt;方法&lt;/h4&gt;使用宽带多维光谱学观测振动相干性的频率，建立涉及锥形交叉级联的模型来解释观测结果，并比较不同配体（胺钝化、羧酸酯钝化、乙酸盐和甲酸盐配体）对弛豫动力学的影响。&lt;h4&gt;主要发现&lt;/h4&gt;锥形交叉级联模型能准确重现观测到的振动相干性频率；配体影响弛豫动力学的机制涉及核心与配体之间的电子或振动耦合；与胺钝化量子点相比，羧酸酯钝化量子点中电子耦合机制较弱；截短配体烷基链会改变模型的预测行为。&lt;h4&gt;结论&lt;/h4&gt;锥形交叉级联框架具有普遍适用性，可应用于不同配体钝化的量子点系统；配体类型（特别是烷基链长度）会影响热载流子弛豫动力学。&lt;h4&gt;翻译&lt;/h4&gt;实验观测胶体半导体纳米晶体中电子激发态的振动相干性，为研究热载流子超快动力学提供了一个窗口。在先前的工作中，我们已经证明，在胺钝化的量子点中，这些相干性是在通过电子激发态之间的锥形交叉级联弛豫过程中产生的。在此，我们将该框架应用于带有表面结合羧酸酯配体的量子点，证明了其普遍性。一个涉及类似锥形交叉级联的模型准确重现了通过宽带多维光谱学观测到的振动相干性频率。配体对弛豫动力学的影响归因于两个不同的机制，涉及核心与配体之间的电子或振动耦合。与先前研究的胺钝化量子点相比，在羧酸酯钝化的量子点中，电子耦合机制不那么显著。此外，乙酸盐和甲酸盐配体的比较表明，截短配体烷基链会改变模型的预测行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Experimental observations of vibronic coherences in electronically excitedcolloidal semiconductor nanocrystals offer a window into the ultrafast dynamicsof hot carrier cooling. In previous work, we showed that, in amine-passivatedquantum dots (QDs), these coherences arise during relaxation through a cascadeof conical intersections between electronically excited states. Here, wedemonstrate the generality of this framework by application to QDs withsurface-bound carboxylate ligands. A model involving a similar cascade ofconical intersections accurately reproduces the frequencies of vibroniccoherences observed with broadband multidimensional spectroscopy. The impact ofligands on the relaxation dynamics is attributed to two distinct mechanismsinvolving either electronic or vibrational coupling between the core andligands. Compared to the amine-passivated QDs studied previously, theelectronic coupling mechanism is less prominent in carboxylate-passivated QDs.Furthermore, comparison of acetate and formate ligands reveals that truncatingthe ligand alkyl chains alters the relaxation behavior predicted by the model.</description>
      <author>example@mail.com (Caitlin V. Hetherington, Nila Mohan T. M., Shanu A. Shameem, Warren F. Beck, Benjamin G. Levine)</author>
      <guid isPermaLink="false">2508.07322v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion</title>
      <link>http://arxiv.org/abs/2508.07162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoopDiff的新型接触一致解耦扩散框架，用于3D人-物体交互预测。该方法通过两个不同分支分别建模人体和物体运动，使用接触点作为共享锚点桥接分支间的运动生成，并引入人驱动的交互模块增强一致性。&lt;h4&gt;背景&lt;/h4&gt;3D人-物体交互预测旨在根据历史上下文预测人类及其操作物体的未来运动。具有关节结构的人和刚性物体因不同物理特性表现出不同运动模式，但现有研究大多忽略这种区别，尝试在单一模型中同时捕捉两者的动力学。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够分别建模人体和物体不同运动模式的框架，提高3D人-物体交互预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出CoopDiff框架，包含两个分支解耦人体和物体运动建模，使用接触点作为共享锚点桥接分支，人体分支预测结构化运动，物体分支处理刚体运动，并通过一致性约束连接分支，同时引入人驱动的交互模块指导物体运动建模。&lt;h4&gt;主要发现&lt;/h4&gt;在BEHAVE和Human-object Interaction数据集上的实验表明，CoopDiff方法优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;通过解耦人体和物体运动建模并使用接触点作为共享锚点，CoopDiff框架能更准确预测3D人-物体交互，同时保持运动一致性。&lt;h4&gt;翻译&lt;/h4&gt;3D人-物体交互预测旨在根据历史上下文预测人类及其操作物体的未来运动。通常，具有关节结构的人和刚性物体由于其不同的内在物理特性而表现出不同的运动模式。然而，大多数现有工作忽略了这种区别，试图在单一的预测模型中捕捉人和物体的动力学。在这项工作中，我们提出了一种新颖的接触一致解耦扩散框架CoopDiff，它采用两个不同的分支来解耦人体和物体运动建模，以人-物体接触点作为共享锚点来桥接不同分支间的运动生成。人体动力学分支旨在预测高度结构化的人体运动，而物体动力学分支专注于具有刚体平移和旋转的物体运动。这两个分支通过一系列具有一致性约束的共享接触点连接，以实现连贯的人-物体运动预测。为进一步增强人-物体一致性和预测可靠性，我们提出了一个人驱动的交互模块来指导物体运动建模。在BEHAVE和Human-object Interaction数据集上的大量实验表明，我们的CoopDiff优于最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D人-物交互（HOI）预测问题，即根据历史信息预测未来人类和他们所操作物体的运动。这个问题在现实中非常重要，因为它有广泛的应用价值，包括机器人技术、动画制作、增强现实和具身AI等领域。同时，现有方法大多忽略了人类和物体在物理特性上的本质区别，导致预测结果不够精确和真实。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人类和物体在交互中表现出完全不同的动态模式：人类是高度关节化的，表现出多样化的运动；而物体通常是刚性的，主要表现为平移和旋转。现有方法使用单一预测模型同时处理两者，导致预测不准确。作者借鉴了扩散模型（如InterDiff）的成功应用，但认识到其局限性，进而提出将人类和物体的动态建模分离，同时保持它们之间的一致性。此外，还借鉴了Transformer架构、SMPL模型和ControlNet等技术来构建整个框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 使用双分支扩散模型分别建模人类和物体的动态；2) 以接触点作为共享锚点连接两个分支，确保运动一致性；3) 引入人类驱动的交互模块，将人类动态作为条件控制注入物体动态建模。整体流程包括：数据表示（人体姿势、物体姿势和接触点）、人类动态分支预测、物体动态分支预测、接触一致性约束、人类驱动的交互模块，以及三阶段模型训练策略。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 联系一致的解耦扩散框架，首次分离建模人类和物体动态；2) 人类驱动的交互模块，强调人类在交互中的主导作用；3) 将接触点作为共享锚点连接两个分支。相比之前工作，不同之处在于：大多数现有方法使用单一模型处理两者动态，而CoopDiff使用双分支分离建模；之前方法将接触点仅用于后处理，而CoopDiff将其作为连接分支的桥梁；现有方法平等对待人类和物体动态，而CoopDiff强调人类的主导角色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新颖的联系一致的解耦扩散框架CoopDiff，通过分离建模人类和物体的不同动态模式并利用接触点保持一致性，实现了更准确、更真实的3D人-物交互预测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D human-object interaction (HOI) anticipation aims to predict the futuremotion of humans and their manipulated objects, conditioned on the historicalcontext. Generally, the articulated humans and rigid objects exhibit differentmotion patterns, due to their distinct intrinsic physical properties. However,this distinction is ignored by most of the existing works, which intend tocapture the dynamics of both humans and objects within a single predictionmodel. In this work, we propose a novel contact-consistent decoupled diffusionframework CoopDiff, which employs two distinct branches to decouple human andobject motion modeling, with the human-object contact points as shared anchorsto bridge the motion generation across branches. The human dynamics branch isaimed to predict highly structured human motion, while the object dynamicsbranch focuses on the object motion with rigid translations and rotations.These two branches are bridged by a series of shared contact points withconsistency constraint for coherent human-object motion prediction. To furtherenhance human-object consistency and prediction reliability, we propose ahuman-driven interaction module to guide object motion modeling. Extensiveexperiments on the BEHAVE and Human-object Interaction datasets demonstratethat our CoopDiff outperforms state-of-the-art methods.</description>
      <author>example@mail.com (Xiaotong Lin, Tianming Liang, Jian-Fang Hu, Kun-Yu Lin, Yulei Kang, Chunwei Tian, Jianhuang Lai, Wei-Shi Zheng)</author>
      <guid isPermaLink="false">2508.07162v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting</title>
      <link>http://arxiv.org/abs/2508.07089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ForeSight的新型联合检测和预测框架，用于自动驾驶车辆的基于视觉的3D感知。该框架通过多任务流式和双向学习方法，使检测和预测能够共享查询内存并无缝传播信息，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;传统方法将检测和预测视为独立的顺序任务，限制了它们利用时间线索的能力。基于跟踪的方法需要显式的对象关联，容易导致错误传播，且难以有效扩展到多帧序列。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时进行检测和预测的框架，克服传统方法的局限性，提高自动驾驶车辆3D感知的性能，减少错误传播，并有效处理多帧序列。&lt;h4&gt;方法&lt;/h4&gt;ForeSight采用多任务流式和双向学习架构，包括预测感知的检测变换器和流式预测变换器。预测感知的检测变换器通过集成来自多假设预测内存队列的轨迹预测来增强空间推理；流式预测变换器使用过去的预测和精细化的检测来提高时间一致性。该框架消除了显式对象关联的需要，采用无跟踪模型设计。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，ForeSight实现了最先进的性能，EPA达到54.9%，比之前的方法高出9.3%，同时在多视图检测和预测模型中获得了最佳的mAP和minADE。&lt;h4&gt;结论&lt;/h4&gt;ForeSight通过将检测和预测统一在一个框架内，并采用创新的内存共享和信息传播机制，显著提高了自动驾驶车辆的3D感知能力。该方法的成功证明了联合处理检测和预测任务的有效性，为未来自动驾驶感知系统的发展提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了ForeSight，这是一种用于自动驾驶车辆中基于视觉的3D感知的新型联合检测和预测框架。传统方法将检测和预测视为独立的顺序任务，限制了它们利用时间线索的能力。ForeSight通过多任务流式和双向学习方法解决了这一限制，使检测和预测能够共享查询内存并无缝传播信息。预测感知的检测变换器通过集成来自多假设预测内存队列的轨迹预测来增强空间推理，而流式预测变换器使用过去的预测和精细化的检测来提高时间一致性。与基于跟踪的方法不同，ForeSight消除了显式对象关联的需要，采用一种无跟踪模型，有效扩展到多帧序列。在nuScenes数据集上的实验表明，ForeSight实现了最先进的性能，EPA达到54.9%，比之前的方法高出9.3%，同时在多视图检测和预测模型中获得了最佳的mAP和minADE。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中物体检测和轨迹预测的整合问题。传统方法将这两个任务视为独立顺序任务，限制了利用时间线索的能力，并且依赖跟踪技术会引入错误传播问题。这个问题在现实中非常重要，因为自动驾驶车辆需要准确理解动态驾驶环境，特别是在处理遮挡或部分可见物体时，这些情况可能带来关键安全风险。准确的物体检测和轨迹预测对自动驾驶系统的安全性和可靠性至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者重新思考了传统自动驾驶系统架构，提出不应将运动预测仅用于规划后丢弃，而应将其反馈到检测和预测任务中。他们借鉴了现有工作中的多个方面：基于稀疏查询的物体检测方法（如DETR3D、PETR）、时域融合方法（如BEVDet4D、StreamPETR）以及运动预测方法。但ForeSight的创新在于将这些方法整合为一个统一框架，并引入了双向查询传播机制，实现检测和预测之间的闭环反馈。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双向查询传播和联合内存系统，实现检测和预测之间的闭环反馈，消除对显式对象关联的依赖，减少错误传播，并使用流式处理提高计算效率。整体流程包括：1)从多视角图像提取场景特征，可选地从高清地图编码道路上下文；2)通过联合流式内存队列管理过去和未来的查询；3)初始化检测查询并使用预测感知检测Transformer融合信息；4)基于检测结果初始化预测查询，并通过联合流式预测Transformer生成未来轨迹；5)使用多任务目标函数优化模型，并将预测查询推入内存供未来使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双向查询传播机制，实现预测信息反馈回检测任务；2)无跟踪的流式预测方法，消除跟踪瓶颈；3)联合内存系统，使过去检测和预测作为当前任务的先验；4)预测感知检测，将轨迹预测集成到空间推理中。相比之前工作，ForeSight将检测和预测统一为联合框架而非独立任务，消除了显式对象关联的需要，通过流式处理提高计算效率，并首次实现双向查询传播形成闭环反馈。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ForeSight通过引入双向查询传播和联合内存系统，实现了自动驾驶系统中物体检测和轨迹预测的高效整合，显著提高了在复杂场景中的感知准确性和预测可靠性，同时消除了传统跟踪方法的瓶颈和错误传播问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ForeSight, a novel joint detection and forecasting framework forvision-based 3D perception in autonomous vehicles. Traditional approaches treatdetection and forecasting as separate sequential tasks, limiting their abilityto leverage temporal cues. ForeSight addresses this limitation with amulti-task streaming and bidirectional learning approach, allowing detectionand forecasting to share query memory and propagate information seamlessly. Theforecast-aware detection transformer enhances spatial reasoning by integratingtrajectory predictions from a multiple hypothesis forecast memory queue, whilethe streaming forecast transformer improves temporal consistency using pastforecasts and refined detections. Unlike tracking-based methods, ForeSighteliminates the need for explicit object association, reducing error propagationwith a tracking-free model that efficiently scales across multi-framesequences. Experiments on the nuScenes dataset show that ForeSight achievesstate-of-the-art performance, achieving an EPA of 54.9%, surpassing previousmethods by 9.3%, while also attaining the best mAP and minADE among multi-viewdetection and forecasting models.</description>
      <author>example@mail.com (Sandro Papais, Letian Wang, Brian Cheong, Steven L. Waslander)</author>
      <guid isPermaLink="false">2508.07089v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Historical Prediction Attention Mechanism based Trajectory Forecasting for Proactive Work Zone Safety in a Digital Twin Environment</title>
      <link>http://arxiv.org/abs/2508.06544v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于基础设施的主动式工作区安全预警系统，利用数字孪生环境整合实时多传感器数据、高精度地图和历史预测注意力机制的轨迹预测模型，可有效预测车辆轨迹并提前预警潜在冲突。&lt;h4&gt;背景&lt;/h4&gt;工作区车辆冲突可能导致严重事故，传统安全系统可能无法准确预测车辆轨迹并提供及时预警。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够预测车辆轨迹并提前预警潜在冲突的系统，以预防工作区相关事故。&lt;h4&gt;方法&lt;/h4&gt;使用数字孪生环境整合实时多传感器数据和高精度地图；采用基于历史预测注意力机制的轨迹预测模型；结合SUMO和CARLA模拟器的联合仿真环境；使用Lanelet2 HD地图和历史预测网络模型；通过车辆边界框和概率冲突建模进行预警生成。&lt;h4&gt;主要发现&lt;/h4&gt;基于基础设施的HPNet模型在工作区数据集上表现优异，最小联合最终位移误差为0.3228米，最小联合平均位移误差为0.1327米，低于Argoverse和Interaction数据集的基准；主动安全预警应用程序能够有效发出潜在车辆冲突的警报。&lt;h4&gt;结论&lt;/h4&gt;基于基础设施的HPNet模型在预测工作区车辆轨迹方面表现优越，结合预警系统可有效预防工作区相关事故。&lt;h4&gt;翻译&lt;/h4&gt;主动安全系统旨在通过预测车辆之间的潜在冲突并实现早期干预来预防工作区相关事故，从而缓解风险。本研究提出了一种基于基础设施的主动式工作区安全预警系统，该系统利用数字孪生环境，整合实时多传感器数据、详细高精度地图以及基于历史预测注意力机制的轨迹预测模型。结合城市交通移动性仿真和CARLA学习行动模拟器的联合仿真环境，以及Lanelet2高精度地图和历史预测网络模型，我们证明了在高速公路工作区中车辆交互的有效轨迹预测和早期预警生成。为评估预测轨迹的准确性，我们使用两个标准指标：联合平均位移误差和联合最终位移误差。具体而言，基于基础设施的HPNet模型在联合仿真环境生成的工作区数据集上表现出优越的性能，最小联合最终位移误差为0.3228米，最小联合平均位移误差为0.1327米，低于Argoverse数据集和Interaction数据集的基准。此外，我们的主动安全预警应用程序利用车辆边界框和概率冲突建模，展示了其发出潜在车辆冲突警报的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Proactive safety systems aim to mitigate risks by anticipating potentialconflicts between vehicles and enabling early intervention to prevent workzone-related crashes. This study presents an infrastructure-enabled proactivework zone safety warning system that leverages a Digital Twin environment,integrating real-time multi-sensor data, detailed High-Definition (HD) maps,and a historical prediction attention mechanism-based trajectory predictionmodel. Using a co-simulation environment that combines Simulation of UrbanMObility (SUMO) and CAR Learning to Act (CARLA) simulators, along with Lanelet2HD maps and the Historical Prediction Network (HPNet) model, we demonstrateeffective trajectory prediction and early warning generation for vehicleinteractions in freeway work zones. To evaluate the accuracy of predictedtrajectories, we use two standard metrics: Joint Average Displacement Error(ADE) and Joint Final Displacement Error (FDE). Specifically, theinfrastructure-enabled HPNet model demonstrates superior performance on thework-zone datasets generated from the co-simulation environment, achieving aminimum Joint FDE of 0.3228 meters and a minimum Joint ADE of 0.1327 meters,lower than the benchmarks on the Argoverse (minJointFDE: 1.0986 m, minJointADE:0.7612 m) and Interaction (minJointFDE: 0.8231 m, minJointADE: 0.2548 m)datasets. In addition, our proactive safety warning generation application,utilizing vehicle bounding boxes and probabilistic conflict modeling,demonstrates its capability to issue alerts for potential vehicle conflicts.</description>
      <author>example@mail.com (Minhaj Uddin Ahmad, Mizanur Rahman, Alican Sevim, David Bodoh, Sakib Khan, Li Zhao, Nathan Huynh, Eren Erman Ozguven)</author>
      <guid isPermaLink="false">2508.06544v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing</title>
      <link>http://arxiv.org/abs/2508.04361v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了OmniPlay基准测试，用于评估通用基础模型在动态、交互式世界中的智能表现，特别是它们在多模态融合和推理方面的能力。研究发现了当前全模态模型在高保真记忆任务与需要推理和战略规划的任务之间的性能差异，以及模态冲突对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;现有的通用基础模型（如Gemini和GPT-4o）展示了令人印象深刻的多模态能力，但现有的评估方法无法测试它们在动态、交互式世界中的智能。静态基准测试缺乏主动性，而交互式基准测试则存在严重的模态瓶颈，通常忽略关键的听觉和时间线索。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一评估差距，研究引入了OmniPlay，一个诊断基准，旨在评估代理模型在完整感官范围内的融合和推理能力。&lt;h4&gt;方法&lt;/h4&gt;OmniPlay基于模态相互依赖的核心理念，包含五个游戏环境，创造协同和冲突的场景，迫使代理执行真正的跨模态推理。研究对六个领先的全模态模型进行了全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;1. 六个领先的全模态模型表现出关键的二元性：它们在高保真记忆任务中表现出超人性能，但在需要稳健推理和战略规划的挑战中存在系统性失败。2. 这种脆弱性源于脆弱的融合机制，导致在模态冲突下性能灾难性下降。3. 研究发现了一个反直觉的'少即是多'悖论，即移除感官信息可以悖论性地提高性能。&lt;h4&gt;结论&lt;/h4&gt;实现稳健AGI的路径需要超越扩展规模的研究重点，明确解决协同融合问题。研究平台可在https://github.com/fuqingbie/omni-game-benchmark获取。&lt;h4&gt;翻译&lt;/h4&gt;虽然像Gemini和GPT-4o这样的通用基础模型展示了令人印象深刻的多模态能力，但现有的评估无法测试它们在动态、交互式世界中的智能。静态基准测试缺乏主动性，而交互式基准测试则存在严重的模态瓶颈，通常忽略关键的听觉和时间线索。为了弥合这一评估差距，我们引入了OmniPlay，一个诊断基准，旨在评估代理模型在整个感官范围内的融合和推理能力。基于模态相互依赖的核心理念，OmniPlay包含一套五个游戏环境，系统地创造协同和冲突的场景，迫使代理执行真正的跨模态推理。我们对六个领先的全模态模型的全面评估揭示了一个关键的二元性：它们在高保真记忆任务中表现出超人性能，但在需要稳健推理和战略规划的挑战中存在系统性失败。我们证明这种脆弱性源于脆弱的融合机制，导致在模态冲突下性能灾难性下降，并发现了一个反直觉的'少即是多'悖论，即移除感官信息可以悖论性地提高性能。我们的研究结果表明，实现稳健AGI的路径需要超越扩展规模的研究重点，明确解决协同融合问题。我们的平台可在https://github.com/fuqingbie/omni-game-benchmark进行匿名评审。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While generalist foundation models like Gemini and GPT-4o demonstrateimpressive multi-modal competence, existing evaluations fail to test theirintelligence in dynamic, interactive worlds. Static benchmarks lack agency,while interactive benchmarks suffer from a severe modal bottleneck, typicallyignoring crucial auditory and temporal cues. To bridge this evaluation chasm,we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,but to probe the fusion and reasoning capabilities of agentic models across thefull sensory spectrum. Built on a core philosophy of modality interdependence,OmniPlay comprises a suite of five game environments that systematically createscenarios of both synergy and conflict, forcing agents to perform genuinecross-modal reasoning. Our comprehensive evaluation of six leading omni-modalmodels reveals a critical dichotomy: they exhibit superhuman performance onhigh-fidelity memory tasks but suffer from systemic failures in challengesrequiring robust reasoning and strategic planning. We demonstrate that thisfragility stems from brittle fusion mechanisms, which lead to catastrophicperformance degradation under modality conflict and uncover a counter-intuitive"less is more" paradox, where removing sensory information can paradoxicallyimprove performance. Our findings suggest that the path toward robust AGIrequires a research focus beyond scaling to explicitly address synergisticfusion. Our platform is available for anonymous review athttps://github.com/fuqingbie/omni-game-benchmark.</description>
      <author>example@mail.com (Fuqing Bie, Shiyu Huang, Xijia Tao, Zhiqin Fang, Leyi Pan, Junzhe Chen, Min Ren, Liuyu Xiang, Zhaofeng He)</author>
      <guid isPermaLink="false">2508.04361v2</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
  <item>
      <title>Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning</title>
      <link>http://arxiv.org/abs/2508.06199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对预训练神经网络在化学和小分子药物设计中的应用进行了迄今为止最广泛的比较评估，发现几乎所有神经网络模型相比基线ECFP分子指纹方法没有显著改进，只有基于分子指纹的CLAMP模型表现明显优于其他模型。&lt;h4&gt;背景&lt;/h4&gt;预训练神经网络在化学和小分子药物设计领域引起了广泛关注，这些模型的嵌入表示被广泛用于分子性质预测、虚拟筛选和分子化学中的小数据学习。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在对各种预训练神经网络模型进行最广泛的比较评估，以了解它们在实际应用中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;研究在公平比较框架下评估了25个不同模态、架构和预训练策略的模型，并使用了专门的分层贝叶斯统计测试模型进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;几乎所有神经网络模型相比基线ECFP分子指纹方法显示出可忽略或没有改进，只有基于分子指纹的CLAMP模型在统计上显著优于其他模型。&lt;h4&gt;结论&lt;/h4&gt;这些发现引发了人们对现有研究评估严谨性的担忧，研究讨论了可能的原因，提出了解决方案，并给出了实用建议。&lt;h4&gt;翻译&lt;/h4&gt;预训练神经网络在化学和小分子药物设计中引起了广泛关注。这些模型的嵌入表示被广泛用于分子性质预测、虚拟筛选和分子化学中的小数据学习。本研究迄今为止对这类模型进行了最广泛的比较，评估了25个模型在25个数据集上的表现。在公平比较框架下，我们评估了涵盖各种模态、架构和预训练策略的模型。使用专门的分层贝叶斯统计测试模型，我们得出了一个令人惊讶的结果：几乎所有神经网络模型相比基线ECFP分子指纹显示出可忽略或没有改进。只有同样基于分子指纹的CLAMP模型在统计上显著优于其他模型。这些发现引发了对现有研究评估严谨性的担忧。我们讨论了潜在原因，提出了解决方案，并给出了实用建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained neural networks have attracted significant interest in chemistryand small molecule drug design. Embeddings from these models are widely usedfor molecular property prediction, virtual screening, and small data learningin molecular chemistry. This study presents the most extensive comparison ofsuch models to date, evaluating 25 models across 25 datasets. Under a faircomparison framework, we assess models spanning various modalities,architectures, and pretraining strategies. Using a dedicated hierarchicalBayesian statistical testing model, we arrive at a surprising result: nearlyall neural models show negligible or no improvement over the baseline ECFPmolecular fingerprint. Only the CLAMP model, which is also based on molecularfingerprints, performs statistically significantly better than thealternatives. These findings raise concerns about the evaluation rigor inexisting studies. We discuss potential causes, propose solutions, and offerpractical recommendations.</description>
      <author>example@mail.com (Mateusz Praski, Jakub Adamczyk, Wojciech Czech)</author>
      <guid isPermaLink="false">2508.06199v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events</title>
      <link>http://arxiv.org/abs/2508.06122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 6 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究应用表征学习算法处理卫星图像，并通过各种天气事件分类评估学习到的潜在空间表现，发现卷积自编码器(CAE)在大多数任务中表现最佳，且高分辨率数据集对深度学习算法有益，但潜在空间维度小于128会导致误报率显著增加。&lt;h4&gt;背景&lt;/h4&gt;卫星图像分析在天气事件监测中具有重要价值，但如何有效提取和利用卫星图像中的表征信息仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;评估不同表征学习算法在卫星图像天气事件分类中的表现，并探索影响分类效果的关键因素。&lt;h4&gt;方法&lt;/h4&gt;研究了三种表征学习算法：主成分分析(PCA)、卷积自编码器(CAE)和预训练残差网络(PT)，通过分类任务评估潜在空间质量，并测试了数据分辨率和潜在空间大小对分类效果的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1) CAE学习的潜在空间在所有分类任务中表现最佳；2) PCA虽高命中率但误报率也高；3) PT在识别热带气旋方面表现突出，其他任务较差；4) 高分辨率数据集对深度学习算法有益；5) 潜在空间维度小于128会导致误报率显著增加；6) CAE学习到的表征与物理属性缺乏直接联系。&lt;h4&gt;结论&lt;/h4&gt;开发物理信息的CAE版本可能是提高卫星图像天气事件分类准确性的有前景方向。&lt;h4&gt;翻译&lt;/h4&gt;本研究将表征学习算法应用于卫星图像，并通过各种天气事件的分类评估学习到的潜在空间。研究的算法包括经典线性变换即主成分分析(PCA)、最先进的深度学习方法即卷积自编码器(CAE)，以及在大型图像数据集上预训练的残差网络(PT)。实验结果表明，CAE学习到的潜在空间在所有分类任务中持续显示更高的威胁分数。PCA的分类产生了高命中率但也产生了高误报率。此外，PT在识别热带气旋方面表现异常出色，但在其他任务中表现较差。进一步的实验表明，从更高分辨率数据集中学习到的表征在深度学习算法(CAE和PT)的所有分类任务中都更优。我们还发现，较小的潜在空间大小对分类任务的命中率影响较小，但潜在空间维度小于128会导致显著更高的误报率。尽管CAE能够有效且高效地学习潜在空间，但学习到的表征与物理属性缺乏直接联系。因此，开发物理信息的CAE版本可能是当前工作的有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.22541/essoar.168394729.95734739/v1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study applied representation learning algorithms to satellite images andevaluated the learned latent spaces with classifications of various weatherevents. The algorithms investigated include the classical lineartransformation, i.e., principal component analysis (PCA), state-of-the-art deeplearning method, i.e., convolutional autoencoder (CAE), and a residual networkpre-trained with large image datasets (PT). The experiment results indicatedthat the latent space learned by CAE consistently showed higher threat scoresfor all classification tasks. The classifications with PCA yielded high hitrates but also high false-alarm rates. In addition, the PT performedexceptionally well at recognizing tropical cyclones but was inferior in othertasks. Further experiments suggested that representations learned fromhigher-resolution datasets are superior in all classification tasks fordeep-learning algorithms, i.e., CAE and PT. We also found that smaller latentspace sizes had minor impact on the classification task's hit rate. Still, alatent space dimension smaller than 128 caused a significantly higher falsealarm rate. Though the CAE can learn latent spaces effectively and efficiently,the interpretation of the learned representation lacks direct connections tophysical attributions. Therefore, developing a physics-informed version of CAEcan be a promising outlook for the current work.</description>
      <author>example@mail.com (Ting-Shuo Yo, Shih-Hao Su, Chien-Ming Wu, Wei-Ting Chen, Jung-Lien Chu, Chiao-Wei Chang, Hung-Chi Kuo)</author>
      <guid isPermaLink="false">2508.06122v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Dual prototype attentive graph network for cross-market recommendation</title>
      <link>http://arxiv.org/abs/2508.05969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICONIP 2025 (Oral)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DGRE的新型跨市场推荐方法，通过同时考虑市场特定和市场共享的洞察力，提高了跨市场推荐系统的泛化能力和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;跨市场推荐系统旨在利用成熟市场的历史数据来促进新兴市场中的跨国产品，但现有方法往往忽视了不同市场用户间潜在共享的偏好，主要专注于建模每个市场内的特定偏好。&lt;h4&gt;目的&lt;/h4&gt;整合市场特定和市场共享的洞察力，增强跨市场推荐系统的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出DGRE方法，基于图表示学习利用物品和用户原型捕获市场特定和市场共享的洞察。通过聚类不同市场用户创建市场共享用户档案，同时聚合市场内物品特征构建物品侧原型，提供市场特定洞察。&lt;h4&gt;主要发现&lt;/h4&gt;在建模中同时考虑市场特定和市场共享方面可以改进跨市场推荐系统的泛化能力和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;考虑市场特定和市场共享两个方面的建模能有效提高跨市场推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;跨市场推荐系统旨在利用成熟市场的历史数据来促进新兴市场中的跨国产品。然而，现有方法往往忽视了不同市场用户之间潜在共享的偏好，主要专注于建模每个市场内的特定偏好。在本文中，我们认为整合市场特定和市场共享的洞察力可以增强CMRS的泛化能力和鲁棒性。我们提出了一种名为DGRE的新方法来解决这个问题。DGRE利用基于图表示学习的物品和用户原型来捕获市场特定和市场共享的洞察力。具体来说，DGRE通过聚类来自不同市场的用户来识别行为相似性并创建市场共享的用户档案，从而整合市场共享原型；同时，它通过聚合每个市场内的物品特征来构建物品侧原型，提供有价值的市场特定洞察。我们在真实的跨市场数据集上进行了广泛的实验来验证DGRE的有效性，结果表明在建模中同时考虑市场特定和市场共享方面可以改进CMRS的泛化能力和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-market recommender systems (CMRS) aim to utilize historical data frommature markets to promote multinational products in emerging markets. However,existing CMRS approaches often overlook the potential for shared preferencesamong users in different markets, focusing primarily on modeling specificpreferences within each market. In this paper, we argue that incorporating bothmarket-specific and market-shared insights can enhance the generalizability androbustness of CMRS. We propose a novel approach called Dual Prototype AttentiveGraph Network for Cross-Market Recommendation (DGRE) to address this. DGREleverages prototypes based on graph representation learning from both items andusers to capture market-specific and market-shared insights. Specifically, DGREincorporates market-shared prototypes by clustering users from various marketsto identify behavioural similarities and create market-shared user profiles.Additionally, it constructs item-side prototypes by aggregating item featureswithin each market, providing valuable market-specific insights. We conductextensive experiments to validate the effectiveness of DGRE on a real-worldcross-market dataset, and the results show that considering bothmarket-specific and market-sharing aspects in modelling can improve thegeneralization and robustness of CMRS.</description>
      <author>example@mail.com (Li Fan, Menglin Kong, Yang Xiang, Chong Zhang, Chengtao Ji)</author>
      <guid isPermaLink="false">2508.05969v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Algorithm for Estimating Intrinsic Geometry</title>
      <link>http://arxiv.org/abs/2508.06355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种量子算法，用于估计点云的局部内在维数和局部标量曲率，这些是几何数据分析中的关键量。该算法在处理具有成对几何距离的数据集时，能够高效地输出给定点的几何特性估计，相比经典算法实现了指数级加速，并且比现有量子算法在扩散映射方面也有显著改进。&lt;h4&gt;背景&lt;/h4&gt;高维数据集通常围绕低维流形聚类，但常常受到严重噪声的影响，这掩盖了下游学习任务所必需的内在几何结构。这种噪声干扰使得准确理解和分析数据的内在几何特性变得困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确估计数据内在几何特性的量子算法，特别是局部内在维数和局部标量曲率，这些量对于降维、特征提取和异常检测等核心任务至关重要，从而为各种数据驱动和数据辅助应用提供支持。&lt;h4&gt;方法&lt;/h4&gt;提出一种量子算法，该算法接受具有成对几何距离的数据集作为输入，输出给定点的局部维数和曲率的估计。该方法利用量子计算的优势，能够更高效地处理高维几何数据分析问题。&lt;h4&gt;主要发现&lt;/h4&gt;该量子算法相比其经典对应算法实现了指数级加速优势。此外，作为推论，研究将主要技术扩展到扩散映射，取得了比现有量子算法更显著的指数级改进。这表明量子计算在几何数据分析方面具有巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;这项研究标志着几何数据分析中高效量子应用的又一步进展，超越了传统的拓扑摘要方法，走向更精确的几何推断。它为量子增强的流形学习开辟了一条新颖的可扩展路径，有望在未来数据科学和机器学习中发挥重要作用。&lt;h4&gt;翻译&lt;/h4&gt;高维数据集通常围绕低维流形聚类，但也常常受到严重噪声的影响，掩盖了下游学习任务所必需的内在几何结构。我们提出了一种量子算法，用于估计点云的内在几何结构，特别是其局部内在维数和局部标量曲率。这些量对于降维、特征提取和异常检测至关重要，而这些任务是各种数据驱动和数据辅助应用的核心。在这项工作中，我们提出了一种量子算法，该算法接受具有成对几何距离的数据集，并输出给定点的局部维数和曲率的估计。我们证明这种量子算法比其经典对应算法具有指数级加速优势，并且作为推论，进一步将我们的主要技术扩展到扩散映射，取得了比现有量子算法更显著的指数级改进。我们的工作标志着几何数据分析中高效量子应用的又一步进展，超越了拓扑摘要，走向精确的几何推断，并为量子增强的流形学习开辟了一条新颖的可扩展路径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何利用量子算法高效估计点云数据的内在几何特性，特别是局部内在维度和局部标量曲率。这个问题在现实中很重要，因为高维数据通常围绕低维流形聚类但受噪声影响，准确估计这些几何特性对于降维、特征提取、异常检测等数据驱动任务至关重要，能帮助有效去噪、减少存储计算成本，同时发现数据中的异常结构。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从经典几何数据分析算法出发，特别是基于[9]中提出的经典估计器，并将其翻译到量子设置中。他们借鉴了扩散几何方法[18]从原始距离估计测地距离，利用块编码/量子奇异值变换框架[22-24]处理矩阵运算，并采用了高效的量子状态准备技术[32]。作者还参考了量子PCA技术[34,35]和已有的量子拓扑数据分析成果，将这些技术整合应用于几何数据分析这一相对未被探索的领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用量子计算的并行性和状态变换能力，通过块编码框架将经典几何算法转化为量子算法，实现指数级加速。整体流程包括：1)输入数据点和成对距离；2)构建核矩阵的块编码；3)估计测地距离；4)为给定点找最近邻；5)计算局部内在维度；6)估计采样密度；7)构建测地球并计算体积；8)拟合二次曲线；9)估计局部标量曲率。整个流程充分利用了量子状态准备和量子奇异值变换等技术处理大规模高维数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出估计点云内在几何特性的量子算法；2)证明相比经典算法具有指数级加速；3)将块编码框架应用于几何数据分析；4)技术扩展到扩散地图实现指数级改进；5)提供量子状态准备技术的有效应用。相比之前工作，该算法不需要访问某些矩阵的oracle，只需成对距离的经典值，复杂度从O(N³)或O(N² log³ N)降低到O(polylog(N, m))，专注于几何特性而非拓扑特性，为量子几何数据分析开辟了新路径。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种量子算法，能够以指数级速度超越经典方法估计高维点云数据的局部内在维度和曲率，为量子增强的流形学习开辟了一条新的可扩展路径。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional datasets typically cluster around lower-dimensionalmanifolds but are also often marred by severe noise, obscuring the intrinsicgeometry essential for downstream learning tasks. We present a quantumalgorithm for estimating the intrinsic geometry of a point cloud --specifically its local intrinsic dimension and local scalar curvature. Thesequantities are crucial for dimensionality reduction, feature extraction, andanomaly detection -- tasks that are central to a wide range of data-driven anddata-assisted applications. In this work, we propose a quantum algorithm whichtakes a dataset with pairwise geometric distance, output the estimation oflocal dimension and curvature at a given point. We demonstrate that thisquantum algorithm achieves an exponential speedup over its classicalcounterpart, and, as a corollary, further extend our main technique todiffusion maps, yielding exponential improvements even over existing quantumalgorithms. Our work marks another step toward efficient quantum applicationsin geometrical data analysis, moving beyond topological summaries towardprecise geometric inference and opening a novel, scalable path toquantum-enhanced manifold learning.</description>
      <author>example@mail.com (Nhat A. Nghiem, Tuan K. Do, Tzu-Chieh Wei, Trung V. Phan)</author>
      <guid isPermaLink="false">2508.06355v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.06203v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AnomalyMoE是一种基于专家混合(MoE)架构的新型通用异常检测框架，通过分层设计和专门的专家网络实现了比现有专业化方法更优越的性能。&lt;h4&gt;背景&lt;/h4&gt;异常检测是许多领域和模态中的关键任务，但现有方法通常高度专业化，限制了它们的泛化能力。这些专业化模型在部署到指定环境之外时表现有限。&lt;h4&gt;目的&lt;/h4&gt;克服现有异常检测方法的局限性，提出一个能够处理多种类型异常的通用框架。&lt;h4&gt;方法&lt;/h4&gt;AnomalyMoE将异常检测问题分解为三个语义层次：局部结构异常、组件级语义异常和全局逻辑异常，并在相应级别使用专门的专家网络。同时引入专家信息排斥(EIR)模块促进专家多样性和专家选择平衡(ESB)模块确保充分利用所有专家。&lt;h4&gt;主要发现&lt;/h4&gt;在8个涵盖工业成像、3D点云、医学成像、视频监控和逻辑异常检测的数据集上，AnomalyMoE建立了新的最先进性能，显著优于各自领域中的专业化方法。&lt;h4&gt;结论&lt;/h4&gt;AnomalyMoE通过分层设计和专门的专家网络，实现了单个模型能够同时理解和检测广泛异常的能力，是一种有效的通用异常检测解决方案。&lt;h4&gt;翻译&lt;/h4&gt;异常检测是众多领域和模态中的关键任务，但现有方法通常高度专业化，限制了它们的泛化能力。这些针对特定异常类型（如纹理缺陷或逻辑错误）定制的专业化模型，在部署到指定环境之外时通常表现有限。为克服这一局限，我们提出了AnomalyMoE，一种基于专家混合(MoE)架构的新型通用异常检测框架。我们的核心见解是将复杂的异常检测问题分解为三个不同的语义层次：局部结构异常、组件级语义异常和全局逻辑异常。AnomalyMoE相应地在补丁、组件和全局级别使用三个专门的专家网络，专门用于重建特征和识别指定语义级别的偏差。这种分层设计使单个模型能够同时理解和检测广泛的异常。此外，我们引入了专家信息排斥(EIR)模块以促进专家多样性，以及专家选择平衡(ESB)模块以确保充分利用所有专家。在8个涵盖工业成像、3D点云、医学成像、视频监控和逻辑异常检测的具有挑战性的数据集上的实验表明，AnomalyMoE建立了新的最先进性能，显著优于各自领域中的专业化方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection is a critical task across numerous domains and modalities,yet existing methods are often highly specialized, limiting theirgeneralizability. These specialized models, tailored for specific anomaly typeslike textural defects or logical errors, typically exhibit limited performancewhen deployed outside their designated contexts. To overcome this limitation,we propose AnomalyMoE, a novel and universal anomaly detection framework basedon a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose thecomplex anomaly detection problem into three distinct semantic hierarchies:local structural anomalies, component-level semantic anomalies, and globallogical anomalies. AnomalyMoE correspondingly employs three dedicated expertnetworks at the patch, component, and global levels, and is specialized inreconstructing features and identifying deviations at its designated semanticlevel. This hierarchical design allows a single model to concurrentlyunderstand and detect a wide spectrum of anomalies. Furthermore, we introducean Expert Information Repulsion (EIR) module to promote expert diversity and anExpert Selection Balancing (ESB) module to ensure the comprehensive utilizationof all experts. Experiments on 8 challenging datasets spanning industrialimaging, 3D point clouds, medical imaging, video surveillance, and logicalanomaly detection demonstrate that AnomalyMoE establishes new state-of-the-artperformance, significantly outperforming specialized methods in theirrespective domains.</description>
      <author>example@mail.com (Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Wei Ge, Ming Tang, Jinqiao Wang)</author>
      <guid isPermaLink="false">2508.06203v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Neural Radio Radiance Field for Localized Statistical Channel Modelling</title>
      <link>http://arxiv.org/abs/2508.06054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MM-LSCM，一种用于下一代网络优化的自监督多模态神经辐射场框架，用于局部统计信道建模。该框架整合RSRP数据和LiDAR点云信息，提高空间感知和预测准确性，采用自监督训练方法无需昂贵标记数据，实验表明其在信道重建精度和噪声鲁棒性上显著优于传统方法。&lt;h4&gt;背景&lt;/h4&gt;传统LSCM方法仅依赖RSRP数据，限制了对影响信号传播的环境结构的建模能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更准确建模环境结构对信号传播影响的LSCM方法，提高信道重建精度和对噪声的鲁棒性，用于下一代网络优化。&lt;h4&gt;方法&lt;/h4&gt;提出MM-LSCM框架，采用双分支神经网络架构整合RSRP数据和LiDAR点云信息，利用基于体积渲染的多模态合成对齐无线电传播与环境障碍物，采用自监督训练方法无需标记数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MM-LSCM在信道重建精度和对噪声的鲁棒性方面显著优于传统方法。&lt;h4&gt;结论&lt;/h4&gt;MM-LSCM是实际无线网络优化的一种有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了MM-LSCM，一种用于下一代网络优化的自监督多模态神经辐射场框架，用于局部统计信道建模。传统LSCM方法仅依赖RSRP数据，限制了其对影响信号传播的环境结构的建模能力。为解决这一问题，我们提出了一种双分支神经网络架构，整合RSRP数据和LiDAR点云信息，提高了空间感知和预测准确性。MM-LSCM利用基于体积渲染的多模态合成来对齐无线电传播与环境障碍物，并采用自监督训练方法，无需昂贵的标记数据。实验结果表明，MM-LSCM在信道重建精度和对噪声的鲁棒性方面显著优于传统方法，使其成为实际无线网络优化的一种有前景的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统局部统计信道建模方法仅依赖RSRP数据而缺乏环境感知能力的问题。这个问题在现实中很重要，因为随着新一代无线网络对无缝连接的需求增长，优化网络参数变得复杂，而传统方法要么成本高（如路测），要么缺乏环境细节（如统计模型），要么计算昂贵（如射线追踪），难以准确预测未探索区域的信道特性，影响网络优化效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统LSCM方法的局限性，然后考虑如何将环境信息融入信道建模。他们借鉴了NeRF在室内信道建模的潜力，以及多模态感知信息（特别是LiDAR点云与RSRP测量同时收集）的最新进展。方法设计上，他们创建了双分支神经网络架构分别处理点云数据和无线电信号，引入'停止概率'作为相干权重对齐无线电传播与环境障碍，并采用自监督训练避免昂贵的标记数据需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态融合将无线电RSRP数据与环境LiDAR点云数据结合，增强空间感知能力。整体流程包括：1) 环境特征设计，基于空间体素化处理3D点云识别障碍物；2) 多模态神经无线电辐射场网络，采用双分支架构分别建模命中概率和方向相关信号；3) 基于体积渲染的多模态合成，沿基站发出的射线查询网络获取信号和概率；4) 自监督训练，结合无线电模态损失和环境监督损失优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 专门针对LSCM的多模态神经无线电辐射网络；2) '停止概率'作为隐式相干权重对齐无线电传播与环境障碍；3) 自监督训练方法利用点云先验深度信息。相比之前工作，不同之处在于整合了RSRP和LiDAR点云信息，适用于更广泛场景而非仅限室内，利用环境数据监督提高预测能力，且在噪声环境下表现更鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了MM-LSCM，一种自监督多模态神经无线电辐射场框架，通过整合RSRP数据和LiDAR点云信息，显著提升了局部统计信道建模的准确性和鲁棒性，为下一代网络优化提供了有效解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents MM-LSCM, a self-supervised multi-modal neural radioradiance field framework for localized statistical channel modeling (LSCM) fornext-generation network optimization. Traditional LSCM methods rely solely onRSRP data, limiting their ability to model environmental structures that affectsignal propagation. To address this, we propose a dual-branch neuralarchitecture that integrates RSRP data and LiDAR point cloud information,enhancing spatial awareness and predictive accuracy. MM-LSCM leveragesvolume-rendering-based multi-modal synthesis to align radio propagation withenvironmental obstacles and employs a self-supervised training approach,eliminating the need for costly labeled data. Experimental results demonstratethat MM-LSCM significantly outperforms conventional methods in channelreconstruction accuracy and robustness to noise, making it a promising solutionfor real-world wireless network optimization.</description>
      <author>example@mail.com (Yiheng Wang, Shutao Zhang, Ye Xue, Tsung-Hui Chang)</author>
      <guid isPermaLink="false">2508.06054v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Neural Field Representations of Mobile Computational Photography</title>
      <link>http://arxiv.org/abs/2508.05907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何使用精心设计的神经场模型从移动摄影数据中直接实现深度估计、分层分离和图像拼接等功能，无需复杂预处理或标记数据。&lt;h4&gt;背景&lt;/h4&gt;移动成像在过去二十年经历了深刻变革，现代手机配备了多种成像技术和非视觉传感器，结合板载处理芯片成为多功能的口袋式计算成像平台。同时，神经场技术能够重建复杂场景而不需要显式的数据表示。&lt;h4&gt;目的&lt;/h4&gt;展示精心设计的神经场模型如何紧凑地表示复杂的几何和光照效果，并实现直接从野外移动摄影数据中应用深度估计、分层分离和图像拼接等功能。&lt;h4&gt;方法&lt;/h4&gt;使用精心设计、自正则化的神经场模型，通过随机梯度下降解决具有挑战性的逆问题，直接适配智能手机的原始测量数据。&lt;h4&gt;主要发现&lt;/h4&gt;这些方法优于最先进的方法，不依赖复杂的预处理步骤、标记的真实数据或机器学习先验。&lt;h4&gt;结论&lt;/h4&gt;神经场模型能够有效表示复杂的几何和光照效果，从移动摄影数据中直接应用各种图像处理任务是可行的，且不需要传统方法所需的预处理和标记数据。&lt;h4&gt;翻译&lt;/h4&gt;在过去的二十年里，移动成像经历了深刻的变革，手机迅速在普及性上超越了所有其他形式的数字摄影。如今的手机配备了各种成像技术——激光测距、多焦摄像头阵列和分像素传感器——以及非视觉传感器，如陀螺仪、加速度计和磁力计。结合用于图像和信号处理的板载集成芯片，使手机成为多功能的口袋式计算成像平台。与此同时，近年来我们见证了神经场如何通过小型神经网络训练将连续的空间输入坐标映射到输出信号，从而能够在没有显式数据表示（如像素阵列或点云）的情况下重建复杂场景。在本论文中，我展示了精心设计的神经场模型如何紧凑地表示复杂的几何和光照效果。实现了直接从收集的野外移动摄影数据中进行深度估计、分层分离和图像拼接等功能。这些方法优于最先进的方法，而不依赖复杂的预处理步骤、标记的真实数据或机器学习先验。相反，它们利用构建良好、自正则化的模型，通过随机梯度下降解决具有挑战性的逆问题，直接适配智能手机的原始测量数据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何利用神经场模型从移动设备采集的野外摄影数据中直接实现深度估计、层分离和图像拼接等应用问题。这个问题很重要，因为现代手机已成为多功能的口袋式计算成像平台，集成了多种传感器和图像处理芯片，能够产生大量高质量数据；同时，神经场技术能紧凑表示复杂几何和光照效果，为计算机视觉研究提供更丰富、多样化的数据集和问题空间，推动增强现实、物体理解等实际应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了移动计算摄影的现状和神经场技术的潜力，认识到手机摄影产生的数据量巨大但存储处理存在挑战。他借鉴了现有的神经辐射场(NeRF)和神经曲面等技术，这些技术通过小型神经网络将连续空间坐标映射到输出信号。作者设计了专门的神经场模型适应移动摄影特点，如使用密集神经场跟踪帧间像素运动估计深度，并通过最小化光度重投影损失实现端到端优化。整个设计过程基于对成像原理、传感器特性和计算挑战的深入理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用神经场模型紧凑表示复杂场景的几何和光照信息，从移动设备数据中直接实现视觉任务。整体流程包括：1) 使用智能手机采集长曝光序列的RAW图像、相机内参和陀螺仪数据；2) 设计神经RGB-D场景拟合模型，包含深度和姿态的显式几何投影；3) 通过最小化光度重投影损失，将模型拟合到数据中联合估计深度和相机姿态；4) 利用估计信息实现深度估计、层分离和图像拼接等应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 端到端的神经RGB-D场景拟合方法，从不稳定长时间曝光摄影中提取高保真深度和姿态估计，无需深度初始化；2) 开发智能手机数据采集应用，获取RAW图像和传感器数据；3) 改进神经场模型，使用多分辨率哈希网格编码提高训练速度和重建质量；4) 全面实验验证，优于现有方法并与高精度扫描对比。相比之前工作，不同之处在于无需依赖LiDAR等初始深度测量，使用更紧凑的前向投影模型，且应用范围更广，不仅限于深度估计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 作者提出了一种基于神经场的端到端方法，能够从移动设备采集的野外摄影数据中直接实现高质量深度估计、层分离和图像拼接，无需复杂预处理或标记数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past two decades, mobile imaging has experienced a profoundtransformation, with cell phones rapidly eclipsing all other forms of digitalphotography in popularity. Today's cell phones are equipped with a diverserange of imaging technologies - laser depth ranging, multi-focal camera arrays,and split-pixel sensors - alongside non-visual sensors such as gyroscopes,accelerometers, and magnetometers. This, combined with on-board integratedchips for image and signal processing, makes the cell phone a versatilepocket-sized computational imaging platform. Parallel to this, we have seen inrecent years how neural fields - small neural networks trained to mapcontinuous spatial input coordinates to output signals - enable thereconstruction of complex scenes without explicit data representations such aspixel arrays or point clouds. In this thesis, I demonstrate how carefullydesigned neural field models can compactly represent complex geometry andlighting effects. Enabling applications such as depth estimation, layerseparation, and image stitching directly from collected in-the-wild mobilephotography data. These methods outperform state-of-the-art approaches withoutrelying on complex pre-processing steps, labeled ground truth data, or machinelearning priors. Instead, they leverage well-constructed, self-regularizedmodels that tackle challenging inverse problems through stochastic gradientdescent, fitting directly to raw measurements from a smartphone.</description>
      <author>example@mail.com (Ilya Chugunov)</author>
      <guid isPermaLink="false">2508.05907v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Situationally-aware Path Planning Exploiting 3D Scene Graphs</title>
      <link>http://arxiv.org/abs/2508.06283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;S-Path是一种情境感知的路径规划器，利用室内3D场景图的度量-语义结构显著提高规划效率，实现平均5.7倍的规划时间减少，同时保持路径最优性，在复杂场景中表现更优。&lt;h4&gt;背景&lt;/h4&gt;3D场景图整合了度量信息和语义信息，但其结构在提高路径规划效率和可解释性方面未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;提出S-Path，一个情境感知的路径规划器，利用室内3D场景图的度量-语义结构来显著提高规划效率。&lt;h4&gt;方法&lt;/h4&gt;S-Path采用两阶段过程：首先在从场景图派生的语义图上进行搜索，产生人类可理解的高层次路径并识别相关规划区域，将问题分解为更小、独立的子问题并行解决；引入重新规划机制，在路径不可行时重用已解决子问题的信息更新语义启发式，优先重用以提高未来规划效率。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界和模拟环境中的广泛实验表明，S-Path实现了平均5.7倍的规划时间减少，保持与经典基于采样的规划器相当的路径最优性，并在复杂场景中优于经典方法。&lt;h4&gt;结论&lt;/h4&gt;S-Path是针对由室内3D场景图表示的环境的高效且可解释的路径规划器。&lt;h4&gt;翻译&lt;/h4&gt;3D场景图整合了度量和语义信息，但它们的结构在提高路径规划效率和可解释性方面仍未得到充分利用。在这项工作中，我们提出了S-Path，一个情境感知的路径规划器，它利用室内3D场景图的度量-语义结构来显著提高规划效率。S-Path遵循两阶段过程：它首先在从场景图派生的语义图上进行搜索，产生人类可理解的高层次路径。这还确定了规划的相关区域，随后允许将问题分解为更小、独立的子问题，可以并行解决。我们还引入了一种重新规划机制，在路径不可行的情况下，重用先前已解决子问题的信息来更新语义启发式，并优先重用以进一步提高未来规划尝试的效率。在真实世界和模拟环境中的广泛实验表明，S-Path实现了平均5.7倍的规划时间减少，同时保持与经典基于采样的规划器相当的路径最优性，并在复杂场景中超越它们，使其成为由室内3D场景图表示的环境的高效且可解释的路径规划器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统室内机器人导航路径规划器的效率问题。传统规划器依赖密集采样配置空间，计算成本随环境复杂度快速增长，限制了效率和实用性。这个问题在现实中很重要，因为实时应用（如服务机器人、自动驾驶）需要快速响应，大型复杂环境中传统方法计算资源需求过高，且缺乏语义理解能力，难以与人类直观交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到3D场景图集成了度量和语义信息，但结构未被充分利用于路径规划。他们设计了两阶段过程：先在语义图上搜索人类可理解的高层次路径，再分解问题为可并行解决的子问题。借鉴了语义感知路径规划和3D场景图在任务规划中的应用，但将其扩展到低级路径规划，并特别参考了Ray等人的工作，同时专注于效率提升和并行处理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D场景图的层次结构（房间、门、墙等）将全局规划分解为更小、独立的子问题，通过并行计算和重用已解决的子问题提高效率。流程：1)输入3D场景图和度量网格；2)环境设置，生成语义图和轮廓；3)语义规划，执行A*搜索得到高层次路径；4)子问题生成，将路径分解为独立子问题；5)几何规划，并行解决各子问题；6)缝合结果形成最终路径；7)必要时重规划，重用已解决子问题。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)充分利用3D场景图的层次结构提高效率和可解释性；2)问题分解与并行处理，显著减少规划时间；3)高效重规划机制，重用子问题信息；4)生成人类可解释的基于语义概念的路径。相比之前工作：传统采样方法效率低；其他语义方法依赖扁平表示，扩展性差；现有场景图应用主要关注任务规划而非路径规划；相比Ray等人工作，S-Path直接利用语义层次结构，专注效率与并行性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; S-Path通过利用3D场景图的层次结构将路径规划分解为可并行子问题，将室内机器人导航的规划效率提高了5.7倍，同时保持路径质量并提供人类可解释的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Scene Graphs integrate both metric and semantic information, yet theirstructure remains underutilized for improving path planning efficiency andinterpretability. In this work, we present S-Path, a situationally-aware pathplanner that leverages the metric-semantic structure of indoor 3D Scene Graphsto significantly enhance planning efficiency. S-Path follows a two-stageprocess: it first performs a search over a semantic graph derived from thescene graph to yield a human-understandable high-level path. This alsoidentifies relevant regions for planning, which later allows the decompositionof the problem into smaller, independent subproblems that can be solved inparallel. We also introduce a replanning mechanism that, in the event of aninfeasible path, reuses information from previously solved subproblems toupdate semantic heuristics and prioritize reuse to further improve theefficiency of future planning attempts. Extensive experiments on bothreal-world and simulated environments show that S-Path achieves averagereductions of 5.7x in planning time while maintaining comparable pathoptimality to classical sampling-based planners and surpassing them in complexscenarios, making it an efficient and interpretable path planner forenvironments represented by indoor 3D Scene Graphs.</description>
      <author>example@mail.com (Saad Ejaz, Marco Giberna, Muhammad Shaheer, Jose Andres Millan-Romera, Ali Tourani, Paul Kremer, Holger Voos, Jose Luis Sanchez-Lopez)</author>
      <guid isPermaLink="false">2508.06283v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</title>
      <link>http://arxiv.org/abs/2508.06259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了SIFThinker，一种空间感知的'图像思维'框架，通过交替使用深度增强的边界框和自然语言来实现注意力校正和图像区域聚焦，解决了现有多模态大语言模型在复杂视觉任务中的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态大语言模型在复杂视觉任务（如空间理解、细粒度感知）方面仍面临重大挑战。先前的方法尝试整合视觉推理，但未能利用空间线索进行注意力校正，从而迭代地优化其对提示相关区域的关注。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够模拟人类视觉感知的空间感知框架，通过注意力校正和图像区域聚焦来提高模型在空间理解和细粒度视觉感知方面的能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了SIFThinker框架，包括两个主要贡献：一是引入反向扩展-前向推理策略，生成交互式的图像-思维链用于过程级监督，从而构建了SIF-50K数据集；二是提出GRPO-SIF训练范式，将深度感知的视觉定位集成到统一的推理流程中，教导模型动态校正和关注提示相关区域。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，SIFThinker在空间理解和细粒度视觉感知方面优于最先进的方法，同时保持强大的通用能力，突显了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;SIFThinker通过空间感知的'图像思维'框架和创新的训练方法，成功解决了多模态大语言模型在复杂视觉任务中的局限性，特别是在空间理解和细粒度感知方面取得了显著进展。&lt;h4&gt;翻译&lt;/h4&gt;当前的多模态大语言模型在复杂的视觉任务（例如，空间理解、细粒度感知）方面仍面临重大挑战。先前的方法尝试整合视觉推理，然而它们未能利用空间线索进行注意力校正，从而迭代地优化其对提示相关区域的关注。在本文中，我们介绍了SIFThinker，一种空间感知的'图像思维'框架，模拟人类视觉感知。具体而言，SIFThinker通过交替使用深度增强的边界框和自然语言来实现注意力校正和图像区域聚焦。我们的贡献有两方面：首先，我们引入了反向扩展-前向推理策略，促进了用于过程级监督的交互式图像-思维链的生成，进而促进了SIF-50K数据集的构建。此外，我们提出了GRPO-SIF，一种将深度感知的视觉定位集成到统一推理流程中的强化训练范式，教导模型动态校正和关注提示相关区域。大量实验表明，SIFThinker在空间理解和细粒度视觉感知方面优于最先进的方法，同时保持强大的通用能力，突显了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current multimodal large language models (MLLMs) still face significantchallenges in complex visual tasks (e.g., spatial understanding, fine-grainedperception). Prior methods have tried to incorporate visual reasoning, however,they fail to leverage attention correction with spatial cues to iterativelyrefine their focus on prompt-relevant regions. In this paper, we introduceSIFThinker, a spatially-aware "think-with-images" framework that mimics humanvisual perception. Specifically, SIFThinker enables attention correcting andimage region focusing by interleaving depth-enhanced bounding boxes and naturallanguage. Our contributions are twofold: First, we introduce areverse-expansion-forward-inference strategy that facilitates the generation ofinterleaved image-text chains of thought for process-level supervision, whichin turn leads to the construction of the SIF-50K dataset. Besides, we proposeGRPO-SIF, a reinforced training paradigm that integrates depth-informed visualgrounding into a unified reasoning pipeline, teaching the model to dynamicallycorrect and focus on prompt-relevant regions. Extensive experiments demonstratethat SIFThinker outperforms state-of-the-art methods in spatial understandingand fine-grained visual perception, while maintaining strong generalcapabilities, highlighting the effectiveness of our method.</description>
      <author>example@mail.com (Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang)</author>
      <guid isPermaLink="false">2508.06259v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines</title>
      <link>http://arxiv.org/abs/2508.06226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了GeoLaux基准测试，用于评估多模态大语言模型(MLLMs)在几何问题解决方面的长步骤推理和辅助线构造能力，通过13个主流模型的实验发现了模型在长步骤推理、证明问题处理和辅助线意识方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;几何问题解决需要模型掌握图表理解、逻辑推理、知识应用、数值计算和辅助线构造等多方面能力，这对多模态大语言模型构成重大挑战。然而现有评估基准忽略了辅助线构造，缺乏细粒度过程评估，不足以评估长步骤推理能力。&lt;h4&gt;目的&lt;/h4&gt;填补现有评估基准的空白，提出一个全面评估MLLMs几何问题解决能力特别是长步骤推理和辅助线构造能力的基准测试。&lt;h4&gt;方法&lt;/h4&gt;构建包含2,186个几何问题的数据集(平均6.51个推理步骤，最多24步，41.8%需辅助线构造)，设计五维度评估策略(答案正确性、过程正确性、过程质量、辅助线影响和错误原因)，对13个主流MLLMs进行实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;1)模型在长步骤推理中性能明显下降(九个模型性能下降超50%)；2)与计算问题相比，MLLMs解决证明问题时倾向于走捷径；3)模型缺乏辅助线意识，增强此能力对几何推理改进特别有益。&lt;h4&gt;结论&lt;/h4&gt;GeoLaux作为评估MLLMs长步骤几何推理和辅助线能力的基准，同时也是能力提升的指导。数据集和代码将随补充材料公开发布。&lt;h4&gt;翻译&lt;/h4&gt;几何问题解决需要模型掌握图表理解、逻辑推理、知识应用、数值计算和辅助线构造。这对多模态大语言模型构成重大挑战。然而现有评估基准忽略了辅助线构造，缺乏细粒度过程评估，不足以评估长步骤推理能力。为填补空白，我们提出GeoLaux基准测试，包含2,186个几何问题，既有计算题也有证明题，平均需6.51个推理步骤(最多24步)，其中41.8%需辅助线构造。基于此数据集，我们设计五维度评估策略，评估13个主流MLLMs后得出：模型在长步骤推理中性能明显下降；解决证明问题时倾向于走捷径；缺乏辅助线意识，增强此能力特别有益于几何推理改进。这些发现确立了GeoLaux作为评估基准和指导。数据集和代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometry problem solving (GPS) requires models to master diagramcomprehension, logical reasoning, knowledge application, numerical computation,and auxiliary line construction. This presents a significant challenge forMultimodal Large Language Models (MLLMs). However, existing benchmarks forevaluating MLLM geometry skills overlook auxiliary line construction and lackfine-grained process evaluation, making them insufficient for assessing MLLMs'long-step reasoning abilities. To bridge these gaps, we present the GeoLauxbenchmark, comprising 2,186 geometry problems, incorporating both calculationand proving questions. Notably, the problems require an average of 6.51reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliaryline construction. Building on the dataset, we design a novel five-dimensionalevaluation strategy assessing answer correctness, process correctness, processquality, auxiliary line impact, and error causes. Extensive experiments on 13leading MLLMs (including thinking models and non-thinking models) yield threepivotal findings: First, models exhibit substantial performance degradation inextended reasoning steps (nine models demonstrate over 50% performance drop).Second, compared to calculation problems, MLLMs tend to take shortcuts whensolving proving problems. Third, models lack auxiliary line awareness, andenhancing this capability proves particularly beneficial for overall geometryreasoning improvement. These findings establish GeoLaux as both a benchmark forevaluating MLLMs' long-step geometric reasoning with auxiliary lines and aguide for capability advancement. Our dataset and code are included insupplementary materials and will be released.</description>
      <author>example@mail.com (Yumeng Fu, Jiayin Zhu, Lingling Zhang, Bo Zhao, Shaoxuan Ma, Yushun Zhang, Yanrui Wu, Wenjun Wu)</author>
      <guid isPermaLink="false">2508.06226v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments</title>
      <link>http://arxiv.org/abs/2508.05852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种视觉-语言框架，通过自然语言描述驾驶员视线变化，使用少样本和零样本学习在单张RGB图像上进行驾驶员视觉注意力预测。&lt;h4&gt;背景&lt;/h4&gt;驾驶员视觉注意力预测是自动驾驶和人机交互研究中的关键任务。大多数先前的研究集中在估计单一时间点的注意力分配，通常使用静态RGB图像如驾驶场景图片。&lt;h4&gt;目的&lt;/h4&gt;提出一种视觉-语言框架，通过自然语言建模驾驶员视线的变化，提高注意力预测的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;从BDD-A数据集收集并完善高质量标题，使用人在回路反馈；微调LLaVA模型使视觉感知与以注意力为中心的场景理解保持一致；整合底层线索和自上而下的上下文；使用基于语言的注视行为描述；在不同训练机制下评估性能；引入特定领域指标进行语义对齐和响应多样性评估。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型在注意力转移检测和可解释性方面优于通用视觉语言模型；这是首次尝试用自然语言生成驾驶员视觉注意力分配和转移预测的研究之一。&lt;h4&gt;结论&lt;/h4&gt;该方法为自动驾驶中的可解释AI提供了新方向，并为行为预测、人机协作和多智能体协调等下游任务奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;驾驶员视觉注意力预测是自动驾驶和人机交互研究中的一个关键任务。大多数先前的研究集中在估计单一时间点的注意力分配，通常使用静态RGB图像如驾驶场景图片。在这项工作中，我们提出了一种视觉-语言框架，通过自然语言建模驾驶员视线的动态变化，在单张RGB图像上使用少样本和零样本学习。我们使用人在回路反馈从BDD-A数据集中收集并完善高质量标题，然后微调LLaVA以使视觉感知与以注意力为中心的场景理解保持一致。我们的方法整合了底层线索和自上而下的上下文（例如，路线语义、风险预期），实现了基于语言的注视行为描述。我们在不同的训练机制（少样本和单样本）下评估性能，并引入了特定领域的指标进行语义对齐和响应多样性评估。结果表明，我们的微调模型在注意力转移检测和可解释性方面优于通用视觉语言模型。据我们所知，这是首次尝试用自然语言生成驾驶员视觉注意力分配和转移预测的研究之一，为自动驾驶中的可解释AI提供了新方向。我们的方法为行为预测、人机协作和多智能体协调等下游任务提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driver visual attention prediction is a critical task in autonomous drivingand human-computer interaction (HCI) research. Most prior studies focus onestimating attention allocation at a single moment in time, typically usingstatic RGB images such as driving scene pictures. In this work, we propose avision-language framework that models the changing landscape of drivers' gazethrough natural language, using few-shot and zero-shot learning on single RGBimages. We curate and refine high-quality captions from the BDD-A dataset usinghuman-in-the-loop feedback, then fine-tune LLaVA to align visual perceptionwith attention-centric scene understanding. Our approach integrates bothlow-level cues and top-down context (e.g., route semantics, risk anticipation),enabling language-based descriptions of gaze behavior. We evaluate performanceacross training regimes (few shot, and one-shot) and introduce domain-specificmetrics for semantic alignment and response diversity. Results show that ourfine-tuned model outperforms general-purpose VLMs in attention shift detectionand interpretability. To our knowledge, this is among the first attempts togenerate driver visual attention allocation and shifting predictions in naturallanguage, offering a new direction for explainable AI in autonomous driving.Our approach provides a foundation for downstream tasks such as behaviorforecasting, human-AI teaming, and multi-agent coordination.</description>
      <author>example@mail.com (Kaiser Hamid, Khandakar Ashrafi Akbar, Nade Liang)</author>
      <guid isPermaLink="false">2508.05852v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging transfer learning for accurate estimation of ionic migration barriers in solids</title>
      <link>http://arxiv.org/abs/2508.06436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络的迁移学习方法，用于准确预测材料中的离子迁移势垒，显著提高了预测精度，为电池等应用中的材料发现提供了有力工具。&lt;h4&gt;背景&lt;/h4&gt;离子迁移率决定电池、燃料电池和电化学传感器等应用的性能表现，它与迁移势垒呈指数关系，而迁移势垒难以测量和计算。以往方法依赖不精确描述符，缺乏可推广的预测模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效准确地预测不同材料中迁移势垒的方法，促进高性能离子导体的发现。&lt;h4&gt;方法&lt;/h4&gt;构建基于图神经网络的架构，采用迁移学习策略：先在七种不同体性质上预训练模型(MPT)，然后修改模型以分类结构中的不同迁移路径，最后在619个手动整理的密度泛函理论计算数据集上进行微调(FT)。&lt;h4&gt;主要发现&lt;/h4&gt;最佳性能的FT模型(MODEL-3)在测试集上达到0.703的R²得分和0.261 eV的平均绝对误差，显著优于传统方法；能区分结构中的不同迁移路径，具有优秀的跨成分和化学性质的泛化能力；作为分类器，在识别'良好'离子导体方面达到80%准确率和82.8%精确率。&lt;h4&gt;结论&lt;/h4&gt;有效利用迁移学习策略和架构修改可进行快速准确的迁移势垒预测，对电池材料发现和数据稀缺材料性质预测具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;离子迁移率决定了电池、燃料电池和电化学传感器等应用的性能表现，并与迁移势垒呈指数相关，而迁移势垒是一个难以测量/计算的量。以往识别高离子迁移率材料的方法依赖于不精确的描述符，因为缺乏可推广的模型来预测迁移势垒。本研究提出了一种基于图神经网络的架构，利用迁移学习原理来高效准确地预测不同材料中的迁移势垒。我们在七种不同体性质上预训练模型，修改模型以分类结构中的不同迁移路径，并在包含619个迁移势垒数据点(使用密度泛函理论计算)的数据集上进行微调。我们的最佳性能模型(MODEL-3)在预测准确性上比传统方法有显著提高，测试集上R²得分为0.703，平均绝对误差为0.261 eV。MODEL-3能区分结构中的不同迁移路径，并在嵌入物成分和化学性质方面表现出优秀的泛化能力。作为分类器，它在识别'良好'离子导体方面达到80%准确率和82.8%精确率。因此，我们的工作展示了有效利用迁移学习策略进行快速准确迁移势垒预测的方法，对电池材料发现和其他数据稀缺材料性质预测具有重要价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ionic mobility determines the rate performance of several applications, suchas batteries, fuel cells, and electrochemical sensors and is exponentiallydependent on the migration barrier ($E_m$), a difficult to measure/calculatequantity. Previous approaches to identify materials with high ionic mobilityhave relied on imprecise descriptors given the lack of generalizable models topredict $E_m$. Here, we present a graph neural network based architecture thatleverages principles of transfer learning to efficiently and accurately predict$E_m$ across a diverse set of materials. We use a model pre-trainedsimultaneously on seven distinct bulk properties (labeled MPT), modify the MPTmodel to classify different migration pathways in a structure, and fine-tune(FT) on a manually-curated literature-derived dataset of 619 $E_m$ data pointscalculated with density functional theory. Importantly, our best-performing FTmodel (labeled MODEL-3) demonstrates substantial improvements in predictionaccuracy compared to classical machine learning methods, graph models trainedfrom scratch, and a universal machine learned interatomic potential, with aR$^2$ score of 0.703 and a mean absolute error of 0.261 eV on the test set.Notably, MODEL-3 is able to distinguish different migration pathways within astructure and also demonstrates excellent ability to generalize acrossintercalant compositions and chemistries. As a classifier, MODEL-3 exhibits80\% accuracy and 82.8\% precision in identifying materials that are `good'ionic conductors (i.e., structures with $E_m &lt;$0.65~eV). Thus, our workdemonstrates the effective use of FT strategies and architectural modificationsnecessary for making swift and accurate $E_m$ predictions, which will be usefulfor materials discovery in batteries and for predicting other data-scarcematerial properties.</description>
      <author>example@mail.com (Reshma Devi, Keith T. Butler, Gopalakrishnan Sai Gautam)</author>
      <guid isPermaLink="false">2508.06436v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors</title>
      <link>http://arxiv.org/abs/2508.06257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GTMancer的图变换器框架，用于多组学癌症亚型分类，通过对比学习和双重注意力机制有效整合多组学数据，在七个真实世界癌症数据集上展现出优于现有最先进算法的性能。&lt;h4&gt;背景&lt;/h4&gt;整合多组学数据可通过数据驱动分析理解复杂生物过程，特别是癌症；图神经网络在利用生物数据关系结构方面表现出色；现有方法常忽略异构组学间的复杂耦合，限制了它们解决癌症亚型异质性的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在癌症亚型分类中的局限性，提出一种能够更好整合多组学数据并捕捉癌症亚型异质性的框架。&lt;h4&gt;方法&lt;/h4&gt;提出GTMancer框架，基于GNN优化问题并扩展到复杂多组学数据；利用对比学习将多组学数据嵌入统一语义空间；在统一空间中展开多图优化问题；引入双重注意力系数集合捕获多组学数据内部和之间的结构图先验，使全局组学信息指导个体组学表示的精炼。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实世界癌症数据集上的实验表明，GTMancer优于现有的最先进算法。&lt;h4&gt;结论&lt;/h4&gt;GTMancer框架有效解决了多组学整合和癌症亚型分类中的关键挑战，为精准肿瘤学提供了新工具。&lt;h4&gt;翻译&lt;/h4&gt;通过数据驱动分析整合多组学数据可以全面理解各种疾病特别是癌症背后的复杂生物过程。图神经网络最近展现出了利用生物数据中关系结构的显著能力，推动了多组学整合在癌症亚型分类中的进展。现有方法常常忽略异构组学之间的复杂耦合，限制了它们解决对精准肿瘤学至关重要的细微癌症亚型异质性的能力。为解决这些局限性，我们提出一个名为GTMancer（用于多组学癌症亚型分类的图变换器）的框架。该框架建立在GNN优化问题基础上，并将其应用扩展到复杂多组学数据。具体而言，我们的方法利用对比学习将多组学数据嵌入统一的语义空间。我们在该统一空间中展开多图优化问题，并引入双重注意力系数集合来捕获多组学数据内部和之间的结构图先验。这种方法使全局组学信息能够指导个体组学表示的精炼。在七个真实世界癌症数据集上的实证实验表明，GTMancer优于现有的最先进算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating multi-omics datasets through data-driven analysis offers acomprehensive understanding of the complex biological processes underlyingvarious diseases, particularly cancer. Graph Neural Networks (GNNs) haverecently demonstrated remarkable ability to exploit relational structures inbiological data, enabling advances in multi-omics integration for cancersubtype classification. Existing approaches often neglect the intricatecoupling between heterogeneous omics, limiting their capacity to resolve subtlecancer subtype heterogeneity critical for precision oncology. To address theselimitations, we propose a framework named Graph Transformer for Multi-omicsCancer Subtype Classification (GTMancer). This framework builds upon the GNNoptimization problem and extends its application to complex multi-omics data.Specifically, our method leverages contrastive learning to embed multi-omicsdata into a unified semantic space. We unroll the multiplex graph optimizationproblem in that unified space and introduce dual sets of attention coefficientsto capture structural graph priors both within and among multi-omics data. Thisapproach enables global omics information to guide the refining of therepresentations of individual omics. Empirical experiments on seven real-worldcancer datasets demonstrate that GTMancer outperforms existing state-of-the-artalgorithms.</description>
      <author>example@mail.com (Jielong Lu, Zhihao Wu, Jiajun Yu, Jiajun Bu, Haishuai Wang)</author>
      <guid isPermaLink="false">2508.06257v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor</title>
      <link>http://arxiv.org/abs/2508.06177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 28th RoboCup International Symposium, Salvador, Brasil&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的机器人定位框架，利用地板特性和图卷积网络实现高精度定位&lt;h4&gt;背景&lt;/h4&gt;机器人导航中的精确定位是一个基本挑战，传统方法如基于Lidar或QR码的系统在复杂环境中存在可扩展性和适应性限制&lt;h4&gt;目的&lt;/h4&gt;提出一种创新的定位框架，利用地板特性和基于图的表示以及图卷积网络(GCNs)来提高机器人定位的准确性和效率&lt;h4&gt;方法&lt;/h4&gt;使用图来表示地板特征，而不是比较单个图像特征，实现更准确（误差0.64厘米）和更高效的机器人定位&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在每一帧中成功解决机器人被绑架的问题，无需复杂的过滤过程&lt;h4&gt;结论&lt;/h4&gt;这些进步为机器人在各种环境中的导航开辟了新的可能性&lt;h4&gt;翻译&lt;/h4&gt;准确的定位代表了机器人导航中的一个基本挑战。传统方法，如基于Lidar或QR码的系统，在复杂环境中存在固有的可扩展性和适应性限制。在这项工作中，我们提出了一种创新的定位框架，利用地板特性，采用基于图的表示和图卷积网络(GCNs)。我们的方法使用图来表示地板特征，这有助于比比较单个图像特征更准确（0.64厘米误差）和更高效地定位机器人。此外，这种方法能够在每一帧中成功解决机器人被绑架的问题，而无需复杂的过滤过程。这些进步为机器人在各种环境中的导航开辟了新的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization represents a fundamental challenge in  robotic navigation. Traditional methodologies, such as Lidar or QR-code basedsystems, suffer from inherent scalability and adaptability con straints,particularly in complex environments. In this work, we propose  an innovative localization framework that harnesses flooring characteris ticsby employing graph-based representations and Graph Convolutional  Networks (GCNs). Our method uses graphs to represent floor features,  which helps localize the robot more accurately (0.64cm error) and more  efficiently than comparing individual image features. Additionally, this  approach successfully addresses the kidnapped robot problem in every  frame without requiring complex filtering processes. These advancements  open up new possibilities for robotic navigation in diverse environments.</description>
      <author>example@mail.com (Dominik Brämer, Diana Kleingarn, Oliver Urbann)</author>
      <guid isPermaLink="false">2508.06177v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification</title>
      <link>http://arxiv.org/abs/2508.06118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于集成方法的图表示方法，用于功能磁共振成像数据的脑状态分类。通过多个基础机器学习模型构建图，边的权重反映认知状态间后验概率差异，在七个认知任务中实现了97.07%到99.74%的高分类准确率，优于传统相关图方法。&lt;h4&gt;背景&lt;/h4&gt;基于神经影像数据理解和分类人类认知脑状态是神经科学中最具挑战性的问题之一，主要由于信号具有高维度和内在噪声特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于集成方法的图表示方法，用于功能磁共振成像(fMRI)数据的二进制脑状态分类任务。&lt;h4&gt;方法&lt;/h4&gt;通过利用多个基础机器学习模型构建图，每条边的权重反映两个认知状态间后验概率差异（取值范围[-1, 1]），编码对给定状态的置信度。将方法应用于人类连接组计划的七个认知任务，仅使用图的平均入射边权重作为特征，并与基于经典相关的图在图神经网络分类任务中进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;在所有实验中，集成图获得了最高的分类准确率。结果表明集成图传递了更丰富的拓扑信息并增强了脑状态区分能力。&lt;h4&gt;结论&lt;/h4&gt;该方法保留了fMRI图表示的边级可解释性，可适应多类和回归任务，并可扩展到其他神经成像模态和病理状态分类。&lt;h4&gt;翻译&lt;/h4&gt;理解和分类基于神经影像数据的人类认知脑状态仍然是神经科学中最重要且最具挑战性的问题之一，这归因于信号的高维度和内在噪声。在这项工作中，我们提出了一种基于集成方法的图表示方法，用于功能磁共振成像(fMRI)数据的二进制脑状态分类任务。我们的方法通过利用多个基础机器学习模型构建图：每条边的权重反映了两个认知状态之间后验概率的差异，产生范围在[-1, 1]内的值，编码了对给定状态的置信度。我们将这种方法应用于人类连接组计划(HCP 1200受试者发布)中的七个认知任务，包括工作记忆、赌博、运动活动、语言、社会认知、关系处理和情绪处理。仅使用图的平均入射边权重作为特征，一个简单的逻辑回归分类器实现了97.07%到99.74%的平均准确率。我们还在图神经网络(GNN)的分类任务中将我们的集成图与基于经典相关的图进行了比较。在所有实验中，集成图获得了最高的分类准确率。这些结果表明集成图传递了更丰富的拓扑信息并增强了脑状态区分能力。我们的方法保留了fMRI图表示的边级可解释性，可适应多类和回归任务，并可扩展到其他神经成像模态和病理状态分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and classifying human cognitive brain states based onneuroimaging data remains one of the foremost and most challenging problems inneuroscience, owing to the high dimensionality and intrinsic noise of thesignals. In this work, we propose an ensemble-based graph representation methodof functional magnetic resonance imaging (fMRI) data for the task of binarybrain-state classification. Our method builds the graph by leveraging multiplebase machine-learning models: each edge weight reflects the difference inposterior probabilities between two cognitive states, yielding values in therange [-1, 1] that encode confidence in a given state. We applied this approachto seven cognitive tasks from the Human Connectome Project (HCP 1200 SubjectRelease), including working memory, gambling, motor activity, language, socialcognition, relational processing, and emotion processing. Using only the meanincident edge weights of the graphs as features, a simple logistic-regressionclassifier achieved average accuracies from 97.07% to 99.74%. We also comparedour ensemble graphs with classical correlation-based graphs in a classificationtask with a graph neural network (GNN). In all experiments, the highestclassification accuracy was obtained with ensemble graphs. These resultsdemonstrate that ensemble graphs convey richer topological information andenhance brain-state discrimination. Our approach preserves edge-levelinterpretability of the fMRI graph representation, is adaptable to multiclassand regression tasks, and can be extended to other neuroimaging modalities andpathological-state classification.</description>
      <author>example@mail.com (Daniil Vlasenko, Vadim Ushakov, Alexey Zaikin, Denis Zakharov)</author>
      <guid isPermaLink="false">2508.06118v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2</title>
      <link>http://arxiv.org/abs/2508.06091v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文解决了图神经网络逻辑表达能力的一个开放性问题，证明了aggregate-combine-readout GNNs的逻辑表达能力严格超过了C2逻辑。&lt;h4&gt;背景&lt;/h4&gt;近年来，研究者们对通过逻辑语言理解图神经网络(GNNs)的表达能力越来越感兴趣。Barceló等人(2020)的开创性研究表明，分级模态逻辑(或C2逻辑的守护片段)刻画了aggregate-combine GNNs的逻辑表达能力。他们将'全C2是否刻画了aggregate-combine-readout GNNs的逻辑表达能力'作为一个'具有挑战性的开放问题'提出，尽管有多次尝试，这个问题仍未解决。&lt;h4&gt;目的&lt;/h4&gt;解决Barceló等人提出的开放性问题，即确定全C2逻辑是否刻画了aggregate-combine-readout GNNs的逻辑表达能力。&lt;h4&gt;方法&lt;/h4&gt;通过证明的方法，展示了aggregate-combine-readout GNNs的逻辑表达能力严格超过了C2逻辑。这一结果在无向图和有向图上都成立。&lt;h4&gt;主要发现&lt;/h4&gt;aggregate-combine-readout GNNs的逻辑表达能力严格超过了C2逻辑，无论是在无向图还是有向图上都是如此。&lt;h4&gt;结论&lt;/h4&gt;解决了GNNs逻辑表达能力领域的一个重要开放性问题，同时也为无穷逻辑的表达能力提供了纯逻辑上的见解。&lt;h4&gt;翻译&lt;/h4&gt;近年来，人们越来越有兴趣通过逻辑语言来理解图神经网络(GNNs)的表达能力。这项研究由Barceló等人(2020)的一个有影响力的结果开创，他们证明了分级模态逻辑(或C2逻辑的守护片段)刻画了aggregate-combine GNNs的逻辑表达能力。他们留下了一个'具有挑战性的开放问题'，即全C2是否刻画了aggregate-combine-readout GNNs的逻辑表达能力。尽管有多次尝试，这个问题仍未得到解决。在本文中，我们通过证明aggregate-combine-readout GNNs的逻辑表达能力严格超过C2，解决了上述开放问题。这一结果在无向图和有向图上都成立。除了对GNNs的启示外，我们的工作还为无穷逻辑的表达能力提供了纯逻辑上的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been growing interest in understanding theexpressive power of graph neural networks (GNNs) by relating them to logicallanguages. This research has been been initialised by an influential result ofBarcel\'o et al. (2020), who showed that the graded modal logic (or a guardedfragment of the logic C2), characterises the logical expressiveness ofaggregate-combine GNNs. As a ``challenging open problem'' they left thequestion whether full C2 characterises the logical expressiveness ofaggregate-combine-readout GNNs. This question has remained unresolved despiteseveral attempts. In this paper, we solve the above open problem by provingthat the logical expressiveness of aggregate-combine-readout GNNs strictlyexceeds that of C2. This result holds over both undirected and directed graphs.Beyond its implications for GNNs, our work also leads to purely logicalinsights on the expressive power of infinitary logics.</description>
      <author>example@mail.com (Stan P Hauke, Przemysław Andrzej Wałęga)</author>
      <guid isPermaLink="false">2508.06091v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>ProvX: Generating Counterfactual-Driven Attack Explanations for Provenance-Based Detection</title>
      <link>http://arxiv.org/abs/2508.06073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProvX是一种基于溯源图的GNN安全模型解释框架，通过反事实解释逻辑和连续优化方法定位关键图结构，提高了解释的精确性和稳定性，实验表明其能有效增强模型对抗对抗性攻击的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;基于溯源图的入侵检测系统用于防御高级持续性威胁，图神经网络(GNN)在检测这些威胁方面表现出色，但其黑盒性质限制了广泛应用。&lt;h4&gt;目的&lt;/h4&gt;提出ProvX框架，解决GNN安全模型缺乏可验证解释的问题，为安全分析师提供模型预测的证据和解释。&lt;h4&gt;方法&lt;/h4&gt;ProvX引入反事实解释逻辑，寻找图中被预测为恶意的最小结构子集；创新地将离散搜索问题转化为连续优化任务；采用分阶段固化策略提高解释精确性和稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;ProvX能定位与真实世界攻击高度相关的关键图结构，平均解释必要性达51.59%，优于当前最先进的解释器；其解释结果可指导模型优化，增强对抗对抗性攻击的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;ProvX有效解决了GNN安全模型的可解释性问题，并通过闭环检测-解释-反馈框架提升了模型性能，为安全分析提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;基于溯源图的入侵检测系统被部署在主机上以防御日益严重的高级持续性威胁。使用图神经网络检测这些威胁已成为研究重点并表现出色。然而，GNN安全模型的广泛应用受到其固有黑盒性质的限制，因为它们无法为安全分析师提供模型预测的可验证解释或关于模型判断与现实世界攻击关系的任何证据。为解决这一挑战，我们提出了ProvX，一个用于解释基于溯源图的GNN安全模型的有效解释框架。ProvX引入反事实解释逻辑，寻找图中被预测为恶意的最小结构子集，当扰动时可以颠覆模型的原始预测。我们创新地将寻找关键子图的离散搜索问题转化为由预测翻转和距离最小化双重目标引导的连续优化任务。此外，还采用分阶段固化策略提高解释的精确性和稳定性。我们在权威数据集上对ProvX进行了广泛评估。实验结果表明ProvX能定位与真实世界攻击高度相关的关键图结构，平均解释必要性达51.59%，这些指标优于当前最先进的解释器。此外，我们探索并提供了闭环检测-解释-反馈增强框架的初步验证，实验证明ProvX的解释结果可以指导模型优化，有效增强其对对抗性攻击的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Provenance graph-based intrusion detection systems are deployed on hosts todefend against increasingly severe Advanced Persistent Threat. Using GraphNeural Networks to detect these threats has become a research focus and hasdemonstrated exceptional performance. However, the widespread adoption ofGNN-based security models is limited by their inherent black-box nature, asthey fail to provide security analysts with any verifiable explanations formodel predictions or any evidence regarding the model's judgment in relation toreal-world attacks. To address this challenge, we propose ProvX, an effectiveexplanation framework for exlaining GNN-based security models on provenancegraphs. ProvX introduces counterfactual explanation logic, seeking the minimalstructural subset within a graph predicted as malicious that, when perturbed,can subvert the model's original prediction. We innovatively transform thediscrete search problem of finding this critical subgraph into a continuousoptimization task guided by a dual objective of prediction flipping anddistance minimization. Furthermore, a Staged Solidification strategy isincorporated to enhance the precision and stability of the explanations. Weconducted extensive evaluations of ProvX on authoritative datasets. Theexperimental results demonstrate that ProvX can locate critical graphstructures that are highly relevant to real-world attacks and achieves anaverage explanation necessity of 51.59\%, with these metrics outperformingcurrent SOTA explainers. Furthermore, we explore and provide a preliminaryvalidation of a closed-loop Detection-Explanation-Feedback enhancementframework, demonstrating through experiments that the explanation results fromProvX can guide model optimization, effectively enhancing its robustnessagainst adversarial attacks.</description>
      <author>example@mail.com (Weiheng Wu, Wei Qiao, Teng Li, Yebo Feng, Zhuo Ma, Jianfeng Ma, Yang Liu)</author>
      <guid isPermaLink="false">2508.06073v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity</title>
      <link>http://arxiv.org/abs/2508.06034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted tp CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自适应异构图神经网络（AHGNN）来解决异构图建模中的异质性挑战，通过考虑跳数和元路径特定的异质性分布，并使用从粗到细的注意力机制整合不同语义空间的信息。&lt;h4&gt;背景&lt;/h4&gt;异构图在实际场景中很常见且通常表现出异质性，但大多数现有研究要么只关注异质性，要么只关注异质性，忽略了实际应用中异质性HG的普遍存在，导致性能下降。&lt;h4&gt;目的&lt;/h4&gt;识别建模异质性HG的两个主要挑战，并提出一种能有效处理这些挑战的自适应异构图神经网络（AHGNN）。&lt;h4&gt;方法&lt;/h4&gt;AHGNN采用异质性感知卷积，考虑跳数和元路径特定的异质性分布，并使用从粗到细的注意力机制整合来自不同语义空间的消息，过滤噪声并强调信息信号。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实世界图和二十个基线的实验中，AHGNN表现出优越的性能，特别是在高异质性的情况下。&lt;h4&gt;结论&lt;/h4&gt;通过考虑异质性分布和语义信息的多样性，AHGNN能有效处理异质性HG，在高异质性情况下表现出色。&lt;h4&gt;翻译&lt;/h4&gt;异构图（HGs）在实际场景中很常见，并且通常表现出异质性。然而，大多数现有研究要么只关注异质性，要么只关注异质性，忽略了实际应用中异质性HG的普遍存在。这种忽视导致了它们的性能下降。在这项工作中，我们首先确定了建模异质性HG的两个主要挑战：（1）跳数和元路径之间异质性分布的变化；（2）不同元路径间语义信息的复杂且通常由异质性驱动的多样性。然后，我们提出了自适应异构图神经网络（AHGNN）来解决这些挑战。AHGNN采用了一种异质性感知卷积，考虑了跳数和元路径特定的异质性分布。然后，它使用从粗到细的注意力机制整合来自不同语义空间的消息，过滤掉噪声并强调信息信号。在七个真实世界图和二十个基线的实验中，证明了AHGNN的优越性能，特别是在高异质性的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous graphs (HGs) are common in real-world scenarios and oftenexhibit heterophily. However, most existing studies focus on eitherheterogeneity or heterophily in isolation, overlooking the prevalence ofheterophilic HGs in practical applications. Such ignorance leads to theirperformance degradation. In this work, we first identify two main challenges inmodeling heterophily HGs: (1) varying heterophily distributions across hops andmeta-paths; (2) the intricate and often heterophily-driven diversity ofsemantic information across different meta-paths. Then, we propose the AdaptiveHeterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNNemploys a heterophily-aware convolution that accounts for heterophilydistributions specific to both hops and meta-paths. It then integrates messagesfrom diverse semantic spaces using a coarse-to-fine attention mechanism, whichfilters out noise and emphasizes informative signals. Experiments on sevenreal-world graphs and twenty baselines demonstrate the superior performance ofAHGNN, particularly in high-heterophily situations.</description>
      <author>example@mail.com (Qin Chen, Guojie Song)</author>
      <guid isPermaLink="false">2508.06034v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Functional Connectivity Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.05786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 5 figures, 24 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种受脑成像多模态分析启发的图神经网络框架，通过结合结构信息和基于持久图同调的功能连接性模块，捕获网络的局部和全局特征，形成功能连接性图神经网络。实验证明该方法在图级分类任务中优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;现实世界的网络通常受益于捕捉局部和全局交互。受脑成像多模态分析的启发，其中结构连接性和功能连接性提供了网络组织的互补视角。&lt;h4&gt;目的&lt;/h4&gt;提出一个图神经网络框架，将脑成像多模态分析方法推广到其他领域。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于持久图同调的功能连接性模块，用于捕获全局拓扑特征。与结构信息相结合，形成了一种称为功能连接性图神经网络的多模态架构。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与现有方法相比，该方法取得了持续的性能提升。&lt;h4&gt;结论&lt;/h4&gt;受大脑启发的表示在不同网络图级分类中具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;现实世界的网络通常受益于捕捉局部和全局交互。受脑成像多模态分析的启发，其中结构连接性和功能连接性提供了网络组织的互补视角，我们提出了一个图神经网络框架，将这种方法推广到其他领域。我们的方法引入了一种基于持久图同调的功能连接性模块，用于捕获全局拓扑特征。与结构信息相结合，这形成了一种称为功能连接性图神经网络的多模态架构。实验表明，与现有方法相比，该方法取得了持续的性能提升，证明了受大脑启发的表示在不同网络图级分类中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world networks often benefit from capturing both local and globalinteractions. Inspired by multi-modal analysis in brain imaging, wherestructural and functional connectivity offer complementary views of networkorganization, we propose a graph neural network framework that generalizes thisapproach to other domains. Our method introduces a functional connectivityblock based on persistent graph homology to capture global topologicalfeatures. Combined with structural information, this forms a multi-modalarchitecture called Functional Connectivity Graph Neural Networks. Experimentsshow consistent performance gains over existing methods, demonstrating thevalue of brain-inspired representations for graph-level classification acrossdiverse networks.</description>
      <author>example@mail.com (Yang Li, Luopeiwen Yi, Tananun Songdechakraiwut)</author>
      <guid isPermaLink="false">2508.05786v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics</title>
      <link>http://arxiv.org/abs/2508.05724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了一种将物理定律表示和分析为加权知识图的新框架，通过构建物理方程数据库、开发增强图表示方法和训练图注意力网络，实现了高效的物理方程链接预测，并发现了物理学的宏观结构和跨领域关系。&lt;h4&gt;背景&lt;/h4&gt;物理定律传统上以方程形式表示，但缺乏系统化的知识组织方式。研究团队构建了一个包含659个不同物理方程的数据库，经过严格的语义清洗解决符号歧义，最终形成400个高级物理方程的语料库。&lt;h4&gt;目的&lt;/h4&gt;开发一种增强的图表示方法，其中物理概念和方程都是节点，通过加权的方程间桥接连接；使用客观定义的权重（变量重叠的标准化度量、物理信息重要性评分和计量学数据）；训练图注意力网络进行链接预测。&lt;h4&gt;方法&lt;/h4&gt;构建物理方程数据库并进行语义清洗；开发增强的图表示方法，物理概念和方程作为节点，通过加权的方程间桥接连接；使用变量重叠、物理信息重要性和计量学数据定义客观权重；训练图注意力网络进行链接预测。&lt;h4&gt;主要发现&lt;/h4&gt;模型自主发现了物理学的已知宏观结构，识别出电磁学和统计力学之间的强概念轴；识别出作为多个物理领域间关键桥梁的中心枢纽方程；模型生成稳定的、计算得出的跨领域关系假设，识别已知原理并建议新的数学类比供进一步理论研究。&lt;h4&gt;结论&lt;/h4&gt;该框架可以生成数百个此类假设， enabling 创建专门的数据集用于针对特定物理学子领域的定向分析；代码和数据可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;这项工作引入了一种将物理定律表示和分析为加权知识图的新框架。我们构建了一个包含659个不同物理方程的数据库，经过严格的语义清洗解决符号歧义，最终形成400个高级物理方程的语料库。我们开发了一种增强的图表示方法，其中物理概念和方程都是节点，通过加权的方程间桥接连接。这些权重使用变量重叠的标准化度量、物理信息重要性评分和计量学数据进行客观定义。我们训练了一个图注意力网络(GAT)进行链接预测，在五次独立运行中测试AUC达到0.9742 +/- 0.0018，显著优于经典启发式方法（最佳基线AUC：0.9487）和已建立的GNN架构如图神经网络（AUC：0.9504，p = 0.029）。统计检验确认了所有比较的显著性（p &lt; 0.05），比最佳基线提高了2.7%。我们的分析揭示了三个关键发现：(i) 模型自主发现了物理学的已知宏观结构，识别出电磁学和统计力学之间的强概念轴。(ii) 它识别出作为多个物理领域间关键桥梁的中心枢纽方程。(iii) 模型生成稳定的、计算得出的跨领域关系假设，识别已知原理并建议新的数学类比供进一步理论研究。该框架可以生成数百个此类假设， enabling 创建专门的数据集用于针对特定物理学子领域的定向分析。代码和数据可在https://github.com/kingelanci/graphysics获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work introduces a novel framework for representing and analyzingphysical laws as a weighted knowledge graph. We constructed a database of 659distinct physical equations, subjected to rigorous semantic cleaning to resolvenotational ambiguities, resulting in a corpus of 400 advanced physicsequations. We developed an enhanced graph representation where both physicalconcepts and equations are nodes, connected by weighted inter-equation bridges.These weights are objectively defined using normalized metrics for variableoverlap, physics-informed importance scores, and bibliometric data. A GraphAttention Network (GAT) was trained for link prediction, achieving a test AUCof 0.9742 +/- 0.0018 across five independent runs, significantly outperformingboth classical heuristics (best baseline AUC: 0.9487) and established GNNarchitectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testingconfirmed significance of all comparisons (p &lt; 0.05), with 2.7% improvementover the best baseline. Our analysis reveals three key findings: (i) The modelautonomously rediscovers the known macroscopic structure of physics,identifying strong conceptual axes between Electromagnetism and StatisticalMechanics. (ii) It identifies central hub equations that serve as criticalbridges between multiple physical domains. (iii) The model generates stable,computationally-derived hypotheses for cross-domain relationships, identifyingboth known principles and suggesting novel mathematical analogies for furthertheoretical investigation. The framework can generate hundreds of suchhypotheses, enabling the creation of specialized datasets for targeted analysisof specific physics subfields. Code and data available athttps://github.com/kingelanci/graphysics</description>
      <author>example@mail.com (Massimiliano Romiti)</author>
      <guid isPermaLink="false">2508.05724v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation</title>
      <link>http://arxiv.org/abs/2508.06452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TRUST，一种新型无监督域适应方法，利用语言模态的鲁棒性来指导视觉模型适应复杂域偏移。&lt;h4&gt;背景&lt;/h4&gt;现有无监督域适应方法在处理经典域偏移（如合成到真实）时表现出色，但在处理复杂域偏移（如地理偏移）时效果不佳，因为这些情况下不同域之间的背景和物体外观存在显著差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理复杂域偏移的无监督域适应方法，利用语言模态的鲁棒性来提高视觉模型在目标域上的性能。&lt;h4&gt;方法&lt;/h4&gt;TRUST方法通过从图像标题生成伪标签，并使用归一化的CLIP相似度分数估计这些伪标签的不确定性；然后利用估计的不确定性重新加权分类损失，减轻低质量标题带来的错误标签影响；此外，还提出多模态软对比学习损失，对齐视觉和语言特征空间，利用标题引导视觉模型在目标图像上的对比训练。&lt;h4&gt;主要发现&lt;/h4&gt;语言模态可以帮助适应过程，对复杂域偏移表现出更强的鲁棒性；提出的多模态软对比学习损失避免了需要确定正负样本对的问题，这在无监督域适应设置中非常关键。&lt;h4&gt;结论&lt;/h4&gt;TRUST方法在经典域偏移（DomainNet）和复杂域偏移（GeoNet）上均超越了先前方法，设立了新的最先进水平。&lt;h4&gt;翻译&lt;/h4&gt;最近的无监督域适应方法在处理经典域偏移（如合成到真实）方面已显示出巨大成功，但在复杂偏移（如地理偏移）下仍然表现不佳，在这种情况下，不同域之间的背景和物体外观存在显著差异。先前的工作表明，语言模态可以在适应过程中提供帮助，对这类复杂偏移表现出更强的鲁棒性。在本文中，我们引入了TRUST，一种新型无监督域适应方法，它利用语言模态的鲁棒性来指导视觉模型的适应。TRUST从图像标题为目标样本生成伪标签，并引入了一种新的不确定性估计策略，使用归一化的CLIP相似度分数来估计生成伪标签的不确定性。然后使用这种估计的不确定性对分类损失进行重新加权，减轻从低质量标题获得的错误伪标签的不利影响。为了进一步增强视觉模型的鲁棒性，我们提出了一种多模态软对比学习损失，通过利用标题来引导视觉模型在目标图像上的对比训练，从而对齐视觉和语言特征空间。在我们的对比损失中，每对图像同时作为正样本对和负样本对，其特征表示根据其标题的相似度比例被吸引和排斥。这种解决方案避免了难以确定正负样本对的需要，这在无监督域适应设置中至关重要。我们的方法超越了先前的方法，在经典域偏移（DomainNet）和复杂域偏移（GeoNet）上设立了新的最先进水平。代码将在接受后提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent unsupervised domain adaptation (UDA) methods have shown great successin addressing classical domain shifts (e.g., synthetic-to-real), but they stillsuffer under complex shifts (e.g. geographical shift), where both thebackground and object appearances differ significantly across domains. Priorworks showed that the language modality can help in the adaptation process,exhibiting more robustness to such complex shifts. In this paper, we introduceTRUST, a novel UDA approach that exploits the robustness of the languagemodality to guide the adaptation of a vision model. TRUST generatespseudo-labels for target samples from their captions and introduces a noveluncertainty estimation strategy that uses normalised CLIP similarity scores toestimate the uncertainty of the generated pseudo-labels. Such estimateduncertainty is then used to reweight the classification loss, mitigating theadverse effects of wrong pseudo-labels obtained from low-quality captions. Tofurther increase the robustness of the vision model, we propose a multimodalsoft-contrastive learning loss that aligns the vision and language featurespaces, by leveraging captions to guide the contrastive training of the visionmodel on target images. In our contrastive loss, each pair of images acts asboth a positive and a negative pair and their feature representations areattracted and repulsed with a strength proportional to the similarity of theircaptions. This solution avoids the need for hardly determining positive andnegative pairs, which is critical in the UDA setting. Our approach outperformsprevious methods, setting the new state-of-the-art on classical (DomainNet) andcomplex (GeoNet) domain shifts. The code will be available upon acceptance.</description>
      <author>example@mail.com (Mattia Litrico, Mario Valerio Giuffrida, Sebastiano Battiato, Devis Tuia)</author>
      <guid isPermaLink="false">2508.06452v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment</title>
      <link>http://arxiv.org/abs/2508.06434v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CLIPin，一个统一的非对比插件，可无缝集成到CLIP架构中，改进多模态语义对齐并提供更强监督，增强对齐鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;大规模自然图像-文本数据集（特别是网络自动收集的）常因弱监督存在松散语义对齐问题；医学数据集虽有高跨模态相关性但内容多样性低，这些特性阻碍了CLIP模型学习鲁棒且可推广的表示能力。&lt;h4&gt;目的&lt;/h4&gt;开发CLIPin以改进多模态语义对齐，提供更强监督并增强对齐鲁棒性，解决现有数据集的语义对齐挑战。&lt;h4&gt;方法&lt;/h4&gt;设计两个共享预投影器分别用于图像和文本模态，促进对比学习和非对比学习以参数折衷方式集成；CLIPin作为即插即用组件兼容各种对比框架。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化下游任务上的广泛实验证明CLIPin作为即插即用组件具有有效性和通用性，可兼容各种对比框架。&lt;h4&gt;结论&lt;/h4&gt;CLIPin成功解决了大规模自然图像-文本数据集和医学数据集的语义对齐问题，提供了一种改进多模态表示学习的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;大规模自然图像-文本数据集，特别是那些从网络自动收集的数据集，通常因弱监督而存在松散的语义对齐问题，而医学数据集往往具有较高的跨模态相关性但较低的内容多样性。这些特性对对比语言-图像预训练（CLIP）构成了共同挑战：它们阻碍了模型学习鲁棒且可推广的表示能力。在这项工作中，我们提出了CLIPin，一个统一的非对比插件，可以无缝集成到CLIP架构中，改进多模态语义对齐，提供更强的监督并增强对齐鲁棒性。此外，分别为图像和文本模态设计了两个共享预投影器，以促进对比学习和非对比学习以参数折衷方式集成。在多样化下游任务上的广泛实验证明了CLIPin作为即插即用组件的有效性和通用性，兼容各种对比框架。代码可在https://github.com/T6Yang/CLIPin获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale natural image-text datasets, especially those automaticallycollected from the web, often suffer from loose semantic alignment due to weaksupervision, while medical datasets tend to have high cross-modal correlationbut low content diversity. These properties pose a common challenge forcontrastive language-image pretraining (CLIP): they hinder the model's abilityto learn robust and generalizable representations. In this work, we proposeCLIPin, a unified non-contrastive plug-in that can be seamlessly integratedinto CLIP-style architectures to improve multimodal semantic alignment,providing stronger supervision and enhancing alignment robustness. Furthermore,two shared pre-projectors are designed for image and text modalitiesrespectively to facilitate the integration of contrastive and non-contrastivelearning in a parameter-compromise manner. Extensive experiments on diversedownstream tasks demonstrate the effectiveness and generality of CLIPin as aplug-and-play component compatible with various contrastive frameworks. Code isavailable at https://github.com/T6Yang/CLIPin.</description>
      <author>example@mail.com (Shengzhu Yang, Jiawei Du, Shuai Lu, Weihang Zhang, Ningli Wang, Huiqi Li)</author>
      <guid isPermaLink="false">2508.06434v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Item Graph Enhancement for Multimodal Recommendation</title>
      <link>http://arxiv.org/abs/2508.06154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进的多模态推荐系统，解决了现有语义图中的语义缺陷问题，包括物品间协作信号建模不足和原始模态特征噪声导致的结构扭曲。通过注入协作信号、设计个性化嵌入扰动机制和双重表示对齐机制，提高了模型对噪声的鲁棒性，并在多个基准数据集上验证了有效性。&lt;h4&gt;背景&lt;/h4&gt;多模态推荐系统通过利用物品的多模态信息提高了性能，受到越来越多的关注。先前的方法通常从原始模态特征构建模态特定的物品-物品语义图，并与用户-物品交互图一起作为补充结构，以增强用户偏好学习。&lt;h4&gt;目的&lt;/h4&gt;解决现有语义图中的语义缺陷问题，包括物品间协作信号建模不足和原始模态特征噪声导致的结构扭曲，从而提高多模态推荐系统的性能。&lt;h4&gt;方法&lt;/h4&gt;1) 从交互图中提取协作信号并注入到模态特定的物品语义图中以增强语义建模；2) 设计基于模的个性化嵌入扰动机制，通过对比学习学习对噪声鲁棒的表示；3) 提出双重表示对齐机制，先使用行为表示作为锚点对齐多个语义表示，再对齐行为表示与融合语义。&lt;h4&gt;主要发现&lt;/h4&gt;现有语义图存在两个主要语义缺陷：物品间协作信号建模不足和原始模态特征噪声导致的结构扭曲，这些缺陷会损害系统性能。通过注入协作信号、设计个性化嵌入扰动机制和双重表示对齐机制，可以有效解决这些问题。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架通过增强语义建模、提高对噪声的鲁棒性和确保表示一致性，有效解决了多模态推荐系统中语义图的语义缺陷问题，在多个基准数据集上验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;多模态推荐系统因利用物品的多模态信息提高性能而受到越来越多的关注。先前的方法通常从原始模态特征构建模态特定的物品-物品语义图，并将它们与用户-物品交互图一起作为补充结构，以增强用户偏好学习。然而，这些语义图存在语义缺陷，包括(1)物品间协作信号建模不足，以及(2)原始模态特征中的噪声引入的结构扭曲，最终损害性能。为解决这些问题，我们首先从交互图中提取协作信号，并将它们注入到每个模态特定的物品语义图中，以增强语义建模。然后，我们设计了一种基于模的个性化嵌入扰动机制，将具有模引导的个性化强度的扰动注入嵌入中，以生成对比视图。这使得模型能够通过对比学习学习对噪声鲁棒的表示，从而减少语义图中结构噪声的影响。此外，我们提出了一种双重表示对齐机制，首先使用行为表示作为锚点，通过设计的基于锚点的InfoNCE损失对齐多个语义表示，然后通过标准InfoNCE将对齐行为表示与融合语义，以确保表示一致性。在四个基准数据集上的大量实验验证了我们框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal recommendation systems have attracted increasing attention fortheir improved performance by leveraging items' multimodal information. Priormethods often build modality-specific item-item semantic graphs from rawmodality features and use them as supplementary structures alongside theuser-item interaction graph to enhance user preference learning. However, thesesemantic graphs suffer from semantic deficiencies, including (1) insufficientmodeling of collaborative signals among items and (2) structural distortionsintroduced by noise in raw modality features, ultimately compromisingperformance. To address these issues, we first extract collaborative signalsfrom the interaction graph and infuse them into each modality-specific itemsemantic graph to enhance semantic modeling. Then, we design a modulus-basedpersonalized embedding perturbation mechanism that injects perturbations withmodulus-guided personalized intensity into embeddings to generate contrastiveviews. This enables the model to learn noise-robust representations throughcontrastive learning, thereby reducing the effect of structural noise insemantic graphs. Besides, we propose a dual representation alignment mechanismthat first aligns multiple semantic representations via a designed Anchor-basedInfoNCE loss using behavior representations as anchors, and then alignsbehavior representations with the fused semantics by standard InfoNCE, toensure representation consistency. Extensive experiments on four benchmarkdatasets validate the effectiveness of our framework.</description>
      <author>example@mail.com (Xiaoxiong Zhang, Xin Zhou, Zhiwei Zeng, Dusit Niyato, Zhiqi Shen)</author>
      <guid isPermaLink="false">2508.06154v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text Clustering</title>
      <link>http://arxiv.org/abs/2508.06126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IOCC是一种解决短文本聚类中语义中心识别困难的小样本对比学习方法，通过IEOT和CACL两个模块的协作，实现了聚类中心和语义中心的对齐，显著提升了聚类效果。&lt;h4&gt;背景&lt;/h4&gt;在聚类任务中，将特征空间结构化为清晰、良好分离的分布至关重要。然而，由于短文本表示的表达能力有限，传统方法难以真正识别出能够捕捉每个类别潜在语义的聚类中心，导致表示在次优方向上被优化。&lt;h4&gt;目的&lt;/h4&gt;提出IOCC，一种新颖的小样本对比学习方法，旨在实现聚类中心和语义中心之间的对齐。&lt;h4&gt;方法&lt;/h4&gt;IOCC包含两个关键模块：1)交互增强最优传输(IEOT)：将个体样本之间的语义交互整合到传统的最优传输问题中，并生成伪标签；2)中心感知对比学习(CACL)：基于这些伪标签，聚合高置信度样本来构建近似语义中心的伪中心，然后将文本表示优化到其对应的伪中心。随着训练进行，两个模块协作逐渐缩小聚类中心和语义中心之间的差距。&lt;h4&gt;主要发现&lt;/h4&gt;在八个基准数据集上的大量实验表明，IOCC优于先前的方法，在具有挑战性的生物医学数据集上实现了高达7.34%的改进，并且在聚类稳定性和效率方面也表现出色。&lt;h4&gt;结论&lt;/h4&gt;通过IOCC方法，模型能够学习到高质量的分布，从而提高聚类性能。&lt;h4&gt;翻译&lt;/h4&gt;在聚类任务中，将特征空间结构化为清晰、良好分离的分布是至关重要的。然而，由于短文本表示的表达能力有限，传统方法难以真正识别出能够捕捉每个类别潜在语义的聚类中心，导致表示在次优方向上被优化。为解决这一问题，我们提出了IOCC，一种新颖的小样本对比学习方法，旨在实现聚类中心和语义中心之间的对齐。IOCC包含两个关键模块：交互增强最优传输(IEOT)和中心感知对比学习(CACL)。具体而言，IEOT将个体样本之间的语义交互整合到传统的最优传输问题中，并生成伪标签。基于这些伪标签，我们聚合高置信度样本来构建近似语义中心的伪中心。随后，CACL将文本表示优化到其对应的伪中心。随着训练的进行，两个模块之间的协作逐渐缩小了聚类中心和语义中心之间的差距。因此，模型将学习到高质量的分布，提高聚类性能。在八个基准数据集上的大量实验表明，IOCC优于先前的方法，在具有挑战性的生物医学数据集上实现了高达7.34%的改进，并且在聚类稳定性和效率方面也表现出色。代码可在以下地址获取：https://anonymous.4open.science/r/IOCC-C438。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In clustering tasks, it is essential to structure the feature space intoclear, well-separated distributions. However, because short textrepresentations have limited expressiveness, conventional methods struggle toidentify cluster centers that truly capture each category's underlyingsemantics, causing the representations to be optimized in suboptimaldirections. To address this issue, we propose IOCC, a novel few-shotcontrastive learning method that achieves alignment between the cluster centersand the semantic centers. IOCC consists of two key modules:Interaction-enhanced Optimal Transport (IEOT) and Center-aware ContrastiveLearning (CACL). Specifically, IEOT incorporates semantic interactions betweenindividual samples into the conventional optimal transport problem, andgenerate pseudo-labels. Based on these pseudo-labels, we aggregatehigh-confidence samples to construct pseudo-centers that approximate thesemantic centers. Next, CACL optimizes text representations toward theircorresponding pseudo-centers. As training progresses, the collaboration betweenthe two modules gradually reduces the gap between cluster centers and semanticcenters. Therefore, the model will learn a high-quality distribution, improvingclustering performance. Extensive experiments on eight benchmark datasets showthat IOCC outperforms previous methods, achieving up to 7.34\% improvement onchallenging Biomedical dataset and also excelling in clustering stability andefficiency. The code is available at:https://anonymous.4open.science/r/IOCC-C438.</description>
      <author>example@mail.com (Jixuan Yin, Zhihao Yao, Wenshuai Huo, Xinmiao Yu, Xiaocheng Feng, Bo Li)</author>
      <guid isPermaLink="false">2508.06126v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.06115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为SynSeg的新型弱监督方法，用于解决开放词汇场景下的语义分割挑战，通过多类别对比学习和特征协同结构实现了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;开放词汇场景下的语义分割面临语义类别范围广、粒度多样的挑战，现有弱监督方法依赖特定类别监督且不适合对比学习的特征构建，导致语义错位和性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为SynSeg的新型弱监督方法，以解决开放词汇场景下语义分割的挑战。&lt;h4&gt;方法&lt;/h4&gt;SynSeg采用多类别对比学习(MCCL)作为更强训练信号，结合名为特征协同结构(FSS)的新特征重建框架；MCCL策略稳健结合类别内和类别间的对齐与分离，使模型学习同一图像中不同类别间的相关性；FSS通过先验融合和语义激活图增强重建判别性特征，避免视觉编码器引入的前景偏差。&lt;h4&gt;主要发现&lt;/h4&gt;SynSeg有效提高了弱监督下的语义定位和判别能力；在基准测试中，该方法性能优于最先进方法，在VOC上高4.5%，在Context上高8.9%，在Object上高2.6%，在City上高2.0%。&lt;h4&gt;结论&lt;/h4&gt;SynSeg是一种有效的弱监督方法，能够解决开放词汇场景下语义分割的挑战，并在多个基准测试上取得最先进性能。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇场景下的语义分割由于语义类别的广泛性和多样性而面临重大挑战。现有的弱监督方法通常依赖于特定类别的监督和不适用的对比学习特征构建方法，导致语义错位和性能不佳。在这项工作中，我们提出了一种新颖的弱监督方法SynSeg来解决这些挑战。SynSeg将多类别对比学习(MCCL)作为更强的训练信号，并采用名为特征协同结构(FSS)的新特征重建框架。具体来说，MCCL策略稳健地结合了类别内和类别间的对齐与分离，使模型能够学习同一图像中不同类别之间的相关性知识。此外，FSS通过先验融合和语义激活图增强为对比学习重建判别性特征，有效避免了视觉编码器引入的前景偏差。总体而言，SynSeg在弱监督下有效提高了语义定位和判别能力。在基准测试上的广泛实验表明，我们的方法优于最先进的性能。例如，SynSeg在VOC上比SOTA基线高4.5%，在Context上高8.9%，在Object上高2.6%，在City上高2.0%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation in open-vocabulary scenarios presents significantchallenges due to the wide range and granularity of semantic categories.Existing weakly-supervised methods often rely on category-specific supervisionand ill-suited feature construction methods for contrastive learning, leadingto semantic misalignment and poor performance. In this work, we propose a novelweakly-supervised approach, SynSeg, to address the challenges. SynSeg performsMulti-Category Contrastive Learning (MCCL) as a stronger training signal with anew feature reconstruction framework named Feature Synergy Structure (FSS).Specifically, MCCL strategy robustly combines both intra- and inter-categoryalignment and separation in order to make the model learn the knowledge ofcorrelations from different categories within the same image. Moreover, FSSreconstructs discriminative features for contrastive learning through priorfusion and semantic-activation-map enhancement, effectively avoiding theforeground bias introduced by the visual encoder. In general, SynSegeffectively improves the abilities in semantic localization and discriminationunder weak supervision. Extensive experiments on benchmarks demonstrate thatour method outperforms state-of-the-art (SOTA) performance. For instance,SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% onContext, 2.6\% on Object and 2.0\% on City.</description>
      <author>example@mail.com (Weichen Zhang, Kebin Liu, Fan Dang, Zhui Zhu, Xikai Sun, Yunhao Liu)</author>
      <guid isPermaLink="false">2508.06115v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</title>
      <link>http://arxiv.org/abs/2508.06189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MA-CBP框架，一种基于多智能体异步协作的犯罪行为预测方法，通过处理实时视频流进行犯罪活动预警。&lt;h4&gt;背景&lt;/h4&gt;城市化加速导致公共场所犯罪行为对社会安全构成日益严重威胁；传统基于特征识别的异常检测方法难以捕获高级行为语义；基于大型语言模型的生成方法无法满足实时要求。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法和生成方法的局限性，开发一种能够有效预测犯罪行为的实时框架。&lt;h4&gt;方法&lt;/h4&gt;提出MA-CBP框架，将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，融合相邻图像帧进行长短期上下文联合推理；构建高质量犯罪行为数据集，提供多尺度语言监督（帧级、摘要级和事件级语义注释）。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多个数据集上实现了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;MA-CBP为城市公共安全场景中的风险预警提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化进程的加速，公共场所的犯罪行为对社会安全构成日益严重的威胁。基于特征识别的传统异常检测方法难以从历史信息中捕获高级行为语义，而基于大型语言模型的生成方法通常无法满足实时要求。为应对这些挑战，我们提出了MA-CBP，一种基于多智能体异步协作的犯罪行为预测框架。该框架将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合相邻图像帧对长短期上下文进行联合推理。所产生的行为决策包括事件主体、地点和原因等关键要素，能够对潜在的犯罪活动进行预警。此外，我们构建了一个高质量的犯罪行为数据集，提供了多尺度语言监督，包括帧级、摘要级和事件级语义注释。实验结果表明，我们的方法在多个数据集上实现了优越的性能，为城市公共安全场景中的风险预警提供了有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the acceleration of urbanization, criminal behavior in public scenesposes an increasingly serious threat to social security. Traditional anomalydetection methods based on feature recognition struggle to capture high-levelbehavioral semantics from historical information, while generative approachesbased on Large Language Models (LLMs) often fail to meet real-timerequirements. To address these challenges, we propose MA-CBP, a criminalbehavior prediction framework based on multi-agent asynchronous collaboration.This framework transforms real-time video streams into frame-level semanticdescriptions, constructs causally consistent historical summaries, and fusesadjacent image frames to perform joint reasoning over long- and short-termcontexts. The resulting behavioral decisions include key elements such as eventsubjects, locations, and causes, enabling early warning of potential criminalactivity. In addition, we construct a high-quality criminal behavior datasetthat provides multi-scale language supervision, including frame-level,summary-level, and event-level semantic annotations. Experimental resultsdemonstrate that our method achieves superior performance on multiple datasetsand offers a promising solution for risk warning in urban public safetyscenarios.</description>
      <author>example@mail.com (Cheng Liu, Daou Zhang, Tingxu Liu, Yuhan Wang, Jinyang Chen, Yuexuan Li, Xinying Xiao, Chenbo Xin, Ziru Wang, Weichao Wu)</author>
      <guid isPermaLink="false">2508.06189v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</title>
      <link>http://arxiv.org/abs/2508.06471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍GLM-4.5，一个开源的混合专家大型语言模型，具有355B总参数和32B激活参数，采用混合推理方法，在多个任务上表现优异，总体排名第3。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的发展，特别是混合专家模型(MoE)在提升模型性能方面的潜力。&lt;h4&gt;目的&lt;/h4&gt;开发一个高性能的开源大型语言模型，支持混合推理，并在代理、推理和编程任务上取得优异成绩。&lt;h4&gt;方法&lt;/h4&gt;构建一个混合专家模型(MoE)，使用23T tokens进行多阶段训练，并通过专家模型迭代和强化学习进行全面的后期训练。&lt;h4&gt;主要发现&lt;/h4&gt;GLM-4.5在TAU-Bench上得分为70.1%，在AIME 24上得分为91.0%，在SWE-bench Verified上得分为64.2%。尽管参数比一些竞争对手少，但在所有评估模型中总体排名第3，在代理基准测试中排名第2。&lt;h4&gt;结论&lt;/h4&gt;GLM-4.5是一个高性能的开源大型语言模型，通过混合推理方法和全面训练，在多个任务上取得了优异的成绩，为研究和应用提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了GLM-4.5，一个开源的混合专家大型语言模型，具有355B总参数和32B激活参数，采用混合推理方法，支持思考模式和直接响应模式。通过23T tokens的多阶段训练和包含专家模型迭代和强化学习的全面后训练，GLM-4.5在代理、推理和编程任务上表现出色，在TAU-Bench上得分为70.1%，在AIME 24上得分为91.0%，在SWE-bench Verified上得分为64.2%。尽管参数比一些竞争对手少，但GLM-4.5在所有评估模型中总体排名第3，在代理基准测试中排名第2。我们发布了GLM-4.5(355B参数)和紧凑版本GLM-4.5-Air(106B参数)，以推动推理和代理AI系统的研究。代码、模型和更多信息可在https://github.com/zai-org/GLM-4.5获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large languagemodel with 355B total parameters and 32B activated parameters, featuring ahybrid reasoning method that supports both thinking and direct response modes.Through multi-stage training on 23T tokens and comprehensive post-training withexpert model iteration and reinforcement learning, GLM-4.5 achieves strongperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% onTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewerparameters than several competitors, GLM-4.5 ranks 3rd overall among allevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355Bparameters) and a compact version, GLM-4.5-Air (106B parameters), to advanceresearch in reasoning and agentic AI systems. Code, models, and moreinformation are available at https://github.com/zai-org/GLM-4.5.</description>
      <author>example@mail.com (GLM-4. 5 Team, :, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang)</author>
      <guid isPermaLink="false">2508.06471v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Deepfake Detection that Generalizes Across Benchmarks</title>
      <link>http://arxiv.org/abs/2508.06248v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LNCLIP-DF是一种仅微调CLIP模型0.03%参数的深度伪造检测方法，通过L2归一化和潜在空间增强实现超球面特征流形，在13个基准数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;深度伪造检测器对未见过的操纵技术的泛化能力仍然是一个实际部署的挑战。尽管许多方法通过引入显著的架构复杂性来适应基础模型，但这项工作证明可以通过参数高效的适应实现稳健泛化。&lt;h4&gt;目的&lt;/h4&gt;解决深度伪造检测器对未见过的操纵技术的泛化挑战，提出一种计算效率高且可复现的方法，证明通过针对预训练CLIP模型进行目标性、最小化的修改可以实现最先进的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了LNCLIP-DF方法，仅微调层归一化参数（占总参数的0.03%），通过使用L2归一化和潜在空间增强来强制执行超球面特征流形，在2019年至2025年间的13个基准数据集上进行了广泛评估。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在来自同一源视频的成对真实-伪造数据上进行训练对于减轻快捷学习并提高泛化能力至关重要；2) 学术数据集上的检测难度并未随时间严格增加，在较旧、多样化的数据集上训练的模型显示出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一种计算效率高且可复现的方法，证明通过针对预训练CLIP模型进行目标性、最小化的修改可以实现最先进的泛化能力，代码将在接受后公开发布。&lt;h4&gt;翻译&lt;/h4&gt;深度伪造检测器对未见操纵技术的泛化能力仍是实际部署的挑战。尽管许多方法通过引入显著架构复杂性来适应基础模型，但本研究证明，通过对预训练的CLIP视觉编码器进行参数高效适应，可实现稳健泛化。所提出的方法LNCLIP-DF仅微调层归一化参数（占总数的0.03%），并通过使用L2归一化和潜在空间增强强制执行超球面特征流形来增强泛化能力。我们在2019年至2025年的13个基准数据集上进行了广泛评估。所提出的方法实现了最先进的性能，在平均跨数据集AUROC上优于更复杂、更近期的方法。我们的分析为该领域得出两个主要发现：1) 在来自同一源视频的成对真实-伪造数据上进行训练对于减轻快捷学习并提高泛化能力至关重要；2) 学术数据集上的检测难度并未随时间严格增加，在较旧、多样化数据集上训练的模型显示出强大的泛化能力。这项工作提供了一种计算效率高且可复现的方法，证明通过针对预训练CLIP模型进行目标性、最小化的修改可实现最先进的泛化能力。代码将在接受后公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generalization of deepfake detectors to unseen manipulation techniquesremains a challenge for practical deployment. Although many approaches adaptfoundation models by introducing significant architectural complexity, thiswork demonstrates that robust generalization is achievable through aparameter-efficient adaptation of a pre-trained CLIP vision encoder. Theproposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters(0.03% of the total) and enhances generalization by enforcing a hypersphericalfeature manifold using L2 normalization and latent space augmentations.  We conducted an extensive evaluation on 13 benchmark datasets spanning from2019 to 2025. The proposed method achieves state-of-the-art performance,outperforming more complex, recent approaches in average cross-dataset AUROC.Our analysis yields two primary findings for the field: 1) training on pairedreal-fake data from the same source video is essential for mitigating shortcutlearning and improving generalization, and 2) detection difficulty on academicdatasets has not strictly increased over time, with models trained on older,diverse datasets showing strong generalization capabilities.  This work delivers a computationally efficient and reproducible method,proving that state-of-the-art generalization is attainable by making targeted,minimal changes to a pre-trained CLIP model. The code will be made publiclyavailable upon acceptance.</description>
      <author>example@mail.com (Andrii Yermakov, Jan Cech, Jiri Matas, Mario Fritz)</author>
      <guid isPermaLink="false">2508.06248v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures</title>
      <link>http://arxiv.org/abs/2508.06127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages,recived by ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Vertex-Refining Simplicial Complex Attack (VeSCA)的新方法，用于评估Segment Anything Model (SAM)的可转移漏洞。VeSCA仅使用SAM的编码器生成可转移对抗样本，通过参数化单纯复数表征共享脆弱区域，并通过迭代顶点细化识别对抗有效区域。实验显示，VeSCA在三个下游模型类别和五个领域特定数据集上比最先进方法提高12.7%的性能。&lt;h4&gt;背景&lt;/h4&gt;Segment Anything Model (SAM)具有零样本能力，改变了交互式分割领域，但其固有漏洞可能导致许多下游应用失败。之前的对抗攻击方法在跨领域探索共同弱点方面不足，导致可转移性有限。&lt;h4&gt;目的&lt;/h4&gt;评估SAM的可转移漏洞，开发一种能够生成高度可转移对抗样本的新方法，以揭示这些漏洞对下游应用的潜在风险。&lt;h4&gt;方法&lt;/h4&gt;提出Vertex-Refining Simplicial Complex Attack (VeSCA)，通过以下步骤实现：(1)仅使用SAM的编码器生成对抗样本；(2)通过参数化单纯复数明确表征SAM和下游模型之间的共享脆弱区域；(3)通过迭代顶点细化识别对抗有效区域内的复数；(4)引入轻量级域自适应策略桥接域差异；(5)通过随机单纯复数采样生成一致可转移的对抗样本。&lt;h4&gt;主要发现&lt;/h4&gt;VeSCA在三个下游模型类别和五个领域特定数据集上比最先进方法提高了12.7%的性能，证明了其生成可转移对抗样本的有效性。研究还强调了SAM漏洞对下游模型的显著风险。&lt;h4&gt;结论&lt;/h4&gt;SAM的漏洞对下游模型构成重大风险，开发更强大的基础模型变得至关重要。本研究为评估基础模型的脆弱性提供了新方法，并强调了提高模型鲁棒性的紧迫性。&lt;h4&gt;翻译&lt;/h4&gt;虽然Segment Anything Model (SAM)通过零样本能力改变了交互式分割，但其固有漏洞呈现单点风险，可能导致许多下游应用失败。因此，主动评估这些可转移漏洞至关重要。针对SAM的先前对抗攻击往往因跨领域共同弱点的探索不足而表现出有限的转移性。为解决这一问题，我们提出了Vertex-Refining Simplicial Complex Attack (VeSCA)，一种仅使用SAM编码器来生成可转移对抗样本的新方法。具体而言，它通过参数化单纯复数明确表征SAM与下游模型之间的共享脆弱区域来实现这一点。我们的目标是通过迭代顶点细化在对抗有效区域内识别此类复数。引入了轻量级域自适应策略，在单纯复数初始化期间使用最少参考数据来桥接域差异。最终，VeSCA通过随机单纯复数采样生成一致可转移的对抗样本。大量实验证明，VeSCA在五个领域特定数据集上的三个下游模型类别中，比最先进方法提高了12.7%的性能。我们的研究结果进一步强调了SAM漏洞对下游模型的风险，并强调了开发更强大基础模型的紧迫性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While the Segment Anything Model (SAM) transforms interactive segmentationwith zero-shot abilities, its inherent vulnerabilities present a single-pointrisk, potentially leading to the failure of numerous downstream applications.Proactively evaluating these transferable vulnerabilities is thus imperative.Prior adversarial attacks on SAM often present limited transferability due toinsufficient exploration of common weakness across domains. To address this, wepropose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method thatleverages only the encoder of SAM for generating transferable adversarialexamples. Specifically, it achieves this by explicitly characterizing theshared vulnerable regions between SAM and downstream models through aparametric simplicial complex. Our goal is to identify such complexes withinadversarially potent regions by iterative vertex-wise refinement. A lightweightdomain re-adaptation strategy is introduced to bridge domain divergence usingminimal reference data during the initialization of simplicial complex.Ultimately, VeSCA generates consistently transferable adversarial examplesthrough random simplicial complex sampling. Extensive experiments demonstratethat VeSCA achieves performance improved by 12.7% compared to state-of-the-artmethods across three downstream model categories across five domain-specificdatasets. Our findings further highlight the downstream model risks posed bySAM's vulnerabilities and emphasize the urgency of developing more robustfoundation models.</description>
      <author>example@mail.com (Yi Qin, Rui Wang, Tao Huang, Tong Xiao, Liping Jing)</author>
      <guid isPermaLink="false">2508.06127v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges</title>
      <link>http://arxiv.org/abs/2508.06111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages and appendices&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SKATE是一种创新的评估框架，通过让大型语言模型相互生成和解决可验证任务来进行评估。该框架完全自动化、无需数据且可扩展，不需要人工输入或领域专业知识，能够实现客观且开放式的模型评估。&lt;h4&gt;背景&lt;/h4&gt;评估基础模型的能力和风险至关重要，但当前方法需要大量领域专业知识，限制了它们的可扩展性，随着模型快速发展，需要更有效的评估方法。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的评估框架SKATE，解决现有评估方法的局限性，创建一种可扩展、客观且开放式的评估方法，以跟上大型语言模型的快速发展。&lt;h4&gt;方法&lt;/h4&gt;在SKATE框架中，大型语言模型通过为彼此生成和解决可验证任务来竞争，将评估视为一种游戏，模型既是任务设置者又是解决者。研究引入了LLM设置的代码输出预测(COP)挑战作为可验证和可扩展的测试框架，并使用基于TrueSkill的排名系统评估六个前沿LLM。&lt;h4&gt;主要发现&lt;/h4&gt;较弱的模型可以可靠地区分和评分更强的模型；基于LLM的系统能够表现出自我偏好行为，生成与其自身能力相匹配的问题；SKATE自动揭示模型之间的细粒度能力差异。&lt;h4&gt;结论&lt;/h4&gt;研究成果朝着通用、可扩展的评估框架迈出了重要一步，这种评估框架能够跟上大型语言模型的进步。&lt;h4&gt;翻译&lt;/h4&gt;评估基础模型的能力和风险至关重要，然而当前方法需要大量领域专业知识，限制了它们随着这些模型快速演变而发展的可扩展性。我们引入了SKATE：一种新颖的评估框架，其中大型语言模型通过为彼此生成和解决可验证任务来竞争。我们的核心洞见是将评估视为一种游戏：模型既是任务设置者又是解决者，被激励创建能够突出自身优势同时暴露他人弱点的问题。SKATE提供了几个关键优势，平衡了可扩展性、开放性和客观性。它完全自动化、无需数据且可扩展，不需要人工输入或领域专业知识。通过使用可验证任务而非LLM裁判，评分是客观的。与领域有限的程序生成基准测试（如国际象棋或空间推理）不同，让LLM创造性地提出挑战能够实现开放且可扩展的评估。作为概念证明，我们引入了LLM设置的代码输出预测(COP)挑战作为一个可验证且可扩展的框架来测试我们的方法。使用基于TrueSkill的排名系统，我们评估了六个前沿LLM并发现：(1)较弱的模型可以可靠地区分和评分更强的模型，(2)基于LLM的系统能够表现出自我偏好行为，生成与其自身能力相匹配的问题，(3)SKATE自动揭示了模型之间的细粒度能力差异。我们的研究成果朝着通用、可扩展的评估框架迈出了重要一步，这种框架能够跟上大型语言模型的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating the capabilities and risks of foundation models is paramount, yetcurrent methods demand extensive domain expertise, hindering their scalabilityas these models rapidly evolve. We introduce SKATE: a novel evaluationframework in which large language models (LLMs) compete by generating andsolving verifiable tasks for one another. Our core insight is to treatevaluation as a game: models act as both task-setters and solvers, incentivizedto create questions which highlight their own strengths while exposing others'weaknesses. SKATE offers several key advantages, balancing scalability,open-endedness, and objectivity. It is fully automated, data-free, andscalable, requiring no human input or domain expertise. By using verifiabletasks rather than LLM judges, scoring is objective. Unlike domain-limitedprogrammatically-generated benchmarks (e.g. chess-playing or spatialreasoning), having LLMs creatively pose challenges enables open-ended andscalable evaluation. As a proof of concept, we introduce LLM-setcode-output-prediction (COP) challenges as a verifiable and extensibleframework in which to test our approach. Using a TrueSkill-based rankingsystem, we evaluate six frontier LLMs and find that: (1) weaker models canreliably differentiate and score stronger ones, (2) LLM-based systems arecapable of self-preferencing behavior, generating questions that align withtheir own capabilities, and (3) SKATE automatically surfaces fine-grainedcapability differences between models. Our findings are an important steptowards general, scalable evaluation frameworks which can keep pace with LLMprogress.</description>
      <author>example@mail.com (Dewi S. W. Gould, Bruno Mlodozeniec, Samuel F. Brown)</author>
      <guid isPermaLink="false">2508.06111v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline</title>
      <link>http://arxiv.org/abs/2508.06094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://conlangcrafter.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ConlangCrafter，一个利用大型语言模型作为计算创造力辅助工具的端到端人工语言创建系统。&lt;h4&gt;背景&lt;/h4&gt;人工语言如世界语和昆雅语在艺术、哲学和国际交流中扮演多种角色，同时大型基础模型已彻底改变了文本、图像等领域的创造性生成。&lt;h4&gt;目的&lt;/h4&gt;利用现代大型语言模型作为端到端人工语言创建的计算创造力辅助工具。&lt;h4&gt;方法&lt;/h4&gt;介绍了ConlangCrafter，一个多跳流水线，将语言设计分解为音系学、形态学、句法学、词汇生成和翻译等模块化阶段。在每个阶段，利用LLMs的元语言推理能力，注入随机性以鼓励多样性，并利用自我完善反馈来鼓励语言描述的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;ConlangCrafter能够在无需人类语言学专业知识的情况下，产生连贯且多样化的人工语言。&lt;h4&gt;结论&lt;/h4&gt;ConlangCrafter在衡量连贯性和类型学多样性的指标上表现出色，证明了其作为人工语言创建工具的有效性。&lt;h4&gt;翻译&lt;/h4&gt;本研究利用现代大型语言模型作为端到端人工语言创建的计算创造力辅助工具，提出了ConlangCrafter，一个将语言设计分解为音系学、形态学、句法学、词汇生成和翻译等模块化阶段的多跳流水线。在每个阶段，我们的方法利用了LLMs的元语言推理能力，注入随机性以鼓励多样性，并利用自我完善反馈来鼓励新兴语言描述的一致性。我们在衡量连贯性和类型学多样性的指标上评估了ConlangCrafter，展示了其无需人类语言学专业知识就能产生连贯且多样化的人工语言的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Constructed languages (conlangs) such as Esperanto and Quenya have playeddiverse roles in art, philosophy, and international communication. Meanwhile,large-scale foundation models have revolutionized creative generation in text,images, and beyond. In this work, we leverage modern LLMs as computationalcreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, amulti-hop pipeline that decomposes language design into modular stages --phonology, morphology, syntax, lexicon generation, and translation. At eachstage, our method leverages LLMs' meta-linguistic reasoning capabilities,injecting randomness to encourage diversity and leveraging self-refinementfeedback to encourage consistency in the emerging language description. Weevaluate ConlangCrafter on metrics measuring coherence and typologicaldiversity, demonstrating its ability to produce coherent and varied conlangswithout human linguistic expertise.</description>
      <author>example@mail.com (Morris Alper, Moran Yanuka, Raja Giryes, Gašper Beguš)</author>
      <guid isPermaLink="false">2508.06094v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?</title>
      <link>http://arxiv.org/abs/2508.06057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IGARSS 2025!&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了卫星光谱图像作为AGI发展中的重要模态，指出其尚未得到应有的关注，同时提出了需要更全面的基准来评估地球观测模型，并提出了一套应包含在基准中的任务。&lt;h4&gt;背景&lt;/h4&gt;通用人工智能(AGI)正接近现实，研究人员正在处理多种模态的数据，包括文本、图像、视频和音频。然而，卫星光谱图像作为一种有价值的模态尚未得到充分研究，尽管它在帮助AGI理解自然世界方面具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;论证地球观测数据对智能模型的有用性，回顾现有基准测试的局限性，并强调需要更全面的基准来评估地球观测模型。&lt;h4&gt;方法&lt;/h4&gt;通过回顾现有基准测试，分析它们在评估基础模型泛化能力方面的局限性，并提出一套应包含在基准中的全面任务。&lt;h4&gt;主要发现&lt;/h4&gt;现有基准测试在评估基础模型在地球观测领域的泛化能力方面存在局限性，需要开发更全面的基准来有效评估模型理解和处理地球观测数据的能力。&lt;h4&gt;结论&lt;/h4&gt;地球观测数据对于AGI的发展至关重要，需要开发更全面的基准测试来评估模型在这一领域的性能，并提出了一套应包含在基准中的任务。&lt;h4&gt;翻译&lt;/h4&gt;通用人工智能(AGI)比以往任何时候都更接近现实，引发了研究界收集和处理各种模态（包括文本、图像、视频和音频）的广泛热情。尽管最近有相关努力，但卫星光谱图像作为额外的模态尚未得到应有的关注。这个领域存在独特的挑战，但在推进AGI理解自然世界的能力方面也具有巨大潜力。在本文中，我们首先论证了地球观测数据对智能模型的有用性，然后回顾了现有的基准测试，并指出了它们在评估基础模型在该领域的泛化能力方面的局限性。本文强调需要更全面的基准来评估地球观测模型。为此，我们提出了一套基准应该包含的全面任务，以有效评估模型理解和处理地球观测数据的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial General Intelligence (AGI) is closer than ever to becoming areality, sparking widespread enthusiasm in the research community to collectand work with various modalities, including text, image, video, and audio.Despite recent efforts, satellite spectral imagery, as an additional modality,has yet to receive the attention it deserves. This area presents uniquechallenges, but also holds great promise in advancing the capabilities of AGIin understanding the natural world. In this paper, we argue why EarthObservation data is useful for an intelligent model, and then we reviewexisting benchmarks and highlight their limitations in evaluating thegeneralization ability of foundation models in this domain. This paperemphasizes the need for a more comprehensive benchmark to evaluate earthobservation models. To facilitate this, we propose a comprehensive set of tasksthat a benchmark should encompass to effectively assess a model's ability tounderstand and interact with Earth observation data.</description>
      <author>example@mail.com (Mojtaba Valipour, Kelly Zheng, James Lowman, Spencer Szabados, Mike Gartner, Bobby Braswell)</author>
      <guid isPermaLink="false">2508.06057v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>When a Paper Has 1000 Authors: Rethinking Citation Metrics in the Era of LLMs</title>
      <link>http://arxiv.org/abs/2508.06004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了SBCI指标，用于在大规模合作论文中评估个体研究者的贡献，解决了传统引用指标无法有效区分大规模论文中个体贡献者的问题。&lt;h4&gt;背景&lt;/h4&gt;作者级别引用指标是衡量学术影响力的实用工具，但在大型语言模型和基础模型领域，论文合作规模激增（如Gemini有1361名作者，19个月内被引用4600次），传统指标如总引用数和h-index无法有效区分个体贡献。&lt;h4&gt;目的&lt;/h4&gt;研究如何在大规模语言模型论文的数千名合作者中识别出突出的研究人员，这对学术招聘和资金决策等场景尤为重要。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新的引用指标SBCI，通过平衡大规模和小规模出版物中的贡献来解决这一挑战，分析了其理论特性，并在合成的出版物数据集上评估了其行为。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的SBCI指标在大规模合作时代为个体学术影响提供了更稳健和更具区分度的评估。&lt;h4&gt;结论&lt;/h4&gt;SBCI指标能够更好地评估大规模合作环境下的个体学术贡献，特别是在大型语言模型和基础模型领域。&lt;h4&gt;翻译&lt;/h4&gt;作者级别的引用指标在复杂的研究生态系统中提供了实用、可解释且可扩展的学术影响力信号，已被广泛用作招聘决策的代理指标。然而，过去五年大型语言模型和基础模型领域出现了大规模出版物，论文合作者从数百到数千不等，并在几个月内获得数万次引用。例如，Gemini有1361名作者，在19个月内被引用约4600次。在这种情况下，传统指标无法有效区分个体贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Author-level citation metrics provide a practical, interpretable, andscalable signal of scholarly influence in a complex research ecosystem. It hasbeen widely used as a proxy in hiring decisions. However, the past five yearshave seen the rapid emergence of large-scale publications in the field of largelanguage models and foundation models, with papers featuring hundreds tothousands of co-authors and receiving tens of thousands of citations withinmonths. For example, Gemini has 1361 authors and has been cited around 4600times in 19 months. In such cases, traditional metrics, such as total citationcount and the $h$-index, fail to meaningfully distinguish individualcontributions. Therefore, we propose the following research question: How canone identify standout researchers among thousands of co-authors in large-scaleLLM papers? This question is particularly important in scenarios such asacademic hiring and funding decisions. In this paper, we introduce a novelcitation metric designed to address this challenge by balancing contributionsacross large-scale and small-scale publications. We propose the SBCI index,analyze its theoretical properties, and evaluate its behavior on syntheticpublication datasets. Our results demonstrate that the proposed metric providesa more robust and discriminative assessment of individual scholarly impact inthe era of large-scale collaborations.</description>
      <author>example@mail.com (Weihang Guo, Zhao Song, Jiahao Zhang)</author>
      <guid isPermaLink="false">2508.06004v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models</title>
      <link>http://arxiv.org/abs/2508.05880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型语言模型(LLMs)如何通过认知维度进行情感推理，超越了传统的表面情感任务。研究引入了一个名为CoRE的大规模基准测试，用于评估LLMs在情感推理过程中使用的内部认知结构。&lt;h4&gt;背景&lt;/h4&gt;情感计算已成为人工智能系统全面发展的重要研究领域。过去的研究主要在监督方式下评估或训练LLMs，使用离散情感标签来处理情感相关任务，但这些评估通常局限于标准和表面的情感识别任务。&lt;h4&gt;目的&lt;/h4&gt;超越表面的情感任务，研究LLMs如何通过认知维度进行情感推理；检查LLMs在处理情感刺激时是否能产生连贯和合理的认知推理；引入CoRE基准测试评估LLMs的内部认知结构；回答模型是否更依赖特定认知维度、哪些认知维度对特定情感重要、以及LLMs中情感表征是否可通过认知维度解释三个问题。&lt;h4&gt;方法&lt;/h4&gt;基于认知评估理论，检查LLMs在处理情感刺激时的认知推理能力；引入CoRE大规模基准测试；进行大量评估实验和分析。&lt;h4&gt;主要发现&lt;/h4&gt;不同LLMs展现出多样化的推理模式；研究提供了新的基准测试和工具，可供进一步研究LLMs的情感认知能力。&lt;h4&gt;结论&lt;/h4&gt;研究揭示了不同LLMs在情感认知推理方面的多样性模式；CoRE基准测试和代码将公开可用，为未来研究提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;情感计算已被确立为一个关键研究领域，以推动人工智能(AI)系统的整体发展。基础模型--特别是大型语言模型(LLMs)--在过去的几项工作中已被评估、训练或指令调优，以成为更好的情感预测器或生成器。然而，大多数研究以监督方式处理情感相关任务，使用与刺激物(如文本、图像、视频、音频)相关的离散情感标签来评估或训练LLMs的能力。评估研究尤其局限于标准和表面的情感相关任务，如识别引发或表达的情感。在本文中，我们超越表面情感任务，研究LLMs如何通过认知维度进行情感推理。借鉴认知评估理论，我们检查LLMs在处理情感刺激时是否能产生连贯和合理的认知推理。我们引入了一个关于情感认知推理的大规模基准测试--CoRE--用于评估LLMs用于情感推理的内部认知结构。通过大量的评估实验和分析，我们试图回答：(a)模型是否更可能隐式依赖特定的认知评估维度？(b)哪些认知维度对于表征特定情感很重要？以及(c)LLMs中不同情感类别的内部表征是否可以通过认知评估维度来解释？我们的结果和分析揭示了不同LLMs之间的多样化推理模式。我们的基准测试和代码将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Affective Computing has been established as a crucial field of inquiry toadvance the holistic development of Artificial Intelligence (AI) systems.Foundation models -- especially Large Language Models (LLMs) -- have beenevaluated, trained, or instruction-tuned in several past works, to becomebetter predictors or generators of emotion. Most of these studies, however,approach emotion-related tasks in a supervised manner, assessing or trainingthe capabilities of LLMs using discrete emotion labels associated with stimuli(e.g., text, images, video, audio). Evaluation studies, in particular, haveoften been limited to standard and superficial emotion-related tasks, such asthe recognition of evoked or expressed emotions. In this paper, we move beyondsurface-level emotion tasks to investigate how LLMs reason about emotionsthrough cognitive dimensions. Drawing from cognitive appraisal theory, weexamine whether LLMs produce coherent and plausible cognitive reasoning whenreasoning about emotionally charged stimuli. We introduce a large-scalebenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internalcognitive structures implicitly used by LLMs for emotional reasoning. Through aplethora of evaluation experiments and analysis, we seek to answer: (a) Aremodels more likely to implicitly rely on specific cognitive appraisaldimensions?, (b) What cognitive dimensions are important for characterizingspecific emotions?, and, (c) Can the internal representations of differentemotion categories in LLMs be interpreted through cognitive appraisaldimensions? Our results and analyses reveal diverse reasoning patterns acrossdifferent LLMs. Our benchmark and code will be made publicly available.</description>
      <author>example@mail.com (Sree Bhattacharyya, Lucas Craig, Tharun Dilliraj, Jia Li, James Z. Wang)</author>
      <guid isPermaLink="false">2508.05880v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction</title>
      <link>http://arxiv.org/abs/2508.05838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Proceedings of the 2025 3rd International Conference  on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 figures, 1  table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的方法，将视觉基础模型与强化学习相结合，以增强在模拟环境中的物体交互能力。&lt;h4&gt;背景&lt;/h4&gt;在模拟环境中提高智能体的物体交互能力是机器人领域的重要挑战。&lt;h4&gt;目的&lt;/h4&gt;通过结合先进的视觉模型与强化学习算法，提高智能体在模拟环境中感知和与物体交互的能力。&lt;h4&gt;方法&lt;/h4&gt;结合Segment Anything Model (SAM)和YOLOv5与Proximal Policy Optimization (PPO)智能体，在AI2-THOR模拟环境中运行，并在四种不同的室内厨房环境中进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;与没有高级感知能力的基线智能体相比，物体交互成功率和导航效率显著提高，平均累积奖励增加68%，物体交互成功率提高52.5%，导航效率提高33%。&lt;h4&gt;结论&lt;/h4&gt;将基础模型与强化学习集成对于复杂的机器人任务具有潜力，为更复杂、更强大的自主智能体铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新颖的方法，将视觉基础模型与强化学习相结合，以增强在模拟环境中的物体交互能力。通过将Segment Anything Model (SAM)和YOLOv5与在AI2-THOR模拟环境中运行的Proximal Policy Optimization (PPO)智能体相结合，我们使智能体能够更有效地感知和与物体交互。我们在四种不同的室内厨房环境中进行的综合实验表明，与没有高级感知能力的基线智能体相比，物体交互成功率和导航效率有显著提高。结果显示平均累积奖励增加了68%，物体交互成功率提高了52.5%，导航效率提高了33%。这些发现强调了将基础模型与强化学习集成用于复杂机器人任务的潜力，为更复杂、更强大的自主智能体铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3747393.3747399&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel approach that integrates vision foundation modelswith reinforcement learning to enhance object interaction capabilities insimulated environments. By combining the Segment Anything Model (SAM) andYOLOv5 with a Proximal Policy Optimization (PPO) agent operating in theAI2-THOR simulation environment, we enable the agent to perceive and interactwith objects more effectively. Our comprehensive experiments, conducted acrossfour diverse indoor kitchen settings, demonstrate significant improvements inobject interaction success rates and navigation efficiency compared to abaseline agent without advanced perception. The results show a 68% increase inaverage cumulative reward, a 52.5% improvement in object interaction successrate, and a 33% increase in navigation efficiency. These findings highlight thepotential of integrating foundation models with reinforcement learning forcomplex robotic tasks, paving the way for more sophisticated and capableautonomous agents.</description>
      <author>example@mail.com (Ahmad Farooq, Kamran Iqbal)</author>
      <guid isPermaLink="false">2508.05838v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios</title>
      <link>http://arxiv.org/abs/2508.05829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TSMS-SAM2框架，通过多时间尺度视频采样增强和记忆分割剪枝机制，解决了外科视频中快速物体运动和记忆冗余的挑战，实现了更高效准确的可提示视频对象分割和跟踪。&lt;h4&gt;背景&lt;/h4&gt;随着Segment Anything Model 2等基础模型的出现，可提示的视频对象分割和跟踪取得了显著进展，但在外科视频分析中的应用仍面临复杂动态运动和记忆冗余导致的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出TSMS-SAM2框架，增强可提示视频对象分割和跟踪在外科视频中的应用效果，解决快速物体运动和记忆冗余问题。&lt;h4&gt;方法&lt;/h4&gt;TSMS-SAM2引入两种关键策略：多时间尺度视频采样增强提高对运动变化的鲁棒性，以及记忆分割和剪枝机制组织和过滤过去帧特征以实现更高效准确的分割。&lt;h4&gt;主要发现&lt;/h4&gt;在EndoVis2017和EndoVis2018数据集上评估，TSMS-SAM2分别获得95.24和86.73的最高平均Dice分数，优于之前的基于SAM和任务特定方法，消融研究证实了多时间尺度增强和记忆分割的有效性。&lt;h4&gt;结论&lt;/h4&gt;TSMS-SAM2框架在复杂外科场景中具有进行鲁棒、高效分割的潜力。&lt;h4&gt;翻译&lt;/h4&gt;可提示的视频对象分割和跟踪随着Segment Anything Model 2等基础模型的出现已取得显著进展；然而，由于其复杂的动态运动和阻碍有效学习的冗余记忆，它们在外科视频分析中的应用仍然具有挑战性。在这项工作中，我们提出了TSMS-SAM2，一个新颖的框架，通过解决SAM2中快速物体运动和记忆冗余的挑战，增强了外科视频中的可提示视频对象分割和跟踪。TSMS-SAM2引入了两个关键策略：多时间尺度视频采样增强，以提高对运动变化的鲁棒性；以及记忆分割和剪枝机制，组织和过滤过去帧特征以实现更高效和准确的分割。在EndoVis2017和EndoVis2018数据集上评估，TSMS-SAM2分别获得了95.24和86.73的最高平均Dice分数，优于之前的基于SAM和任务特定的方法。大量的消融研究证实了多时间尺度增强和记忆分割的有效性，突显了该框架在复杂外科场景中进行鲁棒、高效分割的潜力。我们的源代码将在https://github.com/apple1986/TSMS-SAM2上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Promptable video object segmentation and tracking (VOST) has seen significantadvances with the emergence of foundation models like Segment Anything Model 2(SAM2); however, their application in surgical video analysis remainschallenging due to complex motion dynamics and the redundancy of memory thatimpedes effective learning. In this work, we propose TSMS-SAM2, a novelframework that enhances promptable VOST in surgical videos by addressingchallenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2introduces two key strategies: multi-temporal-scale video sampling augmentationto improve robustness against motion variability, and a memory splitting andpruning mechanism that organizes and filters past frame features for moreefficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,respectively, outperforming prior SAM-based and task-specific methods.Extensive ablation studies confirm the effectiveness of multiscale temporalaugmentation and memory splitting, highlighting the framework's potential forrobust, efficient segmentation in complex surgical scenarios. Our source codewill be available at https://github.com/apple1986/TSMS-SAM2.</description>
      <author>example@mail.com (Guoping Xu, Hua-Chieh Shao, You Zhang)</author>
      <guid isPermaLink="false">2508.05829v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>CF3: Compact and Fast 3D Feature Fields</title>
      <link>http://arxiv.org/abs/2508.05254v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CF3的自上而下管道，用于构建紧凑快速的3D高斯特征场，通过多视图2D特征融合和自适应稀疏化方法，显著减少了高斯数量同时保持了几何细节。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting(3DGS)开始整合来自2D基础模型的信息，但大多数方法采用自下而上的优化过程，将原始2D特征视为真实值，导致计算成本增加。&lt;h4&gt;目的&lt;/h4&gt;提出一种自上而下的管道，构建紧凑且快速的3D高斯特征场，以降低计算成本并提高效率。&lt;h4&gt;方法&lt;/h4&gt;1) 使用预训练的高斯快速加权融合多视图2D特征；2) 直接在提升的特征上训练每个高斯的自动编码器，而非在2D域中训练；3) 引入自适应稀疏化方法，在优化特征场高斯属性的同时剪枝和合并冗余高斯。&lt;h4&gt;主要发现&lt;/h4&gt;自动编码器更好地与特征分布对齐，与Feature-3DGS相比，使用少至5%的高斯实现了具有竞争力的3D特征场。&lt;h4&gt;结论&lt;/h4&gt;CF3方法能够在保持几何细节的同时，构建高效的3D特征场表示。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯喷溅(3DGS)开始整合来自2D基础模型的丰富信息。然而，大多数方法依赖于自下而上的优化过程，将原始2D特征视为真实值，导致计算成本增加。我们提出了一种自上而下的管道，用于构建紧凑且快速的3D高斯特征场，即CF3。我们首先使用预训练的高斯对多视图2D特征进行快速加权融合。这种方法允许直接在提升的特征上训练每个高斯的自动编码器，而不是在2D域中训练自动编码器。因此，自动编码器更好地与特征分布对齐。更重要的是，我们引入了一种自适应稀疏化方法，在剪枝和合并冗余高斯的同时优化特征场的高斯属性，构建了保留几何细节的高效表示。与Feature-3DGS相比，我们的方法使用少至5%的高斯实现了具有竞争力的3D特征场。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何构建紧凑且快速的3D特征场问题。现有方法将2D基础模型的丰富信息整合到3D高斯溅射(3DGS)中时，需要大量高斯元和计算资源，导致存储和效率问题。这个问题在现实中很重要，因为高效紧凑的3D特征场可以实现实时语义理解、开放词汇查询等任务，使这些方法能在更大规模场景和实际应用中部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析现有方法如Feature-3DGS和LangSplat存在联合优化颜色和特征导致冗余、直接嵌入高维特征带来高成本的问题。因此提出自顶向下的管道，首先使用预训练3DGS进行快速多视角特征融合，然后训练每个高斯元的自编码器压缩特征，最后通过自适应稀疏化减少冗余高斯元。作者借鉴了FiT3D和CONDENSE的3D感知训练思想，LightGaussian的剪枝方法，以及高斯混合约简中的矩匹配方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过特征提升、压缩和稀疏化构建紧凑3D特征场，将特征直接存储在RGB通道替代颜色信息。整体流程分三步：1)特征提升：用预训练3DGS加权融合多视角2D特征，过滤噪声；2)特征压缩：训练每个高斯元的自编码器将高维特征压缩到3维；3)自适应稀疏化：优化高斯元属性，剪枝低价值高斯元，合并相邻语义相似的高斯元，使用马氏距离衡量重叠程度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)自顶向下的3D特征场构建方法；2)每个高斯元的自编码器直接在3D提升特征上训练；3)自适应稀疏化方法减少冗余高斯元；4)将压缩特征存储在RGB通道中与3DGS兼容。相比之前工作：CF3不联合优化颜色和特征，仅用5%高斯元实现相似性能；在3D域训练自编码器而非2D域，特征更一致；明确考虑为颜色优化的高斯元对特征表达的冗余性并针对性优化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CF3提出了一种自顶向下的3D特征场构建方法，通过特征提升、每个高斯元的自编码器压缩和自适应稀疏化，实现了使用仅5%高斯元的紧凑且快速的3D特征表示，同时保持与现有方法相当的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2Dfoundation models. However, most approaches rely on a bottom-up optimizationprocess that treats raw 2D features as ground truth, incurring increasedcomputational costs. We propose a top-down pipeline for constructing compactand fast 3D Gaussian feature fields, namely, CF3. We first perform a fastweighted fusion of multi-view 2D features with pre-trained Gaussians. Thisapproach enables training a per-Gaussian autoencoder directly on the liftedfeatures, instead of training autoencoders in the 2D domain. As a result, theautoencoder better aligns with the feature distribution. More importantly, weintroduce an adaptive sparsification method that optimizes the Gaussianattributes of the feature field while pruning and merging the redundantGaussians, constructing an efficient representation with preserved geometricdetails. Our approach achieves a competitive 3D feature field using as littleas 5% of the Gaussians compared to Feature-3DGS.</description>
      <author>example@mail.com (Hyunjoon Lee, Joonkyu Min, Jaesik Park)</author>
      <guid isPermaLink="false">2508.05254v2</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2508.06404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了V*，一种基于图的自动驾驶车辆运动规划器，能够在结构化环境中生成时间最优、无碰撞且满足动态和运动学约束的轨迹。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆在结构化环境中导航需要能够生成时间最优、无碰撞且满足动态和运动学约束的轨迹规划器。&lt;h4&gt;目的&lt;/h4&gt;开发一种名为V*的基于图的运动规划器，将速度和方向作为显式状态变量在离散化的空间-时间-速度格中表示。&lt;h4&gt;方法&lt;/h4&gt;通过搜索扩展过程中的动态图生成将运动维度直接整合到图中；使用六边形离散化策略管理高维搜索；为速度感知运动规划提供形式化数学证明；开发运动学自行车模型中瞬态转向动力学的数学公式；结合几何剪枝策略消除导致不可行转向配置的扩展。&lt;h4&gt;主要发现&lt;/h4&gt;V*能够评估动态允许的机动操作，确保轨迹物理可实现；在包含移动障碍物的复杂动态环境中能有效避免冲突、主动让行；能够生成具有时间推理能力的安全、高效轨迹，用于等待行为和动态协调。&lt;h4&gt;结论&lt;/h4&gt;V*是一种有效的自动驾驶车辆导航规划器，能够在复杂动态环境中生成安全、高效的轨迹。&lt;h4&gt;翻译&lt;/h4&gt;在结构化环境中自动驾驶车辆导航需要能够生成时间最优、无碰撞且满足动态和运动学约束的轨迹规划器。我们介绍了V*，一种基于图的运动规划器，它在离散化的空间-时间-速度格中将速度和方向表示为显式状态变量。与将空间搜索与动态可行性解耦或依赖后处理平滑的传统方法不同，V*通过搜索扩展过程中的动态图生成将运动维度直接整合到图中。为管理高维搜索的复杂性，我们采用六边形离散化策略，并提供形式化数学证明，建立速度感知运动规划在约束转向转换下的最优路径点间距和最小节点冗余。我们开发了运动学自行车模型中瞬态转向动力学的数学公式，模拟指数行为的转向角收敛，并推导收敛率参数的关系。这一理论基础结合几何剪枝策略，消除了导致不可行转向配置的扩展，使V*能够评估动态允许的机动操作，确保每条轨迹无需进一步细化即可物理实现。我们在包含移动障碍物的杂乱和动态环境中通过模拟研究进一步证明了V*的性能，展示了它避免冲突、主动让行以及生成具有时间推理能力的安全、高效轨迹的能力，用于等待行为和动态协调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicle navigation in structured environments requires plannerscapable of generating time-optimal, collision-free trajectories that satisfydynamic and kinematic constraints. We introduce V*, a graph-based motionplanner that represents speed and direction as explicit state variables withina discretised space-time-velocity lattice. Unlike traditional methods thatdecouple spatial search from dynamic feasibility or rely on post-hoc smoothing,V* integrates both motion dimensions directly into graph construction throughdynamic graph generation during search expansion. To manage the complexity ofhigh-dimensional search, we employ a hexagonal discretisation strategy andprovide formal mathematical proofs establishing optimal waypoint spacing andminimal node redundancy under constrained heading transitions forvelocity-aware motion planning. We develop a mathematical formulation fortransient steering dynamics in the kinematic bicycle model, modelling steeringangle convergence with exponential behaviour, and deriving the relationship forconvergence rate parameters. This theoretical foundation, combined withgeometric pruning strategies that eliminate expansions leading to infeasiblesteering configurations, enables V* to evaluate dynamically admissiblemanoeuvres, ensuring each trajectory is physically realisable without furtherrefinement. We further demonstrate V*'s performance in simulation studies withcluttered and dynamic environments involving moving obstacles, showing itsability to avoid conflicts, yield proactively, and generate safe, efficienttrajectories with temporal reasoning capabilities for waiting behaviours anddynamic coordination.</description>
      <author>example@mail.com (Abdullah Zareh Andaryan, Michael G. H. Bell, Mohsen Ramezani, Glenn Geers)</author>
      <guid isPermaLink="false">2508.06404v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Aligning Effective Tokens with Video Anomaly in Large Language Models</title>
      <link>http://arxiv.org/abs/2508.06350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为VA-GPT的新型多模态大语言模型，专门用于总结和定位各种视频中的异常事件。&lt;h4&gt;背景&lt;/h4&gt;理解视频中的异常事件是一项重要且具有挑战性的任务，已在广泛应用领域受到关注。然而，当前的视频理解多模态大语言模型虽然能分析一般视频，但由于异常事件在空间和时间上的稀疏性以及冗余信息，往往难以有效处理异常情况。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉和分析与异常事件相关的空间和时间信息，从而提供更准确响应和交互的视频异常事件理解模型。&lt;h4&gt;方法&lt;/h4&gt;提出VA-GPT模型，通过两个关键模块对齐视觉编码器和大型语言模型之间的有效令牌：空间有效令牌选择(SETS)和时间有效令牌生成(TETG)。此外，还构建了一个用于微调视频异常感知多模态大语言模型的指令遵循数据集，并基于XD-Violence数据集引入了跨领域评估基准。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多个基准测试上优于现有的最先进方法，能够更有效地处理视频中的异常事件。&lt;h4&gt;结论&lt;/h4&gt;VA-GPT模型通过有效利用视觉语言模型和大型语言模型的表示和泛化能力，解决了当前视频理解模型在处理异常事件时面临的挑战，为视频异常事件的理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解视频中的异常事件是一项重要且具有挑战性的任务，已在广泛的应用领域引起了显著关注。尽管当前的视频理解多模态大语言模型能够分析一般视频，但由于异常事件在空间和时间上的稀疏性以及冗余信息，它们往往难以处理异常情况。为了解决这些挑战，我们利用视觉语言模型和大型语言模型的表示和泛化能力，提出了VA-GPT，一种专为总结和定位各种视频中异常事件而设计的新型多模态大语言模型。我们的方法通过两个关键提出的模块有效地对齐视觉编码器和大型语言模型之间的有效令牌：空间有效令牌选择(SETS)和时间有效令牌生成(TETG)。这些模块使我们的模型能够有效地捕捉和分析与异常事件相关的空间和时间信息，从而提供更准确的响应和交互。此外，我们还构建了一个专门用于微调视频异常感知多模态大语言模型的指令遵循数据集，并基于XD-Violence数据集引入了跨领域评估基准。我们提出的方法在各种基准上都优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding abnormal events in videos is a vital and challenging task thathas garnered significant attention in a wide range of applications. Althoughcurrent video understanding Multi-modal Large Language Models (MLLMs) arecapable of analyzing general videos, they often struggle to handle anomaliesdue to the spatial and temporal sparsity of abnormal events, where theredundant information always leads to suboptimal outcomes. To address thesechallenges, exploiting the representation and generalization capabilities ofVison Language Models (VLMs) and Large Language Models (LLMs), we proposeVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events invarious videos. Our approach efficiently aligns effective tokens between visualencoders and LLMs through two key proposed modules: Spatial Effective TokenSelection (SETS) and Temporal Effective Token Generation (TETG). These modulesenable our model to effectively capture and analyze both spatial and temporalinformation associated with abnormal events, resulting in more accurateresponses and interactions. Furthermore, we construct an instruction-followingdataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce across-domain evaluation benchmark based on XD-Violence dataset. Our proposedmethod outperforms existing state-of-the-art methods on various benchmarks.</description>
      <author>example@mail.com (Yingxian Chen, Jiahui Liu, Ruifan Di, Yanwei Li, Chirui Chang, Shizhen Zhao, Wilton W. T. Fok, Xiaojuan Qi, Yik-Chung Wu)</author>
      <guid isPermaLink="false">2508.06350v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback</title>
      <link>http://arxiv.org/abs/2508.06292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 7 Tables, 6 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型多输出尖峰神经元模型，结合了线性SSM状态转换和非线性反馈机制，通过重置机制克服不稳定性，实现了与现有基准相当的性能。&lt;h4&gt;背景&lt;/h4&gt;神经形态计算是一种新兴技术，可实现低延迟和低能耗的信号处理。尖峰神经网络(SNNs)是其关键算法工具，使用有状态的神经元并通过尖峰编码信息。深度状态空间模型(SSMs)也使用有状态构建块，最近在时间建模任务中取得竞争性性能，但通常采用高精度激活函数且无重置机制。&lt;h4&gt;目的&lt;/h4&gt;结合SNNs和深度SSM模型的优势，提出一种新型多输出尖峰神经元模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合线性、通用SSM状态转换和非线性反馈机制的新型多输出尖峰神经元模型，通过重置机制实现非线性反馈。该模型明确区分了尖峰函数、重置条件和重置动作。在关键词发现、基于事件的视觉和顺序模式识别等任务上进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的模型在SNN文献中实现了与现有基准相当的性能。重置机制可以克服不稳定性，即使在线性部分神经元动力学不稳定的情况下也能实现学习，超越了最近深度SSM模型中严格 enforced 的线性动力学稳定性。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型结合了SNNs和深度SSMs的优势，通过重置机制实现了更灵活的学习能力，能够处理不稳定的线性动力学。&lt;h4&gt;翻译&lt;/h4&gt;神经形态计算是一种新兴技术，可实现低延迟和低能耗的信号处理。神经形态计算中的关键算法工具是尖峰神经网络(SNNs)。SNNs是受生物启发的神经网络，使用有状态的神经元，并通过尖峰对信息进行编码和解码，实现低比特数据处理。类似于SNNs，深度状态空间模型(SSMs)也使用有状态的构建块。然而，最近在各种时间建模任务中取得竞争性性能的深度SSMs通常设计为高精度激活函数且无重置机制。为了结合SNNs和最近深度SSM模型的优势，我们提出了一种新型多输出尖峰神经元模型，该模型将线性的通用SSM状态转换通过重置机制与非线性反馈相结合。与现有的SNN神经元模型相比，我们提出的模型明确区分了尖峰函数、重置条件和重置动作。在关键词发现任务、基于事件的视觉任务和顺序模式识别任务等各种任务上的实验结果表明，我们提出的模型在SNN文献中实现了与现有基准相当的性能。我们的结果说明了所提出的重置机制如何能够克服不稳定性，即使在线性部分神经元动力学不稳定的情况下也能实现学习，使我们能够超越最近深度SSM模型中严格 enforced 的线性动力学稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/JSTSP.2025.3595030&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neuromorphic computing is an emerging technology enabling low-latency andenergy-efficient signal processing. A key algorithmic tool in neuromorphiccomputing is spiking neural networks (SNNs). SNNs are biologically inspiredneural networks which utilize stateful neurons, and provide low-bit dataprocessing by encoding and decoding information using spikes. Similar to SNNs,deep state-space models (SSMs) utilize stateful building blocks. However, deepSSMs, which recently achieved competitive performance in various temporalmodeling tasks, are typically designed with high-precision activation functionsand no reset mechanisms. To bridge the gains offered by SNNs and the recentdeep SSM models, we propose a novel multiple-output spiking neuron model thatcombines a linear, general SSM state transition with a non-linear feedbackmechanism through reset. Compared to the existing neuron models for SNNs, ourproposed model clearly conceptualizes the differences between the spikingfunction, the reset condition and the reset action. The experimental results onvarious tasks, i.e., a keyword spotting task, an event-based vision task and asequential pattern recognition task, show that our proposed model achievesperformance comparable to existing benchmarks in the SNN literature. Ourresults illustrate how the proposed reset mechanism can overcome instabilityand enable learning even when the linear part of neuron dynamics is unstable,allowing us to go beyond the strictly enforced stability of linear dynamics inrecent deep SSM models.</description>
      <author>example@mail.com (Sanja Karilanova, Subhrakanti Dey, Ayça Özçelikkale)</author>
      <guid isPermaLink="false">2508.06292v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology</title>
      <link>http://arxiv.org/abs/2508.06066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究为深度时间模型提供了首个非平凡、架构感知的泛化边界和原则性评估方法，发现时间依赖性可在固定信息预算下增强学习，同时揭示了理论与实践之间的差距。&lt;h4&gt;背景&lt;/h4&gt;深度时间架构如时间卷积网络在序列数据上表现出强大的预测性能，但其泛化能力的理论理解仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提供深度时间模型的首个非平凡、架构感知的泛化边界，并建立一种原则性的评估方法。&lt;h4&gt;方法&lt;/h4&gt;对于指数β混合序列，推导出边界为O(R√(Dpn log N/N))，其中D是网络深度，p是核大小，n是输入维度，R是权重范数；提出延迟反馈阻塞机制将相关样本转化为有效独立样本；引入公平比较方法固定有效样本大小以分离时间结构与信息内容的影响。&lt;h4&gt;主要发现&lt;/h4&gt;延迟反馈阻塞机制实现√D缩放而非指数缩放，意味着加倍深度大约需要四倍训练数据；强相关序列比弱相关序列表现出约76%更小的泛化差距；弱依赖遵循N_eff^(-1.21)缩放，强依赖遵循N_eff^(-0.89)，都比预测的N^(-0.5)更陡峭。&lt;h4&gt;结论&lt;/h4&gt;时间依赖性可以在固定信息预算下增强学习，同时理论与实践之间存在差距，这为未来研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;深度时间架构如时间卷积网络在序列数据上实现了强大的预测性能，但其泛化能力的理论理解仍然有限。我们通过提供深度时间模型的首个非平凡、架构感知的泛化边界和一种原则性评估方法来填补这一空白。对于指数β混合序列，我们推导出边界为O(R√(Dpn log N/N))，其中D是网络深度，p是核大小，n是输入维度，R是权重范数。我们的延迟反馈阻塞机制将相关样本转化为有效独立样本，同时仅丢弃O(1/log N)的数据，实现了√D缩放而非指数缩放，意味着加倍深度大约需要四倍训练数据。我们还引入了一种公平比较方法，固定有效样本大小以分离时间结构的影响与信息内容的影响。在N_eff=2,000的情况下，强相关序列(ρ=0.8)比弱相关序列(ρ=0.2)表现出约76%更小的泛化差距，挑战了依赖性纯粹有害的直觉。然而，收敛率与理论预测不符：弱依赖遵循N_eff^(-1.21)缩放，强依赖遵循N_eff^(-0.89)，都比预测的N^(-0.5)更陡峭。这些发现揭示了时间依赖性可以在固定信息预算下增强学习，同时突显了理论与实践之间的差距，这激励了未来的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep temporal architectures such as Temporal Convolutional Networks (TCNs)achieve strong predictive performance on sequential data, yet theoreticalunderstanding of their generalization remains limited. We address this gap byproviding both the first non-vacuous, architecture-aware generalization boundsfor deep temporal models and a principled evaluation methodology.  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is networkdepth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Ourdelayed-feedback blocking mechanism transforms dependent samples intoeffectively independent ones while discarding only $O(1/\log N)$ of the data,yielding $\sqrt{D}$ scaling instead of exponential, implying that doublingdepth requires approximately quadrupling the training data.  We also introduce a fair-comparison methodology that fixes the effectivesample size to isolate the effect of temporal structure from informationcontent. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weaklydependent ones ($\rho=0.2$), challenging the intuition that dependence ispurely detrimental. Yet convergence rates diverge from theory: weakdependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependenciesfollow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.These findings reveal that temporal dependence can enhance learning under fixedinformation budgets, while highlighting gaps between theory and practice thatmotivate future research.</description>
      <author>example@mail.com (Barak Gahtan, Alex M. Bronstein)</author>
      <guid isPermaLink="false">2508.06066v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating Quantum Monte Carlo Calculations with Set-Equivariant Architectures and Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.06441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何使用transformer架构显著加速或绕过变分量子蒙特卡洛计算中的可观测量评估步骤，特别是在处理磁化强度幂次等耗时算子时。通过多种复杂度递增的示例，从经典伊辛模型到具有长程相互作用的量子系统，包括回归和分类任务，证明了该方法的有效性。此外，研究还展示了如何利用迁移学习通过重用不同系统和较小系统规模的知识来降低训练成本。&lt;h4&gt;背景&lt;/h4&gt;机器学习方法（ML）扩展了变分量子蒙特卡洛（QMC）计算的准确性和应用范围，特别是在探索自旋系统表现出的各种量子现象方面。然而，QMC的可扩展性仍然受到其他瓶颈的限制，特别是与基于随机偏差的实际可观测量评估相关的问题。&lt;h4&gt;目的&lt;/h4&gt;展示如何使用transformer架构来显著加速甚至绕过QMC中基于随机偏差的可观测量评估步骤，特别是对于耗时的算子，如磁化强度的幂次。&lt;h4&gt;方法&lt;/h4&gt;通过一系列复杂度递增的例子来说明这一过程，从经典伊辛模型到具有长程相互作用的量子系统，包括回归（预测可观测量）和分类（检测相变）任务。此外，还探讨了如何利用迁移学习通过重用不同系统和较小系统规模的知识来降低训练成本。&lt;h4&gt;主要发现&lt;/h4&gt;transformer架构可以显著加速QMC中的可观测量评估，甚至可以绕过某些计算步骤；该方法适用于从经典到量子的多种系统；迁移学习可以进一步优化计算效率，通过重用已有知识减少训练成本。&lt;h4&gt;结论&lt;/h4&gt;transformer架构在QMC计算中具有显著的加速潜力，特别是在处理复杂算子时，而迁移学习可以进一步优化计算效率，为解决QMC可扩展性问题提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;机器学习方法（ML）极大地扩展了变分量子蒙特卡洛（QMC）计算的准确性和应用范围，特别是在探索自旋系统表现出的各种量子现象时。然而，QMC的可扩展性仍然受到其他几个瓶颈的限制，特别是与基于随机偏差的实际可观测量评估相关的问题，这是该方法的核心所在。在本文中，我们展示了如何使用transformer架构来显著加速甚至绕过这一步骤，特别是对于磁化强度幂次等耗时算子。我们通过一系列复杂度递增的例子来说明这一过程，从经典伊辛模型到具有长程相互作用的量子系统，包括回归（预测可观测量）和分类（检测相变）。此外，我们还展示了如何利用迁移学习通过重用不同系统和较小系统规模的知识来降低训练成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learning (ML) ans\"atze have greatly expanded the accuracy and reachof variational quantum Monte Carlo (QMC) calculations, in particular whenexploring the manifold quantum phenomena exhibited by spin systems. However,the scalability of QMC is still compromised by several other bottlenecks, andspecifically those related to the actual evaluation of observables based onrandom deviates that lies at the core of the approach. Here we show how theset-transformer architecture can be used to dramatically accelerate or evenbypass that step, especially for time-consuming operators such as powers of themagnetization. We illustrate the procedure with a range of examples ofincreasing complexity, from the classical Ising model to quantum systems withlong-range interactions, and comprising both regressions (to predictobservables) and classifications (to detect phase transitions). Moreover, weshow how transfer learning can be leveraged to reduce the training cost byreusing knowledge from different systems and smaller system sizes.</description>
      <author>example@mail.com (Manuel Gallego, Sebastián Roca-Jerat, David Zueco, Jesús Carrete)</author>
      <guid isPermaLink="false">2508.06441v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation</title>
      <link>http://arxiv.org/abs/2508.06429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于GAN的半监督学习框架，专为医学影像中标记数据稀少的场景设计，通过结合生成器、判别器和分类器，在极少量标记数据条件下实现了显著优于现有方法的分类性能。&lt;h4&gt;背景&lt;/h4&gt;深度学习已彻底改变医学影像领域，但其效果受到标记训练数据不足的严重限制，尤其是在医学影像应用中标注成本高昂的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于GAN的半监督学习框架，专门为标记数据稀少的场景设计，在每类5到50个标记样本的设置下进行评估。&lt;h4&gt;方法&lt;/h4&gt;整合三个专门神经网络（类别条件图像转换的生成器、真实性和分类评估的判别器、专用分类器）在三阶段训练框架中工作，交替进行监督训练和利用未标记图像的无监督学习，采用基于集成方法的伪标记，结合判别器和分类器的置信度加权预测，并通过指数移动平均实现时间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在十一个MedMNIST数据集上的评估表明，该方法比六种最先进的基于GAN的半监督方法取得统计学显著改进，特别是在5-shot设置中表现尤为突出，且在所有评估设置（5、10、20和50 shot/类）中均保持优越性。&lt;h4&gt;结论&lt;/h4&gt;该方法为标记成本过高的医学影像应用提供了实用解决方案，即使使用最少的标记数据也能实现强大的分类性能，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;深度学习彻底改变了医学影像，但其效果因标记训练数据不足而受到严重限制。本文介绍了一种基于GAN的新型半监督学习框架，专为标记数据稀少的场景设计，在每类5到50个标记样本的设置下进行了评估。我们的方法整合了三个专门神经网络——用于类别条件图像转换的生成器、用于真实性和分类评估的判别器，以及一个专门的分类器——在一个三阶段训练框架内。该方法在有限标记数据上的监督训练和通过图像到图像转换（而非从噪声生成）利用大量未标记图像的无监督学习之间交替进行。我们采用基于集成方法的伪标记，结合判别器和分类器的置信度加权预测，并通过指数移动平均实现时间一致性，从而能够可靠地估计未标记数据的标签。在十一个MedMNIST数据集上的全面评估表明，我们的方法比六种最先进的基于GAN的半监督方法取得了统计学上的显著改进，特别是在标记数据最稀缺的5-shot设置中表现尤为突出。该框架在所有评估设置（每类5、10、20和50个样本）中都保持着其优越性。我们的方法为标记成本过高的医学影像应用提供了实用的解决方案，即使使用最少的标记数据也能实现强大的分类性能。代码可在https://github.com/GuidoManni/SPARSE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has revolutionized medical imaging, but its effectiveness isseverely limited by insufficient labeled training data. This paper introduces anovel GAN-based semi-supervised learning framework specifically designed forlow labeled-data regimes, evaluated across settings with 5 to 50 labeledsamples per class. Our approach integrates three specialized neural networks --a generator for class-conditioned image translation, a discriminator forauthenticity assessment and classification, and a dedicated classifier --within a three-phase training framework. The method alternates betweensupervised training on limited labeled data and unsupervised learning thatleverages abundant unlabeled images through image-to-image translation ratherthan generation from noise. We employ ensemble-based pseudo-labeling thatcombines confidence-weighted predictions from the discriminator and classifierwith temporal consistency through exponential moving averaging, enablingreliable label estimation for unlabeled data. Comprehensive evaluation acrosseleven MedMNIST datasets demonstrates that our approach achieves statisticallysignificant improvements over six state-of-the-art GAN-based semi-supervisedmethods, with particularly strong performance in the extreme 5-shot settingwhere the scarcity of labeled data is most challenging. The framework maintainsits superiority across all evaluated settings (5, 10, 20, and 50 shots perclass). Our approach offers a practical solution for medical imagingapplications where annotation costs are prohibitive, enabling robustclassification performance even with minimal labeled data. Code is available athttps://github.com/GuidoManni/SPARSE.</description>
      <author>example@mail.com (Guido Manni, Clemente Lauretti, Loredana Zollo, Paolo Soda)</author>
      <guid isPermaLink="false">2508.06429v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?</title>
      <link>http://arxiv.org/abs/2508.06327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICONIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于扩散模型的合成数据生成方法，用于解决心脏磁共振成像中的域偏移问题。该方法生成的合成数据保持了空间和结构保真度，与源域相似且兼容分割掩码。通过域泛化和域适应两种策略，该方法显著提高了分割模型在未见目标域上的性能，减轻了对迁移学习或在线训练的需求，特别适用于数据稀缺的场景。&lt;h4&gt;背景&lt;/h4&gt;磁共振成像（包括心脏磁共振）容易因成像设备和采集协议的不同而产生域偏移问题，这限制了训练好的AI模型在真实场景中的应用，因为模型在未见过的域上性能会下降。&lt;h4&gt;目的&lt;/h4&gt;解决心脏磁共振成像中的域偏移问题，提高AI模型在真实场景中的泛化能力，特别是在数据稀缺的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出一种在源域上训练的扩散模型（DM），生成与给定参考相似的心脏磁共振图像。通过域泛化（在合成的源域数据上训练域不变的分割模型）和域适应（使用DM将目标域数据向源域迁移）两种策略评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;两种策略都比仅使用真实数据训练分割模型在未见目标域上的分割性能有显著提高（基于表面度量，Welch's t-test, p &lt; 0.01）。&lt;h4&gt;结论&lt;/h4&gt;提出的方法减轻了为解决心脏磁共振图像分析中的域偏移问题而进行迁移学习或在线训练的需要，特别是在数据稀缺的情况下特别有用。&lt;h4&gt;翻译&lt;/h4&gt;磁共振成像（包括心脏磁共振）容易因成像设备和采集协议的不同而产生域偏移。这一挑战限制了训练好的AI模型在真实场景中的应用，因为在未见过的域上性能会下降。传统解决方案通过临时图像增强或额外的在线训练/迁移学习来增加数据集大小，但存在一些局限性。合成数据是一个有前途的替代方案，但解剖/结构一致性约束限制了生成模型在创建图像-标签对方面的有效性。为此，我们提出了一种在源域上训练的扩散模型（DM），能够生成与给定参考相似的心脏磁共振图像。合成的数据保持了空间和结构保真度，确保与源域的相似性和与分割掩码的兼容性。我们在多中心心脏磁共振分割任务中评估了我们的生成方法，使用了2D nnU-Net、3D nnU-Net和普通U-Net分割网络。我们探索了域泛化（在合成的源域数据上训练域不变的分割模型）和域适应（使用DM将目标域数据向源域迁移）两种策略。与仅使用真实数据训练分割模型相比，两种策略在未见目标域上的分割性能都有显著提高（基于表面度量，Welch's t-test, p &lt; 0.01）。提出的方法减轻了为解决心脏磁共振图像分析中的域偏移问题而进行迁移学习或在线训练的需要，特别适用于数据稀缺的场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnetic resonance (MR) imaging, including cardiac MR, is prone to domainshift due to variations in imaging devices and acquisition protocols. Thischallenge limits the deployment of trained AI models in real-world scenarios,where performance degrades on unseen domains. Traditional solutions involveincreasing the size of the dataset through ad-hoc image augmentation oradditional online training/transfer learning, which have several limitations.Synthetic data offers a promising alternative, but anatomical/structuralconsistency constraints limit the effectiveness of generative models increating image-label pairs. To address this, we propose a diffusion model (DM)trained on a source domain that generates synthetic cardiac MR images thatresemble a given reference. The synthetic data maintains spatial and structuralfidelity, ensuring similarity to the source domain and compatibility with thesegmentation mask. We assess the utility of our generative approach inmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net andvanilla U-Net segmentation networks. We explore domain generalisation, where,domain-invariant segmentation models are trained on synthetic source domaindata, and domain adaptation, where, we shift target domain data towards thesource domain using the DM. Both strategies significantly improved segmentationperformance on data from an unseen target domain, in terms of surface-basedmetrics (Welch's t-test, p &lt; 0.01), compared to training segmentation models onreal data alone. The proposed method ameliorates the need for transfer learningor online training to address domain shift challenges in cardiac MR imageanalysis, especially useful in data-scarce settings.</description>
      <author>example@mail.com (Xin Ci Wong, Duygu Sarikaya, Kieran Zucker, Marc De Kamps, Nishant Ravikumar)</author>
      <guid isPermaLink="false">2508.06327v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models</title>
      <link>http://arxiv.org/abs/2508.05685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review. Code will be released upon acceptance&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Domain-guided Fine-tuning (DogFit)方法，这是一种用于扩散模型迁移学习的有效引导机制，能够在保持可控性的同时不增加额外的计算开销。DogFit通过将领域感知的引导偏置注入训练损失中，在微调过程中内部化引导行为，并通过轻量级条件机制将引导强度编码为额外模型输入。实验表明，DogFit在多个指标上优于之前的引导方法，同时计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;将扩散模型迁移到较小的目标域具有挑战性，因为简单地微调模型通常会导致泛化能力差。测试时引导方法通过在图像保真度和样本多样性之间进行权衡，有助于缓解这一问题，但这些方法通常需要双向前向传递，计算成本高。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的引导机制，用于扩散模型的迁移学习，能够在保持可控性的同时不增加额外的计算开销。&lt;h4&gt;方法&lt;/h4&gt;提出了Domain-guided Fine-tuning (DogFit)方法，将领域感知的引导偏置注入训练损失中，在微调过程中内部化引导行为。通过轻量级条件机制将引导强度值编码为额外模型输入。研究了训练过程中引导偏置的最佳位置和时间，提出了两种简单的调度策略：late-start和cut-off。&lt;h4&gt;主要发现&lt;/h4&gt;在DiT和SiT骨干网络上，在六个不同的目标域上进行的实验表明，DogFit在FID和FDDINOV2指标上优于之前的引导方法，同时采样所需的TFLOPS减少高达2倍。&lt;h4&gt;结论&lt;/h4&gt;DogFit是一种有效的扩散迁移学习引导机制，能够在保持可控性的同时不增加额外的计算开销，实现更高效的保真度-多样性权衡。&lt;h4&gt;翻译&lt;/h4&gt;将扩散模型迁移到较小的目标域具有挑战性，因为简单地微调模型通常会导致泛化能力差。测试时引导方法通过在图像保真度和样本多样性之间进行权衡，有助于缓解这一问题，但这些方法通常需要双向前向传递，计算成本高。我们提出了Domain-guided Fine-tuning (DogFit)方法，这是一种用于扩散迁移学习的有效引导机制，能够在保持可控性的同时不增加额外的计算开销。DogFit将领域感知的引导偏置注入训练损失中，在微调过程中内部化引导行为。领域感知设计基于我们的观察：在微调过程中，无条件源模型比目标模型提供更强的边际估计。为了在推理时支持高效的保真度-多样性权衡，我们通过轻量级条件机制将引导强度值编码为额外模型输入。我们进一步研究了训练过程中引导偏置的最佳位置和时间，并提出了两种简单的调度策略，即late-start和cut-off，这些策略提高了生成质量和训练稳定性。在DiT和SiT骨干网络上，在六个不同的目标域上进行的实验表明，DogFit在FID和FDDINOV2指标上优于之前的引导方法，同时采样所需的TFLOPS减少高达2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning of diffusion models to smaller target domains ischallenging, as naively fine-tuning the model often results in poorgeneralization. Test-time guidance methods help mitigate this by offeringcontrollable improvements in image fidelity through a trade-off with samplediversity. However, this benefit comes at a high computational cost, typicallyrequiring dual forward passes during sampling. We propose the Domain-guidedFine-tuning (DogFit) method, an effective guidance mechanism for diffusiontransfer learning that maintains controllability without incurring additionalcomputational overhead. DogFit injects a domain-aware guidance offset into thetraining loss, effectively internalizing the guided behavior during thefine-tuning process. The domain-aware design is motivated by our observationthat during fine-tuning, the unconditional source model offers a strongermarginal estimate than the target model. To support efficient controllablefidelity-diversity trade-offs at inference, we encode the guidance strengthvalue as an additional model input through a lightweight conditioningmechanism. We further investigate the optimal placement and timing of theguidance offset during training and propose two simple scheduling strategies,i.e., late-start and cut-off, which improve generation quality and trainingstability. Experiments on DiT and SiT backbones across six diverse targetdomains show that DogFit can outperform prior guidance methods in transferlearning in terms of FID and FDDINOV2 while requiring up to 2x fewer samplingTFLOPS.</description>
      <author>example@mail.com (Yara Bahram, Mohammadhadi Shateri, Eric Granger)</author>
      <guid isPermaLink="false">2508.05685v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    </channel>
</rss>