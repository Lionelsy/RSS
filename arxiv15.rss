<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 31 Dec 2025 14:39:23 +0800</lastBuildDate>
    <item>
      <title>TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References</title>
      <link>http://arxiv.org/abs/2512.21641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为TrackTeller的时间多模态定位框架，用于解决动态3D驾驶场景中通过自然语言引用物体的理解问题。&lt;h4&gt;背景&lt;/h4&gt;理解动态3D驾驶场景中的自然语言物体引用对交互式自动驾驶系统至关重要。许多指代表达通过最近运动或短期交互来描述目标，仅从静态外观或几何形状无法解决。&lt;h4&gt;目的&lt;/h4&gt;研究基于时间语言的3D定位问题，即利用多帧观察来识别当前帧中被指代的物体。&lt;h4&gt;方法&lt;/h4&gt;提出TrackTeller框架，整合了激光雷达-图像融合、语言条件解码和时间推理；构建与文本语义对齐的共享UniScene表示；生成语言感知的3D提案；并使用运动历史和短期动态优化定位决策。&lt;h4&gt;主要发现&lt;/h4&gt;在NuPrompt基准测试上，TrackTeller显著提高了语言引导的跟踪性能，相比强基线，平均多目标跟踪精度相对提高了70%，虚警频率降低了3.15-3.4倍。&lt;h4&gt;结论&lt;/h4&gt;TrackTeller框架在时间语言基础的3D定位任务上表现优异，能够有效解决动态场景中基于运动和交互的物体引用理解问题。&lt;h4&gt;翻译&lt;/h4&gt;理解动态3D驾驶场景中对物体的自然语言引用对交互式自动驾驶系统至关重要。实际上，许多指代表达通过最近运动或短期交互来描述目标，这无法仅从静态外观或几何形状中解决。我们研究了基于时间语言的3D定位，其目标是通过利用多帧观察来识别当前帧中被指代的物体。我们提出了TrackTeller，一种时间多模态定位框架，将激光雷达-图像融合、语言条件解码和时间推理统一在一个架构中。TrackTeller构建了与文本语义对齐的共享UniScene表示，生成语言感知的3D提案，并使用运动历史和短期动态来优化定位决策。在NuPrompt基准上的实验表明，TrackTeller持续提高了语言引导的跟踪性能，相比强基线，平均多目标跟踪精度相对提高了70%，虚警频率降低了3.15-3.4倍。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态3D驾驶场景中的时间语言基础3D定位问题，特别是如何根据物体的最近运动或短期交互行为来识别自然语言描述的目标。这个问题在自动驾驶系统中至关重要，因为现实场景中许多指代表达（如'刚刚穿过街道的那辆白色汽车'）无法仅通过静态图像或几何信息解决，需要系统能够理解物体的时间行为和动态变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，指出大多数3D定位工作主要针对静态场景或单帧快照，无法处理基于时间行为的表达。他们识别出两个关键挑战：复杂的多传感器融合（需要整合LiDAR几何、相机语义和语言提示）和时间模糊性（需要多帧推理来理解物体的近期行为）。基于这些分析，作者设计了TrackTeller框架，借鉴了现有工作中的BEV表示、语言引导的视觉定位、查询跟踪框架和提示推理等技术，但将它们整合到一个专门解决时间动态问题的统一架构中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建统一的UniScene表示整合多模态感知信息，将语言语义与场景表示对齐，并融入时间推理来理解物体的行为动态。整体流程包括：1) 将LiDAR点和多视图图像融合成UniScene标记，保留几何和语义信息；2) 通过语言到场景调制将语言表达与场景标记对齐；3) 使用语言引导的3D解码器生成候选物体提案；4) 通过时间推理模块利用历史上下文和短期动态信息增强提案；5) 最终选择与语言描述和时间行为最匹配的物体作为定位结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次针对自动驾驶场景提出时间语言基础3D定位任务，专门处理基于物体近期行为的表达；2) 设计语言对齐的3D解码框架，将文本语义注入统一的多模态表示；3) 引入语言感知的时间推理模块，结合记忆注意力和运动线索。相比之前的工作，TrackTeller不仅处理静态场景，还能理解物体的时间动态；不仅使用单一模态，而是整合了LiDAR、图像和语言信息；不仅关注空间关系，还明确建模了物体行为和短期交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrackTeller通过整合多传感器感知、语言对齐和时间推理，首次实现了在动态驾驶场景中基于物体近期行为和运动的语言引导3D物体定位，显著提升了自动驾驶系统的交互能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.</description>
      <author>example@mail.com (Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng)</author>
      <guid isPermaLink="false">2512.21641v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
  <item>
      <title>Multimodal Functional Maximum Correlation for Emotion Recognition</title>
      <link>http://arxiv.org/abs/2512.23076v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  manuscript currently under review at IEEE journals, 20 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为多模态功能最大相关（MFMC）的自监督学习框架，用于情感计算中的多模态表示学习，通过最大化高阶多模态依赖关系直接捕捉联合多模态交互，无需依赖成对对比损失。&lt;h4&gt;背景&lt;/h4&gt;情感状态表现为中枢和自主神经系统中的协调但异质性的生理反应，这对多模态表示学习构成挑战。情感标注的稀缺性和主观性进一步增加了学习难度，促使使用自监督学习，但现有方法主要依赖成对对齐目标，无法有效捕捉多模态间的高阶交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有自监督学习方法在表征多模态依赖关系和高阶交互方面的局限性，提出一种能够直接捕捉联合多模态交互的学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出多模态功能最大相关（MFMC）框架，通过双重总相关（DTC）目标最大化高阶多模态依赖关系。通过推导紧密的三明治界限并使用基于功能最大相关分析（FMCA）的迹代理进行优化，直接捕捉多模态交互而不依赖成对对比损失。&lt;h4&gt;主要发现&lt;/h4&gt;在三个情感计算基准测试上，MFMC在受试者依赖和受试者独立评估协议下均实现了最先进或具有竞争力的性能。具体而言，将CEAP-360VR上的受试者依赖准确率从78.9%提高到86.8%，仅使用EDA信号就将受试者独立准确率从27.5%提高到33.1%。在最具挑战性的MAHNOB-HCI EEG受试者独立分割中，性能与最佳方法相差仅0.8个百分点。&lt;h4&gt;结论&lt;/h4&gt;MFMC是一种有效的自监督学习方法，能够直接捕捉多模态情感数据中的高阶交互作用，在各种情感计算任务中表现出优越性能，且对受试者间变异性具有鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;情感状态表现为中枢和自主神经系统中的协调但异质性的生理反应，这对情感计算中的多模态表示学习构成了根本性挑战。学习这种联合动力学因情感标注的稀缺性和主观性而进一步复杂化，这促使了自监督学习（SSL）的使用。然而，大多数现有的SSL方法依赖于成对对齐目标，这些目标不足以表征超过两种模态之间的依赖关系，也无法捕捉来自协调的脑和自主反应的高阶交互作用。为了解决这一局限性，我们提出了多模态功能最大相关（MFMC），这是一种基于原则的自监督学习框架，通过双重总相关（DTC）目标最大化高阶多模态依赖关系。通过推导一个紧密的三明治界限并使用基于功能最大相关分析（FMCA）的迹代理进行优化，MFMC直接捕捉联合多模态交互，而不依赖于成对对比损失。在三个公共情感计算基准测试上的实验表明，MFMC在受试者依赖和受试者独立的评估协议下始终实现了最先进或具有竞争力的性能，突显了其对受试者间变异性的鲁棒性。特别是，MFMC将CEAP-360VR上的受试者依赖准确率从78.9%提高到86.8%，仅使用EDA信号就将受试者独立准确率从27.5%提高到33.1%。此外，在最具挑战性的MAHNOB-HCI EEG受试者独立分割中，MFMC仍保持在最佳性能方法的0.8个百分点以内。我们的代码可在 https://github.com/DY9910/MFMC 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emotional states manifest as coordinated yet heterogeneous physiological responses across central and autonomic systems, posing a fundamental challenge for multimodal representation learning in affective computing. Learning such joint dynamics is further complicated by the scarcity and subjectivity of affective annotations, which motivates the use of self-supervised learning (SSL). However, most existing SSL approaches rely on pairwise alignment objectives, which are insufficient to characterize dependencies among more than two modalities and fail to capture higher-order interactions arising from coordinated brain and autonomic responses.  To address this limitation, we propose Multimodal Functional Maximum Correlation (MFMC), a principled SSL framework that maximizes higher-order multimodal dependence through a Dual Total Correlation (DTC) objective. By deriving a tight sandwich bound and optimizing it using a functional maximum correlation analysis (FMCA) based trace surrogate, MFMC captures joint multimodal interactions directly, without relying on pairwise contrastive losses.  Experiments on three public affective computing benchmarks demonstrate that MFMC consistently achieves state-of-the-art or competitive performance under both subject-dependent and subject-independent evaluation protocols, highlighting its robustness to inter-subject variability. In particular, MFMC improves subject-dependent accuracy on CEAP-360VR from 78.9% to 86.8%, and subject-independent accuracy from 27.5% to 33.1% using the EDA signal alone. Moreover, MFMC remains within 0.8 percentage points of the best-performing method on the most challenging EEG subject-independent split of MAHNOB-HCI. Our code is available at https://github.com/DY9910/MFMC.</description>
      <author>example@mail.com (Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu)</author>
      <guid isPermaLink="false">2512.23076v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks</title>
      <link>http://arxiv.org/abs/2512.22966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了三种先进大语言模型在临床决策支持中的性能，发现提示工程的效果因模型和任务而异，需要定制化的上下文感知策略才能有效整合LLMs到医疗保健中。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在医学知识评估方面显示出潜力，但它们在现实世界临床决策中的实际应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;评估三种最先进的LLMs（ChatGPT-4o、Gemini 1.5 Pro和LIama 3.3 70B）在临床决策支持中的性能，覆盖典型患者就诊的整个临床推理工作流程。&lt;h4&gt;方法&lt;/h4&gt;使用36个案例研究，首先评估LLM在两种温度设置下的开箱即用性能，涵盖五个关键临床决策任务：鉴别诊断、必要立即步骤、相关诊断测试、最终诊断和治疗建议。随后评估提示工程通过MedPrompt框架变体（包括有针对性的和随机的动态少样本学习）对性能的增强效果。&lt;h4&gt;主要发现&lt;/h4&gt;所有模型在不同任务上表现差异显著：在最终诊断方面接近完美准确率，在相关诊断测试方面表现不佳，其他任务表现中等。ChatGPT在零温度下表现更好，而LIama在默认温度下表现更强。提示工程不是一刀切的解决方案，虽提高了最低准确率任务的性能，但对其他任务产生负面影响。有针对性的动态少样本提示并不始终优于随机选择。&lt;h4&gt;结论&lt;/h4&gt;提示工程的影响高度依赖于模型和任务，强调需要定制化的、上下文感知的策略来将LLMs整合到医疗保健中。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型在医学知识评估方面显示出潜力，但它们在现实世界临床决策中的实际应用仍需探索。在本研究中，我们评估了三种最先进的大语言模型——ChatGPT-4o、Gemini 1.5 Pro和LIama 3.3 70B——在典型患者就诊整个临床推理工作流程中的临床决策支持性能。使用36个案例研究，我们首先评估了LLM在两种温度设置（默认与零）下五个关键顺序临床决策任务的开箱即用性能：鉴别诊断、必要立即步骤、相关诊断测试、最终诊断和治疗建议。所有模型在不同任务上表现出高变异性，在最终诊断方面接近完美准确率，在相关诊断测试方面表现不佳，在其他任务上表现中等。此外，ChatGPT在零温度下表现更好，而LIama在默认温度下表现更强。接下来，我们通过应用MedPrompt框架的变体（包括有针对性的和随机的动态少样本学习）评估提示工程是否能增强LLM性能。结果表明提示工程不是一刀切的解决方案。虽然它显著提高了基线准确率最低任务（相关诊断测试）的性能，但对其他任务反而产生负面影响。另一个关键发现是，有针对性的动态少样本提示并不始终优于随机选择，表明紧密匹配示例的预期好处可能被更广泛上下文多样性的损失所抵消。这些发现表明，提示工程的影响高度依赖于模型和任务，突显了需要定制化的、上下文感知的策略来将LLMs整合到医疗保健中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM's out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.</description>
      <author>example@mail.com (Mengdi Chai, Ali R. Zomorrodi)</author>
      <guid isPermaLink="false">2512.22966v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning</title>
      <link>http://arxiv.org/abs/2512.22904v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于持续学习的元学习框架MetaCD用于认知诊断，能够有效解决数据长尾分布和动态变化问题，提高模型在新任务上的准确性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;认知诊断是智能教育中的重要研究课题，旨在评估学生对不同技能的掌握水平。目前许多研究使用深度学习模型来探索学生、问题和技能之间的复杂交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有认知诊断方法因数据长尾分布和动态变化而导致的性能限制问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于持续学习的元学习框架MetaCD，利用元学习学习最优初始化状态缓解长尾问题，并采用参数保护机制使模型能够适应新技能或新任务。&lt;h4&gt;主要发现&lt;/h4&gt;MetaCD不仅能提高模型在单个任务上的可塑性，还能确保模型在顺序任务上的稳定性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;在五个真实数据集上的综合实验表明，MetaCD在准确性和泛化能力上都优于其他基线方法。&lt;h4&gt;翻译&lt;/h4&gt;Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.</description>
      <author>example@mail.com (Jin Wu, Chanjin Zheng)</author>
      <guid isPermaLink="false">2512.22904v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</title>
      <link>http://arxiv.org/abs/2512.23176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GVSynergy-Det框架，通过协同高斯-体素表示学习增强3D目标检测，结合连续高斯和离散体素表示的优势，从仅RGB图像中提取准确几何信息，无需深度传感器或密集3D监督。&lt;h4&gt;背景&lt;/h4&gt;基于图像的3D目标检测旨在仅使用RGB图像识别和定位3D空间中的物体，避免了基于点云方法所需的昂贵深度传感器。现有方法面临两个关键挑战：高精度方法通常需要密集3D监督，而无需密集监督的方法难以仅从图像中提取准确几何信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架，能够仅从RGB图像中提取准确的几何信息进行3D目标检测，同时不需要密集3D监督或深度传感器。&lt;h4&gt;方法&lt;/h4&gt;1) 提出GVSynergy-Det框架，通过协同高斯-体素表示学习增强3D检测；2) 采用双表示架构：使通用的可泛化高斯溅射适应检测任务提取互补几何特征，开发跨表示增强机制用高斯场几何细节丰富体素特征；3) 通过可学习的整合直接利用两种表示的特征，而非依赖场景级优化或仅将高斯表示用于深度正则化。&lt;h4&gt;主要发现&lt;/h4&gt;GVSynergy-Det在ScanNetV2和ARKitScenes等具有挑战性的室内基准测试上实现了最先进的结果，显著优于现有方法，且不需要任何深度或密集3D几何监督（如点云或TSDF）。&lt;h4&gt;结论&lt;/h4&gt;通过协同使用连续高斯和离散体素表示，可以有效地从仅RGB图像中提取准确的几何信息进行3D目标检测，而无需传统方法所需的密集3D监督或深度传感器。&lt;h4&gt;翻译&lt;/h4&gt;基于图像的3D目标检测旨在仅使用RGB图像识别和定位3D空间中的物体，消除了基于点云方法所需的昂贵深度传感器的需求。现有的基于图像的方法面临两个关键挑战：实现高精度的方法通常需要密集的3D监督，而没有这种监督的方法难以仅从图像中提取准确的几何信息。在本文中，我们提出了GVSynergy-Det，一个通过协同高斯-体素表示学习增强3D检测的新颖框架。我们的关键见解是，连续高斯和离散体素表示捕捉了互补的几何信息：高斯擅长建模细粒度表面细节，而体素提供结构化的空间上下文。我们引入了一种双表示架构，它：1) 使可泛化的高斯溅射适应检测任务，提取互补的几何特征，以及2) 开发跨表示增强机制，用高斯场的几何细节丰富体素特征。与之前依赖耗时的场景级优化或仅将高斯表示用于深度正则化的方法不同，我们的协同策略通过可学习的整合直接利用两种表示的特征，实现更准确的目标定位。大量实验表明，GVSynergy-Det在具有挑战性的室内基准测试上实现了最先进的结果，在ScanNetV2和ARKitScenes数据集上显著优于现有方法，且不需要任何深度或密集3D几何监督（如点云或TSDF）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).</description>
      <author>example@mail.com (Yi Zhang, Yi Wang, Lei Yao, Lap-Pui Chau)</author>
      <guid isPermaLink="false">2512.23176v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection</title>
      <link>http://arxiv.org/abs/2512.22972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WRCFormer是一种创新的3D目标检测框架，通过融合原始雷达立方体与相机输入，利用小波注意力模块和几何引导的渐进融合机制，显著提升了4D毫米波雷达在自动驾驶和机器人感知中的性能，特别是在恶劣天气条件下。&lt;h4&gt;背景&lt;/h4&gt;4D毫米波雷达因其低成本和全天候稳健性在自动驾驶和机器人感知中广泛应用，但其固有的稀疏性和有限的语义丰富性限制了感知能力。点云雷达因多阶段信号处理导致信息损失，而直接使用原始4D雷达数据计算成本过高。&lt;h4&gt;目的&lt;/h4&gt;解决4D毫米波雷达感知能力的限制，提出一种有效融合原始雷达数据与相机数据的方法，同时避免信息损失和过高计算成本。&lt;h4&gt;方法&lt;/h4&gt;提出WRCFormer框架，通过解耦雷达立方体的多视图表示融合原始雷达立方体与相机输入；设计小波注意力模块作为基于小波的特征金字塔网络的基本模块，增强稀疏雷达信号和图像数据的表示；引入几何引导的渐进融合机制，有效整合两种模态的多视图特征。&lt;h4&gt;主要发现&lt;/h4&gt;WRCFormer在K-Radar基准测试上达到最先进的性能，在所有场景中比最佳模型提高约2.4%，在雨雪场景中提高1.6%，展示了在恶劣天气条件下的稳健性。&lt;h4&gt;结论&lt;/h4&gt;WRCFormer有效地解决了4D毫米波雷达的感知限制，通过创新地融合原始雷达数据和相机数据，在各种条件下实现了高性能，特别是在恶劣天气条件下表现良好。&lt;h4&gt;翻译&lt;/h4&gt;4D毫米波雷达因其低成本和全天候稳健性已被广泛应用于自动驾驶和机器人感知。然而，其固有的稀疏性和有限的语义丰富性显著限制了感知能力。最近，将相机数据与4D雷达融合已成为一种有前景的经济解决方案，通过利用两种模态的互补优势。然而，基于点云的雷达常常因多阶段信号处理导致信息损失，而直接使用原始4D雷达数据会带来过高的计算成本。为解决这些挑战，我们提出了WRCFormer，这是一种新颖的3D目标检测框架，通过解耦雷达立方体的多视图表示将原始雷达立方体与相机输入融合。具体来说，我们设计了一个小波注意力模块作为基于小波的特征金字塔网络的基本模块，以增强稀疏雷达信号和图像数据的表示。我们进一步引入了一个两阶段基于查询的、模态无关的融合机制，称为几何引导的渐进融合，以有效整合两种模态的多视图特征。大量实验表明，WRCFormer在K-Radar基准测试上达到了最先进的性能，在所有场景中比最佳模型提高约2.4%，在雨雪场景中提高1.6%，突显了其在恶劣天气条件下的稳健性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.</description>
      <author>example@mail.com (Runwei Guan, Jianan Liu, Shaofeng Liang, Fangqiang Ding, Shanliang Yao, Xiaokai Bai, Daizong Liu, Tao Huang, Guoqiang Mao, Hui Xiong)</author>
      <guid isPermaLink="false">2512.22972v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Depth Anything in $360^\circ$: Towards Scale Invariance in the Wild</title>
      <link>http://arxiv.org/abs/2512.22819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://insta360-research-team.github.io/DA360&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了DA360（Depth Anything in 360°），这是一个针对全景深度估计的改进模型，通过学习ViT骨干网的偏移参数和整合循环填充到DPT解码器中，显著提升了模型在室内和室外环境中的零样本泛化能力，建立了新的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;全景深度估计能够捕获完整的360度环境结构信息，对机器人和AR/VR应用具有重要意义。然而，尽管在室内环境中得到了广泛研究，其在开放世界领域的零样本泛化能力仍远落后于透视图像，这是因为透视图像受益于丰富的训练数据。&lt;h4&gt;目的&lt;/h4&gt;缩小全景深度估计与透视图像之间的性能差距，通过从透视域转移能力来提升全景深度估计在开放世界环境中的零样本泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了DA360，一个适应于全景环境的Depth Anything V2版本；2. 创新性地从ViT骨干网络学习偏移参数，将模型的尺度和位移不变输出转换为尺度不变的估计，直接生成良好的3D点云；3. 在DPT解码器中整合循环填充，消除接缝伪影，确保尊重球面连续性的空间相干深度图。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在标准室内基准测试和新的户外数据集Metropolis上评估，DA360相比基础模型实现了显著的提升；2. 在室内和室外基准测试上分别实现了超过50%和10%的相对深度误差减少；3. DA360显著优于强大的全景深度估计方法，在所有三个测试数据集上比PanDA实现了约30%的相对误差改进。&lt;h4&gt;结论&lt;/h4&gt;DA360建立了零样本全景深度估计的最先进性能，有效缩小了全景深度估计与透视图像之间的性能差距，为机器人和AR/VR应用提供了更强大的环境结构信息捕获能力。&lt;h4&gt;翻译&lt;/h4&gt;全景深度估计为捕获完整的360度环境结构信息提供了全面的解决方案，对机器人和AR/VR应用具有重要价值。然而，尽管在室内环境中得到了广泛研究，其在开放世界领域的零样本泛化能力仍远落后于透视图像，后者受益于丰富的训练数据。这种差距使得从透视域转移能力成为一个有吸引力的解决方案。为了缩小这一差距，我们提出了360度环境下的Depth Anything（DA360），这是Depth Anything V2的一个全景适应版本。我们的关键创新在于从ViT骨干网络学习一个偏移参数，将模型的尺度和位移不变输出转换为尺度不变的估计，直接生成良好的3D点云。此外，我们在DPT解码器中整合了循环填充，以消除接缝伪影，确保尊重球面连续性的空间相干深度图。在标准室内基准测试和我们新整理的户外数据集Metropolis上评估，DA360相比其基础模型实现了显著提升，在室内和室外基准测试上分别实现了超过50%和10%的相对深度误差减少。此外，DA360显著优于强大的全景深度估计方法，在所有三个测试数据集上比PanDA实现了约30%的相对误差改进，建立了零样本全景深度估计的最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决全景深度估计在开放世界环境中的零样本泛化能力不足的问题。现有方法主要针对室内环境，难以推广到室外多样化场景，且输出的深度是仿射不变的而非尺度不变的，导致难以直接生成结构良好的3D点云。这个问题很重要，因为全景深度估计能捕获完整的360度环境结构信息，对机器人导航和AR/VR应用至关重要，而全景图像的全局感知能力支持自主系统的全局路径规划和沉浸式体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先评估了现有单目深度模型在全景图像上的表现，发现Depth Anything V2(DAV2)在室外全景深度估计上表现最好。基于此，他们设计了DA360，通过两个关键创新解决尺度不变性和边界一致性问题：1)从ViT骨干网络学习位移参数，将仿射不变的视差转换为尺度不变的视差；2)在DPT解码器中集成圆形填充消除边界伪影。作者借鉴了DAV2作为基础模型，受MiDaS的鲁棒损失函数启发，并应用了圆形填充技术，同时使用了合成数据集进行训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习位移参数实现尺度不变的深度估计，并利用圆形填充确保全景图像边界的空间连续性。整体流程包括：1)使用预训练的DAV2模型初始化；2)通过ViT类标记的MLP回归位移参数，转换视差输出；3)用圆形填充替换DPT解码器的零填充；4)在合成数据集上微调模型；5)使用尺度不变损失函数训练；6)在室内和室外数据集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)位移学习机制，直接生成可用的3D点云；2)圆形填充解码器，消除边界伪影；3)尺度不变的深度估计，减少不确定性；4)新的室外基准数据集Metropolis。相比之前的工作，DA360选择DAV2而非DAV2(Ind.)作为基础模型，在视差空间而非深度空间监督；输出尺度不变的深度而非仿射不变的深度；在室内外数据集上显著优于PanDA、DreamCube等方法；比并发工作DA2更高效且性能更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DA360通过位移学习和圆形填充技术，实现了从针孔深度模型到全景领域的有效能力迁移，显著提升了全景深度估计质量并直接生成了结构良好的3D点云。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Panoramic depth estimation provides a comprehensive solution for capturing complete $360^\circ$ environmental structural information, offering significant benefits for robotics and AR/VR applications. However, while extensively studied in indoor settings, its zero-shot generalization to open-world domains lags far behind perspective images, which benefit from abundant training data. This disparity makes transferring capabilities from the perspective domain an attractive solution. To bridge this gap, we present Depth Anything in $360^\circ$ (DA360), a panoramic-adapted version of Depth Anything V2. Our key innovation involves learning a shift parameter from the ViT backbone, transforming the model's scale- and shift-invariant output into a scale-invariant estimate that directly yields well-formed 3D point clouds. This is complemented by integrating circular padding into the DPT decoder to eliminate seam artifacts, ensuring spatially coherent depth maps that respect spherical continuity. Evaluated on standard indoor benchmarks and our newly curated outdoor dataset, Metropolis, DA360 shows substantial gains over its base model, achieving over 50\% and 10\% relative depth error reduction on indoor and outdoor benchmarks, respectively. Furthermore, DA360 significantly outperforms robust panoramic depth estimation methods, achieving about 30\% relative error improvement compared to PanDA across all three test datasets and establishing new state-of-the-art performance for zero-shot panoramic depth estimation.</description>
      <author>example@mail.com (Hualie Jiang, Ziyang Song, Zhiqiang Lou, Rui Xu, Minglang Tan)</author>
      <guid isPermaLink="false">2512.22819v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis</title>
      <link>http://arxiv.org/abs/2512.22706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCPainter（Street Car Painter）的统一框架，整合3D高斯飞溅汽车资产表示、3D场景点云和基于扩散的生成方法，实现真实的3D资产插入和新视角合成，增强自动驾驶训练数据的多样性。&lt;h4&gt;背景&lt;/h4&gt;3D资产插入和新视角合成是自动驾驶仿真的关键组件，能提高训练数据多样性，包括长尾驾驶场景。现有3D资产重建方法难以捕捉光照和阴影的真实感，而新视角合成方法大多与资产插入能力独立处理。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架，能够同时处理3D资产的真实插入和新视角合成，使资产能与场景交互，支持创建更多样化的新训练场景。&lt;h4&gt;方法&lt;/h4&gt;SCPainter框架整合3D高斯飞溅汽车资产、3D场景点云和扩散模型，将3D资产和场景点云共同投影到新视角，使用这些投影作为扩散模型的条件输入来生成高质量图像。&lt;h4&gt;主要发现&lt;/h4&gt;在Waymo开放数据集上的评估表明，该框架能有效实现3D资产插入和新视角合成，有助于创建多样且真实的驾驶数据。&lt;h4&gt;结论&lt;/h4&gt;SCPainter框架成功结合3D资产插入和新视角合成，为自动驾驶仿真提供解决方案，可生成更加多样化和真实的训练数据。&lt;h4&gt;翻译&lt;/h4&gt;三维资产插入和新视角合成是自动驾驶仿真的关键组成部分，能够增强训练数据的多样性。通过更好、更多样且覆盖广泛情况的训练数据，包括长尾驾驶场景，自动驾驶模型可以变得更加稳健和安全。这促使了一个统一仿真框架的发展，能够同时处理3D资产的 realistic 整合和新视角合成。最近的3D资产重建方法能够从视频中重建动态行为者，并支持它们重新插入到模拟驾驶场景中。虽然整体结构和外观可以准确，但在通过光照或阴影捕捉3D资产的真实感方面仍然存在困难，特别是在插入到场景中时。与此同时，新视角合成方法的最新进展在合成原始记录轨迹之外的视角方面展示了有希望的结果。然而，现有方法大多将资产插入和新视角合成能力分开处理。为了使资产能够与场景的其他部分交互，并支持创建更多样化的新训练场景，真实的3D资产插入应与新视角合成相结合。为此，我们提出了SCPainter（Street Car Painter），一个统一框架，整合了3D高斯飞溅（GS）汽车资产表示和3D场景点云，以及基于扩散的生成方法，共同实现真实的3D资产插入和新视角合成。3D GS资产和3D场景点云被一起投影到新视角，这些投影被用作条件输入到扩散模型中，以生成高质量图像。在Waymo开放数据集上的评估证明了我们的框架实现3D资产插入和新视角合成的能力，促进了多样且真实的驾驶数据的创建。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.</description>
      <author>example@mail.com (Paul Dobre, Jackson Cooper, Xin Wang, Hongzhou Yang)</author>
      <guid isPermaLink="false">2512.22706v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning</title>
      <link>http://arxiv.org/abs/2512.22315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VideoZoomer是一种创新的智能体框架，使多模态大语言模型能够通过动态控制视觉焦点来理解长视频。它通过时间缩放工具获取高帧率片段，并以多轮交互方式收集证据，采用两阶段训练策略，在长视频理解和推理任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视觉语言任务中取得了显著进展，但由于上下文窗口有限，在长视频理解方面仍然受限。现有方法倾向于依赖均匀帧采样或静态预选择，这可能会忽略关键证据，并且在推理过程中无法纠正初始选择错误。&lt;h4&gt;目的&lt;/h4&gt;克服多模态大语言模型在长视频理解方面的局限性，提出一种能够动态控制视觉焦点的智能体框架。&lt;h4&gt;方法&lt;/h4&gt;从粗略的低帧率概述开始，VideoZoomer调用时间缩放工具在自主选择的时刻获取高帧率片段，以多轮交互方式逐步收集细粒度证据。采用两阶段训练策略：在精心策划的蒸馏示例和反思轨迹数据集上进行冷启动监督微调，然后通过强化学习进一步优化智能体策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，7B模型提供了多样化和复杂的推理模式，在广泛的长视频理解和推理基准测试中取得了强大的性能。该模型能够持续超越现有开源模型，甚至在具有挑战性的任务上与专有系统相媲美，同时在减少帧预算的情况下实现了更高的效率。&lt;h4&gt;结论&lt;/h4&gt;VideoZoomer框架有效地解决了多模态大语言模型在长视频理解方面的局限性，通过动态控制视觉焦点和逐步收集证据，模型能够在长视频任务中实现更准确的推理。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在视觉语言任务中取得了显著进展，但由于上下文窗口有限，在长视频理解方面仍然受限。因此，现有方法倾向于依赖均匀帧采样或静态预选择，这可能会忽略关键证据，并且在推理过程中无法纠正其初始选择错误。为了克服这些限制，我们提出了VideoZoomer，一种新颖的智能体框架，使多模态大语言模型能够在推理过程中动态控制其视觉焦点。从粗略的低帧率概述开始，VideoZoomer调用时间缩放工具，在自主选择的时刻获取高帧率片段，从而以多轮交互方式逐步收集细粒度证据。相应地，我们采用两阶段训练策略：在精心策划的蒸馏示例和反思轨迹数据集上进行冷启动监督微调阶段，然后通过强化学习进一步优化智能体策略。广泛的实验表明，我们的7B模型提供了多样化和复杂的推理模式，在广泛的长视频理解和推理基准测试中取得了强大的性能。这些涌现的能力使其能够持续超越现有的开源模型，甚至在具有挑战性的任务上与专有系统相媲美，同时在减少帧预算的情况下实现了更高的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.</description>
      <author>example@mail.com (Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang)</author>
      <guid isPermaLink="false">2512.22315v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs</title>
      <link>http://arxiv.org/abs/2512.22226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VideoScaffold是一种专为流式视频理解设计的动态表示框架，通过自适应调整事件粒度和保留细粒度视觉语义，解决了现有静态策略在处理连续视频流时的碎片化和过度压缩问题。&lt;h4&gt;背景&lt;/h4&gt;理解长视频对多模态大语言模型具有挑战性，主要因为帧间存在大量冗余，且需要时间连贯的表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理连续视频流的动态表示框架，克服现有静态策略在流式视频理解中的局限性。&lt;h4&gt;方法&lt;/h4&gt;VideoScaffold框架包含两个关键组件：Elastic-Scale Event Segmentation (EES)，执行预测引导的分割动态细化事件边界；和Hierarchical Event Consolidation (HEC)，逐步将语义相关的段聚合为多级抽象。两者协同工作实现从细粒度帧理解到抽象事件推理的平滑过渡。&lt;h4&gt;主要发现&lt;/h4&gt;在离线和流式视频理解基准上的广泛实验表明，VideoScaffold达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;VideoScaffold是模块化和即插即用的，能够无缝扩展现有的基于图像的多模态大语言模型以实现连续视频理解，代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;理解长视频对多模态大语言模型而言仍然具有挑战性，这主要是由于帧间存在大量冗余以及需要时间连贯的表示。现有的静态策略，如稀疏采样、帧压缩和聚类，都是为离线环境优化的，当应用于连续视频流时，常常产生碎片化或过度压缩的输出。我们提出了VideoScaffold，这是一种专为流式视频理解设计的动态表示框架。它根据视频时长自适应调整事件粒度，同时保留细粒度的视觉语义。VideoScaffold引入了两个关键组件：Elastic-Scale Event Segmentation (EES)，执行预测引导的分割以动态细化事件边界；和Hierarchical Event Consolidation (HEC)，逐步将语义相关的段聚合为多级抽象。EES和HEC协同工作，使VideoScaffold能够在视频流展开时平滑地从细粒度帧理解过渡到抽象事件推理。在离线和流式视频理解基准上的广泛实验表明，VideoScaffold达到了最先进的性能。该框架是模块化和即插即用的，能够无缝扩展现有的基于图像的多模态大语言模型以实现连续视频理解。代码可在https://github.com/zheng980629/VideoScaffold获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.</description>
      <author>example@mail.com (Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao)</author>
      <guid isPermaLink="false">2512.22226v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</title>
      <link>http://arxiv.org/abs/2512.23545v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PathFound是一种支持病理诊断中证据寻求推理的智能多模态模型，通过整合病理视觉基础模型、视觉语言模型和强化训练的推理模型，在初始诊断、证据寻求和最终决策三个阶段主动获取信息并改进诊断。&lt;h4&gt;背景&lt;/h4&gt;现有病理基础模型在视觉表征学习和多模态交互方面取得进展，但大多采用静态推理范式，只处理全切片图像一次产生预测，不会在诊断不明确时重新评估或有针对性地获取证据，这与临床诊断工作流程形成对比。&lt;h4&gt;目的&lt;/h4&gt;提出PathFound，一个支持病理诊断中证据寻求推理的智能多模态模型，以改进诊断准确性并发现细微病理特征。&lt;h4&gt;方法&lt;/h4&gt;PathFound整合病理视觉基础模型、视觉语言模型和通过强化学习训练的推理模型，通过初始诊断、证据寻求和最终决策三个阶段进行主动信息获取和诊断改进。&lt;h4&gt;主要发现&lt;/h4&gt;在多个大型多模态模型中，采用证据寻求策略一致提高了诊断准确性；PathFound在各种临床场景中取得了最先进的诊断性能；PathFound显示出发现细微细节如核特征和局部侵袭的强大潜力。&lt;h4&gt;结论&lt;/h4&gt;证据寻求的工作流程在计算病理学中是有效的；Path作为一种智能多模态模型，在临床病理诊断中具有显著优势和应用价值。&lt;h4&gt;翻译&lt;/h4&gt;最近的病理基础模型极大地推动了视觉表征学习和多模态交互的发展。然而，大多数模型仍然依赖静态推理范式，即全切片图像只被处理一次以产生预测，而在诊断不明确时不会重新评估或有针对性地获取证据。这与临床诊断工作流程形成对比，临床诊断通过反复观察幻灯片和进一步检查来完善假设。我们提出了PathFound，一个旨在支持病理诊断中证据寻求推理的智能多模态模型。PathFound整合了病理视觉基础模型、视觉语言模型和通过强化训练的推理模型的力量，通过初始诊断、证据寻求和最终决策三个阶段进行主动信息获取和诊断改进。在几个大型多模态模型中，采用这种策略一致提高了诊断准确性，表明证据寻求工作流程在计算病理学中的有效性。在这些模型中，PathFound在各种临床场景中取得了最先进的诊断性能，并显示出发现细微细节的强大潜力，如核特征和局部侵袭。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.</description>
      <author>example@mail.com (Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang)</author>
      <guid isPermaLink="false">2512.23545v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning</title>
      <link>http://arxiv.org/abs/2512.22730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了一种基于自监督预训练的深度学习方法（USF-MAE）用于早期超声图像中囊肿性水瘤的检测，结果显示该方法优于传统监督学习方法，具有更高的准确性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;囊肿性水瘤是一种高风险的产前超声发现，预示着高比率的染色体异常、结构畸形和不良妊娠结局。自动检测可以提高可重复性并支持可扩展的早期筛查项目，但监督深度学习方法受限于小的标记数据集。&lt;h4&gt;目的&lt;/h4&gt;评估超声特定的自监督预训练是否可以促进在早期超声图像中准确、稳健地检测囊肿性水瘤。&lt;h4&gt;方法&lt;/h4&gt;使用在超过370,000张未标记的超声图像上预训练的超声自监督基础模型（USF-MAE）进行微调，用于正常对照组和囊肿性水瘤病例的二分类。使用准确率、敏感性、特异性和ROC曲线下面积进行性能评估，并与DenseNet-169基线模型比较，同时使用Score-CAM可视化进行模型可解释性分析。&lt;h4&gt;主要发现&lt;/h4&gt;USF-MAE在所有评估指标上都优于DenseNet-169基线模型，平均准确率为0.96，敏感性为0.94，特异性为0.98，ROC-AUC为0.98，而基线模型分别为0.93、0.92、0.94和0.94。Score-CAM可视化显示了模型预测的临床相关性，配对统计分析确认性能改进具有统计学意义（p = 0.0057）。&lt;h4&gt;结论&lt;/h4&gt;超声特定的自监督预训练可以有效促进在早期超声图像中准确、稳健地检测囊肿性水瘤，为临床筛查提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;囊肿性水瘤是一种高风险的产前超声发现，预示着高比率的染色体异常、结构畸形和不良妊娠结局。自动检测可以提高可重复性并支持可扩展的早期筛查项目，但监督深度学习方法受限于小的标记数据集。本研究评估了超声特定的自监督预训练是否可以促进在早期超声图像中准确、稳健地深度学习检测囊肿性水瘤。我们使用掩码自编码（USF-MAE）对超声自监督基础模型进行微调，该模型在超过370,000张未标记的超声图像上进行了预训练，用于本研究中正常对照组和囊肿性水瘤病例的二分类。性能评估使用了与DenseNet-169基线模型相同的精选超声数据集、预处理流程和四折交叉验证协议，评估指标包括准确率、敏感性、特性和ROC曲线下面积。使用Score-CAM可视化对模型可解释性进行了定性分析。USF-MAE在所有评估指标上都优于DenseNet-169基线模型。与DenseNet-169基线模型的0.93、0.92、0.94和0.94相比，所提出的模型平均准确率为0.96，敏感性为0.94，特异性为0.98，ROC-AUC为0.98。模型预测的Score-CAM定性可视化通过突出显示胎儿颈部区域的预期区域，显示了临床相关性。使用Wilcoxon符号秩检验进行的配对统计分析确认USF-MAE实现的性能改进具有统计学意义（p = 0.0057）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).</description>
      <author>example@mail.com (Youssef Megahed, Robin Ducharme, Inok Lee, Inbal Willner, Olivier X. Miguel, Kevin Dick, Adrian D. C. Chan, Mark Walker, Steven Hawken)</author>
      <guid isPermaLink="false">2512.22730v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages</title>
      <link>http://arxiv.org/abs/2512.22712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to 2025 EMNLP Multilingual Representation Learning Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究揭示了大型语言模型在跨语言推理中的关键盲点，表明即使模型能够正确回答问题，其推理过程也可能存在严重缺陷，特别是在非拉丁语言中。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型通过思维链提示展现出强大的推理能力，但这种推理质量是否能跨语言转移尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一个人类验证的框架，评估模型生成的推理轨迹是否能在不同语言中逻辑性地支持其结论。&lt;h4&gt;方法&lt;/h4&gt;分析了来自6种语言和6种前沿模型的GlobalMMLU问题的65k条推理轨迹，并开发了一个通过人工注释的错误分类法来表征这些失败。&lt;h4&gt;主要发现&lt;/h4&gt;模型虽然实现了高任务准确率，但推理可能无法支持其结论；非拉丁文字符中的推理轨迹与其结论之间的不一致性至少是拉丁文字符中的两倍；这些失败主要源于证据错误（不支持的声明、模糊的事实），其次是逻辑推理步骤错误。&lt;h4&gt;结论&lt;/h4&gt;当前多语言评估实践无法全面反映模型的推理能力，需要开发推理感知的评估框架。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型通过思维链提示展现出强大的推理能力，但这种推理质量是否能跨语言转移仍需探索。我们引入了一个经过人类验证的框架，用于评估模型生成的推理轨迹是否能跨语言逻辑性地支持其结论。分析了来自6种语言和6种前沿模型的GlobalMMLU问题的65k条推理轨迹后，我们发现了一个关键盲点：尽管模型实现了高任务准确率，但它们的推理可能无法支持其结论。非拉丁文字符中的推理轨迹与其结论之间的不一致性至少是拉丁文字符中的两倍。我们通过人工注释开发了一个错误分类法来表征这些失败，发现它们主要源于证据错误（不支持的声明、模糊的事实），其次是逻辑推理步骤错误。我们的研究结果表明，当前的多语言评估实践无法全面反映模型的推理能力，突显了开发推理感知评估框架的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.</description>
      <author>example@mail.com (Anaelia Ovalle, Candace Ross, Sebastian Ruder, Adina Williams, Karen Ullrich, Mark Ibrahim, Levent Sagun)</author>
      <guid isPermaLink="false">2512.22712v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains</title>
      <link>http://arxiv.org/abs/2512.22664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Cluster Attention Adapter (CLAdapter)的新方法，用于将大规模预训练模型的知识有效适应到数据有限的科学领域下游任务中，通过注意机制和聚类中心实现特征个性化增强，并在多个领域的数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;大数据时代，计算机视觉领域受益于LAION-2B、LAION-400M、ImageNet-21K、Kinetics等大规模数据集，ViT和ConvNeXt等模型在这些数据集上预训练获得了丰富知识。然而，专业化和数据有限的科学领域中的下游任务仍然面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出CLAdapter方法，用于将从大规模数据中学到的丰富表征进行细化和适应，以有效处理各种数据有限的下游任务。&lt;h4&gt;方法&lt;/h4&gt;CLAdapter引入注意机制和聚类中心，通过分布关联和变换矩阵实现转换特征的个性化增强，使模型能够学习针对不同特征集定制化的表征，促进从预训练特征到下游场景的有效适应。其统一接口设计支持与CNN和Transformer等多种架构在二维和三维环境中的无缝集成。&lt;h4&gt;主要发现&lt;/h4&gt;在10个涵盖通用、多媒体、生物、医学、工业、农业、环境、地理、材料科学、分布外和三维分析等领域的广泛实验中，CLAdapter在各种数据有限的科学领域取得了最先进的性能，证明了通过自适应转移释放基础视觉模型潜力的有效性。&lt;h4&gt;结论&lt;/h4&gt;CLAdapter能够有效释放基础视觉模型的潜力，通过自适应转移使模型能够适应各种数据有限的科学领域。&lt;h4&gt;翻译&lt;/h4&gt;在大数据时代，计算机视觉领域受益于LAION-2B、LAION-400M和ImageNet-21K、Kinetics等大规模数据集，在这些数据集上预训练的流行模型如ViT和ConvNeXt系列已经获得了大量知识。然而，在专业化和数据有限的科学领域中的许多下游任务仍然构成重大挑战。在本文中，我们提出了一种新的Cluster Attention Adapter (CLAdapter)，它将从大规模数据中学到的丰富表征进行细化和适应，以适应各种数据有限的下游任务。具体而言，CLAdapter引入了注意机制和聚类中心，通过分布关联和变换矩阵来个性化增强转换后的特征。这使得使用CLAdapter微调的模型能够学习针对不同特征集定制化的表征，促进模型从丰富的预训练特征有效适应到各种下游场景。此外，CLAdapter的统一接口设计允许与CNN和Transformer等多种模型架构在二维和三维上下文中无缝集成。在涵盖通用、多媒体、生物、医学、工业、农业、环境、地理、材料科学、分布外(OOD)和三维分析等10个数据集上的广泛实验中，CLAdapter在各种数据有限的科学领域取得了最先进的性能，证明了通过自适应转移释放基础视觉模型潜力的有效性。代码可在https://github.com/qklee-lz/CLAdapter获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.</description>
      <author>example@mail.com (Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang)</author>
      <guid isPermaLink="false">2512.22664v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma</title>
      <link>http://arxiv.org/abs/2512.22331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于变分自编码器的多视图潜在表示学习框架，用于整合来自不同MRI模态的放射组学特征，以预测胶质母细胞瘤中MGMT启动子甲基化状态。&lt;h4&gt;背景&lt;/h4&gt;放射基因组学的核心目标是通过医学影像无创推断分子肿瘤特征，特别是在胶质母细胞瘤中，MGMT启动子甲基化具有重要的预后和治疗意义。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合多模态MRI特征的方法，解决传统单模态和早期融合方法中存在的高特征冗余和不完整模态特定信息建模问题。&lt;h4&gt;方法&lt;/h4&gt;采用基于变分自编码器的多视图潜在表示学习框架，通过独立概率编码器处理对比增强T1加权和FLAIR MRI模态，在紧凑的潜在空间中进行融合，保留模态特定结构同时实现多模态有效整合。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够保留各模态的特定信息，有效减少特征冗余，并通过潜在嵌入实现MGMT启动子甲基化的分类。&lt;h4&gt;结论&lt;/h4&gt;基于变分自编码器的多视图潜在表示学习框架为整合多模态放射组学特征提供了有效途径，可应用于肿瘤分子特征的无创预测。&lt;h4&gt;翻译&lt;/h4&gt;通过医学影像无创推断分子肿瘤特征是放射基因组学的中心目标，特别是在胶质母细胞瘤中，O6-甲基鸟嘌呤-DNA甲基转移酶启动子甲基化具有重要的预后和治疗意义。尽管基于放射组学的机器学习方法在此任务上显示出前景，但传统的单模态和早期融合方法通常受限于高特征冗余和不完整的模态特定信息建模。在本工作中，我们引入了一种基于变分自编码器的多视图潜在表示学习框架，以整合来自对比增强T1加权和流体衰减反转恢复磁共振成像的互补放射组学特征。通过通过独立概率编码器对每个模态进行编码并在紧凑的潜在空间中进行融合，所提出的方法保留了模态特定结构，同时实现了有效的多模态整合。生成的潜在嵌入随后用于MGMT启动子甲基化分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.</description>
      <author>example@mail.com (Mariya Miteva, Maria Nisheva-Pavlova)</author>
      <guid isPermaLink="false">2512.22331v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation</title>
      <link>http://arxiv.org/abs/2512.23411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SOFTooth是一种语义增强、有序感知的2D-3D融合框架，通过利用冻结的2D语义增强3D牙齿实例分割，解决了边界泄漏、中心漂移和牙齿身份不一致等问题，特别在处理第三磨牙等罕见牙齿类型时表现优异。&lt;h4&gt;背景&lt;/h4&gt;三维牙齿实例分割面临挑战，包括拥挤的牙弓、模糊的牙齿-牙龈边界、缺失牙齿以及罕见但临床重要的第三磨牙。基于3D的方法通常存在边界泄漏、中心漂移和牙齿身份不一致的问题，而2D基础模型(如SAM)提供强大的边界感知语义，但在3D中直接应用不切实际。&lt;h4&gt;目的&lt;/h4&gt;解决3D牙齿实例分割中的挑战，提出一种融合2D-3D的框架，利用2D语义增强3D分割，提高分割准确性，特别是对于罕见牙齿类型和复杂解剖结构。&lt;h4&gt;方法&lt;/h4&gt;提出SOFTooth框架，包含三个关键组件：1)点状残差门控模块将咬合面SAM嵌入注入3D点特征中，改进牙齿-牙龈和牙齿间边界；2)中心引导的掩码细化规范实例掩码和几何质心之间的一致性，减少中心漂移；3)有序感知的匈牙利匹配策略将解剖学牙齿顺序和中心距离整合到基于相似性的分配中，确保连贯的标注。&lt;h4&gt;主要发现&lt;/h4&gt;在3DTeethSeg'22数据集上，SOFTooth实现了最先进的整体准确率和平均IoU，在涉及第三磨牙的案例上有明显提升，表明丰富的2D语义可以有效地转移到3D牙齿实例分割中，而无需2D微调。&lt;h4&gt;结论&lt;/h4&gt;SOFTooth框架成功解决了3D牙齿实例分割中的多个挑战，通过融合2D语义和3D几何信息提高了分割准确性，特别对于罕见牙齿类型和复杂解剖结构有显著改进，为临床应用提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;三维(3D)牙齿实例分割由于拥挤的牙弓、模糊的牙齿-牙龈边界、缺失牙齿以及罕见但临床重要的第三磨牙而仍然具有挑战性。依赖几何线索的原始3D方法通常存在边界泄漏、中心漂移和牙齿身份不一致的问题，特别是对于少数类别和复杂解剖结构。同时，2D基础模型如分割任何模型(SAM)提供强大的边界感知语义，但在临床工作流程中直接应用它们到3D中是不切实际的。为了解决这些问题，我们提出了SOFTooth，一种语义增强、有序感知的2D-3D融合框架，它利用冻结的2D语义而不需要显式的2D掩码监督。首先，点状残差门控模块将咬合面SAM嵌入注入3D点特征中，以细化牙齿-牙龈和牙齿间边界。其次，中心引导的掩码细化规范实例掩码和几何质心之间的一致性，减少中心漂移。此外，一种有序感知的匈牙利匹配策略将解剖学牙齿顺序和中心距离整合到基于相似性的分配中，确保即使在缺失或拥挤的牙列中也能保持连贯的标注。在3DTeethSeg'22上，SOFTooth实现了最先进的整体准确率和平均IoU，在涉及第三磨牙的案例上有明显提升，证明丰富的2D语义可以有效地转移到3D牙齿实例分割中，而无需2D微调。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决三维牙齿实例分割中的挑战性问题，包括牙齿弓形排列紧密、牙龈边界模糊、缺失牙齿和罕见但临床重要的第三磨牙。这些问题在数字牙科中很重要，因为3D牙齿分割是计算机辅助设计治疗的基础，支持自动化诊断、个性化治疗计划和预后评估，而第三磨牙虽罕见(仅占约5%病例)但对拔除计划和阻生评估至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者识别现有3D方法依赖几何线索导致边界泄漏、中心漂移等问题，同时发现2D基础模型如SAM提供强大边界语义但难以直接应用于3D。因此设计SOFTooth框架，整合2D-3D语义融合、中心一致性和解剖学顺序。该方法借鉴了多种现有工作：TSGCNet的双流几何编码、IOSSAM和CrossTooth的跨模态语义融合、基于中心的实例分割方法以及FDI标记系统，但创新性地将它们整合到统一框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合边界敏感的2D语义、稳定的实例中心和解剖学顺序，实现一个统一的端到端管道，处理正常和复杂临床场景。整体流程包括：1)双流3D特征提取几何特征；2)点级残差门控(PRG)模块将冻结的SAM咬合面语义注入3D特征；3)中心引导的掩码细化(CMR)模块强制执行中心-掩码一致性；4)FDI顺序感知匈牙利匹配(FHM)模块整合解剖学顺序和中心距离进行实例分配。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SOFTooth双分支2D-3D融合框架；2)PRG、CMR和FHM三个互补组件；3)利用冻结2D语义而不需显式2D掩码监督。相比之前工作：融合策略上不纯粹依赖几何特征而是整合2D语义；实例处理上强制执行中心-掩码一致性；标记方法上将FDI索引直接整合到匹配成本中；性能上显著提高第三磨牙等少数类的IoU，在复杂场景中提供更稳定的分割和连贯的标记。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SOFTooth通过整合边界敏感的2D语义、稳定的实例中心和解剖学顺序约束，显著提高了3D牙齿实例分割的准确性，特别是在处理罕见第三磨牙和复杂临床场景方面，实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional (3D) tooth instance segmentation remains challenging due to crowded arches, ambiguous tooth-gingiva boundaries, missing teeth, and rare yet clinically important third molars. Native 3D methods relying on geometric cues often suffer from boundary leakage, center drift, and inconsistent tooth identities, especially for minority classes and complex anatomies. Meanwhile, 2D foundation models such as the Segment Anything Model (SAM) provide strong boundary-aware semantics, but directly applying them in 3D is impractical in clinical workflows. To address these issues, we propose SOFTooth, a semantics-enhanced, order-aware 2D-3D fusion framework that leverages frozen 2D semantics without explicit 2D mask supervision. First, a point-wise residual gating module injects occlusal-view SAM embeddings into 3D point features to refine tooth-gingiva and inter-tooth boundaries. Second, a center-guided mask refinement regularizes consistency between instance masks and geometric centroids, reducing center drift. Furthermore, an order-aware Hungarian matching strategy integrates anatomical tooth order and center distance into similarity-based assignment, ensuring coherent labeling even under missing or crowded dentitions. On 3DTeethSeg'22, SOFTooth achieves state-of-the-art overall accuracy and mean IoU, with clear gains on cases involving third molars, demonstrating that rich 2D semantics can be effectively transferred to 3D tooth instance segmentation without 2D fine-tuning.</description>
      <author>example@mail.com (Xiaolan Li, Wanquan Liu, Pengcheng Li, Pengyu Jie, Chenqiang Gao)</author>
      <guid isPermaLink="false">2512.23411v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models</title>
      <link>http://arxiv.org/abs/2512.23239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种无需训练的两阶段数据修剪方法，用于解决遥感生成基础模型训练中的数据冗余、噪声和类别不平衡问题，即使在修剪85%训练数据的情况下也能显著提高模型收敛性和生成质量。&lt;h4&gt;背景&lt;/h4&gt;基于扩散的遥感生成基础模型对下游任务至关重要，但它们依赖的大量代表性数据通常包含冗余、噪声和类别不平衡，降低了训练效率并阻碍收敛。&lt;h4&gt;目的&lt;/h4&gt;解决现有遥感扩散基础模型在数据处理上的局限性，提出一种在高修剪比率下快速选择高质量数据子集的方法，使基础模型能够快速收敛并适用于多种应用。&lt;h4&gt;方法&lt;/h4&gt;提出一种训练自由的两阶段数据修剪方法：首先使用基于熵的标准去除低信息样本；其次利用遥感场景分类数据集作为参考基准，进行场景感知聚类和分层采样；最后通过平衡集群级别均匀性和样本代表性，实现细粒度选择同时保持整体多样性和代表性。&lt;h4&gt;主要发现&lt;/h4&gt;即使修剪85%的训练数据，该方法也能显著提高模型收敛性和生成质量；使用该方法训练的扩散基础模型在超分辨率和语义图像合成等下游任务中持续取得最先进的表现。&lt;h4&gt;结论&lt;/h4&gt;这种数据修剪范式为开发遥感生成基础模型提供了实际指导，解决了数据冗余和代表性问题。&lt;h4&gt;翻译&lt;/h4&gt;基于扩散的遥感生成基础模型对下游任务至关重要。然而，这些模型依赖于大量具有代表性的数据，这些数据通常包含冗余、噪声和类别不平衡，降低了训练效率并阻碍收敛。现有的遥感扩散基础模型通常聚合多个分类数据集或应用简单的去重方法，忽略了生成建模的分布要求和遥感图像的异质性。为解决这些限制，我们提出了一种无需训练的两阶段数据修剪方法，可在高修剪比率下快速选择高质量子集，使初步基础模型能够快速收敛并作为生成、下游微调和其他应用的通用骨干。我们的方法综合考虑了局部信息内容和全局场景级别的多样性和代表性。首先，基于熵的标准有效去除低信息样本。其次，利用遥感场景分类数据集作为参考基准，我们进行场景感知聚类和分层采样，提高聚类效果同时降低大规模未标记数据的计算成本。最后，通过平衡集群级别的均匀性和样本代表性，该方法在高修剪比率下实现细粒度选择，同时保持整体多样性和代表性。实验表明，即使修剪85%的训练数据，我们的方法也显著提高了收敛性和生成质量。此外，使用该方法训练的扩散基础模型在超分辨率和语义图像合成等下游任务中持续取得最先进的表现。这种数据修剪范式为开发遥感生成基础模型提供了实际指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.</description>
      <author>example@mail.com (Fan Wei, Runmin Dong, Yushan Lai, Yixiang Yang, Zhaoyang Luo, Jinxiao Zhang, Miao Yang, Shuai Yuan, Jiyao Zhao, Bin Luo, Haohuan Fu)</author>
      <guid isPermaLink="false">2512.23239v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>The Dawn of Agentic EDA: A Survey of Autonomous Digital Chip Design</title>
      <link>http://arxiv.org/abs/2512.23189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述全面概述了生成式AI和智能代理AI在数字电子设计自动化(EDA)领域的整合，从传统CAD到AI辅助EDA再到新兴的AI原生和智能设计范式的演变。&lt;h4&gt;背景&lt;/h4&gt;数字电子设计自动化领域正在经历从传统计算机辅助设计(CAD)向AI辅助EDA(AI4EDA)的转变，并进一步发展到新兴的AI原生和智能设计范式。&lt;h4&gt;目的&lt;/h4&gt;全面概述生成式AI和智能代理AI在EDA领域的整合，定义新兴的智能EDA领域，并提供从AI辅助工具到完全自主设计工程师转型的战略路线图。&lt;h4&gt;方法&lt;/h4&gt;详细介绍了这些范式在数字芯片设计流程中的应用，包括构建基于多模态基础模型的智能代理认知架构、前端RTL代码生成和智能验证、后端物理设计(包括算法创新和工具编排)，并通过综合案例研究进行验证，特别强调了跨阶段反馈循环的潜力。&lt;h4&gt;主要发现&lt;/h4&gt;通过从微架构定义到GDSII的综合案例研究验证了方法的实践可行性；探讨了安全性对EDA的双重影响，包括新型对抗性风险、自动化漏洞修复和隐私保护基础设施；指出了当前面临的挑战，如幻觉、数据稀缺和黑盒工具问题。&lt;h4&gt;结论&lt;/h4&gt;概述了向L4自主芯片设计的未来趋势，旨在定义新兴的智能EDA领域，并提供从AI辅助工具到完全自主设计工程师转型的战略路线图。&lt;h4&gt;翻译&lt;/h4&gt;本综述全面概述了生成式AI和智能代理AI在数字电子设计自动化(EDA)领域的整合。论文首先回顾了从传统计算机辅助设计(CAD)到AI辅助EDA(AI4EDA)，再到新兴的AI原生和智能设计范式的范式演变。我们详细介绍了这些范式在数字芯片设计流程中的应用，包括基于多模态基础模型构建智能代理认知架构、前端RTL代码生成和智能验证，以及具有算法创新和工具编排的后端物理设计。我们通过综合案例研究验证了这些方法，展示了从微架构定义到GDSII的实践可行性。特别强调了跨阶段反馈循环的潜力，其中代理利用后端PPA指标自主优化前端逻辑。此外，本综述深入探讨了安全性的双重影响，包括新型对抗性风险、自动化漏洞修复和隐私保护基础设施。最后，论文总结了当前与幻觉、数据稀缺和黑盒工具相关的挑战，并概述了向L4自主芯片设计的未来趋势。最终，这项工作旨在定义新兴的智能EDA领域，并提供从AI辅助工具到完全自主设计工程师转型的战略路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This survey provides a comprehensive overview of the integration of Generative AI and Agentic AI within the field of Digital Electronic Design Automation (EDA). The paper first reviews the paradigmatic evolution from traditional Computer-Aided Design (CAD) to AI-assisted EDA (AI4EDA), and finally to the emerging AI-Native and Agentic design paradigms. We detail the application of these paradigms across the digital chip design flow, including the construction of agentic cognitive architectures based on multimodal foundation models, frontend RTL code generation and intelligent verification, and backend physical design featuring algorithmic innovations and tool orchestration. We validate these methodologies through integrated case studies, demonstrating practical viability from microarchitecture definition to GDSII. Special emphasis is placed on the potential for cross-stage feedback loops where agents utilize backend PPA metrics to autonomously refine frontend logic. Furthermore, this survey delves into the dual-faceted impact on security, covering novel adversarial risks, automated vulnerability repair, and privacy-preserving infrastructure. Finally, the paper critically summarizes current challenges related to hallucinations, data scarcity, and black-box tools, and outlines future trends towards L4 autonomous chip design. Ultimately, this work aims to define the emerging field of Agentic EDA and provide a strategic roadmap for the transition from AI-assisted tools to fully autonomous design engineers.</description>
      <author>example@mail.com (Zelin Zang, Yuhang Song, Bingo Wing-Kuen Ling, Aili Wang, Fuji Yang)</author>
      <guid isPermaLink="false">2512.23189v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems</title>
      <link>http://arxiv.org/abs/2512.23132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  56 pages, 18 Figures, 22 Tables, TOSEM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了机器学习基础模型的安全风险，识别了主导的攻击战术、技术和过程(TTPs)，以及影响ML生命周期各阶段的漏洞。研究通过多源数据收集和分析，构建了本体驱动的威胁图，为ML安全提供了全面的威胁情报框架。&lt;h4&gt;背景&lt;/h4&gt;机器学习技术支撑金融、医疗和关键基础设施中的基础模型，使其成为数据投毒、模型提取、提示注入、自动化越狱和基于偏好的黑盒攻击的目标。较大的模型更容易受到内省驱动的越狱和跨模态操纵，而传统网络安全缺乏针对基础模型、多模态和RAG系统的ML特定威胁建模。&lt;h4&gt;目的&lt;/h4&gt;通过识别主导的TTPs、漏洞和目标生命周期阶段，来表征ML安全风险，为开发针对性的安全防护措施提供基础。&lt;h4&gt;方法&lt;/h4&gt;研究从MITRE ATLAS、AI事件数据库和相关文献中提取93个威胁样本，分析854个GitHub/Python存储库，并使用多代理RAG系统(ChatGPT-4o，温度0.4)挖掘300多篇文章构建本体驱动的威胁图，链接TTPs、漏洞和ML生命周期阶段。&lt;h4&gt;主要发现&lt;/h4&gt;研究识别出多个未报告的威胁，包括商业LLM API模型窃取、参数记忆泄露和基于偏好的纯文本越狱。主导的TTPs包括MASTERKEY式越狱、联邦投毒、扩散后门和偏好优化泄露，主要影响预训练和推理阶段。图分析显示，补丁传播差的库中存在密集的漏洞集群。&lt;h4&gt;结论&lt;/h4&gt;需要自适应的、ML特定的安全框架，结合依赖卫生、威胁情报和监控，以减轻ML生命周期中的供应链和推理风险。&lt;h4&gt;翻译&lt;/h4&gt;机器学习(ML)支撑金融、医疗和关键基础设施中的基础模型，使其成为数据投毒、模型提取、提示注入、自动化越狱和基于偏好的黑盒攻击的目标，这些攻击利用模型比较。较大的模型可能更容易受到内省驱动的越狱和跨模态操纵。传统网络安全缺乏针对基础模型、多模态和RAG系统的ML特定威胁建模。目标：通过识别主导的TTPs、漏洞和目标生命周期阶段来表征ML安全风险。方法：我们从MITRE ATLAS(26个)、AI事件数据库(12个)和文献(55个)中提取93个威胁，并分析854个GitHub/Python存储库。一个多代理RAG系统(ChatGPT-4o，温度0.4)挖掘300多篇文章，构建本体驱动的威胁图，链接TTPs、漏洞和阶段。结果：我们识别出未报告的威胁，包括商业LLM API模型窃取、参数记忆泄露和基于偏好的纯文本越狱。主导的TTPs包括MASTERKEY式越狱、联邦投毒、扩散后门和偏好优化泄露，主要影响预训练和推理。图分析显示，补丁传播差的库中存在密集的漏洞集群。结论：自适应的、ML特定的安全框架，结合依赖卫生、威胁情报和监控，对于减轻ML生命周期中的供应链和推理风险至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.</description>
      <author>example@mail.com (Armstrong Foundjem, Lionel Nganyewou Tidjon, Leuson Da Silva, Foutse Khomh)</author>
      <guid isPermaLink="false">2512.23132v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations</title>
      <link>http://arxiv.org/abs/2512.23056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种物理信息多模态基础模型(PI-MFM)框架，通过在预训练和适应阶段直接强制执行控制方程，解决了现有多算子学习方法数据需求量大且忽视物理规律的问题。PI-MFM使用PDE的符号表示作为输入，通过向量化导数计算自动组装PDE残差损失，使模型能够在跨方程族时使用统一的物理信息目标进行训练或适应。&lt;h4&gt;背景&lt;/h4&gt;偏微分方程(PDEs)控制着广泛的物理系统，最近的多模态基础模型在跨不同方程族学习PDE解算子方面显示出前景。然而，现有的多算子学习方法数据需求量大，并且在训练过程中忽视物理规律。&lt;h4&gt;目的&lt;/h4&gt;提出一个物理信息多模态基础模型(PI-MFM)框架，直接在预训练和适应阶段强制执行控制方程，以实现更高效、更准确的PDE求解。&lt;h4&gt;方法&lt;/h4&gt;PI-MFM以PDE的符号表示作为输入，通过向量化导数计算自动组装PDE残差损失。这些设计使得任何PDE编码的多模态基础模型能够在跨方程族时使用统一的物理信息目标进行训练或适应。此外，还分析了自动微分和有限差分在导数计算方面的性能，并展示了零样本物理信息微调到未见过的PDE族的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在13个参数化一维时变PDE族的基准测试中，PI-MFM始终优于纯数据驱动的对应方法，特别是在标记时空点稀少、时间域部分观测或标记函数对较少的情况下表现更佳。物理损失提高了对噪声的鲁棒性，重采样配置点等简单策略显著提高了准确性。零样本物理信息微调能将测试误差降低到约1%，明显优于从头开始的纯物理训练。&lt;h4&gt;结论&lt;/h4&gt;PI-MFM为数据高效、可迁移的PDE求解器提供了实用且可扩展的路径。&lt;h4&gt;翻译&lt;/h4&gt;偏微分方程(PDEs)控制着广泛的物理系统，最近的多模态基础模型在跨不同方程族学习PDE解算子方面显示出前景。然而，现有的多算子学习方法数据需求量大，并且在训练过程中忽视物理规律。在此，我们提出了一种物理信息多模态基础模型(PI-MFM)框架，直接在预训练和适应阶段强制执行控制方程。PI-MFM以PDE的符号表示作为输入，通过向量化导数计算自动组装PDE残差损失。这些设计使得任何PDE编码的多模态基础模型能够在跨方程族时使用统一的物理信息目标进行训练或适应。在包含13个参数化一维时变PDE族的基准测试中，PI-MFM始终优于纯数据驱动的对应方法，特别是在标记时空点稀少、时间域部分观测或标记函数对较少的情况下表现更佳。物理损失进一步提高了对噪声的鲁棒性，简单的策略如重采样配置点显著提高了准确性。我们还分析了PI-MFM中自动微分和有限差分在导数计算方面的准确性、精确度和计算成本。最后，我们展示了零样本物理信息微调到未见过的PDE族：从物理信息预训练模型开始，仅使用PDE残差和初始/边界条件进行适应，无需任何标记的解数据，就能快速将测试误差降低到约1%，明显优于从头开始的纯物理训练。这些结果表明，PI-MFM为数据高效、可迁移的PDE求解器提供了实用且可扩展的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.</description>
      <author>example@mail.com (Min Zhu, Jingmin Sun, Zecheng Zhang, Hayden Schaeffer, Lu Lu)</author>
      <guid isPermaLink="false">2512.23056v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Geometric Structural Knowledge Graph Foundation Model</title>
      <link>http://arxiv.org/abs/2512.22931v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE TPAMI, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Gamma的新型知识图谱基础模型，通过引入多头几何注意力机制和关系条件注意力融合机制，解决了现有方法在处理全新图结构时表达能力受限的问题，在零归纳链接预测任务上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;现有知识图谱基础模型（如Ultra）在推理过程中依赖单一的关系转换（如逐元素乘法），这种限制无法捕捉多样化图中展示的不同关系和结构模式，导致在处理包含未见实体和关系的全新图时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够泛化到包含未见实体和关系的全新图的知识图谱基础模型，提高模型对不同关系和结构模式的捕捉能力，从而提升零归纳链接预测性能。&lt;h4&gt;方法&lt;/h4&gt;Gamma模型引入了多头几何注意力机制，使用多个并行转换（包括实数、复数、分裂复数和双数基础转换）替代单一的关系转换，每种转换专门设计用于建模不同的关系结构。通过关系条件注意力融合机制，使用轻量级门控和熵正则化在链接级别自适应地融合这些转换，使模型能够为每个三元组模式强调最合适的关系偏差。&lt;h4&gt;主要发现&lt;/h4&gt;在56个多样化知识图谱上的实验表明，Gamma在零归纳链接预测任务上始终优于Ultra模型，在归纳基准测试上的平均倒数排名提高了5.5%，在所有基准测试上平均提高了4.4%，证明了互补几何表示的有效性。&lt;h4&gt;结论&lt;/h4&gt;Gamma模型通过结合多种代数变换空间显著增强了知识图谱基础模型的表达能力，使其能够更好地捕捉多样化图中的复杂关系和结构模式，为处理包含未见实体和关系的全新图提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;结构化知识图谱基础模型旨在将推理泛化到包含未见实体和关系的全新图。现有方法（如Ultra）的一个关键局限是它们在消息传递中依赖单一的关系转换（如逐元素乘法），这限制了表达能力，无法捕捉多样化图上展示的不同关系和结构模式。在本文中，我们提出了Gamma，一种引入多头几何注意力进行知识图谱推理的新型基础模型。Gamma用多个并行转换（包括基于实数、复数、分裂复数和双数的转换）替代单一的关系转换，每种转换都设计用于建模不同的关系结构。然后，关系条件注意力融合机制通过轻量级门控和熵正则化在链接级别自适应地融合这些转换，使模型能够稳健地强调每个三元组模式最合适的关系偏差。我们对这些代数消息函数进行了完整的公式化，并讨论了它们的组合如何超越任何单一空间的表达能力。在56个多样化知识图谱上的全面实验表明，Gamma在零归纳链接预测方面始终优于Ultra，在归纳基准测试上的平均倒数排名提高了5.5%，在所有基准测试上提高了4.4%，突显了互补几何表示的益处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.</description>
      <author>example@mail.com (Ling Xin, Mojtaba Nayyeri, Zahra Makki Nayeri, Steffen Staab)</author>
      <guid isPermaLink="false">2512.22931v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings</title>
      <link>http://arxiv.org/abs/2512.22398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;知识图谱基础模型在链接预测中表现良好但无法捕捉个体用户偏好，研究者提出GatedBias框架实现轻量级个性化适应，仅需约300个参数即可实现可解释的每实体偏差，在保持全局准确性的同时显著提升个性化排名性能。&lt;h4&gt;背景&lt;/h4&gt;知识图谱的基础模型在链接预测方面表现出强大的群体级性能，但未能捕捉个体用户偏好，这是通用关系推理与个性化排序之间的关键脱节。&lt;h4&gt;目的&lt;/h4&gt;提出GatedBias，一个轻量级的推理时个性化框架，能够适应冻结的知识图谱嵌入到个体用户上下文中，无需重新训练或损害全局准确性。&lt;h4&gt;方法&lt;/h4&gt;引入结构化门控适应：特定于配置文件的特征与图派生的二进制门相结合，产生可解释的、每实体偏差，仅需约300个可训练参数。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准数据集（Amazon-Book和Last-FM）上评估GatedBias，在保持群体性能的同时，在一致性指标上显示出统计学上的显著改进。反事实扰动实验验证了因果响应性；从特定偏好信号中受益的实体，当这些信号被增强时，显示出6-30倍更大的排名改进。&lt;h4&gt;结论&lt;/h4&gt;基础模型的个性化适应可以是参数高效且因果可验证的，弥合了通用知识表示与个体用户需求之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱的基础模型在链接预测中表现出强大的群体级性能，但未能捕捉个体用户偏好；这是通用关系推理与个性化排序之间的关键脱节。我们提出了GatedBias，一个轻量级的推理时个性化框架，它能够将冻结的知识图谱嵌入适应到个体用户上下文中，无需重新训练或损害全局准确性。我们的方法引入了结构化门控适应：特定于用户配置文件的特征与从图派生的二进制门相结合，产生可解释的、每实体偏差，仅需约300个可训练参数。我们在两个基准数据集（Amazon-Book和Last-FM）上评估了GatedBias，证明在保持群体性能的同时，一致性指标有统计学上的显著改进。反事实扰动实验验证了因果响应性；从特定偏好信号中受益的实体，当这些信号被增强时，显示出6-30倍更大的排名改进。这些结果表明，基础模型的个性化适应可以是参数高效且因果可验证的，弥合了通用知识表示与个体用户需求之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.</description>
      <author>example@mail.com (Ozan Oguztuzun, Cerag Oguztuzun)</author>
      <guid isPermaLink="false">2512.22398v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Graph Attention-based Adaptive Transfer Learning for Link Prediction</title>
      <link>http://arxiv.org/abs/2512.22252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM TIST, 9 tables, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为图注意力自适应转移网络(GAATNet)的新方法，用于解决图神经网络在链接预测任务中面临的挑战，特别是在处理大规模稀疏图和跨数据集转移学习方面。&lt;h4&gt;背景&lt;/h4&gt;图神经网络为链接预测领域带来了革命性进展，但现有方法在处理大规模稀疏图时面临挑战，且转移学习需要不同数据集间的高度对齐。尽管自监督方法在图任务中表现出色，但先前研究忽视了转移学习在不同图数据集间的泛化潜力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一种能够捕获不同规模数据集间全局节点嵌入信息的图注意力自适应转移网络，确保高效知识转移和改进的链接预测性能。&lt;h4&gt;方法&lt;/h4&gt;GAATNet结合预训练和微调的优势，设计了两个关键策略：1)在自注意力模块中将远邻嵌入作为偏差，以捕获全局特征；2)在微调期间引入轻量级自适配器模块，提高训练效率。&lt;h4&gt;主要发现&lt;/h4&gt;在七个公共数据集上的综合实验表明，GAATNet在链接预测任务中实现了最先进的性能，为链接预测任务提供了一种通用且可扩展的解决方案，有效整合了图神经网络与转移学习。&lt;h4&gt;结论&lt;/h4&gt;GAATNet能够有效处理大规模稀疏图，通过转移学习实现了不同图数据集间的知识泛化，源代码和数据集已在GitHub上公开可用。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络为链接预测领域带来了革命性进展，为挖掘图中潜在关系提供了强大工具。然而，现有方法在处理大规模稀疏图时面临挑战，且转移学习中不同数据集之间需要高度对齐。此外，尽管自监督方法在许多图任务中取得了显著成功，但先前研究忽视了转移学习在不同图数据集之间泛化的潜力。为解决这些局限性，我们提出了一种新颖的图注意力自适应转移网络(GAATNet)。它结合了预训练和微调的优势，捕获不同规模数据集间的全局节点嵌入信息，确保高效的知识转移和改进的链接预测性能。为增强模型的泛化能力和加速训练，我们设计了两个关键策略：1)在自注意力模块中将远邻嵌入作为偏差，以捕获全局特征；2)在微调期间引入轻量级自适配器模块，提高训练效率。在七个公共数据集上的综合实验表明，GAATNet在链接预测任务中实现了最先进的性能。本研究为链接预测任务提供了一种通用且可扩展的解决方案，有效整合了图神经网络与转移学习。源代码和数据集可在https://github.com/DSI-Lab1/GAATNet公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have brought revolutionary advancements to the field of link prediction (LP), providing powerful tools for mining potential relationships in graphs. However, existing methods face challenges when dealing with large-scale sparse graphs and the need for a high degree of alignment between different datasets in transfer learning. Besides, although self-supervised methods have achieved remarkable success in many graph tasks, prior research has overlooked the potential of transfer learning to generalize across different graph datasets. To address these limitations, we propose a novel Graph Attention Adaptive Transfer Network (GAATNet). It combines the advantages of pre-training and fine-tuning to capture global node embedding information across datasets of different scales, ensuring efficient knowledge transfer and improved LP performance. To enhance the model's generalization ability and accelerate training, we design two key strategies: 1) Incorporate distant neighbor embeddings as biases in the self-attention module to capture global features. 2) Introduce a lightweight self-adapter module during fine-tuning to improve training efficiency. Comprehensive experiments on seven public datasets demonstrate that GAATNet achieves state-of-the-art performance in LP tasks. This study provides a general and scalable solution for LP tasks to effectively integrate GNNs with transfer learning. The source code and datasets are publicly available at https://github.com/DSI-Lab1/GAATNet</description>
      <author>example@mail.com (Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu)</author>
      <guid isPermaLink="false">2512.22252v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Information is localized in growing network models</title>
      <link>http://arxiv.org/abs/2512.23622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文展示了在广泛的增长网络模型中，模型参数的信息在网络中是局部化的，即似然函数可以用小子图表达。作者采用贝叶斯推断方法，开发了使用有限感受野的图神经网络来近似后验分布的神经密度估计器，即使在非局部化模型中也能高效推断高保真后验分布。&lt;h4&gt;背景&lt;/h4&gt;机制性网络模型可以使用少量特定领域、可解释的机制捕捉经验网络的重要特征，但推断过程具有挑战性，因为似然函数通常不可处理。&lt;h4&gt;目的&lt;/h4&gt;展示在增长网络模型中，模型参数信息在网络中的局部化特性，并开发基于有限感受野的图神经网络进行高效推断的方法。&lt;h4&gt;方法&lt;/h4&gt;采用贝叶斯推断视角，开发神经密度估计器（NDEs）使用具有有限感受野的图神经网络（GNNs）来近似模型参数的后验分布。描述了九种增长网络模型的局部化特性，并在模拟数据上验证了局部化预测与NDEs的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;信息局部化是网络增长的基本特性；即使在非局部化模型中，NDEs也能以较低成本推断出与特定模型推断方法相当的高保真后验分布。&lt;h4&gt;结论&lt;/h4&gt;信息局部化作为网络增长的基本特性，理论上支持了分析嵌入在更大网络中的局部小子图，以及使用有限感受野的GNNs进行无似然推断的合理性。&lt;h4&gt;翻译&lt;/h4&gt;机制性网络模型可以使用少量特定领域、可解释的机制捕捉经验网络的重要特征。然而推断仍然具有挑战性，因为似然函数通常不可处理。我们表明，对于一类广泛的增长网络模型，关于模型参数的信息在网络中是局部化的，即似然函数可以用小子图来表达。我们采用贝叶斯视角进行推断，并开发了神经密度估计器（NDEs）来使用具有有限感受野的图神经网络（GNNs）近似模型参数的后验分布，即GNN只能"看到"小子图。我们从局部化角度描述了九种增长网络模型，并证明局部化预测与模拟数据上的NDEs一致。即使对于非局部化模型，NDEs也能推断出高保真度的后验分布，与特定模型的推断方法相比，成本只是其中的一小部分。我们的研究发现确立了信息局部化作为网络增长的基本特性，理论上证明了嵌入在更大、未观察到的网络中的局部小子图分析的合理性，以及使用具有有限感受野的GNNs进行无似然推断的合理性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mechanistic network models can capture salient characteristics of empirical networks using a small set of domain-specific, interpretable mechanisms. Yet inference remains challenging because the likelihood is often intractable. We show that, for a broad class of growing network models, information about model parameters is localized in the network, i.e., the likelihood can be expressed in terms of small subgraphs. We take a Bayesian perspective to inference and develop neural density estimators (NDEs) to approximate the posterior distribution of model parameters using graph neural networks (GNNs) with limited receptive size, i.e., the GNN can only "see" small subgraphs. We characterize nine growing network models in terms of their localization and demonstrate that localization predictions agree with NDEs on simulated data. Even for non-localized models, NDEs can infer high-fidelity posteriors matching model-specific inference methods at a fraction of the cost. Our findings establish information localization as a fundamental property of network growth, theoretically justifying the analysis of local subgraphs embedded in larger, unobserved networks and the use of GNNs with limited receptive field for likelihood-free inference.</description>
      <author>example@mail.com (Till Hoffmann, Jukka-Pekka Onnela)</author>
      <guid isPermaLink="false">2512.23622v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Task-driven Heterophilic Graph Structure Learning</title>
      <link>http://arxiv.org/abs/2512.23406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了频率引导的图结构学习(FgGSL)框架，解决了图神经网络在异质性图中学习判别性节点表示的困难问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在学习异质性图时存在困难，因为在这类图中，连接的节点往往具有不同标签，特征相似性提供较弱的结构线索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时学习同质性和异质性图结构的端到图推理框架，提高GNN在异质性图上的表现。&lt;h4&gt;方法&lt;/h4&gt;提出频率引导的图结构学习(FgGSL)框架，使用可学习的对称特征驱动掩码函数推断互补图，通过预设计的低通和高通图滤波器组处理，并引入基于标签的结构损失实现任务驱动的图结构学习。&lt;h4&gt;主要发现&lt;/h4&gt;在六个异质性基准测试上，FgGSL始终优于最先进的GNN和图重连方法，证明了结合频率信息和监督拓扑推断的有效性。&lt;h4&gt;结论&lt;/h4&gt;频率引导的图结构学习方法能够有效处理异质性图中的节点表示学习问题，为图神经网络在异质性数据上的应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通常难以学习异质性图的判别性节点表示，在这类图中，连接的节点往往具有不同的标签，特征相似性提供较弱的结构线索。我们提出了频率引导的图结构学习(FgGSL)，这是一个端到端的图推理框架，联合学习同质性和异质性图结构与谱编码器。FgGSL使用可学习的、对称的、特征驱动的掩码函数来推断互补图，这些图通过预设计的低通和高通图滤波器组处理。基于标签的结构损失显式促进同质性和异质性边的恢复，实现任务驱动的图结构学习。我们推导了结构损失的稳定性界限，并建立了图扰动下滤波器组的鲁棒性保证。在六个异质性基准测试上的实验表明，FgGSL始终优于最先进的GNN和图重连方法，突显了结合频率信息与监督拓扑推断的益处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.</description>
      <author>example@mail.com (Ayushman Raghuvanshi, Gonzalo Mateos, Sundeep Prabhakar Chepuri)</author>
      <guid isPermaLink="false">2512.23406v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Causality-Inspired Safe Residual Correction for Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2512.22428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CRC（Causality-inspired Safe Residual Correction）的即插即用框架，用于解决多元预测器在部署中的性能下降问题，通过因果启发的编码器和混合校正器，结合严格的安全机制确保预测性能不会退化。&lt;h4&gt;背景&lt;/h4&gt;现代多元预测器如Transformers和GNNs在基准测试中表现良好，但往往在特定变量或时间跨度上存在系统性错误，且缺乏防止部署性能下降的保证。现有的后验残差校正方法虽然是贪心的，可能提高平均准确性，但可能通过过度校正可靠的预测导致在未见场景中产生局部故障。&lt;h4&gt;目的&lt;/h4&gt;解决预测器部署中的'安全差距'问题，提出一种能够确保预测性能不会下降的残差校正框架。&lt;h4&gt;方法&lt;/h4&gt;CRC采用'分而治之'的哲学，使用因果启发的编码器通过解耦自变量和交叉变量动态来暴露方向感知结构，并使用混合校正器建模残差误差。校正过程由严格的安全机制控制，防止有害更新。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集和预测骨干网络上的实验表明，CRC能够持续提高准确性，而深入的消融研究证实其核心安全机制确保了极高的非退化率（NDR）。&lt;h4&gt;结论&lt;/h4&gt;CRC是一个适合安全可靠部署的校正框架，能够在提高预测准确性的同时确保性能不会下降。&lt;h4&gt;翻译&lt;/h4&gt;虽然现代多元预测器如Transformers和GNNs实现了强大的基准性能，但它们通常在特定变量或时间跨度上遭受系统性错误，并且关键的是，缺乏防止部署中性能下降的保证。现有的后验残差校正方法试图修复这些错误，但本质上是贪心的：尽管它们可能提高平均准确性，但也可能通过过度校正可靠的预测并在未见场景中导致局部故障而'以错误的方式帮助'。为了解决这一关键的'安全差距'，我们提出了CRC（Causality-inspired Safe Residual Correction），一个明确设计为确保非退化的即插即用框架。CRC遵循'分而治之'的哲学：它采用因果启发的编码器通过解耦自变量和交叉变量动态来暴露方向感知结构，并使用混合校正器建模残差误差。关键的是，校正过程由严格的安全机制控制，防止有害更新。在多个数据集和预测骨干网络上的实验表明，CRC持续提高准确性，而深入的消融研究证实其核心安全机制确保了极高的非退化率（NDR），使CRC成为一个适合安全可靠部署的校正框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also "help in the wrong way" by overcorrecting reliable predictions and causing local failures in unseen scenarios.  To address this critical "safety gap," we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.</description>
      <author>example@mail.com (Jianxiang Xie, Yuncheng Hua)</author>
      <guid isPermaLink="false">2512.22428v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>Multi-label Classification with Panoptic Context Aggregation Networks</title>
      <link>http://arxiv.org/abs/2512.23486v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Deep Panoptic Context Aggregation Network (PanCAN)的新方法，通过跨尺度特征聚合整合多阶几何上下文，提高了复杂场景理解能力，并在多个基准测试上取得了优于最先进技术的结果。&lt;h4&gt;背景&lt;/h4&gt;上下文建模对视觉识别至关重要，能够通过整合图像中对象和标签的内在外在关系来提供高度判别性的图像表示。当前方法的局限性在于它们只关注基本几何关系或局部特征，往往忽视了对象之间的跨尺度上下文交互。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，能够有效建模跨尺度上下文交互，提高复杂场景理解能力，从而提升多标签分类性能。&lt;h4&gt;方法&lt;/h4&gt;作者提出了Deep Panoptic Context Aggregation Network (PanCAN)，通过在高维Hilbert空间中进行跨尺度特征聚合，分层次地整合多阶几何上下文。PanCAN通过结合随机游走和注意力机制来学习每个尺度的多阶邻域关系，不同尺度的模块级联连接，在更精细尺度选择显著锚点，并通过注意力机制动态融合其邻域特征。&lt;h4&gt;主要发现&lt;/h4&gt;PanCAN通过结合多阶和跨尺度上下文感知特征，显著增强了复杂场景理解能力。在NUS-WIDE、PASCAL VOC2007和MS-COCO基准上的多标签分类实验表明，PanCAN始终取得具有竞争力的结果，在定量和定性评估中均优于最先进的技术。&lt;h4&gt;结论&lt;/h4&gt;PanCAN能够有效解决现有方法在跨尺度上下文交互建模方面的不足，显著提高多标签分类性能。&lt;h4&gt;翻译&lt;/h4&gt;上下文建模对视觉识别至关重要，通过整合图像中对象和标签的内在外在关系，能够实现高度判别性的图像表示。当前方法的局限性在于它们只关注基本几何关系或局部特征，往往忽视了对象之间的跨尺度上下文交互。本文引入了Deep Panoptic Context Aggregation Network (PanCAN)，一种新颖的方法，通过在高维Hilbert空间中进行跨尺度特征聚合，分层次地整合多阶几何上下文。具体来说，PanCAN通过结合随机游走和注意力机制来学习每个尺度的多阶邻域关系。不同尺度的模块级联连接，在更精细尺度选择显著锚点，并通过注意力机制动态融合其邻域特征。这使得通过结合多阶和跨尺度上下文感知特征，有效实现跨尺度建模，显著增强了复杂场景理解能力。在NUS-WIDE、PASCAL VOC2007和MS-COCO基准上的多标签分类实验表明，PanCAN始终取得具有竞争力的结果，在定量和定性评估中均优于最先进的技术，从而显著提高了多标签分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.</description>
      <author>example@mail.com (Mingyuan Jiu, Hailong Zhu, Wenchuan Wei, Hichem Sahbi, Rongrong Ji, Mingliang Xu)</author>
      <guid isPermaLink="false">2512.23486v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding</title>
      <link>http://arxiv.org/abs/2512.23215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AVOID数据集，用于解决自动驾驶汽车在恶劣条件下实时检测小道路障碍物的挑战。&lt;h4&gt;背景&lt;/h4&gt;道路场景的视觉感知对智能自动驾驶汽车至关重要，特别是在各种恶劣条件下可靠地检测意外的小道路危险。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的数据集AVOID（Adverse Visual Conditions Dataset），用于实时障碍物检测，解决现有数据集的局限性。&lt;h4&gt;方法&lt;/h4&gt;创建包含多种天气和时间条件下意外道路障碍物的数据集，每个图像配有语义图、深度图、LiDAR数据和路径点；在实时网络上进行基准测试，并使用多任务网络进行消融研究。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体发现，但暗示了新数据集的有效性。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及结论。&lt;h4&gt;翻译&lt;/h4&gt;理解道路场景的视觉感知对智能自动驾驶汽车仍然至关重要。特别是，在变化的恶劣条件下（例如天气和日光）可靠地实时检测意外的小道路危险是理想的。然而，现有的道路驾驶数据集仅提供在正常或恶劣场景下获取的大规模图像，并且通常不包含与其他类别在同一视觉域中捕获的道路障碍物。为此，我们引入了一个名为AVOID的新数据集，即恶劣视觉条件数据集，用于在模拟环境中收集的实时障碍物检测。AVOID包含大量位于各路径上的意外道路障碍物，在各种天气和时间条件下捕获。每个图像都配有相应的语义图和深度图、原始和语义LiDAR数据以及路径点，从而支持大多数视觉感知任务。我们在用于障碍物检测的高性能实时网络上对结果进行了基准测试，并提出了使用综合多任务网络进行语义分割、深度和路径点预测任务的消融研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶汽车在恶劣视觉条件下可靠检测意外小型道路障碍物的问题。这个问题很重要，因为全球每年约有130万道路交通死亡人数，恶劣天气条件（如雨天）会使碰撞风险比晴天高出70%，而意外道路障碍物往往是各种大小、形状和颜色的，且突然出现，对自动驾驶汽车的安全构成重大威胁。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有的语义分割数据集设计原则和基于模拟器的道路导航数据集（如CARLA）的工作。他们基于CARLA模拟器构建了一个新数据集，添加了清晰天气条件，共42种不同天气-白天组合，改进了数据收集方法（固定环境而非逐帧改变），添加了障碍物语义类别，并确保了多模态数据的时间同步。这种方法结合了现有数据集的优点，同时解决了它们的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个在恶劣视觉条件下采集的包含各种道路障碍物的大规模数据集，提供高质量的多模态标注。实现流程包括：使用CARLA模拟器创建环境；收集45种不同3D障碍物模型；在7种天气和6种白天条件下共42种组合中采集数据；同步收集立体RGB图像、深度图、LiDAR数据和路径点；自动标注传感器输出；将数据分为训练、验证和测试集；并在数据集上测试各种网络模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 引入了恶劣环境条件下的大规模障碍物检测数据集；2) 提供42种不同恶劣条件下的基准测试结果；3) 评估了基于Transformer的实时障碍物检测网络。相比之前工作，AVOID数据集规模更大、模态更多（包括立体图像、深度图、LiDAR数据等），环境条件更全面（42种组合），环境更稳定（固定而非逐帧改变），并添加了障碍物语义类别和全景视图采集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一个名为AVOID的大规模数据集，在42种不同恶劣视觉条件下采集，包含各种道路障碍物和高质量多模态标注，为自动驾驶汽车在恶劣环境下的障碍物检测提供了新的基准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding road scenes for visual perception remains crucial for intelligent self-driving cars. In particular, it is desirable to detect unexpected small road hazards reliably in real-time, especially under varying adverse conditions (e.g., weather and daylight). However, existing road driving datasets provide large-scale images acquired in either normal or adverse scenarios only, and often do not contain the road obstacles captured in the same visual domain as for the other classes. To address this, we introduce a new dataset called AVOID, the Adverse Visual Conditions Dataset, for real-time obstacle detection collected in a simulated environment. AVOID consists of a large set of unexpected road obstacles located along each path captured under various weather and time conditions. Each image is coupled with the corresponding semantic and depth maps, raw and semantic LiDAR data, and waypoints, thereby supporting most visual perception tasks. We benchmark the results on high-performing real-time networks for the obstacle detection task, and also propose and conduct ablation studies using a comprehensive multi-task network for semantic segmentation, depth and waypoint prediction tasks.</description>
      <author>example@mail.com (Jongoh Jeong, Taek-Jin Song, Jong-Hwan Kim, Kuk-Jin Yoon)</author>
      <guid isPermaLink="false">2512.23215v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2512.22939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ColaVLA是一个统一的视觉-语言-动作框架，通过认知潜在推理器和分层并行规划器解决了基于VLM的自动驾驶规划器面临的关键挑战，实现了高效、准确和安全的轨迹生成，并在nuScenes基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶需要从复杂的多模态输入生成安全可靠的轨迹。传统模块化流水线分离感知、预测和规划，而端到端系统联合学习这些组件。视觉语言模型通过引入跨模态先验和常识推理丰富了这一范式，但当前基于VLM的规划器面临三个关键挑战：离散文本推理与连续控制不匹配、自回归思维链解码导致高延迟、低效或非因果的规划器限制实时部署。&lt;h4&gt;目的&lt;/h4&gt;解决当前基于VLM的自动驾驶规划器面临的三个关键挑战，开发一个能够高效、准确且安全生成轨迹的系统。&lt;h4&gt;方法&lt;/h4&gt;提出ColaVLA框架，将推理从文本转移到统一潜在空间并耦合分层并行轨迹解码器。认知潜在推理器通过自我适应选择将场景理解压缩为紧凑的元动作嵌入，仅需两次VLM前向传播。分层并行规划器在单次前向传播中生成多尺度、因果一致的轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes基准测试上，ColaVLA在开环和闭环设置中都达到最先进性能，具有有利的效率和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;ColaVLA成功解决了基于VLM的自动驾驶规划器面临的关键挑战，在保持VLM泛化性和可解释性的同时，实现了高效、准确和安全的轨迹生成。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶需要从复杂的多模态输入生成安全可靠的轨迹。传统的模块化流水线将感知、预测和规划分离，而最近的端到端系统联合学习它们。视觉语言模型通过引入跨模态先验和常识推理进一步丰富了这一范式，然而当前基于VLM的规划器面临三个关键挑战：(i)离散文本推理与连续控制之间的不匹配，(ii)自回归思维链解码导致的高延迟，(iii)低效或非因果的规划器限制了实时部署。我们提出了ColaVLA，一个统一的视觉-语言-动作框架，它将推理从文本转移到统一的潜在空间，并将其与分层、并行轨迹解码器耦合。认知潜在推理器通过自我适应选择将场景理解压缩为紧凑的、面向决策的元动作嵌入，仅需两次VLM前向传播。然后分层并行规划器在单次前向传播中生成多尺度、因果一致的轨迹。这些组件共同保留了VLM的泛化性和可解释性，同时实现高效、准确和安全的轨迹生成。在nuScenes基准测试上的实验表明，ColaVLA在开环和闭环设置中都取得了最先进的性能，并具有有利的效率和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.</description>
      <author>example@mail.com (Qihang Peng, Xuesong Chen, Chenye Yang, Shaoshuai Shi, Hongsheng Li)</author>
      <guid isPermaLink="false">2512.22939v1</guid>
      <pubDate>Wed, 31 Dec 2025 14:39:23 +0800</pubDate>
    </item>
    <item>
      <title>HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness</title>
      <link>http://arxiv.org/abs/2512.22014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种超图级超图同构网络框架，用于复杂系统的鲁棒性预测，解决了传统方法和现有深度学习方法在计算效率、高阶相关性捕捉和拓扑表达能力方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;复杂系统的鲁棒性具有重要的工程和经济意义，但传统的基于攻击的鲁棒性评估方法计算开销过高。现有的深度学习方法如CNN和GNN虽可作为代理模型进行快速预测，但忽略了真实世界中普遍存在的复杂高阶相关性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉复杂高阶相关性且具有更强拓扑表达能力的超图神经网络方法，用于高效预测复杂系统的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;受图同构网络启发，提出了一种超图级的超图同构网络框架，理论上证明其表达能力与超图Weisfeiler-Lehman测试严格等价，并将其应用于超图鲁棒性预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在保持训练和预测效率的同时，不仅优于现有的基于图的模型，而且在优先考虑拓扑结构表示的任务中也显著超越了传统的超图神经网络。&lt;h4&gt;结论&lt;/h4&gt;所提出的超图同构网络框架在计算效率和预测性能方面均优于现有方法，为复杂系统鲁棒性评估提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;复杂系统中的鲁棒性具有重要的工程和经济意义。然而，传统的基于攻击的事后鲁棒性评估会产生巨大的计算开销。最近，深度学习方法，如卷积神经网络(CNNs)和图神经网络(GNNs)，已被广泛用作代理模型进行快速鲁棒性预测。尽管如此，这些方法忽略了真实世界中普遍存在的复杂高阶相关性，这些相关性自然被建模为超图。虽然超图神经网络(HGNNs)已被广泛用于超图学习，但其拓扑表达能力尚未达到理论上限。为解决这一局限，受图同构网络启发，本文提出了一个超图级的超图同构网络框架。理论上证明，该方法具有与超图Weisfeiler-Lehman测试严格等价的表达能力，并应用于预测超图鲁棒性。实验结果表明，在保持训练和预测的优越效率的同时，所提出的方法不仅优于现有的基于图的模型，而且在优先考虑拓扑结构表示的任务中也显著超越了传统的HGNNs。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robustness in complex systems is of significant engineering and economic importance. However, conventional attack-based a posteriori robustness assessments incur prohibitive computational overhead. Recently, deep learning methods, such as Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), have been widely employed as surrogates for rapid robustness prediction. Nevertheless, these methods neglect the complex higher-order correlations prevalent in real-world systems, which are naturally modeled as hypergraphs. Although Hypergraph Neural Networks (HGNNs) have been widely adopted for hypergraph learning, their topological expressive power has not yet reached the theoretical upper bound. To address this limitation, inspired by Graph Isomorphism Networks, this paper proposes a hypergraph-level Hypergraph Isomorphism Network framework. Theoretically, this approach is proven to possess an expressive power strictly equivalent to the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experimental results demonstrate that while maintaining superior efficiency in training and prediction, the proposed method not only outperforms existing graph-based models but also significantly surpasses conventional HGNNs in tasks that prioritize topological structure representation.</description>
      <author>example@mail.com (Chengyu Tian, Wenbin Pei)</author>
      <guid isPermaLink="false">2512.22014v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
  <item>
      <title>AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification</title>
      <link>http://arxiv.org/abs/2512.21544v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AVP-Fusion的新型两阶段深度学习框架，用于抗病毒肽的准确识别，该框架结合了自适应特征融合和对比学习技术，在基准测试中取得了显著优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;当前计算方法难以捕捉复杂的序列依赖关系并有效处理模糊、难以分类的样本，这限制了抗病毒肽识别的准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种更准确的抗病毒肽识别方法，以加速新型抗病毒药物的开发过程。&lt;h4&gt;方法&lt;/h4&gt;AVP-Fusion是一个两阶段深度学习框架，首先使用10个不同描述符构建全景特征空间，引入自适应门控机制动态调节CNN提取的局部基序和BiLSTM捕获的全局依赖的权重；其次采用基于在线困难样本挖掘和BLOSUM62数据增强的对比学习策略；最后利用迁移学习进行病毒家族和特定病毒的亚类预测。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集Set 1上，AVP-Fusion达到0.9531的准确率和0.9064的MCC值，显著优于现有最先进的方法；即使在样本量有限的情况下，也能对六个病毒家族和八种特定病毒进行精确的亚类预测。&lt;h4&gt;结论&lt;/h4&gt;AVP-Fusion是高通量抗病毒药物筛选的稳健且可解释的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;抗病毒肽的准确识别对于加速新型药物开发至关重要。然而，当前的计算方法难以捕捉复杂的序列依赖关系并有效处理模糊、难以分类的样本。为解决这些挑战，我们提出了AVP-Fusion，一种新颖的两阶段深度学习框架，集成了自适应特征融合和对比学习。与传统的静态特征连接不同，我们使用10个不同的描述符构建全景特征空间，并引入自适应门控机制。该机制根据序列上下文动态调节CNN提取的局部基序和BiLSTM捕获的全局依赖的权重。此外，为解决数据分布挑战，我们采用基于在线困难样本挖掘和BLOSUM62数据增强驱动的对比学习策略，显著增强了模型的决策边界。在基准数据集Set 1上的实验结果表明，AVP-Fusion实现了0.9531的准确率和0.9064的MCC值，显著优于最先进的方法。在第二阶段，利用迁移学习，即使在样本量有限的情况下，该模型也能对六个病毒家族和八种特定病毒进行精确的亚类预测。总之，AVP-Fusion是高通量抗病毒药物筛选的一种稳健且可解释的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate identification of antiviral peptides (AVPs) is critical for accelerating novel drug development. However, current computational methods struggle to capture intricate sequence dependencies and effectively handle ambiguous, hard-to-classify samples. To address these challenges, we propose AVP-Fusion, a novel two-stage deep learning framework integrating adaptive feature fusion and contrastive learning. Unlike traditional static feature concatenation, we construct a panoramic feature space using 10 distinct descriptors and introduce an Adaptive Gating Mechanism.This mechanism dynamically regulates the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Furthermore, to address data distribution challenges, we employ a contrastive learning strategy driven by Online Hard Example Mining (OHEM) and BLOSUM62-based data augmentation, which significantly sharpens the model's decision boundaries. Experimental results on the benchmark Set 1 dataset demonstrate that AVP-Fusion achieves an accuracy of 0.9531 and an MCC of 0.9064, significantly outperforming state-of-the-art methods. In the second stage, leveraging transfer learning, the model enables precise subclass prediction for six viral families and eight specific viruses, even under limited sample sizes. In summary, AVP-Fusion serves as a robust and interpretable tool for high-throughput antiviral drug screening.</description>
      <author>example@mail.com (Xinru Wen, Weizhong Lin, Xuan Xiao)</author>
      <guid isPermaLink="false">2512.21544v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>QSAR-Guided Generative Framework for the Discovery of Synthetically Viable Odorants</title>
      <link>http://arxiv.org/abs/2512.23080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种结合变分自编码器(VAE)和定量结构-活性关系(QSAR)模型的框架，可以从有限的气味分子训练集中生成新型气味分子，解决了生成式人工智能通常需要大量分子进行学习的问题。&lt;h4&gt;背景&lt;/h4&gt;新型气味分子的发现对香水和风味行业至关重要，但在庞大的化学空间中高效筛选具有理想嗅觉特性的结构仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，利用有限的训练数据生成具有理想嗅觉特性的新型气味分子结构。&lt;h4&gt;方法&lt;/h4&gt;结合变分自编码器(VAE)和定量结构-活性关系(QSAR)模型的框架。VAE利用自监督学习能力从ChemBL数据库学习SMILES语法，并通过外部QSAR模型导出的损失项来增强训练目标，根据气味概率构建潜在表示。&lt;h4&gt;主要发现&lt;/h4&gt;模型生成了语法有效的结构(100%有效性)和94.8%的独特结构；潜在空间根据气味可能性有效构建，生成分子与已知气味分子之间的Fréchet ChemNet距离(FCD)约为6.96，而ChemBL基线约为21.6；74.4%的候选物具有不同于训练数据的新核心框架，表明模型进行了广泛的化学空间探索。&lt;h4&gt;结论&lt;/h4&gt;该框架成功利用有限的训练数据生成了新型气味分子，有效探索了化学空间，为香料和风味行业提供了有价值的分子设计工具。&lt;h4&gt;翻译&lt;/h4&gt;新型气味分子的发现对香水和风味行业至关重要，然而，在庞大的化学空间中高效导航以识别具有理想嗅觉特性的结构仍然是一个重大挑战。生成式人工智能为从头分子设计提供了有前景的方法，但通常需要大量分子进行学习。为解决这个问题，我们提出了一种结合变分自编码器(VAE)和定量结构-活性关系(QSAR)模型的框架，可以从有限的气味分子训练集中生成新型气味分子。VAE的自监督学习能力使其能够从ChemBL数据库学习SMILES语法，同时其训练目标通过从外部QSAR模型导出的损失项得到增强，以根据气味概率构建潜在表示。虽然VAE在学习QSAR监督信号方面表现出高度内部一致性，但针对外部未见过的真实数据集(Unique Good Scents)的验证确认，模型生成了语法有效的结构(通过拒绝采样实现了100%有效性)和94.8%的独特结构。潜在空间根据气味可能性得到有效构建，生成分子与已知气味分子之间的Fréchet ChemNet距离(FCD)约为6.96，而ChemBL基线约为21.6。通过Bemis-Murcko骨架进行的结构分析显示，74.4%的候选物具有不同于训练数据的新核心框架，表明模型在已知气味分子的简单衍生之外进行了广泛的化学空间探索。生成的候选物表现出物理化学特性...&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The discovery of novel odorant molecules is key for the fragrance and flavor industries, yet efficiently navigating the vast chemical space to identify structures with desirable olfactory properties remains a significant challenge. Generative artificial intelligence offers a promising approach for \textit{de novo} molecular design but typically requires large sets of molecules to learn from. To address this problem, we present a framework combining a variational autoencoder (VAE) with a quantitative structure-activity relationship (QSAR) model to generate novel odorants from limited training sets of odor molecules. The self-supervised learning capabilities of the VAE allow it to learn SMILES grammar from ChemBL database, while its training objective is augmented with a loss term derived from an external QSAR model to structure the latent representation according to odor probability. While the VAE demonstrated high internal consistency in learning the QSAR supervision signal, validation against an external, unseen ground truth dataset (Unique Good Scents) confirms the model generates syntactically valid structures (100\% validity achieved via rejection sampling) and 94.8\% unique structures. The latent space is effectively structured by odor likelihood, evidenced by a Fréchet ChemNet Distance (FCD) of $\approx$ 6.96 between generated molecules and known odorants, compared to $\approx$ 21.6 for the ChemBL baseline. Structural analysis via Bemis-Murcko scaffolds reveals that 74.4\% of candidates possess novel core frameworks distinct from the training data, indicating the model performs extensive chemical space exploration beyond simple derivatization of known odorants. Generated candidates display physicochemical properties ....</description>
      <author>example@mail.com (Tim C. Pearce, Ahmed Ibrahim)</author>
      <guid isPermaLink="false">2512.23080v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds</title>
      <link>http://arxiv.org/abs/2512.23042v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LAM3C的自监督框架，可以从无标签视频中学习3D表示，无需真实3D传感器。通过构建RoomTours视频生成点云数据集和引入噪声正则化损失，该方法在室内语义和实例分割任务上超越了现有自监督方法的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管3D自监督学习取得了进展，但收集大规模3D场景扫描仍然昂贵且耗时。&lt;h4&gt;目的&lt;/h4&gt;研究是否可以从没有真实3D传感器记录的无标签视频中学习3D表示。&lt;h4&gt;方法&lt;/h4&gt;提出LAM3C（基于Sinkhorn-Knopp的拉普拉斯感知多级3D聚类）自监督框架，从视频生成的点云中学习。构建RoomTours数据集，通过网络收集房间漫游视频并使用现成的前馈重建模型生成49,219个场景。同时提出噪声正则化损失，强制局部几何平滑性和确保在噪声点云下的特征稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;在没有使用任何真实3D扫描的情况下，LAM3C在室内语义和实例分割任务上表现优于之前的自监督方法。&lt;h4&gt;结论&lt;/h4&gt;无标签视频是3D自监督学习的丰富数据来源。&lt;h4&gt;翻译&lt;/h4&gt;尽管最近在3D自监督学习方面取得了进展，但收集大规模3D场景扫描仍然昂贵且耗时。在这项工作中，我们研究了是否可以从没有使用任何真实3D传感器记录的无标签视频中学习3D表示。我们提出了LAM3C（拉普拉斯感知多级3D聚类与Sinkhorn-Knopp），这是一个从无标签视频生成的点云中学习的自监督框架。我们首先引入了RoomTours，这是一个通过从网络上收集房间漫游视频（如房地产旅游）并使用现成的前馈重建模型生成49,219个场景而构建的视频生成点云数据集。我们还提出了一种噪声正则化损失，通过强制局部几何平滑性和确保在噪声点云下的特征稳定性来稳定表示学习。值得注意的是，在没有使用任何真实3D扫描的情况下，LAM3C在室内语义和实例分割上的性能优于先前的自监督方法。这些结果表明，无标签视频是3D自监督学习的丰富数据来源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent progress in 3D self-supervised learning, collecting large-scale 3D scene scans remains expensive and labor-intensive. In this work, we investigate whether 3D representations can be learned from unlabeled videos recorded without any real 3D sensors. We present Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp (LAM3C), a self-supervised framework that learns from video-generated point clouds from unlabeled videos. We first introduce RoomTours, a video-generated point cloud dataset constructed by collecting room-walkthrough videos from the web (e.g., real-estate tours) and generating 49,219 scenes using an off-the-shelf feed-forward reconstruction model. We also propose a noise-regularized loss that stabilizes representation learning by enforcing local geometric smoothness and ensuring feature stability under noisy point clouds. Remarkably, without using any real 3D scans, LAM3C achieves higher performance than the previous self-supervised methods on indoor semantic and instance segmentation. These results suggest that unlabeled videos represent an abundant source of data for 3D self-supervised learning.</description>
      <author>example@mail.com (Ryousuke Yamada, Kohsuke Ide, Yoshihiro Fukuhara, Hirokatsu Kataoka, Gilles Puy, Andrei Bursuc, Yuki M. Asano)</author>
      <guid isPermaLink="false">2512.23042v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs</title>
      <link>http://arxiv.org/abs/2512.22872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为Lamps的自监督学习方法，通过从多角度学习人体解剖结构，在胸部X光影像上预训练基础模型，显著提升了模型的鲁棒性、可转移性和临床应用潜力。&lt;h4&gt;背景&lt;/h4&gt;基础模型在自然语言处理和计算机视觉领域取得成功，因为它们能捕捉自然语言的基础结构。然而，在医学影像领域，基础在于人体解剖学，因为医学影像直接代表人体内部结构，反映了人体解剖的一致性、连贯性和层次性。现有的自监督学习方法往往忽视这些视角，限制了它们有效学习解剖特征的能力。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法的局限性，研究旨在构建一个能从多角度学习人体解剖结构的基础模型，并将其应用于大规模胸部X光影像的预训练。&lt;h4&gt;方法&lt;/h4&gt;研究团队构建了名为Lamps（通过自监督从多角度学习解剖学）的预训练模型，通过和谐地利用人体解剖的一致性、连贯性和层次性作为监督信号，在大规模胸部X光影像上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;在10个数据集上进行的广泛实验，通过微调和涌现特性分析表明，与10个基线模型相比，Lamps具有更强的鲁棒性、可转移性和临床应用潜力。&lt;h4&gt;结论&lt;/h4&gt;通过从多角度学习，Lamps为基础模型提供了独特的机会，使其能够开发出与人体解剖结构对齐的、有意义且鲁棒的表示。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在自然语言处理和计算机视觉中取得成功，因为它们能够捕捉自然语言的基础结构。然而，在医学影像中，关键基础在于人体解剖学，因为这些图像直接代表人体内部结构，反映了人体解剖的一致性、连贯性和层次性。然而，现有的自监督学习方法往往忽视了这些视角，限制了它们有效学习解剖特征的能力。为了克服这一局限性，我们构建了Lamps（通过自监督从多角度学习解剖学），在大规模胸部X光影像上进行预训练，通过和谐地利用人体解剖的一致性、连贯性和层次性作为监督信号。在10个数据集上进行的广泛实验，通过微调和涌现特性分析表明，与10个基线模型相比，Lamps具有更强的鲁棒性、可转移性和临床应用潜力。通过从多角度学习，Lamps为基础模型提供了独特的机会，使其能够开发出与人体解剖结构对齐的、有意义且鲁棒的表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.</description>
      <author>example@mail.com (Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding, Michael B. Gotway, Jianming Liang)</author>
      <guid isPermaLink="false">2512.22872v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction</title>
      <link>http://arxiv.org/abs/2512.22674v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by Fully3D 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于语义特征对比学习的新型损失函数，用于解决X射线CT稀疏视图重建中的条纹伪影问题，通过三阶段U-Net架构实现了高质量的重建效果和低计算复杂度。&lt;h4&gt;背景&lt;/h4&gt;X射线CT在医学影像中广泛应用，稀疏视图重建可减少辐射剂量，但不适定条件常导致严重条纹伪影。基于深度学习的方法虽有改进但仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有CT重建方法的挑战，提高重建质量，降低计算复杂度，为正交CT重建提供实用解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出新颖的语义特征对比学习损失函数，评估高级潜在空间中的语义相似性和浅层潜在空间中的解剖相似性。采用三阶段U-Net架构：粗略重建、细节细化、语义相似性测量。&lt;h4&gt;主要发现&lt;/h4&gt;在胸部数据集上的测试表明，该方法相比其他算法实现了更好的重建质量和更快的处理速度，图像质量显著改善同时保持低计算复杂度。&lt;h4&gt;结论&lt;/h4&gt;该方法是一种实用的正交CT重建解决方案。&lt;h4&gt;翻译&lt;/h4&gt;X射线计算机断层扫描(CT)在医学影像中广泛应用，稀疏视图重建是减少辐射剂量的有效方法。然而，不适定条件通常会导致严重的条纹伪影。基于深度学习的最新方法已提高重建质量，但挑战仍然存在。为应对这些挑战，我们提出了一种新颖的语义特征对比学习损失函数，评估高级潜在空间中的语义相似性和浅层潜在空间中的解剖相似性。我们的方法采用基于三阶段U-Net的架构：一个用于粗略重建，一个用于细节细化，一个用于语义相似性测量。在具有正交投影的胸部数据集上的测试表明，我们的方法相比其他算法实现了更好的重建质量和更快的处理速度。结果显示图像质量有显著改善，同时保持低计算复杂度，使其成为正交CT重建的实用解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决正交CT重建中的稀疏视图重建问题，减少辐射剂量的同时避免严重条纹伪影。这个问题很重要，因为传统CT需要大量X射线投影采样，增加辐射剂量和成像时间，而减少投影数量会引入重建挑战，尤其在正交投影场景下问题更复杂。高质量重建图像对临床诊断、放射治疗等应用至关重要，同时减少辐射剂量能提高患者安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法（GAN和扩散模型）的局限性：GAN存在训练不稳定、模式崩溃和幻觉特征问题，扩散模型则训练收敛慢、计算开销大。然后结合语义对比学习改进重建质量，同时保持计算效率。他们设计了三阶段U-Net架构，借鉴了GAN框架、对比学习方法、U-Net架构以及已有的正交投影重建研究，如X2CT-GAN，并创新性地加入了语义对比学习损失函数。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用语义对比学习损失函数评估高层语义相似性和低层解剖相似性，采用从粗糙到精细的重建框架，并通过语义一致性约束减少幻觉伪影。整体流程分三阶段：1）粗略重建阶段，通过几何关系将正交投影反投影，用3D U-Net生成基本解剖结构；2）细节精炼阶段，用2D U-Net处理每个切片，结合多种损失函数增强细节；3）语义相似性测量阶段，通过学生-教师模型对比重建图像与原始图像的语义相似性，使用正样本计算MSE损失，负样本计算InfoNCE损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）新的语义特征对比学习损失函数；2）三阶段U-Net架构；3）训练使用三个U-Net，推理只用两个，提高效率；4）通过语义一致性约束减少幻觉伪影；5）结合多种损失函数。相比之前工作，与传统方法相比能在极稀疏投影下工作；与GAN方法相比减少幻觉伪影，提高训练稳定性；与扩散模型相比显著提高重建速度（低于0.4秒）；与其他深度学习方法相比结合语义信息提高解剖准确性，在多种评估指标上表现更优，更适合临床应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于语义对比学习的正交CT重建方法，通过创新的损失函数设计和高效的三阶段网络架构，在减少辐射剂量的同时实现了高质量、快速且稳定的医学图像重建，解决了现有方法中存在的伪影、计算效率和训练稳定性问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; X-ray computed tomography (CT) is widely used in medical imaging, with sparse-view reconstruction offering an effective way to reduce radiation dose. However, ill-posed conditions often result in severe streak artifacts. Recent advances in deep learning-based methods have improved reconstruction quality, but challenges still remain. To address these challenges, we propose a novel semantic feature contrastive learning loss function that evaluates semantic similarity in high-level latent spaces and anatomical similarity in shallow latent spaces. Our approach utilizes a three-stage U-Net-based architecture: one for coarse reconstruction, one for detail refinement, and one for semantic similarity measurement. Tests on a chest dataset with orthogonal projections demonstrate that our method achieves superior reconstruction quality and faster processing compared to other algorithms. The results show significant improvements in image quality while maintaining low computational complexity, making it a practical solution for orthogonal CT reconstruction.</description>
      <author>example@mail.com (Jiashu Dong, Jiabing Xiang, Lisheng Geng, Suqing Tian, Wei Zhao)</author>
      <guid isPermaLink="false">2512.22674v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding</title>
      <link>http://arxiv.org/abs/2512.22481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SPECTRE是一种针对sEMG信号的领域特定自监督学习框架，通过基于生理学的预训练任务和圆柱旋转位置编码，显著提升了从非侵入性表面肌电图解码精细运动的能力。&lt;h4&gt;背景&lt;/h4&gt;从非侵入性表面肌电图(sEMG)解码精细运动对于假肢控制是一个挑战，主要来自信号的非平稳性和低信噪比。通用自监督学习框架在sEMG上通常效果不佳，因为它们试图重建嘈杂的原始信号，并且缺乏对电极阵列圆柱拓扑的归纳偏置。&lt;h4&gt;目的&lt;/h4&gt;克服通用SSL框架在sEMG信号处理上的局限性，开发一个针对sEMG领域的特定SSL框架，以提高运动解码的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了SPECTRE框架，包含两个主要贡献：1)基于生理学的预训练任务，使用聚类短时傅里叶变换表示进行离散伪标签的掩码预测；2)圆柱旋转位置嵌入(CyRoPE)，沿线性时间和环形空间维度分解嵌入，明确建模前臂传感器拓扑以捕获肌肉协同作用。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的评估表明，SPECTRE建立了运动解码的新技术水平，显著优于监督基线和通用SSL方法。消融研究验证了频谱预训练和CyRoPE的关键作用。&lt;h4&gt;结论&lt;/h4&gt;SPECTRE为能够处理现实世界sEMG复杂性的实用肌电接口提供了稳健的基础。&lt;h4&gt;翻译&lt;/h4&gt;从非侵入性表面肌电图(sEMG)解码精细运动对于假肢控制是一个挑战，这源于信号的非平稳性和低信噪比。通用的自监督学习(SSL)框架在sEMG上通常产生次优结果，因为它们试图重建嘈杂的原始信号，并且缺乏对电极阵列圆柱拓扑的归纳偏置。为了克服这些局限性，我们引入了SPECTRE，一个领域特定的SSL框架。SPECTRE有两个主要贡献：一个基于生理学的预训练任务和一个新颖的位置编码。预训练涉及从聚类的短时傅里叶变换(STFT)表示中预测离散伪标签，迫使模型学习稳健的、生理相关的频率模式。此外，我们的圆柱旋转位置嵌入(CyRoPE)沿线性时间和环形空间维度分解嵌入，明确建模前臂传感器拓扑以捕获肌肉协同作用。在多个数据集上的评估，包括来自截肢者的具有挑战性的数据，表明SPECTRE为运动解码建立了新的技术水平，显著优于监督基线和通用SSL方法。消融研究验证了频谱预训练和CyRoPE的关键作用。SPECTRE为能够处理现实世界sEMG复杂性的实用肌电接口提供了稳健的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decoding fine-grained movement from non-invasive surface Electromyography (sEMG) is a challenge for prosthetic control due to signal non-stationarity and low signal-to-noise ratios. Generic self-supervised learning (SSL) frameworks often yield suboptimal results on sEMG as they attempt to reconstruct noisy raw signals and lack the inductive bias to model the cylindrical topology of electrode arrays. To overcome these limitations, we introduce SPECTRE, a domain-specific SSL framework. SPECTRE features two primary contributions: a physiologically-grounded pre-training task and a novel positional encoding. The pre-training involves masked prediction of discrete pseudo-labels from clustered Short-Time Fourier Transform (STFT) representations, compelling the model to learn robust, physiologically relevant frequency patterns. Additionally, our Cylindrical Rotary Position Embedding (CyRoPE) factorizes embeddings along linear temporal and annular spatial dimensions, explicitly modeling the forearm sensor topology to capture muscle synergies. Evaluations on multiple datasets, including challenging data from individuals with amputation, demonstrate that SPECTRE establishes a new state-of-the-art for movement decoding, significantly outperforming both supervised baselines and generic SSL approaches. Ablation studies validate the critical roles of both spectral pre-training and CyRoPE. SPECTRE provides a robust foundation for practical myoelectric interfaces capable of handling real-world sEMG complexities.</description>
      <author>example@mail.com (Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu)</author>
      <guid isPermaLink="false">2512.22481v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Head Spectral-Adaptive Graph Anomaly Detection</title>
      <link>http://arxiv.org/abs/2512.22291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种多头谱自适应图神经网络（MHSA-GNN），用于解决金融欺诈和风险控制中的复杂异常模式检测问题，通过动态生成定制化滤波器参数和双重正则化策略，有效保留了高频异常信号并提高了检测性能。&lt;h4&gt;背景&lt;/h4&gt;图异常检测技术在金融欺诈和风险控制中有广泛应用，但现有方法在处理复杂多变的异常模式时面临挑战。异常节点常常伪装并与正常节点混合，导致图中同时存在同质性和异质性。现有谱图神经网络虽取得进展，但通常采用固定、全局共享的滤波器，这种'一刀切'的方法容易导致过平滑，消除欺诈检测所需的关键高频信号，且对不同图实例缺乏自适应能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有图异常检测方法在处理复杂异常模式时的局限性，特别是解决固定全局滤波器导致的过平滑问题和缺乏自适应能力的问题，从而更好地保留高频异常信号并提高检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出多头谱自适应图神经网络（MHSA-GNN），核心创新是设计了一个轻量级超网络，该网络基于包含结构统计量和瑞利商特征的'谱指纹'，动态生成针对每个实例定制的切比雪夫滤波器参数，使每个节点及其局部子图能够采用定制化过滤策略。此外，引入双重正则化策略，结合教师-学生对比学习（TSC）确保表示准确性，以及Barlow Twins多样性损失（BTD）强制各头部之间正交。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界数据集上的广泛实验表明，该方法能有效保留高频异常信号，并显著优于现有最先进的方法，特别是在高度异构数据集上表现出卓越的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MHSA-GNN通过动态自适应的滤波策略和双重正则化机制，有效解决了传统图异常检测方法在复杂场景下的局限性，特别是在金融欺诈检测等需要保留高频异常信号的应用中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;图异常检测技术在金融欺诈和风险控制中有广泛应用。然而，当处理复杂多变的异常模式时，现有的图异常检测方法往往面临重大挑战，因为异常节点常常伪装并与正常节点混合，导致图中同时存在同质性和异质性。最近的谱图神经网络在解决这个问题方面取得了显著进展；然而，当前技术通常采用固定、全局共享的滤波器。这种'一刀切'的方法容易导致过平滑，消除欺诈检测所需的关键高频信号，并且对不同图实例缺乏自适应能力。为了解决这个问题，我们提出了一种多头谱自适应图神经网络（MHSA-GNN）。核心创新是设计了一个轻量级超网络，该网络基于包含结构统计量和瑞利商特征的'谱指纹'，动态生成针对每个实例定制的切比雪夫滤波器参数。这使得每个节点及其局部子图能够采用定制化的过滤策略。此外，为防止多head机制中的模式崩溃，我们引入了一种新颖的双重正则化策略，结合教师-学生对比学习（TSC）确保表示准确性，以及Barlow Twins多样性损失（BTD）强制各头部之间正交。在四个真实世界数据集上的广泛实验表明，我们的方法能有效保留高频异常信号，并显著优于现有的最先进方法，特别是在高度异构数据集上表现出卓越的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection technology has broad applications in financial fraud and risk control. However, existing graph anomaly detection methods often face significant challenges when dealing with complex and variable abnormal patterns, as anomalous nodes are often disguised and mixed with normal nodes, leading to the coexistence of homophily and heterophily in the graph domain. Recent spectral graph neural networks have made notable progress in addressing this issue; however, current techniques typically employ fixed, globally shared filters. This 'one-size-fits-all' approach can easily cause over-smoothing, erasing critical high-frequency signals needed for fraud detection, and lacks adaptive capabilities for different graph instances. To solve this problem, we propose a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN). The core innovation is the design of a lightweight hypernetwork that, conditioned on a 'spectral fingerprint' containing structural statistics and Rayleigh quotient features, dynamically generates Chebyshev filter parameters tailored to each instance. This enables a customized filtering strategy for each node and its local subgraph. Additionally, to prevent mode collapse in the multi-head mechanism, we introduce a novel dual regularization strategy that combines teacher-student contrastive learning (TSC) to ensure representation accuracy and Barlow Twins diversity loss (BTD) to enforce orthogonality among heads. Extensive experiments on four real-world datasets demonstrate that our method effectively preserves high-frequency abnormal signals and significantly outperforms existing state-of-the-art methods, especially showing excellent robustness on highly heterogeneous datasets.</description>
      <author>example@mail.com (Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou)</author>
      <guid isPermaLink="false">2512.22291v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer</title>
      <link>http://arxiv.org/abs/2512.21233v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种统一触觉表示方法UniTacHand，解决了人类与机器人触觉数据不对齐的问题，实现了从人类到机器人的零样本触觉策略转移，提高了数据效率。&lt;h4&gt;背景&lt;/h4&gt;触觉感知对机器人手实现人类灵巧操作至关重要，特别是在视觉遮挡场景下，但应用常受限于难以收集大规模真实世界机器人触觉数据。&lt;h4&gt;目的&lt;/h4&gt;提出使用触觉手套收集低成本人类操作数据用于机器人策略学习，并解决人类与机器人触觉数据不对齐导致的策略转移困难问题。&lt;h4&gt;方法&lt;/h4&gt;提出UniTacHand统一表示方法，将人类和机器人触觉信号投影到MANO手模型的2D表面空间，标准化数据结构并嵌入空间上下文；然后引入对比学习方法，使用仅10分钟配对数据将它们对齐到统一潜在空间。&lt;h4&gt;主要发现&lt;/h4&gt;该方法实现了从人类到真实机器人的零样本触觉策略转移，能泛化到预训练数据中未见过的物体；在混合数据（人类和机器人演示）上共同训练比仅使用机器人数据获得更好的性能和数据效率。&lt;h4&gt;结论&lt;/h4&gt;UniTacHand为基于触觉的灵巧手实现通用、可扩展和数据高效的学习铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;触觉感知对机器人手实现人类灵巧操作水平至关重要，特别是在视觉遮挡场景中。然而，其应用常因难以收集大规模真实世界机器人触觉数据而受到阻碍。在本研究中，我们提出使用触觉手套收集低成本人类操作数据，用于基于触觉的机器人策略学习。人类与机器人触觉数据之间的不对齐使得从人类数据学习到的策略难以转移到机器人上。为弥合这一差距，我们提出了UniTacHand，一种统一表示方法，用于对齐灵巧手捕获的机器人触觉信息与手套获取的人类手触觉信息。首先，我们将人类手和机器人手的触觉信号投影到MANO手模型的形态一致的2D表面空间上。这种统一标准化了异构数据结构，并将空间上下文固有地嵌入触觉信号中。然后，我们引入对比学习方法，将它们对齐到统一的潜在空间，仅使用我们数据收集系统中10分钟的配对数据进行训练。我们的方法实现了从人类到真实机器人的零样本基于触觉的策略转移，泛化到预训练数据中未见过的物体。我们还证明，通过UniTacHand在混合数据（包括人类和机器人演示）上进行共同训练，比仅使用机器人数据获得更好的性能和数据效率。UniTacHand为基于触觉的灵巧手实现通用、可扩展和数据高效的学习铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tactile sensing is crucial for robotic hands to achieve human-level dexterous manipulation, especially in scenarios with visual occlusion. However, its application is often hindered by the difficulty of collecting large-scale real-world robotic tactile data. In this study, we propose to collect low-cost human manipulation data using haptic gloves for tactile-based robotic policy learning. The misalignment between human and robotic tactile data makes it challenging to transfer policies learned from human data to robots. To bridge this gap, we propose UniTacHand, a unified representation to align robotic tactile information captured by dexterous hands with human hand touch obtained from gloves. First, we project tactile signals from both human hands and robotic hands onto a morphologically consistent 2D surface space of the MANO hand model. This unification standardizes the heterogeneous data structures and inherently embeds the tactile signals with spatial context. Then, we introduce a contrastive learning method to align them into a unified latent space, trained on only 10 minutes of paired data from our data collection system. Our approach enables zero-shot tactile-based policy transfer from humans to a real robot, generalizing to objects unseen in the pre-training data. We also demonstrate that co-training on mixed data, including both human and robotic demonstrations via UniTacHand, yields better performance and data efficiency compared with using only robotic data. UniTacHand paves a path toward general, scalable, and data-efficient learning for tactile-based dexterous hands.</description>
      <author>example@mail.com (Chi Zhang, Penglin Cai, Haoqi Yuan, Chaoyi Xu, Zongqing Lu)</author>
      <guid isPermaLink="false">2512.21233v3</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2512.23472v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为MCI-Net的多领域上下文集成网络，用于改进点云配准中的特征学习，通过从不同领域聚合上下文线索提高特征表示和配准性能。&lt;h4&gt;背景&lt;/h4&gt;现有的基于深度学习的点云配准方法通常依赖基于欧几里得邻域的特征提取策略，这些方法难以有效捕捉点云中的隐式语义和结构一致性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法无法有效捕捉点云隐式语义和结构一致性的问题，提高特征表示的鲁棒性和判别性。&lt;h4&gt;方法&lt;/h4&gt;1) 提出图邻域聚合模块，构建全局图捕获点云内的整体结构关系；2) 提出渐进式上下文交互模块，通过域内特征解耦和域间上下文交互增强特征判别性；3) 设计动态内点选择方法，利用多次姿态估计迭代的残差信息优化内点权重。&lt;h4&gt;主要发现&lt;/h4&gt;在室内RGB-D和室外LiDAR数据集上的实验表明，MCI-Net显著优于现有最先进方法，在3DMatch上实现了96.4%的最高配准召回率。&lt;h4&gt;结论&lt;/h4&gt;MCI-Net通过多领域上下文集成有效改进了点云配准性能，特别是在捕捉点云的隐式语义和结构一致性方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;鲁棒且具有判别性的特征学习对于高质量的点云配准至关重要。然而，现有的基于深度学习的方法通常依赖于基于欧几里得邻域的特征提取策略，这些策略难以有效捕捉点云中的隐式语义和结构一致性。为解决这些问题，我们提出了多领域上下文集成网络(MCI-Net)，通过从不同领域聚合上下文线索来改进特征表示和配准性能。具体来说，我们提出了一个图邻域聚合模块，构建全局图以捕获点云内的整体结构关系。然后，我们提出了一个渐进式上下文交互模块，通过执行域内特征解耦和域间上下文交互来增强特征判别性。最后，我们设计了一种动态内点选择方法，利用多次姿态估计迭代的残差信息优化内点权重，从而提高配准的准确性和鲁棒性。在室内RGB-D和室外LiDAR数据集上的大量实验表明，所提出的MCI-Net显著优于现有的最先进方法，在3DMatch上实现了96.4%的最高配准召回率。源代码可在http://www.linshuyuan.com获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust and discriminative feature learning is critical for high-quality point cloud registration. However, existing deep learning-based methods typically rely on Euclidean neighborhood-based strategies for feature extraction, which struggle to effectively capture the implicit semantics and structural consistency in point clouds. To address these issues, we propose a multi-domain context integration network (MCI-Net) that improves feature representation and registration performance by aggregating contextual cues from diverse domains. Specifically, we propose a graph neighborhood aggregation module, which constructs a global graph to capture the overall structural relationships within point clouds. We then propose a progressive context interaction module to enhance feature discriminability by performing intra-domain feature decoupling and inter-domain context interaction. Finally, we design a dynamic inlier selection method that optimizes inlier weights using residual information from multiple iterations of pose estimation, thereby improving the accuracy and robustness of registration. Extensive experiments on indoor RGB-D and outdoor LiDAR datasets show that the proposed MCI-Net significantly outperforms existing state-of-the-art methods, achieving the highest registration recall of 96.4\% on 3DMatch. Source code is available at http://www.linshuyuan.com.</description>
      <author>example@mail.com (Shuyuan Lin, Wenwu Peng, Junjie Huang, Qiang Qi, Miaohui Wang, Jian Weng)</author>
      <guid isPermaLink="false">2512.23472v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Autoregressive Flow Matching for Motion Prediction</title>
      <link>http://arxiv.org/abs/2512.22688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自回归流匹配(ARFM)的新方法，用于对连续序列数据进行概率建模，通过多样化视频数据集训练，能够生成长期的未来点轨迹位置，并在复杂运动预测和下游任务性能提升方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;运动预测研究已在不同背景下进行，现有模型通常在窄分布数据上训练并应用于人类运动预测和机器人等下游任务。同时，最近在视频预测扩展方面的努力展示了令人印象深刻的视觉真实性，但在准确建模复杂运动方面仍然存在困难，尽管规模巨大。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确建模复杂运动的新方法，用于预测人类和机器人的未来运动轨迹，并提高下游任务性能。&lt;h4&gt;方法&lt;/h4&gt;作者开发了自回归流匹配(ARFM)方法，这是一种对连续序列数据进行概率建模的新方法。他们在多样化的视频数据集上训练该方法，以生成长期的未来点轨迹位置。此外，他们还开发了评估基准，用于评估运动预测模型预测人类和机器人运动的能力。&lt;h4&gt;主要发现&lt;/h4&gt;该模型能够预测复杂运动，并且将机器人动作预测和人类运动预测基于预测的未来轨迹进行条件化处理，可以显著提高下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;自回归流匹配(ARFM)方法在复杂运动预测方面表现出色，并且通过将预测的未来轨迹作为条件，可以有效提高下游任务性能。代码和模型已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;运动预测已在不同背景下进行研究，模型在窄分布数据上训练并应用于人类运动预测和机器人等下游任务。同时，最近在扩展视频预测方面的努力展示了令人印象深刻的视觉真实性，但在准确建模复杂运动方面仍然存在困难，尽管规模巨大。受视频生成扩展的启发，我们开发了自回归流匹配(ARFM)，这是一种对连续序列数据进行概率建模的新方法，我们在多样化的视频数据集上对其进行训练，以生成长期的未来点轨迹位置。为了评估我们的模型，我们开发了评估基准，用于评估运动预测模型预测人类和机器人运动的能力。我们的模型能够预测复杂运动，我们证明将机器人动作预测和人类运动预测基于预测的未来轨迹进行条件化处理可以显著提高下游任务性能。代码和模型可在以下网址公开获取：https://github.com/Johnathan-Xie/arfm-motion-prediction。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.</description>
      <author>example@mail.com (Johnathan Xie, Stefan Stojanov, Cristobal Eyzaguirre, Daniel L. K. Yamins, Jiajun Wu)</author>
      <guid isPermaLink="false">2512.22688v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Test-Time Training for Long Context</title>
      <link>http://arxiv.org/abs/2512.23675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/test-time-training/e2e&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提出了一种将长上下文语言建模视为持续学习问题而非架构设计问题的新方法，使用标准Transformer架构但在测试时通过下一个词预测进行持续学习，并通过元学习改进初始化，实现了端到端的测试时训练。&lt;h4&gt;背景&lt;/h4&gt;长上下文语言建模通常被视为架构设计问题，但作者认为应从持续学习的角度重新审视。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决长上下文语言建模问题，能够在测试时持续学习并高效处理长上下文。&lt;h4&gt;方法&lt;/h4&gt;使用标准的Transformer架构（带滑动窗口注意力），在测试时通过给定上下文的下一个词预测进行持续学习，将读取的上下文压缩到权重中；同时通过训练时的元学习改进模型初始化，以便在测试时学习。这种方法被称为测试时训练（TTT-E2E）。&lt;h4&gt;主要发现&lt;/h4&gt;对于使用164B tokens训练的30亿模型，TTT-E2E方法与全注意力Transformer一样随上下文长度扩展，而其他方法（如Mamba 2和Gated DeltaNet）则不能。类似于RNN，TTT-E2E具有恒定的推理延迟，与上下文长度无关，对于128K上下文比全注意力快2.7倍。&lt;h4&gt;结论&lt;/h4&gt;TTT-E2E方法是端到端的，在测试时（通过下一个词预测）和训练时（通过元学习）都是端到端的，这与之前的形式不同。代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;我们将长上下文语言建模表述为一个持续学习问题，而非架构设计问题。在此表述下，我们仅使用标准架构——带滑动窗口注意力的Transformer。然而，我们的模型通过在给定上下文上进行下一个词预测在测试时持续学习，将其读取的上下文压缩到权重中。此外，我们通过训练时的元学习改进模型初始化，以便在测试时学习。总体而言，我们的方法，即测试时训练（TTT）的一种形式，在测试时（通过下一个词预测）和训练时（通过元学习）都是端到端的，与之前的形式形成对比。我们进行了大量实验，重点关注扩展特性。特别是，对于使用164B tokens训练的30亿模型，我们的方法（TTT-E2E）与全注意力Transformer一样随上下文长度扩展，而其他方法，如Mamba 2和Gated DeltaNet，则不能。然而，类似于RNN，TTT-E2E具有恒定的推理延迟，无论上下文长度如何，这使得对于128K上下文它比全注意力快2.7倍。我们的代码已公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.</description>
      <author>example@mail.com (Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, Yu Sun)</author>
      <guid isPermaLink="false">2512.23675v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks</title>
      <link>http://arxiv.org/abs/2512.23210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种自适应选择扩散时间步特征的方法，通过任务感知时间步选择和时间步特征合并两个模块，改进少样本密集预测任务性能。&lt;h4&gt;背景&lt;/h4&gt;去噪扩散概率模型在生成任务中取得显著进展，但当前应用中扩散时间步特征的选择依赖经验直觉，导致次优性能且偏向特定任务。&lt;h4&gt;目的&lt;/h4&gt;解决扩散时间步特征选择的启发式方法问题，通过自适应选择最适合少样本密集预测任务的时间步特征，提高在任意未见任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出两个关键模块：任务感知时间步选择(TTS)基于时间步损失和相似性分数选择理想扩散时间步；时间步特征合并(TFC)合并所选时间步特征以提高密集预测性能；同时采用参数高效的微调适配器。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在仅有少量支持查询的情况下能有效实现密集预测性能的优越性，在Taskonomy数据集上验证了该可学习时间步合并方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;自适应选择扩散时间步特征的方法能够显著改善少样本密集预测任务性能，为实际通用和少样本学习场景提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;去噪扩散概率模型在生成任务中带来了巨大进展，迄今为止取得了最先进的性能。当前基于扩散模型的应用通过附加任务特定解码器，利用多步前向-后向马尔可夫过程学习到的视觉表示能力进行单任务预测。然而，扩散时间步特征的经验选择仍然严重依赖经验直觉，常常导致偏向特定任务的次优性能。为了缓解这一限制，我们通过自适应选择最适合少样本密集预测任务的时间步特征，研究了多功能扩散时间步特征的重要性，在任意未见任务上进行评估。为此，我们提出了两个模块：基于时间步损失和相似性分数选择理想扩散时间步的任务感知时间步选择(TTS)，以及合并所选时间步特征以提高少样本设置下密集预测性能的时间步特征合并(TFC)。配合我们的参数高效微调适配器，我们的框架在仅有少量支持查询的情况下有效实现了密集预测性能的优越性。我们在大规模具有挑战性的Taskonomy数据集上针对密集预测任务进行了实证验证，特别是针对实际的通用和少样本学习场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.</description>
      <author>example@mail.com (Changgyoon Oh, Jongoh Jeong, Jegyeong Cho, Kuk-Jin Yoon)</author>
      <guid isPermaLink="false">2512.23210v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data</title>
      <link>http://arxiv.org/abs/2512.22349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了一种感知伪彩色技术可提高深度神经网络分析生理数据时的可解释性和少样本学习能力，使模型从极少量样本中学习临床相关特征。&lt;h4&gt;背景&lt;/h4&gt;机器视觉模型特别是深度神经网络越来越多应用于生理信号解释如心电图，但通常需要大量训练数据且对预测背后的因果特征理解有限，这限制了其临床可靠性与人类推理的一致性。&lt;h4&gt;目的&lt;/h4&gt;探索感知伪彩色技术如何提升深度神经网络分析复杂数据时的可解释性和少样本学习能力，该技术先前已被证明能增强人类对心电图的理解。&lt;h4&gt;方法&lt;/h4&gt;聚焦于获得性药物诱导的长QT综合征作为案例研究，将临床相关时间特征编码为结构化颜色表示，使用原型网络和ResNet-18架构，在从单个心脏周期和完整10秒节律导出的ECG图像上评估单样本和少样本学习。&lt;h4&gt;主要发现&lt;/h4&gt;伪彩色编码使模型能从仅一个或五个训练样本中学习判别性和可解释特征；引导注意力转向有临床意义的ECG特征同时抑制无关信号分量；聚合多个心脏周期可提高性能，类似于人类对心跳的感知平均。&lt;h4&gt;结论&lt;/h4&gt;人类感知编码可以在医学机器智能中弥合数据效率、可解释性和因果推理之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;机器视觉模型，特别是深度神经网络，越来越多地应用于生理信号解释，包括心电图，但它们通常需要大量训练数据，且对其预测背后的因果特征提供有限见解。这种数据效率和可解释性的缺乏限制了它们的临床可靠性与人类推理的一致性。在此，我们展示了一种感知伪彩色技术，先前已被证明能增强人类对心电图的理解，可以提高深度神经网络分析复杂数据时的可解释性和少样本学习能力。我们聚焦于获得性药物诱导的长QT综合征作为具有挑战性的案例研究，其特征是信号形态异质性、心率可变性和与尖端扭转型室性心动过速等危及生命的心律失常相关的稀少阳性病例。此环境为模型在极端数据稀缺下的泛化能力提供了严格测试。通过将临床相关的时间特征（如QT间期持续时间）编码为结构化颜色表示，模型能够从仅一个或五个训练样本中学习判别性和可解释特征。使用原型网络和ResNet-18架构，我们在从单个心脏周期和完整10秒节律导出的ECG图像上评估单样本和少样本学习。可解释性分析表明，伪彩色引导注意力转向有临床意义的ECG特征，同时抑制无关信号分量。聚合多个心脏周期可进一步提高性能，类似于人类对心跳的感知平均。总之，这些发现表明，类人感知编码可以在医学机器智能中弥合数据效率、可解释性和因果推理之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine vision models, particularly deep neural networks, are increasingly applied to physiological signal interpretation, including electrocardiography (ECG), yet they typically require large training datasets and offer limited insight into the causal features underlying their predictions. This lack of data efficiency and interpretability constrains their clinical reliability and alignment with human reasoning. Here, we show that a perception-informed pseudo-colouring technique, previously demonstrated to enhance human ECG interpretation, can improve both explainability and few-shot learning in deep neural networks analysing complex physiological data.  We focus on acquired, drug-induced long QT syndrome (LQTS) as a challenging case study characterised by heterogeneous signal morphology, variable heart rate, and scarce positive cases associated with life-threatening arrhythmias such as torsades de pointes. This setting provides a stringent test of model generalisation under extreme data scarcity. By encoding clinically salient temporal features, such as QT-interval duration, into structured colour representations, models learn discriminative and interpretable features from as few as one or five training examples. Using prototypical networks and a ResNet-18 architecture, we evaluate one-shot and few-shot learning on ECG images derived from single cardiac cycles and full 10-second rhythms. Explainability analyses show that pseudo-colouring guides attention toward clinically meaningful ECG features while suppressing irrelevant signal components. Aggregating multiple cardiac cycles further improves performance, mirroring human perceptual averaging across heartbeats. Together, these findings demonstrate that human-like perceptual encoding can bridge data efficiency, explainability, and causal reasoning in medical machine intelligence.</description>
      <author>example@mail.com (Alaa Alahmadi, Mohamed Hasan)</author>
      <guid isPermaLink="false">2512.22349v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced geometry prediction in laser directed energy deposition using meta-learning</title>
      <link>http://arxiv.org/abs/2512.22241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于元学习的跨数据集知识转移模型，用于解决激光定向能量沉积中珠体几何形状预测面临的实验数据稀缺和异质性问题。&lt;h4&gt;背景&lt;/h4&gt;在激光定向能量沉积中，精确预测珠体几何形状受到实验数据集稀缺和异质性的阻碍，这些数据集是在不同材料、机器配置和工艺参数下收集的。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用有限数据的模型，用于预测L-DED中的沉积轨道几何形状，实现跨数据集的知识转移。&lt;h4&gt;方法&lt;/h4&gt;研究两种基于梯度的元学习算法（MAML和Reptile），使模型能够使用有限数据快速适应新的沉积条件。该框架使用从同行评议文献和内部实验汇编的多个实验数据集进行，并在粉末送丝、送丝和混合丝粉L-DED工艺中进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;MAML和Reptile都能仅使用三到九个训练样本在未见过的目标任务上实现精确的珠体高度预测，并且在可比数据约束下优于传统的馈送前向神经网络。元学习模型在代表不同打印条件的多个目标任务上实现了强大的泛化性能，R平方值高达约零点九，平均绝对误差在零点零三到零点零八毫米之间。&lt;h4&gt;结论&lt;/h4&gt;元学习方法能够有效地在异质L-DED设置中实现知识转移，解决了实验数据稀缺和异质性问题。&lt;h4&gt;翻译&lt;/h4&gt;在激光定向能量沉积中精确预测珠体几何形状通常受到实验数据集稀缺和异质性的阻碍，这些数据集是在不同材料、机器配置和工艺参数下收集的。为解决这一挑战，提出了一种基于元学习的跨数据集知识转移模型，用于预测L-DED中的沉积轨道几何形状。具体而言，研究了两种基于梯度的元学习算法，即模型无关元学习和Reptile，使模型能够使用有限数据快速适应新的沉积条件。该框架使用从同行评议文献和内部实验汇编的多个实验数据集进行，并在粉末送丝、送丝和混合丝粉L-DED工艺中进行评估。结果表明，MAML和Reptile都能仅使用三到九个训练样本在未见过的目标任务上实现精确的珠体高度预测，并且在可比数据约束下始终优于传统的馈送前向神经网络。在代表不同打印条件的多个目标任务上，元学习模型实现了强大的泛化性能，R平方值高达约零点九，平均绝对误差在零点零三到零点零八毫米之间，证明了在异质L-DED设置中有效的知识转移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate bead geometry prediction in laser-directed energy deposition (L-DED) is often hindered by the scarcity and heterogeneity of experimental datasets collected under different materials, machine configurations, and process parameters. To address this challenge, a cross-dataset knowledge transfer model based on meta-learning for predicting deposited track geometry in L-DED is proposed. Specifically, two gradient-based meta-learning algorithms, i.e., Model-Agnostic Meta-Learning (MAML) and Reptile, are investigated to enable rapid adaptation to new deposition conditions with limited data. The proposed framework is performed using multiple experimental datasets compiled from peer-reviewed literature and in-house experiments and evaluated across powder-fed, wire-fed, and hybrid wire-powder L-DED processes. Results show that both MAML and Reptile achieve accurate bead height predictions on unseen target tasks using as few as three to nine training examples, consistently outperforming conventional feedforward neural networks trained under comparable data constraints. Across multiple target tasks representing different printing conditions, the meta-learning models achieve strong generalization performance, with R-squared values reaching up to approximately 0.9 and mean absolute errors between 0.03-0.08 mm, demonstrating effective knowledge transfer across heterogeneous L-DED settings.</description>
      <author>example@mail.com (Abdul Malik Al Mardhouf Al Saadi, Amrita Basak)</author>
      <guid isPermaLink="false">2512.22241v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy</title>
      <link>http://arxiv.org/abs/2512.22197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新型多模态可解释性模型，利用视觉语言模型和少样本学习来模拟眼科医生的推理过程，通过分析视网膜象限内的病变分布，生成成对的Grad-CAM热图，展示OCT和眼底图像中单个神经元的权重，从而改善糖尿病视网膜病变的诊断。&lt;h4&gt;背景&lt;/h4&gt;糖尿病视网膜病变是全球视力丧失的主要原因，需要早期检测来保护视力。然而，医疗资源有限常常导致DR未被诊断。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够以自然语言识别个体DR病变的定量检测系统，解决当前DR诊断中的关键限制。&lt;h4&gt;方法&lt;/h4&gt;提出一种使用视觉语言模型和少样本学习的多模态可解释性模型，通过分析视网膜象限内的病变分布来模拟眼科医生的推理过程，并生成成对的Grad-CAM热图。&lt;h4&gt;主要发现&lt;/h4&gt;该创新方法利用3000张眼底图像和1000张OCT图像的数据集，解决了当前DR诊断中的关键限制，提供了改善患者结果的实用工具。&lt;h4&gt;结论&lt;/h4&gt;该模型能够模拟眼科医生的推理过程，通过可视化突出显示对DR严重程度分类有贡献的区域，是一种改善患者结果的实用且全面的工具。&lt;h4&gt;翻译&lt;/h4&gt;糖尿病视网膜病变是全球视力丧失的主要原因，需要早期检测来保护视力。医疗资源有限常常导致DR未被诊断。为此，AI模型利用病变分割来提高可解释性；然而，手动注释病变对临床医生来说不切实际。医生需要的是能解释分类推理过程而不仅仅是突出病变位置的模型。此外，当前模型是一维的，依赖单一成像模态进行可解释性，效果有限。相比之下，能够以自然语言识别个体DR病变的定量检测系统将克服这些限制，在筛查、治疗和研究环境中实现多样化应用。为解决这一问题，本文提出了一种新型多模态可解释性模型，利用带有少样本学习的视觉语言模型，通过分析视网膜象限内的病变分布来模拟眼科医生的推理过程。该模型生成成对的Grad-CAM热图，展示OCT和眼底图像中单个神经元的权重，直观地突出显示对DR严重程度分类有贡献的区域。使用包含3000张眼底图像和1000张OCT图像的数据集，这种创新方法解决了当前DR诊断中的关键限制，为改善患者结果提供了实用且全面的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.</description>
      <author>example@mail.com (Shivum Telang)</author>
      <guid isPermaLink="false">2512.22197v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications</title>
      <link>http://arxiv.org/abs/2512.22187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种联合无人机和无人地面车辆的部署和轨迹规划框架，用于在灾害地区建立通信并保证服务质量。通过引入道路图模型和结合元学习的A3C算法，实现了最优的服务质量和通信效率。&lt;h4&gt;背景&lt;/h4&gt;联合部署无人机和无人地面车辆已被证明是在灾害影响地区建立通信的有效方法。然而，在使用尽可能少的无人机的同时确保良好的服务质量，需要对无人机和无人地面车辆进行最优定位和轨迹规划。&lt;h4&gt;目的&lt;/h4&gt;提出一种联合无人机-无人地面车辆定位和轨迹规划框架，以保证地面用户获得最优的服务质量。&lt;h4&gt;方法&lt;/h4&gt;引入道路图模型模拟UGVs的移动性，将速率优化问题重新表述为马尔可夫决策过程，并提出结合元学习的异步优势行动者评论家(A3C)算法以快速适应新环境和动态条件。&lt;h4&gt;主要发现&lt;/h4&gt;提出的Meta-A3C方法在性能上优于A3C和DDPG算法，提供13.1%更高的吞吐量，执行速度快49%，同时满足服务质量要求。&lt;h4&gt;结论&lt;/h4&gt;通过结合元学习的异步优势行动者评论家算法能够有效解决无人机和无人地面车辆的联合部署和轨迹规划问题，在保证服务质量的同时提高通信效率和执行速度。&lt;h4&gt;翻译&lt;/h4&gt;无人机和无人地面车辆的联合部署已被证明是在灾害影响地区建立通信的有效方法。然而，在使用尽可能少的无人机的同时确保良好的服务质量，也需要对无人机和无人地面车辆进行最优定位和轨迹规划。本文提出了一种基于无人机和无人地面车辆联合部署的定位和轨迹规划框架，以保证地面用户获得最优的服务质量。为了模拟无人地面车辆的移动性，我们引入了一个道路图，引导它们沿着有效的道路段移动并遵守道路网络约束。为了解决速率优化问题，我们将该问题重新表述为马尔可夫决策过程，并提出了一种新颖的结合元学习的异步优势行动者评论家算法，以快速适应新环境和动态条件。数值结果表明，我们提出的Meta-A3C方法优于A3C和DDPG，提供13.1%更高的吞吐量和49%更快的执行速度，同时满足服务质量要求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint deployment of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) has been shown to be an effective method to establish communications in areas affected by disasters. However, ensuring good Quality of Services (QoS) while using as few UAVs as possible also requires optimal positioning and trajectory planning for UAVs and UGVs. This paper proposes a joint UAV-UGV-based positioning and trajectory planning framework for UAVs and UGVs deployment that guarantees optimal QoS for ground users. To model the UGVs' mobility, we introduce a road graph, which directs their movement along valid road segments and adheres to the road network constraints. To solve the sum rate optimization problem, we reformulate the problem as a Markov Decision Process (MDP) and propose a novel asynchronous Advantage Actor Critic (A3C) incorporated with meta-learning for rapid adaptation to new environments and dynamic conditions. Numerical results demonstrate that our proposed Meta-A3C approach outperforms A3C and DDPG, delivering 13.1\% higher throughput and 49\% faster execution while meeting the QoS requirements.</description>
      <author>example@mail.com (Ndagijimana Cyprien, Mehdi Sookhak, Hosein Zarini, Chandra N Sekharan, Mohammed Atiquzzaman)</author>
      <guid isPermaLink="false">2512.22187v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering</title>
      <link>http://arxiv.org/abs/2512.23318v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PCR-ORB（点云细化ORB）的增强型ORB-SLAM3框架，通过集成基于深度学习的点云细化技术来减轻动态物体对视觉同步定位与地图构建系统的干扰。&lt;h4&gt;背景&lt;/h4&gt;视觉同步定位与地图构建(vSLAM)系统在动态环境中面临重大挑战，因为移动物体会严重影响跟踪精度和地图一致性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理动态环境干扰的vSLAM系统，提高跟踪精度和地图一致性。&lt;h4&gt;方法&lt;/h4&gt;使用YOLOv8进行语义分割，结合CUDA加速处理实现实时性能，并实施多阶段过滤策略，包括地面平面估计、天空区域去除、边缘过滤和时间一致性验证。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI数据集上的评估显示，序列04取得了显著改进：ATE RMSE提高25.9%，ATE中位数提高30.4%。然而，不同序列上的表现参差不齐，表明效果依赖于具体场景。&lt;h4&gt;结论&lt;/h4&gt;该实现为动态物体过滤的挑战提供了见解，并为在复杂环境中实现稳健导航创造了机会。&lt;h4&gt;翻译&lt;/h4&gt;视觉同步定位与地图构建(vSLAM)系统在动态环境中遇到重大挑战，因为移动物体会损害跟踪精度和地图一致性。本文介绍了PCR-ORB（点云细化ORB），这是一种增强型ORB-SLAM3框架，集成了基于深度学习的点云细化技术，以减轻动态物体干扰。我们的方法采用YOLOv8进行语义分割，并结合CUDA加速处理以实现实时性能。该系统实现了多阶段过滤策略，包括地面平面估计、天空区域去除、边缘过滤和时间一致性验证。在KITTI数据集（序列00-09）上的全面评估展示了不同环境条件和场景类型下的性能特征。在特定序列中观察到显著改进，序列04的ATE RMSE提高了25.9%，ATE中位数提高了30.4%。然而，结果显示不同序列上的表现参差不齐，表明效果依赖于具体场景。该实现为动态物体过滤的挑战提供了见解，并为在复杂环境中实现稳健导航创造了机会。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉SLAM系统在动态环境中面临的挑战，即移动物体会降低跟踪精度和地图一致性问题。这个问题在现实中非常重要，因为自动驾驶车辆、机器人和无人机等自主系统需要在包含车辆、行人等动态元素的真实世界中运行。传统SLAM算法假设环境是静态的，在动态环境中会导致定位错误和地图不一致，影响系统的可靠性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统SLAM系统在动态环境中的局限性，特别是ORB-SLAM3虽然性能出色但在动态环境中仍易受干扰。作者注意到传统处理动态物体的方法缺乏语义理解能力，而深度学习的发展为解决这个问题提供了新机会。设计上，作者选择了YOLOv8进行语义分割，并构建了多阶段过滤策略结合语义信息、几何约束、时间一致性和运动模式分析。作者借鉴了现有工作，包括基于ORB-SLAM3框架、使用YOLOv8语义分割、RANSAC地面平面估计、Lucas-Kanade光流法运动检测以及CUDA加速技术等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过深度学习增强的语义理解来识别和过滤点云中的动态物体，同时保留静态环境特征，提高ORB-SLAM3在动态环境中的定位和建图精度。整体实现流程包括：1) 基于ORB-SLAM3的三线程架构增加并行点云过滤线程；2) 使用YOLOv8进行语义分割并通过CUDA加速实现实时性能；3) 实施多阶段过滤策略，包括语义评分分类、地面平面估计过滤、时间一致性和运动检测、边缘和天空区域过滤；4) 通过CUDA加速实现高效处理；5) 与ORB-SLAM3集成，修改跟踪循环和关键帧管理；6) 实现实时性能优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 集成YOLOv8语义分割到ORB-SLAM3框架中实现实时动态物体过滤；2) 开发多阶段点云细化策略结合语义、几何、时间和空间信息；3) 实现CUDA加速处理管道保持实时性能；4) 构建综合评估框架验证系统性能。相比之前的工作，PCR-ORB不仅使用简单的物体检测，而是实现了更全面的多阶段过滤；不仅考虑语义信息，还结合了几何约束和时间一致性；通过CUDA加速保持了实时性能；将深度学习集成到核心SLAM管道中而非作为后处理步骤；采用了自适应阈值机制根据环境调整过滤参数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PCR-ORB通过集成YOLOv8语义分割和多阶段过滤策略，显著提升了ORB-SLAM3在动态环境中的定位精度，同时通过CUDA加速保持实时性能，为自主导航系统提供了一种鲁棒的动态环境感知解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Simultaneous Localization and Mapping (vSLAM) systems encounter substantial challenges in dynamic environments where moving objects compromise tracking accuracy and map consistency. This paper introduces PCR-ORB (Point Cloud Refinement ORB), an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to mitigate dynamic object interference. Our approach employs YOLOv8 for semantic segmentation combined with CUDA-accelerated processing to achieve real-time performance. The system implements a multi-stage filtering strategy encompassing ground plane estimation, sky region removal, edge filtering, and temporal consistency validation. Comprehensive evaluation on the KITTI dataset (sequences 00-09) demonstrates performance characteristics across different environmental conditions and scene types. Notable improvements are observed in specific sequences, with sequence 04 achieving 25.9% improvement in ATE RMSE and 30.4% improvement in ATE median. However, results show mixed performance across sequences, indicating scenario-dependent effectiveness. The implementation provides insights into dynamic object filtering challenges and opportunities for robust navigation in complex environments.</description>
      <author>example@mail.com (Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin)</author>
      <guid isPermaLink="false">2512.23318v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation</title>
      <link>http://arxiv.org/abs/2512.23180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于3D高斯场景表示的新型统一驾驶世界模型框架，实现了3D场景理解和多模态场景生成能力。&lt;h4&gt;背景&lt;/h4&gt;现有的驾驶世界模型缺乏3D场景理解能力，只能在输入数据条件下生成内容，无法解释或推理驾驶环境。当前方法使用点云或BEV特征表示3D空间信息，不能将文本信息与底层3D场景准确对齐。&lt;h4&gt;目的&lt;/h4&gt;解决现有驾驶世界模型的局限性，实现3D场景理解和多模态场景生成，并实现文本信息与3D场景的直接对齐。&lt;h4&gt;方法&lt;/h4&gt;基于3D高斯场景表示提出统一框架，通过将语言特征嵌入到每个高斯基元中实现早期模态对齐，设计任务感知语言引导采样策略去除冗余3D高斯并注入紧凑3D令牌到LLM，以及设计双条件多模态生成模型结合高级语言条件和低级图像条件。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和NuInteract数据集上的综合研究表明，所提出的方法达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;基于3D高斯场景表示的统一驾驶世界框架有效解决了现有模型的局限性，实现了3D场景理解和多模态场景生成。&lt;h4&gt;翻译&lt;/h4&gt;驾驶世界模型随着生成模型的发展而迅速发展。然而，现有的驾驶世界模型缺乏3D场景理解能力，只能在输入数据条件下生成内容，无法解释或推理驾驶环境。此外，当前方法使用点云或BEV特征表示3D空间信息，不能将文本信息与底层3D场景准确对齐。为解决这些局限性，我们提出了一种基于3D高斯场景表示的新型统一驾驶世界模型框架，该框架同时支持3D场景理解和多模态场景生成，并支持理解和生成任务的上下文丰富化。我们的方法通过将丰富的语言特征嵌入到每个高斯基元中，直接将文本信息与3D场景对齐，从而实现早期模态对齐。此外，我们设计了一种新颖的任务感知语言引导采样策略，去除冗余的3D高斯，并将准确且紧凑的3D令牌注入到LLM中。我们还设计了一种双条件多模态生成模型，其中我们视觉语言模型捕获的信息被用作高级语言条件，与低级图像条件结合，共同指导多模态生成过程。我们在nuScenes和NuInteract数据集上进行了综合研究，以验证我们框架的有效性。我们的方法达到了最先进的性能。我们将在GitHub上公开代码：https://github.com/dtc111111/GaussianDWM。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.</description>
      <author>example@mail.com (Tianchen Deng, Xuefeng Chen, Yi Chen, Qu Chen, Yuyao Xu, Lijin Yang, Le Xu, Yu Zhang, Bo Zhang, Wuxiong Huang, Hesheng Wang)</author>
      <guid isPermaLink="false">2512.23180v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Differentiable Physics-Driven Human Representation for Millimeter-Wave Based Pose Estimation</title>
      <link>http://arxiv.org/abs/2512.23054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的可微分物理驱动人体表示(DIPR)方法，用于解决毫米波人体姿态估计中热图和点云两种输入范式的局限性。DIPR将人体表示为高斯分布集合，通过整合运动学先验和毫米波传播物理来增强人体特征并抑制噪声，实验证明该方法能有效提升现有毫米波人体姿态估计方法的性能。&lt;h4&gt;背景&lt;/h4&gt;毫米波在人体姿态估计方面具有非侵入式传感的优势，但当前基于毫米波的方法面临两种主要输入范式的局限性：热图容易受多径传播和硬件调制噪声影响；点云虽能抑制噪声但导致稀疏的人体相关特征。&lt;h4&gt;目的&lt;/h4&gt;研究提供一种替代输入范式(DIPR)的可行性，解决热图和点云两种输入范式的局限性，提高毫米波人体姿态估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;DIPR将人体表示为具有运动学和电磁参数的高斯分布集合。通过两种策略减轻噪声：1)整合先验运动知识，基于热图初始化DIPR并建立多面优化目标；2)模拟完整毫米波处理流程，从DIPR重新渲染新热图并与原始热图比较，避免过度拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上使用四种方法的实验结果表明，现有毫米波人体姿态估计方法可轻松集成DIPR并实现优越性能。&lt;h4&gt;结论&lt;/h4&gt;DIPR作为一种新的输入范式，通过整合物理模型和运动学先验，有效解决了传统热图和点云方法的局限性，提供了更鲁棒和准确的人体姿态估计。&lt;h4&gt;翻译&lt;/h4&gt;虽然毫米波在人体姿态估计方面通过其非侵入式传感能力具有优势，但当前基于毫米波的人体姿态估计方法在两种主要输入范式上面临局限性：热图和点云。热图表示从毫米波推导出的密集多维特征，但显著受到多径传播和硬件调制噪声的影响。点云是通过将恒虚警率算法应用于热图获得的一组3D点，它能抑制噪声但导致稀疏的人体相关特征。为解决这些局限性，我们研究了提供替代输入范式的可行性：可微分物理驱动人体表示(DIPR)，它将人体表示为具有运动学和电磁参数的高斯分布集合。受高斯飞溅启发，DIPR利用人体运动先验和毫米波传播物理来增强人体特征，并通过两种策略减轻非人体噪声：1)我们整合先验运动知识，基于热图初始化DIPR，建立多面优化目标，确保生物力学有效性并增强运动特征；2)我们模拟完整的毫米波处理流程，从DIPR重新渲染新的热图，并与原始热图比较，避免因运动学约束过度拟合而产生虚假噪声。在三个数据集上使用四种方法的实验结果表明，现有的基于毫米波的人体姿态估计方法可以轻松集成DIPR并实现优越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于毫米波雷达的人体姿态估计中的'信息-噪声权衡'问题。当前方法使用的热图(Heatmap)输入范式虽然包含密集信息但受噪声干扰严重，而点云(Point Cloud)范式虽然噪声较少但特征稀疏。这个问题在现实中很重要，因为毫米波雷达具有隐私保护和环境光照鲁棒性等优势，解决这一问题可以提高人体姿态估计的准确性，促进毫米波雷达在智能家居、医疗监护、人机交互等领域的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者的设计思路借鉴了多个领域的工作：首先深入理解毫米波雷达处理流程和CFAR算法；然后受到计算机视觉中高斯飞溅(Gaussian Splatting)技术的启发，将其概念应用于毫米波信号；同时整合人体生物力学约束确保生成的人体表示物理合理。作者通过识别现有方法的局限性，思考如何结合人体运动学先验和毫米波传播物理来增强人体特征，最终设计了DIPR作为替代表示方法，并通过M-GS流程实现。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将人体表示为具有运动学和电磁参数的高斯分布集合(DIPR)，通过结合人体生物力学原理和毫米波传播物理来增强人体特征，同时抑制非人类噪声。整体实现流程(M-GS)包括：1)位置和速度提取：从热图中提取粗略位置和速度状态；2)关节关联和表示：将人体建模为高斯原语集合，每个关节由位置、缩放、旋转、速度等六个参数描述；3)重新渲染热图：开发可微分管道将DIPR转换回热图，包含信号调制、多普勒调制等四个关键模块；4)优化：使用重建约束和运动学约束进行参数优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出DIPR作为新输入范式，首次将高斯飞溅技术应用于毫米波人体姿态估计；2)设计毫米波特定的高斯参数化，引入速度和多普勒特征等毫米波特定参数；3)结合生物力学约束与信号一致性，确保生成的人体表示物理合理且避免过拟合。相比之前的工作，DIPR通过结构化运动学约束减轻了热图的高噪声干扰，同时解决了点云的稀疏性问题，不依赖预训练模块减少了计算开销，直接基于物理原理提供了理论保证。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了DIPR，一种可微分物理驱动的人体表示方法，通过结合人体生物力学原理和毫米波传播物理，有效解决了毫米波人体姿态估计中的信息-噪声权衡问题，显著提升了现有方法的性能并开辟了新的研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While millimeter-wave (mmWave) presents advantages for Human Pose Estimation (HPE) through its non-intrusive sensing capabilities, current mmWave-based HPE methods face limitations in two predominant input paradigms: Heatmap and Point Cloud (PC). Heatmap represents dense multi-dimensional features derived from mmWave, but is significantly affected by multipath propagation and hardware modulation noise. PC, a set of 3D points, is obtained by applying the Constant False Alarm Rate algorithm to the Heatmap, which suppresses noise but results in sparse human-related features. To address these limitations, we study the feasibility of providing an alternative input paradigm: Differentiable Physics-driven Human Representation (DIPR), which represents humans as an ensemble of Gaussian distributions with kinematic and electromagnetic parameters. Inspired by Gaussian Splatting, DIPR leverages human kinematic priors and mmWave propagation physics to enhance human features while mitigating non-human noise through two strategies: 1) We incorporate prior kinematic knowledge to initialize DIPR based on the Heatmap and establish multi-faceted optimization objectives, ensuring biomechanical validity and enhancing motion features. 2) We simulate complete mmWave processing pipelines, re-render a new Heatmap from DIPR, and compare it with the original Heatmap, avoiding spurious noise generation due to kinematic constraints overfitting. Experimental results on three datasets with four methods demonstrate that existing mmWave-based HPE methods can easily integrate DIPR and achieve superior performance.</description>
      <author>example@mail.com (Shuntian Zheng, Guangming Wang, Jiaqi Li, Minzhe Ni, Yu Guan)</author>
      <guid isPermaLink="false">2512.23054v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2512.22463v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the IEEE/CVF Winter Conference on Applications of Computer Vision 2026 (WACV 2026)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MEGA-PCC是一种完全端到端的、基于学习的点云几何和属性联合压缩框架，通过消除后着色程序和手动比特率调整，实现了数据驱动的比特率分配和简化的整体流程。&lt;h4&gt;背景&lt;/h4&gt;点云几何和属性的联合压缩对高效3D数据表示至关重要，但现有方法依赖后着色程序和推理过程中手动调整的比特率分配，阻碍了端到端优化并增加了系统复杂性。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一个完全端到端的、基于学习的联合压缩框架，实现更优的率失真性能和运行时效率。&lt;h4&gt;方法&lt;/h4&gt;提出MEGA-PCC框架，包含两个专门模型：1)主要压缩模型使用共享编码器将几何和属性信息编码为统一潜在表示，双解码器顺序重建几何和属性；2)基于Mamba的熵模型(MEM)通过捕获空间和通道相关性增强熵编码；两个模型都构建在Mamba架构上，有效建模长距离依赖和丰富上下文特征。&lt;h4&gt;主要发现&lt;/h4&gt;MEGA-PCC通过消除着色需求和启发式比特率调整，能够在训练过程中实现数据驱动的比特率分配，简化整体流程，同时实现更优的率失真性能和运行时效率。&lt;h4&gt;结论&lt;/h4&gt;大量实验表明，MEGA-PCC相比传统和基于学习的方法实现了更优的率失真性能和运行时效率，为AI驱动的点云压缩提供了强大解决方案。&lt;h4&gt;翻译&lt;/h4&gt;点云几何和属性的联合压缩对于高效的3D数据表示至关重要。现有方法通常依赖后着色程序和推理过程中手动调整的比特率分配，这阻碍了端到端优化并增加了系统复杂性。为克服这些限制，我们提出了MEGA-PCC，一种完全端到端的、基于学习的联合压缩框架，包含两个专门模型。主要压缩模型采用共享编码器将几何和属性信息编码为统一潜在表示，然后通过双解码器顺序重建几何和属性。补充这一点，基于Mamba的熵模型(MEM)通过捕获空间和通道相关性增强熵编码，改善概率估计。两个模型都构建在Mamba架构上，有效建模长距离依赖和丰富的上下文特征。通过消除着色需求和启发式比特率调整，MEGA-PCC能够在训练过程中实现数据驱动的比特率分配并简化整体流程。大量实验表明，MEGA-PCC相比传统和基于学习的方法实现了更优的率失真性能和运行时效率，为AI驱动的点云压缩提供了强大解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云几何和属性联合压缩中的效率问题。现有方法需要手动调整几何和属性间的比特分配，并依赖重新着色过程连接压缩管道，这阻碍了端到端优化并增加系统复杂性。这个问题很重要，因为随着元宇宙、VR/AR、自动驾驶等沉浸式技术的发展，对高效3D数据表示的需求不断增长，而点云作为捕捉详细几何结构的主导媒介，其高效压缩对减少存储空间、加快传输速度和提高处理效率至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别现有方法的局限性（如需要手动比特分配和重新着色过程）来思考解决方案。他们认识到几何和属性间的内在依赖关系，提出需要更统一的框架。设计方法借鉴了Mamba架构的状态空间模型用于高效建模长距离依赖，稀疏卷积用于局部结构建模，以及多方向SSM模块（前向、后向和通道翻转）来全面捕获空间上下文。还借鉴了Morton扫描技术来序列化3D数据，以及现有点云压缩方法如PCGCv2和ANF-PCAC的基本思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用统一的编码器同时编码几何和属性信息到共享的潜在表示，采用双解码器架构先重建几何再重建属性，利用Mamba架构建模长距离依赖，并通过基于Mamba的熵模型增强熵编码。整体流程：1)将点云体素化为三通道体积输入；2)统一编码器结合稀疏卷积和多方向SSM生成潜在表示；3)几何骨架无损编码，特征张量量化和熵编码；4)几何解码器先重建点坐标；5)重建的几何坐标指导属性解码器重建属性；6)采用两阶段训练策略优化几何和属性重建质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)单编码器-双解码器架构，避免重新着色步骤；2)三向Mamba(Tri-Mamba)全面捕获空间和通道信息；3)基于Mamba的熵模型(MEM)提高概率估计准确性；4)端到端比特分配，无需耗时的模型匹配；5)高效设计实现线性时间复杂度。相比之前工作，MEGA-PCC消除了重新着色步骤和手动比特分配需求，使用统一的潜在表示而非分开处理，采用更高效的Mamba架构替代Transformer，实现了真正的端到端学习。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MEGA-PCC提出了一种基于Mamba的高效点云压缩框架，通过统一的单编码器-双解码器架构和端到端训练，实现了几何和属性信息的联合优化压缩，显著提升了压缩效率和重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint compression of point cloud geometry and attributes is essential for efficient 3D data representation. Existing methods often rely on post-hoc recoloring procedures and manually tuned bitrate allocation between geometry and attribute bitstreams in inference, which hinders end-to-end optimization and increases system complexity. To overcome these limitations, we propose MEGA-PCC, a fully end-to-end, learning-based framework featuring two specialized models for joint compression. The main compression model employs a shared encoder that encodes both geometry and attribute information into a unified latent representation, followed by dual decoders that sequentially reconstruct geometry and then attributes. Complementing this, the Mamba-based Entropy Model (MEM) enhances entropy coding by capturing spatial and channel-wise correlations to improve probability estimation. Both models are built on the Mamba architecture to effectively model long-range dependencies and rich contextual features. By eliminating the need for recoloring and heuristic bitrate tuning, MEGA-PCC enables data-driven bitrate allocation during training and simplifies the overall pipeline. Extensive experiments demonstrate that MEGA-PCC achieves superior rate-distortion performance and runtime efficiency compared to both traditional and learning-based baselines, offering a powerful solution for AI-driven point cloud compression.</description>
      <author>example@mail.com (Kai-Hsiang Hsieh, Monyneath Yim, Wen-Hsiao Peng, Jui-Chiu Chiang)</author>
      <guid isPermaLink="false">2512.22463v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems</title>
      <link>http://arxiv.org/abs/2512.22439v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了SuperiorGAT，一个基于图注意力的框架，用于重建稀疏激光雷达点云中缺失的高程信息，能够在不增加网络深度的情况下实现准确重建。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统中的激光雷达感知受到固定垂直光束分辨率的限制，并且由于环境遮挡导致的光束脱落进一步降低了性能。&lt;h4&gt;目的&lt;/h4&gt;设计一个框架来重建稀疏激光雷达点云中缺失的高程信息，以提高激光雷达感知的准确性和完整性。&lt;h4&gt;方法&lt;/h4&gt;SuperiorGAT通过将激光雷达扫描建模为感知光束的图，并采用门控残差融合与前馈细化技术，实现准确重建。通过模拟结构化光束脱落（移除每第四个垂直扫描光束）来评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的KITTI环境（包括Person、Road、Campus和City序列）进行的广泛实验表明，SuperiorGAT始终比基于PointNet的模型和更深的GAT基线实现更低的重建误差和改进的几何一致性。定性的X-Z投影进一步证实了该模型在保持结构完整性的同时具有最小垂直失真的能力。&lt;h4&gt;结论&lt;/h4&gt;架构改进提供了一种计算效率高的方法来提高激光雷达分辨率，而无需额外的传感器硬件。&lt;h4&gt;翻译&lt;/h4&gt;基于激光雷达的自动驾驶系统感知受到固定垂直光束分辨率的限制，并且由于环境遮挡导致的光束脱落进一步降低了性能。本文介绍了SuperiorGAT，一个基于图注意力的框架，用于重建稀疏激光雷达点云中缺失的高程信息。通过将激光雷达扫描建模为感知光束的图，并结合门控残差融合和前馈细化，SuperiorGAT能够在不增加网络深度的情况下实现准确重建。为了评估性能，通过移除每第四个垂直扫描光束来模拟结构化光束脱落。在多样化的KITTI环境（包括Person、Road、Campus和City序列）进行的广泛实验表明，SuperiorGAT始终比基于PointNet的模型和更深的GAT基线实现更低的重建误差和改进的几何一致性。定性的X-Z投影进一步证实了该模型在保持结构完整性的同时具有最小垂直失真的能力。这些结果表明，架构改进提供了一种计算效率高的方法来提高激光雷达分辨率，而无需额外的传感器硬件。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云在自动驾驶系统中的稀疏性问题，特别是由光束丢失(beam dropout)导致的垂直分辨率下降问题。这个问题很重要，因为LiDAR是自动驾驶系统的核心传感器，环境遮挡或硬件故障会导致数据不完整，影响物体检测、定位和路径规划等关键任务，而现有方法在精度和计算效率之间难以取得平衡。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：压缩感知方法计算量大不适合实时应用；CNN需要密集3D投影对稀疏数据效率低；点云模型难以处理结构化稀疏模式；标准图神经网络无法有效优先处理关键局部几何特征。作者借鉴了自己之前的会议论文工作，发现多层GAT可有效重建LiDAR高度值，但增加深度会带来计算成本和稳定性问题，因此转向架构优化而非深度提升，设计了结合光束感知图、门控残差融合和前馈细化的方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将LiDAR扫描建模为光束感知图，使用图注意力网络动态优化点与点之间的交互，通过门控残差融合和前馈细化提高重建质量，专注于重建缺失的垂直高度信息。整体流程包括：1)数据表示：将稀疏点云表示为图结构，节点为点，边表示连接关系；2)架构流程：光束感知特征编码→多头注意力聚合→门控残差融合→前馈细化→任务特定输出解码；3)训练评估：在KITTI数据集上模拟光束丢失，使用RMSE和Chamfer距离评估重建质量，测量推理时间评估效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全面的光束丢失模拟框架，准确模拟真实硬件故障；2)新的图注意力重建架构，通过学习注意力权重优化点对点交互；3)计算高效框架，专为实时部署设计。相比之前工作的不同：与传统插值方法相比能保留复杂几何特征避免'阶梯'伪影；与压缩感知相比不需要迭代求解器适合实时应用；与CNN相比避免密集3D投影更适应稀疏数据；与标准图神经网络相比使用注意力机制而非固定权重；与作者之前工作相比不再增加深度而是优化架构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SuperiorGAT通过创新的图注意力网络架构，实现了高效准确的稀疏LiDAR点云垂直信息重建，解决了自动驾驶系统中光束丢失问题，在保持计算效率的同时显著提高了重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model's ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.</description>
      <author>example@mail.com (Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad)</author>
      <guid isPermaLink="false">2512.22439v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation</title>
      <link>http://arxiv.org/abs/2512.22304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 11th Annual Conference on Vision and Intelligent Systems (CVIS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PortionNet的跨模态知识蒸馏框架，能够在不使用深度传感器的情况下，仅通过RGB图像准确估计食物营养成分。&lt;h4&gt;背景&lt;/h4&gt;从单张图像准确估计食物营养成分具有挑战性，因为会丢失3D信息。基于深度的方法虽然能提供可靠的几何信息，但由于需要深度传感器，在大多数智能手机上无法使用。&lt;h4&gt;目的&lt;/h4&gt;开发一种不需要深度传感器就能从RGB图像估计食物营养成分的方法。&lt;h4&gt;方法&lt;/h4&gt;提出PortionNet，一种新的跨模态知识蒸馏框架。在训练过程中从点云学习几何特征，而在推理时只需要RGB图像。采用双模式训练策略，其中轻量级适配器网络模仿点云表示，无需任何专用硬件要求即可实现伪3D推理。&lt;h4&gt;主要发现&lt;/h4&gt;PortionNet在MetaFood3D数据集上达到了最先进的性能，在体积和能量估计方面都优于所有先前的方法。在SimpleFood45数据集上的跨数据集评估进一步证明了其在能量估计方面的强泛化能力。&lt;h4&gt;结论&lt;/h4&gt;PortionNet是一种有效的跨模态知识蒸馏框架，可以在不需要深度传感器的情况下准确估计食物营养成分。&lt;h4&gt;翻译&lt;/h4&gt;从单张图像准确估计食物营养成分具有挑战性，因为会丢失3D信息。虽然基于深度的方法能提供可靠的几何信息，但由于需要深度传感器，在大多数智能手机上仍然无法使用。为克服这一挑战，我们提出了PortionNet，一种新颖的跨模态知识蒸馏框架，在训练过程中从点云学习几何特征，而在推理时只需要RGB图像。我们的方法采用双模式训练策略，其中轻量级适配器网络模仿点云表示，无需任何专用硬件要求即可实现伪3D推理。PortionNet在MetaFood3D上取得了最先进的性能，在体积和能量估计方面都优于所有先前方法。在SimpleFood45上的跨数据集评估进一步证明了其在能量估计方面的强泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate food nutrition estimation from single images is challenging due to the loss of 3D information. While depth-based methods provide reliable geometry, they remain inaccessible on most smartphones because of depth-sensor requirements. To overcome this challenge, we propose PortionNet, a novel cross-modal knowledge distillation framework that learns geometric features from point clouds during training while requiring only RGB images at inference. Our approach employs a dual-mode training strategy where a lightweight adapter network mimics point cloud representations, enabling pseudo-3D reasoning without any specialized hardware requirements. PortionNet achieves state-of-the-art performance on MetaFood3D, outperforming all previous methods in both volume and energy estimation. Cross-dataset evaluation on SimpleFood45 further demonstrates strong generalization in energy estimation.</description>
      <author>example@mail.com (Darrin Bright, Rakshith Raj, Kanchan Keisham)</author>
      <guid isPermaLink="false">2512.22304v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</title>
      <link>http://arxiv.org/abs/2512.23649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了RoboMirror，这是一个无需重定位的视频到运动框架，实现了'先理解后模仿'的理念。它利用视觉语言模型将原始视频提炼为视觉运动意图，直接基于扩散策略生成物理上合理、语义对齐的运动。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的人形运动系统依赖于精心制作的运动捕捉轨迹或稀疏文本命令，在视觉理解和控制之间存在关键差距。文本到运动方法存在语义稀疏和流水线错误问题，而基于视频的方法仅执行机械姿态模仿，缺乏真正的视觉理解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够弥合视觉理解和动作之间差距的框架，使机器人能够通过视觉观察来学习运动，像人类一样先理解后模仿。&lt;h4&gt;方法&lt;/h4&gt;提出RoboMirror框架，利用视觉语言模型将原始的第一人称/第三人称视频提炼为视觉运动意图，这些意图直接条件化一个基于扩散的策略，以生成物理上合理、语义对齐的运动，无需明确的姿态重建或重定位。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验验证了RoboMirror的有效性，它能够通过第一人称视频实现远程呈现，将第三人称控制延迟大幅减少80%，并且比基线方法高3.7%的任务成功率。&lt;h4&gt;结论&lt;/h4&gt;通过围绕视频理解重新构建人形控制，RoboMirror成功弥合了视觉理解和动作之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;人类通过视觉观察学习运动，在模仿动作之前先解释视觉内容。然而，最先进的人形运动系统依赖于精心制作的运动捕捉轨迹或稀疏文本命令，在视觉理解和控制之间留下了关键差距。文本到运动方法遭受语义稀疏和流水线错误，而基于视频的方法仅执行机械姿态模仿，没有真正的视觉理解。我们提出了RoboMirror，这是第一个无需重定位的视频到运动框架，体现了'先理解后模仿'的理念。利用VLMs，它将原始的第一人称/第三人称视频提炼为视觉运动意图，这些意图直接条件化一个基于扩散的策略，以生成物理上合理、语义对齐的运动，无需明确的姿态重建或重定位。广泛的实验验证了RoboMirror的有效性，它能够通过第一人称视频实现远程呈现，大幅将第三人称控制延迟减少80%，并且比基线方法高3.7%的任务成功率。通过围绕视频理解重新构建人形控制，我们弥合了视觉理解和动作之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying "understand before you imitate". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.</description>
      <author>example@mail.com (Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang)</author>
      <guid isPermaLink="false">2512.23649v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</title>
      <link>http://arxiv.org/abs/2512.23646v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website:https://kd-tao.github.io/OmniAgent/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OmniAgent是一个完全由音频引导的主动感知代理，通过动态协调专业工具实现细粒度音频视觉推理，采用从粗到细的音频引导感知范式，在多个基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在统一音频和视觉模态方面取得了显著进展，但通常缺乏细粒度的跨模态理解，并且难以处理多模态对齐问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型在细粒度跨模态理解和多模态对齐方面的局限性，引入OmniAgent实现更细粒度的音频视觉推理。&lt;h4&gt;方法&lt;/h4&gt;OmniAgent采用动态规划来自主按需协调工具调用，战略性地将感知注意力集中在任务相关线索上，核心是一种从粗到细的音频引导感知范式，利用音频线索定位时间事件并指导后续推理。&lt;h4&gt;主要发现&lt;/h4&gt;在三个音频视频理解基准上的大量经验评估表明，OmniAgent取得了最先进的性能，以10% - 20%的准确率优势超越了领先的开源和专有模型。&lt;h4&gt;结论&lt;/h4&gt;论文展示了一种从被动响应生成到主动多模态查询的范式转变，通过动态协调专业工具解决了现有模型的不足。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在统一音频和视觉模态方面取得了显著进展；然而，它们通常缺乏细粒度的跨模态理解，并且难以处理多模态对齐。为了解决这些限制，我们引入了OmniAgent，一个完全由音频引导的主动感知代理，动态协调专业工具以实现更细粒度的音频视觉推理。与依赖刚性静态流程和密集帧字幕的先前工作不同，本文展示了从被动响应生成到主动多模态查询的范式转变。OmniAgent采用动态规划自主按需协调工具调用，战略性地将感知注意力集中在任务相关的线索上。我们方法的核心是一种新颖的从粗到细的音频引导感知范式，利用音频线索定位时间事件并指导后续推理。在三个音频视频理解基准上的大量经验评估表明，OmniAgent取得了最先进的性能，以10% - 20%的准确率优势大幅超越了领先的开源和专有模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.</description>
      <author>example@mail.com (Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang)</author>
      <guid isPermaLink="false">2512.23646v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</title>
      <link>http://arxiv.org/abs/2512.23635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为HAT的时空对齐模块，用于解决自动驾驶端到端感知中的时空对齐问题。该模块允许每个物体从多个假设中自适应解码最佳对齐方案，无需直接监督，结合了显式运动模型和语义特征，显著提高了感知精度和跟踪性能。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶的端到端感知中，时空对齐对时间建模至关重要，能提供有价值的结构和纹理先验信息。现有方法通常依赖注意力机制跨帧对齐物体，使用统一的显式物理模型简化运动模型，倾向于使用语义特征进行隐式对齐，但这种方法在不同类别和帧中物体运动状态和特征变化时表现次优。&lt;h4&gt;目的&lt;/h4&gt;开发一种时空对齐模块，使每个物体能够从多个假设中自适应解码最佳对齐提议，无需直接监督，解决现有方法在不同条件下对齐效果不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;HAT模块首先使用多个显式运动模型为历史实例生成空间锚点和运动感知特征提议，然后通过整合缓存的物体查询中嵌入的语义和运动线索进行多假设解码，最终为目标帧提供最佳对齐提议。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上，HAT在不同基线上持续改进3D时间检测器和跟踪器；与DETR3D检测器配对时，在测试集上达到46.0%的AMOTA最先进跟踪结果；在以物体为中心的端到端自动驾驶方法中，HAT将感知精度提高+1.3% mAP和+3.1% AMOTA，并将碰撞率降低32%；当语义信息被破坏时，HAT仍能保持感知和规划的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;HAT通过结合显式运动模型和语义特征，有效解决了自动驾驶中时空对齐的挑战，在不同条件下都能提高感知和跟踪性能，特别是在语义信息受损的情况下仍能保持鲁棒性，为自动驾驶的端到端感知提供了更可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;时空对齐对于自动驾驶中端到端感知的时间建模至关重要，提供了有价值的结构和纹理先验信息。现有方法通常依赖注意力机制来跨帧对齐物体，使用统一的显式物理模型（如恒定速度）简化运动模型。这些方法倾向于使用语义特征进行隐式对齐，挑战了传统感知范式中显式运动建模的重要性。然而，不同类别和帧中物体的运动状态和特征变化导致这种对齐次优。为解决此问题，我们提出HAT，一种时空对齐模块，使每个物体能够从多个假设中自适应解码最佳对齐提议，无需直接监督。具体而言，HAT首先使用多个显式运动模型为历史实例生成空间锚点和运动感知特征提议。然后通过整合缓存的物体查询中嵌入的语义和运动线索进行多假设解码，最终为目标帧提供最佳对齐提议。在nuScenes上，HAT在不同基线上持续改进3D时间检测器和跟踪器。当与DETR3D检测器配对时，在测试集上达到46.0% AMOTA的最先进跟踪结果。在以物体为中心的端到端自动驾驶方法中，HAT提高了感知精度（+1.3% mAP，+3.1% AMOTA）并将碰撞率降低32%。当语义信息被破坏时（nuScenes-C），HAT通过增强运动建模使端到端自动驾驶的感知和规划更加鲁棒。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.</description>
      <author>example@mail.com (Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun)</author>
      <guid isPermaLink="false">2512.23635v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging</title>
      <link>http://arxiv.org/abs/2512.22976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对单通道EEG的上下文感知且可解释的睡眠分期框架，通过结合多尺度特征提取和时间建模，并采用类别加权损失和数据增强解决数据不平衡问题，在保持可解释性的同时显著提高了睡眠分期性能，特别是在N1阶段检测方面。&lt;h4&gt;背景&lt;/h4&gt;自动睡眠分期是医疗保健中的关键任务，因为全球睡眠障碍普遍存在。单通道脑电图(EEG)是一种实用且广泛可用的信号，用于自动睡眠分期。现有方法面临类别不平衡、感受野建模有限和可解释性不足等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种上下文感知且可解释的框架，用于单通道EEG睡眠分期，特别强调提高N1阶段的检测能力，解决先前模型作为黑盒操作的问题，提供明确且可解释的特征提取作用。&lt;h4&gt;方法&lt;/h4&gt;结合紧凑的多尺度特征提取与时间建模，以捕捉局部和长程依赖关系；使用类别加权损失函数和数据增强解决数据不平衡问题；将EEG信号分割为子时段块，通过跨块平均softmax概率获得最终预测，增强上下文表示和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架总体准确率达到89.72%，宏平均F1得分为85.46%；在具有挑战性的N1阶段达到61.7%的F1分数；在SleepEDF数据集上相比之前的方法有显著改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效提高了睡眠分期性能，同时保持了可解释性，适合实际临床应用。&lt;h4&gt;翻译&lt;/h4&gt;自动睡眠分期是医疗保健中的一个关键任务，因为全球睡眠障碍普遍存在。本研究专注于单通道脑电图(EEG)，这是一种实用且广泛可用的自动睡眠分期信号。现有方法面临类别不平衡、感受野建模有限和可解释性不足等挑战。这项工作提出了一个用于单通道EEG睡眠分期的上下文感知和可解释框架，特别强调提高N1阶段的检测。许多先前的模型作为具有堆叠层的黑盒运行，缺乏明确且可解释的特征提取作用。所提出的模型结合了紧凑的多尺度特征提取与时间建模，以捕捉局部和长程依赖关系。为了解决数据不平衡问题，特别是N1阶段，应用了类别加权损失函数和数据增强。EEG信号被分割为子时段块，通过跨块平均softmax概率获得最终预测，增强了上下文表示和鲁棒性。所提出的框架总体准确率达到89.72%，宏平均F1得分为85.46%。值得注意的是，它在具有挑战性的N1阶段达到了61.7%的F1分数，在SleepEDF数据集上相比先前方法有显著改进。这些结果表明，所提出的方法在保持可解释性和适合实际临床应用的同时，有效提高了睡眠分期性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic sleep staging is a critical task in healthcare due to the global prevalence of sleep disorders. This study focuses on single-channel electroencephalography (EEG), a practical and widely available signal for automatic sleep staging. Existing approaches face challenges such as class imbalance, limited receptive-field modeling, and insufficient interpretability. This work proposes a context-aware and interpretable framework for single-channel EEG sleep staging, with particular emphasis on improving detection of the N1 stage. Many prior models operate as black boxes with stacked layers, lacking clearly defined and interpretable feature extraction roles.The proposed model combines compact multi-scale feature extraction with temporal modeling to capture both local and long-range dependencies. To address data imbalance, especially in the N1 stage, classweighted loss functions and data augmentation are applied. EEG signals are segmented into sub-epoch chunks, and final predictions are obtained by averaging softmax probabilities across chunks, enhancing contextual representation and robustness.The proposed framework achieves an overall accuracy of 89.72% and a macro-average F1-score of 85.46%. Notably, it attains an F1- score of 61.7% for the challenging N1 stage, demonstrating a substantial improvement over previous methods on the SleepEDF datasets. These results indicate that the proposed approach effectively improves sleep staging performance while maintaining interpretability and suitability for real-world clinical applications.</description>
      <author>example@mail.com (Amirali Vakili, Salar Jahanshiri, Armin Salimi-Badr)</author>
      <guid isPermaLink="false">2512.22976v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Visual Language Hypothesis</title>
      <link>http://arxiv.org/abs/2512.23335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文从结构和拓扑角度研究视觉表征学习，提出视觉理解需要视觉语义语言，视觉观测空间应组织为纤维束结构，并推导出两个理论结果：语义商空间不能仅通过光滑变形获得，需要区分性目标；近似商空间对模型架构有结构要求，需要支持拓扑变化的表征机制。&lt;h4&gt;背景&lt;/h4&gt;视觉表征学习中的可迁移性和抽象性假设，以及视觉理解需要语义语言的观点。&lt;h4&gt;目的&lt;/h4&gt;研究视觉表征学习的结构和拓扑特性，理解视觉观测空间的组织方式，以及语义抽象的实现机制。&lt;h4&gt;方法&lt;/h4&gt;从视觉语义语言的基本假设出发，结合表征学习的前提，推导视觉观测空间的纤维束结构，并分析其对语义不变性和模型架构的要求。&lt;h4&gt;主要发现&lt;/h4&gt;语义商空间不是原空间的子流形，不能仅通过光滑变形获得，需要非同胚的区分性目标；语义抽象需要能够支持拓扑变化的表征机制，包括扩展-捕捉过程。&lt;h4&gt;结论&lt;/h4&gt;该框架提供了理解视觉表征学习的拓扑透镜，与大规模判别性和多模态模型中的经验规律以及统计学习理论中的经典原理一致，具有解释性而非规定性。&lt;h4&gt;翻译&lt;/h4&gt;我们从结构和拓扑的角度研究视觉表征学习。我们从一个单一假设出发：视觉理解需要一个视觉语义语言，其中许多感知观测对应于少量离散的语义状态。结合表征学习中广泛假设的可迁移性和抽象性前提，这个假设意味着视觉观测空间必须以类似纤维束的结构组织，其中干扰变量填充纤维，语义对应于商基空间。从这个结构我们推导出两个理论结果。首先，语义商空间不是原空间的子流形，不能仅通过光滑变形获得，语义不变性需要一个非同胚的、有区分性的目标。其次，我们表明近似商空间也对模型架构提出了结构要求。语义抽象不仅需要外部语义目标，还需要能够支持拓扑变化的表征机制：一个扩展-捕捉过程，其中流形首先被几何扩展以分离结构，然后被折叠以形成离散的语义区域。我们强调这些结果是解释性的而非规定性的：该框架提供了一个拓扑透镜，与大规模判别性和多模态模型中观察到的经验规律以及统计学习理论中的经典原理一致。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study visual representation learning from a structural and topological perspective. We begin from a single hypothesis: that visual understanding presupposes a semantic language for vision, in which many perceptual observations correspond to a small number of discrete semantic states. Together with widely assumed premises on transferability and abstraction in representation learning, this hypothesis implies that the visual observation space must be organized in a fiber bundle like structure, where nuisance variation populates fibers and semantics correspond to a quotient base space. From this structure we derive two theoretical consequences. First, the semantic quotient $X/G$ is not a submanifold of $X$ and cannot be obtained through smooth deformation alone, semantic invariance requires a non-homeomorphic, discriminative target, for example, supervision via labels, cross instance identification, or multimodal alignment that supplies explicit semantic equivalence. Second, we show that approximating the quotient also places structural demands on the model architecture. Semantic abstraction requires not only an external semantic target, but a representation mechanism capable of supporting topology change: an expand-and-snap process in which the manifold is first geometrically expanded to separate structure and then collapsed to form discrete semantic regions. We emphasize that these results are interpretive rather than prescriptive: the framework provides a topological lens that aligns with empirical regularities observed in large-scale discriminative and multimodal models, and with classical principles in statistical learning theory.</description>
      <author>example@mail.com (Xiu Li)</author>
      <guid isPermaLink="false">2512.23335v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-based Decentralized Federated Multi-Task Representation Learning</title>
      <link>http://arxiv.org/abs/2512.23161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于投影梯度下降的去中心化算法，用于多任务表示学习，特别是在数据稀缺环境中的应用。&lt;h4&gt;背景&lt;/h4&gt;表示学习是一种广泛采用的框架，用于在数据稀缺环境中从各种不同但相关的任务中获取特征提取器。尽管表示学习已有大量研究，但去中心化的方法仍然相对未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于投影梯度下降的去中心化算法，用于多任务表示学习，特别是在多任务线性回归场景中。&lt;h4&gt;方法&lt;/h4&gt;专注于多任务线性回归问题，其中多个线性回归模型共享一个共同的低维线性表示。提出了一种交替投影梯度下降和最小化算法，用于在基于扩散的去中心化和联邦方式中恢复低秩特征矩阵。&lt;h4&gt;主要发现&lt;/h4&gt;获得了建设性的、可证明的保证，为所提出的算法提供了所需样本复杂度的下限和迭代复杂度的上限。算法在时间和通信复杂度方面表现出快速且高效的特性。&lt;h4&gt;结论&lt;/h4&gt;通过数值模拟验证了算法的性能，并将其与基准算法进行了比较，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;表示学习是一种广泛采用的框架，用于在数据稀缺环境中学习，从各种不同但相关的任务中获取特征提取器或表示。尽管表示学习已有大量研究，但去中心化的方法仍然相对未被充分探索。本文开发了一种基于投影梯度下降的去中心化算法，用于多任务表示学习。我们专注于多任务线性回归问题，其中多个线性回归模型共享一个共同的低维线性表示。我们提出了一种交替投影梯度下降和最小化算法，用于在基于扩散的去中心化和联邦方式中恢复低秩特征矩阵。我们获得了建设性的、可证明的保证，为所提出的算法提供了所需样本复杂度的下限和迭代复杂度的上限。我们分析了算法的时间和通信复杂度，表明它是快速且通信高效的。我们进行了数值模拟以验证算法的性能，并将其与基准算法进行了比较。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning is a widely adopted framework for learning in data-scarce environments to obtain a feature extractor or representation from various different yet related tasks. Despite extensive research on representation learning, decentralized approaches remain relatively underexplored. This work develops a decentralized projected gradient descent-based algorithm for multi-task representation learning. We focus on the problem of multi-task linear regression in which multiple linear regression models share a common, low-dimensional linear representation. We present an alternating projected gradient descent and minimization algorithm for recovering a low-rank feature matrix in a diffusion-based decentralized and federated fashion. We obtain constructive, provable guarantees that provide a lower bound on the required sample complexity and an upper bound on the iteration complexity of our proposed algorithm. We analyze the time and communication complexity of our algorithm and show that it is fast and communication-efficient. We performed numerical simulations to validate the performance of our algorithm and compared it with benchmark algorithms.</description>
      <author>example@mail.com (Donghwa Kang, Shana Moothedath)</author>
      <guid isPermaLink="false">2512.23161v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives</title>
      <link>http://arxiv.org/abs/2512.22983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work is a re-architected core derived from the full survey (arXiv:2510.10903) , refined to highlight the most central themes and representative studies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述从算法角度审视了机器人操作领域，将基于学习的方法分为高层规划和底层控制两部分。在高层，扩展了任务规划概念以包含多种推理形式；在底层，提出了基于训练范式的分类法。文章还确定了开放挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;视觉、语言和多模态学习的最新进展极大地推动了机器人基础模型的发展，其中机器人操作仍然是一个核心且具有挑战性的问题。&lt;h4&gt;目的&lt;/h4&gt;从算法角度审视机器人操作，组织基于学习的方法，确定开放挑战和未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;将基于学习的方法组织在一个统一的抽象框架下，分为高层规划和底层控制两个层面。高层包括语言、代码、运动、可供性和3D表示的推理；底层按照输入建模、潜在表示学习和策略学习进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;扩展了经典任务规划概念，提出了基于训练范式的控制方法分类法，确定了可扩展性、数据效率、多模态物理交互和安全方面的开放挑战。&lt;h4&gt;结论&lt;/h4&gt;这些分析旨在阐明现代机器人操作基础模型的设计空间。&lt;h4&gt;翻译&lt;/h4&gt;视觉、语言和多模态学习的最新进展极大地推动了机器人基础模型的进步，其中机器人操作仍然是一个核心且具有挑战性的问题。本综述从算法角度审视机器人操作，并将基于学习的方法组织在一个高层规划和底层控制的统一抽象框架中。在高层，我们将经典的任务规划概念扩展到包括对语言、代码、运动、可供性和3D表示的推理，强调它们在结构化和长期决策中的作用。在底层，我们提出了一个基于训练范式的分类法用于基于学习的控制，沿着输入建模、潜在表示学习和策略学习组织现有方法。最后，我们确定了与可扩展性、数据效率、多模态物理交互和安全相关的开放挑战和未来研究方向。这些分析共同旨在阐明现代机器人操作基础模型的设计空间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision, language, and multimodal learning have substantially accelerated progress in robotic foundation models, with robot manipulation remaining a central and challenging problem. This survey examines robot manipulation from an algorithmic perspective and organizes recent learning-based approaches within a unified abstraction of high-level planning and low-level control. At the high level, we extend the classical notion of task planning to include reasoning over language, code, motion, affordances, and 3D representations, emphasizing their role in structured and long-horizon decision making. At the low level, we propose a training-paradigm-oriented taxonomy for learning-based control, organizing existing methods along input modeling, latent representation learning, and policy learning. Finally, we identify open challenges and prospective research directions related to scalability, data efficiency, multimodal physical interaction, and safety. Together, these analyses aim to clarify the design space of modern foundation models for robotic manipulation.</description>
      <author>example@mail.com (Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Zhe Li, Pengxiang Ding, Cheng Chi, Chang Xu, Xiaolong Zheng, Donglin Wang, Haoang Li, Shanghang Zhang, Badong Chen)</author>
      <guid isPermaLink="false">2512.22983v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning with the $p$-adics</title>
      <link>http://arxiv.org/abs/2512.22692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了将p-adic数作为替代实数域的机器学习框架的适用性，建立了基于p-adic数的分类、回归和表示学习的理论基础，并展示了如何将Quillian语义网络表示为紧凑的p-adic线性网络。&lt;h4&gt;背景&lt;/h4&gt;现有的机器学习框架在实数域上运行，并在实数向量空间中学习表示，这些几何属性与线性可分性、最小包围球和子空间投影等直观概念相吻合，并且基于梯度的优化方法提供了学习工具。&lt;h4&gt;目的&lt;/h4&gt;探索p-adic数这一非阿基米德空间作为实数域替代方案的适用性，利用其层次结构和无限字符串解释的特性，为编码理论和层次表示学习提供新工具。&lt;h4&gt;方法&lt;/h4&gt;进行了探索性理论研究，建立了基于p-adic数的分类、回归和表示学习的构建模块，提供了学习模型和算法。展示了如何将简单的Quillian语义网络表示为紧凑的p-adic线性网络。&lt;h4&gt;主要发现&lt;/h4&gt;p-adic数的层次结构和无限字符串特性使其成为编码理论和层次表示学习的有吸引力的工具。Quillian语义网络可以表示为紧凑的p-adic线性网络，而这种构造在实数域中是不可能的。&lt;h4&gt;结论&lt;/h4&gt;p-adic数提供了一种新的机器学习框架，为未来研究开辟了新的可能性和开放性问题。&lt;h4&gt;翻译&lt;/h4&gt;现有的机器学习框架在实数域上运行，并在实数向量空间中学习表示。它们的基本几何属性与线性可分性、最小包围球和子空间投影等直观概念相吻合；基础微积分提供了通过基于梯度的优化学术习的工具。但这是唯一可能的选择吗？在本文中，我们研究了一个截然不同的域作为替代方案的适用性——p-adic数的非阿基米德空间。p-adic数的层次结构及其作为无限字符串的解释使它们成为编码理论和层次表示学习的有吸引力的工具。我们的探索性理论研究建立了基于p-adic数的分类、回归和表示学习的构建模块，提供了学习模型和算法。我们展示了如何将简单的Quillian语义网络表示为紧凑的p-adic线性网络，而这种构造在实数域中是不可能的。最后，我们讨论了这一新框架带来的开放性问题和未来研究机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing machine learning frameworks operate over the field of real numbers ($\mathbb{R}$) and learn representations in real (Euclidean or Hilbert) vector spaces (e.g., $\mathbb{R}^d$). Their underlying geometric properties align well with intuitive concepts such as linear separability, minimum enclosing balls, and subspace projection; and basic calculus provides a toolbox for learning through gradient-based optimization.  But is this the only possible choice? In this paper, we study the suitability of a radically different field as an alternative to $\mathbb{R}$ -- the ultrametric and non-archimedean space of $p$-adic numbers, $\mathbb{Q}_p$. The hierarchical structure of the $p$-adics and their interpretation as infinite strings make them an appealing tool for code theory and hierarchical representation learning. Our exploratory theoretical work establishes the building blocks for classification, regression, and representation learning with the $p$-adics, providing learning models and algorithms. We illustrate how simple Quillian semantic networks can be represented as a compact $p$-adic linear network, a construction which is not possible with the field of reals. We finish by discussing open problems and opportunities for future research enabled by this new framework.</description>
      <author>example@mail.com (André F. T. Martins)</author>
      <guid isPermaLink="false">2512.22692v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning</title>
      <link>http://arxiv.org/abs/2512.22675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在数据稀缺环境下的去中心化多任务表示学习方法，提出了一种具有可证明准确性保证的新算法，其通信复杂度与目标精度无关，显著降低了通信成本。&lt;h4&gt;背景&lt;/h4&gt;表示学习是在数据稀缺环境中广泛采用的框架，旨在从相关任务中提取共同特征。虽然集中式方法已被广泛研究，但去中心化方法在很大程度上仍未被探索。在去中心化设置中，任务数据分布在多个节点上，节点间的信息交换受到通信网络限制。&lt;h4&gt;目的&lt;/h4&gt;研究特征共享低秩结构的去中心化多任务表示学习，目标是恢复潜在的低秩特征矩阵，并提出具有可证明准确性保证的算法，同时全面表征时间、通信和样本复杂度。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的交替投影梯度最小化算法，该算法在去中心化环境中工作，考虑多个任务，每个任务有有限数量的数据样本，观测值遵循具有任务特定参数的线性模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 所提算法具有可证明的准确性保证；2) 通信复杂度与目标精度无关，显著降低通信成本；3) 数值模拟在不同维度和网络拓扑下验证了理论分析；4) 展示了去中心化学习在某些情况下优于集中式联邦方法的场景。&lt;h4&gt;结论&lt;/h4&gt;去中心化多任务表示学习是数据稀缺环境下的有效方法，特别是在通信受限环境中。所提算法能在保持低通信复杂度的同时有效恢复低秩特征结构，为去中心化学习提供了新的理论和实践见解。&lt;h4&gt;翻译&lt;/h4&gt;表示学习是在数据稀缺环境中广泛采用的框架，旨在从相关任务中提取共同特征。虽然集中式方法已被广泛研究，但去中心化方法在很大程度上仍未被探索。我们研究了特征共享低秩结构的去中心化多任务表示学习。我们考虑多个任务，每个任务有有限数量的数据样本，其中观测值遵循具有任务特定参数的线性模型。在去中心化设置中，任务数据分布在多个节点上，节点之间的信息交换受到通信网络的限制。目标是恢复潜在的特征矩阵，该矩阵的秩远小于参数维度和任务数量。我们提出了一种具有可证明准确性保证的新交替投影梯度最小化算法。我们全面表征了时间、通信和样本复杂度。重要的是，通信复杂度与目标精度无关，与先前方法相比显著降低了通信成本。数值模拟在不同维度和网络拓扑下验证了理论分析，并展示了去中心化学习在某些情况下优于集中式联邦方法的场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning is a widely adopted framework for learning in data-scarce environments, aiming to extract common features from related tasks. While centralized approaches have been extensively studied, decentralized methods remain largely underexplored. We study decentralized multi-task representation learning in which the features share a low-rank structure. We consider multiple tasks, each with a finite number of data samples, where the observations follow a linear model with task-specific parameters. In the decentralized setting, task data are distributed across multiple nodes, and information exchange between nodes is constrained by a communication network. The goal is to recover the underlying feature matrix whose rank is much smaller than both the parameter dimension and the number of tasks. We propose a new alternating projected gradient and minimization algorithm with provable accuracy guarantees. We provide comprehensive characterizations of the time, communication, and sample complexities. Importantly, the communication complexity is independent of the target accuracy, which significantly reduces communication cost compared to prior methods. Numerical simulations validate the theoretical analysis across different dimensions and network topologies, and demonstrate regimes in which decentralized learning outperforms centralized federated approaches.</description>
      <author>example@mail.com (Donghwa Kang, Shana Moothedath)</author>
      <guid isPermaLink="false">2512.22675v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Tracking by Predicting 3-D Gaussians Over Time</title>
      <link>http://arxiv.org/abs/2512.22489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Video Gaussian Masked Autoencoders (Video-GMAE)，一种自监督视频表示学习方法，通过将图像序列编码为随时间移动的高斯点集合来学习视频表示。&lt;h4&gt;背景&lt;/h4&gt;视频表示学习需要有效捕捉时空信息，现有的自监督方法在视频理解和跟踪方面仍有提升空间。&lt;h4&gt;目的&lt;/h4&gt;开发一种通过将视频表示为随时间变化的高斯集合来学习视频时空表示的方法，实现零样本跟踪和高效的视频表示学习。&lt;h4&gt;方法&lt;/h4&gt;Video-GMAE将视频序列编码为随时间移动的高斯点集合，强制执行2D视频是动态3D场景一致投影的归纳偏置，通过预训练网络自然学习跟踪能力。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用该架构预训练网络时跟踪能力自然涌现；2) 将高斯轨迹映射到图像平面可实现与最先进方法相当的零样本跟踪性能；3) 小规模微调后在Kinetics和Kubric数据集上分别实现34.6%和13.1%的改进；4) 结果超越了现有自监督视频方法。&lt;h4&gt;结论&lt;/h4&gt;Video-GMAE通过将视频表示为随时间变化的高斯集合，有效学习了视频时空表示，实现了卓越的零样本跟踪性能和视频表示能力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出视频高斯掩码自编码器(Video-GMAE)，一种用于表示学习的自监督方法，它将图像序列编码为一组随时间移动的高斯点。将视频表示为一组高斯点强制执行了合理的归纳偏置：即2D视频通常是动态3D场景的一致投影。我们发现，使用这种架构预训练网络时会出现跟踪能力。将学习到的高斯轨迹映射到图像平面上可以实现零样本跟踪性能，与最先进方法相当。通过小规模微调，我们的模型在Kinetics数据集上实现了34.6%的改进，在Kubric数据集上实现了13.1%的改进，超越了现有的自监督视频方法。项目页面和代码可在https://videogmae.org/和https://github.com/tekotan/video-gmae公开获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视频中的像素点跟踪问题，即在视频中持续追踪特定像素点位置的任务。这个问题在现实中非常重要，因为像素跟踪是计算机视觉的基础能力，对于理解视频内容、分析物体运动、场景结构等至关重要；同时，它也是许多高级视觉任务（如3D理解、计算摄影、长期推理等）的基础。传统方法通常需要大量标注数据，而本文提出的方法可以在无需大量标注的情况下实现高质量的像素跟踪。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先发现现有的自监督视频学习方法在点跟踪任务上表现不佳，认为传统的时空块预测目标不能强制执行时间一致性。他们注意到3D物体运动在图像平面上表现为点跟踪，由此产生灵感：通过预测随时间移动的3D高斯基元来学习对应关系。该方法借鉴了掩码自编码器（MAE）的架构、3D高斯溅射表示技术和自监督学习思想，但创新性地将这些技术结合用于视频表示学习和点跟踪任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将视频表示为随时间移动的3D高斯基元的集合，这种表示强制了一个合理的归纳偏置：2D视频通常是动态3D场景的一致投影。通过预测高斯基元随时间的演变，模型能够学习像素级对应关系。整体流程包括：1）预训练阶段：输入16帧视频，编码器处理可见块，解码器预测第一帧的高斯基元和后续帧的高斯增量，通过高斯积分和渲染重建视频；2）零样本跟踪：将高斯轨迹映射到图像平面生成流场，用于点跟踪；3）监督微调：在点跟踪数据集上微调预训练模型，提升跟踪性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出将视频表示为随时间演变的3D高斯集合的新方法；2）通过预测高斯基元的时间演变实现自监督跟踪学习；3）自然涌现出零样本跟踪能力；4）结合自监督学习和可微分渲染的统一框架。相比之前的工作，不同之处在于：传统自监督视频方法（如VideoMAE）不强制时间一致性，而本文通过高斯表示显式强制对应；其他自监督跟踪方法（如CRW、GMRW）使用随机游走或对比学习，而本文基于3D高斯表示提供更强几何约束；相比监督方法，本文需要更少标注数据且零样本性能接近或超过监督方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Video-GMAE通过将视频表示为随时间演变的3D高斯集合，实现了自监督学习下的像素级对应关系，从而在无需标注数据的情况下实现了强大的零样本点跟踪能力，并在微调后达到了最先进的跟踪性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.</description>
      <author>example@mail.com (Tanish Baranwal, Himanshu Gaurav Singh, Jathushan Rajasegaran, Jitendra Malik)</author>
      <guid isPermaLink="false">2512.22489v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment</title>
      <link>http://arxiv.org/abs/2512.22488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可扩展的自适应物联网威胁检测框架，解决了基于AI的模型在动态物联网环境中的部署问题，避免了持续分类器重新训练的需要，同时保持了对概念漂移的鲁棒检测性能。&lt;h4&gt;背景&lt;/h4&gt;基于AI的模型在物联网威胁检测中已取得高准确率，但在企业环境中的应用受到限制，因为这些模型依赖于静态数据集，无法反映真实物联网网络流量的动态特性。真实物联网网络流量经常受到概念漂移的影响，而现有解决方案通常依赖于定期分类器重新训练，导致高计算开销和灾难性遗忘的风险。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，本文提出了一种可扩展的自适应物联网威胁检测框架，消除了对连续分类器重新训练的需求，同时保持对概念漂移的鲁棒检测性能。&lt;h4&gt;方法&lt;/h4&gt;所提出的方法首先在历史流量的潜在空间表示上训练一次分类器，同时一个对齐模型将传入流量映射到学习到的历史潜在空间，然后再进行分类，从而保留先前观察到的攻击知识。为了捕获攻击样本之间的实例关系，低维潜在表示被进一步转换为图结构格式，并使用图神经网络进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界异构物联网流量数据集上的实验评估表明，所提出的框架在概念漂移条件下保持了稳健的检测性能。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了该框架在动态和大规模物联网环境中实际部署的潜力。&lt;h4&gt;翻译&lt;/h4&gt;虽然基于AI的模型在物联网威胁检测中已取得高准确率，但它们在企业环境中的应用受到限制，因为它们依赖于无法反映真实物联网网络流量动态特性的静态数据集，而真实物联网网络流量经常受到概念漂移的影响。现有解决方案通常依赖于定期分类器重新训练，导致高计算开销和灾难性遗忘的风险。为了解决这些挑战，本文提出了一种可扩展的自适应物联网威胁检测框架，消除了对连续分类器重新训练的需求。所提出的方法首先在历史流量的潜在空间表示上训练一次分类器，同时一个对齐模型将传入流量映射到学习到的历史潜在空间，然后再进行分类，从而保留先前观察到的攻击知识。为了捕获攻击样本之间的实例关系，低维潜在表示被进一步转换为图结构格式，并使用图神经网络进行分类。在真实世界异构物联网流量数据集上的实验评估表明，所提出的框架在概念漂移条件下保持了稳健的检测性能。这些结果突显了该框架在动态和大规模物联网环境中实际部署的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although AI-based models have achieved high accuracy in IoT threat detection, their deployment in enterprise environments is constrained by reliance on stationary datasets that fail to reflect the dynamic nature of real-world IoT NetFlow traffic, which is frequently affected by concept drift. Existing solutions typically rely on periodic classifier retraining, resulting in high computational overhead and the risk of catastrophic forgetting. To address these challenges, this paper proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining. The proposed approach trains a classifier once on latent-space representations of historical traffic, while an alignment model maps incoming traffic to the learned historical latent space prior to classification, thereby preserving knowledge of previously observed attacks. To capture inter-instance relationships among attack samples, the low-dimensional latent representations are further transformed into a graph-structured format and classified using a graph neural network. Experimental evaluations on real-world heterogeneous IoT traffic datasets demonstrate that the proposed framework maintains robust detection performance under concept drift. These results highlight the framework's potential for practical deployment in dynamic and large-scale IoT environments.</description>
      <author>example@mail.com (Hassan Wasswa, Timothy Lynar)</author>
      <guid isPermaLink="false">2512.22488v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Galaxy Zoo Evo: 1 million human-annotated images of galaxies</title>
      <link>http://arxiv.org/abs/2512.23691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to NeurIPS Datasets and Benchmarks 2025. Positive reviews but rejected by AC; see OpenReview&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为Galaxy Zoo Evo的大型标注数据集，用于构建和评估星系图像的基础模型，包含823k张图像和104M众包标签，支持多种天文研究应用。&lt;h4&gt;背景&lt;/h4&gt;天文学研究需要大量高质量的星系图像数据来训练和评估模型，特别是随着新空间望远镜如Euclid的发射，对基础模型的需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模、精细标注的星系图像数据集，支持天文领域基础模型的开发、评估和特定下游任务，如寻找强引力透镜和描述新星系。&lt;h4&gt;方法&lt;/h4&gt;收集来自四个望远镜的823k张星系图像，通过众包方式获得104M精细标签，每个图像都有详细的问题和答案描述，同时提供四个较小的特定任务标签集（共167k个星系）。&lt;h4&gt;主要发现&lt;/h4&gt;Galaxy Zoo Evo数据集提供了详细的星系图像标注，可用于预训练或微调模型，支持域适应和学习不确定性等计算机视觉研究，为天文领域基础模型的发展提供了重要资源。&lt;h4&gt;结论&lt;/h4&gt;Galaxy Zoo Evo将成为计算机视觉领域（特别是域适应和学习不确定性）的真实世界基准，支持新一代天文基础模型的发展，对未来天文学家更好地理解宇宙至关重要。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了Galaxy Zoo Evo，这是一个用于构建和评估星系图像基础模型的标注数据集。GZ Evo包含来自四个望远镜的823k张图像，共计104M的众包标签。每张图像都标注了一系列精细的问题和答案（例如'特征星系，两条旋臂，紧密缠绕，与另一个星系合并'）。这些详细标签可用于预训练或微调。我们还包含了四个较小的标签集（总共167k个星系），用于天文学家感兴趣的特定下游任务，包括寻找强引力透镜和描述来自新空间望远镜Euclid的星系。我们希望GZ Evo能作为计算机视觉主题（如域适应（从地面到天文，或望远镜之间）或从众包标签中学习不确定性）的真实世界基准。我们也希望它能支持天文领域新一代基础模型的发展；对于寻求更好理解我们宇宙的未来天文学家来说，这类模型将至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Galaxy Zoo Evo, a labeled dataset for building and evaluating foundation models on images of galaxies. GZ Evo includes 104M crowdsourced labels for 823k images from four telescopes. Each image is labeled with a series of fine-grained questions and answers (e.g. "featured galaxy, two spiral arms, tightly wound, merging with another galaxy"). These detailed labels are useful for pretraining or finetuning. We also include four smaller sets of labels (167k galaxies in total) for downstream tasks of specific interest to astronomers, including finding strong lenses and describing galaxies from the new space telescope Euclid. We hope GZ Evo will serve as a real-world benchmark for computer vision topics such as domain adaption (from terrestrial to astronomical, or between telescopes) or learning under uncertainty from crowdsourced labels. We also hope it will support a new generation of foundation models for astronomy; such models will be critical to future astronomers seeking to better understand our universe.</description>
      <author>example@mail.com (Mike Walmsley, Steven Bamford, Hugh Dickinson, Tobias Géron, Alexander J. Gordon, Annette M. N. Ferguson, Lucy Fortson, Sandor Kruk, Natalie Lines, Chris J. Lintott, Karen L. Masters, Robert G. Mann, James Pearson, Hayley Roberts, Anna M. M. Scaife, Stefan Schuldt, Brooke Simmons, Rebecca Smethurst, Josh Speagle, Kyle Willett)</author>
      <guid isPermaLink="false">2512.23691v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence</title>
      <link>http://arxiv.org/abs/2512.23485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 40th Annual AAAI Conference on Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FRoD是一种新型参数高效微调方法，结合分层联合分解和旋转自由度，通过提取跨层全局共享基并注入稀疏可学习扰动，实现灵活全秩更新，在保持高效的同时提升表达能力。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)方法通过只更新一小部分参数来减少计算和内存成本，使大型基础模型能适应下游任务。但现有方法如LoRA因固有的低秩约束而面临收敛速度慢和适应能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;解决PEFT方法在捕捉多样化任务所需复杂模式方面的局限性，提高微调的表达能力和效率。&lt;h4&gt;方法&lt;/h4&gt;提出FRoD方法，结合分层联合分解与旋转自由度，提取跨层全局共享基，向缩放因子中注入稀疏可学习扰动，实现灵活的全秩更新。&lt;h4&gt;主要发现&lt;/h4&gt;FRoD在20个涵盖视觉、推理和语言理解的基准测试中，仅使用1.72%的可训练参数，就能达到与完整模型微调相当的准确率，同时实现更快更稳健的收敛。&lt;h4&gt;结论&lt;/h4&gt;FRoD成功解决了现有PEFT方法在效率与表达能力之间的权衡问题，在保持高效率的同时实现了与完整模型微调相当的性能。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)方法已成为适应大型基础模型到下游任务的实用解决方案，通过只更新一小部分参数来减少计算和内存成本。其中，像LoRA这样的方法试图在效率和表达能力之间取得平衡，但往往因固有的低秩约束而面临收敛速度慢和适应能力有限的问题。这种权衡阻碍了PEFT方法捕捉多样化任务所需复杂模式的能力。为解决这些挑战，我们提出了FRoD，一种结合分层联合分解与旋转自由度的微调新方法。通过提取跨层的全局共享基并向缩放因子中注入稀疏可学习扰动以实现灵活的全秩更新，FRoD提高了表达能力和效率，实现了更快、更稳健的收敛。在涵盖视觉、推理和语言理解的20个基准测试中，FRoD在相同训练预算下仅使用1.72%的可训练参数，就能达到与完整模型微调相当的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.</description>
      <author>example@mail.com (Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu)</author>
      <guid isPermaLink="false">2512.23485v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Integrating Uncertainty for Domain-Agnostic Segmentation</title>
      <link>http://arxiv.org/abs/2512.23427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Public code at https://github.com/JesseBrouw/UncertSAM | published at the 2nd Workshop on Frontiers in Probabilistic Inference (NeurIPS 2025) | 12 pages, 8 figures (incl. Appendix)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了不确定性量化如何缓解基础分割模型在变化或有限知识领域中的脆弱性，并增强其泛化能力。&lt;h4&gt;背景&lt;/h4&gt;基础分割模型如Segment Anything Model (SAM)系列表现出强大的零样本性能，但在变化或知识有限的领域中仍然脆弱。&lt;h4&gt;目的&lt;/h4&gt;研究不确定性量化是否可以缓解这些挑战，以领域无关的方式增强模型泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1) 整理了UncertSAM基准，包含八个数据集，旨在在具有挑战性的分割条件下测试SAM，包括阴影、透明度和伪装；2) 评估了一套轻量级的后验不确定性估计方法；3) 评估了一个初步的不确定性引导的预测细化步骤。&lt;h4&gt;主要发现&lt;/h4&gt;在评估的方法中，最后一层拉普拉斯近似产生的不确定性估计与分割错误有很好的相关性，表明有有意义的信号。虽然细化的好处是初步的，但研究发现强调将不确定性纳入分割模型的潜力。&lt;h4&gt;结论&lt;/h4&gt;不确定性量化可以帮助缓解基础分割模型在变化或有限知识领域中的脆弱性，支持稳健的、领域无关的性能。研究团队公开了UncertSAM基准和相关代码。&lt;h4&gt;翻译&lt;/h4&gt;用于分割的基础模型如Segment Anything Model (SAM)系列表现出强大的零样本性能，但在变化或知识有限的领域中仍然脆弱。本研究探讨了不确定性量化是否可以缓解这些挑战，并以领域无关的方式增强模型泛化能力。为此，我们(1)整理了UncertSAM基准，包含八个数据集，旨在在具有挑战性的分割条件下测试SAM，包括阴影、透明度和伪装；(2)评估了一套轻量级的后验不确定性估计方法；以及(3)评估了一个初步的不确定性引导的预测细化步骤。在评估的方法中，最后一层拉普拉斯近似产生的不确定性估计与分割错误有很好的相关性，表明有有意义的信号。虽然细化的好处是初步的，但我们的发现强调将不确定性纳入分割模型的潜力，以支持稳健的、领域无关的性能。我们的基准和代码已公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for segmentation such as the Segment Anything Model (SAM) family exhibit strong zero-shot performance, but remain vulnerable in shifted or limited-knowledge domains. This work investigates whether uncertainty quantification can mitigate such challenges and enhance model generalisability in a domain-agnostic manner. To this end, we (1) curate UncertSAM, a benchmark comprising eight datasets designed to stress-test SAM under challenging segmentation conditions including shadows, transparency, and camouflage; (2) evaluate a suite of lightweight, post-hoc uncertainty estimation methods; and (3) assess a preliminary uncertainty-guided prediction refinement step. Among evaluated approaches, a last-layer Laplace approximation yields uncertainty estimates that correlate well with segmentation errors, indicating a meaningful signal. While refinement benefits are preliminary, our findings underscore the potential of incorporating uncertainty into segmentation models to support robust, domain-agnostic performance. Our benchmark and code are made publicly available.</description>
      <author>example@mail.com (Jesse Brouwers, Xiaoyan Xing, Alexander Timans)</author>
      <guid isPermaLink="false">2512.23427v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Agentic AI-Enhanced Semantic Communications: Foundations, Architecture, and Applications</title>
      <link>http://arxiv.org/abs/2512.23294v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文系统阐述了智能体AI如何赋能语义通信(SemCom)，从研究基础、系统架构和应用场景三个角度进行全面分析，并提出统一框架和典型案例研究。&lt;h4&gt;背景&lt;/h4&gt;语义通信作为6G的关键技术之一，正推动网络从比特传输向语义信息交换转变。智能体AI具有感知、记忆、推理和行动能力，为智能通信提供了可行路径。&lt;h4&gt;目的&lt;/h4&gt;系统阐述智能体AI赋能语义通信的方式，包括研究基础、系统架构和应用场景，提出统一框架，并讨论未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;按智能体类型综述现有研究；提出涵盖应用层、语义层和云边缘协作层的统一框架；展示多车协同感知等典型场景；介绍基于智能体知识库的联合信源信道编码案例研究；讨论未来演进方向。&lt;h4&gt;主要发现&lt;/h4&gt;提出的AKB-JSCC在不同信道条件下实现更高信息重建质量；智能体AI为语义通信提供了从意图到评估的闭环系统。&lt;h4&gt;结论&lt;/h4&gt;为智能体语义通信的可移植性、可验证性和可控性研究与部署提供了参考。&lt;h4&gt;翻译&lt;/h4&gt;语义通信(SemCom)作为6G的关键技术之一，正在推动网络从比特传输向语义信息交换转变。在此基础上，引入具有感知、记忆、推理和行动能力的智能体AI为智能通信提供了可行的路径。本文从研究基础、系统架构和应用场景三个角度系统阐述了智能体AI如何赋能语义通信。我们首先按智能体类型对现有研究进行了全面综述，包括嵌入式智能体、大语言模型/大视觉模型智能体和强化学习智能体。此外，我们提出了一个统一的智能体AI增强的语义通信框架，涵盖应用层、语义层和云边缘协作层，形成从意图到编码、传输、解码、行动到评估的闭环。我们还展示了几个典型场景，包括多车协同感知、多机器人协作救援和智能网络的智能体操作。此外，我们介绍了一个基于智能体知识库的联合信源信道编码案例研究AKB-JSCC，其中源知识库和信道知识库分别由大语言模型/大视觉模型智能体和强化学习智能体构建。实验结果表明，AKB-JSCC在不同信道条件下能够实现更高的信息重建质量。最后，我们讨论了未来演进和研究方向，为智能体语义通信的可移植性、可验证性和可控性研究与部署提供了参考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic communications (SemCom), as one of the key technologies for 6G, is shifting networks from bit transmission to semantic information exchange. On this basis, introducing agentic artificial intelligence (AI) with perception, memory, reasoning, and action capabilities provides a practicable path to intelligent communications. This paper provides a systematic exposition of how agentic AI empowers SemCom from the perspectives of research foundations, system architecture, and application scenarios. We first provide a comprehensive review of existing studies by agent types, covering embedded agents, large language model (LLM)/large vision model (LVM) agents, and reinforcement learning (RL) agents. Additionally, we propose a unified agentic AI-enhanced SemCom framework covering the application layer, the semantic layer, and the cloud-edge collaboration layer, forming a closed loop from intent to encoding to transmission to decoding to action to evaluation. We also present several typical scenarios, including multi-vehicle collaborative perception, multi-robot cooperative rescue, and agentic operations for intellicise (intelligent and concise) networks. Furthermore, we introduce an agentic knowledge base (KB)-based joint source-channel coding case study, AKB-JSCC, where the source KB and channel KB are built by LLM/LVM agents and RL agents, respectively. Experimental results show that AKB-JSCC achieves higher information reconstruction quality under different channel conditions. Finally, we discuss future evolution and research directions, providing a reference for portable, verifiable, and controllable research and deployment of agentic SemCom.</description>
      <author>example@mail.com (Haixiao Gao, Mengying Sun, Ruichen Zhang, Yanhan Wang, Xiaodong Xu, Nan Ma, Dusit Niyato, Ping Zhang)</author>
      <guid isPermaLink="false">2512.23294v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control</title>
      <link>http://arxiv.org/abs/2512.23292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于物理验证而非感知推理的智能体物理AI模型，在合成反应堆控制场景中表现出优于通用基础模型的性能。&lt;h4&gt;背景&lt;/h4&gt;当前AI在物理系统中的主流范式是将通用基础模型扩展到多模态推理，但这种方法在控制界面面临根本障碍，即使最先进的视觉语言模型在基础物理任务上的准确率仅50-53%。&lt;h4&gt;目的&lt;/h4&gt;提出一种通向领域特定基础模型的根本不同路径，通过紧凑语言模型作为智能体物理AI，实现基于物理验证的策略优化。&lt;h4&gt;方法&lt;/h4&gt;在合成反应堆控制场景中训练一个3.6亿参数的模型，数据集从10^3扩展到10^5个例子，使用物理验证驱动策略优化而非感知推理。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了通用模型中不存在的尖锐相变；小规模系统表现高方差模仿和灾难性尾部风险；大规模模型经历超过500倍的方差减少，稳定执行级别行为；模型自主拒绝约70%的训练分布，95%的运行时执行集中在单一策略上；学习表示可在不同物理和连续输入模态间转移。&lt;h4&gt;结论&lt;/h4&gt;物理验证驱动的AI模型在物理系统控制中表现更优，能够实现更稳定和可靠的行为，且具有跨模态迁移能力。&lt;h4&gt;翻译&lt;/h4&gt;当前AI物理系统的主流范式是将通用基础模型扩展到通用多模态推理，但在控制界面面临根本性障碍。最近的基准测试显示，即使是最先进的视觉语言模型在基础物理任务上的准确率也只有50-53%，表现为近似猜测者，保持语义合理性但违反物理约束。这种输入不忠实不是扩展缺陷，而是结构限制。以感知为中心的架构优化参数空间模仿，而安全关键控制需要执行动作的结果空间保证。在此，我们提出了一种通向领域特定基础模型的根本不同路径，引入了作为智能体物理AI运行的紧凑语言模型，其中策略优化由基于物理的验证而非感知推理驱动。我们在合成反应堆控制场景中训练了一个3.6亿参数的模型，将数据集从10^3扩展到10^5个例子。这导致了通用模型中不存在的尖锐相变。小规模系统表现出高方差模仿和灾难性尾部风险，而大规模模型经历超过500倍的方差减少，稳定了执行级别行为。尽管接触了四种执行家族，但模型自主拒绝了约70%的训练分布，并将95%的运行时执行集中在单一银行策略上。学习到的表示可以在不同物理和连续输入模态间转移，无需架构修改。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.</description>
      <author>example@mail.com (Yoonpyo Lee, Kazuma Kobayashi, Sai Puppala, Sajedul Talukder, Seid Koric, Souvik Chakraborty, Syed Bahauddin Alam)</author>
      <guid isPermaLink="false">2512.23292v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>MedSAM-based lung masking for multi-label chest X-ray classification</title>
      <link>http://arxiv.org/abs/2512.23089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种分割引导的胸部X光分类流程，整合MedSAM作为肺区域提取模块，用于多标签异常分类。实验表明肺掩模效果既依赖于任务也依赖于架构，应根据临床目标选择合适的空间先验。&lt;h4&gt;背景&lt;/h4&gt;胸部X光成像广泛用于筛查和诊断肺部异常，但自动化解释面临疾病信号弱、数据集偏差和有限空间监督等挑战。&lt;h4&gt;目的&lt;/h4&gt;探索基础模型MedSAM在医学图像分割中的应用，引入基于解剖学的先验知识，提高CXR分析的鲁棒性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出分割引导的CXR分类流程，集成MedSAM作为肺区域提取模块；使用Airlangga大学医院的数据集对MedSAM进行微调；应用在NIH CXR数据集上训练和评估深度卷积神经网络，进行五种异常的多标签预测。&lt;h4&gt;主要发现&lt;/h4&gt;MedSAM能在各种成像条件下生成解剖学合理的肺掩模；在原始图像上训练的ResNet50实现最强异常区分；宽松肺掩模显著改善正常情况识别；紧密掩模提高训练效率但降低异常级别性能；宽松掩模通过保留肺门和周围上下文部分减轻退化。&lt;h4&gt;结论&lt;/h4&gt;肺掩模应被视为可控的空间先验，根据骨干网络和临床目标进行选择，而非统一应用。&lt;h4&gt;翻译&lt;/h4&gt;胸部X光(CXR)成像广泛用于筛查和诊断肺部异常，但由于疾病信号弱、数据集偏差和有限的空间监督，自动化解释仍然具有挑战性。医学图像分割的基础模型(MedSAM)提供了引入基于解剖学先验知识的机会，可能提高CXR分析的鲁棒性和可解释性。我们提出了一种分割引导的CXR分类流程，将MedSAM集成作为多标签异常分类前的肺区域提取模块。使用Airlangga大学医院的公共图像-掩模数据集对MedSAM进行微调。然后将其应用于公共NIH CXR数据集的精选子集，训练和评估深度卷积神经网络，用于五种异常(Mass, Nodule, Pneumonia, Edema和Fibrosis)的多标签预测，并通过派生分数评估正常情况(No Finding)。实验表明，MedSAM能在各种成像条件下生成解剖学合理的肺掩模。我们发现掩模效应既依赖于任务也依赖于架构。在原始图像上训练的ResNet50实现了最强的整体异常区分能力，而宽松的肺掩模提供了相当的宏观AUROC，但显著改善了No Finding的区分，表明在异常特定分类和正常情况筛查之间存在权衡。紧密掩模持续降低异常级别性能但提高训练效率。宽松掩模通过保留肺门和周围上下文部分减轻了这种退化。这些结果表明，肺掩模应被视为一种可控的空间先验，根据骨干网络和临床目标进行选择，而非统一应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chest X-ray (CXR) imaging is widely used for screening and diagnosing pulmonary abnormalities, yet automated interpretation remains challenging due to weak disease signals, dataset bias, and limited spatial supervision. Foundation models for medical image segmentation (MedSAM) provide an opportunity to introduce anatomically grounded priors that may improve robustness and interpretability in CXR analysis. We propose a segmentation-guided CXR classification pipeline that integrates MedSAM as a lung region extraction module prior to multi-label abnormality classification. MedSAM is fine-tuned using a public image-mask dataset from Airlangga University Hospital. We then apply it to a curated subset of the public NIH CXR dataset to train and evaluate deep convolutional neural networks for multi-label prediction of five abnormalities (Mass, Nodule, Pneumonia, Edema, and Fibrosis), with the normal case (No Finding) evaluated via a derived score. Experiments show that MedSAM produces anatomically plausible lung masks across diverse imaging conditions. We find that masking effects are both task-dependent and architecture-dependent. ResNet50 trained on original images achieves the strongest overall abnormality discrimination, while loose lung masking yields comparable macro AUROC but significantly improves No Finding discrimination, indicating a trade-off between abnormality-specific classification and normal case screening. Tight masking consistently reduces abnormality level performance but improves training efficiency. Loose masking partially mitigates this degradation by preserving perihilar and peripheral context. These results suggest that lung masking should be treated as a controllable spatial prior selected to match the backbone and clinical objective, rather than applied uniformly.</description>
      <author>example@mail.com (Brayden Miao, Zain Rehman, Xin Miao, Siming Liu, Jianjie Wang)</author>
      <guid isPermaLink="false">2512.23089v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish</title>
      <link>http://arxiv.org/abs/2512.23065v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 1 figure, 13 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TabiBERT是一种基于ModernBERT架构的单语土耳其语编码器，从零开始在大规模多领域语料库上训练，在计算效率、训练稳定性和长上下文建模方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;自BERT出现以来，仅编码器Transformer模型在计算效率、训练稳定性和长上下文建模方面已显著发展。现代BERT整合了旋转位置编码、FlashAttention和改进的归一化方法，但土耳其NLP领域缺乏采用这些现代架构范式的单语编码器。&lt;h4&gt;目的&lt;/h4&gt;引入TabiBERT，一种基于ModernBERT架构的单语土耳其语编码器，从零开始在大规模精选语料库上训练，以填补土耳其NLP领域的空白。&lt;h4&gt;方法&lt;/h4&gt;在包含848.8亿个标记的多领域语料库上预训练TabiBERT，语料库包括网络文本(73%)、科学出版物(20%)、源代码(6%)和数学内容(0.3%)，采样1万亿个标记。模型支持8,192个标记的上下文长度，使用FlashAttention实现2.65倍推理加速，减少GPU内存消耗。引入TabiBench评估框架，包含8个任务类别下的28个数据集，使用GLUE风格宏平均评估。&lt;h4&gt;主要发现&lt;/h4&gt;TabiBERT在TabiBench上达到77.58分，比BERTurk高出1.62分，在问答(+9.55)、代码检索(+2.41)和文档检索(+0.60)等五个类别建立最先进结果。与专业模型相比，实现+1.47的平均改进，表现出强大的跨领域泛化能力。&lt;h4&gt;结论&lt;/h4&gt;TabiBERT作为土耳其语NLP领域的新基准模型，展示了现代架构在特定语言处理中的有效性，研究团队公开了模型权重、训练配置和评估代码以促进土耳其语编码器研究的透明性和可复现性。&lt;h4&gt;翻译&lt;/h4&gt;自BERT问世以来，仅编码器Transformer模型在计算效率、训练稳定性和长上下文建模方面已取得显著进展。ModernBERT通过整合旋转位置编码、FlashAttention和改进的归一化方法巩固了这些进展。尽管有这些发展，土耳其NLP领域仍缺乏一种从零开始训练并融入此类现代架构范式的单语编码器。本研究介绍了TabiBERT，这是一种基于ModernBERT架构的单语土耳其编码器，在大规模精选语料库上从头开始训练。TabiBERT在包含848.8亿个标记的多领域语料库中采样的1万亿个标记上进行预训练：网络文本(73%)、科学出版物(20%)、源代码(6%)和数学内容(0.3%)。该模型支持8,192个标记的上下文长度(原始BERT的16倍)，实现高达2.65倍的推理加速，并减少GPU内存消耗，使更大的批处理大小成为可能。我们引入了TabiBench，包含八个任务类别中的28个数据集，使用标准化的分割和协议，采用GLUE风格的宏平均进行评估。TabiBERT在TabiBench上达到77.58分，比BERTurk高出1.62分，并在八个类别中的五个类别上建立最先进水平：问答(+9.55)、代码检索(+2.41)和文档检索(+0.60)。与包括TurkishBERTweet等专用模型在内的任务特定先前最佳结果相比，TabiBERT实现了+1.47的平均改进，表明其具有强大的跨领域泛化能力。我们发布了模型权重、训练配置和评估代码，以促进透明且可复现的土耳其语编码器研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73%), scientific publications (20%), source code (6%), and mathematical content (0.3%). The model supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories: question answering (+9.55), code retrieval (+2.41), and document retrieval (+0.60). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.</description>
      <author>example@mail.com (Melikşah Türker, A. Ebrar Kızıloğlu, Onur Güngör, Susan Üsküdarlı)</author>
      <guid isPermaLink="false">2512.23065v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion</title>
      <link>http://arxiv.org/abs/2512.23035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 5 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Co2S的稳定半监督遥感图像语义分割框架，通过协同融合视觉语言模型和自监督模型的先验知识来解决伪标签漂移问题。&lt;h4&gt;背景&lt;/h4&gt;半监督遥感图像语义分割虽能减轻大量标注负担，但存在伪标签漂移现象，即确认偏差导致训练过程中错误累积的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个稳定的半监督遥感图像分割框架，有效缓解伪标签漂移现象，提高分割精度。&lt;h4&gt;方法&lt;/h4&gt;构建异构双学生架构，包含两个基于ViT的视觉基础模型(分别用CLIP和DINOv3初始化)；引入显式-隐式语义共引导机制，利用文本嵌入和可学习查询提供类别级引导；开发全局-局部特征协同融合策略，融合CLIP的全局上下文和DINOv3的局部细节。&lt;h4&gt;主要发现&lt;/h4&gt;在六个流行数据集上的实验表明，所提出的方法在各种分区协议和不同场景下均取得了领先的性能表现。&lt;h4&gt;结论&lt;/h4&gt;Co2S框架通过协同融合不同视觉模型的先验知识，有效解决了半监督遥感图像分割中的伪标签漂移问题，显著提高了分割精度。&lt;h4&gt;翻译&lt;/h4&gt;半监督遥感(RS)图像语义分割提供了一种有前景的解决方案，可以减轻大量标注的负担，但它从根本上受到伪标签漂移的困扰，这是一种确认偏差导致训练过程中错误累积的现象。在这项工作中，我们提出了Co2S，一个稳定的半监督RS分割框架，协同融合了视觉语言模型和自监督模型的先验知识。具体来说，我们构建了一个异构双学生架构，包含两个不同的基于ViT的视觉基础模型，分别用预训练的CLIP和DINOv3初始化，以减少错误累积和伪标签漂移。为了有效融合这些不同的先验知识，我们引入了一种显式-隐式语义共引导机制，分别利用文本嵌入和可学习查询提供显式和隐式的类别级引导，从而共同增强语义一致性。此外，还开发了一种全局-局部特征协同融合策略，有效融合CLIP捕获的全局上下文信息和DINOv3产生的局部细节，使模型能够生成高精度的分割结果。在六个流行数据集上的大量实验证明了所提出方法的优越性，在各种分区协议和不同场景下均持续取得领先性能。项目页面可在 https://xavierjiezou.github.io/Co2S/ 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.</description>
      <author>example@mail.com (Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi)</author>
      <guid isPermaLink="false">2512.23035v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery</title>
      <link>http://arxiv.org/abs/2512.22899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队引入了HiSciBench，一个分层的科学智能评估基准，用于全面评估基础模型在科学工作流不同层次的能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和多模态基础模型的快速发展引发了其在科学研究中应用的兴趣，但科学智能涵盖广泛的能力，现有基准测试仍然碎片化，大多只关注狭窄任务，未能反映科学探究的层次性和多学科性质。&lt;h4&gt;目的&lt;/h4&gt;创建一个分层基准测试，能够全面评估基础模型在科学工作流不同层次的能力，从基础理解到创造性发现。&lt;h4&gt;方法&lt;/h4&gt;HiSciBench包含5个层次对应完整科学工作流：科学素养(L1)、文献解析(L2)、基于文献的问答(L3)、文献综述生成(L4)和科学发现(L5)。基准包含8,735个精心策划的实例，涵盖六个主要科学学科，支持多模态输入和跨语言评估，并提供依赖感知框架进行详细诊断。&lt;h4&gt;主要发现&lt;/h4&gt;对领先模型的评估显示显著性能差距：模型在基础素养任务上准确率可达69%，但在发现级挑战中性能急剧下降至25%。&lt;h4&gt;结论&lt;/h4&gt;HiSciBench为科学智能评估建立了新标准，并为开发更强大、更可靠的模型提供了可操作的见解。该基准将公开发布以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型和多模态基础模型的快速发展引发了其在科学研究中应用的潜力。然而，科学智能涵盖从理解基础知识到进行创造性发现的广泛能力范围，现有基准测试仍然碎片化。大多数基准测试只关注狭窄任务，未能反映科学探究的层次性和多学科性质。我们引入了HiSciBench，一个分层基准测试，用于评估基础模型在五个层次的能力，这些层次对应完整的科学工作流：科学素养、文献解析、基于文献的问答、文献综述生成和科学发现。HiSciBench包含8,735个精心策划的实例，涵盖六个主要科学学科，包括数学、物理、化学、生物学、地理学和天文学，并支持文本、方程、图形和表格等多模态输入，以及跨语言评估。与评估孤立能力的先前基准不同，HiSciBench提供了一个集成、依赖感知的框架，能够详细诊断模型在科学推理不同阶段的能力。对领先模型的全面评估，包括GPT-5、DeepSeek-R1和几个多模态系统，揭示了显著的性能差距：虽然模型在基础素养任务上可实现高达69%的准确率，但在发现级挑战中性能急剧下降至25%。HiSciBench为科学智能评估建立了新标准，并为开发不仅更强大而且更可靠的模型提供了可操作的见解。该基准将公开发布以促进未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.</description>
      <author>example@mail.com (Yaping Zhang, Qixuan Zhang, Xingquan Zhang, Zhiyuan Chen, Wenwen Zhuang, Yupu Liang, Lu Xiang, Yang Zhao, Jiajun Zhang, Yu Zhou, Chengqing Zong)</author>
      <guid isPermaLink="false">2512.22899v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations</title>
      <link>http://arxiv.org/abs/2512.22883v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基于基础模型的人工智能正在从根本上重塑网络安全领域，大语言模型实现了自主规划、工具编排和大规模战略适应，挑战了传统静态安全架构，促使网络安全从以预防为中心转向代理网络弹性。&lt;h4&gt;背景&lt;/h4&gt;传统安全架构建立在静态规则、边界防御和以人为中心的工作流程基础上，而大语言模型等基础模型AI技术正在改变这一格局，使系统能够实现大规模自主决策和适应。&lt;h4&gt;目的&lt;/h4&gt;推动网络安全范式从以预防为中心转向代理网络弹性，构建能够预期中断、在攻击下维持关键功能、高效恢复并持续学习的弹性系统。&lt;h4&gt;方法&lt;/h4&gt;通过历史演变分析定位这一转变，开发系统级框架设计代理AI工作流，引入通用代理架构，将攻击者和防御者工作流分析为耦合的自适应过程，并使用博弈论公式作为统一设计语言。&lt;h4&gt;主要发现&lt;/h4&gt;博弈论公式为自主分配、信息流和时间组成提供了统一的设计语言，自动化渗透测试、修复和网络欺骗的案例研究表明基于均衡的设计可实现系统级弹性。&lt;h4&gt;结论&lt;/h4&gt;网络安全已进入AI增强的新范式，其中自主代理直接参与网络和网络物理系统的感知、推理、行动和适应过程。&lt;h4&gt;翻译&lt;/h4&gt;网络安全正基于基础模型的人工智能被根本性地重塑。大语言模型现在能够实现大规模的自主规划、工具编排和战略适应，挑战了建立在静态规则、边界防御和以人为中心的工作流程上的安全架构。本章主张从以预防为中心的安全转向代理网络弹性。弹性系统必须预期中断、在攻击下维持关键功能、高效恢复并持续学习，而非寻求完美保护。我们将这种转变置于网络安全范式的历史演变中，最终形成一个AI增强的范式，其中自主代理直接参与网络和网络物理系统的感知、推理、行动和适应。然后，我们开发了一个用于设计代理AI工作流的系统级框架。介绍了通用代理架构，并将攻击者和防御者工作流分析为耦合的自适应过程，博弈论公式被证明可以为自主分配、信息流和时间组成提供统一的设计语言。自动化渗透测试、修复和网络欺骗的案例研究说明了基于均衡的设计如何实现系统级弹性设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.</description>
      <author>example@mail.com (Tao Li, Quanyan Zhu)</author>
      <guid isPermaLink="false">2512.22883v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages</title>
      <link>http://arxiv.org/abs/2512.22705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and presented at the 15th International Arab Conference on Information Technology (ICAIT); proceedings not yet published&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个多语言希望言语检测框架，特别关注乌尔都语，使用预训练transformer模型进行分类，并在多语言基准测试中取得了良好性能。&lt;h4&gt;背景&lt;/h4&gt;希望言语在自然语言处理中相对未被充分研究，现有研究主要集中在英语上，导致低资源语言如乌尔都语缺乏相关资源。虽然基于transformer的架构在检测仇恨和冒犯性言语方面已被证明有效，但很少将其应用于希望言语检测或在不同语言环境中进行测试。&lt;h4&gt;目的&lt;/h4&gt;开发一个多语言希望言语检测框架，特别关注乌尔都语，以促进积极在线沟通并构建更有建设性的数字对话。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的transformer模型(如XLM-RoBERTa、mBERT、EuroBERT和UrduBERT)，应用简单的预处理技术，并训练分类器以获得改进的结果。&lt;h4&gt;主要发现&lt;/h4&gt;在PolyHope-M 2025基准测试上评估显示，乌尔都语二元分类的F1分数达到95.2%，乌尔都语多类分类的F1分数达到65.2%，在西班牙语、德语和英语中也取得了具有竞争力的相似结果。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，在低资源环境中实施现有的多语言模型是可行的，这使得识别希望言语变得更加容易，有助于构建更有建设性的数字对话。&lt;h4&gt;翻译&lt;/h4&gt;希望言语在自然语言处理(NLP)中相对未被充分研究。现有研究主要集中在英语上，这导致低资源语言(如乌尔都语)缺乏相关资源。因此，促进积极在线沟通的工具开发仍然有限。虽然基于transformer的架构已被证明在检测仇恨和冒犯性言语方面有效，但在将其应用于希望言语或更广泛地在各种语言环境中测试方面所做的努力很少。本文提出了一个多语言希望言语检测框架，特别关注乌尔都语。使用XLM-RoBERTa、mBERT、EuroBERT和UrduBERT等预训练transformer模型，我们应用简单的预处理并训练分类器以获得改进的结果。在PolyHope-M 2025基准测试上的评估表明性能强劲，乌尔都语二元分类的F1分数达到95.2%，乌尔都语多类分类的F1分数达到65.2%，在西班牙语、德语和英语中也取得了同样具有竞争力的结果。这些结果突显了在低资源环境中实施现有多语言模型的可能性，从而更容易识别希望言语，并帮助构建更有建设性的数字对话。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hope speech has been relatively underrepresented in Natural Language Processing (NLP). Current studies are largely focused on English, which has resulted in a lack of resources for low-resource languages such as Urdu. As a result, the creation of tools that facilitate positive online communication remains limited. Although transformer-based architectures have proven to be effective in detecting hate and offensive speech, little has been done to apply them to hope speech or, more generally, to test them across a variety of linguistic settings. This paper presents a multilingual framework for hope speech detection with a focus on Urdu. Using pretrained transformer models such as XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, we apply simple preprocessing and train classifiers for improved results. Evaluations on the PolyHope-M 2025 benchmark demonstrate strong performance, achieving F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification, with similarly competitive results in Spanish, German, and English. These results highlight the possibility of implementing existing multilingual models in low-resource environments, thus making it easier to identify hope speech and helping to build a more constructive digital discourse.</description>
      <author>example@mail.com (Ahmed Abdullah, Sana Fatima, Haroon Mahmood)</author>
      <guid isPermaLink="false">2512.22705v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading</title>
      <link>http://arxiv.org/abs/2512.22666v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究者提出INTERACT-CMIL，一个多头深度学习框架，用于结膜黑色素细胞上皮内病变的准确分级，通过联合预测五个组织病理学轴，在多中心数据集上实现了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;结膜黑色素细胞上皮内病变的准确分级对治疗和黑色素瘤预测至关重要，但由于形态学线索微妙和诊断标准相互关联，准确分级仍然存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够准确分级CMIL的深度学习框架，提供连贯、可解释的多标准预测，与专家分级保持一致，并为CMIL诊断提供可重现的计算基准。&lt;h4&gt;方法&lt;/h4&gt;提出INTERACT-CMIL，一个多头深度学习框架，通过共享特征学习和组合部分监督以及相互依赖损失函数来联合预测五个组织病理学轴。该框架在包含486个专家注释的结膜活检组织块的多中心数据集上进行训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;INTERACT-CMIL实现了比CNN和基础模型基线一致的改进，相对宏观F1增益最高达到55.1%(WHO4)和25.0%(垂直扩散)，提供了与专家分级一致的可解释多标准预测。&lt;h4&gt;结论&lt;/h4&gt;INTERACT-CMIL为CMIL诊断提供了可重现的计算基准，并向标准化数字眼科病理学迈出了一步。&lt;h4&gt;翻译&lt;/h4&gt;准确的结膜黑色素细胞上皮内病变分级对于治疗和黑色素瘤预测至关重要，但由于形态学线索微妙和诊断标准相互关联，这一过程仍然存在困难。我们引入了INTERACT-CMIL，一个多头深度学习框架，通过共享特征学习和组合部分监督以及相互依赖损失函数，联合预测五个组织病理学轴：WHO4、WHO5、水平扩散、垂直扩散和细胞学异型性。在来自三家大学医院的486个专家注释的结膜活检组织块的新构建多中心数据集上进行训练和评估后，INTERACT-CMIL实现了比CNN和基础模型基线一致的改进，相对宏观F1增益最高达到55.1%(WHO4)和25.0%(垂直扩散)。该框架提供了与专家分级一致的可解释、连贯的多标准预测，为CMIL诊断提供了可重现的计算基准，并向标准化数字眼科病理学迈出了一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL) is essential for treatment and melanoma prediction but remains difficult due to subtle morphological cues and interrelated diagnostic criteria. We introduce INTERACT-CMIL, a multi-head deep learning framework that jointly predicts five histopathological axes; WHO4, WHO5, horizontal spread, vertical spread, and cytologic atypia, through Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss enforcing cross-task consistency. Trained and evaluated on a newly curated, multi-center dataset of 486 expert-annotated conjunctival biopsy patches from three university hospitals, INTERACT-CMIL achieves consistent improvements over CNN and foundation-model (FM) baselines, with relative macro F1 gains up to 55.1% (WHO4) and 25.0% (vertical spread). The framework provides coherent, interpretable multi-criteria predictions aligned with expert grading, offering a reproducible computational benchmark for CMIL diagnosis and a step toward standardized digital ocular pathology.</description>
      <author>example@mail.com (Mert Ikinci, Luna Toma, Karin U. Loeffler, Leticia Ussem, Daniela Süsskind, Julia M. Weller, Yousef Yeganeh, Martina C. Herwig-Carl, Shadi Albarqouni)</author>
      <guid isPermaLink="false">2512.22666v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI</title>
      <link>http://arxiv.org/abs/2512.22568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文讨论了基础模型(如大型语言模型)的当前局限性，并提出通过整合行动、分层组合结构和情节记忆来改进这些模型，以实现更安全、可解释、节能且类人的人工智能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和其他基础模型最近取得了显著进展，这些进展主要是通过最小化下一个token预测损失来优化大规模transformer模型实现的，这是一种预测编码的形式，也是神经科学和认知科学中日益流行的脑功能模型的基础。&lt;h4&gt;目的&lt;/h4&gt;为了实现安全、可解释、节能且类人的人工智能，基础模型应该在不同抽象层次上整合行动、组合生成架构和情节记忆。&lt;h4&gt;方法&lt;/h4&gt;作者提出通过添加三个缺失的组件来改进基础模型：行动与生成模型的紧密集成、分层组合结构和情节记忆。他们引用了神经科学和认知科学的证据支持这些组件的重要性，并讨论了如何将这些组件整合到现有模型中。&lt;h4&gt;主要发现&lt;/h4&gt;当前基础模型存在几个关键缺陷，包括由于缺乏基础导致的幻觉和对概念的肤浅理解、由于缺乏控制导致的代理/责任感缺失、由于缺乏可解释性对安全和可信度的威胁，以及能源效率低下。添加行动、分层组合结构和情节记忆可以解决这些问题。&lt;h4&gt;结论&lt;/h4&gt;重新点燃脑科学与人工智能之间历史上富有成效的思想交流，将为安全和可解释的以人为本的人工智能铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;过去几年中，大型语言模型和其他基础模型的显著进步是基于最小化下一个token预测损失这一令人惊讶的简单目标来优化大规模transformer模型实现的，这是一种预测编码形式，也是神经科学和认知科学中日益流行的脑功能模型的基础。然而，当前基础模型忽略了最先进的预测编码模型的三个其他重要组成部分：行动与生成模型的紧密集成、分层组合结构和情节记忆。我们提出，为了实现安全、可解释、节能且类人的人工智能，基础模型应该在不同抽象层次上整合行动、组合生成架构和情节记忆。我们展示了神经科学和认知科学关于每个组件重要性的最新证据。我们描述了将这些缺失组件添加到基础模型中如何帮助解决一些当前缺陷：由于缺乏基础导致的幻觉和对概念的肤浅理解、由于缺乏控制导致的代理/责任感缺失、由于缺乏可解释性对安全和可信度的威胁，以及能源效率低下。我们将我们的提议与当前趋势进行了比较，例如为基础模型添加思维链推理和检索增强生成，并讨论了用大脑启发的组件增强这些模型的新方法。我们最后认为，重新点燃脑科学与人工智能之间历史上富有成效的思想交流，将为安全和可解释的以人为本的人工智能铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.</description>
      <author>example@mail.com (Rajesh P. N. Rao, Vishwas Sathish, Linxing Preston Jiang, Matthew Bryan, Prashant Rangarajan)</author>
      <guid isPermaLink="false">2512.22568v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>SAM 3D for 3D Object Reconstruction from Remote Sensing Images</title>
      <link>http://arxiv.org/abs/2512.22452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了SAM 3D基础模型在单目遥感建筑物重建中的应用，并与TRELLIS方法进行了比较，展示了其在城市场景重建中的潜力&lt;h4&gt;背景&lt;/h4&gt;单目遥感图像中的3D建筑物重建对于可扩展的城市建模至关重要，但现有方法通常需要特定任务架构和密集监督&lt;h4&gt;目的&lt;/h4&gt;对SAM 3D这个通用图像到3D基础模型进行单目遥感建筑物重建的系统评估，并探索其在城市场景重建中的潜力&lt;h4&gt;方法&lt;/h4&gt;在纽约城市数据集样本上对SAM 3D与TRELLIS进行基准测试，使用Frechet Inception Distance和基于CLIP的最大均值差异作为评估指标，并通过分割-重建-组合流程将SAM 3D扩展到城市场景重建&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与TRELLIS相比，SAM 3D能够产生更连贯的屋顶几何形状和更清晰的边界，在城市场景建模中展现出良好潜力&lt;h4&gt;结论&lt;/h4&gt;这些发现为基础模型在城市3D重建中的部署提供了实际指导，并激励未来整合场景级别结构先验的研究&lt;h4&gt;翻译&lt;/h4&gt;从遥感图像中进行单目3D建筑物重建对于可扩展的城市建模至关重要，然而现有方法通常需要特定任务架构和密集监督。本文首次对SAM 3D（一种通用图像到3D基础模型）进行了系统评估，用于单目遥感建筑物重建。我们在纽约城市数据集的样本上对SAM 3D与TRELLIS进行了基准测试，采用Frechet Inception Distance和基于CLIP的最大均值差异作为评估指标。实验结果表明，与TRELLIS相比，SAM 3D能够产生更连贯的屋顶几何形状和更清晰的边界。我们进一步通过分割-重建-组合流程将SAM 3D扩展到城市场景重建中，展示了其在城市场景建模中的潜力。我们还分析了实际局限性并讨论了未来研究方向。这些发现为基础模型在城市3D重建中的部署提供了实际指导，并激励未来整合场景级别结构先验的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.</description>
      <author>example@mail.com (Junsheng Yao, Lichao Mou, Qingyu Li)</author>
      <guid isPermaLink="false">2512.22452v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy</title>
      <link>http://arxiv.org/abs/2512.22423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Bright-4B是一种40亿参数的基础模型，能够在无需荧光标记的情况下直接从3D明场体积中分割亚细胞结构，解决了传统方法依赖荧光或复杂后处理的局限性。&lt;h4&gt;背景&lt;/h4&gt;无标记3D明场显微镜是一种快速和非侵入性的可视化细胞形态的方法，但稳健的体积分割通常仍依赖于荧光或复杂的后处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接从3D明场体积中分割亚细胞结构的模型，解决无标记3D明场显微镜中稳健体积分割的挑战。&lt;h4&gt;方法&lt;/h4&gt;Bright-4B结合了硬件对齐的原生稀疏注意力机制、深度-宽度残差超连接、软混合专家以及即插即用的各向异性嵌入，在单位超球面上学习，实现几何保真的3D标记化。&lt;h4&gt;主要发现&lt;/h4&gt;Bright-4B仅从明场堆栈中就能产生核、线粒体和其他细胞器的形态准确的分割结果，不需要荧光、辅助通道或手工后处理；在多个共聚焦数据集上保留了跨深度和细胞类型的精细结构细节，优于当代CNN和Transformer基线模型。&lt;h4&gt;结论&lt;/h4&gt;Bright-4B模型解决了无标记3D明场显微镜中稳健体积分割的挑战，将推动大规模无标记3D细胞图谱绘制的发展。&lt;h4&gt;翻译&lt;/h4&gt;无标记3D明场显微镜提供了一种快速和非侵入性的可视化细胞形态的方法，然而稳健的体积分割通常仍依赖于荧光或复杂的后处理。我们通过引入Bright-4B来解决这一差距，这是一个拥有40亿参数的基础模型，它在单位超球面上学习，直接从3D明场体积中分割亚细胞结构。Bright-4B结合了硬件对齐的原生稀疏注意力机制(捕获局部、粗粒度和选定全局上下文)，深度-宽度残差超连接稳定表示流，以及自适应容量的软混合专家。即插即用的各向异性嵌入进一步尊重共聚焦点扩散函数和轴向变薄，实现了几何保真的3D标记化。由此产生的模型仅从明场堆栈中就能产生核、线粒体和其他细胞器的形态准确的分割结果——不需要荧光、辅助通道或手工后处理。在多个共聚焦数据集上，Bright-4B保留了跨深度和细胞类型的精细结构细节，优于当代CNN和Transformer基线模型。所有代码、预训练权重和用于下游微调的模型都将发布，以推动大规模无标记3D细胞图谱绘制。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从3D明场显微镜图像中直接分割亚细胞结构的问题，而不需要荧光标记或复杂的后处理。这个问题很重要，因为明场显微镜是一种快速、非侵入性的细胞成像方法，但传统上难以从中获得准确的分割结果。解决这个问题将使研究人员能够在不干扰细胞的情况下观察细胞结构和动态，对活细胞研究至关重要，并能大大提高处理大量图像数据的效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D明场显微镜的特点：轴向各向异性、深度依赖的对比度变化和低对比度信号。他们指出现有方法（如CNN和Transformer）在处理这些特性时存在局限性。作者借鉴了多种现有技术，包括Native Sparse Attention、HyperConnections和Mixture-of-Experts，但针对明场显微镜的特殊需求进行了创新设计。他们提出在单位超球面上进行学习以提高数值稳定性，使用各向异性3D块嵌入尊重显微镜的物理特性，并结合稀疏注意力和软混合专家来处理明场数据的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在单位超球面上进行学习，使所有令牌状态和投影都进行单位归一化，从而提高大规模模型的数值稳定性。整体流程包括：1) 使用各向异性3D块嵌入处理输入图像，只在z轴上进行抗混滤波；2) 通过本机稀疏注意力机制捕获局部、粗粒度和全局上下文；3) 使用软混合专家进行自适应容量处理；4) 通过动态超连接稳定表示流；5) 最后使用轻量级解码器生成全分辨率的亚细胞结构掩码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 各向异性3D块嵌入，与显微镜PSF对齐；2) 在单位超球面上学习，提高数值稳定性；3) 本机稀疏注意力机制，结合三种信息路径；4) 软混合专家，实现稳定的专家专门化；5) 动态超连接，自适应混合残差流。相比之前的工作，Bright 4B是一个专门为明场显微镜设计的40亿参数基础模型，直接从明场图像预测分割结果，不需要荧光标记或中间步骤，同时通过创新技术处理了明场数据的特殊挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Bright 4B是一种40亿参数的基础模型，通过在单位超球面上学习并结合各向异性嵌入、稀疏注意力和软混合专家，实现了从3D明场显微镜图像中直接分割亚细胞结构，无需荧光标记或复杂后处理。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Label-free 3D brightfield microscopy offers a fast and noninvasive way to visualize cellular morphology, yet robust volumetric segmentation still typically depends on fluorescence or heavy post-processing. We address this gap by introducing Bright-4B, a 4 billion parameter foundation model that learns on the unit hypersphere to segment subcellular structures directly from 3D brightfield volumes. Bright-4B combines a hardware-aligned Native Sparse Attention mechanism (capturing local, coarse, and selected global context), depth-width residual HyperConnections that stabilize representation flow, and a soft Mixture-of-Experts for adaptive capacity. A plug-and-play anisotropic patch embed further respects confocal point-spread and axial thinning, enabling geometry-faithful 3D tokenization. The resulting model produces morphology-accurate segmentations of nuclei, mitochondria, and other organelles from brightfield stacks alone--without fluorescence, auxiliary channels, or handcrafted post-processing. Across multiple confocal datasets, Bright-4B preserves fine structural detail across depth and cell types, outperforming contemporary CNN and Transformer baselines. All code, pretrained weights, and models for downstream finetuning will be released to advance large-scale, label-free 3D cell mapping.</description>
      <author>example@mail.com (Amil Khan, Matheus Palhares Viana, Suraj Mishra, B. S. Manjunath)</author>
      <guid isPermaLink="false">2512.22423v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence</title>
      <link>http://arxiv.org/abs/2512.22334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SciEvalKit是一个用于评估科学AI模型的统一基准测试工具包，专注于科学智能的核心能力，支持多个科学领域，提供灵活可扩展的评估流程，是开源且积极维护的。&lt;h4&gt;背景&lt;/h4&gt;现有通用评估平台无法满足科学AI模型的特殊评估需求，需要专门针对科学智能核心能力的评估工具。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的基准测试工具包，用于评估AI模型在科学领域的核心能力和跨学科表现。&lt;h4&gt;方法&lt;/h4&gt;构建专家级科学基准，使用真实世界领域特定数据集，设计灵活可扩展的评估流程，支持批量评估和自定义集成。&lt;h4&gt;主要发现&lt;/h4&gt;SciEvalKit能够有效评估科学AI模型在多个领域和任务类型上的表现，提供透明、可复制和可比较的结果。&lt;h4&gt;结论&lt;/h4&gt;SciEvalKit为科学基础模型和智能体的评估提供了标准化且可定制的基础设施，通过开源和社区维护促进AI4Science的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了SciEvalKit，这是一个统一的基准测试工具包，旨在评估跨广泛科学学科和任务能力的科学AI模型。与通用评估平台不同，SciEvalKit专注于科学智能的核心能力，包括科学多模态感知、科学多模态推理、科学多模态理解、科学符号推理、科学代码生成、科学假设生成和科学知识理解。它支持六个主要科学领域，从物理和化学到天文学和材料科学。SciEvalKit建立在专家级科学基准的基础上，这些基准来自真实世界的领域特定数据集，确保任务反映真实的科学挑战。该工具包具有灵活、可扩展的评估流程，支持跨模型和数据集的批量评估，支持自定义模型和数据集集成，并提供透明、可复制和可比较的结果。通过将基于能力的评估与学科多样性相结合，SciEvalKit提供了一个标准化且可定制的基准测试基础设施，用于评估下一代科学基础模型和智能体。该工具包是开源的，并积极维护，以促进AI4Science的社区驱动发展和进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.</description>
      <author>example@mail.com (Yiheng Wang, Yixin Chen, Shuo Li, Yifan Zhou, Bo Liu, Hengjian Gao, Jiakang Yuan, Jia Bu, Wanghan Xu, Yuhao Zhou, Xiangyu Zhao, Zhiwang Zhou, Fengxiang Wang, Haodong Duan, Songyang Zhang, Jun Yao, Han Deng, Yizhou Wang, Jiabei Xiao, Jiaqi Liu, Encheng Su, Yujie Liu, Weida Wang, Junchi Yao, Shenghe Zheng, Haoran Sun, Runmin Ma, Xiangchao Yan, Bo Zhang, Dongzhan Zhou, Shufei Zhang, Peng Ye, Xiaosong Wang, Shixiang Tang, Wenlong Zhang, Lei Bai)</author>
      <guid isPermaLink="false">2512.22334v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</title>
      <link>http://arxiv.org/abs/2512.23617v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Le Cam Distortion框架，基于Le Cam统计实验理论，通过学习从源域到目标域的核函数实现转移学习，避免了传统无监督域适应方法中的负迁移问题&lt;h4&gt;背景&lt;/h4&gt;分布偏移是现实世界机器学习的定义性挑战。主流的无监督域适应(UDA)方法通过强制特征不变性来对齐源域和目标域表示，但当域的信息量不均衡时，严格的不变性会导致信息破坏，引起'负迁移'&lt;h4&gt;目的&lt;/h4&gt;提出决策论框架，用构造性近似替代对称不变性，实现方向可模拟性，并引入Le Cam失真作为可模拟条件下转移风险的严格上界&lt;h4&gt;方法&lt;/h4&gt;基于Le Cam统计实验理论构建决策论框架，学习一个从源域模拟目标域的核函数，实现不降低源域效用的转移学习&lt;h4&gt;主要发现&lt;/h4&gt;在五个实验中：(1)HLA基因组学中实现近乎完美的频率估计(r=0.999)；(2)CIFAR-10图像分类中保持81.2%准确率(无源效用损失)，而CycleGAN下降34.7%；(3)在强化学习控制中实现安全策略转移&lt;h4&gt;结论&lt;/h4&gt;Le Cam Distortion为负迁移不可接受的领域(医学成像、自主系统和精准医疗)提供了首个有原则的风险控制转移学习框架&lt;h4&gt;翻译&lt;/h4&gt;分布偏移是现实世界机器学习的定义性挑战。主导范式——无监督域适应(UDA)——强制特征不变性，通过对称散度最小化来对齐源域和目标域表示[Ganin等人，2016]。我们证明这种方法存在根本性缺陷：当域的信息量不均衡时，严格的不变性需要破坏信息，导致'负迁移'，这在安全关键应用中可能是灾难性的[Wang等人，2019]。我们提出了基于Le Cam统计实验理论的决策论框架，使用构造性近似来替代对称不变性，实现方向可模拟性。我们引入Le Cam失真，用缺陷距离δ(E₁, E₂)量化，作为可模拟条件下转移风险的严格上界。我们的框架通过学习一个从源域模拟目标域的核函数，实现了不降低源域效用的转移学习。在五个实验中，Le Cam失真实现了：(1)在HLA基因组学中近乎完美的频率估计；(2)在CIFAR-10图像分类中零源效用损失；(3)在强化学习控制中的安全策略转移。Le Cam失真为负迁移不可接受的领域提供了第一个有原则的风险控制转移学习框架&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing "negative transfer" that can be catastrophic in safety-critical applications [Wang et al., 2019].  We propose a decision-theoretic framework grounded in Le Cam's theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.</description>
      <author>example@mail.com (Deniz Akdemir)</author>
      <guid isPermaLink="false">2512.23617v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning for Detection of Rare Driving Scenarios</title>
      <link>http://arxiv.org/abs/2512.23585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于无监督学习的框架，用于检测稀有和危险的驾驶场景，使用深度隔离森林算法处理自然驾驶数据，有效识别复杂异常，为自动驾驶系统安全提供可扩展解决方案。&lt;h4&gt;背景&lt;/h4&gt;检测稀有和危险的驾驶场景是确保自主系统安全性和可靠性的关键挑战，研究使用自然驾驶数据(NDD)来应对这一挑战。&lt;h4&gt;目的&lt;/h4&gt;探索一种无监督学习框架，用于检测稀有和极端驾驶场景，提高自动驾驶系统的安全性。&lt;h4&gt;方法&lt;/h4&gt;采用深度隔离森林(DIF)异常检测算法，结合神经网络特征表示与隔离森林识别非线性和复杂异常；将感知模块数据预处理为滑动窗口提取的结构化统计特征；使用t-SNE进行降维和可视化以提高可解释性；通过代理真实值结合定量指标和定性视频帧检查进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架能有效识别稀有和危险的驾驶场景，为自动驾驶系统中的异常检测提供了可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;研究方法不可避免地依赖代理真实值和手动定义的特征组合，无法涵盖真实世界驾驶异常的全部范围或其细微的上下文依赖关系。&lt;h4&gt;翻译&lt;/h4&gt;稀有和危险驾驶场景的检测是确保自主系统安全性和可靠性的关键挑战。本研究探索了一种使用自然驾驶数据(NDD)检测稀有和极端驾驶场景的无监督学习框架。我们利用最近提出的深度隔离森林(DIF)异常检测算法，该算法结合了基于神经网络的特征表示与隔离森林(IFs)，以识别非线性和复杂异常。来自感知模块的数据(捕获车辆动力学和环境条件)被预处理为从滑动窗口中提取的结构化统计特征。该框架结合了t-SNE进行降维和可视化，使检测到的异常更具可解释性。评估使用代理真实值进行，结合定量指标和定性视频帧检查。我们的结果表明，所提出的方法能有效识别稀有和危险的驾驶场景，为自动驾驶系统中的异常检测提供了可扩展的解决方案。鉴于研究的方法，不可避免地依赖代理真实值和手动定义的特征组合，这些方法无法涵盖真实世界驾驶异常的全部范围或其细微的上下文依赖关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The detection of rare and hazardous driving scenarios is a critical challenge for ensuring the safety and reliability of autonomous systems. This research explores an unsupervised learning framework for detecting rare and extreme driving scenarios using naturalistic driving data (NDD). We leverage the recently proposed Deep Isolation Forest (DIF), an anomaly detection algorithm that combines neural network-based feature representations with Isolation Forests (IFs), to identify non-linear and complex anomalies. Data from perception modules, capturing vehicle dynamics and environmental conditions, is preprocessed into structured statistical features extracted from sliding windows. The framework incorporates t-distributed stochastic neighbor embedding (t-SNE) for dimensionality reduction and visualization, enabling better interpretability of detected anomalies. Evaluation is conducted using a proxy ground truth, combining quantitative metrics with qualitative video frame inspection. Our results demonstrate that the proposed approach effectively identifies rare and hazardous driving scenarios, providing a scalable solution for anomaly detection in autonomous driving systems. Given the study's methodology, it was unavoidable to depend on proxy ground truth and manually defined feature combinations, which do not encompass the full range of real-world driving anomalies or their nuanced contextual dependencies.</description>
      <author>example@mail.com (Dat Le, Thomas Manhardt, Moritz Venator, Johannes Betz)</author>
      <guid isPermaLink="false">2512.23585v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>A NEAT Approach to Evolving Neural-Network-based Optimization of Chiral Photonic Metasurfaces: Application of a Neuro-Evolution Pipeline</title>
      <link>http://arxiv.org/abs/2512.23558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究将神经进化增强拓扑(NEAT)算法集成到深度学习优化框架中，用于电介质手性超表面的设计，实现了无需手动调整的任务特定架构，并展示了与现有方法相当或更好的性能。&lt;h4&gt;背景&lt;/h4&gt;手性超表面的设计在纳米光子学中是一个核心挑战，因为几何结构与手性光学响应之间存在高度非线性关系。机器学习辅助的优化流程虽能加速这一过程，但其性能很大程度上取决于神经网络架构的选择。&lt;h4&gt;目的&lt;/h4&gt;将NEAT算法集成到现有的深度学习优化框架中，用于电介质手性超表面的设计，实现自适应、自配置的机器学习框架。&lt;h4&gt;方法&lt;/h4&gt;使用NEAT算法自主进化网络拓扑和连接权重，结合强化学习策略进化解空间知识并并行微调模型权重。使用9600个模拟GaP超表面几何形状的数据集，在不同输入维度、特征缩放方法和数据大小下评估NEAT性能。&lt;h4&gt;主要发现&lt;/h4&gt;标准化特征缩放提供了最一致的性能；相对紧凑的NEAT进化NN模型实现了与密集少层感知器相当或更好的预测准确性和泛化能力；这些模型成功推理可见光谱中强圆二色性超表面，并支持模拟与实验数据间的迁移学习。&lt;h4&gt;结论&lt;/h4&gt;该方法为自适应、自配置的机器学习框架提供了一条可扩展的路径，可用于自动化的光子设计，既可以独立使用，也可以作为智能人工智能的构建块。&lt;h4&gt;翻译&lt;/h4&gt;由于几何结构与手性光学响应之间的高度非线性关系，设计具有定制光学性质的手性超表面在纳米光子学中仍然是一个核心挑战。机器学习辅助的优化流程最近已成为加速这一过程的有效工具，但其性能在很大程度上取决于神经网络架构的选择。在这项工作中，我们将神经进化增强拓扑(NEAT)算法集成到一个现有的深度学习优化框架中，用于电介质手性超表面。NEAT自主进化网络拓扑和连接权重，实现无需手动调整的任务特定架构，而框架中的强化学习策略并行进化解空间知识和微调模型权重。使用包含9600个模拟GaP超表面几何形状的管道生成数据集，我们在不同输入维度、特征缩放方法和数据大小下评估了NEAT。对于检查的输出维度，标准化特征缩放提供了最一致的性能，当集成到完整优化管道中时，相对紧凑的NEAT进化NN模型实现了与最初使用的密集少层感知器相似或更好的预测准确性和泛化能力。因此，这些资源高效的模型成功执行了对可见光谱中表现出强圆二色性的超表面的推理，允许在模拟数据和实验数据之间进行迁移学习。这种方法展示了自适应、自配置机器学习框架的可扩展路径，用于自动化的光子设计，既可以独立使用，也可以作为智能人工智能的构建块。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The design of chiral metasurfaces with tailored optical properties remains a central challenge in nanophotonics due to the highly nonlinear relationship between geometry and chiroptical response. Machine-learning-assisted optimization pipelines have recently emerged as efficient tools to accelerate this process, yet their performance strongly depends on the choice of neural-network (NN) architecture. In this work, we integrate the NeuroEvolution of Augmenting Topologies (NEAT) algorithm into an established deep-learning optimization framework for dielectric chiral metasurfaces. NEAT autonomously evolves both network topology and connection weights, enabling task-specific architectures without manual tuning, whereas the reinforcement-learning strategy in our framework evolves knowledge of the solution space and fine-tunes a model's weights in parallel. Using a pipeline-produced dataset of 9,600 simulated GaP metasurface geometries, we evaluate NEAT under varying input dimensionalities, feature-scaling methods, and data sizes. With standardized feature scaling yielding the most consistent performance for both examined output dimensionalities, the relatively compact NEAT-evolved NN models, when integrated into the full optimization pipeline, achieve similar or improved predictive accuracy and generalization compared to initially employed dense few-layer perceptrons. Accordingly, these resource-efficient models successfully perform inference of metasurfaces exhibiting strong circular dichroism in the visible spectrum, allowing for transfer learning between simulated and experimental data. This approach demonstrates a scalable path toward adaptive, self-configuring machine-learning frameworks for automated photonic design both standalone and as building block for agentic artificial intelligence (AI).</description>
      <author>example@mail.com (Davide Filippozzi, Arash Rahimi-Iman)</author>
      <guid isPermaLink="false">2512.23558v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Universal and Experiment-calibrated Prediction of XANES through Crystal Graph Neural Network and Transfer Learning Strategy</title>
      <link>http://arxiv.org/abs/2512.23449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种结合晶体图神经网络和迁移学习的方法，用于快速、通用且经过实验校准的X射线吸收近边结构(XANES)预测。&lt;h4&gt;背景&lt;/h4&gt;理论模拟有助于准确解释实验X射线吸收近边结构(XANES)光谱，这些光谱包含材料丰富的原子和电子结构信息。然而，当需要分析大量数据时，如电池材料的原位表征，当前模拟方法过于复杂，无法提供所需的准确性和及时性。&lt;h4&gt;目的&lt;/h4&gt;解决现有XANES预测模型的问题，包括使用模拟数据训练导致预测与实验光谱差异大，以及模型在不同元素间的通用性不足。&lt;h4&gt;方法&lt;/h4&gt;建立晶体图神经网络，首先在覆盖48种元素的模拟XANES数据上预训练，然后利用迁移学习使用少量实验XANES数据集进行校准。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的晶体图神经网络实现了平均相对平方误差低至0.020223的通用XANES预测；校准后，预测的S、Ti和Fe K边XANES的边能量失配误差显著减少了约55%。&lt;h4&gt;结论&lt;/h4&gt;本研究展示的方法为实现快速、通用且经过实验校准的XANES预测开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;理论模拟有助于准确解释包含材料丰富原子和电子结构信息的实验X射线吸收近边结构(XANES)光谱。然而，当需要分析大量数据时，如电池材料的原位表征，当前的模拟方法通常过于复杂，无法提供所需的准确性和及时性。为解决这些问题，已经开发了人工智能(AI)模型用于XANES预测。然而，现有模型使用模拟数据而非实验XANES数据进行训练，导致预测光谱与实验光谱之间存在显著差异。此外，这类模型在不同元素间的通用性尚未得到充分研究。在本工作中，我们首先建立了一个晶体图神经网络，在覆盖48种元素的模拟XANES数据上进行预训练，实现了平均相对平方误差低至0.020223的通用XANES预测；然后利用迁移学习使用少量实验XANES数据集对模型进行校准。校准后，预测的S、Ti和Fe K边XANES的边能量失配误差显著减少了约55%。本研究展示的方法为实现快速、通用且经过实验校准的XANES预测开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Theoretical simulation is helpful for accurate interpretation of experimental X-ray absorption near-edge structure (XANES) spectra that contain rich atomic and electronic structure information of materials. However, current simulation methods are usually too complex to give the needed accuracy and timeliness when a large amount of data need to be analyzed, such as for in-situ characterization of battery materials. To address these problems, artificial intelligence (AI) models have been developed for XANES prediction. However, instead of using experimental XANES data, the existing models are trained using simulated data, resulting in significant discrepancies between the predicted and experimental spectra. Also, the universality across different elements has not been well studied for such models. In this work, we firstly establish a crystal graph neural network, pre-trained on simulated XANES data covering 48 elements, to achieve universal XANES prediction with a low average relative square error of 0.020223; and then utilize transfer learning to calibrate the model using a small experimental XANES dataset. After calibration, the edge energy misalignment error of the predicted S, Ti and Fe K edge XANES is significantly reduced by about 55%. The method demonstrated in this work opens up a new way to achieve fast, universal, and experiment-calibrated XANES prediction.</description>
      <author>example@mail.com (Zichang Lin, Wenjie Chen, Yitao Lin, Xinxin Zhang, Yuegang Zhang)</author>
      <guid isPermaLink="false">2512.23449v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>The Quest for Winning Tickets in Low-Rank Adapters</title>
      <link>http://arxiv.org/abs/2512.22495v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了彩票假设在低秩适配(LoRA)参数高效微调方法中的适用性，提出了Partial-LoRA方法，通过识别稀疏子网络显著减少可训练参数数量同时保持或提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;彩票假设表明过参数化神经网络包含稀疏子网络('中奖票')，可以从头开始训练时达到与完整模型相当的性能。随着对微调大型预训练模型的依赖增加，需要研究LTH是否可以扩展到参数高效微调方法。&lt;h4&gt;目的&lt;/h4&gt;调查彩票假设是否适用于参数高效微调方法，特别是低秩适配(LoRA)方法，并探索如何在LoRA中识别和利用稀疏子网络以提高效率。&lt;h4&gt;方法&lt;/h4&gt;提出Partial-LoRA方法，系统地识别稀疏子网络并训练与预训练模型任务相关子空间对齐的稀疏低秩适配器。在8个视觉和12个语言任务上进行实验，包括单任务和多任务设置。&lt;h4&gt;主要发现&lt;/h4&gt;彩票假设在LoRA中成立，存在可以匹配密集适配器性能的稀疏子网络；稀疏子网络的有效性主要取决于每层应用的稀疏程度，而非子网络中包含的确切权重；Partial-LoRA可将可训练参数减少高达87%同时保持或提高准确性。&lt;h4&gt;结论&lt;/h4&gt;研究结果加深了对迁移学习以及预训练和微调之间相互作用的理论理解，为开发更高效的模型适应策略开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;彩票假设(LTH)表明，过参数化的神经网络包含稀疏子网络('中奖票')，这些子网络可以从头开始训练时达到与完整模型相当的性能。随着对微调大型预训练模型的依赖日益增加，我们研究了LTH是否可以扩展到参数高效微调(PEFT)方法，特别是低秩适配(LoRA)方法。我们的主要发现是LTH在LoRA中成立，揭示了可以匹配密集适配器性能的稀疏子网络。具体来说，我们发现稀疏子网络的有效性更多地取决于每层应用的稀疏程度，而非子网络中包含的确切权重。基于这一见解，我们提出了Partial-LoRA方法，该方法系统地识别上述子网络并训练与预训练模型任务相关子空间对齐的稀疏低秩适配器。在8个视觉和12个语言任务上的实验(包括单任务和多任务设置)表明，Partial-LoRA将可训练参数数量减少了高达87%，同时保持或提高了准确性。我们的结果不仅加深了我们对迁移学习以及预训练和微调之间相互作用的理论理解，还为开发更高效的适应策略开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Lottery Ticket Hypothesis (LTH) suggests that over-parameterized neural networks contain sparse subnetworks ("winning tickets") capable of matching full model performance when trained from scratch. With the growing reliance on fine-tuning large pretrained models, we investigate whether LTH extends to parameter-efficient fine-tuning (PEFT), specifically focusing on Low-Rank Adaptation (LoRA) methods. Our key finding is that LTH holds within LoRAs, revealing sparse subnetworks that can match the performance of dense adapters. In particular, we find that the effectiveness of sparse subnetworks depends more on how much sparsity is applied in each layer than on the exact weights included in the subnetwork. Building on this insight, we propose Partial-LoRA, a method that systematically identifies said subnetworks and trains sparse low-rank adapters aligned with task-relevant subspaces of the pre-trained model. Experiments across 8 vision and 12 language tasks in both single-task and multi-task settings show that Partial-LoRA reduces the number of trainable parameters by up to 87\%, while maintaining or improving accuracy. Our results not only deepen our theoretical understanding of transfer learning and the interplay between pretraining and fine-tuning but also open new avenues for developing more efficient adaptation strategies.</description>
      <author>example@mail.com (Hamed Damirchi, Cristian Rodriguez-Opazo, Ehsan Abbasnejad, Zhen Zhang, Javen Shi)</author>
      <guid isPermaLink="false">2512.22495v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning</title>
      <link>http://arxiv.org/abs/2512.22243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 11 Figures, Supplementary Materials&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究成功应用神经网络和迁移学习模型预测爱尔兰燕麦作物中的霉菌毒素污染，发现收获前90天的天气历史模式和种子含水量是最重要预测因素，TabPFN模型表现最佳。&lt;h4&gt;背景&lt;/h4&gt;霉菌毒素污染对谷物作物质量、食品安全和农业生产力构成重大风险，准确预测霉菌毒素水平可支持早期干预并减少经济损失。&lt;h4&gt;目的&lt;/h4&gt;研究使用神经网络和迁移学习模型预测爱尔兰燕麦作物中的霉菌毒素污染，作为多响应预测任务处理。&lt;h4&gt;方法&lt;/h4&gt;使用爱尔兰收集的燕麦样本数据集(包含环境、农业和地理预测变量)，评估五种建模方法：基线MLP、预训练MLP和三种迁移学习模型(TabPFN、TabNet和FT-Transformer)。使用回归和分类指标评估性能，并进行变量重要性分析。&lt;h4&gt;主要发现&lt;/h4&gt;TabPFN迁移学习方法提供了最佳整体性能，其次是基线MLP。收获前90天的天气历史模式和种子含水量是最重要的预测变量。&lt;h4&gt;结论&lt;/h4&gt;迁移学习方法(特别是TabPFN)在预测霉菌毒素污染方面表现良好，天气条件和种子含水量是预测霉菌毒素水平的关键因素。&lt;h4&gt;翻译&lt;/h4&gt;霉菌毒素污染对谷物作物质量、食品安全和农业生产力构成显著风险。准确预测霉菌毒素水平可以支持早期干预策略并减少经济损失。本研究调查了使用神经网络和迁移学习模型预测爱尔兰燕麦作物中霉菌毒素污染，作为多响应预测任务。我们的数据集包含在爱尔兰收集的燕麦样本，混合了环境、农业和地理预测变量。评估了五种建模方法：基线多层感知器、预训练的多层感知器和三种迁移学习模型；TabPFN、TabNet和FT-Transformer。使用回归和分类指标评估模型性能，结果按毒素类型和平均值报告。此外，进行了基于排列的变量重要性分析，以识别两个预测任务中最有影响力的预测变量。迁移学习方法TabPFN提供了最佳整体性能，其次是基线多层感知器。我们的变量重要性分析显示，收获前90天的天气历史模式是最重要的预测变量，以及种子含水量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mycotoxin contamination poses a significant risk to cereal crop quality, food safety, and agricultural productivity. Accurate prediction of mycotoxin levels can support early intervention strategies and reduce economic losses. This study investigates the use of neural networks and transfer learning models to predict mycotoxin contamination in Irish oat crops as a multi-response prediction task. Our dataset comprises oat samples collected in Ireland, containing a mix of environmental, agronomic, and geographical predictors. Five modelling approaches were evaluated: a baseline multilayer perceptron (MLP), an MLP with pre-training, and three transfer learning models; TabPFN, TabNet, and FT-Transformer. Model performance was evaluated using regression (RMSE, $R^2$) and classification (AUC, F1) metrics, with results reported per toxin and on average. Additionally, permutation-based variable importance analysis was conducted to identify the most influential predictors across both prediction tasks. The transfer learning approach TabPFN provided the overall best performance, followed by the baseline MLP. Our variable importance analysis revealed that weather history patterns in the 90-day pre-harvest period were the most important predictors, alongside seed moisture content.</description>
      <author>example@mail.com (Alan Inglis, Fiona Doohan, Subramani Natarajan, Breige McNulty, Chris Elliott, Anne Nugent, Julie Meneely, Brett Greer, Stephen Kildea, Diana Bucur, Martin Danaher, Melissa Di Rocco, Lisa Black, Adam Gauley, Naoise McKenna, Andrew Parnell)</author>
      <guid isPermaLink="false">2512.22243v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images</title>
      <link>http://arxiv.org/abs/2512.20374v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, BMVC 2025 submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于CLIP模型的自动BBPS评分框架，通过融合全局视觉特征与粪便相关文本先验，提高肠道清洁度评估的准确性，无需显式分割即可实现。&lt;h4&gt;背景&lt;/h4&gt;准确的肠道清洁度评估对有效的肠镜检查至关重要。波士顿肠道准备评分系统(BBPS)提供了标准化的评分系统，但手动操作时存在主观性和观察者间变异性的问题。&lt;h4&gt;目的&lt;/h4&gt;为了支持稳健的训练和评估，构建高质量的肠镜数据集并提出一种新颖的自动BBPS评分框架，以解决手动评分中的主观性问题。&lt;h4&gt;方法&lt;/h4&gt;构建包含517名受试者的2240张图像的高质量肠镜数据集，附有专家一致的BBPS评分注释；提出基于适配器迁移学习的CLIP模型和专门粪便特征提取分支的自动BBPS评分框架，融合全局视觉特征与粪便相关文本先验。&lt;h4&gt;主要发现&lt;/h4&gt;在自建数据集和公共NERTHU数据集上的实验表明，该方法优于现有基线，突显了其在计算机辅助肠镜分析中临床部署的潜力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效解决手动BBPS评分中的主观性和观察者间变异性问题，有望在临床实践中应用。&lt;h4&gt;翻译&lt;/h4&gt;准确的肠道清洁度评估对有效的肠镜检查程序至关重要。波士顿肠道准备评分系统(BBPS)提供了标准化的评分系统，但在手动执行时存在主观性和观察者间变异性的问题。在本文中，为了支持稳健的训练和评估，我们构建了一个高质量的肠镜数据集，包含来自517名受试者的2240张图像，并附有专家一致的BBPS评分注释。我们提出了一种新颖的自动BBPS评分框架，利用基于适配器的迁移学习的CLIP模型和专门的粪便特征提取分支。我们的方法融合全局视觉特征与粪便相关的文本先验，无需显式分割即可提高肠道清洁度评估的准确性。在我们数据集和公共NERTHU数据集上的大量实验证明了我们方法相对于现有基线的优越性，突显了其在计算机辅助肠镜分析中临床部署的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.</description>
      <author>example@mail.com (Yujia Fu, Zhiyu Dong, Tianwen Qian, Chenye Zheng, Danian Ji, Linhai Zhuo)</author>
      <guid isPermaLink="false">2512.20374v2</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection</title>
      <link>http://arxiv.org/abs/2512.23147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GeoTeacher的半监督3D目标检测方法，通过几何关系监督和数据增强技术提升模型对物体几何信息的理解能力。&lt;h4&gt;背景&lt;/h4&gt;半监督3D目标检测是近年来的活跃研究领域，现有方法主要通过异构教师模型或特征视角一致性来提升性能，但忽略了模型在标记数据有限时对物体几何敏感性低的问题。&lt;h4&gt;目的&lt;/h4&gt;提出GeoTeacher方法，增强学生模型在有限训练数据（尤其是未标记数据）下捕获物体几何关系的能力。&lt;h4&gt;方法&lt;/h4&gt;设计基于关键点的几何关系监督模块，将教师模型的几何知识转移到学生模型；引入体素级数据增强策略增加物体几何多样性；融入距离衰减机制保持远处物体完整性；GeoTeacher可与其他半监督3D检测方法结合使用。&lt;h4&gt;主要发现&lt;/h4&gt;在ONCE和Waymo数据集上的大量实验表明该方法具有有效性和良好的泛化能力，并取得了新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;GeoTeacher通过几何关系监督和数据增强策略，有效解决了半监督3D目标检测中模型对几何信息敏感性低的问题，提升了学生模型的目标感知和定位能力。&lt;h4&gt;翻译&lt;/h4&gt;半监督3D目标检测旨在利用未标记数据提升3D目标检测器性能，近年来已成为一个活跃的研究领域。一些先前的方法通过使用异构教师模型提供高质量伪标签或强制教师和学生网络之间的特征视角一致性，已经显示出显著的改进。然而，这些方法忽略了模型通常在标记数据有限时对物体几何形状敏感性较低的事实，难以捕获几何信息，这对增强学生模型的目标感知和定位能力至关重要。在本文中，我们提出GeoTeacher来增强学生模型在有限训练数据下捕获物体几何关系的能力，特别是未标记数据。我们设计了一个基于关键点的几何关系监督模块，将教师模型对物体几何的知识转移到学生，从而提高学生理解几何关系的能力。此外，我们引入了一种体素级数据增强策略，增加了物体几何的多样性，从而进一步改善学生模型理解几何结构的能力。为了在增强过程中保持远处物体的完整性，我们将距离衰减机制纳入该策略。此外，GeoTeacher可以与不同的半监督3D检测方法结合，以进一步提高它们的性能。在ONCE和Waymo数据集上的大量实验表明了我们方法的有效性和泛化能力，我们取得了最新的最先进结果。代码将在https://github.com/SII-Whaleice/GeoTeacher上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model's ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model's ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student, thereby improving the student's capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model's ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher</description>
      <author>example@mail.com (Jingyu Li, Xiaolong Zhao, Zhe Liu, Wenxiao Wu, Li Zhang)</author>
      <guid isPermaLink="false">2512.23147v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration</title>
      <link>http://arxiv.org/abs/2512.22503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SCAFusion，一种专为月球机器人任务设计的多模态3D目标检测模型，通过创新机制显著提高了小、不规则目标的检测性能。&lt;h4&gt;背景&lt;/h4&gt;月球表面探索中自主导航和操作需要可靠精确地检测小且不规则的目标，如陨石碎片和岩石。现有的为地球自动驾驶设计的多模态3D感知方法在其他世界环境中表现不佳，由于特征对齐差、多模态协同有限和小目标检测能力弱。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门针对月球环境的多模态3D目标检测模型，提高小、不规则目标的检测精度和可靠性。&lt;h4&gt;方法&lt;/h4&gt;基于BEVFusion框架构建SCAFusion模型，集成了认知适配器用于高效调整相机主干网络，对比对齐模块增强相机和LiDAR特征一致性，相机辅助训练分支加强视觉表示，以及分段感知坐标注意力机制专门提升小、不规则目标的检测性能。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes验证集上，模型仅参数和计算量略有增加的情况下，实现了69.7%的mAP和72.1%的NDS，分别比基线提高了5.0%和2.7%。在模拟月球环境中，SCAFusion实现了90.93%的mAP，比基线提高了11.5%，在检测类似陨石的小障碍物方面有显著提升。&lt;h4&gt;结论&lt;/h4&gt;SCAFusion是一种有效的多模态3D目标检测模型，特别适合月球表面探索任务，能够显著提高小、不规则目标的检测性能，为月球表面自主导航提供了可靠的感知能力。&lt;h4&gt;翻译&lt;/h4&gt;月球表面探索中自主导航和操作对小型和不规则物体（如陨石碎片和岩石）的可靠精确检测至关重要。现有的为地球自动驾驶设计的多模态3D感知方法在其他世界环境中表现不佳，由于特征对齐差、多模态协同有限和小目标检测能力弱。本文提出了SCAFusion，一种为月球机器人任务定制化的多模态3D目标检测模型。基于BEVFusion框架构建，SCAFusion集成了认知适配器用于高效调整相机主干网络，对比对齐模块增强相机LiDAR特征一致性，相机辅助训练分支加强视觉表示，最重要的是，分段感知坐标注意力机制专门设计用于提升小、不规则目标的检测性能。在参数和计算量略有增加的情况下，我们的模型在nuScenes验证集上实现了69.7%的mAP和72.1%的NDS，分别比基线提高了5.0%和2.7%。在基于Isaac Sim构建的模拟月球环境中，SCAFusion实现了90.93%的mAP，比基线提高了11.5%，在检测类似陨石的小障碍物方面有显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.</description>
      <author>example@mail.com (Xin Chen, Kang Luo, Yangyi Xiao, Hesheng Wang)</author>
      <guid isPermaLink="false">2512.22503v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction</title>
      <link>http://arxiv.org/abs/2512.23489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MIRAGE-VC，一个多视角检索增强生成框架，用于预测初创公司成功，解决了风险投资预测中的路径爆炸和异质证据融合挑战。&lt;h4&gt;背景&lt;/h4&gt;大多数风险投资失败，只有少数带来超额回报；准确预测初创公司成功需要综合复杂的关系证据；传统机器学习和图神经网络缺乏推理能力；大语言模型具有推理能力但与图数据存在模态不匹配；现有图-LLM方法专注于图内任务，而风险投资预测是图外任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够通过显式推理形成连贯、可解释的投资论点的框架，用于准确预测初创公司成功。&lt;h4&gt;方法&lt;/h4&gt;MIRAGE-VC框架包含信息增益驱动的路径检索器，迭代选择高价值邻居，将投资网络压缩为紧凑链；多智能体架构通过基于公司属性的可学习门控机制整合三种证据流。&lt;h4&gt;主要发现&lt;/h4&gt;在严格的防泄漏控制下，MIRAGE-VC实现了+5.0%的F1和+16.6%的PrecisionAt5；该方法为其他图外预测任务（如推荐和风险评估）提供了见解。&lt;h4&gt;结论&lt;/h4&gt;MIRAGE-VC有效解决了风险投资预测中的路径爆炸和异质证据融合挑战，显著提高了预测性能。&lt;h4&gt;翻译&lt;/h4&gt;大多数风险投资（VC）投资都会失败，而只有少数能带来超额回报。准确预测初创公司成功需要综合复杂的关系证据，包括公司披露、投资者记录和投资网络结构，通过显式推理形成连贯、可解释的投资论点。传统机器学习和图神经网络都缺乏这种推理能力。大语言模型（LLMs）提供强大的推理能力，但与图数据存在模态不匹配问题。最近的图-LLM方法针对图内任务，其中答案存在于图内，而VC预测是图外的：目标存在于网络之外。核心挑战是选择能最大化预测者在外部目标上性能的图路径，同时支持逐步推理。我们提出了MIRAGE-VC，一个多视角检索增强生成框架，解决了两个障碍：路径爆炸（数千条候选路径使LLM上下文过载）和异质证据融合（不同初创公司需要不同的分析重点）。我们的信息增益驱动的路径检索器迭代选择高价值邻居，将投资网络压缩为紧凑链以进行显式推理。多智能体架构通过基于公司属性的可学习门控机制整合三种证据流。在严格的防泄漏控制下，MIRAGE-VC实现了+5.0%的F1和+16.6%的PrecisionAt5，并为其他图外预测任务（如推荐和风险评估）提供了见解。代码：https://anonymous.4open.science/r/MIRAGE-VC-323F。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.</description>
      <author>example@mail.com (Haoyu Pei, Zhongyang Liu, Xiangyi Xiao, Xiaocong Du, Haipeng Zhang, Kunpeng Zhang, Suting Hong)</author>
      <guid isPermaLink="false">2512.23489v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Domain matters: Towards domain-informed evaluation for link prediction</title>
      <link>http://arxiv.org/abs/2512.23371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究系统评估了12种主流链接预测算法在740个跨七个领域的真实世界网络上的性能，发现算法性能在不同领域间一致性低，而在领域内一致性高，并提出了'Winner Score'指标来识别各领域最优算法。&lt;h4&gt;背景&lt;/h4&gt;链接预测是复杂网络分析的基础任务，在社会推荐、药物靶点发现和知识图谱补全等领域有广泛应用。然而，现有评估通常只在有限网络上进行，假设跨领域性能排名一致，忽视了不同领域生成机制和语义背景的显著差异。&lt;h4&gt;目的&lt;/h4&gt;系统评估多种链接预测算法在跨领域网络上的性能，揭示算法在特定领域的表现，并探讨领域属性作为影响算法性能的关键因素。&lt;h4&gt;方法&lt;/h4&gt;评估12种主流链接预测算法在740个跨越七个领域的真实世界网络上的性能，使用主成分分析(PCA)分析算法排名形成的响应向量，观察它们在低维空间中按领域聚类的情况。&lt;h4&gt;主要发现&lt;/h4&gt;1) 跨领域算法排名一致性明显低于领域内一致性；2) PCA分析显示算法排名响应向量按领域明显聚类，证实领域属性是影响算法性能的关键因素；3) 提出了'Winner Score'指标确定了各领域最优算法；4) 特定领域最优算法在其他领域往往表现不佳。&lt;h4&gt;结论&lt;/h4&gt;不存在普遍最优的链接预测算法，选择算法时应考虑其机制与网络结构的匹配性，应根据特定领域选择最适合的算法。&lt;h4&gt;翻译&lt;/h4&gt;链接预测是复杂网络分析中的一个基础任务，在社会推荐、药物靶点发现和知识图谱补全等关键场景中有广泛应用。然而，现有算法评估通常只在有限数量的网络上进行实验，并假设跨领域的性能排名是一致的。尽管不同领域的生成机制和语义背景存在显著差异，但以往研究往往仅通过对跨领域网络的简单平均来错误地强调'普遍最优'的算法。本文系统地评估了12种主流链接预测算法在740个跨越七个领域的真实世界网络上的性能。我们提供了大量实证证据，阐明算法在特定领域的性能。这些发现揭示了跨领域算法排名的一致性程度明显较低，这一现象与单个领域内观察到的高度一致性形成鲜明对比。主成分分析显示，由12种算法排名形成的响应向量在低维空间中按领域明显聚类，从而证实领域属性是影响算法性能的关键因素。我们提出了一个称为'Winner Score'的指标，可以识别每个领域中的优越算法：社会网络是非负矩阵分解，经济学是基于邻域重叠感知的图神经网络，化学是图卷积网络，生物学是基于L3的资源分配。然而，这些特定领域表现最优的算法往往在其他领域表现不佳。这一发现强调了算法机制与网络结构匹配的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction, a foundational task in complex network analysis, has extensive applications in critical scenarios such as social recommendation, drug target discovery, and knowledge graph completion. However, existing evaluations of algorithmic often rely on experiments conducted on a limited number of networks, assuming consistent performance rankings across domains. Despite the significant disparities in generative mechanisms and semantic contexts, previous studies often improperly highlight ``universally optimal" algorithms based solely on naive average over networks across domains. This paper systematically evaluates 12 mainstream link prediction algorithms across 740 real-world networks spanning seven domains. We present substantial empirical evidence elucidating the performance of algorithms in specific domains. This findings reveal a notably low degree of consistency in inter-domain algorithm rankings, a phenomenon that stands in stark contrast to the high degree of consistency observed within individual domains. Principal Component Analysis shows that response vectors formed by the rankings of the 12 algorithms cluster distinctly by domain in low-dimensional space, thus confirming domain attributes as a pivotal factor affecting algorithm performance. We propose a metric called Winner Score that could identify the superior algorithm in each domain: Non-Negative Matrix Factorization for social networks, Neighborhood Overlap-aware Graph Neural Networks for economics, Graph Convolutional Networks for chemistry, and L3-based Resource Allocation for biology. However, these domain-specific top-performing algorithms tend to exhibit suboptimal performance in other domains. This finding underscores the importance of aligning an algorithm's mechanism with the network structure.</description>
      <author>example@mail.com (Yilin Bi, Junhao Bian, Shuyan Wan, Shuaijia Wang, Tao Zhou)</author>
      <guid isPermaLink="false">2512.23371v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use</title>
      <link>http://arxiv.org/abs/2512.23137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了GNN-TF模型，解决了整合非欧几里得脑成像数据与欧几里得表格数据并预测未来结果的挑战，在预测未来烟草使用方面表现出色，是临床结果预测的有价值工具。&lt;h4&gt;背景&lt;/h4&gt;整合非欧几里得脑成像数据与欧几里得表格数据（如临床和人口统计信息）是医学影像分析的重大挑战，尤其在预测未来结果方面。虽然机器学习和深度学习技术已成功应用于横断面分类和预测任务，但在纵向影像研究中有效预测结果仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时整合表格数据和动态脑连接数据的时间感知模型，以解决医学影像分析中的数据整合和预测挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种时间感知图神经网络模型与transformer融合（GNN-TF），该模型灵活整合表格数据和动态脑连接数据，利用变量时间顺序在一致框架内进行分析。研究使用了NCANDA的纵向静息态功能磁共振成像数据集，并与多种成熟的机器学习和深度学习模型进行了比较分析。&lt;h4&gt;主要发现&lt;/h4&gt;GNN-TF超越了现有的最先进方法，在预测未来烟草使用方面提供了更优的预测准确性。端到端的时间感知transformer融合结构成功整合了多种数据模态并有效利用了时间动态。&lt;h4&gt;结论&lt;/h4&gt;GNN-TF模型是专注于临床结果预测的功能性脑影像研究的一个有价值的分析工具，能够有效整合多源数据并捕捉时间动态信息。&lt;h4&gt;翻译&lt;/h4&gt;将非欧几里得脑成像数据与欧几里得表格数据（如临床和人口统计信息）进行整合，对医学影像分析构成了重大挑战，特别是在预测未来结果方面。虽然机器学习和深度学习技术已成功应用于横断面分类和预测任务，但在纵向影像研究中有效预测结果仍然具有挑战性。为应对这一挑战，我们引入了一种时间感知图神经网络模型与transformer融合（GNN-TF）。该模型灵活地整合表格数据和动态脑连接数据，利用这些变量在一致框架内的时间顺序。通过整合来自国家青少年酒精和神经发育联盟（NCANDA）纵向静息态功能磁共振成像数据集的非欧几里得和欧几里得信息源，GNN-TF能够进行全面的分析，捕捉纵向影像数据的关键方面。与多种成熟的机器学习和深度学习模型的比较分析表明，GNN-TF超越了这些最先进的方法，在预测未来烟草使用方面提供了更优的预测准确性。所提出的GNN-TF模型的端到端、时间感知transformer融合结构成功整合了多种数据模态并利用了时间动态，使其成为专注于临床结果预测的功能性脑影像研究的一个有价值的分析工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied successfully to cross-sectional classification and prediction tasks, effectively forecasting outcomes in longitudinal imaging studies remains challenging. To address this challenge, we introduce a time-aware graph neural network model with transformer fusion (GNN-TF). This model flexibly integrates both tabular data and dynamic brain connectivity data, leveraging the temporal order of these variables within a coherent framework. By incorporating non-Euclidean and Euclidean sources of information from a longitudinal resting-state fMRI dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), the GNN-TF enables a comprehensive analysis that captures critical aspects of longitudinal imaging data. Comparative analyses against a variety of established machine learning and deep learning models demonstrate that GNN-TF outperforms these state-of-the-art methods, delivering superior predictive accuracy for predicting future tobacco usage. The end-to-end, time-aware transformer fusion structure of the proposed GNN-TF model successfully integrates multiple data modalities and leverages temporal dynamics, making it a valuable analytic tool for functional brain imaging studies focused on clinical outcome prediction.</description>
      <author>example@mail.com (Runzhi Zhou, Xi Luo)</author>
      <guid isPermaLink="false">2512.23137v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Debugging Tabular Log as Dynamic Graphs</title>
      <link>http://arxiv.org/abs/2512.22903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GraphLogDebugger框架，基于动态图技术调试表格日志，通过简单的动态图神经网络实现比大型语言模型更好的性能，同时提高了灵活性和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;表格日志记录现实世界系统的对象和事件并报告更新，可用于检测系统不一致性。然而，现有方法过度依赖大型语言模型和其他重型模型，导致灵活性和可扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖大型语言模型、具有更好灵活性和可扩展性的表格日志调试方法。&lt;h4&gt;方法&lt;/h4&gt;提出GraphLogDebugger框架，通过构建对象和事件的异构节点并连接节点间的边，将表格日志背后的系统建模为演化的动态图，并使用简单的动态图神经网络进行调试。&lt;h4&gt;主要发现&lt;/h4&gt;基于动态图建模的简单动态图神经网络在调试表格日志方面性能优于大型语言模型，这一发现在真实世界日志数据集上得到验证。&lt;h4&gt;结论&lt;/h4&gt;GraphLogDebugger框架能够有效调试表格日志，相比依赖大型语言模型的方法具有更好的性能、灵活性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;表格日志记录现实世界系统中的对象和事件，并报告它们的更新以反映系统的变化，人们可以通过调试相应的日志条目有效地检测现实世界的不一致性。然而，最近在处理文本丰富的表格日志数据方面的进展过度依赖大型语言模型（LLMs）和其他重型模型，因此存在灵活性和可扩展性有限的问题。本文提出了一个名为GraphLogDebugger的新框架，基于动态图来调试表格日志。通过为对象和事件构建异构节点并连接节点间的边，该框架将表格日志背后的系统恢复为一个演化的动态图。借助动态图建模，一个简单的动态图神经网络（GNN）足以在调试表格日志方面超越大型语言模型，这一发现已在计算机系统和学术论文的真实世界日志数据集上的实验结果中得到验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular log abstracts objects and events in the real-world system and reports their updates to reflect the change of the system, where one can detect real-world inconsistencies efficiently by debugging corresponding log entries. However, recent advances in processing text-enriched tabular log data overly depend on large language models (LLMs) and other heavy-load models, thus suffering from limited flexibility and scalability. This paper proposes a new framework, GraphLogDebugger, to debug tabular log based on dynamic graphs. By constructing heterogeneous nodes for objects and events and connecting node-wise edges, the framework recovers the system behind the tabular log as an evolving dynamic graph. With the help of our dynamic graph modeling, a simple dynamic Graph Neural Network (GNN) is representative enough to outperform LLMs in debugging tabular log, which is validated by experimental results on real-world log datasets of computer systems and academic papers.</description>
      <author>example@mail.com (Chumeng Liang, Zhanyang Jin, Zahaib Akhtar, Mona Pereira, Haofei Yu, Jiaxuan You)</author>
      <guid isPermaLink="false">2512.22903v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2512.22772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GRExplainer的新型时序图神经网络(TGNN)解释方法，该方法具有通用性、高效性和用户友好性三大特点，解决了现有TGNN解释方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;动态图被广泛用于表示不断演化的现实世界网络。时序图神经网络(TGNNs)已成为处理这类图的有力工具，但其缺乏透明度和可解释性限制了实际应用。TGNN可解释性研究面临三个关键问题：当前方法针对特定TGNN类型定制，限制了通用性；计算成本高，不适合大规模网络；忽略了解释的结构连接性，需要先验知识，降低了用户友好性。&lt;h4&gt;目的&lt;/h4&gt;提出GRExplainer，首个通用、高效且用户友好的TGNN解释方法，以解决现有TGNN解释方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;GRExplainer提取节点序列作为统一的特征表示，使其独立于特定输入格式，适用于基于快照和基于事件的TGNNs。通过利用广度优先搜索和时间信息构建输入节点序列，减少了冗余计算并提高了效率。设计了一个基于循环神经网络(RNNs)的生成模型，实现了自动和连续的解释生成。&lt;h4&gt;主要发现&lt;/h4&gt;在六个真实世界数据集上对三种目标TGNNs进行的实验表明，GRExplainer在通用性、效率和用户友好性方面优于现有的基线方法。&lt;h4&gt;结论&lt;/h4&gt;GRExplainer成功解决了现有TGNN解释方法的局限性，提供了更通用、高效和用户友好的解释方案，有助于推动TGNN在实际应用中的采用。&lt;h4&gt;翻译&lt;/h4&gt;动态图被广泛用于表示不断演化的现实世界网络。时序图神经网络(TGNNs)已成为处理这类图的有力工具，但其缺乏透明度和可解释性限制了其实际应用。关于TGNN可解释性的研究仍处于早期阶段，并面临几个关键问题：(i)当前方法针对特定类型的TGNN定制，限制了通用性；(ii)它们计算成本高，不适合大规模网络；(iii)它们通常忽略了解释的结构连接性，并且需要先验知识，降低了用户友好性。为解决这些问题，我们提出了GRExplainer，这是首个通用、高效且用户友好的TGNN解释方法。GRExplainer提取节点序列作为统一的特征表示，使其独立于特定输入格式，因此适用于基于快照和基于事件的TGNNs（TGNN的主要类型）。通过利用广度优先搜索和时间信息构建输入节点序列，GRExplainer减少了冗余计算并提高了效率。为了增强用户友好性，我们设计了一个基于循环神经网络(RNNs)的生成模型，实现了自动和连续的解释生成。在六个真实世界数据集上对三种目标TGNNs进行的实验表明，GRExplainer在通用性、效率和用户友好性方面优于现有的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.</description>
      <author>example@mail.com (Xuyan Li, Jie Wang, Zheng Yan)</author>
      <guid isPermaLink="false">2512.22772v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation</title>
      <link>http://arxiv.org/abs/2512.22608v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SimVC-CAS，一个模拟风险投资决策作为多智能体交互过程的集体智能系统，将初创企业融资预测重新定义为群体决策任务，显著提高了预测准确性并提供了可解释的推理过程。&lt;h4&gt;背景&lt;/h4&gt;初创企业具有高价值和高失败率的特点，预测其成功已成为跨学科研究的关键挑战。现有方法通常从单一决策者的角度建模成功预测，忽略了在风险投资决策中占主导地位的投资者群体的集体动态。&lt;h4&gt;目的&lt;/h4&gt;提出一个集体智能系统，将风险投资决策建模为多智能体交互过程，重新定义初创企业融资预测为群体决策任务，同时捕捉企业基本面和潜在投资者网络的行为动态。&lt;h4&gt;方法&lt;/h4&gt;设计角色扮演智能体和基于图神经网络的监督交互模块，每个智能体代表具有独特特质和偏好的投资者，通过图结构的共同投资网络实现异质评估和真实信息交换，使用PitchBook的真实世界数据并在严格的数据泄露控制下进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;SimVC-CAS显著提高了预测准确性，提供可解释的、多角度推理，在平均精度@10方面实现了约25%的相对改进。&lt;h4&gt;结论&lt;/h4&gt;SimVC-CAS为其他复杂的群体决策场景提供了启示。&lt;h4&gt;翻译&lt;/h4&gt;由于初创企业的高价值和高失败率，预测其成功已成为跨学科研究的关键挑战。现有方法通常从单一决策者的角度对成功预测进行建模，忽略了在现实世界中风险投资决策中占主导地位的投资者群体的集体动态。在本文中，我们提出了SimVC-CAS，一种新的集体智能系统，将风险投资决策模拟为多智能体交互过程。通过设计角色扮演智能体和基于GNN的监督交互模块，我们将初创企业融资预测重新定义为群体决策任务，同时捕捉企业基本面和潜在投资者网络的行为动态。每个智能体代表具有独特特质和偏好的投资者，通过图结构的共同投资网络实现异质评估和真实信息交换。使用PitchBook的真实世界数据并在严格的数据泄露控制下，我们证明SimVC-CAS显著提高了预测准确性，同时提供可解释的、多角度推理，例如，在平均精度@10方面实现了约25%的相对改进。SimVC-CAS还为其他复杂的群体决策场景提供了启示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.</description>
      <author>example@mail.com (Zhongyang Liu, Haoyu Pei, Xiangyi Xiao, Xiaocong Du, Yihui Li, Suting Hong, Kunpeng Zhang, Haipeng Zhang)</author>
      <guid isPermaLink="false">2512.22608v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2512.22388v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for 5th Muslims in ML Workshop co-located with NeurIPS 2025. OpenReview: https://openreview.net/forum?id=VaHubA7Pwv Code: https://github.com/linhthi/BLISS-GNN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BLISS是一种基于多臂老虎机的层重要性采样策略，通过动态选择最有信息量的节点来解决图神经网络在大图应用中的计算瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)是从图结构数据中学习的强大工具，但应用于大型图时受到计算成本的限制，需要为每个节点处理所有邻居，这造成了内存和计算瓶颈。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够动态选择最有信息量节点的方法，以提高GNN在大图应用中的计算效率。&lt;h4&gt;方法&lt;/h4&gt;引入BLISS(Bandit Layer Importance Sampling Strategy)，使用多臂老虎机在每个层动态选择最有信息量的节点，平衡探索和利用以确保全面图覆盖，并能适应节点重要性的演变，可集成于GCN和GAT等模型。&lt;h4&gt;主要发现&lt;/h4&gt;BLISS能够适应不同模型(GCN和GAT)的特定聚合机制，实验表明其保持或超过全批次训练的准确性。&lt;h4&gt;结论&lt;/h4&gt;BLISS是一种有效的采样策略，能够在保持或提高准确性的同时，显著减少GNN在大图应用中的计算成本。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是从图结构数据中学习的强大工具，但它们在大图上的应用受到计算成本的阻碍。需要为每个节点处理所有邻居，这造成了内存和计算瓶颈。为解决这一问题，我们引入了BLISS，一种基于老虎机的层重要性采样策略。它使用多臂老虎机在每个层动态选择最有信息量的节点，平衡探索和利用以确保全面覆盖图结构。与现有的静态采样方法不同，BLISS适应节点重要性的演变，从而实现更明智的节点选择和更好的性能。它通过与图卷积网络(GCN)和图注意力网络(GAT)集成展示了其多功能性，并根据它们的特定聚合机制调整其选择策略。实验表明，BLISS保持或超过了全批次训练的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their application to large graphs is hindered by computational costs. The need to process every neighbor for each node creates memory and computational bottlenecks. To address this, we introduce BLISS, a Bandit Layer Importance Sampling Strategy. It uses multi-armed bandits to dynamically select the most informative nodes at each layer, balancing exploration and exploitation to ensure comprehensive graph coverage. Unlike existing static sampling methods, BLISS adapts to evolving node importance, leading to more informed node selection and improved performance. It demonstrates versatility by integrating with both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), adapting its selection policy to their specific aggregation mechanisms. Experiments show that BLISS maintains or exceeds the accuracy of full-batch training.</description>
      <author>example@mail.com (Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin)</author>
      <guid isPermaLink="false">2512.22388v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>INSIGHT: Spatially resolved survival modelling from routine histology crosslinked with molecular profiling reveals prognostic epithelial-immune axes in stage II/III colorectal cancer</title>
      <link>http://arxiv.org/abs/2512.22262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为INSIGHT的图神经网络方法，能够直接从常规组织学图像中预测II/III期结直肠癌患者的生存情况，通过分析复杂的空间组织结构提取预后信息。&lt;h4&gt;背景&lt;/h4&gt;常规组织学在II/III期结直肠癌中包含丰富的预后信息，这些信息大多嵌入在复杂的空间组织结构中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接从常规组织学图像预测患者生存情况的图神经网络方法。&lt;h4&gt;方法&lt;/h4&gt;研究人员提出了名为INSIGHT的图神经网络，在TCGA（n=342）和SURGEN（n=336）队列上进行训练和交叉验证，生成患者水平的空间解析风险评分。&lt;h4&gt;主要发现&lt;/h4&gt;1) 大型独立验证显示INSIGHT预后性能优于pTNM分期（C指数0.68-0.69对比0.44-0.58）；2) INSIGHT空间风险图谱重现了经典预后组织病理学特征；3) 识别核实心度和圆形度作为定量风险相关因素；4) 整合多组学数据揭示上皮-免疫风险 manifold，包含上皮去分化、胎儿程序、髓源性基质状态和适应性免疫功能障碍。&lt;h4&gt;结论&lt;/h4&gt;INSIGHT能从常规组织学图像中提取预后信息，揭示患者特异性上皮异质性、MSI-High肿瘤内分层及高风险的CDX2/HNF4A丢失和CEACAM5/6相关增殖程序，突出了协调的治疗脆弱性。&lt;h4&gt;翻译&lt;/h4&gt;常规组织学在II/III期结直肠癌中包含丰富的预后信息，其中大部分嵌入在复杂的空间组织结构中。我们提出了INSIGHT，一种直接从常规组织学图像预测生存的图神经网络。在TCGA（n=342）和SURGEN（n=336）上进行训练和交叉验证后，INSIGHT生成患者水平的空间解析风险评分。大型独立验证显示其预后性能优于pTNM分期（C指数0.68-0.69对比0.44-0.58）。INSIGHT空间风险图谱重现了经典的预后组织病理学特征，并将核实心度和圆形性确定为定量风险相关因素。将空间风险与数据驱动的空间转录组学特征、空间蛋白质组学、RNA测序和单细胞参考数据整合，揭示了一个捕获上皮去分化、胎儿程序、髓源性基质状态和适应性免疫功能障碍的上皮-免疫风险 manifold。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Routine histology contains rich prognostic information in stage II/III colorectal cancer, much of which is embedded in complex spatial tissue organisation. We present INSIGHT, a graph neural network that predicts survival directly from routine histology images. Trained and cross-validated on TCGA (n=342) and SURGEN (n=336), INSIGHT produces patient-level spatially resolved risk scores. Large independent validation showed superior prognostic performance compared with pTNM staging (C-index 0.68-0.69 vs 0.44-0.58). INSIGHT spatial risk maps recapitulated canonical prognostic histopathology and identified nuclear solidity and circularity as quantitative risk correlates. Integrating spatial risk with data-driven spatial transcriptomic signatures, spatial proteomics, bulk RNA-seq, and single-cell references revealed an epithelium-immune risk manifold capturing epithelial dedifferentiation and fetal programs, myeloid-driven stromal states including $\mathrm{SPP1}^{+}$ macrophages and $\mathrm{LAMP3}^{+}$ dendritic cells, and adaptive immune dysfunction. This analysis exposed patient-specific epithelial heterogeneity, stratification within MSI-High tumours, and high-risk routes of CDX2/HNF4A loss and CEACAM5/6-associated proliferative programs, highlighting coordinated therapeutic vulnerabilities.</description>
      <author>example@mail.com (Piotr Keller, Mark Eastwood, Zedong Hu, Aimée Selten, Ruqayya Awan, Gertjan Rasschaert, Sara Verbandt, Vlad Popovici, Hubert Piessevaux, Hayley T Morris, Petros Tsantoulis, Thomas Alexander McKee, André D'Hoore, Cédric Schraepen, Xavier Sagaert, Gert De Hertogh, Sabine Tejpar, Fayyaz Minhas)</author>
      <guid isPermaLink="false">2512.22262v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers</title>
      <link>http://arxiv.org/abs/2512.22260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TACAS 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ReVEAL是一种基于图学习的方法，用于乘法器架构的反向工程，以提高代数电路验证技术。&lt;h4&gt;背景&lt;/h4&gt;传统基于规则的乘法器架构验证方法在处理大型优化乘法器时面临可扩展性和准确性挑战。&lt;h4&gt;目的&lt;/h4&gt;提高代数电路验证技术的可扩展性和准确性，特别是针对大型优化乘法器。&lt;h4&gt;方法&lt;/h4&gt;ReVEAL框架利用结构图特性和学习驱动的推理来大规模识别架构模式，能够稳健处理大型优化乘法器。&lt;h4&gt;主要发现&lt;/h4&gt;ReVEAL在各种乘法器基准测试中展示了其适用性，并且与传统基于规则的方法相比，在可扩展性和准确性方面有所提高。&lt;h4&gt;结论&lt;/h4&gt;ReVEAL方法能够与现有验证流程无缝集成，并支持下游代数证明策略，为乘法器架构验证提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了ReVEAL，一种基于图学习的方法，用于乘法器架构的反向工程，以提高代数电路验证技术。我们的框架利用结构图特性和学习驱动的推理来大规模识别架构模式，能够稳健处理大型优化乘法器。我们在各种乘法器基准测试中展示了其适用性，并表明与传统基于规则的方法相比，可扩展性和准确性有所提高。该方法能够与现有验证流程无缝集成，并支持下游代数证明策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present ReVEAL, a graph-learning-based method for reverse engineering of multiplier architectures to improve algebraic circuit verification techniques. Our framework leverages structural graph features and learning-driven inference to identify architecture patterns at scale, enabling robust handling of large optimized multipliers. We demonstrate applicability across diverse multiplier benchmarks and show improvements in scalability and accuracy compared to traditional rule-based approaches. The method integrates smoothly with existing verification flows and supports downstream algebraic proof strategies.</description>
      <author>example@mail.com (Chen Chen, Daniela Kaufmann, Chenhui Deng, Zhan Song, Hongce Zhang, Cunxi Yu)</author>
      <guid isPermaLink="false">2512.22260v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2512.22251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一个基于图注意力网络的框架，用于预测小分子药物对基因表达的影响。通过整合增强的生物医学知识图谱和LINCS L1000药物及细胞系数据，使用基础模型初始化节点嵌入，成功预测了药物-细胞对中978个标志基因的表达变化。&lt;h4&gt;背景&lt;/h4&gt;理解小分子如何干扰基因表达对于揭示药物机制、预测脱靶效应和识别药物重新定位机会至关重要。现有的深度学习框架虽然已将多模态嵌入整合到生物医学知识图谱中，并通过图神经网络改进了表示，但这些模型主要应用于链接预测和二元药物-疾病关联等任务，而非基因干扰任务。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法未应用于基因干扰任务的问题，开发一个能够揭示药物转录组效应机制的框架。&lt;h4&gt;方法&lt;/h4&gt;构建了一个合并的生物医学图谱，整合了PrimeKG++（包含丰富语义嵌入）和LINCS L1000药物及细胞系节点（使用MolFormerXL和BioBERT等基础模型初始化）。使用这个异构图，训练了一个带有下游预测头部的图注意力网络，学习给定药物-细胞对的标志基因表达变化。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在支架分割和随机分割下，优于MLP基线方法对不同表达基因的预测。消融实验进一步证明，生物医学知识图谱提供的边缘增强了干扰级别的预测。&lt;h4&gt;结论&lt;/h4&gt;该框架为机制性药物建模提供了一条路径：超越二元药物-疾病关联任务，转向治疗干预的细粒度转录效应。&lt;h4&gt;翻译&lt;/h4&gt;理解小分子如何干扰基因表达对于揭示药物机制、预测脱靶效应和识别药物重新定位机会至关重要。虽然先前的深度学习框架已将多模态嵌入整合到生物医学知识图谱中，并通过图神经网络消息传递范式进一步改进了这些表示，但这些模型已应用于链接预测和二元药物-疾病关联等任务，而非可能揭示更多机制转录组效应的基因干扰任务。为解决这一差距，我们构建了一个合并的生物医学图谱，整合了PrimeKG++（一个包含丰富语义嵌入的PrimeKG增强版本）和LINCS L1000药物和细胞系节点（使用MolFormerXL和BioBERT等基础模型初始化的多模态嵌入）。使用这个异构图，我们训练了一个带有下游预测头部的图注意力网络，学习给定药物-细胞对的978个标志基因的表达变化谱。我们的结果表明，在支架分割和随机分割下，我们的框架优于MLP基线方法对不同表达基因的预测。边缘打乱和节点特征随机化的消融实验进一步证明，生物医学知识图谱提供的边缘增强了干扰级别的预测。更广泛地说，我们的框架为机制性药物建模提供了一条路径：超越二元药物-疾病关联任务，转向治疗干预的细粒度转录效应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how small molecules perturb gene expression is essential for uncovering drug mechanisms, predicting off-target effects, and identifying repurposing opportunities. While prior deep learning frameworks have integrated multimodal embeddings into biomedical knowledge graphs (BKGs) and further improved these representations through graph neural network message-passing paradigms, these models have been applied to tasks such as link prediction and binary drug-disease association, rather than the task of gene perturbation, which may unveil more about mechanistic transcriptomic effects. To address this gap, we construct a merged biomedical graph that integrates (i) PrimeKG++, an augmentation of PrimeKG containing semantically rich embeddings for nodes with (ii) LINCS L1000 drug and cell line nodes, initialized with multimodal embeddings from foundation models such as MolFormerXL and BioBERT. Using this heterogeneous graph, we train a graph attention network (GAT) with a downstream prediction head that learns the delta expression profile of over 978 landmark genes for a given drug-cell pair. Our results show that our framework outperforms MLP baselines for differentially expressed genes (DEG) -- which predict the delta expression given a concatenated embedding of drug features, target features, and baseline cell expression -- under the scaffold and random splits. Ablation experiments with edge shuffling and node feature randomization further demonstrate that the edges provided by biomedical KGs enhance perturbation-level prediction. More broadly, our framework provides a path toward mechanistic drug modeling: moving beyond binary drug-disease association tasks to granular transcriptional effects of therapeutic intervention.</description>
      <author>example@mail.com (Pascal Passigan, Kevin zhu, Angelina Ning)</author>
      <guid isPermaLink="false">2512.22251v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>SpatialMosaic: A Multiview VLM Dataset for Partial Visibility</title>
      <link>http://arxiv.org/abs/2512.23365v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了SpatialMosaic数据集和基准测试，以及SpatialMosaicVLM混合框架，用于增强多视图空间理解能力，特别是在部分可见性、遮挡和低重叠条件下的推理能力。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)的快速发展为3D场景理解提供了新可能，但现有方法依赖预构建3D表示或重建管道，限制了可扩展性。虽然最近工作探索了从多视图图像直接学习空间推理，但真实环境中的挑战如部分可见性、遮挡和低重叠条件仍未得到充分研究。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在真实环境中的局限性，特别是处理部分可见性、遮挡和低重叠条件等挑战性场景下的空间推理问题。&lt;h4&gt;方法&lt;/h4&gt;1) 提出可扩展的多视图数据生成和注释管道，构建真实空间推理问答对，创建包含200万个QA对的SpatialMosaic数据集；2) 引入SpatialMosaic-Bench基准测试，包含6个任务共100万个QA对；3) 提出SpatialMosaicVLM混合框架，将3D重建模型作为几何编码器集成到VLMs中。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明提出的数据集和VQA任务有效增强了在挑战性多视图条件下的空间推理能力，验证了数据生成管道在构建真实和多样化问答对方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过SpatialMosaic数据集、SpatialMosaic-Bench基准测试和SpatialMosaicVLM框架，解决了多视图空间推理中的关键挑战，特别是在部分可见性、遮挡和低重叠条件下的推理问题。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的快速发展已经释放了增强3D场景理解和空间推理的潜力。然而，现有方法通常依赖于预构建的3D表示或现成的重建管道，这限制了可扩展性和实际应用性。最近的工作探索直接从多视图图像中学习空间推理，使视觉语言模型(VLMs)能够在没有明确3D重建的情况下理解3D场景。然而，真实环境中经常出现的挑战，如部分可见性、遮挡和需要从碎片化视觉线索进行空间推理的低重叠条件，仍未得到充分探索。为了解决这些局限性，我们提出了一个可扩展的多视图数据生成和注释管道，构建真实的空间推理问答对，从而创建了SpatialMosaic，一个包含200万个问答对的全面指令调优数据集。我们进一步引入了SpatialMosaic-Bench，一个在真实和具有挑战性的场景下评估多视图空间推理的基准测试，包含6个任务共100万个问答对。此外，我们提出了SpatialMosaicVLM，一个混合框架，将3D重建模型作为几何编码器集成到VLMs中，以实现稳健的空间推理。大量实验证明，我们提出的数据集和VQA任务有效地增强了在具有挑战性的多视图条件下的空间推理能力，验证了我们的数据生成管道在构建真实和多样化问答对方面的有效性。代码和数据集即将发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多视角视觉语言模型在部分可见性、遮挡和低重叠条件下的空间推理能力不足的问题。这个问题在现实中非常重要，因为真实世界场景往往存在物体部分可见、相互遮挡和视角重叠少的情况，而现有模型难以处理这些复杂情况，限制了它们在机器人导航、增强现实和自动驾驶等实际应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别现有方法在处理部分可见性、遮挡和低重叠条件时的不足，然后从数据和模型两个角度设计解决方案。在数据方面，他们利用ScanNet++数据集的高质量3D几何信息，设计了可扩展的数据生成和注释流程；在模型方面，他们借鉴了VGGT等3D重建模型作为几何编码器，并参考了LLaVA-Next-Video的多视角处理方法，创造性地将几何线索与视觉特征融合。他们的创新点在于引入了物体遮挡比例和视场遮挡比例的量化方法，以及稀疏多视角采样策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过创建专门针对挑战性多视角场景的数据集和基准测试，并设计融合3D几何信息的混合模型架构，提升VLM在现实世界复杂场景中的空间推理能力。整体流程分为三阶段：1)数据准备阶段，计算物体遮挡比例和视场遮挡比例，进行稀疏多视角采样和实例过滤；2)QA生成和关系计算阶段，选择任务相关对象，计算空间关系；3)QA模板和输出阶段，使用预定义模板生成问答对。模型方面，SpatialMosaicVLM并行处理视觉和几何编码器，通过交叉注意力融合特征，再输入语言模型生成答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了SpatialMosaic数据集(200万QA对)和SpatialMosaic-Bench基准(100万QA对)，专注于部分可见性、遮挡和低重叠条件；2)提出了可扩展的数据生成和注释框架；3)设计了SpatialMosaicVLM混合模型架构，将3D重建模型作为几何编码器集成到VLM中；4)引入了物体遮挡比例和视场遮挡比例的量化方法。相比之前工作，SpatialMosaic更关注现实世界挑战，包含更大视角变化的图像，提供更多样化的任务类别，规模更大，并提供了更细粒度的评估框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SpatialMosaic通过创建专门针对部分可见性、遮挡和低重叠条件的大规模多视角空间推理数据集和基准测试，并提出融合3D重建模型的混合视觉语言框架，显著提升了视觉语言模型在现实世界复杂场景中的空间推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.</description>
      <author>example@mail.com (Kanghee Lee, Injae Lee, Minseok Kwak, Kwonyoung Ryu, Jungi Hong, Jaesik Park)</author>
      <guid isPermaLink="false">2512.23365v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2512.22771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Fisher信息的主动学习算法，用于视图选择问题，能够同时处理语义推理和动态场景建模，提高渲染质量和语义分割性能。&lt;h4&gt;背景&lt;/h4&gt;对于各种任务中的具身智能体来说，理解和语义与动态性非常重要，而且这些任务比静态场景理解任务有更多的数据冗余。&lt;h4&gt;目的&lt;/h4&gt;将视图选择问题表述为主动学习问题，优先选择能为模型训练提供最大信息增益的帧。&lt;h4&gt;方法&lt;/h4&gt;提出一种使用Fisher信息的主动学习算法，量化候选视图相对于语义高斯参数和变形网络的信息量。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够同时处理语义推理和动态场景建模，为启发式或随机策略提供了有原则的替代方案。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，该方法在提高渲染质量和语义分割性能方面始终优于基于随机选择和不确定性启发式的基线方法。&lt;h4&gt;翻译&lt;/h4&gt;理解和语义与动态性对于各种任务中的具身智能体至关重要。这些任务比静态场景理解任务有更多的数据冗余。我们将视图选择问题表述为主动学习问题，目标是优先选择能为模型训练提供最大信息增益的帧。为此，我们提出了一种使用Fisher信息的主动学习算法，量化了候选视图相对于语义高斯参数和变形网络的信息量。这种表述使我们的方法能够同时处理语义推理和动态场景建模，为启发式或随机策略提供了有原则的替代方案。我们通过从多摄像头装置中选择信息丰富的帧，在大型静态图像和动态视频数据集上评估了我们的方法。实验结果表明，我们的方法在提高渲染质量和语义分割性能方面始终优于基于随机选择和不确定性启发式的基线方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何智能选择最佳视角来训练3D高斯飞溅模型的问题，特别是在包含语义信息和动态变化的场景中。这个问题很重要，因为3D高斯飞溅技术虽然能实现高质量重建和实时渲染，但训练过程需要大量数据，尤其是在大规模环境和动态场景中。有效的视角选择可以显著减少训练数据需求，同时保持高质量的渲染和语义理解，这对机器人、AR/VR和数字内容创作等领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将视角选择问题转化为主动学习问题，目标是优先选择能为模型训练提供最大信息增益的帧。他们使用Fisher Information来量化候选视图的信息价值，同时考虑几何、语义和时间变化信息。作者借鉴了FisherRF的视角选择思想，Feature 3DGS的语义表示框架，以及4D-GS的动态建模方法。创新点在于将这些技术扩展到动态语义场景，并提出高效的计算方法来处理大规模场景的复杂性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将视角选择作为主动学习问题，使用Fisher Information量化候选视图的信息价值，同时综合评估几何、语义和时序信息。整体流程包括：构建语义3D高斯飞溅和动态高斯飞溅的基础框架；计算高斯参数和变形网络的Fisher Information；从候选视角中选择期望信息增益最高的视角加入训练；使用选定视角训练模型并定期重新计算Fisher Information以动态选择新视角。这种方法能在减少数据需求的同时保持高质量的渲染和语义理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首个针对动态语义3DGS的Fisher Information驱动的NBV选择框架；将Fisher Information公式扩展到语义高斯参数和变形网络的高效计算方法；使用梯度外积迹估计变形网络Fisher Information的新方法。相比之前工作，本文不仅处理静态场景，还扩展到动态语义场景；同时考虑几何、语义和时序信息，而非仅关注几何；将NBV选择直接集成到3DGS骨干中，扩展了应用场景；并通过高效计算方法使大规模场景处理成为可能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于Fisher Information的主动学习框架，能够智能选择最佳视角来训练动态语义3D高斯飞溅模型，在减少训练数据需求的同时显著提高了动态场景的渲染质量和语义理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.</description>
      <author>example@mail.com (Yiqian Li, Wen Jiang, Kostas Daniilidis)</author>
      <guid isPermaLink="false">2512.22771v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement</title>
      <link>http://arxiv.org/abs/2512.22351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文解决了多模态大语言模型在3D场景操作中的应用挑战，通过引入MCP-based API、增强3D场景理解和多智能体框架，显著提高了复杂物体排列任务的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在2D视觉语言任务中取得了显著进展，但在复杂3D场景操作中的应用仍处于探索阶段。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在3D物体排列任务中的三个关键挑战：弱视觉基础、3D场景理解不足以及迭代更新的管理问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了三个创新方法：(1)引入基于MCP的API，将交互从脆弱的原始代码操作转变为更强大的函数级更新；(2)通过专门的视觉工具增强MLLM的3D场景理解能力，包括分析场景状态、收集空间信息和验证行动结果；(3)提出具有规划、执行和验证角色的协作多智能体框架，以处理多步骤指令和从中间错误中恢复。&lt;h4&gt;主要发现&lt;/h4&gt;在25个复杂的物体排列任务上，作者的方法显著优于现有基线，证明了其在处理复杂3D场景操作任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过解决多模态大语言模型在3D场景操作中的关键挑战，作者成功地将这些模型扩展到了更复杂的3D应用领域，为未来3D场景操作的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型在2D视觉语言任务中取得了显著进展，但它们在复杂3D场景操作中的应用仍处于探索阶段。在本文中，我们通过使用多模态大语言模型解决3D物体排列任务中的三个关键挑战，弥合了这一重要差距。首先，为了解决多模态大语言模型的弱视觉基础问题，它们难以将程序化编辑与精确的3D结果联系起来，我们引入了一个基于MCP的API。这使交互从脆弱的原始代码操作转变为更强大、更健壮的函数级更新。其次，我们通过一套专门的视觉工具增强多模态大语言模型的3D场景理解能力，以分析场景状态、收集空间信息和验证行动结果。这种感知反馈循环对于弥合基于语言的更新和精确的3D感知操作之间的差距至关重要。第三，为了管理迭代且容易出错的更新，我们提出了一个协作的多智能体框架，为规划、执行和验证指定了特定角色。这种分解使系统能够稳健地处理多步骤指令并从中间错误中恢复。我们在25个复杂的物体排列任务集上证明了我们方法的有效性，其显著优于现有基线。网站：vulcan-3d.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io</description>
      <author>example@mail.com (Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo)</author>
      <guid isPermaLink="false">2512.22351v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks</title>
      <link>http://arxiv.org/abs/2512.22207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了GamiBench基准测试，用于评估多模态大语言模型(MLLMs)的空间推理能力和2D到3D规划能力，通过折纸启发的折叠任务进行全面评估。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在感知和指令遵循方面表现良好，但在空间推理能力上仍有欠缺。空间推理是人类智能的关键组成部分，但现有基准测试大多只关注静态图像或最终输出，未能考虑这一技能的序列性和视角依赖性。&lt;h4&gt;目的&lt;/h4&gt;弥补现有基准测试的不足，专门评估MLLMs的空间推理能力和2D到3D规划能力，建立标准化评估框架。&lt;h4&gt;方法&lt;/h4&gt;GamiBench包含186个常规和186个不可能的2D折痕图案及其对应的3D折叠形状，来自六个不同视角，涵盖三个视觉问答任务：预测3D折叠配置、区分有效视角和检测不可能图案。该基准评估整个推理过程，包括跨视图一致性、物理可行性和中间折叠步骤解释，并引入视角一致性(VC)和不可能折叠选择率(IFSR)作为新诊断指标。&lt;h4&gt;主要发现&lt;/h4&gt;即使是领先的模型如GPT-5和Gemini-2.5-Pro在单步空间理解方面也存在困难，表明当前MLLMs在空间推理能力上有明显不足。&lt;h4&gt;结论&lt;/h4&gt;GamiBench为评估MLLMs中的几何理解和空间推理建立了标准化框架，数据集和代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在感知和指令遵循方面能力突出，但在空间推理能力上仍存在挑战：即跨视图和时间跟踪和操作对象的能力。空间推理是人类智能的关键组成部分，但大多数现有基准测试只关注静态图像或最终输出，未能体现这一技能的序列性和视角依赖性。为弥补这一差距，我们引入了GamiBench，这是一个通过折纸启发的折叠任务来评估MLLMs空间推理和2D到3D规划的基准测试。GamiBench包含186个常规和186个不可能的2D折痕图案及其对应的3D折叠形状，来自六个不同视角，涵盖三个视觉问答(VQA)任务：预测3D折叠配置、区分有效视角和检测不可能图案。与仅评估最终预测的先前基准不同，GamiBench全面评估整个推理过程——通过不可能折叠检测衡量跨视图一致性和物理可行性，以及对中间折叠步骤的解释。它进一步引入了新的诊断指标——视角一致性(VC)和不可能折叠选择率(IFSR)——来衡量模型处理不同复杂度折叠的能力。我们的实验表明，即使是GPT-5和Gemini-2.5-Pro等领先模型在单步空间理解方面也存在困难。这些贡献为评估MLLMs中的几何理解和空间推理建立了标准化框架。数据集和代码：https://github.com/stvngo/GamiBench。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型（MLLMs）在空间推理能力方面的评估问题。空间推理是人类智能的关键组成部分，涉及跨多个视图和时间跟踪和操作对象的能力，对人工智能系统与物理世界互动（如机器人技术、自动驾驶、家具组装等）至关重要。现有基准测试大多只关注静态图像或最终输出，未能考虑这种技能的顺序性和视角依赖性，因此需要一个能全面评估模型空间推理能力的框架。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从日本传统折纸艺术中获取灵感，因为折纸需要将2D纸张转换为3D结构，涉及数十个中间离散折叠步骤，为测试顺序空间推理能力提供了理想基础。他们设计了一个包含186个常规和186个不可能的2D折痕图案的数据集，每个图案配对相应的3D折叠形状，从六个不同视角捕获。作者借鉴了现有的在线工具如Oriedita和Origami Simulator来创建和验证折痕图案，并参考了折纸的数学原理（如Kawasaki定理、Maekawa定理和Huzita-Hatori公理）来区分物理可行和不可行的折叠。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过折纸启发的任务来评估MLLMs的空间推理和2D到3D规划能力，不仅评估最终预测，还全面评估模型的整个推理过程，包括跨视图一致性、物理可行性和中间折叠步骤的解释。整体流程包括：1)构建数据集（收集折纸实例，使用工具验证可行性和生成3D渲染）；2)定义任务（单步空间理解和多步空间推理）；3)生成问题-答案对（构建多选答案库并确保质量）；4)使用三个指标评估（准确性、不可能折叠选择率和视角一致性）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多视图、顺序空间基准测试，要求在六个视图和时间上保持一致性；2)新的评估轴（视角一致性和不可能折叠选择率），诊断标准准确性指标未发现的故障模式；3)折纸启发的任务套件，测试MLLMs的几何变换规划能力；4)复杂度控制，分析空间规划的规模效应。相比之前工作，GamiBench直接评估2D到3D变换、多视图空间一致性和物理可行性，而不仅仅是静态理解或最终状态准确性，首次全面揭示了当前MLLMs在空间推理方面的显著局限性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GamiBench通过折纸启发的多视图评估框架，首次全面揭示了当前多模态大语言模型在空间推理和2D到3D规划方面的显著局限性，为未来几何理解和空间推理能力的改进提供了标准化的评估基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.</description>
      <author>example@mail.com (Ryan Spencer, Roey Yaari, Ritvik Vemavarapu, Joyce Yang, Steven Ngo, Utkarsh Sharma)</author>
      <guid isPermaLink="false">2512.22207v1</guid>
      <pubDate>Tue, 30 Dec 2025 15:34:40 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco</title>
      <link>http://arxiv.org/abs/2512.21837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将图结构信息整合到大语言模型中的方法，用于烟草病虫害防治的知识推理。基于GraphRAG框架，该方法通过明确整合领域知识图谱中的结构化信息来增强知识检索和推理能力。&lt;h4&gt;背景&lt;/h4&gt;烟草病虫害防治需要专业知识和准确推理能力，而传统方法可能难以有效处理复杂的多跳和比较推理场景。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合图结构信息的大语言模型方法，以提高在烟草病虫害防治领域知识推理的准确性和深度。&lt;h4&gt;方法&lt;/h4&gt;基于GraphRAG框架构建方法，利用LLM辅助构建烟草病虫害知识图谱，采用Transformer架构作为核心推理模型，使用图神经网络学习节点表示，并以ChatGLM为基础模型，使用LoRA进行参数高效微调。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多个评估指标上一致优于基线方法，显著提高了推理的准确性和深度，特别是在复杂的多跳和比较推理场景中表现尤为突出。&lt;h4&gt;结论&lt;/h4&gt;将图结构信息整合到大语言模型中可以有效增强特定领域知识推理能力，为烟草病虫害防治提供了更准确、更深入的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种将图结构信息整合到大语言模型中的方法，用于烟草病虫害防治的知识推理。基于GraphRAG框架，该方法通过明确整合领域知识图谱中的结构化信息来增强知识检索和推理能力。具体来说，首先利用LLM辅助构建烟草病虫害知识图谱，组织疾病、症状、防治方法等关键实体及其关系。基于此图谱，检索相关知识并整合到推理过程中，以支持准确的答案生成。采用Transformer架构作为核心推理模型，同时使用图神经网络学习 expressive 节点表示，捕获知识图谱中的局部和全局关系信息。以ChatGLM为基础的模型作为骨干LLM，并使用LoRA进行微调，实现参数高效的适应性。大量实验结果表明，该方法在多个评估指标上一致优于基线方法，显著提高了推理的准确性和深度，特别是在复杂的多跳和比较推理场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured information from a domain-specific knowledge graph. Specifically, LLMs are first leveraged to assist in the construction of a tobacco pest and disease knowledge graph, which organizes key entities such as diseases, symptoms, control methods, and their relationships. Based on this graph, relevant knowledge is retrieved and integrated into the reasoning process to support accurate answer generation. The Transformer architecture is adopted as the core inference model, while a graph neural network (GNN) is employed to learn expressive node representations that capture both local and global relational information within the knowledge graph. A ChatGLM-based model serves as the backbone LLM and is fine-tuned using LoRA to achieve parameter-efficient adaptation. Extensive experimental results demonstrate that the proposed approach consistently outperforms baseline methods across multiple evaluation metrics, significantly improving both the accuracy and depth of reasoning, particularly in complex multi-hop and comparative reasoning scenarios.</description>
      <author>example@mail.com (Siyu Li, Chenwei Song, Wan Zhou, Xinyi Liu)</author>
      <guid isPermaLink="false">2512.21837v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
  <item>
      <title>Toward Generalizable Surrogate Models for Molecular Dynamics via Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2512.21822v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种基于图神经网络的分子动力学模拟替代框架，可直接预测原子位移并学习原子系统的演化算子，无需传统方法中的力计算和时间积分步骤。&lt;h4&gt;背景&lt;/h4&gt;传统分子动力学模拟依赖于重复的力评估和数值时间积分，计算成本较高，需要更高效的替代方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种计算效率更高且保持准确性的分子动力学模拟替代框架，加速原子尺度模拟。&lt;h4&gt;方法&lt;/h4&gt;将原子环境表示为图结构，结合消息传递层和注意力机制捕获金属系统中的局部配位和多体相互作用，使用块状铝的经典分子动力学轨迹进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;替代模型在训练范围内实现了亚埃级别的精度，在短期到中期时间外推中表现稳定，并通过径向分布函数和均方位移趋势验证了结构和动力学保真度。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的替代积分器可作为传统分子动力学的有效补充，在验证范围内提供计算效率更高的原子模拟方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于图神经网络(GNN)的替代框架，用于分子动力学模拟，可直接预测原子位移并学习原子系统潜在的演化算子。与传统分子动力学不同，传统分子动力学依赖于重复的力评估和数值时间积分，而所提出的替代模型可以在不明确计算力的情况下向前传播原子构型。该方法将原子环境表示为图，并结合消息传递层和注意力机制，以捕获金属系统中的局部配位和多体相互作用。使用块状铝的经典分子动力学轨迹进行训练后，替代模型在训练范围内实现了亚埃级别的精度，并在短期到中期时间范围的外推中表现出稳定行为。通过与参考径向分布函数和均方位移趋势的一致性验证了结构和动力学保真度，证明了该模型在点坐标精度之外保留了关键的物理特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a graph neural network (GNN) based surrogate framework for molecular dynamics simulations that directly predicts atomic displacements and learns the underlying evolution operator of an atomistic system. Unlike conventional molecular dynamics, which relies on repeated force evaluations and numerical time integration, the proposed surrogate model propagates atomic configurations forward in time without explicit force computation. The approach represents atomic environments as graphs and combines message-passing layers with attention mechanisms to capture local coordination and many-body interactions in metallic systems. Trained on classical molecular dynamics trajectories of bulk aluminum, the surrogate achieves sub angstrom level accuracy within the training horizon and exhibits stable behavior during short- to mid-horizon temporal extrapolation. Structural and dynamical fidelity are validated through agreement with reference radial distribution functions and mean squared displacement trends, demonstrating that the model preserves key physical signatures beyond pointwise coordinate accuracy. These results establish GNN-based surrogate integrators as a promising and computationally efficient complement to traditional molecular dynamics for accelerated atomistic simulations within a validated regime.</description>
      <author>example@mail.com (Judah Immanuel, Avik Mahata, Aniruddha Maiti)</author>
      <guid isPermaLink="false">2512.21822v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2512.21391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ALETHEIA系统，用于检测社交媒体网络中的恶意账户并预测其行为，通过图神经网络和时间链接预测机制提高了检测效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;影响力活动在网络空间中日益增长，成为政策制定者、版主和研究人员关注的重点问题。&lt;h4&gt;目的&lt;/h4&gt;开发ALETHEIA系统，规范化检测恶意账户，并预测它们在社交媒体网络中的行为。&lt;h4&gt;方法&lt;/h4&gt;分析Reddit和X平台上的影响力活动，建立基于图形的表示方法，结合拓扑和语言特征，使用图神经网络检测恶意用户，并通过在循环神经网络上堆叠GNN构建时间链接预测机制。&lt;h4&gt;主要发现&lt;/h4&gt;基于图形的检测方法优于标准特征；ALETHEIA在大规模网络上可扩展，F1分数提高3.7%；时间链接预测机制平均AUC达到96.6%，能预测巨魔间的互动及巨魔对普通用户的影响。&lt;h4&gt;结论&lt;/h4&gt;利用影响力操作的联网性质（结构信息）对于预测和检测在线空间中的恶意协调活动至关重要。&lt;h4&gt;翻译&lt;/h4&gt;影响力活动是在网络空间中日益增长的问题。政策制定者、版主和研究人员采取了多种途径来应对这些活动，使在线系统对普通用户更安全。为此，我们的论文提出了ALETHEIA系统，它规范化了此类活动中使用的恶意账户（或称巨魔账户）的检测，并预测它们在社交媒体网络中的行为。我们分析了来自不同国家的Reddit和X平台上的影响力活动，并强调建立在活动图形表示基础上的检测管道，结合拓扑和语言特征，比标准的交互和用户特征有所改进。ALETHEIA使用最先进的图神经网络（GNNs）来检测恶意用户，这些网络可扩展到大规模网络，并在之前使用交互特征的标准分类工作中实现了3.7%的F1分数提升。此外，ALETHEIA采用了一种首次为影响力活动构建的时间链接预测机制，通过在循环神经网络（RNN）上堆叠GNN，可以预测未来巨魔之间以及巨魔与普通用户之间的互动，平均AUC为96.6%。ALETHEIA预测巨魔到巨魔的边缘（TTE）和巨魔到用户的边缘（TUE），这可以帮助识别受到恶意影响力影响的普通用户。总体而言，我们的结果突显了在预测和检测在线空间中的恶意协调活动时，利用影响力操作的联网性质（即结构信息）的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Influence campaigns are a growing concern in the online spaces. Policymakers, moderators and researchers have taken various routes to fight these campaigns and make online systems safer for regular users. To this end, our paper presents ALETHEIA, a system that formalizes the detection of malicious accounts (or troll accounts) used in such operations and forecasts their behaviors within social media networks. We analyze influence campaigns on Reddit and X from different countries and highlight that detection pipelines built over a graph-based representation of campaigns using a mix of topological and linguistic features offer improvement over standard interaction and user features. ALETHEIA uses state-of-the-art Graph Neural Networks (GNNs) for detecting malicious users that can scale to large networks and achieve a 3.7% F1-score improvement over standard classification with interaction features in prior work. Furthermore, ALETHEIA employs a first temporal link prediction mechanism built for influence campaigns by stacking a GNN over a Recurrent Neural Network (RNN), which can predict future troll interactions towards other trolls and regular users with an average AUC of 96.6%. ALETHEIA predicts troll-to-troll edges (TTE) and troll-to-user edges (TUE), which can help identify regular users being affected by malicious influence efforts. Overall, our results highlight the importance of utilizing the networked nature of influence operations (i.e., structural information) when predicting and detecting malicious coordinated activity in online spaces.</description>
      <author>example@mail.com (Mohammad Hammas Saeed, Isaiah J. King, Howie Huang)</author>
      <guid isPermaLink="false">2512.21391v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram</title>
      <link>http://arxiv.org/abs/2512.21380v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SENTINEL框架，利用社交媒体信号早期检测网络攻击，通过结合大型语言模型和图神经网络技术，将网络安全讨论与现实世界威胁对齐。&lt;h4&gt;背景&lt;/h4&gt;网络攻击对现代社会技术系统构成严重威胁，攻击者常通过恶意软件、勒索软件等技术手段实施攻击。传统防御机制多为事后响应而非主动预防，而社交媒体讨论可作为检测此类威胁的可靠指标。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用社交媒体信号早期检测网络攻击的框架，通过分析社交媒体上的讨论来识别潜在网络安全威胁。&lt;h4&gt;方法&lt;/h4&gt;SENTINEL框架利用多模态信号，结合大型语言模型进行语言建模和图神经网络进行协调标记，将网络安全讨论与现实世界网络攻击对齐。研究使用了来自16个与网络安全和开源情报相关的Telegram公共频道的数据，涵盖365k条消息。&lt;h4&gt;主要发现&lt;/h4&gt;社交媒体讨论涉及围绕网络威胁的积极对话，使用SENTINEL可将信号与现实世界威胁对齐，F1分数达到0.89。&lt;h4&gt;结论&lt;/h4&gt;利用语言和网络信号预测在线威胁具有重要价值，SENTINEL框架为网络安全威胁的早期检测提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;网络攻击对现代社会技术系统构成严重威胁，通常导致严重的技术和社会后果。攻击者通常通过恶意软件、勒索软件或其他形式的技术利用方法来系统和基础设施。大多数传统的应对这些威胁的机制依赖于事后检测和缓解策略，在网络事件发生后才响应，而不是主动预防。最近的趋势显示，社交媒体讨论可以作为检测此类威胁的可靠指标。恶意行为者经常利用在线平台分发攻击工具、分享攻击知识和协调。专家们也经常预测正在进行的攻击，并在在线空间讨论潜在的漏洞。在这项工作中，我们提出了SENTINEL，一个利用社交媒体信号早期检测网络攻击的框架。SENTINEL利用多模态信号将网络安全讨论与现实世界网络攻击对齐，即通过大型语言模型进行语言建模，通过图神经网络进行协调标记。我们使用了来自16个与网络安全和开源情报相关的公共Telegram频道的数据，涵盖了365k条消息。我们强调，社交媒体讨论涉及围绕网络威胁的积极对话，并利用SENTINEL将信号与现实世界威胁对齐，F1分数为0.89。我们的工作强调了利用语言和网络信号预测在线威胁的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cyberattacks pose a serious threat to modern sociotechnical systems, often resulting in severe technical and societal consequences. Attackers commonly target systems and infrastructure through methods such as malware, ransomware, or other forms of technical exploitation. Most traditional mechanisms to counter these threats rely on post-hoc detection and mitigation strategies, responding to cyber incidents only after they occur rather than preventing them proactively. Recent trends reveal social media discussions can serve as reliable indicators for detecting such threats. Malicious actors often exploit online platforms to distribute attack tools, share attack knowledge and coordinate. Experts too, often predict ongoing attacks and discuss potential breaches in online spaces. In this work, we present SENTINEL, a framework that leverages social media signals for early detection of cyber attacks. SENTINEL aligns cybersecurity discussions to realworld cyber attacks leveraging multi modal signals, i.e., combining language modeling through large language models and coordination markers through graph neural networks. We use data from 16 public channels on Telegram related to cybersecurity and open source intelligence (OSINT) that span 365k messages. We highlight that social media discussions involve active dialogue around cyber threats and leverage SENTINEL to align the signals to real-world threats with an F1 of 0.89. Our work highlights the importance of leveraging language and network signals in predicting online threats.</description>
      <author>example@mail.com (Mohammad Hammas Saeed, Howie Huang)</author>
      <guid isPermaLink="false">2512.21380v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>AstraNav-World: World Model for Foresight Control and Consistency</title>
      <link>http://arxiv.org/abs/2512.21714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了AstraNav-World，一种在开放动态环境中进行具身导航的端到端世界模型，通过统一的概率框架联合推理未来视觉状态和动作序列。&lt;h4&gt;背景&lt;/h4&gt;在开放、动态环境中进行具身导航需要准确预测世界如何随时间演变以及行动如何展开。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够准确预测世界演变和行动展开的具身导航系统，提高在开放环境中的导航能力。&lt;h4&gt;方法&lt;/h4&gt;AstraNav-World框架集成了基于扩散的视频生成器和视觉语言策略，实现预测场景和计划动作的同步更新。训练优化两个互补目标：生成条件动作的多步视觉预测和基于预测视觉导出轨迹。这种双向约束使视觉预测可执行，使决策基于物理一致且与任务相关的未来。&lt;h4&gt;主要发现&lt;/h4&gt;在各种具身导航基准测试中，AstraNav-World提高了轨迹准确性和成功率。消融研究证实了紧密视觉-动作耦合和统一训练的必要性。真实世界测试显示其具有卓越的零样本能力，无需微调即可适应新场景。&lt;h4&gt;结论&lt;/h4&gt;AstraNav-World捕捉了可转移的空间理解和规划相关的导航动态，而非仅拟合特定数据分布。通过在单个生成模型中统一远见视觉和控制，系统更接近于在开放真实世界环境中可靠、可解释和通用的具身智能体。&lt;h4&gt;翻译&lt;/h4&gt;在开放、动态环境中的具身导航需要准确预测世界将如何演变以及行动将如何随时间展开。我们提出了AstraNav-World，一种端到端世界模型，在统一的概率框架内共同推理未来的视觉状态和动作序列。我们的框架将基于扩散的视频生成器与视觉语言策略相结合，使预测场景和计划动作能够同步更新。训练优化了两个互补目标：生成条件动作的多步视觉预测和基于预测视觉导出轨迹。这种双向约束使视觉预测可执行，并使决策保持基于物理一致且与任务相关的未来，减轻了解耦的'先想象后规划'管道中常见的累积错误。在各种具身导航基准测试中的实验显示提高了轨迹准确性和成功率。消融研究证实了紧密视觉-动作耦合和统一训练的必要性，移除任何一个分支都会降低预测质量和策略可靠性。在真实世界测试中，AstraNav-World表现出卓越的零样本能力，无需任何真实世界微调即可适应前所未见的场景。这些结果表明，AstraNav-World捕捉了可转移的空间理解和规划相关的导航动态，而不仅仅是模拟特定的数据分布。总体而言，通过在单个生成模型中统一远见视觉和控制，我们更接近于在开放真实世界环境中可靠、可解释和通用的具身智能体。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决具身导航中的物理一致性和因果一致性问题。当前导航系统通常采用'先想象后行动'的松耦合范式，导致物理不确定性被放大、因果模糊性增加，以及误差随时间累积，最终影响全局规划效果。这个问题在现实中非常重要，因为导航是具身智能的核心能力，能让代理在复杂、未知的真实环境中自主行动；缺乏对物理规律和时间动态的建模是导航失败的主要原因；小的预测偏差会随时间累积，削弱全局规划的有效性；在开放、动态环境中，准确的预测能力对于可靠导航至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有'envision-then-plan'范式的局限性，识别出需要同时推进'想象未来'和'规划未来'两个能力管道。他们借鉴了多个现有工作：世界模型（如LaDi-WM、MoWM）用于预测未来状态；视觉-语言-动作模型（如WorldVLA、CoT-VLA）但改进其松耦合问题；基于扩散的视频生成模型（如Wan2.2-TI2V-5B）提供高质量视觉先验；视觉-语言模型（如Qwen2.5-VL-3B）处理指令和视觉历史。设计上，作者创建了一个统一的生成框架，通过双向约束和同步滚动进行联合优化，设计了3D-RoPE重新排列策略处理多视图输入，提出了稀疏预见调度平衡实时性，实现了两种策略头，并采用两阶段训练策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的生成世界模型，将'想象未来'和'规划未来'紧密绑定在单一概率框架内，通过双向约束和同步滚动确保视觉预测与动作序列之间的一致性。整体流程：1)接收自然语言指令和历史视觉观察；2)VLM规划器处理这些输入并生成视觉-语言嵌入；3)视频生成器基于VLM嵌入预测未来视觉帧；4)动作策略头（Action Former或Diffusion Policy）生成动作序列；5)采用两阶段训练（组件预训练和联合微调）；6)推理时使用稀疏预见调度平衡实时性和计算效率；7)输出预测的未来视觉帧和对应的动作序列，确保两者一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一的生成框架，将想象和规划紧密绑定；2)VLM作为中央规划器提供高级指导；3)3D-RoPE重新排列策略处理多视图输入；4)多模态融合交叉注意力(MMFCA)实现双向信息流；5)稀疏预见调度(SFS)平衡实时性和计算效率；6)双策略头设计提供确定性/概率性动作生成。相比之前工作：不同于传统'envision-then-plan'方法的松耦合，AstraNav-World实现紧耦合减少误差累积；不同于WorldVLA和CoT-VLA仍遵循松耦合范式，AstraNav-World将视频生成和动作生成统一在单一生成过程中；不同于传统世界模型只关注短期预测，AstraNav-World结合VLM长程理解实现长期规划；通过SFS策略在保持性能的同时大幅减少计算开销。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AstraNav-World通过统一视觉预测与动作规划的生成框架，实现了具身导航中物理一致性和因果一致性的显著提升，在多个导航基准测试中取得最先进性能，并展现出卓越的零样本迁移能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled "envision-then-plan" pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.</description>
      <author>example@mail.com (Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang)</author>
      <guid isPermaLink="false">2512.21714v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization</title>
      <link>http://arxiv.org/abs/2512.21769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at https://github.com/AlevLab-dev/BertsWinMAE and https://github.com/AlevLab-dev/GCond. Zenodo repository (DOI: 10.5281/zenodo.17916932) contains source images, training logs, trained models, and code&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BertsWin的新型混合架构，结合了完整的BERT风格令牌屏蔽和Swin Transformer窗口，用于增强3D医学图像自监督学习中的空间上下文学习，解决了标准方法在处理3D体积图像时的局限性。&lt;h4&gt;背景&lt;/h4&gt;自监督学习和Vision Transformers在2D医学成像领域表现优异，但在3D体积图像上的应用存在困难。标准的Masked Autoencoders作为2D领域的先进解决方案，在预训练过程中难以捕捉三维空间关系，尤其是当75%的令牌被丢弃时。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理3D体积图像的架构，保留空间拓扑结构，同时提高计算效率和收敛速度，用于医学图像处理中的自监督学习任务。&lt;h4&gt;方法&lt;/h4&gt;提出BertsWin混合架构，结合：1)完整的BERT风格令牌屏蔽使用Swin Transformer窗口；2)引入完整3D令牌网格保留空间拓扑；3)使用单层局部Swin窗口平滑ViT的二次复杂度；4)引入结构优先损失函数；5)使用GradientConductor优化器。&lt;h4&gt;主要发现&lt;/h4&gt;BertsWin通过保持完整三维空间拓扑，比标准ViT-MAE基线加速语义收敛5.8倍；结合GradientConductor优化器，达到最先进重建保真度所需的训练周期减少15倍(44对660)；实现加速的同时没有计算惩罚；在标准输入分辨率下保持与稀疏ViT基线理论上的FLOP对等，总计算资源显著减少。&lt;h4&gt;结论&lt;/h4&gt;BertsWin架构有效解决了3D医学图像处理中的自监督学习挑战，通过保留空间拓扑结构和优化计算效率，显著提高了训练速度和模型性能，同时保持了与现有方法相当的计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习和Vision Transformers方法在2D医学成像领域的应用显示出有希望的结果，但这些方法在3D体积图像上的使用充满困难。作为2D最先进解决方案的标准Masked Autoencoders，在预训练过程中丢弃75%的令牌时，难以捕捉三维空间关系。我们提出了BertsWin，这是一种混合架构，结合使用Swin Transformer窗口的完整BERT风格令牌屏蔽，以在自监督预训练期间增强3D中的空间上下文学习。与仅处理可见区域的传统MAE不同，BertsWin引入了完整的3D令牌网格（屏蔽和可见的），保留了空间拓扑。并且为了平滑ViT的二次复杂度，使用了单层局部Swin窗口。我们引入了一种结构优先损失函数，并评估了颞下颌关节锥束计算断层扫描的结果。随后的评估包括3D CT扫描上的TMJ分割。我们证明，BertsWin架构通过保持完整的三维空间拓扑，本质上比标准ViT-MAE基线加速了语义收敛5.8倍。此外，当我们提出的GradientConductor优化器结合使用时，完整的BertsWin框架达到最先进重建保真度所需的训练周期减少了15倍（44对660）。分析显示，BertsWin实现了这种加速，而没有通常与密集体积处理相关的计算惩罚。在标准输入分辨率下，该架构保持与稀疏ViT基线理论上的FLOP对等，由于更快收敛，导致总计算资源显著减少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of self-supervised learning (SSL) and Vision Transformers (ViTs) approaches demonstrates promising results in the field of 2D medical imaging, but the use of these methods on 3D volumetric images is fraught with difficulties. Standard Masked Autoencoders (MAE), which are state-of-the-art solution for 2D, have a hard time capturing three-dimensional spatial relationships, especially when 75% of tokens are discarded during pre-training. We propose BertsWin, a hybrid architecture combining full BERT-style token masking using Swin Transformer windows, to enhance spatial context learning in 3D during SSL pre-training. Unlike the classic MAE, which processes only visible areas, BertsWin introduces a complete 3D grid of tokens (masked and visible), preserving the spatial topology. And to smooth out the quadratic complexity of ViT, single-level local Swin windows are used. We introduce a structural priority loss function and evaluate the results of cone beam computed tomography of the temporomandibular joints. The subsequent assessment includes TMJ segmentation on 3D CT scans. We demonstrate that the BertsWin architecture, by maintaining a complete three-dimensional spatial topology, inherently accelerates semantic convergence by a factor of 5.8x compared to standard ViT-MAE baselines. Furthermore, when coupled with our proposed GradientConductor optimizer, the full BertsWin framework achieves a 15-fold reduction in training epochs (44 vs 660) required to reach state-of-the-art reconstruction fidelity. Analysis reveals that BertsWin achieves this acceleration without the computational penalty typically associated with dense volumetric processing. At canonical input resolutions, the architecture maintains theoretical FLOP parity with sparse ViT baselines, resulting in a significant net reduction in total computational resources due to faster convergence.</description>
      <author>example@mail.com (Evgeny Alves Limarenko, Anastasiia Studenikina)</author>
      <guid isPermaLink="false">2512.21769v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data</title>
      <link>http://arxiv.org/abs/2512.21516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的基于对比学习的多视图聚类框架，用于解决不完整和噪声多视图数据中的聚类问题。该方法通过全局图引导和局部图加权两种对比学习策略，分别处理稀疏配对和错误配对问题，在不进行数据填补的情况下取得了优越的聚类性能。&lt;h4&gt;背景&lt;/h4&gt;对比学习在多视图聚类中发挥着重要作用，用于探索不同视图间的互补信息。然而，现实世界中的多视图数据常存在不完整或噪声问题，导致稀疏配对样本或错误配对样本，这严重影响了基于对比学习的多视图聚类方法的有效性。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的基于对比学习的多视图聚类框架，以增强在不完整和噪声多视图数据上的聚类效果，解决稀疏配对和错误配对问题。&lt;h4&gt;方法&lt;/h4&gt;首先，设计全局图引导的对比学习，让所有视图样本构建全局视图亲和图，形成新的样本对以充分探索互补信息；其次，提出局部图加权的对比学习，利用局部邻居生成成对权重，自适应地加强或削弱成对对比学习。该方法无需数据填补，可集成到统一的框架中。&lt;h4&gt;主要发现&lt;/h4&gt;在不完整和噪声设置的多视图数据上进行了大量实验，结果表明该方法与最先进的方法相比取得了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的全局-局部图引导对比学习框架能够有效处理不完整和噪声多视图数据中的聚类问题，通过解决稀疏配对和错误配对挑战，显著提升了多视图聚类的效果。&lt;h4&gt;翻译&lt;/h4&gt;最近，对比学习在探索多视图聚类的互补信息方面发挥着重要作用，并引起了越来越多的关注。然而，现实世界中的多视图数据存在不完整或噪声问题，导致稀疏配对样本或错误配对样本，这严重挑战了基于对比学习的多视图聚类的有效性。也就是说，稀疏配对问题阻碍了多视图聚类提取足够的互补信息，而错误配对问题导致对比学习向错误方向优化模型。为解决这些问题，我们提出了一个统一的基于对比学习的多视图聚类框架，以增强在不完整和噪声多视图数据上的聚类效果。首先，为了克服稀疏配对问题，我们设计了全局图引导的对比学习，其中所有视图样本构建全局视图亲和图，形成新的样本对以充分探索互补信息。其次，为了减轻错误配对问题，我们提出了局部图加权的对比学习，利用局部邻居生成成对权重，自适应地加强或削弱成对对比学习。我们的方法无需数据填补，可以集成到统一的全局-局部图引导对比学习框架中。在不完整和噪声设置的多视图数据上的大量实验表明，与最先进的方法相比，我们的方法取得了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, contrastive learning (CL) plays an important role in exploring complementary information for multi-view clustering (MVC) and has attracted increasing attention. Nevertheless, real-world multi-view data suffer from data incompleteness or noise, resulting in rare-paired samples or mis-paired samples which significantly challenges the effectiveness of CL-based MVC. That is, rare-paired issue prevents MVC from extracting sufficient multi-view complementary information, and mis-paired issue causes contrastive learning to optimize the model in the wrong direction. To address these issues, we propose a unified CL-based MVC framework for enhancing clustering effectiveness on incomplete and noise multi-view data. First, to overcome the rare-paired issue, we design a global-graph guided contrastive learning, where all view samples construct a global-view affinity graph to form new sample pairs for fully exploring complementary information. Second, to mitigate the mis-paired issue, we propose a local-graph weighted contrastive learning, which leverages local neighbors to generate pair-wise weights to adaptively strength or weaken the pair-wise contrastive learning. Our method is imputation-free and can be integrated into a unified global-local graph-guided contrastive learning framework. Extensive experiments on both incomplete and noise settings of multi-view data demonstrate that our method achieves superior performance compared with state-of-the-art approaches.</description>
      <author>example@mail.com (Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu)</author>
      <guid isPermaLink="false">2512.21516v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer</title>
      <link>http://arxiv.org/abs/2512.21233v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种使用触觉手套收集人类操作数据的方法，并通过UniTacHand统一表示解决了人类与机器人触觉数据不匹配问题，实现了高效的触觉策略迁移。&lt;h4&gt;背景&lt;/h4&gt;触觉感知对机器人手实现人类级灵巧操作至关重要，特别是在视觉遮挡场景中。然而，大规模真实世界机器人触觉数据收集困难，限制了其应用。&lt;h4&gt;目的&lt;/h4&gt;提出使用触觉手套收集低成本的人类操作数据，用于基于触觉的机器人策略学习，并解决人类与机器人触觉数据不匹配问题，实现从人类到机器人的策略迁移。&lt;h4&gt;方法&lt;/h4&gt;提出UniTacHand统一表示方法，将人类手和机器人手的触觉信号投影到MANO手模型的形态一致的2D表面空间上，并引入对比学习方法将它们对齐到统一的潜在空间，仅使用10分钟的配对数据进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;实现了从人类到真实机器人的零样本基于触觉的策略迁移，可推广到预训练数据中未见过的物体；通过UniTacHand在混合数据上进行共同训练，比仅使用机器人数据获得更好的性能和数据效率。&lt;h4&gt;结论&lt;/h4&gt;UniTacHand为基于触觉的灵巧手学习提供了通用、可扩展和数据高效的路径。&lt;h4&gt;翻译&lt;/h4&gt;触觉感知对机器人手实现人类级灵巧操作至关重要，特别是在视觉遮挡场景中。然而，大规模真实世界机器人触觉数据收集的困难常常限制了其应用。在本研究中，我们提出使用触觉手套收集低成本的人类操作数据，用于基于触觉的机器人策略学习。人类与机器人触觉数据之间的不匹配使得将从人类数据中学到的策略转移到机器人上具有挑战性。为了弥合这一差距，我们提出了UniTacHand，一种统一表示方法，用于对齐灵巧手捕获的机器人触觉信息与手套获取的人类手触觉。首先，我们将人类手和机器人手的触觉信号投影到MANO手模型的形态一致的2D表面空间上。这种统一标准化了异构数据结构，并内在地将触觉信号嵌入空间上下文。然后，我们引入了一种对比学习方法，将它们对齐到统一的潜在空间，仅使用我们数据收集系统中10分钟的配对数据进行训练。我们的方法实现了从人类到真实机器人的零样本基于触觉的策略迁移，可推广到预训练数据中未见过的物体。我们还证明，通过UniTacHand在混合数据（包括人类和机器人演示）上进行共同训练，比仅使用机器人数据能获得更好的性能和数据效率。UniTacHand为基于触觉的灵巧手学习通用、可扩展和数据高效的路径铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tactile sensing is crucial for robotic hands to achieve human-level dexterous manipulation, especially in scenarios with visual occlusion. However, its application is often hindered by the difficulty of collecting large-scale real-world robotic tactile data. In this study, we propose to collect low-cost human manipulation data using haptic gloves for tactile-based robotic policy learning. The misalignment between human and robotic tactile data makes it challenging to transfer policies learned from human data to robots. To bridge this gap, we propose UniTacHand, a unified representation to align robotic tactile information captured by dexterous hands with human hand touch obtained from gloves. First, we project tactile signals from both human hands and robotic hands onto a morphologically consistent 2D surface space of the MANO hand model. This unification standardizes the heterogeneous data structures and inherently embeds the tactile signals with spatial context. Then, we introduce a contrastive learning method to align them into a unified latent space, trained on only 10 minutes of paired data from our data collection system. Our approach enables zero-shot tactile-based policy transfer from humans to a real robot, generalizing to objects unseen in the pre-training data. We also demonstrate that co-training on mixed data, including both human and robotic demonstrations via UniTacHand, yields better performance and data efficiency compared with using only robotic data. UniTacHand paves a path toward general, scalable, and data-efficient learning for tactile-based dexterous hands.</description>
      <author>example@mail.com (Chi Zhang, Penglin Cai, Haoqi Yuan, Chaoyi Xu, Zongqing Lu)</author>
      <guid isPermaLink="false">2512.21233v2</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models</title>
      <link>http://arxiv.org/abs/2512.21778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Scene-VLM是一个创新的视觉语言模型框架，用于视频场景分割，它联合处理视觉和文本线索，生成具有因果依赖关系的预测，并引入了上下文焦点窗口机制，同时能够提取置信度分数并生成自然语言解释，在标准基准测试中取得了最先进的表现。&lt;h4&gt;背景&lt;/h4&gt;现有的基于编码器的方法在视频场景分割中存在局限性，包括以视觉为中心的偏见、孤立处理每个镜头而不利用序列依赖关系、缺乏叙事理解和可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出Scene-VLM，第一个针对视频场景分割进行微调的视觉语言模型(VLM)框架，以解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;Scene-VLM联合处理视觉和文本线索（包括帧、转录和可选元数据），实现跨连续镜头的多模态推理。模型顺序生成预测，镜头间存在因果依赖关系，并引入上下文焦点窗口机制确保每个镜头级别决策有充分的时序上下文。此外，提出一种从VLM的标记级logits中提取置信度分数的方案，实现可控制的精度-召回权衡。还展示了模型可以通过最少的有针对性监督，对边界决策生成连贯的自然语言解释。&lt;h4&gt;主要发现&lt;/h4&gt;Scene-VLM在标准场景分割基准测试中取得了最先进的表现。在MovieNet上，相比之前领先的方法，Scene-VLM实现了+6 AP和+13.7 F1的显著提升。&lt;h4&gt;结论&lt;/h4&gt;Scene-VLM通过整合视觉和语言信息，有效解决了现有视频场景分割方法的局限性，提供了更好的性能、可解释性和控制能力。&lt;h4&gt;翻译&lt;/h4&gt;将长格式视频分割成语义连贯的场景是大规模视频理解中的基本任务。现有的基于编码器的方法受视觉中心偏见的限制，孤立地分类每个镜头而不利用序列依赖关系，并且缺乏叙事理解和可解释性。在本文中，我们提出了Scene-VLM，这是第一个针对视频场景分割进行微调的视觉语言模型(VLM)框架。Scene-VLM联合处理视觉和文本线索，包括帧、转录和可选元数据，以实现跨连续镜头的多模态推理。模型顺序生成预测，镜头间存在因果依赖关系，并引入上下文焦点窗口机制，确保每个镜头级别决策有充分的时序上下文。此外，我们提出了一种从VLM的标记级logits中提取置信度分数的方案，实现了以往仅限于基于编码器方法的可控精度-召回权衡。此外，我们证明了我们的模型可以通过最少的有针对性监督，对边界决策生成连贯的自然语言解释。我们的方法在标准场景分割基准测试中取得了最先进的性能。例如，在MovieNet上，Scene-VLM比之前领先的方法实现了+6 AP和+13.7 F1的显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.</description>
      <author>example@mail.com (Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky)</author>
      <guid isPermaLink="false">2512.21778v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation</title>
      <link>http://arxiv.org/abs/2512.21734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'Knot Forcing'的新型流式框架，用于实时人像动画，解决了现有方法在视觉质量、时间连贯性和实时性方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;实时人像动画对虚拟助手和实时头像等交互应用至关重要，需要高视觉保真度、时间连贯性、超低延迟和响应式控制。基于扩散的模型质量高但非因果，阻碍流式部署；因果自回归视频生成方法存在误差累积和长期一致性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现高保真度、时间连贯性、交互式人像动画的流式框架，解决现有方法在误差累积、块间运动不连续性和长期一致性下降方面的问题。&lt;h4&gt;方法&lt;/h4&gt;Knot Forcing框架包含三个关键设计：(1)块状生成策略，通过缓存参考图像KV状态保持全局身份特征，使用滑动窗口注意力进行局部时间建模；(2)时间节点模块，重叠相邻块并通过图像到视频条件传递空间时间线索，平滑块间运动过渡；(3)'提前运行'机制，动态更新参考帧时间坐标，使其语义内容领先于当前帧以支持长期连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;Knot Forcing能够在无限序列上实现高保真度、时间连贯性和交互式人像动画，在消费级GPU上达到实时性能并保持强大的视觉稳定性。&lt;h4&gt;结论&lt;/h4&gt;该框架成功解决了实时人像动画中的关键挑战，实现了高质量、时间连贯的动画效果，同时保持实时性能，为交互应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;实时人像动画对于虚拟助手和实时头像等交互应用至关重要，需要高视觉保真度、时间连贯性、超低延迟以及从参考图像和驱动信号等动态输入获得响应式控制。虽然基于扩散的模型能实现强质量，但其非因果性质阻碍了流式部署。因果自回归视频生成方法能够高效的逐帧生成，但存在误差累积、块间运动不连续和长期一致性下降的问题。在这项工作中，我们提出了一种名为Knot Forcing的新型流式框架用于实时人像动画，通过三个关键设计解决这些挑战：(1)块状生成策略，通过缓存参考图像的KV状态保持全局身份特征，并使用滑动窗口注意力进行局部时间建模；(2)时间节点模块，重叠相邻块并通过图像到视频的条件传递空间时间线索，平滑块间运动过渡；(3)'提前运行'机制，在推理过程中动态更新参考帧的时间坐标，使其语义内容领先于当前展开帧以支持长期连贯性。Knot Forcing能够在无限序列上实现高保真度、时间连贯和交互式的人像动画，在消费级GPU上实现实时性能并保持强大的视觉稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A "running ahead" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.</description>
      <author>example@mail.com (Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang)</author>
      <guid isPermaLink="false">2512.21734v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>World-Coordinate Human Motion Retargeting via SAM 3D Body</title>
      <link>http://arxiv.org/abs/2512.21573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从单目视频中恢复世界坐标系人体运动并重定向到人形机器人的轻量级框架，通过冻结感知主干和机器人友好中间表示，实现了稳定的轨迹和可靠的重定向效果。&lt;h4&gt;背景&lt;/h4&gt;从单目视频中恢复世界坐标系人体运动并重定向到人形机器人对于具身智能和机器人技术具有重要意义，但现有方法通常需要复杂的SLAM流程或重型时序模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个轻量级、面向工程的框架，避免复杂SLAM流程或重型时序模型，从单目视频中生成机器人可直接使用的运动数据。&lt;h4&gt;方法&lt;/h4&gt;使用SAM 3D Body作为冻结感知主干，Momentum HumanRig作为机器人友好中间表示；锁定被追踪对象的身份和骨骼尺度参数确保时间一致性；通过低维MHR潜空间的滑动窗口优化平滑预测；使用可微分软足地接触模型和接触感知全局优化恢复物理合理的根轨迹；采用运动学感知的两阶段逆运动学管道将运动重定向到Unitree G1人形机器人。&lt;h4&gt;主要发现&lt;/h4&gt;在真实单目视频上的测试表明，该方法具有稳定的世界轨迹和可靠的机器人重定向效果。&lt;h4&gt;结论&lt;/h4&gt;具有轻量级物理约束的结构化人体表示可以从单目输入中产生机器人就绪的运动。&lt;h4&gt;翻译&lt;/h4&gt;从单目视频中恢复世界坐标系人体运动并重定向到人形机器人对于具身智能和机器人技术具有重要意义。为避免复杂的SLAM流程或重型时序模型，我们提出了一种轻量级、面向工程的框架，利用SAM 3D Body (3DB)作为冻结感知主干，并使用Momentum HumanRig (MHR)表示作为机器人友好的中间表示。我们的方法(i)锁定每个被追踪对象的身份和骨骼尺度参数，强制执行时间一致的骨骼长度；(ii)通过在低维MHR潜空间中的高效滑动窗口优化来平滑每帧预测；(iii)使用可微分的软足地接触模型和接触感知的全局优化来恢复物理上合理的全局根轨迹。最后，我们使用运动学感知的两阶段逆运动学管道将重建的运动重定向到Unitree G1人形机器人。真实单目视频上的结果表明，我们的方法具有稳定的世界轨迹和可靠的机器人重定向效果，表明具有轻量级物理约束的结构化人体表示可以从单目输入中产生机器人就绪的运动。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从单目视频中恢复世界坐标系下的人体运动并将其重定向到人形机器人上的问题。这个问题很重要，因为单目视频成本低且易于获取，而将人体运动转移到人形机器人对具身智能和机器人应用有重大意义。现有方法通常依赖复杂的SLAM系统或重型时序模型，工程开销大，限制了在实际机器人重定向场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到，虽然单目3D人体重建已取得进展，但这些方法主要在相机坐标系中操作，关注视觉保真度而非全局运动。因此，他们探索如何将结构化人体表示与轻量级物理约束结合，生成机器人可用的运动。他们借鉴了SAM 3D Body作为感知骨干，Momentum Human Rig作为中间表示，以及检测-跟踪模块和滑动窗口优化等技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用结构化人体表示结合轻量物理约束，从单目视频中生成机器人可用运动。具体流程：1)视频预处理和跟踪：用3DB处理每帧，用卡尔曼滤波跟踪主体；2)轨迹级一致性处理：锁定身份和骨骼尺度参数，在MHR潜在空间中进行滑动窗口平滑；3)接触感知的全局根优化：使用软接触模型估计物理合理的根轨迹；4)重定向到机器人：通过两阶段逆运动学将运动映射到Unitree G1机器人。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)面向工程的3DB世界坐标系运动重定向管道；2)轨迹级身份和骨骼尺度锁定策略；3)接触感知的全局根优化方案；4)在实际人形机器人上验证的完整系统。相比之前工作，该方法避免了复杂SLAM或重型时序模型，使用MHR保持骨骼长度一致性，优先考虑鲁棒性和实际部署性而非绝对视觉准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种轻量级、面向工程的管道，通过结构化人体表示和最小物理约束，从单目视频中生成机器人可用的世界坐标系人体运动，并在实际人形机器人上成功展示了其有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering world-coordinate human motion from monocular videos with humanoid robot retargeting is significant for embodied intelligence and robotics. To avoid complex SLAM pipelines or heavy temporal models, we propose a lightweight, engineering-oriented framework that leverages SAM 3D Body (3DB) as a frozen perception backbone and uses the Momentum HumanRig (MHR) representation as a robot-friendly intermediate. Our method (i) locks the identity and skeleton-scale parameters of per tracked subject to enforce temporally consistent bone lengths, (ii) smooths per-frame predictions via efficient sliding-window optimization in the low-dimensional MHR latent space, and (iii) recovers physically plausible global root trajectories with a differentiable soft foot-ground contact model and contact-aware global optimization. Finally, we retarget the reconstructed motion to the Unitree G1 humanoid using a kinematics-aware two-stage inverse kinematics pipeline. Results on real monocular videos show that our method has stable world trajectories and reliable robot retargeting, indicating that structured human representations with lightweight physical constraints can yield robot-ready motion from monocular input.</description>
      <author>example@mail.com (Zhangzheng Tum, Kailun Su, Shaolong Zhu, Yukun Zheng)</author>
      <guid isPermaLink="false">2512.21573v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation</title>
      <link>http://arxiv.org/abs/2512.21402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种数据驱动的短视频内容评估框架，利用视觉语言模型提取视听特征并预测观众参与度，相比传统评估方法更具可解释性和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;现有短视频评估框架如VideoScore-2仅评估视觉和语义保真度，未能捕捉特定视听属性如何真正影响观众参与度。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提取视听特征、聚类为可解释因素并预测短视频教育内容参与度的评估框架。&lt;h4&gt;方法&lt;/h4&gt;使用视觉语言模型提取无监督视听特征，将其聚类为可解释因素，训练回归评估器预测参与度，并构建YouTube Shorts数据集分析特征与参与行为的关系。&lt;h4&gt;主要发现&lt;/h4&gt;预测参与度与实际参与度之间存在强相关性，该轻量级基于特征的评估器相比传统指标（如SSIM、FID）提供了更可解释和可扩展的评估。&lt;h4&gt;结论&lt;/h4&gt;通过基于多模态特征重要性和以人为中心的参与信号进行评估，这种方法推动了稳健和可解释的视频理解发展。&lt;h4&gt;翻译&lt;/h4&gt;评估短视频内容需要超越表面质量指标，转向与人类一致的多模态推理。虽然现有的VideoScore-2等框架评估视觉和语义保真度，但它们没有捕捉特定视听属性如何真正驱动观众参与。在这项工作中，我们提出了一个数据驱动的评估框架，使用视觉语言模型提取无监督视听特征，将其聚类为可解释因素，并训练基于回归的评估器来预测短视频教育内容的参与度。我们精心策划的YouTube Shorts数据集使系统能分析VLM派生特征如何与人类参与行为相关。实验显示预测参与度和实际参与度之间存在强相关性，表明与传统指标相比，我们这种轻量级基于特征的评估器提供了可解释且可扩展的评估。通过将评估基于多模态特征重要性和以人为中心的参与信号，我们的方法朝着稳健和可解释的视频理解迈进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating short-form video content requires moving beyond surface-level quality metrics toward human-aligned, multimodal reasoning. While existing frameworks like VideoScore-2 assess visual and semantic fidelity, they do not capture how specific audiovisual attributes drive real audience engagement. In this work, we propose a data-driven evaluation framework that uses Vision-Language Models (VLMs) to extract unsupervised audiovisual features, clusters them into interpretable factors, and trains a regression-based evaluator to predict engagement on short-form edutainment videos. Our curated YouTube Shorts dataset enables systematic analysis of how VLM-derived features relate to human engagement behavior. Experiments show strong correlations between predicted and actual engagement, demonstrating that our lightweight, feature-based evaluator provides interpretable and scalable assessments compared to traditional metrics (e.g., SSIM, FID). By grounding evaluation in both multimodal feature importance and human-centered engagement signals, our approach advances toward robust and explainable video understanding.</description>
      <author>example@mail.com (Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar)</author>
      <guid isPermaLink="false">2512.21402v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Modeling high dimensional point clouds with the spherical cluster model</title>
      <link>http://arxiv.org/abs/2512.21960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Main text: 4 figures, 15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种称为球形聚类模型（SC）的参数化聚类模型，通过球体近似有限点集，并提出了一个精确求解器。实验显示该算法在多种维度数据集上表现优异，尤其在高维数据分析中有直接应用价值。&lt;h4&gt;背景&lt;/h4&gt;参数化聚类模型是一种统计模型，为定义聚类的点提供几何洞察。球形聚类模型通过球体近似有限点集，其中η=0时退化为K均值聚类中使用的质心。&lt;h4&gt;目的&lt;/h4&gt;研究球形聚类模型的拟合问题，开发精确求解器，并探索该模型在高维数据分析中的应用。&lt;h4&gt;方法&lt;/h4&gt;1. 展示拟合球形聚类模型是一个严格凸但非光滑的组合优化问题；2. 提出使用Clarke梯度的精确求解器，基于从超球面排列定义的分层胞复形；3. 在多种维度（9到10,000）的数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;1. 对于小/中等维度的数据集和小η值，以及高维数据集（d&gt;100），精确算法比基于BFGS的启发式方法快几个数量级；2. SC模型的中心表现为参数化高维中位数。&lt;h4&gt;结论&lt;/h4&gt;球形聚类模型对高维多元数据分析有直接应用价值，将其应用于球形混合模型的设计将在后续论文中报告。&lt;h4&gt;翻译&lt;/h4&gt;参数化聚类模型是一种统计模型，为定义聚类的点提供几何洞察。球形聚类模型（SC）通过球体S(c,r)来近似有限点集P⊂ℝ^d，具体如下：将r取为介于中心c和数据点之间距离的标准差的一个分数η∈(0,1)（超参数），SC模型的成本是所有位于球体S外部的数据点关于S的幂距离之和。SC模型的中心c是使该成本最小化的点。注意，η=0产生K均值聚类中使用的著名质心。我们做出了三项贡献。首先，我们展示了拟合球形聚类产生一个严格凸但不光滑的组合优化问题。其次，我们提出了一个精确求解器，使用Clarke梯度在从超球面排列定义的适当分层胞复形上工作。最后，我们在多种数据集上进行了实验，维度从d=9到d=10,000，主要有两个观察结果。首先，对于小/中等维度的数据集和小η值，以及对于高维数据集（d&gt;100），精确算法比基于BFGS的启发式方法快几个数量级，无论η值如何。其次，SC模型的中心表现为参数化高维中位数。SC模型对高维多元数据分析有直接应用价值，将其应用于球形混合模型的设计将在后续论文中报告。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A parametric cluster model is a statistical model providing geometric insights onto the points defining a cluster. The {\em spherical cluster model} (SC) approximates a finite point set $P\subset \mathbb{R}^d$ by a sphere $S(c,r)$ as follows. Taking $r$ as a fraction $η\in(0,1)$ (hyper-parameter) of the std deviation of distances between the center $c$ and the data points, the cost of the SC model is the sum over all data points lying outside the sphere $S$ of their power distance with respect to $S$. The center $c$ of the SC model is the point minimizing this cost. Note that $η=0$ yields the celebrated center of mass used in KMeans clustering. We make three contributions.  First, we show fitting a spherical cluster yields a strictly convex but not smooth combinatorial optimization problem. Second, we present an exact solver using the Clarke gradient on a suitable stratified cell complex defined from an arrangement of hyper-spheres. Finally, we present experiments on a variety of datasets ranging in dimension from $d=9$ to $d=10,000$, with two main observations. First, the exact algorithm is orders of magnitude faster than BFGS based heuristics for datasets of small/intermediate dimension and small values of $η$, and for high dimensional datasets (say $d&gt;100$) whatever the value of $η$. Second, the center of the SC model behave as a parameterized high-dimensional median.  The SC model is of direct interest for high dimensional multivariate data analysis, and the application to the design of mixtures of SC will be reported in a companion paper.</description>
      <author>example@mail.com (Frédéric Cazals, Antoine Commaret, Louis Goldenberg)</author>
      <guid isPermaLink="false">2512.21960v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>CrownGen: Patient-customized Crown Generation via Point Diffusion Model</title>
      <link>http://arxiv.org/abs/2512.21890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CrownGen是一个自动化患者定制牙冠设计的生成框架，通过去噪扩散模型和新型牙齿级别点云表示解决了牙冠设计中的劳动密集型瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;牙冠设计是修复牙科中的一个劳动密集型的瓶颈问题，需要大量专业时间和技能。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化系统，用于生成患者定制的牙冠设计，以减少设计时间和成本，同时保持高质量。&lt;h4&gt;方法&lt;/h4&gt;使用基于去噪扩散模型的生成框架，结合牙齿级别点云表示，包含边界预测模块和基于扩散的生成模块，可在单次推理中合成多颗牙齿的高保真形态。&lt;h4&gt;主要发现&lt;/h4&gt;在496个外部扫描和26个修复病例的临床研究中验证，CrownGen在几何保真度上超越最先进模型，显著减少设计时间，且牙医评估确认其质量与专家手动设计相当。&lt;h4&gt;结论&lt;/h4&gt;CrownGen通过自动化复杂假体建模，提供了可扩展的解决方案，可降低成本、缩短周转时间，提高患者获得高质量牙科护理的机会。&lt;h4&gt;翻译&lt;/h4&gt;牙冠设计仍然是修复牙科中的一个劳动密集型瓶颈。我们提出了CrownGen，一个生成框架，使用一种新型的牙齿级别点云表示上的去噪扩散模型，自动化患者定制的牙冠设计。该系统采用两个核心组件：边界预测模块用于建立空间先验，以及基于扩散的生成模块用于在单次推理中合成多颗牙齿的高保真形态。我们通过在496个外部扫描上的定量基准测试和26个修复病例的临床研究验证了CrownGen。结果表明，CrownGen在几何保真度上超越了最先进的模型，并显著减少了主动设计时间。经过培训的牙医的临床评估确认，CrownGen辅助的牙冠在质量上与专家技术人员使用手动工作流程生产的牙冠相比没有统计学上的差异。通过自动化复杂的假体建模，CrownGen提供了一个可扩展的解决方案，可以降低成本、缩短周转时间，并增强患者获得高质量牙科护理的机会。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决牙冠设计自动化的问题。在牙科修复领域，牙冠设计目前仍是一个劳动密集型的瓶颈，技师需要手动调整模板，每个牙冠可能需要超过一小时，且随着修复数量增加而线性增长。这个问题很重要，因为它直接影响患者获得高质量牙科护理的成本、等待时间和可及性，同时全球口腔疾病负担持续增加，牙齿脱落严重影响咀嚼功能、面部美观和生活质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：早期2D图像合成方法无法捕捉完整3D解剖结构；3D体素方法在高分辨率临床保真度方面存在困难；而点云方法虽先进但只能生成单颗牙冠且依赖预备好的基牙。作者设计CrownGen时借鉴了去噪扩散模型技术，并创新性地提出将牙列分解为独立的牙齿级点云，而非单一整体。作者还提出了距离加权牙间注意力(DITA)机制来建模牙齿间关系，并采用两阶段伪牙冠训练策略来解决数据异质性问题，使模型能够利用部分无牙颌的临床数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; CrownGen的核心思想是将牙列分解为独立的牙齿级点云，使用去噪扩散模型生成患者定制的牙冠，并通过DITA机制建模牙齿间关系。整体流程包括：1)数据预处理和牙齿分割；2)边界预测模块为每个目标位置预测圆柱形边界作为空间先验；3)基于点云扩散的生成模块以上下文牙齿和预测边界为条件，通过去噪过程生成牙冠点云；4)使用泊松表面重建将点云转换为不透水网格；5)采用两阶段训练策略，先在完整牙列数据上训练，再利用生成伪牙冠扩展数据集进行微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; CrownGen的关键创新点包括：1)首次实现多牙冠一次性生成能力，可在一个推理过程中生成任意数量的解剖学协调修复体；2)创新的牙齿级表示方法，将牙列分解为独立牙齿对象集合；3)边界预测模块提供明确空间先验；4)距离加权牙间注意力(DITA)机制优先考虑局部上下文；5)可扩展的数据利用策略，能利用大规模部分无牙颌临床数据。相比之前工作，CrownGen在多牙修复场景中性能稳定，不依赖基牙预备，能利用不完美临床数据，且将牙列处理为独立牙齿对象而非单一整体。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CrownGen通过创新的牙齿级点云表示和去噪扩散模型，首次实现了在一个推理过程中自动生成任意数量患者定制牙冠的能力，显著提高了牙科修复设计的效率和可及性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital crown design remains a labor-intensive bottleneck in restorative dentistry. We present \textbf{CrownGen}, a generative framework that automates patient-customized crown design using a denoising diffusion model on a novel tooth-level point cloud representation. The system employs two core components: a boundary prediction module to establish spatial priors and a diffusion-based generative module to synthesize high-fidelity morphology for multiple teeth in a single inference pass. We validated CrownGen through a quantitative benchmark on 496 external scans and a clinical study of 26 restoration cases. Results demonstrate that CrownGen surpasses state-of-the-art models in geometric fidelity and significantly reduces active design time. Clinical assessments by trained dentists confirmed that CrownGen-assisted crowns are statistically non-inferior in quality to those produced by expert technicians using manual workflows. By automating complex prosthetic modeling, CrownGen offers a scalable solution to lower costs, shorten turnaround times, and enhance patient access to high-quality dental care.</description>
      <author>example@mail.com (Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen)</author>
      <guid isPermaLink="false">2512.21890v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration</title>
      <link>http://arxiv.org/abs/2512.21831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 19 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;XET-V2X是一种多模态融合端到端跟踪框架，通过双层空间交叉注意力模块和特征聚合策略，在V2X场景中实现了鲁棒且时间稳定的感知性能&lt;h4&gt;背景&lt;/h4&gt;多视图协同感知和多模态融合对自动驾驶中可靠的三维时空理解至关重要，特别是在遮挡、有限视角和V2X场景中的通信延迟情况下&lt;h4&gt;目的&lt;/h4&gt;提出XET-V2X，一种用于V2X协作的多模态融合端到端跟踪框架，在共享时空表示中统一多视图多模态感知&lt;h4&gt;方法&lt;/h4&gt;引入基于多尺度可变形注意力的双层空间交叉注意力模块，首先聚合多视图图像特征增强语义一致性，然后通过更新的空间查询引导点云融合，实现有效跨模态交互同时减少计算开销&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界V2X-Seq-SPD数据集和模拟V2X-Sim-V2V、V2X-Sim-V2I基准测试上，XET-V2X在不同通信延迟条件下检测和跟踪性能有持续改进，定量结果和定性可视化表明其在复杂交通场景中实现了鲁棒且时间稳定的感知&lt;h4&gt;结论&lt;/h4&gt;XET-V2X框架能够有效处理V2X场景中的挑战，提供可靠的3D时空理解&lt;h4&gt;翻译&lt;/h4&gt;多视图协同感知和多模态融合对自动驾驶中可靠的三维时空理解至关重要，特别是在遮挡、有限视角和V2X场景中的通信延迟情况下。本文提出了XET-V2X，一种用于V2X协作的多模态融合端到端跟踪框架，在共享时空表示中统一了多视图多模态感知。为了高效对齐异构视角和模态，XET-V2X引入了基于多尺度可变形注意力的双层空间交叉注意力模块。多视图图像特征首先被聚合以增强语义一致性，然后由更新的空间查询引导的点云融合，实现有效的跨模态交互同时减少计算开销。在真实世界V2X-Seq-SPD数据集和模拟V2X-Sim-V2V、V2X-Sim-V2I基准测试上的实验表明，在不同通信延迟条件下，检测和跟踪性能有持续改进。定量结果和定性可视化都表明XET-V2X在复杂交通场景中实现了鲁棒且时间稳定的感知。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶中的3D时空感知问题，特别是在V2X环境下的多视角协同感知和多模态融合。这个问题很重要，因为在复杂交通场景中，单一车辆或单一传感器的感知能力有限，无法完全覆盖所有情况。通过V2X协同感知可以扩展感知范围，减少遮挡，提高检测和跟踪的可靠性，从而增强自动驾驶的安全性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从三个维度进行思考：1)空间互补性：多视角可提供互补观察，扩展感知范围；2)模态多样性：激光雷达和摄像头具有互补特性，多模态融合可提高鲁棒性；3)时间连续性：需要时间建模保持身份一致性。作者借鉴了多视图协同感知(如V2VNet)、多模态融合(如PointPainting、Transformer架构)、时间信息融合(如循环机制)和端到端多目标跟踪(如MOTR)等现有工作，并将它们整合到一个统一框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过统一的端到端框架，联合建模多视图协作、多模态融合和时间演化，解决V2X环境下的3D时空感知问题。整体流程：1)多视图多模态特征提取(点云用PointPillars，图像用ResNet-101)；2)特征级图像传输减少带宽需求；3)双层空间交叉注意力模块融合特征(先图像后点云)；4)基于MOTR框架进行端到端检测和跟踪。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一的端到端框架整合多视图协作、多模态融合和时间建模；2)双层跨模态跨视图交互模块；3)共享BEV表示实现跨视图一致性；4)特征级传输策略。相比之前工作，XET-V2X将多视图、多模态和时间建模整合在一个框架中，采用端到端学习而非模块化方法，并引入了双层注意力机制和统一的时空建模。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了XET-V2X，一个统一的端到端3D时空感知框架，通过多视图协同、多模态融合和V2X协作，显著提升了自动驾驶环境中的物体检测和跟踪性能，特别是在遮挡、有限视野和通信延迟等挑战场景下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.</description>
      <author>example@mail.com (Zhenwei Yang, Yibo Ai, Weidong Zhang)</author>
      <guid isPermaLink="false">2512.21831v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction</title>
      <link>http://arxiv.org/abs/2512.21707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures, Accepted by AAAI 2026 (oral)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为时空无限制专家混合模型(ST-MoE)的新方法，用于多人运动预测，能够灵活捕捉复杂时空依赖关系并降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;多人运动预测中全面灵活地捕捉复杂时空依赖关系至关重要，但现有方法存在两个主要局限性：时空表示不够灵活和计算成本高。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一种既能灵活探索人体运动中的复杂时空依赖关系又能显著降低计算成本的模型。&lt;h4&gt;方法&lt;/h4&gt;提出时空无限制专家混合模型(ST-MoE)，包含四种不同类型的时空专家，每种专家专门捕捉不同的空间或时间依赖关系；引入双向时空Mamba作为专家，通过不同组合共享双向时间和空间Mamba，实现模型效率和参数经济。&lt;h4&gt;主要发现&lt;/h4&gt;在四个多人基准数据集上的实验表明，该方法在准确性上优于最先进的方法，同时模型参数减少41.38%，训练速度提升3.6倍。&lt;h4&gt;结论&lt;/h4&gt;ST-MoE模型成功解决了多人运动预测中的时空表示灵活性和计算效率问题，为未来研究提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;全面灵活地捕捉人体运动的复杂时空依赖关系对多人运动预测至关重要。现有方法面临两个主要局限性：i) 由于依赖位置编码来捕捉时空信息，导致时空表示不够灵活；ii) 传统注意力机制的二次时间复杂度导致高计算成本。为克服这些局限性，我们提出时空无限制专家混合模型(ST-MoE)，能灵活探索人体运动中的复杂时空依赖关系并显著降低计算成本。为自适应挖掘人体运动中的复杂时空模式，我们的模型纳入了四种不同类型的时空专家，每种专家专门捕捉不同的空间或时间依赖关系。为在整合多个专家时减少潜在计算开销，我们引入双向时空Mamba作为专家，通过不同组合共享双向时间和空间Mamba，实现模型效率和参数经济。在四个多人基准数据集上的广泛实验表明，我们的方法不仅在准确性上优于最先进水平，还减少了41.38%的模型参数，实现了3.6倍的训练加速。代码可在https://github.com/alanyz106/ST-MoE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehensively and flexibly capturing the complex spatio-temporal dependencies of human motion is critical for multi-person motion prediction. Existing methods grapple with two primary limitations: i) Inflexible spatiotemporal representation due to reliance on positional encodings for capturing spatiotemporal information. ii) High computational costs stemming from the quadratic time complexity of conventional attention mechanisms. To overcome these limitations, we propose the Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE), which flexibly explores complex spatio-temporal dependencies in human motion and significantly reduces computational cost. To adaptively mine complex spatio-temporal patterns from human motion, our model incorporates four distinct types of spatiotemporal experts, each specializing in capturing different spatial or temporal dependencies. To reduce the potential computational overhead while integrating multiple experts, we introduce bidirectional spatiotemporal Mamba as experts, each sharing bidirectional temporal and spatial Mamba in distinct combinations to achieve model efficiency and parameter economy. Extensive experiments on four multi-person benchmark datasets demonstrate that our approach not only outperforms state-of-art in accuracy but also reduces model parameter by 41.38% and achieves a 3.6x speedup in training. The code is available at https://github.com/alanyz106/ST-MoE.</description>
      <author>example@mail.com (Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang)</author>
      <guid isPermaLink="false">2512.21707v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning</title>
      <link>http://arxiv.org/abs/2512.21924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Medical Image Analysis (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新型PHI重建框架，通过解耦表示模块和边缘到图像恢复模块，解决了脑部MRI病变检测中无监督学习方法面临的泛化能力有限和性能受限问题，在多个数据集上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;脑部MRI中各种病变的检测在临床上至关重要，但由于病变多样性和成像条件变化而具有挑战性。当前无监督学习方法主要通过正常样本学习将异常图像重建为伪健康图像(PHIs)，然后分析图像差异来检测异常。&lt;h4&gt;目的&lt;/h4&gt;解决当前无监督模型在脑部MRI病变检测中的两个主要限制：对多模态和多中心MRI的泛化能力有限，以及由于异常残差传播导致的性能受限。&lt;h4&gt;方法&lt;/h4&gt;提出两个新模块形成新的PHI重建框架：1)解耦表示模块，将脑部MRI解耦为成像信息和基本成像不变解剖图像，引入脑部解剖先验和可微的一热编码算子增强解耦稳定性；2)边缘到图像恢复模块，从解剖图像的高频边缘信息恢复解剖表示并重新耦合成像信息，通过仅边缘输入减少异常像素输入。&lt;h4&gt;主要发现&lt;/h4&gt;在9个公共数据集(4,443名患者的MRI)上评估，该方法优于17种最先进方法，在AP和DSC指标上分别实现了+18.32%和+13.64%的绝对改进。&lt;h4&gt;结论&lt;/h4&gt;提出的解耦表示和边缘到图像恢复模块有效解决了现有无监督学习方法在脑部MRI病变检测中的局限性，显著提高了检测性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;脑部MRI中各种病变的检测在临床上至关重要，但由于病变多样性和成像条件变化而具有挑战性。当前无监督学习方法主要通过正常样本学习将异常图像重建为伪健康图像(PHIs)，然后分析图像差异来检测异常。然而，这些无监督模型面临两个显著限制：由于依赖正常训练数据中的特定成像信息，其泛化能力受限，难以适应多模态和多中心MRI；由于异常残差从输入图像传播到重建的PHIs，性能受限。为解决这些限制，提出了两个新模块，形成新的PHI重建框架。首先，提出了解耦表示模块，通过将脑部MRI解耦为成像信息和基本成像不变解剖图像来提高泛化能力，确保重建专注于解剖结构。具体而言，引入脑部解剖先验和可微的一热编码算子来约束解耦结果并增强解耦稳定性。其次，设计了边缘到图像恢复模块，通过从解剖图像的高频边缘信息恢复解剖表示，然后重新耦合解耦的成像信息，重建高质量PHIs。该模块通过仅边缘输入减少异常像素输入来抑制PHI中的异常残差，同时利用边缘中保留的结构细节有效重建正常区域。在9个公共数据集(来自多个中心的4,443名患者的MRI)上评估，我们的方法优于17种最先进方法，在AP和DSC指标上分别实现了+18.32%和+13.64%的绝对改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.media.2025.103922&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detection of various lesions in brain MRI is clinically critical, but challenging due to the diversity of lesions and variability in imaging conditions. Current unsupervised learning methods detect anomalies mainly through reconstructing abnormal images into pseudo-healthy images (PHIs) by normal samples learning and then analyzing differences between images. However, these unsupervised models face two significant limitations: restricted generalizability to multi-modality and multi-center MRIs due to their reliance on the specific imaging information in normal training data, and constrained performance due to abnormal residuals propagated from input images to reconstructed PHIs. To address these limitations, two novel modules are proposed, forming a new PHI reconstruction framework. Firstly, the disentangled representation module is proposed to improve generalizability by decoupling brain MRI into imaging information and essential imaging-invariant anatomical images, ensuring that the reconstruction focuses on the anatomy. Specifically, brain anatomical priors and a differentiable one-hot encoding operator are introduced to constrain the disentanglement results and enhance the disentanglement stability. Secondly, the edge-to-image restoration module is designed to reconstruct high-quality PHIs by restoring the anatomical representation from the high-frequency edge information of anatomical images, and then recoupling the disentangled imaging information. This module not only suppresses abnormal residuals in PHI by reducing abnormal pixels input through edge-only input, but also effectively reconstructs normal regions using the preserved structural details in the edges. Evaluated on nine public datasets (4,443 patients' MRIs from multiple centers), our method outperforms 17 SOTA methods, achieving absolute improvements of +18.32% in AP and +13.64% in DSC.</description>
      <author>example@mail.com (Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang)</author>
      <guid isPermaLink="false">2512.21924v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning</title>
      <link>http://arxiv.org/abs/2512.21702v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in 2025 28th International Conference on Computer and Information Technology (ICCIT)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对孟加拉语深度伪造音频检测问题进行了系统评估，通过零样本推理和微调两种方法比较了多种深度学习模型的性能，发现微调显著提升了检测效果。&lt;h4&gt;背景&lt;/h4&gt;语音合成和语音转换系统的快速发展使深度伪造音频成为主要的安全威胁，而孟加拉语深度伪造检测领域尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究使用BanglaFake数据集自动检测孟加拉语音频深度伪造，为这一低资源语言提供首个系统基准。&lt;h4&gt;方法&lt;/h4&gt;评估了Wav2Vec2-XLSR-53、Whisper、PANNsCNN14、WavLM和Audio Spectrogram Transformer等预训练模型的零样本推理能力，并对Wav2Vec2-Base、LCNN、LCNN-Attention、ResNet18、ViT-B16和CNN-BiLSTM等架构进行了微调。&lt;h4&gt;主要发现&lt;/h4&gt;零样本结果显示检测能力有限，最佳模型Wav2Vec2-XLSR-53仅达到53.80%准确率；微调后性能显著提升，ResNet18表现最优，达到79.17%准确率、79.12% F1分数、84.37% AUC和24.35% EER。&lt;h4&gt;结论&lt;/h4&gt;微调深度学习模型在孟加拉语深度伪造音频检测中表现出色，为低资源语言的深度伪造检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;语音合成和语音转换系统的快速增长使深度伪造音频成为主要的安全问题。孟加拉语深度伪造检测在很大程度上仍未被探索。在这项工作中，我们使用BanglaFake数据集研究孟加拉语音频深度伪造的自动检测。我们评估了几个预训练模型的零样本推理能力，包括Wav2Vec2-XLSR-53、Whisper、PANNsCNN14、WavLM和音频频谱变换器。零样本结果显示检测能力有限。最佳模型Wav2Vec2-XLSR-53达到53.80%的准确率、56.60%的AUC和46.20%的EER。然后，我们为孟加拉语深度伪造检测微调了多种架构，包括Wav2Vec2-Base、LCNN、LCNN-Attention、ResNet18、ViT-B16和CNN-BiLSTM。微调后的模型显示出强大的性能提升。ResNet18达到最高的79.17%准确率、79.12%的F1分数、84.37%的AUC和24.35%的EER。实验结果证实微调显著优于零样本推理。这项研究提供了孟加拉语深度伪造音频检测的第一个系统基准。它强调了微调后的深度学习模型在低资源语言中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of speech synthesis and voice conversion systems has made deepfake audio a major security concern. Bengali deepfake detection remains largely unexplored. In this work, we study automatic detection of Bengali audio deepfakes using the BanglaFake dataset. We evaluate zeroshot inference with several pretrained models. These include Wav2Vec2-XLSR-53, Whisper, PANNsCNN14, WavLM and Audio Spectrogram Transformer. Zero-shot results show limited detection ability. The best model, Wav2Vec2-XLSR-53, achieves 53.80% accuracy, 56.60% AUC and 46.20% EER. We then f ine-tune multiple architectures for Bengali deepfake detection. These include Wav2Vec2-Base, LCNN, LCNN-Attention, ResNet18, ViT-B16 and CNN-BiLSTM. Fine-tuned models show strong performance gains. ResNet18 achieves the highest accuracy of 79.17%, F1 score of 79.12%, AUC of 84.37% and EER of 24.35%. Experimental results confirm that fine-tuning significantly improves performance over zero-shot inference. This study provides the first systematic benchmark of Bengali deepfake audio detection. It highlights the effectiveness of f ine-tuned deep learning models for this low-resource language.</description>
      <author>example@mail.com (Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman)</author>
      <guid isPermaLink="false">2512.21702v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Semantic Transfer Learning for High-Dimensional Linear Regression</title>
      <link>http://arxiv.org/abs/2512.21689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了跨语义迁移学习(CSTL)框架，用于处理高维线性回归中的特征不对齐问题，通过比较目标域和源域系数实现更有效的知识迁移&lt;h4&gt;背景&lt;/h4&gt;现有高维线性回归迁移学习方法假设域间特征对齐，限制了在语义匹配特征上的应用，而现实中目标域和源域的不同特征可能扮演相似预测角色&lt;h4&gt;目的&lt;/h4&gt;利用更广泛的跨语义相似性，提出CSTL框架捕获域间潜在关系，保留可迁移信号同时过滤源特定噪声&lt;h4&gt;方法&lt;/h4&gt;CSTL通过比较每个目标系数与所有源系数，利用加权融合惩罚捕获潜在关系，权重由SCAD惩罚导数推导，使用ADMM算法实现计算效率&lt;h4&gt;主要发现&lt;/h4&gt;理论上在温和条件下CSTL能以压倒性概率实现oracle估计器，实证结果显示其在跨语义和部分信号相似性设置下均优于现有方法&lt;h4&gt;结论&lt;/h4&gt;CSTL框架有效解决了特征不对齐的高维线性回归迁移学习问题，通过捕获跨语义相似性实现了更好的性能&lt;h4&gt;翻译&lt;/h4&gt;当前的高维线性回归迁移学习方法假设域间特征对齐，限制了它们在语义匹配特征上的适用性。然而，在许多现实场景中，目标域和源域中的不同特征可以扮演相似的预测角色，形成一种跨语义相似性。为了利用这种更广泛的迁移能力，我们提出了跨语义迁移学习(CSTL)框架。它通过比较每个目标系数与所有源系数，利用加权融合惩罚来捕获潜在关系。权重由SCAD惩罚的导数推导得出，有效近似了理想的加权方案，保留了可迁移信号同时过滤掉源特定噪声。为了计算效率，我们使用交替方向乘子法(ADMM)实现了CSTL。理论上，我们建立了在温和条件下，CSTL能够以压倒性概率实现oracle估计器。来自模拟和真实数据应用的实证结果证实，CSTL在跨语义和部分信号相似性设置下都优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current transfer learning methods for high-dimensional linear regression assume feature alignment across domains, restricting their applicability to semantically matched features. In many real-world scenarios, however, distinct features in the target and source domains can play similar predictive roles, creating a form of cross-semantic similarity. To leverage this broader transferability, we propose the Cross-Semantic Transfer Learning (CSTL) framework. It captures potential relationships by comparing each target coefficient with all source coefficients through a weighted fusion penalty. The weights are derived from the derivative of the SCAD penalty, effectively approximating an ideal weighting scheme that preserves transferable signals while filtering out source-specific noise. For computational efficiency, we implement CSTL using the Alternating Direction Method of Multipliers (ADMM). Theoretically, we establish that under mild conditions, CSTL achieves the oracle estimator with overwhelming probability. Empirical results from simulations and a real-data application confirm that CSTL outperforms existing methods in both cross-semantic and partial signal similarity settings.</description>
      <author>example@mail.com (Jiancheng Jiang, Xuejun Jiang, Hongxia Jin)</author>
      <guid isPermaLink="false">2512.21689v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism</title>
      <link>http://arxiv.org/abs/2512.21452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in *IEEE Transactions on Geoscience and Remote Sensing*&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种全面的框架，用于自动化地面穿透雷达(GPR)图像中的道路缺陷检测，解决了传统方法依赖主观专业知识的局限性。&lt;h4&gt;背景&lt;/h4&gt;地面穿透雷达(GPR)已成为评估地下道路缺陷的关键工具，但传统GPR图像解释主要依赖主观专业知识，导致效率低下和准确性不足。&lt;h4&gt;目的&lt;/h4&gt;解决传统GPR图像解释的局限性，建立一个全面的框架来自动化GPR缺陷检测，平衡计算效率与高准确性。&lt;h4&gt;方法&lt;/h4&gt;1) 基于DCGAN的数据增强策略生成高保真GPR图像缓解数据稀缺问题；2) 提出多模态链和全局注意力网络(MCGA-Net)，包括多模态链特征融合(MCFF)和全局注意力机制(GAM)；3) 使用MS COCO迁移学习微调骨干网络，加速收敛并提高泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;MCGA-Net达到92.8%的精确率、92.5%的召回率和95.9%的mAP@50；在高斯噪声、弱信号和小目标检测中保持鲁棒性并优于其他模型。&lt;h4&gt;结论&lt;/h4&gt;该工作建立了基于GPR的缺陷检测的新范式，在复杂地下环境中平衡了计算效率和高准确性。&lt;h4&gt;翻译&lt;/h4&gt;地面穿透雷达(GPR)已成为评估地下道路缺陷的关键工具。然而，传统GPR图像解释仍然严重依赖主观专业知识，引入了低效和不准确。本研究引入了一个全面的框架来解决这些局限性：(1)基于DCGAN的数据增强策略合成高保真GPR图像，缓解数据稀缺问题，同时保持复杂背景下的缺陷形态；(2)提出了一种新的多模态链和全局注意力网络(MCGA-Net)，集成多模态链特征融合(MCFF)进行分层多尺度缺陷表示，以及全局注意力机制(GAM)进行上下文感知特征增强；(3)MS COCO迁移学习微调骨干网络，加速收敛并提高泛化能力。消融和比较实验验证了该框架的有效性。MCGA-Net实现了精确率(92.8%)、召回率(92.5%)和mAP@50(95.9%)。在高斯噪声、弱信号和小目标检测中，MCGA-Net保持鲁棒性并优于其他模型。这项工作建立了基于GPR的缺陷检测的新范式，在复杂地下环境中平衡计算效率与高准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TGRS.2025.3575293&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework's efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.</description>
      <author>example@mail.com (Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang)</author>
      <guid isPermaLink="false">2512.21452v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems</title>
      <link>http://arxiv.org/abs/2512.21349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Master's thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种物理信息机器学习框架，用于求解二维周期势中粒子的Floquet-Bloch本征值问题，特别关注蜂巢晶格几何结构。&lt;h4&gt;背景&lt;/h4&gt;蜂巢晶格具有独特的能带拓扑特性，包含狄拉克点，与石墨烯等材料相关。&lt;h4&gt;目的&lt;/h4&gt;开发一种无监督的网格求解器，能够同时学习复杂的布洛赫函数及其相关的本征值（能量），恢复能带结构和布洛赫模式。&lt;h4&gt;方法&lt;/h4&gt;利用神经网络学习布洛赫函数和本征值，通过复合损失函数强制执行薛定谔方程、布洛赫周期性和归一化约束，在布里渊区上训练模型，并采用迁移学习技术从近自由电子势适应到强变化势。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够准确恢复能带结构和布洛赫模式，并与传统平面波展开方法的结果相符；能够捕获能带结构拓扑变化的能力。&lt;h4&gt;结论&lt;/h4&gt;该工作为量子本征问题的物理信息机器学习领域做出贡献，提供了关于对称性、能带结构和神经架构之间相互作用的理解。&lt;h4&gt;翻译&lt;/h4&gt;本论文提出了一种物理信息机器学习框架，用于求解与二维周期势中粒子相关的Floquet-Bloch本征值问题，特别关注蜂巢晶格几何结构，因其具有包含狄拉克点的独特能带拓扑及其与石墨烯等材料的相关性。通过利用神经网络同时学习复杂的布洛赫函数及其相关的本征值（能量），我们开发了一种无监督的网格求解器，通过复合损失函数强制执行薛定谔方程、布洛赫周期性和归一化约束。模型在布里渊区上进行训练，以恢复能带结构和布洛赫模式，并通过与传统平面波展开方法的数值验证。我们进一步探索了迁移学习技术，使求解器从近自由电子势适应到强变化势，展示了其捕获能带结构拓扑变化的能力。这项工作为量子本征问题的物理信息机器学习不断发展的领域做出了贡献，提供了关于对称性、能带结构和神经架构之间相互作用的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This thesis presents a physics-informed machine learning framework for solving the Floquet-Bloch eigenvalue problem associated with particles in two-dimensional periodic potentials, with a focus on honeycomb lattice geometry, due to its distinctive band topology featuring Dirac points and its relevance to materials such as graphene. By leveraging neural networks to learn complex Bloch functions and their associated eigenvalues (energies) simultaneously, we develop a mesh-free solver enforcing the governing Schrödinger equation, Bloch periodicity, and normalization constraints through a composite loss function without supervision. The model is trained over the Brillouin zone to recover band structures and Bloch modes, with numerical validation against traditional plane-wave expansion methods. We further explore transfer learning techniques to adapt the solver from nearly-free electron potentials to strongly varying potentials, demonstrating its ability to capture changes in band structure topology. This work contributes to the growing field of physics-informed machine learning for quantum eigenproblems, providing insights into the interplay between symmetry, band structure, and neural architectures.</description>
      <author>example@mail.com (Haaris Mian)</author>
      <guid isPermaLink="false">2512.21349v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Patch-Discontinuity Mining for Generalized Deepfake Detection</title>
      <link>http://arxiv.org/abs/2512.22027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Our paper was accepted by the IEEE Transactions on Multimedia&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GenDF的简单而有效的深度伪造检测框架，通过迁移大规模视觉模型，结合深度伪造特定表征学习、特征空间重新分配和分类不变特征增强策略，实现了在跨域和跨操作设置中的最先进泛化性能。&lt;h4&gt;背景&lt;/h4&gt;生成式人工智能的快速发展使得创建高度逼真的假面部图像成为可能，对个人隐私和在线信息完整性构成严重威胁。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测深度伪造图像的方法，特别是在面对未见过的伪造模式时保持良好的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;GenDF框架将强大的大规模视觉模型转移到深度伪造检测任务中，采用紧凑的网络设计，并集成了深度伪造特定的表征学习、特征空间重新分配以减轻分布不匹配问题，以及分类不变特征增强策略来提高泛化能力而不增加额外参数。&lt;h4&gt;主要发现&lt;/h4&gt;GenDF在跨域和跨操作设置中实现了最先进的泛化性能，同时只需要0.28M的可训练参数，证明了该方法的有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;GenDF框架通过简洁而有效的设计，成功解决了现有深度伪造检测方法在泛化能力方面的局限性，为深度伪造检测提供了一个高效且实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;生成式人工智能的快速发展使得创建高度逼真的假面部图像成为可能，对个人隐私和在线信息完整性构成严重威胁。现有的深度伪造检测方法通常依赖于手工制作的取证线索和复杂架构，在域内设置中表现良好，但在面对未见过的伪造模式时性能显著下降。在本文中，我们提出了GenDF，一个简单而有效的框架，它将强大的大规模视觉模型转移到深度伪造检测任务中，采用紧凑而简洁的网络设计。GenDF集成了深度伪造特定的表征学习，以捕捉真实和假面部图像之间的判别性模式，特征空间重新分配以减轻分布不匹配问题，以及分类不变特征增强策略，在不引入额外可训练参数的情况下提高泛化能力。大量实验表明，GenDF在跨域和跨操作设置中实现了最先进的泛化性能，同时只需要0.28M的可训练参数，验证了所提出框架的有效性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of generative artificial intelligence has enabled the creation of highly realistic fake facial images, posing serious threats to personal privacy and the integrity of online information. Existing deepfake detection methods often rely on handcrafted forensic cues and complex architectures, achieving strong performance in intra-domain settings but suffering significant degradation when confronted with unseen forgery patterns. In this paper, we propose GenDF, a simple yet effective framework that transfers a powerful large-scale vision model to the deepfake detection task with a compact and neat network design. GenDF incorporates deepfake-specific representation learning to capture discriminative patterns between real and fake facial images, feature space redistribution to mitigate distribution mismatch, and a classification-invariant feature augmentation strategy to enhance generalization without introducing additional trainable parameters. Extensive experiments demonstrate that GenDF achieves state-of-the-art generalization performance in cross-domain and cross-manipulation settings while requiring only 0.28M trainable parameters, validating the effectiveness and efficiency of the proposed framework.</description>
      <author>example@mail.com (Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma)</author>
      <guid isPermaLink="false">2512.22027v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition</title>
      <link>http://arxiv.org/abs/2512.21916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PAN框架，首个用于多模态动作识别的以人为中心的图表示学习方法，有效解决了RGB和骨架模态间的异质性问题，实现了更有效的特征融合。&lt;h4&gt;背景&lt;/h4&gt;人类动作识别虽已取得显著成就，但融合RGB和骨架模态的多模态方法仍受其固有异质性困扰，无法充分利用模态间的互补潜力。&lt;h4&gt;目的&lt;/h4&gt;提出一种以人为中心的图表示学习框架，实现更有效的多模态特征融合，减少对高质量骨架数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;将包含人体关节的RGB块标记嵌入表示为时空图；采用人类中心图建模范式抑制RGB帧冗余；提出基于注意力的后校准减少对高质量骨架数据的依赖；开发两种变体：PAN-Ensemble（双路径图卷积网络加后期融合）和PAN-Unified（单网络内统一图表示学习）。&lt;h4&gt;主要发现&lt;/h4&gt;在三个广泛使用的多模态动作识别数据集上，PAN-Ensemble和PAN-Unified在各自的多模态融合设置中实现了最先进性能。&lt;h4&gt;结论&lt;/h4&gt;PAN框架通过以人为中心的图表示学习有效解决了多模态方法中的异质性问题，实现了RGB和骨架模态的更有效融合，并在标准数据集上取得最先进性能。&lt;h4&gt;翻译&lt;/h4&gt;虽然人类动作识别已经取得了显著成就，但融合RGB和骨架模态的多模态方法仍然受到其固有异质性的困扰，无法充分利用它们之间的互补潜力。在本文中，我们提出了PAN，这是第一个用于多模态动作识别的以人为中心的图表示学习框架，其中包含人体关节的RGB块标记嵌入被表示为时空图。人类中心图建模范式抑制了RGB帧中的冗余，并与基于骨架的方法很好地对齐，从而实现更有效和语义一致的多模态特征融合。由于标记嵌入的采样严重依赖二维骨架数据，我们进一步提出了基于注意力的后校准，以在模型性能最小损失的情况下减少对高质量骨架数据的依赖。为了探索PAN与基于骨架方法的集成潜力，我们提出了两种变体：PAN-Ensemble，它采用双路径图卷积网络后接后期融合；以及PAN-Unified，它在单个网络内执行统一的图表示学习。在三个广泛使用的多模态动作识别数据集上，PAN-Ensemble和PAN-Unified在各自的多模态融合设置中分别实现了最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态动作识别中RGB模态和骨骼模态融合的挑战。RGB包含丰富外观信息但冗余且易受环境影响，骨骼数据简洁鲁棒但缺乏外观信息且依赖数据质量。两种模态存在异构性，现有方法无法有效利用它们的互补潜力。这个问题很重要，因为人类动作识别在智能监控、人机交互等领域有广泛应用，结合两种模态可提高识别准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到可区分的视觉线索常位于人体骨骼关节周围，并受视觉图神经网络工作的启发，思考RGB帧是否可采用与骨骼数据相同的图建模范式。他们设计PAN框架，利用视觉基础模型编码RGB帧，基于骨骼数据采样包含人体关节的图像块标记，并通过注意力后校准减少对高质量骨骼数据的依赖。该方法借鉴了骨骼数据建模中的图卷积网络、视觉基础模型以及注意力机制，但创新性地将RGB帧直接建模为图结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将RGB图像中的图像块视为图节点，构建以人为中心的图表示，与骨骼数据的图结构一致，实现结构一致性和语义对齐。整体流程：1)提取RGB帧和同步骨骼数据；2)用视觉基础模型编码RGB帧；3)基于骨骼数据采样包含人体关节的图像块标记；4)通过注意力后校准改进采样标记；5)将视觉标记表示为时空图输入GCN处理；6)设计两种变体实现多模态融合：PAN-Ensemble采用双路径晚期融合，PAN-Unified在单网络内统一学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次提出人体中心图表示学习框架将RGB建模为图；2)引入注意力后校准减少对高质量骨骼数据的依赖；3)实现视觉标记图与骨骼图的结构一致性和语义对齐；4)设计PAN-Ensemble和PAN-Unified两种融合变体。不同之处：之前方法要么将骨骼数据转换为同构表示，要么在分离路径处理，而PAN直接将RGB建模为图；只采样关键图像块减少冗余；通过后校准增强鲁棒性；利用一致图结构实现更精细的跨模态融合；解决时间分辨率不匹配问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAN首次提出将RGB图像块建模为人体中心图表示，通过结构一致的图结构实现RGB和骨骼模态的有效融合，并在多模态动作识别任务上达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While human action recognition has witnessed notable achievements, multimodal methods fusing RGB and skeleton modalities still suffer from their inherent heterogeneity and fail to fully exploit the complementary potential between them. In this paper, we propose PAN, the first human-centric graph representation learning framework for multimodal action recognition, in which token embeddings of RGB patches containing human joints are represented as spatiotemporal graphs. The human-centric graph modeling paradigm suppresses the redundancy in RGB frames and aligns well with skeleton-based methods, thus enabling a more effective and semantically coherent fusion of multimodal features. Since the sampling of token embeddings heavily relies on 2D skeletal data, we further propose attention-based post calibration to reduce the dependency on high-quality skeletal data at a minimal cost interms of model performance. To explore the potential of PAN in integrating with skeleton-based methods, we present two variants: PAN-Ensemble, which employs dual-path graph convolution networks followed by late fusion, and PAN-Unified, which performs unified graph representation learning within a single network. On three widely used multimodal action recognition datasets, both PAN-Ensemble and PAN-Unified achieve state-of-the-art (SOTA) performance in their respective settings of multimodal fusion: separate and unified modeling, respectively.</description>
      <author>example@mail.com (Zeyu Liang, Hailun Xia, Naichuan Zheng)</author>
      <guid isPermaLink="false">2512.21916v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction</title>
      <link>http://arxiv.org/abs/2512.21897v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MMCTOP，一个多模态临床试验结果预测框架，通过整合异构生物医学信号、结合模式引导的文本化和模态感知的表示学习，以及使用稀疏专家混合模型进行融合，解决了高维生物医学信息学中的多模态数据融合挑战。&lt;h4&gt;背景&lt;/h4&gt;高维生物医学信息学中多模态数据融合面临挑战&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态临床试验结果预测框架，提高预测性能和可靠性&lt;h4&gt;方法&lt;/h4&gt;MMCTOP整合三种异构生物医学信号（分子结构表示、协议元数据和资格叙述、疾病本体），使用模式引导的文本化和保真度验证，结合模态感知的表示学习，通过特定领域编码器生成对齐嵌入，并利用药物-疾病条件稀疏专家混合模型进行融合，采用top-k路由保持计算效率，应用温度缩放获得校准概率&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上，MMCTOP在精确度、F1和AUC方面始终优于单模态和多模态基线；消融研究表明模式引导的文本化和选择性专家路由对性能和稳定性有实质性贡献&lt;h4&gt;结论&lt;/h4&gt;MMCTOP通过结合受控的叙述规范化、上下文条件的专家融合和操作保障，推进了多模态试验建模，提高了生物医学信息学中的可审计性和可重复性&lt;h4&gt;翻译&lt;/h4&gt;针对高维生物医学信息学中多模态数据融合的挑战，我们提出了MMCTOP，一个多模态临床试验结果预测框架，整合了跨越分子结构表示、协议元数据和长格式资格叙述以及疾病本体的异构生物医学信号。MMCTOP将模式引导的文本化和保真度验证与模态感知的表示学习相结合，其中特定领域编码器生成对齐的嵌入，并通过药物-疾病条件稀疏专家混合增强的变压器主干进行融合。这种设计明确支持治疗和设计子空间的专业化，同时通过top-k路由保持可扩展计算。在基准数据集上，MMCTOP在精确度、F1和AUC方面始终优于单模态和多模态基线，消融研究表明模式引导的文本化和选择性专家路由对性能和稳定性有实质性贡献。我们 additionally应用温度缩放以获得校准概率，确保下游决策支持的可靠风险估计。总体而言，MMCTOP通过结合受控的叙述规范化、上下文条件的专家融合和旨在提高生物医学信息学中可审计性和可重复性的操作保障，推进了多模态试验建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Addressing the challenge of multimodal data fusion in high-dimensional biomedical informatics, we propose MMCTOP, a MultiModal Clinical-Trial Outcome Prediction framework that integrates heterogeneous biomedical signals spanning (i) molecular structure representations, (ii) protocol metadata and long-form eligibility narratives, and (iii) disease ontologies. MMCTOP couples schema-guided textualization and input-fidelity validation with modality-aware representation learning, in which domain-specific encoders generate aligned embeddings that are fused by a transformer backbone augmented with a drug-disease-conditioned sparse Mixture-of-Experts (SMoE). This design explicitly supports specialization across therapeutic and design subspaces while maintaining scalable computation through top-k routing. MMCTOP achieves consistent improvements in precision, F1, and AUC over unimodal and multimodal baselines on benchmark datasets, and ablations show that schema-guided textualization and selective expert routing contribute materially to performance and stability. We additionally apply temperature scaling to obtain calibrated probabilities, ensuring reliable risk estimation for downstream decision support. Overall, MMCTOP advances multimodal trial modeling by combining controlled narrative normalization, context-conditioned expert fusion, and operational safeguards aimed at auditability and reproducibility in biomedical informatics.</description>
      <author>example@mail.com (Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han)</author>
      <guid isPermaLink="false">2512.21897v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning</title>
      <link>http://arxiv.org/abs/2512.21331v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为TICON的新型transformer-based tile representation contextualizer，用于计算病理学中的全切片图像(WSI)分析。该模型能够为各种应用提供丰富的、具有上下文信息的嵌入表示，解决了标准tile编码器无法充分利用图像上下文信息的问题。&lt;h4&gt;背景&lt;/h4&gt;在计算病理学中，对全切片图像(WSI)中的小tile进行解释通常需要更大的图像上下文。然而，标准的基于tile编码器的管道在提取嵌入时会剥离tile的上下文信息，无法建模对局部和全局任务都至关重要的丰富的幻灯片级信息。此外，不同的tile编码器在不同下游任务上表现出色，因此需要一个统一的模型来对来自任何tile级基础模型的嵌入进行上下文化。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的模型，能够对来自任何tile级基础模型的嵌入进行上下文化，解决现有方法无法充分利用图像上下文信息的问题，并在各种计算病理学任务上提高性能。&lt;h4&gt;方法&lt;/h4&gt;作者提出了TICON，一个基于transformer的tile表示上下文化器，它使用单一的共享编码器，通过掩码建模目标进行预训练，同时统一和丰富来自不同tile级病理学基础模型的表示。此外，作者还在TICON上预训练了一个聚合器，形成一个仅使用11K个WSI的slide级基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，TICON上下文化的嵌入显著提高了许多不同任务的性能，在tile级基准测试(如HEST-Bench, THUNDER, CATCH)和slide级基准测试(如Patho-Bench)上建立了新的最先进结果。此外，仅使用11K个WSI预训练的TICON基础模型，优于使用多达350K个WSI预训练的最先进的slide级基础模型。&lt;h4&gt;结论&lt;/h4&gt;TICON有效地解决了计算病理学中tile表示缺乏上下文信息的问题，提供了一个统一的框架来丰富来自任何tile级基础模型的嵌入。它在多个任务上取得了最先进的结果，并证明即使在较少的训练数据下也能构建高性能的slide级基础模型。&lt;h4&gt;翻译&lt;/h4&gt;在大型全切片图像(WSI)中解释小tile通常需要更大的图像上下文。我们介绍了TICON，这是一种基于transformer的tile表示上下文化器，可以为计算病理学中的'任何'应用生成丰富、具有上下文化的嵌入。标准的基于tile编码器的管道提取剥离了上下文的tile嵌入，无法建模对局部和全局任务都至关重要的丰富的幻灯片级信息。此外，不同的tile编码器在不同的下游任务上表现出色。因此，需要一个统一的模型来对来自'任何'tile级基础模型的嵌入进行上下文化。TICON通过一个单一的共享编码器解决了这一需求，该编码器使用掩码建模目标进行预训练，同时统一和丰富来自不同tile级病理学基础模型的表示。我们的实验证明，TICON上下文化的嵌入显著提高了许多不同任务的性能，在tile级基准测试(即HEST-Bench, THUNDER, CATCH)和slide级基准测试(即Patho-Bench)上建立了新的最先进结果。最后，我们在TICON上预训练了一个聚合器，形成一个slide级基础模型，仅使用11K个WSI，就优于使用多达350K个WSI预训练的最先进slide级基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.</description>
      <author>example@mail.com (Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li, Sofiène Boutaj, Pushpak Pati, Srikar Yellapragada, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Prateek Prasanna, Stergios Christodoulidis, Maria Vakalopoulou, Dimitris Samaras)</author>
      <guid isPermaLink="false">2512.21331v2</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision</title>
      <link>http://arxiv.org/abs/2512.20308v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in Transactions on Machine Learning Research. 30 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为SpidR的自监督语音表征模型，可以直接从语音中学习语义表示，无需文本作为中介。该模型通过掩码预测目标结合自蒸馏和在线聚类进行训练，在下游语言建模任务上表现优异，且显著减少了预训练时间。&lt;h4&gt;背景&lt;/h4&gt;语言建模和语音表征学习的并行进展使得直接从语音学习语言而不需要文本中间层成为可能。这需要直接从语音中提取语义表征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效学习具有高度可访问语音信息表征的自监督语音表征模型，特别适合无文本的口语语言建模。&lt;h4&gt;方法&lt;/h4&gt;1. 引入SpidR，一种自监督语音表征模型；2. 使用原始波形进行训练；3. 采用掩码预测目标结合自蒸馏和在线聚类；4. 学生模型的中间层学习预测来自教师模型中间层的分配；5. 这种学习目标稳定了在线聚类过程，产生更高质量的码本；6. 开源训练代码和模型检查点。&lt;h4&gt;主要发现&lt;/h4&gt;1. SpidR在下游语言建模基准测试(sWUGGY, sBLIMP, tSC)上优于wav2vec 2.0, HuBERT, WavLM和DinoSR；2. 系统评估了模型和层之间语音单元质量(ABX, PNMI)与语言建模性能之间的相关性，验证了这些指标作为可靠代理的有效性；3. SpidR显著减少了预训练时间，相比HuBERT只需要16 GPU上预训练一天，而不是一周。&lt;h4&gt;结论&lt;/h4&gt;SpidR是一种高效的自监督语音表征模型，能够直接从语音中学习语义表示，在性能和训练效率上都优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;语言建模和语音表征学习的并行进展提出了直接从语音学习语言而不需要文本中间层的前景。这需要直接从语音中提取语义表征。我们的贡献有三方面。首先，我们引入了SpidR，一种自监督语音表征模型，它高效地学习具有高度可访问语音信息的表征，这使其特别适合无文本的口语语言建模。它使用掩码预测目标结合自蒸馏和在线聚类在原始波形上进行训练。学生模型的中间层学习预测来自教师模型中间层的分配。与以往方法相比，这种学习目标稳定了在线聚类过程，产生了更高质量的码本。在下游语言建模基准测试(sWUGGY, sBLIMP, tSC)上，SpidR优于wav2vec 2.0、HuBERT、WavLM和DinoSR。其次，我们系统性地评估了模型和层之间语音单元质量(ABX, PNMI)与语言建模性能之间的相关性，验证了这些指标作为可靠代理的有效性。最后，与HuBERT相比，SpidR显著减少了预训练时间，只需要在16 GPU上预训练一天，而不是一周。这种加速是由预训练方法和高效代码库实现的，它允许更快的迭代和更容易的实验。我们在https://github.com/facebookresearch/spidr开源了训练代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.</description>
      <author>example@mail.com (Maxime Poli, Mahi Luthra, Youssef Benchekroun, Yosuke Higuchi, Martin Gleize, Jiayi Shen, Robin Algayres, Yu-An Chung, Mido Assran, Juan Pino, Emmanuel Dupoux)</author>
      <guid isPermaLink="false">2512.20308v2</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Learning-Based Handover Management in NextG O-RAN</title>
      <link>http://arxiv.org/abs/2512.22022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CONTRA是一种创新的框架，首次在O-RAN架构中联合优化传统切换和条件切换，通过元学习算法实现自适应切换决策，提高网络性能，在实际网络环境中表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;传统切换(THOs)曾是移动连接的支柱，但在密集部署和高频段中越来越容易失败和延迟。3GPP引入了条件切换(CHOs)，可以主动预留小区和用户驱动的执行，但两种切换类型在信令、资源使用和可靠性方面存在复杂的权衡。&lt;h4&gt;目的&lt;/h4&gt;提出新的全国性移动管理数据集，提供对这些问题的见解，并呼吁在下一代网络中采用自适应和鲁棒的切换控制。&lt;h4&gt;方法&lt;/h4&gt;提出CONTRA框架，研究两种变体：一种预先分配用户到特定切换类型，另一种动态决定切换类型。使用元学习算法适应运行时观察，保证性能接近拥有完美未来信息的预言机。CONTRA设计为近实时部署为O-RAN xApp，符合6G灵活和智能控制的目标。&lt;h4&gt;主要发现&lt;/h4&gt;利用众包数据集进行的广泛评估显示，CONTRA提高了用户吞吐量，减少了THO和CHO的切换成本，在动态和真实场景中优于3GPP兼容和强化学习基线。&lt;h4&gt;结论&lt;/h4&gt;CONTRA框架有效地解决了传统切换和条件切换的权衡问题，通过自适应和智能的切换控制，提高了网络性能。&lt;h4&gt;翻译&lt;/h4&gt;虽然传统切换(THOs)一直是移动连接的支柱，但它们在密集部署和高频段中越来越容易遭受故障和延迟。为解决这些限制，3GPP引入了条件切换(CHOs)，可以主动预留小区和用户驱动的执行。然而，两种切换类型在信令、资源使用和可靠性方面呈现复杂的权衡。本文提出了一家顶级移动网络运营商(MNO)提供的独特全国性移动管理数据集，为这些问题提供了新的见解，并呼吁在下一代网络中采用自适应和鲁棒的切换控制。受这些发现的启发，我们提出了CONTRA框架，首次在O-RAN架构中联合优化THOs和CHOs。我们研究了CONTRA的两种变体：一种用户预先分配到一种切换类型，反映不同的服务或用户特定需求，以及一种更动态的公式，控制器根据系统条件和需求即时决定切换类型。为此，它依靠一种实用的元学习算法，适应运行时观察，并保证性能接近拥有完美未来信息(无遗憾)的预言机。CONTRA专为近实时部署设计，作为O-RAN xApp，符合6G灵活和智能控制的目标。利用众包数据集进行的广泛评估显示，CONTRA提高了用户吞吐量并减少了THO和CHO的切换成本，在动态和真实场景中优于3GPP兼容和强化学习(RL)基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While traditional handovers (THOs) have served as a backbone for mobile connectivity, they increasingly suffer from failures and delays, especially in dense deployments and high-frequency bands. To address these limitations, 3GPP introduced Conditional Handovers (CHOs) that enable proactive cell reservations and user-driven execution. However, both handover (HO) types present intricate trade-offs in signaling, resource usage, and reliability. This paper presents unique, countrywide mobility management datasets from a top-tier mobile network operator (MNO) that offer fresh insights into these issues and call for adaptive and robust HO control in next-generation networks. Motivated by these findings, we propose CONTRA, a framework that, for the first time, jointly optimizes THOs and CHOs within the O-RAN architecture. We study two variants of CONTRA: one where users are a priori assigned to one of the HO types, reflecting distinct service or user-specific requirements, as well as a more dynamic formulation where the controller decides on-the-fly the HO type, based on system conditions and needs. To this end, it relies on a practical meta-learning algorithm that adapts to runtime observations and guarantees performance comparable to an oracle with perfect future information (universal no-regret). CONTRA is specifically designed for near-real-time deployment as an O-RAN xApp and aligns with the 6G goals of flexible and intelligent control. Extensive evaluations leveraging crowdsourced datasets show that CONTRA improves user throughput and reduces both THO and CHO switching costs, outperforming 3GPP-compliant and Reinforcement Learning (RL) baselines in dynamic and real-world scenarios.</description>
      <author>example@mail.com (Michail Kalntis, George Iosifidis, José Suárez-Varela, Andra Lutu, Fernando A. Kuipers)</author>
      <guid isPermaLink="false">2512.22022v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations</title>
      <link>http://arxiv.org/abs/2512.21633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合元自解码器(MAD)范式的神经Galerkin方法(NGM)增强框架，通过时空解耦和元学习驱动的适应，实现了对参数化偏微分方程的高效求解，显著降低了计算成本同时保持了高准确性。&lt;h4&gt;背景&lt;/h4&gt;参数化偏微分方程(PDEs)是建模受不确定或变化参数影响的物理和工程系统的基本工具。传统基于神经网络的求解器（如物理信息神经网络PINNs和深度Galerkin方法）由于依赖全时空近似，在泛化能力和长时间预测效率方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决传统神经网络求解器在泛化和长时间预测效率方面的问题，开发一种能够实现物理一致性长时程预测且计算开销显著降低的方法。&lt;h4&gt;方法&lt;/h4&gt;提出一种新颖且可扩展的框架，通过结合元自解码器(MAD)范式增强神经Galerkin方法(NGM)。该方法利用时空解耦实现更稳定高效的时间积分，采用元学习驱动的适应实现快速泛化，并应用随机稀疏更新降低计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够以显著降低的计算开销实现复杂参数化演化方程的物理一致性、长时程预测。数值实验表明，该方法在准确性、鲁棒性和适应性方面表现良好。&lt;h4&gt;结论&lt;/h4&gt;结合元自解码器范式、时空解耦、元学习驱动适应和随机稀疏更新的框架，有效解决了传统神经网络求解器在参数化偏微分方程求解中的泛化和效率问题，为复杂物理系统的建模提供了高效工具。&lt;h4&gt;翻译&lt;/h4&gt;参数化偏微分方程(PDEs)是建模受不确定或变化参数影响的广泛物理和工程系统的基本工具。传统的基于神经网络的求解器，如物理信息神经网络(PINNs)和深度Galerkin方法，由于依赖全时空近似，常常面临泛化和长时间预测效率方面的挑战。为解决这些问题，我们提出了一种新颖且可扩展的框架，通过结合元自解码器(MAD)范式显著增强了神经Galerkin方法(NGM)。我们的方法利用时空解耦实现更稳定高效的时间积分，而元学习驱动的适应允许对未见过的参数配置进行快速泛化，且只需最少量的再训练。此外，随机稀疏更新能有效降低计算成本而不损害准确性。这些进步共同使该方法能够以显著降低的计算开销，实现复杂参数化演化方程的物理一致性、长时程预测。在基准问题上的数值实验表明，我们的方法在准确性、鲁棒性和适应性方面表现良好。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parametric partial differential equations (PDEs) are fundamental for modeling a wide range of physical and engineering systems influenced by uncertain or varying parameters. Traditional neural network-based solvers, such as Physics-Informed Neural Networks (PINNs) and Deep Galerkin Methods, often face challenges in generalization and long-time prediction efficiency due to their dependence on full space-time approximations. To address these issues, we propose a novel and scalable framework that significantly enhances the Neural Galerkin Method (NGM) by incorporating the Meta-Auto-Decoder (MAD) paradigm. Our approach leverages space-time decoupling to enable more stable and efficient time integration, while meta-learning-driven adaptation allows rapid generalization to unseen parameter configurations with minimal retraining. Furthermore, randomized sparse updates effectively reduce computational costs without compromising accuracy. Together, these advancements enable our method to achieve physically consistent, long-horizon predictions for complex parameterized evolution equations with significantly lower computational overhead. Numerical experiments on benchmark problems demonstrate that our methods performs comparatively well in terms of accuracy, robustness, and adaptability.</description>
      <author>example@mail.com (Qiuqi Li, Yiting Liu, Jin Zhao, Wencan Zhu)</author>
      <guid isPermaLink="false">2512.21633v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Sparse Recovery Algorithms Using Neural Architecture Search</title>
      <link>http://arxiv.org/abs/2512.21563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at the 59th Asilomar Conference on Signals, Systems, and Computers&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了利用元学习工具（如神经架构搜索NAS）来自动发现信号处理中反问题求解算法的方法，并以ISTA和FISTA算法为例验证了这一方法的可行性。&lt;h4&gt;背景&lt;/h4&gt;设计用于解决信号处理中反问题的新算法是一项极其困难、依赖启发式方法且耗时的任务。&lt;h4&gt;目的&lt;/h4&gt;探索在信号处理领域中通过元学习工具实现算法自动发现的思路。&lt;h4&gt;方法&lt;/h4&gt;开发了一个元学习框架，在包含超过50,000个变量的搜索空间中重新发现ISTA和FISTA算法的关键元素，并验证了该框架的通用性。&lt;h4&gt;主要发现&lt;/h4&gt;元学习框架能够成功地重新发现ISTA和FISTA算法的关键组成部分，证明自动化算法发现在信号处理领域是可行的。&lt;h4&gt;结论&lt;/h4&gt;通过元学习工具可以实现信号处理中反问题求解算法的自动发现，这种方法具有通用性，可应用于不同的数据分布和算法类型。&lt;h4&gt;翻译&lt;/h4&gt;为解决信号处理中的反问题设计新颖算法是一项极其困难、依赖启发式方法且耗时的任务。在这篇简短的文章中，我们探讨了通过元学习工具（如神经架构搜索NAS）在信号处理环境中实现算法自动发现的思路。具体而言，我们将迭代收缩阈值算法(ISTA)及其加速版本快速ISTA(FISTA)作为算法重新发现的候选对象。我们开发了一个元学习框架，在包含超过50,000个变量的搜索空间中，能够重新发现上述两种算法的几个关键元素。然后，我们展示了我们的框架如何应用于ISTA/FISTA之外的各种数据分布和算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The design of novel algorithms for solving inverse problems in signal processing is an incredibly difficult, heuristic-driven, and time-consuming task. In this short paper, we the idea of automated algorithm discovery in the signal processing context through meta-learning tools such as Neural Architecture Search (NAS). Specifically, we examine the Iterative Shrinkage Thresholding Algorithm (ISTA) and its accelerated Fast ISTA (FISTA) variant as candidates for algorithm rediscovery. We develop a meta-learning framework which is capable of rediscovering (several key elements of) the two aforementioned algorithms when given a search space of over 50,000 variables. We then show how our framework can apply to various data distributions and algorithms besides ISTA/FISTA.</description>
      <author>example@mail.com (Patrick Yubeaton, Sarthak Gupta, M. Salman Asif, Chinmay Hegde)</author>
      <guid isPermaLink="false">2512.21563v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</title>
      <link>http://arxiv.org/abs/2512.22046v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了BadVSFM，第一个专门针对提示驱动视频分割基础模型的后门攻击框架，解决了传统后门攻击在VSFM上效果不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;提示驱动的视频分割基础模型如SAM2正被越来越多地应用于自动驾驶和数字病理等领域，引发了关于后门威胁的担忧。&lt;h4&gt;目的&lt;/h4&gt;理解并解决传统后门攻击在VSFM上效果不佳的问题，开发专门针对VSFM的后门攻击框架。&lt;h4&gt;方法&lt;/h4&gt;提出BadVSFM框架，采用两阶段策略：(1)引导图像编码器使触发帧映射到目标嵌入而干净帧保持对齐；(2)训练掩码解码器使触发帧-提示对产生共享目标掩码而干净输出接近参考解码器。&lt;h4&gt;主要发现&lt;/h4&gt;BadVSFM在两个数据集和五个VSFM上实现了强大且可控的后门效果，同时保持干净分割质量；消融实验证实了两阶段设计的必要性；梯度冲突分析显示BadVSFM分离了触发和干净表示；四种代表性防御措施效果有限。&lt;h4&gt;结论&lt;/h4&gt;BadVSFM揭示了当前VSFM中一个未被充分探索的安全漏洞，为理解VSFM的安全特性提供了重要见解。&lt;h4&gt;翻译&lt;/h4&gt;提示驱动的视频分割基础模型如SAM2正越来越多地应用于自动驾驶和数字病理等领域，引发了对后门威胁的担忧。令人惊讶的是，我们发现直接将经典后门攻击(如BadNet)转移到VSFM上几乎无效，攻击成功率低于5%。为理解这一点，我们研究了编码器梯度和注意力图，观察到传统训练使得干净样本和触发样本的梯度保持大致对齐，而注意力仍然集中在真实对象上，阻止编码器学习与触发相关的表示。为应对这一挑战，我们提出了BadVSFM，这是第一个专门针对提示驱动VSFM的后门框架。BadVSFM采用两阶段策略：(1)引导图像编码器，使触发帧映射到指定的目标嵌入，同时干净帧与干净的参考编码器保持对齐；(2)训练掩码解码器，使得跨提示类型的触发帧-提示对产生共享的目标掩码，同时干净输出接近参考解码器。在两个数据集和五个VSFM上的大量实验表明，BadVSFM在各种触发器和提示下实现了强大且可控的后门效果，同时保持了干净分割的质量。对损失、阶段、目标、触发设置和中毒率的消融实验表明，对合理的超参数变化具有鲁棒性，并证实了两阶段设计的必要性。最后，梯度冲突分析和注意力可视化显示，BadVSFM分离了触发和干净表示，并将注意力转移到触发区域，而四种代表性防御措施在很大程度上仍然无效，揭示了当前VSFM中一个未被充分探索的漏洞。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.</description>
      <author>example@mail.com (Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang)</author>
      <guid isPermaLink="false">2512.22046v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision</title>
      <link>http://arxiv.org/abs/2512.21970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了StereoVLA模型，一种利用立体视觉几何线索的视觉-语言-动作模型，通过几何-语义特征提取模块和交互区域深度估计任务提升空间感知和指令跟随能力。&lt;h4&gt;背景&lt;/h4&gt;立体相机模仿人类双目视觉，为机器人精确操作提供丰富的空间线索，但立体视觉在视觉-语言-动作模型中的应用仍未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用立体视觉提供的丰富几何线索的视觉-语言-动作模型，提升机器人在精确操作任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出几何-语义特征提取模块，利用视觉基础模型提取并融合几何特征(来自立体视图差异)和语义特征(来自单视图)；同时引入辅助的交互区域深度估计任务增强空间感知并加速模型收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在立体设置下的多样化任务中，StereoVLA显著优于基线模型，且对相机姿态变化表现出强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过有效利用立体视觉的几何信息，StereoVLA模型提升了视觉-语言-动作模型在机器人操作任务中的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;立体相机 closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: geometric features from subtle stereo-view differences for spatial perception; semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何在视觉-语言-行动模型（VLAs）中有效利用立体视觉来增强机器人的空间感知和操作能力的问题。这个问题很重要，因为当前大多数VLAs依赖单目RGB图像，存在深度模糊问题，难以精确判断物体距离和空间关系，而这对精确的机器人操作至关重要。立体视觉能模拟人类双眼视觉，提供丰富的空间线索，且相比其他补充传感器（如手腕相机、深度传感器）具有视野更广、噪声更小、硬件更简单等优势。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLAs模型在空间感知方面的局限性以及各种补充传感器的缺点，从人类视觉系统获得启发，认识到立体视觉能提供丰富的空间信息。在设计方法时，作者借鉴了FoundationStereo模型提取几何特征，借鉴了PrismaticVLM提取语义特征，参考了GraspVLA的训练和数据生成策略，并利用InternLM-1.8B作为大语言模型骨干。在此基础上，作者创新性地提出了几何-语义特征提取模块和交互区域深度估计任务，将几何特征和语义特征有效融合，使模型既具备几何精度又保持语义理解能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过立体视觉提供丰富的几何信息，同时保持强大的语义理解能力，实现精确的机器人操作。具体流程为：1) 接收一对立体图像，使用FoundationStereo提取几何特征，使用SigLIP和DINOv2提取语义特征；2) 将几何和语义特征在空间上对齐并通过通道级连接融合成混合视觉token；3) 将视觉token与语言token一起输入到大语言模型进行联合处理；4) 使用动作专家通过flow-matching预测末端执行器姿态；5) 在训练过程中引入交互区域深度估计作为辅助任务，仅在夹爪和物体周围采样点预测深度，提高训练效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个系统利用立体视觉增强的VLAs模型；2) 几何-语义特征提取模块，融合立体视觉的几何特征和单目视图的语义特征；3) 交互区域深度估计任务，专注于操作相关区域；4) 解决了直接将立体输入提供给现有多相机VLAs时性能不佳的问题。相比之前工作，StereoVLA使用立体相机而非单目或多相机设置，专门设计了特征提取与融合方法，引入了针对性的深度估计任务，在各种任务上表现更优，特别是在高精度任务上，且对相机姿态变化更具鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; StereoVLA通过创新的几何-语义特征提取方法和交互区域深度估计任务，首次将立体视觉系统性地整合到视觉-语言-行动模型中，显著提升了机器人在精确操作任务中的性能和对相机姿态变化的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: 1) geometric features from subtle stereo-view differences for spatial perception; 2) semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.</description>
      <author>example@mail.com (Shengliang Deng, Mi Yan, Yixin Zheng, Jiayi Su, Wenhao Zhang, Xiaoguang Zhao, Heming Cui, Zhizheng Zhang, He Wang)</author>
      <guid isPermaLink="false">2512.21970v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis</title>
      <link>http://arxiv.org/abs/2512.21881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The code will be released after review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了SLIM-Brain模型，一种新型无图谱基础模型，通过两阶段自适应设计同时提高fMRI分析中的数据和训练效率，在七个公共基准上取得最先进性能，仅需4千预训练会话和30%的GPU内存。&lt;h4&gt;背景&lt;/h4&gt;基础模型正成为fMRI分析的强大范式，但当前方法面临双重瓶颈：基于图谱的方法降低数据维度但丢弃空间细节且需大训练数据；无图谱方法保留空间保真度但内存和计算强度过高，使大规模预训练不可行。&lt;h4&gt;目的&lt;/h4&gt;引入SLIM-Brain(Sample-efficient, Low-memory fMRI Foundation Model for Human Brain)，一种新的无图谱基础模型，同时提高数据和训练效率。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段自适应设计：(i)轻量级时间提取器捕获完整序列的全局上下文并根据显著性对数据窗口排序；(ii)4D分层编码器(Hiera-JEPA)仅从前k个选定窗口学习精细体素级表示，同时删除约70%掩码块。&lt;h4&gt;主要发现&lt;/h4&gt;在七个公共基准上的广泛实验表明，SLIM-Brain在各种任务上建立了新的最先进性能，仅需4千个预训练会话，相比传统体素级方法大约只需要30%的GPU内存。&lt;h4&gt;结论&lt;/h4&gt;SLIM-Brain模型在保持高性能的同时显著提高了数据效率和训练效率，为fMRI分析提供了一种新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;基础模型正在成为fMRI分析的一个强大范式，但当前方法面临数据和训练效率的双重瓶颈。基于图谱的方法将体素信号聚合成固定的感兴趣区域，降低了数据维度但丢弃了精细的空间细节，并且需要非常大的队列才能作为通用基础模型进行有效训练。另一方面，无图谱方法直接在体素级别上操作，保留了空间保真度，但内存和计算强度极高，使得大规模预训练不可行。我们引入SLIM-Brain(高效样本、低内存人脑fMRI基础模型)，一种新的无图谱基础模型，同时提高了数据和训练效率。SLIM-Brain采用两阶段自适应设计：(i)轻量级时间提取器捕获完整序列的全局上下文，并根据显著性对数据窗口进行排序；(ii)4D分层编码器(Hiera-JEPA)仅从前k个选定的窗口中学习精细的体素级表示，同时删除约70%的掩码块。在七个公共基准上的广泛实验表明，SLIM-Brain在各种任务上建立了新的最先进性能，同时仅需4千个预训练会话，并且相比传统的体素级方法，大约只需要30%的GPU内存。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding fine-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. We introduce SLIM-Brain (Sample-efficient, Low-memory fMRI Foundation Model for Human Brain), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-$k$ selected windows, while deleting about 70% masked patches. Extensive experiments across seven public benchmarks show that SLIM-Brain establishes new state-of-the-art performance on diverse tasks, while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory comparing to traditional voxel-level methods.</description>
      <author>example@mail.com (Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen)</author>
      <guid isPermaLink="false">2512.21881v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models</title>
      <link>http://arxiv.org/abs/2512.21860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DIOR方法，一种利用大型视觉-语言模型生成条件图像嵌入的无需训练方法。该方法通过提示模型用与给定条件相关的单个单词描述图像，提取最后一个隐藏状态向量作为条件图像嵌入。&lt;h4&gt;背景&lt;/h4&gt;条件图像嵌入是指专注于图像中由给定文本条件(如颜色、类型)指出的特定方面的特征表示，这是一个具有挑战性的问题。虽然最近的视觉基础模型如CLIP提供了丰富的图像表示，但它们并非设计用来专注于指定条件。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，利用大型视觉-语言模型生成条件图像嵌入，使其能够专注于图像中由给定文本条件指定的特定方面。&lt;h4&gt;方法&lt;/h4&gt;DIOR是一种无需训练的方法，它通过提示大型视觉-语言模型(LVLM)用与给定条件相关的单个单词描述图像，然后提取LVLM最后一个隐藏状态向量作为条件图像嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;DIOR在条件图像相似度任务上表现优于现有无需训练的基线方法，包括CLIP；在多个设置下也优于需要额外训练的方法；提供了通用解决方案，可应用于任何图像和条件，无需额外训练或任务特定先验。&lt;h4&gt;结论&lt;/h4&gt;DIOR是一种有效的条件图像嵌入生成方法，无需训练即可实现高性能，且具有广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;条件图像嵌入是专注于图像中由给定文本条件(如颜色、类型)指出的特定方面的特征表示，这是一个具有挑战性的问题。虽然最近的视觉基础模型，如CLIP，提供了丰富的图像表示，但它们并非设计用来专注于指定条件。在本文中，我们提出了DIOR，一种利用大型视觉-语言模型生成条件图像嵌入的方法。DIOR是一种无需训练的方法，它提示模型用与给定条件相关的单个单词描述图像，然后将最后一个标记的隐藏状态向量提取为条件图像嵌入。DIOR提供了通用解决方案，可应用于任何图像和条件，无需额外训练或任务特定先验。在条件图像相似度任务上的全面实验结果表明，DIOR优于现有无需训练的基线方法，包括CLIP。此外，DIOR在多个设置下也优于需要额外训练的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM's last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.</description>
      <author>example@mail.com (Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue)</author>
      <guid isPermaLink="false">2512.21860v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference</title>
      <link>http://arxiv.org/abs/2512.21730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE INFOCOM 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Hyperion，第一个云设备协作框架，用于在动态网络上使用现成的视觉transformer对超高清视觉数据进行低延迟推理。&lt;h4&gt;背景&lt;/h4&gt;阵列相机摄影技术可实时捕捉超高清视频，提供广阔视野中的丰富视觉信息，但使用基于transformer的视觉基础模型处理此类数据时，在设备计算或云计算中面临显著的计算或传输开销。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架，解决超高清视觉数据处理中的计算和传输瓶颈问题，实现低延迟推理。&lt;h4&gt;方法&lt;/h4&gt;Hyperion整合了三个关键组件：协作感知的重要性评分器识别关键区域；动态调度器自适应调整块传输质量以平衡延迟和准确性；加权集成器融合边缘和云结果提高准确性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与各种网络环境下的最先进基线相比，Hyperion将帧处理速率提高了最多1.61倍，准确性提高了最多20.2%。&lt;h4&gt;结论&lt;/h4&gt;Hyperion有效解决了超高清视觉transformer的计算和传输瓶颈，在保持高准确性的同时显著提高了处理速度。&lt;h4&gt;翻译&lt;/h4&gt;最近阵列相机摄影技术的进步能够实时捕捉超高清视频，提供了广阔视野中的丰富视觉信息。然而，使用最先进的基于transformer的视觉基础模型及时处理此类数据，在设备计算中面临显著的计算开销，或在云计算中面临传输开销。在本文中，我们提出了Hyperion，这是第一个云设备协作框架，能够在动态网络上使用现成的视觉transformer对超高清视觉数据进行低延迟推理。Hyperion利用视觉Transformer模型中的固有特性，解决了超高清视觉transformer的计算和传输瓶颈。具体而言，Hyperion集成了一个协作感知的重要性评分器，用于识别块级别的关键区域；一个动态调度器，能够自适应调整块传输质量，在动态网络条件下平衡延迟和准确性；以及一个加权集成器，融合边缘和云结果以提高准确性。实验结果表明，与各种网络环境下的最先进基线相比，Hyperion将帧处理速率提高了最多1.61倍，准确性提高了最多20.2%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in array-camera videography enable real-time capturing of ultra-high-definition (Ultra-HD) videos, providing rich visual information in a large field of view. However, promptly processing such data using state-of-the-art transformer-based vision foundation models faces significant computational overhead in on-device computing or transmission overhead in cloud computing. In this paper, we present Hyperion, the first cloud-device collaborative framework that enables low-latency inference on Ultra-HD vision data using off-the-shelf vision transformers over dynamic networks. Hyperion addresses the computational and transmission bottleneck of Ultra-HD vision transformers by exploiting the intrinsic property in vision Transformer models. Specifically, Hyperion integrates a collaboration-aware importance scorer that identifies critical regions at the patch level, a dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under dynamic network conditions, and a weighted ensembler that fuses edge and cloud results to improve accuracy. Experimental results demonstrate that Hyperion enhances frame processing rate by up to 1.61 times and improves the accuracy by up to 20.2% when compared with state-of-the-art baselines under various network environments.</description>
      <author>example@mail.com (Linyi Jiang, Yifei Zhu, Hao Yin, Bo Li)</author>
      <guid isPermaLink="false">2512.21730v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Linear Foundation Model for Quantum Embedding: Data-Driven Compression of the Ghost Gutzwiller Variational Space</title>
      <link>http://arxiv.org/abs/2512.21666v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了一种基于主成分分析的量子嵌入线性基础模型，通过数据驱动的主动学习方案显著降低了量子嵌入理论中的计算瓶颈，实现了强关联材料的高效模拟。&lt;h4&gt;背景&lt;/h4&gt;Kohn-Sham密度泛函理论(DFT)在模拟量子物质时被广泛使用，但对强关联系统往往失效。量子嵌入(QE)理论通过映射系统到辅助嵌入哈密顿量(EH)来解决这一局限，但EH通常很大，其迭代解算是主要的计算瓶颈。&lt;h4&gt;目的&lt;/h4&gt;开发一种QE的线性基础模型，利用主成分分析(PCA)压缩解决EH所需的量子态空间，在小变分子子空间内高效解决EH问题。&lt;h4&gt;方法&lt;/h4&gt;采用数据驱动的主动学习方案从EH基态学习变分子空间，将嵌入解简化为约化空间中的确定性基态本征值问题。在鬼Gutzwiller近似(ghost-GA)下对三轨道Hubbard模型进行研究，并在钚元素上进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;在Bethe晶格上学习的变分空间可转移到方形和立方晶格无需额外训练；显著降低了EH步骤的计算成本；在钚元素上，单个变分空间可重现所有六个结晶相的能量学，同时将EH解决方案成本降低几个数量级。&lt;h4&gt;结论&lt;/h4&gt;该方法为克服QE框架的主要计算瓶颈提供了实用途径，使强关联材料的高通量从头算模拟接近DFT成本，为量子物质研究开辟了新方向。&lt;h4&gt;翻译&lt;/h4&gt;量子物质的模拟主要依赖Kohn-Sham密度泛函理论(DFT)，但该方法通常对强关联系统失效。量子嵌入(QE)理论通过将系统映射到描述片段-环境相互作用的辅助嵌入哈密顿量(EH)来解决这一局限，但EH通常很大，其迭代解算是主要的计算瓶颈。我们引入了一种QE的线性基础模型，利用主成分分析(PCA)压缩解决EH所需的量子态空间，从而在小变分子子空间内解决EH问题。通过数据驱动的主动学习方案，我们从EH基态学习这个子空间，将每个嵌入解简化为约化空间中的确定性基态本征值问题。在鬼Gutzwiller近似(ghost-GA)下，我们对三轨道Hubbard模型的研究表明，在Bethe晶格上学习的变分空间可以转移到方形和立方晶格上而无需额外训练，同时显著降低了EH步骤的成本。我们进一步在钚元素上验证了该方法，单个变分空间可重现所有六个结晶相的能量学，同时将EH解决方案的成本降低了几个数量级。这为克服QE框架的主要计算瓶颈提供了实用途径，为强关联材料的高通量从头算模拟开辟了道路，使其成本接近DFT。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulations of quantum matter rely mainly on Kohn-Sham density functional theory (DFT), which often fails for strongly correlated systems. Quantum embedding (QE) theories address this limitation by mapping the system onto an auxiliary embedding Hamiltonian (EH) describing fragment-environment interactions, but the EH is typically large and its iterative solution is the primary computational bottleneck. We introduce a linear foundation model for QE that utilizes principal component analysis (PCA) to compress the space of quantum states needed to solve the EH within a small variational subspace. Using a data-driven active-learning scheme, we learn this subspace from EH ground states and reduce each embedding solve to a deterministic ground-state eigenvalue problem in the reduced space. Within the ghost Gutzwiller approximation (ghost-GA), we show for a three-orbital Hubbard model that a variational space learned on a Bethe lattice is transferable to square and cubic lattices without additional training, while substantially reducing the cost of the EH step. We further validate the approach on plutonium, where a single variational space reproduces the energetics of all six crystalline phases while reducing the cost of the EH solution by orders of magnitude. This provides a practical route to overcome the main computational bottleneck of QE frameworks, paving the way for high-throughput ab initio simulations of strongly correlated materials at a near-DFT cost.</description>
      <author>example@mail.com (Samuele Giuli, Hasanat Hasan, Benedikt Kloss, Marius S. Frank, Tsung-Han Lee, Olivier Gingras, Yong-Xin Yao, Nicola Lanatà)</author>
      <guid isPermaLink="false">2512.21666v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database</title>
      <link>http://arxiv.org/abs/2512.21652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github: https://github.com/wangziblake/CardioMM_MMCMR-427K&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一个通用的心血管磁共振成像重建基础模型CardioMM，并创建了最大的多模态CMR k空间数据库MMCMR-427K，以解决CMR临床应用中的扫描时间长和环境差异问题。&lt;h4&gt;背景&lt;/h4&gt;多模态心血管磁共振成像(CMR)能提供全面且非侵入性的心血管疾病诊断和机制洞察，尽管有数十年的技术进步，其临床应用仍受限于扫描时间长和医疗环境差异。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的重建基础模型用于超快速CMR成像，使其能适应不同的成像场景，并作为所有下游分析的基础。&lt;h4&gt;方法&lt;/h4&gt;创建了MMCMR-427K数据库，包含427,465个多线圈k空间数据，配对结构化元数据，涵盖13个国际中心、12种CMR模式、15台扫描仪和17种CVD类别；基于此资源开发了CardioMM模型，该模型结合语义上下文理解和基于物理的数据一致性。&lt;h4&gt;主要发现&lt;/h4&gt;CardioMM在内部中心实现最先进性能，对未见过的外部设置表现出强大的零样本泛化能力；即使在24倍成像加速下，也能可靠保留关键心脏表型、定量心肌生物标志物和诊断图像质量，显著提高CMR检查吞吐量而不损害临床完整性。&lt;h4&gt;结论&lt;/h4&gt;开源的MMCMR-427K数据库和CardioMM框架建立了通往高吞吐量、高质量和临床可及的心血管成像的可扩展路径。&lt;h4&gt;翻译&lt;/h4&gt;多模态心血管磁共振(CMR)成像为心血管疾病(CVD)诊断和潜在机制提供了全面且非侵入性的见解。尽管有数十年的进步，其广泛的临床应用仍然受到扫描时间长和医疗环境异质性的限制。这凸显了对超快速CMR成像的通用重建基础模型的迫切需求，该模型应能适应多样化的成像场景，并作为所有下游分析的基本基础。为实现这一目标，我们整理了MMCMR-427K，这是迄今为止最大、最全面的多模态CMR k空间数据库，包含427,465个多线圈k空间数据，配对来自13个国际中心、12种CMR模式、15台扫描仪和17种CVD类别的结构化元数据，覆盖三大洲人群。基于这一前所未有的资源，我们引入了CardioMM，一个通用重建基础模型，能够动态适应异构的快速CMR成像场景。CardioMM将语义上下文理解与基于物理的数据一致性相结合，提供跨不同扫描仪、协议和患者表现的稳健重建。全面评估表明，CardioMM在内部中心实现了最先进的性能，并对未见过的外部环境表现出强大的零样本泛化能力。即使在高达24倍的成像加速下，CardioMM也能可靠地保留关键心脏表型、定量心肌生物标志物和诊断图像质量，能够在不损害临床完整性的情况下显著提高CMR检查吞吐量。总之，我们的开源MMCMR-427K数据库和CardioMM框架为建立高吞吐量、高质量和临床可及的心血管成像提供了可扩展的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal cardiovascular magnetic resonance (CMR) imaging provides comprehensive and non-invasive insights into cardiovascular disease (CVD) diagnosis and underlying mechanisms. Despite decades of advancements, its widespread clinical adoption remains constrained by prolonged scan times and heterogeneity across medical environments. This underscores the urgent need for a generalist reconstruction foundation model for ultra-fast CMR imaging, one capable of adapting across diverse imaging scenarios and serving as the essential substrate for all downstream analyses. To enable this goal, we curate MMCMR-427K, the largest and most comprehensive multimodal CMR k-space database to date, comprising 427,465 multi-coil k-space data paired with structured metadata across 13 international centers, 12 CMR modalities, 15 scanners, and 17 CVD categories in populations across three continents. Building on this unprecedented resource, we introduce CardioMM, a generalist reconstruction foundation model capable of dynamically adapting to heterogeneous fast CMR imaging scenarios. CardioMM unifies semantic contextual understanding with physics-informed data consistency to deliver robust reconstructions across varied scanners, protocols, and patient presentations. Comprehensive evaluations demonstrate that CardioMM achieves state-of-the-art performance in the internal centers and exhibits strong zero-shot generalization to unseen external settings. Even at imaging acceleration up to 24x, CardioMM reliably preserves key cardiac phenotypes, quantitative myocardial biomarkers, and diagnostic image quality, enabling a substantial increase in CMR examination throughput without compromising clinical integrity. Together, our open-access MMCMR-427K database and CardioMM framework establish a scalable pathway toward high-throughput, high-quality, and clinically accessible cardiovascular imaging.</description>
      <author>example@mail.com (Zi Wang, Mingkai Huang, Zhang Shi, Hongjie Hu, Lan Lan, Hui Zhang, Yan Li, Xi Hu, Qing Lu, Zongming Zhu, Qiong Yao, Yuxiang Dai, Fanwen Wang, Yinzhe Wu, Jun Lyu, Qianqian Gao, Guangming Xu, Zhenxuan Zhang, Haosen Zhang, Qing Li, Guangming Wang, Tianxing He, Lizhen Lan, Siyue Li, Le Xue, Mengting Sun, Yuntong Lyu, Junpu Hu, Jiayu Zhu, Rizwan Ahmad, Zhengyu Bu, Xianling Qian, Guanke Cai, Ruiyu Cao, Weirui Cai, Chang Xu, Yuyang Ren, Feidan Yu, Siying Ma, Ziqiang Xu, Xinran Chen, Sha Hua, Daniel Kim, Yajing Zhang, Chen Ouyang, Wenjia Bai, Jing Qin, Yucheng Yang, Daniel Rueckert, He Wang, Qian Tao, Claudia Prieto, Michael Markl, Alistair Young, Lianming Wu, Shuo Wang, Chen Qin, Mengsu Zeng, Xihong Hu, Haibo Xu, Xiaobo Qu, Hao Li, Guang Yang, Chengyan Wang)</author>
      <guid isPermaLink="false">2512.21652v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</title>
      <link>http://arxiv.org/abs/2512.21643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 12 figures. ICLR 2026 submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Omni-Weather，这是首个统一天气生成和理解的多模态基础模型，解决了现有方法将这两个目标孤立处理的问题。&lt;h4&gt;背景&lt;/h4&gt;天气建模需要准确的预测和机制解释，而现有方法将生成与理解分开处理，导致两者无法协同工作。&lt;h4&gt;目的&lt;/h4&gt;解决天气建模中预测和解释分离的问题，创建一个能够同时处理天气生成和理解任务的统一模型。&lt;h4&gt;方法&lt;/h4&gt;Omni-Weather整合了雷达编码器用于天气生成任务，采用共享的自注意力机制进行统一处理，并构建了用于天气生成因果推理的Chain-of-Thought数据集，实现可解释输出和改进的感知质量。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明Omni-Weather在天气生成和理解方面都达到最先进性能；天气领域的生成和理解任务可以相互增强；统一天气生成和理解的方法具有可行性和价值。&lt;h4&gt;结论&lt;/h4&gt;Omni-Weather成功实现了天气生成和理解的统一，证明了这种统一方法在性能和可解释性方面的优势。&lt;h4&gt;翻译&lt;/h4&gt;天气建模需要准确的预测和机制解释，然而现有方法将这些目标孤立处理，将生成与理解分开。为解决这一差距，我们提出了Omni-Weather，这是第一个统一天气生成和理解的多模态基础模型。Omni-Weather整合了雷达编码器用于天气生成任务，随后使用共享的自注意力机制进行统一处理。此外，我们构建了用于天气生成因果推理的Chain-of-Thought数据集，使输出具有可解释性并提高了感知质量。大量实验表明，Omni-Weather在天气生成和理解方面都达到了最先进的性能。我们的研究进一步表明，天气领域的生成和理解任务可以相互促进。Omni-Weather还证明了统一天气生成和理解的可行性和价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.</description>
      <author>example@mail.com (Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai)</author>
      <guid isPermaLink="false">2512.21643v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models</title>
      <link>http://arxiv.org/abs/2512.21572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为RefineBridge的新改进模块，基于Schrödinger Bridge生成框架，用于增强基于Transformer的时间序列基础模型(TSFMs)在金融时间序列预测中的性能。&lt;h4&gt;背景&lt;/h4&gt;金融时间序列预测对于基于Transformer的时间序列基础模型(TSFMs)特别具有挑战性，因为数据中存在非平稳性、重尾分布和高频噪声。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来改进TSFMs在金融数据上的预测性能，解决现有LoRA方法在金融数据上表现不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;RefineBridge模块基于可处理的Schrödinger Bridge生成框架构建，将TSFM的预测作为生成先验，观察到的真实值作为目标，学习上下文条件随机传输映射，逐步从低质量先验接近真实目标。&lt;h4&gt;主要发现&lt;/h4&gt;在多个金融基准测试上的模拟表明，RefineBridge在不同预测时间范围内持续提高了最先进TSFMs的性能。&lt;h4&gt;结论&lt;/h4&gt;RefineBridge是一种有效的方法，可以改进基于Transformer的时间序列基础模型在金融时间序列预测任务中的表现。&lt;h4&gt;翻译&lt;/h4&gt;金融时间序列预测对于基于Transformer的时间序列基础模型(TSFMs)尤其具有挑战性，因为数据中存在非平稳性、重尾分布和高频噪声。低秩适应(LoRA)已成为一种流行的参数高效方法，用于将预训练的TSFMs适应到下游数据领域。然而，它在金融数据上仍然表现不佳，因为它保留了TSFMs的网络架构和训练目标，而不是补充基础模型。为了进一步增强TSFMs，我们提出了一种名为RefineBridge的新型改进模块，它构建在可处理的Schrödinger Bridge(SB)生成框架之上。将TSFM的预测作为生成先验，观察到的真实值作为目标，RefineBridge学习上下文条件随机传输映射，以改进TSFM预测，逐步从低质量先验接近真实目标。在多个金融基准测试上的模拟表明，RefineBridge在不同预测时间范围内持续提高了最先进TSFMs的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial time series forecasting is particularly challenging for transformer-based time series foundation models (TSFMs) due to non-stationarity, heavy-tailed distributions, and high-frequency noise present in data. Low-rank adaptation (LoRA) has become a popular parameter-efficient method for adapting pre-trained TSFMs to downstream data domains. However, it still underperforms in financial data, as it preserves the network architecture and training objective of TSFMs rather than complementing the foundation model. To further enhance TSFMs, we propose a novel refinement module, RefineBridge, built upon a tractable Schrödinger Bridge (SB) generative framework. Given the forecasts of TSFM as generative prior and the observed ground truths as targets, RefineBridge learns context-conditioned stochastic transport maps to improve TSFM predictions, iteratively approaching the ground-truth target from even a low-quality prior. Simulations on multiple financial benchmarks demonstrate that RefineBridge consistently improves the performance of state-of-the-art TSFMs across different prediction horizons.</description>
      <author>example@mail.com (Anthony Bolton, Wuyang Zhou, Zehua Chen, Giorgos Iacovides, Danilo Mandic)</author>
      <guid isPermaLink="false">2512.21572v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training</title>
      <link>http://arxiv.org/abs/2512.21515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种困惑度感知的数据扩展定律，用于优化持续预训练过程中的数据选择，解决了简单增加数据导致的边际收益递减问题，通过量化候选训练样本的信息困惑度景观，实现了高效数据子集选择，在医学和通用领域基准测试上取得了优越性能。&lt;h4&gt;背景&lt;/h4&gt;持续预训练是将基础模型适应特定领域应用的基本方法。预训练的扩展定律定义了数据集规模和大语言模型测试损失之间的幂律关系。然而，简单地增加持续预训练的数据会导致边际收益迅速减少，造成数据利用次优和训练效率低下。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的困惑度感知数据扩展定律，建立特定领域数据的困惑度景观与测试损失之间的预测关系，以优化数据选择过程。&lt;h4&gt;方法&lt;/h4&gt;利用预训练模型在领域数据上推导的困惑度作为估计知识差距的代理，有效量化候选训练样本的信息困惑度景观。通过在不同困惑度范围内拟合这个扩展定律，实现自适应选择高效数据子集，优先选择能最大化知识吸收同时最小化冗余和噪声的内容。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够识别接近最优的训练子集，在医学和通用领域基准测试上取得优越性能，证明了困惑度感知数据选择的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过困惑度感知的数据选择方法，可以优化持续预训练过程，提高数据利用效率和模型性能，解决了简单增加数据导致的边际收益递减问题。&lt;h4&gt;翻译&lt;/h4&gt;持续预训练是将基础模型适应特定领域应用的基本方法。预训练的扩展定律定义了数据集规模和大语言模型测试损失之间的幂律关系。然而，简单地增加持续预训练数据的边际收益会迅速减少，导致数据利用次优和训练效率低下。为解决这一挑战，我们提出了一种新的困惑度感知数据扩展定律，建立了特定领域数据的困惑度景观与测试损失之间的预测关系。我们的方法利用预训练模型在领域数据上推导的困惑度作为估计知识差距的代理，有效量化了候选训练样本的信息困惑度景观。通过在不同困惑度范围内拟合这个扩展定律，我们能够自适应选择高效数据子集，优先选择能最大化知识吸收同时最小化冗余和噪声的内容。大量实验证明，我们的方法能够一致地识别接近最优的训练子集，并在医学和通用领域基准测试上取得优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual Pre-training (CPT) serves as a fundamental approach for adapting foundation models to domain-specific applications. Scaling laws for pre-training define a power-law relationship between dataset size and the test loss of an LLM. However, the marginal gains from simply increasing data for CPT diminish rapidly, yielding suboptimal data utilization and inefficient training. To address this challenge, we propose a novel perplexity-aware data scaling law to establish a predictive relationship between the perplexity landscape of domain-specific data and the test loss. Our approach leverages the perplexity derived from the pre-trained model on domain data as a proxy for estimating the knowledge gap, effectively quantifying the informational perplexity landscape of candidate training samples. By fitting this scaling law across diverse perplexity regimes, we enable adaptive selection of high-utility data subsets, prioritizing content that maximizes knowledge absorption while minimizing redundancy and noise. Extensive experiments demonstrate that our method consistently identifies near-optimal training subsets and achieves superior performance on both medical and general-domain benchmarks.</description>
      <author>example@mail.com (Lei Liu, Hao Zhu, Yue Shen, Zhixuan Chu, Jian Wang, Jinjie Gu, Kui Ren)</author>
      <guid isPermaLink="false">2512.21515v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>EVE: A Generator-Verifier System for Generative Policies</title>
      <link>http://arxiv.org/abs/2512.21430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EVE的模块化生成器-验证器交互框架，通过在测试时利用零样本VLM验证器来增强预训练的生成策略性能，无需额外训练。该框架在多种操作任务中一致提高了任务成功率。&lt;h4&gt;背景&lt;/h4&gt;基于生成架构的视觉运动策略在分布变化下性能下降且恢复能力有限。语言建模领域通过测试时计算扩展和零样本验证模块改进了候选解决方案，但这种生成-验证框架在视觉运动策略领域尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究生成策略如何通过额外推理时计算和零样本VLM验证器受益，构建一个系统化的生成-验证框架来提升预训练生成策略在测试时的性能。&lt;h4&gt;方法&lt;/h4&gt;EVE框架将冻结的基础策略与多个零样本VLM验证器代理结合，验证器提出动作改进建议，动作融合器聚合验证器输出并与基础策略预测融合产生最终动作。研究了具有不同能力的验证器系统中生成器-验证器信息交互的设计选择。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的操作任务套件中，EVE在无需任何额外策略训练的情况下，一致提高了任务成功率。通过消融实验分离了验证器能力和动作融合器策略的贡献。&lt;h4&gt;结论&lt;/h4&gt;EVE为构建可扩展的模块化生成器-验证器系统提供了实用指南，可用于具身控制，展示了零样本验证器在增强预训练生成策略方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;基于生成架构的视觉运动策略，如扩散和基于流的匹配，已表现出强大的性能，但在分布变化下性能下降，显示出有限的恢复能力，而无需昂贵的微调。在语言建模领域，测试时计算扩展通过利用额外的推理时计算来改进候选解决方案，从而革新了现代LLM的推理能力。这些方法通常以零样本方式利用基础模型作为验证模块来合成改进的候选解决方案。在这项工作中，我们假设生成策略可以通过采用零样本VLM验证器的额外推理时计算获得类似的好处。关于通过生成-验证框架改进策略性能的系统分析在当前文献中相对未被探索。为此，我们引入了EVE - 一个模块化的生成器-验证器交互框架 - 它可以在测试时提高预训练生成策略的性能，无需额外训练。EVE将一个冻结的基础策略与多个零样本VLM验证器代理包装在一起。每个验证器向基础策略候选动作提出动作改进建议，而动作融合器将聚合的验证器输出融合到基础策略动作预测中，以产生最终执行的动作。我们研究了具有不同能力的验证器系统中生成器-验证器信息交互的设计选择。在多样化的操作任务套件中，EVE一致提高了任务成功率，而无需任何额外的策略训练。通过大量的消融实验，我们分离了验证器能力和动作融合器策略的贡献，为构建可扩展的模块化生成器-验证器系统提供了实用指南，用于具身控制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visuomotor policies based on generative architectures such as diffusion and flow-based matching have shown strong performance but degrade under distribution shifts, demonstrating limited recovery capabilities without costly finetuning. In the language modeling domain, test-time compute scaling has revolutionized reasoning capabilities of modern LLMs by leveraging additional inference-time compute for candidate solution refinement. These methods typically leverage foundation models as verification modules in a zero-shot manner to synthesize improved candidate solutions. In this work, we hypothesize that generative policies can similarly benefit from additional inference-time compute that employs zero-shot VLM-based verifiers. A systematic analysis of improving policy performance through the generation-verification framework remains relatively underexplored in the current literature. To this end, we introduce EVE - a modular, generator-verifier interaction framework - that boosts the performance of pretrained generative policies at test time, with no additional training. EVE wraps a frozen base policy with multiple zero-shot, VLM-based verifier agents. Each verifier proposes action refinements to the base policy candidate actions, while an action incorporator fuses the aggregated verifier output into the base policy action prediction to produce the final executed action. We study design choices for generator-verifier information interfacing across a system of verifiers with distinct capabilities. Across a diverse suite of manipulation tasks, EVE consistently improves task success rates without any additional policy training. Through extensive ablations, we isolate the contribution of verifier capabilities and action incorporator strategies, offering practical guidelines to build scalable, modular generator-verifier systems for embodied control.</description>
      <author>example@mail.com (Yusuf Ali, Gryphon Patlin, Karthik Kothuri, Muhammad Zubair Irshad, Wuwei Liang, Zsolt Kira)</author>
      <guid isPermaLink="false">2512.21430v1</guid>
      <pubDate>Mon, 29 Dec 2025 14:58:24 +0800</pubDate>
    </item>
    <item>
      <title>DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation</title>
      <link>http://arxiv.org/abs/2512.19744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages and 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepBridge是一个8万行的Python库，统一了多维验证、自动合规性验证、知识蒸馏和合成数据生成功能，显著提高了AI模型验证效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;现有AI工具分散且效率低下，需要统一的平台进行多维验证、合规性检查和模型优化。&lt;h4&gt;目的&lt;/h4&gt;提供一个统一的解决方案，整合多维验证、自动合规性检查、知识蒸馏和合成数据生成功能，提高AI模型验证效率和可靠性。&lt;h4&gt;方法&lt;/h4&gt;DeepBridge提供5个验证套件（公平性、鲁棒性、不确定性、弹性和超参数敏感性）、自动EEOC/ECOA/GDPR验证、多格式报告系统、HPM-KD知识蒸馏框架和基于Dask的可扩展合成数据生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过6个案例研究表明，DeepBridge将验证时间减少89%（从150分钟降至17分钟），自动检测公平性违规覆盖率达到100%（10/10特征），HPM-KD在压缩比2.3-7倍时表现优异，可用性研究显示SUS评分87.5（前10%，'优秀'），95%成功率，认知负荷低。&lt;h4&gt;结论&lt;/h4&gt;DeepBridge是一个高效、易用的开源工具，在AI模型验证和优化方面表现卓越，能够显著提高工作效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了DeepBridge，一个8万行的Python库，它统一了多维验证、自动合规性验证、知识蒸馏和合成数据生成。DeepBridge提供：(i)5个验证套件（包含15个指标的公平性、包含弱点检测的鲁棒性、通过保形预测的不确定性、包含5种漂移类型的弹性和超参数敏感性），(ii)自动EEOC/ECOA/GDPR验证，(iii)多格式报告系统（交互式/静态HTML、PDF、JSON），(iv)用于知识蒸馏的HPM-KD框架（结合元学习），以及(v)通过Dask实现的可扩展合成数据生成。通过6个案例研究（信用评分、招聘、医疗保健、抵押贷款、保险、欺诈），我们证明DeepBridge将验证时间减少89%（17分钟对比使用分散工具的150分钟），自动检测公平性违规实现完全覆盖（10/10特征对比现有工具的2/10），并在几分钟内生成审计就绪的报告。HPM-KD在压缩比2.3-7倍（CIFAR100）范围内表现出一致优越性，比直接训练高1.00-2.04个百分点（p&lt;0.05），证实了知识蒸馏在较大教师-学生差距时的有效性。20名参与者的可用性研究显示SUS评分87.5（前10%，'优秀'），95%成功率，以及低认知负荷（NASA-TLX 28/100）。DeepBridge在MIT许可证下开源于https://github.com/deepbridge/deepbridge，完整文档位于https://deepbridge.readthedocs.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present DeepBridge, an 80K-line Python library that unifies multi-dimensional validation, automatic compliance verification, knowledge distillation, and synthetic data generation. DeepBridge offers: (i) 5 validation suites (fairness with 15 metrics, robustness with weakness detection, uncertainty via conformal prediction, resilience with 5 drift types, hyperparameter sensitivity), (ii) automatic EEOC/ECOA/GDPR verification, (iii) multi-format reporting system (interactive/static HTML, PDF, JSON), (iv) HPM-KD framework for knowledge distillation with meta-learning, and (v) scalable synthetic data generation via Dask. Through 6 case studies (credit scoring, hiring, healthcare, mortgage, insurance, fraud) we demonstrate that DeepBridge: reduces validation time by 89% (17 min vs. 150 min with fragmented tools), automatically detects fairness violations with complete coverage (10/10 features vs. 2/10 from existing tools), generates audit-ready reports in minutes. HPM-KD demonstrates consistent superiority across compression ratios 2.3--7x (CIFAR100): +1.00--2.04pp vs. Direct Training (p&lt;0.05), confirming that Knowledge Distillation is effective at larger teacher-student gaps. Usability study with 20 participants shows SUS score 87.5 (top 10%, ``excellent''), 95% success rate, and low cognitive load (NASA-TLX 28/100). DeepBridge is open-source under MIT license at https://github.com/deepbridge/deepbridge, with complete documentation at https://deepbridge.readthedocs.io</description>
      <author>example@mail.com (Gustavo Coelho Haase, Paulo Henrique Dourado da Silva)</author>
      <guid isPermaLink="false">2512.19744v1</guid>
      <pubDate>Fri, 26 Dec 2025 14:02:08 +0800</pubDate>
    </item>
  <item>
      <title>Generalization of Diffusion Models Arises with a Balanced Representation Space</title>
      <link>http://arxiv.org/abs/2512.20963v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages, 19 figures. The first two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了扩散模型中记忆和泛化的区别，通过表征学习的视角揭示了扩散模型的学习机制，并提出了基于表征的检测方法和无需训练的编辑技术。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在生成高质量、多样化的样本方面表现出色，但当过度拟合训练目标时，存在记忆训练数据的风险。&lt;h4&gt;目的&lt;/h4&gt;通过表征学习的视角分析扩散模型中记忆和泛化之间的区别，并探索其在实际应用中的价值。&lt;h4&gt;方法&lt;/h4&gt;研究一个两层ReLU去噪自编码器(DAE)来证明记忆和泛化的区别；在真实世界的无条件和文本到图像扩散模型上验证这些理论发现；提出基于表征的检测记忆的方法和一种无需训练的编辑技术。&lt;h4&gt;主要发现&lt;/h4&gt;记忆对应于模型在编码和解码的学到的权重中存储原始训练样本，产生局部化的'尖峰'表征；泛化发生在模型捕获局部数据统计时，产生'平衡'的表征；在深度生成模型中出现了相同的表征结构。&lt;h4&gt;结论&lt;/h4&gt;学习良好的表征是新颖且有意义的生成建模的核心。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型在生成高质量、多样化的样本方面表现出色，但当过度拟合训练目标时，存在记忆训练数据的风险。我们通过表征学习的视角分析了扩散模型中记忆和泛化之间的区别。通过研究一个两层ReLU去噪自编码器(DAE)，我们证明了(i)记忆对应于模型在编码和解码的学到的权重中存储原始训练样本，产生局部化的'尖峰'表征，而(ii)泛化发生在模型捕获局部数据统计时，产生'平衡'的表征。此外，我们在真实世界的无条件和文本到图像扩散模型上验证了这些理论发现，证明了相同的表征结构出现在具有重大实际意义的深度生成模型中。基于这些见解，我们提出了一种基于表征的检测记忆的方法和一种无需训练的编辑技术，允许通过表征转向进行精确控制。总之，我们的结果强调了学习良好的表征是新颖且有意义的生成建模的核心。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized "spiky" representations, whereas (ii) generalization arises when the model captures local data statistics, producing "balanced" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.</description>
      <author>example@mail.com (Zekai Zhang, Xiao Li, Xiang Li, Lianghe Shi, Meng Wu, Molei Tao, Qing Qu)</author>
      <guid isPermaLink="false">2512.20963v1</guid>
      <pubDate>Fri, 26 Dec 2025 14:02:08 +0800</pubDate>
    </item>
    <item>
      <title>Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms</title>
      <link>http://arxiv.org/abs/2512.15793v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Acceppt by Asia-Pacific Chapter of the Association for Computational Linguistics (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ClarityEthic的新型伦理评估方法，通过生成人类行为背后相互冲突的社会规范来增强价值预测和解释，使用对比学习策略加强语言模型的道德推理能力。&lt;h4&gt;背景&lt;/h4&gt;人类行为常受社会规范指导，而当前利用大规模数据训练的AI系统在评估人类行为时可能难以解释，因此不可信。&lt;h4&gt;目的&lt;/h4&gt;通过考虑社会规范来模拟人类评估者，帮助AI模型更好地理解和预测行为的正负面评价，解决AI系统难以解释的问题。&lt;h4&gt;方法&lt;/h4&gt;引入ClarityEthic伦理评估方法，通过生成人类行为背后相互冲突的社会规范来增强价值预测和解释，使用对比学习策略加强语言模型的道德推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;多个社会规范可能同时影响人的行为，相互冲突的社会规范会产生紧张关系并直接影响人类行为，如举报犯罪时在勇敢和自我保护之间的权衡。&lt;h4&gt;结论&lt;/h4&gt;实验表明该方法优于基线方法，人类评估证实生成的社会规范为人类行为评估提供了合理解释。&lt;h4&gt;翻译&lt;/h4&gt;人类行为通常受社会规范指导或约束，这些规范被定义为共享的、常识性的规则。例如，在'举报目击的犯罪'这一行为背后，有告知我们行为的社会规范，如'举报犯罪应该勇敢'。当前AI系统通过利用大规模数据训练来评估人类行为的正负面评价，但这些训练没有基于明确的规范，因此可能难以解释，从而不可信。通过考虑社会规范来模拟人类评估者，可以帮助AI模型更好地理解和预测行为的正负面评价。虽然多个社会规范可能同时起作用，但相互冲突的规范会产生紧张关系并直接影响人类行为。例如，在决定是否'举报目击的犯罪'时，人们可能会在'勇敢'和'自我保护'之间权衡。在本文中，我们介绍了ClarityEthic，一种新型的伦理评估方法，通过生成人类行为背后相互冲突的社会规范来增强价值预测和解释，使用对比学习策略加强语言模型的道德推理能力。广泛的实验表明，我们的方法优于强大的基线方法，人类评估证实生成的社会规范为人类行为的评估提供了合理的解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human behaviors are often guided or constrained by social norms, which are defined as shared, commonsense rules. For example, underlying an action ``\textit{report a witnessed crime}" are social norms that inform our conduct, such as ``\textit{It is expected to be brave to report crimes}''. Current AI systems that assess valence (i.e., support or oppose) of human actions by leveraging large-scale data training not grounded on explicit norms may be difficult to explain, and thus untrustworthy. Emulating human assessors by considering social norms can help AI models better understand and predict valence. While multiple norms come into play, conflicting norms can create tension and directly influence human behavior. For example, when deciding whether to ``\textit{report a witnessed crime}'', one may balance \textit{bravery} against \textit{self-protection}. In this paper, we introduce \textit{ClarityEthic}, a novel ethical assessment approach, to enhance valence prediction and explanation by generating conflicting social norms behind human actions, which strengthens the moral reasoning capabilities of language models by using a contrastive learning strategy. Extensive experiments demonstrate that our method outperforms strong baseline approaches, and human evaluations confirm that the generated social norms provide plausible explanations for the assessment of human behaviors.</description>
      <author>example@mail.com (Yuxi Sun, Wei Gao, Hongzhan Lin, Jing Ma, Wenxuan Zhang)</author>
      <guid isPermaLink="false">2512.15793v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
  <item>
      <title>GraphCue for SDN Configuration Code Synthesis</title>
      <link>http://arxiv.org/abs/2512.17371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GraphCue是一种用于自动化SDN配置的拓扑检索和智能体循环框架，通过图结构表示和对比学习实现高效配置&lt;h4&gt;背景&lt;/h4&gt;SDN自动化配置面临复杂性和约束条件的挑战，需要更智能的解决方案&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自动化SDN配置的系统，通过拓扑感知的检索和基于约束的条件生成提高配置准确性和效率&lt;h4&gt;方法&lt;/h4&gt;将配置案例抽象为JSON图，使用轻量级三层图卷积网络进行嵌入训练，通过对比学习优化，将最近验证的参考注入结构化提示约束代码生成，并由验证器执行候选配置并反馈形成闭环&lt;h4&gt;主要发现&lt;/h4&gt;在628个验证案例中，GraphCue在20次迭代内实现88.2%的通过率，95%的验证循环在9秒内完成，消融研究表明拓扑感知检索和基于约束的条件是性能的关键驱动因素&lt;h4&gt;结论&lt;/h4&gt;GraphCue结合拓扑感知检索和基于约束的条件生成，显著提高了SDN自动化配置的效率和准确性&lt;h4&gt;翻译&lt;/h4&gt;我们提出了GraphCue，一种用于自动化SDN配置的拓扑检索和智能体循环框架。每个案例被抽象为JSON图，并使用通过对比学习训练的轻量级三层GCN进行嵌入。最近验证的参考被注入到约束代码生成的结构化提示中，而验证器通过执行候选配置并将失败反馈回智能体来形成闭环。在628个验证案例中，GraphCue在20次迭代内实现了88.2%的通过率，95%的验证循环在9秒内完成。没有检索或结构化提示的消融研究表现明显较差，表明拓扑感知检索和基于约束的条件是性能的关键驱动因素。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GraphCue, a topology-grounded retrieval and agent-in-the-loop framework for automated SDN configuration. Each case is abstracted into a JSON graph and embedded using a lightweight three-layer GCN trained with contrastive learning. The nearest validated reference is injected into a structured prompt that constrains code generation, while a verifier closes the loop by executing the candidate configuration and feeding failures back to the agent. On 628 validation cases, GraphCue achieves an 88.2 percent pass rate within 20 iterations and completes 95 percent of verification loops within 9 seconds. Ablation studies without retrieval or structured prompting perform substantially worse, indicating that topology-aware retrieval and constraint-based conditioning are key drivers of performance.</description>
      <author>example@mail.com (Haomin Qi, Fengfei Yu, Chengbo Huang)</author>
      <guid isPermaLink="false">2512.17371v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>STORM: Search-Guided Generative World Models for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2512.18477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了STORM（搜索引导的生成世界模型），一种用于机器人操作中时空推理的新颖框架，统一了基于扩散的动作生成、条件视频预测和基于搜索的规划。&lt;h4&gt;背景&lt;/h4&gt;之前的Vision-Language-Action模型依赖于抽象的潜在动态或将推理委托给语言组件，缺乏明确的视觉基础进行决策。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在机器人操作中进行时空推理的框架，实现可解释性和前瞻性驱动的决策制定。&lt;h4&gt;方法&lt;/h4&gt;基于扩散的VLA策略提出多样化的候选动作，生成式视频世界模型模拟它们的视觉和奖励结果，蒙特卡洛树搜索通过前瞻性评估选择性细化计划。&lt;h4&gt;主要发现&lt;/h4&gt;在SimplerEnv操作基准测试中，STORM实现了51.0%的新最先进平均成功率，超越了CogACT等基线模型；奖励增强的视频预测将Frechet视频距离减少了75%以上；STORM表现出强大的重新规划和故障恢复行为。&lt;h4&gt;结论&lt;/h4&gt;搜索引导的生成世界模型在长时程机器人操作中具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了STORM（搜索引导的生成世界模型），一种用于机器人操作中时空推理的新颖框架，统一了基于扩散的动作生成、条件视频预测和基于搜索的规划。与之前依赖抽象潜在动态或将推理委托给语言组件的Vision-Language-Action模型不同，STORM将规划建立在明确的视觉展开基础上，实现了可解释性和前瞻性驱动的决策制定。基于扩散的VLA策略提出多样化的候选动作，生成式视频世界模型模拟它们的视觉和奖励结果，蒙特卡洛树搜索通过前瞻性评估选择性细化计划。在SimplerEnv操作基准测试中的实验表明，STORM实现了51.0%的新最先进平均成功率，超越了强大的基线模型如CogACT。奖励增强的视频预测显著提高了时空保真度和任务相关性，将Frechet视频距离减少了75%以上。此外，STORM表现出强大的重新规划和故障恢复行为，突显了搜索引导的生成世界模型在长时程机器人操作中的优势。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉-语言-行动（VLA）模型在机器人操作任务中的局限性，即这些模型将推理委托给语言组件或抽象潜在动力学，导致物理交互中关键信息（如空间关系、接触动力学和行动因果后果）丢失。这个问题非常重要，因为机器人操作是实现具身智能（通用人工智能的关键里程碑）的基础，使机器人能够在物理世界中完成复杂任务如烹饪、工具使用或物体操作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先诊断了现有VLA模型的架构瓶颈，然后提出从抽象语言推理转向视觉预见的范式转变，灵感来自人类认知能力。他们确立了'先预测后行动'的核心原则，设计了STORM框架，整合三个关键组件：基于扩散的VLA策略、条件视频预测模型和MCTS规划器。作者确实借鉴了现有工作，包括扩散模型处理多模态分布、视频预测作为世界模型的概念、MCTS与学习动力学模型结合的规划方法，但创新在于将这些方法以新颖方式组合，并将搜索建立在视觉展开而非抽象潜在空间上。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'先预测后行动'原则，让机器人在执行动作前能模拟和评估潜在行动的时空结果，不同于传统反应式策略。整体流程包括：1) 将任务形式化为POMDP；2) 在每个时间步执行决策循环：使用扩散VLA提出多样化候选动作，用视频预测模型模拟这些动作的视觉结果和奖励，用MCTS探索这些模拟未来选择最优策略；3) 执行访问次数最多的动作，实现稳健规划和失败恢复。系统通过选择、扩展、评估和反向传播的迭代过程进行决策。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 整合扩散行动生成、视频预测和搜索规划的显式时空推理框架；2) 将搜索建立在明确视觉展开而非抽象潜在空间上；3) 奖励增强的视频预测作为高效世界模型；4) 模块化架构可与现有VLA模型集成；5) 失败后重新规划的能力。相比之前工作，STORM避免了VLA模型的语言抽象局限性，超越了单纯视频预测模型的孤立使用，区别于在抽象空间中规划的MuZero等方法，并将扩散模型作为提议策略与模拟搜索组件协同工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; STORM通过整合基于扩散的视觉-语言-行动策略、生成视频世界模型和蒙特卡洛树搜索，实现了在机器人操作任务中基于显式视觉预见的稳健时空推理，显著提高了任务成功率和失败恢复能力，为可解释、预见驱动的决策制定建立了新范式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.</description>
      <author>example@mail.com (Wenjun Lin, Jensen Zhang, Kaitong Cai, Keze Wang)</author>
      <guid isPermaLink="false">2512.18477v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</title>
      <link>http://arxiv.org/abs/2512.18745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的多代理框架InSight-o3，包括视觉推理代理和视觉搜索代理，用于增强AI代理的图像推理能力。他们还提出了O3-Bench基准测试和广义视觉搜索任务，显著提高了前沿多模态模型的性能。&lt;h4&gt;背景&lt;/h4&gt;当前开放的多模态AI代理在处理需要复杂推理的视觉任务（如分析包含密集图表/图表的文档和导航地图）方面表现不足，缺乏必要的推理能力。&lt;h4&gt;目的&lt;/h4&gt;开发新的基准测试和多代理框架，以评估和提升AI代理在多模态推理方面的能力，特别是在需要关注视觉细节的任务中。&lt;h4&gt;方法&lt;/h4&gt;提出了O3-Bench基准测试，评估多模态推理能力；开发了InSight-o3多代理框架，包括视觉推理代理(vReasoner)和视觉搜索代理(vSearcher)；引入了广义视觉搜索任务；通过强化学习训练了专门用于此任务的多模态大语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;即使是最先进的系统如OpenAI o3在O3-Bench上的准确率也只有40.8%；提出的vSearcher作为即插即用代理，能够显著提高前沿多模态模型在各种基准测试上的性能。&lt;h4&gt;结论&lt;/h4&gt;InSight-o3框架是向强大的类似o3的开放系统迈出的具体一步，能够有效提升AI代理的图像推理能力。&lt;h4&gt;翻译&lt;/h4&gt;AI代理'用图像思考'的能力需要推理和感知的复杂结合。然而，当前开放的多模态代理在处理真实世界任务（如分析包含密集图表/图表的文档和导航地图）所需的推理方面仍然存在不足。为了解决这一差距，我们引入了O3-Bench，这是一个新的基准测试，旨在评估对视觉细节交错关注的多模态推理。O3-Bench具有具有挑战性的问题，要求代理通过多步推理从不同图像区域拼凑微妙的视觉信息。这些问题即使对于OpenAI o3等前沿系统也极具挑战性，它们在O3-Bench上的准确率仅为40.8%。为了取得进展，我们提出了InSight-o3，一个由视觉推理代理(vReasoner)和视觉搜索代理(vSearcher)组成的多代理框架，我们为vSearcher引入了广义视觉搜索任务——定位以自由形式语言描述的关系性、模糊性或概念性区域，而不仅仅是自然图像中的简单对象或图形。然后，我们通过强化学习为这项任务专门训练了一个多模态大语言模型。作为一个即插即用的代理，我们的vSearcher能够赋能前沿多模态模型（作为vReasoner），显著提高它们在广泛基准测试上的性能。这标志着向强大的类似o3的开放系统迈出的具体一步。我们的代码和数据集可以在https://github.com/m-Just/InSight-o3找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ability for AI agents to "think with images" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .</description>
      <author>example@mail.com (Kaican Li, Lewei Yao, Jiannan Wu, Tiezheng Yu, Jierun Chen, Haoli Bai, Lu Hou, Lanqing Hong, Wei Zhang, Nevin L. Zhang)</author>
      <guid isPermaLink="false">2512.18745v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms</title>
      <link>http://arxiv.org/abs/2512.19037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于多类机器学习的入侵检测框架，用于应对物联网设备和Wi-Fi网络面临的安全威胁，特别是KRACK和Kr00k攻击。该框架结合了先进的特征选择技术和集成学习方法，在AWID3数据集上取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;物联网设备的激增及其对Wi-Fi网络的依赖带来了显著的安全漏洞，特别是KRACK和Kr00k攻击，这些攻击利用WPA2加密的弱点来拦截和操作敏感数据。传统的使用分类器的入侵检测系统面临模型过拟合、特征提取不完整和高误报率等挑战，限制了它们在实际部署中的有效性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，本研究提出了一种稳健的多类机器学习入侵检测框架，旨在提高检测准确性，减少误报率，并增强模型在实际环境中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;该方法集成了先进的特征选择技术来识别关键属性，减少冗余并提高检测准确性。实现了两种不同的机器学习架构：一个基线分类器管道和一个堆叠集成模型，该模型结合了噪声注入、主成分分析(PCA)和元学习，以提高泛化能力和减少误报率。&lt;h4&gt;主要发现&lt;/h4&gt;在AWID3数据集上的评估表明，所提出的集成架构取得了卓越的性能，准确率达到98%，精确率达到98%，召回率达到98%，误报率仅为2%，优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;这项工作证明了将预处理策略与集成学习相结合对于加强网络防御复杂Wi-Fi攻击的有效性，为物联网环境提供了可扩展和可靠的解决方案。未来的工作包括实时部署和对抗性弹性测试，以进一步增强模型的适应性。&lt;h4&gt;翻译&lt;/h4&gt;物联网设备的激增及其对Wi-Fi网络的依赖引入了显著的安全漏洞，特别是KRACK和Kr00k攻击，这些攻击利用WPA2加密的弱点来拦截和操作敏感数据。传统的使用分类器的入侵检测系统面临模型过拟合、特征提取不完整和高误报率等挑战，限制了它们在实际部署中的有效性。为了解决这些挑战，本研究提出了一种稳健的多类机器学习入侵检测框架。该方法集成了先进的特征选择技术来识别关键属性，减少冗余并提高检测准确性。实现了两种不同的机器学习架构：一个基线分类器管道和一个堆叠集成模型，该模型结合了噪声注入、主成分分析(PCA)和元学习，以提高泛化能力和减少误报率。在AWID3数据集上的评估表明，所提出的集成架构取得了卓越的性能，准确率达到98%，精确率达到98%，召回率达到98%，误报率仅为2%，优于现有的最先进方法。这项工作证明了将预处理策略与集成学习相结合对于加强网络防御复杂Wi-Fi攻击的有效性，为物联网环境提供了可扩展和可靠的解决方案。未来的工作包括实时部署和对抗性弹性测试，以进一步增强模型的适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1038/s41598-025-23754-w&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of IoT devices and their reliance on Wi-Fi networks have introduced significant security vulnerabilities, particularly the KRACK and Kr00k attacks, which exploit weaknesses in WPA2 encryption to intercept and manipulate sensitive data. Traditional IDS using classifiers face challenges such as model overfitting, incomplete feature extraction, and high false positive rates, limiting their effectiveness in real-world deployments. To address these challenges, this study proposes a robust multiclass machine learning based intrusion detection framework. The methodology integrates advanced feature selection techniques to identify critical attributes, mitigating redundancy and enhancing detection accuracy. Two distinct ML architectures are implemented: a baseline classifier pipeline and a stacked ensemble model combining noise injection, Principal Component Analysis (PCA), and meta learning to improve generalization and reduce false positives. Evaluated on the AWID3 data set, the proposed ensemble architecture achieves superior performance, with an accuracy of 98%, precision of 98%, recall of 98%, and a false positive rate of just 2%, outperforming existing state-of-the-art methods. This work demonstrates the efficacy of combining preprocessing strategies with ensemble learning to fortify network security against sophisticated Wi-Fi attacks, offering a scalable and reliable solution for IoT environments. Future directions include real-time deployment and adversarial resilience testing to further enhance the model's adaptability.</description>
      <author>example@mail.com (Md Minhazul Islam Munna, Md Mahbubur Rahman, Jaroslav Frnda, Muhammad Shahid Anwar, Alpamis Kutlimuratov)</author>
      <guid isPermaLink="false">2512.19037v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Photonic Spiking Graph Neural Network for Energy-Efficient Structured Data Processing</title>
      <link>http://arxiv.org/abs/2512.19182v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10Pages 10figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种光子脉冲图神经网络(PSGNN)架构，结合了图神经网络的结构建模能力、脉冲神经元的时态动态以及光子硬件的并行计算优势，用于处理图结构数据。&lt;h4&gt;背景&lt;/h4&gt;光子计算因其超高速、低能耗和固有并行性在信号处理和人工智能加速方面显示出巨大潜力。现有光子计算研究主要集中在卷积神经网络(CNNs)和全连接神经网络(FCNNs)上，这些网络适合图像分类和目标检测等任务，但在处理图结构数据时存在局限性。图神经网络(GNNs)专门用于建模复杂的关系结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种光子脉冲图神经网络(PSGNN)架构，将GNN的结构建模能力、脉冲神经元的时态动态和光子硬件的并行计算优势相结合，以有效处理图结构数据。&lt;h4&gt;方法&lt;/h4&gt;通过硬件-软件协同优化，实现了一种针对光子芯片的偏置项模拟方法，使用特征维度扩展，使网络能够有效训练。在KarateClub和PubMed数据集上进行实验，并构建了硅光子4×4马赫-曾德尔干涉仪(MZI)阵列进行硬件验证。&lt;h4&gt;主要发现&lt;/h4&gt;KarateClub数据集上达到100%的训练准确率(±2%)和97%的测试准确率(±1%)；PubMed数据集上达到92%的训练准确率(±2%)和90%的测试准确率(±1%)；硬件验证测试准确率达到93%；系统推理延迟为97皮秒；能效为280 GOPS/W；计算密度为52 GOPS/mm²。&lt;h4&gt;结论&lt;/h4&gt;PSGNN在结构数据处理应用中显示出巨大潜力，其高速度、高能效和高计算密度的特点使其成为处理图结构数据的理想选择。&lt;h4&gt;翻译&lt;/h4&gt;光子计算因其超高速、低能耗和固有并行性，在信号处理和人工智能加速方面显示出巨大潜力。现有的光子计算研究主要集中在卷积神经网络(CNNs)和全连接神经网络(FCNNs)上，这些网络适合图像分类和目标检测等任务，但在处理图结构数据时存在局限性。图神经网络(GNNs)专门用于建模复杂的关系结构。在这项工作中，我们提出了一种光子脉冲图神经网络(PSGNN)架构，它结合了GNN的结构建模能力、脉冲神经元的时态动态以及光子硬件的并行计算优势。通过硬件-软件协同优化，实现了一种针对光子芯片的偏置项模拟方法，使用特征维度扩展，使网络能够有效训练。在KarateClub和PubMed数据集上的实验分别实现了100%(±2%)和92%(±2%)的训练准确率，以及97%(±1%)和90%(±1%)的测试准确率。进一步构建了硅光子4×4马赫-曾德尔干涉仪(MZI)阵列进行硬件验证，实现了93%的测试准确率。该系统展示了97皮秒的推理延迟，280 GOPS/W的能效和52 GOPS/mm²的计算密度。这些结果突显了PSGNN在结构数据处理应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photonic computing shows great potential for signal processing and artificial intelligence (AI) acceleration due to its ultra-high speed, low energy consumption, and inherent parallelism. Existing photonic computing research has mainly focused on convolutional neural networks (CNNs) and fully connected neural networks (FCNNs), which are well suited for tasks such as image classification and object detection but face limitations in handling graph-structured data. Graph neural networks (GNNs) are specifically designed to model complex relational structures. In this work, we propose a photonic spiking graph neural network (PSGNN) architecture that integrates the structural modeling capability of GNNs, the temporal dynamics of spiking neurons, and the parallel computing advantages of photonic hardware. Through hardware-software co-optimization, a bias-term simulation method tailored for photonic chips is implemented using feature-dimension expansion, enabling effective network training. Experiments on the KarateClub and PubMed datasets achieve training accuracies of 100 percent (92 +/- 2 percent) and test accuracies of 97 percent (90 +/- 1 percent). A silicon photonics 4 x 4 Mach-Zehnder interferometer (MZI) array is further constructed for hardware validation, achieving a test accuracy of 93 percent. The system demonstrates an inference latency of 97 ps, with an energy efficiency of 280 GOPS/W and a computational density of 52 GOPS/mm^2. These results highlight the potential of PSGNN for structured-data processing applications.</description>
      <author>example@mail.com (Wanting Yu, Shuiying Xiang, Xingxing Guo, Shangxuan Shi, Haowen Zhao, Xintao Zeng, Yahui Zhang, Hongbo Jiang, Yue Hao)</author>
      <guid isPermaLink="false">2512.19182v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Variational Autoregressive Networks Applied to $φ^4$ Field Theory Systems</title>
      <link>http://arxiv.org/abs/2512.19575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 page,13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究结合强化学习与变分自回归网络，应用于离散伊辛模型和连续φ⁴标量场理论的数据训练和采样。&lt;h4&gt;背景&lt;/h4&gt;在物理学中，伊辛模型和φ⁴标量场理论的采样是重要但具有挑战性的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的数据训练和采样方法，提高采样效率并减少偏差。&lt;h4&gt;方法&lt;/h4&gt;结合强化学习与变分自回归网络(VANs)，通过KL散度量化目标分布复杂性，研究迁移学习，引入单点和块Metropolis-Hastings更新。&lt;h4&gt;主要发现&lt;/h4&gt;KL散度较小的配置需要较少训练步骤；迁移学习可减少训练时间；MH校正减少残余偏差同时保持高采样效率；结果与标准蒙特卡洛基准一致；未观察到明显的临界减速现象。&lt;h4&gt;结论&lt;/h4&gt;结合强化学习与VANs的方法在物理学模型采样中表现出色，具有良好的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们将强化学习与变分自回归网络（VANs）相结合，对离散伊辛模型和连续φ⁴标量场理论进行无数据训练和采样。我们通过磁化分布与参考高斯分布之间的KL散度来量化目标分布的复杂性，并观察到KL散度较小的配置通常需要更少的训练步骤。受此观察启发，我们研究了迁移学习，并显示与从高斯场训练相比，在单个κ值预训练的模型上进行微调可以减少训练时间。此外，受单点和集群蒙特卡洛更新的启发，我们在VAN提案基础上引入了单点和块Metropolis-Hastings（MH）更新。这些MH校正系统地减少了我们在研究参数范围内纯VAN采样的残余偏差，同时保持了高的采样效率（以有效样本量ESS衡量）。对于伊辛模型和φ⁴理论，我们的结果与标准蒙特卡洛基准在误差范围内一致，并且在探索的参数范围内未观察到明显的临界减速现象。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We combine reinforcement learning with variational autoregressive networks (VANs) to perform data-free training and sampling for the discrete Ising model and the continuous $φ^4$ scalar field theory. We quantify the complexity of the target distribution via the KL divergence between the magnetization distribution and a reference Gaussian distribution, and observe that configurations with smaller KL divergence typically require fewer training steps. Motivated by this observation, we investigate transfer learning and show that fine-tuning models pretrained at a single value of $κ$ can reduce training time compared with training from a Gaussian field. In addition, inspired by single-site and cluster Monte Carlo updates, we introduce single-site and block Metropolis--Hastings (MH) updates on top of VAN proposals. These MH corrections systematically reduce the residual bias of pure VAN sampling in the parameter range we study, while maintaining high sampling efficiency in terms of the effective sample size (ESS). For both the Ising model and the $φ^4$ theory, our results agree with standard Monte Carlo benchmarks within errors, and no clear critical slowing down is observed in the explored parameter ranges.</description>
      <author>example@mail.com (Moxian Qian, Shiyang Chen)</author>
      <guid isPermaLink="false">2512.19575v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</title>
      <link>http://arxiv.org/abs/2512.20557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文引入了DSR Suite，这是一个全面解决视觉语言模型(VLM)在动态空间推理(DSR)方面不足的方案，包括自动化数据生成管道、基准测试集和轻量级几何选择模块(GSM)。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在一般理解方面表现出色，但在动态空间推理(对3D空间中物体几何形状和关系随时间演变的推理)方面仍然薄弱，主要原因是缺乏可扩展的4D感知训练资源。&lt;h4&gt;目的&lt;/h4&gt;弥合数据集、基准和模型方面的差距，提高视觉语言模型的动态空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出自动化管道从野外视频中生成多选题-答案对，利用现代视觉基础模型提取几何和运动信息；构建DSR-Train用于学习和DSR-Bench用于评估；引入几何选择模块(GSM)将几何先验无缝整合到VLM中，避免无关知识干扰。&lt;h4&gt;主要发现&lt;/h4&gt;将DSR-Train和GSM整合到Qwen2.5-VL-7B中显著增强了其动态空间推理能力，同时在一般视频理解基准测试上保持了准确性。&lt;h4&gt;结论&lt;/h4&gt;DSR Suite有效解决了视觉语言模型在动态空间推理方面的不足，通过专门的数据和模型组件提高了模型在3D空间推理方面的表现。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLM)在一般理解方面表现出色，但在动态空间推理(DSR)方面仍然薄弱，即对3D空间中物体几何形状和关系随时间演变的推理，这主要由于缺乏可扩展的4D感知训练资源。为了在数据集、基准和模型方面弥合这一差距，我们引入了DSR Suite。首先，我们提出了一种自动化管道，从野外视频中生成用于DSR的多选题-答案对。通过利用现代视觉基础模型，该管道提取了丰富的几何和运动信息，包括相机姿态、局部点云、物体掩码、方向和3D轨迹。这些几何线索能够构建用于学习的DSR-Train和用于评估的经过人工优化的DSR-Bench。与之前的工作相比，我们的数据强调(i)野外视频源，(ii)物体和场景级别的3D要求，(iii)视角变换，(iv)多物体交互，以及(v)细粒度、程序化答案。除了数据外，我们还提出了一个轻量级的几何选择模块(GSM)，将几何先验无缝整合到VLM中，该模块压缩问题语义并从预训练的4D重建先验中提取问题相关知识，形成紧凑的几何令牌集合。这种有针对性的提取避免了模型被无关知识淹没。实验表明，将DSR-Train和GSM整合到Qwen2.5-VL-7B中显著增强了其动态空间推理能力，同时在一般视频理解基准测试上保持了准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉语言模型（VLM）在动态空间推理（DSR）方面的能力不足问题，即模型难以理解3D空间中物体几何和关系随时间如何演变。这个问题很重要，因为动态空间推理能力是机器人、自动驾驶、AR/VR和具身智能等交互式AI系统的核心需求，在这些应用中环境是动态的，空间关系不断变化，缺乏这种能力会限制VLM在实际应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有工作的局限性：大多数研究局限于静态场景或短时间跨度的运动；现有的动态场景研究存在场景多样性有限、推理类型有限和训练数据缺乏的问题；当前方法注入几何知识的方式会导致对特定任务线索的过拟合，损害通用性能。基于这些分析，作者设计了DSR Suite框架，借鉴了现有的视觉基础模型（如Grounded SAM2、Orient Anything、π3等）来提取几何线索，同时创新性地提出了几何选择模块（GSM）来解决几何知识注入的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个全面的动态空间推理框架，通过自动化的数据生成管道从野外视频中生成高质量的问题-答案对，并提出轻量级的几何选择模块（GSM）有选择地将相关几何知识注入VLM。整体实现流程包括：1）视频筛选，从Koala-36M数据集中筛选出有意义物体运动的视频；2）几何线索提取，使用视觉基础模型提取相机姿态、局部点云等；3）数据生成，构建多种选择问题和精细答案；4）模型训练，使用DSR-Train数据集训练VLM并集成GSM；5）评估，使用DSR-Bench基准评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）DSR Suite框架，首个专门针对动态空间推理的综合框架；2）自动化数据生成管道，能从野外视频自动生成高质量问题-答案对；3）几何选择模块（GSM），有选择地将几何知识注入VLM；4）DSR-Train和DSR-Bench数据集，强调野外视频来源、多对象交互和精细答案。相比之前的工作，不同在于：使用野外而非受约束场景；强调多物体和多视角推理而非单物体运动；提供随时间变化的精细答案而非粗略聚合答案；通过GSM有选择注入几何知识而非直接注入大量几何特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了DSR Suite框架，通过自动化数据生成管道和几何选择模块（GSM），显著提升了视觉语言模型在动态空间推理方面的能力，同时保持了其在通用视频理解任务上的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.</description>
      <author>example@mail.com (Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi)</author>
      <guid isPermaLink="false">2512.20557v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition</title>
      <link>http://arxiv.org/abs/2512.20407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at the 2025 IEEE 22nd India Council International Conference (INDICON). 6 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为AUDRON的基于音频的无人机识别网络，是一种混合深度学习框架，用于检测无人机声音。该系统结合多种声学特征表示技术，实现了高准确率的无人机检测，在二元和多类分类中分别达到98.51%和97.11%的准确率。&lt;h4&gt;背景&lt;/h4&gt;无人机在物流、农业、监视和国防等多个领域应用广泛，但其滥用带来安全和安全问题。声学传感作为低成本和非侵入性的替代方案，能够通过识别无人机螺旋桨产生的独特声音模式来检测无人机。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效的无人机声音检测系统，通过结合多种声学特征表示和深度学习技术，实现高准确率的无人机检测，特别是在视觉或雷达传感可能受限的环境中。&lt;h4&gt;方法&lt;/h4&gt;AUDRON框架结合了Mel频率倒谱系数、短时傅里叶变换频谱图（使用卷积神经网络处理）、用于时间建模的循环层和基于自编码器的表示。系统通过特征级融合在分类前整合互补信息，提高检测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;AUDRON能够有效区分无人机声学特征和背景噪声，在保持跨不同条件泛化能力的同时实现高准确率。实验结果表明该系统在二元分类中达到98.51%的准确率，在多类分类中达到97.11%的准确率。&lt;h4&gt;结论&lt;/h4&gt;结合多种特征表示和深度学习对于可靠的声音无人机检测具有显著优势。AUDRON框架在视觉或雷达传感可能受限的安全和监视应用中具有实际部署潜力。&lt;h4&gt;翻译&lt;/h4&gt;无人机（UAVs），通常被称为无人机，正越来越多地用于物流、农业、监视和国防等多个领域。虽然这些系统提供了许多好处，但其滥用引发了安全和安全问题，使得有效的检测机制变得至关重要。声学传感提供了一种低成本和非侵入性的替代方案，可以基于视觉或雷达检测，因为无人机螺旋桨会产生独特的声音模式。本研究引入了AUDRON（基于音频的无人机识别网络），这是一种用于无人机声音检测的混合深度学习框架，采用Mel频率倒谱系数、短时傅里叶变换频谱图（使用卷积神经网络处理）、用于时间建模的循环层和基于自编码器的表示的组合。特征级融合在分类前整合互补信息。实验评估表明，AUDRON能够有效区分无人机声学特征和背景噪声，在保持跨不同条件泛化能力的同时实现高准确率。AUDRON在二元分类和多类分类中分别达到98.51%和97.11%的准确率。结果强调了结合多种特征表示和深度学习进行可靠声音无人机检测的优势，表明该框架在视觉或雷达传感可能受限的安全和监视应用中具有部署潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited.</description>
      <author>example@mail.com (Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra)</author>
      <guid isPermaLink="false">2512.20407v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion</title>
      <link>http://arxiv.org/abs/2512.20128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at WACV 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;milliMamba是一种基于毫米波雷达的2D人体姿态估计框架，通过联合建模时空依赖关系解决了雷达信号因镜面反射而稀疏导致的特征提取难题。&lt;h4&gt;背景&lt;/h4&gt;毫米波雷达作为RGB传感器的替代方案，具有隐私保护和光照不变性的优势，但其信号常因镜面反射而变得稀疏，使特征提取面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从稀疏雷达信号中提取鲁棒特征的2D人体姿态估计框架，以解决镜面反射导致的信息缺失问题。&lt;h4&gt;方法&lt;/h4&gt;采用Cross-View Fusion Mamba编码器高效提取时空特征，使用Spatio-Temporal-Cross Attention解码器预测多帧关节坐标，构建时空建模管道利用相邻帧和关节的上下文线索推断缺失关节，并在训练中结合速度损失强化运动平滑性。&lt;h4&gt;主要发现&lt;/h4&gt;在TransHuPR和HuPR数据集上，该方法分别实现了11.0 AP和14.6 AP的性能提升，显著超过基线方法，同时保持了合理的计算复杂度。&lt;h4&gt;结论&lt;/h4&gt;milliMamba通过时空建模有效克服了雷达信号稀疏性带来的挑战，能够从雷达数据中准确估计人体姿态，代码已公开于GitHub。&lt;h4&gt;翻译&lt;/h4&gt;毫米波雷达为人体姿态估计(HPE)任务提供了一种隐私保护且不受光照影响的RGB传感器替代方案。然而，由于镜面反射，雷达信号通常稀疏，使得从雷达信号中提取鲁棒特征极具挑战性。为此，我们提出了milliMamba，一种基于雷达的2D人体姿态估计框架，联合建模特征提取和解码阶段的时空依赖关系。具体而言，针对雷达输入的高维特性，我们采用跨视图融合Mamba编码器，从更长序列中高效提取时空特征，具有线性复杂度。然后，时空交叉注意力解码器预测多帧关节坐标。这种时空建模管道使模型能够利用相邻帧和关节的上下文线索来推断由镜面反射引起的缺失关节。为了强化运动平滑性，我们在训练过程中将速度损失与标准关键点损失结合。在TransHuPR和HuPR数据集上的实验表明，我们的方法实现了显著的性能提升，分别超过基线11.0 AP和14.6 AP，同时保持合理的复杂度。代码：https://github.com/NYCU-MAPL/milliMamba&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba</description>
      <author>example@mail.com (Niraj Prakash Kini, Shiau-Rung Tsai, Guan-Hsun Lin, Wen-Hsiao Peng, Ching-Wen Ma, Jenq-Neng Hwang)</author>
      <guid isPermaLink="false">2512.20128v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Pretrained Battery Transformer (PBT): A battery life prediction foundation model</title>
      <link>http://arxiv.org/abs/2512.16334v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 figures in the main content&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Pretrained Battery Transformer (PBT)，这是首个用于电池寿命预测的基础模型，通过迁移学习解决了电池领域数据稀缺性和异质性问题，在各种数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;电池循环寿命的早期预测对加速电池研究、制造和部署至关重要。尽管机器学习方法已显示出有希望的结果，但进展受到数据稀缺性和不同老化条件导致的数据异质性的阻碍。&lt;h4&gt;目的&lt;/h4&gt;开发一个基础模型来克服电池寿命预测中的数据稀缺性和异质性问题，实现通用电池寿命预测系统。&lt;h4&gt;方法&lt;/h4&gt;提出了Pretrained Battery Transformer (PBT)，通过领域知识编码的专家混合层开发。在最大的公共电池寿命数据库上验证，从13个锂离子电池数据集中学习可迁移的表示。&lt;h4&gt;主要发现&lt;/h4&gt;PBT比现有模型平均性能提高19.8%。通过迁移学习，PBT在各种操作条件、形成协议和化学成分的15个不同数据集上实现了最先进的表现。&lt;h4&gt;结论&lt;/h4&gt;这项工作为电池寿命预测建立了基础模型途径，为通用电池寿命预测系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;电池循环寿命的早期预测对于加速电池研究、制造和部署至关重要。尽管机器学习方法已显示出令人鼓舞的结果，但进展受到数据稀缺性和不同老化条件导致的数据异质性的阻碍。在其他领域，通过迁移学习在多样化数据集上训练的基础模型已实现了广泛的泛化能力，但尚未报道有用于电池循环寿命预测的基础模型。在此，我们提出了Pretrained Battery Transformer (PBT)，这是首个用于电池寿命预测的基础模型，通过领域知识编码的专家混合层开发。在最大的公共电池寿命数据库上验证，PBT从13个锂离子电池数据集中学习可迁移的表示，比现有模型平均提高19.8%的性能。通过迁移学习，PBT在各种操作条件、形成协议和化学成分的15个不同数据集上实现了最先进的性能。这项工作为电池寿命预测建立了基础模型途径，为通用电池寿命预测系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.</description>
      <author>example@mail.com (Ruifeng Tan, Weixiang Hong, Jia Li, Jiaqiang Huang, Tong-Yi Zhang)</author>
      <guid isPermaLink="false">2512.16334v3</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</title>
      <link>http://arxiv.org/abs/2512.20605v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为'内部RL'的新方法，通过在自回归模型的内部表示中进行动作探索和强化学习，解决了传统逐个标记采样导致的低效学习问题，特别是在奖励稀疏的情况下。&lt;h4&gt;背景&lt;/h4&gt;大规模自回归模型通过下一词预测预训练并使用强化学习微调，已在多个问题领域取得成功。然而，在强化学习过程中逐个生成标记进行探索会导致学习效率低下，尤其是在奖励稀疏的情况下。&lt;h4&gt;目的&lt;/h4&gt;克服自回归模型在强化学习中逐个标记采样导致的学习效率低下问题，特别是在奖励稀疏的情况下。&lt;h4&gt;方法&lt;/h4&gt;引入一个高阶、非因果序列模型，其输出控制基础自回归模型的残差流激活，从而在内部表示中发现时间抽象动作。这种方法被称为'内部RL'，通过直接对内部控制器进行强化学习。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在具有层次结构的网格世界和MuJoCo任务中，高阶模型学会将长的激活序列压缩到内部控制器；2) 每个控制器执行行为上有意义的长时间序列动作，并带有学习的终止条件；3) 组合多个控制器可实现新任务上的高效探索；4) 内部RL能在标准RL微调失败的情况下实现从稀疏奖励中学习。&lt;h4&gt;结论&lt;/h4&gt;自回归模型中的潜在动作生成和强化具有显著优势，内部RL为在基础模型中实现层次化RL提供了有前途的途径。&lt;h4&gt;翻译&lt;/h4&gt;大规模基于下一词预测预训练并通过强化学习(RL)微调的自回归模型已在多个问题领域取得了前所未有的成功。在强化学习过程中，这些模型通过一次生成一个新输出来进行探索。然而，逐个标记采样动作可能导致学习效率低下，特别是在奖励稀疏的情况下。我们展示了一种通过在自回归模型的内部表示中行动和探索来克服此问题的方法。具体来说，为了发现时间抽象动作，我们引入了一个高阶、非因果序列模型，其输出控制基础自回归模型的残差流激活。在具有层次结构的网格世界和基于MuJoCo的任务中，我们发现高阶模型学会将长的激活序列块压缩到内部控制器上。每个控制器执行一系列行为上有意义的动作，这些动作在长时间尺度上展开，并带有学习的终止条件，使得随时间组合多个控制器能够在新任务上实现高效探索。我们证明，直接内部控制器强化（我们称之为'内部RL'）能够在标准RL微调失败的情况下实现从稀疏奖励中学习。我们的结果证明了自回归模型中潜在动作生成和强化的优势，表明内部RL是实现在基础模型中进行层次化RL的有前途的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.</description>
      <author>example@mail.com (Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento)</author>
      <guid isPermaLink="false">2512.20605v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Iterative learning scheme for crystal structure prediction with anharmonic lattice dynamics</title>
      <link>http://arxiv.org/abs/2512.20424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合进化算法、原子基础模型和随机自洽谐波近似(SSCHA)的迭代学习框架，用于考虑非谐晶格动力学的晶体结构预测(CSP)，成功应用于高非谐性H₃S系统。&lt;h4&gt;背景&lt;/h4&gt;基于第一性原理的CSP方法在发现新材料方面很重要，但在接近位移相转变的材料中，离子对自由能的贡献和晶格非谐性限制了CSP的准确性。SSCHA方法准确但计算成本高，机器学习势能方法需要大量训练数据且泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效处理非谐晶格动力学的CSP方法，克服传统方法的局限性，特别是在高非谐性材料系统中的应用。&lt;h4&gt;方法&lt;/h4&gt;提出结合进化算法、原子基础模型和SSCHA的迭代学习框架。基础模型减少随机结构弛豫所需的训练数据，应用于H₃S系统在50至200 GPa压力范围内的相稳定性和振动性质预测。&lt;h4&gt;主要发现&lt;/h4&gt;该框架与密度泛函理论基准结果一致，准确预测H₃S系统的相稳定性和振动性质；SSCHA中的统计平均降低了自由能评估误差，避免了机器学习势能需要极高精度的要求。&lt;h4&gt;结论&lt;/h4&gt;该方法弥合了数据效率和预测能力之间的差距，为具有非谐晶格动力学的CSP建立了实用路径。&lt;h4&gt;翻译&lt;/h4&gt;基于第一性原理的晶体结构预测方法已成为发现新材料的重要工具。然而，在接近位移相转变的固体材料中，离子对自由能的贡献和晶格非谐性变得至关重要，限制了CSP技术确定竞争相热力学稳定性的能力。虽然变分方法如随机自洽谐波近似能够准确处理第一性原理水平的非谐晶格动力学，但其高计算成本使其不适用于CSP。机器学习原子间势能相比纯第一性原理方法能够加速能量景观采样，但其对大量训练数据的依赖和有限的泛化能力限制了实际应用。在此，我们提出了一种结合进化算法、原子基础模型和SSCHA的迭代学习框架，使CSP能够考虑非谐晶格动力学。基础模型能够对随机结构进行稳健弛豫，大幅减少所需的训练数据。应用于高非谐性H₃S系统，我们的框架与基于密度泛函理论的基准结果达成良好一致，准确预测了50至200 GPa压力范围内的相稳定性和振动性质。重要的是，我们发现SSCHA中的统计平均降低了自由能评估的误差，避免了机器学习势能需要极高精度的要求。这种方法弥合了数据效率和预测能力之间的差距，为具有非谐晶格动力学的CSP建立了一条实用路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; First-principles based crystal structure prediction (CSP) methods have revealed an essential tool for the discovery of new materials. However, in solids close to displacive phase transitions, which are common in ferroelectrics, thermoelectrics, charge-density wave systems, or superconducting hydrides, the ionic contribution to the free energy and lattice anharmonicity become essential, limiting the capacity of CSP techniques to determine the thermodynamical stability of competing phases. While variational methods like the stochastic self-consistent harmonic approximation (SSCHA) accurately account for anharmonic lattice dynamics \emph{ab initio}, their high computational cost makes them impractical for CSP. Machine-learning interatomic potentials offer accelerated sampling of the energy landscape compared to purely first-principles approaches, but their reliance on extensive training data and limited generalization restricts practical applications. Here, we propose an iterative learning framework combining evolutionary algorithms, atomic foundation models, and SSCHA to enable CSP with anharmonic lattice dynamics. Foundation models enable robust relaxations of random structures, drastically reducing required training data. Applied to the highly anharmonic H$_3$S system, our framework achieves good agreement with the benchmarks based on density functional theory, accurately predicting phase stability and vibrational properties from 50 to 200 GPa. Importantly, we find that the statistical averaging in the SSCHA reduces the error in the free energy evaluation, avoiding the need for extremely high accuracy of machine-learning potentials. This approach bridges the gap between data efficiency and predictive power, establishing a practical pathway for CSP with anharmonic lattice dynamics.</description>
      <author>example@mail.com (Hao Gao, Yue-Wen Fang, Ion Errea)</author>
      <guid isPermaLink="false">2512.20424v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>SirenPose: Dynamic Scene Reconstruction via Geometric Supervision</title>
      <link>http://arxiv.org/abs/2512.20531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SirenPose，一种结合正弦表示网络和基于关键点的几何监督的方法，用于从单目视频中准确且时间一致地重建动态3D场景。&lt;h4&gt;背景&lt;/h4&gt;现有方法在处理快速运动、多物体交互、遮挡和快速场景变化等具有挑战性的场景时，往往难以保持运动保真度和时空一致性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确重建动态3D场景并保持时空一致性的方法，特别是在具有挑战性的场景中。&lt;h4&gt;方法&lt;/h4&gt;SirenPose采用几何感知的损失函数形式化，结合正弦表示网络的周期性激活特性与基于关键点的几何监督，纳入受物理学启发的约束来保持关键点预测的一致性，利用高频信号建模捕获细粒度几何细节，扩展UniKPT数据集到60万个标注实例，并集成图神经网络建模关键点关系和结构相关性。&lt;h4&gt;主要发现&lt;/h4&gt;在Sintel、Bonn和DAVIS等基准测试上，SirenPose持续优于最先进的方法。在DAVIS上，与MoSCA相比，Siren实现了FVD降低17.8%，FID降低28.7%，LPIPS提高6.0%，并改善了时间一致性、几何准确性、用户评分和运动平滑度。在姿态估计方面，SirenPose优于Monst3R，具有更小的绝对轨迹误差以及更小的平移和旋转相对姿态误差。&lt;h4&gt;结论&lt;/h4&gt;SirenPose在处理快速运动、复杂动力学和物理合理重建方面表现出色，是一种有效的动态3D场景重建方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了SirenPose，一种几何感知的损失函数形式化，它将正弦表示网络的周期性激活特性与基于关键点的几何监督相结合，能够从单目视频中准确且时间一致地重建动态3D场景。现有方法在处理涉及快速运动、多物体交互、遮挡和快速场景变化的具有挑战性的场景时，往往难以保持运动保真度和时空一致性。SirenPose纳入了受物理学启发的约束，强制在空间和时间维度上保持关键点预测的一致性，同时利用高频信号建模来捕获细粒度的几何细节。我们进一步将UniKPT数据集扩展到60万个标注实例，并集成图神经网络来建模关键点关系和结构相关性。在包括Sintel、Bonn和DAVIS在内的基准测试上的大量实验表明，SirenPose持续优于最先进的方法。在DAVIS上，与MoSCA相比，SirenPose实现了FVD降低17.8%，FID降低28.7%，LPIPS提高6.0%。它还改善了时间一致性、几何准确性、用户评分和运动平滑度。在姿态估计方面，SirenPose优于Monst3R，具有更小的绝对轨迹误差以及更小的平移和旋转相对姿态误差，突显了其在处理快速运动、复杂动力学和物理合理重建方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态3D场景重建中的运动建模精度不足和时空一致性问题，特别是在快速移动物体、多目标交互、遮挡和快速场景变化等挑战性场景中。这个问题在现实中非常重要，因为高质量的动态场景重建对虚拟现实、机器人和创意产业等领域有深远影响，而现有方法在处理复杂动态场景时经常产生轨迹抖动、时间不连续性和物理上不合理的变形等伪影。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先将动态4D场景分解为低频和高频分量，分别处理不同的运动特征。他们借鉴了现有工作：利用CAPE等方法处理低频信息，借鉴SIREN网络的高频建模能力，扩展了UniKPT数据集到600K标注实例，并使用图神经网络捕获关键点关系。基于这些，他们设计了SirenPose损失函数，结合周期性激活特性和几何先验，并通过双流架构实现低频和高频信息的融合与处理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将动态4D场景重建分解为低频和高频分量的建模，利用SIREN网络的周期性激活特性捕获高频细节，结合关键点结构的几何先验确保运动连贯性。整体流程是：输入视频被分解为低频（通过CAPE捕获全局结构）和高频（通过SIREN建模快速运动）两个并行流；特征融合后预测动态关键点；使用位置精度损失和几何一致性损失进行监督；最后将SirenPose损失作为正则化项整合到端到端训练框架中，确保重建既视觉准确又物理合理且时间平滑。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 新的SirenPose损失函数，结合SIREN的周期性激活和关键点几何先验；2) 大规模关键点监督训练框架，扩展到600K标注实例并使用图神经网络；3) 频率分解方法，分别处理低频和高频信息。相比之前的工作，SirenPose能够同时捕获高频率运动细节和保持结构完整性，有效解决了现有方法在快速运动或遮挡时产生的伪影问题，在多个基准测试上实现了更优的时空一致性和物理合理性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SirenPose通过结合正弦表示网络的高频建模能力和关键点结构的几何先验，显著提高了动态3D场景重建的精度和时空一致性，有效解决了现有方法在处理复杂动态场景时面临的运动建模不精确和时空不一致问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SirenPose, a geometry-aware loss formulation that integrates the periodic activation properties of sinusoidal representation networks with keypoint-based geometric supervision, enabling accurate and temporally consistent reconstruction of dynamic 3D scenes from monocular videos. Existing approaches often struggle with motion fidelity and spatiotemporal coherence in challenging settings involving fast motion, multi-object interaction, occlusion, and rapid scene changes. SirenPose incorporates physics inspired constraints to enforce coherent keypoint predictions across both spatial and temporal dimensions, while leveraging high frequency signal modeling to capture fine grained geometric details. We further expand the UniKPT dataset to 600,000 annotated instances and integrate graph neural networks to model keypoint relationships and structural correlations. Extensive experiments on benchmarks including Sintel, Bonn, and DAVIS demonstrate that SirenPose consistently outperforms state-of-the-art methods. On DAVIS, SirenPose achieves a 17.8 percent reduction in FVD, a 28.7 percent reduction in FID, and a 6.0 percent improvement in LPIPS compared to MoSCA. It also improves temporal consistency, geometric accuracy, user score, and motion smoothness. In pose estimation, SirenPose outperforms Monst3R with lower absolute trajectory error as well as reduced translational and rotational relative pose error, highlighting its effectiveness in handling rapid motion, complex dynamics, and physically plausible reconstruction.</description>
      <author>example@mail.com (Kaitong Cai, Jensen Zhang, Jing Yang, Keze Wang)</author>
      <guid isPermaLink="false">2512.20531v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection</title>
      <link>http://arxiv.org/abs/2512.20086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 Workshop in AI for Science: The Reach and Limits of AI for Scientific Discovery&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时空图神经网络在固定节点结构领域取得成功，但在缺乏固定锚点的非网格环境(如海事交通)中面临挑战。作者提出新的海事异常检测基准数据集，并计划使用两种基于大型语言模型的代理来构建更丰富的交互上下文和生成语义上有意义的异常。&lt;h4&gt;背景&lt;/h4&gt;时空图神经网络在道路交通和公共交通等结构化领域表现优异，但许多真实世界系统如海事交通缺乏固定锚点，使时空图构建成为基本挑战。非网格环境中的异常检测因缺乏规范参考点、轨迹稀疏不规则以及异常可能以多种粒度表现而尤为困难。&lt;h4&gt;目的&lt;/h4&gt;引入海事领域异常检测基准数据集，将开放海事交通分析数据集扩展为专门用于基于图的异常检测基准，促进非网格时空系统中异常检测的可重复性和方法学进步。&lt;h4&gt;方法&lt;/h4&gt;扩展Open Maritime Traffic Analysis Dataset (OMTAD)为基准数据集，支持三个不同粒度的系统性评估：节点级、边级和图级异常。计划使用两种基于大型语言模型的代理：轨道合成器和异常注入器，以构建更丰富的交互上下文和生成语义上有意义的异常。&lt;h4&gt;主要发现&lt;/h4&gt;摘要主要介绍研究计划而非具体发现，未明确提及研究结果。&lt;h4&gt;结论&lt;/h4&gt;期望该基准能够促进非网格时空系统中异常检测的可重复性，推动该领域方法学的进步。&lt;h4&gt;翻译&lt;/h4&gt;时空图神经网络(ST-GNNs)在道路交通和公共交通等结构化领域取得了显著成功，其中空间实体可以自然地表示为固定节点。相比之下，包括海事交通在内的许多真实世界系统缺乏此类固定锚点，使得时空图的构建成为一个基本挑战。由于缺乏规范参考点、轨迹稀疏且不规则，以及异常可能以多种粒度表现，这些非网格环境中的异常检测尤其困难。在这项工作中，我们引入了一个用于海事领域异常检测的新颖基准数据集，将开放海事交通分析数据集(OMTAD)扩展为专门用于基于图的异常检测的基准。我们的数据集支持三种不同粒度的系统评估：节点级、边级和图级异常。我们计划采用两种专门的基于大型语言模型的代理：轨迹合成器和异常注入器，以构建更丰富的交互上下文并生成语义上有意义的异常。我们期望这个基准能够促进可重复性，并推动非网格时空系统异常检测的方法学进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: \emph{Trajectory Synthesizer} and \emph{Anomaly Injector} to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems.</description>
      <author>example@mail.com (Jeehong Kim, Youngseok Hwang, Minchan Kim, Sungho Bae, Hyunwoo Park)</author>
      <guid isPermaLink="false">2512.20086v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</title>
      <link>http://arxiv.org/abs/2512.20004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基于图的深度学习方法在Android恶意软件检测中的应用及其对抗攻击的防御策略。&lt;h4&gt;背景&lt;/h4&gt;物联网(IoT)广泛采用Android应用，使得检测恶意Android应用变得至关重要。近年来，基于图的深度学习方法被用于从应用中提取关系并生成图嵌入。&lt;h4&gt;目的&lt;/h4&gt;展示基于图的分类方法的有效性，并提出一种针对基于图的GNN Android恶意软件分类器的攻击算法。&lt;h4&gt;方法&lt;/h4&gt;使用基于图神经网络(GNN)的分类器生成API图嵌入，将图嵌入与权限和意图特征结合训练多种模型。提出基于生成对抗网络(GAN)的攻击算法VGAE-MalGAN，其中生成器产生对抗性恶意软件API图，替代检测器模仿目标检测器。&lt;h4&gt;主要发现&lt;/h4&gt;提出的分类方法在CICMaldroid数据集上达到98.33%的准确率，在Drebin数据集上达到98.68%的准确率。基于图的深度学习模型存在脆弱性，攻击者可添加虚假关系逃避检测。VGAE-MalGAN能显著降低GNN分类器的检测率，但使用对抗样本重新训练可提高模型鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;基于图的深度学习方法在Android恶意软件检测中表现优异，但易受对抗攻击。通过对抗样本重新训练可以提高模型鲁棒性，缓解对抗攻击。&lt;h4&gt;翻译&lt;/h4&gt;由于物联网(IoT)广泛采用Android应用，检测恶意Android应用至关重要。近年来，基于图的Android深度学习研究提出了许多从应用中提取关系作为图来生成图嵌入的方法。首先，我们展示了基于图的分类方法的有效性，使用基于图神经网络(GNN)的分类器生成API图嵌入。将图嵌入与权限和意图特征结合，训练多种机器学习和深度学习模型进行Android恶意软件检测。所提出的分类方法在CICMaldroid数据集上达到98.33%的准确率，在Drebin数据集上达到98.68%的准确率。然而，基于图的深度学习模型存在脆弱性，因为攻击者可以添加虚假关系来逃避分类器的检测。其次，我们提出了一种基于生成对抗网络(GAN)的攻击算法，名为VGAE-MalGAN，针对基于图的GNN Android恶意软件分类器。VGAE-MalGAN生成器产生对抗性恶意软件API图，而VGAE-MalGAN替代检测器尝试模仿目标检测器。实验结果表明，VGAE-MalGAN可以显著降低基于GNN的恶意软件分类器的检测率。尽管模型最初未能检测对抗性恶意软件，但使用生成的对抗样本重新训练可以提高鲁棒性，有助于缓解对抗攻击。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/JIOT.2022.3188583&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since the Internet of Things (IoT) is widely adopted using Android applications, detecting malicious Android apps is essential. In recent years, Android graph-based deep learning research has proposed many approaches to extract relationships from applications as graphs to generate graph embeddings. First, we demonstrate the effectiveness of graph-based classification using a Graph Neural Network (GNN)-based classifier to generate API graph embeddings. The graph embeddings are combined with Permission and Intent features to train multiple machine learning and deep learning models for Android malware detection. The proposed classification approach achieves an accuracy of 98.33 percent on the CICMaldroid dataset and 98.68 percent on the Drebin dataset. However, graph-based deep learning models are vulnerable, as attackers can add fake relationships to evade detection by the classifier. Second, we propose a Generative Adversarial Network (GAN)-based attack algorithm named VGAE-MalGAN targeting graph-based GNN Android malware classifiers. The VGAE-MalGAN generator produces adversarial malware API graphs, while the VGAE-MalGAN substitute detector attempts to mimic the target detector. Experimental results show that VGAE-MalGAN can significantly reduce the detection rate of GNN-based malware classifiers. Although the model initially fails to detect adversarial malware, retraining with generated adversarial samples improves robustness and helps mitigate adversarial attacks.</description>
      <author>example@mail.com (Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang)</author>
      <guid isPermaLink="false">2512.20004v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision</title>
      <link>http://arxiv.org/abs/2512.20308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为SpidR的自监督语音表示模型，能够从原始语音中高效学习具有高度可及语音信息的表示，特别适用于无文本的口语语言建模。该模型通过掩码预测目标结合自蒸馏和在线聚类进行训练，在下游任务中表现优异，同时显著减少了预训练时间。&lt;h4&gt;背景&lt;/h4&gt;语言建模和语音表征学习的并行进展使得直接从语音学习语言而不需要文本中间媒介成为可能。这需要直接从语音中提取语义表征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接从语音学习语义表征的自监督模型，用于无文本的口语语言建模，并评估语音单元质量与语言建模性能之间的相关性。&lt;h4&gt;方法&lt;/h4&gt;提出SpidR模型，使用掩码预测目标结合自蒸馏和在线聚类进行训练。学生模型的中间层学习预测来自教师模型中间层的分配，以稳定在线聚类过程并产生更高质量的码本。同时开发了高效的代码库以加速预训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;SpidR在下游语言建模基准测试(sWUGGY, sBLIMP, tSC)上优于wav2vec 2.0, HuBERT, WavLM和DinoSR。系统评估验证了语音单元质量(ABX, PNMI)与语言建模性能之间的相关性可作为可靠指标。SpidR显著减少了预训练时间，仅需16 GPU上一天的时间，而HuBERT需要一周。&lt;h4&gt;结论&lt;/h4&gt;SpidR是一种高效的自监督语音表示模型，能够从原始语音中学习高质量的语义表征，适用于无文本的口语语言建模，并且在计算效率上具有优势。&lt;h4&gt;翻译&lt;/h4&gt;语言建模和语音表征学习的并行进展提出了直接从语音学习语言而不需要文本中间媒介的前景。这需要直接从语音中提取语义表征。我们的贡献有三方面。首先，我们介绍了SpidR，一种自监督语音表示模型，它能够高效学习具有高度可及语音信息的表示，这使得它特别适用于无文本的口语语言建模。它使用掩码预测目标结合自蒸馏和在线聚类在原始波形上进行训练。学生模型的中间层学习预测来自教师模型中间层的分配。与以往的方法相比，这种学习目标稳定了在线聚类过程，从而产生了更高质量的码本。在下游语言建模基准测试上，SpidR优于其他模型。其次，我们在模型和层之间系统评估了语音单元质量与语言建模性能之间的相关性，验证了这些指标作为可靠代理的效用。最后，与HuBERT相比，SpidR显著减少了预训练时间。我们在GitHub上开源了训练代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.</description>
      <author>example@mail.com (Maxime Poli, Mahi Luthra, Youssef Benchekroun, Yosuke Higuchi, Martin Gleize, Jiayi Shen, Robin Algayres, Yu-An Chung, Mido Assran, Juan Pino, Emmanuel Dupoux)</author>
      <guid isPermaLink="false">2512.20308v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning</title>
      <link>http://arxiv.org/abs/2512.20220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages (9 pages + Appendix and references), this is version 1&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究离线多任务强化学习，其中多个任务共享动作值函数的低秩表示，通过固定数据集学习并利用共享结构提高统计效率和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;学习者获得从几个相关任务收集的固定数据集，无法进行在线交互，需要利用共享结构提高效率和泛化。&lt;h4&gt;目的&lt;/h4&gt;通过离线多任务强化学习，利用任务间共享的低秩表示结构提高统计效率和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;分析多任务版本的fitted Q-iteration方法，通过在离线数据上进行Bellman误差最小化，联合学习共享表示和任务特定的值函数。&lt;h4&gt;主要发现&lt;/h4&gt;跨任务合并数据提高估计精度，得出与总样本数相关的1/√(nT)依赖关系；在新任务中重用共享表示可降低下游学习复杂度。&lt;h4&gt;结论&lt;/h4&gt;共享表示在多任务离线Q学习中起重要作用，为无模型、基于值的强化学习中多任务结构提高泛化能力提供了理论依据。&lt;h4&gt;翻译&lt;/h4&gt;我们研究在多个任务共享其动作值函数低秩表示的设置下的离线多任务强化学习。在这种机制下，学习者获得从几个相关任务收集的固定数据集，无法进行进一步的在线交互，并寻求利用共享结构来提高统计效率和泛化能力。我们分析了fitted Q-iteration的一个多任务变体，通过在离线数据上进行Bellman误差最小化，联合学习共享表示和任务特定的值函数。在离线强化学习中常用的可实现性和覆盖假设下，我们为学习到的值函数建立了有限样本泛化保证。我们的分析明确说明了跨任务合并数据如何提高估计精度，得出与跨任务总样本数相关的1/√(nT)依赖关系，同时保留了由于分布偏移而产生的对范围和集中性系数的依赖关系。此外，我们考虑了一个下游离线设置，其中新任务与上游任务共享相同的底层表示。我们研究了在多任务阶段重用学习到的表示如何影响新任务的价值估计，并表明与从头学习相比，它可以降低下游学习的有效复杂度。总的来说，我们的结果阐明了共享表示在多任务离线Q学习中的作用，并为无模型、基于值的强化学习中多任务结构何时以及如何提高泛化能力提供了理论见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.</description>
      <author>example@mail.com (Kausthubh Manda, Raghuram Bharadwaj Diddigi)</author>
      <guid isPermaLink="false">2512.20220v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation</title>
      <link>http://arxiv.org/abs/2512.20117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://trilarflagz.github.io/DDAVS-page/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DDAVS是一种解耦音频语义和延迟双向对齐的框架，通过可学习查询提取音频语义并锚定在结构化语义空间中，以及使用双向交叉注意力减轻视听错位，在视听分割任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;视听分割(AVS)旨在通过结合听觉和视觉信息在像素级别定位发声物体，但现有方法常受多源纠缠和视听错位困扰，导致偏向响亮或大型物体而忽略较弱、较小或同时存在的声源。&lt;h4&gt;目的&lt;/h4&gt;解决多源纠缠和视听错位问题，提高视听分割的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;DDAVS框架使用可学习查询提取音频语义并将其锚定在从音频原型存储库派生的结构化语义空间中，通过对比学习优化；同时引入具有延迟模态交互的双向交叉注意力减轻视听错位。&lt;h4&gt;主要发现&lt;/h4&gt;在AVS-Objects和VPO基准上的实验表明，DDAVS在单源、多源和多实例场景中均优于现有方法，展现出强大的性能和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;DDAVS框架在具有挑战性的真实世界视听分割条件下表现出有效性和良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;视听分割(AVS)旨在通过联合利用听觉和视觉信息，在像素级别定位发声物体。然而，现有方法常常受到多源纠缠和视听错位的困扰，导致对更响亮或更大物体产生偏见，同时忽略了较弱、较小或同时存在的声源。为解决这些挑战，我们提出了DDAVS，一种解耦音频语义和延迟双向对齐的框架。为了减轻多源纠缠，DDAVS使用可学习的查询来提取音频语义，并将其锚定在从音频原型存储库派生的结构化语义空间中。这通过对比学习进一步优化，以提高判别性和鲁棒性。为了减轻视听错位，DDAVS引入了具有延迟模态交互的双向交叉注意力，提高了多模态对齐的鲁棒性。在AVS-Objects和VPO基准上的大量实验表明，DDAVS持续优于现有方法，在单源、多源和多实例场景中展现出强大的性能。这些结果验证了我们的框架在具有挑战性的真实世界视听分割条件下的有效性和泛化能力。项目页面：https://trilarflagz.github.io/DDAVS-page/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/</description>
      <author>example@mail.com (Jingqi Tian, Yiheng Du, Haoji Zhang, Yuji Wang, Isaac Ning Lee, Xulong Bai, Tianrui Zhu, Jingxuan Niu, Yansong Tang)</author>
      <guid isPermaLink="false">2512.20117v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction</title>
      <link>http://arxiv.org/abs/2512.19871v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为HyGE-Occ的新型框架，用于3D全景占用预测，该框架利用混合视图转换分支结合3D高斯和边缘先验，增强了几何一致性和边界感知能力。&lt;h4&gt;背景&lt;/h4&gt;3D全景占用预测旨在通过预测3D空间中每个占用区域的语义类别和实例身份来重建密集的体积场景图。实现这种细粒度的3D理解需要在复杂环境中进行精确的几何推理和空间一致的场景表示。然而，现有方法往往难以保持精确的几何形状并捕获对鲁棒性全景分离至关重要的3D实例的精确空间范围。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提高3D全景占用预测中的几何一致性和边界感知能力。&lt;h4&gt;方法&lt;/h4&gt;提出HyGE-Occ框架，它采用混合视图转换分支，融合基于连续高斯的深度表示与离散深度箱体公式，生成具有改进几何一致性和结构连贯性的BEV特征。同时，从BEV特征中提取边缘图作为辅助信息来学习边缘线索。&lt;h4&gt;主要发现&lt;/h4&gt;在Occ3D-nuScenes数据集上的大量实验中，HyGE-Occ优于现有工作，展现出卓越的3D几何推理能力。&lt;h4&gt;结论&lt;/h4&gt;HyGE-Occ通过混合视图转换方法和边缘辅助学习能够有效提升3D全景占用预测的性能，特别是在几何一致性和边界感知方面。&lt;h4&gt;翻译&lt;/h4&gt;3D全景占用预测旨在通过预测3D空间中每个占用区域的语义类别和实例身份来重建密集的体积场景图。实现这种细粒度的3D理解需要在复杂环境中进行精确的几何推理和空间一致的场景表示。然而，现有方法往往难以保持精确的几何形状并捕获对鲁棒性全景分离至关重要的3D实例的精确空间范围。为了克服这些局限性，我们引入了HyGE-Occ，这是一个新颖的框架，它利用混合视图转换分支结合3D高斯和边缘先验，增强3D全景占用预测中的几何一致性和边界感知能力。HyGE-Occ采用混合视图转换分支，融合基于连续高斯的深度表示与离散深度箱体公式，生成具有改进几何一致性和结构连贯性的BEV特征。同时，我们从BEV特征中提取边缘图并将其用作辅助信息来学习边缘线索。在Occ3D-nuScenes数据集上的大量实验中，HyGE-Occ优于现有工作，展现出卓越的3D几何推理能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D全景占用预测中的两个关键挑战：保持精确的几何形状和空间一致性，以及捕获3D实例的精确空间范围以实现稳健的全景分离。这个问题在现实中非常重要，因为精确的3D占用估计对于自动驾驶等应用至关重要，它帮助系统理解完整的场景布局（包括可见和被遮挡的结构），从而支持安全运动规划和长期场景预测。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法的局限性：离散深度预测方法存在深度量化问题，产生粗糙几何形状和模糊物体边缘；而许多3D占用预测框架难以精确描绘语义和实例边界。作者借鉴了现有工作的优势，包括Lift-Splat-Shoot的离散深度投影方法、3D高斯飞溅的连续深度表示方法，以及边缘感知学习在图像分割中的应用。基于这些观察，作者设计了混合视图转换分支来融合连续和离散深度表示，并引入边缘预测模块来增强边界感知能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合连续高斯表示和离散LSS特征来增强几何一致性，同时通过边缘先验来提高边界感知能力。整体流程包括：1)输入多视角图像；2)使用图像主干网络提取特征；3)进行混合视图转换，包括离散深度反投影、连续高斯反投影和α混合融合；4)从混合BEV特征中提取边缘图并进行优化；5)进行全景占用预测，包括语义占用预测和实例中心预测；6)输出最终的3D全景占用预测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合视图转换分支，首次融合连续高斯表示与离散LSS特征；2)边缘预测模块，引入显式边缘监督到体积解码中；3)架构解耦设计，可无缝集成到各种基于BEV的占用模型中。相比之前的工作，HyGE-Occ不再局限于完全离散或完全连续的深度表示，而是将两者融合；同时引入专门的边界处理机制，而之前的框架往往忽略显式边界推理，导致边界模糊。实验表明，这种方法在几何保真度和全景质量上都优于之前的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HyGE-Occ通过结合连续高斯表示与离散深度特征并引入边缘先验，显著提升了3D全景占用预测中的几何一致性和边界精度，实现了对复杂场景更准确、更连贯的3D理解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.</description>
      <author>example@mail.com (Jong Wook Kim, Wonseok Roh, Ha Dam Baek, Pilhyeon Lee, Jonghyun Choi, Sangpil Kim)</author>
      <guid isPermaLink="false">2512.19871v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>A Community-Enhanced Graph Representation Model for Link Prediction</title>
      <link>http://arxiv.org/abs/2512.21166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种社区增强链接预测(CELP)框架，通过结合社区结构同时建模局部和全局图拓扑，解决了现有图神经网络在链接预测任务中的局限性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已成为图表示学习的主导方法，但在链接预测任务上并不总是优于传统启发式方法。现有GNNs倾向于学习局部节点表示，难以有效捕获节点对间的结构关系，且过度依赖局部邻域信息会导致过平滑问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有GNNs在链接预测中的局限性，通过引入社区结构来同时建模局部和全局图拓扑，提高链接预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出社区增强链接预测(CELP)框架，通过社区感知、置信度引导的边完成和剪枝来增强图，并集成多尺度结构特征以实现更准确的链接预测。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验结果表明，CELP实现了卓越的性能，验证了社区结构在提高链接预测准确性中的关键作用。&lt;h4&gt;结论&lt;/h4&gt;社区结构对于改进链接预测准确性至关重要，CELP框架通过结合社区感知和多尺度特征，有效解决了现有GNNs在链接预测中的局限性。&lt;h4&gt;翻译&lt;/h4&gt;尽管图神经网络(GNNs)已成为图表示学习的主导方法，但它们在链接预测任务上的表现并不总是优于传统的启发式方法，如共同邻居和Jaccard系数。这主要是因为现有的GNNs倾向于学习局部节点表示，难以有效捕获节点对之间的结构关系。此外，过度依赖局部邻域信息可能导致过平滑。先前的研究表明，引入全局结构编码可以部分缓解这一问题。为解决这些局限性，我们提出了一个社区增强链接预测(CELP)框架，该框架结合社区结构来共同建模局部和全局图拓扑。具体而言，CELP通过社区感知、置信度引导的边完成和剪枝来增强图，同时集成多尺度结构特征以实现更准确的链接预测。在多个基准数据集上的实验结果表明，CELP实现了卓越的性能，验证了社区结构在提高链接预测准确性中的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Graph Neural Networks (GNNs) have become the dominant approach for graph representation learning, their performance on link prediction tasks does not always surpass that of traditional heuristic methods such as Common Neighbors and Jaccard Coefficient. This is mainly because existing GNNs tend to focus on learning local node representations, making it difficult to effectively capture structural relationships between node pairs. Furthermore, excessive reliance on local neighborhood information can lead to over-smoothing. Prior studies have shown that introducing global structural encoding can partially alleviate this issue. To address these limitations, we propose a Community-Enhanced Link Prediction (CELP) framework that incorporates community structure to jointly model local and global graph topology. Specifically, CELP enhances the graph via community-aware, confidence-guided edge completion and pruning, while integrating multi-scale structural features to achieve more accurate link prediction. Experimental results across multiple benchmark datasets demonstrate that CELP achieves superior performance, validating the crucial role of community structure in improving link prediction accuracy.</description>
      <author>example@mail.com (Lei Wang, Darong Lai)</author>
      <guid isPermaLink="false">2512.21166v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Refinement with LLMs for Graph Representations</title>
      <link>http://arxiv.org/abs/2512.21106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出DAS（数据自适应语义细化）框架，通过将固定图神经网络和大语言模型结合在闭环反馈系统中，解决图结构数据中的结构-语义异质性问题。&lt;h4&gt;背景&lt;/h4&gt;图结构数据在预测信号来源上存在显著异质性：某些领域节点级语义占主导，其他领域结构模式起核心作用。这种异质性意味着没有固定归纳偏置的图学习模型能在多样化图领域最优泛化。现有方法从模型侧解决此问题，但面对现实世界图的开放多样性存在根本局限。&lt;h4&gt;目的&lt;/h4&gt;从数据中心视角出发，将节点语义视为任务自适应变量，提出DAS框架用于图表示学习，以应对图数据中的结构-语义异质性挑战。&lt;h4&gt;方法&lt;/h4&gt;DAS框架将固定图神经网络(GNN)和大语言模型(LLM)耦合在闭环反馈系统中。GNN提供隐式监督信号指导LLM的语义细化，细化后的语义又反馈回更新同一图学习器。&lt;h4&gt;主要发现&lt;/h4&gt;在文本丰富和文本稀疏的图上评估结果显示，该方法在结构主导的图上取得持续改进，同时在语义丰富的图上保持竞争力，证明了在结构-语义异质性下数据中心语义适应的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过数据中心的语义适应方法可以有效处理图数据中的结构-语义异质性，使模型能够在不同类型的图数据上表现更好。&lt;h4&gt;翻译&lt;/h4&gt;图结构数据在预测信号的来源上表现出显著的异质性：在某些领域中，节点级语义占主导地位，而在其他领域中，结构模式则起着核心作用。这种结构-语义异质性意味着没有具有固定归纳偏置的图学习模型能够在多样化的图领域上最优地泛化。然而，大多数现有方法从模型侧解决这个问题，通过逐步注入新的归纳偏置，这在面对现实世界中图的开端多样性时仍然存在根本性限制。在这项工作中，我们采用数据中心的视角，将节点语义视为任务自适应变量。我们提出了一个用于图表示学习的数据自适应语义细化框架DAS，该框架将固定的图神经网络（GNN）和大语言模型（LLM）耦合在闭环反馈系统中。GNN提供隐式监督信号来指导LLM的语义细化，而细化的语义又被反馈回更新同一个图学习器。我们在文本丰富和文本稀疏的图上都评估了我们的方法。结果表明，在结构主导的图上取得了持续改进，同时在语义丰富的图上保持竞争力，证明了在结构-语义异质性下数据中心语义适应的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.</description>
      <author>example@mail.com (Safal Thapaliya, Zehong Wang, Jiazheng Li, Ziming Li, Yanfang Ye, Chuxu Zhang)</author>
      <guid isPermaLink="false">2512.21106v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate</title>
      <link>http://arxiv.org/abs/2512.20941v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基于图神经网络的代理模型中训练数据大小与预测精度的关系，发布了开源多保真度气动数据集，并发现测试误差随数据大小呈幂律减少，指数为-0.6122，表明数据利用效率较高。&lt;h4&gt;背景&lt;/h4&gt;数据驱动代理模型正越来越多地被采用以加速车辆设计，然而开源的多保真度数据集以及将数据集大小与模型性能关联的经验指导仍然有限。&lt;h4&gt;目的&lt;/h4&gt;研究训练数据大小与基于图神经网络的代理模型预测精度之间的关系，并发布一个开源的多保真度气动数据集用于双三角翼。&lt;h4&gt;方法&lt;/h4&gt;创建包含2448个流场快照的多保真度气动数据集，覆盖272种几何构型，攻角从11度到19度，马赫数为0.3；使用涡格法和雷诺平均纳维-斯托克斯求解器评估；构建六个不同大小的训练数据集；在固定训练预算下训练参数量从0.1到240万的模型。&lt;h4&gt;主要发现&lt;/h4&gt;测试误差随数据大小呈幂律减少，指数为-0.6122；根据缩放定律估计d维设计空间中的最优采样密度约为每维八个样本；较大的代理模型显示出改进的数据利用效率。&lt;h4&gt;结论&lt;/h4&gt;研究提供了关于数据集大小对图神经网络代理模型性能影响的实证指导，并发布开源数据集支持未来研究；模型训练预算与数据集生成成本之间存在潜在权衡。&lt;h4&gt;翻译&lt;/h4&gt;数据驱动的代理模型正越来越多地被采用以加速车辆设计。然而，开源的多保真度数据集以及将数据集大小与模型性能关联的经验指导仍然有限。本研究探讨了用于气动场预测的基于图神经网络的代理模型的训练数据大小与预测精度之间的关系。我们发布了一个开源的、用于双三角翼的多保真度气动数据集，包含2448个流场快照，覆盖272种几何构型，在马赫数为0.3的情况下，使用涡格法和雷诺平均纳维-斯托克斯求解器评估了从11度到19度的攻角。几何构型使用嵌套的Saltelli采样方案生成，以支持未来数据集扩展和基于方差的敏感性分析。使用此数据集，我们通过构建六个训练数据集并在固定训练预算下训练不同参数量的模型，进行了初步的实证缩放研究。我们发现测试误差随数据大小呈幂律减少，指数为-0.6122，表明数据利用效率高。基于这一缩放定律，我们估计最优采样密度约为每维八个样本。研究结果表明，较大的代理模型具有改进的数据利用效率，暗示数据集生成成本与模型训练预算之间可能存在权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data-driven surrogate models are increasingly adopted to accelerate vehicle design. However, open-source multi-fidelity datasets and empirical guidelines linking dataset size to model performance remain limited. This study investigates the relationship between training data size and prediction accuracy for a graph neural network (GNN) based surrogate model for aerodynamic field prediction. We release an open-source, multi-fidelity aerodynamic dataset for double-delta wings, comprising 2448 flow snapshots across 272 geometries evaluated at angles of attack from 11 (degree) to 19 (degree) at Ma=0.3 using both Vortex Lattice Method (VLM) and Reynolds-Averaged Navier-Stokes (RANS) solvers. The geometries are generated using a nested Saltelli sampling scheme to support future dataset expansion and variance-based sensitivity analysis. Using this dataset, we conduct a preliminary empirical scaling study of the MF-VortexNet surrogate by constructing six training datasets with sizes ranging from 40 to 1280 snapshots and training models with 0.1 to 2.4 million parameters under a fixed training budget. We find that the test error decreases with data size with a power-law exponent of -0.6122, indicating efficient data utilization. Based on this scaling law, we estimate that the optimal sampling density is approximately eight samples per dimension in a d-dimensional design space. The results also suggest improved data utilization efficiency for larger surrogate models, implying a potential trade-off between dataset generation cost and model training budget.</description>
      <author>example@mail.com (Yiren Shen, Juan J. Alonso)</author>
      <guid isPermaLink="false">2512.20941v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining</title>
      <link>http://arxiv.org/abs/2512.20915v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了GCO-HPIF，一个基于机器学习的通用框架，用于预测和解释可在图上表示的组合优化问题的计算难度。该框架包含两个阶段：第一阶段创建包含问题无关图特征和难度分类的数据集，并训练机器学习分类算法；第二阶段使用关联规则挖掘算法解释预测结果，并训练回归模型预测计算时间。&lt;h4&gt;背景&lt;/h4&gt;组合优化问题的计算难度预测对于算法选择和性能优化至关重要，需要一种能够同时提供预测和解释功能的通用框架。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的机器学习框架，用于预测和解释组合优化问题的计算难度，特别关注可在图上表示的问题。&lt;h4&gt;方法&lt;/h4&gt;构建包含图特征和难度分类的数据集；训练分类算法将图特征映射到难度类别；使用FP-Growth算法进行关联规则挖掘以解释预测；训练回归模型预测计算时间；在3287个来自COLLAB、IMDB和TWITTER图数据集的最大团问题实例上测试，使用五种算法：Gurobi、CliSAT、MOMC、EGN和HGS。&lt;h4&gt;主要发现&lt;/h4&gt;框架在难度预测方面表现出色，仅使用三个图特征就实现了0.9921的加权F1分数、0.878的少数类F1分数和0.9083的ROC-AUC分数；FP-Growth算法找到的最佳关联规则对困难实例的支持度为0.8829，总体准确率为87.64%；最佳回归模型实现了5.12的百分比RMSE和0.991的R2值。&lt;h4&gt;结论&lt;/h4&gt;GCO-HPIF框架是一个有效的方法，用于预测和解释组合优化问题的计算难度，特别是在图上表示的问题。该框架不仅实现了高精度的预测，还提供了可解释性，有助于理解问题难度背后的原因。&lt;h4&gt;翻译&lt;/h4&gt;本研究引入了GCO-HPIF，一个基于机器学习的通用框架，用于预测和解释可在图上表示的组合优化问题的计算难度。该框架包含两个阶段。在第一阶段，创建包含问题无关图特征和问题实例难度分类的数据集。基于机器学习的分类算法被训练以将图特征映射到难度类别。在第二阶段，框架使用关联规则挖掘算法解释预测结果。此外，基于机器学习的回归模型被训练以预测算法计算时间。GCO-HPIF框架应用于从COLLAB、IMDB和TWITTER图数据集编译的3287个最大团问题实例，使用了五种最先进的算法，即三种精确分支定界算法（Gurobi、CliSAT和MOMC）和两种基于图神经网络的算法（EGN和HGS）。该框架在预测实例难度方面表现出色，仅使用三个图特征就实现了0.9921的加权F1分数、0.878的少数类F1分数和0.9083的ROC-AUC分数。FP-Growth算法找到的用于解释难度预测的最佳关联规则对困难实例的支持度为0.8829，总体准确率为87.64%，强调了该框架在预测和解释方面的实用性。此外，用于预测计算时间的最佳回归模型实现了5.12的百分比RMSE和0.991的R2值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces GCO-HPIF, a general machine-learning-based framework to predict and explain the computational hardness of combinatorial optimization problems that can be represented on graphs. The framework consists of two stages. In the first stage, a dataset is created comprising problem-agnostic graph features and hardness classifications of problem instances. Machine-learning-based classification algorithms are trained to map graph features to hardness categories. In the second stage, the framework explains the predictions using an association rule mining algorithm. Additionally, machine-learning-based regression models are trained to predict algorithmic computation times. The GCO-HPIF framework was applied to a dataset of 3287 maximum clique problem instances compiled from the COLLAB, IMDB, and TWITTER graph datasets using five state-of-the-art algorithms, namely three exact branch-and-bound-based algorithms (Gurobi, CliSAT, and MOMC) and two graph-neural-network-based algorithms (EGN and HGS). The framework demonstrated excellent performance in predicting instance hardness, achieving a weighted F1 score of 0.9921, a minority-class F1 score of 0.878, and an ROC-AUC score of 0.9083 using only three graph features. The best association rule found by the FP-Growth algorithm for explaining the hardness predictions had a support of 0.8829 for hard instances and an overall accuracy of 87.64 percent, underscoring the framework's usefulness for both prediction and explanation. Furthermore, the best-performing regression model for predicting computation times achieved a percentage RMSE of 5.12 and an R2 value of 0.991.</description>
      <author>example@mail.com (Bharat Sharman, Elkafi Hassini)</author>
      <guid isPermaLink="false">2512.20915v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction</title>
      <link>http://arxiv.org/abs/2512.20885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了三种建模方法来提高现代通信网络中流量延迟预测的准确性，包括异构GNN基线模型、FlowKANet模型和符号代理模型。&lt;h4&gt;背景&lt;/h4&gt;准确预测流量延迟对于优化和管理现代通信网络至关重要。&lt;h4&gt;目的&lt;/h4&gt;通过三种不同级别的建模方法提高流量延迟预测的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;首先实现基于注意力的异构GNN建立神经基线；其次提出FlowKANet，用Kolmogorov-Arnold网络替代标准MLP层，减少参数同时保持性能；最后通过块状回归将模型蒸馏为符号代理模型，产生闭式方程。&lt;h4&gt;主要发现&lt;/h4&gt;KAN层在效率和准确性之间提供了有利权衡；符号代理模型强调了轻量级部署和增强透明度的潜力。&lt;h4&gt;结论&lt;/h4&gt;FlowKANet和符号代理模型在流量延迟预测中表现出色，提供了从高效计算到透明解释的多种优势。&lt;h4&gt;翻译&lt;/h4&gt;准确预测流量延迟对于优化和管理现代通信网络至关重要。我们研究了此任务的三种建模级别。首先，我们实现了一个基于注意力的异构GNN，建立了强大的神经基线。其次，我们提出了FlowKANet，其中Kolmogorov-Arnold网络替代了标准MLP层，减少了可训练参数，同时保持了竞争性的预测性能。FlowKANet集成了KAMP-Attn（带有注意力的Kolmogorov-Arnold消息传递），将KAN运算符直接嵌入到消息传递和注意力计算中。最后，我们使用块状回归将模型蒸馏为符号代理模型，产生消除可训练权重但保留图结构依赖性的闭式方程。结果表明，KAN层在效率和准确性之间提供了有利的权衡，而符号代理模型则强调了轻量级部署和增强透明度的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of flow delay is essential for optimizing and managing modern communication networks. We investigate three levels of modeling for this task. First, we implement a heterogeneous GNN with attention-based message passing, establishing a strong neural baseline. Second, we propose FlowKANet in which Kolmogorov-Arnold Networks replace standard MLP layers, reducing trainable parameters while maintaining competitive predictive performance. FlowKANet integrates KAMP-Attn (Kolmogorov-Arnold Message Passing with Attention), embedding KAN operators directly into message-passing and attention computation. Finally, we distill the model into symbolic surrogate models using block-wise regression, producing closed-form equations that eliminate trainable weights while preserving graph-structured dependencies. The results show that KAN layers provide a favorable trade-off between efficiency and accuracy and that symbolic surrogates emphasize the potential for lightweight deployment and enhanced transparency.</description>
      <author>example@mail.com (Sami Marouani, Kamal Singh, Baptiste Jeudy, Amaury Habrard)</author>
      <guid isPermaLink="false">2512.20885v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface</title>
      <link>http://arxiv.org/abs/2512.20813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型双专家集成框架，将野火风险脆弱性分解为环境传播和结构脆弱性两个向量，结合图神经网络和XGBoost模型，实现了对野火在城市环境中传播路径的精确预测和风险评估。&lt;h4&gt;背景&lt;/h4&gt;随着野火日益演变成城市大火灾，传统风险模型将结构视为孤立资产，无法捕捉野地城市界面(WUI)特有的非线性传播动力学特性。&lt;h4&gt;目的&lt;/h4&gt;填补机械物理学与数据驱动学习之间的空白，建立一种新的双专家集成框架，更准确地预测野火在城市环境中的传播路径，为社区韧性提供数据驱动的决策支持。&lt;h4&gt;方法&lt;/h4&gt;建立双专家集成框架：环境专家使用图神经网络(GNN)实现，将社区视为有向传播图，权重基于物理信息化的对流、辐射和飞火概率，并融入高维Google AlphaEarth Foundation嵌入；结构专家通过XGBoost实现，隔离细粒度的资产级弹性；最后通过逻辑堆叠整合不同信号，生成诊断风险拓扑。&lt;h4&gt;主要发现&lt;/h4&gt;应用于2025年伊顿火灾的框架揭示了风险驱动因素的关键二分法：社区规模的环境压力在定义传播路径方面压倒性地胜过内在结构特征；而屋檐是微观尺度上的主要侵入向量。&lt;h4&gt;结论&lt;/h4&gt;通过整合环境传播和结构脆弱性两种信号，该模型使决策者能够超越二元损失预测，精确确定缓解措施优先级：对高连接集群进行植被管理，对建筑结构脆弱的节点进行结构加固，从而实施主动的、数据驱动的社区韧性方法。&lt;h4&gt;翻译&lt;/h4&gt;随着野火日益演变成城市大火灾，传统风险模型将结构视为孤立资产，无法捕捉野地城市界面(WUI)特有的非线性传播动力学特性。本研究通过建立一种新的双专家集成框架弥合了机械物理学与数据驱动学习之间的差距，将脆弱性分解为环境传播和结构脆弱性两个不同的向量。该架构集成了两个专门的预测流：环境专家，实现为图神经网络(GNN)，将社区操作化为有向传播图，权重基于物理信息化的对流、辐射和飞火概率，并融入高维Google AlphaEarth Foundation嵌入；以及结构专家，通过XGBoost实现，隔离细粒度的资产级弹性。应用于2025年伊顿火灾，该框架揭示了风险驱动因素的关键二分法。GNN表明，社区规模的环境压力在定义传播路径方面压倒性地胜过内在结构特征，而XGBoost模型则发现屋檐是微观尺度上的主要侵入向量。通过逻辑堆叠整合这些不同的信号，集成模型实现了稳健的分类，并生成了诊断风险拓扑。这种能力使决策者能够超越二元损失预测，精确地确定缓解措施的优先级：对高连接集群进行植被管理，对建筑结构脆弱的节点进行结构加固，从而实施主动的、数据驱动的社区弹性方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As wildfires increasingly evolve into urban conflagrations, traditional risk models that treat structures as isolated assets fail to capture the non-linear contagion dynamics characteristic of the wildland urban interface (WUI). This research bridges the gap between mechanistic physics and data driven learning by establishing a novel dual specialist ensemble framework that disentangles vulnerability into two distinct vectors, environmental contagion and structural fragility. The architecture integrates two specialized predictive streams, an environmental specialist, implemented as a graph neural network (GNN) that operationalizes the community as a directed contagion graph weighted by physics informed convection, radiation, and ember probabilities, and enriched with high dimensional Google AlphaEarth Foundation embeddings, and a Structural Specialist, implemented via XGBoost to isolate granular asset level resilience. Applied to the 2025 Eaton Fire, the framework reveals a critical dichotomy in risk drivers. The GNN demonstrates that neighborhood scale environmental pressure overwhelmingly dominates intrinsic structural features in defining propagation pathways, while the XGBoost model identifies eaves as the primary micro scale ingress vector. By synthesizing these divergent signals through logistic stacking, the ensemble achieves robust classification and generates a diagnostic risk topology. This capability empowers decision makers to move beyond binary loss prediction and precisely target mitigation prioritizing vegetation management for high connectivity clusters and structural hardening for architecturally vulnerable nodes thereby operationalizing a proactive, data driven approach to community resilience.</description>
      <author>example@mail.com (Miguel Esparza, Vamshi Battal, Ali Mostafavi)</author>
      <guid isPermaLink="false">2512.20813v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Symbolic regression for defect interactions in 2D materials</title>
      <link>http://arxiv.org/abs/2512.20785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;机器学习模型已在所有科学领域广泛应用。神经网络方法虽能获得高精度，但存在缺点。符号回归技术可发现描述数据的解析方程，提供可解释和可推广的模型，随着神经网络技术进步获得新动力。本研究应用深度符号回归算法SEGVAE确定二维材料性质，并与图神经网络方法比较。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型已在所有科学领域得到广泛应用。神经网络方法虽能获得高精度，但存在一些缺点。符号回归技术因其可解释性和结果可推广性而具有优势，随着神经网络技术的发展，符号回归方法获得了新的动力。&lt;h4&gt;目的&lt;/h4&gt;研究深度符号回归算法SEGVAE在确定具有缺陷的二维材料性质方面的应用，并与基于图神经网络的最先进方法进行比较。&lt;h4&gt;方法&lt;/h4&gt;使用深度符号回归算法SEGVAE来确定具有缺陷的二维材料的性质，并将结果与基于图神经网络的最先进方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;SEGVAE算法与最先进的基于图神经网络的方法相比，显示出可比性，在某些情况下甚至产生了相同的结果。&lt;h4&gt;结论&lt;/h4&gt;符号回归方法，特别是结合了神经网络技术的深度符号回归，在自然科学领域具有广泛的应用前景，特别是在需要可解释结果的场景中。&lt;h4&gt;翻译&lt;/h4&gt;机器学习模型已在所有科学领域得到牢固确立。从数据中提取特征并使用神经网络模型基于这些特征进行推断通常能获得高精度；然而，这种方法有几个缺点。符号回归是一种强大的技术，可以发现描述数据的解析方程，提供可解释和可推广的模型，能够预测未见过的数据。随着神经网络技术的进步，符号回归方法获得了新的动力，并提供了几个优势，主要结果是结果的可解释性。在本工作中，我们研究了深度符号回归算法SEGVAE在确定具有缺陷的二维材料性质方面的应用。与最先进的基于图神经网络的方法相比，结果显示出可比性，在某些情况下甚至相同的结果。我们还讨论了这类方法在自然科学中的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning models have become firmly established across all scientific fields. Extracting features from data and making inferences based on them with neural network models often yields high accuracy; however, this approach has several drawbacks. Symbolic regression is a powerful technique for discovering analytical equations that describe data, providing interpretable and generalizable models capable of predicting unseen data. Symbolic regression methods have gained new momentum with the advancement of neural network technologies and offer several advantages, the main one being the interpretability of results. In this work, we examined the application of the deep symbolic regression algorithm SEGVAE to determine the properties of two-dimensional materials with defects. Comparing the results with state-of-the-art graph neural network-based methods shows comparable or, in some cases, even identical outcomes. We also discuss the applicability of this class of methods in natural sciences.</description>
      <author>example@mail.com (Mikhail Lazarev, Andrey Ustyuzhanin)</author>
      <guid isPermaLink="false">2512.20785v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Fast SAM2 with Text-Driven Token Pruning</title>
      <link>http://arxiv.org/abs/2512.21333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种文本引导的token修剪框架，通过选择性减少视觉token密度来提高SAM2模型的推理效率，同时保持分割性能。&lt;h4&gt;背景&lt;/h4&gt;SAM2模型在提示驱动的视频对象分割方面取得显著进展，但其实际部署受限于处理密集视觉token的高计算和内存成本。SAM2管道通常传播所有视觉token，无论是否与目标对象相关，导致可扩展性降低。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法提高SAM2模型的推理效率，减少计算和内存需求，同时保持分割质量，使其更适合实时和资源受限的应用。&lt;h4&gt;方法&lt;/h4&gt;引入文本引导的token修剪框架，在视觉编码后和基于内存的传播前操作。使用轻量级路由机制对token进行排序，该机制整合局部视觉上下文、从文本描述派生的语义相关性以及不确定性提示，保留最具信息量的token进行下游处理。&lt;h4&gt;主要发现&lt;/h4&gt;在多个视频分割基准上的实验表明，该方法与未修剪的基线SAM2相比，实现了42.50%的更快推理速度和37.41%的更低GPU内存使用，同时保持了有竞争力的J和F性能。&lt;h4&gt;结论&lt;/h4&gt;早期token选择显著提高了基于transformer的视频分割系统的可扩展性，为实时和资源受限应用中的高效视频分割提供了实用且有效的途径。&lt;h4&gt;翻译&lt;/h4&gt;Segment Anything Model 2 (SAM2)作为一种视觉基础模型，在提示驱动的视频对象分割方面取得了显著进展，然而其实际部署仍受限于处理密集视觉token的高计算和内存成本。SAM2管道通常将图像编码器产生的所有视觉token传播到下游时间推理模块，无论它们是否与目标对象相关，导致由于二次内存注意力开销而降低了可扩展性。在这项工作中，我们引入了一个文本引导的token修剪框架，通过在时间传播前选择性减少token密度来提高推理效率，同时不修改底层分割架构。在视觉编码后和基于内存的传播前操作，我们的方法使用轻量级路由机制对token进行排序，该机制集成了局部视觉上下文、从以对象为中心的文本描述中派生的语义相关性（用户提供或自动生成）以及不确定性提示，有助于保留模糊或边界关键区域。通过仅保留最具信息量的token进行下游处理，所提出的方法减少了冗余计算，同时保持了分割保真度。在多个具有挑战性的视频分割基准上的广泛实验表明，编码器后token修剪提供了一种实用且高效的提示感知视频分割途径，与未修剪的基线SAM2相比，实现了42.50%的更快推理速度和37.41%的更低GPU内存使用，同时保持了有竞争力的J和F性能。这些结果突出了早期token选择在提高基于transformer的视频分割系统可扩展性方面的潜力，适用于实时和资源受限应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.</description>
      <author>example@mail.com (Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang, Caiyan Qin, Guoqing Wang, Yang Yang, Heng Tao Shen)</author>
      <guid isPermaLink="false">2512.21333v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning</title>
      <link>http://arxiv.org/abs/2512.21331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TICON是一种基于Transformer的瓦片表示上下文化器，能够为计算病理学中的各种应用生成丰富、上下文化的嵌入，显著提高了多个任务的性能，并在多个基准测试上建立了新的最先进结果。&lt;h4&gt;背景&lt;/h4&gt;在计算病理学中，解释大型全幻灯片图像(WSI)中的小瓦片通常需要更大的图像上下文，但标准的瓦片编码器管道无法建模幻灯片级别的丰富信息，且不同瓦片编码器在不同下游任务上表现各异。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的模型来上下文化来自任何瓦片级基础模型的嵌入，解决现有方法无法充分利用幻灯片级信息的问题。&lt;h4&gt;方法&lt;/h4&gt;TICON使用单个共享编码器，通过掩模建模目标进行预训练，同时统一和上下文化来自多样化瓦片级病理学基础模型的表示，并在TICON上预训练聚合器形成幻灯片级基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;TICON上下文化的嵌入显著提高了多个不同任务上的性能，在瓦片级基准测试(HEST-Bench, THUNDER, CATCH)和幻灯片级基准测试(Patho-Bench)上建立了新的最先进结果；仅使用11K个WSI预训练的幻灯片级基础模型就超过了使用多达350K个WSI预训练的最先进模型。&lt;h4&gt;结论&lt;/h4&gt;TICON有效解决了计算病理学中瓦片表示缺乏上下文信息的问题，提供了一个强大而高效的解决方案，能够统一和增强来自不同瓦片编码器的表示，在各种病理学任务中取得了显著性能提升。&lt;h4&gt;翻译&lt;/h4&gt;在大型全幻灯片图像(WSI)中解释小瓦片通常需要更大的图像上下文。我们引入了TICON，一种基于Transformer的瓦片表示上下文化器，它能为计算病理学中的'任何'应用程序生成丰富、上下文化的嵌入。标准的基于瓦片编码器的管道提取的是剥离了上下文的瓦片嵌入，无法建模对本地和全局任务都至关重要的幻灯片级别丰富信息。此外，不同的瓦片编码器在不同的下游任务上表现出色。因此，需要一个统一的模型来上下文化来自'任何'瓦片级基础模型的嵌入。TICON通过使用单个共享编码器满足了这一需求，该编码器使用掩模建模目标进行预训练，同时统一和上下文化来自多样化瓦片级病理学基础模型的表示。我们的实验证明，TICON上下文化的嵌入显著提高了许多不同任务的性能，在瓦片级基准测试(即HEST-Bench, THUNDER, CATCH)和幻灯片级基准测试(即Patho-Bench)上建立了新的最先进结果。最后，我们在TICON上预训练了一个聚合器，形成一个幻灯片级基础模型，仅使用11K个WSI，就超过了使用多达350K个WSI预训练的最先进幻灯片级基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.</description>
      <author>example@mail.com (Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li, Sofiène Boutaj, Pushpak Pati, Srikar Yellapragada, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Prateek Prasanna, Stergios Christodoulidis Maria Vakalopoulou, Dimitris Samaras)</author>
      <guid isPermaLink="false">2512.21331v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential</title>
      <link>http://arxiv.org/abs/2512.21284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SpikeSurgSeg，第一个为手术场景分割设计的脉冲驱动的视频Transformer框架，在非GPU平台上实现了实时潜力，同时保持了高分割精度。&lt;h4&gt;背景&lt;/h4&gt;现代外科手术系统依赖智能场景理解提供及时感知，手术场景分割是核心。尽管深度学习模型取得显著精度，但其计算需求和功耗阻碍了在资源受限环境中的实时部署。&lt;h4&gt;目的&lt;/h4&gt;解决深度学习模型在资源受限环境中实时部署的问题，探索SNN作为高效外科智能的范式。&lt;h4&gt;方法&lt;/h4&gt;提出SpikeSurgSeg框架，采用外科场景掩码自编码预训练策略解决标注数据有限问题，并通过轻量级脉冲驱动的分割头保持低延迟特性。&lt;h4&gt;主要发现&lt;/h4&gt;在EndoVis18和SurgBleed数据集上，SpikeSurgSeg实现了与最先进ANN模型相当的mIoU，推理延迟减少至少8倍，比大多数基础模型基线加速超过20倍。&lt;h4&gt;结论&lt;/h4&gt;SpikeSurgSeg在时间关键的手术场景分割中显示出显著潜力。&lt;h4&gt;翻译&lt;/h4&gt;现代外科手术系统越来越依赖智能场景理解来提供及时的场景感知，以提高手术中的安全性。在这个流程中，手术场景分割在准确感知手术事件方面起着核心作用。虽然最近的深度学习模型，特别是大规模基础模型，取得了显著的分割精度，但其巨大的计算需求和功耗阻碍了在资源受限的手术环境中的实时部署。为解决这一限制，我们探索了新兴的SNN作为一种有前途的高效外科智能范式。然而，其性能仍然受到标注外科数据稀缺性和外科视频表示固有稀疏性的限制。为此，我们提出了SpikeSurgSeg，这是第一个为手术场景分割量身定制的脉冲驱动的视频Transformer框架，具有在非GPU平台上实时运行的潜力。为解决外科标注可用性有限的问题，我们引入了一种针对SNN的外科场景掩码自编码预训练策略，通过逐层管状掩码实现强大的时空表征学习。基于这个预训练骨干网络，我们进一步采用了一个轻量级脉冲驱动的分割头，在保持SNN低延迟特性的同时产生时间一致的预测。在EndoVis18和我们内部的SurgBleed数据集上的大量实验表明，SpikeSurgSeg实现了与最先进的基于ANN的模型相当的mIoU，同时将推理延迟减少了至少8倍。值得注意的是，与大多数基础模型基线相比，它提供了超过20倍的加速，凸显了其在时间关键的手术场景分割中的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在资源受限的手术环境中实现高效且实时的手术场景分割问题。这个问题在现实中非常重要，因为现代手术系统依赖智能场景理解来提供及时的环境感知以提高手术安全性，而现有的深度学习模型虽然精度高，但计算需求和功耗大，难以在手术环境中实时部署。手术环境有物理和安全限制（如空间有限、散热限制、电源预算），使得高端GPU集群无法使用，而仅使用CPU或嵌入式平台又会导致高延迟，阻碍了智能手术系统在临床实践中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到手术场景分割面临的主要障碍是计算效率和实时性能的限制，特别是在资源受限的环境中。他们探索了脑启发计算范式，特别关注脉冲神经网络（SNN），因为其通过模仿大脑神经元动态和二进制脉冲通信实现高效计算。针对SNN在手术场景应用中的两大挑战（标注数据稀缺和领域特定特征捕捉不足），作者设计了SpikeSurgSeg框架，结合了手术场景掩码自动编码预训练策略和轻量级脉冲驱动分割头。该方法借鉴了现有工作包括SNN基本理论、Transformer架构、掩码视觉建模、自监督学习、知识蒸馏和特征金字塔网络等技术，但进行了针对性的创新和整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用脉冲神经网络（SNN）的脑启发计算范式实现高效、实时的手术场景分割，通过模仿大脑神经元动态和二进制脉冲通信实现仅加法的前向计算，结合自监督学习和知识蒸馏解决SNN在手术场景中的数据稀缺和表征能力不足问题。整体实现流程分为三个阶段：1）脉冲驱动视频编码器：结合CNN块和时空Transformer块；2）手术场景掩码自动编码预训练：应用层级管状掩码和知识蒸馏；3）手术视频分割微调：集成记忆读出模块和特征金字塔网络，使用交叉熵和焦点损失进行训练。整个流程在非GPU平台上实现低延迟和低能耗的实时分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）SpikeSurgSeg框架：首个专为手术场景分割设计的SNN框架；2）手术场景掩码自动编码预训练策略：通过层级管状掩码解决数据稀缺问题；3）轻量级脉冲驱动分割头：确保时间一致性和低延迟；4）语义知识蒸馏：增强SNN的语义理解能力。相比之前的工作，不同之处在于：实现了与ANN模型相当的分割精度但显著降低延迟和能耗；不需要用户提示输入；解决了SNN在手术数据方面的挑战；结合了时空建模能力扩展了SNN应用；保持了SNN的仅加法计算特性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SpikeSurgSeg通过创新的脉冲驱动视频Transformer和手术场景特定预训练，在非GPU平台上实现了与最先进ANN模型相当的手术分割精度，同时将推理延迟降低至少8倍，能耗降低5倍以上，为实时可部署的智能手术系统提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\times$. Notably, it delivers over $20\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.</description>
      <author>example@mail.com (Shihao Zou, Jingjing Li, Wei Ji, Jincai Huang, Kai Wang, Guo Dan, Weixin Si, Yi Pan)</author>
      <guid isPermaLink="false">2512.21284v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations</title>
      <link>http://arxiv.org/abs/2512.21004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了NExT-Vid，一种新型的自回归视觉生成预训练框架，通过掩码下一帧预测联合建模图像和视频，解决了现有视觉生成预训练方法中忽略时间信息和生成质量差的问题。&lt;h4&gt;背景&lt;/h4&gt;预训练基础模型在多样化下游任务中显著提升了性能。虽然自回归生成模型如GPT在自然语言处理中取得了革命性进展，但大多数视觉生成预训练方法仍依赖BERT风格的掩码建模，忽视了视频分析中至关重要的时间信息。现有的自回归视觉预训练方法存在语义定位不准确和生成质量差等问题，导致语义表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效建模图像和视频的自回归视觉生成预训练框架，解决现有方法中忽视时间信息和生成质量差的问题，提升视觉表示学习的能力。&lt;h4&gt;方法&lt;/h4&gt;提出了NExT-Vid框架，利用掩码下一帧预测来联合建模图像和视频。该框架引入了上下文隔离的自回归预测器来解耦语义表示与目标解码，以及一个条件流匹配解码器来增强生成质量和多样性。通过上下文隔离的流匹配预训练，该方法获得了强大的表示能力。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模预训练模型上的广泛实验表明，所提出的方法通过注意力探测在下游分类任务中，始终优于之前的视觉表示学习的生成预训练方法。&lt;h4&gt;结论&lt;/h4&gt;NExT-Vid通过创新的架构设计，成功地将自回归生成模型的优势引入视觉领域，特别是在处理视频数据时考虑了时间信息，显著提升了视觉表示学习的性能。&lt;h4&gt;翻译&lt;/h4&gt;最近预训练通用基础模型的进展显著提高了多样化下游任务中的性能。虽然像GPT这样的自回归生成模型革新了自然语言处理，但大多数视觉生成预训练方法仍然依赖于BERT风格的掩码建模，这通常忽视了视频分析中必不可少的时间信息。少数现有的自回归视觉预训练方法存在语义定位不准确和生成质量差等问题，导致语义表现不佳。在这项工作中，我们提出了NExT-Vid，一种新型的自回归视觉生成预训练框架，利用掩码下一帧预测来联合建模图像和视频。NExT-Vid引入了上下文隔离的自回归预测器来解耦语义表示与目标解码，以及一个条件流匹配解码器来增强生成质量和多样性。通过上下文隔离的流匹配预训练，我们的方法获得了强大的表示能力。在大规模预训练模型上的广泛实验表明，我们提出的方法通过注意力探测在下游分类任务中，始终优于之前的用于视觉表示学习的生成预训练方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.</description>
      <author>example@mail.com (Jinghan Li, Yang Jin, Hao Jiang, Yadong Mu, Yang Song, Kun Xu)</author>
      <guid isPermaLink="false">2512.21004v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions</title>
      <link>http://arxiv.org/abs/2512.20967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种结合现货和按需GPU实例的在线调度框架，用于降低大规模基础模型微调成本，通过预测算法和策略选择算法适应现货市场动态，实验表明可提高效用达54.8%。&lt;h4&gt;背景&lt;/h4&gt;随着基础模型规模增长，微调成本日益增加。GPU现货实例虽为低成本替代方案，但其价格和可用性的波动性使截止时间感知调度面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现货实例调度难题，通过混合使用现货和按需实例实现成本高效的调度策略。&lt;h4&gt;方法&lt;/h4&gt;1) 分析现货市场价格和可用性的可预测性；2) 建立整数规划模型捕捉混合实例使用；3) 提出基于预测的在线分配算法；4) 设计无预测的补充算法；5) 开发在线策略选择算法学习最佳策略。&lt;h4&gt;主要发现&lt;/h4&gt;预测算法随误差减小可获更紧性能界限；策略选择算法具有O(√T)遗憾界限；在线框架能根据市场动态自适应选择最佳策略。&lt;h4&gt;结论&lt;/h4&gt;提出的在线框架能持续优于基线方法，最多提高54.8%的效用，有效解决了基础模型微调中的成本优化问题。&lt;h4&gt;翻译&lt;/h4&gt;随着基础模型规模的扩大，对其进行微调变得越来越昂贵。虽然GPU现货实例提供了按需资源的低成本替代方案，但其价格和可用性的波动使得具有截止时间意识的调度特别具有挑战性。我们通过混合使用现货和按需实例来解决这个问题。特别地，我们展示了现货实例市场中价格和可用性的可预测性，预测在实现成本高效调度方面的能力及其对估计误差的敏感性。我们制定了一个整数规划问题，以捕捉在价格和可用性动态下使用混合实例的情况。我们提出了一种基于预测的在线分配算法，采用基于提交范围控制的方法，利用提交级别来执行决策的部分序列。当预测不准确时，我们进一步提出了一种无需预测的补充在线算法。开发了一种在线策略选择算法，它通过变化两个算法的参数从构建的池中学习最佳策略。我们证明，随着预测误差的减小，基于预测的算法可以实现更紧的性能界限，而策略选择算法具有O(√T)的遗憾界限。实验结果表明，我们的在线框架可以根据变化的现货市场动态和预测质量自适应选择最佳策略，持续优于基线方法，最多提高54.8%的效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\mathcal{O}(\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\%.</description>
      <author>example@mail.com (Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu)</author>
      <guid isPermaLink="false">2512.20967v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study</title>
      <link>http://arxiv.org/abs/2512.20948v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了FEND框架，一个综合多模态系统，整合语音和文本模态用于跨生命周期的阿尔茨海默病、抑郁症和自闭症谱系障碍检测，并在13种多语言数据集上进行了系统评估。&lt;h4&gt;背景&lt;/h4&gt;神经精神障碍如阿尔茨海默病、抑郁症和自闭症谱系障碍具有语言和声学异常特征，可作为早期检测的生物标志物。然而，多模态方法面临多语言泛化和缺乏统一评估框架等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决多语言泛化和缺乏统一评估框架的挑战，提出一个综合多模态框架用于检测跨生命周期的神经精神障碍。&lt;h4&gt;方法&lt;/h4&gt;提出FEND框架，整合语音和文本模态。利用13种多语言数据集（英语、中文、希腊语、法语和荷兰语）系统评估多模态融合性能。&lt;h4&gt;主要发现&lt;/h4&gt;多模态融合在AD和抑郁症检测中表现优异，但在ASD检测中表现不佳，原因是数据集异质性；模态不平衡是普遍问题，多模态融合未能超越最佳单模态模型；跨语料库实验显示在任务和语言一致场景中表现稳健，但在多语言和任务异构设置中性能下降。&lt;h4&gt;结论&lt;/h4&gt;FEND通过提供广泛基准和性能影响因素详细分析，推动了自动化、包容全生命周期和多语言神经精神障碍评估领域发展，鼓励研究人员采用该框架进行公平比较和可重复研究。&lt;h4&gt;翻译&lt;/h4&gt;神经精神障碍，如阿尔茨海默病、抑郁症和自闭症谱系障碍，其特征是语言和声学异常，为早期检测提供了潜在生物标志物。尽管多模态方法很有前景，但多语言泛化和缺乏统一评估框架等挑战仍然存在。为解决这些差距，我们提出了FEND，这是一个综合多模态框架，整合了语音和文本模态，用于跨生命周期检测AD、抑郁症和ASD。利用涵盖英语、中文、希腊语、法语和荷兰语的13种多语言数据集，我们系统评估了多模态融合性能。我们的结果表明，多模态融合在AD和抑郁症检测中表现出色，但由于数据集异质性，在ASD检测中表现不佳。我们还发现模态不平衡是一个普遍问题，多模态融合未能超越最佳单模态模型。跨语料库实验显示，在任务和语言一致的场景中表现稳健，但在多语言和任务异构设置中性能明显下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.</description>
      <author>example@mail.com (Zhongren Dong, Haotian Guo, Weixiang Xu, Huan Zhao, Zixing Zhang)</author>
      <guid isPermaLink="false">2512.20948v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Decoding Predictive Inference in Visual Language Processing via Spatiotemporal Neural Coherence</title>
      <link>http://arxiv.org/abs/2512.20929v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Foundation Models for the Brain and Body&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种机器学习框架，通过分析聋人手语使用者的脑电图反应，揭示了大脑处理语言时的预测神经动力学机制，发现左侧半球和额叶的低频神经活动是语言理解的关键，且神经特征与年龄相关。&lt;h4&gt;背景&lt;/h4&gt;人类语言处理依赖于大脑的预测推理能力，特别是聋人手语使用者处理动态视觉语言刺激的方式。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习框架，用于解码聋人对动态视觉语言刺激的神经（EEG）反应。&lt;h4&gt;方法&lt;/h4&gt;使用神经信号和光流推导的运动特征之间的一致性构建预测神经动力学的时空表示，通过基于熵的特征选择，识别区分可解释语言输入和语言紊乱刺激的频率特异性神经特征。&lt;h4&gt;主要发现&lt;/h4&gt;结果显示，左侧半球和额叶低频一致性是语言理解的关键特征，经验依赖性神经特征与年龄相关。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了一种新颖的多模态方法，用于探索大脑中经验驱动的感知生成模型。&lt;h4&gt;翻译&lt;/h4&gt;人类语言处理依赖于大脑的预测推理能力。我们提出了一种机器学习框架，用于解码聋人手语使用者对动态视觉语言刺激的神经（EEG）反应。利用神经信号与光流推导的运动特征之间的一致性，我们构建了预测神经动力学的时空表示。通过基于熵的特征选择，我们能够区分可解释的语言输入和语言紊乱（时间反转）刺激的频率特异性神经特征。我们的研究结果表明，左侧半球和额叶的低频一致性是语言理解的关键特征，且经验依赖性的神经特征与年龄相关。这项工作展示了一种新颖的多模态方法，用于探索大脑中经验驱动的感知生成模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human language processing relies on the brain's capacity for predictive inference. We present a machine learning framework for decoding neural (EEG) responses to dynamic visual language stimuli in Deaf signers. Using coherence between neural signals and optical flow-derived motion features, we construct spatiotemporal representations of predictive neural dynamics. Through entropy-based feature selection, we identify frequency-specific neural signatures that differentiate interpretable linguistic input from linguistically disrupted (time-reversed) stimuli. Our results reveal distributed left-hemispheric and frontal low-frequency coherence as key features in language comprehension, with experience-dependent neural signatures correlating with age. This work demonstrates a novel multimodal approach for probing experience-driven generative models of perception in the brain.</description>
      <author>example@mail.com (Sean C. Borneman, Julia Krebs, Ronnie B. Wilbur, Evie A. Malaia)</author>
      <guid isPermaLink="false">2512.20929v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification</title>
      <link>http://arxiv.org/abs/2512.20892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为领域表示注入(DRI)的新型参数高效微调策略，用于解决跨模态船舶重识别中的模态差异问题。该方法通过视觉基础模型和轻量级偏移编码器实现，在保持模型预训练权重不变的情况下，有效提取和注入领域特定表示，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;跨模态船舶重识别(CMS Re-ID)对实现全天候海事目标跟踪至关重要，但面临显著的模态差异挑战。主流解决方案依赖显式模态对齐策略，这严重依赖于构建大规模配对数据集进行预训练。&lt;h4&gt;目的&lt;/h4&gt;解决跨模态船舶重识别中模态差异导致的挑战，减少对大规模配对数据集的依赖，提高模型在有限参数下的性能。&lt;h4&gt;方法&lt;/h4&gt;基于柏拉图表示假设，将优化视角从权重空间转向特征空间，提出领域表示注入(DRI)策略。具体包括：保持视觉基础模型(VFM)完全冻结；设计轻量级偏移编码器提取领域特定表示；通过调制器自适应转换这些表示；通过加性融合将表示注入中间层，动态重塑特征分布以适应下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;DRI方法在HOSS-ReID数据集上实现了最先进的性能，仅使用1.54M和7.05M参数分别达到57.9%和60.5%的mAP。该方法有效弥合了模态差距，同时保持了模型的通用知识。&lt;h4&gt;结论&lt;/h4&gt;领域表示注入(DRI)是一种有效的跨模态船舶重识别方法，通过在特征空间进行优化，实现了高性能与低参数消耗的平衡，为解决模态差异问题提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;跨模态船舶重识别(CMS Re-ID)对于实现全天候和全天气海事目标跟踪至关重要，但其根本上受到显著模态差异的挑战。主流解决方案通常依赖显式模态对齐策略；然而，这种范式严重依赖于构建大规模配对数据集进行预训练。为此，基于柏拉图表示假设，我们探索了视觉基础模型(VFMs)在弥合模态差距方面的潜力。认识到现有通用的参数高效微调(PEFT)方法在权重空间操作的表现不佳，尤其是在有限容量模型上，我们将优化视角转向特征空间，并提出了一种称为领域表示注入(DRI)的新型PEFT策略。具体而言，在保持VFM完全冻结以最大化保留通用知识的同时，我们设计了一个轻量级、可学习的偏移编码器，从原始输入中提取富含模态和身份属性的领域特定表示。根据不同层中间特征的上下文信息，调制器自适应地转换这些表示。随后，它们通过加性融合注入到中间层，动态重塑特征分布以适应下游任务，同时不改变VFM的预训练权重。大量实验结果表明我们方法的优越性，仅使用最少的可训练参数就实现了最先进的(SOTA)性能。例如，在HOSS-ReID数据集上，我们仅使用1.54M和7.05M参数，分别实现了57.9%和60.5%的mAP。代码可在https://github.com/TingfengXian/DRI获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.</description>
      <author>example@mail.com (Tingfeng Xian, Wenlve Zhou, Zhiheng Zhou, Zhelin Li)</author>
      <guid isPermaLink="false">2512.20892v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task</title>
      <link>http://arxiv.org/abs/2512.20876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了视觉语言模型(VLMs)在理解机器人运动方面的能力，通过视频字幕任务测试了两种能力：机器人任务自动字幕生成和任务序列分割。研究提出了一种结合图像字幕和机器人轨迹数据的方法，以提高VLMs在机器人任务理解和分割方面的性能。&lt;h4&gt;背景&lt;/h4&gt;从机器人未来发展角度看，验证仅通过离线数据（如图像和语言）训练的基础模型能否理解机器人运动至关重要。特别是，视觉语言模型(VLMs)的训练数据中不包含机器人的低级运动信息，因此包含轨迹信息的视频理解仍是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;评估VLMs通过包含低级机器人运动信息的视频字幕任务的两种能力：(1)机器人任务自动字幕生成和(2)任务序列分割。这两种能力旨在通过连接语言和运动来提高机器人模仿学习效率，并作为基础模型性能的衡量标准。&lt;h4&gt;方法&lt;/h4&gt;提出的方法利用机器人任务的图像字幕和轨迹数据生成多个'场景'字幕，然后总结这些单个字幕生成完整任务字幕。此外，通过比较图像字幕的文本嵌入相似性执行子任务分割。在两种字幕任务中，将机器人运动数据（关节和末端执行器状态）作为输入提供给VLMs以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验验证了所提出方法的有效性，表明提供机器人运动数据作为输入可以增强VLMs在机器人任务理解和分割方面的能力。&lt;h4&gt;结论&lt;/h4&gt;结合机器人运动数据与视觉语言模型可以改善对机器人任务的理解和分割，为机器人模仿学习提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;从机器人未来发展的角度来看，验证仅离线数据（如图像和语言）训练的基础模型能否理解机器人运动至关重要。特别是，视觉语言模型(VLMs)在其训练数据中不包括机器人的低级运动信息，因此包含轨迹信息的视频理解仍然是一个重大挑战。在本研究中，我们通过包含低级机器人运动信息的视频字幕任务评估了VLMs的两种能力：(1)机器人任务自动字幕生成和(2)任务序列分割。这两种能力都旨在通过连接语言和运动来提高机器人模仿学习的效率，并作为基础模型性能的衡量标准。所提出的方法利用机器人任务的图像字幕和轨迹数据生成多个'场景'字幕，然后通过总结这些单个字幕生成完整任务字幕。此外，该方法通过比较图像字幕的文本嵌入相似性执行子任务分割。在两种字幕任务中，该方法旨在通过将机器人运动数据（关节和末端执行器状态）作为输入提供给VLMs来提高性能。进行了模拟实验以验证所提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; From the perspective of future developments in robotics, it is crucial to verify whether foundation models trained exclusively on offline data, such as images and language, can understand the robot motion. In particular, since Vision Language Models (VLMs) do not include low-level motion information from robots in their training datasets, video understanding including trajectory information remains a significant challenge. In this study, we assess two capabilities of VLMs through a video captioning task with low-level robot motion information: (1) automatic captioning of robot tasks and (2) segmentation of a series of tasks. Both capabilities are expected to enhance the efficiency of robot imitation learning by linking language and motion and serve as a measure of the foundation model's performance. The proposed method generates multiple "scene" captions using image captions and trajectory data from robot tasks. The full task caption is then generated by summarizing these individual captions. Additionally, the method performs subtask segmentation by comparing the similarity between text embeddings of image captions. In both captioning tasks, the proposed method aims to improve performance by providing the robot's motion data - joint and end-effector states - as input to the VLM. Simulator experiments were conducted to validate the effectiveness of the proposed method.</description>
      <author>example@mail.com (Kanata Suzuki, Shota Shimizu, Tetsuya Ogata)</author>
      <guid isPermaLink="false">2512.20876v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs</title>
      <link>http://arxiv.org/abs/2512.20861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对基于Transformer的基础模型在GPU上部署的内存和计算效率问题，通过优化的块低秩(BLR)压缩技术实现了显著加速和模型压缩。&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的基础模型已成为许多任务的默认选择，但其快速增长使得在单个GPU上完整运行变得困难，计算成本过高。&lt;h4&gt;目的&lt;/h4&gt;解决BLR方法在多令牌推理中面临的内存限制问题，提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;引入带有部分融合和内存布局优化的自定义Triton内核，应用于Monarch和BLAST两种BLR方法。&lt;h4&gt;主要发现&lt;/h4&gt;在内存受限的NVIDIA GPU上，优化后的内核相比PyTorch密集基线实现了高达3.76倍的加速和3倍模型大小压缩，同时支持多种模型如Llama-7/1B、GPT2-S、DiT-XL/2和ViT-B。&lt;h4&gt;结论&lt;/h4&gt;通过优化的BLR方法和自定义Triton内核，可以在保持模型精度的同时显著提高内存效率和计算速度。&lt;h4&gt;翻译&lt;/h4&gt;最近基于Transformer的基础模型的进展使它们成为许多任务的默认选择，但它们快速增长的大小使得在单个GPU上完整运行模型变得越来越困难，且计算成本过高。块低秩(BLR)压缩技术通过学习权重矩阵的紧凑表示来应对这一挑战。虽然传统低秩(LR)方法通常会导致精度急剧下降，但Monarch和BLAST等BLR方法能更好地捕捉底层结构，从而在减少计算和内存占用的同时保持精度。在这项工作中，我们使用roofline分析表明，尽管BLR方法在单令牌推理中实现了理论节省和实际加速，但多令牌推理在实践中往往成为内存限制，尽管在PyTorch中进行了编译器级别的优化。为解决这一问题，我们为Monarch和BLAST引入了带有部分融合和内存布局优化的自定义Triton内核。在内存受限的NVIDIA GPU（如Jetson Orin Nano和A40）上，我们的内核相比使用CUDA后端和编译器级优化的PyTorch密集基线，实现了高达3.76倍的加速和3倍模型大小压缩，同时支持包括Llama-7/1B、GPT2-S、DiT-XL/2和ViT-B在内的各种模型。我们的代码可在https://github.com/pabillam/mem-efficient-blr获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\times$ speedups and $3\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .</description>
      <author>example@mail.com (Pierre Abillama, Changwoo Lee, Juechu Dong, David Blaauw, Dennis Sylvester, Hun-Seok Kim)</author>
      <guid isPermaLink="false">2512.20861v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform</title>
      <link>http://arxiv.org/abs/2512.20761v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)在预测方面具有变革性能力，但同时也引发了评估危机，主要源于信息泄露和全局模式的非法转移。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型能够学习共享的时间动态，这是它们的主要优势。然而，它们在历史档案上的评估往往允许利用观察到的全局冲击，这违反了有效基准测试所需的独立性。&lt;h4&gt;目的&lt;/h4&gt;引入TS-Arena平台，通过将真正未知的未来作为 definitive 测试环境，恢复预测的操作完整性。&lt;h4&gt;方法&lt;/h4&gt;通过在实时数据流上实施预注册机制，确保评估目标在推理期间物理上不存在，从而强制执行严格的全局时间分割。这种方法建立了一个移动的时间前沿，防止历史污染并提供模型泛化的真实评估。&lt;h4&gt;主要发现&lt;/h4&gt;时间序列基础模型的评估存在问题，包括不同模型间训练和测试集重叠导致的信息泄露，以及全局模式向测试数据的非法转移。&lt;h4&gt;结论&lt;/h4&gt;TS-Arena平台为在真实世界约束下比较基础模型提供了可持续的基础设施，最初应用于能源部门，确保了评估的完整性和真实性。&lt;h4&gt;翻译&lt;/h4&gt;虽然时间序列基础模型(TSFMs)为预测提供了变革性能力，但它们同时也可能引发根本性的评估危机。这种危机是由不同模型间训练和测试集重叠导致的信息泄露，以及全局模式向测试数据的非法转移所驱动的。虽然学习共享时间动态的能力代表了这些模型的主要优势，但它们在历史档案上的评估往往允许利用观察到的全局冲击，这违反了有效基准测试所需的独立性。我们引入了TS-Arena平台，它通过将真正未知的未来作为 definitive 测试环境，恢复了预测的操作完整性。通过在实时数据流上实施预注册机制，该平台确保评估目标在推理期间物理上不存在，从而强制执行严格的全局时间分割。这种方法建立了一个移动的时间前沿，防止历史污染并提供模型泛化的真实评估。最初应用于能源部门，TS-Arena为在真实世界约束下比较基础模型提供了可持续的基础设施。该平台的原型可在 https://huggingface.co/spaces/DAG-UPB/TS-Arena 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.</description>
      <author>example@mail.com (Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Henrik Albers, Oliver Müller)</author>
      <guid isPermaLink="false">2512.20761v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</title>
      <link>http://arxiv.org/abs/2512.20605v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'内部RL'的方法，通过在自回归模型的内部表示中进行动作探索，解决了逐个标记采样导致的学习效率低下问题，特别是在奖励稀疏的情况下。&lt;h4&gt;背景&lt;/h4&gt;大型自回归模型通过下一词预测预训练并使用强化学习微调，在许多问题领域取得了前所未有的成功。然而，在RL过程中，逐个标记采样动作可能导致学习效率低下，特别是在奖励稀疏的情况下。&lt;h4&gt;目的&lt;/h4&gt;克服逐个标记采样导致的学习效率低下问题，特别是在奖励稀疏的情况下。&lt;h4&gt;方法&lt;/h4&gt;在自回归模型的内部表示中进行动作探索；引入一个高阶非因果序列模型，其输出控制基础自回归模型的残差流激活；该高阶模型学习将长激活序列块压缩到内部控制器上。&lt;h4&gt;主要发现&lt;/h4&gt;在具有层次结构的网格世界和基于MuJoCo的任务中，高阶模型学习将长激活序列块压缩到内部控制器上；每个控制器执行一系列行为上有意义的动作，这些动作在长时间尺度上展开，并带有学习到的终止条件；通过组合多个控制器可以实现在新任务上的高效探索；'内部RL'在标准RL微调失败的情况下，能够从稀疏奖励中学习。&lt;h4&gt;结论&lt;/h4&gt;自回归模型中的潜在动作生成和强化具有优势；'内部RL'是在基础模型中实现层次化RL的有前途的方向。&lt;h4&gt;翻译&lt;/h4&gt;大型基于下一词预测预训练并通过强化学习(RL)微调的自回归模型在许多问题领域取得了前所未有的成功。在RL期间，这些模型通过一次生成一个新标记来进行探索。然而，逐个标记采样动作可能导致学习效率低下，特别是在奖励稀疏的情况下。在这里，我们表明通过在自回归模型的内部表示中进行动作和探索可以克服这个问题。具体来说，为了发现时间上抽象的动作，我们引入了一个高阶非因果序列模型，其输出控制基础自回归模型的残差流激活。在具有层次结构的网格世界和基于MuJoCo的任务中，我们发现高阶模型学习将长激活序列块压缩到内部控制器上。关键的是，每个控制器执行一系列行为上有意义的动作，这些动作在长时间尺度上展开，并带有学习到的终止条件，使得随时间组合多个控制器能够在新任务上实现高效探索。我们表明，直接内部控制器强化，这一过程我们称为'内部RL'，能够在标准RL微调失败的情况下从稀疏奖励中学习。我们的结果证明了自回归模型中潜在动作生成和强化的优势，表明内部RL是在基础模型中实现层次化RL的有前途的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.</description>
      <author>example@mail.com (Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherrer, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento)</author>
      <guid isPermaLink="false">2512.20605v2</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models</title>
      <link>http://arxiv.org/abs/2512.19083v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by the IEEE Internet of Things Journal (IoT-J) for publication in the Special Issue on "Augmented Edge Sensing Intelligence for Low-Altitude IoT Systems"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoDrone是一个创新的云-边缘-端协同计算框架，将基础模型集成到自主无人机导航中，通过灰度图像、深度估计和一维占据网格导航方法解决计算资源限制问题，并利用深度强化学习神经调度器和视觉语言交互模块提升导航性能。&lt;h4&gt;背景&lt;/h4&gt;无人机自主导航面临机载计算资源有限的挑战，限制了深度神经网络的复杂度；任务卸载到边缘服务器会导致高延迟，造成系统设计中的固有权衡。&lt;h4&gt;目的&lt;/h4&gt;解决资源受限无人机平台的性能问题，提出CoDrone框架作为首个将基础模型集成到自主无人机巡航场景中的协同计算框架。&lt;h4&gt;方法&lt;/h4&gt;CoDrone使用灰度图像减少计算开销；利用Depth Anything V2进行深度估计；引入一维占据网格导航方法实现细粒度场景理解；采用基于深度强化学习的神经调度器集成深度估计与导航决策；开发无人机特定的视觉语言交互模块实现云基础模型与无人机间的有效交互。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明CoDrone在不同飞行速度和网络条件下优于基线方法，平均飞行距离增加40%，平均导航质量提高5%。&lt;h4&gt;结论&lt;/h4&gt;CoDrone框架有效解决了无人机自主导航中的计算资源限制问题，通过协同计算和基础模型集成提升了性能，新方法和模块增强了环境感知、导航决策和复杂场景推理能力。&lt;h4&gt;翻译&lt;/h4&gt;无人机自主导航面临来自有限机载计算资源的关键挑战，这限制了部署的深度神经网络只能采用浅层架构，无法处理复杂环境。将任务卸载到远程边缘服务器会引入高延迟，造成系统设计中的固有权衡。为解决这些限制，我们提出了CoDrone——首个将基础模型集成到自主无人机巡航场景中的云-边缘-端协同计算框架——有效利用基础模型提升资源受限无人机平台的性能。为减少机载计算和数据传输开销，CoDrone对导航模型使用灰度图像。当需要增强环境感知时，CoDrone利用边缘辅助的基础模型Depth Anything V2进行深度估计，并引入了一种新颖的一维占据网格导航方法——在提升效率和表示简单性的同时实现细粒度的场景理解。CoDrone的一个关键组件是基于深度强化学习的神经调度器，将深度估计与自主导航决策无缝集成，实现对动态环境的实时适应。此外，该框架引入了无人机特定的视觉语言交互模块，融入领域定制的低级飞行原语，实现云基础模型与无人机之间的有效交互。VLM的引入增强了在复杂未见场景中的开集推理能力。实验结果表明，CoDrone在不同飞行速度和网络条件下优于基线方法，实现了平均飞行距离增加40%和平均导航质量提高5%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.</description>
      <author>example@mail.com (Pengyu Chen, Tao Ouyang, Ke Luo, Weijie Hong, Xu Chen)</author>
      <guid isPermaLink="false">2512.19083v2</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation</title>
      <link>http://arxiv.org/abs/2512.21204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SpidR-Adapt的新方法，用于在极少量的未标记数据下快速适应新语言。该方法通过元学习框架和多任务自适应预训练协议实现，并提出了首阶双层优化解决方案来降低计算成本。实验表明，该方法比标准训练提高了一百多倍的数据效率。&lt;h4&gt;背景&lt;/h4&gt;人类婴儿仅通过几百小时的语音暴露就能获取新语言的基本单位，这与需要大量数据的自监督语音模型之间存在显著效率差距。现有自监督语音模型在数据效率方面远不如人类婴儿的语言学习能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有自监督语音模型与人类婴儿语言学习能力之间的效率差距，开发一种能够在极少未标记数据下快速适应新语言的方法。&lt;h4&gt;方法&lt;/h4&gt;将低资源语音表征学习构造成元学习问题；构建多任务自适应预训练协议，将适应过程表述为双层优化框架；提出首阶双层优化解决方案避免沉重计算成本；通过交错监督稳定元训练，交替使用自监督和监督目标进行鲁棒初始化。&lt;h4&gt;主要发现&lt;/h4&gt;SpidR-Adapt在音素区分度和口语语言建模方面取得快速提升；在目标语言音频训练时间不足1小时的情况下超过了领域内语言模型的性能；比标准训练方法提高了一百多倍的数据效率；为生物启发、数据高效的表征提供了实用且与架构无关的路径。&lt;h4&gt;结论&lt;/h4&gt;SpidR-Adapt方法成功地缩小了人类婴儿语言学习能力与现有自监督语音模型之间的效率差距，通过创新的元学习框架和优化策略，实现了在极少数据下的高效语言适应。相关代码和模型已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;人类婴儿仅通过几百小时的语音暴露就能获取新语言的基本单位，这凸显了与数据饥渴的自监督语音模型相比的惊人效率差距。为解决这一差距，本文介绍了SpidR-Adapt，用于使用最少的未标记数据快速适应新语言。我们将这种低资源语音表征学习构造成元学习问题，并构建了多任务自适应预训练协议，该协议将适应过程表述为双层优化框架。为了在此框架下实现可扩展的元训练，我们提出了一种新颖的启发式解决方案，即首阶双层优化，避免了沉重的计算成本。最后，我们通过交错监督使用鲁棒初始化来稳定元训练，该方法交替使用自监督和监督目标。实验表明，SpidR-Adapt在音素区分度和口语语言建模方面取得快速提升，在目标语言音频训练时间不足1小时的情况下，超过了领域内语言模型的性能，比标准训练提高了100倍以上的数据效率。这些发现强调了实现生物启发、数据高效表征的一种实用、与架构无关的路径。我们在GitHub开源了训练代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.</description>
      <author>example@mail.com (Mahi Luthra, Jiayi Shen, Maxime Poli, Angelo Ortiz, Yosuke Higuchi, Youssef Benchekroun, Martin Gleize, Charles-Eric Saint-James, Dongyan Lin, Phillip Rust, Angel Villar, Surya Parimi, Vanessa Stark, Rashel Moritz, Juan Pino, Yann LeCun, Emmanuel Dupoux)</author>
      <guid isPermaLink="false">2512.21204v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation</title>
      <link>http://arxiv.org/abs/2512.21133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 7 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SparScene是一种稀疏图学习框架，专为高效可扩展的交通场景表示而设计，解决了现有方法在大规模复杂交通场景中的效率问题。&lt;h4&gt;背景&lt;/h4&gt;多智能体轨迹生成是自动驾驶和智能交通系统的核心问题，但在复杂场景中高效建模众多道路用户和基础设施之间的动态交互仍然是一个开放性问题。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一种高效且可扩展的交通场景表示框架，解决大规模复杂交通场景中的轨迹生成问题。&lt;h4&gt;方法&lt;/h4&gt;SparScene利用车道图拓扑在智能体和车道之间构建结构感知的稀疏连接，采用轻量级图编码器高效聚合智能体-地图和智能体-智能体交互，产生紧凑的场景表示。&lt;h4&gt;主要发现&lt;/h4&gt;SparScene在Waymo开放运动数据集的运动预测基准测试中取得了具有竞争力的性能和显著的效率，能在5毫秒内为200多个智能体生成轨迹，使用2.9GB GPU内存，推理时间仅为54毫秒，可扩展到5000多个智能体和17000多条车道。&lt;h4&gt;结论&lt;/h4&gt;SparScene通过稀疏图学习框架实现了高效且可扩展的交通场景表示，在大规模交通场景中展现出卓越的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;多智能体轨迹生成是自动驾驶和智能交通系统的核心问题。然而，在复杂场景中高效建模众多道路用户和基础设施之间的动态交互仍然是一个开放性问题。现有方法通常采用基于距离或全连接密集图结构来捕获交互信息，这不仅引入了大量冗余边，还需要复杂且高度参数化的网络进行编码，从而导致训练和推理效率低下，限制了在大规模复杂交通场景中的可扩展性。为了克服现有方法的局限性，我们提出了SparScene，一种专为高效可扩展的交通场景表示而设计的稀疏图学习框架。SparScene不依赖于距离阈值，而是利用车道图拓扑在智能体和车道之间构建结构感知的稀疏连接，实现了高效且信息丰富的场景图表示。SparScene采用轻量级图编码器，有效聚合智能体-地图和智能体-智能体交互，产生紧凑的场景表示，显著提高了效率和可扩展性。在Waymo开放运动数据集的运动预测基准测试中，SparScene取得了具有竞争力的性能和显著的效率。它能在5毫秒内为场景中200多个智能体生成轨迹，仅使用2.9GB GPU内存，推理时间仅为54毫秒，可扩展到5000多个智能体和17000多条车道，突出了其在大规模交通场景中的卓越可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.</description>
      <author>example@mail.com (Xiaoyu Mo, Jintian Ge, Zifan Wang, Chen Lv, Karl Henrik Johansson)</author>
      <guid isPermaLink="false">2512.21133v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication</title>
      <link>http://arxiv.org/abs/2512.20739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures. Full research article with MATLAB and NS-3 simulations&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于人工智能的绿色认知无线电网络框架，通过结合多种先进技术优化6G网络的能效和性能。&lt;h4&gt;背景&lt;/h4&gt;6G无线通信需要实现Tb/s峰值数据率、亚毫秒级延迟和大规模物联网/车辆连接，要求可持续的空中音频接入和节能功能。认知无线电网络虽能缓解频谱稀缺问题，但传统方法能耗高且对频谱变化敏感。&lt;h4&gt;目的&lt;/h4&gt;开发一个以人工智能驱动的绿色CRN框架，通过优化组合感知时间表、发射功率、带宽分配和RIS相位选择，提高网络能效和性能。&lt;h4&gt;方法&lt;/h4&gt;该框架整合了深度强化学习与迁移学习、能量收集、可重构智能表面(RIS)以及轻量级遗传优化操作，实现智能资源分配和能效优化。&lt;h4&gt;主要发现&lt;/h4&gt;与传统方法相比，该框架减少了25-30%的能量消耗，感知AUC超过0.90，数据包交付率(PDR)提高了6-13个百分点。&lt;h4&gt;结论&lt;/h4&gt;该集成框架可轻松扩展到大型物联网和车辆应用，为6G认知无线电网络提供了可行且可持续的发展路径。&lt;h4&gt;翻译&lt;/h4&gt;6G无线通信旨在实现Tb/s的峰值数据速率、亚毫秒级延迟以及大规模物联网/车辆连接，这要求可持续的空中音频接入和节能功能。认知无线电网络(CRNs)有助于缓解频谱稀缺问题，但传统的感知和分配方法仍然能耗高，且对快速频谱变化敏感。我们的框架以人工智能驱动的绿色CRN为核心，旨在将深度强化学习(DRL)与迁移学习、能量收集(EH)、可重构智能表面(RIS)以及其他轻量级遗传优化操作相结合，优化组合感知时间表、发射功率、带宽分配和RIS相位选择。与两个基线相比(密集负载下的MATLAB + NS-3、固定策略下的传统CRN、启发式资源分配下的混合CRN)，该框架使用了(25-30%)更少的能量储备，感知AUC大于0.90，PDR提高了6-13个百分点。该集成框架可轻松扩展到大型物联网和车辆应用，为6G CRNs提供了可行且可持续的发展路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 6G wireless aims at the Tb/s peak data rates are expected, a sub-millisecond latency, massive Internet of Things/vehicle connectivity, which requires sustainable access to audio over the air and energy-saving functionality. Cognitive Radio Networks CCNs help in alleviating the problem of spectrum scarcity, but classical sensing and allocation are still energy-consumption intensive, and sensitive to rapid spectrum variations. Our framework which centers on AI driven green CRN aims at integrating deep reinforcement learning (DRL) with transfer learning, energy harvesting (EH), reconfigurable intelligent surfaces (RIS) with other light-weight genetic refinement operations that optimally combine sensing timelines, transmit power, bandwidth distribution and RIS phase selection. Compared to two baselines, the utilization of MATLAB + NS-3 under dense loads, a traditional CRN with energy sensing under fixed policies, and a hybrid CRN with cooperative sensing under heuristic distribution of resource, there are (25-30%) fewer energy reserves used, sensing AUC greater than 0.90 and +6-13 p.p. higher PDR. The integrated framework is easily scalable to large IoT and vehicular applications, and it provides a feasible and sustainable roadmap to 6G CRNs.  Index Terms--Cognitive Radio Networks (CRNs), 6G, Green Communication, Energy Efficiency, Deep Reinforcement Learning (DRL), Spectrum Sensing, RIS, Energy Harvesting, QoS, IoT.</description>
      <author>example@mail.com (Anshul Sharma, Shujaatali Badami, Biky Chouhan, Pushpanjali Pandey, Brijeena Rana, Navneet Kaur)</author>
      <guid isPermaLink="false">2512.20739v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>PUFM++: Point Cloud Upsampling via Enhanced Flow Matching</title>
      <link>http://arxiv.org/abs/2512.20988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PUFM++，一种增强的流匹配框架，用于从稀疏、噪声和部分观测中重建密集且准确的点云。该框架在几何保真度、输入鲁棒性和下游任务一致性三方面进行了改进。&lt;h4&gt;背景&lt;/h4&gt;生成模型在高质量点云上采样方面已显示出强大潜力，但仍存在改进空间。&lt;h4&gt;目的&lt;/h4&gt;开发一个增强的流匹配框架PUFM++，用于从稀疏、噪声和部分观测中重建密集且准确的点云。&lt;h4&gt;方法&lt;/h4&gt;PUFM++引入了两阶段流匹配策略，首先学习从稀疏输入到密集目标的直接流，然后使用噪声扰动样本细化；提出数据驱动的自适应时间调度器提高采样效率；施加流形约束确保生成点与底层表面一致；集成循环接口网络增强层次特征交互。&lt;h4&gt;主要发现&lt;/h4&gt;在合成基准和真实世界扫描上的广泛实验表明，PUFM++在点云上采样方面建立了新的最先进水平，在各种任务中提供了卓越的视觉保真度和定量准确性。&lt;h4&gt;结论&lt;/h4&gt;PUFM++通过三个关键方面的改进（几何保真度、输入鲁棒性和下游任务一致性）显著提升了点云上采样质量，代码和预训练模型已公开。&lt;h4&gt;翻译&lt;/h4&gt;生成建模的最新进展已显示出高质量点云上采样的强大潜力。在这项工作中，我们提出了PUFM++，一种增强的流匹配框架，用于从稀疏、噪声和部分观测中重建密集且准确的点云。PUFM++在三个关键方面改进了流匹配：(i) 几何保真度，(ii) 对不完美输入的鲁棒性，以及(iii) 与下游基于表面的任务的一致性。我们引入了一种两阶段流匹配策略，首先学习从稀疏输入到密集目标的直接直线路径流，然后使用噪声扰动样本对其进行细化，以更好地逼近终端边际分布。为了加速和稳定推理，我们提出了一种数据驱动的自适应时间调度器，基于插值行为提高采样效率。我们在采样过程中进一步施加流形约束，以确保生成的点与底层表面保持一致。最后，我们集成了循环接口网络(RIN)以增强层次特征交互并提高重建质量。在合成基准和真实世界扫描上的广泛实验表明，PUFM++在点云上采样方面建立了新的最先进水平，在各种任务中提供了卓越的视觉保真度和定量准确性。代码和预训练模型可在https://github.com/Holmes-Alan/Enhanced_PUFM公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in generative modeling have demonstrated strong promise for high-quality point cloud upsampling. In this work, we present PUFM++, an enhanced flow-matching framework for reconstructing dense and accurate point clouds from sparse, noisy, and partial observations. PUFM++ improves flow matching along three key axes: (i) geometric fidelity, (ii) robustness to imperfect input, and (iii) consistency with downstream surface-based tasks. We introduce a two-stage flow-matching strategy that first learns a direct, straight-path flow from sparse inputs to dense targets, and then refines it using noise-perturbed samples to approximate the terminal marginal distribution better. To accelerate and stabilize inference, we propose a data-driven adaptive time scheduler that improves sampling efficiency based on interpolation behavior. We further impose on-manifold constraints during sampling to ensure that generated points remain aligned with the underlying surface. Finally, we incorporate a recurrent interface network~(RIN) to strengthen hierarchical feature interactions and boost reconstruction quality. Extensive experiments on synthetic benchmarks and real-world scans show that PUFM++ sets a new state of the art in point cloud upsampling, delivering superior visual fidelity and quantitative accuracy across a wide range of tasks. Code and pretrained models are publicly available at https://github.com/Holmes-Alan/Enhanced_PUFM.</description>
      <author>example@mail.com (Zhi-Song Liu, Chenhang He, Roland Maier, Andreas Rupp)</author>
      <guid isPermaLink="false">2512.20988v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective</title>
      <link>http://arxiv.org/abs/2512.20770v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OccuFly，第一个基于相机的真实世界空中语义场景完成基准数据集，解决了空中场景中SSC研究的不足，并提出了一种无需LiDAR的数据生成框架。&lt;h4&gt;背景&lt;/h4&gt;语义场景完成(SSC)对移动机器人3D感知至关重要，在地面领域已被广泛研究，但在空中场景探索不足。LiDAR作为主要数据生成模态，对无人机存在飞行规定、质量和能量限制以及点云稀疏性等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决空中场景中SSC研究的局限性，创建基于相机的真实世界空中SSC基准数据集，提出减少手动3D标注工作量的LiDAR-free数据生成框架。&lt;h4&gt;方法&lt;/h4&gt;引入OccuFly基准数据集，在50m、40m和30米高度采集，涵盖四季，包括城市、工业和农村场景，提供22个语义类别。提出基于相机模态的数据生成框架，利用传统3D重建将2D掩码提升到点云中实现自动化标签转移。&lt;h4&gt;主要发现&lt;/h4&gt;在OccuFly上对最先进方法进行了基准测试，强调了高处视角特有的挑战，为空中3D场景理解提供了全面的视觉基准。&lt;h4&gt;结论&lt;/h4&gt;OccuFly填补了空中场景中SSC研究的空白，基于相机的方法为无人机提供了避开LiDAR局限性的实用解决方案，该基准和框架将促进空中3D场景理解领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;语义场景完成(SSC)对移动机器人中的3D感知至关重要，它通过联合估计密集体积占用率和体素级语义来实现整体场景理解。尽管SSC已在地面领域（如自动驾驶）中得到广泛研究，但像自主飞行这样的空中场景仍 largely 未被探索，从而限制了下游应用的进展。此外，LiDAR传感器代表SSC数据生成的主要模态，由于飞行规定、质量和能量限制，以及从高处视角获取的LiDAR点云的稀疏性，这对大多数无人机(UAVs)构成了挑战。为解决这些限制，我们引入了OccuFly，这是第一个基于相机的真实世界空中SSC基准，在春季、夏季、秋季和冬季分别在50米、40米和30米的高度采集。OccuFly涵盖城市、工业和农村场景，提供22个语义类别，数据格式遵循既定惯例，便于与现有研究的无缝集成。重要的是，我们提出了一个基于相机模态的LiDAR-free数据生成框架，现代无人机普遍采用这种模态。利用传统的3D重建，我们的框架通过将部分标注的2D掩码提升到重建的点云中来自动化标签转移，从而显著减少了手动3D标注的工作量。最后，我们在OccuFly上对最先进的方法进行了基准测试，并强调了高处视角特有的挑战，为全面的空中3D场景理解提供了一个全面的视觉基准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决空中视角（如无人机飞行）的语义场景完成(SSC)缺乏专用基准数据集的问题。这个问题很重要，因为现有SSC研究主要集中在地面场景，而空中场景理解对自主飞行至关重要；同时，传统基于LiDAR的数据生成方法在无人机应用中面临飞行限制、质量和能量约束以及点云稀疏性等问题，限制了相关技术的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有SSC数据集的局限性，特别是LiDAR在无人机应用中的挑战，然后设计了一个基于相机模态的数据生成框架。他们借鉴了传统3D重建技术(SfM和MVS)、现有SSC数据集的组织结构以及深度估计方法。创新点在于设计了高效的标注转移策略，只需标注少量图像(&lt;10%)，通过2D-3D对应关系将语义标签自动提升到3D点云，大幅减少了标注工作量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用相机而非LiDAR作为主要传感器，通过'少量标注，自动扩展'策略解决空中场景的语义场景完成问题。整体流程包括：1)使用地理参考图像进行3D重建，生成点云和深度图；2)对少量图像进行手动语义标注，然后通过2D-3D对应关系将标签提升到3D点云；3)将语义类分为实例类、地面类和其他类，分别采用DBSCAN聚类、泊松表面重建和直接体素化进行处理；4)通过视锥体裁剪提取每帧地面真值，并构建二进制掩码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个空中视角的SSC基准数据集，包含9个场景、20,000+样本和22个语义类别；2)LiDAR-free数据生成框架，完全基于相机模态；3)高效的标注转移方法，只需标注&lt;10%图像即可覆盖99%的3D点；4)全面的实验评估和针对空中场景的深度估计模型。相比之前工作，不同之处在于专注于空中视角而非地面视角，采用相机而非LiDAR作为主要传感器，提供了更大规模和更多样化的数据，并揭示了空中视角与地面视角的差异和挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OccuFly引入了首个基于相机的空中视角语义场景完成基准数据集，通过创新的LiDAR-free数据生成框架和高效的标注转移方法，大幅减少了3D标注工作量，为空中3D场景理解研究提供了重要资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.</description>
      <author>example@mail.com (Markus Gross, Sai B. Matha, Aya Fahmy, Rui Song, Daniel Cremers, Henri Meess)</author>
      <guid isPermaLink="false">2512.20770v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>SegMo: Segment-aligned Text to 3D Human Motion Generation</title>
      <link>http://arxiv.org/abs/2512.21237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The IEEE/CVF Winter Conference on Applications of Computer Vision 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SegMo是一种新的分段对齐的文本条件人体运动生成框架，通过将文本和运动分解为语义连贯的片段并使用对比学习进行对齐，实现了细粒度的文本-运动对应关系。&lt;h4&gt;背景&lt;/h4&gt;从文本描述生成3D人体运动是视频游戏、虚拟现实和增强现实中的重要研究问题。现有方法在序列级别对齐文本和运动，忽略了模态的内部语义结构。&lt;h4&gt;目的&lt;/h4&gt;提出SegMo框架，实现细粒度的文本-运动对齐，以改进现有的文本到3D人体运动生成方法。&lt;h4&gt;方法&lt;/h4&gt;SegMo包含三个模块：(1)文本分段提取，将复杂描述分解为时序短语；(2)运动分段提取，将运动序列划分为对应片段；(3)细粒度文本-运动对齐，使用对比学习对齐文本和运动片段。&lt;h4&gt;主要发现&lt;/h4&gt;SegMo在两个广泛使用的数据集上改进了强基线，在HumanML3D测试集上实现了0.553的改进TOP 1分数。&lt;h4&gt;结论&lt;/h4&gt;由于学习了文本和运动片段的共享嵌入空间，SegMo也可应用于运动定位和运动到文本检索等检索式任务。&lt;h4&gt;翻译&lt;/h4&gt;从文本描述生成3D人体运动是一个重要的研究问题，在视频游戏、虚拟现实和增强现实中有广泛应用。最近的方法在序列级别对齐文本描述和人体运动，忽略了模态的内部语义结构。然而，运动描述和运动序列都可以自然地分解为更小的语义连贯的片段，这些片段可以作为原子对齐单元来实现更细粒度的对应关系。受此启发，我们提出了SegMo，一种新的分段对齐的文本条件人体运动生成框架，以实现细粒度的文本-运动对齐。我们的框架包含三个模块：(1)文本分段提取，将复杂的文本描述分解为时序短语，每个短语代表一个简单的原子动作；(2)运动分段提取，将完整的运动序列划分为相应的运动片段；(3)细粒度文本-运动对齐，使用对比学习对齐文本和运动片段。大量实验表明，SegMo在两个广泛使用的数据集上改进了强基线，在HumanML3D测试集上实现了0.553的改进TOP 1分数。此外，由于为文本和运动片段学习了共享的嵌入空间，SegMo也可以应用于检索式任务，如运动定位和运动到文本检索。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从文本描述生成3D人体运动时文本与运动对齐不够精细的问题。现有方法只在序列级别对齐文本和运动，忽略了两种模态内部的语义结构，导致生成的运动可能出现缺失动作、重复动作或动作顺序错误等问题。这个问题在现实世界中很重要，因为3D人体运动生成在视频游戏、虚拟现实和增强现实等领域有广泛应用，而更准确、自然的运动生成能显著提升这些技术的用户体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，即只在序列级别对齐文本和运动。然后基于事件分割理论(表明人类自然将连续流分解为有意义片段)的启发，提出将文本和运动都分解为更小的语义片段作为原子对齐单元。作者借鉴了MoMask作为基础框架，使用CLIP文本编码器和对比学习技术，但创新性地设计了三个新模块：文本片段提取(利用LLMs)、运动片段提取(分割运动序列)和精细文本-运动对齐(通过对比学习实现片段级对齐)。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将文本描述和运动序列都分解为更小的、语义连贯的片段，作为原子对齐单元，实现更精细的文本-运动对齐。整体流程包括：1)文本片段提取 - 利用大型语言模型将复杂文本分解为时序有序的短语；2)运动片段提取 - 将完整运动序列分割为对应片段；3)精细文本-运动对齐 - 在共享嵌入空间中对齐文本和运动片段；4)运动生成 - 使用残差VQ-VAE和transformer生成运动序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次引入片段级对齐到文本条件的人体运动生成；2)设计了三个核心模块(文本片段提取、运动片段提取和精细对齐)；3)由于学习共享嵌入空间，支持了检索类任务应用。相比之前工作，不同之处在于：对齐粒度从序列级别提升到片段级别；明确考虑了文本和运动的内部结构；在每个样本内而非跨样本进行对比学习；扩展了模型在检索类任务的能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SegMo通过片段级对齐框架将文本和运动分解为语义片段并实现精细对齐，显著提高了从文本生成3D人体运动的准确性和自然度，同时扩展了模型在检索类任务中的应用能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.</description>
      <author>example@mail.com (Bowen Dang, Lin Wu, Xiaohang Yang, Zheng Yuan, Zhixiang Chen)</author>
      <guid isPermaLink="false">2512.21237v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer</title>
      <link>http://arxiv.org/abs/2512.21233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出UniTacHand统一表示方法，解决人类和机器人触觉数据不对齐问题，实现从人类到机器人的零样本触觉策略迁移，提高数据效率和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;触觉感知对机器人实现人类灵巧操作至关重要，特别是在视觉遮挡场景下。然而，大规模真实世界机器人触觉数据收集困难限制了其应用。&lt;h4&gt;目的&lt;/h4&gt;解决人类和机器人触觉数据间的差距，实现从人类到机器人的触觉策略迁移，提升数据效率和泛化性能。&lt;h4&gt;方法&lt;/h4&gt;1) 将人类和机器人触觉信号投影到MANO手模型的2D表面空间；2) 引入对比学习方法，仅需10分钟配对数据训练，将异构数据对齐到统一潜在空间；3) 提出UniTacHand统一表示方法桥接触觉信息差距。&lt;h4&gt;主要发现&lt;/h4&gt;1) 实现从人类到真实机器人的零样本触觉策略迁移，可推广到未见物体；2) 混合数据（人类+机器人）联合训练比仅用机器人数据获得更好性能和数据效率。&lt;h4&gt;结论&lt;/h4&gt;UniTacHand为触觉灵巧手的学习提供了通用、可扩展和数据高效的途径。&lt;h4&gt;翻译&lt;/h4&gt;触觉感知对于机器人手实现人类级别的灵巧操作至关重要，特别是在视觉遮挡的场景中。然而，其应用常常因为难以收集大规模真实世界机器人触觉数据而受到阻碍。在本研究中，我们提议使用触觉手套收集低成本的人类操作数据，用于基于触觉的机器人策略学习。人类和机器人触觉数据之间的不对齐使得将从人类数据中学到的策略迁移到机器人上变得具有挑战性。为了弥合这一差距，我们提出了UniTacHand，一种统一表示方法，用于对齐由灵巧手捕获的机器人触觉信息与从手套获得的人类手触觉信息。首先，我们将人类手和机器人手的触觉信号投影到MANO手模型的形态一致的2D表面空间上。这种统一标准化了异构数据结构，并内在地将触觉信号嵌入空间上下文。然后，我们引入了一种对比学习方法，将它们对齐到统一的潜在空间中，仅使用我们数据收集系统中10分钟的配对数据进行训练。我们的方法实现了从人类到真实机器人的零样本触觉策略迁移，并推广到预训练数据中未见过的物体。我们还证明，通过UniTacHand对混合数据（包括人类和机器人演示）进行联合训练，比仅使用机器人数据能获得更好的性能和数据效率。UniTacHand为基于触觉的灵巧手的学习铺就了一条通用、可扩展和数据高效的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tactile sensing is crucial for robotic hands to achieve human-level dexterous manipulation, especially in scenarios with visual occlusion. However, its application is often hindered by the difficulty of collecting large-scale real-world robotic tactile data. In this study, we propose to collect low-cost human manipulation data using haptic gloves for tactile-based robotic policy learning. The misalignment between human and robotic tactile data makes it challenging to transfer policies learned from human data to robots. To bridge this gap, we propose UniTacHand, a unified representation to align robotic tactile information captured by dexterous hands with human hand touch obtained from gloves. First, we project tactile signals from both human hands and robotic hands onto a morphologically consistent 2D surface space of the MANO hand model. This unification standardizes the heterogeneous data structures and inherently embeds the tactile signals with spatial context. Then, we introduce a contrastive learning method to align them into a unified latent space, trained on only 10 minutes of paired data from our data collection system. Our approach enables zero-shot tactile-based policy transfer from humans to a real robot, generalizing to objects unseen in the pre-training data. We also demonstrate that co-training on mixed data, including both human and robotic demonstrations via UniTacHand, yields better performance and data efficiency compared with using only robotic data. UniTacHand paves a path toward general, scalable, and data-efficient learning for tactile-based dexterous hands.</description>
      <author>example@mail.com (Chi Zhang, Penglin Cai, Haoqi Yuan, Chaoyi Xu, Zongqing Lu)</author>
      <guid isPermaLink="false">2512.21233v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update</title>
      <link>http://arxiv.org/abs/2512.21153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been published in the proceedings of the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ElfCore是一种28nm数字脉冲神经网络处理器，专门用于事件驱动的感官信号处理，集成了自监督学习、稀疏训练和权重更新机制，在多个任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;事件驱动的感官信号处理领域需要高效能的处理器来处理脉冲神经网络，同时减少功耗和内存需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效处理事件驱动信号、支持自监督学习和稀疏训练的神经网络处理器。&lt;h4&gt;方法&lt;/h4&gt;设计并实现ElfCore处理器，集成本地在线自监督学习引擎、动态结构化稀疏训练引擎和活动依赖的稀疏权重更新机制。&lt;h4&gt;主要发现&lt;/h4&gt;ElfCore在多个任务上表现出色，功耗降低16倍，内存需求减少3.8倍，网络容量效率提高5.9倍。&lt;h4&gt;结论&lt;/h4&gt;ElfCore通过创新的架构设计，在保持高性能的同时显著降低了功耗和内存需求，为事件驱动的感官信号处理提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了ElfCore，一种28nm数字脉冲神经网络处理器，专门用于事件驱动的感官信号处理。ElfCore首次高效集成了：(1)本地在线自监督学习引擎，能够在没有标记输入的情况下进行多层时序学习；(2)动态结构化稀疏训练引擎，支持高精度的稀疏到稀疏学习；(3)活动依赖的稀疏权重更新机制，仅基于输入活动和网络动态选择性更新权重。在手势识别、语音和生物医学信号处理等任务上的演示表明，ElfCore的性能优于最先进的解决方案，功耗降低高达16倍，芯片上内存需求减少3.8倍，网络容量效率提高5.9倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ESSERC66193.2025.11214101&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present ElfCore, a 28nm digital spiking neural network processor tailored for event-driven sensory signal processing. ElfCore is the first to efficiently integrate: (1) a local online self-supervised learning engine that enables multi-layer temporal learning without labeled inputs; (2) a dynamic structured sparse training engine that supports high-accuracy sparse-to-sparse learning; and (3) an activity-dependent sparse weight update mechanism that selectively updates weights based solely on input activity and network dynamics. Demonstrated on tasks including gesture recognition, speech, and biomedical signal processing, ElfCore outperforms state-of-the-art solutions with up to 16X lower power consumption, 3.8X reduced on-chip memory requirements, and 5.9X greater network capacity efficiency.</description>
      <author>example@mail.com (Zhe Su, Giacomo Indiveri)</author>
      <guid isPermaLink="false">2512.21153v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Encrypted Traffic Detection in Resource Constrained IoT Networks: A Diffusion Model and LLM Integrated Framework</title>
      <link>http://arxiv.org/abs/2512.21144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by IEEE Transactions on Network Science and Engineering&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DMLITE是一种集成了扩散模型和大语言模型的流量嵌入框架，用于资源受限的IoT环境中的网络流量检测。该框架通过三阶段架构实现了高准确率的流量分类，同时减少了训练时间。&lt;h4&gt;背景&lt;/h4&gt;物联网基础设施的普及和流量加密的广泛采用带来了显著挑战，特别是在具有动态流量模式、有限计算能力和严格延迟限制的环境中。&lt;h4&gt;目的&lt;/h4&gt;提出DMLITE框架，用于资源受限的IoT环境中的网络流量检测，解决动态流量模式、有限计算能力和严格延迟限制带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;DMLITE采用三阶段架构：流量视觉预处理、基于扩散的多级特征提取和LLM引导的特征优化。框架利用自监督扩散模型通过多级特征融合和代表性样本选择的对比学习来捕获加密流量中的细粒度和抽象模式，并集成了LLM来动态调整粒子群优化参数，实现双重目标函数最小化分类错误和数据分布的方差。&lt;h4&gt;主要发现&lt;/h4&gt;在USTC-TFC、ISCX-VPN和Edge-IIoTset数据集上分别实现了98.87%、92.61%和99.83%的分类准确率。与代表性深度学习模型相比，平均提高了3.7%的分类准确率，平均减少了41.9%的训练时间。&lt;h4&gt;结论&lt;/h4&gt;DMLITE框架有效地解决了资源受限的IoT环境中的网络流量检测挑战，通过结合扩散模型和LLM实现了高准确率和效率。&lt;h4&gt;翻译&lt;/h4&gt;物联网基础设施的普及和流量加密的广泛采用带来了显著挑战，特别是在具有动态流量模式、有限计算能力和严格延迟限制的环境中。本文提出了DMLITE，一种用于资源受限物联网环境中网络流量检测的扩散模型和大语言模型集成的流量嵌入框架。DMLITE通过三阶段架构克服了这些挑战，包括流量视觉预处理、基于扩散的多级特征提取和LLM引导的特征优化。具体而言，该框架利用自监督扩散模型通过多级特征融合和代表性样本选择的对比学习来捕获加密流量中的细粒度和抽象模式，从而能够用最少的标记数据快速适应新的流量模式。此外，DMLITE集成了LLM，通过实现双重目标函数来动态调整粒子群优化参数，该函数最小化分类错误和数据分布的方差。在基准数据集上的全面实验验证证实了DMLITE的有效性，在USTC-TFC、ISCX-VPN和Edge-IIoTset数据集上分别实现了98.87%、92.61%和99.83%的分类准确率。与代表性深度学习模型相比，这平均提高了3.7%的分类准确率，平均减少了41.9%的训练时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of Internet-of-things (IoT) infrastructures and the widespread adoption of traffic encryption present significant challenges, particularly in environments characterized by dynamic traffic patterns, constrained computational capabilities, and strict latency constraints. In this paper, we propose DMLITE, a diffusion model and large language model (LLM) integrated traffic embedding framework for network traffic detection within resource-limited IoT environments. The DMLITE overcomes these challenges through a tri-phase architecture including traffic visual preprocessing, diffusion-based multi-level feature extraction, and LLM-guided feature optimization. Specifically, the framework utilizes self-supervised diffusion models to capture both fine-grained and abstract patterns in encrypted traffic through multi-level feature fusion and contrastive learning with representative sample selection, thus enabling rapid adaptation to new traffic patterns with minimal labeled data. Furthermore, DMLITE incorporates LLMs to dynamically adjust particle swarm optimization parameters for intelligent feature selection by implementing a dual objective function that minimizes both classification error and variance across data distributions. Comprehensive experimental validation on benchmark datasets confirms the effectiveness of DMLITE, achieving classification accuracies of 98.87\%, 92.61\%, and 99.83\% on USTC-TFC, ISCX-VPN, and Edge-IIoTset datasets, respectively. This improves classification accuracy by an average of 3.7\% and reduces training time by an average of 41.9\% compared to the representative deep learning model.</description>
      <author>example@mail.com (Hongjuan Li, Hui Kang, Chenbang Liu, Ruolin Wang, Jiahui Li, Geng Sun, Jiacheng Wang, Shuang Liang, Shiwen Mao)</author>
      <guid isPermaLink="false">2512.21144v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment</title>
      <link>http://arxiv.org/abs/2512.20950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages Published at the SemEval-2025 workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了TriAligner系统，用于多语言和跨语言事实核查主张检索&lt;h4&gt;背景&lt;/h4&gt;错误信息迅速传播的时代，有效事实核查至关重要&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨多种语言有效检索事实核查主张的系统&lt;h4&gt;方法&lt;/h4&gt;TriAligner采用双编码器架构和对比学习，结合不同模态下的原生和英语翻译，学习不同来源的相对重要性，并使用大型语言模型进行数据预处理和增强，采用困难负采样改进表示学习&lt;h4&gt;主要发现&lt;/h4&gt;在单语言和跨语言基准上，TriAligner相比基线在检索准确性和事实核查性能上有显著改进&lt;h4&gt;结论&lt;/h4&gt;TriAligner是一种有效的多语言和跨语言事实核查主张检索方法&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了我们为SemEval-2025任务7：多语言和跨语言事实核查主张检索设计的系统。在错误信息迅速传播的时代，有效的事实核查变得越来越重要。我们引入了TriAligner，一种新颖的方法，它利用双编码器架构和对比学习，并结合不同模态下的原生和英语翻译。我们的方法通过学习不同来源在排列中的相对重要性，有效地跨多种语言检索主张。为了增强鲁棒性，我们使用大型语言模型进行高效的数据预处理和增强，同时采用困难负采样来改进表示学习。我们在单语言和跨语言基准上评估了我们的方法，显示出相比基线在检索准确性和事实核查性能上有显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.</description>
      <author>example@mail.com (Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah)</author>
      <guid isPermaLink="false">2512.20950v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Self-supervised Multiplex Consensus Mamba for General Image Fusion</title>
      <link>http://arxiv.org/abs/2512.20921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026, 9 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了SMC-Mamba框架，一种用于通用图像融合的自监督多路共识Mamba方法，通过创新的模块设计和损失函数实现了高质量的多模态图像融合，在多种任务中优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;图像融合技术整合不同模态的互补信息以生成高质量融合图像，增强下游任务如目标检测和语义分割。现有任务特定技术主要关注模态间信息整合，而通用图像融合需解决广泛任务同时提高性能而不增加复杂度。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用图像融合框架，能够在不增加计算复杂度的情况下提高多种下游任务的性能，有效整合不同模态的互补信息。&lt;h4&gt;方法&lt;/h4&gt;提出了SMC-Mamba框架，包含：1)模态无关特征增强(MAFE)模块，通过自适应门控保留细节，通过空间-通道和频率-旋转扫描增强全局表示；2)多路共识跨模态Mamba(MCCM)模块，实现专家间动态协作达成共识，高效整合多模态信息；3)双层自监督对比学习损失(BSCL)，保留高频信息同时提高下游任务性能。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法在红外-可见光、医学、多焦点、多曝光融合以及下游视觉任务中均优于最先进的图像融合算法。&lt;h4&gt;结论&lt;/h4&gt;SMC-Mamba框架通过创新的模块设计和损失函数，有效解决了通用图像融合中的关键挑战，在不增加计算复杂度的情况下实现了高性能的图像融合，为多种下游视觉任务提供了更高质量的输入。&lt;h4&gt;翻译&lt;/h4&gt;图像融合整合来自不同模态的互补信息，以生成高质量的融合图像，从而增强目标检测和语义分割等下游任务。与主要关注整合模态间信息的任务特定技术不同，通用图像融合需要解决广泛任务的同时提高性能而不增加复杂度。为此，我们提出了SMC-Mamba，一种用于通用图像融合的自监督多路共识Mamba框架。具体而言，模态无关特征增强(MAFE)模块通过自适应门控保留精细细节，并通过空间-通道和频率-旋转扫描增强全局表示。多路共识跨模态Mamba(MCCM)模块使专家之间能够动态协作，达成共识以高效整合多模态的互补信息。MCCM内的跨模态扫描进一步加强了模态间的特征交互，促进来自两个源的关键信息的无缝整合。此外，我们引入了双层自监督对比学习损失(BSCL)，它在不增加计算开销的同时保留高频信息，同时提高下游任务性能。大量实验证明，我们的方法在红外-可见光、医学、多焦点、多曝光融合以及下游视觉任务中优于最先进的图像融合算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image fusion integrates complementary information from different modalities to generate high-quality fused images, thereby enhancing downstream tasks such as object detection and semantic segmentation. Unlike task-specific techniques that primarily focus on consolidating inter-modal information, general image fusion needs to address a wide range of tasks while improving performance without increasing complexity. To achieve this, we propose SMC-Mamba, a Self-supervised Multiplex Consensus Mamba framework for general image fusion. Specifically, the Modality-Agnostic Feature Enhancement (MAFE) module preserves fine details through adaptive gating and enhances global representations via spatial-channel and frequency-rotational scanning. The Multiplex Consensus Cross-modal Mamba (MCCM) module enables dynamic collaboration among experts, reaching a consensus to efficiently integrate complementary information from multiple modalities. The cross-modal scanning within MCCM further strengthens feature interactions across modalities, facilitating seamless integration of critical information from both sources. Additionally, we introduce a Bi-level Self-supervised Contrastive Learning Loss (BSCL), which preserves high-frequency information without increasing computational overhead while simultaneously boosting performance in downstream tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art (SOTA) image fusion algorithms in tasks such as infrared-visible, medical, multi-focus, and multi-exposure fusion, as well as downstream visual tasks.</description>
      <author>example@mail.com (Yingying Wang, Rongjin Zhuang, Hui Zheng, Xuanhua He, Ke Cao, Xiaotong Tu, Xinghao Ding)</author>
      <guid isPermaLink="false">2512.20921v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>MODE: Multi-Objective Adaptive Coreset Selection</title>
      <link>http://arxiv.org/abs/2512.21152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MODE（多目标自适应数据效率）框架，该框架根据核心集选择策略对模型性能的不断演变的贡献动态结合这些策略。&lt;h4&gt;背景&lt;/h4&gt;静态方法无法适应不同训练阶段的数据选择需求。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够根据不同训练阶段动态调整选择标准的框架，以提高数据效率和模型性能。&lt;h4&gt;方法&lt;/h4&gt;MODE框架根据训练阶段调整选择标准：早期强调类别平衡，表示学习阶段强调多样性，收敛阶段强调不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;MODE实现了理论近似保证，具有高效的计算复杂度，在保持竞争力的准确率的同时，提供了对数据效用演化的可解释性见解，并减少了内存需求。&lt;h4&gt;结论&lt;/h4&gt;MODE框架通过动态调整选择策略，有效提高了数据效率和模型性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了MODE（多目标自适应数据效率）框架，该框架根据核心集选择策略对模型性能的不断演变的贡献动态结合这些策略。与静态方法不同，MODE将选择标准适应到训练阶段：早期强调类别平衡，表示学习期间强调多样性，收敛时强调不确定性。我们证明MODE实现了理论近似保证，具有高效复杂度，并在保持竞争力的准确率的同时，提供了对数据效用演化的可解释性见解。实验表明MODE减少了内存需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Mode(Multi-Objective adaptive Data Efficiency), a framework that dynamically combines coreset selection strategies based on their evolving contribution to model performance. Unlike static methods, \mode adapts selection criteria to training phases: emphasizing class balance early, diversity during representation learning, and uncertainty at convergence. We show that MODE achieves (1-1/e)-approximation with O(n \log n) complexity and demonstrates competitive accuracy while providing interpretable insights into data utility evolution. Experiments show \mode reduces memory requirements</description>
      <author>example@mail.com (Tanmoy Mukherjee, Pierre Marquis, Zied Bouraoui)</author>
      <guid isPermaLink="false">2512.21152v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends</title>
      <link>http://arxiv.org/abs/2512.21102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种用于高维多任务时间序列的统一预测框架，旨在满足云原生后端系统在高动态负载、耦合指标和并行任务条件下的预测需求。该方法通过共享编码结构、状态融合机制、跨任务结构传播模块和动态调整机制，实现了对复杂系统行为的准确预测，并在实验验证中表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;云原生后端系统在高动态负载、耦合指标和并行任务条件下运行，面临着复杂的预测挑战。这些系统需要能够处理资源竞争、链路交互和服务拓扑变化等因素形成的复杂结构模式。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一预测框架，能够处理高维多任务时间序列，为云原生后端系统提供可靠的预测能力，支持智能后端管理。&lt;h4&gt;方法&lt;/h4&gt;1. 构建共享编码结构统一表示不同的监控指标；2. 采用状态融合机制捕获不同时间尺度的趋势变化和局部扰动；3. 引入跨任务结构传播模块建模节点间的潜在依赖关系；4. 包含动态调整机制根据系统状态变化自动调节内部特征流，确保在负载突变、拓扑漂移和资源抖动情况下保持稳定预测。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多个误差指标上表现出优越性能，能够为不同运行条件下的未来状态提供更准确的表示。实验通过超参数敏感性、环境敏感性和数据敏感性分析验证了框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;统一预测框架为云原生系统中的高维、多任务和强动态环境提供了可靠的预测能力，为智能后端管理提供了必要的技术支持。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种适用于高维多任务时间序列的统一预测框架，以满足在高度动态负载、耦合指标和并行任务条件下运行的云原生后端系统的预测需求。该方法构建了一个共享编码结构，以统一方式表示多样化的监控指标，并采用状态融合机制来捕获不同时间尺度上的趋势变化和局部扰动。引入了跨任务结构传播模块来建模节点间的潜在依赖关系，使模型能够理解由资源竞争、链路交互和服务拓扑变化形成的复杂结构模式。为了增强对非平稳行为的适应性，该框架集成了动态调整机制，可根据系统状态变化自动调节内部特征流，确保在负载突变、拓扑漂移和资源抖动情况下保持稳定预测。实验评估比较了多个模型在不同指标上的表现，并通过超参数敏感性、环境敏感性和数据敏感性分析验证了框架的有效性。结果表明，所提出的方法在多个误差指标上实现了优越性能，并能针对不同运行条件提供更准确的未来状态表示。总体而言，统一预测框架为云原生系统中的高维、多任务和强动态环境提供了可靠的预测能力，为智能后端管理提供了必要的技术支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study proposes a unified forecasting framework for high-dimensional multi-task time series to meet the prediction demands of cloud native backend systems operating under highly dynamic loads, coupled metrics, and parallel tasks. The method builds a shared encoding structure to represent diverse monitoring indicators in a unified manner and employs a state fusion mechanism to capture trend changes and local disturbances across different time scales. A cross-task structural propagation module is introduced to model potential dependencies among nodes, enabling the model to understand complex structural patterns formed by resource contention, link interactions, and changes in service topology. To enhance adaptability to non-stationary behaviors, the framework incorporates a dynamic adjustment mechanism that automatically regulates internal feature flows according to system state changes, ensuring stable predictions in the presence of sudden load shifts, topology drift, and resource jitter. The experimental evaluation compares multiple models across various metrics and verifies the effectiveness of the framework through analyses of hyperparameter sensitivity, environmental sensitivity, and data sensitivity. The results show that the proposed method achieves superior performance on several error metrics and provides more accurate representations of future states under different operating conditions. Overall, the unified forecasting framework offers reliable predictive capability for high-dimensional, multi-task, and strongly dynamic environments in cloud native systems and provides essential technical support for intelligent backend management.</description>
      <author>example@mail.com (Zixiao Huang, Jixiao Yang, Sijia Li, Chi Zhang, Jinyu Chen, Chengda Xu)</author>
      <guid isPermaLink="false">2512.21102v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition</title>
      <link>http://arxiv.org/abs/2512.21064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Machine Intelligence Research (Journal Impact Factor 8.7, 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一项关于多模态人体动作理解的研究，提出了一种名为'分解与组合'的自监督框架，通过分解和组合策略有效平衡了计算效率和模型性能。&lt;h4&gt;背景&lt;/h4&gt;多模态人体动作理解是计算机视觉中的重要问题，主要挑战是如何有效利用不同模态之间的互补性，同时保持模型效率。现有方法要么使用简单的后期融合（导致大量计算开销），要么使用早期融合（效率高但性能有限）。&lt;h4&gt;目的&lt;/h4&gt;解决在效率和有效性之间平衡的困境。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为'分解与组合'的自监督多模态骨架动作表示学习框架。该方法包含两个策略：分解策略将融合的多模态特征分解为不同的单模态特征，并与相应的真实单模态特征进行对齐；组合策略整合多个单模态特征，利用它们作为自监督指导来增强多模态表示的学习。&lt;h4&gt;主要发现&lt;/h4&gt;在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD II数据集上的大量实验表明，所提出的方法在计算成本和模型性能之间取得了很好的平衡。&lt;h4&gt;结论&lt;/h4&gt;该方法成功解决了多模态人体动作理解中效率和有效性平衡的难题。&lt;h4&gt;翻译&lt;/h4&gt;多模态人体动作理解是计算机视觉中的一个重要问题，核心挑战在于有效利用不同模态之间的互补性，同时保持模型效率。然而，大多数现有方法依赖简单的后期融合来提升性能，这导致了大量的计算开销。虽然对所有模态使用共享骨干网络的早期融合是高效的，但它难以实现优异的性能。为了解决效率和有效性之间的平衡困境，我们引入了一种自监督的多模态骨架动作表示学习框架，名为'分解与组合'。分解策略将融合的多模态特征细致地分解为不同的单模态特征，随后将它们与各自的真实单模态对应特征进行对齐。另一方面，组合策略整合多个单模态特征，利用它们作为自监督指导来增强多模态表示的学习。在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD II数据集上的大量实验表明，所提出的方法在计算成本和模型性能之间取得了出色的平衡。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态骨架动作理解中有效利用不同模态之间的互补性同时保持模型效率的挑战。这个问题重要是因为骨架数据相比传统图像或视频数据有诸多优势（消除背景干扰、保护隐私、计算效率高），而多模态数据可提供互补信息提高识别准确性，在实际应用（如人机交互、视频监控）中需要高效且准确的系统，且自监督学习能克服标记数据获取困难的限制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析多模态动作理解中效率和性能的权衡问题，认识到现有方法要么后期融合（计算量大）要么早期融合（性能受限）。他们借鉴了UmURL[21]的统一编码器思想、对比学习的实例区分任务、时空解耦概念以及VICReg[45]的正则化方法。在此基础上，设计了嵌入融合策略在特征空间整合多模态信息，提出分解策略确保多模态特征包含各模态信息，以及组合策略整合单模态特征作为自监督指导增强多模态表示学习，并引入视角不变训练策略提升泛化能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过分解与组合的自监督训练策略，在嵌入融合框架下利用模态间的互补性，同时保持效率和性能。整体流程包括：1)多模态嵌入融合，将不同模态输入映射到共同嵌入空间并整合；2)时空解耦编码，将输入分为时间视图和空间视图分别处理；3)单模态特征分解，将融合特征分解为各模态特征并与真实特征对齐；4)多模态特征组合，使用后期融合组合各模态特征作为监督信号；5)训练过程，应用正则化防止模型崩溃，结合多视角数据增强视角不变性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)分解与组合训练框架，确保多模态特征包含各模态信息并直接优化多模态表示；2)嵌入融合策略，在特征空间融合多模态信息平衡效率和性能；3)时空解耦的双流学习框架，针对时空特征分别设计损失函数；4)视角不变训练策略，利用多视角数据作为无监督信号。相比之前工作，与UmURL[21]相比增加了对多模态特征的直接优化；与CrosSCLR[26]和CMD[27]相比融合策略更高效；与传统早期/后期融合相比在特征空间融合平衡了优缺点；将时空解耦应用于多模态自监督学习进一步提升特征质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了分解与组合的自监督训练框架，通过嵌入融合策略和时空解耦的双流学习，实现了高效且高性能的多模态骨架动作表示学习，在多种任务和基准数据集上达到了最先进的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal human action understanding is a significant problem in computer vision, with the central challenge being the effective utilization of the complementarity among diverse modalities while maintaining model efficiency. However, most existing methods rely on simple late fusion to enhance performance, which results in substantial computational overhead. Although early fusion with a shared backbone for all modalities is efficient, it struggles to achieve excellent performance. To address the dilemma of balancing efficiency and effectiveness, we introduce a self-supervised multimodal skeleton-based action representation learning framework, named Decomposition and Composition. The Decomposition strategy meticulously decomposes the fused multimodal features into distinct unimodal features, subsequently aligning them with their respective ground truth unimodal counterparts. On the other hand, the Composition strategy integrates multiple unimodal features, leveraging them as self-supervised guidance to enhance the learning of multimodal representations. Extensive experiments on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets demonstrate that the proposed method strikes an excellent balance between computational cost and model performance.</description>
      <author>example@mail.com (Hongsong Wang, Heng Fei, Bingxuan Dai, Jie Gui)</author>
      <guid isPermaLink="false">2512.21064v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces</title>
      <link>http://arxiv.org/abs/2512.21021v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, AAAI 2026 Workshop on New Frontiers in Information Retrieval&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种领域感知的日语文本嵌入方法，以提高日本最大C2C市场Mercari的搜索质量。通过微调、角色特定前缀和俄罗斯套娃表示学习，该方法在离线评估和在线A/B测试中均显示出显著改善。&lt;h4&gt;背景&lt;/h4&gt;C2C市场面临特殊检索挑战，包括简短模糊的查询、嘈杂的用户生成商品列表以及严格的生产约束。&lt;h4&gt;目的&lt;/h4&gt;构建一个领域感知的日语文本嵌入方法，提高Mercari平台的搜索质量。&lt;h4&gt;方法&lt;/h4&gt;基于购买驱动的查询-标题对进行微调，使用角色特定前缀建模查询-项目不对称性，应用俄罗斯套娃表示学习获得紧凑、截断鲁棒的嵌入，并通过离线评估、手动评估和在线A/B测试进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;与通用编码器相比显示一致改进，特别是用俄罗斯套娃截断替换PCA压缩时；更好处理专有名词、市场特定语义和术语重要性对齐；每用户收入和搜索流程效率有统计学显著改善，同时保持交易频率。&lt;h4&gt;结论&lt;/h4&gt;领域感知的嵌入提高了大规模相关性和效率，为更丰富的LLM时代搜索体验提供了实用基础。&lt;h4&gt;翻译&lt;/h4&gt;消费者对消费者(C2C)市场平台带来特殊的检索挑战：简短、模糊的查询；嘈杂的、用户生成的商品列表；以及严格的生产约束。本文报告了我们在Mercari（日本最大的C2C市场）构建领域感知日语文本嵌入方法的实验，以改善搜索质量。我们尝试基于购买驱动的查询-标题对进行微调，使用角色特定前缀来建模查询-项目不对称性。为满足生产约束，我们应用俄罗斯套娃表示学习来获得紧凑、截断鲁棒的嵌入。基于历史搜索日志的离线评估显示，相比强大的通用编码器有稳定提升，特别是在用俄罗斯套娃截断替换PCA压缩时改进显著。手动评估进一步突显了在专有名词、市场特定语义和术语重要性对齐方面的更好处理。此外，初步的在线A/B测试证明了每用户收入和搜索流程效率的统计学显著改善，同时保持了交易频率。结果表明，领域感知的嵌入提高了大规模的相关性和效率，并为更丰富的LLM时代搜索体验形成了实用基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Consumer-to-consumer (C2C) marketplaces pose distinct retrieval challenges: short, ambiguous queries; noisy, user-generated listings; and strict production constraints. This paper reports our experiment to build a domain-aware Japanese text-embedding approach to improve the quality of search at Mercari, Japan's largest C2C marketplace. We experimented with fine-tuning on purchase-driven query-title pairs, using role-specific prefixes to model query-item asymmetry. To meet production constraints, we apply Matryoshka Representation Learning to obtain compact, truncation-robust embeddings. Offline evaluation on historical search logs shows consistent gains over a strong generic encoder, with particularly large improvements when replacing PCA compression with Matryoshka truncation. A manual assessment further highlights better handling of proper nouns, marketplace-specific semantics, and term-importance alignment. Additionally, an initial online A/B test demonstrates statistically significant improvements in revenue per user and search-flow efficiency, with transaction frequency maintained. Results show that domain-aware embeddings improve relevance and efficiency at scale and form a practical foundation for richer LLM-era search experiences.</description>
      <author>example@mail.com (Andre Rusli, Miao Cao, Shoma Ishimoto, Sho Akiyama, Max Frenzel)</author>
      <guid isPermaLink="false">2512.21021v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design</title>
      <link>http://arxiv.org/abs/2512.20958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为ReACT-Drug的全新药物设计框架，该框架基于强化学习，能够生成具有高结合亲和力和高合成可及性的全新药物候选分子，同时保证100%的化学有效性和新颖性。&lt;h4&gt;背景&lt;/h4&gt;从头药物设计是现代药物开发的关键组成部分，但在巨大的化学空间中寻找具有合成可行性和高亲和力的候选分子仍然是一个重大挑战。传统的监督学习方法缺乏多目标优化和探索新型化学空间的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为ReACT-Drug的完全集成、目标无关的分子设计框架，该框架基于强化学习，能够克服传统方法的局限性，自动加速理性药物设计过程。&lt;h4&gt;方法&lt;/h4&gt;ReACT-Drug采用通用方法，利用ESM-2蛋白质嵌入从知识库中识别与给定目标相似的蛋白质，将对应蛋白质的已知药物配体分解以初始化基于片段的搜索空间，使代理偏向生物学相关的子空间。然后采用近端策略优化代理，引导经过ChemBERTa编码的分子通过动态的、基于反应模板的化学有效变换动作空间。&lt;h4&gt;主要发现&lt;/h4&gt;ReACT-Drug能够生成具有竞争性结合亲和力和高合成可及性的全新药物候选分子，同时确保100%的化学有效性和新颖性（根据MOSES基准测试）。&lt;h4&gt;结论&lt;/h4&gt;该架构展示了整合结构生物学、深度表示学习和化学合成规则以自动化和加速理性药物设计的潜力。&lt;h4&gt;翻译&lt;/h4&gt;从头药物设计是现代药物开发的关键组成部分，然而在巨大的化学空间中寻找具有合成可行性和高亲和力的候选分子仍然是一个重大挑战。强化学习通过实现多目标优化和新型化学空间的探索增强了这一过程——这些能力是传统监督学习方法所缺乏的。在这项工作中，我们介绍了ReACT-Drug，这是一个基于强化学习的完全集成、目标无关的分子设计框架。与需要针对特定目标进行微调的模型不同，ReACT-Drug利用通用方法，通过利用ESM-2蛋白质嵌入从知识库中识别与给定目标相似的蛋白质。此后，对应于这些蛋白质的已知药物配体被分解，以初始化基于片段的搜索空间，使代理偏向生物学相关的子空间。对于每个这样的片段，该管道采用近端策略优化代理，引导经过ChemBERTa编码的分子通过动态的、基于反应模板的化学有效变换动作空间。这导致了生成具有竞争性结合亲和力和高合成可及性的全新药物候选分子，同时根据MOSES基准测试确保100%的化学有效性和新颖性。该架构突出了整合结构生物学、深度表示学习和化学合成规则以自动化和加速理性药物设计的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; De novo drug design is a crucial component of modern drug development, yet navigating the vast chemical space to find synthetically accessible, high-affinity candidates remains a significant challenge. Reinforcement Learning (RL) enhances this process by enabling multi-objective optimization and exploration of novel chemical space - capabilities that traditional supervised learning methods lack. In this work, we introduce \textbf{ReACT-Drug}, a fully integrated, target-agnostic molecular design framework based on Reinforcement Learning. Unlike models requiring target-specific fine-tuning, ReACT-Drug utilizes a generalist approach by leveraging ESM-2 protein embeddings to identify similar proteins for a given target from a knowledge base such as Protein Data Base (PDB). Thereafter, the known drug ligands corresponding to such proteins are decomposed to initialize a fragment-based search space, biasing the agent towards biologically relevant subspaces. For each such fragment, the pipeline employs a Proximal Policy Optimization (PPO) agent guiding a ChemBERTa-encoded molecule through a dynamic action space of chemically valid, reaction-template-based transformations. This results in the generation of \textit{de novo} drug candidates with competitive binding affinities and high synthetic accessibility, while ensuring 100\% chemical validity and novelty as per MOSES benchmarking. This architecture highlights the potential of integrating structural biology, deep representation learning, and chemical synthesis rules to automate and accelerate rational drug design. The dataset and code are available at https://github.com/YadunandanRaman/ReACT-Drug/.</description>
      <author>example@mail.com (R Yadunandan, Nimisha Ghosh)</author>
      <guid isPermaLink="false">2512.20958v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Streaming Video Instruction Tuning</title>
      <link>http://arxiv.org/abs/2512.21334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Streamo是一个实时流式视频大语言模型，可作为通用交互式助手，能够执行广泛的流式视频任务，包括实时叙述、动作理解、事件标注等。&lt;h4&gt;背景&lt;/h4&gt;现有的在线视频模型主要专注于问答或字幕生成等狭窄任务，而离线视频感知模型与实时多模态助手之间存在差距。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够执行多种流式视频任务的通用交互式助手，统一处理连续视频流中的各种理解任务。&lt;h4&gt;方法&lt;/h4&gt;构建大规模指令跟随数据集Streamo-Instruct-465K，涵盖多样化时间上下文和多任务监督，通过简化流水线进行端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;Streamo在时间推理、响应交互和跨多种流式基准的广泛泛化方面表现出色，能够弥合离线视频感知模型与实时多模态助手之间的差距。&lt;h4&gt;结论&lt;/h4&gt;Streamo代表了向统一、智能的视频理解迈出的重要一步，能够处理连续视频流中的多种任务。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Streamo，一个实时流式视频大语言模型，可作为通用交互式助手。与现有专注于问答或字幕生成的在线视频模型不同，Streamo执行广泛的流式视频任务，包括实时叙述、动作理解、事件标注、时间事件定位和时间敏感问答。为了实现这种多功能性，我们构建了Streamo-Instruct-465K，这是一个专门用于流式视频理解的大规模指令跟随数据集。该数据集涵盖多样化的时间上下文和多任务监督，使异构流式任务能够进行统一训练。通过简化的流水线在指令跟随数据集上进行端到端训练后，Streamo在时间推理、响应交互和跨各种流式基准的广泛泛化方面表现出色。大量实验表明，Streamo弥合了离线视频感知模型与实时多模态助手之间的差距，在连续视频流中实现统一、智能的视频理解方面迈出了一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.</description>
      <author>example@mail.com (Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou)</author>
      <guid isPermaLink="false">2512.21334v1</guid>
      <pubDate>Thu, 25 Dec 2025 15:13:01 +0800</pubDate>
    </item>
    <item>
      <title>Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2512.17568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first two authors contributed equally. Project Website: https://kinematics-aware-diffusion-policy.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种运动学感知的模仿学习框架，通过在3D空间中保持任务、观察和动作空间的一致性表示，解决了机械臂全身控制中关节空间与任务空间不对齐的问题，提高了策略的样本效率和空间泛化能力。&lt;h4&gt;背景&lt;/h4&gt;全身机械臂控制对于涉及身体碰撞避免或身体物体交互的操作场景至关重要，但仅考虑末端执行器姿态在策略学习中是不够的。传统方法在关节空间中学习动作，但由于关节空间与3D任务空间的不一致性，增加了策略学习的复杂性，使得从有限演示中学习非线性手臂运动学变得困难。&lt;h4&gt;目的&lt;/h4&gt;解决关节空间与3D任务空间不对齐导致策略学习复杂化的问题，提高身体感知操作策略学习的样本效率和空间泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出一个运动学感知的模仿学习框架，在相同的3D空间中表示任务、观察和动作空间。使用臂体上的3D点集表示机器人的状态和动作，与3D点云观察自然对齐。基于扩散策略构建，并在扩散过程中融入运动学先验确保输出动作的运动学可行性，最后通过基于优化的全身逆运动学求解器计算关节角度命令。&lt;h4&gt;主要发现&lt;/h4&gt;仿真和真实实验结果表明，与现有方法相比，该方法在身体感知操作策略学习中具有更高的成功率和更强的空间泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过在3D空间中保持一致性表示，改善了策略的样本效率和空间泛化能力，同时实现了全身控制，为机械臂全身操作提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;对于涉及身体碰撞避免或身体物体交互的操作场景，机械臂的全身控制并了解完整手臂运动学至关重要，这使得在策略学习中仅考虑末端执行器姿态变得不足。全臂操作的典型方法是在机器人的关节空间中学习动作。然而，关节空间与实际任务空间（即3D空间）的不一致性增加了策略学习的复杂性，因为在任务空间中的泛化要求策略内在理解非线性手臂运动学，这很难从有限的演示中学习。为解决这一问题，本文提出了一种运动学感知的模仿学习框架，具有一致的、相同的3D空间中的任务、观察和动作空间。具体而言，我们使用臂体上的3D点集来表示机器人的状态和动作，自然地与3D点云观察对齐。这种空间一致性表示提高了策略的样本效率和空间泛化能力，同时实现了全身控制。基于扩散策略，我们进一步将运动学先验融入扩散过程，以确保输出动作的运动学可行性。最终的关节角度命令通过基于优化的全身逆运动学求解器计算执行。仿真和真实实验结果表明，与现有方法相比，我们的方法在身体感知操作策略学习中具有更高的成功率和更强的空间泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人手臂操作中的全臂控制问题。传统方法要么只关注末端执行器，无法处理需要整个手臂协同操作的任务（如避免碰撞或使用手臂非末端部分与物体交互）；要么在关节空间中学习，但关节空间与任务空间不一致，增加了学习复杂性。这个问题在现实中很重要，因为许多操作场景需要全臂控制，如狭窄空间中的避障或使用肘部等非末端部位进行操作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：末端执行器控制无法实现全臂控制，关节空间学习又面临空间不一致问题。他们设计了一种在3D空间中使用节点表示机器人状态和动作的方法，自然与观察空间对齐。他们借鉴了扩散模型用于动作生成的思想，特别是3D点云表示和空间对齐策略，但创新性地将运动学约束融入扩散过程，并使用节点表示实现全臂控制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 使用一组位于机器人手臂上的3D节点表示状态和动作，与3D观察空间和任务空间一致；2) 将运动学约束融入扩散模型，确保生成的节点位置可行；3) 实现全臂控制而非仅末端控制。实现流程：1) 训练时将关节角度通过正向运动学转换为3D节点；2) 在扩散过程中融入运动学约束；3) 从噪声开始迭代生成3D节点轨迹；4) 通过逆向运动学优化将节点位置转换为关节角度命令执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 3D节点表示机器人状态和动作，自然与观察空间对齐；2) 运动学感知的扩散过程，确保生成的动作可行；3) 结合全臂控制和空间对齐；4) 混合使用MLP(训练时)和优化方法(推理时)。不同之处：相比末端执行器控制方法，能实现全臂控制；相比关节空间方法，避免了空间不一致问题；相比结合笛卡尔和关节空间的方法，不需要策略隐式学习非线性运动学，简化了学习过程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种运动学感知的扩散策略，通过在一致的3D空间中将机器人状态和动作表示为3D节点，实现了高效的全臂机器人操作，同时保证了运动学可行性和空间泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whole-body control of robotic manipulators with awareness of full-arm kinematics is crucial for many manipulation scenarios involving body collision avoidance or body-object interactions, which makes it insufficient to consider only the end-effector poses in policy learning. The typical approach for whole-arm manipulation is to learn actions in the robot's joint space. However, the unalignment between the joint space and actual task space (i.e., 3D space) increases the complexity of policy learning, as generalization in task space requires the policy to intrinsically understand the non-linear arm kinematics, which is difficult to learn from limited demonstrations. To address this issue, this letter proposes a kinematics-aware imitation learning framework with consistent task, observation, and action spaces, all represented in the same 3D space. Specifically, we represent both robot states and actions using a set of 3D points on the arm body, naturally aligned with the 3D point cloud observations. This spatially consistent representation improves the policy's sample efficiency and spatial generalizability while enabling full-body control. Built upon the diffusion policy, we further incorporate kinematics priors into the diffusion processes to guarantee the kinematic feasibility of output actions. The joint angle commands are finally calculated through an optimization-based whole-body inverse kinematics solver for execution. Simulation and real-world experimental results demonstrate higher success rates and stronger spatial generalizability of our approach compared to existing methods in body-aware manipulation policy learning.</description>
      <author>example@mail.com (Kangchen Lv, Mingrui Yu, Yongyi Jia, Chenyu Zhang, Xiang Li)</author>
      <guid isPermaLink="false">2512.17568v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
  <item>
      <title>Zero-shot Reconstruction of In-Scene Object Manipulation from Video</title>
      <link>http://arxiv.org/abs/2512.19684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种从单目RGB视频中重建场景内物体操作的系统，是首个解决此类问题的方法。&lt;h4&gt;背景&lt;/h4&gt;现有方法在以手为中心的坐标系中操作并忽略场景信息，导致度量准确性不足且实用性受限。场景重建存在不适定性问题，手-物体深度关系模糊，且需要物理上合理的交互。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从单目RGB视频中重建场景内物体操作的系统，解决现有方法的局限性，提高度量准确性和实用性。&lt;h4&gt;方法&lt;/h4&gt;首先使用数据驱动的基础模型初始化核心组件（物体网格和姿态、场景点云、手部姿态），然后应用两阶段优化，从抓取到交互恢复完整的手-物体运动，保持与输入视频中观察到的场景信息一致。&lt;h4&gt;主要发现&lt;/h4&gt;通过结合场景信息和手-物体交互，可以更准确地重建物体操作，提高度量准确性。&lt;h4&gt;结论&lt;/h4&gt;提出的系统是首个解决从单目RGB视频中重建场景内物体操作问题的方法，通过结合场景信息和两阶段优化，实现了物理合理且与场景一致的交互重建。&lt;h4&gt;翻译&lt;/h4&gt;我们构建了首个解决从单目RGB视频中重建场景内物体操作问题的系统。由于场景重建不适定、手-物体深度模糊以及需要物理合理交互，这具有挑战性。现有方法在以手为中心的坐标系中操作并忽略场景，阻碍了度量准确性和实际应用。在我们的方法中，我们首先使用数据驱动的基础模型初始化核心组件，包括物体网格和姿态、场景点云以及手部姿态。然后我们应用两阶段优化，从抓取到交互恢复完整的手-物体运动，这保持与输入视频中观察到的场景信息一致。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从单目RGB视频中重建场景中的物体操作问题，即同时重建手部与物体的交互动作以及周围场景。这个问题在现实中很重要，因为机器人灵巧手策略学习需要理解物体如何与周围场景交互，AR/VR应用需要手、物体和场景之间的一致对齐以实现直观控制，而现有方法主要在手中心坐标系下操作，忽略了场景，限制了度量准确性和实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为场景重建、手-物体交互和运动检测与恢复三个子任务，每个子任务采用专门的基础模型处理，然后通过优化整合结果。他们借鉴了多个现有工作：使用HaMeR进行抓取帧手部初始化，使用HaPTIC初始化序列，使用OnePoseviaGen重建场景和物体，使用SAM2获取物体掩码，使用SpatialTrackerV2估计深度和相机参数，使用Hi3DGen/Amodal3R重建物体，使用Foundationpose获取物体姿态，使用Egoallo完成接近动作。这些基础模型提供了可靠的初始估计，然后通过优化进一步改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将手-物体运动分解为抓取和交互两个阶段，分别进行优化，同时利用基础模型初始化核心组件，并通过优化确保重建结果与输入视频中的场景信息一致。整体流程分为两个阶段：1) 初始重建阶段：使用SAM2获取物体掩码，SpatialTrackerV2重建场景，Hi3DGen/Amodal3R重建物体，Foundationpose获取物体姿态，HaPTIC提取手部轨迹；2) 手-物体运动优化阶段：先优化交互阶段，通过接触点约束、穿透约束等确保物理合理性，然后优化抓取阶段，使用Egoallo完成接近动作，确保运动平滑和场景一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个从单目RGB视频中重建场景对齐的手-物体操作的系统；2) 将问题分解为子任务并利用专业模型解决；3) 两阶段优化方法分别处理抓取和交互；4) 利用手部运动先验完成接近动作。相比之前工作，不同之处在于：现有方法主要在手中心坐标系下操作，忽略场景；只关注场景无关交互，不重建周围场景；需要每序列测试时拟合，耗时且无法跨物体/场景/动作使用单一模型；通常不处理非抓取部分，难以处理长时程操作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了首个从单目RGB视频中重建场景对齐的手-物体操作的系统，通过两阶段优化和基础模型初始化，实现了从抓取到交互的完整手-物体运动重建，同时保持与场景信息的一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.</description>
      <author>example@mail.com (Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis)</author>
      <guid isPermaLink="false">2512.19684v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</title>
      <link>http://arxiv.org/abs/2512.19687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PE-AV，一种用于音频和视频理解的新型编码器，通过扩展对比学习进行训练，实现了跨模态的联合嵌入，并在多个基准测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;现有的音频视频理解模型通常局限于单一领域或模态，缺乏统一的跨模态表示能力，难以处理多模态任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够支持多种模态（音频-视频、音频-文本、视频-文本）的统一编码器，提高跨模态理解能力和零样本性能。&lt;h4&gt;方法&lt;/h4&gt;构建了一个强大的音视频数据引擎，为约1亿个音视频对合成高质量字幕，利用十种成对对比目标进行训练，并通过帧级对比目标进行微调，实现了PE-A-Frame的开发。&lt;h4&gt;主要发现&lt;/h4&gt;统一的跨模态嵌入使语音检索等新任务成为可能；在标准音频和视频基准测试中设定了新的最先进水平；扩展跨模态和字幕类型对齐可以加强性能并提高零样本性能；多样化的音频数据（语音、音乐和一般音效）避免了单一领域限制。&lt;h4&gt;结论&lt;/h4&gt;PE-AV通过统一的跨模态嵌入和大规模监督，显著提升了音频和视频理解能力，为多模态学习提供了新的方法，并实现了细粒度的音频帧到文本对齐。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了感知编码器音视频（PE-AV），这是一个用于音频和视频理解的新型编码器家族，通过扩展对比学习进行训练。基于PE，PE-AV在将表示扩展到音频方面做出了几个关键贡献，并原生支持跨音频-视频、音频-文本和视频-文本模态的联合嵌入。PE-AV的统一跨模态嵌入使语音检索等新任务成为可能，并在标准音频和视频基准测试中设定了新的最先进水平。我们通过构建一个强大的音视频数据引擎来实现这一点，该引擎为约1亿个音视频对合成高质量字幕，实现了跨模态的大规模监督。我们的音频数据包括语音、音乐和一般音效，避免了先前工作中常见的单一领域限制。我们利用十种成对对比目标，表明扩展跨模态和字幕类型对齐可以加强性能并提高零样本性能。我们进一步通过帧级对比目标微调PE-AV开发了PE-A-Frame，实现了细粒度的音频帧到文本对齐，用于声音事件检测等任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.</description>
      <author>example@mail.com (Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu)</author>
      <guid isPermaLink="false">2512.19687v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning</title>
      <link>http://arxiv.org/abs/2512.19107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出FC-MIR框架，通过关键帧采样和自适应连接减少视觉冗余，提高多模态大语言模型在移动UI操作轨迹分析中的推理效率，同时扩展任务范围至生成预测后操作和搜索建议。&lt;h4&gt;背景&lt;/h4&gt;从移动UI操作轨迹中识别用户意图对UI理解和任务自动化代理至关重要，但多模态大语言模型在移动设备上的实时部署受限于高计算成本和低效的冗余帧处理。&lt;h4&gt;目的&lt;/h4&gt;解决MLLMs在移动设备上部署的效率问题，扩展任务范围，并评估摘要、预测和建议的实际效用。&lt;h4&gt;方法&lt;/h4&gt;提出FC-MIR框架，利用关键帧采样和自适应连接减少视觉冗余；集成闭源MLLMs或微调模型(如Qwen3-VL)进行轨迹总结和意图预测；构建包含UI-Agents和真实用户交互的UI轨迹数据集；引入细粒度指标评估实际效用。&lt;h4&gt;主要发现&lt;/h4&gt;压缩方法在50%-60%压缩率下保持性能；闭源和微调的MLLMs表现出强大的意图总结能力，支持轻量级设备端部署；但在生成有用和'令人惊讶'的建议方面仍有改进空间。&lt;h4&gt;结论&lt;/h4&gt;将框架部署在真实环境中，整合UI感知和UI-Agent代理，为该领域未来发展奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;从移动UI操作轨迹中识别用户意图对推进UI理解和实现任务自动化代理至关重要。虽然多模态大语言模型在视频理解任务上表现出色，但其实时移动部署受限于高昂的计算成本和低效的冗余帧处理。为解决这些问题，我们提出了FC-MIR框架：利用关键帧采样和自适应连接，减少视觉冗余以提高推理效率，同时集成最先进的闭源MLLMs或微调模型(如Qwen3-VL)进行轨迹总结和意图预测。我们进一步扩展任务范围，探索生成预测后操作和搜索建议，并引入细粒度指标评估摘要、预测和建议的实际效用。为严格评估，我们构建了一个覆盖UI-Agents(Agent-I)和真实用户交互(Person-I)场景的UI轨迹数据集。实验结果表明，我们的压缩方法在50%-60%的压缩率下保持性能；闭源和微调的MLLMs都表现出强大的意图总结能力，支持潜在的轻量级设备端部署。然而，MLLMs在生成有用和'令人惊讶'的建议方面仍有改进空间。最后，我们将框架部署在真实环境中，整合UI感知和UI-Agent代理，为该领域的未来发展奠定基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and "surprising" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.</description>
      <author>example@mail.com (Zhe Yang, Xiaoshuang Sheng, Zhengnan Zhang, Jidong Wu, Zexing Wang, Xin He, Shenghua Xu, Guanjing Xiong)</author>
      <guid isPermaLink="false">2512.19107v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training</title>
      <link>http://arxiv.org/abs/2512.19213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 10 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InvCoSS是一种创新的持续自监督学习框架，通过反转预训练模型生成合成图像而非重放真实数据，有效解决了医学影像学习中的隐私保护和灾难性遗忘问题，同时保持了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;持续自监督学习在医学影像中通过顺序训练基础模型避免了收集多模态图像进行联合训练的需求，保护数据隐私的同时提升下游性能。然而，现有方法大多依赖重放先前数据来防止灾难性遗忘，这损害了隐私并限制了在数据传输受限的现实场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖数据重放的持续自监督学习方法，解决医学多模态图像预训练中的数据隐私问题，在不访问先前真实数据的情况下有效缓解灾难性遗忘。&lt;h4&gt;方法&lt;/h4&gt;提出InvCoSS框架，通过反转预训练的自监督模型生成近似原始训练分布的合成图像，与新任务数据结合进行联合优化；引入具有多尺度融合架构的新型InvUNet提高合成图像保真度；设计排斥表示学习机制增加多样性并防止模式崩溃。&lt;h4&gt;主要发现&lt;/h4&gt;在九个下游任务上的实验验证了InvCoSS的有效性，性能与或优于先前数据重放方法，显著降低了存储需求，消除了数据隐私限制。&lt;h4&gt;结论&lt;/h4&gt;InvCoSS是一种有效的持续自监督学习方法，适用于医学多模态图像预训练，通过生成合成图像而非重放真实数据，解决了隐私和存储问题，提高了实际应用场景中的适用性。&lt;h4&gt;翻译&lt;/h4&gt;医学影像中的持续自监督学习通过顺序训练基础模型，缓解了收集多模态图像进行联合训练的需求，在保护数据隐私的同时为下游性能提供了有希望的改进。然而，大多数现有方法仍然依赖于重放先前阶段的数据来防止灾难性遗忘，这损害了隐私并限制了它们在数据传输通常受限的现实场景中的适用性。在这项工作中，我们提出了InvCoSS，一种用于医学多模态图像预训练的反转驱动持续自监督学习框架。具体来说，在先前任务训练后，InvCoSS反转预训练的自监督模型以生成近似原始训练分布的合成图像。然后，这些合成图像与新任务的数据结合进行联合优化，有效缓解了灾难性遗忘，同时严格遵循无法访问先前真实数据的约束。此外，为了提高合成图像的保真度，我们引入了一种具有多尺度融合架构的新型InvUNet，以恢复反转图像的高频和低频分量。为了增强多样性并防止模式崩溃，我们设计了一种排斥表示学习机制，鼓励合成图像具有多样化的特征空间而无需类别指导。在九个下游任务上的广泛实验验证了InvCoSS的有效性，其性能与甚至优于先前的数据重放方法，同时显著降低了存储需求并消除了数据隐私限制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual self-supervised learning (CSSL) in medical imaging trains a foundation model sequentially, alleviating the need for collecting multi-modal images for joint training and offering promising improvements in downstream performance while preserving data privacy. However, most existing methods still rely on replaying data from previous stages to prevent catastrophic forgetting, which compromises privacy and limits their applicability in real-world scenarios where data transfer across sites is often restricted. In this work, we propose InvCoSS, an inversion-driven continual self-supervised learning framework for medical multi-modal image pre-training. Specifically, after training on a previous task, InvCoSS inverts the pre-trained self-supervised model to generate synthetic images that approximate the original training distribution. These synthetic images are then combined with data from the new task for joint optimization, which effectively mitigates catastrophic forgetting while strictly adhering to the constraint of no access to previous real data. Furthermore, to improve the fidelity of synthetic images, we introduce a novel InvUNet with a multi-scale fusion architecture to restore both high- and low-frequency components of the inverted images. To enhance diversity and prevent mode collapse, we design a repulsive representation-learning mechanism that encourages a diverse feature space for synthetic images without class guidance. Extensive experiments across nine downstream tasks validate the effectiveness of InvCoSS, achieving performance comparable to or even superior to prior data-replay methods while significantly reducing storage requirements and eliminating data privacy constraints.</description>
      <author>example@mail.com (Zihao Luo, Shaohao Rui, Zhenyu Tang, Guotai Wang, Xiaosong Wang)</author>
      <guid isPermaLink="false">2512.19213v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Learning Through Little Eyes: Attribute Discrimination Beyond Objects</title>
      <link>http://arxiv.org/abs/2512.18951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了婴儿在生命前两年学习识别物体类别和精细属性的能力，并通过比较CVCL和CLIP模型在颜色、大小和纹理识别上的表现，揭示了视觉与语言空间之间的差距。&lt;h4&gt;背景&lt;/h4&gt;婴儿在生命的前两年内不仅学习识别物体类别，还学习识别精细的属性如颜色、大小和纹理。先前的工作探索了'婴儿视角对比学习'(CVCL)模型作为早期婴儿学习的计算模型，但仅关注类别级别的识别。&lt;h4&gt;目的&lt;/h4&gt;引入一个系统性地改变颜色、大小和纹理的基准系统，以测试类别内属性识别能力，并确定婴儿规模的学习是否支持属性区分。&lt;h4&gt;方法&lt;/h4&gt;通过引入一个系统性地改变颜色、大小和纹理的基准系统，对CVCL和CLIP模型进行受控测试和性能比较。&lt;h4&gt;主要发现&lt;/h4&gt;CVCL在大小区分方面表现更好，CLIP在颜色区分上达到更高准确性。两个模型都能在图像嵌入中表示纹理，但未能将纹理与语言联系起来。&lt;h4&gt;结论&lt;/h4&gt;婴儿规模的学习确实支持属性区分，但不同模型在不同属性上有优势。视觉和语言空间之间存在差距，这表明视觉表征和语言表征之间的不匹配。&lt;h4&gt;翻译&lt;/h4&gt;婴儿在生命的前两年不仅学习识别物体类别，还学习识别精细的属性，如颜色、大小和纹理。先前的工作探索了'婴儿视角对比学习'(CVCL)，这是一种基于婴儿第一人称视频训练的CLIP风格模型，作为早期婴儿学习的计算模型，但它只关注类别级别的识别。这使得不清楚婴儿规模的学习是否也支持属性区分。为解决这一问题，我们引入了一个基准，系统性地改变颜色、大小和纹理，允许对类别内属性识别进行受控测试。比较CVCL与CLIP显示出明显的差异。CVCL在大小区分方面表现更好，而CLIP在颜色区分上达到更高的准确性。两个模型都在图像嵌入中表示纹理，但未能将纹理与语言联系起来，这表明视觉和语言空间之间存在差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infants learn to recognize not only object categories but also fine grained attributes such as color, size, and texture within their first two years of life. Prior work explores Childs View for Contrastive Learning (CVCL), a CLIP style model trained on infant egocentric video as a computational model of early infant learning, but it focuses only on class level recognition. This leaves it unclear whether infant scale learning also supports attribute discrimination. To address this, we introduce a benchmark that systematically varies color, size, and texture, allowing controlled tests of within class attribute recognition. Comparing CVCL with CLIP shows clear differences. CVCL is better at size discrimination, while CLIP achieves higher accuracy on color discrimination. Both models represent texture in image embeddings but fail to ground texture linguistically, suggesting a gap between visual and language spaces.</description>
      <author>example@mail.com (Patrick Batsell, Tsutsui Satoshi, Bihan Wen)</author>
      <guid isPermaLink="false">2512.18951v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2512.18368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AtomSkill是一种新颖的多任务模仿学习框架，通过结构化的原子技能空间实现可组合的机器人操作，解决了单任务扩展到多任务时的挑战。&lt;h4&gt;背景&lt;/h4&gt;模仿学习在单任务机器人操作中表现出色，但扩展到多任务设置面临次优演示、轨迹噪声和行为多模态等挑战。现有基于技能的方法依赖固定长度分割或环境先验，限制了语义一致性和跨任务泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理多任务机器人操作的学习框架，提高语义一致性和跨任务泛化能力。&lt;h4&gt;方法&lt;/h4&gt;AtomSkill包含两个关键技术贡献：1) 通过夹具状态关键帧检测和视觉语言模型注释构建语义基础的原子技能库；2) 提出具有关键姿势想象的动作生成模块，共同预测技能的长时程终端关键姿势和即时动作序列。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和现实环境中的实验表明，AtomSkill在各种操作任务中始终优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;AtomSkill通过结构化的原子技能空间和创新的动作生成方法，有效解决了多任务模仿学习中的挑战，实现了更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;尽管模仿学习在单任务机器人操作中显示出令人印象深刻的结果，但将其扩展到多任务设置仍然是一个基本挑战，由于次优演示、轨迹噪声和行为多模态等问题。现有的基于技能的方法试图通过将动作分解为可重用的抽象来解决此问题，但它们通常依赖于固定长度的分割或环境先验，这限制了语义一致性和跨任务泛化能力。在这项工作中，我们提出了AtomSkill，一种新颖的多任务模仿学习框架，用于学习和利用结构化的原子技能空间进行可组合的机器人操作。我们的方法建立在两个关键技术贡献之上。首先，我们通过使用夹具状态关键帧检测和视觉语言模型注释将演示分解为可变长度的技能，构建了一个语义基础的原子技能库。对比学习目标确保 resulting skill embeddings 在语义上一致且时间上连贯。其次，我们提出了一个具有关键姿势想象的动作生成模块，共同预测技能的长时程终端关键姿势及其即时动作序列。这使得策略能够同时推理总体运动目标和精细控制，促进稳健的技能链接。在模拟和现实环境中的大量实验表明，AtomSkill 在各种操作任务中始终优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While imitation learning has shown impressive results in single-task robot manipulation, scaling it to multi-task settings remains a fundamental challenge due to issues such as suboptimal demonstrations, trajectory noise, and behavioral multi-modality. Existing skill-based methods attempt to address this by decomposing actions into reusable abstractions, but they often rely on fixed-length segmentation or environmental priors that limit semantic consistency and cross-task generalization. In this work, we propose AtomSkill, a novel multi-task imitation learning framework that learns and leverages a structured Atomic Skill Space for composable robot manipulation. Our approach is built on two key technical contributions. First, we construct a Semantically Grounded Atomic Skill Library by partitioning demonstrations into variable-length skills using gripper-state keyframe detection and vision-language model annotation. A contrastive learning objective ensures the resulting skill embeddings are both semantically consistent and temporally coherent. Second, we propose an Action Generation module with Keypose Imagination, which jointly predicts a skill's long-horizon terminal keypose and its immediate action sequence. This enables the policy to reason about overarching motion goals and fine-grained control simultaneously, facilitating robust skill chaining. Extensive experiments in simulated and real-world environments show that AtomSkill consistently outperforms state-of-the-art methods across diverse manipulation tasks.</description>
      <author>example@mail.com (Yihang Zhu, Weiqing Wang, Shijie Wu, Ye Shi, Jingya Wang)</author>
      <guid isPermaLink="false">2512.18368v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection</title>
      <link>http://arxiv.org/abs/2512.18133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by The Web Conference 2025 (WWW'25). 12 pages, includes implementation details. Code: https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiiii/WWW25-Grad&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Grad的基于关系扩散的图增强模型，用于解决金融场景中欺诈者采用自适应伪装策略导致图欺诈检测(GFD)模型效率下降的问题。该模型通过监督图对比学习增强欺诈-良性差异，并利用引导关系扩散生成器生成辅助同质性关系，从而增强微弱欺诈信号使其可被检测。实验表明，Grad模型在多个数据集上均优于现有方法，AUC和AP指标分别最多提高了11.10%和43.95%。&lt;h4&gt;背景&lt;/h4&gt;随着有组织犯罪集团变得更加专业化，金融场景中的欺诈者采用更复杂的伪装策略，通过模仿平台收集的行为数据使其关键特征与良性用户高度一致，称为'自适应伪装'。这种行为缩小了欺诈者与良性用户在行为特征上的差异，导致当前图欺诈检测(GFD)模型效率下降。&lt;h4&gt;目的&lt;/h4&gt;解决欺诈者采用自适应伪装策略导致当前GFD模型效率下降的问题，提高金融场景中的欺诈检测能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Grad的基于关系扩散的图增强模型。该模型包含两个关键组件：1)监督图对比学习模块，用于增强欺诈-良性差异；2)引导关系扩散生成器，用于从头开始生成辅助的同质性关系。通过这些组件，模型能够在聚合过程中增强微弱的欺诈信号，使其足够明显以被检测。&lt;h4&gt;主要发现&lt;/h4&gt;在两个微信支付提供的真实世界数据集(拥有数十亿用户的最大在线支付平台之一)和三个公共数据集上进行了大量实验。结果表明，Grad模型在各种场景中均优于现有的最先进方法，AUC和AP指标分别最多提高了11.10%和43.95%。&lt;h4&gt;结论&lt;/h4&gt;Grad模型能有效应对自适应伪装带来的挑战，通过增强欺诈-良性差异和生成辅助同质性关系，显著提高了金融场景中的欺诈检测性能。&lt;h4&gt;翻译&lt;/h4&gt;如今，金融场景中的图欺诈检测(GFD)已成为保护在线支付安全的紧迫研究课题。然而，随着现实场景中有组织犯罪集团变得更加专业化，欺诈者正在采用更复杂的伪装策略。具体来说，欺诈者通过模仿平台收集的行为数据来伪装自己，确保其关键特征与良性用户高度一致，我们称之为'自适应伪装'。因此，这缩小了他们与平台数据库中良性用户在行为特征上的差异，从而使当前的GFD模型效率降低。为了解决这个问题，我们提出了一个基于关系扩散的图增强模型Grad。具体而言，Grad利用监督图对比学习模块来增强欺诈-良性差异，并采用引导关系扩散生成器从头开始生成辅助的同质性关系。基于这些，在聚合过程中微弱的欺诈信号将被增强，从而变得足够明显以被捕获。已在微信支付(拥有数十亿用户的最大在线支付平台之一)提供的两个真实世界数据集和三个公共数据集上进行了大量实验。结果表明，我们提出的Grad模型在各种场景中均优于最先进的方法，AUC和AP分别最多提高了11.10%和43.95%。我们的代码已在https://github.com/AI4Risk/antifraud和https://github.com/Muyiiiii/WWW25-Grad上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3696410.3714520&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nowadays, Graph Fraud Detection (GFD) in financial scenarios has become an urgent research topic to protect online payment security. However, as organized crime groups are becoming more professional in real-world scenarios, fraudsters are employing more sophisticated camouflage strategies. Specifically, fraudsters disguise themselves by mimicking the behavioral data collected by platforms, ensuring that their key characteristics are consistent with those of benign users to a high degree, which we call Adaptive Camouflage. Consequently, this narrows the differences in behavioral traits between them and benign users within the platform's database, thereby making current GFD models lose efficiency. To address this problem, we propose a relation diffusion-based graph augmentation model Grad. In detail, Grad leverages a supervised graph contrastive learning module to enhance the fraud-benign difference and employs a guided relation diffusion generator to generate auxiliary homophilic relations from scratch. Based on these, weak fraudulent signals would be enhanced during the aggregation process, thus being obvious enough to be captured. Extensive experiments have been conducted on two real-world datasets provided by WeChat Pay, one of the largest online payment platforms with billions of users, and three public datasets. The results show that our proposed model Grad outperforms SOTA methods in both various scenarios, achieving at most 11.10% and 43.95% increases in AUC and AP, respectively. Our code is released at https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiii/WWW25-Grad.</description>
      <author>example@mail.com (Jie Yang, Rui Zhang, Ziyang Cheng, Dawei Cheng, Guang Yang, Bo Wang)</author>
      <guid isPermaLink="false">2512.18133v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Generative vector search to improve pathology foundation models across multimodal vision-language tasks</title>
      <link>http://arxiv.org/abs/2512.19360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages main (54 total), 2 main figures (9 total)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为随机潜在匹配的生成式向量搜索方法，用于解决基于嵌入的检索系统无法捕捉多概念查询复杂性的问题，特别是在生物医学等高维数据领域。&lt;h4&gt;背景&lt;/h4&gt;基于嵌入的标准检索方法无法捕捉多概念查询的复杂性，特别是在生物医学等领域，其中生物数据本质上具有高维特性，例如组学数据集和临床报告同时表现出多种分子、细胞和生理特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉多概念查询复杂性的检索增强生成方法，以提高检索性能并减少幻觉，特别是在生物医学等高维数据领域。&lt;h4&gt;方法&lt;/h4&gt;提出随机潜在匹配方法，一种生成式向量搜索方法，通过从文本或图像输入中采样查询条件嵌入来增强检索性能，类似于思维链推理使检索系统能够'更广泛搜索'。&lt;h4&gt;主要发现&lt;/h4&gt;STHLM在科学文献、临床记录和组织图像等不同基准测试中显示出比经典向量检索显著的改进，通过测试时计算将检索性能提高了10-30%，同时实现了高达10倍的嵌入维度压缩。&lt;h4&gt;结论&lt;/h4&gt;STHLM是一种有效的检索增强生成方法，能够处理复杂的多概念查询，特别是在生物医学等高维数据领域，通过迭代采样和生成式向量搜索显著提高了检索性能。&lt;h4&gt;翻译&lt;/h4&gt;检索增强生成通过将输出基于外部知识源来改进大型语言模型，减少幻觉并解决知识截止问题。然而，基于嵌入的标准检索无法捕捉多概念查询的复杂性，特别是在生物医学等领域，其中生物数据本质上具有高维特性。例如，组学数据集和临床报告同时表现出许多分子、细胞和生理特征。我们提出了随机潜在匹配，一种生成式向量搜索方法，通过从文本或图像输入中采样查询条件嵌入来增强检索性能。类似于思维链推理使语言模型能够'更长时间思考'复杂问题，STHLM允许检索系统通过迭代采样'更广泛搜索'。STHLM在科学文献、临床记录和组织图像等不同基准测试中显示出比经典向量检索的关键改进，通过测试时计算将检索性能提高了10-30%，同时实现了高达10倍的嵌入维度压缩。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to "think longer" on complex problems, STHLM allows retrieval systems to "search wider" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions.</description>
      <author>example@mail.com (Markus Ekvall, Ludvig Bergenstråhle, Patrick Truong, Ben Murrell, Joakim Lundeberg)</author>
      <guid isPermaLink="false">2512.19360v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis</title>
      <link>http://arxiv.org/abs/2512.19090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;JoyVoice是一种新型拟人化基础模型，用于灵活、无边界地合成多达8个说话者的语音，采用统一的E2E-Transformer-DiT架构和MM-Tokenizer，在多语言生成和零样本声音克隆方面取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;大型语音生成模型正在从单说话者、短句合成向多说话者、长对话生成发展，但当前长格式语音生成模型主要局限于二元、基于轮次的交互。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够灵活、无边界地合成多达8个说话者语音的基础模型，突破当前二元交互的限制。&lt;h4&gt;方法&lt;/h4&gt;JoyVoice采用统一的E2E-Transformer-DiT架构，使用自回归隐藏表示直接作为扩散输入实现端到端优化；提出MM-Tokenizer以12.5 Hz低比特率运行，整合多任务语义和MMSE损失；通过大规模数据扰动进行稳健的文本前端处理。&lt;h4&gt;主要发现&lt;/h4&gt;JoyVoice在多语言生成（中文、英语、日语、韩语）和零样本声音克隆方面取得最先进结果；在Seed-TTS-Eval基准和多说话者长对话声音克隆任务上表现优异；在长格式语音韵律连续性、多说话者对话节奏丰富度、副语言自然性和可理解性方面有显著改进。&lt;h4&gt;结论&lt;/h4&gt;JoyVoice展示了卓越的音频质量和泛化能力，是解决多说话者长对话语音生成挑战的有效方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语音生成模型正在从单说话者、短句合成向多说话者、长对话生成发展。当前长格式语音生成模型主要局限于二元、基于轮次的交互。为此，我们引入了JoyVoice，这是一种新型拟人化基础模型，用于灵活、无边界地合成多达8个说话者的语音。与传统的级联系统不同，JoyVoice采用统一的E2E-Transformer-DiT架构，直接使用自回归隐藏表示作为扩散输入，实现整体端到端优化。我们进一步提出了一个以12.5 Hz低比特率运行的MM-Tokenizer，它整合多任务语义和MMSE损失，有效建模语义和声学信息。此外，该模型通过大规模数据扰动进行稳健的文本前端处理。实验表明，JoyVoice在多语言生成（中文、英语、日语、韩语）和零样本声音克隆方面取得了最先进的结果。JoyVoice在Seed-TTS-Eval基准和多说话者长对话声音克隆任务上都取得了顶级结果，展示了卓越的音频质量和泛化能力。它在长格式语音的韵律连续性、多说话者对话的节奏丰富度、副语言自然性方面取得了显著改进，同时具有优越的可理解性。我们鼓励读者访问https://jea-speech.github.io/JoyVoice收听演示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large speech generation models are evolving from single-speaker, short sentence synthesis to multi-speaker, long conversation geneartion. Current long-form speech generation models are predominately constrained to dyadic, turn-based interactions. To address this, we introduce JoyVoice, a novel anthropomorphic foundation model designed for flexible, boundary-free synthesis of up to eight speakers. Unlike conventional cascaded systems, JoyVoice employs a unified E2E-Transformer-DiT architecture that utilizes autoregressive hidden representations directly for diffusion inputs, enabling holistic end-to-end optimization. We further propose a MM-Tokenizer operating at a low bitrate of 12.5 Hz, which integrates multitask semantic and MMSE losses to effectively model both semantic and acoustic information. Additionally, the model incorporates robust text front-end processing via large-scale data perturbation. Experiments show that JoyVoice achieves state-of-the-art results in multilingual generation (Chinese, English, Japanese, Korean) and zero-shot voice cloning. JoyVoice achieves top-tier results on both the Seed-TTS-Eval Benchmark and multi-speaker long-form conversational voice cloning tasks, demonstrating superior audio quality and generalization. It achieves significant improvements in prosodic continuity for long-form speech, rhythm richness in multi-speaker conversations, paralinguistic naturalness, besides superior intelligibility. We encourage readers to listen to the demo at https://jea-speech.github.io/JoyVoice</description>
      <author>example@mail.com (Fan Yu, Tao Wang, You Wu, Lin Zhu, Wei Deng, Weisheng Han, Wenchao Wang, Lin Hu, Xiangyu Liang, Xiaodong He, Yankun Huang, Yu Gu, Yuan Liu, Yuxuan Wang, Zhangyu Xiao, Ziteng Wang, Boya Dong, Feng Dang, Jinming Chen, Jingdong Li, Jun Wang, Yechen Jin, Yuan Zhang, Zhengyan Sheng, Xin Wang)</author>
      <guid isPermaLink="false">2512.19090v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers</title>
      <link>http://arxiv.org/abs/2512.18634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了预训练数据分布如何引导浅层Transformer模型实现可泛化算法或简单位置捷径的行为选择。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型可以实现两种不同的行为：可泛化的算法（如归纳头）和简单的位置捷径（如记忆固定输出位置），但何种因素决定模型倾向于哪种行为尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;研究预训练数据分布的选择如何引导浅层Transformer倾向于一种行为或另一种行为。&lt;h4&gt;方法&lt;/h4&gt;分析一个最小触发器-输出预测任务，即在特殊触发器的第二次出现时复制其后的标记。对单层Transformer的基于梯度训练进行严格分析，在无限和有限样本情况下研究模型学习机制。&lt;h4&gt;主要发现&lt;/h4&gt;1) 当输入序列具有足够多样性（触发器间距离的'最大和'比率低）时，模型实现归纳头并泛化到新上下文；2) 当该比率大时，模型采用位置捷径，无法分布外泛化；3) 预训练上下文长度与分布外泛化存在权衡；4) 推导出最小化每样本计算成本的最优预训练分布；5) 通过合成实验验证了理论预测。&lt;h4&gt;结论&lt;/h4&gt;研究结果揭示了预训练Transformer的算法偏差，并为数据驱动控制其学习行为提供了概念性指导。&lt;h4&gt;翻译&lt;/h4&gt;Transformer可以实现可泛化算法（如归纳头）和简单位置捷径（如记忆固定输出位置）。在本工作中，我们研究预训练数据分布的选择如何引导浅层Transformer倾向于一种行为或另一种行为。专注于一个最小的触发器-输出预测任务——在特殊触发器的第二次出现时复制其后的标记——我们对单层Transformer的基于梯度训练进行了严格分析。在无限和有限样本情况下，我们证明了学习到的机制存在转变：如果输入序列表现出足够的多样性（通过触发器间距离的'最大和'比率低来衡量），训练后的模型实现归纳头并泛化到未见过的上下文；相反，当这个比率大时，模型采用位置捷径，无法分布外泛化。我们还揭示了预训练上下文长度与分布外泛化之间的权衡，并推导出最小化每样本计算成本的最优预训练分布。最后，我们通过受控合成实验验证了理论预测，证明拓宽上下文分布能稳健地诱导归纳头并实现分布外泛化。我们的结果揭示了预训练Transformer的算法偏差，并为数据驱动控制其学习行为提供了概念性指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers can implement both generalizable algorithms (e.g., induction heads) and simple positional shortcuts (e.g., memorizing fixed output positions). In this work, we study how the choice of pretraining data distribution steers a shallow transformer toward one behavior or the other. Focusing on a minimal trigger-output prediction task -- copying the token immediately following a special trigger upon its second occurrence -- we present a rigorous analysis of gradient-based training of a single-layer transformer. In both the infinite and finite sample regimes, we prove a transition in the learned mechanism: if input sequences exhibit sufficient diversity, measured by a low ``max-sum'' ratio of trigger-to-trigger distances, the trained model implements an induction head and generalizes to unseen contexts; by contrast, when this ratio is large, the model resorts to a positional shortcut and fails to generalize out-of-distribution (OOD). We also reveal a trade-off between the pretraining context length and OOD generalization, and derive the optimal pretraining distribution that minimizes computational cost per sample. Finally, we validate our theoretical predictions with controlled synthetic experiments, demonstrating that broadening context distributions robustly induces induction heads and enables OOD generalization. Our results shed light on the algorithmic biases of pretrained transformers and offer conceptual guidelines for data-driven control of their learned behaviors.</description>
      <author>example@mail.com (Ryotaro Kawata, Yujin Song, Alberto Bietti, Naoki Nishikawa, Taiji Suzuki, Samuel Vaiter, Denny Wu)</author>
      <guid isPermaLink="false">2512.18634v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2512.18176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AtlasSegFM是一个图谱引导的框架，通过单个标注示例定制基础模型，提高医学图像分割准确性，特别是对小精细结构的分割效果。&lt;h4&gt;背景&lt;/h4&gt;准确的医学图像分割对临床诊断和治疗规划至关重要。虽然现有的交互式基础模型通过大规模多模态预训练提高了泛化能力，但仍依赖于精确提示，在训练数据代表性不足的情况下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出AtlasSegFM框架，用单个标注示例将现有基础模型定制到临床环境中，解决基础模型在特定临床场景下表现不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;核心创新包括：1) 通过图谱与查询图像之间的注册，为基础模型提供上下文感知提示的流程；2) 一个测试时适配器，用于融合图谱注册和基础模型的预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;在多种模态和器官的公共和内部数据集上进行的实验表明，AtlasSegFM持续改进了分割效果，特别是对小型精细结构的分割性能显著提升。&lt;h4&gt;结论&lt;/h4&gt;AtlasSegFM为现实临床工作流中基础模型的一次性定制提供了一个轻量级、可部署的解决方案，代码将公开提供。&lt;h4&gt;翻译&lt;/h4&gt;准确的医学图像分割对临床诊断和治疗规划至关重要。虽然最近的交互式基础模型（如nnInteractive）通过大规模多模态预训练提高了泛化能力，但它们仍然依赖于精确的提示，并且在训练数据中代表性不足的情况下表现往往低于预期。我们提出了AtlasSegFM，这是一个图谱引导的框架，可以用单个标注示例将现有的基础模型定制到临床环境中。核心创新包括：1) 一个通过图谱和查询图像之间的注册为基础模型提供上下文感知提示的流程；2) 一个测试时适配器，用于融合图谱注册和基础模型的预测。在跨越多种模态和器官的公共和内部数据集上进行的大量实验表明，AtlasSegFM持续改进了分割效果，特别是对于小型精细结构。AtlasSegFM为现实临床工作流中基础模型的一次性定制提供了一个轻量级、可部署的解决方案。代码将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate medical image segmentation is essential for clinical diagnosis and treatment planning. While recent interactive foundation models (e.g., nnInteractive) enhance generalization through large-scale multimodal pretraining, they still depend on precise prompts and often perform below expectations in contexts that are underrepresented in their training data. We present AtlasSegFM, an atlas-guided framework that customizes available foundation models to clinical contexts with a single annotated example. The core innovations are: 1) a pipeline that provides context-aware prompts for foundation models via registration between a context atlas and query images, and 2) a test-time adapter to fuse predictions from both atlas registration and the foundation model. Extensive experiments across public and in-house datasets spanning multiple modalities and organs demonstrate that AtlasSegFM consistently improves segmentation, particularly for small, delicate structures. AtlasSegFM provides a lightweight, deployable solution one-shot customization of foundation models in real-world clinical workflows. The code will be made publicly available.</description>
      <author>example@mail.com (Ziyu Zhang, Yi Yu, Simeng Zhu, Ahmed Aly, Yunhe Gao, Ning Gu, Yuan Xue)</author>
      <guid isPermaLink="false">2512.18176v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Unifying Deep Predicate Invention with Pre-trained Foundation Models</title>
      <link>http://arxiv.org/abs/2512.17992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了UniPred，一种统一自上而下和自下而上方法的双层学习框架，用于解决长时程机器人任务中的挑战。&lt;h4&gt;背景&lt;/h4&gt;长时程机器人任务很困难，因为它们具有连续的状态-动作空间和稀疏的反馈。现有的方法学习谓词要么自上而下（通过提示基础模型而没有数据基础），要么自下而上（从演示中学习而没有高级先验）。&lt;h4&gt;目的&lt;/h4&gt;引入UniPred，一个双层学习框架，统一了上述两种方法。&lt;h4&gt;方法&lt;/h4&gt;UniPred使用大型语言模型（LLMs）提出谓词效应分布，监督从低级别数据中学习神经谓词；学习到的反馈迭代地改进LLM假设；利用强大的视觉基础模型特征，UniPred在杂乱场景中学习鲁棒的谓词分类器；提出了一种谓词评估方法，支持超出STRIPS假设的符号模型。&lt;h4&gt;主要发现&lt;/h4&gt;在五个模拟和一个真实机器人领域，UniPred比自上而下方法实现2-4倍更高的成功率；UniPred比自下而上方法学习速度快3-4倍。&lt;h4&gt;结论&lt;/h4&gt;UniPred推动了机器人的可扩展和灵活的符号世界建模。&lt;h4&gt;翻译&lt;/h4&gt;长时程机器人任务由于连续的状态-动作空间和稀疏反馈而难以实现。符号世界模型通过将任务分解为捕获对象属性和关系的离散谓词来提供帮助。现有方法要么自上而下地学习谓词（通过提示基础模型而没有数据基础），要么自下而上地学习谓词（从演示中学习而没有高级先验）。我们引入了UniPred，一个统一这两种方法的双层学习框架。UniPred使用大型语言模型（LLMs）提出谓词效应分布，监督从低级别数据中学习神经谓词，同时学习到的反馈迭代地改进LLM假设。利用强大的视觉基础模型特征，UniPred在杂乱场景中学习鲁棒的谓词分类器。我们进一步提出了一种谓词评估方法，支持超出STRIPS假设的符号模型。在五个模拟和一个真实机器人领域中，UniPred比自上而下方法实现2-4倍更高的成功率，比自下而上方法学习速度快3-4倍，推动了机器人可扩展和灵活的符号世界建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-horizon robotic tasks are hard due to continuous state-action spaces and sparse feedback. Symbolic world models help by decomposing tasks into discrete predicates that capture object properties and relations. Existing methods learn predicates either top-down, by prompting foundation models without data grounding, or bottom-up, from demonstrations without high-level priors. We introduce UniPred, a bilevel learning framework that unifies both. UniPred uses large language models (LLMs) to propose predicate effect distributions that supervise neural predicate learning from low-level data, while learned feedback iteratively refines the LLM hypotheses. Leveraging strong visual foundation model features, UniPred learns robust predicate classifiers in cluttered scenes. We further propose a predicate evaluation method that supports symbolic models beyond STRIPS assumptions. Across five simulated and one real-robot domains, UniPred achieves 2-4 times higher success rates than top-down methods and 3-4 times faster learning than bottom-up approaches, advancing scalable and flexible symbolic world modeling for robotics.</description>
      <author>example@mail.com (Qianwei Wang, Bowen Li, Zhanpeng Luo, Yifan Xu, Alexander Gray, Tom Silver, Sebastian Scherer, Katia Sycara, Yaqi Xie)</author>
      <guid isPermaLink="false">2512.17992v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis</title>
      <link>http://arxiv.org/abs/2512.19246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MetaSHAP是一种可扩展的半自动化可解释人工智能方法，使用元学习和Shapley值分析来提供超参数调优的见解，帮助理解超参数的重要性、交互作用和影响范围。&lt;h4&gt;背景&lt;/h4&gt;超参数调优是优化机器学习模型的基本步骤，但计算成本高。理解超参数的相对重要性和交互作用对高效模型开发至关重要。&lt;h4&gt;目的&lt;/h4&gt;介绍MetaSHAP，提供可操作的、数据集感知的调优见解，帮助从业者优先调整关键超参数并理解它们的交互作用。&lt;h4&gt;方法&lt;/h4&gt;MetaSHAP在超过900万个评估过的机器学习管道的基准上运行，学习代理性能模型，使用基于SHAP的分析计算超参数交互，并推导出可解释的调优范围。&lt;h4&gt;主要发现&lt;/h4&gt;MetaSHAP在164个分类数据集和14个分类器的基准上验证，能产生可靠的重要性排名，在指导贝叶斯优化时具有竞争性能。&lt;h4&gt;结论&lt;/h4&gt;MetaSHAP是一种有效的超参数调优解释方法，可以帮助机器学习从业者更高效地优化模型。&lt;h4&gt;翻译&lt;/h4&gt;超参数调优是优化机器学习模型的基本步骤，但计算成本高昂。除优化外，理解超参数的相对重要性和交互作用对高效模型开发至关重要。本文介绍了MetaSHAP，一种可扩展的半自动化可解释人工智能(XAI)方法，它使用元学习和Shapley值分析来提供可操作的、数据集感知的调优见解。MetaSHAP在超过900万个评估过的机器学习管道的庞大基准上运行，能够产生可解释的重要性分数和可操作的调优见解，揭示每个超参数的重要性程度、如何与其他参数交互以及其影响集中的值范围。对于给定的算法和数据集，MetaSHAP从历史配置中学习代理性能模型，使用基于SHAP的分析计算超参数交互，并从最有影响力的超参数中推导出可解释的调优范围。这不仅可以帮助从业者确定优先调整哪些超参数，还可以理解它们的方向性和交互作用。我们在由164个分类数据集和14个分类器组成的多样化基准上对MetaSHAP进行了经验验证，证明它可以产生可靠的重要性排名，并且在用于指导贝叶斯优化时具有竞争性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICTAI66417.2025.00056&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.</description>
      <author>example@mail.com (Moncef Garouani, Ayah Barhrhouj)</author>
      <guid isPermaLink="false">2512.19246v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting</title>
      <link>http://arxiv.org/abs/2512.18661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 Pages, 8 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ASTIF是一种混合智能系统，通过基于置信度的元学习实时适应预测策略，整合语义和时间信息，优于现有深度学习和Transformer模型。&lt;h4&gt;背景&lt;/h4&gt;金融时间序列预测面临信息融合挑战，现有静态架构模型难以整合异构知识源或适应快速制度转变，传统方法仅依赖历史价格序列而忽视政策不确定性和市场叙事等波动性语义驱动因素。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型的局限性，提出一个能够实时适应预测策略的混合智能系统，有效融合定量和定性数据。&lt;h4&gt;方法&lt;/h4&gt;提出ASTIF（加密货币价格预测的自适应语义-时间集成），包含三个互补组件：1)使用MirrorPrompt的双通道小型语言模型提取语义市场线索和数值趋势；2)混合LSTM随机森林模型捕捉顺序时间依赖性；3)置信感知元学习器作为自适应推理层，根据实时不确定性调节各预测器贡献。&lt;h4&gt;主要发现&lt;/h4&gt;在2020-2024年AI聚焦加密货币和科技股票数据集上，ASTIF优于Informer、TFT等领先深度学习和Transformer基线；消融研究证实自适应元学习机制通过在市场动荡期间转移语义和时间通道依赖性成功缓解风险。&lt;h4&gt;结论&lt;/h4&gt;ASTIF为非平稳环境中融合定量和定性数据提供了可扩展的基于知识的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;金融时间序列预测本质上是一个信息融合挑战，然而大多数现有模型依赖静态架构，难以整合异构知识源或适应快速的制度转变。传统方法完全依赖历史价格序列，往往忽视波动性的语义驱动因素，如政策不确定性和市场叙事。为解决这些局限性，我们提出了ASTIF（加密货币价格预测的自适应语义-时间集成），这是一个混合智能系统，通过基于置信度的元学习实时适应其预测策略。该框架整合了三个互补组件：使用MirrorPrompt的双通道小型语言模型提取语义市场线索和数值趋势；混合LSTM随机森林模型捕捉顺序时间依赖性；置信感知元学习器作为自适应推理层，根据实时不确定性调节每个预测器的贡献。在2020年至2024年AI聚焦的加密货币和主要科技股票的多样化数据集上的实验评估显示，ASTIF优于领先的深度学习和Transformer基线（如Informer、TFT）。消融研究进一步证实了自适应元学习机制的关键作用，该机制通过在市场动荡期间在语义和时间通道之间转移依赖性来成功缓解风险。该研究为在非平稳环境中融合定量和定性数据提供了可扩展的基于知识的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial time series forecasting is fundamentally an information fusion challenge, yet most existing models rely on static architectures that struggle to integrate heterogeneous knowledge sources or adjust to rapid regime shifts. Conventional approaches, relying exclusively on historical price sequences, often neglect the semantic drivers of volatility such as policy uncertainty and market narratives. To address these limitations, we propose the ASTIF (Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting), a hybrid intelligent system that adapts its forecasting strategy in real time through confidence-based meta-learning. The framework integrates three complementary components. A dual-channel Small Language Model using MirrorPrompt extracts semantic market cues alongside numerical trends. A hybrid LSTM Random Forest model captures sequential temporal dependencies. A confidence-aware meta-learner functions as an adaptive inference layer, modulating each predictor's contribution based on its real-time uncertainty.  Experimental evaluation on a diverse dataset of AI-focused cryptocurrencies and major technology stocks from 2020 to 2024 shows that ASTIF outperforms leading deep learning and Transformer baselines (e.g., Informer, TFT). The ablation studies further confirm the critical role of the adaptive meta-learning mechanism, which successfully mitigates risk by shifting reliance between semantic and temporal channels during market turbulence. The research contributes a scalable, knowledge-based solution for fusing quantitative and qualitative data in non-stationary environments.</description>
      <author>example@mail.com (Hafiz Saif Ur Rehman, Ling Liu, Kaleem Ullah Qasim)</author>
      <guid isPermaLink="false">2512.18661v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Toward Scalable and Valid Conditional Independence Testing with Spectral Representations</title>
      <link>http://arxiv.org/abs/2512.19510v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于表示学习的条件独立性检验方法，通过偏协方差算子的奇异值分解构建检验统计量，并引入双层对比算法进行学习，解决了传统核方法适应性有限、收敛慢和可扩展性差的问题。&lt;h4&gt;背景&lt;/h4&gt;条件独立性在因果推断、特征选择和图形建模中至关重要，但在许多情况下缺乏额外假设时无法检验。现有CI检验方法通常依赖限制性结构条件，限制了它们在真实数据上的有效性。&lt;h4&gt;目的&lt;/h4&gt;探索表示学习是否能够解决传统核方法在条件独立性检验中的局限性，包括适应性有限、收敛速度慢和可扩展性差等问题。&lt;h4&gt;方法&lt;/h4&gt;专注于从偏协方差算子的奇异值分解中获得的表示，使用这些表示构建类似于Hilbert-Schmidt独立性准则(HSIC)的简单检验统计量，并引入实用的双层对比算法来学习这些表示。&lt;h4&gt;主要发现&lt;/h4&gt;理论将表示学习误差与检验性能联系起来，建立了渐近有效性和功效保证。初步实验表明该方法为可扩展的CI检验提供了实用且统计上有依据的途径。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种连接基于核的理论和现代表示学习的实用方法，为可扩展的条件独立性检验开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;条件独立性(CI)在因果推断、特征选择和图形建模中至关重要，但在没有额外假设的情况下，许多情况下条件独立性是无法检验的。现有的CI检验通常依赖于限制性的结构条件，限制了它们在真实数据上的有效性。使用偏协方差算子的核方法提供了更合理的方法，但适应性有限、收敛速度慢且可扩展性差。在本研究中，我们探索表示学习是否有助于解决这些限制。具体而言，我们专注于从偏协方差算子的奇异值分解中获得的表示，并使用它们构建一个类似于Hilbert-Schmidt独立性准则(HSIC)的简单检验统计量。我们还引入了一个实用的双层对比算法来学习这些表示。我们的理论将表示学习误差与检验性能联系起来，并建立了渐近有效性和功效保证。初步实验表明，这种方法为可扩展的CI检验提供了一种实用且统计上有依据的途径，连接了基于核的理论和现代表示学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning.</description>
      <author>example@mail.com (Alek Frohlich, Vladimir Kostic, Karim Lounici, Daniel Perazzo, Massimiliano Pontil)</author>
      <guid isPermaLink="false">2512.19510v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</title>
      <link>http://arxiv.org/abs/2512.19504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint. Under review at IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种物理感知的表示学习框架，利用多光谱信息建模长期物理过程的稳定特征。通过结合短波红外(SWIR)和热红外(TIR)数据，采用创新的FusionNet架构，在各种光谱配置下超越了最先进的基线模型。&lt;h4&gt;背景&lt;/h4&gt;当前处理多模态视觉信号的深度学习模型往往依赖于与信号形成物理过程不太一致的经验归纳偏差，导致在跨光谱和真实世界条件下表现脆弱。特别是那些优先考虑直接热线索的方法难以捕捉由持续热排放引起的间接但持久的环境变化。&lt;h4&gt;目的&lt;/h4&gt;开发一种物理感知的表示学习框架，利用多光谱信息建模长期物理过程的稳定特征，提高模型在跨光谱和真实世界条件下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入对土壤特性变化敏感的地质短波红外(SWIR)比率，通过中间融合架构与热红外(TIR)数据结合实现为FusionNet。在卷积层中嵌入可训练的微分信号处理先验，结合混合池化策略，并采用更大的感受野来增强跨光谱模态的鲁棒性。使用ImageNet预训练进行迁移学习实验，评估模态感知训练对跨光谱学习的重要性。&lt;h4&gt;主要发现&lt;/h4&gt;系统性消融实验显示每个架构组件都对性能提升有贡献，DGCNN在SWIR比率上达到88.7%的准确率，FusionNet达到90.6%，在五种光谱配置下超越了最先进的基线模型。ImageNet预训练会降低TIR性能，突显了模态感知训练对跨光谱学习的重要性。在真实世界数据上的评估表明，物理感知特征选择与原则性深度学习架构相结合能产生鲁棒且可推广的表示。&lt;h4&gt;结论&lt;/h4&gt;将物理感知的特征选择与原则性的深度学习架构相结合，可以在具有挑战性的条件下改善多光谱学习，说明基于第一原理的信号建模可以改进多光谱学习。&lt;h4&gt;翻译&lt;/h4&gt;现代处理多模态视觉信号的深度学习模型通常依赖于与信号形成物理过程不太一致的经验归纳偏差，导致在跨光谱和真实世界条件下表现脆弱。特别是那些优先考虑直接热线索的方法难以捕捉由持续热排放引起的间接但持久的环境变化。这项工作引入了一种物理感知的表示学习框架，利用多光谱信息来建模长期物理过程的稳定特征。具体来说，一种对土壤特性变化敏感的地质短波红外(SWIR)比率通过中间融合架构与热红外(TIR)数据结合，实现为FusionNet。所提出的骨干网络在卷积层中嵌入了可训练的微分信号处理先验，结合混合池化策略，并采用更大的感受野来增强跨光谱模态的鲁棒性。系统性消融实验显示每个架构组件都对性能提升有贡献，DGCNN在SWIR比率上达到88.7%的准确率，FusionNet达到90.6%，在五种光谱配置下超越了最先进的基线模型。迁移学习实验进一步表明，ImageNet预训练会降低TIR性能，突显了模态感知训练对跨光谱学习的重要性。在真实世界数据上的评估表明，将物理感知的特征选择与原则性的深度学习架构相结合，能够产生鲁棒且可推广的表示，说明了如何通过第一原理的信号建模来改善具有挑战性条件下的多光谱学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.  This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.  Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.  Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.</description>
      <author>example@mail.com (Georgios Voulgaris)</author>
      <guid isPermaLink="false">2512.19504v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction</title>
      <link>http://arxiv.org/abs/2512.19194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了一种因果异构图表示学习方法(CHGRL)用于COPD合并症风险预测，通过构建包含患者-疾病交互的异构数据集，结合因果推理与异构图学习，并融入因果损失函数，提高了COPD早期识别能力&lt;h4&gt;背景&lt;/h4&gt;基层医疗诊断和治疗能力不足，导致COPD急性加重的早期识别和预警存在不足，患病率高、负担重，但筛查率较低&lt;h4&gt;目的&lt;/h4&gt;为改善COPD早期识别和预警不足的情况，开发一种COPD合并症风险预测方法&lt;h4&gt;方法&lt;/h4&gt;构建因果异构图表示学习方法(CHGRL)，包括：构建包含患者与疾病交互的异构数据集；构建因果感知的异构图学习架构，结合因果推理与异构图学习；在模型设计中融入因果损失函数，增加反事实推理学习损失和因果正则化损失&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型与强大的GNN基线相比显示出高检测精度&lt;h4&gt;结论&lt;/h4&gt;CHGRL方法能有效提高COPD合并症风险预测的准确性，有助于改善基层医疗中COPD早期识别不足的问题&lt;h4&gt;翻译&lt;/h4&gt;由于基层诊断和治疗能力不足，慢性阻塞性肺疾病(COPD)急性加重的早期识别和预警仍存在不足，通常导致高患病率和高负担，但筛查率相对较低。为逐步改善这一状况，本文开发了一种用于COPD合并症风险预测的因果异构图表示学习(CHGRL)方法，包括：a)构建包含患者与疾病之间交互的异构数据集；b)构建因果感知的异构图学习架构，结合因果推理机制与异构图学习，支持不同类型关系的异构图因果学习；c)在模型设计中融入因果损失函数，在交叉熵分类损失基础上增加反事实推理学习损失和因果正则化损失。我们评估了该方法并将其性能与强大的GNN基线进行比较。实验评估表明，所提出的模型具有高检测精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to the insufficient diagnosis and treatment capabilities at the grassroots level, there are still deficiencies in the early identification and early warning of acute exacerbation of Chronic obstructive pulmonary disease (COPD), often resulting in a high prevalence rate and high burden, but the screening rate is relatively low. In order to gradually improve this situation. In this paper, this study develop a Causal Heterogeneous Graph Representation Learning (CHGRL) method for COPD comorbidity risk prediction method that: a) constructing a heterogeneous Our dataset includes the interaction between patients and diseases; b) A cause-aware heterogeneous graph learning architecture has been constructed, combining causal inference mechanisms with heterogeneous graph learning, which can support heterogeneous graph causal learning for different types of relationships; and c) Incorporate the causal loss function in the model design, and add counterfactual reasoning learning loss and causal regularization loss on the basis of the cross-entropy classification loss. We evaluate our method and compare its performance with strong GNN baselines. Following experimental evaluation, the proposed model demonstrates high detection accuracy.</description>
      <author>example@mail.com (Leming Zhou, Zuo Wang, Zhigang Liu)</author>
      <guid isPermaLink="false">2512.19194v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Medical Large Vision-Language Models via Alignment Distillation</title>
      <link>http://arxiv.org/abs/2512.18554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI'2026 (Main track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对医学大视觉语言模型(Med-LVLMs)在临床应用中因视觉理解不匹配导致的幻觉输出问题，提出了MEDALIGN框架，通过视觉对齐知识蒸馏技术改善模型的性能和可解释性。&lt;h4&gt;背景&lt;/h4&gt;医学大视觉语言模型(Med-LVLMs)已在临床应用中显示出有前景的结果，但常常由于视觉理解不匹配而产生幻觉输出。&lt;h4&gt;目的&lt;/h4&gt;识别导致Med-LVLMs产生幻觉输出的两个基本限制(视觉表示学习不足和视觉注意力对齐不良)，并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出MEDALIGN，一个简单、轻量级的对齐蒸馏框架，将领域特定的对比语言-图像预训练(CILP)模型的视觉对齐知识转移到Med-LVLMs。引入两种蒸馏损失：空间感知视觉对齐损失和注意力感知蒸馏损失。&lt;h4&gt;主要发现&lt;/h4&gt;在医疗报告生成和医疗视觉问答基准测试中，MEDALIGN一致提高了模型的性能和可解释性，产生更多视觉基础输出。&lt;h4&gt;结论&lt;/h4&gt;MEDALIGN通过解决视觉表示学习和视觉注意力对齐问题，有效改善了Med-LVLMs在临床应用中的表现，减少了幻觉输出。&lt;h4&gt;翻译&lt;/h4&gt;医学大视觉语言模型(Med-LVLMs)已在临床应用中显示出有前景的结果，但常常由于视觉理解不匹配而产生幻觉输出。在本研究中，我们确定了导致此问题的两个基本限制：视觉表示学习不足和视觉注意力对齐不良。为解决这些问题，我们提出了MEDALIGN，一个简单、轻量级的对齐蒸馏框架，将视觉对齐知识从领域特定的对比语言-图像预训练(CILP)模型转移到Med-LVLMs。MEDALIGN引入了两种蒸馏损失：基于视觉标记级相似性结构的空间感知视觉对齐损失，以及引导注意力朝向诊断相关区域的注意力感知蒸馏损失。在医疗报告生成和医疗视觉问答(VQA)基准测试上的广泛实验表明，MEDALIGN一致提高了性能和可解释性，产生了更多视觉基础的输出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical Large Vision-Language Models (Med-LVLMs) have shown promising results in clinical applications, but often suffer from hallucinated outputs due to misaligned visual understanding. In this work, we identify two fundamental limitations contributing to this issue: insufficient visual representation learning and poor visual attention alignment. To address these problems, we propose MEDALIGN, a simple, lightweight alignment distillation framework that transfers visual alignment knowledge from a domain-specific Contrastive Language-Image Pre-training (CLIP) model to Med-LVLMs. MEDALIGN introduces two distillation losses: a spatial-aware visual alignment loss based on visual token-level similarity structures, and an attention-aware distillation loss that guides attention toward diagnostically relevant regions. Extensive experiments on medical report generation and medical visual question answering (VQA) benchmarks show that MEDALIGN consistently improves both performance and interpretability, yielding more visually grounded outputs.</description>
      <author>example@mail.com (Aofei Chang, Ting Wang, Fenglong Ma)</author>
      <guid isPermaLink="false">2512.18554v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics</title>
      <link>http://arxiv.org/abs/2512.18056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 10 figures. Methodological paper on probabilistic user modeling and latent representation learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种概率性数字孪生框架，通过可变分推断学习用户潜在状态，能够提供可解释的、具有不确定性意识的用户表示，超越了传统确定性方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;理解用户身份和行为对于个性化、推荐和决策支持等应用至关重要。现有方法主要依赖确定性嵌入或黑盒预测模型，这些方法在不确定性量化方面有限，并且对潜在表示编码的内容缺乏洞察。&lt;h4&gt;目的&lt;/h4&gt;提出一种概率性数字孪生框架，将每个用户建模为生成观察行为数据的潜在随机状态，通过可变分推断实现可扩展的后验估计，同时保持完全概率解释。&lt;h4&gt;方法&lt;/h4&gt;使用变分自编码器(VAE)实现该框架，应用于用户响应数据集以捕捉用户身份的稳定方面；引入基于统计的解释流程，将潜在维度与可观察的行为模式联系起来；通过分析每个潜在维度极端处的用户，并使用非参数假设检验和效应大小验证差异。&lt;h4&gt;主要发现&lt;/h4&gt;特定维度对应可解释的特征，如意见强度和果断性；用户结构主要是连续的，而不是离散聚类；沿着少数主导潜在轴出现弱但有意义的结构。&lt;h4&gt;结论&lt;/h4&gt;概率性数字孪生可以提供可解释的、具有不确定性意识的表示，超越确定性用户嵌入。&lt;h4&gt;翻译&lt;/h4&gt;理解用户身份和行为是个人化、推荐和决策支持等应用的核心。大多数现有方法依赖于确定性嵌入或黑盒预测模型，提供了有限的不确定性量化，并且对潜在表示编码的内容知之甚少。我们提出了一种概率性数字孪生框架，其中每个用户被建模为生成观察行为数据的潜在随机状态。数字孪生通过可变分推断学习，实现了可扩展的后验估计，同时保持完全的概率解释。我们使用应用于用户响应数据集的变分自编码器(VAE)实例化该框架，该数据集旨在捕捉用户身份的稳定方面。除了基于标准重建的评估外，我们引入了一个基于统计的解释流程，将潜在维度与可观察的行为模式联系起来。通过分析每个潜在维度极端处的用户，并使用非参数假设检验和效应大小验证差异，我们证明特定维度对应可解释的特征，如意见强度和果断性。从经验上看，我们发现用户结构主要是连续的，而不是离散聚类，沿着少数主导潜在轴出现弱但有意义的结构。这些结果表明，概率性数字孪生可以提供超越确定性用户嵌入的可解释、具有不确定性意识的表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding user identity and behavior is central to applications such as personalization, recommendation, and decision support. Most existing approaches rely on deterministic embeddings or black-box predictive models, offering limited uncertainty quantification and little insight into what latent representations encode. We propose a probabilistic digital twin framework in which each user is modeled as a latent stochastic state that generates observed behavioral data. The digital twin is learned via amortized variational inference, enabling scalable posterior estimation while retaining a fully probabilistic interpretation. We instantiate this framework using a variational autoencoder (VAE) applied to a user-response dataset designed to capture stable aspects of user identity. Beyond standard reconstruction-based evaluation, we introduce a statistically grounded interpretation pipeline that links latent dimensions to observable behavioral patterns. By analyzing users at the extremes of each latent dimension and validating differences using nonparametric hypothesis tests and effect sizes, we demonstrate that specific dimensions correspond to interpretable traits such as opinion strength and decisiveness. Empirically, we find that user structure is predominantly continuous rather than discretely clustered, with weak but meaningful structure emerging along a small number of dominant latent axes. These results suggest that probabilistic digital twins can provide interpretable, uncertainty-aware representations that go beyond deterministic user embeddings.</description>
      <author>example@mail.com (Daniel David)</author>
      <guid isPermaLink="false">2512.18056v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset</title>
      <link>http://arxiv.org/abs/2512.19494v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Kolmogorov-Arnold图神经网络(KAGNNs)在无机纳米材料数据集CHILI上的应用，发现KAGNNs在分类任务上显著优于传统图神经网络，达到了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;Kolmogorov-Arnold网络(KANs)是图神经网络(GNNs)领域的最新发展，基于KAN的GNN模型通常比基于多层感知器(MLP)的GNNs更准确。然而，这些模型主要在有机分子图数据集上进行了测试，而忽略了无机纳米材料数据集。&lt;h4&gt;目的&lt;/h4&gt;填补研究空白，将KAGNNs应用于最近发布的大型无机纳米材料数据集CHILI，并为此数据集适配和测试适当的KAGNNs。&lt;h4&gt;方法&lt;/h4&gt;应用和适配KAGNNs到CHILI数据集上，并进行实验测试。&lt;h4&gt;主要发现&lt;/h4&gt;在CHILI数据集上，特别是在CHILI-3K上，KAGNNs在分类任务上显著超越了传统GNNs，达到了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;KAGNNs是处理无机纳米材料数据集的有效方法，其性能优于传统图神经网络。&lt;h4&gt;翻译&lt;/h4&gt;Kolmogorov-Arnold网络(KANs)的最新发展为图神经网络(GNNs)领域引入了新的发现，通过基于KAN的GNN模型扩展了现有模型集，这些模型通常比基于多层感知器(MLP)的GNNs更准确。这些模型在包含有机分子的图数据集上得到了广泛测试；然而，那些研究忽略了无机纳米材料数据集。在本工作中，我们通过将Kolmogorov-Arnold图神经网络(KAGNNs)应用于最近发布的大型无机纳米材料数据集CHILI来填补这一空白。为此，我们适配并测试了适合该数据集的KAGNNs。我们的实验揭示，在CHILI数据集上，特别是在CHILI-3K上，KAGNNs在分类任务上显著超越了传统GNNs，达到了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.</description>
      <author>example@mail.com (Nikita Volzhin, Soowhan Yoon)</author>
      <guid isPermaLink="false">2512.19494v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>A Logical View of GNN-Style Computation and the Role of Activation Functions</title>
      <link>http://arxiv.org/abs/2512.19332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了MPLang语言在数值和布尔方面的表达能力，MPLang是一种通过线性消息传递和激活函数捕获图神经网络计算的声明式语言。&lt;h4&gt;背景&lt;/h4&gt;研究关注图神经网络(GNNs)的计算表达能力，特别是MPLang这种声明式语言。&lt;h4&gt;目的&lt;/h4&gt;分析MPLang的表达能力，包括无激活函数的A-MPLang片段以及有界和无界激活函数情况下的表达能力差异。&lt;h4&gt;方法&lt;/h4&gt;研究无激活函数的A-MPLang片段的表达能力特征；分析有界激活函数的表达能力；比较无界激活函数(如ReLU)和有界激活函数(如截断ReLU)之间的表达能力差异。&lt;h4&gt;主要发现&lt;/h4&gt;1) 无激活函数的A-MPLang的表达能力可以通过行走求和特征来表征；2) 在温和条件下，所有最终常数激活函数在数值和布尔表达方面具有相同的表达能力；3) 使用ReLU的MPLang在数值查询方面比使用最终常数激活函数的MPLang更强大；4) 线性聚合和最终常数非线性之间的微妙相互作用导致了表达能力差异。&lt;h4&gt;结论&lt;/h4&gt;使用ReLU的GNN比仅限于最终常数激活函数和线性层的GNN更具表达能力，这表明激活函数的选择对GNN的表达能力有重要影响。&lt;h4&gt;翻译&lt;/h4&gt;我们研究MPLang的数值和布尔表达能力，MPLang是一种通过线性消息传递和激活函数捕获图神经网络(GNNs)计算的声明式语言。我们从没有激活函数的A-MPLang片段开始，并基于行走求和特征给出了其表达能力的特征化。对于有界激活函数，我们展示(在温和条件下)所有最终常数激活函数产生相同的数值和布尔表达能力，并且它包含了先前建立的、具有最终常数激活函数但没有线性层的GNN逻辑。最后，我们证明了在有线性层的情况下，无界激活函数和有界激活函数之间的首次表达能力差异：使用ReLU的MPLang在数值查询方面比使用最终常数激活函数(如截断ReLU)的MPLang更强大。这取决于线性聚合和最终常数非线性之间的微妙相互作用，并确立了使用ReLU的GNN比那些仅限于最终常数激活函数和线性层的GNN更具表达能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the numerical and Boolean expressiveness of MPLang, a declarative language that captures the computation of graph neural networks (GNNs) through linear message passing and activation functions. We begin with A-MPLang, the fragment without activation functions, and give a characterization of its expressive power in terms of walk-summed features. For bounded activation functions, we show that (under mild conditions) all eventually constant activations yield the same expressive power - numerical and Boolean - and that it subsumes previously established logics for GNNs with eventually constant activation functions but without linear layers. Finally, we prove the first expressive separation between unbounded and bounded activations in the presence of linear layers: MPLang with ReLU is strictly more powerful for numerical queries than MPLang with eventually constant activation functions, e.g., truncated ReLU. This hinges on subtle interactions between linear aggregation and eventually constant non-linearities, and it establishes that GNNs using ReLU are more expressive than those restricted to eventually constant activations and linear layers.</description>
      <author>example@mail.com (Pablo Barceló, Floris Geerts, Matthias Lanzinger, Klara Pakhomenko, Jan Van den Bussche)</author>
      <guid isPermaLink="false">2512.19332v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence</title>
      <link>http://arxiv.org/abs/2512.19540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了主动卷积照明(ACI)技术与深度学习(DL)模型相结合的可行性，提出了一种概念框架，并通过卷积神经网络和迁移学习方法进行了验证，为未来ACI-DL混合架构的发展奠定了基础。&lt;h4&gt;背景&lt;/h4&gt;大气湍流对光学成像、遥感、自由空间光通信等多种应用构成了根本性限制。自适应光学、波前整形和机器学习等领域的最新进展，得益于基础理论、光电硬件和计算算法的协同进步，在减轻湍流引起的畸变方面显示出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;研究将主动卷积照明(ACI)与基于神经网络的方法集成的可行性，探索ACI与数据驱动的深度学习模型如何互补集成，以减轻大气湍流引起的畸变。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一个将ACI与数据驱动模型耦合的概念框架，确定了学习表示可以支持ACI相关性注入机制的条件。作为代表性例子，采用卷积神经网络(CNN)结合迁移学习方法来检查学习模型如何与ACI协同工作。&lt;h4&gt;主要发现&lt;/h4&gt;这项探索性研究证明了将ACI与深度学习模型集成的可行实施路径，为评估未来ACI-DL混合架构的潜力奠定了早期基础，代表了评估ACI与现代DL模型之间更广泛协同相互作用的一步。&lt;h4&gt;结论&lt;/h4&gt;通过将ACI与深度学习方法结合，可以创建更有效的大气湍流补偿系统，这种混合方法代表了湍流减轻技术的重要进步，为未来更先进的混合架构铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;大气湍流对广泛范围内的应用构成根本性限制，包括光学成像、遥感和自由空间光通信。自适应光学、波前整形和机器学习的最新进展，得益于基础理论、光电硬件和计算算法的协同进步，已显示出减轻湍流引起畸变的巨大潜力。最近，主动卷积照明(ACI)被提出作为一种多功能且基于物理的技术，用于在极具挑战性的湍流条件下传输结构化光束，且畸变最小。虽然ACI在表述上与其他方法不同，但它与其他基于物理的畸变校正方法具有概念上的相似性，并有望与数据驱动的深度学习(DL)模型进行互补集成。受最近将深度学习与传统湍流减轻策略结合的工作启发，本文研究了将ACI与基于神经网络的方法集成的可行性。我们概述了将ACI与数据驱动模型耦合的概念框架，并确定了在哪些条件下学习表示可以有意义地支持ACI的相关性注入机制。作为一个代表性例子，我们采用卷积神经网络(CNN)结合迁移学习方法来检查学习模型如何与ACI协同工作。这项探索性研究证明了可行的实施路径，并为评估未来ACI-DL混合架构的潜力奠定了早期基础，代表了评估ACI与现代DL模型之间更广泛协同相互作用的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Atmospheric turbulence imposes a fundamental limitation across a broad range of applications, including optical imaging, remote sensing, and free-space optical communication. Recent advances in adaptive optics, wavefront shaping, and machine learning, driven by synergistic progress in fundamental theories, optoelectronic hardware, and computational algorithms, have demonstrated substantial potential in mitigating turbulence-induced distortions. Recently, active convolved illumination (ACI) was proposed as a versatile and physics-driven technique for transmitting structured light beams with minimal distortion through highly challenging turbulent regimes. While distinct in its formulation, ACI shares conceptual similarities with other physics-driven distortion correction approaches and stands to benefit from complementary integration with data-driven deep learning (DL) models. Inspired by recent work coupling deep learning with traditional turbulence mitigation strategies, the present work investigates the feasibility of integrating ACI with neural network-based methods. We outline a conceptual framework for coupling ACI with data-driven models and identify conditions under which learned representations can meaningfully support ACI's correlation-injection mechanism. As a representative example, we employ a convolutional neural network (CNN) together with a transfer-learning approach to examine how a learned model may operate in tandem with ACI. This exploratory study demonstrates feasible implementation pathways and establishes an early foundation for assessing the potential of future ACI-DL hybrid architectures, representing a step toward evaluating broader synergistic interactions between ACI and modern DL models.</description>
      <author>example@mail.com (Adrian A. Moazzam, Anindya Ghoshroy, Breeanne Heusdens, Durdu O. Guney, Roohollah Askari)</author>
      <guid isPermaLink="false">2512.19540v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm</title>
      <link>http://arxiv.org/abs/2512.18947v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的动态多模态多目标优化算法，通过结合聚类的自编码器预测动态响应机制和自适应小生境策略，有效解决了在时变环境中同时跟踪多个等价帕累托最优集和保持种群多样性的双重挑战。&lt;h4&gt;背景&lt;/h4&gt;动态多模态多目标优化面临双重挑战：同时跟踪多个等价的帕累托最优集和在时变环境中保持种群多样性。现有动态多目标进化算法通常忽略解的模态性，而静态多模态多目标进化算法缺乏对动态变化的适应性。&lt;h4&gt;目的&lt;/h4&gt;解决动态多模态多目标优化中的双重挑战，即同时跟踪多个等价帕累托最优集和保持种群多样性。&lt;h4&gt;方法&lt;/h4&gt;1) 引入一个新的动态多模态多目标测试函数基准套件，融合动态和多模态优化特性构建；2) 提出基于聚类的自编码器预测动态响应机制的新算法，利用自编码器处理匹配簇生成多样化初始种群；3) 集成自适应小生境策略到静态优化器中，平衡算法的收敛性和多样性。&lt;h4&gt;主要发现&lt;/h4&gt;在12个动态多模态多目标测试函数实例上的经验分析表明，与最先进的动态多目标进化算法和多模态多目标进化算法相比，该算法不仅在决策空间中更有效地保持种群多样性，而且在目标空间中实现了更好的收敛性。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法在保持种群多样性和实现收敛性方面均优于现有方法，为动态多模态多目标优化问题提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;动态多模态多目标优化提出了同时跟踪多个等价帕累托最优集和在时变环境中保持种群多样性的双重挑战。然而，现有的动态多目标进化算法通常忽略了解的模态性，而静态多模态多目标进化算法缺乏对动态变化的适应性。为解决上述挑战，本文做出两个主要贡献。首先，我们引入了一个新的动态多模态多目标测试函数基准套件，通过融合动态和多模态优化的特性构建，建立一个严格的评估平台。其次，我们提出了一种以基于聚类的自编码器预测动态响应机制为中心的新算法，该算法利用自编码器模型处理匹配的簇以生成高度多样化的初始种群。此外，为了平衡算法的收敛性和多样性，我们将自适应小生境策略集成到静态优化器中。在12个动态多模态多目标测试函数实例上的经验分析表明，与几种最先进的动态多目标进化算法和多模态多目标进化算法相比，我们的算法不仅在决策空间中更有效地保持种群多样性，而且在目标空间中实现了更好的收敛性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.</description>
      <author>example@mail.com (Li Yan, Bolun Liu, Chao Li, Jing Liang, Kunjie Yu, Caitong Yue, Xuzhao Chai, Boyang Qu)</author>
      <guid isPermaLink="false">2512.18947v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Analysis of Collective and Non-Collective Thomson Scattering Spectra</title>
      <link>http://arxiv.org/abs/2512.18173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了使用深度神经网络和迁移学习改进Thomson散射诊断中电子密度和电子温度估计的方法，特别关注在训练数据有限的情况下的应用效果。&lt;h4&gt;背景&lt;/h4&gt;Thomson散射诊断技术能够可靠且最小程度地扰动地测量等离子体的基本参数如电子密度和电子温度，但当TS光谱被噪声主导或需要实时快速分析时，传统拟合算法可能失效。&lt;h4&gt;目的&lt;/h4&gt;评估迁移学习在深度神经网络估计电子密度和电子温度方面的有效性，特别是在集体和非集体散射区域，以及当训练数据有限时。&lt;h4&gt;方法&lt;/h4&gt;提出五种架构不同的深度神经网络，在合成的TS数据上进行预训练，然后针对实验测量的TS数据进行调整；比较使用和不使用迁移学习时，不同训练集大小对n_e和T_e估计误差的影响。&lt;h4&gt;主要发现&lt;/h4&gt;当训练集包含约200个实验测量的光谱时，使用迁移学习的模型误差明显降低；迁移学习在集体和非集体散射区域估计n_e和T_e方面均有效。&lt;h4&gt;结论&lt;/h4&gt;迁移学习可以显著改善深度神经网络在TS诊断中的应用性能，特别是在训练数据有限的情况下，为实时等离子体参数测量提供了可行方案。&lt;h4&gt;翻译&lt;/h4&gt;Thomson散射诊断提供可靠、最小扰动地测量等离子体基本参数如电子密度和电子温度的方法。当TS光谱被噪声主导或需要快速分析进行实时操作时，传统拟合算法可能失效，而深度神经网络可以提供准确的n_e和T_e估计。虽然深度神经网络通常需要大量训练集，但迁移学习可以通过利用相关源任务的预训练模型来改进目标任务在有限数据下的性能。我们提出了五种架构不同的深度神经网络，在合成TS数据上预训练并针对实验测量TS数据调整，以评估迁移学习在集体和非集体散射区域估计n_e和T_e的效力。我们比较了使用和不使用迁移学习时训练集大小对n_e和T_e估计误差的影响，并观察到当训练集包含约200个实验测量光谱时，迁移学习降低了模型误差。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Thomson scattering (TS) diagnostics provide reliable, minimally perturbative measurements of fundamental plasma parameters, such as electron density ($n_e$) and electron temperature ($T_e$). Deep neural networks can provide accurate estimates of $n_e$ and $T_e$ when conventional fitting algorithms may fail, such as when TS spectra are dominated by noise, or when fast analysis is required for real-time operation. Although deep neural networks typically require large training sets, transfer learning can improve model performance on a target task with limited data by leveraging pre-trained models from related source tasks, where select hidden layers are further trained using target data. We present five architecturally diverse deep neural networks, pre-trained on synthetic TS data and adapted for experimentally measured TS data, to evaluate the efficacy of transfer learning in estimating $n_e$ and $T_e$ in both the collective and non-collective scattering regimes. We evaluate errors in $n_e$ and $T_e$ estimates as a function of training set size for models trained with and without transfer learning, and we observe decreases in model error from transfer learning when the training set contains $\lessapprox$ 200 experimentally measured spectra.</description>
      <author>example@mail.com (T. Van Hoomissen, J. Alhuthali, A. M. Ortiz, D. A. Mariscal, R. S. Dorst, S. Eisenbach, H. Zhang, J. J. Pilgram, C. G. Constantin, L. Rovige, C. Niemann, D. B. Schaeffer)</author>
      <guid isPermaLink="false">2512.18173v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</title>
      <link>http://arxiv.org/abs/2512.20217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LiteFusion的新型多模态3D检测器，通过重新考虑LiDAR在相机-LiDAR融合范式中的作用，解决了现有方法对LiDAR的过度依赖问题，实现了在多样化硬件平台上的友好部署。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测对安全稳健的智能交通系统至关重要。当前多模态3D物体检测器通常依赖复杂架构和训练策略来获得更高检测精度，但这些方法严重依赖LiDAR传感器，导致在LiDAR缺失时性能大幅下降，同时难以在NPU和FPGA等多样化硬件平台上部署。&lt;h4&gt;目的&lt;/h4&gt;解决多模态3D物体检测器对LiDAR的过度依赖问题，提高检测系统在实际场景中的鲁棒性和安全性，创建一个能在多样化硬件平台上部署的检测器。&lt;h4&gt;方法&lt;/h4&gt;重新考虑LiDAR在相机-LiDAR融合范式中的作用，提出LiteFusion检测器，将LiDAR点云视为增强基于相机检测的几何信息的补充来源，而不是独立模态。在四元数空间内将LiDAR的补充特征集成到图像特征中，利用正交约束保留模态间的关系，形成紧凑的跨模态嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上，LiteFusion将基线基于视觉的检测器的mAP提高了20.4%，NDS提高了19.7%，参数仅增加1.1%，且无需专用的LiDAR编码器。即使在没有LiDAR输入的情况下，LiteFusion也能保持良好的结果。&lt;h4&gt;结论&lt;/h4&gt;LiteFusion具有有利的鲁棒性和有效性，适用于多样化的融合范式和部署场景，通过消除对3D骨干网络的依赖，提高了部署友好性。&lt;h4&gt;翻译&lt;/h4&gt;3D物体检测对安全稳健的智能交通系统至关重要。当前多模态3D物体检测器通常依赖复杂架构和训练策略来获得更高检测精度。然而，这些方法严重依赖LiDAR传感器，导致在LiDAR缺失时性能大幅下降，这损害了自主系统在实际场景中的鲁棒性和安全性。此外，由于依赖主要针对NVIDIA GPU优化的3D稀疏卷积算子，现有多模态检测器难以在NPU和FPGA等多样化硬件平台上部署。为解决这些挑战，我们重新考虑了LiDAR在相机-LiDAR融合范式中的作用，并引入了一种新型的多模态3D检测器LiteFusion。LiteFusion不将LiDAR点云视为具有独立特征提取骨干的独立模态，而是利用LiDAR数据作为增强基于相机检测的几何信息的补充来源。这种直接的方法完全消除了对3D骨干网络的依赖，使该方法具有高度部署友好性。具体而言，LiteFusion在四元数空间内将LiDAR的补充特征集成到图像特征中，在网络训练期间保持良好的正交约束。这有助于建模跨模态的特定领域关系，产生紧凑的跨模态嵌入。在nuScenes数据集上的实验表明，LiteFusion将基线基于视觉的检测器的mAP提高了20.4%，NDS提高了19.7%，参数仅增加1.1%，且无需专用的LiDAR编码器。值得注意的是，即使在没有LiDAR输入的情况下，LiteFusion也能保持良好的结果，突显了其在多样化融合范式和部署场景中有利的鲁棒性和有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有多模态3D目标检测器过度依赖LiDAR传感器的问题，以及难以部署在多样化硬件平台上的挑战。当LiDAR数据缺失时，现有方法性能大幅下降，影响自动驾驶系统的鲁棒性和安全性；同时，它们依赖专为NVIDIA GPU优化的3D稀疏卷积算子，在NPU、FPGA等其他平台上部署困难。这些问题限制了自动驾驶系统在实际生产环境中的应用和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者重新思考了LiDAR在相机-LiDAR融合范式中的角色，提出将LiDAR数据视为增强相机检测的补充几何信息来源，而非独立模态。他们从相机检测器出发，用最小结构调整使其适应多模态检测，并开发了策略解决3D LiDAR几何与2D视觉信息间的域差距。方法借鉴了四元数代数处理跨模态融合，使用BEVFormer作为基础相机检测器，并参考了现有的特征融合方法，但提出了不同的单流架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将LiDAR数据视为增强相机检测的补充几何信息来源，而非需要单独特征提取的独立模态，消除对3D特征提取骨干网络的依赖。整体流程包括：1)使用渐进式响应框架将双流网络统一为单流；2)设计LiDAR几何集成器(LGI)，包含深度感知嵌入(DAE)和几何感知嵌入(GAE)；3)在DAE中使用四元数特征适应(Qua-FA)建模正交关系；4)将LiDAR数据投影到PV和BEV格式，通过模块生成几何特征并层层整合到相机特征中；5)增强的特征传递到检测头生成结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的相机辅助LiDAR融合方案，消除复杂的点云骨干网络；2)四元数空间嵌入实现参数高效的跨模态融合；3)渐进式响应框架逐步集成几何信息；4)完全基于标准算子，无需3D稀疏卷积。相比之前工作，LiteFusion使用单流而非双流架构，重新定义LiDAR角色，无需3D稀疏卷积，在LiDAR缺失时保持更强鲁棒性，且参数效率更高(仅增加1.1%参数)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiteFusion通过重新定义LiDAR角色和使用四元数空间融合，实现了无需3D稀疏卷积的高效、鲁棒且易于部署的相机-LiDAR融合框架，在保持最小参数增加的同时显著提升了3D目标检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.</description>
      <author>example@mail.com (Xiangxuan Ren, Zhongdao Wang, Pin Tang, Guoqing Wang, Jilai Zheng, Chao Ma)</author>
      <guid isPermaLink="false">2512.20217v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>The Seismic Wavefield Common Task Framework</title>
      <link>http://arxiv.org/abs/2512.19927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个地震波场机器学习的通用任务框架(CTF)，旨在解决地震学在状态预测和重建方面的挑战，以及管理参数变异性问题。该框架提供结构化和严格的评估基础，包含多种规模的数据集和特定任务指标，有助于提高科学机器学习的严格性和可重复性。&lt;h4&gt;背景&lt;/h4&gt;地震学在状态预测和重建（如地震早期预警和地面运动预测）以及管理源位置、机制和地球模型的参数变异性方面面临基本挑战。模拟方法受到大规模数据量和复杂性的限制，而实际数据工作则受到不充分反映地球复杂性的模型和稀疏传感器测量的限制。&lt;h4&gt;目的&lt;/h4&gt;引入一个通用任务框架(CTF)用于地震波场的机器学习，提供结构化和严格的算法评估基础，取代临时的比较方法，提高科学机器学习的严格性和可重复性。&lt;h4&gt;方法&lt;/h4&gt;开发了一个包含三个不同波场数据集的CTF，提供全球、地壳和局部等多种规模的精选数据集，以及涵盖预测、重建和现实约束下泛化的特定任务指标。受自然语言处理等领域CTF的启发，该框架为算法的面对面评估提供结构化基础。研究展示了两个数据集的评估程序，报告了各种方法和基础模型在重建地震波场方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;CTF分数揭示了不同方法的优点、局限性和特定问题类的适用性，为地震波场机器学习提供了有价值的基准评估。&lt;h4&gt;结论&lt;/h4&gt;通过在隐藏测试集上进行标准化评估，CTF框架有助于提高地震学机器学习研究的严格性和可重复性，取代了以往临时的比较方法。&lt;h4&gt;翻译&lt;/h4&gt;地震学在状态预测和重建（例如地震早期预警和地面运动预测）以及管理源位置、机制和地球模型（例如地下结构和地形效应）的参数变异性方面面临基本挑战。通过模拟解决这些问题受到合成数据量和数值复杂性的大规模限制，而实际数据工作则受到无法充分反映地球复杂性的模型和现场稀疏传感器测量的限制。最近的机器学习(ML)努力有希望，但进展因缺乏适当的表征、公平的报告和严格的比较而模糊不清。为此，我们引入了一个用于地震波场机器学习的通用任务框架(CTF)，从三个不同的波场数据集开始。我们的CTF包含各种规模（全球、地壳和局部）的精选数据集集和特定任务的指标，涵盖预测、重建和现实约束（如噪声和有限数据）下的泛化。受自然语言处理等领域CTF的启发，该框架为算法的面对面评估提供结构化和严格的基础。我们通过报告两个数据集的分数来说明评估程序，展示了各种方法和基础模型在从模拟和真实世界传感器测量重建地震波场的性能。CTF分数揭示了不同方法的优点、局限性和特定问题类的适用性。我们的愿景是用隐藏测试集上的标准化评估取代临时的比较，提高科学机器学习的严格性和可重复性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth's complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.</description>
      <author>example@mail.com (Alexey Yermakov, Yue Zhao, Marine Denolle, Yiyu Ni, Philippe M. Wyder, Judah Goldfeder, Stefano Riva, Jan Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz)</author>
      <guid isPermaLink="false">2512.19927v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</title>
      <link>http://arxiv.org/abs/2512.19909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为CGM-FAS的深度学习方法，作为基于高斯过程的非平稳地震动模型的替代方案，用于模拟傅里叶振幅谱中的非平稳路径效应。该方法通过条件变分自编码器架构直接从数据中学习空间模式和频率间相关性，无需预设相关函数，且计算效率高。&lt;h4&gt;背景&lt;/h4&gt;非平稳地震动模型(GMMs)能够明确模拟源、场地和路径效应中的系统性空间变化，将标准差降低到平稳模型的30-40%，使更准确的场地特定地震危险性分析成为可能。然而，当前基于高斯过程(GP)的非平稳GMM方法存在计算限制，难以应用于大规模预测。&lt;h4&gt;目的&lt;/h4&gt;提出一种深度学习方法(CGM-FAS)作为基于GP的方法的替代方案，用于模拟傅里叶振幅谱(FAS)中的非平稳路径效应，解决现有方法在大规模预测中的计算效率问题。&lt;h4&gt;方法&lt;/h4&gt;CGM-FAS使用条件变分自编码器架构，通过使用地震和站点的地理坐标作为条件变量，直接从数据中学习空间模式和频率间相关性。研究使用旧金山湾区的地震数据对模型进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;CGM-FAS能够一致地预测非平稳路径效应，相比基于GP的方法具有三大优势：无需预设相关函数即可学习空间模式；能够捕获频率间相关性；计算效率高，可在几GB内存的情况下，10秒内为1,000个频率上的10,000个站点生成预测地图。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了在多个频率和大空间域内高效进行非平稳地震动预测的有前景的方向，为地震危险性分析提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;近期非平稳地震动模型(GMMs)的发展明确模拟了源、场地和路径效应中的系统性空间变化，将标准差降低到平稳模型的30-40%，使得更准确的场地特定地震危险性分析成为可能。当前非平稳GMM依赖于具有预设相关函数的高斯过程(GP)方法，因此在大规模预测中存在计算限制。本研究提出了一种名为条件生成傅里叶振幅谱模型(CGM-FAS)的深度学习方法，作为基于GP的方法的替代方案，用于模拟傅里叶振幅谱(FAS)中的非平稳路径效应。CGM-FAS使用条件变分自编码器架构，通过使用地震和站点的地理坐标作为条件变量，直接从数据中学习空间模式和频率间相关性。使用旧金山湾区地震数据，我们将CGM-FAS与该地区最近的基于GP的GMM进行比较，证明了CGM-FAS能够一致地预测非平稳路径效应。此外，与基于GP的方法相比，CGM-FAS在学习空间模式时无需预设相关函数，能够捕获频率间相关性，并能实现快速预测，在几GB内存的情况下，10秒内即可为1,000个频率上的10,000个站点生成地图。CGM-FAS的超参数可以调整，以确保生成的路径效应表现出与基于GP的经验GMM一致的变异性。这项工作展示了在多个频率和大空间域内高效进行非平稳地震动预测的有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.</description>
      <author>example@mail.com (Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney)</author>
      <guid isPermaLink="false">2512.19909v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents</title>
      <link>http://arxiv.org/abs/2512.20092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Memory-T1是一个使用强化学习学习时间感知记忆选择策略的框架，通过粗到细策略从长对话历史中选择相关证据，解决了长对话中时间推理困难的问题。&lt;h4&gt;背景&lt;/h4&gt;在长多轮对话中进行时间推理是对话代理的关键能力，但现有研究表明，随着对话历史长度增加和噪声积累，当前长上下文模型难以准确识别时间相关信息，严重损害推理性能。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架来解决长对话中的时间推理问题，提高模型在长对话历史中识别时间相关信息的能力。&lt;h4&gt;方法&lt;/h4&gt;Memory-T1框架使用强化学习学习时间感知的记忆选择策略，采用粗到细策略，先使用时间和相关性过滤器将对话历史剪枝为候选集，然后由RL代理选择精确证据会话。RL训练由多级奖励函数指导，优化答案准确性、证据基础和时间一致性，时间一致性奖励通过评估与查询时间范围的alignment解决时间歧义。&lt;h4&gt;主要发现&lt;/h4&gt;在Time-Dialog基准测试上，Memory-T1将7B模型性能提升到67.0%，超过14B基线模型10.2%；消融研究表明时间一致性和证据基础奖励共同贡献15.0%性能提升；Memory-T1在高达128k tokens情况下保持鲁棒性，而基线模型崩溃。&lt;h4&gt;结论&lt;/h4&gt;Memory-T1有效解决了长对话中的时间推理问题，在开源模型中建立了新的最先进性能，代码和数据集已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;在长多轮对话中进行时间推理是对话代理的关键能力。然而，现有工作和我们的初步研究表明，随着对话历史长度增加和噪声积累，当前长上下文模型难以准确识别时间相关信息，严重损害推理性能。为此，我们引入了Memory-T1，这是一个使用强化学习学习时间感知记忆选择策略的框架。它采用粗到细策略，首先使用时间和相关性过滤器将对话历史剪枝为候选集，然后由RL代理选择精确的证据会话。RL训练由多级奖励函数指导，优化(i)答案准确性，(ii)证据基础，和(iii)时间一致性。特别是，时间一致性奖励通过在会话级别（时间接近度）和话语级别（时间保真度）评估与查询时间范围的alignment，提供密集信号，使代理能够解决细微的时间歧义。在Time-Dialog基准测试上，Memory-T1将7B模型提升到总体得分67.0%，为开源模型建立了新的最先进性能，并超过14B基线模型10.2%。消融研究表明时间一致性和证据基础奖励共同贡献15.0%的性能提升。此外，Memory-T1在高达128k tokens的情况下保持鲁棒性，而基线模型崩溃，证明了其在广泛对话历史中对噪声的有效性。代码和数据集可在https://github.com/Elvin-Yiming-Du/Memory-T1/公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/</description>
      <author>example@mail.com (Yiming Du, Baojun Wang, Yifan Xiang, Zhaowei Wang, Wenyu Huang, Boyang Xue, Bin Liang, Xingshan Zeng, Fei Mi, Haoli Bai, Lifeng Shang, Jeff Z. Pan, Yuxin Jiang, Kam-Fai Wong)</author>
      <guid isPermaLink="false">2512.20092v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks</title>
      <link>http://arxiv.org/abs/2512.20431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Authors' version of the paper published in proceedings of ECCE, DOI: https://doi.org/10.1109/ECCE64574.2025.11013422&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用CNN软投票集成进行早期皮肤癌分类的方法，通过混合双编码器进行分割，并结合MobileNetV2、VGG19和InceptionV3三个模型进行分类，在三个基准数据集上实现了高准确率。&lt;h4&gt;背景&lt;/h4&gt;皮肤癌可以通过皮肤镜检查和眼科检查来识别，但早期检测可以显著提高生存机会。人工智能（AI）使用带注释的皮肤图像和卷积神经网络（CNNs）可以提高诊断准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种使用CNN软投票集成进行早期皮肤癌分类的方法，提高诊断准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;使用三个基准数据集（HAM10000、ISIC 2016和ISIC 2019），应用重新平衡、图像增强和过滤技术，采用混合双编码器通过迁移学习进行分割，并通过MobileNetV2、VGG19和InceptionV3的集成进行分类，平衡准确性和速度以实现实际部署。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上实现了病变识别准确率：96.32%（HAM10000）、90.86%（ISIC 2016）和93.92%（ISIC 2019），使用既定的皮肤病变检测指标评估系统性能，取得了令人印象深刻的结果。&lt;h4&gt;结论&lt;/h4&gt;通过准确的分割，分类模型能够专注于临床显著特征，减少背景伪影，提高准确性，为皮肤癌的早期检测提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;皮肤癌可以通过皮肤镜检查和眼科检查来识别，但早期检测显著提高生存机会。人工智能（AI）使用带注释的皮肤图像和卷积神经网络（CNNs）提高诊断准确性。本文提出了一种使用CNN软投票集成进行早期皮肤癌分类的方法。本研究使用了三个基准数据集：HAM10000、ISIC 2016和ISIC 2019。过程涉及重新平衡、图像增强和过滤技术，随后通过迁移学习使用混合双编码器进行分割。准确的分割使分类模型能够专注于临床显著特征，减少背景伪影，提高准确性。分类通过MobileNetV2、VGG19和InceptionV3的集成进行，平衡准确性和速度以实现实际部署。该方法在三个数据集上实现了病变识别准确率：96.32%、90.86%和93.92%。系统性能使用既定的皮肤病变检测指标进行了评估，取得了令人印象深刻的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ECCE64574.2025.11013422&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\%, 90.86\%, and 93.92\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.</description>
      <author>example@mail.com (Abdullah Al Shafi, Abdul Muntakim, Pintu Chandra Shill, Rowzatul Zannat, Abdullah Al-Amin)</author>
      <guid isPermaLink="false">2512.20431v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images</title>
      <link>http://arxiv.org/abs/2512.20374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, BMVC 2025 submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于CLIP模型的自动化BBPS评分框架，通过融合全局视觉特征与粪便相关文本先验，提高了肠道清洁度评估的准确性，无需显式分割即可实现。&lt;h4&gt;背景&lt;/h4&gt;准确的肠道清洁度评估对结肠镜检查至关重要，但现有的波士顿肠道准备量表(BBPS)评分系统在手动执行时存在主观性和观察者间变异性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化的BBPS评分框架，减少主观性并提高肠道清洁度评估的准确性。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含2240张图像的高质量结肠镜数据集，来自517名受试者并标注了专家一致的BBPS评分；提出了一种新颖的自动化BBPS评分框架，利用基于适配器的迁移学习CLIP模型和专门的粪便特征提取分支，融合全局视觉特征与粪便相关文本先验。&lt;h4&gt;主要发现&lt;/h4&gt;在自建数据集和公共NERHU数据集上的广泛实验表明，该方法优于现有基线，突显了其在计算机辅助结肠镜分析中的临床应用潜力。&lt;h4&gt;结论&lt;/h4&gt;该方法能够更准确地评估肠道清洁度，有望用于临床实践，提高结肠镜检查的质量和效率。&lt;h4&gt;翻译&lt;/h4&gt;准确的肠道清洁度评估对有效的结肠镜检查至关重要。波士顿肠道准备量表(BBPS)提供了一种标准化的评分系统，但在手动执行时存在主观性和观察者间变异性问题。在本文中，为了支持稳健的训练和评估，我们构建了一个高质量的结肠镜数据集，包含来自517名受试者的2240张图像，并标注了专家一致的BBPS评分。我们提出了一种新颖的自动化BBPS评分框架，利用基于适配器的迁移学习CLIP模型和专门的粪便特征提取分支。我们的方法融合了全局视觉特征与粪便相关的文本先验，提高了肠道清洁度评估的准确性，而无需显式分割。在我们数据集和公共NERHU数据集上的广泛实验证明了我们方法优于现有基线，突显了其在计算机辅助结肠镜分析中临床部署的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.</description>
      <author>example@mail.com (Yujia Fu, Zhiyu Dong, Tianwen Qian, Chenye Zheng, Danian Ji, Linhai Zhuo)</author>
      <guid isPermaLink="false">2512.20374v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models</title>
      <link>http://arxiv.org/abs/2512.20021v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过利用元数据和计算机实验方法来指导机器学习数据采集的策略，以提高模型性能，特别是在代表性不足的条件下。&lt;h4&gt;背景&lt;/h4&gt;收集操作上真实的数据来训练机器学习模型成本高昂，且在收集新数据前了解模型的不足之处非常重要。例如，在稀有物体图像上训练的目标检测器可能在代表性不足的条件下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，通过利用描述训练数据收集背景的元数据（如季节、时间、地点）来指导后续数据采集，从而最大化模型性能。&lt;h4&gt;方法&lt;/h4&gt;通过根据元数据变化训练数据来评估学习器，然后拟合一个高斯过程代理模型来响应这一变化，从而指导新的数据采集。这是一种元学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;这种元学习方法相比随机选择元数据的数据，可以提高学习器性能，这在经典学习示例和一个涉及收集航空图像以搜索飞机的实际应用中得到了验证。&lt;h4&gt;结论&lt;/h4&gt;通过利用元数据和计算机实验工具包来指导数据采集，可以有效提高机器学习模型在代表性不足条件下的性能。&lt;h4&gt;翻译&lt;/h4&gt;收集操作上真实的数据来支持机器学习模型可能成本高昂。在收集新数据之前，了解模型的不足之处是有帮助的。例如，在稀有物体图像上训练的目标检测器可能在代表性不足的条件下表现不佳。我们提出了一种方法，通过利用计算机实验工具包和描述训练数据收集背景的元数据（如季节、一天中的时间、位置）来指导后续的数据采集，以最大化模型性能。我们通过根据元数据变化训练数据来评估学习器，然后拟合一个高斯过程代理模型来响应这一变化，从而指导新的数据采集。这种元学习方法相比随机选择元数据的数据，可以提高学习器性能，作者在经典学习示例和一个涉及收集航空图像以搜索飞机的实际应用中证明了这一点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collecting operationally realistic data to inform machine learning models can be costly. Before collecting new data, it is helpful to understand where a model is deficient. For example, object detectors trained on images of rare objects may not be good at identification in poorly represented conditions. We offer a way of informing subsequent data acquisition to maximize model performance by leveraging the toolkit of computer experiments and metadata describing the circumstances under which the training data was collected (e.g., season, time of day, location). We do this by evaluating the learner as the training data is varied according to its metadata. A Gaussian process (GP) surrogate fit to that response surface can inform new data acquisitions. This meta-learning approach offers improvements to learner performance as compared to data with randomly selected metadata, which we illustrate on both classic learning examples, and on a motivating application involving the collection of aerial images in search of airplanes.</description>
      <author>example@mail.com (Anna R. Flowers, Christopher T. Franck, Robert B. Gramacy, Justin A. Krometis)</author>
      <guid isPermaLink="false">2512.20021v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Regression of Functions by Quantum Neural Networks Circuits</title>
      <link>http://arxiv.org/abs/2512.19978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于遗传算法的框架，用于自动构建量子神经网络架构，特别关注回归任务。研究发现量子模型可以在保持紧凑的同时提供与经典模型相当的性能，并且数据集复杂度指标可以可靠地预测最佳量子架构。&lt;h4&gt;背景&lt;/h4&gt;量子神经网络模型的性能很大程度上取决于架构决策，包括电路深度、参数化操作的放置和数据编码策略。选择有效架构具有挑战性，且与经典神经网络拓扑选择这一计算难题密切相关。&lt;h4&gt;目的&lt;/h4&gt;研究自动化量子电路构建方法，用于回归任务，引入一种遗传算法框架来发现简化的回归器量子神经网络架构，并分析数据集复杂度对量子架构选择的影响。&lt;h4&gt;方法&lt;/h4&gt;提出一种遗传算法框架，探索电路深度、参数化门配置和灵活的数据重新上传模式，将量子回归器的构建表述为优化过程。在22个非线性基准函数和4个解析函数上与17个经典回归模型进行比较，使用12个结构描述符分析数据集复杂度，并在五个递增难度的元学习场景中测试这些度量。&lt;h4&gt;主要发现&lt;/h4&gt;尽管经典方法通常能达到相似结果，但需要更多参数，而进化后的量子模型保持紧凑同时提供有竞争力的性能。数据集复杂度指标可以可靠地预测哪种量子架构表现最佳，在某些场景中表现出完美或接近完美的预测准确性。&lt;h4&gt;结论&lt;/h4&gt;该研究为基于元学习的量子架构设计提供了理论基础，增进了对量子模型在回归设置中行为的理解，这些发现为更系统的、理论基础的量子回归方法铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;量子神经网络模型的性能在很大程度上取决于架构决策，包括电路深度、参数化操作的放置和数据编码策略。选择有效架构具有挑战性，且与经典神经网络拓扑选择这一计算难题密切相关。本研究探讨了用于回归任务的自动化量子电路构建，并引入了一种遗传算法框架，用于发现简化的回归器量子神经网络架构。该方法探索深度、参数化门配置和灵活的数据重新上传模式，将量子回归器的构建表述为优化过程。在22个非线性基准函数和4个解析函数上评估发现的电路，并与17个经典回归模型进行比较。尽管经典方法通常能达到相似结果，但它们通常需要更多参数，而进化后的量子模型在保持紧凑的同时提供有竞争力的性能。我们进一步使用12个结构描述符分析数据集复杂度，并表明，在五个递增难度的元学习场景中，这些度量可以可靠地预测哪种量子架构将表现最佳。在几个场景中，结果表现出完美或接近完美的预测准确性，表明复杂度指标提供了数据集结构的强大而紧凑的表示，并可以有效地指导自动化模型选择。总体而言，本研究为基于元学习的量子架构设计提供了理论基础，并增进了对量子模型在回归设置中行为的理解——这一主题在先前工作中受到有限探索。这些发现为更系统和理论基础的量子回归方法铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of quantum neural network models depends strongly on architectural decisions, including circuit depth, placement of parametrized operations, and data-encoding strategies. Selecting an effective architecture is challenging and closely related to the classical difficulty of choosing suitable neural-network topologies, which is computationally hard. This work investigates automated quantum-circuit construction for regression tasks and introduces a genetic-algorithm framework that discovers Reduced Regressor QNN architectures. The approach explores depth, parametrized gate configurations, and flexible data re-uploading patterns, formulating the construction of quantum regressors as an optimization process. The discovered circuits are evaluated against seventeen classical regression models on twenty-two nonlinear benchmark functions and four analytical functions. Although classical methods often achieve comparable results, they typically require far more parameters, whereas the evolved quantum models remain compact while providing competitive performance. We further analyze dataset complexity using twelve structural descriptors and show, across five increasingly challenging meta-learning scenarios, that these measures can reliably predict which quantum architecture will perform best. The results demonstrate perfect or near-perfect predictive accuracy in several scenarios, indicating that complexity metrics offer powerful and compact representations of dataset structure and can effectively guide automated model selection. Overall, this study provides a principled basis for meta-learning-driven quantum architecture design and advances the understanding of how quantum models behave in regression settings--a topic that has received limited exploration in prior work. These findings pave the way for more systematic and theoretically grounded approaches to quantum regression.</description>
      <author>example@mail.com (Fernando M. de Paula Neto, Lucas dos Reis Silva, Paulo S. G. de Mattos Neto, Felipe F. Fanchini)</author>
      <guid isPermaLink="false">2512.19978v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Tuned In-Context Learners for Efficient Adaptation</title>
      <link>http://arxiv.org/abs/2512.19879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了一种统一方法，将上下文学习直接整合到微调过程中，结合了两种主流大型语言模型适应方法的优点&lt;h4&gt;背景&lt;/h4&gt;在将大型语言模型适应特定下游任务时，主要有两种方法：提示工程(利用模型固有泛化能力)和微调(直接优化模型参数)，但各自在数据量不同的情况下表现各异&lt;h4&gt;目的&lt;/h4&gt;研究一种统一方法，将上下文学习和微调两种范式结合起来，克服各自在数据量不同情况下的局限性&lt;h4&gt;方法&lt;/h4&gt;在特定任务数据上微调模型，这些数据通过添加上下文示例进行增强以模拟k-shot提示结构；使用预评估方法进行低数据量下的超参数选择，避免交叉验证同时利用所有可用数据&lt;h4&gt;主要发现&lt;/h4&gt;统一方法虽然需要每个任务进行微调，但结合了上下文学习的样本效率和微调的性能提升，能够持续匹配并通常显著超过两种基线方法&lt;h4&gt;结论&lt;/h4&gt;通过广泛实证研究比较了微调、上下文学习和统一方法在具体数据下游任务上的预测性能&lt;h4&gt;翻译&lt;/h4&gt;当将大型语言模型适应特定下游任务时，通常采用两种主要方法：(1)提示工程，通常结合上下文少样本学习，利用模型的固有泛化能力；(2)在特定任务数据上进行微调，直接优化模型参数。虽然基于提示的方法在少样本场景下表现出色，但随着数据增加，其效果往往趋于平稳。相反，微调方法能很好地随数据扩展，但在训练样本稀少时可能表现不佳。我们研究了一种统一方法，通过将上下文学习直接整合到微调过程中来桥接这两种范式。具体来说，我们在添加了上下文示例的特定任务数据上微调模型，模拟k-shot提示的结构。这种方法虽然需要每个任务进行微调，但结合了上下文学习的样本效率和微调的性能提升，使其能够持续匹配并通常显著超过这两种基线方法。为了在低数据量下进行超参数选择，我们提出使用预评估方法，消除了昂贵的交叉验证需求，同时利用所有可用数据进行训练并提供稳健的验证信号。我们进行了广泛的实证研究，以确定哪种适应范式——微调、上下文学习或我们提出的统一方法——在具体数据的下游任务上提供最佳的预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model's inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model's parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.</description>
      <author>example@mail.com (Jorg Bornschein, Clare Lyle, Yazhe Li, Amal Rannen-Triki, Xu Owen He, Razvan Pascanu)</author>
      <guid isPermaLink="false">2512.19879v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model</title>
      <link>http://arxiv.org/abs/2512.20157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 8 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性地研究了视觉基础模型的多教师蒸馏方法，提出了AMoE模型，并确定了三种关键因素来提高训练效率，同时创建了OpenLVD200M数据集并发布了相关模型。&lt;h4&gt;背景&lt;/h4&gt;通过多教师蒸馏训练的视觉基础模型为统一的视觉表示提供了有前途的路径，但这类方法的学习动态性和数据效率仍然未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;系统地研究视觉基础模型的多教师蒸馏，并确定能够以较低计算成本进行训练的关键因素。&lt;h4&gt;方法&lt;/h4&gt;引入Agglomerative Mixture-of-Experts视觉基础模型(AMoE)，同时将SigLIP2和DINOv3的知识蒸馏到Mixture-of-Experts学生模型中。&lt;h4&gt;主要发现&lt;/h4&gt;1) 非对称关系知识蒸馏损失函数保留了每个教师的几何特性，同时实现有效的知识转移；2) 令牌平衡批处理将不同分辨率的图像打包成具有统一令牌预算的序列，稳定了不同分辨率的表示学习；3) 训练数据的层次聚类和采样相比随机采样显著提高了多教师蒸馏的样本效率。&lt;h4&gt;结论&lt;/h4&gt;通过结合这些发现，整理了OpenLVD200M，一个2亿图像语料库，展示了多教师蒸馏的卓越效率，并在Mixture-of-Experts架构中实例化，同时发布了OpenLVD200M数据集和蒸馏模型。&lt;h4&gt;翻译&lt;/h4&gt;通过多教师蒸馏训练的视觉基础模型为统一的视觉表示提供了一条有前途的路径，但此类方法的学习动态性和数据效率仍然未被充分探索。在本文中，我们系统性地研究了视觉基础模型的多教师蒸馏，并确定了能够以较低计算成本进行训练的关键因素。我们引入了聚合式专家混合视觉基础模型(AMoE)，该模型同时将SigLIP2和DINOv3的知识蒸馏到专家混合学生模型中。我们证明：(1)我们的非对称关系知识蒸馏损失函数保留了每个教师的几何特性，同时实现了有效的知识转移；(2)令牌平衡批处理将不同分辨率的图像打包成具有统一令牌预算的序列，在不牺牲性能的情况下稳定了不同分辨率的表示学习；(3)训练数据的层次聚类和采样（通常保留给自监督学习）相比随机采样显著提高了多教师蒸馏的样本效率。通过结合这些发现，我们整理了OpenLVD200M，一个2亿图像语料库，展示了多教师蒸馏的卓越效率。在专家混合架构中实例化。我们发布了OpenLVD200M和蒸馏模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.</description>
      <author>example@mail.com (Sofian Chaybouti, Sanath Narayan, Yasser Dahou, Phúc H. Lê Khac, Ankit Singh, Ngoc Dung Huynh, Wamiq Reyaz Para, Hilde Kuehne, Hakim Hacid)</author>
      <guid isPermaLink="false">2512.20157v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</title>
      <link>http://arxiv.org/abs/2512.20145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE/ACM Transactions on Audio, Speech and Language Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RetroPrompt是一种新型提示学习方法，通过利用知识库和检索机制，解决了传统提示学习中记忆与泛化平衡的问题，在零样本和少样本场景下表现优越。&lt;h4&gt;背景&lt;/h4&gt;预训练基础模型已成为促进大规模多模态学习的关键工具。研究人员通过提示学习采用'预训练、提示、预测'范式改进少样本性能，但传统提示学习方法遵循参数化学习范式，可能导致记忆和机械学习的泛化稳定性受影响，且在充分利用非典型实例和避免有限数据下的过拟合方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;克服传统提示学习的局限性，实现记忆和泛化之间的平衡，将知识与单纯的记忆解耦。&lt;h4&gt;方法&lt;/h4&gt;提出名为RetroPrompt的方法，利用从训练数据生成的公开可访问的知识库，在输入、训练和推理阶段都纳入检索机制，使模型能够主动从语料库中检索相关的上下文信息，增强可用线索。&lt;h4&gt;主要发现&lt;/h4&gt;在自然语言处理和计算机视觉任务的各种数据集上进行了全面实验，RetroPrompt在零样本和少样本场景下表现出优越性能。通过对记忆模式的详细分析，观察到RetroPrompt有效减少了对机械记忆的依赖，导致泛化能力增强。&lt;h4&gt;结论&lt;/h4&gt;RetroPrompt通过解耦知识与单纯记忆，有效平衡了记忆和泛化，减少了模型对机械记忆的依赖，从而提高了泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;预训练基础模型已成为促进大规模多模态学习的关键工具。研究人员通过提示学习有效采用'预训练、提示、预测'范式，改进了少样本性能。然而，针对PFMs的提示学习方法仍然遵循参数化学习范式。因此，记忆和机械学习中的泛化稳定性可能会受到影响。更具体地说，传统提示学习在充分利用非典型实例和避免在有限数据下的完全监督训练过程中对浅层模式的过拟合方面可能面临困难。为了克服这些限制，我们提出了名为RetroPrompt的方法，旨在通过将知识与单纯记忆解耦来实现记忆与泛化之间的平衡。与传统提示方法不同，RetroPrompt利用从训练数据生成的公开可访问的知识库，并在输入、训练和推理阶段都纳入了检索机制。这使得模型能够主动从语料库中检索相关的上下文信息，从而增强可用线索。我们在自然语言处理和计算机视觉任务的各种数据集上进行了全面实验，证明了我们提出的方法RetroPrompt在零样本和少样本场景下的优越性能。通过对记忆模式的详细分析，我们观察到RetroPrompt有效减少了对机械记忆的依赖，从而提高了泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TASLPRO.2025.3608936&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.</description>
      <author>example@mail.com (Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang)</author>
      <guid isPermaLink="false">2512.20145v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Reason2Decide: Rationale-Driven Multi-Task Learning</title>
      <link>http://arxiv.org/abs/2512.20074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Reason2Decide是一个创新的两阶段训练框架，解决了临床决策支持系统中预测准确性和解释一致性的关键挑战。该框架通过处理暴露偏差和任务分离问题，在各种医疗数据集上表现出色，同时使用比当代基础模型小40倍的模型实现高性能，使其成为资源受限环境中的理想解决方案。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型已被广泛采用，但临床决策支持系统面临关键挑战：在生成与预测一致的解释的同时实现高预测准确性。当前方法存在暴露偏差问题，导致解释与预测不一致。&lt;h4&gt;目的&lt;/h4&gt;解决自理性化中的关键挑战，包括暴露偏差和任务分离，提出一个两阶段训练框架，能够同时提高预测准确性和解释一致性。&lt;h4&gt;方法&lt;/h4&gt;提出了名为Reason2Decide的两阶段训练框架：第一阶段在推理生成上训练模型；第二阶段联合训练标签预测和推理生成，使用计划采样技术，逐渐从基于真实标签的条件化过渡到基于模型预测的条件化。在三个医疗数据集上评估，包括专用的分诊数据集和公开的生物医学问答数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在不同模型规模下，Reason2Decide在预测（F1）和推理保真度方面优于其他微调基线和一些零样本LLM；在分诊任务中，对多种推理源具有鲁棒性；仅使用LLM生成的推理进行预训练就优于其他微调变体，表明减少了对人工标注的依赖；使用比当代基础模型小40倍的模型实现了这些提升。&lt;h4&gt;结论&lt;/h4&gt;Reason2Decide使临床推理在资源受限部署中更易于访问，同时仍提供可解释的决策支持。该框架能够在保持高性能的同时，大幅减少模型大小，提高临床决策支持系统的实用性。&lt;h4&gt;翻译&lt;/h4&gt;尽管大型语言模型被广泛采用，临床决策支持系统仍面临一个关键挑战：在生成与预测一致解释的同时实现高预测准确性。当前方法因存在暴露偏差而导致解释与预测不一致。我们提出了Reason2Decide，一个两阶段训练框架，解决了自理性化中的关键挑战，包括暴露偏差和任务分离。在第一阶段，我们的模型在推理生成上训练；在第二阶段，我们联合训练标签预测和推理生成，应用计划采样技术，逐渐从基于真实标签的条件化过渡到基于模型预测的条件化。我们在三个医疗数据集上评估Reason2Decide，包括专用的分诊数据集和公开的生物医学问答数据集。在不同模型规模下，Reason2Decide在预测（F1）和推理保真度方面优于其他微调基线和一些零样本LLM。在分诊任务中，Reason2Decide对LLM生成、护士编写和护士后处理的推理具有推理源鲁棒性。在我们的实验中，尽管仅在第一阶段使用LLM生成的推理，Reason2Decide仍优于其他微调变体。这表明LLM生成的推理适合用于模型预训练，减少对人工标注的依赖。值得注意的是，Reason2Decide使用比当代基础模型小40倍的模型实现了这些提升，使临床推理在资源受限部署中更易于访问，同时仍提供可解释的决策支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.</description>
      <author>example@mail.com (H M Quamran Hasan, Housam Khalifa Bashier, Jiayi Dai, Mi-Young Kim, Randy Goebel)</author>
      <guid isPermaLink="false">2512.20074v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification</title>
      <link>http://arxiv.org/abs/2512.19982v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为窗口尺度衰减多实例学习(WSD-MIL)的新方法，用于解决计算病理学中全切片图像分析的问题。该方法通过窗口尺度衰减注意力和挤压-激励区域门控模块，有效处理了不同尺度肿瘤区域的建模，同时提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;近年来，预训练基础模型与多实例学习(MIL)的结合提高了计算病理学中的诊断准确性。然而，现有MIL方法专注于优化特征提取器和聚合策略，忽略了全切片图像中实例间的复杂语义关系。基于Transformer的MIL方法虽能建模实例依赖关系，但其二次计算复杂度限制了在大规模WSI上的应用。此外，不同WSI中肿瘤区域规模的显著变化使得固定尺度注意力方法难以精确捕获局部实例相关性，也无法考虑补丁相关性的距离衰减效应。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够增强对变尺度肿瘤区域建模能力同时提高计算效率的方法，解决现有基于Transformer的MIL方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出窗口尺度衰减多实例学习(WSD-MIL)，包含两个主要模块：1)基于窗口尺度衰减的注意力模块，采用基于聚类的采样策略降低计算成本，同时逐步衰减注意力窗口尺度以捕获不同尺度的局部实例关系；2)基于挤压和激励的区域门控模块，动态调整窗口权重以增强全局信息建模。&lt;h4&gt;主要发现&lt;/h4&gt;WSD-MIL在CAMELYON16和TCGA-BRCA数据集上实现了最先进的性能，同时减少了62%的计算内存消耗。&lt;h4&gt;结论&lt;/h4&gt;WSD-MIL有效解决了现有方法在处理变尺度肿瘤区域和计算效率方面的问题，为计算病理学中的全切片图像分析提供了新的解决方案。代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;近年来，预训练基础模型与多实例学习(MIL)的结合提高了计算病理学中的诊断准确性。然而，现有的MIL方法专注于优化特征提取器和聚合策略，而忽略了全切片图像(WSI)中实例之间的复杂语义关系。尽管基于Transformer的MIL方法旨在建模实例依赖关系，但二次计算复杂度限制了它们在大规模WSI上的可扩展性。此外，由于不同WSI中肿瘤区域规模的显著变化，现有基于Transformer的方法采用固定尺度注意力机制，在精确捕获局部实例相关性方面面临重大挑战，并且无法考虑补丁相关性的距离衰减效应。为解决这些挑战，我们提出了窗口尺度衰减MIL(WSD-MIL)，旨在增强对变尺度肿瘤区域的建模能力同时提高计算效率。WSD-MIL包括：1)基于窗口尺度衰减的注意力模块，采用基于聚类的采样策略降低计算成本，同时逐步衰减注意力窗口尺度以捕获不同尺度的局部实例关系；以及2)基于挤压和激励的区域门控模块，动态调整窗口权重以增强全局信息建模。实验结果表明，WSD-MIL在CAMELYON16和TCGA-BRCA数据集上实现了最先进的性能，同时减少了62%的计算内存。代码将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the integration of pre-trained foundational models with multiple instance learning (MIL) has improved diagnostic accuracy in computational pathology. However, existing MIL methods focus on optimizing feature extractors and aggregation strategies while overlooking the complex semantic relationships among instances within whole slide image (WSI). Although Transformer-based MIL approaches aiming to model instance dependencies, the quadratic computational complexity limits their scalability to large-scale WSIs. Moreover, due to the pronounced variations in tumor region scales across different WSIs, existing Transformer-based methods employing fixed-scale attention mechanisms face significant challenges in precisely capturing local instance correlations and fail to account for the distance-based decay effect of patch relevance. To address these challenges, we propose window scale decay MIL (WSD-MIL), designed to enhance the capacity to model tumor regions of varying scales while improving computational efficiency. WSD-MIL comprises: 1) a window scale decay based attention module, which employs a cluster-based sampling strategy to reduce computational costs while progressively decaying attention window-scale to capture local instance relationships at varying scales; and 2) a squeeze-and-excitation based region gate module, which dynamically adjusts window weights to enhance global information modeling. Experimental results demonstrate that WSD-MIL achieves state-of-the-art performance on the CAMELYON16 and TCGA-BRCA datasets while reducing 62% of the computational memory. The code will be publicly available.</description>
      <author>example@mail.com (Le Feng, Li Xiao)</author>
      <guid isPermaLink="false">2512.19982v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>How Much 3D Do Video Foundation Models Encode?</title>
      <link>http://arxiv.org/abs/2512.19949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://vidfm-3d-probe.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究视频基础模型的3D理解能力&lt;h4&gt;背景&lt;/h4&gt;视频是3D世界的连续2D投影，在大型视频数据上训练后是否会自然产生全局3D理解尚不明确&lt;h4&gt;目的&lt;/h4&gt;评估现有视频基础模型(VidFMs)的3D理解能力&lt;h4&gt;方法&lt;/h4&gt;提出第一个模型无关的框架，通过从特征中估计多个3D属性来测量各种VidFMs的3D意识&lt;h4&gt;主要发现&lt;/h4&gt;最先进的视频生成模型表现出对3D物体和场景的强烈理解，尽管没有在3D数据上训练，这种理解甚至超过专门为3D任务训练的大型专家模型&lt;h4&gt;结论&lt;/h4&gt;主要VidFMs的3D基准测试为构建可扩展的3D模型提供了有价值的观察&lt;h4&gt;翻译&lt;/h4&gt;视频是3D世界的连续2D投影。在大型视频数据上训练后，全局3D理解是否会自然出现？我们通过量化现有视频基础模型(VidFMs)的3D理解能力来研究这一问题，这些模型在大量视频数据上进行了预训练。我们提出了第一个模型无关的框架，通过从特征中估计多个3D属性来测量各种VidFMs的3D意识。我们的研究在多个轴上提供了关于VidFMs的3D意识的有意义发现。特别是，我们表明最先进的视频生成模型表现出对3D物体和场景的强烈理解，尽管它们没有在3D数据上进行训练。这种理解甚至可以超过专门为3D任务训练的大型专家模型。我们的发现以及主要VidFMs的3D基准测试，为构建可扩展的3D模型提供了有价值的观察。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文研究视频基础模型（VidFMs）在多大程度上编码了3D理解能力。这个问题重要是因为视频是3D世界的2D投影，更容易获取大规模数据，而3D数据获取困难且昂贵。理解视频模型是否具有内在的3D理解能力有助于开发可扩展的3D世界模型，对AR/VR和具身AI等领域有广泛应用价值，同时可以确定是否需要额外3D数据来训练强大的3D模型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到从2D视觉恢复3D结构是计算机视觉中的长期问题，3D数据获取困难限制了数据驱动方法发展。他们观察到视频数据更容易获取且具有多样性，提出直接评估视频模型3D理解能力的需求。设计方法时借鉴了Probe3D和Feat2GS等使用密集探测评估3D意识的工作，参考了VGGT的架构设计，并利用了现有的视频生成模型和自监督视频编码器作为基础模型，但提出了一个模型无关的框架，通过浅层读取模块直接估计3D属性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：如果视频模型理解3D世界，应该能够通过前馈方式使用浅层读取模块提取准确的3D属性，无需对基础模型进行后优化或微调。整体流程包括：1)特征提取：从视频运行VidFM提取每帧空间特征，对扩散模型使用类似DIFT的方法；2)3D意识探测：使用浅层transformer架构，交替注意力和三个读取头（点图、深度图和相机姿态）；3)训练与评估：使用多任务目标函数训练探测模型，评估点图、姿态和深度预测误差作为3D意识指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个模型无关框架评估VidFMs的3D意识；2)通过浅层读取模块直接估计3D属性，而非使用间接指标；3)从多个维度（范围、影响因素、定位和实际应用）全面评估3D意识；4)发现前沿视频生成模型具有强大的3D理解能力，甚至可以媲美专门为3D任务训练的模型。相比之前工作，本文使用直接的前馈3D预测任务而非间接指标，评估了各种视频模型而非仅图像模型，从多维度评估3D意识，并发现了视频模型比预期更强的3D理解能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次提出了一种模型无关的框架评估视频基础模型的3D意识，发现前沿视频生成模型具有强大的3D理解能力，甚至可以媲美专门为3D任务训练的模型，为构建可扩展的3D世界模型提供了新见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.</description>
      <author>example@mail.com (Zixuan Huang, Xiang Li, Zhaoyang Lv, James M. Rehg)</author>
      <guid isPermaLink="false">2512.19949v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse</title>
      <link>http://arxiv.org/abs/2512.19903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了六个大型语言模型在无需大量定制的情况下解释真实教育场景的能力，特别关注它们在分类课堂转录文本中教学动作方面的基本性能，并比较了不同提示方法的效果。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型越来越多地被应用于教育技术领域，用于生成教学材料、协助评估设计和辅导等任务。虽然之前的研究已经调查了如何调整或优化模型以适应特定任务，但对于LLMs在无需大量定制的情况下解释真实教育场景的能力了解甚少。&lt;h4&gt;目的&lt;/h4&gt;随着基于LLM的系统在日常学术环境中被广泛采用，了解它们的开箱即用能力对于设定期望和基准测试变得越来越重要。本研究旨在估计LLMs在分类课堂转录文本中教学动作这一简单但重要任务上的基本性能。&lt;h4&gt;方法&lt;/h4&gt;研究人员比较了六个大型语言模型，评估了三种典型的提示方法：零样本提示、单样本提示和少样本提示。他们在真实的课堂转录文本上测试了这些模型分类教学动作的能力。&lt;h4&gt;主要发现&lt;/h4&gt;零样本性能表现中等；提供全面的示例（少样本提示）显著提高了最先进模型的性能，最强配置的Cohen's Kappa值达到了0.58（与专家编码注释相比）；改进既不均匀也不完整：性能因教学动作而异，较高的召回率经常以增加假正价为代价。&lt;h4&gt;结论&lt;/h4&gt;总的来说，这些发现表明基础模型在解释教学话语方面具有有意义但有限的能力，提示设计有助于展现能力，但不能消除基本的可靠性限制。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)越来越多地被应用于教育技术，用于各种任务，从生成教学材料和协助评估设计到辅导。虽然之前的研究已经调查了如何调整或优化模型以适应特定任务，但对于LLMs在无需大量定制的情况下解释真实教育场景的能力知之甚少。随着基于LLM的系统在日常学术环境中被学习者教育者广泛采用，了解它们的开箱即用能力对于设定期望和基准测试变得越来越重要。我们比较了六个LLMs，以估计它们在简单但重要的任务上的基本性能：分类真实课堂转录文本中的教学动作。我们评估了典型的提示方法：零样本、单样本和少样本提示。我们发现，虽然零样本性能中等，但提供全面的示例（少样本提示）显著提高了最先进模型的性能，最强配置的Cohen's Kappa值达到了0.58，与专家编码注释相当。与此同时，改进既不均匀也不完整：性能因教学动作而异，较高的召回率经常以增加假正价为代价。总的来说，这些发现表明基础模型在解释教学话语方面具有有意义但有限的能力，提示设计有助于展现能力，但不能消除基本的可靠性限制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.</description>
      <author>example@mail.com (Kirk Vanacore, Rene F. Kizilcec)</author>
      <guid isPermaLink="false">2512.19903v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs</title>
      <link>http://arxiv.org/abs/2512.20105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LiDARDraft的LiDAR点云生成方法，通过3D布局作为桥梁连接各种条件信号与LiDAR点云，实现了高质量且多样化的可控点云生成，能够从文本描述、图像和草图创建自动驾驶环境。&lt;h4&gt;背景&lt;/h4&gt;生成真实且多样化的LiDAR点云对自动驾驶仿真至关重要，但现有方法在实现高质量结果的同时难以提供多样化的可控性，这源于LiDAR点云的复杂分布与简单控制信号之间的不平衡。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一种能够实现高质量结果和多样化可控性的LiDAR点云生成方法。&lt;h4&gt;方法&lt;/h4&gt;提出LiDARDraft方法，利用3D布局作为各种条件信号与LiDAR点云之间的桥梁，将文本、图像和点云表示为统一的3D布局，并转换为语义和深度控制信号，采用基于rangemap的ControlNet指导点云生成。&lt;h4&gt;主要发现&lt;/h4&gt;像素级对齐方法在可控LiDAR点云生成方面表现出色，能够实现'从零开始'的仿真，可以从任意文本描述、图像和草图创建自动驾驶环境。&lt;h4&gt;结论&lt;/h4&gt;LiDARDraft方法成功解决了LiDAR点云复杂分布与简单控制信号之间的不平衡问题，实现了高质量且多样化的可控LiDAR点云生成。&lt;h4&gt;翻译&lt;/h4&gt;生成真实且多样化的LiDAR点云对自动驾驶仿真至关重要。尽管先前的方法能够从用户输入生成LiDAR点云，但由于LiDAR点云的复杂分布与简单控制信号之间的不平衡，它们难以在获得高质量结果的同时实现多样化的可控性。为解决这一局限，我们提出了LiDARDraft，它利用3D布局在多种条件信号与LiDAR点云之间建立桥梁。3D布局可以从各种用户输入（如文本描述和图像）轻松生成。具体而言，我们将文本、图像和点云表示为统一的3D布局，进一步将其转换为语义和深度控制信号。然后，我们采用基于rangemap的ControlNet来指导LiDAR点云生成。这种像素级对齐方法在可控LiDAR点云生成方面表现出色，实现了'从零开始'的仿真，允许从任意的文本描述、图像和草图创建自动驾驶环境。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从多样化输入（如文本、图像和点云）生成高质量激光雷达点云的问题。这在自动驾驶领域非常重要，因为自动驾驶系统需要大量数据进行训练和验证，而收集真实世界物理数据成本高昂、不安全且难以扩展。生成逼真的点云可以为自动驾驶提供丰富的训练数据，并实现'从零开始的仿真'，仅通过简单输入就能创建自动驾驶环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：早期VAE和GAN方法难以产生高质量结果；现有扩散模型在处理多样化输入时面临挑战；简单输入难以捕捉自动驾驶场景的复杂性。他们提出使用3D布局作为统一条件表示，桥接多样化输入与点云生成。该方法借鉴了扩散模型、ControlNet架构、语义分割、深度估计和大型语言模型等现有工作，但创新性地将它们组合用于点云生成任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D布局作为统一条件表示，连接多样化输入与点云生成。流程包括：1)输入处理：将文本、图像和点云转换为3D布局；2)场景射线投射：将布局投影为范围图像；3)布局引导的生成：使用基于范围图的ControlNet指导点云生成；4)训练：先训练无条件生成模型，再微调ControlNet实现条件生成。这种方法实现了像素级对齐的控制，确保生成结果既符合输入条件又保持多样性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一条件框架：支持文本、图像和点云三种输入模态；2)直接布局到点云控制：首次实现点级控制的布局到点云生成；3)高效训练：只需微调ControlNet，大幅降低训练成本。相比之前工作，LiDARDraft比LiDARGen提供更精细控制；比LiDARDM更真实可靠；比LiDARDiffusion提供更精确的像素级控制；比Text2LiDAR支持更多输入模态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiDARDraft提出了一种创新的统一框架，通过3D布局桥接多样化输入与激光雷达点云生成，实现了从文本、图像和点云生成高质量、可控的激光雷达点云，为自动驾驶仿真提供了'从零开始'的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling "simulation from scratch", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.</description>
      <author>example@mail.com (Haiyun Wei, Fan Lu, Yunwei Zhu, Zehan Zheng, Weiyi Xue, Lin Shao, Xudong Zhang, Ya Wu, Rong Fu, Guang Chen)</author>
      <guid isPermaLink="false">2512.20105v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Lie Groups with Flow Matching</title>
      <link>http://arxiv.org/abs/2512.20043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LieFlow的新方法，通过在李群上进行流匹配直接从数据中学习对称性，成功在2D和3D点云上发现了离散群，包括反射，并解决了对称排列导致的'最后时刻收敛'问题。&lt;h4&gt;背景&lt;/h4&gt;对称性对于理解物理系统至关重要，同时也能改善机器学习中的性能和样本效率，两者都需要了解数据中潜在的对称性。&lt;h4&gt;目的&lt;/h4&gt;开发一种直接从数据中学习对称性的方法，通过流匹配技术在李群上实现对称性的发现。&lt;h4&gt;方法&lt;/h4&gt;将对称性发现表述为学习更大假设组上的分布，使学习分布匹配数据中观察到的对称性；提出LieFlow方法，相比先前工作在可发现的群类型上更灵活且需要更少假设。&lt;h4&gt;主要发现&lt;/h4&gt;在2D和3D点云上成功发现了离散群，包括通过复域上流匹配实现的反射；识别出目标模式对称排列导致的'最后时刻收敛'挑战，样本在流中保持静止直到相对较晚阶段；提出了解决这一挑战的新插值方案。&lt;h4&gt;结论&lt;/h4&gt;LieFlow方法能够有效地从数据中学习对称性，在灵活性和假设要求方面优于先前方法，为对称性发现提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;对称性是理解物理系统的基础，同时也能提高机器学习的性能和样本效率。这两种追求都需要了解数据中潜在的对称性。为此，我们提出通过在李群上进行流匹配直接从数据中学习对称性。我们将对称性发现表述为学习更大假设组上的分布，使学习分布匹配数据中观察到的对称性。与先前工作相比，我们的方法LieFlow在可发现的群类型方面更灵活，且需要更少的假设。在2D和3D点云上的实验成功发现了离散群，包括通过复域上的流匹配实现的反射。我们确定了一个关键挑战：目标模式的对称排列导致'最后时刻收敛'，其中样本在流中保持静止直到相对较晚的阶段，并引入了一种用于对称性发现的流匹配的新插值方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何直接从数据中学习对称性的问题，特别是李群对称性。这个问题在物理学和机器学习中都非常重要，因为对称性是理解物理系统的基础，同时也能提高机器学习的性能和样本效率。然而，现有方法通常需要预先知道确切的对称群，而实际应用中潜在的对称性往往是未知的、近似的或特定于领域的，如在材料化学中减少搜索空间，或在计算机视觉中处理有噪声的部分扫描数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有对称性发现方法的局限性，如只能在特定设置中工作或需要强假设。他们借鉴了流匹配(flow matching)方法，特别是Lipman等人的工作，将其扩展到李群流形上。作者将对称性发现问题重新表述为在李群上的分布学习问题，通过流匹配将先验分布转换到与数据中观察到的对称性相匹配的分布。虽然借鉴了现有工作，但LieFlow解决了之前方法的局限性，如固定的李代数基础假设、不可解释的对称性和特定分布假设等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将对称性发现转化为李群上的流匹配问题，学习一个分布在更大的假设群上，该分布集中在实际存在于数据中的子群上。整体实现流程包括：1)训练过程：从数据采样x1，从先验群采样g，计算x0=gx1，采样时间t，计算曲线上的点xt，通过神经网络预测变换并计算损失；2)生成过程：类似训练但生成新样本和累积变换；3)群元素生成：组合变换得到与目标群一致的元素。整个过程直接在李群流形上操作，条件是数据样本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将对称性发现表述为李群上的流匹配问题；2)提供统一框架发现连续和离散对称性，包括通过复域流匹配发现反射；3)识别并解决'最后时刻模式收敛'问题，引入新时间调度；4)直接在李群流形上学习流。相比之前工作，LieFlow更灵活(不限制群类型)，假设更少(不固定李代数基础)，产生可解释对称性，使用单阶段优化而非两阶段，且在流形而非欧几里得空间操作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LieFlow通过将对称性发现转化为李群上的流匹配问题，提供了一个统一框架来自动从数据中发现连续和离散对称性，解决了物理学和机器学习中需要预先知道对称性的关键限制。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.</description>
      <author>example@mail.com (Jung Yeon Park, Yuxuan Chen, Floor Eijkelboom, Jan-Willem van de Meent, Lawson L. S. Wong, Robin Walters)</author>
      <guid isPermaLink="false">2512.20043v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence</title>
      <link>http://arxiv.org/abs/2512.17781v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LiteGE，一种轻量级的3D表面测地距离计算方法，通过PCA分析UDF样本构建紧凑形状描述符，显著降低内存使用和计算延迟，同时保持高准确度。&lt;h4&gt;背景&lt;/h4&gt;计算3D表面测地距离是3D视觉和几何处理的基础任务，与形状对应等密切相关。现有基于学习的方法表现优异但依赖大型3D骨干网络，导致高内存使用和延迟，限制了在交互式或资源受限环境中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级方法，解决现有测地距离计算方法的高内存占用和高延迟问题，使其适用于资源受限环境。&lt;h4&gt;方法&lt;/h4&gt;LiteGE通过将主成分分析应用于信息体素处的无符号距离场样本，构建紧凑的、类别感知的形状描述符。这种方法计算效率高，无需高容量网络，且支持稀疏点云输入。&lt;h4&gt;主要发现&lt;/h4&gt;LiteGE在仅300个点的稀疏点云上仍保持鲁棒性；相比现有神经方法，内存使用和推理时间减少高达300倍；实现了与最先进基于网格方法相比高达1000倍的加速，同时在非等距形状对上保持相当准确性。&lt;h4&gt;结论&lt;/h4&gt;LiteGE是一种高效、准确的测地距离计算解决方案，特别适合资源受限环境，在点云输入上表现优异，为3D视觉和几何处理任务提供了实用工具。&lt;h4&gt;翻译&lt;/h4&gt;在3D表面上计算测地距离是3D视觉和几何处理中许多任务的基础，与形状对应等任务有深度联系。最近基于学习的方法表现良好，但依赖于大型3D骨干网络，导致高内存使用和延迟，限制了它们在交互式或资源受限环境中的使用。我们引入了LiteGE，一种轻量级方法，通过将主成分分析应用于信息体素处的无符号距离场样本，构建紧凑的、类别感知的形状描述符。这种描述符计算效率高，且不需要高容量网络。LiteGE在稀疏点云上保持鲁棒性，支持仅300个点的输入，而先前的方法在此情况下会失败。大量实验表明，与现有神经方法相比，LiteGE将内存使用和推理时间减少了高达300倍。此外，通过利用测地距离和形状对应之间的内在关系，LiteGE实现了快速且准确的形状匹配。与最先进的基于网格的方法相比，我们的方法实现了高达1000倍的加速，同时在非等距形状对上保持了相当的准确性，包括在点云输入上的评估。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效计算3D表面上的测地距离问题。这个问题很重要，因为测地距离是3D形状的基本属性，广泛应用于形状匹配、表面重建、参数化、纹理映射、分割和3D卷积网络构建等多个领域。现有方法要么计算速度慢，要么内存消耗大，限制了在交互式或资源受限环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于深度学习方法的局限性（内存消耗高、推理时间长），观察到形状在特定类别中共享相似几何结构，通过形状标准化可减少数据变化。作者借鉴了NeuroGF和GeGNN的嵌入公式，但避免了使用大型3D网络；借鉴了UDF表示方法；在形状匹配中使用了T-Net对齐。创新点在于使用PCA处理UDF表示创建紧凑形状嵌入，而非依赖大型3D网络。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用PCA对无符号距离场(UDF)样本进行降维，创建紧凑的形状描述符，并通过形状标准化减少数据变化。实现流程包括：1)形状标准化（中心化、缩放、方向对齐）；2)选择有信息量体素并计算UDF；3)应用PCA降维；4)使用轻量级MLP预测测地距离；5)对于形状匹配，采用从粗到细的策略和多级最近邻缓存；6)支持测地路径追踪。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用PCA处理UDF创建紧凑形状嵌入，减少内存和计算时间；2)在稀疏点云(仅300点)上表现良好；3)直接使用测地距离作为非等距形状匹配的主要监督信号；4)针对不同任务设计形状标准化方法；5)多级最近邻缓存优化形状匹配。相比传统方法，LiteGE计算速度更快；相比现有深度学习方法，内存消耗更低；在形状匹配上，直接使用测地距离作为主要监督信号而非仅作为正则化项，且同时支持网格和点云输入。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiteGE通过创新的PCA处理UDF表示方法，实现了高效、轻量级的测地距离计算和非等距形状匹配，在保持与现有方法相当准确性的同时，显著降低了内存使用和计算时间。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying Principal Component Analysis (PCA) to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.</description>
      <author>example@mail.com (Yohanes Yudhi Adikusuma, Qixing Huang, Ying He)</author>
      <guid isPermaLink="false">2512.17781v2</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning</title>
      <link>http://arxiv.org/abs/2512.19743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, 2 tables, 5 formulas, 34 references, journal paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CUDA-APML的稀疏GPU实现，用于近似最优传输点云匹配，显著减少了内存使用同时保持匹配精度。&lt;h4&gt;背景&lt;/h4&gt;在3D点云模型学习中，损失函数至关重要。常见的Chamfer Distance计算高效但允许多对一对应，而Earth Mover Distance更准确但计算成本高。APML方法使用可微分Sinkhorn迭代和解析温度近似传输，但其密集公式在内存上呈二次方扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种内存高效的GPU实现，减少APML方法的内存使用，同时保持其匹配精度和梯度保留能力。&lt;h4&gt;方法&lt;/h4&gt;CUDA-APML是一种稀疏GPU实现，对可忽略的分配进行阈值处理，直接在COO形式下运行自适应softmax、双向对称化和Sinkhorn归一化，实现近线性内存扩展并保留存储支持上的梯度。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet和MM-Fi数据集上，CUDA-APML在较小容差范围内匹配密集APML的性能，同时将GPU峰值内存减少了99.9%。点对距离评估目前仍是二次方的。&lt;h4&gt;结论&lt;/h4&gt;CUDA-APML成功解决了APML方法的内存扩展问题，实现了近线性的内存使用，同时保持了高精度的点云匹配能力。&lt;h4&gt;翻译&lt;/h4&gt;损失函数是学习准确的3D点云模型的基础，然而常见的选择在几何保真度和计算成本之间进行权衡。Chamfer Distance效率高但允许多对一对应关系，而Earth Mover Distance更好地反映一对一传输但计算成本高。APML使用可微分的Sinkhorn迭代和解析推导的温度近似传输，但其密集公式在内存上呈二次方扩展。我们提出了CUDA-APML，一个稀疏GPU实现，它对可忽略的分配进行阈值处理，并直接在COO形式下运行自适应softmax、双向对称化和Sinkhorn归一化。这实现了近线性的内存扩展并保留了存储支持上的梯度，而点对距离评估在当前实现中仍然是二次方的。在ShapeNet和MM-Fi上，CUDA-APML在较小容差范围内匹配密集APML，同时将GPU峰值内存减少了99.9%。代码可在以下网址获取：https://github.com/Multimodal-Sensing-Lab/apml&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云学习中损失函数的内存效率问题。具体来说，APML（自适应概率匹配损失）虽然能提供准确的3D点云学习，但其密集实现方式在内存上呈二次方增长，限制了在大规模点云和大规模批处理训练中的应用。这个问题在现实中很重要，因为3D点云广泛应用于机器人、AR/VR、无线人类感知和数字孪生等领域，而内存限制会限制批处理大小和点数，增加训练成本，阻碍高分辨率场景的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：Chamfer Distance效率高但允许多对一分配，Earth Mover Distance更好地反映一对一匹配但计算成本高。作者注意到APML在自适应softmax阶段后，未归一化的相似度矩阵中的大多数条目接近于零，这是一个经验性质。基于这一观察，作者设计了稀疏实现，通过修剪微不足道的相似度条目来减少内存使用。作者借鉴了APML最优传输理论、稀疏表示和GPU优化技术（如FlashAttention和KeOps）以及稀疏张量引擎等现有工作，将其应用于解决内存效率问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用APML在自适应softmax阶段后的经验稀疏性，通过修剪微不足道的相似度条目来减少内存使用，同时在GPU上直接处理稀疏数据结构，避免创建和存储密集的N×M张量。整体实现流程包括：1)行方向处理，扫描每一行计算最小值和温度，识别并保存重要配对；2)列方向处理，交换角色重复上述步骤；3)对称化，连接COO条目并排序；4)Sinkhorn迭代，在稀疏支持上执行列缩放和行缩放；5)损失和反向传播，在COO配对上评估损失并通过稀疏运算反向传播。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)稀疏GPU实现CUDA-APML，通过修剪微不足道的分配直接在COO格式中执行各种操作；2)实现了接近线性的内存扩展，同时保持梯度，将峰值GPU内存减少99.9%；3)在保持精度的同时显著提高内存效率；4)使基于传输的监督在实际应用中更加可行。相比之前的工作，CUDA-APML不仅保留了APML的准确性，还解决了其内存瓶颈问题，使其能够处理更大的点云和批处理大小，而无需引入额外的近似或设计选择。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一种名为CUDA-APML的高效稀疏GPU实现，通过利用最优传输的经验稀疏性，在保持APML精度的同时将内存使用减少了99.9%，使得大规模3D点云学习在实际应用中变得可行。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Loss functions are fundamental to learning accurate 3D point cloud models, yet common choices trade geometric fidelity for computational cost. Chamfer Distance is efficient but permits many-to-one correspondences, while Earth Mover Distance better reflects one-to-one transport at high computational cost. APML approximates transport with differentiable Sinkhorn iterations and an analytically derived temperature, but its dense formulation scales quadratically in memory. We present CUDA-APML, a sparse GPU implementation that thresholds negligible assignments and runs adaptive softmax, bidirectional symmetrization, and Sinkhorn normalization directly in COO form. This yields near-linear memory scaling and preserves gradients on the stored support, while pairwise distance evaluation remains quadratic in the current implementation. On ShapeNet and MM-Fi, CUDA-APML matches dense APML within a small tolerance while reducing peak GPU memory by 99.9%. Code available at: https://github.com/Multimodal-Sensing-Lab/apml</description>
      <author>example@mail.com (Sasan Sharifipour, Constantino Álvarez Casado, Manuel Lage Cañellas, Miguel Bordallo López)</author>
      <guid isPermaLink="false">2512.19743v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Bank Systemic Risk of Emerging Markets under Geopolitical Shocks: Empirical Evidence from BRICS Countries</title>
      <link>http://arxiv.org/abs/2512.20515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages and 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BRIDGES框架是一种创新的系统性风险分析方法，能够捕捉金砖国家银行系统的复杂动态和潜在风险，基于信息复杂性层次进行分析和评估。&lt;h4&gt;背景&lt;/h4&gt;金砖国家（BRICS）的经济影响力日益增长，需要能够捕捉复杂、长期动态的风险模型来评估金融系统稳定性。&lt;h4&gt;目的&lt;/h4&gt;介绍BRIDGES（Bank Risk Interlinkage with Dynamic Graph and Event Simulations）框架，用于分析基于信息复杂性层次的系统性风险。&lt;h4&gt;方法&lt;/h4&gt;BRIDGES框架使用动态时间规整（DTW）距离构建基于551家BRICS银行战略相似性的动态网络；利用零阶信息（如资产负债表数据）构建网络，一阶信息（如风险比率趋势）检测行为变化，时序图神经网络（TGNN）学习网络演化并检测二阶信息（如结构关系异常），最后通过基于主体的模型（ABM）模拟评估系统韧性。&lt;h4&gt;主要发现&lt;/h4&gt;最大机构的失败比财务脆弱或动态异常机构失败造成更多系统性损害，由恐慌效应驱动；具有相关国家传播的地缘政治冲击比'太大而不能倒闭'场景造成更严重的系统性损害，近乎导致系统完全崩溃。&lt;h4&gt;结论&lt;/h4&gt;对BRICS金融稳定的主要威胁是二阶恐慌和大范围地缘政治冲击，这些威胁可能被传统风险分析模型忽视。&lt;h4&gt;翻译&lt;/h4&gt;金砖国家日益增长的经济影响力需要能够捕捉复杂、长期动态的风险模型。本文介绍了银行风险动态图与事件模拟互联（BRIDGES）框架，该框架基于信息复杂性层次（零阶、一阶和二阶）分析系统性风险。BRIDGES利用动态时间规整（DTW）距离构建551家金砖国家银行的动态网络，基于其战略相似性，使用零阶信息如2008年至2024年的年度资产负债表数据。然后采用一阶信息，包括风险比率趋势，来检测银行行为的变化。作为BRIDGES核心的时序图神经网络（TGNN）被部署以学习网络演化并检测二阶信息，如银行网络结构关系的异常变化。为了衡量异常变化对网络稳定性的影响，BRIDGES进行基于主体的模型（ABM）模拟，评估银行系统对内部金融失败和外部地缘政治冲击在国家个体层面和整个金砖国家范围内的韧性。模拟结果表明，最大机构的失败比财务脆弱或动态异常机构的失败造成更多系统性损害，由强大的恐慌效应驱动。与这种'太大而不能倒闭'的情景相比，具有相关国家级传播的地缘政治冲击会造成更具破坏性的系统性损害，导致近乎完全的系统崩溃。这表明，对金砖国家金融稳定的主要威胁是二阶恐慌和大范围地缘政治冲击，传统风险分析模型可能无法检测到这些威胁。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing economic influence of the BRICS nations requires risk models that capture complex, long-term dynamics. This paper introduces the Bank Risk Interlinkage with Dynamic Graph and Event Simulations (BRIDGES) framework, which analyzes systemic risk based on the level of information complexity (zero-order, first-order, and second-order). BRIDGES utilizes the Dynamic Time Warping (DTW) distance to construct a dynamic network for 551 BRICS banks based on their strategic similarity, using zero-order information such as annual balance sheet data from 2008 to 2024. It then employs first-order information, including trends in risk ratios, to detect shifts in banks' behavior. A Temporal Graph Neural Network (TGNN), as the core of BRIDGES, is deployed to learn network evolutions and detect second-order information, such as anomalous changes in the structural relationships of the bank network. To measure the impact of anomalous changes on network stability, BRIDGES performs Agent-Based Model (ABM) simulations to assess the banking system's resilience to internal financial failure and external geopolitical shocks at the individual country level and across BRICS nations. Simulation results show that the failure of the largest institutions causes more systemic damage than the failure of the financially vulnerable or dynamically anomalous ones, driven by powerful panic effects. Compared to this "too big to fail" scenario, a geopolitical shock with correlated country-wide propagation causes more destructive systemic damage, leading to a near-total systemic collapse. It suggests that the primary threats to BRICS financial stability are second-order panic and large-scale geopolitical shocks, which traditional risk analysis models might not detect.</description>
      <author>example@mail.com (Haibo Wang)</author>
      <guid isPermaLink="false">2512.20515v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Graph-Sequence Learning Model for Inductive Text Classification</title>
      <link>http://arxiv.org/abs/2512.20097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的图-序列学习模型(TextGSL)用于归纳文本分类，通过结合图神经网络和Transformer的优势，解决了现有方法在处理多样化结构信息和序列信息方面的局限性，提高了文本分类的准确性。&lt;h4&gt;背景&lt;/h4&gt;文本分类在情感分析、假新闻检测和公共舆论分析等多种下游任务中扮演重要角色。基于图神经网络(GNNs)的文本分类最近取得了显著进展，因其具有强大的结构关系学习能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于GNN的文本分类方法的两大局限性：未能充分考虑单词对之间的多样化结构信息，以及在文本图结构信息学习模块中忽略序列信息的问题。&lt;h4&gt;方法&lt;/h4&gt;为每个文本中的所有单词构建单一文本级图，基于单词对之间的不同关系建立不同的边类型；设计自适应多边消息传递范式聚合多样化结构信息；通过融入Transformer层捕获文本数据中的序列信息。&lt;h4&gt;主要发现&lt;/h4&gt;TextGSL能够学习更具区分性的文本表示；在多样化的基准数据集上与多个强基线进行比较，实验结果表明TextGSL在准确性方面优于这些基线。&lt;h4&gt;结论&lt;/h4&gt;TextGSL是一种有效的文本分类方法，通过结合图神经网络和Transformer的优势，解决了现有方法的局限性，提高了分类性能，尤其擅长处理包含新单词和新关系的文本分类任务。&lt;h4&gt;翻译&lt;/h4&gt;文本分类在各种下游文本相关任务中扮演着重要角色，例如情感分析、假新闻检测和公共舆论分析。最近，基于图神经网络(GNNs)的文本分类由于其强大的结构关系学习能力取得了显著进展。然而，这些方法仍面临两个主要局限。首先，这些方法未能充分考虑单词对之间的多样化结构信息，例如共现、句法和语义。此外，它们在文本图结构信息学习模块中忽略了序列信息，无法对新单词和新关系进行文本分类。在本文中，我们提出了一种新颖的图-序列学习模型用于归纳文本分类(TextGSL)来解决上述问题。更具体地说，我们为每个文本中的所有单词构建单一文本级图，并基于单词对之间的不同关系建立不同的边类型。在此基础上，我们设计了一个自适应多边消息传递范式来聚合单词对之间的多样化结构信息。此外，通过融入Transformer层，所提出的TextGSL可以捕获文本数据中的序列信息。因此，TextGSL能够学习更具区分性的文本表示。TextGSL已与多个强基线进行了全面比较。在多样化基准数据集上的实验结果表明，TextGSL在准确性方面优于这些基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.</description>
      <author>example@mail.com (Zuo Wang, Ye Yuan)</author>
      <guid isPermaLink="false">2512.20097v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2512.20094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Jensen-Shannon散度消息传递(JSDMP)的新学习范式，用于处理富文本图中的表示学习问题，通过同时考虑相似性和不相似性来改进节点表示&lt;h4&gt;背景&lt;/h4&gt;广泛存在的上下文和结构差异可能影响富文本图中的表示学习效果&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉上下文和结构差异的学习范式，提升富文本图表示学习的质量&lt;h4&gt;方法&lt;/h4&gt;提出JSDMP学习范式，结合Jensen-Shannon散度捕获结构和文本的相似性与不相似性，并基于此开发了两种图神经网络：DMPGCN和DMPPRG&lt;h4&gt;主要发现&lt;/h4&gt;在富文本数据集上的实验表明，DMPGCN和DMPPRG性能优于多种最先进的基线方法&lt;h4&gt;结论&lt;/h4&gt;Jensen-Shannon散度消息传递范式能有效提升富文本图表示学习的效果&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们研究了广泛存在的上下文和结构差异如何影响富文本图中的表示学习。为此，我们提出了Jensen-Shannon散度消息传递(JSDMP)，一种用于富文本图表示学习的新学习范式。除了考虑结构和文本的相似性外，JSDMP还通过Jensen-Shannon散度进一步捕获它们对应的不相似性。然后，相似性和不相似性被联合用于计算文本节点之间的新消息权重，从而使表示能够从真正相关的文本节点学习上下文和结构信息。基于JSDMP，我们提出了两种新颖的图神经网络，即差异消息传递图卷积网络(DMPGCN)和差异消息传递PageRank图神经网络(DMPPRG)，用于学习富文本图中的表示。DMPGCN和DMPPRG已在成熟的富文本数据集上进行了广泛测试，并与几种最先进的基线方法进行了比较。实验结果表明，DMPGCN和DMPPRG能够优于其他基线方法，证明了所提出的Jensen-Shannon散度消息传递范式的有效性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we investigate how the widely existing contextual and structural divergence may influence the representation learning in rich-text graphs. To this end, we propose Jensen-Shannon Divergence Message-Passing (JSDMP), a new learning paradigm for rich-text graph representation learning. Besides considering similarity regarding structure and text, JSDMP further captures their corresponding dissimilarity by Jensen-Shannon divergence. Similarity and dissimilarity are then jointly used to compute new message weights among text nodes, thus enabling representations to learn with contextual and structural information from truly correlated text nodes. With JSDMP, we propose two novel graph neural networks, namely Divergent message-passing graph convolutional network (DMPGCN) and Divergent message-passing Page-Rank graph neural networks (DMPPRG), for learning representations in rich-text graphs. DMPGCN and DMPPRG have been extensively texted on well-established rich-text datasets and compared with several state-of-the-art baselines. The experimental results show that DMPGCN and DMPPRG can outperform other baselines, demonstrating the effectiveness of the proposed Jensen-Shannon Divergence Message-Passing paradigm</description>
      <author>example@mail.com (Zuo Wang, Ye Yuan)</author>
      <guid isPermaLink="false">2512.20094v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption</title>
      <link>http://arxiv.org/abs/2512.20084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;QE-Catalytic是一个多模态框架，深度耦合大型语言模型Qwen与E(3)等变图Transformer Equiformer-V2，用于复杂催化表面的吸附构型属性预测和逆向设计。&lt;h4&gt;背景&lt;/h4&gt;吸附能是催化反应性的关键描述符，其准确性直接影响机器学习驱动的催化剂筛选可靠性。E(3)等变图神经网络在三维原子坐标操作上表现良好，而基于语言模型的方法在吸附构型能量预测和区分相同系统不同构型方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时支持吸附构型属性预测和逆向设计的多模态框架，提高吸附能预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;QE-Catalytic联合利用三维结构和结构化配置文本，通过图-文本对齐将3D几何信息注入语言通道，使模型在精确坐标不可用时仍可作为高性能基于文本的预测器，并能自回归生成目标能量驱动的结构设计和信息补全的CIF文件。&lt;h4&gt;主要发现&lt;/h4&gt;在OC20数据集上，QE-Catalytic将松弛吸附能的平均绝对误差从0.713 eV降低到0.486 eV，并在多种评估协议中持续优于CatBERTa和GAP-CATBERTa等基线模型。&lt;h4&gt;结论&lt;/h4&gt;QE-Catalytic有效结合了语言模型和图神经网络的优势，在吸附能预测和逆向设计方面表现优异，为催化剂筛选提供了更可靠的工具。&lt;h4&gt;翻译&lt;/h4&gt;吸附能是催化反应性的关键描述符。它从根本上定义为吸附物-表面系统的松弛总能量与适当参考状态之间的差异；因此，松弛能量预测的准确性直接决定了机器学习驱动的催化剂筛选的可靠性。E(3)等变图神经网络可以在周期性边界条件下原生操作三维原子坐标，并在此类任务上表现出强大的性能。相比之下，基于语言模型的方法虽然能够生成人类可读的文本描述并减少对显式图的依赖——从而扩大了适用范围——但在吸附构型能量预测准确性和区分'相同系统不同构型'方面仍然不足，即使在GAP-CATBERTa风格的图辅助预训练下也是如此。为此，我们提出了QE-Catalytic，一个多模态框架，深度耦合大型语言模型(Qwen)与E(3)等变图Transformer(Equiformer-V2)，能够统一支持复杂催化表面上吸附构型属性预测和逆向设计。在预测过程中，QE-Catalytic联合利用三维结构和结构化配置文本，并通过图-文本对齐将'3D几何信息'注入语言通道，使其在精确坐标不可用时可作为高性能基于文本的预测器，同时也能自回归生成目标能量驱动的结构设计和信息补全的CIF文件。在OC20上，QE-Catalytic将松弛吸附能的MAE从0.713 eV降低到0.486 eV，并且在多种评估协议中持续优于CatBERTa和GAP-CATBERTa等基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa.  To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.</description>
      <author>example@mail.com (Yanjie Li, Jian Xu, Xueqing Chen, Lina Yu, Shiming Xiang, Weijun Li, Cheng-lin Liu)</author>
      <guid isPermaLink="false">2512.20084v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis</title>
      <link>http://arxiv.org/abs/2512.20026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Proceedings of the AAAI Conference on Artificial Intelligence 40 (AAAI-26)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多激活平面交互图神经网络(MAPI-GNN)，通过学习多方面图配置来改进多模态医疗诊断中的图神经网络应用，解决了传统方法依赖单一静态图的局限性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络因其固有关系建模能力越来越多地应用于多模态医疗诊断，但其功效常因依赖从无差别特征构建的单个静态图而受限，无法有效建模患者特定病理关系。&lt;h4&gt;目的&lt;/h4&gt;重建传统的单图范式，通过从语义解耦的特征子空间学习多方面图配置，提高医疗诊断的准确性和患者特异性。&lt;h4&gt;方法&lt;/h4&gt;MAPI-GNN框架首先通过多维鉴别器发现潜在图感知模式，这些模式指导动态构建激活图堆栈，最后通过关系融合引擎对多方面配置进行聚合和上下文化，实现稳健诊断。&lt;h4&gt;主要发现&lt;/h4&gt;在两个包含超过1300个患者样本的不同任务上的大量实验表明，MAPI-GNN显著优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;通过学习多方面图配置而非依赖单一静态图，MAPI-GNN有效解决了传统图神经网络在多模态医疗诊断中的局限性，提高了诊断性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络因其固有的关系建模能力，越来越多地被应用于多模态医疗诊断。然而，它们的有效性常常因为普遍依赖从无差别特征构建的单个静态图而受到损害，这阻碍了建模患者特定病理关系的能力。为此，提出的多激活平面交互图神经网络(MAPI-GNN)通过从语义解耦的特征子空间学习多方面图配置来重建这种单图范式。该框架首先通过多维鉴别器发现潜在的图感知模式；这些模式然后指导动态构建一堆激活图；最后，这种多方面配置通过关系融合引擎进行聚合和上下文化，以实现稳健诊断。在两个包含超过1300个患者样本的不同任务上的大量实验表明，MAPI-GNN显著优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.</description>
      <author>example@mail.com (Ziwei Qin, Xuhui Song, Deqing Huang, Na Qin, Jun Li)</author>
      <guid isPermaLink="false">2512.20026v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>A hybrid global local computational framework for ship hull structural analysis using homogenized model and graph neural network</title>
      <link>http://arxiv.org/abs/2512.20020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合等效单层模型和图神经网络的计算框架，用于船体梁的全局-局部结构分析。该框架通过粗网格ESL模型预测全局位移，然后使用异构图变换器预测局部应力场，显著提高了局部分析的效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;船体梁结构需要进行全局和局部分析来评估其性能，但传统方法在计算效率和精度之间存在权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够高效且准确地预测船体梁的全局位移场和局部应力、位移场的计算框架，特别适用于优化设计。&lt;h4&gt;方法&lt;/h4&gt;集成等效单层模型和图神经网络；使用粗网格均质化ESL模型预测全局位移场；开发全局到局部自由度映射和重建程序；使用异构图变换器作为局部分析工具；使用高保真3D面板有限元模型训练HGT；仅需全局ESL解即可生成详细局部响应。&lt;h4&gt;主要发现&lt;/h4&gt;在三个箱梁案例研究中验证，全局预测误差由粗网格ESL解决定，而HGT保持了高局部精度，明显优于传统的基于ESL的应力估计方法。&lt;h4&gt;结论&lt;/h4&gt;该框架结合了ESL模型的全局效率和HGT的局部精度，为船体梁结构分析提供了高效且准确的解决方案，特别适用于优化设计。&lt;h4&gt;翻译&lt;/h4&gt;这项研究提出了一个用于船体梁全局-局部结构分析的计算框架，该框架集成了等效单层模型与图神经网络。粗网格均质化ESL模型可有效预测全局位移场，从中提取加劲板边界上的自由度。开发了全局到局部自由度映射和重建程序，以恢复局部分析的详细边界运动学。重建的自由度与面板几何和载荷一起作为异构图变换器的输入，该变换器能够快速准确地预测船体梁内任何面板的详细应力和位移场。HGT使用具有重建边界条件的高保真3D面板有限元模型进行训练，使其能够泛化到不同的面板几何形状、载荷和边界行为。一旦训练完成，该框架仅需全局ESL解即可生成详细的局部响应，使其非常适用于优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a computational framework for global local structural analysis of ship hull girders that integrates an equivalent single layer (ESL) model with a graph neural network (GNN). A coarse mesh homogenized ESL model efficiently predicts the global displacement field, from which degrees of freedom (DOFs) along stiffened panel boundaries are extracted. A global to local DOF mapping and reconstruction procedure is developed to recover detailed boundary kinematics for local analysis. The reconstructed DOFs, together with panel geometry and loading, serve as inputs to a heterogeneous graph transformer (HGT), a subtype of GNN, which rapidly and accurately predicts the detailed stress and displacement fields for any panel within the hull girder. The HGT is trained using high fidelity 3D panel finite element model with reconstructed boundary conditions, enabling it to generalize across varying panel geometries, loadings, and boundary behaviors. Once trained, the framework requires only the global ESL solution in order to generate detailed local responses, making it highly suitable for optimization. Validation on three box beam case studies demonstrates that the global prediction error is governed by the coarse mesh ESL solution, while the HGT maintains high local accuracy and clearly outperforms conventional ESL based stress estimation method.</description>
      <author>example@mail.com (Yuecheng Cai, Jasmin Jelovica)</author>
      <guid isPermaLink="false">2512.20020v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis</title>
      <link>http://arxiv.org/abs/2512.19970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一种新颖的数据驱动框架，首次在县级规模应用时空图神经网络(STGNN)预测综合可持续性指数，基于畜群级运营记录。&lt;h4&gt;背景&lt;/h4&gt;畜牧业可持续发展评估需要更精确和全面的预测方法，传统方法难以处理复杂的时空依赖关系和数据稀疏性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从畜群级运营记录中预测综合可持续性指数的框架，为爱尔兰牛育种联合会提供2026-2030年的多年预测。&lt;h4&gt;方法&lt;/h4&gt;采用变分自编码器(VAE)增强数据集，通过主成分分析确定四个关键评分维度(繁殖效率、遗传管理、畜群健康和畜群管理)，构建加权综合指数，并使用新型STGNN架构建模时空依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;成功开发并应用了首个县级规模的STGNN框架用于可持续性预测，该框架能够有效处理地理依赖性和非线性时间动态，生成可靠的多年预测结果。&lt;h4&gt;结论&lt;/h4&gt;该数据驱动框架为畜牧业可持续发展评估提供了新方法，能够整合多维度数据并进行长期预测，有助于畜牧业决策和规划。&lt;h4&gt;翻译&lt;/h4&gt;本研究引入了一种新颖的数据驱动框架和首次在县级规模应用的时空图神经网络(STGNN)，用于从畜群级运营记录预测综合可持续性指数。该方法采用了一种新颖的端到端管道，使用变分自编码器(VAE)增强爱尔兰牛育种联合会(ICBF)数据集，同时保持联合分布并缓解稀疏性。通过主成分分析首次推导出基于支柱的评分公式，确定了繁殖效率、遗传管理、畜群健康和畜群管理，以构建加权综合指数。这些指数使用一种新型STGNN架构进行建模，该架构明确编码了地理依赖性和非线性时间动态，以生成2026-2030年的多年预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces a novel data-driven framework and the first-ever county-scale application of Spatio-Temporal Graph Neural Networks (STGNN) to forecast composite sustainability indices from herd-level operational records. The methodology employs a novel, end-to-end pipeline utilizing a Variational Autoencoder (VAE) to augment Irish Cattle Breeding Federation (ICBF) datasets, preserving joint distributions while mitigating sparsity. A first-ever pillar-based scoring formulation is derived via Principal Component Analysis, identifying Reproductive Efficiency, Genetic Management, Herd Health, and Herd Management, to construct weighted composite indices. These indices are modelled using a novel STGNN architecture that explicitly encodes geographic dependencies and non-linear temporal dynamics to generate multi-year forecasts for 2026-2030.</description>
      <author>example@mail.com (Surya Jayakumar, Kieran Sullivan, John McLaughlin, Christine O'Meara, Indrakshi Dey)</author>
      <guid isPermaLink="false">2512.19970v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>QuarkAudio Technical Report</title>
      <link>http://arxiv.org/abs/2512.20151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了QuarkAudio，一个基于仅解码器自回归语言模型的统一音频处理和生成框架，能够处理多种音频任务并提供高质量的音频生成能力。&lt;h4&gt;背景&lt;/h4&gt;现有音频处理和生成模型大多依赖于特定任务架构，导致开发碎片化和扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;设计一个能处理多任务的统一框架，提供强大的指令和音频理解能力，实现高质量的音频生成。&lt;h4&gt;方法&lt;/h4&gt;提出QuarkAudio框架，包含统一的离散音频标记器H-Codec，将自监督学习表示整合到标记化和重建过程中；对H-Codec进行改进，包括动态帧率机制和48kHz音频采样率支持；使用特定任务条件信息作为LM条件序列，以自回归方式预测离散音频标记。&lt;h4&gt;主要发现&lt;/h4&gt;H-Codec以低帧率实现高质量音频重建，提高下游音频生成效率和性能；QuarkAudio在多种任务上达到与最先进系统相当的性能。&lt;h4&gt;结论&lt;/h4&gt;QuarkAudio是一个有效的统一多任务框架，支持语音修复、目标说话人提取、语音分离、语音转换和语言查询音频源分离等多种任务，并可扩展到自然语言引导的通用音频编辑。&lt;h4&gt;翻译&lt;/h4&gt;许多现有的音频处理和生成模型依赖于特定任务架构，导致碎片化的开发努力和有限的扩展性。因此，设计一个能够处理多任务、提供强大的指令和音频理解能力以及高质量音频生成的统一框架是很有前景的。这需要兼容的范式设计、强大的骨干网络和高保真音频重建模块。为满足这些需求，本技术报告介绍了QuarkAudio，一个基于仅解码器自回归(AR)语言模型的生成框架，统一了多种任务。该框架包含一个统一的离散音频标记器H-Codec，将自监督学习表示整合到标记化和重建过程中。我们进一步提出了对H-Codec的几项改进，如动态帧率机制和将音频采样率扩展到48kHz。QuarkAudio通过使用特定任务的条件信息作为仅解码器LM的条件序列，并以自回归方式预测离散目标音频标记来统一任务。该框架支持广泛的音频处理和生成任务，包括语音修复(SR)、目标说话人提取(TSE)、语音分离(SS)、语音转换(VC)和语言查询音频源分离(LASS)。此外，我们将下游任务扩展到由自然语言指令引导的通用自由形式音频编辑（包括语音语义编辑和音频事件编辑）。实验结果表明，H-Codec以低帧率实现高质量音频重建，提高了下游音频生成的效率和性能，并且QuarkAudio在多个任务上提供了与最先进的任务特定或多任务系统具有竞争力或可比的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many existing audio processing and generation models rely on task-specific architectures, resulting in fragmented development efforts and limited extensibility. It is therefore promising to design a unified framework capable of handling multiple tasks, while providing robust instruction and audio understanding and high-quality audio generation. This requires a compatible paradigm design, a powerful backbone, and a high-fidelity audio reconstruction module. To meet these requirements, this technical report introduces QuarkAudio, a decoder-only autoregressive (AR) LM-based generative framework that unifies multiple tasks. The framework includes a unified discrete audio tokenizer, H-Codec, which incorporates self-supervised learning (SSL) representations into the tokenization and reconstruction process. We further propose several improvements to H-Codec, such as a dynamic frame-rate mechanism and extending the audio sampling rate to 48 kHz. QuarkAudio unifies tasks by using task-specific conditional information as the conditioning sequence of the decoder-only LM, and predicting discrete target audio tokens in an AR manner. The framework supports a wide range of audio processing and generation tasks, including speech restoration (SR), target speaker extraction (TSE), speech separation (SS), voice conversion (VC), and language-queried audio source separation (LASS). In addition, we extend downstream tasks to universal free-form audio editing guided by natural language instructions (including speech semantic editing and audio event editing). Experimental results show that H-Codec achieves high-quality audio reconstruction with a low frame rate, improving both the efficiency and performance of downstream audio generation, and that QuarkAudio delivers competitive or comparable performance to state-of-the-art task-specific or multi-task systems across multiple tasks.</description>
      <author>example@mail.com (Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Xiaofu Chen, Bin Gong, Zheng Xue, Gang Song)</author>
      <guid isPermaLink="false">2512.20151v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Evolutionary Neural Architecture Search with Dual Contrastive Learning</title>
      <link>http://arxiv.org/abs/2512.20112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DCL-ENAS的新方法，通过双阶段对比学习训练神经网络预测器，解决了ENAS中训练数据计算成本高的问题，在多个基准测试和实际任务中取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;进化神经网络架构搜索(ENAS)因能自动设计神经网络架构而受到关注。现有研究使用神经网络预测器指导搜索过程，但收集训练数据的计算成本很高，因为每个标签都需要完整训练一个架构。在有限计算预算下实现高精度预测器对ENAS成功至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效方法，在有限计算预算下训练高精度神经网络预测器，以提升ENAS的性能并降低计算成本。&lt;h4&gt;方法&lt;/h4&gt;提出ENAS与双对比学习(DCL-ENAS)方法，包含两个阶段：第一阶段使用对比自监督学习从神经网络架构中学习有意义的表示，无需标签；第二阶段使用对比学习进行微调，准确预测不同架构的相对性能而非绝对性能，这足以指导进化搜索。&lt;h4&gt;主要发现&lt;/h4&gt;在NASBench-101和NASBench-201上，DCL-ENAS实现了最高验证准确率，超越最强已发表基线0.05%(ImageNet16-120)到0.39%(NASBench-101)。在真实ECG心律失常分类任务中，比随机搜索获得的非NAS手动设计模型提高约2.5个百分点，仅需7.7 GPU天。&lt;h4&gt;结论&lt;/h4&gt;DCL-ENAS通过双阶段对比学习有效解决了ENAS中训练数据计算成本高的问题，在有限计算预算下实现了高性能，为神经网络架构搜索提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;进化神经网络架构搜索(ENAS)因能自动设计神经网络架构而受到关注。最近的研究使用神经网络预测器来指导这个过程，但收集训练数据的计算成本很高——因为每个标签都需要完整训练一个架构——这使得在有限计算预算(即完整训练的架构-标签对数量有限)下实现高精度预测器对ENAS成功至关重要。本文引入了带双对比学习的ENAS(DCL-ENAS)，一种使用两个阶段的对比学习来训练神经网络预测器的新方法。在第一阶段，使用对比自监督学习从神经网络架构中学习有意义的表示，不需要标签。在第二阶段，进行对比学习的微调，以准确预测不同架构的相对性能而非绝对性能，这足以指导进化搜索。在NASBench-101和NASBench-201上，DCL-ENAS实现了最高的验证准确率，比最强已发表基线高出0.05%(ImageNet16-120)到0.39%(NASBench-101)。在真实世界的ECG心律失常分类任务中，DCL-ENAS比通过随机搜索获得的非NAS手动设计模型提高了约2.5个百分点的性能，仅需7.7 GPU天。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evolutionary Neural Architecture Search (ENAS) has gained attention for automatically designing neural network architectures. Recent studies use a neural predictor to guide the process, but the high computational costs of gathering training data -- since each label requires fully training an architecture -- make achieving a high-precision predictor with { limited compute budget (i.e., a capped number of fully trained architecture-label pairs)} crucial for ENAS success. This paper introduces ENAS with Dual Contrastive Learning (DCL-ENAS), a novel method that employs two stages of contrastive learning to train the neural predictor. In the first stage, contrastive self-supervised learning is used to learn meaningful representations from neural architectures without requiring labels. In the second stage, fine-tuning with contrastive learning is performed to accurately predict the relative performance of different architectures rather than their absolute performance, which is sufficient to guide the evolutionary search. Across NASBench-101 and NASBench-201, DCL-ENAS achieves the highest validation accuracy, surpassing the strongest published baselines by 0.05\% (ImageNet16-120) to 0.39\% (NASBench-101). On a real-world ECG arrhythmia classification task, DCL-ENAS improves performance by approximately 2.5 percentage points over a manually designed, non-NAS model obtained via random search, while requiring only 7.7 GPU-days.</description>
      <author>example@mail.com (Xian-Rong Zhang, Yue-Jiao Gong, Wei-Neng Chen, Jun Zhang)</author>
      <guid isPermaLink="false">2512.20112v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Vehicle-centric Perception via Multimodal Structured Pre-training</title>
      <link>http://arxiv.org/abs/2512.19934v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Journal extension of VehicleMAE (AAAI 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了VehicleMAE-V2，一种新型的车辆中心预训练大模型，通过利用车辆相关的多模态结构先验来指导掩码令牌重建过程，增强模型学习车辆中心感知的通用表示能力。论文还提出了三个专门设计的模块(SMM、CRM和SRM)来整合对称、轮廓和语义三种结构先验，并构建了一个大规模数据集Autobot4M来支持预训练。实验证明该模型在五个下游任务上表现优越。&lt;h4&gt;背景&lt;/h4&gt;车辆中心感知在许多智能系统中起着关键作用，包括大规模监控系统、智能交通和自动驾驶。然而，现有方法在预训练过程中缺乏对车辆相关知识的有效学习，导致建模通用车辆感知表示的能力较差。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法在预训练过程中缺乏有效学习车辆相关知识的问题，作者提出了一种名为VehicleMAE-V2的新型车辆中心预训练大模型，旨在增强模型学习车辆中心感知的通用表示能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了VehicleMAE-V2模型，并设计了三个专门模块：对称引导掩码模块(SMM)利用车辆对称约束选择高质量掩码图像补丁并减少信息冗余；轮廓引导表示模块(CRM)最小化轮廓特征和重建特征之间的概率分布差异，保持整体车辆结构信息；语义引导表示模块(SRM)通过对比学习和跨模态蒸馏对齐图像文本特征，解决语义理解不足导致的特征混淆问题。此外，作者还构建了Autobot4M数据集，包含约400万车辆图像和12,693个文本描述，以支持预训练。&lt;h4&gt;主要发现&lt;/h4&gt;在五个下游任务上的大量实验表明，VehicleMAE-V2模型具有优越的性能。通过利用车辆相关的多模态结构先验指导掩码令牌重建过程，模型能够学习到更具通用性的车辆中心感知表示。&lt;h4&gt;结论&lt;/h4&gt;VehicleMAE-V2通过探索和利用车辆相关的多模态结构先验来指导掩码令牌重建过程，显著增强了模型学习车辆中心感知的通用表示能力。提出的三个模块(SMM、CRM和SRM)分别整合了对称、轮廓和语义三种结构先验，有效解决了预训练过程中的关键问题。Autobot4M数据集的构建也为车辆感知研究提供了宝贵的资源。&lt;h4&gt;翻译&lt;/h4&gt;车辆中心感知在许多智能系统中起着关键作用，包括大规模监控系统、智能交通和自动驾驶。现有方法在预训练过程中缺乏对车辆相关知识的有效学习，导致建模通用车辆感知表示的能力较差。为了解决这个问题，我们提出了VehicleMAE-V2，一种新型的车辆中心预训练大模型。通过探索和利用车辆相关的多模态结构先验来指导掩码令牌重建过程，我们的方法可以显著增强模型学习车辆中心感知的通用表示能力。具体来说，我们设计了对称引导掩码模块(SMM)、轮廓引导表示模块(CRM)和语义引导表示模块(SRM)，将对称、轮廓和语义三种结构先验分别整合到令牌重建中。SMM利用车辆对称约束避免保留对称补丁，从而可以选择高质量的掩码图像补丁并减少信息冗余。CRM最小化轮廓特征和重建特征之间的概率分布差异，从而可以在像素级重建过程中保持整体车辆结构信息。SRM通过对比学习和跨模态蒸馏对齐图像文本特征，解决掩码重建过程中因语义理解不足导致的特征混淆问题。为了支持VehicleMAE-V2的预训练，我们构建了Autobot4M，一个包含约400万车辆图像和12,693个文本描述的大规模数据集。在五个下游任务上的大量实验证明了VehicleMAE-V2的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.</description>
      <author>example@mail.com (Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo)</author>
      <guid isPermaLink="false">2512.19934v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Next-Embedding Prediction Makes Strong Vision Learners</title>
      <link>http://arxiv.org/abs/2512.16922v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://sihanxu.me/nepa&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Next-Embedding Predictive Autoregression (NEPA)的自监督视觉学习方法，从学习表征转向学习模型，通过训练模型基于过去的嵌入预测未来的嵌入来实现视觉任务。&lt;h4&gt;背景&lt;/h4&gt;受自然语言生成式预训练成功的启发，研究者思考同样的原则是否能产生强大的自监督视觉学习者。&lt;h4&gt;目的&lt;/h4&gt;探索一种简单、可扩展且可能具有模态通用性的自监督视觉学习方法，从传统的学习表征转变为学习模型。&lt;h4&gt;方法&lt;/h4&gt;提出NEPA方法，使用因果掩码和停止梯度训练模型基于过去的嵌入预测未来的嵌入，采用简单Transformer在ImageNet-1k上进行预训练，仅以嵌入预测为学习目标，无需像素重建、离散标记、对比损失或任务特定头。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用嵌入预测作为学习目标的简单Transformer在ImageNet-1K上表现良好，ViT-B和ViT-L骨干网络在微调后分别达到83.8%和85.3%的top-1准确率，并在ADE20K的语义分割任务中有效迁移。&lt;h4&gt;结论&lt;/h4&gt;从嵌入进行生成式预训练为视觉自监督学习提供了一种简单、可扩展且可能具有模态通用性的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;受自然语言生成式预训练成功的启发，我们思考同样的原则是否能产生强大的自监督视觉学习者。我们不是训练模型输出特征供下游使用，而是训练它们生成嵌入来直接执行预测任务。这项工作探索了从学习表征到学习模型的这种转变。具体来说，模型学习基于过去的嵌入预测未来的嵌入，使用因果掩码和停止梯度，我们称之为Next-Embedding Predictive Autoregression (NEPA)。我们证明，一个在ImageNet-1k上仅以嵌入预测为唯一学习目标进行预训练的简单Transformer是有效的 - 不需要像素重建、离散标记、对比损失或任务特定头。这种公式保留了架构的简单性和可扩展性，不需要额外的设计复杂性。NEPA在各项任务上都取得了良好的结果，使用ViT-B和ViT-L骨干网络在微调后在ImageNet-1K上分别达到83.8%和85.3%的top-1准确率，并在ADE20K上的语义分割任务中有效迁移。我们相信，从嵌入进行生成式预训练为视觉自监督学习提供了一种简单、可扩展且可能具有模态通用性的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.</description>
      <author>example@mail.com (Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin, Joyce Chai, Saining Xie, Stella X. Yu)</author>
      <guid isPermaLink="false">2512.16922v2</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition</title>
      <link>http://arxiv.org/abs/2512.20501v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ph.D. manuscript; Supervisors/Mentors: Marie-Francine Moens and Tinne Tuytelaars&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索多模态对齐、翻译、融合和迁移技术，旨在提升机器对复杂输入的理解能力。论文分为五个章节，分别针对多模态机器学习中的不同挑战提出解决方案。&lt;h4&gt;背景&lt;/h4&gt;多模态机器学习面临理解复杂输入的挑战，需要有效处理不同类型数据之间的关系和转换。&lt;h4&gt;目的&lt;/h4&gt;增强机器对复杂多模态输入的理解能力，开发新的方法来处理空间语言、医学文本、知识图谱和动作识别等领域的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 第3章：开发空间推理BERT模型，将文本空间关系转换为视觉表示；2. 第4章：提出基于医学术语空间共现的损失函数，实现医学文本到解剖图谱的映射；3. 第5章：构建自然语言到实体和谓词的基准测试，解决文本提取的歧义问题；4. 第6章：提出融合视频帧和目标检测表示的方法，用于组合动作识别；5. 第7章：研究多模态知识迁移技术，使RGB模型能够模仿多模态融合能力。&lt;h4&gt;主要发现&lt;/h4&gt;1. 空间推理BERT能有效解码空间语言为视觉表示；2. 基于空间共现的损失函数显著提高了医学文本的可导航性；3. 构建的知识图谱基准测试能提供更清晰、可操作的见解；4. 多模态融合方法提高了动作识别的鲁棒性和准确性；5. 多模态知识迁移能在保持性能的同时减少计算需求。&lt;h4&gt;结论&lt;/h4&gt;这些研究贡献推进了空间语言理解、医学文本解释、知识图谱丰富化和动作识别的方法论，增强了计算系统处理复杂多模态输入的能力，为各种应用场景提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;多模态对齐：建立不同模态数据之间的对应关系；多模态翻译：将一种模态的信息转换为另一种模态；多模态融合：将不同模态的信息整合处理；多模态迁移：将一种模态学到的知识应用到另一种模态；空间推理BERT：用于处理空间关系的BERT模型；解剖图谱：人体解剖结构的可视化表示；知识图谱：实体及其关系的语义网络表示；自我中心动作识别：从第一人称视角识别动作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.</description>
      <author>example@mail.com (Gorjan Radevski)</author>
      <guid isPermaLink="false">2512.20501v1</guid>
      <pubDate>Wed, 24 Dec 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>On the abelian structure of noncompetitive chemical reaction networks</title>
      <link>http://arxiv.org/abs/2512.17491v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了非竞争性化学反应网络，发现它们是阿贝尔网络的特例，并建立了分析这类网络长期行为的统一代数和概率框架。&lt;h4&gt;背景&lt;/h4&gt;化学反应网络是描述复杂生化过程的基础模型。在生物化学和系统生物学中，相关化学反应网络嵌入在复杂网络中，使得局部化学反应网络必须响应内部和环境线索。&lt;h4&gt;目的&lt;/h4&gt;研究非竞争性化学反应网络的长期行为，建立统一的分析框架，理解它们如何响应扰动以及其静态状态的特征。&lt;h4&gt;方法&lt;/h4&gt;使用阿贝尔网络理论分析非竞争性化学反应网络，采用沙堆马尔可夫链描述网络对扰动的响应，其中状态空间是化学反应网络的静态状态集合。&lt;h4&gt;主要发现&lt;/h4&gt;非竞争性化学反应网络是阿贝尔网络的特例；对于具有有限状态空间的情况，循环静态状态的比例与阿贝尔网络的临界群一一对应；非竞争性化学反应网络可以实现ReLU神经网络。&lt;h4&gt;结论&lt;/h4&gt;这项工作为分析非竞争性化学反应网络的长期行为建立了统一的代数和概率框架，深化了对这类网络特性的理解。&lt;h4&gt;翻译&lt;/h4&gt;化学反应网络是描述复杂生化过程的基础模型。我们研究非竞争性化学反应网络，这是一类静态状态与速率无关的网络，可以实现ReLU神经网络。这项工作的一个核心贡献是，非竞争性化学反应网络是阿贝尔网络的一个特例，阿贝尔网络是自组织临界性的一个成熟框架。生物化学和系统生物学中感兴趣的化学反应网络嵌入在复杂网络中，因此局部化学反应网络必须响应内部和环境线索。我们使用沙堆马尔可夫链描述网络对这类扰动的响应，其状态空间是化学反应网络的静态状态集合，从这些静态状态出发不可能发生反应。向静态状态添加分子会诱导反应，使系统进入新的静态状态。对于有限状态空间的情况，我们使用阿贝尔网络理论得出，循环静态状态的比例与阿贝尔网络的临界群一一对应。总体而言，这项工作为分析非竞争性化学反应网络的长期行为建立了统一的代数和概率框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chemical reaction networks (CRNs) are foundational models for describing complex biochemical processes. We study noncompetitive CRNs, a class of networks whose static states are rate-independent, and that can implement ReLU neural networks. A central contribution of this work is that noncompetitive CRNs are special instances of Abelian networks (ANs) a well-established framework for self-organized criticality. CRNs of interest in biochemistry and systems biology are embedded in complex networks so that local CRNs have to respond to internal and environmental cues. We describe the network's response to such perturbations using a sandpile Markov chain whose state space is the set of CRN's static states, from where no reaction is possible. The addition of molecules to a static state induces reactions that move the system into a new static state. For noncompetitive CRNs of finite state space, we use AN theory to get that the fraction of static states that are recurrent is in one to one correspondence with the critical group of the AN.  Overall, this work establishes a unified algebraic and probabilistic framework for analyzing the long-term behavior of noncompetitive CRNs.</description>
      <author>example@mail.com (Louis Faul, Xavier Richard, Mary Betrisey, Christian Mazza)</author>
      <guid isPermaLink="false">2512.17491v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
  <item>
      <title>4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</title>
      <link>http://arxiv.org/abs/2512.17012v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种专门的多模态大语言模型4D-RGPT，结合感知4D蒸馏训练框架和新的基准测试R4D-Bench，有效提升了模型对4D结构和时间动态的理解能力。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在处理3D结构和时间动态方面能力有限，受到弱4D感知和时间理解的约束。现有3D和4D视频问答基准测试侧重静态场景，缺乏区域级提示功能。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在4D感知和时间理解方面的局限性，以及现有基准测试的不足。&lt;h4&gt;方法&lt;/h4&gt;作者引入了三个主要组件：(a) 4D-RGPT：专门设计的MLLM，能从视频输入中捕获4D表示并增强时间感知；(b) 感知4D蒸馏(P4D)：训练框架，将冻结专家模型的4D表示转移到4D-RGPT；(c) R4D-Bench：具有区域级提示的深度感知动态场景基准测试，通过混合自动化和人工验证流程构建。&lt;h4&gt;主要发现&lt;/h4&gt;4D-RGPT在现有4D VQA基准测试和提出的R4D-Bench基准测试上均取得显著改进。&lt;h4&gt;结论&lt;/h4&gt;通过4D-RGPT、P4D训练框架和R4D-Bench基准测试，成功解决了多模态大语言模型在4D感知和时间理解方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型取得了进展，但它们对3D结构和时间动态的推理能力仍然有限，受到弱4D感知和时间理解的限制。现有的3D和4D视频问答基准测试也强调静态场景，缺乏区域级提示。我们通过引入以下内容解决这些问题：(a) 4D-RGPT，一种专门设计的MLLM，能够从视频输入中捕获4D表示，增强时间感知；(b) 感知4D蒸馏(P4D)，一种训练框架，将冻结专家模型中的4D表示转移到4D-RGPT中，实现全面的4D感知；以及(c) R4D-Bench，一个具有区域级提示的深度感知动态场景基准测试，通过混合自动化和人工验证流程构建。我们的4D-RGPT在现有的4D VQA基准测试和提出的R4D-Bench基准测试上都取得了显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型(MLLMs)在处理4D信息(即3D空间加上时间维度)方面的能力不足问题。具体来说，当前MLLMs在推理3D结构和时间动态方面表现有限，难以理解深度信息和时间变化。这个问题在现实中非常重要，因为自动驾驶、工业检测等应用场景需要精确的4D理解能力，用户查询也需要指向特定区域而非模糊描述。此外，现有基准测试主要关注静态场景，缺乏区域级提示，无法全面评估模型在复杂动态场景中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统监督微调或强化学习难以处理大规模标注的动态视频数据；外部3D知识主要帮助理解静态视频；集成额外模块会增加推理负担。基于这些分析，作者借鉴了知识蒸馏技术，但针对4D理解进行了创新。他们利用现有的4D感知模型作为教师模型，参考了区域级理解方法（如使用边界框坐标或提取感兴趣区域），并改进了位置编码思想以适应时间维度。主要创新在于设计了感知4D蒸馏框架，通过双重蒸馏策略将4D知识从冻结教师模型转移到学生模型，同时不增加推理成本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过知识蒸馏技术，将专门训练的4D感知专家模型的知识转移到多模态大语言模型中，使其能更好理解和处理4D信息（空间+时间），同时不增加推理负担。整体流程包括：1) 构建基于现有MLLM的4D-RGPT架构，添加训练专用的4D感知模块（包括4D感知解码器、预测头和时间戳位置编码）；2) 使用冻结的4D感知模型作为教师，通过双重蒸馏策略（潜在蒸馏对齐抽象特征，显式蒸馏对齐可解释信号）将知识转移到学生模型；3) 在多个基准测试上评估性能，包括提出的新R4D-Bench基准测试；4) 通过消融实验验证各组件有效性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 4D-RGPT专门设计用于4D理解的多模态大语言模型；2) 感知4D蒸馏(P4D)框架，通过双重蒸馏策略将4D知识从专家模型转移到MLLM，只在训练时添加模块不增加推理成本；3) 时间戳位置编码(TPE)提供明确时间线索，增强时间感知；4) R4D-Bench基准测试，首个专注于区域级4D理解的测试集。相比之前工作，本文不依赖大规模标注数据，不引入额外推理模块，同时处理空间和时间维度，专注于区域级理解，并提供了更全面的评估基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创新的感知4D蒸馏框架，将4D感知能力融入多模态大语言模型，显著提升了其在区域级4D视频问答任务上的表现，同时不增加推理成本。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.</description>
      <author>example@mail.com (Chiao-An Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, Yu-Chiang Frank Wang, Min-Hung Chen)</author>
      <guid isPermaLink="false">2512.17012v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning for Static and Single-Event Dynamic Complex Network Analysis</title>
      <link>http://arxiv.org/abs/2512.17577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文主要开发针对静态和单事件动态网络的图表示学习新算法方法，专注于潜在空间模型特别是潜在距离模型，创建结构感知的网络表示，并实现统一的学习过程。&lt;h4&gt;背景&lt;/h4&gt;图表示学习在静态和单事件动态网络中的应用，以及潜在空间模型特别是潜在距离模型在捕捉网络特性方面的重要性。&lt;h4&gt;目的&lt;/h4&gt;开发新的算法方法来表示网络结构，实现层次化表达、社区表征、极端特征识别和影响动力学量化，并创建统一的学习过程。&lt;h4&gt;方法&lt;/h4&gt;使用潜在空间模型和潜在距离模型，开发结构感知的网络表示方法，实现统一的学习过程。&lt;h4&gt;主要发现&lt;/h4&gt;潜在距离模型能够自然地传递网络的重要特征如同质性、传递性和平衡理论；所提出的方法能够产生网络结构的层次化表达、社区特征表征、网络中极端特征的识别以及时态网络中影响动力学的量化。&lt;h4&gt;结论&lt;/h4&gt;通过统一网络嵌入的方法，能够全面而强大地表征网络结构并处理图分析的多样化任务。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文的主要目标是开发针对静态和单事件动态网络的图表示学习的新算法方法。在这一方向上，我们专注于潜在空间模型家族，特别是潜在距离模型，该模型自然地传递了网络的重要特征，如同质性、传递性和平衡理论。此外，该论文旨在创建结构感知的网络表示，这些表示能够产生网络结构的层次化表达、社区特征表征、网络中极端特征的识别以及时态网络中影响动力学的量化。关键在于，所提出的方法旨在定义统一的学习过程，消除了启发式方法和多阶段过程（如后处理步骤）的需要。我们的目标是探索通向全面而强大的统一网络嵌入的路径，这些嵌入能够表征网络结构并熟练处理图分析提供的多样化任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.</description>
      <author>example@mail.com (Nikolaos Nakis)</author>
      <guid isPermaLink="false">2512.17577v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Inverse-Designed Phase Prediction in Digital Lasers Using Deep Learning and Transfer Learning</title>
      <link>http://arxiv.org/abs/2512.17879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于条件生成对抗网络和改进U-Net架构的深度学习方法，用于数字激光器中空间光调制器加载相位的逆向设计，能够有效处理非线性效应，并通过迁移学习策略提高泛化能力。&lt;h4&gt;背景&lt;/h4&gt;数字激光器通过动态更新空间光调制器的相位模式来控制激光束，但由于存在模式竞争和增益饱和等非线性效应，通常需要依赖特定的人工定制方法或迭代过程来寻找合适的加载相位。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够学习数字激光器中非线性效应的模型，用于逆向设计加载相位，使其能够预测用于分析性和非分析性任意结构光场的空间光调制器加载相位。&lt;h4&gt;方法&lt;/h4&gt;采用基于条件生成对抗网络和改进U-Net架构的深度神经网络模型，并设计了特定的损失函数。同时引入迁移学习策略，允许从一类结构光束中获得的知识重用于另一类，以提高泛化能力和有限训练数据下的性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在L形数字激光器中处理非分析性光场时表现出比当前方法更优越的性能，迁移学习策略使得知识可以在不同类型的结构光束之间有效转移。&lt;h4&gt;结论&lt;/h4&gt;该首次提出的数字激光器学习框架不仅限于L形数字激光器，为其他数字激光系统中生成结构光提供了高效的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;数字激光器通过动态更新激光腔内空间光调制器的相位模式来控制激光束。由于数字激光系统中存在非线性效应，如模式竞争和增益饱和，通常需要依赖特定的人工定制方法或迭代过程来寻找合适的加载相位。本研究提出了一种基于条件生成对抗网络和改进U-Net架构的模型，并设计了特定的损失函数用于逆向设计加载相位。在本工作中，我们采用深度神经网络学习模拟L形数字激光器中的非线性效应，使其能够预测用于分析性和非分析性任意结构光场的空间光调制器加载相位。结果表明，该方法在L形数字激光器中处理非分析性光场时比当前方法具有更优越的性能。此外，引入了迁移学习策略，允许从一类结构光束中获得的知识有效地重用于另一类，从而增强泛化能力并提高有限训练数据下的性能。重要的是，该方法作为首个提出的数字激光器学习框架，不仅限于本研究中讨论的L形数字激光器，为其他数字激光系统中生成结构光提供了高效的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital lasers control the laser beam by dynamically updating the phase patterns of the spatial light modulator (SLM) within the laser cavity. Due to the presence of nonlinear effects, such as mode competition and gain saturation in digital laser systems, it is often necessary to rely on specifically manually tailored approach or iteration processes to find suitable loaded phases in Digital lasers. This study proposes a model based on Conditional Generative Adversarial Networks (cGAN) and a modified U-Net architecture, with designed loss functions to inverse design the loaded phases. In this work, we employ deep neural networks to learn the nonlinear effects in simulated L-shape digital lasers, enabling the prediction of SLM-loaded phases for both analytical and non-analytical arbitrary structured light fields. The results demonstrate superior performance on non-analytical light fields compared to the current methods in L-shape Digital lasers. Furthermore, a transfer learning strategy is introduced, allowing knowledge obtained from one class of structured beams to be effectively reused for another, thereby enhancing generalization and improving performance under limited training data. Importantly, this method, the first proposed learning framework for digital lasers, is not limited to the L-shaped digital lasers discussed in this study, providing an efficient alternative for generating structured light in other digital laser systems.</description>
      <author>example@mail.com (Yu-Che Wu, Kuo-Chih Chang, Shu-Chun Chu)</author>
      <guid isPermaLink="false">2512.17879v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views</title>
      <link>http://arxiv.org/abs/2512.17541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://fangzhou2000.github.io/projects/fleg&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FLEG是一种前馈网络，可以从任意视角重建语言嵌入的3D高斯表示，无需3D注释，能够高效生成具有准确几何、高保真外观和语言对齐语义的3D内容。&lt;h4&gt;背景&lt;/h4&gt;之前直接结合前馈重建与高斯头部的方法存在固定输入视图和3D训练数据不足的问题，限制了重建效果和应用范围。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要3D注释的训练框架，从任意未校准和未posed的多视角图像中进行2D到3D的提升，克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 提出3D注释免费训练框架，利用大规模视频数据和2D实例信息丰富语义嵌入；2) 提出实例引导的对比学习，将2D语义与3D表示对齐；3) 提出几何-语义分层稀疏化策略，缓解密集视图的高内存和计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;FLEG能够以前馈方式从任意稀疏或密集视图高效重建语言嵌入的3D高斯表示，在几何准确性、外观保真度和语义对齐方面表现优异，实验证明其在各种相关任务上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;FLEG通过创新的训练框架和学习策略，成功解决了现有方法在固定输入视图和3D训练数据方面的局限性，实现了从任意视角高效重建高质量语言嵌入的3D表示。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了FLEG，一种从任意视角重建语言嵌入3D高斯的前馈网络。之前直接将前馈重建与高斯头部结合的解决方案存在固定输入视图和3D训练数据不足的问题。相比之下，我们提出了一种3D注释免费训练框架，用于从任意未校准和未posed的多视角图像中进行2D到3D的提升。由于该框架不需要3D注释，我们可以利用具有容易获得的2D实例信息的大规模视频数据来丰富语义嵌入。我们还提出了实例引导的对比学习，将2D语义与3D表示对齐。此外，为了缓解密集视图的高内存和计算成本，我们进一步提出了几何-语义分层稀疏化策略。我们的FLEG能够以前馈方式从任意稀疏或密集视图高效重建语言嵌入的3D高斯表示，同时产生准确的几何、高保真外观和语言对齐的语义。大量实验表明，它在各种相关任务上优于现有方法。项目页面：https://fangzhou2000.github.io/projects/fleg。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何高效构建语言嵌入的3D高斯表示的问题，使系统能从任意角度的多视图图像中进行3D重建，同时支持自然语言交互。这个问题在现实中很重要，因为现有方法要么需要场景级优化（效率低，不适合实时应用如机器人导航），要么只能处理固定数量的输入视图（缺乏灵活性），或者依赖稀缺的3D标注数据。高效的语言嵌入3D表示对机器人导航、操作和增强/虚拟现实等多种应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先意识到3D标注数据稀缺而2D视频数据丰富，因此设计了无需3D标注的训练框架。他们借鉴了VGGT模型用于几何先验提取，CLIP用于视觉-语言特征，DPT用于解码器设计，以及SAM2用于实例分割。通过创新性地组合这些技术，并设计了新视图选择策略、实例引导对比学习和几何-语义分层稀疏化策略，解决了现有方法的局限性，实现了统一的前馈网络。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个前馈网络，能从任意未校准和未posed的多视图图像中重建语言嵌入的3D高斯表示，同时支持稀疏和密集视图输入。实现流程包括：1)使用大型transformer处理图像tokens并添加相机和注册令牌；2)通过VGGT获取几何伪标签并设计新视图选择策略；3)构建InstanceMV-14K数据集并使用CLIP提取语言特征；4)应用实例引导对比学习对齐2D语义和3D表示；5)采用几何-语义分层稀疏化策略减少冗余；6)结合多种损失函数训练网络，最终实现新视图合成、语言查询和3D编辑功能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的前馈网络支持任意数量输入视图；2)3D-无标注训练框架利用大规模2D数据；3)构建InstanceMV-14K数据集丰富语义；4)实例引导对比学习降低计算复杂度；5)几何-语义分层稀疏化策略优化表示。相比之前工作，FLEG效率更高（单次前向传播vs场景级优化），更灵活（支持任意视图数量），性能更稳定（随视图增加性能不下降），更实用（无需3D标注和相机参数），应用更全面（同时支持渲染、查询和编辑）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FLEG提出了一种高效的前馈网络，能够从任意数量的未校准多视图图像中重建语言嵌入的3D高斯表示，实现了高保真渲染、自然语言交互和3D编辑的统一框架，无需3D标注且支持实时应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.</description>
      <author>example@mail.com (Qijian Tian, Xin Tan, Jiayu Ying, Xuhong Wang, Yuan Xie, Lizhuang Ma)</author>
      <guid isPermaLink="false">2512.17541v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2512.17453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, 2 tables. Accepted for presentation at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Lite-STGNN是一种轻量级时空图神经网络，用于长期多元预测，结合了基于分解的时间建模和可学习的稀疏图结构。&lt;h4&gt;背景&lt;/h4&gt;长期多元时间序列预测领域需要高效且准确的模型，特别是对于长预测步长的情况。&lt;h4&gt;目的&lt;/h4&gt;开发一种参数效率高、训练速度快且能保持高准确性的时空图神经网络，用于长期多元时间序列预测。&lt;h4&gt;方法&lt;/h4&gt;结合趋势-季节性分解的时间模块和使用低秩Top-K邻域学习和保守的逐时间步门控的空间模块，实现空间校正以增强强大的线性基线。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上达到最先进的准确性（预测步长最多720步），比基于Transformer的方法训练速度快得多；空间模块比时间基线提高4.6%，Top-K提高局部性3.3%，学习到的邻接矩阵揭示了特定领域的交互动态。&lt;h4&gt;结论&lt;/h4&gt;Lite-STGNN为长期多元时间序列预测提供了一个紧凑、可解释且高效的框架。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Lite-STGNN，一种用于长期多元预测的轻量级时空图神经网络，它集成了基于分解的时间建模和可学习的稀疏图结构。时间模块应用趋势-季节性分解，而空间模块使用低秩Top-K邻域学习和保守的逐时间步门控进行消息传递，实现了增强强大线性基线的空间校正。Lite-STGNN在四个基准数据集上达到了最先进的准确性（预测步长最多720步），同时具有参数效率高且比基于Transformer的方法训练速度快得多的特点。消融研究表明，空间模块比时间基线提高4.6%，Top-K提高局部性3.3%，学习到的邻接矩阵揭示了特定领域的交互动态。因此，Lite-STGNN为长期多元时间序列预测提供了一个紧凑、可解释且高效的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.</description>
      <author>example@mail.com (Henok Tenaw Moges, Deshendran Moodley)</author>
      <guid isPermaLink="false">2512.17453v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs</title>
      <link>http://arxiv.org/abs/2512.17352v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 6 figures, 5 tables, journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种自适应剪枝算法和突发事件预测准确性(SEPA)指标，用于优化时空图神经网络在边缘计算环境中的部署，减少通信开销同时保持预测性能，特别是在处理交通动态变化方面。&lt;h4&gt;背景&lt;/h4&gt;时空图神经网络(ST-GNNs)适合处理智能交通系统中来自地理分布式传感器的高频数据流。然而，在边缘计算节点(云let)上部署时，由于相邻云let之间需要重复传输重叠的节点特征，会产生大量通信开销。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来减少分布式边缘计算环境中的通信开销，同时保持时空图神经网络对交通事件变化的预测能力，特别是针对交通减速和恢复等突发事件的响应能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自适应剪枝算法，动态过滤冗余的邻居特征，同时为预测保留最具信息量的空间上下文。该算法根据最近的模型性能调整剪枝率，使每个云let能够专注于经历交通变化的区域。同时引入了突发事件预测准确性(SEPA)指标，专门用于测量对交通减速和恢复的响应能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与传统指标相比，SEPA能够揭示空间连通性在预测动态和不规则交通方面的真正价值。自适应剪枝算法在所有在线半分布式设置中都保持了预测准确性，同时显著降低了通信成本，证明可以在不损害对关键交通事件响应能力的情况下减少通信。&lt;h4&gt;结论&lt;/h4&gt;自适应剪枝算法能够在保持预测准确性的同时显著降低通信成本，证明了在分布式边缘计算环境中优化通信的可能性，同时保持对关键交通事件的响应能力。&lt;h4&gt;翻译&lt;/h4&gt;时空图神经网络(ST-GNNs)非常适合处理智能交通系统中来自地理分布式传感器的高频数据流。然而，在边缘计算节点(云let)上部署时，由于相邻云let之间需要重复传输重叠的节点特征，会产生大量通信开销。为解决这一问题，我们提出了一种自适应剪枝算法，该算法动态过滤冗余的邻居特征，同时为预测保留最具信息量的空间上下文。该算法根据最近的模型性能调整剪枝率，使每个云let能够专注于经历交通变化的区域，而不会损害准确性。此外，我们引入了突发事件预测准确性(SEPA)，这是一种新型的事件导向指标，专门用于测量对交通减速和恢复的响应能力，而这些能力常常被标准误差指标所忽略。我们在传统联邦学习、无服务器联邦学习和谣言学习三种在线半分布式设置中，使用两个大规模交通数据集(PeMS-BAY和PeMSD7-M)，对短、中、长期预测范围进行了评估。实验表明，与传统指标相比，SEPA揭示了空间连通性在预测动态和不规则交通方面的真正价值。我们的自适应剪枝算法在所有在线半分布式设置中都保持了预测准确性，同时显著降低了通信成本，证明可以在不损害对关键交通事件响应能力的情况下减少通信。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.</description>
      <author>example@mail.com (Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas)</author>
      <guid isPermaLink="false">2512.17352v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations</title>
      <link>http://arxiv.org/abs/2512.17129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了DiffeoMorph，一种端到端可微分的框架，用于学习形态发生协议，指导一群遵循相同规则的智能体形成目标3D形状。该研究通过基于注意力的SE(3)-等变图神经网络实现智能体的位置和内部状态更新，并引入了一种基于3D Zernike多项式的新的形状匹配损失函数，使系统能够形成从简单椭球体到复杂形态的各种形状。&lt;h4&gt;背景&lt;/h4&gt;生物系统能够通过相同智能体（遵循相同内部规则且无需中央控制的细胞）的集体行为形成复杂的三维结构。这种分布式控制如何产生精确的全局模式，不仅是发育生物学中的核心问题，也与分布式机器人学、可编程物质和多智能体学习等领域相关。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够指导智能体群体形成目标3D形状的形态发生协议，解决分布式控制如何产生精确全局模式的问题，并为多个领域提供新的解决方案。&lt;h4&gt;方法&lt;/h4&gt;1. 引入DiffeoMorph框架，一种端到端可微分的形态发生协议学习系统2. 使用基于注意力的SE(3)-等变图神经网络，让每个智能体根据自身状态和其他智能体信号更新位置和内部状态3. 设计基于3D Zernike多项式的形状匹配损失函数，将预测和目标形状作为连续空间分布进行比较，而非离散点云4. 实现SO(3)不变性，包括对旋转不变但对反射敏感的对齐步骤，优化单位四元数以最佳对齐预测和目标Zernike谱5. 使用隐式微分计算通过对齐步骤的梯度6. 进行系统基准测试，建立形状匹配损失与其他标准距离度量的比较优势&lt;h4&gt;主要发现&lt;/h4&gt;1. 新的形状匹配损失函数在形状比较任务中优于其他标准距离度量2. DiffeoMorph能够仅使用最小的空间提示，形成从简单椭球体到复杂形态的各种形状3. 系统能够处理智能体数量变化和刚体变换的不变性&lt;h4&gt;结论&lt;/h4&gt;DiffeoMorph框架为分布式控制形成精确全局模式提供了一种有效解决方案，通过端到端可微的方法学习形态发生协议，在多个领域具有应用潜力，包括发育生物学、分布式机器人学、可编程物质和多智能体学习。&lt;h4&gt;翻译&lt;/h4&gt;生物系统能够通过相同智能体（遵循相同内部规则且无需中央控制的细胞）的集体行为形成复杂的三维结构。这种分布式控制如何产生精确的全局模式，不仅是发育生物学中的核心问题，也与分布式机器人学、可编程物质和多智能体学习等领域相关。在此，我们介绍了DiffeoMorph，一种端到端可微分的框架，用于学习形态发生协议，指导一群智能体形成目标3D形状。每个智能体使用基于注意力的SE(3)-等变图神经网络更新其位置和内部状态，基于自身内部状态和从其他智能体接收的信号。为了训练该系统，我们引入了一种基于3D Zernike多项式的新形状匹配损失函数，它将预测和目标形状作为连续空间分布进行比较，而非离散点云，并且对智能体排序、智能体数量和刚体变换具有不变性。为了实现完整的SO(3)不变性——对旋转不变但对反射敏感，我们包括了一个对齐步骤，在计算损失之前最优地旋转预测的Zernike谱以匹配目标。这形成了一个双层问题，内层循环优化单位四元数以实现最佳对齐，外层循环更新智能体模型。我们使用隐式微分计算通过了对齐步骤的梯度。我们进行系统基准测试，建立我们的形状匹配损失在形状比较任务中优于其他标准距离度量的优势。然后我们证明，DiffeoMorph仅使用最小的空间提示，就能形成从简单椭球体到复杂形态的各种形状。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的问题是：如何让一群遵循相同内部规则的智能体（如细胞）在没有中央控制的情况下，通过集体行为形成精确的复杂三维结构。这个问题在发育生物学中是核心问题，关系到相同细胞如何协调形成复杂结构；在分布式机器人学中关系到自组织系统设计；在可编程物质领域有助于设计自组装材料；在多智能体学习中涉及分布式控制策略学习。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：强化学习效率低且不可微，而端到端可微学习如神经细胞自动机存在形状比较方法的限制。作者借鉴了生物形态发生概念，特别是细胞通过接触介导的粘附和长距离信号相互作用的机制。方法设计上，作者采用了SE(3)-等变图神经网络架构，并扩展了神经细胞自动机的思想到连续空间，同时加入基于注意力的交互机制。关键创新在于设计了基于3D Zernike多项式的形状匹配损失函数和双层优化结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过可微分的基于智能体的模拟，学习一种形态发生协议，使一群遵循相同规则的智能体能够自组织成目标3D形状。整体流程分为两阶段：1)形态发生模拟阶段：每个智能体有基因表达向量和极性向量，通过基于注意力的SE(3)-等变图神经网络感知邻居并更新状态；2)形状匹配阶段：将形状投影到3D Zernike多项式上得到光谱表示，通过优化单位四元数对齐光谱，计算均方误差作为损失。优化采用双层结构：外层更新智能体模型，内层解决光谱对齐，使用隐式微分计算梯度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于3D Zernike多项式的形状匹配损失函数，能处理不同数量智能体、无点对点对应关系和任意旋转的形状比较；2)双层优化结构和隐式微分方法实现高效端到端训练；3)SE(3)-等变力模型确保旋转等变性同时保持反射敏感性。相比之前工作，不同之处在于：不依赖点对点对应关系或固定点数；不通过消除方向信息实现旋转不变性，而是通过显式对齐光谱；计算复杂度不随点数增加而显著增加；是端到端可微的，比强化学习更高效。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DiffeoMorph提出了一种基于3D Zernike多项式的可微分框架，使一群遵循相同规则的智能体能够仅通过最小空间线索自组织成复杂的三维形状，同时解决了形状比较中的旋转不变性和点数鲁棒性问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.</description>
      <author>example@mail.com (Seong Ho Pahng, Guoye Guan, Benjamin Fefferman, Sahand Hormoz)</author>
      <guid isPermaLink="false">2512.17129v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Dirichlet Meets Horvitz and Thompson: Estimating Homophily in Large Networks via Sampling</title>
      <link>http://arxiv.org/abs/2512.17084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于抽样的框架，通过狄利克雷能量估计网络同配性，适用于资源受限或隐私敏感场景。&lt;h4&gt;背景&lt;/h4&gt;评估网络同配性对理解图结构规律和选择合适模型至关重要，但传统方法需要完整网络数据，这在实际应用中常不可行。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需完整网络拓扑和节点特征即可估计同配性的方法，适用于大规模、动态、资源有限或隐私受限的环境。&lt;h4&gt;方法&lt;/h4&gt;提出基于抽样的框架，利用狄利克雷能量（拉普拉斯基的总变分）和Horvitz-Thompson估计器从部分图观测中无偏推断同配性。&lt;h4&gt;主要发现&lt;/h4&gt;狄利克雷能量可以从抽样图中一致估计；实验表明所提HT估计器能有效捕捉网络同配性结构。&lt;h4&gt;结论&lt;/h4&gt;所提出的抽样框架能够在不访问完整网络的情况下可靠估计同配性，适用于各种实际应用场景。&lt;h4&gt;翻译&lt;/h4&gt;评估大规模网络中的同配性对于理解图中的结构规律至关重要，从而指导选择采用从网络数据中学习的模型（如图神经网络）。评估平滑度指标需要访问整个网络拓扑和节点特征，这在几个大规模、动态、资源有限或隐私受限的情况下可能不切实际。在这项工作中，我们提出了一种基于抽样的框架，通过图信号的狄利克雷能量（基于拉普拉斯的总变分）来估计同配性，利用Horvitz-Thompson（HT）估计器从部分图观测中进行无偏推断。狄利克雷能量是图边上节点特征偏差平方的总和；因此，可以在边包含概率可以解析推导并用作所提出的HT估计器中的权重的一般网络抽样设计下进行估计。我们建立了可以从抽样图中一致估计狄利克雷能量的理论，并经验性研究了其他异配性度量。在几个异配性基准数据集上的实验证明了所提出的HT估计器能够可靠地捕捉来自抽样网络测量的同配性结构（或缺乏同配性）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Assessing homophily in large-scale networks is central to understanding structural regularities in graphs, and thus inform the choice of models (such as graph neural networks) adopted to learn from network data. Evaluation of smoothness metrics requires access to the entire network topology and node features, which may be impractical in several large-scale, dynamic, resource-limited, or privacy-constrained settings. In this work, we propose a sampling-based framework to estimate homophily via the Dirichlet energy (Laplacian-based total variation) of graph signals, leveraging the Horvitz-Thompson (HT) estimator for unbiased inference from partial graph observations. The Dirichlet energy is a so-termed total (of squared nodal feature deviations) over graph edges; hence, estimable under general network sampling designs for which edge-inclusion probabilities can be analytically derived and used as weights in the proposed HT estimator. We establish that the Dirichlet energy can be consistently estimated from sampled graphs, and empirically study other heterophily measures as well. Experiments on several heterophilic benchmark datasets demonstrate the effectiveness of the proposed HT estimators in reliably capturing homophilic structure (or lack thereof) from sampled network measurements.</description>
      <author>example@mail.com (Hamed Ajorlou, Gonzalo Mateos, Luana Ruiz)</author>
      <guid isPermaLink="false">2512.17084v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections</title>
      <link>http://arxiv.org/abs/2512.16950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 14 figures, submitted to Forestry: An International Journal of Forest Research&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新方法，将Finer-CAM解释与TLS投影中的树木结构特征片段相联系，系统评估了驱动树种区分的特征。&lt;h4&gt;背景&lt;/h4&gt;树种类分类是森林遥感领域几十年的核心研究方向。虽然新的传感器和分类方法如TLS和深度学习已达到最先进准确率，但它们的决策过程仍不明确。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法链接Finer-CAM解释与TLS投影中表示树木结构特征的片段，系统评估哪些特征驱动树种区分，并理解模型决策过程。&lt;h4&gt;方法&lt;/h4&gt;使用来自欧洲七种树木的2445棵树的TLS数据，训练和验证五个YOLOv8模型，采用交叉验证，并分析630个显著性映射。&lt;h4&gt;主要发现&lt;/h4&gt;模型主要依靠TLS投影中的树冠特征进行物种分类；对于银桦、欧洲山毛榉、英国橡树和挪威云杉尤为明显；欧洲梣、苏格兰松和道格拉斯冷杉则更依赖树干特征；特别是较细树枝的表示有助于模型决策；模型认为相似的树种类与人类专家判断一致。&lt;h4&gt;结论&lt;/h4&gt;需要改进对树种类分类模型决策过程的理解，以揭示数据集和模型的局限性、偏见，并建立对模型预测的信心。&lt;h4&gt;翻译&lt;/h4&gt;树种类分类是森林遥感领域几十年的核心研究方向。新的传感器和分类方法如TLS和深度学习已达到最先进准确率，但它们的决策过程仍不明确。Finer-CAM等方法可以突出显示TLS投影中对目标树种分类有贡献的特征，但在相似的外表特征树种中不常见。我们提出了一种新方法，将Finer-CAM解释与TLS投影中表示树木结构特征的片段联系起来，系统评估哪些特征驱动物种区分。使用来自欧洲七种树木的2445棵树的TLS数据，我们训练和验证了五个YOLOv8模型，并进行交叉验证，平均准确率达到96%（标准差0.24%）。对630个显著性映射的分析显示，模型主要依靠TLS投影中的树冠特征进行物种分类。对于银桦、欧洲山毛榉、英国橡树和挪威云杉，这一结果尤为明显，而欧洲梣、苏格兰松和道格拉斯冷杉的区分则更多依赖树干特征。特别是较细树枝的表示有助于模型的决策。模型认为相似的树种类与人类专家认为的相似种类一致。此外，我们的结果强调需要改进对树种类分类模型决策过程的理解，以帮助揭示数据集和模型的局限性、偏见，并建立对模型预测的信心。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的问题是：虽然基于深度学习的树木物种分类模型已经取得了很高的准确率，但这些模型的决策过程仍然不清楚。这个问题很重要，因为了解模型的决策过程可以帮助识别模型局限性和潜在偏差，建立对模型预测的信任，防止模型学习数据中的'捷径'（shortcut learning），并最终改进数据集和模型以提高分类方法的性能和稳定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到深度学习模型决策过程不透明的问题，然后借鉴了现有的可解释AI方法，特别是类激活映射技术（如Grad-CAM和Finer-CAM）。他们提出将Finer-CAM解释与代表树木结构特征的TLS点云投影片段联系起来，系统评估哪些特征驱动物种区分。作者使用了YOLOv8模型，并设计了一种启发式方法将显著性图中的像素分类为与树木结构相关的部分。作者确实借鉴了现有工作，包括使用FORSpecies20k数据集、采用类似Zou等人的正交投影方法、使用Finer-CAM技术，以及参考Hinns &amp; Martens的半全局解释方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合YOLOv8分类模型和Finer-CAM可解释AI技术，分析模型如何利用树木的结构特征进行物种分类，并将模型的决策过程与树木的具体结构特征关联起来。整体流程包括：1）数据准备（从FORSpecies20k选择数据并预处理）；2）模型训练（使用5折交叉验证训练5个YOLOv8模型）；3）可解释性分析（使用Finer-CAM生成显著性图，并开发启发式方法将图像分割为树木结构的不同部分）；4）结果分析（分析不同树种中哪些结构特征对模型决策最重要，比较模型一致性，评估模型认为哪些树种最相似）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出新方法将Finer-CAM解释与树木结构特征片段联系起来；2）开发启发式方法将显著性图像素分类为树木结构部分，减少认知偏差；3）首次对基于CNN的树木物种分类模型如何使用TLS数据进行全面系统分析；4）分析不同树种中模型关注的结构特征差异。相比之前的工作，不同之处在于：之前研究通常只提供示例实例的显著性图并进行视觉解释，而本研究进行了系统的定量分析；之前研究直接在点云上应用分类模型，而本研究使用2D投影使特征解释更直观；本研究通过将显著性像素与结构关联减少了认知偏差；使用更先进的YOLOv8模型；不仅关注模型性能，还深入分析了决策过程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过结合YOLOv8分类模型和Finer-CAM可解释AI技术，首次系统揭示了基于TLS点云投影的树木物种分类模型如何利用树木的结构特征进行决策，为提高分类模型的透明度、可靠性和性能提供了重要见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.</description>
      <author>example@mail.com (Adrian Straker, Paul Magdon, Marco Zullich, Maximilian Freudenberg, Christoph Kleinn, Johannes Breidenbach, Stefano Puliti, Nils Nölke)</author>
      <guid isPermaLink="false">2512.16950v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>From Pixels to Predicates Structuring urban perception with scene graphs</title>
      <link>http://arxiv.org/abs/2512.19221v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, CAADRIA2026 presentation forthcoming&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图的三阶段管道，将街道视图图像转换为结构化表示，用于预测城市感知指标，显著提高了预测准确性和跨城市泛化能力。&lt;h4&gt;背景&lt;/h4&gt;感知研究越来越多地使用街道景观建模，但许多方法仍依赖像素特征或物体共现统计，忽略了塑造人类感知的显式关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能捕捉城市环境中显式关系的结构化表示方法，用于准确预测城市感知指标。&lt;h4&gt;方法&lt;/h4&gt;三阶段管道：1)使用开放集全景场景图模型(OpenPSG)提取图像中的对象-谓词-对象三元组；2)通过异构图自编码器(GraphMAE)学习场景级嵌入；3)使用神经网络从嵌入预测感知分数。&lt;h4&gt;主要发现&lt;/h4&gt;1)该方法比基线模型平均提高26%的感知预测准确性；2)在跨城市预测任务中保持强泛化性能；3)结构化表示揭示了导致低感知分数的关系模式，如墙上的涂鸦和停在人行道上的汽车。&lt;h4&gt;结论&lt;/h4&gt;基于图的结构为建模城市感知提供了表达性强、可泛化和可解释的信号，推进了以人为中心和上下文感知的城市分析。&lt;h4&gt;翻译&lt;/h4&gt;感知研究越来越多地使用街道景观建模，但许多方法仍然依赖像素特征或物体共现统计，忽略了塑造人类感知的显式关系。本研究提出一个三阶段管道，将街道视图图像(SVI)转换为结构化表示，用于预测六个感知指标。第一阶段使用开放集全景场景图模型(OpenPSG)解析每张图像，提取对象-谓词-对象三元组。第二阶段通过异构图自编码器(GraphMAE)学习紧凑的场景级嵌入。第三阶段使用神经网络从这些嵌入预测感知分数。我们针对准确性、精确度和跨城市泛化，将该方法与仅基于图像的基线模型进行比较。结果表明：(i)我们的方法比基线模型平均提高26%的感知预测准确性，(ii)在跨城市预测任务中保持强泛化性能。此外，结构化表示阐明了哪些关系模式导致城市场景中感知分数降低，例如墙上的涂鸦和停在人行道上的汽车。总体而言，本研究表明基于图的结构为建模城市感知提供了表达性强、可泛化和可解释的信号，推进了以人为中心和上下文感知的城市分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.</description>
      <author>example@mail.com (Yunlong Liu, Shuyang Li, Pengyuan Liu, Yu Zhang, Rudi Stouffs)</author>
      <guid isPermaLink="false">2512.19221v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps</title>
      <link>http://arxiv.org/abs/2506.15849v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This version was submitted to ICRA 2026 conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出PRISM-Loc，一种用于大型户外环境定位的轻量级且稳健的方法，结合紧凑拓扑表示和基于原始激光雷达扫描的扫描匹配与路沿检测模块。&lt;h4&gt;背景&lt;/h4&gt;在资源受限平台上进行大型户外环境定位面临挑战，需要考虑实时性能和对常见城市感知挑战的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;设计适用于资源受限平台的定位方法，强调实时性能和对常见城市感知挑战的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;结合紧凑拓扑表示和新型扫描匹配与路沿检测模块，直接在原始激光雷达扫描上操作，使用全局位置识别和原始扫描匹配技术在紧凑拓扑地图中提供准确定位。&lt;h4&gt;主要发现&lt;/h4&gt;在标准基准测试和嵌入式平台上实验证明方法有效；在ITLP-Campus数据集上实现99%成功率；每次定位运行时间为150毫秒；使用20MB地图进行定位。&lt;h4&gt;结论&lt;/h4&gt;PRISM-Loc是一种有效的大型户外环境定位解决方案，特别适合资源受限平台，具有高准确率和实时性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了PRISM-Loc - 一种用于大型户外环境定位的轻量级且稳健的方法，它结合了紧凑的拓扑表示和一种新颖的扫描匹配及路沿检测模块，直接在原始激光雷达扫描上操作。该方法专为资源受限平台设计，强调实时性能和对常见城市感知挑战的鲁棒性。它使用全局位置识别和原始扫描匹配技术在紧凑的拓扑地图中提供准确的定位。在标准基准测试和嵌入式平台上的实验证明了我们方法的有效性。我们的方法在大型ITLP-Campus数据集上实现了99%的成功率，同时每次定位运行时间为150毫秒，使用20MB的地图进行定位。我们强调了三个主要贡献：(1) 城市规模定位的紧凑表示；(2) 直接在原始激光雷达点上操作的新型路沿检测和扫描匹配流程；(3) 对我们方法的彻底评估和性能分析。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决资源受限平台上的城市环境中远程激光雷达定位问题。传统方法依赖密集的全局激光雷达地图，但随着环境范围扩大，这些地图计算成本高昂且内存需求大，限制了实际应用。在城市环境中，GPS信号可能被遮挡，准确高效的定位对自动驾驶汽车和移动机器人至关重要，因此需要一种轻量级、实时且鲁棒的定位方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法如RTAB-Map在长距离和嘈杂定位源下的局限性，包括内存消耗大和地图不一致问题。他们借鉴了拓扑SLAM的基本概念，将环境表示为图结构（节点为关键位置，边为可导航路径）。方法分为两阶段：先通过位置识别确定大致位置，再用扫描匹配细化估计。作者使用了MinkLoc3D进行位置识别，并改进了之前PRISM-TopoMap的扫描匹配方法，增加了专门针对原始激光雷达点云的路沿检测算法，以提高城市环境中的定位准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用紧凑的拓扑地图表示大型城市环境，结合位置识别和扫描匹配技术实现轻量级、准确的定位。整体流程包括：1) 地图创建：构建图结构，每个节点存储位置描述符和2D网格；2) 定位跟踪：维护当前在图中的位置和相对姿态，根据重叠度决定是否移动到相邻节点；3) 全局定位：使用MinkLoc3D进行位置识别，找到候选位置后通过扫描匹配（结合ORB特征、改进RANSAC和路沿检测）精确估计姿态；4) 路沿检测：通过平面拟合分割地面，检测边缘作为路沿特征，提高城市环境中的匹配准确性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 紧凑的城市规模定位表示，拓扑地图每个位置仅需几KB存储；2) 新颖的激光雷达扫描匹配算法，直接在原始扫描上操作；3) 原始的路沿检测算法，专门针对激光雷达点云设计。相比之前工作，PRISM-Loc显著减少了内存需求（传统方法可能需8GB，而此方法仅需20MB），提高了计算效率（定位时间0.3秒），增强了城市环境中的鲁棒性（通过路沿检测），并且相比其他方法如BEVPlace++在多个指标上表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PRISM-Loc提出了一种轻量级、高效的定位方法，通过结合紧凑的拓扑地图表示、新颖的扫描匹配算法和原始的路沿检测技术，实现了在资源受限平台上的城市环境中高精度、实时的机器人定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-06-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose PRISM-Loc - a lightweight and robust approach for localization in large outdoor environments that combines a compact topological representation with a novel scan-matching and curb-detection module operating on raw LiDAR scans. The method is designed for resource-constrained platforms and emphasizes real-time performance and resilience to common urban sensing challenges. It provides accurate localization in compact topological maps using global place recognition and an original scan matching technique. Experiments on standard benchmarks and on an embedded platform demonstrate the effectiveness of our approach. Our method achieves a 99\% success rate on the large-scale ITLP-Campus dataset while running at 150 ms per localization and using a 20 MB map for localization. We highlight three main contributions: (1) a compact representation for city-scale localization; (2) a novel curb detection and scan matching pipeline operating directly on raw LiDAR points; (3) a thorough evaluation of our method with performance analysis.</description>
      <author>example@mail.com (Kirill Muravyev, Artem Kobozev, Vasily Yuryev, Alexander Melekhin, Oleg Bulichev, Dmitry Yudin, Konstantin Yakovlev)</author>
      <guid isPermaLink="false">2506.15849v2</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</title>
      <link>http://arxiv.org/abs/2512.19402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Real2Edit2Real框架通过结合3D可编辑性与2D视觉数据生成新的操作演示，显著提高了数据效率，减少了实际数据收集的需求。&lt;h4&gt;背景&lt;/h4&gt;机器人学习的最新进展由大规模数据集和强大的视觉运动策略架构推动，但策略的稳健性仍然受到收集多样化演示数据的显著成本限制，特别是在操作任务中的空间泛化方面。&lt;h4&gt;目的&lt;/h4&gt;减少重复的数据收集，提出一个框架通过3D可编辑性与2D视觉数据之间的桥梁来生成新的演示。&lt;h4&gt;方法&lt;/h4&gt;使用多视图RGB观察重建场景几何；在点云上进行深度可靠的3D编辑生成新的操作轨迹；几何校正机器人姿态恢复物理一致的深度；提出以深度为主要控制信号的多条件视频生成模型，合成空间增强的多视图操作视频。&lt;h4&gt;主要发现&lt;/h4&gt;在四个实际操作任务上的实验表明，仅使用1-5个源演示生成的数据训练的策略可以匹配或优于使用50个实际演示训练的策略，数据效率提高了10-50倍；在高度和纹理编辑上的实验结果证明了该框架的灵活性和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;该框架有潜力作为一个统一的数据生成框架，有效解决机器人学习中数据收集成本高的问题。&lt;h4&gt;翻译&lt;/h4&gt;机器人学习的最新进展由大规模数据集和强大的视觉运动策略架构推动，然而策略的稳健性仍然受到收集多样化演示数据的显著成本限制，特别是在操作任务中的空间泛化方面。为了减少重复的数据收集，我们提出了Real2Edit2Real，一个通过3D可编辑性与2D视觉数据之间的桥梁来生成新演示的框架。我们的方法首先使用具有度量尺度3D重建模型的多视图RGB观察重建场景几何。基于重建的几何，我们在点云上进行深度可靠的3D编辑，以生成新的操作轨迹，同时几何校正机器人姿态以恢复物理一致的深度，这作为合成新演示的可靠条件。最后，我们提出了一个以深度为主要控制信号的多条件视频生成模型，结合动作、边缘和射线图，来合成空间增强的多视图操作视频。在四个实际操作任务上的实验表明，仅使用1-5个源演示生成的数据训练的策略可以匹配或优于使用50个实际演示训练的策略，将数据效率提高了10-50倍。此外，在高度和纹理编辑上的实验结果证明了该框架的灵活性和可扩展性，表明其有潜力作为一个统一的数据生成框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人学习中的数据收集成本高、效率低的问题。具体来说，机器人学习需要大量多样化的演示数据，特别是对于空间泛化任务（操作任务中物体随机排列的场景），收集这些数据的成本非常高。这个问题在现实中很重要，因为它限制了机器人学习策略的鲁棒性和可扩展性，使得在实际应用中难以快速部署高性能的机器人系统。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为3D数据比2D图像具有更大的编辑灵活性，因此首先对源演示进行几何重建。为了提高重建在机器人场景中的准确性，他们提出结合真实和模拟数据的混合训练范式。方法设计包含三个主要组件：几何重建、空间编辑和视频生成。作者确实借鉴了现有工作，如使用VGGT作为基础模型重建几何，受DemoGen启发分解点云演示为运动段和技能段，以及参考GE-Sim等使用Transformer架构进行视频生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过3D可编辑性连接2D视觉数据，实现可扩展的演示生成，并将深度图作为3D模态和2D观察之间的自然界面。整体流程分为三步：1)尺度感知的几何重建：使用混合训练范式从多视图RGB观测重建场景几何；2)深度可靠的空间编辑：基于点云编辑和运动规划合成新轨迹，同时校正机器人姿态以获得物理一致的深度图；3)3D控制视频生成：以深度为主要控制信号，结合边缘、动作和射线图生成多视图一致的操作视频。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：不依赖仿真引擎直接从RGB生成数据；尺度感知的几何重建；深度可靠的空间编辑；3D控制视频生成；以及10-50倍的数据效率提升。相比之前工作的不同之处在于：与MimicGen家族不同，避免了Sim2Real差距；与DemoGen不同，兼容多视图RGB相机设置；与Real2Render2Real和RoboSplat不同，不需要密集图像捕获；与基于视频生成的工作不同，不仅增强视觉方面，还增加了物体空间分布和机器人轨迹的多样性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Real2Edit2Real通过结合3D可编辑性和2D视觉数据，实现了高效、真实的多视图机器人演示生成，将数据效率提高了10-50倍，同时保持了视觉真实性和正确的物理交互。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.</description>
      <author>example@mail.com (Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong)</author>
      <guid isPermaLink="false">2512.19402v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation</title>
      <link>http://arxiv.org/abs/2512.19088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2026 Workshop on New Frontiers in Information Retrieval&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种改进的3D物体定位和检索方法，解决了现有方法的计算效率问题和对稀有物体的泛化问题，通过结合2D开放词汇检测器与3D处理，实现了高效准确的结果。&lt;h4&gt;背景&lt;/h4&gt;从场景级点云中定位和检索物体是一个具有挑战性的问题，在机器人和增强现实领域有广泛应用。现有方法依赖SAM和CLIP从点云伴随的图像生成和分类3D实例掩码，导致大量计算开销和处理速度慢，限制了在现实场景中的部署。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的计算效率问题，提高对不常见物体类别的泛化能力，实现从开放文本查询中快速准确地检索稀有实例。&lt;h4&gt;方法&lt;/h4&gt;提出一种从RGB图像生成3D实例掩码的方法，使用2D开放词汇检测器指导生成过程，继承2D检测器识别新物体的能力，同时保持高效的分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;Open-YOLO 3D通过使用实时2D检测器减轻计算负担，直接从点云生成类别无关的掩码，消除了对SAM和CLIP的需求，显著减少了推理时间；然而，该方法通常无法泛化到3D训练数据中很少出现的物体类别。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够处理新颖物体，同时保持高效的分类性能，实现了从开放文本查询中快速准确地检索稀有实例，代码将在https://github.com/ndkhanh360/BoxOVIS上提供。&lt;h4&gt;翻译&lt;/h4&gt;在场景级点云中定位和检索物体是一个具有挑战性的问题，在机器人和增强现实领域有广泛应用。此任务通常被表述为开放词汇3D实例分割。尽管最近的方法表现出强大的性能，但它们严重依赖SAM和CLIP从点云伴随的图像生成和分类3D实例掩码，导致大量计算开销和缓慢的处理速度，限制了它们在现实环境中的部署。Open-YOLO 3D通过使用实时2D检测器来分类由预训练3D分割器直接从点云生成的类别无关掩码，减轻了这一问题，消除了对SAM和CLIP的需求，并显著减少了推理时间。然而，Open-YOLO 3D通常无法泛化到在3D训练数据中很少出现的物体类别。在本文中，我们提出了一种方法，通过2D开放词汇检测器指导，从RGB图像生成新物体的3D实例掩码。我们的方法继承了2D检测器识别新物体的能力，同时保持高效的分类，能够从开放文本查询中快速准确地检索稀有实例。我们的代码将在https://github.com/ndkhanh360/BoxOVIS上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从3D场景中高效且准确地检索对象的问题，特别是针对在训练数据中不常见的罕见对象。这个问题在机器人和增强现实等领域非常重要，因为现有方法要么计算开销大（依赖SAM和CLIP导致处理缓慢），要么对罕见对象识别能力差，限制了它们在实际应用中的部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有开放词汇3D实例分割方法的局限性：计算开销大和对罕见类别泛化能力差。他们借鉴了Open-YOLO 3D的效率思路和Mask3D的点基础掩码生成方法，同时引入YOLO-World作为2D检测器。作者的创新在于利用2D检测器的预测来引导3D点云中罕见对象的掩码生成，避免了计算密集型的SAM模型，同时保留了2D检测器对罕见类别的识别能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D开放词汇检测器的预测来引导3D点云中罕见对象的掩码生成，同时保持高效的分类能力。整体流程包括：1) 使用基于图的分割方法处理点云获取超点；2) 使用Mask3D生成点基础掩码；3) 使用YOLO-World在RGB图像上生成边界框；4) 将2D边界框提升到3D并提取超点形成新掩码；5) 通过投影3D实例到2D标签图进行分类，匹配输入查询。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 框引导的RGBD基础掩码生成，利用2D检测器预测来形成罕见对象掩码；2) 高效的分类方法，避免使用计算密集型的CLIP特征提取；3) 对罕见类别的改进性能。相比Open-YOLO 3D等之前的工作，本文方法不完全依赖3D分割器生成候选对象，而是利用2D检测器来检测和形成罕见对象的掩码，显著提高了对罕见类别的泛化能力，同时保持了较高的处理效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合2D检测器识别能力和3D点云几何信息的开放词汇3D实例分割方法，实现了高效且能够检测罕见对象的3D场景检索系统。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.</description>
      <author>example@mail.com (Khanh Nguyen, Dasith de Silva Edirimuni, Ghulam Mubashar Hassan, Ajmal Mian)</author>
      <guid isPermaLink="false">2512.19088v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation</title>
      <link>http://arxiv.org/abs/2512.18991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了ICP-4D框架，通过几何关系统一空间和时间推理，无需训练即可实现4D LiDAR全景分割，使用ICP算法和Sinkhorn软匹配实现实例关联，在多个数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;现有4D LiDAR全景分割方法通常需要大型叠加点云训练深度神经网络或设计专门的实例关联模块，这些方法执行冗余的点处理导致计算成本高昂，同时忽略了原始点云中的几何先验。&lt;h4&gt;目的&lt;/h4&gt;开发一个简单有效的无需训练的框架，通过实例级点集之间的几何关系统一空间和时间推理，避免冗余计算并利用点云中的几何先验。&lt;h4&gt;方法&lt;/h4&gt;应用迭代最近点(ICP)算法通过估计变换对齐源和目标点集关联时间上一致的实例；引入基于Sinkhorn的软匹配稳定嘈杂预测下的关联；设计考虑静态、动态和缺失三种实例类型的管道，提供计算效率和遮挡感知匹配能力。&lt;h4&gt;主要发现&lt;/h4&gt;在SemanticKITTI和nuScenes数据集上的广泛实验表明，该方法一致性地优于最先进方法，无需额外训练或额外的点云输入即可实现优异性能。&lt;h4&gt;结论&lt;/h4&gt;ICP-4D框架通过利用原始点云中的几何先验关系，解决了现有4D LiDAR全景分割方法中的冗余计算问题，提供了一个简单、高效且无需训练的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;4D LiDAR全景分割的主导范式通常需要使用大型叠加点云训练深度神经网络或设计专门的实例关联模块。然而，这些方法执行冗余的点处理，导致计算成本高昂，同时仍然忽略了原始点云中固有的丰富几何先验。为此，我们引入了ICP-4D，一个简单而有效的无需训练的框架，它通过实例级点集之间的几何关系统一了空间和时间推理。具体来说，我们应用迭代最近点(ICP)算法，通过估计的变换直接对齐源和目标点集，从而关联时间上一致的实例。为了在嘈杂的实例预测下稳定关联，我们引入了基于Sinkhorn的软匹配。这利用了底层实例分布来获得准确的点对应关系，从而实现鲁棒的几何对齐。此外，我们精心设计的管道考虑了三种实例类型（静态、动态和缺失），提供了计算效率和遮挡感知匹配能力。我们在SemanticKITTI和nuScenes上的广泛实验表明，我们的方法一致性地优于最先进的方法，即使没有额外的训练或额外的点云输入。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决4D LiDAR全景分割中的实例关联问题，确保不同时间帧中相同物体能保持一致的ID标识。这个问题在自动驾驶领域至关重要，因为系统需要精确感知动态3D环境，而LiDAR分割是实现全面点理解的关键技术，确保时间连续性对于可靠的环境感知和物体跟踪至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有三种主要方法（IoU关联、查询传播关联、检测与跟踪关联）的局限性，发现它们存在计算开销大、误差累积和依赖训练等问题。作者思考能否仅利用3D全景网络实现高效实例关联，最终借鉴了ICP点云配准算法和Sinkhorn最优传输理论，设计了无需训练的ICP-4D框架。该方法将ICP首次应用于4D LiDAR全景分割，同时引入了软匹配机制处理噪声预测问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过实例级点集之间的几何关系统一空间和时间推理，利用ICP算法直接对齐连续扫描中的实例点集，并结合基于Sinkhorn的软匹配增强鲁棒性。整体流程包括：1)使用预训练3D分割模型获取点云的实例和语义预测；2)将实例分为静态、动态和缺失三类处理；3)静态实例基于中心点和协方差进行快速匹配；4)动态实例使用ICP和Sinkhorn软匹配进行精确对齐；5)缺失实例通过内存银行进行恢复；6)最后通过匈牙利算法优化匹配结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个无需训练的4D LiDAR全景分割框架；2)基于Sinkhorn的软匹配策略增强点对应的鲁棒性；3)针对静态、动态和缺失实例的三种状态条件解决方案；4)内存银行机制处理遮挡问题。相比之前的工作，ICP-4D不依赖大规模点云叠加或专门训练模块，直接利用点云几何先验信息，在单扫描设置下实现了更优性能，同时显著降低了计算复杂度（内存需求减少60.7%-79.4%，运行时间提高26.1%-45.6%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ICP-4D通过结合迭代最近点算法和基于Sinkhorn的软匹配策略，实现了无需训练的高效4D LiDAR全景分割，显著提升了时间连续性并平衡了计算效率与性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.</description>
      <author>example@mail.com (Gyeongrok Oh, Youngdong Jang, Jonghyun Choi, Suk-Ju Kang, Guang Lin, Sangpil Kim)</author>
      <guid isPermaLink="false">2512.18991v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>E-RGB-D: Real-Time Event-Based Perception with Structured Light</title>
      <link>http://arxiv.org/abs/2512.18429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合数字光处理投影仪和事件相机的新型RGB-D传感方法，实现了高速彩色深度感知，解决了传统单色事件相机无法检测静态或缓慢移动物体以及缺乏颜色信息的问题。&lt;h4&gt;背景&lt;/h4&gt;事件相机作为受生物启发的传感器，能异步报告像素亮度变化，具有高动态范围、高时间分辨率、低功耗和计算简单等优点。然而，传统单色事件相机在检测静态或缓慢移动物体方面存在局限性，且缺乏某些应用所需的颜色信息。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统单色事件相机的局限性，作者提出集成数字光处理投影仪，形成主动结构光用于RGB-D传感，以实现颜色和深度的同时检测。&lt;h4&gt;方法&lt;/h4&gt;作者结合事件相机和基于投影技术的优势，通过动态投影调整优化带宽，确保选择性颜色数据获取，产生不牺牲空间分辨率的彩色点云。这种集成通过商业TI LightCrafter 4500投影仪和单目单色事件相机实现。&lt;h4&gt;主要发现&lt;/h4&gt;通过该方法，实现了相当于1400 fps的颜色检测速度和4 kHz的像素深度检测速度，显著推进了从机器人技术到3D重建方法等不同领域的计算机视觉发展。&lt;h4&gt;结论&lt;/h4&gt;这种集成不仅实现了无帧RGB-D传感应用，还取得了显著的性能里程碑，为计算机视觉领域带来了重大进步。&lt;h4&gt;翻译&lt;/h4&gt;事件相机作为一种受生物启发的传感器，能够异步报告像素亮度变化，在视觉感知方面提供了无与伦比的速度和效率。尽管它们具有高动态范围、高时间分辨率、低功耗和计算简单等优点，传统的单色事件相机在检测静态或缓慢移动物体方面存在局限性，并且缺乏某些应用必需的颜色信息。为了应对这些挑战，我们提出了一种新颖的方法，集成数字光处理投影仪，形成主动结构光用于RGB-D传感。通过结合事件相机和基于投影技术的优势，我们的方法能够分别检测每个像素的颜色和深度。动态投影调整优化带宽，确保选择性获取颜色数据，产生不牺牲空间分辨率的彩色点云。这种集成通过商业TI LightCrafter 4500投影仪和单目单色事件相机实现，不仅实现了无帧RGB-D传感应用，还取得了显著的性能里程碑。通过我们的方法，我们实现了相当于1400 fps的颜色检测速度和4 kHz的像素深度检测速度，显著推进了从机器人技术到3D重建方法等不同领域的计算机视觉领域。我们的代码已公开：https://github.com/MISTLab/event_based_rgbd_ros&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event-based cameras (ECs) have emerged as bio-inspired sensors that report pixel brightness changes asynchronously, offering unmatched speed and efficiency in vision sensing. Despite their high dynamic range, temporal resolution, low power consumption, and computational simplicity, traditional monochrome ECs face limitations in detecting static or slowly moving objects and lack color information essential for certain applications. To address these challenges, we present a novel approach that integrates a Digital Light Processing (DLP) projector, forming Active Structured Light (ASL) for RGB-D sensing. By combining the benefits of ECs and projection-based techniques, our method enables the detection of color and the depth of each pixel separately. Dynamic projection adjustments optimize bandwidth, ensuring selective color data acquisition and yielding colorful point clouds without sacrificing spatial resolution. This integration, facilitated by a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, not only enables frameless RGB-D sensing applications but also achieves remarkable performance milestones. With our approach, we achieved a color detection speed equivalent to 1400 fps and 4 kHz of pixel depth detection, significantly advancing the realm of computer vision across diverse fields from robotics to 3D reconstruction methods. Our code is publicly available: https://github.com/MISTLab/event_based_rgbd_ros</description>
      <author>example@mail.com (Seyed Ehsan Marjani Bajestani, Giovanni Beltrame)</author>
      <guid isPermaLink="false">2512.18429v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</title>
      <link>http://arxiv.org/abs/2512.17817v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Chorus，一个多教师预训练框架，用于学习3D高斯溅射场景编码器，通过从2D基础模型中提炼互补信号，实现了从高保真场景表示中提取丰富通用特征的能力。&lt;h4&gt;背景&lt;/h4&gt;3D高斯溅射(3DGS)已成为一种高保真场景表示方法，但直接从其基本元素编码丰富、通用特征的研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;引入Chorus框架，学习一个整体的3D高斯溅射场景编码器，通过从2D基础模型中提炼互补信号来填补这一研究空白。&lt;h4&gt;方法&lt;/h4&gt;Chorus采用共享的3D编码器和教师特定的投影器，从语言对齐、通用和对象感知的教师中学习，鼓励捕获从高级语义到细粒度结构的共享嵌入空间。&lt;h4&gt;主要发现&lt;/h4&gt;Chorus在多种任务上表现优异，包括开放词汇语义和实例分割、线性和解码器探测以及数据高效监督；在仅支持点云的基准测试上，使用39.9倍更少的训练场景就优于点云基线；提出的渲染和提炼适应方法促进了域外微调。&lt;h4&gt;结论&lt;/h4&gt;Chorus是一个有效的多教师预训练框架，能够从3DGS中提取丰富的特征，在各种任务上表现出色，并且在点云处理上也显示出显著优势，代码和模型将在发表后发布。&lt;h4&gt;翻译&lt;/h4&gt;虽然3DGS已成为一种高保真场景表示方法，但直接从其基本元素编码丰富、通用特征的研究仍然不足。我们通过引入Chorus解决了这一差距，这是一个多教师预训练框架，通过从2D基础模型中提炼互补信号来学习一个整体的3D高斯溅射场景编码器。Chorus采用共享的3D编码器和教师特定的投影器，从语言对齐、通用和对象感知的教师中学习，鼓励捕获从高级语义到细粒度结构的共享嵌入空间。我们在广泛的任务上评估Chorus：开放词汇语义和实例分割、线性和解码器探测，以及数据高效监督。除了3DGS外，我们还通过仅使用高斯中心、颜色和估计法线作为输入预训练一个变体，在几个仅支持点云的基准测试上测试了Chorus。有趣的是，该编码器显示出强大的迁移能力，使用39.9倍更少的训练场景就优于点云基线。最后，我们提出了一种渲染和提炼的适应方法，促进域外微调。我们的代码和模型将在发表后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从3D高斯溅射(3DGS)表示中直接提取丰富、通用的特征问题。这很重要，因为虽然3DGS能提供高保真场景表示，但直接从中提取可转移的通用特征仍处于探索阶段，而这类通用特征对于实现多样化的3D场景理解任务(如语义分割、实例分割、视觉问答等)至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了'lift-then-align'范式(如SceneSplat工作)，将2D特征提升到3D空间。同时融合了自监督学习和知识蒸馏的思想，特别是多教师知识蒸馏在2D领域的成功应用。作者注意到现有方法(如SceneSplat)主要关注语义信息，忽略了更广泛的场景理解能力，因此设计了多教师框架，整合语言对齐(SigLIP)、通用视觉特征(DINO)和对象感知(PE-Spatial)三种互补信号，以捕捉从高级语义到精细空间结构的多种信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多教师预训练框架，学习一个通用的前馈3DGS场景编码器，将互补的2D基础模型信号整合到共享的3D嵌入空间。整体流程包括：1)使用3DGS场景数据并从多角度渲染；2)将2D教师特征提升到3D高斯空间并标准化；3)训练共享3D编码器和每个教师特定的轻量投影头，应用匹配损失和对比损失；4)采用分阶段预训练策略；5)设计渲染和蒸馏适应方法实现新域适应；6)应用3DGS感知的增强技术提高鲁棒性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将多教师知识蒸馏应用于3DGS场景编码；2)设计统一的3D场景编码器，实现高度结构化和可转移的特征表示；3)提出渲染和蒸馏适应策略，简化新域适应流程；4)开发3DGS感知的数据增强方法。相比之前工作，Chorus超越了单教师方法(如SceneSplat)的语义局限性，实现了更广泛的场景理解能力；相比点云预训练方法，Chorus使用更少训练数据(8.32-39.9倍)实现了更强性能；相比传统适应方法，Chorus避免了昂贵的3D伪标签预处理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Chorus通过多教师预训练框架，首次实现了从3D高斯溅射中直接提取通用、可转移的3D场景特征，在各种3D场景理解任务上达到最先进性能，同时展示卓越的数据效率和适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.</description>
      <author>example@mail.com (Yue Li, Qi Ma, Runyi Yang, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Theo Gevers, Luc Van Gool, Danda Pani Paudel, Martin R. Oswald)</author>
      <guid isPermaLink="false">2512.17817v2</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</title>
      <link>http://arxiv.org/abs/2512.14696v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CRISP是一种从单目视频中恢复可模拟人体运动和场景几何的方法，通过平面基元拟合和人体-场景接触建模，显著提高了运动跟踪的成功率并加速了模拟过程。&lt;h4&gt;背景&lt;/h4&gt;先前的人体-场景联合重建工作要么依赖数据驱动的先验和无物理循环的联合优化，要么恢复带有噪声的几何形状，导致涉及场景交互的运动跟踪策略失败。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从单目视频中恢复可模拟人体运动和场景几何的方法，解决现有方法中的噪声和物理一致性问题。&lt;h4&gt;方法&lt;/h4&gt;通过基于深度、法向量和流的简单聚类流程，将平面基元拟合到场景的点云重建中；利用人体-场景接触建模重建可能被交互遮挡的场景几何；通过强化学习使用重建结果驱动人形控制器以确保物理合理性。&lt;h4&gt;主要发现&lt;/h4&gt;在以人为中心的视频基准测试上，将运动跟踪失败率从55.2%降低到6.9%；RL模拟吞吐量提高43%；在多种野外视频上验证了方法的有效性，包括 casually-captured视频、互联网视频和Sora生成的视频。&lt;h4&gt;结论&lt;/h4&gt;CRISP能够大规模生成物理有效的人体运动和交互环境，大大推进了机器人和AR/VR领域的现实到模拟应用。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了CRISP，一种从单目视频中恢复可模拟人体运动和场景几何的方法。先前关于人体-场景联合重建的工作依赖于数据驱动的先验和无物理循环的联合优化，或者恢复带有噪声的几何形状，导致涉及场景交互的运动跟踪策略失败。相比之下，我们的关键洞见是通过将平面基元拟合到场景的点云重建中，恢复凸起、干净且可模拟的几何，这通过基于深度、法向量和流的简单聚类流程实现。为了重建交互过程中可能被遮挡的场景几何，我们利用人体-场景接触建模（例如，我们使用人体姿势重建被遮挡的椅子座位）。最后，我们通过强化学习使用重建的人体和场景驱动人形控制器，确保人体和场景重建在物理上是合理的。我们的方法在以人为中心的视频基准测试(EMDB, PROX)上，将运动跟踪失败率从55.2%降低到6.9%，同时使RL模拟吞吐量提高43%。我们在各种野外视频上进一步验证了该方法，包括 casually-captured视频、互联网视频，甚至是Sora生成的视频。这证明了CRISP能够大规模生成物理有效的人体运动和交互环境，大大推进了机器人和AR/VR领域的现实到模拟应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从单目视频中恢复可模拟的3D人体运动和场景几何的问题，特别是在人体与场景交互的情况下。这个问题很重要，因为真正理解视频中的行为需要物理层面的理解，人体与环境的交互是日常生活中的常见场景（如坐在椅子上、爬楼梯等），而现有的重建方法在处理视差和遮挡时表现不佳，重建结果可能包含噪声和伪影，导致物理模拟失败。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法依赖数据驱动的先验和联合优化但没有物理循环，重建的几何结构有噪声导致交互失败。他们的设计思路是通过平面基元拟合点云来获得可模拟的几何，利用人体-场景接触建模重建被遮挡的场景，最后用强化学习确保物理合理性。他们借鉴了MegaSAM进行相机姿态估计，MoGe改进几何质量，GVHMR估计人体姿态，InteractVLM预测接触，MaskedMimic设计观察行动模型，以及使用Isaac Gym和PPO进行模拟和训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将场景分解为一小组（约50个）凸平面基元来解决噪声问题，利用人体-场景接触建模重建被遮挡的场景，并用强化学习确保物理合理性。整体流程：1)初始化：使用MegaSAM和MoGe推断相机参数和场景点云，GVHMR估计人体姿态；2)平面基元拟合：通过聚类算法从点云中提取平面基元；3)接触引导场景完成：使用InteractVLM预测接触并完成被遮挡场景；4)物理运动跟踪：用强化学习训练控制策略在模拟环境中跟踪重建的人体运动。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)平面基元表示：将场景分解为少量凸平面基元生成轻量级可模拟重建；2)接触引导场景完成：利用人体-场景接触建模重建被遮挡几何；3)物理验证：用强化学习确保重建的物理合理性。相比之前工作的不同：与VideoMimic相比，CRISP提供更准确的人体、场景和接触建模，RL成功率从44.8%提高到93.1%，模拟吞吐量从16K提高到23K FPS；与其他几何重建方法相比，避免了过平滑和重复结构；与其他人体运动恢复方法相比，在EMDB数据集上实现了更低的姿态误差。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CRISP提出了一种从单目视频中重建可模拟3D人体运动和场景几何的创新方法，通过平面基元表示、接触引导的场景完成和物理验证，显著提高了人体-场景交互的重建质量和模拟稳定性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.</description>
      <author>example@mail.com (Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins, Shubham Tulsiani, Deva Ramanan)</author>
      <guid isPermaLink="false">2512.14696v2</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection</title>
      <link>http://arxiv.org/abs/2512.18187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ALIGN方法，一种创新的3D目标检测查询初始化策略，通过三个关键组件解决了遮挡和拥挤物体检测的问题，并在nuScenes基准上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;基于查询的3D目标检测方法使用相机和LiDAR输入已显示出强大性能，但现有的查询初始化策略（如随机采样或基于BEV热图的采样）往往导致查询使用效率低下和准确性降低，特别是在被遮挡或拥挤物体的检测中。&lt;h4&gt;目的&lt;/h4&gt;解决现有查询初始化策略的局限性，提出一种对遮挡具有鲁棒性且具有物体感知能力的查询初始化方法。&lt;h4&gt;方法&lt;/h4&gt;提出了ALIGN（Advanced query initialization with LiDAR and Image GuidaNce）方法，包含三个关键组件：(i)遮挡感知中心估计(OCE)：集成LiDAR几何和图像语义准确估计物体中心；(ii)自适应邻域采样(ANS)：从LiDAR聚类生成物体候选，并通过在其周围采样空间和语义对齐的点来补充每个物体；(iii)动态查询平衡(DQB)：自适应地平衡前景和背景区域之间的查询。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes基准上的广泛实验表明，ALIGN持续提高了多个最先进检测器的性能，最多可实现+0.9 mAP和+1.2 NDS的增益，特别是在具有遮挡或密集人群的挑战性场景中表现更好。&lt;h4&gt;结论&lt;/h4&gt;ALIGN方法能有效解决现有查询初始化策略的问题，代码将在发表后公开。&lt;h4&gt;翻译&lt;/h4&gt;最近的基于查询的3D目标检测方法使用相机和LiDAR输入已显示出强大的性能，但现有的查询初始化策略，如随机采样或基于BEV热图的采样，往往导致查询使用效率低下和准确性降低，特别是对于被遮挡或拥挤的物体。为了解决这一限制，我们提出了ALIGN（使用LiDAR和图像引导的高级查询初始化），一种新颖的对遮挡具有鲁棒性、具有物体感知能力的查询初始化方法。我们的模型包含三个关键组件：(i)遮挡感知中心估计(OCE)，它集成LiDAR几何和图像语义来准确估计物体中心；(ii)自适应邻域采样(ANS)，它从LiDAR聚类生成物体候选，并通过在其周围采样空间和语义对齐的点来补充每个物体；(iii)动态查询平衡(DQB)，它自适应地平衡前景和背景区域之间的查询。我们在nuScenes基准上的广泛实验表明，ALIGN持续提高了多个最先进检测器的性能，最多实现+0.9 mAP和+1.2 NDS的增益，特别是在具有遮挡或密集人群的挑战性场景中。我们的代码将在发表后公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于查询的3D目标检测中查询初始化策略的不足问题。现有方法（随机采样或BEV热图采样）在处理被遮挡、拥挤或小型物体时表现不佳。这个问题在现实中非常重要，因为3D目标检测是自动驾驶和机器人的核心能力，准确识别被遮挡物体对于确保安全导航至关重要，而这些场景在复杂的城市环境中非常常见。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有查询初始化方法的局限性，指出它们要么完全随机分布查询不考虑物体相关性，要么仅依赖热图信息而未充分利用激光雷达几何和图像语义信息。作者设计了一种物体感知的查询初始化策略，从估计的物体中心附近采样查询。该方法借鉴了DETR等查询检测框架，结合了图像分割和激光雷达点云投影技术（如OCE模块中的单应性估计），以及Deformable DETR的关键点采样思想（如ANS模块中的邻域采样），同时创新地引入了类特定的深度补偿和动态查询平衡机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合激光雷达的几何信息和图像的语义信息，实现一种物体感知的查询初始化策略，特别是在处理被遮挡物体时更加鲁棒。整体流程包含三个主要组件：(1)遮挡感知中心估计(OCE)：将激光雷达点投影到图像，利用分割掩码和单应性变换估计物体中心，并应用类特定的深度补偿；(2)自适应邻域采样(ANS)：通过DBSCAN聚类识别潜在物体，并在每个聚类核心周围采样语义对齐的点；(3)动态查询平衡(DQB)：根据场景复杂度自适应平衡前景和背景查询分配。最终将生成的查询编码并传递给多模态Transformer解码器进行目标检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)提出物体感知查询初始化策略，从估计的物体中心附近采样查询；(2)设计三个关键模块：OCE结合激光雷达几何和图像语义估计物体中心，ANS通过聚类和邻域采样补充被遮挡物体，DQB平衡前景和背景查询；(3)在查询初始化阶段而非特征融合阶段有效利用多模态信息。相比之前工作，不同之处在于：与随机初始化相比，提供了关键区域的密集覆盖和更高的查询效率；与基于热图的初始化相比，不依赖可能漂移或消失的热图峰值，即使在复杂场景中也能提供更可靠的定位；与其他多模态方法相比，在查询初始化阶段就结合了几何和语义线索。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALIGN提出了一种结合激光雷达几何和图像语义的物体感知查询初始化框架，通过三个关键模块显著提高了被遮挡和密集场景下的3D目标检测性能，同时保持了与现有检测器的兼容性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent query-based 3D object detection methods using camera and LiDAR inputs have shown strong performance, but existing query initialization strategies,such as random sampling or BEV heatmap-based sampling, often result in inefficient query usage and reduced accuracy, particularly for occluded or crowded objects. To address this limitation, we propose ALIGN (Advanced query initialization with LiDAR and Image GuidaNce), a novel approach for occlusion-robust, object-aware query initialization. Our model consists of three key components: (i) Occlusion-aware Center Estimation (OCE), which integrates LiDAR geometry and image semantics to estimate object centers accurately (ii) Adaptive Neighbor Sampling (ANS), which generates object candidates from LiDAR clustering and supplements each object by sampling spatially and semantically aligned points around it and (iii) Dynamic Query Balancing (DQB), which adaptively balances queries between foreground and background regions. Our extensive experiments on the nuScenes benchmark demonstrate that ALIGN consistently improves performance across multiple state-of-the-art detectors, achieving gains of up to +0.9 mAP and +1.2 NDS, particularly in challenging scenes with occlusions or dense crowds. Our code will be publicly available upon publication.</description>
      <author>example@mail.com (Janghyun Baek, Mincheol Chang, Seokha Moon, Seung Joon Lee, Jinkyu Kim)</author>
      <guid isPermaLink="false">2512.18187v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</title>
      <link>http://arxiv.org/abs/2512.19150v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为AMap的新型在线高清地图构建框架，解决了现有方法'空间向后看'的安全缺陷，通过'从未来蒸馏'范式使模型具备前瞻性感知能力，在不增加推理时间成本的情况下显著提升了前方区域的感知准确性。&lt;h4&gt;背景&lt;/h4&gt;在线高清地图构建对自动驾驶至关重要。现有方法主要利用历史时间融合来提高性能，但这些方法存在一个关键的安全缺陷：它们本质上是'空间向后看'的，主要增强已通过区域的地图重建，对前方未见的道路改进有限。分析显示，向后感知错误通常可以容忍，但前方区域的不准确性会直接导致危险的驾驶操作。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决现有在线高清地图方法的安全缺陷，特别关注前方区域的感知准确性，以提高自动驾驶系统的安全性。&lt;h4&gt;方法&lt;/h4&gt;研究提出了AMap框架，采用'从未来蒸馏'范式，让拥有未来上下文访问权限的教师模型指导仅限于当前帧的轻量级学生模型。技术上引入了多级BEV蒸馏策略（具有空间掩码）和不对称查询适应模块，有效将未来感知表示转移到学生的静态查询中。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Argoverse 2基准测试上的大量实验表明，AMap显著增强了当前帧的感知能力。值得注意的是，它在关键的前方区域超越了最先进的时序模型，同时保持了单当前帧推理的效率。&lt;h4&gt;结论&lt;/h4&gt;AMap通过前瞻性感知能力的引入，有效弥补了现有在线高清地图方法的安全差距，在不增加计算成本的情况下提高了前方区域的感知准确性，为自动驾驶提供了更安全的地图构建解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在线高清地图构建对自动驾驶至关重要。虽然最近的方法利用历史时间融合来提高性能，但我们确定这一范式中的一个关键安全缺陷：它本质上'空间向后看'。这些方法主要增强已通过区域的地图重建，对前方未见的道路改进有限。重要的是，我们对下游规划任务的分析揭示了严重的非对称性：虽然向后感知错误通常可以容忍，但前方区域的不准确性会直接导致危险的驾驶操作。为了弥补这一安全差距，我们提出了AMap，一种面向前方的在线高清地图构建的新型框架。我们开创了'从未来蒸馏'范式，其中拥有未来上下文访问权限的教师模型指导受限于当前帧的轻量级学生模型。这个过程将前瞻性知识隐式压缩到学生模型中，使其以零推理时间成本获得'前瞻'能力。技术上，我们引入了具有空间掩码的多级BEV蒸馏策略和不对称查询适应模块，以有效将未来感知表示转移到学生的静态查询中。在nuScenes和Argoverse 2基准测试上的大量实验表明，AMap显著增强了当前帧的感知能力。最值得注意的是，它在关键的前方区域超越了最先进的时序模型，同时保持了单当前帧推理的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online High-Definition (HD) map construction is pivotal for autonomous driving. While recent approaches leverage historical temporal fusion to improve performance, we identify a critical safety flaw in this paradigm: it is inherently ``spatially backward-looking." These methods predominantly enhance map reconstruction in traversed areas, offering minimal improvement for the unseen road ahead. Crucially, our analysis of downstream planning tasks reveals a severe asymmetry: while rearward perception errors are often tolerable, inaccuracies in the forward region directly precipitate hazardous driving maneuvers. To bridge this safety gap, we propose AMap, a novel framework for Ahead-aware online HD Mapping. We pioneer a ``distill-from-future" paradigm, where a teacher model with privileged access to future temporal contexts guides a lightweight student model restricted to the current frame. This process implicitly compresses prospective knowledge into the student model, endowing it with ``look-ahead" capabilities at zero inference-time cost. Technically, we introduce a Multi-Level BEV Distillation strategy with spatial masking and an Asymmetric Query Adaptation module to effectively transfer future-aware representations to the student's static queries. Extensive experiments on the nuScenes and Argoverse 2 benchmark demonstrate that AMap significantly enhances current-frame perception. Most notably, it outperforms state-of-the-art temporal models in critical forward regions while maintaining the efficiency of single current frame inference.</description>
      <author>example@mail.com (Ruikai Li, Xinrun Li, Mengwei Xie, Hao Shan, Shoumeng Qiu, Xinyuan Chang, Yizhe Fan, Feng Xiong, Han Jiang, Yilong Ren, Haiyang Yu, Mu Xu, Yang Long, Varun Ojha, Zhiyong Cui)</author>
      <guid isPermaLink="false">2512.19150v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>D$^{2}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection</title>
      <link>http://arxiv.org/abs/2512.19130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为D²Stream的去耦双流框架，用于视频中的活跃说话人检测，分离了跨帧时间建模和帧内说话人判别，在保持高性能的同时显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;音频-视觉说话人检测旨在通过利用互补的音频和视觉线索来识别视频中的活跃说话人。现有方法通常由于时间建模和说话人交互的联合建模而导致计算效率低下或性能不佳。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在计算效率和性能上的不足，提出一种能够有效分离时间建模和说话人判别的框架，以提高说话人检测的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出D²Stream去耦双流框架，通过跨模态注意力对齐音频和视觉特征，输入两个轻量级流：时间交互流捕获长程时间依赖，说话人交互流建模每帧间的人际关系。两个流提取的特征通过交叉注意力相互作用，并引入轻量级语音门模块减轻非言语面部运动的假阳性。&lt;h4&gt;主要发现&lt;/h4&gt;在AVA-ActiveSpeaker上达到95.6% mAP的最新技术水平，相比基于GNN的模型计算量减少80%，参数比基于注意力的替代方案减少30%，同时在Columbia ASD数据集上表现出良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;D²Stream框架通过分离时间建模和说话人判别，有效解决了现有方法的计算效率和性能问题，实现了更好的性能和效率平衡，具有良好的泛化能力和实用价值。&lt;h4&gt;翻译&lt;/h4&gt;音频-视觉说话人检测旨在通过利用互补的音频和视觉线索来识别视频中的活跃说话人。现有方法通常由于时间建模和说话人交互的联合建模而导致计算效率低下或性能不佳。我们提出了D²Stream，一种去耦双流框架，将跨帧时间建模与帧内说话人判别分离。音频和视觉特征首先通过跨模态注意力进行对齐，然后输入两个轻量级流：时间交互流捕获长程时间依赖，而说话人交互流建模每帧间的人际关系。两个流提取的时间和关系特征通过交叉注意力相互作用以丰富表示。轻量级语音门模块进一步减轻了来自非言语面部运动的假阳性。在AVA-ActiveSpeaker上，D²Stream以95.6% mAP取得了最新的技术水平，计算量比基于GNN的模型减少80%，参数比基于注意力的替代方案减少30%，同时在Columbia ASD上也具有良好的泛化能力。源代码可在https://anonymous.4open.science/r/D2STREAM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-visual speaker detection aims to identify the active speaker in videos by leveraging complementary audio and visual cues. Existing methods often suffer from computational inefficiency or suboptimal performance due to joint modeling of temporal and speaker interactions. We propose D$^{2}$Stream, a decoupled dual-stream framework that separates cross-frame temporal modeling from within-frame speaker discrimination. Audio and visual features are first aligned via cross-modal attention, then fed into two lightweight streams: a Temporal Interaction Stream captures long-range temporal dependencies, while a Speaker Interaction Stream models per-frame inter-person relationships. The temporal and relational features extracted by the two streams interact via cross-attention to enrich representations. A lightweight Voice Gate module further mitigates false positives from non-speech facial movements. On AVA-ActiveSpeaker, D$^{2}$Stream achieves a new state-of-the-art at 95.6% mAP, with 80% reduction in computation compared to GNN-based models and 30% fewer parameters than attention-based alternatives, while also generalizing well on Columbia ASD. Source code is available at https://anonymous.4open.science/r/D2STREAM.</description>
      <author>example@mail.com (Junhao Xiao, Shun Feng, Zhiyu Wu, Jianjun Li, Zhiyuan Ma, Yi Chen)</author>
      <guid isPermaLink="false">2512.19130v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition</title>
      <link>http://arxiv.org/abs/2512.19036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 7 figures. Preprint under review for journal submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出CLIP-SPM框架，解决少样本动作识别中的三个核心挑战：时间建模、视觉相似性和模态差距，包含HSMR模块、SPM策略和PADM方法三个组件。&lt;h4&gt;背景&lt;/h4&gt;少样本动作识别旨在使模型从有限标记样本中快速学习新动作类别，解决现实应用中数据稀缺问题。当前研究面临三大挑战：时间建模易受静态背景干扰、视觉相似动作难以区分、视觉-文本模态差距导致对齐困难。&lt;h4&gt;目的&lt;/h4&gt;解决少样本动作识别中的时间建模、视觉相似性和模态差距三大核心挑战。&lt;h4&gt;方法&lt;/h4&gt;提出CLIP-SPM框架，包含三个组件：1)层次协同运动精炼(HSMR)模块，对齐深度和浅层运动特征减少背景干扰；2)语义原型调制(SPM)策略，生成查询相关文本提示弥合模态差距；3)原型锚点双调制(PADM)方法，精炼支持原型并提升查询特征一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在Kinetics、SSv2-Full、SSv2-Small、UCF101和HMDB51等基准测试上，CLIP-SPM在1-shot、3-shot和5-shot设置下取得竞争性性能，消融研究验证了各组件有效性。&lt;h4&gt;结论&lt;/h4&gt;CLIP-SPM框架通过三个关键组件有效解决少样本动作识别核心挑战，源代码和模型已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;少样本动作识别旨在使模型能够从有限的标记样本中快速学习新的动作类别，解决现实应用中数据稀缺的挑战。当前研究主要解决三个核心挑战：(1)时间建模，模型容易受到无关静态背景信息的干扰，难以捕捉动态动作特征的精髓；(2)视觉相似性，具有细微视觉差异的类别难以区分；(3)视觉-文本支持原型与仅视觉查询之间的模态差距，这使得在共享嵌入空间中对齐变得复杂。为解决这些挑战，本文提出CLIP-SPM框架，包含三个组件：(1)层次协同运动精炼(HSMR)模块，对齐深度和浅层运动特征，通过减少静态背景干扰来改善时间建模；(2)语义原型调制(SPM)策略，生成查询相关的文本提示，弥合模态差距，并将其与视觉特征集成，增强相似动作之间的区分性；(3)原型锚点双调制(PADM)方法，精炼支持原型并将查询特征与全局语义锚点对齐，提高支持样本和查询样本之间的一致性。在Kinetics、SSv2-Full、SSv2-Small、UCF101和HMDB51等标准基准上进行的综合实验表明，我们的CLIP-SPM在1-shot、3-shot和5-shot设置下取得了具有竞争力的性能。大量的消融研究和可视化分析进一步验证了每个组件的有效性及其对解决核心挑战的贡献。源代码和模型已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot action recognition aims to enable models to quickly learn new action categories from limited labeled samples, addressing the challenge of data scarcity in real-world applications. Current research primarily addresses three core challenges: (1) temporal modeling, where models are prone to interference from irrelevant static background information and struggle to capture the essence of dynamic action features; (2) visual similarity, where categories with subtle visual differences are difficult to distinguish; and (3) the modality gap between visual-textual support prototypes and visual-only queries, which complicates alignment within a shared embedding space. To address these challenges, this paper proposes a CLIP-SPM framework, which includes three components: (1) the Hierarchical Synergistic Motion Refinement (HSMR) module, which aligns deep and shallow motion features to improve temporal modeling by reducing static background interference; (2) the Semantic Prototype Modulation (SPM) strategy, which generates query-relevant text prompts to bridge the modality gap and integrates them with visual features, enhancing the discriminability between similar actions; and (3) the Prototype-Anchor Dual Modulation (PADM) method, which refines support prototypes and aligns query features with a global semantic anchor, improving consistency across support and query samples. Comprehensive experiments across standard benchmarks, including Kinetics, SSv2-Full, SSv2-Small, UCF101, and HMDB51, demonstrate that our CLIP-SPM achieves competitive performance under 1-shot, 3-shot, and 5-shot settings. Extensive ablation studies and visual analyses further validate the effectiveness of each component and its contributions to addressing the core challenges. The source code and models are publicly available at GitHub.</description>
      <author>example@mail.com (Xiaoyang Li, Mingming Lu, Ruiqi Wang, Hao Li, Zewei Le)</author>
      <guid isPermaLink="false">2512.19036v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</title>
      <link>http://arxiv.org/abs/2512.18878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了CrashChat，一个基于VideoLLaMA3构建的多模态大语言模型，用于自动化交通事故视频分析。CrashChat通过指令微调获取领域知识，并采用基于任务解耦和分组的新型多任务学习策略，在事故识别、定位和描述推理任务上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;自动化交通事故视频分析对于交通安全研究和自动驾驶责任归因至关重要。由于视频数据中事故事件的复杂时空动态和多样化分析需求，交通事故视频分析是一个具有挑战性的多任务问题，需要涵盖事故识别、时间定位和高级视频理解能力。&lt;h4&gt;目的&lt;/h4&gt;填补现有模型无法在统一框架内执行所有这些任务的空白，并探索此类模型的有效训练策略。&lt;h4&gt;方法&lt;/h4&gt;提出了CrashChat，一个基于VideoLLaMA3构建的多模态大语言模型(MLLM)，通过指令微调获取领域特定知识，并采用基于任务解耦和分组的新型多任务学习策略，最大化组内和跨组联合学习的收益，同时减轻负迁移。&lt;h4&gt;主要发现&lt;/h4&gt;在合并的公共数据集上，CrashChat在各种模型规模和传统基于视觉的方法上都胜过现有的MLLM，达到最先进性能。事故识别达到接近完美准确性，事故定位能力提高176%，事故前定位提高40%。与通用MLLM相比，在事故描述和推理任务中，BLEU分数提高0.18-0.41，ROUGE分数提高0.18-0.42。&lt;h4&gt;结论&lt;/h4&gt;CrashChat是一个方便的端到端分析工具，已准备好实际实施。相关数据集和实现代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;自动化交通事故视频分析对于利用不断增长的驾驶视频数据进行交通安全研究和自动驾驶责任归因至关重要。由于视频数据中事故事件的复杂时空动态和多样化的分析需求，交通事故视频分析是一个具有挑战性的多任务问题。它需要涵盖事故识别、时间定位和高级视频理解的能力。然而，现有模型无法在统一框架内执行所有这些任务，此类模型的有效训练策略仍有待探索。为填补这些空白，本文提出了CrashChat，一个基于VideoLLaMA3构建的多模态大语言模型(MLLM)，用于多任务交通事故分析。CrashChat通过指令微调获取领域特定知识，并采用一种基于任务解耦和分组的新型多任务学习策略，最大化组内和跨组联合学习的收益，同时减轻负迁移。在合并的公共数据集上的数值实验表明，CrashChat在各种模型规模和传统基于视觉的方法上都胜过现有的MLLM，达到最先进性能。在事故识别方面达到接近完美的准确性，事故定位能力提高176%，在更具挑战性的事故前定位方面提高40%。与通用MLLM相比，它在事故描述和推理任务中显著提高了文本准确性和内容覆盖率，BLEU分数提高0.18-0.41，ROUGE分数提高0.18-0.42。除了强大的性能外，CrashChat是一个方便的端到端分析工具，已准备好实际实施。CrashChat的数据集和实现代码可在https://github.com/Liangkd/CrashChat获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\% improvement in crash localization, and a 40\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.</description>
      <author>example@mail.com (Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin)</author>
      <guid isPermaLink="false">2512.18878v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos</title>
      <link>http://arxiv.org/abs/2512.18750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 4 figures. Preprint under review for journal submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种上下文感知网络(CAN)用于动作识别，通过多尺度时间线索模块和分组空间线索模块分别捕捉多尺度的时空特征，在五个基准数据集上取得了有竞争力的性能。&lt;h4&gt;背景&lt;/h4&gt;动作识别是视频理解中的关键任务，需要全面捕捉各种尺度的时空线索。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法忽略动作多粒度性质的问题，提高动作识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出上下文感知网络(CAN)，包含两个核心模块：多尺度时间线索模块(MTCM)用于提取多尺度时间线索，捕捉快速变化的运动细节和整体动作流程；分组空间线索模块(GSCM)通过分组特征图并对每组应用专门的提取方法来提取不同尺度的空间线索。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准数据集(Something-Something V1和V2、Diving48、Kinetics-400和UCF101)上的实验表明，CAN取得了有竞争力的性能，准确率分别为50.4%、63.9%、88.4%、74.9%和86.9%，优于大多数主流方法。&lt;h4&gt;结论&lt;/h4&gt;捕捉多尺度时空线索对鲁棒的动作识别非常重要，所提出的CAN方法有效解决了现有方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;动作识别是视频理解中的关键任务，需要全面捕捉各种尺度的时空线索。然而，现有方法常常忽略动作的多粒度性质。为解决这一局限，我们引入了上下文感知网络(CAN)。CAN包含两个核心模块：多尺度时间线索模块(MTCM)和分组空间线索模块(GSCM)。MTCM有效提取多尺度时间线索，同时捕捉快速变化的运动细节和整体动作流程。另一方面，GSCM通过分组特征图并对每组应用专门的提取方法来提取不同尺度的空间线索。在五个基准数据集(Something-Something V1和V2、Diving48、Kinetics-400和UCF101)上进行的实验证明了CAN的有效性。我们的方法取得了有竞争力的性能，优于大多数主流方法，在Something-Something V1上的准确率为50.4%，Something-Something V2为63.9%，Diving48为88.4%，Kinetics-400为74.9%，UCF101为86.9%。这些结果强调了捕捉多尺度时空线索对鲁棒动作识别的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Action recognition is a critical task in video understanding, requiring the comprehensive capture of spatio-temporal cues across various scales. However, existing methods often overlook the multi-granularity nature of actions. To address this limitation, we introduce the Context-Aware Network (CAN). CAN consists of two core modules: the Multi-scale Temporal Cue Module (MTCM) and the Group Spatial Cue Module (GSCM). MTCM effectively extracts temporal cues at multiple scales, capturing both fast-changing motion details and overall action flow. GSCM, on the other hand, extracts spatial cues at different scales by grouping feature maps and applying specialized extraction methods to each group. Experiments conducted on five benchmark datasets (Something-Something V1 and V2, Diving48, Kinetics-400, and UCF101) demonstrate the effectiveness of CAN. Our approach achieves competitive performance, outperforming most mainstream methods, with accuracies of 50.4% on Something-Something V1, 63.9% on Something-Something V2, 88.4% on Diving48, 74.9% on Kinetics-400, and 86.9% on UCF101. These results highlight the importance of capturing multi-scale spatio-temporal cues for robust action recognition.</description>
      <author>example@mail.com (Xiaoyang Li, Wenzhu Yang, Kanglin Wang, Tiebiao Wang, Qingsong Fei)</author>
      <guid isPermaLink="false">2512.18750v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse</title>
      <link>http://arxiv.org/abs/2512.18671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI26 accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SmartSight是一种无需训练的视频大语言模型幻觉缓解方法，通过生成多个候选响应并利用时间注意力崩溃分数评估幻觉程度，同时识别视觉注意力消失点提高效率，显著降低了幻觉风险并增强了视频理解能力。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型近年来快速发展，但知觉幻觉构成重大安全风险，严重限制了其现实世界应用。现有幻觉缓解方法往往损害模型的理解和推理能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种不损害视频理解和推理能力的幻觉缓解方法，采用无需训练的方式解决视频大语言模型中的幻觉问题。&lt;h4&gt;方法&lt;/h4&gt;SmartSight通过生成多个候选响应来发现低幻觉输出，使用时间注意力崩溃分数评估幻觉程度，识别视觉注意力消失点以提高效率和估计准确性，实现提前终止幻觉响应并降低解码成本。&lt;h4&gt;主要发现&lt;/h4&gt;SmartSight在VRIPT-HAL数据集上将Qwen2.5-VL-7B的幻觉降低了10.59%，同时在VideoMMMU上将性能提高了高达8.86%，增强了视频理解和推理能力。&lt;h4&gt;结论&lt;/h4&gt;SmartSight在提高开源视频大语言模型的可靠性方面是有效的，解决了幻觉问题而不损害模型性能。&lt;h4&gt;翻译&lt;/h4&gt;尽管视频大语言模型近年来迅速发展，但知觉幻觉构成了重大安全风险，严重限制了它们的现实适用性。虽然已经提出了几种幻觉缓解方法，但它们往往损害了模型对视频的理解和推理能力。在这项工作中，我们提出了SmartSight，这是通过利用模型自身的内省能力以无需训练的方式解决此问题的开创性步骤。具体来说，SmartSight生成多个候选响应以揭示通常被标准贪婪解码掩盖的低幻觉输出。它使用时间注意力崩溃分数评估每个响应的幻觉程度，该分数衡量模型在生成响应时是否过度关注输入视频中琐碎的时间区域。为了提高效率，SmartSight识别视觉注意力消失点，使更准确的幻觉估计和提前终止幻觉响应成为可能，从而显著降低了解码成本。实验表明，SmartSight在VRIPT-HAL上将Qwen2.5-VL-7B的幻觉大幅降低了10.59%，同时增强了视频理解和推理能力，在VideoMMMU上将性能提高了高达8.86%。这些结果突显了SmartSight在提高开源视频大语言模型可靠性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite Video Large Language Models having rapidly advanced in recent years, perceptual hallucinations pose a substantial safety risk, which severely restricts their real-world applicability. While several methods for hallucination mitigation have been proposed, they often compromise the model's capacity for video understanding and reasoning. In this work, we propose SmartSight, a pioneering step to address this issue in a training-free manner by leveraging the model's own introspective capabilities. Specifically, SmartSight generates multiple candidate responses to uncover low-hallucinated outputs that are often obscured by standard greedy decoding. It assesses the hallucination of each response using the Temporal Attention Collapse score, which measures whether the model over-focuses on trivial temporal regions of the input video when generating the response. To improve efficiency, SmartSight identifies the Visual Attention Vanishing point, enabling more accurate hallucination estimation and early termination of hallucinated responses, leading to a substantial reduction in decoding cost. Experiments show that SmartSight substantially lowers hallucinations for Qwen2.5-VL-7B by 10.59% on VRIPT-HAL, while simultaneously enhancing video understanding and reasoning, boosting performance on VideoMMMU by up to 8.86%. These results highlight SmartSight's effectiveness in improving the reliability of open-source Video-LLMs.</description>
      <author>example@mail.com (Yiming Sun, Mi Zhang, Feifei Li, Geng Hong, Min Yang)</author>
      <guid isPermaLink="false">2512.18671v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations</title>
      <link>http://arxiv.org/abs/2512.18483v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, IEEE Transactions on Artificial Intelligence (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合显式和隐式图表示与时间建模的事后内部威胁检测框架，通过图卷积网络和双向长短期记忆网络捕获复杂用户行为模式，实验证明该方法在CERT数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;内部威胁检测具有挑战性，因为恶意活动通常由可信用户执行，且行为微妙且隐蔽，难以检测。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获复杂用户行为模式的有效内部威胁检测框架，通过结合图表示和时间建模提高检测准确性。&lt;h4&gt;方法&lt;/h4&gt;构建显式图建模直接关系，使用Gumbel-Softmax技巧学习隐式图发现潜在关系，分别通过图卷积网络处理两种图生成节点嵌入，通过注意力机制强调威胁特征，最后使用双向长短期记忆网络捕获时间依赖性，低于阈值的活动被标记为异常。&lt;h4&gt;主要发现&lt;/h4&gt;在CERT r5.2数据集上达到AUC为98.62，检测率100%，误报率0.05；在更具挑战性的r6.2数据集上获得AUC为88.48，检测率80.15%，误报率0.15，显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;结合基于图和时间表示的方法对于强大的内部威胁检测是有效的，能够准确识别可疑活动同时保持低误报率。&lt;h4&gt;翻译&lt;/h4&gt;内部威胁检测具有挑战性，因为由可信用户执行的恶意活动具有微妙和隐蔽的特性。本文提出了一种事后内部威胁检测框架，结合显式和隐式图表示与时间建模，以捕获复杂的用户行为模式。使用预定义的组织规则构建显式图，建模用户活动之间的直接关系。为了减轻这种手工制作结构中的噪声和限制，使用Gumbel-Softmax技巧从特征相似性中学习隐式图， enabling发现潜在的行为关系。单独的图卷积网络处理显式和隐式图以生成节点嵌入，这些嵌入通过注意力机制连接和细化，以强调威胁相关特征。然后将细化的表示传递到双向长短期记忆网络，以捕获用户行为中的时间依赖性。当活动概率分数低于预定义阈值时，它们被标记为异常。在CERT r5.2和r6.2数据集上的广泛实验表明，所提出的框架优于最先进的方法。在r5.2上，模型达到AUC为98.62，检测率为100%，误报率为0.05。在更具挑战性的r6.2数据集上，它获得AUC为88.48，检测率为80.15%，误报率为0.15，突显了结合基于图和时间表示对于强大的内部威胁检测的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TAI.2025.3647418&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Insider threat detection (ITD) is challenging due to the subtle and concealed nature of malicious activities performed by trusted users. This paper proposes a post-hoc ITD framework that integrates explicit and implicit graph representations with temporal modelling to capture complex user behaviour patterns. An explicit graph is constructed using predefined organisational rules to model direct relationships among user activities. To mitigate noise and limitations in this hand-crafted structure, an implicit graph is learned from feature similarities using the Gumbel-Softmax trick, enabling the discovery of latent behavioural relationships. Separate Graph Convolutional Networks (GCNs) process the explicit and implicit graphs to generate node embeddings, which are concatenated and refined through an attention mechanism to emphasise threat-relevant features. The refined representations are then passed to a bidirectional Long Short-Term Memory (Bi-LSTM) network to capture temporal dependencies in user behaviour. Activities are flagged as anomalous when their probability scores fall below a predefined threshold. Extensive experiments on CERT r5.2 and r6.2 datasets demonstrate that the proposed framework outperforms state-of-the-art methods. On r5.2, the model achieves an AUC of 98.62, a detection rate of 100%, and a false positive rate of 0.05. On the more challenging r6.2 dataset, it attains an AUC of 88.48, a detection rate of 80.15%, and a false positive rate of 0.15, highlighting the effectiveness of combining graph-based and temporal representations for robust ITD.</description>
      <author>example@mail.com (Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang, Deepa Krishnan)</author>
      <guid isPermaLink="false">2512.18483v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation</title>
      <link>http://arxiv.org/abs/2512.18028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了Embodied4C基准测试，用于评估视觉-语言模型在不同具身平台上的核心具身能力，通过三个异构具身平台(自动驾驶汽车、空中无人机和机械臂)评估模型的语义、空间、时间和物理推理能力。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言导航需要代理在具身约束下进行推理和行动。虽然视觉-语言模型展现出强大的泛化能力，但当前基准测试对具身因素(物理平台选择、传感器配置和模态对齐)如何影响感知、推理和控制的理解有限。&lt;h4&gt;目的&lt;/h4&gt;创建一个闭环基准测试(Embodied4C)作为具身推理的图灵测试，评估视觉-语言模型在不同具身环境中的核心能力，并了解具身因素对模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;Embodied4C通过三个异构具身平台评估VLMs，包含约1.1K个一次性推理问题和58个目标导向的导航任务。这些任务评估语义、空间、时间和物理推理四个维度。每个具身平台提供动态传感器配置和环境变化，并整合远域查询以防止具身过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;对十个先进VLMs和四个具身控制基线的评估表明，跨模态对齐和指令调优比模型规模更重要，而空间和时间推理是可靠具身能力的主要瓶颈。&lt;h4&gt;结论&lt;/h4&gt;具身因素对视觉-语言模型性能有显著影响，Embodied4C为评估和改进VLMs的具身推理能力提供了重要工具。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言导航需要代理在具身约束下进行推理和行动。虽然视觉-语言模型展现出强大的泛化能力，但当前基准测试对具身因素如何影响感知、推理和控制的理解有限。我们引入了Embodied4C，一个闭环基准测试，设计为具身推理的图灵测试。该基准测试通过三个异构具身平台评估VLMs的核心具身能力，包含约1.1K个一次性推理问题和58个目标导向的导航任务。这些任务共同评估四个基础维度：语义、空间、时间和物理推理。每个具身平台提供动态传感器配置和环境变化，以测试超越平台特定适应的泛化能力。为防止具身过拟合，Embodied4C整合了针对抽象和跨上下文推理的远域查询。对十个先进VLMs和四个具身控制基线的全面评估表明，跨模态对齐和指令调优比模型规模更重要，而空间和时间推理仍然是可靠具身能力的主要瓶颈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence.</description>
      <author>example@mail.com (Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax)</author>
      <guid isPermaLink="false">2512.18028v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery</title>
      <link>http://arxiv.org/abs/2512.19612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MauBERT，这是HuBERT的一个多语言扩展版本，利用发音特征进行稳健的跨语言语音表征学习。&lt;h4&gt;背景&lt;/h4&gt;现有自监督语音模型在多语言场景下的表征能力有限，需要更有效的跨语言学习方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够学习语言无关表征的多语言语音模型，捕捉多语言语音特性，并适应未见语言。&lt;h4&gt;方法&lt;/h4&gt;基于55种语言的语音到发音特征映射监督继续进行HuBERT预训练，让模型从多语言数据中预测发音特征或音素。&lt;h4&gt;主要发现&lt;/h4&gt;MauBERT模型比最先进的多语言自监督学习模型产生更多上下文不变的表征，能够有效适应未见语言和日常语音，仅需10小时语音的自监督微调。&lt;h4&gt;结论&lt;/h4&gt;为在自监督语音模型中植入语言归纳偏置提供了一种有效方法，提高了模型的跨语言适应性和语音表征能力。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了MauBERT，这是HuBERT的一个多语言扩展版本，利用发音特征进行稳健的跨语言语音表征学习。我们基于55种语言的语音到发音特征映射监督继续进行HuBERT预训练。我们的模型从多语言数据中学习预测发音特征或音素，从而捕捉多语言语音特性的语言无关表征。通过全面的ABX可辨别性测试，我们表明MauBERT模型比最先进的多语言自监督学习模型产生更多上下文不变的表征。此外，模型能够有效适应未见语言和日常语音，只需最少的自监督微调（10小时语音）。这为在自监督语音模型中植入语言归纳偏置建立了一种有效方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces MauBERT, a multilingual extension of HuBERT that leverages articulatory features for robust cross-lingual phonetic representation learning. We continue HuBERT pre-training with supervision based on a phonetic-to-articulatory feature mapping in 55 languages. Our models learn from multilingual data to predict articulatory features or phones, resulting in language-independent representations that capture multilingual phonetic properties. Through comprehensive ABX discriminability testing, we show MauBERT models produce more context-invariant representations than state-of-the-art multilingual self-supervised learning models. Additionally, the models effectively adapt to unseen languages and casual speech with minimal self-supervised fine-tuning (10 hours of speech). This establishes an effective approach for instilling linguistic inductive biases in self-supervised speech models.</description>
      <author>example@mail.com (Angelo Ortiz Tandazo, Manel Khentout, Youssef Benchekroun, Thomas Hueber, Emmanuel Dupoux)</author>
      <guid isPermaLink="false">2512.19612v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</title>
      <link>http://arxiv.org/abs/2512.19605v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的基于核正则器的自监督学习算法家族KerJEPAs，通过扩展可用核和先验的类别，提高了训练稳定性和设计灵活性。&lt;h4&gt;背景&lt;/h4&gt;自监督的联合嵌入预测架构(JEPAs)的最新研究表明，将欧几里得表示正则化为各向同性高斯先验可以在训练稳定性和下游泛化方面带来可证明的提升。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的、灵活的KerJEPAs家族，作为自监督学习算法，提供基于核正则器的解决方案。&lt;h4&gt;方法&lt;/h4&gt;扩展可用的核和先验类别，计算切片最大均值差异的高维闭式极限，开发具有改进特性的替代KerJEPAs算法。&lt;h4&gt;主要发现&lt;/h4&gt;新开发的KerJEPAs算法具有改进的训练稳定性和设计灵活性，其中一个实例对应于最近引入的LeJEPA Epps-Pulley正则器，它使用高斯先验和高斯核来近似切片最大均值差异。&lt;h4&gt;结论&lt;/h4&gt;KerJEPAs为自监督学习提供了一种灵活的新方法，通过多样化的核和先验选择，能够提高模型训练稳定性和设计灵活性。&lt;h4&gt;翻译&lt;/h4&gt;自监督联合嵌入预测架构的最新突破已经证明，将欧几里得表示正则化为各向同性高斯先验可以为训练稳定性和下游泛化带来可证明的提升。我们引入了一个新的、灵活的KerJEPAs家族，这是具有基于核的正则器的自监督学习算法。该家族的一个实例对应于最近引入的LeJEPA Epps-Pulley正则器，它使用高斯先验和高斯核来近似切片最大均值差异。通过扩展可用核和先验的类别，并计算切片MMD的高维闭式极限，我们开发了具有多种有利特性的替代KerJEPAs，包括改进的训练稳定性和设计灵活性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.</description>
      <author>example@mail.com (Eric Zimmermann, Harley Wiltzer, Justin Szeto, David Alvarez-Melis, Lester Mackey)</author>
      <guid isPermaLink="false">2512.19605v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2512.19133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI 2026, first version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了WorldRFT，一种面向规划的潜在世界模型框架，通过分层规划分解和局部感知交互细化机制将场景表示学习与规划对齐，并通过强化学习微调提高安全关键策略性能，在nuScenes和NavSim基准测试上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;潜在世界模型通过时间自监督学习增强场景表示，为端到端自动驾驶提供无需感知标注的范式。然而，以重建为导向的表示学习将感知与规划任务纠缠，导致规划优化次优。&lt;h4&gt;目的&lt;/h4&gt;解决以重建为导向的表示学习与规划任务之间的冲突，提出面向规划的潜在世界模型框架，使场景表示学习与规划目标对齐，提高安全关键策略性能。&lt;h4&gt;方法&lt;/h4&gt;提出WorldRFT框架，集成视觉几何基础模型提高3D空间感知能力，采用分层规划任务分解指导表示优化，利用局部感知迭代细化推导面向规划的驾驶策略，并引入组相对策略优化(GRPO)应用轨迹高斯化和碰撞感知奖励微调驾驶策略。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes上将碰撞率降低83%（0.30%降至0.05%）；在NavSim上仅使用摄像头传感器输入，达到与基于LiDAR的最先进方法DiffusionDrive相当的性能（87.8对88.1 PDMS）。&lt;h4&gt;结论&lt;/h4&gt;WorldRFT通过将场景表示学习与规划目标对齐，显著提高自动驾驶安全性和性能，在开放循环nuScenes和封闭循环NavSim基准测试上都取得最先进性能。&lt;h4&gt;翻译&lt;/h4&gt;潜在世界模型通过时间自监督学习增强场景表示，为端到端自动驾驶呈现了一种无需感知标注的范式。然而，以重建为导向的表示学习将感知与规划任务纠缠在一起，导致规划优化次优。为了解决这一挑战，我们提出了WorldRFT，这是一种面向规划的潜在世界模型框架，通过分层规划分解和局部感知交互细化机制将场景表示学习与规划对齐，并通过强化学习微调增强安全关键策略性能。具体而言，WorldRFT集成了视觉几何基础模型以提高3D空间感知能力，采用分层规划任务分解来指导表示优化，并利用局部感知迭代细化来推导面向规划的驾驶策略。此外，我们引入了组相对策略优化，该优化应用轨迹高斯化和碰撞感知奖励来微调驾驶策略，从而在安全性方面产生系统性改进。WorldRFT在开放循环nuScenes和封闭循环NavSim基准测试上都取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -&gt; 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).</description>
      <author>example@mail.com (Pengxuan Yang, Ben Lu, Zhongpu Xia, Chao Han, Yinfeng Gao, Teng Zhang, Kun Zhan, XianPeng Lang, Yupeng Zheng, Qichao Zhang)</author>
      <guid isPermaLink="false">2512.19133v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners</title>
      <link>http://arxiv.org/abs/2512.18988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种脱离触发对比持续学习(DTCCL)框架，通过真实世界运营改进自动驾驶公交车的规划策略，无需人工监督即可显著提升性能。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶公交车在固定路线上运行，但面临开放、动态的城市环境。脱离事件通常地理上集中在高度互动区域，传统模仿学习方法难以纠正这些策略层面的失败，因为容易对稀疏的脱离数据过拟合。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够通过自动驾驶公交车的真实世界运营来持续改进其规划策略的框架，解决传统学习方法在处理稀疏脱离数据时的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出DTCCL框架，每次脱离事件触发基于云的数据增强，通过扰动周围代理同时保留路线上下文生成正负样本；利用对比学习优化策略表示以更好区分安全与不安全行为；在云-边缘循环中应用持续更新，无需人工监督。&lt;h4&gt;主要发现&lt;/h4&gt;在城市公交路线上的实验表明，与直接重新训练相比，DTCCL将整体规划性能提高了48.6%&lt;h4&gt;结论&lt;/h4&gt;DTCCL验证了其在自动驾驶公共交通中可扩展的闭环策略改进的有效性，为自动驾驶公交车的安全运营提供了新的解决方案&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶公交车在固定路线上运行，但必须在开放、动态的城市环境中运行。这些路线上的脱离事件通常地理上集中，并且通常发生在高度互动区域。这些策略层面的失败很难通过传统的模仿学习来纠正，因为传统模仿学习容易对稀疏的脱离数据过拟合。为解决这一问题，本文提出了脱离触发对比持续学习(DTCCL)框架，使自动驾驶公交车能够通过真实世界运营改进规划策略。每次脱离事件触发基于云的数据增强，通过扰动周围代理同时保留路线上下文来生成正负样本。对比学习优化策略表示以更好地区分安全和不安全行为，并在云-边缘循环中应用持续更新，无需人工监督。城市公交路线上的实验表明，与直接重新训练相比，DTCCL将整体规划性能提高了48.6%，验证了其在自动驾驶公共交通中可扩展的闭环策略改进的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous buses run on fixed routes but must operate in open, dynamic urban environments. Disengagement events on these routes are often geographically concentrated and typically arise from planner failures in highly interactive regions. Such policy-level failures are difficult to correct using conventional imitation learning, which easily overfits to sparse disengagement data. To address this issue, this paper presents a Disengagement-Triggered Contrastive Continual Learning (DTCCL) framework that enables autonomous buses to improve planning policies through real-world operation. Each disengagement triggers cloud-based data augmentation that generates positive and negative samples by perturbing surrounding agents while preserving route context. Contrastive learning refines policy representations to better distinguish safe and unsafe behaviors, and continual updates are applied in a cloud-edge loop without human supervision. Experiments on urban bus routes demonstrate that DTCCL improves overall planning performance by 48.6 percent compared with direct retraining, validating its effectiveness for scalable, closed-loop policy improvement in autonomous public transport.</description>
      <author>example@mail.com (Yanding Yang, Weitao Zhou, Jinhai Wang, Xiaomin Guo, Junze Wen, Xiaolong Liu, Lang Ding, Zheng Fu, Jinyu Miao, Kun Jiang, Diange Yang)</author>
      <guid isPermaLink="false">2512.18988v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing</title>
      <link>http://arxiv.org/abs/2512.18709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the Association for the Advancement of Artificial Intelligence 2026(AAAI2026)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为KeenKT的知识追踪模型，通过使用正态逆高斯分布表示学生知识状态，解决了传统单点估计方法无法区分真实能力与偶然表现的问题，显著提高了预测准确性和对行为波动的敏感性。&lt;h4&gt;背景&lt;/h4&gt;知识追踪(KT)旨在根据学生历史学习交互动态建模学生对知识概念的掌握程度。目前大多数方法依赖于单点估计，无法区分真实能力与爆发性表现或粗心，从而在判断掌握程度时产生歧义。&lt;h4&gt;目的&lt;/h4&gt;解决当前KT方法中单点估计导致的歧义问题，更准确地捕捉学生学习行为的波动性，提高知识追踪的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出知识掌握状态消歧知识追踪(KeenKT)模型，使用正态逆高斯(NIG)分布表示学生知识状态，设计基于NIG距离的注意力机制建模知识状态动态演化，并引入基于扩散的去噪重建损失和分布对比学习损失增强模型鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在六个公共数据集上进行的实验表明，KeenKT在预测准确性和对行为波动的敏感性方面优于最先进的KT模型，最大AUC改进为5.85%，最大ACC改进为6.89%。&lt;h4&gt;结论&lt;/h4&gt;KeenKT模型通过分布表示而非单点估计，能够更好地捕捉学生知识掌握状态的变化，在预测准确性和鲁棒性方面都有显著提升，为知识追踪领域提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;知识追踪(KT)旨在根据学生历史学习交互动态建模学生对知识概念的掌握程度。大多数当前方法依赖于单点估计，无法区分真实能力与爆发性表现或粗心，从而在判断掌握程度时产生歧义。为解决这一问题，我们提出了知识掌握状态消歧知识追踪模型(KeenKT)，该模型使用正态逆高斯(NIG)分布表示学生在每次交互时的知识状态，从而捕捉学习行为的波动。此外，我们设计了一种基于NIG距离的注意力机制来建模知识状态的动态演化。同时，我们引入了一种基于扩散的去噪重建损失和分布对比学习损失，以增强模型的鲁棒性。在六个公共数据集上的大量实验表明，KeenKT在预测准确性和对行为波动的敏感性方面优于最先进的KT模型。所提出的方法最大AUC改进为5.85%，最大ACC改进为6.89%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Tracing (KT) aims to dynamically model a student's mastery of knowledge concepts based on their historical learning interactions. Most current methods rely on single-point estimates, which cannot distinguish true ability from outburst or carelessness, creating ambiguity in judging mastery. To address this issue, we propose a Knowledge Mastery-State Disambiguation for Knowledge Tracing model (KeenKT), which represents a student's knowledge state at each interaction using a Normal-Inverse-Gaussian (NIG) distribution, thereby capturing the fluctuations in student learning behaviors. Furthermore, we design an NIG-distance-based attention mechanism to model the dynamic evolution of the knowledge state. In addition, we introduce a diffusion-based denoising reconstruction loss and a distributional contrastive learning loss to enhance the model's robustness. Extensive experiments on six public datasets demonstrate that KeenKT outperforms SOTA KT models in terms of prediction accuracy and sensitivity to behavioral fluctuations. The proposed method yields the maximum AUC improvement of 5.85% and the maximum ACC improvement of 6.89%.</description>
      <author>example@mail.com (Zhifei Li, Lifan Chen, Jiali Yi, Xiaoju Hou, Yue Zhao, Wenxin Huang, Miao Zhang, Kui Xiao, Bing Yang)</author>
      <guid isPermaLink="false">2512.18709v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing</title>
      <link>http://arxiv.org/abs/2512.18575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究首次全面评估了记忆增强型脉冲神经网络(SNNs)的跨模态泛化能力，发现不同记忆机制在视觉和听觉任务上表现各异，Hopfield网络模态特异性强，监督对比学习更平衡，HGRN多模态联合训练表现优异。&lt;h4&gt;背景&lt;/h4&gt;记忆增强型脉冲神经网络(SNNs)有望实现能效高效的神经形态计算，但其在不同感官模态间的泛化能力尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;研究SNN中记忆机制在视觉和听觉神经形态数据集上的跨模态表现，评估不同记忆机制的适用性和效率。&lt;h4&gt;方法&lt;/h4&gt;对三种记忆机制(Hopfield网络、分层门控循环网络HGRNs和监督对比学习SCL)进行跨模态消融研究，在视觉(N-MNIST)和听觉(SHD)神经形态数据集上评估五种架构的性能。&lt;h4&gt;主要发现&lt;/h4&gt;Hopfield网络在视觉任务上准确率达97.68%，但听觉任务仅76.15%，显示严重的模态特异性；监督对比学习表现出更平衡的跨模态性能(视觉96.72%，听觉82.16%)；HGRN多模态联合训练达到94.41%视觉和79.37%听觉准确率；记忆机制表现出任务特定优势而非普遍适用性；定量印迹分析证实弱跨模态对齐(0.038相似度)，验证了并行架构设计的合理性。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了神经形态系统中模态特定记忆优化的首个实证证据，实现了比传统神经网络高603倍的能效。&lt;h4&gt;翻译&lt;/h4&gt;记忆增强型脉冲神经网络(SNNs)有望实现能效高效的神经形态计算，但它们在感官模态间的泛化能力仍未被探索。我们首次对SNN中的记忆机制进行了全面的跨模态消融研究，评估了在视觉(N-MNIST)和听觉(SHD)神经形态数据集上的Hopfield网络、分层门控循环网络(HGRNs)和监督对比学习(SCL)的表现。我们对五种架构的系统评估揭示了显著的模态依赖性性能模式：Hopfield网络在视觉任务上达到97.68%的准确率，但在听觉任务上仅为76.15%(相差21.53个百分点)，显示出严重的模态特异性专业化，而监督对比学习表现出更平衡的跨模态性能(视觉96.72%，听觉82.16%，相差14.56个百分点)。这些发现确立了记忆机制表现出特定任务的优势，而非普遍适用性。使用HGRN进行联合多模态训练实现了94.41%的视觉和79.37%的听觉准确率(平均88.78%)，通过统一部署与并行HGRN性能相当。定量印迹分析证实了弱跨模态对齐(0.038相似度)，验证了我们的并行架构设计。我们的工作为神经形态系统中的模态特定记忆优化提供了首个实证证据，实现了比传统神经网络高603倍的能效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.</description>
      <author>example@mail.com (Effiong Blessing, Chiung-Yi Tseng, Somshubhra Roy, Junaid Rehman, Isaac Nkrumah)</author>
      <guid isPermaLink="false">2512.18575v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models</title>
      <link>http://arxiv.org/abs/2512.17983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了参数高效的微调技术在人类活动识别中的应用，特别是低秩适应(LoRA)和量化LoRA(Quantized LoRA)，作为全模型微调的可扩展替代方案。&lt;h4&gt;背景&lt;/h4&gt;人类活动识别是普适计算的基础任务。尽管自监督学习和基于Transformer的架构最近显著提高了HAR性能，但将大型预训练模型适应到新领域仍面临挑战，主要受限于目标设备的计算资源。&lt;h4&gt;目的&lt;/h4&gt;研究参数高效的微调技术，特别是LoRA和QLoRA，探索它们作为全模型微调替代方案的可行性和效果，以解决资源受限环境中的模型适应问题。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于掩码自编码器骨干的适应框架，并在五个开放HAR数据集上使用留一数据集验证协议(Leave-One-Dataset-Out)进行性能评估。&lt;h4&gt;主要发现&lt;/h4&gt;LoRA和QLoRA能够达到与全微调相当的识别性能，同时显著减少可训练参数数量、内存使用和训练时间。LoRA在有限监督下仍保持稳健性能，适配器秩提供了准确性和效率间的可控权衡。QLoRA通过量化减少冻结权重内存占用，对分类质量影响最小。&lt;h4&gt;结论&lt;/h4&gt;参数高效的微调技术(LoRA和QLoRA)是全模型微调的有效替代方案，能够在保持高性能的同时显著减少资源需求，特别适合资源受限的HAR应用场景。&lt;h4&gt;翻译&lt;/h4&gt;人类活动识别是普适计算的基础任务。尽管最近自监督学习和基于Transformer架构的进展显著提高了HAR性能，但由于目标设备上计算资源有限，将大型预训练模型适应到新领域仍然是一个实际挑战。本文研究了参数高效的微调技术，特别是低秩适应(LoRA)和量化LoRA，作为全模型微调的可扩展替代方案用于HAR。我们提出了一个基于掩码自编码器骨干的适应框架，并在五个开放HAR数据集上使用留一数据集验证协议评估其性能。实验证明，LoRA和QLoRA都能匹配全微调的识别性能，同时显著减少可训练参数数量、内存使用和训练时间。进一步分析显示，LoRA即使在有限监督下也能保持稳健性能，且适配器秩提供了准确性和效率之间可控的权衡。QLoRA通过量化减少冻结权重的内存占用，同时最小化对分类质量的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Activity Recognition is a foundational task in pervasive computing. While recent advances in self-supervised learning and transformer-based architectures have significantly improved HAR performance, adapting large pretrained models to new domains remains a practical challenge due to limited computational resources on target devices. This papers investigates parameter-efficient fine-tuning techniques, specifically Low-Rank Adaptation (LoRA) and Quantized LoRA, as scalable alternatives to full model fine-tuning for HAR. We propose an adaptation framework built upon a Masked Autoencoder backbone and evaluate its performance under a Leave-One-Dataset-Out validation protocol across five open HAR datasets. Our experiments demonstrate that both LoRA and QLoRA can match the recognition performance of full fine-tuning while significantly reducing the number of trainable parameters, memory usage, and training time. Further analyses reveal that LoRA maintains robust performance even under limited supervision and that the adapter rank provides a controllable trade-off between accuracy and efficiency. QLoRA extends these benefits by reducing the memory footprint of frozen weights through quantization, with minimal impact on classification quality.</description>
      <author>example@mail.com (Irina Seregina, Philippe Lalanda, German Vega)</author>
      <guid isPermaLink="false">2512.17983v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization</title>
      <link>http://arxiv.org/abs/2512.16504v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于片段判别的自监督预训练方法，用于解决基于骨架的时间动作定位问题，通过对比学习提升特征区分能力，并结合U形模块增强特征分辨率，在多个数据集上取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;自监督预训练范式在基于骨架的动作识别中取得了很大成功，但在基于骨架的时间动作定位方面仍然具有挑战性且研究不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够学习有效时间敏感特征的自监督方法，用于基于骨架的时间动作定位，以捕捉动作边界处相邻帧之间的细微差别。&lt;h4&gt;方法&lt;/h4&gt;提出片段判别自监督预训练任务，将骨架序列密集投影到不重叠片段，通过对比学习促进跨视频片段特征区分；结合U形模块融合中间特征，增强帧级定位的特征分辨率。&lt;h4&gt;主要发现&lt;/h4&gt;所提方法在BABEL数据集上对现有基于骨架的对比学习方法进行了改进，在多个子集和评估协议上表现一致；在NTU RGB+D和BABEL上预训练后，在PKUMMD上实现了最先进的迁移学习性能。&lt;h4&gt;结论&lt;/h4&gt;基于片段判别的自监督预训练方法结合特征增强技术能够有效提升基于骨架的时间动作定位性能，为该领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;自监督预训练范式在通过对比学习学习基于骨架的动作识别的3D动作表示方面取得了巨大成功。然而，学习用于基于骨架的时间动作定位的有效表示仍然具有挑战性且研究不足。与视频级动作识别不同，检测动作边界需要时间敏感的特征，这些特征能够捕捉标签变化处相邻帧之间的细微差别。为此，我们提出了一个用于自监督预训练的片段判别预训练任务，该方法将骨架序列密集投影到不重叠的片段中，并通过对比学习促进跨视频的片段特征区分。此外，我们通过融合中间特征和U形模块构建了基于骨架的动作识别模型的强大骨干网络，以提高帧级定位的特征分辨率。我们的方法在BABEL上对现有的基于骨架的对比学习方法进行了改进，在多个子集和评估协议上表现一致。我们还在NTU RGB+D和BABEL上进行预训练后，在PKUMMD上实现了最先进的迁移学习性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.</description>
      <author>example@mail.com (Qiushuo Cheng, Jingjing Liu, Catherine Morgan, Alan Whone, Majid Mirmehdi)</author>
      <guid isPermaLink="false">2512.16504v2</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</title>
      <link>http://arxiv.org/abs/2512.17954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Sigmoid的通用和风格监督对比学习方法（SCS-SupCon），解决了图像分类中的类间差异小和类内变化大问题，通过引入基于Sigmoid的成对对比损失和风格距离约束，显著提升了细粒度识别任务的性能。&lt;h4&gt;背景&lt;/h4&gt;图像分类受到类间差异小和类内变化大的阻碍，限制了现有对比学习方法的有效性。基于InfoNCE损失的有监督对比方法存在负样本稀释问题，且缺乏自适应决策边界，降低了细粒度识别任务的判别能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有对比学习方法的局限性，提出一种能够自适应决策边界、减轻负样本稀释并有效利用监督的新型对比学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出SCS-SupCon框架，引入基于Sigmoid的成对对比损失，具有可学习的温度和偏置参数以实现自适应决策边界，强调困难负样本并减轻负样本稀释；同时添加明确的风格距离约束，解耦风格和内容表示，实现更稳健的特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;在六个基准数据集上的实验表明，SCS-SupCon在CNN和Transformer骨干网络上均达到最先进性能。在CIFAR-100上使用ResNet-50，比SupCon提高约3.9个百分点top-1准确率，比CS-SupCon提高约1.7个百分点；在细粒度数据集上，比CS-SupCon高出0.4-3.0个百分点。消融研究和统计分析确认了方法的稳健性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Friedman检验和Nemenyi事后评估验证了所观察到的改进具有统计显著性，证明了SCS-SupCon框架的稳定性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;图像分类受到类间细微差异和类内显著变化的阻碍，这限制了现有对比学习方法的有效性。基于InfoNCE损失的有监督对比方法遭受负样本稀释问题，且缺乏自适应决策边界，从而降低了细粒度识别任务的判别能力。为解决这些限制，我们提出了基于Sigmoid的通用和风格监督对比学习（SCS-SupCon）。我们的框架引入了基于Sigmoid的成对对比损失，具有可学习的温度和偏置参数，以实现自适应决策边界。这种公式强调困难负样本，减轻负样本稀释，并更有效地利用监督。此外，明确的风格距离约束进一步解耦了风格和内容表示，导致更稳健的特征学习。在包括CUB200-2011和Stanford Dogs在内的六个基准数据集上的综合实验表明，SCS-SupCon在CNN和Transformer骨干网络上均实现了最先进的性能。在CIFAR-100上使用ResNet-50，SCS-SupCon在五折交叉验证下比SupCon提高约3.9个百分点的top-1准确率，比CS-SupCon提高约1.7个百分点。在细粒度数据集上，它比CS-SupCon高出0.4-3.0个百分点。大量的消融研究和统计分析进一步确认了所提出框架的稳健性和泛化能力，Friedman检验和Nemenyi事后评估验证了所观察到的改进的稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image classification is hindered by subtle inter-class differences and substantial intra-class variations, which limit the effectiveness of existing contrastive learning methods. Supervised contrastive approaches based on the InfoNCE loss suffer from negative-sample dilution and lack adaptive decision boundaries, thereby reducing discriminative power in fine-grained recognition tasks. To address these limitations, we propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon). Our framework introduces a sigmoid-based pairwise contrastive loss with learnable temperature and bias parameters to enable adaptive decision boundaries. This formulation emphasizes hard negatives, mitigates negative-sample dilution, and more effectively exploits supervision. In addition, an explicit style-distance constraint further disentangles style and content representations, leading to more robust feature learning. Comprehensive experiments on six benchmark datasets, including CUB200-2011 and Stanford Dogs, demonstrate that SCS-SupCon achieves state-of-the-art performance across both CNN and Transformer backbones. On CIFAR-100 with ResNet-50, SCS-SupCon improves top-1 accuracy over SupCon by approximately 3.9 percentage points and over CS-SupCon by approximately 1.7 points under five-fold cross-validation. On fine-grained datasets, it outperforms CS-SupCon by 0.4--3.0 points. Extensive ablation studies and statistical analyses further confirm the robustness and generalization of the proposed framework, with Friedman tests and Nemenyi post-hoc evaluations validating the stability of the observed improvements.</description>
      <author>example@mail.com (Bin Wang, Fadi Dornaika)</author>
      <guid isPermaLink="false">2512.17954v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale</title>
      <link>http://arxiv.org/abs/2512.19097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  47 pages, 13 figures, 26 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了DIVER-1，一种基于最大规模多样化脑电生理信号数据集训练的基础模型家族，实现了在EEG和iEEG任务上的最先进性能，并首次揭示了该领域的扩展规律。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)和颅内脑电图(iEEG)信号在神经科学、脑机接口和临床应用中至关重要，但现有的基础模型规模有限，尽管有明确证据表明扩大规模可以提高性能。&lt;h4&gt;目的&lt;/h4&gt;开发大规模EEG和iEEG基础模型，研究该领域的扩展规律，并提供高效扩展和资源分配的具体指导。&lt;h4&gt;方法&lt;/h4&gt;训练了迄今为止最大和最多样化的语料库(5.3k小时iEEG和54k小时EEG，来自17.7k+受试者)，将模型规模扩大到18.2亿参数，设计了任意变量注意力、滑动时态条件位置编码和多域重建等架构创新。&lt;h4&gt;主要发现&lt;/h4&gt;首次对脑电生理学领域进行系统扩展规律分析，发现该领域遵循数据约束的扩展规律：对于给定数据和计算量，训练时间更长的小型模型优于训练时间较短的大型模型，这与之前强调模型规模而非训练时间的模型形成对比。&lt;h4&gt;结论&lt;/h4&gt;DIVER-1的iEEG和EEG模型分别在各自基准测试中实现最先进性能，为脑电生理学基础模型开发中的高效扩展和资源分配提供了具体指导。&lt;h4&gt;翻译&lt;/h4&gt;脑电图信号如EEG和iEEG是神经科学、脑机接口和临床应用的核心，然而尽管有明确证据表明扩大规模可以提高性能，现有的基础模型仍然规模有限。我们介绍了DIVER-1，一种迄今为止在最大和最多样化语料库上训练的EEG和iEEG基础模型家族-5.3k小时的iEEG和54k小时的EEG(来自超过17.7k受试者的160万通道小时)-并将规模扩大到18.2亿参数。我们首次对该领域进行了系统的扩展规律分析，表明它们遵循数据约束的扩展规律：对于给定的数据和计算量，训练时间更长的小型模型始终优于训练时间较短的大型模型。这种行为与之前强调模型规模而非训练时间的脑电生理学基础模型形成对比。为实现强性能，我们还设计了架构创新，包括任意变量注意力、滑动时态条件位置编码和多域重建。DIVER-1的iEEG和EEG模型分别在各自基准测试中实现了最先进的性能，为脑电生理学基础模型开发中的高效扩展和资源分配提供了具体指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.</description>
      <author>example@mail.com (Danny Dongyeop Han, Yonghyeon Gwon, Ahhyun Lucy Lee, Taeyang Lee, Seong Jin Lee, Jubin Choi, Sebin Lee, Jihyun Bang, Seungju Lee, David Keetae Park, Shinjae Yoo, Chun Kee Chung, Jiook Cha)</author>
      <guid isPermaLink="false">2512.19097v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models</title>
      <link>http://arxiv.org/abs/2512.19083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by the IEEE Internet of Things Journal (IoT-J) for publication in the Special Issue on "Augmented Edge Sensing Intelligence for Low-Altitude IoT Systems"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoDrone是一种创新的云-边缘-端协同计算框架，通过集成基础模型解决无人机自主导航中的计算资源限制问题，显著提升了性能。&lt;h4&gt;背景&lt;/h4&gt;无人机自主导航面临机载计算资源有限的挑战，限制了深度神经网络的复杂度；将任务卸载到远程边缘服务器会引入高延迟，造成系统设计中的固有权衡。&lt;h4&gt;目的&lt;/h4&gt;解决资源受限无人机平台的自主导航问题，通过集成基础模型提升性能，减少机载计算和数据传输开销，实现高效的环境感知和导航决策。&lt;h4&gt;方法&lt;/h4&gt;采用灰度图像减少计算开销；使用边缘辅助的Depth Anything V2模型进行深度估计；引入基于一维占用网格的导航方法；设计基于深度强化学习的神经调度器；开发无人机特定的视觉语言交互模块，实现云端基础模型与无人机的有效交互。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明CoDrone在不同飞行速度和网络条件下优于基线方法，实现平均飞行距离增加40%，平均导航质量提高5%。&lt;h4&gt;结论&lt;/h4&gt;CoDrone成功解决了无人机自主导航中的计算资源限制问题，通过云-边缘-端协同计算框架和基础模型的集成，实现了高效且适应性强的自主导航系统。&lt;h4&gt;翻译&lt;/h4&gt;无人机自主导航面临来自机载计算资源有限的关键挑战，这限制了部署的深度神经网络只能使用浅层架构，无法处理复杂环境。将任务卸载到远程边缘服务器会引入高延迟，造成系统设计中的固有权衡。为解决这些限制，我们提出了CoDrone——首个将基础模型集成到无人机自主巡航场景的云-边缘-端协同计算框架——有效利用基础模型提升资源受限无人机平台的性能。为减少机载计算和数据传输开销，CoDrone使用灰度图像进行导航模型。当需要增强环境感知时，CoDrone利用边缘辅助的基础模型Depth Anything V2进行深度估计，并引入基于一维占用网格的新型导航方法——在提高自主导航效率和表示简单性的同时实现细粒度场景理解。CoDrone的关键组件是一个基于深度强化学习的神经调度器，无缝集成深度估计与自主导航决策，实现实时适应动态环境。此外，该框架引入了无人机特定的视觉语言交互模块，包含领域定制的低级飞行原语，实现云端基础模型与无人机之间的有效交互。VLM的引入增强了在复杂未见场景中的开放集推理能力。实验结果表明，CoDrone在不同飞行速度和网络条件下优于基线方法，实现了平均飞行距离增加40%和平均导航质量提高5%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无人机自主导航中的计算资源限制和网络延迟问题。由于无人机上的计算资源有限，通常只能部署浅层神经网络，无法处理复杂环境；而将任务卸载到远程边缘服务器又会引入高延迟。这个问题很重要，因为低空经济正在成为全球数字和工业转型的重要驱动力，预计到2030年市场价值将达到4000亿美元，而无人机作为低空物联网系统的主要平台，在精准农业、智能制造、无人机配送等领域应用广泛，自主导航对它们完成任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了无人机导航面临的三大挑战：传统DNN方法在复杂环境中的局限性、网络条件波动带来的性能问题、以及VLMs计算需求高的部署障碍。针对这些问题，作者设计了三层解决方案：使用灰度图像减少计算负担，在边缘部署深度估计模型增强环境感知；引入基于DRL的神经调度器优化任务分配；设计无人机特定的视觉语言交互框架实现云端智能调用。作者借鉴了现有工作如Depth Anything V2深度估计模型、Qwen-VL-Max视觉语言模型、A2C调度算法等，但进行了创新性整合和应用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建云-边-端协同计算框架，将基础模型集成到无人机自主巡航中，通过结合灰度图像处理、深度估计、一维占用网格导航和神经调度器，实现资源受限条件下的高效自主导航。整体流程：1)无人机采集灰度图像；2)神经调度器决定处理位置和压缩比；3)导航模块处理图像推断转向角和碰撞概率；4)边缘服务器使用深度估计模型生成占用网格地图；5)控制算法调整飞行命令；6)调用策略模块决定是否激活云端视觉语言模型；7)飞行控制器执行最终命令控制无人机。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首创云-边-端协同自主无人机导航框架；2)使用灰度图像和轻量级DNN优化计算；3)引入一维占用网格导航方法增强环境感知；4)设计无人机特定的视觉语言交互模块。相比之前工作：与传统DNN方法相比，增强了复杂环境中的环境感知能力；与adaDrone等边缘辅助框架相比，更好地平衡了计算卸载和本地处理；与早期DRL路径规划相比，提供了更强的环境理解能力；与直接部署VLM的方法相比，通过云端部署和智能调用策略解决了计算需求问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CoDrone通过创新性地整合云-边-端计算架构和基础模型，解决了无人机自主导航中的计算资源限制和环境感知挑战，实现了在复杂环境中的高效、安全自主飞行。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.</description>
      <author>example@mail.com (Pengyu Chen, Tao Ouyang, Ke Luo, Weijie Hong, Xu Chen)</author>
      <guid isPermaLink="false">2512.19083v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</title>
      <link>http://arxiv.org/abs/2512.19020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为CETCAM的相机可控视频生成框架，通过一致的标记化方案消除对相机标注的需求，利用几何基础模型估计深度和相机参数，并采用渐进式训练策略实现高质量视频生成。&lt;h4&gt;背景&lt;/h4&gt;在视频生成中实现精确的相机控制具有挑战性，现有方法依赖相机姿态标注，这些标注难以扩展到大型动态数据集，且常与深度估计不一致，导致训练-测试差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种不需要相机标注的相机可控视频生成框架，解决现有方法在可扩展性和一致性方面的问题。&lt;h4&gt;方法&lt;/h4&gt;引入CETCAM框架，利用几何基础模型估计深度和相机参数并转换为几何感知标记，通过轻量级上下文块集成到预训练的视频扩散网络中，采用两阶段渐进式训练：先从原始视频数据学习相机可控性，再用高保真数据集细化视觉质量。&lt;h4&gt;主要发现&lt;/h4&gt;CETCAM在多个基准测试中实现了最先进的几何一致性、时间稳定性和视觉真实性，同时对修复和布局控制等额外控制模态表现出强大的适应性。&lt;h4&gt;结论&lt;/h4&gt;CETCAM成功解决了视频生成中精确相机控制的挑战，通过创新的标记化方法和渐进式训练策略实现了无需相机标注的高质量视频生成，具有良好的可扩展性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;在视频生成中实现精确的相机控制仍然具有挑战性，因为现有方法通常依赖于相机姿态标注，这些标注难以扩展到大型和动态数据集，并且经常与深度估计不一致，导致训练-测试差异。我们引入了CETCAM，一种相机可控的视频生成框架，它通过一致的、可扩展的标记化方案消除了对相机标注的需求。CETCAM利用几何基础模型（如VGGT）的最新进展来估计深度和相机参数，并将它们转换为统一的、几何感知的标记。这些标记通过轻量级上下文块无缝集成到预训练的视频扩散主干网络中。CETCAM采用两个渐进阶段进行训练：首先从多样化的原始视频数据中学习强大的相机可控性，然后使用精选的高保真数据集细化细粒度的视觉质量。在多个基准测试上的广泛实验证明了最先进的几何一致性、时间稳定性和视觉真实性。此外，CETCAM对额外的控制模态（包括修复和布局控制）表现出强大的适应性，突显了其在相机控制之外的灵活性。项目页面可在https://sjtuytc.github.io/CETCam_project_page.github.io/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.</description>
      <author>example@mail.com (Zelin Zhao, Xinyu Gong, Bangya Liu, Ziyang Song, Jun Zhang, Suhui Wu, Yongxin Chen, Hao Zhang)</author>
      <guid isPermaLink="false">2512.19020v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</title>
      <link>http://arxiv.org/abs/2512.19001v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新的OR引导的'预训练-强化'框架，用于弥合AI的自适应感知能力与OR的结构化严谨性之间的差距。通过模拟增强的OR模型生成高质量参考决策，设计领域信息深度学习基础模型，并利用强化学习进行微调，实现了显著的供应链管理改进。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能(AI)与运筹学(OR)在处理复杂库存系统方面的协同追求日益增强，一个关键挑战依然存在：如何有效调和AI的自适应感知能力与OR的结构化严谨性。&lt;h4&gt;目的&lt;/h4&gt;为了弥合AI与OR之间的差距，提出一种OR引导的'预训练-强化'框架，结合AI的自适应能力和OR的结构化严谨性，以优化复杂库存系统的管理。&lt;h4&gt;方法&lt;/h4&gt;1. 提出模拟增强的OR模型生成高质量参考决策；2. 使用OR衍生的决策作为训练标签，设计领域信息深度学习基础模型；3. 通过强化学习微调，使AI代理内化OR的最优性原则；4. 利用探索改进通用策略，允许专家指导进行场景特定适应；5. 通过数值实验和京东实地部署验证效果。&lt;h4&gt;主要发现&lt;/h4&gt;1. 模型显著优于现有工业实践；2. 库存周转天数减少5.27天；3. 库存率提高2.29%；4. 持有成本降低29.95%；5. 轻量级、领域信息模型在结构化OR逻辑引导下可实现先进性能和强大可转移性。&lt;h4&gt;结论&lt;/h4&gt;这种方法为智能供应链管理提供了一种可扩展且具有成本效益的范式，突显了深度对齐AI与OR的价值。与单纯依靠模型规模扩大的趋势不同，结合领域知识和结构化逻辑的方法能够取得更好的效果。&lt;h4&gt;翻译&lt;/h4&gt;随着人工智能(AI)与运筹学(OR)在处理复杂库存系统方面的协同追求日益增强，一个关键挑战依然存在：如何有效调和AI的自适应感知能力与OR的结构化严谨性。为了弥合这一差距，我们提出了一种创新的OR引导的'预训练-强化'框架。为了提供结构化指导，我们提出了一种模拟增强的OR模型，生成高质量的参考决策，隐式地捕捉复杂的业务约束和管理偏好。利用这些OR衍生的决策作为基础训练标签，我们设计了一个领域信息深度学习基础模型，建立基础决策能力，然后是强化学习(RL)微调阶段。独特的是，我们将RL定位为深度对齐机制，使AI代理能够内化OR的最优性原则，同时利用探索进行通用策略改进，并允许专家指导进行场景特定适应（如促销活动）。通过大量数值实验和京东公司的实地部署（辅以双重差分(DiD)分析）验证，我们的模型显著优于现有的工业实践，实现了现实世界的收益：库存周转天数减少5.27天，库存率提高2.29%，同时持有成本降低29.95%。与当前流行的蛮力模型扩展趋势相反，我们的研究表明，当由结构化的OR逻辑引导时，轻量级、领域信息模型可以实现最先进的性能和强大的可转移性。这种方法为智能供应链管理提供了一种可扩展且具有成本效益的范式，突显了深度对齐AI与OR的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided "Pretrain-then-Reinforce" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.</description>
      <author>example@mail.com (Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen)</author>
      <guid isPermaLink="false">2512.19001v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Towards AI-Guided Open-World Ecological Taxonomic Classification</title>
      <link>http://arxiv.org/abs/2512.18994v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 figures, 11 tables, and 15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了TaxoNet框架，解决AI指导生态分类中的多重挑战，包括类别不平衡、细粒度变异、时空域偏移和封闭集限制，显著提升了稀有分类单元的分类性能。&lt;h4&gt;背景&lt;/h4&gt;AI指导的生态分类对全球可持续发展如生物多样性监测、保护规划和政策制定至关重要，但面临长尾分类分布、细粒度分类变异、测试时空域偏移和封闭集假设等挑战。&lt;h4&gt;目的&lt;/h4&gt;引入开放世界生态分类学分类框架，捕捉现实生态环境中这些挑战的共存情况，并提出TaxoNet方法应对这些相互关联的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出TaxoNet，一种基于嵌入的编码器，采用双边缘惩罚损失函数，加强对稀有代表性分类单元的学习信号，减轻代表性过强分类单元的主导地位。&lt;h4&gt;主要发现&lt;/h4&gt;模型在Google Auto-Arborist、iNat-Plantae和NAFlora-Mini等多样化生态领域评估中始终优于基线方法，尤其对稀有分类单元表现更好，同时发现通用多模态基础模型在植物领域应用中受限。&lt;h4&gt;结论&lt;/h4&gt;TaxoNet框架有效解决了生态分类中的多种挑战，为开放世界植物分类监测提供了强大基础，特别提升了稀有分类单元的分类性能。&lt;h4&gt;翻译&lt;/h4&gt;AI指导的生态科、属、种分类支持全球可持续发展努力，如生物多样性监测、保护规划和政策制定。实现这一目标的进展受到长尾分类分布（由类别不平衡导致）、细粒度分类变异、测试时空域偏移以及只能识别已见过分类单元的封闭集假设的阻碍。我们引入了开放世界生态分类学分类，这是一个统一框架，捕捉了现实生态环境中这些挑战的共存情况。为应对这些挑战，我们提出了TaxoNet，一种基于嵌入的编码器，采用双边缘惩罚损失函数，加强来自稀有代表性分类单元的学习信号，同时减轻代表性过强分类单元的主导地位，直接应对相互关联的挑战。我们在多样化的生态领域评估了我们的方法：Google Auto-Arborist（城市树木）、iNat-Plantae（来自iNaturalist-2019各种生态系统的植物观察）和NAFlora-Mini（精选的标本集收藏）。我们的模型始终优于基线方法，特别是对稀有分类单元，为开放世界植物分类监测奠定了坚实基础。我们的研究进一步表明，通用多模态基础模型在植物领域应用中仍然受限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that general-purpose multimodal foundation models remain constrained in plant-domain applications.</description>
      <author>example@mail.com (Cheng Yaw Low, Heejoon Koo, Jaewoo Park, Kaleb Mesfin Asfaw, Meeyoung Cha)</author>
      <guid isPermaLink="false">2512.18994v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model for Unified Characterization of Optical Quantum States</title>
      <link>http://arxiv.org/abs/2512.18801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了第一个用于表征复杂度广泛的光学量子态的基础模型，能够在不进行完整层析的情况下预测广泛的光学量子态特性，特别是多模非高斯态。&lt;h4&gt;背景&lt;/h4&gt;机器学习方法已被用于推断有限族光学量子态的特定性质，但目前仍缺乏一个统一的模型可以在不进行完整层析的情况下预测广泛的光学量子态特性，特别是多模非高斯态。&lt;h4&gt;目的&lt;/h4&gt;引入第一个用于表征复杂度广泛的光学量子态的基础模型，该模型由三个关键因素定义：非高斯性、模数和压缩程度。&lt;h4&gt;方法&lt;/h4&gt;使用在低复杂度态上预训练的单个模型，可以直接应用于表征高复杂度态；通过有限的微调，模型可以适应下游任务，如预测保真度和Wigner负性。&lt;h4&gt;主要发现&lt;/h4&gt;模型可以表征实验相关态的广泛类别，包括强非高斯Schrödinger猫态、多达十个模的多模系统以及压缩程度高达10.4dB的高度压缩态。&lt;h4&gt;结论&lt;/h4&gt;建立了一个从有限测量数据表征光学量子态的统一框架，能够有效认证与光学量子信息计算、通信和计量学相关的量子态。&lt;h4&gt;翻译&lt;/h4&gt;机器学习方法已被用于推断有限族光学量子态的特定性质，但一个能够在不进行完整层析的情况下预测广泛的光学量子态特性（特别是多模非高斯态）的统一模型仍然缺乏。本文我们引入了第一个用于表征广泛复杂度的光学量子态的基础模型，该模型由三个关键因素定义：非高斯性、模数和压缩程度。我们展示了在低复杂度态上预训练的单个模型可以直接应用于表征高复杂度态。通过有限的微调，模型可以适应下游任务，如预测广泛实验相关态类的量子保真度和Wigner负性，包括强非高斯Schrödinger猫态、多达十个模的多模系统以及压缩程度高达10.4dB的高度压缩态。我们的结果建立了一个从有限测量数据表征光学量子态的统一框架，能够有效认证与光学量子信息计算、通信和计量学相关的量子态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning methods have been used to infer specific properties of limited families of optical quantum states, but a unified model that predicts a broad range of properties for practically relevant-especially multimode non-Gaussian-states without full tomography is still lacking. Here we introduce the first foundation model for the characterization of optical quantum states across a wide range of complexity, defined by three key factors: non-Gaussianity, number of modes, and degree of squeezing. We show that a single model pretrained on low-complexity states can be directly applied to characterize states of higher complexity. With limited fine-tuning, the model adapts to downstream tasks such as predicting quantum fidelity and Wigner negativity over a broad class of experimentally relevant states, including strongly non-Gaussian Schrödinger cat states, multimode systems with up to ten modes, and highly squeezed states with squeezing levels up to 10.4dB. Our results establish a unified framework for characterizing optical quantum states from limited measurement data, enabling efficient certification of quantum states relevant to optical quantum information computation, communication and metrology.</description>
      <author>example@mail.com (Xiaoting Gao, Yan Zhu, Feng-Xiao Sun, Ya-Dong Wu, Qiongyi He)</author>
      <guid isPermaLink="false">2512.18801v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>In-Context Audio Control of Video Diffusion Transformers</title>
      <link>http://arxiv.org/abs/2512.18772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ICAC框架，研究在视频生成中集成音频信号，通过掩码3D注意力机制实现稳定的训练和卓越的性能，达到良好的口型同步和视频质量。&lt;h4&gt;背景&lt;/h4&gt;视频生成领域正转向基于Transformer的统一基础模型，但这些模型主要关注文本、图像和深度图等模态，而音频等严格时间同步信号研究不足。&lt;h4&gt;目的&lt;/h4&gt;引入ICAC框架，研究在统一的完全注意力架构中集成音频信号，用于语音驱动的视频生成。&lt;h4&gt;方法&lt;/h4&gt;系统探索了三种注入音频条件的机制：标准交叉注意力、2D自注意力和统一的3D自注意力；并提出掩码3D注意力机制来限制注意力模式，强制时间对齐。&lt;h4&gt;主要发现&lt;/h4&gt;3D注意力在捕捉时空音频视觉相关性方面具有最高潜力，但存在显著的训练挑战；掩码3D注意力机制能够实现稳定训练和卓越性能。&lt;h4&gt;结论&lt;/h4&gt;基于音频流和参考图像的条件，该方法实现了良好的口型同步和视频质量。&lt;h4&gt;翻译&lt;/h4&gt;最近的视频生成进展已经转向统一的、基于Transformer的基础模型，这些模型可以在上下文中处理多种条件输入。然而，这些模型主要关注文本、图像和深度图等模态，而音频等严格时间同步信号研究不足。本文介绍了视频扩散变压器的上下文音频控制（ICAC），这是一个研究在统一的完全注意力架构中集成音频信号用于语音驱动视频生成的框架。我们系统探索了三种不同的音频条件注入机制：标准交叉注意力、2D自注意力和统一的3D自注意力。我们的研究揭示，虽然3D注意力在捕捉时空音频视觉相关性方面具有最高潜力，但它带来了显著的训练挑战。为克服这一问题，我们提出了掩码3D注意力机制，通过限制注意力模式来强制时间对齐，从而实现稳定训练和卓越性能。我们的实验证明，这种方法在音频流和参考图像的条件下实现了良好的口型同步和视频质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in video generation have seen a shift towards unified, transformer-based foundation models that can handle multiple conditional inputs in-context. However, these models have primarily focused on modalities like text, images, and depth maps, while strictly time-synchronous signals like audio have been underexplored. This paper introduces In-Context Audio Control of video diffusion transformers (ICAC), a framework that investigates the integration of audio signals for speech-driven video generation within a unified full-attention architecture, akin to FullDiT. We systematically explore three distinct mechanisms for injecting audio conditions: standard cross-attention, 2D self-attention, and unified 3D self-attention. Our findings reveal that while 3D attention offers the highest potential for capturing spatio-temporal audio-visual correlations, it presents significant training challenges. To overcome this, we propose a Masked 3D Attention mechanism that constrains the attention pattern to enforce temporal alignment, enabling stable training and superior performance. Our experiments demonstrate that this approach achieves strong lip synchronization and video quality, conditioned on an audio stream and reference images.</description>
      <author>example@mail.com (Wenze Liu, Weicai Ye, Minghong Cai, Quande Liu, Xintao Wang, Xiangyu Yue)</author>
      <guid isPermaLink="false">2512.18772v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</title>
      <link>http://arxiv.org/abs/2512.18684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI 20206, Project website: geovit-aaai26.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用视觉Transformer学习多视图几何任务（如光流估计），通过微调视频基础模型。研究发现通用视频预训练模型可简单迁移到多视图问题，只需少量调整。通过在Transformer骨干上添加线性解码器和迭代细化，实现了最先进的性能，并在多种几何视觉任务中展示了强大的多功能性。&lt;h4&gt;背景&lt;/h4&gt;先前的方法涉及自定义架构设计和特定任务预训练，而本研究采用了一种更简单的方法，利用通用视频预训练模型。这种新方法避免了复杂的定制设计，而是利用了模型在预训练过程中已经学习到的时间和空间信息。&lt;h4&gt;目的&lt;/h4&gt;研究旨在验证通用视频预训练模型在多视图几何任务（如光流估计、3D深度估计和立体匹配）中的有效性，并探索通过简单调整实现最先进性能的可能性。&lt;h4&gt;方法&lt;/h4&gt;通过微调视频基础模型，在Transformer骨干网络上添加线性解码器，并采用迭代细化技术来提升性能。这种方法利用了补丁之间通用注意力机制学习的时间和空间信息，用于几何推理。&lt;h4&gt;主要发现&lt;/h4&gt;1. 通用视频预训练模型可以很容易地迁移到多视图问题，只需少量调整2. 在Transformer骨干网络上添加线性解码器可以产生满意的结果3. 迭代细化可以将性能提升到最先进的水平4. 在光流估计任务上实现了跨数据集的最先进泛化结果5. 在3D深度估计和立体匹配中也显示出强大性能&lt;h4&gt;结论&lt;/h4&gt;视频预训练模型在解决几何视觉任务中具有多功能性和有效性。这种概念上简单的方法通过利用预训练模型中已经存在的时间和空间信息，避免了复杂的定制设计，实现了最先进的性能，为多视图几何任务提供了一种新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了通过微调视频基础模型，将视觉Transformer学习应用于多视图几何任务（如光流估计）。与涉及自定义架构设计和特定任务预训练的先前方法不同，我们的研究发现通用视频预训练模型可以很容易地迁移到多视图问题，只需少量调整。核心见解是补丁之间的通用注意力机制学习了用于几何推理的时间和空间信息。我们证明，在Transformer骨干网络上添加线性解码器可以产生满意的结果，而迭代细化可以将性能提升到最先进的水平。这种概念上简单的方法在光流估计上实现了跨数据集的最先进泛化结果，在Sintel clean、Sintel final和KITTI数据集上分别达到了0.69、1.78和3.15的端点误差(EPE)。我们的方法还在在线测试基准上创造了新记录，EPE值分别为0.79和1.88，F1值为3.79。在3D深度估计和立体匹配中的应用也显示出强大性能，说明了视频预训练模型在解决几何视觉任务中的多功能性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要研究如何通过微调视频基础模型来解决多视图几何任务（如光流估计、立体匹配和深度估计）。这个问题在研究中很重要，因为目前大多数几何任务需要复杂的手工设计和任务特定的预训练策略；在现实中，这些任务是计算机视觉的基础，广泛应用于自动驾驶、机器人导航、增强现实和三维重建等领域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现视频transformer中的跨帧注意力机制能学习时空信息，可用于几何推理。他们首先尝试在预训练视频transformer上添加简单线性解码器，然后引入迭代细化机制进一步提高性能。作者借鉴了视频基础模型的预训练表示、RAFT的迭代细化思想，但创新性地用图像扭曲代替了成本体积查询。他们还调整了预训练3D ViT的位置编码，使其能处理两帧任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是视频预训练transformer通过自注意力学习到的特征包含丰富时空信息，可有效迁移到多视图几何任务。实现流程包括：1)调整预训练视频模型的位置编码使其适应两帧输入；2)添加线性解码器直接回归几何属性；3)可选的迭代细化机制，通过图像扭曲和残差预测逐步改进结果。整个过程无需复杂架构设计，只需微调预训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出简单通用框架，只需微调预训练视频transformer；2)创新性地调整位置编码适应两帧任务；3)设计基于图像扭曲而非成本体积的迭代细化机制；4)统一框架处理多种几何任务；5)展示强大的跨数据集泛化能力。相比之前工作，本文方法无需复杂架构设计和任务特定预训练，比CroCo等更通用，比RAFT等更适合全局上下文建模，比FlowFormer++等性能更优但预训练更简单。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文证明了通过简单微调通用视频预训练transformer模型，可以有效地解决多种多视图几何任务并取得最先进性能，为视频预训练模型在几何视觉任务中的应用开辟了新途径。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.</description>
      <author>example@mail.com (Huimin Wu, Kwang-Ting Cheng, Stephen Lin, Zhirong Wu)</author>
      <guid isPermaLink="false">2512.18684v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>brat: Aligned Multi-View Embeddings for Brain MRI Analysis</title>
      <link>http://arxiv.org/abs/2512.18679v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First round accept at WACV 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;brat是一个针对脑部MRI的多视角表示学习框架，使用与临床报告配对的MRI数据进行训练&lt;h4&gt;背景&lt;/h4&gt;脑部MRI面临独特挑战，因为存在大量、高度多样化且通常微妙的异常，这些异常往往仅分布在3D体积的少数切片中&lt;h4&gt;目的&lt;/h4&gt;解决脑部MRI分析中的挑战，开发一个能够有效处理这些异常的多视角表示学习框架&lt;h4&gt;方法&lt;/h4&gt;引入了一个比现有数据集大10倍的脑部MRI数据集(约80,000个3D扫描及其相应的放射学报告)，提出受文档检索启发的多视角预训练方法，开发隐式查询-特征匹配机制，采用质量多样性概念获得与临床特征对齐的MRI多视角嵌入&lt;h4&gt;主要发现&lt;/h4&gt;在多个视觉语言和视觉任务上评估，显示出显著的性能提升&lt;h4&gt;结论&lt;/h4&gt;brat基础模型已成功开发并公开发布，能够有效处理脑部MRI分析中的挑战&lt;h4&gt;翻译&lt;/h4&gt;我们提出了brat(脑部报告对齐变换器)，这是一个针对脑部磁共振成像(MRI)的多视角表示学习框架，使用与临床报告配对的MRI数据进行训练。由于脑部MRI中存在大量、高度多样化且通常微妙的异常，这些异常往往仅分布在3D体积的少数切片中，因此脑部MRI面临独特挑战。为应对这些挑战，我们引入了一个比现有数据集大10倍的脑部MRI数据集，包含约80,000个3D扫描及其相应的放射学报告，并提出了一种受文档检索进展启发的多视角预训练方法。我们开发了一种隐式查询-特征匹配机制，并采用质量多样性概念，获得与报告句子提供的临床特征对齐的MRI多视角嵌入。我们在多个视觉语言和视觉任务上评估了我们的方法，显示出显著的性能提升。brat基础模型已公开发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决脑部MRI分析中的挑战：3D脑部MRI图像中存在众多、高度变化且通常微妙的异常，这些异常往往仅限于3D体积中的几个切片。这个问题很重要，因为脑部MRI是诊断脑部疾病的标准方法，但开发有效的AI模型面临数据稀缺和现有方法无法充分利用临床报告中丰富信息的问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了文档检索中的多向量检索方法(如ColBERT)，将脑部MRI视为'文档'，报告句子视为'查询'。他们还参考了BLIP-2中的Q-Former架构，使用可学习的查询令牌作为潜在变量。结合这些思想，作者设计了brat框架，通过成对视图对齐(PVA)算法将多视图嵌入与临床特征对齐，并使用行列式点过程(DPPs)增强嵌入多样性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用多视图嵌入表示脑部MRI中的多样化信息，将报告分解为句子嵌入，通过对比学习将这些子单元与MRI对齐。流程包括：1)处理MSKBrain数据集；2)从MRI和报告中提取特征；3)生成多视图嵌入；4)使用PVA算法对齐嵌入；5)通过DPPs优化多样性；6)计算对比损失和DPP损失进行训练；7)将模型应用于下游任务如报告生成和肿瘤分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引入MSKBrain数据集(比现有数据集大10倍)；2)提出多视图嵌入方法，专门针对3D医学图像和长临床报告；3)开发成对视图对齐(PVA)算法；4)首次将行列式点过程(DPPs)应用于多视图表示学习。相比之前工作，brat专门处理3D医学图像和长报告，使用多视图而非单一嵌入，结合了对齐和多样性优化，并在大规模数据集上预训练并公开模型权重。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入大规模脑部MRI数据集、多视图表示学习框架和质量-多样性优化方法，显著提升了脑部MRI分析在图像-文本检索、报告生成和疾病分类等任务上的性能，为医学影像分析提供了新的基础模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present brat (brain report alignment transformer), a multi-view representation learning framework for brain magnetic resonance imaging (MRI) trained on MRIs paired with clinical reports. Brain MRIs present unique challenges due to the presence of numerous, highly varied, and often subtle abnormalities that are localized to a few slices within a 3D volume. To address these challenges, we introduce a brain MRI dataset $10\times$ larger than existing ones, containing approximately 80,000 3D scans with corresponding radiology reports, and propose a multi-view pre-training approach inspired by advances in document retrieval. We develop an implicit query-feature matching mechanism and adopt concepts from quality-diversity to obtain multi-view embeddings of MRIs that are aligned with the clinical features given by report sentences. We evaluate our approach across multiple vision-language and vision tasks, demonstrating substantial performance improvements. The brat foundation models are publicly released.</description>
      <author>example@mail.com (Maxime Kayser, Maksim Gridnev, Wanting Wang, Max Bain, Aneesh Rangnekar, Avijit Chatterjee, Aleksandr Petrov, Harini Veeraraghavan, Nathaniel C. Swinburne)</author>
      <guid isPermaLink="false">2512.18679v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</title>
      <link>http://arxiv.org/abs/2512.18406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于SAM 2模型的马赛克图像分割方法，通过微调模型和创建专门的数据集，显著提高了马赛克图像中小块的分割准确度，为文化遗产的数字化保存提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;艺术被广泛视为文明的反映，马赛克作为文化遗产的重要组成部分，是由小块通过粘合剂在表面排列组成的古老艺术形式。由于其年代久远和易碎性，马赛克容易受损，因此需要进行数字化保存。&lt;h4&gt;目的&lt;/h4&gt;解决马赛克数字化过程中的图像分割问题，即将马赛克中的小块从背景中分离出来，实现文化遗产的有效数字化保存。&lt;h4&gt;方法&lt;/h4&gt;采用Meta AI的Segment Anything Model 2基础模型，并创建了一个专门标注的马赛克图像数据集用于微调和评估模型。&lt;h4&gt;主要发现&lt;/h4&gt;与基线SAM 2模型相比，微调后的模型性能显著提升：交并比从89.00%提高到91.02%，召回率从92.12%提高到95.89%。在先前方法提出的基准测试上，模型的F-measure比先前方法高3%，预测与实际小块之间的绝对误差从0.20降低到0.02。&lt;h4&gt;结论&lt;/h4&gt;微调后的SAM 2模型与新创建的标注数据集相结合，为马赛克图像的实时分割铺平了道路，对文化遗产的数字化保存具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;艺术被广泛视为文明的反映，马赛克代表文化遗产的重要组成部分。马赛克是一种古老的艺术形式，通过使用粘合剂将称为小块的小块在表面上排列而创造。由于其年代久远和易碎性，它们容易受损，突显了数字化保存的必要性。本文通过在计算机视觉的图像分割领域中分割小块以将它们与背景分离，解决了马赛克数字化的问题。我们提出了一种利用Meta AI的Segment Anything Model 2的方法，这是一个基础模型，其性能优于大多数传统分割模型，可以自动分割马赛克。由于该领域开放数据集有限，我们还创建了一个标注的马赛克图像数据集，用于微调和评估模型。在我们的测试数据集上的定量评估显示，与基线SAM 2模型相比有显著改进，交并比从89.00%提高到91.02%，召回率从92.12%提高到95.89%。此外，在先前方法提出的基准测试上，我们的模型的F-measure比先前方法高3%，并将预测小块与实际小块之间的绝对误差从0.20降低到仅0.02。微调后的SAM 2模型的显著性能与新创建的标注数据集一起，可以为马赛克图像的实时分割铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IST66504.2025.11268445&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Art is widely recognized as a reflection of civilization and mosaics represent an important part of cultural heritage. Mosaics are an ancient art form created by arranging small pieces, called tesserae, on a surface using adhesive. Due to their age and fragility, they are prone to damage, highlighting the need for digital preservation. This paper addresses the problem of digitizing mosaics by segmenting the tesserae to separate them from the background within the broader field of Image Segmentation in Computer Vision. We propose a method leveraging Segment Anything Model 2 (SAM 2) by Meta AI, a foundation model that outperforms most conventional segmentation models, to automatically segment mosaics. Due to the limited open datasets in the field, we also create an annotated dataset of mosaic images to fine-tune and evaluate the model. Quantitative evaluation on our testing dataset shows notable improvements compared to the baseline SAM 2 model, with Intersection over Union increasing from 89.00% to 91.02% and Recall from 92.12% to 95.89%. Additionally, on a benchmark proposed by a prior approach, our model achieves an F-measure 3% higher than previous methods and reduces the error in the absolute difference between predicted and actual tesserae from 0.20 to just 0.02. The notable performance of the fine-tuned SAM 2 model together with the newly annotated dataset can pave the way for real-time segmentation of mosaic images.</description>
      <author>example@mail.com (Charilaos Kapelonis, Marios Antonakakis, Konstantinos Politof, Aristomenis Antoniadis, Michalis Zervakis)</author>
      <guid isPermaLink="false">2512.18406v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale</title>
      <link>http://arxiv.org/abs/2512.18373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Master's Thesis at the University of Pennsylvania&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了神经网络优化在现代AI研究中的重要性，研究了从经典一阶方法到现代高阶技术的优化算法演变，展示了如何通过有原则的算法设计来阐明训练过程，并提供了将这些方法整合到现代深度学习工作流中的实用建议。&lt;h4&gt;背景&lt;/h4&gt;神经网络优化是现代AI研究中最重要但理解最不充分的挑战之一。尽管随机梯度下降(SGD)及其变体已成为训练深度网络的标准方法，但在过度参数化情况下，它们的成功似乎更多是经验性的而非基于原则的。&lt;h4&gt;目的&lt;/h4&gt;研究SGD等传统优化方法在训练深度网络时表现出的'经验性成功多于原则性'这一悖论，通过追踪优化算法的演变，揭示如何通过有原则的算法设计来阐明训练过程，并弥合理论理解与实际部署之间的差距。&lt;h4&gt;方法&lt;/h4&gt;从基本原理开始分析SGD和自适应梯度方法，逐步揭示这些传统方法在面对真实世界数据各向异性时的局限性，进而探索基于曲率信息的复杂替代方法，如二阶近似技术、逐层预调节和自适应学习率等，并研究这些优化算法与神经网络训练工具包中其他元素的相互作用。&lt;h4&gt;主要发现&lt;/h4&gt;传统优化方法在面对真实世界数据的各向异性时存在局限性；基于曲率信息的复杂替代方法(如二阶近似技术、逐层预调节、自适应学习率等)可能更有效；优化算法与神经网络训练工具包中其他元素(如最大更新参数化、学习率计划和指数移动平均等)的相互作用对实证成功同样重要。&lt;h4&gt;结论&lt;/h4&gt;通过有原则的算法设计可以更好地理解和改进神经网络训练过程；将高级优化技术与训练工具包中的其他方法结合使用可以显著提高模型性能；提供实用的建议和实施策略有助于弥合理论理解与实际部署之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;神经网络优化仍然是现代AI研究中最重要但理解最不充分的挑战之一，其中训练算法的改进可以导致基础模型中特征学习的增强，训练时间的数量级减少，以及对网络学习方式的更好解释。虽然随机梯度下降(SGD)及其变体已成为训练深度网络的事实标准，但它们在这些过度参数化情况下的成功往往看起来更多是经验性的而非基于原则的。本文通过追踪优化算法从经典一阶方法到现代高阶技术的演变来研究这一明显悖论，展示如何通过有原则的算法设计来阐明训练过程。从SGD和自适应梯度方法的基本原理开始，分析逐步揭示了这些传统方法在面对代表真实世界数据的各向异性时的局限性。这些局限性促使探索基于曲率信息的复杂替代方法：二阶近似技术、逐层预调节、自适应学习率等。接下来，这些优化算法与更广泛的神经网络训练工具包之间的相互作用(包括最大更新参数化、学习率计划和指数移动平均等近期发展)被证明对实证成功同样重要。为了弥合理论理解与实际部署之间的差距，本文提供了将这些方法整合到现代深度学习工作流中的实用建议和实施策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural network optimization remains one of the most consequential yet poorly understood challenges in modern AI research, where improvements in training algorithms can lead to enhanced feature learning in foundation models, order-of-magnitude reductions in training time, and improved interpretability into how networks learn. While stochastic gradient descent (SGD) and its variants have become the de facto standard for training deep networks, their success in these over-parameterized regimes often appears more empirical than principled. This thesis investigates this apparent paradox by tracing the evolution of optimization algorithms from classical first-order methods to modern higher-order techniques, revealing how principled algorithmic design can demystify the training process. Starting from first principles with SGD and adaptive gradient methods, the analysis progressively uncovers the limitations of these conventional approaches when confronted with anisotropy that is representative of real-world data. These breakdowns motivate the exploration of sophisticated alternatives rooted in curvature information: second-order approximation techniques, layer-wise preconditioning, adaptive learning rates, and more. Next, the interplay between these optimization algorithms and the broader neural network training toolkit, which includes prior and recent developments such as maximal update parametrization, learning rate schedules, and exponential moving averages, emerges as equally essential to empirical success. To bridge the gap between theoretical understanding and practical deployment, this paper offers practical prescriptions and implementation strategies for integrating these methods into modern deep learning workflows.</description>
      <author>example@mail.com (Ansh Nagwekar)</author>
      <guid isPermaLink="false">2512.18373v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition</title>
      <link>http://arxiv.org/abs/2512.18263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at IEEE ASRU 2025 Satellite Workshop-AI for Children's Speech and Language&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种改进的儿童语音识别方法TICL+，通过结合语义和声学信息，显著提高了儿童语音识别的准确性。&lt;h4&gt;背景&lt;/h4&gt;儿童语音识别面临显著挑战，包括声学和语言变异性大、标记数据有限，以及与成人语音存在显著差异。&lt;h4&gt;目的&lt;/h4&gt;提高语音基础模型在上下文学习中的效果，特别是优化上下文示例的选择方式，以提升儿童语音识别性能。&lt;h4&gt;方法&lt;/h4&gt;扩展了现有的基于检索的TICL方法，引入声学重排序步骤创建TICL+，优先选择在语义和声学上都与测试输入对齐的示例。&lt;h4&gt;主要发现&lt;/h4&gt;在四个儿童语音语料库实验中，TICL+相比零样本性能实现了高达53.3%的相对词错误率降低，相比基线TICL实现了37.6%的相对词错误率降低。&lt;h4&gt;结论&lt;/h4&gt;结合语义和声学信息对于开发鲁棒、可扩展的儿童语音自动识别系统具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;儿童语音识别由于显著的声学和语言变异性、有限的标记数据以及与成人语音的显著差异而仍然具有挑战性。语音基础模型可以通过语音上下文学习（SICL）解决这些挑战，允许无需微调即可适应新领域。然而，SICL的有效性取决于如何选择上下文示例。我们扩展了一个现有的基于检索的方法，用于SICL的文本嵌入KNN（TICL），引入了一个声学重排序步骤来创建TICL+。这种扩展优先选择在语义和声学上都与测试输入对齐的示例。在四个儿童语音语料库上的实验表明，TICL+相比零样本性能实现了高达53.3%的相对词错误率降低，相比基线TICL实现了37.6%的降低，突显了结合语义和声学信息对于儿童语音鲁棒、可扩展ASR的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Children's speech recognition remains challenging due to substantial acoustic and linguistic variability, limited labeled data, and significant differences from adult speech. Speech foundation models can address these challenges through Speech In-Context Learning (SICL), allowing adaptation to new domains without fine-tuning. However, the effectiveness of SICL depends on how in-context examples are selected. We extend an existing retrieval-based method, Text-Embedding KNN for SICL (TICL), introducing an acoustic reranking step to create TICL+. This extension prioritizes examples that are both semantically and acoustically aligned with the test input. Experiments on four children's speech corpora show that TICL+ achieves up to a 53.3% relative word error rate reduction over zero-shot performance and 37.6% over baseline TICL, highlighting the value of combining semantic and acoustic information for robust, scalable ASR in children's speech.</description>
      <author>example@mail.com (Haolong Zheng, Yekaterina Yegorova, Mark Hasegawa-Johnson)</author>
      <guid isPermaLink="false">2512.18263v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>SAM Audio: Segment Anything in Audio</title>
      <link>http://arxiv.org/abs/2512.18099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAM Audio是一个基础模型，用于通用音频分离，统一了文本、视觉和时间跨度提示，在多种音频分离任务中取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;通用音频源分离是能够感知和推理声音的多模态AI系统的关键能力。尽管近年来取得进展，但现有分离模型要么是领域特定的（针对语音或音乐等固定类别），要么可控性有限（仅支持文本提示等单一提示模态）。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为SAM Audio的基础模型，用于通用音频分离，该模型在单一框架内统一了文本、视觉和时间跨度提示。&lt;h4&gt;方法&lt;/h4&gt;SAM Audio基于扩散Transformer架构，使用流匹配技术在大规模音频数据（涵盖语音、音乐和一般声音）上进行训练，可以灵活地分离由语言、视觉掩码或时间跨度描述的目标源。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在多样化的基准测试中取得了最先进的性能，包括在自然和专业制作的音频中进行一般声音、语音、音乐和乐器分离，显著优于之前的通用和专业系统。此外，研究者还引入了一个具有人工标注多模态提示的新现实世界分离基准，以及一个与人类判断高度相关的无参考评估模型。&lt;h4&gt;结论&lt;/h4&gt;SAM Audio通过统一多种提示模态，实现了更灵活和强大的音频分离能力，在多个领域都表现出色，为多模态AI系统提供了重要的音频感知能力。&lt;h4&gt;翻译&lt;/h4&gt;通用音频源分离是能够感知和推理声音的多模态AI系统的关键能力。尽管近年来取得了重大进展，但现有的分离模型要么是领域特定的，设计用于语音或音乐等固定类别，要么可控性有限，仅支持文本等单一提示模态。在这项工作中，我们提出了SAM Audio，这是一个用于通用音频分离的基础模型，在单一框架内统一了文本、视觉和时间跨度提示。SAM Audio基于扩散Transformer架构，使用流匹配技术在涵盖语音、音乐和一般声音的大规模音频数据上进行训练，可以灵活地分离由语言、视觉掩码或时间跨度描述的目标源。该模型在多样化的基准测试中取得了最先进的性能，包括在自然和专业制作的音频中进行一般声音、语音、音乐和乐器分离，显著优于之前的通用和专业系统。此外，我们还引入了一个具有人工标注多模态提示的新现实世界分离基准，以及一个与人类判断高度相关的无参考评估模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.</description>
      <author>example@mail.com (Bowen Shi, Andros Tjandra, John Hoffman, Helin Wang, Yi-Chiao Wu, Luya Gao, Julius Richter, Matt Le, Apoorv Vyas, Sanyuan Chen, Christoph Feichtenhofer, Piotr Dollár, Wei-Ning Hsu, Ann Lee)</author>
      <guid isPermaLink="false">2512.18099v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis</title>
      <link>http://arxiv.org/abs/2512.18073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究建立了首个用于评估多模态大语言模型在指纹理解领域能力的全面基准测试FPBench。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型已在复杂数据分析、视觉问答、生成和推理方面取得显著进展，并被用于分析虹膜和人脸图像的生物测量效用，但在指纹理解方面的能力尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;设计一个全面的基准测试FPBench，用于评估多模态大语言模型在指纹理解方面的性能。&lt;h4&gt;方法&lt;/h4&gt;创建了名为FPBench的基准测试，评估了20个开源和专有的多模态大语言模型，在7个真实和合成数据集上执行8个生物识别和法医任务，采用零样本和思维链提示策略。&lt;h4&gt;主要发现&lt;/h4&gt;从性能和可解释性角度讨论了研究发现，并分享了关于挑战和局限性的见解。&lt;h4&gt;结论&lt;/h4&gt;FPBench作为首个用于指纹领域理解的多模态大语言模型全面基准，为指纹基础模型的发展铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在复杂数据分析、视觉问答、生成和推理方面已获得显著关注。最近，它们被用于分析虹膜和人脸图像的生物测量效用。然而，它们在指纹理解方面的能力尚未被探索。在这项工作中，我们设计了一个名为FPBench的全面基准，该基准使用零样本和思维链提示策略，在7个真实和合成数据集上，通过8个生物识别和法医任务，评估了20个多模态大语言模型（开源和专有）的性能。我们从性能和可解释性角度讨论了研究结果，并分享了关于挑战和局限性的见解。我们建立了FPBench作为首个用于指纹领域理解的多模态大语言模型全面基准，为指纹基础模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal LLMs (MLLMs) have gained significant traction in complex data analysis, visual question answering, generation, and reasoning. Recently, they have been used for analyzing the biometric utility of iris and face images. However, their capabilities in fingerprint understanding are yet unexplored. In this work, we design a comprehensive benchmark, \textsc{FPBench} that evaluates the performance of 20 MLLMs (open-source and proprietary) across 7 real and synthetic datasets on 8 biometric and forensic tasks using zero-shot and chain-of-thought prompting strategies. We discuss our findings in terms of performance, explainability and share our insights into the challenges and limitations. We establish \textsc{FPBench} as the first comprehensive benchmark for fingerprint domain understanding with MLLMs paving the path for foundation models for fingerprints.</description>
      <author>example@mail.com (Ekta Balkrishna Gavas, Sudipta Banerjee, Chinmay Hegde, Nasir Memon)</author>
      <guid isPermaLink="false">2512.18073v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients</title>
      <link>http://arxiv.org/abs/2512.18031v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了三种AI方法在心房颤动检测中的表现，结果表明ECG基础模型表现最佳，为ICU患者心房颤动的自动监测提供了新思路。&lt;h4&gt;背景&lt;/h4&gt;心房颤动是重症监护室患者最常见的心律失常，可能导致不良健康影响。目前缺乏针对ICU患者心房颤动检测的专门数据集和性能基准。&lt;h4&gt;目的&lt;/h4&gt;发布一个标记的ICU数据集和心房颤动检测的基准，比较机器学习模型在三种基于数据的人工智能方法中的表现，确定最适合心房颤动检测的AI方法。&lt;h4&gt;方法&lt;/h4&gt;比较三种AI方法：基于特征的分类器、深度学习和心电图基础模型。使用加拿大ICU的心电图和2021年PhysioNet/计算心脏病学挑战赛的数据进行实验，测试多种训练配置，从零样本推理到迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;平均而言，在两个数据集上，ECG基础模型表现最好，其次是深度学习，最后是基于特征的分类器。通过迁移学习策略的ECG-FM模型在ICU测试集上获得了最高的F1分数(0.89)。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了使用人工智能构建自动患者监测系统的巨大潜力，通过发布标记的ICU数据集和性能基准，为研究社区继续推进IC中心房颤动检测的最新技术提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;目的：心房颤动是重症监护室患者最常见的心律失常，可能导致不良健康影响。在本研究中，我们发布了一个标记的ICU数据集和心房颤动检测的基准。方法：我们比较了三种基于数据的人工智能方法的机器学习模型：基于特征的分类器、深度学习和心电图基础模型。这种比较解决了文献中的一个关键空白，旨在确定哪种AI方法最适合心房颤动检测。使用加拿大ICU的心电图和2021年PhysioNet/计算心脏病学挑战赛的数据进行实验。测试了多种训练配置，从零样本推理到迁移学习。结果：平均而言，在两个数据集上，ECG基础模型表现最好，其次是深度学习，然后是基于特征的分类器。在我们的ICU测试集上获得最高F1分数的模型是通过迁移学习策略的ECG-FM(F1=0.89)。结论：这项研究展示了使用人工智能构建自动患者监测系统的巨大潜力。意义：通过发布我们的标记ICU数据集(链接待添加)和性能基准，这项工作使研究社区能够继续推进IC中心房颤动检测的最新技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Objective: Atrial fibrillation (AF) is the most common cardiac arrhythmia experienced by intensive care unit (ICU) patients and can cause adverse health effects. In this study, we publish a labelled ICU dataset and benchmarks for AF detection. Methods: We compared machine learning models across three data-driven artificial intelligence (AI) approaches: feature-based classifiers, deep learning (DL), and ECG foundation models (FMs). This comparison addresses a critical gap in the literature and aims to pinpoint which AI approach is best for accurate AF detection. Electrocardiograms (ECGs) from a Canadian ICU and the 2021 PhysioNet/Computing in Cardiology Challenge were used to conduct the experiments. Multiple training configurations were tested, ranging from zero-shot inference to transfer learning. Results: On average and across both datasets, ECG FMs performed best, followed by DL, then feature-based classifiers. The model that achieved the top F1 score on our ICU test set was ECG-FM through a transfer learning strategy (F1=0.89). Conclusion: This study demonstrates promising potential for using AI to build an automatic patient monitoring system. Significance: By publishing our labelled ICU dataset (LinkToBeAdded) and performance benchmarks, this work enables the research community to continue advancing the state-of-the-art in AF detection in the ICU.</description>
      <author>example@mail.com (Sarah Nassar, Nooshin Maghsoodi, Sophia Mannina, Shamel Addas, Stephanie Sibley, Gabor Fichtinger, David Pichora, David Maslove, Purang Abolmaesumi, Parvin Mousavi)</author>
      <guid isPermaLink="false">2512.18031v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation</title>
      <link>http://arxiv.org/abs/2512.18412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于结构图的方法，用于在小样本情况下分类轮廓图像，无需使用反向传播。核心思想是将结构作为解释的载体：图像被编码为属性图，关键点和线作为带有几何属性的节点，通过形成概念吸引子实现泛化。&lt;h4&gt;背景&lt;/h4&gt;在小样本学习领域，需要一种能够从少量样本中学习并做出透明决策的方法，同时避免使用反向传播。&lt;h4&gt;目的&lt;/h4&gt;设计和实验验证一种架构，其中类概念通过结构和参数缩减从少量示例（每类5-6个）形成，提供透明的决策并消除反向传播的需要。&lt;h4&gt;方法&lt;/h4&gt;轮廓矢量化后构建二部图（点/线作为节点），具有标准化的几何属性，如坐标、长度、角度和方向；缩减包括消除不稳定子结构或噪声以及对齐关键点之间的路径。概念通过样本的迭代组合形成，分类通过选择最佳图到概念匹配（使用近似GED）执行。&lt;h4&gt;主要发现&lt;/h4&gt;在MNIST子集上（每类5-6个基础示例，单轮训练），获得约82%的一致准确率，且决策具有完全可追溯性：误分类可以通过明确的结构相似性来解释。提供了与SVM、MLP、CNN以及度量和元学习基线的比较。&lt;h4&gt;结论&lt;/h4&gt;带有概念吸引子的结构图方案能够在没有反向传播的情况下实现小样本学习，并通过显式图结构提供内置解释。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种结构图方法，用于在小样本情况下分类轮廓图像，无需使用反向传播。核心思想是使结构成为解释的载体：图像被编码为属性图（关键点和线表示为带有几何属性的节点），并通过形成概念吸引子（类级概念图）来实现泛化。目的：设计和实验验证一种架构，其中类概念通过结构和参数缩减从少量示例（每类5-6个）形成，提供透明的决策并消除反向传播。方法：轮廓矢量化后构建二部图（点/线作为节点），具有标准化的几何属性，如坐标、长度、角度和方向；缩减包括消除不稳定子结构或噪声以及对齐关键点之间的路径。概念通过样本的迭代组合形成，分类通过选择最佳图到概念匹配（使用近似GED）执行。结果：在MNIST子集上（每类5-6个基础示例，单轮训练），获得约82%的一致准确率，且决策具有完全可追溯性：误分类可以通过明确的结构相似性来解释。提供了与SVM、MLP、CNN以及度量和元学习基线的比较。带有概念吸引子的结构图方案能够在没有反向传播的情况下实现小样本学习，并通过显式图结构提供内置解释。局限性涉及GED的计算成本和骨架化的质量；有前途的方向包括分类算法优化、静态场景工作和联想识别。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a structural-graph approach to classifying contour images in a few-shot regime without using backpropagation. The core idea is to make structure the carrier of explanations: an image is encoded as an attributed graph (critical points and lines represented as nodes with geometric attributes), and generalization is achieved via the formation of concept attractors (class-level concept graphs). Purpose. To design and experimentally validate an architecture in which class concepts are formed from a handful of examples (5 - 6 per class) through structural and parametric reductions, providing transparent decisions and eliminating backpropagation. Methods. Contour vectorization is followed by constructing a bipartite graph (Point/Line as nodes) with normalized geometric attributes such as coordinates, length, angle, and direction; reductions include the elimination of unstable substructures or noise and the alignment of paths between critical points. Concepts are formed by iterative composition of samples, and classification is performed by selecting the best graph-to-concept match (using approximated GED). Results. On an MNIST subset with 5 - 6 base examples per class (single epoch), we obtain a consistent accuracy of around 82% with full traceability of decisions: misclassifications can be explained by explicit structural similarities. An indicative comparison with SVM, MLP, CNN, as well as metric and meta-learning baselines, is provided. The structural-graph scheme with concept attractors enables few-shot learning without backpropagation and offers built-in explanations through the explicit graph structure. Limitations concern the computational cost of GED and the quality of skeletonization; promising directions include classification-algorithm optimization, work with static scenes, and associative recognition.</description>
      <author>example@mail.com (Mykyta Lapin, Kostiantyn Bokhan, Yurii Parzhyn)</author>
      <guid isPermaLink="false">2512.18412v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Convolutional-neural-operator-based transfer learning for solving PDEs</title>
      <link>http://arxiv.org/abs/2512.17969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将卷积神经算子扩展到少样本学习场景的方法，通过预训练和参数调整策略，发现神经元线性变换在求解PDE时具有最高的代理精度。&lt;h4&gt;背景&lt;/h4&gt;卷积神经算子是一种基于CNN的架构，用于强制保持连续-离散等价性，并能够真实、无混叠地学习偏微分方程的解算子。该算子在某些情况下超越了DeepONet、傅里叶神经算子和Galerkin变换器等基线模型，但在少样本学习方面尚未得到验证。&lt;h4&gt;目的&lt;/h4&gt;将卷积神经算子扩展到少样本学习场景，通过先在源数据集上预训练，然后仅使用小的目标数据集调整已训练神经算子的参数。&lt;h4&gt;方法&lt;/h4&gt;研究三种调整已训练神经算子参数的策略：微调、低秩适应和神经元线性变换，并在Kuramoto-Sivashinsky方程、Brusselator扩散反应系统和Navier-Stokes方程等PDE上进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;在求解PDE时，神经元线性变换策略具有最高的代理精度，优于微调和低秩适应策略。&lt;h4&gt;结论&lt;/h4&gt;神经元线性变换策略在少样本学习场景下表现最佳，能够有效提高卷积神经算子在求解PDE时的代理精度。&lt;h4&gt;翻译&lt;/h4&gt;卷积神经算子是一种基于CNN的架构，最近被提出用于强制保持连续-离散等价性，并能够真实、无混叠地学习偏微分方程的解算子。该神经算子在某些情况下被证明在代理精度方面优于一些基线模型，如DeepONet、傅里叶神经算子和Galerkin变换器。然而，卷积神经算子似乎尚未在少样本学习中得到验证。我们通过首先使用源数据集预训练卷积神经算子，然后仅使用小的目标数据集调整已训练神经算子的参数，将模型扩展到少样本学习场景。我们研究了三种调整已训练神经算子参数的策略，包括微调、低秩适应和神经元线性变换，并发现神经元线性变换策略在求解Kuramoto-Sivashinsky方程、Brusselator扩散反应系统和Navier-Stokes方程等PDE时具有最高的代理精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Convolutional neural operator is a CNN-based architecture recently proposed to enforce structure-preserving continuous-discrete equivalence and enable the genuine, alias-free learning of solution operators of PDEs. This neural operator was demonstrated to outperform for certain cases some baseline models such as DeepONet, Fourier neural operator, and Galerkin transformer in terms of surrogate accuracy. The convolutional neural operator, however, seems not to be validated for few-shot learning. We extend the model to few-shot learning scenarios by first pre-training a convolutional neural operator using a source dataset and then adjusting the parameters of the trained neural operator using only a small target dataset. We investigate three strategies for adjusting the parameters of a trained neural operator, including fine-tuning, low-rank adaption, and neuron linear transformation, and find that the neuron linear transformation strategy enjoys the highest surrogate accuracy in solving PDEs such as Kuramoto-Sivashinsky equation, Brusselator diffusion-reaction system, and Navier-Stokes equations.</description>
      <author>example@mail.com (Peng Fan, Guofei Pang)</author>
      <guid isPermaLink="false">2512.17969v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach</title>
      <link>http://arxiv.org/abs/2512.17928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于梯度的元学习(GML)框架，用于解决STAR-RIS辅助通信系统中的联合优化问题。该框架通过将优化梯度直接输入轻量级神经网络，避免了预训练需求，实现了快速适应。针对独立相位和耦合相位STAR-RIS模型设计了专门的GML方案，在保持接近基准AO方法性能的同时，显著降低了计算复杂度，提高了可扩展性。&lt;h4&gt;背景&lt;/h4&gt;STAR-RIS作为一种新兴技术，有望在下一代无线网络中实现全空间覆盖并提高频谱效率。然而，基站预编码矩阵与STAR-RIS传输和反射系数矩阵的联合设计导致了一个高维度、强非凸且NP难的优化问题。传统的交替优化(AO)方案通常涉及重复的大规模矩阵求逆运算，计算复杂度高且可扩展性差，而现有的深度学习方法往往依赖昂贵的预训练和大型网络模型。&lt;h4&gt;目的&lt;/h4&gt;解决STAR-RIS辅助通信系统中的联合优化问题，降低计算复杂度，提高方法的可扩展性，同时保持接近基准AO方法的性能，避免现有方法中预训练和大型网络模型的依赖。&lt;h4&gt;方法&lt;/h4&gt;开发了一种基于梯度的元学习(GML)框架，直接将优化梯度输入轻量级神经网络，去除预训练需求并实现快速适应。针对独立相位和耦合相位STAR-RIS模型设计了专门的GML方案，有效处理各自的幅度和相位约束，实现接近AO基准的加权和速率性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 提出的GML方法在两种相位模型下都显著降低了计算开销；2. 计算复杂度随基站天线数和STAR-RIS单元数的增长呈近似线性增长；3. 相比AO方法实现了高达10倍的运行时间加速；4. 在保持接近AO方法性能的同时，验证了所提GML方法在大规模STAR-RIS辅助通信中的可扩展性和实用性。&lt;h4&gt;结论&lt;/h4&gt;基于梯度的元学习框架为STAR-RIS辅助通信系统提供了一种高效、可扩展的解决方案，能够有效处理复杂的联合优化问题，同时保持接近最优方法的性能，为大规模STAR-RIS技术的实际应用提供了可行途径。&lt;h4&gt;翻译&lt;/h4&gt;同时传输和反射可重构智能表面(STAR-RIS)已成为一项有前景的技术，可在下一代无线网络中实现全空间覆盖并提高频谱效率。然而，基站预编码矩阵以及STAR-RIS传输和反射系数矩阵的联合设计导致了一个高维度、强非凸且NP难的优化问题。传统的交替优化(AO)方案通常涉及重复的大规模矩阵求逆运算，导致高计算复杂性和 poor 可扩展性，而现有的深度学习方法往往依赖昂贵的预训练和大型网络模型。在本文中，我们开发了一种基于梯度的元学习(GML)框架，直接将优化梯度输入轻量级神经网络，从而无需预训练并实现快速适应。具体而言，我们为独立相位和耦合相位STAR-RIS模型设计了专门的GML方案，有效处理各自的幅度和相位约束，同时实现非常接近基于AO的基准的加权和速率性能。大量仿真表明，对于两种相位模型，所提出的方法显著降低了计算开销，当基站天线和STAR-RIS单元数量增加时，复杂度呈近似线性增长，相比AO方法实现了高达10倍的运行时间加速，这证实了所提GML方法在大规模STAR-RIS辅助通信中的可扩展性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged as a promising technology to realize full-space coverage and boost spectral efficiency in next-generation wireless networks. Yet, the joint design of the base station precoding matrix as well as the STAR-RIS transmission and reflection coefficient matrices leads to a high-dimensional, strongly nonconvex, and NP-hard optimization problem. Conventional alternating optimization (AO) schemes typically involve repeated large-scale matrix inversion operations, resulting in high computational complexity and poor scalability, while existing deep learning approaches often rely on expensive pre-training and large network models. In this paper, we develop a gradient-based meta learning (GML) framework that directly feeds optimization gradients into lightweight neural networks, thereby removing the need for pre-training and enabling fast adaptation. Specifically, we design dedicated GML-based schemes for both independent-phase and coupled-phase STAR-RIS models, effectively handling their respective amplitude and phase constraints while achieving weighted sum-rate performance very close to that of AO-based benchmarks. Extensive simulations demonstrate that, for both phase models, the proposed methods substantially reduce computational overhead, with complexity growing nearly linearly when the number of BS antennas and STAR-RIS elements grows, and yielding up to 10 times runtime speedup over AO, which confirms the scalability and practicality of the proposed GML method for large-scale STAR-RIS-assisted communications.</description>
      <author>example@mail.com (Dongdong Yang, Bin Li, Jiguang He, Yicheng Yan, Xiaoyu Zhang, Chongwen Huang)</author>
      <guid isPermaLink="false">2512.17928v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>ShibuyaSocial: Multi-scale Model of Pedestrian Flows in Scramble Crossing</title>
      <link>http://arxiv.org/abs/2512.18550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于学习的行人流模型，整合了多尺度行为（如全局路线选择和局部碰撞避免），特别关注涩谷十字路口的行人移动。&lt;h4&gt;背景&lt;/h4&gt;行人流过度拥挤可能导致严重事故，因此数学建模和预测行人行为对于预防事故和提供安全舒适的环境至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时考虑全局路线选择和局部碰撞避免的行人行为模型，以更准确地预测行人在城市空间中的行为。&lt;h4&gt;方法&lt;/h4&gt;提出一个整合局部行为和全局路线选择的模型，使用注意力机制确保全局和局部行为预测的一致性。使用涩谷十字路口的行人行走轨迹数据训练该模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过基于训练模型的行人行为模拟，定性和定量验证了所提出的模型能够适当预测行人行为。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型成功整合了行人的多尺度行为，能够更准确地预测行人在复杂环境中的行为，有助于提高行人环境的安全性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于学习的行人流模型，整合了城市空间中的多尺度行为，如全局路线选择和局部碰撞避免，特别关注涩谷十字路口的行人移动。由于行人流过度拥挤可能导致严重事故，数学建模和预测行人行为对于预防此类事故和提供安全舒适的环境非常重要。尽管许多研究已经调查了基于学习的建模方法，但它们大多只关注行人的局部行为，如与邻居和环境物体的碰撞避免。在实际环境中，行人行为涉及更复杂的决策，包括全局路线选择。此外，交通灯处从停止到行走的转变状态应同时考虑。在本研究中，所提出的模型使用注意力机制整合局部行为和全局路线选择，以确保全局和局部行为预测的一致性。我们录制了涩谷十字路口行人的视频数据，并使用从视频中获得的行人行走轨迹数据训练了所提出的模型。基于训练模型的行人行为模拟定性和定量地验证了所提出的模型能够适当预测行人行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a learning-based model of pedestrian flows that integrates multi scale behaviors such as global route selection and local collision avoidance in urban spaces, particularly focusing on pedestrian movements at Shibuya scramble crossing. Since too much congestion of pedestrian flows can cause serious accidents, mathematically modeling and predicting pedestrian behaviors is important for preventing such accidents and providing a safe and comfortable environment. Although numerous studies have investigated learning-based modeling methods, most of them focus only on the local behavior of pedestrians, such as collision avoidance with neighbors and environmental objects. In an actual environment, pedestrian behavior involves more complicated decision making including global route selection. Moreover, a state transition from stopping to walking at a traffic light should be considered simultaneously. In this study, the proposed model integrates local behaviors with global route selection, using an Attention mechanism to ensure consistent global and local behavior predictions. We recorded video data of pedestrians at Shibuya scramble crossing and trained the proposed model using pedestrian walking trajectory data obtained from the video. Simulations of pedestrian behaviors based on the trained model qualitatively and quantitatively validated that the proposed model can appropriately predict pedestrian behaviors.</description>
      <author>example@mail.com (Akihiro Sakurai, Naoya Kajio, Ko Yamamoto)</author>
      <guid isPermaLink="false">2512.18550v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</title>
      <link>http://arxiv.org/abs/2512.18211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LLaViDA是一种基于视觉语言模型的自动驾驶轨迹规划方法，通过两阶段训练流程实现了对复杂场景的更好理解和更准确的轨迹规划，在基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;轨迹规划是自动驾驶中的一个基础但具有挑战性的组成部分。端到端规划器在恶劣天气、不可预测的人类行为或复杂的道路布局下经常失败，主要是因为它们缺乏在训练数据之外的强泛化能力或少样本学习能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够更好地处理各种驾驶场景的轨迹规划方法，提高系统在复杂环境下的鲁棒性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了LLaViDA（大型语言视觉驾驶助手），利用视觉语言模型（VLM）进行目标运动预测、语义定位和思维链推理。采用两阶段训练流程：监督微调后接轨迹偏好优化（TPO），通过注入基于回归的监督增强场景理解和轨迹规划。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes基准测试中，LLaViDA在开环轨迹规划任务上超越了最先进的端到端和其他最新的VLM/LLM基线，在NuScenes测试集上实现了平均L2轨迹误差为0.31米，碰撞率为0.10%。&lt;h4&gt;结论&lt;/h4&gt;LLaViDA是一种强大的'VLM自动驾驶轨迹规划器'，能够有效处理各种复杂的驾驶场景，显著提高了自动驾驶系统的安全性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;轨迹规划是自动驾驶中的一个基础但具有挑战性的组成部分。端到端规划器在恶劣天气、不可预测的人类行为或复杂的道路布局下经常失败，主要是因为它们缺乏在训练数据之外的强泛化能力或少样本学习能力。我们提出了LLaViDA，这是一种大型语言视觉驾驶助手，它利用视觉语言模型（VLM）进行目标运动预测、语义定位和思维链推理，用于自动驾驶的轨迹规划。一个两阶段训练流程——监督微调后接轨迹偏好优化（TPO）——通过注入基于回归的监督，增强了场景理解和轨迹规划，产生了一个强大的'VLM自动驾驶轨迹规划器'。在NuScenes基准测试中，LLaViDA在开环轨迹规划任务上超越了最先进的端到端和其他最新的VLM/LLM基线，在NuScenes测试集上实现了平均L2轨迹误差为0.31米，碰撞率为0.10%。本文的代码可在GitHub上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory planning is a fundamental yet challenging component of autonomous driving. End-to-end planners frequently falter under adverse weather, unpredictable human behavior, or complex road layouts, primarily because they lack strong generalization or few-shot capabilities beyond their training data. We propose LLaViDA, a Large Language Vision Driving Assistant that leverages a Vision-Language Model (VLM) for object motion prediction, semantic grounding, and chain-of-thought reasoning for trajectory planning in autonomous driving. A two-stage training pipeline--supervised fine-tuning followed by Trajectory Preference Optimization (TPO)--enhances scene understanding and trajectory planning by injecting regression-based supervision, produces a powerful "VLM Trajectory Planner for Autonomous Driving." On the NuScenes benchmark, LLaViDA surpasses state-of-the-art end-to-end and other recent VLM/LLM-based baselines in open-loop trajectory planning task, achieving an average L2 trajectory error of 0.31 m and a collision rate of 0.10% on the NuScenes test set. The code for this paper is available at GitHub.</description>
      <author>example@mail.com (Yudong Liu, Spencer Hallyburton, Jiwoo Kim, Yueqian Lin, Yiming Li, Qinsi Wang, Hui Ye, Jingwei Sun, Miroslav Pajic, Yiran Chen, Hai Li)</author>
      <guid isPermaLink="false">2512.18211v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</title>
      <link>http://arxiv.org/abs/2512.17012v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://ca-joe-yang.github.io/resource/projects/4D_RGPT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对多模态大语言模型在3D结构和时间动态推理方面的局限性，提出了4D-RGPT模型、P4D训练框架和R4D-Bench基准测试，显著提升了模型在4D视频问答任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;尽管多模态大语言模型取得了进展，但它们对3D结构和时间动态的推理能力仍然有限，受限于弱的4D感知和时间理解能力。现有的3D和4D视频问答基准测试也强调静态场景，缺乏区域级提示。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在4D感知和时间理解方面的局限性，以及现有基准测试中缺乏区域级提示的问题。&lt;h4&gt;方法&lt;/h4&gt;引入4D-RGPT，一种专门设计的多模态大语言模型，能够从视频输入中捕获4D表示并增强时间感知；提出感知4D蒸馏训练框架，将冻结专家模型的4D表示转移到4D-RGPT中；构建R4D-Bench基准测试，这是一个具有区域级提示的深度感知动态场景基准测试，通过混合自动化和人工验证的流程构建。&lt;h4&gt;主要发现&lt;/h4&gt;4D-RGPT在现有4D视频问答基准测试和提出的R4D-Bench基准测试上都取得了显著的改进。&lt;h4&gt;结论&lt;/h4&gt;通过4D-RGPT、P4D训练框架和R4D-Bench基准测试，有效解决了多模态大语言模型在4D感知和时间理解方面的局限性，提升了模型在动态场景理解和推理任务上的表现。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型取得了进展，但它们对3D结构和时间动态的推理能力仍然有限，受限于弱的4D感知和时间理解能力。现有的3D和4D视频问答基准测试也强调静态场景，缺乏区域级提示。我们通过引入以下内容解决这些问题：(a) 4D-RGPT，一种专门设计的多模态大语言模型，能够从视频输入中捕获4D表示并增强时间感知；(b) 感知4D蒸馏，一种训练框架，将冻结专家模型的4D表示转移到4D-RGPT中，实现全面的4D感知；(c) R4D-Bench，一个具有区域级提示的深度感知动态场景基准测试，通过混合自动化和人工验证的流程构建。我们的4D-RGPT在现有4D视频问答基准测试和提出的R4D-Bench基准测试上都取得了显著的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.</description>
      <author>example@mail.com (Chiao-An Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, Yu-Chiang Frank Wang, Min-Hung Chen)</author>
      <guid isPermaLink="false">2512.17012v2</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>On Network-Aware Semantic Communication and Edge-Cloud Collaborative Intelligence Systems</title>
      <link>http://arxiv.org/abs/2512.19563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述全面总结了语义通信和边缘-云协作智能作为下一代智能服务基础支撑技术的最新进展，强调语义通信通过传输任务相关语义表示而非完美比特传输，实现了通信效率与性能之间的自适应权衡。&lt;h4&gt;背景&lt;/h4&gt;下一代智能服务在严格的带宽、延迟和资源约束下运行，需要新的通信范式来满足需求，传统的比特完美传输方式已无法满足这些要求。&lt;h4&gt;目的&lt;/h4&gt;提供对边缘-云接口处语义通信最新进展的全面和系统性的综合，包括架构模型、表示学习技术、编码策略和优化机制，并探讨其实际应用及未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;采用系统级综合方法，涵盖协作智能的架构模型、表示学习和语义抽象技术、网络感知和资源自适应的语义编码策略，以及学习驱动的优化和编排机制。&lt;h4&gt;主要发现&lt;/h4&gt;语义通信能够实现通信开销、推理准确性、计算负载和端到端延迟之间的自适应权衡，并在安全、信任、弹性和可扩展性等方面具有实际运营价值，与零信任网络和物理层安全等新兴范式相关联。&lt;h4&gt;结论&lt;/h4&gt;语义通信是构建AI原生网络和6G就绪智能系统的关键模块，具有广阔的应用前景和研究价值，但仍面临开放挑战需要进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;语义通信和边缘-云协作智能正逐渐被认为是下一代智能服务的基础支撑技术，这些服务在严格的带宽、延迟和资源约束下运行。通过将通信目标从完美的比特传输转向传输任务相关的语义表示，语义通信实现了通信开销、推理准确性、计算负载和端到端延迟之间的自适应权衡。这篇综述全面总结了边缘-云接口处语义通信的最新进展，包括协作智能的架构模型、表示学习和语义抽象技术、网络感知和资源自适应的语义编码策略，以及学习驱动的优化和编排机制。除了效率考虑外，该综述还将语义通信置于实际的运营环境中，包括安全、信任、弹性和可扩展性，并联系到零信任网络、物理层安全和新兴的边缘-云控制范式。最后，确定了开放挑战和研究方向，强调了语义通信作为AI原生网络和6G就绪智能系统的关键构建模块的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic communication and edge-cloud collaborative intelligence are increasingly recognized as foundational enablers for next-generation intelligent services operating under stringent bandwidth, latency, and resource constraints. By shifting the communication objective from bit-perfect delivery toward the transmission of task-relevant semantic representations, semantic communication enables adaptive tradeoffs among communication overhead, inference accuracy, computational load, and end-to-end latency. This survey provides a comprehensive and system-level synthesis of recent advances in semantic communication at the edge-cloud interface, encompassing architectural models for collaborative intelligence, representation learning and semantic abstraction techniques, network-aware and resource-adaptive semantic encoding strategies, and learning-driven optimization and orchestration mechanisms. Beyond efficiency considerations, the survey situates semantic communication within practical operational contexts, including security, trust, resilience, and scalability, drawing connections to zero-trust networking, physical-layer security, and emerging edge-cloud control paradigms. Finally, open challenges and research directions are identified, highlighting the role of semantic communication as a key building block for AI-native networking and 6G-ready intelligent systems.</description>
      <author>example@mail.com (Murdadha Nasif, Ahmed Refaey Hussein)</author>
      <guid isPermaLink="false">2512.19563v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement</title>
      <link>http://arxiv.org/abs/2512.19530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个混合GNN架构，用于预测连续溶剂组成范围内的反应结果，显著提高了预测精度。&lt;h4&gt;背景&lt;/h4&gt;预测连续溶剂组成范围内的反应结果是有机合成和工艺化学中的关键挑战。传统机器学习方法将溶剂视为离散变量，无法进行系统内推和外推。&lt;h4&gt;目的&lt;/h4&gt;开发能够处理连续溶剂组成范围的预测方法，并创建基准数据集评估不同方法在连续溶剂空间中的性能。&lt;h4&gt;方法&lt;/h4&gt;引入'Catechol Benchmark'数据集，包含1227个实验产率测量值，涵盖24种纯溶剂及其二元混合物；评估多种架构；提出混合GNN架构，结合图注意力网络、差分反应指纹和混合感知溶剂编码。&lt;h4&gt;主要发现&lt;/h4&gt;经典表格方法和大型语言模型在定量精度上表现不佳(MSE分别为0.099和0.129)；提出的混合GNN架构实现MSE为0.0039，比基线减少60%错误，比表格集成提高25倍以上；分子图消息传递和连续混合编码对稳健泛化至关重要。&lt;h4&gt;结论&lt;/h4&gt;完整数据集、评估协议和参考实现已发布，将促进数据高效的反应预测和连续溶剂表示学习。&lt;h4&gt;翻译&lt;/h4&gt;在有机合成和工艺化学中，预测连续溶剂组成范围内的反应结果仍然是一个关键挑战。传统的机器学习方法通常将溶剂身份视为离散的分类变量，这阻碍了在溶剂空间中的系统内插和外推。这项工作引入了'Catechol Benchmark'，一个包含1227个实验产率测量值的高通量瞬态流动化学数据集，涉及在24种纯溶剂及其二元混合物中取代烯丙基的儿茶酚的重排，通过连续体积分数(%B)参数化。我们在严格的留一溶剂和留一混合物协议下评估各种架构，以测试对未见化学环境的泛化能力。我们的结果表明，经典表格方法(如梯度提升决策树)和大语言模型嵌入(如Qwen-7B)在定量精度上表现不佳，分别产生0.099和0.129的平均平方误差(MSE)。相比之下，我们提出了一种基于混合GNN的架构，集成了图注意力网络(GATs)、差分反应指纹(DRFP)和学习的混合感知溶剂编码。该方法实现了0.0039的MSE(±0.0003)，比竞争基线减少了60%的错误，比表格集成提高了25倍以上。消融研究证实，明确的分子图消息传递和连续混合编码对于稳健泛化是必不可少的。完整的数据集、评估协议和参考实现已发布，以促进数据高效的反应预测和连续溶剂表示学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \textbf{MSE of 0.0039} ($\pm$ 0.0003), representing a 60\% error reduction over competitive baselines and a $&gt;25\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.</description>
      <author>example@mail.com (Hongsheng Xing, Qiuxin Si)</author>
      <guid isPermaLink="false">2512.19530v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation</title>
      <link>http://arxiv.org/abs/2512.19438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究重新思考了深度图像水印方法，引入协作交互机制(CIM)和自适应特征调制模块(AFMM)，通过建立嵌入器和提取器之间的直接双向通信实现协作优化，显著提升了水印提取准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有深度图像水印方法遵循固定嵌入-失真-提取流水线，嵌入器和提取器通过最终损失函数弱耦合且独立优化，缺乏明确协作机制，没有结构化方式让嵌入器整合解码感知线索或让提取器指导嵌入过程。&lt;h4&gt;目的&lt;/h4&gt;解决传统架构限制，将嵌入和提取重新设计为明确协作组件，引入协作交互机制建立双向通信，并提出自适应特征调制模块支持有效交互。&lt;h4&gt;方法&lt;/h4&gt;引入协作交互机制(CIM)建立嵌入器和提取器间直接双向通信，采用相互教师训练范式；提出自适应特征调制模块(AFMM)通过解耦调制结构和强度实现内容感知特征调节，引导水印嵌入向稳定图像特征同时抑制宿主干扰；两侧AFMM形成闭环协作使嵌入行为与提取目标一致。&lt;h4&gt;主要发现&lt;/h4&gt;架构级重新设计改变了水印系统中鲁棒性的学习方式，鲁棒性不再依赖详尽失真模拟，而是从嵌入和提取间的协调表示学习中涌现；实验证明该方法在真实世界和AI生成数据集上均优于最先进方法，同时保持高感知质量。&lt;h4&gt;结论&lt;/h4&gt;所提出的CIM和AFMM架构成功解决了传统深度图像水印方法的架构限制，通过明确协作机制实现了更好的水印提取准确性和感知质量，在各种数据集上表现出强大的鲁棒性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;现有的深度图像水印方法遵循固定的嵌入-失真-提取流水线，其中嵌入器和提取器通过最终损失函数弱耦合，并且独立优化。这种设计缺乏明确的协作，没有结构化的机制让嵌入器整合解码感知线索，也没有让提取器在训练过程中指导嵌入。为了解决这一架构限制，我们通过将嵌入和提取重新明确设计为协作组件来重新思考深度图像水印。为了实现这一重新设计，我们引入了协作交互机制(CIM)，该机制在嵌入器和提取器之间建立直接的双向通信，实现了相互教师训练范式和协调优化。基于这种明确协作的架构，我们进一步提出了自适应特征调制模块(AFMM)来支持有效交互。AFMM通过解耦调制结构和强度实现内容感知特征调节，引导水印嵌入向稳定的图像特征，同时在提取过程中抑制宿主干扰。在CIM下，两侧的AFMM形成闭环协作，使嵌入行为与提取目标保持一致。这种架构级的重新设计改变了水印系统中鲁棒性的学习方式。鲁棒性不再依赖于详尽的失真模拟，而是从嵌入和提取之间的协调表示学习中涌现。在真实世界和AI生成的数据集上的实验表明，所提出的方法在水印提取准确性方面始终优于最先进的方法，同时保持高感知质量，显示出强大的鲁棒性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.</description>
      <author>example@mail.com (Fei Ge, Ying Huang, Jie Liu, Guixuan Zhang, Zhi Zeng, Shuwu Zhang, Hu Guan)</author>
      <guid isPermaLink="false">2512.19438v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Cluster-Based Generalized Additive Models Informed by Random Fourier Features</title>
      <link>http://arxiv.org/abs/2512.19373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 13 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于随机傅里叶特征和广义加性模型混合体的新方法，用于实现高预测性能同时保持模型可解释性。&lt;h4&gt;背景&lt;/h4&gt;可解释机器学习旨在平衡预测准确性和模型透明性，特别是在黑盒预测模型（如深度神经网络或基于核的方法）表现出色但难以解释的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合表示学习和透明统计建模的方法，通过广义加性模型混合体来揭示数据中的局部自适应结构。&lt;h4&gt;方法&lt;/h4&gt;利用随机傅里叶特征表示来发现数据中的局部自适应结构，首先学习基于RFF的嵌入，然后通过主成分分析进行压缩，使用高斯混合模型对数据进行软聚类，构建混合GAM框架，其中每个局部GAM通过可解释的单变量平滑函数捕获非线性效应。&lt;h4&gt;主要发现&lt;/h4&gt;在加州住房、NASA翼型自噪声和自行车共享等真实世界回归基准数据集上的数值实验表明，该方法相对于经典可解释模型具有改进的预测性能。&lt;h4&gt;结论&lt;/h4&gt;这种构建为将表示学习与透明统计建模相结合提供了原则性的方法。&lt;h4&gt;翻译&lt;/h4&gt;可解释机器学习旨在平衡预测准确性和模型透明性，特别是在黑盒预测模型（如深度神经网络或基于核的方法）取得强大经验性能但仍然难以解释的情况下。这项工作引入了广义加性模型混合体，其中利用随机傅里叶特征表示来揭示数据中的局部自适应结构。在所提出的方法中，首先学习基于RFF的嵌入，然后通过主成分分析进行压缩。所得的低维表示通过高斯混合模型执行数据的软聚类。然后应用这些聚类分配来构建混合GAM框架，其中每个局部GAM通过可解释的单变量平滑函数捕获非线性效应。在加州住房、NASA翼型自噪声和自行车共享等真实世界回归基准数据集上的数值实验表明，相对于经典可解释模型具有改进的预测性能。总体而言，这种构建为将表示学习与透明统计建模相结合提供了原则性的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.</description>
      <author>example@mail.com (Xin Huang, Jia Li, Jun Yu)</author>
      <guid isPermaLink="false">2512.19373v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization</title>
      <link>http://arxiv.org/abs/2512.19365v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SpikeViMFormer，这是第一个专为无人机视图地理定位(DVGL)设计的脉冲神经网络(SNN)框架，解决了传统人工神经网络(ANN)高功耗问题以及SNN在表征学习中的信息丢失和长程依赖学习困难问题。&lt;h4&gt;背景&lt;/h4&gt;传统基于ANN的DVGL方法虽表现优异，但依赖密集计算导致高功耗；SNN虽有低功耗优势，但在DVGL领域的潜力尚未充分探索，且其脉冲驱动计算的固有稀疏性会导致关键信息丢失和学习长程依赖的困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种SNN框架，解决DVGL应用中的功耗问题，同时克服SNN在表征学习中的信息丢失和长程依赖学习困难。&lt;h4&gt;方法&lt;/h4&gt;提出SpikeViMFormer框架，采用轻量级脉冲驱动Transformer主干网络提取特征；设计脉冲驱动选择性注意力(SSA)块通过门控机制实现选择性特征增强；引入脉冲驱动混合状态空间(SHS)块学习长程依赖；推理阶段仅使用主干网络降低计算成本；提出分层重排序对齐学习(HRAL)策略优化主干网络。&lt;h4&gt;主要发现&lt;/h4&gt;SpikeViMFormer性能优于最先进的SNN，与先进的ANN相比也具有竞争力，同时保持了SNN的低功耗优势。&lt;h4&gt;结论&lt;/h4&gt;SpikeViMFormer是首个专为DVGL设计的SNN框架，有效解决了SNN在该应用中的关键问题，在保持低功耗的同时实现了与先进ANN相当的性能。&lt;h4&gt;翻译&lt;/h4&gt;传统的基于人工神经网络(ANN)的无人机视图地理定位(DVGL)方法已取得了显著性能。然而，ANN依赖密集计算，导致高功耗。相比之下，受益于脉冲驱动计算的脉冲神经网络(SNN) inherently 提供低功耗。遗憾的是，SNN在DVGL方面的潜力尚未得到充分研究。同时，脉冲驱动计算在表征学习场景中的固有稀疏性也导致关键信息丢失和对齐异构视觉数据源时学习长程依赖的困难。为解决这些问题，我们提出了SpikeViMFormer，这是第一个为DVGL设计的SNN框架。在该框架中，采用轻量级脉冲驱动Transformer主干网络提取粗粒度特征。为减轻关键信息丢失，设计了脉冲驱动选择性注意力(SSA)块，使用脉冲驱动门控机制实现选择性特征增强，突出判别性区域。此外，引入了脉冲驱动混合状态空间(SHS)块，使用混合状态空间学习长程依赖。而且，在推理阶段仅使用主干网络以减少计算成本。为确保主干网络有效性，提出了一种新的分层重排序对齐学习(HRAL)策略。它通过邻域重排序细化特征，保持跨批次一致性以直接优化主干网络。实验结果表明，SpikeViMFormer优于最先进的SNN。与先进的ANN相比，它也取得了具有竞争力的性能。我们的代码可在https://github.com/ISChenawei/SpikeViMFormer获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer</description>
      <author>example@mail.com (Zhongwei Chen, Hai-Jun Rong, Zhao-Xu Yang, Guoqi Li)</author>
      <guid isPermaLink="false">2512.19365v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation</title>
      <link>http://arxiv.org/abs/2512.19061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种创新的基于图的欺诈检测方法，通过区分硬链接和软链接，结合图转换技术和LINE与HDBSCAN算法，有效解决了大规模异构图聚类问题，显著提高了欺诈检测的覆盖范围和效率。&lt;h4&gt;背景&lt;/h4&gt;协作欺诈问题日益严重，多个欺诈账户协调利用在线支付系统形成复杂网络结构。传统检测方法存在局限性：仅依赖高置信度身份链接的方法覆盖范围有限，而使用所有可用链接的方法会导致图形碎片化，降低聚类效果。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于图的欺诈检测框架，解决大规模异构图聚类挑战，通过有原则的链接转换方法提高欺诈检测的覆盖范围和效率。&lt;h4&gt;方法&lt;/h4&gt;区分硬链接（高置信度身份关系，如电话号码、信用卡、国家ID）和软链接（行为关联，包括设备指纹、cookies和IP地址）。采用图转换技术：通过硬链接识别连接组件，合并为超节点，重建加权软链接图。使用LINE进行表示学习，使用HDBSCAN进行基于密度的聚类发现。&lt;h4&gt;主要发现&lt;/h4&gt;在真实支付平台数据集上实验表明：实现显著的图形规模缩减（从2500万节点减少到770万节点）；与仅使用硬链接的基线方法相比，检测覆盖范围提高一倍；在识别的欺诈聚类中保持高精度。&lt;h4&gt;结论&lt;/h4&gt;该框架为工业规模的欺诈检测系统提供了可扩展且实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;协作欺诈是指多个欺诈账户协调利用在线支付系统，由于形成复杂的网络结构而带来重大挑战。仅依赖高置信度身份链接的传统检测方法存在覆盖范围有限的缺陷，而使用所有可用链接的方法往往导致碎片化的图形，降低聚类效果。在本文中，我们提出了一种新颖的基于图的欺诈检测框架，通过有原则的链接转换方法解决大规模异构图聚类挑战。我们的方法区分硬链接（高置信度身份关系，如电话号码、信用卡和国家ID）和软链接（行为关联，包括设备指纹、cookies和IP地址）。我们引入了一种图转换技术，首先通过硬链接识别连接组件，将它们合并为超节点，然后重建一个适合高效嵌入和聚类的加权软链接图。使用LINE（大规模信息网络嵌入）处理变换后的图进行表示学习，接着使用HDBSCAN（基于密度的空间聚类应用与噪声）进行基于密度的聚类发现。在真实支付平台数据集上的实验表明，我们的方法实现了显著的图形规模缩减（从2500万减少到770万节点），比仅使用硬链接的基线方法提高了一倍的检测覆盖率，并在识别的欺诈聚类中保持高精度。我们的框架为工业规模的欺诈检测系统提供了可扩展且实用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.</description>
      <author>example@mail.com (Chi Liu)</author>
      <guid isPermaLink="false">2512.19061v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>FairExpand: Individual Fairness on Graphs with Partial Similarity Information</title>
      <link>http://arxiv.org/abs/2512.18180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FairExpand框架，用于在只有部分节点对相似性信息的情况下实现图表示学习中的个体公平性，解决了现有方法需要所有节点对预定义相似性信息的不现实假设问题。&lt;h4&gt;背景&lt;/h4&gt;个体公平性已成为公平机器学习的核心原则，在图表示学习中受到关注，因为它在高风险的Web领域（如用户建模、推荐系统和搜索）具有实际重要性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在解决现有方法需要所有节点对预定义相似性信息的不现实假设，提出一种在只有部分相似性信息的情况下促进个体公平性的实用框架。&lt;h4&gt;方法&lt;/h4&gt;FairExpand采用两步流程，交替使用骨干模型（如图神经网络）细化节点表示和逐渐传播相似性信息，使公平性能够有效扩展到整个图。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，FairExpand在保持模型性能的同时持续增强个体公平性，使其成为在具有部分相似性信息的现实应用中实现基于图的个体公平性的有效解决方案。&lt;h4&gt;结论&lt;/h4&gt;FairExpand为在现实世界应用中实现图表示学习的个体公平性提供了实用解决方案，克服了现有方法对完整相似性信息依赖的限制。&lt;h4&gt;翻译&lt;/h4&gt;个体公平性要求算法系统对相似个体给予相似对待，已成为公平机器学习的中心原则。个体公平性在图表示学习中受到关注，因为它在高风险的Web领域（如用户建模、推荐系统和搜索）具有实际重要性。然而，现有方法假设所有节点对都存在预定义的相似性信息，这是一个通常不现实的要求，阻碍了它们的实际应用。本文假设相似性信息仅适用于有限的节点对子集，并提出了FairExpand框架，促进在更现实的局部信息场景下的个体公平性。FairExpand遵循两步流程，交替使用骨干模型（如图神经网络）细化节点表示和逐渐传播相似性信息，使公平性能够有效扩展到整个图。大量实验表明，FairExpand在保持性能的同时持续增强个体公平性，使其成为在具有部分相似性信息的现实应用中实现基于图的个体公平性的实用解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Individual fairness, which requires that similar individuals should be treated similarly by algorithmic systems, has become a central principle in fair machine learning. Individual fairness has garnered traction in graph representation learning due to its practical importance in high-stakes Web areas such as user modeling, recommender systems, and search. However, existing methods assume the existence of predefined similarity information over all node pairs, an often unrealistic requirement that prevents their operationalization in practice. In this paper, we assume the similarity information is only available for a limited subset of node pairs and introduce FairExpand, a flexible framework that promotes individual fairness in this more realistic partial information scenario. FairExpand follows a two-step pipeline that alternates between refining node representations using a backbone model (e.g., a graph neural network) and gradually propagating similarity information, which allows fairness enforcement to effectively expand to the entire graph. Extensive experiments show that FairExpand consistently enhances individual fairness while preserving performance, making it a practical solution for enabling graph-based individual fairness in real-world applications with partial similarity information.</description>
      <author>example@mail.com (Rebecca Salganik, Yibin Wang, Guillaume Salha-Galvan, Jian Kang)</author>
      <guid isPermaLink="false">2512.18180v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications</title>
      <link>http://arxiv.org/abs/2512.18135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 14 figures, 5 algorithms&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;因果推断与强化学习的集成已成为解决传统强化学习局限性的有力范式，通过明确建模因果关系提高系统的可解释性、鲁棒性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统强化学习技术依赖相关性驱动的决策，在面对分布偏移、混杂变量和动态环境时表现不佳，存在可解释性低、鲁棒性差和泛化能力不足等局限性。&lt;h4&gt;目的&lt;/h4&gt;系统性地回顾因果推断与强化学习交叉领域的最新进展，识别现有方法的挑战，强调实际应用中的成功经验，讨论开放性问题，并提供未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;将现有方法分类为因果表示学习、反事实策略优化、离线因果强化学习、因果迁移学习和因果可解释性，并通过结构化分析进行系统综述。&lt;h4&gt;主要发现&lt;/h4&gt;因果强化学习通过利用因果推断的基本原理，明确建模因果关系，为传统强化学习面临的挑战提供了有希望的解决方案，并在实际应用中取得了成功。&lt;h4&gt;结论&lt;/h4&gt;因果强化学习在开发鲁棒、可泛化和可解释的人工智能系统方面具有巨大潜力，为未来人工智能研究提供了重要方向。&lt;h4&gt;翻译&lt;/h4&gt;将因果推断(CI)与强化学习(RL)集成已成为解决传统RL关键局限性的有力范式，包括低可解释性、缺乏鲁棒性和泛化失败。传统RL技术通常依赖相关性驱动的决策，在面对分布偏移、混杂变量和动态环境时表现不佳。因果强化学习(CRL)利用因果推断的基本原理，通过明确建模因果关系，为这些挑战提供了有希望的解决方案。在本综述中，我们系统地回顾了因果推断与RL交叉领域的最新进展。我们将现有方法分为因果表示学习、反事实策略优化、离线因果RL、因果迁移学习和因果可解释性。通过这种结构化分析，我们确定了当前面临的挑战，强调了实际应用中的成功经验，并讨论了开放性问题。最后，我们提供了未来研究方向，强调了CRL在开发鲁棒、可泛化和可解释的人工智能系统方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.</description>
      <author>example@mail.com (Cristiano da Costa Cunha, Wei Liu, Tim French, Ajmal Mian)</author>
      <guid isPermaLink="false">2512.18135v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning</title>
      <link>http://arxiv.org/abs/2512.18117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by WSDM'26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于因子传输的多视图学习框架，解决了电子商务平台中现有视觉语言模型仅关注主图像而忽略辅助图像和文本信息的问题。该方法在训练时优化主视图并随机采样辅助视图，在推理时将所有视图融合为单个缓存嵌入，在大型电商数据集上显著提升了检索性能。&lt;h4&gt;背景&lt;/h4&gt;电子商务的快速增长需要能够捕捉用户生成列表中多样化信号的鲁棒多模态表示。现有的视觉语言模型通常只将标题与主图像对齐，忽略了在开放市场平台中提供关键语义的非主图像和辅助文本视图。&lt;h4&gt;目的&lt;/h4&gt;提出一种统一多模态和多视图学习的框架，通过因子传输方法（最优传输的轻量级近似）解决现有VLMs的局限性，提高可扩展性和部署效率。&lt;h4&gt;方法&lt;/h4&gt;1) 提出基于因子传输的框架统一多模态和多视图学习；2) 训练时强调主视图并随机采样辅助视图，将训练成本从二次降低到常数级；3) 推理时将所有视图融合为单个缓存嵌入，保留双塔检索效率；4) 在包含100万产品列表和30万次交互的工业数据集上进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;在工业数据集上，该方法在跨视图和查询到项目检索方面取得持续改进，相比强大的多模态基线，Recall@500提升了高达7.9%。&lt;h4&gt;结论&lt;/h4&gt;该框架将可扩展性与基于最优传输的学习相结合，使多视图预训练在大规模电子商务搜索中变得实用。&lt;h4&gt;翻译&lt;/h4&gt;电子商务的快速增长需要能够捕捉用户生成列表中多样化信号的鲁棒多模态表示。现有的视觉语言模型通常将标题与主图像对齐，即单视图方法，但忽略了在Etsy或Poshmark等开放市场中提供关键语义的非主图像和辅助文本视图。为此，我们提出了一种通过因子传输统一多模态和多视图学习的框架，这是最优传输的轻量级近似，专为可扩展性和部署效率而设计。在训练期间，该方法强调主视图，同时随机采样辅助视图，将训练成本从与视图数量的二次关系降低到每项目常数级。在推理时，所有视图被融合为单个缓存嵌入，保留了双塔检索的效率，没有额外的在线开销。在包含100万个产品列表和30万次交互的工业数据集上，我们的方法在跨视图和查询到项目检索方面取得了持续改进，相比强大的多模态基线，Recall@500提升了高达7.9%。总体而言，我们的框架将可扩展性与基于最优传输的学习相结合，使多视图预训练在大规模电子商务搜索中变得实用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search.</description>
      <author>example@mail.com (Xiwen Chen, Yen-Chieh Lien, Susan Liu, María Castaños, Abolfazl Razi, Xiaoting Zhao, Congzhe Su)</author>
      <guid isPermaLink="false">2512.18117v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Greater than the Sum of Its Parts: Building Substructure into Protein Encoding Models</title>
      <link>http://arxiv.org/abs/2512.18114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Magneton，一个用于开发亚结构感知蛋白质模型的环境，提供了大规模亚结构注释数据集、训练框架和多层次基准任务。通过亚结构微调方法，成功将亚结构知识整合到预训练蛋白质模型中，改善了功能预测并产生了更一致的表示。&lt;h4&gt;背景&lt;/h4&gt;蛋白质表示学习随着序列和结构监督的规模扩大而快速发展，但大多数模型仍将蛋白质编码为残基序列或全局嵌入，忽略了蛋白质是由重复的、进化上保守的亚结构组成这一关键特性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够识别和利用蛋白质亚结构的环境，以改进蛋白质表示学习，并探索亚结构信息如何补充全局结构信息。&lt;h4&gt;方法&lt;/h4&gt;创建了Magneton环境，包含(1)530,601个蛋白质的亚结构注释数据集，(2)亚结构整合训练框架，(3)多层次基准任务套件。并开发了亚结构微调方法，将亚结构知识蒸馏到预训练蛋白质模型中。&lt;h4&gt;主要发现&lt;/h4&gt;亚结构微调改善了功能预测，产生了对未观察过的亚结构类型更一致的表示，表明亚结构监督提供了与全局结构输入互补的信息。&lt;h4&gt;结论&lt;/h4&gt;亚结构感知的蛋白质模型能够更好地捕捉蛋白质的生物学特性，Magneton环境为开发此类模型提供了完整工具链。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质表示学习随着序列和结构监督的规模扩大而迅速发展，但大多数模型仍然将蛋白质编码为每个残基的标记序列或单个全局嵌入。这忽略了蛋白质组织的定义特性：蛋白质是由重复的、进化上保守的亚结构组成，这些亚结构集中了生化活性并介导核心分子功能。尽管结构域和功能位点等亚结构已被系统性地分类，但它们很少被用作蛋白质模型中的训练信号或表示单位。我们引入了Magneton，这是一个用于开发亚结构感知蛋白质模型的环境。Magneton提供(1)一个包含530,601个蛋白质的数据集，标注了超过170万个亚结构，涵盖13,075种类型，(2)一个将亚结构整合到现有蛋白质模型中的训练框架，(3)一个包含13个任务的基准套件，探测残基、亚结构和蛋白质水平的表示。使用Magneton，我们开发了亚结构微调，这是一种监督微调方法，可以将亚结构知识预训练到蛋白质模型中。在最先进的序列和结构模型中，亚结构微调改善了功能预测，产生了对在微调过程中从未观察到的亚结构类型更一致的表示，并表明亚结构监督提供了与全局结构输入互补的信息。Magneton环境、数据集和亚结构微调模型都是公开可用的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein representation learning has advanced rapidly with the scale-up of sequence and structure supervision, but most models still encode proteins either as per-residue token sequences or as single global embeddings. This overlooks a defining property of protein organization: proteins are built from recurrent, evolutionarily conserved substructures that concentrate biochemical activity and mediate core molecular functions. Although substructures such as domains and functional sites are systematically cataloged, they are rarely used as training signals or representation units in protein models. We introduce Magneton, an environment for developing substructure-aware protein models. Magneton provides (1) a dataset of 530,601 proteins annotated with over 1.7 million substructures spanning 13,075 types, (2) a training framework for incorporating substructures into existing protein models, and (3) a benchmark suite of 13 tasks probing representations at the residue, substructural, and protein levels. Using Magneton, we develop substructure-tuning, a supervised fine-tuning method that distills substructural knowledge into pretrained protein models. Across state-of-the-art sequence- and structure-based models, substructure-tuning improves function prediction, yields more consistent representations of substructure types never observed during tuning, and shows that substructural supervision provides information that is complementary to global structure inputs. The Magneton environment, datasets, and substructure-tuned models are all openly available (https://github.com/rcalef/magneton/).</description>
      <author>example@mail.com (Robert Calef, Arthur Liang, Manolis Kellis, Marinka Zitnik)</author>
      <guid isPermaLink="false">2512.18114v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Learning General Policies with Policy Gradient Methods</title>
      <link>http://arxiv.org/abs/2512.19366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning (KR 2023)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究结合强化学习与组合方法，探索如何使深度强化学习策略优化方法学习到类似组合方法的泛化能力。通过将策略建模为状态转换分类器并使用图神经网络表示值函数，实现了与组合方法几乎相当的泛化效果，同时避免了组合方法的可扩展性瓶颈。&lt;h4&gt;背景&lt;/h4&gt;强化学习在多个领域已取得显著成果，但泛化能力（即可靠系统地产生泛化策略）仍是挑战。经典规划中已通过组合方法解决了泛化问题，学习了在给定领域所有实例上具有可证明正确性的泛化策略。&lt;h4&gt;目的&lt;/h4&gt;将强化学习与组合方法两个研究线索结合，阐明深度强化学习方法，特别是策略优化方法，在何种条件下可学习到像组合方法那样泛化的策略。&lt;h4&gt;方法&lt;/h4&gt;1. 将策略建模为状态转换分类器，因为基础动作不具有泛化性；2. 使用适应关系结构的图神经网络表示规划状态上的值函数和策略；3. 应用actor-critic方法学习泛化策略；4. 通过添加派生谓词和替代成本结构解决GNNs表达限制及最优性与泛化权衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;1. actor-critic方法可学习到与组合方法几乎一样好的泛化策略，同时避免可扩展性瓶颈和特征池使用；2. DRL方法限制源于GNNs表达限制及最优性与泛化权衡，而非深度学习或强化学习算法本身；3. 通过添加派生谓词和替代成本结构可在不改变基本DRL方法的情况下解决这些限制。&lt;h4&gt;结论&lt;/h4&gt;通过结合组合方法和深度强化学习的优势，提出的方法使DRL能够学习到与组合方法几乎一样好的泛化策略，同时避免组合方法的可扩展性瓶颈。通过解决GNNs表达限制和最优性与泛化权衡问题，进一步提高了方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;虽然强化学习方法在许多场景中已经取得了显著成果，但泛化能力，即可靠且系统地产生能泛化的策略的能力，仍然是一个挑战。在经典规划中，泛化问题已经得到了正式解决，使用组合方法学习了在给定领域所有实例上具有可证明正确性的泛化策略。本工作的目标是将这两个研究线索结合起来，阐明深度强化学习方法，特别是策略优化方法，在何种条件下可以学习到像组合方法那样泛化的策略。我们从之前组合方法和深度学习方法中汲取经验，并以方便的方式扩展它们。从前者的经验中，我们将策略建模为状态转换分类器，因为基础动作不具有泛化性，在不同实例间会变化。从后者的经验中，我们使用适应关系结构的图神经网络来表示规划状态上的值函数，在我们的案例中是策略。有了这些组件，我们发现actor-critic方法可以用来学习泛化策略，其效果几乎与组合方法获得的策略一样好，同时避免了可扩展性瓶颈和特征池的使用。此外，DRL方法在所考虑基准测试中的限制与深度学习或强化学习算法关系不大，而是源于图神经网络众所周知的表达限制，以及最优性和泛化之间的权衡（在某些领域中，泛化策略不能是最优的）。通过添加派生谓词和替代成本结构来优化，可以在不改变基本DRL方法的情况下解决这两个限制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.</description>
      <author>example@mail.com (Simon Ståhlberg, Blai Bonet, Hector Geffner)</author>
      <guid isPermaLink="false">2512.19366v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies</title>
      <link>http://arxiv.org/abs/2512.18540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于强化学习的网络化系统分布式控制方法，通过结合图神经网络和类似Youla的参数化方法，实现了可扩展、表达能力强且稳定的控制策略。&lt;h4&gt;背景&lt;/h4&gt;在网络化系统中进行分布式控制时，神经策略需要同时满足可扩展性、表达能力和稳定性，这是一个具有挑战性的问题。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够同时满足可扩展性、表达能力和稳定性的分布式控制策略，并通过强化学习方法进行训练，确保网络级闭环稳定性。&lt;h4&gt;方法&lt;/h4&gt;引入一种将图神经网络嵌入到类似Youla的幅值-方向参数化中的策略参数化方法，其中幅值部分由GNN作用于扰动反馈实现为稳定算子，方向部分由GNN作用于局部观测，并与近端策略优化集成。&lt;h4&gt;主要发现&lt;/h4&gt;训练在小网络上的策略可以直接扩展到更大的网络和未见过的网络拓扑；与最先进的MARL基线相比，所提策略实现了更高的回报和更低的方差，同时保持稳定性。&lt;h4&gt;结论&lt;/h4&gt;所提出的策略参数化方法有效地解决了网络化系统中的分布式控制问题，具有良好的泛化能力和鲁棒性，能够在保持稳定性的同时实现高性能。&lt;h4&gt;翻译&lt;/h4&gt;我们通过强化学习研究网络化系统的分布式控制，其中神经策略必须同时具备可扩展性、表达能力和稳定性。我们引入了一种将图神经网络（GNNs）嵌入到类似Youla的幅值-方向参数化中的策略参数化方法，从而产生分布式随机控制器，通过设计保证网络级闭环稳定性。幅值部分实现为稳定算子，由GNN作用于扰动反馈，而方向部分为由GNN作用于局部观测。我们证明了闭环系统对图拓扑和模型参数扰动的鲁棒性，并展示了如何将我们的参数化方法与近端策略优化集成。在多智能体导航任务上的实验表明，训练在小网络上的策略可以直接扩展到更大的网络和未见过的网络拓扑，与最先进的MARL基线相比，实现了更高的回报和更低的方差，同时保持了稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study distributed control of networked systems through reinforcement learning, where neural policies must be simultaneously scalable, expressive and stabilizing. We introduce a policy parameterization that embeds Graph Neural Networks (GNNs) into a Youla-like magnitude-direction parameterization, yielding distributed stochastic controllers that guarantee network-level closed-loop stability by design. The magnitude is implemented as a stable operator consisting of a GNN acting on disturbance feedback, while the direction is a GNN acting on local observations. We prove robustness of the closed loop to perturbations in both the graph topology and model parameters, and show how to integrate our parameterization with Proximal Policy Optimization. Experiments on a multi-agent navigation task show that policies trained on small networks transfer directly to larger ones and unseen network topologies, achieve higher returns and lower variance than a state-of-the-art MARL baseline while preserving stability.</description>
      <author>example@mail.com (John Cao, Luca Furieri)</author>
      <guid isPermaLink="false">2512.18540v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study</title>
      <link>http://arxiv.org/abs/2512.18524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a preprint version of a manuscript currently under review at The Journal of Supercomputing (Springer)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了结合图神经网络(GNNs)和可解释图论特征进行合成图分类的混合方法，发现GraphSAGE和GTN架构表现最佳，达到98.5%的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;区分生成图模型对于理解合成图及其模拟的现实世界复杂结构模式至关重要。虽然图神经网络在图分类任务中效果显著，但很少有研究探索它们与可解释图论特征的结合。&lt;h4&gt;目的&lt;/h4&gt;研究一种混合方法，将图神经网络与工程化的图论特征相结合，用于合成图家族的分类任务。&lt;h4&gt;方法&lt;/h4&gt;生成包含五个代表性生成家族(Erdos-Renyi、Watts-Strogatz、Barab'asi-Albert、Holme-Kim和Stochastic Block Model)的大型结构多样合成数据集；提取并修剪节点和图级别特征；将特征整合到六种GNN架构(GCN、GAT、GATv2、GIN、GraphSAGE和GTN)中；使用Optuna进行超参数优化；与基于手工特征的支持向量机基线模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;GraphSAGE和GTN实现了最高的分类性能，准确率达98.5%，t-SNE和UMAP可视化显示良好的类别分离；GCN和GIN表现良好；基于GAT的模型表现较差，因其捕获全局结构能力有限；SVM基线证实了消息传递功能对性能提升和有意义类别分离的重要性。&lt;h4&gt;结论&lt;/h4&gt;结合图神经网络和工程化图论特征的混合方法在合成图分类任务中表现优异，特别是GraphSAGE和GTN架构。&lt;h4&gt;翻译&lt;/h4&gt;区分生成图模型的能力对于理解合成图以及它们所模拟的现实世界复杂结构模式至关重要。虽然图神经网络(GNNs)在图分类任务中已见广泛应用并取得显著效果，但很少有研究探索它们与可解释图论特征的结合。本文研究了使用混合方法分类合成图家族，该方法结合了GNNs和工程化的图论特征。我们生成了一个大型结构多样的合成数据集，包含五个代表性生成家族的图：Erdos-Renyi、Watts-Strogatz、Barab'asi-Albert、Holme-Kim和Stochastic Block Model。这些图的大小最多达到1x10^4个节点，包含最多1.1x10^5条边。为每个图提取了全面的节点和图级别特征，并使用基于随机森林的特征选择流程进行了修剪。将这些特征整合到六种GNN架构中：GCN、GAT、GATv2、GIN、GraphSAGE和GTN。使用Optuna进行超参数选择优化。最后，将模型与仅基于手工制作特征训练的基线支持向量机(SVM)进行比较。我们的评估表明，GraphSAGE和GTN实现了最高的分类性能，准确率达98.5%，t-SNE和UMAP可视化显示了良好的类别分离。GCN和GIN也表现良好，而基于GAT的模型表现滞后，因为它们捕获全局结构的能力有限。SVM基线确认了消息传递功能对性能提升和有意义类别分离的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ability to discriminate between generative graph models is critical to understanding complex structural patterns in both synthetic graphs and the real-world structures that they emulate. While Graph Neural Networks (GNNs) have seen increasing use to great effect in graph classification tasks, few studies explore their integration with interpretable graph theoretic features. This paper investigates the classification of synthetic graph families using a hybrid approach that combines GNNs with engineered graph-theoretic features. We generate a large and structurally diverse synthetic dataset comprising graphs from five representative generative families, Erdos-Renyi, Watts-Strogatz, Barab'asi-Albert, Holme-Kim, and Stochastic Block Model. These graphs range in size up to 1x10^4 nodes, containing up to 1.1x10^5 edges. A comprehensive range of node and graph level features is extracted for each graph and pruned using a Random Forest based feature selection pipeline. The features are integrated into six GNN architectures: GCN, GAT, GATv2, GIN, GraphSAGE and GTN. Each architecture is optimised for hyperparameter selection using Optuna. Finally, models were compared against a baseline Support Vector Machine (SVM) trained solely on the handcrafted features. Our evaluation demonstrates that GraphSAGE and GTN achieve the highest classification performance, with 98.5% accuracy, and strong class separation evidenced by t-SNE and UMAP visualisations. GCN and GIN also performed well, while GAT-based models lagged due to limitations in their ability to capture global structures. The SVM baseline confirmed the importance of the message passing functionality for performance gains and meaningful class separation.</description>
      <author>example@mail.com (Janek Dyer, Jagdeep Ahluwalia, Javad Zarrin)</author>
      <guid isPermaLink="false">2512.18524v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification</title>
      <link>http://arxiv.org/abs/2512.18473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 2 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出APC-GNN++，一种自适应以患者为中心的图神经网络，用于糖尿病分类，通过整合上下文感知边注意力、置信度引导的特征混合和邻域一致性正则化来捕捉患者间的临床关系。&lt;h4&gt;背景&lt;/h4&gt;糖尿病分类需要捕捉患者间的临床关系，传统方法难以有效处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理未见患者并提供可解释预测的图神经网络模型，用于糖尿病分类。&lt;h4&gt;方法&lt;/h4&gt;设计APC-GNN++模型，集成上下文感知边注意力、置信度引导的节点特征和图表示混合、邻域一致性正则化；引入小图方法处理新患者；在阿尔及利亚地区医院收集的真实数据集上评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;APC-GNN++优于传统机器学习模型和普通GCN，获得更高的测试准确率和宏F1分数；节点级置信度分析显示模型在不同患者群体中平衡自我信息和基于图的证据。&lt;h4&gt;结论&lt;/h4&gt;APC-GNN++能有效捕捉患者间的临床关系，提供实时可解释预测，并已嵌入GUI供医疗专业人员使用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出APC-GNN++，一种自适应以患者为中心的图神经网络，用于糖尿病分类。我们的模型整合了上下文感知的边注意力、置信度引导的节点特征和图表示混合，以及邻域一致性正则化，以更好地捕捉患者之间的临床有意义的关系。为处理未见患者，我们引入了一种小图方法，利用新患者的最近邻，无需重新训练全局模型即可实现实时可解释预测。我们在阿尔及利亚地区医院收集的真实世界糖尿病数据集上评估了APC-GNN++，结果表明它优于传统机器学习模型（MLP、随机森林、XGBoost）和普通GCN，获得了更高的测试准确率和宏F1分数。节点级置信度分数的分析进一步揭示了模型如何在不同患者群体中平衡自我信息和基于图的证据，提供了可解释的以患者为中心的见解。该系统还嵌入了一个基于Tkinter的图形用户界面（GUI），供医疗专业人员交互使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose APC-GNN++, an adaptive patient-centric Graph Neural Network for diabetes classification. Our model integrates context-aware edge attention, confidence-guided blending of node features and graph representations, and neighborhood consistency regularization to better capture clinically meaningful relationships between patients. To handle unseen patients, we introduce a mini-graph approach that leverages the nearest neighbors of the new patient, enabling real-time explainable predictions without retraining the global model. We evaluate APC-GNN++ on a real-world diabetes dataset collected from a regional hospital in Algeria and show that it outperforms traditional machine learning models (MLP, Random Forest, XGBoost) and a vanilla GCN, achieving higher test accuracy and macro F1- score. The analysis of node-level confidence scores further reveals how the model balances self-information and graph-based evidence across different patient groups, providing interpretable patient-centric insights. The system is also embedded in a Tkinter-based graphical user interface (GUI) for interactive use by healthcare professionals .</description>
      <author>example@mail.com (Khaled Berkani)</author>
      <guid isPermaLink="false">2512.18473v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>A Distributed Hierarchical Spatio-Temporal Edge-Enhanced Graph Neural Network for City-Scale Dynamic Logistics Routing</title>
      <link>http://arxiv.org/abs/2512.18441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分布式分层时空边缘增强图神经网络（HSTE-GNN），用于解决超大型城市道路网络中的动态物流路由问题，通过区域并行处理和跨区域协调，显著提高了路由效率和实时适应性。&lt;h4&gt;背景&lt;/h4&gt;随着都市道路网络增长到数千万条边，在高流动性需求下交通状况快速变化，传统集中式路由算法和整体图神经网络模型在可扩展性、延迟和实时适应性方面存在局限，难以有效处理大型城市物流系统。&lt;h4&gt;目的&lt;/h4&gt;提出一种分布式分层时空边缘增强图神经网络（HSTE-GNN），用于超大型道路网络的动态路由，解决传统方法的可扩展性、延迟和实时适应性问题。&lt;h4&gt;方法&lt;/h4&gt;将城市规模图划分为区域子图在分布式计算节点上并行处理；在每个区域内使用边缘增强时空模块建模节点状态、动态边属性和短期时间依赖；通过分层协调层和异步参数服务器机制聚合跨区域表示，确保高频交通更新下的全局路由一致性。&lt;h4&gt;主要发现&lt;/h4&gt;分布式分层设计平衡了局部响应性和全局一致性；在北京和纽约的真实大规模交通数据集上，HSTE-GNN优于ST-GRAPH等基线模型，实现了34.9%更低的路由延迟，14.7%更低的MAPE，11.8%更低的RMSE，全局路由一致性提高了7.3%。&lt;h4&gt;结论&lt;/h4&gt;所提出的HSTE-GNN框架为下一代智能交通系统和大规模物流平台提供了可扩展、自适应且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;城市规模的物流路由随着都市道路网络增长到数千万条边以及在高流动性需求下交通状况快速变化而变得越来越具有挑战性。传统的集中式路由算法和整体图神经网络模型在可扩展性、高延迟和实时适应性方面存在局限，这限制了它们在大型城市物流系统中的有效性。为解决这些挑战，本文提出了一种用于超大型道路网络动态路由的分布式分层时空边缘增强图神经网络（HSTE-GNN）。该框架将城市规模图划分为区域子图，在分布式计算节点上并行处理，实现局部交通动力学的高效学习。在每个区域内，边缘增强时空模块联合建模节点状态、动态边属性和短期时间依赖。分层协调层通过异步参数服务器机制进一步聚合跨区域表示，确保在高频交通更新下的全局路由一致性。这种分布式分层设计平衡了局部响应性与全局一致性，显著提高了可扩展性和推理效率。在北京和纽约真实世界大规模交通数据集上的实验表明，HSTE-GNN优于ST-GRAPH等强时空基线模型，实现了34.9%更低的路由延迟，14.7%更低的MAPE，11.8%更低的RMSE，同时将全局路由一致性提高了7.3%。这些结果证实，所提出的框架为下一代智能交通系统和大规模物流平台提供了可扩展、自适应且高效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; City-scale logistics routing has become increasingly challenging as metropolitan road networks grow to tens of millions of edges and traffic conditions evolve rapidly under high-volume mobility demands. Conventional centralized routing algorithms and monolithic graph neural network (GNN) models suffer from limited scalability, high latency, and poor real-time adaptability, which restricts their effectiveness in large urban logistics systems. To address these challenges, this paper proposes a Distributed Hierarchical Spatio-Temporal Edge-Enhanced Graph Neural Network (HSTE-GNN) for dynamic routing over ultra-large road networks. The framework partitions the city-scale graph into regional subgraphs processed in parallel across distributed computing nodes, enabling efficient learning of localized traffic dynamics. Within each region, an edge-enhanced spatio-temporal module jointly models node states, dynamic edge attributes, and short-term temporal dependencies. A hierarchical coordination layer further aggregates cross-region representations through an asynchronous parameter-server mechanism, ensuring global routing coherence under high-frequency traffic updates. This distributed hierarchical design balances local responsiveness with global consistency, significantly improving scalability and inference efficiency. Experiments on real-world large-scale traffic datasets from Beijing and New York demonstrate that HSTE-GNN outperforms strong spatio-temporal baselines such as ST-GRAPH, achieving 34.9% lower routing delay, 14.7% lower MAPE, and 11.8% lower RMSE, while improving global route consistency by 7.3%. These results confirm that the proposed framework provides a scalable, adaptive, and efficient solution for next-generation intelligent transportation systems and large-scale logistics platforms.</description>
      <author>example@mail.com (Zihan Han, Lingran Meng, Jingwei Zhang)</author>
      <guid isPermaLink="false">2512.18441v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval</title>
      <link>http://arxiv.org/abs/2512.18407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PRISm是一个基于语义图重要性预测的剪枝图像检索多模态框架，通过两个新组件改进图像到图像的检索：重要性预测模块和边缘感知图神经网络。该模型能够明确建模对象及其交互的语义重要性，实现与人类感知一致的图像检索。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉中，准确检索语义相似的图像仍然是一个基本挑战，因为传统方法往往无法捕捉场景的关系和上下文细微差别。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更好地捕捉图像中对象及其关系语义重要性的图像检索方法，使检索结果更接近人类感知。&lt;h4&gt;方法&lt;/h4&gt;重要性预测模块识别并保留图像中最关键的对象和关系三元组，同时修剪无关元素；边缘感知图神经网络明确编码关系结构并整合全局视觉特征，以生成语义感知的图像嵌入；结合关系推理与视觉表示，实现语义基础的检索。&lt;h4&gt;主要发现&lt;/h4&gt;在基准和真实世界数据集上的广泛实验展示了持续优越的顶级排名性能；定性分析表明PRISm准确捕捉了关键对象和交互，产生了可解释和语义上有意义的结果。&lt;h4&gt;结论&lt;/h4&gt;PRISm通过明确建模对象及其交互的语义重要性，实现了与人类感知紧密对齐的图像检索，其架构有效地结合了关系推理与视觉表示，实现了语义基础的检索。&lt;h4&gt;翻译&lt;/h4&gt;准确检索语义相似的图像在计算机视觉中仍然是一个基本挑战，因为传统方法往往无法捕捉场景的关系和上下文细微差别。我们介绍了PRISm（基于语义图重要性预测的剪枝图像检索），这是一个多模态框架，通过两个新颖组件推进图像到图像的检索。首先，重要性预测模块识别并保留图像中最关键的对象和关系三元组，同时修剪无关元素。其次，边缘感知图神经网络明确编码关系结构并整合全局视觉特征，以生成语义感知的图像嵌入。PRISm通过明确建模对象及其交互的语义重要性，实现了与人类感知紧密对齐的图像检索，这是先前方法中 largely 缺乏的能力。其架构有效地结合了关系推理与视觉表示，实现了语义基础的检索。在基准和真实世界数据集上的广泛实验展示了持续优越的顶级排名性能，同时定性分析表明PRISm准确捕捉了关键对象和交互，产生了可解释和语义上有意义的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately retrieving images that are semantically similar remains a fundamental challenge in computer vision, as traditional methods often fail to capture the relational and contextual nuances of a scene. We introduce PRISm (Pruning-based Image Retrieval via Importance Prediction on Semantic Graphs), a multimodal framework that advances image-to-image retrieval through two novel components. First, the Importance Prediction Module identifies and retains the most critical objects and relational triplets within an image while pruning irrelevant elements. Second, the Edge-Aware Graph Neural Network explicitly encodes relational structure and integrates global visual features to produce semantically informed image embeddings. PRISm achieves image retrieval that closely aligns with human perception by explicitly modeling the semantic importance of objects and their interactions, capabilities largely absent in prior approaches. Its architecture effectively combines relational reasoning with visual representation, enabling semantically grounded retrieval. Extensive experiments on benchmark and real-world datasets demonstrate consistently superior top-ranked performance, while qualitative analyses show that PRISm accurately captures key objects and interactions, producing interpretable and semantically meaningful results.</description>
      <author>example@mail.com (Dimitrios Georgoulopoulos, Nikolaos Chaidos, Angeliki Dimitriou, Giorgos Stamou)</author>
      <guid isPermaLink="false">2512.18407v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning</title>
      <link>http://arxiv.org/abs/2512.18295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AL GNN的新型持续图学习框架，通过解析学习理论原理，将学习转化为递归最小二乘优化过程，无需反向传播和回放缓冲区，有效解决了现有方法的隐私和效率问题。&lt;h4&gt;背景&lt;/h4&gt;持续图学习旨在使图神经网络能够从流式图数据中增量学习而不遗忘先前知识。现有基于经验回放的方法需要存储和重新访问过去的图数据，但存在隐私问题和效率低下等显著局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的持续图学习框架，消除对反向传播和回放缓冲区的需求，同时解决隐私问题和效率问题。&lt;h4&gt;方法&lt;/h4&gt;AL GNN框架利用解析学习理论原理，将学习公式化为递归最小二乘优化过程。通过封闭形式的分类器更新和正则化特征自相关矩阵来分析和更新模型知识，实现每个任务的高效单次训练，并避免存储历史样本以保护数据隐私。&lt;h4&gt;主要发现&lt;/h4&gt;在多个动态图分类基准上，AL GNN与现有方法相比具有竞争力或更优的性能：在CoraFull上平均性能提高10%，在Reddit上减少超过30%的遗忘，同时由于无反向传播设计，训练时间减少近50%。&lt;h4&gt;结论&lt;/h4&gt;AL GNN是一种有效的持续图学习框架，能够解决现有方法的局限性，同时保持或提高性能，并在保护数据隐私方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;持续图学习(CGL)旨在使图神经网络能够从流式图结构数据中增量学习，同时不忘记之前获得的知识。现有方法，特别是基于经验回放的方法，通常存储和重新访问过去的图数据来减轻灾难性遗忘。然而，这些方法存在显著局限性，包括隐私问题和效率低下。在本工作中，我们提出了AL GNN，一种用于持续图学习的新型框架，消除了对反向传播和回放缓冲区的需求。相反，AL GNN利用解析学习理论原理，将学习公式化为递归最小二乘优化过程。它通过封闭形式的分类器更新和正则化特征自相关矩阵来分析和更新模型知识。这种设计使得每个任务只需高效的单次训练，并通过避免存储历史样本来本质上保护数据隐私。在多个动态图分类基准上的广泛实验表明，与现有方法相比，AL GNN实现了竞争性或更优的性能。例如，它在CoraFull上平均性能提高了10%，在Reddit上减少了超过30%的遗忘，同时由于其无反向传播的设计，训练时间减少了近50%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual graph learning (CGL) aims to enable graph neural networks to incrementally learn from a stream of graph structured data without forgetting previously acquired knowledge. Existing methods particularly those based on experience replay typically store and revisit past graph data to mitigate catastrophic forgetting. However, these approaches pose significant limitations, including privacy concerns, inefficiency. In this work, we propose AL GNN, a novel framework for continual graph learning that eliminates the need for backpropagation and replay buffers. Instead, AL GNN leverages principles from analytic learning theory to formulate learning as a recursive least squares optimization process. It maintains and updates model knowledge analytically through closed form classifier updates and a regularized feature autocorrelation matrix. This design enables efficient one pass training for each task, and inherently preserves data privacy by avoiding historical sample storage. Extensive experiments on multiple dynamic graph classification benchmarks demonstrate that AL GNN achieves competitive or superior performance compared to existing methods. For instance, it improves average performance by 10% on CoraFull and reduces forgetting by over 30% on Reddit, while also reducing training time by nearly 50% due to its backpropagation free design.</description>
      <author>example@mail.com (Xuling Zhang, Jindong Li, Yifei Zhang, Menglin Yang)</author>
      <guid isPermaLink="false">2512.18295v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation</title>
      <link>http://arxiv.org/abs/2512.18232v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AutoSchA的新方法，利用图神经网络实现自动分层音乐分析，该方法在分析巴洛克赋格主题时表现与人类专家相当。&lt;h4&gt;背景&lt;/h4&gt;分层表示为分析多种音乐流派提供了强大且系统的方法，如Schenkerian分析。然而，分层音乐分析成本高昂，需要专家投入大量时间和精力，且将其表示为计算机可读格式存在挑战。&lt;h4&gt;目的&lt;/h4&gt;利用分层深度学习和计算机可读数据量增加的最新进展，建立一个自动的分层音乐表示框架。&lt;h4&gt;方法&lt;/h4&gt;提出AutoSchA方法，扩展了图神经网络在分层音乐分析中的应用。该方法包含三个关键贡献：新的分层音乐表示图学习框架；基于节点隔离的新图池化机制；以及集成这些发展的最先进架构。&lt;h4&gt;主要发现&lt;/h4&gt;在一系列实验中，AutoSchA在分析巴洛克赋格主题时表现可与人类专家相媲美。&lt;h4&gt;结论&lt;/h4&gt;AutoSchA是一种有前景的自动分层音乐分析方法，能够达到专家级别的性能。&lt;h4&gt;翻译&lt;/h4&gt;分层表示为分析许多音乐流派提供了强大而系统的方法。这种表示方法已经在音乐理论中得到广泛研究，例如通过Schenkerian分析(SchA)。然而，分层音乐分析成本很高；分析一首音乐需要专家投入大量的时间和精力。将分层分析表示为计算机可读格式是另一个挑战。鉴于最近在分层深度学习和计算机可读数据量增加方面的进展，将此类工作扩展到自动分层表示框架非常有前景。因此，本文引入了一种新方法AutoSchA，它扩展了图神经网络(GNNs)在分层音乐分析中的最新进展。AutoSchA具有三个关键贡献：1)一种用于分层音乐表示的新图学习框架；2)一种基于节点隔离的新图池化机制，直接优化学习的池化分配；3)一种集成这些发展用于自动分层音乐分析的最先进架构。我们在一系列实验中表明，当分析巴洛克赋格主题时，AutoSchA的表现可与人类专家相媲美。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical representations provide powerful and principled approaches for analyzing many musical genres. Such representations have been broadly studied in music theory, for instance via Schenkerian analysis (SchA). Hierarchical music analyses, however, are highly cost-intensive; the analysis of a single piece of music requires a great deal of time and effort from trained experts. The representation of hierarchical analyses in a computer-readable format is a further challenge. Given recent developments in hierarchical deep learning and increasing quantities of computer-readable data, there is great promise in extending such work for an automatic hierarchical representation framework. This paper thus introduces a novel approach, AutoSchA, which extends recent developments in graph neural networks (GNNs) for hierarchical music analysis. AutoSchA features three key contributions: 1) a new graph learning framework for hierarchical music representation, 2) a new graph pooling mechanism based on node isolation that directly optimizes learned pooling assignments, and 3) a state-of-the-art architecture that integrates such developments for automatic hierarchical music analysis. We show, in a suite of experiments, that AutoSchA performs comparably to human experts when analyzing Baroque fugue subjects.</description>
      <author>example@mail.com (Stephen Ni-Hahn, Rico Zhu, Jerry Yin, Yue Jiang, Cynthia Rudin, Simon Mak)</author>
      <guid isPermaLink="false">2512.18232v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization</title>
      <link>http://arxiv.org/abs/2512.18228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the author-accepted manuscript of a paper published in Automated Software Engineering Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GraphRank的新型测试输入优先排序框架，用于图神经网络(GNNs)的测试，通过结合模型感知属性和模型无关属性，并利用图结构信息提高测试效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理图结构数据方面表现出色，但部署后会出现故障，可能导致严重后果。全面测试需要大量手动标注数据，增加了成本。&lt;h4&gt;目的&lt;/h4&gt;降低标注成本，有策略地优先选择高质量无标签输入进行测试，在有限预算下发现更多模型故障。&lt;h4&gt;方法&lt;/h4&gt;GraphRank框架引入模型无关属性弥补模型感知属性的局限性，利用图结构信息聚合相邻节点属性增强特征，结合二元分类器作为排序模型，并通过迭代训练提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明GraphRank在测试输入优先排序方面优于现有技术。&lt;h4&gt;结论&lt;/h4&gt;GraphRank有效解决了现有测试输入优先排序技术的局限性，提高了GNNs测试的效率和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在处理图结构数据方面表现出色；然而，它们在部署后会出现故障，可能导致严重后果。因此，部署前进行全面测试对于确保GNNs的可靠性变得至关重要。然而，全面测试需要大量手动标注的测试数据。为了降低标注成本，有策略地优先选择和标注高质量的无标签输入用于测试变得至关重要，这有助于在有限的标注预算下发现更多的模型故障。不幸的是，现有的测试输入优先排序技术要么忽略了图结构中包含的有价值信息，要么过度依赖于从目标模型提取的属性（即模型感知属性），而这些属性的质量可能差异很大。为了解决这些问题，我们提出了一个名为GraphRank的新型测试输入优先排序框架，专门用于GNNs。GraphRank引入了模型无关属性来弥补模型感知属性的局限性。它还利用图结构信息来聚合来自相邻节点的属性，从而增强模型感知和模型无关属性。此外，GraphRank将上述属性与二元分类器结合，使用它作为排序模型来优先处理输入。该分类器经过迭代训练，使其能够从每轮的反馈中学习并相应地提高其性能。大量实验证明了GraphRank相对于现有技术的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s10515-025-00554-0&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable efficacy in handling graph-structured data; however, they exhibit failures after deployment, which can cause severe consequences. Hence, conducting thorough testing before deployment becomes imperative to ensure the reliability of GNNs. However, thorough testing requires numerous manually annotated test data. To mitigate the annotation cost, strategically prioritizing and labeling high-quality unlabeled inputs for testing becomes crucial, which facilitates uncovering more model failures with a limited labeling budget. Unfortunately, existing test input prioritization techniques either overlook the valuable information contained in graph structures or are overly reliant on attributes extracted from the target model, i.e., model-aware attributes, whose quality can vary significantly. To address these issues, we propose a novel test input prioritization framework, named GraphRank, for GNNs. GraphRank introduces model-agnostic attributes to compensate for the limitations of the model-aware ones. It also leverages the graph structure information to aggregate attributes from neighboring nodes, thereby enhancing the model-aware and model-agnostic attributes. Furthermore, GraphRank combines the above attributes with a binary classifier, using it as a ranking model to prioritize inputs. This classifier undergoes iterative training, which enables it to learn from each round's feedback and improve its performance accordingly. Extensive experiments demonstrate GraphRank's superiority over existing techniques.</description>
      <author>example@mail.com (Lichen Yang, Qiang Wang, Zhonghao Yang, Daojing He, Yu Li)</author>
      <guid isPermaLink="false">2512.18228v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>PROVEX: Enhancing SOC Analyst Trust with Explainable Provenance-Based IDS</title>
      <link>http://arxiv.org/abs/2512.18199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种全面的XAI框架，用于增强基于图神经网络的入侵检测系统的可解释性，使安全分析师能够理解系统为何发出警报，从而提高信任度和分类速度。&lt;h4&gt;背景&lt;/h4&gt;现代入侵检测系统利用图神经网络检测系统来源数据中的恶意活动，但这些系统的决策过程对分析师来说往往是一个黑箱，缺乏透明度。&lt;h4&gt;目的&lt;/h4&gt;开发一个可解释人工智能框架，通过使基于图的检测透明化，弥合安全运营中心中的信任差距，使分析师能够理解模型决策的原因。&lt;h4&gt;方法&lt;/h4&gt;在KAIROS（一种先进的基于时间图的入侵检测系统）上实现了该框架，并集成了三种GNN解释方法（GraphMask、GNNExplainer和VA-TGExplainer），使其适用于时间来源上下文，通过事后解释突出显示触发警报的关键因果子图和事件。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够生成人类可解释的异常行为表示，包括重要边和不确定性估计；在DARPA CADETS数据集上测试显示，解释器以高保真度保留了原始模型的决策，突出了恶意文件交互和异常网络流量等关键特征；平均解释开销为每个事件3-5秒。&lt;h4&gt;结论&lt;/h4&gt;通过提供模型推理的洞察，该框架有效提高了分析师对入侵检测系统的信任度和事件分类速度，同时保持了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;现代入侵检测系统利用图神经网络来检测系统来源数据中的恶意活动，但它们的决策对分析师来说往往是一个黑箱。本文提出了一个全面的XAI框架，旨在通过使基于图的检测透明化，弥合安全运营中心中的信任差距。我们在KAIROS（一种最先进的基于时间图的IDS）上实现了这个框架，尽管我们的设计可以以最小的适应应用于任何基于时间图的检测器。完整代码库可在https://github.com/devang1304/provex.git获取。我们通过事后解释增强了检测管道，突出了触发警报的原因，识别关键的因果子图和事件。我们使三种GNN解释方法适应时间来源上下文：GraphMask、GNNExplainer和变分时间GNN解释器（VA-TGExplainer）。这些工具输出异常行为的人类可解释表示，包括重要的边和不确定性估计。我们的贡献专注于这些解释器的实际集成，解决了内存管理和可重复性方面的挑战。我们在DARPA CADETS Engagement 3数据集上展示了我们的框架，并证明它为检测到的攻击生成简洁的窗口级解释。我们的评估揭示了这些解释器以高保真度保留了TGNN的决策，突出了关键的边，如恶意文件交互和异常网络流量。平均解释开销为每个事件3-5秒。通过提供模型推理的洞察，我们的框架旨在提高分析师的信任度和分类速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern intrusion detection systems (IDS) leverage graph neural networks (GNNs) to detect malicious activity in system provenance data, but their decisions often remain a black box to analysts. This paper presents a comprehensive XAI framework designed to bridge the trust gap in Security Operations Centers (SOCs) by making graph-based detection transparent. We implement this framework on top of KAIROS, a state-of-the-art temporal graph-based IDS, though our design is applicable to any temporal graph-based detector with minimal adaptation. The complete codebase is available at https://github.com/devang1304/provex.git. We augment the detection pipeline with post-hoc explanations that highlight why an alert was triggered, identifying key causal subgraphs and events. We adapt three GNN explanation methods - GraphMask, GNNExplainer, and a variational temporal GNN explainer (VA-TGExplainer) - to the temporal provenance context. These tools output human-interpretable representations of anomalous behavior, including important edges and uncertainty estimates. Our contributions focus on the practical integration of these explainers, addressing challenges in memory management and reproducibility. We demonstrate our framework on the DARPA CADETS Engagement 3 dataset and show that it produces concise window-level explanations for detected attacks. Our evaluation reveals that the explainers preserve the TGNN's decisions with high fidelity, surfacing critical edges such as malicious file interactions and anomalous netflows. The average explanation overhead is 3-5 seconds per event. By providing insight into the model's reasoning, our framework aims to improve analyst trust and triage speed.</description>
      <author>example@mail.com (Devang Dhanuka, Nidhi Rastogi)</author>
      <guid isPermaLink="false">2512.18199v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>On Swarm Leader Identification using Probing Policies</title>
      <link>http://arxiv.org/abs/2512.18146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种交互式群体领导者识别（iSLI）方法，通过对抗性探测代理与机器人群体成员的物理交互来识别领导者。研究将问题建模为POMDP，并采用深度强化学习训练探测策略，创新的神经网络架构结合了时间图关系层（TGR）和简化结构状态空间序列（S5）模型，在模拟和真实机器人实验中均表现出色。&lt;h4&gt;背景&lt;/h4&gt;在机器人群体中识别领导者至关重要，特别是在对抗环境中，领导者隐藏对于任务成功是必要的。传统的领导者识别方法可能难以应对动态和对抗性的环境挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖的交互式群体领导者识别方法，通过对抗性探测代理与群体成员的物理交互来识别领导者，并验证该方法在不同条件下的有效性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;将iSLI问题建模为部分可观察马尔可夫决策过程（POMDP），使用深度强化学习中的近端策略优化（PPO）算法训练探测代理，设计创新的神经网络架构结合时间图关系层（TGR）和简化结构状态空间序列（S5）模型，TGR层处理基于图的群体观测并捕获时间依赖性，进行广泛的模拟实验和真实机器人实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;基于TGR的模型优于基准图神经网络架构，展现出显著的零样本泛化能力；训练后的探测代理能高精度识别领导者，即使在训练分布之外的场景中也能保持性能；预测表现出适当的置信度；真实机器人实验证实了模拟到现实的迁移能力；方法对动态变化（如意外代理断开连接）具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的iSLI方法结合创新的神经网络架构，能够在对抗环境中有效识别机器人群体中的领导者，具有良好的泛化能力和鲁棒性，适用于真实世界的应用场景。&lt;h4&gt;翻译&lt;/h4&gt;识别机器人群体中的领导者至关重要，特别是在需要隐藏领导者以确保任务成功的对抗环境中。这项工作引入了交互式群体领导者识别（iSLI）问题，这是一种新颖的方法，其中对抗性探测代理通过与群体成员的物理交互来识别群体的领导者。我们将iSLI问题表述为部分可观察马尔可夫决策过程（POMDP），并采用深度强化学习，特别是近端策略优化（PPO），来训练探测代理的策略。提出的方法利用了一种新颖的神经网络架构，具有时间图关系层（TGR）和简化结构状态空间序列（S5）模型。TGR层有效地处理基于图的群体观测，捕获时间依赖性并使用学习的门控机制融合关系信息，为策略学习生成信息丰富的表示。广泛的模拟表明，我们的基于TGR的模型优于基准图神经网络架构，并在不同于训练所使用的不同群体大小和速度上展现出显著的零样本泛化能力。训练后的探测代理在识别领导者方面具有高准确性，即使在训练分布之外的场景中也能保持性能，并在其预测中表现出适当的置信度水平。使用物理机器人的真实世界实验进一步验证了该方法，证实了成功的模拟到现实迁移以及对动态变化（如意外代理断开连接）的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying the leader within a robotic swarm is crucial, especially in adversarial contexts where leader concealment is necessary for mission success. This work introduces the interactive Swarm Leader Identification (iSLI) problem, a novel approach where an adversarial probing agent identifies a swarm's leader by physically interacting with its members. We formulate the iSLI problem as a Partially Observable Markov Decision Process (POMDP) and employ Deep Reinforcement Learning, specifically Proximal Policy Optimization (PPO), to train the prober's policy. The proposed approach utilizes a novel neural network architecture featuring a Timed Graph Relationformer (TGR) layer combined with a Simplified Structured State Space Sequence (S5) model. The TGR layer effectively processes graph-based observations of the swarm, capturing temporal dependencies and fusing relational information using a learned gating mechanism to generate informative representations for policy learning. Extensive simulations demonstrate that our TGR-based model outperforms baseline graph neural network architectures and exhibits significant zero-shot generalization capabilities across varying swarm sizes and speeds different from those used during training. The trained prober achieves high accuracy in identifying the leader, maintaining performance even in out-of-training distribution scenarios, and showing appropriate confidence levels in its predictions. Real-world experiments with physical robots further validate the approach, confirming successful sim-to-real transfer and robustness to dynamic changes, such as unexpected agent disconnections.</description>
      <author>example@mail.com (Stergios E. Bachoumas, Panagiotis Artemiadis)</author>
      <guid isPermaLink="false">2512.18146v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations</title>
      <link>http://arxiv.org/abs/2512.17984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种混合归纳-转导网络(HINT)和相应的训练策略，用于解决未监测位置的交通流量预测问题，通过结合归纳式流量预测与转导速度、交通模拟和外部地理空间信息，显著提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;准确预测未监测位置的交通流量存在挑战：环形检测器提供精确但稀疏的测量数据；探测车辆的速度数据广泛可用但与流量相关性弱；附近路段通常表现出强烈的流量规模异质性(如匝道与主线)，这打破了标准图神经网络(GNN)的假设。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合归纳-转导网络(HINT)和相应的INDU-TRANSDUCTIVE训练策略，将速度视为转导的、网络范围的信号，同时归纳地学习流量以推广到未见过位置，提高交通流量预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;HINT结合了三个组件：(i)归纳式空间变换器，学习相似度驱动的长程交互；(ii)基于FiLM条件化的扩散GCN，利用丰富的静态上下文；(iii)节点级校准层，纠正尺度偏差。训练使用掩码重建、节点采样、困难节点挖掘和噪声注入，图结构基于驾驶距离构建。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集(MOW、UTD19-Torino和UTD19-Essen)上，HINT始终优于最先进的归纳基线。相比KITS基线，HINT在MOW上将MAE减少了约42%(基本模拟)和50%(校准模拟)，在Torino上减少约22%，在Essen上减少约12%。即使没有模拟，HINT在MOW和Torino上仍表现更好，而在Essen上模拟至关重要。&lt;h4&gt;结论&lt;/h4&gt;将归纳流量预测与转导速度、交通模拟和外部地理空间信息相结合，可以显著提高未监测位置交通流量预测的准确性，为解决交通监测数据稀疏性问题提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;准确预测未监测位置的交通流量很困难：环形检测器提供精确但稀疏的测量，探测车辆的速度数据广泛可用但与流量相关性弱，且附近路段通常表现出强烈的流量规模异质性(例如匝道与主线)，这打破了标准GNN的假设。我们提出了HINT，一种混合归纳-转导网络，以及一种INDU-TRANSDUCTIVE训练策略，将速度视为转导的、网络范围的信号，同时归纳地学习流量以推广到未见过位置。HINT结合了(i)一个归纳式空间变换器，从节点特征学习相似度驱动的长程交互；(ii)一个基于FiLM条件化的扩散GCN，利用丰富的静态上下文(从OSM派生的属性和交通模拟)；以及(iii)一个节点级校准层，纠正每段的尺度偏差。训练使用掩码重建，按节点采样，困难节点挖掘以强调困难传感器，并在可见流量上注入噪声以防止恒等映射，同时图结构基于驾驶距离构建。在三个真实世界数据集上，MOW(比利时安特卫普)、UTD19-Torino和UTD19-Essen，HINT始终超越最先进的归纳基线。相对于KITS，HINT在MOW上将MAE减少了约42%(基本模拟)和约50%(校准模拟)，在Torino上减少了约22%，在Essen上减少了约12%。即使没有模拟，HINT在MOW和Torino上仍然表现更好，而在Essen上模拟至关重要。这些结果表明，将归纳流量预测与转导速度、交通模拟和外部地理空间相结合可以提高上述任务的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately imputing traffic flow at unsensed locations is difficult: loop detectors provide precise but sparse measurements, speed from probe vehicles is widely available yet only weakly correlated with flow, and nearby links often exhibit strong heterophily in the scale of traffic flow (e.g., ramps vs. mainline), which breaks standard GNN assumptions. We propose HINT, a Hybrid INductive-Transductive Network, and an INDU-TRANSDUCTIVE training strategy that treats speed as a transductive, network-wide signal while learning flow inductively to generalize to unseen locations. HINT couples (i) an inductive spatial transformer that learns similarity-driven, long-range interactions from node features with (ii) a diffusion GCN conditioned by FiLM on rich static context (OSM-derived attributes and traffic simulation), and (iii) a node-wise calibration layer that corrects scale biases per segment. Training uses masked reconstruction with epoch-wise node sampling, hard-node mining to emphasize difficult sensors, and noise injection on visible flows to prevent identity mapping, while graph structure is built from driving distances.  Across three real-world datasets, MOW (Antwerp, Belgium), UTD19-Torino, and UTD19-Essen, HINT consistently surpasses state-of-the-art inductive baselines. Relative to KITS, HINT reduces MAE on MOW by $\approx42$% with basic simulation and $\approx50$% with calibrated simulation; on Torino by $\approx22$%, and on Essen by $\approx12$%. Even without simulation, HINT remains superior on MOW and Torino, while simulation is crucial on Essen. These results show that combining inductive flow imputation with transductive speed, traffic simulations and external geospatial improves accuracy for the task described above.</description>
      <author>example@mail.com (Mohammadmahdi Rahimiasl, Ynte Vanderhoydonc, Siegfried Mercelis)</author>
      <guid isPermaLink="false">2512.17984v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Deep learning directed synthesis of fluid ferroelectric materials</title>
      <link>http://arxiv.org/abs/2512.16671v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  104 pages, 76 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于深度学习的从数据到分子的流水线，用于有目标地设计和合成新型有机流体铁电体，实现了对可合成流体铁电体的闭环发现方法。&lt;h4&gt;背景&lt;/h4&gt;流体铁电体是一类新发现的液晶材料，具有可切换的长程极化有序性，在超快电光技术、响应性软物质和下一代能量材料方面有应用前景。然而，该领域的进展几乎完全依赖于直觉和偶然发现，限制了领域发展。&lt;h4&gt;目的&lt;/h4&gt;开发并实验验证一种深度学习数据到分子流水线，实现对新型有机流体铁电体的有目标设计和合成。&lt;h4&gt;方法&lt;/h4&gt;1. 收集所有已知纵向极化液晶材料数据集；2. 训练图神经网络预测铁电行为；3. 使用图变分自编码器生成全新分子结构；4. 通过分类器和回归器过滤候选物；5. 结合计算逆合成引擎和数字化化学库存缩小设计空间；6. 合成并表征11个候选物；7. 比较实验结果与神经网络预测。&lt;h4&gt;主要发现&lt;/h4&gt;1. 开发了准确率高达95%的图神经网络预测铁电行为；2. 成功生成了具有预测铁电液晶行为的全新分子结构；3. 实验验证了新型材料的存在；4. 实验验证的材料增强了数据集质量，有助于未来研究。&lt;h4&gt;结论&lt;/h4&gt;这些结果展示了一种实用的、闭环的发现可合成流体铁电体的方法，标志着向自主设计功能性软材料迈出了一步。&lt;h4&gt;翻译&lt;/h4&gt;流体铁电体是一类最近发现的液晶，它们表现出可切换的长程极化有序性，为超快电光技术、响应性软物质和下一代能量材料提供了机会。然而，它们的发现几乎完全依赖于直觉和偶然，限制了该领域的进展。在这里，我们开发和实验验证了一种深度学习数据到分子的流水线，使新型有机流体铁电体的有目标设计和合成成为可能。我们整理了所有已知的纵向极化液晶材料的综合数据集，并训练了图神经网络，这些网络能够以高达95%的准确率预测铁电行为，并将转变温度的均方根误差低至11K。图变分自编码器生成全新的分子结构，使用高性能分类器和回归器组合进行过滤，以识别具有预测铁电液晶行为和可及转变温度的候选分子。与计算逆合成引擎和数字化化学库存的进一步整合将设计空间缩小到合成就绪的长名单。通过既定的基于混合物的外推方法合成并表征了11个候选物。从中，外推的铁电液晶转变与神经网络预测进行了比较。新型材料的实验验证通过质量反馈数据增强了原始数据集，从而有助于未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fluid ferroelectrics, a recently discovered class of liquid crystals that exhibit switchable, long-range polar order, offer opportunities in ultrafast electro-optic technologies, responsive soft matter, and next-generation energy materials. Yet their discovery has relied almost entirely on intuition and chance, limiting progress in the field. Here we develop and experimentally validate a deep-learning data-to-molecule pipeline that enables the targeted design and synthesis of new organic fluid ferroelectrics. We curate a comprehensive dataset of all known longitudinally polar liquid-crystal materials and train graph neural networks that predict ferroelectric behaviour with up to 95% accuracy and achieve root mean square errors as low as 11 K for transition temperatures. A graph variational autoencoder generates de novo molecular structures which are filtered using an ensemble of high-performing classifiers and regressors to identify candidates with predicted ferroelectric nematic behaviour and accessible transition temperatures. Integration with a computational retrosynthesis engine and a digitised chemical inventory further narrows the design space to a synthesis-ready longlist. 11 candidates were synthesised and characterized through established mixture-based extrapolation methods. From which extrapolated ferroelectric nematic transitions were compared against neural network predictions. The experimental verification of novel materials augments the original dataset with quality feedback data thus aiding future research. These results demonstrate a practical, closed-loop approach to discovering synthesizable fluid ferroelectrics, marking a step toward autonomous design of functional soft materials.</description>
      <author>example@mail.com (Charles Parton-Barr, Stuart R. Berrow, Calum J. Gibb, Jordan Hobbs, Wanhe Jiang, Caitlin O'Brien, Will C. Ogle, Helen F. Gleeson, Richard J. Mandle)</author>
      <guid isPermaLink="false">2512.16671v2</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance</title>
      <link>http://arxiv.org/abs/2512.16661v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于注意力的子图检索器，这是一种GNN-as-retriever模型，应用基于注意力的剪枝技术提取精细的子图，然后将子图传递给大型语言模型进行高级知识推理。&lt;h4&gt;背景&lt;/h4&gt;在当今信息驱动的世界，获取科学出版物变得越来越容易，但同时从大量可用研究中筛选出有价值的信息变得比以往更具挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于注意力的子图检索器模型，用于提高科学文献检索和知识推理的效率。&lt;h4&gt;方法&lt;/h4&gt;提出一种GNN-as-retriever模型，应用基于注意力的剪枝技术提取精细的子图，然后将提取的子图传递给大型语言模型进行高级知识推理。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络和图注意力机制在搜索大规模信息数据库方面显示出强大的有效性，当与现代大型语言模型结合时，效果更佳。&lt;h4&gt;结论&lt;/h4&gt;该模型旨在通过结合GNN和大型语言模型的能力，提高科学文献检索和知识推理的效率。&lt;h4&gt;翻译&lt;/h4&gt;在当今信息驱动的世界中，获取科学出版物变得越来越容易。与此同时，从大量可用研究中进行筛选比以往任何时候都更具挑战性。图神经网络和图注意力机制在搜索大规模信息数据库方面显示出强大的有效性，特别是与现代大型语言模型结合时。在本文中，我们提出了一种基于注意力的子图检索器，这是一种GNN-as-retriever模型，应用基于注意力的剪枝技术提取精细的子图，然后将其传递给大型语言模型进行高级知识推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.</description>
      <author>example@mail.com (Shikshya Shiwakoti, Samuel Goldsmith, Ujjwal Pandit)</author>
      <guid isPermaLink="false">2512.16661v2</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models</title>
      <link>http://arxiv.org/abs/2512.19509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by FSE 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了编程语言之间的深层关系，并提出了一种基于嵌入的框架来揭示编程语言的潜在家族，进而优化多语言代码大语言模型的训练和推理。&lt;h4&gt;背景&lt;/h4&gt;多种编程语言的快速增长为开发多语言代码大语言模型带来机遇和挑战，但现有技术通常仅通过简单聚合多语言代码数据来训练，很少探索编程语言间的深层关系及其对模型训练的优化作用。&lt;h4&gt;目的&lt;/h4&gt;研究两个基本问题：1) 编程语言之间有哪些深层语言关系？2) 如何利用这些关系改进多语言代码大语言模型？&lt;h4&gt;方法&lt;/h4&gt;提出基于嵌入的框架，定义21种编程语言主要特征，使用大语言模型生成多语言特征对齐代码样本，通过嵌入19种语言的语义平行代码片段构建相似性矩阵，执行层次聚类发现语言关系。&lt;h4&gt;主要发现&lt;/h4&gt;编程语言间存在清晰层次结构，关系密切的语言形成明确定义的簇(如C、C++、Java和Swift)，而Go作为中心语言具有最高的跨语言相似性。&lt;h4&gt;结论&lt;/h4&gt;基于发现的语言家族，提出三种增强多语言大语言模型训练的策略：语言相关间的迁移学习、语言接近度引导的课程学习和基于质心的中间代码翻译，实验证明这些方法显著提升了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;多种编程语言的快速增长为开发多语言代码大语言模型带来了机遇和挑战。虽然现有技术通常通过简单地聚合多语言代码数据来训练代码大语言模型，但很少探索编程语言之间的深层关系以及如何利用这些关系来优化代码大语言模型训练和推理。在本工作中，我们研究了两个基本问题：1) 编程语言之间的深层语言关系是什么？以及2) 如何利用这些关系来改进多语言代码大语言模型？我们提出了一种基于嵌入的框架来揭示编程语言的潜在家族。我们的方法首先定义了21种编程语言的主要语言特征，如变量定义、控制结构和方法声明，然后使用大语言模型在多种语言中生成特征对齐的代码样本。通过嵌入来自19种语言的语义平行代码片段，我们构建了一个相似性矩阵并进行层次聚类以发现固有的语言关系。我们的分析揭示了编程语言之间的清晰层次结构。关系密切的语言形成明确定义的簇(例如，C、C++、Java和Swift组合在一起)，而Go表现出作为中心语言的特点，具有最高的跨语言相似性。基于发现的语言家族，我们提出了三种策略来增强多语言大语言模型训练：在语言相关的语言之间进行迁移学习、语言接近度引导的课程学习以及基于质心的中间代码翻译。在4个代码智能任务上的实验证明，我们的方法显著提高了多语言大语言模型的性能。这项工作为编程语言提供了通用视角，并推进了更有效的多语言代码大语言模型训练策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.</description>
      <author>example@mail.com (Shangbo Yun, Xiaodong Gu, Jianghong Huang, Beijun Shen)</author>
      <guid isPermaLink="false">2512.19509v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Controllable Probabilistic Forecasting with Stochastic Decomposition Layers</title>
      <link>http://arxiv.org/abs/2512.18815v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为随机分解层(Stochastic Decomposition Layers, SDL)的新方法，用于将确定性机器学习天气模型转换为概率性集合预测系统。SDL基于StyleGAN的分层噪声注入，通过潜在驱动的调制、逐像素噪声和通道缩放在三个解码器尺度上应用扰动。该方法计算成本低，仅需不到2%的基线模型训练成本，且每个集合成员由紧凑的潜在张量(5 MB)生成，可实现完美可重复性和后推理扩散调整。&lt;h4&gt;背景&lt;/h4&gt;基于潜在噪声注入并通过连续排名概率分数(CRPS)优化的AI天气预测集合已经产生了准确且校准良好的预测，与基于扩散的方法相比计算成本大大降低。然而，当前的CRPS集合方法在训练策略和噪声注入机制上存在差异，大多数通过条件归一化在整个网络中全局注入噪声，这种结构增加了训练成本，并限制了随机扰动的物理解释性。&lt;h4&gt;目的&lt;/h4&gt;引入随机分解层(Stochastic Decomposition Layers)，用于将确定性机器学习天气模型转换为概率性集合系统，解决现有方法训练成本高和物理解释性差的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 基于StyleGAN的分层噪声注入机制设计SDL；2. 在三个解码器尺度上应用学习到的扰动：潜在驱动的调制、逐像素噪声和通道缩放；3. 通过迁移学习将SDL应用于WXFormer模型；4. 使用紧凑的潜在张量(5 MB)生成每个集合成员；5. 通过潜在重缩放实现后推理扩散调整；6. 在2022年ERA5再分析数据集上评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. SDL仅需不到2%的基线模型训练计算成本；2. 集合的扩散-技能比接近 unity，排名直方图在中等范围预报中逐渐趋于均匀；3. 校准性能可与业务IFS-ENS相竞争；4. 多尺度实验揭示了分层不确定性：粗层调节天气尺度模式，细层控制中尺度变异性；5. 显式的潜在参数化为业务预报和气候应用提供了可解释的不确定性量化。&lt;h4&gt;结论&lt;/h4&gt;SDL提供了一种高效且可解释的方法来生成天气预测集合，它不仅计算成本低，而且能够提供物理上有意义的不确定性表示，对业务预报和气候应用具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;AI天气预测集合通过潜在噪声注入并用连续排名概率分数(CRPS)优化，与基于扩散的方法相比，产生了既准确又校准良好的预测，且计算成本大大降低。然而，当前的CRPS集合方法在训练策略和噪声注入机制上各不相同，大多数通过条件归一化在整个网络中全局注入噪声。这种结构增加了训练成本，并限制了随机扰动的物理解释性。我们引入了随机分解层(Stochastic Decomposition Layers, SDL)，用于将确定性机器学习天气模型转换为概率性集合系统。SDL借鉴了StyleGAN的分层噪声注入，通过潜在驱动的调制、逐像素噪声和通道缩放，在三个解码器尺度上应用学习到的扰动。当通过迁移学习应用于WXFormer时，SDL需要的计算成本不到基线模型训练的2%。每个集合成员由紧凑的潜在张量(5 MB)生成，实现了完美可重复性，并通过潜在重缩放实现后推理扩散调整。在2022年ERA5再分析数据集上的评估显示，集合的扩散-技能比接近 unity，排名直方图在中等范围预报中逐渐趋于均匀，校准性能可与业务IFS-ENS相媲美。多尺度实验揭示了分层不确定性：粗层调节天气尺度模式，而细层控制中尺度变异性。显式的潜在参数化为业务预报和气候应用提供了可解释的不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI weather prediction ensembles with latent noise injection and optimized with the continuous ranked probability score (CRPS) have produced both accurate and well-calibrated predictions with far less computational cost compared with diffusion-based methods. However, current CRPS ensemble approaches vary in their training strategies and noise injection mechanisms, with most injecting noise globally throughout the network via conditional normalization. This structure increases training expense and limits the physical interpretability of the stochastic perturbations. We introduce Stochastic Decomposition Layers (SDL) for converting deterministic machine learning weather models into probabilistic ensemble systems. Adapted from StyleGAN's hierarchical noise injection, SDL applies learned perturbations at three decoder scales through latent-driven modulation, per-pixel noise, and channel scaling. When applied to WXFormer via transfer learning, SDL requires less than 2\% of the computational cost needed to train the baseline model. Each ensemble member is generated from a compact latent tensor (5 MB), enabling perfect reproducibility and post-inference spread adjustment through latent rescaling. Evaluation on 2022 ERA5 reanalysis shows ensembles with spread-skill ratios approaching unity and rank histograms that progressively flatten toward uniformity through medium-range forecasts, achieving calibration competitive with operational IFS-ENS. Multi-scale experiments reveal hierarchical uncertainty: coarse layers modulate synoptic patterns while fine layers control mesoscale variability. The explicit latent parameterization provides interpretable uncertainty quantification for operational forecasting and climate applications.</description>
      <author>example@mail.com (John S. Schreck, William E. Chapman, Charlie Becker, David John Gagne, Dhamma Kimpara, Nihanth Cherukuru, Judith Berner, Kirsten J. Mayer, Negin Sobhani)</author>
      <guid isPermaLink="false">2512.18815v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System</title>
      <link>http://arxiv.org/abs/2512.18269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7page&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种视觉暗模式检测框架，通过构建专有数据集和应用YOLOv12x模型，实现了高准确性和实时性能的暗模式检测。&lt;h4&gt;背景&lt;/h4&gt;数字化转型加速和在线平台普及导致暗模式问题日益突出，企业平台设计策略日益复杂，而监管机构主要采用被动方法，需要主动和实时检测技术。&lt;h4&gt;目的&lt;/h4&gt;提出一种视觉暗模式检测框架，提高检测准确性和实时性能。&lt;h4&gt;方法&lt;/h4&gt;构建包含4,066个UI/UX截图的专有数据集，标注五种暗模式相关UI组件，采用YOLOv12x目标检测模型和应用迁移学习优化性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在检测准确率方面达到92.8%，同时保持40.5帧每秒的实时推理速度，确认了在实际在线环境中部署的有效性。&lt;h4&gt;结论&lt;/h4&gt;构建的数据集已公开发布，支持暗模式检测领域的进一步研究和开发，数据集可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;随着数字化转型步伐的加快和在线平台的广泛采用，关于暗模式（即削弱用户做出明智和理性选择能力的用户界面设计）的社会和技术问题日益突出。随着企业在线平台在设计策略上变得更加复杂，监管机构主要采用的被动方法之外，迫切需要主动和实时检测技术。在本文中，我们提出了一种视觉暗模式检测框架，提高了检测准确性和实时性能。为此，我们通过手动收集来自韩国和国外六个主要行业的194个网站的4,066个包含暗模式的UI/UX截图，构建了一个专有的视觉目标检测数据集。收集的图像被标注了五种与暗模式相关的代表性UI组件：按钮、复选框、输入字段、弹出窗口和二维码。该数据集已公开发布，以支持该领域的进一步研究和开发。为实现实时检测，本研究采用了YOLOv12x目标检测模型，并应用迁移学习优化其性能以实现视觉暗模式识别。实验结果表明，所提出的方法在检测准确率方面达到了92.8%，同时保持40.5帧每秒的实时推理速度，证实了其在在线环境中实际部署的有效性。此外，为促进未来研究并为技术进步做出贡献，本研究构建的数据集已在GitHub上公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the accelerating pace of digital transformation and the widespread adoption of online platforms, both social and technical concerns regarding dark patterns-user interface designs that undermine users' ability to make informed and rational choices-have become increasingly prominent. As corporate online platforms grow more sophisticated in their design strategies, there is a pressing need for proactive and real-time detection technologies that go beyond the predominantly reactive approaches employed by regulatory authorities. In this paper, we propose a visual dark pattern detection framework that improves both detection accuracy and real-time performance. To this end, we constructed a proprietary visual object detection dataset by manually collecting 4,066 UI/UX screenshots containing dark patterns from 194 websites across six major industrial sectors in South Korea and abroad. The collected images were annotated with five representative UI components commonly associated with dark patterns: Button, Checkbox, Input Field, Pop-up, and QR Code. This dataset has been publicly released to support further research and development in the field. To enable real-time detection, this study adopted the YOLOv12x object detection model and applied transfer learning to optimize its performance for visual dark pattern recognition. Experimental results demonstrate that the proposed approach achieves a high detection accuracy of 92.8% in terms of mAP@50, while maintaining a real-time inference speed of 40.5 frames per second (FPS), confirming its effectiveness for practical deployment in online environments. Furthermore, to facilitate future research and contribute to technological advancements, the dataset constructed in this study has been made publicly available at https://github.com/B4E2/B4E2-DarkPattern-YOLO-DataSet.</description>
      <author>example@mail.com (Se-Young Jang, Su-Yeon Yoon, Jae-Woong Jung, Dong-Hun Lee, Seong-Hun Choi, Soo-Kyung Jun, Yu-Bin Kim, Young-Seon Ju, Kyounggon Kim)</author>
      <guid isPermaLink="false">2512.18269v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching</title>
      <link>http://arxiv.org/abs/2512.18219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种增强教师网络的学生-教师框架用于异常检测，在图像级别和像素级别上都取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;异常检测或离群点检测是无监督学习中的一个挑战性课题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高性能的学生-教师框架用于异常检测，通过增强教师网络来提高检测性能。&lt;h4&gt;方法&lt;/h4&gt;首先在ImageNet上预训练ResNet-18网络，然后在MVTech-AD数据集上进行微调，提出名为ET-STPM的模型。&lt;h4&gt;主要发现&lt;/h4&gt;在图像级别和像素级别上的实验结果表明，该方法比先前方法取得了更好的指标，ET-STPM模型在图像级别达到0.971的平均准确率，在像素级别达到0.977的平均准确率。&lt;h4&gt;结论&lt;/h4&gt;增强教师网络的学生-教师框架在异常检测任务中表现优异，具有很高的实用价值。&lt;h4&gt;翻译&lt;/h4&gt;异常检测或离群点是无监督学习中的一个挑战性课题。本文提出了一种用于异常检测的学生-教师框架，其教师网络被增强以实现高性能指标。为此，我们首先在ImageNet上预训练ResNet-18网络，然后在MVTech-AD数据集上进行微调。图像级别和像素级别的实验结果表明，这一想法比先前方法取得了更好的指标。我们的模型，增强教师学生-教师特征金字塔(ET-STPM)，在图像级别异常检测中达到0.971的平均准确率，在像素级别达到0.977的平均准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CSICC55295.2022.9780522&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection or outlier is one of the challenging subjects in unsupervised learning . This paper is introduced a student-teacher framework for anomaly detection that its teacher network is enhanced for achieving high-performance metrics . For this purpose , we first pre-train the ResNet-18 network on the ImageNet and then fine-tune it on the MVTech-AD dataset . Experiment results on the image-level and pixel-level demonstrate that this idea has achieved better metrics than the previous methods . Our model , Enhanced Teacher for Student-Teacher Feature Pyramid (ET-STPM), achieved 0.971 mean accuracy on the image-level and 0.977 mean accuracy on the pixel-level for anomaly detection.</description>
      <author>example@mail.com (Mohammad Zolfaghari, Hedieh Sajedi)</author>
      <guid isPermaLink="false">2512.18219v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>cardinalR: Generating Interesting High-Dimensional Data Structures</title>
      <link>http://arxiv.org/abs/2512.18172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为cardinalR的R包，该包提供了生成多种高维数据结构的新方法，用于测试、验证和改进降维、监督学习和无监督学习算法。&lt;h4&gt;背景&lt;/h4&gt;高维数据具有多个相互依赖或关联的变量，这些关联可能是线性的、非线性的、聚类形式或异常形式。这类数据对于算法测试和验证非常重要。&lt;h4&gt;目的&lt;/h4&gt;提供生成各种高维结构的新方法，帮助研究人员更好地理解和改进不同的分析方法，特别关注非线性降维方法。&lt;h4&gt;方法&lt;/h4&gt;使用数学函数和统计分布来生成高维结构，并将其组织到R包cardinalR中，同时提供了多个示例数据集。&lt;h4&gt;主要发现&lt;/h4&gt;成功开发了多种生成高维数据的方法和相关示例数据集，这些工具可以帮助研究人员评估和改进算法性能。&lt;h4&gt;结论&lt;/h4&gt;cardinalR包丰富了用于评估算法的基准数据集集合，为高维数据分析提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;模拟的高维数据对于测试、验证和改进降维、监督学习和无监督学习中使用的算法非常有用。高维数据的特点是具有多个以某种方式相互依赖或关联的变量，如线性、非线性、聚类或异常。本文我们提供了使用数学函数和统计分布生成各种高维结构的新方法，并将其组织到R包cardinalR中。同时提供了几个示例数据集。这些将有助于研究人员更好地理解和改进不同的分析方法，特别关注非线性降维方法。这个包丰富了用于评估算法的基准数据集集合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulated high-dimensional data is useful for testing, validating, and improving algorithms used in dimension reduction, supervised and unsupervised learning. High-dimensional data is characterized by multiple variables that are dependent or associated in some way, such as linear, nonlinear, clustering or anomalies. Here we provide new methods for generating a variety of high-dimensional structures using mathematical functions and statistical distributions organized into the R package cardinalR. Several example data sets are also provided. These will be useful for researchers to better understand how different analytical methods work and can be improved, with a special focus on nonlinear dimension reduction methods. This package enriches the existing toolset of benchmark datasets for evaluating algorithms.</description>
      <author>example@mail.com (Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore, Thiyanga S. Talagala)</author>
      <guid isPermaLink="false">2512.18172v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Pretrained Battery Transformer (PBT): A battery life prediction foundation model</title>
      <link>http://arxiv.org/abs/2512.16334v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 figures in the main content&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了预训练电池Transformer(PBT)，这是首个用于电池寿命预测的基础模型，通过迁移学习在多样化数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;电池循环寿命的早期预测对加速电池研究、制造和部署至关重要，但机器学习方法因数据稀缺性和异质性而受限。虽然基础模型在其他领域通过迁移学习实现了广泛泛化，但尚未应用于电池寿命预测。&lt;h4&gt;目的&lt;/h4&gt;开发首个用于电池寿命预测的基础模型，解决数据稀缺性和异质性问题，提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出预训练电池Transformer(PBT)，通过领域知识编码的专家混合层开发，并在最大的公开电池寿命数据库上验证，从13个锂离子电池数据集中学习可迁移表示。&lt;h4&gt;主要发现&lt;/h4&gt;PBT比现有模型平均性能提高19.8%，通过迁移学习在15个包含各种操作条件、形成协议和化学成分的多样化数据集上实现了最先进性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为电池寿命预测建立了基础模型路径，为开发通用电池寿命预测系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;电池循环寿命的早期预测对于加速电池研究、制造和部署至关重要。尽管机器学习方法已经显示出令人鼓舞的结果，但由于不同老化条件导致的数据稀缺性和异质性，进展受到阻碍。在其他领域，通过迁移学习在多样化数据集上训练的基础模型已经实现了广泛的泛化能力，但尚未有关于电池循环寿命预测的基础模型的报道。在此，我们提出了预训练电池Transformer(PBT)，这是首个用于电池寿命预测的基础模型，通过领域知识编码的专家混合层开发。在最大的公开电池寿命数据库上验证，PBT从13个锂离子电池数据集中学习可迁移表示，比现有模型平均性能提高19.8%。通过迁移学习，PBT在15个包含各种操作条件、形成协议和化学成分的多样化数据集上实现了最先进性能。这项工作为电池寿命预测建立了基础模型路径，为通用电池寿命预测系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.</description>
      <author>example@mail.com (Ruifeng Tan, Weixiang Hong, Jia Li, Jiaqiang Huang, Tong-Yi Zhang)</author>
      <guid isPermaLink="false">2512.16334v2</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>MapTrace: Scalable Data Generation for Route Tracing on Maps</title>
      <link>http://arxiv.org/abs/2512.19609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可扩展的合成数据生成方法，用于改善多模态大语言模型在细粒度空间理解（特别是地图路线追踪）方面的能力，通过构建23k路径样本的数据集进行微调，显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视觉和文本推理任务上已达到类人性能，但在细粒度空间理解（如地图路线追踪）方面表现有限。与人类能快速解析和导航地图不同，当前模型往往无法遵守基本路径约束，部分原因是收集大规模像素级精确路径标注的成本和难度过高。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在细粒度空间理解（特别是地图路线追踪）方面的局限性，通过创建精确的标注数据集来提升模型的类人空间能力。&lt;h4&gt;方法&lt;/h4&gt;引入了一个可扩展的合成数据生成管道，利用合成地图图像和像素级解析自动生成具有精确标注的挑战性任务数据。使用该管道构建了包含4k张地图上23k个路径样本的微调数据集，并使用该数据集对开源和专有的多模态大语言模型进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在MapBench上的结果显示，微调显著提高了模型的鲁棒性，成功率提高了最多6.4个百分点，同时减少了路径追踪错误（NDTW）。这些增益表明，预训练模型中缺乏的细粒度空间推理能力可以通过合成监督明确地教授给模型。&lt;h4&gt;结论&lt;/h4&gt;细粒度空间推理能力，尽管在预训练模型中不存在，但可以通过合成监督明确地教授给模型，从而改善多模态大语言模型在空间理解任务上的表现。&lt;h4&gt;翻译&lt;/h4&gt;虽然多模态大语言模型在许多视觉和文本推理任务上已达到类人性能，但它们在细粒度空间理解（如地图路线追踪）方面的能力仍然有限。与人类能够快速学习解析和导航地图不同，当前模型往往无法遵守基本的路径约束，部分原因是收集大规模像素级精确路径标注的成本和难度过高。为此，我们引入了一个可扩展的合成数据生成管道，利用合成地图图像和像素级解析自动为这一具有挑战性的任务生成精确标注。使用该管道，我们构建了一个包含4k张地图上23k个路径样本的微调数据集，使模型能够获得更类人的空间能力。使用该数据集，我们对开源和专有的多模态大语言模型进行了微调。MapBench上的结果显示，微调显著提高了鲁棒性，将成功率提高了最多6.4个百分点，同时减少了路径追踪错误（NDTW）。这些增益表明，预训练模型中缺乏的细粒度空间推理能力可以通过合成监督明确地教授。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.</description>
      <author>example@mail.com (Artemis Panagopoulou, Aveek Purohit, Achin Kulshrestha, Soroosh Yazdani, Mohit Goyal)</author>
      <guid isPermaLink="false">2512.19609v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</title>
      <link>http://arxiv.org/abs/2512.18954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为VOIC的新型双解码器框架，通过引入可见区域标签提取策略，将3D语义场景完成任务解耦为可见区域语义感知和遮挡区域场景完成两个子任务，有效解决了单图像输入导致的特征稀释和错误传播问题。&lt;h4&gt;背景&lt;/h4&gt;基于相机的3D语义场景完成是自动驾驶和机器人场景理解的关键任务，旨在从单张图像推断完整的3D体积表示（包括语义和几何）。现有方法忽略了单图像输入导致的可见区域与遮挡区域之间的干扰问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有单图像3D语义场景完成方法中可见区域感知与遮挡区域推理之间的干扰，提高几何完成和语义分割的准确性。&lt;h4&gt;方法&lt;/h4&gt;引入离线可见区域标签提取策略分离可见和遮挡区域的监督；提出VOIC双解码器框架，将SSC解耦为可见区域语义感知和遮挡区域场景完成；通过融合图像特征和深度信息构建基础3D体素表示；可见解码器生成几何和语义先验，遮挡解码器利用先验和跨模态交互进行全局场景推理。&lt;h4&gt;主要发现&lt;/h4&gt;在SemanticKITTI和SSCBench-KITTI360基准上的实验表明，VOIC在几何完成和语义分割准确性方面均优于现有单目SSC方法，达到最先进性能。&lt;h4&gt;结论&lt;/h4&gt;通过将SSC任务明确解耦并利用VRLE策略净化监督空间，有效提高了3D语义场景完成的性能，解决了单图像输入的局限性。&lt;h4&gt;翻译&lt;/h4&gt;基于相机的3D语义场景完成是自动驾驶和机器人场景理解的关键任务。它旨在从单张图像推断出包含语义和几何信息的完整3D体积表示。现有方法通常专注于端到端的2D到3D特征提升和体素完成。然而，它们常常忽略了单图像输入导致的高置信度可见区域感知与低置信度遮挡区域推理之间的干扰，这可能导致特征稀释和错误传播。为解决这些挑战，我们引入了一种离线可见区域标签提取策略，该策略明确分离并提取来自密集3D真实可见区域的体素级监督。这一策略为两个互补的子任务净化了监督空间：可见区域感知和遮挡区域推理。基于这一理念，我们提出了可见-遮挡交互完成网络，一种新型双解码器框架，明确将SSC解耦为可见区域语义感知和遮挡区域场景完成。VOIC首先通过融合图像特征和深度推导的占用率构建基础3D体素表示。可见解码器专注于生成高保真度的几何和语义先验，而遮挡解码器则利用这些先验以及跨模态交互执行连贯的全局场景推理。在SemanticKITTI和SSCBench-KITTI360基准上的大量实验表明，VOIC在几何完成和语义分割准确性方面均优于现有的单目SSC方法，达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.</description>
      <author>example@mail.com (Zaidao Han, Risa Higashita, Jiang Liu)</author>
      <guid isPermaLink="false">2512.18954v1</guid>
      <pubDate>Tue, 23 Dec 2025 16:01:08 +0800</pubDate>
    </item>
    <item>
      <title>Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?</title>
      <link>http://arxiv.org/abs/2512.15748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  website and code: https://tian1327.github.io/POC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉物种识别(VSR)任务中大型多模态模型(LMMs)与少样本学习(FSL)专家模型的性能比较，并提出了后验校正(POC)方法，通过LMMs对FSL专家模型的预测结果进行重新排序，显著提高了VSR任务的准确性。&lt;h4&gt;背景&lt;/h4&gt;视觉物种识别(VSR)对生物多样性评估和保护、进化研究和生态系统管理至关重要。训练机器学习模型进行VSR需要大量带注释的图像，但物种级别的注释需要领域专业知识，导致专家只能注释少量样本。这些有限的标记数据促使通过少样本学习(FSL)训练'专家'模型。同时，先进的大型多模态模型(LMMs)在通用识别任务上表现出色。&lt;h4&gt;目的&lt;/h4&gt;研究大型多模态模型(LMMs)在高度专业的VSR任务中的表现，以及它们是否优于FSL专家模型，并探索如何结合两者的优势。&lt;h4&gt;方法&lt;/h4&gt;比较LMMs与FSL专家模型在VSR任务上的性能；发现LMMs表现不佳；提出后验校正(POC)方法，使用LMMs对FSL专家模型的预测结果进行重新排序；POC方法使用丰富的提示，包括softmax置信度和少样本视觉示例；POC无需额外训练、验证或人工干预即可应用。&lt;h4&gt;主要发现&lt;/h4&gt;LMMs在VSR任务中表现不佳，显著低于FSL专家模型；LMMs可以有效后验校正专家模型的不正确预测；当使用来自FSL专家模型的前几个预测结果作为提示时，LMMs可以恢复真实标签；POC方法在五个具有挑战性的VSR基准测试中，比先前的FSL方法高出6.4%的准确率。&lt;h4&gt;结论&lt;/h4&gt;POC方法可以作为即插即用模块，显著增强现有的FSL方法；POC可以推广到不同的预训练骨干网络和LMMs；结合FSL专家模型和LMMs的优势可以有效提高VSR任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;视觉物种识别(VSR)对生物多样性评估和保护、进化研究和生态系统管理至关重要。为VSR训练机器学习模型通常需要大量带注释的图像。然而，物种级别的注释需要领域专业知识，这使得领域专家只能注释少量样本成为现实。这些有限的标记数据促使通过少样本学习(FSL)训练'专家'模型。与此同时，先进的大型多模态模型(LMMs)在通用识别任务上已表现出显著性能。直接询问LMMs是否在高度专业的VSR任务中表现出色，以及它们是否优于FSL专家模型是很自然的。有些令人惊讶的是，我们发现尽管使用了各种既定的提示技术，LMMs在这个任务中仍然表现挣扎。LMMs甚至显著低于FSL专家模型的性能，这些专家模型只是在少样本图像上微调预训练的视觉编码器一样简单。然而，我们的深入分析揭示LMMs可以有效后验校正专家模型的不正确预测。简而言之，给定测试图像，当使用来自FSL专家模型的前几个预测结果作为提示时，LMMs可以恢复真实标签。基于这一见解，我们推导出一种简单的方法称为后验校正(POC)，它提示LMM使用包含softmax置信度和少样本视觉示例的丰富提示，对专家模型的前几个预测结果进行重新排序。在五个具有挑战性的VSR基准测试中，POC在不增加额外训练、验证或人工干预的情况下，以+6.4%的准确率优于先前的FSL技术。重要的是，POC可以推广到不同的预训练骨干网络和LMMs，作为即插即用模块显著增强现有的FSL方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Species Recognition (VSR) is pivotal to biodiversity assessment and conservation, evolution research, and ecology and ecosystem management. Training a machine-learned model for VSR typically requires vast amounts of annotated images. Yet, species-level annotation demands domain expertise, making it realistic for domain experts to annotate only a few examples. These limited labeled data motivate training an ''expert'' model via few-shot learning (FSL). Meanwhile, advanced Large Multimodal Models (LMMs) have demonstrated prominent performance on general recognition tasks. It is straightforward to ask whether LMMs excel in the highly specialized VSR task and whether they outshine FSL expert models. Somewhat surprisingly, we find that LMMs struggle in this task, despite using various established prompting techniques. LMMs even significantly underperform FSL expert models, which are as simple as finetuning a pretrained visual encoder on the few-shot images. However, our in-depth analysis reveals that LMMs can effectively post-hoc correct the expert models' incorrect predictions. Briefly, given a test image, when prompted with the top predictions from an FSL expert model, LMMs can recover the ground-truth label. Building on this insight, we derive a simple method called Post-hoc Correction (POC), which prompts an LMM to re-rank the expert model's top predictions using enriched prompts that include softmax confidence scores and few-shot visual examples. Across five challenging VSR benchmarks, POC outperforms prior art of FSL by +6.4% in accuracy without extra training, validation, or manual intervention. Importantly, POC generalizes to different pretrained backbones and LMMs, serving as a plug-and-play module to significantly enhance existing FSL methods.</description>
      <author>example@mail.com (Tian Liu, Anwesha Basu, James Caverlee, Shu Kong)</author>
      <guid isPermaLink="false">2512.15748v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
  <item>
      <title>MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</title>
      <link>http://arxiv.org/abs/2512.16909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;家庭环境中的移动操作机器人需要统一的场景表示，研究者提出了MomaGraph，结合了空间-功能关系和部分级别交互元素，并创建了相关数据集、评估套件和视觉语言模型，在实验中取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;家庭环境中的移动操作机器人需要同时进行导航和操作，这需要一个紧凑的、语义丰富的场景表示，能够捕捉物体的位置、功能和可操作部分。&lt;h4&gt;目的&lt;/h4&gt;解决现有场景表示的局限性，包括空间和功能关系的分离、静态场景处理以及忽略与当前任务最相关信息的问题。&lt;h4&gt;方法&lt;/h4&gt;提出MomaGraph统一场景表示，创建MomaGraph-Scenes数据集和MomaGraph-Bench评估套件，开发基于强化学习的MomaGraph-R1视觉语言模型，采用Graph-then-Plan框架进行任务规划。&lt;h4&gt;主要发现&lt;/h4&gt;MomaGraph-R1在基准测试上达到71.6%的准确率，比最佳基线高出11.4%，能够跨公共基准泛化，并在真实机器人实验中有效迁移。&lt;h4&gt;结论&lt;/h4&gt;MomaGraph及相关方法为家庭环境中的移动操作机器人提供了有效的场景表示和任务规划能力，在多个方面超越了现有方法。&lt;h4&gt;翻译&lt;/h4&gt;家庭环境中的移动操作机器人必须同时进行导航和操作。这需要一个紧凑的、语义丰富的场景表示，能够捕捉物体的位置、功能以及哪些部分是可操作的。场景图是一个自然的选择，但先前的工作通常将空间和功能关系分开，将场景视为没有物体状态或时间更新的静态快照，而忽略了与完成当前任务最相关的信息。为了解决这些局限性，我们引入了MomaGraph，这是一种为智能体设计的统一场景表示，它整合了空间-功能关系和部分级别的交互元素。然而，推进这样的表示需要合适的数据和严格的评估，而这些在很大程度上一直缺失。因此，我们贡献了MomaGraph-Scenes，这是家庭环境中第一个大规模的、丰富注释的、任务驱动的场景图数据集，以及MomaGraph-Bench，一个涵盖从高级规划到细粒度场景理解的六种推理能力的系统性评估套件。在此基础上，我们进一步开发了MomaGraph-R1，这是一个在MomaGraph-Scenes上通过强化训练的70亿参数视觉语言模型。MomaGraph-R1预测任务导向的场景图，并在Graph-then-Plan框架下作为零样本任务规划器。大量实验表明，我们的模型在开源模型中达到了最先进的结果，在基准测试上达到71.6%的准确率（比最佳基线高出11.4%），同时能够跨公共基准泛化并有效地迁移到真实机器人实验中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有场景图的三大局限性：1)只编码单一类型关系(空间或功能)，而非两者结合；2)局限于静态场景，无法适应动态环境变化；3)缺乏任务相关性，未强调与任务执行直接相关的信息。这个问题在现实中很重要，因为家庭移动机器人需要同时导航和操作，需要理解物体位置、功能及可操作部分。在研究中重要，因为现有场景图无法提供完整、动态且任务相关的场景表示，限制了机器人在家庭环境中的有效操作能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从移动机器人在家庭环境中的基本挑战出发，分析了现有场景图的局限性，并借鉴认知科学研究发现人类感知是动态和任务导向的。设计思路是创建统一的空间-功能场景图，整合空间关系和功能关系，同时引入部分级别交互节点。借鉴了现有场景图概念、视觉-语言模型架构(特别是Qwen2.5-VL-7B)、强化学习算法(DAPO)和图结构表示思想，但针对机器人任务规划进行了专门设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建统一的空间-功能场景图表示，将空间关系和功能关系整合在一个框架中，包含部分级别交互节点，并支持动态更新。采用'Graph-then-Plan'框架：1)输入多视角图像和自然语言指令；2)构建任务导向的场景图，包含相关物体节点、空间关系和功能关系；3)通过动态更新机制根据环境状态变化调整场景图；4)使用强化学习训练模型，基于精心设计的奖励函数优化场景图生成；5)基于生成的场景图执行任务规划，提高推理有效性和可解释性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)MomaGraph表示法，统一空间和功能关系，引入部分级别交互节点，支持动态更新；2)MomaGraph-Scenes数据集，首个大型任务驱动的家庭场景图数据集；3)MomaGraph-Bench评估套件，系统评估六种核心推理能力；4)MomaGraph-R1模型，首个使用强化学习优化空间-功能推理的7B视觉-语言模型。相比之前工作，MomaGraph统一建模空间和功能关系，支持动态适应环境变化，强调任务相关性，使用强化学习训练，并提供系统化评估基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MomaGraph通过统一的空间-功能场景图表示、首个任务驱动的家庭场景图数据集、系统化的评估基准以及基于强化学习的视觉-语言模型，显著提升了机器人在复杂家庭环境中的场景理解和任务规划能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.</description>
      <author>example@mail.com (Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath)</author>
      <guid isPermaLink="false">2512.16909v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</title>
      <link>http://arxiv.org/abs/2512.17908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Re-Depth Anything框架，一种测试时自监督方法，通过融合深度估计模型与大规模2D扩散模型先验知识，解决了基础模型在处理真实世界图像时的性能局限，显著提升了深度估计的准确性和真实感。&lt;h4&gt;背景&lt;/h4&gt;单目深度估计面临挑战，因为最近的基础模型（如Depth Anything V2）在处理远离训练分布的真实世界图像时表现不佳，存在域差距问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够弥合域差距的方法，提高深度估计模型在真实世界图像上的性能和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Re-Depth Anything框架，一种测试时自监督方法，通过融合DA-V2与大规模2D扩散模型先验知识；在输入图像上直接进行无标签精炼，通过重新光照预测的深度图和增强输入；利用形状从明暗恢复（SfS）线索在生成上下文中使用分数蒸馏采样（SDS）替代传统光度重建；采用有针对性优化策略，冻结编码器，只更新中间嵌入并微调解码器。&lt;h4&gt;主要发现&lt;/h4&gt;Re-Depth Anything在多个基准测试中相比DA-V2在深度准确性和真实感方面取得显著提升；通过增强几何推理开辟了自监督的新途径。&lt;h4&gt;结论&lt;/h4&gt;Re-Depth Anything框架有效解决了基础模型在处理真实世界图像时的局限性，展示了通过融合不同模型先验知识和创新优化策略提升深度估计性能的有效性。&lt;h4&gt;翻译&lt;/h4&gt;单目深度估计仍然具有挑战性，因为最近的基础模型，如Depth Anything V2（DA-V2），在处理远离训练分布的真实世界图像时表现不佳。我们引入了Re-Depth Anything，一种测试时自监督框架，通过将DA-V2与大规模2D扩散模型的强大先验知识融合来弥合这种域差距。我们的方法通过重新光照预测的深度图和增强输入，直接在输入图像上进行无标签精炼。这种重新合成方法通过在新的生成上下文中利用从明暗恢复形状（SfS）线索并使用分数蒸馏采样（SDS），替代了传统的光度重建。为了防止优化崩溃，我们的框架采用了一种有针对性的优化策略：我们冻结编码器，只更新中间嵌入，同时微调解码器，而不是直接优化深度或微调整个模型。在多个基准测试中，Re-Depth Anything在深度准确性和真实感方面相比DA-V2取得了显著提升，展示了通过增强几何推理实现自监督的新途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决单目深度估计模型（特别是Depth Anything V2）在处理远离训练分布的真实世界图像时表现不佳的问题。这个问题很重要，因为单目深度估计是计算机视觉的基础任务，对3D重建、自动驾驶、机器人导航和虚拟现实等众多应用至关重要。虽然基础模型已取得进展，但在真实场景中仍有提升空间，特别是在处理与训练数据分布差异较大的图像时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了单目深度估计在真实世界场景中的挑战，特别是测试数据与训练数据分布不匹配的问题。他们借鉴了DreamFusion等利用2D扩散模型作为3D重建先验的工作，以及Shape from Shading原理，但避免了传统方法的局限性。作者创新性地设计了针对性优化策略（冻结编码器，只更新中间嵌入和解码器），并提出了一种新的重照明方法，通过随机光照条件重新照亮预测的深度图，利用2D扩散模型评估结果的真实性来指导优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D扩散模型的强大先验知识，通过自监督方式改进深度估计，通过重照明预测的深度图并使用扩散模型评估结果的真实性来指导深度图的优化，不依赖精确的光度重建，而是通过添加额外的阴影线索来增强几何推理。整体流程：1)接收图像并生成初始深度图；2)从深度图计算法线并使用Blinn-Phong模型合成重照明图像；3)生成文本提示并计算SDS损失；4)联合优化中间特征嵌入和解码器权重；5)多次运行优化并集成结果生成最终深度图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)新的测试时优化框架，无需额外标记数据；2)单图像重照明模型，可微分连接深度图与输入图像；3)针对性优化方案，联合优化解码器输入嵌入和权重。不同之处：不同于传统光度重建方法，该方法在监督方法之上应用自监督学习；避免精确重建外观，通过重新合成增强输入图像；采用针对性优化策略而非直接优化深度图或微调整个网络；专注于改进现有深度估计模型在真实图像上的表现，而非从文本生成3D内容。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Re-Depth Anything通过自监督重照明和2D扩散模型先验，在测试时优化深度估计模型，显著提高了其在真实世界图像上的深度准确性和真实感。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.</description>
      <author>example@mail.com (Ananta R. Bhattarai, Helge Rhodin)</author>
      <guid isPermaLink="false">2512.17908v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Robustness of Vision in Open Foundation Models</title>
      <link>http://arxiv.org/abs/2512.17902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2两种视觉语言模型对抗攻击的鲁棒性，发现视觉模态是降低当代开放权重视觉语言模型性能的有效攻击向量，且对抗鲁棒性与标准基准性能不一定直接相关。&lt;h4&gt;背景&lt;/h4&gt;随着深度学习的发展，理解AI系统识别对象的模型变得越来越困难，攻击者可能通过添加看不见的元素来修改图像，从而混淆AI对实体的识别。&lt;h4&gt;目的&lt;/h4&gt;研究LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2两种视觉语言模型的对抗鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用未针对的PGD方法针对视觉输入模态进行测试，在视觉问答v2数据集子集上进行实证评估，使用标准的VQA准确率指标量化对抗攻击结果，并比较两种模型的准确率下降情况。&lt;h4&gt;主要发现&lt;/h4&gt;Llama 3.2 Vision尽管基线准确率较低，但在受到攻击时性能下降较小，特别是在较高扰动水平下；视觉模态是降低当代开放权重视觉语言模型性能的有效攻击向量。&lt;h4&gt;结论&lt;/h4&gt;对抗鲁棒性不一定与标准基准性能直接相关，可能受到底层架构和训练因素的影响。&lt;h4&gt;翻译&lt;/h4&gt;随着深度学习的增加，理解AI系统如何识别对象的模型变得越来越困难。因此，攻击者可能通过添加看不见的元素来修改图像，从而混淆AI对实体的识别。本文因此研究了LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2的对抗鲁棒性。这些模型针对视觉输入模态进行了未针对的PGD测试，并在视觉问答v2数据集子集上进行了实证评估。这些对抗攻击的结果然后使用标准的VQA准确率指标进行量化。然后将此评估与LLaVA和Llama 3.2 Vision的准确率下降进行比较。一个关键发现是，Llama 3.2 Vision尽管在此设置中具有较低的基线准确率，但在受到攻击时性能下降较小，特别是在较高的扰动水平下。总体而言，研究结果证实视觉模态是降低当代开放权重视觉语言模型性能的有效攻击向量，包括Meta的Llama 3.2 Vision。此外，研究结果强调对抗鲁棒性不一定与标准基准性能直接相关，可能受到底层架构和训练因素的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ACCESS.2025.3645997&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.</description>
      <author>example@mail.com (Jonathon Fox, William J Buchanan, Pavlos Papadopoulos)</author>
      <guid isPermaLink="false">2512.17902v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>RadarGen: Automotive Radar Point Cloud Generation from Cameras</title>
      <link>http://arxiv.org/abs/2512.17897v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://radargen.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RadarGen是一种创新的扩散模型，能够从多视角相机图像生成真实的汽车雷达点云，为多模态生成仿真提供了新方向。&lt;h4&gt;背景&lt;/h4&gt;汽车雷达点云生成是自动驾驶感知系统中的重要挑战，现有方法在生成真实且物理合理的雷达数据方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从多视角相机图像合成真实汽车雷达点云的模型，减少生成数据与真实数据之间的差距，促进跨感知模态的统一生成仿真。&lt;h4&gt;方法&lt;/h4&gt;RadarGen采用扩散模型技术，通过鸟瞰图表示雷达测量并编码空间结构、RCS和多普勒属性；使用轻量级恢复步骤重建点云；并融入从预训练基础模型提取的BEV对齐深度、语义和运动线索，指导生成过程朝向物理合理的雷达模式。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模驾驶数据上的评估表明，RadarGen能够有效捕捉特征雷达测量分布，显著缩小了与在真实数据上训练的感知模型之间的性能差距。&lt;h4&gt;结论&lt;/h4&gt;RadarGen代表了跨感知模态统一生成仿真的重要进展，其基于图像的条件化设计使其与现有视觉数据集和仿真框架兼容，为自动驾驶领域提供了可扩展的多模态生成仿真解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了RadarGen，一个用于从多视角相机图像合成真实汽车雷达点云的扩散模型。RadarGen通过将雷达测量表示为鸟瞰图形式来将高效的图像-潜在扩散模型适应到雷达领域，该形式编码了空间结构以及雷达散射截面(RCS)和多普勒属性。轻量级恢复步骤从生成的地图中重建点云。为了更好地使生成与视觉场景对齐，RadarGen融合了从预训练基础模型中提取的BEV对齐的深度、语义和运动线索，这些线索将随机生成过程引导向物理上合理的雷达模式。基于图像的条件化使该方法原则上与现有的视觉数据集和仿真框架广泛兼容，为多模态生成仿真提供了可扩展的方向。在大规模驾驶数据上的评估显示，RadarGen捕捉了特征雷达测量分布，并减少了与在真实数据上训练的感知模型之间的差距，标志着跨感知模态统一生成仿真的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.</description>
      <author>example@mail.com (Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany)</author>
      <guid isPermaLink="false">2512.17897v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</title>
      <link>http://arxiv.org/abs/2512.17891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为关键点计数分类器(KCCs)的新方法，可将任何训练好的基于视觉Transformer(ViT)的模型转换为自解释模型(SEM)，无需重新训练，从而提高模型的透明度和可靠性。&lt;h4&gt;背景&lt;/h4&gt;当前设计自解释模型的方法需要复杂的训练流程和特定架构，不切实际。随着基于视觉Transformer的通用基础模型的发展，这种不切实际性更加严重，需要新方法提供透明度和可靠性。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需重新训练即可将任何基于ViT的模型转换为自解释模型的方法，以提高ViT基础模型的透明度和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出关键点计数分类器(KCCs)方法，利用ViT能够高精度自动识别图像间匹配关键点的特性，创建一个易于解释且可在输入中可视化的决策过程。&lt;h4&gt;主要发现&lt;/h4&gt;广泛评估表明，KCCs比最近的基线方法改善了人机通信效果。&lt;h4&gt;结论&lt;/h4&gt;KCCs是使基于ViT的基础模型更加透明和可靠的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;当前设计自解释模型(SEMs)的方法需要复杂的训练流程和特定的架构，这使得它们不切实际。随着基于视觉Transformer(ViT)的通用基础模型的进步，这种不切实际性变得更加严重。因此，需要新的方法为基于ViT的基础模型提供透明度和可靠性。在这项工作中，我们提出了一种新方法，可以将任何训练好的基于ViT的模型转换为SEM，而无需重新训练，我们称之为关键点计数分类器(KCCs)。最近的研究表明，ViT可以高精度地自动识别图像之间的匹配关键点，我们基于这些结果创建了一个易于解释的决策过程，该过程在输入中本质上是可可视化的。我们进行了广泛的评估，表明与最近的基线相比，KCCs改善了人机通信。我们相信KCCs是使基于ViT的基础模型更加透明和可靠的重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.</description>
      <author>example@mail.com (Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen)</author>
      <guid isPermaLink="false">2512.17891v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</title>
      <link>http://arxiv.org/abs/2512.17853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 25 figures. The first four authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了AnyTask框架，这是一个结合大规模并行GPU模拟和基础模型的自动化框架，用于设计多样化的操作任务并合成机器人数据。研究人员提出了三种AnyTask代理来生成专家演示，并在生成的数据上训练行为克隆策略，最终在真实机器人硬件上部署，成功实现了44%的平均成功率。&lt;h4&gt;背景&lt;/h4&gt;通用机器人学习仍然受到数据的限制：大规模、多样化且高质量的交互数据在现实世界中收集成本高昂。虽然模拟已成为扩大数据收集规模的有前途的方法，但相关任务（包括模拟任务设计、任务感知场景生成、专家演示合成和模拟到现实迁移）仍然需要大量的人力投入。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化框架，减少机器人学习过程中对大规模、多样化且高质量交互数据的依赖，降低数据收集成本，并减少人工干预。&lt;h4&gt;方法&lt;/h4&gt;提出AnyTask框架，结合大规模并行GPU模拟和基础模型。引入三种AnyTask代理：1) ViPR，一种新颖的任务和运动规划代理，具有循环并行细化；2) ViPR-Eureka，一种具有生成密集奖励和LLM引导接触采样的强化学习代理；3) ViPR-RL，一种混合规划和学习方法，仅使用稀疏奖励共同产生高质量演示。&lt;h4&gt;主要发现&lt;/h4&gt;训练的行为克隆策略在模拟中得到验证，并直接部署在真实机器人硬件上。这些策略能够泛化到新的物体姿态，在一套真实世界的抓放、抽屉开启、接触丰富的推动和长期操作任务中实现了44%的平均成功率。&lt;h4&gt;结论&lt;/h4&gt;AnyTask框架能够自动化设计多样化的操作任务并合成机器人数据，减少了对大规模、高质量人工收集数据的依赖，并在真实世界任务中取得了良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;通用机器人学习仍然受到数据的限制：大规模、多样化且高质量的交互数据在现实世界中收集成本高昂。虽然模拟已成为扩大数据收集规模的有前途的方法，但相关任务，包括模拟任务设计、任务感知场景生成、专家演示合成和模拟到现实迁移，仍然需要大量的人力投入。我们提出了AnyTask，一个将大规模并行GPU模拟与基础模型配对的自动化框架，用于设计多样化的操作任务并合成机器人数据。我们引入了三种AnyTask代理，用于生成专家演示，旨在解决尽可能多的任务：1) ViPR，一种新颖的任务和运动规划代理，具有循环并行细化；2) ViPR-Eureka，一种具有生成密集奖励和LLM引导接触采样的强化学习代理；3) ViPR-RL，一种混合规划和学习方法，仅使用稀疏奖励共同产生高质量演示。我们在生成的数据上训练行为克隆策略，在模拟中验证它们，并将它们直接部署在真实机器人硬件上。这些策略能够泛化到新的物体姿态，在一套真实世界的抓放、抽屉开启、接触丰富的推动和长期操作任务中实现了44%的平均成功率。我们的项目网站是https://anytask.rai-inst.com。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人学习中的数据瓶颈问题：大规模、多样化、高质量的机器人交互数据在现实世界中收集极其昂贵和耗时。这个问题很重要，因为机器人学习成功依赖于大量高质量数据，而现实世界数据收集成本高、效率低；虽然模拟可以扩展数据收集，但任务设计和数据生成仍需大量人工干预，限制了数据的多样性和机器人系统的能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到基础模型（如大型语言模型）在机器人任务中表现出色，可以利用它们来自动化创建机器人模拟环境的关键步骤。设计上结合了大规模并行GPU模拟和基础模型，从高层次文本目标自动生成任务和数据。该方法借鉴了现有工作：使用任务和运动规划（TAMP）和强化学习（RL）两种常用方法，利用IsaacLab模拟器，并参考了基础模型的能力，但试图解决现有系统在自动化程度和sim-to-real转移方面的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模并行GPU模拟和基础模型（特别是大型语言模型和视觉语言模型）来自动化整个机器人数据生成流程，从高层次文本目标开始，自动生成任务、场景和专家演示数据。整体流程包括：1)构建对象数据库存储物体信息；2)任务生成器提出任务和对象；3)模拟生成器创建可执行代码；4)密集注释系统生成详细的环境描述；5)使用三种代理（VIPR、VIPR-EUREKA和VIPR-RL）生成专家演示；6)在生成的数据上训练策略并直接部署到真实机器人。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)完全自动化的端到端框架；2)三种多样化代理生成专家演示；3)密集注释系统；4)基于网格的接触采样算法；5)两阶段数据收集机制。相比之前工作，不同之处在于：实现了更高程度的自动化；生成任务描述具有更好多样性；实现了纯合成数据训练的零样本sim-to-real转移；利用大规模GPU并行处理提高效率；结合了TAMP和RL的优点，而非单独使用其中一种。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AnyTask框架通过自动化任务设计、场景生成和专家演示创建，利用大规模GPU模拟和基础模型，实现了从纯合成数据到零样本sim-to-real策略学习的完整流程，显著减少了机器人学习中对真实世界数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</description>
      <author>example@mail.com (Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper)</author>
      <guid isPermaLink="false">2512.17853v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</title>
      <link>http://arxiv.org/abs/2512.17817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Chorus是一个多教师预训练框架，通过从2D基础模型中提炼互补信号，学习一个整体的3D高斯溅射场景编码器，解决了直接从3DGS基元编码丰富通用特征的研究空白。&lt;h4&gt;背景&lt;/h4&gt;3DGS已成为高保真场景表示方法，但直接从其基元编码丰富、通用特征的研究仍不充分。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架，能够从3DGS基元中学习丰富的、通用的特征表示。&lt;h4&gt;方法&lt;/h4&gt;Chorus使用共享的3D编码器和教师特定的投影仪，从语言对齐、通用和物体感知的教师那里学习，鼓励捕获从高级语义到细粒度结构的共享嵌入空间。&lt;h4&gt;主要发现&lt;/h4&gt;1) Chorus在多种任务上表现出色，包括开放词汇的语义和实例分割等；2) 在点云基准上，使用少39.9倍训练场景的情况下优于点云基线；3) 提出的渲染和提炼适应方法有助于领域外微调。&lt;h4&gt;结论&lt;/h4&gt;Chorus成功解决了从3DGS基元编码丰富通用特征的研究空白，并在多种任务和基准上展示了优越性能。&lt;h4&gt;翻译&lt;/h4&gt;虽然3DGS已成为一种高保真场景表示方法，但直接从其基元编码丰富、通用特征的研究仍不充分。我们通过引入Chorus（一个多教师预训练框架）解决了这一研究空白，该框架通过从2D基础模型中提炼互补信号，学习一个整体的、前馈的3D高斯溅射场景编码器。Chorus使用共享的3D编码器和教师特定的投影仪，从语言对齐、通用和物体感知的教师那里学习，鼓励捕获从高级语义到细粒度结构的共享嵌入空间。我们在广泛任务上评估了Chorus：开放词汇的语义和实例分割、线性和解码器探测，以及数据高效监督。除了3DGS外，我们还通过仅使用高斯的中心、颜色和估计法线作为输入预训练了一个变体，在仅支持点云的几个基准上测试了Chorus。有趣的是，这个编码器展示了强大的迁移能力，并且使用少39.9倍训练场景的情况下优于点云基线。最后，我们提出了一种渲染和提炼的适应方法，有助于领域外微调。我们的代码和模型将在发表后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.</description>
      <author>example@mail.com (Yue Li, Qi Ma, Runyi Yang, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Theo Gevers, Luc Van Gool, Danda Pani Paudel, Martin R. Oswald)</author>
      <guid isPermaLink="false">2512.17817v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation</title>
      <link>http://arxiv.org/abs/2512.17795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了智能知识挖掘框架(IKMF)，一个全面的概念模型，旨在弥合动态AI驱动分析与可信长期保存之间的关键差距。&lt;h4&gt;背景&lt;/h4&gt;数字数据的空前增长给所有数据密集型部门带来了访问、整合和价值创造的挑战。有价值的信息经常被封装在分散的系统、非结构化文档和异构格式中，形成了阻碍有效利用和协作决策的信息孤岛。&lt;h4&gt;目的&lt;/h4&gt;设计一个概念模型，连接动态AI驱动分析与可信长期保存，解决数据孤岛问题，促进数据的有效利用和协作决策。&lt;h4&gt;方法&lt;/h4&gt;提出双流架构：水平挖掘流程将原始数据转换为语义丰富、机器可操作的知识；并行的可信归档流确保这些资产的完整性、来源和计算可重现性。&lt;h4&gt;主要发现&lt;/h4&gt;通过定义这种共生关系的蓝图，为将静态存储库转变为促进可操作智能从生产者流向消费者的活跃生态系统提供了基础模型。&lt;h4&gt;结论&lt;/h4&gt;本文概述了指导框架研发的动机、问题陈述和关键研究问题，介绍了潜在的科学方法论，并详细说明了其概念设计和建模。&lt;h4&gt;翻译&lt;/h4&gt;数字数据的空前增长给所有数据密集型部门在访问、整合和价值创造方面带来了重大挑战。有价值的信息经常被封装在分散的系统、非结构化文档和异构格式中，形成了阻碍有效利用和协作决策的信息孤岛。本文介绍了智能知识挖掘框架(IKMF)，一个全面的概念模型，旨在弥合动态AI驱动分析与可信长期保存之间的关键差距。该框架提出了双流架构：一个水平挖掘流程，系统地将原始数据转换为语义丰富、机器可操作的知识；以及一个并行的可信归档流，确保这些资产的完整性、来源和计算可重现性。通过定义这种共生关系的蓝图，本文为将静态存储库转变为促进可操作智能从生产者流向消费者的活跃生态系统提供了基础模型。本文概述了指导框架研发的动机、问题陈述和关键研究问题，介绍了潜在的科学方法论，并详细说明了其概念设计和建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The unprecedented proliferation of digital data presents significant challenges in access, integration, and value creation across all data-intensive sectors. Valuable information is frequently encapsulated within disparate systems, unstructured documents, and heterogeneous formats, creating silos that impede efficient utilization and collaborative decision-making. This paper introduces the Intelligent Knowledge Mining Framework (IKMF), a comprehensive conceptual model designed to bridge the critical gap between dynamic AI-driven analysis and trustworthy long-term preservation. The framework proposes a dual-stream architecture: a horizontal Mining Process that systematically transforms raw data into semantically rich, machine-actionable knowledge, and a parallel Trustworthy Archiving Stream that ensures the integrity, provenance, and computational reproducibility of these assets. By defining a blueprint for this symbiotic relationship, the paper provides a foundational model for transforming static repositories into living ecosystems that facilitate the flow of actionable intelligence from producers to consumers. This paper outlines the motivation, problem statement, and key research questions guiding the research and development of the framework, presents the underlying scientific methodology, and details its conceptual design and modeling.</description>
      <author>example@mail.com (Binh Vu)</author>
      <guid isPermaLink="false">2512.17795v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image</title>
      <link>http://arxiv.org/abs/2512.17545v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages,16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ClothHMR方法，通过服装剪裁和基于基础人体视觉模型的网格恢复两个模块，准确恢复穿着各种服装的人体3D网格，解决了现有方法在处理宽松服装时表现不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;随着3D数据作为多媒体信息的重要形式迅速涌现，3D人体网格恢复技术也相应发展。然而，当前方法主要处理穿着紧身服装的人体，在估计各种服装（特别是宽松服装）下的人体形状和姿态时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;为了准确恢复穿着各种服装的人体3D网格，作者提出了两个关键见解：(1)将服装剪裁以适合人体可以减轻服装对3D人体网格恢复的不利影响；(2)利用来自大型基础模型的人体视觉信息可以增强估计的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;ClothHMR主要包含两个模块：1. 服装剪裁(CT)模块：采用身体语义估计和身体边缘预测来剪裁服装，确保其适合身体轮廓；2. 基于FHVM的网格恢复(MR)模块：通过不断将3D网格的中间表示与从基础人体视觉模型(FHVM)推断出的中间表示对齐，来优化3D人体网格的初始参数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ClothHMR在基准数据集和野外图像上显著优于现有的最先进方法。此外，开发了一个由ClothHMR驱动的在线时尚购物网络应用，展示了ClothHMR可以有效服务实际使用场景。&lt;h4&gt;结论&lt;/h4&gt;ClothHMR能够准确恢复穿着各种服装的人体3D网格，精确估计他们的身体形状和姿态。该方法的代码和模型已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;随着3D数据迅速成为多媒体信息的重要形式，3D人体网格恢复技术也相应发展。然而，当前方法主要处理穿着紧身服装的人体，在估计各种服装（特别是宽松服装）下的人体形状和姿态时表现不佳。为此，我们提出了两个关键见解：(1)将服装剪裁以适合人体可以减轻服装对3D人体网格恢复的不利影响，(2)利用来自大型基础模型的人体视觉信息可以增强估计的泛化能力。基于这些见解，我们提出了ClothHMR，以准确恢复穿着各种服装的人体3D网格。ClothHMR主要包含两个模块：服装剪裁(CT)和基于FHVM的网格恢复(MR)。CT模块采用身体语义估计和身体边缘预测来剪裁服装，确保其适合身体轮廓。MR模块通过不断将3D网格的中间表示与从基础人体视觉模型(FHVM)推断出的中间表示对齐，来优化3D人体网格的初始参数。ClothHMR能够准确恢复穿着各种服装的人体3D网格，精确估计他们的身体形状和姿态。实验结果表明，ClothHMR在基准数据集和野外图像上显著优于现有的最先进方法。此外，开发了一个由ClothHMR驱动的在线时尚购物网络应用，展示了ClothHMR可以有效服务实际使用场景。ClothHMR的代码和模型可在以下网址获取：https://github.com/starVisionTeam/ClothHMR。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D人体网格恢复技术在处理穿着多样化衣物（尤其是宽松衣物）时效果不佳的问题。这个问题在现实中很重要，因为准确的3D人体网格恢复在虚拟试衣、在线健身、沉浸式游戏等领域有广泛应用，而人们日常穿着各种类型的衣物，特别是宽松衣物非常常见。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于两个关键洞察：1) 将衣物裁剪以适合人体轮廓可以减轻衣物对3D人体网格恢复的不利影响；2) 利用大型基础模型中的人体视觉信息可以增强估计的泛化能力。作者设计了ClothHMR方法，包括服装裁剪和基于FHVM的网格恢复两个模块。作者借鉴了SMPL等参数化人体模型、Sapiens基础视觉模型、迭代优化策略以及特征金字塔融合等技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过服装裁剪减少衣物遮挡的影响，并使用单一基础模型提取一致的中间表示，然后通过迭代优化对齐不同表示。整体流程是：输入单张RGB图像→服装裁剪模块（身体语义估计、边缘预测和衣物裁剪）→网格恢复模块（初始化3D网格→基础模型提取中间表示→迭代优化网格参数）→输出精确的3D人体网格。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出服装裁剪方案处理多样化衣物；2) 首次将基础人体视觉模型引入3D人体网格恢复；3) 提出ClothHMR方法恢复穿着多样化衣物的3D网格。相比之前工作，ClothHMR通过服装裁剪减少遮挡影响，使用单一基础模型确保中间表示一致性，并在多个基准数据集上显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ClothHMR通过服装裁剪和基础人体视觉模型的结合，实现了从单张图像中准确恢复穿着多样化衣物的人体3D网格，显著提高了在宽松衣物和复杂姿势下的估计精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3731715.3733288&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \url{https://github.com/starVisionTeam/ClothHMR}.</description>
      <author>example@mail.com (Yunqi Gao, Leyuan Liu, Yuhan Li, Changxin Gao, Yuanyuan Liu, Jingying Chen)</author>
      <guid isPermaLink="false">2512.17545v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals</title>
      <link>http://arxiv.org/abs/2512.17527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SafeBench-Seq是一个仅包含元数据、可重现的蛋白质序列危险筛查基准测试和基线分类器，在同源控制下评估，可在普通CPU上运行。&lt;h4&gt;背景&lt;/h4&gt;蛋白质设计基础模型带来了具体的生物安全风险，但社区缺乏一种简单、可重现的序列级别危险筛查基线，这种基线需要在同源控制下明确评估，并且可以在普通CPU上运行。&lt;h4&gt;目的&lt;/h4&gt;引入SafeBench-Seq，一个仅包含元数据、可重现的基准测试和基线分类器，完全基于公共数据（SafeProtein危险数据和UniProt良性数据）和可解释特征（全局物理化学描述符和氨基酸组成）构建。&lt;h4&gt;方法&lt;/h4&gt;通过同源聚类（&lt;=40%同一性）组合数据集模拟'前所未见'的威胁；执行聚类级别的保留（训练/测试间无聚类重叠）；报告区分度（AUROC/AUPRC）和筛查操作点（TPR@1% FPR；FPR@95% TPR）及95%自举置信区间；提供校准概率；使用Brier分数、期望校准误差和可靠性图表量化概率质量；通过保持组成的残基洗牌和仅长度/组成的消融探测快捷键易感性。&lt;h4&gt;主要发现&lt;/h4&gt;随机分割相对于同源聚类评估显著高估了稳健性；校准的线性模型表现出相对较好的校准性，而树集成保持略高的Brier/ECE。&lt;h4&gt;结论&lt;/h4&gt;SafeBench-Seq仅使用CPU、可重现，并且仅发布元数据（访问号、聚类ID、分割标签），能够在不传播危险序列的情况下进行严格评估。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质设计基础模型带来了具体的生物安全风险，但社区缺乏一种简单、可重现的序列级别危险筛查基线，这种基线需要在同源控制下明确评估，并且可以在普通CPU上运行。我们引入了SafeBench-Seq，这是一个仅包含元数据、可重现的基准测试和基线分类器，完全基于公共数据（SafeProtein危险数据和UniProt良性数据）和可解释特征（全局物理化学描述符和氨基酸组成）构建。为了模拟'前所未见'的威胁，我们在&lt;=40%同一性下对组合数据集进行同源聚类，并执行聚类级别的保留（训练/测试之间无聚类重叠）。我们报告了区分度（AUROC/AUPRC）和筛查操作点（TPR@1% FPR；FPR@95% TPR），并附有95%的自举置信区间（n=200），我们通过CalibratedClassifierCV（逻辑回归/随机森林使用等距；线性SVM使用Platt Sigmoid）提供校准概率。我们使用Brier分数、期望校准误差（ECE；15个区间）和可靠性图表来量化概率质量。通过保持组成的残基洗牌和仅长度/组成的消融来探测快捷键易感性。经验上，与同源聚类评估相比，随机分割显著高估了稳健性；校准的线性模型表现出相对较好的校准性，而树集成保持略高的Brier/ECE。SafeBench-Seq仅使用CPU、可重现，并且仅发布元数据（访问号、聚类ID、分割标签），能够在不传播危险序列的情况下进行严格评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at &lt;=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.</description>
      <author>example@mail.com (Muhammad Haris Khan)</author>
      <guid isPermaLink="false">2512.17527v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection</title>
      <link>http://arxiv.org/abs/2512.17514v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FALCON-SFOD的框架，用于增强域偏移下的目标聚焦适应，解决了现有方法在无源目标检测中因域偏移导致的目标表示能力下降问题。&lt;h4&gt;背景&lt;/h4&gt;当前无源目标检测的最先进方法通常依赖Mean-Teacher自标记，但域偏移会降低检测器保持强目标聚焦表示的能力，导致在背景杂乱中产生高置信度激活，进而生成不可靠的伪标签。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，加强特征空间本身，增强目标聚焦表示能力，以解决域偏移下目标检测性能下降的问题。&lt;h4&gt;方法&lt;/h4&gt;提出FALCON-SFOD框架，包含两个互补组件：1) SPAR（空间先验感知正则化）：利用视觉基础模型的泛化能力正则化检测器特征空间，通过OV-SAM派生的类无关二值掩码引导网络朝向目标区域；2) IRPL（不平衡感知噪声鲁棒伪标记）：促进严重前景-背景不平衡下的平衡和噪声容忍学习。&lt;h4&gt;主要发现&lt;/h4&gt;域偏移导致检测器目标聚焦能力下降，现有方法主要关注伪标签的改进而忽略了特征空间本身的加强；通过理论分析将组件设计与更紧密的定位和分类误差边界联系起来。&lt;h4&gt;结论&lt;/h4&gt;FALCON-SFOD框架通过加强目标聚焦表示和特征空间，在SFOD基准测试中取得了具有竞争力的性能。&lt;h4&gt;翻译&lt;/h4&gt;当前无源目标检测的最先进方法通常依赖于Mean-Teacher自标记。然而，域偏移往往会降低检测器保持强目标聚焦表示的能力，导致在背景杂乱中产生高置信度激活。这种弱目标聚焦导致检测头产生不可靠的伪标签。虽然之前的工作主要改进这些伪标签，但它们忽略了加强特征空间本身的基本需求。我们提出了FALCON-SFOD（具有杂波抑制和噪声鲁棒性的基础对齐学习），一个旨在加强域偏移下目标聚焦适应的框架。它包含两个互补组件。SPAR（空间先验感知正则化）利用视觉基础模型的泛化能力来正则化检测器的特征空间。使用从OV-SAM派生的类无关二值掩码，SPAR通过引导网络朝向目标区域来促进结构化和前景聚焦的激活。IRPL（不平衡感知噪声鲁棒伪标记）通过在严重前景-背景不平衡下促进平衡和噪声容忍学习来补充SPAR。在将这些设计与更紧密的定位和分类误差边界联系起来的理论分析指导下，FALCON-SFOD在SFOD基准测试中取得了具有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.</description>
      <author>example@mail.com (Sairam VCR, Rishabh Lalla, Aveen Dayal, Tejal Kulkarni, Anuj Lalla, Vineeth N Balasubramanian, Muhammad Haris Khan)</author>
      <guid isPermaLink="false">2512.17514v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort</title>
      <link>http://arxiv.org/abs/2512.17499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages, 8 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了第一个来自中东地区的外部验证队列，评估了AI在前列腺癌诊断和Gleason分级中的性能，证明了AI模型达到病理学家水平的表现，并展示了低成本扫描仪在AI病理学应用中的可行性。&lt;h4&gt;背景&lt;/h4&gt;人工智能正在提高癌症诊断的效率和准确性，但病理学AI系统的性能几乎只在欧美大型中心的人群中进行了评估。为了在全球范围内采用病理学AI，需要对代表性不足的人群进行验证研究，这些人群从AI支持中获得的潜在收益可能最大。&lt;h4&gt;目的&lt;/h4&gt;展示第一个来自中东地区的外部验证队列研究，专注于基于AI的前列腺癌诊断和Gleason分级。&lt;h4&gt;方法&lt;/h4&gt;收集并数字化了来自伊拉克库尔德地区的339个前列腺活检标本，代表2013-2024年间连续的185例患者。评估了一个任务特定的端到端AI模型和两个基础模型，分析它们与病理学家的一致性和在不同扫描仪型号上数字化样本的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;AI与病理学家之间的分级一致性类似于病理学家之间的分级一致性（Cohen二次加权kappa值0.801 vs 0.799，p=0.9824）。所有AI模型和扫描仪对的跨扫描仪一致性都很高（二次加权kappa &gt; 0.90），包括低成本紧凑型扫描仪。&lt;h4&gt;结论&lt;/h4&gt;AI模型在前列腺组织病理学评估中表现出与病理学家相当的性能。紧凑型扫描仪可为非数字化环境中的验证研究提供途径，并使样本量有限的实验室能够经济有效地采用AI。这是中东第一个公开可用的数字病理数据集，支持进一步研究全球公平的AI病理学。&lt;h4&gt;翻译&lt;/h4&gt;背景：人工智能正在提高癌症诊断的效率和准确性。病理学AI系统的性能几乎只在来自大型中心的欧美队列上进行了评估。为了在全球范围内采用病理学AI，需要对目前代表性不足的人群进行验证研究，这些人群从AI支持中获得的潜在收益可能最大。我们展示了第一个来自中东的外部验证队列研究，专注于基于AI的前列腺癌诊断和Gleason分级。方法：我们收集并数字化了来自伊拉克库尔德地区的339个前列腺活检标本，代表2013-2024年间连续的185例患者。我们评估了一个任务特定的端到端AI模型和两个基础模型，在它们与病理学家的一致性和在三种扫描仪型号（Hamamatsu、Leica和Grundium）上数字化样本的一致性方面。发现：AI与病理学家之间的分级一致性类似于病理学家之间的分级一致性，Cohen二次加权kappa值为0.801 vs 0.799（p=0.9824）。所有AI模型和扫描仪对的跨扫描仪一致性都很高（二次加权kappa &gt; 0.90），包括低成本紧凑型扫描仪。结论：AI模型在前列腺组织病理学评估中表现出与病理学家相当的性能。紧凑型扫描仪可为非数字化环境中的验证研究提供途径，并使样本量有限的实验室能够经济有效地采用AI。这是中东第一个公开可用的数字病理数据集，支持进一步研究全球公平的AI病理学。资金：SciLifeLab和Wallenberg数据驱动生命科学项目，Instrumentarium科学基金会，卡罗林斯卡研究所研究基金会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background: Artificial intelligence (AI) is improving the efficiency and accuracy of cancer diagnostics. The performance of pathology AI systems has been almost exclusively evaluated on European and US cohorts from large centers. For global AI adoption in pathology, validation studies on currently under-represented populations - where the potential gains from AI support may also be greatest - are needed. We present the first study with an external validation cohort from the Middle East, focusing on AI-based diagnosis and Gleason grading of prostate cancer.  Methods: We collected and digitised 339 prostate biopsy specimens from the Kurdistan region, Iraq, representing a consecutive series of 185 patients spanning the period 2013-2024. We evaluated a task-specific end-to-end AI model and two foundation models in terms of their concordance with pathologists and consistency across samples digitised on three scanner models (Hamamatsu, Leica, and Grundium).  Findings: Grading concordance between AI and pathologists was similar to pathologist-pathologist concordance with Cohen's quadratically weighted kappa 0.801 vs. 0.799 (p=0.9824). Cross-scanner concordance was high (quadratically weighted kappa &gt; 0.90) for all AI models and scanner pairs, including low-cost compact scanner.  Interpretation: AI models demonstrated pathologist-level performance in prostate histopathology assessment. Compact scanners can provide a route for validation studies in non-digitalised settings and enable cost-effective adoption of AI in laboratories with limited sample volumes. This first openly available digital pathology dataset from the Middle East supports further research into globally equitable AI pathology.  Funding: SciLifeLab and Wallenberg Data Driven Life Science Program, Instrumentarium Science Foundation, Karolinska Institutet Research Foundation.</description>
      <author>example@mail.com (Peshawa J. Muhammad Ali, Navin Vincent, Saman S. Abdulla, Han N. Mohammed Fadhl, Anders Blilie, Kelvin Szolnoky, Julia Anna Mielcarz, Xiaoyi Ji, Nita Mulliqi, Abdulbasit K. Al-Talabani, Kimmo Kartasalo)</author>
      <guid isPermaLink="false">2512.17499v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding</title>
      <link>http://arxiv.org/abs/2512.17492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为MMLANDMARKS的多模态地标数据集，包含四种模态：高分辨率航空图像、地面视角图像、文本信息和地理坐标，用于支持多种地理空间分析任务。&lt;h4&gt;背景&lt;/h4&gt;地理空间分析受益于多模态方法，因为每个地理位置可以通过多种方式描述。然而，当前地理空间基准在模态覆盖方面有限，限制了该领域的进展，因为现有方法无法在统一框架内整合所有相关模态。&lt;h4&gt;目的&lt;/h4&gt;引入MMLANDMARKS数据集，这是一个包含四种模态的基准数据集，用于训练和评估各种地理空间任务的模型。&lt;h4&gt;方法&lt;/h4&gt;MMLANDMARKS数据集包含197k高分辨率航空图像、329k地面视角图像、文本信息和美国18,557个不同地标的地理坐标，每个模态间有一一对应关系，支持跨视图地面到卫星检索、地面和卫星地理定位、文本到图像和文本到GPS检索等任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过采用一个简单的受CLIP启发的基线方法，作者展示了在不同任务上的广泛泛化能力和与现成基础模型和专门的最先进模型相比的竞争性能。&lt;h4&gt;结论&lt;/h4&gt;实现广泛的地理空间理解需要多模态数据集的支持。&lt;h4&gt;翻译&lt;/h4&gt;地理空间分析通过多模态方法受益，因为每个地理位置都可以通过多种方式描述（不同视角的图像、文本描述和地理坐标）。当前的地理空间基准在模态覆盖方面有限，显著限制了该领域的进展，因为当前方法无法在统一框架内整合所有相关模态。我们引入了多模态地标数据集（MMLANDMARKS），这是一个由四种模态组成的基准：197k高分辨率航空图像、329k地面视角图像、文本信息以及美国18,557个不同地标的地理坐标。MMLANDMARKS数据集在每个模态上都具有一一对应关系，能够为各种地理空间任务（包括跨视图地面到卫星检索、地面和卫星地理定位、文本到图像和文本到GPS检索）的训练和基准测试模型。通过采用一个简单的受CLIP启发的基线方法，我们展示了在不同任务上的广泛泛化能力和与现成基础模型和专门的最先进模型相比的竞争性能，这说明了实现广泛地理空间理解需要多模态数据集的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geo-spatial analysis of our world benefits from a multimodal approach, as every single geographic location can be described in numerous ways (images from various viewpoints, textual descriptions, and geographic coordinates). Current geo-spatial benchmarks have limited coverage across modalities, considerably restricting progress in the field, as current approaches cannot integrate all relevant modalities within a unified framework. We introduce the Multi-Modal Landmark dataset (MMLANDMARKS), a benchmark composed of four modalities: 197k highresolution aerial images, 329k ground-view images, textual information, and geographic coordinates for 18,557 distinct landmarks in the United States. The MMLANDMARKS dataset has a one-to-one correspondence across every modality, which enables training and benchmarking models for various geo-spatial tasks, including cross-view Ground-to-Satellite retrieval, ground and satellite geolocalization, Text-to-Image, and Text-to-GPS retrieval. We demonstrate broad generalization and competitive performance against off-the-shelf foundational models and specialized state-of-the-art models across different tasks by employing a simple CLIP-inspired baseline, illustrating the necessity for multimodal datasets to achieve broad geo-spatial understanding.</description>
      <author>example@mail.com (Oskar Kristoffersen, Alba R. Sánchez, Morten R. Hannemose, Anders B. Dahl, Dim P. Papadopoulos)</author>
      <guid isPermaLink="false">2512.17492v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing</title>
      <link>http://arxiv.org/abs/2512.17224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Any Optical Model (AOM)，一种通用的遥感基础模型，能够处理不同光学卫星的任意波段组成、传感器类型和分辨率尺度，解决了现有模型在缺失波段、跨传感器融合和跨分辨率场景下的局限性。&lt;h4&gt;背景&lt;/h4&gt;光学卫星具有多样化的波段布局和地面采样距离，为生态系统监测到应急响应等多种任务提供关键证据。然而，不同光学传感器的波段组成和空间分辨率差异显著，现有的遥感基础模型通常在固定配置下预训练，难以应对现实世界中的复杂场景。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应任意波段组成、传感器类型和分辨率尺度的通用遥感基础模型，提高模型在现实应用中的泛化能力和实用性。&lt;h4&gt;方法&lt;/h4&gt;AOM采用三种关键技术：1) 光谱无关的tokenizer为每个通道分配专门波段嵌入，明确编码光谱特性；2) 多尺度自适应块嵌入机制动态调整感受野，捕获从亚米到百米尺度的纹理和上下文模式；3) 多尺度语义对齐机制结合通道级自监督掩码重建预训练策略，共同建模光谱-空间关系并保持全局语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在超过10个公共数据集（包括Sentinel-2、Landsat和HLS数据）上的广泛实验表明，AOM在缺失波段、跨传感器和跨分辨率设置等挑战性条件下持续达到最先进(SOTA)性能。&lt;h4&gt;结论&lt;/h4&gt;AOM成功解决了现有遥感基础模型在处理不同光学卫星数据时的局限性，为遥感基础模型在现实世界应用中的泛化能力和实际部署提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;光学卫星凭借其多样的波段布局和地面采样距离，为从生态系统监测到应急响应等任务提供了不可或缺的证据。然而，不同光学传感器在波段组成和空间分辨率上的显著差异，给现有的遥感基础模型(RSFMs)带来了主要挑战。这些模型通常在固定的波段配置和分辨率上预训练，在面对涉及缺失波段、跨传感器融合和未见空间尺度的现实场景时容易受到影响，从而限制了它们的泛化能力和实际部署。为解决这些局限性，我们提出了Any Optical Model (AOM)，一种专门设计用于适应任意波段组成、传感器类型和分辨率尺度的通用RSFM。为在缺失或新增波段时保持独特的光谱特性，AOM引入了一种与光谱无关的tokenizer，为每个通道分配专门的波段嵌入，实现光谱身份的明确编码。为有效捕获从亚米到百米图像的纹理和上下文模式，我们设计了一种多尺度自适应块嵌入机制，可动态调整感受野。此外，为在不同分辨率间保持全局语义一致性，AOM结合了多尺度语义对齐机制和通道级自监督掩码重建预训练策略，共同建模光谱-空间关系。在Sentinel-2、Landsat和HLS等超过10个公共数据集上的广泛实验表明，AOM在缺失波段、跨传感器和跨分辨率设置等挑战条件下持续取得了最先进(SOTA)的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.</description>
      <author>example@mail.com (Xuyang Li, Chenyu Li, Danfeng Hong)</author>
      <guid isPermaLink="false">2512.17224v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors</title>
      <link>http://arxiv.org/abs/2512.17146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队引入了安全代理基因组评估器(SAGE)，用于审计基因组基础模型(GFMs)的对抗性漏洞，发现即使是最先进的模型如ESM2也容易受到软提示攻击的影响。&lt;h4&gt;背景&lt;/h4&gt;基因组基础模型(GFMs)如进化尺度建模(ESM)在变异效应预测方面表现出色，但它们在对抗性操纵下的安全性和鲁棒性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入安全代理基因组评估器(SAGE)，一个用于审计GFMs对抗性漏洞的代理框架，以解决基因组基础模型安全性研究空白的问题。&lt;h4&gt;方法&lt;/h4&gt;SAGE通过可解释和自动化的风险审计循环运行，注入软提示扰动，监控模型在训练检查点上的行为，计算AUROC和AUPR等风险指标，并使用基于大型语言模型的叙述解释生成结构化报告，从而在不修改底层模型的情况下持续评估嵌入空间的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;即使是最先进的GFMs如ESM2也容易受到针对软提示攻击的影响，这些攻击会导致可测量的性能下降，揭示了基因组基础模型中关键且先前隐藏的漏洞。&lt;h4&gt;结论&lt;/h4&gt;代理风险审计对于确保临床变异解释等生物医学应用的安全性至关重要。&lt;h4&gt;翻译&lt;/h4&gt;基因组基础模型（GFMs），如进化尺度建模（ESM），在变异效应预测方面已经显示出显著的成功。然而，它们在对抗性操纵下的安全性和鲁棒性在很大程度上仍未被探索。为了解决这一空白，我们引入了安全代理基因组评估器（SAGE），这是一个用于审计GFMs对抗性漏洞的代理框架。SAGE通过可解释和自动化的风险审计循环运行。它注入软提示扰动，监控模型在训练检查点上的行为，计算AUROC和AUPR等风险指标，并使用基于大型语言模型的叙述解释生成结构化报告。这种代理过程能够在不修改底层模型的情况下持续评估嵌入空间的鲁棒性。使用SAGE，我们发现即使是像ESM2这样的最先进的GFMs也容易受到针对软提示攻击的影响，导致可测量的性能下降。这些发现揭示了基因组基础模型中关键且先前隐藏的漏洞，表明在保障临床变异解释等生物医学应用中，代理风险审计的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.</description>
      <author>example@mail.com (Huixin Zhan)</author>
      <guid isPermaLink="false">2512.17146v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction</title>
      <link>http://arxiv.org/abs/2512.17137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了可扩展深度展开模型(SDUM)，一个通用的MRI深度学习重建框架，能够在多种成像协议上实现高性能重建，无需针对特定任务进行微调。&lt;h4&gt;背景&lt;/h4&gt;临床MRI包含多种成像协议，涵盖不同的解剖目标、对比度、采样模式和加速因子，而当前深度学习重建方法通常是特定于协议的，限制了其泛化和部署能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的MRI深度学习重建框架，能够处理多种不同的成像协议，实现高性能重建。&lt;h4&gt;方法&lt;/h4&gt;提出可扩展深度展开模型(SDUM)，这是一个通用框架，包含基于Restormer的重建器、学习的线圈敏感度图估计器(CSME)、采样感知的加权数据一致性(SWDC)、级联索引和协议元数据的通用条件化(UC)以及渐进式级联扩展训练。&lt;h4&gt;主要发现&lt;/h4&gt;SDUM表现出类似基础模型的扩展行为；单个SDUM在异构数据上训练后，在CMRxRecon2025挑战赛的四个赛道上达到最先进结果，比专用基线高出最多1.0 dB；在CMRxRecon2024上比获胜方法高出0.55 dB；在fastMRI大脑上超过PC-RNN，高出1.8 dB；消融研究验证了各组件的有效性。&lt;h4&gt;结论&lt;/h4&gt;SDUM为通用、可扩展的MRI重建提供了一条实用路径。&lt;h4&gt;翻译&lt;/h4&gt;临床MRI包含多种成像协议，涵盖不同的解剖目标（心脏、大脑、膝盖）、对比度（T1、T2、mapping）、采样模式（笛卡尔、径向、螺旋、kt空间）和加速因子，然而当前深度学习重建通常是特定于协议的，阻碍了泛化和部署。我们引入了可扩展深度展开模型（SDUM），这是一个通用框架，结合了基于Restormer的重建器、学习的线圈敏感度图估计器（CSME）、采样感知的加权数据一致性（SWDC）、级联索引和协议元数据的通用条件化（UC）以及渐进式级联扩展训练。SDUM表现出类似基础模型的扩展行为：重建质量随参数的对数增长而提高，相关系数r=0.986（R²=0.973），最多到18个级联，证明了随着模型深度的性能增益可预测。单个在异构数据上训练的SDUM无需任务特定的微调，在CMRxRecon2025挑战赛的所有四个赛道上实现了最先进的结果——多中心、多疾病、5T和儿科，比专用基线高出最多1.0 dB。在CMRxRecon2024上，SDUM比获胜方法PromptMR+高出0.55 dB；在fastMRI大脑上，它超过了PC-RNN，高出1.8 dB。消融研究验证了每个组件的贡献：SWDC比标准DC高出0.43 dB，每个级联的CSME高出0.51 dB，UC高出0.38 dB。这些结果确立了SDUM作为通用、可扩展MRI重建的实用路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clinical MRI encompasses diverse imaging protocols--spanning anatomical targets (cardiac, brain, knee), contrasts (T1, T2, mapping), sampling patterns (Cartesian, radial, spiral, kt-space), and acceleration factors--yet current deep learning reconstructions are typically protocol-specific, hindering generalization and deployment. We introduce Scalable Deep Unrolled Model (SDUM), a universal framework combining a Restormer-based reconstructor, a learned coil sensitivity map estimator (CSME), sampling-aware weighted data consistency (SWDC), universal conditioning (UC) on cascade index and protocol metadata, and progressive cascade expansion training. SDUM exhibits foundation-model-like scaling behavior: reconstruction quality follows PSNR ${\sim}$ log(parameters) with correlation $r{=}0.986$ ($R^2{=}0.973$) up to 18 cascades, demonstrating predictable performance gains with model depth. A single SDUM trained on heterogeneous data achieves state-of-the-art results across all four CMRxRecon2025 challenge tracks--multi-center, multi-disease, 5T, and pediatric--without task-specific fine-tuning, surpassing specialized baselines by up to ${+}1.0$~dB. On CMRxRecon2024, SDUM outperforms the winning method PromptMR+ by ${+}0.55$~dB; on fastMRI brain, it exceeds PC-RNN by ${+}1.8$~dB. Ablations validate each component: SWDC ${+}0.43$~dB over standard DC, per-cascade CSME ${+}0.51$~dB, UC ${+}0.38$~dB. These results establish SDUM as a practical path toward universal, scalable MRI reconstruction.</description>
      <author>example@mail.com (Puyang Wang, Pengfei Guo, Keyi Chai, Jinyuan Zhou, Daguang Xu, Shanshan Jiang)</author>
      <guid isPermaLink="false">2512.17137v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Sigma-MoE-Tiny Technical Report</title>
      <link>http://arxiv.org/abs/2512.16248v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Sigma-MoE-Tiny，一种实现最高稀疏性的Mixture-of-Experts语言模型，通过细粒度专家分割和渐进式稀疏化调度解决了专家负载平衡问题，尽管只激活0.5B参数，仍能实现与更大规模模型相当的性能。&lt;h4&gt;背景&lt;/h4&gt;Mixture-of-Experts (MoE)已成为基础模型的一种有前途的范式，因其高效和强大的可扩展性。然而，MoE模型面临的主要挑战是专家负载平衡问题，特别是在极端稀疏的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种实现最高稀疏性的MoE语言模型，解决极端稀疏性带来的专家负载平衡问题，并在保持训练稳定性的同时平衡专家利用率。&lt;h4&gt;方法&lt;/h4&gt;采用细粒度专家分割，每层最多96个专家，为每个token只激活一个专家，实现20B总参数中仅0.5B激活；提出渐进式稀疏化调度来解决负载平衡问题；在多样化和高质量语料上进行预训练，然后进行后训练以进一步释放能力。&lt;h4&gt;主要发现&lt;/h4&gt;广泛使用的负载平衡损失在低层设置下变得无效；渐进式稀疏化调度能有效平衡专家利用率和训练稳定性；整个训练过程保持稳定，没有出现不可恢复的损失尖峰；尽管只激活0.5B参数，Sigma-MoE-Tiny仍能在可比或显著更大规模的同类模型中实现顶级性能。&lt;h4&gt;结论&lt;/h4&gt;Sigma-MoE-Tiny证明了在极端稀疏条件下实现高性能MoE模型的可能性；对高度稀疏MoE模型中负载平衡的深入讨论为未来MoE架构的稀疏性进步提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;混合专家模型已成为基础模型的一种有前途的范式，因其高效和强大的可扩展性。在本工作中，我们提出了Sigma-MoE-Tiny，一种MoE语言模型，与现有开源模型相比实现了最高的稀疏性。Sigma-MoE-Tiny采用细粒度专家分割，每层最多96个专家，同时只为每个token激活一个专家，从而在20B总参数中仅激活0.5B。这种极端稀疏性带来的主要挑战在于专家负载平衡。我们发现，在这种设置下，广泛使用的负载平衡损失在较低层 tends to become ineffective。为解决这一问题，我们提出了一种渐进式稀疏化调度，旨在平衡专家利用率和训练稳定性。Sigma-MoE-Tiny在多样化和高质量语料上进行预训练，随后进行后训练以进一步释放其能力。整个训练过程保持显著稳定，没有发生不可恢复的损失尖峰。全面评估显示，尽管仅激活0.5B参数，Sigma-MoE-Tiny在可比或显著更大规模的同类模型中实现了顶级性能。此外，我们对高度稀疏MoE模型中的负载平衡进行了深入讨论，为未来MoE架构的稀疏性进步提供了见解。项目页面：https://qghuxmu.github.io/Sigma-MoE-Tiny 代码：https://github.com/microsoft/ltp-megatron-lm&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny  Code: https://github.com/microsoft/ltp-megatron-lm</description>
      <author>example@mail.com (Qingguo Hu, Zhenghao Lin, Ziyue Yang, Yucheng Ding, Xiao Liu, Yuting Jiang, Ruizhe Wang, Tianyu Chen, Zhongxin Guo, Yifan Xiong, Rui Gao, Lei Qu, Jinsong Su, Peng Cheng, Yeyun Gong)</author>
      <guid isPermaLink="false">2512.16248v2</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2512.17774v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的3D医学图像分割骨干网络MedNeXt-v2，通过改进微架构和数据缩放策略，实现了在大规模监督预训练下的最先进性能。研究还发现，更强的骨干网络在相似数据上表现更好，表示缩放对病理分割的益处更大，且模态特定预训练在完全微调后几乎没有额外好处。&lt;h4&gt;背景&lt;/h4&gt;大规模监督预训练正在迅速改变3D医学图像分割领域，但现有工作主要专注于增加数据集规模，忽略了骨干网络在大规模时是否是有效的表示学习器这一问题。&lt;h4&gt;目的&lt;/h4&gt;重新审视用于体积分割的ConvNeXt架构，引入MedNeXt-v2这一复合缩放的3D ConvNeXt，利用改进的微架构和数据缩放来实现最先进的性能。&lt;h4&gt;方法&lt;/h4&gt;首先证明常用骨干网络在大规模预训练中通常不是最优的；然后在扩展前进行全面的骨干网络基准测试，证明更强的从头开始性能可靠地预测预训练后的下游性能；结合3D全局响应归一化模块，使用深度、宽度和上下文缩放改进架构；在18k个体积CT上预训练MedNeXt-v2，并在六个CT和MR基准测试上进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;MedNeXt-v2在六个具有挑战性的CT和MR基准测试(144个结构)上展示了最先进的性能，比七个公开发布的预训练模型有显著改进；更强的骨干网络在相似数据上产生更好结果；表示缩放不成比例地有利于病理分割；模态特定预训练在完全微调后几乎没有额外好处。&lt;h4&gt;结论&lt;/h4&gt;MedNeXt-v2是3D医学图像分割中大规模监督表示学习的强大骨干网络，相关代码和预训练模型已公开在官方nnUNet存储库中。&lt;h4&gt;翻译&lt;/h4&gt;大规模监督预训练正在迅速改变3D医学图像分割。然而，现有工作主要专注于增加数据集规模，而忽略了骨干网络在大规模时是否是有效的表示学习器这一问题。在这项工作中，我们通过重新审视用于体积分割的ConvNeXt架构来解决这个问题，并引入了MedNeXt-v2，这是一种复合缩放的3D ConvNeXt，利用改进的微架构和数据缩放来实现最先进的性能。首先，我们证明大规模预训练流程中常用的骨干网络通常不是最优的。随后，我们在扩展前使用全面的骨干网络基准测试，并证明更强的从头开始性能可靠地预测预训练后的更强下游性能。在这些发现的指导下，我们结合了3D全局响应归一化模块，并使用深度、宽度和上下文缩放来改进我们的架构，以实现有效的表示学习。我们在18k个体积CT上预训练MedNeXt-v2，并在六个具有挑战性的CT和MR基准测试(144个结构)上进行微调时展示了最先进的性能，比七个公开发布的预训练模型显示出一致的改进。除了改进之外，我们对这些模型的基准测试还揭示更强的骨干网络在相似数据上产生更好的结果，表示缩放不成比例地有利于病理分割，并且一旦应用完全微调，模态特定的预训练几乎没有好处。总之，我们的研究结果表明MedNeXt-v2是3D医学图像分割中大规模监督表示学习的强大骨干网络。我们的代码和预训练模型可在官方nnUNet存储库中获取：https://www.github.com/MIC-DKFZ/nnUNet&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的问题是：在3D医学图像分割的大规模监督预训练中，研究者往往只关注增加数据集规模，却忽视了骨干网络是否适合作为有效的表示学习器。这个问题很重要，因为不合适的骨干网络会限制大规模数据的利用效率，影响下游分割任务的性能，导致计算资源浪费和模型效果不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有大规模预训练方法的三个系统性问题：使用遗留的骨干架构、未在验证骨干网络前扩展数据、仅与非预训练基线比较。通过系统性实验，他们发现更强的骨干网络在预训练后能带来更好的下游性能。基于这一发现，作者借鉴了ConvNeXt架构和EfficientNet的复合缩放理念，在MedNeXt基础上添加了3D全局响应归一化(GRN)模块，并采用了深度、宽度和上下文缩放策略，最终设计了MedNeXt-v2。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过改进骨干网络的微架构和合理缩放网络规模，结合大规模数据预训练，提高3D医学图像分割的性能。实现流程包括：1)在小规模数据集上验证多种骨干网络性能；2)在MedNeXt架构中添加3D GRN模块防止特征崩溃；3)应用深度、宽度和上下文缩放增加网络容量；4)在18k个体积的CT数据上进行预训练；5)在下游任务上微调，使用更大的输入上下文(192×192×192)提升性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统性验证骨干网络选择在大规模预训练中的重要性；2)引入3D全局响应归一化(GRN)模块提升表示学习效果；3)采用复合缩放策略同时优化深度、宽度和上下文；4)提出'小上下文预训练+大上下文微调'的策略平衡计算成本和性能；5)首次对七个公开发布的预训练模型进行系统性基准测试。相比之前工作，MedNeXt-v2更注重骨干网络质量而非仅关注数据规模，通过微架构改进和合理缩放实现了更高效的表示学习。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MedNeXt-v2通过改进3D ConvNeXt架构的微架构和复合缩放策略，结合大规模预训练，实现了3D医学图像分割的最先进性能，并首次系统性地评估了多种预训练模型的性能，为该领域提供了重要的见解和基准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet</description>
      <author>example@mail.com (Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein)</author>
      <guid isPermaLink="false">2512.17774v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Alzheimer's Disease Brain Network Mining</title>
      <link>http://arxiv.org/abs/2512.17276v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MATCH-AD是一种半监督学习框架，通过整合深度表示学习、基于图的标签传播和最优传输理论，解决了阿尔茨海默病诊断中标签稀缺的问题，实现了接近完美的诊断准确率。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病诊断的机器学习方法面临根本性挑战，临床评估昂贵且有创，导致神经影像数据集中只有小部分有真实标签。全球积累了大量部分注释的神经影像数据，但其诊断潜力尚未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;开发一种半监督学习框架，解决标签稀缺问题，利用神经影像数据中的流形结构，将有限标注样本的诊断信息传播到更大的未标注人群，并使用Wasserstein距离量化认知状态之间的疾病进展。&lt;h4&gt;方法&lt;/h4&gt;提出了Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD)框架，整合了深度表示学习、基于图的标签传播和最优传输理论。在近5000名国家阿尔茨海默病协调中心受试者上进行了评估，使用了来自数百个脑区域的结构MRI测量、脑脊液生物标志物和临床变量。&lt;h4&gt;主要发现&lt;/h4&gt;尽管真实标签不足三分之一，MATCH-AD实现了接近完美的诊断准确率；框架显著优于所有基线方法，实现了几乎完美的一致性(kappa)，而最佳基线方法仅有微弱一致性；即使在严重标签稀缺的情况下，性能仍保持临床有用性；提供了理论收敛保证，并证明了标签传播误差和传输稳定性的界限。&lt;h4&gt;结论&lt;/h4&gt;原则性的半监督学习可以解锁全球积累的大量部分注释神经影像数据的诊断潜力，大大减少了注释负担，同时保持了适合临床部署的准确性。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病(AD)诊断的机器学习方法面临根本性挑战。临床评估昂贵且有创，导致神经影像数据集中只有小部分有真实标签。我们引入了针对异质性阿尔茨海默病的多视图自适应传输聚类(MATCH-AD)，这是一个半监督框架，整合了深度表示学习、基于图的标签传播和最优传输理论来解决这个问题。该框架利用神经影像数据中的流形结构，将有限标注样本的诊断信息传播到更大的未标注人群，同时使用Wasserstein距离量化认知状态之间的疾病进展。在国家阿尔茨海默病协调中心近5000名受试者上进行了评估，包括来自数百个脑区域的结构MRI测量、脑脊液生物标志物和临床变量，尽管真实标签不足三分之一，MATCH-AD仍实现了接近完美的诊断准确率。该框架显著优于所有基线方法，与最佳基线方法的微弱一致性相比，实现了表示几乎完美一致性的kappa值，这是诊断可靠性的质的转变。即使在严重的标签稀缺情况下，性能仍保持临床有用性，我们提供了理论收敛保证，并证明了标签传播误差和传输稳定性的界限。这些结果表明，原则性的半监督学习可以解锁全球积累的大量部分注释神经影像数据的诊断潜力，大大减少注释负担，同时保持适合临床部署的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.</description>
      <author>example@mail.com (Alireza Moayedikia, Sara Fin)</author>
      <guid isPermaLink="false">2512.17276v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>A Theoretical Analysis of State Similarity Between Markov Decision Processes</title>
      <link>http://arxiv.org/abs/2512.17265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to an IEEE Transactions. arXiv admin note: substantial text overlap with arXiv:2509.18714&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了广义双模拟度量(GBSM)，解决了多个马尔可夫决策过程(MDP)之间状态相似性测量的挑战。GBSM具有严格的数学基础，并在策略转移、状态聚合和基于采样的估计等多个应用中表现出色。&lt;h4&gt;背景&lt;/h4&gt;双模拟度量(BSM)是分析马尔可夫决策过程中状态相似性的有力工具，已被成功用于强化学习中的状态表征学习和策略探索。然而，BSM应用于多个MDP之间的状态相似性仍然具有挑战性，先前的工作虽试图扩展BSM到成对MDP，但缺乏完善的数学性质限制了进一步的理论分析。&lt;h4&gt;目的&lt;/h4&gt;正式建立一种广义双模拟度量(GBSM)，用于测量任意成对MDP之间的状态相似性，并为其提供严格的数学基础。&lt;h4&gt;方法&lt;/h4&gt;严格证明GBSM具有三个基本度量性质：对称性、跨MDP三角不等式和相同空间上的距离界限。利用这些性质理论上分析跨MDP的策略转移、状态聚合和基于采样的估计，提供比基于标准BSM的现有界限更严格的显式界限，并给出改进的样本复杂度估计。&lt;h4&gt;主要发现&lt;/h4&gt;GBSM被严格证明具有三个基本度量性质；理论分析获得了比现有方法更严格的界限；提供了改进的样本复杂度估计；数值结果验证了理论发现并证明了GBSM在多MDP场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;广义双模拟度量(GBSM)为多个MDP之间的状态相似性测量提供了严格的数学基础和有效的工具，在策略转移、状态聚合和基于采样的估计等多个应用中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;双模拟度量(BSM)是分析马尔可夫决策过程(MDP)内状态相似性的有力工具，揭示了在BSM中更接近的状态具有更相似的最优值函数。虽然BSM已在强化学习(RL)中被成功用于状态表征学习和策略探索等任务，但其应用于多个MDP之间的状态相似性仍然具有挑战性。先前的工作试图将BSM扩展到成对的MDP，但缺乏完善的数学性质限制了MDP之间进一步的理论分析。在这项工作中，我们正式建立了一种广义双模拟度量(GBSM)，用于测量任意成对MDP之间的状态相似性，并通过三个基本度量性质严格证明了GBSM，即GBSM对称性、跨MDP三角不等式和相同空间上的距离界限。利用这些性质，我们理论上分析了跨MDP的策略转移、状态聚合和基于采样的估计，获得了比从标准BSM推导出的现有界限更严格的显式界限。此外，GBSM为估计提供了闭式样本复杂度，改进了基于BSM的现有渐近结果。数值结果验证了我们的理论发现，并证明了GBSM在多MDP场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.</description>
      <author>example@mail.com (Zhenyu Tao, Wei Xu, Xiaohu You)</author>
      <guid isPermaLink="false">2512.17265v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS</title>
      <link>http://arxiv.org/abs/2512.17262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SHARP-QoS的统一策略，用于联合预测多种服务质量(QoS)参数。该方法解决了现有方法在处理稀疏、嘈杂且具有层次依赖关系的QoS数据时面临的挑战，包括负迁移问题和表示学习不足的问题。&lt;h4&gt;背景&lt;/h4&gt;可靠的服务导向计算依赖于多种服务质量(QoS)参数来评估服务最优性。然而，现实世界中的QoS数据极其稀疏、嘈杂，并且受到QoS交互、地理和网络级别因素产生的层次依赖关系的影响，这使得准确的QoS预测具有挑战性。现有方法通常分别预测每个QoS参数，需要多个相似模型，增加了计算成本并导致泛化能力差。尽管最近的联合QoS预测研究探索了共享架构，但由于不同QoS参数间不一致的数值范围导致的损失缩放问题而遭受负迁移，并且在表示学习方面也存在不足，导致精度下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一的联合QoS预测策略，解决现有方法中的负迁移问题和表示学习不足的问题，提高QoS预测的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;SHARP-QoS采用三个组件解决这些问题：1) 双机制：通过在庞加莱球中定义的双曲卷积从QoS和上下文结构中提取层次特征；2) 自适应特征共享机制：允许在信息丰富的QoS和上下文信号之间进行特征交换，使用门控特征融合模块支持动态特征选择；3) 基于EMA的损失平衡策略：允许稳定的联合优化，从而减轻负迁移。&lt;h4&gt;主要发现&lt;/h4&gt;在具有两个、三个和四个QoS参数的三个数据集上的评估表明，SHARP-QoS优于单任务和多任务基线。广泛的研究表明，该模型有效地解决了主要挑战，包括稀疏性、对异常值的鲁棒性和冷启动问题，同时保持适度的计算开销。&lt;h4&gt;结论&lt;/h4&gt;SHARP-QoS能够进行可靠的联合QoS预测，解决了现有方法中的关键问题，并在多个数据集上展示了优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;可靠的服务导向计算依赖于多种服务质量(QoS)参数，这些参数对于评估服务最优性至关重要。然而，现实世界中的QoS数据极其稀疏、嘈杂，并且受到QoS交互、地理和网络级别因素产生的层次依赖关系的影响，这使得准确的QoS预测具有挑战性。现有方法通常分别预测每个QoS参数，需要多个相似模型，这增加了计算成本并导致泛化能力差。尽管最近的联合QoS预测研究探索了共享架构，但由于不同QoS参数间不一致的数值范围导致的损失缩放问题而遭受负迁移，并且在表示学习方面也存在不足，导致精度下降。本文提出了一种名为SHARP-QoS的联合QoS预测统一策略，它使用三个组件解决了这些问题。首先，我们引入了一种双机制，通过在庞加莱球中定义的双曲卷积从QoS和上下文结构中提取层次特征。其次，我们提出了一种自适应特征共享机制，允许在信息丰富的QoS和上下文信号之间进行特征交换。采用门控特征融合模块支持在结构化和共享表示之间的动态特征选择。第三，我们设计了一种基于EMA的损失平衡策略，允许稳定的联合优化，从而减轻负迁移。在具有两个、三个和四个QoS参数的三个数据集上的评估表明，SHARP-QoS优于单任务和多任务基线。广泛的研究表明，我们的模型有效地解决了主要挑战，包括稀疏性、对异常值的鲁棒性和冷启动问题，同时保持适度的计算开销，凸显了其进行可靠的联合QoS预测的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.</description>
      <author>example@mail.com (Suraj Kumar, Arvind Kumar, Soumi Chattopadhyay)</author>
      <guid isPermaLink="false">2512.17262v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Disentangled representations via score-based variational autoencoders</title>
      <link>http://arxiv.org/abs/2512.17127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SAMI（基于评分的自编码器用于多尺度推断），一种结合扩散模型和VAE理论框架的无监督表征学习方法，通过统一证据下界制定目标函数，利用扩散过程评分指导学习表征，使隐式结构信息变得明确可解释。&lt;h4&gt;背景&lt;/h4&gt;扩散模型和变分自编码器（VAE）是重要的无监督表征学习方法，但各自有不同的理论框架和应用特点，如何结合两者的优势是一个研究挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合扩散模型和VAE理论框架的无监督表征学习方法，以学习能够自动捕捉数据中有意义结构的表征。&lt;h4&gt;方法&lt;/h4&gt;提出SAMI方法，通过统一扩散模型和VAE的证据下界，制定基于原则的目标函数，利用底层扩散过程的评分指导来学习表征，并展示如何从预训练的扩散模型中提取有用表征。&lt;h4&gt;主要发现&lt;/h4&gt;SAMI能恢复合成数据集的真实生成因子；从复杂自然图像中学习分解的、语义的潜在维度；将视频编码为比其他编码器更直的潜在轨迹，尽管仅在静态图像上训练；可从预训练扩散模型中提取有用表征；其显式概率性公式为识别语义轴提供了新方法。&lt;h4&gt;结论&lt;/h4&gt;扩散模型中的隐式结构信息可以通过与VAE的协同组合变得明确且可解释，SAMI能够产生有意义的、结构化的表征，具有理论保证和实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了用于多尺度推断的基于评分的自编码器（SAMI），这是一种无监督表征学习方法，结合了扩散模型和VAE的理论框架。通过统一它们各自的证据下界，SAMI制定了一个基于原则的目标，通过底层扩散过程的评分指导来学习表征。得到的表征自动捕捉数据中的有意义结构：在我们的合成数据集中恢复真实生成因子，从复杂自然图像中学习分解的、语义的潜在维度，并将视频序列编码为比其他编码器更直的潜在轨迹，尽管仅在静态图像上训练。此外，SAMI可以从预训练的扩散模型中提取有用的表征，只需最少的额外训练。最后，显式概率性公式为在没有监督标签的情况下识别语义轴提供了新方法，其数学精确性使我们能够对学习表征的性质做出正式陈述。总体而言，这些结果表明，扩散模型中的隐式结构信息可以通过与变分自编码器的协同组合变得明确且可解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.</description>
      <author>example@mail.com (Benjamin S. H. Lyo, Eero P. Simoncelli, Cristina Savin)</author>
      <guid isPermaLink="false">2512.17127v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</title>
      <link>http://arxiv.org/abs/2512.17620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures. This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StereoMV2D是一种统一框架，将时间立体建模集成到2D检测引导的多视角3D检测器中，通过利用相邻帧间同一物体的跨时间视差增强深度感知，优化查询先验，并在2D感兴趣区域内高效完成计算，实现了优越的检测性能。&lt;h4&gt;背景&lt;/h4&gt;多视角3D目标检测是自动驾驶感知中的基础任务，平衡检测精度和计算效率至关重要。稀疏查询式3D检测器通过一组可学习查询高效聚合多视角图像中的目标相关特征，但单帧2D检测中的固有深度模糊限制了3D查询生成的准确性。&lt;h4&gt;目的&lt;/h4&gt;解决单帧2D检测中的深度模糊问题，提高多视角3D目标检测的精度和召回率，同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出StereoMV2D框架，集成时间立体建模到2D检测引导的多视角3D检测器中；利用相邻帧间同一物体的跨时间视差增强深度感知；在2D感兴趣区域内高效完成计算；采用动态置信度门控机制评估时间立体线索的可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;通过利用跨时间视差，StereoMV2D能够增强深度感知并优化查询先验；动态置信度门控机制确保在物体外观变化和遮挡情况下的鲁棒检测；在nuScenes和Argoverse 2数据集上实现了优越的检测性能，且没有显著增加计算开销。&lt;h4&gt;结论&lt;/h4&gt;StereoMV2D是一种有效的统一框架，能够提高多视角3D目标检测的准确性，同时保持计算效率，适用于自动驾驶感知任务。&lt;h4&gt;翻译&lt;/h4&gt;多视角3D目标检测是自动驾驶感知中的基础任务，其中平衡检测精度和计算效率仍然至关重要。稀疏查询式3D检测器通过一组可学习查询高效地从多视角图像中聚合目标相关特征，提供了一种简洁且端到端的检测范式。基于这一基础，MV2D利用2D检测结果为查询初始化提供高质量的目标先验，实现更高的精度和召回率。然而，单帧2D检测中的固有深度模糊仍然限制了3D查询生成的准确性。为解决这一问题，我们提出了StereoMV2D，这是一个统一框架，将时间立体建模集成到2D检测引导的多视角3D检测器中。通过利用相邻帧间同一物体的跨时间视差，StereoMV2D增强了深度感知并优化了查询先验，同时在2D感兴趣区域(RoIs)内高效完成所有计算。此外，动态置信度门控机制通过学习来自帧间匹配矩阵的统计模式以及外观一致性，自适应地评估时间立体线索的可靠性，确保在物体外观和遮挡情况下的鲁棒检测。在nuScenes和Argoverse 2数据集上的大量实验表明，StereoMV2D实现了优越的检测性能，而没有带来显著的计算开销。代码将在https://github.com/Uddd821/StereoMV2D上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.</description>
      <author>example@mail.com (Di Wu, Feng Yang, Wenhui Zhao, Jinwen Yu, Pan Liao, Benlian Xu, Dingwen Zhang)</author>
      <guid isPermaLink="false">2512.17620v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2512.08557v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 Pages, 27 Figures, This work has been submitted to the IEEE Sensors Journal for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用LiDAR扫描的连续运动特性，通过检测帧间变化的区域来集中目标检测，减少计算量同时保持准确性。&lt;h4&gt;背景&lt;/h4&gt;LiDAR扫描会产生大量数据，传统方法需要处理整个点云，计算效率低下。&lt;h4&gt;目的&lt;/h4&gt;减少处理时间，提高计算效率，同时保持检测准确性。&lt;h4&gt;方法&lt;/h4&gt;使用滑动时间窗口和短步长处理，考虑时间维度存储卷积结果，忽略未变化区域，提出具有时间数据回收的稀疏散射卷积算法(SSCATeR)，仅对点云变化部分进行处理。&lt;h4&gt;主要发现&lt;/h4&gt;处理时间减少了最多6.61倍，输出的特征图与传统稀疏卷积技术相同，大幅提高了网络计算效率。&lt;h4&gt;结论&lt;/h4&gt;通过专注于变化区域并重用数据，该方法能够在保持相同检测精度的同时显著减少计算时间。&lt;h4&gt;翻译&lt;/h4&gt;这项工作利用LiDAR扫描的连续扫描运动特性，通过将目标检测集中在从一帧到另一帧点数据发生变化的特定区域。我们通过使用短步长的滑动时间窗口实现这一点，并通过存储连续扫描间的卷积结果来考虑时间维度。这使得我们可以忽略未变化的区域，显著减少每前向传播的卷积操作数量，而不会牺牲准确性。这种数据重用方案为检测数据引入了极端稀疏性。为了利用这种稀疏性，我们扩展了之前基于散射的卷积工作，允许数据重用，并提出了具有时间数据回收的稀疏散射卷积算法(SSCATeR)。该操作将传入的LiDAR数据视为连续流，仅对点云的变化部分进行处理。通过这样做，我们实现了最多6.61倍的处理时间减少，同时获得相同的结果。我们的测试结果显示，我们方法输出的特征图与传统稀疏卷积技术产生的特征图相同，同时大大提高了网络的计算效率。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云处理中的实时3D目标检测延迟问题。在自主无人机和自动驾驶车辆中，感知和检测（SAD）对于在动态环境中运行至关重要，而处理延迟会影响安全性。例如，商业无人机如DJI M300在200毫秒内可以移动超过4米，因此快速响应对于避免碰撞非常重要。现有方法即使在点云大部分区域不变的情况下，也会重复计算，导致不必要的延迟。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到LiDAR扫描具有时空特性，连续帧间大部分点云保持不变。他们思考如何将计算工作集中在最新变化的区域，避免重复计算。他们借鉴了PointPillars架构，扩展了自己之前关于基于散射的卷积的工作，并利用了稀疏卷积技术处理点云稀疏性。关键创新是引入短时间窗口（10毫秒）和变化地图机制，只处理变化区域并重用未变化区域的先前计算结果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用LiDAR扫描的连续性，只处理点云中发生变化的部分，重用未变化区域的先前计算结果。整体流程包括：1)使用10毫秒间隔收集点云数据；2)在柱特征网络中将点云组织成柱，创建变化地图跟踪哪些区域发生变化；3)在卷积骨干中，只对变化站点应用稀疏散射卷积，使用反卷积移除先前输入的影响；4)在检测头生成3D边界框预测。这种方法通过'时间数据回收'机制显著减少计算量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)时间数据回收机制，重用未变化区域的计算结果；2)短时间窗口处理（10毫秒而非传统的100毫秒）；3)变化地图技术，精确跟踪变化区域；4)稀疏散射卷积算法（SSCATeR）。相比之前工作，SSCATeR减少了59.85%的处理时间，避免了LSTM方法3.5-4倍的处理时间增加，避免了Transformer的慢推理问题，也避免了基于采样方法可能排除安全关键对象的风险，同时进一步利用了时间维度而非仅空间稀疏性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SSCATeR通过只处理LiDAR点云中的变化部分并重用未变化区域的先前计算结果，实现了实时3D目标检测，将处理时间减少了最多6.61倍而不牺牲准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.</description>
      <author>example@mail.com (Alexander Dow, Manduhu Manduhu, Matheus Santos, Ben Bartlett, Gerard Dooly, James Riordan)</author>
      <guid isPermaLink="false">2512.08557v2</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Data-Driven Calibration of Large Liquid Detectors with Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2512.17866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用无监督深度学习从大型液体闪烁探测器的物理数据中提取光电倍增管校准时间常数的新方法。&lt;h4&gt;背景&lt;/h4&gt;在大型液体闪烁探测器中，光电倍增管(PMT)的校准对于精确的物理测量至关重要，需要开发新的校准技术。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从物理数据中自动提取PMT校准时间常数的方法，无需依赖传统的校准源或复杂的人工干预。&lt;h4&gt;方法&lt;/h4&gt;使用无监督深度学习技术，在损失函数中嵌入简化的光子传输物理模型，将PMT校准常数作为自由参数，并假设单个事件代表点状发射，通过深度学习架构和自动微分框架实现可处理性。&lt;h4&gt;主要发现&lt;/h4&gt;使用SNO+探测器的9300个PMT的数据，该方法成功为超过7500个在线PMT中的每一个提取了3个校准常数，利用的是放射性背景事件。&lt;h4&gt;结论&lt;/h4&gt;这种基于深度学习的PMT校准方法可靠且有效，并且可以轻松推广到各种应用场景中，为大型探测器的校准提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文展示了一种新颖的方法，利用无监督深度学习的机制，从大型液体闪烁探测器的物理数据中提取光电倍增管(PMT)校准时间常数。该方法在损失函数中使用简化的光子传输物理模型，将PMT校准常数视为自由参数，并简单假设单个事件代表点状发射。因此，问题有效地简化为大规模回归问题，通过深度学习架构和自动微分框架使其变得可处理。使用SNO+探测器的9300个PMT的数据，该方法已被证明能够可靠地为超过7500个在线PMT中的每一个提取3个校准常数，使用的是放射性背景事件。我们相信这种方法可以轻松推广到广泛的应用中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper demonstrates a novel method to extract photomultiplier tube (PMT) calibration timing constants in large liquid scintillation detectors from physics data using the machinery of unsupervised deep learning. The approach uses a simplified physical model of optical photon transport in the loss function, with PMT calibration constants treated as free parameters, and the simple assumption that individual events represent point-like emission. The problem is, thus, effectively reduced to that of regression on a very large scale, made tractable by deep learning architectures and automatic differentiation frameworks. Using data from the 9,300 PMTs in the SNO+ detector, the method has been shown to reliably extract 3 calibration constants for each of the over 7,500 online PMTs using radioactive background events. We believe that this basic approach can be straightforwardly generalised for a wide range of applications.</description>
      <author>example@mail.com (Scott DeGraw, Steve Biller, Armin Reichold)</author>
      <guid isPermaLink="false">2512.17866v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection</title>
      <link>http://arxiv.org/abs/2512.17730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用大型视觉-语言模型CLIP解决深度伪造检测的泛化问题，提出Diff-Gen数据集和AdaptPrompt框架，在25个测试集上建立了新的最先进水平。&lt;h4&gt;背景&lt;/h4&gt;图像生成技术的进步使得高度逼真的合成媒体广泛可用，增加了深度伪造检测的难度。主要挑战是泛化能力，因为针对有限类型生成器训练的检测器在面对未见过的模型时往往会失败。&lt;h4&gt;目的&lt;/h4&gt;解决对可泛化深度伪造检测的迫切需求，利用大型视觉-语言模型识别各种生成技术合成的虚假内容。&lt;h4&gt;方法&lt;/h4&gt;引入Diff-Gen数据集（包含10万张扩散生成伪造图像）和AdaptPrompt框架（参数高效的迁移学习方法，联合学习文本提示和视觉适配器，同时保持CLIP主干冻结）。通过层消融实验优化视觉编码器结构以增强高频生成伪影的保留。&lt;h4&gt;主要发现&lt;/h4&gt;在Diff-Gen上训练的模型表现出更强的跨域泛化能力；在25个具有挑战性的测试集上评估，涵盖GAN、扩散模型和商业工具生成的合成内容；在标准和跨域场景中都建立了新的最先进水平；通过少样本泛化（仅320张图像）和源归因证明了框架的多功能性。&lt;h4&gt;结论&lt;/h4&gt;通过利用大型视觉-语言模型和提出的新方法，显著提高了深度伪造检测的泛化能力，Diff-Gen数据集和AdaptPrompt框架为解决深度伪造检测中的泛化挑战提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图像生成的最新进展使得高度逼真的合成媒体广泛可用，增加了可靠深度伪造检测的难度。关键挑战是泛化能力，因为针对有限类型生成器训练的检测器在面对未见过的模型时往往会失败。在这项工作中，我们通过利用大型视觉-语言模型（特别是CLIP）来识别各种生成技术合成的虚假内容，解决了对可泛化检测的迫切需求。首先，我们引入Diff-Gen，一个包含10万张扩散生成伪造图像的大规模基准数据集，捕获了传统GAN数据集所不具备的广泛频谱伪影。在Diff-Gen上训练的模型表现出更强的跨域泛化能力，特别是在面对未见过的图像生成器时。其次，我们提出AdaptPrompt，一个参数高效的迁移学习框架，联合学习任务特定的文本提示和视觉适配器，同时保持CLIP主干网络冻结。我们通过层消融实验进一步表明，剪裁视觉编码器的最后一个transformer块可以增强高频生成伪影的保留，显著提高检测准确率。我们的评估涵盖了25个具有挑战性的测试集，包括GAN、扩散模型和商业工具生成的合成内容，在标准和跨域场景中都建立了新的最先进水平。我们通过少样本泛化（使用仅320张图像）和源归因进一步证明了框架的多功能性，能够在封闭集设置中精确识别生成器架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in image generation have led to the widespread availability of highly realistic synthetic media, increasing the difficulty of reliable deepfake detection. A key challenge is generalization, as detectors trained on a narrow class of generators often fail when confronted with unseen models. In this work, we address the pressing need for generalizable detection by leveraging large vision-language models, specifically CLIP, to identify synthetic content across diverse generative techniques. First, we introduce Diff-Gen, a large-scale benchmark dataset comprising 100k diffusion-generated fakes that capture broad spectral artifacts unlike traditional GAN datasets. Models trained on Diff-Gen demonstrate stronger cross-domain generalization, particularly on previously unseen image generators. Second, we propose AdaptPrompt, a parameter-efficient transfer learning framework that jointly learns task-specific textual prompts and visual adapters while keeping the CLIP backbone frozen. We further show via layer ablation that pruning the final transformer block of the vision encoder enhances the retention of high-frequency generative artifacts, significantly boosting detection accuracy. Our evaluation spans 25 challenging test sets, covering synthetic content generated by GANs, diffusion models, and commercial tools, establishing a new state-of-the-art in both standard and cross-domain scenarios. We further demonstrate the framework's versatility through few-shot generalization (using as few as 320 images) and source attribution, enabling the precise identification of generator architectures in closed-set settings.</description>
      <author>example@mail.com (Yichen Jiang, Mohammed Talha Alam, Sohail Ahmed Khan, Duc-Tien Dang-Nguyen, Fakhri Karray)</author>
      <guid isPermaLink="false">2512.17730v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>HydroGym: A Reinforcement Learning Platform for Fluid Dynamics</title>
      <link>http://arxiv.org/abs/2512.17534v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了HydroGym，一个用于流体控制研究的独立求解器的强化学习平台，集成了复杂的流动控制基准测试、可扩展运行时基础设施和最先进的强化学习算法，包含42个经过验证的环境。&lt;h4&gt;背景&lt;/h4&gt;流体流动建模和控制对交通、能源和医学等科学工程领域至关重要，可带来升力增加、阻力减少等效益。然而流体控制面临高维、非线性、多尺度等挑战，强化学习应用受限于缺乏标准化基准平台和高计算需求。&lt;h4&gt;目的&lt;/h4&gt;解决流体控制中强化学习应用的挑战，提供标准化平台和降低计算需求。&lt;h4&gt;方法&lt;/h4&gt;开发HydroGym平台，包含42个验证环境（从层流到湍流），提供不可微和可微求解器，通过梯度增强优化提高样本效率。&lt;h4&gt;主要发现&lt;/h4&gt;强化学习代理能发现边界层操纵等稳健控制原理；迁移学习显示控制器适应新条件可减少50%训练需求；平台具有高度可扩展性。&lt;h4&gt;结论&lt;/h4&gt;HydroGym为流体控制研究提供了全面解决方案，促进流体动力学、机器学习和控制领域的交叉研究，推动科技进步。&lt;h4&gt;翻译&lt;/h4&gt;建模和控制流体流动对于科学和工程的几个领域（包括交通、能源和医学）至关重要。有效的流动控制可以带来，例如，升力增加、阻力减少、混合增强和噪声减少。然而，控制流体面临几个重大挑战，包括空间和时间上的高维、非线性和多尺度相互作用。强化学习（RL）最近在复杂领域（如机器人和蛋白质折叠）中显示出巨大成功，但其在流动控制中的应用受到缺乏标准化基准平台和流体模拟计算需求的阻碍。为了解决这些挑战，我们介绍了HydroGym，这是一个用于流动控制研究的独立求解器的强化学习平台。HydroGym集成了复杂的流动控制基准测试、可扩展的运行时基础设施和最先进的强化学习算法。我们的平台包括42个经过验证的环境，涵盖从经典层流到复杂的三维湍流场景，在广泛的雷诺数范围内进行了验证。我们为传统强化学习提供了不可微求解器，为通过梯度增强优化显著提高样本效率的可微求解器。全面的评估显示，强化学习代理在各种配置中一致地发现稳健的控制原理，如边界层操纵、声学反馈干扰和尾流重组。迁移学习研究表明，在一个雷诺数或几何形状下学习的控制器能够有效地适应新条件，需要大约50%更少的训练周期。HydroGym平台具有高度可扩展性和可扩展性，为流体动力学、机器学习和控制领域的研究人员提供了一个框架，用于添加环境、代理模型和控制算法，以推进科学和技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.</description>
      <author>example@mail.com (Christian Lagemann, Sajeda Mokbel, Miro Gondrum, Mario Rüttgers, Jared Callaham, Ludger Paehler, Samuel Ahnert, Nicholas Zolman, Kai Lagemann, Nikolaus Adams, Matthias Meinke, Wolfgang Schröder, Jean-Christophe Loiseau, Esther Lagemann, Steven L. Brunton)</author>
      <guid isPermaLink="false">2512.17534v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models</title>
      <link>http://arxiv.org/abs/2512.17385v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IPC的无监督框架，通过探测大型语言模型内部知识和置信度模式，实现了无需外部语料库的代码生成，性能可与监督方法媲美且减少了对标注数据和计算资源的依赖。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在代码生成任务中表现出色，但其有效性依赖于有监督训练，需要大量标注或未标注数据，而这些数据往往难以大规模获取且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需任何外部语料库（包括未标注代码片段）的无监督框架，用于代码生成任务，减少对标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;IPC框架包含问题空间探测、测试理解探测、解决方案空间探测、知识巩固和强化，用于探测LLMs内部知识和置信度模式；通过自一致性机制和基于表示的质量识别可靠代码候选，训练UCoder模型。&lt;h4&gt;主要发现&lt;/h4&gt;在多个代码基准测试上验证，无监督方法可与监督方法性能相当；内部模型状态包含关于代码质量和正确性的丰富信号，适当利用这些信号可实现有效的无监督学习。&lt;h4&gt;结论&lt;/h4&gt;为在资源受限场景下训练代码大型语言模型开辟了新方向，证明了无监督方法在代码生成任务中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在代码生成任务中表现出了卓越的能力。然而，它们的有效性很大程度上依赖于使用大量标注（如问答对）或未标注数据集（如代码片段）进行监督训练，而这些数据通常难以大规模获取且成本高昂。为解决这一局限性，本文提出了一种名为IPC的方法，这是一个无监督框架，利用大型语言模型内部探测进行代码生成，无需任何外部语料库，甚至是未标注的代码片段。我们引入了问题空间探测、测试理解探测、解决方案空间探测以及知识巩固和强化，来探测LLMs内部存在的知识和置信度模式。此外，IPC通过自一致性机制和基于表示的质量估计来识别可靠的代码候选，以训练UCoder（具有无监督学习的编码器）。我们在多个代码基准测试上验证了所提出的方法，证明无监督方法可以与监督方法实现竞争性性能，同时显著减少对标注数据和计算资源的依赖。分析实验表明，内部模型状态包含关于代码质量和正确性的丰富信号，适当利用这些信号能够实现代码生成任务的有效无监督学习，为在资源受限场景下训练代码大型语言模型开辟了新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.</description>
      <author>example@mail.com (Jiajun Wu, Jian Yang, Wei Zhang, Lin Jing, Yuqing Ma, Ensheng Shi, Yuchi Ma, Zhoujun Li, Xianglong Liu)</author>
      <guid isPermaLink="false">2512.17385v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>An Interpretable Latent Space reveals changing dynamics of European heatwaves</title>
      <link>http://arxiv.org/abs/2512.17097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 7 figures, 3 appendix figures, submitted for peer review to Machine Learning: Earth&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用深度学习技术基于大气环流对欧洲热浪进行分类和时间变化研究，发现不同类型热浪呈现不同变化趋势，强调了单独研究每种热浪类型的必要性。&lt;h4&gt;背景&lt;/h4&gt;由于气候变化，热浪变得越来越频繁和强烈，西欧经历了北半球中纬度地区最强的热浪趋势。部分温度趋势是由环流变化引起的，但这些变化在气候模型中没有准确捕捉。&lt;h4&gt;目的&lt;/h4&gt;使用深度学习技术基于大气环流对欧洲热浪进行分类，并研究这些热浪相关的时间变化。&lt;h4&gt;方法&lt;/h4&gt;使用变分自编码器（VAE）降低热浪样本的维度，在提取的特征上对热浪进行聚类。VAE在大型集合气候模型模拟上进行训练，并能很好地泛化到观测数据。研究引入了新的可解释性方法来研究潜在空间。&lt;h4&gt;主要发现&lt;/h4&gt;ERA5中与热浪相关的环流特征与气候模型热浪一致；大西洋羽流型热浪随时间变得越来越频繁，而大西洋高压型热浪变得越来越少；每种热浪类型都在经历其独特的环流变化，例如大西洋低压型热浪显示出随时间大西洋沿岸低压系统加深的趋势。&lt;h4&gt;结论&lt;/h4&gt;需要分别研究每种热浪类型，突显了单独研究的必要性；该方法可用于增强极端事件的特定方面；如果当前趋势持续，热浪环流可能会在未来发生变化，在某些情况下特征会加强。&lt;h4&gt;翻译&lt;/h4&gt;由于气候变化，热浪变得越来越频繁和强烈，西欧经历了北半球中纬度地区最强的热浪趋势。部分温度趋势是由环流变化引起的，这些变化在气候模型中没有准确捕捉。我们在此部署深度学习技术，基于大气环流对欧洲热浪进行分类，并研究它们相关的时间变化。我们使用变分自编码器（VAE）降低热浪样本的维度，然后在提取的特征上对它们进行聚类。VAE在大型集合气候模型模拟上进行训练，我们展示VAE无需迁移学习就能很好地泛化到ERA5再分析中的观测热浪环流。ERA5中与热浪相关的环流特征与气候模型热浪一致。回归分析显示，大西洋羽流型热浪随时间变得越来越频繁，而大西洋高压型热浪变得越来越少。我们引入了新的简单可解释性方法来研究潜在空间，包括特征重要性的识别和时间变化。我们研究与潜在空间中最重要的节点相关的环流特征，以及潜在空间如何随时间变化。例如，我们发现大西洋低压型热浪显示出随时间大西洋沿岸低压系统加深的趋势。每种热浪类型都在经历其独特的环流变化，突显了分别研究每种热浪类型的必要性。我们的方法还可用于增强极端事件的特定方面，我们说明了如果当前趋势持续，热浪环流在未来可能如何变化，在某些情况下特征会加强。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to climate change, heatwaves are becoming more frequent and intense, with western Europe experiencing the strongest trends in the Northern Hemisphere mid-latitudes. Part of the temperature trends are caused by circulation changes, which are not accurately captured in climate models. Here we deploy Deep Learning techniques to classify European heatwaves based on their atmospheric circulation and to study their associated changes over time. We use a Variational Autoencoder (VAE) to reduce the dimensionality of the heatwave samples, after which we cluster them on their extraced features. The VAE is trained on large ensemble climate model simulations and we show that the VAE generalizes well to observed heatwave circulations in ERA5 reanalysis, without the need for transfer learning. The circulation features relevant for heatwaves in ERA5 are consistent with the climate model heatwaves. Regression analysis reveals that the Atlantic Plume type of heatwaves are becoming more frequent over time, while the Atlantic High heatwaves are becoming less frequent. We introduce new and straightforward interpretability methods to study the latent space, including feature importance identification and changes over time. We investigate which circulation features are associated with the most important nodes in the latent space and how the latent space changes over time. For example, we find that the Atlantic Low heatwave shows a deepening of the low pressure system off the Atlantic coast over time. Each heatwave type is undergoing unique changes in their circulation, highlighting the necessity to study each heatwave type separately. Our method can furthermore be used to boost specific aspects of extreme events, and we illustrate how heatwave circulation could change in the future if the current trends persist, with in some cases an intensification of features.</description>
      <author>example@mail.com (Tamara Happé, Jasper Wijnands, Paolo Scussolini, Peter Pfleiderer, Dim Coumou)</author>
      <guid isPermaLink="false">2512.17097v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>A robust morphological classification method for galaxies using dual-encoding contrastive learning and multi-clustering voting on JWST/NIRCam images</title>
      <link>http://arxiv.org/abs/2512.17162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in A&amp;A&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一个名为USmorph的两步星系形态学分类框架，结合了无监督和有监督机器学习方法，对COSMOS-Web场中的46,176个星系进行了分类，并验证了该分类系统的可靠性。&lt;h4&gt;背景&lt;/h4&gt;星系形态分类是天文学研究中的重要内容，随着观测技术的进步，需要更有效的分类方法来处理大量星系数据。&lt;h4&gt;目的&lt;/h4&gt;开发并验证一个可靠的星系形态分类系统，用于即将进行的中国空间站望远镜大天区巡天研究。&lt;h4&gt;方法&lt;/h4&gt;采用双步框架：增强无监督机器学习步骤，使用双编码器架构(ConvNeXt和ViT)编码图像，应用对比学习提取特征，使用主成分分析降维；然后使用改进的框架对星系进行形态分类，并通过参数化和非参数化测量验证分类结果。&lt;h4&gt;主要发现&lt;/h4&gt;成功将46,176个星系分为五类：33%球形(SPH)、25%早期盘状星系(ETD)、25%晚期盘状星系(LTD)、7%不规则星系(IRR)和10%未分类星系(UNC)；形态参数分析显示，SPH和ETD星系具有较高的Sérsic指数、Gini系数和集中度，倾向于更以凸起为主导且更紧凑。&lt;h4&gt;结论&lt;/h4&gt;USmorph分类系统可靠有效，能够准确区分不同形态的星系，适用于未来的大天区星系巡天研究。&lt;h4&gt;翻译&lt;/h4&gt;USmorph两步星系形态分类框架成功结合了无监督机器学习与有监督机器学习方法。为增强无监督学习步骤，我们采用双编码器架构(ConvNeXt和ViT)有效编码图像，使用对比学习准确提取特征，并应用主成分分析高效降维。基于此改进框架，使用JWST近红外图像对COSMOS-Web场中选取的46,176个红移范围0&lt;z&lt;4.2的星系分为五类：33%球形(SPH)、25%早期盘状星系(ETD)、25%晚期盘状星系(LTD)、7%不规则星系(IRR)和10%未分类星系(UNC)。我们还对大质量星系(M*&gt;10^9 M⊙)进行了参数化(Sérsic指数n和有效半径re)和非参数化测量(Gini系数G、光二阶矩M20、集中度C、多重性Ψ以及MID统计中的其他三个参数)，以验证星系形态分类系统的有效性。形态参数分析表明，与其它类型星系相比，SPH和ETD星系具有较高的n、G和C值，倾向于更以凸起为主导且更紧凑。这证明了该分类系统的可靠性，该系统将对中国空间站望远镜即将进行的大天区巡天研究有用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The two-step galaxy morphology classification framework {\tt USmorph} successfully combines unsupervised machine learning (UML) with supervised machine learning (SML) methods. To enhance the UML step, we employed a dual-encoder architecture (ConvNeXt and ViT) to effectively encode images, contrastive learning to accurately extract features, and principal component analysis to efficiently reduce dimensionality. Based on this improved framework, a sample of 46,176 galaxies at $0&lt;z&lt;4.2$, selected in the COSMOS-Web field, is classified into five types using the JWST near-infrared images: 33\% spherical (SPH), 25\% early-type disk (ETD), 25\% late-type disk (LTD), 7\% irregular (IRR), and 10\% unclassified (UNC) galaxies. We also performed parametric (S{é}rsic index, $n$,and effective radius, $r_{\rm e}$) and nonparametric measurements (Gini coefficient, $G$, the second-order moment of light, $M_{\rm 20}$, concentration, $C$, multiplicity, $Ψ$, and three other parameters from the MID statistics) for massive galaxies ($M_*&gt;10^9 M_\odot$) to verify the validity of our galaxy morphological classification system. The analysis of morphological parameters is consistent with our classification system: SPH and ETD galaxies with higher $n$, $G$, and $C$ tend to be more bulge-dominated and more compact compared with other types of galaxies. This demonstrates the reliability of this classification system, which will be useful for a forthcoming large-sky survey from the Chinese Space Station Telescope.</description>
      <author>example@mail.com (Xiaolei Yin, Guanwen Fang, Shiying Lu, Zesen Lin, Yao Dai, Chichun Zhou)</author>
      <guid isPermaLink="false">2512.17162v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation</title>
      <link>http://arxiv.org/abs/2512.17762v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ECHO基准测试，专门用于评估图神经网络处理长距离图传播的能力，包含三个合成图任务和两个真实世界数据集，通过测试揭示了现有GNN架构在长距离传播方面的性能差距。&lt;h4&gt;背景&lt;/h4&gt;在图神经网络研究中，有效捕获长距离交互仍然是一个基本但尚未解决的关键挑战，这对科学不同领域的应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;为了系统地解决长距离交互捕获问题，引入ECHO基准测试，旨在严格评估GNN在处理非常长范围图传播方面的能力。&lt;h4&gt;方法&lt;/h4&gt;ECHO包括三个合成图任务（单源最短路径、节点离心率和图直径），构建在多样化和结构上具有挑战性的拓扑上，引入显著信息瓶颈；还包括两个真实世界数据集：ECHO-Charge（预测原子部分电荷）和ECHO-Energy（预测分子总能量），参考计算基于密度泛函理论水平。&lt;h4&gt;主要发现&lt;/h4&gt;通过对流行GNN架构的广泛基准测试，发现了明显的性能差距，强调了真实长范围传播的难度，并突出了能够克服固有局限性的设计选择。&lt;h4&gt;结论&lt;/h4&gt;ECHO为评估长距离信息传播设定了新的标准，同时也为科学中人工智能的需求提供了引人注目的例证。&lt;h4&gt;翻译&lt;/h4&gt;在图神经网络研究中，有效捕获长距离交互仍然是一个基本但尚未解决的关键挑战，对科学不同领域的应用至关重要。为了系统地解决这个问题，我们引入了ECHO（评估长跳通信能力），这是一个新的基准测试，专门设计用于严格评估GNN处理非常长范围图传播的能力。ECHO包括三个合成图任务，即单源最短路径、节点离心率和图直径，每个任务都构建在多样化和结构上具有挑战性的拓扑上，专门设计以引入显著的信息瓶颈。ECHO还包括两个真实世界数据集：ECHO-Charge和ECHO-Energy，它们分别为预测原子部分电荷和分子总能量定义了基于化学的基准，参考计算在密度泛函理论水平上获得。这两个任务本质上都依赖于捕获复杂的长距离分子相互作用。我们对流行GNN架构的广泛基准测试揭示了明显的性能差距，强调了真实长范围传播的难度，并突出了能够克服固有局限性的设计选择。ECHO因此为评估长距离信息传播设定了新的标准，同时也为科学中人工智能的需求提供了一个引人注目的例证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.</description>
      <author>example@mail.com (Luca Miglior, Matteo Tolloso, Alessio Gravina, Davide Bacciu)</author>
      <guid isPermaLink="false">2512.17762v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting</title>
      <link>http://arxiv.org/abs/2512.17696v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种空间感知Transformer混合架构，将地统计学归纳偏差直接注入自注意力机制，实现了高性能的时空过程建模，同时保持了物理感知和数据驱动学习的优势。&lt;h4&gt;背景&lt;/h4&gt;高维时空过程建模面临经典地统计学与深度学习之间的根本分歧。高斯过程提供理论一致性和精确不确定性量化，但计算复杂度高，对大规模传感器网络不实用；现代Transformer架构擅长序列建模但缺乏几何归纳偏差，将空间传感器视为排列不变标记，没有距离的本理解。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够结合地统计学理论严谨性与深度学习灵活性的混合架构，用于高维时空过程建模。&lt;h4&gt;方法&lt;/h4&gt;提出空间感知Transformer，通过可学习协方差核将地统计学归纳偏差注入自注意力机制；将注意力结构形式化分解为平稳物理先验和非平稳数据驱动残差，施加软拓扑约束，有利于空间近程交互同时保留建模复杂动力学的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实现了'深度变异学'现象，网络通过反向传播成功恢复底层过程的真实空间衰减参数；在合成高斯随机场和真实世界交通基准上的实验证实，该方法优于最先进的图神经网络；提供校准良好的概率预测，兼具卓越的预测准确性和可靠性。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地弥合了物理感知建模与数据驱动学习之间的差距，为高维时空过程建模提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高维时空过程的建模呈现出经典地统计学的概率严谨性与深度学习的灵活、高容量表示之间的根本分歧。虽然高斯过程提供理论一致性和精确的不确定性量化，但其 prohibitive 计算扩展使其对大规模传感器网络不切实际。相反，现代Transformer架构擅长序列建模，但固有地缺乏几何归纳偏差，将空间传感器视为排列不变的标记，没有对距离的本理解。在这项工作中，我们提出了一种空间感知Transformer，一种混合架构，通过可学习的协方差核将地统计学归纳偏差直接注入自注意力机制。通过将注意力结构形式化分解为平稳物理先验和非平稳数据驱动残差，我们施加了一个软拓扑约束，有利于空间近程交互，同时保留建模复杂动力学的能力。我们证明了'深度变异学'现象，其中网络通过反向传播成功恢复底层过程的真实空间衰减参数。在合成高斯随机场和真实世界交通基准上的广泛实验证实，我们的方法优于最先进的图神经网络。此外，严格的统计验证证实，所提出的方法不仅提供卓越的预测准确性，还提供校准良好的概率预测，有效地弥合了物理感知建模与数据驱动学习之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.</description>
      <author>example@mail.com (Yuri Calleo)</author>
      <guid isPermaLink="false">2512.17696v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>From Priors to Predictions: Explaining and Visualizing Human Reasoning in a Graph Neural Network Framework</title>
      <link>http://arxiv.org/abs/2512.17255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  44 pages, 7 figures, 3 suppl figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究引入了一个结合图论和图神经网络的框架，将归纳偏置形式化为结构和抽象上的显式先验，用于理解人类推理和开发更符合人类的AI系统。&lt;h4&gt;背景&lt;/h4&gt;人类能够通过归纳偏置解决新的推理问题，但这些偏置的计算形式和神经实现仍不清楚。&lt;h4&gt;目的&lt;/h4&gt;形式化归纳偏置为显式、可操作的先验，解释人类推理中的个体差异，并开发更符合人类的AI系统。&lt;h4&gt;方法&lt;/h4&gt;结合图论和图神经网络创建框架，使用改编自ARC的人类行为数据集，开发优化管道搜索图配置，以及可视化方法识别关键计算图。&lt;h4&gt;主要发现&lt;/h4&gt;基于图的先验可以解释人类解决方案中的个体差异；泛化依赖于特定的先验结构和内部处理；不正确或不完整的先验会导致类似人类的错误。&lt;h4&gt;结论&lt;/h4&gt;该研究为建模泛化背后的表征假设和计算动力学提供了有原则的、可解释的框架，为人类推理提供了新见解，并为更符合人类的AI系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;人类擅长通过归纳偏置解决新的推理问题，这些偏置是对哪些实体和关系重要的假设。然而，这些偏置的计算形式和神经实现仍不清楚。我们引入了一个结合图论和图神经网络的框架，将归纳偏置形式化为结构和抽象上的显式、可操作的先验。使用改编自抽象与推理语料库的人类行为数据集，我们表明基于图的先验差异可以解释人类解决方案中的个体差异。我们的方法包括一个优化管道，搜索图配置，变化边缘连通性和节点抽象，以及一种识别计算图的可视化方法，即对模型预测最重要的节点和边的子集。系统性消融研究揭示了泛化如何依赖于特定的先验结构和内部处理，暴露了为什么类似人类的错误会从不正确或不完整的先验中出现。这项工作为建模泛化背后的表征假设和计算动力学提供了一个有原则的、可解释的框架，为人类推理提供了新的见解，并为更符合人类的AI系统奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans excel at solving novel reasoning problems from minimal exposure, guided by inductive biases, assumptions about which entities and relationships matter. Yet the computational form of these biases and their neural implementation remain poorly understood. We introduce a framework that combines Graph Theory and Graph Neural Networks (GNNs) to formalize inductive biases as explicit, manipulable priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus (ARC), we show that differences in graph-based priors can explain individual differences in human solutions. Our method includes an optimization pipeline that searches over graph configurations, varying edge connectivity and node abstraction, and a visualization approach that identifies the computational graph, the subset of nodes and edges most critical to a model's prediction. Systematic ablation reveals how generalization depends on specific prior structures and internal processing, exposing why human like errors emerge from incorrect or incomplete priors. This work provides a principled, interpretable framework for modeling the representational assumptions and computational dynamics underlying generalization, offering new insights into human reasoning and a foundation for more human aligned AI systems.</description>
      <author>example@mail.com (Quan Do, Caroline Ahn, Leah Bakst, Michael Pascale, Joseph T. McGuire, Chantal E. Stern, Michael E. Hasselmo)</author>
      <guid isPermaLink="false">2512.17255v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning</title>
      <link>http://arxiv.org/abs/2512.17185v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了系统风险雷达(SRR)框架，通过将金融市场建模为多层图来检测系统脆弱性和崩溃状态的早期预警信号，并在三大危机中验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;金融危机在各部门、市场和投资者行为中累积结构性脆弱性时出现，预测这些系统转变具有挑战性，因为它们源于市场参与者之间不断变化的互动，而非孤立的价格变动。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够检测金融系统脆弱性和崩溃状态转变早期迹象的框架，以提前预警金融危机。&lt;h4&gt;方法&lt;/h4&gt;提出系统风险雷达(SRR)框架，将金融市场建模为多层图，并在互联网泡沫崩溃、全球金融危机和COVID-19冲击三大危机中进行评估，比较了快照图神经网络、时序GNN原型以及逻辑回归和随机森林等基线模型。&lt;h4&gt;主要发现&lt;/h4&gt;结构网络信息比仅基于特征的模型提供了更有用的早期预警信号，图派生特征能够有效捕获压力事件期间市场结构的有意义变化。&lt;h4&gt;结论&lt;/h4&gt;研究结果支持扩展SRR框架，增加额外的图层次（如行业/因子敞口、情绪）和更复杂的时序架构（如LSTM/GRU或Transformer编码器），以更好地处理多样化的危机类型。&lt;h4&gt;翻译&lt;/h4&gt;金融危机在各部门、市场和投资者行为中累积结构性脆弱性时出现。预测这些系统转变具有挑战性，因为它们源于市场参与者之间不断变化的互动，而不仅仅是孤立的价格变动。我们提出了'系统风险雷达'(SRR)框架，该框架将金融市场建模为多层图，以检测系统脆弱性和崩溃状态转变的早期迹象。我们在三大危机中评估了SRR：互联网泡沫崩溃、全球金融危机和COVID-19冲击。我们的实验比较了快照图神经网络、简化的时序GNN原型以及标准基线（逻辑回归和随机森林）。结果表明，与仅基于特征的模型相比，结构网络信息提供了有用的早期预警信号。SRR的这种基于相关性的实例表明，图派生特征能够捕获压力事件期间市场结构的有意义变化。这些发现促使我们扩展SRR，增加额外的图层次（行业/因子敞口、情绪）和更具表现力的时序架构（LSTM/GRU或Transformer编码器），以更好地处理不同类型的危机。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.</description>
      <author>example@mail.com (Sandeep Neela)</author>
      <guid isPermaLink="false">2512.17185v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</title>
      <link>http://arxiv.org/abs/2512.17504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, project page: https://myyzzzoooo.github.io/InsertAnywhere/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了InsertAnywhere框架，实现了几何一致的对象放置和外观忠实于原始视频的合成，解决了视频对象插入中的4D场景理解、遮挡处理和光照效果等关键挑战。&lt;h4&gt;背景&lt;/h4&gt;基于扩散的视频生成技术为可控视频编辑开辟了新可能性，但现实中的视频对象插入(VOI)仍面临挑战，主要由于有限的4D场景理解以及对遮挡和光照效果的不当处理。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现几何一致对象放置和外观忠实视频合成的视频对象插入框架，解决现有方法在4D场景理解、遮挡处理和光照效果方面的不足。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了4D感知的掩码生成模块，重建场景几何并保持时间一致性和遮挡一致性；2. 扩展了基于扩散的视频生成模型，共同合成插入对象及其周围局部变化；3. 构建了ROSE++光照感知合成数据集，支持监督训练。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够在各种真实世界场景中产生几何合理且视觉连贯的对象插入效果，显著优于现有研究和商业模型。&lt;h4&gt;结论&lt;/h4&gt;InsertAnywhere框架通过结合几何一致的对象放置和外观忠实于原始视频的合成，为可控视频编辑提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于扩散的视频生成最新进展为可控视频编辑开辟了新的可能性，但由于有限的4D场景理解以及对遮挡和光照效果的不当处理，现实中的视频对象插入(VOI)仍然具有挑战性。我们提出了InsertAnywhere，一种新的VOI框架，实现了几何一致的对象放置和外观忠实于原始视频的合成。我们的方法从4D感知的掩码生成模块开始，该模块重建场景几何并在保持时间一致性和遮挡一致性的同时传播用户指定的对象放置。在此基础上，我们扩展了基于扩散的视频生成模型，以共同合成插入对象及其周围局部变化，如光照和阴影。为了支持监督训练，我们引入了ROSE++，这是一个光照感知的合成数据集，通过将ROSE对象移除数据集转换为对象移除视频、对象存在视频和VLM生成的参考图像三元组来构建。通过大量实验，我们证明我们的框架能够在各种真实世界场景中产生几何合理且视觉连贯的对象插入效果，显著优于现有研究和商业模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视频对象插入(VOI)的问题，具体是现有方法在4D场景理解和处理遮挡、光照效果方面的不足。这个问题在商业广告、电影后期制作和虚拟产品植入等领域非常重要，因为这些应用需要高质量的对象插入技术，而现有方法难以在复杂场景中实现几何一致且视觉效果真实的对象插入。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将VOI问题分解为两个主要挑战：4D感知对象放置和高保真视频生成。他们设计了一个两阶段框架：第一阶段使用4D场景重建和用户交互式放置生成几何一致的掩码序列；第二阶段基于扩散模型生成高质量视频。作者借鉴了Uni4D的4D场景建模、SEA-RAFT的光流计算、SAM2的分割技术，并基于ROSE数据集构建了新的ROSE++数据集，同时使用LoRA技术微调扩散模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合4D场景几何理解与扩散模型，实现几何一致且光照真实的视频对象插入。整体流程分为两个阶段：第一阶段是4D感知掩码生成，包括4D场景重建、用户控制对象放置和场景流驱动的对象传播；第二阶段是视频对象插入，先使用图像模型生成高质量的第一帧，然后扩散模型生成整个视频，同时处理光照和阴影等局部变化。此外，还构建了ROSE++数据集来支持训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 4D感知掩码生成模块，能处理复杂遮挡情况；2) 扩展视频修复模型，可合成对象周围的局部变化；3) ROSE++数据集，通过VLM生成参考图像；4) 场景流驱动的对象传播，确保物理一致性。相比之前工作，传统方法仅编辑给定掩码内区域，无法处理局部变化；其他方法依赖整个空间区域掩码，无法处理真实遮挡；GenProp等方法未考虑可见性随时间的变化，导致在遮挡场景下不一致。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InsertAnywhere通过结合4D场景几何理解和扩散模型，实现了在复杂场景中几何一致且光照真实的高质量视频对象插入，显著优于现有商业生成工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.</description>
      <author>example@mail.com (Hoiyeong Jin, Hyojin Jang, Jeongho Kim, Junha Hyung, Kinam Kim, Dongjin Kim, Huijin Choi, Hyeonji Kim, Jaegul Choo)</author>
      <guid isPermaLink="false">2512.17504v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</title>
      <link>http://arxiv.org/abs/2512.17495v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GroundingME基准测试，用于评估多模态大语言模型(MLLMs)在视觉定位任务上的真实能力，发现当前模型与人类表现存在显著差距，并探索了两种改进策略。&lt;h4&gt;背景&lt;/h4&gt;视觉定位是从自然语言描述中定位对象，是语言和视觉理解之间的关键桥梁。尽管MLLMs在现有基准测试上表现优异，但它们是否真正能以类人的复杂性将语言视觉化，还是仅在简化数据集上进行模式匹配仍存疑问。&lt;h4&gt;目的&lt;/h4&gt;引入GroundingME基准测试，严格评估MLLMs的真实能力，系统性地挑战模型在四个关键维度上的表现，并探索改进策略。&lt;h4&gt;方法&lt;/h4&gt;创建四个关键维度的基准测试：区分性(区分相似对象)、空间性(理解复杂关系)、有限性(处理遮挡或微小对象)和拒绝性(识别无法定位的查询)。通过自动生成和人工验证相结合，创建1005个具有挑战性的例子。评估25个最先进的MLLMs，并测试两种改进策略：测试时扩展和数据混合训练。&lt;h4&gt;主要发现&lt;/h4&gt;最佳模型仅达到45.1%的准确率，大多数模型在拒绝任务上得分为0%，反射性地产生幻觉对象而非承认缺失，引发部署安全担忧。测试时扩展将复杂定位能力提高最多2.9%，数据混合训练将拒绝准确率从0%提高到27.9%。&lt;h4&gt;结论&lt;/h4&gt;GroundingME既揭示了MLLMs的当前局限性，也是实现类人视觉定位的路线图。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位，即从自然语言描述中定位对象，代表了语言和视觉理解之间的关键桥梁。尽管多模态大语言模型在现有基准测试上取得了令人印象深刻的分数，但一个基本问题仍然存在：MLLMs是否真正能以类人的复杂性将语言视觉化，还是它们只是在简化的数据集上进行模式匹配？当前的基准测试无法捕捉真实世界的复杂性，而人类却能轻松处理模糊的引用并识别何时无法进行定位。为了严格评估MLLMs的真实能力，我们引入了GroundingME基准测试，该测试系统性地挑战模型在四个关键维度上的表现：(1)区分性，区分高度相似的对象；(2)空间性，理解复杂的关系描述；(3)有限性，处理遮挡或微小对象；(4)拒绝性，识别无法定位的查询。通过结合自动生成和人工验证的精心筛选，我们创建了1005个具有挑战性的例子，模拟真实世界的复杂性。评估25个最先进的MLLMs揭示了严重的能力差距：最佳模型仅达到45.1%的准确率，而大多数模型在拒绝任务上得分为0%，反射性地产生幻觉对象而不是承认它们的缺失，这引发了部署的关键安全问题。我们探索了两种改进策略：(1)测试时扩展通过思考轨迹选择最佳响应，将复杂定位能力提高最多2.9%；(2)数据混合训练教模型识别无法定位的查询，将拒绝准确率从0%提高到27.9%。因此，GroundingME既是一个诊断工具，揭示了MLLMs的当前局限性，也是实现类人视觉定位的路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.</description>
      <author>example@mail.com (Rang Li, Lei Li, Shuhuai Ren, Hao Tian, Shuhao Gu, Shicheng Li, Zihao Yue, Yudong Wang, Wenhan Ma, Zhe Yang, Jingyuan Ma, Zhifang Sui, Fuli Luo)</author>
      <guid isPermaLink="false">2512.17495v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos</title>
      <link>http://arxiv.org/abs/2512.17229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为VideoDetective的高效问题感知记忆机制，使多模态大语言模型能够从长视频中反复寻求关键线索，有效处理长视频问答任务，同时减少内存消耗。&lt;h4&gt;背景&lt;/h4&gt;长视频问答对多模态大语言模型构成重大挑战，因为长视频包含巨大的上下文和过载的信息，导致高昂的内存消耗。现有方法通过减少视觉标记或扩展模型上下文长度来解决这些问题，但可能会遗漏有用信息或需要大量计算。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够有效处理长视频问答任务的方法，使多模态大语言模型能够在有限内存条件下从大量信息中准确识别并利用关键线索。&lt;h4&gt;方法&lt;/h4&gt;提出VideoDetective方法，通过迭代处理视频子片段实现任务简化。对每个子片段采用问题感知压缩策略，引入少量特殊记忆标记实现有目的的压缩，使模型在减少视觉标记的同时有效寻求关键线索。同时反复聚合和存储这些记忆标记以更新历史上下文，供后续子片段重用。此外，研究团队引入了GLVC数据集，用于更有效地衡量模型的长视频理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法使具有32K有限上下文长度的多模态大语言模型能够高效处理100K标记（3600帧，1fps采样的一小时视频），仅需2分钟和37GB GPU内存使用。多个长视频基准的评估结果表明，该方法能够从大量信息中更有效地寻求关键线索。&lt;h4&gt;结论&lt;/h4&gt;VideoDetective通过问题感知记忆机制有效解决了长视频问答中的内存消耗和信息过载问题，使模型能够在有限资源条件下高效处理长视频内容，准确提取关键信息。&lt;h4&gt;翻译&lt;/h4&gt;长视频问答由于巨大的上下文和过载的信息，为多模态大语言模型带来了重大挑战，这也可能导致高昂的内存消耗。虽然现有方法试图通过减少视觉标记或扩展模型上下文长度来解决这些问题，但它们可能会遗漏有用信息或需要大量计算。事实上，在回答给定问题时，只需要少量关键信息。因此，我们提出了一种高效的问题感知记忆机制，使多模态大语言模型能够反复寻求这些关键线索。我们的方法名为VideoDetective，通过迭代处理视频子片段来简化此任务。对于每个子片段，通过引入少量特殊记忆标记来实现问题感知压缩策略，从而实现有目的的压缩。这使得模型能够在减少视觉标记的同时有效寻求关键线索。然后，由于历史上下文可能有显著影响，我们反复聚合和存储这些记忆标记以更新历史上下文，该上下文将被后续子片段重用。此外，为了更有效地衡量模型的长视频理解能力，我们引入了GLVC，这是一个长视频问答数据集，其特点是定位散布在整个视频中的关键和具体线索。实验结果表明，我们的方法使具有32K有限上下文长度的多模态大语言模型能够高效处理100K标记（3600帧，1fps采样的一小时视频），仅需2分钟和37GB GPU内存使用。多个长视频基准的评估结果表明，我们的方法能够从大量信息中更有效地寻求关键线索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long Video Question-Answering (LVQA) presents a significant challenge for Multi-modal Large Language Models (MLLMs) due to immense context and overloaded information, which could also lead to prohibitive memory consumption. While existing methods attempt to address these issues by reducing visual tokens or extending model's context length, they may miss useful information or take considerable computation. In fact, when answering given questions, only a small amount of crucial information is required. Therefore, we propose an efficient question-aware memory mechanism, enabling MLLMs to recurrently seek these critical clues. Our approach, named VideoDetective, simplifies this task by iteratively processing video sub-segments. For each sub-segment, a question-aware compression strategy is employed by introducing a few special memory tokens to achieve purposefully compression. This allows models to effectively seek critical clues while reducing visual tokens. Then, due to history context could have a significant impact, we recurrently aggregate and store these memory tokens to update history context, which would be reused for subsequent sub-segments. Furthermore, to more effectively measure model's long video understanding ability, we introduce GLVC (Grounding Long Video Clues), a long video question-answering dataset, which features grounding critical and concrete clues scattered throughout entire videos. Experimental results demonstrate our method enables MLLMs with limited context length of 32K to efficiently process 100K tokens (3600 frames, an hour-long video sampled at 1fps), requiring only 2 minutes and 37GB GPU memory usage. Evaluation results across multiple long video benchmarks illustrate our method can more effectively seek critical clues from massive information.</description>
      <author>example@mail.com (Henghui Du, Chang Zhou, Chunjie Zhang, Xi Chen, Di Hu)</author>
      <guid isPermaLink="false">2512.17229v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</title>
      <link>http://arxiv.org/abs/2512.16978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了LongSHOTBench，一个新的诊断基准测试，用于评估长形式多模态视频理解能力，同时提出了LongSHOTAgent智能体系统。研究显示当前最先进的MLLM在该基准测试上表现不佳，突显了长形式视频理解的挑战。&lt;h4&gt;背景&lt;/h4&gt;长形式多模态视频理解需要整合视觉、语音和环境音频，并进行连贯的长程推理。现有基准测试要么强调时间长度，要么强调多模态丰富性，但很少同时兼顾两者，且主要依赖单一分数准确率，掩盖了失败模式。&lt;h4&gt;目的&lt;/h4&gt;引入LongSHOTBench，一个包含开放式、意图驱动问题；单轮和多轮对话；以及需要跨视频、音频和语音进行多模态推理和智能体工具使用任务的诊断基准测试。&lt;h4&gt;方法&lt;/h4&gt;LongSHOTBench通过可扩展、人工验证的流程生成，确保覆盖率和可重复性，所有样本都经过人工验证和纠正。同时提出LongSHOTAgent智能体系统，通过预处理、搜索和迭代分析来分析长视频。&lt;h4&gt;主要发现&lt;/h4&gt;在LongSHOTBench上，最先进的MLLM表现差距明显：Gemini-2.5-Flash达到52.95%，开源模型低于30%，LongSHOTAgent达到44.66%。这些结果强调了现实世界长形式视频理解的难度。&lt;h4&gt;结论&lt;/h4&gt;LongSHOTBench为评估和改进MLLMs提供了实用、可重复的基础，所有资源都在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;长形式多模态视频理解需要整合视觉、语音和环境音频，并进行连贯的长程推理。现有基准测试要么强调时间长度，要么强调多模态丰富性，但很少同时兼顾两者。虽然一些基准测试包含开放式问题和高级指标，但它们主要依赖单一分数准确率，掩盖了失败模式。我们引入了LongSHOTBench，这是一个包含开放式、意图驱动问题的诊断基准；单轮和多轮对话；以及需要跨视频、音频和语音进行多模态推理和智能体工具使用的任务。每个项目都包含参考答案和分级评分标准，以便进行可解释和可追溯的评估。LongSHOTBench通过可扩展、人工验证的流程生成，以确保覆盖率和可重复性。我们LongSHOTBench中的所有样本都经过人工验证和纠正。此外，我们提出了LongSHOTAgent，一个通过预处理、搜索和迭代分析来分析长视频的智能体系统。在LongSHOTBench上，最先进的MLLM显示出很大差距：Gemini-2.5-Flash达到52.95%，开源模型低于30%，LongSHOTAgent达到44.66%。这些结果强调了现实世界长形式视频理解的难度。LongSHOTBench为评估和改进MLLMs提供了实用、可重复的基础。所有资源都在GitHub上提供：https://github.com/mbzuai-oryx/longshot。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.</description>
      <author>example@mail.com (Mohammed Irfan Kurpath, Jaseel Muhammad Kaithakkodan, Jinxing Zhou, Sahal Shaji Mullappilly, Mohammad Almansoori, Noor Ahsan, Beknur Kalmakhanbet, Sambal Shikhar, Rishabh Lalla, Jean Lahoud, Mariette Awad, Fahad Shahbaz Khan, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal)</author>
      <guid isPermaLink="false">2512.16978v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence</title>
      <link>http://arxiv.org/abs/2512.17781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LiteGE的轻量级方法，用于计算3D表面上的测地距离，解决了现有方法内存占用高、延迟大的问题。&lt;h4&gt;背景&lt;/h4&gt;计算3D表面上的测地距离是3D视觉和几何处理的基础任务，与形状对应等密切相关。现有基于学习的方法表现良好但依赖大型3D骨干网络，导致高内存使用和高延迟，限制了在交互式或资源受限环境中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级方法，减少内存使用和推理时间，使其能够适用于交互式或资源受限的应用场景。&lt;h4&gt;方法&lt;/h4&gt;LiteGE通过对有意义的体素处的无符号距离场(UDF)样本应用主成分分析(PCA)，构建紧凑的、类别感知的形状描述符。这种方法计算效率高，不需要高容量网络，且在稀疏点云上保持鲁棒性，支持仅300个点的输入。&lt;h4&gt;主要发现&lt;/h4&gt;与现有神经方法相比，LiteGE将内存使用和推理时间减少了高达300倍；通过利用测地距离和形状对应的内在关系，实现了快速准确的形状匹配；与最先进的基于网格的方法相比，实现了高达1000倍的加速，同时在非等距形状对上保持相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;LiteGE是一种高效的方法，能够在保持准确性的同时大幅减少计算资源需求，特别适合资源受限的应用场景，包括点云输入。&lt;h4&gt;翻译&lt;/h4&gt;计算3D表面上的测地距离是3D视觉和几何处理中许多任务的基础，与形状对应等任务有密切关系。最近的基于学习的方法取得了强大性能，但依赖于大型3D骨干网络，导致高内存使用和延迟，限制了它们在交互式或资源受限环境中的使用。我们介绍了LiteGE，一种轻量级方法，通过对有意义的体素处的无符号距离场(UDF)样本应用PCA，构建紧凑的、类别感知的形状描述符。这种描述符计算效率高，不需要高容量网络。LiteGE在稀疏点云上保持鲁棒性，支持仅300个点的输入，而先前方法在此情况下失败。大量实验表明，与现有神经方法相比，LiteGE将内存使用和推理时间减少了高达300倍。此外，通过利用测地距离和形状对应之间的内在关系，LiteGE实现了快速准确的形状匹配。与最先进的基于网格的方法相比，我们的方法实现了高达1000倍的加速，同时在非等距形状对上保持相当的准确性，包括在点云输入上的评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying PCA to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.</description>
      <author>example@mail.com (Yohanes Yudhi Adikusuma, Qixing Huang, Ying He)</author>
      <guid isPermaLink="false">2512.17781v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation</title>
      <link>http://arxiv.org/abs/2512.17764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first two authors contributed equally. Project page: https://unistatedlo.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UniStateDLO，这是第一个完整的可变形线性物体(DLOs)感知管道，使用深度学习方法在严重遮挡情况下实现鲁棒性能。该方法将单帧状态估计和跨帧状态跟踪都表述为条件生成问题，利用扩散模型处理部分观测和高维DLO状态之间的复杂映射。UniStateDLO能够处理各种遮挡模式，仅使用合成数据训练即可实现零样本仿真到现实的泛化，并在实验中优于所有最先进方法。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的可变形线性物体感知方法虽然已被广泛探索，但在受约束操作环境中容易受到遮挡影响，这些环境通常由周围障碍物、大型和变化的变形以及有限视角引起。此外，状态空间的高维性、缺乏明显的视觉特征以及传感器噪声的存在进一步增加了可靠DLO感知的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有DLO感知方法在遮挡环境下的脆弱性问题，包括周围障碍物导致的遮挡、大型和变化的变形、有限的视角、高维状态空间、缺乏明显视觉特征以及传感器噪声等问题。&lt;h4&gt;方法&lt;/h4&gt;提出UniStateDLO，将单帧状态估计和跨帧状态跟踪都表述为条件生成问题，利用扩散模型捕获部分观测和高维DLO状态之间的复杂映射能力。仅使用大规模合成数据训练整个网络，实现零样本仿真到现实的泛化，无需任何真实世界训练数据。&lt;h4&gt;主要发现&lt;/h4&gt;1) UniStateDLO能有效处理各种遮挡模式，包括初始遮挡、自遮挡和由多个物体引起的遮挡；2) 该方法表现出强大的数据效率，仅通过合成数据训练即可实现零样本仿真到现实的泛化；3) 在模拟和真实世界实验中，UniStateDLO在估计和跟踪方面都优于所有最先进的基线方法；4) 即使在大量遮挡的情况下，也能实时生成全局平滑且局部精确的DLO状态预测；5) 作为闭环DLO操作系统的前端模块集成时，能在复杂受限的3D环境中支持稳定的反馈控制。&lt;h4&gt;结论&lt;/h4&gt;UniStateDLO通过创新的深度学习方法解决了DLO感知在遮挡环境中的关键挑战，实现了高性能的DLO状态估计和跟踪。该方法不仅在各种遮挡条件下表现出色，还通过仅使用合成数据进行训练实现了零样本仿真到现实的泛化，大大降低了数据收集的成本和复杂性。其作为前端模块在闭环DLO操作系统中的成功集成进一步验证了其在实际应用中的价值。&lt;h4&gt;翻译&lt;/h4&gt;可变形线性物体(DLOs)（如电缆、绳索和电线）的感知是成功下游操作的基础。尽管基于视觉的方法已被广泛探索，但它们在受约束操作环境中由于周围障碍物、大型和变化的变形以及有限视角而常见遮挡的情况下仍然非常脆弱。此外，状态空间的高维性、缺乏明显的视觉特征以及传感器噪声的存在进一步增加了可靠DLO感知的挑战。为解决这些开放性问题，本文提出了UniStateDLO，这是第一个使用深度学习方法的完整DLO感知管道，能够在严重遮挡下实现鲁棒性能，涵盖了从部分点云的单帧状态估计和跨帧状态跟踪。这两个任务都被表述为条件生成问题，利用扩散模型捕获部分观测和高维DLO状态之间复杂映射的强大能力。UniStateDLO能够有效处理各种遮挡模式，包括初始遮挡、自遮挡和由多个物体引起的遮挡。此外，由于整个网络仅在大规模合成数据集上训练，它表现出强大的数据效率，能够在没有任何真实世界训练数据的情况下实现零样本仿真到现实的泛化。全面的模拟和真实世界实验表明，UniStateDLO在估计和跟踪方面都优于所有最先进的基线方法，即使在大量遮挡的情况下也能实时产生全局平滑且局部精确的DLO状态预测。作为闭环DLO操作系统中的前端模块集成，它进一步展示了在复杂受限3D环境中支持稳定反馈控制的能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决可变形线性物体（如电缆、绳索、电线）在遮挡情况下的三维状态估计和跟踪问题。这个问题在现实中非常重要，因为准确感知DLO的形状和位置是机器人成功操作它们的基础，在制造、服务和医疗等领域有广泛应用。然而，在受限操作环境中，常见的遮挡问题（由障碍物、物体自遮挡或复杂变形引起）使得现有视觉方法难以可靠地估计DLO状态，这限制了机器人在复杂环境中的操作能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有DLO感知方法的局限性：单帧估计方法忽略时间连续性，而跟踪方法严重依赖准确初始化。受扩散模型在处理复杂概率分布方面能力的启发，作者将DLO状态估计和跟踪都表述为条件生成任务。他们设计了一个两分支网络架构，一个利用全局信息实现遮挡鲁棒性，另一个利用局部信息确保节点级别的精确估计，最后通过扩散模型融合这些预测。该方法借鉴了扩散模型在图像生成、人体姿态估计等领域的应用，以及PointNet++用于点云特征提取和非刚性点集配准算法的思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将DLO状态估计和跟踪都表述为条件生成问题，利用扩散模型捕捉部分观测和高维DLO状态之间复杂的映射关系。整体流程包括：1) 单帧状态估计模块：输入部分点云，通过点云标准化、PointNet++特征提取、两分支处理（直接回归和点到点投票）以及扩散模型融合生成最终三维节点预测；2) 跨帧状态跟踪模块：利用当前点云和前一帧节点估计，通过KNN特征聚合和扩散模型预测节点运动；3) 预处理和后处理：包括点云标准化、B样条拟合和跟踪失败检测机制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的DLO感知管道，首次同时支持单帧估计和跨帧跟踪；2) 条件生成方法，利用扩散模型解决遮挡情况下的节点级别不确定性；3) 零样本模拟到现实泛化能力，仅在合成数据上训练即可泛化到真实世界。相比之前工作，该方法不依赖手动设计的配准参数，不需要多视图相机或仿真引擎，在严重遮挡下表现更鲁棒，能处理更复杂的变形和拓扑结构，且计算效率更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniStateDLO通过统一的条件生成框架，利用扩散模型解决了可变形线性物体在严重遮挡情况下的三维状态估计和跟踪问题，实现了高精度、鲁棒性和实时性能，并在零样本模拟到现实泛化中表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Perception of deformable linear objects (DLOs), such as cables, ropes, and wires, is the cornerstone for successful downstream manipulation. Although vision-based methods have been extensively explored, they remain highly vulnerable to occlusions that commonly arise in constrained manipulation environments due to surrounding obstacles, large and varying deformations, and limited viewpoints. Moreover, the high dimensionality of the state space, the lack of distinctive visual features, and the presence of sensor noises further compound the challenges of reliable DLO perception. To address these open issues, this paper presents UniStateDLO, the first complete DLO perception pipeline with deep-learning methods that achieves robust performance under severe occlusion, covering both single-frame state estimation and cross-frame state tracking from partial point clouds. Both tasks are formulated as conditional generative problems, leveraging the strong capability of diffusion models to capture the complex mapping between highly partial observations and high-dimensional DLO states. UniStateDLO effectively handles a wide range of occlusion patterns, including initial occlusion, self-occlusion, and occlusion caused by multiple objects. In addition, it exhibits strong data efficiency as the entire network is trained solely on a large-scale synthetic dataset, enabling zero-shot sim-to-real generalization without any real-world training data. Comprehensive simulation and real-world experiments demonstrate that UniStateDLO outperforms all state-of-the-art baselines in both estimation and tracking, producing globally smooth yet locally precise DLO state predictions in real time, even under substantial occlusions. Its integration as the front-end module in a closed-loop DLO manipulation system further demonstrates its ability to support stable feedback control in complex, constrained 3-D environments.</description>
      <author>example@mail.com (Kangchen Lv, Mingrui Yu, Shihefeng Wang, Xiangyang Ji, Xiang Li)</author>
      <guid isPermaLink="false">2512.17764v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Voxel-GS: Quantized Scaffold Gaussian Splatting Compression with Run-Length Coding</title>
      <link>http://arxiv.org/abs/2512.17528v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DCC 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Voxel-GS，一个简单而高效的框架，用于压缩高斯散射格式点云，使用轻量级速率代理和游程编码实现竞争性性能。&lt;h4&gt;背景&lt;/h4&gt;高斯散射格式点云需要有效的压缩处理。&lt;h4&gt;目的&lt;/h4&gt;提出一个简单但高效的框架Voxel-GS，用于压缩高斯散射点云。&lt;h4&gt;方法&lt;/h4&gt;使用可微分量化将Scaffold-GS的高斯属性离散化，设计基于拉普拉斯的速率代理施加熵约束，最后使用八叉树和游程编码对整数型高斯点云进行无损压缩。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的速率代理能够准确估计游程编码的比特率，使Voxel-GS能够消除冗余并优化更紧凑的表示。&lt;h4&gt;结论&lt;/h4&gt;该方法实现了显著的压缩比，并且比先前技术具有更快的编码速度。&lt;h4&gt;翻译&lt;/h4&gt;大规模高斯散射格式点云需要有效的压缩。在本文中，我们提出了Voxel-GS，一个简单而高效的框架，它脱离了先前工作中复杂的神经熵模型，而是仅使用轻量级速率代理和游程编码实现竞争性性能。具体而言，我们采用可微分量化将Scaffold-GS的高斯属性离散化。随后，设计了一种基于拉普拉斯的速率代理来施加熵约束，指导生成高保真和紧凑的重建。最后，使用八叉树和游程编码对这种整数型高斯点云进行无损压缩。实验验证了所提出的速率代理能够准确估计游程编码的比特率，使Voxel-GS能够消除冗余并优化更紧凑的表示。因此，我们的方法实现了显著的压缩比，并且比先前技术具有更快的编码速度。代码可在https://github.com/zb12138/VoxelGS获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Substantial Gaussian splatting format point clouds require effective compression. In this paper, we propose Voxel-GS, a simple yet highly effective framework that departs from the complex neural entropy models of prior work, instead achieving competitive performance using only a lightweight rate proxy and run-length coding. Specifically, we employ a differentiable quantization to discretize the Gaussian attributes of Scaffold-GS. Subsequently, a Laplacian-based rate proxy is devised to impose an entropy constraint, guiding the generation of high-fidelity and compact reconstructions. Finally, this integer-type Gaussian point cloud is compressed losslessly using Octree and run-length coding. Experiments validate that the proposed rate proxy accurately estimates the bitrate of run-length coding, enabling Voxel-GS to eliminate redundancy and optimize for a more compact representation. Consequently, our method achieves a remarkable compression ratio with significantly faster coding speeds than prior art. The code is available at https://github.com/zb12138/VoxelGS.</description>
      <author>example@mail.com (Chunyang Fu, Xiangrui Liu, Shiqi Wang, Zhu Li)</author>
      <guid isPermaLink="false">2512.17528v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Delaunay-Rips filtration: a study and an algorithm</title>
      <link>http://arxiv.org/abs/2512.17382v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了Delaunay-Rips过滤，一种在低维欧几里得点云中比传统Rips过滤更轻量、更快速的替代方法，并提供了全面的理论和经验分析。&lt;h4&gt;背景&lt;/h4&gt;Delaunay-Rips过滤是一种比众所周知的Rips过滤更轻量、更快速的替代方法，特别适用于低维欧几里得点云。然而，尽管有这些优势，它很少被研究。&lt;h4&gt;目的&lt;/h4&gt;论文旨在弥合这一研究空白，为Delaunay-Rips过滤提供全面的理论和经验分析。&lt;h4&gt;方法&lt;/h4&gt;从理论角度分析Delaunay-Rips持久图对Rips持久图的近似性，描述当输入点云被扰动时Delaunay-Rips持久图的不稳定性，并引入一个在任意维度计算Delaunay-Rips过滤持久图的算法。&lt;h4&gt;主要发现&lt;/h4&gt;Delaunay-Rips持久图可以近似Rips持久图；当输入点云被扰动时，Delaunay-Rips持久图存在不稳定性；作者提出的新算法在低维中比传统方法更快且内存占用更少。&lt;h4&gt;结论&lt;/h4&gt;Delaunay-Rips过滤是低维欧几里得点云中Rips过滤的有效替代方案，具有计算和内存效率优势。作者提供了C++实现和Python绑定可供使用。&lt;h4&gt;翻译&lt;/h4&gt;Delaunay-Rips过滤是众所周知的Rips过滤在低维欧几里得点云中的一种更轻量、更快速的替代方法。尽管有这些优势，它很少被研究。在本文中，我们旨在通过提供对这种构造的全面理论和经验分析来弥合这一差距。从理论角度来看，我们展示了与Delaunay-Rips过滤相关的持久图如何近似于使用Rips过滤获得的持久图。此外，我们描述了当输入点云被扰动时，Delaunay-Rips持久图的不稳定性。最后，我们引入了一个在任意维度计算Delaunay-Rips过滤持久图的算法。我们证明，我们的方法在低维中比传统方法更快且内存占用更少。我们的C++实现（带有Python绑定）可在https://github.com/MClemot/GeoPH获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Delaunay-Rips filtration is a lighter and faster alternative to the well-known Rips filtration for low-dimensional Euclidean point clouds. Despite these advantages, it has seldom been studied. In this paper, we aim to bridge this gap by providing a thorough theoretical and empirical analysis of this construction. From a theoretical perspective, we show how the persistence diagrams associated with the Delaunay-Rips filtration approximate those obtained with the Rips filtration. Additionally, we describe the instabilities of the Delaunay-Rips persistence diagrams when the input point cloud is perturbed. Finally, we introduce an algorithm that computes persistence diagrams of Delaunay-Rips filtrations in any dimension. We show that our method is faster and has a lower memory footprint than traditional approaches in low dimensions. Our C++ implementation, which comes with Python bindings, is available at https://github.com/MClemot/GeoPH.</description>
      <author>example@mail.com (Mattéo Clémot, Julie Digne, Julien Tierny)</author>
      <guid isPermaLink="false">2512.17382v1</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Fully Dynamic Algorithms for Chamfer Distance</title>
      <link>http://arxiv.org/abs/2512.16639v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究在完全动态设置下计算Chamfer距离的问题，提出首个在ℓ_p范数下维护Chamfer距离近似的动态算法。&lt;h4&gt;背景&lt;/h4&gt;Chamfer距离是点云广泛使用的差异性度量，在机器学习等领域有实际应用，常需要在对动态变化的数据集进行重复评估时使用。&lt;h4&gt;目的&lt;/h4&gt;高效维护对动态变化点集A和B的Chamfer距离的近似值，其中点集通过插入或删除操作动态变化。&lt;h4&gt;方法&lt;/h4&gt;提出一种近似最近邻搜索的动态算法，该算法在更新Chamfer距离近似值时只需较少的开销。&lt;h4&gt;主要发现&lt;/h4&gt;算法可在tilde(O)(ε^{-d})更新时间内获得(1+ε)-近似，或在tilde(O)(d n^{ε^2} ε^{-4})更新时间内获得O(1/ε)-近似。&lt;h4&gt;结论&lt;/h4&gt;在真实数据集上的评估表明，该方法与自然基线方法相比具有竞争力。&lt;h4&gt;翻译&lt;/h4&gt;我们研究在完全动态设置下计算Chamfer距离的问题，其中两个点集A、B⊂ℝ^d（每个大小最多为n）通过点的插入或删除动态变化，目标是高效维护对dist_CH(A,B)=∑_{a∈A} min_{b∈B} dist(a,b)的近似，其中dist是一种距离度量。Chamfer距离是点云广泛使用的差异性度量，在许多需要重复评估动态变化数据集的实际应用中都有应用，例如在机器学习中用作损失函数。在本文中，我们首次提出了在ℓ_p范数下（p∈{1,2}）维护Chamfer距离近似的动态算法。我们的算法近似于最近邻搜索，只需少量开销。代入标准的ANN界限，我们在tilde(O)(ε^{-d})更新时间内获得(1+ε)-近似，在tilde(O)(d n^{ε^2} ε^{-4})更新时间内获得O(1/ε)-近似。我们在真实数据集上评估了我们的方法，并证明其与自然基线方法相比具有竞争力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在动态变化的点集上高效计算Chamfer距离的问题。当两个点集通过插入或删除点不断变化时，如何高效维护它们之间的Chamfer距离近似值。这个问题在现实中非常重要，因为Chamfer距离是点云间常用的不相似性度量，广泛应用于机器学习（如作为损失函数）、计算机视觉（如3D物体重建）和医学成像（如跟踪解剖结构变化）等领域。在这些应用中，点云数据经常动态变化，需要持续评估点云间的相似性，而现有静态算法在每次更新后重新计算的效率太低。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到Chamfer距离计算本质上可以转化为近似最近邻搜索问题，因为每个点到另一个点集的距离就是该点到点集中最近点的距离。他们借鉴了现有的动态最近邻数据结构作为子程序，并改进了静态Chamfer距离算法中的重要性采样框架。在动态场景中，作者发现无法显式维护点之间的近似分配关系，因为这会导致每次更新时大量点的分配关系发生变化。因此，他们设计了一种隐式表示距离估计的方法，通过动态四叉树结构来维护每个点的'匹配单元格'信息，从而高效地估计距离而不显式存储所有分配关系。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用重要性采样和空间划分来高效估计Chamfer距离。具体来说，算法使用动态四叉树将空间递归划分为嵌套单元格，对于每个点a∈A，确定其'匹配单元格'（同时包含a和B中某个点的最小单元格），并利用单元格大小作为距离的近似估计。整体流程分为三部分：1) 数据结构维护：在四叉树节点中存储A点数、B点数和匹配点数；2) 更新处理：当点插入或删除时，更新受影响路径上的节点信息和采样器；3) 查询处理：通过采样单元格而非直接采样点，对采样出的点使用近似最近邻查询获取距离估计，最后通过重要性采样公式组合多个样本结果得到Chamfer距离的估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个动态Chamfer距离算法，能够在点集变化时高效维护距离近似值；2) 将Chamfer距离计算问题转化为近似最近邻搜索问题；3) 设计隐式距离估计方法，通过四叉树结构中的匹配信息实现；4) 提供多种参数配置下的高效更新方案。相比之前的工作，不同之处在于：静态算法每次更新需重新计算，复杂度高达O(nd·ε^{-2})；动态EMD算法仅适用于二维空间且只能达到O(1/ε)近似；本文方法适用于任意维度，在低维可实现(1+ε)近似，且证明了维护点集映射关系的下界，表明无法高效维护精确分配关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提出了在动态变化的点集上高效计算Chamfer距离的算法，通过将问题转化为近似最近邻搜索并利用重要性采样，实现了比重新计算高得多的效率，为处理动态点云的应用提供了实用工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the problem of computing Chamfer distance in the fully dynamic setting, where two set of points $A, B \subset \mathbb{R}^{d}$, each of size up to $n$, dynamically evolve through point insertions or deletions and the goal is to efficiently maintain an approximation to $\mathrm{dist}_{\mathrm{CH}}(A,B) = \sum_{a \in A} \min_{b \in B} \textrm{dist}(a,b)$, where $\textrm{dist}$ is a distance measure. Chamfer distance is a widely used dissimilarity metric for point clouds, with many practical applications that require repeated evaluation on dynamically changing datasets, e.g., when used as a loss function in machine learning. In this paper, we present the first dynamic algorithm for maintaining an approximation of the Chamfer distance under the $\ell_p$ norm for $p \in \{1,2 \}$. Our algorithm reduces to approximate nearest neighbor (ANN) search with little overhead. Plugging in standard ANN bounds, we obtain $(1+ε)$-approximation in $\tilde{O}(ε^{-d})$ update time and $O(1/ε)$-approximation in $\tilde{O}(d n^{ε^2} ε^{-4})$ update time. We evaluate our method on real-world datasets and demonstrate that it performs competitively against natural baselines.</description>
      <author>example@mail.com (Gramoz Goranci, Shaofeng Jiang, Peter Kiss, Eva Szilagyi, Qiaoyuan Yang)</author>
      <guid isPermaLink="false">2512.16639v2</guid>
      <pubDate>Mon, 22 Dec 2025 15:11:14 +0800</pubDate>
    </item>
    <item>
      <title>Deep learning directed synthesis of fluid ferroelectric materials</title>
      <link>http://arxiv.org/abs/2512.16671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发并验证了一种深度学习数据到分子的管道，实现了有机流体铁电体的靶向设计和合成，实验验证了11个候选材料，展示了发现可合成流体铁电体的实用闭环方法，标志着向功能软材料的自主设计迈进了一步。&lt;h4&gt;背景&lt;/h4&gt;流体铁电体是一类新发现的液晶材料，具有可切换的长程极化有序性，在超快电光技术、响应性软物质和下一代能源材料方面有应用前景。目前这一领域的发现主要依赖于直觉和偶然性，限制了进展。&lt;h4&gt;目的&lt;/h4&gt;开发并实验验证一个深度学习数据到分子的管道，实现有机流体铁电体的靶向设计和合成。&lt;h4&gt;方法&lt;/h4&gt;整理所有已知纵向极化液晶材料的综合数据集；训练图神经网络预测铁电行为（准确率高达95%）和转变温度（均方根误差低至11K）；使用图变分自编码器生成新分子结构；通过高性能分类器和回归器组合过滤候选物；与计算逆合成引擎和数字化化学库存集成缩小设计空间；合成并表征11个候选物，通过基于混合的外推方法比较实验结果与神经网络预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了新型流体铁电体材料；通过质量反馈数据增强了原始数据集；展示了发现可合成流体铁电体的实用闭环方法。&lt;h4&gt;结论&lt;/h4&gt;该研究代表了一种发现可合成流体铁电体的实用闭环方法，标志着向功能软材料的自主设计迈进了一步。&lt;h4&gt;翻译&lt;/h4&gt;流体铁电体是一类最近发现的液晶，表现出可切换的长程极化有序性，为超快电光技术、响应性软物质和下一代能源材料提供了机会。然而，它们的发现几乎完全依赖于直觉和偶然性，限制了该领域的进展。在这里，我们开发并实验验证了一种深度学习数据到分子的管道，使新型有机流体铁电体的靶向设计和合成成为可能。我们整理了所有已知的纵向极化液晶材料的综合数据集，并训练了图神经网络，以高达95%的准确率预测铁电行为，并实现低至11K的转变温度均方根误差。图变分自编码器生成全新的分子结构，使用高性能分类器和回归器组合进行过滤，以识别具有预测铁电向列行为和可及转变温度的候选物。与计算逆合成引擎和数字化化学库存的集成进一步将设计空间缩小到可合成的长名单。合成了11个候选物并通过既定的基于混合的外推方法进行了表征。从中，外推的铁电向列转变与神经网络预测进行了比较。新型材料的实验验证通过质量反馈数据增强了原始数据集，从而有助于未来的研究。这些结果展示了一种发现可合成流体铁电体的实用闭环方法，标志着向功能软材料的自主设计迈进了一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fluid ferroelectrics, a recently discovered class of liquid crystals that exhibit switchable, long-range polar order, offer opportunities in ultrafast electro-optic technologies, responsive soft matter, and next-generation energy materials. Yet their discovery has relied almost entirely on intuition and chance, limiting progress in the field. Here we develop and experimentally validate a deep-learning data-to-molecule pipeline that enables the targeted design and synthesis of new organic fluid ferroelectrics. We curate a comprehensive dataset of all known longitudinally polar liquid-crystal materials and train graph neural networks that predict ferroelectric behaviour with up to 95% accuracy and achieve root mean square errors as low as 11 K for transition temperatures. A graph variational autoencoder generates de novo molecular structures which are filtered using an ensemble of high-performing classifiers and regressors to identify candidates with predicted ferroelectric nematic behaviour and accessible transition temperatures. Integration with a computational retrosynthesis engine and a digitised chemical inventory further narrows the design space to a synthesis-ready longlist. 11 candidates were synthesised and characterized through established mixture-based extrapolation methods. From which extrapolated ferroelectric nematic transitions were compared against neural network predictions. The experimental verification of novel materials augments the original dataset with quality feedback data thus aiding future research. These results demonstrate a practical, closed-loop approach to discovering synthesizable fluid ferroelectrics, marking a step toward autonomous design of functional soft materials.</description>
      <author>example@mail.com (Charles Parton-Barr, Stuart R. Berrow, Calum J. Gibb, Jordan Hobbs, Wanhe Jiang, Caitlin O'Brien, Will C. Ogle, Helen F. Gleeson, Richard J. Mandle)</author>
      <guid isPermaLink="false">2512.16671v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
  <item>
      <title>Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance</title>
      <link>http://arxiv.org/abs/2512.16661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于注意力的子图检索器模型，结合图神经网络和大型语言模型，通过注意力剪枝技术提取精炼子图，用于高级知识推理。&lt;h4&gt;背景&lt;/h4&gt;在信息驱动的时代，获取科学出版物变得容易，但从海量研究中筛选信息却面临前所未有的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效处理大规模信息数据库检索的模型，解决科研文献筛选困难的问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于注意力的子图检索器，应用图神经网络作为检索器，通过注意力剪枝提取精炼子图，再传递给大型语言模型进行知识推理。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络和图注意力机制在搜索大规模信息数据库方面表现出强大有效性，特别是与现代大型语言模型结合时。&lt;h4&gt;结论&lt;/h4&gt;结合图神经网络和大型语言模型的优势，并应用注意力机制进行子图提取，可有效解决大规模科研文献筛选挑战。&lt;h4&gt;翻译&lt;/h4&gt;在当今信息驱动的世界中，获取科学出版物变得越来越容易。与此同时，从海量可用研究文献中进行筛选比以往任何时候都更具挑战性。图神经网络(GNNs)和图注意力机制在搜索大规模信息数据库方面显示出强大的有效性，特别是与现代大型语言模型结合时。在本文中，我们提出了一种基于注意力的子图检索器，这是一种将图神经网络作为检索器的模型，它应用基于注意力的剪枝技术提取精炼的子图，然后将该子图传递给大型语言模型进行高级知识推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.</description>
      <author>example@mail.com (Jacob Reiss, Shikshya Shiwakoti, Samuel Goldsmith, Ujjwal Pandit)</author>
      <guid isPermaLink="false">2512.16661v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Riemannian Stochastic Interpolants for Amorphous Particle Systems</title>
      <link>http://arxiv.org/abs/2512.16607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于生成无定形材料平衡构型的等变黎曼随机插值框架，结合了几何约束和对称性，显著提高了生成性能。&lt;h4&gt;背景&lt;/h4&gt;现代生成模型在物理系统模拟任务中潜力巨大，但需要针对特定领域约束进行适应。生物分子和晶体材料已取得进展，而无定形材料（玻璃）作为缺乏原子周期性的无序粒子系统，其平衡构型采样极为困难。&lt;h4&gt;目的&lt;/h4&gt;开发能够产生具有明确定义似率的平衡构型的生成框架，以克服玻璃形成材料平衡构型采样缓慢困难的问题。&lt;h4&gt;方法&lt;/h4&gt;利用等变黎曼随机插值框架，结合黎曼随机插值和等变流匹配。严格纳入周期边界条件和多组分粒子系统的对称性，调整等变图神经网络以直接在环面上操作。&lt;h4&gt;主要发现&lt;/h4&gt;在模型无定形系统上的数值实验表明，强制执行几何和对称约束显著提高了生成性能。&lt;h4&gt;结论&lt;/h4&gt;通过结合等变黎曼随机插值框架和几何对称约束，可以有效生成无定形材料的平衡构型，为无序粒子系统模拟提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;现代生成模型在加速涉及物理系统模拟的多样化任务方面具有巨大潜力，但它们必须针对每个领域的特定约束进行适应。在生物分子和晶体材料方面已取得显著进展。在此，我们解决无定形材料（玻璃）的问题，它们是缺乏原子周期性的无序粒子系统。采样玻璃形成材料的平衡构型是一个 notoriously 缓慢且困难的任务。通过开发能够产生具有明确定义似率的平衡构型的生成框架，可以克服这一障碍。在这项工作中，我们通过利用等变黎曼随机插值框架来解决这一挑战，该框架结合了黎曼随机插值和等变流匹配。我们的方法严格纳入周期边界条件和多组分粒子系统的对称性，调整等变图神经网络以直接在环面上操作。我们在模型无定形系统上的数值实验表明，强制执行几何和对称约束显著提高了生成性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern generative models hold great promise for accelerating diverse tasks involving the simulation of physical systems, but they must be adapted to the specific constraints of each domain. Significant progress has been made for biomolecules and crystalline materials. Here, we address amorphous materials (glasses), which are disordered particle systems lacking atomic periodicity. Sampling equilibrium configurations of glass-forming materials is a notoriously slow and difficult task. This obstacle could be overcome by developing a generative framework capable of producing equilibrium configurations with well-defined likelihoods. In this work, we address this challenge by leveraging an equivariant Riemannian stochastic interpolation framework which combines Riemannian stochastic interpolant and equivariant flow matching. Our method rigorously incorporates periodic boundary conditions and the symmetries of multi-component particle systems, adapting an equivariant graph neural network to operate directly on the torus. Our numerical experiments on model amorphous systems demonstrate that enforcing geometric and symmetry constraints significantly improves generative performance.</description>
      <author>example@mail.com (Louis Grenioux, Leonardo Galliano, Ludovic Berthier, Giulio Biroli, Marylou Gabrié)</author>
      <guid isPermaLink="false">2512.16607v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>GFLAN: Generative Functional Layouts</title>
      <link>http://arxiv.org/abs/2512.16275v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GFLAN的生成框架，通过将平面图生成分解为拓扑规划和几何实现两个阶段来解决现有方法在捕捉建筑推理方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;自动平面图生成涉及组合搜索、几何约束满足和功能设计要求的交叉领域，这一领域一直难以实现统一的计算处理。虽然最近的深度学习方法提高了技术水平，但它们往往难以捕捉建筑推理的关键方面。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法难以捕捉建筑推理（拓扑关系优先、功能约束传播、循环模式形成）的基本挑战，引入GFLAN框架重构平面图合成过程。&lt;h4&gt;方法&lt;/h4&gt;给定一个外部边界和前门位置，采用两阶段分解：阶段A使用具有双编码器的特殊卷积架构，通过离散概率图顺序分配房间质心；阶段B构建异构图并应用增强Transformer的图神经网络联合回归房间边界。&lt;h4&gt;主要发现&lt;/h4&gt;现有深度学习方法虽然提高了技术水平，但往往难以捕捉建筑推理的关键方面，包括拓扑关系优先于几何实例化、功能约束通过邻接网络传播以及从局部连接决策中出现循环模式。&lt;h4&gt;结论&lt;/h4&gt;GFLAN框架通过明确分解为拓扑规划和几何实现两个阶段，能够更好地处理平面图生成中的建筑推理问题。&lt;h4&gt;翻译&lt;/h4&gt;自动平面图生成位于组合搜索、几何约束满足和功能设计要求的交叉点 - 这一交汇点历史上一直抵制统一的计算处理。虽然最近的深度学习方法提高了技术水平，但它们往往难以捕捉建筑推理：拓扑关系优先于几何实例化、功能约束通过邻接网络传播、以及从局部连接决策中出现循环模式。为了解决这些基本挑战，本文引入了GFLAN，一个生成框架，通过明确分解为拓扑规划和几何实现来重构平面图合成。给定单个外部边界和前门位置，我们的方法不采用直接的像素到像素或墙壁追踪生成，而是采用原则性的两阶段分解。阶段A采用具有双编码器的特殊卷积架构 - 将不变的空间上下文与演变的布局状态分离 - 通过在可行放置上的离散概率图顺序分配房间质心。阶段B构建一个将房间节点链接到边界顶点的异构图，然后应用增强Transformer的图神经网络(GNN)，该网络联合回归房间边界。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.</description>
      <author>example@mail.com (Mohamed Abouagour, Eleftherios Garyfallidis)</author>
      <guid isPermaLink="false">2512.16275v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Sharpness-aware Federated Graph Learning</title>
      <link>http://arxiv.org/abs/2512.16247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by WSDM'26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SEAL的联邦图学习算法，通过同时最小化损失函数及其尖锐度，并引入基于本地表示相关矩阵的正则化器，解决了联邦图学习中数据异构性问题导致的本地模型泛化能力弱和维度崩塌问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)应用于大规模真实世界图数据的主要障碍是集中式训练的挑战，这需要聚合来自不同组织的数据并引发隐私问题。联邦图学习(FGL)通过在不共享私有数据的情况下协作训练GNN模型来解决这一问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的优化目标，使其意识到本地GNN模型的尖锐度，通过同时最小化损失函数及其尖锐度寻找在损失值均匀较低的区域中的模型参数，提高对异构数据的泛化能力；并通过引入基于本地表示相关矩阵的正则化器，缓解学习模型的维度崩塌问题。&lt;h4&gt;方法&lt;/h4&gt;提出了名为SEAL(Sharpness-aware fEderated grAph Learning)的算法，该方法结合了损失尖锐度感知和表示相关性正则化，通过最小化损失函数及其尖锐度同时放松由单个本地图样本生成的表示之间的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;SEAL算法能够增强联邦图学习中本地GNN模型的分类准确性和泛化能力。在多个图分类基准上的实验研究表明，SEAL始终优于最先进的FGL基线，并为更多参与者带来收益。&lt;h4&gt;结论&lt;/h4&gt;SEAL算法通过同时考虑损失尖锐度和表示相关性，有效解决了联邦图学习中的数据异构性问题，提高了本地GNN模型的泛化能力和分类准确性。&lt;h4&gt;翻译&lt;/h4&gt;将图神经网络(GNNs)应用于大规模真实世界图数据的许多障碍之一是集中式训练的挑战，这需要聚合来自不同组织的数据，引发隐私问题。联邦图学习(FGL)通过在不共享私有数据的情况下协作训练GNN模型来解决这一问题。然而，FGL系统中的一个核心挑战是客户端间本地训练数据分布的差异，即数据异构性问题。大多数现有解决方案存在两个问题：(1)基于经验风险最小化的典型优化器容易导致本地模型陷入尖锐的谷底，削弱其对分布外图数据的泛化能力。(2)本地图数据学习表示中的普遍维度崩塌对GNN模型的分类能力产生不利影响。为此，我们提出了一种新颖的优化目标，使其意识到本地GNN模型的尖锐度(即损失曲面的曲率)。通过同时最小化损失函数及其尖锐度，我们寻找在损失值均匀较低的区域中的模型参数，从而提高对异构数据的泛化能力。通过引入基于本地表示相关矩阵的正则化器，我们放松由单个本地图样本生成的表示之间的相关性，以缓解学习模型的维度崩塌。所提出的SEAL(Sharpness-aware fEderated grAph Learning)算法能够增强联邦图学习中本地GNN模型的分类准确性和泛化能力。在多个图分类基准上的实验研究表明，SEAL始终优于最先进的FGL基线，并为更多参与者带来收益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One of many impediments to applying graph neural networks (GNNs) to large-scale real-world graph data is the challenge of centralized training, which requires aggregating data from different organizations, raising privacy concerns. Federated graph learning (FGL) addresses this by enabling collaborative GNN model training without sharing private data. However, a core challenge in FGL systems is the variation in local training data distributions among clients, known as the data heterogeneity problem. Most existing solutions suffer from two problems: (1) The typical optimizer based on empirical risk minimization tends to cause local models to fall into sharp valleys and weakens their generalization to out-of-distribution graph data. (2) The prevalent dimensional collapse in the learned representations of local graph data has an adverse impact on the classification capacity of the GNN model. To this end, we formulate a novel optimization objective that is aware of the sharpness (i.e., the curvature of the loss surface) of local GNN models. By minimizing the loss function and its sharpness simultaneously, we seek out model parameters in a flat region with uniformly low loss values, thus improving the generalization over heterogeneous data. By introducing a regularizer based on the correlation matrix of local representations, we relax the correlations of representations generated by individual local graph samples, so as to alleviate the dimensional collapse of the learned model. The proposed \textbf{S}harpness-aware f\textbf{E}derated gr\textbf{A}ph \textbf{L}earning (SEAL) algorithm can enhance the classification accuracy and generalization ability of local GNN models in federated graph learning. Experimental studies on several graph classification benchmarks show that SEAL consistently outperforms SOTA FGL baselines and provides gains for more participants.</description>
      <author>example@mail.com (Ruiyu Li, Peige Zhao, Guangxia Li, Pengcheng Wu, Xingyu Gao, Zhiqiang Xu)</author>
      <guid isPermaLink="false">2512.16247v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models</title>
      <link>http://arxiv.org/abs/2512.16244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CFC框架，利用大语言模型解决图神经网络在开放世界场景中的OOD分类问题，通过粗到细的分类方法显著提高了OOD检测和分类性能。&lt;h4&gt;背景&lt;/h4&gt;开集分类方法对于在开放世界场景中部署图神经网络至关重要，但现有方法通常将所有OOD样本视为单一类别，而真实世界应用需要对OOD样本有更深入的了解。&lt;h4&gt;目的&lt;/h4&gt;解决在没有真实标签信息的情况下，能否将OOD检测扩展到OOD分类的问题，并提出一种利用大语言模型的从粗到细的开集分类框架。&lt;h4&gt;方法&lt;/h4&gt;CFC框架包含三个关键组件：粗分类器使用LLM提示进行OOD检测和异常标签生成；基于GNN的细分类器使用识别的OOD样本进行训练；通过LLM提示和后处理的OOD标签实现精细的OOD分类。该方法采用基于内在意义的语义OOD实例，而非合成或辅助样本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，CFC在图和文本领域比最先进方法提高了10%的OOD检测性能，在图数据集上实现了高达70%的OOD分类准确率。&lt;h4&gt;结论&lt;/h4&gt;CFC框架能够有效地进行OOD检测和分类，不需要真实标签信息，提高了可解释性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;开发能够对分布内(ID)数据进行分类同时检测分布外(OOD)样本的开集分类方法，对于在开放世界场景中部署图神经网络(GNNs)至关重要。现有方法通常将所有OOD样本视为单一类别，尽管在现实世界的应用中，特别是高风险设置如欺诈检测和医疗诊断，需要对OOD样本有更深入的了解，包括它们可能的标签。这引出了一个关键问题：在没有真实标签信息的情况下，能否将OOD检测扩展到OOD分类？为解决这一问题，我们提出了一个利用大语言模型(LLMs)处理图数据集的从粗到细的开集分类(CFC)框架。CFC包含三个关键组件：使用LLM提示进行OOD检测和异常标签生成的粗分类器；使用粗分类器识别的OOD样本进行训练以增强OOD检测和ID分类的基于GNN的细分类器；以及通过LLM提示和后处理的OOD标签实现的精细OOD分类。与依赖合成或辅助OOD样本的方法不同，CFC采用基于其内在意义的语义OOD实例，提高了可解释性和实用性。实验结果表明，CFC在图和文本领域比最先进方法提高了10%的OOD检测性能，并在图数据集上实现了高达70%的OOD分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.</description>
      <author>example@mail.com (Xueqi Ma, Xingjun Ma, Sarah Monazam Erfani, Danilo Mandic, James Bailey)</author>
      <guid isPermaLink="false">2512.16244v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models</title>
      <link>http://arxiv.org/abs/2512.16236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 1 figure, Accepted in CLNLP'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是对信息检索系统中重排序技术的全面综述，涵盖了从基础方法到先进神经网络架构的发展历程，以及在现代检索增强生成(RAG)管道中的应用。文章还探讨了提高重排序效率的技术，特别是知识蒸馏方法，以及大型语言模型在重排序中的新兴应用。&lt;h4&gt;背景&lt;/h4&gt;重排序是当代信息检索系统中的关键阶段，通过优化初始候选集来提高用户呈现最终结果的相关性。特别是在现代检索增强生成(RAG)管道中，检索到的文档对输出质量有显著影响。&lt;h4&gt;目的&lt;/h4&gt;提供对不断发展的重排序技术格局的全面指南，清晰展示重排序方法的进展。阐明各种重排序策略的基本思想、相对有效性、计算特性和实际权衡。&lt;h4&gt;方法&lt;/h4&gt;作者进行了历史轨迹的梳理，从基础方法开始，探索了各种复杂的神经网络架构，包括交叉编码器、类似T5的序列生成模型和用于结构信息的图神经网络(GNNs)。分析了提高神经重排序器效率的技术，特别是知识蒸馏方法。还研究了在重排序中集成大型语言模型(LLMs)的新兴领域，包括新的提示策略和微调技术。&lt;h4&gt;主要发现&lt;/h4&gt;论文展示了重排序技术的多样性和发展历程，认识到先进神经重排序器的计算成本问题，分析了提高效率的方法，特别是在知识蒸馏方面。发现了大型语言模型在重排序中的新兴应用和潜力。&lt;h4&gt;结论&lt;/h4&gt;这篇综述提供了各种重排序范式的结构化综合，突出了它们的基本原理和相对优缺点，为理解和应用重排序技术提供了全面的视角。&lt;h4&gt;翻译&lt;/h4&gt;重排序是当代信息检索系统中的关键阶段，通过优化初始候选集来提高用户呈现最终结果的相关性。本文是对不断发展的重排序技术格局的全面指南，清晰展示了重排序方法的进展。我们介绍了信息检索中使用的重排序模型的综合调查，特别是在现代检索增强生成(RAG)管道中，检索到的文档显著影响输出质量。我们按时间顺序梳理了重排序技术的历史轨迹，从基础方法开始，然后探索了各种复杂的神经网络架构，如交叉编码器、类似T5的序列生成模型以及用于结构信息的图神经网络(GNNs)。认识到先进神经重排序器的计算成本，我们分析了提高效率的技术，特别是知识蒸馏，用于创建具有竞争力的轻量级替代方案。此外，我们探索了在重排序中集成大型语言模型的新兴领域，研究了新的提示策略和微调技术。本综述旨在阐明各种重排序策略的基本思想、相对有效性、计算特性和实际权衡。该综述提供了各种重排序范式的结构化综合，突出了它们的基本原理和相对优缺点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.</description>
      <author>example@mail.com (Tejul Pandit, Sakshi Mahendru, Meet Raval, Dhvani Upadhyay)</author>
      <guid isPermaLink="false">2512.16236v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-scale Fused Graph Neural Network with Inter-view Contrastive Learning for Spatial Transcriptomics Data Clustering</title>
      <link>http://arxiv.org/abs/2512.16188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为stMFG的多尺度交互融合图网络，用于解决空间转录组学中空间域识别的挑战，通过引入层级跨视图注意力和跨视图对比学习，实现了空间和基因特征的有效整合，并在实验中取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学能够在原生组织环境中进行全基因组表达分析，但由于基因-空间相互作用的复杂性，识别空间域仍然具有挑战性。现有方法通常采用'分别编码，后期融合'的方式，分别处理空间和特征视图，仅在最输出层进行融合，这限制了多尺度语义捕获和跨视图交互。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更好地捕获多尺度语义并促进跨视图交互的方法，以改进空间转录组学中的空间域识别性能。&lt;h4&gt;方法&lt;/h4&gt;提出了stMFG（多尺度交互融合图网络），该方法引入了层级跨视图注意力机制，在每个卷积后动态整合空间和基因特征。模型结合了跨视图对比学习和空间约束，以提高区分性同时保持空间连续性。&lt;h4&gt;主要发现&lt;/h4&gt;在DLPFC和乳腺癌数据集上，stMFG超越了最先进的方法，在某些切片上实现了高达14%的ARI（调整兰德指数）改进。&lt;h4&gt;结论&lt;/h4&gt;stMFG通过有效整合空间和基因特征，显著提高了空间域识别的性能，为空间转录组学分析提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;空间转录组学能够在原生组织环境中进行全基因组表达分析，但由于基因-空间相互作用的复杂性，识别空间域仍然具有挑战性。现有方法通常分别处理空间和特征视图，仅在最输出层进行融合——这是一种'分别编码，后期融合'的范式，限制了多尺度语义捕获和跨视图交互。因此，我们提出了stMFG，这是一种多尺度交互融合图网络，它引入了层级跨视图注意力，在每个卷积后动态整合空间和基因特征。该模型结合了跨视图对比学习和空间约束，以提高区分性同时保持空间连续性。在DLPFC和乳腺癌数据集上，stMFG超越了最先进的方法，在某些切片上实现了高达14%的ARI改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial transcriptomics enables genome-wide expression analysis within native tissue context, yet identifying spatial domains remains challenging due to complex gene-spatial interactions. Existing methods typically process spatial and feature views separately, fusing only at output level - an "encode-separately, fuse-late" paradigm that limits multi-scale semantic capture and cross-view interaction. Accordingly, stMFG is proposed, a multi-scale interactive fusion graph network that introduces layer-wise cross-view attention to dynamically integrate spatial and gene features after each convolution. The model combines cross-view contrastive learning with spatial constraints to enhance discriminability while maintaining spatial continuity. On DLPFC and breast cancer datasets, stMFG outperforms state-of-the-art methods, achieving up to 14% ARI improvement on certain slices.</description>
      <author>example@mail.com (Jianping Mei, Siqi Ai, Ye Yuan)</author>
      <guid isPermaLink="false">2512.16188v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Approach to Alzheimer's Diagnosis: Geometric Insights from Cube Copying and Cognitive Assessments</title>
      <link>http://arxiv.org/abs/2512.16184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种创新的多模态机器学习方法，通过分析手绘立方体草图来辅助阿尔茨海默病的早期检测。该方法利用图神经网络捕捉立方体绘制的几何和拓扑特征，并结合人口统计和神经心理学数据，为AD筛查提供了一种可解释、非侵入性和可扩展的新工具。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病的早期和可及检测仍然是一个关键的临床挑战，立方体临摹任务提供了一种简单但信息丰富的视空间功能评估方法。&lt;h4&gt;目的&lt;/h4&gt;提出一个多模态框架，将手绘立方体草图转换为图结构表示，捕捉几何和拓扑特性，并将这些特征与人口统计信息和神经心理学测试分数结合用于AD分类。&lt;h4&gt;方法&lt;/h4&gt;将立方体绘制建模为具有节点特征的图，节点特征编码空间坐标、局部基于图let的拓扑和角度几何，使用图神经网络处理这些特征，并在晚期融合模型中与年龄、教育和NPT特征融合。&lt;h4&gt;主要发现&lt;/h4&gt;基于图的表示提供了强大的单模态基线，明显优于基于像素的卷积模型；多模态融合进一步提高了性能和对类别不平衡的鲁棒性；基于SHAP的可解释性分析确定了特定的图let基序和几何失真作为关键预测因子，与临床观察到的AD患者立方体绘制组织紊乱现象高度一致。&lt;h4&gt;结论&lt;/h4&gt;基于图的立方体临摹分析成为一种可解释、非侵入性和可扩展的阿尔茨海默病筛查方法。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病的早期和可及检测仍然是一个关键的临床挑战，立方体临摹任务提供了一种简单但信息丰富的视空间功能评估方法。这项工作提出了一个多模态框架，将手绘立方体草图转换为捕获几何和拓扑特性的图结构表示，并将这些特征与人口统计信息和神经心理学测试分数结合用于AD分类。立方体绘制被建模为具有节点特征的图，这些节点特征编码空间坐标、局部基于图let的拓扑和角度几何，使用图神经网络处理这些特征，并在晚期融合模型中与年龄、教育和NPT特征融合。实验结果表明，基于图的表示提供了强大的单模态基线，明显优于基于像素的卷积模型，而多模态融合进一步提高了性能和对类别不平衡的鲁棒性。基于SHAP的可解释性分析确定了特定的图let基序和几何失真作为关键预测因子，与临床观察到的AD患者立方体绘制组织紊乱现象高度一致。总的来说，这些结果确立了基于图的立方体临摹分析作为一种可解释、非侵入性和可扩展的阿尔茨海默病筛查方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early and accessible detection of Alzheimer's disease (AD) remains a critical clinical challenge, and cube-copying tasks offer a simple yet informative assessment of visuospatial function. This work proposes a multimodal framework that converts hand-drawn cube sketches into graph-structured representations capturing geometric and topological properties, and integrates these features with demographic information and neuropsychological test (NPT) scores for AD classification. Cube drawings are modeled as graphs with node features encoding spatial coordinates, local graphlet-based topology, and angular geometry, which are processed using graph neural networks and fused with age, education, and NPT features in a late-fusion model. Experimental results show that graph-based representations provide a strong unimodal baseline and substantially outperform pixel-based convolutional models, while multimodal integration further improves performance and robustness to class imbalance. SHAP-based interpretability analysis identifies specific graphlet motifs and geometric distortions as key predictors, closely aligning with clinical observations of disorganized cube drawings in AD. Together, these results establish graph-based analysis of cube copying as an interpretable, non-invasive, and scalable approach for Alzheimer's disease screening.</description>
      <author>example@mail.com (Jaeho Yang, Kijung Yoon)</author>
      <guid isPermaLink="false">2512.16184v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation</title>
      <link>http://arxiv.org/abs/2512.16143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SegGraph的新框架，用于解决少样本3D部件分割中有效聚合2D基础模型知识到3D的问题。通过基于SAM分割图的传播方法，显式学习几何特征并保持语义一致性，显著提高了分割性能。&lt;h4&gt;背景&lt;/h4&gt;最近的进展表明2D基础模型在低样本3D部件分割方面有很大潜力，但如何有效聚合2D基础模型的3D知识仍是一个开放问题。现有方法要么忽略3D特征学习的几何结构，要么忽视SAM的高质量分组线索，导致分割不足和不一致的部件标签。&lt;h4&gt;目的&lt;/h4&gt;解决如何有效聚合2D基础模型知识到3D的问题，提高3D部件分割的性能，特别是在小部件和部件边界上的表现。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于SAM分割图的传播方法SegGraph，通过建模分割之间的相互重叠和邻接关系编码几何特征，构建分割图表示空间关系，使用图神经网络传播特征，并采用视图方向加权的融合将分割特征映射到3D点以保持语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在PartNet-E上的实验表明，SegGraph比所有竞争基线至少高出6.9%的mIoU，特别在小部件和部件边界上表现优异，展示了优越的几何理解能力。&lt;h4&gt;结论&lt;/h4&gt;SegGraph方法通过显式学习几何特征和保持语义一致性，有效解决了2D基础模型知识到3D的聚合问题，显著提高了3D部件分割的性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种用于少样本3D部件分割的新框架。最近的进展已经证明2D基础模型在低样本3D部件分割方面具有巨大潜力。然而，如何有效聚合2D基础模型的3D知识仍然是一个开放问题。现有方法要么忽略3D特征学习的几何结构，要么忽视SAM的高质量分组线索，导致分割不足和不一致的部件标签。我们设计了一种基于SAM分割图的传播方法，名为SegGraph，用于显式学习SAM分割掩码中编码的几何特征。我们的方法通过建模分割之间的相互重叠和邻接关系来编码几何特征，同时保持分割内的语义一致性。我们构建了一个分割图，概念上类似于地图集，其中节点代表分割，边捕获它们的空间关系（重叠/邻接）。每个节点自适应调制2D基础模型特征，然后通过图神经网络传播以学习全局几何结构。为了强制执行分割内的语义一致性，我们使用一种新的视图方向加权的融合将分割特征映射到3D点，减弱来自低质量分割的贡献。在PartNet-E上的大量实验表明，我们的方法比所有竞争基线至少高出6.9%的mIoU。进一步分析显示，SegGraph在小部件和部件边界上实现了特别强的性能，展示了其优越的几何理解能力。代码可在以下网址获取：https://github.com/YueyangHu2000/SegGraph。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何有效将2D基础模型（如SAM）的知识聚合到3D空间，用于少样本3D部件分割的问题。这个问题很重要，因为3D部件分割是计算机视觉和图形学的基础任务，广泛应用于形状分析、3D建模和机器人操作等领域。许多实际应用涉及新颖形状，需要仅用少量标注就能实现高质量的3D部件分割，而2D基础模型虽有强大泛化能力，但2D图像和3D几何形状间的模态差距限制了它们在3D领域的直接应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于三个关键洞察设计了SegGraph方法：1)SAM的高质量分割为3D点提供了一致的段内分组线索；2)段之间的空间关系（重叠和相邻）为部件分割提供了重要先验；3)SAM段的质量与观察方向相关。作者借鉴了现有工作，如利用SAM进行2D分割、使用多视图渲染技术、采用图神经网络（GATv2）进行特征传播，并受微分几何中图册概念的启发，将段图视为3D形状的图册表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建基于SAM分割的段图，通过图神经网络传播特征，将2D基础模型的知识有效聚合到3D空间，同时保留几何结构和语义一致性。整体流程包括：1)特征编码：将3D点云渲染为多视图图像，提取并聚合特征；2)段生成：使用SAM进行分割并聚合为3D段；3)段编码：计算自适应点贡献和段特征；4)构建段图：以段为节点，空间关系为边构建图；5)图传播：使用GATv2传播特征；6)视角质量感知特征池化：根据视角质量将段特征映射回3D点；7)部件预测：结合特征预测部件标签。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出SAM段图框架，显式学习SAM分割掩码编码的几何特征；2)通过建模段间重叠和相邻关系编码几何特征；3)构建类似于图册的段图结构；4)提出视角方向加权的融合方法。相比之前工作的不同：相比3D标签聚合方法，考虑了几何结构；相比3D特征聚合方法，使用段图而非简单KNN传播；相比蒸馏方法，不需要大量3D数据训练，更适合少样本场景；显式利用SAM分割质量与观察方向的相关性提高鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SegGraph通过构建SAM段图并利用图神经网络传播特征，有效解决了2D基础模型到3D部件分割的知识转移问题，在少样本场景下实现了最先进的性能，特别是在小部件和边界区域表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.</description>
      <author>example@mail.com (Yueyang Hu, Haiyong Jiang, Haoxuan Song, Jun Xiao, Hao Pan)</author>
      <guid isPermaLink="false">2512.16143v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks for Interferometer Simulations</title>
      <link>http://arxiv.org/abs/2512.16051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究将图神经网络(GNNs)应用于物理科学中的仪器设计领域，以LIGO模拟为例，展示了GNNs在准确捕捉复杂光学物理的同时，实现了比现有模拟技术快815倍的运行效率，并提供了高质量的数据集作为未来研究的基准。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)近年来在高能物理、材料科学和流体动力学问题中显示出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;探索GNNs在物理科学仪器设计领域的新应用，以LIGO模拟为案例研究。&lt;h4&gt;方法&lt;/h4&gt;应用图神经网络技术模拟激光干涉引力波天文台(LIGO)的模型，捕捉复杂的光学物理特性。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs能够准确模拟复杂的光学物理，同时运行速度比现有最先进的模拟软件包快815倍。&lt;h4&gt;结论&lt;/h4&gt;该研究讨论了仪器设计问题为机器学习模型带来的独特挑战，并提供了三种干涉仪拓扑结构的高保真光学物理模拟数据集，可作为未来研究的基准测试套件。&lt;h4&gt;翻译&lt;/h4&gt;近年来，图神经网络(GNNs)在解决高能物理、材料科学和流体动力学问题方面显示出巨大潜力。在本研究中，我们介绍了GNNs在物理科学中的新应用：仪器设计。作为案例研究，我们将GNNs应用于激光干涉引力波天文台(LIGO)的模型模拟，表明它们能够准确捕捉其中复杂的光学物理，同时运行速度比最先进的模拟软件包快815倍。我们讨论了该问题为机器学习模型带来的独特挑战。此外，我们提供了三种干涉仪拓扑结构的高保真光学物理模拟数据集，可作为未来研究方向的基准测试套件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, graph neural networks (GNNs) have shown tremendous promise in solving problems in high energy physics, materials science, and fluid dynamics. In this work, we introduce a new application for GNNs in the physical sciences: instrumentation design. As a case study, we apply GNNs to simulate models of the Laser Interferometer Gravitational-Wave Observatory (LIGO) and show that they are capable of accurately capturing the complex optical physics at play, while achieving runtimes 815 times faster than state of the art simulation packages. We discuss the unique challenges this problem provides for machine learning models. In addition, we provide a dataset of high-fidelity optical physics simulations for three interferometer topologies, which can be used as a benchmarking suite for future work in this direction.</description>
      <author>example@mail.com (Sidharth Kannan, Pooyan Goodarzi, Evangelos E. Papalexakis, Jonathan W. Richardson)</author>
      <guid isPermaLink="false">2512.16051v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2512.16724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at RA-L 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VERM方法，通过利用基础模型知识从3D点云中生成虚拟任务自适应视图，过滤冗余信息，提高机器人3D操作任务的效率和性能。&lt;h4&gt;背景&lt;/h4&gt;机器人在执行3D操作任务时，需要基于多个固定摄像头的感知进行动作规划。多摄像头设置引入了大量冗余和无关信息，增加了计算成本，并迫使模型花费额外的训练时间来提取关键的任务相关细节。&lt;h4&gt;目的&lt;/h4&gt;过滤冗余信息并准确提取任务相关特征，提高机器人3D操作任务的效率和性能。&lt;h4&gt;方法&lt;/h4&gt;提出了VERM（Virtual Eye for Robotic Manipulation）方法，利用基础模型中的知识，从构建的3D点云中想象一个虚拟任务自适应视图，有效捕获必要信息并减轻遮挡。同时设计了深度感知模块和动态粗到细的过程以促进3D动作规划和精细操作。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟基准RLBench和现实世界评估中的大量实验结果证明了该方法的有效性，超越了之前的先进方法，同时实现了训练时间1.89倍加速和推理速度1.54倍加速。&lt;h4&gt;结论&lt;/h4&gt;VERM方法能够有效解决多摄像头系统中的冗余信息问题，提高机器人在3D操作任务中的性能和效率。&lt;h4&gt;翻译&lt;/h4&gt;当执行3D操作任务时，机器人必须基于多个固定摄像头的感知来执行动作规划。多摄像头设置引入了大量冗余和无关信息，增加了计算成本，并迫使模型花费额外的训练时间来提取关键的任务相关细节。为了过滤冗余信息并准确提取任务相关特征，我们提出了VERM（用于机器人操作的虚拟眼睛）方法，利用基础模型中的知识，从构建的3D点云中想象一个虚拟任务自适应视图，有效捕获必要信息并减轻遮挡。为了促进3D动作规划和精细操作，我们进一步设计了一个深度感知模块和一个动态粗到细的过程。在模拟基准RLBench和现实世界评估中的大量实验结果证明了我们方法的有效性，超越了之前的先进方法，同时实现了训练时间1.89倍加速和推理速度1.54倍加速。更多结果可以在我们的项目网站https://verm-ral.github.io上找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的问题是机器人在执行3D操作任务时，多摄像头系统引入大量冗余和不相关信息，增加了计算成本，并迫使模型花费额外训练时间提取关键任务相关信息。这个问题很重要，因为它限制了机器人系统的效率和实时性，在实际应用中计算资源有限的环境下尤为重要，同时现有方法要么处理整个3D表示（计算量大），要么依赖专家知识选择视角（缺乏灵活性）。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类视觉注意机制的启发，思考如何让机器人像人类一样只需一眼和想象就能确定最佳视角。他们借鉴了认知科学对人类视觉选择的研究，并利用GPT-4o等基础模型的空间推理能力。该方法借鉴了现有工作中的多视图融合（如C2F-ARM、Peract）、虚拟相机平面投影（如RVT系列）和粗到细策略（如C2F-ARM、RVT-2），但创新性地将基础模型用于自动选择任务自适应视角，而非依赖预定义视角或专家知识。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用基础模型创建一个'虚拟眼睛'，从3D点云中生成任务自适应的虚拟视角，高效捕获必要信息并减少遮挡。整体流程包括：1)使用结构化提示引导GPT-4o预测最佳相机姿态；2)将多摄像头RGB-D输入转换为统一3D点云并投影到虚拟相机平面上；3)通过深度感知模块将2D动作预测扩展到3D空间；4)采用动态粗到细过程，在任务关键阶段自动触发视角放大以细化动作，只在必要时进行高精度计算。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)结构化提示框架将基础模型转化为空间推理代理；2)验证了方法在不同基础模型上的通用性；3)深度感知模块支持3D动作规划；4)动态粗到细机制只在必要时细化动作。相比之前工作，VERM不依赖预定义视角或专家知识，而是自动生成任务自适应视角；简化输入为单图像而非处理整个3D表示或多个视图；显著提升效率（训练加速1.89倍，推理加速1.54倍）；揭示并利用了大型多模态模型的3D空间推理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VERM通过利用基础模型创建虚拟眼睛，为机器人3D操作提供了高效、自适应的视觉感知方法，显著提升了操作效率并减少了计算负担。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .</description>
      <author>example@mail.com (Yixiang Chen, Yan Huang, Keji He, Peiyan Li, Liang Wang)</author>
      <guid isPermaLink="false">2512.16724v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Fully Dynamic Algorithms for Chamfer Distance</title>
      <link>http://arxiv.org/abs/2512.16639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究动态环境下计算Chamfer距离的问题，提出首个在l_p范数下维护Chamfer距离近似的动态算法，算法简化为近似最近邻搜索，具有高效更新时间&lt;h4&gt;背景&lt;/h4&gt;Chamfer距离是点云之间广泛使用的不相似性度量，有多个实际应用，特别是在机器学习中作为损失函数时需要重复评估动态变化的数据集&lt;h4&gt;目的&lt;/h4&gt;高效维护两个动态变化点集之间的Chamfer距离近似值，支持点插入和删除操作&lt;h4&gt;方法&lt;/h4&gt;提出一种动态算法，将Chamfer距离计算简化为近似最近邻搜索，使用标准ANN界限来保证近似性能&lt;h4&gt;主要发现&lt;/h4&gt;算法能够实现(1+ε)-近似，更新时间为Õ(ε^{-d})；或实现O(1/ε)-近似，更新时间为Õ(d n^{ε^2} ε^{-4})&lt;h4&gt;结论&lt;/h4&gt;该方法在真实数据集上评估表现良好，与自然基线相比具有竞争力&lt;h4&gt;翻译&lt;/h4&gt;我们研究在完全动态环境下计算Chamfer距离的问题，其中两个点集A和B（每个大小最多为n）通过点的插入或删除动态变化，目标是在高效维护对dist_CH(A,B)的近似，其中dist是一种距离度量。Chamfer距离是点云之间广泛使用的不相似性度量，有许多实际应用需要重复评估动态变化的数据集，例如在机器学习中用作损失函数时。本文首次提出了在l_p范数下（p∈{1,2}）维护Chamfer距离近似的动态算法。我们的算法简化为近似最近邻搜索，开销很小。代入标准ANN界限，我们在Õ(ε^{-d})更新时间内获得(1+ε)-近似，在Õ(d n^{ε^2} ε^{-4})更新时间内获得O(1/ε)-近似。我们在真实数据集上评估了我们的方法，并证明其与自然基线相比具有竞争力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在动态变化环境下高效计算两个点集之间的Chamfer距离问题。当点集通过插入或删除操作动态变化时，如何维持对Chamfer距离的近似值。这个问题在现实中非常重要，因为Chamfer距离是点云分析中常用的不相似性度量，广泛应用于机器学习（如作为损失函数）、计算机视觉（如3D点云重建）和医学成像（如解剖结构跟踪）等领域，这些应用都需要在动态变化的数据集上重复计算Chamfer距离。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到Chamfer距离计算可以转化为最近邻搜索问题，这是算法设计的基础。他们借鉴了静态算法中的重要性采样框架，特别是[BIJ+23]的工作，但在动态环境中显式维护近似分配变得困难且成本高。因此，作者设计了一个动态采样器，通过隐式表示距离估计来获得采样保证，避免了显式维护每个点的匹配单元。他们使用随机平移四叉树结构来隐式维护距离估计，并为每个节点维护动态采样器，允许根据单元大小和匹配点数量比例高效采样点。这种设计解决了动态环境中维护匹配单元的高成本问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将Chamfer距离计算转化为最近邻搜索问题，使用动态最近邻oracle作为子程序，并通过重要性采样框架结合隐式距离表示来避免显式维护近似分配。整体实现流程包括：1) 维护动态四叉树结构，每个节点存储单元中A点和B点的数量；2) 为每个节点维护动态采样器；3) 处理更新时沿四叉树路径更新相关节点的匹配点计数；4) 回答查询时使用重要性采样从A集中采样点，查询近似最近邻oracle估计距离，计算加权平均值作为Chamfer距离估计；5) 通过多次采样和平均提高估计准确性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个动态Chamfer距离算法，适用于ℓp范数(p∈{1,2})；2) 显著优于线性时间更新障碍的效率，低维达˜O(ε⁻ᵈ)，高维达˜O(dnε²ε⁻⁴)；3) 通过隐式表示距离估计而非显式维护近似分配，解决动态环境中的高成本问题；4) 基于随机平移四叉树的动态采样器设计；5) 与现有最近邻oracle的集成。相比之前工作，不同之处在于：之前工作主要关注静态设置，而本文首次解决动态环境下的维护问题；时间复杂度显著优于线性时间障碍；将问题转化为最近邻搜索；适用于任意维度；并在真实数据集上进行了实验验证。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提出了在动态变化点集上高效维护Chamfer距离的算法，通过问题转化和创新的动态采样框架，显著优于线性时间更新障碍，并在实际应用中表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the problem of computing Chamfer distance in the fully dynamic setting, where two set of points $A, B \subset \mathbb{R}^{d}$, each of size up to $n$, dynamically evolve through point insertions or deletions and the goal is to efficiently maintain an approximation to $\mathrm{dist}_{\mathrm{CH}}(A,B) = \sum_{a \in A} \min_{b \in B} \textrm{dist}(a,b)$, where $\textrm{dist}$ is a distance measure. Chamfer distance is a widely used dissimilarity metric for point clouds, with many practical applications that require repeated evaluation on dynamically changing datasets, e.g., when used as a loss function in machine learning. In this paper, we present the first dynamic algorithm for maintaining an approximation of the Chamfer distance under the $\ell_p$ norm for $p \in \{1,2 \}$. Our algorithm reduces to approximate nearest neighbor (ANN) search with little overhead. Plugging in standard ANN bounds, we obtain $(1+ε)$-approximation in $\tilde{O}(ε^{-d})$ update time and $O(1/ε)$-approximation in $\tilde{O}(d n^{ε^2} ε^{-4})$ update time. We evaluate our method on real-world datasets and demonstrate that it performs competitively against natural baselines.</description>
      <author>example@mail.com (Gramoz Goranci, Shaofeng Jiang, Peter Kiss, Eva Szilagyi, Qiaoyuan Yang)</author>
      <guid isPermaLink="false">2512.16639v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</title>
      <link>http://arxiv.org/abs/2512.16461v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SNOW是一个无需训练且与主干无关的框架，用于统一的4D场景理解，整合了视觉语言模型衍生的语义与点云几何和时间一致性，通过处理同步RGB图像和3D点云，生成对象级提议，进行分割，并通过时空标记化块编码产生多模态标记，最终形成4D场景图作为下游推理的先验。&lt;h4&gt;背景&lt;/h4&gt;自主机器人系统需要对动态环境进行时空理解以确保可靠导航和交互。视觉语言模型提供开放世界语义先验但缺乏3D几何和时间动态基础，而几何感知捕捉结构和运动但语义稀疏。&lt;h4&gt;目的&lt;/h4&gt;提出SNOW框架，实现统一的4D场景理解，整合VLM衍生的语义与点云几何和时间一致性。&lt;h4&gt;方法&lt;/h4&gt;SNOW处理同步RGB图像和3D点云，使用HDBSCAN聚类生成对象级提议引导SAM2分割，通过STEP编码产生捕获语义、几何和时间属性的多模态标记，增量整合到4D场景图中，轻量级SLAM后端提供空间锚定和全局参考对齐。&lt;h4&gt;主要发现&lt;/h4&gt;SNOW在多样化基准测试上实现精确的4D场景理解和空间基础推理，在多个设置中达到最先进性能，证明结构化4D先验对具身推理和自主机器人的重要性。&lt;h4&gt;结论&lt;/h4&gt;SNOW成功整合VLM语义与几何感知，为自主机器人提供统一的4D场景理解能力，使视觉语言模型能够直接解释空间场景结构和时间动态。&lt;h4&gt;翻译&lt;/h4&gt;自主机器人系统需要对动态环境进行时空理解，以确保可靠的导航和交互。虽然视觉语言模型提供开放世界语义先验，但它们缺乏3D几何和时间动态的基础。相反，几何感知捕捉结构和运动，但仍然语义稀疏。我们提出了SNOW(使用开放世界知识的场景理解)，一个无需训练且与主干无关的框架，用于统一的4D场景理解，将VLM衍生的语义与点云几何和时间一致性相结合。SNOW处理同步的RGB图像和3D点云，使用HDBSCAN聚类生成对象级提议，引导基于SAM2的分割。每个分割区域通过我们提出的时空标记化块编码(STEP)进行编码，产生捕获局部语义、几何和时间属性的多模态标记。这些标记被增量整合到4D场景图(4DSG)中，作为下游推理的4D先验。轻量级SLAM后端将所有STEP标记在环境中空间锚定，提供全局参考对齐，并确保跨越时间的明确空间基础。 resulting 4DSG形成了一个可查询的统一世界模型，通过VLMs可以直接解释空间场景结构和时间动态。在多样化的基准测试上的实验表明，SNOW能够实现精确的4D场景理解和空间基础推理，从而在多个设置中设置了新的最先进性能，突显了结构化4D先验对于具身推理和自主机器人的重要性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自主机器人在动态环境中进行时空理解的问题，即如何将视觉语言模型的开放世界语义知识与精确的3D几何和时间动态信息相结合。这个问题在现实中非常重要，因为自主机器人需要在非结构化、动态环境中导航和交互，不仅要识别物体，还要理解它们在3D空间中的位置以及如何随时间变化，而这正是现有方法的不足之处。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的三个主要限制（依赖大量训练、特定主干架构、缺乏显式几何）来设计方法。他们借鉴了HDBSCAN聚类进行点云分组、SAM2进行分割、SLAM后端维持空间对齐等现有技术，但创新性地将这些技术组合成一个无需训练且与主干无关的框架。作者特别强调了解决VLMs缺乏几何定位和几何感知缺乏语义表达的问题，通过统一4D表示连接语义抽象与空间时间定位。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的4D场景表示，将视觉语言模型的语义知识与3D几何和时间动态信息融合。整体流程包括：1)点云聚类和采样，使用HDBSCAN识别高密度区域；2)掩码生成和STEP编码，将分割区域编码为包含语义、几何和时间信息的多模态标记；3)4D场景图构建，通过滑动窗口聚合时空信息；4)使用VLM在统一场景图上进行推理。整个流程通过SLAM后端确保空间对齐，形成持久的4D世界模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SNOW框架，无需训练即可融合VLM语义与3D感知；2)STEP编码，联合编码语义、几何和时间信息；3)4D场景图，提供结构化时空表示；4)在多个基准上实现最先进性能。相比之前工作，SNOW无需训练和微调，支持多种传感器模态，显式建模几何和时间信息，且能提供长期一致的场景表示，而不仅仅是2D图像处理或短时推理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SNOW通过创新的STEP编码和4D场景图构建方法，成功将视觉语言模型的开放世界语义知识与精确的3D几何和时间动态信息相融合，为自主机器人在复杂环境中的时空理解和推理提供了无需训练且通用的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.</description>
      <author>example@mail.com (Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax)</author>
      <guid isPermaLink="false">2512.16461v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced 3D Shape Analysis via Information Geometry</title>
      <link>http://arxiv.org/abs/2512.16213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于信息几何框架的三维点云形状分析方法，通过将点云表示为统计流形上的高斯混合模型，并引入具有理论保证边界的修改的对称Kullback-Leibler散度，解决了传统点云比较方法中的局限性。&lt;h4&gt;背景&lt;/h4&gt;三维点云为物体提供了高度精确的数字表示，在计算机图形学、摄影测量、计算机视觉和机器人学等领域有重要应用。然而，由于点云的非结构特性和所表示表面的复杂几何形状，比较点云面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;引入一个信息几何框架用于三维点云形状分析，通过将点云表示为统计流形上的高斯混合模型，并提供一种稳定的散度度量方法。&lt;h4&gt;方法&lt;/h4&gt;证明了高斯混合模型空间形成一个统计流形，并提出了修改的对称Kullback-Leibler(MSKL)散度，具有理论上保证的上界和下界，确保所有GMM比较的数值稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在人体姿态辨别(MPI-FAUST数据集)和动物形状比较(G-PCD数据集)的综合实验，证明MSKL提供了稳定且单调变化的值，直接反映几何变化，优于传统距离和现有的KL近似。&lt;h4&gt;结论&lt;/h4&gt;MSKL散度在点云比较中表现优异，解决了传统方法无法捕捉全局统计结构、对异常值敏感以及数值不稳定等问题。&lt;h4&gt;翻译&lt;/h4&gt;三维点云为物体提供了高度精确的数字表示，对于计算机图形学、摄影测量、计算机视觉和机器人学中的应用至关重要。然而，由于点云的非结构性质和它们所表示表面的复杂几何形状，比较点云面临重大挑战。传统的几何度量，如Hausdorff距离和Chamfer距离，通常无法捕捉全局统计结构，并且对异常值敏感，而现有的高斯混合模型的Kullback-Leibler(KL)散度近似可能产生无界或数值不稳定的结果。本文通过将点云表示为统计流形上的高斯混合模型(GMMs)，引入了一种用于三维点云形状分析的信息几何框架。我们证明了GMM空间形成一个统计流形，并提出了具有理论上保证的上界和下界的修改的对称Kullback-Leibler(MSKL)散度，确保所有GMM比较的数值稳定性。通过在人体姿态辨别(MPI-FAUST数据集)和动物形状比较(G-PCD数据集)的综合实验，我们证明MSKL提供了稳定且单调变化的值，直接反映几何变化，优于传统距离和现有的KL近似。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决三维点云比较的挑战问题。点云是物体高度准确的数字表示，广泛应用于计算机图形学、摄影测量、计算机视觉和机器人等领域。然而，由于点云的非结构化特性和表面复杂几何形状，比较点云面临重大挑战。传统几何度量方法无法捕捉全局统计结构且对异常值敏感，而现有的高斯混合模型KL散度近似方法可能产生无界或数值不稳定的结果。解决这个问题对于形状分析任务如姿态估计、变形跟踪、配准和检索至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于信息几何框架设计方法，将点云表示为统计流形上的高斯混合模型(GMM)。首先分析了现有方法的局限性，然后建立了点云作为GMM的统计流形表示，证明了GMM空间形成统计流形。接着提出修改的对称KL散度(MSKL)解决现有KL近似的不稳定性问题。最后设计了完整的计算流程。作者借鉴了信息几何理论、GMM用于点云表示、Isomap流形学习方法以及现有的KL散度近似方法(如KLWA和KLMB)。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将三维点云表示为统计流形上的概率分布(特别是高斯混合模型)，利用信息几何中的散度度量来比较这些分布，从而捕捉点云的全局统计结构而非仅关注点对点距离。整体流程包括：1)数据预处理(最远点采样下采样和归一化)；2)几何特征提取(计算局部协方差矩阵和11个几何特征)；3)流形嵌入(Isomap将14维描述符嵌入二维潜在空间)；4)统计流形建模(BIC确定混合组件数，EM算法拟合GMM)；5)散度计算(在规则网格上评估MSKL等散度度量)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)建立点云作为GMM的统计流形表示框架；2)提出具有理论保证边界的MSKL散度，解决数值稳定性问题；3)推导MSKL散度的可计算上界和下界，确保理论适定性；4)在多个数据集上全面验证方法有效性。相比传统几何距离(如Hausdorff和Chamfer)仅捕捉点对点距离，MSKL能捕捉全局统计结构；相比现有KL近似方法可能低估或高估真实散度且可能无界，MSKL具有理论保证的边界，提供数值稳定性并更好反映几何变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过将点云表示为统计流形上的高斯混合模型并引入具有理论保证边界的修改对称KL散度，为3D点云形状分析提供了一种稳定、可解释且高效的比较方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.</description>
      <author>example@mail.com (Amit Vishwakarma, K. S. Subrahamanian Moosath)</author>
      <guid isPermaLink="false">2512.16213v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications</title>
      <link>http://arxiv.org/abs/2512.15823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种解决沉浸式格式(360°和6DoF点云视频)在实时AR/VR流媒体中带宽和延迟问题的系统，通过降采样、部分加密和机器学习超分辨率技术实现。&lt;h4&gt;背景&lt;/h4&gt;沉浸式格式如360°和6DoF点云视频需要高带宽和低延迟，这对实时AR/VR流媒体构成挑战，带宽消耗和加解密延迟是总体延迟的主要因素。&lt;h4&gt;目的&lt;/h4&gt;减少带宽消耗和加解密延迟，这两个导致总体延迟的关键因素，以改善沉浸式内容的实时流媒体体验。&lt;h4&gt;方法&lt;/h4&gt;设计一个系统，在源服务器端对点云内容进行降采样并应用部分加密，在客户端使用基于机器学习的超分辨率模型进行解密和放大。&lt;h4&gt;主要发现&lt;/h4&gt;评估显示，随着降采样分辨率的降低，带宽/延迟和加解密开销呈近似线性减少，超分辨率模型能以最小误差和适度推理时间有效重建原始全分辨率点云。&lt;h4&gt;结论&lt;/h4&gt;结合降采样、部分加密和基于机器学习的超分辨率技术可有效解决沉浸式内容流媒体中的带宽和延迟问题，同时保持内容质量。&lt;h4&gt;翻译&lt;/h4&gt;沉浸式格式如360°和6DoF点云视频需要高带宽和低延迟，这对实时AR/VR流媒体构成了挑战。这项工作专注于减少带宽消耗和加解密延迟，这是总体延迟的两个主要贡献因素。我们设计了一个系统，在源服务器端对点云内容进行降采样并应用部分加密。在客户端，内容被解密并使用基于机器学习的超分辨率模型进行放大。我们的评估表明，随着降采样分辨率的降低，带宽/延迟和加解密开销呈近似线性减少，同时超分辨率模型能够以最小误差和适度的推理时间有效重建原始全分辨率点云。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决沉浸式媒体格式（如360°视频和6DoF点云视频）的高带宽需求和低延迟挑战，这对实时AR/VR流媒体传输至关重要。这个问题在现实中非常重要，因为高延迟会导致用户运动 sickness 并降低沉浸感，而高带宽需求则可能导致网络拥塞和数据包丢失，进一步增加延迟。随着Apple Vision Pro和Meta Quest 3等设备的普及，用户能够以高质量消费这些内容，因此需要有效的解决方案来处理这些高要求格式。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到沉浸式媒体格式的带宽和延迟挑战，然后提出结合超分辨率和属性基加密(ABE)的新方法。他们设计了一个系统，在源服务器对点云内容进行下采样并应用部分加密，在客户端进行解密和基于机器学习的上采样。作者借鉴了多项现有工作，包括点云超分辨率研究（如PU-Net）、属性基加密技术、以及他们之前在视频流中使用ABE的研究，特别是针对点云数据的选择性坐标加密方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云下采样与基于机器学习的超分辨率和属性基加密相结合，以减少带宽使用和加密/解密开销，同时通过训练AI/ML模型从下采样数据重建全分辨率点云。整体流程包括：1)在源服务器处对点云进行下采样（50%、25%、12.5%）；2)使用ABE对下采样内容进行部分加密；3)通过CDN分发加密内容；4)客户端获取并解密内容；5)使用基于AI/ML的超分辨率模型进行上采样；6)渲染重建的全分辨率点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将安全性集成到点云超分辨率管道中；2)结合ABE与点云下采样和ML超分辨率；3)开发高效AI/ML模型重建全分辨率点云；4)实验证明近线性减少网络延迟、带宽和加密时间。相比之前的工作，本研究首次将安全性与点云超分辨率结合，而之前的研究主要关注带宽优化而忽略安全性；同时，虽然之前有ABE在视频流中的应用，但本研究将其扩展到点云数据并结合了下采样和超分辨率技术。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本研究通过结合点云下采样、基于机器学习的超分辨率与属性基加密，为实时混合现实应用提供了一种安全高效的解决方案，显著降低了带宽需求和加密开销，同时以最小误差重建原始点云质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Immersive formats such as 360° and 6DoF point cloud videos require high bandwidth and low latency, posing challenges for real-time AR/VR streaming. This work focuses on reducing bandwidth consumption and encryption/decryption delay, two key contributors to overall latency. We design a system that downsamples point cloud content at the origin server and applies partial encryption. At the client, the content is decrypted and upscaled using an ML-based super-resolution model. Our evaluation demonstrates a nearly linear reduction in bandwidth/latency, and encryption/decryption overhead with lower downsampling resolutions, while the super-resolution model effectively reconstructs the original full-resolution point clouds with minimal error and modest inference time.</description>
      <author>example@mail.com (Mohammad Waquas Usmani, Sankalpa Timilsina, Michael Zink, Susmit Shannigrahi)</author>
      <guid isPermaLink="false">2512.15823v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title>
      <link>http://arxiv.org/abs/2512.13107v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DiffFusion框架，通过基于扩散的恢复和自适应跨模态融合，增强多模态3D目标检测在恶劣天气条件下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;多模态3D目标检测对机器人和自动驾驶中的可靠感知至关重要，但在恶劣天气条件下，其有效性受到天气引起的失真和不同数据模态间错位的限制。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够增强在挑战性天气条件下鲁棒性的多模态3D目标检测框架。&lt;h4&gt;方法&lt;/h4&gt;DiffFusion框架利用扩散模型强大的去噪和数据生成能力，引入Diffusion-IR恢复因天气影响退化的图像，使用点云恢复(PCR)利用图像对象线索补偿损坏的LiDAR数据，并开发双向自适应融合和对齐模块(BAFAM)处理模态间错位，实现动态多模态融合和双向鸟瞰图对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的广泛实验表明，DiffFusion在恶劣天气条件下实现了最先进的鲁棒性，同时保持了强大的干净数据性能；在真实世界DENSE数据集上的零样本结果进一步验证了其泛化能力。&lt;h4&gt;结论&lt;/h4&gt;DiffFusion框架有效解决了恶劣天气条件下的多模态3D目标检测挑战，实现将作为开源发布。&lt;h4&gt;翻译&lt;/h4&gt;多模态3D目标检测对机器人和自动驾驶中的可靠感知很重要。然而，由于天气引起的失真和不同数据模态之间的错位，其在恶劣天气条件下的有效性仍然有限。在这项工作中，我们提出了DiffFusion，一个通过基于扩散的恢复和自适应跨模态融合来增强在挑战性天气条件下鲁棒性的新框架。我们的关键见解是扩散模型具有强大的去噪和生成数据的能力，可以适应各种天气条件。基于此，DiffFusion引入了Diffusion-IR来恢复因天气影响而退化的图像，并使用点云恢复(PCR)利用图像对象线索补偿损坏的LiDAR数据。为了解决两种模态之间的错位问题，我们开发了双向自适应融合和对齐模块(BAFAM)。它实现了动态多模态融合和双向鸟瞰图(BEV)对齐，以保持一致的空间对应关系。在三个公共数据集上的广泛实验表明，DiffFusion在恶劣天气条件下实现了最先进的鲁棒性，同时保持了强大的干净数据性能。在真实世界DENSE数据集上的零样本结果进一步验证了其泛化能力。我们的DiffFusion实现将作为开源发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态3D目标检测在恶劣天气条件下性能下降的问题，特别是图像和LiDAR传感器数据因天气影响产生的失真和模态间错位。这个问题在自动驾驶和机器人领域非常重要，因为这些系统需要在各种天气条件下可靠运行，而现有方法在雨、雾、强光等条件下性能显著下降，可能导致安全隐患。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到多模态3D检测在恶劣天气下效果不佳，分析现有方法主要针对干净数据优化，缺乏处理天气失真的机制。他们发现扩散模型在去噪和生成数据方面有强大能力，可以适应各种天气条件。方法设计借鉴了扩散模型、特征金字塔网络、CenterNet等现有技术，但创新性地将它们整合到一个专门处理恶劣天气的统一框架中，并设计了新的双向自适应融合和对齐模块来解决模态间错位问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用扩散模型恢复恶劣天气下的传感器数据，并通过图像检测结果指导点云修复，实现跨模态信息互补，同时设计双向融合机制解决模态间错位。整体流程分为三部分：1)扩散基础恢复模块，包含图像分支(Diffusion-IR)和点云分支(PCR)；2)双向自适应融合和对齐模块(BAFAM)，包含交叉注意力自适应融合和双向BEV对齐；3)将融合后的特征输入3D检测头进行目标检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DiffFusion统一框架系统地解决天气引起的传感器退化；2)双分支恢复结构同时处理图像和点云恢复；3)BAFAM模块实现动态多模态融合和双向BEV对齐。相比之前工作，不同之处在于：传统方法通常针对单一天气条件训练，而DiffFusion可处理多种天气；现有方法缺乏处理模态间错位的机制；大多数方法只关注单一模态恢复，而DiffFusion同时处理两种模态；扩散模型在3D目标检测中的应用是新颖的，特别是与多模态融合的结合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DiffFusion通过基于扩散模型的多模态恢复和自适应融合机制，显著提升了3D目标检测在恶劣天气条件下的鲁棒性，同时保持了在正常天气条件下的高性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.</description>
      <author>example@mail.com (Zhijian He, Feifei Liu, Yuwei Li, Zhanpeng Luo, Jintao Cheng, Xieyuanli Chen, Xiaoyu Tang)</author>
      <guid isPermaLink="false">2512.13107v2</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm</title>
      <link>http://arxiv.org/abs/2512.16694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究旨在自动化圣训主题分组，使用无监督学习方法和Apriori算法识别未标记文本数据中的关联模式和语义关系。&lt;h4&gt;背景&lt;/h4&gt;随着伊斯兰文本数字化进程加速，自动化圣训主题分组的需求日益紧迫。&lt;h4&gt;目的&lt;/h4&gt;自动化圣训主题分组，促进伊斯兰文本数字化处理。&lt;h4&gt;方法&lt;/h4&gt;使用布哈里圣训印尼语翻译数据集，经过大小写转换、标点清理、分词、停用词移除和词干提取等预处理，然后应用Apriori算法进行关联规则挖掘分析，使用支持度、置信度和提升度参数。&lt;h4&gt;主要发现&lt;/h4&gt;发现了拜次数-祈祷、经文-启示和圣训-故事等有意义的关联模式，分别描述了崇拜、启示和圣训叙述的主题。&lt;h4&gt;结论&lt;/h4&gt;Apriori算法能自动发现潜在语义关系，为数字伊斯兰研究和基于技术的学习系统发展做出贡献。&lt;h4&gt;翻译&lt;/h4&gt;这项研究源于自动化圣训主题分组的紧迫性，以符合伊斯兰文本日益数字化的发展。基于文献综述，使用Apriori算法的无监督学习方法已被证明在识别未标记文本数据中的关联模式和语义关系方面有效。使用的数据集是布哈里圣训的印尼语翻译，首先经过包括大小写转换、标点符号清理、分词、停用词移除和词干提取的预处理阶段。接下来，使用具有支持度、置信度和提升度参数的Apriori算法进行了关联规则挖掘分析。结果表明存在有意义的关联模式，如拜次数-祈祷、经文-启示和圣训-故事之间的关系，这些关系描述了崇拜、启示和圣训叙述的主题。这些发现表明，Apriori算法能够自动发现潜在的语义关系，同时为数字伊斯兰研究和基于技术的学习系统的发展做出贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.</description>
      <author>example@mail.com (Wisnu Uriawan, Achmad Ajie Priyajie, Angga Gustian, Fikri Nur Hidayat, Sendi Ahmad Rafiudin, Muhamad Fikri Zaelani)</author>
      <guid isPermaLink="false">2512.16694v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization</title>
      <link>http://arxiv.org/abs/2512.16504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于骨架的时间动作定位的自监督学习方法，通过片段判别预任务和特征融合技术提高了动作边界检测的准确性。&lt;h4&gt;背景&lt;/h4&gt;自监督预训练范式在基于骨架的动作识别中已取得成功，但在时间动作定位方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发能够捕捉时间敏感特征的有效表示学习方法，用于检测动作边界和定位动作片段。&lt;h4&gt;方法&lt;/h4&gt;提出片段判别预任务，将骨架序列投影到不重叠片段并通过对比学习区分；使用U形模块融合中间特征以增强帧级定位的特征分辨率。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在BABEL数据集上持续改进了现有方法；在PKUMMD上实现了最先进的迁移学习性能。&lt;h4&gt;结论&lt;/h4&gt;自监督预训练结合时间敏感的特征学习可以有效解决基于骨架的时间动作定位问题。&lt;h4&gt;翻译&lt;/h4&gt;自监督预训练范式在使用对比学习学习基于骨架的动作识别的3D动作表示方面取得了巨大成功。然而，学习用于基于骨架的时间动作定位的有效表示仍然具有挑战性且研究不足。与视频级动作识别不同，检测动作边界需要时间敏感的特征，能够捕捉标签变化处相邻帧之间的细微差别。为此，我们提出了一种用于自监督预训练的片段判别预任务，该方法将骨架序列密集投影到不重叠的片段中，并通过对比学习促进跨视频区分这些片段的特征。此外，我们通过使用U形模块融合中间特征，构建了基于骨架的动作识别模型的强大骨干网络，以增强帧级定位的特征分辨率。我们的方法在BABEL数据集上，针对不同的子集和评估协议，持续改进了现有的基于骨架的对比学习方法用于动作定位。在NTU RGB+D和BABEL上进行预训练后，我们在PKUMMD上实现了最先进的迁移学习性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于骨架的自监督表示学习在动作定位（action localization）任务中的挑战。这个问题很重要，因为获得帧级别的动作标签比视频级别标签更昂贵、耗时，而动作定位不仅是分类动作，还需要检测视频中的时间边界，这对医疗保健（如帕金森病步态分析）和监控等实际应用至关重要。现有骨架对比学习方法通过全局平均池化产生序列级表示，导致学习到的表示对帧级变化不敏感，而这对于需要时间敏感特征的定位任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有骨架对比学习方法的局限性，即它们主要关注视频级别动作识别，使用全局投影导致学习到的表示对帧级变化不敏感。作者从图像密集预测任务的对比学习框架中汲取灵感，设计了片段判别代理任务来促进时间敏感特征学习。具体设计包括密集片段对比学习和多尺度特征融合。作者借鉴了MoCo-based视频级别骨架对比学习框架、RGB研究的密集对比学习（DenseCL）以及U-Net和特征金字塔网络的设计。方法采用两阶段范式：第一阶段在自监督对比预训练中集成密集片段级对比目标；第二阶段在微调阶段将U形模块插入预训练编码器以融合多尺度特征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1）片段级对比学习，将骨架序列细分为多个片段并在片段级别进行对比学习，使模型能够学习更细粒度的时间敏感特征；2）多尺度特征融合，使用U形模块融合不同尺度特征，恢复时间分辨率并保留语义信息和细粒度细节。整体流程分为两阶段：第一阶段是密集片段对比预训练，输入骨架序列后通过不同增强获得锚点-正样本对，使用编码器获取骨架编码，同时应用全局投影和密集投影模块构建视频级和片段级嵌入，通过基于相似性的匹配确定片段对应关系，计算视频级和片段级对比损失；第二阶段是多尺度特征融合微调，保留预训练编码器作为骨干，插入U形模块通过跳跃连接融合不同尺度特征并逐步上采样，最后添加分类器生成帧级预测并进行后处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）片段级判别代理任务，首次在未修剪、无标签骨架序列上学习时间敏感表示；2）密集片段对比学习，将骨架序列密集投影到非重叠片段并通过对比学习促进跨视频区分；3）多尺度特征融合的U形模块，增强现有骨干网络的特征分辨率；4）两阶段训练范式，专门针对动作定位任务。相比之前工作的不同：1）对比粒度从视频级别提升到片段级别，提高时间敏感性；2）应用任务从动作识别转向更具挑战性的动作定位；3）能从未修剪、无标签骨架序列中学习；4）引入U形模块进行多尺度特征融合；5）使用基于相似性的匹配策略确定片段对应关系，而非假设相同时间位置的片段在增强后仍然语义相关。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入片段级对比学习和多尺度特征融合，首次实现了在未修剪骨架序列上学习时间敏感表示，显著提高了骨架动作定位的性能，并减少了对密集帧级标签的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.</description>
      <author>example@mail.com (Qiushuo Cheng, Jingjing Liu, Catherine Morgan, Alan Whone, Majid Mirmehdi)</author>
      <guid isPermaLink="false">2512.16504v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning</title>
      <link>http://arxiv.org/abs/2512.16489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了如何通过迁移学习改进个体治疗效果(ITE)的估计，特别是在小样本情境下，通过利用源数据集知识并将其适应到新环境中。&lt;h4&gt;背景&lt;/h4&gt;将因果知识推广到不同环境具有挑战性，尤其是当需要将大规模数据集的估计应用于较小或系统性不同的情境时。基于机器学习的ITE估计器需要大样本量，限制了其在行为科学等拥有较小数据集领域的应用。&lt;h4&gt;目的&lt;/h4&gt;展示如何利用源数据集知识并通过迁移学习将其适应到新环境中，以改进ITE的估计。&lt;h4&gt;方法&lt;/h4&gt;使用Treatment Agnostic Representation Networks (TARNet)估计ITE，并通过迁移学习改进为TL-TARNet。在模拟中改变源样本量和目标样本量，考虑随机化和非随机化干预设置。在印度人类发展调查(IHDS-II)中实证应用，估计母亲收集柴火时间对孩子学习时间的影响。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习扩展TL-TARNet优于标准TARNet，当存在大的无偏源样本且目标样本量小时，减少了ITE误差并减弱了偏差。在实证应用中，迁移学习将目标平均ITE拉向源ITE估计，减少了无迁移获得的估计中的偏差。&lt;h4&gt;结论&lt;/h4&gt;因果模型的迁移学习可以改进小样本中的ITE估计。&lt;h4&gt;翻译&lt;/h4&gt;将因果知识推广到不同环境具有挑战性，特别是当必须将大规模数据集的估计应用于较小或系统性不同的情境时，外部有效性至关重要。来自机器学习的个体治疗效果(ITE)的基于模型的估计器需要大样本量，限制了其在行为科学等拥有较小数据集的领域的适用性。我们展示了如何通过利用源数据集的知识并通过迁移学习将其适应到新环境中来改进使用Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017)的ITE估计(TL-TARNet; Aloui et al., 2023)。在改变源样本量和目标样本量并考虑随机化和非随机化干预目标设置的模拟中，迁移学习扩展TL-TARNet优于标准TARNet，当存在大的无偏源且目标样本量小时，减少了ITE误差并减弱了偏差。在使用印度人类发展调查(IHDS-II)的实证应用中，我们估计母亲收集柴火时间对孩子每周学习时间的影响；迁移学习将目标平均ITE拉向源ITE估计，减少了无迁移获得的估计中的偏差。这些结果表明，因果模型的迁移学习可以改进小样本中的ITE估计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalizing causal knowledge across diverse environments is challenging, especially when estimates from large-scale datasets must be applied to smaller or systematically different contexts, where external validity is critical. Model-based estimators of individual treatment effects (ITE) from machine learning require large sample sizes, limiting their applicability in domains such as behavioral sciences with smaller datasets. We demonstrate how estimation of ITEs with Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017) can be improved by leveraging knowledge from source datasets and adapting it to new settings via transfer learning (TL-TARNet; Aloui et al., 2023). In simulations that vary source and sample sizes and consider both randomized and non-randomized intervention target settings, the transfer-learning extension TL-TARNet improves upon standard TARNet, reducing ITE error and attenuating bias when a large unbiased source is available and target samples are small. In an empirical application using the India Human Development Survey (IHDS-II), we estimate the effect of mothers' firewood collection time on children's weekly study time; transfer learning pulls the target mean ITEs toward the source ITE estimate, reducing bias in the estimates obtained without transfer. These results suggest that transfer learning for causal models can improve the estimation of ITE in small samples.</description>
      <author>example@mail.com (Seyda Betul Aydin, Holger Brandt)</author>
      <guid isPermaLink="false">2512.16489v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning-based Optimal Control for Colloidal Self-Assembly</title>
      <link>http://arxiv.org/abs/2512.16402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 5 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了机器学习在胶体自组装控制中的应用潜力，通过结合无监督学习和深度强化学习，实现了高精度的胶体自组装控制。&lt;h4&gt;背景&lt;/h4&gt;精确控制胶体自组装成特定图案一直是一个长期存在的挑战，因为过程动力学复杂。&lt;h4&gt;目的&lt;/h4&gt;采用机器学习为基础的最优控制框架，提供一种数据驱动的控制方法，可以推广到其他多体自组装系统。&lt;h4&gt;方法&lt;/h4&gt;采用机器学习为基础的最优控制框架，结合无监督学习和图卷积神经网络进行状态观测，使用基于深度强化学习的最优控制策略计算，并通过布朗动力学模拟进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的基于序参数的状态描述相比表现出优越的性能，在电场介导的系统中获得有序的二维球形胶体自组装，实际成功率达到97%。&lt;h4&gt;结论&lt;/h4&gt;该方法为实现可自动化和可推广的图案化胶体组装提供了有前景的途径。&lt;h4&gt;翻译&lt;/h4&gt;实现胶体自组装成特定图案的精确控制长期以来一直是一个挑战，这源于复杂的过程动力学。最近，基于机器学习的状态表示和基于强化学习的控制策略在该领域开始积累人气，显示出在实现可自动化和可推广的图案化胶体组装方法方面具有巨大潜力。在这项工作中，我们采用了基于机器学习的最优控制框架，结合无监督学习和图卷积神经网络进行状态观测，并使用基于深度强化学习的最优控制策略计算，提供了一种数据驱动的控制方法，该方法可能推广到其他多体自组装系统。通过布朗动力学模拟，我们证明了其与传统基于序参数的状态描述相比具有优越的性能，以及在电场介导系统中获得有序二维球形胶体自组装的有效性，实际成功率达到97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving precise control of colloidal self-assembly into specific patterns remains a longstanding challenge due to the complex process dynamics. Recently, machine learning-based state representation and reinforcement learning-based control strategies have started to accumulate popularity in the field, showing great potential in achieving an automatable and generalizable approach to producing patterned colloidal assembly. In this work, we adopted a machine learning-based optimal control framework, combining unsupervised learning and graph convolutional neural work for state observation with deep reinforcement learning-based optimal control policy calculation, to provide a data-driven control approach that can potentially be generalized to other many-body self-assembly systems. With Brownian dynamics simulations, we demonstrated its superior performance as compared to traditional order parameter-based state description, and its efficacy in obtaining ordered 2-dimensional spherical colloidal self-assembly in an electric field-mediated system with an actual success rate of 97%.</description>
      <author>example@mail.com (Andres Lizano-Villalobos, Fangyuan Ma, Wentao Tang, Wei Sun, Xun Tang)</author>
      <guid isPermaLink="false">2512.16402v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Pretrained Battery Transformer (PBT): A battery life prediction foundation model</title>
      <link>http://arxiv.org/abs/2512.16334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 figures in the main content&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究人员开发了Pretrained Battery Transformer (PBT)，这是首个用于电池寿命预测的基础模型，通过领域知识编码的专家混合层实现，在多个锂离子电池数据集上表现出色。&lt;h4&gt;背景&lt;/h4&gt;电池循环寿命的早期预测对加速电池研究、制造和部署至关重要，但机器学习方法因数据稀缺性和不同老化条件导致的异质性而进展受阻。&lt;h4&gt;目的&lt;/h4&gt;开发一种基础模型来解决电池寿命预测中的数据稀缺性和异质性问题，实现更准确的预测。&lt;h4&gt;方法&lt;/h4&gt;提出Pretrained Battery Transformer (PBT)，通过领域知识编码的专家混合层开发，并在最大的公共电池寿命数据库上验证，从13个锂离子电池数据集中学习可迁移的表示。&lt;h4&gt;主要发现&lt;/h4&gt;PBT比现有模型平均性能提高19.8%，通过迁移学习在涵盖各种操作条件、形成协议和锂离子电池化学成分的15个不同数据集上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作为电池寿命预测建立了基础模型途径，为开发通用电池寿命预测系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;电池循环寿命的早期预测对于加速电池研究、制造和部署至关重要。尽管机器学习方法已显示出令人鼓舞的结果，但由于不同老化条件导致的数据稀缺性和异质性，进展受到阻碍。在其他领域，通过迁移学习在多样化数据集上训练的基础模型已实现了广泛泛化，但尚未有报道用于电池循环寿命预测的基础模型。在此，我们提出了Pretrained Battery Transformer (PBT)，这是首个用于电池寿命预测的基础模型，通过领域知识编码的专家混合层开发。在最大的公共电池寿命数据库上验证，PBT从13个锂离子电池数据集中学习可迁移的表示，比现有模型平均高出19.8%。通过迁移学习，PBT在涵盖各种操作条件、形成协议和锂离子电池化学成分的15个不同数据集上实现了最先进的性能。这项工作为电池寿命预测建立了基础模型途径，为通用电池寿命预测系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.</description>
      <author>example@mail.com (Ruifeng Tan, Weixiang Hong, Jia Li, Jiaqiang Huang, Tong-Yi Zhang)</author>
      <guid isPermaLink="false">2512.16334v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results</title>
      <link>http://arxiv.org/abs/2512.16013v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了FTBSC-KGML框架，一种基于预训练和微调的、具有空间变异性感知能力的、知识引导的机器学习方法，用于准确量化农业生态系统碳循环。&lt;h4&gt;背景&lt;/h4&gt;准确且经济有效地量化决策相关尺度的农业生态系统碳循环对气候缓解和可持续农业至关重要，但迁移学习和空间变异性的利用面临挑战，传统方法依赖位置无关参数化和独立训练，未能充分利用空间异质性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用迁移学习和空间异质性的机器学习框架，以准确量化农业生态系统碳循环，特别是在数据有限的情况下提高局部准确性。&lt;h4&gt;方法&lt;/h4&gt;提出FTBSC-KGML框架，通过预训练-微调过程结合遥感GPP、气候和土壤协变量，采用空间异质性感知的迁移学习方案，在每个州或站点对全局预训练模型进行微调，学习位置感知表示。&lt;h4&gt;主要发现&lt;/h4&gt;FTBSC-KGML比纯全局模型具有更低的验证误差和更大的解释能力一致性，能够更好地捕捉各州之间的空间变异性，在有限数据情况下提高局部准确性而不牺牲可解释性。&lt;h4&gt;结论&lt;/h4&gt;这项工作扩展了先前的SDSA-KGML框架，为农业生态系统碳循环的准确量化提供了新的机器学习方法。&lt;h4&gt;翻译&lt;/h4&gt;准确且经济有效地量化决策相关尺度的农业生态系统碳循环对于气候缓解和可持续农业至关重要。然而，在这个领域中，迁移学习和空间变异性的利用具有挑战性，因为它们涉及异构数据和复杂的跨尺度依赖关系。传统方法通常依赖于位置无关的参数化和独立训练，未能充分利用迁移学习和输入数据的空间异质性，限制了其在具有显著变异性区域的应用。我们提出了FTBSC-KGML（基于微调的站点校准-知识引导机器学习），一种基于预训练和微调的、具有空间变异性感知能力的、知识引导的机器学习框架，通过预训练-微调过程增强KGML-ag，并加入站点特定参数。使用从多个中西部站点收集的遥感GPP、气候和土壤协变量进行预训练-微调过程，FTBSC-KGML估计土地排放，同时利用迁移学习和空间异质性。关键组件是一个空间异质性感知的迁移学习方案，这是一个全局预训练的模型，在每个州或站点进行微调，以学习位置感知的表示，从而在有限数据的情况下提高局部准确性，同时不牺牲可解释性。实证表明，FTBSC-KGML比纯全局模型具有更低的验证误差和更大的解释能力一致性，从而更好地捕捉各州之间的空间变异性。这项工作扩展了先前的SDSA-KGML框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.</description>
      <author>example@mail.com (Ruolei Zeng, Arun Sharma, Shuai An, Mingzhou Yang, Shengya Zhang, Licheng Liu, David Mulla, Shashi Shekhar)</author>
      <guid isPermaLink="false">2512.16013v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>An updated efficient galaxy morphology classification model based on ConvNeXt encoding with UMAP dimensionality reduction</title>
      <link>http://arxiv.org/abs/2512.15137v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in AJ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队在USmorph分类框架中引入了增强的无监督机器学习模块，结合预训练ConvNeXt神经网络和UMAP降维技术，成功优化了星系形态分类，实现了计算效率的大幅提升。&lt;h4&gt;背景&lt;/h4&gt;研究人员之前已开发USmorph分类框架，现需要改进该框架以提高星系形态分类的效率和准确性，以适应未来大规模巡天观测的需求。&lt;h4&gt;目的&lt;/h4&gt;开发一个增强的无监督机器学习模块，用于更高效、准确地分类星系形态，使其适合中国空间站望远镜(CSST)等计划进行的大规模巡天观测。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的ConvNeXt卷积神经网络进行分层特征提取，结合UMAP进行非线性流形学习实现拓扑感知降维；应用于99,806个红移0.2&lt;z&lt;1.2的COSMOS星系I波段图像；将聚类数量从50优化为20；将20个算法识别聚类合并为五种物理形态类型；测试质量大于10^9太阳质量的巨大星系形态参数。&lt;h4&gt;主要发现&lt;/h4&gt;约51%的星系(50,056个)成功分类；分类结果与星系演化理论高度一致；聚类数量减少实现了显著计算节省；改进算法显著提高了星系形态分类效率。&lt;h4&gt;结论&lt;/h4&gt;改进的算法显著提高了星系形态分类效率，使其适合大规模巡天观测，如中国空间站望远镜(CSST)计划进行的观测。&lt;h4&gt;翻译&lt;/h4&gt;我们在之前的USmorph分类框架中提出了一个增强的无监督机器学习模块，包含两个组件：(1)通过使用迁移学习的预训练ConvNeXt卷积神经网络进行分层特征提取，以及(2)使用均匀流形近似和投影进行非线性流形学习，实现拓扑感知的降维。这种双阶段设计能够从大规模视觉数据集高效迁移知识，同时通过UMAP的邻域保持保留形态模式几何结构。我们将升级后的UML应用于99,806个红移0.2&lt;z&lt;1.2(确保静止帧光学形态)且I波段星等小于25的COSMOS星系的I波段图像。预定义的聚类数量被优化为20(从原始框架中的50减少)，实现了显著的计算节省。20个算法识别的聚类被合并为五种物理形态类型。约51%的星系(50,056个)被成功分类。为了评估分类效果，我们测试了质量大于10^9太阳质量的巨大星系的形态参数。我们的分类结果与星系演化理论高度一致。这种改进的算法显著提高了星系形态分类效率，使其适合中国空间站望远镜等计划进行的大规模巡天观测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an enhanced unsupervised machine learning (UML) module within our previous \texttt{USmorph} classification framework featuring two components: (1) hierarchical feature extraction via a pre-trained ConvNeXt convolutional neural network (CNN) with transfer learning, and (2) nonlinear manifold learning using Uniform Manifold Approximation and Projection (UMAP) for topology-aware dimensionality reduction. This dual-stage design enables efficient knowledge transfer from large-scale visual datasets while preserving morphological pattern geometry through UMAP's neighborhood preservation. We apply the upgraded UML on I-band images of 99,806 COSMOS galaxies at redshift $0.2&lt;z&lt;1.2$ (to ensure rest-frame optical morphology) with $I_{\mathrm{mag}}&lt;25$. The predefined cluster number is optimized to 20 (reduced from 50 in the original framework), achieving significant computational savings. The 20 algorithmically identified clusters are merged into five physical morphology types. About 51\% of galaxies (50,056) were successfully classified. To assess classification effectiveness, we tested morphological parameters for massive galaxies with $M_{*}&gt;10^{9}~M_{\odot}$. Our classification results align well with galaxy evolution theory. This improved algorithm significantly enhances galaxy morphology classification efficiency, making it suitable for large-scale sky surveys such as those planned with the China Space Station Telescope (CSST).</description>
      <author>example@mail.com (Guanwen Fang, Shiwei Zhu, Jun Xu, Shiying Lu, Chichun Zhou, Yao Dai, Zesen Lin, Xu Kong)</author>
      <guid isPermaLink="false">2512.15137v2</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Predictions of Process-Induced Deformation in Carbon/Epoxy Composites Using a Deep Operator Network</title>
      <link>http://arxiv.org/abs/2512.13746v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了复合材料制造中的工艺诱导变形问题，通过结合物理模型和深度学习方法，实现了PID的准确预测和优化。&lt;h4&gt;背景&lt;/h4&gt;纤维增强体和聚合物基体对制造条件的响应不同，由于热膨胀系数不匹配和热固性树脂固化过程中的基体收缩，这些异质性在多个长度尺度上产生残余应力，部分释放导致工艺诱导变形(PID)。&lt;h4&gt;目的&lt;/h4&gt;需要准确预测和减轻PID，通过优化的非等温固化周期来实现。&lt;h4&gt;方法&lt;/h4&gt;研究考虑单向AS4碳纤维/胺双功能环氧预浸料，使用双机制框架模拟PID；开发基于深度算子网络(DeepONets)的数据驱动替代模型；扩展到特征线性调制(FiLM) DeepONet；使用迁移学习解决实验数据有限问题；使用集合卡尔曼反演(EKI)量化不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;物理模型和非等温固化周期可预测和减轻PID；DeepONet和FiLM DeepONet能有效预测PID；迁移学习方法可解决实验数据有限问题；EKI可量化不确定性并支持优化固化计划。&lt;h4&gt;结论&lt;/h4&gt;开发了结合物理模型和数据驱动方法的综合框架，能够预测、量化和优化复合材料制造过程中的PID。&lt;h4&gt;翻译&lt;/h4&gt;纤维增强和聚合物基体由于热膨胀系数不匹配和热固性树脂固化过程中的基体收缩，对制造条件的响应不同。这些异质性在多个长度尺度上产生残余应力，其部分释放导致工艺诱导变形(PID)，需要通过优化的非等温固化周期进行准确预测和缓解。本研究考虑单向AS4碳纤维/胺双功能环氧预浸料，并使用考虑热膨胀/收缩和固化收缩的双机制框架模拟PID。该模型经过制造试验验证以确定初始和边界条件，然后用于为多样化的非等温固化周期(时间-温度曲线)生成PID响应。基于这一物理基础，我们开发了基于深度算子网络(DeepONets)的数据驱动替代模型。DeepONet在一个结合高保真模拟和PID目标实验测量的数据集上进行训练。我们将其扩展到特征线性调制(FiLM) DeepONet，其中分支网络特征由包括初始固化程度的外部参数调制，从而能够预测固化程度、粘度和变形的时间历程。由于实验数据仅在有限时间点可用(例如最终变形)，我们使用迁移学习：模拟训练的主干和分支网络保持固定，仅使用测量的最终变形更新最后一层。最后，我们使用集合卡尔曼反演(EKI)增强该框架，量化实验条件下的不确定性，并支持优化复合材料中减少PID的固化计划。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fiber reinforcement and polymer matrix respond differently to manufacturing conditions due to mismatch in coefficient of thermal expansion and matrix shrinkage during curing of thermosets. These heterogeneities generate residual stresses over multiple length scales, whose partial release leads to process-induced deformation (PID), requiring accurate prediction and mitigation via optimized non-isothermal cure cycles. This study considers a unidirectional AS4 carbon fiber/amine bi-functional epoxy prepreg and models PID using a two-mechanism framework that accounts for thermal expansion/shrinkage and cure shrinkage. The model is validated against manufacturing trials to identify initial and boundary conditions, then used to generate PID responses for a diverse set of non-isothermal cure cycles (time-temperature profiles). Building on this physics-based foundation, we develop a data-driven surrogate based on Deep Operator Networks (DeepONets). A DeepONet is trained on a dataset combining high-fidelity simulations with targeted experimental measurements of PID. We extend this to a Feature-wise Linear Modulation (FiLM) DeepONet, where branch-network features are modulated by external parameters, including the initial degree of cure, enabling prediction of time histories of degree of cure, viscosity, and deformation. Because experimental data are available only at limited time instances (for example, final deformation), we use transfer learning: simulation-trained trunk and branch networks are fixed and only the final layer is updated using measured final deformation. Finally, we augment the framework with Ensemble Kalman Inversion (EKI) to quantify uncertainty under experimental conditions and to support optimization of cure schedules for reduced PID in composites.</description>
      <author>example@mail.com (Elham Kiyani, Amit Makarand Deshpande, Madhura Limaye, Zhiwei Gao, Sai Aditya Pradeep, Srikanth Pilla, Gang Li, Zhen Li, George Em Karniadakis)</author>
      <guid isPermaLink="false">2512.13746v2</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>R3ST: A Synthetic 3D Dataset With Realistic Trajectories</title>
      <link>http://arxiv.org/abs/2512.16784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了R3ST（Realistic 3D Synthetic Trajectories）合成数据集，它通过结合合成3D环境和来自真实世界的轨迹，解决了现有合成数据集中车辆运动不够真实的问题，为道路车辆轨迹预测研究提供了准确的地面真实注释和真实的人类驾驶车辆轨迹。&lt;h4&gt;背景&lt;/h4&gt;数据集对于训练和评估用于交通分析和提高道路安全的计算机视觉模型至关重要。现有的真实数据集适合真实场景，捕捉真实的道路物体行为，但通常缺乏精确的地面真实注释。相比之下，合成数据集可以在没有额外成本或时间的情况下标注大量帧，但通常缺乏真实的车辆运动，因为轨迹是使用AI模型或基于规则的系统生成的。&lt;h4&gt;目的&lt;/h4&gt;创建一个合成数据集，克服现有合成数据集缺乏真实车辆运动的局限性，缩小合成数据与真实轨迹之间的差距，推进道路车辆轨迹预测的研究。&lt;h4&gt;方法&lt;/h4&gt;引入R3ST（Realistic 3D Synthetic Trajectories）合成数据集，生成合成3D环境，并整合来自SinD（无人机航拍记录的鸟瞰图数据集）的真实世界轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的数据集成功缩小了合成数据与真实轨迹之间的差距，提供了准确的多模态地面真实注释和真实的人类驾驶车辆轨迹。&lt;h4&gt;结论&lt;/h4&gt;R3ST数据集通过结合合成环境和真实轨迹，解决了现有数据集的局限性，为交通分析和道路安全研究提供了更好的工具。&lt;h4&gt;翻译&lt;/h4&gt;数据集对于训练和评估用于交通分析和提高道路安全的计算机视觉模型至关重要。现有的真实数据集适合真实场景，捕捉真实的道路物体行为，但通常缺乏精确的地面真实注释。相比之下，合成数据集可以在没有额外成本或时间的情况下标注大量帧，扮演着重要角色。然而，合成数据集的一个普遍缺点是缺乏真实的车辆运动，因为轨迹是使用AI模型或基于规则的系统生成的。在这项工作中，我们引入了R3ST（Realistic 3D Synthetic Trajectories），一个合成数据集，它通过生成合成3D环境并整合来自SinD（无人机航拍记录的鸟瞰图数据集）的真实世界轨迹，克服了这一局限性。所提出的数据集缩小了合成数据与真实轨迹之间的差距，推进了道路车辆轨迹预测的研究，同时提供了准确的多模态地面真实注释和真实的人类驾驶车辆轨迹。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-05060-1_30&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.</description>
      <author>example@mail.com (Simone Teglia, Claudia Melis Tonti, Francesco Pro, Leonardo Russo, Andrea Alfarano, Leonardo Pentassuglia, Irene Amerini)</author>
      <guid isPermaLink="false">2512.16784v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models</title>
      <link>http://arxiv.org/abs/2512.15957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CAMP-VLM的视觉语言模型框架，用于从第三人称视角预测多人类行为，该框架通过整合视觉输入的上下文特征和场景图的空间感知来增强人类-场景交互预测能力。&lt;h4&gt;背景&lt;/h4&gt;准确预测人类行为对于在人类环境中运行的移动机器人至关重要。然而，先前的研究主要关注从第一人称视角预测单人场景中的动作，而多个机器人应用需要从第三人称视角理解多个人类行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从第三人称视角预测多人类行为的框架，弥补现有研究的不足，满足多个机器人应用的需求。&lt;h4&gt;方法&lt;/h4&gt;提出CAMP-VLM框架，整合视觉输入的上下文特征和场景图的空间感知。由于缺乏适合的数据集，研究人员使用逼真模拟器生成的合成人类行为数据对模型进行微调，并在合成和真实世界序列上评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;CAMP-VLM在预测准确性上比最佳基线模型提高了高达66.9%，证明了其在多人类行为预测任务上的优越性能和良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CAMP-VLM框架有效地解决了从第三人称视角预测多人类行为的挑战，通过结合视觉语言模型、上下文特征和空间感知，显著提高了预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;准确预测人类行为对于在人类环境中运行的移动机器人至关重要。虽然先前的研究主要关注从第一人称视角预测单人场景中的动作，但多个机器人应用需要从第三人称视角理解多个人类行为。为此，我们提出了CAMP-VLM（上下文感知的多人类行为预测）：一种基于视觉语言模型的框架，它整合了视觉输入的上下文特征和场景图的空间感知，以增强人类-场景交互的预测能力。由于缺乏适合从观察者视角进行多人类行为预测的数据集，我们使用逼真模拟器生成的合成人类行为数据对CAMP-VLM进行微调，并在合成和真实世界序列上评估生成的模型，以评估其泛化能力。利用监督微调（SFT）和直接偏好优化（DPO），CAMP-VLM在预测准确性上比最佳基线模型提高了高达66.9%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.</description>
      <author>example@mail.com (Utsav Panchal, Yuchen Liu, Luigi Palmieri, Ilche Georgievski, Marco Aiello)</author>
      <guid isPermaLink="false">2512.15957v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</title>
      <link>http://arxiv.org/abs/2512.16905v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  project page: https://kxding.github.io/project/Alchemist/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了Al框架，一种基于元梯度的数据选择方法，用于提高文本到图像生成模型的训练效率和效果。&lt;h4&gt;背景&lt;/h4&gt;文本到图像生成模型如Imagen、Stable Diffusion和FLUX虽在视觉质量上取得进步，但其性能受训练数据质量限制。网络爬取和合成数据集常含低质量或冗余样本，导致视觉保真度降低、训练不稳定和计算效率低下。现有方法依赖昂贵的人工筛选或基于单维特征的启发式评分。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动、可扩展的数据选择框架，提高文本到图像模型训练的数据效率，并解决图像模态下的元学习应用问题。&lt;h4&gt;方法&lt;/h4&gt;提出Alchemist框架，包含数据评级和数据修剪两个阶段。训练轻量级评分器基于梯度信息估计样本影响，并增强多粒度感知；使用Shift-G采样策略选择信息丰富的子集进行高效模型训练。&lt;h4&gt;主要发现&lt;/h4&gt;Alchemist是首个用于文本到图像模型训练的自动、可扩展、基于元梯度的数据选择框架。实验表明，使用Alchemist选择的50%数据进行训练可优于使用完整数据集的效果，持续提高视觉质量和下游性能。&lt;h4&gt;结论&lt;/h4&gt;Alchemist通过智能选择数据子集，有效解决了文本到图像生成模型中的数据质量问题，在减少计算资源的同时提高了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;最近的文本到图像生成模型进展，如Imagen、Stable Diffusion和FLUX，已在视觉质量方面取得了显著改进。然而，它们的性能从根本上受限于训练数据的质量。网络爬取和合成图像数据集通常包含低质量或冗余样本，导致视觉保真度降低、训练不稳定和计算效率低下。因此，有效的数据选择对提高数据效率至关重要。现有方法依赖于昂贵的人工筛选或基于文本到图像数据过滤中单维特征的启发式评分。尽管基于元学习的方法已在大型语言模型中得到探索，但尚未有针对图像模态的适应。为此，我们提出了Alchemist，一个基于元梯度的框架，用于从大规模文本-图像数据对中选择合适的子集。我们的方法从数据中心视角迭代优化模型，自动学习评估每个样本的影响。Alchemist包含两个关键阶段：数据评级和数据修剪。我们训练一个轻量级评分器，基于梯度信息估计每个样本的影响，并增强多粒度感知。然后我们使用Shift-G采样策略选择信息丰富的子集以进行高效模型训练。Alchemist是首个用于文本到图像模型训练的自动、可扩展、基于元梯度的数据选择框架。在合成和网络爬取数据集上的实验表明，Alchemist持续提高了视觉质量和下游性能。使用Alchemist选择的50%数据进行训练可以优于使用完整数据集进行训练的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.</description>
      <author>example@mail.com (Kaixin Ding, Yang Zhou, Xi Chen, Miao Yang, Jiarong Ou, Rui Chen, Xin Tao, Hengshuang Zhao)</author>
      <guid isPermaLink="false">2512.16905v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios</title>
      <link>http://arxiv.org/abs/2512.16019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究利用大型语言模型的小样本学习能力，在社交导航任务中改进机器人预测用户对其性能感知的能力，并通过实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;理解人类在人机交互过程中如何评价机器人行为对开发符合人类期望的社交感知机器人至关重要。传统方法是进行用户研究，而最近研究提出使用机器学习替代。然而，现有数据驱动方法需要大量标记数据，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;利用大型语言模型的小样本学习能力，改进机器人预测用户对其性能感知的能力，并在社交导航任务中实验验证这一想法。&lt;h4&gt;方法&lt;/h4&gt;扩展SEAN TOGETHER数据集，增加真实人机导航情境和参与者反馈；评估多种LLMs基于时空线索预测人类对机器人性能感知的能力；进行消融研究确定LLMs依赖的传感器信息类型；探索个性化示例在上下文学习中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs能够匹配或超越传统监督学习模型的性能，同时需要的标记实例数量少一个数量级；随着上下文示例增加，预测性能提高，证实了方法的可扩展性；来自同一被评估用户的个性化示例可以进一步提高预测准确性。&lt;h4&gt;结论&lt;/h4&gt;这项工作为通过用户中心反馈以可扩展方式改进机器人行为铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;理解人类在人机交互过程中如何评价机器人行为对于开发符合人类期望的社交感知机器人至关重要。虽然捕捉这些评价的传统方法是进行用户研究，但最近的工作提出利用机器学习来替代。然而，现有的数据驱动方法需要大量标记数据，这限制了它们在实际中的应用。为了解决这一差距，我们提出利用大型语言模型的小样本学习能力来提高机器人预测用户对其性能感知的能力，并在社交导航任务中实验性地研究了这一想法。为此，我们扩展了SEAN TOGETHER数据集，增加了更多真实世界的人机导航情境和参与者反馈。使用这个增强的数据集，我们评估了多种LLMs基于观察到的机器人和周围人类运动的时空线索，从少量上下文示例中预测人类对机器人性能感知的能力。我们的结果表明，LLMs能够匹配或超越传统监督学习模型的性能，同时需要的标记实例数量少一个数量级。我们进一步证明，预测性能随着更多上下文示例的增加而提高，证实了我们方法的可扩展性。此外，我们通过进行消融研究，研究了LLMs依赖哪种基于传感器的信息来进行这些推断，消融研究针对用于性能预测的输入特征。最后，我们探索了个性化示例在上下文学习中的新颖应用，即从被评估的同一用户中抽取示例，发现它们可以进一步提高预测准确性。这项工作为通过用户中心反馈以可扩展方式改进机器人行为铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.</description>
      <author>example@mail.com (Qiping Zhang, Nathan Tsoi, Mofeed Nagib, Hao-Tien Lewis Chiang, Marynel Vázquez)</author>
      <guid isPermaLink="false">2512.16019v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>DenseBEV: Transforming BEV Grid Cells into 3D Objects</title>
      <link>http://arxiv.org/abs/2512.16818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures, accepted by WACV 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DenseBEV的新方法，通过直接使用鸟瞰图(BEV)特征单元作为锚点，改进了多摄像头3D目标检测的性能。该方法采用两阶段锚点生成和基于BEV的非极大值抑制技术，有效解决了注意力计算扩展问题，并利用混合时间建模进一步提升了检测效果。&lt;h4&gt;背景&lt;/h4&gt;当前研究越来越多地使用基于鸟瞰图(BEV)的transformer进行多摄像头3D目标检测。传统模型通常使用随机查询作为锚点并进行连续优化，而最近的进展则使用辅助网络的检测结果来补充或替代这些随机查询。&lt;h4&gt;目的&lt;/h4&gt;提出一种更直观、高效的方法，直接使用BEV特征单元作为锚点；开发专门用于多摄像头3D目标检测的两阶段锚点生成方法；解决大量查询导致的注意力计算扩展问题；利用BEV特征中固有的时间信息增强检测性能。&lt;h4&gt;方法&lt;/h4&gt;使用BEV特征单元直接作为锚点；提出新的两阶段锚点生成方法；应用基于BEV的非极大值抑制，允许梯度仅通过未被抑制的对象流动；使用编码器的BEV特征直接作为对象查询，嵌入时间信息；引入混合时间建模方法，整合先前的检测结果以进一步提高检测性能。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上与基线相比，NDS和mAP有持续且显著的改进；即使使用更稀疏的BEV网格，方法仍然有效；对小物体特别有效，在nuScenes上行人检测mAP提高了3.8%，在Waymo上LET-mAP提高了8%；应用于Waymo Open数据集实现了最先进的性能，LET-mAP达到60.7%，比之前的最佳结果提高了5.4%。&lt;h4&gt;结论&lt;/h4&gt;直接使用BEV特征单元作为锚点是一种更直观且高效的3D目标检测方法；所提出的方法有效解决了计算扩展问题；混合时间建模进一步增强了检测性能；DenseBEV在多个数据集上实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;在当前研究中，基于鸟瞰图(BEV)的transformer越来越多地用于多摄像头3D目标检测。传统模型通常采用随机查询作为锚点，并对其进行连续优化。最近的进展通过辅助网络的检测结果来补充或替代这些随机查询。我们提出了一种更直观、高效的方法，直接使用BEV特征单元作为锚点。这种端到端方法利用BEV查询的密集网格，将每个单元视为最终检测任务中潜在的对象。因此，我们引入了一种专门为多摄像头3D目标设计的新型两阶段锚点生成方法。为了解决大量查询导致的注意力计算扩展问题，我们应用了基于BEV的非极大值抑制，允许梯度仅通过未被抑制的对象流动。这确保了高效训练，无需后处理。通过使用编码器(如BEVFormer)的BEV特征直接作为对象查询，时间BEV信息被自然嵌入。基于我们对象查询中已经嵌入的时间BEV信息，我们通过整合先前的检测结果引入了混合时间建模方法，以进一步提高检测性能。在nuScenes数据集上评估我们的方法，显示与基线相比，NDS和mAP有持续且显著的改进，即使使用更稀疏的BEV网格(因此初始锚点更少)。该方法对小物体特别有效，在nuScenes上行人检测mAP提高了3.8%，在Waymo上LET-mAP提高了8%。将我们的方法(命名为DenseBEV)应用于具有挑战性的Waymo Open数据集，实现了最先进的性能，LET-mAP达到60.7%，比之前的最佳结果提高了5.4%。代码可在https://github.com/mdaehl/DenseBEV获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于鸟瞰图(BEV)的transformer模型在多摄像头3D物体检测中如何更有效地初始化物体查询的问题。这个问题很重要，因为3D物体检测对自动驾驶和机器人应用至关重要，需要准确检测各种大小的物体(特别是小型物体如行人、自行车等)，同时保持计算效率以实现实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到Deformable DETR的两阶段物体查询初始化方法的启发，考虑直接使用BEV特征网格中的每个单元作为潜在物体的锚点。为了解决密集锚点带来的计算问题和重复检测问题，作者引入了非极大值抑制(NMS)到训练过程中。作者还借鉴了DDQ在2D检测中使用NMS过滤特征的概念，以及StreamPETR的时间建模方法，设计了混合时间建模方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将BEV特征网格中的每个网格单元直接作为潜在物体的锚点，而不是使用随机查询或辅助网络生成的查询。整体流程是：1)使用BEVFormer编码器处理多摄像头图像生成BEV特征网格；2)将每个网格单元通过辅助检测头解码为边界框和置信度分数；3)应用BEV-NMS过滤冗余查询；4)将过滤后的查询输入解码器进一步优化；5)对于时间增强版本，将当前BEV查询与先前检测到的物体结合实现混合时间建模。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出DenseBEV，第一个通过NMS实现密集先验的端到端多视图3D物体检测方法；2)实现物体中心时间建模和BEV时间建模的合并；3)在Waymo数据集上实现最先进性能。相比之前的工作，不同之处在于：不再依赖辅助网络生成初始锚点；将NMS集成到训练过程；使用BEV平面而非3D空间进行NMS；实现混合时间建模；特别擅长检测小型物体。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DenseBEV通过直接利用BEV特征网格作为密集锚点并集成非极大值抑制到训练过程中，显著提高了多摄像头3D物体检测的效率和准确性，特别是在检测小型物体方面，同时实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.</description>
      <author>example@mail.com (Marius Dähling, Sebastian Krebs, J. Marius Zöllner)</author>
      <guid isPermaLink="false">2512.16818v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Auto-Vocabulary 3D Object Detection</title>
      <link>http://arxiv.org/abs/2512.16077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自动词汇3D目标检测方法(AV3DOD)，能够自动生成检测对象的类别名称而无需用户输入，在多个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的开放词汇3D目标检测方法虽然名称上暗示可以检测训练过程中未见过的类别，但实际上仍需用户在训练和推理时指定类别。&lt;h4&gt;目的&lt;/h4&gt;研究自动词汇3D目标检测(AV3DOD)，其中检测到的对象的类别名称可以自动生成，无需任何用户输入。&lt;h4&gt;方法&lt;/h4&gt;引入语义分数(SS)评估生成类别名称的质量；开发AV3DOD框架，利用2D视觉-语言模型通过图像标注、伪3D框生成和特征空间语义扩展来生成丰富的语义候选。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNetV2和SUNRGB-D数据集上，AV3DOD在定位(mAP)和语义质量(SS)方面都达到了最先进的性能；在ScanNetV2上超越了CoDA方法，整体mAP提高3.48，SS相对提升24.5%。&lt;h4&gt;结论&lt;/h4&gt;AV3DOD方法实现了自动词汇3D目标检测，无需用户指定类别，同时保持了高水平的检测性能和语义质量。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇3D目标检测方法能够定位训练过程中未见过的类别的3D边界框。尽管名称如此，现有方法在训练和推理时都依赖于用户指定的类别。我们研究了自动词汇3D目标检测(AV3DOD)，其中检测到的对象的类别名称无需任何用户输入即可自动生成。为此，我们引入语义分数(SS)来评估生成的类别名称的质量。然后我们开发了一个新框架AV3DOD，它利用2D视觉-语言模型(VLMs)通过图像标注、伪3D框生成和特征空间语义扩展来生成丰富的语义候选。在ScanNetV2和SUNRGB-D数据集上，AV3DOD在定位(mAP)和语义质量(SS)方面都达到了最先进的性能。值得注意的是，它超越了SOTA方法CoDA，整体mAP提高了3.48，在ScanNetV2上SS相对提升了24.5%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动词汇3D物体检测问题，即让3D物体检测模型能够自主发现和生成物体类别名称，而不依赖于用户预定义的词汇。这个问题在现实中很重要，因为真实世界中的物体种类繁多且不断变化，无法预先定义所有可能的类别；在研究中很重要，因为现有方法限制了3D感知系统的开放世界特性，无法完全捕捉3D场景中的语义多样性，这对于实现真正的开放世界3D理解、应用于自动驾驶、机器人和embodied AI等领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先明确指出现有开放词汇3D物体检测方法的局限性，然后设计了新任务和评估指标。方法设计上借鉴了2D视觉-语言模型(VLMs)的能力，特别是图像描述功能；引入了伪3D框生成技术，从2D图像生成3D物体提案；设计了特征空间语义扩展策略(FSSE)，在连续嵌入空间中采样新的语义原型。确实借鉴了现有工作，包括3DETR用于3D物体检测、Florence2用于图像描述、CLIP用于跨模态特征对齐等，但在伪3D框生成和特征空间语义扩展方面进行了创新和扩展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让3D物体检测模型能够自主发现和生成物体类别，不依赖用户预定义词汇，通过利用2D视觉-语言模型能力和特征空间语义扩展实现。整体流程分为：1)物体定位模块：使用3DETR生成类无关的3D物体提案和特征；2)新颖语义探索模块：整合基础类别特征、VLM描述特征、伪标签特征和扩展特征；3)语义对齐模块：将检测到的3D物体特征与超词汇特征匹配预测类别；4)训练过程：结合定位损失和语义损失联合优化；5)推理过程：仅使用定位和语义对齐模块，排除扩展特征提高标签可读性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出自动词汇3D物体检测新任务；2)设计语义分数(SS)评估指标；3)开发伪3D框生成技术；4)提出特征空间语义扩展(FSSE)策略；5)整合多源语义构建丰富语义空间。相比之前工作不同：传统OV3DOD需要预定义词汇，AV3DOD完全自主生成；现有自动词汇方法依赖离散文本标签，AV3DOD支持连续特征空间扩展；许多方法需要2D图像输入，AV3DOD推理时直接在3D点云上进行；性能上显著提升，如ScanNetV2上mAP高出3.48，SS提升24.5%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了自动词汇3D物体检测新任务和框架，通过整合2D视觉-语言模型和特征空间语义扩展，使3D物体检测能够自主发现和生成物体类别，无需预定义词汇，显著提升了定位精度和语义理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.</description>
      <author>example@mail.com (Haomeng Zhang, Kuan-Chuan Peng, Suhas Lohit, Raymond A. Yeh)</author>
      <guid isPermaLink="false">2512.16077v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems</title>
      <link>http://arxiv.org/abs/2512.16454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MPBS的可扩展任务招募框架，通过行为感知和移动性预测来优化无人系统的任务分配。&lt;h4&gt;背景&lt;/h4&gt;随着无人机和无人地面车辆等无人系统在城市场景感知和应急响应等应用中的重要性日益增加，高效招募这些自主设备来执行时间敏感任务已成为关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个可扩展的任务招募框架MPBS(Mobility-aware Prediction and Behavior-based Scheduling)，将每个设备视为可招募的'用户'，以实现高效的任务分配。&lt;h4&gt;方法&lt;/h4&gt;MPBS集成了三个关键模块：行为感知的KNN分类器、用于预测设备移动性的时变马尔可夫预测模型，以及考虑任务紧急程度和基站性能的动态优先级调度机制。通过结合行为分类与时空预测，MPBS能够实时地将任务分配给最合适的设备。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界的GeoLife数据集上的实验评估表明，MPBS显著提高了任务完成效率和资源利用率。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为无人系统中的智能协作调度提供了预测性、行为感知的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着无人机和无人地面车辆等无人系统在城市场景感知和应急响应等应用中的重要性日益增加，高效招募这些自主设备来执行时间敏感任务已成为一个关键挑战。本文提出了MPBS（移动感知预测和行为调度），一种可扩展的任务招募框架，将每个设备视为可招募的'用户'。MPBS集成了三个关键模块：行为感知的KNN分类器、用于预测设备移动性的时变马尔可夫预测模型，以及考虑任务紧急程度和基站性能的动态优先级调度机制。通过结合行为分类与时空预测，MPBS能够实时地将任务分配给最合适的设备。在真实世界的GeoLife数据集上的实验评估显示，MPBS显著提高了任务完成效率和资源利用率。所提出的框架为无人系统中的智能协作调度提供了预测性、行为感知的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable "user". MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.</description>
      <author>example@mail.com (Tianhao Shao, Kaixing Zhao, Feng Liu, Lixin Yang, Bin Guo)</author>
      <guid isPermaLink="false">2512.16454v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals</title>
      <link>http://arxiv.org/abs/2512.16791v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;KineST是一种新颖的运动学引导的状态空间模型，用于解决AR/VR应用中基于稀疏信号重建全身姿势的问题，在准确性和时间一致性方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;全身运动追踪在AR/VR应用中连接物理和虚拟交互至关重要，但基于头戴式显示器的稀疏信号重建真实多样的全身姿势具有挑战性。现有方法计算成本高或分别建模时空依赖性，难以平衡准确性、时间一致性和效率。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效提取时空依赖性、整合局部和全局姿势感知的全身姿势重建方法，同时平衡准确性、时间一致性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出KineST模型，包含两个核心创新：1)将状态空间对偶框架内的扫描策略重新公式化为运动学引导的双向扫描，嵌入运动学先验；2)采用混合时空表示学习方法，紧密耦合空间和时间上下文；3)引入几何角速度损失，对旋转变化施加物理约束提高运动稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;KineST在轻量级框架内实现了优越的性能，在准确性和时间一致性方面均表现出色。&lt;h4&gt;结论&lt;/h4&gt;KineST是一种有效的全身姿势重建解决方案，能够在保持计算效率的同时提供高质量、时间一致的运动重建结果。&lt;h4&gt;翻译&lt;/h4&gt;全身运动追踪在AR/VR应用中扮演着重要角色，连接物理和虚拟交互。然而，基于AR/VR场景中的主要设备头戴式显示器获得的稀疏信号来重建真实多样的全身姿势具有挑战性。现有的姿势重建方法通常计算成本高或依赖分别建模空间和时间依赖性，难以平衡准确性、时间一致性和效率。为解决这一问题，我们提出了KineST，一种新颖的运动学引导的状态空间模型，它能够有效提取时空依赖性，同时整合局部和全局姿势感知。创新来自两个核心想法。首先，为了更好地捕捉复杂的关节关系，将状态空间对偶框架内的扫描策略重新公式化为运动学引导的双向扫描，嵌入运动学先验。其次，采用混合时空表示学习方法，紧密耦合空间和时间上下文，平衡准确性和平滑度。此外，引入几何角速度损失对旋转变化施加物理上有意义的约束，进一步提高运动稳定性。大量实验证明，KineST在轻量级框架内具有优越的性能，在准确性和时间一致性方面均表现出色。项目页面：https://kaka-1314.github.io/KineST/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从稀疏信号（如头戴显示器HMD捕获的头部和手部信号）重建真实、多样且平滑的全身运动姿势的问题。这个问题在AR/VR应用中至关重要，因为它连接了物理和虚拟交互，影响用户体验的沉浸感。现有的方法往往计算成本高、参数量大，或难以平衡姿势准确性和运动平滑性，限制了实际应用。在真实场景中，HMD只能提供非常有限的信息，使得从这些稀疏信号推断全身运动变得极具挑战性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：高计算成本、参数量大、难以平衡准确性和平滑性。他们选择了状态空间对偶（SSD）框架作为基础，因为该框架在高效时间序列建模方面显示出潜力，但发现直接应用效果不佳。作者创新性地设计了运动学引导的双向扫描策略和混合时空表示学习机制，借鉴了状态空间模型（如Mamba和SSD）、SMPL人体表示方法以及常用的损失函数设计，但将这些元素进行了创新性整合和改进，以适应全身运动重建的特殊需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过运动学引导的时空建模，从稀疏信号中重建准确且平滑的全身运动。具体包括：1）将人体骨骼的运动学先验整合到模型设计中；2）通过混合时空表示学习方法紧密耦合空间和时间上下文；3）采用基于运动学树的双向扫描策略捕捉更完整的人体运动特征；4）使用物理约束的损失函数提高运动稳定性。整体流程：首先接收HMD的稀疏跟踪信号并嵌入为姿势特征；然后通过时间流模块（TFM）处理时间依赖关系；接着通过时空运动学流模块（SKFM）整合空间和时间信息；最后使用线性回归器估计全身运动姿势，并用组合损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）运动学引导的双向扫描策略（KTSS），基于人体运动学树重新设计扫描顺序；2）时空混合机制（STMM），紧密耦合空间和时间上下文；3）几何角速度损失（L_geo_angvel），在流形空间中对旋转变化施加物理约束；4）轻量级高效架构，参数量仅11M。相比之前的工作，KineST不再分别建模空间和时间依赖关系，而是通过STMM实现联合建模；显式整合运动学先验，而大多数现有方法忽略人体骨骼结构；使用符合旋转几何特性的损失函数，而非简单的欧几里得距离；在轻量级框架内同时实现了高准确性和良好的时间一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; KineST通过运动学引导的时空状态空间模型和创新的几何角速度损失，实现了从稀疏信号中高效重建准确且平滑的全身运动，为AR/VR应用提供了轻量级但高性能的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/</description>
      <author>example@mail.com (Shuting Zhao, Zeyu Xiao, Xinrong Chen)</author>
      <guid isPermaLink="false">2512.16791v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</title>
      <link>http://arxiv.org/abs/2512.16913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://insta360-research-team.github.io/DAP_website/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种全景度量深度基础模型，能够泛化到不同场景距离，通过数据闭环范式结合多种数据源构建大规模数据集，并采用三阶段伪标签整理管道减少域差距，同时引入优化的模型架构提高鲁棒性和几何一致性。&lt;h4&gt;背景&lt;/h4&gt;深度估计在计算机视觉中具有重要意义，但现有方法在不同场景距离和类型上泛化能力有限。缺乏能够处理全景图像且在各种真实场景中保持稳定性能的基础模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够泛化到不同场景距离的全景度量深度基础模型，提高在室内外、合成与真实数据上的性能，并确保跨视图的几何一致性。&lt;h4&gt;方法&lt;/h4&gt;1. 数据收集：结合公共数据集、UE5模拟器和文本到图像模型生成的合成数据，以及网络上的真实全景图像；2. 数据处理：引入三阶段伪标签整理管道，减少室内/室外和合成/真实数据间的域差距；3. 模型设计：采用DINOv3-Large作为骨干网络，引入即插即用的范围掩码头、以清晰度为中心的优化和以几何为中心的优化&lt;h4&gt;主要发现&lt;/h4&gt;模型在多个基准测试（Stanford2D3D、Matterport3D和Deep360）上表现出强大的性能和零样本泛化能力，特别是在各种真实场景中能够提供稳健和稳定的度量深度预测。&lt;h4&gt;结论&lt;/h4&gt;该全景度量深度基础模型通过创新的数据闭环范式和优化的模型架构，成功解决了在不同场景距离和类型上的泛化问题，为计算机视觉中的深度估计提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们提出了一种全景度量深度基础模型，能够泛化到不同的场景距离。我们从数据构建和框架设计的角度探索了数据闭环范式。我们通过结合公共数据集、来自我们UE5模拟器和文本到图像模型的高质量合成数据，以及来自网络的真实全景图像，收集了一个大规模数据集。为了减少室内/室外和合成/真实数据之间的域差距，我们引入了一个三阶段伪标签整理管道，为未标记图像生成可靠的真值。对于模型，我们采用DINOv3-Large作为骨干网络，因为它具有强大的预训练泛化能力，并引入了即插即用的范围掩码头、以清晰度为中心的优化和以几何为中心的优化，以提高对不同距离的鲁棒性并强制执行跨视图的几何一致性。在多个基准测试（如Stanford2D3D、Matterport3D和Deep360）上的实验展示了强大的性能和零样本泛化能力，特别是在各种真实场景中具有稳健和稳定的度量预测。项目页面可以在以下网址找到：https://insta360-research-team.github.io/DAP_website/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决全景深度估计的泛化能力不足问题，特别是现有方法难以适应多样化的真实世界场景（尤其是室外场景）。这个问题在现实中非常重要，因为全景深度估计能提供360°×180°的完整环境覆盖，对机器人导航中的全向避障、增强现实和空间智能等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用'数据在回路'范式，从数据构建和框架设计两方面思考。数据方面整合了公共数据集、UE5模拟器生成的合成数据及网络收集的真实图像；模型方面借鉴了DINOv3-Large作为骨干网络，并引入即插即用的范围掩码头和多种优化损失函数。作者借鉴了现有数据集和模拟平台，以及传统深度估计方法中的损失函数设计，但进行了创新性组合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个能统一跨不同域和场景类型的大规模全景深度基础模型。整体流程包括：1)构建包含2M+样本的多源数据集；2)实施三阶段训练管道（场景不变标签器→真实感不变标签器→DAP模型训练）；3)设计即插即用范围掩码头和多损失优化框架。这种方法通过大规模数据扩展和渐进式训练策略，实现了对室内外场景的强泛化能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)大规模数据引擎(2M+跨域样本)；2)三阶段伪标签精炼管道；3)即插即用范围掩码与几何-锐度双导向优化；4)强大的跨场景泛化能力。相比之前工作，DAP在数据规模上大幅提升(如PanDA仅12万样本，DAC仅80万样本)，训练方法从单一阶段改进为渐进式三阶段，并首次引入可适应不同距离的场景范围掩码机制，在室外场景表现尤为突出。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过构建大规模全景数据集和设计三阶段训练管道，提出了DAP全景深度基础模型，实现了跨室内-室外场景的强泛化能力和最先进的深度估计性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}</description>
      <author>example@mail.com (Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li, Bo Du, Ming-Hsuan Yang, Truong Nguyen, Lu Qi)</author>
      <guid isPermaLink="false">2512.16913v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies</title>
      <link>http://arxiv.org/abs/2512.16881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website: https://polaris-evals.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为PolaRiS的可扩展真实到模拟框架，用于高保真模拟机器人评估。该框架利用神经重建方法将真实世界场景的短视频扫描转换为交互式模拟环境，并开发了模拟数据共训练方法，以弥合真实到模拟的差距，实现零样本评估。研究表明PolaRiS评估与真实世界通用策略性能的相关性比现有模拟基准更强。&lt;h4&gt;背景&lt;/h4&gt;机器人学习研究面临准确测量和比较机器人策略性能的挑战。由于随机性、可重复性差和真实世界部署耗时长的特点，机器人领域的基准测试历来困难。对于需要在各种场景和任务中评估的通用策略，这一挑战更加突出。现有模拟评估与真实世界之间存在视觉和物理域差距，导致评估结果不可靠。此外，构建真实多样的模拟环境需要大量专业知识和人力投入。&lt;h4&gt;目的&lt;/h4&gt;为了弥合模拟与真实世界之间的差距，研究者引入了PolaRiS框架，旨在提供一种可扩展的高保真模拟机器人评估方法，能够准确反映真实世界中的机器人策略性能。&lt;h4&gt;方法&lt;/h4&gt;PolaRiS利用神经重建方法将真实世界场景的短视频扫描转换为交互式模拟环境。同时，开发了一种简单的模拟数据共训练方法，用于弥合剩余的真实到模拟差距，并实现在未见过的模拟环境中的零样本评估。通过在模拟和真实世界之间进行大量成对评估来验证方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，PolaRiS评估与真实世界通用策略性能的相关性比现有模拟基准强得多。此外，该方法的简单性使得能够快速创建多样化的模拟环境，降低了高质量模拟环境创建的门槛。&lt;h4&gt;结论&lt;/h4&gt;这项工作朝着为下一代机器人基础模型实现分布式和民主化评估迈出了一步。PolaRiS框架不仅提供了一种更可靠的机器人策略评估方法，还加速了高质量模拟环境的创建，有望推动机器人学习领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;机器人学习研究的一个重大挑战是我们准确测量和比较机器人策略性能的能力。由于随机性、可重复性和真实世界部署的耗时性，机器人领域的基准测试历来具有挑战性。对于最近的通用策略，这种挑战更加突出，因为它们需要在各种场景和任务中进行评估。模拟评估为真实世界评估提供了可扩展的补充，但现有模拟基准与真实世界之间的视觉和物理域差距使得它们成为策略改进的不可靠信号。此外，构建真实且多样化的模拟环境传统上需要大量的人力投入和专业知识。为了弥合这一差距，我们介绍了PolaRiS（模拟中的策略评估和环境重建），这是一个用于高保真模拟机器人评估的可扩展真实到模拟框架。PolaRiS利用神经重建方法将真实世界场景的短视频扫描转换为交互式模拟环境。此外，我们开发了一种简单的模拟数据共训练方法，用于弥合剩余的真实到模拟差距，并实现在未见过的模拟环境中的零样本评估。通过在模拟和真实世界之间进行大量成对评估，我们证明PolaRiS评估与真实世界通用策略性能的相关性比现有模拟基准强得多。其简单性也使得能够快速创建多样化的模拟环境。因此，这项工作朝着为下一代机器人基础模型实现分布式和民主化评估迈出了一步。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何准确评估通用机器人策略性能的问题。这个问题很重要，因为随着通用机器人策略的发展，需要大规模经验验证，而直接在真实机器人上评估既耗时又昂贵；现有模拟环境与真实世界之间存在视觉和物理差距，无法可靠预测真实世界中的策略性能；此外，构建逼真模拟环境需要大量专业知识，限制了评估的可扩展性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有模拟评估方法与真实世界之间存在差距的问题，特别是对于通用机器人策略。他们借鉴了神经重建技术（如2D高斯飞溅）和3D生成模型（如TRELLIS）来创建高保真环境，同时结合行为克隆等现有技术进行策略微调。创新性地将这些技术整合到一个专门针对通用机器人策略评估的框架中，并解决了支持手腕相机和零样本评估等特定挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过神经重建将真实世界场景转换为高保真模拟环境，再通过简单模拟数据共训练弥合真实到模拟差距，实现策略准确评估。流程包括：1)拍摄真实场景视频并用2D高斯飞溅重建环境和机器人；2)自动处理机器人关节使高斯飞溅组件随物理运动学移动；3)用TRELLIS生成模型创建3D物体；4)组合场景并设置物理参数；5)用少量模拟演示共训练策略，使其在未见环境中也能准确评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用神经重建创建高保真3D模拟环境，支持完整3D渲染而非2D背景复制；2)通过少量共训练实现零样本评估能力；3)支持手腕相机评估（之前方法如SIMPLER不支持）；4)快速环境创建（&lt;20分钟人力）；5)简单有效的共训练方法。相比之前工作，PolaRiS解决了移动相机支持问题，提供了比Libero等基准更高的保真度，比视频模型更可靠的物理交互，并首次实现了在未见环境中评估通用策略的能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PolaRiS提出了一种可扩展的真实到模拟框架，通过神经重建技术和简单的模拟数据共训练方法，实现了对通用机器人策略的高保真评估，在未见过的模拟环境中与真实世界性能表现出强相关性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.</description>
      <author>example@mail.com (Arhan Jain, Mingtong Zhang, Kanav Arora, William Chen, Marcel Torne, Muhammad Zubair Irshad, Sergey Zakharov, Yue Wang, Sergey Levine, Chelsea Finn, Wei-Chiu Ma, Dhruv Shah, Abhishek Gupta, Karl Pertsch)</author>
      <guid isPermaLink="false">2512.16881v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2512.16740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种任务导向的数据合成框架(TODSynth)，用于解决遥感图像语义分割中合成数据的效用限制问题。&lt;h4&gt;背景&lt;/h4&gt;随着可控生成技术的快速发展，训练数据合成成为扩大遥感领域标记数据集和缓解人工标注的有效途径。然而，语义掩模控制的复杂性和采样质量的不确定性限制了合成数据在下游语义分割任务中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决语义掩模控制复杂性和采样质量不确定性带来的挑战，提高合成数据在遥感语义分割任务中的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出TODSynth框架，包含具有统一三重注意力的多模态扩散变换器(MM-DiT)和任务引导的即插即用采样策略；基于DiT生成基础模型系统评估不同控制方案；提出控制-修正流匹配(CRFM)方法，在早期高塑性阶段动态调整采样方向。&lt;h4&gt;主要发现&lt;/h4&gt;文本-图像-掩模联合注意力方案结合图像和掩模分支的完全微调显著增强了遥感语义分割数据合成的效果，特别是在少样本和复杂场景下；CRFM方法减轻了生成图像的不稳定性，缩小了合成数据与下游任务之间的差距。&lt;h4&gt;结论&lt;/h4&gt;该方法一致优于最先进的可控生成方法，能产生更稳定和任务导向的合成数据用于遥感语义分割任务。&lt;h4&gt;翻译&lt;/h4&gt;随着可控生成的快速进展，训练数据合成已成为扩大标记数据集和缓解遥感领域人工标注的一种有前景的方法。然而，语义掩模控制的复杂性和采样质量的不确定性常常限制了合成数据在下游语义分割任务中的效用。为应对这些挑战，我们提出了一个任务导向的数据合成框架(TODSynth)，包括具有统一三重注意力的多模态扩散变换器(MM-DiT)和由任务引导的即插即用采样策略。基于强大的基于DiT的生成基础模型，我们系统评估了不同的控制方案，表明文本-图像-掩模联合注意力方案结合图像和掩模分支的完全微调显著增强了遥感语义分割数据合成的有效性，特别是在少样本和复杂场景场景下。此外，我们提出了控制-修正流匹配(CRFM)方法，在早期高塑性阶段由语义损失引导动态调整采样方向，减轻了生成图像的不稳定性，并缩小了合成数据与下游分割任务之间的差距。大量实验证明，我们的方法一致优于最先进的可控生成方法，为遥感语义分割产生了更稳定和任务导向的合成数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.</description>
      <author>example@mail.com (Yunkai Yang, Yudong Zhang, Kunquan Zhang, Jinxiao Zhang, Xinying Chen, Haohuan Fu, Runmin Dong)</author>
      <guid isPermaLink="false">2512.16740v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Discovering and Learning Probabilistic Models of Black-Box AI Capabilities</title>
      <link>http://arxiv.org/abs/2512.16733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用PDDL风格表示来学习和建模黑盒AI系统规划能力的方法，通过蒙特卡洛树搜索系统创建测试任务、获取数据并修剪假设空间，能够提供对黑盒AI能力的健全且可解释的表示。&lt;h4&gt;背景&lt;/h4&gt;黑盒AI系统如基础模型越来越多地用于顺序决策，为确保这些系统的安全操作和部署，需要开发高效方法来提供对黑盒AI能力的健全且可解释的表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够有效地学习和建模输入黑盒AI的规划能力，并提供对黑盒AI能力的健全且可解释的表示。&lt;h4&gt;方法&lt;/h4&gt;使用PDDL风格的表示来学习和建模黑盒AI的规划能力，采用蒙特卡洛树搜索范式系统性地创建测试任务、获取数据并修剪可能的符号模型的假设空间，学习的模型描述黑盒AI的能力、执行条件以及执行的可能结果和相关概率。&lt;h4&gt;主要发现&lt;/h4&gt;理论结果表明学习到的模型具有健全性、完整性和收敛性，多个黑盒AI系统的实证结果展示了所提出方法的范围、效率和准确性。&lt;h4&gt;结论&lt;/h4&gt;PDDL风格的表示可以有效地用于学习和建模黑盒AI的规划能力，所提出的方法能够提供对黑盒AI能力的健全且可解释的表示。&lt;h4&gt;翻译&lt;/h4&gt;黑盒AI系统如基础模型正越来越多地用于顺序决策。为确保此类系统的安全操作和部署，必须开发能够提供黑盒AI能力的健全且可解释表示的高效方法。本文表明，PDDL风格的表示可用于有效地学习和建模输入黑盒AI的规划能力。它采用蒙特卡洛树搜索范式系统性地创建测试任务、获取数据并修剪可能符号模型的假设空间。学习到的模型描述了黑盒AI的能力、执行条件以及执行的可能结果及其相关概率。理论结果表明学习到的模型的健全性、完整性和收敛性。多个黑盒AI系统的实证结果展示了所提出方法的范围、效率和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.</description>
      <author>example@mail.com (Daniel Bramblett, Rushang Karia, Adrian Ciotinga, Ruthvick Suresh, Pulkit Verma, YooJung Choi, Siddharth Srivastava)</author>
      <guid isPermaLink="false">2512.16733v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</title>
      <link>http://arxiv.org/abs/2512.16636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了REGLUE（表示纠缠与全局-局部统一编码），一种统一的潜在扩散框架，通过联合建模VAE图像潜在表示、局部VFM语义和全局[CLS]令牌，有效改善了潜在扩散模型的训练效率和样本质量。&lt;h4&gt;背景&lt;/h4&gt;潜在扩散模型(LDMs)在图像合成方面表现优异，但其重建风格去噪目标仅提供间接语义监督，高级语义出现缓慢，需要更长训练时间并限制样本质量。现有方法要么通过表示对齐从外部注入语义，要么在扩散过程中仅联合建模VFM特征的窄片，未能充分利用VFM中丰富、非线性、多层空间语义。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一的潜在扩散框架，充分利用视觉基础模型(VFMs)中的丰富、非线性、多层空间语义，以改善潜在扩散模型的训练效率和样本质量。&lt;h4&gt;方法&lt;/h4&gt;REGLUE框架在单个SiT主干网络中联合建模三种表示：(i) VAE图像潜在表示，(ii) 紧凑的局部（块级）VFM语义，和(iii) 全局（图像级）[CLS]令牌。使用轻量级卷积语义压缩器将多层VFM特征非线性聚合为低维空间结构化表示，并在扩散过程中与VAE潜在表示纠缠。同时，通过外部对齐损失将内部表示向冻结的VFM目标正则化。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明：(a) 空间VFM语义对性能至关重要，(b) 非线性压缩是解锁VFM特征全部益处的关键，(c) 全局令牌和外部对齐作为互补的轻量级增强，在全局-局部-潜在联合建模框架中发挥作用。&lt;h4&gt;结论&lt;/h4&gt;REGLUE在ImageNet 256x256上持续改进FID指标并加速收敛，超过了多个基线方法。该方法通过有效结合全局和局部语义表示，充分利用了VFM特征，并通过外部对齐进一步优化了表示，为潜在扩散模型提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;潜在扩散模型(LDMs)实现了最先进的图像合成，但它们的重建风格去噪目标仅提供间接的语义监督：高级语义出现缓慢，需要更长的训练时间并限制了样本质量。最近的工作通过表示对齐从视觉基础模型(VFMs)外部注入语义，或者在扩散过程中仅联合建模VFM特征的一个窄片，未能充分利用丰富、非线性、多层空间语义的可用性。我们引入了REGLUE（Representation Entanglement with Global-Local Unified Encoding），这是一种统一的潜在扩散框架，在单个SiT主干网络中联合建模了(i) VAE图像潜在表示，(ii) 紧凑的局部（块级）VFM语义，和(iii) 全局（图像级）[CLS]令牌。一个轻量级卷积语义压缩器将多层VFM特征非线性聚合为低维空间结构化表示，该表示在扩散过程中与VAE潜在表示纠缠。外部对齐损失进一步将内部表示向冻结的VFM目标正则化。在ImageNet 256x256上，REGLUE在FID方面持续改进并加速收敛，超过了SiT-B/2和SiT-XL/2基线，以及REPA、ReDi和REG。大量实验表明，(a)空间VFM语义至关重要，(b)非线性压缩是解锁其全部益处的关键，(c)全局令牌和外部对齐在我们的全局-局部-潜在联合建模框架中作为互补的轻量级增强。代码可在https://github.com/giorgospets/reglue获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .</description>
      <author>example@mail.com (Giorgos Petsangourakis, Christos Sgouropoulos, Bill Psomas, Theodoros Giannakopoulos, Giorgos Sfikas, Ioannis Kakogeorgiou)</author>
      <guid isPermaLink="false">2512.16636v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2512.16567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为Causal-Tune的新型微调策略，用于从视觉基础模型中提取因果因素并抑制非因果因素，从而提高领域泛化语义分割的性能。&lt;h4&gt;背景&lt;/h4&gt;现有方法主要关注训练轻量级适配器或改进中间特征来实现对未见领域的更好泛化，但忽视了长期预训练的视觉基础模型中存在的伪影(artifacts)，这些伪影阻碍了有价值表征的利用，最终降低了领域泛化语义分割的性能。&lt;h4&gt;目的&lt;/h4&gt;明确检查视觉基础模型中特征的因果和非因果因素，并提出一种简单有效的方法来识别和解耦它们，实现更强大的领域泛化能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了Causal-Tune方法：1)使用离散余弦变换(DCT)提取每层特征的频谱；2)应用高斯带通滤波器分离频谱为因果和非因果成分；3)引入因果感知的可学习令牌在频域操作并丢弃非因果成分；4)通过逆DCT将精炼特征转换回空间域并传递到下一层。&lt;h4&gt;主要发现&lt;/h4&gt;通过因果机制观察发现，视觉基础模型中的伪影与非因果因素相关，这些因素通常位于模型频谱的低频和高频分量中。实验证明Causal-Tune在恶劣天气条件下表现优异，特别是在雪天条件下比基线方法提高了4.8%的mIoU。&lt;h4&gt;结论&lt;/h4&gt;Causal-Tune方法通过识别和解耦视觉基础模型中的因果与非因果因素，显著提高了领域泛化语义分割的性能，特别是在恶劣天气条件下表现优异。&lt;h4&gt;翻译&lt;/h4&gt;使用少量参数微调视觉基础模型在领域泛化语义分割任务中表现出显著性能。大多数现有工作要么训练轻量级适配器，要么改进中间特征，以实现对未见领域的更好泛化。然而，它们都忽视了长期预训练的视觉基础模型常常存在伪影，这阻碍了有价值表征的利用，最终降低了领域泛化语义分割性能。受因果机制的启发，我们观察到这些伪影与非因果因素相关，这些因素通常位于视觉基础模型频谱的低频和高频分量中。在本文中，我们明确检查了视觉基础模型中特征的因果和非因果因素，并提出了一种简单有效的方法来识别和解耦它们，从而实现更强大的领域泛化能力。具体来说，我们提出了Causal-Tune，一种新型微调策略，旨在从视觉基础模型的特征中提取因果因素并抑制非因果因素。首先，我们使用离散余弦变换提取每层特征的频谱。然后应用高斯带通滤波器将频谱分离为因果和非因果成分。为了进一步精炼因果成分，我们引入了一组因果感知的可学习令牌在频域操作，而非因果成分则被丢弃。最后，通过逆离散余弦变换将精炼的特征转换回空间域并传递到下一层。在各种跨域任务上进行的广泛实验证明了Causal-Tune的有效性。特别是，我们的方法在恶劣天气条件下实现了优越性能，在雪天条件下比基线提高了4.8%的平均交并比。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.</description>
      <author>example@mail.com (Yin Zhang, Yongqiang Zhang, Yaoyue Zheng, Bogdan Raducanu, Dan Liu)</author>
      <guid isPermaLink="false">2512.16567v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</title>
      <link>http://arxiv.org/abs/2512.16378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project available at https://github.com/sarapapi/hearing2translate&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文评估了将大型语言模型与语音结合的SpeechLLMs在语音翻译任务上的表现，并与传统的级联架构进行了比较。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型正在扩展到文本以外的领域，将语音作为原生模态整合，催生了SpeechLLMs，这些模型旨在直接翻译口语，绕过传统的基于转录的流水线架构。&lt;h4&gt;目的&lt;/h4&gt;探究SpeechLLMs的整合是否比现有的级联架构提高了语音到文本翻译的质量。&lt;h4&gt;方法&lt;/h4&gt;提出了第一个全面的测试套件"Hearing to Translate"，严格地将5种最先进的SpeechLLMs与16个强大的直接和级联系统进行了基准测试，这些系统结合了领先的语音基础模型和多语言LLMs。&lt;h4&gt;主要发现&lt;/h4&gt;级联系统仍然是最可靠的，当前的SpeechLLMs只在选定的设置中与级联系统相匹配，语音基础模型则落后于两者。&lt;h4&gt;结论&lt;/h4&gt;将大型语言模型整合到模型中或流水线中，对于高质量的语音翻译是必不可少的。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型扩展到文本以外的领域，将语音作为原生模态进行整合催生了SpeechLLMs，这些模型旨在直接翻译口语，从而绕过传统的基于转录的流水线。然而，这种整合是否比已建立的级联架构提高了语音到文本翻译的质量，仍然是一个开放性问题。我们提出了"Hearing to Translate"，这是第一个全面的测试套件，严格地将5种最先进的SpeechLLMs与16个强大的直接和级联系统进行了基准测试，这些系统结合了领先的语音基础模型和多语言LLMs。我们的分析涵盖了16个基准测试、13种语言对和9个具有挑战性的条件，包括不流畅、嘈杂和长篇语音。在这广泛的评估中，我们发现级联系统仍然是最可靠的，而当前的SpeechLLMs只在选定的设置中与级联系统相匹配，语音基础模型则落后于两者，这表明将大型语言模型整合到模型中或流水线中，对于高质量的语音翻译是必不可少的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.</description>
      <author>example@mail.com (Sara Papi, Javier Garcia Gilabert, Zachary Hopton, Vilém Zouhar, Carlos Escolano, Gerard I. Gállego, Jorge Iranzo-Sánchez, Ahrii Kim, Dominik Macháček, Patricia Schmidtova, Maike Züfle)</author>
      <guid isPermaLink="false">2512.16378v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Adaptation of Agentic AI</title>
      <link>http://arxiv.org/abs/2512.16301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个系统性框架，统一了智能体AI系统中的适应机制，包括智能体适应和工具适应两大类，并进一步细分为不同形式。该框架有助于明确适应策略的设计空间，权衡利弊，并为系统设计提供实用指导。&lt;h4&gt;背景&lt;/h4&gt;前沿的智能体AI系统建立在基础模型之上，这些模型可以被调整以进行规划、推理和与外部工具交互，从而执行日益复杂和专业的任务。随着这些系统能力和范围的扩大，适应机制成为提高性能、可靠性和泛化能力的关键。&lt;h4&gt;目的&lt;/h4&gt;统一快速扩展的研究领域，构建涵盖智能体适应和工具适应的系统性框架；明确适应策略的设计空间，权衡利弊，为系统设计提供实用指导；回顾各类代表性方法，分析优势和局限，突出关键开放挑战和未来机遇。&lt;h4&gt;方法&lt;/h4&gt;构建了一个系统性框架，将适应机制分为智能体适应和工具适应两大类；将智能体适应细分为工具执行信号驱动的智能体适应和智能体输出信号驱动的智能体适应；将工具适应细分为智能体无关的工具适应和智能体监督的工具适应。&lt;h4&gt;主要发现&lt;/h4&gt;该框架有助于明确智能体AI中适应策略的设计空间，使权衡关系更加明确，并为系统设计过程中选择或切换策略提供实用指导。&lt;h4&gt;结论&lt;/h4&gt;该论文旨在为寻求构建更强大、高效和可靠的智能体AI系统的研究人员和从业者提供概念基础和实用路线图。&lt;h4&gt;翻译&lt;/h4&gt;前沿的智能体AI系统建立在基础模型之上，这些模型可以被调整以进行规划、推理和与外部工具交互，从而执行日益复杂和专业的任务。随着这些系统能力和范围的扩大，适应机制成为提高性能、可靠性和泛化能力的关键。在本文中，我们将快速扩展的研究领域统一为一个系统性框架，涵盖了智能体适应和工具适应。我们进一步将智能体适应细分为工具执行信号驱动的智能体适应和智能体输出信号驱动的智能体适应，以及将工具适应细分为智能体无关的工具适应和智能体监督的工具适应。我们证明，该框架有助于明确智能体AI中适应策略的设计空间，使权衡关系明确，并为系统设计过程中选择或切换策略提供实用指导。然后，我们回顾了各类中的代表性方法，分析了它们的优势和局限性，并突出了关键的开放挑战和未来机遇。总体而言，本文旨在为寻求构建更强大、高效和可靠的智能体AI系统的研究人员和从业者提供概念基础和实用路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.</description>
      <author>example@mail.com (Pengcheng Jiang, Jiacheng Lin, Zhiyi Shi, Zifeng Wang, Luxi He, Yichen Wu, Ming Zhong, Peiyang Song, Qizheng Zhang, Heng Wang, Xueqiang Xu, Hanwen Xu, Pengrui Han, Dylan Zhang, Jiashuo Sun, Chaoqi Yang, Kun Qian, Tian Wang, Changran Hu, Manling Li, Quanzheng Li, Hao Peng, Sheng Wang, Jingbo Shang, Chao Zhang, Jiaxuan You, Liyuan Liu, Pan Lu, Yu Zhang, Heng Ji, Yejin Choi, Dawn Song, Jimeng Sun, Jiawei Han)</author>
      <guid isPermaLink="false">2512.16301v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Sigma-Moe-Tiny Technical Report</title>
      <link>http://arxiv.org/abs/2512.16248v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Sigma-MoE-Tiny，一种高度稀疏的混合专家语言模型，通过细粒度专家分段和渐进式稀疏化方案实现了高效的参数利用和稳定的训练过程，同时保持了顶尖的性能。&lt;h4&gt;背景&lt;/h4&gt;混合专家模型已成为基础模型的一种有前途的范式，因为它具有高效且强大的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一种MoE语言模型，实现比现有开源模型更高的稀疏度，同时保持卓越的性能。&lt;h4&gt;方法&lt;/h4&gt;采用细粒度专家分段，每层最多96个专家，每个标记只激活一个专家；提出渐进式稀疏化调度方案解决专家负载均衡问题；在多样化和高质量语料库上进行预训练，然后进行后训练以释放更多能力。&lt;h4&gt;主要发现&lt;/h4&gt;尽管只激活50亿参数（占总参数的2.5%），Sigma-MoE-Tiny在规模相当或显著更大的同类模型中实现了顶尖性能；训练过程保持稳定，没有发生不可恢复的损失峰值；常用的负载均衡损失在较低层变得无效，需要新的解决方案。&lt;h4&gt;结论&lt;/h4&gt;通过创新的专家分段和渐进式稀疏化方案，Sigma-MoE-Tiny成功解决了极端稀疏性带来的负载均衡挑战，为未来MoE架构的稀疏化发展提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;混合专家模型因其高效且强大的可扩展性已成为基础模型的一种有前途的范式。在这项工作中，我们提出了Sigma-MoE-Tiny，一种MoE语言模型，与现有开源模型相比实现了最高的稀疏度。Sigma-MoE-Tiny采用细粒度专家分段，每层最多96个专家，同时只为每个标记激活一个专家，总计200亿参数中只有50亿被激活。这种极端稀疏性带来的主要挑战在于专家负载均衡。我们发现，在这种设置下，广泛使用的负载均衡损失在较低层往往变得无效。为了解决这个问题，我们提出了一种渐进式稀疏化调度方案，旨在平衡专家利用率和训练稳定性。Sigma-MoE-Tiny在多样化和高质量的语料库上进行预训练，随后进行后训练以进一步释放其能力。整个训练过程保持显著稳定，没有发生不可恢复的损失峰值。全面的评估表明，尽管只激活50亿参数，Sigma-MoE-Tiny在规模相当或显著更大的同类模型中实现了顶尖性能。此外，我们对高度稀疏MoE模型中的负载平衡进行了深入讨论，为未来MoE架构的稀疏化进展提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny  Code: https://github.com/microsoft/ltp-megatron-lm</description>
      <author>example@mail.com (Qingguo Hu, Zhenghao Lin, Ziyue Yang, Yucheng Ding, Xiao Liu, Yuting Jiang, Ruizhe Wang, Tianyu Chen, Zhongxin Guo, Yifan Xiong, Rui Gao, Lei Qu, Jinsong Su, Peng Cheng, Yeyun Gong)</author>
      <guid isPermaLink="false">2512.16248v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints</title>
      <link>http://arxiv.org/abs/2512.16245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AlignMerge是一种几何感知的大型语言模型合并方法，能够在组合多个模型能力的同时保持对齐性，通过在Fisher-Rao几何空间中优化合并过程，确保模型在保持性能的同时不损害安全性和对齐性。&lt;h4&gt;背景&lt;/h4&gt;合并大型语言模型(LLMs)是一种实用的方法，可以从多个微调检查点组合能力而无需重新训练。然而，标准方案（线性权重混合、任务向量和Fisher加权平均）可以在保持损失的同时悄悄破坏对齐。&lt;h4&gt;目的&lt;/h4&gt;提出一种几何感知的合并框架，使对齐成为显式不变量，解决模型合并过程中对齐性被破坏的问题。&lt;h4&gt;方法&lt;/h4&gt;引入AlignMerge框架，在基于指令微调的基础模型的局部Fisher图中估计对齐子空间，使用投影器P_A，并优化目标函数L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud，其中L_geo保持合并结果接近专家模型，L_align惩罚沿对齐敏感方向的移动，L_bud执行软对齐预算。使用解码不变的对齐质量指数(AQI)作为对齐函数。&lt;h4&gt;主要发现&lt;/h4&gt;在五个模型家族（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）上测试，AlignMerge提高了对齐指标（AQI、毒性、LLM判断对齐），在指令遵循、推理和帮助性方面匹配或超过最佳专家，且与Fisher混合、TIES、SafeMerge和MergeAlign相比，对齐子空间漂移更小，预算违规更少。&lt;h4&gt;结论&lt;/h4&gt;使保持对齐的合并成为一流的设计目标，为未来基础模型的几何感知组合提供了一条路径。&lt;h4&gt;翻译&lt;/h4&gt;合并大型语言模型(LLMs)是一种实用的方法，可以从多个微调检查点组合能力而无需重新训练。然而，标准方案（线性权重混合、任务向量和Fisher加权平均）可以在保持损失的同时悄悄破坏对齐。我们认为合并不是数字技巧，而是围绕已对齐锚点的几何约束操作：融合必须引导以尊重安全几何，而不是事后验证。我们引入了AlignMerge，一种几何感知的合并框架，使对齐成为显式不变量。在基于指令微调的基础模型的局部Fisher图中，我们使用投影器P_A估计一个对齐子空间，并优化：L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud，其中L_geo使合并结果在Fisher-Rao几何中接近其专家模型，L_align惩罚沿对齐敏感方向的移动，L_bud强制执行软对齐预算。作为对齐函数，我们使用解码不变的对齐质量指数(AQI)，这是一个潜在空间标准，捕捉表示空间中对齐和非对齐行为的分离程度。在五个模型家族（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）上，将安全锚点与任务专家合并时，AlignMerge提高了对齐指标（AQI、毒性、LLM判断对齐），同时在指令遵循、推理和帮助性方面匹配或超过最佳专家。与Fisher混合、TIES、SafeMerge和MergeAlign相比，它显示出更小的对齐子空间漂移和更少的预算违规。这些结果使保持对齐的合并成为一流的设计目标，并暗示了未来基础模型几何感知组合的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.</description>
      <author>example@mail.com (Aniruddha Roy, Jyoti Patel, Aman Chadha, Vinija Jain, Amitava Das)</author>
      <guid isPermaLink="false">2512.16245v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting</title>
      <link>http://arxiv.org/abs/2512.16022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新方法，将大型语言模型(LLM)重新定位为智能评判者，用于评估、解释和协调时间序列基础模型的集成。通过R1风格的微调过程和基于SHAP的忠实度分数，使LLM能够理解时间序列数据的动态特性，并通过多轮对话优化集成策略。该方法在GIFT-Eval基准上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型众多，但没有一种方法能够持续表现出优越性。大型语言模型虽然具有强大的推理能力，但直接应用于时间序列预测效果不佳。&lt;h4&gt;目的&lt;/h4&gt;解决时间序列预测中的模型选择和集成问题，通过将LLM作为智能评判者来优化基础模型集成，提高预测性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;1. 将LLM重新定位为智能评判者，评估和协调基础模型集成2. 引入R1风格的微调过程，由基于SHAP的忠实度分数指导3. 教会模型将集成权重解释为关于时间动态的因果陈述4. 通过迭代的多轮对话进行前瞻性评估和策略优化&lt;h4&gt;主要发现&lt;/h4&gt;在GIFT-Eval基准上，该方法在23个数据集的97种设置中，在CRPS和MASE指标上显著领先于其他时间序列基础模型。&lt;h4&gt;结论&lt;/h4&gt;将LLM作为智能评判者来协调时间序列基础模型集成是一种有效的方法，能够显著提高预测性能，同时提供基于因果的解释，增强了模型的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型的激增创造了一种局面，没有单一方法能够持续表现出优越性，将核心挑战框定为寻找最佳模型，而是编排具有可解释性的最优集成。虽然大型语言模型(LLMs)提供了强大的推理能力，但它们在时间序列预测中的直接应用已被证明无效。我们通过将LLM重新定位为智能评判者来解决这一差距，该评判者评估、解释和战略性地协调基础模型集成。为了克服LLM在时间序列领域特定知识的固有缺乏，我们引入了一种R1风格的微调过程，由基于SHAP的忠实度分数指导，教会模型将集成权重解释为关于时间动态的有意义因果陈述。训练后的代理随后进行迭代的多轮对话，进行前瞻性评估，为其权重决策提供基于因果的解释，并自适应地优化策略。在GIFT-Eval基准上验证，涵盖23个数据集的97种设置，我们的方法在CRPS和MASE指标上显著领先于领先的时间序列基础模型，建立了新的最先进结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.</description>
      <author>example@mail.com (Defu Cao, Michael Gee, Jinbo Liu, Hengxuan Wang, Wei Yang, Rui Wang, Yan Liu)</author>
      <guid isPermaLink="false">2512.16022v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Are vision-language models ready to zero-shot replace supervised classification models in agriculture?</title>
      <link>http://arxiv.org/abs/2512.15977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Draft version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了视觉语言模型(VLMs)在农业分类任务中的表现，发现当前现成的VLMs不适合作为独立农业诊断系统，但可作为辅助组件与特定策略结合使用。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)被越来越多地提议作为视觉识别任务的通用解决方案，但它们在农业决策支持中的可靠性仍不清楚。&lt;h4&gt;目的&lt;/h4&gt;评估VLMs在农业分类任务中的适用性和性能，了解它们是否适合作为农业诊断系统。&lt;h4&gt;方法&lt;/h4&gt;在AgML集合中的27个农业分类数据集上测试多种开源和闭源VLMs，涵盖162个类别；比较零样本VLMs与监督基线(YOLO11)的性能；测试多选题和开放式提示方法；应用LLM-based语义判断；进行任务级别分析。&lt;h4&gt;主要发现&lt;/h4&gt;零样本VLMs显著低于监督基线；多选题提示下最佳模型(Gemini-3 Pro)达62%准确率，开放式提示低于25%；语义判断可提高开放式准确率；开源模型中Qwen-VL-72B表现最佳；植物分类比虫害识别更容易。&lt;h4&gt;结论&lt;/h4&gt;当前现成的VLMs不适合作为独立农业诊断系统，但当与受限界面、显式标签本体和领域感知评估策略结合时，可作为有效辅助组件。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)越来越多地被提议作为视觉识别任务的通用解决方案，但它们在农业决策支持中的可靠性仍不明确。我们在AgML集合中的27个农业分类数据集上对多种开源和闭源VLMs进行了基准测试，涵盖162个类别，包括植物病害、虫害和损害以及植物和杂草物种识别。在所有任务中，零样本VLMs的表现显著低于监督任务特定基线(YOLO11)，后者始终比任何基础模型获得明显更高的准确率。在多选题提示下，表现最佳的VLM(Gemini-3 Pro)达到约62%的平均准确率，而开放式提示的表现要低得多，原始准确率通常低于25%。应用基于LLM的语义判断提高了开放式准确率(例如，顶级模型从21%提高到30%)并改变了模型排名，证明评估方法显著影响报告的结论。在开源模型中，Qwen-VL-72B表现最好，在受限提示下接近闭源性能，但仍落后于顶级专有系统。任务级别分析显示，植物和杂草物种分类始终比虫害和损害识别更容易，后者是所有模型中最具挑战性的类别。总体而言，这些结果表明当前的现成VLMs还不适合作为独立的农业诊断系统，但当与受限界面、显式标签本体和领域感知评估策略配对时，可以作为辅助组件发挥作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.</description>
      <author>example@mail.com (Earl Ranario, Mason J. Earles)</author>
      <guid isPermaLink="false">2512.15977v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning</title>
      <link>http://arxiv.org/abs/2512.15943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了通过优化小型语言模型(SLM)替代大型语言模型(LLM)驱动的工作流程，以实现生成式AI的成本优化和运营效率提升。研究训练了一个领域适应的SLM来执行传统上由LLM处理的任务，实验结果显示微调后的SLM在ToolBench评估中取得77.55%的通过率，显著优于所有基线模型。&lt;h4&gt;背景&lt;/h4&gt;随着组织扩展采用生成式AI，模型成本优化和运营效率已成为决定可持续性和可访问性的关键因素。大型语言模型虽然在各种任务上展现出令人印象深刻的能力，但其广泛的计算需求使其成为常规企业使用的成本障碍，这促使研究人员探索小型语言模型。&lt;h4&gt;目的&lt;/h4&gt;探索用优化的小型语言模型替代大型语言模型驱动的工作流程的可行性，训练一个领域适应的SLM来执行传统上由LLM处理的代表性任务，并评估其性能。&lt;h4&gt;方法&lt;/h4&gt;采用领域适应方法训练小型语言模型，使用facebook/opt-350m模型仅通过一个epoch进行微调，采用Hugging Face TRL中的监督微调训练器。该模型执行文档摘要、查询回答和结构数据解释等任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，微调后的SLM在ToolBench评估中取得了77.55%的通过率，显著优于所有基线模型，包括ChatGPT-CoT(26.00%)、ToolLLaMA-DFS(30.18%)和ToolLLaMA-CoT(16.27%)。这表明即使在350M参数规模，模型也可以对指令微调管道做出有意义贡献。&lt;h4&gt;结论&lt;/h4&gt;小型语言模型的精心设计和针对性训练可以显著降低采用障碍，使生成式AI能够以经济有效的方式大规模集成到生产系统中。&lt;h4&gt;翻译&lt;/h4&gt;随着组织扩展采用生成式AI，模型成本优化和运营效率已成为决定可持续性和可访问性的关键因素。虽然大型语言模型在各种任务上展现出令人印象深刻的能力，但其广泛的计算需求使其成为常规企业使用的成本障碍。这一局限性促使研究人员探索小型语言模型，它们可以在特定应用中提供可比较的性能，同时显著降低基础设施开销。在本工作中，我们研究了用优化的小型语言模型替代大型语言模型驱动的工作流程的可行性。我们训练了一个领域适应的SLM来执行传统上由LLM处理的代表性任务，如文档摘要、查询回答和结构数据解释。作为实验的一部分，我们使用Hugging Face TRL研究了facebook/opt-350m模型的微调，特别是监督微调训练器。实验结果表明，我们微调的SLM在ToolBench评估中取得了77.55%的通过率，显著优于所有基线模型。这些发现强调，小型语言模型的精心设计和针对性训练可以显著降低采用障碍，使生成式AI能够以经济有效的方式大规模集成到生产系统中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.</description>
      <author>example@mail.com (Polaris Jhandi, Owais Kazi, Shreyas Subramanian, Neel Sendas)</author>
      <guid isPermaLink="false">2512.15943v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research</title>
      <link>http://arxiv.org/abs/2512.15931v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, accepted at the 3rd Workshop on Imageomics: Discovering Biological Knowledge from Images Using AI (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了BarcodeMamba+，一种基于状态空间模型架构的真菌条形码分类基础模型，采用预训练和微调范式，结合多种增强技术解决了真菌分类中的挑战。&lt;h4&gt;背景&lt;/h4&gt;从DNA条形码进行准确分类是全球生物多样性监测的基础，但真菌因标记稀疏和长尾分类群分布而面临极大挑战。传统监督学习方法难以泛化到未见物种且无法捕捉数据层次结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的基础模型解决真菌分类挑战，特别是在标记稀疏环境下提高分类性能并捕捉数据层次性质。&lt;h4&gt;方法&lt;/h4&gt;构建BarcodeMamba+基础模型；采用预训练和微调范式利用部分标记数据；在微调过程中整合层次标签平滑、加权损失函数和来自MycoAI的多头输出层等增强技术。&lt;h4&gt;主要发现&lt;/h4&gt;预训练和微调范式在数据稀疏环境中比传统全监督方法更有效；每种增强技术都带来显著性能提升；在具有挑战性的真菌分类基准测试上，最终模型在各种分类水平上都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该研究为基于基因组的生物多样性研究提供了强大新工具，并为这一具有挑战性的领域建立了有效且可扩展的训练范式。&lt;h4&gt;翻译&lt;/h4&gt;从DNA条形码进行准确的分类是全球生物多样性监测的基石，但由于标记稀疏和长尾分类群分布，真菌面临着极端挑战。传统的监督学习方法在这个领域往往表现不佳，难以泛化到未见过的物种，也无法捕捉数据的层次性质。为解决这些限制，我们引入了BarcodeMamba+，这是一种基于强大高效的状态空间模型架构构建的真菌条形码分类基础模型。我们采用预训练和微调范式，利用部分标记数据，并证明在数据稀疏环境中，这比传统全监督方法更有效。在微调过程中，我们系统地整合和评估了一系列增强技术，包括层次标签平滑、加权损失函数以及来自MycoAI的多头输出层，以专门解决真菌分类学的挑战。我们的实验表明，每个组件都带来了显著的性能提升。在一个具有挑战性的真菌分类基准测试上，该基准测试的训练集与测试集之间存在明显的分类分布差异，我们的最终模型在各种分类水平上都优于一系列现有方法。我们的工作为基于基因组的生物多样性研究提供了强大的新工具，并为这一具有挑战性的领域建立了有效且可扩展的训练范式。我们的代码已在https://github.com/bioscan-ml/BarcodeMamba上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba.</description>
      <author>example@mail.com (Tiancheng Gao, Scott C. Lowe, Brendan Furneaux, Angel X Chang, Graham W. Taylor)</author>
      <guid isPermaLink="false">2512.15931v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2512.15885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了JARVIS，一种受JEPA启发的框架，用于提高多模态大语言模型(MLLMs)的视觉理解能力，特别是在基础视觉推理任务方面。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在连接视觉和语言方面展现了令人印象深刻的能力，但在基础视觉推理任务方面的能力仍然有限。&lt;h4&gt;目的&lt;/h4&gt;解决MLLMs在视觉理解方面的局限性，提高其在视觉推理任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;引入JARVIS框架，将I-JEPA学习范式整合到MLLMs训练的标准视觉语言对齐流程中。利用冻结的视觉基础模型作为上下文和目标编码器，训练LLM的早期层作为预测器，从图像中学习结构和语义规律，减少对语言监督的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;在标准MLLM基准上的大量实验表明，JARVIS在不同LLM家族的以视觉为中心的基准上一致提高了性能，同时没有降低多模态推理能力。&lt;h4&gt;结论&lt;/h4&gt;JARVIS框架有效提升了MLLMs的视觉理解能力，特别是在视觉推理任务方面，为多模态模型的发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)最近在连接视觉和语言方面展示了令人印象深刻的能力，然而它们在基础视觉推理任务方面的熟练程度仍然有限。这种限制可以归因于MLLMs主要从文本描述中学习视觉理解，而文本描述构成了主观且本质上不完整的监督信号。此外，与大规模仅文本预训练相比，多模态指令调整的规模较小，导致MLLMs过度拟合语言先验而忽视视觉细节。为了解决这些问题，我们引入了JARVIS，一种受JEPA启发的框架，用于MLLMs中的自监督视觉增强。具体来说，我们将I-JEPA学习范式整合到MLLMs训练的标准视觉语言对齐流程中。我们的方法利用冻结的视觉基础模型作为上下文和目标编码器，同时训练预测器(实现为LLM的早期层)，从图像中学习结构和语义规律，而不完全依赖语言监督。在标准MLLM基准上的大量实验表明，JARVIS在不同LLM家族的以视觉为中心的基准上一致提高了性能，同时没有降低多模态推理能力。我们的源代码已在https://github.com/aimagelab/JARVIS公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.</description>
      <author>example@mail.com (Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Pier Luigi Dovesi, Shaghayegh Roohi, Mark Granroth-Wilding, Rita Cucchiara)</author>
      <guid isPermaLink="false">2512.15885v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Reusable theory representations for colliders: a demonstrator SMEFT foundation model</title>
      <link>http://arxiv.org/abs/2512.15862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 12 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文开发了一个用于探索标准模型有效场理论的基础模型，通过对比表示理论模拟的中性电流Drell-Yan截面构建而成。&lt;h4&gt;背景&lt;/h4&gt;在高能对撞机上进行新物理搜索需要有效的理论分析方法，标准模型有效场理论(SMEFT)是描述超出标准模型物理的理论框架。&lt;h4&gt;目的&lt;/h4&gt;开发一个可重用、物理对齐的基础表示，用于在高能对撞机上探索新物理理论，特别是通过分析SMEFT引起的Drell-Yan谱变形。&lt;h4&gt;方法&lt;/h4&gt;通过Warsaw基中维度-6 Wilson系数空间的受控采样生成高分辨率微分分布，使用监督对比损失训练最小参数化编码器网络，创建低维潜在流形来表示SMEFT引起的变形。&lt;h4&gt;主要发现&lt;/h4&gt;潜在方向与特征性的SMEFT形状畸变相关；嵌入中的簇对应于具有相似现象学影响的Wilson系数配置族；学习到的表示支持分类、异常检测和最近邻检索等下游任务。&lt;h4&gt;结论&lt;/h4&gt;这项研究为高能对撞机上的新物理搜索理论提供了基础表示的第一步，尽管目前仅限于leading-order SMEFT和简化的不确定性建模。&lt;h4&gt;翻译&lt;/h4&gt;我们为对撞机尺度的标准模型有效场理论(SMEFT)探索开发了一个演示基础模型，该模型由理论模拟的中性电流Drell-Yan截面的对比表示构建而成。通过在Warsaw基维度-6 Wilson系数空间O(Λ^-2)阶进行受控采样，我们生成了m_ℓℓ和p_T中的高分辨率微分分布，并添加了具有相关不确定性的物理动机蒙特卡罗副本。使用具有监督对比损失的最小参数化编码器网络进行训练，以产生低维潜在流形，SMEFT引起的Drell-Yan谱变形在此流形上获得明确的几何结构。我们分析得到的嵌入并证明：(i)潜在方向与特征性的SMEFT形状畸变相关，包括能量增长的四费米子贡献和电弱顶点修正；(ii)嵌入中的簇对应于具有相似现象学影响的Wilson系数配置族；(iii)学习到的表示支持带不确定性量化的分类、异常检测和最近邻检索等下游任务。虽然仅限于leading-order SMEFT和简化的不确定性建模，这项研究为高能对撞机上新物理搜索理论提供了第一个可重用、物理对齐的基础表示步骤。我们概述了向完整全局分析扩展的方案，包括多进程训练语料库、高阶修正和多目标预训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We develop a demonstrator foundation model for collider-scale explorations of the Standard Model Effective Field Theory (SMEFT), constructed from contrastive representations of theoretically simulated neutral-current Drell-Yan cross sections. Using a controlled sampling of the Warsaw-basis dimension-6 Wilson-coefficient space at $O(Λ^{-2})$, we generate a corpus of high-resolution differential distributions in $m_{\ell\ell}$ and $p_{T}$, augmented by physics-motivated Monte Carlo replicas with correlated uncertainties. A minimally parameterized encoder network is trained with a supervised contrastive loss to produce a low-dimensional latent manifold on which SMEFT-induced deformations of the Drell-Yan spectrum acquire a well-defined geometric structure. We analyze the resulting embedding and demonstrate that (i) latent directions correlate with characteristic SMEFT shape distortions, including energy-growing four-fermion contributions and electroweak vertex corrections; (ii) clusters in the embedding correspond to families of Wilson-coefficient configurations with similar phenomenological impact; and (iii) the learned representation supports downstream tasks such as classification with uncertainty quantification, anomaly detection, and nearest-neighbor retrieval. While restricted to leading-order SMEFT and simplified uncertainty modeling, this study provides the first step toward a reusable, physics-aligned foundational representation for the theory of New-Physics searches at high-energy colliders. We outline extensions towards a complete global analyses, including multi-process training corpora, higher-order corrections, and multi-objective pretraining.</description>
      <author>example@mail.com (Supratim Das Bakshi, T. J. Hobbs, Brandon Kriesten)</author>
      <guid isPermaLink="false">2512.15862v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Large Video Planner Enables Generalizable Robot Control</title>
      <link>http://arxiv.org/abs/2512.15840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了一种替代范式，使用大规模视频预训练作为构建机器人基础模型的主要模态，而非传统的多模态大语言模型扩展方法。研究团队收集了互联网规模的视频数据集，训练了生成式机器人规划模型，并通过真实机器人实验验证了其任务泛化能力和物理执行可行性。&lt;h4&gt;背景&lt;/h4&gt;通用机器人需要能在不同任务和环境中泛化的决策模型。最近的工作通过将多模态大语言模型(MLLMs)扩展为动作输出来构建机器人基础模型，创建视觉-语言-动作(VLA)系统，这些工作基于MLLMs的大规模语言和图像预训练可以有效地转移到动作输出模态的直觉。&lt;h4&gt;目的&lt;/h4&gt;探索使用大规模视频预训练作为构建机器人基础模型的主要模态的替代范式，而非依赖静态图像和语言。&lt;h4&gt;方法&lt;/h4&gt;收集互联网规模的人类活动和任务演示视频数据集；首次在基础模型规模上训练开放视频模型用于生成式机器人规划；生成零样本视频计划并后处理提取可执行机器人动作；通过第三方选择的野外任务和真实机器人实验评估任务级别的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够成功执行物理任务；展示了强大的指令跟随能力、强大的泛化能力和现实世界的可行性。&lt;h4&gt;结论&lt;/h4&gt;视频预训练可以作为构建机器人基础模型的有效替代方法；研究团队发布了模型和数据集以支持开放、可复现的基于视频的机器人学习。&lt;h4&gt;翻译&lt;/h4&gt;通用机器人需要能在不同任务和环境中泛化的决策模型。最近的工作通过将多模态大语言模型(MLLMs)扩展为动作输出来构建机器人基础模型，创建视觉-语言-动作(VLA)系统。这些工作的动机是直觉认为MLLMs的大规模语言和图像预训练可以有效地转移到动作输出模态。在这项工作中，我们探索了使用大规模视频预训练作为构建机器人基础模型的主要模态的替代范式。与静态图像和语言不同，视频捕获了物理世界中状态和动作的时空序列，这些序列与机器人行为自然对齐。我们收集了一个互联网规模的人类活动和任务演示视频数据集，并首次在基础模型规模上训练了一个开放视频模型，用于生成式机器人规划。该模型为新颖场景和任务生成零样本视频计划，我们对其进行后处理以提取可执行的机器人动作。我们通过第三方选择的野外任务和真实机器人实验评估任务级别的泛化能力，展示了成功的物理执行。总之，这些结果表明了强大的指令跟随能力、强大的泛化能力和现实世界的可行性。我们发布了模型和数据集以支持开放、可复现的基于视频的机器人学习。我们的网站可在https://www.boyuan.space/large-video-planner/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.</description>
      <author>example@mail.com (Boyuan Chen, Tianyuan Zhang, Haoran Geng, Kiwhan Song, Caiyi Zhang, Peihao Li, William T. Freeman, Jitendra Malik, Pieter Abbeel, Russ Tedrake, Vincent Sitzmann, Yilun Du)</author>
      <guid isPermaLink="false">2512.15840v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models in Biomedical Imaging: Turning Hype into Reality</title>
      <link>http://arxiv.org/abs/2512.15808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 figures and 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基础模型正在推动人工智能在生物医学成像等领域的显著变革，但临床评估和部署面临重大挑战。文章评估了当前技术状态，提供了推理分类法，讨论了因果推断的重要性，以及部署中的信任、偏见和安全问题。未来发展方向是开发混合的、具有因果意识且可验证安全的系统，增强而非替代人类专业知识。&lt;h4&gt;背景&lt;/h4&gt;基础模型正在推动人工智能在生物医学成像等不同领域的变革，这些模型旨在超越狭隘的模式识别，模拟复杂的临床推理、理解复杂的空间关系，并以前所未有的灵活性整合多模态数据。然而，这种潜力与现实之间存在显著差距。&lt;h4&gt;目的&lt;/h4&gt;批判性评估基础模型在生物医学领域的现状，分析其核心能力和局限性；提供推理分类法，评估这些模型是否表现出真正的认知或仅模仿表面模式；讨论部署中的关键问题，如可信度、偏见和安全；呼吁更包容、严谨且临床相关的验证框架。&lt;h4&gt;方法&lt;/h4&gt;通过批判性评估当前技术状态，分析炒作现象；提供从模拟顺序逻辑和空间理解到显式符号知识整合的推理分类法；讨论因果推断的重要性；剖析部署中的信任、偏见和安全问题。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在生物医学领域的临床评估和部署面临重大挑战；需要超越统计相关性，追求因果推断；部署中存在算法偏见、数据偏见和隐私以及模型幻觉等关键问题；需要更包容、严谨且临床相关的验证框架。&lt;h4&gt;结论&lt;/h4&gt;虽然自主AI医生的愿景仍然遥远，但当前现实是出现了将受益于临床实践的有力技术和辅助工具。基础模型在生物医学成像的未来不仅取决于规模，还取决于开发混合的、具有因果意识且可验证安全的系统，增强而非替代人类专业知识。&lt;h4&gt;翻译&lt;/h4&gt;基础模型正在推动人工智能在不同领域（包括生物医学成像）的重大转变。这些模型旨在超越狭隘的模式识别，模拟复杂的临床推理，理解复杂的空间关系，并以前所未有的灵活性整合多模态数据。然而，这种潜力与现实之间存在显著差距，基础模型的临床评估和部署受到重大挑战的阻碍。在此，我们批判性评估当前技术状态，通过检查基础模型在生物医学领域的核心能力和局限性来分析炒作现象。我们还提供了推理分类法，范围从模拟顺序逻辑和空间理解到显式符号知识的整合，以评估这些模型是否表现出真正的认知或仅模仿表面模式。我们认为，一个关键的前沿领域在于超越统计相关性，追求因果推断，这对于构建理解因果关系和效果的稳健模型至关重要。此外，我们讨论了源于可信度、偏见和安全性的部署关键问题，剖析了算法偏见、数据偏见和隐私以及模型幻觉的挑战。我们还呼吁需要更包容、严谨且临床相关的验证框架，以确保其安全且合乎道德的应用。我们得出结论，虽然自主AI医生的愿景仍然遥远，但当前现实是出现了将受益于临床实践的有力技术和辅助工具。基础模型在生物医学成像的未来不仅取决于规模，还取决于开发混合的、具有因果意识且可验证安全的系统，增强而非替代人类专业知识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) are driving a prominent shift in artificial intelligence across different domains, including biomedical imaging. These models are designed to move beyond narrow pattern recognition towards emulating sophisticated clinical reasoning, understanding complex spatial relationships, and integrating multimodal data with unprecedented flexibility. However, a critical gap exists between this potential and the current reality, where the clinical evaluation and deployment of FMs are hampered by significant challenges. Herein, we critically assess the current state-of-the-art, analyzing hype by examining the core capabilities and limitations of FMs in the biomedical domain. We also provide a taxonomy of reasoning, ranging from emulated sequential logic and spatial understanding to the integration of explicit symbolic knowledge, to evaluate whether these models exhibit genuine cognition or merely mimic surface-level patterns. We argue that a critical frontier lies beyond statistical correlation, in the pursuit of causal inference, which is essential for building robust models that understand cause and effect. Furthermore, we discuss the paramount issues in deployment stemming from trustworthiness, bias, and safety, dissecting the challenges of algorithmic bias, data bias and privacy, and model hallucinations. We also draw attention to the need for more inclusive, rigorous, and clinically relevant validation frameworks to ensure their safe and ethical application. We conclude that while the vision of autonomous AI-doctors remains distant, the immediate reality is the emergence of powerful technology and assistive tools that would benefit clinical practice. The future of FMs in biomedical imaging hinges not on scale alone, but on developing hybrid, causally aware, and verifiably safe systems that augment, rather than replace, human expertise.</description>
      <author>example@mail.com (Amgad Muneer, Kai Zhang, Ibraheem Hamdi, Rizwan Qureshi, Muhammad Waqas, Shereen Fouad, Hazrat Ali, Syed Muhammad Anwar, Jia Wu)</author>
      <guid isPermaLink="false">2512.15808v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Next-Embedding Prediction Makes Strong Vision Learners</title>
      <link>http://arxiv.org/abs/2512.16922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://sihanxu.me/nepa&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为NEPA的自监督视觉学习方法，通过预测未来嵌入而非传统方法中的特征表示，实现了简单而有效的视觉模型预训练。&lt;h4&gt;背景&lt;/h4&gt;受自然语言生成式预训练成功的启发，探索相同原则是否能应用于视觉领域。&lt;h4&gt;目的&lt;/h4&gt;探索从学习表征到学习模型的转变，开发一种简单、可扩展的自监督视觉学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出Next-Embedding Predictive Autoregression (NEPA)，训练模型基于过去的patch嵌入预测未来的patch嵌入，使用因果掩码和stop gradient技术。仅使用next embedding prediction作为学习目标，无需像素重建、离散token、对比损失或任务特定头。&lt;h4&gt;主要发现&lt;/h4&gt;简单的Transformer在ImageNet-1k上仅以next embedding prediction为学习目标进行预训练是有效的。NEPA在ImageNet-1K上达到83.8%和85.3%的top-1准确率，并能有效转移到ADE20K上的语义分割任务。&lt;h4&gt;结论&lt;/h4&gt;生成式预训练从嵌入提供了一种简单、可扩展且可能独立于模态的视觉自监督学习替代方案。&lt;h4&gt;翻译&lt;/h4&gt;受自然语言生成式预训练成功的启发，我们探索相同原则是否能产生强大的自监督视觉学习器。我们训练模型生成嵌入来直接执行预测任务，而不是输出特征供下游使用。这项工作探索了从学习表征到学习模型的转变。具体来说，模型学习使用因果掩码和stop gradient基于过去的patch嵌入预测未来的patch嵌入，我们称之为Next-Embedding Predictive Autoregression (NEPA)。我们证明，在ImageNet-1k上仅以next embedding prediction为唯一学习目标进行预训练的简单Transformer是有效的-不需要像素重建、离散token、对比损失或任务特定头。这种 formulation保留了架构简单性和可扩展性，不需要额外的设计复杂性。NEPA在多个任务上取得了良好的结果，使用ViT-B和ViT-L骨干网络在ImageNet-1K上微调后达到83.8%和85.3%的top-1准确率，并能有效转移到ADE20K上的语义分割。我们相信从嵌入进行生成式预训练为视觉自监督学习提供了一种简单、可扩展且可能独立于模态的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.</description>
      <author>example@mail.com (Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin, Joyce Chai, Saining Xie, Stella X. Yu)</author>
      <guid isPermaLink="false">2512.16922v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>InfoDCL: Informative Noise Enhanced Diffusion Based Contrastive Learning</title>
      <link>http://arxiv.org/abs/2512.16576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为InfoDCL的新型基于扩散的对比学习框架，用于解决推荐系统中现有方法无法捕获足够语义信息的问题。&lt;h4&gt;背景&lt;/h4&gt;对比学习在推荐系统中显示出良好的潜力，但现有方法通常通过随机扰动原始交互图构建更稀疏视图，由于不了解真实用户偏好和推荐数据的稀疏性，只能捕获不足的语义信息。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在推荐系统中捕获语义信息不足的问题，提高推荐性能。&lt;h4&gt;方法&lt;/h4&gt;提出InfoDCL框架，采用单步扩散过程将噪声与辅助语义信息集成，生成真实的用户偏好作为对比视图；构建协作训练目标策略将生成和偏好学习之间的干扰转化为协作；仅在推理阶段使用多层GCN融入高阶共现信息，同时保持训练效率。&lt;h4&gt;主要发现&lt;/h4&gt;在五个真实世界数据集上的大量实验表明，InfoDCL显著优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;InfoDCL为提高推荐性能提供了有效的解决方案，并为在对比学习框架中应用扩散方法提供了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;对比学习在推荐系统中显示出良好的潜力。现有方法通常通过随机扰动原始交互图构建更稀疏的视图，因为它们不了解真实的用户偏好。由于推荐数据的稀疏性，这种范式只能捕获不足的语义信息。为解决这一问题，我们提出了InfoDCL，一种用于推荐的新型基于扩散的对比学习框架。我们不是注入随机采样的高斯噪声，而是采用单步扩散过程，将噪声与辅助语义信息集成以生成信号，并将它们输入到标准扩散过程中，生成真实的用户偏好作为对比视图。此外，基于对InfoDCL中生成和偏好学习之间相互影响的全面分析，我们构建了协作训练目标策略，将它们之间的干扰转化为相互协作。此外，我们仅在推理阶段使用多层GCN，以融入高阶共现信息，同时保持训练效率。在五个真实世界数据集上的大量实验表明，InfoDCL显著优于最先进的方法。我们的InfoDCL为提高推荐性能提供了有效的解决方案，并为在对比学习框架中应用扩散方法提出了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning has demonstrated promising potential in recommender systems. Existing methods typically construct sparser views by randomly perturbing the original interaction graph, as they have no idea about the authentic user preferences. Owing to the sparse nature of recommendation data, this paradigm can only capture insufficient semantic information. To address the issue, we propose InfoDCL, a novel diffusion-based contrastive learning framework for recommendation. Rather than injecting randomly sampled Gaussian noise, we employ a single-step diffusion process that integrates noise with auxiliary semantic information to generate signals and feed them to the standard diffusion process to generate authentic user preferences as contrastive views. Besides, based on a comprehensive analysis of the mutual influence between generation and preference learning in InfoDCL, we build a collaborative training objective strategy to transform the interference between them into mutual collaboration. Additionally, we employ multiple GCN layers only during inference stage to incorporate higher-order co-occurrence information while maintaining training efficiency. Extensive experiments on five real-world datasets demonstrate that InfoDCL significantly outperforms state-of-the-art methods. Our InfoDCL offers an effective solution for enhancing recommendation performance and suggests a novel paradigm for applying diffusion method in contrastive learning frameworks.</description>
      <author>example@mail.com (Xufeng Liang, Zhida Qin, Chong Zhang, Tianyu Huang, Gangyi Ding)</author>
      <guid isPermaLink="false">2512.16576v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>BrepLLM: Native Boundary Representation Understanding with Large Language Models</title>
      <link>http://arxiv.org/abs/2512.16413v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BrepLLM是一种新型框架，使大语言模型能够解析和推理原始3D边界表示数据，弥合结构化3D几何与自然语言之间的模态差距。&lt;h4&gt;背景&lt;/h4&gt;当前基于token序列的大语言模型不适合直接处理包含复杂几何和拓扑信息的3D边界表示模型，存在结构化3D几何与自然语言之间的模态差距。&lt;h4&gt;目的&lt;/h4&gt;提出BrepLLM框架，使大语言模型能够直接处理和理解3D边界表示数据，实现3D几何与自然语言之间的有效沟通。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段训练流程：第一阶段使用自适应UV采样策略将Brep转换为图表示，设计分层BrepEncoder提取特征并通过对比学习与文本嵌入对齐；第二阶段将预训练的BrepEncoder集成到LLM中，使用三阶段渐进式训练策略对节点token序列进行对齐，并构建了包含269,444个Brep-文本问答对的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，BrepLLM在3D物体分类和字幕任务上取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;BrepLLM成功地使大语言模型能够处理和理解3D边界表示数据，有效地连接了3D几何表示和自然语言理解。&lt;h4&gt;翻译&lt;/h4&gt;当前基于token序列的大语言模型不太适合直接处理包含复杂几何和拓扑信息的3D边界表示模型。我们提出了BrepLLM，这是第一个使大语言模型能够解析和推理原始Brep数据的框架，弥合了结构化3D几何与自然语言之间的模态差距。BrepLLM采用两阶段训练流程：跨模态对齐预训练和多阶段LLM微调。在第一阶段，自适应UV采样策略将Brep转换为包含几何和拓扑信息的图表示。我们设计了一个分层BrepEncoder从几何(即面和边)和拓扑中提取特征，生成单个全局token和一系列节点token。然后通过对比学习将全局token与冻结的CLIP文本编码器(ViT-L/14)的文本嵌入对齐。在第二阶段，我们将预训练的BrepEncoder集成到LLM中。然后使用三阶段渐进式训练策略对其节点token序列进行对齐：(1)训练基于MLP的从Brep表示到2D的语义映射，利用2D-LLM先验。(2)对LLM进行微调。(3)设计查询专家混合(MQE)以增强几何多样性建模。我们还构建了Brep2Text数据集，包含269,444个Brep-文本问答对。实验表明，BrepLLM在3D物体分类和字幕任务上取得了最先进的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决现有基于标记序列的大语言模型无法直接处理包含复杂几何和拓扑信息的3D边界表示（Brep）模型的问题。这一问题在工业设计中至关重要，因为CAD模型由Brep精确定义，直接理解Brep将推动工业模型设计和智能制造的重大进步，使AI系统能够真正理解CAD模型的几何结构和拓扑关系，而不仅仅是生成命令序列。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法只能通过生成CAD命令序列间接处理Brep，无法捕获固有形状结构。他们借鉴了CLIP的跨模态对齐思想，将其应用于几何-语言对齐；参考了BLIP-2的Q-Former架构处理视觉-语言对齐；并受到PointLLM和ShapeLLM等3D-LLM工作的启发，但针对Brep数据进行了专门改进。作者采用两阶段训练管道，先进行跨模态对齐预训练，再进行多阶段LLM微调，以实现Brep与自然语言的深度融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使大语言模型能够直接解析和推理原始Brep数据，弥合结构化3D几何和自然语言之间的模态差距。整体流程分为两个阶段：1）跨模态对齐预训练：使用自适应UV采样将Brep转换为图表示，通过分层BrepEncoder提取几何和拓扑特征，生成全局标记和节点标记序列，然后使用对比学习将全局标记与CLIP文本嵌入对齐；2）多阶段LLM微调：先训练Brep表示到2D的语义映射，再微调LLM，最后引入混合查询专家（MQE）增强几何多样性建模。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首个使大语言模型直接解析原始Brep模型的框架；2）自适应UV采样策略和分层BrepEncoder实现多粒度几何和拓扑特征表示；3）三阶段策略将2D-LLM先验适应于Brep领域，形成残差MQE；4）建立首个Brep为中心的语言理解任务的大规模基准（269,444个Brep-语言对）。相比之前工作，BrepLLM直接处理原始Brep数据而非命令序列，能够执行真正的几何和拓扑推理，同时使用分层编码器同时处理几何和拓扑信息，并通过MQE增强几何多样性建模。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BrepLLM首次实现了大语言模型对CAD边界表示数据的直接理解和推理，通过创新的分层编码器和三阶段训练策略，弥合了结构化3D几何与自然语言之间的模态差距，为工业设计和智能制造提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.</description>
      <author>example@mail.com (Liyuan Deng, Hao Guo, Yunpeng Bai, Yongkang Dai, Huaxi Huang, Yilei Shi)</author>
      <guid isPermaLink="false">2512.16413v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval</title>
      <link>http://arxiv.org/abs/2512.16294v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为多标签自适应对比学习(MACL)的方法，解决了多标签遥感图像检索中的语义重叠、标签分布不平衡和类间共现模式复杂等挑战。&lt;h4&gt;背景&lt;/h4&gt;土地覆盖类别间的语义重叠、高度不平衡的标签分布以及复杂的类间共现模式构成了多标签遥感图像检索的重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理语义不平衡问题的方法，实现常见和稀有类别之间的平衡表示学习。&lt;h4&gt;方法&lt;/h4&gt;提出多标签自适应对比学习(MACL)，作为对比学习的扩展，整合了标签感知采样、频率敏感加权和动态温度缩放技术。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集(DLRSD、ML-AID和WHDLD)上的大量实验表明，MACL持续优于基于对比损失的基线方法，有效减轻了语义不平衡问题，并在大规模遥感档案中提供了更可靠的检索性能。&lt;h4&gt;结论&lt;/h4&gt;MACL是一种有效的解决方案，能够应对多标签遥感图像检索中的挑战，作者将在论文接受后通过GitHub发布代码、预训练模型和评估脚本。&lt;h4&gt;翻译&lt;/h4&gt;土地覆盖类别之间的语义重叠、高度不平衡的标签分布以及复杂的类间共现模式构成了多标签遥感图像检索的重大挑战。本文引入了多标签自适应对比学习(MACL)，作为对比学习的扩展来解决这些问题。它整合了标签感知采样、频率敏感加权和动态温度缩放，以实现常见和稀有类别之间的平衡表示学习。在三个基准数据集(DLRSD、ML-AID和WHDLD)上的大量实验表明，MACL持续优于基于对比损失的基线方法，有效减轻了语义不平衡问题，并在大规模遥感档案中提供了更可靠的检索性能。代码、预训练模型和评估脚本将在接受后发布于https://github.com/amna/MACL。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.</description>
      <author>example@mail.com (Amna Amir, Erchan Aptoula)</author>
      <guid isPermaLink="false">2512.16294v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space</title>
      <link>http://arxiv.org/abs/2512.16133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to WACV 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种从单张图像自动检测牛群行为交互的方法及其应用，这对智能畜牧管理（如发情检测）至关重要。&lt;h4&gt;背景&lt;/h4&gt;人类行为交互检测已被积极研究，但牛的行为交互检测面临挑战，特别是缺乏包含交互行为的综合数据集，因为牛群交互是稀有事件。&lt;h4&gt;目的&lt;/h4&gt;开发一种数据高效的交互检测方法，用于智能畜牧管理。&lt;h4&gt;方法&lt;/h4&gt;提出CattleAct方法，将交互分解为个体牛的行为组合；从大规模牛行为数据集中学习动作潜在空间；使用对比学习微调预训练的潜在空间以嵌入稀有交互；构建动作和交互的统一潜在空间；开发整合视频和GPS输入的实用工作系统。&lt;h4&gt;主要发现&lt;/h4&gt;在商业牧场上的实验表明，该方法相比基线实现了准确的交互检测。&lt;h4&gt;结论&lt;/h4&gt;CattleAct方法有效解决了牛群交互检测的挑战，为智能畜牧管理提供了实用工具。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种从单张图像自动检测牛群行为交互的方法及其应用，这对畜牧业的智能畜牧管理（如发情检测）至关重要。尽管人类行为交互检测已被积极研究，但牛交互检测存在一个非平凡的挑战，特别是缺乏包含交互的综合行为数据集，因为牛群交互是稀有事件。因此，我们提出了CattleAct，一种通过将交互分解为个体牛的行为组合来实现数据高效的交互检测方法。具体来说，我们首先从大规模牛行为数据集中学习动作潜在空间。然后，我们通过使用对比学习微调预训练的潜在空间来嵌入稀有交互，从而构建动作和交互的统一潜在空间。在提出的方法基础上，我们开发了一个整合视频和GPS输入的实用工作系统。在商业牧场上的实验表明，与基线相比，我们的方法实现了准确的交互检测。我们的实现可在https://github.com/rakawanegan/CattleAct获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.</description>
      <author>example@mail.com (Ren Nakagawa, Yang Yang, Risa Shinoda, Hiroaki Santo, Kenji Oyama, Fumio Okura, Takenao Ohkawa)</author>
      <guid isPermaLink="false">2512.16133v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>From Minutes to Days: Scaling Intracranial Speech Decoding with Supervised Pretraining</title>
      <link>http://arxiv.org/abs/2512.15830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Linnea Evanson* and Mingfang (Lucy) Zhang* are joint first authors. Pierre Bourdillon** and Jean-Rémi King** are joint last authors&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了一种利用长期临床监测数据训练语音解码模型的新框架，显著提高了解码性能，并发现大脑活动存在跨日变异性。&lt;h4&gt;背景&lt;/h4&gt;传统的脑电活动解码语音研究依赖于在短暂且高度受控的实验中收集的有限神经记录数据，限制了模型的训练效果和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发能够利用长期临床监测数据训练语音解码模型的方法，提高解码性能，并探索大脑活动随时间的变异性。&lt;h4&gt;方法&lt;/h4&gt;引入一个框架，利用患者临床监测期间长达一周的颅内和音频记录，将训练数据集大小增加了两个数量级以上；使用对比学习模型进行预训练，并分析学习到的表示。&lt;h4&gt;主要发现&lt;/h4&gt;1. 使用预训练的对比学习模型显著优于仅使用传统实验数据训练的模型；2. 性能提升与数据集大小呈对数线性关系；3. 大脑活动虽然表示语音特征，但其全局结构在几天内会漂移；4. 需要明确考虑跨日变异性的模型。&lt;h4&gt;结论&lt;/h4&gt;该方法为在真实生活和受控任务环境中解码和建模大脑表征提供了一条可扩展的途径，为脑机接口和神经科学研究提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;从大脑活动中解码语音通常依赖于在短暂且高度受控的实验中收集的有限神经记录。在这里，我们引入了一个框架，利用接受临床监测的患者长达一周的颅内和音频记录，有效地将训练数据集的大小增加了两个数量级以上。通过这种预训练，我们的对比学习模型明显优于仅使用传统实验数据训练的模型，性能提升与数据集大小呈对数线性关系。对学习到的表示的分析表明，虽然大脑活动表示语音特征，但其全局结构在几天内会漂移，突显了需要明确考虑跨日变异性的模型。总体而言，我们的方法为在真实生活和受控任务环境中解码和建模大脑表征开辟了一条可扩展的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decoding speech from brain activity has typically relied on limited neural recordings collected during short and highly controlled experiments. Here, we introduce a framework to leverage week-long intracranial and audio recordings from patients undergoing clinical monitoring, effectively increasing the training dataset size by over two orders of magnitude. With this pretraining, our contrastive learning model substantially outperforms models trained solely on classic experimental data, with gains that scale log-linearly with dataset size. Analysis of the learned representations reveals that, while brain activity represents speech features, its global structure largely drifts across days, highlighting the need for models that explicitly account for cross-day variability. Overall, our approach opens a scalable path toward decoding and modeling brain representations in both real-life and controlled task settings.</description>
      <author>example@mail.com (Linnea Evanson, Mingfang, Zhang, Hubert Banville, Saarang Panchavati, Pierre Bourdillon, Jean-Rémi King)</author>
      <guid isPermaLink="false">2512.15830v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Dual-coding contrastive learning based on ConvNeXt and ViT models for morphological classification of galaxies in COSMOS-Web</title>
      <link>http://arxiv.org/abs/2512.15129v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in APJS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于对比学习的自监督方法来升级USmorph框架中的无监督机器学习部分，提高了星系形态分类的效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;作者之前提出了一个名为USmorph的机器学习框架用于星系形态分类，本研究是对该框架中无监督机器学习部分的升级。&lt;h4&gt;目的&lt;/h4&gt;提出一种自监督方法来升级USmorph框架中的无监督机器学习部分，以提高特征提取步骤的效率。&lt;h4&gt;方法&lt;/h4&gt;升级的UML方法包括：1)使用卷积自编码器去噪和自适应极坐标变换增强旋转不变性；2)采用基于ConvNeXt和ViT的预训练双编码器卷积神经网络进行图像编码，并用对比学习降低特征维度；3)使用基于Bagging的聚类模型对星系进行分组。应用于红移范围0.5 &lt; z &lt; 6.0的COSMOS-Web场域星系。&lt;h4&gt;主要发现&lt;/h4&gt;改进的UML方法成功分类了73%的星系，剩余27%使用GoogleNet算法分类。分类结果与其他星系形态参数比较显示与星系演化有良好一致性。&lt;h4&gt;结论&lt;/h4&gt;由于该算法具有更高的效率，非常适合应用于未来的中国空间望远镜任务。&lt;h4&gt;翻译&lt;/h4&gt;在我们之前的工作中，我们提出了一个名为USmorph的机器学习框架用于高效分类星系形态。在本研究中，我们提出了一种称为对比学习的自监督方法来升级USmorph框架中的无监督机器学习部分，旨在提高该步骤中特征提取的效率。升级的UML方法主要包括以下三个方面。(1)我们采用卷积自编码器对星系图像去噪，并使用自适应极坐标变换增强模型的旋转不变性。(2)使用基于ConvNeXt和ViT的预训练双编码器卷积神经网络对图像数据进行编码，然后应用对比学习降低特征维度。(3)我们采用基于Bagging的聚类模型将具有相似特征的星系聚集成不同的组。通过仔细划分红移区间，我们将该模型应用于红移范围0.5 &lt; z &lt; 6.0的COSMOS-Web场域星系的光学图像。与之前的算法相比，改进的UML方法成功分类了73%的星系。使用GoogleNet算法，我们对剩余27%的星系进行形态分类。为了验证更新算法的可靠性，我们将分类结果与其他星系形态参数进行了比较，发现与星系演化有良好的一致性。得益于其更高的效率，该更新算法非常适合应用于未来的中国空间望远镜任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In our previous works, we proposed a machine learning framework named \texttt{USmorph} for efficiently classifying galaxy morphology. In this study, we propose a self-supervised method called contrastive learning to upgrade the unsupervised machine learning (UML) part of the \texttt{USmorph} framework, aiming to improve the efficiency of feature extraction in this step. The upgraded UML method primarily consists of the following three aspects. (1) We employ a Convolutional Autoencoder to denoise galaxy images and the Adaptive Polar Coordinate Transformation to enhance the model's rotational invariance. (2) A pre-trained dual-encoder convolutional neural network based on ConvNeXt and ViT is used to encode the image data, while contrastive learning is then applied to reduce the dimension of the features. (3) We adopt a Bagging-based clustering model to cluster galaxies with similar features into distinct groups. By carefully dividing the redshift bins, we apply this model to the rest-frame optical images of galaxies in the COSMOS-Web field within the redshift range of $0.5 &lt; z &lt; 6.0$. Compared to the previous algorithm, the improved UML method successfully classifies 73\% galaxies. Using the GoogleNet algorithm, we classify the morphology of the remaining 27\% galaxies. To validate the reliability of our updated algorithm, we compared our classification results with other galaxy morphological parameters and found a good consistency with galaxy evolution. Benefiting from its higher efficiency, this updated algorithm is well-suited for application in future China Space Station Telescope missions.</description>
      <author>example@mail.com (Shiwei Zhu, Guanwen Fang, Chichun Zhou, Jie Song, Zesen Lin, Yao Dai, Xu Kong)</author>
      <guid isPermaLink="false">2512.15129v2</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>SARMAE: Masked Autoencoder for SAR Representation Learning</title>
      <link>http://arxiv.org/abs/2512.16635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code and models will be available at https://github.com/MiliLab/SARMAE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SARMAE，一种用于自监督SAR表征学习的噪声感知掩码自编码器，通过构建百万级SAR数据集和设计斑点感知表征增强方法，解决了SAR图像处理中的数据稀缺和斑点噪声挑战。&lt;h4&gt;背景&lt;/h4&gt;SAR图像在全天候、日夜遥感应用中起关键作用，但现有面向SAR的深度学习受限于数据稀缺，且SAR图像中固有的斑点噪声阻碍了细粒度语义表征学习。&lt;h4&gt;目的&lt;/h4&gt;解决SAR图像处理中的数据稀缺问题，应对斑点噪声挑战，实现噪声感知和鲁棒的表征学习。&lt;h4&gt;方法&lt;/h4&gt;1) 构建SAR-1M，首个百万级SAR数据集并配有配对光学图像；2) 设计斑点感知表征增强(SARE)，将SAR特定斑点噪声注入掩码自编码器；3) 引入语义锚定表征约束(SARC)，利用配对光学先验对齐SAR特征并确保语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个SAR数据集上的实验表明，SARMAE在分类、检测和分割任务上达到最先进性能，代码和模型已开源。&lt;h4&gt;结论&lt;/h4&gt;SARMAE有效解决了SAR图像处理中的数据稀缺和斑点噪声问题，通过自监督学习实现了高性能的SAR表征学习。&lt;h4&gt;翻译&lt;/h4&gt;合成孔径雷达(SAR)图像在全天候、日夜遥感应用中起着关键作用。然而，现有的面向SAR的深度学习受限于数据稀缺，而SAR图像中基于物理的斑点噪声进一步阻碍了细粒度语义表征学习。为应对这些挑战，我们提出了SARMAE，一种用于自监督SAR表征学习的噪声感知掩码自编码器。具体来说，我们构建了SAR-1M，首个百万级SAR数据集，并配有额外的配对光学图像，以实现大规模预训练。基于此，我们设计了斑点感知表征增强(SARE)，将SAR特定的斑点噪声注入掩码自编码器，以促进噪声感知和鲁棒的表征学习。此外，我们引入了语义锚定表征约束(SARC)，利用配对的光学先验对齐SAR特征并确保语义一致性。在多个SAR数据集上的广泛实验表明，SARMAE在分类、检测和分割任务上实现了最先进的性能。代码和模型将在https://github.com/MiliLab/SARMAE上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.</description>
      <author>example@mail.com (Danxu Liu, Di Wang, Hebaixu Wang, Haoyang Chen, Wentao Jiang, Yilin Cheng, Haonan Guo, Wei Cui, Jing Zhang)</author>
      <guid isPermaLink="false">2512.16635v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data</title>
      <link>http://arxiv.org/abs/2512.16277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种锐度感知的二阶潜在因子(SSLF)模型，通过Hessian-向量积获取二阶信息并将锐度项注入到曲率中，解决了传统SLF模型优化困难的问题，在多个工业数据集上表现出优于最先进基线的性能。&lt;h4&gt;背景&lt;/h4&gt;二阶潜在因子(SLF)模型作为一种低秩表示学习方法，能够从高维且不完整的数据中提取节点间交互模式，但其优化过程因双线性和非凸特性而异常困难。&lt;h4&gt;目的&lt;/h4&gt;解决SLF模型优化困难的问题，提高表示学习模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出锐度感知的SLF(SSLF)模型，包含两个关键思想：(1)通过Hessian-向量积获取二阶信息；(2)通过设计的Hessian-向量积将锐度项注入到曲率(Hessian)中。&lt;h4&gt;主要发现&lt;/h4&gt;在多个工业数据集上的实验表明，所提出的SSLF模型性能始终优于现有的最先进基线方法。&lt;h4&gt;结论&lt;/h4&gt;SSLF模型通过结合锐度感知最小化技术与二阶潜在因子模型，有效解决了传统SLF模型的优化难题，提高了表示学习效果。&lt;h4&gt;翻译&lt;/h4&gt;二阶潜在因子模型是一种低秩表示学习方法，已证明能够从高维且不完整的数据中提取节点间交互模式。然而，由于其双线性和非凸特性，其优化过程异常困难。锐度感知最小化最近被提出用于在最小化非凸目标时寻找平坦的局部最小值，从而提高表示学习模型的泛化能力。为了应对这一挑战，我们提出了一个锐度感知的二阶潜在因子模型。该模型体现了两个关键思想：(1)通过Hessian-向量积获取二阶信息；(2)通过设计的Hessian-向量积将锐度项注入到曲率中。在多个工业数据集上的实验表明，所提出的模型性能始终优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.</description>
      <author>example@mail.com (Jialiang Wang, Xueyan Bao, Hao Wu)</author>
      <guid isPermaLink="false">2512.16277v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification</title>
      <link>http://arxiv.org/abs/2512.16271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been published in the IEEE proceedings of the 8th International Conference of Computer and Informatics Engineering (IC2IE)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DACH-TIC模型，一种领域无关因果感知分层音频Transformer，用于稳健的婴儿哭声分类，通过集成因果注意力和多任务学习等方法，在准确率和泛化能力上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;准确且可解释的婴儿哭声分类对新生儿早期distress检测和临床决策支持至关重要，但现有深度学习方法易受噪声、伪线索和环境变化的影响。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健的婴儿哭声分类模型，能够抵抗环境变化和噪声干扰，同时保持高准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;DACH-TIC模型整合了因果注意力机制、分层表示学习、多任务监督和对抗性域泛化，使用结构化Transformer主干，结合局部令牌级和全局语义编码器，并通过因果注意力掩码和受控扰动训练来近似反事实声学变化。&lt;h4&gt;主要发现&lt;/h4&gt;DACH-TIC在Baby Chillanto和Donate-a-Cry数据集上表现优异，准确率提高2.6%，宏F1分数提高2.2点，同时增强了因果保真度；模型能有效泛化到未见过的声学环境，域性能差距仅为2.4%。&lt;h4&gt;结论&lt;/h4&gt;DACH-TIC模型适合应用于现实世界的新生儿声学监测系统，能够提供准确且可解释的婴儿哭声分类结果。&lt;h4&gt;翻译&lt;/h4&gt;准确且可解释的婴儿哭声音韵学分类对新生儿早期distress检测和临床决策支持至关重要。然而，许多现有的深度学习方法依赖于相关性驱动的声学表示，这使得它们容易受到噪声、伪线索和不同录制环境域转移的影响。我们提出了DACH-TIC，一种领域无关因果感知分层音频Transformer，用于稳健的婴儿哭声分类。该模型在一个统一框架内集成了因果注意力、分层表示学习、多任务监督和对抗性域泛化。DACH-TIC采用具有局部令牌级和全局语义编码器的结构化Transformer主干，通过因果注意力掩码和受控扰动训练来近似反事实声学变化。领域对抗性目标促进环境不变的表示，而多任务学习联合优化哭声类型识别、痛苦强度估计和因果相关性预测。该模型在Baby Chillanto和Donate-a-Cry数据集上进行了评估，并使用ESC-50环境噪声叠加进行域增强。实验结果表明，DACH-TIC优于最先进的基线方法，包括HTS-AT和SE-ResNet Transformer，在准确率上提高了2.6%，在宏F1分数上提高了2.2点，同时增强了因果保真度。模型能有效泛化到未见过的声学环境，域性能差距仅为2.4%，证明了其适用于现实世界的新生儿声学监测系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IC2IE67206.2025.11283358&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and interpretable classification of infant cry paralinguistics is essential for early detection of neonatal distress and clinical decision support. However, many existing deep learning methods rely on correlation-driven acoustic representations, which makes them vulnerable to noise, spurious cues, and domain shifts across recording environments. We propose DACH-TIC, a Domain-Agnostic Causal-Aware Hierarchical Audio Transformer for robust infant cry classification. The model integrates causal attention, hierarchical representation learning, multi-task supervision, and adversarial domain generalization within a unified framework.  DACH-TIC employs a structured transformer backbone with local token-level and global semantic encoders, augmented by causal attention masking and controlled perturbation training to approximate counterfactual acoustic variations. A domain-adversarial objective promotes environment-invariant representations, while multi-task learning jointly optimizes cry type recognition, distress intensity estimation, and causal relevance prediction. The model is evaluated on the Baby Chillanto and Donate-a-Cry datasets, with ESC-50 environmental noise overlays for domain augmentation.  Experimental results show that DACH-TIC outperforms state-of-the-art baselines, including HTS-AT and SE-ResNet Transformer, achieving improvements of 2.6 percent in accuracy and 2.2 points in macro-F1 score, alongside enhanced causal fidelity. The model generalizes effectively to unseen acoustic environments, with a domain performance gap of only 2.4 percent, demonstrating its suitability for real-world neonatal acoustic monitoring systems.</description>
      <author>example@mail.com (Geofrey Owino, Bernard Shibwabo Kasamani, Ahmed M. Abdelmoniem, Edem Wornyo)</author>
      <guid isPermaLink="false">2512.16271v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting</title>
      <link>http://arxiv.org/abs/2512.16046v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Big Data 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出CauStream框架，联合学习径流因果图和路由图，实现可解释且泛化能力强的径流预测，在美国三个主要流域的实验中优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;径流预测对水资源管理和风险缓解至关重要。现有深度学习模型忽略物理过程，限制可解释性；因果学习方法虽整合领域知识，但依赖固定因果图无法适应数据变化。&lt;h4&gt;目的&lt;/h4&gt;开发CauStream框架，解决现有方法中固定因果图无法适应数据的问题，提高模型可解释性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;CauStream联合学习(i)气象强迫因素间的径流因果图，和(ii)捕捉站点间动态依赖关系的路由图；在非参数条件下建立因果结构可识别性条件；在美国三个主要流域的三个预测时间跨度上评估。&lt;h4&gt;主要发现&lt;/h4&gt;模型持续优于最先进方法，长期预测窗口上性能差距扩大，表明泛化能力强；学习到的因果图与领域知识高度一致，提供流域动力学可解释见解。&lt;h4&gt;结论&lt;/h4&gt;CauStream为因果时空建模提供有原则基础，可扩展到广泛的科学和环境应用。&lt;h4&gt;翻译&lt;/h4&gt;径流预测对水资源管理和风险缓解至关重要。虽然深度学习模型已经取得了强大的预测性能，但它们往往忽略了潜在的物理过程，限制了可解释性和泛化能力。最近的因果学习方法通过整合领域知识解决了这些问题，但它们通常依赖于固定的因果图，无法适应数据变化。我们提出了CauStream，一个用于因果时空径流预测的统一框架。CauStream联合学习(i)气象强迫因素之间的径流因果图，和(ii)捕捉站点间动态依赖关系的路由图。我们进一步在非参数条件下建立了这些因果结构的可识别性条件。我们在美国三个主要流域的三个预测时间跨度上评估了CauStream。该模型持续优于先前最先进的方法，性能差距在更长的预测窗口上扩大，表明对未见条件的泛化能力更强。除了预测功能外，CauStream还学习捕捉水文因素和站点之间关系的因果图。推断出的结构与已建立的领域知识高度一致，为流域动力学提供了可解释的见解。CauStream为因果时空建模提供了有原则的基础，有潜力扩展到广泛的科学和环境应用中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.</description>
      <author>example@mail.com (Shu Wan, Reepal Shah, John Sabo, Huan Liu, K. Selçuk Candan)</author>
      <guid isPermaLink="false">2512.16046v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>In-Context Semi-Supervised Learning</title>
      <link>http://arxiv.org/abs/2512.15934v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了Transformer在上下文半监督学习中的能力，展示了Transformer如何利用未标记上下文学习鲁棒表示，从而在标签稀少的情况下提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;近期对Transformer在上下文学习能力的研究兴趣浓厚，但大多数理论研究集中在有明确标签对的有监督设置上。&lt;h4&gt;目的&lt;/h4&gt;引入并研究上下文半监督学习，探索Transformer如何利用未标记上下文学习表示。&lt;h4&gt;方法&lt;/h4&gt;研究包含少量标记示例和许多未标记点的上下文半监督学习场景，分析Transformer如何利用未标记上下文学习鲁棒且依赖于上下文的表示。&lt;h4&gt;主要发现&lt;/h4&gt;Transformer能够利用未标记上下文学习鲁棒的上下文相关表示，实现准确预测，并在低标签情况下显著提高性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现为Transformer如何在ICL框架内利用未标记上下文进行表示学习提供了基础性见解。&lt;h4&gt;翻译&lt;/h4&gt;近期对理解Transformer在上下文学习能力方面有显著兴趣，但大多数理论聚焦于具有明确标签对的有监督设置。在实践中，即使标签稀少或不存在，Transformer通常表现良好，这表明未标记的上下文演示中存在重要结构。我们引入并研究了上下文半监督学习，其中一小部分标记示例伴随着许多未标记点，并证明Transformer可以利用未标记上下文学习鲁棒且依赖于上下文的表示。这种表示能够实现准确预测，并在低标签情况下显著提高性能，为Transformer如何在ICL框架内利用未标记上下文进行表示学习提供了基础性见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.</description>
      <author>example@mail.com (Jiashuo Fan, Paul Rosu, Aaron T. Wang, Michael Li, Lawrence Carin, Xiang Cheng)</author>
      <guid isPermaLink="false">2512.15934v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2512.16561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://n3d-vlm.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了N3D-VLM框架，通过原生3D物体感知和3D感知视觉推理的集成，解决了多模态模型在3D场景理解方面的局限性，实现了精确的3D定位和可解释的空间理解。&lt;h4&gt;背景&lt;/h4&gt;当前多模态模型能够基于2D图像回答问题，但缺乏内在的3D物体感知能力，限制了它们对3D场景中空间关系和深度线索的理解能力。&lt;h4&gt;目的&lt;/h4&gt;提出N3D-VLM，一个新颖的统一框架，无缝集成原生3D物体感知和3D感知视觉推理，实现精确的3D定位和可解释的空间理解。&lt;h4&gt;方法&lt;/h4&gt;赋予模型原生3D物体感知能力，使其能直接根据文本描述在3D空间中定位物体；开发可扩展的数据构建管道，利用深度估计将大规模2D标注提升到3D空间，生成空间问答数据集促进3D物体定位和3D空间推理的联合训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该统一框架不仅在3D定位任务上取得了最先进的性能，而且在视觉语言模型的3D空间推理中也持续超越现有方法。&lt;h4&gt;结论&lt;/h4&gt;N3D-VLM通过原生3D物体感知和3D感知视觉推理的集成，有效解决了多模态模型在3D场景理解方面的局限性，显著提升了3D物体定位和空间推理能力。&lt;h4&gt;翻译&lt;/h4&gt;虽然当前的多模态模型可以基于2D图像回答问题，但它们缺乏内在的3D物体感知能力，限制了它们对3D场景中空间关系和深度线索的理解能力。在这项工作中，我们提出了N3D-VLM，一个新颖的统一框架，无缝集成原生3D物体感知与3D感知视觉推理，实现精确的3D定位和可解释的空间理解。与直接从RGB/RGB-D输入预测答案的传统端到端模型不同，我们的方法赋予模型原生3D物体感知能力，使其能够直接根据文本描述在3D空间中定位物体。基于准确的3D物体定位，模型进一步在3D中进行显式推理，实现更可解释和结构化的空间理解。为支持这些能力的稳健训练，我们开发了一个可扩展的数据构建管道，利用深度估计将大规模2D标注提升到3D空间，显著增加了3D物体定位数据的多样性和覆盖率，生成的数据量比现有最大的单图像3D检测数据集大六倍以上。此外，该管道生成针对3D中思维链(CoT)推理的空间问答数据集，促进3D物体定位和3D空间推理的联合训练。实验结果表明，我们的统一框架不仅在3D定位任务上取得了最先进的性能，而且在视觉语言模型的3D空间推理中也持续超越现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前多模态模型缺乏内在3D物体感知能力的问题，限制了它们对3D场景中空间关系和深度线索的理解。这个问题在现实中很重要，因为真实世界的应用往往需要深入理解3D结构和空间关系，而有效的3D空间推理需要准确的物体级3D感知能力。没有这种能力，模型很难推断空间配置或推理物理环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将3D空间理解分解为两个核心能力：3D物体定位和随后的3D空间推理。他们借鉴了现有工作，如集成外部感知模型获取物体信息、假设预定义空间信息、以及在点云中定位物体等方法，但这些方法各有局限。作者的创新在于设计了一个统一的视觉语言模型，它具有原生3D物体感知能力，能够准确定位物体并捕获深度线索，然后基于这些结构化的3D表示进行空间推理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是原生3D定位与基于定位的空间推理相结合的统一框架。整体实现流程包括：1) 3D数据构建，通过深度估计将2D注释提升到3D空间，生成大规模3D检测和空间推理数据；2) 3D感知视觉编码，将RGB图像和深度图作为输入，通过深度估计和位置编码融合空间信息；3) 两阶段训练，先训练3D物体定位，再混合训练空间推理；4) 支持两种推理模式，直接回答空间问题或先进行3D定位再回答后续问题。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的视觉语言模型架构，集成3D检测、定位和推理；2) 大规模3D数据构建管道，将2D注释提升到3D空间；3) 空间推理基准，包含显式推理过程；4) 3D感知视觉编码，注入明确深度线索。相比之前工作，该方法不依赖外部模块或预定义信息，不局限于狭窄场景，能处理多样化环境和广泛物体类别，支持显式空间推理而不仅是物体检测，能输出完整3D边界框而非仅位置或方向。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; N3D-VLM通过原生3D物体定位和显式3D感知推理的统一框架，显著提升了视觉语言模型在3D空间理解方面的能力，同时通过创新的数据构建方法解决了3D训练数据稀缺的问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.</description>
      <author>example@mail.com (Yuxin Wang, Lei Ke, Boqiang Zhang, Tianyuan Qu, Hanxun Yu, Zhenpeng Huang, Meng Yu, Dan Xu, Dong Yu)</author>
      <guid isPermaLink="false">2512.16561v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception</title>
      <link>http://arxiv.org/abs/2512.16265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了车辆协同感知中的隐私问题，并提出了解决方案&lt;h4&gt;背景&lt;/h4&gt;车辆间的协同感知有望提供强大可靠的场景理解能力，实验系统研究正在建立测试平台共享原始空间传感器数据&lt;h4&gt;目的&lt;/h4&gt;解决原始数据共享带来的隐私问题，推动利益相关者采用基于原始数据的协同感知&lt;h4&gt;方法&lt;/h4&gt;提出SHARP研究框架，旨在最小化隐私泄露&lt;h4&gt;主要发现&lt;/h4&gt;原始数据共享会引发新的隐私问题，阻碍利益相关者参与&lt;h4&gt;结论&lt;/h4&gt;讨论了实现SHARP框架所需的跨领域合作和研究方向&lt;h4&gt;翻译&lt;/h4&gt;车辆间的协同感知有望提供强大可靠的场景理解。最近，我们正在见证实验系统研究建立测试平台，用于共享原始空间传感器数据进行协同感知。虽然这种方法在准确性方面有显著提高，并且是自然的发展方向，但我们有必要考虑这种方法最终被汽车制造商采用时可能存在的问题。在本文中，我们首先指出，新的隐私问题会出现，这会阻碍利益相关者分享原始传感器数据。接下来，我们提出了SHARP研究框架，旨在最小化隐私泄露，并推动利益相关者朝着基于原始数据的协同感知这一宏伟目标前进。最后，我们讨论了网络系统、移动计算、感知研究人员、行业和政府在实现我们提出的框架时需要考虑的开放性问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cooperative perception between vehicles is poised to offer robust and reliable scene understanding. Recently, we are witnessing experimental systems research building testbeds that share raw spatial sensor data for cooperative perception. While there has been a marked improvement in accuracies and is the natural way forward, we take a moment to consider the problems with such an approach for eventual adoption by automakers. In this paper, we first argue that new forms of privacy concerns arise and discourage stakeholders to share raw sensor data. Next, we present SHARP, a research framework to minimize privacy leakage and drive stakeholders towards the ambitious goal of raw data based cooperative perception. Finally, we discuss open questions for networked systems, mobile computing, perception researchers, industry and government in realizing our proposed framework.</description>
      <author>example@mail.com (Bangya Liu, Chengpo Yan, Chenghao Jiang, Suman Banerjee, Akarsh Prabhakara)</author>
      <guid isPermaLink="false">2512.16265v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis</title>
      <link>http://arxiv.org/abs/2512.16237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SPRITE是一种新型框架，通过将真实值生成视为代码生成任务，利用模拟器和大型语言模型合成高质量、多样化的空间推理数据。该方法克服了传统方法的困境，提供了可扩展、语言多样且计算精确的数据集，实验证明能有效提升视觉语言模型的空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;身体智能是人工智能的重大挑战，当前模型的空间理解和推理能力有限。现有的通过增强视觉语言模型来解决这一问题的努力面临困境：基于模板的数据集可扩展但结构僵化，而手工注释语言多样但不可扩展且计算上不精确。&lt;h4&gt;目的&lt;/h4&gt;引入SPRITE框架，克服现有方法的困境，利用模拟器和大型模型可编程地合成可扩展、多样且高质量的空间推理数据。&lt;h4&gt;方法&lt;/h4&gt;SPRITE的核心创新是将真实值生成重新定义为代码生成任务。使用大型语言模型将复杂的空间问题编译成可执行程序，然后与从模拟器中提取的高精度场景元信息进行验证。确保真实值在计算上精确且可验证，同时大型语言模型的生成能力提供了广泛的语言多样性。利用此流程整理了包含3个模拟器、11k+场景和300k+图像/视频指令微调对的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在SPRITE数据上训练的视觉语言模型在多个空间基准测试上取得了显著的性能提升，优于同等大小的其他开源数据集。可扩展性分析证实了假设：克服传统模板方法的低多样性性质对于构建强大、可泛化的空间智能至关重要。&lt;h4&gt;结论&lt;/h4&gt;将SPRITE框架代码和完整的300k+数据集公开，以促进未来空间智能研究。&lt;h4&gt;翻译&lt;/h4&gt;身体智能是人工智能的重大挑战，从根本上受到当前模型有限的空间理解和推理能力的制约。通过增强视觉语言模型来解决这一问题的现有努力陷入两难：基于模板的数据集可扩展但结构僵化，而手工注释语言多样但不可扩展，关键的是，计算上不精确。我们引入了SPRITE，一个新型框架，通过利用模拟器和大型模型可编程地合成可扩展、多样且高质量的空间推理数据，从而克服这一困境。SPRITE的核心创新是将真实值生成重新定义为代码生成任务。我们利用大型语言模型将复杂的空间问题编译成可执行程序，然后与从模拟器中提取的高精度场景元信息进行验证。这确保了我们的真实值在计算上精确且可验证，同时大型语言模型的生成能力提供了广泛的语言多样性。利用此流程，我们整理了一个包含3个模拟器、11k+场景和300k+图像/视频指令微调对的数据集。我们证明，在我们的数据上训练的视觉语言模型在多个空间基准测试上取得了显著的性能提升，并优于同等大小的其他开源数据集。此外，可扩展性分析证实了我们的假设：克服传统模板方法的低多样性性质对于构建强大、可泛化的空间智能至关重要。我们将公开SPRITE框架代码和完整的300k+数据集，以促进未来空间智能研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型在空间理解和推理方面的局限性问题。这个问题很重要，因为空间理解和推理能力是AI系统与现实世界交互的基础，它超越了简单的物体识别，需要模型对物体关系、3D姿态和场景有深刻理解。没有这种能力，AI模型无法在复杂真实世界中可靠地规划、导航或操作物体，这限制了具身智能的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到当前多模态大语言模型在空间理解方面的不足，并分析了现有数据生成方法的局限性：基于模板的方法虽然可扩展但结构僵化，缺乏多样性；人工标注虽然语言多样但不可扩展且计算不精确。作者设计了SPRITE框架，借鉴了现有的模拟器技术、视觉-语言模型的成功经验以及大语言模型的生成能力，将真实标签生成重新定义为代码生成任务，同时利用模拟器提供高精度场景元信息和大语言模型提供语言多样性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将真实标签生成重新定义为代码生成任务，利用大语言模型将复杂空间问题编译成可执行程序，然后与从模拟器中提取的高精度场景元信息进行验证，确保真实标签在计算上精确且可验证，同时利用大语言模型的生成能力提供丰富的语言多样性。整体流程包括：1)从模拟器收集多模态数据和对象元信息；2)使用VLM解决对象命名歧义；3)使用GPT-4o生成多样化结构化问题；4)使用代码大语言模型生成可执行代码获取真实标签；5)通过自动质量控制确保数据质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将真实标签生成重新定义为代码生成任务，确保计算精确性和可验证性；2)提出SPRITE框架，同时解决数据生成的多样性、可扩展性和精确性三难困境；3)构建大规模数据集，覆盖多种空间推理任务。相比之前的工作，SPRITE克服了基于模板方法的僵化结构和语言多样性不足的问题，解决了人工标注的计算不精确和不可扩展性问题，同时保证了数据的多样性、可扩展性和精确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPRITE框架通过将真实标签生成重新定义为代码生成任务，解决了多模态大语言模型在空间推理数据生成中的多样性、可扩展性和精确性难以兼顾的问题，显著提升了模型在空间理解任务上的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.</description>
      <author>example@mail.com (Zhi Helu, Huang Jingjing, Xu Wang, Xu Yangbin, Zhang Wanyue, Jiang Baoyang, Deng Shirui, Zhu Liang, Li Fangfang, Zhao Tiejun, Lin Yankai, Yao Yuan)</author>
      <guid isPermaLink="false">2512.16237v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Unified Semantic Transformer for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2512.14364v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://unite-page.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UNITE，一种用于3D场景理解的统一语义Transformer，是一种新颖的前馈神经网络，可以在单一模型中统一多种3D语义任务。&lt;h4&gt;背景&lt;/h4&gt;全面的3D场景理解涉及捕获和解析非结构化的3D环境。由于现实世界的固有复杂性，现有模型主要是为特定任务开发和限制的。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在单一模型中统一多种3D语义任务的方法，实现对未见场景的快速、全面的3D语义理解。&lt;h4&gt;方法&lt;/h4&gt;UNITE是一种前馈神经网络，以完全端到端的方式处理未见过的场景，仅需几秒钟即可推断完整的3D语义几何。该方法能够直接从RGB图像预测多个语义属性，包括3D场景分割、实例嵌入、开放词汇特征，以及功能和关节运动。训练采用2D蒸馏结合自监督，并利用新颖的多视图损失确保3D视图一致性。&lt;h4&gt;主要发现&lt;/h4&gt;UNITE在几种不同的语义任务上实现了最先进的性能，在许多情况下甚至超过了特定任务模型的性能，超越了在真实3D几何上运行的方法。&lt;h4&gt;结论&lt;/h4&gt;UNITE是一种统一多种3D语义任务的有效方法，可以在未见过的场景上快速工作并实现高性能，为3D场景理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;全面的3D场景理解涉及捕获和解析非结构化的3D环境。由于现实世界的固有复杂性，现有模型主要被开发并局限于特定任务。我们引入了UNITE，一种用于3D场景理解的统一语义Transformer，这是一种新颖的前馈神经网络，可以在单一模型中统一多种3D语义任务。我们的模型以完全端到端的方式处理未见过的场景，只需几秒钟即可推断完整的3D语义几何。我们的方法能够直接从RGB图像预测多个语义属性，包括3D场景分割、实例嵌入、开放词汇特征，以及功能和关节运动。该方法使用2D蒸馏进行训练，严重依赖自监督，并利用新颖的多视图损失来确保3D视图一致性。我们证明UNITE在几种不同的语义任务上实现了最先进的性能，甚至在许多情况下超过了特定任务模型的性能，超越了在真实3D几何上运行的方法。项目网站见unite-page.github.io&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何实现一个统一的3D场景理解模型，能够同时处理多种3D语义任务（语义分割、实例分割、开放词汇搜索和关节预测）的问题。这个问题在现实和研究中很重要，因为3D场景理解是AR/VR和机器人的基础应用，使系统能够感知周围环境并构建丰富的3D表示。现有方法大多是任务特定的，缺乏通用性，或者依赖于复杂的后处理步骤，无法实现真正的端到端学习，限制了它们在复杂现实环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：辐射场方法依赖于已知相机姿态和场景特定训练；蒸馏方法需要3D重建；基于提升的技术依赖不可微分的后处理。作者借鉴了VGGT作为几何基础，使用DPT学习开放词汇语义，并利用SAM进行实例分割和CLIP作为视觉语言特征提取器。设计思路是创建一个统一的前馈transformer，通过联合学习几何和语义实现原生3D一致性，使用多视图一致性损失确保不同视图中相同3D点的特征一致，并利用对比学习学习度量嵌入空间。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的前馈神经网络(UNITE)，能够在单一模型中统一多种3D语义任务，通过联合学习几何和语义实现原生3D一致性，避免手工设计的提升步骤。整体流程：1)输入多视图RGB图像；2)使用基于VGGT的预训练编码器处理图像，预测相机姿态和点图；3)使用DPT头预测语义特征；4)使用另一个DPT头预测实例特征；5)使用专门DPT头预测关节；6)使用2D蒸馏损失和多视图一致性损失进行端到端训练；7)输出带密集语义、实例和关节特征的3D点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一模型架构，首次在单一前馈模型中统一多种3D语义任务；2)原生3D语义一致性，通过联合学习几何和语义避免手工提升步骤；3)多视图一致性损失，确保不同视图中相同3D点的特征一致；4)对比学习实例分割，学习度量嵌入空间；5)端到端训练，无需手动标注。相比之前的工作，UNITE是单一前馈模型而非多模型组合，避免了不可微分的后处理，不需要已知相机姿态和场景特定训练，且能泛化到新环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UNITE是一个统一的前馈transformer模型，能够在单一网络中从RGB图像实现几何和语义的联合3D场景理解，通过多视图一致性损失确保不同视角间的语义一致性，并在多种3D语义任务上实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io</description>
      <author>example@mail.com (Sebastian Koch, Johanna Wald, Hidenobu Matsuki, Pedro Hermosilla, Timo Ropinski, Federico Tombari)</author>
      <guid isPermaLink="false">2512.14364v2</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</title>
      <link>http://arxiv.org/abs/2512.16891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了LinkedOut，一种新的表示方法，用于从视频中直接提取VLLM世界知识，实现快速推理，支持多视频历史，并消除语言瓶颈。它是首个基于VLLM的、无需手工标注标签即可在原始帧上运行的视频推荐方法，在标准基准测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型(VLLMs)通过在互联网规模数据上的预训练，实现了具有世界知识感知的视频理解，并在电影分析和视频问答等任务上显示出潜力。然而，将VLLMs部署到下游任务（如视频推荐）仍然具有挑战性，因为真实系统需要多视频输入、轻量级骨干网络、低延迟顺序推理和快速响应。&lt;h4&gt;目的&lt;/h4&gt;解决VLLMs在视频推荐等下游任务部署中面临的挑战，包括：解码器生成导致顺序推理高延迟、典型接口不支持多视频输入、以及将输出限制为语言会丢弃下游视觉任务重要的细粒度视觉细节。&lt;h4&gt;方法&lt;/h4&gt;提出了LinkedOut表示方法，它使用VLLMs从原始帧中提取语义基础、知识感知的标记，由可提示的查询和可选的辅助模态引导。引入了一个跨层知识融合的MoE（专家混合模型），从丰富的VLLM特征中选择适当的抽象层次，实现个性化、可解释和低延迟的推荐。&lt;h4&gt;主要发现&lt;/h4&gt;LinkedOut是首个基于VLLM的、无需手工标注标签即可在原始帧上运行的视频推荐方法，在标准基准测试中取得了最先进的结果。可解释性研究和消融实验证实了层多样性和逐层融合的益处。&lt;h4&gt;结论&lt;/h4&gt;LinkedOut为下游视觉任务（如推荐）提供了一个实用路径，能够充分利用VLLM的世界知识先验和视觉推理能力。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(VLLMs)通过在互联网规模数据上的预训练，实现了具有世界知识感知的视频理解，并在电影分析和视频问答等任务上显示出潜力。然而，将VLLMs部署到下游任务（如视频推荐）仍然具有挑战性，因为真实系统需要多视频输入、轻量级骨干网络、低延迟顺序推理和快速响应。在实践中，(1)仅解码生成会导致顺序推理的高延迟，(2)典型接口不支持多视频输入，以及(3)将输出限制为语言会丢弃下游视觉任务重要的细粒度视觉细节。我们认为这些局限性源于缺乏一种既能保留像素级细节又能利用世界知识的表示。我们提出了LinkedOut，一种表示方法，它直接从视频中提取VLLM世界知识，以实现快速推理，支持多视频历史，并消除语言瓶颈。LinkedOut使用VLLMs从原始帧中提取语义基础、知识感知的标记，由可提示的查询和可选的辅助模态引导。我们引入了一个跨层知识融合MoE，从丰富的VLLM特征中选择适当的抽象层次，实现个性化、可解释和低延迟的推荐。据我们所知，LinkedOut是首个基于VLLM的、无需手工标注标签即可在原始帧上运行的视频推荐方法，在标准基准测试中取得了最先进的结果。可解释性研究和消融实验证实了层多样性和逐层融合的益处，指出了一个充分利用VLLM世界知识先验和视觉推理能力用于下游视觉任务（如推荐）的实用路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.</description>
      <author>example@mail.com (Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu)</author>
      <guid isPermaLink="false">2512.16891v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>CPMamba: Selective State Space Models for MIMO Channel Prediction in High-Mobility Environments</title>
      <link>http://arxiv.org/abs/2512.16315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出CPMamba，一种基于选择性状态空间模型的高效信道预测框架，通过特征提取网络和堆叠残差Mamba模块有效捕获信道长程依赖关系，在保持线性计算复杂度的同时实现高精度预测，并显著减少参数数量。&lt;h4&gt;背景&lt;/h4&gt;信道预测是提高MIMO-OFDM系统中预编码、自适应调制和资源分配等性能的关键技术。在高移动性场景下，面对快速时变信道，信道预测对抵抗信道老化、确保通信质量至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有信道预测方法的高复杂度和无法准确建模信道时间变化的问题，开发一种高效且准确的信道预测框架。&lt;h4&gt;方法&lt;/h4&gt;提出CPMamba框架，包含专门设计的特征提取和嵌入网络用于从历史CSI中提取特征，以及堆叠的残差Mamba模块进行时间建模，利用输入相关的选择性机制动态调整状态转换。&lt;h4&gt;主要发现&lt;/h4&gt;在3GPP标准信道模型下，CPMamba在所有场景实现最先进预测精度，具有优越泛化能力和鲁棒性，相比现有基线模型减少约50%参数数量，同时实现相当或更好的性能。&lt;h4&gt;结论&lt;/h4&gt;CPMamba是解决MIMO-OFDM系统高移动性场景下信道预测问题的有效方案，显著降低了实际部署门槛，为通信系统性能提升提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;信道预测是提高MIMO-OFDM系统中预编码、自适应调制和资源分配等各项功能性能的关键技术。特别是在具有快速时变信道的移动性高的场景下，抵抗信道老化和确保通信质量至关重要。然而，现有方法存在高复杂度和无法准确建模信道时间变化的问题。为解决这一问题，本文提出了CPMamba——一种基于选择性状态空间模型的高效信道预测框架。所提出的CPMamba架构通过专门设计的特征提取和嵌入网络从历史信道状态信息中提取特征，并使用堆叠的残差Mamba模块进行时间建模。通过利用输入相关的选择性机制动态调整状态转换，它能够有效捕获CSI之间的长程依赖关系，同时保持线性计算复杂度。在3GPP标准信道模型下的模拟结果表明，CPMamba在所有场景下实现了最先进的预测精度，同时具有优越的泛化能力和鲁棒性。与现有基线模型相比，CPMamba减少了约50%的参数数量，同时实现了相当或更好的性能，从而显著降低了实际部署的门槛。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Channel prediction is a key technology for improving the performance of various functions such as precoding, adaptive modulation, and resource allocation in MIMO-OFDM systems. Especially in high-mobility scenarios with fast time-varying channels, it is crucial for resisting channel aging and ensuring communication quality. However, existing methods suffer from high complexity and the inability to accurately model the temporal variations of channels. To address this issue, this paper proposes CPMamba -- an efficient channel prediction framework based on the selective state space model. The proposed CPMamba architecture extracts features from historical channel state information (CSI) using a specifically designed feature extraction and embedding network and employs stacked residual Mamba modules for temporal modeling. By leveraging an input-dependent selective mechanism to dynamically adjust state transitions, it can effectively capture the long-range dependencies between the CSIs while maintaining a linear computational complexity. Simulation results under the 3GPP standard channel model demonstrate that CPMamba achieves state-of-the-art prediction accuracy across all scenarios, along with superior generalization and robustness. Compared to existing baseline models, CPMamba reduces the number of parameters by approximately 50 percent while achieving comparable or better performance, thereby significantly lowering the barrier for practical deployment.</description>
      <author>example@mail.com (Sheng Luo, Jiashu Xie, Yueling Che, Junmei Yao, Jian Tian, Daquan Feng, Kaishun Wu)</author>
      <guid isPermaLink="false">2512.16315v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation</title>
      <link>http://arxiv.org/abs/2512.16313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级一体化视频修复网络LaverNet，解决了现有方法在处理时变退化时的两个主要挑战：退化主导时间建模和依赖大型模型。LaverNet仅有362K参数，通过引入选择性传播退化无关特征的机制，实现了与现有模型相当甚至更优的性能。&lt;h4&gt;背景&lt;/h4&gt;最近的研究探索了一体化视频修复，使用统一模型处理多种退化问题，但这些方法在处理时变退化时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决一体化视频修复中时变退化处理的两个挑战：退化主导时间建模和依赖大型模型的问题。&lt;h4&gt;方法&lt;/h4&gt;提出轻量级一体化视频修复网络LaverNet（362K参数），引入选择性传播机制，仅传输退化无关特征跨帧，减轻退化对时间建模的影响。&lt;h4&gt;主要发现&lt;/h4&gt;LaverNet尽管体积小（现有模型的参数不到1%），但在基准测试中实现了相当甚至更优的性能，证明紧凑网络可以实现强大的一体化修复。&lt;h4&gt;结论&lt;/h4&gt;通过LaverNet，作者展示了使用紧凑网络可以实现强大的一体化视频修复，解决了现有方法在处理时变退化时的主要问题。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究探索了一体化视频修复，使用统一模型处理多种退化问题。然而，这些方法在处理时变退化时仍面临两个挑战。首先，退化可能主导时间建模，使模型专注于伪影而非视频内容。其次，当前方法通常依赖大型模型处理一体化修复，掩盖了潜在的困难。为解决这些挑战，我们提出了一个轻量级的一体化视频修复网络LaverNet，只有362K参数。为减轻退化对时间建模的影响，我们引入了一种新的传播机制，选择性地仅传输与退化无关的特征跨帧。通过LaverNet，我们证明了一个紧凑的网络可以实现强大的一体化修复。尽管体积小（现有模型的参数不到1%），LaverNet在基准测试中实现了相当甚至更优的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.</description>
      <author>example@mail.com (Haiyu Zhao, Yiwen Shan, Yuanbiao Gou, Xi Peng)</author>
      <guid isPermaLink="false">2512.16313v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding</title>
      <link>http://arxiv.org/abs/2512.16250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AMUSE基准测试和RAFT框架，用于评估和改进多模态大语言模型在多说话人对话场景中的代理推理能力。&lt;h4&gt;背景&lt;/h4&gt;当前多模态大语言模型如GPT-4o和Qwen3-Omni在感知能力上表现出色，但在需要代理推理的多说话人、对话中心化场景中表现不佳，这些场景要求模型跟踪说话者、保持角色并在时间上锚定事件，这对多模态音频视频理解至关重要。&lt;h4&gt;目的&lt;/h4&gt;设计专门的基准测试和框架来评估和改进多模态大语言模型在多说话人对话场景中的推理能力。&lt;h4&gt;方法&lt;/h4&gt;引入AMUSE基准测试，围绕本质上需要代理的任务设计，要求模型将复杂音频视频交互分解为规划、锚定和反思步骤；评估MLLMs在零样本、引导式和代理式三种模式下的表现；提出RAFT框架，整合奖励优化和内在多模态自评估作为奖励，以及选择性参数适应。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型在所有模式下都表现出较弱的多说话人推理能力，在非代理和代理评估下行为不一致；使用RAFT框架可实现高达39.52%的相对准确率提升。&lt;h4&gt;结论&lt;/h4&gt;AMUSE和RAFT共同提供了一个实用平台，用于检验多模态模型中的代理推理能力并改进其性能。&lt;h4&gt;翻译&lt;/h4&gt;最近的多模态大语言模型如GPT-4o和Qwen3-Omni展现出强大的感知能力，但在需要代理推理的多说话人、对话中心化场景中表现挣扎，这些场景要求模型跟踪谁在说话、保持角色并在时间上锚定事件。这些场景对于多模态音频视频理解至关重要，在对话式视频助手和会议分析等应用中，模型必须共同推理音频和视频流。我们引入了AMUSE，一个围绕本质上需要代理的任务设计的基准测试，要求模型将复杂的音频视频交互分解为规划、锚定和反思步骤。它评估了MLLMs在零样本、引导式和代理式三种模式以及六个任务家族上的表现，包括时空说话者锚定和多模态对话摘要。在所有模式下，当前模型都表现出较弱的多说话人推理能力，并且在非代理和代理评估下行为不一致。受这些任务本质上具有的代理性质和最近LLM代理进展的启发，我们提出了RAFT，一个数据高效的代理对齐框架，将奖励优化与内在多模态自评估作为奖励相结合，并采用选择性参数适应实现数据和参数高效更新。使用RAFT，我们在基准测试上实现了高达39.52%的相对准确率提升。AMUSE和RAFT共同提供了一个实用平台，用于检验多模态模型中的代理推理能力并改进其性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.</description>
      <author>example@mail.com (Sanjoy Chowdhury, Karren D. Yang, Xudong Liu, Fartash Faghri, Pavan Kumar Anasosalu Vasu, Oncel Tuzel, Dinesh Manocha, Chun-Liang Li, Raviteja Vemulapalli)</author>
      <guid isPermaLink="false">2512.16250v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>Explainable AI in Big Data Fraud Detection</title>
      <link>http://arxiv.org/abs/2512.16037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 figures, research project&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了如何将可解释人工智能(XAI)集成到大数据分析流程中，用于欺诈检测和风险管理。作者回顾了大数据特征、分析工具和XAI方法，并提出了一个结合大数据基础设施与上下文感知解释机制的框架，同时指出了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;大数据已成为现代金融、保险和网络安全应用的核心，使机器学习系统能够进行大规模风险评估和欺诈检测。然而，对自动分析日益增长的关注引发了关于透明度、监管合规性和信任的重要问题。&lt;h4&gt;目的&lt;/h4&gt;本文旨在研究如何将可解释人工智能(XAI)整合到大数据分析流程中，用于欺诈检测和风险管理，以提高透明度、监管合规性和信任度。&lt;h4&gt;方法&lt;/h4&gt;作者回顾了大数据特征和主要分析工具，包括分布式存储系统、流处理平台和先进的欺诈检测模型（如异常检测器、基于图的方法和集成分类器）。此外，作者还介绍了广泛使用的XAI方法，包括LIME、SHAP、反事实解释和注意力机制，并分析了它们在规模化部署时的优势和局限性。&lt;h4&gt;主要发现&lt;/h4&gt;研究确定了与可扩展性、实时处理以及图模型和时序模型可解释性相关的主要研究差距。为解决这些挑战，作者概述了一个概念框架，将可扩展的大数据基础设施与上下文感知的解释机制和人类反馈相结合。&lt;h4&gt;结论&lt;/h4&gt;论文以可扩展XAI、隐私保护解释和可解释欺诈检测系统标准化评估方法的开放研究方向作为结尾。&lt;h4&gt;翻译&lt;/h4&gt;大数据已成为现代金融、保险和网络安全应用的核心，使机器学习系统能够执行大规模风险评估和欺诈检测。然而，对自动分析日益增长的依赖引入了关于透明度、监管合规性和信任的重要问题。本文研究了如何将可解释人工智能(XAI)集成到大数据分析流程中，用于欺诈检测和风险管理。我们回顾了关键的大数据特征，并调查了主要分析工具，包括分布式存储系统、流处理平台和先进的欺诈检测模型，如异常检测器、基于图的方法和集成分类器。我们还介绍了广泛使用的XAI方法的系统回顾，包括LIME、SHAP、反事实解释和注意力机制，并分析了它们在规模化部署时的优势和局限性。基于这些发现，我们确定了与可扩展性、实时处理以及图模型和时序模型可解释性相关的主要研究差距。为解决这些挑战，我们概述了一个概念框架，将可扩展的大数据基础设施与上下文感知的解释机制和人类反馈相结合。论文以可扩展XAI、隐私保护解释和可解释欺诈检测系统标准化评估方法的开放研究方向作为结尾。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Big Data has become central to modern applications in finance, insurance, and cybersecurity, enabling machine learning systems to perform large-scale risk assessments and fraud detection. However, the increasing dependence on automated analytics introduces important concerns about transparency, regulatory compliance, and trust. This paper examines how explainable artificial intelligence (XAI) can be integrated into Big Data analytics pipelines for fraud detection and risk management. We review key Big Data characteristics and survey major analytical tools, including distributed storage systems, streaming platforms, and advanced fraud detection models such as anomaly detectors, graph-based approaches, and ensemble classifiers. We also present a structured review of widely used XAI methods, including LIME, SHAP, counterfactual explanations, and attention mechanisms, and analyze their strengths and limitations when deployed at scale. Based on these findings, we identify key research gaps related to scalability, real-time processing, and explainability for graph and temporal models. To address these challenges, we outline a conceptual framework that integrates scalable Big Data infrastructure with context-aware explanation mechanisms and human feedback. The paper concludes with open research directions in scalable XAI, privacy-aware explanations, and standardized evaluation methods for explainable fraud detection systems.</description>
      <author>example@mail.com (Ayush Jain, Rahul Kulkarni, Siyi Lin)</author>
      <guid isPermaLink="false">2512.16037v1</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>LADY: Linear Attention for Autonomous Driving Efficiency without Transformers</title>
      <link>http://arxiv.org/abs/2512.15038v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LADY，第一个完全基于线性注意力机制的端到端自动驾驶生成模型，能够有效融合长程时间上下文并支持跨模态信息交换，在保持高性能的同时显著降低计算复杂度。&lt;h4&gt;背景&lt;/h4&gt;端到端范式在自动驾驶领域显示出巨大潜力，但大多数现有方法基于Transformer架构，其二次方注意力成本限制了建模长时空序列的能力，特别是在资源受限的边缘平台上。现有的线性注意力架构仅限于自注意力，缺乏对自动驾驶至关重要的跨模态和跨时间交互支持。&lt;h4&gt;目的&lt;/h4&gt;开发一种完全基于线性注意力机制的端到端自动驾驶生成模型，实现长程时间上下文融合，同时保持恒定的计算和内存成本，并引入轻量级线性交叉注意力机制实现有效的跨模态信息交换。&lt;h4&gt;方法&lt;/h4&gt;提出了LADY模型，该模型能够在推理时融合长程时间上下文，具有恒定的计算和内存成本，无论相机和LiDAR特征的历史长度如何。同时引入了轻量级线性交叉注意力机制，实现有效的跨模态信息交换。&lt;h4&gt;主要发现&lt;/h4&gt;在NAVSIM和Bench2Drive基准测试上，LADY实现了最先进的性能，具有恒定时间和内存复杂度，提供改进的规划性能并显著降低计算成本。该模型已在边缘设备上部署和验证，展示了在资源有限场景下的实用性。&lt;h4&gt;结论&lt;/h4&gt;线性注意力机制可以有效解决Transformer架构在自动驾驶中的局限性，LADY模型能够在保持高性能的同时显著降低计算复杂度，并在资源受限的边缘设备上具有良好的实用性和部署能力。&lt;h4&gt;翻译&lt;/h4&gt;端到端范式已经展示了自动驾驶的巨大潜力。此外，大多数现有方法都基于Transformer架构构建。然而，Transformer会产生二次方注意力成本，限制了它们建模长时空序列的能力，特别是在资源受限的边缘平台上。由于自动驾驶本质上需要高效的时间建模，这一挑战严重限制了它们的部署和实时性能。最近，线性注意力机制由于其优越的时空复杂度而获得了越来越多的关注。然而，现有的线性注意力架构仅限于自注意力，缺乏对自动驾驶至关重要的跨模态和跨时间交互的支持。在这项工作中，我们提出了LADY，这是第一个完全基于线性注意力机制的端到端自动驾驶生成模型。LADY能够在推理时融合长程时间上下文，具有恒定的计算和内存成本，无论相机和LiDAR特征的历史长度如何。此外，我们引入了一种轻量级线性交叉注意力机制，实现了有效的跨模态信息交换。在NAVSIM和Bench2Drive基准上的实验表明，LADY以恒定的时间和内存复杂度实现了最先进的性能，提供了改进的规划性能并显著降低了计算成本。此外，该模型已在边缘设备上部署和验证，展示了其在资源有限场景下的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.</description>
      <author>example@mail.com (Jihao Huang, Xi Xia, Zhiyuan Li, Tianle Liu, Jingke Wang, Junbo Chen, Tengju Ye)</author>
      <guid isPermaLink="false">2512.15038v2</guid>
      <pubDate>Fri, 19 Dec 2025 15:56:37 +0800</pubDate>
    </item>
    <item>
      <title>In Pursuit of Pixel Supervision for Visual Pre-training</title>
      <link>http://arxiv.org/abs/2512.15715v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://github.com/facebookresearch/pixio&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'Pixio'的增强型掩码自编码器，证明了基于自编码器的自监督学习方法在今天仍然具有竞争力，能够为下游任务生成强大表示。&lt;h4&gt;背景&lt;/h4&gt;像素是我们感知世界视觉信息的基本来源，包含从低级属性到高级概念的所有层次信息。自编码器是一种从像素或其他原始输入学习表示的经典且长期存在的范式。&lt;h4&gt;目的&lt;/h4&gt;展示基于自编码器的自监督学习方法在今天仍然具有竞争力，能够为下游任务生成强大表示，同时保持简单、稳定和高效。&lt;h4&gt;方法&lt;/h4&gt;作者提出了名为'Pixio'的模型，这是一种增强型掩码自编码器，具有更具挑战性的预训练任务和更强大的架构。模型使用自策略策略在20亿网络爬取的图像上进行训练，最小化人工策展。&lt;h4&gt;主要发现&lt;/h4&gt;Pixio在广泛的下游任务中表现具有竞争力，包括单目深度估计、前馈3D重建、语义分割和机器人学习，性能优于或匹敌类似规模训练的DINOv3。&lt;h4&gt;结论&lt;/h4&gt;像素空间的自监督学习可以作为潜在空间方法的一种有前景的替代和补充。&lt;h4&gt;翻译&lt;/h4&gt;在最基本的层面上，像素是我们通过其感知世界的视觉信息的来源。像素包含所有层次的信息，从低级属性到高级概念。自编码器代表了一种从像素或其他原始输入学习表示的经典且长期存在的范式。在这项工作中，我们证明了基于自编码器的自监督学习在今天仍然具有竞争力，可以为下游任务生成强大的表示，同时保持简单、稳定和高效。我们的模型命名为'Pixio'，是一种增强型掩码自编码器，具有更具挑战性的预训练任务和更强大的架构。该模型使用自策展策略在20亿网络爬取的图像上进行训练，最小化人工策展。Pixio在广泛的下游任务中表现具有竞争力，包括单目深度估计、前馈3D重建、语义分割和机器人学习，性能优于或匹敌类似规模训练的DINOv3。我们的结果表明，像素空间的自监督学习可以作为一种有前景的替代和补充，补充潜在空间方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何改进基于像素级监督的自监督视觉预训练方法，使其能够更好地捕捉从低级属性到高级概念的全层次视觉信息。这个问题很重要，因为像素是视觉信息的根本来源，而现有的自监督学习方法要么依赖过多人类先验（如对比学习），要么在小规模数据上训练，限制了模型的泛化能力和性能上限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先回顾了视觉表示学习的演变，从基于人工标注的监督学习，到基于网络图像-文本对的方法，再到自监督学习。他们注意到MAE（掩码自编码器）是一个经典但可能被低估的方法，在大数据、大模型场景下设计不够优化。作者借鉴了MAE的基本框架，但从算法和数据两方面进行了改进：算法上增加了预训练难度（更大掩码块）和模型能力（更深解码器、更多类别令牌）；数据上构建了大规模多样化数据集并采用软自策略筛选。这些改进基于对MAE局限性的深入分析和实验验证。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是像素作为视觉信息的根本来源，包含了从低级到高级的全层次信息，通过掩码和像素重建任务，模型可以学习压缩和重新组织这些视觉知识。整体实现流程包括：1) 基于MAE进行三个关键算法改进（更深解码器、更大掩码块、更多类别令牌）；2) 收集20亿网络爬取图像并采用软自策略进行数据筛选；3) 在大规模数据集上使用高掩码比进行预训练；4) 在多种下游任务（深度估计、3D重建、语义分割、机器人学习）上评估模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 算法改进：更深解码器解决编码器需牺牲容量处理低级细节的问题，更大掩码块提供更丰富上下文并减少真实值泄露，更多类别令牌捕获多样化全局属性；2) 数据策略：使用20亿网络爬取图像并采用软自策略筛选，减少人工偏见；3) 性能表现：在多种下游任务上达到或超越DINOv3等先进模型。相比之前工作，Pixio基于像素级监督而非潜在空间目标，避免了过多人类先验；相比原始MAE，在算法和数据上进行了全面改进；相比需要大量人工标注的方法，采用最小人工干预的数据策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过改进掩码自编码器的架构设计并采用大规模多样化数据训练，证明了基于像素级监督的自学习方法可以成为潜在空间方法的强大替代方案，在多种视觉任务上达到或超越最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed "Pixio", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.</description>
      <author>example@mail.com (Lihe Yang, Shang-Wen Li, Yang Li, Xinjie Lei, Dong Wang, Abdelrahman Mohamed, Hengshuang Zhao, Hu Xu)</author>
      <guid isPermaLink="false">2512.15715v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
  <item>
      <title>Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2512.15508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种新的前馈架构，通过亚像素级别的3D高斯原语检测，用自适应的'非网格'分布替代了像素网格，解决了传统前馈3D高斯飞溅模型中像素对齐原语放置次优的问题。该模型能够实时生成照片级真实感的场景，在无需相机姿态标签的情况下实现了前馈模型的最先进新视角合成性能。&lt;h4&gt;背景&lt;/h4&gt;现有的前馈3D高斯飞溅(3DGS)模型可以实现实时场景生成，但受到次优像素对齐原语放置的限制，这种放置依赖于密集的刚性网格，从而限制了质量和效率。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的前馈架构，能够以亚像素级别检测3D高斯原语，用自适应的'非网格'分布替代传统的像素网格，从而提高场景生成的质量和效率，减少伪影，并减少所需原语的数量。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一种受关键点检测启发的新前馈架构，使用多分辨率解码器学习在图像块上分布原语。该模块通过自监督学习与3D重建骨干网络进行端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;新模型在使用更少原语的情况下优于竞争对手，证明了更准确和高效的原语分配能够捕捉精细细节并减少伪影。此外，通过学习渲染3D高斯，3D重建骨干网络能够改进相机姿态估计。&lt;h4&gt;结论&lt;/h4&gt;研究表明，通过用自适应的'非网格'分布替代传统的像素网格，可以显著提高前馈3D高斯飞溅模型的性能，为无标签训练基础模型提供了可能性。&lt;h4&gt;翻译&lt;/h4&gt;前馈3D高斯飞溅(3DGS)模型能够实现实时场景生成，但受到次优像素对齐原语放置的限制，这种放置依赖于密集的刚性网格，从而限制了质量和效率。我们引入了一种新的前馈架构，能够在亚像素级别检测3D高斯原语，用自适应的'非网格'分布替代像素网格。受关键点检测启发，我们的多分辨率解码器学习在图像块上分布原语。该模块通过自监督学习与3D重建骨干网络进行端到端训练。我们得到的无需姿态模型能够在几秒钟内生成照片级真实感的场景，实现了前馈模型在新型视角合成方面的最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决前馈式3D高斯泼溅(3DGS)模型中基元放置次优的问题。现有方法使用密集、刚性的网格(如像素对齐或体素对齐)来放置3D高斯基元，这限制了模型的质量和效率。这个问题很重要，因为3D高斯泼溅能实现照片级实时渲染，但前馈模型需要大量基元(通常每个像素一个)，只能处理低分辨率图像，且渲染质量有限。改进基元分配可以减少计算资源需求，提高渲染质量，扩展应用场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者质疑规则网格是否是最佳基元分配方式，观察到优化技术能自适应分布基元但前馈模型缺乏此能力。他们提出在3个级别控制基元分配：子像素级检测基元位置、多密度解码器分配更多基元到详细区域、学习置信值聚合基元。方法借鉴了关键点检测技术(如SuperPoint)使用热图提取连续坐标；借鉴APT工作使用熵度量块内容密度；采用标准U-Net架构；并在VGGT模型基础上进行微调。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'Off-The-Grid'高斯，不受限于像素网格，可在子像素级精确放置基元，并自适应分配基元数量。整体流程：1)使用VGGT骨干网络预测深度和相机参数；2)3D高斯解码器通过U-Net提取特征，用热图确定基元位置，根据图像块熵分配不同密度基元；3)将2D点转换为3D高斯并预测参数；4)多视图聚合时用置信值修剪冗余基元；5)通过光度损失、几何一致性损失等自监督训练，无需3D标注。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)子像素级基元检测而非像素对齐；2)自适应密度机制根据内容分配基元；3)置信值学习聚合多视图；4)自监督训练无需3D标注；5)改进相机姿态估计。相比之前工作：不同于像素对齐方法(如PixelSplat)为每个像素分配基元，我们使用更少基元实现更好质量；不同于体素对齐方法(如AnySplat)使用规则3D网格，我们避免了网格可见性问题；实现了更好的几何和渲染质量，特别是在放大时；训练方式上通过渲染反馈同时改进基元检测和3D重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出'Off-The-Grid'高斯基元检测方法，通过自适应分配和子像素级定位，显著提高了前馈式3D高斯泼溅模型的效率和渲染质量，实现了无需相机姿态标注的高质量3D场景重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, "Off The Grid" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.</description>
      <author>example@mail.com (Arthur Moreau, Richard Shaw, Michal Nazarczuk, Jisu Shin, Thomas Tanay, Zhensong Zhang, Songcen Xu, Eduardo Pérez-Pellitero)</author>
      <guid isPermaLink="false">2512.15508v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</title>
      <link>http://arxiv.org/abs/2512.15396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SMART（语义匹配对比学习模型）的方法，用于解决部分视图对齐聚类问题，通过减轻跨视图分布偏移来充分利用对齐和非对齐数据中的语义关系。&lt;h4&gt;背景&lt;/h4&gt;多视图聚类可通过利用多视图数据的互补信息提高学习性能，但在现实场景中收集严格对齐的视图具有挑战性，因此需要从对齐和非对齐数据中学习。部分视图对齐聚类（PVC）旨在学习未对齐视图样本间的对应关系，以利用跨视图的一致性和互补性。&lt;h4&gt;目的&lt;/h4&gt;解决现有PVC方法未能利用非对齐数据捕获同一聚类样本间共享语义的问题，以及多视图数据异质性导致的表示分布偏移问题，这些问题阻碍了建立有意义的跨视图特征对应关系。&lt;h4&gt;方法&lt;/h4&gt;提出SMART（语义匹配对比学习模型），通过减轻跨视图分布偏移的影响，促进语义匹配对比学习，从而充分利用对齐和非对齐数据中的语义关系。&lt;h4&gt;主要发现&lt;/h4&gt;在八个基准数据集上的实验表明，该方法在PVC问题上始终优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;SMART模型通过有效处理跨视图分布偏移问题，能够更好地利用对齐和非对齐数据中的语义关系，在部分视图对齐聚类任务上取得更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;多视图聚类已被经验证明可以通过利用多个数据视图之间的内在互补信息来提高学习性能。然而，在现实场景中，收集严格对齐的视图具有挑战性，并且从对齐和非对齐数据中学习成为一种更实用的解决方案。部分视图对齐聚类（PVC）旨在学习未对齐视图样本之间的对应关系，以更好地利用跨视图的潜在一致性和互补性，包括对齐和非对齐数据。然而，大多数现有的PVC方法未能利用非对齐数据来捕获同一聚类样本之间的共享语义。此外，多视图数据的内在异质性会导致表示中的分布偏移，从而在建立跨视图潜在特征之间的有意义对应关系时产生不准确，进而损害学习效果。为了解决这些挑战，我们为PVC提出了一种语义匹配对比学习模型（SMART）。我们方法的主要思想是减轻跨视图分布偏移的影响，从而促进语义匹配对比学习，充分利用对齐和非对齐数据中的语义关系。在八个基准数据集上进行的大量实验表明，我们的方法在PVC问题上始终优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.</description>
      <author>example@mail.com (Liang Peng, Yixuan Ye, Cheng Liu, Hangjun Che, Fei Wang, Zhiwen Yu, Si Wu, Hau-San Wong)</author>
      <guid isPermaLink="false">2512.15396v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment</title>
      <link>http://arxiv.org/abs/2512.15315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了AutoMAC-MRI框架，通过监督对比学习和亲和评分方法，实现了对MRI图像中运动伪影的可解释性分级评估，有助于提高MRI质量控制效率并减少不必要的重扫描。&lt;h4&gt;背景&lt;/h4&gt;运动伪影会降低MRI图像质量并增加患者重检率。现有的自动化质量评估方法大多仅限于二元决策，提供有限的可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一个可解释的框架，用于评估不同MRI对比度和方向上的运动伪影等级，使等级分配透明且可解释。&lt;h4&gt;方法&lt;/h4&gt;使用监督对比学习学习运动严重性的判别性表示，在特征空间中计算特定等级的亲和分数来量化图像与每个运动等级的接近程度。在5000多张专家标注的脑部MRI切片上进行了评估，这些切片涵盖了多种对比度和视图。&lt;h4&gt;主要发现&lt;/h4&gt;亲和分数与专家标签的评估显示良好的一致性，支持将亲和分数用作运动严重性的可解释度量。&lt;h4&gt;结论&lt;/h4&gt;通过将准确的等级检测与每个等级的亲和评分相结合，AutoMAC-MRI能够实现MRI质量控制，有潜力减少不必要的重扫描并提高工作效率。&lt;h4&gt;翻译&lt;/h4&gt;运动伪影会降低MRI图像质量并增加患者重检率。现有的自动化质量评估方法大多仅限于二元决策，提供有限的可解释性。我们引入了AutoMAC-MRI，这是一个可解释的框架，用于评估不同MRI对比度和方向上的运动伪影等级。该方法使用监督对比学习来学习运动严重性的判别性表示。在这个特征空间中，我们计算特定等级的亲和分数，量化图像与每个运动等级的接近程度，从而使等级分配透明且可解释。我们在5000多张专家标注的脑部MRI切片上评估了AutoMAC-MRI，这些切片涵盖了多种对比度和视图。将亲和分数与专家标签进行对比的实验显示，这些分数与专家判断高度一致，支持将其用作运动严重性的可解释度量。通过将准确的等级检测与每个等级的亲和评分相结合，AutoMAC-MRI能够实现MRI质量控制，有潜力减少不必要的重扫描并提高工作效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.</description>
      <author>example@mail.com (Antony Jerald, Dattesh Shanbhag, Sudhanya Chatterjee)</author>
      <guid isPermaLink="false">2512.15315v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Dual-coding contrastive learning based on ConvNeXt and ViT models for morphological classification of galaxies in COSMOS-Web</title>
      <link>http://arxiv.org/abs/2512.15129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种改进的无监督机器学习方法，通过对比学习升级了USmorph框架，提高了星系形态分类的效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;作者之前提出了一个名为USmorph的机器学习框架，用于高效分类星系形态。在这项研究中，他们提出了一种自监督方法，称为对比学习，以升级USmorph框架中的无监督机器学习部分。&lt;h4&gt;目的&lt;/h4&gt;提高特征提取步骤的效率。&lt;h4&gt;方法&lt;/h4&gt;使用卷积自编码器对星系图像去噪并增强旋转不变性；采用基于ConvNeXt和ViT的预训练双编码器卷积神经网络编码图像数据并应用对比学习降低特征维度；使用基于Bagging的聚类模型对星系进行分组；将模型应用于COSMOS-Web场中红移范围在0.5至6.0的星系光学图像。&lt;h4&gt;主要发现&lt;/h4&gt;改进的无监督机器学习方法成功分类了73%的星系；使用GoogleNet算法对剩余27%的星系进行形态分类；分类结果与星系演化具有良好一致性。&lt;h4&gt;结论&lt;/h4&gt;由于其更高的效率，更新的算法非常适合应用于未来的中国空间望远镜任务。&lt;h4&gt;翻译&lt;/h4&gt;在我们之前的工作中，我们提出了一个名为USmorph的机器学习框架，用于高效分类星系形态。在本研究中，我们提出了一种称为对比学习的自监督方法，以升级USmorph框架中的无监督机器学习部分，旨在提高此步骤中特征提取的效率。升级的无监督机器学习方法主要包括以下三个方面。（1）我们采用卷积自编码器对星系图像去噪，并使用自适应极坐标变换增强模型的旋转不变性。（2）使用基于ConvNeXt和ViT的预训练双编码器卷积神经网络对图像数据进行编码，然后应用对比学习降低特征的维度。（3）我们采用基于Bagging的聚类模型将具有相似特征的星系聚类到不同的组中。通过仔细划分红移区间，我们将此模型应用于COSMOS-Web场中红移范围在0.5 &lt; z &lt; 6.0的星系的光学图像。与之前的算法相比，改进的无监督机器学习方法成功分类了73%的星系。使用GoogleNet算法，我们对剩余27%的星系进行形态分类。为了验证我们更新算法的可靠性，我们将分类结果与其他星系形态参数进行了比较，发现与星系演化具有良好的一致性。得益于其更高的效率，这种更新的算法非常适合应用于未来的中国空间望远镜任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In our previous works, we proposed a machine learning framework named \texttt{USmorph} for efficiently classifying galaxy morphology. In this study, we propose a self-supervised method called contrastive learning to upgrade the unsupervised machine learning (UML) part of the \texttt{USmorph} framework, aiming to improve the efficiency of feature extraction in this step. The upgraded UML method primarily consists of the following three aspects. (1) We employ a Convolutional Autoencoder to denoise galaxy images and the Adaptive Polar Coordinate Transformation to enhance the model's rotational invariance. (2) A pre-trained dual-encoder convolutional neural network based on ConvNeXt and ViT is used to encode the image data, while contrastive learning is then applied to reduce the dimension of the features. (3) We adopt a Bagging-based clustering model to cluster galaxies with similar features into distinct groups. By carefully dividing the redshift bins, we apply this model to the rest-frame optical images of galaxies in the COSMOS-Web field within the redshift range of $0.5 &lt; z &lt; 6.0$. Compared to the previous algorithm, the improved UML method successfully classifies 73\% galaxies. Using the GoogleNet algorithm, we classify the morphology of the remaining 27\% galaxies. To validate the reliability of our updated algorithm, we compared our classification results with other galaxy morphological parameters and found a good consistency with galaxy evolution. Benefiting from its higher efficiency, this updated algorithm is well-suited for application in future China Space Station Telescope missions.</description>
      <author>example@mail.com (Shiwei Zhu, Guanwen Fang, Chichun Zhou, Jie Song, Zesen Lin, Yao Dai, Xu Kong)</author>
      <guid isPermaLink="false">2512.15129v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Trustworthy Neighborhoods Mining: Homophily-Aware Neutral Contrastive Learning for Graph Clustering</title>
      <link>http://arxiv.org/abs/2512.15027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NeuCGC的新型邻域中性对比图聚类方法，通过引入中性对和两个关键组件，解决了传统对比学习在低同质性图中面临的邻域信息不可靠问题，有效处理了不同同质性水平的图聚类任务。&lt;h4&gt;背景&lt;/h4&gt;邻居对比学习被引入到聚类中以有效利用邻域信息，但这些方法依赖于同质性假设（连接节点共享相似类别标签），未能考虑现实世界中图的不同同质性水平，导致在低同质性图中应用对比学习时可能产生难以区分的节点表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理不同同质性水平图的图聚类方法，克服传统对比学习在同质性假设上的局限性，识别具有不同同质性水平图中的可信邻域。&lt;h4&gt;方法&lt;/h4&gt;提出NeuCGC方法，扩展传统对比学习，引入中性对（视为加权正对而非严格正或负对），这些中性对根据图的同质性水平动态调整；包含两个关键组件：(1)自适应对比邻域分布对齐，根据给定属性图的同质性水平调整；(2)对比邻域节点特征一致性学习机制，利用高置信度图的可信邻域信息学习鲁棒节点表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，NeuCGC方法在图聚类任务中表现出有效性和鲁棒性，优于其他最先进的图聚类方法，能够有效减轻不同同质性水平的负面影响并充分利用可信邻域信息。&lt;h4&gt;结论&lt;/h4&gt;NeuCGC通过引入中性对和自适应机制，能够更灵活、鲁棒地处理不同同质性水平的图聚类问题，有效解决了传统对比学习在同质性假设上的局限性。&lt;h4&gt;翻译&lt;/h4&gt;最近，基于邻居的对比学习已被引入到聚类中，以有效利用邻域信息。然而，这些方法依赖于同质性假设-即连接的节点共享相似的类别标签，因此在特征空间中应该接近-这未能考虑现实世界中图的不同同质性水平。因此，将对比学习应用于低同质性图可能导致节点表示难以区分，因为邻域信息不可靠，使得在具有不同同质性水平的图中识别可信邻域在图聚类中具有挑战性。为了解决这个问题，我们引入了一种新颖的邻域中性对比图聚类方法NeuCGC，它通过引入中性对-被视为加权正对而非严格正或负对，扩展了传统对比学习。这些中性对根据图的同质性水平动态调整，使学习过程更加灵活和鲁棒。利用对比学习中的中性对，我们的方法包含两个关键组件：(1)自适应对比邻域分布对齐，根据给定属性图的同质性水平进行调整，确保邻域分布的有效对齐；(2)对比邻域节点特征一致性学习机制，利用来自高置信度图的可信邻域信息学习鲁棒节点表示，减轻不同同质性水平的负面影响，并有效利用高度可信的邻域信息。实验结果证明了我们方法的有效性和鲁棒性，优于其他最先进的图聚类方法。我们的代码可在https://github.com/THPengL/NeuCGC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, neighbor-based contrastive learning has been introduced to effectively exploit neighborhood information for clustering. However, these methods rely on the homophily assumption-that connected nodes share similar class labels and should therefore be close in feature space-which fails to account for the varying homophily levels in real-world graphs. As a result, applying contrastive learning to low-homophily graphs may lead to indistinguishable node representations due to unreliable neighborhood information, making it challenging to identify trustworthy neighborhoods with varying homophily levels in graph clustering. To tackle this, we introduce a novel neighborhood Neutral Contrastive Graph Clustering method, NeuCGC, that extends traditional contrastive learning by incorporating neutral pairs-node pairs treated as weighted positive pairs, rather than strictly positive or negative. These neutral pairs are dynamically adjusted based on the graph's homophily level, enabling a more flexible and robust learning process. Leveraging neutral pairs in contrastive learning, our method incorporates two key components: (1) an adaptive contrastive neighborhood distribution alignment that adjusts based on the homophily level of the given attribute graph, ensuring effective alignment of neighborhood distributions, and (2) a contrastive neighborhood node feature consistency learning mechanism that leverages reliable neighborhood information from high-confidence graphs to learn robust node representations, mitigating the adverse effects of varying homophily levels and effectively exploiting highly trustworthy neighborhood information. Experimental results demonstrate the effectiveness and robustness of our approach, outperforming other state-of-the-art graph clustering methods. Our code is available at https://github.com/THPengL/NeuCGC.</description>
      <author>example@mail.com (Liang Peng, Yixuan Ye, Cheng Liu, Hangjun Che, Man-Fai Leung, Si Wu, Hau-San Wong)</author>
      <guid isPermaLink="false">2512.15027v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning</title>
      <link>http://arxiv.org/abs/2512.15410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 9 figures, MIDL 2026 conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了多重组织成像中自监督表示学习的最佳架构方法，发现轻量级通道独立架构比深度早期融合模型表现更好。&lt;h4&gt;背景&lt;/h4&gt;多重组织成像技术能够测量每个细胞的数十种蛋白质标记，但大多数深度学习模型仍采用早期通道融合，假设标记间存在共享结构。&lt;h4&gt;目的&lt;/h4&gt;研究保持标记独立性结合浅层架构是否比增加模型规模更适合多重数据中的自监督表示学习。&lt;h4&gt;方法&lt;/h4&gt;使用包含145,000个细胞和49个标记的霍奇金淋巴瘤CODEX数据集，比较标准早期融合CNN与通道分离架构，包括标记感知基线和新型浅层通道独立模型(CIM-S，5.5K参数)，采用对比预训练和线性评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;早期融合模型保留标记特定信息能力有限，尤其在稀有细胞区分方面表现不佳；通道独立架构特别是CIM-S尽管尺寸紧凑但实现了更强表示能力；这些发现在多个自监督框架中保持一致，在不同增强设置下稳定，并且在49个和18个标记设置中均可重复。&lt;h4&gt;结论&lt;/h4&gt;轻量级、通道独立的架构可以匹配或超越深度早期融合CNN和基础模型，用于多重表示学习。&lt;h4&gt;翻译&lt;/h4&gt;多重组织成像测量每个细胞的数十种蛋白质标记，但大多数深度学习模型仍应用早期通道融合，假设标记间存在共享结构。我们研究保持标记独立性，结合故意浅层架构，是否比增加模型规模更适合多重数据中的自监督表示学习。使用包含145,000个细胞和49个标记的霍奇金淋巴瘤CODEX数据集，我们比较标准的早期融合CNN与通道分离架构，包括标记感知基线和我们的新型浅层通道独立模型(CIM-S，含5.5K参数)。经过对比预训练和线性评估后，早期融合模型显示保留标记特定信息的能力有限，特别是在稀有细胞区分方面表现不佳。通道独立架构，特别是CIM-S，尽管尺寸紧凑，实现了显著更强的表示能力。这些发现在多个自监督框架中保持一致，在不同增强设置下稳定，并且在49个标记和减少到18个标记的设置中均可重复。这些结果表明，轻量级、通道独立的架构可以匹配或超越深度早期融合CNN和基础模型，用于多重表示学习。代码可在https://github.com/SimonBon/CIM-S获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiplexed tissue imaging measures dozens of protein markers per cell, yet most deep learning models still apply early channel fusion, assuming shared structure across markers. We investigate whether preserving marker independence, combined with deliberately shallow architectures, provides a more suitable inductive bias for self-supervised representation learning in multiplex data than increasing model scale. Using a Hodgkin lymphoma CODEX dataset with 145,000 cells and 49 markers, we compare standard early-fusion CNNs with channel-separated architectures, including a marker-aware baseline and our novel shallow Channel-Independent Model (CIM-S) with 5.5K parameters. After contrastive pretraining and linear evaluation, early-fusion models show limited ability to retain marker-specific information and struggle particularly with rare-cell discrimination. Channel-independent architectures, and CIM-S in particular, achieve substantially stronger representations despite their compact size. These findings are consistent across multiple self-supervised frameworks, remain stable across augmentation settings, and are reproducible across both the 49-marker and reduced 18-marker settings. These results show that lightweight, channel-independent architectures can match or surpass deep early-fusion CNNs and foundation models for multiplex representation learning. Code is available at https://github.com/SimonBon/CIM-S.</description>
      <author>example@mail.com (Simon Gutwein, Arthur Longuefosse, Jun Seita, Sabine Taschner-Mandl, Roxane Licandro)</author>
      <guid isPermaLink="false">2512.15410v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Topological Metric for Unsupervised Embedding Quality Evaluation</title>
      <link>http://arxiv.org/abs/2512.15285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Persistence，一种基于持久同调的无监督评估指标，用于量化嵌入空间的几何结构和拓扑丰富性，无需标签数据即可评估嵌入质量。&lt;h4&gt;背景&lt;/h4&gt;现代表示学习越来越多地依赖在大规模无标签数据上训练的无监督和自监督方法，这些方法虽在跨任务和跨领域实现了令人印象深刻的泛化能力，但在无标签情况下评估嵌入质量仍是开放挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为Persistence的拓扑感知指标，基于持久同调理论，以完全无监督的方式量化嵌入空间的几何结构和拓扑丰富性。&lt;h4&gt;方法&lt;/h4&gt;Persistence是一种基于持久同调的拓扑感知指标，能够捕捉嵌入空间的全局和多尺度组织结构，不假设线性可分性或依赖协方差结构。&lt;h4&gt;主要发现&lt;/h4&gt;在多个不同领域的实验结果表明，Persistence与下游性能始终保持顶级相关性，优于现有的无监督指标，并能够实现可靠模型和超参数选择。&lt;h4&gt;结论&lt;/h4&gt;Persistence提供了一种有效的无监督嵌入质量评估方法，不依赖于标签数据，能够捕捉嵌入空间的全局拓扑结构，在模型选择和超参数优化方面具有实用价值。&lt;h4&gt;翻译&lt;/h4&gt;现代表示学习越来越依赖于在大规模无标签数据上训练的无监督和自监督方法。虽然这些方法在跨任务和跨领域方面实现了令人印象深刻的泛化能力，但在没有标签的情况下评估嵌入质量仍然是一个开放的挑战。在这项工作中，我们提出了Persistence，一种基于持久同调的拓扑感知指标，以完全无监督的方式量化嵌入空间的几何结构和拓扑丰富性。与假设线性可分性或依赖协方差结构的指标不同，Persistence能够捕捉全局和多尺度的组织结构。在多个不同领域的实验结果表明，Persistence与下游性能始终保持顶级相关性，优于现有的无监督指标，并能够实现可靠模型和超参数选择。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何在没有标签的情况下评估无监督学习嵌入质量的问题。随着现代表征学习越来越依赖无监督和自监督方法，评估这些方法生成的嵌入质量变得至关重要，因为可靠的标签无关评估方法对于可扩展和自主的学习系统（如推荐系统和信息检索）不可或缺，而现有评估方法存在局限性，无法全面捕捉嵌入空间的几何结构复杂性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到嵌入必须保留数据流形的几何结构才能在未见任务上表现良好，而现有评估方法依赖简化假设（如线性可分性或协方差结构）。作者借鉴了拓扑数据分析中的持久同调工具，这是一种已被成功应用于其他领域但在嵌入评估中较少探索的技术。通过构建Vietoris-Rips复形并计算拓扑特征寿命的总和，作者设计了Persistence指标来量化嵌入空间的几何丰富性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是嵌入质量可以通过其拓扑结构评估，高质量的嵌入应保留原始数据的拓扑特性。实现流程包括：1)构建嵌入点云的Vietoris-Rips复形；2)追踪当连接半径增加时拓扑特征的出现和消失；3)记录拓扑特征的出生和死亡值；4)计算持久条形图总结跨尺度拓扑转换；5)计算总持久性作为拓扑复杂性的度量，反映嵌入保留的内在维度和判别结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出基于持久同调的Persistence指标；构建Vietoris-Rips复形分析嵌入空间拓扑结构；计算H0和H1同调群的持久图；提供无需标签的几何敏感度量。相比之前工作，Persistence不依赖线性可分性假设或协方差结构；能捕捉全局和多尺度结构；是模型无关的；评估更全面的几何结构，包括环、空腔等复杂拓扑特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于持久同调的拓扑感知指标Persistence，能够在无监督条件下准确评估嵌入质量，通过量化嵌入空间的几何结构丰富性，为模型选择和超参数优化提供了可靠指导，在多个领域的实验中显著优于现有无监督评估方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.</description>
      <author>example@mail.com (Aleksei Shestov, Anton Klenitskiy, Daria Denisova, Amurkhan Dzagkoev, Daniil Petrovich, Andrey Savchenko, Maksim Makarenko)</author>
      <guid isPermaLink="false">2512.15285v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>On the Use of Self-Supervised Representation Learning for Speaker Diarization and Separation</title>
      <link>http://arxiv.org/abs/2512.15224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at ASRU25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了近期自监督语音模型在说话人分割和语音分离这两个与说话人身份相关任务上的质量表现，指出了当前文献中的评估不足问题。&lt;h4&gt;背景&lt;/h4&gt;过去几年，wav2vec2.0和WavLM等自监督语音模型已被证明能显著提高许多下游语音任务的性能，特别是在低资源环境中。&lt;h4&gt;目的&lt;/h4&gt;研究近期自监督语音表示在说话人分割和语音分离这两个与说话人身份相关任务上的质量。&lt;h4&gt;方法&lt;/h4&gt;对近期自监督语音表示在说话人分割和语音分离任务上的质量进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;当前文献中存在评估不足的问题，这源于现有基准测试的限制，特别是评估数据集的多样性不足以及与分割和分离相关的下游系统种类有限。&lt;h4&gt;结论&lt;/h4&gt;需要更全面和多样化的评估来充分了解自监督语音模型在说话人相关任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，wav2vec2.0和WavLM等自监督语音模型已被证明能显著提高许多下游语音任务的性能，特别是在低资源环境中。尽管如此，在说话人分割和语音分离等任务上的评估仍然有限。本文研究了近期自监督语音表示在这两个与说话人身份相关任务上的质量，指出了当前文献中的差距，这些差距源于现有基准测试的限制，特别是评估数据集的多样性不足以及与分割和分离相关的下游系统种类有限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised speech models such as wav2vec2.0 and WavLM have been shown to significantly improve the performance of many downstream speech tasks, especially in low-resource settings, over the past few years. Despite this, evaluations on tasks such as Speaker Diarization and Speech Separation remain limited. This paper investigates the quality of recent self-supervised speech representations on these two speaker identity-related tasks, highlighting gaps in the current literature that stem from limitations in the existing benchmarks, particularly the lack of diversity in evaluation datasets and variety in downstream systems associated to both diarization and separation.</description>
      <author>example@mail.com (Séverin Baroudi, Hervé Bredin, Joseph Razik, Ricard Marxer)</author>
      <guid isPermaLink="false">2512.15224v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption</title>
      <link>http://arxiv.org/abs/2512.15112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FUEL的无监督节点表示学习方法，它能够自适应地学习图卷积使用的适当程度，通过增强嵌入空间中的类内相似性和类间可分性来提升性能。FUEL利用节点特征识别节点簇，并将这些簇视为类的代理。在15种基线方法和14个基准数据集上的广泛实验表明，FUEL在不同同质性的图上都能实现最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督节点表示学习旨在不依赖节点标签的情况下获取有意义的节点嵌入。通常使用图卷积来聚合邻居节点的信息，从而编码节点特征和图拓扑结构。然而，过度依赖图卷积可能是次优的，特别是在非同质图中，因为它可能对特征或拓扑属性不同的节点产生过于相似的嵌入。&lt;h4&gt;目的&lt;/h4&gt;解决无监督场景下调整图卷积使用程度的研究不足问题，提出一种自适应学习图卷积使用适当程度的方法，以增强嵌入空间中的类内相似性和类间可分性。&lt;h4&gt;方法&lt;/h4&gt;提出FUEL方法，它自适应地学习图卷积使用的适当程度，利用节点特征识别节点簇，并将这些簇视为类的代理，从而增强嵌入空间中的类内相似性和类间可分性。&lt;h4&gt;主要发现&lt;/h4&gt;通过15种基线方法和14个基准数据集的广泛实验，证明了FUEL在下游任务中的有效性，在不同同质性的图上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FUEL成功解决了无监督场景下调整图卷积使用程度的问题，通过自适应学习图卷积使用的适当程度，显著提升了节点表示学习的性能。&lt;h4&gt;翻译&lt;/h4&gt;无监督节点表示学习旨在不依赖节点标签的情况下获取有意义的节点嵌入。为实现这一目标，通常采用图卷积（聚合来自邻居节点的信息）来编码节点特征和图拓扑结构。然而，过度依赖图卷积可能是次优的，特别是在非同质图中，因为它可能对特征或拓扑属性不同的节点产生过于相似的嵌入。因此，调整图卷积使用程度的问题已在监督学习环境中得到积极探索，而在无监督场景中，此类方法仍研究不足。为解决这一问题，我们提出了FUEL，它通过旨在增强嵌入空间中的类内相似性和类间可分性，自适应地学习图卷积使用的适当程度。由于类别未知，FUEL利用节点特征识别节点簇，并将这些簇视为类别的代理。通过使用15种基线方法和14个基准数据集的广泛实验，我们证明了FUEL在下游任务中的有效性，在不同同质性水平的图上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.</description>
      <author>example@mail.com (Sunwoo Kim, Soo Yong Lee, Kyungho Kim, Hyunjin Hwang, Jaemin Yoo, Kijung Shin)</author>
      <guid isPermaLink="false">2512.15112v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</title>
      <link>http://arxiv.org/abs/2512.14796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 5 tables, submitted to AMIA 2026 Informatics Summit&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为放大感知蒸馏(MAD)的自监督策略，解决了全切片图像(WSIs)中不同放大级别被视为独立视图的问题，使模型能够学习到分辨率不变的表示。&lt;h4&gt;背景&lt;/h4&gt;全切片图像包含分布在多个放大级别上的组织信息，但大多数自监督方法将这些尺度视为独立视图，这种分离阻止了模型学习在分辨率变化时保持稳定的表示，而这对于神经病理学工作流程至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够连接低放大倍数上下文与高放大倍数细节的自监督方法，使模型学习粗略组织结构与精细细胞模式之间的关系，实现分辨率不变的表示学习。&lt;h4&gt;方法&lt;/h4&gt;引入放大感知蒸馏(MAD)策略，基于跨尺度对应关系完全无注释地训练基础模型MAD-NP，将低放大倍数上下文与空间对齐的高放大倍数细节联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;仅在10倍嵌入上训练的线性分类器应用于未见过的40倍切片时保持96.7%的性能，证明了强大的分辨率不变表示学习能力；分割输出在不同放大倍数下保持一致，保留解剖边界并最小化噪声。&lt;h4&gt;结论&lt;/h4&gt;研究结果突显了使用统一嵌入空间进行可扩展、放大倍数鲁棒的全切片图像分析的可行性。&lt;h4&gt;翻译&lt;/h4&gt;全切片图像包含分布在多个放大级别上的组织信息，然而大多数自监督方法将这些尺度视为独立视图。这种分离阻止了模型学习在分辨率变化时保持稳定的表示，这是实际神经病理学工作流程的关键要求。本研究引入了放大感知蒸馏(MAD)，一种自监督策略，将低放大倍数上下文与空间对齐的高放大倍数细节联系起来，使模型能够学习粗略组织结构与精细细胞模式之间的关系。由此产生的基础模型MAD-NP完全通过这种跨尺度对应关系进行训练，无需注释。仅在10倍嵌入上训练的线性分类器应用于未见过的40倍切片时保持96.7%的性能，证明了强大的分辨率不变表示学习能力。分割输出在不同放大倍数下保持一致，保留解剖边界并最小化噪声。这些结果突显了使用统一嵌入空间进行可扩展、放大倍数鲁棒的全切片图像分析的可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whole-slide images (WSIs) contain tissue information distributed across multiple magnification levels, yet most self-supervised methods treat these scales as independent views. This separation prevents models from learning representations that remain stable when resolution changes, a key requirement for practical neuropathology workflows. This study introduces Magnification-Aware Distillation (MAD), a self-supervised strategy that links low-magnification context with spatially aligned high-magnification detail, enabling the model to learn how coarse tissue structure relates to fine cellular patterns. The resulting foundation model, MAD-NP, is trained entirely through this cross-scale correspondence without annotations. A linear classifier trained only on 10x embeddings maintains 96.7% of its performance when applied to unseen 40x tiles, demonstrating strong resolution-invariant representation learning. Segmentation outputs remain consistent across magnifications, preserving anatomical boundaries and minimizing noise. These results highlight the feasibility of scalable, magnification-robust WSI analysis using a unified embedding space</description>
      <author>example@mail.com (Mahmut S. Gokmen, Mitchell A. Klusty, Peter T. Nelson, Allison M. Neltner, Sen-Ching Samson Cheung, Thomas M. Pearce, David A Gutman, Brittany N. Dugger, Devavrat S. Bisht, Margaret E. Flanagan, V. K. Cody Bumgardner)</author>
      <guid isPermaLink="false">2512.14796v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation</title>
      <link>http://arxiv.org/abs/2512.14755v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 0 figures. Accepted at Advances in Representation Learning for Earth Observation (REO) at EurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了SkyCap数据集，一个用于线性基础设施监测的双时相高分辨率光学-SAR数据集，并评估了基础模型在SAR振幅变化检测任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;线性基础设施监测需要可靠的高分辨率数据和规律的采集节奏。光学超高分辨率图像易于解释和标注，但云层会破坏采集节奏。合成孔径雷达(SAR)可以在任何天气条件下获取数据，但难以标注。&lt;h4&gt;目的&lt;/h4&gt;构建一个双时相VHR光学-SAR数据集；通过光学到SAR的标签转移获得SAR振幅变化检测标签，无需SAR专家标注；评估基础模型在SAR ACD任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;通过档案匹配和共配准SkySat(光学)和Capella Space(SAR)场景构建SkyCap数据集；使用光学到SAR的标签转移技术获取SAR ACD标签；在SAR数据上继续预训练SARATR-X模型；在SkyCap数据集上评估经过预训练的SAR特定基础模型和光学基础模型，使用不同的预处理选择。&lt;h4&gt;主要发现&lt;/h4&gt;MTP(ViT-B+RVSA)，一个光学基础模型，使用dB+Z-score预处理获得最佳结果(F1_c = 45.06)，优于直接在Capella数据上进一步预训练的SAR特定基础模型；模型对与预训练统计对齐的预处理非常敏感；光学模型在光学变化检测上的排名不会一对一地转移到SAR ACD。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是第一个在VHR SAR ACD上评估基础模型的研究，表明光学基础模型在某些条件下可能优于专门针对SAR训练的模型。&lt;h4&gt;翻译&lt;/h4&gt;线性基础设施监测的变化检测需要可靠的高分辨率数据和规律的采集节奏。光学超高分辨率(VHR)图像易于解释和标注，但云层会破坏这种采集节奏。合成孔径雷达(SAR)能够在任何天气条件下获取数据，但难以标注。我们介绍了SkyCap，一个通过档案匹配和共配准(光学)SkySat和Capella Space(SAR)场景构建的双时相VHR光学-SAR数据集。我们利用光学到SAR的标签转移来获取SAR振幅变化检测(ACD)标签，而无需SAR专家标注。我们在SAR数据上对SARATR-X进行了持续预训练，并在SkyCap数据集上，针对不同的预处理选择，将由此产生的SAR特定基础模型(FMs)与SARATR-X以及光学FMs进行了基准测试。在评估的模型中，MTP(ViT-B+RVSA)，一个光学FM，使用dB+Z-score预处理获得了最佳结果(F1_c = 45.06)，优于直接在Capella数据上进一步预训练的SAR特定FMs。我们观察到模型对与预训练统计对齐的预处理非常敏感，并且光学模型在光学变化检测上的排名不会一对一地转移到SAR ACD。据我们所知，这是第一个在VHR SAR ACD上评估基础模型的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Change detection for linear infrastructure monitoring requires reliable high-resolution data and regular acquisition cadence. Optical very-high-resolution (VHR) imagery is interpretable and straightforward to label, but clouds break this cadence. Synthetic Aperture Radar (SAR) enables all-weather acquisitions, yet is difficult to annotate. We introduce SkyCap, a bitemporal VHR optical-SAR dataset constructed by archive matching and co-registration of (optical) SkySat and Capella Space (SAR) scenes. We utilize optical-to-SAR label transfer to obtain SAR amplitude change detection (ACD) labels without requiring SAR-expert annotations. We perform continued pretraining of SARATR-X on our SAR data and benchmark the resulting SAR-specific foundation models (FMs) together with SARATR-X against optical FMs on SkyCap under different preprocessing choices. Among evaluated models, MTP(ViT-B+RVSA), an optical FM, with dB+Z-score preprocessing attains the best result (F1$_c$ = 45.06), outperforming SAR-specific FMs further pretrained directly on Capella data. We observe strong sensitivity to preprocessing alignment with pretraining statistics, and the ranking of optical models on optical change detection does not transfer one-to-one to SAR ACD. To our knowledge, this is the first evaluation of foundation models on VHR SAR ACD.</description>
      <author>example@mail.com (Paul Weinmann, Ferdinand Schenck, Martin Šiklar)</author>
      <guid isPermaLink="false">2512.14755v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Spatia: Video Generation with Updatable Spatial Memory</title>
      <link>http://arxiv.org/abs/2512.15716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://zhaojingjing713.github.io/Spatia/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Spatia是一种空间记忆感知的视频生成框架，通过保存3D场景点云作为持久空间记忆，并使用视觉SLAM更新它，解决了现有模型在长期空间和时间一致性方面的问题。&lt;h4&gt;背景&lt;/h4&gt;现有视频生成模型难以维持长期空间和时间一致性，因为视频信号具有密集、高维的特性。&lt;h4&gt;目的&lt;/h4&gt;克服现有视频生成模型在维持长期空间和时间一致性方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出Spatia框架，明确保存3D场景点云作为持久空间记忆，基于此空间记忆迭代生成视频片段，并通过视觉SLAM持续更新它。采用动态-静态解耦设计增强空间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;动态-静态解耦设计可以增强整个生成过程中的空间一致性，同时保留了模型产生逼真动态实体的能力。&lt;h4&gt;结论&lt;/h4&gt;Spatia实现了显式相机控制和3D感知交互编辑等应用，为可扩展、内存驱动的视频生成提供了几何基础框架。&lt;h4&gt;翻译&lt;/h4&gt;现有的视频生成模型由于视频信号的密集、高维特性，难以维持长期的空间和时间一致性。为了克服这一局限，我们提出了Spatia，一种空间记忆感知的视频生成框架，明确地将3D场景点云保存为持久空间记忆。Spatia基于此空间记忆迭代生成视频片段，并通过视觉SLAM持续更新它。这种动态-静态解耦设计增强了整个生成过程中的空间一致性，同时保留了模型产生逼真动态实体的能力。此外，Spatia实现了显式相机控制和3D感知交互编辑等应用，为可扩展、内存驱动的视频生成提供了几何基础框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有视频生成模型难以维持长期空间和时间一致性的问题。视频信号是密集且高维的，导致模型难以编码长期历史信息，例如5秒视频就包含约36,000个时空token，而相同数量的token在语言模型中可表示约27,000个单词。这个问题在现实中很重要，因为许多应用如世界模型、AI游戏生成和具身AI需要长时间的视频生成，要求时间一致性和持久记忆，而现有模型难以满足这些需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有视频生成模型的局限性，借鉴了语言模型中的记忆机制，但考虑到视频与文本的本质差异，设计了专门针对视频的显式记忆机制。他们借鉴了视觉SLAM技术用于3D场景重建和更新，参考了3D高斯溅射的渲染过程用于相机控制，并采用了多模态条件生成框架。通过将静态场景与动态实体分离，使用3D场景点云作为空间记忆，并迭代更新，解决了长期一致性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是维护一个3D场景点云作为显式的空间记忆，在视频生成过程中基于这个记忆进行条件生成，并通过视觉SLAM不断更新它。这种方法实现了动态-静态分离，将静态场景作为持久记忆，同时生成与场景交互的动态实体。训练阶段包括数据预处理、视图特定场景点云估计、参考帧检索和多模态条件生成。推理阶段从初始图像估计初始点云，用户指定文本和相机轨迹，渲染场景投影视频，检索参考帧，生成新视频片段，然后更新空间记忆，迭代执行以生成长视频。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 动态-静态分离，将静态场景作为空间记忆同时生成动态实体，而之前方法通常只能处理静态场景；2) 空间一致性生成，可从不同视角生成保持空间结构一致的视频；3) 显式相机控制，通过3D感知方式直接应用相机路径到点云并渲染，而非间接编码相机轨迹；4) 3D感知交互式编辑，用户可直接编辑点云并反映在生成视频中。相比之前工作，Spatia首次实现了显式空间记忆机制，同时支持动态内容生成、多视角一致性、精确相机控制和交互式编辑。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Spatia通过维护和迭代更新3D场景点云作为显式空间记忆，实现了具有长期空间和时间一致性的高质量视频生成，同时支持动态内容生成、多视角一致性、精确相机控制和3D感知交互式编辑。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.</description>
      <author>example@mail.com (Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu, Yan Lu)</author>
      <guid isPermaLink="false">2512.15716v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>OMCL: Open-vocabulary Monte Carlo Localization</title>
      <link>http://arxiv.org/abs/2512.15557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE RA-L&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于视觉-语言特征的蒙特卡洛定位方法，实现机器人测量与环境地图特征的稳健关联，支持通过自然语言描述进行全局定位初始化，并在室内外场景中展示了良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;稳健的机器人定位是导航规划的重要前提。当环境地图由不同传感器创建时，机器人测量必须与地图特征进行稳健关联。&lt;h4&gt;目的&lt;/h4&gt;扩展蒙特卡洛定位方法，利用视觉-语言特征实现机器人观测与3D地图之间的稳健关联，并支持通过自然语言描述进行全局定位初始化。&lt;h4&gt;方法&lt;/h4&gt;使用视觉-语言特征扩展蒙特卡洛定位，这些开放词汇特征能根据相机姿态和由RGB-D图像或对齐点云创建的3D地图，稳健计算视觉观测的可能性；抽象视觉-语言特征可关联不同模态的观测和地图元素；全局定位可通过位置周围物体的自然语言描述初始化。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在Matterport3D和Replica数据集的室内场景以及SemanticKITTI户外场景中进行了评估，展示了良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;基于视觉-语言特征的蒙特卡洛定位方法能够有效处理多传感器创建的环境地图，实现稳健的机器人定位，并通过自然语言描述支持全局定位初始化。&lt;h4&gt;翻译&lt;/h4&gt;稳健的机器人定位是导航规划的重要前提。如果环境地图是由不同传感器创建的，机器人测量必须与地图特征进行稳健关联。在这项工作中，我们使用视觉-语言特征扩展了蒙特卡洛定位。这些开放词汇特征能够根据相机姿态和由姿态RGB-D图像或对齐点云创建的3D地图，稳健地计算视觉观测的可能性。抽象视觉-语言特征能够关联来自不同模态的观测和地图元素。全局定位可以通过位置周围存在的物体的自然语言描述来初始化。我们使用Matterport3D和Replica评估了室内场景的方法，并在SemanticKITTI上展示了户外场景的泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人在复杂环境中的鲁棒定位问题，特别是在使用不同传感器创建的地图中如何将机器人观测与地图特征进行稳健关联。这个问题在现实中非常重要，因为准确的定位是机器人导航的前提，而现实环境中的地图通常由多种传感器（如RGB-D相机、激光雷达等）创建，不同传感器数据存在模态差异，如何有效关联这些数据是机器人自主导航的关键挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统定位方法在处理多模态传感器数据时的局限性，然后考虑如何利用语义信息提高定位鲁棒性。他们借鉴了蒙特卡洛定位(MCL)的基本框架，但扩展它以使用开放词汇的视觉-语言特征。作者采用了对比语言-图像预训练模型(如CLIP)来提取抽象特征，这些特征能够关联不同模态的观测和地图元素。他们还设计了两种映射方法处理不同传感器数据，并引入了基于文本提示的初始化方法。作者确实借鉴了现有工作，包括MCL框架、视觉-语言模型、语义分割技术和八叉树地图表示方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用开放词汇的视觉-语言特征来表示环境和观测，将蒙特卡洛定位与这些特征相结合，通过计算观测特征与地图特征之间的相似度来确定机器人位置。整体流程分为三个阶段：1)映射阶段：创建八叉树语言地图存储视觉-语言特征，支持从RGB-D图像或点云构建；2)定位阶段：使用蒙特卡洛定位，从RGB图像提取特征，通过光线追踪获取地图对应位置特征，计算相似度赋予权重，采用分层光线采样提高效率；3)初始化阶段：使用文本提示描述可能初始位置，在匹配位置附近初始化粒子。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)语言基础的定位：将位姿估计建立在语言特征基础上，使用开放词汇提示加速全局定位；2)跨模态传感器使用：构建统一的稀疏语言地图，支持不同传感器用于映射和定位；3)泛化能力：兼容独立构建的点云，可在室内外环境有效工作。相比之前的工作，不同之处在于：不依赖特定几何特征而使用语义一致性；使用开放词汇特征而非预定义类别；专注于定位任务而非场景理解；使用八叉树语言地图高效存储特征；支持从不同传感器输入构建统一地图。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OMCL通过结合蒙特卡洛定位与开放词汇的视觉-语言特征，实现了跨模态传感器下的鲁棒机器人定位，并支持自然语言描述的初始化，显著提升了定位的准确性和灵活性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.</description>
      <author>example@mail.com (Evgenii Kruzhkov, Raphael Memmesheimer, Sven Behnke)</author>
      <guid isPermaLink="false">2512.15557v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision</title>
      <link>http://arxiv.org/abs/2512.15020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为隐式场景监督（ISS）策略的3D视觉运动DiT扩散模型，通过点云观测预测连续动作序列，解决了基于视觉的模仿学习训练效率低和泛化能力差的问题。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的模仿学习已实现令人印象深刻的机器人操作技能，但依赖物体外观而忽略底层3D场景结构，导致训练效率低下和泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉模仿学习方法效率低和泛化能力差的问题，提高机器人操作的性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;扩展DiT模型，添加新颖的隐式场景监督模块，鼓励模型产生与场景几何演化一致的输出，从点云观测中预测连续动作序列。&lt;h4&gt;主要发现&lt;/h4&gt;ISS策略在单臂操作任务（MetaWorld）和灵巧手操作（Adroit）上达到最先进性能；在真实世界实验中表现出强大泛化能力和鲁棒性；方法能随数据和参数增加有效扩展。&lt;h4&gt;结论&lt;/h4&gt;隐式场景监督策略显著提高了机器人操作的性能和泛化能力，代码和视频将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的模仿学习已实现了令人印象深刻的机器人操作技能，但其对物体外观的依赖而忽略底层3D场景结构导致训练效率低下和泛化能力差。为解决这些挑战，我们引入隐式场景监督（ISS）策略，一种基于3D视觉运动DiT的扩散策略，从点云观测中预测连续动作序列。我们通过扩展DiT并添加新颖的隐式场景监督模块来鼓励模型产生与场景几何演化一致的输出，从而提高策略的性能和鲁棒性。值得注意的是，ISS策略在单臂操作任务（MetaWorld）和灵巧手操作（Adroit）上都达到了最先进的性能。在真实世界实验中，它还表现出强大的泛化能力和鲁棒性。额外的消融研究表明，我们的方法能够随着数据和参数的增加而有效扩展。代码和视频将发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉引导机器人学习方法中过度依赖物体外观而忽略底层3D场景结构的问题，这导致训练效率低下和泛化能力差。这个问题很重要，因为机器人需要在非结构化环境中执行复杂任务，而精确物体状态数据稀缺且获取成本高；2D图像方法缺乏深度信息造成空间歧义；现有3D方法计算密集且学习效率低下，限制了机器人操作在实际应用中的性能和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D方法缺乏深度信息，3D方法计算密集且学习效率低。基于这些分析，他们设计了一个基于DiT的3D视觉运动策略，使用点云作为输入提供丰富几何信息。他们借鉴了扩散模型在机器人控制中的应用（如Diffusion Policy和DP3），采用DiT架构提高可扩展性，并创新性地引入了隐式场景监督模块，灵感来自如果模型输出正确动作应能准确预测未来场景的观点。整个设计旨在平衡几何理解与计算效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过隐式场景监督模块预测未来点云特征，强制执行长期几何一致性，使策略能隐式建模场景动态，同时利用点云提供的丰富几何信息提高操作性能。整体流程包括：1)将单视图深度转换为稀疏点云并编码为上下文表示；2)使用基于DiT的编码器-解码器架构处理带噪声的动作序列；3)通过隐式场景监督模块预测K步未来点云；4)结合去噪损失和隐式场景监督损失进行训练；5)推理时仅使用DiT策略，不增加计算开销。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引入隐式场景监督模块，强制模型输出与场景几何演化一致；2)使用稀疏点云而非密集体素网格，平衡几何信息与计算效率；3)采用DiT架构提高可扩展性和计算效率；4)设计计划采样策略稳定训练。相比2D方法，本文提供更丰富的几何上下文，更好处理遮挡和精细接触任务；相比现有3D方法，本文引入额外几何监督信号，提高学习效率，具有更好可扩展性和稳定性，推理速度更快（比DP3快约3倍）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了ISS Policy，一种通过隐式场景监督有效利用点云几何信息的3D视觉运动策略，实现了高训练效率、强可扩展性和卓越泛化能力，在模拟和真实世界机器人操作任务中达到最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.</description>
      <author>example@mail.com (Wenlong Xia, Jinhao Zhang, Ce Zhang, Yaojia Wang, Youmin Gong, Jie Mei)</author>
      <guid isPermaLink="false">2512.15020v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction</title>
      <link>http://arxiv.org/abs/2512.12378v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了M4Human，一个大规模多模态基准数据集，用于人类网格重建研究。该数据集包含661K帧的高分辨率毫米波雷达、RGB和深度数据，是目前最大规模的同类数据集，比之前最大的数据集大9倍。数据集包括20个受试者和50种不同动作的高质量动作捕捉标注。&lt;h4&gt;背景&lt;/h4&gt;现有的大规模HMR数据集主要依赖视线RGB输入，但视觉感知受到遮挡、光照变化和隐私问题的限制。虽然毫米波雷达可解决这些问题，但当前雷达数据集存在稀疏骨架标签、规模有限和动作简单等限制。&lt;h4&gt;目的&lt;/h4&gt;为了推进HMR研究社区，引入M4Human这一大规模多模态基准数据集，克服现有数据集的局限性，为雷达基础人体建模研究提供更丰富的资源。&lt;h4&gt;方法&lt;/h4&gt;创建M4Human数据集，包含原始雷达张量(RT)和处理后的雷达点云(RPC)，支持不同级别的射频信号粒度研究；提供高质量动作捕捉标注，包括3D网格和全局轨迹；涵盖20个受试者和50种多样动作；建立RT和RPC模态基准，以及与RGB-D模态的多模态融合基准。&lt;h4&gt;主要发现&lt;/h4&gt;广泛实验结果突显了M4Human对雷达基础人体建模的重要性，同时揭示了在快速、不受约束运动下仍然存在的挑战。&lt;h4&gt;结论&lt;/h4&gt;M4Human数据集和代码将在论文发表后发布，这将促进HMR研究社区的发展，特别是在使用毫米波雷达进行隐私保护室内人体感知方面。&lt;h4&gt;翻译&lt;/h4&gt;人类网格重建(HMR)为身体与环境之间的直接互动提供了洞察，使各种沉浸式应用成为可能。虽然现有的大规模HMR数据集严重依赖视线RGB输入，但基于视觉的感知受到遮挡、光照变化和隐私问题的限制。为了克服这些限制，最近的研究探索了射频毫米波雷达用于隐私保护的室内人体感知。然而，当前的雷达数据集受到稀疏骨架标签、规模有限和简单原地动作的限制。为了推进HMR研究社区，我们引入了M4Human，这是目前最大规模(661K帧)(比之前最大的数据集大9倍)的多模态基准，具有高分辨率的毫米波雷达、RGB和深度数据。M4Human提供原始雷达张量(RT)和处理后的雷达点云(RPC)，以支持不同级别的射频信号粒度的研究。M4Human包含具有3D网格和全局轨迹的高质量动作捕捉(MoCap)标注，涵盖20个受试者和50种不同动作，包括原地动作、原地坐姿和自由空间运动或康复动作。我们在RT和RPC模态以及与RGB-D模态的多模态融合上建立了基准。广泛的结果突显了M4Human对于雷达基础人体建模的重要性，同时揭示了在快速、不受约束运动下仍然存在的挑战。该数据集和代码将在论文发表后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有人类网格重建(HMR)数据集过度依赖RGB视觉输入的问题，以及毫米波雷达数据集规模小、动作简单、标签质量有限的问题。这个问题很重要，因为视觉传感在隐私敏感场景(如老人和儿童护理)中存在隐私问题，且容易受光照变化和遮挡影响，而毫米波雷达能提供隐私保护、光照不变性和抗遮挡能力，对VR/AR、虚拟试衣、康复训练等多种应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有视觉传感和毫米波雷达在HMR中的局限性，设计了一个整合高分辨率毫米波雷达、RGB-D相机和Vicon动作捕捉系统的多模态平台。他们借鉴了现有动作捕捉技术进行高质量标注，参考了雷达数据处理方法(包括RT和RPC)，并采用了SMPL-X模型表示人体。同时，他们参考了现有评估指标，但创新性地结合了多种模态和高质量标注，构建了一个前所未有的大规模数据集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个大规模、高质量、多模态的毫米波雷达数据集，支持高保真度的RF-based HMR。实现流程包括：1)使用高分辨率雷达、RGB-D相机和Vicon系统采集多模态数据；2)通过PnP问题和雷达可见目标进行精确的空间校准；3)使用Vicon系统进行标记采集和人工清理；4)用SOMA神经网络重建SMPL-X风格人体网格；5)组织包含20个受试者和50类动作的多样化数据集；6)建立单模态和多模态融合的基准测试。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)构建了目前最大的毫米波雷达HMR数据集(661K帧，是之前最大数据集的9倍)；2)提供四种同步模态(RGB、深度、RT和RPC)；3)使用基于标记的Vicon系统提供高质量3D网格标注；4)包含50类多样化动作，突破简单原地动作限制；5)同时提供原始雷达张量(RT)和雷达点云(RPC)；6)提出首个直接从RT进行HMR的RT-Mesh方法。相比之前工作，M4Human在规模、动作复杂性、标注质量和数据模态上都有显著突破。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; M4Human是一个大规模、高质量、多模态的毫米波雷达数据集，通过提供原始雷达张量和高质量3D网格标注，突破了现有数据集在规模、动作复杂性和标注质量上的局限，为毫米波雷达在高保真度人类网格重建中的应用奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.</description>
      <author>example@mail.com (Junqiao Fan, Yunjiao Zhou, Yizhuo Yang, Xinyuan Cui, Jiarui Zhang, Lihua Xie, Jianfei Yang, Chris Xiaoxuan Lu, Fangqiang Ding)</author>
      <guid isPermaLink="false">2512.12378v2</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis</title>
      <link>http://arxiv.org/abs/2512.15295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;控制器合成是一种形式化方法，用于自动生成满足特定性质的标记转换系统控制器。本文提出GCRL方法，通过集成图神经网络增强基于强化学习的控制器合成方法，提高了学习效率和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;控制器合成的效率严重依赖于探索策略，现有方法通常依赖固定规则或仅考虑有限当前特征的强化学习策略，限制了性能提升。&lt;h4&gt;目的&lt;/h4&gt;解决现有控制器合成方法中探索策略的局限性，开发一种能捕捉更广泛上下文信息的方法，提高合成效率和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出GCRL方法，将标记转换系统(LTS)的探索历史编码为图结构，利用图神经网络(GNN)捕捉非基于当前的环境上下文，增强强化学习方法的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准域的对比实验中，GCRL在四个域中表现出比最先进方法更优的学习效率和泛化能力，但在一个具有高度对称性和严格局部交互的特殊域中表现不佳。&lt;h4&gt;结论&lt;/h4&gt;GCRL通过集成图神经网络和考虑探索历史，显著提高了控制器合成的效率和泛化能力，但在处理具有高度对称性和严格局部交互的系统时仍有改进空间。&lt;h4&gt;翻译&lt;/h4&gt;控制器合成是一种形式化方法，用于自动生成满足特定性质的标记转换系统控制器。然而，合成过程的效率严重依赖于探索策略。这些策略通常依赖固定规则或通过强化学习学习的策略，仅考虑有限的当前特征集。为解决这一限制，本文引入GCRL，一种通过集成图神经网络增强基于RL的方法。GCRL将LTS探索历史编码为图结构，使其能够捕捉更广泛的非基于当前的环境上下文。在与最先进方法的对比实验中，GCRL在五个基准域中的四个表现出更优的学习效率和泛化能力，除一个具有高度对称性和严格局部交互的特殊域外。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.</description>
      <author>example@mail.com (Toshihide Ubukata, Enhong Mu, Takuto Yamauchi, Mingyue Zhang, Jialong Li, Kenji Tei)</author>
      <guid isPermaLink="false">2512.15295v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures</title>
      <link>http://arxiv.org/abs/2512.15228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DBCata是一种深度生成模型，通过结合周期性布朗桥框架和等变图神经网络，能够在不明确需要能量或力信息的情况下，准确预测吸附结构和能量，比现有方法更准确，可用于加速催化剂筛选。&lt;h4&gt;背景&lt;/h4&gt;目前广泛使用的机器学习原子间势(MLIP)的训练数据主要来自近平衡结构，分布有限，导致吸附结构和吸附能量预测不可靠。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够可靠预测吸附结构和能量的方法，用于大规模催化剂筛选。&lt;h4&gt;方法&lt;/h4&gt;提出了DBCata，这是一个深度生成模型，结合了周期性布朗桥框架和等变图神经网络，建立了非弛豫结构和DFT弛豫结构之间的低维过渡流形，不需要明确的能量或力信息。&lt;h4&gt;主要发现&lt;/h4&gt;DBCata在Catalysis-Hub数据集上达到0.035 Å的原子间距离平均绝对误差(DMAE)，比当前最先进的机器学习势模型好近三倍；通过结合化学启发式和自监督异常检测方法，在94%的情况下，相应的DFT精度可提高0.1 eV以内。&lt;h4&gt;结论&lt;/h4&gt;DBCata的卓越性能促进了高效合金催化剂在氧还原反应中的加速高通量计算筛选，突显了DBCata作为催化剂设计和优化强大工具的潜力。&lt;h4&gt;翻译&lt;/h4&gt;吸附能量作为大规模催化剂筛选的关键描述符，然而，广泛使用的机器学习原子间势(MLIP)的训练数据分布有限，主要来自近平衡结构，导致吸附结构和吸附能量预测不可靠。在此背景下，我们提出了DBCata，一个深度生成模型，它结合了周期性布朗桥框架和等变图神经网络，建立了非弛豫结构和DFT弛豫结构之间的低维过渡流形，不需要明确的能量或力信息。训练后，DBCata能有效生成高保真吸附几何结构，在Catalysis-Hub数据集上达到0.035 Å的原子间距离平均绝对误差(DMAE)，比当前最先进的机器学习势模型好近三倍。此外，通过结合化学启发式和自监督异常检测方法识别和优化异常预测，在94%的情况下，相应的DFT精度可提高0.1 eV以内。我们证明了DBCata的卓越性能促进了高效合金催化剂在氧还原反应中的加速高通量计算筛选，突显了DBCata作为催化剂设计和优化强大工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The adsorption energy serves as a crucial descriptor for the large-scale screening of catalysts. Nevertheless, the limited distribution of training data for the extensively utilised machine learning interatomic potential (MLIP), predominantly sourced from near-equilibrium structures, results in unreliable adsorption structures and consequent adsorption energy predictions. In this context, we present DBCata, a deep generative model that integrates a periodic Brownian-bridge framework with an equivariant graph neural network to establish a low-dimensional transition manifold between unrelaxed and DFT-relaxed structures, without requiring explicit energy or force information. Upon training, DBCata effectively generates high-fidelity adsorption geometries, achieving an interatomic distance mean absolute error (DMAE) of 0.035 \textÅ on the Catalysis-Hub dataset, which is nearly three times superior to that of the current state-of-the-art machine learning potential models. Moreover, the corresponding DFT accuracy can be improved within 0.1 eV in 94\% of instances by identifying and refining anomalous predictions through a hybrid chemical-heuristic and self-supervised outlier detection approach. We demonstrate that the remarkable performance of DBCata facilitates accelerated high-throughput computational screening for efficient alloy catalysts in the oxygen reduction reaction, highlighting the potential of DBCata as a powerful tool for catalyst design and optimisation.</description>
      <author>example@mail.com (Songze Huo, Xiao-Ming Cao)</author>
      <guid isPermaLink="false">2512.15228v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>RELIC-GNN: Efficient State Registers Identification with Graph Neural Network for Reverse Engineering</title>
      <link>http://arxiv.org/abs/2512.15037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RELIC-GNN是一种基于图神经网络的状态寄存器识别方法，用于门级网表逆向工程，能有效分离控制信号和数据信号。&lt;h4&gt;背景&lt;/h4&gt;门级网表逆向工程对于硬件木马检测和设计盗版应对至关重要，现有方法通过拓扑比较识别状态寄存器分离控制信号和数据信号。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的状态寄存器识别方法，解决现有方法在大规模网表中效率低下的问题。&lt;h4&gt;方法&lt;/h4&gt;RELIC-GNN将寄存器的路径结构建模为图，在训练过程中考虑节点属性和图结构来生成相应表示，训练后的GNN模型可高效识别寄存器类型。&lt;h4&gt;主要发现&lt;/h4&gt;RELIC-GNN在不同设计上平均达到100%的召回率、30.49%的精确度和88.37%的准确率，比之前的方法有显著改进。&lt;h4&gt;结论&lt;/h4&gt;RELIC-GNN是一种高效的状态寄存器识别方法，能够有效解决大规模网表中的门级网表逆向工程问题。&lt;h4&gt;翻译&lt;/h4&gt;门级网表的逆向工程对于硬件木马检测和设计盗版应对至关重要。门级逆向工程的主要任务是从网表中分离控制信号和数据信号，这主要通过识别具有拓扑比较的状态寄存器来实现。然而，这些方法对于大规模网表变得效率低下。在这项工作中，我们提出了RELIC-GNN，一种基于图神经网络的状态寄存器识别方法，以解决这些问题。RELIC-GNN将寄存器的路径结构建模为图，并在训练过程中通过考虑节点属性和图结构来生成相应的表示。训练后的GNN模型可以非常高效地找到寄存器类型。实验结果表明，RELIC-GNN在不同设计上平均可以达到100%的召回率、30.49%的精确度和88.37%的准确率，比之前的方法获得了显著的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reverse engineering of gate-level netlist is critical for Hardware Trojans detection and Design Piracy counteracting. The primary task of gate-level reverse engineering is to separate the control and data signals from the netlist, which is mainly realized by identifying state registers with topological comparison.However, these methods become inefficient for large scale netlist. In this work, we propose RELIC-GNN, a graph neural network based state registers identification method, to address these issues. RELIC-GNN models the path structure of register as a graph and generates corresponding representation by considering node attributes and graph structure during training. The trained GNN model could be adopted to find the registers type very efficiently. Experimental results show that RELIC-GNN could achieve 100% in recall, 30.49% in precision and 88.37% in accuracy on average across different designs, which obtains significant improvements than previous approaches.</description>
      <author>example@mail.com (Weitao Pan, Meng Dong, Zhiliang Qiu, Jianlei Yang, Zhixiong Di, Yiming Gao)</author>
      <guid isPermaLink="false">2512.15037v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs</title>
      <link>http://arxiv.org/abs/2512.14908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ATLAS是一种新颖的图学习算法，解决了图神经网络在异质图准确率下降和大规模图可扩展性方面的挑战，通过提取多级社区拓扑信息并应用多层感知器，实现了与基线方法相当的准确率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理异质图时准确率会下降，且迭代特征聚合限制了其在大规模图上的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;解决GNNs在异质图上的准确率下降问题，提高GNNs在大规模图上的可扩展性。&lt;h4&gt;方法&lt;/h4&gt;ATLAS通过提取多级细化的图社区拓扑信息，将社区分配连接到特征向量，并对得到的表示应用多层感知器(MLPs)，提供节点及其邻域的拓扑上下文而不需要聚合。&lt;h4&gt;主要发现&lt;/h4&gt;ATLAS在广泛图集上实现了与基线方法相当的准确率；对于具有负结构偏置的异质图，比GCN提高高达20个百分点；对于同质图，比MLP提高11个百分点；多分辨率社区特征系统性地调节同质和异质设置中的性能。&lt;h4&gt;结论&lt;/h4&gt;ATLAS为可解释的图学习提供了一条原则性的路径，通过多分辨率社区特征系统性地调节性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了ATLAS（针对同质图和异质图的基于自适应拓扑的大规模学习），这是一种新颖的图学习算法，解决了图神经网络(GNNs)中的两个重要挑战。首先，当图是异质图时，GNNs的准确率会下降。其次，迭代特征聚合限制了GNNs在大规模图上的可扩展性。我们通过提取多级细化的图社区拓扑信息，将社区分配连接到特征向量，并对得到的表示应用多层感知器(MLPs)来解决这些挑战。这提供了关于节点及其邻域的拓扑上下文，而不需要调用聚合。由于MLPs通常比GNNs更具可扩展性，我们的方法适用于大规模图而无需采样。在广泛的图集中，ATLAS实现了与基线方法相当的准确率，对于具有负结构偏置的异质图比GCN提高高达20个百分点，对于同质图比MLP提高11个百分点。此外，我们展示了多分辨率社区特征如何系统性地调节同质和异质设置中的性能，为可解释的图学习开辟了一条原则性的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present ATLAS (Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs), a novel graph learning algorithm that addresses two important challenges in graph neural networks (GNNs). First, the accuracy of GNNs degrades when the graph is heterophilic. Second, iterative feature aggregation limits the scalability of GNNs to large graphs. We address these challenges by extracting topological information about graph communities at multiple levels of refinement, concatenating community assignments to the feature vector, and applying multilayer perceptrons (MLPs) to the resulting representation. This provides topological context about nodes and their neighborhoods without invoking aggregation. Because MLPs are typically more scalable than GNNs, our approach applies to large graphs without the need for sampling. Across a wide set of graphs, ATLAS achieves comparable accuracy to baseline methods, with gains as high as 20 percentage points over GCN for heterophilic graphs with negative structural bias and 11 percentage points over MLP for homophilic graphs. Furthermore, we show how multi-resolution community features systematically modulate performance in both homophilic and heterophilic settings, opening a principled path toward explainable graph learning.</description>
      <author>example@mail.com (Turja Kundu, Sanjukta Bhowmick)</author>
      <guid isPermaLink="false">2512.14908v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>A Roadmap for Applying Graph Neural Networks to Numerical Data: Insights from Cementitious Materials</title>
      <link>http://arxiv.org/abs/2512.14855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了图神经网络(GNN)在混凝土材料设计中的应用，通过将表格数据转换为图形表示，实现了与传统机器学习方法相当的性能，为水泥基材料的多模态和物理信息AI模型奠定了基础。&lt;h4&gt;背景&lt;/h4&gt;机器学习在混凝土研究中应用日益增多，但面临可用数据库规模和多样性有限的挑战。传统ML框架通常局限于单一数据模态，而多模态数据库（结合数值和图形数据）是解决这一问题的有前景方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用图形结构数据的神经网络方法，建立将表格数据转换为图形表示的清晰且可复现的路径，为从传统ML向先进AI架构过渡提供基础路线图。&lt;h4&gt;方法&lt;/h4&gt;采用图神经网络(GNN)架构，利用k近邻(k-NN)方法将表格数据转换为图形表示，系统优化模型超参数和特征选择以提高预测性能，并将物理定律直接嵌入GNN架构中。&lt;h4&gt;主要发现&lt;/h4&gt;GNN能够捕获不规则或依赖于拓扑的连接关系，不仅适用于图形数据，还能从数值数据集中提取相关性。GNN性能与基准随机森林相当，后者已被证明对水泥基材料能产生可靠预测，且GNN提供了可解释的物理信息预测能力。&lt;h4&gt;结论&lt;/h4&gt;本研究是首批实施GNN设计混凝土的研究之一，所提出的框架为未来的多模态和物理信息GNN模型奠定了坚实基础，这些模型能够捕获复杂的材料行为，并加速水泥基材料的设计和优化。&lt;h4&gt;翻译&lt;/h4&gt;机器学习(ML)越来越多地应用于混凝土研究以优化性能和混合设计。然而，将ML应用于水泥基材料的主要挑战是可用数据库的规模和多样性有限。多模态数据库（结合数值和图形数据）是一个有前景的解决方案。传统ML框架在水泥研究中通常局限于单一数据模态。图神经网络(GNN)是新一代神经网络架构，能够从图形结构数据中学习，通过不规则或依赖于拓扑的连接关系捕获关系，而不仅仅是固定的空间坐标。虽然GNN专为图形数据设计，但可以调整以从数值数据集中提取相关性，并将物理定律直接嵌入其架构中，实现可解释的物理信息预测。本研究是首批实施GNN设计混凝土的研究之一，重点是建立使用k近邻(k-NN)方法将表格数据转换为图形表示的清晰且可复现的路径。系统优化模型超参数和特征选择以提高预测性能。GNN的性能与基准随机森林相当，后者已被许多研究证明对水泥基材料能产生可靠预测。总体而言，本研究为从传统ML向先进AI架构过渡提供了基础路线图。所提出的框架为未来的多模态和物理信息GNN模型奠定了坚实基础，这些模型能够捕获复杂的材料行为，并加速水泥基材料的设计和优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning (ML) has been increasingly applied in concrete research to optimize performance and mixture design. However, one major challenge in applying ML to cementitious materials is the limited size and diversity of available databases. A promising solution is the development of multi-modal databases that integrate both numerical and graphical data. Conventional ML frameworks in cement research are typically restricted to a single data modality. Graph neural network (GNN) represents a new generation of neural architectures capable of learning from data structured as graphs, capturing relationships through irregular or topology-dependent connections rather than fixed spatial coordinates. While GNN is inherently designed for graphical data, they can be adapted to extract correlations from numerical datasets and potentially embed physical laws directly into their architecture, enabling explainable and physics-informed predictions. This work is among the first few studies to implement GNNs to design concrete, with a particular emphasis on establishing a clear and reproducible pathway for converting tabular data into graph representations using the k-nearest neighbor (K-NN) approach. Model hyperparameters and feature selection are systematically optimized to enhance prediction performance. The GNN shows performance comparable to the benchmark random forest, which has been demonstrated by many studies to yield reliable predictions for cementitious materials. Overall, this study provides a foundational roadmap for transitioning from traditional ML to advanced AI architectures. The proposed framework establishes a strong foundation for future multi-modal and physics-informed GNN models capable of capturing complex material behaviors and accelerating the design and optimization of cementitious materials.</description>
      <author>example@mail.com (Mahmuda Sharmin, Taihao Han, Jie Huang, Narayanan Neithalath, Gaurav Sant, Aditya Kumar)</author>
      <guid isPermaLink="false">2512.14855v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space</title>
      <link>http://arxiv.org/abs/2512.14418v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种双轴表示-完全收敛学习（RCCL）策略，通过结合图卷积网络和无桥图编码，实现了对广阔化学空间的近乎完全覆盖，并展示了分子表示、结构完整性和模型泛化之间的定量联系。&lt;h4&gt;背景&lt;/h4&gt;机器学习正在深刻改变分子和材料建模，但化学空间极其庞大（10^30-10^60），模型能否在这个空间中实现收敛学习仍然是一个开放的科学问题。&lt;h4&gt;目的&lt;/h4&gt;解决机器学习在广阔化学空间中能否实现收敛学习的科学问题，并建立分子表示与模型泛化之间的定量联系。&lt;h4&gt;方法&lt;/h4&gt;引入RCCL策略，使用结合图卷积网络（GCN）编码的局部价环境和无桥图（NBG）编码的环/笼拓扑结构的分子表示方法；开发FD25数据集，覆盖13,302个局部价单位和165,726个环/笼拓扑结构，实现有机分子（H/C/N/O/F元素）的近乎完全组合覆盖。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络在FD25上训练表现出表示完全收敛学习和强大的分布外泛化能力；在外部基准测试中，整体预测误差约为1.0 kcal/mol MAE；建立了分子表示、结构完整性和模型泛化之间的定量联系。&lt;h4&gt;结论&lt;/h4&gt;该研究为可解释、可转移和数据高效的分子智能提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;机器学习正在深刻改变分子和材料建模；然而，鉴于化学空间的巨大规模（10^30-10^60），模型能否在这个空间中实现收敛学习仍然是一个开放的科学问题。我们引入了一种双轴表示-完全收敛学习（RCCL）策略，通过结合基于现代价键理论的图卷积网络（GCN）对局部价环境的编码，以及无桥图（NBG）对环/笼拓扑结构的编码，提供了一种化学空间覆盖的定量度量。该框架形式化了表示完整性，为构建支持大模型收敛学习的数据集奠定了原则性基础。在该RCCL框架指导下，我们开发了FD25数据集，系统覆盖了13,302个局部价单位和165,726个环/笼拓扑结构，实现了含H/C/N/O/F元素的有机分子的近乎完全组合覆盖。在FD25上训练的图神经网络表现出表示完全收敛学习和强大的分布外泛化能力，在外部基准测试中整体预测误差约为1.0 kcal/mol MAE。我们的结果建立了分子表示、结构完整性和模型泛化之间的定量联系，为可解释、可转移和数据高效的分子智能奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.</description>
      <author>example@mail.com (Dejun Hu, Zhiming Li, Jia-Rui Shen, Jia-Ning Tu, Zi-Hao Ye, Junliang Zhang)</author>
      <guid isPermaLink="false">2512.14418v2</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence</title>
      <link>http://arxiv.org/abs/2512.15160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 7 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EagleVision是一种双阶段空间智能框架，通过宏观感知和微观验证解决空间思维链中的关键挑战，在开源视觉语言模型中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的空间智能方法通常将3D线索附加到2D推理流程中，或者将多模态大语言模型与黑盒重建模块耦合，导致空间一致性弱，视点多样性有限，证据链无法追溯到支持视图。'图像思维'框架虽然展示了逐步多模态推理的能力，但未解决空间思维链中的三个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;解决空间思维链中的三个关键挑战：在严格的token预算下建立全局空间感知；明确将3D假设与视频帧关联以进行验证；为强化学习设计空间基础奖励。&lt;h4&gt;方法&lt;/h4&gt;提出EagleVision双阶段框架：1) 宏观感知阶段：使用语义-视角融合确定性点过程在固定token预算下从长视频中选择几何和语义感知关键帧；2) 微观验证阶段：将空间思维链形式化为BEV基础姿态查询，代理迭代预测姿态，检索最近实际帧，并通过空间基础奖励进行强化学习训练。&lt;h4&gt;主要发现&lt;/h4&gt;在VSI-Bench基准测试上，EagleVision在开源视觉语言模型中实现了最先进的性能，展示了强大且可泛化的空间理解能力。&lt;h4&gt;结论&lt;/h4&gt;EagleVision通过双阶段框架有效解决了空间思维链中的关键挑战，在空间智能任务上表现出色，具有强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;最近的空间智能方法通常将3D线索附加到2D推理流程中，或将多模态大语言模型与黑盒重建模块耦合，导致空间一致性弱，视点多样性有限，且证据链无法追溯到支持视图。'图像思维'框架（如ChatGPT-o3和DeepEyes）表明，通过交错假设形成和主动获取视觉证据，可以逐步出现多模态推理，但它们没有解决空间思维链中的三个关键挑战：在严格的token预算下建立全局空间感知，明确将3D假设与视频帧关联以进行验证，以及为强化学习设计空间基础奖励。为解决这些问题，我们提出了EagleVision，一个通过宏观感知和微观验证进行渐进式空间认知的双阶段框架。在宏观感知阶段，EagleVision使用语义-视角融合确定性点过程在固定token预算下从长视频中选择紧凑的几何和语义感知关键帧集合。在微观验证阶段，我们将空间思维链形式化为BEV基础姿态查询：代理在BEV平面上迭代预测姿态，检索最近的实际帧，并通过纯粹的空间基础奖励强化学习进行训练，该奖励评分预测姿态与观察视图之间的一致性。在VSI-Bench上，EagleVision在开源视觉语言模型中实现了最先进的性能，展示了强大且可泛化的空间理解能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决空间智能中的三个关键问题：空间一致性弱、视角多样性有限以及证据链无法追溯到支持视图。这些问题在现实中很重要，因为空间智能对于具身感知和多模态理解至关重要，而现有方法未能充分利用空间作为主动推理工作区，难以在有限资源下构建全局空间理解，也缺乏有效的机制将抽象的3D假设与具体视觉证据关联起来。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有空间智能方法的局限性，识别出空间链式思维的三大挑战：全局空间感知构建、3D假设与视频帧的关联、空间奖励设计。他们借鉴了'thinking with images'的方法论，如ChatGPT-o3和DeepEyes，并使用了SLAM系统(Vipe)、FG-CLIP视觉语言模型、确定性点过程(DPP)和强化学习(GRPO)等技术。在此基础上，他们创新性地设计了双阶段框架：宏观感知使用SPF-DPP选择关键帧，微观验证实现BEV基础的链式思维。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将空间推理分解为宏观感知和微观验证两个阶段，使用BEV表示作为共同坐标系，并通过主动查询机制获取额外视觉证据形成可验证证据链。宏观感知阶段使用SLAM估计相机姿态和深度，投影到BEV平面，结合语义相关性通过SPF-DPP选择关键帧；微观验证阶段维护推理状态，通过BEV定位工具主动查询特定视角的帧，添加到证据集合中，使用强化学习训练查询策略。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：双阶段框架设计、SPF-DPP算法、BEV基础的空间链式思维形式化、空间奖励机制、纯强化学习训练方法。相比之前工作，它不仅解决了空间一致性和视角多样性问题，还建立了可追溯的证据链，明确将3D假设与视觉证据关联，并在严格token预算下实现了高效的空间推理，无需依赖黑盒3D重建模块。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EagleVision通过双阶段框架结合基于BEV的链式思维，实现了高效的空间智能推理，在VSI-Bench上取得了开源视觉语言模型的最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for "thinking with images" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.</description>
      <author>example@mail.com (Jiaxu Wan, Xu Wang, Mengwei Xie, Hang Zhang, Mu Xu, Yang Han, Hong Zhang, Ding Yuan, Yifan Yang)</author>
      <guid isPermaLink="false">2512.15160v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles</title>
      <link>http://arxiv.org/abs/2512.15047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍HERO框架，用于构建层次化可遍历3D场景图，解决了现有方法在处理可操作障碍物方面的局限性，提高了导航效率和可达性。&lt;h4&gt;背景&lt;/h4&gt;3D场景图是对物理世界的强大表示，能明确建模实体间复杂的空间、语义和功能关系。具身导航利用3DSGs实现长时程推理和规划，但先前工作依赖静态世界假设，仅基于静态空间布局定义可遍历空间，将可交互障碍物视为不可遍历，限制了在真实世界场景中的有效性。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D场景图导航方法在处理可操作障碍物方面的局限性，提高导航系统的效率、可达性和扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出HERO框架构建层次化可遍历3D场景图，将可操作障碍物建模为路径，捕捉它们的物理交互性、功能语义和场景的层次关系来重新定义可遍历性。&lt;h4&gt;主要发现&lt;/h4&gt;与基线相比，HERO在部分阻塞环境中将路径长度减少了35.1%，在完全阻塞环境中将成功率提高了79.4%，显示出更高的效率和可达性。&lt;h4&gt;结论&lt;/h4&gt;HERO框架通过更好地建模可操作障碍物，显著提高了3D场景图导航系统的性能，特别是在充满障碍物的环境中。&lt;h4&gt;翻译&lt;/h4&gt;3D场景图是对物理世界的强大表示，其特点是能够明确建模实体之间复杂的空间、语义和功能关系，提供了基础理解，使智能体能够与其环境进行智能交互并执行多样化行为。具身导航作为此类能力的关键组成部分，利用3DSGs的紧凑和表达性，在复杂大规模环境中实现长时程推理和规划。然而，先前的工作依赖于静态世界假设，仅基于静态空间布局定义可遍历空间，从而将可交互障碍物视为不可遍历。这一基本限制严重削弱了它们在真实世界场景中的有效性，导致有限的可达性、低效率和较差的扩展性。为解决这些问题，我们提出了HERO，一个用于构建层次化可遍历3D场景图的新框架，它通过将可操作障碍物建模为路径，捕捉它们的物理交互性、功能语义和场景的层次关系来重新定义可遍历性。结果显示，与基线相比，HERO在部分阻塞环境中将路径长度减少了35.1%，在完全阻塞环境中将成功率提高了79.4%，显示出更高的效率和可达性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D场景图在导航任务中的局限性，即它们基于'静态世界假设'，将可交互的障碍物(如门、窗帘、可移动家具)视为不可通行区域。这个问题在现实中很重要，因为传统方法会导致机器人导航能力受限，表现为：有限的可达性(某些物理可达区域因障碍物被认为不可达)、低效率(过于保守的避障规划导致不必要的绕路)和差的可扩展性(无法完成需要与物体交互的复杂任务)。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类导航行为获得灵感：人类不会将所有阻挡物视为绝对障碍，而是评估物体属性和潜在可用性。作者借鉴了现有工作：基于H-3DSGs的层次结构(如HOV-SG)、Voronoi导航图、语义分割技术(SAM)、可视性净化策略和拓扑聚类策略。创新点在于将可移动障碍物整合到导航图中，并基于效率驱动的角度重新定义可通行性，而不仅仅是基于物理属性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建'可通行的分层3D场景图'，它不仅捕捉场景结构和语义，还明确建模物体的交互性和可移动性，将可移动障碍物视为潜在通路而非绝对障碍。实现流程分三阶段：1)粗粒度场景图构建(几何分解为楼层-房间结构，应用可见性净化策略)；2)细粒度场景图构建(拓扑聚类策略聚合物体节点，恢复语义信息)；3)可通行拓扑图构建(可通行性更新策略，识别可移动物体并整合到导航图中)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)可通行的分层3D场景图表示，明确建模物体交互属性；2)可见性净化策略解决跨房间语义污染；3)拓扑聚类策略利用3D拓扑完整性聚合物体；4)可通行性更新策略基于效率评估确定物体可移动性。相比之前工作：传统方法将障碍物视为静态不可通行，而HERO将其视为潜在通路；传统方法产生保守的避障路径，而HERO重新定义可通行区域提高效率；传统场景图缺乏交互性建模，而HERO的表示支持与物体交互的导航决策。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HERO通过构建可通行的分层3D场景图，将可移动障碍物整合为潜在通路而非绝对障碍，显著提高了机器人在复杂环境中的导航可达性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.</description>
      <author>example@mail.com (Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu)</author>
      <guid isPermaLink="false">2512.15047v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation</title>
      <link>http://arxiv.org/abs/2512.15033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出几何稳定性框架评估大型语言模型在复杂推理领域的真实理解能力，发现高准确率模型在几何变换下表现极差，表明其可能依赖模式匹配而非抽象空间逻辑。&lt;h4&gt;背景&lt;/h4&gt;在复杂推理领域评估大型语言模型通常依赖于与真实标准的一致性表现。在国际象棋领域，这表现为与Stockfish等强大引擎的准确率基准测试。然而，高标量准确率并不一定意味着稳健的概念理解。&lt;h4&gt;目的&lt;/h4&gt;解决标准准确率指标无法区分真实几何推理和规范棋盘状态的表面记忆这一问题，提出一种新的评估方法来更全面地评估模型的推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出几何稳定性框架，一种新颖的评估方法，严格测试模型在不变变换下的一致性，包括棋盘旋转、镜像对称、颜色反转和格式转换。使用约3000个位置的数据集对六种最先进的LLM（包括GPT-5.1、Claude Sonnet 4.5和Kimi K2 Turbo）进行比较分析。&lt;h4&gt;主要发现&lt;/h4&gt;发现了显著的准确率-稳定性悖论。GPT-5.1等模型在标准位置上接近最佳准确率，但在几何扰动下表现出灾难性退化，特别是在旋转任务中错误率激增600%以上。相反，Claude Sonnet 4.5和Kimi K2 Turbo表现出优越的双重稳健性，在所有变换轴上都保持高度一致性。此外，Gemini 2.5 Flash在拒绝非法状态方面表现最佳（96.0%）。&lt;h4&gt;结论&lt;/h4&gt;几何稳定性为AI评估提供了正交且必要的指标，为分离大规模模型的推理能力、数据污染和过拟合提供了必要的代理。&lt;h4&gt;翻译&lt;/h4&gt;在复杂推理领域评估大型语言模型通常依赖于与真实标准的一致性表现。在国际象棋领域，这一标准表现为与Stockfish等强大引擎的准确率基准测试。然而，高标量准确率并不一定意味着稳健的概念理解。本文认为标准准确率指标无法区分真实的几何推理和规范棋盘状态的表面记忆。为解决这一差距，我们提出了几何稳定性框架，一种新颖的评估方法，严格测试模型在不变变换下的一致性，包括棋盘旋转、镜像对称、颜色反转和格式转换。我们将此框架应用于六种最先进的大型语言模型的比较分析，包括GPT-5.1、Claude Sonnet 4.5和Kimi K2 Turbo，使用了约3000个位置的数据集。我们的结果揭示了显著的准确率-稳定性悖论。虽然GPT-5.1等模型在标准位置上实现了接近最佳的准确率，但在几何扰动下表现出灾难性退化，特别是在旋转任务中错误率激增600%以上。这种差异表明了对模式匹配而非抽象空间逻辑的依赖。相反，Claude Sonnet 4.5和Kimi K2 Turbo表现出优越的双重稳健性，在所有变换轴上都保持高度一致性。此外，我们分析了有用性和安全性之间的权衡，确定Gemini 2.5 Flash在拒绝非法状态方面处于领先地位（96.0%）。我们得出结论，几何稳定性为AI评估提供了正交且必要的指标，为分离大规模模型的推理能力、数据污染和过拟合提供了必要的代理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.</description>
      <author>example@mail.com (Xidan Song, Weiqi Wang, Ruifeng Cao, Qingya Hu)</author>
      <guid isPermaLink="false">2512.15033v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>An updated efficient galaxy morphology classification model based on ConvNeXt encoding with UMAP dimensionality reduction</title>
      <link>http://arxiv.org/abs/2512.15137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队改进了原有的USmorph分类框架，通过结合预训练的ConvNeXt卷积神经网络和UMAP非线性流形学习，提高了星系形态分类的效率，使其适用于大规模巡天项目。&lt;h4&gt;背景&lt;/h4&gt;星系形态分类在天文学中具有重要意义，特别是在大规模巡天时代，需要高效自动化的分类方法。&lt;h4&gt;目的&lt;/h4&gt;开发增强的无监督机器学习模块，提高星系形态分类效率，以适应中国空间站望远镜(CSST)等大型巡天项目。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的ConvNeXt CNN进行分层特征提取（迁移学习），结合UMAP进行拓扑感知的降维；将算法识别的20个聚类合并为5个物理形态类型；应用于红移0.2&lt;z&lt;1.2的99,806个COSMOS星系的I波段图像。&lt;h4&gt;主要发现&lt;/h4&gt;聚类数量从50优化为20，实现显著计算节省；约51%的星系被成功分类；对大质量星系的形态参数测试表明分类结果与星系演化理论高度一致。&lt;h4&gt;结论&lt;/h4&gt;改进的算法显著提高了星系形态分类效率，适合大规模巡天项目如中国空间站望远镜(CSST)。&lt;h4&gt;翻译&lt;/h4&gt;我们展示了在之前的USmorph分类框架内增强的无监督机器学习模块，包含两个组成部分：通过使用迁移学习的预训练ConvNeXt卷积神经网络进行分层特征提取，以及使用均匀流形近似和投影进行非线性流形学习实现拓扑感知的降维。我们将升级后的UML应用于红移0.2&lt;z&lt;1.2且I波段星等小于25的99,806个COSMOS星系的I波段图像。预定义的聚类数量优化为20（从原始框架的50减少），实现了显著的计算节省。20个算法识别的聚类被合并为五个物理形态类型。约51%的星系被成功分类。我们的分类结果与星系演化理论高度一致。这种改进的算法显著提高了星系形态分类的效率，适合中国空间站望远镜等计划进行的大规模巡天项目。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an enhanced unsupervised machine learning (UML) module within our previous \texttt{USmorph} classification framework featuring two components: (1) hierarchical feature extraction via a pre-trained ConvNeXt convolutional neural network (CNN) with transfer learning, and (2) nonlinear manifold learning using Uniform Manifold Approximation and Projection (UMAP) for topology-aware dimensionality reduction. This dual-stage design enables efficient knowledge transfer from large-scale visual datasets while preserving morphological pattern geometry through UMAP's neighborhood preservation. We apply the upgraded UML on I-band images of 99,806 COSMOS galaxies at redshift $0.2&lt;z&lt;1.2$ (to ensure rest-frame optical morphology) with $I_{\mathrm{mag}}&lt;25$. The predefined cluster number is optimized to 20 (reduced from 50 in the original framework), achieving significant computational savings. The 20 algorithmically identified clusters are merged into five physical morphology types. About 51\% of galaxies (50,056) were successfully classified. To assess classification effectiveness, we tested morphological parameters for massive galaxies with $M_{*}&gt;10^{9}~M_{\odot}$. Our classification results align well with galaxy evolution theory. This improved algorithm significantly enhances galaxy morphology classification efficiency, making it suitable for large-scale sky surveys such as those planned with the China Space Station Telescope (CSST).</description>
      <author>example@mail.com (Guanwen Fang, Shiwei Zhu, Jun Xu, Shiying Lu, Chichun Zhou, Yao Dai, Zesen Lin, Xu Kong)</author>
      <guid isPermaLink="false">2512.15137v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball</title>
      <link>http://arxiv.org/abs/2512.15386v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文引入了篮球广播视频中的动作预测任务，专注于预测投篮后哪个队伍将获得球权。研究团队构建了一个包含10万个篮球视频片段、超过300小时视频和2000多个手动标注的篮板事件的新数据集，并使用最先进的动作预测方法提供了全面的基线结果。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉和视频理解已经通过从广播录像中实现大规模、自动化的比赛动态分析，彻底改变了体育分析领域。尽管在球员和球跟踪、姿态估计、动作定位和自动犯规识别方面取得了显著进展，但在体育视频中预测动作发生之前的研究相对较少。&lt;h4&gt;目的&lt;/h4&gt;引入篮球广播视频中的动作预测任务，专注于预测投篮后哪个队伍将获得球权，并建立这一任务的基准数据集，探索篮板分类和篮板检测等互补任务。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含10万个篮球视频片段、超过300小时视频和2000多个手动标注的篮板事件的新数据集，并使用最先进的动作预测方法提供了全面的基线结果，代表了深度学习技术首次应用于篮球篮板预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果突显了预测篮板的可行性和内在挑战，为动态多代理体育场景的预测建模提供了有价值的见解。通过预测篮板发生前的队伍球权，使实时自动广播和赛后分析工具支持决策的应用成为可能。&lt;h4&gt;结论&lt;/h4&gt;这项工作通过预测篮板发生前的队伍球权，为实时自动广播和赛后分析工具支持决策的应用奠定了基础，为动态多代理体育场景的预测建模提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉和视频理解已经通过从广播录像中实现大规模、自动化的比赛动态分析，彻底改变了体育分析领域。尽管在球员和球跟踪、姿态估计、动作定位和自动犯规识别方面取得了显著进展，但在体育视频中预测动作发生之前的研究相对较少。这项工作引入了篮球广播视频中的动作预测任务，专注于预测投篮后哪个队伍将获得球权。为了建立这一任务的基准，展示了一个新的自建数据集，包含10万个篮球视频片段、超过300小时视频和2000多个手动标注的篮板事件。研究团队使用最先进的动作预测方法提供了全面的基线结果，代表了深度学习技术首次应用于篮球篮板预测。此外，还探索了两个互补任务：篮板分类和篮板检测，展示该数据集支持广泛的篮球视频理解应用，目前没有类似的数据集存在。实验结果突显了预测篮板的可行性和内在挑战，为动态多代理体育场景的预测建模提供了有价值的见解。通过预测篮板发生前的队伍球权，这项工作使实时自动广播和赛后分析工具支持决策的应用成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage. Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention. This work introduces the task of action anticipation in basketball broadcast videos, focusing on predicting which team will gain possession of the ball following a shot attempt. To benchmark this task, a new self-curated dataset comprising 100,000 basketball video clips, over 300 hours of footage, and more than 2,000 manually annotated rebound events is presented. Comprehensive baseline results are reported using state-of-the-art action anticipation methods, representing the first application of deep learning techniques to basketball rebound prediction. Additionally, two complementary tasks, rebound classification and rebound spotting, are explored, demonstrating that this dataset supports a wide range of video understanding applications in basketball, for which no comparable datasets currently exist. Experimental results highlight both the feasibility and inherent challenges of anticipating rebounds, providing valuable insights into predictive modeling for dynamic multi-agent sports scenarios. By forecasting team possession before rebounds occur, this work enables applications in real-time automated broadcasting and post-game analysis tools to support decision-making.</description>
      <author>example@mail.com (Arnau Barrera Roy, Albert Clapés Sintes)</author>
      <guid isPermaLink="false">2512.15386v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning</title>
      <link>http://arxiv.org/abs/2512.15153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的人体动作形式评估（AFA）任务和相关数据集CoT-AFA，以及一个名为可解释性健身评估器的框架，用于评估人类动作的标准化程度并提供解释和解决方案。&lt;h4&gt;背景&lt;/h4&gt;评估人类行为标准化并提供反馈在现实场景中重要但具挑战性，当前视频理解方法主要关注行为是什么和在哪里，无法满足需求；现有数据集缺乏表示行为标准化程度的标签，行为质量评估数据集缺乏可解释性和详细反馈。&lt;h4&gt;目的&lt;/h4&gt;定义新的AFA任务，创建包含健身和武术视频的多样化数据集CoT-AFA，并提出可解释性健身评估器框架，用于判断动作、解释原因并提供解决方案。&lt;h4&gt;方法&lt;/h4&gt;创建包含多级注释的CoT-AFA数据集，采用Chain-of-Thought解释范式提供完整推理过程；提出可解释性健身评估器框架，该框架采用两个并行处理流和动态门控机制融合视觉和语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在解释生成（CIDEr提高+16.0%）、动作分类（准确率提高+2.7%）和质量评估（准确率提高+2.1%）方面取得了改进，展示了CoT-AFA对未来研究的巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;CoT-AFA数据集和可解释性健身评估器框架为动作标准化评估提供了有效解决方案，数据集和源代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;评估人类行为是否标准并提供合理反馈以改进行为标准化在现实场景中非常重要但具有挑战性。然而，当前视频理解方法主要关注行为是什么和在哪里，无法满足需求。同时，大多数现有数据集缺乏表示行为标准化程度的标签，而行为质量评估数据集缺乏可解释性和详细反馈。因此，我们定义了一个新的人体动作形式评估（AFA）任务，并引入了一个新的多样化数据集CoT-AFA，其中包含大量健身和武术视频，具有多级注释用于全面视频分析。我们通过新颖的思维链解释范式丰富了CoT-AFA数据集。与提供孤立反馈不同，我们的解释提供了完整的推理过程——从识别动作步骤到分析结果并提出具体解决方案。此外，我们提出了一个名为可解释性健身评估器的框架，不仅可以判断动作，还可以解释原因并提供解决方案。该框架采用两个并行处理流和动态门控机制来融合视觉和语义信息，从而提高其分析能力。实验结果表明，我们的方法在解释生成（例如，CIDEr提高+16.0%）、动作分类（准确率提高+2.7%）和质量评估（准确率提高+2.1%）方面取得了改进，揭示了CoT-AFA对未来研究的巨大潜力。我们的数据集和源代码可在https://github.com/MICLAB-BUPT/EFA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.</description>
      <author>example@mail.com (Mengshi Qi, Yeteng Wu, Xianlin Zhang, Huadong Ma)</author>
      <guid isPermaLink="false">2512.15153v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation</title>
      <link>http://arxiv.org/abs/2512.15116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication. 15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FADTI的基于扩散的框架，用于多变量时间序列插值，通过傅里叶偏置投影模块注入频率感知特征调制，结合自注意力和门控卷积进行时间建模，在多个基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列插值在医疗保健、交通预测和生物建模等领域至关重要，传感器故障和不规则采样导致数据普遍存在缺失值。&lt;h4&gt;目的&lt;/h4&gt;解决现有Transformer和扩散模型在处理结构化缺失模式和分布变化时泛化能力不足的问题，提高高缺失率情况下的插值性能。&lt;h4&gt;方法&lt;/h4&gt;提出FADTI框架，通过可学习的傅里叶偏置投影（FBP）模块注入频率感知特征调制，结合自注意力和门控卷积进行时间建模，FBP支持多种频谱基以编码平稳和非平稳模式。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试（包括新引入的生物时间序列数据集）上，FADTI始终优于最先进的方法，特别是在高缺失率情况下表现优异。&lt;h4&gt;结论&lt;/h4&gt;通过将频域归纳偏置注入生成插值过程，FADTI有效提升了多变量时间序列插值的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列插值在医疗保健、交通预测和生物建模等应用中是基础性的，其中传感器故障和不规则采样导致普遍的缺失值。然而，现有的基于Transformer和扩散的模型缺乏明确的归纳偏置和频率感知能力，限制了它们在结构化缺失模式和分布变化情况下的泛化能力。我们提出了FADTI，一个基于扩散的框架，它通过可学习的傅里叶偏置投影（FBP）模块注入频率感知的特征调制，并通过自注意力和门控卷积与时间建模相结合。FBP支持多种频谱基，能够自适应编码平稳和非平稳模式。这种设计将频域归纳偏置注入到生成插值过程中。在多个基准测试（包括新引入的生物时间序列数据集）上的实验表明，FADTI始终优于最先进的方法，特别是在高缺失率情况下。代码可在https://anonymous.4open.science/r/TimeSeriesImputation-52BF获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF</description>
      <author>example@mail.com (Runze Li, Hanchen Wang, Wenjie Zhang, Binghao Li, Yu Zhang, Xuemin Lin, Ying Zhang)</author>
      <guid isPermaLink="false">2512.15116v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>LADY: Linear Attention for Autonomous Driving Efficiency without Transformers</title>
      <link>http://arxiv.org/abs/2512.15038v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LADY，首个基于完全线性注意力的生成模型，用于端到端自动驾驶，解决了Transformer架构的二次方注意力成本问题，实现了高效的跨模态和跨时序交互。&lt;h4&gt;背景&lt;/h4&gt;端到端范式在自动驾驶中表现出巨大潜力，但现有方法大多基于Transformer架构，其二次方注意力成本限制了在资源受限边缘平台上的应用，特别是对长时空序列建模能力的影响。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效融合长期时序上下文并支持跨模态交互的自动驾驶模型，解决Transformer架构的计算效率问题。&lt;h4&gt;方法&lt;/h4&gt;提出LADY模型，采用完全线性注意力机制，实现恒定计算和内存成本的长期时序上下文融合，并引入轻量级线性交叉注意力机制促进跨模态信息交换。&lt;h4&gt;主要发现&lt;/h4&gt;在NAVSIM和Bench2Drive基准测试中，LADY实现了最先进的性能，具有恒定的时间和内存复杂度，提供改进的规划性能和显著降低的计算成本，已在边缘设备上成功部署验证。&lt;h4&gt;结论&lt;/h4&gt;LADY通过线性注意力机制有效解决了Transformer的计算效率瓶颈，实现了自动驾驶所需的跨模态和跨时序交互，在资源受限的边缘平台上表现出色，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;端到端范式已展现出自动驾驶的巨大潜力。此外，大多数现有方法都基于Transformer架构构建。然而，Transformer会产生二次方注意力成本，限制了其在资源受限的边缘平台上对长时空序列建模的能力。由于自动驾驶本质上需要高效的时序建模，这一挑战严重限制了它们的部署和实时性能。最近，线性注意力机制因其优越的时空复杂度而受到越来越多的关注。然而，现有的线性注意力架构仅限于自注意力，缺乏对跨模态和跨时序交互的支持，而这两种交互对自动驾驶都至关重要。在这项工作中，我们提出了LADY，这是首个基于完全线性注意力的生成模型，用于端到端自动驾驶。LADY能够在推理时融合长期时序上下文，具有恒定的计算和内存成本，无论相机和LiDAR特征的历史长度如何。此外，我们引入了一种轻量级线性交叉注意力机制，实现了有效的跨模态信息交换。在NAVSIM和Bench2Drive基准测试上的实验表明，LADY实现了最先进的性能，具有恒定的时间和内存复杂度，提供了改进的规划性能和显著降低的计算成本。此外，该模型已在边缘设备上部署和验证，展示了其在资源受限场景中的实用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中基于Transformer的端到端模型计算效率低下的问题，特别是Transformer的二次方注意力复杂度限制了其在资源受限的边缘平台上的实时性能。这个问题在现实中非常重要，因为自动驾驶系统需要实时处理多帧传感器数据并做出安全决策，而现有方法难以在保持高性能的同时满足实时性和边缘设备部署的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有自动驾驶方法的局限性，特别是Transformer的计算瓶颈。他们借鉴了语言模型中的线性注意力机制(如RWKV-7)，将其应用于自动驾驶领域。同时参考了扩散模型(DiffusionDrive)用于生成多模态轨迹，以及iPad的评分机制来选择最佳轨迹。作者设计了一个完全基于线性注意力的架构，包括创新的线性交叉注意力(LICA)机制，以实现高效的多模态信息交换和长时序上下文融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用线性注意力替代Transformer的二次方注意力机制，实现计算复杂度的线性化，同时保持模型性能。整体流程包括：1)多帧摄像头和LiDAR特征通过RWKV-7进行融合；2)使用线性交叉注意力(LICA)进行跨模态信息交换；3)通过扩散模型生成多模态轨迹；4)使用评分机制选择最佳轨迹。训练时多帧特征并行处理，推理时当前帧特征与时间隐藏状态顺序融合，支持长时序上下文。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个完全线性注意力端到端自动驾驶模型；2)轻量级线性交叉注意力(LICA)机制；3)支持长时序上下文融合且计算开销恒定；4)多模态轨迹生成与评分。相比之前工作，LADY完全避免了Transformer的二次方复杂度，实现了真正的线性计算复杂度；能够融合无限长度的历史信息而不增加计算开销；支持跨模态交互的线性注意力机制；在保持或提高性能的同时显著降低了计算和内存需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LADY首次将完全线性注意力机制引入自动驾驶领域，通过创新的线性交叉注意力和长时序上下文融合方法，在保持高性能的同时显著降低了计算复杂度，实现了资源受限边缘平台上的高效端到端自动驾驶。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.</description>
      <author>example@mail.com (Jihao Huang, Xi Xia, Zhiyuan Li, Tianle Liu, Jingke Wang, Junbo Chen, Tengju Ye)</author>
      <guid isPermaLink="false">2512.15038v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</title>
      <link>http://arxiv.org/abs/2512.14870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了HERBench，一个专门用于评估视频大语言模型跨时间整合多个视觉证据能力的新型视频问答基准测试。该基准包含26,000个五选一多项选择题，组织成12个组合任务，评估身份绑定、跨实体关系、时间顺序等能力。研究发现当前最先进的Video-LLMs在HERBench上表现不佳，准确率仅31-42%，主要存在检索缺陷和融合缺陷两个瓶颈。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型(Video-LLMs)正在快速发展，但当前的视频问答(VideoQA)基准测试通常允许问题从单一显著线索中得到回答，无法充分测试需要聚合多个时间分离的视觉证据的推理能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门的视频问答基准测试(HERBench)，用于评估模型跨时间整合多个证据的能力，确保问题必须聚合至少三个不重叠的证据线索，仅依靠语言先验或单一快照无法回答。&lt;h4&gt;方法&lt;/h4&gt;创建HERBench基准，包含26,000个五选一多项选择题，组织成12个组合任务；引入'最小必需帧集'(MRFS)概念来量化证据需求；评估13个最先进的Video-LLMs在HERBench上的表现；分析失败原因，分为检索缺陷和融合缺陷。&lt;h4&gt;主要发现&lt;/h4&gt;HERBench对证据的要求显著高于先前数据集(平均MRFS为5.5，而先前数据集为2.6-4.2)；13个最先进的Video-LLMs在HERBench上的准确率仅为31-42%，仅略高于20%的随机猜测基线；模型失败主要归因于检索缺陷和融合缺陷两个关键瓶颈。&lt;h4&gt;结论&lt;/h4&gt;通过使跨时间证据成为不可避免且可量化的内容，HERBench为推进稳健的组合视频理解建立了原则性目标。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(Video-LLMs)正在迅速发展，但当前的视频问答(VideoQA)基准测试通常允许问题从单一显著线索中得到回答，这无法充分测试需要聚合多个时间分离的视觉证据的推理能力。我们提出了HERBench，这是一个专门构建的视频问答基准，用于评估跨时间多证据整合。每个问题需要聚合至少三个来自不同视频段的不重叠证据线索，因此语言先验或单一快照都不足以回答。HERBench包含26,000个五选一多项选择题，组织成12个组合任务，探索身份绑定、跨实体关系、时间顺序、共现验证和计数能力。为了使证据需求可量化，我们引入了'最小必需帧集'(MRFS)，即模型必须融合的最小帧数才能正确回答，并表明HERBench的要求显著高于先前数据集(平均MRFS为5.5，而先前为2.6-4.2)。在HERBench上评估13个最先进的Video-LLMs显示出普遍失败：准确率仅为31-42%，仅略高于20%的随机猜测基线。我们将这种失败分解为两个关键瓶颈：(1)检索缺陷，帧选择器忽略关键证据；(2)融合缺陷，即使提供了所有必要证据，模型也无法整合信息。通过使跨时间证据成为不可避免且可量化的内容，HERBench为推进稳健的组合视频理解建立了原则性目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.</description>
      <author>example@mail.com (Dan Ben-Ami, Gabriele Serussi, Kobi Cohen, Chaim Baskin)</author>
      <guid isPermaLink="false">2512.14870v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</title>
      <link>http://arxiv.org/abs/2512.15581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026. 22 pages, 8 figures. Includes supplementary material&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IMKD的新型雷达-相机融合框架，基于多级知识蒸馏实现在不使用LiDAR情况下的高性能3D目标检测，该方法保留各传感器固有特性并增强互补优势。&lt;h4&gt;背景&lt;/h4&gt;现有知识蒸馏方法通常将特定模态特征直接传输到各传感器，这会扭曲传感器独特特性并降低各自优势，而高性能雷达-相机3D目标检测可通过知识蒸馏实现且推理时无需LiDAR。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中特征传输导致传感器特性扭曲的问题，提出IMKD框架以保留各传感器固有特性同时增强互补优势。&lt;h4&gt;方法&lt;/h4&gt;IMKD采用三阶段强度感知蒸馏策略：(1)LiDAR到雷达的强度感知特征蒸馏，用细粒度结构线索增强雷达表示；(2)LiDAR到融合的强度引导特征蒸馏，选择性地突出几何和深度信息，促进模态互补性；(3)相机-雷达强度引导融合机制，促进有效特征对齐和校准。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes基准上，IMKD达到67.0%的NDS和61.0%的mAP，性能优于所有先前基于蒸馏的雷达-相机融合方法。&lt;h4&gt;结论&lt;/h4&gt;IMKD框架成功解决了现有知识蒸馏方法的问题，通过多级知识蒸馏保留传感器特性并增强互补优势，实现了高性能3D目标检测。&lt;h4&gt;翻译&lt;/h4&gt;高性能的雷达-相机3D目标检测可以通过利用知识蒸馏在推理时不使用LiDAR来实现。然而，现有的蒸馏方法通常将特定模态的特征直接传输到每个传感器，这可能会扭曲它们的独特特性并降低它们各自的优势。为了解决这个问题，我们引入了IMKD，这是一种基于多级知识蒸馏的雷达-相机融合框架，在保留每个传感器固有特性的同时增强它们的互补优势。IMKD应用三阶段、强度感知的蒸馏策略来丰富整个架构中的融合表示：(1)LiDAR到雷达的强度感知特征蒸馏，用细粒度结构线索增强雷达表示；(2)LiDAR到融合的强度引导特征蒸馏，在融合级别选择性地突出有用的几何和深度信息，促进模态间的互补性而非强制对齐；(3)相机-雷达强度引导融合机制，促进有效的特征对齐和校准。在nuScenes基准上的大量实验表明，IMKD达到了67.0%的NDS和61.0%的mAP，优于所有先前基于蒸馏的雷达-相机融合方法。我们的代码和模型可在https://github.com/dfki-av/IMKD/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有知识蒸馏方法在雷达-摄像头融合3D目标检测中的局限性：这些方法通常直接将模态特定特征传输到每个传感器，扭曲了它们的独特特性并降低了各自优势。这个问题在现实中很重要，因为它关系到如何构建低成本但高效的自动驾驶感知系统——LiDAR虽然精确但昂贵且范围有限，而摄像头和雷达的组合可以提供更经济的替代方案，但需要有效融合两种传感器的优势。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法独立将知识蒸馏到每个模态的局限性，忽略了传感器独特特性。他们设计了一个三阶段强度感知蒸馏策略，包括LiDAR到雷达的特征蒸馏、LiDAR到融合特征的蒸馏以及摄像头-雷达融合机制。作者借鉴了LabelDistill的标签引导知识蒸馏、BEVDepth的深度估计和BEVFormer的时序融合等现有工作，但创新性地引入了强度感知机制，利用LiDAR强度作为可靠性先验，突出几何一致区域，同时保留雷达的鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用强度感知的多级知识蒸馏增强摄像头-雷达融合，保留每个传感器的内在特性同时放大互补优势，使用LiDAR强度作为可靠性先验指导知识传递。整体流程包括：1)摄像头特征提取与视图转换；2)雷达特征提取与网格化；3)强度感知特征融合；4)自适应强度引导雷达特征增强；5)LiDAR引导特征增强；6)基于标签的知识蒸馏；7)多损失联合优化。训练时使用LiDAR和标签作为特权信息，推理时仅使用摄像头和雷达，确保高效部署。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)强度感知的多级知识蒸馏框架；2)三阶段强度感知蒸馏策略；3)在联合融合特征空间而非单个模态上执行知识蒸馏；4)强度感知的雷达-摄像头融合模块；5)引入结构化监督减少对LiDAR的依赖。相比之前工作，IMKD避免了直接强制雷达或摄像头模仿LiDAR表示，保留了传感器独特特性；引入了强度感知机制和自适应权重指导监督，而非使用均匀或二元掩码；在融合级别而非模态级别执行蒸馏，更好地利用了跨模态交互信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IMKD通过引入强度感知的多级知识蒸馏框架，有效解决了雷达-摄像头融合3D目标检测中传感器特性被扭曲的问题，在保持推理效率的同时显著提升了检测性能，为低成本自动驾驶感知系统提供了新思路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.</description>
      <author>example@mail.com (Shashank Mishra, Karan Patil, Didier Stricker, Jason Rambach)</author>
      <guid isPermaLink="false">2512.15581v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</title>
      <link>http://arxiv.org/abs/2512.15000v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DreamPRM-Code是一种专注于编程的进程奖励模型(PRM)，通过函数链提示策略和元学习校正机制解决了传统PRM在编程应用中的局限性，在LiveCodeBench上取得了80.9%的pass@1率，超越了OpenAI o4-mini。&lt;h4&gt;背景&lt;/h4&gt;进程奖励模型(PRM)已成为通过测试时扩展提高大型语言模型的重要工具，但在编程领域的应用效果有限，主要原因是代码中缺乏有意义的步骤分解以及蒙特卡洛生成的部分标签存在噪声。&lt;h4&gt;目的&lt;/h4&gt;提出一个专注于编程的PRM(DreamPRM-Code)，以解决传统PRM在编程应用中的局限性，提高其在代码生成任务中的效果。&lt;h4&gt;方法&lt;/h4&gt;DreamPRM-Code将函数视为推理步骤，采用函数链提示策略(Chain-of-Function prompting)来诱导模块化代码生成；同时引入基于元学习的校正机制，利用干净的最终解决方案单元测试标签，并通过双层优化来改进中间标签。&lt;h4&gt;主要发现&lt;/h4&gt;在测试时扩展应用中，DreamPRM-Code在LiveCodeBench上取得了最先进的性能，pass@1率达到80.9%，超过了OpenAI o4-mini。&lt;h4&gt;结论&lt;/h4&gt;DreamPRM-Code通过创新的方法解决了PRM在编程领域应用中的关键问题，实现了显著的性能提升，为编程任务中的进程奖励模型应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;进程奖励模型(PRM)已成为通过测试时扩展提高大型语言模型的重要工具，但在编程领域的应用效果有限，原因是代码中缺乏有意义的步骤分解以及蒙特卡洛生成的部分标签存在噪声。我们提出了DreamPRM-Code，这是一种专注于编程的PRM，它使用函数链提示策略将函数视为推理步骤，以诱导模块化代码生成，使PRM的训练和应用类似于数学推理任务。为解决标签噪声问题，DreamPRM-Code引入了一种基于元学习的校正机制，利用干净的最终解决方案单元测试标签，并进行双层优化来改进中间标签。在测试时扩展应用中，DreamPRM-Code在LiveCodeBench上取得了80.9%的pass@1率的最先进性能，超越了OpenAI o4-mini。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.</description>
      <author>example@mail.com (Ruiyi Zhang, Peijia Qin, Qi Cao, Pengtao Xie)</author>
      <guid isPermaLink="false">2512.15000v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</title>
      <link>http://arxiv.org/abs/2512.13857v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了EvoLattice框架，用于程序和多智能体系统的演化，通过在有向无环图中维护多个候选方案，解决了传统方法中丢弃有用变体和破坏性编辑的问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型越来越多地用于程序和多智能体系统的演化，但大多数现有方法基于覆盖式突变，一次只维护一个候选方案，导致有用变体被丢弃、破坏性编辑和脆弱的搜索空间。&lt;h4&gt;目的&lt;/h4&gt;引入EvoLattice框架，提供一种能够在单一结构中维护整个候选种群的方法，同时保证结构正确性和提供更密集的反馈信号。&lt;h4&gt;方法&lt;/h4&gt;EvoLattice在一个有向无环图中表示整个候选程序或智能体行为的种群，每个节点存储多个持久性替代方案，每条有效路径定义一个不同的可执行候选，产生大的组合搜索空间而不重复结构。&lt;h4&gt;主要发现&lt;/h4&gt;EvoLattice实现了细粒度的替代级别评估，统计数据显示局部设计选择如何影响全局性能，为LLM引导的演化提供密集、数据驱动的反馈，同时通过自修复机制保证结构正确性，并能自然扩展到智能体演化。&lt;h4&gt;结论&lt;/h4&gt;在程序合成方面，EvoLattice比之前的LLM引导方法产生更稳定的演化、更大的表达能力和更强的改进轨迹，其动力学类似于质量多样性优化，是从内部多替代表示中隐式出现的。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)越来越多地用于程序和多智能体系统的演化，但大多数现有方法依赖于基于覆盖的突变，一次只维护一个候选方案。这类方法丢弃有用的变体，容易受到破坏性编辑的影响，并探索一个容易发生结构性失败的脆弱搜索空间。我们引入了EvoLattice，一个框架，它在一个有向无环图中表示整个候选程序或智能体行为的种群。每个节点存储多个持久性替代方案，通过图中的每条有效路径定义一个不同的可执行候选，产生大的组合搜索空间而不重复结构。EvoLattice通过对每条路径中出现的每个替代方案进行评分，实现细粒度的替代级别评估，产生统计数据显示局部设计选择如何影响全局性能。这些统计为LLM引导的突变、重组和修剪提供了密集、数据驱动的反馈信号，同时保留成功的组件。结构正确性通过确定性自修复机制保证，该机制独立于LLM强制执行无环性和依赖一致性。EvoLattice通过将替代方案解释为提示片段或子智能体行为，自然地扩展到智能体演化。在程序合成(代理和优化器元学习)方面，EvoLattice比之前的LLM引导方法产生更稳定的演化、更大的表达能力和更强的改进轨迹。由此产生的动力学类似于质量多样性优化，是从EvoLattice的内部多替代表示中隐式出现的，而不是来自显式的外部档案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.</description>
      <author>example@mail.com (Kamer Ali Yuksel)</author>
      <guid isPermaLink="false">2512.13857v2</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Foundation Models</title>
      <link>http://arxiv.org/abs/2512.15708v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将基础模型转换为多视图基础模型的方法，通过添加3D感知注意力层增强Transformer模型，实现多视图图像中对应点特征的一致性匹配。&lt;h4&gt;背景&lt;/h4&gt;基础模型在计算机视觉应用中至关重要，它们输入单张RGB图像并输出有用的深度特征表示。然而，当处理同一3D场景的多个视图时，这些模型独立处理每张图像，无法保证同一3D点特征的一致性。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法将基础模型转换为多视图基础模型，输入一组图像，为每张图像输出特征图，使对应点的特征尽可能保持一致。&lt;h4&gt;方法&lt;/h4&gt;通过增强基于Transformer的基础模型（如DINO、SAM、CLIP）添加中间3D感知注意力层，帮助匹配不同视图的特征。这种方法绕过了构建一致的3D特征模型的必要性，允许在图像空间直接操作。&lt;h4&gt;主要发现&lt;/h4&gt;定量实验表明，与当前基础模型相比，该方法显著改善了特征匹配性能，在表面法线估计和多视图分割等任务中表现优异。&lt;h4&gt;结论&lt;/h4&gt;所提出的多视图基础模型能够有效处理多视图图像中的特征一致性问题，为计算机视觉应用提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;基础模型是各种计算机视觉应用中的重要工具。它们输入单张RGB图像并输出适用于各种应用的深度特征表示。然而，当我们有同一3D场景的多个视图时，它们独立处理每张图像，并不总是为同一3D点生成一致的特征。我们提出了一种将基础模型转换为多视图基础模型的方法。这种模型输入一组图像，为每张图像输出特征图，使得对应点的特征尽可能一致。这种方法绕过了构建一致的3D特征模型的必要性，允许在图像空间直接操作。具体来说，我们展示了如何通过添加中间3D感知注意力层来增强基于Transformer的基础模型（即DINO、SAM、CLIP），帮助匹配不同视图的特征。作为主要示例，我们展示了表面法线估计和多视图分割任务。定量实验表明，与当前基础模型相比，我们的方法显著改善了特征匹配。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决2D基础模型（如DINO、SAM、CLIP）在处理同一3D场景的多个视图时无法为相同3D点产生一致特征的问题。这个问题很重要，因为许多3D视觉任务（如3D重建、增强现实、多视图几何）需要一致的特征表示，而现有方法要么需要昂贵的场景级优化，要么缺乏全局3D一致性，限制了基础模型在3D应用中的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到2D基础模型缺乏3D意识，并认识到将2D扩展到3D的挑战。他们借鉴了'lifting'原则（从2D扩展到3D）、FiT3D工作（但发现其缺乏多视图一致性）、Kim等人的时间注意力层工作（扩展到3D几何）以及LoRA微调技术。作者设计了一种在预训练模型中插入多视图适配器的方案，通过参数高效训练保留2D模型的语义能力，同时添加3D意识。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是为预训练2D模型添加多视图适配器，使其能够跨不同视图进行特征匹配，同时明确融入相机姿态信息确保几何一致性。整体流程：1)在Transformer骨干网络每个块间插入空间适配器；2)用Plücker嵌入表示相机姿态构建射线图；3)将姿态编码与特征连接；4)使用几何感知密集损失训练；5)添加正则化防止特征偏离原始空间；6)推理时输入多视图图像输出一致特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新：1)提出可扩展多视图适配框架，实现单次前向传播产生跨视图几何一致特征；2)设计相机条件的多视图适配器和高效训练策略；3)纯推理时操作，避免昂贵的3D特征表示和场景级优化。相比之前工作：与需要场景级优化的方法相比计算效率更高；与FiT3D相比使用多视图创建统一3D表示；与Lift3D相比不需要渲染新视图；适用于多种基础模型；在保持语义一致性的同时显著提高几何一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过多视图适配器将2D基础模型转换为具有3D意识的多视图一致模型的方法，在保持原始模型语义能力的同时显著提高了跨视图的几何一致性，且无需场景级优化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.</description>
      <author>example@mail.com (Leo Segre, Or Hirschorn, Shai Avidan)</author>
      <guid isPermaLink="false">2512.15708v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors</title>
      <link>http://arxiv.org/abs/2512.15577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了MoonSeg3R方法，实现在线零样本单目3D实例分割，利用CUT3R模型和三个关键组件，达到与基于RGB-D系统相媲美的性能。&lt;h4&gt;背景&lt;/h4&gt;现有在线零样本单目3D实例分割方法表现不佳，因为它们依赖于已配准的RGB-D序列，而实际应用中往往只有单目RGB输入。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，开发一种能够仅从单目RGB流中进行在线3D实例分割的方法。&lt;h4&gt;方法&lt;/h4&gt;利用CUT3R重构基础模型提供几何先验，并提出MoonSeg3R框架，包含三个关键组件：(1)带有空间-语义蒸馏的自监督查询细化模块；(2)3D查询索引内存；(3)状态分布令牌作为掩码身份描述符。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet200和SceneNN数据集上，MoonSeg3R是首个实现在线单目3D分割的方法，性能与最先进的基于RGB-D系统相当。&lt;h4&gt;结论&lt;/h4&gt;MoonSeg3R成功解决了在线零样本单目3D实例分割的挑战，证明了仅使用单目RGB输入也能达到高性能3D分割。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们专注于在线零样本单目3D实例分割，这是一个新颖实用的设置，因为现有方法依赖于已配准的RGB-D序列而无法在此设置下表现。为了克服这一限制，我们利用了CUT3R，一种最近的重构基础模型，从单个RGB流中提供可靠的几何先验。我们提出了MoonSeg3R，它引入了三个关键组件：(1)带有空间-语义蒸馏的自监督查询细化模块，将2D视觉基础模型的分割掩码转换为判别性3D查询；(2)3D查询索引内存，通过检索上下文查询提供时间一致性；(3)来自CUT3R的状态分布令牌，作为掩码身份描述符，增强跨帧融合。在ScanNet200和SceneNN上的实验表明，MoonSeg3R是首个实现在线单目3D分割的方法，并实现了与最先进的基于RGB-D系统相竞争的性能。代码和模型将发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目在线零样本3D实例分割问题，即仅从单目RGB图像流（无需深度信息）实时进行3D场景重建和实例分割。这个问题在现实中非常重要，因为许多应用场景（如机器人导航、自动驾驶）中深度传感器可能不可用或不实用，现有方法依赖于RGB-D输入或需要3D真实值监督，限制了在没有专用深度传感器的平台上的部署。在复杂环境中，能够仅从单目摄像头进行实时3D感知对具身智能和自主操作至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者的设计思路是结合两种基础模型的优势：利用重建基础模型（RFM，特别是CUT3R）提供可靠的几何先验，以及利用视觉基础模型（VFM，如CropFormer）提供强大的2D掩码先验。作者首先识别现有方法依赖RGB-D输入或需要3D真实值监督的局限性，发现CUT3R等RFM可从单目RGB流进行实时3D重建，但优化目标是重建而非分割，缺乏语义意识、几何预测有噪声且记忆难以解释。因此，作者设计了将RFM几何先验与VFM语义能力相结合的方法。该方法借鉴了CUT3R的重建能力、CropFormer的分割能力，以及查询机制和记忆机制设计了查询索引记忆（QIM）。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将2D视觉基础模型生成的分割掩码提升到3D，通过自监督查询精炼和跨帧关联实现一致的3D分割，同时利用重建基础模型的几何先验替代显式深度监督。整体流程包括：1)输入处理：RFM预测几何信息，VFM生成分割掩码；2)3D原型表示：将2D掩码提升为3D原型查询；3)查询精炼：通过交叉注意力精炼查询并融入上下文信息；4)空间-语义蒸馏：自监督训练查询参数；5)查询索引记忆：维护全局查询库和索引图实现跨帧关联；6)在线掩码融合：帧内合并和跨帧匹配实现最终分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个单目在线零样本3D分割框架，直接从单目RGB流进行在线3D实例分割；2)自监督查询精炼和蒸馏策略，强制实例级判别性和几何一致性；3)3D查询索引记忆(QIM)，通过空间键和上下文查询检索实现跨帧关联；4)基于注意力的在线掩码融合策略，利用状态分布token增强跨帧融合。相比之前工作，本文方法仅需单目RGB输入（无需深度信息），完全无监督（无需3D几何或掩码监督），实现实时在线处理，且在ScanNet200和SceneNN上实现了与最先进RGB-D系统相当的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MoonSeg3R首次实现了仅从单目RGB流进行在线零样本3D实例分割，通过结合重建基础模型的几何先验和视觉基础模型的语义能力，无需深度或掩码监督即可实现实时3D场景重建和分割。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.</description>
      <author>example@mail.com (Zhipeng Du, Duolikun Danier, Jan Eric Lenssen, Hakan Bilen)</author>
      <guid isPermaLink="false">2512.15577v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation</title>
      <link>http://arxiv.org/abs/2512.15564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了SAM3框架在遥感图像分割中的应用，比较了不同提示策略和监督规模下的性能表现。&lt;h4&gt;背景&lt;/h4&gt;遥感图像分割受限于标注数据的有限性，以及高空图像与用于训练基础模型的自然图像之间的差距，这促使在有限监督下进行有效适应。&lt;h4&gt;目的&lt;/h4&gt;评估SAM3框架用于遥感图像分割的效果，比较文本、几何和混合提示策略在不同监督规模下的性能，包括零样本推理和轻量级微调。&lt;h4&gt;方法&lt;/h4&gt;使用SAM3概念驱动框架，该框架可以从文本提示生成掩码而无需任务特定修改。研究在四种目标类型上评估了不同提示策略，并在不同规模的监督下进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;结合语义和几何提示在所有目标和指标上表现最佳；纯文本提示表现最差，特别是对不规则形状目标；文本提示配合轻量微调对几何规则和视觉显著目标提供了实用的性能-努力权衡；随着监督规模增加，性能提升但收益递减；精确度和IoU之间持续存在差距，表明欠分割和边界不准确仍是主要问题。&lt;h4&gt;结论&lt;/h4&gt;SAM3框架在有限监督下对遥感图像分割有效，适度的几何标注工作量足以实现有效适应，但欠分割和边界不准确仍需进一步解决。&lt;h4&gt;翻译&lt;/h4&gt;遥感图像分割受限于标注数据的有限性以及高空图像与用于训练基础模型的自然图像之间的差距。这促使在有限监督下进行有效适应。SAM3概念驱动框架可以从文本提示生成掩码而无需任务特定修改，这可能实现这种适应。我们在四种目标类型上评估了SAM3用于遥感图像的效果，比较了文本、几何和混合提示策略，在逐渐增加监督的轻量级微调规模下进行测试，同时包括零样本推理。结果表明，结合语义和几何提示在所有目标和指标上产生最高性能。纯文本提示表现最差，对于不规则形状的目标存在明显的分数差距，这反映了SAM3文本表示与其高空外观之间的语义对齐有限。然而，对于几何规则和视觉显著的目标，文本提示配合轻量微调提供了实用的性能-努力权衡。在所有目标中，性能在零样本推理和微调之间有所提升，随后随着监督规模增加而收益递减。也就是说，适度的几何标注工作量足以实现有效适应。精确度和IoU之间持续存在的差距进一步表明，欠分割和边界不准确仍然是遥感任务中的常见错误模式，特别是对于不规则和不常见目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing (RS) image segmentation is constrained by the limited availability of annotated data and a gap between overhead imagery and natural images used to train foundational models. This motivates effective adaptation under limited supervision. SAM3 concept-driven framework generates masks from textual prompts without requiring task-specific modifications, which may enable this adaptation. We evaluate SAM3 for RS imagery across four target types, comparing textual, geometric, and hybrid prompting strategies, under lightweight fine-tuning scales with increasing supervision, alongside zero-shot inference. Results show that combining semantic and geometric cues yields the highest performance across targets and metrics. Text-only prompting exhibits the lowest performance, with marked score gaps for irregularly shaped targets, reflecting limited semantic alignment between SAM3 textual representations and their overhead appearances. Nevertheless, textual prompting with light fine-tuning offers a practical performance-effort trade-off for geometrically regular and visually salient targets. Across targets, performance improves between zero-shot inference and fine-tuning, followed by diminishing returns as the supervision scale increases. Namely, a modest geometric annotation effort is sufficient for effective adaptation. A persistent gap between Precision and IoU further indicates that under-segmentation and boundary inaccuracies remain prevalent error patterns in RS tasks, particularly for irregular and less prevalent targets.</description>
      <author>example@mail.com (Roni Blushtein-Livnon, Osher Rafaeli, David Ioffe, Amir Boger, Karen Sandberg Esquenazi, Tal Svoray)</author>
      <guid isPermaLink="false">2512.15564v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Reducing Pilots in Channel Estimation With Predictive Foundation Models</title>
      <link>http://arxiv.org/abs/2512.15562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于预测基础模型的信道估计框架，用于实现准确、低开销和可泛化的信道状态信息(CSI)获取。该框架结合了跨领域训练的基础模型和基于视觉Transformer的导频处理网络，通过高效融合机制实现可靠CSI重建。&lt;h4&gt;背景&lt;/h4&gt;精确的信道状态信息(CSI)获取对现代无线系统至关重要，但在大天线阵列、严格的导频开销限制和多样化的部署环境下变得日益困难。现有人工智能解决方案通常缺乏鲁棒性，无法在不同场景中有效泛化。&lt;h4&gt;目的&lt;/h4&gt;解决现有AI解决方案缺乏鲁棒性和跨场景泛化能力的问题，实现准确、低开销且可泛化的CSI获取方法。&lt;h4&gt;方法&lt;/h4&gt;提出预测基础模型框架，使用在跨领域大规模CSI数据上训练的基础模型提取通用信道表示并提供预测先验；设计基于视觉Transformer架构的导频处理网络捕获空间、时间和频率相关性；实现高效融合机制整合预测先验与实时测量。&lt;h4&gt;主要发现&lt;/h4&gt;在不同配置下的广泛评估表明，所提出的估计器在准确性、鲁棒性和泛化能力方面显著优于传统和数据驱动的基线方法。&lt;h4&gt;结论&lt;/h4&gt;预测基础模型框架能够实现准确、低开销和可泛化的CSI获取，即使在稀疏或有噪声条件下也能可靠重建CSI。&lt;h4&gt;翻译&lt;/h4&gt;精确的信道状态信息(CSI)获取对现代无线系统至关重要，但在大天线阵列、严格的导频开销限制和多样化的部署环境下，这变得越来越困难。现有人工智能解决方案通常缺乏鲁棒性，无法在不同场景中泛化。为解决这一局限，本文引入了一种基于预测基础模型的信道估计框架，可实现准确、低开销和可泛化的CSI获取。所提出的框架使用在跨领域大规模CSI数据上训练的预测基础模型来提取通用信道表示，并提供具有强跨场景可转移性的预测先验。进一步设计了基于视觉Transformer架构的导频处理网络，从导频观测中捕获空间、时间和频率相关性。高效的融合机制将预测先验与实时测量相结合，即使在稀疏或有噪声条件下也能实现可靠的CSI重建。在多种配置下的广泛评估表明，所提出的估计器在准确性、鲁棒性和泛化能力方面显著优于传统和数据驱动的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate channel state information (CSI) acquisition is essential for modern wireless systems, which becomes increasingly difficult under large antenna arrays, strict pilot overhead constraints, and diverse deployment environments. Existing artificial intelligence-based solutions often lack robustness and fail to generalize across scenarios. To address this limitation, this paper introduces a predictive-foundation-model-based channel estimation framework that enables accurate, low-overhead, and generalizable CSI acquisition. The proposed framework employs a predictive foundation model trained on large-scale cross-domain CSI data to extract universal channel representations and provide predictive priors with strong cross-scenario transferability. A pilot processing network based on a vision transformer architecture is further designed to capture spatial, temporal, and frequency correlations from pilot observations. An efficient fusion mechanism integrates predictive priors with real-time measurements, enabling reliable CSI reconstruction even under sparse or noisy conditions. Extensive evaluations across diverse configurations demonstrate that the proposed estimator significantly outperforms both classical and data-driven baselines in accuracy, robustness, and generalization capability.</description>
      <author>example@mail.com (Xingyu Zhou, Le Liang, Hao Ye, Jing Zhang, Chao-Kai Wen, Shi Jin)</author>
      <guid isPermaLink="false">2512.15562v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry</title>
      <link>http://arxiv.org/abs/2512.15423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对单目深度基础模型在几何平面但感知模糊输入中产生幻觉性3D结构的现象（称为'3D Mirage'）提出了首个端到端框架，包括基准测试、评估方法和改进策略。&lt;h4&gt;背景&lt;/h4&gt;单目深度基础模型通过学习大规模语义先验知识实现了显著的泛化能力，但这导致了一个关键弱点：它们会在几何平面但感知模糊的输入中产生幻觉性的3D结构。&lt;h4&gt;目的&lt;/h4&gt;探测、量化和控制这种未量化的安全风险（3D Mirage现象），并提供诊断和减轻这一现象的基本工具。&lt;h4&gt;方法&lt;/h4&gt;1. 探测：提出3D-Mirage基准测试，包含现实世界幻觉案例（如街头艺术）及其精确平面区域标注；2. 量化：基于拉普拉斯评估框架，引入偏差综合分数(DCS)和混淆综合分数(CCS)；3. 控制：提出Grounded Self-Distillation策略，在幻觉区域强制平面性同时保留背景知识。&lt;h4&gt;主要发现&lt;/h4&gt;单目深度基础模型存在'3D Mirage'现象，即在几何平面但感知模糊的输入中会产生幻觉性的3D结构。&lt;h4&gt;结论&lt;/h4&gt;研究提供了诊断和减轻3D Mirage现象的工具，呼吁深度估计评估从像素级准确性转向结构和上下文鲁棒性，并将公开代码和基准测试促进该研究方向。&lt;h4&gt;翻译&lt;/h4&gt;单目深度基础模型通过学习大规模语义先验知识实现了显著的泛化能力，但这造成了一个关键弱点：它们会从几何平面但感知模糊的输入中产生幻觉性的3D结构。我们将这种失败称为'3D Mirage'。本文介绍了第一个端到端框架，用于探测、量化和控制这一未量化的安全风险。为了探测，我们提出了3D-Mirage，这是第一个具有精确平面区域标注和上下文限制裁剪的现实世界幻觉基准测试（例如街头艺术）。为了量化，我们提出了一个基于拉普拉斯的评估框架，包含两个指标：用于衡量虚假非平面性的偏差综合分数(DCS)和用于衡量上下文不稳定性的混淆综合分数(CCS)。为了控制这种失败，我们引入了Grounded Self-Distillation，这是一种参数高效策略，可以在幻觉ROI上强制执行平面性，同时使用冻结的教师模型保留背景知识，从而避免灾难性遗忘。我们的工作提供了诊断和减轻这一现象的基本工具，呼吁MDE评估从像素级准确性向结构和上下文鲁棒性的必要转变。我们的代码和基准测试将公开可用，以促进这一令人兴奋的研究方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目深度估计模型中的'3D Mirage'问题，即模型会将几何上平坦但感知上模糊的输入（如3D街头艺术）产生幻觉式的3D结构。这个问题在现实中非常重要，特别是在自动驾驶等安全关键应用中，当视野受限或部分遮挡时，模型可能会产生不存在的3D障碍物，导致严重的感知错误和安全隐患。此外，现有的评估指标（如MAE、RMSE）无法检测这种结构失败，使得这一漏洞在模型部署前难以被发现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别并定义了'3D Mirage'这一失败模式，发现现有SOTA模型在感知模糊的2D模式和受限视野场景中普遍失败。他们意识到标准评估指标无法检测这种结构失败，因此提出需要系统性地探测、量化和控制3D幻觉。在方法设计上，作者借鉴了参数高效微调（PEFT）方法如低秩自适应（LoRA），并创新性地应用了自蒸馏概念。他们还参考了现有深度估计模型架构（如Depth-Anything V2）和数据集（如Penn-Fudan和CamVid）进行正则化，以防止过度平坦化和边缘漂移。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过参数高效的微调方法，在保持模型原有知识的同时，专门针对幻觉区域进行校正。具体实现流程包括：1) 创建3D-Mirage基准数据集，包含真实世界3D错觉图像和平面ROI标注；2) 在预训练模型编码器中注入LoRA适配器；3) 设计三流训练过程（教师流、学生完整图像流、学生裁剪流）；4) 使用复合损失函数（LHKR损失强制幻觉区域平面性，LNKP损失保持背景区域原始知识）；5) 仅优化LoRA参数，训练1个周期以避免灾难性遗忘。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次系统性识别并分析'3D Mirage'现象；2) 创建首个专注于真实世界光学错觉场景的基准数据集；3) 提出基于拉普拉斯的评估框架和DCS/CCS指标；4) 提出Grounded Self-Distillation参数高效缓解策略；5) 设计复合损失函数同时处理幻觉校正和知识保存。相比之前工作，本文不关注传统像素级精度，而是关注结构和上下文鲁棒性；方法参数高效，避免灾难性遗忘；专门针对幻觉区域进行手术式校正，而非全局调整；上下文受限裁剪专门针对视野受限场景进行设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次系统性地识别、量化和缓解了单目深度估计模型中的'3D Mirage'幻觉现象，通过创新的基准数据集、评估指标和参数高效的微调方法，显著提高了模型在安全关键应用中的可靠性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction.</description>
      <author>example@mail.com (Hoang Nguyen, Xiaohao Xu, Xiaonan Huang)</author>
      <guid isPermaLink="false">2512.15423v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis</title>
      <link>http://arxiv.org/abs/2512.15250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at NeurIPS 2025 Workshop on Foundation Models for the Brain and Body&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种方法来解决多模态生理信号（ECG和EEG）集成的挑战，特别是在标记数据有限的情况下。通过调整CBraMod编码器并引入双掩码策略，研究者成功捕获了导联内和导联间的依赖关系，并通过简单的嵌入连接融合不同模态的表示，实现了有效的下游学习。&lt;h4&gt;背景&lt;/h4&gt;生理信号如心电图(ECG)和脑电图(EEG)能提供关于人类健康和认知的互补见解，但由于多模态标记数据有限和模态特定差异，多模态集成具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;克服多模态生理信号集成的挑战，特别是在标记数据有限的情况下实现有效的下游学习。&lt;h4&gt;方法&lt;/h4&gt;为大规模自监督ECG预训练调整CBraMod编码器，引入双掩码策略捕获导联内和导联间的依赖关系；利用预训练的CBraMod编码器处理EEG，预训练对称的ECG编码器；通过简单的嵌入连接融合这些表示，允许分类头学习跨模态交互。&lt;h4&gt;主要发现&lt;/h4&gt;在情感识别任务上，该方法取得了接近最先进的性能，证明精心设计的生理编码器即使使用简单的融合也能显著提高下游性能。&lt;h4&gt;结论&lt;/h4&gt;基础模型方法在利用生理信号的特性方面具有潜力，能够为医疗保健和情感计算提供可扩展、标记效率高且通用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;生理信号如心电图(ECG)和脑电图(EEG)能提供关于人类健康和认知的互补见解，但由于多模态标记数据有限和模态特定差异，多模态集成具有挑战性。在这项工作中，我们为大规模自监督ECG预训练调整了CBraMod编码器，引入了双掩码策略来捕获导联内和导联间的依赖关系。为了克服上述挑战，我们利用预训练的CBraMod编码器处理EEG，并预训练了一个对称的ECG编码器，为每个模态配备了丰富的基础表示。然后通过简单的嵌入连接融合这些表示，允许分类头学习跨模态交互，从而在有限的多模态监督下实现有效的下游学习。在情感识别任务上的评估表明，我们的方法取得了接近最先进的性能，证明精心设计的生理编码器即使使用简单的融合也能显著提高下游性能。这些结果突显了基础模型方法在利用生理信号整体特性方面的潜力，为医疗保健和情感计算提供了可扩展、标记效率高且通用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.</description>
      <author>example@mail.com (Youssef Ghallab, Omar Iraqy, Mohamed Kandil, Mohamed Ashraf, Saadeldine Eletter, Morougue Ghazal, Ayman Khalafallah, Nagwa El-Makky)</author>
      <guid isPermaLink="false">2512.15250v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Nudged Elastic Band Method using Neural Network Bayesian Algorithm Execution</title>
      <link>http://arxiv.org/abs/2512.14993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NN-BAX是一种新框架，通过联合学习能量景观和最小能量路径，显著减少了发现亚稳态间最小能量路径所需的计算量，在保持高精度的同时将计算时间从数周缩短至数小时或数天。&lt;h4&gt;背景&lt;/h4&gt;发现亚稳态之间的最小能量路径对于催化剂和生物分子设计等科学任务至关重要，但标准的NEB算法需要数百到数万次计算密集型模拟，使得复杂系统应用成本过高。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够减少计算需求的方法，用于高效发现亚稳态之间的最小能量路径。&lt;h4&gt;方法&lt;/h4&gt;引入神经网络贝叶斯算法执行(NN-BAX)，这是一种联合学习能量景观和最小能量路径的框架。NN-BAX通过主动选择样本针对性地改进最小能量路径，逐步微调基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;在Lennard-Jones和嵌入原子方法系统上的测试表明，该方法在能量和力评估方面实现了1-2个数量级的减少，同时最小能量路径的精度损失可忽略不计，并且可扩展到100维以上的系统。&lt;h4&gt;结论&lt;/h4&gt;这项工作为消除科学相关系统中最小能量路径发现的计算障碍提供了有希望的步骤，表明可能以最小的精度损失将数周长的计算缩短到几小时或几天。&lt;h4&gt;翻译&lt;/h4&gt;发现亚稳态之间的最小能量路径对于包括催化剂和生物分子设计在内的科学任务至关重要。然而，标准的弹力带算法需要数百到数万次计算密集型模拟，这使得在复杂系统中的应用成本过高。我们引入了神经网络贝叶斯算法执行(NN-BAX)，这是一种联合学习能量景观和最小能量路径的框架。NN-BAX通过主动选择样本针对性地改进最小能量路径，逐步微调基础模型。在Lennard-Jones和嵌入原子方法系统上的测试表明，我们的方法在能量和力评估方面实现了1-2个数量级的减少，同时最小能量路径的精度损失可忽略不计，并展示了扩展到100维以上系统的能力。因此，这项工作是消除科学相关系统中最小能量路径发现的计算障碍的有希望的一步，表明可能以最小的精度损失将数周长的计算缩短到几小时或几天。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The discovery of a minimum energy pathway (MEP) between metastable states is crucial for scientific tasks including catalyst and biomolecular design. However, the standard nudged elastic band (NEB) algorithm requires hundreds to tens of thousands of compute-intensive simulations, making applications to complex systems prohibitively expensive. We introduce Neural Network Bayesian Algorithm Execution (NN-BAX), a framework that jointly learns the energy landscape and the MEP. NN-BAX sequentially fine-tunes a foundation model by actively selecting samples targeted at improving the MEP. Tested on Lennard-Jones and Embedded Atom Method systems, our approach achieves a one to two order of magnitude reduction in energy and force evaluations with negligible loss in MEP accuracy and demonstrates scalability to &gt;100-dimensional systems. This work is therefore a promising step towards removing the computational barrier for MEP discovery in scientifically relevant systems, suggesting that weeks-long calculations may be achieved in hours or days with minimal loss in accuracy.</description>
      <author>example@mail.com (Pranav Kakhandiki, Sathya Chitturi, Daniel Ratner, Sean Gasiorowski)</author>
      <guid isPermaLink="false">2512.14993v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis</title>
      <link>http://arxiv.org/abs/2512.14922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 5 figures, 6 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了PANDA-PLUS-Bench基准数据集，用于评估人工智能基础模型在前列腺癌格里森分级中的稳健性，揭示了模型在区分生物信号与标本特异性伪影方面的差异。&lt;h4&gt;背景&lt;/h4&gt;人工智能基础模型越来越多地用于前列腺癌格里森分级，其中GP3/GP4的区别直接影响治疗决策。然而，这些模型可能通过学习标本特异性伪影而非可推广的生物特征来实现高验证准确率，限制了其在真实世界临床中的应用价值。&lt;h4&gt;目的&lt;/h4&gt;创建一个专门的数据集来量化人工智能基础模型在前列腺癌格里森分级中可能出现的失败模式，即学习标本特异性伪影而非生物特征。&lt;h4&gt;方法&lt;/h4&gt;开发了PANDA-PLUS-Bench基准数据集，包含来自九名患者的九个全载玻图像，具有不同的格里森模式，并在八种增强条件下提取了512x512和224x224像素分辨率的非重叠组织块。使用此基准评估了七个基础模型将生物信号与载玻片水平混淆因素分离的能力。&lt;h4&gt;主要发现&lt;/h4&gt;不同模型之间的稳健性存在显著差异；专门在前列腺组织上训练的HistoEncoder表现最佳，具有最高的跨载玻片准确率和最强的载玻片水平编码；所有模型都表现出可测量的载玻片内与跨载玻片准确率差距，幅度从19.9个百分点到26.9个百分点不等。&lt;h4&gt;结论&lt;/h4&gt;PANDA-PLUS-Bench为评估基础模型在前列腺癌格里森分级中的稳健性提供了专门资源，解决了该领域评估中的一个关键空白，并提供了开源工具供研究人员进一步评估模型。&lt;h4&gt;翻译&lt;/h4&gt;人工智能基础模型越来越多地被用于前列腺癌格里森分级，其中GP3/GP4的区别直接影响治疗决策。然而，这些模型可能通过学习标本特异性伪影而非可推广的生物特征来实现高验证准确率，限制了其在真实世界临床中的应用价值。我们引入了PANDA-PLUS-Bench，这是一个从专家注释的前列腺活检中精心策划的基准数据集，专门用于量化这种失败模式。该基准包含来自九名独特患者的九个精心挑选的全载玻图像，包含不同的格里森模式，并在八种增强条件下提取了512x512和224x224像素分辨率的非重叠组织块。使用此基准，我们评估了七个基础模型将生物信号与载玻片水平混淆因素分离的能力。我们的结果显示不同模型之间的稳健性存在显著差异：Virchow2在大规模模型中具有最低的载玻片水平编码，但表现出第二低的跨载玻片准确率。专门在前列腺组织上训练的HistoEncoder显示出最高的跨载玻片准确率和最强的载玻片水平编码，表明组织特异性训练可能增强生物特征捕获和载玻片特异性特征。所有模型都表现出可测量的载玻片内与跨载玻片准确率差距，尽管幅度从19.9个百分点到26.9个百分点不等。我们提供了一个开源的Google Colab笔记本，使研究人员能够使用标准化指标评估额外的基础模型与我们的基准。PANDA-PLUS-Bench通过提供专门为格里森分级这一临床重要背景下的稳健性评估而设计的资源，解决了基础模型评估中的一个关键空白。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence foundation models are increasingly deployed for prostate cancer Gleason grading, where GP3/GP4 distinction directly impacts treatment decisions. However, these models may achieve high validation accuracy by learning specimen-specific artifacts rather than generalizable biological features, limiting real-world clinical utility. We introduce PANDA-PLUS-Bench, a curated benchmark dataset derived from expert-annotated prostate biopsies designed specifically to quantify this failure mode. The benchmark comprises nine carefully selected whole slide images from nine unique patients containing diverse Gleason patterns, with non-overlapping tissue patches extracted at both 512x512 and 224x224 pixel resolutions across eight augmentation conditions. Using this benchmark, we evaluate seven foundation models on their ability to separate biological signal from slide-level confounders. Our results reveal substantial variation in robustness across models: Virchow2 achieved the lowest slide-level encoding among large-scale models (81.0%) yet exhibited the second-lowest cross-slide accuracy (47.2%). HistoEncoder, trained specifically on prostate tissue, demonstrated the highest cross-slide accuracy (59.7%) and the strongest slide-level encoding (90.3%), suggesting tissue-specific training may enhance both biological feature capture and slide-specific signatures. All models exhibited measurable within-slide vs. cross-slide accuracy gaps, though the magnitude varied from 19.9 percentage points to 26.9 percentage points. We provide an open-source Google Colab notebook enabling researchers to evaluate additional foundation models against our benchmark using standardized metrics. PANDA-PLUS-Bench addresses a critical gap in foundation model evaluation by providing a purpose-built resource for robustness assessment in the clinically important context of Gleason grading.</description>
      <author>example@mail.com (Joshua L. Ebbert, Dennis Della Corte)</author>
      <guid isPermaLink="false">2512.14922v1</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>MMGR: Multi-Modal Generative Reasoning</title>
      <link>http://arxiv.org/abs/2512.14691v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MMGR（多模态生成推理评估和基准）框架，用于评估视频和图像生成模型在物理、逻辑、空间和时间推理方面的能力，揭示了当前模型在抽象推理和长期空间规划方面的显著局限性。&lt;h4&gt;背景&lt;/h4&gt;视频基础模型能够生成视觉上逼真且时间上一致的内容，但它们作为世界模拟器的可靠性取决于是否能捕捉物理、逻辑和空间约束。现有评估指标如FVD过于关注感知质量，忽略了推理失败问题。&lt;h4&gt;目的&lt;/h4&gt;开发全面的评估框架，衡量生成模型在物理、逻辑、3D空间、2D空间和时间推理上的表现，揭示当前模型局限性并指导未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;提出基于五种推理能力的MMGR评估框架，在抽象推理、具身导航和物理常识三个领域进行评估，应用细粒度指标要求视频和图像生成整体正确，并对领先的视频和图像模型进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;模型在物理常识任务上表现中等，但在抽象推理上表现较差（ARC-AGI准确率低于10%），在具身环境中的长期空间规划方面存在困难。当前模型存在过度依赖感知数据、全局状态一致性弱，以及奖励视觉 plausible 性而非因果正确性的局限性。&lt;h4&gt;结论&lt;/h4&gt;MMGR提供了统一的诊断基准，朝向推理感知的生成世界模型发展，有助于改进现有模型并指导未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;视频基础模型生成视觉上逼真且时间上一致的内容，但它们作为世界模拟器的可靠性取决于是否能捕捉物理、逻辑和空间约束。现有指标如Frechet Video Distance强调感知质量而忽略了推理失败，包括因果性、物理和全局一致性的违反。我们引入了MMGR（多模态生成推理评估和基准），这是一个基于五种推理能力的原则性评估框架：物理、逻辑、3D空间、2D空间和时间。MMGR评估了三个领域的生成推理：抽象推理、具身导航和物理常识。MMGR应用了细粒度指标，要求视频和图像生成在整体上正确。我们对领先的视频模型和图像模型进行了基准测试，揭示了不同领域之间存在明显的性能差距。模型在物理常识任务上表现中等，但在抽象推理上表现较差，并且在具身环境中的长期空间规划方面存在困难。我们的分析揭示了当前模型的关键局限性，包括过度依赖感知数据、全局状态一致性弱，以及奖励视觉 plausible 性而非因果正确性的目标。MMGR提供了一个统一的诊断基准和朝向推理感知的生成世界模型的发展路径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的问题是现有视频和图像生成模型缺乏对现实世界的基本推理能力，包括物理、逻辑、空间和时间约束的理解。这个问题很重要，因为现有评估指标主要关注视觉质量而非推理能力，导致模型可能生成违反物理规律或逻辑一致性的内容（如台球穿过彼此），这些内容虽然视觉吸引人但不符合现实世界规律。提高生成模型的推理能力对电影制作、科学可视化和机器人等领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有评估指标的局限性，基于人类认知理论提出了五种核心推理能力框架。他们设计了三个互补评估领域：抽象推理、具身导航和物理常识。借鉴了人类认知理论中的'核心知识'概念、ARC-AGI评估任务、VideoPhy本体以及VLM评估方法。他们结合了现有工作并进行了创新，构建了一个系统评估生成模型推理能力的全新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是基于五种推理能力（物理、逻辑、3D空间、2D空间、时间推理）构建评估框架，使用细粒度指标要求整体正确性而非部分成功。整体流程包括：1) 创建包含1,853个样本的评估数据集；2) 对每个提示生成5个样本；3) 使用VLM自动评估和人工评估验证；4) 分析模型在不同推理能力上的表现。三个评估领域分别是抽象推理（迷宫、数独等）、具身导航（3D导航等）和物理常识（体育活动等）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个系统评估生成模型推理能力的框架；2) 五种核心推理能力框架；3) 三个互补评估领域；4) 细粒度整体正确性评估指标；5) 多模态评估视频和图像模型；6) 人机结合评估。相比之前工作，MMGR从'理解'转向'生成'评估，超越感知保真度关注逻辑一致性，提供全面评估而非特定失败模式，并使用更严格的整体正确性指标。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MMGR通过系统评估生成模型在物理、逻辑、空间和时间推理方面的能力，揭示了当前模型在抽象推理和长期空间规划上的严重不足，为开发真正具有世界模拟能力的生成模型提供了明确的评估框架和改进方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.</description>
      <author>example@mail.com (Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Wen Xiao, Jiuxiang Gu, Nanyun Peng, Junjie Hu)</author>
      <guid isPermaLink="false">2512.14691v2</guid>
      <pubDate>Thu, 18 Dec 2025 15:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning-Based Surrogate Modeling for Nonlinear Time-History Response Analysis of High-Fidelity Structural Models</title>
      <link>http://arxiv.org/abs/2512.14161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种使用迁移学习构建高保真度响应分析替代模型的框架，以解决基于性能的地震工程框架中非线性时程响应分析计算成本高的问题。该框架利用低保真度模型作为预训练模型，显著减少了高保真度模型的训练成本。案例研究表明，仅使用20个样本训练的模型能够准确预测20层钢框架结构的地震响应。&lt;h4&gt;背景&lt;/h4&gt;在基于性能的地震工程框架中，需要进行大量地震动的非线性时程响应分析来评估建筑物或土木工程结构的地震风险。然而，这些数值模拟计算成本高，限制了框架的实际应用。先前的研究使用机器学习预测结构响应，但这些研究大多专注于计算成本较低的单自由度模型，而高保真度模型需要更多的训练数据，进一步增加了计算负担。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够用少量训练样本实现高保真度响应分析替代模型的方法，以减少计算成本，同时保持预测的准确性，从而促进基于性能的地震工程框架的实际应用。&lt;h4&gt;方法&lt;/h4&gt;提出了一种使用迁移学习构建高保真度响应分析替代模型的框架。该方法使用低保真度响应分析的替代模型作为预训练模型，并将其知识迁移到高保真度响应分析替代模型的构建中，从而显著降低计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;作为案例研究，仅使用20个样本作为训练数据集，成功构建了预测20层钢框架结构响应的替代模型。该模型预测的地震动响应与特定场地的基于时间的危险性一致，证明了所提出框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的迁移学习框架能够有效地减少高保真度响应分析替代模型的训练成本，同时保持预测的准确性。这种方法为基于性能的地震工程框架的实际应用提供了新的可能性，特别是在需要高保真度模型但计算资源有限的情况下。&lt;h4&gt;翻译&lt;/h4&gt;在基于性能的地震工程框架中，需要对大量地震动进行非线性时程响应分析，以评估建筑物或土木工程结构的地震风险。然而，这类数值模拟计算成本高，限制了框架的实际应用。为解决这一问题，先前的研究使用机器学习以较低的计算成本预测结构对地震动的响应。这些研究通常对几百个地震动进行非线性时程响应分析，并利用结果训练和验证替代模型。然而，大多数先前的研究专注于计算成本较低的响应分析模型，如单自由度系统。基于性能的地震工程中需要高保真度响应分析的替代模型，以丰富用于损伤评估的信息数量和多样性。值得注意的是，随着响应分析模型保真度的提高，创建训练和验证数据集的计算成本也会增加。因此，需要一种能够用少量训练样本实现高保真度响应分析替代模型的方法。本研究提出了一种使用迁移学习构建高保真度响应分析替代模型的框架。该框架使用低保真度响应分析的替代模型作为预训练模型，并将其知识迁移到高保真度响应分析替代模型的构建中，从而显著降低计算成本。作为案例研究，仅使用20个样本作为训练数据集，构建了预测20层钢框架结构响应的替代模型。该替代模型预测的地震动响应与特定场地的基于时间的危险性一致。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In a performance based earthquake engineering (PBEE) framework, nonlinear time-history response analysis (NLTHA) for numerous ground motions are required to assess the seismic risk of buildings or civil engineering structures. However, such numerical simulations are computationally expensive, limiting the real-world practical application of the framework. To address this issue, previous studies have used machine learning to predict the structural responses to ground motions with low computational costs. These studies typically conduct NLTHAs for a few hundreds ground motions and use the results to train and validate surrogate models. However, most of the previous studies focused on computationally-inexpensive response analysis models such as single degree of freedom. Surrogate models of high-fidelity response analysis are required to enrich the quantity and diversity of information used for damage assessment in PBEE. Notably, the computational cost of creating training and validation datasets increases if the fidelity of response analysis model becomes higher. Therefore, methods that enable surrogate modeling of high-fidelity response analysis without a large number of training samples are needed. This study proposes a framework that uses transfer learning to construct the surrogate model of a high-fidelity response analysis model. This framework uses a surrogate model of low-fidelity response analysis as the pretrained model and transfers its knowledge to construct surrogate models for high-fidelity response analysis with substantially reduced computational cost. As a case study, surrogate models that predict responses of a 20-story steel moment frame were constructed with only 20 samples as the training dataset. The responses to the ground motions predicted by constructed surrogate model were consistent with a site-specific time-based hazard.</description>
      <author>example@mail.com (Keiichi Ishikawa, Yuma Matsumoto, Taro Yaoyama, Sangwon Lee, Tatsuya Itoi)</author>
      <guid isPermaLink="false">2512.14161v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
  <item>
      <title>Unreasonable effectiveness of unsupervised learning in identifying Majorana topology</title>
      <link>http://arxiv.org/abs/2512.13825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究结合无监督和监督学习，使用自编码器分析马约拉纳纳米线中的无标签数据，以识别拓扑特性并确定拓扑与平庸状态的交叉点。&lt;h4&gt;背景&lt;/h4&gt;无监督学习在深度学习中使用无标签数据，迫使算法发现数据中的隐藏模式。这种方法可能成为识别拓扑序的有力工具，因为拓扑特性（如拓扑超导性）并不总是以明显的物理方式显现。&lt;h4&gt;目的&lt;/h4&gt;结合无监督和监督学习方法，利用自编码器分析马约拉纳纳米线中的无标签数据，以区分拓扑状态和平庸状态，并确定它们在参数空间中的交叉点。&lt;h4&gt;方法&lt;/h4&gt;结合无监督和监督学习，使用自编码器分析真实短无序纳米线中的马约拉纳分裂的无标签数据。&lt;h4&gt;主要发现&lt;/h4&gt;无标签数据不仅能区分'拓扑'和'平庸'状态，还能确定它们在相关参数空间中的交叉点。&lt;h4&gt;结论&lt;/h4&gt;这种结合无监督和监督学习的方法可能成为识别马约拉纳纳米线拓扑特性的有用工具。&lt;h4&gt;翻译&lt;/h4&gt;在无监督学习中，深度学习的训练数据不带有任何标签，因此迫使算法发现数据中的隐藏模式以辨别有用信息。原则上，这可能是一个识别拓扑序的有力工具，因为拓扑并不总是以明显的物理方式（例如拓扑超导性）表现以供确认。然而，问题在于无监督学习是一个困难的挑战，需要巨大的计算资源，且不一定总是有效。在当前工作中，我们使用自编码器结合无监督和监督学习，证明在真实短无序纳米线中的马约拉纳分裂的无标签数据不仅可以区分'拓扑'和'平庸'，还可以确定它们在相关参数空间中的交叉位置。这可能是识别马约拉纳纳米线拓扑特性的有用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In unsupervised learning, the training data for deep learning does not come with any labels, thus forcing the algorithm to discover hidden patterns in the data for discerning useful information. This, in principle, could be a powerful tool in identifying topological order since topology does not always manifest in obvious physical ways (e.g., topological superconductivity) for its decisive confirmation. The problem, however, is that unsupervised learning is a difficult challenge, necessitating huge computing resources, which may not always work. In the current work, we combine unsupervised and supervised learning using an autoencoder to establish that unlabeled data in the Majorana splitting in realistic short disordered nanowires may enable not only a distinction between `topological' and `trivial', but also where their crossover happens in the relevant parameter space. This may be a useful tool in identifying topology in Majorana nanowires.</description>
      <author>example@mail.com (Jacob Taylor, Haining Pan, Sankar Das Sarma)</author>
      <guid isPermaLink="false">2512.13825v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Predictions of Process-Induced Deformation in Carbon/Epoxy Composites Using a Deep Operator Network</title>
      <link>http://arxiv.org/abs/2512.13746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种结合物理模型和数据驱动方法的框架，用于预测和优化复合材料制造过程中的过程诱导变形(PID)。研究使用双机制模型模拟PID，并通过深度算子网络和迁移学习技术提高预测精度，同时利用集合卡尔曼反演量化不确定性。&lt;h4&gt;背景&lt;/h4&gt;纤维增强体和聚合物基体在制造条件下的热膨胀系数不匹配以及热固性树脂固化过程中的基体收缩，会产生多尺度的残余应力，这些应力的部分释放会导致过程诱导变形(PID)，需要通过优化的非等温固化循环进行准确预测和缓解。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测复合材料制造过程中PID的框架，并通过优化固化循环减少PID，同时量化预测中的不确定性。&lt;h4&gt;方法&lt;/h4&gt;研究使用双机制框架模拟PID，考虑热膨胀/收缩和固化收缩；基于物理模型开发深度算子网络(DeepONets)代理模型，结合高保真模拟和实验数据；扩展到特征线性调制(FiLM) DeepONet，能够预测固化程度、粘度和变形的时间历程；使用迁移学习处理有限实验数据；通过集合卡尔曼反演(EKI)量化不确定性并支持优化。&lt;h4&gt;主要发现&lt;/h4&gt;双机制模型能够准确模拟PID；深度算子网络结合物理模型和数据驱动方法提高了PID预测的准确性；特征线性调制DeepONet能够预测多个参数的时间历程；迁移学习有效处理了有限实验数据；集合卡尔曼反演能够量化预测不确定性并支持优化。&lt;h4&gt;结论&lt;/h4&gt;结合物理模型和数据驱动方法的框架能够有效预测和优化复合材料制造过程中的PID，为减少复合材料制造中的变形提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;纤维增强体和聚合物基体由于热膨胀系数不匹配以及热固性树脂固化过程中的基体收缩，对制造条件的响应不同。这些异质性在多个长度尺度上产生残余应力，其部分释放导致过程诱导变形(PID)，需要通过优化的非等温固化循环进行准确预测和缓解。本研究考虑单向AS4碳纤维/胺双官能团环氧预浸料，并使用考虑热膨胀/收缩和固化收缩的双机制框架模拟PID。模型通过制造试验验证以识别初始和边界条件，然后用于生成多种非等温固化循环的PID响应。基于此物理基础，我们开发了基于深度算子网络(DeepONets)的数据驱动代理模型。DeepONet在结合高保真模拟和PID目标实验测量的数据集上进行训练。我们进一步扩展到特征线性调制(FiLM) DeepONet，其中分支网络特征由包括初始固化程度的外部参数调制，能够预测固化程度、粘度和变形的时间历程。由于实验数据仅在有限时间点可用(例如最终变形)，我们使用迁移学习：模拟训练的主干和分支网络保持固定，仅使用测量的最终变形更新最后一层。最后，我们通过集合卡尔曼反演(EKI)增强框架，以量化实验条件下的不确定性并支持减少复合材料PID的固化时间表优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fiber reinforcement and polymer matrix respond differently to manufacturing conditions due to mismatch in coefficient of thermal expansion and matrix shrinkage during curing of thermosets. These heterogeneities generate residual stresses over multiple length scales, whose partial release leads to process-induced deformation (PID), requiring accurate prediction and mitigation via optimized non-isothermal cure cycles. This study considers a unidirectional AS4 carbon fiber/amine bi-functional epoxy prepreg and models PID using a two-mechanism framework that accounts for thermal expansion/shrinkage and cure shrinkage. The model is validated against manufacturing trials to identify initial and boundary conditions, then used to generate PID responses for a diverse set of non-isothermal cure cycles (time-temperature profiles). Building on this physics-based foundation, we develop a data-driven surrogate based on Deep Operator Networks (DeepONets). A DeepONet is trained on a dataset combining high-fidelity simulations with targeted experimental measurements of PID. We extend this to a Feature-wise Linear Modulation (FiLM) DeepONet, where branch-network features are modulated by external parameters, including the initial degree of cure, enabling prediction of time histories of degree of cure, viscosity, and deformation. Because experimental data are available only at limited time instances (for example, final deformation), we use transfer learning: simulation-trained trunk and branch networks are fixed and only the final layer is updated using measured final deformation. Finally, we augment the framework with Ensemble Kalman Inversion (EKI) to quantify uncertainty under experimental conditions and to support optimization of cure schedules for reduced PID in composites.</description>
      <author>example@mail.com (Elham Kiyani, Amit Makarand Deshpande, Madhura Limaye, Zhiwei Gao, Sai Aditya Pradeep, Srikanth Pilla, Gang Li, Zhen Li, George Em Karniadakis)</author>
      <guid isPermaLink="false">2512.13746v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning of Density Estimates with Topological Optimization</title>
      <link>http://arxiv.org/abs/2512.08895v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于拓扑的无监督学习方法，用于自动选择最优的核带宽参数，并通过与经典技术比较展示了其在不同维度上的潜力。&lt;h4&gt;背景&lt;/h4&gt;核密度估计是机器学习、贝叶斯推断、随机动力学和信号处理等多种算法的关键组成部分。然而，这种无监督密度估计技术需要调整关键超参数：核带宽。带宽选择至关重要，因为它通过过度或不足平滑拓扑特征来控制偏差-方差权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种使用基于拓扑的损失函数的无监督学习方法，以自动选择最优带宽，并在不同维度上展示其有效性。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于拓扑的损失函数的无监督学习方法，用于自动和无监督地选择最优带宽，并与经典技术进行基准比较。&lt;h4&gt;主要发现&lt;/h4&gt;基于拓扑的损失函数可以有效选择最优核带宽参数，这种方法在不同维度上都显示出良好性能。&lt;h4&gt;结论&lt;/h4&gt;基于拓扑的无监督带宽选择方法有效，可以在不同维度上自动选择最优带宽，无需人工干预。&lt;h4&gt;翻译&lt;/h4&gt;核密度估计是机器学习、贝叶斯推断、随机动力学和信号处理等多种算法的关键组成部分。然而，这种无监督密度估计技术需要调整一个关键的超参数：核带宽。带宽的选择至关重要，因为它通过过度或不足平滑拓扑特征来控制偏差-方差权衡。拓扑数据分析提供了数学量化拓扑特征的方法，如连通分量、环、空隙等，即使在无法可视化密度估计的高维空间中也是如此。在本文中，我们提出了一种使用基于拓扑的损失函数的无监督学习方法，用于自动和无监督地选择最优带宽，并与经典技术进行比较——展示了其在不同维度上的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.</description>
      <author>example@mail.com (Sunia Tanweer, Firas A. Khasawneh)</author>
      <guid isPermaLink="false">2512.08895v2</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Unified Semantic Transformer for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2512.14364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://unite-page.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UNITE是一个统一的3D场景理解语义变换器，是一种新颖的前馈神经网络，能够将多种3D语义任务整合到单一模型中，仅从RGB图像直接预测多个语义属性。&lt;h4&gt;背景&lt;/h4&gt;全面的3D场景理解涉及捕捉和解析非结构化的3D环境。由于现实世界的固有复杂性，现有模型主要被开发为特定任务的，并且仅限于特定任务。&lt;h4&gt;目的&lt;/h4&gt;引入UNITE，一个统一的3D场景理解语义变换器，旨在将多种3D语义任务统一到一个单一模型中，实现端到端的3D场景理解。&lt;h4&gt;方法&lt;/h4&gt;UNITE是一个新颖的前馈神经网络，能够在完全端到端的方式下对未见过的场景进行操作，仅需几秒钟即可推断完整的3D语义几何。该方法结合了2D蒸馏进行训练，严重依赖自监督，并利用新颖的多视图损失来确保3D视图一致性。&lt;h4&gt;主要发现&lt;/h4&gt;UNITE在几种不同的语义任务上达到了最先进的性能，甚至在许多情况下超越了特定任务的模型，超过了那些在真实3D几何上运行的方法。&lt;h4&gt;结论&lt;/h4&gt;UNITE是一个统一的3D场景理解模型，能够高效地处理多种语义任务，并且性能优于现有的特定任务模型。&lt;h4&gt;翻译&lt;/h4&gt;全面的3D场景理解涉及捕捉和解析非结构化的3D环境。由于现实世界的固有复杂性，现有模型主要被开发为特定任务的，并且仅限于特定任务。我们介绍了UNITE，一个用于3D场景理解的统一语义变换器，这是一个新颖的前馈神经网络，将多种3D语义任务统一到一个单一模型中。我们的模型以完全端到端的方式对未见过的场景进行操作，仅需几秒钟即可推断完整的3D语义几何。我们的方法能够仅从RGB图像直接预测多个语义属性，包括3D场景分割、实例嵌入、开放词汇特征以及功能性和关节性。该方法结合了2D蒸馏进行训练，严重依赖自监督，并利用新颖的多视图损失来确保3D视图一致性。我们证明UNITE在几种不同的语义任务上达到了最先进的性能，甚至在许多情况下超越了特定任务的模型，超过了那些在真实3D几何上运行的方法。项目网站见 unite-page.github.io&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有3D场景理解模型大多是特定任务的问题，无法统一处理多种语义任务。这个问题在AR/VR和机器人应用中非常重要，因为3D场景理解是这些应用的基础，使系统能够感知周围环境并构建结合了几何和高级语义的丰富3D表示。统一的模型可以简化流程，提高效率，同时处理多种语义任务如语义分割、实例分割、开放词汇搜索和关节预测等。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D场景理解方法的局限性：辐射场方法依赖相机姿态和场景特定训练；蒸馏方法需要3D重建；基于提升的方法依赖手工设计且难以扩展。作者借鉴了VGGT作为几何基础，使用DPT学习语义特征，采用对比学习方法学习实例嵌入，并使用专门的DPT头预测关节。训练方法上，作者使用2D基础模型的蒸馏，主要依赖自监督，并设计了新的多视图一致性损失来确保不同视角的一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个单一的前馈transformer模型，能够同时处理多种3D语义任务，并在单一网络中联合学习几何和语义，避免手工设计的提升步骤。整体流程为：1)输入多视角RGB图像；2)使用VGGT backbone预测几何属性；3)通过DPT头预测开放词汇语义特征；4)学习实例嵌入；5)预测物体关节；6)通过多视图一致性损失确保不同视角的一致性；7)输出包含语义特征、实例特征和关节预测的3D点云重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在单一前馈网络中整合多种3D语义任务；2)完全端到端训练，无需手工后处理；3)提出新的多视图一致性损失；4)联合几何-语义学习；5)实现开放词汇推理。与之前工作不同，UNITE不需要场景特定训练(对比辐射场方法)，不需要推理时的3D重建(对比蒸馏方法)，不依赖手工设计的视图选择(对比基于提升的方法)，且是真正统一的模型(对比其他需要多阶段的语义前馈模型)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UNITE是一个统一的语义转换器，它通过在单一前馈网络中联合学习几何和语义，实现了从RGB图像直接进行多任务3D场景理解，并在多个任务上达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io</description>
      <author>example@mail.com (Sebastian Koch, Johanna Wald, Hide Matsuki, Pedro Hermosilla, Timo Ropinski, Federico Tombari)</author>
      <guid isPermaLink="false">2512.14364v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Consistent Instance Field for Dynamic Scene Understanding</title>
      <link>http://arxiv.org/abs/2512.14126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一致实例场，一种用于动态场景理解的连续且概率性的时空表示方法，通过建模每个时空点的占用概率和条件实例分布，将可见性与持久对象身份分离。&lt;h4&gt;背景&lt;/h4&gt;先前的方法依赖于离散跟踪或视图相关特征，存在局限性，需要新的表示方法来解决动态场景理解中的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理动态场景理解的连续且概率性的时空表示方法，解决可见性与对象身份分离的问题。&lt;h4&gt;方法&lt;/h4&gt;1) 引入基于可变形3D高斯的新实例嵌入表示；2) 联合编码辐射度和语义信息；3) 通过可微分光栅化直接从输入RGB图像和实例掩码中学习；4) 引入新机制校准每个高斯身份并重新采样高斯朝向语义活跃区域。&lt;h4&gt;主要发现&lt;/h4&gt;在HyperNeRF和Neu3D数据集上的实验表明，该方法在新型视图全景分割和开放词汇4D查询任务上显著优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;一致实例场方法通过连续概率表示有效解决了动态场景理解中的可见性与对象身份分离问题，在多个任务上取得了优越性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一致实例场，一种用于动态场景理解的连续且概率性的时空表示。与依赖于离散跟踪或视图相关特征的先前方法不同，我们的方法通过建模每个时空点的占用概率和条件实例分布，将可见性与持久对象身份分离。为此，我们引入了一种基于可变形3D高斯的新实例嵌入表示，该表示联合编码辐射度和语义信息，并通过可微分光栅化直接从输入RGB图像和实例掩码中学习。此外，我们引入了新的机制来校准每个高斯的身份并重新采样高斯朝向语义活跃区域，确保在空间和时间上保持一致的实例表示。在HyperNeRF和Neu3D数据集上的实验表明，我们的方法在新型视图全景分割和开放词汇4D查询任务上显著优于最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决动态场景理解中的实例一致性建模问题。现有方法在处理动态场景时往往依赖于离散跟踪或视图相关特征，导致实例身份识别不稳定，特别是在物体变形或遮挡情况下。这个问题在现实中非常重要，因为动态场景理解是增强/虚拟现实、自动驾驶和机器人等广泛应用的基础，需要系统能够准确识别物体身份并保持时间上一致的语义理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：它们依赖于视图相关特征，通过RGB调制进行监督，导致语义在动态场景中不一致。然后提出新视角：将焦点从跟踪变化的外观转向建模物体在4D空间中的持久组成。作者借鉴了基于NeRF和基于高斯的可变形表示，但扩展了它们以编码实例语义信息。同时参考了视觉语言特征和2D掩码监督方法，但解决了它们的视图依赖性问题。最终引入了实例身份估计和实例引导重采样两个关键机制，以对齐离散高斯表示与底层连续场。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入一致的实例场(Consistent Instance Field)，一种连续的时空表示，联合编码物体的存在性和身份。将物体的存在性与持久身份解耦：占用概率建模物理存在的时空连续性，条件身份分布保持变形和运动中的一致归属。整体实现流程包括：1)实例嵌入高斯表示，将场景表示为一组编码几何、颜色、不透明度、占用概率和身份分布的高斯；2)实例身份估计，通过跨视图和时间的聚合推断高斯身份并减轻可见性偏差；3)实例引导重采样，根据实例场信号自适应重新分配高斯密度；4)场感知溅射，通过可微分渲染联合优化所有组件。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了一致的实例场(CIF)，一种连续且概率化的时空表示，联合编码占用和实例身份；2)引入实例身份估计和实例引导重采样两个机制，将离散高斯表示与连续场对齐；3)实现了在4D场景中连贯的几何、外观和身份建模。相比之前的工作，CIF解决了视图依赖性问题，避免了将身份与辐射关联，防止了可见性偏差导致的漂移；提供了更高效、更可解释的显式表示；不仅编码几何和外观，还明确建模实例身份；通过物理存在的持久性建立语义与几何和外观的原则性连接。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一致的实例场框架，通过联合建模物体的存在性和持久身份，实现了动态场景中时空一致的实例理解，显著提升了新视角全景分割和开放词汇4D查询任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.</description>
      <author>example@mail.com (Junyi Wu, Van Nguyen Nguyen, Benjamin Planche, Jiachen Tao, Changchang Sun, Zhongpai Gao, Zhenghao Zhao, Anwesa Choudhuri, Gengyu Zhang, Meng Zheng, Feiran Wang, Terrence Chen, Yan Yan, Ziyan Wu)</author>
      <guid isPermaLink="false">2512.14126v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning Perspective of Scene Understanding in Autonomous Robots</title>
      <link>http://arxiv.org/abs/2512.14020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages. Review Paper on Deep Learning Perspective of Scene Understanding in Autonomous Robots&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了深度学习在自主机器人场景理解中的应用，包括目标检测、语义和实例分割、深度估计、3D重建和视觉SLAM等创新技术，强调了这些技术如何解决传统几何模型的局限性并提升机器人的环境感知能力。&lt;h4&gt;背景&lt;/h4&gt;自主机器人在复杂环境中进行场景理解面临挑战，传统几何模型存在局限性，需要更先进的技术来提高感知能力。&lt;h4&gt;目的&lt;/h4&gt;综述深度学习技术在自主机器人场景理解中的应用，分析其优势和局限性，并指出未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;对深度学习在自主机器人场景理解中的应用进行文献综述，重点关注目标检测、语义和实例分割、深度估计、3D重建和视觉SLAM等技术。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习技术能够解决传统几何模型的局限性，提高实时深度感知能力（即使在遮挡和无纹理表面情况下），增强语义推理能力，使机器人能更好地理解环境，并在集成到动态和非结构化环境中时提升决策、导航和交互效果。&lt;h4&gt;结论&lt;/h4&gt;基于学习的场景理解对自主机器人至关重要，但仍存在一些问题需要进一步研究解决，未来研究应关注解决这些问题以推进该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;本文综述了深度学习在自主机器人场景理解中的应用，包括目标检测、语义和实例分割、深度估计、3D重建以及视觉SLAM等方面的创新。论文强调了这些技术如何解决传统几何模型的局限性，提高对遮挡和无纹理表面的实时深度感知能力，并增强语义推理以更好地理解环境。当这些感知模块集成到动态和非结构化环境中时，它们在决策、导航和交互方面变得更加有效。最后，该综述概述了现有问题和研究方向，以推进基于学习的自主机器人场景理解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自主机器人在复杂、动态和非结构化环境中的场景理解问题。这个问题在现实中非常重要，因为随着自主机器人在医疗、物流和制造等领域的广泛应用，它们需要具备高级感知和认知能力来安全有效地与人类和环境互动。传统计算机视觉方法在处理遮挡、无纹理表面和动态环境时存在局限，而深度学习能够提供更强大的特征提取、上下文分析和多模态融合能力，使机器人能够实现更准确的环境理解，从而支持更复杂的决策、导航和交互任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作为一篇综述论文，作者并非设计新方法，而是系统性地梳理和分析了现有深度学习技术在机器人场景理解中的应用。作者首先指出了传统计算机视觉方法的局限性，然后深入探讨了各种深度学习架构（CNN、RNN、GAN、Transformer）如何解决这些局限。作者确实大量借鉴了现有工作，论文引用了81篇参考文献，涵盖了从基础理论研究到实际应用的广泛内容。通过整合这些现有工作，作者构建了一个全面的框架，展示了深度学习如何提升机器人在目标检测、语义分割、深度估计、3D重建和视觉SLAM等关键任务中的性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这篇论文的核心思想是利用深度学习技术，特别是卷积神经网络、循环神经网络、生成对抗网络和Transformer等架构，使机器人能够从原始传感器数据中提取丰富的层次化特征，实现对环境的全面理解。这种方法超越了传统的几何模型，能够处理遮挡、无纹理表面等挑战，并在动态和非结构化环境中提供更好的语义推理。整体实现流程包括：首先介绍各种深度学习基础架构及其特点；然后分析这些技术在具体任务（目标检测、语义分割、深度估计、3D重建和视觉SLAM）中的应用；接着探讨在动态环境中处理动态物体、运动预测和场景理解的挑战；最后指出现有方法的局限性并提出未来研究方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 作为综述论文，本文的关键创新点在于：1) 全面性：涵盖了深度学习在机器人场景理解中的多个方面，从基础架构到具体应用，再到未来挑战；2) 整合视角：将几何和语义信息整合到场景理解中，超越了传统方法仅关注几何的局限；3) 强调动态环境：特别关注了动态环境中的场景理解，包括动态物体的识别、运动预测等；4) 前沿技术整合：探讨了Transformer、NeRFs等最新技术在机器人场景理解中的应用；5) 实用导向：关注实际应用中的挑战，如实时性能、计算效率、鲁棒性等。相比之前的综述工作，本文更新了最新研究成果，更加强调动态环境处理，更全面地探讨实际应用挑战，并更注重几何和语义信息的整合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文系统性地综述了深度学习技术在自主机器人场景理解中的应用，从基础架构到具体任务，再到实际挑战，为理解和推进机器人环境感知能力提供了全面的视角和指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.</description>
      <author>example@mail.com (Afia Maham, Dur E Nayab Tashfa)</author>
      <guid isPermaLink="false">2512.14020v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</title>
      <link>http://arxiv.org/abs/2512.13177v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MMDrive是一个多模态视觉-语言模型框架，通过整合占用地图、LiDAR点云和文本描述，实现了从2D图像理解到3D场景理解的转变，显著提升了自动驾驶场景理解和推理能力。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型通过多源信息融合能够理解和推理复杂的交通场景，是自动驾驶的核心技术。然而，现有模型受限于二维平面图像理解范式，无法有效感知3D空间信息和执行深度语义融合，导致在复杂自动驾驶环境中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出MMDrive框架，将传统的图像理解扩展到广义的3D场景理解框架，以解决现有视觉-语言模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;MMDrive集成了三种互补模态：占用地图、LiDAR点云和文本场景描述。引入两个新颖组件：1) 面向文本的多模态调制器，根据问题语义线索动态加权各模态贡献；2) 跨模态抽象器，使用可学习抽象令牌生成紧凑的跨模态摘要，突出关键区域和基本语义。&lt;h4&gt;主要发现&lt;/h4&gt;在DriveLM和NuScenes-QA基准测试上，MMDrive显著超越现有模型：DriveLM上BLEU-4得分为54.56，METEOR得分为41.78；NuScenes-QA上准确得分为62.7%。&lt;h4&gt;结论&lt;/h4&gt;MMDrive突破了传统仅图像理解的障碍，能够在复杂驾驶环境中实现强大的多模态推理，为可解释的自动驾驶场景理解提供了新基础。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型通过多源信息融合使复杂交通场景的理解和推理成为可能，使其成为自动驾驶的核心技术。然而，现有的视觉-语言模型受限于二维平面图像理解范式，限制了它们感知3D空间信息和执行深度语义融合的能力，导致在复杂的自动驾驶环境中表现不佳。本研究提出了MMDrive，一个多模态视觉-语言模型框架，将传统的图像理解扩展到广义的3D场景理解框架。MMDrive集成了三种互补模态，包括占用地图、LiDAR点云和文本场景描述。为此，它引入了两个用于自适应跨模态融合和关键信息提取的新组件。具体来说，面向文本的多模态调制器根据问题中的语义线索动态加权每种模态的贡献，引导上下文感知的特征集成。跨模态抽象器使用可学习的抽象令牌生成紧凑的跨模态摘要，突出关键区域和基本语义。在DriveLM和NuScenes-QA基准测试上的综合评估表明，MMDrive在自动驾驶视觉-语言模型方面取得了显著的性能提升，在DriveLM上BLEU-4得分为54.56，METEOR得分为41.78，在NuScenes-QA上准确得分为62.7%。MMDrive有效地突破了传统的仅图像理解的障碍，能够在复杂的驾驶环境中实现强大的多模态推理，为可解释的自动驾驶场景理解提供了新的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶场景中现有视觉-语言模型受限于2D图像理解范式，无法有效处理3D空间信息和进行深度语义融合的问题。这个问题在现实中非常重要，因为自动驾驶环境复杂动态，仅依靠2D视觉表示缺乏必要的3D空间信息和深度感知，无法满足自动驾驶场景理解的精确要求，特别是在复杂环境中的安全决策方面。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有自动驾驶视觉-语言模型的局限性，发现它们主要遵循传统的'图像理解'范式，无法充分利用3D信息和语义先验。作者借鉴了现有的多模态融合技术和视觉-语言模型架构，但针对自动驾驶场景的特殊需求进行了创新。具体来说，作者整合了占用图、LiDAR点云和场景描述三种互补模态，并设计了TMM和CMA两个新组件，实现了动态多模态融合和关键信息提取。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将传统的'图像理解'范式扩展到通用的'场景理解'范式，通过整合多种互补模态(占用图、LiDAR点云和场景描述)来增强场景理解能力。整体实现流程包括：1)多模态信息编码，使用不同编码器处理各类输入；2)面向文本的多模态调制(TMM)，根据问题语义动态调整各模态权重；3)跨模态抽象(CMA)，生成紧凑的跨模态摘要；4)大语言模型处理，将融合后的输入序列生成最终答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)面向文本的多模态调制器(TMM)，根据问题语义动态调整各模态权重；2)跨模态抽象器(CMA)，使用可学习抽象令牌生成紧凑摘要；3)整合占用图、LiDAR点云和场景描述三种互补模态；4)两阶段场景描述生成策略。相比之前的工作，MMDrive突破了仅依赖图像的理解限制，实现了更全面的多模态融合和更高效的信息提取，在复杂驾驶场景中表现出更强的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MMDrive通过整合多种互补模态并引入自适应多模态融合和关键信息提取机制，实现了从传统图像理解到全面场景理解的范式转变，显著提升了自动驾驶场景中视觉-语言模型的性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.</description>
      <author>example@mail.com (Minghui Hou, Wei-Hsing Huang, Shaofeng Liang, Daizong Liu, Tai-Hao Wen, Gang Wang, Runwei Guan, Weiping Ding)</author>
      <guid isPermaLink="false">2512.13177v2</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Audio-Visual Camera Pose Estimation with Passive Scene Sounds and In-the-Wild Video</title>
      <link>http://arxiv.org/abs/2512.12165v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文展示了一种利用被动场景声音作为补充线索来估计相对相机姿态的方法，在视觉条件恶化的情况下表现优于纯视觉方法。&lt;h4&gt;背景&lt;/h4&gt;理解相机运动是具身感知和3D场景理解的基本问题，现有视觉方法在视觉条件恶化时（如运动模糊或遮挡）表现不佳。&lt;h4&gt;目的&lt;/h4&gt;探索被动场景声音作为野外视频相对相机姿态估计的补充线索的可能性。&lt;h4&gt;方法&lt;/h4&gt;引入了一个简单但有效的视听框架，将到达方向(DOA)谱和双耳嵌入集成到最先进的纯视觉姿态估计模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大型数据集上的实验结果显示，该方法相比强大的视觉基线有持续提升，并且在视觉信息被损坏时具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是首次成功利用音频进行真实世界视频中的相对相机姿态估计的工作，确立了日常音频作为解决经典空间挑战的有前景信号。&lt;h4&gt;翻译&lt;/h4&gt;理解相机运动是具身感知和3D场景理解的一个基本问题。虽然视觉方法已经取得了快速发展，但它们在视觉条件恶化的情况下（如运动模糊或遮挡）往往表现不佳。在这项工作中，我们展示了被动场景声音可以为野外视频的相对相机姿态估计提供补充线索。我们引入了一个简单但有效的视听框架，将到达方向(DOA)谱和双耳嵌入集成到最先进的纯视觉姿态估计模型中。我们在两个大型数据集上的结果表明，相比强大的视觉基线有持续提升，并且在视觉信息被损坏时具有鲁棒性。据我们所知，这是首次成功利用音频进行真实世界视频中的相对相机姿态估计的工作，它将偶然的日常音频确立为解决经典空间挑战的一个意外但有前景的信号。项目：http://vision.cs.utexas.edu/projects/av_camera_pose。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何利用被动场景声音（如环境音、对话、音乐等）来增强相机姿态估计的准确性，特别是在真实世界视频和视觉条件差的情况下（如运动模糊、遮挡、低光照）。这个问题很重要，因为相机姿态估计是AR、VR、机器人和自动驾驶等领域的基础任务，而真实世界视频常常面临视觉退化问题，声音可以提供视觉失效时的替代信息，使系统更加鲁棒。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到在视觉条件差的场景（如昏暗的演唱会），声音仍然能提供丰富的空间信息（如音乐随距离变化），因此提出假设：结合视觉和空间音频可以提高姿态估计准确性。他们设计了一个音频-视觉框架，将方向到达谱和双耳化嵌入集成到Reloc3r视觉模型中。他们借鉴了现有工作，包括MUSIC++算法用于DoA估计、NVAS任务用于双耳特征提取，以及SOFA工具箱用于音频双耳化处理，但创新性地将这些技术组合用于相机姿态估计任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用被动场景声音中蕴含的空间信息作为视觉信息的补充，特别是在视觉信息受损时提供可靠的替代线索。整体流程包括：1)空间音频编码器处理多通道音频，提取DoA谱和双耳特征并融合；2)基于Reloc3r的音频-视觉姿态预测器，通过后期融合将音频特征与视觉特征结合；3)使用交叉注意力建立视觉特征对应关系；4)通过回归头预测相机相对姿态。模型训练包括双耳特征提取器训练（通过NVAS任务）和姿态预测器训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次利用被动场景声音进行真实世界视频中的相机姿态估计；2)设计了结合DoA谱和双耳特征的空间音频编码器；3)在真实世界数据集（Ego-Exo4D和HM3D-SS）上验证方法；4)证明了在视觉信息受损情况下的鲁棒性。相比之前工作，不同之处在于：不同于纯视觉方法，添加了音频模态提高鲁棒性；不同于主动回声定位方法，利用已有被动声音而非主动发射信号；不同于声音定位工作，专注于相机姿态估计而非声音源定位；不同于自监督音频-视觉学习，专注于几何空间任务而非语义任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次展示了如何利用真实世界视频中的被动场景声音作为补充线索，显著提高相机姿态估计的准确性和鲁棒性，特别是在视觉信息受损的情况下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.</description>
      <author>example@mail.com (Daniel Adebi, Sagnik Majumder, Kristen Grauman)</author>
      <guid isPermaLink="false">2512.12165v2</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</title>
      <link>http://arxiv.org/abs/2512.14696v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CRISP是一种从单目视频中恢复可模拟人体运动和场景几何的方法，通过拟合平面基本元到场景点云重建中，恢复凸起、干净且可模拟的几何形状，并利用人体-场景接触建模和强化学习确保物理合理性。&lt;h4&gt;背景&lt;/h4&gt;先前关于人体-场景联合重建的工作依赖于数据驱动的先验和无物理循环的联合优化，或者恢复带有噪声的几何形状，这些噪声会导致与场景交互的运动跟踪策略失败。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从单目视频中恢复可模拟人体运动和场景几何的方法，以改进人体与场景交互的模拟效果。&lt;h4&gt;方法&lt;/h4&gt;通过简单的聚类管道（基于深度、法线和流）将平面基本元拟合到场景点云重建中；利用人体-场景接触建模（例如使用人体姿势重建被遮挡的椅子座位）；使用恢复的人体和场景通过强化学习驱动人形控制器，确保重建在物理上是合理的。&lt;h4&gt;主要发现&lt;/h4&gt;在以人为中心的视频基准测试(EMDB, PROX)上，将运动跟踪失败率从55.2%降低到6.9%；RL模拟吞吐量提高了43%；在casually-captured videos、Internet videos和Sora-generated videos等多种类型视频上验证了有效性。&lt;h4&gt;结论&lt;/h4&gt;CRISP能够大规模生成物理有效的人体运动和交互环境，极大地推进了机器人和AR/VR领域的现实到模拟应用。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了CRISP，一种从单目视频中恢复可模拟人体运动和场景几何的方法。先前关于人体-场景联合重建的工作依赖于数据驱动的先验和无物理循环的联合优化，或者恢复带有噪声的几何形状，这些噪声会导致与场景交互的运动跟踪策略失败。相比之下，我们的关键见解是通过将平面基本元拟合到场景的点云重建中，恢复凸起、干净且可模拟的几何形状，这是通过基于深度、法线和流的简单聚类管道实现的。为了重建可能在交互过程中被遮挡的场景几何，我们利用人体-场景接触建模（例如，我们使用人体姿势来重建被遮挡的椅子座位）。最后，我们通过使用恢复的人体和场景通过强化学习驱动人形控制器，确保人体和场景重建在物理上是合理的。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从单目视频中恢复可模拟的人体运动和场景几何的问题。这个问题在现实中很重要，因为它能帮助我们将普通拍摄的视频转换为可用于物理模拟的3D资产，从而推动机器人学习、物理上合理的角色动画和AR/VR应用的发展。现有方法在处理人体与场景交互时往往产生噪声和伪影，导致运动跟踪策略失败，而精确的物理模拟需要干净、凸起的几何形状。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法在处理人体-场景交互时的局限性，包括依赖数据驱动的先验但没有物理循环，以及重建的几何形状有噪声和伪影。他们的设计思路是使用平面基元拟合点云重建场景，利用人体-场景接触建模重建被遮挡的场景部分，并通过强化学习确保物理合理性。他们借鉴了多项现有工作，包括使用MegaSAM进行相机姿态估计、GVHMR进行人体姿态估计、InteractVLM进行接触预测，以及参考Peng等人(2018)和MaskedMimic的强化学习运动跟踪方法，但将这些技术以创新方式组合起来。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用平面基元表示场景几何，利用人体-场景接触信息重建被遮挡场景部分，并通过物理模拟验证重建结果。整体流程包括：1)初始化阶段使用MegaSAM估计相机参数和深度图，GVHMR估计人体姿态；2)平面基元拟合阶段通过法线计算、K-means聚类、DBSCAN空间聚类、跨帧关联和RANSAC平面拟合将场景分解为约50个凸平面基元；3)接触引导场景重建阶段使用InteractVLM预测接触并应用时间-运动学过滤；4)物理模拟运动跟踪阶段使用强化学习训练策略使模拟人体精确跟踪参考动作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用平面基元而非密集网格表示场景几何；2)利用人体-场景接触信息重建被遮挡场景部分；3)通过强化学习验证重建结果的物理合理性；4)端到端的从单目视频到可模拟资产的完整流程。相比之前的工作(如VideoMimic)，CRISP使用更高效且适合物理模拟的平面基元而非密集网格，明确利用接触信息重建场景，并通过强化学习验证物理合理性，实现了显著更好的性能(运动跟踪失败率从55.2%降至6.9%，RL模拟吞吐量提高43%)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CRISP通过平面基元表示和接触引导重建，实现了从单目视频中恢复物理上合理的人体运动和场景几何，大大提高了真实到模拟的转换质量和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.</description>
      <author>example@mail.com (Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins, Shubham Tulsiani, Deva Ramanan)</author>
      <guid isPermaLink="false">2512.14696v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning</title>
      <link>http://arxiv.org/abs/2512.14274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为拓扑理解网络(TUN)的多模态网络，用于自动检测一维持久性图中的重要点，解决了拓扑数据分析在实际应用中的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;持久性图(PDs)是理解点云底层形状拓扑的强大工具，但识别哪些点编码真实信号仍然具有挑战性，这阻碍了拓扑数据分析在许多需要自动化可靠解释的下游决策应用中的采用。&lt;h4&gt;目的&lt;/h4&gt;研究一维持久性图的自动显著性检测，提供一种自动化解决方案来识别持久性图中的重要点。&lt;h4&gt;方法&lt;/h4&gt;提出拓扑理解网络(TUN)，一种多模态网络，结合了增强的PD描述符、自注意力机制、类似PointNet的点云编码器、学习融合、每点分类、稳定的预处理和不平衡感知训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明TUN在检测持久性图中的重要点方面优于经典方法，在实际应用中展现出有效性。&lt;h4&gt;结论&lt;/h4&gt;TUN为持久性图中重要点的识别提供了自动化且有效的解决方案，这些重要点对下游应用至关重要。&lt;h4&gt;翻译&lt;/h4&gt;持久性图(PDs)为理解点云底层形状的拓扑提供了强大工具。然而，识别PDs中哪些点编码真实信号仍然具有挑战性。这一挑战直接阻碍了拓扑数据分析在许多应用中的实际采用，在这些应用中，持久性图的自动化和可靠解释对下游决策至关重要。在本文中，我们研究了一维持久性图的自动显著性检测。具体来说，我们提出了拓扑理解网络(TUN)，这是一种多模态网络，结合了增强的PD描述符、自注意力、类似PointNet的点云编码器、学习融合和每点分类，以及稳定的预处理和不平衡感知训练。它为识别PDs中的重要点提供了自动化且有效的解决方案，这些点对下游应用至关重要。实验表明，TUN在检测PDs中的重要点方面优于经典方法，展示了其在实际应用中的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在持久同调图（PDs）中准确识别哪些点代表了真正重要的拓扑特征（显著点）的问题。这个问题很重要，因为PDs是理解点云数据底层拓扑结构的强大工具，广泛应用于机器人、医学影像和计算生物学等领域。准确识别显著点对下游任务（如属性预测和控制）至关重要，而传统方法仅基于持久性判断，无法准确区分重要特征和噪声，阻碍了拓扑数据分析在实际应用中的有效使用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析传统方法（如基于持久性的阈值处理和聚类方法）的局限性，发现它们忽略了拓扑特征与形状拓扑不变量之间的联系。作者设计了一个多模态深度学习框架TUN，将拓扑理解转化为逐点分类问题。该方法借鉴了PointNet风格的点云编码器、自注意力机制以及现有的PD处理方法（如PersLay和Persformer）的思想，但创新性地将这些技术结合起来，同时利用PD的拓扑信息和原始点云的几何信息来解决显著点识别问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态深度学习框架，同时利用持久同调图的拓扑信息和原始点云的几何信息，准确识别PD中的显著点。整体流程包括：1) 输入特征提取和处理（增强PD特征、原始点云数据和14维辅助特征）；2) 模型架构（PD编码器、点云编码器、多模态融合模块和显著性分类器）；3) 使用加权Focal Loss处理类别不平衡问题。这种方法突破了传统方法的局限，不仅考虑持久性，还考虑了绝对时间、点间关系和原始几何特性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 构建了第一个用于监督学习1维PD中拓扑特征显著性标记的大规模数据集；2) 提出了TUN，第一个专门用于PD自动化拓扑理解的深度学习框架；3) 设计了增强的PD描述符和全面的辅助特征；4) 采用加权Focal Loss处理类别不平衡。相比传统方法（如2-means聚类和置信集方法），TUN不仅考虑持久性，还整合了几何上下文；相比其他PD处理方法（如PersLay和Persformer），TUN是首个专注于显著点检测的深度学习方法，同时考虑了拓扑和几何信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了拓扑理解网络（TUN），一种多模态深度学习框架，通过融合持久同调图的拓扑信息和原始点云的几何信息，实现了对持久同调图中显著点的自动化准确识别，解决了拓扑数据分析中的一个关键瓶颈问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.</description>
      <author>example@mail.com (Yu Chen, Hongwei Lin)</author>
      <guid isPermaLink="false">2512.14274v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2512.14235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为4D-RaDiff的新框架，用于生成4D雷达点云数据，解决了雷达标注数据有限的问题。该框架通过扩散模型处理潜在点云表示，能够在对象或场景级别进行条件控制，将未标注边界框转换为高质量雷达标注，并将LiDAR点云转换为逼真雷达场景。&lt;h4&gt;背景&lt;/h4&gt;汽车雷达因其成本效益和在恶劣天气条件下的鲁棒性，在环境感知方面展现出良好发展前景。然而，标注雷达数据的有限性对基于雷达的感知系统发展构成了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架来生成4D雷达点云，用于训练和评估目标检测器，以解决标注雷达数据不足的问题。&lt;h4&gt;方法&lt;/h4&gt;4D-RaDiff方法不同于基于图像的扩散，它专门考虑雷达点云的稀疏性和独特特性，通过将扩散应用于潜在点云表示来生成数据。在潜在空间中，生成可通过对象或场景级别的条件控制。该方法能够将未标注的边界框转换为高质量雷达标注，并将现有LiDAR点云数据转换为逼真雷达场景。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，将4D-RaDiff生成的合成雷达数据作为训练时的数据增强方法，相比仅使用真实数据训练，能持续提高目标检测性能。此外，在合成数据上进行预训练可将所需的标注雷达数据量减少高达90%，同时实现相当的目标检测性能。&lt;h4&gt;结论&lt;/h4&gt;4D-RaDiff框架有效解决了雷达数据标注有限的问题，通过生成高质量合成数据增强目标检测器性能，同时大幅减少对真实标注数据的需求。&lt;h4&gt;翻译&lt;/h4&gt;汽车雷达由于其成本效益和在恶劣天气条件下的鲁棒性，在环境感知方面展现出良好的发展前景。然而，标注雷达数据的有限性对推进基于雷达的感知系统构成了重大挑战。为解决这一限制，我们提出了一种新框架来生成4D雷达点云，用于训练和评估目标检测器。与基于图像的扩散不同，我们的方法通过将扩散应用于潜在点云表示，专门考虑了雷达点云的稀疏性和独特特性。在此潜在空间中，生成可通过对象或场景级别的条件控制。所提出的4D-RaDiff能将未标注的边界框转换为高质量的雷达标注，并将现有的LiDAR点云数据转换为逼真的雷达场景。实验证明，在训练中将4D-RaDiff的合成雷达数据作为数据增强方法，与仅使用真实数据训练相比，能持续提高目标检测性能。此外，在我们的合成数据上进行预训练，可在实现相当目标检测性能的同时，将所需的标注雷达数据量减少高达90%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D雷达点云数据稀缺的问题。在自动驾驶领域，雷达传感器在恶劣天气条件下比相机和LiDAR更可靠，但获取标注好的雷达数据非常困难、耗时且成本高昂。这种数据 scarcity 限制了基于雷达的感知系统的发展，因此生成高质量的合成雷达数据对推动自动驾驶技术进步至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了雷达数据的独特特性（稀疏性和不规则分布），然后借鉴了多个现有工作：1) 潜在扩散模型(LDMs)的概念，将扩散过程应用于低维潜在空间以提高效率；2) 点云处理中的变分自编码器(VAE)技术，用于将雷达点云映射到规则化的潜在空间；3) LayoutDiffusion中的边界框编码方法，并将其从2D扩展到3D；4) PointPillars用于将LiDAR点云转换为中间表示。作者结合这些技术，针对雷达数据的特殊性进行了创新设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在潜在点云空间而非原始点云空间或图像空间中应用扩散模型，同时分离前景和背景的生成过程。整体流程分为三步：1) 首先训练一个点基础的VAE将雷达点云编码到规则化的潜在点云空间；2) 然后在潜在表示上训练扩散模型，前景生成以3D边界框为条件，背景生成以LiDAR点云为条件；3) 最后将生成的背景和前景点合并，形成完整的合成4D雷达点云，包含Doppler和RCS特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出首个直接在点基础雷达表示上操作的潜在扩散框架；2) 引入前景-背景分离生成流程；3) 同时生成雷达特有的Doppler和RCS特征。相比之前工作，不同之处在于：与基于图像的扩散方法不同，考虑了雷达点云的稀疏特性；与直接在原始点云上扩散的方法不同，在潜在空间中操作解决了不规则分布问题；与仅生成空间特征的雷达模型不同，完整保留了雷达特性；与需要雷达数据作为条件的方法不同，能从头生成全新雷达数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 4D-RaDiff通过创新的潜在扩散模型生成了高质量的4D雷达点云，解决了数据稀缺问题，可将标注数据需求减少90%同时提升3D对象检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.</description>
      <author>example@mail.com (Jimmie Kwok, Holger Caesar, Andras Palffy)</author>
      <guid isPermaLink="false">2512.14235v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth</title>
      <link>http://arxiv.org/abs/2512.14001v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CLAIM，一种新颖的相机-激光雷达数据对齐方法，利用单深度模型潜力，通过从粗到细的搜索方法，结合基于补丁皮尔逊相关的结构损失和基于互信息的纹理损失，实现高效对齐，无需复杂的数据处理、特征提取或特征匹配步骤。&lt;h4&gt;背景&lt;/h4&gt;相机-激光雷达校准是计算机视觉和自动驾驶领域的重要问题，现有方法通常需要复杂的数据处理、特征提取或特征匹配步骤，限制了其应用场景和适应性。&lt;h4&gt;目的&lt;/h4&gt;释放单深度模型在相机-激光雷达校准中的潜力，提出一种简单、适应性强且高性能的对齐方法。&lt;h4&gt;方法&lt;/h4&gt;CLAIM方法利用初始猜测和图像-激光雷达点云对，采用从粗到细的搜索策略，寻找最小化两种损失的最优变换：基于补丁皮尔逊相关的结构损失和基于互信息的纹理损失，这两种损失作为对齐结果的度量指标。&lt;h4&gt;主要发现&lt;/h4&gt;CLAIM方法在KITTI、Waymo和MIAS-LCEC等公开数据集上表现出色，与最先进的方法相比具有更好的性能，且方法简单适应性强，适用于大多数场景。&lt;h4&gt;结论&lt;/h4&gt;CLAIM是一种有效的相机-激光雷达数据对齐方法，通过创新的损失函数和搜索策略，实现了简单、适应性强且高性能的对齐效果。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们释放了强大的单深度模型在相机-激光雷达校准中的潜力，并提出了CLAIM，一种新颖的相机和激光雷达数据对齐方法。给定初始猜测和图像与激光雷达点云对，CLAIM利用从粗到细的搜索方法，寻找最小化基于补丁皮尔逊相关的结构损失和基于互信息的纹理损失的最优变换。这两种损失作为相机-激光雷达对齐结果的良好指标，不需要像大多数方法那样复杂的数据处理、特征提取或特征匹配步骤，使我们的方法简单且适应大多数场景。我们在公开的KITTI、Waymo和MIAS-LCEC数据集上验证了CLAIM，实验结果表明其性能优于最先进的方法。代码可在https://github.com/Tompson11/claim获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.</description>
      <author>example@mail.com (Zhuo Zhang, Yonghui Liu, Meijie Zhang, Feiyang Tan, Yikang Ding)</author>
      <guid isPermaLink="false">2512.14001v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Repurposing 2D Diffusion Models for 3D Shape Completion</title>
      <link>http://arxiv.org/abs/2512.13991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种框架，将二维扩散模型适应为从点云完成三维形状的任务，通过引入Shape Atlas这一紧凑的二维表示方法，解决了三维扩散模型面临的数据稀缺和模态差距问题。&lt;h4&gt;背景&lt;/h4&gt;文本到图像的扩散模型在丰富的二维数据上取得了显著成功，但三维扩散模型由于高质量三维数据集的稀缺以及三维输入与二维潜在空间之间持续的模态差距而发展滞后。&lt;h4&gt;目的&lt;/h4&gt;克服三维扩散模型面临的限制，使二维扩散模型能够有效地用于三维形状完成，并从有限的3D数据中学习，生成高质量、细节保留的形状完成。&lt;h4&gt;方法&lt;/h4&gt;引入Shape Atlas，这是一种3D几何的紧凑2D表示，它能够充分利用预训练2D扩散模型的生成能力，并对齐条件输入和输出空间之间的模态，实现更有效的条件设置。这种统一的2D表述使得从有限的3D数据中学习成为可能。&lt;h4&gt;主要发现&lt;/h4&gt;在PCN和ShapeNet-55数据集上验证了该方法的有效性，能够生成高质量、细节保留的形状完成。此外，展示了从完成的点云创建艺术家创作的网格模型的下游应用。&lt;h4&gt;结论&lt;/h4&gt;该方法通过统一的2D表述解决了3D扩散模型面临的数据稀缺和模态差距问题，能够从有限的3D数据中学习并生成高质量的3D形状完成，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种框架，该框架将二维扩散模型适应为从点云完成三维形状。虽然文本到图像的扩散模型在丰富的二维数据上取得了显著成功，但由于高质量三维数据集的稀缺以及三维输入与二维潜在空间之间持续的模态差距，三维扩散模型的发展相对滞后。为了克服这些限制，我们引入了Shape Atlas，这是一种三维几何的紧凑二维表示，它(1)能够充分利用预训练二维扩散模型的生成能力，以及(2)对齐条件输入和输出空间之间的模态，实现更有效的条件设置。这种统一的二维表述使得从有限的3D数据中学习成为可能，并产生高质量、细节保留的形状完成。我们在PCN和ShapeNet-55数据集上验证了我们结果的有效性。此外，我们展示了从完成的点云创建艺术家创作的网格模型的下游应用，进一步证明了我们方法的实用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用2D扩散模型来完成3D形状补全的问题，即从不完整的点云恢复完整的3D几何形状。这个问题在现实中非常重要，因为传感器（如LiDAR或RGBD相机）捕获的数据常因遮挡、有限视角和噪声而不完整，而可靠的形状补全是自动驾驶、机器人和AR/VR等应用的关键技术。在研究中，这一挑战也很重要，因为3D扩散模型因高质量3D数据集稀缺和3D与2D空间间的模态差距而发展滞后，无法达到2D扩散模型的性能水平。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到2D扩散模型在图像生成上成功但3D扩散模型受限的问题。他们思考如何利用2D模型的强大能力来解决3D问题，决定将3D形状补全重新表述为2D生成问题。他们借鉴了现有的3D到2D转换方法（特别是球面偏移和平面偏移技术），并采用了预训练的2D扩散模型（如Stable Diffusion）和ControlNet风格的条件控制方法。创新之处在于设计了Shape Atlas作为3D几何的紧凑2D表示，优化了球面偏移算法效率，并引入了条件U-Net和去噪U-Net的双网络架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D形状补全转化为2D生成问题，使用Shape Atlas作为3D几何的紧凑2D表示，解决3D与2D间的模态差距，并利用预训练2D扩散模型的强大生成能力。整体流程包括：1) 将3D点云转换为2D Shape Atlas（通过球面偏移和平面偏移）；2) 训练条件扩散模型（包括条件U-Net和去噪U-Net）；3) 使用扩散模型从不完整Shape Atlas生成完整Shape Atlas；4) 将完整Shape Atlas转换回3D点云。训练过程中结合了扩散损失和多种3D重建损失（如Chamfer距离、InfoCD和网格损失）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出Shape Atlas作为3D几何的紧凑2D表示，保留拓扑信息；2) 优化球面偏移算法，显著降低计算复杂度；3) 设计条件扩散模型的双网络架构；4) 结合多种损失函数提高生成质量。相比之前的工作，这种方法解决了3D与2D间的模态不一致问题，无需额外2D-3D融合模块，更有效利用2D扩散模型的能力，并支持从部分观察的条件生成，而不仅仅是无条件生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过引入Shape Atlas作为3D几何的2D表示，成功地将预训练的2D扩散模型重新用于高质量的3D形状补全，解决了3D扩散模型因数据稀缺和模态差距而发展受限的问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a framework that adapts 2D diffusion models for 3D shape completion from incomplete point clouds. While text-to-image diffusion models have achieved remarkable success with abundant 2D data, 3D diffusion models lag due to the scarcity of high-quality 3D datasets and a persistent modality gap between 3D inputs and 2D latent spaces. To overcome these limitations, we introduce the Shape Atlas, a compact 2D representation of 3D geometry that (1) enables full utilization of the generative power of pretrained 2D diffusion models, and (2) aligns the modalities between the conditional input and output spaces, allowing more effective conditioning. This unified 2D formulation facilitates learning from limited 3D data and produces high-quality, detail-preserving shape completions. We validate the effectiveness of our results on the PCN and ShapeNet-55 datasets. Additionally, we show the downstream application of creating artist-created meshes from our completed point clouds, further demonstrating the practicality of our method.</description>
      <author>example@mail.com (Yao He, Youngjoong Kwon, Tiange Xiang, Wenxiao Cai, Ehsan Adeli)</author>
      <guid isPermaLink="false">2512.13991v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2512.14698v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://timelens-arc-lab.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TimeLens，一个系统性研究多模态大语言模型(MLLMs)在视频时序定位(VTG)上表现的框架，从数据质量和算法设计两个维度进行优化，建立了高质量基准数据集和有效的训练方法，实现了超越专有模型的性能。&lt;h4&gt;背景&lt;/h4&gt;视频时序定位(VTG)是视频理解的核心能力，虽然多模态大语言模型在多种视频理解任务上表现出色，但针对VTG的优化方法研究不足。现有VTG基准存在质量问题，导致评估不可靠。&lt;h4&gt;目的&lt;/h4&gt;系统性地研究构建具有强大VTG能力的MLLMs，从数据质量和算法设计两个主要维度进行探索，建立可靠的评估标准和高效的训练方法。&lt;h4&gt;方法&lt;/h4&gt;1) 揭示现有VTG基准质量问题，创建TimeLens-Bench（三个流行基准的严格质量标准重新注释版本）；2) 通过自动重新注释管道创建TimeLens-100K大规模高质量训练数据集；3) 探索算法设计原则，包括交错文本编码、可验证奖励的强化学习(RLVR)训练范式和精心设计的RLVR训练方法。&lt;h4&gt;主要发现&lt;/h4&gt;与传统基准相比，模型排名发生显著变化，证实了先前评估标准的不可靠性；提出了一系列有意义的见解和有效但高效的实践方法，如交错文本编码和RLVR训练方法。&lt;h4&gt;结论&lt;/h4&gt;开发的TimeLens模型系列在开源模型中实现了最先进的VTG性能，甚至超过了专有模型如GPT-5和Gemini-2.5-Flash；所有代码、数据和模型将公开以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;本文并未引入新方法，而是为视频时序定位(VTG)建立了一个简单、增量但重要的基线，这是视频理解的核心能力。虽然多模态大语言模型(MLLMs)在多种视频理解任务上表现出色，但针对VTG的优化方法研究不足。在本文中，我们提出TimeLens，一个系统性研究构建具有强大VTG能力的MLLMs的框架，沿着两个主要维度：数据质量和算法设计。我们首先揭示了现有VTG基准中的关键质量问题，并引入TimeLens-Bench，包含三个流行基准的严格质量标准的精心重新注释版本。我们的分析显示与传统基准相比模型排名发生显著变化，证实了先前评估标准的不可靠性。我们还通过自动重新注释管道解决了嘈杂的训练数据问题，创建了TimeLens-100K，这是一个大规模、高质量的训练数据集。基于我们的数据基础，我们对算法设计原则进行了深入探索，得出了一系列有意义的见解和有效但高效的实践方法。这些包括用于时间表示的交错文本编码，一种可验证奖励的强化学习(RLVR)方法作为训练范式，以及精心设计的RLVR训练方法。这些努力最终促成了TimeLens模型系列，这是一组在开源模型中具有最先进VTG性能的MLLMs，甚至超过了GPT-5和Gemini-2.5-Flash等专有模型。所有代码、数据和模型都将被发布以促进未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.</description>
      <author>example@mail.com (Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang)</author>
      <guid isPermaLink="false">2512.14698v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics</title>
      <link>http://arxiv.org/abs/2512.14471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队引入了Kinetic-Mamba，一种基于Mamba的神经算子框架，用于化学动力学建模，该框架结合了神经算子的表达能力和Mamba架构的高效时间建模能力，能够准确预测复杂动力学行为。&lt;h4&gt;背景&lt;/h4&gt;准确的化学动力学建模对燃烧模拟至关重要，因为它控制着复杂反应路径和热化学状态的演化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测化学动力学行为的神经网络框架，仅使用状态变量的初始条件即可实现高保真度预测。&lt;h4&gt;方法&lt;/h4&gt;该框架包含三个互补模型：(i)独立Mamba模型预测热化学状态变量的时间演化；(ii)约束Mamba模型强制质量守恒；(iii)基于区域的架构使用两个Mamba模型捕获温度相关区域的动态。还开发了潜在Kinetic-Mamba变体，在简化空间中演化动态并重建完整状态。通过时间分解和递归预测策略评估模型，并在不同分布外数据集上测试外推能力。&lt;h4&gt;主要发现&lt;/h4&gt;在Syngas和GRI-Mech 3.0反应机制上的计算实验表明，Kinetic-Mamba框架仅使用状态变量的初始条件就能在预测复杂动力学行为方面实现高保真度。&lt;h4&gt;结论&lt;/h4&gt;Kinetic-Mamba框架能够准确预测化学动力学行为，为燃烧模拟提供了有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;准确的化学动力学建模对燃烧模拟至关重要，因为它控制着复杂反应路径和热化学状态的演化。在这项工作中，我们引入了Kinetic-Mamba，一种基于Mamba的神经算子框架，它结合了神经算子的表达能力和Mamba架构的高效时间建模能力。该框架包含三个互补模型：(i)一个独立Mamba模型，根据给定初始条件预测热化学状态变量的时间演化；(ii)一个约束Mamba模型，在学习状态动态的同时强制执行质量守恒；以及(iii)一个基于区域的架构，采用两个独立Mamba模型来捕获温度相关区域的动态。我们还开发了一个潜在Kinetic-Mamba变体，在简化的潜在空间中演化动态，并在物理流形上重建完整状态。我们使用时间分解和递归预测策略评估了Kinetic-Mamba的准确性和鲁棒性。我们进一步评估了模型在不同分布外数据集上的外推能力。在Syngas和GRI-Mech 3.0反应机制上的计算实验表明，我们的框架仅使用状态变量的初始条件就能在预测复杂动力学行为方面实现高保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.</description>
      <author>example@mail.com (Additi Pandey, Liang Wei, Hessam Babaee, George Em Karniadakis)</author>
      <guid isPermaLink="false">2512.14471v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in</title>
      <link>http://arxiv.org/abs/2512.14273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://xiaoqian-shen.github.io/Zoom-Zero/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Zoom-Zero框架，解决大型视频语言模型在时间感知方面的局限性，通过从粗到细的方法提高视频问答任务中的时间定位准确性并减少幻觉。&lt;h4&gt;背景&lt;/h4&gt;Grounded video question answering (GVQA)旨在定位视频中相关时间段并生成准确答案，但大型视频语言模型(LVLMs)存在时间感知有限的问题。基于Group Relative Policy Optimization (GRPO)的方法虽试图改进时间定位，但仍难以忠实地将答案定位到相关视频证据，导致时间错位和幻觉。&lt;h4&gt;目的&lt;/h4&gt;解决GRPO在GVQA任务中的局限性，提高时间定位准确性，减少幻觉现象，增强长视频理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出Zoom-Zero框架，采用从粗到细的方法：首先定位查询相关时间段，然后时间放大到最显著帧进行精细视觉验证。包含两个关键创新：(1)放大精度奖励，验证时间定位预测保真度；(2)令牌选择性信用分配，将奖励归因于负责时间定位或答案生成的令牌。&lt;h4&gt;主要发现&lt;/h4&gt;Zoom-Zero在NExT-GQA上提高时间定位5.2%，在ReXTime上提高4.6%，平均答案准确率提高2.4%。在推理过程中的从粗到细放大方法在长视频基准测试上平均提高6.4%。&lt;h4&gt;结论&lt;/h4&gt;Zoom-Zero方法显著改进了基于视频的问答任务，通过从粗到细的放大方法能够在保持全局上下文的同时保留关键视觉细节，有效提升了时间定位和答案生成的准确性。&lt;h4&gt;翻译&lt;/h4&gt;Grounded video question answering (GVQA)旨在定位视频中相关的时间段并针对给定问题生成准确答案；然而，大型视频语言模型(LVLMs)表现出有限的时间感知能力。虽然基于Group Relative Policy Optimization (GRPO)的现有方法试图提高时间定位，它们仍然难以将答案忠实地定位到相关视频证据中，导致时间错位和幻觉。在这项工作中，我们提出了Zoom-Zero，一个从粗到细的框架，首先定位查询相关的时间段，然后时间放大到最显著的帧进行更精细的视觉验证。我们的方法通过两个关键创新解决了GRPO在GVQA任务中的局限性：(i)放大精度奖励，验证时间定位预测的保真度并促进对已定位帧的细粒度视觉验证；(ii)令牌选择性信用分配，将奖励归因于负责时间定位或答案生成的令牌，减轻GRPO处理多面奖励信号的问题。我们提出的方法推进了基于视频的问答任务，在NExT-GQA上提高时间定位5.2%，在ReXTime上提高4.6%，同时平均答案准确率提高2.4%。此外，推理过程中的从粗到细放大方法通过在不损害全局上下文的情况下保留关键视觉细节，进一步受益于长视频理解，在长视频基准测试上平均提高6.4%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.</description>
      <author>example@mail.com (Xiaoqian Shen, Min-Hung Chen, Yu-Chiang Frank Wang, Mohamed Elhoseiny, Ryo Hachiuma)</author>
      <guid isPermaLink="false">2512.14273v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding</title>
      <link>http://arxiv.org/abs/2512.14017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  WACV2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了KFS-Bench，首个针对长视频问答中关键帧采样的基准测试，具有多场景注释功能，能够直接评估采样策略。研究发现采样精度、场景覆盖和采样平衡都是影响问答性能的关键因素，并提出了自适应平衡采样方法。&lt;h4&gt;背景&lt;/h4&gt;关键帧采样对高效的长视频理解至关重要。在长视频问答中，选择信息丰富的帧可以使多模态大语言模型提高准确性和效率。先前的工作仅通过问答准确率间接评估帧选择质量，存在局限性。&lt;h4&gt;目的&lt;/h4&gt;创建一个基准测试来直接评估长视频问答中的关键帧采样策略，解决先前工作只能间接评估帧选择质量的问题。&lt;h4&gt;方法&lt;/h4&gt;提供每个问题所需的多不重叠场景的真实注释；设计了一个新的采样质量指标，与问答准确率相关；开发了一种新的关键帧采样方法，利用问题-视频相关性来平衡采样多样性与问题-帧相似性，提高相关场景的覆盖范围。&lt;h4&gt;主要发现&lt;/h4&gt;采样精度不是唯一影响因素；场景覆盖和采样平衡也是影响问答性能的关键因素；自适应平衡采样方法在关键帧采样和问答性能方面都取得了优异的效果。&lt;h4&gt;结论&lt;/h4&gt;KFS-Bench基准测试为直接评估长视频问答中的关键帧采样策略提供了新工具，通过多场景注释实现了更直接和稳健的评估。研究强调了采样质量不仅仅是精度问题，还包括场景覆盖和平衡性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了KFS-Bench，这是首个针对长视频问答中关键帧采样的基准测试，具有多场景注释功能，能够直接且稳健地评估采样策略。关键帧采样对高效的长视频理解至关重要。在长视频问答中，选择信息丰富的帧可以使多模态大语言模型提高准确性和效率。KFS-Bench解决了先前工作仅通过问答准确率间接评估帧选择质量的局限性。通过提供每个问题所需的多不重叠场景的真实注释，KFS-Bench使我们能够直接分析不同采样方法如何捕捉整个长视频中的基本内容。使用KFS-Bench，我们对关键帧采样方法进行了全面研究，并发现不仅采样精度，而且场景覆盖和采样平衡都是影响问答性能的关键因素。关于所有这些因素，我们设计了一个与问答准确率相关的新型采样质量指标。此外，我们开发了一种新的关键帧采样方法，利用问题-视频相关性来平衡采样多样性与问题-帧相似性，从而提高相关场景的覆盖范围。我们的自适应平衡采样方法在关键帧采样和问答性能方面都取得了优异的效果。该基准测试可在https://github.com/NEC-VID/KFS-Bench获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.</description>
      <author>example@mail.com (Zongyao Li, Kengo Ishida, Satoshi Yamazaki, Xiaotong Ji, Jianquan Liu)</author>
      <guid isPermaLink="false">2512.14017v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos</title>
      <link>http://arxiv.org/abs/2512.14601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FakeRadar，一种新型深度伪造视频检测框架，旨在解决现实场景中跨领域泛化的挑战。&lt;h4&gt;背景&lt;/h4&gt;现有检测方法通常依赖特定篡改线索，在已知伪造类型上表现良好，但对新兴篡改技术表现出严重局限性，无法有效适应未见过的篡改模式。&lt;h4&gt;目的&lt;/h4&gt;利用大规模预训练模型（如CLIP）主动探测特征空间，明确显示真实视频、已知伪造品和未见操纵之间的分布差距。&lt;h4&gt;方法&lt;/h4&gt;FakeRadar引入了'伪造异常值探测'，采用动态子聚类建模和聚类条件异常值生成来合成估计子聚类边界附近的异常样本；设计了'异常值引导的三重训练'，使用异常值驱动的对比学习和异常值条件的交叉熵损失来优化检测器。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在各种深度伪造视频检测基准数据集上，FakeRadar都优于现有方法，特别是在跨领域评估中，通过处理各种新兴的操纵技术。&lt;h4&gt;结论&lt;/h4&gt;FakeRadar通过主动探测特征空间和生成异常样本，有效解决了深度伪造视频检测中的跨领域泛化问题。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了FakeRadar，一种新型深度伪造视频检测框架，旨在解决现实场景中跨领域泛化的挑战。现有的检测方法通常依赖于特定的篡改线索，在已知的伪造类型上表现良好，但对新兴的篡改技术表现出严重的局限性。这种泛化能力差的原因是它们无法有效适应未见过的篡改模式。为了克服这个问题，我们利用大规模预训练模型（如CLIP）来主动探测特征空间，明确显示真实视频、已知伪造品和未见操纵之间的分布差距。具体来说，FakeRadar引入了'伪造异常值探测'，它采用动态子聚类建模和聚类条件异常值生成来合成估计子聚类边界附近的异常样本，模拟已知操纵类型之外的新型伪造伪影。此外，我们设计了'异常值引导的三重训练'，它使用提出的异常值驱动的对比学习和异常值条件的交叉熵损失来优化检测器，以区分真实、伪造和异常样本。实验表明，在各种深度伪造视频检测的基准数据集上，FakeRadar都优于现有方法，特别是在跨领域评估中，通过处理各种新兴的操纵技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.</description>
      <author>example@mail.com (Zhaolun Li, Jichang Li, Yinqi Cai, Junye Chen, Xiaonan Luo, Guanbin Li, Rushi Lan)</author>
      <guid isPermaLink="false">2512.14601v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>SuperCLIP: CLIP with Simple Classification Supervision</title>
      <link>http://arxiv.org/abs/2512.14480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025. Code: https://github.com/hustvl/SuperCLIP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SuperCLIP通过在对比学习中添加基于分类的监督，解决了CLIP模型未能充分利用文本细粒度语义信号的问题，显著提升了视觉-文本对齐能力。&lt;h4&gt;背景&lt;/h4&gt;CLIP模型在视觉语言任务中表现出强大的泛化能力，但研究表明CLIP类模型未能充分利用文本中的细粒度语义信号，尤其是在处理长而详细的描述时。&lt;h4&gt;目的&lt;/h4&gt;解决CLIP模型在细粒度视觉-文本对齐方面的局限性，提高模型在视觉语言任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出SuperCLIP框架，在视觉编码器中添加轻量级线性层，利用标记级别线索增强视觉-文本对齐，仅需增加0.077%的总FLOPs，不需要额外标注数据。&lt;h4&gt;主要发现&lt;/h4&gt;SuperCLIP在零样本分类、图像-文本检索和纯视觉任务中都有持续改进；这种改进在使用原始网络数据或丰富的重新描述数据进行训练时都成立；SuperCLIP减轻了CLIP在小批量训练中的性能下降问题。&lt;h4&gt;结论&lt;/h4&gt;SuperCLIP是一个简单而有效的框架，能够增强CLIP模型在视觉语言任务中的性能，特别是在细粒度语义对齐方面。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-图像预训练（CLIP）通过在共享嵌入空间中对齐图像和文本，在视觉语言任务中实现了强大的泛化能力。然而，最近的研究表明，CLIP类模型仍然未能充分利用文本中的细粒度语义信号，当处理长而详细的描述时，这一问题变得更加明显。这源于CLIP的训练目标，它只优化全局图像-文本相似性，而忽略了标记级别的监督，限制了其实现细粒度视觉-文本对齐的能力。为解决这一问题，我们提出了SuperCLIP，一个简单而有效的框架，通过基于分类的监督增强对比学习。通过仅在视觉编码器中添加一个轻量级线性层，SuperCLIP利用标记级别线索来增强视觉-文本对齐，仅增加了0.077%的总FLOPs，且不需要额外的标注数据。实验表明，SuperCLIP在零样本分类、图像-文本检索和纯视觉任务中都有持续改进。这些改进在模型使用原始网络数据或丰富的重新描述数据进行训练时都成立，证明了SuperCLIP在两种情况下都能恢复文本监督的能力。此外，SuperCLIP通过基于分类的监督减轻了CLIP在小批量训练中的性能下降问题。代码和模型将开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.</description>
      <author>example@mail.com (Weiheng Zhao, Zilong Huang, Jiashi Feng, Xinggang Wang)</author>
      <guid isPermaLink="false">2512.14480v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition</title>
      <link>http://arxiv.org/abs/2512.14309v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PSMamba是一种渐进式自监督框架，结合Vision Mamba的高效序列建模和双学生层次蒸馏策略，有效捕捉植物病害图像的层次化、多尺度病变模式，在三个基准数据集上表现优于最先进的SSL方法。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为无标注表示学习的强大范式，但现有框架主要关注全局对齐，难以捕捉植物病害图像特有的层次化、多尺度病变模式。&lt;h4&gt;目的&lt;/h4&gt;提出PSMamba框架，解决现有SSL方法在捕捉植物病害图像层次化、多尺度病变模式方面的不足，提高在领域迁移和细粒度场景中的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;PSMamba采用共享的全局教师和两个专门的学生：一个处理中等尺度视图捕捉病变分布和静脉结构，另一个专注于局部视图捕捉纹理不规则性和早期病变等细粒度线索，通过多粒度监督和一致性损失实现跨尺度对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集上的实验表明，PSMamba始终优于最先进的SSL方法，在领域迁移和细粒度场景中提供更高的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;PSMamba通过整合Vision Mamba的高效序列建模和双学生层次蒸馏策略，有效解决了植物病害图像中层次化、多尺度病变模式捕捉的问题，提升了自监督学习在植物病害识别中的性能。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为一种无需人工标注的强大表征学习范式。然而，大多数现有框架专注于全局对齐，难以捕捉植物病害图像特有的层次化、多尺度病变模式。为解决这一差距，我们提出了PSMamba，一种渐进式自监督框架，将Vision Mamba(VM)的高效序列建模与双学生层次蒸馏策略相结合。与传统单一教师-学生设计不同，PSMamba采用共享的全局教师和两个专门的学生：一个处理中等尺度视图以捕捉病变分布和静脉结构，而另一个专注于局部视图以捕捉纹理不规则性和早期病变等细粒度线索。这种多粒度监督促进了上下文和详细表示的联合学习，一致性损失确保了跨尺度对齐的连贯性。在三个基准数据集上的实验表明，PSMamba始终优于最先进的SSL方法，在领域迁移和细粒度场景中提供更高的准确性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.</description>
      <author>example@mail.com (Abdullah Al Mamun, Miaohua Zhang, David Ahmedt-Aristizabal, Zeeshan Hayder, Mohammad Awrangjeb)</author>
      <guid isPermaLink="false">2512.14309v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Gain from Data Filtering in Multimodal Contrastive Learning</title>
      <link>http://arxiv.org/abs/2512.14230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages, 8 figures, 1 table. This work is accepted to the Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;现代多模态表示学习的成功依赖于互联网规模数据集，而数据筛选已成为训练流程中的关键步骤。基于教师模型的筛选方法利用预训练模型计算质量分数，已被证明是一种有效的解决方案。理论分析表明，数据筛选可提供可证明的好处，显著降低对比学习的误差。&lt;h4&gt;背景&lt;/h4&gt;现代多模态表示学习的成功依赖于互联网规模的数据集。然而，由于大量原始网络数据质量低下，数据筛选已成为训练流程中的关键步骤。&lt;h4&gt;目的&lt;/h4&gt;解释基于教师模型筛选方法的经验成功，并表征在标准双模态数据生成模型下经过筛选的对比学习性能。&lt;h4&gt;方法&lt;/h4&gt;使用基于训练模型的筛选方法（即基于教师模型的筛选），利用预训练模型计算质量分数，并在线性对比学习设置下进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;未筛选时的误差上下界为1/(η√n)，其中η是正确匹配模态的数据比例，n是成对样本数量；使用基于教师模型的筛选后，在大η情况下误差上界为1/√(ηn)，在小η情况下误差上界为1/√n。&lt;h4&gt;结论&lt;/h4&gt;数据筛选提供了可证明的好处，基于教师模型的筛选方法在不同数据质量情况下都能有效降低误差。&lt;h4&gt;翻译&lt;/h4&gt;现代多模态表示学习的成功依赖于互联网规模的数据集。由于大量原始网络数据质量低下，数据筛选已成为训练流程中的关键步骤。利用训练好的模型进行筛选（即基于教师模型的筛选）已成为一种成功的解决方案，它利用预训练模型计算质量分数。为了解释基于教师模型筛选方法的经验成功，我们在标准双模态数据生成模型下表征了经过筛选的对比学习的性能。将η∈(0,1]表示为n个成对样本中正确匹配模态的数据比例，我们利用线性对比学习设置展示了数据筛选的可证明好处：(i)未筛选时的误差上下界为1/(η√n)，(ii)使用基于教师模型的筛选后，在大η情况下误差上界为1/√(ηn)，在小η情况下误差上界为1/√n。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\frac{1}{η\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\frac{1}{\sqrt{ηn}}$ in the large $η$ regime, and by $\frac{1}{\sqrt{n}}$ in the small $η$ regime.</description>
      <author>example@mail.com (Divyansh Pareek, Sewoong Oh, Simon S. Du)</author>
      <guid isPermaLink="false">2512.14230v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting</title>
      <link>http://arxiv.org/abs/2512.14115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种联合多模态对比学习框架，用于改进声学词嵌入在语音检索任务中的性能，解决了现有方法的局限性&lt;h4&gt;背景&lt;/h4&gt;声学词嵌入(AWEs)提高了语音检索任务(如口语词检测(STD)和关键词定位(KWS))的效率&lt;h4&gt;目的&lt;/h4&gt;解决现有AWE方法的局限性，包括单模态监督、音频-音频和音频-文本对齐的分离优化，以及需要特定任务模型的问题&lt;h4&gt;方法&lt;/h4&gt;提出了一种联合多模态对比学习框架，在共享嵌入空间中统一了声学和跨模态监督。该方法同时优化：(i)基于CLAP损失的音频-文本对比学习，对齐音频和文本表示；(ii)通过深度词辨别(DWD)损失的音频-音频对比学习，增强类内紧凑性和类间分离性&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在词辨别任务上优于现有的AWE基线，同时灵活支持STD和KWS任务&lt;h4&gt;结论&lt;/h4&gt;据我们所知，这是首个此类综合方法&lt;h4&gt;翻译&lt;/h4&gt;声学词嵌入(AWEs)提高了语音检索任务(如口语词检测(STD)和关键词定位(KWS))的效率。然而，现有方法存在局限性，包括单模态监督、音频-音频和音频-文本对齐的分离优化，以及需要特定任务模型。为解决这些不足，我们提出了一种联合多模态对比学习框架，在共享嵌入空间中统一了声学和跨模态监督。我们的方法同时优化：(i)受CLAP损失启发的音频-文本对比学习，对齐音频和文本表示；(ii)通过深度词辨别(DWD)损失的音频-音频对比学习，增强类内紧凑性和类间分离性。所提出的方法在词辨别任务上优于现有的AWE基线，同时灵活支持STD和KWS。据我们所知，这是首个此类综合方法&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.</description>
      <author>example@mail.com (Ramesh Gundluru, Shubham Gupta, Sri Rama Murty K)</author>
      <guid isPermaLink="false">2512.14115v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>AsarRec: Adaptive Sequential Augmentation for Robust Self-supervised Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2512.14047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为AsarRec的自适应增强框架，用于解决顺序推荐系统中的噪声问题，通过学习生成变换矩阵来优化推荐性能。&lt;h4&gt;背景&lt;/h4&gt;顺序推荐系统在建模用户动态偏好和捕捉项目转换模式方面表现出色，但真实世界用户行为往往因人为错误、不确定性和行为模糊性而存在噪声，导致推荐性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有自监督学习方法中依赖静态增强策略的问题，提出一种能够自适应选择增强类型的框架，以提高顺序推荐系统的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将基本增强操作统一为结构化变换矩阵，通过将用户序列编码为概率转移矩阵并使用可微的半-Sinkhorn算法投影到硬半双随机矩阵来学习生成变换矩阵，同时联合优化多样性、语义不变性和信息量三个目标。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法依赖的静态增强策略存在两个关键挑战：最优增强类型在不同场景下可能有显著差异；不适当的增强甚至可能降低推荐性能。AsarRec在不同噪声水平下的三个基准数据集上验证了其有效性。&lt;h4&gt;结论&lt;/h4&gt;AsarRec框架表现出卓越的鲁棒性和一致的改进，能够有效解决顺序推荐系统中的噪声问题。&lt;h4&gt;翻译&lt;/h4&gt;顺序推荐系统在建模用户动态偏好和捕捉项目转换模式方面已展现出强大的能力。然而，由于人为错误、不确定性和行为模糊性等因素，真实世界的用户行为往往存在噪声，这可能导致推荐性能下降。为解决这个问题，最近的方法广泛采用自监督学习，特别是对比学习，通过生成用户交互序列的扰动视图并最大化它们之间的互信息来提高模型鲁棒性。然而，这些方法严重依赖其预定义的静态增强策略（一旦选定，增强类型保持不变）来构建增强视图，导致两个关键挑战：（1）最优增强类型在不同场景下可能有显著差异；（2）不适当的增强甚至可能降低推荐性能，限制了自监督学习的有效性。为了克服这些局限性，我们提出了一个自适应增强框架。我们首先通过结构化变换矩阵将现有的基本增强操作统一为一个统一的公式。在此基础上，我们引入了AsarRec（用于鲁棒顺序推荐的自适应顺序增强），它通过将用户序列编码为概率转移矩阵，并使用可微的半-Sinkhorn算法将其投影到硬半双随机矩阵来学习生成变换矩阵。为确保学习到的增强有利于下游性能，我们联合优化了三个目标：多样性、语义不变性和信息量。在三个不同噪声水平下的基准数据集上进行的大量实验验证了AsarRec的有效性，展示了其卓越的鲁棒性和一致的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommender systems have demonstrated strong capabilities in modeling users' dynamic preferences and capturing item transition patterns. However, real-world user behaviors are often noisy due to factors such as human errors, uncertainty, and behavioral ambiguity, which can lead to degraded recommendation performance. To address this issue, recent approaches widely adopt self-supervised learning (SSL), particularly contrastive learning, by generating perturbed views of user interaction sequences and maximizing their mutual information to improve model robustness. However, these methods heavily rely on their pre-defined static augmentation strategies~(where the augmentation type remains fixed once chosen) to construct augmented views, leading to two critical challenges: (1) the optimal augmentation type can vary significantly across different scenarios; (2) inappropriate augmentations may even degrade recommendation performance, limiting the effectiveness of SSL. To overcome these limitations, we propose an adaptive augmentation framework. We first unify existing basic augmentation operations into a unified formulation via structured transformation matrices. Building on this, we introduce AsarRec (Adaptive Sequential Augmentation for Robust Sequential Recommendation), which learns to generate transformation matrices by encoding user sequences into probabilistic transition matrices and projecting them into hard semi-doubly stochastic matrices via a differentiable Semi-Sinkhorn algorithm. To ensure that the learned augmentations benefit downstream performance, we jointly optimize three objectives: diversity, semantic invariance, and informativeness. Extensive experiments on three benchmark datasets under varying noise levels validate the effectiveness of AsarRec, demonstrating its superior robustness and consistent improvements.</description>
      <author>example@mail.com (Kaike Zhang, Qi Cao, Fei Sun, Xinran Liu)</author>
      <guid isPermaLink="false">2512.14047v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Unleashing the Power of Image-Tabular Self-Supervised Learning via Breaking Cross-Tabular Barriers</title>
      <link>http://arxiv.org/abs/2512.14026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CITab，一种新型跨表格多模态自监督学习框架，通过语义感知的表格建模机制和原型引导的线性层混合模块，有效整合医学图像和表格数据，在阿尔茨海默病诊断任务上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;多模态学习整合医学图像和表格数据已显著推进临床决策，但现有自监督学习方法常局限于特定数据队列，因其僵化的表格建模机制难以处理异构表格数据，形成跨表格障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够学习强大跨表格多模态特征表示的自监督学习框架，促进可转移医学知识的学习，并提高利用多种数据源进行预训练的可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出CITab框架，从语义感知角度设计表格建模机制，整合列标题作为语义线索；同时提出原型引导的线性层混合(P-MoLin)模块处理表格数据异质性并探索潜在医学概念。&lt;h4&gt;主要发现&lt;/h4&gt;在包含4461名受试者的三个阿尔茨海默病诊断数据队列上的综合评估表明，CITab优于最先进的方法，为有效且可扩展的跨表格多模态学习铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;CITab通过创新的表格建模机制和特征专业化方法，成功克服了跨表格多模态学习中的障碍，为医学图像和表格数据的有效整合提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;整合医学图像和表格数据的多模态学习近年来显著推进了临床决策。自监督学习(SSL)已成为在这些大规模未标记图像-表格数据上预训练这些模型的有力范式，旨在学习判别性表示。然而，现有的用于图像-表格表示学习的SSL方法通常局限于特定的数据队列，主要是由于其建模异构表格数据时僵化的表格建模机制。这种跨表格障碍阻碍了多模态SSL方法有效学习跨不同队列的可转移医学知识。在本文中，我们提出了一个新的SSL框架，即CITab，旨在以跨表格方式学习强大的多模态特征表示。我们从语义感知角度设计表格建模机制，通过整合列标题作为语义线索，这促进了可转移知识的学习以及利用多种数据源进行预训练的可扩展性。此外，我们提出了一个原型引导的线性层混合(P-MoLin)模块用于表格特征专业化，使模型能够有效处理表格数据的异质性并探索潜在的医学概念。我们在三个包含4461名受试者的公开可用数据队列上对阿尔茨海默病诊断任务进行了全面评估。实验结果表明，CITab优于最先进的方法，为有效且可扩展的跨表格多模态学习铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal learning integrating medical images and tabular data has significantly advanced clinical decision-making in recent years. Self-Supervised Learning (SSL) has emerged as a powerful paradigm for pretraining these models on large-scale unlabeled image-tabular data, aiming to learn discriminative representations. However, existing SSL methods for image-tabular representation learning are often confined to specific data cohorts, mainly due to their rigid tabular modeling mechanisms when modeling heterogeneous tabular data. This inter-tabular barrier hinders the multi-modal SSL methods from effectively learning transferrable medical knowledge shared across diverse cohorts. In this paper, we propose a novel SSL framework, namely CITab, designed to learn powerful multi-modal feature representations in a cross-tabular manner. We design the tabular modeling mechanism from a semantic-awareness perspective by integrating column headers as semantic cues, which facilitates transferrable knowledge learning and the scalability in utilizing multiple data sources for pretraining. Additionally, we propose a prototype-guided mixture-of-linear layer (P-MoLin) module for tabular feature specialization, empowering the model to effectively handle the heterogeneity of tabular data and explore the underlying medical concepts. We conduct comprehensive evaluations on Alzheimer's disease diagnosis task across three publicly available data cohorts containing 4,461 subjects. Experimental results demonstrate that CITab outperforms state-of-the-art approaches, paving the way for effective and scalable cross-tabular multi-modal learning.</description>
      <author>example@mail.com (Yibing Fu, Yunpeng Zhao, Zhitao Zeng, Cheng Chen, Yueming Jin)</author>
      <guid isPermaLink="false">2512.14026v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment</title>
      <link>http://arxiv.org/abs/2512.14019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EXAONE Path 2.5是一种病理学基础模型，通过联合建模组织学、基因组、表观基因组和转录组模态，捕捉癌症进展中多个生物层之间的相互作用，产生更全面的肿瘤生物学表征。&lt;h4&gt;背景&lt;/h4&gt;癌症进展源于多个生物层之间的相互作用，特别是超越形态学层面，在分子层面上的相互作用，这些层面仅依靠图像模型无法捕捉。&lt;h4&gt;目的&lt;/h4&gt;开发能够捕捉更广泛生物学景观的病理学基础模型，通过综合多种生物模态，产生反映肿瘤生物学更全面整合的患者表征。&lt;h4&gt;方法&lt;/h4&gt;提出EXAONE Path 2.5病理学基础模型，包含三个关键组件：(1)多模态SigLIP损失，支持异构模态间的全对比学习；(2)片段感知的旋转位置编码模块，保留WSI中的空间结构和组织片段拓扑；(3)领域专业化的内部基础模型，用于WSI和RNA-seq，提供生物学基础的嵌入以实现稳健的多模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在内部真实世界临床数据集和Patho-Bench基准(涵盖80项任务)上评估，框架显示出高数据效率和参数效率，在Patho-Bench上与最先进的基础模型性能相当，在内部临床环境中表现出最高的适应性。&lt;h4&gt;结论&lt;/h4&gt;生物学信息丰富的多模态设计具有重要价值，集成的基因型到表型建模对下一代精准肿瘤学具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;癌症进展源于多个生物层之间的相互作用，特别是超越形态学层面，在分子层面上的相互作用，这些层面仅依靠图像模型无法捕捉。为了捕捉这一更广泛的生物学景观，我们提出了EXAONE Path 2.5，一种病理学基础模型，它联合建模组织学、基因组、表观基因组和转录组模态，产生反映肿瘤生物学更全面整合的患者表征。我们的方法包含三个关键组件：(1)多模态SigLIP损失，支持异构模态间的全对比学习；(2)片段感知的旋转位置编码模块，保留WSI中的空间结构和组织片段拓扑；(3)领域专业化的内部基础模型，用于WSI和RNA-seq，提供生物学基础的嵌入以实现稳健的多模态对齐。我们在两个互补基准上评估了EXAONE Path 2.5：内部真实世界临床数据集和涵盖80项任务的Patho-Bench基准。我们的框架显示出高数据效率和参数效率，在Patho-Bench上与最先进的基础模型性能相当，同时在内部临床环境中表现出最高的适应性。这些结果强调了生物学信息丰富的多模态设计价值，并突出了集成的基因型到表型建模对下一代精准肿瘤学的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.</description>
      <author>example@mail.com (Juseung Yun, Sunwoo Yu, Sumin Ha, Jonghyun Kim, Janghyeon Lee, Jongseong Jang, Soonyoung Lee)</author>
      <guid isPermaLink="false">2512.14019v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training</title>
      <link>http://arxiv.org/abs/2512.13770v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MV-SupGCN是一种半监督图卷积网络模型，通过整合互补组件来有效利用多视图数据中的互补信息，提高特征表示和模型性能。&lt;h4&gt;背景&lt;/h4&gt;图卷积网络(GCN)为基础的多视图学习为整合异构视图的结构信息提供了强大框架，但现有方法往往无法充分利用跨视图的互补信息，导致次优的特征表示和有限的性能。&lt;h4&gt;目的&lt;/h4&gt;提出MV-SupGCN模型，通过整合互补组件来更好地利用跨视图的互补信息，改进特征表示，提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;MV-SupGCN包含三个主要组件：1)设计联合损失函数结合交叉熵损失和监督对比损失；2)结合基于KNN和半监督的图构建方法；3)整合对比学习和伪标记技术，强制多视图嵌入一致性并增强语义对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，MV-SupGCN持续超越最先进的方法，验证了整合方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;MV-SupGCN通过整合互补组件，有效解决了现有方法无法充分利用跨视图互补信息的问题，提高了特征表示质量和模型性能。&lt;h4&gt;翻译&lt;/h4&gt;基于图卷积网络(GCN)的多视图学习的出现为整合异构视图的结构信息提供了强大框架，能够对复杂的多视图数据进行有效建模。然而，现有方法往往无法充分利用跨视图的互补信息，导致次优的特征表示和有限的性能。为了解决这个问题，我们提出了MV-SupGCN，这是一种半监督GCN模型，集成了几个具有明确动机和相互强化的互补组件。首先，为了更好地捕捉判别性特征并提高模型泛化能力，我们设计了一个联合损失函数，将交叉熵损失与监督对比损失相结合，鼓励模型在潜在空间中同时最小化类内方差和最大化类间可分性。其次，认识到单图构建方法的不稳定性和不完整性，我们在每个视图上结合了基于KNN和半监督的图构建方法，从而增强了数据结构表示的鲁棒性并减少了泛化误差。第三，为了有效利用丰富的未标记数据并增强多视图之间的语义对齐，我们提出了一个统一框架，整合对比学习以强制多视图嵌入之间的一致性并捕捉有意义的跨视图关系，同时结合伪标记，为交叉熵和对比损失函数提供额外的监督，以增强模型泛化能力。大量实验表明，MV-SupGCN在多个基准测试中持续超越最先进的方法，验证了我们整合方法的有效性。源代码可在https://github.com/HuaiyuanXiao/MVSupGCN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN</description>
      <author>example@mail.com (Huaiyuan Xiao, Fadi Dornaika, Jingjun Bi)</author>
      <guid isPermaLink="false">2512.13770v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis</title>
      <link>http://arxiv.org/abs/2512.13164v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  68 pages, 9 figures, 16 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了CRAFTS框架，这是一种针对病理学的文本到图像生成基础模型，通过克服数据稀缺性和生成质量问题，为临床级病理学人工智能提供了新的解决方案。&lt;h4&gt;背景&lt;/h4&gt;临床级病理学人工智能的发展受限于高质量注释数据集的稀缺性，而现有生成模型虽能提供潜在解决方案，但存在语义不稳定性和形态幻觉问题，影响诊断可靠性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够生成高质量、多样化病理图像的生成模型，解决病理学中高质量注释数据集稀缺的问题，同时确保生成图像的生物学准确性和诊断可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出CRAFTS（Correlation-Regulated Alignment Framework for Tissue Synthesis）框架，采用双阶段训练策略，基于约280万图像-标题对进行训练，并引入一种新的对齐机制来抑制语义漂移，确保生物学准确性。同时探索了CRAFTS与ControlNet的结合，以实现对组织架构的精确控制。&lt;h4&gt;主要发现&lt;/h4&gt;CRAFTS能够生成跨越30种癌症类型的多样化病理图像，其质量通过客观指标和病理学家评估得到验证。使用CRAFTS增强的数据集显著提高了分类、跨模态检索、自监督学习和视觉问答等多种临床任务的表现。结合ControlNet后，模型能够从核分割掩模和荧光图像等输入精确控制组织架构。&lt;h4&gt;结论&lt;/h4&gt;CRAFTS克服了病理学人工智能发展中数据稀缺性和隐私关注的关键障碍，提供了无限的多样化、注释组织学数据源，有效解锁了罕见和复杂癌症表型的稳健诊断工具开发。&lt;h4&gt;翻译&lt;/h4&gt;病理学临床级人工智能的发展受限于多样化、高质量注释数据集的稀缺性。生成模型提供了潜在解决方案，但存在语义不稳定性和形态幻觉，影响诊断可靠性。为应对这一挑战，我们引入了CRAFTS（组织合成的相关性调节对齐框架），这是首个病理学特定的文本到图像生成生成基础模型。通过利用约280万图像-标题对的双阶段训练策略，CRAFTS结合了一种新的对齐机制，抑制语义漂移以确保生物学准确性。该模型生成跨越30种癌症类型的多样化病理图像，其质量通过客观指标和病理学家评估严格验证。此外，CRAFTS增强的数据集提高了分类、跨模态检索、自监督学习和视觉问答等各种临床任务的表现。此外，将CRAFTS与ControlNet结合能够从核分割掩模和荧光图像等输入精确控制组织架构。通过克服数据稀缺性和隐私关注的关键障碍，CRAFTS提供了无限的多样化、注释组织学数据源，有效解锁了罕见和复杂癌症表型的稳健诊断工具的开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.</description>
      <author>example@mail.com (Xianchao Guan, Zhiyuan Fan, Yifeng Wang, Fuqiang Chen, Yanjiang Zhou, Zengyang Che, Hongxue Meng, Xin Li, Yaowei Wang, Hongpeng Wang, Min Zhang, Heng Tao Shen, Zheng Zhang, Yongbing Zhang)</author>
      <guid isPermaLink="false">2512.13164v2</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection</title>
      <link>http://arxiv.org/abs/2512.13736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TF-MCL的时间-频率融合和多域交叉损失模型，用于提高基于脑电图信号的重度抑郁症检测性能。&lt;h4&gt;背景&lt;/h4&gt;近年来基于脑电图信号的重度抑郁症监督检测方法使用增加，但标记过程具有挑战性。对比学习作为自监督学习方法可解决监督学习过度依赖标签的问题，但现有对比学习方法未专门设计来表征EEG信号的时间-频率分布，获取低语义数据表示的能力不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有对比学习方法在MDD检测中的局限性，提出一种能更有效表征EEG信号时间-频率分布的模型。&lt;h4&gt;方法&lt;/h4&gt;TF-MCL模型通过融合映射头(FMH)生成时间-频率混合表示，将时间-频率域信息重新映射到融合域；同时通过优化多域交叉损失函数，重建时间-频率域和融合域中表示的分布，提高模型获取融合表示的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集MODMA和PRED+CT上的评估显示，该模型准确率显著提高，分别比现有最先进方法提高了5.87%和9.96%。&lt;h4&gt;结论&lt;/h4&gt;TF-MCL模型能有效提升基于EEG信号的重度抑郁症检测性能，为临床应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，基于脑电图信号的重度抑郁症监督检测方法使用显著增加。然而，MDD的标记过程仍然具有挑战性。作为自监督学习方法，对比学习可以解决监督学习方法在MDD检测中过度依赖标签的缺点。然而，现有对比学习方法并非专门设计来表征EEG信号的时间-频率分布，它们获取低语义数据表示的能力对于MDD检测任务仍然不足。为了解决对比学习方法的问题，我们提出了一种用于MDD检测的时间-频率融合和多域交叉损失(TF-MCL)模型。TF-MCL通过融合映射头(FMH)生成时间-频率混合表示，有效地将时间-频率域信息重新映射到融合域，从而可以增强模型合成时间-频率信息的能力。此外，通过优化多域交叉损失函数，重建了时间-频率域和融合域中表示的分布，从而提高了模型获取融合表示的能力。我们在公开数据集MODMA和PRED+CT上评估了模型的性能，显示准确率显著提高，分别比现有最先进(SOTA)方法提高了5.87%和9.96%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.</description>
      <author>example@mail.com (Li-Xuan Zhao, Chen-Yang Xu, Wen-Qiang Li, Bo Wang, Rong-Xing Wei, Qing-Hao Menga)</author>
      <guid isPermaLink="false">2512.13736v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2512.14619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by WSDM 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为PageRank Transformer (ParaFormer)的新型图神经网络架构，解决了全局注意力机制中存在的严重过度平滑问题。&lt;h4&gt;背景&lt;/h4&gt;Graph Transformers (GTs)作为一种有前景的图学习工具，利用全连接特性有效捕获全局信息。为解决深度GNN中的过度平滑问题，最初引入了全局注意力机制，消除了使用深度GNN的必要性。&lt;h4&gt;目的&lt;/h4&gt;减轻全局注意力机制中存在的严重过度平滑问题，使节点表示保持可区分性。&lt;h4&gt;方法&lt;/h4&gt;提出PageRank Transformer (ParaFormer)，其特点是包含一个PageRank增强的注意力模块，专门设计来模仿深度Transformers的行为。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论和实证分析，验证了ParaFormer通过作为自适应通滤波器来减轻过度平滑问题。在11个从几千到几百万节点的数据集上，ParaFormer在节点分类和图分类任务中实现了一致的性能提升。&lt;h4&gt;结论&lt;/h4&gt;ParaFormer有效解决了全局注意力中的过度平滑问题，在多种图学习任务中展现出优越的性能和有效性。&lt;h4&gt;翻译&lt;/h4&gt;图变换器(GTs)已成为一种有前景的图学习工具，利用其全连接特性有效捕获全局信息。为解决深度图神经网络(GNNs)中的过度平滑问题，最初引入了全局注意力，消除了使用深度GNN的必要性。然而，通过实证和理论分析，我们验证了引入的全局注意力表现出严重的过度平滑，由于其固有的低通滤波特性导致节点表示变得不可区分。这种效应甚至比在GNN中观察到的效应更强。为缓解这一问题，我们提出了PageRank Transformer (ParaFormer)，它具有一个PageRank增强的注意力模块，设计用来模仿深度Transformers的行为。我们通过理论和实证证明，ParaFormer通过作为自适应通滤波器来减轻过度平滑。实验显示，ParaFormer在11个从几千到几百万节点的数据集上，在节点分类和图分类任务中实现了性能的持续提升，验证了其有效性。补充材料，包括代码和附录，可在https://github.com/chaohaoyuan/ParaFormer找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.</description>
      <author>example@mail.com (Chaohao Yuan, Zhenjie Song, Ercan Engin Kuruoglu, Kangfei Zhao, Yang Liu, Deli Zhao, Hong Cheng, Yu Rong)</author>
      <guid isPermaLink="false">2512.14619v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning</title>
      <link>http://arxiv.org/abs/2512.14331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种能够实时更新以处理机器人系统非线性动态变化的框架，能够有效应对逐渐漂移、瞬时波动和突然转变等多种变化模式。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的机器人必须在不断变化的动态条件下运行，这些变化由变化的操作条件、外部干扰和未建模效应引起，表现为逐渐漂移、瞬时波动或突然转变。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实时更新的非线性动态建模框架，实现对机器人系统动态变化的实时适应，既对短期变化具有鲁棒性，又能对持久变化做出响应。&lt;h4&gt;方法&lt;/h4&gt;提出将表示学习与在线适应解耦的框架，使用离线学习的潜在表示支持在线闭式贝叶斯更新；引入变化点感知机制，通过从数据可能性推断的潜在变量来指示连续性或转变；根据连续性或转变情况分别采用积累证据或调整信息的策略。&lt;h4&gt;主要发现&lt;/h4&gt;框架的自适应遗憾随时间仅呈对数增长，随转变次数呈线性增长，与知道转变时间的预言家相当；在cartpole模拟和真实四旋翼飞行器验证中，显示出比相关基线更好的预测准确性、更快的恢复能力和更精确的闭环跟踪。&lt;h4&gt;结论&lt;/h4&gt;该框架能够维持校准的不确定性，支持对瞬时、逐渐或结构变化的概率推理，为处理现实世界中机器人面临的动态变化提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的机器人必须在不断变化的动态条件下运行，这些变化由变化的操作条件、外部干扰和未建模效应引起。这些可能表现为逐渐漂移、瞬时波动或突然转变，要求实时适应既对短期变化具有鲁棒性，又能对持久变化做出响应。我们提出一个用于建模机器人系统非线性动态的框架，该框架可以从流数据中实时更新。该方法将表示学习与在线适应解耦，使用离线学习的潜在表示来支持在线闭式贝叶斯更新。为了处理不断变化的情况，我们引入了一个具有变化点感知能力的机制，通过从数据可能性推断的潜在变量来指示连续性或转变。当连续性可能时，证据积累以完善预测；当检测到转变时，调整过去信息以实现快速重新学习。这维持了校准的不确定性，并支持对瞬时、逐渐或结构变化的概率推理。我们证明了该框架的自适应遗憾随时间仅呈对数增长，随转变次数呈线性增长，与知道转变时间的预言家相当。我们在cartpole模拟和带有摆动载荷和飞行中丢弃载荷的真实四旋翼飞行器上进行了验证，显示出比相关基线更好的预测准确性、更快的恢复能力和更精确的闭环跟踪。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.</description>
      <author>example@mail.com (Rishabh Dev Yadav, Avirup Das, Hongyu Song, Samuel Kaski, Wei Pan)</author>
      <guid isPermaLink="false">2512.14331v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes</title>
      <link>http://arxiv.org/abs/2512.14092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProtoFlow是一种新型框架，通过学习动态场景图原型来建模复杂外科手术流程，结合了自监督预训练和基于原型的微调，在有限数据条件下表现出色，并能提供可解释的手术见解。&lt;h4&gt;背景&lt;/h4&gt;详细的外科手术识别对于推进AI辅助手术至关重要，但进展受到高标注成本、数据稀缺和缺乏可解释模型的阻碍。场景图虽然提供了手术事件的结构化抽象，但其潜力尚未完全开发。&lt;h4&gt;目的&lt;/h4&gt;引入ProtoFlow框架，学习动态场景图原型，以可解释和稳健的方式建模复杂的外科手术流程。&lt;h4&gt;方法&lt;/h4&gt;ProtoFlow利用图神经网络编码器-解码器架构，结合自监督预训练进行丰富的表征学习，以及基于原型的微调阶段，发现和完善封装重复的、具有临床意义的手术交互模式的核心原型。&lt;h4&gt;主要发现&lt;/h4&gt;在CAT-SG数据集上，ProtoFlow在总体准确性和少样本场景中均优于标准GNN基线，即使仅使用一个外科视频训练也能保持强大性能，并能识别不同的外科子技术，提供对工作流偏差和罕见并发症的可解释见解。&lt;h4&gt;结论&lt;/h4&gt;ProtoFlow结合了稳健的表征学习和固有的可解释性，朝着开发更透明、可靠和数据高效的AI系统迈出了重要一步，加速了其在外科培训、实时决策支持和工作流优化方面的临床应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;目的：详细的外科手术识别对于推进AI辅助手术至关重要，但进展受到高标注成本、数据稀缺和缺乏可解释模型的阻碍。虽然场景图提供了手术事件的结构化抽象，但其全部潜力仍未被开发。在这项工作中，我们引入了ProtoFlow，一个新颖的框架，它学习动态场景图原型，以可解释和稳健的方式建模复杂的外科手术流程。方法：ProtoFlow利用图神经网络编码器-解码器架构，结合自监督预训练进行丰富的表征学习，以及基于原型的微调阶段。这一过程发现和完善了封装重复的、具有临床意义的手术交互模式的核心原型，为工作流分析形成了可解释的基础。结果：我们在细粒度的CAT-SG数据集上评估了我们的方法。ProtoFlow不仅在总体准确性上优于标准GNN基线，而且在有限数据、少样本场景中也表现出卓越的稳健性，即使在仅使用一个外科视频进行训练的情况下也能保持强大的性能。我们的定性分析进一步表明，学习到的原型成功识别了不同的外科子技术，并对工作流偏差和罕见并发症提供了清晰、可解释的见解。结论：通过结合稳健的表征学习和固有的可解释性，ProtoFlow朝着开发更透明、可靠和数据高效的AI系统迈出了重要一步，加速了它们在外科培训、实时决策支持和工作流优化方面的临床应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.</description>
      <author>example@mail.com (Felix Holm, Ghazal Ghazaei, Nassir Navab)</author>
      <guid isPermaLink="false">2512.14092v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains</title>
      <link>http://arxiv.org/abs/2512.13852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新框架，通过整合持久同调特征和稳定性正则化来增强图神经网络对结构扰动的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为图表示学习的标准方法，但对结构扰动仍然脆弱。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，通过整合持久同调特征和稳定性正则化来增强图神经网络的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;基于持久同调的稳定性定理，将GIN架构与从持久图像中提取的多尺度拓扑特征相结合，并通过Hiraoka-Kusano启发的稳定性约束进行强制执行。&lt;h4&gt;主要发现&lt;/h4&gt;在六个涵盖生化、社交和协作网络的多样化数据集上，该方法对边扰动表现出卓越的鲁棒性，同时保持有竞争力的准确性。在扰动下观察到最小的性能下降(大多数数据集上为0-4%)，显著优于基线稳定性。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了一种理论上合理且经验验证的鲁棒图学习方法，符合拓扑正则化的最新进展。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为图表示学习的标准方法，但对结构扰动仍然脆弱。我们提出了一种新框架，整合持久同调特征和稳定性正则化以增强鲁棒性。基于持久同调的稳定性定理，我们的方法将GIN架构与从持久图像中提取的多尺度拓扑特征相结合，并通过受Hiraoka-Kusano启发的稳定性约束强制执行。在六个涵盖生化、社交和协作网络的多样化数据集上，我们的方法在保持竞争力的准确性的同时，对边扰动表现出卓越的鲁棒性。值得注意的是，在扰动下我们观察到最小的性能下降(大多数数据集上为0-4%)，显著优于基线稳定性。我们的工作提供了一种理论上合理且经验验证的鲁棒图学习方法，符合拓扑正则化的最新进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology \cite{cohen2007stability}, our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization</description>
      <author>example@mail.com (Jelena Losic)</author>
      <guid isPermaLink="false">2512.13852v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention</title>
      <link>http://arxiv.org/abs/2512.11811v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Updated author list to include additional contributor. Revised title and improved methodology section based on collaborative feedback&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出VPR-AttLLM框架，通过整合大型语言模型的语义推理和地理知识到视觉位置识别流程中，改进了社交媒体众包街景图像的地理定位性能，特别是在城市洪水等危机事件场景下。&lt;h4&gt;背景&lt;/h4&gt;社交媒体众包的街景图像提供了城市洪水和其他危机事件的实时视觉证据，但通常缺乏可靠的地理元数据用于应急响应。现有的图像地理定位方法在应用于此类图像时性能显著下降，因为跨场景情况下的视觉失真和域偏移。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需重新训练模型或添加额外数据就能提高检索性能的框架，解决社交媒体图像地理定位中的挑战。&lt;h4&gt;方法&lt;/h4&gt;VPR-AttLLM是一个与模型无关的框架，通过注意力引导的描述符增强，将大型语言模型的语义推理和地理知识整合到现有的视觉位置识别流程中。利用大型语言模型识别城市背景中的信息性区域并抑制视觉噪声。&lt;h4&gt;主要发现&lt;/h4&gt;将VPR-AttLLM与三种最先进的VPR模型（CosPlace、EigenPlaces和SALAD）结合，始终提高了召回性能，相对增益通常在1-3%之间，在最具挑战性的真实洪水图像上达到8%。&lt;h4&gt;结论&lt;/h4&gt;VPR-AttLLM建立了视觉检索系统中大型语言模型引导的多模态融合的可推广范式。通过将城市感知理论原理嵌入到注意力机制中，桥接了类人的空间推理与现代视觉位置识别架构。其即插即用设计、强大的跨源鲁棒性和可解释性突显了其在可扩展城市监测和众包危机图像快速地理定位方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体众包的街景图像为城市洪水和其他危机事件提供了实时视觉证据，但它通常缺乏应急响应所需的可靠地理元数据。现有的图像地理定位方法，也称为视觉位置识别模型，在应用于此类图像时表现出显著的性能下降，这是由于跨源场景中的视觉失真和域偏移。本文提出了VPR-AttLLM，一个与模型无关的框架，通过注意力引导的描述符增强，将大型语言模型的语义推理和地理知识整合到既定的视觉位置识别流程中。通过利用大型语言模型识别城市背景中的信息性区域并抑制视觉噪声，VPR-AttLLM在不要求重新训练模型或添加额外数据的情况下提高了检索性能。在扩展基准上进行了全面评估，包括用真实社交媒体洪水图像增强的SF-XL、在已建立的查询集和Mapillary照片上的合成洪水场景，以及捕捉形态各异城市景观的新HK-URBAN数据集。将VPR-AttLLM与三种最先进的视觉位置识别模型（CosPlace、EigenPlaces和SALAD）集成，始终提高了召回性能，相对增益通常在1-3%之间，在最具挑战性的真实洪水图像上达到8%。除了在检索准确性方面可衡量的增益外，本研究还建立了视觉检索系统中大型语言模型引导的多模态融合的可推广范式。通过将城市感知理论的原理嵌入到注意力机制中，VPR-AttLLM桥接了类人的空间推理与现代视觉位置识别架构。其即插即用设计、强大的跨源鲁棒性和可解释性突显了其在可扩展城市监测和众包危机图像快速地理定位方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crowdsourced street-view imagery from social media provides real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing image geo-localization approaches, also known as Visual Place Recognition (VPR) models, exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geo-knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.</description>
      <author>example@mail.com (Fengyi Xu, Jun Ma, Waishan Qiu, Cui Guo, Jack C. P. Cheng)</author>
      <guid isPermaLink="false">2512.11811v2</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT</title>
      <link>http://arxiv.org/abs/2512.11870v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了休斯顿市如何实现2050年净零排放目标，重点关注道路交通减排策略，通过建立排放基准和评估政策来加速零排放车辆采用并减少车辆行驶里程。&lt;h4&gt;背景&lt;/h4&gt;全球道路交通占温室气体排放的15%，导致约38.5万人因PM2.5过早死亡；城市占全球能源相关温室气体排放的75%；休斯顿道路交通占其气候行动计划基准排放的48%。&lt;h4&gt;目的&lt;/h4&gt;建立道路排放基准并评估政策，利用社会经济指标和智能交通系统加速零排放车辆采用并减少车辆行驶里程，以帮助休斯顿实现2050年净零排放目标。&lt;h4&gt;方法&lt;/h4&gt;采用智能停车、公共交通激励、安全数据系统和零排放车队管理等策略；在Unity 3D中开发模拟环境，支持城市动态建模和政策情景可视化。&lt;h4&gt;主要发现&lt;/h4&gt;休斯顿作为低密度、汽车依赖型城市，89%的道路排放来自汽车和小型卡车，公共交通使用有限；社会经济差异制约了零排放车辆采用；可通过扩大ZEV获取渠道和将VMT减少20%的策略应对挑战。&lt;h4&gt;结论&lt;/h4&gt;依赖汽车的城市为实现2050年排放目标可从论文讨论的指标、度量和技术中受益。&lt;h4&gt;翻译&lt;/h4&gt;全球范围内，道路交通占温室气体排放的15%，并估计有385,000人因PM2.5而过早死亡。城市在实现IPCC目标中发挥关键作用，占全球能源相关温室气体排放的75%。在德克萨斯州的休斯顿，道路交通占气候行动计划中基准排放的48%。到2050年实现净零排放，气候行动计划目标是比2014年基准减少70%的排放，30%通过可再生能源抵消。这一目标具有挑战性，因为休斯顿是低密度、汽车依赖型城市，89%的道路排放来自汽车和小型卡车，公共交通使用有限。社会经济差异进一步制约了零排放车辆的采用。策略重点在于扩大ZEV获取渠道并通过公共交通改善和城市设计将车辆行驶里程减少20%。本文介绍了建立道路排放基准的方法和评估政策，这些政策利用社会经济指标和智能交通系统加速ZEV采用并减少VMT。智能停车、公共交通激励、安全数据系统和ZEV车队管理支持模式分割和系统可靠性的改善。分析了政策选项并确定了潜在行动。为支持评估，在Unity 3D中开发了模拟环境，实现了城市动态建模和政策情景可视化。依赖汽车的城市为实现2050年排放目标可从讨论的指标、度量和技术中受益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.</description>
      <author>example@mail.com (Mulham Fawakherji, Bruce Race, Driss Benhaddou)</author>
      <guid isPermaLink="false">2512.11870v2</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration</title>
      <link>http://arxiv.org/abs/2512.13903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了人机协作中随机人类运动预测的局限性，通过整合人类和机器人运动信息提高预测质量，同时保持实时性。&lt;h4&gt;背景&lt;/h4&gt;随机人类运动预测在工业再制造人机协作中至关重要，可捕捉人类运动的不确定性和多模态行为。早期方法产生不现实运动，近期方法关注准确性和实时性但仍有改进空间，且现有研究常孤立考虑人类运动，忽略机器人运动的影响。&lt;h4&gt;目的&lt;/h4&gt;解决研究空白，实现实时、逼真且交互感知的人类运动预测。&lt;h4&gt;方法&lt;/h4&gt;提出预测-精炼框架，整合人类和机器人观察运动精炼预训练预测器的初始预测，采用Flow Matching结构处理不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在人机协作桌面拆卸数据集上实验表明，方法显著提高预测准确性，保留人类运动的不确定性和多模态性，总推理时间在预算内。&lt;h4&gt;结论&lt;/h4&gt;该方法具有有效性和实用性，适合实际应用场景。&lt;h4&gt;翻译&lt;/h4&gt;随机人类运动预测对于工业再制造中安全有效的人机协作至关重要，因为它捕捉了人类运动的不确定性和多模态行为，而确定性方法无法处理这一点。虽然早期工作强调高度多样化的预测，但它们常常产生不现实的人类运动。最近的方法关注准确性和实时性能，然而在不超过时间预算的情况下，仍有进一步提高预测质量的潜力。此外，人机协作中随机人类运动预测的当前研究通常孤立地考虑人类运动，忽略了机器人运动对人类行为的影响。为了解决这些研究空白并实现实时、逼真且交互感知的人类运动预测，我们提出了一种新颖的预测-精炼框架，该框架整合人类和机器人的观察运动来精炼预训练的最先进预测器产生的初始预测。精炼模块采用Flow Matching结构来考虑不确定性。在人机协作桌面拆卸数据集上的实验研究表明，我们的方法显著提高了预测准确性，同时保留了人类运动的不确定性和多模态性。此外，所提出框架的总推理时间保持在时间预算内，突显了我们方法的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.</description>
      <author>example@mail.com (Sibo Tian, Minghui Zheng, Xiao Liang)</author>
      <guid isPermaLink="false">2512.13903v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>MMGR: Multi-Modal Generative Reasoning</title>
      <link>http://arxiv.org/abs/2512.14691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了MMGR（多模态生成推理评估与基准），一个基于五种推理能力的评估框架，用于评估生成模型在物理、逻辑和空间约束方面的表现。&lt;h4&gt;背景&lt;/h4&gt;视频基础模型能生成视觉逼真和时间一致的内容，但作为世界模拟器的可靠性取决于它们能否捕捉物理、逻辑和空间约束。现有指标如Frechet Video Distance强调感知质量而忽略推理失败。&lt;h4&gt;目的&lt;/h4&gt;引入MMGR评估框架，全面评估生成模型在多种推理任务上的表现，填补现有评估方法在推理能力评估上的空白。&lt;h4&gt;方法&lt;/h4&gt;MMGR基于五种推理能力构建：物理、逻辑、3D空间、2D空间和时间。评估三个领域：抽象推理（ARC-AGI、数独）、具身导航（现实世界3D导航和定位）和物理常识（运动和组合交互）。应用细粒度指标要求视频和图像生成中整体正确性。&lt;h4&gt;主要发现&lt;/h4&gt;对领先视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）的基准测试显示各领域存在明显性能差距。模型在物理常识任务上表现中等，但在抽象推理上表现不佳（ARC-AGI准确率低于10%），在具身环境中长程空间规划方面存在困难。&lt;h4&gt;结论&lt;/h4&gt;当前模型存在关键局限性：过度依赖感知数据、弱全局状态一致性，以及奖励视觉合理性而非因果正确性。MMGR提供统一诊断基准，为推理感知的生成世界模型发展指明方向。&lt;h4&gt;翻译&lt;/h4&gt;视频基础模型生成视觉上逼真和时间上一致的内容，但它们作为世界模拟器的可靠性取决于它们是否能够捕捉物理、逻辑和空间约束。现有的指标如Frechet Video Distance强调感知质量而忽略了推理失败，包括因果性、物理学和全局一致性的违反。我们介绍了MMGR（多模态生成推理评估与基准），这是一个基于五种推理能力的评估框架：物理、逻辑、3D空间、2D空间和时间。MMGR评估三个领域的生成推理：抽象推理（ARC-AGI、数独）、具身导航（现实世界3D导航和定位）和物理常识（运动和组合交互）。MMGR应用细粒度指标，要求在视频和图像生成中整体正确性。我们对领先的视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）进行了基准测试，发现在各领域存在明显的性能差距。模型在物理常识任务上表现出中等成功，但在抽象推理上表现不佳（ARC-AGI准确率低于10%），并且在具身环境中的长程空间规划方面存在困难。我们的分析揭示了当前模型的关键局限性，包括过度依赖感知数据、弱全局状态一致性，以及奖励视觉合理性而非因果正确性的目标。MMGR提供了一个统一的诊断基准，并为推理感知的生成世界模型发展指明了方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有视频生成模型虽然能生成视觉上引人注目的内容，但无法内化和遵守现实世界的物理、逻辑和空间约束的问题。这个问题很重要，因为随着生成AI在电影制作、科学可视化、机器人等领域的应用，如果模型不理解现实约束，会产生严重错误，限制其可靠性和实用性。同时，现有评估指标（如FVD）主要关注视觉保真度，无法检测这些推理失败，导致对模型能力的误判。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于人类认知理论，提出世界模拟需要五种核心推理能力（物理、逻辑、3D空间、2D空间和时间），并基于此设计了MMGR评估框架。他们借鉴了认知科学中的核心知识理论，参考了Matterport3D等导航数据集，利用了VideoPhy本体论评估物理常识，并改编了ARC-AGI等抽象推理基准用于生成模型评估。这些现有工作为MMGR提供了理论基础和任务设计灵感。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是基于五种推理能力框架开发一个全面的基准测试套件，评估生成模型在抽象推理、具身导航和物理常识三个领域的推理能力。整体流程包括：1)定义三个领域的具体任务；2)创建多样化的测试样本并控制难度；3)使用先进生成模型创建响应；4)通过VLM评估器和人工评估进行评估；5)分析模型表现并识别优势和局限性。评估强调整体正确性而非部分成功，使用细粒度领域特定指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于五种推理能力的系统性评估框架；2)首个同时评估视频和图像生成模型推理能力的多模态基准；3)抽象推理、具身导航和物理常识三个互补评估领域；4)要求整体正确性的细粒度评估指标；5)通过分析识别训练数据不平衡、架构弱点等局限性。相比之前工作，MMGR超越了传统感知保真度评估，关注推理能力；从理解转向生成评估；强调长时间范围内的全局一致性；首次统一评估不同模态模型，揭示性能不对称性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MMGR通过全面的评估框架和基准测试，首次系统揭示了当前多模态生成模型在物理、逻辑和空间推理方面的关键局限性，为开发真正能理解和模拟现实世界约束的下一代生成模型指明了方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.</description>
      <author>example@mail.com (Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Xiao Wen, Jiuxiang Gu, Nanyun Peng, Junjie Hu)</author>
      <guid isPermaLink="false">2512.14691v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images</title>
      <link>http://arxiv.org/abs/2512.14640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了首个多中心淋巴瘤基准测试数据集，评估了五种病理学基础模型与两种多实例学习聚合器在不同放大倍率下的性能，并探讨了模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;及时准确的淋巴瘤诊断对癌症治疗至关重要。标准诊断结合HE染色切片与多种测试确定淋巴瘤亚型，需要昂贵设备、专业人员并导致治疗延迟。深度学习方法可从常规HE染色切片提取诊断信息协助病理学家，但缺乏多中心数据上的淋巴瘤亚型分型基准测试。&lt;h4&gt;目的&lt;/h4&gt;创建首个覆盖四种常见淋巴瘤亚型和健康对照组织的多中心淋巴瘤基准测试数据集，并系统评估不同模型在多种放大倍率下的性能。&lt;h4&gt;方法&lt;/h4&gt;评估五种公开病理学基础模型(H-optimus-1, H0-mini, Virchow2, UNI2, Titan)与两种多实例学习聚合器(基于注意力的AB-MIL和基于transformer的TransMIL)在三种放大倍率(10x, 20x, 40x)下的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在分布内测试集上，模型在所有放大倍率下多类平衡准确率超过80%，所有基础模型表现相似，两种聚合方法结果相当；40x分辨率足够，更高分辨率无性能提升；但在分布外测试集上性能降至约60%，显示显著泛化挑战。&lt;h4&gt;结论&lt;/h4&gt;为推进该领域，需要覆盖更多罕见淋巴瘤亚型的更大规模多中心研究，研究提供了自动化基准测试流程促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;及时准确的淋巴瘤诊断对于指导癌症治疗至关重要。标准诊断实践结合了苏木精-伊红(HE)染色全切片图像与免疫组织化学、流式细胞术和分子遗传学测试来确定淋巴瘤亚型，这一过程需要昂贵的设备、熟练的人员，并导致治疗延迟。深度学习方法可以通过从常规可用的HE染色切片中提取诊断信息来协助病理学家，但在多中心数据上进行淋巴瘤亚型分型的全面基准测试仍然缺乏。在这项工作中，我们提出了首个覆盖四种常见淋巴瘤亚型和健康对照组织的多中心淋巴瘤基准测试数据集。我们系统地评估了五种公开的病理学基础模型(H-optimus-1, H0-mini, Virchow2, UNI2, Titan)与基于注意力的(AB-MIL)和基于transformer的(TransMIL)多实例学习聚合器在三种放大倍率(10x, 20x, 40x)下的性能。在分布内测试集上，模型在所有放大倍率下都实现了超过80%的多类平衡准确率，所有基础模型表现相似，两种聚合方法也显示出相当的结果。放大倍率研究表明，40x分辨率足够，更高分辨率或跨放大倍率聚合没有带来性能提升。然而，在分布外测试集上，性能显著下降到约60%，突显了显著的泛化挑战。为了推进该领域，需要覆盖更多罕见淋巴瘤亚型的更大规模多中心研究。我们提供了一个自动化基准测试流程，以促进未来的此类研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.</description>
      <author>example@mail.com (Rao Muhammad Umer, Daniel Sens, Jonathan Noll, Christian Matek, Lukas Wolfseher, Rainer Spang, Ralf Huss, Johannes Raffler, Sarah Reinke, Wolfram Klapper, Katja Steiger, Kristina Schwamborn, Carsten Marr)</author>
      <guid isPermaLink="false">2512.14640v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency</title>
      <link>http://arxiv.org/abs/2512.14499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ReVision视网膜基础模型，通过真实医疗实践中的远程医疗数据直接学习临床原生智能，无需大量任务特定优化即可在资源有限环境中高效部署。&lt;h4&gt;背景&lt;/h4&gt;当前视网膜基础模型受限于缺乏真实临床环境的研究数据集，需要针对每个应用进行大量特定任务优化，限制了在资源有限环境中的部署效率。&lt;h4&gt;目的&lt;/h4&gt;克服现有模型的局限性，直接从真实医疗实践中构建临床原生智能，实现医疗AI系统在资源有限环境中的高效部署。&lt;h4&gt;方法&lt;/h4&gt;利用大规模远程医疗项目作为学习资源库，构建ReVision视网膜基础模型，从485,980张彩色眼底照片和相应诊断报告的自然对齐中学习，这些数据通过跨越中国162家医疗机构的十年远程医疗项目积累。&lt;h4&gt;主要发现&lt;/h4&gt;ReVision在27个眼科基准测试中实现高效部署；无需特定任务训练，在12个公共基准测试上实现零样本疾病检测，平均AUROC为0.946，在3个独立临床队列上为0.952；在最小适应情况下，与大量微调的替代方案相匹配，但需要的参数和标记示例少几个数量级；学习到的表示能有效迁移到新的临床环境和任务；在33名眼科医生的研究中，提高诊断准确率14.8%。&lt;h4&gt;结论&lt;/h4&gt;临床原生智能可以直接从临床档案中提取，无需额外注释，构建适合各种资源有限环境的医疗AI系统。&lt;h4&gt;翻译&lt;/h4&gt;当前视网膜基础模型仍受限于缺乏真实临床环境的研究数据集，需要针对每个应用进行大量特定任务优化，限制了它们在资源有限环境中的部署效率。在这里，我们表明这些障碍可以通过直接从真实医疗实践中构建临床原生智能来克服。我们的关键见解是，大型远程医疗项目（专家中心为分散设施提供远程咨询）代表了学习临床图像解释的自然资源库。我们提出了ReVision，一个视网膜基础模型，从485,980张彩色眼底照片及其相应诊断报告的自然对齐中学习，这些数据通过跨越中国162家医疗机构的十年远程医疗项目积累。通过对27个眼科基准测试的广泛评估，我们证明ReVision能够以最少的本地资源实现高效部署。无需任何特定任务训练，ReVision在12个公共基准测试上实现零样本疾病检测，平均AUROC为0.946，在3个独立临床队列上为0.952。当最小适应可行时，ReVision与大量微调的替代方案相匹配，但需要的可训练参数和标记示例少几个数量级。学习到的表示也有效迁移到新的临床站点、成像领域、成像方式和系统性健康预测任务。在33名眼科医生的前瞻性读者研究中，ReVision的零样本帮助提高了各经验水平医生的诊断准确率14.8%。这些结果表明，临床原生智能可以直接从临床档案中提取，无需任何进一步注释，来构建适合各种资源有限环境的医疗AI系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.</description>
      <author>example@mail.com (Jia Guo, Jiawei Du, Shengzhu Yang, Shuai Lu, Wenquan Cheng, Kaiwen Zhang, Yihua Sun, Chuhong Yang, Weihang Zhang, Fang Chen, Yilan Wu, Lie Ju, Guochen Ning, Longfei Ma, Huiping Yao, Jinyuan Wang, Peilun Shi, Yukun Zhou, Jie Xu, Pearse A. Keane, Hanruo Liu, Hongen Liao, Ningli Wang, Huiqi Li)</author>
      <guid isPermaLink="false">2512.14499v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</title>
      <link>http://arxiv.org/abs/2512.14442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了A4-Agent，一个无需训练的智能体框架，通过解耦可预测性预测为三个阶段，显著提升了在新物体和未见环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;可预测性预测对具身AI至关重要，但当前端到端模型将高级推理和低级定位耦合在单一流水线中，依赖标注数据训练，导致泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;超越当前范式，提出一个无需训练的智能体框架，将可预测性预测解耦为三阶段流水线，提升模型泛化能力。&lt;h4&gt;方法&lt;/h4&gt;A4-Agent框架在测试时协调三个专门的基础模型：Dreamer使用生成模型可视化交互过程；Thinker利用视觉-语言模型决定交互对象部位；Spotter协调视觉模型精确定位交互区域。整个框架无需任务特定微调。&lt;h4&gt;主要发现&lt;/h4&gt;通过利用预训练模型的互补优势，该零样本框架在多个基准测试中显著优于最先进的监督方法，并展现出对真实世界设置的强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;A4-Agent通过解耦预测过程并利用预训练模型，实现了更好的泛化能力，代表了可预测性预测的新范式。&lt;h4&gt;翻译&lt;/h4&gt;可预测性预测是基于语言指令识别物体上交互区域的过程，对具身AI至关重要。当前主流的端到端模型将高级推理和低级定位耦合到单一流水线中，依赖标注数据训练，导致在新物体和未见环境中泛化能力差。本文提出了A4-Agent，一个无需训练的智能体框架，将可预测性预测解耦为三阶段流水线：Dreamer使用生成模型可视化交互如何呈现；Thinker利用大型视觉-语言模型决定与物体哪部分交互；Spotter协调视觉基础模型精确定位交互区域。通过利用预训练模型的互补优势，该零样本框架在多个基准测试中显著优于最先进的监督方法，并表现出对真实世界设置的强大泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.</description>
      <author>example@mail.com (Zixin Zhang, Kanghao Chen, Hanqing Wang, Hongfei Zhang, Harold Haodong Chen, Chenfei Liao, Litao Guo, Ying-Cong Chen)</author>
      <guid isPermaLink="false">2512.14442v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation</title>
      <link>http://arxiv.org/abs/2512.14358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages(/wo references), 4 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了TiCard，一个低侵入性、基于修正的框架，用于增强数据库的原生基数估计器，解决了传统估计器无法捕捉相关性和学习型估计器需要侵入性集成的问题。&lt;h4&gt;背景&lt;/h4&gt;基数估计是基于成本的查询优化的关键瓶颈，但可部署的改进仍然困难。传统的估计器无法捕捉相关性，而学习型估计器通常需要工作负载特定的训练流程，并且需要侵入性地集成到优化器中。&lt;h4&gt;目的&lt;/h4&gt;提出TiCard框架，这是一个低侵入性、基于修正的框架，用于增强(而非替换)数据库的原生估计器，提高基数估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;TiCard仅使用EXPLAIN功能学习乘法残差修正，并仅使用EXPLAIN ANALYZE进行离线标签。研究两种实际实现：(i)梯度提升回归器，用于亚毫秒级推理；(ii)TabPFN，一种上下文表格基础模型，通过刷新小型参考集来适应，无需梯度重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;在TiDB上使用TPCH和连接顺序基准测试，在低跟踪设置下，TiCard显著提高了操作级别的尾部准确性：P90 Q-error从312.85降至13.69，P99从37,974.37降至3,416.50，而仅连接策略保留了接近完美的中位数行为。&lt;h4&gt;结论&lt;/h4&gt;将TiCard定位为专注于可部署性的AI4DB构建块：明确范围、保守的集成策略，以及从离线修正到优化器内使用的集成路线图。&lt;h4&gt;翻译&lt;/h4&gt;基数估计是基于成本的查询优化的关键瓶颈，但可部署的改进仍然困难：传统估计器无法捕捉相关性，而学习型估计器通常需要工作负载特定的训练流程和侵入性集成到优化器中。本文介绍了TiCard，一个低侵入性、基于修正的框架，它增强(而非替换)数据库的原生估计器。TiCard使用仅EXPLAIN功能学习乘法残差修正，并仅使用EXPLAIN ANALYZE进行离线标签。我们研究了两种实际实现：(i)用于亚毫秒级推理的梯度提升回归器，和(ii)TabPFN，一种通过刷新小型参考集来适应而无需梯度重新训练的上下文表格基础模型。在TiDB上使用TPCH和连接顺序基准测试，在低跟踪设置下(总共执行263次；157次用于学习)，TiCard显著提高了操作级别的尾部准确性：P90 Q-error从312.85(原生)降至13.69(TiCard-GBR)，P99从37,974.37降至3,416.50(TiCard-TabPFN)，而仅连接策略保留了接近完美的中位数行为。我们将TiCard定位为专注于可部署性的AI4DB构建块：明确范围、保守的集成策略，以及从离线修正到优化器内使用的集成路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.</description>
      <author>example@mail.com (Qizhi Wang)</author>
      <guid isPermaLink="false">2512.14358v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2512.14253v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FLAME是一个轻量级时间序列基础模型家族，支持确定性和概率性预测，通过Legendre Memory及其变体实现高效和准确的预测。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测需要能够处理长期依赖关系并保持计算效率的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个轻量级但功能强大的时间序列基础模型，支持确定性和概率性预测，同时保持高效和稳健性。&lt;h4&gt;方法&lt;/h4&gt;FLAME利用Legendre Memory及其变体（平移Legendre和缩放Legendre）在编码和解码阶段捕捉数据中的归纳偏置，并采用基于归一化流的预测头来建模预测范围内的复杂分布。&lt;h4&gt;主要发现&lt;/h4&gt;FLAME在TSFM-Bench和ProbTS等基准测试上实现了最先进的零样本性能，在确定性和概率性预测任务上表现一致优异。&lt;h4&gt;结论&lt;/h4&gt;FLAME是一个高效且稳健的时间序列基础模型，通过结合Legendre Memory和归一化流预测头，实现了在确定性和概率性预测任务上的最先进性能。&lt;h4&gt;翻译&lt;/h4&gt;这项工作中，我们引入了FLAME，一个极其轻量且功能强大的时间序列基础模型家族，它通过生成概率建模支持确定性和概率性预测，从而确保了效率和稳健性。FLAME利用Legendre Memory获得强大的泛化能力。通过在编码和解码阶段调整Legendre Memory的变体，即平移Legendre (LegT)和缩放Legendre (LegS)，FLAME可以有效捕捉数据中的固有归纳偏置，并进行高效的长程推理。为了在保持高效的同时提高概率预测的准确性，FLAME采用了基于归一化流的预测头，可以以生成方式对预测范围内的任意复杂分布进行建模。在TSFM-Bench和ProbTS等公认基准上的全面实验表明，FLAME在确定性和概率性预测任务上 consistently达到了最先进的零样本性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.</description>
      <author>example@mail.com (Xingjian Wu, Hanyin Cheng, Xiangfei Qiu, Zhengyu Li, Jilin Hu, Chenjuan Guo, Bin Yang)</author>
      <guid isPermaLink="false">2512.14253v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control</title>
      <link>http://arxiv.org/abs/2512.14106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Supplementary materials, datasets, and implementation code will be made publicly available upon acceptance for publication in a peer-reviewed journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究介绍了HydroGEM，一个用于大陆尺度河流流量质量控制的基础模型，它通过两阶段训练和混合TCN-Transformer架构实现了高效的数据质量检测和重建。&lt;h4&gt;背景&lt;/h4&gt;实时河流流量监测网络每年产生数百万观测数据，但在维护数千个远程传感器的数据质量方面仍然需要大量人工劳动。&lt;h4&gt;目的&lt;/h4&gt;开发一个基础模型HydroGEM用于大陆尺度的河流流量质量控制，减少人工维护工作量，提高数据质量检测和重建的效率。&lt;h4&gt;方法&lt;/h4&gt;HydroGEM采用两阶段训练：首先在6.03万个美国地质调查局站点的序列上进行自监督预训练学习水文表示，然后使用合成异常进行微调以进行检测和重建。模型采用混合TCN-Transformer架构（1420万参数），捕获局部时间模式和长程依赖关系，同时使用分层归一化处理六个数量级的流量变化。&lt;h4&gt;主要发现&lt;/h4&gt;在包含799个站点和18种专家验证的异常类型的保留合成测试中，HydroGEM的检测F1得分为0.792，重建误差减少了68.7%，比现有方法提高了36.3%。在100个加拿大环境和气候变化站点的零样本迁移测试中，F1得分为0.586，超过了所有基线，展示了跨国家泛化能力。模型在不同校正幅度下保持一致的检测效果，并与操作季节性模式保持一致。&lt;h4&gt;结论&lt;/h4&gt;HydroGME设计用于人机协同工作流程，输出的是需要专家审核的质量控制建议，而不是自主校正。&lt;h4&gt;翻译&lt;/h4&gt;实时河流流量监测网络每年产生数百万观测数据，但在维护数千个远程传感器的数据质量方面仍然需要大量人工劳动。我们介绍了HydroGEM（水文监测通用编码器），这是一个用于大陆尺度河流流量质量控制的基础模型。HydroGEM使用两阶段训练：在3724个美国地质调查局站点的603万个序列上进行自监督预训练学习水文表示，然后使用合成异常进行微调以进行检测和重建。混合TCN-Transformer架构（1420万参数）捕获局部时间模式和长程依赖关系，而分层归一化处理六个数量级的流量变化。在包含799个站点和18种专家验证的异常类型的保留合成测试中，HydroGEM的检测F1得分为0.792，重建误差减少了68.7%，比现有方法提高了36.3%。在100个加拿大环境和气候变化站点的零样本迁移测试中，F1得分为0.586，超过了所有基线，展示了跨国家泛化能力。模型在不同校正幅度下保持一致的检测效果，并与操作季节性模式保持一致。HydroGME设计用于人机协同工作流程，输出的是需要专家审核的质量控制建议，而不是自主校正。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.</description>
      <author>example@mail.com (Ijaz Ul Haq, Byung Suk Lee, Julia N. Perdrial, David Baude)</author>
      <guid isPermaLink="false">2512.14106v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries</title>
      <link>http://arxiv.org/abs/2512.14102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为RUNE（基于神经符号实体的推理）的新方法，结合大语言模型和神经符号AI进行遥感文本到图像检索，通过显式推理提高性能和可解释性。&lt;h4&gt;背景&lt;/h4&gt;随着针对航空和卫星图像的大视觉语言模型兴起，遥感文本到图像检索技术快速发展，但有限的可解释性和复杂空间关系处理能力仍是实际应用中的主要挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有遥感大视觉语言模型的可解释性差和复杂空间关系处理能力不足的问题，提高遥感图像检索的性能、鲁棒性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;RUNE结合大语言模型和神经符号AI，将文本查询转换为一阶逻辑表达式，通过推理检测到的实体与逻辑表达式间的兼容性检索图像；提出逻辑分解策略在实体子集上操作，缩短执行时间；仅利用基础模型生成逻辑表达式，将推理任务委托给神经符号推理模块。&lt;h4&gt;主要发现&lt;/h4&gt;RUNE在重新利用的DOTA数据集上表现出优越性能；引入查询复杂度检索鲁棒性(RRQC)和图像不确定性检索鲁棒性(RRIU)两个新指标；在复杂遥感检索任务中优于联合嵌入模型，在性能、鲁棒性和可解释性方面都有提升；通过洪水后卫星图像检索用例展示了现实应用潜力。&lt;h4&gt;结论&lt;/h4&gt;RUNE通过结合大语言模型和神经符号AI，有效解决了遥感文本到图像检索中的可解释性和复杂空间关系处理问题，为现实世界遥感应用提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;遥感(RS)领域的文本到图像检索随着专门针对航空和卫星图像的大视觉语言模型(LVLMs)的兴起而快速发展，最终形成了遥感大视觉语言模型(RS-LVLMs)。然而，有限的可解释性和对复杂空间关系的处理能力不足仍然是实际应用中的主要挑战。为了解决这些问题，我们引入了RUNE（基于神经符号实体的推理），一种结合大语言模型(LLMs)和神经符号AI的方法，通过推理检测到的实体与从文本查询推导出的一阶逻辑(FOL)表达式之间的兼容性来检索图像。与依赖隐式联合嵌入的RS-LVLMs不同，RUNE执行明确的推理，提高了性能和可解释性。为了扩展性，我们提出了一种逻辑分解策略，在检测到的实体子集上操作，保证了比神经方法更短的执行时间。我们仅利用基础模型生成FOL表达式，将推理任务委托给神经符号推理模块。为了评估，我们重新利用了原本用于目标检测的DOTA数据集，通过添加比现有基准更复杂的查询来增强它。我们展示了LLM在文本到逻辑翻译方面的有效性，并与最先进的RS-LVLMs进行了比较，证明了RUNE的优越性能。我们引入了两个指标：查询复杂度检索鲁棒性(RRQC)和图像不确定性检索鲁棒性(RRIU)，它们评估了相对于查询复杂度和图像不确定性的性能。在复杂的遥感检索任务中，RUNE优于联合嵌入模型，在性能、鲁棒性和可解释性方面都有所提升。我们通过洪水后卫星图像检索的用例，展示了RUNE在现实世界遥感应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.</description>
      <author>example@mail.com (Emanuele Mezzi, Gertjan Burghouts, Maarten Kruithof)</author>
      <guid isPermaLink="false">2512.14102v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Frameworks for Real-World Audio-Visual Speech Recognition</title>
      <link>http://arxiv.org/abs/2512.14083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD Dissertation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对音频-视觉语音识别系统在现实环境中因噪声和干扰导致的性能下降问题，提出了一种系统化的分层解决方案，在表示、架构和系统三个层面实现鲁棒可扩展性。&lt;h4&gt;背景&lt;/h4&gt;音频-视觉语音识别系统在实际部署中面临严重性能下降问题，主要源于现实环境中不可预测的声学噪声和视觉干扰。&lt;h4&gt;目的&lt;/h4&gt;通过系统化的分层方法克服这些挑战，在表示、架构和系统层面实现稳健的可扩展性。&lt;h4&gt;方法&lt;/h4&gt;在表示层面：研究构建统一模型的方法，学习对多样化现实世界干扰具有内在鲁棒性的音频-视觉特征；在架构层面：探索如何有效扩展模型容量，同时确保多模态输入的自适应和可靠使用；在系统层面：通过与大尺度基础模型的模块化集成扩展系统功能，利用其强大的认知和生成能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过在这三个层面提供系统化的解决方案，可以构建下一代具有高可靠性的稳健可扩展AVSR系统。&lt;h4&gt;结论&lt;/h4&gt;该研究旨在构建一个在现实应用中具有高可靠性的下一代、稳健且可扩展的AVSR系统。&lt;h4&gt;翻译&lt;/h4&gt;音频-视觉语音识别(AVSR)系统的实际部署从根本上受到现实环境中显著性能下降的挑战，这些环境以不可预测的声学噪声和视觉干扰为特征。本论文认为，系统化的分层方法是克服这些挑战所必需的，在表示、架构和系统层面实现稳健的可扩展性。在表示层面，我们研究构建统一模型的方法，该模型学习对多样化现实世界干扰具有内在鲁棒性的音频-视觉特征，从而使模型能够推广到新环境而无需专门模块。为解决架构可扩展性，我们探索如何有效扩展模型容量，同时确保多模态输入的自适应和可靠使用，开发了一种基于输入特征智能分配计算资源的框架。最后，在系统层面，我们提出通过与大尺度基础模型的模块化集成扩展系统功能的方法，利用其强大的认知和生成能力最大化最终识别精度。通过在这三个层面中的每个层面提供系统化的解决方案，本论文旨在构建一个下一代、稳健且可扩展的AVSR系统，在现实应用中具有高可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The practical deployment of Audio-Visual Speech Recognition (AVSR) systems is fundamentally challenged by significant performance degradation in real-world environments, characterized by unpredictable acoustic noise and visual interference. This dissertation posits that a systematic, hierarchical approach is essential to overcome these challenges, achieving the robust scalability at the representation, architecture, and system levels. At the representation level, we investigate methods for building a unified model that learns audio-visual features inherently robust to diverse real-world corruptions, thereby enabling generalization to new environments without specialized modules. To address architectural scalability, we explore how to efficiently expand model capacity while ensuring the adaptive and reliable use of multimodal inputs, developing a framework that intelligently allocates computational resources based on the input characteristics. Finally, at the system level, we present methods to expand the system's functionality through modular integration with large-scale foundation models, leveraging their powerful cognitive and generative capabilities to maximize final recognition accuracy. By systematically providing solutions at each of these three levels, this dissertation aims to build a next-generation, robust, and scalable AVSR system with high reliability in real-world applications.</description>
      <author>example@mail.com (Sungnyun Kim)</author>
      <guid isPermaLink="false">2512.14083v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value</title>
      <link>http://arxiv.org/abs/2512.14051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpenDataArena (ODA)是一个全面的开放平台，用于基准测试训练后数据的内在价值，解决了大型语言模型训练数据不透明的问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)的快速发展依赖于训练后数据集的质量和多样性，但这些数据集仍是'黑盒'，其组成、来源和评估都缺乏透明度，阻碍了可重复性并模糊了数据特征与模型行为之间的因果关系。&lt;h4&gt;目的&lt;/h4&gt;引入OpenDataArena (ODA)平台，建立训练后数据评估的全面生态系统，推动数据为中心AI的研究从试错方法转向原则性科学。&lt;h4&gt;方法&lt;/h4&gt;ODA建立四个关键支柱：(i)统一的训练-评估管道确保跨模型和领域的公平比较；(ii)多维评分框架沿数十个轴评估数据质量；(iii)交互式数据血统探索器可视化数据集谱系；(iv)完全开源的工具包促进数据研究。&lt;h4&gt;主要发现&lt;/h4&gt;分析揭示了数据复杂性与任务性能之间的固有权衡；通过血统追踪确定了流行基准测试中的冗余；绘制了数据集之间的谱系关系图。&lt;h4&gt;结论&lt;/h4&gt;ODA平台和所有相关资源已公开发布，旨在促进高质量数据评估的民主化访问，推动数据为中心AI的科学化发展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)的快速发展依赖于训练后数据集的质量和多样性。然而，一个关键的两难问题仍然存在：虽然模型受到严格的基准测试，但支撑它们的数据仍然是黑盒——其特征是不透明的组成、不确定的来源和缺乏系统评估。这种不透明性阻碍了可重复性，并模糊了数据特征与模型行为之间的因果关系。为了弥合这一差距，我们引入了OpenDataArena (ODA)，一个全面的开放平台，用于基准测试训练后数据的内在价值。ODA建立了由四个关键支柱组成的全面生态系统：(i)统一的训练-评估管道，确保在不同模型(如Llama、Qwen)和领域间进行公平、开放的比较；(ii)多维评分框架，沿数十个不同轴描绘数据质量；(iii)交互式数据血统探索器，可视化数据集谱系并分析组成部分来源；(iv)完全开源的工具包，用于训练、评估和评分，促进数据研究。在ODA上进行的大量实验——涵盖多个领域120多个训练数据集、22个基准测试，由600多次训练运行和4000万个处理数据点验证——揭示了重要见解。我们的分析揭示了数据复杂性与任务性能之间的固有权衡，通过血统追踪确定了流行基准测试中的冗余，并绘制了数据集之间的谱系关系。我们发布了所有结果、工具和配置，以促进对高质量数据评估的民主化访问。ODA的目标不仅仅是扩展排行榜，而是推动数据策从试错方法转向数据为中心AI的原则性科学，为数据混合规律和基础模型的战略组合的严谨研究铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.</description>
      <author>example@mail.com (Mengzhang Cai, Xin Gao, Yu Li, Honglin Lin, Zheng Liu, Zhuoshi Pan, Qizhi Pei, Xiaoran Shang, Mengyuan Sun, Zinan Tang, Xiaoyang Wang, Zhanping Zhong, Yun Zhu, Dahua Lin, Conghui He, Lijun Wu)</author>
      <guid isPermaLink="false">2512.14051v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training</title>
      <link>http://arxiv.org/abs/2512.13996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DTop-p MoE的新型稀疏专家混合架构，通过动态调整概率阈值来控制稀疏性，解决了传统Top-k路由策略的局限性，并在大型语言模型和扩散Transformer实验中表现出优于基线方法的效果。&lt;h4&gt;背景&lt;/h4&gt;稀疏专家混合(MoE)架构通过只激活每个输入令牌的专家子集来有效扩展模型容量。然而，标准的Top-k路由策略采用统一的稀疏模式，忽略了令牌难度的差异。虽然Top-p路由提供了灵活的替代方案，但现有实现通常依赖固定的全局概率阈值，导致计算成本不可控且对超参数选择敏感。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够控制稀疏性的动态Top-p路由机制，解决优化不可微分阈值的问题，并允许不同层学习不同的专家选择模式，同时利用全局概率阈值。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了DTop-p MoE，一种稀疏可控的动态Top-p路由机制；2. 使用比例-积分(PI)控制器动态调整概率阈值，使运行中的激活专家稀疏度与指定目标保持一致；3. 引入动态路由归一化机制，适应层间路由logits，允许不同层学习不同的专家选择模式。&lt;h4&gt;主要发现&lt;/h4&gt;1. DTop-p在大型语言模型和扩散Transformer上的实验中，始终优于Top-k和固定阈值的Top-p基线方法；2. DTop-p能够保持对激活专家数量的精确控制，同时自适应地分配不同令牌和层的资源；3. DTop-p在专家粒度、专家容量、模型大小和数据集大小方面表现出良好的扩展性。&lt;h4&gt;结论&lt;/h4&gt;DTop-p为大规模MoE预训练提供了一个强大的框架，能够有效平衡模型容量和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;稀疏专家混合(MoE)架构通过仅激活每个输入令牌的专家子集来有效扩展模型容量。然而，标准的Top-k路由策略施加了统一的稀疏模式，忽略了令牌难度的差异。虽然Top-p路由提供了灵活的替代方案，但现有实现通常依赖固定的全局概率阈值，导致计算成本不可控且对超参数选择敏感。在本文中，我们提出了DTop-p MoE，一种稀疏可控的动态Top-p路由机制。为了解决优化不可微分阈值的挑战，我们利用比例-积分(PI)控制器动态调整概率阈值，使运行中的激活专家稀疏度与指定目标保持一致。此外，我们引入了动态路由归一化机制，适应层间路由logits，允许不同层学习不同的专家选择模式，同时利用全局概率阈值。在大型语言模型和扩散Transformer上的大量实验表明，DTop-p始终优于Top-k和固定阈值Top-p基线。我们的分析确认，DTop-p在保持对激活专家数量的精确控制的同时，自适应地分配不同令牌和层的资源。此外，DTop-p在专家粒度、专家容量、模型大小和数据集大小方面表现出强大的扩展性，为大规模MoE预训练提供了强大的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.</description>
      <author>example@mail.com (Can Jin, Hongwu Peng, Mingcan Xiang, Qixin Zhang, Xiangchi Yuan, Amit Hasan, Ohiremen Dibua, Yifan Gong, Yan Kang, Dimitris N. Metaxas)</author>
      <guid isPermaLink="false">2512.13996v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Informing Acquisition Functions via Foundation Models for Molecular Discovery</title>
      <link>http://arxiv.org/abs/2512.13935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出一种无需似然性的贝叶斯优化方法，通过整合大型语言模型和化学基础模型的先验知识，显著提升分子发现过程中的可扩展性、鲁棒性和样本效率。&lt;h4&gt;背景&lt;/h4&gt;贝叶斯优化是加速分子发现的关键方法，通过估计分子到其性质的映射来寻找最优候选分子。传统方法在数据不足、先验知识有限和候选空间巨大的情况下性能受限。大型语言模型和化学基础模型虽能提供丰富先验，但高维特征、上下文学习成本高和深度贝叶斯代理模型的计算负担阻碍了其充分利用。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型和化学基础模型在增强贝叶斯优化时面临的挑战，开发一种无需显式代理建模的方法，直接利用这些模型的先验知识来指导采集函数。&lt;h4&gt;方法&lt;/h4&gt;提出一种无需似然性的贝叶斯优化方法：1)绕过显式代理建模，直接利用通用LLMs和化学特定基础模型的先验知识；2)学习分子搜索空间的树结构分区，使用局部采集函数；3)通过蒙特卡洛树搜索实现高效候选选择；4)集成基于LLMs的粗粒度聚类，将采集函数评估限制在高性质值的簇中，提高可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验和消融研究表明，该方法显著提高了LLM引导的贝叶斯优化在分子发现中的可扩展性、鲁棒性和样本效率。&lt;h4&gt;结论&lt;/h4&gt;该方法解决了传统贝叶斯优化在低数据条件和高维特征空间中的局限性，通过整合大型语言模型和化学基础模型的先验知识，实现了更高效的分子发现过程。&lt;h4&gt;翻译&lt;/h4&gt;贝叶斯优化是一种通过估计分子与其性质之间的映射来加速分子发现并寻找最优候选分子的关键方法。通常，贝叶斯优化迭代更新这种映射的概率代理模型，并优化从模型中导出的采集函数来指导分子选择。然而，在数据不足、先验知识有限和候选空间巨大的情况下，其性能受到限制。大型语言模型和化学基础模型提供了丰富的先验知识以增强贝叶斯优化，但高维特征、昂贵的上下文学习和深度贝叶斯代理模型的计算负担阻碍了它们的充分利用。为解决这些挑战，我们提出了一种无需似然性的贝叶斯优化方法，绕过显式代理建模，直接利用通用大型语言模型和化学特定基础模型的先验知识来指导采集函数。我们的方法还学习了分子搜索空间的树结构分区，并使用局部采集函数，通过蒙特卡洛树搜索实现高效的候选选择。通过进一步整合基于大型语言模型的粗粒度聚类，它通过将采集函数评估限制在具有统计上更高性质值的簇中，显著提高了对大型候选集的可扩展性。通过大量实验和消融研究，我们证明该方法显著提高了大型语言模型引导的贝叶斯优化在分子发现中的可扩展性、鲁棒性和样本效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.</description>
      <author>example@mail.com (Qi Chen, Fabio Ramos, Alán Aspuru-Guzik, Florian Shkurti)</author>
      <guid isPermaLink="false">2512.13935v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</title>
      <link>http://arxiv.org/abs/2512.13507v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Seedance 1.5 pro Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了Seedance 1.5 pro模型，这是一个专门用于原生音视频联合生成的基础模型，采用双分支Diffusion Transformer架构，实现了出色的音视频同步和高质量生成，并通过多种优化技术和加速框架提高了实用性。&lt;h4&gt;背景&lt;/h4&gt;视频生成领域的最新进展为统一的音视频生成铺平了道路。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于原生、联合音视频生成的基础模型，实现高质量的音视频同步和生成。&lt;h4&gt;方法&lt;/h4&gt;采用双分支Diffusion Transformer架构；集成交叉模态联合模块；使用专门的多阶段数据管道；实施监督微调(SFT)在高质量数据集上；应用基于人类反馈的强化学习(RLHF)和多维奖励模型；引入加速框架提高推理速度。&lt;h4&gt;主要发现&lt;/h4&gt;实现了出色的音视频同步和卓越的生成质量；通过优化技术提高了实用性；推理速度提升超过10倍；实现了精确的多语言和方言口型同步；支持动态电影摄像机控制；增强了叙事连贯性。&lt;h4&gt;结论&lt;/h4&gt;Seedance 1.5 pro是一个强大的专业级内容创作引擎，已在火山引擎上提供访问。&lt;h4&gt;翻译&lt;/h4&gt;最近视频生成领域的进展为统一的音视频生成铺平了道路。在这项工作中，我们提出了Seedance 1.5 pro，这是一个专门为原生、联合音视频生成而设计的基础模型。利用双分支Diffusion Transformer架构，该模型集成了交叉模态联合模块和专门的多阶段数据管道，实现了卓越的音视频同步和卓越的生成质量。为确保实用性，我们实施了精细的后训练优化，包括在高质量数据集上进行监督微调(SFT)和使用多维奖励模型进行基于人类反馈的强化学习(RLHF)。此外，我们还引入了一个加速框架，将推理速度提高了10倍以上。Seedance 1.5 pro以其精确的多语言和方言口型同步、动态电影摄像机控制和增强的叙事连贯性而脱颖而出，定位为专业级内容创作的强大引擎。Seedance 1.5 pro现已在火山引擎上可通过https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.</description>
      <author>example@mail.com (Heyi Chen, Siyan Chen, Xin Chen, Yanfei Chen, Ying Chen, Zhuo Chen, Feng Cheng, Tianheng Cheng, Xinqi Cheng, Xuyan Chi, Jian Cong, Jing Cui, Qinpeng Cui, Qide Dong, Junliang Fan, Jing Fang, Zetao Fang, Chengjian Feng, Han Feng, Mingyuan Gao, Yu Gao, Dong Guo, Qiushan Guo, Boyang Hao, Qingkai Hao, Bibo He, Qian He, Tuyen Hoang, Ruoqing Hu, Xi Hu, Weilin Huang, Zhaoyang Huang, Zhongyi Huang, Donglei Ji, Siqi Jiang, Wei Jiang, Yunpu Jiang, Zhuo Jiang, Ashley Kim, Jianan Kong, Zhichao Lai, Shanshan Lao, Yichong Leng, Ai Li, Feiya Li, Gen Li, Huixia Li, JiaShi Li, Liang Li, Ming Li, Shanshan Li, Tao Li, Xian Li, Xiaojie Li, Xiaoyang Li, Xingxing Li, Yameng Li, Yifu Li, Yiying Li, Chao Liang, Han Liang, Jianzhong Liang, Ying Liang, Zhiqiang Liang, Wang Liao, Yalin Liao, Heng Lin, Kengyu Lin, Shanchuan Lin, Xi Lin, Zhijie Lin, Feng Ling, Fangfang Liu, Gaohong Liu, Jiawei Liu, Jie Liu, Jihao Liu, Shouda Liu, Shu Liu, Sichao Liu, Songwei Liu, Xin Liu, Xue Liu, Yibo Liu, Zikun Liu, Zuxi Liu, Junlin Lyu, Lecheng Lyu, Qian Lyu, Han Mu, Xiaonan Nie, Jingzhe Ning, Xitong Pan, Yanghua Peng, Lianke Qin, Xueqiong Qu, Yuxi Ren, Kai Shen, Guang Shi, Lei Shi, Yan Song, Yinglong Song, Fan Sun, Li Sun, Renfei Sun, Yan Sun, Zeyu Sun, Wenjing Tang, Yaxue Tang, Zirui Tao, Feng Wang, Furui Wang, Jinran Wang, Junkai Wang, Ke Wang, Kexin Wang, Qingyi Wang, Rui Wang, Sen Wang, Shuai Wang, Tingru Wang, Weichen Wang, Xin Wang, Yanhui Wang, Yue Wang, Yuping Wang, Yuxuan Wang, Ziyu Wang, Guoqiang Wei, Wanru Wei, Di Wu, Guohong Wu, Hanjie Wu, Jian Wu, Jie Wu, Ruolan Wu, Xinglong Wu, Yonghui Wu, Ruiqi Xia, Liang Xiang, Fei Xiao, XueFeng Xiao, Pan Xie, Shuangyi Xie, Shuang Xu, Jinlan Xue, Shen Yan, Bangbang Yang, Ceyuan Yang, Jiaqi Yang, Runkai Yang, Tao Yang, Yang Yang, Yihang Yang, ZhiXian Yang, Ziyan Yang, Songting Yao, Yifan Yao, Zilyu Ye, Bowen Yu, Jian Yu, Chujie Yuan, Linxiao Yuan, Sichun Zeng, Weihong Zeng, Xuejiao Zeng, Yan Zeng, Chuntao Zhang, Heng Zhang, Jingjie Zhang, Kuo Zhang, Liang Zhang, Liying Zhang, Manlin Zhang, Ting Zhang, Weida Zhang, Xiaohe Zhang, Xinyan Zhang, Yan Zhang, Yuan Zhang, Zixiang Zhang, Fengxuan Zhao, Huating Zhao, Yang Zhao, Hao Zheng, Jianbin Zheng, Xiaozheng Zheng, Yangyang Zheng, Yijie Zheng, Jiexin Zhou, Jiahui Zhu, Kuan Zhu, Shenhan Zhu, Wenjia Zhu, Benhui Zou, Feilong Zuo)</author>
      <guid isPermaLink="false">2512.13507v2</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Robust Beamforming for Multiuser MIMO Systems with Unknown Channel Statistics: A Hybrid Offline-Online Framework</title>
      <link>http://arxiv.org/abs/2512.14165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种混合离线-在线框架，用于解决多用户多输入多输出系统中信道状态信息不完美时的稳健波束形成问题，特别是在信道估计误差统计未知的情况下。&lt;h4&gt;背景&lt;/h4&gt;在多用户多输入多输出系统中，当信道状态信息不完美且信道估计误差统计未知时，稳健波束形成是一个基本挑战。传统方法依赖误差协方差矩阵的先验知识，而深度学习方法对未见信道条件泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法的局限性，实现有效的离线学习和快速在线适应，提高系统在未知或非平稳信道条件下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出混合离线-在线框架：离线阶段使用用户共享的深度神经网络学习信道估计误差协方差，无需先验统计知识；采用稀疏增广低秩方法降低复杂度；在线阶段通过少量梯度步骤快速微调网络；引入多模型无关元学习策略维护多个元初始化并动态选择最佳初始化。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果表明，该框架在各种信道条件下表现出强大的鲁棒性，性能显著优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;混合离线-在线框架能有效解决信道状态信息不完美情况下的波束形成问题，特别是在信道估计误差统计未知的情况下具有优越性能。&lt;h4&gt;翻译&lt;/h4&gt;在信道状态信息不完美的情况下进行稳健波束形成设计是多用户多输入多输出系统中的一个基本挑战，特别是在信道估计误差统计未知的情况下。传统的模型驱动方法通常依赖于误差协方差矩阵的先验知识，而数据驱动的深度学习方法对未见过的信道条件泛化能力差。为解决这些限制，本文提出了一种混合离线-在线框架，实现了有效的离线学习和快速的在线适应。在离线阶段，我们提出了一种用户共享的深度神经网络，能够从观测样本中学习信道估计误差协方差，从而无需先验统计知识即可实现稳健波束形成。同时，为了便于实时部署，我们提出了一种稀疏增广低秩方法，在保持可比性能的同时降低复杂度。在线阶段，我们证明提出的网络可以用最少的梯度步骤进行快速微调。此外，进一步提出了一种多模型无关元学习策略，通过维护多个元初始化并在在线动态选择最佳初始化，可以提高该框架在未见或非平稳信道条件下的适应和泛化能力。模拟结果表明，提出的离线-在线框架在各种信道条件下表现出强大的鲁棒性，能够显著优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust beamforming design under imperfect channel state information (CSI) is a fundamental challenge in multiuser multiple-input multiple-output (MU-MIMO) systems, particularly when the channel estimation error statistics are unknown. Conventional model-driven methods usually rely on prior knowledge of the error covariance matrix and data-driven deep learning approaches suffer from poor generalization capability to unseen channel conditions. To address these limitations, this paper proposes a hybrid offline-online framework that achieves effective offline learning and rapid online adaptation. In the offline phase, we propose a shared (among users) deep neural network (DNN) that is able to learn the channel estimation error covariance from observed samples, thus enabling robust beamforming without statistical priors. Meanwhile, to facilitate real-time deployment, we propose a sparse augmented low-rank (SALR) method to reduce complexity while maintaining comparable performance. In the online phase, we show that the proposed network can be rapidly fine-tuned with minimal gradient steps. Furthermore, a multiple basis model-agnostic meta-learning (MB-MAML) strategy is further proposed to maintain multiple meta-initializations and by dynamically selecting the best one online, we can improve the adaptation and generalization capability of the proposed framework under unseen or non-stationary channels. Simulation results demonstrate that the proposed offline-online framework exhibits strong robustness across diverse channel conditions and it is able to significantly outperform state-of-the-art (SOTA) baselines.</description>
      <author>example@mail.com (Wenzhuo Zou, Ming-Min Zhao, An Liu, Min-Jian Zhao)</author>
      <guid isPermaLink="false">2512.14165v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</title>
      <link>http://arxiv.org/abs/2512.13857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EvoLattice是一种新颖的框架，通过在有向无环图中存储多个替代方案，解决了传统LLM引导进化方法中丢弃有用变体、遭受破坏性编辑和探索脆弱搜索空间的问题。&lt;h4&gt;背景&lt;/h4&gt;大语言模型(LLMs)越来越多地用于进化和程序及多智能体系统，但大多数现有方法基于覆盖式突变，一次只维护单个候选。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保留多个候选变体、避免破坏性编辑、提供更稳定搜索空间的进化框架。&lt;h4&gt;方法&lt;/h4&gt;EvoLattice框架将整个候选种群表示在一个有向无环图中，每个节点存储多个持久性替代方案，每条有效路径定义一个可执行候选。通过替代级别评估提供数据驱动的反馈信号，并使用确定性自修复机制保证结构正确性。&lt;h4&gt;主要发现&lt;/h4&gt;EvoLattice在程序合成方面产生了更稳定的进化、更大的表达能力和更强的改进轨迹，其动态类似于质量-多样性优化。&lt;h4&gt;结论&lt;/h4&gt;EvoLattice提供了一种更有效、更稳定的方法来使用LLMs进行程序和多智能体系统的进化，能够保留成功组件并提供密集的反馈信号。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)越来越多地用于进化和程序及多智能体系统，然而大多数现有方法依赖于基于覆盖的突变，一次只维护单个候选。此类方法丢弃有用的变体，遭受破坏性编辑，并探索一个易受结构故障影响的脆弱搜索空间。我们引入了EvoLattice，一个在有向无环图中表示整个候选程序或智能体行为种群的框架。每个节点存储多个持久性替代方案，图中的每条有效路径定义一个不同的可执行候选，产生大的组合搜索空间而不重复结构。EvoLattice通过对每个替代方案出现在所有路径中进行评分，实现细粒度的替代级别评估，产生揭示局部设计选择如何影响全局性能的统计信息。这些统计信息为LLM指导的突变、重组和修剪提供密集、数据驱动的反馈信号，同时保留成功组件。结构正确性由确定性自修复机制保证，该机制强制执行无环性和依赖一致性，独立于LLM。EvoLattice通过将替代方案解释为提示片段或子智能体行为，自然地扩展到智能体进化。在程序合成(代理和优化器元学习)方面，EvoLattice比先前的LLM引导方法产生更稳定的进化、更大的表达能力和更强的改进轨迹。所得动态类似于质量-多样性优化，从EvoLattice的内部多替代表示中隐式出现，而不是来自显式的外部档案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.</description>
      <author>example@mail.com (Kamer Ali Yuksel)</author>
      <guid isPermaLink="false">2512.13857v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models</title>
      <link>http://arxiv.org/abs/2512.13806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为Disentangled Decoding Decomposition (D3)的弱监督方法，用于跨EEG数据集训练深度学习模型，通过分离脑活动的潜在成分来解决隐藏的过拟合问题，提高模型在实际应用中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度学习在解码EEG信号方面已获得关注，许多研究声称达到最先进准确率，但在受控BCI基准测试上的表现与在实际环境中缺乏泛化能力之间的脱节表明存在隐藏的过拟合问题，成功转化为实际应用的情况有限。&lt;h4&gt;目的&lt;/h4&gt;引入一种弱监督方法，用于跨EEG数据集训练深度学习模型，解决隐藏的过拟合问题，提高模型在实际应用中的泛化能力，并分离脑活动的潜在成分以提高模型可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出D3方法，通过预测输入窗口在试验序列中的采样位置来分离脑活动的潜在成分，类似于非线性ICA；使用具有完全独立子网络的新型模型架构以确保可解释性；提出特征解释范式对比不同数据集上的成分激活曲线并检查相关时间和空间滤波器。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在运动想象数据上可靠分离脑活动潜在成分；在适当成分子集上训练下游分类器可防止任务相关伪影引起的隐藏过拟合；利用线性可分离潜在空间实现有效少样本学习；能区分真实脑活动成分和虚假特征，避免隐藏过拟合问题。&lt;h4&gt;结论&lt;/h4&gt;该方法能生成避免隐藏过拟合且能很好泛化到实际应用的模型，仅需最少标记数据；为神经科学研究人员提供分离单个脑过程的工具，可能发现未知动态。&lt;h4&gt;翻译&lt;/h4&gt;深度学习用于解码脑电图信号已获得关注，许多研究声称达到了最先进的准确率。然而，尽管基准测试性能令人信服，但成功转化为实际应用的情况有限。在受控脑机接口基准测试上的性能与缺乏泛化到实际环境能力之间的频繁脱节表明存在隐藏的过拟合问题。我们引入了解耦解码分解方法，这是一种跨脑电图数据集训练深度学习模型的弱监督方法。通过预测输入窗口在相应试验序列中的采样位置，该方法分离了脑活动的潜在成分，类似于非线性独立成分分析。我们利用一种具有完全独立子网络的新型模型架构以确保严格的可解释性。我们概述了一种特征解释范式，用于对比不同数据集上的成分激活曲线并检查相关的时间和空间滤波器。该方法在运动想象数据上可靠地分离了脑活动的潜在成分。在这些成分的适当子集上训练下游分类器可以防止由任务相关伪影引起的隐藏过拟合，这种伪影严重影响端到端分类器。我们进一步利用线性可分离的潜在空间在睡眠阶段分类中实现有效的少样本学习。能够区分脑活动的真实成分和虚假特征，使得模型能够避免隐藏的过拟合问题并很好地泛化到实际应用，同时只需要最少的标记数据。对神经科学界而言，该方法为研究人员提供了一种分离单个脑过程的工具，甚至可能发现迄今为止未知的动态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.</description>
      <author>example@mail.com (Siegfried Ludwig, Stylianos Bakas, Konstantinos Barmpas, Georgios Zoumpourlis, Dimitrios A. Adamos, Nikolaos Laskaris, Yannis Panagakis, Stefanos Zafeiriou)</author>
      <guid isPermaLink="false">2512.13806v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2512.13749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures. Submitted to IEEE IATMSI-2026 (Track: AI, IoT and Computer Vision Enabled Technologies)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了资源受限环境下金融新闻情感分类的嵌入方法，发现预训练嵌入在数据稀缺时收益有限，并提出了替代解决方案。&lt;h4&gt;背景&lt;/h4&gt;金融情感分析有助于市场理解，但标准自然语言处理方法在小数据集应用时面临显著挑战。&lt;h4&gt;目的&lt;/h4&gt;对资源受限环境下金融新闻情感分类的嵌入方法进行评估比较，寻找适合小数据集的解决方案。&lt;h4&gt;方法&lt;/h4&gt;评估Word2Vec、GloVe和句子变换器表示与梯度 boosting结合的效果，使用手动标注的头条新闻进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;验证集和测试性能存在显著差距；模型表现不如简单基线模型；预训练嵌入在数据充足度低于临界阈值时收益递减；小型验证集导致模型选择过程中的过拟合。&lt;h4&gt;结论&lt;/h4&gt;嵌入质量本身无法解决情感分类中的基本数据稀缺问题；资源有限的从业者应考虑少样本学习、数据增强或基于词典的混合方法等替代方案。&lt;h4&gt;翻译&lt;/h4&gt;金融情感分析增强了市场理解；然而，当应用于小型数据集时，标准自然语言处理方法会遇到显著挑战。本研究提供了资源受限环境下基于嵌入的金融新闻情感分类方法的比较评估。Word2Vec、GloVe和句子变换器表示与梯度 boosting相结合，在手动标注的头条新闻上进行了评估。实验结果确定了验证和测试性能之间存在显著差距，尽管验证指标强劲，但模型表现不如简单基线。分析表明，预训练嵌入在数据充足度低于关键阈值时收益递减，并且小型验证集在模型选择过程中导致过拟合。通过每周情感聚合和叙事总结用于市场监控工作流程，展示了实际应用。研究结果提供了经验证据，证明仅嵌入质量无法解决情感分类中的基本数据稀缺问题。对于资源有限的从业者，结果表明当标记样本稀缺时，需要考虑替代方法，如少样本学习、数据增强或基于词典的混合方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.</description>
      <author>example@mail.com (Joyjit Roy, Samaresh Kumar Singh)</author>
      <guid isPermaLink="false">2512.13749v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints</title>
      <link>http://arxiv.org/abs/2512.13717v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种两阶段联邦少样本学习框架，用于解决EEG癫痫检测中数据稀缺和隐私限制的问题。&lt;h4&gt;背景&lt;/h4&gt;许多深度学习方法依赖大型集中式标注数据集，但临床实践中EEG数据稀缺、患者特定且分布在不同机构，受隐私法规限制无法集中数据。&lt;h4&gt;目的&lt;/h4&gt;解决在真实医疗环境中创建可用AI癫痫检测模型的挑战，提出一种两阶段联邦少样本学习框架用于个性化EEG癫痫发作检测。&lt;h4&gt;方法&lt;/h4&gt;在包含六种EEG事件类别的TUH事件语料库上训练和评估。第一阶段使用联邦学习在非独立同分布模拟医院站点上微调预训练生物信号变换器；第二阶段使用联邦少样本个性化仅用五个标记EEG段将分类器适配到每个患者。&lt;h4&gt;主要发现&lt;/h4&gt;联邦微调达到平衡准确率0.43，Cohen's kappa 0.42，加权F1 0.69；FFSL阶段客户端特定模型在四个站点上平均达到平衡准确率0.77，Cohen's kappa 0.62，加权F1 0.73。&lt;h4&gt;结论&lt;/h4&gt;FFSL可以在现实数据可用性和隐私限制条件下支持有效的患者自适应癫痫发作检测。&lt;h4&gt;翻译&lt;/h4&gt;许多深度学习方法已被开发用于基于脑电图(EEG)的癫痫发作检测；然而，大多数依赖于访问大型集中式标注数据集。在临床实践中，EEG数据通常稀缺，患者特定且分布在各机构，并受严格隐私法规限制禁止数据集中。因此，在真实医疗环境中创建可用的AI癫痫检测模型仍然具有挑战性。为解决这些限制，我们提出了一种用于个性化EEG癫痫发作检测的两阶段联邦少样本学习(FFSL)框架。该方法在包含六种EEG事件类别的TUH事件语料库上进行训练和评估。在第一阶段，使用联邦学习在非独立同分布模拟医院站点上微调预训练的生物信号变换器(BIOT)，实现共享表示学习而不集中EEG记录。在第二阶段，联邦少样本个性化使用仅五个标记EEG段将分类器适配到每个患者，同时保留癫痫特定信息并受益于跨站点知识。联邦微调达到平衡准确率0.43(集中式:0.52)，Cohen's kappa 0.42(0.49)，和加权F1 0.69(0.74)。在FFSL阶段，客户端特定模型在四个具有异构事件分布的站点上平均达到平衡准确率0.77，Cohen's kappa 0.62，和加权F1 0.73。这些结果表明，FFSL可以在现实数据可用性和隐私限制条件下支持有效的患者自适应癫痫发作检测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.</description>
      <author>example@mail.com (Ekaterina Sysoykova, Bernhard Anzengruber-Tanase, Michael Haslgrubler, Philipp Seidl, Alois Ferscha)</author>
      <guid isPermaLink="false">2512.13717v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN</title>
      <link>http://arxiv.org/abs/2512.13715v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is submitted to IEEE Open Journal of the Communications Society&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种自适应的Meta-Hierarchical Reinforcement Learning框架，用于优化O-RAN中的资源分配和网络切片，结合分层控制和元学习实现全局和本地适应，提高网络管理效率和适应性。&lt;h4&gt;背景&lt;/h4&gt;现代应用复杂性要求无线网络具备实时适应性和高效资源管理能力。O-RAN架构及其RIC模块成为动态资源管理和网络切片的关键解决方案，但大多数AI驱动方法在不可预测和高度动态条件下难以保持性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种自适应的Meta-Hierarchical Reinforcement Learning框架，基于Model Agnostic Meta Learning灵感，联合优化O-RAN中的资源分配和网络切片。&lt;h4&gt;方法&lt;/h4&gt;框架结合分层控制与元学习：高层控制器在各切片间分配资源，低级代理执行切片内调度。自适应元更新机制通过时间差分误差方差加权任务，提高稳定性并优先处理复杂网络场景。理论分析建立了两级学习过程的亚线性收敛性和遗憾保证。&lt;h4&gt;主要发现&lt;/h4&gt;模拟显示该方法比基线方法提高19.8%的网络管理效率，具有更快适应性和更高的eMBB、URLLC和mMTC切片服务质量满意度。消融和可扩展性研究证实了方法的鲁棒性，实现高达40%的更快速适应，并在网络规模扩大时保持一致的公平性、延迟和吞吐量性能。&lt;h4&gt;结论&lt;/h4&gt;自适应Meta-Hierarchical Reinforcement Learning框架在O-RAN环境中提供了有效的资源分配和网络切片解决方案，能够适应动态变化的网络条件并在各种场景下保持高性能。&lt;h4&gt;翻译&lt;/h4&gt;现代应用日益复杂，要求无线网络具备实时适应性和高效资源管理能力。具有RAN智能控制器模块的开放无线接入网络架构已成为动态资源管理和网络切片的关键解决方案。尽管人工智能驱动的方法显示出潜力，但大多数方法在不可预测和高度动态的条件下难以保持性能。本文提出了一种自适应的Meta-Hierarchical Reinforcement Learning框架，其灵感来自Model Agnostic Meta Learning，用于联合优化O-RAN中的资源分配和网络切片。该框架将分层控制与元学习相结合，实现全局和本地适应：高层控制器在各切片间分配资源，而低级代理执行切片内调度。自适应元更新机制通过时间差分误差方差加权任务，提高稳定性并优先处理复杂的网络场景。理论分析建立了两级学习过程的亚线性收敛性和遗憾保证。模拟结果显示，与基线RL和meta-RL方法相比，该方法提高了19.8%的网络管理效率，同时具有更快的适应性和更高的eMBB、URLLC和mMTC切片的服务质量满意度。额外的消融和可扩展性研究证实了该方法的鲁棒性，实现了高达40%的更快速适应，并在网络规模扩大时保持一致的公平性、延迟和吞吐量性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.</description>
      <author>example@mail.com (Fatemeh Lotfi, Fatemeh Afghah)</author>
      <guid isPermaLink="false">2512.13715v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>A Graph-Based Forensic Framework for Inferring Hardware Noise of Cloud Quantum Backend</title>
      <link>http://arxiv.org/abs/2512.14541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于图神经网络(GNN)的法医框架，用于预测未知量子处理器的单量子比特和量子比特链路错误率，仅使用拓扑信息和转译电路特征，无需访问校准数据。&lt;h4&gt;背景&lt;/h4&gt;云量子平台让用户可访问多种不同量子比特技术、耦合布局和噪声级别的后端，但电路执行依赖于用户无法观察的内部分配和路由策略。提供商可能将作业重定向到错误率更高的区域，导致保真度下降但仍提供过时的校准数据，造成用户无法验证电路是否在计费硬件上执行的安全漏洞。&lt;h4&gt;目的&lt;/h4&gt;开发一种法医方法，从用户可见的工件推断后端行为，使用户能够验证量子电路执行环境并评估实际错误率。&lt;h4&gt;方法&lt;/h4&gt;构建基于图神经网络(GNN)的法医框架，从IBM 27量子比特设备收集数据，合并静态校准特征和动态转译特征，为单量子比特和双量子比特错误分别训练GNN回归器，并在推理时从用户可访问的特征重建完整错误图。&lt;h4&gt;主要发现&lt;/h4&gt;模型在目标后端上准确恢复后端错误率，单量子比特错误平均不匹配度约22%，量子比特链路错误约18%；预测错误值排序与实际校准错误高度匹配(Spearman相关性高)；框架能持续识别弱链路和高噪声量子比特，在现实噪声漂移条件下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;基于GNN的法医框架能从用户可访问信息准确预测量子处理器错误特性，为用户提供验证电路执行环境的方法，填补了云量子平台透明度方面的安全漏洞。&lt;h4&gt;翻译&lt;/h4&gt;云量子平台让用户可以访问具有不同量子比特技术、耦合布局和噪声级别的多种后端。然而，电路的执行依赖于用户无法观察到的内部分配和路由策略。提供商可能会将作业重定向到错误率更高的区域以节省资源、平衡负载或其他不透明原因，从而导致保真度下降，同时仍然提供过时或平均的校准数据。这种缺乏透明性创造了一个安全漏洞：用户无法验证他们的电路是否在为其计费的硬件上执行。因此，从用户可见的工件推断后端行为的法医方法变得至关重要。在这项工作中，我们介绍了一种基于图神经网络(GNN)的法医框架，该框架仅使用拓扑信息和从转译电路中提取的聚合特征来预测未知后端的单量子比特和量子比特链路错误率。我们从几个IBM 27量子比特设备构建数据集，将静态校准特征与动态转译特征合并，并为单量子比特和双量子比特错误分别训练GNN回归器。在推理时，模型无需访问目标后端的校准数据，并从用户可用的特征重建完整的错误图。我们在目标后端上的结果显示，后端错误率的准确恢复，单量子比特错误的平均不匹配度约为22%，量子比特链路错误的平均不匹配度约为18%。该模型还表现出强排名一致性，预测错误值排序与实际校准错误排序高度匹配，如高Spearman相关性所示。该框架持续识别弱链路和高噪声量子比特，并在现实的噪声漂移条件下保持鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cloud quantum platforms give users access to many backends with different qubit technologies, coupling layouts, and noise levels. The execution of a circuit, however, depends on internal allocation and routing policies that are not observable to the user. A provider may redirect jobs to more error-prone regions to conserve resources, balance load or for other opaque reasons, causing degradation in fidelity while still presenting stale or averaged calibration data. This lack of transparency creates a security gap: users cannot verify whether their circuits were executed on the hardware for which they were charged. Forensic methods that infer backend behavior from user-visible artifacts are therefore becoming essential. In this work, we introduce a Graph Neural Network (GNN)-based forensic framework that predicts per-qubit and per-qubit link error rates of an unseen backend using only topology information and aggregated features extracted from transpiled circuits. We construct a dataset from several IBM 27-qubit devices, merge static calibration features with dynamic transpilation features and train separate GNN regressors for one- and two-qubit errors. At inference time, the model operates without access to calibration data from the target backend and reconstructs a complete error map from the features available to the user. Our results on the target backend show accurate recovery of backend error rate, with an average mismatch of approximately 22% for single-qubit errors and 18% for qubit-link errors. The model also exhibits strong ranking agreement, with the ordering induced by predicted error values closely matching that of the actual calibration errors, as reflected by high Spearman correlation. The framework consistently identifies weak links and high-noise qubits and remains robust under realistic temporal noise drift.</description>
      <author>example@mail.com (Subrata Das, Archisman Ghosh, Swaroop Ghosh)</author>
      <guid isPermaLink="false">2512.14541v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space</title>
      <link>http://arxiv.org/abs/2512.14418v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种双轴表示-完全收敛学习(RCCL)策略，通过整合图卷积网络和无桥图编码实现分子表示的完整性，开发了FD25数据集，实现了有机分子的近乎完全组合覆盖，并展示了图神经网络的优异预测性能。&lt;h4&gt;背景&lt;/h4&gt;机器学习正在深刻改变分子和材料建模领域，但由于化学空间的巨大规模（10的30次方到10的60次方），模型能否在这个空间中实现收敛学习仍然是一个开放的科学问题。&lt;h4&gt;目的&lt;/h4&gt;解决机器学习模型在庞大化学空间中实现收敛学习的科学问题，建立分子表示、结构完整性和模型泛化之间的定量联系，为可解释、可迁移和数据高效的分子智能奠定基础。&lt;h4&gt;方法&lt;/h4&gt;提出双轴表示-完全收敛学习策略，开发整合图卷积网络编码局部价键环境和无桥图编码环/笼状拓扑的分子表示方法，基于RCCL框架开发FD25数据集，覆盖13,302个局部价键单元和165,726个环/笼状拓扑，使用图神经网络进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络在FD25数据集上训练后表现出表示完全收敛学习和强分布外泛化能力，在外部基准测试中整体预测误差约为1.0千卡/摩尔平均绝对误差，建立了分子表示、结构完整性和模型泛化之间的定量联系。&lt;h4&gt;结论&lt;/h4&gt;RCCL框架和FD25数据集为分子建模提供了新方法，实现了近乎完全的组合覆盖，并展示了优异的预测性能，为可解释、可迁移和数据高效的分子智能奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;机器学习正在深刻重塑分子和材料建模；然而，鉴于化学空间的巨大规模（10的30次方到10的60次方），模型能否在这个空间中实现收敛学习仍然是一个开放的科学问题。我们引入了一种双轴表示-完全收敛学习策略，通过一种分子表示方法实现该方法，该方法整合了基于现代价键理论的局部价键环境的图卷积网络编码，以及环/笼状拓扑的无桥图编码，提供了化学空间覆盖度的定量度量。该框架形式化了表示完整性，为构建支持大模型收敛学习的数据集建立了原则性基础。在该RCCL框架指导下，我们开发了FD25数据集，系统覆盖了13,302个局部价键单元和165,726个环/笼状拓扑，实现了含有H/C/N/O/F元素的有机分子的近乎完全组合覆盖。在FD25上训练的图神经网络表现出表示完全收敛学习和强分布外泛化能力，在外部基准测试中整体预测误差约为1.0千卡/摩尔平均绝对误差。我们的结果建立了分子表示、结构完整性和模型泛化之间的定量联系，为可解释、可迁移和数据高效的分子智能奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.</description>
      <author>example@mail.com (Dejun Hu, Zhiming Li, Jia-Rui Shen, Jia-Ning Tu, Zi-Hao Ye, Junliang Zhang)</author>
      <guid isPermaLink="false">2512.14418v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Graph Signal Denoising Using Regularization by Denoising and Its Parameter Estimation</title>
      <link>http://arxiv.org/abs/2512.14213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to APSIPA Transactions on Signal and Information Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于去噪正则化(RED)的图信号可解释去噪方法，展示了RED技术在图信号处理中的应用价值，并提出了基于深度算法展开的参数估计方法，提高了算法在无监督设置下的适用性。&lt;h4&gt;背景&lt;/h4&gt;RED是一种最初为图像恢复开发的技术，在优化问题的正则化项中使用高效的去噪器。通过RED，可以明确使用去噪器设计优化问题，并在温和条件下轻松计算正则化项的梯度。&lt;h4&gt;目的&lt;/h4&gt;将RED技术扩展应用于图像处理之外的图信号去噪领域，提高图信号去噪的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;适应RED用于图信号去噪；证明多种图信号去噪器（包括图神经网络）满足RED条件；从图滤波器角度研究RED的有效性；提出基于深度算法展开的监督和无监督参数估计方法。&lt;h4&gt;主要发现&lt;/h4&gt;许多图信号去噪器理论上或实践上满足RED条件；从图滤波器角度验证了RED的有效性；提出的参数估计方法提高了算法适用性，特别是在无监督设置下。&lt;h4&gt;结论&lt;/h4&gt;在合成和真实数据集的去噪实验中，所提方法在均方误差方面比现有图信号去噪方法表现更好，证明了RED技术在图信号去噪中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种基于去噪正则化(RED)的图信号可解释去噪方法。RED是一种为图像恢复开发的技术，在优化问题的正则化项中使用高效（有时是黑盒）的去噪器。通过使用RED，可以明确使用去噪器设计优化问题，并在温和条件下轻松计算正则化项的梯度。我们将RED适应用于图像处理之外的图信号去噪。我们证明许多图信号去噪器，包括图神经网络，理论上或实践上满足RED的条件。我们还从图滤波器的角度研究了RED的有效性。此外，我们提出了基于深度算法展开的监督和无监督参数估计方法。这些方法旨在提高算法的适用性，特别是在无监督设置下。合成和真实数据集的去噪实验表明，我们提出的方法在均方误差方面提高了信号去噪精度，比现有的图信号去噪方法更好。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose an interpretable denoising method for graph signals using regularization by denoising (RED). RED is a technique developed for image restoration that uses an efficient (and sometimes black-box) denoiser in the regularization term of the optimization problem. By using RED, optimization problems can be designed with the explicit use of the denoiser, and the gradient of the regularization term can be easily computed under mild conditions. We adapt RED for denoising of graph signals beyond image processing. We show that many graph signal denoisers, including graph neural networks, theoretically or practically satisfy the conditions for RED. We also study the effectiveness of RED from a graph filter perspective. Furthermore, we propose supervised and unsupervised parameter estimation methods based on deep algorithm unrolling. These methods aim to enhance the algorithm applicability, particularly in the unsupervised setting. Denoising experiments for synthetic and real-world datasets show that our proposed method improves signal denoising accuracy in mean squared error compared to existing graph signal denoising methods.</description>
      <author>example@mail.com (Hayate Kojima, Hiroshi Higashi, Yuichi Tanaka)</author>
      <guid isPermaLink="false">2512.14213v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2512.14023v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HSMGNN的混合对称正定流形图神经网络，用于多元时间序列预测。该模型结合了欧几里得和黎曼几何框架，能够更有效地捕捉现实数据中的复杂几何结构和时空依赖关系。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列预测在交通管理和预测性维护等实际应用中具有重要作用。现有方法通常在欧几里得或黎曼空间中建模MTS数据，限制了它们捕捉现实世界中数据多样几何结构和复杂时空依赖的能力。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一种能够捕捉数据几何特性的混合几何表示框架，用于更准确和全面的多元时间序列预测。&lt;h4&gt;方法&lt;/h4&gt;作者提出了HSMGNN模型，包括三个关键组件：(1)子流形-交叉段(SCS)嵌入，将输入MTS投影到欧几里得和黎曼空间；(2)自适应距离库(ADB)层，通过可训练记忆机制降低黎曼距离的计算成本；(3)融合图卷积网络(FGCN)，通过可学习融合算子整合双空间特征进行准确预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在三个基准数据集上，HSMGNN在预测准确率上比最先进的基线方法提高了最多13.8%，证明了混合几何表示在MTS预测中的有效性。&lt;h4&gt;结论&lt;/h4&gt;HSMGNN是首个利用混合几何表示进行MTS预测的工作，通过结合欧几里得和黎曼框架，能够更全面地建模数据的几何特性，从而提高预测性能。&lt;h4&gt;翻译&lt;/h4&gt;多元时间序列预测在交通管理和预测性维护等各种实际应用中发挥着至关重要的作用。现有方法通常在欧几里得或黎曼空间中对MTS数据进行建模，限制了它们捕捉现实数据中固有的多样几何结构和复杂时空依赖的能力。为了克服这一限制，我们提出了混合对称正定流形图神经网络(HSMGNN)，这是一种基于图神经网络的创新模型，能够在混合欧几里得-黎曼框架内捕捉数据几何特性。据我们所知，这是首个利用混合几何表示进行MTS预测的工作，能够表达和全面建模几何特性。具体而言，我们引入了子流形-交叉段(SCS)嵌入，将输入MTS投影到欧几里得和黎曼空间，从而在不同几何域中捕捉时空变化。为了减轻黎曼距离的高计算成本，我们进一步设计了一个具有可训练记忆机制的自适应距离库(ADB)层。最后，我们设计了一个融合图卷积网络(FGCN)，通过可学习融合算子整合双空间特征，以实现准确预测。在三个基准数据集上的实验表明，HSMGNN在预测准确率上比最先进的基线方法提高了最多13.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.</description>
      <author>example@mail.com (Yong Fang, Na Li, Hangguan Shan, Eryun Liu, Xinyu Li, Wei Ni, Er-Ping Li)</author>
      <guid isPermaLink="false">2512.14023v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>A Complete Guide to Spherical Equivariant Graph Transformers</title>
      <link>http://arxiv.org/abs/2512.13927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is a technical version of the article originally published in Alchemy Bio (99 pages, 46 figures)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;球面等变图神经网络(EGNNs)为三维分子和生物分子系统学习提供了尊重物理旋转对称性的框架，通过将节点和边特征表示为球面张量，扩展了传统GNN和Transformer，并提供了完整的理论基础和实现指南。&lt;h4&gt;背景&lt;/h4&gt;在三维分子和生物分子系统中，预测必须尊重物理学中固有的旋转对称性，这需要专门的神经网络架构来处理。&lt;h4&gt;目的&lt;/h4&gt;开发一个完整的、直观的球面等变建模基础，并构建相应的架构，使研究人员能够理解和实现球面EGNNs应用于化学、分子性质预测等领域。&lt;h4&gt;方法&lt;/h4&gt;将节点和边特征表示为在旋转群SO(3)不可约表示下变换的球面张量，构建SO(3)-等变核，并开发张量场网络和SE(3)-Transformer架构，实现等变消息传递和注意力操作。&lt;h4&gt;主要发现&lt;/h4&gt;通过球面等变建模，可以确保神经网络在输入旋转时预测以物理上有意义的方式变化，这对于分子和生物分子系统学习至关重要。&lt;h4&gt;结论&lt;/h4&gt;球面EGNNs为处理具有旋转对称性的三维数据提供了有效框架，该指南通过清晰的数学推导和代码示例，为研究人员提供了理解和实现这些模型的全面资源。&lt;h4&gt;翻译&lt;/h4&gt;球面等变图神经网络(EGNNs)为学习三维分子和生物分子系统提供了原则性框架，其中预测必须尊重物理学中固有的旋转对称性。这些模型通过将节点和边特征表示为在旋转群SO(3)不可约表示下变换的球面张量，扩展了传统的消息传递GNN和Transformer，确保在输入旋转时预测以物理上有意义的方式变化。本指南从群表示和球谐函数开始，建立了完整的球面等变建模直观基础，包括张量积、Clebsch-Gordan分解和SO(3)-等变核的构建。基于这一基础，我们构建了张量场网络和SE(3)-Transformer架构，并解释了它们如何在几何图上进行等变消息传递和注意力操作。通过清晰的数学推导和带注释的代码片段，本指南为寻求理解或实现球面EGNNs应用于化学、分子性质预测、蛋白质结构建模和生成建模的研究人员和学习者提供了自包含的介绍。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是三维分子和生物分子系统学习中的旋转对称性问题。标准神经网络在处理三维分子数据时无法保持旋转对称性，导致模型在旋转输入后可能无法识别相同的分子结构，从而造成效率低下、泛化能力差或物理不一致的预测。这个问题在研究中非常重要，因为分子系统的许多特性不依赖于它们在空间中的绝对方向，粒子间的物理相互作用在旋转下可预测地变换。保持旋转对称性对于准确建模分子间相互作用、预测分子性质、蛋白质结构建模和生成建模等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到标准神经网络无法处理三维数据的旋转对称性，然后引入群表示理论，特别是SO(3)旋转群，作为处理旋转对称性的数学框架。他们使用球形张量表示节点和边特征，这些张量在SO(3)的不可约表示下变换，并构建等变核确保预测在输入旋转时以物理上有意义的方式变化。作者借鉴了多项现有工作，包括SE(3)-Transformers论文（Fuchs et al., 2020）、Deep Graph Library (DGL)框架，以及量子力学中的球谐函数、不可约表示和Clebsch-Gordan系数等概念。论文还基于现有的球形等变图神经网络（EGNNs）框架，如Fuchs et al., Geiger and Smidt, Thomas等人的工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用球形张量表示节点和边特征，这些张量在SO(3)旋转群的不可约表示下变换，构建等变核函数确保模型在输入旋转时保持等变性，并将消息传递和注意力机制设计为等变的以保持物理一致性。整体实现流程包括：1)将点云转换为几何图，节点代表原子或残基；2)将节点特征表示为球形张量列表；3)使用球谐函数将边的位移向量投影到球形张量；4)构建结合球谐函数、Clebsch-Gordan分解和可学习径向函数的等变核；5)实现等变消息传递机制（Tensor Field Network）；6)构建基于注意力的等变架构（SE(3)-Transformer）；7)应用到分子性质预测任务中。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提供了球形等变图神经网络的完整指南，从基础数学概念到实际实现；2)清晰解释了复杂的数学概念，如群表示、球谐函数、张量积和Clebsch-Gordan分解；3)将Tensor Field Network和SE(3)-Transformer架构统一在球形等变框架下；4)提供了详细的数学推导和带注释的代码实现；5)构建了完整的工具包，使研究人员能够实现球形等变神经网络。相比之前的工作，本指南提供了更全面和直观的基础理论解释，包含更多实现细节和代码示例，降低了入门门槛，更系统地解释了球形等变建模的各个方面，并将不同球形等变架构统一在一个框架下。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提供球形等变图神经网络的完整理论基础和实用指南，使研究人员能够构建尊重物理旋转对称性的三维分子系统模型，从而提高了分子性质预测和生物分子建模的准确性和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.</description>
      <author>example@mail.com (Sophia Tang)</author>
      <guid isPermaLink="false">2512.13927v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention</title>
      <link>http://arxiv.org/abs/2512.13758v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了混合定向注意力时空图神经网络(HDA-STGNN)，一种用于网络范围交通量估计的深度学习框架，利用速度曲线、静态道路属性和道路网络拓扑预测所有路段的每日交通量曲线，无需在推理时依赖交通量数据。&lt;h4&gt;背景&lt;/h4&gt;现有交通量估计方法通常只处理配备传感器道路的交通量预测或使用附近传感器进行空间插补。预测模型不考虑未监测道路，而空间插补方法虽处理网络范围估计，但在推理时依赖交通量数据，限制了在传感器稀少城市中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用更广泛可用的探测车辆速度数据和静态道路属性，而非依赖交通量数据进行网络范围交通量估计的方法。&lt;h4&gt;方法&lt;/h4&gt;开发混合定向注意力时空图神经网络(HDA-STGNN)框架，利用速度曲线、静态道路属性和道路网络拓扑来预测网络中所有路段的每日交通量曲线。&lt;h4&gt;主要发现&lt;/h4&gt;通过消融研究证明模型能捕捉复杂时空依赖关系，并强调拓扑信息对在没有推理时交通量数据的情况下准确进行网络范围交通量估计的价值。&lt;h4&gt;结论&lt;/h4&gt;HDA-STGNN提供了一种有效方法，可在不依赖推理时交通量数据的情况下，利用速度曲线、静态道路属性和道路网络拓扑进行网络范围交通量估计。&lt;h4&gt;翻译&lt;/h4&gt;现有的交通量估计方法通常只处理预测配备传感器的道路上的交通量或使用附近传感器进行空间插补来估算缺失的交通量这两种情况之一。虽然预测模型在设计上通常不考虑未监测的道路，但空间插补方法明确处理网络范围的估计；然而，这种方法在推理时依赖于交通量数据，限制了其在传感器稀少城市中的适用性。与交通量数据不同，探测车辆速度和静态道路属性更广泛可得，并支持大多数城市网络中路段的完全覆盖。在这项工作中，我们提出了混合定向注意力时空图神经网络(HDA-STGNN)，这是一个归纳深度学习框架，旨在解决网络范围的交通量估计问题。我们的方法利用速度曲线、静态道路属性和道路网络拓扑来预测网络中所有路段的每日交通量曲线。为了评估我们方法的有效性，我们进行了大量的消融研究，证明了模型捕捉复杂时空依赖关系的能力，并强调了拓扑信息在没有推理时交通量数据的情况下进行准确网络范围交通量估计的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.</description>
      <author>example@mail.com (Léo Hein, Giovanni de Nunzio, Giovanni Chierchia, Aurélie Pirayre, Laurent Najman)</author>
      <guid isPermaLink="false">2512.13758v1</guid>
      <pubDate>Wed, 17 Dec 2025 15:38:52 +0800</pubDate>
    </item>
    <item>
      <title>LitePT: Lighter Yet Stronger Point Transformer</title>
      <link>http://arxiv.org/abs/2512.13689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://litept.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文分析了3D点云处理网络中卷积层和注意力模块的最佳组合方式，提出了一种新的骨干网络架构，在早期使用卷积，深层使用注意力，并引入PointROPE位置编码，实现了更高效且性能相当的LitePT模型。&lt;h4&gt;背景&lt;/h4&gt;现代3D点云处理神经网络架构同时包含卷积层和注意力模块，但最佳组合方式尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;分析不同计算模块在3D点云网络中的作用，并提出改进的3D点云骨干网络架构。&lt;h4&gt;方法&lt;/h4&gt;基于卷积适合早期高分辨率层提取低级几何特征，注意力适合后期低分辨率层捕获高级语义和上下文信息的发现，提出新的3D点云骨干网络，在早期阶段使用卷积，在深层转向注意力机制，并引入无需训练的3D位置编码PointROPE以避免丢失空间布局信息。&lt;h4&gt;主要发现&lt;/h4&gt;卷积适合在早期高分辨率层提取低级几何特征，而注意力更适合在后期低分辨率层捕获高级语义和上下文信息。&lt;h4&gt;结论&lt;/h4&gt;提出的LitePT模型比最先进的Point Transformer V3参数少3.6倍，速度快2倍，内存使用少2倍，但在各种任务和数据集上表现相当或更好。&lt;h4&gt;翻译&lt;/h4&gt;现代用于3D点云处理的神经网络架构包含卷积层和注意力模块，但最佳的组合方式尚不清楚。我们分析了不同计算模块在3D点云网络中的作用，并发现了一种直观的行为：在早期高分辨率的层中，卷积足以提取低级几何特征，而注意力机制代价高昂且没有带来任何益处；在低分辨率的深层中，注意力机制能够更有效地捕获高级语义和上下文信息。受这一设计原则的指导，我们提出了一种新的改进的3D点云骨干网络，在早期阶段采用卷积，在深层转向注意力机制。为了避免在丢弃冗余卷积层时丢失空间布局信息，我们引入了一种新颖的无需训练的3D位置编码，PointROPE。由此产生的LitePT模型比最先进的Point Transformer V3参数少3.6倍，运行速度快2倍，内存使用少2倍，但在各种任务和数据集上仍能匹配甚至超越其性能。代码和模型可在以下网址获取：https://github.com/prs-eth/LitePT。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云处理模型中计算效率和性能之间的权衡问题。现有的高性能模型（如Point Transformer V3）虽然效果好，但参数量大、计算成本高、内存占用大，限制了它们在资源受限设备上的应用。这个问题很重要，因为3D点云处理在机器人、自动驾驶、定位、制图和环境监测等现实应用中至关重要，需要高效且准确的模型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前最先进的Point Transformer V3架构，发现它同时使用大量卷积层和注意力块，但两种操作在不同层次的作用和效率不同。通过实验发现，卷积在早期高分辨率层提取局部几何特征更有效，而注意力在后期低分辨率层捕获高级语义更有效。作者借鉴了Transformer的注意力机制、U-Net的层次结构、稀疏卷积在点云处理中的应用，以及旋转位置编码(RoPE)的思想，并将其适应到3D点云处理中，提出了PointROPE。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'卷积用于低级几何，注意力用于高级关系'。整体实现流程是：1)采用U-Net架构，编码器分为5个阶段；2)前3个阶段使用稀疏卷积块提取局部几何特征；3)后2个阶段使用PointROPE增强的注意力块捕获高级语义；4)PointROPE将特征分成三个子空间，分别应用1D RoPE嵌入；5)解码器有两种设计：简化版(LitePT-S)或对称混合版(LitePT-S*)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出根据网络层次需求使用不同操作的设计原则；2)发明PointROPE参数免费位置编码；3)设计高效LitePT架构；4)验证分层使用卷积和注意力的最优性。相比之前工作：1)比PTv3参数少3.6倍，速度快2倍，内存少2倍，但性能相当或更好；2)与其他混合模型不同，后者在所有层次使用相同混合块，而LitePT根据层次使用不同纯块；3)PointROPE比传统卷积位置编码更高效且无参数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LitePT通过在点云网络不同层次使用最适合的操作（早期卷积、后期注意力）并引入高效的PointROPE位置编码，实现了在大幅减少计算资源需求的同时保持或提升性能的3D点云处理架构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.</description>
      <author>example@mail.com (Yuanwen Yue, Damien Robert, Jianyuan Wang, Sunghwan Hong, Jan Dirk Wegner, Christian Rupprecht, Konrad Schindler)</author>
      <guid isPermaLink="false">2512.13689v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
  <item>
      <title>MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</title>
      <link>http://arxiv.org/abs/2512.13177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MMDrive是一个创新的多模态视觉-语言模型框架，将传统2D图像理解扩展到3D场景理解，通过整合占用地图、激光雷达点云和文本描述三种模态，显著提升了自动驾驶场景中的理解和推理能力。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型通过多源信息融合实现复杂交通场景的理解和推理，是自动驾驶的核心技术。然而，现有模型受限于2D平面图像理解范式，难以有效感知3D空间信息和执行深层语义融合，导致在复杂自动驾驶环境中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出MMDrive框架，将传统的图像理解扩展到广义的3D场景理解框架，以克服现有视觉-语言模型的局限性，提升自动驾驶场景中的理解能力。&lt;h4&gt;方法&lt;/h4&gt;MMDrive整合三种互补模态：占用地图、激光雷达点云和文本场景描述。引入两个关键组件：1) 面向文本的多模态调制器，根据问题语义线索动态加权各模态贡献；2) 跨模态抽象器，使用可学习抽象令牌生成突出关键区域的跨模态摘要。&lt;h4&gt;主要发现&lt;/h4&gt;在DriveLM和NuScenes-QA基准测试中，MMDrive显著超越现有模型：DriveLM上BLEU-4得分为54.56，METEOR得分为41.78；NuScenes-QA上准确率达62.7%。该模型有效突破了传统仅图像理解的限制，实现复杂驾驶环境中的强大多模态推理。&lt;h4&gt;结论&lt;/h4&gt;MMDrive通过整合多模态信息和引入创新组件，成功打破了传统图像理解的障碍，为自动驾驶场景的理解提供了新的基础，并增强了系统的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型通过多源信息融合实现复杂交通场景的理解和推理，使其成为自动驾驶的核心技术。然而，现有的视觉-语言模型受限于2D平面图像理解范式，限制了它们感知3D空间信息和执行深度语义融合的能力，导致在复杂自动驾驶环境中表现不佳。本研究提出了MMDrive，一个多模态视觉-语言模型框架，将传统的图像理解扩展到广义的3D场景理解框架。MMDrive集成了占用地图、激光雷达点云和文本场景描述三种互补模态。为此，它引入了两个用于自适应跨模态融合和关键信息提取的新组件。具体而言，面向文本的多模态调制器根据问题中的语义线索动态加权每种模态的贡献，引导上下文感知的特征集成。跨模态抽象器使用可学习的抽象令牌生成突出关键区域和基本语义的紧凑跨模态摘要。在DriveLM和NuScenes-QA基准上的全面评估表明，MMDrive相比现有的自动驾驶视觉-语言模型取得了显著的性能提升，在DriveLM上BLEU-4得分为54.56，METEOR得分为41.78，在NuScenes-QA上准确得分为62.7%。MMDrive有效打破了传统仅图像理解的障碍，能够在复杂的驾驶环境中实现强大的多模态推理，为可解释的自动驾驶场景理解提供了新的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有自动驾驶视觉语言模型受限于2D图像理解范式，无法有效感知3D空间信息和执行深度语义融合的问题。这个问题在现实中很重要，因为自动驾驶场景本质上是动态和复杂的，需要全面的环境理解和精确的空间感知，仅依赖2D图像的方法在复杂场景（如遮挡、恶劣天气）中表现不佳，无法提供可靠的环境感知，限制了自动驾驶系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法遵循传统的'图像理解'范式不足以应对自动驾驶场景的复杂性，因此需要扩展到更全面的'场景理解'范式。作者发现不同文本查询关注不同的模态，需要动态调整不同模态的贡献权重。在动态复杂环境中，模型需要有效优先处理大量多模态空间中的信息。作者借鉴了多种现有自动驾驶VLM方法（如DriveLM-Agent、EM-VLM4AD等）的优点，同时使用了现有的编码器架构（如UniRepLKNet、T5）和预训练模型，但创新性地整合了这些技术并设计了新的模块来解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将传统的'图像理解'范式扩展为'场景理解'范式，通过融合占用图、LiDAR点云和场景描述三种互补模态信息，并引入两个关键组件：Text-oriented Multimodal Modulator (TMM)和Cross-Modal Abstractor (CMA)。整体流程包括：1)多模态信息编码（图像、文本、占用图、LiDAR和场景描述）；2)使用TMM动态调整模态权重并执行跨模态注意力融合；3)使用CMA生成跨模态摘要；4)将处理后的多模态输入送入大语言模型生成答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)从'图像理解'到'场景理解'的范式转变，整合占用图、LiDAR和场景描述三种互补模态；2)Text-oriented Multimodal Modulator (TMM)，根据文本查询语义动态调整模态权重；3)Cross-Modal Abstractor (CMA)，使用可学习抽象令牌生成紧凑跨模态摘要；4)两阶段场景描述生成策略。相比之前的工作，MMDrive不仅融合了更多样化的模态信息，还通过动态权重调整和注意力机制实现了自适应融合，并通过抽象机制提高了信息处理效率，解决了传统方法难以处理不同查询对模态差异化需求的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MMDrive通过融合多模态信息并引入创新的自适应融合与关键信息提取组件，将自动驾驶视觉语言模型从图像理解提升到全面的场景理解，显著提升了复杂驾驶环境中的多模态推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.</description>
      <author>example@mail.com (Minghui Hou, Wei-Hsing Huang, Shaofeng Liang, Daizong Liu, Tai-Hao Wen, Gang Wang, Runwei Guan, Weiping Ding)</author>
      <guid isPermaLink="false">2512.13177v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Less Is More: Sparse and Cooperative Perturbation for Point Cloud Attacks</title>
      <link>http://arxiv.org/abs/2512.13119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI'2026 (Oral)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCP的稀疏协作扰动框架，通过选择和利用点的紧凑子集，实现高效的点云对抗攻击，仅需修改少量点即可达到100%攻击成功率。&lt;h4&gt;背景&lt;/h4&gt;大多数针对点云的对抗攻击会扰动大量点，导致广泛的几何变化，限制了在实际场景中的应用。而现有的稀疏攻击方法由于单个扰动的影响有限，往往难以保持有效性。&lt;h4&gt;目的&lt;/h4&gt;开发一种稀疏且高效的对抗攻击方法，通过点的联合扰动产生放大的对抗效果，减少对点云几何结构的破坏。&lt;h4&gt;方法&lt;/h4&gt;SCP框架识别那些相对于其联合扰动使分类错误损失局部凸的点子集，通过检查相应Hessian块的正定性来确定，然后优化该子集以生成高影响对抗示例。&lt;h4&gt;主要发现&lt;/h4&gt;SCP在实验中实现了100%的攻击成功率，超越了最先进的稀疏攻击方法，并且比密集攻击具有更好的不可感知性，同时修改的点数显著减少。&lt;h4&gt;结论&lt;/h4&gt;SCP是一种高效的点云对抗攻击方法，通过稀疏协作扰动实现了高攻击成功率和良好的不可感知性，在实际应用中具有更大潜力。&lt;h4&gt;翻译&lt;/h4&gt;大多数针对点云的对抗攻击会扰动大量点，导致广泛的几何变化，限制了在实际场景中的应用。虽然最近的工作探索了稀疏攻击，仅修改少数点，但这类方法往往由于单个扰动的有限影响而难以保持有效性。在本文中，我们提出了SCP，一种稀疏和协作扰动框架，它选择并利用点的紧凑子集，其联合扰动能够产生放大的对抗效果。具体而言，SCP识别子集，使得相对于它们的联合扰动，分类错误损失是局部凸的，这是通过检查相应Hessian块的正定性来确定的。然后优化选定的子集，以生成具有最小修改的高影响对抗示例。大量实验表明，SCP实现了100%的攻击成功率，超越了最先进的稀疏攻击，并且与密集攻击相比，具有更好的不可感知性，且修改的点数更少。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云对抗攻击中的效率问题，即如何在修改尽可能少的点的情况下实现有效的对抗攻击。这个问题很重要，因为随着3D传感器和深度学习技术在自动驾驶、机器人操作等关键领域的广泛应用，这些系统容易受到对抗攻击的威胁，而现有方法要么需要修改大量点导致明显几何变化（不实用），要么稀疏攻击方法因忽略点间相互作用而效果有限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到点云网络中点之间存在强烈非线性相互作用，假设联合扰动某些点能产生比单独扰动更强的效果。他们借鉴了现有稀疏攻击和几何感知优化的思想，但指出这些方法通常假设点对攻击的贡献是独立的。因此，作者提出需要超越孤立点选择，识别能产生协同效应的点子集，并基于Hessian矩阵的正定性来量化这种合作效应，从而设计了SCP框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是识别点云中具有协同效应的紧凑点子集，这些点的联合扰动能产生放大的对抗效果。实现流程分为三步：1)基于梯度分析选择有影响力的初始点集；2)通过Schur补条件检查进行凸性保持扩展，确保选定的点子集位于损失函数的局部凸区域；3)对选定的合作子集进行联合扰动优化，生成高效且不可感知的对抗样本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)识别了稀疏点云攻击中被忽视的合作相互作用，并建立了基于Hessian的准则来表征这种协同；2)开发了SCP框架，通过梯度筛选和Schur补引导的扩展选择紧凑合作子集；3)实现了100%攻击成功率，同时修改点数极少。相比之前工作，SCP不仅考虑点的独立影响力，还建模点间二阶相互作用，实现了比现有稀疏攻击更高的成功率、更好的不可感知性，且比密集攻击修改点数少两个数量级。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCP通过识别和利用点云中具有协同效应的紧凑点子集，实现了以极少数点修改达到100%攻击成功率的稀疏且高效的点云对抗攻击，同时保持了优于现有方法的不可感知性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most adversarial attacks on point clouds perturb a large number of points, causing widespread geometric changes and limiting applicability in real-world scenarios. While recent works explore sparse attacks by modifying only a few points, such approaches often struggle to maintain effectiveness due to the limited influence of individual perturbations. In this paper, we propose SCP, a sparse and cooperative perturbation framework that selects and leverages a compact subset of points whose joint perturbations produce amplified adversarial effects. Specifically, SCP identifies the subset where the misclassification loss is locally convex with respect to their joint perturbations, determined by checking the positivedefiniteness of the corresponding Hessian block. The selected subset is then optimized to generate high-impact adversarial examples with minimal modifications. Extensive experiments show that SCP achieves 100% attack success rates, surpassing state-of-the-art sparse attacks, and delivers superior imperceptibility to dense attacks with far fewer modifications.</description>
      <author>example@mail.com (Keke Tang, Tianyu Hao, Xiaofei Wang, Weilong Peng, Denghui Zhang, Peican Zhu, Zhihong Tian)</author>
      <guid isPermaLink="false">2512.13119v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title>
      <link>http://arxiv.org/abs/2512.13107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出DiffFusion框架，通过扩散模型和自适应跨模态融合增强多模态3D目标检测在恶劣天气条件下的鲁棒性&lt;h4&gt;背景&lt;/h4&gt;多模态3D目标检测对机器人和自动驾驶的可靠感知至关重要，但在恶劣天气条件下效果受限，主要受天气导致的失真和不同数据模态间不匹配的影响&lt;h4&gt;目的&lt;/h4&gt;开发一个能够增强在挑战性天气条件下鲁棒性的框架，解决多模态3D目标检测在恶劣环境中的性能下降问题&lt;h4&gt;方法&lt;/h4&gt;利用扩散模型去噪和生成能力适应各种天气条件；Diffusion-IR恢复受天气影响的图像；点云恢复(PCR)使用图像对象线索补偿损坏的LiDAR数据；双向自适应融合和对齐模块(BAFAM)实现动态多模态融合和双向鸟瞰图对齐&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的实验表明DiffFusion在恶劣天气下实现了最先进的鲁棒性，同时保持了强大的清洁数据性能；真实世界DENSE数据集上的零样本结果验证了其泛化能力&lt;h4&gt;结论&lt;/h4&gt;DiffFusion框架通过扩散模型和自适应融合有效解决了恶劣天气条件下的多模态3D目标检测问题，将以开源形式发布&lt;h4&gt;翻译&lt;/h4&gt;多模态3D目标检测对机器人和自动驾驶中的可靠感知至关重要。然而，由于天气引起的失真和不同数据模态之间的不匹配，其在恶劣天气条件下的有效性仍然有限。在这项工作中，我们提出了DiffFusion，一个基于扩散恢复和自适应跨模态融合设计的新框架，旨在增强在挑战性天气下的鲁棒性。我们的关键见解是，扩散模型具有强大的去噪和生成数据能力，可以适应各种天气条件。基于此，DiffFusion引入了Diffusion-IR恢复受天气影响退化的图像，以及点云恢复(PCR)使用图像对象线索补偿损坏的LiDAR数据。为了解决两种模态之间的不匹配问题，我们开发了双向自适应融合和对齐模块(BAFAM)。它实现了动态多模态融合和双向鸟瞰图(BEV)对齐，以保持一致的空间对应关系。在三个公共数据集上的广泛实验表明，DiffFusion在恶劣天气下实现了最先进的鲁棒性，同时保持了强大的清洁数据性能。在真实世界DENSE数据集上的零样本结果进一步验证了其泛化能力。我们的DiffFusion实现将以开源形式发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在恶劣天气条件下（如雨、雾、强光等）多模态3D目标检测性能显著下降的问题。这个问题在现实中非常重要，因为自动驾驶和机器人技术需要在各种天气条件下可靠运行，而现有的3D目标检测方法在恶劣天气下性能大幅下降，限制了这些技术在真实世界环境中的安全部署。提高恶劣天气条件下的感知能力对于自动驾驶技术的实用化和普及至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了恶劣天气下多模态3D目标检测面临的两个主要挑战：传感器数据退化和模态间空间失配。核心洞察是扩散模型具有强大的去噪和生成能力，可以适应各种天气条件。基于此，作者设计了DiffFusion框架，包含基于扩散的恢复模块和双向自适应融合模块。作者借鉴了现有工作如DDPM/DDIM扩散模型、BEV-based多模态融合方法、点云上采样技术和CenterNet等2D检测方法，但创新性地将这些技术整合到一个专门解决恶劣天气问题的统一框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用扩散模型的去噪和生成能力恢复恶劣天气下退化的传感器数据，并通过自适应的多模态融合和对齐机制建立不同模态间的可靠对应关系。整体流程分为三步：1) 基于扩散的恢复模块，包括Diffusion-IR恢复图像和PCR利用图像信息恢复点云；2) 双向自适应融合和对齐模块(BAFAM)，包括交叉注意力自适应融合和双向BEV对齐；3) 将融合后的特征输入3D检测头生成最终检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的扩散模型多模态恢复框架，同时处理图像和点云退化；2) 双向自适应融合和对齐模块(BAFAM)，解决模态间空间失配问题；3) 端到端的恶劣天气处理，在保持干净数据性能的同时提高恶劣天气鲁棒性。相比之前工作，DiffFusion不同于传统多模态方法（主要优化于干净数据），也不同于其他恶劣天气处理方法（如TripleMixer只处理单模态，SAMFusion计算开销大），它系统性地解决了天气引起的传感器退化和模态失配问题，并实现了强大的跨域泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DiffFusion通过结合基于扩散的传感器数据恢复和自适应多模态融合，显著提高了自动驾驶系统在恶劣天气条件下的3D目标检测鲁棒性和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.</description>
      <author>example@mail.com (Zhijian He, Feifei Liu, Yuwei Li, Zhanpeng Liu, Jintao Cheng, Xieyuanli Chen, Xiaoyu Tang)</author>
      <guid isPermaLink="false">2512.13107v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs</title>
      <link>http://arxiv.org/abs/2512.12984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VoroLight是一个基于Voronoi网格化的可微分3D形状重建框架，能够从多种输入生成平滑、水密且拓扑一致的体积网格。&lt;h4&gt;背景&lt;/h4&gt;3D形状重建领域需要能够处理多种输入格式并生成高质量表面和体积网格的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从图像、隐式形状level-set场、点云和网格等多种输入直接生成高质量3D形状的可微分框架。&lt;h4&gt;方法&lt;/h4&gt;VoroLight采用三阶段方法：1)使用可微分Voronoi公式初始化表面；2)通过多边形面球训练阶段改进表面质量；3)重新使用可微分Voronoi公式进行体积优化，添加内部生成点。&lt;h4&gt;主要发现&lt;/h4&gt;基于Voronoi网格化的可微分框架能够生成平滑、水密且拓扑一致的体积网格，适用于多种输入格式。&lt;h4&gt;结论&lt;/h4&gt;VoroLight提供了一个灵活且有效的3D形状重建方法，能够处理多种输入并生成高质量的表面和体积表示。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了VoroLight，一个基于Voronoi网格化的可微分3D形状重建框架。我们的方法能够直接从多种输入（包括图像、隐式形状level-set场、点云和网格）生成平滑、水密且拓扑一致的体积网格。VoroLight分为三个阶段：首先使用可微分Voronoi公式初始化表面，然后通过多边形面球训练阶段改进表面质量，最后重新使用可微分Voronoi公式进行体积优化，添加额外的内部生成点。项目页面：https://jiayinlu19960224.github.io/vorolight/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从多种输入（图像、隐式形状水平集场、点云和网格）生成高质量、防水且拓扑一致的体积Voronoi网格的问题。这个问题在现实中很重要，因为现有的3D重建方法要么只关注表面而忽略内部结构，要么依赖于干净输入或固定离散化，限制了几何灵活性。高质量的体积Voronoi网格对模拟、分析和制造等应用至关重要，因为它们具有凸的、防水的和拓扑一致的特性，适合物理应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了多个现有工作：VoroMesh的可微分Voronoi表面优化、VoroCrust的基于球体的边界一致Voronoi网格构建，以及Differentiable Voronoi Diagrams的闭式可微分公式。作者设计了一个三阶段框架：首先使用可微分Voronoi公式初始化表面，然后通过多边形面球体训练细化表面质量，最后重用可微分Voronoi公式进行体积优化。作者特别扩展了VoroCrust的球体构建方法，使其成为可微分、可学习的公式，并比较了三角形面与多边形面约束的效果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过优化可微分Voronoi结构，从多种输入直接生成高质量的体积Voronoi网格，利用Voronoi图自然产生凸的、防水的和拓扑一致的单元的特性。整体流程分为三阶段：1) 初始多边形表面初始化：通过边界反射放置生成器，使用可微分Voronoi公式优化；2) 表面球体细化：为每个顶点定义可训练球体，优化球体参数使相交于同一面的球体通过面的两个交点；3) 体积网格生成：固定表面生成器，引入内部生成器并在质心Voronoi剖分损失下优化，产生一致的体积结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) VoroLight三阶段框架，支持多种输入类型；2) 多边形面球体训练公式，实现自正则化表面细化；3) 发现三角形面约束比多边形面约束产生更高质量表面。相比之前工作，VoroLight不同于VoroMesh（仅限表面和点云输入），不同于VoroCrust（确定性方法且仅限三角形网格输入），不同于TetSphere（生成局部四面体簇而非全局一致网格），也不同于传统体积网格化方法（不依赖干净输入或固定离散化）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VoroLight提出了一种可微分三阶段框架，通过优化Voronoi结构从多种输入直接生成高质量的防水体积Voronoi网格，结合基于球体的表面细化和质心Voronoi剖分的体积优化，实现了从图像、点云、隐式场和网格输入的一致拓扑重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/</description>
      <author>example@mail.com (Jiayin Lu, Ying Jiang, Yin Yang, Chenfanfu Jiang)</author>
      <guid isPermaLink="false">2512.12984v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</title>
      <link>http://arxiv.org/abs/2512.12822v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了Lemon，一种统一的transformer架构，用于解决大规模多模态模型扩展到3D理解时的挑战。通过联合处理3D点云补丁和语言令牌，实现了早期的空间-语言融合，提高了参数效率，并支持更有效的模型扩展。&lt;h4&gt;背景&lt;/h4&gt;将大规模多模态模型扩展到3D理解面临独特挑战：点云数据稀疏且不规则；现有模型依赖具有模态特定编码器的碎片化架构；训练流程通常不稳定且可扩展性差。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的transformer架构来解决3D理解中的挑战，实现更有效的模型扩展，并在3D理解和推理任务上建立新的最先进性能。&lt;h4&gt;方法&lt;/h4&gt;提出了Lemon统一transformer架构，联合处理3D点云补丁和语言令牌作为单一序列；开发了保留空间上下文的结构化补丁化和令牌化方案；设计了三阶段训练课程，从对象级识别逐步构建到场景级空间推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;Lemon在全面的3D理解和推理任务上建立了新的最先进性能，包括对象识别、描述到3D场景中的空间推理；随着模型大小和训练数据的增加，Lemon展示了强大的扩展特性。&lt;h4&gt;结论&lt;/h4&gt;该工作为推进现实世界应用中的3D空间智能提供了统一的基础。&lt;h4&gt;翻译&lt;/h4&gt;将大规模多模态模型扩展到3D理解面临独特挑战：点云数据稀疏且不规则，现有模型依赖具有模态特定编码器的碎片化架构，训练流程通常不稳定且可扩展性差。我们介绍了Lemon，一种统一transformer架构，通过将3D点云补丁和语言令牌作为单一序列联合处理来解决这些挑战。与依赖模态特定编码器和跨模态对齐模块的先前工作不同，这种设计实现了早期的空间-语言融合，消除了冗余编码器，提高了参数效率，并支持更有效的模型扩展。为了处理3D数据的复杂性，我们开发了一种保留空间上下文的结构化补丁化和令牌化方案，以及一个三阶段训练课程，逐步从对象级识别构建到场景级空间推理能力。Lemon在全面的3D理解和推理任务上建立了新的最先进性能，从对象识别和描述到3D场景中的空间推理，同时展示了随着模型大小和训练数据增加的强大扩展特性。我们的工作为推进现实世界应用中的3D空间智能提供了统一的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文旨在解决将大型多模态模型扩展到3D理解领域的挑战，特别是点云数据稀疏不规则、现有模型依赖碎片化架构以及训练流程不稳定和可扩展性差的问题。这个问题非常重要，因为3D理解对于机器人技术、AR/VR系统和空间AI应用至关重要，能够使AI系统更好地理解和交互物理世界，但目前缺乏像2D视觉语言模型那样强大的通用3D理解能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有3D多模态模型的局限性，包括3D编码器适应性有限、3D数据规模不足以及架构不平衡等问题。他们设计了一个统一的transformer架构，将3D点云块和语言标记作为单一序列进行联合处理，消除了对模态特定编码器的需求。作者借鉴了2D视觉语言模型（如VisualBERT、Fuyu-8B）的统一架构设计思想，以及现有的3D基础模型（如Point-BERT、ULIP）的处理方法，并在训练策略上参考了多模态模型训练框架如LLaMA-Factory。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的transformer架构，将3D点云块和语言标记作为单一序列进行联合处理，直接将3D数据嵌入到语言模型框架中。整体实现流程包括：1)数据预处理：使用FPS采样对点云采样，通过递归3D空间方案分区点云为块，使用线性投影器将3D块映射到语言嵌入空间；2)模型架构：使用专门标记编码3D模态，连接3D块嵌入与文本标记创建统一序列；3)三阶段训练：从对象级识别到对象级标题生成再到场景级空间问答；4)推理应用：处理各种3D多模态任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的transformer架构，首次在单个序列中处理点云块和语言标记；2)动态3D分块和标记化方案，将不规则点云转换为结构化标记序列；3)三阶段渐进式训练课程，逐步构建能力；4)首次系统分析3D LMM的扩展定律。相比之前工作，LEMON的不同之处在于：使用统一架构而非模态特定编码器；直接处理3D数据而非依赖预训练编码器；采用渐进式训练而非单一阶段训练；在多个任务上实现更优性能并展示更好的可扩展性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LEMON通过统一的transformer架构和渐进式训练课程，首次实现了高效可扩展的3D多模态理解，为空间智能和具身AI提供了新的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.</description>
      <author>example@mail.com (Yongyuan Liang, Xiyao Wang, Yuanchen Ju, Jianwei Yang, Furong Huang)</author>
      <guid isPermaLink="false">2512.12822v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs</title>
      <link>http://arxiv.org/abs/2512.12805v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Transformers表现出显著的大小泛化能力，能够从小型token集合外推到更长的token集合，这种现象在点云、图和自然语言等多种应用中已被观察到，但缺乏严格的理论解释。&lt;h4&gt;背景&lt;/h4&gt;尽管Transformers在大小泛化方面取得了经验性成功，但这种能力仍缺乏严谨的理论表征，需要建立理论框架来解释这一现象。&lt;h4&gt;目的&lt;/h4&gt;开发一个理论框架来分析几何数据的大小泛化现象，这些几何数据被表示为来自连续源的离散样本（如来自流形的点云，来自图论的图）。&lt;h4&gt;方法&lt;/h4&gt;提出一个关于离散样本的Transformer输出与其连续域等效输出之间误差的界限，证明对于具有稳定位置编码的Transformers，这个界限由采样密度和数据流形的内在维度决定。&lt;h4&gt;主要发现&lt;/h4&gt;Transformers的输出误差界限取决于采样密度和数据流形的内在维度，这一发现为理解Transformers的大小泛化能力提供了理论基础。&lt;h4&gt;结论&lt;/h4&gt;在各种尺寸的图和点云上的实验证实了所提出的理论界限的紧致性，验证了理论框架的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Transformers表现出显著的'大小泛化'特性，能够从小型token集合外推到显著更长的token集合。这种行为在点云、图和自然语言等多种应用中都有记录。尽管 empirically 成功，但这种能力仍缺乏一些严谨的理论表征。在本文中，我们开发了一个理论框架来分析几何数据中的这种现象，我们将几何数据表示为来自连续源的离散样本（例如，来自流形的点云，来自图论的图）。我们的核心贡献是提出了一个关于离散样本的Transformer输出与其连续域等效输出之间误差的界限。我们证明，对于具有稳定位置编码的Transformers，这个界限由采样密度和数据流形的内在维度决定。在各种尺寸的图和点云上的实验证实了我们的理论界限的紧致性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Transformer模型在不同规模输入数据上的泛化能力缺乏理论解释的问题。具体来说，虽然Transformer在实践中表现出能够从小规模训练数据泛化到显著更大的测试数据（如从小分子到大分子，或从固定到更长的上下文窗口），但这种能力缺乏严格的数学理论支持。这个问题在研究中很重要，因为它填补了理论与实践之间的鸿沟，帮助我们理解为什么Transformer能够处理规模变化的数据，这对于开发更强大、更可靠的模型至关重要，特别是在处理点云、图和自然语言等规模变化的数据时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了消息传递神经网络(MPNNs)在图数据上的理论框架，特别是基于图论(graphon)的理论。作者注意到，虽然MPNNs的大小泛化性质已经被广泛研究，但Transformer由于使用全局注意力和位置编码，其泛化性质尚未得到充分理解。作者设计的方法包括：1)将数据样本建模为'tokenset'（token集合），这些token是从连续域中独立采样的；2)定义了一个适用于连续和离散数据的Transformer类；3)推导了离散tokenset与连续极限之间输出误差的界限；4)证明了对于具有稳定位置编码的Transformer，这个界限由采样密度和数据流形的内在维度决定。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：将Transformer的泛化能力与数据采样密度和位置编码的稳定性联系起来。具体来说，作者证明了Transformer在离散tokenset上的输出与连续极限上的输出之间的误差界限，这个界限随着采样密度的增加而减小，并且位置编码的稳定性对泛化性能至关重要。整体实现流程如下：1)定义连续tokenset和离散tokenset的概念；2)定义适用于连续和离散数据的Transformer类；3)推导单层Transformer的离散化误差和扰动误差的界限；4)将单层误差界限扩展到多层Transformer；5)将输出误差界限转化为学习任务的泛化界限；6)通过实验验证理论界限的紧致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次为Transformer在几何数据（如图和点云）上的大小泛化提供了严格的理论保证；2)提出了位置编码稳定性(ρ-stability)的概念，并证明这是Transformer泛化能力的关键因素；3)推导了泛化误差的界限，该界限由离散化误差和位置编码误差组成；4)证明了随机游走位置编码是稳定的，而最短路径距离位置编码是不稳定的。相比之前的工作，这篇论文的主要不同在于：之前的工作主要集中在自然语言处理领域，而本文专注于几何数据；之前的工作大多关注MPNNs，而本文将理论框架扩展到Transformer架构；本文首次明确了位置编码稳定性对Transformer泛化能力的重要性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文为Transformer在不同规模输入数据上的泛化能力提供了第一个严格的理论保证，证明了这种泛化能力取决于数据采样密度和位置编码的稳定性，并通过实验验证了理论结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.</description>
      <author>example@mail.com (Anastasiia Alokhina, Pan Li)</author>
      <guid isPermaLink="false">2512.12805v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning</title>
      <link>http://arxiv.org/abs/2512.12799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DrivePI是一种新颖的空间感知4D多模态大语言模型，作为统一的视觉-语言-行动框架，在自动驾驶的3D感知、预测和规划任务中表现出色，即使使用较小的模型也能超越现有专门模型。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在多个领域表现出强大能力，但在自动驾驶中生成细粒度3D感知和预测输出方面的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出DrivePI，一种新颖的具有空间感知能力的4D MLLM，作为统一的视觉-语言-行动(VLA)框架，同时兼容视觉-行动(VA)模型。&lt;h4&gt;方法&lt;/h4&gt;通过端到端优化并行执行空间理解、3D感知、预测和规划；在统一的MLLM架构中整合点云、多视图图像和语言指令；开发数据引擎生成文本-占用和文本流问答对用于4D空间理解。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用0.5B的Qwen2.5模型作为骨干，DrivePI作为单一统一模型就能匹配或超越现有VLA和VA模型；在nuScenes-QA上的平均准确率比OpenDriveVLA-7B高2.5%；在nuScenes上的碰撞率比ORION降低70%；在OpenOcc上的3D占用比FB-OCC高10.3 RayIoU；在OpenOcc上的占用流将mAVE从0.591降至0.509；在nuScenes上的规划比VAD低32%的L2误差。&lt;h4&gt;结论&lt;/h4&gt;DrivePI是一个统一的多模态模型，在自动驾驶的3D感知、预测和规划任务中表现出色，即使使用较小的模型也能超越现有专门模型。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型(MLLMs)已在多个领域展现出强大能力，但它们在自动驾驶中生成细粒度3D感知和预测输出方面的应用仍探索不足。在本文中，我们提出了DrivePI，一种新颖的空间感知4D MLLM，作为统一的视觉-语言-行动(VLA)框架，同时也兼容视觉-行动(VA)模型。我们的方法通过端到端优化并行执行空间理解、3D感知(即3D占用)、预测(即占用流)和规划(即行动输出)。为了获得精确的几何信息和丰富的视觉外观，我们的方法在统一的MLLM架构中整合了点云、多视图图像和语言指令。我们进一步开发了一个数据引擎，用于生成文本-占用和文本流问答对，以实现4D空间理解。值得注意的是，仅使用0.5B的Qwen2.5模型作为MLLM骨干，DrivePI作为单一统一模型就能匹配或超越现有的VLA模型和专门的VA模型。具体而言，与VLA模型相比，DrivePI在nuScenes-QA上的平均准确率比OpenDriveVLA-7B高2.5%，在nuScenes上的碰撞率比ORION降低70%(从0.37%降至0.11%)。与专门的VA模型相比，DrivePI在OpenOcc上的3D占用比FB-OCC高10.3 RayIoU，在OpenOcc上的占用流将mAVE从0.591降至0.509，在nuScenes上的规划比VAD低32%的L2误差(从0.72m降至0.49m)。代码将在https://github.com/happinesslz/DrivePI上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中如何结合视觉-动作(VA)模型的精确空间感知能力和视觉-语言-动作(VLA)框架的自然语言交互能力的问题。这个问题很重要，因为现有VA模型虽然空间感知精确但缺乏语言交互，而VLA框架虽有良好交互能力却缺乏可靠的细粒度3D感知输出，导致自动驾驶系统要么无法与人类自然交互，要么缺乏可解释性和安全保证。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了VA模型(如UniAD、VAD)和VLA框架(如OpenDriveVLA、ORION)各自的优缺点，然后思考能否创建一个统一框架结合两者的优势。他们借鉴了VA模型的多模态输入处理方式和模块化设计，以及VLA框架的交互能力和语言理解，同时采用了现有的多模态视觉编码器和BEV表示方法，在此基础上创新性地引入LiDAR作为补充传感模态，并设计了专门的空间投影机制和数据引擎。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的视觉-语言-动作框架，结合粗粒度语言空间理解和细粒度3D感知能力，引入LiDAR提供精确3D几何信息，同时生成中间细粒度的3D感知和预测表示。整体流程包括：1)使用多模态视觉编码器处理图像和LiDAR数据获取BEV特征；2)通过空间投影将BEV特征转换为视觉令牌；3)将视觉令牌和文本令牌输入MLLM；4)通过四个专门头部(文本、3D占用、占用流、动作扩散)生成输出；5)所有任务通过端到端方式联合优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出首个统一的空间感知4D MLLM框架；2)引入LiDAR作为补充传感模态提供精确3D几何信息；3)实现准确的3D感知和预测以增强可解释性和安全性；4)开发空间理解基准评估语言空间推理能力；5)仅用0.5B参数模型就超越现有专用模型。相比之前工作，DrivePI同时结合了VA模型的精确空间感知和VLA框架的交互能力，不仅依赖相机图像还引入LiDAR，生成中间细粒度3D表示，并将3D数据集成到自然语言描述中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DrivePI通过统一的空间感知4D多模态大语言模型框架，成功结合了自动驾驶系统中精确的空间感知能力与自然语言交互能力，仅用0.5B参数模型就实现了超越现有专用模型的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI</description>
      <author>example@mail.com (Zhe Liu, Runhui Huang, Rui Yang, Siming Yan, Zining Wang, Lu Hou, Di Lin, Xiang Bai, Hengshuang Zhao)</author>
      <guid isPermaLink="false">2512.12799v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</title>
      <link>http://arxiv.org/abs/2512.12410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于图注意力网络（GAT）的框架，用于重建旋转式激光雷达传感器中因硬件老化、灰尘、雪花、雾或明亮反射等原因丢失的垂直光束，仅使用当前激光雷达帧而不需要额外信息，能有效恢复点云中的缺失垂直切片。&lt;h4&gt;背景&lt;/h4&gt;垂直光束丢失在旋转式激光雷达传感器中是一个严重问题，可能由硬件老化、灰尘、雪花、雾或明亮反射等因素触发，会导致点云中缺失整个垂直切片，严重影响自动驾驶汽车中的3D感知能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅使用当前激光雷达帧就能重建缺失垂直通道的方法，无需依赖相机图像或时间信息，以改善自动驾驶汽车中的3D感知能力。&lt;h4&gt;方法&lt;/h4&gt;将每个激光雷达扫描表示为非结构化空间图（点作为节点，边连接附近点并保持原始光束索引顺序），使用多层图注意力网络学习局部几何邻域的自适应注意力权重，并直接回归丢失位置的高程（z）值。&lt;h4&gt;主要发现&lt;/h4&gt;在1,065个具有模拟通道丢失的KITTI序列上测试，平均高度均方根误差为11.67厘米；87.98%的重建点落在10厘米误差阈值内；单GPU每帧推理时间为14.65秒；重建质量对不同邻域大小k保持稳定。&lt;h4&gt;结论&lt;/h4&gt;纯图注意力模型仅基于原始点云几何就能有效恢复现实传感器退化情况下的丢失垂直光束，为自动驾驶系统提供了一种可靠的点云重建方法。&lt;h4&gt;翻译&lt;/h4&gt;旋转式激光雷达传感器中由硬件老化、灰尘、雪花、雾或明亮反射引起的垂直光束丢失会从点云中移除整个垂直切片，严重降低自动驾驶汽车中的3D感知能力。本文提出了一种基于图注意力网络（GAT）的框架，仅使用当前激光雷达帧就能重建这些缺失的垂直通道，不需要相机图像或时间信息。每个激光雷达扫描被表示为一个非结构化空间图：点是节点，边连接附近的点同时保持原始光束索引顺序。多层GAT学习局部几何邻域的自适应注意力权重，并直接回归丢失位置的高程（z）值。在1,065个具有模拟通道丢失的原始KITTI序列上训练和评估，该方法实现了平均高度均方根误差为11.67厘米，87.98%的重建点落在10厘米误差阈值内。在单个GPU上，每帧推理时间为14.65秒，重建质量对于不同的邻域大小k保持稳定。这些结果表明，仅基于原始点云几何操作的纯图注意力模型能有效恢复现实传感器退化情况下的丢失垂直光束。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决旋转式激光雷达传感器中垂直光束丢失的重建问题。在现实应用中，由于硬件老化、灰尘、雪、雾或强反射等原因，LiDAR传感器可能会丢失整个垂直光束切片，导致点云中出现垂直不连续性。这个问题在自动驾驶领域尤为重要，因为垂直光束包含关键的高度信息(z坐标)，对于区分可行驶表面和障碍物、保持几何一致性以及准确识别车辆周围物体至关重要。缺失这些光束会严重降低3D感知系统的可靠性，影响物体检测、深度估计和自由空间判断等关键任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了LiDAR垂直光束丢失的原因和影响，认识到现有基于体素CNN或插值方法难以捕捉真实扫描的不规则结构。他们转向图神经网络(GNNs)领域，特别是图注意力网络(GAT)，因为GAT能够自适应地加权邻近点的贡献，强调最具信息量的几何线索。作者借鉴了GNN在点云处理中的应用经验，以及先前关于LiDAR垂直结构重要性的研究，但针对垂直光束重建这一特定问题进行了创新设计。他们构建了一个多层GAT框架，将LiDAR扫描表示为空间图，通过注意力机制学习局部几何关系，仅使用单帧点云信息重建缺失光束，而不依赖相机图像或多帧时间信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将LiDAR点云表示为一个非结构化空间图，其中点是节点，边连接空间邻近的点同时保留原始光束索引顺序，然后使用图注意力网络(GAT)学习节点间的自适应权重关系，通过多层GAT结构捕获局部和全局几何上下文信息，最终回归缺失的高度(z)值。整体实现流程包括：1)将每个LiDAR扫描构建为k近邻图结构；2)通过多层GAT层进行特征学习和注意力权重计算；3)每层GAT执行线性投影、注意力分数计算、权重归一化和加权聚合；4)最终通过回归头预测缺失的垂直坐标；5)在KITTI数据集上训练和评估，模拟垂直光束丢失场景并重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)纯LiDAR-based解决方案，不依赖相机图像或多帧时间信息；2)首次将图注意力网络直接应用于LiDAR垂直光束重建任务；3)非结构化空间图表示，保留原始光束索引同时连接空间邻近点；4)多层GAT架构捕获更广泛上下文信息。相比之前工作，本文方法与传统插值方法相比能更好捕捉不规则结构；与基于体素CNN的方法相比避免了数据转换；与其他图方法相比充分利用了注意力机制优势；与多模态方法相比在纯LiDAR场景下更适用；与GLiDR等模型相比专注于光束级重建而非仅全局形状一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于图注意力网络的纯LiDAR点云重建框架，能够仅利用当前帧的点云几何信息有效恢复因传感器退化或环境因素导致的垂直光束缺失，显著提高了自动驾驶系统在点云数据不完整情况下的3D感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.</description>
      <author>example@mail.com (Khalfalla Awedat, Mohamed Abidalrekab, Mohammad El-Yabroudi)</author>
      <guid isPermaLink="false">2512.12410v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction</title>
      <link>http://arxiv.org/abs/2512.12378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了M4Human，目前最大规模的多模态人体网格重建基准数据集，包含高分辨率毫米波雷达、RGB和深度数据，旨在解决现有视觉传感方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有人体网格重建数据集主要依赖RGB输入，但视觉传感受限于遮挡、光照变化和隐私问题。虽然毫米波雷达被探索用于隐私保护的室内人体感知，但当前雷达数据集存在骨架标签稀疏、规模有限和动作简单等限制。&lt;h4&gt;目的&lt;/h4&gt;为了推进人体网格重建研究，作者提出了M4Human数据集，旨在克服现有数据集的局限性，为雷达-based人体建模提供更全面的数据支持。&lt;h4&gt;方法&lt;/h4&gt;作者构建了M4Human数据集，包含661K帧数据（是之前最大数据集的9倍），提供原始雷达张量(RT)和处理后的雷达点云(RPC)两种数据形式，涵盖20个受试者和50种多样化动作，包括原地、坐姿原地和自由空间运动或康复动作。&lt;h4&gt;主要发现&lt;/h4&gt;通过在RT和RPC模态以及RGB-D多模态融合上建立的基准实验，突显了M4Human对基于雷达的人体建模的重要性，同时揭示了在快速、不受限制运动下仍存在持续挑战。&lt;h4&gt;结论&lt;/h4&gt;M4Human作为目前最大规模的多模态人体网格重建基准数据集，为雷达-based人体建模研究提供了重要资源，有助于推动隐私保护的室内人体感知技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;人体网格重建(HMR)直接提供身体与环境交互的洞察，使各种沉浸式应用成为可能。虽然现有的大规模HMR数据集主要依赖视距RGB输入，但基于视觉的传感受限于遮挡、光照变化和隐私问题。为了克服这些限制，最近的研究探索了射频毫米波雷达用于隐私保护的室内人体感知。然而，当前雷达数据集受到稀疏骨架标签、有限规模和简单原地动作的限制。为了推进HMR研究社区，我们介绍了M4Human，这是目前最大规模（661K帧，是之前最大规模的9倍）的多模态基准，具有高分辨率毫米波雷达、RGB和深度数据。M4Human提供原始雷达张量(RT)和处理后的雷达点云(RPC)，以支持不同级别的射频信号粒度的研究。M4Human包含高质量的运动捕捉(MoCap)注释，具有3D网格和全局轨迹，涵盖20个受试者和50种多样化动作，包括原地、坐姿原地和自由空间运动或康复动作。我们在RT和RPC模态以及RGB-D模态的多模态融合上建立了基准。大量结果突显了M4Human对基于雷达的人体建模的重要性，同时揭示了在快速、不受限制运动下的持续挑战。数据集和代码将在论文发表后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于毫米波雷达的高保真人体网格重建(HMR)数据集不足的问题。这个问题在现实中很重要，因为现有视觉传感方法存在隐私泄露风险、易受光照和遮挡影响，而毫米波雷达可以在保护隐私的同时适应各种环境条件，支持VR游戏、老人护理、康复训练等多种应用场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有HMR数据集的局限性，认识到毫米波雷达作为替代方案的优势。他们设计了一个整合高分辨率毫米波雷达、RGB-D相机和Vicon MoCap系统的统一平台，提供原始雷达张量(RT)和处理后的雷达点云(RPC)两种表示。作者借鉴了现有RGB-based数据集的数据收集方法，但在标注方面采用基于标记的MoCap系统而非依赖RGB相机，提供更高质量的标注。他们还参考了现有毫米波雷达数据集的采集技术，但进行了改进以提高数据质量和多样性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多模态的毫米波雷达基准数据集，支持高保真的人体网格重建。整体实现流程包括：1) 系统设置：集成毫米波雷达、RGB-D相机和MoCap系统；2) 数据采集：在多样化场景和距离下采集多模态数据；3) 高质量标注：使用标记式MoCap系统提供精确的3D网格标注；4) 数据集构建：包含661K帧、20名受试者和50个动作类别的多样化数据集，同时提供原始和处理后的雷达数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 规模最大的毫米波雷达HMR数据集(661K帧，是之前最大数据集的9倍)；2) 提供四种同步模态(RGB、深度、RT和RPC)；3) 使用标记式MoCap提供高质量3D网格标注；4) 包含50个多样化动作类别(日常、康复和体育)；5) 提出RT-Mesh，首个直接从原始雷达张量进行HMR的方法。相比之前工作，M4Human规模更大、数据质量更高、动作更复杂多样，同时提供原始雷达张量这一新表示，首次支持了基于RT的HMR研究。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; M4Human是一个大规模、多模态的毫米波雷达基准数据集，通过提供高质量标注和多样化动作，首次支持了基于原始雷达张量的人体网格重建，推动了隐私保护、环境适应性强的3D人体感知技术的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.</description>
      <author>example@mail.com (Junqiao Fan, Yunjiao Zhou, Yizhuo Yang, Xinyuan Cui, Jiarui Zhang, Lihua Xie, Jianfei Yang, Chris Xiaoxuan Lu, Fangqiang Ding)</author>
      <guid isPermaLink="false">2512.12378v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset</title>
      <link>http://arxiv.org/abs/2512.12377v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;INDOOR-LIDAR是一个全面的室内3D激光雷达点云混合数据集，旨在推进机器人感知研究&lt;h4&gt;背景&lt;/h4&gt;现有的室内激光雷达数据集通常存在规模有限、注释格式不一致以及数据收集中人为引起的变异性等问题&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集的局限性，提供一致且可复现的基准，用于推进复杂室内环境中的机器人感知研究&lt;h4&gt;方法&lt;/h4&gt;通过整合模拟环境和使用自主地面机器人获取的真实世界扫描创建混合数据集，每个样本包含密集点云数据和KITTI风格注释&lt;h4&gt;主要发现&lt;/h4&gt;该数据集支持多种应用，包括3D目标检测、鸟瞰图感知、SLAM、语义场景理解和模拟与真实室内领域之间的域适应&lt;h4&gt;结论&lt;/h4&gt;通过弥合合成和真实世界数据之间的差距，INDOOR-LIDAR建立了可扩展、真实且可复现的基准，用于推进复杂室内环境中的机器人感知&lt;h4&gt;翻译&lt;/h4&gt;我们提出了INDOOR-LIDAR，这是一个全面的室内3D激光雷达点云混合数据集，旨在推进机器人感知研究。现有的室内激光雷达数据集通常存在规模有限、注释格式不一致以及数据收集中人为引起的变异性等问题。INDOOR-LIDAR通过整合模拟环境和使用自主地面机器人获取的真实世界扫描来解决这些局限性，提供了一致的覆盖范围和受控变化条件下的真实传感器行为。每个样本由密集的点云数据和强度测量值以及KITTI风格的注释组成。注释模式涵盖了各种场景中的常见室内物体类别。模拟子集允许灵活配置布局、点密度和遮挡，而真实世界子集捕获了真实室内环境特有的真实传感器噪声、杂乱和特定领域伪影。INDOOR-LIDAR支持广泛的应用，包括3D目标检测、鸟瞰图感知、SLAM、语义场景理解以及模拟和真实室内领域之间的域适应。通过弥合合成和真实世界数据之间的差距，INDOOR-LIDAR为推进复杂室内环境中的机器人感知建立了可扩展、真实且可复现的基准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有室内LiDAR数据集的局限性，包括规模有限、标注格式不一致以及手持采集方法导致的视野盲区问题。这个问题很重要，因为室内机器人需要完整360度的环境信息进行安全导航和感知，而现有数据集的局限性使训练出的模型难以直接应用于实际系统，限制了室内机器人感知算法的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的缺陷，认识到LiDAR在几何一致性方面的优势，决定结合模拟环境和真实扫描创建混合数据集。设计过程中借鉴了Unity和MuJoCo的环境建模、Taichi的高性能光线投射、SUSTechPOINTS标注工具以及KITTI数据格式标准，但创新性地从机器人视角采集数据，确保完整的360度视野覆盖。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个结合模拟与真实数据的混合数据集，从机器人视角采集确保完整视野，提供标准化标注促进算法发展。流程包括：1)使用Unity/MuJoCo和Taichi生成模拟数据；2)用自主地面机器人采集真实数据；3)对模拟数据进行完美标注，真实数据采用半自动标注；4)组织为.bin点云文件和KITTI格式标注；5)支持3D检测、鸟瞰图感知、SLAM等多种应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：混合真实与模拟环境、机器人中心360度数据采集、全面的检测基准。相比之前工作，INDOOR-LiDAR避免了手持采集的视野盲区，结合了模拟与真实数据而非单一来源，提供了更全面的标注和基线测试，且包含了强度信息、点级标注、360度视野和3D模拟模型等特性，大多数现有数据集缺乏这些特性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; INDOOR-LiDAR通过结合模拟与真实世界的机器人中心360度LiDAR数据，提供了一个规模大、标注一致、无遮挡的混合数据集，有效弥合了室内机器人感知中模拟到现实的差距，为开发和评估鲁棒的室内感知算法建立了新的基准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.</description>
      <author>example@mail.com (Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund)</author>
      <guid isPermaLink="false">2512.12377v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>A Framework for Scalable Digital Twin Deployment in Smart Campus Building Facility Management</title>
      <link>http://arxiv.org/abs/2512.12149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个全面的数字孪生框架，用于智能校园建筑的可扩展部署，通过整合3D激光扫描、BIM建模和物联网数据可视化来支持设施运营和维护。案例研究表明该框架可实现集中式资产文档、改进系统可见性以及增强维护工作流程。&lt;h4&gt;背景&lt;/h4&gt;数字孪生为校园设施管理提供了重大机遇，但现有研究往往只关注孤立领域，如点云几何或能源分析，缺乏将建筑几何、设备元数据和运营数据整合到统一平台的可扩展和互操作工作流程。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的框架，用于智能校园建筑中可扩展的数字孪生部署，通过整合3D激光扫描、BIM建模和物联网支持的数据可视化，来支持设施运营和维护。&lt;h4&gt;方法&lt;/h4&gt;包括三个主要部分：(1)使用地面激光扫描和结构化点云处理进行现实捕捉；(2)开发包含建筑、机械、电气、管道、运输和传感器系统的丰富BIM模型；(3)创建数字孪生环境，将设备元数据、维护策略和模拟物联网数据链接到数字孪生管理平台。&lt;h4&gt;主要发现&lt;/h4&gt;在佐治亚理工学院Price Gilbert大楼的案例研究中，共建模了509个设备项目并嵌入OmniClass分类，开发了10个交互式仪表板可视化系统性能。结果表明该框架实现了集中式资产文档、改进系统可见性和增强维护工作流程。&lt;h4&gt;结论&lt;/h4&gt;尽管由于现有传感器基础设施有限，大多数物联网数据是模拟的，但该原型验证了可扩展数字孪生用于设施管理的可行性，并为实时监控、分析集成和未来自主建筑运营建立了参考模型。&lt;h4&gt;翻译&lt;/h4&gt;数字孪生(DT)为校园环境中的设施管理(FM)提供了重大机遇。然而，现有研究往往只专注于孤立领域，如点云几何或能源分析，没有提供可扩展和互操作的工作流程，将建筑几何、设备元数据和运营数据整合到统一的FM平台中。本研究提出了一个全面的框架，用于智能校园建筑中可扩展的数字孪生部署，通过整合3D激光扫描、BIM建模和物联网支持的数据可视化来支持设施运营和维护。该方法包括：(1)使用地面激光扫描和结构化点云处理进行现实捕捉；(2)开发包含建筑、机械、电气、管道、运输和传感器系统的丰富BIM模型；(3)创建数字孪生环境，将设备元数据、维护策略和模拟的物联网数据链接到数字孪生管理平台。佐治亚理工学院Price Gilbert大楼的案例研究展示了此工作流程的实施。共建模了509个设备项目，并将其嵌入到带有OmniClass分类的数字孪生中。开发了10个交互式仪表板来可视化系统性能。结果表明，提出的框架实现了集中式资产文档编制、改进的系统可见性以及增强的预防和反应性维护工作流程。尽管由于现有传感器基础设施有限，大多数物联网数据是模拟的，但原型验证了可扩展数字孪生用于设施管理的可行性，并为实时监控、分析集成和未来自主建筑运营建立了参考模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决数字孪生技术在校园建筑设施管理中缺乏可扩展、可互工作流程的问题，现有研究通常只关注单一领域(如点云几何或能源分析)，没有将建筑几何、设备元数据和运营数据整合到统一的设施管理平台。这个问题很重要，因为传统设施管理依赖人工检查和纸质文档，导致效率低下、维护成本高、设备故障难以及时发现，而数字孪生技术可以提供实时监测、预测维护和数据驱动的决策支持，从而提高校园建筑的运营效率、可持续性和用户满意度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了校园设施管理的挑战，包括缺乏实时可见性、被动维护、资产信息碎片化等问题，然后研究了数字孪生技术的潜在应用。通过文献 review 发现现有研究的不足，特别是缺乏建筑规模的参考模型和AI在核心建筑系统中的应用细节。作者借鉴了多项现有工作，包括Lu等人在建筑和城市规模数字孪生的应用、点云驱动的工作流程、基于工具的数字孪生架构、结合IoT数据与BIM的能源研究，以及数字孪生与移动IoT传感技术的整合。基于这些基础，作者设计了一个整合3D激光扫描、BIM建模和IoT数据可视化的综合框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将物理校园建筑转化为动态的数字孪生，整合3D几何模型、设备元数据和实时运营数据，通过统一的数字孪生平台实现集中化的资产文档、提高建筑系统可见性、增强预防性和反应性维护工作流程。整体实现流程分为三个阶段：1)激光扫描阶段：使用地面激光扫描仪捕获建筑空间数据，进行数据采集、配准和处理，生成统一的点云文件；2)BIM模型开发阶段：使用点云数据在Revit中创建包含建筑、机械、电气、管道等系统的三维模型，并为每个元素添加元数据；3)数字孪生开发阶段：创建数字孪生环境，部署IoT传感器网络，使用数字孪生管理平台整合BIM模型和实时数据流，开发交互式仪表板可视化系统性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了针对校园建筑设施管理的全面可扩展数字孪生部署框架；2)整合了3D激光扫描、BIM建模和IoT数据可视化；3)开发了详细的实施方法，包括点云处理、BIM模型增强和数字孪生环境创建；4)在Price Gilbert建筑案例中实现了该工作流程，建模了509个设备项目；5)开发了10个交互式仪表板可视化关键系统性能。相比之前工作，该框架更全面，不仅关注单一技术领域，还提供了可扩展、可互操作的工作流程；不仅关注技术实现，还整合了设施管理工作流程；不仅使用模拟数据，还结合了实际建筑案例验证可行性；特别关注核心建筑系统(如HVAC、电气系统)的数字孪生应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一个整合3D激光扫描、BIM建模和IoT数据可视化的可扩展数字孪生框架，为校园建筑设施管理提供了集中化资产文档、提高系统可见性和增强维护工作流程的创新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital twin (DT) offers significant opportunities for enhancing facility management (FM) in campus environments. However, existing research often focuses narrowly on isolated domains, such as point-cloud geometry or energy analytics, without providing a scalable and interoperable workflow that integrates building geometry, equipment metadata, and operational data into a unified FM platform. This study proposes a comprehensive framework for scalable digital-twin deployment in smart campus buildings by integrating 3D laser scanning, BIM modeling, and IoT-enabled data visualization to support facility operations and maintenance. The methodology includes: (1) reality capture using terrestrial laser scanning and structured point-cloud processing; (2) development of an enriched BIM model incorporating architectural, mechanical, electrical, plumbing, conveying, and sensor systems; and (3) creation of a digital-twin environment that links equipment metadata, maintenance policies, and simulated IoT data within a digital-twin management platform. A case study of the Price Gilbert Building at Georgia Tech demonstrates the implementation of this workflow. A total of 509 equipment items were modeled and embedded with OmniClass classifications into the digital twin. Ten interactive dashboards were developed to visualize system performance. Results show that the proposed framework enables centralized asset documentation, improved system visibility, and enhanced preventive and reactive maintenance workflows. Although most IoT data were simulated due to limited existing sensor infrastructure, the prototype validates the feasibility of a scalable digital twin for facility management and establishes a reference model for real-time monitoring, analytics integration, and future autonomous building operations.</description>
      <author>example@mail.com (Thyda Siv)</author>
      <guid isPermaLink="false">2512.12149v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2512.12013v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于毫米波雷达点云的人体活动识别系统，使用离散动态图神经网络(DDGNN)处理点云稀疏性和可变尺寸问题，实现了高精度的活动分类。&lt;h4&gt;背景&lt;/h4&gt;人体活动识别需要提取准确的人体运动时空特征，但毫米波雷达点云系统由于信号物理特性存在稀疏性和可变尺寸问题，现有基于视觉的密集点云预处理方法可能不适用于毫米波雷达系统。&lt;h4&gt;目的&lt;/h4&gt;设计一种适合毫米波雷达点云特性的方法，解决其稀疏性和可变尺寸问题，提高人体活动识别准确率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图表示方法，使用离散动态图神经网络(DDGNN)探索人体运动特征的时空表示；设计了星形图描述静态中心点与动态毫米波雷达点间的高维相对关系；采用DDGNN学习可变尺寸星形图中的特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法优于其他基线方法；系统总体分类准确率达94.27%，接近基于视觉骨骼数据的97.25%最优性能；在树莓派4上测试证明了其在资源受限平台上的有效性；系统优于三种最近的雷达特定方法，无需重采样或帧聚合器。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于图表示和DDGNN的方法能有效处理毫米波雷达点云的稀疏性和可变尺寸问题，实现高精度的人体活动识别，适用于资源受限平台。&lt;h4&gt;翻译&lt;/h4&gt;人体活动识别需要提取准确的人体运动时空特征。毫米波雷达点云系统由于毫米波信号的物理特性，存在稀疏性和可变尺寸问题。现有工作通常借鉴基于视觉的密集点云系统的预处理算法，但这些方法可能不适用于毫米波雷达系统。在这项工作中，我们提出了一种使用离散动态图神经网络的图表示方法，用于探索人体运动相关特征的时空表示。具体而言，我们设计了一种星形图来描述手动添加的静态中心点与同一帧和连续帧中的动态毫米波雷达点之间的高维相对关系。然后，我们采用DDGNN来学习可变尺寸星形图中的特征。实验结果表明，我们的方法在使用真实世界人体活动识别数据集的其他基线方法中表现更优。我们的系统总体分类准确率达到94.27%，接近基于视觉骨骼数据的97.25%的最优性能。我们还在树莓派4上进行了推理测试，证明了其在资源受限平台上的有效性。我们提供了针对可变DDGNN结构的全面消融研究，以验证我们的模型设计。我们的系统还优于三种最近的雷达特定方法，无需重采样或帧聚合器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于毫米波雷达的人体活动识别中的点云数据稀疏性和尺寸变化问题。这个问题很重要，因为毫米波雷达可以保护隐私（不同于摄像头），适合在浴室等敏感环境使用，同时能提供与视觉传感器互补的功能，在人机交互和机器人控制等领域有广泛应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有视觉传感器HAR方法的局限性，指出它们不适合毫米波雷达的稀疏点云。作者借鉴了图神经网络在视觉HAR中的应用，特别是骨架图概念，但针对毫米波雷达特点进行了创新设计。作者观察到毫米波雷达点云虽稀疏但能捕捉人体动态部分，因此设计了星形图结构，将静态参考点与动态雷达点连接，并开发了DDGNN模型来处理可变大小的图结构，无需额外预处理算法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用星形图表示人体活动，其中包含一个静态中心点和动态雷达点，捕捉它们之间的相对关系；使用DDGNN模型处理可变大小的图结构。整体流程包括：1)点云预处理（噪声减少和DBSCAN聚类）；2)图生成（添加静态中心点并构建星形图）；3)DDGNN模型处理（空间特征提取器使用两层图卷积网络，时间特征提取器使用双向LSTM）；4)分类器生成预测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)星形图表示，专注于静态中心点和动态雷达点的相对关系；2)DDGNN模型，能处理可变大小的图结构；3)无需重采样或帧聚合器；4)在资源受限平台上的高效实现。相比之前工作，不同于视觉骨架图方法依赖密集点云，也不同于其他基于图的方法（如MMPointGNN和Tesla-Rapture）的高计算复杂度或需要重采样，同时避免了体素化方法在稀疏点云中产生大量空体素的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种基于星形图表示和离散动态图神经网络的毫米波雷达人体活动识别方法，有效解决了点云数据稀疏性和尺寸变化问题，实现了94.27%的分类准确率，接近基于视觉骨架数据的性能，同时在资源受限平台上展现出高效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TMC.2025.3634221&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.</description>
      <author>example@mail.com (Senhao Gao, Junqing Zhang, Luoyu Mei, Shuai Wang, Xuyu Wang)</author>
      <guid isPermaLink="false">2512.12013v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder</title>
      <link>http://arxiv.org/abs/2512.11926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为TransBridge的新型transformer上采样块和DSRecon模块，通过联合完成和检测框架提高了自动驾驶中远距离稀疏点云区域的3D物体检测性能。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测在自动驾驶中至关重要，能提供关于移动物体和障碍物的关键信息。然而，在远距离区域只有少量LiDAR点的情况下进行检测仍然是一个挑战，许多策略已开发出来通过密集化来解决点云稀疏问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个联合完成和检测框架，在保持成本不变的同时提高稀疏区域的检测特征。&lt;h4&gt;方法&lt;/h4&gt;提出了TransBridge，一种基于transformer的上采样块，融合了检测和完成网络的特征；设计了动态-静态重建(DSRecon)模块，为完成网络生成密集的LiDAR数据；使用transformer机制建立通道和空间关系之间的连接，生成用于完成的高分辨率特征图。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Waymo数据集上的实验证明了框架的有效性；该框架一致地改进了端到端的3D物体检测，平均精度(mAP)在多种方法中从0.7到1.5不等，表明其泛化能力；对于两阶段检测框架，mAP提升了高达5.78个百分点。&lt;h4&gt;结论&lt;/h4&gt;所提出的联合完成和检测框架有效解决了远距离稀疏点云的3D物体检测问题，显著提升了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;三维物体检测在自动驾驶中至关重要，它提供了关于移动物体和障碍物的关键信息。在只有少量LiDAR点的远距离区域进行检测仍然是一个挑战，已经开发出许多策略通过密集化来解决点云稀疏问题。本文提出了一个联合完成和检测框架，在保持成本不变的同时提高了稀疏区域的检测特征。具体来说，我们提出了TransBridge，这是一种新颖的基于transformer的上采样块，融合了检测和完成网络的特征。检测网络可以从获取由完成网络派生的隐式完成特征中受益。此外，我们设计了动态-静态重建(DSRecon)模块，为完成网络生成密集的LiDAR数据，满足密集点云地面真实值的需求。此外，我们采用transformer机制建立通道和空间关系之间的连接，生成用于完成的高分辨率特征图。在nuScenes和Waymo数据集上的大量实验证明了所提框架的有效性。结果表明，我们的框架一致地改进了端到端的3D物体检测，在多种方法中的平均精度(mAP)从0.7到1.5不等，表明其泛化能力。对于两阶段检测框架，它也将mAP提升了高达5.78个百分点。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D物体检测中的点云稀疏性问题，特别是在自动驾驶中远处区域LiDAR点数据稀少导致的物体检测困难。这个问题在现实中非常重要，因为准确检测远处物体对自动驾驶系统的安全决策至关重要，稀疏点云会导致漏检和误检，影响系统可靠性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了LiDAR数据的固有稀疏性和非均匀性是3D物体检测的主要挑战，评估了现有方法如虚拟点云、深度信息增强和点云补全等技术的局限性。作者设计了一个联合检测和补全的框架，借鉴了CenterPoint等检测器作为基础，同时改进了点云补全方法。核心创新是设计了TransBridge模块，利用transformer机制连接检测和补全特征，并引入DSRecon模块生成高质量训练数据，借鉴了表面重建技术来处理点云数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练3D物体检测和场景级点云补全，改善稀疏区域的检测特征，使检测网络能够从补全网络获取隐式特征。整体流程包括：1)接收LiDAR点云并转换为体素；2)使用共享编码器生成多尺度检测特征图；3)通过检测头生成3D边界框；4)TransBridge模块处理特征，包含上采样桥(UB)和解释桥(IB)；5)使用稀疏控制模块(SCM)保持计算效率；6)利用DSRecon生成的密集点云作为监督信号；7)输出补全特征图和检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)联合检测和补全框架，共享编码器但有独立输出模块；2)TransBridge模块，包含上采样桥和解释桥，补偿信息损失并解释不同语义；3)动态-静态重建(DSRecon)模块，生成高质量密集点云训练数据；4)稀疏控制模块(SCM)，确保高效计算。相比之前工作，本文将检测和补全联合训练而非独立训练，保持原始推理速度而非增加计算需求，培养出能更好区分空体素的鲁棒特征提取器，并通过DSRecon减少了远处区域的噪声。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TransBridge通过联合训练3D物体检测和场景级点云补全，使用基于transformer的桥接模块和动态-静态重建技术，显著提高了自动驾驶中远处稀疏区域物体的检测精度，同时保持了原始推理效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TITS.2025.3617527&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.</description>
      <author>example@mail.com (Qinghao Meng, Chenming Wu, Liangjun Zhang, Jianbing Shen)</author>
      <guid isPermaLink="false">2512.11926v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications</title>
      <link>http://arxiv.org/abs/2512.11925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FloraForge是一个LLM辅助框架，使植物科学领域专家能够通过自然语言交互生成生物准确的完全参数化3D植物模型，无需专业编程知识。&lt;h4&gt;背景&lt;/h4&gt;准确的3D植物模型对计算表型和基于物理的模拟至关重要，但当前方法存在局限性：基于学习的方法需要大量特定物种训练数据且缺乏可编辑性；程序建模需要专业几何建模知识和复杂程序规则的深入理解。&lt;h4&gt;目的&lt;/h4&gt;开发一个使植物科学领域专家能够轻松生成高质量3D植物模型的框架，无需专业编程或几何建模知识。&lt;h4&gt;方法&lt;/h4&gt;利用LLM协同设计改进生成参数化植物几何的Python脚本，植物几何表示为具有植物约束的层次B样条曲面，包含控制点和参数变形函数。通过植物描述符(PD)文件进行手动细化，将模型拟合到经验点云数据。&lt;h4&gt;主要发现&lt;/h4&gt;框架成功应用于玉米、大豆和绿豆的3D建模，能够生成双重输出：用于可视化的三角形网格和带有额外参数元数据用于定量分析的三角形网格。该方法结合了LLM辅助模板创建、数学连续表示和直接参数控制。&lt;h4&gt;结论&lt;/h4&gt;FloraForge使植物科学领域能够民主化复杂的几何建模，同时保持数学严谨性，为植物科学研究提供了强大而易用的工具。&lt;h4&gt;翻译&lt;/h4&gt;准确的3D植物模型对计算表型和基于物理的模拟至关重要；然而，当前方法面临显著限制。基于学习的方法需要大量特定物种的训练数据且缺乏可编辑性。程序建模提供参数控制，但需要几何建模的专业知识和对复杂程序规则的深入理解，使得领域科学家难以使用。我们提出了FloraForge，这是一个LLM辅助框架，使领域专家能够通过迭代自然语言植物细化(PR)生成生物准确的完全参数化3D植物模型，最小化编程专业知识需求。我们的框架利用LLM协同设计来改进生成参数化植物几何的Python脚本，这些几何作为具有植物约束的层次B样条曲面表示，包含显式控制点和参数变形函数。这种表示可以轻松细分为任意精度的多边形网格，确保与功能结构植物分析工作流程的兼容性，如光模拟、计算流体动力学和有限元分析。我们在玉米、大豆和绿豆上展示了该框架，通过手动细化植物描述符(PD)（人类可读文件）将程序模型拟合到经验点云数据。该流程生成双重输出：用于可视化的三角形网格和带有额外参数元数据用于定量分析的三角形网格。这种方法独特地结合了LLM辅助模板创建、同时支持表型和渲染的数学连续表示以及通过PD的直接参数控制。该框架使植物科学能够民主化复杂的几何建模，同时保持数学严谨性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决农业应用中生成准确、可编辑且分析就绪的3D植物几何模型的挑战。这个问题很重要，因为精确的植物模型对计算表型分析和基于物理的模拟至关重要，可用于植物育种、农学研究等场景，但现有方法要么需要大量训练数据且不可编辑，要么需要专业几何知识，限制了植物科学中先进3D建模的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法(学习重建、传统程序建模)的局限性，然后利用大型语言模型(LLM)在代码生成和多模态推理方面的能力。作者借鉴了多个领域的工作：程序植物建模(L-systems)、逆程序建模、学习植物重建、参数几何表示(NURBS/B样条)和LLM辅助3D建模。设计了一个两阶段流程：第一阶段通过自然语言对话让LLM生成程序植物生成器和植物描述符；第二阶段通过调整参数使模型与实际点云数据匹配，实现了无需专业编程知识即可生成植物模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用LLM辅助生成程序化的植物几何模型，使植物专家能通过自然语言描述和可编辑参数文件生成准确3D植物模型。整体流程分两阶段：1) LLM辅助模板设计：用户通过自然语言'植物精炼'与LLM交互，生成Python程序生成器和YAML参数文件，创建初始3D模板；2) 手动参数调整：专家调整参数文件使模型匹配实际点云数据。植物器官用B样条曲面表示，支持单子叶和双子叶不同植物架构，输出包括可视化网格和分析就绪的参数化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) LLM辅助程序植物生成器，无需编程知识；2) 分析就绪的连续B样条曲面表示，兼具数学严谨性和可视化能力；3) 人类可读的参数控制界面，实现分层参数继承；4) 跨物种验证能力。相比之前工作：1) 学习重建方法需大量训练数据且不可编辑，而FloraForge无需训练数据且完全可编辑；2) 传统程序建模需专业几何知识，FloraForge通过LLM简化了模板创建；3) 逆程序方法依赖预定义模板，FloraForge可自动适应新物种；4) 现有LLM 3D系统缺乏植物领域知识，FloraForge专为植物形态学设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FloraForge通过利用大型语言模型自动化程序化植物模板创建，使植物科学家无需编程专业知识即可生成可编辑、分析就绪且生物准确的3D植物几何模型，解决了传统方法中可访问性与分析效用之间的根本矛盾。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.</description>
      <author>example@mail.com (Mozhgan Hadadi, Talukder Z. Jubery, Patrick S. Schnable, Arti Singh, Bedrich Benes, Adarsh Krishnamurthy, Baskar Ganapathysubramanian)</author>
      <guid isPermaLink="false">2512.11925v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification</title>
      <link>http://arxiv.org/abs/2512.13617v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LightTopoGAT是一种轻量级图注意力网络，通过拓扑增强提高节点特征表示，在图分类任务中实现了优越性能且不增加架构复杂性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在图分类任务中取得显著成功，但通常需要大量计算资源且难以有效捕捉全局图特性。&lt;h4&gt;目的&lt;/h4&gt;引入LightTopoGAT，一种轻量级图注意力网络，通过拓扑增强增强节点特征，改进图表示学习。&lt;h4&gt;方法&lt;/h4&gt;通过整合节点度和局部聚类系数进行拓扑增强，利用简化的注意力机制保持参数效率，并整合局部消息传递方案通常忽略的结构信息。&lt;h4&gt;主要发现&lt;/h4&gt;在MUTAG、ENZYMES和PROTEINS三个基准数据集上，相比GCN、GraphSAGE和标准GAT等基线，LightTopoGAT在MUTAG上准确率提高6.6%，在PROTEINS上提高2.2%；消融研究证实性能提升直接来自于拓扑特征的引入。&lt;h4&gt;结论&lt;/h4&gt;LightTopoGAT展示了简单而有效的策略来增强图神经网络性能，无需增加架构复杂性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在图分类任务中已经展示了显著的成功，但它们通常需要大量的计算资源，并且难以有效捕捉全局图特性。我们引入了LightTopoGAT，一种轻量级图注意力网络，通过拓扑增强来增强节点特征，通过整合节点度和局部聚类系数来改进图表示学习。所提出的方法通过简化的注意力机制保持参数效率，同时整合了局部消息传递方案通常忽略的结构信息。通过在MUTAG、ENZYMES和PROTEINS三个基准数据集上的全面实验，我们表明LightTopoGAT相比包括GCN、GraphSAGE和标准GAT在内的既定基线实现了优越性能，在MUTAG上准确率提高了6.6%，在PROTEINS上提高了2.2%。消融研究进一步证实，这些性能提升直接来自于拓扑特征的包含，展示了一种简单而有效的策略来增强图神经网络性能，而无需增加架构复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.</description>
      <author>example@mail.com (Ankit Sharma, Sayan Roy Gupta)</author>
      <guid isPermaLink="false">2512.13617v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization</title>
      <link>http://arxiv.org/abs/2512.12809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Source code, experiment scripts, and results are publicly available at https://github.com/junbolian/OPAL. The real-world application part hasn't been done yet&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为操作符编程算法(OPAL)的黑盒优化新框架，通过元学习为每个问题实例单独设计优化策略，在CEC 2017测试套件上取得了与最先进算法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;黑盒优化通常依赖进化和群体智能算法，但这些算法的性能高度依赖于具体问题，且大多数现有方法是基于启发式比喻的。&lt;h4&gt;目的&lt;/h4&gt;开发一种景观感知的框架，能够针对每个问题实例单独学习优化策略，以提升连续黑盒优化性能。&lt;h4&gt;方法&lt;/h4&gt;将优化器视为小型搜索操作符程序，使用标准差分进化基线探测问题景观，构建采样点的k最近邻图，通过图神经网络编码轨迹，最后由元学习器生成分阶段的探索、重启和局部搜索操作符调度。&lt;h4&gt;主要发现&lt;/h4&gt;单个元训练的OPAL策略在CEC 2017测试套件上与最先进的自适应差分进化算法具有统计竞争力，且显著优于简单基线；消融研究验证了设计阶段、轨迹图和操作符编程表示的选择；元组件仅增加适度的计算开销。&lt;h4&gt;结论&lt;/h4&gt;操作符编程结合景观感知的每实例设计是黑盒优化中超越传统启发式比喻算法的有效实用方法。&lt;h4&gt;翻译&lt;/h4&gt;黑盒优化通常依赖于进化和群体智能算法，其性能高度依赖于问题本身。我们将优化器视为一个小型搜索操作符的程序，并为每个问题实例单独学习这个操作符程序。我们在操作符编程算法(OPAL)中实现了这一想法，这是一个用于连续黑盒优化的景观感知框架，它使用标准差分进化基线以小的设计预算探测景观，构建采样点的k最近邻图，并使用图神经网络编码这一轨迹。然后，元学习器将得到的表示映射到分阶段的探索、重启和局部搜索操作符调度。在CEC 2017测试套件上，单个元训练的OPAL策略与最先进的自适应差分进化变体具有统计竞争力，并在非参数测试下比简单基线方法取得显著改进。对CEC 2017的消融研究验证了设计阶段、轨迹图和操作符编程表示的选择，同时元组件仅增加了适度的时钟开销。总体而言，结果表明操作符编程、景观感知的每实例设计是黑盒优化中超越启发式比喻算法的实用前进方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.</description>
      <author>example@mail.com (Junbo Jacob Lian, Mingyang Yu, Kaichen Ouyang, Shengwei Fu, Rui Zhong, Yujun Zhang, Jun Zhang, Huiling Chen)</author>
      <guid isPermaLink="false">2512.12809v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization</title>
      <link>http://arxiv.org/abs/2512.12669v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DynaGen是一种统一的时间知识图谱推理方法，通过动态构建实体中心子图和条件扩散过程，有效解决了内插和外推任务中的关键挑战，在多个基准数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;时间知识图谱推理(TKGR)旨在完成时间线上的缺失事实元素。根据查询时间位置，任务分为内插和外推。现有内插方法通常将时间信息嵌入单个事实中完成历史知识，外推技术则利用图快照序列模型识别重复模式预测未来事件。这些方法面临内插上下文建模有限和外推认知泛化偏差两大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的TKGR方法，解决现有方法在内插和外推方面的关键挑战，提高预测性能。&lt;h4&gt;方法&lt;/h4&gt;DynaGen方法包含两部分：对于内插，动态构建以实体为中心的子图，使用协同双分支GNN编码器处理以捕获不断变化的上下文结构；对于外推，应用条件扩散过程，迫使模型学习底层进化原理而非表面模式，增强预测未见未来事件的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在六个基准数据集上的大量实验表明，DynaGen实现了最先进的性能。与第二好的模型相比，DynaGen将内插的倒数排名平均值(MRR)提高了2.61分，外推提高了1.45分。&lt;h4&gt;结论&lt;/h4&gt;DynaGen通过统一的方法有效解决了时间知识图谱推理中的内插和外推挑战，实验证明其在多个数据集上显著优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;时间知识图谱推理(TKGR)旨在完成时间线上的缺失事实元素。根据查询的时间位置，任务分为内插和外推。现有的内插方法通常将时间信息嵌入到单个事实中以完成缺失的历史知识，而外推技术则经常利用图快照上的序列模型来识别重复模式用于未来事件预测。这些方法面临两个关键挑战：内插中的上下文建模有限和外推中的认知泛化偏差。为解决这些问题，我们提出了一个统一的TKGR方法，命名为DynaGen。对于内插，DynaGen动态构建以实体为中心的子图，并使用协同双分支GNN编码器处理它们，以捕获不断变化的上下文结构。对于外推，它应用条件扩散过程，迫使模型学习底层进化原理而非表面模式，增强其预测未见未来事件的能力。在六个基准数据集上的大量实验表明，DynaGen实现了最先进的性能。平均而言，与第二好的模型相比，DynaGen将内插的倒数排名平均值(MRR)提高了2.61分，外推提高了1.45分。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.</description>
      <author>example@mail.com (Jiawei Shen, Jia Zhu, Hanghui Guo, Weijie Shi, Guoqing Ma, Yidan Liang, Jingjiang Liu, Hao Chen, Shimin Di)</author>
      <guid isPermaLink="false">2512.12669v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2512.12654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图的框架，将乌尔都语小说建模为角色交互网络，以检验仅从叙事结构是否可以推断作者风格。&lt;h4&gt;背景&lt;/h4&gt;作者分析传统上关注文本中的词汇和风格线索，而叙事结构等更高级别的特征研究不足，特别是在低资源语言如乌尔都语方面。&lt;h4&gt;目的&lt;/h4&gt;探索仅通过叙事结构而非传统的词汇和风格特征来推断乌尔都语小说作者风格的可行性。&lt;h4&gt;方法&lt;/h4&gt;将每部乌尔都语小说表示为图，节点对应角色，边表示他们在叙事中的共现；系统比较多种图表示方法，包括全局结构特征、节点级语义摘要、无监督图嵌入和监督图神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;在52部由7位作者撰写的乌尔都语小说数据集上，学习的图表示显著优于手工制作和无监督基线，在严格的作者感知评估协议下准确率达到0.857。&lt;h4&gt;结论&lt;/h4&gt;通过角色交互网络建模叙事结构可以有效推断乌尔都语小说的作者风格，证明了叙事结构在作者分析中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;作者分析传统上关注文本中的词汇和风格线索，而更高层次的叙事结构研究不足，特别是对于乌尔都语这样的低资源语言。这项工作提出了一种基于图的框架，将乌尔都语小说建模为角色交互网络，以检验仅从叙事结构是否可以推断作者风格。每部小说被表示为一个图，其中节点对应角色，边表示他们在叙事邻近中的共现。我们系统地比较了多种图表示方法，包括全局结构特征、节点级语义摘要、无监督图嵌入和监督图神经网络。在包含52部由七位作者撰写的乌尔都语小说的数据集上的实验表明，学习的图表示显著优于手工制作和无监督基线，在严格的作者感知评估协议下达到0.857的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.</description>
      <author>example@mail.com (Hassan Mujtaba, Hamza Naveed, Hanzlah Munir)</author>
      <guid isPermaLink="false">2512.12654v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2512.12642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Torch Geometric Pool (tgp) 是一个用于图神经网络分层池化的库，基于Pytorch Geometric构建，提供多种池化算子，具有一致API和模块化设计。&lt;h4&gt;背景&lt;/h4&gt;图神经网络需要分层池化操作，但目前可能缺乏统一且高效的解决方案。&lt;h4&gt;目的&lt;/h4&gt;介绍Torch Geometric Pool库，展示其结构并进行全面基准测试，比较不同下游任务中实现的图池化方法的性能。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于Pytorch Geometric的库，提供多种池化算子，采用一致API和模块化设计，包括预计算池化功能以加速训练。展示了库的结构并进行了广泛的基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;最优池化算子的选择取决于具体的任务和数据，支持需要能够快速原型的库。&lt;h4&gt;结论&lt;/h4&gt;Torch Geometric Pool库提供了多样化和高效的池化操作，能够支持不同任务下的快速原型开发。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了Torch Geometric Pool (tgp)，这是一个用于图神经网络分层池化的库。基于Pytorch Geometric构建，Torch Geometric Pool (tgp) 提供了多种池化算子，在一致的API和模块化设计下统一。该库强调可用性和可扩展性，包括预计算池化等功能，这些功能显著加速了一类算子的训练。在本文中，我们展示了tgp的结构并进行了广泛的基准测试。后者展示了库的功能，并系统比较了不同下游任务中实现的图池化方法的性能。结果表明，最优池化算子的选择取决于任务和手头的数据，这支持了需要能够快速原型的库的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Torch Geometric Pool (tgp), a library for hierarchical pooling in Graph Neural Networks. Built upon Pytorch Geometric, Torch Geometric Pool (tgp) provides a wide variety of pooling operators, unified under a consistent API and a modular design. The library emphasizes usability and extensibility, and includes features like precomputed pooling, which significantly accelerate training for a class of operators. In this paper, we present tgp's structure and present an extensive benchmark. The latter showcases the library's features and systematically compares the performance of the implemented graph-pooling methods in different downstream tasks. The results, showing that the choice of the optimal pooling operator depends on tasks and data at hand, support the need for a library that enables fast prototyping.</description>
      <author>example@mail.com (Filippo Maria Bianchi, Carlo Abate, Ivan Marisca)</author>
      <guid isPermaLink="false">2512.12642v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling</title>
      <link>http://arxiv.org/abs/2512.12526v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 3 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究将经验模态分解应用于MSCI世界指数，将得到的本征模态函数转换为图表示，以便使用图神经网络进行建模。研究通过拓扑分析发现了不同频率分量的图结构特性，为金融时间序列预测提供了指导。&lt;h4&gt;背景&lt;/h4&gt;金融时间序列分析需要有效的方法来捕捉不同时间尺度的特征，图神经网络为时间序列建模提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;将经验模态分解得到的本征模态函数转换为图表示，以便使用图神经网络进行建模，并分析不同频率分量的图结构特性。&lt;h4&gt;方法&lt;/h4&gt;使用CEEMDAN提取9个IMFs，涵盖从高频波动到长期趋势的各个时间尺度。每个IMF通过四种时间序列到图的方法转换为图：自然可见性图、水平可见性图、递归图和转移图。&lt;h4&gt;主要发现&lt;/h4&gt;拓扑分析显示出明显的尺度依赖结构：高频IMFs产生密集的高度连接的小世界图，而低频IMFs产生具有更长特征路径长度的稀疏网络。基于可见性的方法对幅度变化更敏感，通常产生更高的聚类，而递归图更好地保留了时间依赖性。&lt;h4&gt;结论&lt;/h4&gt;这些结果为设计针对分解分量的结构特性的GNN架构提供了指导，支持金融时间序列更有效的预测建模。&lt;h4&gt;翻译&lt;/h4&gt;本研究将经验模态分解应用于MSCI世界指数，并将得到的本征模态函数转换为图表示，以便使用图神经网络进行建模。使用CEEMDAN，我们提取了九个本征模态函数，涵盖了从高频波动到长期趋势的各个时间尺度。每个本征模态函数通过四种时间序列到图的方法转换为图：自然可见性图、水平可见性图、递归图和转移图。拓扑分析显示出明显的尺度依赖结构：高频本征模态函数产生密集的高度连接的小世界图，而低频本征模态函数产生具有更长特征路径长度的稀疏网络。基于可见性的方法对幅度变化更敏感，通常产生更高的聚类，而递归图更好地保留了时间依赖性。这些结果为设计针对分解分量的结构特性的图神经网络架构提供了指导，支持金融时间序列更有效的预测建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.</description>
      <author>example@mail.com (Agustín M. de los Riscos, Julio E. Sandubete, Diego Carmona-Fernández, León Beleña)</author>
      <guid isPermaLink="false">2512.12526v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>GoMS: Graph of Molecule Substructure Network for Molecule Property Prediction</title>
      <link>http://arxiv.org/abs/2512.12489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为GoMS的新型图神经网络架构，用于分子性质预测，通过建模分子子结构之间的相互作用和空间排列，优于现有的ESAN等方法。&lt;h4&gt;背景&lt;/h4&gt;现有的图神经网络方法如ESAN将分子视为独立子结构的集合，忽略了这些组件之间的重要关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够明确建模分子子结构之间相互作用和空间排列的新型架构，以提高分子性质预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Graph of Molecule Substructures (GoMS)，构建一个图结构，其中节点代表子图，边捕获它们的结构关系，保留关于子结构如何在分子中连接和重叠的关键拓扑信息。&lt;h4&gt;主要发现&lt;/h4&gt;GoMS在公共分子数据集上优于ESAN和其他基线方法，特别是对于大分子(超过100个原子)；随着分子尺寸增加，性能差距扩大；理论分析表明GoMS可以区分具有相同子图组成但不同空间排列的分子。&lt;h4&gt;结论&lt;/h4&gt;GoMS通过捕获基于集合的方法中丢失的子结构关系，代表了面向真实世界应用的可扩展和可解释分子性质预测的重要进展。&lt;h4&gt;翻译&lt;/h4&gt;尽管图神经网络在分子性质预测中显示出显著的成功，但当前的方法如等变子结构聚合网络(ESAN)将分子视为独立子结构的集合，忽略了这些组件之间的关键关系。我们提出了分子子结构图(GoMS)，一种新型架构，明确建模分子子结构之间的相互作用和空间排列。与ESAN的基于集合的表示不同，GoMS构建一个图，其中节点代表子图，边捕获它们的结构关系，保留了关于子结构如何在分子中连接和重叠的关键拓扑信息。通过在公共分子数据集上的广泛实验，我们证明GoMS优于ESAN和其他基线方法，特别是对于包含超过100个原子的大分子有显著改进。随着分子尺寸的增加，性能差距扩大，展示了GoMS在模拟工业规模分子方面的有效性。我们的理论分析表明，GoMS可以区分具有相同子图组成但不同空间排列的分子。我们的方法在涉及复杂分子的材料科学应用中显示出特别的前景，这些分子的性质来自多个功能单元之间的相互作用。通过捕获基于集合的方法中丢失的子结构关系，GoMS代表了面向真实世界应用的可扩展和可解释分子性质预测的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While graph neural networks have shown remarkable success in molecular property prediction, current approaches like the Equivariant Subgraph Aggregation Networks (ESAN) treat molecules as bags of independent substructures, overlooking crucial relationships between these components. We present Graph of Molecule Substructures (GoMS), a novel architecture that explicitly models the interactions and spatial arrangements between molecular substructures. Unlike ESAN's bag-based representation, GoMS constructs a graph where nodes represent subgraphs and edges capture their structural relationships, preserving critical topological information about how substructures are connected and overlap within the molecule. Through extensive experiments on public molecular datasets, we demonstrate that GoMS outperforms ESAN and other baseline methods, with particularly improvements for large molecules containing more than 100 atoms. The performance gap widens as molecular size increases, demonstrating GoMS's effectiveness for modeling industrial-scale molecules. Our theoretical analysis demonstrates that GoMS can distinguish molecules with identical subgraph compositions but different spatial arrangements. Our approach shows particular promise for materials science applications involving complex molecules where properties emerge from the interplay between multiple functional units. By capturing substructure relationships that are lost in bag-based approaches, GoMS represents a significant advance toward scalable and interpretable molecular property prediction for real-world applications.</description>
      <author>example@mail.com (Shuhui Qu, Cheolwoo Park)</author>
      <guid isPermaLink="false">2512.12489v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Can Graphs Improve Tabular Foundation Models?</title>
      <link>http://arxiv.org/abs/2512.12405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了BOLERO，一种结合简单图先验的轻量级方法，用于增强预训练的表格转换器。通过在大量数据集上的评估，证明了这种方法的有效性，并在分类和回归任务上都取得了最佳性能。&lt;h4&gt;背景&lt;/h4&gt;表格数据是许多现实世界系统的核心。虽然最近的表格转换器和上下文学习器包含有限的行间推理能力，但大多数方法仍然缺乏对实例之间关系的明确建模机制，尽管相似样本通常共享相关结果。&lt;h4&gt;目的&lt;/h4&gt;研究引入简单的图先验是否能增强预训练的表格转换器。&lt;h4&gt;方法&lt;/h4&gt;提出了BOLERO，这是一个轻量级的静态二分图头，它增强了RoBERTa-Tab（一种使用掩码令牌预测进行预训练的RoBERTa风格表格主干）。每个实例连接到特征/值锚点；一个小型图神经网络优化行表示，同时主干保持冻结。在80个分类和64个回归数据集上进行了评估，并与多种强基线方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;BOLERO在分类和回归中都取得了最多的统计显著胜利，表明轻量级图先验显著改善了预训练的表格转换器。研究使用了Wilcoxon符号秩检验和效应大小分析来确保统计上可靠的结论。&lt;h4&gt;结论&lt;/h4&gt;轻量级图先验可以有效地增强预训练的表格转换器，提高其在表格数据任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;表格数据是许多现实世界系统的核心。虽然最近的表格转换器和上下文学习器（如SAINT、TP-BERTa、TabPFN、TabICL和MITRA）包含有限的行间推理能力，但大多数方法仍然缺乏对实例之间关系的明确建模机制，尽管相似样本通常共享相关结果。我们研究了引入简单图先验是否能增强预训练的表格转换器。具体来说，我们引入了BOLERO，这是一个轻量级的静态二分图头，它增强了RoBERTa-Tab（一种使用掩码令牌预测进行预训练的RoBERTa风格表格主干）。每个实例连接到特征/值锚点；一个小型图神经网络优化行表示，同时主干保持冻结。我们在TP-BERTa基准套件的80个分类和64个回归数据集上进行了评估，并与包括XGBoost、CatBoost、TabPFN-v2、MITRA、TabICL、TP-BERTa和RoBERTa-Tab在内的强基线进行比较。为确保统计上可靠的结论，我们遵循多数据集评估的最佳实践：使用每对数据集分数差异的Wilcoxon符号秩检验和效应大小（中位数改进及置信区间），而不是依赖于竞争者池的均值秩事后检验。BOLERO在分类和回归中都取得了最多的统计显著胜利，证明了轻量级图先验显著改善了预训练的表格转换器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \emph{simple graph priors} can enhance \emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.</description>
      <author>example@mail.com (Franck Le, Keith Grueneberg, Erich Nahum, Vadim Sheinin)</author>
      <guid isPermaLink="false">2512.12405v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>High-Dimensional Tensor Discriminant Analysis: Low-Rank Discriminant Structure, Representation Synergy, and Theoretical Guarantees</title>
      <link>http://arxiv.org/abs/2512.12122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高维CP低秩张量判别分析方法（CP-TDA），针对高维张量值预测器的分类问题。该方法结合随机复合PCA初始化和迭代优化算法，在张量高斯混合模型下实现了全局收敛和最优误分类率。同时开发了半参数张量判别模型处理非正态张量数据，实验表明该方法在高维小样本情况下显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;高维张量值预测器在现代应用中日益普遍，特别是作为神经网络的表示学习结果。然而，现有的张量分类方法依赖于稀疏性或Tucker结构，通常缺乏理论保证。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于CP低秩结构的判别张量建模方法，解决现有张量分类方法的局限性，并提供理论保证，特别是在处理相关性和各向异性噪声、弱信号强度和不相干条件下。&lt;h4&gt;方法&lt;/h4&gt;1) 在张量高斯混合模型下提出CP-TDA；2) 使用随机复合PCA(rc-PCA)初始化处理相关性和各向异性噪声；3) 开发迭代优化算法；4) 建立全局收敛和最优误分类率理论；5) 开发半参数张量判别模型；6) 通过生成模型将张量表示映射到适合CP-TDA的潜在空间；7) 将误分类风险分解为表示、近似和估计误差。&lt;h4&gt;主要发现&lt;/h4&gt;1) CP低秩结构适用于判别张量建模；2) rc-PCA初始化对处理相关性和各向异性噪声至关重要；3) 在较弱信号强度下仍能有效工作；4) 建立了全局收敛和最优误分类率理论；5) 半参数模型能有效处理非正态张量数据；6) 在图分类任务中显著优于现有方法，特别是在高维小样本情况下。&lt;h4&gt;结论&lt;/h4&gt;CP-TDA方法为高维张量分类提供了有效解决方案，结合CP低秩结构和优化算法不仅提供了理论保证，还在实验中表现出优越性能，特别是在处理神经网络学习得到的表示和高维小样本情况下。&lt;h4&gt;翻译&lt;/h4&gt;高维张量值预测器在现代应用中日益普遍，特别是作为从神经网络学习得到的表示。现有的张量分类方法依赖于稀疏性或Tucker结构，通常缺乏理论保证。受判别信号集中在少数多线性分量上的经验证据启发，我们引入了判别张量的CP低秩结构，这是一种尚未被探索的建模视角。在张量高斯混合模型下，我们提出了具有随机复合PCA（rc-PCA）初始化的高维CP低秩张量判别分析（CP-TDA），这对于处理较弱信号强度和不相干条件下的相关性和各向异性噪声至关重要，随后是迭代优化算法。我们建立了全局收敛和最小最大最优误分类率。为了处理偏离张量正态性的张量数据，我们开发了首个半参数张量判别模型，其中学习到的张量表示通过深度生成模型映射到为CP-TDA量身定制的潜在空间。误分类风险分解为表示误差、近似误差和估计误差。在图分类上的数值研究和真实数据分析表明，该方法比现有的张量分类器和最先进的图神经网络有显著提升，特别是在高维小样本情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional tensor-valued predictors arise in modern applications, increasingly as learned representations from neural networks. Existing tensor classification methods rely on sparsity or Tucker structures and often lack theoretical guarantees. Motivated by empirical evidence that discriminative signals concentrate along a few multilinear components, we introduce CP low-rank structure for the discriminant tensor, a modeling perspective not previously explored. Under a Tensor Gaussian Mixture Model, we propose high-dimensional CP low-rank Tensor Discriminant Analysis (CP-TDA) with Randomized Composite PCA (\textsc{rc-PCA}) initialization, that is essential for handling dependent and anisotropic noise under weaker signal strength and incoherence conditions, followed by iterative refinement algorithm. We establish global convergence and minimax-optimal misclassification rates.  To handle tensor data deviating from tensor normality, we develop the first semiparametric tensor discriminant model, in which learned tensor representations are mapped via deep generative models into a latent space tailored for CP-TDA. Misclassification risk decomposes into representation, approximation, and estimation errors. Numerical studies and real data analysis on graph classification demonstrate substantial gains over existing tensor classifiers and state-of-the-art graph neural networks, particularly in high-dimensional, small-sample regimes.</description>
      <author>example@mail.com (Elynn Chen, Yuefeng Han, Jiayu Li)</author>
      <guid isPermaLink="false">2512.12122v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Scalable IP Mimicry: End-to-End Deceptive IP Blending to Overcome Rectification and Scale Limitations of IP Camouflage</title>
      <link>http://arxiv.org/abs/2512.12061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出两种新型端到端模型，解决现有IP伪装技术的局限性，包括高开销后处理、不易扩展以及逻辑表示与RE分析不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;半导体知识产权盗窃造成巨大经济损失，估计每年损失2250亿至6000亿美元，尽管有CHIPS法案等举措，许多半导体设计仍然容易受到逆向工程攻击。&lt;h4&gt;目的&lt;/h4&gt;解决现有IP Camouflage技术的高开销、不易扩展以及逻辑表示与RE分析不匹配的问题。&lt;h4&gt;方法&lt;/h4&gt;提出两种新颖的端到端模型：图匹配算法解决表示问题，基于DNAS的NAND阵列模型实现可扩展性，并引入拟态感知分区方法实现大规模设计的分而治之。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型能够抵抗SAT和GNN-RE攻击，为端到端欺骗性IP设计提供了高效且可扩展的路径。&lt;h4&gt;结论&lt;/h4&gt;这些新型模型克服了现有IP伪装技术的局限性，为半导体知识产权保护提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;半导体知识产权盗窃造成估计每年2250亿至6000亿美元的损失。尽管有CHIPS法案等举措，许多半导体设计仍然容易受到逆向工程攻击。IP Camouflage是一种突破性技术，通过'拟态欺骗'使整个模块伪装成不同的IP，超越了传统伪装的逻辑门隐藏方法。然而，它面临关键局限性：需要高开销的后生成校正步骤，不易扩展，且使用的AIG逻辑表示与标准RE分析流程不匹配。本文通过引入两种新颖的端到端模型解决了这些缺点。我们提出了图匹配算法来解决表示问题，以及基于DNAS的NAND阵列模型来实现可扩展性。为此，我们还引入了拟态感知分区方法，使大规模设计能够采用分而治之的方法。我们的结果表明，这些模型能够抵抗SAT和GNN-RE攻击，为端到端欺骗性IP设计提供了高效且可扩展的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor intellectual property (IP) theft incurs estimated annual losses ranging from $225 billion to $600 billion. Despite initiatives like the CHIPS Act, many semiconductor designs remain vulnerable to reverse engineering (RE). IP Camouflage is a recent breakthrough that expands beyond the logic gate hiding of traditional camouflage through "mimetic deception," where an entire module masquerades as a different IP. However, it faces key limitations: requires a high-overhead post-generation rectification step, is not easily scalable, and uses an AIG logic representation that is mismatched with standard RE analysis flows. This paper addresses these shortcommings by introducing two novel, end-to-end models. We propose a Graph-Matching algorithm to solve the representation problem and a DNAS-based NAND Array model to achieve scalability. To facilitate this, we also introduce a mimicry-aware partitioning method, enabling a divide-and-conquer approach for large-scale designs. Our results demonstrate that these models are resilient to SAT and GNN-RE attacks, providing efficient and scalable paths for end-to-end deceptive IP design.</description>
      <author>example@mail.com (Junling Fan, George Rushevich, Giorgio Rusconi, Mengdi Zhu, Reiner Dizon-Paradis, Domenic Forte)</author>
      <guid isPermaLink="false">2512.12061v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp</title>
      <link>http://arxiv.org/abs/2512.12048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CAMAC-DRA框架，通过Smart2Charge应用优化智能电动汽车充电生态系统，协调250辆电动汽车和45个充电站网络中的自主充电智能体，通过上下文感知决策适应动态环境条件。&lt;h4&gt;背景&lt;/h4&gt;随着电动汽车普及，充电基础设施管理和优化成为重要问题，需要能够协调多方利益并适应动态环境变化的解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够平衡电动汽车用户、电网运营商、充电站运营商、车队运营商和环境因素等多方利益的智能充电系统，提高能源效率、降低成本并减少电网压力。&lt;h4&gt;方法&lt;/h4&gt;采用多智能体方法，结合协调的深度Q网络、图神经网络和注意力机制，处理20个上下文特征，通过加权协调机制和共识协议平衡各方需求。&lt;h4&gt;主要发现&lt;/h4&gt;在441,077笔充电交易的真实数据集上验证，相比基线算法表现更优，达到92%协调成功率，15%能源效率提升，10%成本降低，20%电网压力减少，2.3倍更快收敛速度，88%训练稳定性和85%样本效率，商业可行性验证显示净现值为-122,962美元，通过可再生能源整合实现69%成本降低。&lt;h4&gt;结论&lt;/h4&gt;CAMAC-DRA框架成功实现了上下文感知的多利益相关者协调，平衡竞争性目标并适应实时变量，是智能电动汽车协调充电和可持续交通电气化的突破性解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文通过Smart2Charge应用提出了一个新颖的上下文敏感多智能体协调动态资源分配框架，用于优化智能电动汽车充电生态系统。该系统协调250辆电动汽车和45个充电站网络中的自主充电智能体，并通过上下文感知决策适应动态环境条件。我们的多智能体方法采用协调的深度Q网络与图神经网络和注意力机制相结合，处理包括天气模式、交通状况、电网负载波动和电价在内的20个上下文特征。该框架通过加权协调机制和共识协议平衡五个生态系统利益相关者的需求。使用包含441,077笔充电交易的真实世界数据集进行的全面验证表明，与基线算法相比性能更优。CAMAC-DRA框架实现92%的协调成功率，15%的能源效率提升，10%的成本降低，20%的电网压力减少，以及2.3倍的更快收敛速度，同时保持88%的训练稳定性和85%的样本效率。真实世界验证证实了其商业可行性，净现值为-122,962美元，通过可再生能源整合实现69%的成本降低。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.</description>
      <author>example@mail.com (Muddsair Sharif, Huseyin Seker)</author>
      <guid isPermaLink="false">2512.12048v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating Sparse Matrix-Matrix Multiplication on GPUs with Processing Near HBMs</title>
      <link>http://arxiv.org/abs/2512.12036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于哈希的多阶段稀疏矩阵乘法(HM-SpGEMM)和间接内存访问加速技术(AIA)，通过硬件-软件协同设计优化GPU高带宽内存上的稀疏矩阵乘法操作，显著提升了性能。&lt;h4&gt;背景&lt;/h4&gt;稀疏矩阵乘法(SpGEMM)是众多科学计算和数据分析应用中的基本操作，常因不规则内存访问模式而成为性能瓶颈。&lt;h4&gt;目的&lt;/h4&gt;开发一种创新的近内存处理方法，优化GPU HBM上的SpGEMM操作，解决内存访问效率问题。&lt;h4&gt;方法&lt;/h4&gt;提出Hash based Multi-phase SpGEMM on GPU和AIA技术，构建硬件-软件协同设计框架，并在多种图工作负载上进行评估，包括图收缩、马尔可夫聚类和图神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;对于图分析应用，AIA比纯软件实现减少17.3%时间；与cuSPARSE相比，图收缩减少76.5%时间，马尔可夫聚类减少58.4%时间；对于GNN训练，混合方法平均加速1.43倍，比cuSPARSE加速1.95倍，在大规模数据集上最高达4.18倍。&lt;h4&gt;结论&lt;/h4&gt;该框架在处理复杂、特定应用的工作负载时，比现有方法展现显著性能改进，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;稀疏矩阵乘法(SpGEMM)是众多科学计算和数据分析应用中的基本操作，常因不规则内存访问模式而成为瓶颈。本文提出基于哈希的多阶段GPU稀疏矩阵乘法和间接内存访问加速技术(AIA)，这是一种创新的近内存处理方法，用于优化GPU高带宽内存上的SpGEMM。我们的SpGEMM硬件-软件协同设计框架在处理复杂、特定应用的工作负载时，比最先进方法展现出显著的性能改进。我们在多种图工作负载上评估了该方法，包括图收缩、马尔可夫聚类和图神经网络(GNNs)，展示了其实际应用价值。对于图分析应用，AIA比纯软件实现减少17.3%的时间，与cuSPARSE相比，图收缩减少76.5%时间，马尔可夫聚类减少58.4%时间。对于具有结构化全局剪枝的GNN训练应用，我们的混合方法在六个基准数据集和三种架构(GCN、GIN、GraphSAGE)上平均比纯软件实现加速1.43倍，与cuSPARSE相比GNN工作负载加速1.95倍，在大规模数据集上最高可达4.18倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse General Matrix-Matrix Multiplication (SpGEMM) is a fundamental operation in numerous scientific computing and data analytics applications, often bottlenecked by irregular memory access patterns. This paper presents Hash based Multi-phase SpGEMM on GPU and the Acceleration of Indirect Memory Access (AIA) technique, a novel custom near-memory processing approach to optimizing SpGEMM on GPU HBM. Our hardware-software co-designed framework for SpGEMM demonstrates significant performance improvements over state-of-the-art methods, particularly in handling complex, application-specific workloads. We evaluate our approach on various graph workloads, including graph contraction, Markov clustering, and Graph Neural Networks (GNNs), showcasing its practical applicability. For graph analytics applications, AIA demonstrates up to 17.3% time reduction from the software-only implementation, while achieving time reduction of 76.5% for Graph Contraction and 58.4% for Markov Clustering compared to cuSPARSE. For GNN training applications with structured global pruning, our hybrid approach delivers an average of 1.43x speedup over software-only implementation across six benchmark datasets and three architectures (GCN, GIN, GraphSAGE), and shows 1.95x speedup for GNN workloads when compared to cuSPARSE, with up to 4.18x gains on large-scale datasets.</description>
      <author>example@mail.com (Shiju Li, Younghoon Min, Hane Yie, Hoshik Kim, Soohong Ahn, Joonseop Sim, Chul-Ho Lee, Jongryool Kim)</author>
      <guid isPermaLink="false">2512.12036v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org</title>
      <link>http://arxiv.org/abs/2512.11935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了AGAPI（AtomGPT.org API），一个开放访问的智能AI平台，整合了多个开源大型语言模型和材料科学API端点，通过统一的编排框架连接数据库、模拟工具和机器学习模型，为材料研究提供可扩展、透明的基础，促进可复现的AI加速材料发现。&lt;h4&gt;背景&lt;/h4&gt;人工智能正在重塑科学发现，但其在材料研究中的应用受到碎片化计算生态系统、可复现性挑战以及对商业大型语言模型依赖的限制。&lt;h4&gt;目的&lt;/h4&gt;开发一个开放访问的智能AI平台，整合多种开源LLMs和材料科学API端点，统一数据库、模拟工具和机器学习模型，通过自主构建和执行多步骤工作流程，实现AI加速的材料发现。&lt;h4&gt;方法&lt;/h4&gt;AGAPI采用Agent-Planner-Executor-Summarizer架构，自主构建和执行跨越材料数据检索、图神经网络属性预测、机器学习力场优化、紧束缚计算、衍射分析和逆向设计的多步骤工作流程。通过端到端工作流程（包括异质结构构建、粉末X射线衍射分析和半导体缺陷工程）进行演示，并使用30多个示例提示作为测试案例评估AGAPI，将智能代理预测与实验数据进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;AGAPI能够执行多达十个连续操作的工作流程，包括异质结构构建、粉末X射线衍射分析和半导体缺陷工程。评估表明，AGAPI提供了可扩展和透明的基础，用于可复现、AI加速的材料发现。&lt;h4&gt;结论&lt;/h4&gt;AGAPI作为一个拥有1,000多名活跃用户的平台，为材料研究提供了有效的AI解决方案，解决了计算生态系统碎片化、可复现性挑战和对商业模型依赖的问题。&lt;h4&gt;翻译&lt;/h4&gt;人工智能正在重塑科学发现，但其在材料研究中的应用受到碎片化计算生态系统、可复现性挑战以及对商业大型语言模型依赖的限制。在此，我们介绍了AGAPI（AtomGPT.org API），一个开放访问的智能AI平台，整合了八个以上的开源大型语言模型和二十多个材料科学API端点，通过统一的编排框架连接数据库、模拟工具和机器学习模型。AGAPI采用Agent-Planner-Executor-Summarizer架构，自主构建和执行跨越材料数据检索、图神经网络属性预测、机器学习力场优化、紧束缚计算、衍射分析和逆向设计的多步骤工作流程。我们通过端到端工作流程展示了AGAPI的能力，包括异质结构构建、粉末X射线衍射分析和需要多达十个连续操作的半导体缺陷工程。此外，我们使用三十多个示例提示作为测试案例评估了AGAPI，并将智能代理预测与实验数据进行了比较。拥有1,000多名活跃用户的AGAPI，为可复现、AI加速的材料发现提供了可扩展和透明的基础。AGAPI-Agents代码库可在https://github.com/atomgptlab/agapi获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.</description>
      <author>example@mail.com (Jaehyung Lee, Justin Ely, Kent Zhang, Akshaya Ajith, Charles Rhys Campbell, Kamal Choudhary)</author>
      <guid isPermaLink="false">2512.11935v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention</title>
      <link>http://arxiv.org/abs/2512.11811v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VPR-AttLLM是一个模型无关的框架，整合大型语言模型的语义推理和地理空间知识到视觉位置识别流程中，通过注意力引导的描述符增强提高检索性能，无需重新训练模型或额外数据。&lt;h4&gt;背景&lt;/h4&gt;社交媒体众包的街景图像为城市洪水等危机事件提供实时视觉证据，但缺乏可靠地理元数据；现有VPR模型在跨源场景中因视觉失真和域偏移导致性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;提出VPR-AttLLM框架，利用LLMs识别位置信息区域并抑制视觉噪声，提高VPR检索性能。&lt;h4&gt;方法&lt;/h4&gt;利用LLMs识别城市背景下具有位置信息的区域并抑制瞬时视觉噪声；在SF-XL、合成洪水场景、Mapillary照片和HK-URBAN数据集上评估；将VPR-AttLLM与CosPlace、EigenPlaces和SALAD三种VPR模型集成。&lt;h4&gt;主要发现&lt;/h4&gt;VPR-AttLL一致提高召回性能，相对增益通常为1-3%，在最具挑战性的真实洪水图像上达到8%；建立了LLM引导的多模态融合范式；桥接了类人空间推理与现代VPR架构。&lt;h4&gt;结论&lt;/h4&gt;VPR-AttLLM的即插即用设计、跨源鲁棒性和可解释性使其在可扩展城市监测和众包危机图像快速地理定位方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体众包的街景图像为城市洪水和其他危机事件提供了宝贵的实时视觉证据，但它通常缺乏可靠的地理元数据用于应急响应。现有的视觉位置识别模型在应用于此类图像时表现出显著的性能下降，这是由于跨源场景中固有的视觉失真和域偏移。本文提出了VPR-AttLLM，一个模型无关的框架，通过注意力引导的描述符增强，将大型语言模型的语义推理和地理空间知识整合到现有的VPR流程中。通过利用LLMs识别城市背景中具有位置信息的区域并抑制瞬时视觉噪声，VPR-AttLLM在不要求模型重新训练或额外数据的情况下提高了检索性能。在包含真实社交媒体洪水图像的SF-XL、在已建立的查询集和Mapillary照片上的合成洪水场景以及捕捉形态各异城市景观的新HK-URBAN数据集上进行了全面评估。将VPR-AttLLM与三种最先进的VPR模型-CosPlace、EigenPlaces和SALAD-集成，一致提高了召回性能，相对增益通常在1-3%之间，在最具挑战性的真实洪水图像上达到8%。除了在检索准确性方面可衡量的提升外，这项研究为视觉检索系统中LLM引导的多模态融合建立了可推广的范式。通过将城市感知理论的原理嵌入到注意力机制中，VPR-AttLLM桥接了类人的空间推理与现代VPR架构。它的即插即用设计、强大的跨源鲁棒性和可解释性突显了其在可扩展城市监测和众包危机图像快速地理定位方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.</description>
      <author>example@mail.com (Fengyi Xu, Jun Ma, Waishan Qiu, Cui Guo)</author>
      <guid isPermaLink="false">2512.11811v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Recurrent Video Masked Autoencoders</title>
      <link>http://arxiv.org/abs/2512.13684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出循环视频掩码自编码器（RVM），一种新型视频表示学习方法&lt;h4&gt;背景&lt;/h4&gt;现有视频表示学习方法在效率和时空结构捕捉方面存在局限性&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉视频时空结构的高效视频表示学习方法&lt;h4&gt;方法&lt;/h4&gt;使用基于Transformer的循环神经网络聚合密集图像特征，通过非对称掩码预测任务和标准像素重建目标进行学习&lt;h4&gt;主要发现&lt;/h4&gt;RVM在视频级任务上与最先进视频模型性能相当，在几何和空间理解任务上优于图像模型，小模型情况下表现出色且无需知识蒸馏，参数效率提高30倍，循环特性允许长时间稳定特征传播&lt;h4&gt;结论&lt;/h4&gt;RVM能够学习场景语义、结构和运动的丰富表示&lt;h4&gt;翻译&lt;/h4&gt;我们提出循环视频掩码自编码器（RVM）：一种新型视频表示学习方法，使用基于Transformer的循环神经网络聚合密集图像特征，有效捕捉自然视频数据的时空结构。RVM通过非对称掩码预测任务学习，仅需标准像素重建目标。这种设计产生了一个高效的通用编码器：RVM在视频级任务（如动作识别和点/对象跟踪）上实现了与最先进视频模型（如VideoMAE、V-JEPA）相当的性能，同时在测试几何和密集空间理解的任务上也优于图像模型（如DINOv2）。值得注意的是，RVM在小模型情况下表现出色，无需知识蒸馏，参数效率比竞争的视频掩码自编码器高30倍。此外，我们证明RVM的循环特性允许在长时间范围内稳定传播特征，计算成本为线性，克服了标准时空注意力架构的一些局限性。最后，我们使用定性可视化强调RVM学习了场景语义、结构和运动的丰富表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.</description>
      <author>example@mail.com (Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira, Andrew Zisserman)</author>
      <guid isPermaLink="false">2512.13684v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</title>
      <link>http://arxiv.org/abs/2512.13683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种重新编程预训练3D实例生成器的方法，使其作为场景级别学习者，解决了3D场景生成中的泛化挑战。通过用模型为中心的空间监督取代数据集边界监督，该方法使生成器能够处理未见过的布局和新的物体组合。&lt;h4&gt;背景&lt;/h4&gt;现有的基于学习的3D场景生成方法在空间理解方面依赖于有限的场景数据集，限制了模型在新布局上的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决3D场景生成中的泛化问题，使模型能够处理未见过的布局和新的物体组合。&lt;h4&gt;方法&lt;/h4&gt;重新编程预训练的3D实例生成器，使其作为场景级别学习者，用模型为中心的空间监督取代数据集边界监督。用一种以视图为中心的场景空间公式取代广泛使用的规范空间，实现完全前馈、可泛化的场景生成器。&lt;h4&gt;主要发现&lt;/h4&gt;即使训练场景是随机组合的物体，空间推理仍然会出现。生成器的可转移场景先验为从纯几何线索推断接近性、支撑性和对称性提供了丰富的学习信号。&lt;h4&gt;结论&lt;/h4&gt;3D实例生成器是一个隐含的空间学习者和推理者，这为交互式3D场景理解和生成的基础模型指明了方向。&lt;h4&gt;翻译&lt;/h4&gt;通用性仍然是交互式3D场景生成的核心挑战。现有的基于学习的方法将空间理解建立在有限的场景数据集上，限制了在新布局上的泛化能力。我们重新编程了一个预训练的3D实例生成器，使其作为场景级别的学习者，用模型为中心的空间监督取代了数据集边界监督。这种重新编程解锁了生成器的可转移空间知识，使其能够泛化到未见过的布局和新的物体组合。值得注意的是，即使训练场景是随机组合的物体，空间推理仍然会出现。这表明生成器的可转移场景先验为从纯几何线索推断接近性、支撑性和对称性提供了丰富的学习信号。我们用一种以视图为中心的场景空间公式取代了广泛使用的规范空间，产生了一个完全前馈、可泛化的场景生成器，直接从实例模型中学习空间关系。定量和定性结果表明，3D实例生成器是一个隐含的空间学习者和推理者，指向了交互式3D场景理解和生成的基础模型。项目页面：https://luling06.github.io/I-Scene-project/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决交互式3D场景生成中的泛化问题。现有方法受限于有限的场景数据集，无法生成多样化的布局，这对于虚拟内容创建、模拟和具身AI等应用至关重要，因为这些应用需要生成可编辑、有感知能力且空间连贯的物体排列。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到预训练的3D实例模型隐式编码了可转移的空间知识（如深度、遮挡、尺度等），因此重新编程这个先验作为场景级别的空间学习器。他们借鉴了现有的3D生成技术如TRELLIS骨干网络，但创新性地用视图中心场景空间取代规范物体空间，并引入场景上下文注意力机制，使实例生成能够参考全局场景布局。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将预训练的3D实例模型重新编程为空间学习器，利用视图中心场景空间保留几何关系，并从非语义合成场景中学习空间知识。实现流程包括：1) 双分支架构（空间指导分支和实例生成分支）；2) 场景上下文注意力机制；3) 视图中心场景空间表示；4) 在随机组合的非语义场景上训练；5) 使用条件修正流方法作为训练目标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 以模型为中心的监督，重新编程实例模型作为空间学习器；2) 视图中心场景空间，保留几何关系线索；3) 数据独立的布局学习，从非语义场景学习空间知识；4) 强大的泛化能力，能处理未见过的布局。相比之前工作，I-Scene不受限于特定数据集的偏见，能生成更复杂的空间关系（如小物体在大物体上），且保持实例级别的可编辑性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; I-Scene通过重新编程预训练的3D实例模型并利用视图中心场景空间和非语义合成数据，实现了在未见过的布局上生成连贯3D场景的强大泛化能力，同时保持实例级别的可编辑性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/</description>
      <author>example@mail.com (Lu Ling, Yunhao Ge, Yichen Sheng, Aniket Bera)</author>
      <guid isPermaLink="false">2512.13683v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</title>
      <link>http://arxiv.org/abs/2512.13660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://zhoues.github.io/RoboTracer&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了RoboTracer，一种创新的3D感知视觉语言模型，用于解决机器人空间追踪这一复杂任务。通过结合监督微调和强化微调方法，以及大规模数据集的支持，RoboTracer在空间理解、测量和指代方面表现出色，具有广泛的适用性，可在不同类型的机器人上执行复杂任务。&lt;h4&gt;背景&lt;/h4&gt;空间追踪作为机器人的基本具身交互能力，本质上具有挑战性，因为它需要多步基于度量的推理，结合复杂的空间指代和真实世界度量测量。然而，现有方法难以处理这种组合任务。&lt;h4&gt;目的&lt;/h4&gt;提出RoboTracer，一种3D感知的视觉语言模型，实现空间指代和测量功能；增强监督微调过程中的尺度感知能力；通过强化微调推进多步基于度量的推理；创建支持训练的大规模数据集；提出具有挑战性的基准测试来评估空间追踪能力。&lt;h4&gt;方法&lt;/h4&gt;提出RoboTracer，使用通用空间编码器和回归监督解码器实现3D空间指代和测量；通过度量敏感的过程奖励进行强化微调，推进多步基于度量的推理；引入TraceSpatial，包含3000万个问答对的大规模数据集，涵盖室外/室内/桌面场景；提出TraceSpatial-Bench基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;RoboTracer在空间理解、测量和指代方面优于基线方法，平均成功率为79.1%；在TraceSpatial-Bench上取得最先进性能，比Gemini-2.5-Pro高出36%的准确率；可与各种控制策略集成，在混乱的真实世界场景中执行长期、动态任务；可在不同机器人(UR5、G1人形机器人)上使用。&lt;h4&gt;结论&lt;/h4&gt;RoboTracer有效解决了机器人空间追踪的挑战；通过结合监督微调和强化微调，实现了高级空间推理能力；提出的数据集和基准测试为领域发展提供了支持。&lt;h4&gt;翻译&lt;/h4&gt;空间追踪作为机器人的基本具身交互能力，本质上具有挑战性，因为它需要多步基于度量的推理，结合复杂的空间指代和真实世界度量测量。然而，现有方法难以处理这种组合任务。为此，我们提出了RoboTracer，一种3D感知的视觉语言模型，首次通过通用空间编码器和回归监督解码器实现3D空间指代和测量，在监督微调过程中增强尺度感知能力。此外，RoboTracer通过具有度量敏感过程奖励的强化微调，推进多步基于度量的推理，监督关键中间感知线索以准确生成空间轨迹。为支持SFT和RFT训练，我们引入了TraceSpatial，一个包含3000万个问答对的大规模数据集，涵盖室外/室内/桌面场景，并支持复杂的推理过程(最多9步)。我们还提出了TraceSpatial-Bench，一个具有挑战性的基准测试，填补了评估空间追踪能力的空白。实验结果表明，RoboTracer在空间理解、测量和指代方面优于基线方法，平均成功率为79.1%，并且在TraceSpatial-Bench上以较大优势取得了最先进的性能，准确率比Gemini-2.5-Pro高出36%。值得注意的是，RoboTracer可以与各种控制策略集成，在混乱的真实世界场景中执行长期、动态任务，适用于不同类型的机器人(UR5、G1人形机器人)。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人空间追踪问题，即让机器人能够根据空间约束指令（如'从左到右浇花，喷壶在每个花上方1-5厘米处'）生成3D位置序列。这个问题很重要，因为空间追踪是机器人具身交互的基础能力，现有方法难以处理需要多步骤推理和真实世界度量的复杂任务，限制了机器人在复杂环境中的自主性和适应性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将空间追踪任务分解为3D空间指代和3D空间测量两个关键组件，借鉴了现有视觉-语言模型(VLMs)的基本架构，但针对3D空间理解和度量感知进行了改进。他们参考了强化微调(RFT)方法如GRPO，利用现有3D几何模型作为先验知识，并借鉴了2D视觉轨迹生成方法但扩展到3D空间。设计上采用了通用空间编码器和比例解码器，结合监督微调(SFT)和强化微调(RFT)两阶段训练策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个3D感知的VLM，通过通用空间编码器灵活整合几何信息，使用比例解码器增强真实世界尺度感知，并采用两阶段训练方法。整体流程：1)输入处理(接受RGB图像和任务指令，可选几何配置)；2)编码阶段(视觉编码器处理图像，空间编码器处理几何信息，比例解码器预测度量比例)；3)推理阶段(LLM处理指令和编码信息，生成多步骤推理和空间轨迹)；4)训练阶段(SFT学习基础能力，RFT使用度量敏感奖励改进推理)；5)输出3D空间轨迹和可选中间推理步骤。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)3D感知VLM架构，包含通用空间编码器和比例解码器；2)两阶段训练方法(SFT+RFT)；3)大规模TraceSpatial数据集(4.5M样本，30M问答对)；4)新的TraceSpatial-Bench评估基准；5)实际应用验证。相比之前工作，RoboTracer从2D扩展到3D，增强了组合推理能力，具有真正的度量感知，能处理任意几何输入，且数据规模和质量远超现有工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RoboTracer通过结合3D感知的视觉-语言模型架构、两阶段训练方法和大规模数据集，首次实现了机器人空间追踪任务中的多步骤、度量引导推理，显著提升了机器人在复杂3D环境中执行空间约束指令的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.</description>
      <author>example@mail.com (Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang)</author>
      <guid isPermaLink="false">2512.13660v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass</title>
      <link>http://arxiv.org/abs/2512.13122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DePT3R是一个新颖的框架，能够在单一前向传递中同时执行密集点跟踪和动态场景的3D重建，无需已知相机姿态，显著提高了适应性和效率。&lt;h4&gt;背景&lt;/h4&gt;当前动态场景密集3D点跟踪方法通常依赖于成对处理、已知相机姿态或假设输入帧有时间顺序，限制了其灵活性和适用性。最近进展已实现从大规模无姿态图像集合中进行高效3D重建，为动态场景理解的统一方法提供了机会。&lt;h4&gt;目的&lt;/h4&gt;提出DePT3R框架，同时执行密集点跟踪和动态场景的3D重建，从多张图像在单一前向传递中完成，提高方法的适应性和效率。&lt;h4&gt;方法&lt;/h4&gt;通过多任务学习实现，使用强大的骨干网络提取深时空特征，并用密集预测头回归像素级映射。DePT3R操作不需要相机姿态，特别适合快速变化的动态环境。&lt;h4&gt;主要发现&lt;/h4&gt;在多个具有挑战性的动态场景基准测试中验证了DePT3R，展示了强大的性能，与现有最先进方法相比在内存效率方面有显著改进。数据和代码可通过开放存储库获取。&lt;h4&gt;结论&lt;/h4&gt;DePT3R是一个有效的框架，能够在不依赖相机姿态的情况下同时进行密集点跟踪和3D重建，在动态场景中表现出色，特别是在内存效率方面有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;当前动态场景中密集3D点跟踪方法通常依赖于成对处理，需要已知的相机姿态，或假设输入帧有时间顺序，限制了它们的灵活性和适用性。此外，最近的进展已成功实现从大规模、无姿态图像集合中进行高效3D重建，凸显了动态场景理解统一方法的机会。受此启发，我们提出了DePT3R，一个新颖的框架，能够在单一前向传递中同时从多张图像执行密集点跟踪和动态场景的3D重建。这种多任务学习是通过使用强大的骨干网络提取深时空特征，并用密集预测头回归像素级映射来实现的。关键的是，DePT3R操作不需要相机姿态，显著提高了其适应性和效率——这对快速变化的动态环境尤其重要。我们在几个涉及动态场景的具有挑战性的基准测试中验证了DePT3R，展示了其强大的性能和与现有最先进方法相比在内存效率方面的显著改进。数据和代码可通过开放存储库获取：https://github.com/StructuresComp/DePT3R&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决动态场景中密集3D点跟踪和3D重建的联合处理问题，特别是消除对已知相机姿态、成对处理或时间顺序假设的依赖。这个问题在自主导航、增强现实和机器人等领域至关重要，因为准确跟踪动态环境中的点和重建3D结构是智能系统有效感知和与现实世界交互的基础，而传统方法在处理复杂动态场景时往往表现不佳且效率低下。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到近期DUSt3R等工作的启发，注意到这些方法虽为静态场景设计但也在动态内容上表现良好。作者观察到现有方法如VGGT在动态场景处理上存在局限性：大幅非刚性变形下表现不佳，且内存占用限制了密集跟踪效率。作者基于VGGT架构进行扩展，借鉴了St4RTrack的时间相关点图表示和DINOv2图像标记化技术，同时添加了专用的运动预测头和查询机制，以实现更高效的全局时间推理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用'帧到查询'的公式，通过全局图像聚合在单次前向传递中同时实现点跟踪和3D重建，无需帧到帧的链接过程。整体流程为：1) 输入多帧图像和查询时间；2) 为每帧预测点位置和运动场；3) 使用扩展的VGGT骨干网络处理图像特征；4) 通过专用运动头预测点运动；5) 添加相机内参嵌入提高准确性；6) 使用多任务损失函数联合优化所有预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 帧到查询的运动场预测公式，实现长程变形推理；2) 全局时间注意力机制，提高计算效率和准确性；3) 相机内参嵌入，解决尺度模糊问题；4) 专用运动预测头，与重建任务共享特征；5) 两阶段训练策略，提高模型稳定性。相比之前工作，DePT3R不需要成对处理、已知相机姿态或时间顺序假设，内存效率更高，在处理大幅非刚性变形时表现更好，且能在单次前向传递中处理大量帧。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DePT3R提出了一种创新的前馈框架，通过全局图像聚合和帧到查询的公式，在单次前向传递中实现了动态场景的密集点跟踪和3D重建，无需相机姿态信息，同时显著提高了内存效率和跟踪精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R</description>
      <author>example@mail.com (Vivek Alumootil, Tuan-Anh Vu, M. Khalid Jawed)</author>
      <guid isPermaLink="false">2512.13122v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos</title>
      <link>http://arxiv.org/abs/2512.13080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种空间感知的视觉-语言-动作预训练范式，通过在预训练过程中对齐视觉空间和物理空间，使模型在机器人策略学习前获得三维空间理解能力。研究者实现了VIPA-VLA模型，一种双编码器架构，通过三维视觉编码器增强语义视觉表示。当应用于下游机器人任务时，该模型显著改善了二维视觉与三维动作之间的对齐，产生更鲁棒和可泛化的机器人策略。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言-动作模型为机器人学习提供了一种有前景的范式，但大多数现有方法依赖二维视觉输入在三维物理环境中执行动作，导致感知与动作基础之间存在显著差距。&lt;h4&gt;目的&lt;/h4&gt;为了弥合二维视觉感知与三维物理动作之间的差距，研究者提出了一种空间感知的VLA预训练范式，使模型在机器人策略学习前获得三维空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;研究者从预训练的视觉语言模型开始，利用大规模人类演示视频提取三维视觉和三维动作注释，形成新的监督源，将二维视觉观察与三维空间推理对齐。实现了VIPA-VLA双编码器架构，集成了三维视觉编码器来增强语义视觉表示。&lt;h4&gt;主要发现&lt;/h4&gt;当适应下游机器人任务时，VIPA-VLA实现了二维视觉和三维动作之间显著改善的对齐，产生了更鲁棒和可泛化的机器人策略。&lt;h4&gt;结论&lt;/h4&gt;通过空间感知的预训练范式和VIPA-VLA模型，成功弥合了二维视觉感知与三维物理动作之间的差距，为机器人学习提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作模型通过整合视觉感知与语言引导的策略学习，为机器人学习提供了一种有前景的范式。然而，大多数现有方法依赖于二维视觉输入在三维物理环境中执行动作，这导致了感知与动作基础之间的重要差距。为了弥合这一差距，我们提出了一种空间感知的VLA预训练范式，在预训练过程中明确对齐视觉空间和物理空间，使模型在机器人策略学习前获得三维空间理解能力。从预训练的视觉语言模型开始，我们利用大规模人类演示视频提取三维视觉和三维动作注释，形成一种新的监督源，用于将二维视觉观察与三维空间推理对齐。我们通过VIPA-VLA实现了这一范式，它是一种双编码器架构，集成了三维视觉编码器，用于通过三维感知特征增强语义视觉表示。当适应下游机器人任务时，VIPA-VLA实现了二维视觉和三维动作之间显著改善的对齐，产生了更鲁棒和可泛化的机器人策略。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉-语言-行动（VLA）模型中2D视觉感知与3D物理行动之间的显著差距问题。这个问题很重要，因为它限制了机器人对物理世界的有效理解和交互，导致模型难以将2D视觉信息准确映射到3D空间中的行动，从而影响机器人在复杂环境中的表现和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有VLA模型依赖2D视觉输入进行3D物理环境行动的局限性，然后提出利用大规模人类演示视频作为监督来源，因为人类视频中自然包含了2D视觉观察与3D物理行动之间的对应关系。他们借鉴了现有的视觉-语言模型作为基础，利用3D视觉技术进行点云估计，采用人类动作捕捉和手部姿态标注技术，并参考了现有VLA模型架构，但增加了3D空间感知能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'视觉-物理对齐'建立2D视觉观察与3D物理空间之间的明确对应关系。整体流程包括：1)构建Hand3D数据集，从人类视频中提取3D视觉和动作标注；2)设计VIPA-VLA双编码器架构，结合语义视觉和3D空间特征；3)进行两阶段预训练，第一阶段对齐2D视觉与3D空间，第二阶段学习物理基础的动作先验；4)在机器人数据上进行微调，添加动作头生成可执行行动。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)空间感知VLA预训练范式，在预训练阶段显式对齐视觉与物理空间；2)Hand3D数据集，提供3D视觉和动作标注的监督信号；3)VIPA-VLA双编码器架构，有效整合语义和空间特征；4)两阶段预训练策略。相比之前工作，该方法明确建模2D视觉与3D物理空间的对齐关系，专注于建立3D感知与物理行动的对应，利用人类视频而非昂贵的机器人数据进行预训练，避免了embodiment mismatch问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文通过从人类视频中提取3D视觉和动作标注，提出了一种空间感知的视觉-语言-行动预训练范式，有效弥合了2D视觉感知与3D物理行动之间的差距，显著提升了机器人在各种任务中的空间定位能力和泛化性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.</description>
      <author>example@mail.com (Yicheng Feng, Wanpeng Zhang, Ye Wang, Hao Luo, Haoqi Yuan, Sipeng Zheng, Zongqing Lu)</author>
      <guid isPermaLink="false">2512.13080v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework</title>
      <link>http://arxiv.org/abs/2512.12945v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted into R-AL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了SLIM-VDB，一种新的轻型语义映射系统，具有概率语义融合功能，支持封闭集或开放集字典。&lt;h4&gt;背景&lt;/h4&gt;计算机图形学社区的数据结构进步（如OpenVDB）在体积场景表示中显示出计算和内存效率的显著提高。虽然OpenVDB已被用于机器人应用中的几何映射，但使用OpenVDB进行场景理解的语义映射仍未被探索。此外，现有的语义映射系统缺乏在单一框架内集成固定类别和开放语言标签预测的支持。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的3D语义映射系统，利用OpenVDB数据结构，并集成统一的贝叶斯更新框架，用于封闭集和开放集语义融合。&lt;h4&gt;方法&lt;/h4&gt;利用OpenVDB数据结构，并集成统一的贝叶斯更新框架进行语义融合。&lt;h4&gt;主要发现&lt;/h4&gt;与当前最先进的语义映射方法相比，SLIM-VDB在内存和集成时间方面实现了显著减少，同时保持了相当的映射精度。&lt;h4&gt;结论&lt;/h4&gt;SLIM-VDB是一种高效的语义映射系统，在保持精度的同时显著提高了效率。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了SLIM-VDB，一种新的轻型语义映射系统，具有用于封闭集或开放集字典的概率语义融合功能。计算机图形学社区的数据结构进步，如OpenVDB，在体积场景表示中显示出计算和内存效率的显著提高。虽然OpenVDB已被用于机器人应用中的几何映射，但使用OpenVDB进行场景理解的语义映射仍未被探索。此外，现有的语义映射系统缺乏在单一框架内集成固定类别和开放语言标签预测的支持。在本文中，我们提出了一种新颖的3D语义映射系统，利用OpenVDB数据结构，并集成了统一的贝叶斯更新框架，用于封闭集和开放集语义融合。与当前最先进的语义映射方法相比，我们提出的SLIM-VDB框架在内存和集成时间方面实现了显著减少，同时保持了相当的映射精度。带有Python接口的开源C++代码库可在https://github.com/umfieldrobotics/slim-vdb获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有语义地图系统内存占用大、难以实时运行的问题，以及缺乏对固定类别(封闭集)和开放语言标签(开放集)预测的统一支持。这个问题很重要，因为机器人需要准确的世界地图和场景理解来指导行动，而实时语义地图对移动机器人至关重要，同时能够处理两种语义类型的框架会更加灵活和强大，适应复杂环境需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有语义地图系统的局限性，然后借鉴计算机图形学领域的OpenVDB数据结构，该结构在体积场景表示方面表现出色。在语义融合方面，作者借鉴了贝叶斯推理方法，如ConvBKI和LatentBKI，分别用于封闭集和开放集语义融合。在几何映射方面，借鉴了VDBFusion的工作。作者将这些现有技术与创新的统一贝叶斯更新框架相结合，创造出SLIM-VDB系统，实现了高效、实时的语义地图构建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用OpenVDB数据结构的高效体积表示能力，结合统一的贝叶斯概率框架，实现实时、内存高效的语义地图构建，同时支持封闭集和开放集语义。整体流程包括：1)感知阶段：输入RGB图像、深度图像和机器人位姿，生成语义预测；2)传感器处理：将语义分割结果投影到3D空间形成语义点云；3)主要集成循环：进行光线投射和TSDF更新进行几何更新，使用贝叶斯推理进行语义更新；4)可视化：使用NanoVDB进行GPU加速的地图渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将OpenVDB数据结构应用于语义地图构建，提供O(1)的查找、插入和删除时间；2)统一的贝叶斯语义融合框架，同时支持封闭集和开放集语义；3)高效的内存和计算效率，显著减少内存占用和集成时间；4)开源实现，提供C++代码库和Python接口。相比之前工作，SLIM-VDB不同于VDBFusion(只支持几何映射)、ConvBKI和SEE-CSOM(只支持封闭集语义)、LatentBKI(无法实时运行且内存占用大)以及SNI-SLAM(需要专业级GPU)，实现了更高效、更灵活的语义地图构建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SLIM-VDB首次将OpenVDB数据结构与统一的贝叶斯概率框架相结合，实现了实时、内存高效且同时支持封闭集和开放集语义的3D语义地图构建系统。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.</description>
      <author>example@mail.com (Anja Sheppard, Parker Ewen, Joey Wilson, Advaith V. Sethuraman, Benard Adewole, Anran Li, Yuzhen Chen, Ram Vasudevan, Katherine A. Skinner)</author>
      <guid isPermaLink="false">2512.12945v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2512.12307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures. Supplementary material and code will be provided at the end of January&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MRD（metamers rendered differentiably）方法，这是一种基于物理的可微分渲染技术，用于探测视觉模型对3D场景属性的隐式理解。&lt;h4&gt;背景&lt;/h4&gt;深度学习在视觉任务中取得成功，但其模型表示和决策难以解释。视觉模型虽在2D数据上训练，但被认为能形成对底层3D场景的隐式理解。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来探测视觉模型如何理解和表示3D场景的物理属性，以增进对模型决策过程的理解。&lt;h4&gt;方法&lt;/h4&gt;MRD方法通过寻找物理不同但产生相同模型激活（模型元形）的3D场景参数来探测模型。与基于像素的方法不同，这些重建结果基于物理场景描述，可独立控制形状、材质和光照等参数。&lt;h4&gt;主要发现&lt;/h4&gt;评估显示模型在目标场景和优化场景间激活高度相似，但视觉效果不同。这些重建能揭示模型对哪些物理属性敏感或不变。&lt;h4&gt;结论&lt;/h4&gt;MRD方法通过分析物理参数如何驱动模型响应变化，有助于增进对计算机视觉和人类视觉的理解。&lt;h4&gt;翻译&lt;/h4&gt;虽然深度学习方法在许多视觉基准测试中取得了令人印象深刻的成功，但理解和解释这些模型的表示和决策仍然很困难。虽然视觉模型通常在2D输入上训练，但人们通常假设它们能够对底层3D场景形成隐式表示（例如，对部分遮挡表现出容忍度，或能够推理相对深度）。在这里，我们介绍了MRD（metamers rendered differentiably），一种基于物理的可微分渲染方法，通过寻找物理不同但产生相同模型激活（即模型元形）的3D场景参数，来探测视觉模型对生成性3D场景属性的隐式理解。与以往评估模型表示的基于像素的方法不同，这些重建结果总是基于物理场景描述。这意味着，例如，我们可以在保持材质和光照不变的情况下探测模型对物体形状的敏感性。作为原理证明，我们评估了多个模型恢复几何形状（形状）和双向反射分布函数（材质）场景参数的能力。结果显示目标场景和优化场景之间的模型激活具有高度相似性，但视觉效果有所不同。从质量上讲，这些重建有助于研究模型对哪些物理场景属性敏感或不变。MRD通过使分析物理参数如何驱动模型响应的变化，有望促进我们对计算机视觉和人类视觉的理解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的问题是：如何理解和解释深度视觉模型的内部表示和决策过程，特别是这些模型如何理解3D场景。这个问题很重要，因为随着深度学习在计算机视觉领域的广泛应用，理解这些模型如何'思考'变得至关重要，有助于提高模型的透明度和可信度，揭示模型可能存在的偏见或局限性，并推动计算机视觉和人类视觉的共同研究。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到深度学习模型虽然只在2D图像上训练，但似乎能够理解3D场景结构。他们想找到一种方法来探测模型对3D场景的隐式理解，而不仅仅是表面的2D特征。作者借鉴了基于物理的渲染和可微分渲染技术，以及人类视觉研究中使用的metamer概念（产生相同感知但物理上不同的刺激），将这些技术结合起来创建了MRD方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于物理的可微分渲染技术来找到与目标场景在视觉模型中产生相同激活但物理上不同的3D场景参数。整体流程是：从已知参数的场景开始，渲染一组真实图像；初始化一个具有不同参数的新场景；在采样相机位置渲染图像并计算与真实图像的损失；计算梯度并更新目标场景参数；重复优化过程直到找到产生相同模型激活的物理不同场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将基于物理的可微分渲染与metamer概念结合；能够将模型激活与物理环境属性直接联系起来；允许分离物理原因，专门探测模型对特定场景属性的敏感性；提供在物理单位中解释视觉模型表示的方法。相比之前工作，MRD不同于传统的像素级方法，能够探测3D场景属性理解；不同于仅关注2D特征的解释方法；不同于基于神经网络的逆渲染方法，基于物理光传输；不同于简单的特征可视化，能找到物理上不同但在模型中等效的场景表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了MRD方法，通过基于物理的可微分渲染技术来探测视觉模型对3D场景的隐式理解，建立了模型激活与物理场景属性之间的桥梁，为理解和解释深度视觉模型提供了新视角。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.</description>
      <author>example@mail.com (Benjamin Beilharz, Thomas S. A. Wallis)</author>
      <guid isPermaLink="false">2512.12307v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection</title>
      <link>http://arxiv.org/abs/2512.12205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures. Submitted to Data in Brief (Elsevier)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个大规模、长期的城市路灯视觉数据集，由英国布里斯托尔部署的22个固定角度摄像头从2021年到2025年拍摄。数据集包含超过526,000张图像，每小时收集一次，涵盖不同光照、天气和季节条件，并附带丰富的元数据。研究还提供了基于卷积变分自编码器的自监督框架和两种漂移指标，为评估长期视觉系统稳定性提供了真实基准。&lt;h4&gt;背景&lt;/h4&gt;随着智能城市部署的发展，视觉漂移、异常检测和MLOps策略的研究需要大规模、长期的真实世界数据支持，而现有数据集可能不足以满足这些研究需求。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模、长期的城市路灯视觉数据集，用于详细研究智能城市部署中的视觉漂移、异常检测和MLOps策略，同时提供自监督框架和漂移指标以促进二次分析。&lt;h4&gt;方法&lt;/h4&gt;部署22个固定角度摄像头在英国布里斯托尔从2021年到2025年收集数据；每小时收集图像记录不同条件；为每张图像提供元数据；基于卷积变分自编码器构建自监督框架；为每个摄像头节点和日/夜图像集分别训练模型；定义相对质心漂移和相对重建误差两种漂移指标。&lt;h4&gt;主要发现&lt;/h4&gt;创建了包含超过526,000张图像的大规模数据集，覆盖多种环境条件；提供的自监督框架能有效处理长期视觉数据；定义的漂移指标能量化模型性能随时间的变化。&lt;h4&gt;结论&lt;/h4&gt;该数据集为评估长期模型稳定性、漂移感知学习和可部署视觉系统提供了真实的、细粒度基准，数据集公开发布支持可重复性和多种下游应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个大规模、长期的城市路灯视觉数据集，由英国布里斯托尔部署的22个固定角度摄像头从2021年到2025年拍摄。该数据集包含超过526,000张图像，每小时收集一次，涵盖不同的光照、天气和季节条件。每张图像都附带丰富的元数据，包括时间戳、GPS坐标和设备标识符。这一独特的真实世界数据集 enables详细研究智能城市部署中的视觉漂移、异常检测和MLOps策略。为了促进二次分析，我们还提供了一个基于卷积变分自编码器(CNN-VAEs)的自监督框架。模型为每个摄像头节点和日/夜图像集分别训练。我们定义了两种每样本漂移指标：相对质心漂移，捕获与基线季度的潜在空间偏差；和相对重建误差，测量归一化图像域退化。该数据集为评估长期模型稳定性、漂移感知学习和可部署视觉系统提供了真实的、细粒度基准。图像和结构化元数据以JPEG和CSV格式公开发布，支持可重复性和下游应用，如路灯监测、天气推断和城市场景理解。该数据集可在https://doi.org/10.5281/zenodo.17781192和https://doi.org/10.5281/zenodo.17859120找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.</description>
      <author>example@mail.com (Peizheng Li, Ioannis Mavromatis, Ajith Sahadevan, Tim Farnham, Adnan Aijaz, Aftab Khan)</author>
      <guid isPermaLink="false">2512.12205v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video</title>
      <link>http://arxiv.org/abs/2512.12165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了利用被动场景声音作为视觉信息的补充，用于相对相机姿态估计，提出了一种简单但有效的视听框架，在视觉条件恶化的情况下表现出色。&lt;h4&gt;背景&lt;/h4&gt;理解相机运动是具身感知和3D场景理解的基本问题。尽管视觉方法发展迅速，但在视觉条件恶化的情况下（如运动模糊或遮挡）往往表现不佳。&lt;h4&gt;目的&lt;/h4&gt;展示被动场景声音可以为野外视频中的相对相机姿态估计提供补充线索，特别是在视觉信息不可靠的情况下。&lt;h4&gt;方法&lt;/h4&gt;研究团队引入了一个简单但有效的视听框架，将到达方向(DOA)谱和双耳嵌入集成到最先进的纯视觉姿态估计模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大型数据集上的结果显示，相比强大的视觉基线模型，该方法取得了持续的性能提升，并且在视觉信息被损坏时表现出鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是首次成功利用音频进行真实世界视频中相对相机姿态估计的研究，确立了偶然的日常音频作为解决经典空间挑战的一个意外但有前景的信号。&lt;h4&gt;翻译&lt;/h4&gt;理解相机运动是具身感知和3D场景理解的基本问题。虽然视觉方法已经取得了快速发展，但它们在视觉条件恶化的情况下（如运动模糊或遮挡）往往表现不佳。在这项工作中，我们表明被动场景声音可以为野外视频中的相对相机姿态估计提供补充线索。我们介绍了一个简单但有效的视听框架，将到达方向(DOA)谱和双耳嵌入集成到最先进的纯视觉姿态估计模型中。我们在两个大型数据集上的结果显示，相比强大的视觉基线模型，该方法取得了持续的性能提升，并且在视觉信息被损坏时表现出鲁棒性。据我们所知，这代表了首次成功利用音频进行真实世界视频中相对相机姿态估计的工作，它确立了偶然的日常音频作为解决经典空间挑战的一个意外但有前景的信号。项目：http://vision.cs.utexas.edu/projects/av_camera_pose.&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是：在真实世界视频中进行相机姿态估计时，如何利用被动场景声音来辅助视觉信息，提高相机姿态估计的准确性，特别是在视觉信息受损的情况下（如运动模糊、光照不足、遮挡等）。这个问题在现实中很重要，因为许多实际应用（如增强现实、机器人导航、自动驾驶）都需要准确的相机姿态信息，而这些应用场景常常会遇到视觉条件差的情况。此外，这是首次成功利用音频进行真实世界视频中的相对相机姿态估计工作，填补了研究空白。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到在视觉条件差的场景（如黑暗拥挤的演唱会），尽管视觉信息严重退化，但周围的声音却能提供清晰的空间信息。他们意识到被动场景声音具有非侵入性、多样性和自然可用等优点。设计方法时，作者借鉴了现有的视觉相机姿态估计模型（Reloc3r），以及音频处理技术（如MUSIC++算法进行方向到达估计）和自监督学习方法（设计新颖视角声学合成NVAS任务）。作者设计了一个空间音频编码器，结合两种互补的音频特征，并将这些特征与视觉特征进行后期融合，增强现有的视觉模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用被动场景声音中的空间信息来补充视觉信息，特别是在视觉信息受损的情况下。通过结合两种互补的音频特征（显式的方向到达信息和隐式的双耳音频特征）提供丰富的空间线索。整体实现流程：1) 输入处理：视频帧通过Reloc3r提取视觉特征，音频片段通过STFT转换为频谱图；2) 空间音频编码：使用MUSIC++算法计算DoA频谱，训练NVAS模型学习双耳音频特征，然后将两种特征拼接；3) 音频-视觉融合：将空间音频嵌入与视觉特征后期融合；4) 姿态预测：融合后的特征输入到姿态预测头，估计相对相机姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 首次利用被动场景声音（非主动发出的声音）进行真实世界视频的相机姿态估计；2) 设计空间音频编码器，结合方向到达频谱和双耳音频嵌入两种互补特征；3) 使用多样化的真实世界视频数据训练模型，而非静态环境或模拟数据；4) 方法在视觉信息受损情况下仍保持良好性能，展示跨模态鲁棒性。相比之前工作的不同：之前工作主要使用主动声波定位或模拟音频，在静态环境中训练；本文使用自然发生的场景声音和真实世界数据，专注于相机姿态估计这一任务，并采用分析性与学习性特征相结合的方式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次证明了可以利用被动场景声音作为互补信号，通过结合方向到达信息和双耳音频特征，显著提升真实世界视频中的相机姿态估计准确性，特别是在视觉信息受损的情况下展现出更强的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.</description>
      <author>example@mail.com (Daniel Adebi, Sagnik Majumder, Kristen Grauman)</author>
      <guid isPermaLink="false">2512.12165v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics</title>
      <link>http://arxiv.org/abs/2512.11903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Aion框架，将时间流动态直接嵌入到分层3D场景图中，有效整合了时间维度，提高了自主导航在复杂动态环境中的规划和交互能力。&lt;h4&gt;背景&lt;/h4&gt;自主导航在动态环境中需要能够捕捉语义结构和时间演化的空间表示。现有的3D场景图(3DSGs)虽然提供分层多分辨率抽象，编码几何和语义信息，但动态扩展主要关注单个对象或智能体。而动态地图(MoDs)模拟典型运动模式和时序规律，但通常基于网格离散化，缺乏语义感知，且不易扩展到大型环境。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效整合时间维度的框架，将时间流动态直接嵌入到分层3D场景图中，提高自主导航在复杂动态环境中的能力。&lt;h4&gt;方法&lt;/h4&gt;Aion框架采用基于图的稀疏MoD表示来捕获任意时间间隔的运动流，并将其附加到场景图中的导航节点上，产生更可解释和可扩展的预测。&lt;h4&gt;主要发现&lt;/h4&gt;Aion框架产生了更可解释和可扩展的预测，提高了在复杂动态环境中的规划和交互能力。&lt;h4&gt;结论&lt;/h4&gt;Aion框架有效整合了时间和空间信息，通过将时间流动态嵌入到分层3D场景图中，解决了现有方法在语义感知和可扩展性方面的局限性，提高了自主导航在复杂动态环境中的性能。&lt;h4&gt;翻译&lt;/h4&gt;在动态环境中的自主导航需要捕捉语义结构和时间演化的空间表示。3D场景图(3DSGs)提供分层多分辨率抽象，编码几何和语义信息，但现有的动态扩展主要关注单个对象或智能体。同时，动态地图(MoDs)模拟典型运动模式和时序规律，但通常基于网格离散化，缺乏语义感知，且不易扩展到大型环境。本文介绍了Aion框架，它将时间流动态直接嵌入到分层3D场景图中，有效整合了时间维度。Aion采用基于图的稀疏MoD表示来捕获任意时间间隔的运动流，并将其附加到场景图中的导航节点上，产生更可解释和可扩展的预测，从而改善复杂动态环境中的规划和交互。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D场景图缺乏时间维度的问题，使其能够捕捉和预测动态环境中的运动模式。这个问题在自主导航中至关重要，因为机器人需要理解空间结构随时间的演变，才能安全高效地在人类环境中导航，预测人类移动并避免碰撞。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了3D场景图(3DSGs)的分层语义结构和动态地图(MoDs)的时间建模能力，思考如何将两者优势互补。他们借鉴了3DSGs的几何语义表示和MoDs的运动模式捕捉，同时扩展了FreMEn模型用于时间周期性预测，并参考了Hydra系统实现实时集成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将时间流动动力学直接整合到分层的3D场景图中，创建4D时空表示。实现流程包括：1)构建分层的3D场景图结构；2)为导航节点维护方向直方图进行时空建模；3)使用稀疏空间哈希实现可扩展的时间建模；4)通过全局时间模型架构预测运动趋势；5)采用时间所有权转移机制确保长期一致性；6)通过异步处理实现实时集成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于图的动态地图，在有意义的位置而非任意网格上进行时间建模；2)动态拓扑时间建模，通过位置不变索引保持一致性；3)3DSG集成和流动预测，为导航节点提供直接的时间信息。相比之前工作，Aion不仅捕捉个体对象运动，还关注集体活动模式，利用语义结构而非统一网格，并解决了动态图结构上的时间一致性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Aion通过将时间流动动力学整合到分层3D场景图中，创建了首个语义感知的4D时空表示，使机器人能够更有效地预测和导航复杂动态环境中的活动模式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.</description>
      <author>example@mail.com (Iacopo Catalano, Eduardo Montijano, Javier Civera, Julio A. Placed, Jorge Pena-Queralta)</author>
      <guid isPermaLink="false">2512.11903v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2512.12884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, accepted at IV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Transformer的端到端跨层融合方法，将智能传感器提供的对象列表信息与原始摄像头图像结合用于3D目标检测，显著提升了性能并展示了良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在汽车传感器融合系统中，智能传感器和V2X模块被广泛使用，但这些系统提供的数据通常是处理后的对象列表，而非传统传感器的原始数据。传统方法需要单独处理其他原始数据然后在对象级别进行融合。&lt;h4&gt;目的&lt;/h4&gt;提出一种端到端的跨层融合概念，将高度抽象的对象列表信息与原始摄像头图像结合用于3D目标检测，避免传统方法的复杂处理流程。&lt;h4&gt;方法&lt;/h4&gt;使用Transformer架构，将对象列表作为去噪查询输入，与可学习查询一起通过特征聚合过程传播；从对象列表的位置和尺寸先验中导出可变形高斯掩码并集成到Transformer解码器中；提出通过模拟状态噪声和假阳性假阴性从真实边界框生成伪对象列表的方法。&lt;h4&gt;主要发现&lt;/h4&gt;作为首个进行跨层融合的工作，在nuScenes数据集上相比基于视觉的基线方法显示出显著的性能提升；方法能够在不同噪声级别的模拟对象列表和真实检测器上展示其泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该跨层融合方法有效结合了抽象对象列表信息和原始图像数据，提高了3D目标检测的性能，为汽车传感器融合提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在汽车传感器融合系统中，智能传感器和车联网(V2X)模块被广泛使用。这些系统的传感器数据通常仅以处理后的对象列表形式提供，而非传统传感器的原始传感器数据。我们提出了一种基于Transformer的端到端跨层融合概念，将高度抽象的对象列表信息与原始摄像头图像结合用于3D目标检测，而不是像传统方法那样单独处理其他原始数据然后在对象级别进行融合。对象列表作为去噪查询输入到Transformer中，并与可学习查询一起通过后续特征聚合过程传播。此外，从对象列表的位置和尺寸维度先验中导出的可变形高斯掩码被显式集成到Transformer解码器中，这引导注意力集中在感兴趣的目标区域并加速模型训练收敛。进一步地，由于没有包含对象列表作为独立模态的公共数据集，我们提出了一种通过模拟状态噪声和假阳性假阴性从真实边界框生成伪对象列表的方法。作为首个进行跨层融合的工作，我们的方法在nuScenes数据集上相比基于视觉的基线方法显示出显著的性能提升，并在不同噪声级别的模拟对象列表和真实检测器上展示了其泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何在3D目标检测中融合不同抽象级别的传感器数据，特别是将高抽象级别的'对象列表'与低抽象级别的原始相机图像数据进行融合。这个问题在现实中很重要，因为现代自动驾驶系统中的智能传感器和V2X模块通常只提供处理后的对象列表而非原始数据，有效的跨级别融合可以提高3D目标检测的准确性和鲁棒性，增强自动驾驶系统在动态场景中的感知能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了不同级别的传感器融合方法的局限性，认识到在自动驾驶场景中往往只能获得高级别的对象列表。他们选择基于Transformer的方法，特别是借鉴了DETR的查询机制和PETRv2框架作为基础。具体设计中，作者采用了查询去噪(QDN)机制受DN-DETR启发，显式注意力引导借鉴了SMCA方法，并开发了伪对象列表生成(POLG)模块来处理数据缺失问题。这些现有技术被创新性地组合和改进，以实现跨级别融合这一新概念。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过端到端的Transformer框架，将高抽象级别的对象列表信息与低抽象级别的原始相机图像数据进行融合，利用对象列表作为'去噪查询'并配合可变形高斯掩码来引导注意力机制。整体流程包括：1)处理多视角相机图像和对象列表输入；2)提取图像特征和生成3D位置感知特征；3)将对象列表转换为去噪查询并与检测查询一起输入Transformer；4)利用对象列表信息创建高斯掩码引导交叉注意力；5)通过检测头生成最终的目标边界框；6)使用伪对象列表进行训练和评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出并实现跨级别传感器融合概念；2)创新的查询去噪(QDN)机制融合对象列表；3)显式注意力引导通过高斯掩码关注目标区域；4)伪对象列表生成(POLG)模块解决数据缺失问题。相比之前的工作，本文实现了不同抽象级别的端到端融合，而非同一级别的融合；采用基于学习的端到端方法替代传统贝叶斯方法；专门针对稀疏对象列表设计了融合策略，而非处理点云等密集数据；利用查询架构更适合处理结构化的对象信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提出了跨级别传感器融合的概念，通过创新的查询去噪和显式注意力引导机制，实现了高抽象级别对象列表与低级别原始相机图像的端到端融合，显著提升了3D目标检测的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IV64158.2025.11097627&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.</description>
      <author>example@mail.com (Xiangzhong Liu, Jiajie Zhang, Hao Shen)</author>
      <guid isPermaLink="false">2512.12884v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification</title>
      <link>http://arxiv.org/abs/2512.13428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种轻量级高效的植物叶片疾病识别框架，结合领域适应的MobileNetV2和MobileNetV3模型作为特征提取器，使用特征融合技术和带有注意力机制的双向LSTM分类器，在少样本学习场景下取得了接近SOTA的性能，同时保持模型轻量且适合移动设备。&lt;h4&gt;背景&lt;/h4&gt;植物叶片疾病的准确及时识别对可持续农业至关重要，但大多数深度学习方法依赖于大型标注数据集和计算密集型模型，不适合数据稀缺和资源有限的环境。&lt;h4&gt;目的&lt;/h4&gt;开发一种少样本学习方法，在轻量级高效的框架中解决植物叶片疾病识别的挑战，使其适合数据稀缺和资源有限的环境。&lt;h4&gt;方法&lt;/h4&gt;结合领域适应的MobileNetV2和MobileNetV3模型作为特征提取器，使用特征融合技术生成鲁棒特征表示，并通过带有注意力机制的双向LSTM分类器捕获序列依赖性并关注最相关特征，实现复杂环境下的最优分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;在PlantVillage数据集上，15-shot时达到98.23%±0.33%的准确率，接近SOTA基准；在真实世界条件下(Dhan Shomadhan数据集)15-shot时达到69.28%±1.49%；在六种疾病的PlantVillage子集上以15-shot学习达到99.72%，超越之前96.0%的SOTA；模型大小约40MB，推理复杂度约1.12 GFLOPs。&lt;h4&gt;结论&lt;/h4&gt;该工作为数据稀缺地区的精确植物疾病诊断建立了可扩展、移动就绪的基础，能够在资源有限的环境中实现高效的植物疾病识别。&lt;h4&gt;翻译&lt;/h4&gt;准确及时的植物叶片疾病识别对弹性可持续农业至关重要，但大多数深度学习方法依赖于大型标注数据集和计算密集型模型，不适合数据稀缺和资源受限环境。为解决这些挑战，我们提出了一种少样本学习方法，在轻量级高效框架中结合领域适应的MobileNetV2和MobileNetV3模型作为特征提取器，以及特征融合技术来生成鲁棒特征表示。对于分类任务，融合特征通过带有注意力机制增强的双向LSTM分类器，以捕获序列依赖性并关注最相关特征，从而即使在具有噪声或杂乱背景的复杂真实世界环境中也能实现最佳分类性能。所提出的框架在多种实验设置中进行了评估，包括实验室控制和野外采集的数据集。在PlantVillage数据集的番茄叶片疾病上，它从1到15 shot场景中持续提高性能，在15 shot时达到98.23±0.33%，接近Transductive LSTM with attention实现的99.98% SOTA基准，同时保持轻量级和移动友好。在Dhan Shomadhan数据集的野外图像真实世界条件下，它保持鲁棒性能，在15-shot时达到69.28±1.49%，并展现出对复杂背景的强韧性。值得注意的是，它在PlantVillage的六种疾病上也超过了之前96.0%的SOTA准确率，仅用15-shot学习就达到了99.72%。凭借约40MB的紧凑模型大小和约1.12 GFLOPs的推理复杂度，这项工作为数据稀缺地区精确植物疾病诊断建立了可扩展、移动就绪的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.</description>
      <author>example@mail.com (Anika Islam, Tasfia Tahsin, Zaarin Anjum, Md. Bakhtiar Hasan, Md. Hasanul Kabir)</author>
      <guid isPermaLink="false">2512.13428v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners</title>
      <link>http://arxiv.org/abs/2512.12824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures. Accepted to VISAPP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了CoCa模型在少样本图像分类任务上的适应能力，系统评估了从训练自由混合原型到LoRA深度参数适应的策略，发现了数据增强对不同方法的不同影响，以及监督对比损失的优越性。&lt;h4&gt;背景&lt;/h4&gt;大规模多模态基础模型如CoCa通过统一对比对齐与生成字幕化取得了最先进结果，但其对极端数据稀缺情况的适应能力尚未被充分探索，现有研究主要关注双编码器架构如CLIP，而对CoCa独特潜在空间的研究存在空白。&lt;h4&gt;目的&lt;/h4&gt;研究CoCa视觉主干在少样本图像分类任务上的适应方法，评估不同参数高效微调策略的性能，并提供针对数据稀缺的实证参考设置。&lt;h4&gt;方法&lt;/h4&gt;进行系统实证研究，评估从无需训练的混合原型到通过低秩适应（LoRA）进行深度参数适应的层次化策略，研究数据增强的影响，以及结合监督对比（SupCon）损失的混合目标性能。&lt;h4&gt;主要发现&lt;/h4&gt;1) 存在'增强发散'现象：强数据增强在低样本设置中降低线性探测性能但对稳定LoRA微调至关重要；2) 结合SupCon损失的混合目标在不同样本数量下都优于标准交叉熵；3) 表征了训练配置对数据稀缺的敏感性。&lt;h4&gt;结论&lt;/h4&gt;提供了正则化、秩和采样策略的实证参考设置，有助于高效适应生成-对比基础模型。&lt;h4&gt;翻译&lt;/h4&gt;大规模多模态基础模型，特别是对比字幕生成器（CoCa），通过统一对比对齐与生成字幕化实现了最先进的结果。虽然零样本迁移能力已有充分记录，但这些生成-对比混合模型对下游任务的适应，尤其是在极端数据稀缺（少样本学习）情况下的适应，仍未被充分探索。现有文献主要关注双编码器架构如CLIP，留下了关于CoCa独特潜在空间如何响应参数高效微调（PEFT）的研究空白。本文提出了对CoCa视觉主干进行少样本图像分类适应的综合实证研究。我们系统评估了从无需训练的混合原型到通过低秩适应（LoRA）进行深度参数适应的策略层次。首先，我们确定了'增强发散'：虽然强数据增强在低样本设置中降低了线性探测性能，但它对于稳定LoRA微调至关重要。我们还证明，在不同样本数量下，结合监督对比（SupCon）损失的混合目标比标准交叉熵始终带来性能提升。关键地，我们表征了训练配置对数据稀缺的敏感性，提供了正则化、秩和采样策略的实证参考设置，以促进生成-对比基础模型的高效适应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.</description>
      <author>example@mail.com (N. K. B. M. P. K. B. Narasinghe, Uthayasanker Thayasivam)</author>
      <guid isPermaLink="false">2512.12824v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2512.12268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Meta Test-Time Prompt Tuning (MetaTPT)，一种元学习框架，通过自监督辅助任务指导测试时提示调整，以改进视觉语言模型在域偏移下的测试时适应能力。&lt;h4&gt;背景&lt;/h4&gt;Vision-language models (VLMs)如CLIP虽然表现出强大的零样本泛化能力，但对测试时的域偏移仍然敏感。现有的测试时提示调整(TPT)通过固定增强来缓解这个问题，但在更具挑战性的场景中可能表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种元学习框架，学习自监督辅助任务来指导测试时提示调整，以解决域偏移问题。&lt;h4&gt;方法&lt;/h4&gt;MetaTPT采用双循环优化范式：内循环学习自监督任务生成信息丰富的视图，外循环通过在这些视图间保持一致性执行提示调整。辅助任务动态学习每个样本的参数化增强，能够捕获目标域中本质特征的更具表现力的变换。&lt;h4&gt;主要发现&lt;/h4&gt;通过将增强学习与提示调整相结合，MetaTPT改进了域偏移下的测试时适应能力，在领域泛化和跨数据集基准测试上达到最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;大量实验表明，MetaTPT在领域泛化和跨数据集基准测试上实现了最先进的性能，有效解决了视觉语言模型在域偏移下的测试时适应问题。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型（如CLIP）展现出强大的零样本泛化能力，但在测试时对域偏移仍然敏感。测试时提示调整（TPT）通过固定增强来缓解这一问题，但在更具挑战性的场景中可能表现不佳。在这项工作中，我们提出了元测试时提示调整（MetaTPT），这是一个元学习框架，通过学习自监督辅助任务来指导测试时提示调整。辅助任务为每个样本动态学习参数化增强，能够捕获目标域中本质特征的更具表现力的变换。MetaTPT采用双循环优化范式：内循环学习自监督任务以生成信息丰富的视图，而外循环通过在这些视图间保持一致性来执行提示调整。通过将增强学习与提示调整相结合，MetaTPT改进了域偏移下的测试时适应能力。大量实验表明，MetaTPT在领域泛化和跨数据集基准测试上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.</description>
      <author>example@mail.com (Yuqing Lei, Yingjun Du, Yawen Huang, Xiantong Zhen, Ling Shao)</author>
      <guid isPermaLink="false">2512.12268v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion</title>
      <link>http://arxiv.org/abs/2512.12182v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于生成表示的少样本知识图谱补全框架，结合了两阶段注意力三元组增强器和U-KAN扩散模型，在公共数据集上取得了最先进结果。&lt;h4&gt;背景&lt;/h4&gt;知识图谱因其简洁高效的三元组结构已被广泛应用于智能问答和推荐系统等领域。然而，现实世界数据的异构性和多面性导致关系分布呈现长尾特性，使得在有限样本下补全缺失事实变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;重新审视知识图谱补全问题，从生成表示角度出发，提出一种能有效利用有限样本进行知识图谱补全的方法。&lt;h4&gt;方法&lt;/h4&gt;提出一种少样本知识图谱补全框架，整合了两阶段注意力三元组增强器和基于U-KAN的扩散模型。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集上的大量实验表明，该方法取得了新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;通过结合两阶段注意力三元组增强器和U-KAN扩散模型，该方法有效解决了关系分布长尾情况下的有限样本知识图谱补全问题，性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱（KGs）凭借其简洁高效的三元组结构，已被广泛应用于智能问答、推荐系统等领域。然而，现实世界数据的异构性和多面性不可避免地导致关系分布呈现长尾特性，这使得在有限样本下补全缺失事实变得至关重要。以往研究主要基于度量匹配或元学习，但它们要么未能充分利用图中的邻域信息，要么忽视了对比信号的分布特征。本文我们从生成表示的角度重新审视这一问题，并提出了一种结合两阶段注意力三元组增强器和基于U-KAN的扩散模型的少样本知识图谱补全框架。在两个公共数据集上的大量实验表明，我们的方法取得了最新的最先进结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.</description>
      <author>example@mail.com (Xinyu Gao)</author>
      <guid isPermaLink="false">2512.12182v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation</title>
      <link>http://arxiv.org/abs/2512.12177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the proceedings of the IEEE International Conference on Big Data (IEEE BigData 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Floorplan2Guide是一种创新的室内导航解决方案，通过将平面图转换为知识图谱和使用大语言模型，减少了手动预处理需求，提高了视障人士在室内环境中的导航能力。&lt;h4&gt;背景&lt;/h4&gt;室内导航对视障人士来说仍然是一个重大挑战。当前解决方案主要基于基础设施的系统，限制了在动态环境中安全导航的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的导航方法，利用基础模型将平面图转换为可导航的知识图谱，并生成人类可读的导航指令。&lt;h4&gt;方法&lt;/h4&gt;提出了Floorplan2Guide方法，该方法利用基础模型将平面图转换为可导航的知识图谱，并集成大语言模型（LLM）从建筑布局中提取空间信息，减少了早期平面图解析方法所需的手动预处理。系统采用基于图的空间结构和上下文学习来提高导航性能。&lt;h4&gt;主要发现&lt;/h4&gt;少样本学习在模拟和真实世界评估中比零样本学习提高了导航准确性。Claude 3.7 Sonnet在评估的模型中达到了最高准确率，在5-shot提示下分别为：短路线92.31%，中路线76.92%，长路线61.54%。基于图的空间结构的成功率比直接视觉推理高15.4%。&lt;h4&gt;结论&lt;/h4&gt;图形表示和上下文学习使解决方案对视障人士（BLV用户）的室内导航更加精确。&lt;h4&gt;翻译&lt;/h4&gt;室内导航对视障人士来说仍然是一个重大挑战。目前的解决方案主要依赖于基于基础设施的系统，这限制了它们在动态环境中安全导航的能力。我们提出了一种新颖的导航方法，利用基础模型将平面图转换为可导航的知识图谱，并生成人类可读的导航指令。Floorplan2Guide集成了大语言模型（LLM）从建筑布局中提取空间信息，减少了早期平面图解析方法所需的手动预处理。实验结果表明，与在模拟和真实世界评估中的零样本学习相比，少样本学习提高了导航准确性。在评估的模型中，Claude 3.7 Sonnet在MP-1平面图的5-shot提示下分别达到了短路线92.31%、中路线76.92%和长路线61.54%的最高准确率。所有模型中，基于图的空间结构的成功率比直接视觉推理高15.4%，这证实了图形表示和上下文学习提高了导航性能，并使我们的解决方案对盲人和低视力（BLV）用户的室内导航更加精确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.</description>
      <author>example@mail.com (Aydin Ayanzadeh, Tim Oates)</author>
      <guid isPermaLink="false">2512.12177v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking</title>
      <link>http://arxiv.org/abs/2512.13197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Pure and Applied Geophysics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何通过迁移学习将Phase Neural Operator (PhaseNO)网络范围地震相拾取器适应微地震监测，使用预训练模型并在少量微地震数据上微调，显著提高了拾取性能。&lt;h4&gt;背景&lt;/h4&gt;地震相拾取是微地震监测和地下成像的关键技术，但传统手动处理不适用于实时应用或大型阵列。基于深度学习的拾取器虽可自动化，但通常针对高信噪比、长时间网络优化，难以处理微地震数据集的挑战。&lt;h4&gt;目的&lt;/h4&gt;展示如何将PhaseNO网络范围地震相拾取器通过迁移学习适应微地震监测任务。&lt;h4&gt;方法&lt;/h4&gt;从在57,000多个三分量地震和噪声记录上预训练的PhaseNO模型开始，使用仅200个来自水力压裂环境中诱发事件的标记地震图和噪声地震图对模型进行微调，保留从丰富地震数据中学到的时空表示，同时适应微地震相的特征和标记约定。&lt;h4&gt;主要发现&lt;/h4&gt;与原始PhaseNO和传统工作流程相比，调整后的模型将F1分数和准确率提高了高达30%，显著减少了系统时间偏差和拾取不确定性。&lt;h4&gt;结论&lt;/h4&gt;这种适应方法依赖于小型、特定于校准的数据集，可轻松转移到其他微地震任务，相关代码将在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;地震相拾取经常用于微地震监测和地下成像。传统手动处理不适用于实时应用或大型阵列。基于深度学习的拾取器在大型地震目录上训练，提供了自动化的替代方案。然而，它们通常针对高信噪比、长时间网络进行优化，难以处理微地震数据集的挑战，这些数据集是为有限时间而构建的，且没有预先检测到的地震活动。在本研究中，我们展示了如何通过网络范围的地震相拾取器Phase Neural Operator (PhaseNO)使用迁移学习适应微地震监测。从在57,000多个三分量地震和噪声记录上预训练的PhaseNO模型开始，我们仅使用来自水力压裂环境中诱发事件的200个标记地震图和噪声地震图对模型进行微调。微调后的模型保留了从丰富地震数据中学到的丰富时空表示，同时适应了微地震相的特征和标记约定，这些相通常在峰值或谷值处拾取，而不是在起始点处。我们在三个具有不同网络几何形状和采集参数的真实世界微地震数据集上评估性能。与原始PhaseNO和传统工作流程相比，调整后的模型将F1分数和准确率提高了高达30%，并显著减少了系统时间偏差和拾取不确定性。由于适应依赖于小型、特定于活动校准的数据集，该方法可轻松转移到其他微地震任务，其中公共地震数据和预训练模型是可访问的。相关代码将在https://github.com/ayratabd/MicroPhaseNO上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Seismic phase picking is very often used for microseismic monitoring and subsurface imaging. Traditional manual processing is not feasible for either real-time applications or large arrays. Deep learning-based pickers trained on large earthquake catalogs offer an automated alternative. However, they are typically optimized for high signal-to-noise, long-duration networks and struggle with the challenges presented by microseismic datasets, which are purpose-built for limited time without previously detected seismicity. In this study, we demonstrate how a network-wide earthquake phase picker, the Phase Neural Operator (PhaseNO), can be adapted to microseismic monitoring using transfer learning. Starting from a PhaseNO model pre-trained on more than 57,000 three-component earthquake and noise records, we fine-tune the model using only 200 labeled and noise seismograms from induced events in hydraulic-fracturing settings. The fine-tuned model thus preserves the rich spatio-temporal representation learned from abundant earthquake data, while adapting to the characteristics and labeling conventions of microseismic phases, which are often picked on peaks or troughs rather than onsets. We evaluate performance on three distinct real-world microseismic datasets with different network geometries and acquisition parameters. Compared to the original PhaseNO and a conventional workflow, the adapted model increases F1 score and accuracy by up to 30%, and strongly reduces systematic timing bias and pick uncertainty. Because the adaptation relies on a small, campaign-specific calibration set, the approach is readily transferable to other microseismic tasks where public earthquake data and pre-trained models are accessible. The associated code will be released openly at https://github.com/ayratabd/MicroPhaseNO.</description>
      <author>example@mail.com (Ayrat Abdullin, Umair bin Waheed, Leo Eisner, Naveed Iqbal)</author>
      <guid isPermaLink="false">2512.13197v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Multi-fidelity aerodynamic data fusion by autoencoder transfer learning</title>
      <link>http://arxiv.org/abs/2512.13069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多保真度深度学习框架，结合自编码器迁移学习和多分割保形预测策略，在数据极度稀缺情况下实现高精度空气动力学预测并量化不确定性。&lt;h4&gt;背景&lt;/h4&gt;高精度空气动力学预测通常依赖高保真度模拟，但其计算成本极高，限制了在数据驱动建模中的应用。这促使研究人员开发多保真度策略，利用低成本的低保真度信息而不牺牲准确性。&lt;h4&gt;目的&lt;/h4&gt;解决数据极度稀缺情况下的不确定性感知空气动力学数据融合问题，开发一种高效可靠的空气动力学回归解决方案。&lt;h4&gt;方法&lt;/h4&gt;利用丰富的低保真度数据学习紧凑的潜在物理表示作为冻结知识库，然后使用稀缺的高保真度样本对解码器进行微调，并结合新开发的多分割保形预测策略进行不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;模型成功校正了低保真度偏差，使用极少的高保真度训练数据实现了高精度压力预测；MSCP框架产生了稳健、可操作的不确定性区间，点覆盖率超过95%；该方法在二维NACA翼型和三维跨音速机翼数据库测试中表现良好。&lt;h4&gt;结论&lt;/h4&gt;通过结合极端数据效率和不确定性量化，为数据稀缺环境中的空气动力学回归提供了可扩展且可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的空气动力学预测通常依赖于高保真度模拟；然而，其高昂的计算成本严重限制了它们在数据驱动建模中的应用。这一局限性促使了多保真度策略的发展，该策略利用低成本的低保真度信息而不牺牲准确性。为应对这一挑战，本研究提出了一个多保真度深度学习框架，结合基于自编码器的迁移学习和新开发的多分割保形预测策略，在极端数据稀缺情况下实现不确定性感知的空气动力学数据融合。该方法利用丰富的低保真度数据学习紧凑的潜在物理表示，该表示作为冻结知识库，用于随后使用稀缺的高保真度样本进行微调的解码器。在NACA翼型二维和跨音速机翼三维数据库的表面压力分布测试中，模型成功校正了低保真度偏差，并使用极少的高保真度训练数据实现了高精度压力预测。此外，MSCP框架产生了稳健、可操作的不确定性区间，点覆盖率超过95%。通过结合极端数据效率与不确定性量化，本研究为数据稀缺环境中的空气动力学回归提供了可扩展且可靠的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.</description>
      <author>example@mail.com (Javier Nieto-Centenero, Esther Andrés, Rodrigo Castellanos)</author>
      <guid isPermaLink="false">2512.13069v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing</title>
      <link>http://arxiv.org/abs/2512.13018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures. Comprehensive evaluation of preprocessing, data augmentation, and transfer learning for cross-environment generalization in deep learning-based mmWave radar sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次全面评估了空间泛化技术，这些技术对于深度学习射频感应的实际部署至关重要。研究聚焦于使用调频连续波多输入多输出雷达进行室内环境中的人数统计，系统研究了多种方法。&lt;h4&gt;背景&lt;/h4&gt;空间泛化技术对于深度学习射频感应的实际部署至关重要，研究关注室内环境中使用调频连续波多输入多输出雷达进行人数统计。&lt;h4&gt;目的&lt;/h4&gt;系统性评估空间泛化技术，探索不同方法在跨环境性能上的表现。&lt;h4&gt;方法&lt;/h4&gt;研究包括幅度统计预处理（sigmoid加权和阈值归零）、频域滤波、基于自编码器的背景抑制、数据增强策略和迁移学习。实验在两种不同布局的环境中收集结果。&lt;h4&gt;主要发现&lt;/h4&gt;基于sigmoid的幅度加权方法在跨环境性能上始终表现最佳，与基线方法相比，RMSE和MAE分别降低了50.1%和55.2%。数据增强提供了额外的适度益处，MAE最多提高8.8%。对于大的空间变化，迁移学习至关重要，使用540个目标域样本，RMSE和MAE分别降低了82.1%和91.3%。&lt;h4&gt;结论&lt;/h4&gt;通过结合深度学习模型、基于幅度的预处理和高效的迁移学习，可以为雷达感应系统建立一个高度实用的方向，使其能够在空间变化下保持准确度。&lt;h4&gt;翻译&lt;/h4&gt;本研究首次全面评估了空间泛化技术，这些技术对于基于深度学习的射频感应的实际部署至关重要。研究聚焦于使用调频连续波多输入多输出雷达在室内环境中进行人数统计，我们系统性地研究了广泛的方法集，包括基于幅度的统计预处理（sigmoid加权和阈值归零）、频域滤波、基于自编码器的背景抑制、数据增强策略和迁移学习。在两种不同布局的环境中收集的实验结果表明，基于sigmoid的幅度加权始终实现优异的跨环境性能，与基线方法相比，均方根误差和平均绝对误差分别降低了50.1%和55.2%。数据增强提供了额外的适度益处，平均绝对误差最多提高8.8%。相比之下，对于大的空间变化，迁移学习至关重要，使用540个目标域样本，均方根误差和平均绝对误差分别降低了82.1%和91.3%。综合来看，这些发现为开发雷达感应系统建立了一个高度实用的方向，通过将深度学习模型与基于幅度的预处理和高效的迁移学习相结合，使其能够在空间变化下保持准确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.</description>
      <author>example@mail.com (Tomoya Tanaka, Tomonori Ikeda, Ryo Yonemoto)</author>
      <guid isPermaLink="false">2512.13018v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised learning of multiscale switching dynamical system models from multimodal neural data</title>
      <link>http://arxiv.org/abs/2512.12881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种新颖的无监督学习算法，用于从多尺度神经观测中学习切换多尺度动力学系统模型，解决了依赖性非平稳性建模的挑战，并展示了在行为解码和神经动力学建模方面的优越性能。&lt;h4&gt;背景&lt;/h4&gt;神经群体活动常表现出依赖性非平稳性，表现为切换动力学。现有方法主要从单一神经模态学习模型，而实际研究经常记录多种神经模态。此外，训练数据中通常没有可用的依赖性标签，这给学习依赖性切换动力学模型带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种无监督学习算法，仅使用多尺度神经观测来学习多尺度切换动力学系统模型，以更准确地建模复杂神经动力学并提高行为解码性能。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新颖的无监督学习算法，使用多尺度神经观测来学习切换多尺度动力学系统模型的参数。通过模拟和两个不同的实验数据集（包含不同运动任务期间的多模态尖峰-LFP观测）验证该方法。&lt;h4&gt;主要发现&lt;/h4&gt;切换多尺度动力学系统模型比切换单尺度动力学模型更准确地解码行为，证明了多尺度神经融合的成功。此外，这些模型优于平稳多尺度模型，说明了在多模态神经数据中跟踪依赖性非平稳性的重要性。&lt;h4&gt;结论&lt;/h4&gt;开发的无监督学习框架通过利用多模态记录中的信息并纳入依赖性切换，能够更准确地建模复杂的多尺度神经动力学。这种方法有望提高脑机接口随时间的性能和鲁棒性，并增进我们对行为神经基础的理解。&lt;h4&gt;翻译&lt;/h4&gt;神经群体活动通常表现为依赖性非平稳性，以切换动力学的形式存在。学习准确的切换动力学系统模型可以揭示行为如何在神经活动中被编码。现有的切换方法主要专注于从单一神经模态（连续高斯信号或离散泊松信号）学习模型。然而，为了测量大脑活动的不同时空尺度，经常同时记录多种神经模态，并且所有这些模态都可以编码行为。此外，训练数据中通常没有可用的依赖性标签，这给学习依赖性切换动力学模型带来了重大挑战。为了应对这些挑战，我们开发了一种新颖的无监督学习算法，仅使用多尺度神经观测来学习切换多尺度动力学系统模型的参数。我们使用模拟和两个不同的实验数据集（包含不同运动任务期间的多模态尖峰-LFP观测）展示了我们的方法。我们发现，我们的切换多尺度动力学系统模型比切换单尺度动力学模型更准确地解码行为，表明了多尺度神经融合的成功。此外，我们的模型优于平稳多尺度模型，说明了在多模态神经数据中跟踪依赖性非平稳性的重要性。开发的无监督学习框架通过利用多模态记录中的信息并纳入依赖性切换，能够更准确地建模复杂的多尺度神经动力学。这种方法有望提高脑机接口随时间的性能和鲁棒性，并增进我们对行为神经基础的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.</description>
      <author>example@mail.com (DongKyu Kim, Han-Lin Hsieh, Maryam M. Shanechi)</author>
      <guid isPermaLink="false">2512.12881v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk</title>
      <link>http://arxiv.org/abs/2512.12795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TRACER是一种基于迁移学习的框架，用于应对临床决策支持工具中的性能漂移问题，特别是在人口时间变化和混合人群的情况下。&lt;h4&gt;背景&lt;/h4&gt;基于电子健康记录的临床决策支持工具常常因人口时间变化而出现性能漂移，当临床环境变化最初只影响部分患者时，会导致转变为混合人群。这种病例组合变化通常发生在系统级运营更新或新疾病（如COVID-19）出现之后。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够识别就诊级别过渡成员身份并适应预测模型的框架，无需完全重新训练，以保持临床决策支持工具的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了TRACER（基于迁移学习的临床 evolving 风险实时适应）框架，该框架使用迁移学习技术来适应预测模型，而不需要完全重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟研究中，TRACER优于在历史或当代数据上训练的静态模型。在预测COVID-19转变后急诊科就诊后住院的真实世界应用中，TRACER提高了区分度和校准度。&lt;h4&gt;结论&lt;/h4&gt;TRACER提供了一种可扩展的方法，可以在不断变化和异质的临床条件下保持强大的预测性能。&lt;h4&gt;翻译&lt;/h4&gt;基于电子健康记录构建的临床决策支持工具经常因人口时间变化而出现性能漂移，特别是当临床环境变化最初只影响部分患者时，导致转变为混合人群。这种病例组合变化通常发生在系统级运营更新或新疾病（如COVID-19）出现之后。我们提出了TRACER（基于迁移学习的临床 evolving 风险实时适应）框架，该框架识别就诊级别的过渡成员身份，并使用迁移学习适应预测模型，无需完全重新训练。在模拟研究中，TRACER优于在历史或当代数据上训练的静态模型。在预测COVID-19转变后急诊科就诊后住院的真实世界应用中，TRACER提高了区分度和校准度。TRACER提供了一种可扩展的方法，可以在不断变化和异质的临床条件下保持强大的预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.</description>
      <author>example@mail.com (Mengying Yan, Ziye Tian, Siqi Li, Nan Liu, Benjamin A. Goldstein, Molei Liu, Chuan Hong)</author>
      <guid isPermaLink="false">2512.12795v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning Predictive Analytics for Social Media Enabled Women's Economic Empowerment in Pakistan</title>
      <link>http://arxiv.org/abs/2512.12685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了年轻女性赋权与巴基斯坦经济增长之间的关系，重点关注社交媒体如何增强她们的业务并推动经济发展。研究发现社交媒体使用与创业之间存在潜力，但也存在显著差距和多种障碍。&lt;h4&gt;背景&lt;/h4&gt;研究背景为巴基斯坦父权制社会中年轻女性赋权与经济增长的关系，社交媒体在其中的作用尚未被充分研究。&lt;h4&gt;目的&lt;/h4&gt;调查社交媒体使用如何增强年轻女性的业务并推动巴基斯坦经济发展，分析社交媒体参与模式与创业预测之间的关系。&lt;h4&gt;方法&lt;/h4&gt;采用混合研究方法设计，结合线上和线下随机抽样调查51名受访者，并利用社交媒体使用数据(n=1000)和创业数据(n=1092)进行分析。使用无监督学习识别社交媒体参与模式，应用监督模型进行创业预测，逻辑回归表现最佳。&lt;h4&gt;主要发现&lt;/h4&gt;39.4%的受访者认为社交媒体对经济有积极影响，但只有14%参与创业，存在显著差距。YouTube(66.7%)和WhatsApp(62.7%)是最常用的平台。主要障碍包括网络骚扰、数字素养有限和文化约束。52.9%的受访者不了解支持女性创业者的政府倡议。&lt;h4&gt;结论&lt;/h4&gt;社交媒体在女性赋权和经济发展中具有潜力，但需要解决数字参与与商业采用之间的差距，以及文化、教育等多重障碍，同时加强政策覆盖和支持。&lt;h4&gt;翻译&lt;/h4&gt;我们的研究调查了年轻女性赋权与巴基斯坦经济增长之间的相互作用，重点关注社交媒体使用如何增强她们的业务并推动经济发展。我们采用混合研究方法设计，结合线上和线下随机抽样，对51名受访者进行调查。我们还利用了包含社交媒体使用(n=1000)和创业(n=1092)的现有数据集。我们的分析通过无监督学习识别不同的社交媒体参与模式，并应用监督模型进行创业预测，逻辑回归在预测准确性和稳定性方面优于所有其他算法。在社交媒体使用方面，聚类分析显示在K=2时，用户形成紧密的、良好分离的参与群体。结果表明，39.4%的受访者认为社交媒体通过帮助企业增加收入对经济产生积极影响。然而，只有14%的受访者参与创业，突显了数字参与与商业采用之间的显著差距。分析表明，每日社交媒体使用普遍，YouTube(66.7%)和WhatsApp(62.7%)是最常使用的平台。确定的主要障碍是网络骚扰、数字素养有限以及在巴基斯坦这样的父权制社会中的文化约束。此外，52.9%的受访者不知道支持女性创业者的政府倡议，表明政策覆盖有限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Our study investigates the interplay between young women's empowerment and Pakistan's economic growth, focusing on how social media use enhances their businesses and drives economic advancement. We utilize a mixed-methods research design, integrating both online and offline random sampling, for our survey of 51 respondents. We also utilized existing datasets consisting of both social media usage (n = 1000) and entrepreneurship (n = 1092). Our analysis identifies distinct social media engagement patterns via unsupervised learning and applies supervised models for entrepreneurship prediction, with logistic regression outperforming all other algorithms in terms of predictive accuracy and stability. In social media use, the cluster analysis reveals that at K=2, users form tightly packed, well-separated engagement groups. The results indicate that 39.4 percent of respondents believe social media positively impacts the economy by enabling businesses to generate increased revenue. However, only 14 percent of respondents participate in entrepreneurship, highlighting a substantial gap between digital engagement and business adoption. The analysis indicates that daily social media consumption is widespread with YouTube (66.7 percent) and WhatsApp (62.7 percent) being the most frequently used platforms. Key barriers identified are online harassment, limited digital literacy, and cultural constraints in a patriarchal society such as Pakistan. Additionally, 52.9 percent of respondents are unaware of government initiatives supporting women entrepreneurs, indicating limited policy outreach.</description>
      <author>example@mail.com (Maryam Arif, Soban Saeed)</author>
      <guid isPermaLink="false">2512.12685v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search</title>
      <link>http://arxiv.org/abs/2512.12296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to WACV 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GrowTAS的渐进式训练框架，用于解决Transformer架构搜索(TAS)中子网络权重共享导致的干扰问题，并通过GrowTAS+进一步优化大型子网络性能，在多个基准测试上证明了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的TAS方法通常训练一个包含所有候选架构的超网络，但这些子网络共享相同权重，导致干扰，严重影响小型子网络性能。&lt;h4&gt;目的&lt;/h4&gt;减少子网络间的干扰，稳定训练过程，提高Transformer架构搜索的效率，自动发现更优的视觉Transformer架构。&lt;h4&gt;方法&lt;/h4&gt;提出GrowTAS渐进式训练框架，从训练小型子网络开始，逐步纳入更大的子网络；进一步提出GrowTAS+，仅微调部分权重以提高大型子网络性能。&lt;h4&gt;主要发现&lt;/h4&gt;训练良好的小型子网络可以作为训练大型子网络的良好基础；渐进式训练可以减少子网络间的干扰并稳定训练过程。&lt;h4&gt;结论&lt;/h4&gt;GrowTAS和GrowTAS+在ImageNet和多个迁移学习基准测试上均优于当前TAS方法，证明了该渐进式训练框架的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Transformer架构搜索(TAS)旨在自动发现高效的视觉Transformer(ViT)，减少手动设计的需求。现有的TAS方法通常训练一个包含所有候选架构（即子网络）的过参数化网络（即超网络）。然而，所有子网络共享相同的权重集，这导致干扰严重降低了小型子网络的性能。我们发现训练良好的小型子网络可以作为训练大型子网络的良好基础。受此启发，我们提出了一个名为GrowTAS的渐进式训练框架，从训练小型子网络开始，并逐步纳入更大的子网络。这减少了干扰并稳定了训练过程。我们还引入了GrowTAS+，它仅微调部分权重以进一步提高大型子网络的性能。在ImageNet和多个迁移学习基准测试（包括CIFAR-10/100、Flowers、CARS和INAT-19）上的大量实验证明了我们的方法优于当前的TAS方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods</description>
      <author>example@mail.com (Hyunju Lee, Youngmin Oh, Jeimin Jeon, Donghyeon Baek, Bumsub Ham)</author>
      <guid isPermaLink="false">2512.12296v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Autonomous Air Vehicles</title>
      <link>http://arxiv.org/abs/2512.08512v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  in IEEE Transactions on Industrial Informatics,2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于轻量级迁移学习的锂离子电池健康状态监测方法，称为结构化增量迁移学习(CITL)，解决了传统方法在便携式移动设备上计算资源消耗大的问题，通过半监督学习和迭代添加网络节点提高监测精度。&lt;h4&gt;背景&lt;/h4&gt;准确快速的锂离子电池健康状态监测对电池供电的便携式移动设备至关重要，但传统迁移学习方法需要大量目标工作条件下的训练数据，且在便携式设备上会消耗大量计算资源，降低工作续航时间。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级的迁移学习方法，适用于便携式移动设备的SOH监测，解决传统方法计算资源消耗大的问题，并利用目标域中的未标记数据提高监测精度。&lt;h4&gt;方法&lt;/h4&gt;提出基于结构化增量迁移学习(CITL)的轻量级SOH监测方法，利用目标域未标记数据实现半监督迁移学习，通过迭代添加网络节点最小化监测残差，并通过结构风险最小化、传输失配最小化和流形一致性最大化保证跨域学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;CITL方法在AAV电池数据集上表现优异，比SS-TCA、MMD-LSTM-DA、DDAN、BO-CNN-TL和AS$^3$LSTM分别提高83.73%、61.15%、28.24%、87.70%和57.34%的SOH估计精度，通过均方根误差指标评估。&lt;h4&gt;结论&lt;/h4&gt;CITL方法有效解决了传统迁移学习方法在便携式移动设备上的计算资源消耗问题，通过半监督学习和结构化增量迁移学习显著提高SOH监测准确性，同时减少计算资源需求，适合在便携式移动设备上使用。&lt;h4&gt;翻译&lt;/h4&gt;准确且快速的锂离子电池健康状态监测对电池供电的便携式移动设备指示能量信息起着重要作用。为了应对变化的工作条件，迁移学习成为一种有前景的技术，可以利用数据丰富的源工作条件中的知识，显著减少SOH监测所需的训练数据量。然而，当应用于便携式移动设备时，传统的基于迁移学习的SOH监测不可行，因为在迁移学习阶段会消耗大量计算资源并降低工作续航时间。为解决这些挑战，本文提出了一种基于轻量级迁移学习的SOH监测方法，采用结构化增量迁移学习。首先，利用目标域中的未标记数据，提出半监督迁移学习机制，通过迭代添加网络节点以建设性方式最小化监测残差。其次，通过结构风险最小化、传输失配最小化和流形一致性最大化，全面保证了CITL节点参数的跨域学习能力。此外，提供了CITL的收敛分析，理论上保证了迁移学习性能和网络紧凑性的有效性。最后，通过使用从数十次飞行任务收集的真实自主飞行器电池数据集进行的广泛实验验证了所提出的方法。具体而言，使用均方根误差指标评估时，CITL在SOH估计方面分别优于其他方法，提高幅度显著。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TII.2025.3631012&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic autonomous air vehicles (AAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.</description>
      <author>example@mail.com (Jiang Liu, Yan Qin, Wei Dai, Chau Yuen)</author>
      <guid isPermaLink="false">2512.08512v2</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging</title>
      <link>http://arxiv.org/abs/2512.13434v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了一种自监督超声基础模型在自动胎儿肾脏畸形分类中的性能，结果表明该模型在所有评估指标上均优于传统基线模型，特别是在多类分类任务中表现出显著优势。&lt;h4&gt;背景&lt;/h4&gt;产前超声是检测肾脏和泌尿道先天性畸形的基础，但诊断受到操作者依赖性和成像条件不佳的限制。&lt;h4&gt;目的&lt;/h4&gt;评估一种自监督超声基础模型在自动胎儿肾脏畸形分类中的性能。&lt;h4&gt;方法&lt;/h4&gt;使用包含969张二维超声图像的精选数据集，对预训练的超声自监督掩码自编码基础模型(USF-MAE)进行微调，用于二元分类和多类分类（正常肾脏、尿路扩张和囊性发育不良肾），并与DenseNet-169卷积基线模型进行比较，使用交叉验证和独立测试集评估模型。&lt;h4&gt;主要发现&lt;/h4&gt;USF-MAE在所有评估指标上均优于基线模型；验证集上AUC提高约1.87%，F1分数提高7.8%；独立测试集上AUC提高2.32%，F1分数提高4.33%；多类分类设置中改进最大：AUC提高16.28%，F1分数提高46.15%；Score-CAM可视化表明模型预测基于已知的临床相关肾脏结构。&lt;h4&gt;结论&lt;/h4&gt;超声特定的自监督学习可以生成有用的表示作为下游诊断任务的基础；提出的框架为产前检测肾脏畸形提供了一种稳健、可解释的方法；基础模型在产科影像学中显示出前景。&lt;h4&gt;翻译&lt;/h4&gt;产前超声是检测肾脏和泌尿道先天性畸形的基础，但诊断受到操作者依赖性和成像条件不佳的限制。我们旨在评估一种自监督超声基础模型在自动胎儿肾脏畸形分类中的性能，使用包含969张二维超声图像的精选数据集。预训练的超声自监督掩码自编码基础模型(USF-MAE)被微调用于二元和多类分类，包括正常肾脏、尿路扩张和囊性发育不良肾。模型使用交叉验证和独立测试集与DenseNet-169卷积基线进行比较。USF-MAE在二元和多类设置的所有评估指标上一致优于基线。在验证集上，USF-MAE的AUC提高了约1.87%，F1分数提高了7.8%；在独立测试集上，AUC提高了2.32%，F1分数提高了4.33%。最大的改进出现在多类设置中，AUC提高了16.28%，F1分数提高了46.15%。为促进模型可解释性，将Score-CAM可视化适配到transformer架构，表明模型预测基于已知的临床相关肾脏结构，包括尿路扩张中的肾盂和囊性发育不良肾中的囊性区域。这些结果表明，超声特定的自监督学习可以生成有用的表示作为下游诊断任务的基础。所提出的框架为支持产前肾脏畸形检测提供了一种稳健、可解释的方法，并展示了基础模型在产科影像学中的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.</description>
      <author>example@mail.com (Youssef Megahed, Inok Lee, Robin Ducharme, Kevin Dick, Adrian D. C. Chan, Steven Hawken, Mark C. Walker)</author>
      <guid isPermaLink="false">2512.13434v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations</title>
      <link>http://arxiv.org/abs/2512.13093v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PVP的本体感觉-特权对比学习框架，解决了人形机器人全身控制中强化学习的样本效率问题，并通过开发的SRL4Humanoid框架进行了系统评估，实验证明该方法显著提高了样本效率和最终性能。&lt;h4&gt;背景&lt;/h4&gt;人形机器人在动态环境中执行复杂任务需要高效且鲁棒的全身控制(WBC)。尽管强化学习(RL)在此领域取得了成功，但由于人形机器人的复杂动力学和部分可观察性，其样本效率低仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决强化学习在人形机器人控制中的样本效率问题，提高学习速度和稳定性。&lt;h4&gt;方法&lt;/h4&gt;提出了PVP(Proprioceptive-Privileged对比学习框架)，利用本体感觉状态和特权状态之间的内在互补性。PVP能够学习紧凑且与任务相关的潜在表示，不需要手工设计的数据增强。同时开发了SRL4Humanoid框架，这是第一个统一且模块化的框架，为人形机器人学习提供了高质量的状态表示学习(SRL)方法实现。&lt;h4&gt;主要发现&lt;/h4&gt;在LimX Oli机器人上的广泛实验表明，与基线SRL方法相比，PVP显著提高了样本效率和最终性能，特别是在速度跟踪和运动模仿任务中。&lt;h4&gt;结论&lt;/h4&gt;研究提供了将SRL与RL集成用于人形机器人全身控制的实用见解，为数据高效的人形机器人学习提供了有价值的指导。&lt;h4&gt;翻译&lt;/h4&gt;实现高效且鲁棒的全身控制(WBC)对于使人形机器人在动态环境中执行复杂任务至关重要。尽管强化学习(RL)在此领域取得了成功，但由于人形机器人的复杂动力学和部分可观察性，其样本效率低仍然是一个重大挑战。为解决这一局限性，我们提出了PVP，一个本体感觉-特权对比学习框架，它利用本体感觉状态和特权状态之间的内在互补性。PVP学习紧凑且与任务相关的潜在表示，不需要手工设计的数据增强，从而实现更快且更稳定的策略学习。为支持系统评估，我们开发了SRL4Humanoid，这是第一个统一且模块化的框架，为人形机器人学习提供了高质量的状态表示学习(SRL)方法实现。在LimX Oli机器人上进行的广泛实验，包括速度跟踪和运动模仿任务，证明与基线SRL方法相比，PVP显著提高了样本效率和最终性能。我们的研究进一步提供了将SRL与RL集成用于人形机器人全身控制的实用见解，为数据高效的人形机器人学习提供了宝贵的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.</description>
      <author>example@mail.com (Mingqi Yuan, Tao Yu, Haolin Song, Bo Li, Xin Jin, Hua Chen, Wenjun Zeng)</author>
      <guid isPermaLink="false">2512.13093v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Citation importance-aware document representation learning for large-scale science mapping</title>
      <link>http://arxiv.org/abs/2512.13054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种考虑引文重要性的对比学习框架，通过测量引文重要性并整合到对比学习过程中，解决了科学映射中引文异质性问题，有效提高了文档表示质量和科学映射准确性，并在大规模数据集上成功应用。&lt;h4&gt;背景&lt;/h4&gt;科学映射是科学计量学和情报研究中的重要任务，但面临引文复杂性和异质性的挑战。先前研究虽尝试通过整合引文和语义信息改进文档表示，但忽视了引文的异质性。&lt;h4&gt;目的&lt;/h4&gt;解决引文异质性问题，提出一个考虑引文重要性的对比学习框架，以改进文档表示和科学映射的准确性。&lt;h4&gt;方法&lt;/h4&gt;开发基于位置、频率和自引特征的引文重要性可扩展测量方法；将引文重要性整合到对比学习过程中，通过重要性感知采样策略选择低重要性引文作为困难样本；使用SciBERT模型进行微调；在SciDocs和PubMed基准数据集上评估；将模型应用于Web of Science的3300多万篇文档。&lt;h4&gt;主要发现&lt;/h4&gt;在文档表示质量和科学映射准确性方面均取得一致改进；生成的科学地图准确可视化科学的全球和局部智力结构；揭示了跨学科研究前沿。&lt;h4&gt;结论&lt;/h4&gt;通过将引文异质性转化为可扩展的计算框架，证明了区分引文重要性可有效改进文档表示和科学映射。&lt;h4&gt;翻译&lt;/h4&gt;有效的科学映射依赖于高质量的科学文档表示。作为科学计量学和情报研究中的重要任务，科学映射常常面临引文复杂和异质性的挑战。虽然先前研究试图通过整合引文和语义信息来改进文档表示，但引文的异质性常被忽视。为解决这一问题，本研究提出了一个考虑引文重要性的对比学习框架，以优化监督信号。我们首先开发了一种基于位置、频率和自引特征的引文重要性可扩展测量方法。然后通过重要性感知采样策略将引文重要性整合到对比学习过程中，选择低重要性引文作为困难样本。这迫使模型学习能够区分重要性和形式性引文的细粒度表示。为验证所提框架的有效性，我们微调了SciBERT模型，并在SciDocs和PubMed基准数据集上进行了广泛评估。结果表明，在文档表示质量和科学映射准确性方面均取得了一致的改进。此外，我们将训练模型应用于Web of Science中的3300多万篇文档。生成的科学地图准确可视化了科学的全球和局部智力结构，并揭示了跨学科研究前沿。通过将引文异质性转化为可扩展的计算框架，本研究证明了如何通过区分引文重要性来有效改进文档表示和科学映射。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.ipm.2025.104557&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective science mapping relies on high-quality representations of scientific documents. As an important task in scientometrics and information studies, science mapping is often challenged by the complex and heterogeneous nature of citations. While previous studies have attempted to improve document representations by integrating citation and semantic information, the heterogeneity of citations is often overlooked. To address this problem, this study proposes a citation importance-aware contrastive learning framework that refines the supervisory signal. We first develop a scalable measurement of citation importance based on location, frequency, and self-citation characteristics. Citation importance is then integrated into the contrastive learning process through an importance-aware sampling strategy, which selects low-importance citations as hard negatives. This forces the model to learn finer-grained representations that distinguish between important and perfunctory citations. To validate the effectiveness of the proposed framework, we fine-tune a SciBERT model and perform extensive evaluations on SciDocs and PubMed benchmark datasets. Results show consistent improvements in both document representation quality and science mapping accuracy. Furthermore, we apply the trained model to over 33 million documents from Web of Science. The resulting map of science accurately visualizes the global and local intellectual structure of science and reveals interdisciplinary research fronts. By operationalizing citation heterogeneity into a scalable computational framework, this study demonstrates how differentiating citations by their importance can be effectively leveraged to improve document representation and science mapping.</description>
      <author>example@mail.com (Zhentao Liang, Nees Jan van Eck, Xuehua Wu, Jin Mao, Gang Li)</author>
      <guid isPermaLink="false">2512.13054v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>BLADE: A Behavior-Level Data Augmentation Framework with Dual Fusion Modeling for Multi-Behavior Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2512.12964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BLADE的多行为序列推荐框架，通过双重项目-行为融合架构和行为级数据增强方法解决用户行为异质性和数据稀疏性问题，在三个真实数据集上证明了其有效性。&lt;h4&gt;背景&lt;/h4&gt;多行为序列推荐旨在通过建模用户随时间变化的多种类型交互来捕捉用户的动态兴趣，尽管已有研究探索此领域，但推荐性能仍不理想。&lt;h4&gt;目的&lt;/h4&gt;解决多行为序列推荐中的两个基本挑战：用户行为的异质性和数据稀疏性，从而提升推荐性能。&lt;h4&gt;方法&lt;/h4&gt;提出BLADE框架，包含双重项目-行为融合架构（在输入和中间层面融入行为信息）和行为级数据增强方法（直接在行为序列上操作，生成多样化增强视图并保持语义一致性），通过对比学习增强表示学习和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实数据集上的实验证明了BLADE框架的有效性，能够提升多行为序列推荐性能。&lt;h4&gt;结论&lt;/h4&gt;BLADE框架通过解决用户行为异质性和数据稀疏性问题，有效提升了多行为序列推荐的性能。&lt;h4&gt;翻译&lt;/h4&gt;多行为序列推荐旨在通过建模用户随时间变化的多种类型交互来捕捉用户的动态兴趣。尽管已有一些研究探索了这个领域，但推荐性能仍然不理想，主要由于两个基本挑战：用户行为的异质性和数据稀疏性。为解决这些挑战，我们提出了BLADE框架，该框架增强多行为建模同时减轻数据稀疏性。具体而言，为处理行为异质性，我们引入了双重项目-行为融合架构，在输入和中间层面都融入行为信息，从而能够从多个角度进行偏好建模。为减轻数据稀疏性，我们设计了三种行为级数据增强方法，这些方法直接在行为序列上操作，而不是在核心项目序列上。这些方法生成多样化的增强视图，同时保持项目序列的语义一致性。这些增强视图通过对比学习进一步增强了表示学习和泛化能力。在三个真实数据集上的实验证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-behavior sequential recommendation aims to capture users' dynamic interests by modeling diverse types of user interactions over time. Although several studies have explored this setting, the recommendation performance remains suboptimal, mainly due to two fundamental challenges: the heterogeneity of user behaviors and data sparsity. To address these challenges, we propose BLADE, a framework that enhances multi-behavior modeling while mitigating data sparsity. Specifically, to handle behavior heterogeneity, we introduce a dual item-behavior fusion architecture that incorporates behavior information at both the input and intermediate levels, enabling preference modeling from multiple perspectives. To mitigate data sparsity, we design three behavior-level data augmentation methods that operate directly on behavior sequences rather than core item sequences. These methods generate diverse augmented views while preserving the semantic consistency of item sequences. These augmented views further enhance representation learning and generalization via contrastive learning. Experiments on three real-world datasets demonstrate the effectiveness of our approach.</description>
      <author>example@mail.com (Yupeng Li, Mingyue Cheng, Yucong Luo, Yitong Zhou, Qingyang Mao, Shijin Wang)</author>
      <guid isPermaLink="false">2512.12964v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection</title>
      <link>http://arxiv.org/abs/2512.12906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TCSVT2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于预测样本分配(PSA)的语义连贯分布外检测框架，通过双阈值三元样本分配策略和概念对比表示学习损失，有效解决了现有方法中引入大量噪声样本的问题。&lt;h4&gt;背景&lt;/h4&gt;语义连贯分布外检测(SCOOD)是一种新兴的检测设置，它使用标记的分布内数据和混合的分布内与分布外无标签数据作为训练数据，旨在让模型准确识别测试数据中的分布外样本。现有方法主要采用基于聚类的分布内样本过滤策略，但不可避免地引入大量噪声样本。&lt;h4&gt;目的&lt;/h4&gt;解决现有SCOOD方法中引入大量噪声样本的问题，提高分布内和分布外样本集的纯度，并增强模型在表示空间中区分分布内和分布外样本的能力。&lt;h4&gt;方法&lt;/h4&gt;提出基于预测样本分配(PSA)的SCOOD框架，包括：(1)基于预测能量分数的双阈值三元样本分配策略，将不确定的无标签数据分配到丢弃样本集；(2)概念对比表示学习损失，扩大表示空间中分布内和分布外样本的距离；(3)重训练策略，帮助模型充分拟合所选的辅助样本。&lt;h4&gt;主要发现&lt;/h4&gt;在两个标准SCOOD基准上的实验表明，所提出的方法以显著优势优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;基于预测样本分配的SCOOD框架能有效解决现有方法中的噪声样本问题，提高样本纯度和模型区分能力，从而在分布外检测任务上取得更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;语义连贯的分布外检测(SCOOD)是一种最近提出的现实分布外检测设置：给定标记的分布内数据和混合的分布内与分布外无标签数据作为训练数据，SCOOD旨在使训练好的模型能够准确识别测试数据中的分布外样本。当前的SCOOD方法主要采用各种基于聚类的分布内样本过滤策略从无标签数据中选择干净的分布内样本，并将剩余样本视为辅助分布外数据，这不可避免地在训练中引入了大量噪声样本。为了解决上述问题，我们提出了一种基于预测样本分配(PSA)的简洁SCOOD框架。PSA包括一个基于预测能量分数的双阈值三元样本分配策略，通过将不确定的无标签数据分配到额外的丢弃样本集中，显著提高所选分布内和分布外样本集的纯度，以及一个概念对比表示学习损失，进一步扩大表示空间中分布内和分布外样本之间的距离，辅助分布内/分布外区分。此外，我们还引入了一种重训练策略，帮助模型充分拟合所选的辅助分布内/分布外样本。在两个标准SCOOD基准上的实验表明，我们的方法以显著优势优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.</description>
      <author>example@mail.com (Zhimao Peng, Enguang Wang, Xialei Liu, Ming-Ming Cheng)</author>
      <guid isPermaLink="false">2512.12906v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Learning Common and Salient Generative Factors Between Two Image Datasets</title>
      <link>http://arxiv.org/abs/2512.12800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the author's version of a work submitted to IEEE for possible publication. The final version may differ from this version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种称为对比分析(CA)的新方法，用于分离两个图像数据集之间的共同生成因素和显著因素。该方法可适应GAN和扩散模型，通过新的学习策略和损失函数实现高质量的图像合成。&lt;h4&gt;背景&lt;/h4&gt;图像合成领域的最新进展使高质量图像生成和操作成为可能。大多数研究工作集中在条件操作(基于给定属性修改图像)和解纠缠表示学习(每个潜在方向表示不同语义属性)这两种方法上。&lt;h4&gt;目的&lt;/h4&gt;论文旨在解决一个不同且研究较少的问题：对比分析(CA)。给定两个图像数据集，目标是分离共同生成因素(两个数据集共享)和显著因素(仅属于一个数据集)。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的CA框架，可适应GAN和扩散模型，学习共同和显著因素。通过定义新的、适应性强的学习策略和损失函数，确保共同和显著因素之间的相关分离，同时保持高质量生成。&lt;h4&gt;主要发现&lt;/h4&gt;在多种数据集上(包括人脸、动物图像和医学扫描)的评估表明，该框架与先前方法相比，展示了更好的分离能力和图像质量合成。&lt;h4&gt;结论&lt;/h4&gt;所提出的对比分析方法为图像合成领域提供了一种新的研究思路，能够在不依赖属性作为监督信号的情况下，仅使用数据集信号实现高质量的图像生成和因素分离。&lt;h4&gt;翻译&lt;/h4&gt;图像合成的最新进展已经实现了高质量图像的生成和操作。大多数研究工作集中在：1)条件操作，即根据给定属性修改图像，或2)解纠缠表示学习，其中每个潜在方向应代表不同的语义属性。在本文中，我们关注一个不同且研究较少的问题，称为对比分析(CA)。给定两个图像数据集，我们希望将共同的生成因素(在两个数据集间共享)与显著因素(仅属于一个数据集)分离开来。与使用属性作为编辑监督信号(如眼镜、性别)的现有方法相比，所提出的方法较弱，因为它仅使用数据集信号。我们提出了一个用于CA的新框架，可以适应GAN和扩散模型，学习共同和显著因素。通过定义新的且适应性强的学习策略和损失函数，我们确保了共同和显著因素之间的相关分离，同时保持高质量生成。我们在多种数据集上评估了我们的方法，包括人脸、动物图像和医学扫描。与先前方法相比，我们的框架展示了更好的分离能力和图像质量合成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.</description>
      <author>example@mail.com (Yunlong He, Gwilherm Lesné, Ziqian Liu, Michaël Soumm, Pietro Gori)</author>
      <guid isPermaLink="false">2512.12800v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images</title>
      <link>http://arxiv.org/abs/2512.12662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SSMT-Net是一种创新的半监督多任务Transformer网络，解决了甲状腺结节分割中的关键挑战，通过结合未标记数据和联合优化多个任务，显著提高了分割性能。&lt;h4&gt;背景&lt;/h4&gt;准确的甲状腺结节超声图像分割对诊断和治疗计划至关重要。然而，结节与周围组织之间的模糊边界、尺寸变化以及标注超声数据的稀缺性，给自动化分割带来了重大挑战。现有的深度学习模型难以整合甲状腺腺体的上下文信息，并有效泛化到多样化的案例中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用未标注数据的方法，解决甲状腺结节分割面临的挑战，提高分割的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出了SSMT-Net（半监督多任务Transformer网络），在初始无监督阶段利用未标记数据增强基于Transformer的编码器特征提取能力；在监督阶段，模型联合优化结节分割、腺体分割和结节大小估计，整合局部和全局上下文特征。&lt;h4&gt;主要发现&lt;/h4&gt;在TN3K和DDTI数据集上的广泛评估表明，SSMT-Net优于最先进的方法，具有更高的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;SSMT-Net显示出在真实临床应用中的潜力，能够有效解决甲状腺结节分割中的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;准确的甲状腺结节超声图像分割对诊断和治疗计划至关重要。然而，结节与周围组织之间的模糊边界、尺寸变化以及标注超声数据的稀缺性，给自动化分割带来了重大挑战。现有的深度学习模型难以整合甲状腺腺体的上下文信息，并有效泛化到多样化的案例中。为应对这些挑战，我们提出了SSMT-Net，一种半监督多任务Transformer网络，它在初始无监督阶段利用未标记数据增强基于Transformer的编码器特征提取能力。在监督阶段，模型联合优化结节分割、腺体分割和结节大小估计，整合局部和全局上下文特征。在TN3K和DDTI数据集上的广泛评估表明，SSMT-Net优于最先进的方法，具有更高的准确性和鲁棒性，显示出其在真实临床应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.</description>
      <author>example@mail.com (Muhammad Umar Farooq, Abd Ur Rehman, Azka Rehman, Muhammad Usman, Dong-Kyu Chae, Junaid Qadir)</author>
      <guid isPermaLink="false">2512.12662v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Supervised Contrastive Frame Aggregation for Video Representation Learning</title>
      <link>http://arxiv.org/abs/2512.12549v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种监督对比学习框架，用于视频表示学习，通过视频到图像聚合策略和对比学习目标，有效学习视频表示，在分类任务上取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;视频表示学习是计算机视觉领域的重要任务，现有方法如视频Transformer模型计算复杂度高，需要更多计算资源。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的视频表示学习方法，在保持高分类准确率的同时减少计算资源消耗。&lt;h4&gt;方法&lt;/h4&gt;1. 提出监督对比学习框架，利用时间全局上下文；2. 设计视频到图像聚合策略，将多个视频帧空间排列成单个输入图像；3. 使用预训练的卷积神经网络骨干网络（如ResNet50）；4. 设计对比学习目标，直接比较模型生成的成对投影；5. 定义正样本为来自相同标签视频的投影，其他为负样本；6. 通过不同时间帧采样创建同一视频的多个自然视图。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在Penn Action数据集上达到76%的分类准确率，优于ViVIT的43%；2. 在HMDB51数据集上达到48%的准确率，优于ViVIT的37%；3. 该方法在计算资源消耗上少于现有方法；4. 帧级变化产生具有全局上下文的多样化正样本，减少过拟合。&lt;h4&gt;结论&lt;/h4&gt;提出的监督对比帧聚合方法在监督和自监督设置中都能学习有效的视频表示，支持视频分类和字幕等任务，且在准确率和计算效率上均优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于视频表示学习的监督对比学习框架，它利用时间全局上下文。我们引入了一种视频到图像聚合策略，将每个视频的多个帧在空间上排列成单个输入图像。这种设计使得可以使用预训练的卷积神经网络骨干网络（如ResNet50），并避免了复杂视频Transformer模型的计算开销。然后，我们设计了一个对比学习目标，直接比较模型生成的成对投影。正样本定义为来自共享相同标签的视频的投影，而所有其他投影被视为负样本。使用同一底层视频的不同时间帧采样创建同一视频的多个自然视图。这些帧级变化产生具有全局上下文的多样化正样本，减少过拟合，而非依赖数据增强。在Penn Action和HMDB51数据集上的实验表明，所提出的方法在分类准确率上优于现有方法，同时需要更少的计算资源。提出的监督对比帧聚合方法在监督和自监督设置中都能学习有效的视频表示，并支持视频分类和字幕等任务。该方法在Penn Action上实现了76%的分类准确率，而ViVIT实现了43%；在HMDB51上实现了48%的准确率，而ViVIT实现了37%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.</description>
      <author>example@mail.com (Shaif Chowdhury, Mushfika Rahman, Greg Hamerly)</author>
      <guid isPermaLink="false">2512.12549v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields</title>
      <link>http://arxiv.org/abs/2512.12402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepVekua是一种混合架构，统一了几何深度学习和谱分析，用于在稀疏数据条件下求解偏微分方程。&lt;h4&gt;背景&lt;/h4&gt;在稀疏数据条件下求解偏微分方程是一个挑战性问题，传统方法难以有效处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够统一几何深度学习和谱分析的方法，以解决稀疏数据条件下的偏微分方程问题。&lt;h4&gt;方法&lt;/h4&gt;学习一种微分同胚坐标变换，将复杂几何映射到潜在调和空间，将几何学习与物理学习分离，并以封闭形式求解最优谱权重。&lt;h4&gt;主要发现&lt;/h4&gt;在平流-扩散系统中，该方法比最先进的隐式表示表现更好，比基线谱方法提高100倍性能。&lt;h4&gt;结论&lt;/h4&gt;DeepVekua为解决稀疏数据条件下的偏微分方程问题提供了一种有效方法，克服了标准基于坐标网络的谱偏差问题。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了DeepVekua，这是一种混合架构，统一了几何深度学习和谱分析，用于在稀疏数据条件下求解偏微分方程。通过学习一种微分同胚坐标变换，将复杂几何映射到潜在调和空间，我们的方法在平流-扩散系统中优于最先进的隐式表示。与难以克服谱偏差的标准基于坐标的网络不同，DeepVekua将几何学习与物理学习分离，并以封闭形式求解最优谱权重。我们展示了比基线谱方法提高100倍的性能。代码可在https://github.com/VladimerKhasia/vekuanet获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从稀疏数据中重建连续物理场的问题，特别是那些由偏微分方程(PDEs)控制的物理场。这个问题在计算物理、医学成像和计算机图形学等多个领域都很重要，因为现有方法要么难以处理高频细节(标准神经网络存在频谱偏差)，要么在复杂几何形状上表现不佳(经典频谱方法受吉布斯现象影响)，要么需要密集数据(现有混合方法)。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从分析现有方法的局限性入手：标准多层感知机存在频谱偏差问题，经典频谱方法在复杂几何形状上失败，混合方法要么需要密集数据要么牺牲物理可解释性。受I.N. Vekua的广义解析函数变形原理启发，作者提出'复杂物理场可能是扭曲的调和函数'这一观点。方法设计上借鉴了坐标神经网络、物理信息神经网络(PINNs)、傅里叶神经算子和多分辨率网格等现有工作，但通过学习微分同胚坐标变换将复杂几何映射到潜在调和空间，实现了创新性结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将近似分解为两个部分：可学习的微分同胚坐标变换(几何)和闭式频谱基投影(物理)。不同于直接近似函数，DeepVekua近似的是函数变为线性的坐标系。整体流程是：输入坐标通过多个残差块处理，每个块首先通过神经网络学习坐标变形，然后将坐标嵌入复平面，构造径向调制傅里叶基函数，最后通过可微最小二乘法解析求解最优频谱权重。各层预测结果累加得到最终输出，形成双层优化结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合架构统一几何深度学习与频谱分析；2)学习微分同胚坐标变换将复杂几何映射到潜在调和空间；3)提出数值稳定的径向调制傅里叶基，结合振荡分量和增长趋势；4)在正向传播中以闭式形式求解最优基权重。相比SIREN等标准坐标网络，DeepVekua不直接近似函数而是近似坐标系，解决了频谱偏差问题；相比经典频谱方法，能处理复杂几何避免吉布斯现象；相比现有混合方法，不需要密集数据且保持物理可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DeepVekua通过结合几何深度学习和频谱分析，提出了一种能够从稀疏数据中高效重建物理场的新方法，特别适用于平流-扩散方程系统，在保持物理可解释性的同时显著提高了重建精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5281/zenodo.17918695&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present DeepVekua, a hybrid architecture that unifies geometric deep learning with spectral analysis to solve partial differential equations (PDEs) in sparse data regimes. By learning a diffeomorphic coordinate transformation that maps complex geometries to a latent harmonic space, our method outperforms state-of-the-art implicit representations on advection-diffusion systems. Unlike standard coordinate-based networks which struggle with spectral bias, DeepVekua separates the learning of geometry from the learning of physics, solving for optimal spectral weights in closed form. We demonstrate a 100x improvement over spectral baselines. The code is available at https://github.com/VladimerKhasia/vekuanet.</description>
      <author>example@mail.com (Vladimer Khasia)</author>
      <guid isPermaLink="false">2512.12402v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Grained Zero-Shot Learning with Attribute-Centric Representations</title>
      <link>http://arxiv.org/abs/2512.12219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AttributeCentric Representations (ACR)的零样本学习框架，通过属性解纠缠解决细粒度类别识别中的视觉差异区分问题，在多个基准数据集上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;识别未见过的细粒度类别需要能够区分细微视觉差异的模型，通常通过将已见类别的视觉属性关系迁移到未见类别来实现。&lt;h4&gt;目的&lt;/h4&gt;解决属性纠缠问题，即传统模型将不同属性（如颜色、形状和纹理）压缩为单个视觉嵌入导致的干扰问题。&lt;h4&gt;方法&lt;/h4&gt;ACR框架包含两个混合专家组件：Mixture of Patch Experts (MoPE)使用双重路由机制将图像块分派给专门专家；Mixture of Attribute Experts (MoAE)将专家细化的特征投影到稀疏的、面向部分的属性图。&lt;h4&gt;主要发现&lt;/h4&gt;传统方法的事后解决方案不足以处理已混合的表示，而ACR在表示学习过程中强制属性解纠缠，有效解决了属性纠缠问题。&lt;h4&gt;结论&lt;/h4&gt;在零样本学习基准数据集CUB、AwA2和SUN上，ACR框架取得了持续的最先进结果，证明了其在细粒度零样本分类任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;识别未见过的细粒度类别需要一个能够区分细微视觉差异的模型。这通常通过将已见类别的视觉属性关系迁移到未见类别来实现。核心挑战是属性纠缠，传统模型将颜色、形状和纹理等不同属性压缩为单个视觉嵌入。这种干扰掩盖了这些关键区别。先前工作的事后解决方案不足，因为它们已在混合的表示上操作。我们提出一个零样本学习框架，学习以属性为中心的表示（ACR）来解决这个问题，在表示学习过程中强制属性解纠缠。ACR通过两个混合专家组件实现，包括Mixture of Patch Experts (MoPE)和Mixture of Attribute Experts (MoAE)。首先，MoPE使用双重路由机制插入到transformer中，有条件地将图像块分派给专门的专家。这确保了连贯的属性族由专门的专家处理。最后，MoAE头将这些专家细化的特征投影到稀疏的、面向部分的属性图，以实现鲁棒的零样本分类。在零样本学习基准数据集CUB、AwA2和SUN上，我们的ACR取得了持续的最先进结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.</description>
      <author>example@mail.com (Zhi Chen, Jingcai Guo, Taotao Cai, Yuxiang Cai)</author>
      <guid isPermaLink="false">2512.12219v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title>
      <link>http://arxiv.org/abs/2512.12069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为表示对比评分（RCS）的新型越狱检测框架，通过分析LVLM内部表示的几何结构来区分良性输入和恶意输入，实现了良好的泛化能力和实用性。&lt;h4&gt;背景&lt;/h4&gt;大型视觉-语言模型（LVLMs）容易受到多模态越狱攻击的威胁，现有防御方法要么针对特定攻击模式（限制了泛化能力），要么带来高计算开销。轻量级异常检测方法虽然前景广阔，但其常见的一类设计倾向于将新型良性输入与恶意输入混淆，导致不可靠的过度拒绝。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测越狱攻击的方法，同时保持良好的泛化能力和实用性，解决现有轻量级异常检测方法中良性输入与恶意输入混淆的问题。&lt;h4&gt;方法&lt;/h4&gt;提出表示对比评分（RCS）框架，基于LVLM内部表示中最强安全信号存在的洞察。检查这些表示的内部几何结构，学习轻量级投影在安全关键层中分离良性输入和恶意输入，实现简单而强大的对比分数。具体实现包括MCD（马氏对比检测）和KCD（K最近邻对比检测）。&lt;h4&gt;主要发现&lt;/h4&gt;通过对适当的内部表示应用简单、可解释的统计方法，可以实现有效的越狱检测。RCS框架能够区分真正的恶意攻击与新型良性输入，MCD和KCD在针对未见过的攻击类型的泛化能力上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;有效且实用的越狱检测可以通过对适当的内部表示应用简单、可解释的统计方法实现，为更安全的LVLM部署提供了实用路径。相关代码已在Github上公开。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉-语言模型（LVLMs）容易受到越来越多的多模态越狱攻击的威胁，需要既能推广到新型威胁又适合实际部署的防御方法。许多当前策略要么针对特定攻击模式（这限制了泛化能力），要么带来高计算开销。虽然轻量级异常检测方法提供了一个有前景的方向，但我们发现其常见的一类设计倾向于将新型良性输入与恶意输入混淆，导致不可靠的过度拒绝。为此，我们提出了表示对比评分（RCS），这是一个基于关键洞察构建的框架：最强的安全信号存在于LVLM自身的内部表示中。我们的方法检查这些表示的内部几何结构，学习一个轻量级投影，在安全关键层中最大程度地分离良性输入和恶意输入。这使得一个简单而强大的对比分数能够区分真正的恶意意图与仅仅是新颖性。我们的实现MCD（马氏对比检测）和KCD（K最近邻对比检测）在一个具有挑战性的评估协议上实现了最先进的性能，该协议旨在测试对未见攻击类型的泛化能力。这项工作证明，通过对适当的内部表示应用简单、可解释的统计方法，可以实现有效的越狱检测，为更安全的LVLM部署提供了实用路径。我们的代码可在Github上获取：https://github.com/sarendis56/Jailbreak_Detection_RCS。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.</description>
      <author>example@mail.com (Peichun Hua, Hao Li, Shanghao Shi, Zhiyuan Yu, Ning Zhang)</author>
      <guid isPermaLink="false">2512.12069v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities</title>
      <link>http://arxiv.org/abs/2512.11901v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  WACV; Supplementary material is available on CVF proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CLARGA是一种通用的多模态融合架构，用于多模态表示学习，可处理任意数量和类型的模态，无需改变底层框架。&lt;h4&gt;背景&lt;/h4&gt;需要一种能够灵活处理多种模态数据的通用框架，以适应各种机器学习任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自适应融合不同模态表示的架构，提高跨模态一致性和对噪声输入的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过构建特征上的注意力加权图并使用多头图注意力网络传递消息，为每个样本学习模态间的相互影响；使用可学习掩码适应缺失模态；结合监督任务损失和对比InfoNCE损失进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;CLARGA在7个跨金融、人机交互、多媒体分类和情感计算的数据集上持续优于基线和最先进模型；展示了对缺失输入的鲁棒性和在特定任务上的出色能力。&lt;h4&gt;结论&lt;/h4&gt;CLARGA可以轻松集成到各种机器学习模型中，有效且高效地学习跨任务的表示。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了CLARGA，一种用于多模态表示学习的通用多模态融合架构，它可以在不改变底层框架的情况下处理任意数量和类型的模态。给定监督数据集，CLARGA可以应用于几乎任何机器学习任务，以融合不同的多模态表示供下游层处理。在样本基础上，CLARGA通过在特征上构建注意力加权图并使用多头图注意力网络沿此图传递消息，学习模态之间应如何相互影响。这不仅使CLARGA高度自适应（因为它为不同样本构建独特图），而且随着模态数量增长，它实现了具有亚二次复杂度的高效融合。通过可学习掩码，它还可以适应缺失的模态输入。该模型使用混合目标函数进行训练，结合了监督任务损失和对比InfoNCE损失，提高了跨模态一致性和对噪声输入的鲁棒性。我们在7个跨金融、人机交互、通用多媒体分类和情感计算的数据集上展示了CLARGA在多样化多模态表示学习任务中的有效性。它持续优于基线、最先进的模型和消融实验。额外的实验还证明了其对缺失输入的鲁棒性以及在特定任务上的出色能力。总体而言，CLARGA可以轻松插入机器学习模型中，以有效且高效地学习各种任务的表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.</description>
      <author>example@mail.com (Santosh Patapati)</author>
      <guid isPermaLink="false">2512.11901v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency</title>
      <link>http://arxiv.org/abs/2512.13665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Grab-3D的几何感知transformer框架，用于检测AI生成的视频，通过分析3D几何时间一致性来区分真实视频与AI生成视频。&lt;h4&gt;背景&lt;/h4&gt;基于扩散模型的生成技术使AI能够创建高度逼真的视频，这使得可靠的检测机制变得更为重要，但现有检测方法对生成视频中存在的3D几何模式探索有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于3D几何时间一致性来检测AI生成视频的方法，揭示真实视频与AI生成视频在几何一致性方面的根本差异。&lt;h4&gt;方法&lt;/h4&gt;使用消失点作为3D几何模式的显式表示；构建静态场景的AI生成视频数据集以实现可靠的3D几何特征提取；提出具有几何位置编码、时间-几何注意力和基于EMA的几何分类器头的几何感知transformer框架。&lt;h4&gt;主要发现&lt;/h4&gt;真实视频与AI生成视频在几何一致性方面存在根本差异；Grab-3D在检测AI生成视频方面显著优于现有最先进的检测器。&lt;h4&gt;结论&lt;/h4&gt;Grab-3D能够实现强大的跨领域泛化能力，对未见过的生成器也有效，为AI生成视频检测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近的扩散生成技术进展使AI模型能够生成高度逼真的视频，增加了对可靠检测机制的需求。然而，现有的检测方法对生成视频中存在的3D几何模式仅提供了有限的探索。在本文中，我们使用消失点作为3D几何模式的显式表示，揭示了真实视频与AI生成视频在几何一致性方面的基本差异。我们引入了Grab-3D，一种基于3D几何时间一致性的几何感知transformer框架，用于检测AI生成的视频。为了实现可靠的评估，我们构建了一个静态场景的AI生成视频数据集，允许稳定的3D几何特征提取。我们提出了一种几何感知transformer，配备了几何位置编码、时间-几何注意力和基于EMA的几何分类器头，将3D几何感知显式注入到时间建模中。实验证明，Grab-3D显著优于最先进的检测器，实现了对未见过的生成器的强大跨领域泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何检测AI生成的视频问题。随着AI视频生成技术快速发展，生成的视频越来越逼真，难以与真实视频区分，这对媒体信息的真实性和安全性构成重大挑战。现有检测方法主要依赖纹理、伪影或RGB运动模式，但这些特征在不同生成器和内容变化下不够鲁棒，且缺乏对3D几何一致性的建模，而3D几何一致性是真实视频中的基本物理特性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到真实视频中3D几何结构随时间保持物理一致性，而AI生成视频则表现出这种一致性的偏差。他们选择消失点作为3D几何的显式表示，发现真实视频中消失点运动稳定，而AI生成视频则波动明显。为避免物体运动干扰，他们构建了仅含相机运动的静态场景数据集。方法借鉴了3D感知生成模型(如ViewDiff、CamCo)的几何一致性思想，以及现有视频检测方法(如DeMamba)的transformer架构，但创新性地将3D几何感知注入时间建模。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D几何时间一致性区分真实和AI生成视频。真实视频中场景几何结构随时间保持物理一致性，而AI生成视频无法保持这种一致性，特别是在消失点运动轨迹上表现不稳定。整体流程包括：1)提取消失点特征并构建3D几何表示；2)构建静态场景视频数据集；3)设计几何感知transformer框架(GRAB-3D)，包含几何位置编码、时间-几何注意力和几何分类器头；4)预训练几何头并优化分类器；5)在静态场景数据集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统研究AI生成视频中的3D几何时间一致性；2)构建首个静态场景AI生成视频数据集(3,322对真实-生成视频)；3)提出GRAB-3D框架，包含几何位置编码、时间-几何注意力和几何分类器头。相比之前工作，不同之处在于：专注于3D几何一致性而非纹理或运动特征；显式将3D几何感知注入时间建模；使用完全静态场景确保可靠特征提取；在跨域检测(未见生成器)上表现出色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过利用3D几何时间一致性和创新的几何感知transformer框架，显著提升了AI生成视频检测的准确性和跨域泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.</description>
      <author>example@mail.com (Wenhan Chen, Sezer Karaoglu, Theo Gevers)</author>
      <guid isPermaLink="false">2512.13665v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding</title>
      <link>http://arxiv.org/abs/2512.13511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 Pages. Project page at http://bpiyush.github.io/tara-website&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了TARA（时间感知检索适应）方法，构建了一个通用的、具有时间感知能力的视频-文本嵌入模型，用于检索任务，无需使用任何视频数据即可将多模态大语言模型适应为时间感知模型。&lt;h4&gt;背景&lt;/h4&gt;现有视频-文本嵌入模型可能缺乏对时间信息的感知能力，需要一种方法来增强模型的时间感知特性。&lt;h4&gt;目的&lt;/h4&gt;构建一个通用的、具有时间感知能力的视频-文本嵌入模型，用于检索任务，并且不需要使用任何视频数据。&lt;h4&gt;方法&lt;/h4&gt;提出TARA方法，将多模态大语言模型（MLLMs）适应为时间感知的视频-文本嵌入模型；同时提出新的基准测试，使用时间相反（手性）动作作为困难负样本，并包含手性和非手性动作的精心划分。&lt;h4&gt;主要发现&lt;/h4&gt;TARA在提出的手性基准上优于所有现有视频-文本模型；在标准基准上也取得良好结果；TARA还具有否定感知能力，在视频中的动词和副词理解方面取得最先进性能。&lt;h4&gt;结论&lt;/h4&gt;TARA产生了一个强大、多功能、具有时间感知能力的视频-文本嵌入模型，具有最先进的零样本性能。&lt;h4&gt;翻译&lt;/h4&gt;我们的目标是构建一个用于检索的通用时间感知视频-文本嵌入模型。为此，我们提出了一种简单高效的方案，称为TARA（时间感知检索适应），无需使用任何视频数据即可将多模态大语言模型适应为时间感知的视频-文本嵌入模型。为了评估检索中的时间感知能力，我们提出了一个新的基准，使用时间相反的手性动作作为困难负样本，并包含手性和非手性动作的精心划分。我们证明TARA在该手性基准上优于所有现有的视频-文本模型，同时在标准基准上也取得了良好结果。此外，我们发现了TARA在时间感知能力之外的额外优势：TARA嵌入具有否定感知能力，在评估视频检索中否定的NegBench基准中所示，TARA在视频中的动词和副词理解方面取得了最先进的性能。总体而言，TARA产生了一个强大、多功能、具有时间感知能力的视频-文本嵌入模型，具有最先进的零样本性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.</description>
      <author>example@mail.com (Piyush Bagad, Andrew Zisserman)</author>
      <guid isPermaLink="false">2512.13511v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition</title>
      <link>http://arxiv.org/abs/2512.13415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一时空建模(USTM)框架，用于连续手语识别，通过结合增强型Swin Transformer主干网络和轻量级时间适配器与位置编码(TAPE)，有效捕捉细粒度空间特征和短期长期时间上下文，在多个基准数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;连续手语识别需要精确的时空建模来准确识别视频中的手势序列，而现有框架通常基于CNN的空间主干网络结合时间卷积或循环模块，这些技术在捕捉细粒度手部和面部线索以及建模长程时间依赖性方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有技术在捕捉细粒度特征和建模长程时间依赖性方面的局限性，提出一种能够有效建模复杂模式的统一时空建模框架。&lt;h4&gt;方法&lt;/h4&gt;提出USTM框架，一种时空编码器，结合增强型Swin Transformer主干网络和轻量级时间适配器与位置编码(TAPE)，能够捕捉细粒度空间特征以及短期和长期时间上下文，无需依赖多流输入或辅助模态，从RGB视频中实现鲁棒的手语识别。&lt;h4&gt;主要发现&lt;/h4&gt;在PHOENIX14、PHOENIX14T和CSL-Daily等基准数据集上的实验表明，USTM在基于RGB的方法以及多模态CSLR方法上达到了最先进的性能，同时与多流方法相比保持了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;USTM框架在连续手语识别方面展示了其优势和有效性，代码已在GitHub上公开：https://github.com/gufranSabri/USTM&lt;h4&gt;翻译&lt;/h4&gt;连续手语识别(CSLR)需要精确的时空建模来准确识别视频中的手势序列。现有框架通常依赖于基于CNN的空间主干网络，结合时间卷积或循环模块。这些技术在捕捉细粒度的手部和面部线索以及建模长程时间依赖性方面存在不足。为解决这些限制，我们提出了统一时空建模(USTM)框架，一种时空编码器，通过结合增强型Swin Transformer主干网络和轻量级时间适配器与位置编码(TAPE)，有效建模复杂模式。我们的框架捕捉细粒度空间特征以及短期和长期时间上下文，使系统能够仅从RGB视频中实现鲁棒的手语识别，而无需依赖多流输入或辅助模态。在PHOENIX14、PHOENIX14T和CSL-Daily等基准数据集上的大量实验表明，USTM在基于RGB以及多模态CSLR方法上达到了最先进的性能，同时与多流方法相比保持了具有竞争力的性能。这些结果突显了USTM框架在CSLR方面的优势和有效性。代码可在https://github.com/gufranSabri/USTM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM</description>
      <author>example@mail.com (Ahmed Abul Hasanaath, Hamzah Luqman)</author>
      <guid isPermaLink="false">2512.13415v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>What Happens Next? Next Scene Prediction with a Unified Video Model</title>
      <link>http://arxiv.org/abs/2512.13015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文引入了Next Scene Prediction (NSP)任务，提出了一种结合Qwen-VL和LTX的统一框架，通过三阶段训练方法在NSP数据集上实现了最先进的性能，提高了多模态系统对未来场景的预测能力。&lt;h4&gt;背景&lt;/h4&gt;最近的统一理解和生成模型显著提升了视觉生成能力，但这些模型主要关注传统任务如文本到视频生成，导致统一模型的时间推理潜力在很大程度上未被探索。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，作者引入了NSP任务，推动统一视频模型进行时间和因果推理，要求模型从先前上下文预测合理的未来场景。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一个统一框架，结合Qwen-VL进行理解和LTX进行合成，通过潜查询嵌入和连接器模块桥接。模型在三个阶段训练：文本到视频预训练、监督微调和强化学习（通过GRPO）以及提出的因果一致性奖励。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，该模型在基准测试上达到了最先进的性能，显著提高了通用多模态系统预测接下来会发生什么的能力。&lt;h4&gt;结论&lt;/h4&gt;通过NSP任务和提出的统一框架，作者展示了统一模型在时间和因果推理方面的潜力，为多模态系统预测未来场景提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;最近的统一理解和生成模型显著提升了视觉生成能力。然而，这些模型对传统任务（如文本到视频生成）的关注，使得统一模型的时间推理潜力在很大程度上未被探索。为了填补这一空白，我们引入了Next Scene Prediction (NSP)，这是一个新任务，推动统一视频模型进行时间和因果推理。与文本到视频生成不同，NSP要求从先前上下文预测合理的未来，需要更深层次的理解和推理。为了解决这一任务，我们提出了一个统一框架，结合Qwen-VL进行理解和LTX进行合成，通过潜查询嵌入和连接器模块桥接。该模型在我们新收集的大规模NSP数据集上分三个阶段训练：文本到视频预训练、监督微调和强化学习（通过GRPO）以及我们提出的因果一致性奖励。实验表明，我们的模型在基准测试上取得了最先进的性能，提高了通用多模态系统预测接下来会发生什么的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.</description>
      <author>example@mail.com (Xinjie Li, Zhimin Chen, Rui Zhao, Florian Schiffers, Zhenyu Liao, Vimal Bhat)</author>
      <guid isPermaLink="false">2512.13015v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion</title>
      <link>http://arxiv.org/abs/2512.12935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at AAAI Workshop 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的多模态时刻检索系统，通过三个创新解决了现有方法的三大挑战：固定权重融合策略的局限性、时间建模难以捕捉连贯事件序列以及需要手动模态选择的问题。&lt;h4&gt;背景&lt;/h4&gt;视频内容的指数级增长对高效的多模态时刻检索系统提出了迫切需求，现有方法难以满足这一需求。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理跨模态噪声、模糊查询，并能捕捉连贯事件序列且无需手动模态选择的多模态时刻检索系统。&lt;h4&gt;方法&lt;/h4&gt;提出三个关键创新：1) 级联双重嵌入流水线（结合BEIT-3、SigLIP和BLIP-2）；2) 时间感知评分机制（通过束搜索应用指数衰减惩罚）；3) Agent引导的查询分解（使用GPT-4o自动解释和分解模糊查询）。&lt;h4&gt;主要发现&lt;/h4&gt;定性分析表明，该系统能够有效处理模糊查询，检索时间连贯的序列，并动态适应融合策略，显著提升了交互式时刻搜索能力。&lt;h4&gt;结论&lt;/h4&gt;该统一多模态时刻检索系统通过创新方法解决了现有技术的关键挑战，推进了交互式时刻搜索领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;视频内容的指数级增长对高效的多模态时刻检索系统提出了迫切需求。然而，现有方法面临三个关键挑战：(1)固定权重融合策略无法处理跨模态噪声和模糊查询，(2)时间建模难以捕捉连贯的事件序列，同时对不现实的间隔进行惩罚，(3)系统需要手动选择模态，降低了可用性。我们提出了一种统一的多模态时刻检索系统，具有三个关键创新。首先，级联双重嵌入流水线结合BEIT-3和SigLIP进行广泛检索，通过基于BLIP-2的重排序来平衡召回率和精确率。其次，时间感知评分机制通过束搜索对大的时间间隔应用指数衰减惩罚，构建连贯的事件序列而非孤立帧。第三，Agent引导的查询分解（GPT-4o）自动解释模糊查询，将其分解为特定模态的子查询（视觉/OCR/ASR），并进行自适应分数融合，消除了手动模态选择。定性分析表明，我们的系统能够有效处理模糊查询，检索时间连贯的序列，并动态适应融合策略，推进了交互式时刻搜索能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.</description>
      <author>example@mail.com (Toan Le Ngo Thanh, Phat Ha Huu, Tan Nguyen Dang Duy, Thong Nguyen Le Minh, Anh Nguyen Nhu Tinh)</author>
      <guid isPermaLink="false">2512.12935v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation</title>
      <link>http://arxiv.org/abs/2512.12929v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MADTempo是一个统一了时间搜索和网络规模视觉基础的视频检索框架，通过事件级连续性建模和外部网络图像扩展，解决了现有方法在时间依赖建模和罕见概念处理方面的不足。&lt;h4&gt;背景&lt;/h4&gt;在线平台上视频内容的快速扩展加速了对检索系统的需求，这些系统不仅要理解独立的视觉时刻，还要理解复杂事件的时间结构。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在建模多个事件间时间依赖关系和处理引用未见或罕见视觉概念的查询方面的不足。&lt;h4&gt;方法&lt;/h4&gt;1) 时间搜索机制：通过聚合连续视频片段的相似度分数捕捉事件级连续性，实现多事件查询的连贯检索；2) 基于Google图片搜索的回退模块：使用外部网络图像扩展查询表示，提高对分布外查询的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;这两个组件共同提升了现代视频检索系统的时间推理和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这种方法为大规模视频语料库中更语义感知和自适应的检索铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;在线平台上视频内容的快速扩展加速了对检索系统的需求，这些系统不仅要理解独立的视觉时刻，还要理解复杂事件的时间结构。现有方法通常难以建模多个事件间的时间依赖关系，以及处理引用未见或罕见视觉概念的查询。为应对这些挑战，我们引入了MADTempo，一个由AIO_Trinh团队开发的视频检索框架，它统一了时间搜索与网络规模视觉基础。我们的时间搜索机制通过聚合连续视频片段的相似度分数来捕捉事件级连续性，实现多事件查询的连贯检索。互补地，基于Google图片搜索的回退模块使用外部网络图像扩展查询表示，有效弥合预训练视觉嵌入的差距，并提高对分布外查询的鲁棒性。这些组件共同提升了现代视频检索系统的时间推理和泛化能力，为大规模视频语料库中更语义感知和自适应的检索铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.</description>
      <author>example@mail.com (Huu-An Vu, Van-Khanh Mai, Trong-Tam Nguyen, Quang-Duc Dam, Tien-Huy Nguyen, Thanh-Huong Le)</author>
      <guid isPermaLink="false">2512.12929v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding</title>
      <link>http://arxiv.org/abs/2512.12560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于token剪枝的方法来解决多模态大语言模型应用于在线视频理解时的计算效率问题，通过减少上下文长度同时保留关键信息，显著提高了准确性且剪枝延迟极低。&lt;h4&gt;背景&lt;/h4&gt;在线视频理解对于公共监控和AI眼镜等应用至关重要，但将多模态大语言模型应用于此领域面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决视频帧数量庞大导致的GPU内存使用量高和计算延迟问题，减少上下文长度同时保留关键信息。&lt;h4&gt;方法&lt;/h4&gt;提出token剪枝方法，引入MSSAVT冗余度量指标，设计掩码剪枝策略确保只修剪互不相邻的标记，并整合现有时间冗余剪枝方法消除视频模态的时间冗余。&lt;h4&gt;主要发现&lt;/h4&gt;在多个在线和离线视频理解基准测试中，该方法显著提高了准确性（最多提高4%）且剪枝延迟可忽略不计（小于1毫秒）。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了多模态大语言模型应用于视频理解领域的计算效率挑战，完整实现将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;在线视频理解对于公共监控和AI眼镜等应用至关重要。然而，将多模态大语言模型应用于此领域具有挑战性，因为视频帧数量庞大，导致GPU内存使用量高和计算延迟。为解决这些挑战，我们提出token剪枝作为减少上下文长度同时保留关键信息的方法。具体来说，我们引入了一种新的冗余度量指标——与空间相邻视频标记的最大相似性（MSSAVT），该指标考虑了标记相似性和空间位置。为了缓解剪枝与冗余之间的双向依赖关系，我们进一步设计了一种掩码剪枝策略，确保只修剪互不相邻的标记。我们还整合了一个现有的基于时间冗余的剪枝方法，以消除视频模态的时间冗余。在多个在线和离线视频理解基准测试上的实验结果表明，我们的方法显著提高了准确性（最多提高4%），同时产生可忽略不计的剪枝延迟（小于1毫秒）。我们的完整实现将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.</description>
      <author>example@mail.com (Xinqi Jin, Hanxun Yu, Bohan Yu, Kebin Liu, Jian Liu, Keda Tao, Yixuan Pei, Huan Wang, Fan Dang, Jiangchuan Liu, Weiqiang Wang)</author>
      <guid isPermaLink="false">2512.12560v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding</title>
      <link>http://arxiv.org/abs/2512.12360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了VideoARM，一种用于长视频理解的基于智能体推理-分层记忆的范式。该系统通过自适应、实时的智能体推理和记忆构建，克服了现有方法依赖手工推理流程和消耗大量令牌预处理的问题。VideoARM通过观察-思考-行动-记忆的循环，结合分层多模态记忆，显著提高了长视频理解的效率和性能。&lt;h4&gt;背景&lt;/h4&gt;长视频理解因其扩展的时间结构和密集的多模态线索而具有挑战性。尽管最近有所进展，但许多现有方法仍依赖手工制作的推理流程或消耗大量令牌的视频预处理来引导多模态大型语言模型(MLLMs)进行自主推理。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一种更高效的长视频理解方法，减少令牌消耗并提高性能。&lt;h4&gt;方法&lt;/h4&gt;VideoARM执行自适应、实时的智能体推理和记忆构建，而非静态的详尽预处理。具体实现包括：(1)观察-思考-行动-记忆的自适应连续循环；(2)控制器自主调用工具以粗到细方式解释视频；(3)分层多模态记忆持续捕获和更新多级线索，为决策提供精确上下文。&lt;h4&gt;主要发现&lt;/h4&gt;在主流基准测试上，VideoARM优于最先进的方法DVD，同时显著减少了长视频的令牌消耗，实现了更高的效率和性能。&lt;h4&gt;结论&lt;/h4&gt;VideoARM通过自适应智能体推理和分层记忆构建，有效解决了长视频理解中的挑战，在性能和效率方面均优于现有方法，为长视频理解提供了新的有效范式。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解由于其扩展的时间结构和密集的多模态线索仍然具有挑战性。尽管最近有所进展，但许多现有方法仍然依赖手工制作的推理流程或采用消耗大量令牌的视频预处理来引导多模态大型语言模型进行自主推理。为了克服这些局限性，我们提出了VideoARM，一种用于长视频理解的基于智能体推理-分层记忆的范式。VideoARM执行自适应、实时的智能体推理和记忆构建，而不是静态的、详尽的预处理。具体来说，VideoARM执行观察、思考、行动和记忆的自适应连续循环，其中控制器自主调用工具以粗到细的方式解释视频，从而显著减少令牌消耗。同时，分层多模态记忆在整个智能体操作过程中持续捕获和更新多级线索，为控制器提供精确的上下文信息以支持决策。在主流基准测试上的实验表明，VideoARM优于最先进的方法DVD，同时显著减少了长视频的令牌消耗。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.</description>
      <author>example@mail.com (Yufei Yin, Qianke Meng, Minghao Chen, Jiajun Ding, Zhenwei Shao, Zhou Yu)</author>
      <guid isPermaLink="false">2512.12360v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>CORE: Contrastive Masked Feature Reconstruction on Graphs</title>
      <link>http://arxiv.org/abs/2512.13235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了图自监督学习中生成式和对比式方法的关系，提出了一种新的CORE框架，整合了掩码特征重构和对比学习，在节点和图分类任务上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;图自监督学习领域中，生成式和对比式方法是两种主导方法。掩码特征重构(MFR)是一种生成式技术，模型通过自监督方式学习恢复被掩码节点的原始特征。&lt;h4&gt;目的&lt;/h4&gt;基于MFR和图对比学习(GCL)都旨在最大化相似元素之间一致性的观察，探索这两种方法的整合，以增强图自监督学习性能。&lt;h4&gt;方法&lt;/h4&gt;提出对比掩码特征重构(CORE)框架，将对比学习整合到MFR中。仅在掩码节点的原始特征和重构特征之间形成正样本对，鼓励编码器优先考虑上下文信息；同时利用掩码节点本身作为负样本，结合MFR的重构能力和GCL的判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;在特定条件下，MFR和节点级GCL的目标会收敛，表明这些方法互补而非根本不同；CORE框架在节点分类和图分类任务上显著优于MFR，达到最先进结果，超越GraphMAE和GraphMAE2分别高达2.80%和3.72%(节点分类)，以及3.82%和3.76%(图分类)。&lt;h4&gt;结论&lt;/h4&gt;CORE框架通过整合对比学习和掩码特征重构，有效结合了两种方法的优点，在图自监督学习任务中取得了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;在图自监督学习这一快速发展的领域中，生成式和对比式方法已成为两种主导方法。我们的研究专注于掩码特征重构(MFR)，这是一种生成式技术，模型通过自监督方式学习恢复被掩码节点的原始特征。我们观察到MFR和图对比学习(GCL)都旨在最大化相似元素之间的一致性。基于这一观察，我们揭示了一个新的理论见解：在特定条件下，尽管操作机制不同，MFR和节点级GCL的目标会收敛。这一理论连接表明这些方法互补而非根本不同，促使我们探索它们的整合以增强图自监督学习。我们的研究提出了对比掩码特征重构(CORE)，这是一种新的图自监督学习框架，将对比学习整合到MFR中。具体来说，我们仅在掩码节点的原始特征和重构特征之间形成正样本对，鼓励编码器优先考虑上下文信息而非节点自身的特征。此外，我们利用掩码节点本身作为负样本，结合MFR的重构能力和GCL的判别能力，以更好地捕获图的内在结构。从实验上看，我们提出的CORE框架在节点分类和图分类任务上显著优于MFR，展示了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.</description>
      <author>example@mail.com (Jianyuan Bo, Yuan Fang)</author>
      <guid isPermaLink="false">2512.13235v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis</title>
      <link>http://arxiv.org/abs/2512.13164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  67 pages, 9 figures, 16 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CRAFTS是首个用于病理学特定文本到图像合成的生成式基础模型，通过双阶段训练和新型对齐机制解决数据稀缺问题，提供多样化、高质量病理图像数据源。&lt;h4&gt;背景&lt;/h4&gt;临床级病理学人工智能发展受限于多样化、高质量标注数据集稀缺；生成式模型虽有潜力但存在语义不稳定和形态幻觉问题，影响诊断可靠性。&lt;h4&gt;目的&lt;/h4&gt;解决病理学AI发展中的数据稀缺和隐私问题，创建多样化、标注组织学数据的无限来源，开发罕见和复杂癌症表型的强大诊断工具。&lt;h4&gt;方法&lt;/h4&gt;引入CRAFTS框架，利用约280万对图像-标题对进行双阶段训练，采用新型对齐机制抑制语义漂移确保生物学准确性，并与ControlNet结合实现对组织架构的精确控制。&lt;h4&gt;主要发现&lt;/h4&gt;CRAFTS生成了涵盖30种癌症类型的多样化病理图像，质量通过客观指标和病理学家评估验证；CRAFTS增强的数据集提高了分类、跨模态检索、自监督学习和视觉问答等临床任务性能。&lt;h4&gt;结论&lt;/h4&gt;CRAFTS克服了数据稀缺和隐私问题的关键障碍，为病理学AI发展提供了可靠的数据来源，有效解锁了罕见和复杂癌症表型的诊断工具创建。&lt;h4&gt;翻译&lt;/h4&gt;病理学临床级人工智能的发展受到多样化、高质量标注数据集稀缺的限制。生成式模型提供了潜在的解决方案，但存在语义不稳定和形态幻觉问题，影响诊断可靠性。为了应对这一挑战，我们引入了组织合成的相关性调节对齐框架（CRAFTS），这是首个用于病理学特定文本到图像合成的生成式基础模型。通过利用约280万对图像-标题对进行双阶段训练，CRAFTS采用了一种新型对齐机制，抑制语义漂移以确保生物学准确性。该模型生成了涵盖30种癌症类型的多样化病理图像，其质量通过客观指标和病理学家评估得到严格验证。此外，CRAFTS增强的数据集提高了各种临床任务的性能，包括分类、跨模态检索、自监督学习和视觉问答。此外，将CRAFTS与ControlNet结合，可以根据核分割掩码和荧光图像等输入精确控制组织架构。通过克服数据稀缺和隐私问题的关键障碍，CRAFTS提供了多样化、标注组织学数据的无限来源，有效解锁了罕见和复杂癌症表型的强大诊断工具的创建。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.</description>
      <author>example@mail.com (Xianchao Guan, Zhiyuan Fan, Yifeng Wang, Fuqiang Chen, Yanjiang Zhou, Zengyang Che, Hongxue Meng, Xin Li, Yaowei Wang, Hongpeng Wang, Min Zhang, Heng Tao Shen, Zheng Zhang, Yongbing Zhang)</author>
      <guid isPermaLink="false">2512.13164v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>BUT Systems for WildSpoof Challenge: SASV in the Wild</title>
      <link>http://arxiv.org/abs/2512.12851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了BUT团队在WildSpoof挑战中的提交方案，专注于Spoofing-robust Automatic Speaker Verification (SASV)赛道。&lt;h4&gt;背景&lt;/h4&gt;WildSpoof挑战中的SASV赛道，关注语音欺骗检测和自动说话人验证。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够抵抗欺骗的自动说话人验证系统，提高系统对未知神经声码器和录制环境变化的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出一种SASV框架，集成多种自监督学习前端（从通用音频模型如Dasheng到语音特定编码器如WavLM），使用轻量级多头因子化注意力后端聚合表示，引入基于分布不确定性的特征域增强策略减轻域偏移，并将鲁棒的CM分数与先进ASV系统融合。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在最小化a-DCFs和EERs方面取得了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;SASV框架和特征域增强策略有效提高了系统对欺骗攻击的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了BUT团队在WildSpoof挑战中的提交方案，专注于Spoofing-robust Automatic Speaker Verification (SASV)赛道。我们提出了一种SASV框架，旨在弥合通用音频理解和专业语音分析之间的差距。我们的子系统集成了多种自监督学习前端，从通用音频模型（如Dasheng）到语音特定编码器（如WavLM）。这些表示通过轻量级多头因子化注意力后端进行聚合，以完成相应的子任务。此外，我们引入了一种基于分布不确定性的特征域增强策略，以明确建模并减轻由未知神经声码器和录制环境引起的域偏移。通过将这些鲁棒的CM分数与最先进的ASV系统融合，我们的方法在最小化a-DCFs和EERs方面取得了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents the BUT submission to the WildSpoof Challenge, focusing on the Spoofing-robust Automatic Speaker Verification (SASV) track. We propose a SASV framework designed to bridge the gap between general audio understanding and specialized speech analysis. Our subsystem integrates diverse Self-Supervised Learning front-ends ranging from general audio models (e.g., Dasheng) to speech-specific encoders (e.g., WavLM). These representations are aggregated via a lightweight Multi-Head Factorized Attention back-end for corresponding subtasks. Furthermore, we introduce a feature domain augmentation strategy based on Distribution Uncertainty to explicitly model and mitigate the domain shift caused by unseen neural vocoders and recording environments. By fusing these robust CM scores with state-of-the-art ASV systems, our approach achieves superior minimization of the a-DCFs and EERs.</description>
      <author>example@mail.com (Junyi Peng, Jin Li, Johan Rohdin, Lin Zhang, Miroslav Hlaváček, Oldrich Plchot)</author>
      <guid isPermaLink="false">2512.12851v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment</title>
      <link>http://arxiv.org/abs/2512.12678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出β-CLIP，一种多粒度文本条件对比学习框架，通过实现多层次文本粒度与对应视觉区域之间的层次对齐，显著改善了CLIP在细粒度任务上的表现，达到了最先进的密集对齐效果。&lt;h4&gt;背景&lt;/h4&gt;CLIP模型通过全局视觉和文本表示的对比学习，在零样本图像-文本检索任务中表现出色，但在细粒度任务上表现不佳，即使使用长而详细的标题进行微调也是如此。&lt;h4&gt;目的&lt;/h4&gt;设计一个多粒度文本条件对比学习框架，实现从完整标题到句子和短语的多层次文本粒度与其对应视觉区域之间的层次对齐。&lt;h4&gt;方法&lt;/h4&gt;β-CLIP为每个粒度级别使用交叉注意力动态池化图像块，生成上下文化的视觉嵌入。为解决层次结构中固有的语义重叠问题，引入了β-上下文化对比对齐损失(β-CAL)，该目标参数化了特定查询的严格匹配与松弛的图像内上下文化之间的权衡，支持软交叉熵和硬二元交叉熵两种形式。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验，β-CLIP显著改善了密集对齐：在Urban1K上达到91.8%的T2I和92.3%的I2T@R1，在FG-OVD(Hard)上达到30.9%，在不使用硬负样本训练的方法中达到了最先进水平。&lt;h4&gt;结论&lt;/h4&gt;β-CLIP为密集视觉-语言对应关系建立了强大且自适应的基线。代码和模型已在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;CLIP通过对齐全局视觉和文本表示，在零样本图像-文本检索中取得了强大性能，然而即使在长详细标题上微调后，它在细粒度任务上仍然表现不佳。在这项工作中，我们提出β-CLIP，一种多粒度文本条件对比学习框架，旨在实现从完整标题到句子和短语的多层次文本粒度与其对应视觉区域之间的层次对齐。对于每个粒度级别，β-CLIP使用交叉注意力动态池化图像块，生成上下文化的视觉嵌入。为解决此层次结构中固有的语义重叠问题，我们引入了β-上下文化对比对齐损失(β-CAL)。该目标参数化了特定查询的严格匹配与松弛的图像内上下文化之间的权衡，支持软交叉熵和硬二元交叉熵两种形式。通过大量实验，我们证明β-CLIP显著改善了密集对齐：在Urban1K上达到91.8%的T2I和92.3%的I2T@R1，在FG-OVD(Hard)上达到30.9%，在不使用硬负样本训练的方法中建立了最先进的水平。β-CLIP为密集视觉-语言对应关系建立了强大且自适应的基线。代码和模型已在https://github.com/fzohra/B-CLIP发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.</description>
      <author>example@mail.com (Fatimah Zohra, Chen Zhao, Hani Itani, Bernard Ghanem)</author>
      <guid isPermaLink="false">2512.12678v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems</title>
      <link>http://arxiv.org/abs/2512.12523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under revision&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于奇异值分解和半正交约束训练的神经网络架构，用于在复杂嘈杂的时间序列数据中检测临界转换。&lt;h4&gt;背景&lt;/h4&gt;在复杂、嘈杂的时间序列数据中检测临界转换是科学与工程领域的基础性挑战。临界转换可能通过低维序参量的出现来预测，但其特征通常被高幅度随机变异性所掩盖。&lt;h4&gt;目的&lt;/h4&gt;解决传统对比学习方法参数过多、对噪声敏感的问题，提高临界点识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一种使用奇异值分解技术构建的神经网络架构，结合严格的半正交约束训练算法，以增强传统对比学习的性能。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在识别临界转换方面与传统对比学习技术性能相当，但明显更轻量级，并且对噪声的抗性显著更强。&lt;h4&gt;结论&lt;/h4&gt;通过奇异值分解和半正交约束训练可以有效改进对比学习方法，使其在检测临界转换时更加高效和鲁棒。&lt;h4&gt;翻译&lt;/h4&gt;在复杂、嘈杂的时间序列数据中检测临界转换是科学与工程领域的基础性挑战。此类临界转换可能通过低维序参量的出现来预测，但其特征通常被高幅度随机变异性所掩盖。基于深度神经网络的对比学习方法虽然对检测临界转换有前景，但通常参数过多，对无关噪声敏感，导致临界点识别不准确。为解决这些局限性，我们提出了一种使用奇异值分解技术构建的神经网络架构，结合严格的半正交约束训练算法，以增强传统对比学习的性能。大量实验表明，所提出的方法在识别临界转换方面与传统对比学习技术性能相当，但明显更轻量级，并且对噪声的抗性显著更强。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.</description>
      <author>example@mail.com (Wenqi Fang, Ye Li)</author>
      <guid isPermaLink="false">2512.12523v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2512.12477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了MetaHGNIE，一个基于元路径诱导的超图对比学习框架，用于异构知识图中的节点重要性估计。该方法通过显式建模高阶交互和跨模态对齐，解决了现有方法忽视高阶依赖性和独立处理结构与语义信号的问题。&lt;h4&gt;背景&lt;/h4&gt;异构知识图中的节点重要性估计(NIE)是一个关键且具有挑战性的任务，对于推荐、知识推理和问答等应用至关重要。现有方法通常依赖于成对连接，忽视了多个实体和关系之间的高阶依赖性，并且将结构和语义信号独立处理，阻碍了有效的跨模态集成。&lt;h4&gt;目的&lt;/h4&gt;解决现有NIE方法中忽视高阶依赖性和独立处理结构与语义信号的问题，通过显式建模高阶交互和跨模态对齐来提高异构知识图中节点重要性估计的性能。&lt;h4&gt;方法&lt;/h4&gt;MetaHGNIE是一个元路径诱导的超图对比学习框架，用于解耦和对齐结构信息与语义信息。该方法通过元路径序列构建更高阶的知识图，其中类型化的超边捕获多实体关系上下文。结构依赖性通过局部注意力进行聚合，而语义表示则通过配备稀疏分块的超图transformer进行编码以减少冗余。最后，一个多模态融合模块在对比学习和辅助监督下集成结构和语义嵌入，确保鲁棒的跨模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在基准NIE数据集上的大量实验表明，MetaHGNIE始终优于最先进的基线方法。这些结果突显了在异构知识图中显式建模高阶交互和跨模态对齐的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过显式建模高阶交互和跨模态对齐，MetaHGNIE在异构知识图节点重要性估计任务中取得了显著性能提升，为相关应用提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;异构知识图中的节点重要性估计(NIE)是一个关键且具有挑战性的任务，对于推荐、知识推理和问答等应用至关重要。现有方法通常依赖于成对连接，忽视了多个实体和关系之间的高阶依赖性，并且将结构和语义信号独立处理，阻碍了有效的跨模态集成。为了解决这些挑战，我们提出了MetaHGNIE，一个用于解耦和对齐结构信息与语义信息的元路径诱导超图对比学习框架。MetaHGNIE通过元路径序列构建更高阶的知识图，其中类型化的超边捕获多实体关系上下文。结构依赖性通过局部注意力进行聚合，而语义表示则通过配备稀疏分块的超图transformer进行编码以减少冗余。最后，一个多模态融合模块在对比学习和辅助监督下集成结构和语义嵌入，确保鲁棒的跨模态对齐。在基准NIE数据集上的大量实验表明，MetaHGNIE始终优于最先进的基线方法。这些结果突显了在异构知识图中显式建模高阶交互和跨模态对齐的有效性。我们的代码可在https://github.com/SEU-WENJIA/DualHNIE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE</description>
      <author>example@mail.com (Jiawen Chen, Yanyan He, Qi Shao, Mengli Wei, Duxin Chen, Wenwu Yu, Yanlong Zhao)</author>
      <guid isPermaLink="false">2512.12477v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction</title>
      <link>http://arxiv.org/abs/2512.12445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的知识引导的ViT-based Masked Autoencoder，通过将科学领域知识嵌入自监督重建过程，结合线性光谱混合模型和光谱角映射器作为物理约束，提高了模型的可解释性、泛化能力和数据效率。&lt;h4&gt;背景&lt;/h4&gt;将领域知识整合到深度学习中是提高模型可解释性、泛化能力和数据效率的有前景的方向。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的知识引导的ViT-based Masked Autoencoder，在自监督重建过程中嵌入科学领域知识，使学习到的表示符合观测信号与其潜在成分之间已知的结构关系。&lt;h4&gt;方法&lt;/h4&gt;引入线性光谱混合模型作为物理约束，基于物理的光谱角映射器，确保学习到的表示符合观测信号与其潜在成分之间已知的结构关系。框架联合优化LSMM和SAM损失与传统的Huber损失目标，促进特征空间中的数值精度和几何一致性。&lt;h4&gt;主要发现&lt;/h4&gt;这种知识引导的设计增强了重建保真度，在有限监督下稳定训练，并产生基于物理原理的可解释潜在表示。实验结果表明，所提出的模型显著提高了重建质量并改善了下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;在基于transformer的自监督学习中嵌入物理归纳偏见具有很大潜力，能够有效提升模型性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;将领域知识整合到深度学习中已成为提高模型可解释性、泛化能力和数据效率的一个有前景的方向。在这项工作中，我们提出了一种新颖的知识引导的ViT-based Masked Autoencoder，在自监督重建过程中嵌入科学领域知识。除了依赖数据驱动的优化外，我们提出的方法将线性光谱混合模型作为物理约束和基于物理的光谱角映射器纳入其中，确保学习到的表示符合观测信号与其潜在成分之间已知的结构关系。该框架联合优化LSMM和SAM损失与传统的Huber损失目标，促进特征空间中的数值精度和几何一致性。这种知识引导的设计增强了重建保真度，在有限监督下稳定训练，并产生基于物理原理的可解释潜在表示。实验结果表明，所提出的模型显著提高了重建质量并改善了下游任务性能，强调了在基于transformer的自监督学习中嵌入物理归纳偏见的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.</description>
      <author>example@mail.com (Abdul Matin, Rupasree Dey, Tanjim Bin Faruk, Shrideep Pallickara, Sangmi Lee Pallickara)</author>
      <guid isPermaLink="false">2512.12445v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation</title>
      <link>http://arxiv.org/abs/2512.12086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Cloak是一种基于潜在扩散模型的新型数据混淆框架，通过对比学习提取解耦表示来平衡隐私保护和数据效用，在资源受限环境中表现出色。&lt;h4&gt;背景&lt;/h4&gt;数据混淆是缓解半可信方对传感器时间序列数据进行属性推断攻击的有效技术，现有方法利用条件生成模型结合对抗训练或互信息正则化来平衡隐私与效用。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需修改下游任务、计算效率高且能灵活调整隐私-效用权衡的数据混淆框架，使其适合在资源受限的移动物联网设备上部署。&lt;h4&gt;方法&lt;/h4&gt;提出Cloak框架，基于潜在扩散模型，采用对比学习提取解耦表示，指导扩散过程保留有用信息同时隐藏私人信息，使用户能根据隐私需求调整保护级别。&lt;h4&gt;主要发现&lt;/h4&gt;在四个时间序列数据集和一个面部图像数据集上的实验表明，Cloak在隐私保护和数据效用方面均优于现有技术，且适合资源受限环境部署。&lt;h4&gt;结论&lt;/h4&gt;Cloak为时间序列数据提供了高效实用的数据混淆解决方案，解决了现有方法在部署灵活性和计算效率方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;数据混淆是一种有前途的技术，可通过半可信方访问的传感器发出的时间序列数据来缓解属性推断攻击。最近的进展利用条件生成模型结合对抗训练或基于互信息的正则化来平衡数据隐私和效用。然而，这些方法通常需要修改下游任务，难以实现令人满意的隐私-效用权衡，或者计算密集，使得它们难以在资源受限的移动物联网设备上部署。我们提出Cloak，一种基于潜在扩散模型的新型数据混淆框架。与先前工作不同，我们采用对比学习来提取解耦表示，这些表示指导潜在扩散过程保留有用信息同时隐藏私人信息。这种方法使具有不同隐私需求的用户能够以最少的再训练来导航隐私-效用权衡。在四个公共时间序列数据集（涵盖多种传感模态）和一个面部图像数据集上的广泛实验表明，Cloak始终优于最先进的混淆技术，并且适合在资源受限的环境中部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.</description>
      <author>example@mail.com (Xin Yang, Omid Ardakanian)</author>
      <guid isPermaLink="false">2512.12086v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Towards Channel-Robust and Receiver-Independent Radio Frequency Fingerprint Identification</title>
      <link>http://arxiv.org/abs/2512.12070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种三阶段射频指纹识别方法，用于物联网设备认证，能够有效缓解信道和接收效应的影响，同时减少训练数据需求。&lt;h4&gt;背景&lt;/h4&gt;射频指纹识别(RFFI)是一种新兴的物联网设备认证方法，利用设备固有的硬件缺陷进行分类。基于深度学习的RFFI表现良好，但仍面临公共训练数据集有限以及信道和接收效应影响等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效缓解信道和接收效应影响的三阶段RFFI方法，减少对大量训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出了一种三阶段RFFI方法：(1)对比学习增强的预训练；(2)基于Siamese网络的分类网络训练；(3)推理阶段。具体技术包括使用频谱图作为信号表示以分离发射机缺陷，提出无监督对比学习方法预训练信道鲁棒的RFF提取器，并通过数据增强和对比损失增强Siamese网络方案。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够有效同时缓解信道和接收效应的影响；预训练可以显著减少微调所需的数据量；在动态非视距场景下，每台设备仅有20个数据包时，准确率超过90%。&lt;h4&gt;结论&lt;/h4&gt;提出的三阶段RFFI方法在减少数据需求的同时，能够有效应对信道和接收效应的挑战，在动态非视距场景下仍保持高准确率。&lt;h4&gt;翻译&lt;/h4&gt;射频指纹识别(RFFI)是一种用于认证物联网设备的 emerging 方法。RFFI 利用固有的、独特的硬件缺陷来分类物联网设备。基于深度学习的 RFFI 已表现出优异的性能。然而，仍然存在一些研究挑战，如公共训练数据集有限以及信道和接收效应的影响。在本文中，我们提出了一种三阶段 RFFI 方法，包括对比学习增强的预训练、基于 Siamese 网络的分类网络训练和推理阶段。具体来说，我们采用频谱图作为信号表示，以将发射机缺陷与信道效应和接收机效应解耦。我们提出了一种无监督对比学习方法来预训练信道鲁棒的 RFF 提取器。此外，基于 Siamese 网络的方案通过数据增强和对比损失得到增强，能够共同缓解信道和接收机效应的影响。我们使用三个公共 LoRa 数据集和一个自收集的 LoRa 数据集进行了全面的实验评估。结果表明，我们的方法能够有效且同时缓解信道和接收机效应的影响。我们还表明，预训练可以显著减少微调所需的数据量。在每台设备仅有 20 个数据包的动态非视距(NLOS)场景下，我们提出的方法实现了超过 90% 的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radio frequency fingerprint identification (RFFI) is an emerging method for authenticating Internet of Things (IoT) devices. RFFI exploits the intrinsic and unique hardware imperfections for classifying IoT devices. Deep learning-based RFFI has shown excellent performance. However, there are still remaining research challenges, such as limited public training datasets as well as impacts of channel and receive effects. In this paper, we proposed a three-stage RFFI approach involving contrastive learning-enhanced pretraining, Siamese network-based classification network training, and inference. Specifically, we employed spectrogram as signal representation to decouple the transmitter impairments from channel effects and receiver impairments. We proposed an unsupervised contrastive learning method to pretrain a channel-robust RFF extractor. In addition, the Siamese network-based scheme is enhanced by data augmentation and contrastive loss, which is capable of jointly mitigating the effects of channel and receiver impairments. We carried out a comprehensive experimental evaluation using three public LoRa datasets and one self-collected LoRa dataset. The results demonstrated that our approach can effectively and simultaneously mitigate the effects of channel and receiver impairments. We also showed that pretraining can significantly reduce the required amount of the fine-tuning data. Our proposed approach achieved an accuracy of over 90% in dynamic non-line-of-sight (NLOS) scenarios when there are only 20 packets per device.</description>
      <author>example@mail.com (Jie Ma, Junqing Zhang, Guanxiong Shen, Linning Peng, Alan Marshall)</author>
      <guid isPermaLink="false">2512.12070v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2512.13635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SCR2-ST是一个利用单细胞先验知识指导高效数据获取和准确表达预测的统一框架，结合了单细胞引导的强化学习主动采样和混合回归-检索预测网络。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学(ST)是一种新兴技术，可研究组织形态背后的分子关系，但获取ST数据成本高昂，传统固定网格采样策略会导致对形态相似或生物学信息不丰富区域的冗余测量，从而限制了当前方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，利用单细胞测序提供的丰富生物学数据作为辅助来源，解决ST数据获取成本高和效率低的问题。&lt;h4&gt;方法&lt;/h4&gt;SCR2-ST整合了单细胞引导的强化学习(SCRL)主动采样和混合回归-检索预测网络SCR2Net。SCRL结合单细胞基础模型嵌入和空间密度信息构建奖励信号，选择性获取信息丰富的组织区域；SCR2Net结合回归建模和检索增强推理，使用多数细胞类型过滤机制抑制噪声匹配，检索到的表达谱作为辅助监督的软标签。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开ST数据集上评估显示，SCR2-ST在采样效率和预测准确性方面都达到了最先进的性能，特别是在低预算场景下表现优异。&lt;h4&gt;结论&lt;/h4&gt;SCR2-ST框架有效地利用单细胞先验知识解决了ST数据获取成本高和效率低的问题，为空间转录组学研究提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;空间转录组学(ST)是一项新兴技术，使研究人员能够研究组织形态背后的分子关系。然而，获取ST数据仍然成本高昂，传统的固定网格采样策略导致对形态相似或生物学信息不丰富的区域的冗余测量，从而限制了当前方法的数据稀缺问题。然而，成熟且完善的单细胞测序领域可以提供丰富的生物学数据作为有效的辅助来源来缓解这一限制。为了弥合这些差距，我们引入了SCR2-ST，这是一个利用单细胞先验知识指导高效数据获取和准确表达预测的统一框架。SCR2-ST整合了基于单细胞引导的强化学习(SCRL)主动采样和混合回归-检索预测网络SCR2Net。SCRL结合单细胞基础模型嵌入和空间密度信息构建生物学基础的奖励信号，在有限的测序预算下能够选择性获取信息丰富的组织区域。SCR2Net然后通过结合基于回归的建模和检索增强推理的混合架构，利用主动采样的数据，其中多数细胞类型过滤机制抑制噪声匹配，检索到的表达谱作为辅助监督的软标签。我们在三个公共ST数据集上评估了SCR2-ST，证明了其在采样效率和预测准确性方面都达到了最先进的性能，特别是在低预算场景下。代码已在GitHub上公开：https://github.com/hrlblab/SCR2ST&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST</description>
      <author>example@mail.com (Junchao Zhu, Ruining Deng, Junlin Guo, Tianyuan Yao, Chongyu Qu, Juming Xiong, Siqi Lu, Zhengyi Lu, Yanfan Zhu, Marilyn Lionts, Yuechen Yang, Yalin Zheng, Yu Wang, Shilin Zhao, Haichun Yang, Yuankai Huo)</author>
      <guid isPermaLink="false">2512.13635v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis</title>
      <link>http://arxiv.org/abs/2512.13608v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了首个数字断层合成乳腺成像(DBT)基础模型DBT-DINO，并在乳腺密度分类、5年乳腺癌风险预测和病灶检测三个临床任务上进行了评估。DBT-DINO在前两项任务上表现优于基线模型，但在病灶检测任务上表现不一，需要进一步方法改进。&lt;h4&gt;背景&lt;/h4&gt;基础模型在医学影像领域显示出潜力，但在三维成像模态中研究不足。目前尚无针对数字断层合成乳腺成像(DBT)的基础模型，尽管DBT已被用于乳腺癌筛查。&lt;h4&gt;目的&lt;/h4&gt;开发并评估一个用于DBT的基础模型(DBT-DINO)在多个临床任务上的表现，并评估领域特定预训练的影响。&lt;h4&gt;方法&lt;/h4&gt;使用DINOv2方法进行自监督预训练，数据包括超过2500万个来自27,990名患者的487,975个体积的2D切片。评估三个下游任务：乳腺密度分类(5,000次筛查检查)、5年乳腺癌发展风险预测(106,417次筛查检查)和病灶检测(393个注释体积)。&lt;h4&gt;主要发现&lt;/h4&gt;乳腺密度分类：DBT-DINO准确率为0.79，优于MetaAI DINOv2基线(0.73)和DenseNet-121(0.74)。5年乳腺癌风险预测：DBT-DINO的AUROC为0.78，高于DINOv2的0.76。病灶检测：DINOv2的平均敏感性为0.67，高于DBT-DINO的0.62，但DBT-DINO在癌性病灶检测率上更高(78.8% vs 77.3%)。&lt;h4&gt;结论&lt;/h4&gt;使用前所未有的数据集开发了DBT-DINO，这是首个DBT基础模型。DBT-DINO在乳腺密度分类和癌症风险预测方面表现出色，但领域特定预训练在检测任务上的益处有限，表明局部检测任务需要进一步的方法开发。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在医学影像领域显示出潜力，但在三维成像模态中研究不足。目前尚无针对数字断层合成乳腺成像(DBT)的基础模型，尽管DBT已被用于乳腺癌筛查。为开发并评估一个用于DBT的基础模型(DBT-DINO)在多个临床任务上的表现并评估领域特定预训练的影响，我们使用DINOv2方法进行了自监督预训练，数据包括超过2500万个来自27,990名患者的487,975个体积的2D切片。评估了三个下游任务：(1)使用5,000次筛查检查的乳腺密度分类；(2)使用106,417次筛查检查的5年乳腺癌发展风险预测；(3)使用393个注释体积的病灶检测。在乳腺密度分类方面，DBT-DINO的准确率为0.79，优于MetaAI DINOv2基线(0.73)和DenseNet-121(0.74)。在5年乳腺癌风险预测方面，DBT-DINO的AUROC为0.78，高于DINOv2的0.76。在病灶检测方面，DINOv2的平均敏感性为0.67，高于DBT-DINO的0.62，但DBT-DINO在癌性病灶检测率上更高(78.8% vs 77.3%)。使用前所未有的数据集，我们开发了DBT-DINO，这是首个DBT基础模型。DBT-DINO在乳腺密度分类和癌症风险预测方面表现出色。然而，领域特定预训练在检测任务上的益处不一，ImageNet基线在一般病灶检测方面优于DBT-DINO，表明局部检测任务需要进一步的方法开发。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决数字乳腺断层合成（DBT）缺乏专门基础模型的问题。DBT虽已用于乳腺癌筛查并比传统2D乳腺摄影效果更好，但其影像解读耗时更长，需要自动化工具提高效率。然而，开发这些工具需要大量带标注的数据，而基础模型可以通过在大量未标记数据上预训练，再在小数据集上微调，有效解决这一挑战，对提高乳腺癌筛查效率和准确性具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到DBT作为三维成像模态，其应用在基础模型领域尚不充分，且面临数据量大、图像各向异性、病灶信息局部化等挑战。他们借鉴了Meta AI的DINOv2自监督学习方法，从ImageNet预训练模型初始化，然后在大规模DBT数据集上继续预训练。设计上采用ViT作为骨干网络，针对不同下游任务（密度分类、风险预测、病灶检测）使用不同的特征聚合和任务特定头，同时探索了多种特征聚合策略以优化性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在大量未标记的DBT图像上进行自监督预训练，学习通用的视觉特征表示，这些特征可迁移到多种临床任务中，减少对外部标注数据的依赖。整体流程包括：1）收集大规模DBT数据集（48万个体积，2.8万名患者）；2）使用DINOv2方法进行自监督预训练；3）针对三个下游任务（密度分类、风险预测、病灶检测）设计不同的特征聚合和预测头；4）评估模型性能并与ImageNet预训练基线比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）开发首个专门针对DBT的基础模型DBT-DINO；2）使用前所未有的大规模DBT数据集进行预训练；3）系统探索跨图像体积的特征聚合方法；4）在多个临床任务上全面评估模型；5）进行详细的偏差和公平性分析。相比之前工作，不同之处在于：专注于三维DBT而非2D乳腺摄影；采用自监督学习减少标注依赖；单一模型支持多任务应用；使用更大规模数据；更全面的评估包括公平性分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文开发了DBT-DINO，首个针对数字乳腺断层合成的基础模型，通过大规模自监督预训练实现了在乳腺密度分类和癌症风险预测等临床任务上的高性能表现，为DBT影像分析提供了高效、准确的自动化解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p&lt;.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p&lt;.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.</description>
      <author>example@mail.com (Felix J. Dorfner, Manon A. Dorster, Ryan Connolly, Oscar Gentilhomme, Edward Gibbs, Steven Graham, Seth Wander, Thomas Schultz, Manisha Bahl, Dania Daye, Albert E. Kim, Christopher P. Bridge)</author>
      <guid isPermaLink="false">2512.13608v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides</title>
      <link>http://arxiv.org/abs/2512.13600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DA-SSL的域自适应自监督适配器，用于解决病理学基础模型在特定癌症类型上的局限性问题，特别是在膀胱肿瘤电切术样本中的应用。该方法能够在不修改基础模型的情况下重新对齐预训练特征，并在治疗反应预测中表现出色。&lt;h4&gt;背景&lt;/h4&gt;深度学习框架，特别是多实例学习结合病理学基础模型在组织病理学中表现强大，但这些模型在某些癌症类型上存在局限性，这是由于域偏移造成的。膀胱肿瘤电切术样本含有组织碎片和电灼伪影，在公开可用的基础模型中未被广泛使用。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法使预训练的病理学基础模型特征能够适应膀胱肿瘤电切术域，而无需微调基础模型本身，并利用这些特征预测治疗反应，识别将从新辅助化疗中受益的患者。&lt;h4&gt;方法&lt;/h4&gt;提出域自适应自监督适配器(DA-SSL)，重新对齐预训练的PFM特征到TURBT域。在多中心研究中使用五折交叉验证和外部测试评估性能，采用多数投票方法进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;DA-SSL在五折交叉验证中实现了0.77+/-0.04的AUC，外部测试准确率达到0.84，敏感性为0.71，特异性为0.91。轻量级域自适应与自监督能有效增强基于PFM的MIL流程。&lt;h4&gt;结论&lt;/h4&gt;轻量级域自适应与自监督可有效增强基于病理学基础模型的多实例学习流程，用于解决具有临床挑战性的组织病理学任务，特别是在特定域适应方面。&lt;h4&gt;翻译&lt;/h4&gt;最近的组织病理学深度学习框架，特别是多实例学习与病理学基础模型相结合，已展现出强大的性能。然而，PFMs在某些癌症类型或样本类型上存在局限性，这是由于域偏移造成的 - 这些癌症类型很少用于预训练，或者样本包含在预训练人群中很少见的基于组织的伪影。膀胱肿瘤电切术就是这种情况，它是诊断肌肉浸润性膀胱癌所必需的，但含有组织碎片和电灼伪影，并且在公开可用的PFMs中未被广泛使用。为了解决这个问题，我们提出了一种简单而有效的域自适应自监督适配器，它将预训练的PFM特征重新对齐到TURBT域，而不需要微调基础模型本身。我们在TURBT的治疗反应预测中试点了这个框架，其中组织形态学特征目前未被充分利用，并且识别将从新辅助化疗中受益的患者具有挑战性。在我们的多中心研究中，DA-SSL在五折交叉验证中实现了0.77+/-0.04的AUC，使用多数投票方法在外部测试中达到了0.84的准确率、0.71的敏感性和0.91的特异性。我们的结果表明，使用自监督的轻量级域自适应可以有效地增强基于PFM的MIL流程，用于具有临床挑战性的组织病理学任务。代码可在https://github.com/zhanghaoyue/DA_SSL_TURBT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.</description>
      <author>example@mail.com (Haoyue Zhang, Meera Chappidi, Erolcan Sayar, Helen Richards, Zhijun Chen, Lucas Liu, Roxanne Wadia, Peter A Humphrey, Fady Ghali, Alberto Contreras-Sanz, Peter Black, Jonathan Wright, Stephanie Harmon, Michael Haffner)</author>
      <guid isPermaLink="false">2512.13600v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Memory in the Age of AI Agents</title>
      <link>http://arxiv.org/abs/2512.13564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于智能体记忆研究的综述文章，旨在提供当前智能体记忆研究的全景图。文章通过形式、功能和动态三个视角对智能体记忆进行了分类和分析，并提供了记忆基准和开源框架的综合总结，同时探讨了未来的研究前沿。&lt;h4&gt;背景&lt;/h4&gt;记忆已成为基于基础模型的智能体的核心能力，并将继续如此。智能体记忆研究迅速扩展并吸引了前所未有的关注，但该领域变得越来越碎片化。现有工作在动机、实现和评估协议上存在显著差异，而记忆术语的激增进一步模糊了概念清晰性。传统的长期/短期记忆分类方法不足以捕捉当代智能体记忆系统的多样性。&lt;h4&gt;目的&lt;/h4&gt;提供当前智能体记忆研究的最新全景图，明确定义智能体记忆的范围并将其与相关概念区分开，为未来智能体智能设计中记忆作为第一类原语提供概念基础。&lt;h4&gt;方法&lt;/h4&gt;通过形式、功能和动态的统一视角检查智能体记忆；从形式视角识别三种主要实现方式；从功能视角提出更精细的分类法；从动态视角分析记忆随时间的形成、演变和检索过程；编写记忆基准和开源框架的综合总结；探讨新兴研究前沿。&lt;h4&gt;主要发现&lt;/h4&gt;智能体记忆可标记级、参数化和潜在三种形式实现；功能上可分为事实记忆、经验记忆和工作记忆；记忆随时间形成、演变和检索的动态过程；提供了记忆基准和开源框架的综合总结。&lt;h4&gt;结论&lt;/h4&gt;这项调查不仅作为现有工作的参考，也为重新思考记忆作为未来智能体设计中第一类原语的概念基础。&lt;h4&gt;翻译&lt;/h4&gt;记忆已经成为并将继续成为基于基础模型的智能体的核心能力。随着智能体记忆研究的迅速扩展并吸引了前所未有的关注，该领域也变得越来越碎片化。属于智能体记忆范畴的现有工作在动机、实现和评估协议上往往存在显著差异，而大量定义松散的记忆术语进一步模糊了概念清晰性。传统的长期/短期记忆等分类方法已被证明不足以捕捉当代智能体记忆系统的多样性。本文旨在提供当前智能体记忆研究的最新全景图。我们首先明确定义智能体记忆的范围，并将其与LLM记忆、检索增强生成(RAG)和上下文工程等相关概念区分开来。然后，我们通过形式、功能和动态的统一视角来审视智能体记忆。从形式视角，我们确定了智能体记忆的三种主要实现方式，即标记级记忆、参数化记忆和潜在记忆。从功能视角，我们提出了更精细的分类法，区分事实记忆、经验记忆和工作记忆。从动态视角，我们分析了记忆如何随时间形成、演变和检索。为支持实际开发，我们整理了记忆基准和开源框架的综合总结。除整合外，我们还阐述了关于新兴研究前沿的前瞻性观点，包括记忆自动化、强化学习集成、多模态记忆、多智能体记忆和可信度问题。我们希望这项调查不仅作为现有工作的参考，也为重新思考记忆作为未来智能体设计中第一类原语的概念基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.</description>
      <author>example@mail.com (Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, Senjie Jin, Jiejun Tan, Yanbin Yin, Jiongnan Liu, Zeyu Zhang, Zhongxiang Sun, Yutao Zhu, Hao Sun, Boci Peng, Zhenrong Cheng, Xuanbo Fan, Jiaxin Guo, Xinlei Yu, Zhenhong Zhou, Zewen Hu, Jiahao Huo, Junhao Wang, Yuwei Niu, Yu Wang, Zhenfei Yin, Xiaobin Hu, Yue Liao, Qiankun Li, Kun Wang, Wangchunshu Zhou, Yixin Liu, Dawei Cheng, Qi Zhang, Tao Gui, Shirui Pan, Yan Zhang, Philip Torr, Zhicheng Dou, Ji-Rong Wen, Xuanjing Huang, Yu-Gang Jiang, Shuicheng Yan)</author>
      <guid isPermaLink="false">2512.13564v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains</title>
      <link>http://arxiv.org/abs/2512.13534v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025. Code available at: https://github.com/mariannerakic/Pancakes&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Pancakes框架，它能够自动为来自新领域的生物医学图像生成多种合理协议的多标签分割图，同时保持相关图像间的语义一致性，显著优于现有基础模型。&lt;h4&gt;背景&lt;/h4&gt;生物医学图像可根据不同应用需求进行多种有意义分割，如大脑MRI可按组织类型、血管区域、解剖区域等分割。现有自动分割模型通常只支持单一训练协议或需要繁琐手动提示指定分割方式。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架，使给定来自新领域图像时，能自动生成多种合理协议的多标签分割图，同时保持相关图像间的语义一致性。&lt;h4&gt;方法&lt;/h4&gt;引入名为Pancakes的框架，该框架采用一种新的问题表述方式，在七个保留数据集上进行了一系列实验来验证其性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Pancakes模型在生成多种合理的全图像分割方面显著优于现有基础模型，且这些分割在图像间保持语义连贯性。&lt;h4&gt;结论&lt;/h4&gt;Pancakes框架解决了现有自动分割模型的局限性，能够自动生成多标签分割图并保持语义一致性，为生物医学图像分割提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;单个生物医学图像可以根据所需应用以多种有意义的方式进行分割。例如，大脑MRI可以根据组织类型、血管区域、广泛的解剖区域、精细的解剖结构或病理等进行分割。现有的自动分割模型通常要么只支持单一协议（即它们训练时所用的协议），要么需要繁琐的手动提示来指定所需的分割方式。我们引入了Pancakes框架，该框架能够根据来自先前未见领域的新图像，自动为多种合理的协议生成多标签分割图，同时在相关图像之间保持语义一致性。Pancakes引入了一种新的问题表述，这是现有基础模型目前无法实现的。在七个保留数据集上进行的一系列实验中，我们证明与现有基础模型相比，我们的模型在生成几种合理的全图像分割方面表现显著更好，且这些分割在图像之间是语义连贯的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.</description>
      <author>example@mail.com (Marianne Rakic, Siyu Gai, Etienne Chollet, John V. Guttag, Adrian V. Dalca)</author>
      <guid isPermaLink="false">2512.13534v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</title>
      <link>http://arxiv.org/abs/2512.13507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Seedance 1.5 pro Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Seedance 1.5 pro是一个专为原生联合音频视频生成设计的基础模型，采用双分支Diffusion Transformer架构，实现了出色的视听同步性和卓越的生成质量，并提供了精确的多语言口型同步、动态摄像机控制和增强的叙事连贯性。&lt;h4&gt;背景&lt;/h4&gt;视频生成领域的最新进展为统一的视听生成铺平了道路。&lt;h4&gt;目的&lt;/h4&gt;介绍Seedance 1.5 pro，这是一个专门为原生、联合音频视频生成设计的基础模型。&lt;h4&gt;方法&lt;/h4&gt;采用双分支Diffusion Transformer架构，集成了跨模态联合模块和专门的多阶段数据管道；实施后训练优化，包括监督微调(SFT)和人类反馈强化学习(RLHF)；引入加速框架，提高推理速度10倍以上。&lt;h4&gt;主要发现&lt;/h4&gt;Seedance 1.5 pro具备精确的多语言和方言口型同步能力、动态电影摄像机控制功能和增强的叙事连贯性。&lt;h4&gt;结论&lt;/h4&gt;Seedance 1.5 pro是专业级内容创作的强大引擎，现在可通过火山引擎访问。&lt;h4&gt;翻译&lt;/h4&gt;视频生成领域的最新进展为统一的视听生成铺平了道路。在这项工作中，我们提出了Seedance 1.5 pro，这是一个专门为原生、联合音频视频生成设计的基础模型。利用双分支Diffusion Transformer架构，该模型集成了跨模态联合模块和专门的多阶段数据管道，实现了出色的视听同步性和卓越的生成质量。为确保实用性，我们实施了精细的后训练优化，包括在高质量数据集上进行监督微调(SFT)和使用多维奖励模型进行人类反馈强化学习(RLHF)。此外，我们还引入了一个加速框架，将推理速度提高了10倍以上。Seedance 1.5 pro以其精确的多语言和方言口型同步、动态电影摄像机控制和增强的叙事连贯性而脱颖而出，成为专业级内容创作的强大引擎。Seedance 1.5 pro现可通过火山引擎访问，网址为https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.</description>
      <author>example@mail.com (Siyan Chen, Yanfei Chen, Ying Chen, Zhuo Chen, Feng Cheng, Xuyan Chi, Jian Cong, Qinpeng Cui, Qide Dong, Junliang Fan, Jing Fang, Zetao Fang, Chengjian Feng, Han Feng, Mingyuan Gao, Yu Gao, Qiushan Guo, Boyang Hao, Qingkai Hao, Bibo He, Qian He, Tuyen Hoang, Ruoqing Hu, Xi Hu, Weilin Huang, Zhaoyang Huang, Zhongyi Huang, Siqi Jiang, Wei Jiang, Yunpu Jiang, Zhuo Jiang, Ashley Kim, Jianan Kong, Zhichao Lai, Shanshan Lao, Ai Li, Feiya Li, Gen Li, Huixia Li, JiaShi Li, Liang Li, Ming Li, Tao Li, Xian Li, Xiaojie Li, Xiaoyang Li, Xingxing Li, Yameng Li, Yifu Li, Yiying Li, Chao Liang, Ying Liang, Zhiqiang Liang, Wang Liao, Yalin Liao, Heng Lin, Kengyu Lin, Shanchuan Lin, Xi Lin, Zhijie Lin, Feng Ling, Fangfang Liu, Gaohong Liu, Jiawei Liu, Jie Liu, Shouda Liu, Shu Liu, Sichao Liu, Songwei Liu, Xin Liu, Xue Liu, Yibo Liu, Zikun Liu, Zuxi Liu, Junlin Lyu, Lecheng Lyu, Qian Lyu, Han Mu, Xiaonan Nie, Jingzhe Ning, Xitong Pan, Yanghua Peng, Lianke Qin, Xueqiong Qu, Yuxi Ren, Yuchen Shen, Guang Shi, Lei Shi, Yan Song, Yinglong Song, Fan Sun, Li Sun, Renfei Sun, Zeyu Sun, Wenjing Tang, Zirui Tao, Feng Wang, Furui Wang, Jinran Wang, Junkai Wang, Ke Wang, Kexin Wang, Qingyi Wang, Rui Wang, Sen Wang, Shuai Wang, Tingru Wang, Weichen Wang, Xin Wang, Yanhui Wang, Yue Wang, Yuping Wang, Yuxuan Wang, Ziyu Wang, Guoqiang Wei, Wanru Wei, Di Wu, Guohong Wu, Hanjie Wu, Jian Wu, Jie Wu, Ruolan Wu, Xinglong Wu, Yonghui Wu, Ruiqi Xia, Liang Xiang, Fei Xiao, XueFeng Xiao, Pan Xie, Shuangyi Xie, Shuang Xu, Jinlan Xue, Bangbang Yang, Ceyuan Yang, Jiaqi Yang, Runkai Yang, Tao Yang, Yang Yang, Yihang Yang, ZhiXian Yang, Ziyan Yang, Yifan Yao, Zilyu Ye, Bowen Yu, Chujie Yuan, Linxiao Yuan, Sichun Zeng, Weihong Zeng, Xuejiao Zeng, Yan Zeng, Chuntao Zhang, Heng Zhang, Jingjie Zhang, Kuo Zhang, Liang Zhang, Liying Zhang, Manlin Zhang, Ting Zhang, Weida Zhang, Xiaohe Zhang, Xinyan Zhang, Yan Zhang, Yuan Zhang, Zixiang Zhang, Fengxuan Zhao, Huating Zhao, Yang Zhao, Hao Zheng, Jianbin Zheng, Xiaozheng Zheng, Yangyang Zheng, Yijie Zheng, Jiexin Zhou, Kuan Zhu, Shenhan Zhu, Wenjia Zhu, Benhui Zou, Feilong Zuo)</author>
      <guid isPermaLink="false">2512.13507v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis</title>
      <link>http://arxiv.org/abs/2512.13491v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, no figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了神经缩放定律与齐普夫定律之间的演绎关系，证明了在特定假设条件下，神经缩放定律是齐普夫定律的必然结果。&lt;h4&gt;背景&lt;/h4&gt;神经缩放定律描述基础模型（如大型语言模型）的交叉熵率随训练标记数量、参数量和计算量的变化规律；齐普夫定律则指出标记分布呈现幂律尾部特征。&lt;h4&gt;目的&lt;/h4&gt;展示神经缩放定律如何从齐普夫定律推导而来，揭示两者之间的理论基础联系。&lt;h4&gt;方法&lt;/h4&gt;通过一系列推导步骤，从齐普夫定律到希普定律，再到希尔伯格假说，最后推导出神经缩放定律；并通过圣塔菲过程的玩具例子进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;在特定假设条件下，神经缩放定律是齐普夫定律的必然结果；展示了从齐普夫定律推导出神经缩放定律的完整链条。&lt;h4&gt;结论&lt;/h4&gt;神经缩放定律与齐普夫定律之间存在理论基础联系，这为理解语言模型的缩放行为提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;我们检查了神经缩放定律与齐普夫定律之间的演绎关系——这两个在机器学习和定量语言学中讨论的陈述。神经缩放定律描述基础模型（如大型语言模型）的交叉熵率如何随训练标记数量、参数量和计算量的变化而变化。相比之下，齐普夫定律认为标记的分布呈现幂律尾部特征。虽然在更具体的情境中已有类似主张，但我们展示了在揭示的某些广泛假设条件下，神经缩放定律是齐普夫定律的必然结果。推导步骤如下：我们从齐普夫定律推导出关于词汇增长的希普定律，从希普定律推导出关于熵缩放的希尔伯格假说，再从希尔伯格假说推导出神经缩放定律。我们通过满足所有四种统计定律的圣塔菲过程的玩具例子来说明这些推理步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.</description>
      <author>example@mail.com (Łukasz Dębowski)</author>
      <guid isPermaLink="false">2512.13491v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Test-Time Modification: Inverse Domain Transformation for Robust Perception</title>
      <link>http://arxiv.org/abs/2512.13454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种使用扩散模型在测试时将目标图像映射回源分布的新方法，用于解决领域泛化问题，在多个视觉任务上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;生成式基础模型包含广泛的视觉知识，能够产生多样化的图像变化，在推进领域泛化任务方面具有前景。然而，使用它们进行训练数据增强存在合成目标领域变化缓慢、昂贵且不完整的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种替代传统数据增强的方法，在测试时利用扩散模型将目标图像映射回源分布，以提高领域泛化性能。&lt;h4&gt;方法&lt;/h4&gt;该方法仅需源领域描述，保留原始任务模型，无需生成大规模合成数据。研究者在具有未知目标分布的真实到真实领域泛化场景中，针对分割、检测和分类任务进行了验证，并分析了多种生成和下游模型，包括用于增强鲁棒性的集成变体。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多个挑战性环境变化场景中取得了持续改进：在BDD100K-Night上实现了137%的相对增益，在ImageNet-R上实现了68%的相对增益，在DarkZurich上实现了62%的相对增益。&lt;h4&gt;结论&lt;/h4&gt;使用扩散模型在测试时将目标图像映射回源分布是一种有效的方法，能够克服传统生成式模型在领域泛化中的局限性，显著提升模型在未知目标分布上的性能。&lt;h4&gt;翻译&lt;/h4&gt;生成式基础模型包含广泛的视觉知识，并能产生多样化的图像变化，使其在推进领域泛化任务方面特别有前景。虽然它们可用于训练数据增强，但合成全面的目标领域变化仍然缓慢、昂贵且不完整。我们提出了一种替代方案：在测试时使用扩散模型将目标图像映射回下游模型训练的源分布。这种方法仅需源领域描述，保留任务模型，并消除了大规模合成数据生成。我们在具有挑战性环境变化的真实到真实领域泛化场景中，针对分割、检测和分类任务展示了持续改进，这些场景具有未知的目标分布。我们的分析涵盖了多种生成和下游模型，包括用于增强鲁棒性的集成变体。该方法实现了显著的相对增益：在BDD100K-Night上为137%，在ImageNet-R上为68%，在DarkZurich上为62%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.</description>
      <author>example@mail.com (Arpit Jadon, Joshua Niemeijer, Yuki M. Asano)</author>
      <guid isPermaLink="false">2512.13454v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>RecTok: Reconstruction Distillation along Rectified Flow</title>
      <link>http://arxiv.org/abs/2512.13421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RecTok是一种新型视觉tokenizer，通过流语义蒸馏和重建-对齐蒸馏两种创新方法克服了高维视觉tokenizer的局限性，实现了优越的图像重建、生成质量和判别性能，在gFID-50K上取得了最先进结果。&lt;h4&gt;背景&lt;/h4&gt;视觉tokenizer在扩散模型中起关键作用，潜在空间维度决定重建保真度和语义表达能力，但现有方法受限于低维潜在空间，因为维度和生成质量之间存在基本权衡。&lt;h4&gt;目的&lt;/h4&gt;解决高维视觉tokenizer表现不如低维tokenizer的问题，通过创新方法提高高维tokenizer的性能。&lt;h4&gt;方法&lt;/h4&gt;提出RecTok，包含两个关键创新：流语义蒸馏和重建-对齐蒸馏。使流匹配中的前向流语义丰富，作为扩散变换器的训练空间，将VFM中的语义信息蒸馏到流匹配中的前向流轨迹中，并通过引入掩码特征重建损失进一步增强语义。&lt;h4&gt;主要发现&lt;/h4&gt;RecTok实现了优越的图像重建、生成质量和判别性能；在有和无分类器自由指导设置下，在gFID-50K上取得了最先进结果；保持了语义丰富的潜在空间结构；随着潜在维度增加，观察到持续改进。&lt;h4&gt;结论&lt;/h4&gt;RecTok成功解决了高维视觉tokenizer的局限性，通过流语义蒸馏和重建-对齐蒸馏方法提高了性能，代码和模型已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;视觉标记器在扩散模型中起着关键作用。潜在空间的维度决定了重建保真度和潜在特征的语义表达能力。然而，维度和生成质量之间存在基本权衡，这限制了现有方法只能使用低维潜在空间。尽管最近的工作利用视觉基础模型来丰富视觉标记器的语义并加速收敛，但高维标记器的表现仍然不如低维标记器。在这项工作中，我们提出了RecTok，它通过两个关键创新克服了高维视觉标记器的局限性：流语义蒸馏和重建-对齐蒸馏。我们的关键洞见是使流匹配中的前向流具有丰富的语义，作为扩散变换器的训练空间，而不是像以前的工作那样专注于潜在空间。具体来说，我们的方法将VFM中的语义信息蒸馏到流匹配中的前向流轨迹中。我们通过引入掩码特征重建损失进一步增强了语义。我们的RecTok实现了卓越的图像重建、生成质量和判别性能。在有和无分类器自由指导设置下，它在gFID-50K上取得了最先进的结果，同时保持了语义丰富的潜在空间结构。此外，随着潜在维度的增加，我们观察到持续改进。代码和模型可在https://shi-qingyu.github.io/rectok.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.</description>
      <author>example@mail.com (Qingyu Shi, Size Wu, Jinbin Bai, Kaidong Yu, Yujing Wang, Yunhai Tong, Xiangtai Li, Xuelong Li)</author>
      <guid isPermaLink="false">2512.13421v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2512.13101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE TMI for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Uncertainty-informed Collaborative Learning (UnCoL)的双重教师框架，用于解决半监督医学图像分割中的泛化和专业化平衡问题。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型通过大规模、异构预训练在医学图像分割中展示了强大的泛化能力，但在有限标注或罕见病理变异条件下难以推广到专门临床任务，原因是通用先验与任务特定需求不匹配。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法协调半监督医学图像分割中的泛化和专业化，解决基础模型在特定临床任务中的局限性。&lt;h4&gt;方法&lt;/h4&gt;UnCoL框架从冻结的基础模型中提取视觉和语义表示传递通用知识，同时保持渐进适应的教师捕获细粒度和任务特定表示；通过预测不确定性自适应调节伪标签学习，选择性抑制不可靠监督，稳定模糊区域学习。&lt;h4&gt;主要发现&lt;/h4&gt;在多种2D和3D分割基准测试中，UnCoL持续优于最先进的半监督方法和基础模型基线；在显著减少标注需求的情况下，实现了接近全监督的性能。&lt;h4&gt;结论&lt;/h4&gt;UnCoL有效解决了基础模型在专门临床任务中的泛化挑战，为半监督医学图像分割提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型通过利用大规模、异构预训练在医学图像分割中展示了强大的泛化能力。然而，由于通用先验与任务特定需求之间的不匹配，它们通常难以在有限的标注或罕见的病理变异条件下推广到专门的临床任务。为此，我们提出了不确定性感知协作学习(UnCoL)，一种双重教师框架，用于在半监督医学图像分割中协调泛化和专业化。具体来说，UnCoL从冻结的基础模型中提炼视觉和语义表示以转移通用知识，同时保持一个渐进适应的教师来捕获细粒度和任务特定的表示。为了平衡两个教师的指导，UnCoL中的伪标签学习通过预测不确定性进行自适应调节，选择性地抑制不可靠的监督，并在模糊区域稳定学习。在各种2D和3D分割基准上的实验表明，UnCoL持续优于最先进的半监督方法和基础模型基线。此外，我们的模型在显著减少标注需求的情况下，提供了接近全监督的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉基础模型在医学图像分割中难以泛化到特定临床任务的问题，特别是在有限标注或罕见病理条件下。这个问题很重要，因为医学图像分割是临床诊断、治疗计划和疾病监测的基础，而专家标注成本高昂且耗时；基础模型虽然泛化能力强但缺乏任务特定精度，传统半监督方法又无法充分利用外部知识先验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析发现基础模型提供通用先验但缺乏任务特定精度，而传统半监督方法专注于单个任务但无法整合外部知识。他们借鉴了知识蒸馏思想、半监督学习中的不确定性估计以及教师-学生框架，但创新性地设计了双教师框架，结合冻结基础模型和自适应专业教师，并通过双重路径知识蒸馏和基于不确定性的伪标记策略实现协同学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过不确定性信息协调泛化和专业化，实现通用先验和任务特定知识的协同学习。整体流程分为两阶段：1)数据高效预训练阶段，学生模型从标注数据学习并通过双重路径知识蒸馏(视觉和语义)从冻结基础模型获取知识；2)半监督微调阶段，同时使用标注和无标注数据，构建EMA专业教师，使用基于不确定性的伪标记策略动态选择可靠教师的预测来监督学习。推理时仅使用学生模型，无需提示或教师指导。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)不确定性信息协作学习框架(UnCoL)，协调泛化和专业化；2)双重路径知识蒸馏，转移视觉和语义表示；3)基于不确定性的伪标记策略，动态选择可靠教师。相比之前工作，该方法不依赖手动提示实现全自动管道，在域转移和有限标注下表现更好；不假设基础模型能准确捕获任务特定语义；使用像素级不确定性加权而非全局一致性假设；结合视觉和语义双重路径蒸馏而非仅对齐概率输出。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种不确定性信息协作学习框架，通过双教师设计和双重路径知识蒸馏，有效结合了基础模型的通用先验与自适应教师的任务特定知识，在半监督医学图像分割中实现了接近全监督的性能，同时显著减少了对标注数据的需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.</description>
      <author>example@mail.com (Wenjing Lu, Yi Hong, Yang Yang)</author>
      <guid isPermaLink="false">2512.13101v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era</title>
      <link>http://arxiv.org/abs/2512.13089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniVCD的无监督、开放词汇表变化检测方法，基于冻结的SAM2和CLIP视觉基础模型构建，能够在无需标注数据的情况下检测多样化场景中的变化，实验证明其性能优于或匹敌现有方法。&lt;h4&gt;背景&lt;/h4&gt;变化检测(CD)用于从多时相观测中识别场景变化，广泛应用于城市发展和环境监测。现有CD方法大多依赖监督学习，导致性能高度依赖数据集且标注成本高，同时通常只关注少数预定义类别，对多样化场景的泛化能力差。随着SAM2和CLIP等视觉基础模型的兴起，为解决这些问题提供了新机会。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需标注数据或配对变化图像的、无监督的、开放词汇表的变化检测方法，能够在多样化场景和成像几何条件下检测类别无关的变化。&lt;h4&gt;方法&lt;/h4&gt;提出Unified Open-Vocabulary Change Detection (UniVCD)方法，基于冻结的SAM2和CLIP构建。引入轻量级特征对齐模块，连接SAM2的空间详细表示和CLIP的语义先验，实现高分辨率、语义感知的变化估计，同时保持可训练参数数量较少。此外，还引入了简化的后处理流程来抑制噪声和伪变化，改善具有明确定边界的物体的检测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共BCD(二元变化检测)和SCD(语义变化检测)基准测试上，UniVCD取得了一致的强性能。在F1和IoU等关键指标上，匹配或超过了现有的开放词汇表CD方法。&lt;h4&gt;结论&lt;/h4&gt;使用冻结的视觉基础模型和轻量级多模态对齐的无监督变化检测是开放词汇表CD的一种实用且有效的范式。代码和预训练模型将在https://github.com/Die-Xie/UniVCD发布。&lt;h4&gt;翻译&lt;/h4&gt;变化检测(CD)从多时相观测中识别场景变化，广泛应用于城市发展和环境监测。大多数现有CD方法依赖监督学习，使性能高度依赖数据集并产生高标注成本；它们通常只关注少数预定义类别，对多样化场景泛化能力差。随着SAM2和CLIP等视觉基础模型的兴起，为缓解这些约束提供了新机会。我们提出了Unified Open-Vocabulary Change Detection (UniVCD)，一种基于冻结SAM2和CLIP构建的无监督、开放词汇表变化检测方法。UniVCD无需任何标注数据或配对变化图像，就能在多样化场景和成像几何条件下检测类别无关的变化。引入了轻量级特征对齐模块，以连接SAM2的空间详细表示和CLIP的语义先验，实现高分辨率、语义感知的变化估计，同时保持可训练参数数量较少。在此基础上，进一步引入了简化的后处理流程，以抑制噪声和伪变化，提高具有明确定边界的物体的检测准确性。在多个公共BCD(二元变化检测)和SCD(语义变化检测)基准上的实验表明，UniVCD在F1和IoU等关键指标上取得了一致的强性能，匹配或超过了现有的开放词汇表CD方法。结果表明，使用冻结的视觉基础模型和轻量级多模态对齐的无监督变化检测是开放词汇表CD的一种实用且有效的范式。代码和预训练模型将在https://github.com/Die-Xie/UniVCD发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.</description>
      <author>example@mail.com (Ziqiang Zhu, Bowei Yang)</author>
      <guid isPermaLink="false">2512.13089v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models</title>
      <link>http://arxiv.org/abs/2512.13072v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多模态生物医学视觉语言模型的持续学习框架，结合检索增强生成和动态知识蒸馏技术，解决了模型在持续学习中保持细粒度模内特征同时跨越不同模态间领域差距的难题，并在医学通用任务增量学习基准测试中取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;多模态生物医学视觉语言模型在持续学习领域展现出巨大潜力，但面临核心困境：如何在保持细粒度模内特征的同时，跨越不同模态间的显著领域差距。&lt;h4&gt;目的&lt;/h4&gt;解决多模态生物医学视觉语言模型在持续学习中的核心困境，即保持细粒度模内特征同时跨越不同模态间的显著领域差距。&lt;h4&gt;方法&lt;/h4&gt;1. 基于PubMed科学论文构建1800万多模态综合医学检索数据库；2. 首次将检索增强生成(RAG)集成到持续学习中；3. 采用多模态、多层RAG系统提供实时指导；4. 引入动态知识蒸馏框架，动态调整参数空间重要性、知识粒度和数据分布；5. 设计医学通用任务增量学习(MGTIL)基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在所有指标上均取得最先进性能，证明了模型在适应重大领域转移、保留细微领域特征和实时学习新型复杂医学任务方面的卓越能力。&lt;h4&gt;结论&lt;/h4&gt;通过整合检索增强生成和动态知识蒸馏技术，成功解决了多模态生物医学视觉语言模型在持续学习中的核心困境，显著提升了模型性能，为医学领域的持续学习提供了新解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态生物医学视觉语言模型在持续学习领域展现出巨大潜力。然而，它们面临一个核心困境：如何在保持细粒度模内特征的同时，跨越不同模态间的显著领域差距。为应对这一挑战，我们提出了一个全面框架。利用我们从PubMed科学论文中获取的1800万多模态综合医学检索数据库，我们开创性地将检索增强生成(RAG)集成到持续学习中。具体而言，我们采用多模态、多层RAG系统，通过动态、按需知识检索为模型微调提供实时指导。在此基础上，我们引入了动态知识蒸馏框架，该框架通过根据所需详细程度动态调整参数空间重要性、蒸馏知识粒度和参考数据集的数据分布，精确解决了上述核心困境。为彻底验证我们策略的临床价值，我们设计了一个更严格的医学通用任务增量学习(MGTIL)基准。该基准旨在同时评估模型适应重大领域转移、保留细微领域特征和实时学习新型复杂医学任务的能力。大量实验结果表明，我们提出的方法在所有指标上均取得了最先进的性能。代码见补充材料。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.</description>
      <author>example@mail.com (Zizhi Chen, Yizhen Gao, Minghao Han, Yizhou Liu, Zhaoyu Chen, Dingkang Yang, Lihua Zhang)</author>
      <guid isPermaLink="false">2512.13072v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing</title>
      <link>http://arxiv.org/abs/2512.13055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的非对称视觉位置识别(VPR)框架，通过结合高容量图库模型和轻量级查询网络，解决了高容量模型在资源受限设备上的部署问题。&lt;h4&gt;背景&lt;/h4&gt;视觉位置识别(VPR)已通过高容量基础模型(如DINOv2)取得显著进展，但这些模型的计算成本很高，使其在资源受限设备上的部署不切实际。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的非对称VPR框架，使其能够在资源受限环境中有效部署，同时保持高性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种非对称VPR框架，包含用于离线特征提取的高容量图库模型和用于在线处理的轻量级查询网络；引入地理记忆库利用地理位置元数据组织图库特征，避免昂贵的k-NN计算；采用隐式嵌入增强技术提升查询网络建模特征变化的能力。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著降低了计算成本，同时性能优于现有的非对称检索技术，为资源受限环境下的VPR建立了新的方向。&lt;h4&gt;结论&lt;/h4&gt;该非对称VPR框架能够在减少计算成本的同时提高性能，为资源受限环境中的视觉位置识别提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉位置识别(VPR)已通过高容量基础模型(如DINOv2)取得显著进展，实现了卓越的性能。然而，其巨大的计算成本使得在资源受限设备上的部署不切实际。在本文中，我们引入了一种高效的非对称VPR框架，该框架包含用于离线特征提取的高容量图库模型和用于在线处理的轻量级查询网络。在这种设置中的一个关键挑战是确保这些异构网络之间的兼容性，传统方法通过计算昂贵的基于k-NN的兼容性训练来解决这一问题。为了克服这一挑战，我们提出了一种地理记忆库，利用VPR数据库中固有的地理位置元数据来组织图库特征，消除了对穷举k-NN计算的需求。此外，我们引入了一种隐式嵌入增强技术，增强查询网络以建模特征变化，尽管其容量有限。大量实验证明，我们的方法不仅显著降低了计算成本，而且性能优于现有的非对称检索技术，为资源受限环境中的VPR建立了新的方向。代码可在https://github.com/jaeyoon1603/AsymVPR获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR</description>
      <author>example@mail.com (Jaeyoon Kim, Yoonki Cho, Sung-Eui Yoon)</author>
      <guid isPermaLink="false">2512.13055v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Light Field Based 6DoF Tracking of Previously Unobserved Objects</title>
      <link>http://arxiv.org/abs/2512.13007v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于光场图像的目标跟踪方法，该方法不依赖预训练模型，对复杂视觉行为（如反射）具有鲁棒性。通过使用视觉基础模型提取语义和几何特征，并转换为视图相关的高斯斑点作为统一对象表示，支持可微分渲染和姿态优化。实验表明，该方法在困难情况下与最先进的基于模型的跟踪器具有竞争力。&lt;h4&gt;背景&lt;/h4&gt;目标跟踪是机器人和自动驾驶流程中的重要步骤，需要泛化到前所未见和复杂的物体。现有的高性能方法通常依赖预捕获的目标视图来构建显式参考模型，这限制了它们只能处理固定集合的已知物体。然而，这类参考模型在处理视觉复杂外观时存在困难，降低了跟踪质量。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖预训练模型的目标跟踪方法，该方法能够处理具有复杂视觉行为（如反射）的物体，提高跟踪质量，并扩展到更广泛的物体类别。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于光场图像的目标跟踪方法，使用视觉基础模型从光场输入中提取语义和几何特征，并将它们转换为视图相关的高斯斑点。这些斑点作为统一的对象表示，支持可微分渲染和姿态优化。同时引入了一个包含具有精确真实姿态的有挑战性反射物体的光场目标跟踪数据集。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在处理具有挑战性的反射物体时，与最先进的基于模型的跟踪器具有竞争力。该方法不依赖预训练模型，能够处理复杂的视觉行为，如反射。&lt;h4&gt;结论&lt;/h4&gt;该方法为机器人系统中的通用目标跟踪铺平了道路，通过不依赖预训练模型和对复杂视觉行为的鲁棒性，扩展了目标跟踪的应用范围。&lt;h4&gt;翻译&lt;/h4&gt;目标跟踪是机器人和自动驾驶流程中的重要步骤，必须泛化到前所未见和复杂的物体。现有的高性能方法通常依赖预捕获的目标视图来构建显式参考模型，这限制了它们只能处理固定集合的已知物体。然而，这类参考模型在处理视觉复杂外观时存在困难，降低了跟踪质量。在这项工作中，我们介绍了一种基于光场图像的目标跟踪方法，该方法不依赖预训练模型，同时对复杂的视觉行为（如反射）具有鲁棒性。我们使用视觉基础模型从光场输入中提取语义和几何特征，并将它们转换为视图相关的高斯斑点。这些斑点作为统一的对象表示，支持可微分渲染和姿态优化。我们进一步引入了一个包含具有精确真实姿态的有挑战性反射物体的光场目标跟踪数据集。实验表明，在这些困难情况下，我们的方法与最先进的基于模型的跟踪器具有竞争力，为机器人系统中的通用目标跟踪铺平了道路。代码/数据可在 https://github.com/nagonch/LiFT-6DoF 获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是物体跟踪方法对未知物体的泛化问题。现有方法通常需要预捕获的物体视图或CAD模型来构建参考模型，这限制了它们只能处理已知物体。在机器人技术和自动驾驶领域，系统经常需要处理以前未见过的复杂物体，特别是具有反射表面的物体，这对实现通用物体跟踪至关重要。解决这一问题对于提高机器人系统在现实世界中的适应性和实用性具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括对预定义模型的依赖和对复杂视觉外观处理不足的问题。他们借鉴了多个领域的最新进展：视觉基础模型用于提取语义和几何特征，神经场景表示方法用于建模物体外观，以及光场成像技术提供丰富的几何线索。核心设计思路是将光场图像本身作为物体模型，通过将视觉基础模型与在线构建的高斯斑点表示相结合，实现对未知物体的鲁棒跟踪。具体实现中，他们使用了Grounding DINO和SAM2进行分割，Depth Anything进行深度估计，以及高斯斑点作为物体表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将光场图像本身视为物体模型，同时捕获几何和外观信息，为在线姿态跟踪提供自然基础。通过将视觉基础模型提供的语义和几何理解与在线参考物体模型（由高斯斑点表示）的外观监督相结合，实现对复杂视觉行为的鲁棒性。整体实现流程包括：1) 光场处理：使用Grounding DINO和SAM2分割物体，融合光场视差和Depth AnyThing的深度估计，生成3D点云；2) 高斯斑点初始化和训练：通过球谐系数初始化高斯斑点，并进行在线优化；3) 高斯斑点配准：使用彩色ICP进行粗配准，然后通过光度优化进行精细姿态调整。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出基于光场图像的在线物体建模方法，无需预训练模型；2) 将光场图像转换为视图相关的高斯斑点作为跟踪参考；3) 基于高斯斑点的姿态优化模块，对镜面物体表现优异；4) 创建了包含挑战性镜面物体的光场跟踪数据集。相比之前的工作，不同之处在于：不需要预定义的CAD模型或预捕获的物体视图；能够处理以前未见过的物体；对反射表面等复杂视觉行为具有更强的鲁棒性；使用光场图像而非RGB-D提供更丰富的几何信息；采用高斯斑点而非网格表示更好地建模物体外观。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于光场图像的6自由度物体跟踪方法，无需预先训练的物体模型，即可实现对具有复杂反射表面的未知物体的鲁棒跟踪，为机器人系统中的通用物体跟踪开辟了新途径。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.</description>
      <author>example@mail.com (Nikolai Goncharov, James L. Gray, Donald G. Dansereau)</author>
      <guid isPermaLink="false">2512.13007v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Investigating Data Pruning for Pretraining Biological Foundation Models at Scale</title>
      <link>http://arxiv.org/abs/2512.12932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种影响引导的数据修剪框架，用于降低生物基础模型（BioFMs）的训练成本，同时保持模型性能&lt;h4&gt;背景&lt;/h4&gt;生物基础模型（BioFMs）在大规模生物序列预训练后，能为多种生物信息学任务提供有意义的表示，但其训练需要大量计算资源和参数，导致学术实验室难以复现和使用&lt;h4&gt;目的&lt;/h4&gt;解决BioFMs训练计算成本高的问题，提高其可访问性和可持续性&lt;h4&gt;方法&lt;/h4&gt;提出一种后验影响引导的数据修剪框架，引入基于子集的自影响公式来估计样本重要性，并开发了Top-k影响（Top I）和以覆盖为中心的影响（CCI）两种选择策略&lt;h4&gt;主要发现&lt;/h4&gt;在RNA-FM和ESM-C两个代表性BioFMs上的实验表明，该方法在超过99%的极端修剪率下仍优于随机选择；在RNA和蛋白质任务中，其核心集表现优于十倍大小的随机子集，显示生物序列数据中存在大量冗余&lt;h4&gt;结论&lt;/h4&gt;影响引导的数据修剪能显著降低BioFM预训练的计算成本，为更高效、可访问和可持续的生物AI研究开辟了新途径&lt;h4&gt;翻译&lt;/h4&gt;生物基础模型是在大规模生物序列上预训练的模型，最近显示出为各种下游生物信息学任务提供有意义表示的强大潜力。然而，这类模型通常依赖数百万至数十亿的训练序列和数十亿的参数，导致计算成本高昂，并对可重复性和可访问性构成重大障碍，特别是在学术实验室中。为解决这些挑战，我们研究了数据修剪对BioFM预训练的可行性，并提出了一种专门针对生物领域的后验影响引导数据修剪框架。我们的方法引入了一种基于子集的自影响公式，能够以低计算成本高效估计样本重要性，并在此基础上构建了两种简单而有效的选择策略，即Top-k影响和以覆盖为中心的影响。我们在两个代表性的BioFMs上实证验证了我们的方法。对于RNA，在超过99%的极端修剪率下，我们的框架始终优于随机选择基线，证明了其有效性。此外，我们使用ESM-C在蛋白质相关任务上展示了我们框架的通用性。特别是在RNA和蛋白质设置中，我们的核心集甚至比十倍大的随机子集表现更好，揭示了生物序列数据集中存在大量冗余。这些发现强调了影响引导的数据修剪在显著降低BioFM预训练计算成本方面的潜力，为更高效、可访问和可持续的生物AI研究铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.</description>
      <author>example@mail.com (Yifan Wu, Jiyue Jiang, Xichen Ye, Yiqi Wang, Chang Zhou, Yitao Xu, Jiayang Chen, He Hu, Weizhong Zhang, Cheng Jin, Jiao Yuan, Yu Li)</author>
      <guid isPermaLink="false">2512.12932v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification</title>
      <link>http://arxiv.org/abs/2512.12887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  1st Place in VLM3D Challenge&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出AnyMC3D，一种从2D基础模型适配的可扩展3D分类器，解决了医学基础模型在3D医学图像分类中的三个关键缺陷：数据集偏差、次优适应和任务覆盖不足。&lt;h4&gt;背景&lt;/h4&gt;3D医学图像分类对现代临床工作流程至关重要。医学基础模型(FMs)作为扩展到新任务的有前途方法，但当前研究存在三个关键缺陷：数据集偏差、次优适应和任务覆盖不足。&lt;h4&gt;目的&lt;/h4&gt;解决医学基础模型在3D医学图像分类中的关键缺陷，并提出一种可扩展的3D分类器框架。&lt;h4&gt;方法&lt;/h4&gt;提出AnyMC3D，通过在单一冻结主干上添加轻量级插件(每个任务约100万参数)来高效扩展到新任务。该框架支持多视图输入、辅助像素级监督和可解释热图生成。作者建立了包含12个任务的全面基准，涵盖多种病理、解剖结构和模态。&lt;h4&gt;主要发现&lt;/h4&gt;分析揭示了三个关键见解：(1)有效的适应对于释放基础模型潜力至关重要；(2)如果适当适应，通用基础模型可以匹配医学专用基础模型；(3)基于2D的方法在3D分类中优于3D架构。&lt;h4&gt;结论&lt;/h4&gt;首次证明了使用单一可扩展框架在各种应用中实现最先进性能的可行性(包括在VLM3D挑战赛中获得第一名)，消除了对特定任务模型的单独需求。&lt;h4&gt;翻译&lt;/h4&gt;3D医学图像分类对现代临床工作流程至关重要。医学基础模型(FMs)已成为扩展到新任务的一种有前途的方法，然而当前研究存在三个关键缺陷：数据集偏差、次优适应和任务覆盖不足。在本文中，我们解决了这些缺陷，并引入了AnyMC3D，一种从2D FMs适配的可扩展3D分类器。我们的方法通过在单一冻结主干上仅添加轻量级插件(每个任务约100万参数)来高效扩展到新任务。这个多功能的框架还支持多视图输入、辅助像素级监督和可解释热图生成。我们建立了一个包含12个任务的全面基准，涵盖不同的病理、解剖结构和模态，并系统分析了最先进的3D分类技术。我们的分析揭示了关键见解：(1)有效的适应对于释放FM潜力至关重要，(2)如果适当适应，通用FMs可以匹配医学专用FMs，以及(3)对于3D分类，基于2D的方法优于3D架构。我们首次证明了使用单一可扩展框架在各种应用中实现最先进性能的可行性(包括在VLM3D挑战赛中获得第一名)，消除了对单独的特定任务模型的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.</description>
      <author>example@mail.com (Han Liu, Bogdan Georgescu, Yanbo Zhang, Youngjin Yoo, Michael Baumgartner, Riqiang Gao, Jianing Wang, Gengyan Zhao, Eli Gibson, Dorin Comaniciu, Sasa Grbic)</author>
      <guid isPermaLink="false">2512.12887v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding</title>
      <link>http://arxiv.org/abs/2512.12842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAGA是一个通用且自适应的视觉运动控制框架，能够跨不同环境、任务目标和用户规范进行泛化，通过解耦高层语义意图与低层视觉运动控制实现高效学习。&lt;h4&gt;背景&lt;/h4&gt;机器人视觉运动控制面临泛化挑战，需要在各种环境和任务条件下有效工作，现有端到端和模块化方法在泛化能力上存在局限。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨多种环境、任务目标和用户规范进行泛化的视觉运动控制框架，实现高效学习和任务执行。&lt;h4&gt;方法&lt;/h4&gt;采用基于效用的任务表示法，利用多模态基础模型将任务表示锚定到机器人的视觉观察中生成3D效用热图，基于锚定效用训练条件策略实现全身控制，构建统一框架支持多种任务指定形式。&lt;h4&gt;主要发现&lt;/h4&gt;SAGA在十一个真实世界任务上的实验中显著优于端到端和模块化基线，结构化的效用锚定为通用移动操作提供了可扩展且有效的途径。&lt;h4&gt;结论&lt;/h4&gt;通过解耦高层语义意图与低层视觉运动控制，并基于观察到的环境明确锚定任务目标，SAGA实现了跨多种环境和任务的高效泛化。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了SAGA，一个通用且自适应的视觉运动控制框架，可以跨各种环境、任务目标和用户规范进行泛化。为了高效学习这种能力，我们的核心思想是通过在观察到的环境中明确锚定任务目标，将高层语义意图与低层视觉运动控制解耦。使用基于效用的任务表示，我们以统一、结构化的形式表达多样化和复杂的行为。通过利用多模态基础模型，SAGA将提出的任务表示锚定到机器人的视觉观察中，作为3D效用热图，突出显示任务相关实体，同时抽象出会阻碍泛化的表面外观变化。这些锚定的效用使我们能够有效地在多任务演示数据上训练条件策略，实现全身控制。在一个统一框架中，SAGA可以解决以不同形式指定的任务，包括语言指令、选定点和示例演示，实现零样本执行和少样本适应。我们在四足操作器上实例化了SAGA，并在十一个真实世界任务上进行了广泛实验。SAGA始终以显著优势优于端到端和模块化基线。这些结果共同表明，结构化的效用锚定为通用移动操作提供了一种可扩展且有效的途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让机器人在开放世界中灵活执行移动操作任务，并能泛化到各种环境、任务目标和用户指令形式的问题。这个问题在现实中很重要，因为它关系到机器人能否在家庭、办公室等真实环境中执行广泛任务，提高实用性；在研究中，它是实现真正通用机器人的关键挑战，因为当前系统通常只能在受控环境中执行特定任务，缺乏适应新环境的能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：端到端模型需要大量数据训练，而模块化框架在非结构化环境中不够强大。他们提出将高级语义意图与低级视觉运动控制解耦，通过明确将任务目标与环境相关联来实现开放世界操作。作者借鉴了多模态基础模型用于视觉识别、热力图作为空间任务表示、扩散策略用于视觉运动控制、基于功能的机器人表示以及上下文学习等现有工作，但通过结构化功能接地提供了更有效的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构化功能接地将高级语义意图与低级视觉运动控制解耦，实现开放世界移动操作。整体流程包括：1)任务表示：使用'功能-实体对'表示任务目标；2)功能接地：将任务表示与视觉观察关联为3D功能热力图；3)条件策略：基于热力图点云训练条件扩散策略预测动作；4)多样化接口：支持语言指令、选定点和示例演示三种用户输入方式；5)移动操作系统：在四足操作平台上实现全身控制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)结构化功能任务表示，支持多种交互类型和复杂任务组合；2)3D功能热力图接地，明确将任务目标关联到环境；3)多样化用户接口，支持零样本执行和少样本适应；4)高效通用移动操作，使用更少数据实现更好泛化。相比端到端方法，SAGA不需要将所有处理压缩在黑盒模型中；相比模块化方法，不依赖手工设计的低级模块；相比仅关注抓取的方法，支持更广泛的交互类型；相比传统热力图方法，提供更丰富的语义表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAGA通过结构化功能接地将高级语义意图与低级视觉运动控制解耦，实现了一种高效、通用的开放世界移动操作框架，仅需少量演示数据即可在各种环境和任务目标中表现出强大的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.</description>
      <author>example@mail.com (Kuan Fang, Yuxin Chen, Xinghao Zhu, Farzad Niroui, Lingfeng Sun, Jiuguang Wang)</author>
      <guid isPermaLink="false">2512.12842v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Generative Spatiotemporal Data Augmentation</title>
      <link>http://arxiv.org/abs/2512.12508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索使用视频基础模型进行时空数据增强，以多样化相机视角和场景动态，并在低数据设置中取得了性能提升。&lt;h4&gt;背景&lt;/h4&gt;现有的数据增强方法主要基于简单的几何变换或外观扰动，难以生成多样化的3D空间和时间变化。&lt;h4&gt;目的&lt;/h4&gt;利用现成的视频扩散模型从给定的图像数据集中生成真实的3D空间和时间变化，作为补充训练数据以提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;利用现成的视频扩散模型生成合成视频片段，将其作为补充训练数据，并提供实践指导包括选择适当的时空生成设置、将标注转移到合成帧以及处理遮挡区域。&lt;h4&gt;主要发现&lt;/h4&gt;在COCO子集和无人机捕获数据集上的实验表明，合理应用时空增强可以沿着传统和先前生成方法代表性不足的轴扩展数据分布。&lt;h4&gt;结论&lt;/h4&gt;时空数据增强为提高数据稀缺情况下的模型性能提供了有效的手段。&lt;h4&gt;翻译&lt;/h4&gt;我们探索使用视频基础模型进行时空数据增强，以多样化相机视角和场景动态。与基于简单几何变换或外观扰动的现有方法不同，我们的方法利用现成的视频扩散模型从给定的图像数据集中生成真实的3D空间和时间变化。将这些合成的视频片段作为补充训练数据，在低数据设置（如标注稀缺的无人机捕获图像）中能带来一致的性能提升。除了实证改进，我们还提供了实践指导，包括选择适当的时空生成设置、将标注转移到合成帧以及处理遮挡区域（在生成的视图中新出现且未标记的区域）。在COCO子集和无人机捕获数据集上的实验表明，合理应用时空增强可以沿着传统和先前生成方法代表性不足的轴扩展数据分布，为提高数据稀缺情况下的模型性能提供了有效的手段。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决计算机视觉任务中数据稀缺和多样性不足的问题。在现实世界中，收集多视角、多场景动态的数据（特别是无人机、机器人等领域）需要昂贵设备和大量人力，手动标注成本高昂。传统数据增强方法仅能进行简单几何变换或外观扰动，无法有效增加数据的时空多样性，限制了模型在数据有限场景下的性能表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现代视频扩散模型编码了强大的隐式3D和动态先验知识，可以直接用于感知任务。他们思考如何利用这些现成的视频扩散模型作为数据增强引擎，从单张输入图像生成新的视角和合理的时间变化。方法设计借鉴了现有工作：利用DimensionX和LTX-Video-13B等视频扩散模型进行数据生成，使用SAM 2视频分割模型进行自动标注，以及采用TAPNext和All-Tracker等点跟踪技术处理遮挡问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用现成的视频扩散模型作为时空数据增强引擎，从单张图像生成具有不同视角和动态变化的视频序列，并通过自动化标注传递将原始图像的标注信息传递到生成的视频帧中。整体流程包括：1) 数据生成阶段，使用视频扩散模型生成空间视角变化或时间动态变化的视频；2) 标注传递阶段，使用视频分割模型跟踪物体并生成边界框标注；3) 遮挡处理阶段，通过点跟踪确定可见区域；4) 模型训练阶段，结合原始数据和生成数据进行训练，保持1:1采样比例。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次证明现成的视频扩散模型可以生成合成数据并显著提高物体检测性能；2) 提供完全自动化的标注传递流程，利用时间一致性为所有生成帧生成准确边界框；3) 通过大量实验提供关于如何有效使用时空增强的实用指南。相比之前工作，本文不仅关注视觉外观多样化，更专注于空间视角和时间动态的多样性；不局限于位置不变的增强或有给定边界框的物体生成，而是生成连续视频帧并使用基础模型进行自动标注，提供了更大的灵活性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于视频扩散模型的时空数据增强方法，通过从单张图像生成多视角和动态变化的视频序列，显著提高了数据稀缺环境下物体检测的性能，并提供了实用的自动化标注传递流程。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.</description>
      <author>example@mail.com (Jinfan Zhou, Lixin Luo, Sungmin Eum, Heesung Kwon, Jeong Joon Park)</author>
      <guid isPermaLink="false">2512.12508v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation</title>
      <link>http://arxiv.org/abs/2512.12425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BokehDepth是一个两阶段框架，解耦了Bokeh合成与深度预测，将散焦作为辅助的无监督几何线索，在提高Bokeh渲染质量的同时增强了单目深度估计的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;Bokeh和单目深度估计通过相同的镜头成像几何紧密耦合，但当前方法利用这种连接不完整。高质量Bokeh渲染依赖噪声深度图，放大误差为可见伪影；现代单目深度模型在弱纹理、远距离和几何模糊区域表现不佳，而这些区域恰恰是散焦线索最有信息量的地方。&lt;h4&gt;目的&lt;/h4&gt;解决Bokeh合成与深度预测之间的耦合问题，将散焦作为一种辅助的、无需监督的几何线索，同时提高Bokeh渲染质量和深度估计准确性。&lt;h4&gt;方法&lt;/h4&gt;提出BokehDepth两阶段框架：第一阶段使用物理引导的可控Bokeh生成器，基于预训练图像编辑主干网络从单张锐利图像生成无深度Bokeh堆栈；第二阶段将轻量级散焦感知聚合模块插入现有单目深度编码器，沿散焦维度融合特征，同时保持下游解码器不变。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的基准测试中，BokehDepth相比基于深度图的Bokeh基线方法提高了视觉保真度，并一致地增强了强大单目深度基础模型的度量准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过解耦Bokeh合成与深度预测，并利用散焦作为辅助几何线索，可以同时改善Bokeh渲染和深度估计的质量，解决当前方法中存在的局限性。&lt;h4&gt;翻译&lt;/h4&gt;Bokeh和单目深度估计通过相同的镜头成像几何紧密耦合，然而当前方法以不完整的方式利用这种连接。高质量的Bokeh渲染流程通常依赖于噪声深度图，这会将估计误差放大为可见的伪影，而现代单目度量深度模型在弱纹理、远距离和几何模糊区域仍然表现不佳，这些区域是散焦线索最有信息量的地方。我们引入了BokehDepth，一个两阶段框架，它将Bokeh合成与深度预测解耦，并将散焦作为辅助的无监督几何线索。在第一阶段，基于强大的预训练图像编辑主干网络构建的物理引导的可控Bokeh生成器，从单个锐利输入生成无深度的Bokeh堆栈，并校准Bokeh强度。在第二阶段，一个轻量级的散焦感知聚合模块插入现有的单目深度编码器中，沿散焦维度融合特征，同时暴露稳定的深度敏感变化，且保持下游解码器不变。在具有挑战性的基准测试中，BokehDepth相比基于深度图的Bokeh基线方法提高了视觉保真度，并一致地增强了强大单目深度基础模型的度量准确性和鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目深度估计与散景生成之间的耦合关系问题。现有高质量散景渲染依赖噪声深度图，会放大深度估计误差；而现代单目深度模型在弱纹理、远距离和几何模糊区域表现不佳，这些区域恰恰是散景线索最有信息量的地方。这个问题很重要，因为深度估计是计算机视觉的基础任务，对3D场景理解、自动驾驶、增强现实等应用至关重要，而散景效果影响图像美观性和主体突出效果，两者间的物理联系尚未被充分利用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到散景和单目深度估计通过镜头成像几何紧密相连，但现有方法利用这种联系不完整。核心洞察是将散景合成与深度预测解耦，利用散焦作为无监督几何线索。方法采用两阶段框架：第一阶段基于FLUX-Kontext图像编辑模型添加散景交叉注意力适配器生成散景堆栈；第二阶段将Divided Space Focus Attention模块插入现有深度编码器融合散焦线索。借鉴了薄透镜模型、扩散模型和现有深度估计模型的工作，通过统一的散景强度参数K对齐真实和合成数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将散景生成与深度估计解耦，利用物理引导的散景生成提供无监督几何线索增强深度估计。整体流程分两阶段：第一阶段基于FLUX-Kontext，通过物理模型生成不同强度K的散景堆栈；第二阶段将原始图像和散景堆栈输入深度编码器，插入DSFA模块融合散焦特征，保留参考帧特征通过原解码器输出增强后的深度图。DSFA模块包含空间注意力(帧内)和焦点注意力(帧间)，利用散景堆栈中的变化作为深度线索。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)解耦的散景-深度框架，避免传统方法中深度误差放大问题；2)物理引导的无监督散景生成，基于薄透镜模型统一控制参数K；3)散焦感知的DSFA模块，可插入现有深度编码器增强特征融合；4)跨域泛化能力，能提升多种基础深度模型性能。相比之前工作：不依赖深度图的散景生成减少伪影；利用散景线索改善弱纹理区域深度估计；仅需单张图像而非多孔径对；物理模型确保散景与深度间一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BokehDepth通过解耦散景生成与深度估计，并利用物理引导的散景堆栈作为无监督几何线索，显著提升了单目深度估计的准确性和鲁棒性，特别是在传统方法表现不佳的弱纹理和远距离区域。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.</description>
      <author>example@mail.com (Hangwei Zhang, Armando Teles Fortes, Tianyi Wei, Xingang Pan)</author>
      <guid isPermaLink="false">2512.12425v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining</title>
      <link>http://arxiv.org/abs/2512.12384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了在金融领域继续预训练大型语言模型的效果，发现即使是中等规模的模型也能有效适应特定领域，且不会损失通用能力。&lt;h4&gt;背景&lt;/h4&gt;领域自适应预训练(DAPT)是一种在不完全重新训练的情况下，使大型语言模型适应高价值领域的实用方法。&lt;h4&gt;目的&lt;/h4&gt;研究在特定金融领域上继续预训练大型语言模型的效果，探索不同参数规模模型在不同训练量下的表现。&lt;h4&gt;方法&lt;/h4&gt;使用400M token的金融语料库训练10亿和30亿参数的Llama-3.2模型，在50M、100M、200M和400M token处设置验证检查点，并进行幂律拟合分析。&lt;h4&gt;主要发现&lt;/h4&gt;两个模型在SEC领域验证损失持续改善，最大收益出现在前200M token；幂律拟合显示金融语言高度规则且可有效学习；通用领域验证损失基本不变，表明没有灾难性遗忘；模型向专业化方向发展，几乎没有混合领域退化。&lt;h4&gt;结论&lt;/h4&gt;有意义的领域适应可以通过相对适度的token预算实现；在预期的数据需求下，更大规模的模型(7B-70B)仍然可行。&lt;h4&gt;翻译&lt;/h4&gt;领域自适应预训练(DAPT)为在不完全重新训练的情况下使大型语言模型专业化应用于高价值领域提供了实用路径。我们在美国SEC文件上进行了持续预训练的早期扩展定律分析，使用400M token的金融语料库训练了10亿和30亿参数的Llama-3.2模型，并在50M、100M、200M和400M token处设置了验证检查点。结果显示两个模型在SEC领域验证损失方面均有持续改善，最大收益出现在前200M token，之后收益递减。幂律拟合显示指数较浅，表明金融语言高度规则，在持续预训练下可有效学习。通用领域验证损失在所有token预算下基本保持不变，表明最小漂移和没有灾难性遗忘的迹象。数据效率前沿进一步显示，两个模型都向改善专业化方向发展，几乎没有混合领域退化。总之，这些发现为扩展金融基础模型提供了早期的经验指导，表明有意义的领域适应可以通过相对适度的token预算实现，且在预期的数据需求下更大规模的模型(7B-70B)仍然可行。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.</description>
      <author>example@mail.com (Jesse Ponnock)</author>
      <guid isPermaLink="false">2512.12384v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training</title>
      <link>http://arxiv.org/abs/2512.12210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI-2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出EEG-DLite数据蒸馏框架，通过选择性去除大型EEG数据集中的噪声和冗余样本，实现更高效的预训练，仅使用5%的数据即可达到与完整数据集相当甚至更好的性能。&lt;h4&gt;背景&lt;/h4&gt;大规模EEG基础模型在多种下游任务中表现出强大的泛化能力，但由于EEG数据的数量大、质量参差不齐，其训练仍然非常消耗资源。&lt;h4&gt;目的&lt;/h4&gt;开发一种数据蒸馏框架，能够从大型EEG数据集中高效筛选出高质量、低冗余的数据子集，减少训练资源需求同时保持模型性能。&lt;h4&gt;方法&lt;/h4&gt;EEG-DLite首先使用自监督自编码器将EEG段编码为紧凑的潜在表示，然后基于这些表示过滤异常值并最小化冗余，得到一个更小但信息丰富的子集，保留了有效基础模型训练所需的多样性。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验证明，仅使用EEG-DLite筛选的2500小时数据集中5%的数据进行训练，在多个下游任务上可以与使用完整数据集训练的性能相媲美，甚至在某些情况下更好。&lt;h4&gt;结论&lt;/h4&gt;EEG-DLite为更有效和高效的生理基础建模提供了可扩展且实用的途径，这是首次对EEG基础模型预训练数据蒸馏的系统性研究。&lt;h4&gt;翻译&lt;/h4&gt;大规模脑电图基础模型已在多种下游任务中展现出强大的泛化能力，但由于脑电图数据的数量和质量参差不齐，其训练仍然资源密集。在本工作中，我们引入了EEG-DLite，这是一种数据蒸馏框架，通过从大型脑电图数据集中选择性去除噪声和冗余样本，实现更高效的预训练。EEG-DLite首先使用自监督自编码器将脑电图段编码为紧凑的潜在表示，使样本选择能够高效进行并降低对噪声的敏感性。基于这些表示，EEG-DLite过滤异常值并最小化冗余，产生一个更小但信息丰富的子集，保留了有效基础模型训练所需的多样性。通过大量实验，我们证明仅使用EEG-DLite筛选的2500小时数据集中5%的数据进行训练，在多个下游任务上能够达到与使用完整数据集训练相当甚至更好的性能。据我们所知，这是首次在脑电图基础模型背景下对预训练数据蒸馏的系统性研究。EEG-DLite为更有效和高效的生理基础建模提供了可扩展且实用的途径。代码可在https://github.com/t170815518/EEG-DLite获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.</description>
      <author>example@mail.com (Yuting Tang, Weibang Jiang, Shanglin Li, Yong Li, Chenyu Liu, Xinliang Zhou, Yi Ding, Cuntai Guan)</author>
      <guid isPermaLink="false">2512.12210v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography</title>
      <link>http://arxiv.org/abs/2512.12107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EchoVLM，一种基于新型数据集EchoGround-MIMIC的视觉语言模型，用于超声心动图解释。该模型结合了视图信息对比损失和否定感知对比损失，在多种临床任务上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;超声心动图是心脏病学中最广泛使用的成像方式，但其解释劳动密集且多模态。现有视觉语言模型在超声心动图领域的应用受到缺乏大规模临床数据集和基于测量的推理的限制。&lt;h4&gt;目的&lt;/h4&gt;创建首个基于测量的多模态超声心动图数据集EchoGround-MIMIC，并开发EchoVLM视觉语言模型以实现端到端的超声心动图解释。&lt;h4&gt;方法&lt;/h4&gt;构建包含19,065个图像-文本对的数据集，标准化视图、结构化测量和疾病标签。EchoVLM采用两种新型预训练目标：视图信息对比损失和否定感知对比损失。&lt;h4&gt;主要发现&lt;/h4&gt;EchoVLM在36项任务上实现最先进性能，包括零样本疾病分类(AUC 86.5%)和视图分类(准确率95.1%)。临床基础的多模态预训练产生可转移的视觉表示。&lt;h4&gt;结论&lt;/h4&gt;EchoVLM被确立为端到端超声心动图解释的基础模型，EchoGround-MIMIC和数据整理代码将被发布以促进可重复性和进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;超声心动图是心脏病学中最广泛使用的成像方式，但其解释仍然劳动密集且本质上多模态，需要视图识别、定量测量、定性评估和基于指南的推理。虽然最近的视觉语言模型在自然图像和某些医学领域取得了广泛成功，但它们在超声心动图方面的潜力受到缺乏大规模、临床基础的图像-文本数据集以及缺乏超声解释中基于测量的推理的限制。我们介绍了EchoGround-MIMIC，第一个基于测量的多模态超声心动图数据集，包含来自1,572名患者的19,065个图像-文本对，具有标准化视图、结构化测量、基于测量的标题和从指南中派生的疾病标签。基于此资源，我们提出了EchoVLM，一种视觉语言模型，它包含两个新的预训练目标：(i) 编码超声心动图成像视图相关结构的视图信息对比损失，以及(ii) 区分临床上关键的阴性发现和阳性发现的否定感知对比损失。在涵盖多模态疾病分类、图像-文本检索、视图分类、心室分割和landmark检测的五类临床应用和36项任务中，EchoVLM取得了最先进的性能（零样本疾病分类的AUC为86.5%，视图分类的准确率为95.1%）。我们证明了临床基础的多模态预训练产生可转移的视觉表示，并将EchoVLM确立为端到端超声心动图解释的基础模型。我们将发布EchoGround-MIMIC和数据整理代码，实现可重复性和多模态超声心动图解释的进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.</description>
      <author>example@mail.com (Yuheng Li, Yue Zhang, Abdoul Aziz Amadou, Yuxiang Lai, Jike Zhong, Tiziano Passerini, Dorin Comaniciu, Puneet Sharma)</author>
      <guid isPermaLink="false">2512.12107v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer</title>
      <link>http://arxiv.org/abs/2512.12083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RePack的表示打包框架，用于解决预训练视觉基础模型增强扩散模型时的高维度表示导致的信息过载问题。RePack通过将VFM表示投影到低维流形，实现更紧凑的表示，加速模型收敛并提高生成性能。&lt;h4&gt;背景&lt;/h4&gt;预训练视觉基础模型具有优异的表示能力，已被用于增强潜在扩散模型。这些方法将高维VFM表示注入到LDMs的不同阶段，加速学习并提高生成性能。然而，VFM表示的高维度可能导致信息过载，特别是当VFM特征超过原始图像大小时进行解码。&lt;h4&gt;目的&lt;/h4&gt;解决VFM表示的高维度导致的信息过载问题，同时保留VFM特征的效用，提高扩散模型特别是DiTs的性能和收敛速度。&lt;h4&gt;方法&lt;/h4&gt;提出RePack框架，通过将VFM表示投影到低维流形，将其转换为更紧凑的、解码器友好的表示。这种方法能够过滤非语义噪声，同时保留高保真重建所需的核心结构信息。&lt;h4&gt;主要发现&lt;/h4&gt;RePack可以有效过滤非语义噪声，保留核心结构信息，显著加速DiT收敛。实验表明，RePack性能优于直接将原始VFM特征注入解码器的方法。在DiT-XL/2上，仅用64个周期就实现了3.66的FID，比最先进方法快35%。&lt;h4&gt;结论&lt;/h4&gt;RePack成功提取了VFM表示的核心语义，同时避免了其高维度的副作用，是一种简单有效的改进扩散模型的方法。&lt;h4&gt;翻译&lt;/h4&gt;预训练视觉基础模型的优异表示能力已被用于增强潜在扩散模型。这些方法在不同阶段将高维VFM表示注入LDMs，从而加速学习并提高生成性能。然而，VFM表示的高维度也可能导致信息过载，特别是当VFM特征超过原始图像大小时进行解码。为解决这一问题同时保留VFM特征的效用，我们提出了RePack，一个简单有效的改进扩散Transformer的框架。RePack通过将VFM表示投影到低维流形，将其转换为更紧凑、解码器友好的表示。我们发现RePack可以有效过滤非语义噪声，同时保留高保真重建所需的核心结构信息。实验结果表明，RePack显著加速了DiT收敛，并优于最近将原始VFM特征直接注入解码器进行图像重建的方法。在DiT-XL/2上，RePack仅用64个周期就实现了3.66的FID，比最先进方法快35%。这证明了RePack成功提取了VFM表示的核心语义，同时避免了其高维度的副作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.</description>
      <author>example@mail.com (Guanfang Dong, Luke Schultz, Negar Hassanpour, Chao Gao)</author>
      <guid isPermaLink="false">2512.12083v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction</title>
      <link>http://arxiv.org/abs/2512.11988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CARI4D，首个类别无关的方法，可以从单目RGB视频中重建空间和时间一致的4D人-物交互，达到度量尺度。&lt;h4&gt;背景&lt;/h4&gt;从常见的RGB传感器准确捕捉人-物交互对于人类理解、游戏和机器人学习应用非常重要。&lt;h4&gt;目的&lt;/h4&gt;解决从单一RGB视图推断4D交互的挑战，包括未知物体和人类信息、深度歧义、遮挡和复杂运动等问题。&lt;h4&gt;方法&lt;/h4&gt;提出姿态假设选择算法，稳健集成基础模型的个体预测；通过学习的渲染和比较范式联合优化预测，确保空间、时间和像素对齐；推理复杂接触点以进一步优化，满足物理约束。&lt;h4&gt;主要发现&lt;/h4&gt;在重建误差方面，CARI4D在分布内数据集上比之前的方法好38%，在未见过的数据集上好36%；模型可以泛化到训练类别之外，可以零样本应用于野外网络视频。&lt;h4&gt;结论&lt;/h4&gt;CARI4D是首个类别无关的4D人-物交互重建方法，能够从单目RGB视频中重建空间和时间一致的交互，具有很好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;从常见的RGB传感器准确捕捉人-物交互对于人类理解、游戏和机器人学习应用非常重要。然而，由于物体和人类信息未知、深度歧义、遮挡和复杂运动等因素，从单一RGB视图推断4D交互极具挑战性，这阻碍了一致的3D和时间重建。先前的方法通过假设真实物体模板或限制到有限物体类别集合来简化设置。我们提出了CARI4D，这是首个类别无关的方法，可以从单目RGB视频中重建空间和时间一致的4D人-物交互，达到度量尺度。为此，我们提出了一种姿态假设选择算法，稳健地集成基础模型的个体预测，通过学习的渲染和比较范式联合优化它们，确保空间、时间和像素对齐，最后推理复杂的接触点以进一步优化，满足物理约束。实验表明，在重建误差方面，我们的方法在分布内数据集上比之前的方法好38%，在未见过的数据集上好36%。我们的模型泛化能力超越了训练类别，因此可以零样本应用于野外网络视频。我们的代码和预训练模型将公开发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从单目RGB视频中重建四维(3D空间+时间)人类-物体交互的问题。这个问题在现实中很重要，因为传统的人类-物体交互捕获需要昂贵且繁琐的多视角相机设置，难以扩展；而单目RGB视频丰富易得，能快速经济地获取数据，对人类理解、游戏、机器人学习等应用至关重要。现有方法要么需要真实物体模板，要么限制在特定物体类别，缺乏泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出从单目RGB视频中重建人类-物体交互的挑战：未知物体和人类信息、深度歧义、遮挡和复杂运动。他们指出现有方法的局限性，然后设计了一个类别无关的方法。作者借鉴了多个基础模型：使用Hunyuan3D-2进行物体重建，UniDepth进行深度估计，FoundationPose进行物体姿态估计，NLF进行人体姿态估计。将这些模型的预测整合到一个统一框架中，并设计了特定算法处理不一致性和噪声。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个框架整合基础模型预测获得鲁棒初始化，再通过类别无关的交互推理模块提高接触一致性，使用渲染和比较范式优化人类和物体姿态。整体流程包括：1)物体重建和度量尺度估计；2)人类和物体姿态初始化；3)接触推理和细化(CoCoNet)；4)基于接触的联合优化。这种方法能够在不依赖物体模板的情况下，从单目RGB视频中重建空间和时间一致的4D人类-物体交互。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个类别无关的4D人类-物体交互重建方法；2)姿态假设选择算法，能在严重遮挡下鲁棒跟踪物体姿态；3)CoCoNet，类别无关的接触推理模型；4)接触感知的联合优化框架。相比之前工作，CARI4D不需要真实物体模板或预定义类别，能处理各种物体类别，实现零样本泛化，在空间和时间维度上保持一致重建，并能处理复杂遮挡，在重建误差上比现有方法提高36-38%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CARI4D首次实现了从单目RGB视频中类别无关地重建四维人类-物体交互，无需物体模板即可处理未见过的物体类别，并在重建精度和泛化能力上显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.</description>
      <author>example@mail.com (Xianghui Xie, Bowen Wen, Yan Chang, Hesam Rabeti, Jiefeng Li, Ye Yuan, Gerard Pons-Moll, Stan Birchfield)</author>
      <guid isPermaLink="false">2512.11988v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>HMPCC: Human-Aware Model Predictive Coverage Control</title>
      <link>http://arxiv.org/abs/2512.12717v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于模型预测控制的人机感知覆盖框架(HMPCC)，用于协调机器人团队在未知环境中安全高效地进行覆盖任务，同时避免与人类等非合作代理的碰撞。&lt;h4&gt;背景&lt;/h4&gt;传统覆盖策略通常依赖简化假设，如已知或凸环境以及静态密度函数，难以适应真实世界场景，特别是当涉及人类时。在人类存在的情况下，机器人团队协调覆盖环境面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决机器人团队协调覆盖未知环境的问题，确保安全操作并避免与非合作代理碰撞，使机器人能够适应动态条件，特别是与人类互动的情况。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于模型预测控制(MPC)的人机感知覆盖框架(HMPCC)，将人类运动预测集成到规划过程中；在MPC范围内预测人类轨迹，使机器人能够主动协调行动；环境建模为高斯混合模型(GMM)；团队成员以完全去中心化的方式运作，不依赖显式通信。&lt;h4&gt;主要发现&lt;/h4&gt;人类轨迹预测使覆盖更加高效和自适应，改善了人类和机器人代理之间的协调。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够更好地适应动态环境，特别是与人类互动的情况；去中心化方法在通信受限或敌对场景中具有优势。&lt;h4&gt;翻译&lt;/h4&gt;我们解决了协调机器人团队覆盖未知环境的问题，同时确保安全操作并避免与非合作代理的碰撞。传统的覆盖策略通常依赖简化假设，如已知或凸环境以及静态密度函数，并且难以适应真实世界场景，特别是当涉及人类时。在这项工作中，我们提出了一种基于模型预测控制(MPC)的人机感知覆盖框架，即HMPCC，其中人类运动预测被集成到规划过程中。通过在MPC范围内预测人类轨迹，机器人可以主动协调其行动，避免冗余探索，并适应动态条件。环境被建模为高斯混合模型(GMM)，表示感兴趣区域。团队成员以完全去中心化的方式运作，不依赖显式通信，这在敌对或通信受限场景中是 essential 特征。我们的结果表明，人类轨迹预测使覆盖更加高效和自适应，改善了人类和机器人代理之间的协调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.</description>
      <author>example@mail.com (Mattia Catellani, Marta Gabbi, Lorenzo Sabattini)</author>
      <guid isPermaLink="false">2512.12717v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2512.11872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了WAM-Diff，一个基于掩码扩散的视觉-语言-动作框架，用于自动驾驶中的轨迹生成，通过三个创新实现了高性能：掩码扩散的系统性适应、稀疏MoE架构扩展模型容量，以及使用GSPO的在线强化学习。&lt;h4&gt;背景&lt;/h4&gt;基于视觉-语言-动作模型的端到端自动驾驶系统整合多模态传感器输入和语言指令以生成规划和控制信号。虽然自回归大语言模型和连续扩散策略很普遍，但离散掩码扩散在轨迹生成方面的潜力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;探索离散掩码扩散在自动驾驶轨迹生成中的应用潜力，开发一个有效的VLA框架，能够处理多模态输入并生成高质量的轨迹规划。&lt;h4&gt;方法&lt;/h4&gt;提出WAM-Diff框架，采用掩码扩散来迭代优化表示未来自车轨迹的离散序列。方法包括：掩码扩散的系统性适应以支持灵活的非因果解码顺序；通过稀疏MoE架构扩展模型容量，该架构在运动预测和面向驾驶的视觉问答上进行联合训练；使用组序列策略优化进行在线强化学习，以优化序列级别的驾驶奖励。&lt;h4&gt;主要发现&lt;/h4&gt;在NAVSIM-v1上达到91.0 PDMS，在NAVSIM-v2上达到89.7 EPDMS，证明了掩码扩散在自动驾驶中的有效性。该方法是自回归和基于扩散的策略的有前途的替代方案，支持场景感知的解码策略用于轨迹生成。&lt;h4&gt;结论&lt;/h4&gt;掩码扩散在自动驾驶轨迹生成中具有巨大潜力，WAM-Diff框架通过三个关键创新实现了高性能，为自动驾驶领域提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉-语言-动作模型的端到端自动驾驶系统整合多模态传感器输入和语言指令以生成规划和控制信号。虽然自回归大语言模型和连续扩散策略很普遍，但离散掩码扩散在轨迹生成方面的潜力尚未得到充分探索。本文提出了WAM-Diff，一个采用掩码扩散迭代优化表示未来自车轨迹的离散序列的VLA框架。我们的方法有三个关键创新：系统性地适应自动驾驶的掩码扩散，支持灵活的非因果解码顺序；通过稀疏MoE架构扩展模型容量，该架构在运动预测和面向驾驶的视觉问答上进行联合训练；使用组序列策略优化进行在线强化学习，以优化序列级别的驾驶奖励。值得注意的是，我们的模型在NAVSIM-v1上达到91.0 PDMS，在NAVSIM-v2上达到89.7 EPDMS，证明了掩码扩散在自动驾驶中的有效性。该方法为自回归和基于扩散的策略提供了有前途的替代方案，支持场景感知的解码策略用于轨迹生成。本文的代码将在https://github.com/fudan-generative-vision/WAM-Diff公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff</description>
      <author>example@mail.com (Mingwang Xu, Jiahao Cui, Feipeng Cai, Hanlin Shang, Zhihao Zhu, Shan Luan, Yifang Xu, Neng Zhang, Yaoyi Li, Jia Cai, Siyu Zhu)</author>
      <guid isPermaLink="false">2512.11872v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj</title>
      <link>http://arxiv.org/abs/2511.18248v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures, accepted to the AI4TS Workshop at AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为CausalTraj的时间因果似然模型，用于解决多交互智能体联合轨迹预测的挑战，该模型在保持个体准确度的同时，显著提升了联合预测能力，能够生成连贯、真实的比赛演变。&lt;h4&gt;背景&lt;/h4&gt;多交互智能体的联合轨迹预测是体育分析等涉及复杂群体动态领域的核心挑战。现有模型主要使用每个智能体的独立准确度指标(minADE, minFDE)进行评估，这些指标忽略了模型是否学习到哪些预测轨迹可以共同形成合理的多智能体未来。&lt;h4&gt;目的&lt;/h4&gt;提出CausalTraj模型，一个基于时间因果关系的似然模型，旨在生成共同可能的多智能体轨迹预测，并引入联合指标(minJADE, minJFDE)来更好地评估集体建模能力。&lt;h4&gt;方法&lt;/h4&gt;CausalTraj是一个基于时间因果关系的似然模型，专门设计用于生成共同可能的多智能体轨迹预测。研究使用联合指标评估模型在最佳生成的场景样本中跨智能体的联合准确度。&lt;h4&gt;主要发现&lt;/h4&gt;在NBA SportVU、Basketball-U和Football-U数据集上评估，CausalTraj实现了具有竞争力的每个智能体准确度，同时在联合指标上取得了最佳记录结果，并能产生质量上连贯且真实的比赛演变。&lt;h4&gt;结论&lt;/h4&gt;CausalTraj模型有效解决了多智能体联合轨迹预测的挑战，不仅个体准确度有竞争力，还在联合预测方面表现优异，能够生成连贯、真实的比赛演变，适用于体育分析等领域。&lt;h4&gt;翻译&lt;/h4&gt;多交互智能体的联合轨迹预测是体育分析和其他涉及复杂群体动态领域的核心挑战。准确的预测可以实现真实的模拟和对比赛演变的战略理解。大多数现有模型仅使用每个智能体的准确度指标(minADE, minFDE)进行评估，这些指标独立评估每个智能体在其k个预测中的最佳表现。然而，这些指标忽略了模型是否学习到哪些预测的轨迹可以共同形成合理的多智能体未来。许多最先进的模型主要基于这些指标进行设计和优化。因此，它们在联合预测方面可能表现不佳，并且在团队体育中无法生成连贯、可解释的多智能体场景。我们提出CausalTraj，一个基于时间因果关系的似然模型，旨在生成共同可能的多智能体轨迹预测。为了更好地评估集体建模能力，我们强调联合指标(minJADE, minJFDE)，这些指标衡量在最佳生成的场景样本中跨智能体的联合准确度。在NBA SportVU、Basketball-U和Football-U数据集上评估，CausalTraj实现了具有竞争力的每个智能体准确度和最佳的联合指标记录结果，同时产生了质量上连贯且真实的比赛演变。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.</description>
      <author>example@mail.com (Wei Zhen Teoh)</author>
      <guid isPermaLink="false">2511.18248v2</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT</title>
      <link>http://arxiv.org/abs/2512.11870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了休斯顿市如何减少道路交通排放以实现2050年净零排放目标，提出利用社会经济指标和智能交通系统加速零排放车辆采用和减少车辆行驶里程的策略。&lt;h4&gt;背景&lt;/h4&gt;全球道路交通占温室气体排放15%，导致约385,000人因PM2.5过早死亡；城市贡献75%全球能源相关温室气体排放；休斯顿道路交通占其气候行动计划基线排放48%；休斯顿是低密度、依赖私家车的城市，89%道路排放来自汽车和小型卡车，公共交通使用有限；社会经济差异进一步限制零排放车辆采用。&lt;h4&gt;目的&lt;/h4&gt;建立道路交通排放基线方法；评估利用社会经济指标和智能交通系统加速零排放车辆采用和减少车辆行驶里程的政策；分析政策选项并确定潜在行动。&lt;h4&gt;方法&lt;/h4&gt;开发基于Unity 3D的模拟环境，支持城市移动动态建模和政策场景可视化；研究智能停车、公共交通激励、安全数据系统和零排放车辆车队管理等策略；这些策略旨在改善交通模式划分和系统可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;休斯顿气候行动计划设定以2014年为基准减少70%排放目标，通过30%可再生能源抵消；休斯顿低密度和私家车依赖性使目标具挑战性；策略重点是扩大零排放车辆获取渠道，通过改善公共交通和城市设计将车辆行驶里程减少20%。&lt;h4&gt;结论&lt;/h4&gt;依赖私家车的城市若要实现2050年排放目标，可从论文讨论的指标、度量和技术中受益；论文提供了评估和实施政策以加速零排放车辆采用和减少车辆行驶里程的方法。&lt;h4&gt;翻译&lt;/h4&gt;全球范围内，道路交通占温室气体排放的15%，并估计有385,000人因PM2.5而过早死亡。城市在实现IPCC目标中发挥着关键作用，产生了75%的全球能源相关温室气体排放。在德克萨斯州的休斯顿，道路交通占气候行动计划基线排放的48%。为了在2050年实现净零排放，气候行动计划设定了以2014年为基准减少70%排放的目标，并通过30%的可再生能源抵消。这一目标具有挑战性，因为休斯顿是低密度且依赖私家车的城市，89%的道路排放来自汽车和小型卡车，公共交通使用有限。社会经济差异进一步限制了零排放车辆的采用。策略重点是扩大ZEV的获取渠道，并通过改善公共交通和城市设计将车辆行驶里程减少20%。本文介绍了建立道路交通排放基线和评估利用社会经济指标和智能交通系统加速ZEV采用和减少VMT的政策的方法。智能停车、公共交通激励、安全数据系统和ZEV车队管理等支持改善模式划分和系统可靠性。分析了政策选项并确定了潜在行动。为了支持评估，在Unity 3D中开发了一个模拟环境，能够实现城市移动的动态建模和政策场景的可视化。旨在实现2050年排放目标的依赖私家车的城市可以从讨论的指标、度量和技术中受益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-12-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.</description>
      <author>example@mail.com (Mulham Fawkherji, Bruce Race, Driss Benhaddou)</author>
      <guid isPermaLink="false">2512.11870v1</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    <item>
      <title>Establishing Reality-Virtuality Interconnections in Urban Digital Twins for Superior Intelligent Road Inspection and Simulation</title>
      <link>http://arxiv.org/abs/2412.17699v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种结合多模态传感器平台与城市数字孪生系统的智能道路检测方法，用于解决传统道路检测方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;道路检测对维护道路服务性和确保交通安全至关重要，但传统人工评估方法劳动密集、成本高且耗时。数据驱动方法面临真实世界道路缺陷数据稀缺和空间稀疏的挑战，现有模拟器缺乏道路缺陷模型，且涉及道路表面交互的高级驾驶任务尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决传统道路检测方法的局限性，提出一种多模态传感器平台与城市数字孪生系统集成的智能道路检测方法。&lt;h4&gt;方法&lt;/h4&gt;使用车载传感器收集真实世界驾驶数据构建分层道路模型；生成数字道路孪生创建模拟环境；将这些场景导入模拟器促进数据采集和物理模拟。&lt;h4&gt;主要发现&lt;/h4&gt;高保真道路缺陷场景显著改善了包括感知和决策在内的驾驶任务性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的多模态传感器平台与城市数字孪生系统集成的智能道路检测方法能够生成高质量的道路缺陷场景，对道路检测任务有显著帮助。&lt;h4&gt;翻译&lt;/h4&gt;道路检测对于维护道路服务性和确保交通安全至关重要，因为道路缺陷会逐渐发展并影响功能。传统的人工评估方法劳动密集、成本高且耗时。虽然数据驱动方法正在获得关注，但真实世界道路缺陷的稀缺性和空间稀疏性在获取高质量数据集方面带来了重大挑战。然而，现有设计用于生成详细合成驾驶场景的模拟器缺乏道路缺陷模型。此外，涉及与道路表面交互的高级驾驶任务，如缺陷区域的规划和控制，仍未得到充分探索。为解决这些局限性，我们提出了一种与城市数字孪生系统集成的多模态传感器平台，用于智能道路检测。首先，使用车载传感器收集的真实世界驾驶数据构建分层道路模型，生成道路缺陷结构和表面高度的高详细度表示。接下来，生成数字道路孪生，创建模拟环境，用于算法性能的综合分析和评估。然后将这些场景导入模拟器，促进数据采集和物理模拟。实验结果表明，包括感知和决策在内的驾驶任务从我们系统生成的高保真道路缺陷场景中受益显著。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统道路检测方法效率低下、成本高以及数据驱动方法面临真实世界道路缺陷数据稀缺的问题。这个问题在现实中非常重要，因为道路缺陷会逐渐发展并影响道路功能，不仅会引起车辆振动，还会加速车辆部件磨损，及时检测和修复这些缺陷对确保交通安全至关重要。传统人工检测方法既危险又对交通流造成干扰，难以大规模应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统道路检测方法的局限性，注意到数据驱动方法面临的挑战，并发现现有模拟器缺乏道路缺陷模型。基于这些观察，他们设计了一个结合多模态传感器平台和城市数字孪生(UDT)系统的智能道路检测方案。该方法借鉴了数字孪生技术、多模态传感器数据处理、深度学习在道路缺陷检测中的应用，以及现有的模拟环境(如CARLA)进行实验。作者的创新点在于将这些现有技术整合到一个专门针对道路检测的系统中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是建立现实-虚拟互联(RVI)的城市数字孪生系统，专门用于智能道路检测。该系统不仅创建道路实体的数字孪生，还构建包含道路缺陷的模拟环境，支持感知和决策算法的评估。整体实现流程包括：1)使用便携式多模态传感器设备收集真实世界的道路数据；2)通过分层道路模型创建器(包括粗粒度流和细粒度流)重建道路表面和缺陷；3)使用数字道路孪生生成器将缺陷模型与表面模型集成，创建高保真场景；4)将这些场景应用于感知任务(如语义分割、立体匹配)和决策任务(如路径规划、速度控制)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开发便携式多模态传感器实验装置；2)提出分层道路模型创建器和数字道路孪生生成器；3)创建包含语义和实例级标注的综合数据集；4)设计将道路缺陷视为负障碍物的实验框架；5)实现现实-虚拟互联的城市数字孪生系统。与之前工作不同，该方法在场景级别集成了有缺陷的道路实体，而非单独处理；重建了3D道路结构而非使用2D平面表示；允许车辆安全绕过或滑过某些缺陷而非完全避免；采用轮胎级碰撞检测而非车身边界框检查，支持更灵活的障碍物避免策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于城市数字孪生的现实-虚拟互联系统，通过高保真道路缺陷场景生成和物理模拟，显著提升了智能道路检测和驾驶任务中感知与决策算法的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3640982&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Road inspection is crucial for maintaining road serviceability and ensuring traffic safety, as road defects gradually develop and compromise functionality. Traditional inspection methods, which rely on manual evaluations, are labor-intensive, costly, and time-consuming. While data-driven approaches are gaining traction, the scarcity and spatial sparsity of real-world road defects present significant challenges in acquiring high-quality datasets. Existing simulators designed to generate detailed synthetic driving scenes, however, lack models for road defects. Moreover, advanced driving tasks that involve interactions with road surfaces, such as planning and control in defective areas, remain underexplored. To address these limitations, we propose a multi-modal sensor platform integrated with an urban digital twin (UDT) system for intelligent road inspection. First, hierarchical road models are constructed from real-world driving data collected using vehicle-mounted sensors, resulting in highly detailed representations of road defect structures and surface elevations. Next, digital road twins are generated to create simulation environments for comprehensive analysis and evaluation of algorithm performance. These scenarios are then imported into a simulator to facilitate both data acquisition and physical simulation. Experimental results demonstrate that driving tasks, including perception and decision-making, benefit significantly from the high-fidelity road defect scenes generated by our system.</description>
      <author>example@mail.com (Yikang Zhang, Chuang-Wei Liu, Jiahang Li, Yingbing Chen, Jie Cheng, Rui Fan)</author>
      <guid isPermaLink="false">2412.17699v2</guid>
      <pubDate>Tue, 16 Dec 2025 17:31:57 +0800</pubDate>
    </item>
    </channel>
</rss>